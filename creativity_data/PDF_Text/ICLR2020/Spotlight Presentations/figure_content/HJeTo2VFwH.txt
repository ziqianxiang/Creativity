Figure 1: (left) layerwise sparsity patterns c ∈ {0, 1}100×100 obtained as a result of pruning for thesparsity level K = {10,.., 90}%. Here, black(0)/White(I) pixels refer to pruned/retained parameters;(right) connection sensitivities (cs) measured for the parameters in each layer. All networks areinitialized With γ = 1.0. Unlike the linear case, the sparsity pattern for the tanh netWork is non-uniform over different layers. When pruning for a high sparsity level (e.g., κK = 90%), this becomescritical and leads to poor learning capability as there are only a feW parameters left in later layers.
Figure 2: (a) Signal propagation (mean Jacobian singular values) in sparse networks pruned forvarying sparsity levels κ, and (b) training behavior of the sparse network at K = 90%. Signalpropagation, pruning scheme, and overparameterization affect trainability of sparse neural networks.
Figure 3: Neural architecture sculptingresults on CIFAR-10. We report gener-alization errors (avg. over 5 runs). Allnetworks have the same number of pa-rameters (269k) and trained identically.
Figure 4:	Full results for (a) signal propagation (all signular value statistics), and (b) training behav-ior (including accuracy) for 7-layer linear and tanh MLP networks. We provide results of LDI-Rand,LDI-Rand-AI, VS-CS, LDI-CS, LDI-CS-AI on the linear case for both singular value statistics andtraining log. We also plot results of LDI-Mag and LDI-Dense on the tanh case for trainability; thetraining results of non-pruned (LDI-Dense) and magnitude (LDI-Mag) pruning are only reported forthe tanh case, because the learning rate had to be lowered for the linear case (otherwise it explodes),which makes the comparison not entirely fair. We provide the singular value statistics for the mag-nitude pruning in Figure 6 to avoid clutter. Also, extended training logs for random and magnitudebased pruning are provided separately in Figure 5 to illustrate the difference in convergence speed.
Figure 5:	Extended training log (i.e., Loss and Accuracy) for random (Rand) and magnitude (Mag)pruning. The sparse networks obtained by random or magnitude pruning take a much longer time totrain than that obtained by pruning based on connection sensitivity. All methods are pruned at thelayerwise orthogonal initialization, and trained the same way as before.
Figure 6:	Signal propagation measurments (all signular value statistics) for the magnitude basedpruning (Mag) on the 7-layer linear and tanh MLP networks. As described in the experiment settingsin Appendix B, the magnitude based pruning is performed on a pretrained model. Notice that unlikeother cases where pruning is done at initialization (i.e., using either random or connection sensitivitybased pruning methods), the singular value distribution changes abruptly when pruned (i.e., note ofthe sharp change of singular values from 0 to 10% sparsity). Also, the singular values are notconcentrated (note of high standard deviations), which explains rather inferior trainability comparedto other methods. We conjecture that naively pruning based on the magnitude of parameters in asingle-shot, without pruning gradually or employing some sophisticated tricks such as layerwisethresholding, can lead to a failure of training compressed networks.
Figure 7:	Signal propagation and training behavior for ReLU and Leaky-ReLU activation functions.
Figure 8: Training performance (loss and accuracy) by different methods for VGG16 on CIFAR-10.
