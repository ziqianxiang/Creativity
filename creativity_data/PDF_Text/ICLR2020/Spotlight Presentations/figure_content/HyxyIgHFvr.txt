Figure 1: (a) The relative norm of the neural tangent kernel as a function of the number of parametersis shown for several networks. This figure highlights the difference between the behavior of ResNetsand other architectures. Figure 1c visualizes the same data in a logarithmic scale. (b) The correlationof the neural tangent kernel before and after training. We expect this coefficient to converge toward1 in the infinite-width limit for multi-layer networks as in Jacot et al. (2018). We do not observe thistrend for ResNets as is clear from the curve corresponding to the WideResNet. (d) The average normof parameter change decreases for simple architectures but stays nearly constant for the WideResNet.
Figure 2: This plot shows the effective rank of each filter for the ResNet-18 models. The filters areindexed on the x-axis, so moving to the right is like moving through the layers of the network. Ourroutines designed to manipulate the rank have exactly the desired effect as shown here.
Figure 3: Plotting the evolution of NTK parameters during training epochs. Left: Norm of the NTKTensor, Middle: Correlation of current NTK iterate versus initial NTK. Right: Reference plot of thenetwork parameter norms.
Figure 4: The similarity coefficient of the neural tangent kernel after training with its initialization.
Figure 5: For reference we record the test accuracy of all models from 1 in the left plot and therelative change in parameters in the right plot.
Figure 6: The correlation coefficient of the neural tangent kernel after training with its initializa-tion for different WideResNet variants - namely WideResNet without batch normalizations andWideResNet without skip connections. We interestingly find that removing either of both prop-erties, which are widely regarding as beneficial for neural network training, stabilizes the trend seenin the default WideResNet. However both variants hardly converge toward 1, even when samplingvery wide ResNets.
