Figure 1: Top: the macro skeleton of each architecture candidate. Bottom-left: examples of neuralcell with 4 nodes. Each cell is a directed acyclic graph, where each edge is associated with anoperation selected from a predefined operation set as shown in the Bottom-right.
Figure 2: Training, validation, test accuracy of each architecture on CIFAR-10, CIFAR-100, andImageNet-16-120. We also visualize the results of ResNet in the orange star marker.
Figure 3: The ranking of each ar-chitecture on three datasets, sortedby the ranking in CIFAR-10.
Figure 4: We report the correlation coefficient betweenthe accuracy on 6 sets, i.e., CIFAR-10 validation set (C10-V), CIFAR-10 test set (C10-T), CIFAR-100 validation set(C100-V), CIFAR-100 test set (C100-T), ImageNet-16-120validation set (I120-V), ImageNet-16-120 test set (I120-T).
Figure 5: The ranking of all architectures based on the validation accuracy at different time stamps(y axis) sorted by the final test accuracy (x axis).
Figure 6:	We show results of 500 runs for RS, REA, REINFORCE, and BOHB on CIFAR-10.
Figure 7:	Results keeping keep running estimates for BN layers in each Searching cell. We useparameter sharing based NAS methods to search the architecture on CIFAR-10. After each search-ing epoch, We derive the architecture and show its validation accuracy (VALID) and test accuracy(TEST) on CIFAR-10. The 0-th epoch indicates the architecture is derived from the randomly ini-tialized architecture encoding.
Figure 8:	Results using batch statistics without keeping keep running estimates for BN layers ineach searching cell. We use parameter sharing based NAS methods to search the architecture onCIFAR-10. After each searching epoch, we derive the architecture and show its validation accuracy(VALID) and test accuracy (TEST) on CIFAR-10. The 0-th epoch indicates the architecture isderived from the randomly initialized architecture encoding.
