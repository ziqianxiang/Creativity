Figure 1:	An illustration of the proposed problem.
Figure 2:	The domain-specific language (DSL) for con-structing programs. Each program is composed of do-main dependent perception, subtasks, and control floWs.
Figure 3: Program Guided Agent. The proposed modular framework comprehends and fulfills a desiredtask specified by a program. The program interpreter executes the program by altering between queryingthe perception module with a query q when an environment condition is encountered (e.g. env[Gold]>0,is_there[River]) and instructing a policy when it needs to fulfill a goal/subtask g (e.g. mine(Gold),build_bridge()). The perception module produces a response h to answer the query, determining whichpaths in the program should be chosen. The policy takes a sequence of low-level actions a (e.g. moveUp,moveLeft, Pickup) interacting with the environment to accomplish the given subtask (e.g. mine(Gold)).
Figure 4: Learning a multitask policy via learned modulation. (a) A multitask policy takes both a state s and agoal specification g as inputs and produces an action distribution a 〜π(s,g). Instead of simply concatenatingthe state and goal in a raw space or a latent space, we propose to modulate state features es using the goal.
Figure 5: Analysis on end-to-end learning models: (a) Models learning from programs generalize better tolonger instructions. Transformer is more robust to longer instructions (Upper). Tree-RNN exploiting the programstructure generalizes the best, but performs worst for shorter programs (Lower). (b) Seq-LSTM learning fromboth instructions performs worse as the diversity increases. Transformer learns better from natural languagewhen the instructions are less diverse (Upper). Transformer and Tree-RNN learning from programs are moreconsistent as the diversity increases, yet perform worse on less diverse instructions (Lower).
Figure 6: Exemplar rendered environment map. The agent, objects, and stuff are represented as blocks with theircorresponding textures. Specifically, the agent is represented as a female character. gold is represented as agolden block, wood is shown as a tree, and iron is represented as a sliver block. River is shown as a bluegrid with water texture while bridge is presented as wooden grid. merchant is shown as an alpaca, whichis supposed to transport the sold objects. Notice that there are 2 merchants in (a) and (b), while (c) and (d)contains 3 and 4 of them, respectively. The boundaries of the map are shown as brick walls.
Figure 7:	First failure rate of subtasks. Every colored grid shows the first failure rate of each subtask. From top-left to bottom-right, each block of grids show the results for subtask category goto, place, build_bridge,mine, and sell. Warmer colors indicate higher first failure rate; while colder colors indicate lower first failurerate. White grids indicate subtasks that either never occurs as first failed task in any execution or do not exist inthe executions that lead to this figure.
Figure 8:	Average time cost of subtasks. The setup of this plot is similar to that of Figure 7. Warmer colorsindicate higher average subtask time cost; while colder colors indicate lower average subtask time cost. Whitegrids indicate subtasks that do not exist in the executions that lead to this figure.
Figure 9:	Additional analysis on completion rates. The results of executing program instructions on both datasetsare used to produce the six plots above. In each plot, each color corresponds to a different model that we propose.
Figure 10: Exemplar data and languages ambiguity. The goal of the examples above is to show that naturallanguage instructions while being flexible enough to capture the high-level semantics of the task, can beambiguous in different ways and thus might lead to impaired performance. In example (a), the modifier "repeatthe following 3 times" has an unclear scope, resulting in two possible interpretations shown in program format onthe right side; in example (b), "repeat 4 times" can be used to modify either the previous part of the descriptionor the latter part of it, resulting in ambiguity; in example (c), the last sentence starting with "If" has unclearscope. In all of the above cases, a model that learns to execute instructions presented in natural language formatmight fail to execute the instructions successfully because of the ambiguity of the language instructions.
Figure 11: Program set statistics for training set (train).
Figure 12:	Program set statistics for same complexity testing set (test).
Figure 13:	Program set statistics for more complex testing set (test-complex).
