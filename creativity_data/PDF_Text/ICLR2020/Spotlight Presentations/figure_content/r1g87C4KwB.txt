Figure 1: Visualization of the early part of the training trajectories on CIFAR-10 (before reaching65% training accuracy) of a simple CNN model optimized using SGD with learning rates η = 0.01(red) and η = 0.001 (blue). Each model on the training trajectory, shown as a point, is representedby its test predictions embedded into a two-dimensional space using UMAP. The background colorindicates the spectral norm of the covariance of gradients K (λ1K, left) and the training accuracy(right). For lower η, after reaching what we call the break-even point, the trajectory is steeredtowards a region characterized by larger λ1K (left) for the same training accuracy (right). See Sec. 4.1for details. We also include an analogous figure for other quantities that we study in App. A.
Figure 2: The spectral norm of H (λ1H, left) and ∆L (difference in the training loss computed be-tween two consecutive steps, right) versus λ1K at different training iterations. Experiment was per-formed with SimpleCNN on the CIFAR-10 dataset with two different learning rates (color). Consis-tently with our theoretical model, λ1K is correlated initially with λ1H, and training is generally stable(∆L > 0) prior to achieving the maximum value of λ1K .
Figure 3: The spectrum of K (left) and H (right) at the training iteration corresponding to the largestvalue of λ1K and λ1H, respectively. Experiment was performed with SimpleCNN on the CIFAR-10dataset with two different learning rates (color). Consistently with Conjecture 2, training with lowerlearning rate results in finding a region of the loss surface characterized by worse conditioning of Kand H (visible in terms of the large number of “spikes” in the spectrum, see also Fig. 4).
Figure 4: The variance reduction and the pre-conditioning effect of SGD in various settings. Theoptimization trajectories corresponding to higher learning rates (η) or lower batch sizes (S) arecharacterized by lower maximum λ1K (the spectral norm of the covariance of gradients) and largermaximum λK/λK (the condition number of the covariance of gradients). Vertical lines mark epochsat which the training accuracy is larger (for the first time) than a manually picked threshold, whichillustrates that the effects are not explained by differences in training speeds.
Figure 5: The variance reduction effect of SGD, for ResNet-32 and SimpleCNN. Trajectories cor-responding to higher learning rates (η, left) or smaller batch sizes (S, right) are characterized bya lower maximum λ1H (the spectral norm of the Hessian) along the trajectory. Vertical lines markepochs at which the training accuracy is larger (for the first time) than a manually picked threshold.
Figure 6: The variance reduction and the pre-conditioning effect of SGD, demonstrated on twolarger scale settings: BERT on the MNLI dataset (left), and DenseNet on the ImageNet dataset(right). For each setting We report λK (left) and λ*/λK (right). Vertical lines mark epochs at whichthe training accuracy is larger (for the first time) than a manually picked threshold.
Figure 7: The effect of changing the learning rate on various metrics (see text for details) for Sim-pleCNN with and without batch normalization layers (SimpleCNN-BN and SimpleCNN).
Figure 8: Analogous to Fig. 1. The background color indicates the conditioning of the covarianceof gradients K (λK/λK, left), the trace of the covariance of gradients (Tr(K), middle), and thespectral norm of the Hessian (λ1H , right).
Figure 9: Pearson correlation between λ1K calculated using either M = 128 or M = 1 for threedifferent values of η = 0.1, 0.01 and 0.001.
Figure 10: Additional metrics for the experiments using SimpleCNN on the CIFAR-10 dataset withdifferent learning rates. From left to right: training accuracy, validation accuracy, Tr(K).
Figure 11: Additional metrics for the experiments using SimpleCNN on the CIFAR-10 dataset withdifferent batch sizes. From left to right: training accuracy, validation accuracy, and Tr(K).
Figure 12: Additional figures for the experiments using ResNet-32 on the CIFAR-10 dataset withdifferent learning rates. From left to right: the evolution of accuracy, validation accuracy, Tr(K).
Figure 13: Additional figures for the experiments using ResNet-32 on the CIFAR-10 dataset withdifferent batch sizes. From left to right: training accuracy, validation accuracy, Tr(K).
Figure 14: Additional figures for the experiments using LSTM on the IMDB dataset with differentlearning rates. From left to right: the evolution of accuracy, validation accuracy, and Tr(K).
Figure 15: Additional metrics for the experiments using LSTM on the IMDB dataset with differentbatch sizes. From left to right: training accuracy, validation accuracy, λ1H and Tr(K).
Figure 16: Additional metrics for the experiments using BERT on the MNLI dataset with differentlearning rates. From left to right: training accuracy, validation accuracy, and Tr(K).
Figure 17: Additional metrics for the experiments using DenseNet on the ImageNet dataset withdifferent learning rates. From left to right: training accuracy, validation accuracy, and Tr(K).
Figure 18: Results of experiment using the MLP model on the FashionMNIST dataset for differentlearning rates. From left to right: λK, λK/λK, λ% and Tr(K).
Figure 19: Training accuracy and validation accuracy for experiment in Fig. 18.
Figure 20: The variance reduction and the pre-conditioning effect for SimpleCNN trained usingSGD with momentum. From left to right: λK, λ*/λK and Tr(K).
Figure 21:	Training accuracy and validation accuracy for the experiment in Fig. 20.
Figure 22:	The variance reduction and the pre-conditioning effect for SimpleCNN trained usingSGD with momentum. From left to right: λK, λK/λK and Tr(K).
Figure 23:	Training accuracy and validation accuracy for the experiment in Fig. 22.
Figure 24: The variance reduction and the pre-conditioning effect for SimpleCNN trained usingSGD with learning rate schedule. From left to right: λK, λ*/λK and Tr(K).
Figure 25: Training accuracy and validation accuracy for the experiment in Fig. 24.
Figure 26: Evolution of various metrics that quantify conditioning of the loss surface for SimpleCNNwith with batch normalization layers (SimpleCNN-BN), for different batch sizes.
