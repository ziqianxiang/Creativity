Figure 1: Retraining accuracy vs. epoch numbers at which the subnetworks are drawn, for bothPreResNet101 and VGG16 on the CIFAR-10/100 datasets, where P indicates the channel pruningratio and the dashed line shows the accuracy of the corresponding dense model on the same dataset,☆ denotes the retraining accuracies of subnetworks drawn from the epochs with the best searchaccuracies, and error bars show the minimum and maximum of three runs.
Figure 2: Retraining accuracy and total training FLOPs comparison vs. epoch number at which thesubnetwork is drawn, when using 8 bits precision during the stage of identifying EB tickets basedon the VGG16 model and CIFAR-10/100 datasets, where p indicates the channel-wise pruning ratioand the dashed line shows the accuracy of the corresponding dense model on the same dataset.
Figure 3: Visualization of the pairwise mask distance matrix for VGG16 and PreResNet101 onCIFAR-100.
Figure 4: A high-level overview of the commonly adoptedprogressive pruning and training scheme and our EB Train.
Figure 5: The total training FLOPs vs. the epochs at which the subnetworks are drawn from, for boththe PreResNet101 and VGG16 models on the CIFAR-10 and CIFAR-100 datasets, where p indicatesthe channel-wise pruning ratio for extracting the subnetworks. Note that the EB tickets at all casesachieve comparable or higher accuracies and consume less FLOPs than those of the “ground-truth”winning tickets (drawn after the full training of 160 epochs).
Figure 6: The energy measurement setup (from left to right) with a MAC Air latptop, an embeddedGPU (JETSON TX2 (NVIDIA Inc.)), and a power meter.
