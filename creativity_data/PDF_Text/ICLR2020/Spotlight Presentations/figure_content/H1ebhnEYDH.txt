Figure 2: A) Classification images of a CNN trained on MNIST (with 99.2% test accuracy). Imagetitles show ground truth, predicted class for the bias map, and the frequency of the noise patternsclassified as that digit. B) Classification images of logistic regression over MNIST with 92.46% testaccuracy. C) Confusion matrices of four classifiers (CNN and log. reg. biases, mean digit image,and log. reg. weights). The classification was done via template matching using dot product.
Figure 3: A) Mean train-ing images (top) and meanwhite noise pattern/bias maps(bottom) across CIFAR-10classes. Image titles showground truth class and predic-tion of the bias map, respec-tively. B) Confusion matricesusing mean images (top) andbias maps (bottom) as classi-fiers, respectively. Notice thatfor some classes, it is easier toguess the class label from themean image (e.g., frog).
Figure 4: Progressivebuild-up of the bias mapsfor 0, 1, and 2.
Figure 5: Classification images, some sample generated images, confusion matrices of bias mapclassifiers, as well as one sample image and its reconstruction using Gabor wavelets over MNIST(left), Fashion-MNIST (middle) and CIFAR-10 (right) datasets. Please see Appendix for details onGabor filter bank, image generation using linear regression, and PCA. We used 960, 960 and 1520Gabor wavelets over MNIST, Fashion-MNIST, and CIFAR-10, respectively. The correspondingnumber of PCA components are 250, 250 and 600 (per color channel).
Figure 6: A) Adding bias to a digit changes it to the target class in many cases (here with γ = 0.8).
Figure 7: A) Top: A 10-way CNN trained on MNIST (with half of the zeros augmented with apatch and relabeled as 1) performs very well on a clean test set (top confusion matrix). On a test setcontaining all zeros contaminated, it (incorrectly) classifies them as one. Classification images (rightside) successfully reveal the perturbed region. Bottom: Same as above but over 8 and 9 digits. B)Classification images reveal the adversarial patch attack over CIFAR-10. Here, half of the birds arecontaminated with a patch and are labeled as cat. C) Turning a frog into a car by adding the activationof the conv6 layer, computed using white noise, of the car category to the frog. See supplement. D)Average gradients before the adversarial patch attack (top) and after the attack (middle). The smallyellow region on the top-right of digit 8 means that increasing those pixels increases the loss andthus leads to misclassification (i.e., turns 8 to another digit). (bottom) Average gradient with all 8scontaminated and relabeled as 9. The blue region on the top-right of digit 9 means that increasingthose pixels lowers the loss and thus leads to classifying a digit as 9. This analysis is performed overthe MNIST training set. Please see also Figs. 18 and 21.
Figure 8: Example filters	MNISTderived using spike trig-gered averaging (STA) forthe first two conv layers ofa CNN trained on MNISTdataset (left; RF sizes are5 × 5 and 14 × 14) and4 layers of a CNN onCIFAR-10 dataset (right;RF sizes in order are 3 ×3, 5 × 5, 14 × 14 and 32× 32). See also Fig. 22in the supplement for filterweights (i.e., CNN trainedover real data).
Figure 9:	Psychometriccurves of a CNN trained onMNIST. The x-axis shows themagnitude of the signal addedto the noise (panel D). They-axis shows the accuracy.
Figure 10:	The architecture of the models used in this study including MLP, CNN, RNN, AutoEn-coder, and VAE.
Figure 11:	The architecture of the CNN used to classify CIFAR-10 images.
Figure 12: More examples and illustration of classification images concept.
Figure 13: Classification images for a two layer MLP (784 -→ 1000 -→ 10) shown at the topand an RNN classifier at the bottom. None of the noise patterns were classified as 1 using bothclassifiers. While the derived biases do not resemble digits, they still convey information to predictthe class of a test digit. Please see Figs. 2 and Fig. 3 in the main text.
Figure 14: Analysis of sample complexity for deriving classification images from a CNN trained onMNIST dataset (see Fig. 2). With around 10K samples, computed biases already start to resemblethe target digits.
Figure 15: Using an AutoEncoder and a VAE to generate samples containing faint structures to beused for computing the classification images over MNIST dataset, using a CNN classifier. Bothgenerators were trained only for two epochs to prohibit the CNN to from generating perfect samples(shown at the top). Bottom panels show classification images derived using 100, 1K, and 10Ksamples from each generator. Note that classification images converge much faster now comparedwith the white noise stimuli.
Figure 16: Top: frequency of Gabor noise classified as a Fashion MNIST class. Middle: same asabove but using white noise. Bottom: Classification images using white noise. See also Fig. 3.
Figure 18: Confusion matrices for adversarial patch attack on CIFAR-10 dataset (bird to cat). Classnames: plane, car, bird, cat, deer, dog, frog, horse, ship and truck. See Fig. 7.
Figure 19: Mean MNIST digits, clean training set (top) and adversarial training set (bottom). SeeFig. 7.
Figure 20: Top) Average layer activation using noise (left) and real data (right) over a CNN trainedon CIFAR-10 dataset. Bottom) Mean distance between average layer activations of different classesacross model layers.
Figure 21: Effect of adding activation at conv6, conv4, conv2 and input of noises classified asdifferent classes to real images. The figure shows CIFAR-10 model misclassification ratio vs. γ,where input to the model is((1 - γ) × noise activation of a certain class + γ × real data input image).
Figure 22: Trained model weights (i.e., convolutional kernels) of the first layer of a CNN trained onMNIST or CIFAR-10. These are not calculated by feeding noise patterns. They are derived aftertraining the model on data. Interestingly, they are the same as those derived using white noise (SeeFig. 8 in the main text.
Figure 23: Result of microstimulation over MNIST digits using a 10-way CNN classifier. Bias isincreased for all layers.
Figure 24: Result of microstimulation over MNIST digits using a 10-way CNN classifier. Bias isdecreased for all layers.
Figure 25: Results of microstimulation for a two binary decision making tasks using a CNN classifier(1 vs. 3) and (2 vs. 8). Left(right) panels show increasing (decreasing) bias for each layer. See Fig. 9in the main text.
