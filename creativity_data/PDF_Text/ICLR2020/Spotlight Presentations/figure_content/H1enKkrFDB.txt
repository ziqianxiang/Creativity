Figure 1: Noise Sensitivity(lower the better). Test ac-curacy: SRN (73.1%), SN(71.5%), and Vanilla (72.4%).
Figure 2: Test accuracies on CIFAR100 for clean data. Higher is better.
Figure 3: Train accuracies on CIFAR100 for shattering experiment. Lower indicate less memorization,thus, better.
Figure 4: (log) Sample complexity (Calg) of ResNet-110 (Figure 4a to 4c), WideResNet-28-10 (Fig-ure 4d to 4f), and Densenet-100 (Figure 4g to 4i) quantified using the three measures discussed in thepaper. Left is better. Vanilla is omitted from Figure 4b, 4c, 4h and 4i as it is too far to the right. Also,in situations where SRN-50 and SN performed the same, we removed the histogram to avoid clutter.
Figure 5: eLhist for unconditional GAN on CIFAR10. Dashed vertical lines represent 95th percentile.
Figure 6: Test Error and Generalization Error of AlexNet trained with SGD with lr = 0.1 on (clean)CIFAR-100. (Lower is betterwâˆ•o WDWD(b) Test Errormore aggressive stable rank constraint decreases fitting the random data . Similar results are seen forResNet-110 in Figure 7b.
Figure 7: Training error on randomly labelled CIFAR-100 with a learning rate of 0.01 and with/ without weight decay. (Higher is better.
Figure 8: Test accuracies on CIFAR100 for clean data using a stopping criterion based on trainaccuracy. Higher is better.
Figure 9: Test accuracies on CIFAR10 for clean data using a stopping criterion based on trainaccuracy. Higher is better.
Figure 11: Training accuracy on randomly labelled CIFAR-10 (Lower is better).
Figure 10: Test accuracies on CIFAR10 for clean data using the number of epochs as a stoppingcriterion. Higher is better.
Figure 12: Comparison: eLhist of the discriminator in the conditional GAN setting with projection discriminatoron CIFAR100.
Figure 14: Comparison: eLhist of the discriminator for pairs of samples from the real distribution on CIFAR10.
Figure 13: Comparison: eLhist of the discriminator for pairs of samples selected from the generator onCIFAR10(b) Unconditional GAN setting.
Figure 15: Jacobian norm of the discriminator in the neighbourhood of the samples from the generator trainedon CIFAR10.
Figure 16: Jacobian norm of the discriminator in the neighbourhood of the samples from the real dataset(CIFAR10).
Figure 17: Loss incurred by the discriminator. The loss of SRN-GAN with the stable rank constraint of 70 isshifted upwards by 0.2 so that we can compare the change of the loss during training as opposed to the absolutemagnitude of the loss.
Figure 18: Image samples generated from the unconditional SRN-GAN.
Figure 19: Image samples generated from the unconditional SN-GAN.
Figure 20: Image samples generated from the unconditional SRN-GAN, SN-GAN, and WGAN-GP.
