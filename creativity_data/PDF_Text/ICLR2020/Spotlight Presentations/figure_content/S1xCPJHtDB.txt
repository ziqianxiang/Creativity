Figure 1: Main loop of SimPLe. 1) the agent starts interacting with the real environment following the latestpolicy (initialized to random). 2) the collected observations will be used to train (update) the current worldmodel. 3) the agent updates the policy by acting inside the world model. The new policy will be evaluatedto measure the performance of the agent as well as collecting more data (back to 1). Note that world modeltraining is self-supervised for the observed states and supervised for the reward.
Figure 2: Architecture of the proposed stochastic model with discrete latent. The input to the model is fourstacked frames (as well as the action selected by the agent) while the output is the next predicted frame andexpected reward. Input pixels and action are embedded using fully connected layers, and there is per-pixelsoftmax (256 colors) in the output. This model has two main components. First, the bottom part of the networkwhich consists of a skip-connected convolutional encoder and decoder. To condition the output on the actionsof the agent, the output of each layer in the decoder is multiplied with the (learned) embedded action. Secondpart of the model is a convolutional inference network which approximates the posterior given the next frame,similarly to Babaeizadeh et al. (2017a). At training time, the sampled latent values from the approximatedposterior will be discretized into bits. To keep the model differentiable, the backpropagation bypasses thediscretization following Kaiser & Bengio (2018). A third LSTM based network is trained to approximate eachbit given the previous ones. At inference time, the latent bits are predicted auto-regressively using this network.
Figure 3: Comparison with Rainbow and PPO. Each bar illustrates the number of interactions with environmentrequired by Rainbow (left) or PPO (right) to achieve the same score as our method (SimPLe). The red lineindicates the 100K interactions threshold which is used by the our method.
Figure 4: Fractions of Rainbow and PPO scores at different numbers of interactions calculated with the formula(SimPLe_score@100K - random_score)/(baseline_score - random_score); if denominator is smallerthan 0, both nominator and denominator are increased by 1. From left to right, the baselines are: Rainbow at100K, Rainbow at 200K, PPO at 100K, PPO at 200K. SimPLe outperforms Rainbow and PPO even when thoseare given twice as many interactions.
Figure 5: Behaviour with respect to the number of used samples. We report number of frames required by PPOto reach the score of our models. Results are averaged over all games.
Figure 6: Impact of the environment stochasticity.
Figure 7: Ablations part 1. The graphs are in the same format as Figure 3: each bar illustrates the number ofinteractions with environment required by Rainbow to achieve the same score as a particular variant of SimPLe.
Figure 8: Ablations part 2. The graphs are in the same format as Figure 3: each bar illustrates the number ofinteractions with environment required by Rainbow to achieve the same score as a particular variant of SimPLe.
Figure 9: (left) CDF of the number of iterations to acquire maximum score. The vertical axis represents thefraction of all games. (right) Comparison of random starts vs no random starts on Seaquest (for betterreadability we clip game rewards to {-1, 0, 1}). The vertical axis shows a mean reward and the horizontal axisthe number of iterations of Algorithm 1.
Figure 10: Frames from the Pong environment.
Figure 11: Frames from the Kung Fu Master environment (left) and its model (right).
Figure 12: Fractions of the rainbow scores at given number of samples. These were calculate with the formula(SimP Le_score - random_score)/(rainbow_score - random_score); if denominator is smaller than 0,both nominator and denominator are increased by 1.
Figure 13: Fractions of the ppo scores at given number of samples. These were calculate with the formula(SimP Le_score - random_score)/(ppo_score - random_score); if denominator is smaller than 0, bothnominator and denominator are increased by 1.
Figure 14: Comparison of scores from Simple against Rainbow and PPO at different numbers of interactions.
