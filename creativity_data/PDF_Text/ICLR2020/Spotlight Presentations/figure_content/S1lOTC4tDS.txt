Figure 1: Dreamerlearns a world modelfrom past experienceand efficiently learnsfarsighted behaviors inits latent space bybackpropagating valueestimates back throughimagined trajectories.
Figure 2: Image observations for 5 of the 20 visual control tasks used in our experiments. The taskspose a variety of challenges including contact dynamics, sparse rewards, many degrees of freedom,and 3D environments. Several of these tasks could previously not be solved through world models.
Figure 3: Components OfDreamer. (a) From the dataset of past experience, the agent learns to encodeobservations and actions into compact latent states (∙), for example via reconstruction, and predictsenvironment rewards ( ). (b) In the compact latent space, Dreamer predicts state values (l.j) andactions (BSe) that maximize future value predictions by propagating gradients back through imaginedtrajectories. (c) The agent encodes the history of the episode to compute the current model state andpredict the next action to execute in the environment. See Algorithm 1 for pseudo code of the agent.
Figure 4: Imagination horizons. We compare the final performance of Dreamer, learning an actionmodel without value prediction, and online planning using PlaNet. Learning a state value model toestimate rewards beyond the imagination horizon makes Dreamer more robust to the horizon length.
Figure 5: Reconstructions of long-term predictions. We apply the representation model to the first 5images of two hold-out trajectories and predict forward for 45 steps using the latent dynamics, givenonly the actions. The recurrent state space model (RSSM; Hafner et al., 2018) performs accuratelong-term predictions, enabling Dreamer to learn successful behaviors in a compact latent space.
Figure 6:	Performance comparison to existing methods. Dreamer inherits the data-effiCiency ofPlaNet while exceeding the asymptotic performance of the best model-free agents. After 5 × 106environment steps, Dreamer reaches an average performance of 823 across tasks, compared to PlaNetat 332 and the top model-free D4PG agent at 786 after 109 steps. Results are averages over 5 seeds.
Figure 7:	Dreamer succeeds at visual control tasks that require long-horizon credit assignment, suchas the acrobot and hopper tasks. Optimizing only imagined rewards within the horizon via an actionmodel or by online planning yields shortsighted behaviors that only succeed in reactive tasks, such asin the walker domain. The performance on all 20 tasks is summarized in Figure 6 and training curvesare shown in Appendix D. See Tassa et al. (2018) for performance curves of D4PG and A3C.
Figure 8:	Comparison of representation learning objectives to be used with Dreamer. Pixel recon-struction performs best for the majority of tasks. The contrastive objective solves about half of thetasks, while predicting rewards alone was not sufficient in our experiments. The results suggest thatfuture developments in learning representations are likely to translate into improved task performancefor Dreamer. The performance curves for all tasks are included in Appendix E.
Figure 9: Performance of Dreamer in environments with discrete actions and early termination.
Figure 10: Comparison of action selection schemes on the continuous control tasks of the DeePMindControl Suite from pixel inputs. The lines show mean scores over environment steps and the shadedareas show the standard deviation across 5 seeds. We compare Dreamer that learns both actionsand values in imagination, to only learning actions in imagination, and Planet that selects actionsby online planning instead of learning a policy. The baselines include the top model-free algorithmD4PG, the well-known A3C agent, and the hybrid SLAC agent.
Figure 11: Comparison of representation learning methods for Dreamer. The lines show mean scoresand the shaded areas show the standard deviation across 5 seeds. We compare generating bothimages and rewards, generating rewards and using a contrastive loss to learn about the images, andonly predicting rewards. Image reconstruction provides the best learning signal across most of thetasks, followed by the contrastive objective. Learning purely from rewards was not sufficient in ourexperiments and might require larger amounts of experience.
Figure 12: Robustness of Dreamer to different control frequencies. Reinforcement learning methodscan be sensitive to this hyper parameter, which could be amplified when learning dynamics modelsat the control frequency of the environment. For this experiment, we train Dreamer with differentamounts of action repeat. The areas show one standard deviation across 2 seeds. We used a previoushyper parameter setting for this experiment. We find that a value of R = 2 works best across tasks.
