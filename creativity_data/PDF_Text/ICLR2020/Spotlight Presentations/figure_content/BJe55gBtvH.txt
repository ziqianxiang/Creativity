Figure 1: The neural netWork instantiations that are used to create tWo different tent maps Which vary only inthe maximum value. This is effected by a small change of weights in the output layer. All activation functionsare ReLU’s.
Figure 2:	Compositions of the logistic map f(x) = 3.9x(1 — x) defined on the interval [0, 1]. This map iswell known to exhibit chaos and in the above figure has non-vanishing oscillations that grow with the numberof compositions, albeit irregularly.
Figure 3:	Compositions of t(x; μ) with different parameters μ = 2 and μ = 1 are shown. The compositionscreate (exponential) non-vanishing oscillations when μ = 2, however the compositions remain unchangedwhen μ = 1.
Figure 4: The covering relations of intervals J0, ..., Jr from Lemma 1. Observe that the graph is a directedcycle with a self loop at interval J0. Note that there might be more relations (“edges”).
Figure 5: Compositions of a piecewise linear function that has a point of period 3.
Figure 6: A piecewise linear function f : [1, 4] → [1, 4] that has prime period four.
Figure 7: Compositions of a piecewise linear function that has a point of period 5. We start with the all onesvector for δ0 and each composition arises from the covering relation between the sets.
Figure 8: The compositions of the logistic map f(x; r) := rx(1 - x) with different parameter values areshown here. The left column has the functions themselves while the right column shows the correspondingcompositions. We can see that oscillations in these family of functions vary vastly with changes in r and thesechanges are made in the weights of an appropriate neural network (see Telgarsky (2016),Schmitt (2000)).
Figure 9: We see that f3 (x) = x has solutions other than just the fixed point, since it has intersections in 3other places, other than the fixed point. However, g3 (x) = x does not have any solutions other than the fixedpoint, as there are no other intersections.
Figure 10: We see that depth does reduce the classification error for this particular task and when depth is 5,the classification error is close to 0. The saturation in between may be attributed to the general uncertainties inthe training/optimization.
