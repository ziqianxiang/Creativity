Figure 1: Architecture of the word2ket (left) and word2ketXS (right) embeddings. The word2ketexample depicts a representation of a single-word 256-dimensional embedding vector using rank 5,order 4 tensor P5k=1 Nj4=1 vjk that uses twenty 4-dimensional vectors vjk as the underlying train-able parameters. The word2ketXS example depicts representation ofa full 81-word, 16-dimensionalembedding matrix as P5k=1 Nj4=1 Fjk that uses twenty 3 × 2 matrices Fjk as trainable parameters.
Figure 2: Dynamics of the test-set F1 score on SQuAD dataset using DrQA model with differentembeddings: rank-2 order-2 word2ketXS, rank-1 order-4 word2ketXS, and regular embedding.
Figure 3: Test set questions and answers from DrQA model trained using rank-1 order-4word2ketXS embedding that utilizes only 380 parameters (four 19 × 5 matrices Fjk, see eq. 4)to encode the full, 118,655-word embedding matrix.
