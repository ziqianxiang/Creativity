Figure 1: 1D illustration. Fita 1D func-tion (green dotted curve) using a k-WTAmodel provided with a set of points (red).
Figure 2: Different activation functions. ReLU: all neurons with negative activation values willbe set to zero. Max-pooling: only the largest activation in each group is transmitted to the next layer,and this effectively downsample the output. LWTA: the largest activation in each group retains itsvalue when entering the next layer, others are set to zero. k-WTA: the k largest activations in theentire layer retain their values when entering the next layer, others are set to zero (k = 3 in thisexample). Note that the output is not downsampled through ReLU, LWTA and k-WTA.
Figure 3: (a, b) We plot the change of 10 logits values when conducting untargeted PGD attack with100 iterations. X-axis indicates the perturbation size and Y-axis indicates the 10 color-coded logitsvalues. (a) When we apply PGD attack on k-WTA ResNet18, the strong discontinuities w.r.t. to inputinvalidate gradient estimation, effectively defending well against the attack. (b) In contrast, for aReLU ResNet18, PGD attack can easily find adversarial examples due to the model’s smooth changew.r.t. input. (c) In the process of training k-WTA ResNet18, the loss change w.r.t. model weights islargely smooth. Thus, the training is not harmed by k-WTA’s discontinuities.
Figure 4: Robustness changing w.r.t. γ on CIFAR. When γ decreases, the standard test accuracy (left)starts to drop after a certain point. The robust accuracy (right) first increases then decreases.
Figure 5: Gradient-based attack’s loss landscapes in k-WTA (a, b) and conventional ReLU models (c,d). (a,b) k-WTA Models have much more non-convex and non-smooth landscapes. Also, the modeloptimized by adversarial training (b) has a lower absolute value of loss.
Figure 6: Efficacy of incremental training. We sweep through a range of sparsity ratios, andevaluate the standard and robust accuracies of two network structures (left: ResNet18 and right: WideResNet). We compare the performance differences between the regular training (i.e., training withoutincremental fine-tuning) and the training with incremental fine-tuning.
Figure 7: Visualization of ResNet18 and DenseNet121 with different γ values. The last one(DenseNet121-0.1+adv) is the result using adversarial training. The others are optimized usingnatural (non-adversarial) training.
