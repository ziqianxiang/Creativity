Figure 1: Model architecture. Parameters of p*(τ∕θi)are generated based on the conditional information ci .
Figure 2: Normalizing flows define aflexible distribution via transformations.
Figure 3: NLL loss for event time prediction without marks (left) and with marks (right). NLLof each model is standardized by subtracting the score of LogNormMix. Lower score is better.
Figure 4: By sampling the missing values from p*(τ) during training, LogNormMix learns the trueunderlying data distribution. Other imputation strategies overfit the partially observed sequence.
Figure 5: Conditional informa-tion improves performance.
Figure 7: Sequence embeddingslearned by the model.
Figure 6: Sequences generatedbased on different embeddings.
Figure 8: Different choices for modeling p(τ): exponential distribution (left), Gompertz distribution(center), log-normal mixture (right). Mixture distribution can approximate any density while beingtractable and easy to sample from.
Figure 9: Models learn different conditional distribution p(τ|H) on YelP dataset. Since check-insoccur during the opening hours, true distribution of the next check-in resembles the one on the right.
