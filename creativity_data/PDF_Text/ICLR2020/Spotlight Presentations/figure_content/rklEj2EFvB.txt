Figure 1: Bernoulli gradient variance (on log scale) as a function of the number of model evaluations(including baseline evaluations, so the sum-and-sample estimators with sampled baselines use twiceas many evaluations). Note that for some estimators, the variance is 0 (log variance -âˆž) for k = 8.
Figure 2: VAE smoothed training curves (-ELBO) of two independent runs when training withdifferent estimators with k = 1, 4 or 8 (thicker lines) samples (ARSM has a variable number).
Figure 3: TSP validation set optimality gap measured during training. Raw results are light,smoothed results are darker (2 random seeds). We compare our estimator against different un-biased and biased (dotted) multi-sample estimators and against single-sample REINFORCE, withbatch-average or greedy rollout baseline.
Figure 4:	Gradient log variance of different unbiased estimators with k = 4 samples, estimated every100 (out of 1000) epochs while training using REINFORCE with replacement. Each estimator iscomputed 1000 times with different latent samples for a fixed minibatch (the first 100 records oftraining data). We report (the logarithm of) the sum of the variances per parameter (trace of thecovariance matrix). Some lines coincide, so we sort the legend by the last measurement and reportits value.
Figure 5:	Smoothed validation -ELBO curves during training of two independent runs when withdifferent estimators with k = 1, 4 or 8 (thicker lines) samples (ARSM has a variable number). Somelines coincide, so we sort the legend by the lowest -ELBO achieved and report this value.
Figure 6:	Smoothed training and validation -ELBO curves during training on the standard binarizedMNIST dataset (Salakhutdinov & Murray, 2008; Larochelle & Murray, 2011) of two independentruns when with different estimators with k = 1, 4 or 8 (thicker lines) samples (ARSM has a variablenumber). Some lines coincide, so we sort the legend by the lowest -ELBO achieved and report thisvalue.
