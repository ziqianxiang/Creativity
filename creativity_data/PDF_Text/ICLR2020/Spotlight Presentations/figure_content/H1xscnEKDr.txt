Figure 1: (a) An example of the eyeglass frame attack. Left: original face input image. Middle:modified input image (adversarial eyeglasses superimposed on the face). Right: an image of thepredicted individual with the adversarial input in the middle image. (b) An example of the stop signattack. Left: original stop sign input image. Middle: adversarial mask. Right: stop sign image withadversarial stickers, classified as a speed limit sign.
Figure 2: Performance of adversarial training (left), curriculum adversarial training (with 7 PGDiterations) (middle), and randomized smoothing (right) against the eyeglass frame attack.
Figure 3: Performance of adversarial training (left), curriculum adversarial training (with 7 PGDiterations) (middle), and randomized smoothing (right) against the stop sign attack.
Figure 4: Performance of DOA (using the 100 × 50 rectangle) against the eyeglass frame attack incomparison with conventional methods. Left: comparison between DOA, adversarial training, andrandomized smoothing (using the most robust variants of these). Middle/Right: Comparing DOAperformance for different rectangle dimensions and numbers of PGD iterations inside the rectangle.
Figure 5: Performance of the best case of adversarial training, randomized smoothing, and DOAagainst the stop sign attack (left) and adversarial patch attack (right).
Figure 6: Examples of the ROA attack on face recognition, using a rectangle of size 100 × 50. (a)Left: the original A. J. Buckley’s image. Middle: modified input image (ROA superimposed on theface). Right: an image of the predicted individual who is Aaron Tveit with the adversarial input inthe middle image. (b) Left: the original Abigail Spencer’s image. Middle: modified input image(ROA superimposed on the face). Right: an image of the predicted individual who is Aaron Yoowith the adversarial input in the middle image.
Figure 7: Examples of the ROA attack on traffic sign, using a rectangle of size 7 × 7. (a) Left: theoriginal Speedlimit45 sign. Middle: modified input image (ROA superimposed on the sign). Right:an image of the predicted which is Speedlimit30 with the adversarial input in the middle image. (b)Left: the original Stop sign. Middle: modified input image (ROA superimposed on the sign). Right:an image of the predicted Yield sign with the adversarial input in the middle image.
Figure 8: Examples of different search techniques. From left to right: 1) the original input image, 2)the plot of input gradient, 3) face with ROA location identified using gradient-based search, 4) facewith ROA location identified using exhaustive search. Each row is a different example.
Figure 9: Effectiveness of DOA using the gradient-based method and the 100 × 50 region againstthe eyeglass frame attack, varying the number of PGD iterations for adversarial perturbations insidethe rectangle. Left: using exhaustive search. Right: using gradient-based search.
Figure 10: Effectiveness of DOA using the gradient-based method and the 70 × 70 region againstthe eyeglass frame attack, varying the number of PGD iterations for adversarial perturbations insidethe rectangle. Left: using exhaustive search. Right: using gradient-based search.
Figure 11: Examples of the eyeglass attack on face recognition. From left to right: 1) the originalinput image, 2) image with adversarial eyeglass frames, 3) face predicted by a model generatedthrough adversarial training, 4) face predicted by a model generated through randomized smoothing,5) face predicted (correctly) by a model generated through DOA. Each row is a separate example.
Figure 12: Examples of the stop sign attack. From left to right: 1) the original input image, 2)image with adversarial eyeglass frames, 3) face predicted by a model generated through adversarialtraining, 4) face predicted by a model generated through randomized smoothing, 5) face predicted(correctly) by a model generated through DOA. Each row is a separate example.
Figure 13: L0 attacks on face recognition and traffic sign classification.
Figure 14: Additional mask-based attacks on face recognition.
Figure 15:Additional mask-based attacks on traffic sign classification.
