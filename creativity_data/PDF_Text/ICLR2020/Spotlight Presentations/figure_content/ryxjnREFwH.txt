Figure 1: Comparison of NeRd with previous approaches for reading comprehension requiring com-plex reasoning. The components in grey boxes are the neural architectures. Previous works mainlytake two approaches: (1) augmenting pre-trained language model such as BERT with specializedmodules for each type of questions, which is hard to scale to multiple domains or multi-step complexreasoning; (2) applying neural semantic parser to the structured parses of the passage, which suffersseverely from the cascade error. In contrast, the neural architecture of NeRd is domain-agnostic,which includes a reader, e.g., BERT, and a programmer, e.g., LSTM, to generate compositionalprograms that are directly executed over the passages.
