Table 1: Task-averaged test accuracy (± SEM, n = 20) on the permuted (‘P10’) and split (‘S’)MNIST experiments. In the table, EWC refers to online EWC and DGR refers to DGR+distill (resultsreproduced from van de Ven & Tolias, 2019). We tested three hypernetwork-based models: forHNET+ENT (HNET alone for CL1), we inferred task identity based on the entropy of the predictivedistribution; for HNET+TIR, we trained a hypernetwork-protected recognition-replay network (basedon a VAE, cf. Fig. A1) to infer the task from input patterns; for HNET+R the main classifier wastrained by mixing current task data with synthetic data generated from a hypernetwork-protectedVAE.
Table 2: Task-averaged test accuracy (± SEM, n = 20) on the permuted (‘P10’) and split (‘S’)MNIST experiments. For HNET+R and DGR+distill (van de Ven & Tolias, 2019) the classificationnetwork is trained sequentially on data from the current task and replayed data from all previoustasks. Our HNET+R comes close to saturating the corresponding replay upper bound RPL-UB.
Table 3: Task-averaged test accuracy (± SEM, n = 20) on the permutedMNIST-10 (‘P10’) andsplitMNIST (‘S’) experiments during and after training.
Table 4: Task-averaged test accuracy (± SEM, n = 5) on the permutedMNIST-100 (‘P100’)experiments during and after training.
Table 5: Task-averaged test accuracy (± SEM, n = 5) on split CIFAR-10/100 on CL1 on twodifferent target network architectures.
Table 6: Comparison of HNET and HAT, Serra et al. (2018). Task-averaged test accuracy on thePermutedMNIST experiment with T = 10 and T = 100 tasks (’P10’, ’P100’) with three differenttarget network sizes, i.e., three fully connected neural networks with hidden layer sizes of (100, 100)or (500, 500) or (2000, 2000) are shown. For these architectures, a single accuracy was reported bySerra et al. (2018) without statistics provided. We reran HAT for PermutedMNIST-100 with codeprovided at https://github.com/joansj/hat, and for PermutedMNIST-10 with hiddenlayer size (1000, 1000) to match our setup. HAT and HNET perform similarly on large targetnetworks for PermutedMNIST-10, while HNET is able to achieve larger performances with smallertarget networks as well as for long task sequences.
