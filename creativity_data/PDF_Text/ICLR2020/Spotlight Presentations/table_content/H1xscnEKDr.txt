Table 1: Curriculum Adversarial Training against 7 Iterations L∞ Attacks on Face RecognitionAttack Strength	=0	=2	E = 4	E=8	E=16Clean Model	98.94%	57.87%	13.62%	0%	0%CAdv. Training: = 4	97.45%	94.68%	87.02%	65.11%	17.23%CAdv. Training: = 8	96.17%	93.40%	89.36%	75.53%	31.49%CAdv. Training: = 16	90.64%	88.09%	84.04%	75.96%	45.74%CAdv. Training: = 32	80.85%	76.60%	74.04%	65.10%	47.87%Table 2: Curriculum Adversarial Training against 20 Iterations L∞ Attacks on Face RecognitionAttack Strength	E=0	E=2	E = 4	E=8	E=16Clean Model	98.94%	44.04%	1.70%	0%	0%CAdv. Training: E = 4	97.45%	94.68%	85.74%	46.60%	5.11%CAdv. Training: E = 8	96.17%	93.19%	88.94%	69.36%	9.57%CAdv. Training: E = 16	90.64%	88.08%	83.83%	73.62%	35.96%CAdv. Training: E = 32	80.85%	76.17%	74.04%	63.82%	43.62%We begin with our results on the face recognition dataset. Tables 1 and 2 present results for (curricu-lum) adversarial training for varying of the l∞ attacks, separately for training and evaluation. Aswe can see, curriculum adversarial training with = 16 is generally the most robust, and remainsreasonably effective for relatively large perturbations. However, we do observe a clear tradeoff
Table 2: Curriculum Adversarial Training against 20 Iterations L∞ Attacks on Face RecognitionAttack Strength	E=0	E=2	E = 4	E=8	E=16Clean Model	98.94%	44.04%	1.70%	0%	0%CAdv. Training: E = 4	97.45%	94.68%	85.74%	46.60%	5.11%CAdv. Training: E = 8	96.17%	93.19%	88.94%	69.36%	9.57%CAdv. Training: E = 16	90.64%	88.08%	83.83%	73.62%	35.96%CAdv. Training: E = 32	80.85%	76.17%	74.04%	63.82%	43.62%We begin with our results on the face recognition dataset. Tables 1 and 2 present results for (curricu-lum) adversarial training for varying of the l∞ attacks, separately for training and evaluation. Aswe can see, curriculum adversarial training with = 16 is generally the most robust, and remainsreasonably effective for relatively large perturbations. However, we do observe a clear tradeoffbetween accuracy on non-adversarial data and robustness, as one would expect.
Table 3: Randomized Smoothing against 20 Iterations L? Attacks on Face RecognitionAttack Strength	=0	= 0.5	=1	E = 1.5	E=2	E = 2.5	E=3Clean Model	98.94%	93.19%	70.85%	44.68%	22.13%	8.29%	3.19%RS: σ = 0.25	98.51%	97.23%	95.53%	91.70%	81.06%	67.87%	52.97%RS: σ = 0.5	97.65%	94.25%	93.61%	91.70%	87.87%	82.55%	71.70%RS: σ = 1	92.97%	93.19%	91.70%	91.06%	88.51%	85.53%	82.98%Table 3 presents the results of using randomized smoothing on face recognition data, when facingthe l2 attacks. Again, we observe a high level of robustness and, in most cases, relatively limiteddrop in performance, with σ = 0.5 perhaps striking the best balance.
Table 4: Curriculum Adversarial Training against 7 Iterations L∞ Attacks on Traffic SignsAttack Strength	E=0	E=2	E=4	E=8	E=16Clean Model	98.69%	90.24%	65.24%	33.80%	10.10%CAdv. Training: E = 4	99.13%	97.13%	93.47%	63.85%	20.73%CAdv. Training: E = 8	98.72%	96.86%	93.90%	81.70%	38.24%CAdv. Training: E = 16	96.95%	95.03%	92.77%	87.63%	64.02%CAdv. Training: E = 32	65.63%	53.83%	50.87%	46.69%	38.07%Table 5: Curriculum Adversarial Training against 20 Iterations L∞ Attacks on Traffic SignsAttack Strength	E=0	E=2	E=4	E=8	E=16Clean Model	98.69%	89.54%	61.58%	24.65%	5.14%CAdv. Training: E = 4	99.13%	96.95%	91.90%	56.53%	12.02%CAdv. Training: E = 8	98.72%	96.68%	93.64%	76.22%	28.13%CAdv. Training: E = 16	96.95%	95.03%	92.51%	86.76%	54.01%CAdv. Training: E = 32	65.63%	53.83%	50.78%	46.08%	36.49%Tables 4 and 5 present evaluation on traffic sign data for curriculum adversarial training against thel∞ attack for varying . As with face recognition data, we can observe that the approaches tendto be relatively robust, and effective on non-adversarial data for adversarial training methods using<32.
Table 5: Curriculum Adversarial Training against 20 Iterations L∞ Attacks on Traffic SignsAttack Strength	E=0	E=2	E=4	E=8	E=16Clean Model	98.69%	89.54%	61.58%	24.65%	5.14%CAdv. Training: E = 4	99.13%	96.95%	91.90%	56.53%	12.02%CAdv. Training: E = 8	98.72%	96.68%	93.64%	76.22%	28.13%CAdv. Training: E = 16	96.95%	95.03%	92.51%	86.76%	54.01%CAdv. Training: E = 32	65.63%	53.83%	50.78%	46.08%	36.49%Tables 4 and 5 present evaluation on traffic sign data for curriculum adversarial training against thel∞ attack for varying . As with face recognition data, we can observe that the approaches tendto be relatively robust, and effective on non-adversarial data for adversarial training methods using<32.
Table 6: Randomized Smoothing against 20 Iterations L? Attacks on Traffic SignsAttack Strength	E=0	E = 0.5	E=1	E = 1.5	E=2	E = 2.5	E=3Clean Model	98.69%	61.67%	25.78%	11.50%	7.84%	4.97%	3.57%RS: σ = 0.25	98.22%	89.08%	55.69%	34.06%	23.46%	18.61%	14.75%RS: σ = 0.5	96.28%	90.80%	76.13%	52.64%	35.31%	23.43%	16.52%RS: σ = 1	88.21%	83.68%	75.49%	64.90%	50.03%	36.53%	26.22%The results of randomized smoothing on traffic sign data are given in Table 6. Since images aresmaller here than in VGGFace, lower values of for the l2 attacks are meaningful, and for ≤ 1 we13Published as a conference paper at ICLR 2020generally see robust performance on randomized smoothing, with σ = 0.5 providing a good balancebetween non-adversarial accuracy and robustness, just as before.
Table 7: Comparison of effectiveness of different approaches against the stop sign attack. Bestparameter choices were made for each.
Table 8: Effectiveness of 10 × 5 DOA using exhaustive search with different numbers of PGDiterations against the stop sign attack.
Table 9: Effectiveness of 10 × 5 gradient-based DOA with different numbers of PGD iterationsagainst the stop sign attack.
Table 10: Effectiveness of 7 × 7 DOA using exhaustive search with different numbers of PGDiterations against the stop sign attack.
Table 11: Effectiveness of 7 × 7 gradient-based DOA with different numbers of PGD iterationsagainst the stop sign attack.
Table 12: Comparison of effectiveness of different approaches against the adversarial patch attackon the face recognition data. Best parameter choices were made for each.
Table 13: Effectiveness of adversarial training against the adversarial patch attack on face recogni-tion data._______________________________________________________________________________________Size of Attacking Region	0%	5%	10%	15%	20%	25%Clean Model	99.30%	80.69%	41.21%	19.41%	9.76%	5.75%7 Iterations Adv. Training: = 4	92.28%	57.05%	39.15%	34.71%	10.52%	7.81%7 Iterations Adv. Training: = 8	66.54%	27.98%	23.54%	21.26%	18.55%	17.03%50 Iterations Adv. Training: = 4	90.29%	57.38%	53.58%	36.01%	28.85%	16.05%50 Iterations Adv. Training: = 8	53.68%	21.80%	21.15%	17.79%	17.79%	16.70%Table 14: Effectiveness of curriculum adversarial training against the adversarial patch attack onface recognition data.
Table 14: Effectiveness of curriculum adversarial training against the adversarial patch attack onface recognition data.
Table 15: Effectiveness of randomized smoothing against the adversarial patch attack on face recog-nition data.____________________________________________________________________________________Size of Attacking Region	0%	5%	10%	15%	20%	25%Clean Model	99.30%	80.69%	41.21%	19.41%	9.76%	5.75%Randomized Smoothing: σ = 0.25	98.95%	60.35%	26.67%	13.80%	6.67%	5.15%Randomized Smoothing: σ = 0.5	97.66%	81.05%	58.83%	33.92%	26.55%	14.74%Randomized Smoothing: σ = 1	98.83%	61.99%	36.26%	33.80%	24.68%	16.02%Table 16: Effectiveness of DOA (50 PGD iterations) against the adversarial patch attack on facerecognition data.
Table 16: Effectiveness of DOA (50 PGD iterations) against the adversarial patch attack on facerecognition data.
Table 17: Comparison of effectiveness of different approaches against the adversarial patch attackon the traffic sign data. Best parameter choices were made for each.
Table 18: Effectiveness of adversarial training against the adversarial patch attack on traffic signdata. ____________________________________________________________________________________Size of Attacking Region	0%	5%	10%	15%	20%	25%Clean Model	98.38%	75.87%	58.62%	40.68%	33.10%	22.47%Adv. Training: = 4	98.96%	82.14%	64.46%	45.73%	31.36%	23.78%Adv. Training: = 8	95.62%	76.74%	62.89%	46.60%	37.37%	25.09%20Published as a conference paper at ICLR 2020Table 19: Effectiveness of curriculum adversarial training against the adversarial patch attack ontraffic sign data.
Table 19: Effectiveness of curriculum adversarial training against the adversarial patch attack ontraffic sign data.
Table 20: Effectiveness of randomized smoothing against the adversarial patch attack on traffic signdata.___________________________________________________________________________________________Size of Attacking Region	0%	5%	10%	15%	20%	25%Clean Model	98.38%	75.87%	58.62%	40.68%	33.10%	22.47%Randomized Smoothing: σ = 0.25	98.27%	82.24%	66.78%	54.67%	40.37%	33.79%Randomized Smoothing: σ = 0.5	95.39%	83.28%	68.17%	53.86%	48.79%	33.10%Randomized Smoothing: σ = 1	85.47%	74.39%	54.44%	45.44%	40.48%	29.06%Table 21: Effectiveness of DOA (30 PGD iterations) against the adversarial patch attack on trafficsign data.
Table 21: Effectiveness of DOA (30 PGD iterations) against the adversarial patch attack on trafficsign data.
Table 22: Effectiveness of DOA (50 PGD iterations) against the adversarial patch attack on trafficsign data.
Table 23: Effectiveness of DOA (50 PGD iterations) against 20 Iterations L∞ Attacks on FaceRecognitionAttack Strength	=0	=2	E = 4	E=8	E=16Clean Model	98.94%	44.04%	1.70%	0%	0%(100 × 50)Exhaustive Search	98.72%	39.79%	1.06%	0%	0%(100 × 50)Gradient Based Search	98.51%	40.64%	3.40%	0%	0%(70 × 70)Exhaustive Search	98.51%	35.74%	0.43%	0%	0%(70 × 70)Gradient Based Search	97.45%	33.40%	0.43%	0%	0%Table 24: Effectiveness of DOA (50 PGD iterations) against 20 Iterations L∞ Attacks on TrafficSign ClassificationAttack Strength	E=0	E=2	E = 4	E=8	E=16Clean Model	98.69%	89.54%	61.58%	24.65%	5.14%(10 × 5)Exhaustive Search	95.87%	91.55%	76.57%	39.02%	7.75%(10 × 5)Gradient Based Search	96.46%	91.81%	78.83%	46.86%	7.84%(7 × 7)Exhaustive Search	92.94%	89.54%	77.09%	42.77%	5.83%(7 × 7)Gradient Based Search	95.59%	91.20%	79.52%	46.86%	6.70%Table 23 presents results of several variants of DOA in the context of PGD attacks in the context offace recognition, while Table 24 considers these in traffic sign classification. The results are quite
Table 24: Effectiveness of DOA (50 PGD iterations) against 20 Iterations L∞ Attacks on TrafficSign ClassificationAttack Strength	E=0	E=2	E = 4	E=8	E=16Clean Model	98.69%	89.54%	61.58%	24.65%	5.14%(10 × 5)Exhaustive Search	95.87%	91.55%	76.57%	39.02%	7.75%(10 × 5)Gradient Based Search	96.46%	91.81%	78.83%	46.86%	7.84%(7 × 7)Exhaustive Search	92.94%	89.54%	77.09%	42.77%	5.83%(7 × 7)Gradient Based Search	95.59%	91.20%	79.52%	46.86%	6.70%Table 23 presents results of several variants of DOA in the context of PGD attacks in the context offace recognition, while Table 24 considers these in traffic sign classification. The results are quiteconsistent with intuition: DOA is largely unhelpful against these attacks. The reason is that DOAfundamentally assumes that the attacker only modifies a relatively small proportion (〜5%) of thescene (and the resulting image), as otherwise the physical attack would be highly suspicious. l∞bounded attacks, on the other hand, modify all pixels.
