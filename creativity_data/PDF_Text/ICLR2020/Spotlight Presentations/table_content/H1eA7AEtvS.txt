Table 1: The configurations of the main BERT and ALBERT models analyzed in this paper.
Table 2: Dev set results for models pretrained over BookCorpus and Wikipedia for 125k steps.
Table 3: The effect of vocabulary embedding size on the performance of ALBERT-base.
Table 4: The effect of cross-layer parameter-sharing strategies, ALBERT-base configuration.
Table 5: The effect of sentence-prediction loss, NSP vs. SOP, on intrinsic and downstream tasks.
Table 6:	The effect of controlling for training time, BERT-large vs ALBERT-xxlarge configurations.
Table 7:	The effect of additional training data using the ALBERT-base configuration.
Table 8:	The effect of removing dropout, measured for an ALBERT-xxlarge configuration.
Table 9: State-of-the-art results on the GLUE benchmark. For single-task single-model results, wereport ALBERT at 1M steps (comparable to RoBERTa) and at 1.5M steps. The ALBERT ensembleuses models trained with 1M, 1.5M, and other numbers of steps.
Table 10: State-of-the-art results on the SQuAD and RACE benchmarks.
Table 11: The effect of increasing the number of layers for an ALBERT-large configuration.
Table 12: The effect of increasing the hidden-layer size for an ALBERT-large 3-layer configuration.
Table 13: The effect of a deeper network using an ALBERT-xxlarge configuration.						The answer is given by the results from Table 13. The difference between 12-layer and 24-layerALBERT-xxlarge configurations in terms of downstream accuracy is negligible, with the Avg scorebeing the same. We conclude that, when sharing all cross-layer parameters (ALBERT-style), thereis no need for models deeper than a 12-layer configuration.
Table 14: Hyperparameters for ALBERT in downstream tasks. LR: Learning Rate. BSZ: BatchSize. DR: Dropout Rate. TS: Training Steps. WS: Warmup Steps. MSL: Maximum SequenceLength.
