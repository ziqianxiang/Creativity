Table 1: Overview of our domain-specific language. See Table 2 for the sample usage.
Table 2: Examples of correct predictions on DROP development set.
Table 3: An example in MathQA dataset.
Table 4: Results on DROP dataset. On the development set, we present the mean and standarderror of 10 NeRd models, and the test result of a single model. For all models, the performancebreakdown of different question types is on the development set. Note that the training data ofBERT-Calc model (Andor et al., 2019) for test set evaluation is augmented with CoQA (Reddyet al., 2019).
Table 5: Results of counting and sorting questions on DROP development set, where we comparevariants of NeRd with and without the corresponding operations. (a): counting; (b): sorting. Foreach setting, we present the best results on development set.
Table 6: Examples of counting and sorting questions on DROP development set, where NeRd withthe corresponding operations gives the correct predictions, while the variants without them do not.
Table 7: Results of different training algorithmson DROP development set. For each setting, wepresent the best results on the development set.
Table 8: Results on MathQA test set, with NeRdand two variants: (1) no pre-training; (2) using20% of the program annotations in training.
Table 9: Some samples in DROP training set with the wrong annotations, which are discarded byNeRd because none of the annotated programs passes the threshold of our training algorithm.
Table 10: Examples of wrong predictions on DROP dev set.
