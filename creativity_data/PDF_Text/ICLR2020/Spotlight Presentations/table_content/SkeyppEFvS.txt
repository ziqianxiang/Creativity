Table 1: Comparison with human per-formance in the BlockTowerCF scenarioobtained with AMT studies. We report 2Dpixel error for each block, as well as globalmean and variance σ (reference resolution448 × 448) on the test set with K=3 blocks.
Table 2: BlocktowerCF: MSE on 3D pose av-erage over time. IN sup. methods in the last col-umn exploit the ground truth confounder quantitiesas input and thus represent a soft upper bound (arenot comparable). * Test ConfoUnder configurationsnot seen during training (50/50 split).
Table 3: BallsCF: MSE on 2D pose averageover time. IN sup. methods in the last columnexploit the ground truth confounder quantities asinput and thus is not directly comparable.
Table 4: CollisionCF: MSE on 3D pose av-erage over time. IN sup. methods in the last col-umn exploit the ground truth confounder quanti-ties as input and thus is not directly comparable(still showing inferior performance).
Table 5: Ablations on BlockTowerCF: confounder prediction(masses, friction coefficients) from the joint latent representation U.
Table 6: Ablation study on BlockTowerCF: (Left) Impact of each component of our model (MSEon 3D pose average over time). (Middle) Impact of the confounder estimate (MSE on 3D pose averageover time, validation set). Feedforward methods do not estimate the confounder, counterfactualmethods do. We compare against soft upper bounds, which use the ground truth confounder as inputor supervise its estimation. (Right) Stability prediction (accuracy per block). With the ground truthconfounder values as input, graph convolutional networks (GCN(C)) reach a performance of 85.4and 77.3 in the 3 → 3 and 3 → 4 settings respectively (a soft upper bound, not comparable).
Table 7: BlocktowerCF: MSE on 3D pose averaged over time. We evaluate different types oftraining: from the ground-truth positions (GT) or from the estimated positions (w/o GT). ^Testconfounder configurations not seen during training (50/50 split).
