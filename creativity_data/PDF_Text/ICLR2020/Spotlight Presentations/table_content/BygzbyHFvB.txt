Table 1: Results (median and variance) on the dev sets of GLUE based on the RoBERTa-large model, from 5runs with the same hyperparameter but different random seeds. ReImp is our reimplementation of RoBERTa-large. The training process can be very unstable even with the vanilla version. Here, both PGD on STS-B andFreeAT on RTE demonstrates such instability, with one unconverged instance out of five.
Table 2: Results on GLUE from the evaluation server, as of Sep 25, 2019. Metrics are the same as theleaderboard. Number under each task’s name is the size of the training set. FreeLB-BERT is the single-modelresults of BERT-base finetuned with FreeLB, and FreeLB-RoB is the ensemble of 7 RoBERTa-Large modelsfor each task. References: 1: (Devlin et al., 2019); 2: (Liu et al., 2019a); 3: (Yang et al., 2019); 4: (Liu et al.,2019b).
Table 3: Results on ARC and CommonsenseQA (CQA). ARC-Merge is the combination of ARC-Easy andARC-Challenge, “MTL” stands for multi-task learning and “Ens” stands for ensemble. Results of XLNet +RoBERTa (MTL+Ens) and AristoRoBERTaV7 (MTL) are from the ARC leaderboards. Test (E) denotes thetest set results with ensembles. For CQA, we report the highest dev and test accuracies among all models. Themodels with 78.81/72.19 dev/test accuracy (as in the table) have 71.84/78.64 test/dev accuracies respectively.
Table 4: The median and standard deviation of the scores on the dev sets of RTE, CoLA and MRPC from theGLUE benchmark, computed from 5 runs with the same hyper-parameters except for the random seeds. Weuse FreeLB-m to denote FreeLB With m ascent steps, and FreeLB-3* to denote the version without reusing thedropout mask.
Table 5: Median of the maximum increase in loss in the vicinity of the dev set samples for RoBERTa-Largemodel finetuned with different methods. Vanilla models are naturally trained RoBERTa’s. M-Inc: Max Inc, M-Inc (R): Max Inc (R). Nat Loss (N-Loss) is the loss value on clean samples. Notice we require all clean sampleshere to be correctly classified by all models, which results in 227, 850 and 355 samples for RTE, CoLA andMRPC, respectively. We also give the variance in the Appendix.
Table 6: Additional hyper-parameters on GLUE tasks.
Table 7: Median and Standard Deviation of the maximum increase in loss in the vicinity of the dev set samplesfor RoBERTa-Large model finetuned with different methods. Vanilla models are naturally trained RoBERTa’s.
Table 8: The median and standard deviation of the scores on the dev sets of STS-B, SST-2, QNLI, QQP andMNLI from the GLUE benchmark, each computed from 5 runs with the same hyper-parameters except for therandom seeds (except for the results with YOPO on QQP, which are from 4 runs). Also note here we use a stepsize of α for the adversary of YOPO-m-n, so YOPO effectively uses a step size of nα. We use FreeLB-m todenote FreeLB with m ascent steps, and YOPO-3-n to denote YOPO with n shallow-layer ascents.
