Table 1: The success rates (%±std over 5 random runs) of black-box attacks (untargeted) craftedby PGD and its “skip gradient” (SGM) version, on different source models against a Inception V3target model. The best results are in bold.
Table 2: One-step transferability: the success rates (%±std over 5 random runs) of black-box attackscrafted by different methods on 2 source models against 7 unsecured target models. The best resultsare in bold.
Table 3: Multi-step transferability: the success rates (%±std over 5 random runs) of black-boxattacks crafted by different methods on 2 source models against 7 unsecured target models. The bestresults are in bold.
Table 4: Combined with existing methods: the success rates (%) of attacks crafted on source modelDN201 against 7 unsecured target models. The best results are in bold and + indicates improvement.
Table 5: Transferability against secured models: the success rates (%±std over 5 random runs) ofmulti-step attacks crafted on RN152 and DN201 source models against 3 secured models. The bestresults are in bold.
Table 6: Multi-step transferability of ensemble-based attack: the success rates (%±std over 5 ran-dom runs) of multi-step attacks crafted by different methods on an ensemble of 3 source models(e.g. RN34, RN152 and DN201) against 7 unsecured target models. The best results are in bold.
Table 7: Transferability of ensemble-based attack against secured models: the success rates (%±stdover 5 random runs) of black-box attacks crafted on an ensemble of 3 source models (e.g. RN34,RN152 and DN201). The best results are in bold.
Table 8: Previously reported attack success rates (%) of baseline single-source attacks against 6target models. “-” means no results were reported.
Table 9: Previously reported attack success rates (%) of ensemble-based baseline attacks against 6target models. “-” means no results were reported.
Table 10: Source models used by existing single-source and ensemble-based black-box attacks.
Table 11: Difference in experimental settings of our work compared to previous works.“NeurIPS2017” indicates the dataset used for NeurIPS 2017 adversarial competition. : maximum per-pixelperturbation; N : number of attack steps; α: attack step size.
