Table 1: TIMIT phoneme error rate (PER%) for different methods. Our kaleidoscope, raw-inputversion of the model (row 3) performs competitively with the original model trained on MFSC features(row 1), with only an 0.4% drop in PER. It significantly outperforms existing approaches that learnfrom raw audio, i.e. without handcrafted featurization (e.g. SincNet [row 2], which to our knowledgeattains the previous state-of-the-art for learning from raw audio), and is only 0.8% less accurate thanthe overall state-of-the-art on TIMIT.3 Additional comparisons are given in Appendix B.1.
Table 2: Top-1 classification accuracy of ShuffleNet on ImageNet validation set (parameter counts inparentheses). We compare our approach (col. 3) with our reimplementation of ‘vanilla’ ShuffleNet(col. 1) and a recent approach based on the Hadamard transform (col. 2).4 We report results fordifferent network width multipliers (# channels). The last column shows the differences in accuracyand parameter count between our approach and vanilla ShuffleNet; using a learnable K-matrix inplace of each fixed permutation (shuffle) or Hadamard matrix improves accuracy by up to 5%.
Table 3: Permuted CIFAR-10 validation set classification accuracy (%). Our kaleidoscope layer isable to nearly perfectly recover the latent structure, allowing a downstream CNN to approach theaccuracy of a standard ResNet18 on the unpermuted dataset (last column).
Table 4: Inference speed on the IWSLT-14 German-English translation task (test set). Using K-matrices instead of dense matrices in the DynamicConv decoder linear layers results in 36% fasterinference speed (measured on a single-threaded CPU with a batch size of 1 and beam size of 1).
Table 5: TIMIT phoneme error rate (PER%, ± standard deviation across 5 random seeds).
Table 6: Top-5 classification accuracy of ShuffleNet on ImageNet validation set. We report results fordifferent network width multipliers (number of channels), and for different kinds of matrices used forchannel mixing. Using a learnable K-matrix in place of each fixed permutation (shuffle) or Hadamardmatrix improves top-5 accuracy by up to 4.8%. Parameter counts are the same as in Table 2.
Table 7: Expressiveness of different classes of structured matrices: Frobenius error of represent-ing common structured matrices (columns) of dimension 256 using three structured representationsof matrices with adjustable numbers of parameters. (Left group: Target matrices in the same classas the methods. Middle group: Target matrices with fixed number of parameters. Right: Randommatrix to show typical scale of error.) Each method is allotted the same number of parameters, equalto a log n factor more than that of the target matrix. Low-rank and sparse matrices are unable tocapture any structure outside their own class, while the minima for kaleidoscope matrices found viaoptimization better capture the actual structure for out-of-class targets better than the baselines.
