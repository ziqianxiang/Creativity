Table 1: Average pixel MSE over a 30 step unroll on the train and test data on four physical systems. Allvalues are multiplied by 1e+4. We evaluate two versions of the Hamiltonian Neural Network (HNN)(Greydanus et al., 2019): the original architecture and a convolutional version closely matched to thearchitecture of HGN. We also compare four versions of our proposed Hamiltonian Generative Network(HGN): the full version, a version trained and tested with an Euler rather than a leapfrog integrator, adeterministic rather than a generative version, and a version of HGN with no extra network between theposterior and the initial state.
Table 2: Variance of the Hamiltonian on four physical systems over single train and test rollouts shownin Fig. 6. The numbers reported are scaled by a factor of 1e+4. High variance indicates that the energyis not conserved by the learned Hamiltonian throughout the rollout. Many HNN Hamiltonians havecollapsed to 0, as indicated by N/A. HGN Hamiltonians are meaningful, and different versions of HGNconserve energy to varying degrees.
