Table 1: Sentiment prediction experiments over 10 restarts	Acc.,%	Race gap	Gend. gap ∣ Cuis. gap	SenSR	94±1	0.30±.05	0.19±.03	0.23±.05SenSR-E	93±1	0.11±.04	0.04±.03	1.11±.15Baseline	95±1	7.01±.44	5.59±.37	4.10±.44Project	94±1	1.00±.56	1.99±.58	1.70±.41Sinha+	94±1	3.88±.26	1.42±.29	1.33±.18Bolukb.+	94±1	6.85±.53	4.33±.46	3.44±.29Figure 2: Box-plots of sentiment scores4.1	Fair sentiment prediction with word embeddingsProblem formulation We study the problem of classifying the sentiment of words using positive(e.g. ‘smart’) and negative (e.g. ‘anxiety’) words compiled by Hu & Liu (2004). We embed wordsusing 300-dimensional GloVe (Pennington et al., 2014) and train a one layer neural network with1000 hidden units. Such classifier achieves 95% test accuracy, however it entails major individualfairness violation. Consider an application of this sentiment classifier to summarizing customer re-views, tweets or news articles. Human names are typical in such texts and should not affect thesentiment score, hence we consider fair metric between any pair of names to be 0. Then sentimentscore for all names should be the same to satisfy the individual fairness. To make a connection togroup fairness, following the study of Caliskan et al. (2017) that reveals the biases in word embed-dings, we evaluate the fairness of our sentiment classifier using male and female names typical for
Table 2: Summary of Adult classification experiments over 10 restarts	B-Acc,%	S-Con.	GR-Con.	GapRGMS	GapRRMS	GapGmax	GapRmaxSenSR	78.9	.934	.984	.068	.055	.087	.067Baseline	82.9	.848	.865	.179	.089	.216	.105Project	82.7	.868	1.00	.145	.064	.192	.086Adv. Debias.	81.5	.807	.841	.082	.070	.110	.078CoCL	79.0	-	-	.163	.080	.201	.109out the sensitive subspace that we used for training SenSR (this is analogous to Prost et al. (2019));training a distributionally robust classifier with Euclidean distance cost (Sinha et al., 2017). Allapproaches improved upon the baseline, however only SenSR can be considered individually fair.
Table 3: SenSR hyperparameter choices in the experiments	E	B	s	se		f	feSentiment	4K	1K	0.1	10	0.1	0.01	10Adult	12K	1K	10	50	10-3	10-4	40Table 4: Summary of Adult classification experiments over 10 restarts	Accuracy	B-TPR	GaPGMS	GaPRRMS	GaPGmax	GaPRmaxSenSR	.787±.003	.789±.003	.068±.004	.055±.003	.087±.005	.067±.004Baseline	.813±.001	.829±.001	.179±.004	.089±.003	.216±.003	.105±.003Project	.813±.001	.827±.001	.145±.004	.064±.003	.192±.004	.086±.004Adv. Debias.	.812±.001	.815±.002	.082±.005	.070±.006	.110±.006	.078±.005CoCL	-	.790	.163	.080	.201	.109D Additional Adult experiment detailsD.1 PreprocessingThe continuous features in Adult are the following: age, fnlwgt, capital-gain,capital-loss, hours-per-week, and education-num. The categorical features are thefollowing: workclass, education, marital-stataus, occupation, relationship,race, sex, native-country. See Dua & Graff (2017) for a description of each feature. Weremove fnlwgt and education but keep education-num, which is a integer representationof education. We do not use native-country, but use race and sex as predictive features.
Table 4: Summary of Adult classification experiments over 10 restarts	Accuracy	B-TPR	GaPGMS	GaPRRMS	GaPGmax	GaPRmaxSenSR	.787±.003	.789±.003	.068±.004	.055±.003	.087±.005	.067±.004Baseline	.813±.001	.829±.001	.179±.004	.089±.003	.216±.003	.105±.003Project	.813±.001	.827±.001	.145±.004	.064±.003	.192±.004	.086±.004Adv. Debias.	.812±.001	.815±.002	.082±.005	.070±.006	.110±.006	.078±.005CoCL	-	.790	.163	.080	.201	.109D Additional Adult experiment detailsD.1 PreprocessingThe continuous features in Adult are the following: age, fnlwgt, capital-gain,capital-loss, hours-per-week, and education-num. The categorical features are thefollowing: workclass, education, marital-stataus, occupation, relationship,race, sex, native-country. See Dua & Graff (2017) for a description of each feature. Weremove fnlwgt and education but keep education-num, which is a integer representationof education. We do not use native-country, but use race and sex as predictive features.
Table 5: Summary of individual fairness metrics in Adult classification experiments over 10 restarts	Spouse Consistency	Gender and Race ConsistencySenSR	.934±.012	.984±.000Baseline	.848±.008	.865±.004Project	.868±.005	1±0Adv. Debias.	.807±.002	.841±.01217Published as a conference paper at ICLR 2020D.4 Hyperparameters and trainingFor each model, we use the same 10 train/test splits where use 80% of the data for training. Becauseof the class imbalance, each minibatch is sampled so that there are an equal number of trainingpoints from both the “income at least $50k class” and the “income below $50k class.”D.4.1 Baseline, Project, and SenSRSee Table 3 for the hyperparameters we used when training Baseline, Project, and SenSR (Baselineand Project use a subset). Hyperparameters are defined in Appendix C.
