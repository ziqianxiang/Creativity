Table 1: Jacobian singular values and resulting sparse networks for the 7-layer tanh MLP networkconsidered in section 3.1. SG, CN, and Sparsity refer to Scaled Gaussian, Condition Number (i.e.,smax/smin, where smax and smin are the maximum and minimum Jacobian singular values), and a ratioof pruned prameters to the total number of parameters, respectively. SG (γ=10-2) is equivalent to thevariance scaling initialization as in LeCUn et al. (1998); Glorot & Bengio (2010). The failure Casescorrespond to unreliable connection sensitivity resulted from poorly conditioned initial Jacobians.
Table 2: Pruning results for various neural networks on different datasets. All networks are pruned atinitialization for the sparsity K = 90% based on connection sensitivity scores as in Lee et al. (2019).
Table 3: Pruning results for VGG16 and ResNet32with different activation functions on CIFAR-10. Wereport generalization errors (avg. over 5 runs), andthe first and Second best results are highlighted.
Table 4: Unsupervised pruning results forK-layer MLP networks on MNIST. Allnetworks are pruned for sparsity κK = 90%at orthogonal initialization. We report gen-eralization errors (avg. over 10 runs).
Table 5: Transfer of sparsity experiment results forLeNet. We prune for 用 = 97% at orthogonal initial-ization, and report gen. errors (average over 10 runs).
Table 6: All models (Equivalents 1,2,3) are initially bigger than the base network (ResNet20), byeither being wider or deeper, but pruned to have the same number of parameters as the base network(269k). The widening factor (k) refers to the filter multiplier; e.g., for the basic filter size of 16,the widening factor of k=2 will result in 32 filters. The block size refers to the number of residualblocks in each block layer; all models have three block layers. More/less number of residual blocksmeans the network is deeper/shallower. The reported generalization errors are averages over 5 runs.
