Table 1: Accuracy of knowledge selection withand without knowing the response. We test withGRU (Cho et al., 2014), Transformer (Vaswaniet al., 2017) and BERT (Devlin et al., 2019) asthe sentence encoder. For human evaluation, werandomly sample 20 dialogues and ask humanannotators to select the most likely knowledgesentence from the pool.
Table 2: Quantitative results on the Wizard of Wikipedia dataset (Dinan et al., 2019). The methodwith [*] does not use the knowledge loss. The scores of E2E Transformer MemNett and Transformer(no knowledge)t are from the original paper. The variant (BERT vocab∕ is re-runned using theauthors’ code, since the vocabulary is different from original paper due to the use of BERT.
Table 3: Quantitative results on the Holl-E dataset (Moghe et al., 2018) with single reference andmultiple references test set.
Table 4: Single-turn human evaluation results on the Wizard of Wikipedia. We report the meanratings and their standard errors of different methods for engagingness and knowledgeability scores.
Table 5: Multi-turn human evaluation results on the Wizard of Wikipedia. We report the averagesand standard deviations (in parentheses).
Table 6: Knowledge selection accuracy for each turn on the Wizard of Wikipedia (Dinan et al.,2019). The method with [*] uses no knowledge loss. TMN stands for E2E Transformer MemNet.
Table 7: Performance of our model with partial knowledge labels on Wizard of Wikipedia (Dinanet al., 2019).
