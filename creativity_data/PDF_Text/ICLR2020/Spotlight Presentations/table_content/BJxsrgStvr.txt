Table 1: The retraining accuracy of subnetworks drawn at different training epochs usingdifferent learning rate schedules, with a pruning ratio of 0.5. Here [0, 100] represents[0LR-0.01,1O0LR→o.00i] while [80,120] denotes [80lr→o.oi, 120LR→o.ooι],for compactness.
Table 2: Comparing the accuracy and energy/FLOPs of EB Train (including its variants), NS (Liuet al. (2017)), LT (Frankle & Carbin, 2019), SNIP (Lee et al., 2019), and ThiNet (Luo et al. (2017)).
Table 3: Comparing the accuracy and total training FLOPs of EB Train, Network Slimming (Liuet al., 2017), ThiNet (Luo et al., 2017), and SFP (He et al., 2018). The “Acc. Improv.” is theaccuracy of the pruned model minus that of the unpruned model, so a positive number means thepruned model has a higher accuracy.
Table 4: The retraining accuracy of subnetworks drawn at different training epochs using differentinital learning rate, where the pruning ratio is 0.5.
