Table 1: Hyperparameters for DRILHyperparameter	Values Considered	Final ValuePolicy Learning rate	2.5 ∙ 10-2,2.5 ∙ 10-3, 2.5 ∙ 10-4	2.5 ∙ 10-3Quantile cutoff	0.8, 0.9, 0.95, 0.98	0.98Number of supervised updates	1,5	1Number of policies in ensemble	5	5Gradient clipping	0.1	0.1Entropy coefficient	0.01	0.01Value loss coefficient	0.5	0.5Number of steps	128	128Parallel Environments	16	16We then chose the best values and kept those hyperparameters fixed for all other environments. Allother A2C hyperparameters follow the default values in the repo (Kostrikov, 2018): policy networksconsisted of 3-layer convolutional networks with 8-32-64 feature maps followed by a single-layerMLP with 512 hidden units.
Table 2: Hyperparameters for GAILHyperparameter	Values Considered	Final ValuePolicy Learning rate	2.5 ∙ 10-2,2.5 ∙ 10-3, 2.5 ∙ 10-4	2.5 ∙ 10-3Discriminator Learning rate	2.5 ∙ 10-2,2.5 ∙ 10-3, 2.5 ∙ 10-4	2.5 ∙ 10-3Number of discriminator updates	1,5,10	5Gradient clipping	0.1	0.1Entropy coefficient	0.01	0.01Value loss coefficient	0.5	0.5Number of steps	128	128Parallel Environments	16	16C.2 Continuous ControlAll behavior cloning and ensemble models were trained to minimize the mean-squared error regres-sion loss on the demonstration data for 500 epochs using Adam (Kingma & Ba, 2014) and a learningrate of 2.5 ∙ 10-4. Policy networks were 2-layer fully-connected MLPs with tanh activations and 64hidden units.
Table 3: Hyperparameters (our method)Hyperparameter	Values Considered	Final ValuePolicy Learning rate	2.5 ∙ 10-3, 2.5 ∙1 0-4,1 ∙ 10-4, 5 ∙ 10-5	2.5 ∙ 10-5Quantile cutoff	0.98	0.98Number of supervised updates	1	1Number of policies in ensemble	5	5Gradient clipping	0.1	0.1Entropy coefficient	0.01	0.01Value loss coefficient	0.5	0.5Number of steps	128	128Parallel Environments	16	1618Published as a conference paper at ICLR 2020D Ablation ExperimentsIn this section we provide ablation experiments examining the effects of the cost function clippingand the role of the BC loss. We also compare the ensemble approach to a dropout-based approxima-tion and show that DRIL works well in both cases.
Table 4: Ablation Experiments with 3 expert trajectoriesEnvironment	SpaceInvaders	Breakout	BeamRiderDRIL (ensemble)	-355^	286.7	2033.4DRIL (dropout)	581.4	205.4	2124.5DRIL (raw cost)	421.8	70.9	1265.5DRIL (no BC cost)	102.1	78.3	538.4BC		257.0	2.7	689.7Results are shown in Figure 4. First, switching from the clipped cost in {-1, +1} to the the rawcost causes a drop in performance. One explanation may be that since the raw costs are alwayspositive (which corresponds to a reward which is always negative), the agent may learn to terminatethe episode early in order to minimize the total cost incurred. Using a cost/reward which has bothpositive and negative values avoids this behavior.
