Table 1: The training hyper-parameter set HLoptimizer	SGD	initial LR	0.1Nesterov	X	ending LR	0momentum	0.9	LR schedule	cosineweight decay	0.0005	epoch	200batch size	256	initial channel	16V	4	N	5random flip	p=0.5	random crop	Xnormalization	X		cay as 0.0005 and decay the learning rate from 0.1 to 0 with a cosine annealing (Loshchilov &Hutter, 2017). We use the same Ht on different datasets, except for the data augmentation which isslightly different due to the image resolution. On CIFAR, we use the random flip with probabilityof 0.5, the random crop 32×32 patch with 4 pixels padding on each border, and the normalizationover RGB channels. On ImageNet-16-120, we use a similar strategy but random crop 16×16 patchwith 2 pixels padding on each border. Apart from using Ht for all datasets, we also use a differenthyper-parameter set H for CIFAR-10. It is similar to Ht but its total number of training epochsis 12. In this way, we could provide bandit-based algorithms (Falkner et al., 2018; Li et al., 2018)more options for the usage of short training budget (see more details in appendix).
Table 2: NAS-Bench-201 provides the followingmetrics with Ht. ‘Acc.’ means accuracy.
Table 3: We summarize some characteristics of NAS-Bench-101 and NAS-Bench-201. Our NAS-Bench-201 can directly be applicable to almost any up-to-date NAS algorithms. In contrast, aspointed in (Ying et al., 2019), NAS algorithms based on parameter sharing or network morphismscannot be directly evaluated on NAS-Bench-101. Besides, NAS-Bench-201 provides train/valida-tion/test performance on three (one for NAS-Bench-101) different datasets so that the generality ofNAS algorithms can be evaluated. It also provides some diagnostic information that may provideinsights to design better NAS algorithms.
Table 4: The utility of our NAS-Bench-201 for different NAS algorithms. We show whether a NASalgorithm can use our NAS-Bench-201 to accelerate the searching and evaluation procedure.
Table 5: We evaluate 10 different searching algorithms in our NAS-Bench-201. The first blockshows results of parameter sharing based NAS methods. The second block is similar to the firstone, however, BN layers in the searching cells do not keep running estimates but always use batchstatistics. The third block shows results of NAS methods without parameter sharing. Each algorithmuses the training and validation set of CIFAR-10 for searching. We show results of their searchedarchitectures for (1) training on the CIFAR-10 train set and evaluating on its validation set; (2)training on the CIFAR-10 train+validation sets and evaluating on its test set; (3) training on theCIFAR-10 or ImageNet-16-120 train set and evaluating on their validation or test sets. “optimal”indicates the highest mean accuracy for each set. We report the mean and std of 500 runs for RS,REA, REINFORCE, and BOHB andof3 runs for RSPS, DARTS, GDAS, SETN, and ENAS.
Table 6: We compare the correlation of different training strategies. The correlation coefficientbetween the validation accuracy after several training epochs on CIFAR-10 and (1) the validationaccuracy of full trained models on the CIFAR-10 training set, (2) the test accuracy on CIFAR-10trained with the training and validation sets, (3) the validation/test accuracy on CIFAR-100 trainedwith the CIFAR-100 training set, (4) the validation/test accuracy on ImageNet-16-120 trained withthe ImageNet-16-120 training set. We use the validation accuracy after “EPOCHS“ training epochs,where the the cosine annealing converged after “TOTAL” epochs.
Table 7: The correlation between the probability or the one-shot validation accuracy (OSVA) andthe ground truth accuracy on the CIFAR-10 validation set. “BN with Train” indicates that, duringevaluation, the mean and variance of BN layers are calculated within each mini-batch. “BN withEval” indicates that we accumulate mean and variance of BN layers in the training set and use theseaccumulated mean and variance for evaluation. We report the correlation as the average of 3 runs.
