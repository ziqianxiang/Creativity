Table 1: Two-parameter exponential family members and selected properties.
Table 2: Network architecture for artificial data experimentsType of block	Number	Input shape	Afine coupling function layer widthsFully Connected Coupling	8	10	5 → 10 → 10 → 10Table 3: Network architecture for EMNIST experimentsType of block	Number	Input shape	Afine coupling function layer widthsDownsampling	1	(1, 28, 28)	Convolutional Coupling	4	(4,14,14)	2 → 16 → 16 → 4Downsampling	1	(4,14,14)	Convolutional Coupling	4	(16,7, 7)	8 → 32 → 32 → 16Flattening	1	(16, 7, 7)	Fully Connected Coupling	2	784	392 → 392 → 392 → 784D. 1 Note on Optimization MethodIn the experiments described in this paper, the mean and variance of a mixture component wasupdated at each iteration as the mean and variance of the transformations to latent space of all datapoints belonging to that mixture component in a minibatch of data D :μi(u0; θ) — Ed：u=uo(g-1(x; θ))	(36)σ2(u0; θ) — VarD：u=uo(g-1(x； θ)).	(37)Hence the parameters of the mixture components would change in each batch, according to the datapresent. The notation μi(u; θ) does not indicate that μ is directly parameterized by θ and learned,instead it indicates that μ is a function of g-1 which is parameterized by θ. A change in θ will also
Table 3: Network architecture for EMNIST experimentsType of block	Number	Input shape	Afine coupling function layer widthsDownsampling	1	(1, 28, 28)	Convolutional Coupling	4	(4,14,14)	2 → 16 → 16 → 4Downsampling	1	(4,14,14)	Convolutional Coupling	4	(16,7, 7)	8 → 32 → 32 → 16Flattening	1	(16, 7, 7)	Fully Connected Coupling	2	784	392 → 392 → 392 → 784D. 1 Note on Optimization MethodIn the experiments described in this paper, the mean and variance of a mixture component wasupdated at each iteration as the mean and variance of the transformations to latent space of all datapoints belonging to that mixture component in a minibatch of data D :μi(u0; θ) — Ed：u=uo(g-1(x; θ))	(36)σ2(u0; θ) — VarD：u=uo(g-1(x； θ)).	(37)Hence the parameters of the mixture components would change in each batch, according to the datapresent. The notation μi(u; θ) does not indicate that μ is directly parameterized by θ and learned,instead it indicates that μ is a function of g-1 which is parameterized by θ. A change in θ will alsochange the output of μ, given the same mini-batch of data D. The same holds for σi(u; θ).
