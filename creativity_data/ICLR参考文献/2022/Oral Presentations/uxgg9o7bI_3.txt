Published as a conference paper at ICLR 2022
A New Perspective on "How Graph Neural Net-
works Go Beyond Weisfeiler-Lehman?"
Asiri Wijesinghe & Qing Wang
School of Computing, Australian National University, Canberra, Australia
{asiri.wijesinghe, qing.wang}@anu.edu.au
Ab stract
We propose a new perspective on designing powerful Graph Neural Networks
(GNNs). In a nutshell, this enables a general solution to inject structural properties
of graphs into a message-passing aggregation scheme of GNNs. As a theoretical
basis, we develop a new hierarchy of local isomorphism on neighborhood sub-
graphs. Then, we theoretically characterize how message-passing GNNs can be
designed to be more expressive than the Weisfeiler Lehman test. To elaborate
this characterization, we propose a novel neural model, called GraphSNN, and
prove that this model is strictly more expressive than the Weisfeiler Lehman test in
distinguishing graph structures. We empirically verify the strength of our model on
different graph learning tasks. It is shown that our model consistently improves the
state-of-the-art methods on the benchmark tasks without sacrificing computational
simplicity and efficiency.
1	Introduction
Many Graph Neural Networks (GNNs) employ a message-passing aggregation scheme to learn
low-dimensional vector space representations for nodes in a graph (KiPf & Welling, 2017; Velickovic
et al., 2017; Hamilton et al., 2017; Gilmer et al., 2017; Sato, 2020; Loukas, 2020; de Haan et al.,
2020). Let G = (V, E) be a graph. For each node v ∈ V , a message-passing aggregation scheme
recursively aggregates the feature vectors of nodes in the neighborhood of v and combines the
aggregated information with the feature vector of v itself to obtain a representation. Since there is
no natural ordering on nodes, such message-passing aggregation schemes are usually required to be
permutation-invariant (Maron et al., 2018; Keriven & Peyr6, 2019; Garg et al., 2020).
Despite advances of GNNs in various graph learning tasks such as node classification (Kipf &
Welling, 2017; Xu et al., 2018), graph classification (Xu et al., 2019; Wu et al., 2019) and link
prediction (Zhang & Chen, 2017), there is still a lack of theoretical understanding of how to design
powerful and practically useful GNNs that can capture rich structural information of graphs. Recent
studies (Xu et al., 2019; Morris et al., 2019) have explored the connections between GNNs and the
Weisfeiler-Lehman (WL) test (Weisfeiler & Leman, 1968). By representing a neighborhood as a
multiset of feature vectors and treating the neighborhood aggregation as an aggregation function
over multisets, Xu et al. (2019) showed that message-passing GNNs are at most as powerful as the
WL test in distinguishing graph structures. However, many simple graph structures still cannot be
distinguished by the WL test, e.g., G1 and G2 shown in Figure 1. A question is: how to design
expressive yet simple GNNs that can go beyond the WL test with a theoretically provable guarantee?
Recently, there have been three main directions of extending GNNs beyond WL: (1) building GNNs
for higher-order WL (i.e. k-WL with k ≥ 3) or variants (Maron et al., 2019; Morris et al., 2020;
2019); (2) counting on pre-defined substructures as additional features (Bouritsas et al., 2020); (3)
augmenting node identifiers or random features into GNNs (You et al., 2021; Vignac et al., 2020;
Sato et al., 2021). Unlike these works, we aim to introduce a general solution upon which GNNs can
be enhanced to capture structural properties of graphs. This solution enables GNNs to provably be
more expressive than the Weisfeiler-Lehman test, but still computationally efficient. It overcomes
the following limitations of existing works. Compared with higher-order WL methods in (1) which
require high computational overhead and are impractical, our method goes beyond the WL test but
is still computationally efficient. Compared with the methods on counting substructures in (2), our
1
Published as a conference paper at ICLR 2022
Figure 1: An overview of our proposed framework for GNNs that can go beyond the WL test
in distinguishing non-isomorphic graphs Gi and G?. The overlap subgraphs of Gi and G2 are
structurally different, which are captured by structural coefficients defined in Eq. 4.
method does not require to handcraft substructures. Compared with the methods of augmenting node
identifiers or random features in (3), our method can flexibly quantify local structures (see examples
in Figure 3) and also capture different classes of local structures w.r.t. different graph learning tasks.
Our work is grounded in three observations: (i) Treating a neighborhood as a multiset of feature
vectors ignores the rich structure information among vertices in the neighborhood, thereby limiting
the representational capacity of the model. Thus, we represent a neighborhood as a neighborhood
subgraph in which vertices are structurally related, and show that the WL test is only as powerful
as distinguishing neighborhood subgraphs in terms of their subtree structures in the neighborhood.
(ii) There exists a natural class of isomorphic graphs, which strictly lies in between neighborhood
subgraph isomorphism and neighborhood subtree isomorphism. We call it overlap (subgraph)
isomorphism. The notion of overlap subgraph enables us to characterize structural interactions of
vertices and inject them into a message-passing aggregation scheme for GNNs. (iii) By designing a
proper function for quantifying structural interactions of vertices and preserving the injectiveness of
a message-passing aggregation scheme, more expressive GNNs can be developed. We propose a new
GNN model that is strictly more expressive than the WL test to demonstrate an instance of this kind.
Contributions. In summary, the main contributions of this work are as follows:
•	We introduce a new hierarchy of local isomorphism to characterise different classes of local
structures in neighborhood subgraphs, and discuss its connections with the WL test and
GNNs (Section 2 and Theorems 1-2).
•	We develop a simple yet powerful framework to inject structural properties into a message-
passing aggregation scheme, and theoretically characterize how GNNs can be designed to
be more expressive beyond the WL test (Section 3 and Theorem 3).
•	We propose a novel neural model for graph learning, called GraphSNN, and prove that
GraphSNN is strictly more expressive than the the WL test in distinguishing graph structures
(Section 4 and Theorem 4).
•	We show that, due to the way of injecting structural properties into a structured-message-
passing aggregation scheme, GraphSNN can overcome the oversmoothing issue (Chen et al.,
2020a; Zhao & Akoglu, 2019; Li et al., 2018) (Section 5.4).
We have conducted experiments on benchmark tasks (Hu et al., 2020). The experimental results show
that our model is highly efficient and can significantly improve the state-of-the-art methods without
sacrificing computational simplicity.
Related work. Weisfeiler-Lehman (WL) hierarchy is a well-established framework for graph
isomorphism tests (Grohe, 2017). Introduced by Weisfeiler and Lehman (Weisfeiler & Leman, 1968),
the Weisfeiler-Lehman algorithm (also called 1-WL or color refinement) is a computationally efficient
heuristic for testing graph isomorphism (Babai & Kucera, 1979). It is known that k-WL is strictly
more powerful than (k-1)-WL when k≥3 (Cai et al., 1992; Grohe, 2017).
2
Published as a conference paper at ICLR 2022
Message-passing GNNs are typically considered as a differentiable neural generalization of the
Weisfeiler-Lehman algorithms on graphs. It has been reported (Xu et al., 2019) that some popular
GNNs such as GCN (Kipf & Welling, 2017) and GraphSAGE (Hamilton et al., 2017) are at most
powerful as 1-WL in distinguishing graph structures. Xu et al. (2019) has shown that Graph
Isomorphism Network (GIN) can be as powerful as 1-WL. At its core, GIN provides an injective
aggregation scheme that is defined as a function over multisets of feature vectors, and thus GIN
has the representational power to map any two different multisets of feature vectors to different
representations in an embedding space.
A considerable amount of efforts has been devoted to improve the expressive power of GNNs beyond
1-WL. Generally, there are three directions: (1) Several works proposed higher-order variants of
GNNs that are as powerful as k-WL with k ≥ 3 (Azizian & Lelarge, 2020). For example, Morris
et al. (2019) introduced k-order graph networks that are expressive as a set-based variant of k-WL,
Maron et al. (2019) proposed a reduced 2-order graph network that is as expressive as 3-WL, and
Morris et al. (2020) proposed a local version of k-WL which considers only a subset of vertices in a
neighborhood. However, these more expressive GNNs are impractical to use due to their inherent high
computational costs and sophisticated design. (2) Some works attempted to incorporate inductive
biases based on isomorphism counting on pre-defined topological features such as triangles, cliques,
and rings (Bouritsas et al., 2020; Liu et al., 2020; Monti et al., 2018), similar to the traditional ideas of
graph kernels (Yanardag & Vishwanathan, 2015). However, pre-defining topological features requires
domain-specific expertise, which is often not readily available. (3) Most recently, several works
explored the ideas of augmenting GNNs using node identifiers or random features. For example,
Vignac et al. (2020) proposed a method that maintains a “local context" for each node based on
manipulating node identifiers in a permutation equivariant way. You et al. (2021) developed ID-GNNs
by taking into account the identity information of vertices. Chen et al. (2020b) and Murphy et al.
(2019) assigned one-hot IDs to vertices based on the ideas of relational pooling. Sato et al. (2021)
added a random feature to each node to improve the representational capability of GNNs.
Our work is fundamentally different from existing models by injecting properties of structural
interactions among vertices based on a natural class of isomorphic graphs in the local neighborhood
(i.e., overlap subgraph isomorphism) into a message-passing aggregation scheme of GNNs.
2	A New Hierarchy of Local Isomorphism
In this section, we characterize a hierarchy of graph isomorphism based on local neighborhood
subgraphs and explore its connections to 1-WL.
Let G = (V, E) be a simple, undirected graph with a set V of vertices and a set E of edges. The set
of neighbors of a vertex v is denoted by N (v) = {u ∈ V |(v, u) ∈ E}. The neighborhood subgraph
of a vertex v, denoted by Sv, is the subgraph induced in G by N(v) = N(v) ∪ {v}, which contains
all edges in E that have both endpoints in N (v). For two adjacent vertices v and u, i.e., (v, u) ∈ E,
the overlap subgraph Svu between v and u is defined as Svu = Sv ∩ Su .
Let Si and Sj be the neighborhood subgraphs of two vertices i and j that are not necessarily adjacent,
and hv be the feature vector of a vertex v ∈ V . In the following, we define three notions of
isomorphism, which correspond to different classes of local structures in neighborhood subgraphs.
Definition 1. Si and Sj are subgraph-isomorphic, denoted as Si 'subgraph Sj, if there exists a
bijective mapping g : N(i) → N(j) such that g(i) = j and for any two vertices v1, v2 ∈ N (i), v1
and v2 are adjacent in Si iff g(v1) and g(v2) are adjacent in Sj, and hv1 = hg(v1) and hv2 = hg(v2).
Definition 2. Si and Sj are overlap-isomorphic, denoted as Si 'overlap Sj, if there exists a bijective
mapping g : N(i) → N(j) such that g(i) = j and for any v0 ∈ N(i) and g(v0) = u0, Siv0 and Sju0
are subgraph-isomorphic.
Definition 3. Si and Sj are subtree-isomorphic, denoted as Si 'subtree Sj, if there exists a bijective
mapping g : N(i) → N(j) such that g(i) = j and for any v0 ∈ N(i) and g(v0) = u0, hv0 = hu0.
Theorem 1 states that there is a hierarchy among these notions of local isomorphism on neighborhood
subgraphs, where subgraph-isomorphism is the strongest one, subtree-isomorphism is the weakest,
3
Published as a conference paper at ICLR 2022
Figure 2: (a) Si and Sj are overlap-isomorphic (i.e., having the same overlap subgraph) but not
subgraph-isomorphic; (b) Four neighborhood subgraphs {Svi |i = 1, 2, 3,4} are subtree-isomorphic
(i.e., having the same subtree) but not overlap-isomorphic.
and overlap-isomorphism lies in between. Figure 2 shows two groups of graphs: one is distinguishable
w.r.t. subgraph-isomorphism but not overlap-isomorphism, while the other is distinguishable by
overlap-isomorphism but not subtree-isomorphism.
Theorem 1.	The following statements are true: (a) If Si 'subgraph Sj, then Si 'overlap Sj; but not
vice versa; (b) If Si 'overlap Sj, then Si 'subtree Sj; but not vice versa.
Let S = {Sv |v ∈ V } and ζ : S → Rd mapping each neighborhood subgraph in S into a node
embedding in Rd. The following theorem states that GNNs that are as powerful as 1-WL can
distinguish two neighborhood subgraphs only w.r.t. subtree-isomorphism at each layer.
Theorem 2.	Let M be a GNN. M is as powerful as 1-WL in distinguishing non-isomorphic graphs
if M has a sufficient number of layers and each layer can map any Si and Sj in S into two different
embeddings (i.e., ζ(Si) 6= ζ(Sj)) if and only if Si 6'subtree Sj.
The complete proofs of these theorems are provided in Appendix C.
3	A Generalised Message-Passing Framework
In this section, we present a generalised message-passing framework (GMP) which enables to inject
local structure into an aggregation scheme, in light of overlap subgraphs. We theoretically characterize
how GNNs can be designed to be more expressive than 1-WL in this framework.
Let S * = {Svu | (v,u) ∈ E} be the set of overlap subgraphs in G. We define structural coefficients for
each vertex V and its neighbors, i.e., ω : SXS* → R such that Avu = ω(Sv, Svu). A question arising
is: what are the desirable properties of such a function ω? Ideally, it should quantify how a vertex v
structurally interacts with its neighbor u in the local neighborhood. Thus, given Svu = (Vvu, Evu)
and Svu0 = (Vvu0, Evu0), a carefully designed ω should exhibit the following properties:
(1)	Local closeness: ω(Sv , Svu) > ω(Sv , Svu0 ) if Svu and Svu0 are complete graphs with
Svu = Ki , Svu0 = Kj , and i > j, where Ki refers to a complete graph on i vertices.
(2)	Local denseness: ω(Sv, Svu) > ω(Sv, Svu0 ) if Svu and Svu0 have the same number of
vertices but differ in the number of edges s.t. |Vvu| = |Vvu0 | and |Evu| > |Evu0 |.
(3)	Isomorphic invariant: ω(Sv, Svu) = ω(Sv, Svu0) if Svu and Svu0 are isomorphic.
Figure 3 illustrates the first two properties. Let {∙} denote a multiset, A = (Avu)v,u∈v where Avu
is a normalised value of Avu, and X ∈ RlVl×f be a matrix of input feature vectors where Xv ∈ Rf
associates each v ∈ V . We denote the feature vector of v at the t-th layer by h(vt) and h(v0) = xv .
Then, the (t+1)-th layer of an aggregation scheme can be defined as:
m't) = AGGREGATEN (K(Avu,hut))∣u ∈N(ν)j)),	(1)
mvt) = AGGREGATEI(KAvu|u ∈ N(ν)j}) hvt),	(2)
h(vt+1) = COMBINE m(vt), m(at) .	(3)
Aggregaten(∙) and Aggregate1 (∙) are two possibly different parameterized functions. Here,
m(at) is a message aggregated from the neighbors of v and their structural coefficients, and m(vt) is an
4
Published as a conference paper at ICLR 2022
Figure 3: (a) Local closeness: for overlap subgraphs that are complete graphs, their structural
coefficients increase with the number of vertices; (b) Local denseness: for overlap subgraphs that
have the same number of vertices, their structural coefficients increase with the number of edges.
“adjusted” message from V after performing an element-wise multiplication between Aggregate1 (∙)
and hVt) to account for structural effects from its neighbors. Then, mVt and mOt) are combined by
COMBINE(∙) to obtain the feature vector hVt+1).
The following theorem states that a GNN can be more expressive than 1-WL if ω is powerful enough
to distinguish structure beyond neighborhood subtrees and the neighborhood aggregation function Φ
is injective under a sufficient number of layers. The proof is provided in Appendix C.
Theorem 3.	Let M be a GNN whose aggregation scheme Φ is defined by Eq. 1-Eq. 3. M is strictly
more expressive than 1-WL in distinguishing non-isomorphic graphs if M has a sufficient number of
layers and also satisfies the following conditions:
(1)	M can distinguish at least two neighborhood subgraphs Si and Sj with Si 'subtree Sj,
Si 6'subgraph Sj and {Aiv0 |v ∈ N(i) } 6= {Aju0 |u ∈ N (j) };
(2)	φ(hVt), Ihut)Iu ∈ N(v)⅛, {(Avu, hUt))∣u ∈ N(v)⅛^ is injective.
4 GRAPHSNN
Generally, there are many different ways of designing ω and Φ functions, leading to GNNs with
different expressive powers. To elaborate this, we propose a novel GNN model, named GraphSNN,
whose aggregation scheme is an instantiation of our generalised message-passing framework. We
prove that the expressive power of GraphSNN goes beyond 1-WL.
Model design. In the following, we provide a definition of ω that satisfies the properties of local
closeness, local denseness, and isomorphic invariant. One key idea behind this definition is to make it
capable of being generalized to support different graph learning tasks, controlled by λ > 0 (will be
further discussed in Section 5.3):
ω(Sv ,Svu)= IVVuI lEuu- 1| lVvulλ.
(4)
This definition allows us to formulate a weighted adjacency matrix A = (AVu)V,u∈V for Graph-
SNN. To compare structural coefficients across different nodes, we normalize A to A by AVu =
-Avu A-. Alternatively, A can be normalized using Softmax or other normalization techniques.
u∈N (v) Avu
For each vertex v ∈ V , the feature vector at the (t+1)-th layer is generated by
hVt+1)= MLPθ(γ⑴(X Avu + ι)hVt) + X (Avu + l)hut)
u∈N (v)	u∈N (v)
(5)
where γ(t) is a learnable scalar parameter. Since N(v) refers to one-hop neighbors of v, one can
stack multiple layers to handle more than one-hop neighborhood. Note that, to ensure the injectivity
in the feature aggregation in the presence of structural coefficients, we add 1 into the first and second
terms in Eq. 5. This design is critical for guaranteeing the expressiveness of GraphSNN beyond
1-WL, as will be discussed in the proofs of the lemmas and Theorem 4 later.
Expressiveness analysis. We first generalise the result of universal functions over multisets (Xu et al.,
2019) to universal functions over pairs of multisets since Eq. 5 involves not only node features but
5
Published as a conference paper at ICLR 2022
Method	Cora	Citeseer	PUbmed	NELL	ogbn-arxiv
GCN	81.5 ±0.4	70.3 ± 0.5	79.0 ± 0.5	66.0 ± 1.7	71.74 ± 0.29
GraPhSNNGCN	83.1 ± 1.8	72.3 ± 1.5	79.8 ± 1.2	68.3 ± 1.6	72.20 ± 0.90
GAT	83.0 ± 0.6	72.6 ± 0.6	78.5 ± 0.3	-	-
GraPhSNNGAT	83.8 ± 1.2	73.5 ± 1.6	79.6 ± 1.4	-	-
GIN	77.6 ± 1.1	66.1 ± 1.5	77.0 ± 1.2	61.5 ± 2.3	-
GraPhSNNGIN	79.2 ± 1.7	68.3 ± 1.5	78.8 ± 1.3	63.8 ± 2.7	-
GraPhSAGE	79.2 ± 3.7	71.6 ± 1.9	77.4 ± 2.2	63.7 ± 5.2	71.49 ± 0.27
GraPhSNNGraphSAGE	80.5 ± 2.5	72.7 ± 3.2	79.0 ± 3.5	66.3 ± 5.6	71.80 ± 0.70
Table 1: Classification accuracy (%) averaged over 10 runs on node classification.
also structural coefficients. Assume that H, A and W are countable sets where H is a node feature
space, A is a structural coefficient space, and W = {Aijhi|Aij ∈ A, hi ∈ H}. Let H and W be two
multisets containing elements from H and W, respectively, and |H| = |W |. We can prove Lemma 1,
Lemma 2 and Theorem 4 below, where the proof details are provided in Appendix C.
Lemma 1. There exists a function f s.t. π(H, W) =	h∈H,w∈W f(h, w) is unique for any distinct
pair of multisets (H, W).
Then, the injectiveness of π(H, W) can be extended to π0(a, H, W) as in the lemma below.
Lemma 2. There exists a function f s.t. π0(hv, H, W) = γf(hv, |H|hv) + h∈H,w∈W f(h, w) is
unique for any distinct (hv, H, W), where hv ∈ H, |H |hv ∈ W, and γ can be an irrational number.
Since any function over (hv, H, W) can be decomposed as g(γf (hv, |H |hv) + Ph∈H,w∈W f(h, w)),
similar to Xu et al. (2019), we use a parameterized multi-layer perceptron (MLP) to learn f and g.
The following theorem characterises the expressive power of GraphSNN.
Theorem 4.	GraphSNN is more expressive than 1-WL in testing non-isomorphic graphs.
Since GIN is as powerful as 1-WL (Xu et al., 2019), this theorem implies that GraphSNN is more
expressive than GIN, i.e., GraphSNN can map at least two different neighborhood subgraphs that
correspond to the same multiset of feature vectors to different representations.
Complexity analysis. Similar to GCN and GIN, GraphSNN is computationally efficient. The time
complexity and memory complexity are linear w.r.t. the number of edges in a graph. Further, due
to the locality of GraphSNN, the computation of aggregating feature vectors from neighborhood
subgraphs at each layer can be parallelized across all vertices. Structural coefficients can be pre-
computed with the time complexity O(ml), where m is the number of edges and l is the maximum
degree of vertices in a graph, and this computation can also be parallelized across all edges. Table
9 in Appendix A summarizes the time and space complexities of several popular message-passing
GNNs in comparison with GraphSNN.
5	Numerical Experiments
In this section, we evaluate our models on node classification and graph classification benchmark
tasks. All the results of our models are statistically significant at 0.05 level of significance.
5.1	Node Classification
Datasets. We use five datasets: three citation network datasets Cora, Citeseer, and Pubmed (Sen et al.,
2008) for semi-supervised document classification, one knowledge graph dataset NELL (Carlson
et al., 2010) for semi-supervised entity classification, and one OGB dataset ogbn-arxiv from (Hu
et al., 2020). Table 10 in Appendix B contains statistics for these datasets.
Baseline methods. We consider the popular message-passing GNNs: GCN (Kipf & Welling, 2017),
GAT (Velickovic et al., 2017), GIN (XU et al., 2019), and GraPhSAGE (Hamilton et al., 2017). For
each of these baselines, we construct a GraphSNNM model by replacing its aggregation scheme by
oUr aggregation scheme, which is detailed in APPendix A. The PUrPose of this setUP is to evalUate
6
Published as a conference paper at ICLR 2022
Method	MUTAG	PTC-MR	PROTEINS	D&D	BZR	COX2	IMDB-B	RDT-M5K
WL	90.4 ± 5.7	59.9 ± 4.3	75.0 ± 3.1	79.4 ± 0.3	78.5 ± 0.6	81.7 ± 0.7	73.8 ± 3.9	52.5 ± 2.1
RetGK	90.3 ± 1.1	62.5 ± 1.6	75.8 ± 0.6	81.6 ± 0.3	-	-	71.9 ± 1.0	-
GNTK	90.0 ± 8.5	67.9 ± 6.9	75.6 ± 4.2	75.6 ± 3.9	83.6 ± 2.9	-	76.9 ± 3.6	-
P-WL	90.5 ± 1.3	64.0 ± 0.8	75.2 ± 0.3	78.6 ± 0.3	-	-	-	-
WL-PM	87.7 ± 0.8	61.4 ± 0.8	-	78.6 ± 0.2	-	-	-	-
WWL	87.2 ± 1.5	66.3 ± 1.2	74.2 ± 0.5	79.6 ± 0.5	84.4 ± 2.0	78.2 ± 0.4	74.3 ± 0.8	-
FGW	88.4 ± 5.6	65.3 ± 7.9	74.5 ± 2.7	-	85.1 ± 4.1	77.2 ± 4.8	63.8 ± 3.4	-
DGCNN	85.8 ± 1.7	58.6 ± 2.5	75.5 ± 0.9	79.3 ± 0.9	-	-	70.0 ± 0.9	48.7 ± 4.5
CapsGNN	86.6 ± 6.8	66.0 ± 1.8	76.2 ± 3.6	75.4 ± 4.1	-	-	73.1 ± 4.8	52.9 ± 1.5
t GraphSAGE	85.1 ± 7.6	63.9 ± 7.7	75.9 ± 3.2	72.9 ± 2.0	-	-	72.3 ± 5.3	50.0 ± 1.3
tGIN	89.4 ± 5.6	64.6 ± 7.0	75.9 ± 2.8	-	-	-	75.1 ± 5.1	57.5 ± 1.5
t GraphSNN (S)	91.57 ± 2.8	66.70 ± 3.7	76.83 ± 2.5	81.97 ± 2.6	88.69 ± 3.2	82.86 ± 3.1	77.86 ± 3.6	58.43 ± 2.3
tGraphSNN (R)	91.24 ± 2.5	66.96 ± 3.5	76.51 ± 2.5	82.46 ± 2.7	88.97 ± 2.9	83.13 ± 3.5	76.93 ± 3.3	58.51 ± 2.7
GraphSNN (S)	94.70 ± 1.9	70.58 ± 3.1	78.42 ± 2.7	83.92 ± 2.3	91.12 ± 3.0	86.28 ± 3.3	78.51 ± 2.8	59.86 ± 2.6
GraphSNN (R)	94.14 ± 1.2	71.01 ± 3.6	78.21 ± 2.9	84.61 ± 1.5	91.88 ± 3.2	86.72 ± 2.9	77.87 ± 3.1	60.23 ± 2.2
Table 2: Classification accuracy (%) averaged over 10 runs on graph classification. The results of
WL and RetGK are taken from (Du et al., 2019), GraphSAGE from (Xu et al., 2019), DGCNN from
(Maron et al., 2019) and others from their original papers. f indicates the reporting setting used in
GIN and further details on the experimental settings are discussed in Appendix B.
how effectively our aggregation scheme with structural coefficients can learn representations for
vertices, compared with the standard message-passing aggregation scheme.
Experimental setup. We use the Adam optimizer (Kingma & Ba, 2015) and λ = 1. For ogbn-
arxiv, our models are trained for 500 epochs with the learning rate 0.01, dropout 0.5, hidden
units 256, and γ = 0.1. For the other datasets, we use 200 epochs with the learning rate 0.001,
and choose the best values for weight decay from {0.001, 0.002, ..., 0.009} and hidden units from
{64, 128, 256, 512}. For γ and dropout at each layer, the best value for each model in each dataset is
selected from {0.1, 0.2, ..., 0.6}. GraphSNNGAT uses the attention dropout 0.6 and 8 multi-attention
heads. GraphSNNGraphSAGE uses the neighborhood sample size 25 with the mean aggregation.
We consider two settings of data splits for all datasets except for ogbn-arxiv: (1) the standard splits in
Kipf & Welling (2017), i.e., 20 nodes from each class for training, 500 nodes for validation and 1000
nodes for testing, for which the results are presented in Table 1; (2) the random splits in Pei et al.
(2020), i.e., randomly splitting nodes into 60%, 20% and 20% for training, validation and testing,
respectively, for which the results are presented in Table 13 in Appendix B. For ogbn-arxiv, we follow
Hu et al. (2020) to use a time-based data split based on publication dates.
5.2	Graph Classification
We evaluate GraphSNN from three aspects: (1) small standard graph datasets, (2) large graph datasets
and (3) comparison with GNNs that are go beyond 1-WL.
Experiments on small graphs. We use eight datasets from two categories: (1) bioinformatics
datasets: MUTAG, PTC-MR, COX2, BZR, PROTEINS, and D&D (Debnath et al., 1991; Kriege et al.,
2016; Wale et al., 2008; Shervashidze et al., 2011; Sutherland et al., 2003; Borgwardt & Kriegel,
2005); (2) social network datasets: IMDB-B and RDT-M5K (Yanardag & Vishwanathan, 2015).
Table 11 in Appendix B contains statistics for these small graph datasets.
We compare against eleven baselines: (1) Graph kernel based methods: WL subtree kernel (Sher-
vashidze et al., 2011), RetGK (Zhang et al., 2018b), GNTK (Du et al., 2019), P-WL (Rieck et al.,
2019), WL-PM (Nikolentzos et al., 2017), WWL (Togninalli et al., 2019) and FGW (Titouan et al.,
2019); (2) GNN based methods: DGCNN (Zhang et al., 2018a), CapsGNN (Xinyi & Chen, 2018),
GIN (Xu et al., 2019), and GraphSAGE (Hamilton et al., 2017).
Both the standard stratified splits (Xu et al., 2019) and the random splits are considered. We use
10-fold cross validation with 90% training and 10 % testing, and report the best mean accuracy. For
both settings, we use the Adam optimizer (Kingma & Ba, 2015), batch size 64, hidden dimension 64,
weight decay of 0.009, a 2-layer MLP with batch normalization, 500 epochs and dropout of 0.6, and
γ = 0.1 over all datasets. The readout function as in (Xu et al., 2019) is used which concatenates
representations of all layers to obtain a final graph representation. For the standard stratified splits,
we use the learning rate 0.009 over all datasets. For the random splits, we use the learning rate 0.008
for MUTAG and RDT-M5K, and 0.007 for the other datasets. Table 2 presents the results.
7
Published as a conference paper at ICLR 2022
Method	ogbg-molhiv	ogbg-moltox21	ogbg-moltoxcast	ogbg-ppa	ogbg-molpcba
GIN	75.58±1.40	-74.91±0.51-	63.41±0.74	68.92±1.00	22.66±0.28
GIN+VN	75.20±1.30	76.21±0.82	66.18±0.68	70.37±1.07	27.03±0.23
GSN	77.99±1.00	-	-	-	-
PNA	79.05±1.30	-	-	-	28.38±0.35
ID-GNN	78.30±2.00	-	-	-	-
Deep LRP	77.19±1.40	-	-	-	-
GraphSNN	78.51±1.70	-75.45±1.10-	65.40±0.71	70.66±1.65	24.96±1.50
GraphSNN+VN	79.72±1.83	76.78±1.27	67.68±0.92	72.02±1.48	28.50±1.68
Table 3: Classification accuracy (%) averaged over 10 runs on graph classification, where λ = 2. The
results of the baselines are taken from (Hu et al., 2020) and the leaderboard of the OGB website.
	Method	MUTAG	PTC-MR	PROTEINS	BZR	IMDB-B
GSN	GSN-e	90.6 ± 7.5	68.2 ± 7.2	76.6 ± 5.0	-	77.8 ± 3.3
	GSN-v	92.2 ± 7.5	67.4 ± 5.7	74.5 ± 5.0	-	76.8 ± 2.0
ID-GNNs	ID-GNN Fast	96.5 ± 3.2	61.9 ± 5.4	78.0 ± 3.5	86.4 ± 3.0	-
	ID-GNN Full	93.0 ± 5.6	62.5 ± 5.3	77.9 ± 2.4	88.1 ± 4.0	-
Ours	GraphSNN	91.57 ± 2.8	66.70 ± 3.7	76.83 ± 2.5	88.69 ± 3.2	77.86 ± 3.6
	1-GNNNT	82.7 ± 0.0	51.2 ± 0.0	-	-	69.4 ± 0.0
k-WL	1-GNN	82.2 ± 0.0	59.0 ± 0.0	-	-	71.2 ± 0.0
GNNs	1-2-3-GNNNT	84.4 ± 0.0	59.3 ± 0.0	-	-	70.3 ± 0.0
	1-2-3-GNN	86.1 ± 0.0	60.9 ± 0.0	-	-	74.2 ± 0.0
Ours	GraphSNN	87.30 ± 3.1	61.63 ± 2.8 -	74.01 ± 3.2	82.72 ± 3.9	74.81 ± 3.5
Table 4: Classification accuracy (%) averaged over 10 runs on graph classification, where λ = 2.
The results of the baselines are taken from their original papers. GSN and ID-GNNs use the same
experimental setup as GIN, while k-WL GNNs uses the same experimental setup as CapsGNN. These
experimental setups are detailed in Appendix B.
Experiments on large graphs. We use five large graph datasets from Open Graph Benchmark
(OGB) Hu et al. (2020), including four molecular graph datasets (ogbg-molhiv, ogbg-moltox21,
ogbg-moltoxcast and ogb-molpcba) and one protein-protein association network (ogbg-ppa). Table
12 in Appendix B contains statistics for these large graph datasets.
We compare against the following methods that have reported the results on the above OGB datasets:
GIN and GIN+VN (Hu et al., 2020), GSN (Bouritsas et al., 2020), PNA (Corso et al., 2020), ID-GNNs
(You et al., 2021) and Deep LRP (Chen et al., 2020b). In addition to the original model of GraphSNN,
we also consider a variant, denoted as GraphSNN+VN, which performs the message passing over
augmented graphs with virtual nodes in GraphSNN (Hu et al., 2020; Ishiguro et al., 2019).
We follow the same experiment setup as in Hu et al. (2020). We use the Adam optimizer with learning
rate 0.001, batch size 32, dropout 0.5 and 100 epochs for all datasets. GraphSNN uses a 8-layer MLP
with embedding dimension 512 for ogbg-moltoxcast and ogbg-moltox21, while GraphSNN+VN has
the embedding dimensions 300 and 256, and 8-layer and 5-layer MLPs for ogbg-moltoxcast and
ogbg-moltox21, respectively. For ogbg-molhiv, ogbg-molpcba and ogbg-ppa, both GraphSNN and
GraphSNN+VN use a 5-layer MLP and embedding dimension 200. Table 3 shows the results for
the classification accuracy. Table 15 in Appendix B shows the results for the running time of the
prepocessing step.
Comparison with GNNs beyond 1-WL. We compare GraphSNN with the other GNNs that are
more expressive than 1-WL, including: GSN (Bouritsas et al., 2020), ID-GNNs (You et al., 2021)
and k-WL GNN (Morris et al., 2019). We use the same experimental setup as in (Xu et al., 2019;
Bouritsas et al., 2020; Maron et al., 2019). Table 4 shows the results.
5.3	Ablation Study
We perform an ablation study to analyze the effect of λ values on model performance. Tables 5 and 6
show that λ = 1 yields the highest performance for node classification, while λ = 2 is the best for
graph classification. This reflects a critical point - different classes of structure information are needed
by different graph learning tasks. λ = 1 captures local density, e.g., two overlap subgraphs may
8
Published as a conference paper at ICLR 2022
Dataset	Method	λ=1	λ二2	λ=3	λ=4	λ=5
	GraphSNNGCN	83.1±1.8	82.8±1.3	82.3±2.4	81.8±1.6	82.1±1.6
Cora	GraphSNNGIN	79.2±1.7	78.8±1.2	78.5±1.3	78.1±1.6	77.7±1.2
	GraphSNNGraphSAGE	80.5±2.5	80.3±2.1	79.8±1.9	79.2±1.9	79.4±2.2
	GraphSNNGAT	83.8±1.2	83.5±1.5	83.2±1.7	82.8±1.3	83.2±1.9
	GraphSNNGCN	72.3±1.5	71.7±1.3	71.1±1.6	70.6±1.2	70.9±1.1
Citeseer	GraphSNNGIN	68.3±1.5	68.3±1.9	67.7±1.4	67.1±1.3	67.3±1.4
	GraphSNNGraphSAGE	72.7±3.2	72.0±2.5	71.6±2.9	71.9±2.1	71.3±2.3
	GraphSNNGAT	73.5±1.6	72.9±1.7	72.5±1.1	72.6±1.6	72.0±1.3
Table 5: Classification accuracy (%) averaged over 10 runs on node classification with standard splits.
Dataset	Method	λ=1	λ=2	λ=3	λ=4	λ=5
MUTAG		92.66±2.4	94.14±1.2	93.38±1.5	92.25±2.1	92.79±2.0
PTC-MR		70.76±5.1	71.01±3.6	70.67±2.8	69.59±2.1	69.97±3.1
PROTEINS		77.90±4.9	78.21±2.9	78.15±2.1	77.20±3.1	76.93±3.2
D&D	GraphSNN	82.70±4.6	84.61±1.5	84.34±1.2	82.60±2.6	82.30±2.3
BZR		87.61±4.9	91.88±3.2	91.45±2.6	91.38±2.1	90.90±3.1
COX2		86.20±3.3	86.72±2.9	83.81±3.1	83.13±2.6	83.94±3.2
IMDB-B		77.07±5.2	77.87±3.1	77.60±3.6	77.32±3.2	77.10±3.3
RDT-M5K		59.53±2.6	60.23±2.2	60.10±2.3	60.00±2.1	59.90±2.6
Table 6: Classification accuracy (%) averaged over 10 runs on graph classification with random splits.
considerably vary in the number of vertices but their local density can be very close. Our experiments
show that injecting such local density helps improve the performance of node classification. λ = 2
captures local similarity, i.e., how similar two overlap subgraphs are. Two overlap subgraphs that
considerably differ in the number of vertices would have very different structural coefficients. Since
graph classification requires to compare the similarity of two graphs, λ = 2 is thus the best.
5.4	Oversmoothing Analysis
We analyse the impact of model depth (number of layers) on node classification performance. In
addition to GCN and GraphSNNGCN, we also compare these models with a residual connection
(i.e., GCN+residual and GraphSNNGC N +residual). We evaluate all the models on Cora dataset using
the standard splits and same hyperparameters as in Section 5.1. Table 7 shows the results. When
increasing the model depth, GraphSNNGCN performs consistently better than GCN at each layer.
This is because structural coefficients capture structural connectivity between a target vertex and its
neighbors. Thus, a neighbor whose structural connectivity is weak would pass little messages to
the target vertex, whereas a neighbor whose structural connectivity is strong would pass a strong
message to the target vertex. GraphSNN helps alleviate the oversmoothing issue even in the presence
of residual connections. Further results of the oversmoothing analysis are provided in Appendix B.
#Layers	GCN	GCN+residual	GraPhSNNGCN	GraPhSNNGCN +residual
1	79.6±0.5	80.3±0.7	80.1±0.8	81.6±1.6
2	81.5±0.4	82.8±1.2	83.1±1.8	84.1±1.7
3	80.3±0.6	82.3±0.5	82.0±0.8	83.4±0.7
4	78.2±0.9	81.5±0.9	80.1±0.7	82.9±0.9
5	74.3±1.3	81.0±1.3	79.1±1.2	82.3±0.3
6	35.6±1.5	80.6±0.5	76.5±1.3	81.5±1.2
7	31.6±0.9	79.7±0.6	76.3±1.3	80.9±0.9
8	16.2±1.2	78.4±1.1	75.7±1.2	80.3±1.3
Table 7: Classification accuracy (%) averaged over 10 runs on Cora dataset.
6	Conclusions
In this paper, we have introduced a GNN framework, which enables a general way of injecting
structural information into a message-passing aggregation scheme. We have also introduced a novel
GNN model, GraphSNN, for graph learning, and prove that GraphSNN is more expressive than
1-WL in distinguishing graph structures. It is shown that GraphSNN consistently outperforms all the
state-of-the-art approaches in both node classification and graph classification benchmark tasks.
9
Published as a conference paper at ICLR 2022
References
Waiss Azizian and Marc Lelarge. Expressive power of invariant and equivariant graph neural networks.
In International Conference on Learning Representations (ICLR), 2020.
LgSzl6 Babai and Ludik Kucera. Canonical labelling of graphs in linear average time. In 20th Annual
Symposium on Foundations ofComputer Science (SFCS),pp. 39-46, 1979.
Karsten M Borgwardt and Hans-Peter Kriegel. Shortest-path kernels on graphs. In Fifth IEEE
international conference on data mining (ICDM’05), pp. 8-pp. IEEE, 2005.
Giorgos Bouritsas, Fabrizio Frasca, Stefanos Zafeiriou, and Michael M Bronstein. Improving graph
neural network expressivity via subgraph isomorphism counting. arXiv preprint arXiv:2006.09252,
2020.
Jin-Yi Cai, Martin Furer, and Neil Immerman. An optimal lower bound on the number of variables
for graph identification. Combinatorica, 12(4):389-410, 1992.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R Hruschka, and Tom M
Mitchell. Toward an architecture for never-ending language learning. In Proceedings of the AAAI
Conference on Artificial Intelligence (AAAI), 2010.
Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and relieving the over-
smoothing problem for graph neural networks from the topological view. In Proceedings of the
AAAI Conference on Artificial Intelligence (AAAI), pp. 3438-3445, 2020a.
Zhengdao Chen, Lei Chen, Soledad Villar, and Joan Bruna. Can graph neural networks count
substructures? Advances in neural information processing systems (NeurIPS), 2020b.
Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Lid, and Petar Velickovic. Principal
neighbourhood aggregation for graph nets. Advances in Neural Information Processing Systems
(NeurIPS), 2020.
Pim de Haan, Taco Cohen, and Max Welling. Natural graph networks. arXiv preprint
arXiv:2007.08349, 2020.
Asim Kumar Debnath, Rosa L Lopez de Compadre, Gargi Debnath, Alan J Shusterman, and Corwin
Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds.
correlation with molecular orbital energies and hydrophobicity. Journal of medicinal chemistry, 34
(2):786-797, 1991.
Simon S Du, Kangcheng Hou, Barnabgs P6czos, Ruslan Salakhutdinov, Ruosong Wang, and Keyulu
Xu. Graph neural tangent kernel: Fusing graph neural networks with graph kernels. arXiv preprint
arXiv:1905.13192, 2019.
Federico Errica, Marco Podda, Davide Bacciu, and Alessio Micheli. A fair comparison of graph
neural networks for graph classification. 2020.
Vikas Garg, Stefanie Jegelka, and Tommi Jaakkola. Generalization and representational limits of
graph neural networks. In International Conference on Machine Learning (ICML), pp. 3419-3430,
2020.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In International Conference on Machine Learning (ICML),
pp. 1263-1272. PMLR, 2017.
Martin Grohe. Descriptive complexity, canonisation, and definable graph structure theory, volume 47.
Cambridge University Press, 2017.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In
Advances in Neural Information Processing Systems (NeurIPS), pp. 1024-1034, 2017.
Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta,
and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in
Neural Information Processing Systems (NeurIPS), 2020.
10
Published as a conference paper at ICLR 2022
Katsuhiko Ishiguro, Shin-ichi Maeda, and Masanori Koyama. Graph warp module: an auxiliary
module for boosting the power of graph neural networks in molecular graph analysis. arXiv
preprint arXiv:1902.01020, 2019.
Nicolas Keriven and Gabriel Peyr6. Universal invariant and equivariant graph neural networks. In
Advances in Neural Information Processing Systems (NeurIPS), 2019.
Diederick P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations (ICLR), 2015.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.
In International Conference on Learning Representations (ICLR), 2017.
Nils M Kriege, Pierre-Louis Giscard, and Richard Wilson. On valid optimal assignment kernels
and applications to graph classification. In Advances in Neural Information Processing Systems
(NeurIPS), pp. 1623-1631, 2016.
Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for
semi-supervised learning. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),
2018.
Xin Liu, Haojie Pan, Mutian He, Yangqiu Song, Xin Jiang, and Lifeng Shang. Neural subgraph
isomorphism counting. In Proceedings of the 26th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining, pp. 1959-1969, 2020.
Andreas Loukas. What graph neural networks cannot learn: depth vs width. In International
Conference on Learning Representations (ICLR), 2020.
Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph
networks. In International Conference on Learning Representations (ICLR), 2018.
Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph
networks. Advances in Neural Information Processing Systems (NeurIPS), 2019.
Federico Monti, Karl Otness, and Michael M Bronstein. Motifnet: a motif-based graph convolutional
network for directed graphs. In 2018 IEEE Data Science Workshop (DSW), pp. 225-228, 2018.
Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav
Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks.
In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), pp. 4602-4609, 2019.
Christopher Morris, Gaurav Rattan, and Petra Mutzel. Weisfeiler and leman go sparse: Towards
scalable higher-order graph embeddings. Advances in Neural Information Processing Systems
(NeurIPS), 2020.
Ryan Murphy, Balasubramaniam Srinivasan, Vinayak Rao, and Bruno Ribeiro. Relational pooling for
graph representations. In International Conference on Machine Learning (ICML), pp. 4663-4673,
2019.
Giannis Nikolentzos, Polykarpos Meladianos, and Michalis Vazirgiannis. Matching node embeddings
for graph similarity. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),
2017.
Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric
graph convolutional networks. In International Conference on Learning Representations (ICLR),
2020.
Bastian Rieck, Christian Bock, and Karsten Borgwardt. A persistent weisfeiler-lehman procedure for
graph classification. In International Conference on Machine Learning (ICML), pp. 5448-5458.
PMLR, 2019.
Ryoma Sato. A survey on the expressive power of graph neural networks. arXiv preprint
arXiv:2003.04078, 2020.
11
Published as a conference paper at ICLR 2022
Ryoma Sato, Makoto Yamada, and Hisashi Kashima. Random features strengthen graph neural
networks. In Proceedings of the 2021 SIAM International Conference on Data Mining (SDM), pp.
333-341,2021.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.
Collective classification in network data. AI magazine, 29(3):93-93, 2008.
Nino Shervashidze, Pascal Schweitzer, Erik Jan Van Leeuwen, Kurt Mehlhorn, and Karsten M
Borgwardt. Weisfeiler-lehman graph kernels. Journal of Machine Learning Research, 12(9), 2011.
Jeffrey J Sutherland, Lee A O’brien, and Donald F Weaver. Spline-fitting with a genetic algorithm:
A method for developing classification structure- activity relationships. Journal of chemical
information and computer sciences, 43(6):1906-1915, 2003.
Vayer Titouan, Nicolas Courty, Romain Tavenard, and Remi Flamary. Optimal transport for structured
data with application on graphs. In International Conference on Machine Learning (ICML), pp.
6275-6284, 2019.
Matteo Togninalli, Elisabetta Ghisu, Felipe Llinares-Ldpez, Bastian Rieck, and Karsten Borgwardt.
Wasserstein weisfeiler-lehman graph kernels. In Advances in Neural Information Processing
Systems (NeurIPS), pp. 6439-6449, 2019.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. International Conference on Learning Representations (ICLR),
2017.
Clement Vignac, Andreas Loukas, and Pascal Frossard. Building powerful and equivariant graph
neural networks with structural message-passing. In Advances in Neural Information Processing
Systems (NeurIPS), 2020.
Nikil Wale, Ian A Watson, and George Karypis. Comparison of descriptor spaces for chemical
compound retrieval and classification. Knowledge and Information Systems, 14(3):347-375, 2008.
Boris Weisfeiler and Andrei Leman. The reduction of a graph to canonical form and the algebra
which appears therein. NTI, Series, 2(9):12-16, 1968.
Asiri Wijesinghe and Qing Wang. Dfnets: Spectral cnns for graphs with feedback-looped filters.
Advances in neural information processing systems (NeurIPS), 2019.
Jun Wu, Jingrui He, and Jiejun Xu. Demo-net: Degree-specific graph neural networks for node
and graph classification. In Proceedings of the 25th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining, pp. 406-415, 2019.
Zhang Xinyi and Lihui Chen. Capsule graph neural network. In International conference on learning
representations (ICLR), 2018.
Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie
Jegelka. Representation learning on graphs with jumping knowledge networks. In International
Conference on Machine Learning (ICML), pp. 5453-5462. PMLR, 2018.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? In International Conference on Learning Representations (ICLR), 2019.
Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In Proceedings of the 21th ACM
SIGKDD international conference on knowledge discovery and data mining, pp. 1365-1374, 2015.
Jiaxuan You, Jonathan Gomes-Selman, Rex Ying, and Jure Leskovec. Identity-aware graph neural
networks. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2021.
Muhan Zhang and Yixin Chen. Weisfeiler-lehman neural machine for link prediction. In Proceedings
of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
pp. 575-583, 2017.
12
Published as a conference paper at ICLR 2022
Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. An end-to-end deep learning archi-
tecture for graph classification. In Proceedings of the AAAI Conference on Artificial Intelligence
(AAAI), 2018a.
Zhen Zhang, Mianzhi Wang, Yijian Xiang, Yan Huang, and Arye Nehorai. Retgk: Graph kernels
based on return probabilities of random walks. In Advances in Neural Information Processing
Systems (NeurIPS),pp. 3964-3974, 2018b.
Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. In International
Conference on Learning Representations (ICLR), 2019.
13
Published as a conference paper at ICLR 2022
Appendix
A.	Connections to Previous Work
In the following, we discuss how our framework generalizes the existing message-passing GNNs
in the literature such as GCN (Kipf & Welling, 2017), GraphSAGE (Hamilton et al., 2017), GAT
(Velickovic et al., 2017) and GIN (XU et al., 2019) as special cases. Table 8 presents the local
aggregation schemes used by these existing GNN models. They differ from each other w.r.t. the way
of aggregating featUre vectors in a neighborhood and how they are combined with the cUrrent vertex’s
featUre itself, i.e., sUmmation or concatenation. Here, αvu is an attention coefficient captUring the
importance of a neighbor in GAT, is a learnable or fixed scalar parameter Used in GIN, W is a
learnable weight matrix and σ is a non-linear activation fUnction, sUch as ReLU.
Note that, as defined in EqUation 3, m(at) and m(vt) refer to the messages aggregated by AGGRE-
GATEN(∙) and Aggregate1 (∙), respectively.
GNN Model AGGREGATEN (∙)	Aggregate1 (∙)	COMBINe(∙)
GCN
GraphSAGE
GAT
GIN
P ∈N(P (∈NP ∈N(
vv
v
P ∈N(
u
v
W (t) h(t) 	U		W (t)hVt)	σ(SUM(m(Vt) , m(at)))
√∣N (u)||N (V) |	√∣N (v)l∣N (V)I	
hut Wπ	h(vt)	σ(W(t) ∙ CONCAT(mVt), m!t)))
αvuW (t)h(ut)	αVV W (t)h(Vt)	σ(SUM(m(Vt) , m(at)))
h(ut)	(1 + )h(Vt)	MLPθ (SUM(m(Vt), m(at)))
Table 8: Comparison of the aggregation schemes Used in existing message-passing GNNs
Complexity Analysis
Table 9 sUmmarizes the time and space complexities of several popUlar message-passing GNNs and
GraphSNN, where n and m are the nUmbers of vertices and edges in a graph, respectively, k refers to
the nUmber of layers, f and d are the dimensions of inpUt and oUtpUt featUre vectors, respectively, a
is the nUmber of attention heads Used in GAT, and s is the nUmber of neighbors sampled for each
node at each layer in GraphSAGE.
GNN Model	Time Complexity	Memory Complexity
GCN (Kipf & Welling, 2017)	O(kmfd)	O(m)
GIN (XU et al., 2019)	O(kmfd)	O(m)
GAT (Velickovic et al., 2017)	O(k(anfd + amd))	O(n2)
GraphSAGE (Hamilton et al., 2017)	O(Snfd)	O(n)
GraphSNN (oUrs)	O(kmfd)	O(m)
Table 9: Time and space complexities of message-passing GNNs and GraphSNN.
Formulation of GraphGNNM
For each of these message-passing GNNs, denoted as M, we constrUct a variant GraphSNNM by
replacing its existing aggregation scheme by oUr aggregation scheme with strUctUral coefficients as
formUlated in Eq. 5. These variants are Used in oUr experiments for node classification benchmark
tasks (see Section 5.1) in order to evalUate how oUr aggregation scheme with strUctUral coefficients
can improve performance, compared with their standard message-passing aggregation schemes.
Below are the details of these variants.
GCN and GraphSNNGCN
14
Published as a conference paper at ICLR 2022
Graph Convolutional Network (GCN) (Kipf & Welling, 2017) applies a normalized mean aggregation
to combine the feature vector of a node v with the feature vectors in its neighborhood N (v):
h(t+1) = σ( LAM)	+ X 「A) hU ∖
V	vp∣N(v)l∣N(v) I UMv)} PN(V) l∣N(u) V
(6)
，|N(u)||N(V) ∣ is a normalization constant for the edge (v, u), which originates from the normalized
adjacency matrix D-1/2AD-1/2. W(t) is a trainable weight matrix and σ is a non-linear activa-
tion function such as ReLU. We generalise GCN to a model under the GMP framework, namely
GraphSNNGCN, to improve the expressive power of GCN. We first construct a normalized structural
coefficient matrix A. Formally, each neural layer of GraPhSNNGCN may then be expressed as:
hvt+1,=σSu.X V)AvU+1)q≤t+u∈X JAvU+1)qsy.⑺
GraphSAGE and GraphSNNGraphSAGE
GraphSAGE (Hamilton et al., 2017) learns aggregation functions to induce new node feature vectors
by sampling and aggregating features from a node’s local neighborhood. GraphSAGE has considered
three different aggregation functions such as mean aggregator, LSTM aggregator and pooling aggre-
gator. In our work, we mainly focus on the mean aggregator that, for each vertex V, takes the mean of
the feature vectors of the nodes in its neighborhood and concatenates it with the feature vector of V as
shown below:
h(vt+1) = σ W(t)
• Concat(INVT X hUt),hvt))),
V U∈N (v)
(8)
where W(t) is a learnable weight matrix, and σ represents a non-linear activation function. We
also generalise GraphSNN to a model under the GMP framework, namely GraphSNNGraphSAGE .
This model first takes a mean aggregation of the feature vectors in the neighborhoodN(V) and then
concatenates it with the feature vector of V itself in the following manner:
h(vt+1) = σ W(t)
• CONCAT ( I ” ∖∣
IN (V)I
Σ
U∈N (v)
(AvU + ι)hUt),γ(t)( X AvU + ι)hvt))). (9)
U∈N (v)
GAT and GraphSNNGAT
Graph Attention Network (GAT) (VeIiCkOviC et al., 2017) linearly transforms the input feature
vectors and performs a weighted sum of the feature vectors for vertices in a neighborhood after the
transformation. GAT computes attention weights αv(tU) using an attention mechanism and aggregates
the feature vectors in a neighborhood as follows:
h(vt+1) =σ X	α(vtU)W(t)h(Ut) ,
(v,U)∈E
(10)
where W(t) is a trainable weight matrix and σ represents a non-linear activation function. We
generalise GAT to a model, called GraphSNNGAT , in the GMP framework. Firstly, we aggregate
the feature vectors based on structural coefficients in our aggregation scheme, i.e., we compute
γ(t)( X AUz + 1)
z∈N (U)
q
N(U) I∣N(u)I
+ zXU)(AUz + 1) q≡≡	(II)
and
γ(t)( X Avz, + 1)	hv 〜+ X (Avz0 + 1) J .(12)
z0∈N (v)	,N(v)∣N(v)I	z0∈N (v)	,∣N(ζ0)I∣N(v)I
15
Published as a conference paper at ICLR 2022
We then construct attention coefficients α(vtu) on these aggregated feature vectors as follows:
exp(LeakyReLU (aτ [W (t)hVt)∣∣W (t) hUt) ]))
α(t)	------------------------------------------------
""	Pz∈N(V) exp(LeakyReLU(aτ[W(t)hVt)∣∣W(t)hZt)]))
(13)
where || represents the concatenation, W(t) is a learnabe weight matrix and a is a learnable weight
vector. After that, we aggregate the neighborhood features as follows using attention coefficients.
hVt+1)=σ( X	ɑVtu)W(t)hUt)),
(v,u)∈E
(14)
where W(t) is a learnable weight matrix, and σ represents a non-linear activation function. We use
multi-head attention as stated in the original work VeliCkovic et al. (2017).
GIN and GraphSNNGIN
Graph Isomorphism Network (GIN) (Xu et al., 2019) takes the sum aggregation over a neighborhood,
followed by a 2-layer MLP. The (t+1) is a learnable parameter or fixed scalar. Each neural layer is
expressed as:
h(vt+1) = MLP(t+1) ((1 + (t+1))h(vt) + X h(ut)).	(15)
u∈N (v)
Here, we consider one of GIN variants employed in the original paper, where the learnable parameter
= 0, and generalise it to GraphSNNGIN as defined bwlow:
hVt+1) = MLPU) 1)( X Avu + ι)hVt) + X (Avu + ι)hUt)).	(16)
u∈N (v )	u∈N (v)
B.	Experiments
Datasets
Table 10 contains the statistics for the five datasets used in our experiments for node classification in Section 5.1.					
Dataset	Type	#Nodes	#Edges	#Classes	#Features
Cora	Citation network	-^2,708	5,429	7	1,433
Citeseer	Citation network	3,327	4,732	6	3,703
Pubmed	Citation network	19,717	44,338	3	500
NELL	Knowledge graph	65,755	266,144	210	5,414
ogbn-arxiv	Citation network	169,343	1,166,243	40	128
Table 10: Statistics for node classification datasets.
Table 11 below contains the statistics for the datasets used in our experiments on small graph
classification in Section 5.2, as well as the datasets used in an additional experiment for graph
classification following the data splits and experimental setup in (Errica et al., 2020). The results of
this additional experiment are reported under Section “Graph Classification using Setup (Errica et al.,
2020)” in Appendix B.
Table 12 contains the statistics for the five large graph datasets from from Open Graph Benchmark
(OGB) Hu et al. (2020), used in our experiments for large graph classification in Section 5.2.
Experimental Setup on Small Graphs
Previously, several experimental setups have been considered for evaluating graph classification on
small graphs in TUD benchmark datasets (https://chrsmrrs.github.io/datasets/).
All the baseline methods in our paper use the 10-fold cross validation technique. However, they differ
in how they split training/validation/testing data and how they report the final results in terms of
classification accuracy. Below, we discuss the details of their experimental setups.
16
Published as a conference paper at ICLR 2022
Dataset	#Graphs	Avg # Nodes	Avg # Edges	#Classes
MUTAG	188	17:93	19.79	2
PTC-MR	344	14.29	14.69	2
BZR	405	35.75	38.36	2
COX2	467	41.22	43.45	2
ENZYMES	600	32.63	64.14	6
IMDB-B	1000	19.77	96.53	2
PROTEINS	1113	39.06	72.82	2
D&D	1178	284.32	715.66	2
NCI1	4110	29.87	32.30	2
RDT-M5K	5000	508.52	594.87	5
COLLAB	5000	74.49	2457.78	3
Table 11: Statistics for small graph classification datasets.
Dataset	#Graphs	Avg # Nodes	Avg # Edges	#Tasks	Task Type
ogbg-molmolhiv	41,127	25.5	27.5	1	Binary classification
ogbg-moltox21	7,831	18.6	19.3	12	Binary classification
ogbg-moltoxcast	8,576	18.8	19.3	617	Binary classification
ogbg-molpcba	437,929	26.0	28.1	128	Binary classification
ogbg-ppa	158,100	243.4	2,266.1	1	Multi-class classification
Table 12: Statistics for large graph classification dataset (OGB graph datasets).
•	CapsGNN (Xinyi & Chen, 2018) splits the datasets into 80 % for training, 10 % for
validation, and 10 % for testing. The training is stopped when the performance on the
validation set goes to the highest. Then they obtain the test set accuracy that corresponds to
the epoch with the highest validation accuracy in each fold. The final results are reported by
computing the mean accuracy and standard deviation over 10 folds.
•	DGCNN Zhang et al. (2018a) splits the datasets into 90 % for training and 10 % for testing.
They obtain the test accuracy of the last epoch in each fold. They report the final results by
computing the mean accuracy and standard deviation on the test accuracy over 10 folds.
•	GIN and GraphSAGE (Xu et al., 2019) split the datasets into 90 % for training and 10 % for
testing. They average the test accuracy on 10 folds and select the epoch with the highest
averaged accuracy. Then they report the final results by computing the mean accuracy and
standard deviation based on the selected epoch.
•	FGW (Titouan et al., 2019) splits the datasets into 90 % for training and 10 % for testing.
Then, they use the nested cross validation technique on the same folds, and repeat the
process 10 times. They report the final results by computing the mean accuracy and standard
deviation.
•	The other baseline methods split the datasets into 90 % for training and 10 % for testing,
and repeat their experiment 10 times. Then they report the final results by computing the
mean accuracy and standard deviation.
In our work, we split the datasets into 90 % for training and 10 % for testing. We obtain the best
validation accuracy on each fold. Then we report the final results by computing the mean accuracy
and standard deviation over 10 folds1.
Node Classification using Random Splits
Following the work Pei et al. (2020), we randomly split graph nodes into 60%, 20% and 20% for
training, validation and testing, respectively. The other hyperparameter settings are the same as
in Section 5.1. Table 13 shows the results. We see that our models consistently outperform all of
the baseline methods on all benchmark datasets. Specifically, GraphSNGCN improves upon GCN
by a margin of 1.5%, 1.7%, 1.6% and 2.4% on Cora, Citeseer, Pubmed and NELL, respectively.
1The implementation can be found at: https://github.com/wokas36/GraphSNN
17
Published as a conference paper at ICLR 2022
GraphSNGAT improves upon GAT by 1.3%, 1.6% and 2.0% on Cora, Citeseer and Pubmed, respec-
tively. GraphSNGIN improves upon GIN by 3.8%, 1.7%, 1.8% and 1.6% on Cora, Citeseer, Pubmed
and NELL, respectively. GraphSNGraphSAGE improves upon GraphSAGE by 1.3%, 1.7%, 1.1% and
2.3% on Cora, Citeseer, Pubmed and NELL, respectively.
Method	Cora	Citeseer	Pubmed	NELL
GCN	85.7 ± 1.6	73.6 ± 1.0	88.1 ± 1.2	72.2 ± 5.6
GraphSNNGCN	87.2 ± 1.5	75.3 ± 1.3	89.7 ± 1.7	74.6 ± 6.3
GAT	86.3 ± 0.3	74.3 ± 0.3	87.6 ± 0.1	-
GraphSNNGAT	87.6 ± 0.9	75.9 ± 0.8	89.6 ± 0.6	-
GIN	82.5 ± 0.8	70.8 ± 1.9	85.0 ± 1.5	66.7 ± 3.3
GraphSNNGIN	86.3 ± 0.7	72.5 ± 1.5	86.8 ± 1.2	68.3 ± 3.7
GraphSAGE	86.8 ± 1.9	74.2 ± 1.8	88.3 ± 1.1	69.4 ± 4.3
GraphSNNGraphSAGE	88.1 ± 1.5	75.9 ± 1.3	89.4 ± 2.4	71.7 ± 4.5
Table 13: Classification accuracy (%) averaged over 10 random splits on node classification.
Graph Classification using Setup (Errica et al., 2020)
Following the data splits and experiment setup introduced in (Errica et al., 2020), we further evaluate
our method. The experimental setup in (Errica et al., 2020) provides a fair performance comparison
process on GNN methods. The evaluation process has two different phases: (1) model selection on
the validation set, (2) model assessment on the test set. More specifically, they first split the datasets
into 90 % for training and 10 % for testing. Then the entire training set is further split into 90% of
training and 10% of validation. They apply the inner hold-out method to select the best model based
on validation accuracy. After selecting the best model, they train the model three times on the entire
training set with early stopping.
We have conducted experiments on four bioinformatics datasets (NCI1, PROTEINS, ENZYMES and
D&D) and three social network datasets (COLLAB, IMDB-B and REDDIT-5k) with node features.
The results of the baseline, DGCNN and GIN are taken from the paper (Errica et al., 2020). Note that
the final results of DGCNN and GIN from the paper (Errica et al., 2020) are reported by computing
the mean accuracy and standard deviation on the test set in these three runs, which are different from
the original papers of DGCNN and GIN. Table 14 shows the results.
Method	NCI1	PROTEINS	ENZYMES	D&D	COLLAB	IMDB-B	REDDIT-5k
Baseline	69.8±2.2	75.8 ± 3.7	65.2±6.4	78.4 ± 4.5	70.2±1.5	70.8±5.0	52.2±1.5
DGCNN	76.4±1.7	72.9±3.5	38.9±5.7	76.6±4.3	71.2±1.9	69.2±3.0	49.2±1.2
GIN	80.0±1.4	73.3±4.0	59.6±4.5	75.3±2.9	75.6±2.3	71.2±3.9	56.1±1.7
GraphSNN	81.6 ± 2.8	74.5 ± 3.5	61.7 ± 3.4	77.1 ± 3.3	77.0 ± 3.1	72.3 ± 3.6	57.1 ± 3.1
Table 14: Classification accuracy (%) averaged over 10 runs on graph classification.
Graph Classification on OGB Graph Datasets
Table 15 shows the results for the running time of the prepocessing step in our method GraphSNN
for large graph datasets (averaged over 5 runs). Note that the preprocessing step can be parallellized
efficiently at the node level. The CPU time shows the total preprocessing time of a dataset in which
each node is preprocessed sequentially, and the CPU time per node shows the average preprocessing
time per node.
Oversmoothing Analysis
We have also conducted further experiments to analyze the effectiveness of our method in alleviating
the over-smoothing issue. We compare GIN (i.e., a spatial GNN), DFNets (Wijesinghe & Wang,
2019) (i.e., a spectral GNN), GraphSNNGIN and GraphSNNGCN. For a fair comparison, we remove
the dense-net architecture of DFNets and use the same hyperparameters as in the original paper. We
18
Published as a conference paper at ICLR 2022
Dataset	CPU time (seconds)	CPU time per node (milliseconds)
ogbg-molhiv	6697	0.06383
ogbg-moltox21	79.37	0.54565
ogbg-moltoxcast	380.84	2.36417
ogbg-ppa	820.12	4.71235
Table 15: Running time of the prepocessing step for large graph datasets averaged over 5 runs.
evaluate all models over the cora dataset using the standard splits. The classification accuracy is
averaged over 10 runs on node-classification.
#Layers	GIN	GraPhSNNGIN	DFNet	GraPhSNNGCN
1	73.3±1.5	76.1±1.6	80.5±0.6	80.1±0.8
2	77.6±1.3	79.2±1.7	81.9±0.5	83.1±1.8
3	75.2±1.7	78.5±1.3	82.6±0.3	82.0±0.8
4	48.6±2.1	77.2±2.3	80.7±0.6	80.1±0.7
5	40.3±1.9	75.9±2.1	75.6±0.3	79.1±1.2
6	36.1±2.3	73.3±1.8	65.3±1.3	76.5±1.3
7	27.5±2.1	71.9±1.5	60.9±1.5	76.3±1.3
8	20.3±1.8	69.3±2.2	53.6±1.3	75.7±1.2
Table 16: Oversmoothing analysis of GIN and spectral GNN (DFNet) on cora dataset.
GraphSNN can alleviate over-smoothing is because structural coefficients capture structural connec-
tivity between a target vertex and its neighbors. Thus, a neighbor whose structural connectivity is
weak would pass little message to the target vertex, whereas a neighbor whose structural connectivity
is strong would pass strong message to the target vertex.
Figure 4 shows the results of GCN and GraphSNNGCN on the datasets Cora, Citeseer and Pubmed,
in terms of classification accuracy averaged over 10 runs in the setting of standard splits.
Figure 4: Oversmoothing analysis w.r.t. the model depth for node classification.
Ablation S tudy with Augmented Node Features
We consider an experimental evaluation setup called BL, which serves as the baseline for all experi-
ments in this ablation study. In the setting of BL, the AGGREGATEI in GraphSNN is set to 1. Then,
different variants of BL consider different local substructure counts as additional node features. This
allows us to analyse what types of local substructures our proposed architecture can distinguish.
There are five variants of BL being considered in the ablation study:
(1)	BLSC : Setting AGGREGATIONI of GraphSNN to 1 and keeping structural coefficients for
neighbors.
(2)	BLcNliFque: Setting AGGREGATIONI of GraphSNN to 1, removing structural coefficients
for neighbors, and adding additional node features (triangle and 4-clique counts) into the
original feature vectors.
19
Published as a conference paper at ICLR 2022
Method	GSN-v	clique BLNF	BLSC	clique BLSC+NF	GraphSNN
MUTAG	92.20±7.5	90.21±2.3	94.06±2.4	95.16±2.5	94.70±1.9
PTC-MR	67.40±5.7	67.13±2.9	70.18±3.1	71.04±3.1	70.58±3.1
PROTEINS	74.59±5.0	76.42±2.6	78.05±2.3	78.66±2.1	78.42±2.7
BZR	-	86.82±3.1	90.67±3.1	91.98±3.2	91.12±3.0
IMDB-B	76.80±2.0	77.00±3.1	77.23±2.8	78.53±2.9	78.01±2.8
Table 17: Analysis the effects of our structural coefficients with substructure counts, i.e, triangle and
4-clique counts. Classification accuracy (%) averaged over 10 runs on graph classification.
Method	ID-GNN	cycle BLNF	BLSC	cycle BLSC+NF	GraphSNN
MUTAG	96.50±3.2	91.36±2.1	94.06±2.4	96.61±2.3	94.70±1.9
PTC-MR	61.90±5.4	67.57±3.3	70.18±3.1	71.76±3.2	70.58±3.1
PROTEINS	78.00±3.5	77.26±2.5	78.05±2.3	78.95±2.5	78.42±2.7
BZR	86.40±3.0	86.83±3.3	90.67±3.1	91.75±3.4	91.12±3.0
IMDB-B	-	76.36±2.6	77.23±2.8	78.58±2.4	78.01±2.8
Table 18: Analysis the effects of our structural coefficients with substructure counts, i.e, cycle counts.
Classification accuracy (%) averaged over 10 runs on graph classification.
(3)	BLcSlCiq+ueN F: Setting AGGREGATIONI of GraphSNN to 1, keeping structural coefficients
for neighbors, and adding additional node features (triangle and 4-clique counts) into the
original feature vectors.
(4)	BLcNyFcle: Setting AGGREGATIONI of GraphSNN to 1, removing structural coefficients
for neighbors, and adding additional node features (cycle counts) into the original feature
vectors.
(5)	BLcSyCcl+eN F: Setting AGGREGATIONI of GraphSNN to 1, keeping structural coefficients
for neighbors, and adding additional node features (cycle counts) into the original feature
vectors.
We compare GraphSNN with GSN-v (Bouritsas et al., 2020), BLcNliFque, BLSC, and BLcSlCiq+ueNF to
analyze how our proposed architecture relates to the models with triangle and 4 clique counts as
additional node features. Similarly, we compare GraphSNN with ID-GNNs (You et al., 2021),
BLcNyFcle, BLSC, and BLcSyCcl+eNF to analyze how our proposed architecture relates to the models
with cycle counts as additional node features. We concatenate the counts of cycles with length 1
to 4 starting and ending at the given source node with its original feature vector as in (You et al.,
2021). Table 17 and Table 18 show the experimental results. As AGGREGATEI is set to 1 in
the setting of BL, the performance gap between BLNF and BLSC+NF reflects the effectiveness of
structural coefficients on enhancing relational inference between a target vertex and its neighbors. The
performance gap between BLSC and GraphSNN above shows the effectiveness of AGGREGATEI
in our proposed model GraphSNN. Furthermore, BLSC+NF consistently performs best since we
incorporate both extra node features and structural coefficients into the feature aggregation. There is
a small performance gap between BLSC+NF and GraphSNN due to augmented node features that
can capture additional structural information that cannot be captured using structural coefficients.
C. Proofs for Lemmas and Theorems
Proof for Theorem 1
Theorem 1.	The following statements are true: (a) If Si 'subgraph Sj, then Si 'overlap Sj; but not
vice versa; (b) If Si 'overlap Sj, then Si 'subtree Sj; but not vice versa.
Proof. In the following, we prove the statements in this theorem one by one.
For Statement (a), by Si 'subgraph Sj and Definition 1, we know that there exists a bijective mapping
g0 : N(i) → N(j) such that for the vertex i and any vertex v0 ∈ N (i), i and v0 are adjacent in Si
iff j = g(i) and u0 = g(v0) are adjacent in Sj, and hi = hj and hv0 = hu0, where g is a bijective
mapping between Si and Sj as defined by Definition 1. Then for each pair of overlap subgraphs Siv0
20
Published as a conference paper at ICLR 2022
and Sju0, we can further extend g0 along g on Siv0 and Sju0. That is, g0 (v) = u iff g(v) = u. If v in
Siv0, by the definition of overlap subgraph, v must either be i or a neighbor of i. Hence u = g0 (v)
in this case must be either j or a neighbor of j. By the definition of g and the fact that g0 (v) = u
iff g(v) = u, we know that for any two vertices v1 and v2 in Siv0, they are adjacent in Siv0 iff their
corresponding vertices g0(v1) and g0(v2) are adjacent in Sju0 and their corresponding feature vectors
are indistinguishable, i.e, Siv0 'subgraph Sju0 for any v0 ∈ N(i) and g(v0) = u0. Conversely, if
Si 'overlap Sj , then it is possible that Si 6'subgraph Sj as shown by the two graphs in Figure 2(a).
For Statement (b), if Si 'overlap Sj , then to prove Si 'subtree Sj we need to show that there exists
a bijective mapping g : N(i) → N(j) such that g(i) = j and, for any v0 ∈ N(i) and g(v0) = u0, the
feature vectors of v0 and u0 are indistinguishable, i.e., hv0 = hu0. By Def. 2, we can find a bijective
mapping g0 : N (i) → N(j) such that g0(i) = j and, for any v0 ∈ N(i) and g0(v0) = u0, Siv0 and
Sju0 are subgraph-isomorphic. This implies that g0 cannot distinguish the feature vectors of v0 and
U for any V ∈ NN(i) and g(v0) = u0. Similarly, the converse does not necessarily hold and one
counterexample is the set of graphs as shown in Figure 2(b) which are subtree-isomorphic but not
overlap-isomorphic.	□
Proof for Theorem 2
Theorem 2.	Let M be a GNN. M is as powerful as 1-WL in distinguishing non-isomorphic graphs
if M has a sufficient number of layers and each layer can map any Si and Sj in S into two different
embeddings (i.e., ζ(Si) 6= ζ(Sj)) if and only if Si 6'subtree Sj.
Proof. We first show that, for any two graphs G1 and G2, if they can be distinguished by 1-WL,
then they must be distinguishable by such a GNN M as well. Suppose that 1-WL takes k iterations
to distinguish G1 and G2, i.e., 1-WL yields the same multiset of node labels on G1 and G2 in the
iterations from 0 to k-1, but two different multisets of node labels on G1 and G2 in the k-th iteration.
To derive a contradiction, we assume that a GNN M that satisfies the above two conditions cannot
distinguish G1 and G2 in the iterations from 0 to k. Since 1-WL can distinguish G1 and G2 in the
k-th iteration, it means that there must exist two neighborhood subgraphs, say Si and Sj , which
correspond to two different multisets of node labels on G1 and G2 at the k-th iteration. These two
different multisets of node labels correspond to two different multisets of feature vectors in their
neighborhoods, i.e., {hv|v ∈ N (i) } 6= {hu|u ∈ N(j) }. By Def. 3, we know that Si 6'subtree Sj.
Then this means that ζ(Si) 6= ζ(Sj), which contradicts the assumption that M cannot distinguish G1
and G2 in the iteration k.
Now, we show the other direction that, for any two graphs G1 = (V1, E1) and G2 = (V2, E2), if
they can be distinguished by such a GNN M, then they must be distinguishable by 1-WL. Similarly,
suppose that at the k-th iteration, M maps the neighborhood subgraphs of these two graphs into two
different multisets of node embeddings, i.e.,{Z(Sv)|v ∈ ViR = £Z(Su)∣v ∈ V2⅛. ThiS is means
that we can find at least two different neighborhood subgraphs Si and Sj such that ζ(Si) 6= ζ(Sj).
For such neighborhood subgraphs Si and Sj , we know that Si 6'subtree Sj . Then this means that
Si and Sj correspond to either hi 6= hj or {hv |v ∈ N(i)} 6= {hu|u ∈ N (j)}, which can be
relabeled by 1-WL into two different new labels. Thus, 1-WL can also distinguish such neighborhood
subgraphs, and accordingly distinguish G1 and G2 .
The proof is completed.	□
Proof for Theorem 3
Theorem 3.	Let M be a GNN whose aggregation scheme Φ is defined by Eq. 1-Eq. 3. M is strictly
more expressive than 1-WL in distinguishing non-isomorphic graphs if M has a sufficient number of
layers and also satisfies the following conditions:
(1)	M can distinguish at least two neighborhood subgraphs Si and Sj with Si 'subtree Sj,
Si 6'subgraph Sj and {Aiv0 |v ∈ N(i) } 6= {Aju0 |u ∈ N (j) };
(2)	φ(hvt), £hUt)|u ∈N(v)⅛, {(Avu,hUt))∣u ∈ N(v)⅛^ is injective.
21
Published as a conference paper at ICLR 2022
Proof. We prove this theorem in two steps. First, we prove that a GNN M satisfying the above
conditions can distinguish any two graphs that are distinguishable by 1-WL by contradiction. Assume
that there exist two graphs G1 and G2 which can be distinguished by 1-WL but cannot be distinguished
by M. Further, suppose that 1-WL cannot distinguish these two graphs in the iterations from 0 to k-1,
but can distinguish them in the k-th iteration. Then, there must exist two neighborhood subgraphs
Si and Sj whose neighboring nodes correspond to two different multisets of node labels at the k-th
iteration, i.e., {h(vk) |v ∈ N(i) } 6= {h(uk) |u ∈ N(j) }. By the above condition (2), we know that Φ
is injective. Thus, for Si and Sj , Φ would yield two different feature vectors at the k-th iteration.
This means that M can also distinguish G1 and G2 , which contradicts the assumption. Our proof in
the first step is done. For the second step, we can prove that there exist at least two graphs that can be
distinguished by M but cannot be distinguished by 1-WL. Figure 1 presents two of such graphs. □
Proof for Theorem 4
We consider that, for each vertex in a graph, its node features are from a countable set; similarly, for
each pair of adjacent vertices in a graph, its structural coefficient is also from a countable set. Assume
that H, A and W are countable sets where H is a node feature space, A is a structural coefficient
space, and W = {Aijhi|Aij ∈ A, hi ∈ H}. Let H and W be two multisets containing elements
from H and W, respectively, and |H| = |W|.
Lemma 1. There exists a function f s.t. π(H, W) =	h∈H,w∈W f(h, w) is unique for any distinct
pair of multisets (H, W).
Proof. Since H and W are countable, there must exist two functions ψ1 : H → Nodd mapping h ∈ H
to odd natural numbers and ψ2 : W → Neven mapping w ∈ W to even natural numbers. Further,
for any pair of multisets (H, W), since the cardinality of H and W is bounded, there must exist a
number N ∈ N such that |H| < N and |W | < N. Thus, we can find a prime number P > 2N. Then
we have a mapping f as f(h, w) = P -ψ1 (h) + P -ψ2 (w) such that Ph∈H,w∈W f(h, w) is unique for
each distinct pair of (H, W).	□
Lemma 2. There exists a function f s.t. π0(hv, H, W) = γf(hv, |H|hv) + h∈H,w∈W f(h, w) is
unique for any distinct (hv, H, W), where hv ∈ H, |H |hv ∈ W, and γ can be an irrational number.
Proof. As hv ∈ H and |H|hv ∈ W, we may have f (h, |H|hv) = P-ψ1(hv) + P-ψ1(IHIhv) where
ψ1 : H → Nodd and ψ2 : W → Neven as defined in the proof for Lemma 1. Let (hv1, H1, W1) and
(hv2 , H2 , W2 ) be two different tuples. Then, there are two cases:
(1)	When	hv1	=	hv2	but	(H1,	W1)	6=	(H2, W2),	by Lemma 1, we know
that Ph∈H1,w∈W1f(h,w) 6= Ph∈H2,w∈W2 f(h, w). Thus, π0(hv1, H1, W1) 6=
π0(hv2, H2, W2).
(2)	When hv1 6= hv2, we prove π0(hv1, H1, W1) 6= π0(hv2, H2, W2) by contradiction. Assume
that π0(hv1, H1, W1) = π0(hv2, H2, W2). Then, we have:
γf(hv1, |H1|hv1) + X	f(h,w) = γf (hv2, |H2|hv2) + X	f(h, w).
h∈H1 ,w∈W1	h∈H2 ,w∈W2
This gives us the following equation:
γf(hv1, |H1|hv1)-f(hv2, |H2|hv2) = X	f(h,w)- X	f(h, w).
h∈H2,w∈W2	h∈H1,w∈W1
When γ is an irrational number, L.H.S. of the above equation is irrational but R.H.S. is
rational. There is a contradiction. Thus, π0(hv1, H1, W1) 6= π0(hv2, H2, W2).
□
Based on Lemma 1 and Lemma 2, we can prove the following theorem.
22
Published as a conference paper at ICLR 2022
Theorem 4.	GraphSNN is more expressive than 1-WL in testing non-isomorphic graphs.
Proof. We prove this theorem by showing that GraphSNN is a GNN satisfying the conditions stated
in Theorem 3. For the first condition, consider the two graphs shown in Figure 1. GraphSNN can
distinguish these two neighborhood subgraphs Si and Sj with {Aiv0 |v0 ∈ N(i) } 6= {Aju0 |u0 ∈
N(j) }. For the second condition, by Lemmas 1 and 2 as well as the fact that MLP as a universal
approximator (Xu et al., 2019) can be used to model and learn the functions f and g, we know that
GraPhSNN also satisfies this condition.	□
23