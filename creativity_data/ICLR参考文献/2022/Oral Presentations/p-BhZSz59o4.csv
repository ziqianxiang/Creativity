论文题目,会议名称
" Asano, Christian Rupprecht, and Andrea Vedaldi", Self-labelling via simultaneous clusteringand representation learning
 Learning representations by maximiz-ing mutual information across views, In Advances in Neural Information Processing Systems
 UniLMv2: Pseudo-masked language modelsfor unified language model pre-training, In Proceedings of the 37th International Conferenceon Machine Learning
Unsupervised learning of visual features by contrasting cluster assignments, In Advances in NeuralInformation Processing Systems
 Emerging properties in self-supervised vision transformers, arXiv preprintarXiv:2104
Generative pretraining from pixels, In Hal Daume III and Aarti Singh (eds
 Exploring simple siamese representation learning, preprintarXiv:2011
 Improved baselines with momentumcontrastive learning, preprint arXiv:2003
 An empirical study of training self-supervised visiontransformers, ArXiv
 BERT: pre-training of deepbidirectional transformers for language understanding, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 An imageis worth 16x16 words: Transformers for image recognition at scale, preprint arXiv:2010
 Bootstrap your own latent:A new approach to self-supervised learning, In NeurIPS
 Momentum contrast forunsupervised visual representation learning, In CVPR
 Learning deep representations by mutual information estimationand maximization, In International Conference on Learning Representations
 Unsupervised representation learning by predicting imagerotations, In International Conference on Learning Representations (ICLR)
 Prototypical contrastive learning of unsu-pervised representations, In International Conference on Learning Representations
 Efficient trainingof visual transformers with small datasets, In Thirty-Fifth Conference on Neural InformationProcessing Systems
 Decoupled weight decay regularization, In International Confer-ence on Learning Representations
 Unsupervised learning of visual representations by solving jigsawpuzzles, In European conference on computer vision
 Representation learning with contrastive predictivecoding, preprint arXiv:1807
Intermediate-task transfer learningwith pretrained language models: When and why does it work?,In Proceedings of the 58thAnnual Meeting of the Association for Computational Linguistics
 Generating diverse high-fidelity images withVQ-VAE-2, In Advances in Neural Information Processing Systems
 Imagenetlarge scale visual recognition challenge, IJCV
 Neural machine translation of rare words withsubword units, In Proceedings of the 54th Annual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers)
 Goingdeeper with image transformers, arXiv preprint arXiv:2103
 Selfie: Self-supervised pretraining for imageembedding, arXiv preprint arXiv:1906
 Neural discrete representation learn-ing, In Proceedings of the 31st International Conference on Neural Information ProcessingSystems
 Unsupervised feature learning vianon-parametric instance discrimination, In CVPR
 Unified perceptual parsing forscene understanding, In ECCV
 Self-supervisedlearning with swin transformers, arXiv preprint arXiv:2105
 Scaling vision transformers,arXiv preprint arXiv:2106
 Colorful image colorization, In ECCV
