论文题目,会议名称
 Muppet: Massive multi-task representations with pre-finetuning, arXiv preprintarXiv:2101
 Massively multilingual neural machinetranslation in the wild: Findings and challenges, arXiv preprint arXiv:1907
 Program synthesis with largelanguage models, arXiv preprint arXiv:2108
 Domain adaptation via pseudo in-domain dataselection, In Proceedings of the 2011 Conference on Empirical Methods in Natural LanguageProcessing
" Forcada, Amir Kamran, Faheem Kirefu, Philipp Koehn, Sergio Ortiz Rojas, LeopoldoPla Sempere, Gema Ramirez-Sgnchez, Elsa Sardas, Marek Strelec, Brian Thompson, WilliamWaites, Dion Wiggins, and JaUme Zaragoza", ParaCrawl: Web-scale acqUisition of parallel corpora
" Climbing towards NLU: On meaning, form, and Under-standing in the age of data", In Proceedings of the 58th Annual Meeting of the Association forComputational Linguistics
" Onthe dangers of stochastic parrots: Can langUage models be too big? In Proceedings of the2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT ’21, pp", 610-623
 The Fifth PASCAL RecognizingTextUal Entailment Challenge, In TAC
 PIQA: ReasoningaboUt physical commonsense in natUral langUage, In Thirty-Fourth AAAI Conference on ArtificialIntelligence
" Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, DoraDemszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, S Ermon, J Etchemendy, KawinEthayarajh, L Fei-Fei, Chelsea Finn, Trevor Gale, Lauren E Gillespie, Karan Goel, Noah D Good-man, S Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E", Ho
" Bowman, Gabor Angeli, Christopher Potts, and Christopher D", Manning
 Language models are few-shot learners, In Advances in Neural Information Pro-cessing Systems
 Description based text classification withreinforcement learning, In Proceedings of the International Conference on Machine Learning
 Evaluating large language models trainedon code, arXiv preprint arXiv:2107
 QuAC: Question answering in context, In Proceedings of the 2018 Conferenceon Empirical Methods in Natural Language Processing
 BoolQ: Exploring the surprising difficulty of natural yes/no questions, In Proceedingsof the 2019 Conference of the North American Chapter of the Association for ComputationalLinguistics: Human Language Technologies
" Kevin Clark, Minh-Thang Luong, Urvashi Khandelwal, Christopher D Manning, and Quoc V Le",BAM! born-again multi-task networks for natural language understanding
" Think you have solved question answering? Try ARC, the AI2 reasoning challenge",arXiv preprint arXiv:1803
 Natural language processing (almost) from scratch, Journal of Machine Learning Research
 Hybrid emoji-based masked language models for zero-shot abusive language detection, In Findings of theAssociation for Computational Linguistics: EMNLP 2020
 The PASCAL Recognising Textual Entailmentchallenge, In Proceedings of the First International Conference on Machine Learning Challenges:Evaluating Predictive Uncertainty Visual Object Classification
 Semi-supervised sequence learning, In Proceedings of the Confer-ence on Neural Information Processing Systems
 The CommitmentBank:Investigating projection in naturally occurring discourse, In Proceedings of Sinn und Bedeutung
 BERT: Pre-training ofdeep bidirectional transformers for language understanding, In Proceedings of the 2019 Con-ference of the North American Chapter of the Association for Computational Linguistics: Hu-man Language Technologies
 Automatically constructing a corpus of sentential paraphrases,In Proceedings of the Third International Workshop on Paraphrasing (IWP2005)
 GLaM: Efficient scaling of languagemodels with mixture-of-experts, arXiv preprint arXiv:2112
DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs, InProceedings of the 2019 Conference of the North American Chapter of the Association for Com-putational Linguistics: Human Language Technologies
 Edinburgh’s phrase-basedmachine translation systems for WMT-14, In Proceedings of the Ninth Workshop on StatisticalMachine Translation
 Semantic noise matters for neural naturallanguage generation, In Proceedings of the 12th International Conference on Natural LanguageGeneration
 Understanding back-translation at scale,In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing
 The Turking Test: Can language models understand instructions? arXivpreprint arXiv:2010,11982
 Multi-neWs: A large-scalemulti-document summarization dataset and abstractive hierarchical model, In Proceedings of the57th Annual Meeting of the Association for Computational Linguistics
AI, Yelp Sentiment Classification Dataset
 Model-agnostic meta-learning for fast adaptation ofdeep networks, In Proceedings of the International Conference on Machine Learning (ICML)
 Making pre-trained language models better few-shotlearners, In Proceedings of the 59th Annual Meeting of the Association for Computational Linguis-tics and the 11th International Joint Conference on Natural Language Processing (Volume 1: LongPapers)
"Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini", The WebNLGchallenge: Generating text from RDF data
" The GEM benchmark: Natural language generation, its evaluation and metrics",In Proceedings of the 1st Workshop on Natural Language Generation
 The third PASCAL recognizingtextual entailment challenge, In Proceedings of the ACL-PASCAL Workshop on Textual Entailmentand Paraphrasing
 SAMSum corpus: A human-annotated dialogue dataset for abstractive summarization, In Proceedings of the 2nd Workshopon New Frontiers in Summarization
 Twitter sentiment classification using distant supervision,CS224N project report
pdf,Dan Goldwasser and Dan Roth
"Max Grusky, Mor Naaman, and Yoav Artzi", Newsroom: A dataset of 1
 The Second PASCAL Recognising Textual Entailment Challenge, In Proceedingsof the Second PASCAL Challenges Workshop on Recognising Textual Entailment
 Question-answer driven semantic role labeling:Using natural language to annotate natural language, In Proceedings of the 2015 Conferenceon Empirical Methods in Natural Language Processing
 Surface formcompetition: Why the highest probability answer isn’t always right, In Proceedings of the2021 Conference on Empirical Methods in Natural Language Processing
 To-ward semantics-based answer pinpointing, In Proceedings of the First International Confer-ence on Human Language Technology Research
 Universal language model fine-tuning for text classification, InProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume1: Long Papers)
 Cosmos QA: Machine readingcomprehension with contextual commonsense reasoning, In Proceedings of the 2019 Conferenceon Empirical Methods in Natural Language Processing and the 9th International Joint Conferenceon Natural Language Processing (EMNLP-IJCNLP)
 Google’s multilingual neural machine translation system: Enabling zero-shot transla-tion, Transactions of the Association for Computational Linguistics
 TriviaQA: A large scale distantlysupervised challenge dataset for reading comprehension, In Proceedings of the 55th AnnualMeeting of the Association for Computational Linguistics (Volume 1: Long Papers)
 Lookingbeyond the surface: A challenge set for reading comprehension over multiple sentences, InProceedings of the 2018 Conference of the North American Chapter of the Association for Compu-tational Linguistics: Human Language Technologies
 UNIFIEDQA: Crossing format boundaries with a single QA system, InFindings of the Association for Computational Linguistics: EMNLP 2020
 From group to individuallabels using deep features, Proceedings of the 21th ACM SIGKDD International Conferenceon Knowledge Discovery and Data Mining
 Sentencepiece: A simple and language independent subwordtokenizer and detokenizer for neural text processing, In Eduardo Blanco and Wei Lu (eds
 Ask me anything: Dynamic memory networks fornatural language processing, In Proceedings of the International Conference on Machine Learning
 Natural Questions: A benchmark for question answering research, Transactions of theAssociationfor Computational Linguistics
 WikiLingua: A newbenchmark dataset for cross-lingual abstractive summarization, In Findings of the Associ-ation for Computational Linguistics: EMNLP 2020
 Learning to detect unseen objectclasses by between-class attribute transfer, In 2009 IEEE Conference on Computer Vision andPattern Recognition
 From zero to hero: On thelimitations of zero-shot language transfer with multilingual Transformers, In Proceedings of the2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)
 Latent retrieval for weakly supervisedopen domain question answering, In Proceedings of the 57th Annual Meeting of the Associationfor Computational Linguistics
 Gshard: Scaling giant models with conditionalcomputation and automatic sharding, In International Conference on Learning Representations
 The power of scale for parameter-efficient prompttuning, In Proceedings of the Conference on Empirical Methods in Natural Language Processing
 The Winograd Schema Challenge, InThirteenth International Conference on the Principles of Knowledge Representation and Reasoning
 Zero-shot relation extractionvia reading comprehension, In Proceedings of the 21st Conference on Computational NaturalLanguage Learning (CoNLL 2017)
" BART: Denoising sequence-to-sequence pre-trainingfor natural language generation, translation, and comprehension", In Proceedings of the 58thAnnual Meeting of the Association for Computational Linguistics
 Prefix-tuning: Optimizing continuous prompts for generation, InProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)
 A unifiedMRC framework for named entity recognition, In Proceedings of the 58th Annual Meet-ing of the Association for Computational Linguistics
 Learning question classifiers, In COLING 2002: The 19th International Confer-ence on Computational Linguistics
 CommonGen: A constrained text generation challenge for generative commonsensereasoning, In Findings of the Association for Computational Linguistics: EMNLP 2020
Reconstructing capsule networks for zero-shot intent classification, In Proceedings of the 2019Conference on Empirical Methods in Natural Language Processing and the 9th International JointConference on Natural Language Processing (EMNLP-IJCNLP)
" Pre-train,prompt, and predict: A systematic survey of prompting methods in natural language processing",arXiv preprint arXiv:2107
 Multi-task deep neural networks fornatural language understanding, In Proceedings of the 57th Annual Meeting of the Associationfor Computational Linguistics
 Multi-tasksequence to sequence learning, Proceedings of ICLR
 Learning word vectors for sentiment analysis, In Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics: Human Language Technologies
 Programs with common sense, RLE and MIT computation center
 Can a suit of armor conductelectricity? A new dataset for open book question answering, In Proceedings of the 2018Conference on Empirical Methods in Natural Language Processing
 Metaicl: Learning to learnin context, arXiv preprint arXiv:2110
 Natural Instructions:Benchmarking generalization to new tasks from natural language instructions, arXiv preprintarXiv:2104
 A corpus and cloze evaluation for deeper understandingof commonsense stories, In Proceedings of the 2016 Conference of the North American Chapterof the Association for Computational Linguistics: Human Language Technologies
DART: Open-domain structured data record to text generation, In Proceedings of the 2021Conference of the North American Chapter of the Association for Computational Linguistics:Human Language Technologies
 Annotated Gigaword, In Pro-ceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scaleKnowledge Extraction (AKBC-WEKEX)
" Cohen, and Mirella Lapata", Don’t give me the details
 AdversarialNLI: A new benchmark for natural language understanding, In Proceedings of the 58th AnnualMeeting of the Association for Computational Linguistics
" Wainwright, Pamela Mishkin, ChongZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton,Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, andRyan Lowe", Training language models to follow instructions with human feedback
 Deep contextualized word representations, In Proceedings of the 2018Conference of the North American Chapter of the Association for Computational Linguistics:Human Language Technologies
 Improving zero-shot translationwith language-independent constraints, In Proceedings of the Fourth Conference on MachineTranslation (Volume 1: Research Papers)
 WiC: the word-in-context dataset forevaluating context-sensitive meaning representations, In Proceedings of the 2019 Confer-ence of the North American Chapter of the Association for Computational Linguistics: Hu-man Language Technologies
" Massively multilingual ASR: 50 languages, 1 model, 1 billionparameters", arXiv preprint arXiv:2007
 Learning how to ask: Querying LMs with mixtures of soft prompts,In Proceedings of the 2021 Conference of the North American Chapter of the Association forComputational Linguistics: Human Language Technologies (NAACL-HLT)
 Improvinglanguage understanding by generative pre-training, https://blog
Language models are unsupervised multitask learners, OpenAI blog
 Exploring the limits of transfer learning with a unifiedtext-to-text transformer, Journal of Machine Learning Research
 Resolving complex cases of definite pronouns: The Winogradschema challenge, In Proceedings of the 2012 Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational Natural Language Learning
" SQuAD: 100,000+ questions formachine comprehension of text", In Proceedings of the 2016 Conference on Empirical Methods inNatural Language Processing
 Know what you don’t know: Unanswerable questionsfor SQuAD, In Proceedings of the 56th Annual Meeting of the Association for ComputationalLinguistics (Volume 2: Short Papers)
 CoQA: A conversational question answeringchallenge, Transactions of the Association for Computational Linguistics
 Arecipe for arbitrary text style transfer with large language models, arXiv preprint arXiv:2109
 Choice of plausible alternatives: Anevaluation of commonsense causal reasoning, In AAAI Spring Symposium Series
 An embarrassingly simple approach to zero-shotlearning, In Proceedings of the International Conference on Machine Learning
 An overview of multi-task learning in deep neural networks, arXiv preprintarXiv:1706
 WinoGrande: An adver-sarial winograd schema challenge at scale, In Proceedings of the AAAI Conference on ArtificialIntelligence
 Multitask prompted trainingenables zero-shot task generalization, Proceedings of the International Conference on LearningRepresentations
 Analysing mathematicalreasoning abilities of neural models, Proceedings of the International Conference on LearningRepresentations
 Exploiting cloze-questions for few-shot text classification andnatural language inference, In Proceedings of the 16th Conference of the European Chapterof the Association for Computational Linguistics: Main Volume
 Get to the point: Summarization withpointer-generator networks, In Proceedings of the 55th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers)
 Edinburgh neural machine translation systemsfor WMT 16, In Proceedings of the First Conference on Machine Translation: Volume 2
 Adafactor: Adaptive learning rates with sublinear memorycost, In International Conference on Machine Learning
 Recursive deep models for semantic compositionality over a sentiment treebank,In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing
 Process for adapting language models to society (palms) withvalues-targeted datasets, arXiv preprint arXiv:2106
 Zero-shot learning of classifiers from naturallanguage quantification, In Proceedings of the 56th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers)
 Improvingand simplifying pattern exploiting training, In Proceedings of the 2021 Conference on EmpiricalMethods in Natural Language Processing (EMNLP)
 Lamda: Language models for dialogapplications, arXiv preprint arXiv:2201
 Meta-learning: A survey, arXiv preprint arXiv:1810
 Seq2seq and multi-task learning for joint intent and contentextraction for domain specific interpreters, arXiv preprint arXiv:1808
 GLUE:A multi-task benchmark and analysis platform for natural language understanding, In Proceedingsof the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks forNLP
 Superglue: A stickier benchmark for general-purpose languageunderstanding systems, Conference on Neural Information Processing Systems (NeurIPS)
 Neural network-based abstract generation for opinions and arguments,In Proceedings of the 2016 Conference of the North American Chapter of the Association forComputational Linguistics: Human Language Technologies
 Multi-agent dual learning, In Proceedings of the International Conference on Learning Representations(ICLR) 2019
 Neural network acceptability judgments,Transactions ofthe Associationfor Computational Linguistics
 Chain of thought prompting elicits reasoning in large language models, arXiv preprintarXiv:2201
 A broad-coverage challenge corpus for sen-tence understanding through inference, In Proceedings of the 2018 Conference of the North Ameri-can Chapter of the Association for Computational Linguistics: Human Language Technologies
 Recursively summarizing books with human feedback, arXiv preprint arXiv:2109
 CorefQA: Coreference resolution asquery-based span prediction, In Proceedings of the 58th Annual Meeting of the Associationfor Computational Linguistics
 Crossfit: A few-shot learning challenge for cross-taskgeneralization in NLP, In Proceedings of the 2021 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP)
" Benchmarking zero-shot text classification: Datasets,evaluation and entailment approach", In Proceedings of the 2019 Conference on Empirical Methodsin Natural Language Processing and the 9th International Joint Conference on Natural LanguageProcessing (EMNLP-IJCNLP)
" HellaSwag: Can amachine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Associationfor Computational Linguistics, pp", 4791-4800
 This email could save your life: Introducing the task of email subjectline generation, In Proceedings of the 57th Annual Meeting of the Association for ComputationalLinguistics
Record: Bridging the gap between human and machine commonsense reading comprehension,CoRR
 Character-level convolutional networks for text clas-sification, In NIPS
 PAWS: Paraphrase adversaries from word scrambling,In Proceedings of the 2019 Conference of the North American Chapter of the Association forComputational Linguistics: Human Language Technologies
 Meta-tuning language models to answerprompts better, arXiv preprint arXiv:2104
" Results for translation and struct totext are shown in Table 1, and the results for eight NLU task clusters are shown in Table 2",We show FLAN’s performance using the best ofup to ten instruction templates as well as the templatewith the best performance on the dev set
", 2018; Yin et al",
",2022)", Although the success of these approaches depends heavily on model scale (Lester et al
" In comparison, our paper is much larger in scope, empirically demonstrating the ideaon a wide range of tasks with a much larger model", Other work has used QA-based task formulationfor more-targeted applications including semantic role labeling (He et al
