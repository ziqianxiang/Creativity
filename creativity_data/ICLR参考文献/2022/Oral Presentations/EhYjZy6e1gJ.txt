Published as a conference paper at ICLR 2022
PiCO: Contrastive Label Disambiguation for
Partial Label Learning
Haobo Wang1	Ruixuan Xiao1	Yixuan Li2	Lei Feng34
Gang Niu4	Gang Chen1	Junbo Zhao1*
1Zhejiang University 2University of Wisconsin-Madison
3Chongqing University 4RIKEN
Ab stract
Partial label learning (PLL) is an important problem that allows each training ex-
ample to be labeled with a coarse candidate set, which well suits many real-world
data annotation scenarios with label ambiguity. Despite the promise, the perfor-
mance of PLL often lags behind the supervised counterpart. In this work, we
bridge the gap by addressing two key research challenges in PLL—representation
learning and label disambiguation—in one coherent framework. Specifically,
our proposed framework PiCO consists of a contrastive learning module along
with a novel class prototype-based label disambiguation algorithm. PiCO pro-
duces closely aligned representations for examples from the same classes and
facilitates label disambiguation. Theoretically, we show that these two compo-
nents are mutually beneficial, and can be rigorously justified from an expectation-
maximization (EM) algorithm perspective. Extensive experiments demonstrate
that PiCO significantly outperforms the current state-of-the-art approaches in PLL
and even achieves comparable results to fully supervised learning. Code and data
available: https://github.com/hbzju/PiCO.
1 Introduction
The training of modern deep neural networks typically requires massive labeled data, which imposes
formidable obstacles in data collection. Of a particular challenge, data annotation in the real-world
can naturally be subject to inherent label ambiguity and noise. For example, as shown in Figure
1, identifying an Alaskan Malamute from a Siberian Husky can be difficult for a human annotator.
The issue of labeling ambiguity is prevalent yet often overlooked in many applications, such as web
mining (Luo & Orabona, 2010) and automatic image annotation (Chen et al., 2018). This gives rise
to the importance of partial label learning (PLL) (Hullermeier & Beringer, 2006; Cour et al., 2011),
where each training example is equipped with a set of candidate labels instead of the exact ground-
truth label. This stands in contrast to its supervised counterpart where one label must be chosen as
the “gold”. Arguably, the PLL problem is deemed more common and practical in various situations
due to its relatively lower cost to annotations.
Despite the promise, a core challenge in PLL is label disambiguation, i.e., identifying the ground-
truth label from the candidate label set. Existing methods typically require a good feature representa-
tion (Liu & Dietterich, 2012; Zhang et al., 2016; Lyu et al., 2021), and operate under the assumption
that data points closer in the feature space are more likely to share the same ground-truth label.
However, the reliance on representations has led to a non-trivial dilemma—the inherent label uncer-
tainty can undesirably manifest in the representation learning process—the quality of which may, in
turn, prevent effective label disambiguation. To date, few efforts have been made to resolve this.
This paper bridges the gap by reconciling the intrinsic tension between the two highly depen-
dent problems—representation learning and label disambiguation—in one coherent and synergistic
framework. Our framework, Partial label learning with COntrastive label disambiguation (dubbed
PiCO), produces closely aligned representations for examples from the same classes and facilitates
label disambiguation. Specifically, PiCO encapsulates two key components. First, we leverage con-
trastive learning (CL) (Khosla et al., 2020) to partial label learning, which is unexplored in previous
* Correspondence to j.zhao@zju.edu.cn.
1
Published as a conference paper at ICLR 2022
PLL literature. To mitigate the key challenge of constructing positive pairs, we employ the classi-
fier’s output and generate pseudo positive pairs for contrastive comparison (Section 3.1). Second,
based on the learned embeddings, we propose a novel prototype-based label disambiguation strategy
(Section 3.2). Key to our method, we gradually update the pseudo target for classification, based
on the closest class prototype. By alternating the two steps above, PiCO converges to a solution
with a highly distinguishable representation for accurate classification. Empirically, PiCO estab-
lishes state-of-the-art performance on three benchmark datasets, outperforming the baselines by a
significant margin (Section 4) and obtains results that are competitive with fully supervised learning.
Theoretically, we demonstrate that our contrastive representation learning and prototype-based la-
bel disambiguation are mutually beneficial, and can be rigorously interpreted from an Expectation-
Maximization (EM) algorithm perspective (Section 5). First, the refined pseudo labeling improves
contrastive learning by selecting pseudo positive examples accurately. This can be analogous to
the E-step, where we utilize the classifier’s output to assign each data example to one label-specific
cluster. Second, better contrastive performance in turn improves the quality of representations and
thus the effectiveness of label disambiguation. This can be reasoned from an M-step perspective,
where the contrastive loss partially maximizes the likelihood by clustering similar data examples.
Finally, the training data will be mapped to a mixture of von Mises-Fisher distributions on the unit
hypersphere, which facilitates label disambiguation by using the component-specific label.
Our main contributions are summarized as follows:
1	(Methodology). To the best of our knowledge, our paper pioneers the exploration of contrastive
learning for partial label learning and proposes a novel framework termed PiCO. As an integral part
of our algorithm, we also introduce a new prototype-based label disambiguation mechanism, that
leverages the contrastively learned embeddings.
2	(Experiments). Empirically, our proposed PiCO framework establishes the state-of-the-art per-
formance on three PLL tasks. Moreover, we make the first attempt to conduct experiments on
fine-grained classification datasets, where we show classification performance improvement by up
to 9.61% compared with the best baseline on the CUB-200 dataset.
3	(Theory). We theoretically interpret our framework from the expectation-maximization perspec-
tive. Our derivation is also generalizable to other CL methods and shows the alignment property in
CL (Wang & Isola, 2020) mathematically equals the M-step in center-based clustering algorithms.
2 Background
The problem of partial label learning (PLL) is defined using the
following setup. Let X be the input space, and Y = {1, 2, ..., C}
be the output label space. We consider a training dataset D =
{(xi, Yi)}in=1, where each tuple comprises of an image xi ∈ X
and a candidate label set Yi ⊂ Y . Identical to the supervised
learning setup, the goal of PLL is to obtain a functional mapping
that predicts the one true label associated with the input. Yet
differently, the PLL setup bears significantly more uncertainty in
the label space. A basic assumption of PLL is that the ground-
truth label yi is concealed in its candidate set, i.e., yi ∈ Yi, and is
invisible to the learner. For this reason, the learning process can
suffer from inherent ambiguity, compared with the supervised
learning task with explicit ground-truth.
A dog image xi with
Yi = {Husky, MaIamlIte, Samoyed)
Figure 1: An input image with three
candidate labels, where the ground-
truth is Malamute.
The key challenge of PLL is to identify the ground-truth label from the candidate label set. During
training, we assign each image xi a normalized vector si ∈ [0, 1]C as the pseudo target, whose
entries denote the probability of labels being the ground-truth. The total probability mass of 1 is
allocated among candidate labels in Yi . Note that si will be updated during the training procedure.
Ideally, si should put more probability mass on the (unknown) ground-truth label yi over the course
of training. We train a classifier f : X → [0, 1]C using cross-entropy loss, with si being the target
prediction. The per-sample loss is given by:
C
Lcls(f;xi,Yi)	= j=1 -si,jlog(fj(xi))	s.t. j∈Y	si,j = 1 and	si,j	= 0,∀j	∈/	Yi,	(1)
2
Published as a conference paper at ICLR 2022
AUgmented
Ouprv Vipw
Momentum
Encoder
Figure 2: Illustration of PiCO. The classifier’s output is used to determine the positive peers for contrastive
learning. The contrastive prototypes are then used to gradually update the pseudo target. The momentum
embeddings are maintained by a queue structure. ’//’ means stop gradient.
where j denotes the indices of labels. si,j denotes the j-th pseudo target of xi. Here f is the softmax
output of the networks and we denote fj as its j-th entry. In the remainder of this paper, we omit
the sample index i when the context is clear. We proceed by describing our proposed framework.
3 Method
In this section, we describe our novel Partial label learning with COntrastive label disambigua-
tion (PiCO) framework in detail. In a nutshell, PiCO comprises two key components tackling the
representation quality (Section 3.1) and label ambiguity respectively (Section 3.2). The two com-
ponents systematically work as a whole and reciprocate each other. We further rigorously provide a
theoretical interpretation of PiCO from an EM perspective in Section 5.
3.1	Contrastive Representation Learning for PLL
The uncertainty in the label space posits a unique obstacle for learning effective representations. In
PiCO, we couple the classification loss in Eq. (1) with a contrastive term that facilitates a clustering
effect in the embedding space. While contrastive learning has been extensively studied in recent
literature, it remains untapped in the domain of PLL. The main challenge lies in the construction
of a positive sample set. In conventional supervised CL frameworks, the positive sample pairs can
be easily drawn according to the ground-truth labels (Khosla et al., 2020). However, this is not
straightforward in the setting of PLL.
Training Objective. To begin with, we describe the standard contrastive loss term. We adopt
the most popular setups by closely following MoCo (He et al., 2020) and SupCon (Khosla et al.,
2020). Given each sample (x, Y ), we generate two views—a query view and a key view—by
way of randomized data augmentation Aug(x). The two images are then fed into a query network
g(∙) and a key network g0(∙), yielding a pair of L2-normalized embeddings q = g(Augq(x)) and
k = g 0 (Augk (x)). In implementations, the query network shares the same convolutional blocks
as the classifier, followed by a prediction head (see Figure 2). Following MoCo, the key network
uses a momentum update with the query network. We additionally maintain a queue storing the
most current key embeddings k, and we update the queue chronologically. To this end, we have the
following contrastive embedding pool:
A = Bq ∪ Bk ∪ queue,	(2)
where Bq and Bk are vectorial embeddings corresponding to the query and key views of the current
mini-batch. Given an example x, the per-sample contrastive loss is defined by contrasting its query
embedding with the remainder of the pool A,
Lcont (g; x, τ, A)
1 X	io._exp(q>k+/T)_
|P(x)l z-k+∈P(x) g Pk0∈A(χ) exp(q>k0∕τ)
(3)
3
Published as a conference paper at ICLR 2022
where P (x) is the positive set and A(x) = A\{q}. τ ≥ 0 is the temperature.
Positive Set Selection. As mentioned earlier, the crucial challenge is how to construct the positive
set P(x). We propose utilizing the predicted label y = argmaxj∈γ fj(Augq(x)) from the classi-
fier. Note that we restrict the predicted label to be in the candidate set Y . The positive examples are
then selected as follows,
P(X) = {k0∣k0∈ A(χ),y0 = y}.
(4)
where y0 is the predicted label for the corresponding training example of k0. For computational
efficiency, we also maintain a label queue to store past predictions. In other words, we define the
positive set of X to be those examples carrying the same approximated label prediction y. Despite its
simplicity, we show that our selection strategy can be theoretically justified (Section 5) and also lead
to superior empirical results (Section 4). Note that more sophisticated selection strategies can be
explored, for which we discuss in Appendix B.4. Putting it all together, we jointly train the classifier
as well as the contrastive network. The overall loss function is:
L = Lcls + λLcont.
(5)
Still, our goal of learning high-quality representation by CL relies on accurate classifier prediction
for positive set selection, which remains unsolved in the presence of label ambiguity. To this end,
we further propose a novel label disambiguation mechanism based on contrastive embeddings and
show that these two components are mutually beneficial.
3.2	Prototype-based Label Disambiguation
As we mentioned (and later theoretically prove in Section 5), the contrastive loss poses a clustering
effect in the embedding space. As a collaborative algorithm, we introduce our novel prototype-based
label disambiguation strategy. Importantly, We keep a prototype embedding vector μc corresponding
to each class c ∈ {1, 2, ..., C}, which can be deemed as a set of representative embedding vectors.
Categorically, a naive version of the pseudo target assignment is to find the nearest prototype of the
current embedding vector. Notably this primitive resembles a clustering step. We additionally soften
this hard label assignment version by using a moving-average style formula. To this end, we may
posit intuitively that the employment of the prototype builds a connection with the clustering effect
in the embedding space brought by the contrastive term (Section 3.1). We provide a more rigorous
justification in Section 5.
Pseudo Target Updating. We propose a softened and moving-average style strategy to update
the pseudo targets. Specifically, we first initialize the pseudo targets with a uniform distribution,
Sj = ∣Y∣I(j ∈ Y). We then iteratively update it by the following moving-average mechanism,
S = φs + (1- φ)z,	Zc = I1 if C = argmaxj∈Y q>"j,
0 else
(6)
where φ ∈ (0,1) is a positive constant, and μj is a prototype corresponding to the j-th class. The
intuition is that fitting uniform pseudo targets results in a good initialization for the classifier since
the contrastive embeddings are less distinguishable at the beginning. The moving-average style
strategy then smoothly updates the pseudo targets towards the correct ones, and meanwhile ensures
stable dynamics of training; see Appendix B.2. With more rigorous validation provided later in
Section 5, we provide an explanation for the prototype as follows: (i)-for a given input X, the closest
prototype is indicative of its ground-truth class label. At each step, s has the tendency to slightly
move toward the one-hot distribution defined by z based on Eq. (6); (ii)-if an example consistently
points to one prototype, the pseudo target s can converge (almost) to a one-hot vector with the least
ambiguity.
Prototype Updating. The most canonical way to update the prototype embeddings is to compute
it in every iteration of training. However, this would extract a heavy computational toll and in turn
cause unbearable training latency. As a result, we update the class-conditional prototype vector
similarly in a moving-average style:
μc = Normalize(γμc + (1 - γ)q),	if C = argmaxj∈γ fj(AUgq(X))),
(7)
where the momentum prototype μc of class C is defined by the moving-average of the normalized
query embeddings q whose predicted class conforms to C. γ is a tunable hyperparameter.
4
Published as a conference paper at ICLR 2022
Table 1: Accuracy comparisons on benchmark datasets. Bold indicates superior results. Notably, PiCO
achieves comparable results to the fully supervised learning (less than 1% in accuracy with ≈ 1 false candidate).
Dataset	Method	q = 0.1	q=0.3	q=0.5
	PiCO (ours)	94.39 ± 0.18%	94.18 ± 0.12%	93.58 ± 0.06%
CIFAR-10	LWS PRODEN CC MSE EXP	90.30 ± 0.60% 90.24 ± 0.32% 82.30 ± 0.21% 79.97 ± 0.45% 79.23 ± 0.10%	88.99 ± 1.43% 89.38 ± 0.31% 79.08 ± 0.07% 75.64 ± 0.28% 75.79 ± 0.21%	86.16 ± 0.85% 87.78 ± 0.07% 74.05 ± 0.35% 67.09 ± 0.66% 70.34 ± 1.32%
	Fully Supervised	94.91 ± 0.07%		
Dataset	Method	q = 0.01	q=0.05	q=0.1
	PiCO (ours)	73.09 ± 0.34%	72.74 ± 0.30%	69.91 ± 0.24%
CIFAR-100	LWS PRODEN CC MSE EXP	65.78 ± 0.02% 62.60 ± 0.02% 49.76 ± 0.45% 49.17 ± 0.05% 44.45 ± 1.50%	59.56 ± 0.33% 60.73 ± 0.03% 47.62 ± 0.08% 46.02 ± 1.82% 41.05 ± 1.40%	53.53 ± 0.08% 56.80 ± 0.29% 35.72 ± 0.47% 43.81 ± 0.49% 29.27 ± 2.81%
	Fully Supervised	73.56 ± 0.10%		
3.3	Synergy between Contrastive Learning and Label Disambiguation
While seemingly separated from each other, the two key components of PiCO work in a collaborative
fashion. First, as the contrastive term favorably manifests a clustering effect in the embedding space,
the label disambiguation module further leverages via setting more precise prototypes. Second, a set
of well-polished label disambiguation results may, in turn, reciprocate the positive set construction
which serves as a crucial part in the contrastive learning stage. The entire training process converges
when the two components perform satisfactorily. We further rigorously draw a resemblance of
PiCO with a classical EM-style clustering algorithm in Section 5. Our experiments, particularly
the ablation study displayed in Section 4.3, further justify the mutual dependency of the synergy
between the two components. The pseudo-code of our complete algorithm is shown in Appendix C.
4	Experiments
4.1	Setup
Datasets and Baselines. First, we evaluate PiCO on two commonly used benchmarks — CIFAR-10
and CIFAR-100 (Krizhevsky et al., 2009). Adopting the identical experimental settings in previous
work (Lv et al., 2020; Wen et al., 2021), we generate partially labeled datasets by flipping negative
labels y = y to false positive labels with a probability q = P(y ∈ Y|y = y). In other words, all
C - 1 negative labels have a uniform probability to be false positive and we aggregate the flipped
ones with the ground-truth to form the candidate set. We consider q ∈ {0.1, 0.3, 0.5} for CIFAR-
10 and q ∈ {0.01, 0.05, 0.1} for CIFAR-100. In Section 4.4, we further evaluate our method on
fine-grained classification tasks, where label disambiguation can be more challenging.
We choose the five best-performed partial label learning algorithms to date: 1) LWS (Wen et al.,
2021) weights the risk function by means of a trade-off between losses on candidate labels and the
remaining; 2) PRODEN (Lv et al., 2020) iteratively updates the latent label distribution in a self-
training style; 3) CC (Feng et al., 2020b) is a classifier-consistent method that assumes set-level
uniform data generation process; 4) MSE and EXP (Feng et al., 2020a) are two simple baselines that
adopt mean square error and exponential loss as the risks. The hyperparameters are tuned according
to the original methods. The detailed implementation of our method PiCO is presented in Ap-
pendix B.1. For all experiments, we report the mean and standard deviation based on 5 independent
runs (with different random seeds).
4.2	Main Empirical Results
PiCO achieves SOTA results. As shown in Table 1, PiCO significantly outperforms all the rivals
by a significant margin on all datasets. Specifically, on CIFAR-10 dataset, we improve upon the
best baseline by 4.09%, 4.80%, and 5.80% where q is set to 0.1, 0.3, 0.5 respectively. Moreover,
PiCO consistently achieves superior results as the size of the candidate set increases, while the
baselines demonstrate a significant performance drop. Besides, it is worth pointing out that previous
5
Published as a conference paper at ICLR 2022
(a) Uniform features
(b) PRODEN features
(c) PiCO features (ours)
Figure 3: T-SNE visualization of the image representation on CIFAR-10 with q = 0.5. Different colors
represent the corresponding classes.
works (Lv et al., 2020; Wen et al., 2021) are typically evaluated on datasets with a small label space
(C = 10). We challenge this by showing additional results on CIFAR-100. When q = 0.1, all the
baselines fail to obtain satisfactory performance, whereas PiCO remains competitive. Finally, we
observe that PiCO achieves results that are comparable to the fully supervised contrastive learning
model, showing that disambiguation is sufficiently accomplished. The comparison highlights the
superiority of our label disambiguation strategy.
PiCO learns more distinguishable representations. We visualize the image representation pro-
duced by the feature encoder using t-SNE (Van der Maaten & Hinton, 2008) in Figure 3. Different
colors represent different ground-truth class labels. We use the CIFAR-10 dataset with q = 0.5. We
contrast the t-SNE embeddings of three approaches: (a) a model trained with uniform pseudo targets,
i.e., sj = 1/|Y | (j ∈ Y ), (b) the best baseline PRODEN, and (c) our method PiCO. We can observe
that the representation of the uniform model is indistinguishable since its supervision signals suffer
from high uncertainty. The features of PRODEN are improved, yet with some class overlapping
(e.g., blue and purple). In contrast, PiCO produces well-separated clusters and more distinguishable
representations, which validates its effectiveness in learning high-quality representation.
4.3	Ablation Studies
In this section, we present part of our ablation results to show the effectiveness of PiCO. We refer
readers to Appendix B.2 for more ablation experiments.
Effect of Lcont and label disambiguation. We ablate the
contributions of two key components of PiCO: contrastive
learning and prototype-based label disambiguation. In par-
ticular, we compare PiCO with two variants: 1) PiCO w/o
disambiguation which keeps the pseudo target as uniform
1/|Y |; and 2) PiCO w/o Lcont, which further removes the
contrastive learning and only trains a classifier with uniform
pseudo targets. From Table 2, we can observe that vari-
ant 1 substantially outperforms variant 2 (e.g., +8.04% on
CIFAR-10), which signifies the importance of contrastive
learning for producing better representations. Moreover,
with label disambiguation, PiCO obtains results close to
fully supervised setting, which verifies the ability of PiCO
in identifying the ground-truth.
Figure 4: Performance of PiCO with
varying φ on CIFAR-100 (q = 0.05).
Different disambiguation strategy. Based on the contrastive prototypes, various strategies can
be used to disambiguate the labels, which corresponds to the E-step in our theoretical analysis. We
choose the following variants: 1) One-hot Prototype always assigns a one-hot pseudo target s = z
by using the nearest prototype (φ = 0); 2) Soft Prototype Probs follows (Li et al., 2021a) and uses
a soft class probability Si = P exp(qpμi∕μ) /τ)as the pseudo target (φ = 0); 3) MA Soft Prototype
Probs gradually updates pseudo target from uniform by using the soft probabilities in a moving-
average style. From Table 2, we can see that directly using either soft or hard prototype-based
label assignment leads to competitive results. This corroborates our theoretical analysis in Section
5, since center-based class probability estimation is common in clustering algorithms. However,
MA Soft Prototype Probs displays degenerated performance, suggesting soft label assignment is less
reliable in identifying the ground-truth. Finally, PiCO outperforms the best variant by ≈ 2% in
accuracy on both datasets, showing the superiority of our label disambiguation strategy.
6
Published as a conference paper at ICLR 2022
Table 2: Ablation study on CIFAR-10 with q = 0.5 and CIFAR-100 with q = 0.05.
Ablation	Lcont	Label Disambiguation	CIFAR-10 (q =。5)	CIFAR-100 (q = 0.05)
PiCO	X	Ours	93.58^^	72.74
PiCO w/o Disambiguation	X	Uniform Pseudo Target	84.50	64.11
PiCO w/o Lcont	X	Uniform Pseudo Target	76.46	56.87
PiCO with φ = 0	X	Soft Prototype Probs	-9160-	71.07
PiCO with φ = 0	X	One-hot Prototype	91.41	70.10
PiCO	X	MA Soft Prototype Probs	81.67	63.75
Effect of moving-average factor φ. We then explore the effect of pseudo target updating factor
φ on PiCO performance. Figure 4 shows the learning curves of PiCO on CIFAR-100 (q = 0.05).
We can see that the best result is achieved at φ = 0.9 and the performance drops when φ takes a
smaller value, particularly on the early stage. When φ = 0, PiCO obtains a competitive result but is
much lower than φ = 0.9. This confirms that trusting the uniform pseudo targets at the early stage is
crucial in obtaining superior performance. At the other extreme value φ = 1, uniform pseudo targets
are used, and PiCO demonstrates a degenerated performance and severe overfitting phenomena. In
general, PiCO performs well when φ ≈ 0.9.
4.4 Further Extension: Fine-Grained Partial Label Learning
Recall the dog example highlighted in Section 2,	Table 3: Accuracy comparisons on fine-grained		
where semantically similar classes are more likely	ClaSSificatiOn datasets.			
to cause label ambiguity. It begs the question of	Method	CUB-200	CIFAR-100-H
whether PiCO is effective on the challenging fine-		(q = 0.05)	(q =。5)
grained image classification tasks. To verify this,	PiCO	72.17 ± 0.72%	72.04 ± 0.31%
we conduct experiments on two datasets: 1) CUB-	LWS	39.74 ± 0.47%	57.25 ± 0.02%
200 dataset (Welinder et al., 2010) contains 200	PRODEN	62.56 ± 0.10%	60.89 ± 0.03%
bird species; 2) CIFAR-100 with hierarchical labels	CC	55.61 ± 0.02%	42.60 ± 0.11%
(CIFAR-100-H), where we generate candidate labels	MSE	22.07 ± 2.36%	39.52 ± 0.28%
that belong to the same superclass1. We set q = 0.05	EXP	9.44 ± 2.32%	35.08 ± 1.71%
for CUB-200 and q = 0.5 for CIFAR-100 with hi-
erarchical labels. In Table 3, we compare PiCO with baselines, where PiCO outperforms the best
method PRODEN by a large margin (+9.61% on CUB-200 and +11.15% on CIFAR-100-H). Our
results validate the effectiveness of our framework, even in the presence of strong label ambiguity.
5	Why PiCO improves partial label learning
In this section, we provide theoretical justification on why the contrastive prototypes help disam-
biguate the ground-truth label. We show that the alignment property in contrastive learning (Wang
& Isola, 2020) intrinsically minimizes the intraclass covariance in the embedding space, which coin-
cides with the objective of classical clustering algorithms. It motivates us to interpret PiCO through
the lens of the expectation-maximization algorithm. To see this, we consider an ideal setting: in
each training step, all data examples are accessible and the augmentation copies are also included in
the training set, i.e., A = D. Then, the contrastive loss is calculated as,
Lcont(g；T, D) = 1 Xl-ɪ X log P exp(q>k+!	∖
n X∈D IIP(X)I k+∈P(x)	Ek0∈A(χ) exp(q>k0/τ)
n X )-∣p⅛)∣	X(q>k+/T)
x∈D	k+ ∈P(x)
I	、z
{z,^
(a)
+n X l log X eχpg>k7τ) ∖.
x∈D	k0 ∈A(x)
/ 1--------------------
(b)
(8)

We focus on analyzing the first term (a), which is often dubbed as the alignment term (Wang &
Isola, 2020). The main functionality of this term is to optimize the tightness of the clusters in the
embedding space. In this work, we connect it with classical clustering algorithms. We first split
the dataset to C subsets Sj ∈ DC (1 ≤ j ≤ C), where each subset contains examples possessing
1CIFAR-100 dataset consists of 20 superclasses, with 5 classes in each superclass.
7
Published as a conference paper at ICLR 2022
the same predicted labels. In effect, our selection strategy in Eq. (4) constructs the positive set by
selecting examples from the same subset. Therefore, we have,
(a) = -X UD IPLl X“ UPe)(Mq - k+∣∣2 - 2)/(2t)
n Z"""χ∈D ∣P(x)∣ Zk++eP(x)
≈ 1— XqUrn ∣-1T X	y q l|g(x) - g(〃)ll2 + K	⑼
2τn	Sj ∈DC ∣Sj ∣	x,x0∈Sj
=Tn Xs.∈Dc Xχ∈s. ∣∣g(x)- μj∣∣2 + K,
τn j ∈ C x∈ j
where K isa constant and μj is the mean center of Sj. Here We approximate 高 ≈ ∣s,∣-1 = ∣p(xj
since n is usually large. We omitted the augmentation operation for simplicity. The uniformity term
(b) can benefit information-preserving, and has been analyzed in (Wang & Isola, 2020).
We are now ready to interpret the PiCO algorithm as an expectation-maximization algorithm that
maximizes the likelihood of a generative model. At the E-step, the classifier assigns each data
example to one specific cluster. At the M-step, the contrastive loss concentrates the embeddings to
their cluster mean direction, which is achieved by minimizing Eq. (9). Finally, the training data will
be mapped to a mixture of von Mises-Fisher distributions on the unit hypersphere.
EM Perspective. Recall that the candidate label set is a noisy version of the ground-truth. To
estimate the likelihood P(Yi, xi), we need to establish the relationship between the candidate and
the ground-truth label. Following (Liu & Dietterich, 2012), we make a mild assumption,
Assumption 1. All labels yi in the candidate label set have the same probability of generating Yi,
butno label outside of Yi can generate Yi, i.e. P(YiIyi) = ~(Yi) if yi ∈ Yi else 0. Here ~(∙) is some
function making it a valid probability distribution.
Then, we show that the PiCO implicitly maximizes the likelihood as follows,
E-Step. First, we introduce some distributions over all examples and the candidates πij ≥ 0 (1 ≤
i ≤ n,1 ≤ j ≤ C) such that πij = 0 ifj ∈/ Yi and Pj∈Y πij =1. Let θ be the parameters ofg. Our
goal is to maximize the likelihood below,
Xn	nn
i=1 log P (Yi, xi∣θ) = argmθax	i=1log	yi∈YiP(xi,yi∣θ)+	i=1log(~(Yi))
=argmaχ Xi=Ilog Xyi∈Yi πyi P(Xy i∣θ)
≥ argmaχ Xi=I fy.∈Yi πyi log P(Xy i∣θ).
(10)
The last step of the derivation uses Jensen's inequality. By using the fact that log(∙) function is
concave, the inequality holds with equality when P(Xiyyi ∣θ) is Some constant. Therefore,
πi
yi =	P (Xi ,yi∣θ)	=	P (χi,yi∣θ)
Pyi∈YiP(Xi,yi 网=Pyi=I P(χi,yi∣θ)
P (χi,yi∣θ)
P(Xi∣θ)
P (yi∣Xi, θ),
(11)
which is the posterior class probability. In PiCO, it is estimated by using the classifier’s output.
To estimate P(yi∣Xi, θ), classical unsupervised clustering methods intuitively assign the data exam-
ples to the cluster centers, e.g. k-means. As in the supervised learning setting, we can directly use
the ground-truth. However, under the setting of PLL, the supervision signals are situated between
the supervised and unsupervised setups. Based on empirical findings, the candidate labels are more
reliable for posterior estimation at the beginning; yet alongside the training process, the prototypes
tend to become more trustful. This empirical observation has motivated us to update the pseudo tar-
gets in a moving-average style. Thereby, we have a good initialization in estimating class posterior,
and it will be smoothly refined during the training procedure. This is verified in our empirical stud-
ies; see Section 4.3 and Appendix B.2. Finally, we take one-hot prediction yi = arg maxj∙∈ γ fj (xi)
since each example inherently belongs to exactly one label and hence, we have πj = I(yi = j).
M-Step. At this step, we aim at maximizing the likelihood under the assumption that the poste-
rior class probability is known. We show that under mild assumptions, minimizing Eq. (9) also
maximizes a lower bound of likelihood in Eq. (10).
8
Published as a conference paper at ICLR 2022
Theorem 1. Assume data from the same class in the contrastive output space follow a d-
variate Von Mises-Fisher (vMF) distribution whose probabilistic density is given by f (x∣μi, K)=
Cd(K) eκμ>g((X), where μ i = μi∕∣∣μ∕∣ is the mean direction, K is the concentration parameter, and
cd(κ) is the normalization factor. We further assume a uniform class prior P (yi = j) = 1/C. Let
nj = |Sj |. Then, optimizing Eq. (9) and Eq. (10) equal to maximize R1 and R2 below, respectively.
R1 = XsrDc nj ll"j7 * * * ll2 ≤ XSj ∈Dc nj llMj 11 = R2.	(12)
The proof can be found in Appendix A. Theorem 1 indicates that minimizing Eq. (9) also maximizes
a lower bound of the likelihood in Eq. (10). The lower bound is tight when ∣∣μj∣∣ is close to 1,
which in effect means a strong intraclass concentration on the hypersphere. Intuitively, when the
hypothesis space is rich enough, it is possible to achieve a low intraclass covariance in the Euclidean
space, resulting in a large norm of the mean vector ∣∣μj||. Then, normalized embeddings in the
hypersphere also have an intraclass concentration in a strong sense, because a large ∣∣μj || also results
in a large K (Banerjee et al., 2005). Regarding the visualized representation in Figure 3, we note that
PiCO is indeed able to learn compact clusters. Therefore, we have that minimizing the contrastive
loss also partially maximizes the likelihood defined in Eq. (10).
6 Related Works
Partial Label Learning (PLL) allows each training example to be annotated with a candidate label
set, in which the ground-truth is guaranteed to be included. The most intuitive solution is average-
based methods (Hullermeier & Beringer, 2006; Cour et al., 2011; Zhang & Yu, 2015), which treat all
candidates equally. However, the key and obvious drawback is that the predictions can be severely
misled by false positive labels. To disambiguate the ground-truth from the candidates, identification-
based methods (Jin & Ghahramani, 2002), which regard the ground-truth as a latent variable, have
recently attracted increasing attention; representative approaches include maximum margin-based
methods (Nguyen & Caruana, 2008; Wang et al., 2020), graph-based methods (Zhang et al., 2016;
Wang et al., 2019; Xu et al., 2019; Lyu et al., 2021), and clustering-based approaches (Liu & Diet-
terich, 2012). Recently, self-training methods (Feng et al., 2020b; Lv et al., 2020; Wen et al., 2021)
have achieved state-of-the-art results on various benchmark datasets, which disambiguate the candi-
date label sets by means of the model outputs themselves. But, few efforts have been made to learn
high-quality representations to reciprocate label disambiguation.
Contrastive Learning (CL) (van den Oord et al., 2018; He et al., 2020) is a framework that learns
discriminative representations through the use of instance similarity/dissimilarity. A plethora of
works has explored the effectiveness of CL in unsupervised representation learning (van den Oord
et al., 2018; Chen et al., 2020; He et al., 2020). Recently, Khosla et al. (2020) propose supervised
contrastive learning (SCL), an approach that aggregates data from the same class as the positive set
and obtains improved performance on various supervised learning tasks. The success of SCL has
motivated a series of works to apply CL to a number of weakly supervised learning tasks, including
noisy label learning (Li et al., 2021a; Wu et al., 2021), semi-supervised learning (Li et al., 2020;
Zhang et al., 2021), etc. Despite promising empirical results, however, these works, lack theoretical
understandings. Wang & Isola (2020) theoretically show that the CL favors alignment and unifor-
mity, and thoroughly analyzed the properties of uniformity. But, to date, the terminology alignment
remains confusing; we show it inherently maps data points to a mixture of vMF distributions.
7 Conclusion
In this work, we propose a novel partial label learning framework PiCO. The key idea is to identify
the ground-truth from the candidate set by using contrastively learned embedding prototypes. Em-
pirically, we conducted extensive experiments and show that PiCO establishes state-of-the-art per-
formance. Our results are competitive with the fully supervised setting, where the ground-truth label
is given explicitly. Theoretical analysis shows that PiCO can be interpreted from an EM-algorithm
perspective. Applications of multi-class classification with ambiguous labeling can benefit from our
method, and we anticipate further research in PLL to extend this framework to tasks beyond image
classification. We hope our work will draw more attention from the community toward a broader
view of using contrastive prototypes for partial label learning.
9
Published as a conference paper at ICLR 2022
Acknowledgments
HW, RX, GC and JZ are supported by the Key R&D Program of Zhejiang Province (Grant No.
2020C01024). JZ would also like to thank the Fundamental Research Funds for the Central Uni-
versities. YL is supported by Wisconsin Alumni Research Foundation (WARF), Facebook Research
Award, and funding from Google Research. FL is supported by the National Natural Science Foun-
dation of China (Grant No. 62106028).
References
Sercan Omer Arik and Tomas Pfister. Attention-based prototypical learning towards interpretable,
confident and robust deep neural networks. CoRR, abs/1902.06292, 2019.
Arindam Banerjee, Inderjit S Dhillon, Joydeep Ghosh, and Suvrit Sra. Clustering on the unit hy-
persphere using Von mises-fisher distributions. J. Mach. Learn. Res., 6:1345-1382, 2005.
Leon BottoU and Yoshua Bengio. Convergence properties of the k-means algorithms. In Gerald
Tesauro, David S Touretzky, and Todd K Leen (eds.), NIPS, pp. 585-592. MIT Press, 1994.
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
Unsupervised learning of visual features by contrasting cluster assignments. In NeurIPS, 2020.
Ching-Hui Chen, Vishal M Patel, and Rama Chellappa. Learning from ambiguously labeled face
images. IEEE Trans. Pattern Anal. Mach. Intell., 40(7):1653-1667, 2018.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E Hinton. A simple framework for
contrastive learning of visual representations. In ICML, volume 119 of Proceedings of Machine
Learning Research, pp. 1597-1607. PMLR, 2020.
Timothee Cour, Benjamin Sapp, and Ben Taskar. Learning from partial labels. J. Mach. Learn. Res.,
12:1501-1536, 2011.
Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical data
augmentation with no separate search. CoRR, abs/1909.13719, 2019.
Lei Feng, Takuo Kaneko, Bo Han, Gang Niu, Bo An, and Masashi Sugiyama. Learning with multiple
complementary labels. In ICML, volume 119 of Proceedings of Machine Learning Research, pp.
3072-3081. PMLR, 2020a.
Lei Feng, Jiaqi Lv, Bo Han, Miao Xu, Gang Niu, Xin Geng, Bo An, and Masashi Sugiyama. Prov-
ably consistent partial-label learning. In NeurIPS, 2020b.
Tao Han, Junyu Gao, Yuan Yuan, and Qi Wang. Unsupervised semantic aggregation and deformable
template matching for semi-supervised learning. In NeurIPS 2020, 2020.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum contrast for
unsupervised visual representation learning. In CVPR, pp. 9726-9735. IEEE, 2020.
Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution
examples in neural networks. In ICLR. OpenReview.net, 2017.
Eyke Hullermeier and JUrgen Beringer. Learning from ambiguously labeled examples. Intell. Data
Anal., 10(5):419-439, 2006.
Rong Jin and Zoubin Ghahramani. Learning with multiple labels. In NeurIPS, pp. 897-904. MIT
Press, 2002.
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron
Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. In NeurIPS, 2020.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
10
Published as a conference paper at ICLR 2022
Junnan Li, Caiming Xiong, and Steven C H Hoi. Comatch: Semi-supervised learning with con-
trastive graph regularization. CoRR, abs/2011.11183, 2020.
Junnan Li, Caiming Xiong, and Steven C H Hoi. Mopro: Webly supervised learning with momen-
tum prototypes. In ICLR. OpenReview.net, 2021a.
Junnan Li, Pan Zhou, Caiming Xiong, and Steven C H Hoi. Prototypical contrastive learning of
unsupervised representations. In ICLR. OpenReview.net, 2021b.
Li-Ping Liu and Thomas G Dietterich. A conditional multinomial mixture model for superset label
learning. In NeurIPS,pp. 557-565, 2012.
Jie LUo and Francesco Orabona. Learning from candidate labeling sets. In NeurIPS, pp. 1504-1512.
Curran Associates, Inc., 2010.
Jiaqi Lv, Miao XU, Lei Feng, Gang NiU, Xin Geng, and Masashi SUgiyama. Progressive identifi-
cation of trUe labels for partial-label learning. In ICML, volUme 119 of Proceedings of Machine
Learning Research, pp. 6500-6510. PMLR, 2020.
GengyU LyU, Songhe Feng, Tao Wang, Congyan Lang, and Yidong Li. GM-PLL: graph matching
based partial label learning. IEEE Trans. Knowl. Data Eng., 33(2):521-535, 2021.
Nam NgUyen and Rich CarUana. Classification with partial labels. In SIGKDD, pp. 551-559. ACM,
2008.
NikUnj SaUnshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, and Hrishikesh Khandeparkar.
A theoretical analysis of contrastive UnsUpervised representation learning. In ICML, volUme 97
of Proceedings of Machine Learning Research, pp. 5628-5637. PMLR, 2019.
Jake Snell, Kevin Swersky, and Richard S Zemel. Prototypical networks for few-shot learning. In
NeurIPS, pp. 4077-4087, 2017.
KihyUk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin Raffel, Ekin Do-
gUs CUbUk, Alexey KUrakin, and ChUn-Liang Li. Fixmatch: Simplifying semi-sUpervised learning
with consistency and confidence. In NeurIPS, 2020.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. CoRR, abs/1807.03748, 2018.
LaUrens Van der Maaten and Geoffrey Hinton. VisUalizing data Using t-sne. Journal of machine
learning research, 9(11), 2008.
Deng-Bao Wang, Li Li, and Min-Ling Zhang. Adaptive graph gUided disambigUation for partial
label learning. In SIGKDD, pp. 83-91. ACM, 2019.
Haobo Wang, YUzhoU Qiang, Chen Chen, Weiwei LiU, Tianlei HU, Zhao Li, and Gang Chen. Online
partial label learning. In ECML PKDD, volUme 12458 of Lecture Notes in Computer Science, pp.
455-470. Springer, 2020.
TongzhoU Wang and Phillip Isola. Understanding contrastive representation learning throUgh align-
ment and Uniformity on the hypersphere. In ICML, volUme 119 of Proceedings of Machine Learn-
ing Research, pp. 9929-9939. PMLR, 2020.
P Welinder, S Branson, T Mita, C Wah, F Schroff, S Belongie, and P Perona. Caltech-UCSD
Birds 200. Technical Report CNS-TR-2010-001, California InstitUte of Technology, 2010.
Hongwei Wen, Jingyi CUi, HanyUan Hang, Jiabin LiU, Yisen Wang, and ZhoUchen Lin. Lever-
aged weighted loss for partial label learning. In ICML, volUme 139 of Proceedings of Machine
Learning Research, pp. 11091-11100. PMLR, 2021.
Zhi-Fan WU, Tong Wei, Jianwen Jiang, Chaojie Mao, Mingqian Tang, and YU-Feng Li. NGC: A
Unified framework for learning with open-world noisy data. CoRR, abs/2108.11035, 2021.
Ning XU, Jiaqi Lv, and Xin Geng. Partial label learning via label enhancement. In AAAI, pp. 5557-
5564. AAAI Press, 2019.
11
Published as a conference paper at ICLR 2022
Wenjia Xu, Yongqin Xian, Jiuniu Wang, Bernt Schiele, and Zeynep Akata. Attribute prototype net-
work for zero-shot learning. In Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual, 2020.
Min-Ling Zhang and Fei Yu. Solving the partial label learning problem: An instance-based ap-
Proach. In IJCAI,pp. 4048-4054. AAAI Press, 2015.
Min-Ling Zhang, Bin-Bin Zhou, and Xu-Ying Liu. Partial label learning via feature-aware disam-
biguation. In KDD, pp. 1335-1344. ACM, 2016.
Yuhang Zhang, Xiaopeng Zhang, Robert C Qiu, Jie Li, Haohang Xu, and Qi Tian. Semi-supervised
contrastive learning with similarity co-calibration. CoRR, abs/2105.07387, 2021.
Appendix
A Theoretical Analysis
Derivation of Eq. (9). We provide the derivation of the second equality in Eq. (9). It suffices to
ShoWthatLHS =旖 Pχ,χ,∈sj ||g(X)- g(XO) ||2 = Pχ∈sj ||g(X)- 〃j||2 = rhs∙ Wehave,
LHS = 21- XX (IIg(X)||2 - 2g(X)Tg(XO) + ||g(xO)IF)
j x∈Sj x0 ∈Sj
=—X Sj- g(X)>( X g(XO)))
nj
x∈Sj	x0 ∈Sj
1 j	j	(13)
=一(nj - g(X)T (njμj)))
nj
x∈Sj
=nj - ( X g(X))Tμj) = nj(I - llμj ||2).
x∈Sj
On the other hand,
RHS = X (IIg(X)II2 - 2g(X)>μj + IIμj||2)
x∈Sj
= (nj- 2( X g(X))>μj + 1川〃川2)	(14)
x∈Sj
=nj(I-"μj U2) = lhs∙
Proof of Theorem 1. By regarding πij as constants W.r.t θ, We can get the folloWing derivation
from Eq. (10),
arg max Xn X
πyi log P(XiyyM = arg max X X πyi log P(Xi 加 θ)p(yi)
θ	πθ
i=1 yi∈Yi	i	i=1 yi∈Yi
n
= arg max E E I(yi = yi)logP(XiIyi, θ)
i=1 yi∈Yi
= arg mθax Σ Σ logP(XIy=j, θ)	(15)
Sj ∈DC x∈Sj
= arg max E E(Kμ>g(X)+log(Cd(K)))
Sj ∈DC x∈Sj
= arg max X n>II
Sj ∈DC
Where nj = ISj I. Here We ignore the constant factor - Pin=1 Py ∈Y πiyi log πiyi W.r.t. θin the first
equality. In the last equality, we use the fact that μj = ： Px∈s^ g(X) and μj is the unit directional
12
Published as a conference paper at ICLR 2022
vector of μj. From Eq. (9), We have that,
arg minXX
l∣g(x) - μj ||2 = arg min X X (IIg(X)|12- 2g(x)>μj + MIF)
Sj ∈DC x∈Sj	Sj∈DC x∈Sj
=arg min E Sj- nj ||〃j | F)
θ
Sj ∈DC
=argmαx χ n||〃j||2.
Sj ∈DC
(16)
Note that the contrastive embeddings are distributed on the hypersphere Sd-1 and thus ∣∣μj∙∣∣ ∈
[0, 1]. It can be directly derived that,
RI= X	nj ιiμj 112 ≤ x	nj ιiμj 11=R2.
(17)
Therefore, maximizing the intraclass covariance in Eq. (9) is equivalent to maximizing a loWer
bound of the likelihood in Eq. (10). It can also be shoWn that (R2)2 ≤ R1 folloWed by the convexity
of the squared function. Since arg max R2 = argmax(R2)2, We have that the contrastive loss also
maximizes an upper bound of R2 .
Unfortunately, there is no guarantee that the loWer bound is tight Without further assumptions. To see
this, assume that We have tWo classes y ∈ {1, 2} With equal-sized samples and their mean vectors
have the norm of ∣∣μι∣∣ = 0.5 and ∣∣μι∣∣ = 1. We have that Ri = 0.625 and R2 = 0.75, which
demonstrates a large discrepancy. It is interesting to see that When the norm of the mean vectors
are the same, i.e. ∣∣μj-1∣ = ∣∣μk∣∣ for all 1 ≤ j ≤ k ≤ C, we have (R2)2 = Ri by the Jensen's
inequality, making the upper bound tight. But, it is not a trivial condition.
To obtain a tight lower bound, what we need is a rich enough hypothesis space to achieve a low
intraclass covariance in Eq. (9), and hence a large Ri. We show that it inherently produces compact
vMF distributions. To see this, it should be noted that the concentration parameter κ of a vMF
distribution is given by the inverse of the ratio of Bessel functions of mean vector μj∙. Though it is
not possible to obtain an analytic solution of κ, we have the following well-known approximation
(Banerjee et al., 2005),
d-1
K ≈ 2(1 - ∣∣μ,∣∣),	validforlarge ||〃j||,	(18)
K ≈ d∖∖μj|| (1 + dd2∖∣μj||2 + (dd 22+814 ll"j||4), ValidfOrSman ||〃j||.	(19)
d + 2	(d + 2) (d + 4)
The above approximations show that a larger norm of Euclidean,s mean μj- typically leads to a
stronger concentration on the hypersphere. By Eq. (16), we know that contrastive loss encourages a
large norm of μj∙, and thus also tightly clusters the embeddings on the hypersphere; see Figure 5.
We further note that we do not include a k-means process in our PiCO method. PiCO is related
to center-based clustering algorithms in our theoretical analysis. Since we restrict the gold label to
be included in the candidate set, we believe that this piece of information could largely help avoid
the bad optimum problem that occurs in a pure unsupervised setup. For the convergence properties
of our PiCO algorithm, we did not empirically find any issues with PiCO converging to a (perhaps
locally) optimal point. However, we want to refer the readers to the proof of k-means clustering in
(Bottou & Bengio, 1994) for the convergence analysis.
Discussion. Regarding our empirical results where PiCO does indeed learn compact representa-
tions for each class, we can conclude that PiCO implicitly clusters data points in the contrastive
embeddings space as a mixture of vMF distributions. In each iteration, our algorithm can be viewed
as alternating the two steps until convergence, though different in detail. First, it is intractable to
handle the whole training dataset, and thus we accelerate via a MoCo-style dictionary and MA-
updated prototypes. Second, the contrastive loss also encourages the uniformity of the embeddings
to maximally preserve information, which serves as a regularizer and typically leads to better rep-
resentation. Finally, we use two copies of data examples such that data are also aligned in its local
13
Published as a conference paper at ICLR 2022
Figure 5: A large norm of Euclidean's mean vector also leads to a strong concentration of unit vectors to its
mean direction.
region. Moreover, our theoretical result also answers why merely taking the pseudo-labels to select
positive examples also leads to superior results, since the selected positive set will be refined as the
training procedure proceeds.
Our theoretical results are also generalizable to other contrastive learning methods. For example, the
classical unsupervised contrastive learning (He et al., 2020) actually assumes n-prototypes to cluster
all locally augmented data; prototypical contrastive learning (Li et al., 2021b) directly assigns the
data examples to one cluster to get the posterior, since there are no supervision signals; supervised
contrastive learning (Khosla et al., 2020) chooses the known ground-truth as the posterior. In our
problem, we follow two extreme settings to progressively obtain an accurate posterior estimation.
Finally, it is also noteworthy that the objective in Eq. (9) has a close connection to the intraclass
deviation (Saunshi et al., 2019), minimizing which is proven to be beneficial in obtaining tighter
generalization error bound on downstream tasks. It should further be noted that our work differs
from existing clustering-based CL methods (Caron et al., 2020; Li et al., 2021b), which explicitly
involves clustering to aggregate the embeddings; instead, our results are derived from the loss itself.
B Additional Experimental Setups and Results
B.1	Implementation Details
Following the standard experimental setup in PLL (Feng et al., 2020b; Wen et al., 2021), we split a
clean validation set (10% of training data) from the training set to select the hyperparameters. After
that, we transform the validation set back to its original PLL form and incorporate it into the training
set to accomplish the final model training. We use an 18-layer ResNet as the backbone for feature
extraction. Most of experimental setups for the contrastive network follow previous works (He
et al., 2020; Khosla et al., 2020). The projection head of the contrastive network is a 2-layer MLP
that outputs 128-dimensional embeddings. We use two data augmentation modules SimAugment
(Khosla et al., 2020) and RandAugment (Cubuk et al., 2019) for query and key data augmentation
respectively. Empirically, we find that even weak augmentation for key embeddings also leads to
good results. The size of the queue that stores key embeddings is fixed to be 8192. The momentum
coefficients are set as 0.999 for contrastive network updating and γ = 0.99 for prototype calculation.
For pseudo target updating, we linearly ramp down φ from 0.95 to 0.8. The temperature parameter
is set as τ = 0.07. The loss weighting factor is set as λ = 0.5. The model is trained by a standard
SGD optimizer with a momentum of 0.9 and the batch size is 256. We train the model for 800
epochs with cosine learning rate scheduling. We also empirically find that classifier warm-up leads
to better performance when there are many candidates. Hence, we disable contrastive learning in
the first 100 epoch for CIFAR-100 with q = 0.1 and 1 epoch for the remaining experiments.
B.2	Ablation Studies
Moving-average updating factor φ. We first present more ablation results about the effect of
pseudo target updating factor φ on PiCO performance. Figure 6 (a) shows the results on two datasets
14
Published as a conference paper at ICLR 2022
(a) Ablation φ
Figure 6: More ablation results on CIFAR-10 (q = 0.5) and CIFAR-100 (q = 0.05). (a) Performance of PiCO
with varying φ. (b) Performance of PiCO with varying λ.
77.5
75.0
72.5
10^2	10^1 IO0 IO1
λ (log scale)
(b) Ablation λ
CIFAR-10 (q = 0.5) and CIFAR-100 (q = 0.05). The overall trends on both datasets follow our
arguments in section 4.3. Specifically, performance on CIFAR-100 achieves the best result when
φ = 0.7, and slight drops when φ = 0.9. Therefore, in practice, we may achieve a better result
with careful fine-tuning on φ value. In contrast, PiCO works well in a wide range of φ values on
CIFAR-10. The reason might be that CIFAR-10 is a simpler version of CIFAR-100, and thus the
prototypes can be high-quality quickly. But, setting φ to either 0 or 1 leads to a worse result, which
has been discussed earlier.
Loss weight λ. Figure 6 (b) reports the performance of PiCO with varying λ values that trade-off
the classification and contrastive losses. λ is selected from {0.01, 0.1, 0.5, 5, 50}. We can observe
that on CIFAR-10, the performance is stable, but on CIFAR-100, the best performance is obtained
at λ = 5. When λ = 50, PiCO shows inferior results on both two datasets. In general, a relatively
small λ (< 10) usually leads to good performance than a much larger value. When λ is large, the
contrastive network tends to fit noisy labels at the early stage of training.
Table 4: Performance of PiCO With varying Y on CIFAR-10 and CIFAR-100.
Dataset	I Y = 0∙1		γ = 0.5	γ=0.9	γ = 0.99	γ = 0.999
CIFAR-10 (q =	0.5)	93.61	93.51	93.52	93.58	93.66
CIFAR-100 (q =	0.05)	72.87	73.09	72.54	72.74	67.33
Prototype updating factor γ. Finally, we show the effect of γ that controls the speed of prototype
updating and the results are listed in Table 4. On the CIFAR-10 dataset, the performance is stable
With varying γ. But, on CIFAR-100, it can be seen that too large λ leads to a significant performance
drop, Which may be caused by insufficient label disambiguation.
Table 5: Training accuracy of pseudo targets on CIFAR-10 and CIFAR-100.
Dataset	q = 0.1	CIFAR-10 q=0.3	q=0.5	q = 0.01	CIFAR-100 q=0.05	q=0.1
Accuracy	98.28	98.26	96.79	99.06	96.27	90.58
The disambiguation ability of PiCO. Finally, We evaluate the disambiguation ability of the pro-
posed PiCO. To see this, We calculate the max confidence to represent the uncertainty of an exam-
ple, Which has been Widely used in recent Works (Hendrycks & Gimpel, 2017). If one example
is uncertain about its ground-truth, then it typically associates With loW max confidence maxj sj .
To represent the uncertainty of the Whole training dataset, We calculate the mean max confidence
(MMC) score. In Figure 7, We plot the MMC scores of different label disambiguation strategies in
different training epochs. First, We can observe the MMC score of PiCO smoothly increases and
finally achieves near 1 results, Which means most of the labels are Well disambiguated. In contrast,
15
Published as a conference paper at ICLR 2022
Epoch
(a) CIFAR-10 (q = 0.5)
(b) CIFAR-100 (q = 0.05)
Figure 7: The mean max confidence curves of different label disambiguation strategies.
the Soft Prototype Probs strategy oscillates at the beginning, and then also increases to a certain
value, which means that directly adopting the soft class probability also helps disambiguation. But,
it is worth noting that it ends with a smaller MMC score compared with PiCO. The reason might
be that the cosine distances to non-ground-truth prototypes are still at a scale. Hence, the Soft
Prototype Probs strategy always holds a certain degree of ambiguity. Finally, we can see that the
MA Soft Prototype Probs strategy fails to achieve great disambiguation ability. Compared with the
non-moving-average version, it fails to get rid of severe label ambiguity and will finally converge to
uniform pseudo targets again.
Furthermore, we evaluated the accuracy of the pseudo targets over training examples. From Table
5, we can find that the pseudo targets achieve high training accuracy. Combined with the fact that
the mean max confidence score of the pseudo target is close to 1, the training examples finally
become near supervised ones. Thus, the proposed PiCO is able to achieve near-supervised learning
performance, especially when the label ambiguity is not too high. These results verify that PiCO
has a strong label disambiguation ability to handle the PLL problem.
B.3	Additional Results on Fine-Grained Classification
In the sequel, we show the full setups and experimental results on fine-grained classification datasets.
In particular, on CUB-200, we set the length of the momentum queue as 4192. For CUB-200, we
set the input image resolution as 224 × 224 and select q ∈ {0.01, 0.05, 0.1}. When q = 0.05/0.1,
we warm up for 20/100 epochs and train the model for 200/300 epochs, respectively. For CIFAR-
100-H, we select q ∈ {0.1, 0.5, 0.8} and warm up the model for 100 epochs when q = 0.8. Other
hyperparameters are the same as our default setting. The baselines are also fine-tuned to achieve
their best results.
From Table 6, we can observe that PiCO significantly outperforms all the baselines mentioned in
section 4 on all the datasets. Moreover, as the size of candidate sets grows larger, PiCO con-
sistently leads by an even wider margin. For example, on CIFAR-100-H, compared with the
best baseline, performance improvement reaches 9.50%, 11.15% and 22.53% in accuracy when
q = 0.1, 0.5, 0.8, respectively. The comparison emerges the dominance of our label disambiguation
strategy among semantically similar classes.
B.4	Strategies for Positive Selection
While our positive set selection strategy is simple and effective, one may still explore more compli-
cated strategies to boost performance. We have empirically tested two strategies: 1) Filter-based:
We set a filter ∣Yi∩Y] ≤ P (P = 0.5) to remove example pairs Who have dissimilar candidate sets at
the early stage. 2) Threshold-based: we set a threshold max fj (Augq(x)) ≤ δ (δ = 0.95) to re-
move those uncertain examples at the end of training, Which has been Widely used in semi-supervised
learning (Sohn et al., 2020). Our basic principle is that contrastive learning is robust to noisy nega-
16
Published as a conference paper at ICLR 2022
Table 6: Accuracy comparisons on fine-grained datasets. Bold indicates superior results.
Dataset	Method	q = 0.1	q=0.5	q = 0.8
	PiCO (ours)	73.41 ± 0.27%	72.04 ± 0.31%	66.17 ± 0.23%
	LWS	62.41 ± 0.03%	57.25 ± 0.02%	20.64 ± 0.48%
CIFAR-100-H	PRODEN	62.91 ± 0.01%	60.89 ± 0.03%	43.64 ± 1.82%
	CC	50.40 ± 0.20%	42.60 ± 0.11%	37.80 ± 0.09%
	MSE	46.05 ± 0.17%	39.52 ± 0.28%	15.18 ± 0.73%
	EXP	45.73 ± 0.22%	35.08 ± 1.71%	22.31 ± 0.39%
Dataset	Method	q = 0.01	q = 0.05	q= 0.1
	PiCO (ours)	74.14 ± 0.24%	72.17 ± 0.72%	62.02 ± 1.16%
	LWS	73.74 ± 0.23%	39.74 ± 0.47%	12.30 ± 0.77%
	PRODEN	72.34 ± 0.04%	62.56 ± 0.10%	35.89 ± 0.05%
CUB-200	CC	56.63 ± 0.01%	55.61 ± 0.02%	17.01 ± 1.44%
	MSE	61.12 ± 0.51%	22.07 ± 2.36%	11.40 ± 2.42%
	EXP	55.62 ± 2.25%	9.44 ± 2.32%	7.3 ± 0.99%
	Fully Supervised	76.02 ± 0.19%		
tive pairs and thus, we can flip those less reliable positive pairs to negative. Unfortunately, we did
not observe statistically significant improvement to our vanilla strategy in experiments. These nega-
tive results suggest that the proposed PiCO has a strong error correction ability, which corroborates
our theoretical analysis.
B.5	The Influence of Data Generation
In practice, some labels may be more analogous to
the true label than others, which makes their prob-
ability of label flipping q larger than others. In
other words, the data generation procedure is non-
uniform. In Section 4.4, we have shown one such
case on CIFAR-100-H, where semantically similar
labels have a larger probability of being a false posi-
tive. Moreover, we follow (Wen et al., 2021) to con-
duct empirical comparisons on data with alternative
generation processes. In particular, we test two com-
Table 7: Accuracy comparisons with different
data generation processes on CIFAR-10.
Method I Case (1)	∣ Case (2)
PiCO	94.49 ± 0.08%	94.11 ± 0.25%
LWS	90.78 ± 0.01%	68.37 ± 0.04%
PRODEN	90.53 ± 0.01%	87.02 ± 0.02%
CC	75.81 ± 0.13%	66.51 ± 0.11%
MSE	68.11 ± 0.23%	39.49 ± 0.41%
EXP	71.62 ± 0.79%	48.87 ± 2.32%
monly used cases on CIFAR-10 with the following flipping matrix, respectively:
	-1	0.5	0	∙	•• 0		-1	0.9	0.7	0.5	0.3	0.1	0	・	・ 0-
	0	1	0.5 ∙	•• 0		0	1	0.9	0.7	0.5	0.3	0.1 •	・ 0
(1)=	. . . 0.5	0	• ∙ ∙ 0	・	. . . •• 1	,(2)=	. . . 0.9	0.7	0.5	0.3	••• 0.1	0	0	・	. . . ・・ 1
where each entry denotes the probability of a label being a candidate. As shown in Table 7, PiCO
outperforms other baselines in both cases. It is worth noting that in Case (2), each ground-truth
label has a maximum probability of 0.9 of being coupled with the same false positive label. In such
a challenging setup, PiCO still achieves promising results that are competitive with the supervised
performance, which further verifies its strong disambiguation ability.
B.6	The Influence of Prototype Calculation
There are several ways to calculate the prototypes
and hence, we further test a variant of PiCO that
re-computes the prototypes by averaging embed-
dings of all training examples at the end of each
epoch. We train the models using one Quadro
P5000 GPU respectively and evaluate the average
training time per epoch. From Table 8, we can ob-
serve that the Re-Compute variant achieves com-
petitive results, but is much slower than PiCO.
Table 8: Training time (min/epoch) and accuracy of
different prototype calculation methods.
Dataset	Method	Time	Accuracy
CIFAR-10	PiCO	0.94	93.58
(q = 0.5)	Re-Compute	1.39	93.55
CIFAR-100	PiCO	0.96	72.74
(q = 0.05)	Re-Compute	1.40	72.35
17
Published as a conference paper at ICLR 2022
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
Algorithm 1: Pseudo-code of PiCO (one epoch).
Input: Training dataset D, classifier f, query network g, key network g0, momentum queue, uniform
pseudo-labels Si associated With Xi in D, class prototypes μj (1 ≤ j ≤ C).
for iter = 1, 2, . . . , do
sample a mini-batch B from D
// query and key embeddings generation
Bq = {qi = g(Augq(xi))|xi ∈ B}
Bk = {ki = g0(Augk(xi))|xi ∈ B}
A = Bq ∪ Bk ∪ queue
for xi ∈ B do
// classifier prediction
yi = argmaxj∈Yi fj (Augq(Xi))
// momentum prototype updating
μc = Normalize(γμc + (1 - γ)qi), if yi = C
// positive set generation
P (Xi) = {k0∣k0 ∈ A(xi),y0 = yi}
end
// prototype-based label disambiguation
for qi ∈ Bq do
Zi = OneHot(argmaxj∈居 q>μj)
Si = φsi + (1 - φ)zi
end
// contrastive loss calculation
Lcont (g;τ, A)=禹 Pqi∈Bq {一∣p(x-)i pk+∈p (xi )log PkocApjiek+z? k"τ)}
// classification loss calculation
Lcls(f; B) = |B, Pχ-∈B PC=1 -Si,j log(fj(Augq(Xi)))
// network updating
minimize loss L = Lcls + λLcont
// update the key network and momentum queue
momentum update g0 by using g
enqueue Bk and classifier predictions and dequeue
end
C Pseudo-Code of PiCO
We summarize the pseudo-code of our PiCO method in Algorithm 1.
D	The Literature of Prototype Learning
Prototype learning (PL) aims to learn a metric space Where examples are enforced to be closer to its
class prototype. PL is typically more robust in handling feW-shot learning (Snell et al., 2017), zero-
shot learning (Xu et al., 2020), and out-of-distribution samples (Arik & Pfister, 2019). Recently, PL
has demonstrated promising results in Weakly-supervised learning, such as semi-supervised learn-
ing (Han et al., 2020), noisy-label learning (Li et al., 2021a), etc. For example, USADTM (Han
et al., 2020) shoWs that informative class prototypes usually lead to better pseudo-labels for semi-
supervised learning than classical pseudo-labeling algorithms (Sohn et al., 2020) Which reuse clas-
sifier outputs. Motivated by this, We also employ contrastive prototypes for label disambiguation.
18