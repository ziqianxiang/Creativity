Published as a conference paper at ICLR 2022
Minibatch vs Local SGD with Shuffling:
Tight Convergence Bounds and Beyond
Chulhee Yun	Shashank Rajput	Suvrit Sra
KAIST AI	Univ. of Wisconsin-Madison CS	MIT EECS
chulhee.yun@kaist.ac.kr	rajput3@wisc.edu	suvrit@mit.edu
Ab stract
In distributed learning, local SGD (also known as federated averaging) and its
simple baseline minibatch SGD are widely studied optimization methods. Most
existing analyses of these methods assume independent and unbiased gradient es-
timates obtained via with-replacement sampling. In contrast, we study shuffling-
based variants: minibatch and local Random Reshuffling, which draw stochastic
gradients without replacement and are thus closer to practice. For smooth func-
tions satisfying the Polyak-LojasieWicz condition, We obtain convergence bounds
(in the large epoch regime) which show that these shuffling-based variants con-
verge faster than their With-replacement counterparts. Moreover, We prove match-
ing loWer bounds shoWing that our convergence analysis is tight. Finally, We
propose an algorithmic modification called synchronized shuffling that leads to
convergence rates faster than our loWer bounds in near-homogeneous settings.
1	Introduction
Distributed learning within the framework of federated learning (KoneCny et al., 2016; McMahan
et al., 2017) has Witnessed increasing interest recently. A key property of this frameWork is that
models are trained locally using only private data on devices/machines distributed across a network,
while parameter updates are aggregated and synchronized at a server.1 Communication is often the
key bottleneck for federated learning, which drives the search for algorithms that can train fast while
requiring less communication—see Li et al. (2020a); Kairouz et al. (2021) for recent surveys.
A basic algorithm for federated learning is local stochastic gradient descent (SGD), also known as
federated averaging. The goal is to minimize the global objective that is an average of the local
objectives. In local SGD, we have M machines and a server. After each round of communication,
each of the M machines locally runs B steps of SGD on its local objective. Every B iterations, the
server aggregates the updated local iterates from the machines, averages them, and then synchronizes
the machines with the average. Convergence analysis of local SGD and its variants has drawn
great interest recently (Dieuleveut & Patel, 2019; Haddadpour et al., 2019; Haddadpour & Mahdavi,
2019; Stich, 2019; Yu et al., 2019; Li et al., 2020b;c; Koloskova et al., 2020; Khaled et al., 2020;
Spiridonoff et al., 2020; Karimireddy et al., 2020; Stich & Karimireddy, 2020; Qu et al., 2020).
Of the many, the biggest motivation for our paper comes from the line of work by Woodworth
et al. (2020a;b; 2021). In (Woodworth et al., 2020a;b), minibatch SGD is studied as a simple
yet powerful baseline for this intermittent communication setting. Instead of locally updating the
iterates B times, minibatch SGD aggregates B gradients (evaluated at the last synced iterate) from
each of the M machines, forms a minibatch of size MB, and then updates the shared iterate. Given
the same M and B, local SGD and minibatch SGD have the same number of gradient computations
per round of communication, so it is worthwhile to understand which converges faster. Woodworth
et al. (2020a;b) point out that many existing analyses on local SGD show inferior convergence rate
compared to minibatch SGD. Through their new upper and lower bounds, they identify regimes
where local SGD can be faster than minibatch SGD.
While the theory of local and minibatch SGD has seen recent progress, there is still a gap between
what is analyzed versus what is actually used. Most theoretical results assume independent and
1A distinctive feature of federated learning is that not all devices necessarily participate in the updates;
however, we focus on the full participation setting in this paper.
1
Published as a conference paper at ICLR 2022
unbiased gradient estimates obtained via with-replacement sampling of stochastic gradients (i.e.,
choosing training data indices uniformly at random). In contrast, most practitioners use without-
replacement sampling, where they shuffle indices randomly and access them sequentially.
Convergence analysis of without-replacement methods is challenging because gradients sampled
within an epoch lack independence. As a result, the standard theory based on independent gradient
estimates does not apply to shuffling-based methods. While shuffling-based methods are believed to
be faster in practice (Bottou, 2009), broad theoretical understanding of such methods remains elu-
sive, except for noteworthy recent progress mainly focusing on the analysis of SGD (Gurbuzbalaban
et al., 2019; Haochen & Sra, 2019; Nagaraj et al., 2019; Nguyen et al., 2020; Safran & Shamir,
2020; 2021; Rajput et al., 2020; 2021; Ahn et al., 2020; Mishchenko et al., 2020; 2021; Tran et al.,
2021). These results indicate that in the large-epoch regime (where the number of epochs is greater
than some threshold), without-replacement SGD converges faster than with-replacement SGD.
1.1	Our contributions
We analyze convergence rates of without-replacement versions of local and minibatch SGD, where
local component functions are reshuffled at every epoch. We call the respective algorithms local
RR (Algorithm 1) and minibatch RR (Algorithm 2), and their with-replacement counterparts local
Sgd and minibatch Sgd. Our key contributions are as follows:
•	In Section 3, we present convergence bounds on minibatch and local RR for L-smooth func-
tions satisfying the μ-Polyak-Eojasiewicz condition (Theorems 1 & 2). Our theorems give high-
probability bounds, a departure from the common in-expectation bounds in the literature. We
show that minibatch and local RR converge faster than minibatch and local SGD when the num-
ber of epochs is sufficiently large. We also identify a regime where local RR converges as fast
as minibatch RR: when synchronization happens frequently enough and local objectives are not
too heterogeneous. See also Appendix A for a detailed comparison with existing upper bounds.
•	In Section 4, we prove that the upper bounds obtained in Section 3 are tight, in all factors except
L and μ. We present Theorems 3 & 4 and Proposition 5 which show lower bounds that match
the upper bound UP to a factor of L2/μ2. Our lower bound on local RR indicates that if the
synchronization interval B is too large, then local RR has no gain from parallel computation.
•	In Section 5, we propose a simple modification called synchronized shuffling that allows us to
bypass the lower bounds in Section 4, atthecostofa slight increase in communication. By having
the server broadcast random permutations to local machines, we show that in near-homogeneous
settings, the modified algorithms converge faster than the lower bounds (Theorems 6 & 7).
•	In Appendix C, we present numerical experiments that corroborate our theoretical findings.
2	Problem setup
Notation. For a natural number a ∈ N, let [a] := {1, 2, . . . , a}. Let Sa be the set of all permutations
of [a]. Since our indices start from 1, we redefine the modulo operation between a ∈ Z and b ∈ N
as a mod b := a 一 [a-1 1b, to make a mod b ∈ [b].
Optimization task. Consider M machines, each with its objective Fm(x) := N PN=1 fim(x),
for m ∈ [M]. The m-th machine has access only to the gradients of its own N local components
f1m(x), . . . , fNm (x). In this setting, we wish to minimize the global objective function which is an
average of the local objectives: F(X) :=焉 PMM=I Fm(X) = MN PMM=I PNL1 fm(X).
Further, we assume that each individual component function fim is L-smooth, so that
fim(y) ≤ fim(x) + Efm(X), y - Xi + 2 ky - xk2, for all X, y ∈ Rd,	(1)
and that the global objective F satisfies the μ-Polyak土OjaSieWiCz (PE) condition.2
2 ∣∣VF(x)『≥ μ(F(x) 一 F*) for all X ∈ Rd,	where μ > 0.	(2)
Algorithms. Under the above setting, we analyze local RR (Algorithm 1) and minibatch RR (Algo-
rithm 2) and characterize their worst-case convergence rates.3 The algorithms are run over K epochs,
2PE functions can be thought as a nonconvex generalization of strongly convex functions.
3In Algorithms 1 and 2, consider SyncShuf as False for now. We will discuss SyncShuf in Section 5.
2
Published as a conference paper at ICLR 2022
Algorithm 1 Local RR (with and without SYNCSHUF)
	Input: Initialization y0, step-size η, # machines M, # components N, # epochs K, sync interval B.
1:	Initialize x1m,0 := y0 for all m ∈ [M].
2:	for k ∈ [K] do
3:	if SYNCSHUF = TRUE then	. Local RR with SYNCSHUF
4:	Sample σ 〜Unif(SN), ∏ 〜Unif(SM).
5:	Set σm(i) := σ((i + Mπ(m)) mod N) for all m ∈ [M],i ∈ [N].
6:	else	. Local RR
7:	Sample σ'〜Unif(SN) independently and locally, for all m ∈ [M].
8:	end if
9:	for i ∈ [N] do
10	:	for m ∈ [M] do locally
11	UPdate Xmi：= Xmi-I- ηvfmm(i) (Xmi-1).
12	:	end for
13	:	if B divides i then
14	Aggregate and average yk,B ：= M PM=I xmi.
15	Synchronize Xmi := yk ɪ, for all m ∈ [M].
16	end if	B
17	:	end for
18	xm+1,0 ：= yk N, for all m ∈ [M].
19	end for
20	return the last iterate yκ N.
Algorithm 2 Minibatch RR (with and without SYNCSHUF)
	Input: Initialization X0, step-size η, # machines M, # components N, # epochs K, sync interval B.
1: 2: 3: 4: 5: 6: 7: 8: 9: 10 11 12 13 14	Initialize X1,0 := X0. for k ∈ [K] do if SYNCSHUF = TRUE then	. Minibatch RR with SYNCSHUF Sample σ 〜Unif(SN), ∏ 〜Unif(SM). Set σmm(i) := σ((i + Mπ(m)) mod N) for allm ∈ [M],i ∈ [N]. else	. Minibatch RR Sample σml 〜 Unif(SN) independently and locally, for all m ∈ [M]. end if for i ∈ [B] do	M	.b UPdate χk,i ：= χk,i-1 - MX	B X .(i-i)B, i VfmmCj)(Xk,i-1). m=1	j=(i-1)B+1 end for		V	‘ Xk+1,0 ：= Xk, N .	averagingdonelocally end for return the last iterate XKN.
i.e., K passes over the entire component functions. At the beginning of epoch k, each machine m
shuffles its local component functions {fm}N=ι using a random permutation 吸 〜Unif(SN).
In local RR, each machine makes B local RR updates to its iterate by sequentially accessing its
shuffled component functions, before the server aggregates iterates from all the machines and then
synchronizes the machines with the average iterate. In minibatch RR, instead of making B local
updates, each machine collects B gradients evaluated at the last iterate, and the server aggregates
them to make an update using these MB gradients. Since these two algorithms use the same amount
of communication and local gradients, minibatch RR is a simple yet powerful baseline for local RR.
Below, we collect our assumptions on the algorithm parameters used throughout the paper.
Assumption 1 (Algorithm parameters). We assume M ≥ 1, N ≥ 2, and K ≥ 1. Also, assume that
B divides N. We restrict 1 ≤ B ≤ N for minibatch RR because B = N makes the algorithm equal
to GD. We also assume 2 ≤ B ≤ N for local RR because B = 1 makes the two algorithms the
same. We choose a constant step-size scheme, i.e., η > 0 is kept constant over all updates.
We next state assumptions on intra- and inter-machine deviations used in this paper.4
4Assumptions 2, 3 & 4 require that they hold for the whole Rd. We discuss ways to avoid it in Appendix D.7.
3
Published as a conference paper at ICLR 2022
Assumption 2 (Intra-machine deviation). There exists ν ≥ 0 such that for all m ∈ [M] andi ∈ [N],
∣∣Vfim(x) -VF m(x)k ≤ ν, for all X ∈ Rd.
Assumption 2 requires that the difference between the gradient of each local component function
fim(x) and its corresponding local objective function F m (x) is uniformly bounded. It models the
variance of local components fim within each machine. While the uniform boundedness requirement
may look strong, we use this assumption to prove high-probability upper bounds, which are stronger
than the common in-expectation bounds. See Appendix A for comparisons with other assumptions,
and also Appendix D.7 for ways to avoid uniform boundedness over the entire Rd.
The next two assumptions capture the deviation across different machines, i.e., the degree of hetero-
geneity, in two different levels of granularity: objective-wise and component-wise.
Assumption 3 (Objective-wise inter-machine deviation). There exist τ ≥ 0 and ρ ≥ 1 such that
MM XM=1∣VF m(x)k ≤ T + P ∣VF (x)∣, for all X ∈ Rd.
Assumption 3 models the heterogeneity by bounding the mean of ∣VF m ∣ by a constant plus a
multiplicative factor times ∣∣VF ∣∣. The assumption includes the homogeneous case (i.e., F1 =…=
FM = F) by τ = 0 and ρ = 1. Assumption 3 is weaker than many other heterogeneity assumptions
in the literature (e.g., Karimireddy et al. (2020)); see Appendix A for detailed comparisons.
Assumption 3 measures heterogeneity by only considering the local objectives F m, not the local
components fim . We consider a more fine-grained notion of heterogeneity in Assumption 4:
Assumption 4 (Component-wise inter-machine deviation). Forall i ∈ [N], let f :=吉 PM=I fim.
There exist λ ≥ 0 such that for all m ∈ [M] and i ∈ [N],
IIVfm(X)-Vfi(X)Il ≤ λ, for all X ∈ Rd.
Assumption 4 states that the gradients of the i-th components of local machines are “close” to each
other. The assumption subsumes the component-wise homogeneous setting, i.e., f1 = ff2 =…=
fiM , by λ = 0. In distributed learning, this choice corresponds to the setting where each machine
has the same training dataset. Assumption 4 with λ > 0 is also relevant to the case where each
device has a slightly perturbed (e.g., by data augmentation techniques) version of a certain dataset.
It is straightforward to check that Assumption 4 implies Assumption 3 with τ = λ and ρ = 1.
We conclude this section by defining the function classes we study in this paper.
Definition 1 (Function classes). We consider two classes of global objective functions F, also taking
into account their local objectives F m and local components fim. We assume throughout that fim
are differentiable and F is bounded from below.
Fobj (L, μ, ν,τ,ρ) := {F | F is μ-PL; fm are L-smooth; F, Fm, fim satisfy Assumptions 2 & 3},
FcmP (L, μ, ν, λ) := {F | F is μ-PL; fm are L-smooth; F, Fm, fim satisfy Assumptions 2 & 4}.
Notice that Fɔbj (L, μ, ν, τ, ρ) ⊃ FcmP(L, μ, ν, T) for any P ≥ 1. We only make the PL assumption
on the global objective F, not on the local objectives F m nor on the local components fim . Using
L and μ, We define the condition number K := L∕μ ≥ 1.
3 Convergence analysis of minibatch and local RR
3.1 Upper bound for minibatch RR
We first begin with the convergence result for minibatch RR on FObj (L, μ, ν, τ, ρ), which exhibits
a faster large-epoch rate compared to the single-machine setting. For upper bounds, we use O(∙) to
hide universal constants and logarithmic factors of δ, M, N, K, and B.
Theorem 1 (Upper bound for minibatch RR). Suppose that minibatch RR has parameters satisfying
Assumption 1. For any F ∈ Fobj(L,μ,ν,τ,ρ), consider running the algorithm using step-size
η = B Iog(MNK ) for epochs K ≥ 6κ log(MNK2). Then, with probability at least 1 一 δ,
F(XK B ) - F * ≤
F(X0)— F* + O (L2	V2	ʌ
MNK2	+	V3 MNK2).
(3)
4
Published as a conference paper at ICLR 2022
Proof. The proof is in Appendix D.2. The key challenge in the convergence analysis of our
shuffling-based method stems from the indices sampled within an epoch being dependent on each
other. For example, if f1m is accessed already, then the index i = 1 will not be used in later iterations
of the epoch; this dependence significantly complicates the analysis. Our approach starts with real-
izing that for any permutation σ, PN=I fmi) = NFm. We decompose gradients Vfmm(j)(xk,i—i)
(see Line 10 of Algorithm 2) into V于^⑺口⑪ plus noise, then aggregate all updates over an
epoch to get “one big step of GD plus noise”: xk+1,0 = xk,0 - ηN VF (xk,0) + η2rk. We bound
the noise rk using Lemma 8 (Appendix D.6), which is our extension of the Hoeffding-Serfling in-
equality to the mean of Mindependent without-replacement sums of vectors; the lemma might be of
independent interest too. Lemma 8 shows that averaging accumulated gradients over M machines
reduces variance by M, which leads to the reduction by a factor of M in the bound (3).
□
Theorem 1 shows that for large enough epochs K & κ, minibatch RR converges at a rate of
22	22
O( μ3MNκ2)，with high probability. Compared to the large-epoch rate O( /彘2) of single-
machine RR (e.g., Ahn et al. (2020)), we see an additional factor Min the denominator, which
highlights the advantage of multiple machines. If we compare against the with-replacement coun-
terpart, itis known that for strongly convex and smooth F, the optimal convergence rate of minibatch
SGD is Θ( .MnK )5 which is worse than our bound (3) if K & κ2. Also notable is that the con-
vergence rate does not depend on the heterogeneity constants (i.e., τ and ρ from Assumption 3) of
the local objective functions. This observation that minibatch RR is “immune” to heterogeneity is
consistent with minibatch Sgd in the with-replacement setting (Woodworth et al., 2020b).
Epoch vs communication complexity. One might wonder why (3) does not have the batch size B .
In (3), we wrote convergence rates in terms of epochs K, which captures the gradient computation
complexity because the same number of gradients are evaluated in a single epoch regardless ofB. If
we are interested in communication complexity instead, we can write (3) in terms of the number of
communication rounds R := NBK and get a rate of O(μLMBNR2). From these, We can also discuss
the overall cost of the algorithm. If the cost of a communication round is cc, and the cost of local
gradient computations over an epoch is ce, then the total cost to obtain an -accurate solution is
Cminibatch() = O
CcV √N ι
B√M + √MNlJ,
ceν
(4)
omitting L and μ for simplicity. The total cost shows that there is essentially no harm increasing the
batch size B in minibatch RR, as we can get more accurate estimates of true gradients as B becomes
larger. In the next subsection, we will see that this is not the case in local RR.
What about K . κ? We remark that all upper bounds in this paper hold only for the “large-epoch”
regime, where K & κ. Such requirements are common in the literature of without-replacement
SGD (Haochen & Sra, 2019; Nagaraj et al., 2019; Rajput et al., 2020; Ahn et al., 2020), and there is
a recent result (Safran & Shamir, 2021) suggesting that faster convergence of without-replacement
SGD may not be possible in the K . κ regime. We defer a more detailed discussion on this regime
to Section 4, after Theorem 3.
3.2 Upper bound for local RR
Next, we are interested in how fast local RR can converge, what is the optimal batch size B, and
whether local RR can be as fast as minibatch RR.
Theorem 2 (Upper bound for local RR). Suppose that local RR has parameters satisfying As-
sumption 1. For any F ∈ Fobj(L, μ,ν,τ, ρ), consider running the algorithm using step-size
η = log(μNNκ ) for epochs K ≥ 7ρκ log(MNK2). Then, with probability at least 1 一 δ,
F(yK N) - F * ≤
F (y0) 一 F * 否(L2	( v2 v 2B τ 2B2	))
MNK2	+ Vm3 MNK2	+ N 2K2	+ N 2K2	.
(5)
5The optimal rate for (with-replacement) SGD after R iterations is Θ(μνR) (see e.g., Rakhlin et al. (2012)).
With-replacement minibatching reduces the variance V2 to MMMB, and R = NBK. However, achieving the optimal
rate for last iterates typically requires carefully designed step-size schemes (Jain et al., 2019).
5
Published as a conference paper at ICLR 2022
Proof. The proof is in Appendix D.3. We take the same “big GD step plus noise” approach as in
Theorem 1; however, due to local updates, bounding the noise is much more involved. In the proof,
We obtain the epoch update yk+1,0 = yk,o - nNVF(xk,o) + η2rk,1 + η2rk,2 - η3rk,3, where rk,ι
and rk,3 contain errors introduced by local updates. Noise from local updates accumulates over B
iterations, which cannot be remedied by averaging over M machines. They result in two additional
terms in the rate (5), one from intra-maChine variance and the other from heterogeneity.	□
3.2.1 Discussion of Theorem 2
Let us compare our high-probability bound (5) with existing in-expectation bounds. For strongly
L 2	L2 2B L2 2B2 6
convex F, the corresponding last-iterate bound of local SGD is O( μ2MNκ + μLSB + ~LN2K2 )6
(Khaled et al., 2020; Spiridonoff et al., 2020; Qu et al., 2020). Notice that (5) is better than this
with-replacement bound when K & κ. For average iterates, there are known bounds O( .MNK +
*熏B2 + ,NB22 )6 (Koloskova et al., 2020; Woodworth et al., 2020b) which are smaller than the
last-iterate bound by a factor of κ. It is unclear if averaging iterates could improve our rate, because
most such analyses exploit Jensen’s inequality, which we cannot use for nonconvex F.
Dependence on τ and ρ. Out of the two heterogeneity constants τ and ρ (Assumption 3), ρ does
not appear in (5), and it only affects the epoch requirement K & ρκ. Consider the case τ = 0 and
ρ > 1, which is heterogeneous but in the “interpolation regime,” because VFm(x) = 0 whenever
VF (x) = 0. In such a case, the rate (5) is equal to the homogeneous case.
Using B = Θ(N) is no better than single-machine. A close look at Theorem 2 reveals a rather
surprising fact. Even in the homogeneous case (τ = 0), if we choose B = Θ(N), then local RR
converges at the rate of O(NK2): the same rate as the single-machine RR! In Section 4, we show
that this observation is not due to a suboptimal analysis; the rate O(NK2) is tight for B = Θ(N).
Trade-off in the choice of B. As done for Theorem 1, we can compute from (5) that the total cost
of local RR for e-accuracy is (omitting L and μ for simplicity)
Clocal(e)= O(cc( ν√NΓ	+ √⅛	+ √ )+ Ce( √⅛Γ +	ν√B	+ (B)).	(6)
V ∖B√Me BBe e[^) VVMNe	NVe Nyfi)
Note that for local RR, there exists a trade-off between communication and epoch complexity in the
choice of B . If B is too small, this reduces the number of epochs required but increases communi-
cation costs. On the other hand, if B is too large, this reduces communication rounds but errors that
accumulate in local updates get severer, resulting in the need for more epochs. Hence, the optimal
choice of B must balance the two complexity measures. The existence of this trade-off is indeed
different from minibatch RR where larger B always reduces the total cost Cminibatch (e).
When can local RR match minibatch RR? Comparing the convergence rates (3) and (5), we
can identify some regimes in which local RR converges as fast as minibatch RR. In a nutshell, if
machines are not too heterogeneous and communication happens frequently, then local RR can have
the same upper bound as minibatch RR. For example, if B is chosen to be a constant, M . N,
and T . v'N/M, then the O( *34晨2) term in (5) becomes the dominating factor and hence
matches (3). Another example of such a regime is when B . M and T . ν,M∕N. Note that this
comparison assumes that the same values of B are chosen for both algorithms. Also, such “frequent
communication” regimes are favorable if the communication cost cc is small.
Can local RR ever beat minibatch RR? The upper bounds (3) and (5) indicate that local RR is
always no better than minibatch RR, at least for the function class Fobj (L, μ, ν, τ, ρ). This is in fact
consistent with Woodworth et al. (2020a;b), because the authors identify a regime where local Sgd
performs better than minibatch Sgd for convex objective functions, but fail to do so for strongly
convex functions. However, as was also pointed out in Woodworth et al. (2020a), there is a simple
extreme scenario in which local RR can be faster: when ν ≈ T ≈ 0 and ρ ≈ 1. In this case, we
have fim ≈ F for all m and i, so local RR corresponds to NK steps of GD, whereas minibatch RR
corresponds to NK steps of GD. Clearly, local RR will converge faster, exploiting the advantage of
more updates. Finding out other SUCh regimes is an important future direction. 6 *
6Due to differences in assumptions, many existing rates cannot be compared directly. These rates are the
ones we consider “comparable” to our bound. See Appendix A for more detailed comparisons.
6
Published as a conference paper at ICLR 2022
4 Matching lower bounds
In Section 3, we presented large-epoch upper bounds (i.e., for K & κ) for constant step-size mini-
batch and local RR. In this section, we prove matching lower bounds to show that the upper bounds
are tight, in all factors except L and μ. We use Ω(∙) to hide universal constants in lower bounds.
4.1 Lower bound for minibatch RR
Theorem 3 (Lower bound for minibatch RR). Suppose that minibatch RR has parameters satisfying
Assumption 1. Additionally, assume that N is a multiple of 2. Then, there exist large enough
constants c1,c2 > 0 such that the following holds: For L and μ satisfying K = L ≥ % there exists
afunction F ∈ FcmP (L, μ,ν, 0) such that for any constant step-size η,
E hF (xκ, K) - F *] = [： I μMNK)
I ω ∖μMNK2
ifK < c2κ,
if K ≥ c2 κ.
(7)
Proof. We prove Theorem 3 in Appendix E. The proof is an extension of Rajput et al. (2020); Safran
& Shamir (2020; 2021) to minibatch RR. We will sketch some key intuitions after Theorem 4.	□
First notice that the function F is from FcmP (L, μ, ν, 0), where allthe machines are component-wise
homogeneous. As seen in Definition 1, FcmP(L, μ, ν, 0) ⊂ FObj (L, μ, ν, τ, P) for any T ≥ 0 and
P ≥ 1, so Theorem 3 provides a lower bound for Fcmp(∙) and FObj (∙), with arbitrary heterogeneity
constants. We assume that N is even because we construct functions g1 and g2 such that fim := g1
if i ≤ nN , and fim := g2 if i > /.One can remove this assumption by using a zero function when
N is odd (see e.g., Safran & Shamir (2020)). It is rather unsatisfactory that our theorem requires
large enough constants c1 and c2; we believe a tighter analysis can relax this restriction.
Theorem 3 proves lower bounds for two different regimes: K & κ and K . κ. In the large-epoch
2
regime (K & κ), we can observe that the lower bound Ω( μMNκ) matches the upper bound (3) in
Theorem 1, modulo a factor of κ2 . Tightening the κ2 gap between upper and lower bounds is left
2
for future work. In the small-epoch regime (K . κ), we observe that the lower bound Ω( .MNk )
exactly matches the convergence rate of (with-replacement) minibatch Sgd; hence, the lower bound
implies that minibatch RR has no hope for faster convergence than minibatch Sgd, at least in the
constant step-size and small-epoch regime. This observation is in line with Safran & Shamir (2021).
Upper bounds for K . κ? Even for single-machine RR (M = 1), proving an upper bound that
2
matches the small-epoch lower bound Ω(-NK) still remains a challenge. Nagaraj et al. (2019, The-
2
orem 2) prove an upper bound for non-quadratic strongly convex functions that matches Ω( μNκ) if
N K & κ2 ; however, they use suffix averaging, so it is not directly comparable to Theorem 3 which
considers last iterates. Safran & Shamir (2021) prove upper bounds for quadratic strongly convex
functions, but assume that their Hessian matrices commute. For noncommutative cases, proving a
small-epoch upper bound seems to require some form of matrix AM-GM inequalities, whose avail-
ability is an open problem (Recht & Re, 2012; Lai & Lim, 2020; De Sa, 2020; Yun et al., 2021).
Remark 1 (Strong convexity in construction). We note that all lower bounds in this paper are con-
structed with strongly convex functions, a stronger assumption than PL functions (2). Thus, our
lower bounds are also applicable to strong convexity counterparts of FObj (∙) and Fcmp(∙).
4.2 Lower bounds for local RR
In this subsection, we present lower bounds for local RR. We prove two bounds that correspond to
homogeneous and heterogeneous cases. By combining the two bounds, we get a lower bound that
matches our upper bound (5) in Theorem 2 up to a factor of κ2.
Theorem 4 (Lower bound for local RR: homogeneous case). Suppose that local RR has parameters
satisfying Assumption 1. Additionally, assume that B is a multiple of 4. Then, there exist large
enough constants c3,c4 > 0 such that the following holds: For L and μ satisfying K = L ≥ c3,
there exists afunction F ∈ FcmP (L, μ, ν, 0) such that for any constant step-size η,
E hF ®k,k ) - F *] = [：1 μMNκ)+	.2B
ω ∖μMNK2 + μN2K2
if K < max c4K
if K ≥ max c4K
MB
N~ J，
MB 1
NT J∙
(8)
7
Published as a conference paper at ICLR 2022
Proof. The proof is in Appendix G. For the large-epoch lower bounds in Theorems 3 and 4, we
use “skewed" quadratics fim(x) = (L1χ≤o + μ1χ>o) x2 + zwx, where Zi = +1 if i ≤ N and
zi = -1 otherwise. For x ≈ 0, the imbalance results in a “drift” towards positive x, whose strength
is approximately proportional to the absolute value of partial sums of random permutations over N
+1's and NN -1's. By averaging the sums over M machines (minibatch RR), their absolute values
shrink by √M; in contrast, if each machine makes local updates (local RR), the magnitude of the
drift cannot be reduced with M, because we average after local iterates already have taken B “big”
steps. The proof uses techniques from Rajput et al. (2020).	□
Proposition 5 (Lower bound for local RR: heterogeneous case). Suppose that local RR has param-
eters satisfying Assumption 1. Additionally, assume that B is a multiple of 2 and K = L ≥ 2. Then,
there exists afunction F ∈ Fobj (L,μ, 0,τ, 1) such thatfor any constant step-size η,
τ2B2
E [F (yκ, κ)- F *]=ω (k I	⑼
Proof. We note that Proposition 5 is almost identical to Theorem II of Karimireddy et al. (2020);
however, we provide a proof specific to our algorithm in Appendix I.	□
Theorem 4 constructs a component-wise homogeneous function from FcmP(L, μ, ν, 0) and Propo-
sition 5 constructs a heterogeneous function from Fobj(L,μ, 0,τ, 1). Since FcmP(L,μ,ν, 0) ∪
Fobj(L,μ, 0,τ, 1) ⊂ Fobj(L,μ,ν,τ,ρ) for any P ≥ 1, combining (8) and (9) for the K ≥
max{c4κ, MB} case gives a lower bound Ω(max{ *MvK2 + μN^EK2, μNBK∑}) that matches the
large-epoch upper bound (5) in Theorem 2, up to a factor of κ2 . When κN & MB, c4κ becomes
the dominating term in the max, in which case the threshold in (8) is Θ(κ). Tightening the κ2 gap as
well as removing additional requirements such as κ ≥ c3 and κN & MB are left for future work.
Using B = Θ(N) does not help, indeed. In Section 3.2.1, we observed that if B = Θ(N), then
even in the homogeneous case (τ = 0), local RR converges at the rate of O(NKI). This is the same
rate as single-machine RR, meaning the efforts by M - 1 machines become meaningless. Our lower
bound (8) shows that O(NKI) is in fact the best we can hope for (treating L and μ as constants).
In order to make the best use of M machines, B should be smaller than Θ(N), as suggested in
Section 3.2.1. In an existing work, Mishchenko et al. (2021) consider local RR with B = N as a
special case of a proximal algorithm. In Theorem 8 of Mishchenko et al. (2021), the authors claim
“the convergence bound improves with the number of devices involved” because the bound has a
factor of M in the denominator. However, at least under our assumption, this is not the case; if we
apply our Assumption 3 to upper-bound their σ*, the term "Nσ2" in the numerator grows linearly
with M. Hence, our bounds do not contradict Mishchenko et al. (2021); see Appendix A for details.
Remark 2 (Small-epoch bound is likely loose). We note that while we focused on deriving a match-
ing large-epoch lower bound, we did not try hard to tighten the small-epoch lower bound. Our
2B
small-epoch lower bound in (8) misses a term (such as .N2K2) that corresponds to the error from
local updates. We leave investigations on small-epoch lower and upper bounds for future work.
5 S ynchronized shuffling: how to bypass lower bounds
Recall from the total complexity of minibatch RR (4) that the total cost shrinks with a factor of √=.
Using M machines, we are only getting a √M-factor speedup. Ideally, we hope to see a linear
speedup, i.e., cost inverse proportional to M. Hence, Theorem 1 falls short of achieving this goal,
and our lower bound in Theorem 3 confirms that linear speedup is indeed impossible.
In this section, we show that the desired linear speedup is possible, at least in some special cases.
We consider the component-wise near-homogeneous case (i.e., Assumption 4 with small λ) and
discuss how a simple modification to minibatch and local RR can let us “break” the lower bounds
and achieve linear speedup. This comes at a cost of broadcasting permutations: at the beginning
of the k-th epoch, the server samples σ 〜Unif(SN) and ∏ 〜Unif(SM), and broadcasts them
to the machines. Then, local machines choose their permutations σkm to be shifted versions of σ,7
i.e., σ7m(i) := σ ((i + Mπ(m)) mod N). We call this trick synchronized shuffling, denoted as
7We assume for simplicity that M divides N .
8
Published as a conference paper at ICLR 2022
SyncShuf. Please revisit Algorithms 1 and 2 for the precise descriptions of the modified algo-
rithms local RR with SyncShuf and minibatch RR with SyncShuf, respectively.
The intuition why this should help is simple. In the proof of RR, we aggregate the component gra-
dients over an epoch (i.e., N iterations) to write it as a full gradient plus noise. If we are in the
component-wise homogeneous setting and permutations are synchronized, then instead of aggre-
gating N component gradients on a single machine, We can aggregate N component gradients on
M machines to get a full gradient. This allows us to reduce the “noise” from without-replacement
sampling. We emphasize here that we do not necessarily set B = N to get a full gradient every
time; our analysis Works for arbitrary B and M, as long as both divide N. See Appendix B for a
detailed illustration of SyncShuf; also, see Appendix C for experiments showing its effectiveness.
The idea of synchronized shuffling is similar to approaches in distributed learning that shuffle and
partition datasets and distribute them to local machines (see e.g., Lee et al. (2017); Meng et al.
(2017)). In contrast, we do not communicate data, but communicate how to permute datasets stored
in local machines. Meng et al. (2017, Theorem 3.3) provide an analysis for a distributed method
similar to minibatch RR, but fail to show convergence to global minima in strongly convex cases.
We also note that an independent concurrent result (Szlendak et al., 2021) uses the same idea as
SyncShuf to build compressors for communication-efficient distributed optimization.
5.1	Upper bounds for minibatch and local RR with SyncShuf
With SYNCSHUF, we can show that the M ’s appearing in the convergence rates ((3) and (5)) in
Theorems 1 and 2 can be replaced with M2, for a more stringent function class Fcmp (∙) that requires
bounded component-wise inter-machine deviation (Assumption 4).
Theorem 6 (Upper bound for minibatch RR with SYNCSHUF). Suppose that minibatch RR with
SYNCSHUF has parameters satisfying Assumption 1. Additionally assume that M divides N. For
any F ∈ FcmP(L, μ, ν, λ), consider running the algorithm using step-size η = B bog(MNjNK ) for
epochs K ≥ 6κ log(M 2N K2). Then, with probability at least 1 - δ,
F(XK,B)-F* ≤ FMlNKF + O (L3 (MNK + MK)).	(10)
The proof of Theorem 6 is presented in Appendix D.4. One can check that if the component-
wise deviation constant λ satisfies λ . √MMN (i.e., near-homogeneous), then the rate (10) becomes
0( M2Nk2 ). It is then easy to confirm that M machines reduce total costs by M —a linear speedup.
A similar speedup can be shown for local RR. In Appendix D.5, we prove that
Theorem 7 (Upper bound for local RR with SYNCSHUF). Suppose that local RR with SYNCSHUF
has parameters satisfying Assumption 1. Additionally assume that M divides N. For any F ∈
FcmP(L, μ, ν, λ), consider running the algorithm with step-size η = "虱MNNK)for epochs K ≥
7κ log(M2NK2). Then, with probability at least 1 - δ,
F(	)_ F* ≤ F(yo) - F* + o (L2 ( V2
(yK,B) -	≤ M2NK2 +	(μ3 (M2NK2
ν 2B	λ2B2
+ N 2K2 + N 2K2 +
MK)).(II)
We can similarly check that if B . MN and λ . √Mn , i.e., frequent communication and near-
homogeneity, then the 0( M 2 N K) term dominates in (11), and hence gives a linear speedup that
matches the best rate of minibatch RR with SyncShuf (10). Nevertheless, we note again that for
local RR, such a small B is favorable only when the communication cost cc is small (recall (6)).
6 Conclusion
We studied convergence bounds for local RR and minibatch RR, which are the practical without-
replacement versions of local and minibatch Sgd studied in the theory literature. For smooth func-
tions satisfying the Polyak土OjaSieWiCz condition, we showed large-epoch convergence bounds for
minibatch and local RR that are faster than their with-replacement counterparts. We also proved
matching lower bounds showing that our convergence analysis is tight. We also proposed a sim-
ple modification called synchronized shuffling that leads to convergence rates faster than our lower
bounds in near-homogeneous settings. Immediate future research directions include extension to
small-epoch regimes, as well as to general convex and nonconvex functions.
9
Published as a conference paper at ICLR 2022
Ethics S tatement
This paper develops theoretical guarantees for popular distributed stochastic optimization algo-
rithms. Therefore, the authors do not see any particular concerns related to its ethical aspects or
future societal consequences.
Reproducibility S tatement
This paper is a theoretical work, without any experimental results. Definitions and assumptions are
provided in Section 2. Our theoretical contributions as well as some additionally required assump-
tions are clearly stated in Sections 3, 4, and 5. Complete proofs of all the theorems are provided in
the appendix.
References
Kwangjun Ahn, Chulhee Yun, and Suvrit Sra. SGD with shuffling: optimal rates without component
convexity and large epoch requirements. In Advances in Neural Information Processing Systems,
2020.
Leon Bottou. Curiously fast convergence of some stochastic gradient descent algorithms. In Pro-
ceedings of the symposium on learning and data science, Paris, 2009.
Christopher M De Sa. Random reshuffling is not always better. Advances in Neural Information
Processing Systems, 33, 2020.
Aymeric Dieuleveut and Kumar Kshitij Patel. Communication trade-offs for local-sgd with large
step size. Advances in Neural Information Processing Systems, 32:13601-13612, 2019.
Mert Gurbuzbalaban, ASu Ozdaglar, and Pablo Parrilo. Why random reshuffling beats stochastic
gradient descent. Mathematical Programming, pp. 1-36, 2019.
Farzin Haddadpour and Mehrdad Mahdavi. On the convergence of local descent methods in feder-
ated learning. arXiv preprint arXiv:1910.14425, 2019.
Farzin Haddadpour, Mohammad Mahdi Kamani, Mehrdad Mahdavi, and Viveck Cadambe. Local
sgd with periodic averaging: Tighter analysis and adaptive synchronization. Advances in Neural
Information Processing Systems, 32:11082-11094, 2019.
Jeff Haochen and Suvrit Sra. Random shuffling beats SGD after finite epochs. In International
Conference on Machine Learning, pp. 2624-2633, 2019.
Prateek Jain, Dheeraj Nagaraj, and Praneeth Netrapalli. Making the last iterate of sgd information
theoretically optimal. In Conference on Learning Theory, pp. 1752-1755. PMLR, 2019.
Peter Kairouz, H. Brendan McMahan, Brendan Avent, AUrelien Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael
G. L. D’Oliveira, Hubert Eichner, Salim El Rouayheb, David Evans, Josh Gardner, Zachary
Garrett, Adria Gascon, Badih Ghazi, Phillip B. Gibbons, Marco Gruteser, Zaid Harchaoui,
Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Ja-
vidi, Gauri Joshi, Mikhail Khodak, Jakub Konecny, Aleksandra Korolova, Farinaz Koushanfar,
Sanmi Koyejo, TanCrede Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer
Ozgur, Rasmus Pagh, Hang Qi, Daniel Ramage, Ramesh Raskar, Mariana Raykova, Dawn Song,
Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tramer, Pra-
neeth Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and
Sen Zhao. Advances and open problems in federated learning. Foundations and Trends® in
Machine Learning, 14(1-2):1-210, 2021. ISSN 1935-8237. doi: 10.1561/2200000083. URL
http://dx.doi.org/10.1561/2200000083.
Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-
gradient methods under the Polyak-Icjasiewicz condition. In Joint European Conference on Ma-
chine Learning and Knowledge Discovery in Databases, pp. 795-811. Springer, 2016.
10
Published as a conference paper at ICLR 2022
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In
International Conference on Machine Learning, pp. 5132-5143. PMLR, 2020.
Ahmed Khaled, Konstantin Mishchenko, and Peter Richtarik. Tighter theory for local Sgd on identi-
cal and heterogeneous data. In International Conference on Artificial Intelligence and Statistics,
pp. 4519-4529. PMLR, 2020.
Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian Stich. A unified
theory of decentralized sgd with changing topology and local updates. In International Confer-
ence on Machine Learning, pp. 5381-5393. PMLR, 2020.
Jakub Konecny, H Brendan McMahan, Felix X Yu, Peter Richtarik, Ananda Theertha Suresh, and
Dave Bacon. Federated learning: Strategies for improving communication efficiency. arXiv
preprint arXiv:1610.05492, 2016.
Zehua Lai and Lek-Heng Lim. ReCht-Re noncommutative arithmetic-geometric mean conjecture is
false. In International Conference on Machine Learning, 2020.
Kangwook Lee, Maximilian Lam, Ramtin Pedarsani, Dimitris Papailiopoulos, and Kannan Ram-
chandran. Speeding up distributed machine learning using codes. IEEE Transactions on Infor-
mation Theory, 64(3):1514-1529, 2017.
Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges,
methods, and future directions. IEEE Signal Processing Magazine, 37(3):50-60, 2020a.
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia
Smith. Federated optimization in heterogeneous networks. In I. Dhillon, D. Papailiopou-
los, and V. Sze (eds.), Proceedings of Machine Learning and Systems, volume 2, pp.
429-450, 2020b. URL https://proceedings.mlsys.org/paper/2020/file/
38af86134b65d0f10fe33d30dd76442e- Paper.pdf.
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of
fedavg on non-iid data. In International Conference on Learning Representations, 2020c.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial intelli-
gence and statistics, pp. 1273-1282. PMLR, 2017.
Qi Meng, Wei Chen, Yue Wang, Zhi-Ming Ma, and Tie-Yan Liu. Convergence analysis of distributed
stochastic gradient descent with shuffling. arXiv preprint arXiv:1709.10432, 2017.
Konstantin Mishchenko, Ahmed Khaled, and Peter Richtarik. Random reshuffling: Simple analysis
with vast improvements. arXiv preprint arXiv:2006.05988, 2020.
Konstantin Mishchenko, Ahmed Khaled, and Peter Richtarik. Proximal and federated random
reshuffling. arXiv preprint arXiv:2102.06704, 2021.
Dheeraj Nagaraj, Prateek Jain, and Praneeth Netrapalli. SGD without replacement: Sharper rates
for general smooth convex functions. In International Conference on Machine Learning, pp.
4703-4711, 2019.
Lam M. Nguyen, Quoc Tran-Dinh, Dzung T. Phan, Phuong Ha Nguyen, and Marten van Dijk. A uni-
fied convergence analysis for shuffling-type gradient methods. arXiv preprint arXiv:2002.08246,
2020.
Iosif Pinelis. An approach to inequalities for the distributions of infinite-dimensional martingales.
In Probability in Banach Spaces, 8: Proceedings of the Eighth International Conference, pp.
128-134. Springer, 1992.
Iosif Pinelis. Optimum bounds for the distributions of martingales in banach spaces. The Annals of
Probability, pp. 1679-1706, 1994.
11
Published as a conference paper at ICLR 2022
Zhaonan Qu, Kaixiang Lin, Jayant Kalagnanam, Zhaojian Li, Jiayu Zhou, and Zhengyuan Zhou.
Federated learning’s blessing: Fedavg has linear speedup. arXiv preprint arXiv:2007.05690,
2020.
Shashank Rajput, Anant Gupta, and Dimitris Papailiopoulos. Closing the convergence gap of SGD
without replacement. In International Conference on Machine Learning, 2020.
Shashank Rajput, Kangwook Lee, and Dimitris Papailiopoulos. Permutation-based sgd: Is random
optimal? arXiv preprint arXiv:2102.09718, 2021.
Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for
strongly convex stochastic optimization. In Proceedings of the 29th International Coference on
International Conference on Machine Learning, pp. 1571-1578, 2012.
Benjamin Recht and Christopher Re. Toward a noncommutative arithmetic-geometric mean inequal-
ity: conjectures, case-studies, and consequences. In Conference on Learning Theory, pp. 11-1,
2012.
Itay Safran and Ohad Shamir. How good is SGD with random shuffling? In Conference on Learning
Theory, pp. 3250-3284. PMLR, 2020.
Itay Safran and Ohad Shamir. Random shuffling beats SGD only after many epochs on ill-
conditioned problems. arXiv preprint arXiv:2106.06880, 2021.
Markus Schneider. Probability inequalities for kernel embeddings in sampling without replacement.
In Artificial Intelligence and Statistics, pp. 66-74, 2016.
Robert J Serfling. Probability inequalities for the sum in sampling without replacement. The Annals
of Statistics, pp. 39-48, 1974.
Artin Spiridonoff, Alex Olshevsky, and Ioannis Ch Paschalidis. Local sgd with a communication
overhead depending only on the number of workers. arXiv preprint arXiv:2006.02582, 2020.
Sebastian U Stich. Local sgd converges fast and communicates little. In International Conference
on Learning Representations, 2019.
Sebastian U Stich and Sai Praneeth Karimireddy. The error-feedback framework: Better rates for
sgd with delayed gradients and compressed updates. Journal of Machine Learning Research, 21:
1-36, 2020.
RafaI Szlendak, Alexander Tyurin, and Peter Richtarik. Permutation compressors for provably faster
distributed nonconvex optimization. arXiv preprint arXiv:2110.03300, 2021.
Trang H Tran, Lam M Nguyen, and Quoc Tran-Dinh. Smg: A shuffling gradient-based method with
momentum. In International Conference on Machine Learning, pp. 10379-10389. PMLR, 2021.
Blake Woodworth, Kumar Kshitij Patel, Sebastian Stich, Zhen Dai, Brian Bullins, Brendan Mcma-
han, Ohad Shamir, and Nathan Srebro. Is local SGD better than minibatch SGD? In International
Conference on Machine Learning, pp. 10334-10343. PMLR, 2020a.
Blake Woodworth, Brian Bullins, Ohad Shamir, and Nathan Srebro. The min-max complexity
of distributed stochastic convex optimization with intermittent communication. arXiv preprint
arXiv:2102.01583, 2021.
Blake E Woodworth, Kumar Kshitij Patel, and Nati Srebro. Minibatch vs local SGD for heteroge-
neous distributed learning. Advances in Neural Information Processing Systems, 33:6281-6292,
2020b.
Hao Yu, Sen Yang, and Shenghuo Zhu. Parallel restarted sgd with faster convergence and less
communication: Demystifying why model averaging works for deep learning. In Proceedings of
the AAAI Conference on Artificial Intelligence, volume 33, pp. 5693-5700, 2019.
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Open problem: Can single-shuffle SGD be better than
reshuffling SGD and GD? In Conference on Learning Theory, pp. 4653-4658. PMLR, 2021.
12
Published as a conference paper at ICLR 2022
Contents
1 Introduction	1
1.1 Our contributions ....................................................................... 2
2 Problem setup	2
3	Convergence analysis of minibatch and local RR	4
3.1	Upper bound for minibatch RR ........................................... 4
3.2	Upper bound for local RR ............................................... 5
4	Matching lower bounds	7
4.1	Lower bound for minibatch RR ................................................. 7
4.2	Lower bounds for local RR .............................................. 7
5	Synchronized shuffling: how to bypass lower bounds	8
5.1	Upper bounds for minibatch and local RR with SyncShuf .................... 9
6	Conclusion	9
A Comparisons with assumptions and rates in existing results	14
B More detailed illustration of synchronized shuffling	18
C	Experimental results	19
C.1 SyncShuf improves convergence of minibatch/local RR ....................... 20
C.2 Local RR becomes closer to single-machine RR as B → N ..................... 21
C.3 With- vs. without-replacement sampling .................................... 22
D	Proofs of upper bounds	23
D.1 Proof outline ............................................................. 24
D.2	Proof of upper bound	for minibatch RR (Theorem 1) ........................ 24
D.3	Proof of upper bound	for local RR (Theorem 2) ............................ 29
D.4	Proof of upper bound	for minibatch RR with SyncShuf (Theorem 6) .......... 35
D.5	Proof of upper bound	for local RR with SyncShuf (Theorem 7) .............. 39
D.6 A generalized vector-valued Hoeffding-Serfling inequality ................. 42
D.7 How can we avoid uniform bounds over Rd in our assumptions? ............... 44
E Proof of lower bound for minibatch RR	(Theorem 3)	46
E.1	Lower bound for η	≤	μNκ................................................... 47
E.2	Lower bound for η	≥	μNκ	and η	≤	^1BLN................................... 47
E.3	Lower bound for η	≥	μNκ	and η	≥	^^BLN................................... 50
13
Published as a conference paper at ICLR 2022
F Proofs of helper lemmas for Appendix E	52
F.1	Proof of Lemma	11 .................................................... 52
F.2	Proof of Lemma	12 .................................................... 55
F.3	Proof of Lemma	13 .................................................... 55
F.4	Proof of Lemma	14 .................................................... 57
F.5	Proof of Lemma	15 .................................................... 59
G Proof of lower bound for local RR: homogeneous case (Theorem 4)	59
G.1	Lower bound for η	≤	μNκ.......................................... 60
G.2	Lower bound for η	≥	μN1κ and η	≤	m5LN............................ 60
G.3	Lower bound for η	≥	μN1κ and η	≥	m,LN............................ 64
H Proofs of helper lemmas for Appendix G	66
H.1	Proof of Lemma 16 ....................................................... 66
H.2	Proof of Lemma 17 ....................................................... 70
H.3	Proof of Lemma 18 ....................................................... 70
H.4	Proof of Lemma 19 ....................................................... 72
I Proof of lower bound for local RR: heterogeneous case (Proposition 5)	73
I.1	Lower bound for q ɪ ≤ η ≤ and ≤ η ≤ 1.............................. 73
8μNK — 8 — 8μB	8μB — / 一 μ
I.2	Lower bound for η ≤ &.NK and η ≥ μ ................................ 75
A Comparisons with assumptions and rates in existing results
In this section, we compare our assumptions and convergence bounds against other existing results
mentioned in the main text. Most existing results that study independent and unbiased gradient
estimates state their assumptions in terms of the expectation over the randomness in the estimate;
for such assumptions, we adapt them to our finite sum setting in order to make for easier comparison.
Heterogeneity assumptions. We start by discussing our definition of objective-wise heterogeneity
(Assumption 3), namely that there exist τ ≥ 0 and ρ ≥ 1 such that
1M
IM EkVFm(x)k≤ T + P kVF(x)k, for all X ∈ Rd.	(12)
Perhaps the most relevant to this assumption is the (G, B)-BGD assumption that appears in Karim-
ireddy et al. (2020): For all x ∈ Rd ,
1M
IM E kVFm(x)k2 ≤ G2 + B2 kVF(x)k2.	(13)
m=1
Note that thanks to Jensen’s inequality and a2 + b2 ≤ (a + b)2 for a, b ≥ 0, (13) implies
1M	2	1M
(MM ∑ kVFm(x)k∣ ≤ 而 Z kVFm(x)k2 ≤ G2 + B2 kVF(x)『≤ (G + B ∣∣VF(x)k)2,
and hence (12) with τ = G and ρ = B. Therefore, our Assumption 3 is weaker than the (G, B)-
BGD assumption. Several papers (Haddadpour & Mahdavi, 2019; Li et al., 2020b) use the same
14
Published as a conference paper at ICLR 2022
assumption (13), with G = 0; therefore, Assumption 3 also subsumes the heterogeneity assumption
posed in these papers. Note that G = 0 implies that, the minima for F are also the minima for Fm,
for every m, and hence G = 0 results in a weak form of heterogeneity.
Some papers (Yu et al., 2019; Li et al., 2020c; Qu et al., 2020) assume bounded local gradients: for
all m ∈ [M] and x ∈ Rd ,
1N
nn EkVfm(x)k2 ≤ G2,	(14)
i=1
and this in fact implies the (G, 0)-BGD assumption (13). To see why, from Jensen’s inequality
kVFm(x)k2 :
1N
nn X Vfm(X)
2	1N
≤ nn X kvfm(χ)k2 ≤ g2.
i=1
Therefore, (14) is a stronger assumption for objective-wise heterogeneity than Assumption 3.
In Theorem 3 of Woodworth et al. (2020b), the authors use the following assumption on heterogene-
ity: for all x ∈ Rd ,
1M
M ZkVFm(x)-VF(x)k2 ≤ Z2.	(15)
Noting that Mm Pm=I kVFm(X)- VF(X)『=MM Pm=I kVFm(x)k2 -kVF(X)k2, We can see
that (15) implies (13) and hence Assumption 2 (12), with T =《and P = 1.
Indeed, there are also some results that make Weaker heterogeneity assumptions than ours (12), by
requiring bounded deviation only at the global optimum x*. Given the global optimum x* of F,
Koloskova et al. (2020) define
1M
〈2 = M EkVFm(x*)k2,	(16)
m=1
and use this constant in their bounds. Khaled et al. (2020) also define a similar quantity that can
capture heterogeneity, but does not provide a result on strongly convex functions in the heteroge-
neous setting. While assuming bounded ζ* (16) is weaker than Assumption 3 in the sense that only
a bound at X* is required, we note that these assumptions cannot be applied easily in nonconvex
settings; in fact, for nonconvex (but not necessarily PL) functions, Koloskova et al. (2020) also use
(13) as their heterogeneity assumption.
Intra-machine variance assumptions. We next consider our notion of intra-machine deviation
(Assumption 2), namely that there exists ν ≥ 0 such that for all m ∈ [M] and i ∈ [N ],
kVfim(X) -VFm(X)k ≤ ν, for all X ∈ Rd.	(17)
In many existing results using independent and unbiased gradient estimates (Karimireddy et al.,
2020; Yu et al., 2019; Li et al., 2020c; Qu et al., 2020; Woodworth et al., 2020a;b; 2021), the
bounded variance assumption is adopted: For all m ∈ [M] and X ∈ Rd,
1N
NN IkVfm(X)-VFm(x)k2 ≤ σ2.	(18)
We note that the bounded local gradients assumption (14) also implies (18), by
1N	1N
N E kVfm(X) - VFm(X)k2 ≤ NN E kVfm(X) - VFm(X)k2 + kVF"x)『
1N
=N IkVfm(X)『≤ G2.	(19)
15
Published as a conference paper at ICLR 2022
Some other papers (Haddadpour et al., 2019; Haddadpour & Mahdavi, 2019; Spiridonoff et al.,
2020) consider a generalized version of (18), namely that for all m ∈ [M] and x ∈ Rd,
1N
NN ∑ kVfim(x) - VFm(x)k2 ≤ C kVFm(x)k2 + σ2,	(20)
for c, σ ≥ 0. There are other papers (Koloskova et al., 2020; Khaled et al., 2020) that use intra-
machine variance at the global minimum x* in their bounds. Koloskova et al. (2020) define
MN
σ2 ：= M1N XXkVfim(x*)-VFm(x*)k2,	(21)
m=1 i=1
for the global minimum x*, and use it in their strong convexity and convexity bounds. Khaled et al.
(2020) define
MN
σθpt ：= M1N XXkVfim(x*)k2 ,	(22)
m=1 i=1
which is used in their bound on strongly convex functions in homogeneous cases. Note that for
homogeneous cases, VFm(x*) = VF(x*) = 0, so σ2 = σθp-
We note that in contrast to our discussion on inter-machine deviation (Assumption 3), the intra-
machine variance assumptions in the existing literature are weaker than our Assumption 2. However,
we utilize our stronger assumption to prove our high-probability upper bounds, which is a departure
from in-expectation bounds in the literature.
Existing upper bounds on local Sgd. In the discussion after Theorem 2, we mentioned some
recent upper bounds on (with-replacement) local Sgd. We make more detailed comparisons here.
For the reader’s convenience, we restate our theorem on local RR below.
Theorem 2 (Upper bound for local RR). Suppose that local RR has parameters satisfying As-
sumption 1. For any F ∈ Fobj(L,μ,ν,τ,ρ), consider running the algorithm using step-size
η = '。虱μMNK ) for epochs K ≥ 7ρκ log(MNK2). Then, with probability at least 1 一 δ,
F(yκ, B)- F * ≤
F(y0)- F* + O(三(V2	+ V2B_ + τ2B^C
MNK 2	+	V3	MNK2 + N 2K2 + N 2K2	.
(5)
We start with last-iterate bounds in homogeneous cases. Theorem 3 and Corollary 3 of Khaled
et al. (2020) consider local SGD on μ-strongly convex and L-smooth F. Khaled et al. (2020) allow
variable synchronization intervals, where the maximum interval is upper-bounded by B . In this
setting, Corollary 3 of Khaled et al. (2020) shows that local SGD after T total local update steps
yield
E UxT - x*k2i = O (kx0 -2x*k2 + ⅛ + LyJ 1)! ,	(23)
T 2	μ2M T	μ3T 2
where Xi is the average over all M machines, i-th local iterates, and σopt is from (22). Noting that
T corresponds to NK in local SGD and σopt corresponds to ν in Assumption 2, (23) is comparable
to an upper bound8
E [F(XT) — F*]= O ( 2 LVNK + *⅛) .	(24)
μ2MNK	μ3N2K 2 J
Here, We do not compare FMNK* and LkxT2x*k because in Theorem 2, the term FMNK* can
be made arbitrarily “small” (e.g., FMNKFz for any l ∈ N) by changing the log factor in η.
A similar homogeneous, strongly convex, and smooth setting is considered in Spiridonoff et al.
(2020), under a intra-machine variance assumption defined in (20). Theorem 1 of Spiridonoff et al.
(2020) proves general theorem statement for arbitrary synchronization intervals. Ifwe specialize to
constant interval B, Corollary 1 of Spiridonoff et al. (2020) gives
不、 N	H β2(F (Xo) - F *L Lσ2 * L2σ2(B - 1))
E[F (XT) - F ] = 01---------T--------+ μ2Mτ +	〃3T 2	b	(25)
8 Note that an additional factor L is due to conversion from squared distance to function value.
16
Published as a conference paper at ICLR 2022
where β ≥ 2κ2 is a constant defined to choose algorithm parameters such as the step-size. Noting
again that T corresponds to NK and σ in (20) is comparable to ν in Assumption 2, (25) also
translates to (24).
The next last-iterate bound we compare against is Qu et al. (2020). Theorem 1 of Qu et al. (2020)
uses bounded intra-machine variance assumption (18) and bounded gradient assumption (14). Spe-
Cianzing Theorem 1 of Qu et al. (2020) to full device participation and uniform weight (pi =…=
PN = N) case, their bound reads
E [F(XT) — F*] = O (-⅛ + LG12) .	(26)
μ2MT	μ3T 2 J
Recalling that T corresponds to NK, σ to ν in Assumption 2, and G to the heterogeneity constant
τ in Assumption 3 and also ν (due to (19)), (26) translates to
E [F(XT) — F*] = O ( 2LVNK + LH.T2严).	(27)
μ2MNK	μ3N 2K2	J
Comparing the local Sgd last-iterate bounds (24) and (27) against our local RR bound (5) in Theo-
rem 2, we can see that the last iterate of local RR satisfies a smaller upper bound as soon as K ≥ κ.
Admittedly, this is not a fully rigorous comparison given the differences in assumptions and types
of bounds; nevertheless, we believe that the comparison at least provides some degree of evidence
for faster convergence of local RR than local Sgd. It is also interesting to see that the “error from
local updates” terms match in with- and without-replacement bounds.
Next, we review existing average-iterate bounds mentioned in the main text, which are better than the
last-iterate bounds. Koloskova et al. (2020) present a unifying framework for analyzing distributed
optimization algorithms over networks, which can specialize to local Sgd. For μ-strongly convex
and L-smooth F, Theorem 2 of Koloskova et al. (2020) shows that
E [F(X) — F*] = O (LB kxo — x*『exp (— g) + ɪ +	+ LlBV)，	(28)
LB J	μM T	μ2 T 2	μ2 T 2 ,
where X is some weighted average ofiterates and Z2 and σ2 are defined in (16) and (21), respectively.
Noting that T corresponds to NK,《* to T in Assumption 3, and σ to V in Assumption 2 (although
our assumptions are stronger), we can see that the bound (28) for large enough T can be translated
into
E [F(χ) — F*] = O (_V_______+ LVB + LTBi2 )	(29)
[()	]= VMNK + μ2N2K2 + μ2N2K2) .	(9)
Notice that (29) is smaller than the last-iterate bounds (24) and (27) by a factor of κ. Similarly,
Theorem 3 of Woodworth et al. (2020b) proves that for μ-strongly convex and L-smooth F,
E [F(X) — F*] = O
L2 kxo — x*『+ 上 + Lσ2B + Lζ2B2
LT + μT2	+ μMT + μ2T2 + μ2T2
(30)
for some weighted average of iterates X. Here, σ2, σ2, and ζ2 are as defined in (21), (18), and (15),
respectively. Substituting V to its comparable constants σ* and σ, and T to Z we can similarly check
that (30) can be converted to (29).
Comparison to Mishchenko et al. (2021). In Mishchenko et al. (2021), the authors study a prox-
imal algorithm referred to as Proximal Random Reshuffling (ProxRR), and obtain a distributed op-
timization algorithm called FedRR as a special case. If we set R ≡ 0 in FedRR, the algorithm then
is equal to local RR with B = N, i.e., the one that synchronizes only after one entire epoch.
Assuming that all component functions fm (x) are μ-strongly convex9 and L-smooth, and objective-
wise homogeneity F1 = … = Fm = F, the authors obtain Theorem 8 (Mishchenko et al., 2021),
which states that
E UyK — x*『]≤ (1 — ημ)NK kyo — x*『+ ""MN*,	(31)
9This is in fact quite strong compared to this paper, because we only assume F to be P匕，not Fm nor fim.
17
Published as a conference paper at ICLR 2022
where y0 and yK are the initialization and last iterate of the algorithm, and σo2pt was defined above
in (22). The term MNσOpt in (31) corresponds to the term "Nσ2" as per the notation in Mishchenko
et al. (2021). If we apply our Assumption 3 to bound σo2pt , we get σo2pt ≤ ν2 , which reduces the last
term in (31) to ":"N. If We substitute η = Iog(MNK ) to the bound (31), We get
μ	μiN ʃv
E hkyκ-χU]≤⅛K≠+O (μLκ )，	(32)
which translates to the same convergence rate on F(yκ) - F* as single-machine RR. For the
heterogeneous setting, applying Lemma 3 of Mishchenko et al. (2021) to Theorem 2, We can obtain
E [kyκ - χ*k2i ≤ (i-ημ)NK ky0 - χ*k2
2η2 L M	1 N
+ 2M~E∖N2 kyF m(x*)k2 + 4 ∑ kVfim(x*) - VF m(x*)k2∖
= (i-ημ)NK kyo - χ*k2 + 2η2MN2 X kVFm(χ*)k2 + η¾¾ (33)
μ	m=1	μ
where σ2 was defined in (21). Note that in Lemma 3 (Mishchenko et al., 2021), the function
“Fm” in the authors’ notation is equal to NFm in our notation. Recall that the (G, B)-BGD as-
sumption (13) is “comparable” to Assumption 3 (12). If we apply (13) to the bound (33), we get
M PM=I ∣∣VFm(x*)k2 ≤ G2. Similarly, if we apply Assumption 3, we get σ2 ≤ ν2. Substituting
these upper bounds and η = log(MKK ) to (33) gives
E hk,,—尸k2i ≤ ky0 - x*『+ o ( LG2 + LV ∖	(34)
E kyyκ k J ≤ MNK2 + (μ3K2+μ3NK2) ,	( )
and after translating this into a bound on function value, we get an upper bound O( μLNK + LG)
which in fact matches our upper bound (5) in Theorem 2 when we set B = N.
The two upper bounds obtained for homogeneous (32) and heterogeneous (34) settings indicate that,
at least under assumptions on intra- and inter-machine deviation such as ours, the claimed advantage
that “the convergence bound improves with the number of devices involved” (Mishchenko et al.,
2021) is not achievable. As our lower bound shows, one needs to choose B smaller than N in order
to get the most out of parallelism. That being said, since Mishchenko et al. (2021) is free of uniform
intra- and inter-machine deviation assumptions, there may still exist certain scenarios where multiple
machines can speed up performance even with B = N.
B More detailed illustration of synchronized shuffling
In this section, we provide a more detailed explanation on synchronized shuffling that we intro-
duced in Section 5. For the illustration, let us consider the component-wise homogeneous case.
Component-wise homogeneity means that all the machines have the same set of components:
fi1 = fi2 =…=fM =: fi for i ∈ [N]. Hence, we have F = Fm for all m ∈ [M] and our
goal is to minimize F = N PN=I fi.
In the proof of Theorem 1 (presented in Appendix D.2; see Appendix D.1 for a sketch), we add the
component gradients over an epoch and then use the following key identity: for any permutation σ,
NN
X	Vfσ (i) = X	Vfi = NVF.	(35)
i=1	i=1
We use this identity (35) to represent the per-epoch progress as “one big GD step plus noise.” For
the rest of the proof we bound the “noise” term, and the key to bounding it is to upper bound the
norm of summations of the following form, for i ∈ [N/B - 1] (see (41) and (43)):
M iB
MM ΣΣVfσkm (j) (xk,0).	(36)
m=1 j=1
18
Published as a conference paper at ICLR 2022
To bound the norm, we decompose it into two terms using the triangle inequality
1 M iB
M XX vfσm(j)(xk,0)
m=1 j=1
≤
1 M iB
M X X ^fσm(j)(xk,0) - iBVF(xk,0)
m=1 j=1
+iB kVF (xk,0)k,
and We use concentration bounds (44) on the first term of the RHS, which gives a high-probability
upper bound on the RHS of the form O (Vʌ/iB) + iB ∣∣VF∣∣ (45).
The key to proving fast convergence is to make the upper bound above as small as possible. To make
the bound (45) even smaller, we wish to be able to apply the identity (35) to the summation (36).
However, under the standard way of choosing permutations σkm independently over machines, one
cannot apply the identity because we do not sum over all j = 1, . . . , N. This limitation motivates
our proposed technique synchronized shuffling, a manipulation on the choices of σkm that lets us
prove even faster convergence.
Recall the definition of synchronized shuffling. At the beginning of the k-th epoch, the server sam-
PleS σ 〜 Unif(SN) and ∏ 〜 Unif(SM), and broadcasts them to the machines. Then, local machines
choose their permutations σ/ to be shifted versions of σ: σm(i) := σ ((i + Nπ(m)) mod N).
Now set N = 6 andM = 3. Assume for simplicity that the permutation π ∈ S3 of machines satisfies
π(m) = m for m ∈ [3].10 Suppose the server samples σ = (σ(1), σ(2), σ(3), σ(4), σ(5), σ(6)) and
broadcasts it. Under synchronized shuffling, the local machines choose
σk1 = (σ(3), σ(4), σ(5), σ(6), σ(1), σ(2)),
σk2 = (σ(5), σ(6), σ(1), σ(2), σ(3), σ(4)),
σk3 = (σ(1), σ(2), σ(3), σ(4), σ(5), σ(6)).
One can see that each permutation is a shifted version of σ, with an offset that is a multiple of
N = 2
M	2.
Now consider adding Vfσm(i) over m = 1, 2, 3 and j = 1, 2 (in fact, any two consecutive j’s will
do). By synchronized shuffling, we get a summation over all N = 6 component functions, which
by (35) gives us the full gradient:
32	6
XX
Vfσkm (j) = X Vfi = 6VF.
m=1 j =1	i=1
The point here is that the permutation identity (35) can be applied to the summations (36) to further
reduce their norm bounds. This is in contrast to sampling independent σkm ’s where one cannot
apply (35). For this reason, synchronized shuffling significantly reduces the noise that comes from
without-replacement sampling, thus resulting in faster convergence rates in (near-)homogeneous
cases.
C Experimental results
In this section, we present some simple numerical experiments that support our theoretical analy-
sis. We evaluate the performance of the algorithms considered in this paper on the “hard instance”
constructed in our lower bounds (Theorems 3 and 4).
Our hard instance F ∈ FcmP(L, μ, ν, 0) is a function in the component-wise homogeneous setting,
where all the machines have the same set of local component functions: f1 = ff2 = … =fM =： fi.
In the proofs of Theorems 3 and 4, we construct the global objective F(X) = * PN=I fi (x) as the
following:
x2	+1 if1 ≤ i ≤ N/2,
fi(X)= (L1x≤0 + μ1x>0)^2^ + ziνx, zi = ɪ -1 if n/2 < i ≤ N
10In fact, permuting the machines by π is not required in the component-wise homogeneous setting (i.e.,
when λ = 0 in Assumption 4).
19
Published as a conference paper at ICLR 2022
With this set of component functions, the global objective F(x) = (L1χ≤o+μ1χ>o) x2 is μ-strongly
convex and L-smooth with a unique global minimizer at x = 0.
We compare the performance of the algorithms on this problem instance, with L = 100, μ = 1,
ν = 1, N = 768, and M = 16, while varying the choice of B ∈ {1, 4, 16, 64, 256} and
K ∈ {1,3,5,7, 10, 30, 50, 70, 100, 300, 500, 700, 1000}. For each value ofB and K, we run the
algorithms for K epochs (KN/B communication rounds for with-replacement algorithms) starting
at x0 = 0 and return the values of F evaluated at the last iterates. Note that the algorithms are not
deterministic, because the sampling/shuffling schemes are random. In order to account for random-
ness, for each combination of (algorithm, B, K) we execute 20 independent runs of the algorithm
and plot the mean, first quartile, and third quartile of the final objective values.
In the subsequent subsections, we compare the following seven algorithms with constant step-sizes.
•	Minibatch RR, η = B Sg；NKK);
•	LocalRR, η = ^NP;
Minibatch RR with SYNCSHUF, η
B log(MNK2).
μNκ ;
•	Local RR with SYNCSHUF, η = log(MNK2);
Single-machine RR with minibatch size B, η
log(NK2).
μNK ;
•	With-replacement minibatch Sgd, η = B log(MMNK );
•	With-replacement local Sgd, η = log(MNNK ).
C.1 SyncShuf improves convergence of minibatch/local RR
In Figure 1, we compare minibatch RR and local RR, with and without SyncShuf. Each plot in
Figure 1 shows how the methods’ performance changes with K, for a fixed value B . Each point
on the curve is the mean of the final objective function values over 20 independent runs of the
corresponding algorithm with the specific B and K, and its error bar indicates the first and third
quartiles.
Recall from our Theorems 1, 2, 6, and 7 that the four methods satisfy the following convergence
bounds, in homogeneous settings (i.e., τ = λ = 0):
•	Minibatch RR: O L2M MNK2
Local RR: O (L2
ν2
MNK2
N⅛)),
Minibatch RR with SyncShuf:
L2	ν2
μ3 M2NK2
•	Local RR with SYNCSHUF: O (L2 (M2NK2 + NK
In fact, if B = 1, local RR is identical to minibatch RR. Figure 1(a) confirms that this is indeed
true, and also that the versions with SyncShuf outperforms the ones without SyncShuf. This
corroborates the additional M factor speedup in our bounds. In Figure 1(b) and 1(c), we can see
that as B increases, the performance of local RR with SYNCSHUF degrades and becomes closer to
local RR without SYNCSHUF. This shows that the O (L2 N2⅛) term starts to dominate. Also,
as we increase B further, in Figure 1(c) and 1(d) we see that local RR (without SYNCSHUF) also
starts to degrade and its gap between minibatch RR becomes larger. Again, this means that the
O (L2 NV⅛) term becomes the dominant factor in the local RR bound. The performance of mini-
batch RR, with and without SYNCSHUF, stays relatively independent ofB. One thing to note is that
for large values of B, the small-epoch behavior of minibatch RR looks rather unstable. The choice
of step-size η = B lo/MNK ) seems to cause overshooting when B is large and K is small. We
20
Published as a conference paper at ICLR 2022
(C) B = 16
10-1
(b) B = 4
(d) B = 64
10-9
100
10-2
10-3
10-4
10-5
10-6
10-7
10-8
101
102
103
(e) B = 256
Figure 1:	Comparison between minibatCh RR and loCal RR, with and without SyncShuf. Best
viewed in Color. The algorithm versions with SyncShuf Converge faster. Also note the perfor-
manCe degradation as B inCreases, as expeCted by our theory.
note that this does not ContradiCt our ConvergenCe analysis beCause our theorems only CharaCterize
the large-epoCh behavior (K above Certain thresholds) of the algorithms. Perhaps in the small-epoCh
regime, our ChoiCe of η is not neCessarily optimal and a smaller η is needed to prevent overshooting.
C.2 LOCAL RR BECOMES CLOSER TO SINGLE-MACHINE RR AS B → N
Our next set of plots presented in Figure 2 provides a Comparison of minibatCh RR, loCal RR, and
single-maChine RR (i.e., minibatCh RR with M = 1). In Theorems 2 and 4, we showed that when
B = Θ(N), then the ConvergenCe of loCal RR beComes just as fast as the single-maChine RR.
21
Published as a conference paper at ICLR 2022
10-1
10-2
10-3
10-4
10-5
10-6
10-7
10-8
(b) B = 4
(d) B = 64
101
102	103
(e) B = 256
10-9 l-
100
Figure 2:	Comparison of minibatch RR, local RR, and single-machine RR. Best viewed in color.
The large-epoch performance of local RR becomes similar to that of single-shuffle RR as B be-
comes closer to N .
Indeed, we can observe from Figure 2 that this is really the case. As B increases, the curve of local
RR moves closer and closer to that of single-machine RR, especially in the large-epoch regime.
C.3 With- vs. without-replacement sampling
Lastly, in Figure 3 we compare the with-replacement and without-replacement versions of mini-
batch/local SGD. In all plots, we can see that the without-replacement versions outperform with-
replacement ones, at least for our problem instance. It is also intriguing to note that the two versions
perform very similarly in the small-epoch regime (for K UPto 〜10), but WithoUt-rep山Cement starts
22
Published as a conference paper at ICLR 2022
(a) B = 1
(b) B = 4
(c) B = 16
(d) B = 64
10-9 l-
100
101	102	103
(e) B = 256
Figure 3: Comparison of with-replacement (Sgd) and without-replacement (RR) versions. Best
viewed in color. Without-replacement versions converge faster than with-replacement versions, at
least in our problem instance. Also note that the two versions perform similarly in the small-epoch
regime, which supports our theoretical findings.
to outperform for larger K’s. This observation supports our theoretical prediction from Theorem 3
that in the small-epoch regime, minibatch RR can at best perform as fast as minibatch Sgd.
D	Proofs of upper bounds
In this section, we provide proofs of our upper bounds stated in Sections 3 and 5. We start by describ-
ing a high-level proof outline that we use for all the proofs presented in this section (Appendix D.1).
In the subsequent subsections, we prove Theorems 1, 2, 6, and 7, in the order they appeared in the
main text. The next subsection (Appendix D.6) states and proves a key lemma that gives concen-
23
Published as a conference paper at ICLR 2022
tration bounds for the mean of multiple without-replacement sums of vectors. This general-purpose
lemma can be of independent interest and it can prove useful in various other settings. Lastly, in
Appendix D.7 we discuss how we can modify the theorem statements to remove the requirement in
Assumptions 2, 3, and 4 that they must hold for the entire Rd .
Notation. Throughout this section, we use the product notation Q in a slightly unconventional
manner. For indices i ≤ j and square matrices Ai, Ai+1, . . . , Aj-1, Aj, we use Qli=j Al to denote
the matrix product Aj Aj-I •…Ai+ιAi. If i > j,then Qli=j Al = I .
D.1 Proof outline
The proofs of upper bounds follow a common structure, consisting of the following three steps:
1.	writing one epoch as one step of GD plus noise;
2.	getting a high-probability upper bound on the noise term using concentration inequalities;
3.	obtaining the convergence rate using the bounds on the noise term.
We first unroll the update equations over an epoch, and write an epoch of the algorithms as one step
of gradient descent plus noise:11
Xk+1,0 = Xk,0 - nNVF(xk,o) + η2rk.
Substituting the above to the definition of L-smoothness of F and arranging terms, we obtain
F(xk+1,0) - F (xk,0)
≤ hVF(xk,o), Xk+1,0 - Xk,oi + 2 ∣∣Xk+1,0 - Xk,ok2
≤ - nN kvF(Xk,O)k2 + n2 ∣vf(Xk,O)kkrkk + n2L- kNvf(χk,o) + nrkk2
≤(-ηN+η2LN2)∣VF(xk,0)∣2+η2∣VF(xk,0)∣ ∣rk∣ +η4L∣rk∣2.	(37)
The next step is to get high-probability upper bounds on the term krkk. This is done by applying
our concentration inequality lemma (Lemma 8) to partial without-replacement sums of component
gradients. As a result, we will get upper bounds on krkk and krk k2, for k = 1, . . . , K, which hold
with probability at least 1 - δ.
In the last part, We substitute the high-probability bounds to (37) and invoke the definition of PL
functions to get a per-epoch progress bound. We then unroll the per-epoch inequality for all epochs
k = 1, . . . , K . Arranging the terms in the resulting inequality gives our desired convergence bound
that holds With probability at least 1 - δ.
D.2 Proof of upper bound for minibatch RR (Theorem 1)
One epoch as one step of GD plus noise. To simplify the notation throughout the proof, We Will
prove the same convergence rate for a rescaled update rule and step-size:
M iB
xk,i := xk,i-1 - M X X	vfσmm(j)(xk,i-1)	(38)
m=1 j=(i-1)B+1
for i ∈ [N/B] and k ∈ [K], and n = logt^MNK ). Note that the gradient term is scaled UP by B and
the step-size is scaled doWn by B. We Will prove the convergence rate for this equivalent algorithm.
We start the proof by unrolling the update equations over an epoch and expressing the progress as
Xk+1,O = Xk,O - nN F (Xk,O) + n2rk,
i.e., one step of full gradient descent plus some noise.
11In case of local RR, We replace xk,0 With yk,0.
24
Published as a conference paper at ICLR 2022
To this end, We decompose the gradient Vfmm(j)(xk,i-i) into the signal Vfmm(j)(xk,o) and noise:
▽f》(j)(Xk,i-ι) = Vfmm(j)(xk,0) + Vfmm(j)(xk,i-ι) - Vfmm(j)(xk,0)
= Vfσmkm(j)(xk,0) +
V2fσmm(j)(xk,0 + t(xk,i-1 - xk,0))dt
(xk,i-1 - xk,0),
Where V2f(x) denotes the Hessian of f at x, Whenever it exists. We remark that the integral
exists, due to the folloWing reason. Since We assumed that each fσmm (j ) is differentiable and smooth,
its gradient Vfσmm (j) is Lipschitz continuous, and hence absolutely continuous. This means that
Vfσmkm(j) is differentiable almost everyWhere (i.e., V2fσmm(j)(x) exists a.e.), and the fundamental
theorem of calculus for Lebesgue integral holds; hence the integral exists.
To simplify notation, We define the folloWing for all i ∈ [N/B]:
M iB
gi := M X X	Vfmm(j)(xk,0)，
m=1 j=(i-1)B+1
1 M iB	1
Hi := M X x / V2fmm(j)(xk,0+t(xk,i-ι—χk,0))dt,
m=1 j=(i-1)B+1 0
so that We can Write (38) as
xk,i = xk,i-1 - ηgi - ηHi(xk,i-1 - xk,0).	(39)
From L-smoothness of fim ’s, it is straightforWard to check that kHik ≤ LB. Unrolling (39) for
i = 1, . . . , N/B, it turns out that We can Write
N/B / i+1	∖
xk+1,0 = xk,0 - η	(I - ηHj) gi.
i=1 j=N/B
Due to summation by parts, the folloWing identity holds:
N/B	N/B	N/B-1	i
Σ ai bi = aN/B	bj-	(ai+1 - ai)	bj.
i=1	j=1	i=1	j=1
We apply this to the last term, by substituting ai = Qij+=1N/B(I -ηHj) and bi =gi:
N/B ( i+1	∖
η X Y (I - ηHj) gi
i=1	j=N/B
N/B	N/B-1 / i+2	i+1	∖ i
=η X gj	- η X	I Y (I- ηHt) - Y (I	- ηHt) I Xgj
j=1	i=1 t=N/B	t=N/B	j=1
N/B-1 / i+2	∖	i
= ηN VF (xk,0) -η2 X Y (I - ηHt) Hi+1 Xgj .
i=1	t=N/B	j=1
、------------------V-----------------}
=:rk
With the “noise” rk defined as above, We can Write xk+1,0 = xk,0 - ηNVF (xk,0) + η2rk, as
desired. Next, it folloWs from L-smoothness of F that
F (xk+1,0) - F (xk,0)
≤ hVF(xk,0), xk+1,0 - xk,0i + L ∣∣xk+1,0 - xk,0k2
≤ - ηN ∣vf(χk,0)k2 + η2 ∣vf(χk,0)k krkk + η2LL kNvf(xk,0) + ηrkk2
≤ (-ηN+η2LN2) ∣VF (xk,0)∣2 +η2 ∣VF (xk,0)∣ ∣rk∣ +η4L∣rk∣2 ,	(40)
Where the last inequality used ka + bk2 ≤ 2 kak2 + 2 kbk2 .
25
Published as a conference paper at ICLR 2022
Bounding noise term using concentration. It is left to bound krk k. We have
krk k
N/B-1	i+2	i
X I Y (I - ηHt)∣ Hi+1 X gj
i=1	t=N/B	j=1
N/B-1 ∣∣∕ i+2	∖	i
≤ X Y (I - ηHt) Hi+1 X gj
i=1	t=N/B	j=1
N/B-1
≤ LB(1 + nLB)N/B X
i=1
i
Xgj
j=1
(41)
where the last step used kHi k ≤ LB for i ∈ [N/B]. Recall from the theorem statement that
K ≥ 6κ ∖og(MNK 2) and η = '。虱MNNK) .This means that
(1 + ηLB)N∕B =(1 + ^NKI Γ ≤ (1 +焉 L el“	(42)
Now, we use Lemma 8 to bound the norm of
i	M iB
X gj = M XX ▽趣(j)(χk,0).	(43)
j=1	m=1 j=1
Note that for any epoch k, the permutations σk1 , . . . , σkM are independent of the first iterate xk,0
of the epoch, and hence independent of all Vfm(Xk,0). Therefore, We can apply Lemma 8 to the
partial sum (43), with Vm J Vfm(Xk,0), n J iB, and δ J NK. By Lemma 8, with probability
at least 1 一 NK, we have
1 W ㈡	8 log 2NK
iBM XXvfσmj)(Xk,0) -vf(Xk,0) ≤ Vy iBM .
m=1 j=1
(44)
Using this concentration bound, with probability at least 1 一 NK we have
i
Xgj
j=1
8 log 2NK
≤ iBν∖ 指渭 +iB kVF(xk,0)k
iBM
/ 8iB log 2NK
Vy-Mɪ + iB kVF(Xk,0)k .
(45)
We can now substitute (42) and (45) to (41) to get
Ek≤α/LB X (V S 8iB lM 2NK + iB kVF (xk,0)∣j
≤ ^M^ /"/B √dt- + ei∕6LB2 kVF(xk,0)kNX
M	1	i=1
≤5LV(NM2ι-B3/2) rlog2NK+LN (N 一B MVF (χk,0)k,
which holds with probability at least 1 一 K, due to the union bound over i = 1,...,N∕B 一 1.
The bound (46) holds for all k ∈ [K] with probability 1 一 δ if we apply the union bound over
k = 1, . . . , K. Next, by (a + b)2 ≤ 2a2 + 2b2, we have
krkk2 ≤ 25L2ν2(NX - B" log 2NK +2L2N2(N 一 B)2 kVF初,。)『,(47)
2M	B δ
which also holds for all k ∈ [K] with probability at least 1 一 δ.
26
Published as a conference paper at ICLR 2022
Getting a high-probability convergence rate. Given our high-probability bounds (46) and (47),
we can substitute them to (40) and get
F (Xk+1,0) - F (Xk,0)
≤(-ηN+η2LN2)∣VF(Xk,0)∣2+η2∣VF(Xk,0)∣ ∣rk∣ +η4L∣rk∣2
≤ (-ηN + η2LN2 + η2LN(N - B) + 2η4L3N2(N - B)2) ∣∣VF(xk,o)∣2
+
5η2Lν(N3/2 - B3/2)
2M1/2
∖∕log2NK ∣VF(xk,0)k
Bδ
+
25η4L3ν2(N3/2 - B3/2)
2M
2	2NK
TOg k
(48)
The second term in the RHS of (48) can be bounded using ab ≤ a2 + bT:
5η2Lν(N3/2 - B3/2)
2M1/2
Jlog 2NK I∣vf(xk,o)k
Bδ
MN 1/2
η~2— kVF (xk,o)∣
5η3∕2Lν(N3/2 - B3/2)
M1/2N1/2
2^^2NK
VlOg F
≤ ηN ∣VF(xk,o)∣2 +
25η3L2ν2(N3/2 - B3/2)
2MN
2	2NK
一log F
Putting this inequality to (48) and noting N - B ≤ N gives
F(Xk+1,0)- F(xk,o) ≤ (-7ηN + 2η2LN2 + 2η4L3N4) ∣VF(xk,o)∣2
+
25η3L2(1 + ηLN)ν2(N3/2 - B3/2)
2MN
2	2NK
一 log -bΓ-
(49)
Recall from K ≥ 6κ log(MNK2) and η = log(μNNK ) that ηLN ≤ ɪ. Since the inequality
-8Z + 2z2 + 2z4 ≤ -1Z holds on Z ∈ [0,16], we have
-7ηN + 2η2LN2 + 2η4L3N4 ≤ -1 ηN.
82
Applying this bound to (49) results in
F(Xk+1,0)- F(Xk,0)≤ - -ɪ ||VF(Xk,0)『+
15η3L2ν2(N3/2 - B3/2)
MN
2 2NK
"log BbT-
We now recall that F is μ-P匕 so ∣∣VF(xk,o)k2 ≥ 2μ(F(xk,o) - F*):
F(Xk+ι,o) - F* ≤ (I - ημN)(F(xk,o) - F*) +
15η3L2ν2(N3/2 — B 3/2)
MN
2	2NK
Tog F (50)
Recall that (50) holds for all k ∈ [K], with probability 1 - δ. Therefore, by unrolling the inequality,
F (xκ, B) - F * ≤ (1 -
ημN)k(F(xo)- F*)
15η3L2ν 2(N3/2 — B3/2)
MN
∙2log2NK X1(i-ημN )k
Bδ
k=0
≤ (1 - ημN)k(F(xo)- F*) +
15η2L2ν2(N3/2 - B3/2)
μMN 2
ɪ log2NK.(51)
Bδ
+
Lastly, substituting η = log(MNK ) gives
F (V	)_ F * ≤ F (xo)- F
(K, B)	≤	MNK2
=F(xo)- F
=MNK2
*
+
15L2ν2(N3/2 - B3/2)2 log 2NK log2(MNK2)
*
→ O
μ3MN 4K 2
L2ν2	1	、
μ3 MNK2 ).
(52)
27
Published as a conference paper at ICLR 2022
Getting an in-expectation bound from the high-probability bound. We conclude this subsec-
tion by briefly describing how we can obtain an in-expectation bound from the high-probability
bound we just proved. Recall that the bound we proved above holds under the event E that all the
concentration bounds used throughout the proof hold. The key to proving an in-expectation bound
is to obtain an upper bound under its complement Ec, i.e., conditioned on the event that at least one
of our concentration bounds does not hold. We do so by repeating the same proof without ever using
the Hoeffding-Serfling bounds (Lemma 8). Of course, this leads to a much looser bound, but we can
choose δ to be small enough so that the desired bound O (L1?2 MNK2) holds in expectation.
For the version without concentration bounds, the proof proceeds in the same way until it starts
diverge at (44). Instead of applying concentration inequalities, we loosely bound the quantity as the
following:
M iB
焉XXNfσm(j)(xk,0)-NF (Xk,o)
m=1 j=1
M iB	M
E X X vfm⅛)(χk,0) - M X F m(χk,0)
m=1 j=1	m=1
M	iB
≤ M XliB X <fm⅛)(χk,0)- F m(χk,0)
m=1	j=1
≤ ν.
With this bound, the RHS of the upper bound (45) on IlPj=I gj k becomes iBν + iB ∣∣VF (xk,0) k.
This results in the bounds on krkk and krk k2 (corresponding to (46) and (47)) that read
∣rk∣ ≤LνN(N-B)+LN(N-B)∣VF(xk,0)∣ ,
∣rk∣ ≤ 2L2ν2N2(N - B)2 + 2L2N2(N - B)2 ∣VF(xk,0)∣2.
The rest is substituting the bounds above to (40), and going through the same steps to obtain the
final bound. The resulting bound that corresponds to (51) is
F (xκ, N) — F * ≤ (1-ημN )K (F (X0)— F *) +
7η2L2ν2(N — B)2
3μ
which, by substituting η = log(MKK ), yields
F (xκ, N) — F * ≤
F(χ0)- F * + o( L2ν2 ɪ ʌ
MNK2	+	μ3 K2).
(53)
To finish the proof of in-expectation bound, choose δ = MN. Recall that the probabilistic event E
occurs when all our concentration bounds hold. Conditioned on E , which occurs with probability
at least 1 一 MN, the tighter bound (52) holds, with log 1 replaced by log(MN). The complement
event Ec occurs with probability at most MN, under which the looser bound (53) is true. Thus, in
expectation,
E hF (xκ, B) — F *] = P(E)E hF (xκ, B) — F * I Ei + P(Ec)E [F (xκ, B) — F * I Eci
≤ E [F (xκ, B) - F * IE i + MN E [F (xκ, B) - F * I Eci
≤ 3(F(x。)— F*) + O (二 1 A
≤	2MNK2	+	μ3 MNK2).
For the remaining high-probability upper bounds proved in the paper, we can similarly follow this
process to obtain matching (up to log factors) in-expectation upper bounds.
28
Published as a conference paper at ICLR 2022
D.3 Proof of upper bound for local RR (Theorem 2)
One epoch as one step of GD plus noise. The update rule of local RR can be written as the
following. For k ∈ [K], i ∈ [N], and m ∈ [M],
xm, := ∫χm,i-ι - η^fmm(i)(χm,i-ι)
,I 吉 Pm=I(Xmi-1 - ηVfmm(i) (Xmi-I)) =: yk,i∕B
if B does not divide i,
if B divides i.
(54)
Recall that Xkm,0’s are the initial points of an epoch and they all the same regardless of the machine
m. We define yk,0 := X1k,0. For any i in the range of (l - 1)B + 1 ≤ i ≤ lB for some l ∈ [M],
we will use yk,l-1 as the “pivot” and decompose the gradients into the ones evaluated at yk,l-1 plus
noise terms.
Vfmm(i)(xkmi-1) = Vfmmci)(yk,1-1) + Vfσmci)(xkmi-ι) - Vfm^)(yk,ι-ι)
=Vfσmkm(i)(yk,ι-1)+	Z 1 V2fσmkm(i)(yk,ι-1 + t(Xkm,i-1 - yk,ι-1))dt (Xkm,i-1 - yk,ι-1),
0
where the integral Him exists due to the reason discussed in Appendix D.2. Also note from L-
smoothness of fim’s that kHim k ≤ L. Using the decomposition, one can unroll the updates (54) and
write yk,ι in terms of yk,ι-1 in the following way:
M	IB	/ i+1	∖
yk,ι =	yk,ι-ι- MM X	X	(Y	(I- ηHm) I	Vfmmci)(yk,ι-ι),	(55)
m=1 i=(ι-1)B+1 j=ιB
for l = 1, . . . , N/B. Next, we again decompose the gradient Vfσmm(i)(yk,ι-1), this time using yk,0
as the pivot:
Vfσmkm(i)(yk,ι-1) = Vfσmkm(i)(yk,0) + Vfσmkm(i)(yk,ι-1) - Vfσmkm(i)(yk,0)
= Vfσmkm(i)(yk,0) +
V2fσmm(i)(yk,0 + t(yk,ι-1 - yk,0))dt (yk,ι-1
- yk,0).
=Hm
This decomposition allows us to rewrite (55) in the following form
yk,ι = yk,ι-1 - ηtι - ηSι (yk,ι-1 - yk,0),
where
M ιB	i+1
tι := M X	X	I	Y	(I -	ηHm)	I	Vfmm(i)(yk,0),
m=1 i=(ι-1)B+1 j=ιB
M ιB	i+1
Sι := M X	X	I	Y	(I -	ηHm)∣	Hm.
m=1 i=(ι-1)B+1 j=ιB
Unrolling (56) for l = 1, . . . , N/B then gives the progress over an epoch:
N/B / ι + 1	∖
yk+1,0 = yk,0 - η I	(I-ηSj)I tι.
ι=1	j=N/B
(56)
(57)
(58)
(59)
As done in the proof of Theorem 1 (Appendix D.2), we will express (59) as one step of GD on F
plus some noise. Of course, the noise terms here will be more complicated to handle than they were
in Theorem 1. Due to summation by parts, the following identity holds:
N/B	N/B	N/B-1	ι
Σ aι bι = aN/B	bj-	(aι+1 - aι )	bj.
ι=1	j=1	ι=1	j=1
29
Published as a conference paper at ICLR 2022
We apply this to the last term of (59), by substituting ai = Qlj+=1N/B(I - ηSj) and bl = tl:
N/B / l + 1	∖	N/B	N/B-1 / l+2	∖	l
ηX( Y (I3)1 tι = ηX…X I Y Hj)1 S1+1X.	(60)
l=1 j=N/B	l=1	l=1	j=N/B	j=1
We also apply the summation by parts to the inner summation of tl ’s (57):
M lB	i+1
tι ：= MM X X I Y (I - ηHm)) v*)(yk,0)
m=1 i=(l-1)B+1 j=lB
M	lB
=M X	X	Nfmm(Mnk，。)
m=1 i=(l-1)B+1
M	lB-1	i+2	i
-M X	X	Y(I- ηHtm)	Hi+ι	X	wσ⅛)(yk,0)	(61)
m=1 i=(l-1)B+1 t=lB	j=(l-1)B+1
Substituting (61) to (60) gives
yk+1,0 = yk,0 - ηN NF (yk,0) + η2rk,1 + η2rk,2 - η3rk,3,
where rk,1, rk,2, and rk,3 are noise terms defined as
N/B M lB-1	i+2	i
rk,ι :=	MM XX X	Y (I	-	ηHtm)	Hm+ι X wσ⅛)(yk,0),
l=1 m=1 i=(l-1)B+1	t=lB	t=(l-1)B+1
1 N/B-1 / l+2	∖	M IB
rk,2 := M X I Y (I-ηSj)) S1+1 XXNfmm(EQ，
l=1	j =N/B	m=1 t=1
1 N/B-1 ( l+2	∖
rk,3 := M X 1 Y (I-ηsj)) S1+1×
l=1	j=N/B
l M	jB-1	(	i+2	∖	i
XX	X	I	Y	(I - ηHtm))	Him+1	X	Nfσmkm(t)(yk,0).
j=1 m=1 i=(j-1)B+1 t=jB	t=(j-1)B+1
Defining rk := rk,1 + rk,2 - ηrk,3, it follows from L-smoothness of F that
F (yk+1,0) - F (yk,0)
≤ hNF(yk,0), yk+1,0 - yk,0i + 2 kyk + 1,0 - yk,0k2
≤ -ηNkNF(yk,0)k2 + η2 kNF(yk,0)kkrkk + η2L kNVF(yk0 + ηrkk2
≤ (-ηN+η2LN2) kNF (yk,0)k2 +η2 kNF (yk,0)k krkk + η4L krkk2 .	(62)
Bounding noise terms using concentration. We next bound krk k by bounding each krk,1 k,
krk,2 k, and krk,3k. From this point on, we write gtm := Nfσmm(t)(yk,0) to simplify notation.
1	N/B M	lB -1	i+2	i
krk,ιk =IMXX X Y(I-ηHm) Hi+ι	X	gm
l=1 m=1 i=(l-1)B+1 t=lB	t=(l-1)B+1
N/B M lB -1	i+2	i
≤MXX X II Y(I-ηHm) Hi+ι	X	gm
l=1 m=1 i=(l-1)B+1	t=lB	t=(l-1)B+1
≤
L(1 + ηL)B
M
N/B M	lB-1
XX	X
l=1 m=1 i=(l-1)B+1
i
X	gtm
t=(l-1)B+1
(63)
30
Published as a conference paper at ICLR 2022
where we used kHimk ≤ L. Recall from the theorem statement that K ≥ 7ρκ log(MNK2) and
log(MNK2)
μNK
. This means that
(1 + nL)B =(1 + ^MKI )B ≤(1 + 备)B≤(1 + ) )B≤ 犹.(64)
Also note from the definition of Sl (58) that kSl k ≤ LB(1 + ηL)B ≤ e1/7LB, which we use to get
similar bounds for the next two terms rk,2 and rk,3.
I 1 N/B-1
krk,2k = IlM X
l+2
M lB
∏ (i-nSj)	Sι+1∑ ∑>m
j=N/B
m=1 t=1
< e1/7LB(1 + MnLB)NB
N/B-1
M lB
gtm
(65)
l=1
m=1 t=1
η
M

and we can bound
(1 + e1/nLBf∕B = (1 + e
1/7KB Iog(MNK 2) N/B
NK
e1/7B N/B
≤ (1 + k)	≤ exp
We similarly bound the norm of the last noise term rk,3:
(66)
krk,3k ≤
e1/7LB(1 + e1"nLB)NB X
N/B-1
l=1
M
l M jB-1	i+2
j=1 m=1 i=(j-1)B+1	t=jB
(I-ηHtm) Him+1
i
X	gtm
t=(j-1)B+1
e1/7L2B(1 + e1/7nLB)N/B(I + nL)
M
B N/B-1 l M	jB-1
X XX	X
l=1 j=1 m=1 i=(j-1)B+1
i
X
t=(j-1)B+1
gtm
L2B N/B-1 l M jB-1
11L B
7M	乙乙乙 乙
l=1 j=1 m=1 i=(j-1)B+1
i
X gtm
t=(j-1)B+1
(67)

≤
≤
where the last inequality used (64), (66), and e2/7 exp(e1/7/7) ≤ 11/7.
Given the bounds (63), (65), and (67), we now use Lemma 8 to get high-probability bounds for the
partial sums of gtm that appear in the bounds. For any i satisfying (j - 1)B+ 1 ≤ i ≤ jB - 1, where
j ∈ [N/B], and for any m ∈ [M], the following bound holds with probability at least 1 -
δ
2MNK :
i-(j-1)B
iN
X	gm-NXgm
i-(j-1)B
t=(l-1)B+1
i
X	Vfσmkm (t)
t=(l-1)B+1
t=1
(yk,o) — VF m(yk,o)
∣81og 4MNK
≤ Vd i-(j- 1)b .
1
1
From this, with probability at least 1 一 2MNκ We have
i
X	gtm
t=(j-1)B+1
≤ V ,8(i-(j- 1)B)log 4MNK + (i — (j — 1)B) kVF m(yk,ο)k .	(68)
Similarly, for l ∈ [N/B — 1], the following bound holds with probability at least 1 一 2Nκ:
M lB	M N
Il 1BM mXι Xgm 一 MN Xi Xgm
31
Published as a conference paper at ICLR 2022
M lB
IBM X X Vfmm(t)(yk,0) - vf(yk,0)
m=1 t=1
≤ v ∕8 W
which gives us
XX gt ≤ V ,8lBM log 4NK + IBM ∣VF (yk,o)∣
M lB
(69)
m=1 t=1
By applying the union bound, with probability at least 1 - K, the bound (68) holds for all m ∈ [M]
and i ∈ SjN=/1B[(j - 1)B + 1 : jB - 1], and the bound (69) holds for all l ∈ [N/B - 1].
We now substitute the bounds (68) and (69) to (63), (65), and (67) to get upper bounds for lrk,1 l,
lrk,2l, and lrk,3l,respectively. First,
lrk,1l
1/7L N/B M IB-I /	------------------------ ∖
≤ eML XX X	V √8(i-(l - 1)B) log 4MNK + (i - (l - 1)B) kVFm(yk,o)k )
l=1 m=1 i=(l-1)B+1
e1∕7√8LvN B-1 / I	4MNK , e1/7LN (R 1 彩 Erm,
=-B— (A ") Vlog — + -ɪ (A iJ(M AkVF m(yk,0)k )
< 24LVN(B3/2 — 1)
11B
log
4MNK	3LN(B - 1)
-ɪ +
(T + P l∣VF (yk,O)II),
(70)
5
where the last inequality used PB-LI √ ≤ RB √Zdz = 3 (B3/2 一 1) and Assumption 3. For the
next noise term, we have e1/7(1 + e1/7nLB)N/B ≤ 7/5, so
7LB N/BT / Z--------------
krk,2k≤ τM ∑	V J8lBMlog 4NK + IBM ∣VF(y®,。)∣
5M
l=1
7√8LνB3/2
5M1/2
NB-1 Λ Γ-KNK	7LB? (N?l
E √ιι ylog-Br+下-1 E li kvF(yk,o)k
≤
8Lν(N3/2 — B3/2)
3M1/2
log
4NK + 7LN(N - B)
Bδ
10
kVF(yk,o)k .
(71)
Lastly,
lrk,3l ≤
11L2 B
7M
N/B-1	l M
XXX
l=1
jB-1
X
j=1 m=1 i=(j-1)B+1
v√8(i - (j - 1)B)log 4mNk
+ (i-(j-1)B) ∣VFm(yk,o)k
≤
…COX1 √i!rlog
4MNK
-δ-
11L2B NI
一[Xl
3L2VN(N-B)(B3/2-1)
M
XlVFm(yk,0)l
m=1
4~4MNK
VlOg -ɪ
2B
2L2N(N - B)(B - 1)
(τ+ρlVF(yk,0)l).
(72)
+
5
32
Published as a conference paper at ICLR 2022
Recalling the definition rk := rk,1 + rk,2 - ηrk,3, we get an upper bound for krkk from (70), (71),
and (72):
krkk ≤ krk,1 k + krk,2k + η krk,3k
,,Γ~Nmnk
≤ LVV log —δ—
24N(B3/2 - 1)	8(N3/2 - B3/2)
11B	+	3M1/2
ι 3ηLN(N - B)(B3/2 - 1)
+	2B
+ Lτ
3N(B - 1)	2ηLN(N - B)(B - 1)
5	+	5
+ L kVF(yk,o)k
3ρN(B - 1)	7N(N - B)	2ηLρN(N - B)(B - 1)
5	+	1θ	+	5
(73)
Recall again that We have K ≥ 7ρκ log(MNK2) and η = logμMNK ), so ηLN ≤ 1/7. Using this
and N - B ≤ N, we can further simplify (73).
krkk ≤ Lν
log
4MNK (12N(B3/2 - 1)	8(N3/2 - B3/2) A 2LτN(B - 1)
一δ — V	5B	+	3M1/2	+ +	3
X--------------V--------------}
=Φ
+ L kVF (yk,0)k(2ρNA + 7NN-)
≤ LV φf^δNK +^NB^ + 牛 kVF (yk,0)k，
(74)
which holds with probability at least 1 - K. The bound (74) holds for all k ∈ [K] with probability
1 - δ if We apply the union bound over k = 1, . . . , K. Next, by (a + b + c)2 ≤ 3a2 + 3b2 + 3c2,
we have
krkk2 ≤ 3L2ν2φ2 log4MNK + 4L2τ2N；(B - 1)2 + ≡⅛2n4 kVF以,。)『,(75)
δ	3	25
which also holds for all k ∈ [K] with probability at least 1 - δ.
Getting a high-probability convergence rate. Given our high-probability bounds (74) and (75),
we can substitute them to (62) and get
F (yk+1,0) - F (yk,0)
≤(-ηN+η2LN2)kVF(yk,0)k2+η2kVF(yk,0)kkrkk+η4Lkrkk2
≤ (-ηN + η2LN2 + RPN2 + 147吸/N4 ) ∣∣VF(y小)『
5	25
+ η2LVΦ Jog 4MNK kVF(yk,0)k + 3η4L3ν2Φ2 log 4MNK
+ -LUB - I) kVF(yk,0)k + 例4“2N2(B - 1)2 .	(76)
The following terms in the RHS of (76) can be bounded using ab ≤ a2 + b22:
η2LνΦyiθgiMNK kVF (yk,0)k
A1/2N1/2	√8η3^LνΦ /	4MNK∖
=(-^rkVF(yk,0)kX	N1/2	Vlog -ɪj
≤ M kVF(yk,0)k2 + 半产log4MNκ,	(77)
2	- - I) kVF(yk,0)k
33
Published as a conference paper at ICLR 2022
(n1/2N1/2
kVF (yk,o)k
)(2V8n3/2LTN1/2(B — 1)
≤ η1N kVF(yk,0)k2 +
16η3L2τ2N(B - 1)2
9
(78)
Substituting (77) and (78) to (76) results in
F (yk+1,0) - F(yk,0)
≤
(-7nN + η2LN2 + — + 1^2N4) kVF(yk,o)k2
8	5	25
n3L2ν2(4 + 3nLN)Φ2	4MNK	4η3L2τ2(4 + 3nLN)N(B - 1)2
+ J~ N log -ɪ +」~'~1一—一
(79)
Again, We have K ≥ 7ρκlog(MNK2) and η = log(μMNK ), so ηLρN ≤ 7. Since the inequality
-8Z + 152z2 + 1257z4 ≤ - 2Z holds on Z ∈ [0,11 ], we have
-7nN + n2LN2 + 7n3 + 147n4L3P2N4
8	5	25
7	12n2LρN2	147n4L3ρ3N4	1
≤- 8 nN + —5— + -25- ≤ - 2 nN.
Substituting this inequality to (79), together with 4 + 3nLN ≤苧< 22, yields
F(yk+1,0)- F(yk,。)≤ -nN kVF(yk,0)k2 + 9n!LNφ! Iog4M-NK + 2n3L2τ2N(B - 1)2.
We now recall that F is μ-P匕 so ∣∣VF (yk,o)k2 ≥ 2μ(F (yk,。)- F *):
F (yk+1,0) - F * ≤ (1 - nμN )(F (yk,o) - F *)
+ 吗产 log 4MNK + 2n3L2τ2 N (B - 1)2.	(80)
2N	-
Recall that (80) holds for all k ∈ [K], with probability 1 - . Therefore, by unrolling the inequality,
F (yκ, N) - F * ≤ (1-nμN )k (F (yo) - F *)
+ (9nR2φ2 log 4MNK + 2n3L2τ2N(B -1)2) Kd(1 - nμN)k
2N	-
k=。
≤ (1 - nμN)κ(F(yo) - F*)
十 9n2L2ν2Φ2 ɪɑg 4MNK + 2n2L2τ2(B - 1)2
2μN2	δ	μ
(81)
12N(B3/2-1)	8(N 3/2 -B3/2)
Recall that Φ :=--k5B---- + > 3M山——-,hence
Φ2 ≤
288N2(B3/2 - 1)2	128(N3/2 - B3/2)2
25b2	+	9Μ
Substituting this inequality and also n = '。虱MNKK ) gives
F (yκ, N) - F *
≤ F(y。)- F*
≤ MNK2
2L2τ2(B - 1)2
~μ3N 2K2-
g2(MNK2)
+
9L2ν2	4MNK 2	2
+ κK2log -ɪ- Iog (MNK)
288N2(B3/2 - 1)2	128(N3/2 - B3/2)2
25B	+	9M
F (y。)- F * 行(L2T2 B2 λ 行(L2ν2 B λ 行(L2ν2	1 λ
MNK2	+ V μ3	N2K2 J	+ V μ3 N2K2 J	+ v μ3	MNK2 J	,
with probability at least 1 - . This finishes the proof.
34
Published as a conference paper at ICLR 2022
D.4 Proof of upper bound for minibatch RR with SyncShuf (Theorem 6)
The first part (“One epoch as one step of GD plus noise”) of the proof is identical to that of Theo-
rem 1. We start from the second part.
Bounding noise term using concentration. It is left to bound krk k. As seen in (41), we have
N/B-1
krkk ≤ LB(1+ DLB)NB X
i=1
i
Xgj
j=1
(82)
Recall from the theorem statement that K ≥ 6κ log(MNK2) and η = log(MNK ). This means that
(1 + ηLB)N∕B =(1 + KBogr^ YB ≤(1 +焉厂 ≤ ei/6.
Next, we bound the norm of
i	M iB
X gj = M XX wmm(j)(xk,0),
j=1	m=1 j=1
(83)
(84)
exploiting our modification SYNCSHUF as well as Lemma 8. For each ▽/&(,) (xk,o), we first add
and subtract its corresponding Vfσm(j)(xk,o), where f := 吉 PM=I fim as defined in Assump-
tion 4. This way, (84) can be decomposed into two sums Pj=ι gj = M (Pi + qi), where
M iB
Pi ：= E EVfσm(j) (Xk,0、
m=1 j=1
M iB
q，：= E EVfmm(j)(χk,0) -Vfσm(j)(χk,0).
m=1 j=1
Using this decomposition, we will derive high-probability bounds for kPi k and kqi k.
To simplify expressions to follow, we decompose iBM (i.e., the total number of component gradi-
ents that are summed up) into a multiple of N and the remainder. Let
α(i) ：
iBM
N
β(i) ：= iBM - N α(i),
so that iBM is decomposed into N α(i) and the remainder 0 ≤ β(i) < N. Using this new notation,
we can write Pi as
N α(i)
M F	M iB
Pi=E Σ Vfσm (j)(Xk,0)+E E	Vfσm(j)(Xk,0).	(85)
m=1 j=1	m=1 -_ Nα(i) I
j= -M +1
Here, recall that with SyncShuf, we defined σm(j) := σ((j + M∏(m)) mod N). With this choice
of “shifted” permutations, one can notice that {σmm(j)}M=Mj=I = [N], meaning that adding 力mj)
for m ∈ [M] and j ∈ [N/M] results in the sum of all N fi's. In fact, this happens if we sum over
m ∈ [M] and any N/M consecutive j,s. From this observation and F = NN PN=1 fi, (85) can be
written as
M iB
Pi = Nα(i)VF(xk,0) + Σ Σ	Vf"((j+ Nm ) mod N)(xk,0).	(86)
m=1	Na⑸ ∣1
J= -M +1
Assume for now that β(i) > 0, i.e., N α(i) < iBM. The summation in the second term of RHS
in (86) is a without-replacement sum (note that the indices j + NMm do not overlap) of β(i) terms.
35
Published as a conference paper at ICLR 2022
Hence, it is equal in distribution to Pef) ^fσ(j)(xk,o). Also, from Assumption 2, it can be easily
checked that for any i ∈ [N]
Il▽力(Xk,0)- VF(xk,0)∣∣ ≤ ν.
These observations mean that We can apply Lemma 8 to get a concentration bound, with M - 1,
v1 J Vfi(xk,0), n J β(i), and δ J 2BNK. By Lemma 8, with probability at least 1 - 2∣K (over
the randomness in σ), we have
1 M iB
函 X X	vfσ((j+ NMm )mod N )(xk,0 ) -VF (xk,O)
β( ) m=1 ] — Nα(i) , 1
J- —M	+1
≤ν
8log 4NK
β(i)
(87)
Combining (86) and (87), we get the following upper bound on kpi k, which holds with probability
at least 1 — 2 NK.
kpik ≤ kiBMVF(xk,0)k +
M	iB
vfσ	vfσ	vfσ((j+ Nm) mod N) (xk,0) - lβ(i) VF(xk,0)
m=1	Na⑴ +ι
j--M +1
C ,, 一一 、,，	4^^一、	4NK
≤ iBM ∣∣VF(xk,0)k + ν∖ 8β(i)∖og-^~
Bδ
≤ iBM kVF(xk,0)k + ν√Nʌʌlog
4NK
Bδ ,
(88)
where the last inequality used β(i) < N. Also recall that we assumed β(i) > 0 in order to use
Lemma 8 and derive (88). However, note that even with β(i) = 0, the bound (88) trivially holds.
We next bound ∣qi ∣. This time, we will apply Lemma 8 to the permutation π over the local ma-
chines. To do this, we will condition on a fixed instantiation of the permutation σ and derive a
high-probability bound that holds with conditional probability at least 1 - 2BNK. The conditional
probability is at least 1 - 2NδK irrespective of the choice of σ, so we can conclude that the (uncon-
ditional) probability that our bound holds is also at least 1 - 2NK.
Without loss of generality, choose the instantiation σ(l) = l for all l ∈ [N]. With this σ, we have
σm(j) := (j + Mπ(m)) mod N, so the vector qi reads
M iB
qi=	Vfm+ M∏(m)) mod N (xk,0) - Vf(j+ Mπ(m)) mod N (xk，0)
m=1 j=1
(89)
Let us consider rewriting this summation as the sum over l ∈ [N], where l appears in the subscript
of the component functions. One can check that
NN	M
l = ( j + Mn mod N ⇔ M | (l - j) and π(m) = (l - j)N mod M,
where a | b denotes “a divides b.” From this, we can rewrite (89) as
N
qi=X X
l=1 j∈[iB]
M1(l-j)
-1((l-j)M mod
(xk,0) - Vfl(Xk,0).
(90)
^^{^^™
=:qi,l
}
By the same reasoning above and below (86), we can see from (90) that for a given index l ∈ [N],
the cardinality of the set Jl := {j ∈ [iB] : M | (l - j)} is either a(i) or a(i) + 1. From this,
we notice that each qi,l is a without-replacement sum of α(i) or α(i) + 1 terms. For now, suppose
α(i) > 0. For each qi,l, we can apply Lemma 8 to it, and show that with probability (conditioned
on the instantiation σ(l) = l) at least 1 - 2NB2κ, We have
kq ；k ≤ 卜,8(α(i))log 4B2K	if |Jl| = α(i),
qi,l — [λ,8(α(i) + 1)log 4BK	if IJlI = α(i) + 1.
36
Published as a conference paper at ICLR 2022
,16Mi)IOg 4BK
Note that cases in the RHS are all bounded from above by λ
. Applying union
bound on all l ∈ [N], we get that with probability at least 1 - 2B^K, We have
kqik ≤ λN Jl6α(i)log4NK ≤ λN J16iBM Iog^^ = 4λJiBMN log4N^ .(91)
Bδ	N	Bδ	Bδ
Now consider the case α(i) = 0. Recall from the definition α(i) := |_iBM_| that α(i) = 0 implies
iBM < N. In this case, qi,l = 0 for N - iBM indices l satisfying |Jl| = 0, and kqi,lk ≤ λ for the
remaining iBM l’s satisfying |Jl| = 1. Summing up, kqik is bounded from above by λiBM, which
is in fact less than the upper bound in (91). Therefore, the bound (91) holds even for α(i) = 0.
Recall that our goal was to find a bound on the norm of Pj=ι gj = M(Pi + qi). From the high-
probability bounds obtained in (88) and (91), with probability at least 1 - NBK,
J	V√N /	4NK	∕iBN /	4N2K
Xgj ≤ iB NF(Xk,0)k + -M-V8log-BT + N~MM F∙	(92)
j=1
We can now substitute (83) and (92) to (82) to get
krkk ≤ e1/6LBNX-IiB kVF(xk,0)k + -√N J8lOg4BK +4入庠 Jog4NK
i=1
1,6rn2	3 .	eV6√8L-N1/2 (N - B) ∕^—4NK
≤ e1/6LB2kVF(xk,o)k E i + --———^---------------√log -Br
i=1	M Bδ
4e1/6LXB3/2N1/2 fN/B r /~4N2K
+ ——/— J1	7tdtVlog -BΓ
≤ LN(N - B) kVF(Xk,o)k + 7L-NC - B) Jlog4NK
2M Bδ
+
7LλN1/2(N3/2 - B3/2)
2—1/2
4~4N2 K
VlOg -bΓ
(93)
which holds with probability at least 1 - K, due to the union bound over i = 1,...,N∕B - 1.
The bound (93) holds for all k ∈ [K] with probability 1 -δ if we apply the union bound over
k = 1, . . . , K. Next, by (a + b + c)2 ≤ 3a2 + 3b2 + 3c2, we have
krkk2 ≤ 3L2N2(N - B)2 kVF(xk,0)k2 +
147L2-2N(N - B)2
4—
4NK
log F
+
147L2λ2N(N3/2 - B3/2)2 1	4N2K
----------4M----------log -bΓ
(94)
which also holds for all k ∈ [K] with probability at least 1 -.
Getting a high-probability convergence rate. Given our high-probability bounds (93) and (94),
we can substitute them to (40) and get
F (xk+1,0) - F(xk,0)
≤(-ηN+η2LN2)kVF(xk,0)k2+η2kVF(xk,0)kkrkk+η4Lkrkk2
≤ (-ηN + η2LN2 + η2LN(N - B) + 3η4L3N2(N - B)2) kVF(xk,0)k2
7η2 L-N1/2(N - B) ∕^~4NK llππz 、“	147η4L3ν 2N (N - B)2,	4NK
+ --2—~- Vlog KkVF (X k,0)k +	4M 2~Llog K
7η2LλN1/2(N3/2 - B3/2) J_4N 2K	EV ...
+--------2—7^--------Vlog-bΓ kVF(xk,0)k
147η4L3λ2N (N 3/2 - B3/2)2	4N2K
+ -.........4—.......L log F∙	(95)
37
Published as a conference paper at ICLR 2022
TWo terms in the RHS of (95) can be bounded using ab ≤ a2 + b2:
7η2LνN1/2(N - B) ∕∣	4NK
------2M------VlogF kvF(xk,0)k
A1/2N1/2	76n3/2LV(N - B) r 4NK∖
=K√^ kVF(Xk叫(---------------M-----------Vlog-bΓ)
ηN	2	49η3L2ν2(N - B)2	4NK
≤ % kvF(Xk,0)k2 +	M2	log F	(96)
7η2LλN1/2(N3/2 - B3/2) /.	4N2K
------2M1/2---------Vlog -BΓ kVF(Xk,0)k
(ηi∕2N1/2	7√2η3〃Lλ(N3/2 - B3/2) ∕^^4N2K\
=∏√^ kvF(Xk叫(---------------B---------------Vlog -bγ)
ηN	2	49η3L2λ2(N 3/2 - B3/2)2	4N2K
≤ ηi6 kvF(Xk,0)k2 + ~η~LM--------L log -ɪ,	(97)
Putting inequalities (96) and (97) to (95) and noting N - B ≤ N gives
F(Xk+1,ο) - F(Xk,ο) ≤ (-7ηN + 2η2LN2 +3η4L3N4) ∣∣VF耳,。)『
49η3L2(4 + 3ηLN)ν2(N - B)2	4NK
+	4M2	og Bδ
49η3L2(4 + 3ηLN)λ2(N3/2 - B3/2)2 1	4N2K
+------------4M--------------log -bT.	(98)
Recall from K ≥ 6κlog(M2NK2) and η = '。鼠MNNK ) that ηLN ≤ 1. Since the inequality
一8Z + 2z2 + 3z4 ≤ 一 1Z holds on Z ∈ [0,16], we have
-7ηN + 2η2LN2 + 3η4L3N4 ≤ -1 ηN.
82
Applying this bound to (98) results in
ηN	2	56η3L2ν2(N - B)2	4NK
F(Xk+1,0)- F(Xk,0)≤ -~2 llVF(Xk,0)k +	M	log Bδ
56η3L2λ2(N3/2 - B3/2)2 1	4N2K
+--⅛------------------log k
We now recall that F is μ-P匕 so ∣∣VF(Xk,0)k2 ≥ 2μ(F(Xk,0) - F*):
F (Xk+1,0) - F * ≤ (1 - ημN )(F (Xk,0) - F *) + 56η3L2νM N - B)2 log 4NK
56η3L2λ2(N3/2 - B3/2)2	4N2K
+	M	og Bδ	( )
Recall that (99) holds for all k ∈ [K], with probability 1 - δ. Therefore, by unrolling the inequality,
and using PK-11(1 - ημN)k ≤ n1N, Weget
F (xk, N) - F * ≤ (1 - ημN )k (F (x0) - F *) +
56η2 L2 ν2 (N - B)
56η2L2λ2(N 3/2 - B3/2)
μMN
2
log
μM 2N
4N2K
2	4NK
"log F
Bδ
(100)
+
Lastly, substituting η = log(^KK)to (100) gives
FY	F * v F N)- F * 工 56L2ν2(N - B)2 log 4NK log2(M 2NK2)
(XK, B)	≤ M 2NK2 +	μ3M 2N 3K2
38
Published as a conference paper at ICLR 2022
ι 56L2λ2(N3/2 - B3/2)2 log 4NδK log2(M2NK2)
+	μ3MN 3K2
F(χo)- F* + O(L (	VV	+ 上口
M 2NK2 + μ3 M 2NK2 + MK2	.
D.5 Proof of upper bound for local RR with SyncShuf (Theorem 7)
The first part (“One epoch as one step of GD plus noise”) of the proof is identical to that of Theo-
rem 2. The first part defines our “noise” rk as the sum of three terms rk := rk,1 + rk,2 - ηrk,3. We
start from the second part.
Bounding noise terms using concentration. We next bound krk k by bounding each krk,1 k,
krk,2 k, and krk,3k. We have already seen from (63), (64), (65), (66), and (67) in Appendix D.3
that
1/7 N/B M	lB-1	i
krk,ιk≤ eM- XX	X	X	Vfmm(My®,。)
l=1 m=1 i=(l-1)B+1 t=(l-1)B+1
7LB N/B-1
krk,2k ≤ IM	工
l=1
M lB
X X Vfσmkm (t)(yk,。)
m=1 t=1
L2B N/B-1 l M	jB-1
…誓XXX	X
l=1 j=1 m=1 i=(j-1)B+1
i
X
t=(j-1)B+1
Vfmm(t)(yk,o)
(101)
(102)
(103)
As in the previous subsections, the key is to bound the norm of the partial sums of Vfσmm (t) (yk,。)
using Lemma 8. For the summations appearing in (101) and (103), we apply Lemma 8 in the same
way as (68). For any i satisfying (j - 1)B + 1 ≤ i ≤ jB - 1, where j ∈ [N/B], and for any
m ∈ [M], the following bound holds with probability at least 1 一 §/长:
i	_________________________
X	Vfmm(t)(yk,o) ≤ V ,8(i-(j- 1)B)log 6Mnk
t=(j-1)B+1
+ (i-j-1)B) kVFm(yk,o)k.	(104)
For the summation that appear in (102), we use the techniques from Appendix D.4. For each l ∈
[N/B — 1], We can similarly decompose Pm=I Pt=ι Vfmm(t)(y®,。)into Pl + qι, where
M lB
pl:=	Vfam(t)(yk,。),
m=1 t=1
M lB
ql ：= XXVfmm(t)(yk,o)-Vfσm(t)(yk,o).
m=1 t=1
As done in Appendix D.4, we can follow the same steps and show a high-probability bound, which
is a slightly different version of (88): with probability at least 1 — 3NK,
kplk ≤ IBM kVF(yk,o)k + ν√N∖Alog6NK.
Bδ
Similarly, for kqlk we can show a slight modification of (91):
kqlk ≤ 4λ JlBMNlog 6NK,
Bδ
(105)
(106)
which holds with probability at least 1 — 3NK. Combining (105) and (106), with probability at least
1 — 3Nδ, we have
M lB
X X Vfσmkm (t)(yk,。)
m=1 t=1
≤ IBM kVF(yk,o)k+ ν√N
8log
6NK
Bδ
39
Published as a conference paper at ICLR 2022
+ 4λ
6	^^^6N 2K
VlBMN bgF~.
(107)
By applying the union bound, with probability at least 1 - K, the bound (104) holds for all m ∈ [M]
and i ∈ SjN=/1B[(j - 1)B + 1 : jB - 1], and the bound (107) holds for all l ∈ [N/B - 1].
We now substitute the bounds (104) and (107) to (101), (102), and (103) to get upper bounds for
krk,1k, krk,2k, and krk,3k, respectively. For krk,1 k and krk,3k, we can apply the same calculations
as in (70) and (72), modulo the fact that Assumption 3 is now implied by Assumption 4, with
constants τ = λ and ρ = 1. We obtain
ll ll 24LVN(B3/2 - 1) ∕^―6MNK 3LN(B - 1) ,、	、”、
krk,ιk ≤ ---11b——)ʌ/log	+	(5) (λ + ∣NF(yk,0)k)
ll ll	3L2νN(N - B)(B3/2 - 1) ∕^―-MNK
E,3k ≤ ——H/---------------- Vl°g -ɪ
2L2N(N-B)(B- 1)
+ ——(~~~——) (λ + kVF (yk,o)k).
5
For krk,2k, we have
(108)
(109)
krk,2k≤ 7LB I- (lBM kVF(yk,0)k + ν√N∖Alog6NK +4λ JlBMNl°g6N2K )
5M	Bδ	Bδ
7LB2 (NW-1,∖gm 、“	7√8LνN1/2(N - B) A—-NK
=M [X IJVF(yk,0)k+ ——5M~)√l°g-ΒT
28LXB3/2N1/2 (NP Λ ∣^λ~6N2K
+	5M=	[ ⅛ 6) Vl°g -ΒΓ
7LN(N - B)	、”	4LνN1/2(N - B) ∕^~6NK
≤ (io )kVF(yk,0)k + ~~M~)Vl°g-Βτ
15LλN1/2(N3/2 - B3/2) ∕^~6N2K
+ ——4MV2——-Vl°g F	(IK))
Recalling the definition rk := rk,1 + rk,2 - ηrk,3, we get an upper bound for krk k from (108),
(109), and (110):
krkk ≤ krk,1k + krk,2k + η krk,3k
,,/, 6MNK
≤ LVV l°g —^—
(24N(B3/ - 1)	4N1/2 (N - B)
V 11B + M +
3ηLN(N - B)(B3/2 - 1)
2b
3N(B - 1)	15N1/2(N3/2 - B3/2) [、~6N2K	2ηLN(N - B)(B - 1)
+ Lλ	5— +--------4M^--------V l°g ~bΓ +---------5-------
+ L kVF(yk,0)k
3N(B - 1)	7N(N - B)	2ηLN(N - B)(B - 1)
5	+	1θ	+	5
(111)
Recall again that We have K ≥ 7κ l°g(M2NK2) and η = '。虱MNNK ), so ηLN ≤ 1/7. Using this
and N - B ≤ N, we can further simplify (111).
krkk ≤ LV
l°g
6MNK (12N(B3/2 - 1)	4N1/2(N - B)
~δ~ V	5Β	+ M
S------------{z----------
=Φν
+ Lλ
l°g
6N2K 2N(B - 1)
Bδ V 3	+
15N 1/2(N3/2 - B3/2) ∖
4M1/2	)
} -	-
{z
=Φλ
40
Published as a conference paper at ICLR 2022
2N(B - 1)	7N(N - B)
+ L kVF(yk,o)k (-k3-) + —4o-~)J
6	6MNK	6	6N 2K	7LN2
≤ LV Φν fog -τ~ + LλΦλ NIog -Br + -5— kVF (yk,o)k,	(112)
which holds with probability at least 1 - K. The bound (112) holds for all k ∈ [K] with probability
1 -δ if we apply the union bound over k = 1, . . . , K. Next, by (a + b + c)2 ≤ 3a2 + 3b2 + 3c2,
we have
krk k2 ≤ 3L2ν2ΦV log 6MBK + 3L2λ2Φλ log (BK + 147L2N4 ∣∣VF®k,0)『,(113)
δ	Bδ 25
which also holds for all k ∈ [K] with probability at least 1 - δ.
Getting a high-probability convergence rate. Given our high-probability bounds (112) and
(113), we can substitute them to (62) and get
F (yk+1,0) - F (yk,0)
≤(-ηN+η2LN2)kVF(yk,0)k2+η2kVF(yk,0)kkrkk+η4Lkrkk2
≤ (-ηN+ηLN2 + 7η2LN2 + 147η2L3N4) kVF(yk,0)『
5	25
2τ 6-MNK	4τ3 2*2-MNK
+ η LνΦνylog	δ- ∣∣VF(yk,o)k +3η4L3ν2φV log	$-
+ η2LλΦλ∖∕log6BK kVF(yk,0)k + 3η4L3λ2Φλ log 6BK.	(114)
Bδ	Bδ
The following terms in the RHS of (114) can be bounded using ab ≤ a2 + b2:
η2LνΦν∕log 6MBK kVF(yk,0)k
(#/2N1/2	建//2LVΦν ∕	-mnk∖
=(kVF(yk,0)kX	N1/2	Vlog-ɪj
ηN	2	4η3L2V2Φ2	-MNK
≤ 金 kVF(yk,o)k2 +	ν log —,	(115)
η2LλΦλ∖/log 6B2K kVF(yk,o)k
Bδ
(产N1/2	√8η3^LλΦλ /	6N2Kʌ
=(kVF(yk,0)kX	N1/2	VlogF)
ηN	2	4η3L2λ2Φ2λ	6N2K
≤ -⅛ I∣vf (yk,o)k + —B— log ~^~.	(116)
Substituting (115) and (116) to (114) results in
F (yk+1,0) - F (yk,0)
≤ (-7ηB +1⅛N2 + ^4LpB) kVF(yk,o)∣2
8	5	25
η3L2V2(4 + 3ηLN)Φ2	6MNK	η3L2λ2 (4 + 3ηLN)Φ2	6N2K
+ -- N	log -ɪ + -- N	log k∙	(117)
Again, We have K ≥ 7κlog(M2NK2) and η = '。鼠MNNK ), so ηLN ≤ 11. Since the inequality
-8Z + 152z2 + 1251 z4 ≤ -2Z holds on Z ∈ [0, 1 ], We have
7 12η2LρN2	147η4L3ρ3N4
-8 ηN +5+	25
≤ - 2 ηB.
41
Published as a conference paper at ICLR 2022
Substituting this inequality to (117), together with 4 + 3ηLN ≤ 371 < 9, yields
F(yk+ι,0)- F(yk,o) ≤ -η∣- l∣vF(yk,0)k2 +
9η3L2ν2ΦV
2N
6MNK
log —
9η3L2λ2φλ
2N
log
6 N 2K
Bδ
+
We now recall that F is μ-PL, so ∣∣VF ®k,o)『≥ 2μ(F (yk,o) - F *):
9η3L2ν2Φ2	6MNK
F (yk+1,0)- F * ≤ (1 - ημN )(F (yk,o) - F *) + η 2N	V log —δ-
9η3L2λ2φλ	6N 2K
+ —2N — og Bδ '
(118)
Recall that (118) holds for all k ∈ [K], with probability 1 -δ. Therefore, by unrolling the inequality,
and USing PK-OI(I - ημN)k ≤ ημ1N, we get
F (yκ,N) - F * ≤ (1 - ημN )K (F (yo) - F *) +
+
9η2L2λ2φλ	6N 2K
2μN2 og ~HΓ
9η2L2ν2Φ2ν	6MNK
2μN 2	log —
(119)
Recall that Φν := 12N(B3/2T) + WI and 中1:=当0 + 15N”(N弋[-B3/2, hence
5B	M	3	4M /
288N2(B3/2 - 1)2	32N(N - B)2
25B	+ M2
Φ2λ ≤
8N2(B - 1)2	225N(N3/2 - B3/2)2
9	+	2M
Substituting these inequalities and also η
log(M 2NK2)
μNκ
gives
F In κ, B) - F *
F (yo)- F *	9L2ν2 log 6MδNκ log2(M 2NK2) ( 288N2 (B3/2 - 1)2	32N(N - B)2
≤ M 2NK2 +	2μ3 N 4K2	1	25B	+ M2
9L2λ2 log 6BK log2(M2NK2) (8N2(B - 1)2	225N(N3/2 - B3/2)2、
+	2μ3N 4K2	V 9	+	2M	)
_ F (yo) - F *	- (L2	( V 2B ν2 λ2B2 λ2 N
= M 2NK2	+ Vm3 N 2K2	+ M2NK2	+ N 2K2	+ MK2	,
with probability at least 1 - δ. This finishes the proof.
D.6 A generalized vector-valued Hoeffding- S erfling inequality
We extend the vector-valued Hoeffding-Serfling inequality proved in Schneider (2016) to account
for the mean of multiple independent without-replacement sums.
Lemma 8. Suppose there are MN vectors {vm}M=N,i=ι ∈ Rd that satisfy ∣∣vm — Vmk ≤ V
for m ∈ [M], where Vm := N PN=I Vm. Consider M independently and UnifarmIy SamPIed
permutations σι,..., om 〜Unif(SN). For any n ≤ N 一 1, with probability at least 1 一 δ, we have
1
Mn	M
Mn XX vmm(i) - M X Vm
m=1 i=1	m=1
≤V
8(1- nN1 )log 2
Mn
(120)
Proof. The proof is an extension of Theorem 2 of Schneider (2016) which proves the M = 1 case
for vectors in smooth separable Banach spaces. We prove our extended concentration inequality for
Rd, but we note that the proof technique can be applied directly to general smooth separable Banach
spaces, as done in Schneider (2016). Below, we state a special case of Theorem 3 of Pinelis (1992)
and Theorem 3.5 of Pinelis (1994), because this Rd case serves our purpose.
42
Published as a conference paper at ICLR 2022
Lemma 9 (Pinelis (1992; 1994)). Suppose that a sequence of random variables {xj }j≥0 is a mar-
tingale taking values in	Rd,	and Pj∞=1	ess sup	kxj	- xj-1	k2	≤	c2	for some c >	0.	Then, for
λ>0,
P sup{kxj k : j ≥ 0} ≥ λ ≤ 2exp
λ
2c2
The proof of Lemma 8 proceeds by defining a sequence of random variables {xj}, showing that it is
a martingale, and applying Lemma 9 to prove our concentration bound. For m ∈ [M], define index
functions km : N ∪ {0} → [0 : n] in the following way:
(0	if j ≤ (m — 1)n,
km(j) := max{0, min{n, j - (m - 1)n}} = j - (m - 1)n if (m - 1)n + 1 ≤ j ≤ mn,
In	if j ≥ mn + 1.
Using these index functions, we introduce the following sequence of random variables {xj }:
M	km(j)
Xj = X N—Mj) X — vm),
m=1	i=1
and show that this is a martingale, i.e.,
E[xj | x1, . . . , xj-1] = xj-1	(121)
for all j ≥ 1. Notice first that by definition of km’s we have xMn = xMn+1 = xMn+2 = . . . , so
(121) is trivially satisfied for all j > Mn. Next, for any j satisfying (l — 1)n + 1 ≤ j ≤ ln where
l ∈ [M], we have
l-1	n
Xj = X N^ X(明⑺
m=1	i=1
1	kl(j)
- vm)+ N — kι(j) X(VMi)- Vl)
1	1	kl(j)-1 l	l
=XjT + (N — kl(j) - N — kl(j) + 1) ∑ (Vσl⑴- V )
+ N — kl(j) (vσMklj)) - Vl)
Now note that for any k ∈ [N — 1], we have
(122)
EVl(k) — vl| σl(1),...,σl(k — 1)]
—
1N
N⅛ XWl(i) - Vl)
i=k
k-1
N⅛ X(Vσ l⑴-Vl),
i=1
where the last equality used PN=I(v：Mi) - Vl) = 0.
1__________1	= ________1__________
N-kl(j) - N-kl(j)+1 = (N-kl(j))(N-kl(j) + 1),
Using applying this fact to (122) and noting
1	kl(j)-1
E[Xj|X1，...，XjT]= XjT +(N -kl (j))(N - kl (j) + 1) X W l(i) - Vl)
+ N —kl (j)叫咪 l(kl(j)) - Vl | xi,..., Xj-i]
= Xj-1,
hence proving that {Xj }j≥0 is a martingale. We now apply Lemma 9 to our {Xj }. For j such that
(l - 1)n + 1 ≤ j ≤ ln, notice from (122) that
1	kl(j)-1
(N - klCj))(Xj- XjT) = (N - kl(j) + 1) X (V： l(i) - Vl ) + (V'σ l(kl(j)) - Vl)
43
Published as a conference paper at ICLR 2022
N
1
(N - kι(j ) + 1)
Σ (Vσi(i)- Vl)
i=kl (j)
+(Vσ ι(kι(j)) - vl),
which leads to
Vmin{kl(j)- 1,N - kl(j) + 1}
(N - kl(j)) kxj - Xj-Ik ≤ ----------N - .(j) + 1---------+ V ≤ 2ν,
by the triangle inequality. From this, we get the bound c2 in the statement of Lemma 9:
∞	Mn	n	4V2
EesssUP kxj - Xj-I『=EesssUP IlXj - Xj-I『≤ ME (N - k)2
j=1	j=1	k=1
4V2M	N-1	1	4V2M	4V2M(n - 1)	4V2 Mn	n - 1
-------+4ν 2M	≤----------1---------= =------ 1------
(N - n)2 +	+]k2 ≤ (N - n)2 + (N - n)N	(N - n)2	N 卜
where the second inequality used the inequality that Pk.=a+1 吉 ≤ °：-0L) (Serfling, 1974,
Lemma 2.1). Now, applying Lemma 9 to {xj } with c2 = (N2Mn2(1 - n-1) gives
P( kχMnk ≥ λ)	≤ P(sup{kχjIl:j ≥	0}	≥	λ)	≤ 2eχp	(-8ν2MN-z⅛Ξiy)	.	(123)
Recall from the definition of {Xj } that
Mn
XMn = N— XX(%(i)- vm).
m=1 i=1
Substituting λ = Mnn to (123) gives
P (II ɪ X X (Vm ⑴- vm)	≥ e! ≤	2exp (-	,MJ ],
UMn 与 M σm ⑺	J~ V	8ν 2 (1-nN1))
which finishes the proof.	□
D.7 HOW CAN WE AVOID UNIFORM BOUNDS OVER Rd IN OUR ASSUMPTIONS
In Section 2, we introduced Assumptions 2, 3, and 4 on the intra- and inter-machine deviation. The
assumptions required that inequalities such as ∣∣Vf,m(x) 一 VFm(x)k ≤ V hold for all X ∈ Rd. In
this subsection, we discuss more on this strong requirement “entire Rd.”
In fact, the entire-Rd requirement is posed in our assumptions to simplify the exposition of the main
results, and is not strictly necessary. One can easily check from our proofs that the assumptions are
only applied to the beginning iterates xk,0 (for minibatch RR) or yk,0 (for local RR) of epochs.
Hence, if these iterates lie in a bounded set, then the constants V, τ , ρ, and λ may become much
smaller, depending on problem instances. Actually, if we explicitly assume that the iterates lie in
a compact set S,12 then Assumptions 2-4 are even guaranteed to hold for some constants; e.g., for
Assumption 2, we can choose
V:= max	kVfim (x)-VFm (x)k,
m∈[M],i∈[N],x∈S
since the maximum always exists.
However, assuming that the iterates lie in a specific set S can be problematic because the distance
that the iterates travel depend on the objective functions. One cannot know a priori if all iterates will
stay in a fixed set S; hence, explicitly assuming bounded iterates should be avoided.
Then, a natural question is whether we can prove bounded iterates under some reasonable conditions,
instead of assuming it. We point out that this can be done by applying the technique developed in
Ahn et al. (2020, Theorem 1) to our upper bound theorems. Using the technique, a modified version
of our Theorem 1 can be written as follows:
12This bounded iterates assumption is indeed used in some existing results such as Haochen & Sra (2019);
Nagaraj et al. (2019); Rajput et al. (2020); Ahn et al. (2020).
44
Published as a conference paper at ICLR 2022
Theorem 10 (Best-iterate version of Theorem 1). Suppose that minibatch RR has parameters sat-
isfying Assumption 1. Assume that all local component functions fim are L-smooth, the global
objective function F is μ-PE and the Set ofglobal minima of F is nonempty and compact. Consider
running the algorithm using step-size η = B Iog(MNK ) and initialization x1,0 := xo, for epochs
K ≥ 6κ log(M N K2). Then, with probability at least 1 - δ,
k∈m+i] F (XkQ- F * ≤ F⅛NκF1 + O (L32 MNK2),
where the constant ν < ∞ is defined as
V := sup max max IIVfm(x) -VFm(x)k .	(124)
x:F (x)≤F (x0) i∈[N] m∈[M]
In the theorem, We used xκ+1,0 to denote the last iterate of the algorithm xκ, N. Theorem 10
differs from Theorem 1 in three aspects: 1) it considers the best-iterate, not the last-iterate; 2) it
additionally assumes that the set of global minima of F is nonempty and compact, Which alWays
holds if F is strongly convex; and 3) it does not rely on Assumption 2, but instead “proves” it for
the F(x0)-sublevel set of F (124). Note that the constant ν (124) can be much smaller than the
uniform bound required to make Assumption 2 hold for the entire Rd. For Theorems 2, 6, and 7, We
can also apply similar techniques to prove best-iterate bounds With smaller intra- and inter-machine
deviation constants ν, τ , ρ, and λ; We omit the precise statements.
We conclude this subsection With the proof of Theorem 10.
Proof. The proof folloWs that of Ahn et al. (2020, Theorem 1).
Existence of ν. We first shoW the existence of ν < ∞ (124). The global objective function F is
μ-P匕 If we denote the set of global minima of F as X*, the set X* is nonempty and compact by
assumption. Then, by Karimi et al. (2016, Theorem 2) μ-PE functions satisfy quadratic growth, i.e.,
denoting by x* the closest global minimum in X* to the point x,
F (x) - F * ≥ 2μ ∣x - x*k2 .
Define the sublevel set S := {x | F(x) ≤ F(x0)}. Due to the quadratic growth property, we have
F (x0) — F * ≥ F (x) 一 F * ≥ 2μ ∣∣x 一 x*『for all x ∈ S. This implies that
S := {x ∈ Rd | F(x) ≤ F(xo)} ⊂ (X ∈ Rd∣∣x - x*∣2 ≤ F(x0) - F*
2μ
Since we assumed that X* is compact, S is also bounded, and hence compact. Now, for any m ∈ [M]
and i ∈ [N], ∣Vfim(x) - VFm(x)∣ is a continuous function on a compact set S, so there must exist
a constant νim < ∞ such that ∣Vfim(x) - VFm(x)∣ ≤ νim for all x ∈ S. Taking the maximum of
νim over all m and i gives ν.
Proving the best-iterate bound. With the constant ν (124), if all the iterates {xk,0}k∈[K+1] stay
within the sublevel set S := {x | F(x) ≤ F(x0)}, one can consider Assumption 2 to be true with
constant ν. From this observation, we consider two cases:
1.	All the iterates {xk,0}k∈[K+1] stay in the sublevel set S.
2.	There exists an iterate xk,0 ∈/ S.
In fact, the first case can be proven by exactly the same steps as Theorem 1, described in Ap-
pendix D.2.
For the second case, suppose that there exists an iterate xk,0 that escapes the sublevel set S. Let
k0 ∈ {2, . . . , K + 1} be the first such k. Then, since xk0-1,0 is still in S, it follows from (50) in
Appendix D.2 that we have
F (Xk，,0)-F * ≤ (1 -ημN )(F (xkj,0)-F *)+15η3L2ν 2(^3/2 - B3/2)2 log2NK. (125)
M N	Bδ
45
Published as a conference paper at ICLR 2022
However, the fact that xk0,0 ∈/ S and xk0-1,0 ∈ S implies
F (xk0,0) > F(x0) ≥ F (xk0-1,0).
(126)
Combining the two bounds (125) and (126), we get
0 <-ημN(F(Xko-ι,o) - F*)+ 15η3L2ν2(N；	log 2NK
MN	Bδ
which implies
umi+uF(Xk，o)- F *≤f (XkrO)- F * <
15η2L2ν2(N3/2 — B3/2)2	2NK
μMN2	og Bδ
Substituting η
log(MΝK 2) 13
μNK
gives the desired bound and finishes the proof.
□
E Proof of lower bound for minibatch RR (Theorem 3)
For Theorem 3, we consider three step-size ranges and do case analysis for each of them. We
construct functions for each corresponding step-size regime such that the convergence of minibatch
RR is “slow” for the functions on their corresponding step-size regime. The final lower bound is
the minimum among the lower bounds obtained for the three regimes. More concretely, we will
construct three one-dimensional functions F1(x), F2(x), and F3(x) satisfying L-smoothness (1),
μ-PE condition (2), and Assumption 2 such that* 14
•	Minibatch RR on F1 (x) with η ≤ μNκ and initialization xo = V results in
E[Fι(xκ,B)] = ω (Vr).
•	Minibatch RR on F2(x) with η ≥ μNκ and η ≤ ∏3LN and initialization xo = 0 results
in
E[F2(xκ,B)] =。(μMNK2) ∙
Note that the step-size range requires K ≥ 513κ, hence this lower bound occurs only in
the “large-epoch” regime, i.e., K & κ.
•	Minibatch RR on F3(x) with η ≥ μNκ and η ≥ 513LN and initialization xo = 0 results
in
E[F3(xκ,B)] = a (μMNK) ∙
Then, the three dimensional function F ([x, y, z]>) = F1(x) + F2(y) + F3(z) will show bad con-
vergence in any step-size regime. Furthermore,
μI W min(V2Fι, V2F2, V2F⅛)I W V2F W max(V2F1, V2F2, V2F⅛)I W LI,
that is, if Fι, F2 and F3 are μ-strongly convex and L-smooth, then so is F. Moreover, since the
component functions in each coordinate are designed to satisfy Assumption 2 with V, the resulting
three dimensional function F also satisfies Assumption 2 with √3ν.
Since the final lower bound is the minimum among the lower bounds obtained in the step-size ranges,
the lower bound becomes Ω (μMNκτ) if K ≥ 513κ, and Ω (.M；K ) if K < 513κ (in which case
the second step-size range does not exist).
In the subsequent subsections, we prove the lower bounds for F1, F2, and F3 separately.
B log(MΝK2)
μNK
13Recall that this is different from η
in the theorem statement, because for the proofs, we
consider an equivalent “rescaled” version of minibatch RR defined in the beginning of Appendix D.2.
14In fact, the functions constructed in this theorem are μ-strongly convex, which is stronger than μ-PL
required in Definition 1.
46
Published as a conference paper at ICLR 2022
E.1 LOWER BOUND FOR η ≤ -ɪ^
μN K
Consider the case where every function at every machine is the same: for all i ∈ [N] and m ∈ [M],
22
fim(χ):=看.Hence, F1(χ)=〜.
Let Xk,o and Xk N denote the iterates where the k-th epoch starts and ends respectively. Then,
χk+1,0 = Xk, N = (1 - ημ)B χk,o.
Initializing at x1,0 = V and unrolling this for K epochs, We get
NK
V〉V
μ 4μ
since N ≥ 2, K ≥ 1, and B divides N. Hence, Fι(xK,N) = Ω( V).
E∙2 LOWERBOUNDFOR η ≥ μBK AND η ≤ 513BN
For most part of this subsection, we consider iterates within a single epoch, and hence we will omit
the subscripts denoting epochs. Let X0 denote the iterate at the beginning of the epoch, and Xi denote
the iterate after the i-th communication round in that epoch. In our construction, each machine will
have the same set of component functions, that is, there will be no inter-machine deviation. We
therefore omit the superscript m from the local component functions fim . The function we construct
for the lower bound and its component functions are as follows:
1 (N	N	∖
F2(X) := N Ef+1(x) + E f-1(x) , where
i=1	i=N+1
X2
f+ι(X) = (L1x≤o+ μ1χ>o)^2^ + νx, and
X2
f-ι(X) = (L1x≤o + μ1x>o)—- VX
Note that the function F?(x) = (L1χ≤o + μ1χ>o) χ2 is μ-strongly convex and L-smooth with
minimizer at 0, and also satisfies Assumption 2.
Let σm be a random permutation of N +1's and NN -1's. Then, machine m computes gradients on
f-1 and f+1 in the order given by σm. Let σjm denote the j-th ordered element ofσm. Then,
▽fom (X) = (L1x≤0 + μ1x>O)X + νσj .
Hence, the last iterate of an epoch, X N, is given by
N-1 (	M (i+1)B
XN-x°=X (-MBX X vfσm(Xi)
i=0	m=1 j=iB+1
M (i+1)B
-MB X X ((L1χi≤0 + μ1Xi>0)χi + νσjm)
m=1 j=iB+1
M (i+1)B	∖
-MB X X (L1χi≤0 + μ1χi>o)Xi I	(Since PN=I σjm = 0)
m=1 j=iB+1
N-1
-η	(L1xi≤0 + μ1xi>0)Xi.	(127)
i=0
N-1
X(
i=0
N-1 /
Xl
i=0
47
Published as a conference paper at ICLR 2022
N -J
Thus, E[xN - xo] = -η Ei=O E[(L1χi≤o + μ1χ>o)xi]. We want to prove that E[xN] keeps
increasing over an epoch, that is E[χN - xo] > 0 when xo is close enough to the minimizer 0.
For this, we first consider the case where the first iterate xo of the epoch satisfies xo ≥ 0. The
xo < 0 case will be considered later. For the case xo ≥ 0, we will show that whenever xo is small,
the expected amount of update made in the (i + 1)-th iteration, E[(L1χi≤o + μ1χi>o)x/, is negative
in the first half of the epoch and not too big in the second half.
We use the following lemmas, proven in Appendices F.1 and F.2, respectively.
Lemma 11. For xo ≥ 0, 0 ≤ i ≤ 12N ∣, η ≤ 十BrN, and L ≥ 7695,
2 2B	513LlV	μ
6 ηLν i
E[(L1χi≤o + μ1x>O)Xi] ≤ 7LXO - 1536 ʌ/ mb
Lemma 12. For xo ≥ 0, 0 ≤ i ≤ N — 1, and η ≤ 5、BLN,
E[(L1χi≤o+Mx〉。)”] ≤μ (1+5¾2L) χ0+5⅞F∖∕m⅛.
The key intuition is that, for L big enough, We can use the lemmas above in (127) to get
B-1
E[x N - Xo] = -η E (L1χi≤0 + μ1χi>o)xi
i=0
N
≈ ω η-ηLν
B
Whenever |x0 | is small. Multiplying the above by K (for K epochs) Will give us the required loWer
bound (UP to factors of L). We will make this approximate calculation precise in the rest of the
proof.
Using the two lemmas above in (127), we get that
E[x N - xo]
B-1
=-η): E[(L1Xi≤0 + μ1x>O)xi]
i=o
b NC	N-1
=-η E E[(L1χi≤o + μ1χi>o)xi] - η E	E[(L1χi≤o + μ1χ>o)xi]
i=0	i=b N C+1
I N I /	,_、
L 6	ηLν i\
≥ - η 入(7LxO-询 V MB)
X1	513iηL	513ημν r i ʌ
-η二(μ (,1 + ^I^ Jxo + ^i^ MBb)-
i=b 2Bc + 1 '	J
Since iηL ≤ ηLBN ≤ +,μ ≤ TL5, and N/B ≥ 2, the following bound holds:
(128)
b 2NBC	N-1	/
X 7 L + X μ (1 +
i=0	i=b N C + 1	\
513iηL
512
2B
6LN
≤ ~BB~ +
6L	N
+ 1R+ (B
LN / 7LN
7680B ≤ 8B .
N
2B
- 1； 7695 11 + 512
(129)
≤
N
L
1
—
48
Published as a conference paper at ICLR 2022
Also, note that [备C ≥ 备 whenever N/B ≥ 2. We have
b
2
≤ —
一3
b 2NB C
XXX √i ≥Z
N T	「
∙MXc+J≤ Z
2N 3/2
9√3B3/2 ,
3/2
N3/2
≤ 2B3/2.
(130)
(131)
Substituting the bounds (129), (130), and (131) into (128), and using μ ≤ 7L5,
7ηLN
EixN - χ0] ≥-oʃɔ χ0 +
B	8B
7ηLN
≥- ^8B-x0 +
η2LνN 3/2
513η2μνN 3/2
6912√3M1/2 B2
η2LνN 3/2
56000M1/2B2 .
1024M1/2B 2
(132)
—
For the other case x0 < 0, we have the following Lemma, which we prove in Appendix F.3:
Lemma 13. If η ≤ 51BVL and an epoch starts at xo < 0, then
E[xN | xo < 0] ≥ fl - 7⅛N)x0.
B	8B
Further, if the first epoch of the algorithm is initialized at 0, then for any starting iterate xo of any
following epoch, we have P(xo ≥ 0) ≥ 1/2.
Using (132) and Lemma 13 we get
E[x N ] = P(xo ≥ 0)E[x N | xo ≥ 0] + P(xo < 0)E[xN | xo < 0]
≥ P(xo ≥ 0)
7ηLN∖
8B )
xo +
η2LνN 3/2
56000M 1/2B2 ) + (x0 < )
1-
7ηLN
FJxO
Λ	7ηLN)	η2LνN3/2
≥ ∖	8B~ J x0 + 112000M1/2B2 .
Thus far, we have characterized the expected per-epoch update, starting from the initial iterate xo and
iterating until the last iterate xN of the epoch. Now recall that we run the algorithm for K epochs.
B
Using xk,i to denote the i-th iterate of the k-th epoch, we get a lower bound on the expectation of
the last iterate xfc, N if we initialize at x1,0 = 0:
7ηLN∖ K	η2LνN3/2	K-	7ηLN
[xK, B ]≥( - E) x1,0 + 112000M1/2B2 ∑l - F
η2LνN3/2	1 - (1 - 7⅛v)
112000M1/2B2	7r∣LN
(Since η ≥ μNκ)
Note that since μ ≥ 7695 and K ≥ 5μL (which is implied by μVBκ ≤ η ≤ 5i⅛v),
1 -(1 - MIK ≥ 1 - e-7L ≥ 1 - e-6733 ≈ 1.
8μK	—	—
49
Published as a conference paper at ICLR 2022
Hence, We get from η ≥ μNκ that
E[xκ,B] = ω (MNBF) =ω
μM1/2N1/2K	,
ν
and by Jensen’s inequality, We finally have
1	ν2
E[F(XK,B)] ≥ 2E[μxκ,B] = a(〃E[xK,B] ) = ω μμNκκ2
E3 LOWER BOUND FOR η ≥ μΝBK AND η ≥ ≡⅛
Similar to earlier parts of the proof, here as Well, each machine Will have the same component
functions, that is, there Will be no inter-machine deviation. The proof uses a similar construction as
Safran & Shamir (2020; 2021):
1	N2	N
F3(x) := K Ef+ι(X)+ E f-ι(χ) , where
V=1	i= N+ι
f+1(X) :
Lx2 + νx, and f-ι(x) := ILx - νx.
2	,	-1	2
Hence, F3(x) = Lx2, and has its minimizer at 0.
We first compute the expected “progress” over a given epoch. For simplicity, let us omit the subscript
for epochs for now. Let X0 denote the iterate at the beginning of the epoch and Xi denote the iterate
after the i-th communication round in that epoch. For a given epoch, let σm be the permutation of
NN +1's and NN -1's, sampled by machine m. Then,
MN
XB = XN-1- MB X X (LXN-1 + νσm)
m=1 j = ( B-1)B + 1
MN
= (IinL)XNT- MB X X	σjm
m=1 j = ( B-1)B + 1
N	MiB
(1 - nL)bX0- MB X(1 - nL)B-i X X	σjm.
i=1	m=1 j=(i-1)B+1
For the rest of this proof, Xi2 refers to the square of the i-th iterate. Then,
E[XN] = (i-nL芦X0 - 2nν(IMBBx°E
M	iB
](1-nL)B-i X	X	σj
1	m=1 j=(i-1)B+1
2 2	B B	M	iB
+M⅛E	1X(1 - nL) B-i X	X	σ
i=1	m=1 j=(i-1)B+1
22
(1 - nL 芦 X0 + M⅛ E
’B	MiB
X(1 - nL) B-i X X	σj
i=1	m=1 j=(i-1)B+1
(133)
where we used the fact that E[σjm ] = 0. Further, because σm and σm0 are independent and identi-
cally distributed for different m and m0, we get that
E
B	MiB
X(1-nL)B-i X X	σj
i=1	m=1 j=(i-1)B+1
50
Published as a conference paper at ICLR 2022
iB
2^∣
E I XX(1-ηL)B-i	X	σm
m=1 i=1
j = (i-1)B + 1
iB
EE E(I-ηL)B-i	E
_m
σj
m=1
i=1
j = (i-1)B + 1
iB
EE E(i-ηL)B-i	E
m
σj
iB
m=m0
i=1
iB
j=(i-1)B + 1
2
X(1 - ηL) b-i	X	σm0
i=1
j=(i-1)B + 1
X(1 - ηL)B-i	X σi
i=1
j=(i-1)B + 1
M B
M
+
M E
N
N
2
N
一 N
N
E
-N
where the last equality used the fact that E[σm] = 0 for all m ∈ [M] and i ∈ [N], and that σm are
identically distributed. Since we only consider the permutation σ1 (i.e., the one for machine 1) from
now on, we henceforth omit the superscript. Substituting this to (133) gives
N
2
22
E[xN ] = (1 - ηL) ɪ x0 + MR2 E
B	MIB
iB
X(1-ηL)B-i	X σ
(134)
i=1
j=(i-1)B + 1
^{^^^^^^^^^^^^^^≡
=:Φ
、
,
From (134), We have calculated the per-epoch expected update. Recall that We run the algorithm
for K epochs. Using xk,i to denote the i-th iterate of the k-th epoch, we get a lower bound on the
expectation of the last iterate χk, n squared:
2NK r	η2ν2 ɪ X—、/	ʃ、2Nk
E[xk,BI = (I- ηL) B χ1,0+ MB2 φ∑(1 - ηL) B ≥
k=0
where the inequality used x1,0 = 0 and PK=01(1 - ηL)喑 ≥ (1 - ηL)0
the expectation term, i.e., Φ, defined in (134).
η2ν2
----Φ
MB2 ,
(135)
1. Next, We analyze
iB
Φ:= E I X(1-ηL)B-i	X σ
i=1
j=(i-1)B + 1
N
2
N^	,z
=X(1 - ηL)2(B-L⅛kc-1)σj2 + X(1 - ηL)B-L⅛kCT(I - ηL)B-Ljɪc-1E[σjσj,]
j=1	j=j0
Noting that σ2 = 1 and E[σ7-σ7-/ ] = - N-ɪ, we get
B-ι
Φ = B X (1 - ηL)2j
j=0
B-ι
=B X (1-ηL)2j
j=0
—
—
—
B2
N - 1
n1-1 X(1 - ηL)b-L得c-1(1 - ηL)B-L⅛iCT
1
N - 1
j=j0
'/ N
X(1 - ηL)B-L得CT
2
—
N
X(1-ηL)2 B T jB1 CT)
j=ι
1
N - 1
N B-ι
N X (1 - ηL)2j
J=0
2	B-ι
-B X (1 - ηL)2j
j=0
51
Published as a conference paper at ICLR 2022
B2(N - 1)
N - 1
(136)
Note that the term in the parenthesis is exactly the right hand side of Equation (23) in Safran &
Shamir (2020) modulo n and α replaced with B and ηL, respectively. Hence, by Lemma 1 of
Safran & Shamir (2020), we have
1
1-
η2L2N31
B3	卜
(137)
for some universal constant c > 0. Using the fact that η ≥ 51BN, it is easy to check that the RHS
of (137) is lower-bounded by 袅,where c0 > 0 is a universal constant. Combining (135), (136), and
(137) gives
E[x2 ] ≥ ην B2(B - 1)	c0 = c0ην2 N - B
[ k，N] ≥ MB2 N - 1 ηL = LMBN - 1 .
Since 2B divides N, We have B ≤ N/2. Since N ≥ 2, we have N--I ≥ 2(NLI) ≥ 1. Using this
and the fact that η ≥ μBκ, we get
L	c0ν2
E[F3(χk,N)]= 2 E[xk,N] ≥ 4μMNK.
F Proofs of helper lemmas for Appendix E
F.1 Proof of Lemma 11
First, if i = 0 then the lemma trivially holds, because x0 ≥ 0 gives
6
E[(L1χo≤0 + μ1χo>o)xo] = μxo ≤ 7Lxo.
The inequality holds because μ ≥ 7695.
For the rest of the proof, we consider the case 1 ≤ i ≤ 2B. By the law of total expectation we have
M iB
XX
σj > 0 I E
m=1 j=1
M iB
(LIxi≤0 + μ1χi>0)Xi ∑∑σm > 0
m=1 j=1
M iB
XX σjm ≤ 0I E
m=1 j=1
M iB
XX σjm > 0I LE
m=1 j=1
M iB
(L1xi≤0 + μ1xi>0)Xi XX σm ≤ 0
m=1 j=1
M iB
xiXXσjm>0
m=1 j=1
M iB
XX σj ≤ 0I μE
m=1 j=1
M iB
xiXXσjm ≤0
m=1 j=1
(138)
where the last inequality used the fact that (L1t≤0 + μ1t>0)t ≤ Lt and (L1t≤0 + μ1t>0)t ≤ μt for
any t ∈ R.
Define E := PmM=1 PijB=1 σjm. We handle each of the two expectations in (138) separately. We first
bound E [xi |E > 0].
i-1	M (j+1)B
E[xi∣E > 0] = E X0 - X MB X X (νσkm + (L1x°≤0 + 〃1叼>0向)E > 0
j=0	m=1 k=jB+1
52
Published as a conference paper at ICLR 2022
i-1	M (j+1)B
E χ0 - X MnB X X (νσm + (Llχj≤0 + μ1χj>0)(xj - χ0))
j=0	m=1 k=jB + 1
-i-1	M (j+1)B	-
+ E - X MB X X (LIxj≤0 + μlχj>0)x0 E > 0
j=0	m=1 k=jB + 1
E > 0
i-1
x°E 1 — η ∑(L1xj ≤0 + μ1xj >0) IE > 0
_	j=0
i-1
-ηf E[(L1xj≤0+ μ1xj>0)(xj- x0) IE > 0]
j=0
-ME 昨 > 0]
i-1
≤ x0E 1 - η ^^(LIxj≤0 + μ1xj >0) E > 0----Ty-E [E∣E > 0]
j^	j—	J	MB
_	j=0	_
i-1
+ ηL X^ E[∣xj - x0∣ ∣ E > 0].
j=0
(139)
Next, We use the following lemma to bound the conditional expectations that arise in (139). This
lemma is proven in Appendix F.4 and it may be of independent interest to readers.
Lemma 14. For m ∈ [M ] ,let σm be a random permutation of N +1'S and N -1’s. Then,for any
i ≤ N and k ≤ B, we have
:(√M + √k) ≤ E ](M X X σj+ X σM .
∖	)	|_| ∖	m=1 j = 1	)	j=i+1	_
Furthermore, for any 0 ≤ i ≤ N and 0 ≤ k ≤ N satisfying i + k ≤ N, we have
口/	M i	∖ i+k	]	/—
E 1( M XX σm) + X σM	≤ √⅛ + √k.
|_| ∖	m=1 j=1	)	j=i+1
Lastly, for any 0 ≤ i ≤ N and 0 ≤ k ≤ B satisfying i + k ≥ 1, we have
M i	i+k	M i	i+k
XX σm + M X σM > 0) = P(XX σm + M X σjM < 0
m=1 j=1	j=i+1	m=1 j = 1	j=i+1
≥ 1.
一6
Lemma 14 implies that MrE[E∣E > 0] ∈ 卜 JiB, JMŋ and P(E > 0) = P(E < 0) ≥ 1/6. From
this, we get
UE [E∣E > 0] ≥
MB [ ∣	] ≥ 64 V MB,
E[∣叼-x0∣ ∣ E > 0] ≤ EPa -对]≤ 6E[∣叼-x0∣].
P(E > 0)
Also, since η ≤ LBN we have
i-1	LN
1 - iημ ≥ 1 - η	(LIxj≤0 + μ1xj>0) ≥ 1 —B- ≥ 0,
j=0
which implies that
i-1
x°E 1 一 η ∑(L1xj ≤0+ μ1xj>0)IE > 0 ≤ (1 -iημ)x0.
j=0
(140)
(141)
(142)
53
Published as a conference paper at ICLR 2022
Substituting (140), (141), and (142) to (139), we obtain
E [xi|E > 0] ≤ (1 - iημ)xo -詈
64
―i—	i-1
MB + 6ηL X E[|xj - xo|].
j=0
(143)
Next, we have the following lemma that we can apply to E[|xj - x0|]. Proof of Lemma 15 can be
found in Appendix F.5.
Lemma 15. For xo ≥ 0, 0 ≤ i ≤ N 一 1 and η ≤ 5、BLN,
513 i 513
E[|xi - xo|] ≤ 512ηνVMB + 512
iηLx0.
Applying this lemma to (143), we get
E[x∕E > 0] ≤ (1 - iημ)xo - ην
64
MB +
1539η2Lν iX
256√MB j=0
1539η2L2x0
256
i-1
Xj
j=0
ην I―i —	513i3/2n2Lv	1539i2η2L2
≤ (1 - iημ)x0 - 64V MB + 128√MB +	512	x0
1539i2η2L2	1
1-iημ +	512	)X0 - lδ4
513i
F
—
≤ 1 - iημ +
ην i
x0 - 128V MB.
(144)
where We got the last inequality by using the fact that iηL ≤ ηLBN- ≤ +,which follows from
η ≤ 5i3LN. So far, we have obtained an upper bound for E [xi |E > 0].
Recall that there is another conditional expectation in (138) that we want to bound, namely
E [xi |E ≤ 0]. We bound it below, using the tools developed so far. For i ≤ N,
E [xi |E ≤ 0] = x0 + E [xi - x0 | E ≤ 0]
≤ x0 + E [|xi - x0 | | E ≤ 0]
E [|xi -x0|]
≤ x0 + P(E ≤ 0)
≤ x0 + 6E [|xi - x0|]
(Using Lemma 14)
(Using Lemma 15)
(145)
1539ην / i	1539iηLxo
≤ x0 + ^15^ M MB +	256
1539iηL∖	1539ην / i
≤ (1 + F5^)x+ + ^5^VMB
Using (144) and (145) in (138), we get that for i ≤ ∙N∙:
E[(L1g≤0 + μ1χi>o)xi]
≤ P (E > 0) LE [xi |E > 0] + P (E ≤ 0) μE 岛 |E ≤ 0]
≤P(E >0)L
iημ +
ην
xo - 1⅞
+ P(E≤ 0) μ	1 +
1539iηL
^^5^ Jxo+
1539ην
256
(146)
From Lemma 14, note that 6 ≤ P (E > 0) ≤ 5 and 6 ≤ P (E ≤ 0) ≤ ∣. We use these inequalities,
along with iηL ≤ ηLBN- ≤ 去 and L ≥ 7695, to bound the terms appearing in (146).
P (E > 0) L I 1 — iημ +
xo + P (E≤ 0) μ (1 + 1535ηL>0
54
Published as a conference paper at ICLR 2022
≤ 6 L (1+
ɪ)xo + 5 ∙L f1 + A)xo ≤ 6LX0.
87552	0	6 7695	256	0 - 7	0
(147)
We also have
-P(E > 0) ηLV J + P (E≤ 0) 1539ημν J
(	) 128 V MB + ( 一 )	256 V MB
≤
—
ηLν / i	2565ημν / i
768 V MB + 512 M MB
≤
ηLν i
1536 V MB,
(148)
where We used the assumption L ≥ 7695. Substituting (147) and (148) to (146), We get
6 ηLν i
E[(L1χi≤0 + μ1χi>o)χi] ≤ 7Lχ0 - 1536γ MB,
as desired.
F.2 Proof of Lemma 12
E[(L1Xi≤0 + μ1Xi>0)xi] ≤ μE[xi]
=μx0 + μE[xi - x0]
≤ μx0 + μE[∣Xi - x0|]
≤
513	7
μx0 + 512 iηLμx0 +512 ημνVMB.
(Using Lemma 15)
F.3 Proof of Lemma 13
We consider iterates Within a single epoch, and hence We Will omit the subscripts denoting epochs.
In our construction, each machine has the same set of component functions, that is, there Will be no
inter-machine deviation. We therefore omit the superscript m from the local component functions.
Consider the function
1 M	N
G2(x):= N £g+i(x) + E g-1 (x) , where
i=1	i= N+1
Lx2	Lx2
g+ι(x) ：= ~2~ + νx, andg-ι(x) := -...νx.
Hence, G2(χ) = L-. We will prove the lemma by coupling iterates corresponding to F? and G2.
In particular, we will perform minibatch RR on F2 and G2 such that both start the given epoch at x0
and all the corresponding machines use the same random permutations. Let xi,F be the iterate after
the i-th round of communication for F2 and xi,G be the iterate after the i-th round of communication
for G2. We use mathematical induction to prove that xi,F ≥ xi,G for alli = 0,..., B. After that,
we will use this to prove our desired statement E[χN,f | χ0 < 0] ≥ (1 - 7r8BN)x0. Let σm be a
random permutation of N +1's and N -1,s.
Base case. x0,F ≥ x0,G since both start the epoch at the same point x0 .
Inductive case. There can be three cases:
• Case 1: xi,F ≥ xi,G ≥ 0. Then,
xi+1,F - xi+1,G = xi,F - xi,G -
M (i+1)B
MB X X (Vfσm (xi,F) - Vgσm (xi,G))
m=1 j=iB+1
55
Published as a conference paper at ICLR 2022
M (i+1)B
=xi,F - xi,G - MB E E (μxi,F + νσm - Lxi,G - νσm)
m=1 j=iB+1
=χi,F - χi,G - η (μχi,F - Lxi,G)
=x”(1 - ημ) - χi,G(i - ηL)
≥ 0.
• Case 2: 0 ≥ xi,F ≥ xi,G. Then,
M (i+1)B
xi+1,F - xi+1,G = xi,F - xi,G - MB Σ Σ (Vfσm (Xi,F) -Vgσm (xi,G))
m=1 j=iB+1
M (i+1)B
=xi,F - xi,G - MB £	£	(Lxi,F + νσm - Lxi,G - νσm)
m=1 j=iB+1
= xi,F - xi,G - η (Lxi,F - Lxi,G)
= xi,F (1 - ηL) - xi,G(1 - ηL)
≥ 0.
• Case 3: xi,F ≥ 0 ≥ xi,G. Then,
M (i+1)B
xi+1,F - xi+1,G = xi,F - xi,G - MB Σ Σ (Vfσjm (xi,F) - Vgσjm (xi,G))
m=1 j=iB+1
M (i+1)B
=xi,F - xi,G - MB X X (μxi,F + νσm - Lxi,G - νσm)
m=1 j=iB+1
=Xi,F - Xi,G - η (μXi,F - Lxi,G)
=χi,F(1 - ημ) - χi,G(i - ηL).
Note that since η ≤ LN, Xi,F(1 - ημ) ≥ 0 and Xi,G(1 - ηL) ≤ 0, which proves that
xi+1,F - xi+1,G ≥ 0.
Thus, we see that Xi+1,F ≥ Xi+1,G. Further, by linearity of expectation and gradient, it is easy to
check that
E[x B ,G] = E[x B-1,G
-ηVG2 (X B-1,G)]
(1 - ηL)E[χ B-1,G]
，	一、N
=(1 - ηL) B X0.
Using the result that XN ,f ≥ XN G which we proved above, we get E[χN ,f] ≥ (1 - ηL)N xo for
any initial iterate X0 . Specifically for xo < 0, this implies E[xN,f | xo < 0] ≥ (1 - ηL)Nxo.
Further, since ηL ≤ 51BN, We have (1 - ηL) B ≤ 1 - 7rILBN. This is because 1 - TzN - (1 - Z) B
is nonnegative on the interval [0,1 - (7/8)N/B-1 ], and 1 - (7/8)N/BT ≥ 5B3N for all N ≥ 2. To
see why, note that (1 - 513(；—]))n-1 ≥ 8 for all n ≥ 2, and this gives 1 - (7/8)n-1 ≥ 513(；—]),
which then implies 1 — (7/8) n-1 ≥ 513； for all n ≥ 2. Therefore, for xo < 0, we have E[xNF |
xo < 0] ≥ (1 - 7rBN)xo.	n	B'
For the last statement of the lemma, note that by symmetry of the function G2, if we initialize the
Algorithm 2 at 0, then for any starting iterate of an epoch we have P(xo,G ≥ 0) ≥ 1/2. This
combined with the fact that xi,F ≥ xi,G gives us that P(xo,F ≥ 0) ≥ 1/2.
56
Published as a conference paper at ICLR 2022
F.4 Proof of Lemma 14
For m = 1,...,M, let σm be a random permutation of / ÷1,s and /一1’s. Then, We first show
that for any i ≤ N/2 and k ≤ B/2,
A (癖 + √k) ≤ EK ⅛ 与 σj + £jM
m=1 j = 1	j=i+1
To prove the lower bound, we will use Khintchine,s inequality along with Lemma 12 from Rajput
et al. (2020). Let us define random variables am := | M Pj=ι σm∣, Xm := Sign(Pj=1 σm), b^ :=
| Pi=k+1 σM ∣, and VM ：= Sign(p；=k+1. σjM). For xm, if the sum Pi=I σjm = 0 then Xm is +1
with probability 0.5 and -1 with probability 0.5. Ties occurring in yM are also broken similarly.
We can note that xm,'s and yM are i.i.d. Rademacher random variables, which allows us to apply
Khintchine,s inequality, Then, by Khintchine,s inequality,
1 M i
M∙ ∑∑ σ
m=1 j=1
E
i+k
M
+ T σj
j=i+1
^ M
E	amXm + b yM
_ m=1
E
1
≥√2
1/2-
ByaPPIying ∣∣zk2 ≥ % IlNIlI for Z ∈ Rd twice, we get
+ bM
1/2
≥ 1E
-2
M
Eam + bM
m=1
Next, noticing that am,s are i.i.d.,
2
+
i+k
E σM
j=i+1
(Lemma 12 from Rajput et al. (2020))
Note that Lemma 12 from Rajput et al. (2020) has the requirement that N ≥ 256. However, that
requirement is for the entire lemma to hold, whereas we need only the first inequality in the lemma.
For that, the requirement is simply N ≥ 8. Further, note that for N = 2,4, and 6 it can be manually
verified that the required inequalities in Lemma 12 of Rajput et al. (2020) hold. Hence, this lemma
holds for all even N .
The upper bound comes from Jensen,s inequality:
ElH 年σm
i+k
M
+工σj
j=i+1
Mi
EE畤
m=1 j=1
+E
i+k
E σM
j=i+1
1
≤——
≤ M
1
Mt
M
EE
m=1
i+k
E σM
j=i+1
≤ MM e
(Since Pj=1 σm for m = 1, 2... are mean 0 and independent.)
57
Published as a conference paper at ICLR 2022
uE
t
i+k	∖ 2
X OMI
j=i+1
ʌ M (i + XEMMσMM]) + Sk + XE[，MσMM].
j 6=l	j 6=l
E[σjM σlM] ≤ 0 because σjM and σlM are negatively correlated. Hence, we get
「I / IMi ∖	i+k	^∣ Γ~
E IMXXσm) + X σM	≤√⅛+√k,
I	m=1 j=1	j =i+1	I
as desired.
Next, it is left to show that for 0 ≤ i ≤ 苧 and 0 ≤ k ≤ B satisfying i + k ≥ 1, we have
(M i	i+k	∖	(Mi	i+k
XX σm + M X σM> 0)= P ∣XX σm + M X σf < 0)≥ |.
m=1 j =1	j =i+1	m=1 j =1	j=i+1
By symmetry, proving the equality is straightforward, and hence it is sufficient prove that
M i	i+k
XXσj + M X σM = 0)≤ 2.
m=1 j=1	j =i+1
(149)
For this, it in fact suffices to show that
≤ 2 for all 1 ≤ l ≤ N - 1,
一 3	,
(150)
because (149) can be derived from (150). We first explain why (150) implies (149), and then show
(150).
Suppose (150) is true. Then,
•	Case 1: If i = 0, then (149) becomes P(Pk=I σ, = 0) ≤ 3, which is true due to (150).
•	Case 2: If M = 1, then (149) becomes P(Pj=I o； = 0) ≤ 2, which is true due to (150).
•	Case 3: Ifi ≥ 1 and M ≥ 2, then we can consider two events that partition the probability
space:
1.	E1 := {PmM=2Pij=1 σjm + M Pij+=ki+1 σjM = 0}. Conditioned on this event E1,
(Mi	i+k	∖	i i	∖ ɔ
XX σj + M	X	σM	= 0 1 EiI	= P ∣X σi	=0I	≤ 3
m=1 j =1	j =i+1	j =1
due to independence of machines and (150).
2.	E2:= {PmM=2Pij=1 σjm + M Pij+=ki+1 σjM 6= 0}. Conditioned on this event E2, let
c:= PmM=2 Pij=1 σjm + M Pij+=ki+1 σjM. Then,
(Mi	i+1	∖	( i	∖
XX σm + M	X σM	= 0 1 E2)	= P ∣X σi	= -c I	.
m=1 j =1	j =i+1	j =1
However, by symmetry, P (Pj=I σj = -C) = P (Pj=I σj = C) ≤ ɪ.
58
Published as a conference paper at ICLR 2022
From these two events, we conclude that (149) must hold.
It is now left to prove (150). It is clear that P(Pij=1 σj1 = 0) = 0 for all odd i, so we assume that
i is even. Also note that P(Pij=1 σj1 = 0) = P(PjN=i+1 σj1 = 0) = P(PjN=-1i σj1 = 0), since σj1 ’s
sum to zero. Therefore, for the rest of the proof, We can focus on even i's in the range 2 ≤ i ≤ N.
Note that P(Pj=I σm = 0) is just the probability of having ^i +1's and 2 -1's in the first i spots
in a random shuffling of N +1's and N -1's. This is equivalent to choosing 22 indices (for +1) out
of the first i, and then choosing N-i indices out of the remaining N — i. Thus,
P (Eσm = 0
(，2
Note that the term above is a decreasing function ofi for i ≤ N/2. Hence, putting i = 2 to the RHS
We get
P (XX σj" = 0), 勺
Where the inequality holds for N ≥ 4.
N
2
2(N - 1) ≤ 3,
F.5 Proof of Lemma 15
i-1 M (j+1)B
E[∣χi-X0∣] = EIlMnBEE E νσm + (LIxj<0 + μ1xj ≥O)XjIl
l	j=0 m=1 k=jB+1	l
V ii^B	i-1
≤ By M + nE £(LIxj <0 + μ1xj ≥o)χj∣	(By Lemma 14)
Ij=0	I
≤
nν
nν
'-：-	i-1
MB + nL EE[|xj|]
j=0
≤
―i-	i-1
MB + inLχo + nL£E[|xj - x0|].
M B	j=0
Let h(i) := nνy MiB + iηLx0 + nL Pj=O h(j), starting with h(0) = 0. Then using induction, it
can be seen that E[|xi - x0|] ≤ h(i). Further, since h(i) is an increasing function ofi, We get
Since i ≤ B and n ≤
nν JMB + inLx0
h(i) ≤----i—-γ---------.
1 - inL
513BLN , we get that 1-1ηL ≤ 512, so E[lxi -x0|] ≤ 512nν JMB + 513inLx0.
G Proof of lower bound for local RR: homogeneous case
(Theorem 4)
Recall that Theorem 4 gives the bound for local RR in the homogeneous setting, where all machines
have the same local objectives. Similar to Theorem 3, we consider three step-size ranges and do case
analysis for each of them. We construct functions for each corresponding step-size regime such that
the convergence of local RR is “slow” for the functions on their corresponding step-size regime. The
final lower bound is the minimum among the lower bounds obtained for the three regimes. More
concretely, we will construct three one-dimensional functions F1(x), F2(x), and F3(x) satisfying
L-smoothness (1), μ-PΕ condition (2), and Assumption 2 such that15
15Again, the functions constructed in this theorem are μ-strongly convex, which is stronger than μ-PL re-
quired in Definition 1. Also, our functions satisfy Assumption 3 with τ = 0, ρ = 1.
59
Published as a conference paper at ICLR 2022
•	Local RR on F1(x) with η ≤ μNκ and initialization yo = V results in
E[Fι(yκ,B)] = ω (νμ).
•	Local RR on F2(x) with η ≥ μNκ and η ≤ 10⅛ and initialization y0 = 0 results in
ν2	ν2B
[ 2(yK,B)] =	Qmnk2 + μN2K2)'
Note that the step-size range requires K ≥ 1025κ, hence this lower bound occurs only in
the “large-epoch” regime, i.e., K & κ.
• Local RR on F3(x) with η ≥ μNκ and η ≥ ≡⅛N and initialization y0 = 0 results in
ν2
E[F3(yκ N)] = Ω I —
l 3 ιyK, B	μMNK
Then, the three dimensional function F ([x, y, z]>) = F1(x) + F2 (y) + F3(z) will show bad con-
vergence in any step-size regime. Furthermore,
μI W min(V2Fι, V2F2, V2F3)I W V2F W max(V2F1, V2F2, V2F3)I W LI,
that is, if F1, F2 and F3 are μ-strongly convex and L-smooth, then so is F . Moreover, since the
component functions in each coordinate are designed to satisfy Assumption 2 with V, the resulting
three dimensional function F also satisfies Assumption 2 with √3ν.
Since the final lower bound is the minimum among the lower bounds obtained in the step-size ranges,
the lower bound becomes Ω (.MNk + μV⅛2) if K ≥ 1025κ and K ≥ MB (this inequality is
2B	2	2
required to make sure μN⅛ ≤ μMNκ) and Ω "MNκ) otherwise.
In the subsequent subsections, we prove the lower bounds for F1, F2, and F3 separately.
G.1 Lower bound FOR η ≤ -ɪ
μN κ
Consider the case where every function at every machine is the same: for all i ∈ [N] and m ∈ [M],
22
fm(χ) := μx-. Hence, Fι(x) = μx-.
Since all fim ’s are the same, the local updates in all the machines are identical. Hence, for this
subsection we omit the superscript for local machines. Let xk,0 and xk,N denote the local iterates at
the beginning and end of the k-th epoch. Then,
xk,N = (1 - ημ)N xk,0.
Initializing at χ1,0 = V and repeating this for K epochs, we get that after K epochs, the last iterate
yκ, N = χκ,N satisfies
Nκ
ν	1	νν
yκ,B =XKN = (1-ημ)	∙ μ ≥ (1-NK)	∙ μ ≥ 4μ,
since N ≥ 2, and K ≥ 1. Hence, Fι(yκ,N) = Ω( V).
G.2 Lower bound for η ≥ -ɪ and η ≤ ,，一
μN κ	1025LN
For most part of this subsection, we consider iterates within a single epoch, and hence we will
omit the subscripts denoting epochs. Let X0 denote the iterate at the beginning of the epoch (which
is the same across all the machines), and Xim denote the i-th local iterate for machine m. After
every B local iterates, the server aggregates the local iterates XimB, computes their average yi :=
M PM=I Xm, and synchronizes all the machines Xm := yi.
60
Published as a conference paper at ICLR 2022
Let y0 denote the iterate at the beginning of the epoch (which is the same across all the machines
x0m = y0), and xim denote the i-th local iterate at machine m. In our construction, each machine
will have the same set of component functions, that is, there will be no inter-machine deviation. We
therefore omit the superscript m from the local component functions fim . The function we construct
for the lower bound and its component functions are as follows:
1 / N	N	∖
F2(x) := N Ef+1(X) + E f-1 (x) , where
i=1	i=N+1
x2
f+ι(x) := (L1χ≤o+ μ1χ>o)~2 + νx, and
x2
f-ι(X) = (L1x≤o+ μ1χ>o)~2— Vx
Note that the function F2(x) = (L1χ≤o + μ1χ>o) χ2 is μ-strongly convex and L-Smooth with
minimizer at 0, and also satisfies Assumption 2.
Let σm be a random permutation of N +1's and NN -1's. Then, machine m computes gradients on
f-1 and f+1 in the order given by σm. Let σjm denote the j-th ordered element ofσm. Then,
Vfσm (X) = (L1x≤0 + μ1x>0)x + νσj .
Hence, the last iterate of an epoch, y N, is given by
BT /	M B-1	∖
y B - y0 = X I- M XX vfσB+j + l (XmHj) I
i=0	m=1 j=0
B T M B-1
=-M X X X((LIxmB+j≤0 + μ1χmB+j>0)χiB+j+ νσiB+j+ι)
i=0 m=1 j=0
B-1 M B-1
=-M XXX(Llx 器+ j≤0 + μ1xmB+j >0)xmB+j，
i=0 m=1 j=0
where, in the last line, we used the fact that PjN=1 σjm = 0.
Recall that in the construction, each machine has the same component functions. Hence,
Γ N-1 M B-1	-
E[y B- yo] =- M E XXX(LIxmB+j≤0+ μ1xmB+j>O)XmB+j
i=0 m=1 j=0
B-1 B-1
=-η X X E [(Lιx1B+j≤0+ μ1x1B+j>0)X1B+j],	(151)
i=0 j=0
where the last equality holds because the iterates XimB+j are identically distributed across different
m ∈ [M ]. Hence, we need to bound E[(L1xιβ 十 ≤0 + μ1xιβ ^.>o)X^iB+j ]. As we did for Theorem 3,
We want to prove that E[yN] keeps increasing over an epoch, that is E[yN - yo] > 0 when yo is
close enough to the minimizer 0.
For this, we first consider the case where the first iterate y0 of the epoch satisfies y0 ≥ 0. The
y0 < 0 case will be considered later. For the case y0 ≥ 0, we will show that whenever y0 is small, the
expected amount of update made in the (iB+j + 1)-th iteration, E[(L1xιβ十 ≤°+μ1xιβ十 >o)X1B+j],
is negative if i ≤ b 2B C and B4 ≤ j ≤ B, and not too big otherwise.
We use the following lemmas, proven in Appendices H.1 and H.2, respectively.
Lemma 16. For yo	≥ 0,	0 ≤ i ≤ [备C,	B ≤ j	≤ B, η ≤	35LN, and L ≥	15375,
1	1	6 ηLν I ∕iB 尸∖
E[(L1xlB+j≤0 + μ1xlB+j>O)XiB+j] ≤ 7LyO- 1536 Vm + Pj .
61
Published as a conference paper at ICLR 2022
Lemma 17. For yo ≥ 0, 0 ≤ i ≤ N 一 1, 0 ≤ j ≤ B 一 1, and η ≤ ^5LN,
E[(LIxIB+ j≤0 + μ1xlB+j >0)x1B+j ] ≤ μ i1+ 1025/2+ j)" ) y0 + 10SΓ (ΠB + pj
Next, we apply the two lemmas above in (151). For now, we consider the case N/B ≥ 2. The case
B = N will be handled separately at the end of this subsection. For simplicity of notation, define
EiB+j ：= E[(L1xιβ十 ≤o + μ1xιβ+ >0)%1月十八 We will divide the summation in (151) into four
groups; for one of them we can apply Lemma 16, and for the other three we apply Lemma 17.
N-1 B-1
E[yN - yo] = -η ∑S Σ EiB+j
/bNBC	BB-1	b2NBC BB	bNC	B-1	N-1	B-1
-η l∑	Σ EiB+j	+ ∑ Σ>iB+j+E	∑	EiB+j	+ E	∑EiB+j
i=0 j=0	i=0 j=B	i=0 j=B+1	i=b 第c+ι j=0
≥-ηXcX1(μ GF
i=0 j=0
b器C导
- η ΣΣ
i=0 j=B
))
X X	1025(iB + j
一ηT T μ (1 + -E
i=0 j= BB+1 \	\
N-I B^	1025(iB + j
η E £ (〃 ( +	1024
i=b N C + 1 j=0
1025ημν ( [B /
y0 +	(VM + j )
1025ημν ( [B /
y0 + F^ (VM + Vj)
≥ 一 η
i=0 j
N- B-	1025(iB + j
-η S j=0 (μ
y0 +
(152)
where the last inequality is true because the RHS of the inequality in Lemma 17 is nonnegative.
First consider the terms in (152) that involve y°. Since (iB + j)ηL ≤ ηLΝ ≤ 1025, μ ≤ 15L5,
N/B ≥ 2, and B ≥ 4, we have the following loose bound:
b器c 陈.N-1 B-1	/
X XL + X S μ (1+
1025(iB + j )ηL
1024
Ν	B 6 Ν	2L	1
≤ 2 ∖bb∖ +1)(彳+ 1) 7L + B ∙ B ∙ 15375 (k1+ ≡4
N B 6	LN 7LN
≤ B * 22 * 7L + 7680 ≤	8 .
(153)
We next bound the terms in (152) that involve summation of square roots. From N/B ≥ 2, we have
b 2BC ≥ 3B and b 2BC + 1 ≥ 2B, so
1 N 1 B /	,_ 、
S jS (rM+ρ!=("
4
N
2B
+1
B
2
Spj
j=BB
62
Published as a conference paper at ICLR 2022
For the other sum, we have
X1 X1 (ri +p
i=0 j=0
B3/2
≥ 4M1/2
B3/2
≥ 6M1/2
3/2
N3/2	(2√2 - 1)NB1/2
18√3M1/2 +	24
3//,/2 BT	N B-I
MW X √i+N Xpj
i=0
j=0
(154)
B3/2	/N	N B r-1
≤ EJO √tdt + B J0 √tdt
2B3/2	N 3/2	2N 3/2
= 3MV2 (B)	+ 3BB
_ 2N3/2	2NB1/2
=3M1/2 + —3 —.
(155)
Substituting the bounds (153), (154), and (155) into (152), and using μ ≤ 40L00,
7ηLN
E[yN - yo] ≥-3— yo +
B8
1025η2μν
η2Lν	N3/2
^1536	18√3M1/2 +
2N 3/2	2NB1/2
(2√2 - V)NBI/2
24
1024
7ηLN
≥----&一 yo +
8
、3M1/2 + —3 —)
盖(M +NB").
(156)
—
For the other case yO < 0, we have the following lemma, a local RR counterpart of Lemma 13. In
Appendix H.3, we prove the following:
Lemma 18. If η ≤ 丁2,LN and an epoch starts at yo < 0, then
Further, if the first epoch of the algorithm is initialized at 0, then for any starting iterate yO of any
following epoch, we have P(yo ≥ 0) ≥ 1/2.
Using (156) and Lemma 18, we get
E[y N ]
=P(yo ≥ 0)E[yN | yo ≥ 0] + P(yo < 0)E[yN | yo < 0]
BB
≥P(yo ≥0) ((I- 7ηLN)yo + 盖(M2+NB1/2))+P(y。<0) (I- 7ηLN) yo
≥ (I- 7ηLN) yo + 4η≡0 (M^ + NB1/2).
Thus far, we have characterized the expected per-epoch update, starting from the initial iterate yo
and iterating until the last iterate yN of the epoch. Now recall that We run the algorithm for K
epochs. Using yk,i to denote the i-th aggregated iterate of the k-th epoch, we get a lower bound on
the expectation of the last iterate yk,N if we initialize at y1,0 = 0:
η2 Lν
480000
(N3/2
M1/2
+ NB1/2
k
63
Published as a conference paper at ICLR 2022
Note that since L
μ
1 - 7ηLN)K
7ηLN
ην	( N1/2 + B1/2
420000 M1/2 +
ην	( N1/2 + B1/2
420000 M1/2
η2Lν (N!/! + NB
480000 M1/2 +
8μK
(Since η ≥ μN1κ)
≥ 40000 and K ≥ 罕
1」1-马'K
(which is implied by μNκ ≤ η ≤ 1021LN),
8μK J	一
Hence, We get from η ≥ μNK that
E[yK,N] = ω (nM/2 + ηνB1∕2) = ω
≥ 1-e-8μ ≥ 1 - e-35000 ≈ 1.
VB1/2
μM1/2N1/2K + μNK
and by Jensen’s inequality, we finally have
E[F(yκ,B)] ≥ 1 E[μyK,N] =。(〃％定,君]2) = ω
ν2
ν2B
μMNK2 + μN2K2 J
≥
ν
Recall that, from the paragraph beloW Lemmas 16 and 17 to this point, We have assumed N/B ≥ 2.
We handle the case B = N now. In this case, notice that all the qM terms that appear in (152)
disappear, because We alWays have i = 0. Therefore, the proof goes through in the same Why,
modulo the fact that we do not have the terms that originate from the q∕iB terms in (152). Therefore,
we can show
E[F(yK,B)] ≥ 2E[μyK,N] = o(ME[yK,N]2) = ω (μNK2) ∙
G.3 Lower bound FOR η ≥ μΝ1κ AND η ≥ 瀛LN
Similar to earlier parts of the proof, here as well, each machine will have the same component
functions, that is, there will be no inter-machine deviation. The proof uses a similar construction as
Safran & Shamir (2020; 2021):
1 号	N
F3(x) := N Ef+1(x) + E f-1(x) , where
i=1	i= N + 1
f+1(x) :
Lx2 + νx, and f-1(x) := Lx2 - νx.
Hence, F3(x) = Lχ2, and has its minimizer at 0.
We first compute the expected “progress” over a given epoch. For simplicity, let us omit the subscript
for epochs for now. Let y0 denote the iterate at the beginning of the epoch (which is the same across
all the machines x0m = y0) and xim denote the i-th local iterate for machine m. After every B local
iterates, the server aggregates the local iterates ximB, computes their average yi :=吉 Pm=1 Xm,
and synchronizes all the machines ximB := yi.
For the epoch, let σm be the permutation of N +1's and N -1's sampled by machine m. Upon
receiving the aggregated iterate ximB = yi, each machine m performs B local updates. Unrolling
the local update rules, the iterate after the B updates (and before synchronization) can be written as
follows:
xm+1)B = xm+1)B-1 - η^fσm+ι)B (xm+1)B-1)
64
Published as a conference paper at ICLR 2022
= x(mi+1)B-1 - η(Lx(mi+1)B-1 + νσ(mi+1)B)
= (1 - ηL)x(mi+1)B-1 - ηνσ(mi+1)B
=♦♦♦
(i+1)B
= (1 - ηL)BximB - ην X (1 - ηL)(i+1)B-jσjm
j=iB+1
(i+1)B
= (1-ηL)Byi-ην X (1 - ηL)(i+1)B-jσjm,
j=iB+1
After synchronization, we get
M	M (i+1)B
yi+ι ：= MM X χm+i)B = (i-ηL)B@-MV X X Q-…*5.
m=1	m=1 j=iB+1
Unrolling the equation above from y N (the final iterate of the epoch, after synchronization) to yo
(the starting iterate), we get
MN
yB = (I- ηL)ByB-ι- MM X X (I-ηL)N-jσm
m=1 j=N-B+1
=♦♦♦
B	MiB
=(1 - ηL)Ny0 - X(1-"L)N-iBMV X X (1 - ηL)iB-jσjm
i=1	m=1 j=(i-1)B+1
MN
ML)N-jσjm
Then, by squaring both sides and taking expectations, E[yN] = (1 - ηL)2Ny0 - 2ην(I -MLNx0E "X X m=1 i=1 + TEKXLX(1 -gNM = (1-ηL)2N yo + ηMV2 e](x X (1-ηL)N m=1 i=1 where we used the fact that E[σim] = 0. Further, because σm and σm0	1 - ML)N-iσim -iσm !21	(157) σi	,	(157) are independent‘ and identi-
cally distributed for different m and m0, we get that 「/	M N	2^ EI MV XX(1-ηL)N-iσm m=1 i=1 M	「/ N	'2]	Γ N =X E	X(1 -ML)N-iσim	+ X E X(1-ηL m=1	i=1	m6=m0	i=1 =M E (X (1-mL)n-iσl), where the last equality used the fact that E[σjm] = 0 for all m identically distributed. Since we only consider the permutation	N )N-iσim E X(1 - ML)N-iσim0 i=1 ∈ [M ] and i ∈ [N], and that σm are σ1 (i.e., the one for machine 1) from
65
Published as a conference paper at ICLR 2022
now on, we henceforth omit the superscript. Substituting this to (157) gives
2
:=Φ
(158)
}
From (158), We have calculated the per-ePoch expected update, because the final iterate y N is also
the initial iterate of the next epoch. Recall that we run the algorithm for K epochs. We now use yk,i
to denote the iterate after the i-th communication round in the k-th epoch. Using (158), We get a
lower bound on the expectation of the last iterate y&N squared:
2 2 K-1	2 2
E[yK, N ] = (1 - ηL)2NKy2,o + %φ X (1 - ηL)2Nk ≥ InMrΦ∙	(159)
k=0
where the inequality used that we initialize at y1,0 = 0 and PkK=-01(1 - ηL)2Nk ≥ (1 - ηL)0 = 1.
Next, we bound the expectation term, i.e., Φ, defined in (158). Using Lemma 1 from Safran &
Shamir (2020) with n and α replaced with N and ηL respectively, we have
Φ ≥ C ∙ min { ɪ, η2L2N3} ,	(160)
for some universal constant c > 0. Using the fact that η ≥ 102⅛N , it is easy to check that the RHS
of (160) is lower-bounded by 枭,where c0 > 0 is a universal constant. Combining (159) and (160)
gives
E[y2 N ] ≥ η2ν2 ∙ J =芷.
[yK,N ] ≥ M ηL LM
Also using the fact that η ≥ μN1κ, we get
L	c0ν 2
E[F3(yκ,B)] = 2E[yK,N] ≥ 2μMNK.
H Proofs of helper lemmas for Appendix G
H.1 Proof of Lemma 16
The proof of Lemma 16 is similar to its minibatch RR counterpart, Lemma 11. From the given
0 ≤ i ≤ B - 1, 0 ≤ j ≤ B - 1, define k := iB + j, in order to simplify notation. We also define
E := M (PM=I PB σm)+ Pk=iB+ι σi.
By the law of total expectation we have
E[(L1χk≤0 + ”1χk>o)xk] = P (E > 0) E h(L1χk≤0 + μ1χk>o)xk IE > 0]
+ P (E≤ 0) E h(L1χk≤0 + μ1χk>o)xk ∣E ≤ 0]
≤ P (E > 0) LE Ixk ∣E > 0] + P (E ≤ 0) μE Ixk ∣E ≤ 0] ,	(161)
where the last inequality used the fact that (L1t≤0 + μlt>0)t ≤ Lt and (Llt≤0 + μlt>0)t ≤ μt for
any t ∈ R.
We handle each of the two expectations in (161) separately. We first bound E x1k ∣E > 0 . Recall
from the definition of algorithm iterates that
M iB-1	k-1
xk-y0=	x1B+j	-	y0	=-9 X X	vfσlm+1 (Xm)- η X	fσ1	I(X1).	(162)
M	l+1	l+1
m=1 l=0	l=iB
Expanding (162) using the definition of Vfσm 's, we obtain
EIx1k∣∣E >0]
66
Published as a conference paper at ICLR 2022
-	M iB-1
E yo-MXX(νσm+ι + (LIxm≤0 + μlχm>0)xmn)
_	m=1 l=0
k-1	'
-η X (νσ1+ι + (LIx1≤o 十 μiχ1>0)χ1) E >0
l=iB	.
-	M iB-1
E y。-卷EE(Llxm≤0 + μlxm>0)(Xmn - y0)
_	m=1 l=0
k-1	一
-η X (LIx1≤0+ μ1x1>0)(XI-y0) E > 0 - ηVE [E∣E > 0]
l=iB
- M iB-1	k-1	-
-E 芸EE(LIxm≤0+ μixm>0)y0+ η Σ (LIx1≤0+μix1>0)y0 I E >0
_	m=1 l=0	l=iB	.
M iB-1	k-1
y0E 1 - MΣΣ(LIxm≤0 + μ1xm>0) - η Σ (LIx1≤0+μix1>0) I E >0
_	m=1 l=0	l=iB	.
M iB-1
-E MXX(Llxm≤0 + μlxm>0)(χn — y0)
_	m=1 l=0
k-1
+η X (LIx1≤0+ μ1xι>0)(x1 - y0) E > 0 - ηνe [EIE > 0]
l=iB
M iB-1	k-1
≤ y0E 1 - MΣΣ(L1xm≤0 + μ1xm>0) 一 η Σ (LIx1≤0 + μ1x1>0) 1E > 0
_	m=1 l=0	l=iB	.
+ η"- I) iX1 E [屠-y0∣ IE > 0] + M iX1 E [∣χ1 - y0∣ IE > 0]
l=0	l=0
k-1
+ ηL X E [IM - y0∣ | E > 0] - ηνE [E∣E > 0],
l=iB
where the last inequality used the fact that (L1≤ + μL>0)t ≤ Lt and (L1t≤0 + μ1t>0)t ≤ μt
for any t ∈ R; and that for different m = 2,...,M, the local iterates Xn are identically distributed
conditioned on E > 0. Next, We use the fact that for any nonnegative random variable v and event
∆, we have that E[v∣∆] ≤ E[v]∕P(∆). Hence,
E [xk ∣ E > 0]
M iB-1	k-1
≤ y0E 1 - MΣΣ(LIxm≤0+ μ1xm>0) - η Σ (LIx1≤0+ μ1x1>0) | E >0
_	m=1 l = 0	l=iB	.
+ ηL(M - 1) 2XιE [∣χ2 - y0∣] + 匹 2XιE [∣x1 - y0∣] + L XX E [∣x1 - y0∣]
+ M 幺	P(E > 0) + My	P(E > 0) + η PE P(E > 0)
-ηνE [E∣E > 0]
M iB-1	k-1
y0E 1 - MΣΣ(L1xm≤0 + μ1xm>0) 一 η Σ (LIx1≤0+ μ1x1>0) | E >0
_	m=1 l = 0	l=iB	_
k-1
—ηνE [E∣E > 0] + ηL ^X
l=0
e [∣χ1 - y0∣]
P(E > 0)
(163)
where the last equality used the fact that for different m ∈ [M], the local iterates Xn are identically
distributed when they are not conditioned.
67
Published as a conference paper at ICLR 2022
Next, We use Lemma 14 again to bound the conditional expectations that arise in (163). We restate
the lemma for the reader,s convenience.
Lemma 14. For m ∈ [M ], let σm be a random permutation of NN +1'S and NN —1’s. Then,for any
i ≤ NN and k ≤ B, we have
:(√M + √k) ≤ E ](M XX X σj+ X σM .
∖	)	|_| ∖ m=1 j = 1	)	j=i+1	_
Furthermore, for any 0 ≤ i ≤ N and 0 ≤ k ≤ N satisfying i + k ≤ N, we have
「1/ M i ∖	i+k	] r—
E III M ∑∑σm) + E σM ≤ ' + √k.
|_| ∖ m=lj=1	)	j=i+1
Lastly, for any 0 ≤ i ≤ N and 0 ≤ k ≤ B satisfying i + k ≥ 1, we have
(M i	i+k	M	(Mi	i+k
EE σm + M E σM> 0)= P I EE σjm + M E σM < 0)≥ |.
m=1 j=1	j=i+1	)	∖m=1 j = 1	j=i+1
Lemma 14 implies that E [E|E > 0] ∈ [= (^M + √j) , ^MB + √j∣ and P(E > 0) = P(E <
0) ≥ 1/6. From this, we get
ηνE[EIE > 0] ≥ 64 ('g + √j) ,	(164)
即—yj ≤6E [I，；—y0∣].	(165)
Also, since η ≤ LN we have
M iB-1	k-1
1—kημ ≥1—M∑∑(LIxm≤0 + μ1χm>O) ― η E (LIx1≤0+μ1xι>0) ≥1 — ηLN ≥0,
m=1 l=0	l=iB
which implies that
y0E
M iB-1	k-1	-
YEE(LIxm≤0 + μ1xm>O) ― η E(LIx1≤0+μ1xι>0) IE >0 ≤(I- kημ)y0.
m=1 l=0	l=iB	_
(166)
Substituting (164), (165), and (166) to (163), we obatin
ElxkI e > 0] ≤ (1 - kημ)y0 — ην
k-1
+ 6ηL E E [|x1 - y01].
l=0
(167)
Next, we have the following lemma that we can apply to E[∣x1 — y0∣]. Proof of Lemma 19 can be
found in Appendix H.4.
Lemma 19. For y0 ≥ 0, 0 ≤ i ≤ B — 1, 0 ≤ j ≤ B — 1, and η ≤ 也.LN,
E[∣x1B+j — y0∣]
+』+ j)ηLy
+ 1024'	+ j" y
0.
Applying this lemma to (167) and arranging the bounds (recall that k := iB + j), we get
k1
1025 E E [∣x1 — y0∣]
l=0
68
Published as a conference paper at ICLR 2022
≤ ην X (rBbp + P-WM)+ ηLyo X /
l=0	l=0
= ηνX1 rB BbMMBc + pl-Bbl/Bc!
+ ηνiBχτ(rBbM? + P-WW!+ ηLyo X ι
l=iB	l=0
/ i-1 ∣~j~^	B-	B-1	j-1	k-1
=ηV(BX Vm + j,ʌ/M + i X √l + X√l) + ηLyoXl.	(168)
l=0	l=0	l=0	l=0
The terms in (168) can be bounded using Pc-I √7 ≤ Rc √tdt:
1024 S ι	2i3/2B3/\ ∕2jB"∖ 2iB3/\ 2j3/2	k，Ly
诉W叫E-y01] ≤ην[二- +	+ T + k) + T
(2iB + 3j) CB 4iBj1/	2j3/2	k2ηLyo
≤ ην ( -3— Vm + ^^ + -J+^
=ην (生产rMB +@BFpj! + 甘
≤ 号 E+pj!+?,
(169)
where the second last inequality used B ≤ j (and hence B1/2 ≤ 2j1/2), and the last inequality used
2iB + 3j ≤ 4(iB + j) = 4k and 4iB + 2j ≤ 4(iB + j) = 4k. Substituting (169) to (167), we get
Ex1kE >0
(170)
1 — kημ +
≤	1 — kημ +
≤ (1 — kημ)yo
The last inequality here used kηL ≤ ηLN ≤ ^^, which follows from η ≤ 3]LN. Thus far, We
have obtained an upper bound for E x1k E > 0 .
Recall that there is another conditional expectation in (161) that we want to bound, namely
E Ixk ∣E ≤ 0]. We bound it below, using the tools developed so far. For i ≤ 2N and B ≤ j ≤ B,
E x1k E ≤ 0 = y0 + E x1k - y0 | E ≤ 0
≤ y0 + E |x1k - y0 | | E ≤ 0
≤	+ E [|xk - xo|]
-y0 + P(E ≤ 0)
≤ y0 + 6E |x1k - x0|
3075ην ( ∕iB	厂、3075kηLyo
≤ y0+ 行武(VM + VjJ +	5iL
3075kηL∖	3075ην ( /TB	厂
≤ (1+F^ )y+^12l 吠+pj
(Using Lemma 14)
(Using Lemma 19)
(172)
69
Published as a conference paper at ICLR 2022
Using (171) and (172) in (161), We get that for i ≤ 甚 and B ≤ j ≤ B
E[(L1χk≤0 + μ1χk>o)xk ]
≤ P (E > 0) LE [xk ∣E > 0] + P (E ≤ 0) μE [xk ∣E ≤ 0]
≤P(E >0) L e-kημ+信)y0 - 黑(∏+ρj!!
+P (E≤ 0) μ ((1+") yo+3075F E+p)!∙
(173)
From Lemma 14, note that 11 ≤ P (E > 0) ≤ 5 and 11 ≤ P (E ≤ 0) ≤ ∣. We use these inequalities,
along with kηL ≤ ηLN ≤ ɪ^ and L ≥ 15375, to bound the terms appearing in (173).
P (E > 0) L I 1 — kημ +
5	2L
yo + P(E≤ 0) μ (ι + 3075kηL)yo
≤ 6L 9+
We also have
1049600 y y0 + 6 ^ 15375 ∖1+512
6
yo ≤ 7 Ly 0.
(174)
3
3
m/c 、ηLν	/ IiB	3075ημν (IiB	厂
- P (E > 0)128 Mm	+ pj	+ p (E≤0) k Vm +	pj
≤-累陷+pj +端
≤ - ⅛6( V MB + pj),
(175)
where We used the assumption L ≥ 15375. Substituting (174) and (175) to (173), We get
E[(LIxk ≤o+μ1xk >o)xk ] ≤ 7 LyO- 1L6 (ʌ/MB+ρj),
Which finishes the proof.
H.2 Proof of Lemma 17
E[(LIxIB+j≤0 + μ1x1B+j>0)x1B+j'] ≤ μE[x1B+j]
=μyo + μE[χ1B+j - yo]
≤ μyo + μE[∣χ1B+j - χo∣]
1025(iB + j)ηLμ	1025ημν / IiB	厂
≤ μyo +	1024 yo + FT 吠 + 5
where the last inequality used Lemma 19.
H.3 Proof of Lemma 18
We consider iterates within a single epoch, and hence we omit the subscripts denoting epochs. In
our construction, each machine has the same set of component functions, that is, there will be no
inter-machine deviation. We therefore omit the superscript m from the local component functions.
Consider the function
1 工	N
G2(x) ：= N £g+i(x) + E g-1 (x) , where
i=1	i=N+1
70
Published as a conference paper at ICLR 2022
Lx2	Lx2
g+ι(x) ：= -ɪ + νx, andg-ι(x):=———νx.
Hence, G2(χ) = Lχ2. We prove the lemma by coupling iterates corresponding to F? and G?. In
particular, we perform local RR on F2 and G2 such that both start the given epoch at y0 and all the
corresponding machines use the same random permutations. Let ximB+j,F and ximB+j,G denote the
iterates (for (iB + j)-th iteration at machine m) for F? and G? respectively. We use mathematical
induction to prove that XmB+j F ≥ XmB+j G for all i = 0,..., N -1 and j = 0,...,B and machines
m = 1,...,M. After that, We will use this to prove our desired statement E[y N F | yo < 0]=
E[~M PMM=I xN,F | y0 < 0] ≥ (I- 7η8BN)y0.
Let σm be a random permutation of NN +1's and N -1's. First we consider i = 0 and 0 ≤ j ≤ B.
Base case. For the base case, we know that X0m,F ≥ X0m,G, since X0m,F = X0m,G = y0 for all m.
Inductive case. There can be three cases:
•	Case 1: XimB+j,F ≥ XimB+j,G ≥ 0. Then,
mm
XiB+j+1,F - XiB+j+1,G
=xmB+j,F - xmB+j,G - η( V fσiB+j+ι (xiB+j,F ) - ^gσm^+j + ι (xiB+j,G))
=xmB+j,F - xmB+j,G - η (μxmB+j,F + νσmB+j+1 - LxmB+j,G - νσmB+j + l)
=xmB+j,F - xmB+j,G - η (μxmB+j,F - LxmB+j,G)
= XmB+j,F(I- ημ) - xiB+j,G(1 - ηL) ≥ 0.
•	Case 2: 0 ≥ ximB+j,F ≥ximB+j,G.Then,
ximB+j+1,F - ximB+j+1,G
= ximB+j,F - ximB+j,G - η(VfσimB+j+1 (xiB+j,F) - VgσimB+j +1 (xiB+j,G))
=xmB+j,F - xmmG - η (LxmB+j,F + νσmB+j + 1 - LxmB+j,G - νσmB+j + l)
=xmB+j,F - xmB+j,G - η (LxmB+j,F - LxiB+j,G)
= ximB+j,F (1 - ηL) - ximB+j,G (1 - ηL) ≥ 0.
•	Case 3: xiB+j,F ≥ 0 ≥ xiB+j,G. Then,
ximB+j+1,F - ximB+j+1,G
=ximB+j,F - ximB+j,G - η(VfσimB+j+1 (xiB+j,F) - VgσimB+j +1 (xiB+j,G))
=xmB+j,F - xmB+j,G - η (μxmB+j,F + νσmB+j+ι - LxiB+j,G - νσmB+j + ι)
=xmB+j,F - xmB+j,G - η (μxmB+j,F - LxmB+j,G)
=xmB+j,F(I - ημ) - ximB+j,G(1 - ηL).
Note that since η ≤ LN, we get that xmB+j,F(1 - ημ) ≥ 0 and xmB+j,G(1 - ηL) ≤ 0,
which proves that ximB+j+1,F - ximB+j+1,G ≥ 0.
Thus, we see that ximB+j+1,F ≥ ximB+j+1,G for all the three cases, which proves by mathematical
induction that ximB+j,F ≥ ximB+j,G for all 0 ≤ j ≤ B and i = 0. Note that this implies that, the
aggregated averages yι,F := MM PM=I xbM,f and yι,G := MM PM=I xm,G satisfy yι,F ≥ yιG.
Hence, after synchronization is complete, we get that for i = 1 and j = 0, ximB+j,F ≥ ximB+j,G for
all machines m. This proves the base case for i = 1. Now, we can repeat the Inductive cases for
1 ≤ j ≤ B and i = 1, and thereby prove that y?,F ≥ y?,G. Continuing on this process, we get that
xmB+j,F ≥ xmB+j,G for all 0 ≤ j ≤ B and 0 ≤ i ≤ N - 1, and consequently, y殍尸 ≥ yNG.
Further, by linearity of expectation and gradient, it is easy to check that for any machine m,
E[y N ,g] = (I- ηL)Nyo.
71
Published as a conference paper at ICLR 2022
Using the result that yN,f ≥ yN,g which We proved above, We get E[yN,f] ≥ (1 - ηL)Nyo for
any initial iterate yo. Specifically for yo < 0, this implies E[yN,f | yo < 0] ≥ (1 - ηL)Nyo.
Further, since ηL ≤ 二&>, we have (1 - ηL)N ≤ 1 - 7η^N. This is because 1 - 7¾N - (1 - Z)N
is nonnegative on the interval [0,1 - (7/8) N-1], and 1 - (7/8) N-1 ≥ yo⅛N for all N ≥ 2. To
see why, note that (1 -逐—))n-1 ≥ 8 for all n ≥ 2, and this gives 1 - (7/8)n-1 ≥ 迪"),
which then implies 1 - (7/8)n-1 ≥ ɪo⅛n for all n ≥ 2. Therefore, for yo < 0, we have E[yN ,f |
yo < 0] ≥ (1 - 7ηLN )yo.
For the last statement of the lemma, note that by symmetry of the function G2, if we initialize
Algorithm 1 at 0, then for any starting iterate of an epoch we have P(yo,G ≥ 0) ≥ 1/2. This
combined with the fact that yi,F ≥ yi,G gives us that P(yo,F ≥ 0) ≥ 1/2.
H.4 Proof of Lemma 19
E[|xi1B+j - yo|]
M iB-1
iB+j-1
Mffνσm+ι + (L1xm<o + μ1xm≥o)χm + η ^X νσ1+ι + (LIxI<o + μ1x1≥o
m=1 l=o
l=iB
iB
M +
iB+j-1
ppj + ηE	E (LIxI<o + μ1x1 ≥o)χ1
l=o
TB	∖	iB+jT
M + √j + ηL E EMI
l=o
(By Lemma 14)
E
iB	∖	iB+jT
M + Vj + (iB + j)ηLyo + ηL E E[∣χ1 - yo ∣] ∙
l=o
Now define
Mk) :=ην (r BbkMBc
)k-1
+ kηLyo + ηL X h(l).
l=o
In terms of i and j, note that k corresponds to k = iB + j. Then using induction, it can be seen that
E[|x1k - yo|] ≤ h(k). Further, since h(k) is an increasing function of k, we get
h(k)
ην
BbMBc + √k - B[k/B\) + kηLyo + ηL X h(l)
l=o
ην
BbMBc + √k - Bbk/B\ ) + kηLyo + kηLh(k)
ην
=⇒ h(k)
BbM/Bc + √k - B\k/B\	+ kηLyo
1 - kηL
r
r
q
≤
≤
Since k ≤ N and η ≤
io21LΝ, we get that
E[|xi1B+j
1025
-yo|] ≤ 嗝
MB + Pj) +1024(iB + j)ηLyo,
as desired.
72
Published as a conference paper at ICLR 2022
I Proof of lower bound for local RR: heterogeneous case
(Proposition 5)
Recall that Proposition 5 gives the bound for local RR in the heterogeneous setting, where different
machines have different local objectives. In this section, we construct examples where there is no
intra-maChine variation (i.e., fm = f2m = … = fN for all m ∈ [M]), but there is certain level of
heterogeneity among different machines.
Similar to the other two lower bounds, we consider four step-size ranges and do case analysis for
each of them. This time, we construct a single function F for these step-size regimes such that
the convergence of local RR is “slow” for F . The final lower bound is the minimum among the
lower bounds obtained for the four regimes. More concretely, we will construct a one-dimensional
function F(x) satisfying L-Smoothness (1), μ-PE condition(2), and Assumption 3 such that16
•	Local RR on F (x) with η ≤ &.NK and initialization yo = T results in
E[F(yκ,B)] = ω (Tμ).
•	Local RR on F (x) with &.NK ≤ η ≤ 81^ and initialization yo = 0 results in
τ 2B2
E[F (yκ, B)] =。(k).
•	Local RR on F (x) with 81B ≤ η ≤ 1 and initialization yo = 0 results in
E[F (yκ, B)] = ω
•	Local RR on F (x) with η ≥ 1 and initialization yo
彳J
=T results in
μ
TT).
E[F (yκ, B)] = ω
In the subsequent subsections, we prove the lower bounds for F for the four step-size intervals.
I∙1 LOWERBOUNDFOR ɪ ≤ η ≤ 81B AND 81B ≤ η ≤ 1
We first consider the two intervals in the middle, because they are more interesting cases. The global
objective function F and its local objective functions are as follows.
1 (j2	m	∖
F(X) := M(EfI(X) + E f2(x) , where
i=1	i=M+1
fι(x) := -τx, and f2(x) := μx2 + τx
In this construction, M/2 machines will have the function f1 as their N local component functions
(and hence their local objective functions) and the other M/2 machines will have the function f2 .
Then, B local RR updates in each machine corresponds to B updates using either f1 or f2. If we
start from XimB = yi , the B local updates on machine m result in
Xm	= XimB+	ητB
(i+I)B	[(1- 2ημ)Bχm -ητ PB-I(I- 2ημ)j
if machine m has f1,
if machine m has f2 .
Taking the average of the M machines, we get that
1
yi+1 = M
MM
~2(y + ητB)+ y
B-1
(I- 2ημ产yi - ητ X(I- 2ημ)j
j=o
16Again, the functions constructed in this theorem are μ-strongly convex, which is stronger than μ-PL re-
quired in Definition 1. Also, our functions satisfy Assumption 2 with ν = 0.
73
Published as a conference paper at ICLR 2022
2 (1 + (I- 2ημ)B ) yi + % (B - X (I- 2ημ) j j .
j=0
Since there are total NK such communication rounds over K epochs, at the end of the run we have
NK
B~
y0
yκ,B = f| (1 + (1 — '2ημ)B)
B-1
NK	-I
F	1
-E(I- 2ημ)j	E
j=0
l=0
1l
2 (1 + (1 — 2ημ)B))
EB -
NK
1 — (1 - 2ημ)B	1 - (1(1 + (1 - 2ημ)B)) B
2ημ
1 — 1 (1 + (1— 2ημ)B)
(176)
where we used initialization y0 = 0. Having defined the function and calculated its last iterate (176),
let us now handle the two step-size regimes separately.
We first consider 8μNK ≤ η ≤ 81B . In this case, we exploit the fact that
1 — 2ημB + η2μ2B2 ≤ (1 — 2ημ)B ≤ 1 — 2ημB + 4η2μ2B2,
(177)
when 0 ≤ η ≤ 8lB. To see why, consider substituting Z := 2ημ. Then hι(z) := 1 — Bz + B2z2 —
(1 — z)B has h010(z) ≥ 0 on z ∈ [0, 1], h01(0) = 0, and h1(0) = 0, implying that h1(z) ≥ 0 on
z ∈ [0,1]. On the other hand, let h2(z) := 1 — Bz + B24z2 — (1 — Z)B. If B = 2, then h2 ≡ 0. If
B > 2, then it can be checked that h2(z) ≤ 0 for small enough interval [0, 4B].
Using (177) on (176),
yκ, B
NK
1 — (1 - 2ημ)B	1 - (1 (1 + (1 - 2ημ)B)) B
2ημ
1 — 1 (1 + (1— 2ημ)B)
NK
1 — (1 — 2ημB + η2μ2B2) 1 — (1(1 + 1 — 2ημB + 4η2μ2B2)) B
2ημ
ητ
^2^
1 —(1 — ημB + 2η2μ2B2) B
ημB — 2 η2μ2B2
T	NK
1 — 1 (1 + 1 — 2ημB + η2μ2B2)
NK
η2μτB2 1 —(1 - ημB + 4ημB) b
≥
ημB
4
+蜀B
ητ B -
≥
ητ B -
where We used ημB ≤ 1. Now, substituting η ≥ 8μNκ to above, we obtain
yK, B
τB	3B
≥ ------- ---------------
—32μNK	32NK
(1 - e-3/32)TB
≥	32μNK.
Therefore, F (yκ, B) = ω ( μN⅛).
Next, consider 81B ≤ η ≤ 1. We take a close look at the term that appears in (176):
“ B - XI- 2ημ)j=B - 1-.
j=0	2ημ
74
Published as a conference paper at ICLR 2022
For this term, We would like to find a lower bound which holds for all η ∈ [8μ^, ɪ]. To this end,
consider substituting Z := 2ημ. Then, the function
B-1
h3(z):=B- X(1-z)j	(178)
j=0
is increasing on Z ∈ [!,1], and we have
h3 (4B) = B ― 4B (1 - (1 - 4Β V) ≤ h3(Z), for all Z ∈ [4Β, 1].
Using B ≥ 2, h3 (4Β) can be lower-bounded as
h3 (4Β) ≥ B - 4B (1 - (1 - 8 )2) = B.
Next, for Z ≥ 1, the derivative of h3(z) := B - Pj=-I(I - z)j = B - 1-(1-z) is h3(z)=
1-(1-z)——Z((BT)z+1). Since B ≥ 2 is assumed to be even, it is easy to check that h3(Z)≥ 0 for
Z ≥ 1, which means that h3 keeps increasing on [1, 2]. Therefore, we conclude that
86 ≤ h3 (4Β) ≤ h3(Z), for all Z ∈ [4Β, 2],
and hence
B-8	B
B - E(I- 2ημ)j ≥ 16,
j=0
for all 81B ≤ η ≤ ɪ. Using y&N from (176), we get
B-8
NK 1
F 1
yκ, N = η2τ IB - X(1 - 2ημ)j) X
j=0
/	B-1
≥ ητ (B - X (1 - 2ημ)j
l=0
1l
2 (1 + (1 - 2ημ)B))
τ
256μ
≥啖≥
Here, we used the fact that P= T (8 (1 + (1 - 2ημ)B))l
Hence, we obtain F(yκ,N) = Ω(3), finishing the proof.
≥ (2 (1 + (1 - 2ημ)B))0 = 1.
I.2 LOWER BOUND FOR η ≤ j AND η ≥ 8
8μN K	μ
We now conclude with the “extreme” step-size regimes. We consider the same function F as in the
previous subsection, but with a different initialization yo = μ.
For F, recall from (176) that
NK
yκ,N = (2 (1 +(I- 2ημ)B))	yo
B-8
+ ητ B - X (1 - 2ημ)j
j=o
NK	-I
F	1
X
l=o
(2 (1 + (1 - 2ημ)B)).
(179)
This time, we want to lower-bound the second term on the RHS of (179) with zero and focus on the
first term. To this end, we revisit our discussion on h3 (178). It is easy to check that h3 is in fact
increasing on the entire [0, ∞), and h3(0) = 0. This shows B - PB=-I(I - 2ημ)j ≥ 0 for any
η ≥ 0. Next, since B is even, 8(1 + (1 - 2ημ)B) ≥ 0 for any η ≥ 0. This gives
NK	NK
yκ,N ≥ (2 (1 +(I- 2ημ)B))	yo = (2 (1 +(I- 2ημ)B)) μ. (180)
75
Published as a conference paper at ICLR 2022
First, consider the interval 0 ≤ η ≤ &.NK. Recall from (177) that for this η,
(1 一 2ημ)B ≥ 1 一 2ημB + η2μ2B2 ≥ 1 一 2ημB,
so
NK	NK
yκ, B ≥ (2(1+(1一 2ημ)B)) BT ≥(I-ημB)NK μ ≥ (ι- 8nK yT ≥
since NBK ≥ 1. Hence, F(yKN) = Ω(τμ2).
Finally, if η ≥ μ, then we have (1 一 2ημ)B ≥ 1, so
/1,	Q NK T τ
yκ,B ≥ Q(1 + (1一 2ημ))) μ ≥ μ.
As a result, F(yKN) = Ω(T).
(181)
7τ
8μ,
76