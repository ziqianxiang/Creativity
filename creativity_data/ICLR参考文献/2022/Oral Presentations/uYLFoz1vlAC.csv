论文题目,会议名称
 Unitary evolution recurrent neural networks, InThe International Conference on Machine Learning (ICML)
 Adaptive input representations for neural language modeling,arXiv preprint arXiv:1809
 An empirical evaluation of generic convolutional andrecurrent networks for sequence modeling, arXiv preprint arXiv:1803
 Trellis networks for sequence modeling, In TheInternational Conference on Learning Representations (ICLR)
 Dilated recurrent neural networks, InAdvances in Neural Information Processing Systems (NeurIPS)
 Generating long sequences with sparsetransformers, arXiv preprint arXiv:1904
 Parallelizing legendre memory unit training, The Interna-tional Conference on Machine Learning (ICML)
 Rethinking attentionwith performers, In The International Conference on Learning Representations (ICLR)
 Language modeling with gatedconvolutional networks, In International conference on machine learning
 Gru-ode-bayes: Continuousmodeling of sporadically-observed time series, In Advances in Neural Information ProcessingSystems (NeurIPS)
 Adversarial audio synthesis, In ICLR
 Animage is worth 16x16 words: Transformers for image recognition at scale, arXiv preprintarXiv:2010
 Lipschitz recurrent neural networks, In International Conference on Learning Represen-tations
" Matrix computations, volume 3", JHU press
 Hippo: Recurrent memory withoptimal polynomial projections, In Hugo Larochelle
"Combining recurrent, convolutional, and continuous-time models with the structured learnablelinear state space layer", In Advances in Neural Information Processing Systems (NeurIPS)
 Long short-term memory, Neural computation
 Transformers are rnns:Fast autoregressive transformers with linear attention, In International Conference on MachineLearning
 Cheap orthogonal constraints in neural networks:A simple parametrization of the orthogonal and unitary group, In The International Conference onMachine Learning (ICML)
 Independently recurrent neural network(IndRNN): Building a longer and deeper RNN, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 Time-aware large kernel convolutions, In International Confer-ence on Machine Learning
 Scalable languagemodeling: Wikitext-103 on a single gpu in 12 hours, SysML
 Wavenet: A generative model for rawaudio, arXiv preprint arXiv:1609
 Structured matrices and polynomials: unified superfast algorithms, Springer Science &Business Media
 Fast approximate computations with cauchy matrices and polynomials, Mathematics ofComputation
 Transformations of matrix structures work again, Linear Algebra and Its Applications
 On the difficulty of training recurrent neuralnetworks, In International conference on machine learning
 Fast parametric learning with activationmemorization, The International Conference on Machine Learning (ICML)
 Fast generation forconvolutional autoregressive models, arXiv preprint arXiv:1704
 Ckconv:Continuous kernel convolution for sequential data, arXiv preprint arXiv:2102
 Latent ordinary differential equationsfor irregularly-sampled time series, In Advances in Neural Information Processing Systems
 Unicornn: A recurrent model for learning very long timedependencies, The International Conference on Machine Learning (ICML)
 Pixelcnn++: Improving thepixelcnn with discretized logistic mixture likelihood and other modifications, arXiv preprintarXiv:1701
 Long range arena : A benchmark for efficienttransformers, In International Conference on Learning Representations
 Learning longer-term depen-dencies in RNNs with auxiliary losses, In The International Conference on Machine Learning(ICML)
 A method of analysing the behaviour of linear systems in terms of time series, Journalof the Institution of Electrical Engineers-Part IIA: Automatic Regulators and Servo Mechanisms
 Legendre memory units: Continuous-time represen-tation in recurrent neural networks, In Advances in Neural Information Processing Systems
 Dynamical systems in spiking neuromorphic hardware, PhD thesis
 Speech commands: A dataset for limited-vocabulary speech recognition, ArXiv
 Inverting modified matrices, Memorandum report
 Pay less attention withlightweight and dynamic convolutions, In The International Conference on Learning Representa-tions (ICLR)
Informer: Beyond efficient transformer for long sequence time-series forecasting, In The Thirty-Fifth AAAI Conference on Artificial Intelligence
" Our work is most closely related to a line of work originally motivated by aparticular biologically-inspired SSM, which led to mathematical models for addressing LRDs",Voelker (2019); Voelker et al
