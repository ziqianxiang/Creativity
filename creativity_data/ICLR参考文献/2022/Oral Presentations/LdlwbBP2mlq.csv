论文题目,会议名称
 SGD with shuffling: optimal rates without componentconvexity and large epoch requirements, In Advances in Neural Information Processing Systems
 Curiously fast convergence of some stochastic gradient descent algorithms, In Pro-ceedings of the symposium on learning and data science
 Random reshuffling is not always better, Advances in Neural InformationProcessing Systems
 Communication trade-offs for local-sgd with largestep size, Advances in Neural Information Processing Systems
 Why random reshuffling beats stochasticgradient descent, Mathematical Programming
 On the convergence of local descent methods in feder-ated learning, arXiv preprint arXiv:1910
 Localsgd with periodic averaging: Tighter analysis and adaptive synchronization, Advances in NeuralInformation Processing Systems
 Random shuffling beats SGD after finite epochs, In InternationalConference on Machine Learning
 Making the last iterate of sgd informationtheoretically optimal, In Conference on Learning Theory
" Brendan McMahan, Brendan Avent, AUrelien Bellet, Mehdi Bennis, Arjun NitinBhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, RafaelG", L
 Scaffold: Stochastic controlled averaging for federated learning, InInternational Conference on Machine Learning
 Tighter theory for local Sgd on identi-cal and heterogeneous data, In International Conference on Artificial Intelligence and Statistics
 A unifiedtheory of decentralized sgd with changing topology and local updates, In International Confer-ence on Machine Learning
 Federated learning: Strategies for improving communication efficiency, arXivpreprint arXiv:1610
 ReCht-Re noncommutative arithmetic-geometric mean conjecture isfalse, In International Conference on Machine Learning
 Speeding up distributed machine learning using codes, IEEE Transactions on Infor-mation Theory
" Federated learning: Challenges,methods, and future directions", IEEE Signal Processing Magazine
 Federated optimization in heterogeneous networks, In I
Communication-efficient learning of deep networks from decentralized data, In Artificial intelli-gence and statistics
 Convergence analysis of distributedstochastic gradient descent with shuffling, arXiv preprint arXiv:1709
 Random reshuffling: Simple analysiswith vast improvements, arXiv preprint arXiv:2006
 Proximal and federated randomreshuffling, arXiv preprint arXiv:2102
 SGD without replacement: Sharper ratesfor general smooth convex functions, In International Conference on Machine Learning
" Nguyen, Quoc Tran-Dinh, Dzung T", Phan
 An approach to inequalities for the distributions of infinite-dimensional martingales,In Probability in Banach Spaces
 Optimum bounds for the distributions of martingales in banach spaces, The Annals ofProbability
Federated learning’s blessing: Fedavg has linear speedup, arXiv preprint arXiv:2007
 Closing the convergence gap of SGDwithout replacement, In International Conference on Machine Learning
 Permutation-based sgd: Is randomoptimal? arXiv preprint arXiv:2102,09718
 Making gradient descent optimal forstrongly convex stochastic optimization, In Proceedings of the 29th International Coference onInternational Conference on Machine Learning
" Toward a noncommutative arithmetic-geometric mean inequal-ity: conjectures, case-studies, and consequences", In Conference on Learning Theory
" How good is SGD with random shuffling? In Conference on LearningTheory, pp", 3250-3284
 Random shuffling beats SGD only after many epochs on ill-conditioned problems, arXiv preprint arXiv:2106
 Probability inequalities for kernel embeddings in sampling without replacement,In Artificial Intelligence and Statistics
 Probability inequalities for the sum in sampling without replacement, The Annalsof Statistics
 Local sgd with a communicationoverhead depending only on the number of workers, arXiv preprint arXiv:2006
 Local sgd converges fast and communicates little, In International Conferenceon Learning Representations
 The error-feedback framework: Better rates forsgd with delayed gradients and compressed updates, Journal of Machine Learning Research
 Permutation compressors for provably fasterdistributed nonconvex optimization, arXiv preprint arXiv:2110
 Smg: A shuffling gradient-based method withmomentum, In International Conference on Machine Learning
" Is local SGD better than minibatch SGD? In InternationalConference on Machine Learning, pp", 10334-10343
 The min-max complexityof distributed stochastic convex optimization with intermittent communication, arXiv preprintarXiv:2102
 Minibatch vs local SGD for heteroge-neous distributed learning, Advances in Neural Information Processing Systems
" Open problem: Can single-shuffle SGD be better thanreshuffling SGD and GD? In Conference on Learning Theory, pp", 4653-4658
1 Our contributions ,
1), Now
