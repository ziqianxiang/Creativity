论文题目,会议名称
 A model of inductive bias learning, Journal of artificial intelligence research
 Mirror descent and nonlinear projected subgradient methods forconvex optimization, Operations Research Letters
 Concentration inequalities: A nonasymptotictheory of independence, Oxford university press
 Transfer learning for nonParametric classification: Minimax rate andadaPtive classifier, The Annals of Statistics
 Double/debiased machine learning for treatment and structuralParameters, The Econometrics Journal
 How fine-tuning allows for effective meta-learning,Advances in Neural Information Processing Systems
 Learning bounds for imPortance weighting,In Nips
 BERT: Pre-training of deePbidirectional transformers for language understanding, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
" Kakade, Jason D", Lee
 Model-agnostic meta-learning for fast adaPtation ofdeeP networks, In International Conference on Machine Learning
 A comParison of loss weighting strategies for multi tasklearning in deeP neural networks, IEEE Access
 Automatedcurriculum learning for neural networks, In international conference on machine learning
 On the value of target data in transfer learning, In Advances inNeural Information Processing Systems
 SamPle sPlitting and threshold estimation, Econometrica
 Foreseeing the benefits of incidentalsuPervision, In Proceedings of the 2021 Conference on Empirical Methods in Natural LanguageProcessing
 Distilling the knowledge in a neural network, arXivpreprint arXiv:1503
 Ontonotes:the 90% solution, In Proceedings of the human language technology conference of the NAACL
 Instance weighting for domain adaptation in nlp, In Proceedings ofthe 45th annual meeting of the association of computational linguistics
 Survey on deep learning with class imbalance, Journalof Big Data
 Minimax lower bounds for transfer learning with linear and one-hiddenlayer neural networks, Neural Information Processing Systems
 Adam: A method for stochastic optimization, ICLR
 Local rademacher complexities and oracle inequalities in risk minimization,The Annals of Statistics
 Meta-learning transferable representationswith a single target domain, arXiv preprint arXiv:2011
 Multi-task deep neural networks fornatural language understanding, In Proceedings of the 57th Annual Meeting of the Association forComputational Linguistics
 The benefit of multitaskrepresentation learning, The Journal of Machine Learning Research
 The natural languagedecathlon: Multitask learning as question answering, arXiv preprint arXiv:1806
" What is being transferred in transfer learning?In Advances in Neural Information Processing Systems, volume 33, pp", 512-523
 Glove: Global vectors for wordrepresentation, In Proceedings of the 2014 conference on empirical methods in natural languageprocessing (EMNLP)
 Improving predictive inference under covariate shift by weighting the log-likelihood function, Journal of statistical planning and inference
 Introduction to the conll-2000 shared task: chunking,In Proceedings of the 2nd workshop on Learning language in logic and the 4th conference onComputational natural language learning-Volume 7
 On the theory of transfer learning: The importance oftask diversity, In Advances in Neural Information Processing Systems
 Provable meta-learning of linear representations, InInternational Conference on Machine Learning
 The Nature of Statistical Learning Theory, Springer science & business media
 Weak convergence and empirical processes: with applications tostatistics, Springer Science & Business Media
 Transformers: State-of-the-artnatural language processing, In Proceedings of the 2020 Conference on Empirical Methods inNatural Language Processing: System Demonstrations
Taskonomy: Disentangling task transfer learning, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 A survey on multi-task learning, IEEE Transactions on Knowledge andData Engineering
" For a set S, we let 1S be its indicator function and weuse #S and |S | interchangeably to denote its cardinality", For two positive sequences {an} and{bn}
