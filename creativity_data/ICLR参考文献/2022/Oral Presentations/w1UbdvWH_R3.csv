论文题目,会议名称
 Optimization algorithms on matrix manifolds,Princeton University Press
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks, In InternationalConference on Machine Learning
 Layer normalization, arXiv preprintarXiv:1607
 Theory iii: Dynamics and generalization in deep networks, arXivpreprint arXiv:1903
 Cross-validation stability of deep networks, Technical report
 Boosting the margin: A newexplanation for the effectiveness of voting methods, The annals of statistics
 Reconciling modern machine-learningpractice and the classical bias-variance trade-off, Proceedings of the National Academy of Sciences
 On the global convergence of gradient descent for over-parameterizedmodels using optimal transport, Advances in neural information processing systems
 Implicit bias of gradient descent for wide two-layer neural networkstrained with the logistic loss, arXiv preprint arXiv:2002
 Exploring the role of loss functions in multiclassclassification, In 2020 54th Annual Conference on Information Sciences and Systems (CISS)
 Imagenet: A large-scalehierarchical image database, In Computer Vision and Pattern Recognition
 A self-stabilized minor subspace rule, IEEE SignalProcessing Letters
 On the emergence of tetrahedral symmetry in the final andpenultimate layers of neural network classifiers, arXiv preprint arXiv:2012
 Revealing the structure of deep neural networks via convex duality,arXiv preprint arXiv:2002
 Layer-peeled model: Toward understandingwell-trained deep neural networks, arXiv preprint arXiv:2101
 Tables of the existence of equiangular tight frames, arXivpreprint arXiv:1504
 Introduction to Statistical Pattern Recognition, Elsevier
 Discriminant analysis by gaussian mixtures, Journal of theRoyal Statistical Society: Series B (Methodological)
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification, In Proceedings of the IEEE internationalconference on computer vision
 Deep residual learning for imagerecognition, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Matrix analysis, Cambridge university press
 Densely connectedconvolutional networks, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Evaluation of neural architectures trained with square loss vs cross-entropy in classification tasks, arXiv preprint arXiv:2006
 Batch normalization: Accelerating deep network training byreducing internal covariate shift, In International conference on machine learning
 Learning multiple layers of features from tiny images,Technical report
 Imagenet classification with deep con-volutional neural networks, Advances in neural information processing systems
 MNIST handwritten digit database, AT&T Labs[Online]
 Gradient descent with early stopping isprovably robust to label noise for overparameterized neural networks, In International conferenceon artificial intelligence and statistics
 Neural collapse with cross-entropy loss, arXiv preprintarXiv:2012
 A mean field view of the landscape of two-layer neural networks, Proceedings of the National Academy of Sciences
 Neural collapse with unconstrained features, arXivpreprint arXiv:2011
 Y, Han
" Pytorch: An imperative style,high-performance deep learning library", In Advances in neural information processing systems
 Explicit regularization and implicit bias in deep network classifierstrained with the square loss, arXiv preprint arXiv:2101
 Implicit dynamic regularization in deep networks, Technical report
" Early stopping-but when? In Neural Networks: Tricks of the trade, pp", 55-69
 Overfitting in adversarially robust deep learning, InInternational Conference on Machine Learning
 Parameters as interacting particles: long time conver-gence and asymptotic error scaling of neural networks, In Proceedings of the 32nd InternationalConference on Neural Information Processing Systems
 Weight normalization: A simple reparameterization to acceleratetraining of deep neural networks, Advances in neural information processing systems
 The implicitbias of gradient descent on separable data, The Journal of Machine Learning Research
 Grassmannian frames with applications to coding andcommunication, Applied and computational harmonic analysis
 Differentiating the singular value decomposition, Technical report
 Instance normalization: The missingingredient for fast stylization, arXiv preprint arXiv:1607
 The optimised internal representation of multilayer classifiernetworks performs nonlinear discriminant analysis, Neural Networks
 Group normalization, In Proceedings of the European conference oncomputer vision (ECCV)
 Understandingdeep learning requires rethinking generalization, 2016
1	Datasets ,
