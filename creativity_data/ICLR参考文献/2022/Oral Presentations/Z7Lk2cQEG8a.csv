论文题目,会议名称
 A convergence theory for deep learning via over-parameterization, arXiv preprint arXiv:1811
 Understanding deep neuralnetworks with rectified linear units, arXiv preprint arXiv:1611
 Breaking the curse of dimensionality with convex neural networks, The Journal ofMachine Learning Research
 On the capabilities of multilayer perceptrons, Journal of complexity
 Convexneural networks, In Advances in neural information processing systems
" Conservative set valued fields, automatic differentiation,stochastic gradient method and deep learning", arXiv preprint arXiv:1909
 Convex analysis and nonlinear optimization: theory andexamples, Springer Science & Business Media
 Convex optimization, Cambridgeuniversity press
 On the global convergence of gradient descent for over-parameterizedmodels using optimal transport, In Advances in neural information processing systems
 Generalized gradients and applications, Transactions of the American MathematicalSociety
 Geometrical and statistical properties of systems of linear inequalities withapplications in pattern recognition, IEEE transactions on electronic computers
 Stochastic subgradient methodconverges on tame functions, Foundations of computational mathematics
 On the power of over-parametrization in neural networks with quadraticactivation, arXiv preprint arXiv:1803
 Gradient descent provably optimizesover-parameterized neural networks, arXiv preprint arXiv:1810
 Convex geometry of two-layer relu networks: Implicit autoencodingand interpretable models, In International Conference on Artificial Intelligence and Statistics
 Topology and geometry of half-rectified network optimization,arXiv preprint arXiv:1611
" Global optimality in tensor factorization, deep learning, andbeyond", arXiv preprint arXiv:1506
 Global optimality in neural network training, In Proceedingsof the IEEE Conference on Computer Vision and Pattern Recognition
 Neural tangent kernel: Convergence andgeneralization in neural networks, In Advances in neural information processing systems
 On the computational efficiency of training neuralnetworks, In Advances in neural information processing systems
 On connected sublevel sets in deep learning, In International Conference on MachineLearning
 On the proof of global convergence of gradient descent for deep relu networks withlinear widths, arXiv preprint arXiv:2101
 The loss surface of deep and wide neural networks, In Proceedingsof the 34th International Conference on Machine Learning-Volume 70
 When are solutions connected in deep networks?arXiv preprint arXiv:2102,09671
 Enumeration of linear threshold functions from the lattice of hyperplane intersections,IEEE Transactions on Neural Networks
 The inductive bias of relu networks on orthogonallyseparable data, In International Conference on Learning Representations
 Neural networks are convex regularizers: Exact polynomial-timeconvex optimization formulations for two-layer networks, arXiv preprint arXiv:2002
 Convex regularizationbehind neural reconstruction, arXiv preprint arXiv:2012
 Bounds on over-parameterization for guaranteed existence of descent paths in shallow relu networks, In In-ternational Conference on Learning Representations
 Theoretical insights into the optimizationlandscape of over-parameterized shallow neural networks, IEEE Transactions on InformationTheory
 No bad local minima: Data independent training error guaranteesfor multilayer neural networks, arXiv preprint arXiv:1605
 Spurious valleys in one-hidden-layer neuralnetwork optimization landscapes, Journal of Machine Learning Research
 Mathematics of deep learning, arXivpreprint arXiv:1712
" Learning relu networks on linearly separabledata: Algorithm, optimality, and generalization", IEEE Transactions on Signal Processing
 Harmless overparametrization in two-layer neural networks, arXivpreprint arXiv:2106
 Partitions of n-space by hyperplanes, SIAM Journal on Applied Mathematics
