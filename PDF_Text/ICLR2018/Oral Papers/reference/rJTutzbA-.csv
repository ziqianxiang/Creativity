title,year,conference
 The tradeoffs of large scale learning,2007, In NIPS 20
 Entropy-sgd: Biasing gradientdescent into wide valleys,2017, CoRR
 SAGA: A fast incremental gradientmethod with support for non-strongly convex composite objectives,2014, In NIPS 27
 First-order methods of smooth convexoptimization with inexact oracle,2014, Mathematical Programming
 Un-regularizing: approximate proximalpoint and faster stochastic algorithms for empirical risk minimization,2015, In ICML
 Competing with the empirical riskminimizer in a single pass,2015, In COLT
 Identity mappings in deep residualnetworks,2016, In ECCV (4)
 Deep residual learning for image recog-nition,2016, In CVPR
 Reducing the dimensionality of data with neuralnetworks,2006, science
 Par-allelizing stochastic approximation through mini-batching and tail-averaging,2016, arXiv preprintarXiv:1610
 Accelerat-ing stochastic gradient descent,2017, arXiv preprint arXiv:1704
 Accelerating stochastic gradient descent using predictive variancereduction,2013, In NIPS 26
 Adam: A method for stochastic optimization,2014, CoRR
 Learning multiple layers of features from tiny images,2009, 2009
 Linearly convergent stochastic heavy ball method for minimiz-ing generalization error,2017, 2017
 Deep learning via hessian-free optimization,2010, In International conference on machinelearning
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, In International conference on machine learning
 Gradient methods for minimizing composite functions,2012, Mathematical ProgrammingSeries B
 Path-sgd: Path-normalized opti-mization in deep neural networks,2015, CoRR
 The computation of eigenvalues and eigenvectors of very large sparse matri-ces,1971, PhD Thesis
 Introduction to Optimization,1987, Optimization Software
 Channel identification for high speed digital communications,1974, IEEE Transactionson Automatic Control
 Making gradient descent optimal forstrongly convex stochastic optimization,2012, In ICML
 A generic approach for escaping saddle points,2017, arXiv preprintarXiv:1709
 A stochastic gradient method with an expo-nential convergence rate for strongly-convex optimization with finite training sets,2012, In NIPS 25
 Analysis of the momentum lms algorithm,1990, IEEE Transactions onAcoustics
 Stochastic dual coordinate ascent methods for regularizedloss minimization,2012, CoRR
 Accelerated proximal stochastic dual coordinate ascent forregularized loss minimization,2014, In ICML
 Analysis of momentum adaptivefiltering algorithms,1998, IEEE Transactions on Signal Processing
