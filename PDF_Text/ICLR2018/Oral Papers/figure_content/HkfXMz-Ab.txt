Figure 1:	Programs generated by Bayou with the API method name readLine as a label. Namesof variables of type T whose values are obtained from the environment are of the form $T.
Figure 2:	Bayes net for Then，g(X) = argmaxProg P(Prog|X, θ*).
Figure 3: Grammar for sketchesLet us define a random variable Y = α(Prog). Weassume that the variables X , Y and Prog are relatedas in the Bayes net in Figure 2. Specifically, givenY , Prog is conditionally independent of X . Fur-ther, let us assume a distribution family P(Y |X, θ)parameterized on θ .
Figure 4: Statistics on labels					contain rich information about what programs do. Figure 4 gives some statistics on the sizes ofthe labels in the data. From the extracted data, we randomly selected 10,000 programs to be in thetesting and validation data each.
Figure 5: 2-dimensional projection of latent spaceTo visualize clustering in the 32-dimensionallatent space, we provided labels X from thetesting data and sampled Z from P (Z |X),and then used it to sample a sketch fromP(Y |Z). We then used t-SNE (Maaten & Hin-ton, 2008) to reduce the dimensionality of Zto 2-dimensions, and labeled each point withthe API used in the sketch Y. Figure 5 showsthis 2-dimensional space, where each label hasbeen coded with a different color. It is imme-diately apparent from the plot that the modelhas learned to cluster the latent space neatlyaccording to different APIs. Some APIs suchas java.io have several modes, and we no-ticed separately that each mode corresponds todifferent usage scenarios of the API, such asreading versus writing in this case.
Figure 6: Accuracy of different models on testing data. Ged-Aml and Gsnn-Aml are baselinemodels trained over Aml ASTs, Ged-Sk and Gsnn-Sk are models trained over sketches.
Figure 7: Programs generated in a typical run of Bayou, given the API method name readLineand the type FileReader.
Figure 8: Grammar for AmlAml is a core language that is designed tocapture the essence of API usage in Java-likelanguages. Now we present this language.
Figure 9:	The abstraction function α.
Figure 10:	Tree representation of the sketch in Figure 7(a)units in the encoder for API calls, and let Wh ∈ RICallsl×h, b ∈ Rh, Wd ∈ Rh×d, bd ∈ Rd bereal-valued weight and bias matrices of the neural network. The encoding function f (XCalls,i) canbe defined as follows:f(XCalls,i) = tanh((Wh. X0Calls,i + bh). Wd + bd)-2xwhere tanh is a non-linearity defined as tanh(χ)= ']3.This would map any given API callinto a d-dimensional real-valued vector. The values of entries in the matrices Wh, bh, Wd and bdwill be learned during training. The encoder for types can be defined analogously, with its own setof matrices and hidden state.
Figure 11:	Computing the hidden state and output of the decoderone level deeper in the tree (i.e., the LHS with a term in the RHS of a rule). We consider a sequenceof API calls connected by sequential composition as siblings. The root of the entire tree is a specialnode named root, and so the first pair in all production paths is (root, child). The last edge in aproduction path is irrelevant (∙) as it does not connect the node to any subsequent nodes.
Figure 12:	2-dimensional projection of latent space of the Gsnn-Sk modelsampled. If only one type of edge is feasible (for instance, if the node is a terminal in the grammar,only a sibling edge is possible with the next node), then only that edge is provided. If both edgesare feasible, then both possibilities are recursively explored, growing the tree in both directions.
Figure 13: Qualitative usage scenarios of Bayou.
