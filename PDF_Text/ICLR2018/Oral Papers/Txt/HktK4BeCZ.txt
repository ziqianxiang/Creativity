Published as a conference paper at ICLR 2018
Learning Deep Mean Field Games for Model-
ing Large Population Behavior
Jiachen Yang1, Xiaojing Ye2, Rakshit Trivedi1, Huan Xu1 & Hongyuan Zha1
yjiachen@gmail.com, xye@gsu.edu, rstrivedi@gatech.edu,
huan.xu@isye.gatech.edu, zha@cc.gatech.edu
1 Georgia Institute of Technology
2Georgia State University
Ab stract
We consider the problem of representing collective behavior of large popula-
tions and predicting the evolution of a population distribution over a discrete state
space. A discrete time mean field game (MFG) is motivated as an interpretable
model founded on game theory for understanding the aggregate effect of individ-
ual actions and predicting the temporal evolution of population distributions. We
achieve a synthesis of MFG and Markov decision processes (MDP) by showing
that a special MFG is reducible to an MDP. This enables us to broaden the scope
of mean field game theory and infer MFG models of large real-world systems via
deep inverse reinforcement learning. Our method learns both the reward function
and forward dynamics of an MFG from real data, and we report the first empirical
test of a mean field game model of a real-world social media population.
1	Introduction
Nothing takes place in the world whose meaning is not that of some maximum or
minimum.	(Leonhard Euler)
Major global events shaped by large populations in social media, such as the Arab Spring, the Black
Lives Matter movement, and the fake news controversy during the 2016 U.S. presidential election,
provide significant impetus for devising new models that account for macroscopic population behav-
ior resulting from the aggregate decisions and actions taken by all individuals (Howard et al., 2011;
Anderson & Hitlin, 2016; Silverman, 2016). Just as physical systems behave according to the prin-
ciple of least action, to which Euler’s statement alludes, population behavior consists of individual
actions that may be optimal with respect to some objective. The increasing usage of social media in
modern societies lends plausibility to this hypothesis (Perrin, 2015), since the availability of infor-
mation enables individuals to plan and act based on their observations of the global population state.
For example, a population’s behavior directly affects the ranking of a set of trending topics on social
media, represented by the global population distribution over topics, while each user’s observation
of this global state influences their choice of the next topic in which to participate, thereby con-
tributing to future population behavior (Twitter, 2017). In general, this feedback may be present in
any system where the distribution of a large population over a state space is observable (or partially
observable) by each individual, whose behavior policy generates actions given such observations.
This motivates multiple criteria for a model of population behavior that is learnable from real data:
1.	The model captures the dependency between population distribution and their actions.
2.	It represents observed individual behavior as optimal for some implicit reward.
3.	It enables prediction of future population distribution given measurements at previous times.
We present a mean field game (MFG) approach to address the modeling and prediction criteria.
Mean field games originated as a branch of game theory that provides tractable models of large
agent populations, by considering the limit of N -player games as N tends to infinity (Lasry &
Lions, 2007). In this limit, an agent population is represented via their distribution over a state
space, and each agent’s optimal strategy is informed by a reward that is a function of the population
distribution and their aggregate actions. The stochastic differential equations that characterize MFG
1
Published as a conference paper at ICLR 2018
can be specialized to many settings: optimal production rate of exhaustible resources such as oil
among many producers (GUeant et al., 2011); optimizing between conformity to popular opinion and
consistency with one’s initial position in opinion networks (Bauso et al., 2016); and the transition
between competing technologies with economy of scale (Lachapelle et al., 2010). Representing
agents as a distribution means that MFG is scalable to arbitrary population sizes, enabling it to
simulate real-world phenomenon such as the Mexican wave in stadiums (GUeant et al., 2011).
As the model detailed in Section 3 will show, MFG naturally addresses the modeling criteria in
our problem context while overcoming limitations of alternative predictive methods. For example,
time series analysis builds predictive models from data, but these models are incapable of repre-
senting any motivation (i.e. reward) that may produce a population’s behavior policy. Alternatively,
methods that employ the underlying population network structure have assumed that nodes are only
influenced by a local neighborhood, do not account for a global state, and may face difficulty in
explaining events as the result of any implicit optimization. (Farajtabar et al., 2015; De et al., 2016).
MFG is unique as a descriptive model whose solution tells us how a system naturally behaves ac-
cording to its underlying optimal control policy. This observation enables us to draw a connection
with the framework of Markov decision processes (MDP) and reinforcement learning (RL) (Sut-
ton & Barto, 1998). The crucial difference from a traditional MDP viewpoint is that we frame the
problem as MFG model inference via MDP policy optimization: we use the MFG model to describe
natural system behavior by solving an associated MDP, without imposing any control on the sys-
tem. MFG offers a computationally tractable framework for adapting inverse reinforcement learning
(IRL) methods (Ng & Russell, 2000; Ziebart et al., 2008; Finn et al., 2016), with flexible neural net-
works as function approximators, to learn complex reward functions that may explain behavior of
arbitrarily large populations. In the other direction, RL enables us to devise a data-driven method for
solving an MFG model of a real-world system for temporal prediction. While research on the the-
ory of MFG has progressed rapidly in recent years, with some examples of numerical simulation of
synthetic toy problems, there is a conspicuous absence of scalable methods for empirical validation
(Lachapelle et al., 2010; Achdou et al., 2012; Bauso et al., 2016). Therefore, while we show how
MFG is well-suited for the specific problem of modeling population behavior, we also demonstrate
a general data-driven approach to MFG inference via a synthesis of MFG and MDP.
Our main contributions are the following. We propose a data-driven approach to learn an MFG
model along with its reward function, showing that research in MFG need not be confined to toy
problems with artificial reward functions. Specifically, we derive a discrete time graph-state MFG
from general MFG and provide detailed interpretation in a real-world setting (Section 3). Then we
prove that a special case can be reduced to an MDP and show that finding an optimal policy and
reward function in the MDP is equivalent to inference of the MFG model (Section 4). Using our
approach, we empirically validate an MFG model of a population’s activity distribution on social
media, achieving significantly better predictive performance compared to baselines (Section 5). Our
synthesis of MFG with MDP has potential to open new research directions for both fields.
2	Related work
Mean field games originated in the work of Lasry & Lions (2007), and independently as stochastic
dynamic games in Huang et al. (2006), both of which proposed mean field problems in the form of
differential equations for modeling problems in economics and analyzed the existence and unique-
ness of solutions. GUeant et al. (2011) provided a survey of MFG models and discussed various
applications in continuous time and space, such as a model of population distribution that informed
the choice of application in our work. Even though the MFG framework is agnostic towards the
choice of cost function (i.e. negative reward), prior work make strong assumptions on the cost in
order to attain analytic solutions. We take a view that the dynamics of any game is heavily impacted
by the reward function, and hence we propose methods to learn the MFG reward function from data.
Discretization of MFGs in time and space have been proposed (Gomes et al., 2010; Achdou et al.,
2012; Gueant, 2015), serving as the starting point for our model of population distribution over dis-
crete topics; while these early work analyze solution properties and lack empirical verification, we
focus on algorithms for attaining solutions in real-world settings. Related to our application case,
prior work by Bauso et al. (2016) analyzed the evolution of opinion dynamics in multi-population
environments, but they imposed a Gaussian density assumption on the initial population distribution
2
Published as a conference paper at ICLR 2018
and restrictions on agent actions, both of which limit the generality of the model and are not assumed
in our work. There is a collection of work on numerical finite-difference methods for solving contin-
uous mean field games (Achdou et al., 2012; Lachapelle et al., 2010; Carlini & Silva, 2014). These
methods involve forward-backward or Newton iterations that are sensitive to initialization and have
inherent computational challenges for large real-valued state and action spaces, which limit these
methods to toy problems and cannot be scaled to real-world problems. We overcome these limi-
tations by showing how the MFG framework enables adaptation of RL algorithms that have been
successful for problems involving unknown reward functions in large real-world domains.
In reinforcement learning, there are numerous value- and policy-based algorithms employing deep
neural networks as function approximators for solving MDPs with large state and action spaces
(Mnih et al., 2013; Silver et al., 2014; Lillicrap et al., 2015). Even though there are generalizations
to multi-agent settings (Hu et al., 1998; Littman, 2001; Lowe et al., 2017), the MDP and Markov
game frameworks do not easily suggest how to represent systems involving thousands of interacting
agents whose actions induce an optimal trajectory through time. In our work, mean field game
theory is the key to framing the modeling problem such that RL can be applied.
Methods in unknown MDP estimation and inverse reinforcement learning aim to learn an optimal
policy while estimating an unknown quantity of the MDP, such as the transition law (Burnetas &
Katehakis, 1997), secondary parameters (Budhiraja et al., 2012), and the reward function (Ng &
Russell, 2000). The maximum entropy IRL framework has proved successful at learning reward
functions from expert demonstrations (Ziebart et al., 2008; Boularias et al., 2011; Kalakrishnan
et al., 2013). This probabilistic framework can be augmented with deep neural networks for learning
complex reward functions from demonstration samples (Wulfmeier et al., 2015; Finn et al., 2016).
Our MFG model enables us to extend the sample-based IRL algorithm in Finn et al. (2016) to the
problem of learning a reward function under which a large population’s behavior is optimal, and we
employ a neural network to process MFG states and actions efficiently.
3	Mean field games
We begin with an overview of a continuous-time mean field games over graphs, and derive a general
discrete-time graph-state MFG (GUeant, 2015). Then We give a detailed presentation of a discrete-
time MFG over a complete graph, which will be the focus for the rest of this paper.
3.1	Mean field games on graphs
Let G = (V, E) bea directed graph, Where the vertex set V = {1, . . . , d} represents d possible states
of each agent, and E ⊆ V × V is the edge set consisting of all possible direct transition betWeen states
(i.e., a agent can hop from i to j only if (i, j) ∈ E). For each node i ∈ V, define Vi+ := {j : (j, i) ∈
E}, V- ：= {j ： (i,j) ∈ E}, and V+ := V+ ∪ {i} and V- ：= V- ∪ {i}. Let ∏i(t) be the density
(proportion) of agent popUlation in state i at time t, and π(t) := (π1 (t), . . . , πd(t)). PopUlation
dynamics are generated by right stochastic matrices P (t) ∈ S(G), where S(G) := Sι(G) X •…X
Sd(G) and each row Pi(t) belongs to Si(G):= {p ∈ ∆d-1 | SUPP(P) ⊂ V-} where ∆d-1 is the
simplex in Rd. Moreover, we have a valUe fUnction Vi(t) of state i at time t, and a reward fUnction
ri(π(t), Pi(t)) 1 , qUantifying the instantaneoUs reward for agents in state i taking transitions with
probability Pi(t) when the cUrrent distribUtion is π(t). We are mainly interested in a discrete time
graph state MFG, which is derived from a continUoUs time MFG by the following proposition.
Appendix A provides a derivation from the continUoUs time MFG.
Proposition 1. Under a semi-implicit discretization scheme with unit time step labeled by n, the
backward Hamilton-Jacobi-Bellman (HJB) equation and the forward Fokker-Planck equation for
each i ∈ {1, . . . , d} and n = 0, . . . , N - 1 in a discrete time graph state MFG are given by:
(HJB)	Vn =maxpn∈Si(G) {ri(πn,Pn) + Xj∈V-PjVn+1}	⑴
(FOkker-PIanCk)	πp+1 = ^X 三力 Pnnj	(2)
1We here consider a rather special formUlation where the reward fUnction ri only depends on the overall
popUlation distribUtion π(t) and the choice Pi the players in state i made.
3
Published as a conference paper at ICLR 2018
3.2	Discrete time MFG over complete graph
Proposition 1 shows that a discrete time MFG given in Gomes et al. (2010) can be seen as a special
case of a discrete time graph state MFG with a complete graph (such that S(G) = ∆d-1 ×∙∙∙× ∆d-1
(d of ∆d-1)). We focus on the complete graph in this paper, as the methodology can be readily
applied to general directed graphs. While Section 4 will show a connection between MFG and MDP,
we note here that a “state” in the MFG sense is a node in V and not an MDP state. 2 We now interpret
the model using the example of evolution of user activity distribution over topics on social media,
to provide intuition and set the context for our real-world experiments in Section 5. Independent
of any particular interpretation, the MFG approach is generally applicable to any problem where
population size vastly outnumbers a set of discrete states.
•	Population distribution πn ∈ ∆d-1 for n = 0, . . . , N. Each πn is a discrete probability distri-
bution over d topics, where πin is the fraction of people who posted on topic i at time n. Although
a person may participate in more than one topic within a time interval, normalization can be en-
forced by a small time discretization orby using a notion of “effective population size”, defined as
population size multiplied by the max participation count of any person during any time interval.
π0 is a given initial distribution.
•	Transition matrix Pn ∈ S(G). Pinj is the probability of people in topic i switching to topic j at
time n, so we refer to Pin as the action of people in topic i. Pn generates the forward equation
d
πjn+1 = X Pinj πin	(3)
i=1
•	Reward ri (πn, Pin) := Pjd=1 Pinj rij (πn, Pin), for i ∈ {1, . . . , d}. This is the reward received
by people in topic i who choose action Pin at time n, when the distribution is πn . In contrast
to previous work, we learn the reward function from data (Section 4.1). We make a locality
assumption: reward for i depends only on Pin, not on the entire Pn, which means that actions by
people in j 6= i have no instantaneous effect on the reward for people in topic i. 3
•	Value function V n ∈ Rd . Vin is the expected maximum total reward of being in topic i at time
n. A terminal value V N is given, which we set to zero to avoid making any assumption on the
problem structure beyond what is contained in the learned reward function.
•	Average reward ei(π, P, V ), for i ∈ {1, . . . , d} and V ∈ Rd and P ∈ S(G). This is the average
reward received by agents at topic i when the current distribution is π, action P is chosen, and the
subsequent expected maximum total reward is V . For a general rij(π, P), it is defined as:
d
ei(π,P,V) = XPij(rij(π,P) + Vj)	(4)
j=1
Intuitively, agents want to act optimally in order to maximize their expected total average reward.
For P ∈ S(G) and a vector q ∈ Si(G), define P(P, i, q) to be the matrix equal to P, except with the
i-th row replaced by q. Then a Nash maximizer is defined as follows:
Definition 1. A right stochastic matrix P ∈ S(G) is a Nash maximizer ofe(π, P, V ) if, given a fixed
π ∈ ∆d-1 and a fixed V ∈ Rd, there is
ei(π,P,V) ≥ei(π,P(P,i,q),V)	(5)
for any i ∈ {1, . . . , d} and any q ∈ Si(G).
The rows ofP form a Nash equilibrium set of actions, since for any topic i, the people in topic i can-
not increase their reward by unilaterally switching their action from Pi to any q. Under Definition 1,
the value function of each topic i at each time n satisfies the optimality criteria:
Vin= max	Xd qj rij(πn, P(P n, i, q)) + Vjn+1	(6)
q∈Si (G) j=1
A solution of the MFG is a sequence of pairs {(πn, V n)}n=0,...,N satisfying optimality criteria (6)
and forward equation (3).
2Section 4 explains that the population distribution π is the appropriate definition of an MDP state.
3If this assumption is removed, there is a resemblance between the discrete time MFG and a Markov game
in a continuous state and continuous action space (Littman, 2001; Hu et al., 1998). However, it turns out that
the general MFG is a strict generalization of a multi-agent MDP (Appendix G).
4
Published as a conference paper at ICLR 2018
4 Inference of MFG via MDP optimization
A Markov decision process is a well-known framework for optimization problems. We focus on
the discrete time MFG in Section 3.2 and prove a reduction to a finite-horizon deterministic MDP,
whose state trajectory under an optimal policy coincides with the forward evolution of the MFG.
This leads to the essential insight that solving the optimization problem of an MDP is equivalent to
solving an MFG that describes population behavior. This connection will enable us to apply efficient
inverse RL methods, using measured population trajectories, to learn an MFG model along with its
reward function in Section 4.1. The MDP is constructed as follows:
Definition 2. A finite-horizon deterministic MDP for a discrete time MFG over a complete graph is
defined as:
•	States: πn ∈ ∆d-1, the population distribution at time n.
•	Actions: Pn ∈ S(G), the transition probability matrix at time n.
•	Reward: R(πn, P n) := Pid=1 πin Pjd=1 Pinj rij (πn, Pin)
•	Finite-horizon state transition, given by Eq (3): ∀n ∈ {0, . . . , N - 1} : πjn+1 = Pid=1 Pinj πin.
Theorem 2. The value function of a solution to the discrete time MFG over a complete graph defined
by optimality criteria (6) and forward equation (3) is a solution to the Bellman optimality equation
of the MDP in Definition 2.
Proof. Since rij depends on Pn only through row Pin, optimality criteria 6 can be written as
Vn=Pmaxj XPijrij(∏n,Pi) + XPijVnT .	⑺
Pi ∈Si (G)
jj
We now define V * (∏n) as follows and show that it is the value function of the constructed MDP in
Definition 2 by verifying that it satisfies the Bellman optimality equation:
dd	d	d
V S=XnnVin=Xnn PmSaxG){X Si+X Pj
max
P ∈S(G)
πin	Pijrij(πn,Pi)+
j=1
j=1
Xd Pijπin	Vjn+1
d
Pm∈Sa(xG) R(πn,P)+Xπjn+1Vjn+1
j=1
Pm∈Sa(xG)R(πn,P)+V*(πn+1)
(8)
(9)
(10)
(11)
d
d
which is the Bellman optimality equation for the MDP in Definition 2.	□
Corollary 1. Given a start state π0, the state trajectory under the optimal policy of the MDP in
Definition 2 is equivalent to the forward evolution part of the solution to the MFG.
Proof. Under the optimal policy, equations 11 and 8 are satisfied, which means the matrix P gen-
erated by the optimal policy at any state πn is the Nash maximizer matrix. Therefore, the state
trajectory {∏n}n=o,…,n is the forward part of the MFG solution.	□
4.1 Reinforcement learning solution for MFG
MFG provides a general framework for addressing the problem of modeling population dynamics,
while the new connection between MFG and MDP enables us to apply inverse RL algorithms to
solve the MDP in Definition 2 with unknown reward. In contrast to previous MFG research, most of
which impose reward functions that are quadratic in actions and logarithmic in the state distribution
5
Published as a conference paper at ICLR 2018
(GUeant, 2009; LachaPelle et al., 2010; BaUso et al., 2016), We learn a reward function using demon-
stration trajectories measured from actual population behavior, to ground the MFG representation of
PoPulation dynamics on real data.
We leverage the MFG forward dynamics (Eq 3) in a samPle-based IRL method based on the maxi-
mum entroPy IRL framework (Ziebart et al., 2008). From this Probabilistic viewPoint, we minimize
the relative entroPy between a Probability distribution p(τ) over a sPace of trajectories T := {τi}i
and a distribution q(τ) from which demonstrated exPert trajectories are generated (Boularias et al.,
2011). This is related to a Path integral IRL formulation, where the likelihood of measured oPti-
mal trajectories is evaluated only using trajectories generated from their local neighborhood, rather
than uniformly over the whole trajectory sPace (Kalakrishnan et al., 2013). SPecifically, making no
assumPtion on the true distribution of oPtimal demonstration other than matching of reward exPec-
tation, we Posit that demonstration trajectories τi = (π0, P1, . . . , πN-1, PN-1)i are samPled from
the maximum entroPy distribution (Jaynes, 1957):
P(T) = ZeXp(RW (T))
(12)
where RW(T) = n RW (πn, Pn) is the sum of reward of single state-action Pairs over a trajec-
tory T, and W are the Parameters of the reward function aPProximator (derivation in APPendix E).
Intuitively, this means that trajectories with higher reward are exPonentially more likely to be sam-
Pled. Given M samPle trajectories Tj ∈ DsamP from k distributions F1 (T), . . . , Fk(T), an unbi-
ased estimator of the Partition function Z = eXp(RW (T))dT using multiPle imPortance sam-
pling is Z := M PT) Zj exp(Rw (Tj)) (Owen & Zhou, 2000), where importance weights are
Zj := [ 1 Pk Fk (Tj)] 1 (derivation in Appendix F). Each action matrix P is sampled from a
stochastic policy Fk (P; π, θ) (overloading notation with F(T)), where π is the current state and
θ the policy parameter. The negative log likelihood of L demonstration trajectories Ti ∈ Ddemo is:
L(W) = - j- X RW (Ti) + log
L
τi ∈Dd
emo
MM X Zj eχp(Rw(Tj))
τj ∈Dsamp
(13)
We build on Guided Cost Learning (GCL) in Finn et al. (2016) (Alg 1) to learn a deep neural
network approximation of RW (π, P) via stochastic gradient descent on L(W), and learn a policy
F(P; π, θ) using a simple actor-critic algorithm (Sutton & Barto, 1998). In contrast to GCL, we
employ a combination of convolutional neural nets and fully-connected layers to process both the
action matrix P and state vector π efficiently in a single architecture (Appendix C), analogous
to how Lillicrap et al. (2015) handle image states in Atari games. Due to our choice of policy
parameterization (described below), we also set importance weights to unity for numerical stability.
These implementation choices result in successful learning of a reward representation (Fig 1).
Our forward MDP solver (Alg 2) performs gradient ascent on the policy’s expected start value
E[v(∏0)∣F (P; ∏,θ)] w.r.t. θ, to find successively better policies Fk (P ； ∏, θ). We construct the joint
distribution F(P; π, θ) informed by domain knowledge about human population behavior on social
media, but this does not reduce the generality of the MFG framework since it is straightforward
to employ flexible policy and value networks in a DDPG algorithm when intuition is not available
(Silver et al., 2014; Lillicrap et al., 2015). Our joint distribution is d instances of a d-dimensional
Dirichlet distribution, each parameterized by an αi ∈ Rd+ . Each row Pi is sampled from
1d	i
f(Pii,…,Pid； α1,..., αd) = —— ∏(Pij )αj-1	(14)
1 d	B(αi) j=1
where B(∙) is the Beta function and αj is defined using the softplus function ɑj(∏,θ):=
ln(1 + exp{θ(πj - πi)}), which is a monotonically increasing function of the population density
difference πj - πi . In practice, a constant scaling factor c ∈ R can be applied to α for variance
reduction. Finally, we let F(Pn; πn, θ) = Qid=1 f(Pin; αi(πn, θ)) denote the parameterized policy,
from which Pn is sampled based on ∏n, and whose logarithmic gradient Vθ ln(F) can be used in a
policy gradient algorithm. We learned an approximate value function V (π; w) as a baseline for vari-
ance reduction, approximated as a linear combination of all polynomial features of π up to second
order, with parameter w (Sutton et al., 2000).
6
Published as a conference paper at ICLR 2018
Reward density fortraining demo and generated transitions
(a) Reward densities on train set
Reward density for test demo and generated transitions
(b) Reward densities on test set
(c) Reward of state-action pairs
Figure 1: (a) JSD between train demo and generated transitions is 0.130. (b) JSD between test demo
and generated transitions is 0.017. (c) Reward of state-action pairs. States: large negative mass
gradient from π1 to πd (S0), less negative gradient (S1), uniform (S2). Actions: high probability
transitions to smaller indices (A0), uniform transition (A1), row-reverse of A0 (A2).
5 Experiments
We demonstrate the effectiveness of our method with two sets of experiments: (i) inference of an
interpretable reward function and (ii) prediction of population trajectory over time. Our experiment
matches the discrete time mean field game given in Section 3.2: we use data representing the ac-
tivity of a Twitter population consisting of 406 users. We model the evolution of the population
distribution over d = 15 topics and N = 16 time steps (9am to midnight) each day for 27 days.
The sequence of state-action pairs {(πn, P n)}n=0,...,N -1 measured on each day shall be called a
demonstration trajectory. Although the set of topics differ semantically each day, indexing topics
in order of decreasing initial popularity suffices for identifying the topic sets across all days. As
explained earlier, the MFG framework can model populations of arbitrarily large size, and we find
that our chosen size is sufficient for extracting an informative reward and policy from the data. For
evaluating performance on trajectory prediction, we compare MFG with two baselines:
VAR. Vector autoregression of order 18 trained on 21 demonstration trajectories.
RNN. Recurrent neural network with a single fully-connected layer and rectifier nonlinearity.
We use Jenson-Shanon Divergence (JSD) as metric to report all our results. Appendix D provides
comprehensive implementation details.
5.1	Interpretation of reward function
We evaluated the reward using four sets of state-action pairs acquired from: 1. all train demo
trajectories; 2. trajectories generated by the learned policy given initial states π0 of train trajectories;
3. all test demo trajectories; 4. trajectories generated by the learned policy given initial states π0 of
test trajectories. We find three distinct modes in the density of reward values for both the train group
of sets 1 and 2 (Fig 1a) and the test group of sets 3 and 4 (Fig 1b). Although we do not have access to
a ground truth reward function, the low JSD values of 0.13 and 0.017 between reward distributions
for demo and generated state-action pairs show generalizability of the learned reward function. We
further investigated the reward landscape with nine state-action pairs (Figure 1c), and find that the
mode with highest rewards is attained by pairing states that have large mass in topics having high
initial popularity (S0) with action matrices that favor transition to topics with higher density (A0).
Uniformly distributed state vectors (S2) attain the lowest rewards, and states with a small negative
mass gradient from topic 1 to topic d (S1) attain medium rewards. Simply put, MFG agents who
optimize for this reward are more likely to move towards more popular topics. While this numerical
exploration of the reward reveals interpretable patterns, the connection between such rewards learned
via our method and any optimization process in the population requires more empirical study.
5.2	Trajectory prediction
To test the usefulness of the reward and MFG model for prediction, the learned policy was used with
the forward equation to generate complete trajectories, given initial distributions. Fig 2a (log scale)
shows that MFG has 58% smaller error than VAR when evaluated on the JSD between generated
7
Published as a conference paper at ICLR 2018
(a) Prediction error
O 5	10	O 5	10
(b) Action matrices
(a) Topic 0 test trajectory
Figure 2: (a) Test error on final distribution and mean over entire trajectory (log scale). MFG: (2.9e-
3, 4.9e-3), VAR: (7.0e-3, 8.1e-3), RNN: (0.58, 0.57). (b) heatmap of action matrix P ∈ R15×15
averaged element-wise over demo train set, and absolute difference between average demo action
matrix and average matrix generated from learned policy.
(b) Topic 2 test trajectory
Figure 3: (a) Measured and predicted trajectory of topic 0 popularity over test days for MFG and
VAR (RNN outside range and not shown). (b) Measured and predicted trajectory of topic 2 popu-
larity over test days for all methods.
and measured final distributions JSD(πgNen-er1ated, πmNe-as1ured), and 40% smaller error when evaluated on
the average JSD over all hours in a day N PN-1 JSD(∏nenerated, ∏Seasured). Both measures Were av-
eraged over M = 6 held-out test trajectories. It is worth emphasizing that learning the MFG model
required only the initial population distribution of each day in the training set (line 4 in Alg 2), While
VAR and RNN used the distributions over all hours of each day. MFG achieves better prediction per-
formance even With feWer training samples, possibly because it is a more structured approximation
of the true mechanism underlying population dynamics, in contrast to VAR and RNN that rely on
regression. As shoWn by sample trajectories for topic 0 and 2 in Figures 3, and the average transition
matrices in Figure 2b, MFG correctly represents the fact that the real population tends to congre-
gate to topics With higher initial popularity (loWer topic indices), and that the popularity of topic
0 becomes more dominant across time in each day. The small real-World dataset size, and the fact
that RNN mainly learns state transitions Without accounting for actions, could be contributing fac-
tors to the loWer performance of RNN. We acknoWledge that our design of policy parameterization,
although informed by domain knoWledge, introduced bias and resulted in noticeable differences be-
tWeen demonstration and generated transition matrices. This can be addressed using deep policy
and value netWorks, since the MFG frameWork is agnostic toWards choice of policy representation. 6
6 Conclusion
We have motivated and demonstrated a data-driven method to solve a mean field game model of pop-
ulation evolution, by proving a connection to Markov decision processes and building on methods
8
Published as a conference paper at ICLR 2018
in reinforcement learning. Our method is scalable to arbitrarily large populations, because the MFG
framework represents population density rather than individual agents, while the representations are
linear in the number of MFG states and quadratic in the transition matrix. Our experiments on real
data show that MFG is a powerful framework for learning a reward and policy that can predict tra-
jectories of a real world population more accurately than alternatives. Even with a simple policy
parameterization designed via some domain knowledge, our method attained superior performance
on test data. It motivates exploration of flexible neural networks for more complex applications.
An interesting extension is to develop an efficient method for solving the discrete time MFG in a
more general setting, where the reward at each state i is coupled to the full population transition
matrix. Our work also opens the path to a variety of real-world applications, such as a synthesis
of MFG with models of social networks at the level of individual connections to construct a more
complete model of social dynamics, and mean field models of interdependent systems that may
display complex interactions via coupling through global states and reward functions.
Acknowledgments
We sincerely thank our anonymous ICLR reviewers for critical feedback that helped us to improve
the clarity and precision of our presentation. This work was supported in part by NSF CMMI-
1745382 and NSF IIS-1717916.
References
Yves Achdou, Fabio Camilli, and Italo Capuzzo-Dolcetta. Mean field games: numerical methods for the
planning problem. SIAM Journal on Control and Optimization, 50(1):77-109, 2012.
Monica Anderson and Paul Hitlin. Social Media Conversations About Race. Pew Research Center, August
2016.
Dario Bauso, Raffaele Pesenti, and Marco Tolotti. Opinion dynamics and stubbornness via multi-population
mean-field games. Journal of Optimization Theory and Applications, 170(1):266293, 2016.
Abdeslam Boularias, Jens Kober, and Jan Peters. Relative entropy inverse reinforcement learning. In Pro-
ceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pp. 182-189,
2011.
Amarjit Budhiraja, Xin Liu, and Adam Shwartz. Action time sharing policies for ergodic control of markov
chains. SIAM Journal on Control and Optimization, 50(1):171-195, 2012.
Apostolos N Burnetas and Michael N Katehakis. Optimal adaptive policies for markov decision processes.
Mathematics of Operations Research, 22(1):222-255, 1997.
Elisabetta Carlini and FranCisCo Jose Silva. A fully discrete semi-lagrangian scheme for a first order mean field
game problem. SIAM Journal on Numerical Analysis, 52(1):45-67, 2014.
Abir De, Isabel Valera, Niloy Ganguly, Sourangshu Bhattacharya, and Manuel Gomez Rodriguez. Learning and
forecasting opinion dynamics in social networks. In Advances in Neural Information Processing Systems,
pp. 397-405, 2016.
Mehrdad Farajtabar, Yichen Wang, Manuel Gomez Rodriguez, Shuang Li, Hongyuan Zha, and Le Song. Co-
evolve: A joint point process model for information diffusion and network co-evolution. In Advances in
Neural Information Processing Systems, pp. 1954-1962, 2015.
Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control via policy
optimization. In International Conference on Machine Learning, pp. 49-58, 2016.
Diogo A Gomes, Joana Mohr, and Rafael Rigao Souza. Discrete time, finite state space mean field games.
Journal de mathematiques Pures et appliquees, 93(3):308-328, 2010.
Olivier Gueant. A reference case for mean field games models. Journal de mathematiques pures et appliquees,
92(3):276-294, 2009.
Olivier Gueant. Existence and uniqueness result for mean field games with congestion effect on graphs. Applied
Mathematics & Optimization, 72(2):291-303, 2015.
Olivier Gueant, Jean-Michel Lasry, and Pierre-Louis Lions. Mean field games and applications. In Paris-
Princeton lectures on mathematical finance 2010, pp. 205-266. Springer, 2011.
Philip N Howard, Aiden Duffy, Deen Freelon, Muzammil M Hussain, Will Mari, and Marwa Maziad. Opening
closed regimes: what was the role of social media during the arab spring?, 2011.
9
Published as a conference paper at ICLR 2018
Junling Hu, Michael P Wellman, et al. Multiagent reinforcement learning: theoretical framework and an
algorithm. In ICML, volume 98, pp. 242-250. Citeseer, 1998.
Minyi Huang, Roland P Malhame, Peter E Caines, et al. Large population stochastic dynamic games: Closed-
loop mckean-vlasov systems and the nash certainty equivalence principle. Communications in Information
& Systems, 6(3):221-252, 2006.
Edwin T Jaynes. Information theory and statistical mechanics. Physical review, 106(4):620, 1957.
Mrinal Kalakrishnan, Peter Pastor, Ludovic Righetti, and Stefan Schaal. Learning objective functions for
manipulation. In Robotics and Automation (ICRA), 2013 IEEE International Conference on, pp. 1331-1336.
IEEE, 2013.
Aime Lachapelle, Julien Salomon, and Gabriel Turinici. Computation of mean field equilibria in economics.
Mathematical Models and Methods in Applied Sciences, 20(04):567-588, 2010.
Jean-Michel Lasry and Pierre-Louis Lions. Mean field games. Japanese journal of mathematics, 2(1):229-260,
2007.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver,
and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971,
2015.
Michael L Littman. Value-function reinforcement learning in markov games. Cognitive Systems Research, 2
(1):55-66, 2001.
Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for
mixed cooperative-competitive environments. arXiv preprint arXiv:1706.02275, 2017.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and
Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.
Andrew Y. Ng and Stuart Russell. Algorithms for inverse reinforcement learning. In in Proc. 17th International
Conf. on Machine Learning, pp. 663-670. Morgan Kaufmann, 2000.
Art Owen and Yi Zhou. Safe and effective importance sampling. Journal of the American Statistical Associa-
tion, 95(449):135-143, 2000.
Andrew Perrin. Social Media Usage: 2005-2015. Pew Research Center, October 2015.
Skipper Seabold and Josef Perktold. Statsmodels: Econometric and statistical modeling with python. In 9th
Python in Science Conference, 2010.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Determinis-
tic policy gradient algorithms. In Proceedings of the 31st International Conference on Machine Learning
(ICML-14), pp. 387-395, 2014.
Craig Silverman. This analysis shows how viral fake election news stories outperformed real
news on facebook, 2016. URL https://www.buzzfeed.com/craigsilverman/
viral-fake-election-news-outperformed-real-news-on-facebook.
Richard S. Sutton and Andrew G. Barto. Introduction to Reinforcement Learning. MIT Press, Cambridge, MA,
USA, 1st edition, 1998. ISBN 0262193981.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for
reinforcement learning with function approximation. In Advances in neural information processing systems,
pp. 1057-1063, 2000.
Twitter. Faqs about trends on twitter, 2017. URL https://support.twitter.com/articles/
101125.
Markus Wulfmeier, Peter Ondruska, and Ingmar Posner. Maximum entropy deep inverse reinforcement learn-
ing. arXiv preprint arXiv:1507.04888, 2015.
Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse reinforce-
ment learning. In AAAI, volume 8, pp. 1433-1438. Chicago, IL, USA, 2008.
10
Published as a conference paper at ICLR 2018
A Proof of Propostion 1
Given the definitions in Section 3.1, a mean field game is defined by a Hamilton-Jacobi-Bellman
(HJB) equation evolving backwards in time and a Fokker-Planck equation evolving forward in time.
The continuous-time Hamilton-Jacobi-Bellman (HJB) equation on G is
Vi ⑴=- maxPi { £ yp- Pij (t)(Vj ⑴-Vi(t)) + ri(π(t),Pi(t))
j∈Vi
(15)
where ri (π, Pi ) is the reward function, and Vi (t) is the value function of state i at time t. Note that
the reward function ri(π(t), Pi(t)) is often presented as -ci(π(t), Pi(t)) for some cost function
ci(π(t), Pi(t)) in the MFG context, and similarly for Vi(t). In addition, we set ri(π(t), Pi(t)) =
一∞ if Pi(t) / Si(G) (i.e. P(t) must be a valid transition matrix). For any fixed ∏(t),let Hi(∏(t), ∙)
be the Legendre transform of ci(∏(t'), ∙) defined by
Hi(∏(t), ∙) = maxPi {h∙,Pii - Ci(∏(t),Pi)} = maxPi {h∙,Pii + ri(π(t),Pi)}	(16)
Then the HJB equation (15) is an analogue to the backward equation in mean field games
Vi0(t) + Hi(∏(t), [Vj(t) - Vi(t)]j∈V-) = 0	(17)
where [Vj(t) - Vi(t)]j∈V- / RIVi | is the dual variable of Pi. We can discretize (15) using a
semi-implicit scheme with unit time step labeled by n to obtain
Vn+1 - Vn = - maxPi {Xj∈v-Pin(Vn+1 - Vn+1) + ri(∏n, Pn)}	(18)
Rearranging (18) yields the discrete time HJB equation over a graph (19)
Vn = maxPi 卜(∏n,Pin)+ Xj∈V- PjVn+11	(19)
The forward evolving Fokker-Planck equation for the continuous-time graph-state MFG is given by
πi0(t) = j∈V+ Qji(t)πj(t) - j∈V- Qij(t)πi(t)	(20)
where Qji⑴=dUiHj(n(t), [Vk⑴- Vj(t)]k∈v-)	QI)
where ∂ui Hj (π, u) is the partial derivative w.r.t. the coordinate corresponding to the i-th index of
the argument u ∈ R|Vj- | . We can set Qji(t) = 0 for all (j, i) ∈/ E, so that Q(t) := [Qji(t)] can be
regarded as the d-by-d infinitesimal generator matrix of states π(t), and hence (20) can be written as
π0(t) = π(t)Q(t), where π(t) ∈ Rd is a row vector. Then an Euler discretization of (20) with unit
time step reduces to πn+1 - πn = πn Qn , which can be written as
∏n+1 = Xj∈v+ Pni∏n	(22)
where Pinj := Qinj + δij . If the graph G is complete, meaning E = {(i, j) : 1 ≤ i, j ≤ d}, then
the summation is taken over j = 1, . . . , d. For ease of presentation, we only consider the complete
graph in this paper, as all derivations can be carried out similarly for general directed graphs. A
solution ofa mean field game defined by (19) and (22) is a collection of Vin and πin for i = 1, . . . , d
and n = 0, . . . , N .
11
Published as a conference paper at ICLR 2018
B Algorithms
We learn a reward function and policy using an adaptation of GCL (Finn et al., 2016) in Alg 1 and
a simple actor-critic Alg 2 (Sutton & Barto, 1998) as a forward RL solver.
Algorithm 1 Guided cost learning
1	: procedure GUIDED COST LEARNING
2	:	Initialize F0(P; π, θ) as random policy and reward network weights W0
3	:	for iteration 1 to I do
4	:	Generate sample trajectories Dtraj from Fk(P; π, θ)
5	:	Dsamp J Dsamp ∪ Dtraj
6	while IAvgni,pi〜DdemO (RWt (∏i,Pi) - RWt-ι (∏i,Pi))∣ > dR do
7	:	Sample demonstration Ddemo ⊂ Ddemo from expert demonstration
8	C	i zr>	一 zr> :	Sample Dsamp ⊂ Dsamp
9	Wt+1 J Wt iVL(Wt) using DDdemo and DSamP
10	:	end while
11	:	Run Alg 2 with new RW for improved Fk+1
12	:	end for
13	:	return Final reward function RW (π, P) and policy F(P; π, θ)
14	: end procedure
Algorithm 2 Actor-critic algorithm for MFG
Input: Generative model F(P; ∏, θ), value function V(∏; w), training data {∏0}m days
Output: Policy parameter θ, value function parameter w
1： procedure ACTOR-CRmC-MFG(F, V, {∏0}Mdays, β, ξ, RW)
2:	initialize θ and w
3： for episodes s = 1, . . . , S do
4：	Sample initial distribution π0 from {π0}M days
5：	for time step n = 0, . . . , N - 1 do
6:	Sample action Pn 〜F (P; ∏n,θ)
7：	Generate πn+1 using Eq 3
8:	Receive reward RW (πn, Pn)
9:	δ — R + V(πn+1; W) - V(πn; W)
10:	w J w + ξδVw V(πn; W)
11:	θ J θ + βδVθ log(F(P; ∏n,θ))
12:	end for
13:	end for
14:	end procedure
C Reward network
Our reward network uses two convolutional layers to process the 15 × 15 action matrix P, which
is then flattened and concatenated with the state vector π and processed by two fully-connected
layers regularized with L1 and L2 penalties and dropout (probability 0.6). The first convolutional
layer zero-pads the input into a 19 × 19 matrix and convolves one filter of kernel size 5 × 5 with
stride 1 and applies a rectifier nonlinearity. The second convolutional layer zero-pads its input into
a 17 × 17 matrix and convolves 2 filters of kernel size 3 × 3 with stride 1 and applies a rectifier
nonlinearity. The fully connected layers have 8 and 4 hidden rectifier units respectively, and the
output is a single fully connected tanh unit. All layers were initialized using the Xavier normal
initializer in Tensorflow.
12
Published as a conference paper at ICLR 2018
D Experiment details
By default, Twitter users in a certain geographical region primarily see the trending topics specific
to that region (Twitter, 2017). This experiment focused on the population and trending topics in the
city of Atlanta in the U.S. state of Georgia. First, a set of 406 active users were collected to form
the fixed population. This was done by collecting a set of high-visibility accounts in Atlanta (e.g.
the Atlanta Falcons team), gathering all Twitter users who follow these accounts, filtering for those
whose location was set to Atlanta, and filtering for those who responded to least two trending topics
within four days.
Data collection proceeded as follows for 27 days: at 9am of each day, a list of the top 14 trending
topics on Twitter in Atlanta was recorded; for each hour until midnight, for each topic, the number
of users who responded to the topic and the transition counts among topics within the past hour was
recorded. Whether or not a user responded to a topic was determined by checking for posts by the
user containing unique words for that topic; the “hashtag” convention of trending topics on Twitter
reduces the likelihood of false positives. The hourly count of people who did not respond to any
topic was recorded as the count for a “null topic”. Although some users may respond to more than
one topic within each hour, the data shows that this is negligible, and a shorter time interval can be
used to reduce this effect. The result of data collection is a set of trajectories, one trajectory per day,
where each trajectory consists of hourly measurements of the population distribution over d = 15
topics and their transition matrix over N = 16 hours.
The training set consists of trajectories {π0,m, P0,m, . . . , PN-2,m, πN-1,m}m=1,...,M over the first
M = 21 days. MFG uses the initial distribution π0 of each day, along with the transition equation of
the constructed MDP and the policy F(P; πn, θ), to produce complete trajectories for training (Alg 2
lines 4,6,7). In contrast, VAR and RNN are supervised learning methods and they use all measured
distributions. RNN employs a simple recurrent unit with ReLU as nonlinear activation and weight
matrix of dimension d × d. VAR was implemented using the Statsmodels module in Python, with
order 18 selected via random sub-sampling validation with validation set size 5 (Seabold & Perktold,
2010). For prediction accuracy, all three methods were evaluated against data from 6 held-out test
days. Table 1 shows parameters of Alg 2 and 1.
Table 1: Parameters
Parameter	Use	Value
S	max actor-critic episodes	4000
β	critic learning rate	O(1/s)
ξ	actor learning rate	O(1/s ln ln s)
c	αij scaling factor	1e4
	Adam optimizer learning rate for reward	1e-4
dR	convergence threshold for reward iteration	1e-4
θfinal	learned policy parameter	8.64
E Maximum entropy distribution
Given a finite set of trajectories {τi }i, where each trajectory is a sequence of state-action pairs
τi = (si1, ai1, . . . , ). Suppose each trajectory τi has an unknown probability pi. The entropy of
the probability distribution is H = - Pi pi ln(pi). In the continuous case, we write the differential
entropy:
H
p(τ ) ln(p(τ))dτ
where p(∙) is the probability density We want to derive. The constraints are:
J r(τ)p(τ)dτ = E[r(τ)] = μr
p(τ )dτ = 1
13
Published as a conference paper at ICLR 2018
The first constraint says: the expected reward over all trajectories is equal to an empirical measure-
ment μr. We write the Lagrangian L:
L = — /p(τ)ln(p(τ))dτ - λι (/p(τ)dτ - l) - λ? (/ r(τ)p(τ)dτ -出丁
For L to be stationary, the Euler-Lagrange equation with integrand denoted by L says
∂L
∂p = 0
since L does not depend on 冬.Hence
λ1 = ln	e-λ2r(τ)dτ - 1
p(τ) = exp - ln	e-λ2r(τ) dτ -
where Z := e-λ2r(τ)dτ. Then the constant λ2 is determined by:
一1一 e-λ2r(T)
Z (λ2)
μr = /p(τ)r(τ)dτ = Z(λ, / e-λ2r(τ)r(τ)dτ
∂
=一而-In(Z(λ2))
∂λ2
F Multiple importance sampling
We show how multiple importance sampling (Owen & Zhou, 2000) can be used to estimate the par-
tition function in the maximum entropy IRL framework. The problem is to estimate Z := f (x)dx.
Let p1 , . . . , pm be m proposal distributions, with nj samples from the j-th proposal distribution, so
that samples can be denoted Xij for i = 1, . . . , nj and j = 1, . . . , m. Let wj (x) for j = 1, . . . , m
satisfy
m
0 ≤ wj(x) ≤	wj(x) = 1
j=1
Then define the estimator
m	nj
Z=X羡X Wj(Xij)
n
j=1	j i=1
f (Xij)
Pj (Xij )
Let S(pj) = {x | pj (x) > 0} be the support ofpj and S(wj) = {x | wj (x) > 0} be the support of
wj, and let them satisfy S(wj) ⊂ S(pj). Under these assumptions:
In particular, choose
E[Z] = /
f (x)dx
wj(x)
njPj(x)
Pm=I nk Pk (X)
Z
Then the estimate becomes
m nj
Z = XX
f(Xji)
Pk=I nkPk (X)
l X X	f (χji)
n j=1 i=1 P乙味pk (X)
where n = Pjm=1 nj is the total count of samples. Further assuming that samples are drawn uni-
formly from all proposal distributions, so that nj = nk = n/m for all j, k ∈ {1, . . . , m}, the
expression for Z reduces to the form used in Eq 13:
Z=I X
all samples
f(X)
mm Pm=IPk(X)
14
Published as a conference paper at ICLR 2018
G	A comparis on of mean field games and multi-agent MDPs
In this section, we discuss the reason that the general MFG, whose reward function rij (πn, Pn)
depends on the full Nash maximizer matrix Pn, is neither reducible to a collection of distinct single-
agent MDPs nor equivalent to a multi-agent MDP. Let a state in the discete space MFG be called a
“topic”, to avoid confounding with an MDP state.
G.1 Collection of single-agent MDPs
Consider each topic i as a separate entity associated with a value, rather than subsuming it into an
average (as is the case in Section 4). In order to assign a value to each topic, each tuple (i, πn) must
be defined as a state, which leads to the problem: since a state requires specification of πn, and state
transitions depend on the actions for all other topics, the action at each topic is not sufficient for
fully specifying the next state. More formally, consider a value function on the state:
V (i, πn)
qjrij(πn,P(Pn,i,q))+XqjV(j,(Pn)Tπn)
j
(23)
Superficially, this resembles the Bellman optimality equation for the value function in a single-agent
stochastic MDP, where s is a state, a is an action, R is an immediate reward, and P(s0|s, a) is the
probability of transition to state s0 from state s, given action a:
V *(s) = max{R(s,a) + EP(SlS,a)V *(s0)}	(24)
s0
In equation 23, qj can be interpreted as a transition probability, conditioned on the fact that the
current topic is i. The action q selected in the state (i, πn) induces a stochastic transition to a
next topic j , but the next distribution πn+1 is given by the deterministic forward equation πn+1 =
(Pn )T πn , where Pn is the true Nash maximizer matrix. This means that qj does not completely
specify the next state (j,∏n+1), and there is a formal difference between P(s0∣s, a)V*(s0) and
qjV(j, (Pn)Tπn). Also notice that the Bellman equation sums over all possible next states S0, but
equation 23 only sums over topics j rather than full states (j, π).
G.2 Multi-agent MDP
Short of modeling every single agent in the MFG, an exact reduction from the MFG to a multi-agent
MDP (i.e. Markov game) is not possible. A discrete state space discrete action space multi-agent
MDP is defined by d agents moving within a set S of environment states; a collection {A1, . . . , Ad}
of action spaces; a transition function P(S0|S, a1, . . . , ad) giving the probability of the environment
transitioning from current state S to next state s0, given that agents choose actions a := (aι ,...,ad);
a collection of reward functions {Ri(S, a1, . . . , ad)}i; and a discount factorγ.
Let the set of πn (with appropriate discretization) be the state space and limit the set of actions to
some discretization of the simplex. The alternative to modeling individual MFG agents is to consider
each topic as a single “agent”. Now, the agent representing topic i is no longer identified with the set
of people who selected topic i: topics have fixed labels for all time, so an agent can only accumulate
reward for a single topic, whereas people in the MFG can move among topics. Therefore, the value
function for agent i in a Markov game is defined only in terms of itself, never depending on the
value function of agents j 6= i:
Vμ(s) = XYμj(aj|s) (Ri(S,a) + YX P (s0∣s, a)V"(s0))	(25)
ɑ j	S0	)
where μ := (μι,..., μd) is a set of stationary policies of all agents. However, recall that the MFG
equation for Vin explicitly depends on Vjn+1 of all topics j , which would require a different form
such as the following:
Vμ(∏n) = X Yμj(Pjl∏n) (XPikTik(∏n,P) + XPikVKPTn))	(26)
P∈S(G) j=1	k	k
where the last terms sums over value functions Vμ for all topics k. This mixing between value
functions prevents a reduction from the MFG to a standard Markov game.
15