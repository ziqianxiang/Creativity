Published as a conference paper at ICLR 2018
Certifying Some Distributional Robustness
with Principled Adversarial Training
Aman Sinha*,1, Hongseok Namkoong*,2, John Duchi1,3
Departments of 1Electrical Engineering, 2Management Science & Engineering, 3Statistics
Stanford University
Stanford, CA 94305
{amans,hnamk,jduchi}@stanford.edu
Ab stract
Neural networks are vulnerable to adversarial examples and researchers have pro-
posed many heuristic attack and defense mechanisms. We address this problem
through the principled lens of distributionally robust optimization, which guar-
antees performance under adversarial input perturbations. By considering a La-
grangian penalty formulation of perturbing the underlying data distribution in a
Wasserstein ball, we provide a training procedure that augments model param-
eter updates with worst-case perturbations of training data. For smooth losses,
our procedure provably achieves moderate levels of robustness with little compu-
tational or statistical cost relative to empirical risk minimization. Furthermore,
our statistical guarantees allow us to efficiently certify robustness for the popu-
lation loss. For imperceptible perturbations, our method matches or outperforms
heuristic approaches.
1 Introduction
Consider the classical supervised learning problem, in which we minimize an expected loss
EPo ['(θ; Z)] over a parameter θ ∈ Θ, where Z 〜Po, Po is a distribution on a space Z, and '
is a loss function. In many systems, robustness to changes in the data-generating distribution P0 is
desirable, whether they be from covariate shifts, changes in the underlying domain (Ben-David et al.,
2010), or adversarial attacks (Goodfellow et al., 2015; Kurakin et al., 2016). As deep networks be-
come prevalent in modern performance-critical systems (perception for self-driving cars, automated
detection of tumors), model failure is increasingly costly; in these situations, it is irresponsible to
deploy models whose robustness and failure modes we do not understand or cannot certify.
Recent work shows that neural networks are vulnerable to adversarial examples; seemingly imper-
ceptible perturbations to data can lead to misbehavior of the model, such as misclassification of the
output (Goodfellow et al., 2015; Nguyen et al., 2015; Kurakin et al., 2016; Moosavi-Dezfooli et al.,
2016). Consequently, researchers have proposed adversarial attack and defense mechanisms (Pa-
pernot et al., 2016a;b;c; Rozsa et al., 2016; Carlini & Wagner, 2017; He et al., 2017; Madry et al.,
2017; Tramer et al., 2017). These works provide an initial foundation for adversarial training, but it
is challenging to rigorously identify the classes of attacks against which they can defend (or if they
exist). Alternative approaches that provide formal verification of deep networks (Huang et al., 2017;
Katz et al., 2017a;b) are NP-hard in general; they require prohibitive computational expense even on
small networks. Recently, researchers have proposed convex relaxations of the NP-hard verification
problem with some success (Kolter & Wong, 2017; Raghunathan et al., 2018), though they may be
difficult to scale to large networks. In this context, our work is situated between these agendas: we
develop efficient procedures with rigorous guarantees for small to moderate amounts of robustness.
We take the perspective of distributionally robust optimization and provide an adversarial training
procedure with provable guarantees on its computational and statistical performance. We postulate
a class P of distributions around the data-generating distribution Po and consider the problem
minimize sup EP [`(θ; Z)].
θ∈Θ P∈P
(1)
* Equal contribution
1
Published as a conference paper at ICLR 2018
The choice of P influences robustness guarantees and computability; we develop robustness sets
P with computationally efficient relaxations that apply even when the loss ` is non-convex. We
provide an adversarial training procedure that, for smooth `, enjoys convergence guarantees simi-
lar to non-robust approaches while certifying performance even for the worst-case population loss
SuPP∈p EP ['(θ; Z)]. On a simple implementation in Tensorflow, our method takes 5-10× as long
as stochastic gradient methods for empirical risk minimization (ERM), matching runtimes for other
adversarial training procedures (Goodfellow et al., 2015; Kurakin et al., 2016; Madry et al., 2017).
We show that our procedure—which learns to protect against adversarial perturbations in the training
dataset—generalizes, allowing us to train a model that prevents attacks to the test dataset.
We briefly overview our approach. Let c : Z × Z → R+ ∪ {∞}, where c(z, z0) is the “cost” for
an adversary to perturb z0 to z (we typically use c(z, z0) = kz - z0kp2 with p ≥ 1). We consider
the robustness region P = {P : Wc(P, P0) ≤ ρ}, a ρ-neighborhood of the distribution P0 under
the Wasserstein metric Wc(∙, ∙) (see Section 2 for a formal definition). For deep networks and other
complex models, this formulation of problem (1) is intractable with arbitrary ρ. Instead, we consider
its Lagrangian relaxation for a fixed penalty parameter γ ≥ 0, resulting in the reformulation
minimize F F (θ) := sup {Ep['(θ; Z)] - γWc(P,P°)} = Ep0 [φγ (θ; Z )]∖	(2a)
θ∈Θ	P	0
where φγ(θ; zo) := SuP {'(θ; Z) - γc(z,z0)} .	(2b)
z∈Z
(See Proposition 1 for a rigorous statement of these equalities.) Here, we have replaced the usual
loss `(θ; Z) by the robust surrogate φγ (θ; Z); this surrogate (2b) allows adversarial perturbations of
the data z, modulated by the penalty γ. We typically solve the penalty problem (2) with P0 replaced
by the empirical distribution Pn, as P0 is unknown (we refer to this as the penalty problem below).
The key feature of the penalty problem (2) is that moderate levels of robustness—in particular,
defense against imperceptible adversarial perturbations—are achievable at essentially no computa-
tional or statistical cost for smooth losses `. Specifically, for large enough penalty γ (by duality,
small enough robustness ρ), the function z 7→ `(θ; z) - γc(z, z0) in the robust surrogate (2b) is
strongly concave and hence easy to optimize if `(θ, z) is smooth in z. Consequently, stochastic
gradient methods applied to problem (2) have similar convergence guarantees as for non-robust
methods (ERM). In Section 3, we provide a certificate of robustness for any ρ; we give an efficiently
computable data-dependent upper bound on the worst-case loss SuPPWc(P,p0)≤ρ EP ['(θ; Z)]. That
is, the worst-case performance of the output of our principled adversarial training procedure is guar-
anteed to be no worse than this certificate. Our bound is tight when ρ = ρbn, the achieved robustness
for the empirical objective. These results suggest advantages of networks with smooth activations
rather than ReLU’s. We experimentally verify our results in Section 4 and show that we match or
achieve state-of-the-art performance on a variety of adversarial attacks.
Robust optimization and adversarial training The standard robust-optimization approach mini-
mizes losses of the form SuPu∈U `(θ; z + u) for some uncertainty setU (Ratliff et al., 2006; Ben-Tal
et al., 2009; Xu et al., 2009). Unfortunately, this approach is intractable except for specially struc-
tured losses, such as the composition ofa linear and simple convex function (Ben-Tal et al., 2009; Xu
et al., 2009; 2012). Nevertheless, this robust approach underlies recent advances in adversarial train-
ing (Szegedy et al., 2013; Goodfellow et al., 2015; Papernot et al., 2016b; Carlini & Wagner, 2017;
Madry et al., 2017), which heuristically perturb data during a stochastic optimization procedure.
One such heuristic uses a locally linearized loss function (proposed with p = ∞ as the “fast gradient
sign method” (Goodfellow et al., 2015)):
∆χi (θ) := argmax{Vχ'(θ; (xi, yi))Tη} and perturb Xi → Xi + ∆χi (θ).	(3)
kηkp≤
One form of adversarial training trains on the losses `(θ; (Xi + ∆xi (θ), yi)) (Goodfellow et al.,
2015; Kurakin et al., 2016), while others perform iterated variants (Papernot et al., 2016b; Carlini
& Wagner, 2017; Madry et al., 2017; Tramer et al., 2017). Madry et al. (2017) observe that these
procedures attempt to optimize the objective EP0 [SuPkuk ≤ `(θ; Z + u)], a constrained version of
the penalty problem (2). This notion of robustness is typically intractable: the inner supremum is
generally non-concave in u, so it is unclear whether model-fitting with these techniques converges,
2
Published as a conference paper at ICLR 2018
and there are possibly worst-case perturbations these techniques do not find. Indeed, it is NP-hard
to find worst-case perturbations when deep networks use ReLU activations, suggesting difficulties
for fast and iterated heuristics (see Lemma 2 in Appendix B). Smoothness, which can be obtained
in standard deep architectures with exponential linear units (ELU’s) (Clevert et al., 2015), allows us
to find Lagrangian worst-case perturbations with low computational cost.
Distributionally robust optimization To situate the current work, we review some of the sub-
stantial body of work on robustness and learning. The choice of P in the robust objective (1) affects
both the richness of the uncertainty set we wish to consider as well as the tractability of the result-
ing optimization problem. Previous approaches to distributional robustness have considered finite-
dimensional parametrizations for P , such as constraint sets for moments, support, or directional
deviations (Chen et al., 2007; Delage & Ye, 2010; Goh & Sim, 2010), as well as non-parametric
distances for probability measures such as f -divergences (Ben-Tal et al., 2013; Bertsimas et al.,
2013; Lam & Zhou, 2015; Miyato et al., 2015; Duchi et al., 2016; Namkoong & Duchi, 2016), and
Wasserstein distances (Esfahani & Kuhn, 2015; Shafieezadeh-Abadeh et al., 2015; Blanchet et al.,
2016). In constrast to f -divergences (e.g. χ2- or Kullback-Leibler divergences) which are effective
when the support of the distribution P0 is fixed, a Wasserstein ball around P0 includes distributions
Q with different support and allows (in a sense) robustness to unseen data.
Many authors have studied tractable classes of uncertainty sets P and losses `. For example,
Ben-Tal et al. (2013) and Namkoong & Duchi (2017) use convex optimization approaches for f-
divergence balls. For worst-case regions P formed by Wasserstein balls, Esfahani & Kuhn (2015),
Shafieezadeh-Abadeh et al. (2015), and Blanchet et al. (2016) show how to convert the saddle-point
problem (1) to a regularized ERM problem, but this is possible only for a limited class of convex
losses ` and costs c. In this work, we treat a much larger class of losses and costs and provide direct
solution methods for a Lagrangian relaxation of the saddle-point problem (1). One natural appli-
cation area is in domain adaptation (Lee & Raginsky, 2017); concurrently with this work, Lee &
Raginsky provide guarantees similar to ours for the empirical minimizer of the robust saddle-point
problem (1) and give specialized bounds for domain adaptation problems. In contrast, our approach
is to use the distributionally robust approach to both defend against imperceptible adversarial per-
turbations and develop efficient optimization procedures.
2	Proposed approach
Our approach is based on the following simple insight: assume that the function z 7→ `(θ; z) is
smooth, meaning there is some L for which Rz'(θ; ∙) is L-Lipschitz. Then for any C : Z × Z →
R+ ∪ {∞} 1-strongly convex in its first argument, a Taylor expansion yields
'(θ; z0) 一 γc(z0,zo) ≤ '(θ; Z) — γc(z, Z0)+ hRz('(θ; Z) — γc(z,Z0)), z0 - Zi + L- Y I∣z 一 z0∣∣2 .
(4)
For γ ≥ L this is the first-order condition for (γ — L)-strong concavity ofZ 7→ (`(θ; Z) — γc(Z, Z0)).
Thus, whenever the loss is smooth enough in Z and the penalty γ is large enough (corresponding to
less robustness), computing the surrogate (2b) is a strongly-concave optimization problem.
We leverage the insight (4) to show that as long as we do not require too much robustness, this
strong concavity approach (4) provides a computationally efficient and principled approach for ro-
bust optimization problems (1). Our starting point is a duality result for the minimax problem (1)
and its Lagrangian relaxation for Wasserstein-based uncertainty sets, which makes the connections
between distributional robustness and the “lazy” surrogate (2b) clear. We then show (Section 2.1)
how stochastic gradient descent methods can efficiently find minimizers (in the convex case) or
approximate stationary points (when ` is non-convex) for our relaxed robust problems.
Wasserstein robustness and duality Wasserstein distances define a notion of closeness between
distributions. Let Z ⊂ Rm, and let (Z, A, P0) be a probability space. Let the transportation cost
c : Z × Z → [0, ∞) be nonnegative, lower semi-continuous, and satisfy c(Z, Z) = 0. For exam-
ple, for a differentiable convex h : Z → R, the Bregman divergence c(Z, Z0 ) = h(Z) — h(Z0 ) —
hRh(Z0), Z — Z0i satisfies these conditions. For probability measures P and Q supported on Z,
let Π(P, Q) denote their couplings, meaning measures M on Z2 with M(A, Z) = P (A) and
3
Published as a conference paper at ICLR 2018
M(Z, A) = Q(A). The Wasserstein distance between P and Q is
Wc(P, Q) :=	inf	EM [c(Z, Z0)].
M∈Π(P,Q)
For ρ ≥ 0 and distribution P0, we let P = {P : Wc(P, P0) ≤ ρ}, considering the Wasserstein
form of the robust problem (1) and its Lagrangian relaxation (2) with γ ≥ 0. The following duality
result (Blanchet & Murthy, 2016) gives the equality (2) for the relaxation and an analogous result for
the problem (1). We give an alternative proof in Appendix C.1 for convex, continuous cost functions.
Proposition 1. Let ` : Θ × Z → R and c : Z × Z → R+ be continuous. Let φγ (θ; z0) =
suPz∈z {'(θ; Z) — γc(z, zo)} be the robust surrogate (2b). For any distribution Q and any ρ > 0,
sup	EP['(θ; Z)] = inf {γρ + Eq[Φy(θ; Z)]},	(5)
PWc(P,Q)≤ρ	γ≥0
and for any γ ≥ 0, we have
SUp {Ep['(θ; Z)] - γWc(P, Q)} = Eq[φγ(θ; Z)].	(6)
P
Leveraging the insight (4), we give up the requirement that we wish a prescribed amount ρ of ro-
bustness (solving the worst-case problem (1) for P = {P : Wc(P, P0) ≤ ρ}) and focus instead on
the Lagrangian penalty problem (2) and its empirical counterpart
minimize {耳⑹:=sy {e['(Θ; Z)] - γW°(P, Pj} = EPn [φγ(θ; Z)]}.	⑺
2.1	Optimizing the robust loss by stochastic gradient descent
We now develop stochastic gradient-type methods for the relaxed robust problem (7), making clear
the computational benefits of relaxing the strict robustness requirements of formulation (5). We be-
gin with assumptions we require, which roughly quantify the amount of robustness we can provide.
Assumption A. Thefunction C : Z×Z → R+ is continuous. For each zo ∈ Z, c(∙,zo) is 1-strongly
convex with respect to the norm ∣∣∙k.
To guarantee that the robust surrogate (2b) is tractably computable, we also require a few smoothness
assumptions. Let k∙∣* be the dual norm to ∣∣∙∣∣； We abuse notation by using the same norm ∣∣∙∣ on Θ
and Z, though the specific norm is clear from context.
Assumption B. The loss ` : Θ × Z → R satisfies the Lipschitzian smoothness conditions
∣Vθ'(θ; Z)- Vθ'(θ0; z)∣* ≤	Lθθ	∣θ - θ0∣	,	∣Nz'(θ; Z)-	Vz'(θ; z0)k* ≤	Lzz IIz - z0∣	,
∣Vθ'(θ; Z)- Vθ'(θ; z0)k* ≤	Lθz	IlZ - z0k	,	∣∣Vz'(θ; Z)-	Vz'(θ0; z)∣* ≤	Lzθ ∣∣θ - θ0∣∣	.
These properties guarantee both (i) the Well-behavedness of the robust surrogate φγ and (ii) its
efficient computability. Making point (i) precise, Lemma 1 shoWs that if γ is large enough and
Assumption B holds, the surrogate φγ is still smooth. Throughout, We assume Θ ⊆ Rd .
Lemma 1. Let f : Θ×Z → R be differentiable and λ-strongly concave in Z with respect to the norm
∣∣∙∣, and define f(θ) = suPz∈z f (θ, Z). Let gθ(θ, Z) = Vθf (θ, z) and gz(θ, Z) = Vzf (θ, Z), and
assume gθ and gz satisfy Assumption B with '(θ; z) replaced with f (θ, z). Then f is differentiable,
and letting z*(θ) = argmaXz∈z f (θ, z), we have V∕(θ) = gθ(θ, z*(θ)). Moreover,
kz*(θι) - Z*(θ2)k ≤ Lλθ ∣θι - θ2∣ and ∣∣V∕(θ) - V∕(θ0)∣∣? ≤ LLθθ + ⅛Lzθ) ∣θ - θ0∣.
See Section C.2 for the proof. Fix zo ∈ Z and focus on the '2-norm case where c(z, zo) satisfies
Assumption A with ∣∣∙∣2. Noting that f (θ, z) := '(θ, z) - γc(z, zo) is (γ - Lzz)-StrOngly concave
from the insight (4) (with L := Lzz), let us apply Lemma 1. Under Assumptions A, B, φγ(∙; Zo)
then has L = Lθθ + [：#--Lipschitz gradients, and
Vθφγ(θ; Zo) = Vθ'(θ; z?(zo, θ)) where z?(zo, θ) = argmax{'(θ; z) — γc(z, zo)}.
z∈Z
4
Published as a conference paper at ICLR 2018
Algorithm 1 Distributionally robust optimization with adversarial training
INPUT: Sampling distribution P0, constraint sets Θ and Z, stepsize sequence {αt > 0}tT=-01
for t = 0, . . . , T - 1 do
Sample Zt 〜 Po and find an ^-approximate maximizer bt of '(θt; Z) 一 γc(z, Zt)
θt+1 J Projθ(θt - atVθ'(θt; ZD)
This motivates Algorithm 1, a stochastic-gradient approach for the penalty problem (7). The benefits
of Lagrangian relaxation become clear here: for `(θ; Z) smooth in Z and γ large enough, gradient
ascent on '(θt; Z)-γc(z, Zt) in Z converges linearly and We can compute (approximate) bt efficiently
(we initialize our inner gradient ascent iterations with the sampled natural example Zt).
Convergence properties of Algorithm 1 depend on the loss `. When ` is convex in θ and γ is large
enough that Z 7→ (`(θ; Z) 一 γc(Z, Z0)) is concave for all (θ, Z0) ∈ Θ × Z, We have a stochastic
monotone variational inequality, which is efficiently solvable (JUditsky et al., 2011; Chen et al.,
2014) with convergence rate 1∕√T. When the loss ' is nonconvex in θ, the following theorem
guarantees convergence to a stationary point of problem (7) at the same rate when γ ≥ Lzz . Recall
that F(θ) = EP0 [φγ (θ; Z)] is the robust surrogate objective for the Lagrangian relaxation (2).
Theorem 2 (Convergence of Nonconvex SGD). Let Assumptions A and B hold with the '2 -norm
and let Θ = Rd. Let Δf ≥ F(θ0) — infθ F(θ). Assume E[∣∣VF(θ) — Vθφγ(θ, Z)k2] ≤ σ2 and
take constant stepsizes ɑ = JL∆F72 where Lφ ：= Lθθ + Y-Lzθ. For T ≥ F, Algorithm 1
satisfies
T X E [WF (町 2i - γ⅛ e ≤ 4σ
See Section C.3 for the proof. We make a few remarks. First, the condition
E[kVF(θ) — Vθφγ(θ, Z)k2] ≤ σ2 holds (to within a constant factor) whenever ∣∣Vθ'(θ, z)∣∣2 ≤ σ
for all θ, Z . Theorem 2 shows that the stochastic gradient method achieves the rates of convergence
on the penalty problem (7) achievable in standard smooth non-convex optimization (Ghadimi &
Lan, 2013). The accuracy parameter has a fixed effect on optimization accuracy, independent of
T: approximate maximization has limited effects.
Key to the convergence guarantee of Theorem 2 is that the loss ` is smooth in Z : the inner supre-
mum (2b) is NP-hard to compute for non-smooth deep networks (see Lemma 2 in Section B
for a proof of this for ReLU’s). The smoothness of ` is essential so that a penalized version
`(θ, Z) — γc(Z, Z0) is concave in Z (which can be approximately verified by computing Hessians
V2z'(θ, Z) for each training datapoint), allowing computation and our coming certificates of opti-
mality. Replacing ReLU’s with sigmoids or ELU’s (Clevert et al., 2015) allows us to apply Theo-
rem 2, making distributionally robust optimization tractable for deep learning.
In supervised-learning scenarios, we are often interested in adversarial perturbations only to feature
vectors (and not labels). Letting Z = (X, Y ) where X denotes the feature vector (covariates) and
Y the label, this is equivalent to defining the Wasserstein cost function c : Z × Z → R+ ∪ {∞} by
c(z, Z0) := Cχ(x, x0) + ∞ ∙ 1 {y = y0}	(8)
where cx : X × X → R+ is the transportation cost for the feature vector X. All of our results
suitably generalize to this setting with minor modifications to the robust surrogate (2b) and the
above assumptions (see Section D). Similarly, our distributionally robust framework (2) is general
enough to consider adversarial perturbations to only an arbitrary subset of coordinates in Z. For
example, it may be appropriate in certain applications to hedge against adversarial perturbations to a
small fixed region ofan image (Brown et al., 2017). By suitably modifying the cost function c(Z, Z0)
to take value ∞ outside this small region, our general formulation covers such variants.
3	Certificate of robustness and generalization
From results in the previous section, Algorithm 1 provably learns to protect against adversarial per-
turbations of the form (7) on the training dataset. Now we show that such procedures generalize,
5
Published as a conference paper at ICLR 2018
allowing us to prevent attacks on the test set. Our subsequent results hold uniformly over the space of
parameters θ ∈ Θ, including θWRM, the output of the stochastic gradient descent procedure in Sec-
tion 2.1. Our first main result, presented in Section 3.1, gives a data-dependent upper bound on the
population worst-case objective SuPPWc(P,p0)≤ρ EP ['(θ; Z)] for any arbitrary level of robustness
ρ; this bound is optimal for ρ = ρbn, the level of robustness achieved for the empirical distribution
by solving (7). Our bound is efficiently computable and hence certifies a level of robustness for
the worst-case population objective. Second, we show in Section 3.2 that adversarial perturbations
on the training set (in a sense) generalize: solving the empirical penalty problem (7) guarantees a
similar level of robustness as directly solving its population counterpart (2).
3.1	Certificate of robustness
Our main result in this section is a data-dependent upper bound for the worst-case population ob-
jective: SuPPWc(P,Po)≤ρ Ep['(θ; Z)] ≤ γρ + EPJΦγ(θ; Z)] + O(1∕√n) for all θ ∈ Θ, with high
probability. To make this rigorous, fix γ > 0, and consider the worst-case perturbation, typically
called the transportation map or Monge map (Villani, 2009),
Tγ(θ; zo) := argmax{'(θ; Z) - γc(z, z0)}.	(9)
z∈Z
Under our assumptions, Tγ is easily computable when γ ≥ Lzz . Letting δz denote the point mass at
z, Proposition 1 shows the empirical maximizers of the Lagrangian formulation (6) are attained by
1n
Pn(θ) := argmax {Ep['(Θ; Z)] - γW°(P,Pn)}=n Xδτ,也名)and
P	i=1
. ,. . , . ^ . - . .-
Pn (θ) ：= Wc(P^(θ),Pn) = EPn[c(Tγ (仇 Z), Z)]∙
(10)
Our results imply, in particular, that the empirical worst-case loss EP* ['(θ; Z)] gives a certificate
of robustness to (population) Wasserstein perturbations up to level pn EP*(θ) ['(θ; Z)] is efficiently
computable via (10), providing a data-dependent guarantee for the worst-case population loss.
Our bound relies on the usual covering numbers for the model class {'(θ; ∙) : θ ∈ Θ} as the notion
of complexity (e.g. van der Vaart & Wellner, 1996), so, despite the infinite-dimensional problem (7),
we retain the same uniform convergence guarantees typical of empirical risk minimization. Recall
that for a set V, a collection vι, ∙ ∙ ∙, vn is an E-c^ver of V innorm ∣∣∙k if for each V ∈ V, there exists
Vi such that ∣v - Vik ≤ e. The covering number of V with respect to ∣∣∙∣ is
N (V, e, ∣∙∣) := inf {N ∈ N | there is an E-COVer of V with respect to ∣∣∙∣}.
For F := {'(θ, ∙) : θ ∈ Θ} equipped with the L∞(Z) norm IlfkL∞(Z) := suPz∈z |f (z)|, we state
our results in terms of ∣∙∣∣ l∞(Z)-covering numbers of F. To ease notation, we let
En(t) :
γb1
n'~^~ Z JlogN(F, M'E, k∙∣L∞(z))dE + b2M'
where b1 , b2 are numerical constants.
We are now ready to state the main result of this section. We first show from the duality result (6)
that we can provide an upper bound for the worst-case population performance for any level of
robustness ρ. For ρ = ρbn(θ) and θ = θWRM, this certificate is (in a sense) tight as we see below.
Theorem 3.	Assume ∣'(θ; z)| ≤ M' forall θ ∈ Θ and Z ∈ Z. Then,fora fixed t > 0 and numerical
constants b1, b2 > 0, with probability at least 1 - e-t, simultaneously for all θ ∈ Θ, ρ ≥ 0, γ ≥ 0,
SuP	EP [`(θ; Z)] ≤ γρ + EPb [φγ (θ; Z)] + En(t)∙	(11)
P:Wc(P,P0 )≤ρ	n
In particular, ifρ = ρbn(θ) then with probability at least 1 - e-t, for all θ ∈ Θ
SuP	EP [`(θ; Z)] ≤ γρbn(θ) + EPb [φγ(θ; Z)] + En(t)
P:Wc(P,P0 )≤ρbn (θ)	n
= SuP	EP [`(θ; Z)] + En(t)∙	(12)
.^ .	...
PWc(P,Pn)≤bn(θ)
6
Published as a conference paper at ICLR 2018
See Section C.4 for its proof. We now give a concrete variant of Theorem 3 for Lipschitz functions.
When Θ is finite-dimensional (Θ ⊂ Rd), Theorem 3 provides a robustness guarantee scaling linearly
with d despite the infinite-dimensional Wasserstein penalty. Assuming there exist θ0 ∈ Θ, Mθ0 < ∞
such that ∣'(θo; Z) | ≤ Mθ0 for all Z ∈ Z, We have the following corollary (See proof in Section C.5).
Corollary 1. Let '(∙; z) be L-Lipschitz with respect to some norm ∣∣∙k for all Z ∈ Z. Assume that
Θ ⊂ Rd satisfies diam(Θ) = supθ,θ0 ∈Θ kθ - θ0k < ∞. Then, the bounds (11) and (12) hold with
en(t) = bι∖ IdL diam(θ)+⅜ + b2(L diam(Θ) + M©。)jɪ	(13)
n	0n
for some numerical constants b1 , b2 > 0.
A key consequence of the bound (11) is that γρ + EPb [φγ (θ; Z)] certifies robustness for the worst-
case population objective for any ρ and θ. For a given θ, this certificate is tightest at the achieved
level of robustness ρbn(θ), as noted in the refined bound (12) which follows from the duality result
Ep [φγ (θ; Z)]+ γpn(θ) = sup	Ep['(θ; Z)]= Epn(θ)['(θ; Z)].	(14)
ɑ^*{z---------}	|-{}	PWc(P,Pn)≤bn(θ)
surrogate loss robustness
(See Section C.4 for a proof of these equalities.) We expect θWRM, the output of Algorithm 1, to be
close to the minimizer of the surrogate loss EPb [φγ (θ; Z)] and therefore have the best guarantees.
Most importantly, the certificate (14) is easy to compute via expression (10): as noted in Section 2.1,
the mappings T(θ, Zi) are efficiently computable for large enough γ, and ρbn = EPb [c(T (θ, Z), Z)].
The bounds (11)-(13) may be too large•一because of their dependence on covering numbers and
dimension—for practical use in security-critical applications. With that said, the strong duality
result, Proposition 1, still applies to any distribution. In particular, given a collection of test examples
Zitest , we may interrogate possible losses under perturbations for the test examples by noting that,
if Ptest denotes the empirical distribution on the test set (say, with putative assigned labels), then
1n
----E sup	{'(θ; z)} ≤ sup	Ep['(θ; Z)] ≤ γρ + EPtest [φγ(θ; Z)]	(15)
ntest i=1 z:C(Z,ZteSt)≤ρ	P=Wc(P,Ptest)≤P	θs
for all γ, ρ ≥ 0. Whenever γ is large enough (so that this is tight for small ρ), we may efficiently
compute the Monge-map (9) and the test loss (15) to guarantee bounds on the sensitivity of a pa-
rameter θ to a particular sample and predicted labeling based on the sample.
3.2 Generalization of Adversarial Examples
We can also show that the level of robustness on the training set generalizes. Our starting point is
Lemma 1, which shows that TY(∙; z) is smooth under Assumptions A and B:
kTY(θι; z) - TY(θ2; z)k ≤ [γ -LLzz]+ ∣θι - θ2k
(16)
for all θι, θ2, where we recall that Lzz is the Lipschitz constant of Nz'(θ; z). Leveraging this
smoothness, we show that ρbn(θ) = EPb [c(TY(θ; Z), Z)], the level of robustness achieved for the
empirical problem, concentrates uniformly around its population counterpart.
Theorem 4.	Let Z ⊂	{z	∈	Rm	: ∣z∣	≤	Mz} so that ∣Z∣	≤	Mz	almost surely and assume either
that (i) c(∙, ∙) is Lc-LipSchitz over Z with respect to the norm ∣∣∙∣ in each argument, or (ii) that
'(θ, z) ∈ [0, m`] and Z → '(θ, z) is YLc-LipSChitzfor all θ ∈ Θ.
If Assumptions A and B hold, then with probability at least 1 - e-t,
sup ∣Epn[c(TY(θ; Z), Z)] — Epo [c(TY(θ; Z), Z)]| ≤ 4B jɪ (t + log N (θ, [Y-LL丁+, |"|)).
(17)
where B = LcMz under assumption (i) and B = M'∕γ under assumption (ii).
7
Published as a conference paper at ICLR 2018
See Section C.6 for the proof. For Θ ⊂ Rd, We have logN(Θ, g ∣∣∙∣∣) ≤ dlog(1 + dia?(θ)) so
that the bound (30) gives the usual ,d/n generalization rate for the distance between adversarial
perturbations and natural examples. Another consequence of Theorem 4 is that ρbn (θWRM) in the
certificate (12) is positive as long as the loss ` is not completely invariant to data. To see this, note
from the optimality conditions for TY(θ; Z) that Ep。[c(Tγ(θ; Z), Z)] = 0 iff Nz'(θ; z) = 0 almost
surely, and hence for large enough n, we have ρbn(θ) > 0 by the bound (30).
4	Experiments
Our technique for distributionally robust optimization with adversarial training extends beyond su-
pervised learning. To that end, we present empirical evaluations on supervised and reinforcement
learning tasks where we compare performance with empirical risk minimization (ERM) and, where
appropriate, models trained with the fast-gradient method (3) (FGM) (Goodfellow et al., 2015), its
iterated variant (IFGM) (Kurakin et al., 2016), and the projected-gradient method (PGM) (Madry
et al., 2017). PGM augments stochastic gradient steps for the parameter θ with projected gradient
ascent over x 7→ `(θ; x, y), iterating (for data point xi, yi)
∆xt+1(θ) := argmax{Vχ'(θ; xt,yi)Tη} and xt+1 :=口B 3){x： + αt∆xt(θ)}	(18)
kηkp≤	, i
for t = 1, . . . , Tadv, where Π denotes projection onto B,p(xi) := {x : kx - xikp ≤ }.
The adversarial training literature (e.g. Goodfellow et al. (2015)) usually considers ∣∣∙k∞-norm at-
tacks, which allow imperceptible perturbations to all input features. In most scenarios, however,
it is reasonable to defend against weaker adversaries that instead perturb influential features more.
We consider this setting and train against ∣∙∣2-norm attacks. Namely, we use the squared Euclidean
cost for the feature vectors cx(x, x0) := ∣x - x0∣22 and define the overall cost as the covariate-shift
adversary (8) for WRM (Algorithm 1), and we use p = 2 for FGM, IFGM, PGM training in all
experiments; we still test against adversarial perturbations with respect to the norms p = 2, ∞. We
use Tadv = 15 iterations for all iterative methods (IFGM, PGM, and WRM) in training and attacks.
In Section 4.1, we visualize differences between our approach and ad-hoc methods to illustrate the
benefits of certified robustness. In Section 4.2 we consider a supervised learning problem for MNIST
where we adversarially perturb the test data. Finally, we consider a reinforcement learning problem
in Section 4.3, where the Markov decision process used for training differs from that for testing.
WRM enjoys the theoretical guarantees of Sections 2 and 3 for large γ, but for small γ (large adver-
sarial budgets), WRM becomes a heuristic like other methods. In Appendix A.4, we compare WRM
with other methods on attacks with large adversarial budgets. In Appendix A.5, we further compare
WRM—which is trained to defend against ∣∙∣ 2 -adversaries —with other heuristics trained to defend
against ∣∣∙k∞-adversaries. WRM matches or outperforms other heuristics against imperceptible at-
tacks, while it underperforms for attacks with large adversarial budgets.
4.1	Visualizing the benefits of certified robustness
For our first experiment, we generate synthetic data Z = (X, Y)〜Po by Xi 吧 N(02, I2) with
labels Yi = sign(∣χ∣2 — √2), where X ∈ R2 and I2 is the identity matrix in R2. Furthermore, to
create a wide margin separating the classes, we remove data with ∣∣X∣∣2 ∈ (√2∕1.3,1.3√2). We
train a small neural network with 2 hidden layers of size 4 and 2 and either all ReLU or all ELU
activations between layers, comparing our approach (WRM) with ERM and the 2-norm FGM. For
our approach we use γ = 2, and to make fair comparisons with FGM we use
= ρbn(θWRM) = Wc(Pn (θWRM), Pbn) = EPbn [c(T(θWRM, Z), Z)],	(19)
for the fast-gradient perturbation magnitude , where θWRM is the output of Algorithm 1.1
Figure 1 illustrates the classification boundaries for the three training procedures over the ReLU-
activated (Figure 1(a)) and ELU-activated (Figure 1(b)) models. Since 70% of the data are of the
1For ELU activations with scale parameter 1, γ = 2 makes problem (2b) strongly concave over the training
data. ReLU,s have no guarantees, but we use 15 gradient steps with stepsize 1∕√t for both activations.
8
Published as a conference paper at ICLR 2018
(a) ReLU model
(b) ELU model
Figure 1. Experimental results on synthetic data. Training data are shown in blue and red. Classifica-
tion boundaries are shown in yellow, purple, and green for ERM, FGM, and WRM respectively. The
boundaries are shown with the training data as well as separately with the true class boundaries.
(a) Synthetic data
(b) MNIST
Figure 2. Empirical comparison between certificate of robustness (11) (blue) and test worst-case per-
formance (red) for experiments with (a) synthetic data and (b) MNIST. We omit the certificate’s error
term n(t). The vertical bar indicates the achieved level of robustness on the training set ρbn(θWRM).
blue class (∣∣X|卜 ≤ √2∕1.3), distributional robustness favors pushing the classification boundary
outwards; intuitively, adversarial examples are most likely to come from pushing blue points out-
wards across the boundary. ERM and FGM suffer from sensitivities to various regions of the data,
as evidenced by the lack of symmetry in their classification boundaries. For both activations, WRM
pushes the classification boundaries further outwards than ERM or FGM. However, WRM with
ReLU’s still suffers from sensitivities (e.g. radial asymmetry in the classification surface) due to the
lack of robustness guarantees. WRM with ELU’s provides a certified level of robustness, yielding an
axisymmetric classification boundary that hedges against adversarial perturbations in all directions.
Recall that our certificates of robustness on the worst-case performance given in Theorem 3 applies
for any level of robustness ρ. In Figure 2(a), we plot our certificate (11) against the out-of-sample
(test) worst-case performance SuPPWc(P,p0)<ρ EP ['(θ; Z)] for WRM with ELU's. Since the worst-
case loss is hard to evaluate directly, we solve its Lagrangian relaxation (6) for different values of
γadv. For each γadv, we consider the distance to adversarial examples in the test dataset
btest(θ) := EPtest [c(Tγadv(θ,Z),Z)],	(20)
where Ptest is the test distribution, c(z, z0) := ∣∣x - x1∣2 + ∞ ∙ 1 {y = y0} as before, and
Tγadv (θ, Z) = argmaxz{'(θ; z) - Yadvc(z, Z)} is the adversarial perturbation of Z (Monge map)
for the model θ. The worst-case losses on the test dataset are then given by
EPt t [φγadv (θwRM; Z)] + Yadvbtest(θWRM)=	SuP	EP ['/wRM； Z)].
P:Wc (P,Ptest)≤ρbtest (θWRM)
As anticipated, our certificate is almost tight near the achieved level of robustness ρbn (θWRM) for
WRM (10) and provides a performance guarantee even for other values of ρ.
9
Published as a conference paper at ICLR 2018
Figure 3. PGM attacks on the MNIST dataset. (a) and (b) show test misclassification error vs. the
adversarial perturbation level adv for the PGM attack with respect to Euclidean and ∞ norms re-
spectively. The vertical bar in (a) indicates the perturbation level used for training the PGM, FGM, and
IFGM models as well as the estimated radius PP (Θwrm). For MNIST, C2 = 9.21 and C∞ = 1.00.
(b) Test error vs. “v for ∣∣ ∙ k∞ attack
4.2	Learning a more robust classifier
We now consider a standard benchmark—training a neural network classifier on the MNIST dataset.
The network consists of 8 × 8, 6 × 6, and 5 × 5 convolutional filter layers with ELU activations
followed by a fully connected layer and softmax output. We train WRM with γ = 0.04EPb [kXk2],
and for the other methods we choose as the level of robustness achieved by WRM (19).2 3 In the
figures, We scale the budgets 1∕γadv and eadv for the adversary with Cp := EP [∣∣X∣∣p].3
First, in Figure 2(b) we again illustrate the validity of our certificate of robustness (11) for the worst-
case test performance for arbitrary level of robustness ρ. We see that our certificate provides a
performance guarantee for out-of-sample worst-case performance.
We now compare adversarial training techniques. All methods achieve at least 99% test-set accuracy,
implying there is little test-time penalty for the robustness levels (e and γ) used for training. It is
thus important to distinguish the methods’ abilities to combat attacks. We test performance of the
five methods (ERM, FGM, IFGM, PGM, WRM) under PGM attacks (18) with respect to 2- and
∞-norms. In Figure 3(a) and (b), all adversarial methods outperform ERM, and WRM offers more
robustness even with respect to these PGM attacks. Training with the Euclidean cost still provides
robustness to ∞-norm fast gradient attacks. We provide further evidence in Appendix A.1.
Next we study stability of the loss surface with respect to perturbations to inputs. We note that
small values of ρbtest (θ), the distance to adversarial examples (20), correspond to small magni-
tudes of Vz'(θ; Z) in a neighborhood of the nominal input, which ensures stability of the model.
Figure 4(a) shows that ρbtest differs by orders of magnitude between the training methods (models
θ = θERM , θFGM , θIFGM , θPGM , θWRM); the trend is nearly uniform over all γadv, with θWRM
being the most stable. Thus, we see that our adversarial-training method defends against gradient-
exploiting attacks by reducing the magnitudes of gradients near the nominal input.
In Figure 4(b) we provide a qualitative picture by adversarially perturbing a single test datapoint until
the model misclassifies it. Specifically, we again consider WRM attacks and we decrease γadv until
each model misclassifies the input. The original label is 8, whereas on the adversarial examples
IFGM predicts 2, PGM predicts 0, and the other models predict 3. WRM’s “misclassifications”
appear consistently reasonable to the human eye (see Appendix A.2 for examples of other digits);
WRM defends against gradient-based exploits by learning a representation that makes gradients
point towards inputs of other classes. Together, Figures 4(a) and (b) depict our method’s defense
mechanisms to gradient-based attacks: creating a more stable loss surface by reducing the magnitude
of gradients and improving their interpretability.
2For this γ, φγ (θWRM; z) is strongly concave for 98% of the training data.
3For the standard MNIST dataset, C2 := EPb ∣X∣2 = 9.21 and C∞ := EPb ∣X∣∞ = 1.00.
10
Published as a conference paper at ICLR 2018
(a) Ptest vs. VYadV
(b) Perturbations on a test datapoint
Figure 4. Stability of the loss surface. In (a), we show the average distance of the perturbed distribution
ρbtest for a given γadV , an indicator of local stability to inputs for the decision surface. The vertical bar
in (a) indicates the γ we use for training WRM. In (b) we visualize the smallest WRM perturbation
(largest γadV) necessary to make a model misclassify a datapoint. More examples are in Appendix A.2.
4.3	Robust Markov decision processes
For our final experiments, we consider distributional robustness in the context of Q-learning, a
model-free reinforcement learning technique. We consider Markov decision processes (MDP’s)
(S , A, Psa , r) with state space S, action space A, state-action transition probabilities Psa , and re-
wards r : S → R. The goal of a reinforcement-learning agent is to maximize (discounted) cumula-
tive rewards Pt λtE[r(st)] (with discount factor λ); this is analogous to minimizing EP [`(θ; Z)] in
supervised learning. Robust MDP’s consider an ambiguity set Psa for state-action transitions. The
goal is maximizing the worst-case realization inf P ∈Psa Pt λtEP [r(st)], analogous to problem (1).
In a standard MDP, Q-learning learns a quality function Q : S × A → R via the iterations
Q(st, at) — Q(st, at) + at 卜(St) + λ max Q(st+1, a) - Q(st, at))	(21)
such that argmaxa Q(s, a) is (eventually) the optimal action in state s to maximize cumulative re-
ward. In scenarios where the underlying environment has a continuous state-space and we represent
Q with a differentiable function (e.g. Mnih et al. (2015)), we can modify the update (21) with an
adversarial state perturbation to incorporate distributional robustness. Namely, we draw the nominal
state-transition update St+1 〜Psa(St, at), and proceed with the update (21) using the perturbation
st+1 - argmin {r(s) + λmax Q(s, a) + γc(s, ^t+1)} .	(22)
For large γ, we can again solve problem (22) efficiently using gradient descent. This procedure
provides robustness to uncertainties in state-action transitions. For tabular Q-learning, where we
represent Q only over a discretized covering of the underlying state-space, we can either neglect the
second term in the update (22) and, after performing the update, round St+1 as usual, or we can per-
form minimization directly over the discretized covering. In the former case, since the update (22)
simply modifies the state-action transitions (independent of Q), standard results on convergence for
tabular Q-learning (e.g. Szepesvari & Littman (1999)) apply under these adversarial dynamics.
We test our adversarial training procedure in the cart-pole environment, where the goal is to balance
a pole on a cart by moving the cart left or right. The environment caps episode lengths to 400 steps
and ends the episode prematurely if the pole falls too far from the vertical or the cart translates too
far from its origin. We use reward r(β) := e-lβl for the angle β of the pole from the vertical. We
use a tabular representation for Q with 30 discretized states forβ and 15 for its time-derivative β (we
perform the update (22) without the Q-dependent term). The action space is binary: push the cart
left or right with a fixed force. Due to the nonstationary, policy-dependent radius for the Wasserstein
ball, an analogous for the fast-gradient method (or other variants) is not well-defined. Thus, we
only compare with an agent trained on the nominal MDP. We test both models with perturbations
to the physical parameters: we shrink/magnify the pole’s mass by 2, the pole’s length by 2, and the
strength of gravity g by 5. The system’s dynamics are such that the heavy, short, and strong-gravity
cases are more unstable than the original environment, whereas their counterparts are less unstable.
11
Published as a conference paper at ICLR 2018
Environment Regular Robust
Original ∣ 399.7 ± 0.1	400.0 ± 0.0
Easier environments
Light	400.0 ± 0.0	400.0 ± 0.0
Long	400.0 ± 0.0	400.0 ± 0.0
Soft g	400.0 士。0	400.0 ± 0.0
Harder environments
Heavy	150.1 ± 4.7	334.0 ± 3.7
Short	245.2 ± 4.8	400.0 ± 0.0
Strong g	189.8 ± 2.3	398.5 ± 0.3
Table 1. Episode length over 1000 trials
(mean ± standard error)
The environment caps episodes to 400 steps.
Table 1 shows the performance of the trained models over the original MDP and all of the perturbed
MDPs. Both models perform similarly over easier environments, but the robust model greatly out-
performs in harder environments. Interestingly, as shown in Figure 5, the robust model also learns
more efficiently than the nominal model in the original MDP. We hypothesize that a potential side-
effect of robustness is that adversarial perturbations encourage better exploration of the environment.
5 Discussion and future work
Explicit distributional robustness of the form (5) is intractable except in limited cases. We provide
a principled method for efficiently guaranteeing distributional robustness with a simple form of
adversarial data perturbation. Using only assumptions about the smoothness of the loss function
`, we prove that our method enjoys strong statistical guarantees and fast optimization rates for a
large class of problems. The NP-hardness of certifying robustness for ReLU networks, coupled with
our empirical success and theoretical certificates for smooth networks in deep learning, suggest that
using smooth networks may be preferable if we wish to guarantee robustness. Empirical evaluations
indicate that our methods are in fact robust to perturbations in the data, and they match or outperform
less-principled adversarial training techniques. The major benefit of our approach is its simplicity
and wide applicability across many models and machine-learning scenarios.
There remain many avenues for future investigation. Our optimization result (Theorem 2) applies
only for small values of robustness ρ and to a limited class of Wasserstein costs. Furthermore, our
statistical guarantees (Theorems 3 and 4) use ∣∣∙k∞-covering numbers as a measure of model com-
plexity, which can become prohibitively large for deep networks. In a learning-theoretic context,
where the goal is to provide insight into convergence behavior as well as comfort that a proce-
dure will “work” given enough data, such guarantees are satisfactory, but this may not be enough
in security-essential contexts. This problem currently persists for most learning-theoretic guaran-
tees in deep learning, and the recent works of Bartlett et al. (2017), Dziugaite & Roy (2017), and
Neyshabur et al. (2017) attempt to mitigate this shortcoming. Replacing our current covering num-
ber arguments with more intricate notions such as margin-based bounds (Bartlett et al., 2017) would
extend the scope and usefulness of our theoretical guarantees. Of course, the certificate (15) still
holds regardless.
More broadly, this work focuses on small-perturbation attacks, and our theoretical guarantees show
that it is possible to efficiently build models that provably guard against such attacks. Our method
becomes another heuristic for protection against attacks with large adversarial budgets. Indeed, in
the large-perturbation regime, efficiently training certifiably secure systems remains an important
open question. We believe that conventional ∣∣∙∣∞-defense heuristics developed for image classifi-
cation do not offer much comfort in the Iarge-PertUrbatiOn/perceptible-attack setting: ∣∙∣∞-attacks
with a large budget can render images indiscernible to human eyes, while, for example, ∣∙∣ ι -attacks
allow a concerted perturbation to critical regions of the image. Certainly ∣∣∙∣∞-attack and defense
models have been fruitful in building a foundation for security research in deep learning, but moving
beyond them may be necessary for more advances in the large-perturbation regime.
12
Published as a conference paper at ICLR 2018
Acknowledgments
We thank Jacob Steinhardt for valuable feedback. AS, HN, and JD were partially supported by
the SAIL-Toyota Center for AI Research. AS was also partially supported by a Stanford Graduate
Fellowship and a Fannie & John Hertz Foundation Fellowship. HN was partially supported by a
Samsung Fellowship. JD was partially supported by the National Science Foundation award NSF-
CAREER-1553086.
References
P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural
results. Journal ofMachine Learning Research, 3:463-482, 2002.
P. L. Bartlett, D. J. Foster, and M. J. Telgarsky. Spectrally-normalized margin bounds for neural
networks. In Advances in Neural Information Processing Systems, pp. 6241-6250, 2017.
S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. Vaughan. A theory of learning
from different domains. Machine Learning, 79:151-175, 2010.
A. Ben-Tal, L. E. Ghaoui, and A. Nemirovski. Robust Optimization. Princeton University Press,
2009.
A. Ben-Tal, D. den Hertog, A. D. Waegenaere, B. Melenberg, and G. Rennen. Robust solutions of
optimization problems affected by uncertain probabilities. Management Science, 59(2):341-357,
2013.
D. Bertsimas, V. Gupta, and N. Kallus. Data-driven robust optimization. arXiv:1401.0212
[math.OC], 2013. URL http://arxiv.org/abs/1401.0212.
P. Billingsley. Convergence of Probability Measures. Wiley, Second edition, 1999.
J. Blanchet and K. Murthy. Quantifying distributional model risk via optimal transport.
arXiv:1604.01446 [math.PR], 2016.
J. Blanchet, Y. Kang, and K. Murthy. Robust Wasserstein profile inference and applications to
machine learning. arXiv:1610.05627 [math.ST], 2016.
J. F. Bonnans and A. Shapiro. Perturbation analysis of optimization problems. Springer Science &
Business Media, 2013.
S.	Boucheron, O. Bousquet, and G. Lugosi. Theory of classification: a survey of some recent
advances. ESAIM: Probability and Statistics, 9:323-375, 2005.
S.	Boucheron, G. Lugosi, and P. Massart. Concentration Inequalities: a Nonasymptotic Theory of
Independence. Oxford University Press, 2013.
T.	Brown, D. Mane, A. Roy, M. Abadi, and J. Gilmer. Adversarial patch. In Machine Learning and
Computer Security Workshop, Neural Information Processing Systems, 2017.
N. Carlini and D. Wagner. Towards evaluating the robustness of neural networks. In Security and
Privacy (SP), 2017 IEEE Symposium on, pp. 39-57. IEEE, 2017.
X. Chen, M. Sim, and P. Sun. A robust optimization perspective on stochastic programming. Oper-
ations Research, 55(6):1058-1071, 2007.
Y. Chen, G. Lan, and Y. Ouyang. Accelerated schemes for a class of variational inequalities.
arXiv:1403.4164 [math.OC], 2014.
D.-A. Clevert, T. Unterthiner, and S. Hochreiter. Fast and accurate deep network learning by expo-
nential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.
E. Delage and Y. Ye. Distributionally robust optimization under moment uncertainty with application
to data-driven problems. Operations Research, 58(3):595-612, 2010.
J. C. Duchi, P. W. Glynn, and H. Namkoong. Statistics of robust optimization: A generalized
empirical likelihood approach. arXiv:1610.03425 [stat.ML], 2016. URL https://arxiv.
org/abs/1610.03425.
G. K. Dziugaite and D. M. Roy. Computing nonvacuous generalization bounds for deep (stochastic)
neural networks with many more parameters than training data. arXiv:1703.11008 [cs.LG], 2017.
P. M. Esfahani and D. Kuhn. Data-driven distributionally robust optimization using the Wasserstein
metric: Performance guarantees and tractable reformulations. arXiv:1505.05116 [math.OC],
2015.
S. Ghadimi and G. Lan. Stochastic first- and zeroth-order methods for nonconvex stochastic pro-
gramming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
13
Published as a conference paper at ICLR 2018
J. Goh and M. Sim. Distributionally robust optimization and its tractable approximations. Opera-
tions Research, 58(4):902-917, 2010.
I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. In
International Conference on Learning Representations, 2015.
W.	He, J. Wei, X. Chen, N. Carlini, and D. Song. Adversarial example defenses: Ensembles of weak
defenses are not strong. arXiv:1706.04701 [cs.LG], 2017.
X.	Huang, M. Kwiatkowska, S. Wang, and M. Wu. Safety verification of deep neural networks. In
International Conference on Computer Aided Verification, pp. 3-29. Springer, 2017.
A.	Juditsky, A. Nemirovski, and C. Tauvel. Solving variational inequalities with the stochastic
mirror-prox algorithm. Stochastic Systems, 1(1):17-58, 2011.
G.	Katz, C. Barrett, D. Dill, K. Julian, and M. Kochenderfer. Reluplex: An efficient SMT solver for
verifying deep neural networks. arXiv:1702.01135 [cs.AI], 2017a.
G.	Katz, C. Barrett, D. L. Dill, K. Julian, and M. J. Kochenderfer. Towards proving the adversarial
robustness of deep neural networks. arXiv:1709.02802 [cs.LG], 2017b.
J. Z. Kolter and E. Wong. Provable defenses against adversarial examples via the convex outer
adversarial polytope. arXiv:1711.00851 [cs.LG], 2017. URL https://arxiv.org/abs/
1711.00851.
A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial machine learning at scale. arXiv:1611.01236
[cs.CV], 2016.
H. Lam and E. Zhou. Quantifying input uncertainty in stochastic optimization. In Proceedings of
the 2015 Winter Simulation Conference. IEEE, 2015.
J. Lee and M. Raginsky. Minimax statistical learning and domain adaptation with Wasserstein
distances. arXiv:1705.07815 [cs.LG], 2017.
D. Luenberger. Optimization by Vector Space Methods. Wiley, 1969.
A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models
resistant to adversarial attacks. arXiv:1706.06083 [stat.ML], 2017.
T. Miyato, S.-i. Maeda, M. Koyama, K. Nakae, and S. Ishii. Distributional smoothing with virtual
adversarial training. arXiv:1507.00677 [stat.ML], 2015.
V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Ried-
miller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement
learning. Nature, 518(7540):529-533, 2015.
S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard. Deepfool: a simple and accurate method to fool
deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 2574-2582, 2016.
H. Namkoong and J. C. Duchi. Stochastic gradient methods for distributionally robust optimization
with f -divergences. In Advances in Neural Information Processing Systems 29, 2016.
H. Namkoong and J. C. Duchi. Variance regularization with convex objectives. In Advances in
Neural Information Processing Systems 30, 2017.
B.	Neyshabur, S. Bhojanapalli, D. McAllester, and N. Srebro. Exploring generalization in deep
learning. In Advances in Neural Information Processing Systems, pp. 5949-5958, 2017.
A. Nguyen, J. Yosinski, and J. Clune. Deep neural networks are easily fooled: High confidence
predictions for unrecognizable images. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 427-436, 2015.
N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and A. Swami. Practical black-box
attacks against deep learning systems using adversarial examples. arXiv:1602.02697 [cs.CR],
2016a.
N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami. The limitations of
deep learning in adversarial settings. In Security and Privacy (EuroS&P), 2016 IEEE European
Symposium on, pp. 372-387. IEEE, 2016b.
N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami. Distillation as a defense to adversarial
perturbations against deep neural networks. In Security and Privacy (SP), 2016 IEEE Symposium
on, pp. 582-597. IEEE, 2016c.
N. Parikh and S. Boyd. Proximal algorithms. Foundations and Trends in Optimization, 1(3):123-
231, 2013.
A. Raghunathan, J. Steinhardt, and P. Liang. Certified defenses against adversarial examples.
arXiv:1801.09344 [cs.LG], 2018. URL https://arxiv.org/abs/1801.09344.
14
Published as a conference paper at ICLR 2018
N. Ratliff, J. A. Bagnell, and M. Zinkevich. Maximum margin planning. In Proceedings of the 23rd
International Conference on Machine Learning, 2006.
R.	T. Rockafellar and R. J. B. Wets. Variational Analysis. Springer, New York, 1998.
A. Rozsa, M. Gunther, and T. E. Boult. Towards robust deep neural networks with bang.
arXiv:1612.00138 [cs.CV], 2016.
S.	Shafieezadeh-Abadeh, P. M. Esfahani, and D. Kuhn. Distributionally robust logistic regression.
In Advances in Neural Information Processing Systems, pp. 1576-1584, 2015.
C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing
properties of neural networks. arXiv:1312.6199 [cs.CV], 2013.
C. Szepesvari and M. L. Littman. A unified analysis of value-function-based reinforcement-learning
algorithms. Neural computation, 11(8):2017-2060, 1999.
F. Tramer, A. Kurakin, N. Papernot, D. Boneh, and P. McDaniel. Ensemble adversarial training:
Attacks and defenses. arXiv:1705.07204 [stat.ML], 2017.
A. W. van der Vaart and J. A. Wellner. Weak Convergence and Empirical Processes: With Applica-
tions to Statistics. Springer, New York, 1996.
C. Villani. Optimal Transport: Old and New. Springer, 2009.
H. Xu, C. Caramanis, and S. Mannor. Robustness and regularization of support vector machines.
The Journal of Machine Learning Research, 10:1485-1510, 2009.
H. Xu, C. Caramanis, and S. Mannor. A distributional interpretation of robust optimization. Mathe-
matics of Operations Research, 37(1):95-110, 2012.
15
Published as a conference paper at ICLR 2018
A Additional Experiments
A.1 MNIST attacks
We repeat Figure 3 using FGM (tow row of Figure 6) and IFGM (bottom row of Figure 6) attacks.
The same trends are evident as in Figure 3.
100
S io-1
曷
10-2
0	0.05	0.1	0.15	0.2	0.25
ɛadv/ɑs
(C) Test error vs. 6adv for ∣∣ ∙ k2-IFGM attack
Figure 6. Further attacks on the MNIST dataset. We illustrate test misclassification error vs. the
adversarial perturbation level adv . Top row: FGM attacks, bottom row: IFGM attacks. Left column:
Euclidean-norm attacks, right column: ∞-norm attacks. The vertical bar in (a) and (c) indicates the
perturbation level that was used for training the PGM, FGM, and IFGM models and the estimated
radius VZPn (Θwrm).
A.2 MNIST stability of loss surface
In Figure 7, we repeat the illustration in Figure 4(b) for more digits. WRM’s “misclassifications”
are consistently reasonable to the human eye, as gradient-based perturbations actually transform the
original image to other labels. Other models do not exhibit this behavior with the same consistency
(if at all). Reasonable misclassifications correspond to having learned a data representation that
makes gradients interpretable.
A.3 MNIST EXPERIMENTS WITH VARIED γ
In Figure 8, we choose a fixed WRM adversary (fixed γadv) and perturb WRM models trained with
various penalty parameters γ. As the bound (11) with η = γ suggests, even when the adversary has
more budget than that used for training (1∕γ < 1∕γadv), degradation in performance is still smooth.
Further, as we decrease the penalty γ , the amount of achieved robustness—measured here by test
error on adversarial perturbations with Yadv—has diminishing gains; this is again consistent with
our theory which says that the inner problem (2b) is not efficiently computable for small γ .
16
Published as a conference paper at ICLR 2018
(a)
IFGM	PGM	WRM
(b)
IFGM	PGM	WBM
IFGM	PGM	WRM
IFGM PGM WRM
(e)
HHEl
Original ERM FGM
ħħħ
IFGM PGM WRM
(h)
(c)
(f)
(d)
IFGM	PGM	WRM
(g)
(i)
Figure 7.	Visualizing stability over inputs. We illustrate the smallest WRM perturbation (largest γadv)
necessary to make a model misclassify a datapoint.
0	20	40	60	80
c,2∕7
(a) bn vs. 1∕γ
0	20	40	60	80
c,2∕7
(b) Test error vs. 1∕γ
Figure 8.	(a) Stability and (b) test error for a fixed adversary. We train WRM models with various
levels of γ and perturb them with a fixed WRM adversary (γadv indicated by the vertical bar).
17
Published as a conference paper at ICLR 2018
A.4 MNIST experiments with a larger adversarial budget
Figures 9 and 10 repeat Figures 2(b), 3, and 6 for a larger training adversarial budget (γ = 0.02C2)
as well as larger test adversarial budgets. The distinctions in performance between various methods
are less apparent now. For our method, the inner supremum is no longer strongly concave for over
10% of the data, indicating that we no longer have guarantees of performance. For large adversaries
(i.e. large desired robustness values) our approach becomes a heuristic just like the other approaches.
3.5
3
2.5
2
1.5
0.5
P：瓶(E-)≤p
0
0	2	4	6	8	10	12	14
Figure 9.	Empirical comparison between certificate of robustness (11) (blue) and out-of-sample (test)
worst-case performance (red) for the experiments on MNIST with a larger training adversary. The
statistical error term n(t) is omitted from the certificate. The vertical bar indicates the achieved level
of robustness on the training set ρbn(θWRM).
18
Published as a conference paper at ICLR 2018
100
10-2
0	0.1	0.2	0.3	0.4
《adv/G
(C) Test error vs. 6adv for ∣∣ ∙ k2-FGM attack
10o
10o
自 10-1
10-2
0	0.1	0.2	0.3	0.4
ɛadv/。2
(e)	Test error vs. €adv for ∣∣ ∙ 12-IFGM attack
昌 io-1
曷
10-2
0	0.1	0.2	0.3	0.4
Eadv/GOO
(f)	Test error vs.』dv for ∣∣ ∙∣∞-IFGM attack
Figure 10.	Attacks on the MNIST dataset with larger (training and test) adversarial budgets. We
illustrate test misclassification error vs. the adversarial perturbation level adv . Top row: PGM attacks,
middle row: FGM attacks, bottom row: IFGM attacks. Left column: Euclidean-norm attacks, right
column: ∞-norm attacks. The vertical bar in (a), (c), and (e) indicates the perturbation level that was
used for training the PGM, FGM, and IFGM models and the estimated radius ʌ/pbn (Θwrm ).
A.5 MNIST ∞-NORM EXPERIMENTS
We consider training FGM, IFGM, and PGM with p = ∞. We first compare with WRM trained
in the same manner as before—with the squared Euclidean cost. Then, we consider a heuristic
Lagrangian approach for training WRM with the squared ∞-norm cost.
A.5.1 Comparison with standard WRM
Our method (WRM) is trained to defend against kJb-norm attacks by using the cost function
c((x,y), (x0,y0)) = ∣∣x - xo∣∣2 + ∞ ∙ 1 {y = yo}
with the convention that 0 ∙ ∞ = 0. Standard adversarial training methods often train to defend
against ∣∣∙∣∞-norm attacks, which We compare our method against in this subsection. Direct com-
parison between these approaches is not immediate, as we need to determine a suitable to train
19
Published as a conference paper at ICLR 2018
FGM, IFGM, and PGM in the ∞-norm that corresponds to the penalty parameter Y for the kJb
norm that we use. Similar to the expression (19), we use
E := Epn[kT(Θwrm,Z)- Zk∞]	(23)
as the adversarial training budget for FGM, IFGM and PGM with ∣∣∙k∞-norms. Because 2-norm
adversaries tend to focus budgets on a subset of features, the resulting ∞-norm perturbations are
relatively large. In Figure 11 we show the results trained with a small training adversarial budget.
In this regime, (large γ, small E), WRM matches the performance of other techniques.
In Figure 12 we show the results trained with a large training adversarial budget. In this regime
(small γ, large E), performance between WRM and other methods diverge. WRM, which provably
defends against small perturbations, outperforms other heuristics against imperceptible attacks for
both Euclidean and ∞ norms. Further, it outperforms other heuristics on natural images, showing
that it consistently achieves a smaller price of robustness. On attacks with large adversarial budgets
(large Eadv), however, the performance of WRM is worse than that of the other methods (especially
in the case of ∞-norm attacks). These findings verify that WRM is a practical alternative over
existing heuristics for the moderate levels of robustness where our guarantees hold.
A.5.2 Comparison with ∣∣∙∣∞-WRM
Our computational guarantees given in Theorem 2 does not hold anymore when we consider ∞-
norm adversaries:
C((X, y),(Xo, yO)) = Ilx — xok∞ + ∞ ∙ 1 {y = yo}.	(24)
optimizing the Lagrangian formulation (2b) with the ∞-norm is difficult since subtracting a multiple
of the ∞-norm does not add (negative) curvature in all directions. in Appendix E, we propose a
heuristic algorithm for solving the inner supremum problem (2b) with the above cost function (24).
our approach is based on a variant of proximal algorithms.
We compare our proximal heuristic introduced in Appendix E with other adversarial training pro-
cedures that were trained against ∞-norm adversaries. Results are shown in Figure 13 for a small
training adversary and Figure 14 for a large training adversary. We observe that similar trends as in
Section A.5.1 hold again.
20
Published as a conference paper at ICLR 2018
Figure 11. Attacks on the MNIST dataset. We compare standard WRM with ∞-norm PGM, FGM,
IFGM. We illustrate test misclassification error vs. the adversarial perturbation level adv . Top row:
PGM attacks, middle row: FGM attacks, bottom row: IFGM attacks. Left column: Euclidean-norm
attacks, right column: ∞-norm attacks. The vertical bar in (a), (c), and (e) indicates the estimated
radius ∖Jpn (Θwrm). The vertical bar in (b), (d), and (f) indicates the perturbation level that was used
for training the PGM, FGM, and IFGM models via (23).
21
Published as a conference paper at ICLR 2018
Figure 12. Attacks on the MNIST dataset with larger (training and test) adversarial budgets. We
compare standard WRM with ∞-norm PGM, FGM, IFGM models. We illustrate test misclassification
error vs. the adversarial perturbation level adv . Top row: PGM attacks, middle row: FGM attacks,
bottom row: IFGM attacks. Left column: Euclidean-norm attacks, right column: ∞-norm attacks. The
vertical bar in (a), (c), and (e) indicates the estimated radius ʌ/pbn (Θwrm). The vertical bar in (b), (d),
and (f) indicates the perturbation level that was used for training the PGM, FGM, and IFGM models
via (23).
22
Published as a conference paper at ICLR 2018
0	0.05	0.1	0.15	0.2	0.25
⅛dv∕G2
(a) Test error vs. 6adv for k∙∣∣2-PGM attack
10or	_
.ERM
.IFGM
心FGM
APGM
■ WRM
0
0.05
0.1	0.15
^adv/ɑoo
一 ・2>|■
ERM
IFGM
FGM
PGM
WRM
0.2
(b) Test error vs.」dv for ∣∣ ∙ k∞-PGM attack
0	0.05	0.1	0.15	0.2	0.25
⅛dv∕C*2
10o
ERM
IFGM
FGM
PGM
WRM
Figure 13. Attacks on the MNIST dataset. All models are trained in the ∞-norm. We illustrate test
misclassification error vs. the adversarial perturbation level adv . Top row: PGM attacks, middle row:
FGM attacks, bottom row: IFGM attacks. Left column: Euclidean-norm attacks, right column: ∞-
norm attacks. The vertical bar in (b), (d), and (f) indicates the perturbation level that was used for
training the PGM, FGM, and IFGM models and the estimated radius ʌ/'pn (Θwrm).
0	0.05	0.1	0.15	0.2
Gdv/GOO
23
Published as a conference paper at ICLR 2018
100
10-2
APGM
■ WRM
0.4
Figure 14. Attacks on the MNIST dataset with larger (training and test) adversarial budgets. All mod-
els are trained in the ∞-norm. We illustrate test misclassification error vs. the adversarial perturbation
level adv. Top row: PGM attacks, middle row: FGM attacks, bottom row: IFGM attacks. Left column:
Euclidean-norm attacks, right column: ∞-norm attacks. The vertical bar in (b), (d), and (f) indicates
the perturbation level that was used for training the PGM, FGM, and IFGM models and the estimated
radius VZPn (Θwrm).
100
自 10-1
曷
10-2
0	0.1	0.2	0.3	0.4
Eadv/GOO
(f) Test error vs.』dv for ∣∣ ∙ k∞-IFGM attack
B Finding worst-case perturbations with ReLU’s is NP-hard
We show that computing worst-case perturbations supu∈U `(θ; z + u) is NP-hard for a large class
of feedforward neural networks with ReLU activations. This result is essentially due to Katz et al.
(2017a). In the following, we use polynomial time to mean polynomial growth with respect to m,
the dimension of the inputs z.
An optimization problem is NPO (NP-Optimization) if (i) the dimensionality of the solution grows
polynomially, (ii) the language {u ∈ U} can be recognized in polynomial time (i.e. a deterministic
algorithm can decide in polynomial time whether u ∈ U ), and (iii) ` can be evaluated in polynomial
time. We restrict analysis to feedforward neural networks with ReLU activations such that the cor-
24
Published as a conference paper at ICLR 2018
responding worst-case perturbation problem is NPO.4 Furthermore, we impose separable structure
on U, that is, U := {v ≤ u ≤ w} for some v < w ∈ Rm.
Lemma 2. Consider feedforward neural networks with ReLU’s and letU := {v ≤ u ≤ w}, where
v < w such that the optimization problem maxu∈U `(θ; z + u) is NPO. Then there exists θ such that
this optimization problem is also NP-hard.
Proof First, we introduce the decision reformulation of the problem: for some b, we ask whether
there exists some u such that `(θ; z + u) ≥ b. The decision reformulation for an NPO problem is
in NP, as a certificate for the decision problem can be verified in polynomial time. By appropriate
scaling of θ, v, and w, Katz et al. (2017a) show that 3-SAT Turing-reduces to this decision problem:
given an oracle D for the decision problem, we can solve an arbitrary instance of 3-SAT with a
polynomial number of calls to D. The decision problem is thus NP-complete.
Now, consider an oracle O for the optimization problem. The decision problem Turing-reduces
to the optimization problem, as the decision problem can be solved with one call to O. Thus, the
optimization problem is NP-hard.	□
C Proofs
C.1 Proof of Proposition 1
For completeness, we provide an alternative proof to that given in Blanchet & Murthy (2016) using
convex analysis. Our proof is less general, requiring the cost function c to be continuous and convex
in its first argument. The below general duality result gives Proposition 1 as an immediate special
case. Recalling Rockafellar & Wets (1998, Def. 14.27 and Prop. 14.33), we say that a function
g : X × Z → R is a normal integrand if for each α, the mapping
z 7→ {x | g (x, z ) ≤ α}
is closed-valued and measurable. We recall that if g is continuous, then g is a normal inte-
grand (Rockafellar & Wets, 1998, Cor. 14.34); therefore, g(x, z) = γc(x, z) - `(θ; x) is a normal
integrand. We have the following theorem.
Theorem 5. Let f, c be such that for any γ ≥ 0, the function g(x, z) = γ c(x, z) - f(x) is a normal
integrand. (For example, continuity of f and closed convexity of c is sufficient.) For any ρ > 0 we
have
sup	f (x)dP (x) = inf	sup {f (x) - γc(x, z)} dQ(z) + γρ .
P:Wc(P,Q)	γ≥0	x∈X
Proof First, the mapping P 7→ Wc (P, Q) is convex in the space of probability measures. As
taking P = Q yields Wc(Q, Q) = 0, Slater’s condition holds and we may apply standard (infinite
dimensional) duality results (Luenberger, 1969, Thm. 8.7.1) to obtain
sup	f (x)dP (x) = sup inf	f (x)dP (x) - γWc (P, Q) + γρ
P:Wc(P,Q)	P:Wc(P,Q) γ≥0
inf sup	f (x)dP (x) - γWc(P, Q) + γρ .
γ≥0 P:Wc(P,Q)
Now, noting that for any M ∈ Π(P, Q) we have fdP =	f (x)dM (x, z), we have that the
rightmost quantity in the preceding display satisfies
f (x)dP (x)
- γ inf	c(x, z)dM (x, z) = sup	[f (x) - γc(x, z)]dM (x, z) .
M∈Π(P,Q)	M∈Π(P,Q)
That is, we have
sup	f (x)dP (x) = inf sup	[f (x) - γc(x, z)]dM (x, z) + γρ .	(25)
P:Wc(P,Q)	γ≥0 P,M∈Π(P,Q)
4 Note that z, u ∈ Rm, so trivially the dimensionality of the solution grows polynomially.
25
Published as a conference paper at ICLR 2018
Now, we note a few basic facts. First, because we have a joint supremum over P and measures
M ∈ Π(P, Q) in expression (25), we have that
sup
P,M∈Π(P,Q)
[f(x)
- γc(x, z)]dM (x, z) ≤
sup[f (x) - γc(x
, z)]dQ(z).
We would like to show equality in the above. To that end, we note that if P denotes the space of
regular conditional probabilities (Markov kernels) from Z to X , then
sup
P,M∈Π(P,Q)
[f (x) - γc(x,
z)]dM (x, z) ≥ sup
P∈P
加X)-Yc(X⑶]dP(X | z)dQ(z).
Recall that a conditional distribution P(∙ | z) is regular if P(∙ | z) is a distribution for each Z
and for each measurable A, the function z 7→ P(A | z) is measurable. Let X denote the space
of all measurable mappings z 7→ X(z) from Z to X. Using the powerful measurability results of
Rockafellar & Wets (1998, Theorem 14.60), we have
sup
x∈X
[f(X(z)) - γc(X(z),
z)]dQ(z) =	sup [f (X) - γc(X, z)]dQ(z)
x∈X
because f - c is upper semi-continuous, and the latter function is measurable. Now, let X(z) be
any measurable function that is -close to attaining the supremum above. Define the conditional
distribution P(∙ | z) to be supported on χ(z), which is evidently measurable. Then using the
preceding display, we have
[f (X) - γc(X, z)]dP(X
| z)dQ(z) =
f(X(z)) - γc(X(z), z)]dQ(z)
≥	sup [f (X) - γc(X, z)]dQ(z) -
x∈X
≥
sup
P,M∈Π(P,Q)
[f (X) - γ c(X, z)]dM (X, z) -
.
As > 0 is arbitrary, this gives
sup
P,M ∈Π(P,Q)
[f(X)
- γ c(X, z)]dM (X, z) =	sup [f (X) - γc(X, z)]dQ(z)
x∈X
as desired, which implies both equality (6) and completes the proof.
□
C.2 Proof of Lemma 1
First, note that z?(θ) is unique and well-defined by the strong convexity of f (θ, ∙). For Lipschitzness
of z?(θ), We first argue that z?(θ) is continuous in θ. For any θ, optimality of z?(θ) implies that
gz(θ, z*(θ))T(z -z*(θ)) ≤ 0. By strong concavity, for any θι, θ? and z? = z*(θι) and z? = z"θ2),
we have
2 llz? - z? k2 ≤ f (θ2,z2)-f (θ2, z?) and f (θ2, z?) ≤ f (θ2, z1) + gz(θ2, z?)T (z2-z1)-/ llz? - z2k2 .
Summing these inequalities gives
λ lz1? -	z??l	≤ gz(θ?, z1?)T(z??	- z1?)	≤	(gz(θ?, z1?) -	gz(θ1,	z1?))T(z??	- z1?),
where the last inequality follows because gz(θ1, z1?)T(z?? - z1?) ≤ 0. Using a cross-Lipschitz condi-
tion from above and Holder’s inequality, we obtain
λ kz? - z?k2 ≤ kgz(θ2, z?) - gz(θι, z?)k? kz? - z?k ≤ Lzθ kθι - θ2kkz? - z?k ,
that is,
kz? - z?k ≤ -zθ kθ1 - θ2k .	(26)
λ
26
Published as a conference paper at ICLR 2018
To see the second inequality, We show that f is differentiable with ^f(θ) = gθ(θ, z*(θ)). By using
a variant of the envelope (or DanSkin's) theorem, we first show directional differentiability of f.
Recall that we say f is inf-compact if for all θ0 ∈ Θ, there exists α > 0 and a compact set C ⊂ Θ
such that
0 = {z ∈ Z : f(θ,z) ≤ α}⊂ C
for all θ in some neighborhood of θ0 (Bonnans & Shapiro, 2013). See Bonnans & Shapiro (2013,
Theorem 4.13) for a proof of the following result.
Lemma 3. Suppose that f (∙, z) is differentiable in θ for all Z ∈ Z, and f, Nz f are continuous on
Θ XZ .If f is inf-compact, then f is directionally differentiable with
f0(θ,d)= SUp Vzf(θ,z)>d
z∈S(θ)
where S(θ) = argminz f(θ, z).
Now, note that from Assumption B, we have
|f (θ,z) — f (θo, z) — Vθf (θo, z)>(θ - θo)∣ ≤ Lθθ kθ - θok
from which it is easy to see that f is inf-compact. Applying Lemma 3 to f and noting that S(θ) is
unique by strong convexity of f (θ, ∙), we have that f is directionally differentiable with Vf(θ)=
gθ(θ, z*(θ)). Since gθ is continuous by Assumption B and z?(θ) is Lipschitz (26), we conclude that
f is differentiable.
Finally, we have
kgθ(θι,z?) — gθ(θ2, z?)k? ≤ kgθ(θι,z?) — gθ(θι, z?)k? + kgθ(θι,z?) — gθ(θ2, z?)k?
≤ Lθz kz1? - z2? k + Lθθ kθ1 - θ2 k
≤ (Lθθ + LzLzθ) kθl — θ2k ,
where we have used inequality (26) again. This is the desired result.
C.3 Proof of Theorem 2
Our proof is based on that of Ghadimi & Lan (2013). For shorthand, let f(θ, z; z0) = `(θ; z) —
γc(z, z0), noting that we perform gradient steps with
gt = Vθf (θt,zt; zt)
for zbt an ^-approximate maximizer of f (θ, z; zt) in z, and θt+1 = θt — ɑtgt. We assume at ≤ 亡
in the rest of the proof, which is satisfied for the constant stepsize a = JL'Tσ? and T ≥ L^F .
By a Taylor expansion using the Lφ-smoothness of the objective F, we have
F(θt+1) ≤ F(θt) +〈VF(θt),θt+1 — θt)+ Lφ ∣∣θt+1 — θt∣∣2
=F(θt) — αt ∣∣VF(θt)∣∣2 + Lφθi ∣∣gt∣∣2 + α〈VF(θt), VF(θt) - gt)
=F(θt) — at (l — 2Lφα) ∣∣VF(θt)∣∣2	(27)
+ at (1 — Lφat)〈VF(θt), VFOgt) + 警 ∣∣gt — VF(θt)∣∣2 .
27
Published as a conference paper at ICLR 2018
Recalling the definition (2b) of φγ (θ; z0) = supz∈Z f (θ, z; z0), we define the potentially biased
errors δt = gt - Vθφγ(θt; zt). Substituting the this into the progress guarantee (27), We have
F(θt+1) ≤ F(θt) - at(1 - 1 Lφα) ∣∣VF(θt)俏 + at (1 - Lφαt) (VF(θt), VF(θt) - Vθφγ(θ; ZtN
-at (1 - Lφat) (VF(θt), δt) + ILa ∣∣Vθφγ(θ; Zt) + δt - VF(θt)∣∣2
=F(θt) - at(1 - 1 Lφa) ∣∣VF(θt) ∣∣2 + at (1 - LφaQ(VF(θt), VF(θt) - Vθφγ(θ; zt))
- at (1 - Iφat) (VF(θt), δt
+ ILa (∣∣δt∣∣2 + ∣∣Vθφγ(θ;zt) - VF(θt)∣∣2 + 2(Vθφγ(θ;zt) -VF(θt),δt>).
Using ± ha, b〉≤ 2 ∣∣ak2 + 1 ∣∣bk2 in the preceding display, we get
F(θt+1) ≤ F(θt) - a2t ∣∣VF(θt) ∣∣2 + at (1 - Lφat) (VF(θt), VF(θt) - Vθφγ(θ; zt))
+ at(I ∖Lφat) ∣∣δt∣∣2 + Lφa2 ∣Vθφγ(θ; zt) - VF(θt)∣∣2	(28)
Letting Z?t = argmaxz f (θt, Z; Zt), note that the error δt satisfies
∣∣δt∣∣2 = ∣∣Vθ φγ (θt; zt) - Vθ f(θ,b; zt)∣∣2 = ∣∣Vθ '(θ,z?) -Vθ '(θ,为 ∣∣2
2I2
≤ Lθzkb - z?k2 ≤ fz e,
λ
where the final inequality uses the λ = γ - Izz strong-concavity of Z 7→ f(θ, Z; Z0). For shorthand,
2L2	t t t
let e = Y-LzZ & Taking conditional expectations in the bound (28) and using E[Vθφγ(θt; zt) | θt]=
VF (θt), we have
E[F (θt+1) - F (θt) | θt] ≤-a2t ∣∣VF (θt)∣∣2 + at(1 +2Lφat) b + Lφa2 ∣∣Vθ φγ (θ; zt) - VF (θt)∣∣2
≤ -a2t ∣∣VF(θt)∣∣2 + atb+ Lφa2 ∣∣Vθφγ(θ; zt) - VF(θt)∣∣2 ,
where we use the fact that at ≤ 亡.For a fixed stepsize a, taking the total expectation yields
E h∣∣VF(θt)∣∣2i - 2b≤ 2E[F(θt) - F(θt+1)]+2Lφaσ2
since we have E[∣Vφγ (θ; Z) - VF(θ)∣22] ≤ σ2 by assumption. Summing over t, we have
1 T-1	2
T X E [∣∣ VF(θt)∣2] - 2b≤ -T (F(θ0) - E[F(θτ)]) + 2Lφaσ2
T t=0	aT
≤ -TF- + 2Lφaσ2,
aT
where the latter inequality holds since infθ F(θ) ≤ F(θT). Plugging in a
result.
J⅛F2 gives the
C.4 Proof of Theorem 3
We first show the bound (11). From the duality result (5), we have the deterministic result that
sup	Eq ['(θ; Z)] ≤ YP + Eq[Φy (θ; Z)]
PWc(P,Q)≤ρ
for all ρ > 0, distributions Q, and γ ≥ 0. Next, we show that EPb [φγ (θ; Z)] concentrates around its
population counterpart at the usual rate (Boucheron et al., 2005).
28
Published as a conference paper at ICLR 2018
First, we have that
φγ(θ; Z) ∈ [-M', M'],
because -M' ≤ '(θ; z) ≤ φγ(θ; z) ≤ SuPz '(θ; z) ≤ m`. Thus, the functional θ → Fn(θ)
satisfies bounded differences (Boucheron et al., 2013, Thm. 6.2), and applying standard results on
Rademacher complexity (Bartlett & Mendelson, 2002) and entropy integrals (van der Vaart & Well-
ner, 1996, Ch. 2.2) gives the result.
To see the second result (12), we substitute ρ = ρbn in the bound (11). Then, with probability at least
1 - e-t, we have
suP	EP [`(θ; Z)] ≤ γρbn(θ) + EPb [φγ(θ; Z)] + n(t).
P :Wc(P,Pθ)≤pn(θ)	n
Since we have
suP	EP [`(θ; Z)] = EPbn [φγ (θ; Z)] + γρbn(θ).
P = Wc(P,Pn)≤bn(θ)	八
from the strong duality in Proposition 1, our second result follows.
C.5 Proof of Corollary 1
The result is essentially standard (van der Vaart & Wellner, 1996), which we now give for complete-
ness. Note that for F = {'(θ; ∙) : θ ∈ Θ}, any (e, k∙k)-covering {θι,... ,Θn } of Θ guarantees that
mini ∣'(θ; z) — '(θi; z)| ≤ Le for all θ, z, or
N(F,e, k∙kL∞(Z)) ≤ N(Θ,e∕L, k∙k) ≤(1 + diam≡L[
where diam(Θ) = suPθ,θo∈θ ∣∣θ - θ0∣∣. Noting that ∣'(θ; Z)| ≤ Ldiam(Θ) + Mo =: M', we have
the result.
C.6 Proof of Theorem 4
Define
Pn*(θ) := argmax EP [`(θ; Z)] - YWc(P, Pbn) ,
P*(θ) := argmax {Ep['(Θ; Z)] - γWc(P,Po)}.
P
First, we show that P* (θ) and Pn*(θ) are attained for all θ ∈ Θ. We omit the dependency on θ for
notational simplicity and only show the result for P* (θ) as the case for Pn*(θ) is symmetric. Let P
be an e-maximizer, so that
EP e['(θ; Z)] - γWc(Pe, Po) ≥ Sup {Ep ['(θ; Z)] - YWc(Pn, P。)} - e.
P
As Z is compact, the collection {P 1∕k}k∈N is a uniformly tight collection of measures. By Pro-
horov’s theorem (Billingsley, 1999, Ch 1.1, p. 57), (restricting to a subsequence if necessary), there
exists some distribution P* on Z such that P1/k →d P* as k → ∞. Continuity properties of
Wasserstein distances (Villani, 2009, Corollary 6.11) then imply that
lim Wc(P1/k,Po) = Wc(P*,Po).
k→∞
Combining (29) and the monotone convergence theorem, we obtain
Ep* ['(θ; Z)] - YWc(P * ,Po) = lim(Epi/k ['(θ; Z)] - YWc(P 1/k ,P。))
k→∞
≥ SuP {Ep['(θ; Z)] - γWc(P, Po)}.
P
(29)
We conclude that P* is attained for all Po .
29
Published as a conference paper at ICLR 2018
Next, we show the concentration result (30). Recall the definition (9) of the transportation mapping
T(θ, z) := argmax {'(θ; z0) — γc(z0, z)},
z0∈Z
which is unique and well-defined under our strong concavity assumption that γ > Lzz , and smooth
(recall Eq. (16)) in θ. Then by Proposition 1 (or by using a variant of Kantorovich duality (Villani,
2009, Chs. 9-10)), We have
Epn(θ)['(θ; Z)= Epn['(θ;T (θ; Z))] and Ep*(θ)['(Θ; Z) = Ep0 ['(θ;T (θ; Z))]
Wc(Pn(θ),Pn)= Epn[c(T(θ; Z),Z)] and W0(P*(θ),Po)= Ep0[c(T(θ; Z),Z)].
We noW proceed by shoWing the uniform convergence of
Epn[c(T(θ; Z),Z)] to Epo[c(T(θ; Z),Z)]
under both cases (i), that c is Lipschitz, and (ii), that ` is Lipschitz in z, using a covering argument
on Θ. Recall inequality (16) (i.e. Lemma 1), Which is that
kT(θι; z) — T(Θ2; z)k ≤ I-Γ-T k& - θ2k .
[γ — Lzz]+
We have the folloWing lemma.
Lemma 4. Assume the conditions of Theorem 4. Then for any θ1 , θ2 ∈ Θ,
Ic(T(θi; z), Z)-C(T(θ2; z),z)| ≤
LcLzθ
[γ - Lzz] +
kθ1 - θ2k .
Proof In the first case, that c is Lc-Lipschitz in its first argument, this is trivial: We have
∣c(T(θι; z),z)-c(T(Θ2; z), z)I≤ LckT(θι; z) - T(Θ2; z)k ≤
LcLzθ
[γ - Lzz] +
kθ1 - θ2 k
by the smoothness inequality (16) for T.
In the second case, that z 7→ `(θ, z) is Lc-Lipschitz, let zi = T(θi; z) for shorthand. Then We have
γc(z2,z) - Yc(zi,z) = γc(z2,z) - '(Θ2,z2) + '(Θ2,z2) - Yc(zi,z)
≤ Yc(zi,z) - '(Θ2,zi) + '(Θ2,z2) - Yc(zi,z) = '(Θ2,z2) - '(Θ2,zi),
and similarly,
γc(z2,z) - Yc(zι,z) = γc(z2,z) - '(θι,zι) + '(θι,zι) - Yc(zι,z)
≥ γc(z2,z) - '(θι,zι) + '(Θ1,z2) - γc(z2,z) = '(Θ1,z2) - '(θι,zι).
Combining these tWo inequalities and using that
I'(θ,z2) - '(θ,zι)∣ ≤ YLc I∣z2 - zιk
for any θ gives the result.	□
Using Lemma 4 We obtain that θ 7→	|EPb [c(T (θ; Z), θ)] - EP0 [c(T (θ; Z), Z)]| is
2LcLzθ/ [γ - Lzz]+-Lipschitz. Let Θgver = {θι, ∙∙∙ , Θn} be a [[工用+ t-Cover of Θ with respect
to ∣∣∙∣. From Lipschitzness of |Ep [c(T(θ; Z),Z)] - Epo[c(T(θ; Z), Z)]∣, we have that if for all
θ ∈ {Θcover},
∣Epn[c(T(θ;Z),Z)] - Epo[c(T(θ;Z),θ)]∣ ≤ t,
then it follows that
sup |EPb [c(T(θ; Z), Z)] - EP0 [c(T(θ; Z), Z)]| ≤t.
θ∈Θ	n
30
Published as a conference paper at ICLR 2018
Under the first assumption (i), we have |c(T (θ; Z), Z)| ≤ 2LcMz. Applying Hoeffding’s inequality,
for any fixed θ ∈ Θ
PnEPJc(T(θ; Z), Z)] - Epo [c(T(θ; Z),Z)]| ≥ D ≤ 2exp (--n22->| .
n	2	32Lc2Mz2
Taking a union bound over θι,…，Θn, we conclude that
P(Sup ∣Epn[c(T(θ; Z),Z)] - Epo[c(T(θ; Z),Z)]| ≥ t) ≤ 2N (θ, [Y4L" t, |卜||) exp (-32^)
which was our desired result (30).
Under the second assumption (ii), we have from the definition of the transport map T
γc(T(θ; z), Z) ≤ '(θ; Z) ≤ M'
and hence |c(T(θ; Z), Z)| ≤ M'∕γ. The result for the second case follows from an identical rea-
soning.
D Supervised Learning
In supervised learning settings, it is often natural—for example, in classification—to only consider
adversarial perturbations to the feature vectors (covariates). In this section, we give an adapation of
the results in Sections 2 and 3 (Theorems 2 and 4) to such scenarios. Let Z = (X, Y ) ∈ X × R
where X ∈ X is a feature vector5 and Y ∈ R is a label. In classification settings, we have Y ∈
{1, . . . , K}. We consider an adversary that can only perturb the feature vector X (Goodfellow et al.,
2015), which can be easily represented in our robust formulation (2) by defining the cost function
c : Z × Z → R+ ∪ {∞} as follows: for Z = (x, y) and Z0 = (x0, y0), recall the covariate shift cost
function (8)
c(z,z0)：= cχ(χ, χ0) + ∞ ∙ 1 {y = y0},
where cx : X × X → R+ is the transportation cost for the feature vector X. As before, we assume
that cx is nonnegative, continuous, convex in its first argument and satisfies cx (x, x) = 0.
Under the cost function (8), the robust surrogate loss in the penalty problem (2) and its empirical
counterpart (7) becomes
Φγ(θ; (χo,yo)) = Sup {'(θ; (χ,yo)) - γcχ(χ,χo)}.
x∈X
Similarly as in Section 2.1, we require the following two assumptions that guarantee efficient com-
putability of the robust surrogate φγ .
Assumption C. The function Cx : X × X → R+ is continuous. For each xo ∈ X, cχ(∙,xo) is
1-strongly convex with respect to the norm ∣∣∙k.
Let k∙k* be the dual norm to ∣∣∙∣∣; We again abuse notation by using the same norm ∣∣∙∣ on Θ and X,
though the specific norm is clear from context.
Assumption D. The loss ` : Θ × Z → R satisfies the Lipschitzian smoothness conditions
∣∣Vθ'(θ; (x, y)) - Vθ'(θ0; (x,y))k* ≤	Lθθ ∣θ - θ0∣,	∣∣Vx'(θ; (x,	y))	- Vx'(θ; (x0, y))∣*	≤	LXX ∣x	- x0∣,
I∣Vθ'(θ; (χ,y)) - vθ'(θ; (χ0,y))k* ≤	Lθχ kx - χ0k,	∣∣vx'(θ; (x,y))	- vx'(θ0; (x,y))k*	≤	Lxθ kθ	- θ0k.
Under Assumptions C and D, an analogue of Lemma 1 still holds. The proof of the following result
is nearly identical to that of Lemma 1; we state the full result for completeness.
Lemma 5. Let f : Θ × X → R be differentiable and λ-strongly concave in x with respect to the
norm k∙k, and define /(θ) = supx∈χ f (θ, x) .Let gθ (θ, x) = Vθ f (θ, x) and gχ(θ, x) = Vxf (θ, x),
and assume gθ and gχ satisfy the Lipschitz conditions OfASSUmPtion B. Then f is differentiable, and
letting x*(θ) = argmaxx∈χ f (θ, x), we have V∕(θ) = gθ(θ, x*(θ)). Moreover,
kx?(θι)-x*(θ2)k ≤	Lλθ	kθ1-θ2k and	∣∣V∕(θ)-V∕(θ0)∣L	≤	(lθθ	+ LxLxθ) kθ - θ0k .
5We assume that X is a subset of normed vector space.
31
Published as a conference paper at ICLR 2018
From Lemma 5, our previous results (Theorems 2 and 4) follow. The following is an analogue of
Theorem 2 for the cost function (8).
Theorem 6	(Convergence of Nonconvex SGD). Let Assumptions C and D hold with the '2 -norm
and let Θ = Rd Let Δf ≥ F(θ0) - infθ F(θ). Assume E[∣∣VF(θ) -Vθφγ(θ,Z)∣∣2] ≤ σ2 and
take constant stepsizes ɑ = JL∆F72 where Lφ := Lθθ + L-Lxθ. For T ≥ L*F, Algorithm 1
satisfies
1
T
T-1
X E hVF (θt)22i -
t=0
Similarly, an analogous result to Theorem 4 holds. Define the transport map for the covariate shift
Tγ(θ; (xo,yo)) := argmax{'(θ; (x,yo)) — γcχ(x,xo)}.
x∈X
Theorem 7.	Let Z ⊂ {z ∈ Rm : kzk ≤ Mz} so that kZk ≤ Mz almost surely and assume either
that (i) Cχ(∙, ∙) is Lc-LipSchitz over X with respect to the norm ∣∣∙k in each argument, or (ii) that
'(θ, z) ∈ [0, m`] and X → '(θ, (x, y)) is YLc-LipSchitzfOr all θ ∈ Θ. IfAssumptions C and D hold,
then with probability at least 1 - e-t,
SUp ∣Epn [c(Tγ (θ; Z), Z)] —Epo [c(Tγ (θ; Z), Z )]| ≤ 4D∖ 1 (t + log N (θ, [Y — L；/+ t, ∣∙f)).
θ∈Θ	n	n	4LcLxθ
(30)
where B = LcMZ under assumption (i) and B = M'∕γ under assumption (ii).
For both results, the proofs are essentially identical as before, but with an application of Lemma 5
instead of Lemma 1.
E Proximal algorithm for ∣∣∙∣∞-norm robustness
In this section, We give a efficient training algorithm that learns to defend against ∣∙∣∞-norm Per-
turbations. For simplicity, we assume Z = Rm for the rest of this section. Let θ ∈ Θ be some fixed
model, z0 ∈ Z a natural examPle6 and define f(z) := `(θ; z) to ease notation. Concretely, We are
interested in solving the oPtimization Problem
maximizef(Z) — α ∣∣z — z01∣∞
Note that this is equivalent to computing the surrogate loss φγ(θ; z0) = supz∈z{'(θ; z)-γc(z, z0)}
for c(z, z0 ) = ∣∣z — z0 ∣∣2∞ and α = 2γ. our folloWing treatment can easily be modified for the
supervised learning scenario c((x, y), (x0, y0)) = ∣∣x — x0∣∣∞+∞∙1 {y = y0} with the convention
that ∞ ∙ 0 = 0. To make our notation consistent with the optimization literature, we consider the
minimization problem
minimize —f(z) + 2 ∣∣z — z0∣∣∞ .	(31)
A simple gradient descent algorithm applied to the problem (31) may be slow to converge in practice.
Intuitively, this is because the subgradient of z → 1 ∣∣z — z0∣∣∞ is given by ∣∣z — z0∣∣∞ ∙ S where
s is a m-dimensional vector taking values in [—1, 1] whose coordinates are non-zero only when
|zj — zo,j | = ∣∣z — z01∣ . Hence, at any given iteration of gradient descent, the ∣∣∙∣∞-norm penalty
term only gets accounted for by at most a few coordinates.
To remedy this issue, we consider a proximal algorithm for solving the problem (31) (see, for ex-
ample, Parikh & Boyd (2013) for an comprehensive review of proximal algorithms). For a function
g : Z → R and a positive number λ > 0, the proximal operator for λg is defined by
proxλg(V) := argmin 卜(Z) + 2λ kz — vk2}.
6We depart from our convention of denoting original datapoints as z0 to ease forthcoming notation.
32
Published as a conference paper at ICLR 2018
Algorithm 2 Proximal Algorithm for Maximizing f (Z) — αα ∣∣z 一 z0∣∣∞
INPUT: Stepsizes λt
for t = 0, . . . , T — 1 do
zt+ 2 - Zt + λtVf (Zt)
Vt — sort(∣ zt+ 2 — z01, dec)
Compute βt as in (33)
zt+1 —	zt+ 2	—	h∣zt+ 1	— Z0I	—	βti	sign	(zt+ 2 —	Z0
Then, the proximal algorithm on the problem (31) consists of two steps at each iteration t: (i) for
the smooth function —f (z), take a gradient descent step at the current iterate Zt (Zt+ 1 below) and
(ii) for the non-smooth function ∣∣z — z0∣∣∞, take a proximal step for the function 峥 ∣∣∙ — z0∣∣∞
at Zt+ 2 (Zt+1 below):
Zt+1 = Zt + λtVf(Zt),	Zt+1 = proxλtɑk∙-z0k2(Zt+1).	(32)
The following proposition shows that we can compute the proximal step Zt+1 efficiently, simply by
sorting the vector ∣Zt+ 1 — z0∣. We denote by vt, the sorted vector of ∣Zt+ 1 — Z01 in decreasing
order. In the proposition, We use the notation [•]+ = max(∙, 0).
Proposition 8. Define the scale parameter β t > 0 by
1	jt
βt := 1 + αλtjt Zvt where jt := max j∈ ∈
Then, Zt+1 in the proximal update (32) is given by
Zt+1 = Zt+ 2 — h∣Zt+ 2 — Z01 — βti sign(Zt+ 2 — z0
[m]: X Vi- (αλt+ (j—1)) vj < 0}.
i=1	(33)
(34)
See Section E.1 for the proof of the proposition. From the proposition, we obtain the proximal
procedure in Algorithm 2 that can be used to solve for the approximate maximizer of `(θ; Z) —
γc(Z, Z0) in Algorithm 1. Heuristically, ignoring the truncation term in the proximal update (34),
we have
Zt+1 ≈ Z0 + βt sign(Zt + λtVf(Zt) — Z0).
Here, we move towards the sign of Zt + λtVf(Zt) — Z0 modulated by the term βt, as opposed to
just the sign of Vf(Zt) for the iterated fast sign gradient method (Goodfellow et al., 2015; Kurakin
et al., 2016).
E.1 Proof of Proposition 8
In this proof, we drop the subscript on the iteration t to ease notation. We assume without loss of
generality that Zt+ 1 — z0 = 0. For some convex, lower semi-continuous function g : Rm → R, let
g*(s) = sups{s>t — g(t)} be the Fenchel conjuagte of g. From the Moreau decomposition (Parikh
& Boyd, 2013, Section 2.5), we have
proxg (W) + proxg* (W) = W
for any W ∈ Rm. Noting that the conjugate of Z → 等 ∣∣z — z0∣∣∞ is given by Z → z>z0+2∣λ l∣Zk2,
we have
prox 弩 k∙-zok∞(W) = W - PrOXhz0,∙i+2⅛ IHlI(W) = W -prox 2αλ k∙k1(W — Z0)
Let us denote the sorted vector (in decreasing order) of |W—Z0| by v. Then, in light of the preceeding
display, it suffices to show that
prox2.1λk∙k2(w — z0) = [|w — z| — β?]+ sign (W — z0)	(35)
33
Published as a conference paper at ICLR 2018
where β? is defined as in (33). To show that equality (35) holds, note that the first order optimality
conditions for
Prox 2αλ k∙k2(W - ZO) = argmin {2 Ilzk2 + α2λ Ilz- (W - ZO)Il2
is given by
kzk1 sign(zi) + αλ(zi - Wi + ziO) = 0 if |zi| 6= 0	(36a)
kzk1 [-1, 1] - αλ(Wi - ziO) 3 0 if |zi| = 0.	(36b)
Now, we use the following elementary lemma.
Lemma 6. For 0 6= v ≥ 0 with decreasing coordinates, the solution to the equation
(vi - β) = αλβ
i:vi >β
exists and is given by
β? ：= 1 + ：》j? X Vi where j? ：= max j∈ ∈ [m] ： X Vi - (1 +( - 1))Vj < 0} ∙
Proof of Lemma First, note that β 7→ i:v >β(Vi - β) - αλβ =: h(β) is decreasing. Noting
that kVk1 > 0 and -αλ kVk∞ < 0, there exists β0 such that h(β0) = 0 and β0∈	(0, kVk∞). Since
Vi ’s are decreasing and nonnegative, there exists j0 such that Vj0 > β0 ≥ Vj0+1 (we abuse notation
and let Vm+1 := 0). Then, we have
j0-1
(Vi - Vj0) - αλVj0
i=1
0
j0
< 0 ≤	(Vi - Vj0+1) - αλVj0+1.
i=1
That is, j 0 = j ? . Solving for β0 in
j?
0 = h(β0) = XVi - (αλ+j?) β0,
i=1
we obtain β0 = β? as claimed.
□
Now, define
z? = [|w - z0∣ - β?] + sign (W - z0).
Then, we have from Lemma 6 that
j?
kz?k1 =	(|Wi - ziO| - β?) =	(Vi - β?) = αλβ?.
i：|wi-z0 I>β?	i=1
If zi? > 0, then sign(zi?) = sign(Wi - ziO) so that
kz?k1 sign(zi) + αλ(zi? - Wi + ziO) = 0.
If zi? = 0, then |Wi - ziO | ≤ β? and
kz?k1 [-1, 1] - αλ(Wi - ziO) = αλβ ? [-1, 1] - αλ(Wi - ziO) 3 0.
Hence, z? satisfies the optimality condition (36) as desired.
34