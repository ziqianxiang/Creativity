Table 1: Grammar definitions and examplesGrammar	Definition	ExamplesSequence	Continue the sequence for a random length	123: 4, 45, ...
Table 2: Results of artificial grammars with different algorithms. Evaluation label Acc (%) is theaccuracy of argmax samples, AccS (%) is the accuracy of softmax samples, and Cov (%) is thecoverage of softmax samples over permissible answers. The dash (-) here indicates that the algorithmintroduced no improvements based on the pre-trained model.
Table 3: Human evaluation and BLEU score for dialogue generation. CoHS (%) is coherence humanscore. SHS (%) is sentence structure human score.
Table 4: Pseudo code of stepwise GANAlgorithm 1 StePWise GAN (StePGAN) Training1	for number of training iterations do2	for i=1, D-stePs do3	SamPle (y, xR) from real data4	Sample XG 〜PG(Jy)5	UPdate D using equation (7)6	D0(x|y) = PtT=1 αtDD(x1...t|y)7	D* =argmaxD Ey~pR(y)©R~pr(x®)[log(D0(xR∣y))]+ Ey~PR(y),xG~PG(X∣y)[log(I- DO(XGIy))]8	end for9	for i=1, G-stePs do10	SamPle y from real data11	Sample XG ~ PG(Jy)12	UPdate G using equation (9)13	Θg - Θg + naGD(xi...tIy)^log(pG(xGIy,xG...t-i))14	end for15	end forB Weighted factors search of StepGAN and StepGAN-SeqTable 5: StepGAN and StepGAN-Seq With different Weight factors. The dash (-) here notes that the
Table 5: StepGAN and StepGAN-Seq With different Weight factors. The dash (-) here notes that theWeight factors yield neither improvements nor deterioration.
Table 6: Results of artificial grammars by EBStepGAN. Please check Table 2 for other results.
Table 7: Examples of neural dialogue generation trained on OpenSubtitlesInput	hello ,i'm senator snatch .
