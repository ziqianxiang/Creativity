Table 1: The hyperparameters used during our experiments for each of the UCI datasets. Shownare the number of layers, the number of hidden neurons per layer, the architecture, and the dropoutconstant. Conic layers start with the given number of hidden units in the first layer and then decreasethe number of hidden units to the size of the output layer, according to the geometric progression.
Table 2: All datasets from the UCI Repository with over 1000 samples. For each activation function,we run the baseline network and the network with our per-class learning rate method. The bold fontsmark the best performance per each pair for matching experiments.
Table 3: Mean % of neurons with class specialization based on Wilcoxon rank- sum test (p < 10-7).
