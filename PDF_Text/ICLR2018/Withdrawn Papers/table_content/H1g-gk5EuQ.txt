Table 1: Four beam candidates for an utterance in WSJ dataset. Mistakes are annotated in red.
Table 2: Statistics of the used datasetsdataset	common-crawl	WSJ	Fisher	IWSLTtask	baseline-LM	ASR	ASR	SMT# train	ΠG	37.4K	2.1M	133.3K# dev	400K	503	1,000	1,533# test	N/A	333	4,458	1,268OOV	0%	—	0.28%	0.05%	1.07%Embedding	SamPledSoftmaX2048 2048 512	2048 512	400KFigure 3: Architecture of baseline-LM4.1	Experimental ProtocolThe experimental setup is as follows. First we train an ASR/SMT model on the training set. Then weextract B beam candidates for every training sample. This beam set, together with the correspond-ing ground-truth text, are used as the training data for LMLM and rank-LMLM. We then re-scorethe beams by linearly combining ASR/SMT and language model scores. The combination weight5Under review as a conference paper at ICLR 2018is found by optimizing the WER/BLEU on the dev set. Finally, WER/BLEU on the test set arereported.
Table 3: Scores for the four beam candidates in table 1	sentence		scores (margin)								baseline-LM		LMLM		rank-LMLM	true	user fees .	. . loans and . . .	-81.42		-86.48		-104.21	0	user fees .	. . loans and . . .	-81.42	(+0.00)	-86.48	(+0.00)	-104.21	(+0.00)1	user fee ..	. loans and . . .	-84.87	(+3.45)	-89.82	(+3.34)	-110.21	(+6.00)2	USSer fees	. . . loans end . . .	-81.58	(+0.16)	-112.38	(+25.90)	-124.54	(+20.33)3	usser fees	. . . loan end . . .	-80.34	(-1.08)	-111.47	(+24.99)	-127.43	(+23.22)Table 4: WER and CER on WSJ test set	no re-score	baseline-LM	refine-LM	interp-LM	LMLM	rank-LMLMWER	7.58	6.89	-673-	-6.80-	567	553CER	2.72	2.67	2.58	2.65	2.24	2.19Table 5: WER and CER on Fisher test set (SWBD and CallHome)	no re-score	baseline-LM	refine-LM	interp-LM	LMLM	rank-LMLMWER	-28.38-	27.57	-26.89-	-27.58-	260	2599CER	11.66	11.56	11.49	11.56	11.32	11.30Table 6: BLEU scores on IWSLT test set	no re-score	baseline-LM	refine-LM	interp-LM	LMLM	rank-LMLMBLEU	14.18	14.18	14.18	14.18	15.51	15.80More interestingly, rank-LMLM is able to assign larger score for beam 2 than beam 3, showing more
Table 4: WER and CER on WSJ test set	no re-score	baseline-LM	refine-LM	interp-LM	LMLM	rank-LMLMWER	7.58	6.89	-673-	-6.80-	567	553CER	2.72	2.67	2.58	2.65	2.24	2.19Table 5: WER and CER on Fisher test set (SWBD and CallHome)	no re-score	baseline-LM	refine-LM	interp-LM	LMLM	rank-LMLMWER	-28.38-	27.57	-26.89-	-27.58-	260	2599CER	11.66	11.56	11.49	11.56	11.32	11.30Table 6: BLEU scores on IWSLT test set	no re-score	baseline-LM	refine-LM	interp-LM	LMLM	rank-LMLMBLEU	14.18	14.18	14.18	14.18	15.51	15.80More interestingly, rank-LMLM is able to assign larger score for beam 2 than beam 3, showing moreselectivity than LMLM.
Table 5: WER and CER on Fisher test set (SWBD and CallHome)	no re-score	baseline-LM	refine-LM	interp-LM	LMLM	rank-LMLMWER	-28.38-	27.57	-26.89-	-27.58-	260	2599CER	11.66	11.56	11.49	11.56	11.32	11.30Table 6: BLEU scores on IWSLT test set	no re-score	baseline-LM	refine-LM	interp-LM	LMLM	rank-LMLMBLEU	14.18	14.18	14.18	14.18	15.51	15.80More interestingly, rank-LMLM is able to assign larger score for beam 2 than beam 3, showing moreselectivity than LMLM.
Table 6: BLEU scores on IWSLT test set	no re-score	baseline-LM	refine-LM	interp-LM	LMLM	rank-LMLMBLEU	14.18	14.18	14.18	14.18	15.51	15.80More interestingly, rank-LMLM is able to assign larger score for beam 2 than beam 3, showing moreselectivity than LMLM.
