Table 1: Capacity of different architectures. Overall, the capacity of LSTM changes gradually, butCBOW does not. CNN and LSTM with attention mechanism can effectively increase the capacityof models.
Table 2: Steps of convergence. LSTM converges more readily and faster than CBOW.
Table 3: Impact of weight decay. The effect of weight decay on model capacity is very large.
Table 4: Impact of random and short contexts. LSTM, CNN and LSTM with attention do not fail tofit the random labels. For short context, LSTM performs better.
Table 5: Impact of optimizer. Adagrad and vanilla gradient descent fail to converge for samples ofrandom labels.
Table 6: Test accuracy. Large data can increase the generalization ability over small data. A modelwith more parameters can generalize better than one with less parameters with early stop.
