Table 1: Accuracy results for approach 1Classes	Network	Incremental Training Accuracy (%)			With partial network sharing	Without partial network sharing0-9 (base)	[784 4c5 2s 4c5 2s 10o]	—	96.68A-E	[784 4c5 2s 6c5 2s 5o]	98.50	98.82F-J	[784 4c5 2s 8c5 2s 5o]	98.95	99.90K-O	[784 4c5 2s 10c5 2s 5o]	98.03	98.14P-T	[784 4c5 2s 12c5 2s 5o]	98.17	98.41U-Z	[784 4c5 2s 14c5 2s 5o]	96.57	96.76We can observe from the above table that the accuracy degradation due to ‘partial network sharing’is negligible compared to the network ‘without partial sharing’. Note that ‘without partial networksharing’ is the case when new classes are learned using all trainable parameters, none of which areshared with the already learned network parameters; In this case, the new layers are initialized usingthe model with data A (old), and then fine-tuned with data B (new) without fixing any parameters.
Table 2: Accuracy results for approach 2Classes	Network	Incremental Training Accuracy (%)			With partial network sharing	Without partial network sharing10 (all classes)	[1024 X 3 (5 X 5)128c 100fc	—	88.904 (base)	64fc (3 × 3)mp (5 × 5)128c	—	91.821	128fc 128fc (3 X 3)mp	89.60	90.53	(3 X 3) 128c 128fc 4/3/3/10o]	96.07	96.4010 (updated)		58.72	60.495Under review as a conference paper at ICLR 20183.3	Replacing Part of the Base Network with New Convolutional LayersA large DCNN usually has many convolutional layers followed by a fully connected final classi-fier. To apply our approach in a large DCNN, we implemented ResNet (He et al., 2016a;b) for areal-world object recognition application. CIFAR100 (Krizhevsky & Hinton, 2009) was used asthe benchmark dataset. We trained a base network (ResNet101) that contained 100 convolutionallayers with 50 classes out the 100 in CIFAR100. Then we added rest of the 50 classes to the existingnetwork in three installments of 10, 20 and 20. Each time we retrain the network for new classes, weclone the last convolutional layer and retrain it using new examples for the new set of classes. Thatmeans we shared 99 convolutional layers and retrained only the last convolutional layer. We com-pared the accuracies achieved by this method with the accuracy of a network of same depth, trained
Table 3: Accuracy results for approach 3Classes	Network	Incremental Training Accuracy (%)			With partial network sharing	Without partial network sharing100 (all classes)	ReSNet101 (He et al., 2016a):	—	74.2350 (base)	100 Convolution,	—	78.16^T0	100 Batch Normalization,	88.8	""87.2TG	100 ReLU, 1 average pooling,	78.3	-8T4-20	1 Output Prediction layer	85.25	-86.9100 (updated)		59.08	60.656Under review as a conference paper at ICLR 2018applied together. But for incremental learning, all training samples are not available together, henceit is not possible to get that high accuracy even without any network sharing. If we go beyond80%, classification accuracy degrades drastically. Therefore, we can conclude that for this networkstructure and application, this is the optimal configuration. Based on this observation we developedthe incremental training methodology for maximum benefits, which will be explained in the nextsub-section.
Table 4: BenchmarksApplication	Dataset	DCNN StructureCharacter Recog.	TiCH	[784 4c5 2s4c5 2s 10∕5∕6o]Object Recog.	CIFAR10	[1024 X 3 (5 X 5)128c 100fc 64fc (3 X 3)mp (5 X 5)128c 128fc 128fc (3 X 3)mp (3 X 3)128c 128fc 4/3/3o]Object Recog.	CIFAR100	ReSNet101 100 Conv., 100 Batch Normalization, 100 ReLU, 1 average pooling, 1 Output Prediction layerObject Recog.	ImageNet	ReSNet34 33 Conv., 33 Batch Normalization, 33 ReLU, 1 average pooling, 1 Output Prediction layer5	ResultsIn this section, we present results that demonstrate the accuracy obtained, the energy efficiencyand reduction in training time, storage requirements and memory access achieved by our proposeddesign.
Table 5: Accuracy results for ResNet34 trained on ImageNet#Classes	AccUracy(%) w/o sharing		AccUracy(%) w/ sharing		Top 1%	Top5%	Top 1%	Top 5%1000 (all classes)	59.91	84.13	-	-500	69.85	^^9T2	-	-300	64.34	85.05	60.88	82.42200	63.9	^^85	63.73	851000 (updated)	59.05	82.24	58.11	81.15Efficacy of incremental learning largely depends on the quality of the base network. In table 5, wecan observe that the updated network performance is very close to the regular network compared tothe performance on CIFAR10 and CIFAR100, on table 2 and 3, respectively. This is due to the factthat in the case of ImageNet, the base network had learned sufficient features since it was trainedwith large number of classes and examples. Another important fact to be noted here is that theamount of network sharing is largely dependent on the network size. Larger networks allow morelearning parameters to be shared without performance loss. We could share lot more of the learningparameters from the base network in ResNet34 compared to ResNet18. Also for CIFAR100 trainedusing ResNet101, we could share up to 80% of the learning parameters. We could conclude that ifthe network depth is increased, the sharing percentage will also increase while maintaining networkperformance.
