Table 1: The overall network parameters and accuracy before and after compression	Conv QP	FC QP	Top1-Error	Top5-Error	BPW	Compression ratioAlexNet	Γ^	1	0.4336	0.2062	32^	一	256	256	0.4402	0.2044	1.48	21.7	256	512	0.4394	0.2116	0.95	33.5	512	512	0.4426	0.2094	0.93	34.3	512	1024	0.4492	0.2196	0.51	62.0VGG-16	Γ^	1	0.3442	0.1342	32^	一	256	256	0.3500	0.1320	0.89	36.2	256	512	0.3518	0.1356	0.55	58.3	512	512	0.3630	0.1452	0.49	64.9	512	1024	0.3788	0.1566	0.31	101.7Table 2: The compression ratio per layer of the AlexNet	The proposed algorithm			The state-of-the-art algorithm	Layer	QP	BPW	Compression ratio	BPW	Compression ratioConv1	"^56"	2.24	nɪ	6.57^^	4.9Conv2	256	2.41	13.3	3.02	10.6Conv3	256	2.80	11.4	2.70	11.9Conv4	256	2.94	10.9	2.92	11.0Conv5	256	2.93	10.9	3.02	10.6
Table 2: The compression ratio per layer of the AlexNet	The proposed algorithm			The state-of-the-art algorithm	Layer	QP	BPW	Compression ratio	BPW	Compression ratioConv1	"^56"	2.24	nɪ	6.57^^	4.9Conv2	256	2.41	13.3	3.02	10.6Conv3	256	2.80	11.4	2.70	11.9Conv4	256	2.94	10.9	2.92	11.0Conv5	256	2.93	10.9	3.02	10.6FC6	""512-	0.77	413-	0.76^^	41.8FC7	512	0.98	32.6	0.79	40.7FC8	512	1.44	22.2	1.87	17.1Avg	—	0.95	33.5	0.92	34.7been compressed. The BPW in the table means the bits per weight, which is 32 for theuncompressed network. We tried different combinations of the quantization parameters forthe convolution layers and the full connection layers to obtain a better trade-off to theoverall performance.
Table 3: The compression ratio per layer of the VGG-16						The proposed algorithm			The state-of-the-art algorithm	Layer	QP	BPW	Compression ratio	BPW	Compression ratioConv1_1	^256	3.75	85^	9.59^^	3.3Conv1_2	256	2.99	10.7	2.24	14.3Conv2_1	256	2.95	10.9	2.85	11.2Conv2_2	256	2.80	11.4	2.98	10.7Conv3_1	256	2.54	12.6	3.57	9.0Conv3_2	256	2.34	13.7	1.81	17.6Conv3_3	256	2.38	13.5	2.87	11.2Conv4_1	256	2.19	14.6	2.33	13.7Conv4_2	256	2.03	15.8	1.90	16.9Conv4_3	256	2.10	15.2	2.39	13.4Conv5_1	256	2.18	14.7	2.56	12.5Conv5_2	256	2.05	15.6	2.09	15.3Conv5_3	256	1.97	16.3	2.56	12.5FC6	-512	0.27	∏96^	0.35^^	90.9FC7	512	1.73	44.0	0.40	80.0FC8	512	1.23	26.1	1.68	19.1Avg	一	0.55	58.3	0.66	48.8
Table 4: The quantization parameter and PSNR relationship of different layersLayer	QP	PSNR(dB)	QP	PSNR(dB)	QP	PSNR(dB)Conv1_1	256	46.24	-5T2^	40.20	1024	34.51Conv1_2	256	40.55	512	34.84	1024	29.36Conv2_1	256	36.80	512	30.94	1024	25.32Conv2_2	256	37.21	512	31.42	1024	25.97Conv3_1	256	34.20	512	28.43	1024	23.13Conv3_2	256	32.90	512	27.31	1024	22.21Conv3_3	256	33.64	512	28.01	1024	22.93Conv4_1	256	30.63	512	25.11	1024	20.24Conv4_2	256	29.37	512	24.04	1024	19.39Conv4_3	256	30.05	512	24.61	1024	19.91Conv5_1	256	30.45	512	24.95	1024	20.28Conv5_2	256	30.24	512	24.95	1024	20.28Conv5_3	256	30.30	512	25.08	1024	20.73FC6^	256	18.37	""512"	15.09	1024	13.42FC7	256	20.00	512	14.01	1024	9.27FC8	256	25.43	512	19.41	1024	13.42Table 5: The performance when compressing FC6 of VGG-16 individually	FC6 QP	Top1-Error	Top5-Error	BPW	Compression ratio
Table 5: The performance when compressing FC6 of VGG-16 individually	FC6 QP	Top1-Error	Top5-Error	BPW	Compression ratioVGG-16	T	0.3442	0.1342	3∑^	一	256	0.3470	0.1338	0.61	52.4	512	0.3452	0.1348	0.27	119.6	1024	0.3488	0.1348	0.13	250.8	2048	0.3814	0.1514	0.06	581.8	4096	0.9044	0.7974	0.03	1167.9As we have explained above, all the layers are not independent, the output of the currentlayer will be the input of the next layer. Therefore, compressing one layer is totally differentfrom compressing all the layers. In Table 5, we also show the case when we only compressthe FC6 layer of the VGG-16. We can obviously see that if we do not compress the otherlayers, we can compress the FC6 with 250 times or even 500 times without influencing theoverall classification accuracy. It obviously demonstrates that the accuracy of one layer willhave influence on the compression of the other layers. This can also be extended to thecombination of some layers. If we keep some of the layers uncompressed, we can compressthe other layers with higher compression ratio while maintaining the overall performance.
