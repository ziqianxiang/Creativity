title,year,conference
 Learning to learn by gradient descent by gradient descent,2016, In Advances inNeural Information Processing Systems
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Learning to optimize,2016, arXiv preprint arXiv:1606
 Hy-perband: A novel bandit-based approach to hyperparameter optimization,2016, arXiv preprintarXiv:1603
 Scaling Distributed Machine Learning with System and Algorithm Co-design,2017, PhD thesis
 Learning gradient descent: Better generalization and longerhorizons,2017, arXiv preprint arXiv:1703
 Taking thehuman out of the loop: A review of bayesian optimization,2016, Proceedings of the IEEE
 Practical bayesian optimization of machinelearning algorithms,2012, In Advances in neural information processing systems
