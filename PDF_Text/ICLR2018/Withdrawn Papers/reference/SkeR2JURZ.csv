title,year,conference
 Large-scale machine learning with stochastic gradient descent,2010, In Proc
 Concentration Inequalities and Empirical Processes Theory Applied to the Anal-ysis of Learning Algorithms,2002, PhD thesis
 The tradeoffs of large scale learning,2007, NIPS
 Convex optimization,2004, Cambridge university press
 A stochastic quasi-newtonmethod for large-scale optimization,2015, SIAM J
 LIBSVM: A library for support vector machines,2011, ACMTransactions on Intelligent Systems and Technology
 Saga: A fast incremental gradient methodwith support for non-strongly convex composite objectives,2014, NIPS
 Large scale empirical risk minimization viatruncated adaptive newton method,2017, arXiv preprint arXiv:1705
 Large scale distributedhessian-free optimization for deep neural network,2016, arXiv preprint arXiv:1606
 Accelerating stochastic gradient descent using predictive variancereduction,2013, NIPS
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Semi-stochastic gradient descent methods,2017, Frontiers in AppliedMathematics and Statistics
 Distributed inexact damped newton method: Data partitioning andload-balancing,2016, arXiv preprint arXiv:1603
 Un-derestimate sequences via quadratic averaging,2017, arXiv preprint arXiv:1710
 Adaptive newton method for empirical risk minimizationto statistical accuracy,2016, arXiv preprint arXiv:1605
 First-order adaptive sample size methods to reduce com-plexity of empirical risk minimization,2017, arXiv preprint arXiv:1709
 Sarah: A novel method for machinelearning problems using stochastic recursive gradient,2017, arXiv preprint arXiv:1703
 A stochastic gradient method with an expo-nential convergence _rate for finite training sets,2012, In Advances in Neural Information ProcessingSystems
 A stochastic quasi-newton method for onlineconvex optimization,2007, In Artificial Intelligence and Statistics
 The nature of statistical learning theory,2013, Springer
 Communication-efficient distributed optimization of self-concordantempirical loss,2015, arXiv preprint arXiv:1501
