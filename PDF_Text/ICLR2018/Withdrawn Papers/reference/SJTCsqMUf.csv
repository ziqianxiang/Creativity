title,year,conference
 Layer normalization,2016, CoRR
 Enriching word vectorswith subword information,2017, TACL
 A large anno-tated corpus for learning natural language inference,2015, In Proceedings of the 2015 Conference onEmpirical Methods in Natural Language Processing (EMNLP)
 Enhanced lstmfor natural language inference,2017, In ACL
 Named entity recognition with bidirectional LSTM-CNNs,2016, In TACL
 Advancing multi-paragraph reading comprehension,2017, arXivpreprint
 Deep reinforcement learning for mention-ranking coref-erence models,2016, In EMNLP
 Supervisedlearning of universal sentence representations from natural language inference data,2017, In EMNLP
 Semi-supervised sequence learning,2015, In NIPS
 Easy victories and uphill battles in coreference resolution,2013, In EMNLP
 A theoretically grounded application of dropout in recurrentneural networks,2016, In NIPS
 Natural language inference over interaction space,2017, CoRR
 A joint many-taskmodel: Growing a neural network for multiple nlp tasks,2017, In EMNLP
 Deep semantic role labeling: Whatworks and whatâ€™s next,2017, In ACL
 Long short-term memory,1997, Neural COmputatiOn
 Embeddings for word sensedisambiguation: An evaluation study,2016, In ACL
 An empirical exploration of recurrentnetwork architectures,2015, In ICML
 Exploring thelimits of language modeling,2016, CORR
 Character-aware neural languagemodels,2015, In AAAI 2016
 Adam: A method for stochastic optimization,2015, In ICLR
 Skip-thought vectors,2015, In NIPS
 Imagenet classification with deep convo-lutional neural networks,2012, In Advances in neural information processing systems
 Ask me anything: Dynamic memory networks fornatural language processing,2016, In ICML
 Conditional random fields: Probabilis-tic models for segmenting and labeling sequence data,2001, In ICML
 Distributed representations of sentences and documents,2014, In ICML
 End-to-end neural coreferenceresolution,2017, In EMNLP
 Finding function in form: Compositional character models for openvocabulary word representation,2015, In EMNLP
 End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF,2016, In ACL
 Building a large annotatedcorpus of english: The penn treebank,1993, Computational Linguistics
 Learned in translation:Contextualized word vectors,2017, CoRR
 context2vec: Learning generic context embed-ding with bidirectional lstm,2016, In CoNLL
 On the state of the art of evaluation in neural languagemodels,2017, CoRR
 Regularizing and optimizing lstm lan-guage models,2017, CoRR
 Distributed representa-tions of words and phrases and their compositionality,2013, In NIPS
 Usinga semantic concordance for sense identification,1994, In HLT
 Neural tree indexers for text understanding,2017, In EACL
 Efficient non-parametric estimation of multiple embeddings per word in vector space,2014, In EMNLP
 The proposition bank: An annotated corpus ofsemantic roles,2005, Computational Linguistics
 Glove: Global vectors for wordrepresentation,2014, In EMNLP
 Semi-supervisedsequence tagging with bidirectional language models,2017, In ACL
 Towards robust linguistic analysis using ontonotes,2013, InCoNLL
 Neural sequence learning modelsfor word sense disambiguation,2017, In EMNLP
 Word sense disambiguation:A unified evaluation framework and empirical comparison,2017, In EACL
 Improving sequence to sequence learning with unla-beled data,2017, In EMNLP
 Bidirectional attentionflow for machine comprehension,2017, In ICLR
 Fully convolutional networks for semanticsegmentation,2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Recursive deep models for semantic compositionality over a sentimenttreebank,2013, In EMNLP
 Training very deep networks,2015, InNIPS
 Word representations: A simple and generalmethod for semi-supervised learning,2010, In ACL
 Charagram: Embedding words andsentences via character n-grams,2016, In EMNLP
 Adadelta: An adaptive learning rate method,2012, CoRR
 Text classificationimproved by integrating bidirectional lstm with two-dimensional max pooling,2016, In COLING
3	Textual EntailmentOur baseline SNLI model is the ESIM sequence model from Chen et al,2015, (2017)
 The pretrained GloVe vectors are fine-tuned during training,2015, The final dense layer andall cells of all LSTMs are initialized to be orthogonal
