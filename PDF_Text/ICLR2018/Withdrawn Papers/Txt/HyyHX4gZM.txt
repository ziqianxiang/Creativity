Under review as a conference paper at ICLR 2018
Towards Quantum Inspired Convolution Net-
WORKS
Anonymous authors
Paper under double-blind review
Ab stract
Deep Convolution Neural Networks (CNNs), rooted by the pioneer work of
Rumelhart et al. (1986); LeCun (1985); Krizhevsky et al. (2012), and summa-
rized in LeCun et al. (2015), have been shown to be very useful in a variety of
fields. The state-of-the art CNN machines such as image rest net He et al. (2016)
are described by real value inputs and kernel convolutions followed by the local
and non-linear rectified linear outputs. Understanding the role of these layers, the
accuracy and limitations of them, as well as making them more efficient (fewer
parameters) are all ongoing research questions.
Inspired in quantum theory, we propose the use of complex value kernel functions,
followed by the local non-linear absolute (modulus) operator square. We argue
that an advantage of quantum inspired complex kernels is robustness to realistic
unpredictable scenarios (such as clutter noise, data deformations). We study a con-
crete problem of shape detection and show that when multiple overlapping shapes
are deformed and/or clutter noise is added, a convolution layer with quantum in-
spired complex kernels outperforms the statistical/classical kernel counterpart and
a ”Bayesian shape estimator” . The superior performance is due to the quantum
phenomena of interference, not present in classical CNNs.
1 Introduction
The convolution process in machine learning maybe summarized as follows. Given an input
fL-1 (x) ≥ 0 to a convolution layer L, it produces an output
gL (y) =	K(y-
x)fL-1 (x) dx
From gL(y) a local and non-linear function is applied, fL(y) = f(gL(y)), e.g., f = ReLu (rectified
linear units) or f = |.|, the magnitude operator. This output is then the input to the next convolution
layer (L+1) or simply the output of the whole process. We can also write a discrete form of these
convolutions, as itis implemented in computers. We write giL = Pj wijfjL-1, where the continuous
variables y, x becomes the integers i, j respectively, the kernel function K(y - x) → wij becomes
the weights of the CNN and the integral over dx becomes the sum over j .
These kernels are learned from data so that an error (or optimization criteria) is minimized. The ker-
nels used today a real value functions. We show how our understanding of the optimization criteria
”dictate” the construction of the quantum inspired complex value kernel. In order to concentrate
and study our proposal of quantum inspired kernels, we simplify the problem as much as possible
hoping to identify the crux of the limitation of current use of real value kernels.
We place known shapes in an image, at any location, and in the presence of deformation and clutter
noise. These shapes may have been learned by a CNN. Our main focus is on the feedforward
performance, when new inputs are presented. Due to this focus, we are able to construct a Bayesian
a posteriori probability model to the problem, which is based on real value prior and likelihood
models, and compare it to the quantum inspired kernel method.
The main advantage of the quantum inspired method over existing methods is its high resistance to
deviations from the model, such as data deformation, multiple objects (shapes) overlapping, clutter
noise. The main new factor is the quantum interference phenomenon Feynman & Hibbs (2012);
1
Under review as a conference paper at ICLR 2018
Feynman (1971), and we argue it is a desired phenomena for building convolution networks. It
can be carried out by developing complex value kernels driven by classic data driven optimization
criteria. Here we demonstrate its strength on a shape detection problem where we can compare it to
state of the art classical convolution techniques. We also can compare to the MAP estimator of the
Bayesian model for the shape detection problem.
To be clear, we do not provide (yet) a recipe on how to build kernels for the full CNN framework
for machine learning, and so the title of this paper reflects that. Here, we plant a seed on the topic
of building complex value kernels inspired in quantum theory, by demonstrating that for a given one
layer problem of shape detection (where the classic data optimization criteria is well defined), we
can build such complex value kernel and demonstrate the relevance of the interference phenomena.
To our knowledge such a demonstration is a new contribution to the field. We also speculate on how
this process can be generalized.
1.1	The Specific Problem
We are given an image I with some known objects to be detected. The data is a set of N feature
points, X = {x1, x2, ..., xN} in RD. Here we focus on 2-dimensional data, so D = 2. An image
I may be described by the set of feature points, as shown for example in figure 1. Or, the feature
points can be extracted from I, using for example, SIFT features, HOG features, or maximum of
wavelet responses (which are convolutions with complex value kernels). It maybe be the first two
or so layers of a CNN trained in image recognition tasks. The problem of object detection has been
well addressed for example by the SSD machine Liu et al. (2016), using convolution networks. Here
as we will demonstrate, given the points, we can construct a ONE layer CNN that solves the problem
of shape detection and so we focus on this formulation. It allows us to study in depth its performance
(including an analytical study notjust empirical).
12
10
8
6
4
2	4	6	8	10	12	14
(a) A deformed circle
Figure 1: (a) A circle centered at (8, 8), radius 3, and with 100 points, is deformed as follows: for
each point xi a random value drawn from a uniform distribution with range (-ηi, ηi), ηi = 0.05, is
added along the radius. (b) 1000 points of clutter are added by sampling from a uniform distribution
inside a box of size 9 × 9, with corners at points: (4,4), (4,13), (13,4), (13,13).
12-
10-
8-
6-
2	4	6	8	10	12	14
(b) Uniformly distributed noise added to (a)
1.2	Paper Organization
We organize the paper as follows. Section 2 presents a general description of shapes, which is
easily adapted to any optimization method. Section 3 presents the Bayesian method and the Hough
transform method (and a convolution implementation) to the shape detection problem. Section 4 lays
out our main proposal of using quantum theory to address the shape detection problem. The theory
also leads naturally to a classical statistical method behaving like a voting scheme, and we establish
a connection to Hough transforms. Section 5 presents a theoretical and empirical analysis of the
quantum method for shape detection and a comparison with the classical statistical method. We
demonstrate that for large deformations or clutter noise scenarios the quantum method outperforms
the classical statistical method. Section 6 concludes the paper.
2
Under review as a conference paper at ICLR 2018
2 Shape Detection
A shape S may be defined by the set of points x satisfying SΘ(x) = 0, where Θ is a set of parameters
describing S. Let μ be a shape's center (in our setting μ = (μχ, μy)). The choice of μ is in general
arbitrary, though frequently there is a natural choice for μ for a given shape, such as its “center of
mass”: the average position of its coordinates. We consider all the translations ofSΘ(x) to represent
the same shape, so that a shape is translation invariant. It is then convenient to describe the shapes
as Sθ(X - μ), with the parameters Θ not including the parameters of μ. Thus We describe a shape
by the set of points X such that
Sθ (X — μ) = 0 for all X ∈ X.
The more complex a shape is, the larger is the set of parameters required to describe it. For example,
to describe a circle, we use three parameters {μχ,μy,r} representing the center and the radius of
the circle, i.e., Sθ={r}(x — μ) = 1 - (X-仪 (see figure 1 a.) An ellipse can be described by
Sθ(x - μ) = 1 - (x - μ)τ Σ-1 (x - μ), where Θ = {Σ} is the covariance matrix, specified by
three independent parameters.
We also require that a shape representation be such that if all the values of the parameters in Θ are
0, then the the set of points that belong to the shape “collapses” to just X = {μ}. This is the case
for the parameterizations of the two examples above: the circle and the ellipse.
Energy Model: Given a shape model we can create an energy model per data point x as
Esθ (χ - μ) = ∣Sθ (χ - μ)∣p	(1)
where the parameter p ≥ 0 defines the Lp norm (after the sum over the points is taken and the 1/p
root is applied). The smaller Es& (χ - μ), the more it is likely that the data point X belongs to the
shape Sθ with center μ. In this paper, we set P = 1 because of its simplicity and robust properties.
2.1	Deformations
To address realistic scenarios we must study the detection of shapes under deformations. When
deformations are present, the energy (1) is no longer zero for deformed points associated to the
shape Sθ(χ - μ). Let each ideal shape data point XS be deformed by adding η to its coordinates,
so Xi = XiS + ηi . Then the deformed points Xi satisfy
0 = ∣Sθ(XS - μ)∣ = ∣Sθ(Xi- ηi - μ)∣
Deformations ofa shape are only observed in the directions perpendicular to the shape tangents, i.e.,
along the direction of VχSθ (x - μ)l , where Nx is the gradient operator.
xi
For example, for a (deformed) circle shape, Θ = {r} and Sr (x - μ) = 1 - (X-£ , and so VxSr (x -
μ)l	= 2 (xir-μ) 8 ri, where ^ is a unit vector pointing outwards in the radius direction at point
xi	r
Xi. Thus, η =力*.
3	Classical Methods: A Bayesian and a Hough Transform
Given a set of data points X = {xι, x2,...,xn} in RD originated from a shape Sθ (Xi - μ) = 0. We
assume that each data point is independently deformed by ηi (a random variable, since the direction
is along the shape gradient), conditional on being a shape point.
3.1	A Bayesian Approach
Based on the energy model (1), for p = 1 (for simplicity and robust properties), we can write the
likelihood model as P(X∣Θ, μ) = C QN=I e-λESθ (xi-μ) = -C QN=I e-λlSθ(xi-μ)1, where C is a
normalization constant and λ a constant that scale the errors/energies. The product over all points
3
Under review as a conference paper at ICLR 2018
is a consequence of the conditional independence of the deformations given the shape parameter
(Θ,μ). Assuming a prior distributions on the parameters to be uniform, We conclude that the a
posteriori distribution is simply the likelihood model up to a normalization, i.e.,
1N	1N
P(Θ,μ∣X) = -∩e-λESθ (Xif) = -∩e-λlSθ (Xi)I
Where Z is a normalization constant (does not depend on the parameters). The parameters that
maximize the likelihood L(Θ,μ) = logP(Θ, μ∣X) = -Z - λ PN=I ∣Sθ(Xi — μ)∣ also minimize
the total energy E(Θ,μ) = PIi=I ∣Sθ(Xi- μ)∣.
3.2	Hough Transform
A Hough transform cast binary votes from each data point. The votes are for the shape parameter
values that are consistent With the data point. More precisely, each vote is given by
v(Θ, μ∣Xi) = u ( 1 - ∣Sθ(Xi - μ)∣
α
(2)
Where u(X) is the Heaviside step function, u(X) = 1 if X ≥ 0 and zero otherWise, i.e., u = 1 if
∣Sθ (Xi 一 μ)∣ ≤ 1 and U = 0 otherwise. The parameter α clearly defines the error tolerance for
a data point Xi to belong to the shape Sθ(X — μ), the larger is α the smaller is the tolerance. One
can carry out this Hough transform for center detection as a convolution process. More precisely,
create a kernel, K H (x) centered at (0,0) and define it as K H (x) = u (1 — ∣Sθ (x)| ) for X in
a rectangular (or square) shape that includes all X for which u (1 -∣Sθ (x)| ) = 1. The Hough
transform for center detection is then the convolution of the kernel with the input image. The result
of the convolution at each location is the Hough vote for that location to be the center.
3.3 A comparison of the two methods
Figure 2: (a) A circle centered at (8, 8), radius 3, and with 100 points, is deformed with η = 0.5.
100 clutter noise points are added uniformly to a box with diagonal corners at (0, 0) and (8, 8).
(b) The Bayesian method (showing the probability value). The radius is fed to the method. The
method mixes all data yielding the highest probability in the wrong place. increasing the parameter
p can only improve a little as all data participate in the final estimation. (c) The Hough method with
α = 2.769 estimated to include all circle points that have been deformed. The method is resistant to
clutter noise.
(c) Hough method
When we have one circle with deformations (e.g., see figure 1a), the Bayesian approach is just the
”perfect” model. Even if noise distributed uniformly across the image is added (e.g., see figure 1b),
the Bayesian method will work very well. However, as one adds clutter noise to the data (noise
that is not uniform and ”may correspond to clutter” in an image) as shown in figure 2, the Bayesian
method mix all the data, has no mechanism to discard any data, and the Hough method outperforms
the Bayesian one. Even applying robust measures, decreasing p in the energy model, will have
limited effect compared to the Hough method that can discard completely the data. Consider another
4
Under review as a conference paper at ICLR 2018
scenario of two overlapping and deformed circles, shown in figure 3. Again, the Bayesian approach
does not capture the complexity of the data, two circles and not just one, and end up yielding the
best one circle fit in the ”middle”, while the Hough method cope with this data by widening the
center detection probabilities (bluring the center probabilities) and thus, including both true centers.
Still, the Hough method is not able to suggest that there are two circles/two peaks. In summary,
the Bayesian model is always the best one, as long as the data follows the exact model generation.
However, it is weak at dealing with *real world uncertainty* on the data (clutter data, multiple
figures), other scenarios that occur often. The Hough method, modeled after the same true positive
event (shape detection) is more robust to these data variations and for the center detection problem
can be carried out as a convolution.
(a) Two overlapping circles
(b) Bayesian estimation
(c) Hough method
Figure 3: (a) Two overlapping circles deformed with η = 0.5, radius r = 3 and with 100 points
each. The circle centers are at (8, 8) and (7.2, 7.2). (b) The Bayesian method (showing the proba-
bility value). The radius is fed to the method. The method mixes all data yielding the highest proba-
bility approximately in the ”middle” of both centers and no suggestion of two peaks/circles/centers
exists. (c) The Hough method with α = 2.769 estimated to include all circle points that have been
deformed. The method yields a probability that is more diluted and includes the correct centers, but
does not suggest two peaks.
4	Quantum Shape Detection and Interference
Quantum theory was developed for system of particles that evolve over time. For us to utilize here
the benefits of such a theory for the shape detection problem we invoke a hidden time parameter.
We refer to this time parameter as hidden since the input is only one static picture of a shape. A
hidden shape dynamics is not a new idea in computer vision, for example, scale space was proposed
to describe shape evolution and allows for better shapes comparisons Witkin (1983); Lindeberg
(1994). Hidden shape dynamics was also employed to describe a time evolution equation to produce
shape-skeletons Siddiqi & Kimia (1996). Since our optimization criteria per point is given by ”the
energy” of (1), we refer to classic concept of action, the one that is optimized to produce the optimal
path, as
Aμ→Θ(PT)= T ∣Sθ(X -μ)∣
where we are adopting for simplicity p = 1. The idea is that a shapes evolve from the center
μ = x(t = 0) to the shape point X = x(t = T) in a time interval T. During this evolutions all other
parameters also evolve from Θ(t = 0) = 0 to Θ(t = T) = Θ. The evolution is reversible, so we
may say equivalently, the shape point X contracts to the center μ in the interval of time T.
Following the path integral point of view of quantum theory Feynman & Hibbs (2012), we consider
the wave propagation to evolve by the integral over all path
Ψ0 (μ)=∕d PT K(PT) Ψθ (x)	(3)
where ψΘ(t) (X(t)) is the probability amplitude that characterize the state of the shape, P0T is a path
of shape contraction, from an initial state (X(0), Θ(0)) = (X, Θ) to a final state (X(T), Θ(T)) =
(μ, 0). The integral is over all possible paths that initialize in (x(0), Θ(0)) = (x, Θ) and end in
(X(T), Θ(T)) = (μ, 0). The Kernel K is of the form
5
Under review as a conference paper at ICLR 2018
K(PT ) = lei 1 AΘ→μ0(PT)
C
1 ei ~ 1SΘ (χ-μ)1
C e
(4)
where a new parameter, ~, is introduced. It has the notation used in quantum mechanics for the
reduced Planck’s constant, but here it will have its own interpretation and value (see section 5.1.3).
We now address the given image described by X = {x1, x2, ..., xN} ⊂ R2 (e.g., see figure 1). We
consider an empirical estimation of ψΘ(x) to be given by a set of impulses at the empirical data set
X, i.e., ψθ(x) = √N PN=I δ(x 一 xi), where δ(x) is the Dirac delta function. The normalization
ensure the probability 1 when integrated everywhere. Note that ψΘ(x) is a pure state, a superposition
of impulses. Then, substituting this state into equation (3), with the kernel provided by (4), yields
the evolution of the probability amplitude
N	1	1	1N
Ψθ (μ) ≈ X / dxc ei ~ lSθ (X-μ)1 √N*(x — Xi) = ~C√^ X ei ~ lSθ (Xi-*)1 .	(5)
where C = R eiT lSθ(x-μ)1 dμ. Thus shape points with deformations, Xi, are interpreted as evi-
dence of different quantum paths, not just the optimal classical path (which has no deformation).
Equation 5 is a convolution of the kernel K(x) = ei T lSθ(X)I throughout the center candidates,
except it is discretized at the locations where data is available. According to quantum theory, the
probability associated with this probability amplitude (a pure state) is given by P(Θ) = ∣ψθ(μ)∣2,
i.e.,
pθ (μ) = ψθ (μ) ψΘ (μ)
∣Sθ (χi-μ)I
e- i ~ 1SΘ(Xj-μX
which can also be expanded as
1N
Pθ(μ) = C2N X
i=1
N
1+ 2 Xcos
j>i
T
~ (ISθ(Xi - M)I- 1SΘ(Xj - M)I)
(6)
It is convenient to define the phase φj = T (∣Sθ(Xi 一 μ)∣ - ∣Sθ(Xj 一 μ)∣).
4.1	Interference
Note the interference phenomenon arising from the cosine terms in the probability (6). More pre-
cisely, a pair of data points that belongs to the shape will have a small magnitude difference,
∣φij ∣	1, and will produce a large cosine term, cos φij ≈ 1. Two different data points that be-
long to the clutter will likely produce different phases, scaled inversely according to ~, so that small
values of ~ will create larger phase difference. Pairs of clutter data points, not belonging to the
shape, with large and varying phase differences, will produce quite different cosine terms, positive
and/or negative ones. If an image contains a large amount of clutter, the clutter points will end
up canceling each other. If an image contains little clutter, the clutter points will not contribute
much. This effect can be described by the following property for large numbers: if N 1 then
PN=I PN=ι Cos(Ei - Cj) ≈ N∏ ∏∕4《 N, when each Ek is a random variable.
Figure 4 shows the performance of the quantum method on the same data as shown in figure 2a
and figure 3a. The accuracy of the detection of the centers and the identification of two centers
shows how the quantum inspired method outperforms the classical counterparts. In figure 4a, due to
interference, clutter noise cancels out (negative terms on the probability equation 6 balance positive
ones), and the center is peaked. We do see effects of the noise inducing some fluctuation. In figure
4b the two circle center peaks outperform both classical methods results as depicted in figure 3.
A more thorough analysis is carried out in the next section to better understand and compare the
performance of these different methods.
6
Under review as a conference paper at ICLR 2018
4.2	Linear-complexity Computation in the Size of the Data Set
Note that even though the probability reflects a pair-wise computation as seen in (6), we evaluate
it by taking the magnitude square of the probability amplitude (given by equation (5)), which is
computed as a sum of N complex numbers. Thus, the complexity of the computations is linear in
the data set size. After all, it is a convolution process.
(a) Quantum Probability on figure 2a
Figure 4: Quantum Probability depicted for input shown in figure 2a and figure 3a, respectively. The
parameters used where T = 1, ~ = 0.12. The quantum method outperform the classical methods, as
the center detection shown in (a) is more peaked than in the Hough method and in (b) the two peaks
emerge. These are results of the interference phenomena, as cancellation of probabilities (negative
terms on the probability equation 6) contribute to better resolve the center detection problem.
(b) Quantum Probability on figure 3a
4.3	A Classical Statistical Version of the Quantum Criterion
we derive a classical probability from the quantum probability amplitude via the Wick rotation Wick
(1954). It is a mathematical technique frequently employed in physics, which transforms quantum
physical systems into statistical physical systems and vice-versa. It consists in replacing the term i ~
by a real parameter α in the probability amplitude. Considering the probability amplitude equation
(5), the Wick rotation yields
1N
PΘ (M) = ZEe- α 1 * * * 5SΘ (xi-μ)1	(7)
Z i=1
We can interpret this as follows. Each data point Xi produces a vote v(Θ, μ∣χj = e- ɑ lSθ(xi-μ)1,
with values between 0 and 1. The parameter α controls the weight decay of the vote. Interestingly,
this probability model resembles a Hough transform with each vote (7) being approximated by the
binary vote described by (2)
5 Analysis for the Circle S hape
We analyze the quantum method described by the probability (6), derived from the amplitude (5),
and compare it with the classical statistical method described by (7) and its approximation (2). This
analysis and experiments is carried for a simple example, the circle.
We consider the circle shape, S『* (x - μ) = 1 - (Xrζμ2 of radius r* and its evaluation not only at the
true center μ* but also at small displacements from it μ = μ* + δμ where rμ < 1 with δμ = ∣δμ∣.
5.1 Quantum Paths as Deformations
The points of an original circle are deformed to create the final ”deformed” circle shape. Each point
is moved by a random vector η pointing along the radius, i.e., η = ηi^ with r* being the unit
7
Under review as a conference paper at ICLR 2018
vector along the radius. Thus, we may write each point as xi = xiC + ηi so that xiC belong to
the original circle or Sr*(x? - μ*) = 0 The deformation is assumed to vary independently and
uniformly point wise. Thus, η ∈ (-η, η) and P(ηi)=击.Plugging in the deformations and center
displacement into the shape representation, Si = Sr* (xi — μ), We get
Si =1 -	(X—字=1 -	(XC	+ ηi二∕- δ">2	= -[2(ai	+	bi) + (a2	+ b2 -	2。也)](8)
不不
where δμi = ^ ∙ δμ, ai = r⅛ and b = δμi, b = δμμ = maxi bi. We are investigating small
deformations |ai| < 1 and small displacements from the true center |bi| < 1. We interpret |Si|
as a random variable and we assumed the sampling of the circle is such that the variation of bi is
uniform. So Pb (bi)= 击= 2rμ and with the deformations uniformly distributed we also have
Pa(αi) = 2a = 2η. Finally, since the deformations are independent of the shape position, ai and b
are independent and
1	(r*)2
PS (|Si|) = Pa(ai)Pb(bi)=——=lɪ
S(IiI) a( i) b( i)	4ab	4ηδμ
For the special case of the evaluation of the shape at the true center, δμ = 0 we obtain Si =
-2rηi -(r*i)2 = -[2ai + a2]. The action for each path is given by ∣Sθ(Xi - μ)∣ and we have
multiple paths samples from the data. Note that when we apply the quantum method, we interpret
data derived from shape deformation as evidence of quantum trajectories (paths) that are not optimal
while the classical interpretation for such data is a statistical error/deformation. Both are different
probabilistic interpretations that lead to different methods of evaluations of the optimal parameters
as we analyze next.
5.1.1	Quantum Probability Amplitude
We interpret the probability amplitude in equation (5), the sum over i = 1, . . . , N, as a sum over
many independent samples. In general given a function f (IS I), then the sum over all points,
PiN=C1 f(ISiI), can be interpreted as NC times the statistical average of the function f over the
random variable Si. In this case, the random variable Si (%, δμi) represent two independent and
uniform random variables,(力，δμi), or (ai, bi).
Inserting shape equation (8) into the quantum probability amplitude of equation (5) result in
1	NC 1
Ψr*(μ + δμ) ≈C√= X ei ~ lSr* (xi-μ)1
√NCτ- /、
C Zab(C)
where Zab(e)= 志 Raa dai R'bb db ei 1 |2(ai+bi)+(a2+b2-2aibi)| and at the true center we get
ψr*(μ*) ≈ √NCZa(e), where Za(e) = ɪ faaα dai ei1 |2ai+a21. Theratio of the probabilities (mag-
nitude square of the probability amplitudes) for the circle is then given by
QC(a,b,~)
P")
P r* (μ* + δμ)
∣Zα(e)∣2
∣Zab(e)∣2
(9)
These integrals can be evaluated numerically (or via integration of a Taylor series expansions, and
then a numerical evaluation of the expansion).
5.1.2	Classical Hough Transform
Inserting shape equation (8) into the vote for the Hough transform giving by equation (2) result in
v(r*, μ* + δμ∣Xi) = u (------∣2(ai + bi) + (a2 + b2 — 2°也)|
8
Under review as a conference paper at ICLR 2018
and interpreting the Hough total vote, VrH(μ*) = PN=CI v(r*,μ*∣xi), as an average over a function
of the random variable |Si| multiplied by the number of votes, we get
VH(μ* + δμ) ≈NcIab(U)	⇒	VH(μ*) ≈ NCza(U)
where Iab(U)= 贵 J'aα dai Rbb dbi U (1 一 ∣2(ai + bi) + (a + b2 - 20也)|) and at the true cen-
ter Ia(U) = 2α J-a dai u (十 一 ∣2ai + a2∣). The ratio of the votes at the true center and the dis-
placed center is then given by
m b	VrHg	_ Ia(U)
H (a,b,α)= VH(〃* + δμ) = E	()
We now address the choice of hyper-parameters of the models, namely ~ for the quantum model and
α for the classical counterpart so that the detection of the true center is as accurate as possible.
5.1.3 Evaluation of ~ and a FOR Center Detection μ
In this section, without loss of generality, we set T = 1. In this way we can concentrate on un-
derstanding ~ role (and not T). The amplitude probability given by equation (5) has a parameter ~
where the inverse of ~ scales up the magnitude of the shape values. The smaller is ~, the more the
phase φi = ~ |S8 (Xi 一 μ)∣ reaches any point in the unit circle. A large ~ can make the phase φi
very small and a small ~ can send each shape point to any point in the unit circle .
The parameter ~ large can help in aligning shape points to similar phases. That suggests ~ as large
as possible. At the same time, ~ should help in misaligning pair of points where at least one of
them does not belong to the shape. That suggests small values of ~. Similarly, if one is evaluating a
shape with the ”wrong” set of parameters, we woulud want the shape points to cancel each other. in
our example of the circle, we would like that shape points evaluated at a center displacement from
the true center to yield some cancellation. That suggests ~ small. one can explore the parameter
~ that maximize the ratio QC (a, b, ~) given by equation (9). We can also attempt to analytically
balance both requests (high amplitude at the true center and low amplitude at the displaced center)
by choosing values of ~ such that ψi = ~|Sr* (xi 一 μ*)∣ ≤ π ∀i = 1,..., NC. More precisely, by
choosing
~ = 1 max |Sr* (xi - μ*)∣ ≈ 2a + a
πi	π
(11)
Figure 5 suggests this choice of ~ gives high ratios for Q(a, b, ~).
Now we discuss the estimation of α. we note that for αα > 2a + a2 = 2rη* + (r*)2 all shape points
will vote for the true center. Thus, choosing α so that largest value of its inverse is 1 = 2a + a2
will guarantee all votes and give a lower vote for shape points evaluated from the displaced center.
one could search for higher values of α, smaller inverse values, so that reducing votes at the true
center and expecting to reduce them further at the displaced center, i.e., to maximize H(a, b, α) in
equation (10). Figure 5 suggests such changes do not improve the Hough transform performance.
Figure 5 demonstrates that the quantum method outperforms the classical Hough transform on ac-
curacy detection.
We can also perform a similar analysis adding noise, to obtain similar results. This will require
another two pages (a page of analysis and a page of graphs), and if the conference permits, we will
be happy to add.
6 Conclusions
Deep Convolution Neural Networks (CNNs), rooted on the pioneer work of Rumelhart et al. (1986);
LeCun (1985); Krizhevsky et al. (2012), and summarized in LeCun et al. (2015), have been shown
to be very useful in a variety of fields.
inspired in quantum theory, we investigated the use of complex value kernel functions, followed
by the local non-linear absolute (modulus) operator square. We studied a concrete problem of
9
Under review as a conference paper at ICLR 2018
Figure 5: Quantum method vs classical Hough transform for accuracy of the detection of the center.
For all figures We fixed the radius r* = 3 and deformations η = 0.5, thus, a = 1. For each of the
figures 5a, 5b,5c we vary we vary b = 11 a, a, 2a (or center displacements δμ = 0.25,0.5,1), respec-
tively. These figures depict ratios Q(a, b, ~) X ~ (blue) for ~ ∈ (0.047,0.2802) and H(a, b,α) × t-
(red) for I- ∈ (22.727, 2.769) (The reverse arrow implies the x-axis start at the maximum value
and decreases thereafter). All plots have 200 points, with uniform steps in their respective range.
Note that our proposed parameter value is ~ = 0.1401, the solution to equation (11), and indeed
gives a high ratio. Also, α = 2.769 is the smallest value to yield all Hough votes in the cen-
ter. Clearly the quantum ratio outperforms the best classical Hough method, which does not vary
much across α values. As the center displacement increases, the quantum method probability, for
~ = 0.1401, decreases much faster than the Hough method probability. Final figure 5d display
values of ∣ψ∣2(μ*) × ~ (atthe true center) in blue, for ~ ∈ (0.047,0.2802), with 200 uniform steps.
In red, V(μ*) × t- for(- ∈ (22.727, 2.769), with 200 uniform steps.
shape detection and showed that when multiple overlapping shapes are deformed and/or clutter
noise is added, a convolution layer with quantum inspired complex kernels outperforms the statis-
tical/classical kernel counterpart and a ”Bayesian shape estimator”. It is worth to mention that the
Bayesian shape estimator is the best method as long as the data satisfy the model assumptions. Once
we add multiple shapes, or add clutter noise (not uniform noise), the Bayesian method breaks down
rather easily, but not the quantum method nor the statistical version of it (the Hough method being
an approximation to it). An analysis comparing the Quantum method to the Hough method was
carried out to demonstrate the superior accuracy performance of the quantum method, due to the
quantum phenomena of interference, not present in the classical CNN.
We have not focused on the problem of learning the shapes here. Given the proposed quantum kernel
method, the standard techniques of gradient descent method should also work to learn the kernels,
since complex value kernels are also continuous and differentiable. Each layer of the networks
carries twice as many parameters, since complex numbers are a compact notation for two numbers,
10
Under review as a conference paper at ICLR 2018
but the trust of the work is to suggest that they may perform better and reduce the size of the
entire network. These are just speculations and more investigation of the details that entice such a
construction are needed. Note that many articles in the past have mentioned ”quantum” and ”neural
networks" together. Several of them use Schrodinger equation, a quantum physics modeling of the
world. Here in no point we visited a concept in physics (forces, energies), as Schrodinger equation
would imply, the only model is the one of shapes (computer vision model). Quantum theory is here
used as an alternative statistical method, a purely mathematical construction that can be applied to
different models and fields, as long as it brings benefits. Also, in our search, we did not find an article
that explores the phenomena of interference and demonstrate its advantage in neural networks. The
task of brining quantum ideas to this field must require demonstrations of its utility, and we think
we did that here.
References
R. Feynman. The Feynman Lectures on Physics, volume 3. Addison Wesley, 1971.
R. Feynman and A. Hibbs. Quantum Mechanics and Path Integrals: Emended by D. F. Steyer. Dover
Publications, 2012.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (eds.),
Advances in Neural Information Processing Systems 25, pp. 1097-1105. 2012.
Y. LeCun. A learning scheme for assymetric threshold network, 1985.
Y. LeCun, Y. Bengio, and G. Hinton. Deep learning, 2015.
T. Lindeberg. Scale-space theory: A basic tool for analysing structures at different scales. Journal
of Applied Statistics (Supplement on Advances in Applied Statistics: Statistics and Images: 2), 2
(21):224270, 1994.
W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C. Fu, and A. Berg. SSD: Single Shot MultiBox
Detector, pp. 21-37. Springer International Publishing, 2016.
D. Rumelhart, G. Hinton, and R. Williams. Learning representations by back-propagating error,
1986.
K. Siddiqi and B. B. Kimia. A shock grammar for recognition. Computer Vision and Pattern
Recognition, 1996. Proceedings CVPR ’96, 1996.
G. C. Wick. Properties of Bethe-Salpeter wave functions. Phys. Rev., 96:1124-1134, Nov 1954.
A. P. Witkin. Scale-space filtering. Proc. 8th Int. Joint Conf. Art. Intell., pp. 10191022, 1983.
11