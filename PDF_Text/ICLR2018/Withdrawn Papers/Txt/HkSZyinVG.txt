Under review as a conference paper at ICLR 2018
Improved Learning in Convolutional Neural
Networks with Shifted Exponential Linear
UNITS (SHELUS)
Anonymous authors
Paper under double-blind review
Ab stract
The Exponential Linear Unit (ELU) has been proven to speed up learning and im-
prove the classification performance over activation functions such as ReLU and
Leaky ReLU for convolutional neural networks. The reasons behind the improved
behavior are that ELU reduces the bias shift, it saturates for large negative inputs
and it is continuously differentiable. However, it remains open whether ELU has
the optimal shape and we address the quest for a superior activation function.
We use a new formulation to tune a piecewise linear activation function during
training, to investigate the above question, and learn the shape of the locally op-
timal activation function. With this tuned activation function, the classification
performance is improved and the resulting, learned activation function shows to
be ELU-shaped irrespective if it is initialized as a RELU, LReLU or ELU. Inter-
estingly, the learned activation function does not exactly pass through the origin
indicating that a shifted ELU-shaped activation function is preferable. This ob-
servation leads us to introduce the Shifted Exponential Linear Unit (ShELU) as a
new activation function.
Experiments on Cifar-100 show that the classification performance is further im-
proved when using the ShELU activation function in comparison with ELU. The
improvement is achieved when learning an individual bias shift for each neuron.
1	Introduction
The classification accuracy of Convolutional Neural Networks (CNNs) has improved remarkably
over the last years. The reason for the improvement is manifold: more sophisticated layer designs
(Lin et al., 2013; He et al., 2016), effective regularization techniques reducing overfitting such as
dropout (Srivastava et al., 2014) and batch normalization (Ioffe & Szegedy, 2015), new nonlinear
activation functions (Clevert et al., 2015; Trottier et al., 2016), improved weight initialization meth-
ods (Glorot & Bengio, 2010; He et al., 2016), data augmentation and large scale data as ImageNet
(Deng et al., 2009).
In this work, we focus on the nonlinear activation function and its effect on the network learning
behavior. Since the introduction of the Rectified Linear Unit (ReLU) (Glorot et al., 2011), it is gen-
erally accepted that the activation should be noncontractive to avoid the vanishing gradient problem.
The vanishing gradient hampered the learning for the sigmoid and tanh activations. As ReLU is not
symmetric, its mean response will be non-negative and will introduce a bias shift for the units in the
next layer. The Leaky Rectified Linear Unit (LReLU) (Maas et al., 2013) was proposed to alleviate
this bias shift. The LReLU introduces a small linear activation for negative inputs controlled by a
constant hyperparameter α.
Centering the activation, i.e. reducing the bias shift, is claimed to speed up learning (Le Cun et al.,
1991). When the Exponential Linear Unit (ELU) was introduced by Clevert et al. (2015), one of
the reason for its success and fast learning capability was claimed to be that the activation saturates
for large negative inputs. ELU is also controlled by a hyperparameter that determines the saturation
level. Another activation that is saturated for negative inputs is the Shifted ReLU (SReLU). Its
shape is similar to ReLU, but the ”kink” is at -1 instead of 0. This reduces the bias shift while
being saturated for negative inputs. ELU learns both faster and better than SReLU (Clevert et al.,
1
Under review as a conference paper at ICLR 2018
Table 1: Activation functions.
Activation	x > 0	X ≤ 0
ReLU	X	0
LReLU	X	α X
SReLU	x	max(x,-1)
ELU	x	α(exp(x)-1)
PELU	x	α(exp(β x)-1)
2015), but it is not obvious which properties of ELU that actually create this improvement. It may
be the smooth exponential decay for small negative inputs and/or the fact that it is continuously
differentiable. The question also remains whether the shape of ELU is truly the optimal activation
function or if there are other shapes, not yet found, that would further speed up and improve learning.
And if they exist, how are they to be found. These were the type of issues that we wanted to explore
when starting this work.
To improve the learning capabilities for the above mentioned activation functions, tuneable variants
of them have been published where the control parameters are tuneable and learned instead of being
set as a constant parameter according to their original publications. The Parametric ReLU (PReLU)
was introduced by He et al. (2016) where the single control parameter α for LReLU is now learned
during training. The Parametric ELU (PELU) was introduced by Trottier et al. (2016), also tuning
the control parameters for ELU. Classification results were shown to improve with parameter tuning
in both papers. The Scaled ELU (SELU)1 was defined by Klambauer et al. (2017) and is essentially
a more simple variant of PELU. The activation functions mentioned so far are defined in Table 1.
Piecewise linear activation functions have previously been used by Agostinelli et al. (2014) to im-
prove the performance compared to LReLU. In this work, we use the same concept with tuneable
piecewise linear activation functions, but now with the additional objective to investigate the shape
of the learned activation function. We apply this approach for nonlinear regression of the optimal
activation function. We initialized the activation as linearized versions of ReLU, LReLU and ELU,
and they all resulted in the same shape after tuning the network. The ReLU and LReLU activation
functions are tuned into an ELU-shaped function whereas the ELU activation function retains its
shape. However, we also noted that the tuned ELU-shaped activation function does not exactly pass
through the origin. There is a small shift introduced around the origin while retaining the overall
shape of the activation function.
Based on this observation, we introduce a shifted variant of the ELU activation function. In our
experiments, we found that a horizontal shift is favorable and we call this new activation function
Shifted Exponential Linear Unit (ShELU). The shift is tuneable during training and the shift is
individual for each neuron. Experiments show that the classification performance is improved when
allowing this shift in the activation function.
Our main contribution is the introduction of the shifted activation function ShELU. The second
contribution is a new formulation of a tuneable piecewise linear activation function with constraints
to make it continuous. This formulation can be used to explore for other, up to now unseen, shapes of
activation functions. The third contribution is experimental support that an ELU-shaped activation
function is favorable for learning; the tuneable piecewise linear activation function adapts to an
ELU-shape during training, but with a small shift around the origin.
2	Piecewise Linear Activation functions
Piecewise linear activation functions were first introduced by Agostinelli et al. (2014). In this work,
we use the same idea but now with the additional objective to investigate the shape of the learned
activation function. If we initialize the activation function as a linearized version of ReLU, LReLU
or ELU, how will the shape be changed during training?
Our formulation of a piecewise linear and continuous activation function is different from the one
in Agostinelli et al. (2014) and consists of two steps: first a soft histogram is formed as in Felsberg
1SELU is a variant of PELU and to avoid confusion We name the Shifted ELU ShELU.
2
Under review as a conference paper at ICLR 2018
& Granlund (2006), second, a weighted sum of the histogram outputs is computed. The piecewise
linear activation functions are learned individually for each neuron.
2.1	Soft histogram
As it has been shown by Felsberg & Granlund (2006), a soft histogram can be represented by two
components; one offset component and one histogram component. We use N bins for positive input
values and another N bins for negative input values. All bins in the histogram have constant and
unity width. The bin limits take integer values and the bin centers are at -0.5, 0.5, 1.5 etc. The
concept of the soft histogram is illustrated in Figure 1. In the figure, N is equal to 4, but in the
experiments we also used more bins like N = 8 and 16. Within a certain activation layer, we extract
the maximum and the minimum input over a minibatch. We then linearly scale the positive input
values to lie in the range [0 N] and the negative input values to lie in the range [-N 0]. Now, as an
example, consider two units with scaled input values of 2.68 and -1.80, respectively. For the first
unit, the histogram component will be 1 for the bin centered at 2.5 and 0 for all other bins. The
corresponding offset component will be 0.18, i.e. the signed distance from the bin center. For the
second unit, the histogram component will be 1 for the bin centered at -1.5 and 0 for all other bins.
The corresponding offset component will be -0.30. The output from the soft histogram for the two
units will be
0 0 0 0 0 0 0.18 0		(1a)
000000	1	0		
0 0 -0.30 0 0 0 0	0	(1b)
00	1	0000	0	
y(2.68)
y(-1.80)
An analytical formulation of the soft histogram can be given as
y(x)
(x - floor(x) - 0.5)mν (x)	o0
mν(x)	2×2N	h0
o1
h1
o2N-1
h2N-1
(2)
where mν (x) denotes the membership of the respective bins. The membership is 1 if the unit belongs
to that bin and 0 for all other bins. The soft histogram output can alternatively be expressed with the
offset and histogram components as in the right hand side of (2).
For backpropagation we need to compute the derivative of the soft histogram. In our formulation,
we consider the membership mν (x) to be locally constant. This leads to a particular choice of
subgradient that is also used in most implementations of max pooling, and it works well in practice.
The derivative is computed as
dy _ mν (x)
dX =	0
x	2×2N
(3)
The output from the soft histogram is independent of the activation function, but any activation
function can be represented or approximated by a weighted sum of the histogram output.
2.2	Weighted sum
Different piecewise linear activation functions can now be realized by varying the weights applied
to the soft histogram outputs. For each activation layer, we define a matrix W with weights for the
Figure 1: Soft histogram decomposed into rectangular and linear basis functions.
3
Under review as a conference paper at ICLR 2018
Figure 2: Weighted sum examples, ReLU activation (left) and LReLU activation (right).
offset components and the histogram components
W=
wo0	wo1
wh0	wh1
wo2N-1
wh2N-1
(4)
To obtain a ReLU activation function, we set all weights to 1 for the offset components on the posi-
tive side and 0 for all offset components on the negative side. The weights for the offset components
correspond to the slope of the activation function for each linear piece. Further, for a ReLU, we
set the weights for the histogram components to 0.5, 1.5, 2.5, etc. on the positive side and to 0 on
the negative side. The weights for the histogram components correspond to the bias level at the bin
centers of the activation function for each linear piece.
To obtain a LReLU activation function, the weights (slopes) for the negative offset components are
set to the value for the hyperparameter α, e.g. to 0.1. The weights for the offset components on the
positive side are the same as for ReLU. For α = 0.1, the weights for the histogram components on
the negative side are set to -0.35, -0.25, -0.15 and -0.05, i.e. the bias level at the bin centers.
To summarize, the weight matrices for the ReLU and LReLU activation functions are defined as
WReLU =
000	1	1	1
0 0 0 0.5	1.5 2.5
1
3.5
(5a)
	0.1	0.1	0.1	0.1	1	111
WLReLU =	-0.35	-0.25	-0.15	-0.05 0.5	1.5 2.5 3.5
(5b)
0
0
The output for the weighted sum is the sum of an elementwise multiplication of the soft histogram
with the weight matrix
y =	woν oν + whν hν .	(6)
ν
The weighted sum output for the two example units is illustrated in Figure 2. For the ReLU activa-
tion, the outputs will be 2.5 × 1 + 1 × 0.18 = 2.68 and 0 × 1 + 0 × (-0.30) = 0, respectively. For the
LReLU activation, the output for the unit with input value -1.80 will be -0.15 × 1 + 0.1 × (-0.30) =
-0.18, as desired.
Our formulation will generate a piecewise linear activation function for any values chosen as weights
for the offset and histogram components. However, it is obvious that constraints need to be put on
the weights if a continuous activation function is to be obtained. The weights (slopes) for the
offset components can be set independently but only one weight for the histogram component is
independent from the other weights. Assume that wh0 is set as desired. To obtain a continuous
linear function, the remaining histogram weights then need to be set as
wh1	=	wh0 + 0.5(wo0 + wo1)
wh2	=	wh0 + 0.5(wo0 + 2wo1 + wo2)
(7)
Wh2N-1	=	Who	+	0.5(Wθ0	+	2Woι	+---+ 2Wθ2N-2 +	Wθ2N-l)	∙
The constraints in (7) must be enforced when updating the weights in the backpropagation step.
3	Shifted activation functions
The results presented in section 4.1 show that an ELU-shaped activation function which is shifted
around the origin seems favorable to improve learning. Hence, we introduce the ShELU activation
4
Under review as a conference paper at ICLR 2018
Table 2: Shifted activation functions.
Activation	Value	Region 1	Value	Region 2
ShELU-	X + δ	x + δ > 0	α(exp(x + δ) — 1)	X + δ ≤ 0
SvELU	X + δ	x > 0	α(exp(x) — 1)+ δ	X ≤ 0
PShELU	α (X + δ)	x + δ > 0	α(eχp( x+δ) - 1)	X + δ ≤ 0
function with horizontal shift and the SvELU activation function with vertical shift and define them
as in Table 2. The hyperparameter α is considered to be a pre-set constant and it is not tuned
during training. In our experiments, we set α = 1. We also define PShELU, a variant of PELU
with horizontal shift. The parameters α and β for PShELU are learned during training. In the
experiments, they were initialized as α = β = 1.0, i.e. as an original ELU activation.
Note that the introduced shifts δ in Table 2 are individual for all neurons. As an example, consider
the first layers in the Lenet network (LeCun et al., 1998) shown in Figure 3. The input is an image
32 × 32 × 3. In the first convolutional layer, there are 192 filters with size 5 × 5 × 3. The output
from the convolutional layer consists of 192 feature maps with size 32 × 32. The output includes
a bias level for each feature map (each large square in the convolutional output), i.e. a total of 192
bias levels. The activation function is applied to the individual neurons resulting in an output with
size 32 × 32 × 192. When we say that we introduce individual shifts for all neurons, it means that
there is one tuneable shift for each of the 32 × 32 × 192 neurons (all small squares in the activation
output) where the activation function is applied.
In Goodfellow et al. (2016), chapter 9.5, it is stated that for CNNs it is natural to have shared bi-
ases with the same tiling pattern as the convolutional kernels, but that individual biases for each
neuron ”would allow the model to correct for differences in the image statistics at different loca-
tions”. By introducing the activation function ShELU, with individual shifts for each neuron, we
have indirectly created individual biases for the convolutional layer feature map output. Note that
a convolutional layer with a shared bias level for each feature map output followed by a ShELU
activation is equivalent with a convolutional layer with individual bias levels for each feature map
output followed by an ELU activation. This equivalence was verified with experiments presented
in section 4.2.1. However, frameworks as Caffe (Jia et al., 2014) and MatConvNet do not allow for
individual biases in a convolutional layer but is restricted to shared biases.
4	Experiments
4.1	Experiments with piecewise linear activation functions
To investigate the behavior of the piecewise linear activation function we made some experiments
with the Lenet network and the Cifar-100 dataset (Krizhevsky & Hinton, 2009). We used the imple-
mentation of Lenet as provided when downloading the MatConvNet framework. We ran the Lenet
network with the ReLU activation function, and also replaced all activation layers with LReLU and
Conv Filters
(5x5x3)x192
Conv Output	Activation Activation Output
32x32x192	Function 32x32x192
Figure 3: Bias levels for convolutional layers and shifts for activation function. Biases/shifts can
either be shared (large squares) or individual for each neuron (small squares).
5
Under review as a conference paper at ICLR 2018
Table 3: Top1error on Cifar-100 with Lenet network.
Activation	ToPIerror (%)	Activation	Top1error (%)	Activation	Top1error (%)
ReLU	4658	LReLU	454	ELU	44:96
TUned ReLU	45.92	TUnedLReLU	45.18	TUnedELU	44.51
ELU. We then exchanged the activation layers with the piecewise linear activation layer. We initial-
ized the layers as a linear version of ReLU, LReLU and ELU respectively. We consistently noticed a
slight improvement (a few tenths of a percent) in classification performance when using the tuneable
piecewise linear activation function compared to its corresponding fixed activation function, see Ta-
ble 3. Besides the slight classification improvement, it is also interesting to analyse the shape of the
activation functions after tuning, see Figure 4. All three activation functions remain linear and with
almost unity slope on the positive side. All three tuned activation functions exhibit a smooth ex-
ponential decay for small negative inputs and then remain fairly constant for larger negative inputs.
The resulting shape after tuning for all three initializations is close to the ELU shape. However,
notice that all tuned activation functions tend to return a variable but negative output for zero input
and that they do not pass through the origin. These results suggest that we introduce the Shifted Ex-
ponential Linear Unit (ShELU) as an activation function. From the results it is not obvious whether
the introduced shift around the origin should be vertical or horizontal. For a horizontal shift, the
saturation level remains constant for large negative inputs which may seem more intuitive. For a
vertical shift, the saturation level will vary depending on the shift which better matches the achieved
results on the Lenet network.
4.2 Experiments with shifted activation functions
4.2.1	Experiments on Cifar- 1 00 with Lenet network
We now want to evaluate if the classification performance improves with the new activation func-
tions ShELU and SvELU compared to ELU. We start with the Lenet network and replace all ELU
activations with either the ShELU or the SvELU activation. The learning rate was set to 0.005 for
the first 40 epochs, then lowered by a factor of 10 every 20 epochs, running a total of 80 epochs.
The learning rate momentum was set to 0.9 and the weight decay to 0.0005. Image data was pre-
processed with global contrast normalization and whitening (Coates et al., 2011). Note that the
complete dataset was divided by a factor of 10 (compared to the preprocessing provided with the
MatConvNet download) to better match the variance with Xavier initialization. During training the
dataset was augmented with random horizontal flipping and by randomly cropping images from the
original images zero padded with a frame of width four.
The classification errors for the training and test sets shown in Figure 5 are the average over 8 runs
for each activation function. The top1errors in Table 4 are the average over the last 15 epochs for the
lowest learning rate. The learning rate for the ShELU and SvELU activation layer weights was set
to 2% of the base learning rate. The results show that there is a small improvement on the top1error
using the ShELU and SvELU activation functions compared with the original ELU. Futhermore, the
shifted activation function PShELU achieves a slightly better test results than both ELU and PELU.
Figure 4:	Initialization (red) and 20, 50 and 80 percentiles for tuned activation functions in last layer;
ReLU (left), LReLU (middle) and ELU (right).
6
Under review as a conference paper at ICLR 2018
0.6
0.55
0.5
0.45
0.4
0.35 -------------'-------------'------------'------------'
0	20	40	60	80
Epochs
Figure 5:	Training (dashed) and test (solid) errors on Cifar-100 with network Lenet (left). Test errors
(final part) for ELU, SvELU and ShELU (middle), and ELU, PELU and PShELU (right).
Table 4: Topl test errors on Cifar-100 with Lenet and Clevert-11 networks.
Activation	Lenet network	Clevert-11 network
-ELU	44:96	2876
SvELU	44.70	28.85
ShELU	44.77	28.57
PELU	45.03	28.78
PShELU	44.76	28.74
ConvIndBias + ELU	44.78	-
The training behavior is very similar for all activation functions but the shifted activation functions
exhibit a slightly better generalization behavior. However, the significance of these results is limited
as Lenet is a rather shallow network.
We also created a network layer named ”ConvIndBias”, which is an identity mapping but it also
adds an individually learned bias shift for each neuron. The results in Table 4 confirm that a ShELU
activation is equivalent to the combination of a ConvIndBias layer and an ELU activation as was
stated in section 3.
4.2.2 Experiments on Cifar- 1 00 with Clevert-11
To further evaluate the shifted activation functions in comparison with ELU, we built the 11-layer
network used by Clevert et al. (2015) to replicate the experiments when ELU was introduced. We
denote the network Clevert-11. Parameter settings and weight initializations were as in Clevert et al.
(2015). Our results are the average over 9 runs for each activation function. Our classification results
with the network Clevert-11 on the Cifar-100 dataset for the activation functions ELU, SvELU,
ShELU, PELU and PShELU are presented in Figure 6 and summarized in Table 4. The results in the
table are the average top1error over the last 20 epochs for each activation function. The results show
that the test error for the ShELU activation function is significantly better than for ELU, whereas
the error for SvELU is slightly inferior. The results suggest that a horizontal shift for the activation
function is preferable to a vertical shift. The training behavior is almost identical for ELU and
Figure 6:	Training (dashed) and test (solid) errors on Cifar-100 with network Clevert-11 (left). Test
errors (final part) for ELU, SvELU and ShELU (middle), and ELU, PELU and PShELU (right).
7
Under review as a conference paper at ICLR 2018
0.1
0.05
LaYerI	LaYer 2	LaYer 3	LaYer 4	LaYer 5
0.1	0.1	0.1	0.1 I~,~,~,~I
-03A Ii
-4 -2 0 2 4
×10-3
-4 -2 0 2 4
×10-3
-4 -2 0 2 4
×10-3
-4 -2 0 2 4
×10-3
-4 -2 0 2 4
×10-3
0.1
0.05
LaYer 6	0.1 LaYer 7	0.1 LaYer 8	0.1 LaYer 9	0.1 LaYfO
A “I A “I A0"」
-4 -2 0 2 4
×10-3
-4 -2 0 2 4
×10-3
-4 -2 0 2 4
×10-3
-4 -2 0 2 4	-0.2
×10-3
0.2
Spatial shift ShELU layer1
0 1
0 1
0
Figure 7:	Learned shifts for ShELU activation function, relative frequency (left), kurtosis (middle)
and spatial variation (right).
ShELU. We believe that the improved test result can be attributed to that ShELU adaptively learns
where to set the reference level between the linear and exponential parts of the activation function.
ELU, PELU and PShELU all show very similar test errors. Note, however, that the training error
is by far lower for PELU indicating pronounced overfitting compared to ELU. The training error
is lower for PShELU than for ShELU but the test error is inferior. This suggests that PShELU
suffers from overfitting when allowed to tune the hyperparameters α and β . Note that we were
able to almost exactly reproduce the results for ELU achieved in Clevert et al. (2015) who report a
top1error of 28.75%.
4.3 Learned shifts for ShELU
In all experments, we initialized the individual shifts for the ShELU activation from a Gaussian
distribution with standard deviation 0.001. The learned shifts after training in the 10 activation layers
of the Clevert-11 network are shown as normalized frequency histograms in Figure 7, together with
the kurtosis and the spatial variation for the shifts.
The shape of the learned shifts is almost a perfect Gaussian distribution for all layers. This is
supported by the computed kurtosis which is close to 3.0. The kurtosis increases slightly for the
last three layers where the distribution tends to be somewhat skewed towards the negative side. The
standard deviation for the shift is relatively constant for the first nine layers but grows considerably
for the last layer.
Figure 7 shows the learned shifts for the first ShELU activation layer where the shifts for the 192
feature maps have been placed as 12 × 16 tiles side by side. Interestingly, the spatial variation for
the learned shift seems to be completely random. Any statistical difference spatially over the image
cannot be perceived.
5 Conclusions
We use a new formulation to tune a continuous piecewise linear activation function during training
and learn the shape of the locally optimal activation function. With this tuned activation function, the
classification performance for convolutional neural networks is improved and the resulting, learned
activation function shows to be ELU-shaped irrespective whether it is initialized as a RELU, LReLU
or ELU activation function. The learned activation function exhibits a variable shift around the
origin for each neuron, indicating that a shifted ELU-shaped activation function is preferable. This
observation leads us to introduce the Shifted Exponential Linear Unit (ShELU) as a new activation
function.
Experiments on Cifar-100 show that the classification performance is further improved when using
the ShELU activation function in comparison with ELU. Normally in a convolutional network layer,
one shared bias shift is learned for each feature map output. The improvement for the ShELU
activation is achieved when learning an individual bias shift for each neuron. The equivalent to
the ShELU activation function would be to learn an individual bias shift for each neuron in the
convolutional layer output and then apply an ELU activation, which however is not supported by
commonly used deep learning frameworks. The implementation of individual biases in the activation
function is therefore preferable and leads to the ShELU activation function.
8
Under review as a conference paper at ICLR 2018
References
Forest Agostinelli, Matthew Hoffman, Peter Sadowski, and Pierre Baldi. Learning activation func-
tions to improve deep neural networks. arXiv preprint arXiv:1412.6830, 2014.
Djork-Ame Clevert, Thomas Unterthiner, and SePP Hochreiter. Fast and accurate deep network
learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.
Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsuPervised
feature learning. In Proceedings of the fourteenth international conference on artificial intelli-
gence and statistics, pp. 215-223, 2011.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical
Image Database. In CVPR09, 2009.
Michael Felsberg and Gosta Granlund. P-Channels: Robust multivariate m-estimation of large
datasets. In Pattern Recognition, 2006. ICPR 2006. 18th International Conference on, volume 3,
pp. 262-267. IEEE, 2006.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence
and Statistics, pp. 249-256, 2010.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In
Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,
pp. 315-323, 2011.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In Francis Bach and David Blei (eds.), Proceedings of the 32nd
International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning
Research, pp. 448-456, Lille, France, 07-09 Jul 2015. PMLR. URL http://proceedings.
mlr.press/v37/ioffe15.html.
Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Ser-
gio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embed-
ding. arXiv preprint arXiv:1408.5093, 2014.
Gunter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing
neural networks. arXiv preprint arXiv:1706.02515, 2017.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
Yann Le Cun, Ido Kanter, and Sara A Solla. Eigenvalues of covariance matrices: Application to
neural-network learning. Physical Review Letters, 66(18):2396, 1991.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400,
2013.
Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectifier nonlinearities improve neural net-
work acoustic models. In Proc. ICML, volume 30, 2013.
MatConvNet. http://www.vlfeat.org/matconvnet/,v.beta-20.
Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. Journal of machine learning
research, 15(1):1929-1958, 2014.
9
Under review as a conference paper at ICLR 2018
LUdovic Trottier, PhiliPPe Giguere, and Brahim Chaib-draa. Parametric exponential linear unit for
deep convolutional neural networks. arXiv preprint arXiv:1605.09332, 2016.
Appendix
Derivatives of ShELU and SvELU
For backProPagation, the derivates of ShELU and SvELU with resPect to the inPut x and the shift δ
are comPuted as
dShELU	1, if x+δ> 0	(8a)
dx	α(exp(x + δ)), if x + δ ≤ 0	
dShELU	1, if x + δ > 0	(8b)
-dδ二	α(exp(x + δ)), if x + δ ≤ 0	
dSvELU	1, if x > 0	(8c)
dx	α(exp(x)), if x ≤ 0	
dSvELU	1	(8d)
-dδ-二		
Derivatives of PShELU
For backProPagation, the derivates of PShELU with resPect to the inPut x, the hyPerParameters α
and β, and the shift δ are comPuted as		
dPShELU _	( α, if X + δ> 0 ∣α(exp(x+δ)),if x + δ ≤ 0	(9a)
dx		
dPShELU _	( x+δ, if x + δ> 0	(9b)
dα	(exp(x+δ) 一 1.0, if x + δ ≤ 0	
dPShELU	( 一节(x + δ), if x + δ > 0 ［一防(exp(x+δ)),if X + δ ≤ 0	(9c)
dβ=		
dPShELU	( α, if x + δ > 0 Ia(exp(x+δ)),if X + δ ≤ 0	(9d)
-dδ-=		
10