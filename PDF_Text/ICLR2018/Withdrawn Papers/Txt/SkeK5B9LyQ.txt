Under review as a conference paper at ICLR 2018
A Neural-Symbolic Approach to
Natural Language Tasks
Anonymous authors
Paper under double-blind review
Ab stract
Deep learning (DL) has in recent years been widely used in natural language pro-
cessing (NLP) applications due to its superior performance. However, while natural
languages are rich in grammatical structure, DL does not have an internal repre-
sentation to explicitly represent and enforce such structures. This paper proposes
a new architecture to bridge this gap by exploiting tensor product representations
(TPR), a structured neural-symbolic framework developed in cognitive science over
the past 20 years, with the aim of integrating DL with explicit language structures
and rules. We call it the Tensor Product Generation Network (TPGN), and apply
it to image captioning. The key ideas of TPGN are: 1) unsupervised learning
of role-unbinding vectors of words via a TPR-based deep neural network, and
2) integration of TPR with typical DL architectures including Long Short-Term
Memory (LSTM) models. The novelty of our approach lies in its ability to generate
a sentence and extract partial grammatical structure of the sentence by using role-
unbinding vectors, which are obtained in an unsupervised manner. Experimental
results demonstrate the effectiveness of the proposed approach.
1	Introduction
Deep learning is an important tool in many current natural language processing (NLP) applications.
However, language rules or structures cannot be explicitly represented in deep learning architectures.
The tensor product representation developed in Smolensky (1990); Smolensky & Legendre (2006)
has the potential of integrating deep learning with explicit rules (such as logical rules, grammar
rules, or rules that summarize real-world knowledge). This paper develops a TPR approach for
deep-learning-based NLP applications, introducing the Tensor Product Generation Network (TPGN)
architecture. To demonstrate the effectiveness of the proposed architecture, we apply it to a important
NLP application: image captioning.
A TPGN model generates natural language descriptions via learned representations. The represen-
tations learned in the TPGN can be interpreted as encoding grammatical roles for the words being
generated. This layer corresponds to the role-encoding component of a general, independently-
developed architecture for neural computation of symbolic functions, including the generation of
linguistic structures. The key to this architecture is the notion of Tensor Product Representation
(TPR), in which vectors embedding symbols (e.g., lives, frodo) are bound to vectors embed-
ding structural roles (e.g., verb, subject) and combined to generate vectors embedding symbol
structures ([frodo lives]). TPRs provide the representational foundations for a general compu-
tational architecture called Gradient Symbolic Computation (GSC), and applying GSC to the task of
natural language generation yields the specialized architecture defining the model presented here. The
generality of GSC means that the results reported here have implications well beyond the particular
tasks we address here.
The paper is organized as follows. Section 2 discusses related work. In Section 3, we review the
basics of tensor product representation. Section 4 presents the rationale for our proposed architecture.
Section 5 describes our proposed model in detail. In Section 6, we present our experimental results.
Finally, Section 7 concludes the paper.
1
Under review as a conference paper at ICLR 2018
2	Related work
Deep learning plays a dominant role in many NLP applications due to its exceptional performance.
Hence, we focus on recent deep-learning-based literature for an important NLP application, i.e.,
image captioning.
This work follows a great deal of recent caption-generation literature in exploiting end-to-end deep
learning with a CNN image-analysis front end producing a distributed representation that is then
used to drive a natural-language generation process, typically using RNNs Mao et al. (2015); Vinyals
et al. (2015); Devlin et al. (2015); Chen & Zitnick (2015); Donahue et al. (2015); Karpathy & Fei-Fei
(2015); Kiros et al. (2014a;b); Fang et al. (2015). Our grammatical interpretation of the structural
roles of words in sentences makes contact with other work that incorporates deep learning into
grammatically-structured networks Tai et al. (2015); Kumar et al. (2016); Kong et al. (2017); Andreas
et al. (2015); Yogatama et al. (2016); Maillard et al. (2017); Socher et al. (2010); Pollack (1990).
Here, the network is not itself structured to match the grammatical structure of sentences being
processed; the structure is fixed, but is designed to support the learning of distributed representations
that incorporate structure internal to the representations themselves — filler/role structure.
TPRs are also used in NLP in Palangi et al. (2017) but there the representation of each individual
input word is constrained to be a literal TPR filler/role binding. (The idea of using the outer product
to construct internal representations was also explored in Fukui et al. (2016).) Here, by contrast,
the learned representations are not themselves constrained, but the global structure of the network
is designed to display the somewhat abstract property of being TPR-capable: the architecture uses
the TPR unbinding operation of the matrix-vector product to extract individual words for sequential
output.
3	Review of tensor product representation
Tensor product representation (TPR) is a general framework for embedding a space of symbol
structures S into a vector space. This embedding enables neural network operations to perform
symbolic computation, including computations that provide considerable power to symbolic NLP
systems (Smolensky & Legendre (2006); Smolensky (2012)). Motivated by these successful examples,
we are inspired to extend the TPR to the challenging task of learning image captioning. And as a
by-product, the symbolic character of TPRs makes them amenable to conceptual interpretation in a
way that standard learned neural network representations are not.
A particular TPR embedding is based in a filler/role decomposition of S . A relevant example is
when S is the set of strings over an alphabet {a, b, . . .}. One filler/role decomposition deploys the
positional roles {rk}, k ∈ N, where the filler/role binding a/rk assigns the ‘filler’ (symbol) a to the
kth position in the string. A string such as abc is uniquely determined by its filler/role bindings,
which comprise the (unordered) set B(abc) = {b/r2, a/r1, c/r3}. Reifying the notion role in this
way is key to TPR’s ability to encode complex symbol structures.
Given a selected filler/role decomposition of the symbol space, a particular TPR is determined by an
embedding that assigns to each filler a vector in a vector space VF = RdF, and a second embedding
that assigns to each role a vector in a space VR = RdR. The vector embedding a symbol a is denoted
by fa and is called a filler vector; the vector embedding a role rk is rk and called a role vector. The
TPR for abc is then the following 2-index tensor in VF 0 VR = RdF ×dR:
Sabc = fb 0 r2 + fa 0 ri + fc 0 r3,	(1)
where 0 denotes the tensor product. The tensor product is a generalization of the vector outer product
that is recursive; recursion is exploited in TPRs for, e.g., the distributed representation of trees, the
neural encoding of formal grammars in connection weights, and the theory of neural computation
of recursive symbolic functions. Here, however, it suffices to use the outer product; using matrix
notation we can write (1) as:
Sabc = fbr2> + far1> + fcr3> .	(2)
Generally, the embedding of any symbol structure S ∈ S is P{fi 0 ri | fi/ri ∈ B(S)}; here:
P{firi> | fi/ri ∈ B(S)} (Smolensky (1990); Smolensky & Legendre (2006)).
2
Under review as a conference paper at ICLR 2018
A key operation on TPRs, central to the work presented here, is unbinding, which undoes binding.
Given the TPR in (2), for example, we can unbind r2 to get fb ; this is achieved simply by fb = Sabcu2 .
Here u2 is the unbinding vector dual to the binding vector r2. To make such exact unbinding possible,
the role vectors should be chosen to be linearly independent. (In that case the unbinding vectors are
the rows of the inverse of the matrix containing the binding vectors as columns, So that r2 ∙ u2 = 1
while rk ∙ u2 = 0 for all other role vectors rk = r2; this entails that SabC u2 = b, the filler vector
bound to r2 . Replacing the matrix inverse with the pseudo-inverse allows approximate unbinding
when the role vectors are not linearly independent).
Figure 1: Architecture of TPGN, a TPR-CaPable generation network. “反］” denotes the matrix-vector
product.
4	A TPR-capable generation architecture
In this work we ProPose an aPProach to network architecture design we call the TPR-capable method.
The architecture we use (see Fig. 1) is designed so that TPRs could, in theory, be used within the
architecture to Perform the target task — here, generating a caPtion one word at a time. Unlike
Previous work where TPRs are hand-crafted, in our work, end-to-end deeP learning will induce
rePresentations which the architecture can use to generate caPtions effectively.
In this section, we consider the Problem of image caPtioning. As shown in Fig. 1, our ProPosed
system is denoted by N, which is from “N” in “TPGN”. The inPut of N is an image feature vector
v and the outPut of N is a caPtion. The image feature vector v is extracted from a given image
by a Pre-trained CNN. The first Part of our system N is a sentence-encoding subnetwork S which
maPs v to a rePresentation S which will drive the entire caPtion-generation Process; S contains all
the image-sPecific information for Producing the caPtion. (We will call a caPtion a “sentence” even
though it may in fact be just a noun Phrase.)
If S were a TPR of the caPtion itself, it would be a matrix (or 2-index tensor) S which is a sum of
matrices, each of which encodes the binding of one word to its role in the sentence constituting the
caPtion. To serially read out the words encoded in S, in iteration 1 we would unbind the first word
from S, then in iteration 2 the second, and so on. As each word is generated, S could uPdate itself,
for examPle, by subtracting out the contribution made to it by the word just generated; St denotes
the value of S when word wt is generated. At time steP t we would unbind the role rt occuPied
by word wt of the caPtion. So the second Part of our system N — the unbinding subnetwork U
— would generate, at iteration t, the unbinding vector ut . Once U Produces the unbinding vector
ut, this vector would then be aPPlied to S to extract the symbol ft that occuPies word t’s role; the
symbol rePresented by ft would then be decoded into word wt by the third Part of N, i.e., the lexical
decoding subnetwork L, which outPuts xt, the 1-hot-vector encoding of wt.
3
Under review as a conference paper at ICLR 2018
Recalling that unbinding in TPR is achieved by the matrix-vector product, the key operation in
generating wt is thus the unbinding of rt within S, which amounts to simply:
Stut = ft .	(3)
This matrix-vector product is denoted “反Tin Fig. 1.
Thus the system N of Fig. 1is TPR-capable. This is what we propose as the Tensor-Product
Generation Network (TPGN) architecture. The learned representation S will not be proven to literally
be a TPR, but by analyzing the unbinding vectors ut the network learns, we will gain insight into the
process by which the learned matrix S gives rise to the generated caption.
What type of roles might the unbinding vectors be unbinding? A TPR for a caption could in principle
be built upon positional roles, syntactic/semantic roles, or some combination of the two. In the
caption a man standing in a room with a suitcase, the initial a and man might respectively occupy
the positional roles of POS(ITION)1 and POS2 ; standing might occupy the syntactic role of VERB;
in the role of SPATIAL-P(REPOSITION); while a room with a suitcase might fill a 5-role schema
Det(erminer)1 N(oun)1 P Det2 N2. In fact we will see evidence below that our network learns
just this kind of hybrid role decomposition.
What form of information does the sentence-encoding subnetwork S need to encode in S? Continuing
with the example of the previous paragraph, S needs to be some approximation to the TPR summing
several filler/role binding matrices. In one of these bindings, a filler vector fa — which the lexical
subnetwork L will map to the article a — is bound (via the outer product) to a role vector rPOS1 which
is the dual of the first unbinding vector produced by the unbinding subnetwork U : uPOS1 . In the first
iteration of generation the model computes S1uPOS1 = fa, which L then maps to a. Analogously,
another binding approximately contained in S2 is fmanrP>OS . There are corresponding bindings for the
remaining words of the caption; these employ syntactic/semantic roles. One example is fstandingrV> .
At iteration 3, U decides the next word should be a verb, so it generates the unbinding vector uV
which when multiplied by the current output of S, the matrix S3, yields a filler vector fstanding which
L maps to the output standing. S decided the caption should deploy standing as a verb and included
in S the binding fstandingrV>. It similarly decided the caption should deploy in as a spatial preposition,
including in S the binding fin rS>PATIAL-P ; and so on for the other words in their respective roles in the
caption.
5	System Description
The unbinding subnetwork U and the sentence-encoding network S of Fig. 1 are each implemented
as (1-layer, 1-directional) LSTMs (see Fig. 2); the lexical subnetwork L is implemented as a linear
transformation followed by a softmax operation. In the equations below, the LSTM variables internal
to the S subnet are indexed by 1 (e.g., the forget-, input-, and output-gates are respectively fι, iι, oι)
while those of the unbinding subnet U are indexed by 2.
Thus the state updating equations for S are, for t = 1,…，T = caption length:
fι,t	=	二	σg(Wι,f pt-1 - Dι,fWext-I + Uι,f St-ι)	(4)
^ i1,t 二	σ σg(W1,iPt-1 - D1,iWext-1 + Uι,iSt-ι)	(5)
o1,t 二	∕τ≡ τ	τ^~∖	Ti τ	τ τ	r^i	\ σg(W1,opt-1 - D1,oWext-1 + U1,oSt-1)	(6)
g1,t	σh(W1,cpt-1 - D1,cWext-1 + U1,cSt-1)	(7)
c1,t	二	fι,t。Cι,t-1 + iι,t。gι,t	(8)
St 二	二 0ι,t Θ σh(cι,t)	(9)
where q,t, iι,t, Oι,t, gι,t, cι,t, St ∈ Rd×d, Pt ∈ Rd, σg(∙) is the (element-wise) logistic sigmoid
function; σh(∙) is the hyperbolic tangent function; the operator Θ denotes the Hadamard (element-
wise) product; W1,f, W1,i, W1,o, W1,c ∈ Rd×d×d, D1,f, D1,i, D1,o, D1,c ∈ Rd×d×d, U1,f, U1,i,
U1,o, U1,c ∈ Rd×d×d×d. For clarity, biases — included throughout the model — are omitted from
all equations in this paper. The initial state So is initialized by:
So = Cs(V — V)	(10)
4
Under review as a conference paper at ICLR 2018
Figure 2: The sentence-encoding subnet S and the unbinding subnet U are inter-connected LSTMs;
v encodes the visual input while the xt encode the words of the output caption.
where v ∈ R2048 is the vector of visual features extracted from the current image by ResNet (Gan
et al. (2017)) and V is the mean of all such vectors; Cs ∈ Rd×d×2048. On the output side, Xt ∈ RV
is a 1-hot vector with dimension equal to the size of the caption vocabulary, V , and We ∈ Rd×V
is a word embedding matrix, the i-th column of which is the embedding vector of the i-th word in
the vocabulary; it is obtained by the Stanford GLoVe algorithm with zero mean (Pennington et al.
(2017)). X0 is initialized as the one-hot vector corresponding to a “start-of-sentence” symbol.
For U in Fig. 1, the state updating equations are:
^ ^2,t	=	二	σg (St-1w2,f - D2,f Wext-I + U2,f pt-1)	(11)
ʌ i2,t 二	二	σg(St-1W2,i 一 D2,iWext-1 + U2,iPt-1)	(12)
O2,t 二	二	σg(St-1W2,o - D2,oWext-1 + U2,oPt-1)	(13)
g2,t	二	σh(St-1W2,c - D2,cWext-1 + U2,cpt-1)	(14)
c2,t	二 %t Θ C2,t-1 + i2,t Θ g2,t	(15)
pt	二	^2,t Θ σh(c2,t)	(16)
where w2,f, w2,i, w2,o, w2,c ∈ Rd, D2,f, D2,i, D2,o, D2,c ∈ Rd×d, and U2,f, U2,i, U2,o, U2,c ∈
Rd×d. The initial state p0 is the zero vector.
The dimensionality of the crucial vectors shown in Fig. 1, ut and ft, is increased from d × 1 to d2 × 1
as follows. A block-diagonal d2 X d2 matrix St is created by placing d copies of the d X d matrix St
as blocks along the principal diagonal. This matrix is the output of the sentence-encoding subnetwork
S. Now, following Eq. (3), the ‘filler vector’ ft ∈ Rd2 — ‘unbound’ from the sentence representation
St with the ‘unbinding vector’ ut — is obtained by Eq. (17).
ft = Stut	(17)
Here ut ∈ Rd2, the output of the unbinding subnetwork U, is computed as in Eq. (18), where
Wu ∈ Rd2 ×d is U ’s output weight matrix.
ut = σh(Wupt)	(18)
Finally, the lexical subnetwork L produces a decoded word Xt ∈ RV by
Xt = σs(Wxft)	(19)
5
Under review as a conference paper at ICLR 2018
where σs(∙) is the Softmax function and Wx ∈ RV×d2 is the overall output weight matrix. Since
Wx plays the role of a word de-embedding matrix, we can set
Wx = (We)>	(20)
where We is the word-embedding matrix. Since We is pre-defined, we directly set Wx by Eq. (20)
without training L through Eq. (19). Note that S and U are learned jointly through the end-to-end
training.
6	Experimental results
6.1	Dataset
To evaluate the performance of our proposed architecture, we use the COCO dataset (COCO (2017)).
The COCO dataset contains 123,287 images, each of which is annotated with at least 5 captions. We
use the same pre-defined splits as Karpathy & Fei-Fei (2015); Gan et al. (2017): 113,287 images for
training, 5,000 images for validation, and 5,000 images for testing. We use the same vocabulary as
that employed in Gan et al. (2017), which consists of 8,791 words.
6.2	Evaluation of image captioning system
For the CNN of Fig. 1, we used ResNet-152 (He et al. (2016)), pretrained on the ImageNet dataset.
The feature vector v has 2048 dimensions. Word embedding vectors in We are downloaded from the
web (Pennington et al. (2017)). The model is implemented in TensorFlow (Abadi et al. (2015)) with
the default settings for random initialization and optimization by backpropagation.
In our experiments, we choose d = 25 (where d is the dimension of vector pt). The dimension of St
is 625 X 625 (while St is 25 X 25); the vocabulary size V = 8, 791; the dimension of Ut and f is
d2 = 625.
Table 1: Performance of the proposed TPGN model on the COCO dataset.
Methods	METEOR	BLEU-I	BLEU-2	BLEU-3	BLEU-4	CIDEr
NIC Vinyals et al. (2015)	—	0.666~~	0.461	0.329	0.246	—
CNN-LSTM	0.238	0.698	0.525	0.390	0.292	0.889
TPGN	0.243	0.709	0.539	0.406	0.305	0.909
The main evaluation results on the MS COCO dataset are reported in Table 1. The widely-used BLEU
(Papineni et al. (2002)), METEOR (Banerjee & Lavie (2005)), and CIDEr (Vedantam et al. (2015))
metrics are reported in our quantitative evaluation of the performance of the proposed schemes. In
evaluation, our baseline is the widely used CNN-LSTM captioning method originally proposed in
Vinyals et al. (2015). For comparison, we include results in that paper in the first line of Table 1. We
also re-implemented the model using the latest ResNet feature and report the results in the second
line of Table 1. Our re-implementation of the CNN-LSTM method matches the performance reported
in Gan et al. (2017), showing that the baseline is a state-of-the-art implementation. As shown in
Table 1, compared to the CNN-LSTM baseline, the proposed TPGN significantly outperforms the
benchmark schemes in all metrics across the board. The improvement in BLEU-n is greater for
greater n; TPGN particularly improves generation of longer subsequences. The results clearly attest
to the effectiveness of the TPGN architecture.
7	Conclusion
In this paper, we proposed a new Tensor Product Generation Network (TPGN) for natural language
generation and related tasks. The model has a novel architecture based on a rationale derived from
the use of Tensor Product Representations for encoding and processing symbolic structure through
neural network computation. In evaluation, we tested the proposed model on captioning with the MS
COCO dataset, a large-scale image captioning benchmark. Compared to widely adopted LSTM-based
models, the proposed TPGN gives significant improvements on all major metrics including METEOR,
6
Under review as a conference paper at ICLR 2018
BLEU, and CIDEr. Moreover, we observe that the unbinding vectors contain important grammatical
information. Our findings in this paper show great promise of TPRs. In the future, we will explore
extending TPR to a variety of other NLP tasks.
References
Martin Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.
Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew
Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath
Kudlur, Josh LeVenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike
Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent
Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg,
Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on
heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software available
from tensorflow.org.
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Deep compositional question
answering with neural module networks. arxiv preprint. arXiv preprint arXiv:1511.02799, 2, 2015.
Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved
correlation with human judgments. In Proceedings of the ACL workshop on intrinsic and extrinsic
evaluation measuresfor machine translation and/or summarization, pp. 65-72. Association for
Computational Linguistics, 2005.
Xinlei Chen and Lawrence Zitnick. Mind’s eye: A recurrent visual representation for image caption
generation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 2422-2431, 2015.
COCO. Coco dataset for image captioning. http://mscoco.org/dataset/#download,
2017.
Jacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta, Li Deng, Xiaodong He, Geoffrey Zweig, and
Margaret Mitchell. Language models for image captioning: The quirks and what works. arXiv
preprint arXiv:1505.01809, 2015.
Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venu-
gopalan, Kate Saenko, and Trevor Darrell. Long-term recurrent convolutional networks for visual
recognition and description. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 2625-2634, 2015.
Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh K Srivastava, Li Deng, Piotr Dollar, Jianfeng Gao,
Xiaodong He, Margaret Mitchell, John C Platt, et al. From captions to visual concepts and back. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1473-1482,
2015.
Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach.
Multimodal compact bilinear pooling for visual question answering and visual grounding. arXiv
preprint arXiv:1606.01847, 2016.
Zhe Gan, Chuang Gan, Xiaodong He, Yunchen Pu, Kenneth Tran, Jianfeng Gao, Lawrence Carin,
and Li Deng. Semantic compositional networks for visual captioning. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 770-778, 2016.
Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3128-
3137, 2015.
7
Under review as a conference paper at ICLR 2018
Ryan Kiros, Ruslan Salakhutdinov, and Rich Zemel. Multimodal neural language models. In
Proceedings ofthe 31st International Conference on Machine Learning (ICML-14), pp. 595-603,
2014a.
Ryan Kiros, Ruslan Salakhutdinov, and Richard S Zemel. Unifying visual-semantic embeddings with
multimodal neural language models. arXiv preprint arXiv:1411.2539, 2014b.
Lingpeng Kong, Chris Alberti, Daniel Andor, Ivan Bogatyy, and David Weiss. Dragnn: A transition-
based framework for dynamically connected neural networks. arXiv preprint arXiv:1703.04474,
2017.
Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor
Zhong, Romain Paulus, and Richard Socher. Ask me anything: Dynamic memory networks for
natural language processing. In International Conference on Machine Learning, pp. 1378-1387,
2016.
Jean Maillard, Stephen Clark, and Dani Yogatama. Jointly learning sentence embeddings and syntax
with unsupervised tree-lstms. arXiv preprint arXiv:1705.09189, 2017.
Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, and Alan Yuille. Deep captioning with
multimodal recurrent neural networks (m-rnn). In Proceedings of International Conference on
Learning Representations, 2015.
Hamid Palangi, Paul Smolensky, Xiaodong He, and Li Deng. Deep learning of grammatically-
interpretable representations through question-answering. arXiv preprint arXiv:1705.08432, 2017.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of the 40th annual meeting on association for
computational linguistics, pp. 311-318. Association for Computational Linguistics, 2002.
Jeffrey Pennington, Richard Socher, and Christopher Manning. Stanford glove: Global vectors for
word representation. https://nlp.stanford.edu/projects/glove/, 2017.
Jordan B Pollack. Recursive distributed representations. Artificial Intelligence, 46(1):77-105, 1990.
Paul Smolensky. Tensor product variable binding and the representation of symbolic structures in
connectionist systems. Artificial intelligence, 46(1-2):159-216, 1990.
Paul Smolensky. Symbolic functions from neural computation. Philosophical Transactions of the
Royal Society —A: Mathematical, Physical and Engineering Sciences, 370:3543 - 3569, 2012.
Paul Smolensky and Geraldine Legendre. The harmonic mind: From neural computation to OptimaIity-
theoretic grammar. Volume 1: Cognitive architecture. MIT Press, 2006.
Richard Socher, Christopher D Manning, and Andrew Y Ng. Learning continuous phrase representa-
tions and syntactic parsing with recursive neural networks. In Proceedings of the NIPS-2010 Deep
Learning and Unsupervised Feature Learning Workshop, pp. 1-9, 2010.
Kai Sheng Tai, Richard Socher, and Christopher D Manning. Improved semantic representations
from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075, 2015.
Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image
description evaluation. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 4566-4575, 2015.
Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural
image caption generator. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 3156-3164, 2015.
Dani Yogatama, Phil Blunsom, Chris Dyer, Edward Grefenstette, and Wang Ling. Learning to
compose words into sentences with reinforcement learning. arXiv preprint arXiv:1611.09100,
2016.
8