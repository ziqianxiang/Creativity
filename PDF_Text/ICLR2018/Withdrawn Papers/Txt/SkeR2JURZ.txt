Under review as a conference paper at ICLR 2018
Distributed Restarting NewtonCG Method
for Large-Scale Empirical Risk Minimization
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we propose a distributed damped Newton method in which sam-
ple size is gradually increasing to quickly obtain a solution whose empirical loss
is under satisfactory statistical accuracy. Our proposed method is multistage in
which the solution of one stage serves as a warm start for the next stage which
contains more samples (including the samples in the previous stage). This overall
multistage algorithm reduce the number of passes over data. Moreover, our al-
gorithm in nature is easy to be distributed and shares the strong scaling property
indicating that acceleration is always expected by using more computing nodes.
Various iteration complexity results regarding descent direction computation and
stopping criteria are analyzed under convex setting. Our results of experiments
illustrate that the proposed algorithm can outperform other comparable methods
for training machine learning tasks including neural networks.
1 Introduction
In the field of machine learning, solving the expected risk minimization problem has received lots
of attentions over decades, which is in the form
min L(w) = min Ez [f (w, z)],
w∈Rd	w∈Rd
(1)
where z is a d + 1 dimensional random variable containing both feature variables and a response
variable. f (w, z) is a loss function with respect to w and any fixed value of z. In most practical
problems, the distribution ofz is either unknown or leading great difficulties to evaluate the expected
loss. One general idea is to estimate the expectation with a statistical average over a large number
of independent and identically distributed data samples of z, which is denoted by {z1, z2, . . . , zN}
where N is the total number of samples. Thus, the problem in (1) can be rewritten as the Empirical
Risk Minimization (ERM) problem
min LN (w)
w∈Rd
1
min 而 ɪ^fi(w),
w∈Rd N
i=1
(2)
wherefi(w) =f(w,	zi).
A lot of studies have been done on developing optimization algorithms to find an optimal solution
of above problem under different setting. For example, Beck & Teboulle (2009); Nesterov (2013);
Drusvyatskiy et al. (2016); Ma et al. (2017) are some of the gradient-based methods which require
at least one pass over all data samples to evaluate the gradient RLN(W) As the sample size N
becomes larger, these methods would be less efficient compared to stochastic gradient methods
where the gradient is approximated based on a small number of samples Johnson & Zhang (2013);
Roux et al. (2012); Defazio et al. (2014); Shalev-Shwartz & Zhang (2013); Konecny & Richtarik
(2017); Nguyen et al. (2017).
Second order methods are well known to share faster convergence rate by utilizing the Hessian
information. Recently, several papers Byrd et al. (2015); Schraudolph et al. (2007); Mokhtari &
Ribeiro (2015) have studied how to apply second orders methods to solve ERM problem. However,
getting the inverse of Hessian matrix ofa good approximation ofitis always quite expensive, leading
to a significant difficulty on applying these methods on large scale problems.
Following the idea of adaptive sample size discussed in Mokhtari & Ribeiro (2017); Eisen et al.
1
Under review as a conference paper at ICLR 2018
(2017); Mokhtari & Ribeiro (2016), the complexity of Newton’s method can be reduced (Mokhtari
& Ribeiro, 2016) if the dimension d is small, but it is impractical to compute the Hessian inverse
for large dimensional problems. In order to decrease the cost of computing the Hessian inverse,
Eisen et al. (2017) proposed the k-Truncated Adaptive Newton (k-TAN) approach. In this method,
the inverse of such approximated Hessian is calculated by increasing the sample size adaptively and
using a rank-k approximation of the Hessian. The cost per iteration is O((log k+n)d2). Again, note
that either when d is large, or in the case when k is close to d, this method can be quite inefficient.
In this paper, we propose an increasing sample size second-order method which solves the Newton
step in ERM problems more efficiently. Our proposed method, called Restarting NewtonCG (RNC)
method, starts with a tiny number of samples and only considering the corresponding empirical risk
based on these samples. This problem is solved up to some accuracy, and the solution of this stage
is a warm start for the next stage in which we solve the next empirical risk with a larger number
of samples, which contains all the previous samples. Such procedure is run iteratively until either
all the samples have been included, or we find that it is unnecessary to further increase the sample
size. Our RNC method combines the idea of increasing sample size and the inexact damped Newton
method discussed in Zhang & Xiao (2015) and Ma & Takac (2016). Instead of solving the NeWton
system directly, we apply preconditioned conjugate gradient (PCG) method as the solver for each
NeWton step. We shoW the required number of PCG steps in order to reach the statistical accuracy
of the full dataset.
Also, it is alWays a challenging problem of running first order algorithms such as SGD and Adam
Kingma & Ba (2014) in a distributed Way. HoWever, our algorithm is designed naturally to be easily
parallelized and shares the strong scaling property. While splitting the gradient and Hessian-vector
product computation based on local data stored across different machines, it is alWays expected
to get extra acceleration via increasing the number of computational nodes. We shoW that, under
distributed setting, our RNC algorithm is communication efficient in both theory and experiments.
We organize this paper as folloWing. In Section 2, We introduce the necessary assumptions and the
definition of statistical accuracy. Section 3 describes the proposed algorithm and its distributed ver-
sion. Section 4 explores the theoretical guarantees on complexity. In Section 5, We demonstrate the
outstanding performance of our algorithm in practice. Section 6 is the concludes our contribution.
2	Problem Formulation
In this paper, we focus on finding the optimal solution w* of (1). As described earlier, we are
trying to find a solution for the empirical loss function LN (w), Which is the statistical mean over N
samples. Now, consider the empirical loss Ln (w) associated with n ≤ N samples. In Bousquet &
Bottou (2007) and Bottou (2010s), the error between the expected loss and the empirical loss Ln is
calculated. In Mokhtari & Ribeiro (2016), it is mentioned that Ln approximates the expected loss
with statistical accuracy Vn for all w ∈ Rd with high probability(w.h.p),
sup |L(w) - Ln (w)| ≤ Vn.	(3)
w∈Rd
In other words, there exists a constant δ such that the inequality (3) holds with probability of at least
1 - δ. Generally speaking, statistical accuracy Vn depends on n (although it depends on δ too, but
for simplicity in notation wejust consider the size of the samples), and is of the order Vn = O(亲)
where γ ∈ [0.5, 1] (Vapnik (2013); Bousquet (2002); Bartlett et al. (2006)).
For problem (2), if we’ve found an approximate solution wn which satisfies the inequality Ln(wn) -
Ln(Wn) ≤ Vn, where Wn is the true minimizer of Ln, it is not necessary to go further and find a
better solution (a solution with less optimization error). The reason comes from the fact that for
a more accurate solution the summation of estimation and optimization errors does not become
smaller than Vn . Therefore, when we say that Wn is an Vn -suboptimal solution for the risk Ln , it
means that Ln(Wn) - Ln(Wn) ≤ Vn. In other words, Wn solves problem (2) to reach its statistical
accuracy.
It is crucial to note that if we add an additional term in the magnitude of Vn to the empirical loss Ln ,
the new solution is also in the similar magnitude as Vn to the expected loss L. Therefore, we can
regularize the non-strongly convex loss function Ln by CVnkWk2 and consider it as the following
2
Under review as a conference paper at ICLR 2018
problem:
min Rη(w) := 1 Pn=Ifi(W) + cVVn kwk2.
w∈Rd
(4)
The noticeable feature of the new empirical risk Rn is that Rn is cVn -strongly convex, where c is
a positive constant. Thus, we can utilize any practitioner-favorite algorithm. Specifically, we are
willing to apply the inexact damped Newton method, which will be discussed in the next section.
Due to the fact that a larger strong-convexity parameter leads to a faster convergence, we could
expect that the first few steps would converge fast since the values of cVn in these steps are large
(larger statistical accuracy), as discussed in Theorem 1. From now on, when we say wn is an Vn-
SUboPtimal solution of the risk Rn it means that Rn(Wn) - Rn(Wn) ≤ Vn, where Wn is the true
optimal solution of the risk Rn . Our final aim is to find wN which is VN -optimal solution for the
risk RN which is the risk over the whole dataset.
3 Restarting NewtonCG method with increasing sample size
The inexact damPed Newton method, which is discussed in the study of Zhang & Xiao (2015), is to
find the next iterate based on an aPProximated Newton-tyPe uPdate. It has two imPortant differences
comParing to exact Newton method. First, as it clear from the word “damPed”, the learning rate of
the inexact damPed Newton tyPe uPdate is not 1, since it dePends on the aPProximation of Newton
decrement. The second distinction is that there is no need to comPute exact Newton direction (which
is very exPensive to calculate in one steP). Alternatively, an aPProximated inexact Newton tyPe
direction is calculated by aPPlying an iterative Process to obtain a direction with desirable accuracy
under some measurement.
In order to utilize the imPortant features of ERM, we combine the idea of increasing samPle size
and the inexact damPed Newton method. In our ProPosed method, we start with handling a small
number of samPles, assume m0 samPles. We then solve its corresPonding ERM to its statistical
accuracy, i.e. Vm0, by inexact damPed Newton algorithm. At the next iteration, we increase the
number of samPles geometrically with rate of α, i.e., αm0 samPles. The aPProximated solution of
the Previous ERM can be used as a warm start Point to find the solution of the new ERM. The samPle
size increases until it equals the number of full samPles.
Consider the iterate Wm within the statistical accuracy of the set with m samPles, i.e. Sm for the risk
Rm . The inexact damPed Newton method with increasing samPle size finds the iterate Wn which is
Vn-SUboPtimaI solution for the sample set Sn, i.e. Rn(Wn) — Rn(Wn) ≤ Vn after Tn iterations. We
initialize Wo = Wm and consider the following update:
Wk+1 = Wk - ι+δnL(Wk)vk,	(5)
where vk is k-Newton direction defined as
Definition 1 (k -Newton direction).
∣N2 Rn(Wk)Vk- NRnIWk )k ≤ 4∙	(6)
Note that k has a crucial effect in the speed of the algorithm. We use preconditioned CG (PCG)
(by considering the preconditioned matrix P = Hn + μI, where Hn = 占 Pi∈A N2Rni (W)
and A ⊂ Sn) in order to find the vector vk, which is an approximate solution of the system
P-1V2Rn(Wk )vk
PTVRn(Wk) Moreover, δn(Wk) = JvTV2Rn(Wk)vk
is the approxi-
mation of (exact) Newton decrement. Also, we have
nn
VRn(W) = 1 X Vfi(W) + cVnW,	V2Rn(W) = 1 X V2fi(W) + cVnI.	(7)
Thus, after Tn-PCG iterations, Wn = WTn (see Theorem 1). Also, because of Ek = 0, vk is the exact
Newton direction, and the update in (5) is the exact damped Newton step. Furthermore, in Theorem
1 we show that the number of total PCG iterations to reach VN -optimal solution for the risk RN
is TN . It means that when we start with the iterate Wm0 with corresponding m0 samples, after TN
PCG iterations, we reach the point WN with statistical accuracy of VN for the whole dataset. Our
proposed method is summarized in Algorithm 1. In the inner for loop of Algorithm 1, in order
3
Under review as a conference paper at ICLR 2018
to calculate the approximate Newton direction and approximate Newton decrement, we use PCG
algorithm which is shown in Algorithm 2.
Thus, after Tn-PCG iterations, Wn = WTn (See Theorem 1). Also, We can note that when Ek = 0,
then vk is the exact Newton direction, and the update in (5) is the exact damped Newton step.
Furthermore, in Theorem 1 we show that the number of total PCG iterations to reach VN -optimal
solution for the risk RN is TN. It means that when we start with the iterate Wm0 with corresponding
m0 samples, after TN PCG iterations, we reach the point WN with statistical accuracy of VN for the
whole dataset.
Our proposed method is summarized in Algorithm 1. In the inner for loop of Algorithm 1, in order
to calculate the approximate Newton direction and approximate Newton decrement, we use PCG
algorithm which is shown in Algorithm 2.
Stopping Criterion Here we discuss two stopping criterions to fulfill the 10th line from Algo-
rithm 1. At first, considering Wn is unknown in practice, we can use strong convexity inequality as
Rn(Wk) - Rn(Wn) ≤ 2c^ ∣VRn(Wk) k2 to find a stopping criterion for the inner loop, which satis-
fies ∣∣VRn(iPk)k < (√2c)Vn. However, this stopping criterion can be too conservative in practice.
Another stopping criterion is discussed by Zhang & Xiao (2015), using the fact that the risk Rn is
self-concordant. This criterion can be written as δn(Wk) ≤ (1 一 β∖√Vn (see section 7.1), where
β ≤ 2io. The later stopping criterion implies that Rn(Wk) 一 Rn(Wn) ≤ Vn whenever Vn ≤ 0.682.
To compare these two criterions, the later criterion is more practical due to the fact that we have
δn(Wk) in every iteration. While we need to calculate the gradient of the risk Rn in each iteration of
the inner loop to use the first criterion.
Algorithm 1 Restarting NewtonCG algorithm
1	Initialization: Sample size increase constant a, initial sample size n = mo and Wn = Wm° with
	∣VRn(Wn)k < (√2c)Vn
2	: while n ≤ N do
3	:	Update Wm = Wn and m = n
4	: Increase sample size: n = max{αm, N}
5	Set W0 = Wm and set k = 0
6	: repeat
7	Calculate Vk and δn(Wk) by Algorithm 2 PCG
8	Set Wk+1 = Wk - 1+δr1(Wk) Vk
9	:	k=k+1
10	: until a stopping criterion is satisfied
11	Set Wn = Wk
12	: end while
Algorithm 2 PCG - Algorithm 2 in Zhang & Xiao (2015)	
1 2 3 4 5 6 7 8 9 10 11 12	Input: Wk ∈ Rd, Ek, and μ ≥ 0 Let H denote the Hessian VIRn(Wk) and P =占 Pi∈∕ V2Rn(Wk) + μI Set r(O) = VRn(Wk), S(O) = PTr(O), V(O) = 0, U(O) = S(O), t = 0 : repeat :	Calculate Hu(t) and HV (t) hr(t) s(t)i COmPute Yt = hU(t),Hu(ti)i :	Set V(t+1) = V(t) + γtu(t), r(t+1) = r(t) 一 γtHu(t) hr(t+1) s(t+1) i Compute βt = ' "找" i :	Set S(t+1) = P -1r(t+1), u(t+1) = S(t+1) + βtu(t) :	Set t = t + 1 until ∣∣rt+1k ≤ Ek return Vk = v(t+1) and δn(Wk) = JVTHv(t) + YtVTHu(t)
4
Under review as a conference paper at ICLR 2018
Distributed Implementation Similar to the algorithm in Zhang & Xiao (2015), Algorithm 1 and
2 can also be implemented in a distributed environment. Suppose the entire dataset is stored across
K machines, i.e., each machine stores Ni data samples such that PiK=1 Ni = N. Under this setting,
each iteration in Algorithm 1 can be executed on different machines in parallel with PiK=1 ni =
n, where ni is the batch-size on ith machine. To implement Algorithm 2 in a distributed way, a
broadcast operation is needed at each iteration to guarantee that each machine will share the same
Wk value. Moreover, the gradient and Hessian-vector product can be computed locally and later
reduce to the master machine. With the increasing of batch size, computation work on each machine
will increase while we still have the same amount of communication need. As a consequence,
the computation expense will gradually dominate the communication expense before the algorithm
terminates. Therefore the proposed algorithm could take advantage of utilizing more machines to
shorten the running time of Algorithm 2.
4 Convergence Analysis
In this section, first we define the self-concordant function. This kind of function has the property
that its third derivative can be controlled by its second derivative. By assuming that function f :
Rd → R has continuous third derivative, we define self-concordant function as follows.
Definition 2. A convex function f : Rd → R is Mf -self-concordant if for any w ∈ dom(f) and
u ∈ Rd we have
3
IuT(f (w)[u])u∣≤ Mf(UTV2f(w)u)2,	(8)
where f 000 (w)[u] := limt→o 1 (V2f (w + tu) - V2f (w)). As it is discussed in Nesterov (2013), any
self-concordant function f with parameter Mf can be rescaled to become standard self-concordant
(with parameter 2). There are many well-known empirical loss functions which are self-concordant
such as linear regression, Logistic regression and squared hinge loss. In order to prove our results
the following conditions are considered in our analysis.
Assumption 1. The loss functions f(w, z) are convex w.r.t w for all values of z. In addition, their
gradients Vf (w, z) are L-smooth
kVf(w, z) - Vf(w0,z)k ≤ Lkw - w0k, ∀z.	(9)
Assumption 2. The loss functions f(w, z) are self-concordant w.r.t w for all values of z.
The immediate conclusion of Assumption 1 is that both L(w) and Ln(w) are convex and L-smooth.
Also, we can note that Rn(w) is cVn-strongly convex and (cVn + L)-smooth. As it is discussed in
Zhang & Xiao (2015) we use the following auxiliary functions, which will be used in the analysis
of the self-concordant functions:
ω(t) = t 一 log(1 + t),	t ≥ 0. and	ω*(t) = —t — log(1 一 t),	0 ≤ t < 1.	(10)
The above functions can be very helpful in analyzing the self-concordant functions. Also, for the
risk Rn, the same as Zhang & Xiao (2015) we can define the following auxiliary vectors:
Un(Wk ) = [V2 Rn(Wk)]-1/2VRn(Wk )	and	Vn(Wk ) = [V2Rn(Wk )]1/2Vn.	(11)
We can note that kUn(Wk)k = /VRn(Wk)[V2Rn(Wk)]-1VRn(Wk), which is the exact Newton
decrement, and, the norm ∣∣Vn(Wk)k = δn(Wk) which is the approximation of Newton decrement
(and Un(Wk) = Vn(Wk) in the case when Ek = 0).
In the rest of this section, we analyze the upper bound for the number of iterations needed to solve
every subproblem up to its statistical accuracy.
We prove a linear convergence rate for our algorithm. We analyze the case when we have Wm which
is a Vm-suboptimal solution of the risk Rm, and we are interested in deriving a bound for the number
of required iterations, Tn, to ensure that Wn is a Vn-suboptimal solution for the risk Rn. We use the
analysis of DiSCO algorithm discussed in Zhang & Xiao (2015) to find the bound for Tn .
Theorem 1. Suppose that Assumptions 1 and 2 hold. Consider Wm which satisfies Rm (Wm) —
Rm(Wm) ≤ Vm and also the risk Rn corresponding to sample set Sn ⊃ Sm where n = am, α > L
Set the parameter Ek (the error in (6)) as following
Ek= β( L+VVvn )1/2kVRn(Wk )k,	(12)
5
Under review as a conference paper at ICLR 2018
where β ≤ 击.Then the variable Wn is an Vn -SuboPtimal solution for the risk Rn, i.e Rn (Wn) 一
Rn(Wn) ≤ Vn if the number of iterations Tn satisfies in thefollowing:
Tn ≥ (lRn(Wm)-Rn(U)m +1log2(中)])("CVn)】吗(2⅛+Li)]), WhP.(13)
Here dte shows the smallest nonnegative integer larger than or equal to t.
By utilizing Theorem 1, the following number of iterations, TN , is needed to reach the statistical
accuracy of VN of the full training set with high probability:
|P|	J_________
tn ≥ x(i Rp[i](wp[M)—『(wp [i]) m+1 iog2(十)])iq^+⅛ i0g2( ¾+L2)m，
i=2
(14)
where P = {m0, αm0, α2m0, . . . , N}. Also, based on the result in (13), by considering the risk
Rn, we can note that when the strong-convexity parameter for the mentioned risk (cVn) is large, less
number of iterations are needed (or equally faster convergence is achieved) to reach the iterate with
Vn-suboptimal solution; and this happens in the first steps.
Corollary 1. SuPPose that AssumPtions 1 and 2 hold. By assuming that Wm is Vm -suboPtimal
solution and also consider the risk Rn corresPonding to samPle set Sn ⊃ Sm where n = 2m. If we
set parameter Ek (the error in (6)) as (12), then after Tn iterations, where with high probability:
Tn ≥ (i⅛h¾⅞w⅛m m+1 iog2( 2ω≡)])(ιq∏⅛ i0g2( 2cv+L)m),
(15)
we reach the point Wn with statistical accuracy of Vn for the risk Rn. Moreover, after TN iterations
we reach a point with the statistical accuracy of VN of the full training set:
Tn ≥(2iog2 mo+("s： ωi2+Γk2))	Vmj
+ l0g2 m lθg2( 2ωVN/6) )! (iq(1 + CVT)l0g2 (2 + 2L . VN)]) , w.h.p,	(16)
where m0 is the number of initial sample.
By Corollary 1, We can notice that TN = O(γ(log2 N)2√Nγl0g2 NY), and when Y = 1, we have
I1
TN = O((l0g2 N)3√N), andfor Y = 0.5, the result is TN = O((l0g2 N)3N4).
5 Numerical Experiments
In this section, we present numerical experiments on several large real-world datasets to show that
our restarting NewtonCG algorithm can outperform other existed methods on solving both convex
and non-convex problems. Also, we compare the results from utilizing different number of machines
to demonstrate the nice scaling property for our algorithm. All the experiments are performed on a
cluster with 24 Xeon E5-2620 CPUs (2.40GHz), and all the algorithms are implemented in Python
with PyTorch library. In the plots, we use the pink vertical dashed lines to represent when restarting
happens in our NewtonCG algorithm.
Convex case First, we compare our restarting NewtonCG algorithm with two other distributed
optimization algorithms CoCoA Smith et al. (2016) and Disco Zhang & Xiao (2015), on solving
convex problems. We choose these two algorithms in consideration of attaining a fair comparison
between distributed first-order (CoCoA) method and distributed second-order (DiSCO) approach.
Binary classification tasks based on two datasets rcv1 and news20 chosen from Chang & Lin (2011)
are solved using logistic regression model. We choose this two datasets following the principle
from Zhang & Xiao (2015), where those two datasets show different relations between number
of features and number of data samples (larger and smaller). The empirical loss function we are
trying to minimize is stated as in (4). We use logistic loss function defined as fi(W) := l0g(1 +
6
Under review as a conference paper at ICLR 2018
1.0
αβ
lαe
04
02
ERM, rcvjrain
3 CoCoA
Dlseo
NewtonCG
----Restart
10
1$
epochs
20
2S
ERM, rcvjrain
02
-i CoCoA
Disco
T- NewtonCG
----Restart
S
β4
2S
ERM, rcvjrain
αβ
l0β
04
CoCoA
-→- Disco
w —NewtonCG
O
5
10
15
20
epochs
-Disco
w NewtonCG
300
2S0
300
ERM, neun20
O	50 IOO 150	2∞
running-time
«00
CoCoA
Dteco
NewtonCG
o Sa) ιm is® sæo æoo wæ XOo
running-time
Figure 1: Restarting Newton-CG v.s. DiSCO v.s CoCoA for Logistic Regression
15	20 '	25	30 K 40
epochs
O
50
100
150
a»
2S0
running-time
ERM, nevra20
ERM, neun20
D
5
,0 ;A√MC
exp(-yiwT xi)), where xi ∈ Rd is data sample and yi ∈ {-1, 1} are binary label. Note that there
is a fixed regularization parameter 10-6 in DiSCO and CoCoA, while our restarting NeWtonCG has
Regularization of 1/√√m which depends on the size of samples m.
We run all these three algorithms using 8 nodes. The starting batch-size on each node for restarting
NewtonCG is set to 16 for a faster beginning, while other two will go over the whole dataset at
each iteration. For restarting NewtonCG implementation, number of samples used to form the new
ERM loss are doubled from previous iteration after each restarting. Furthermore, restarting happens
whenever norm of loss gradient is lower than 1 /√m.
From Figure 1, we observe consistently that the restarting NewtonCG algorithm has a better per-
formance over the other two in the begin stages. Both loss value and training accuracy under our
restarting NewtonCG algorithm converges to optimality by passing a very small number of samples,
which suggests that the restarting NewtonCG can find a good solution in a warm starting manner.
Compared with DiSCO, our restarting approach helps to get rid of spending too much computation
at the beginning iterations, where second order methods are usually less efficient than first order
methods. Also, our algorithm can still converge fast when we are close to optimal solution, while
first order methods become weak since the gradient becomes more and more close to zero.
Non-convex case Even though the iteration complexity analysis only covers the convex case, we
want to point out that our algorithm is also able to handle nonconvex problems efficiently. In this
section, we compare the performance of increasing sample size method with the well known Adam
Kingma & Ba (2014) method on solving convolution neural network. We do experiments on the
standard image classification dataset Mnist with a 5-layer convolutional neural network. In Fig-
7
Under review as a conference paper at ICLR 2018
2β
1J1
Oe
NalveCNet, Mnlst
→-.
Adafn
NerftonCG
Restart
M0β1-0 1.βZ0ZβMaβ44)
epochs
1Λ
OS
船
S
04 1j
02 ɪ
(M)
NaIveCNet, Mnlst
(M)
Figure 2: Restarting Newton-CG v.s. Adam using NaiveCNet on Mnist dataset
AdaH
-→-- NevfmnCG
---ReStart
0β 1Λ 1.β	2J> ZS M 3jS ΛJ>
epochs

n
8
ure 2, we compare Adam with our restarting NewtonCG approach. The Adam is implemented
using bulid-in optimizer in pytorch library, and we choose the best batch-size as 64 from the range
{16, 32, 64, 128} and initial learning rate as 0.005 from the range {0.001, 0.005, 0.01, 0.05}. Re-
garding our restarting NewtonCG, we experiment on 32 nodes. The initial batch size are set to 8 for
each node, i.e, set 256 as our initial total batch size across all nodes. As it is clear shown in Figure 2,
our restarting NewtonCG with 32 nodes could outperform serial Adam. Note that Adam, i.e., the
stochastic first-order method variant, can not be distributed easily, since a small batch size is require
to have start-of-art performance He et al. (2016). While we could further improve our restarting
approach by utilizing more nodes.
Figure 3: Performance of restarting NewtonCG algorithm with different computing threads.
Strong scaling As the last experiment, we demonstrate that our restarting NewtonCG algorithm
shares a strong scaling property. As shown in Figure 3, whenever we increase the number of nodes,
we can always obtain acceleration towards optimality. The leftmost plot in Figure 3 shows that the
speed of passing over data increases along the increase of number of nodes used, since distributed
computation among nodes will obtain the gradient and Hessian-vector product faster. As it is shown
in rightmost plot in Figure 3, to reach 0.96 testing accuracy, itis about 12 times slower by only using
1 node than using 32 nodes.
6 Conclusion
We propose a restarting NewtonCG method with increasing sample size strategy to solve the ex-
pected risk minimization problem. Our algorithm can converge to a low statistical accuracy in very
few epochs and also be implemented in a distributed environment naturally. We show linear conver-
gence rate for convex empirical risk minimization under mild assumptions. Numerical experiments
are presented to demonstrate the advantages of our proposed algorithm on both convex and non-
convex problems.
8
Under review as a conference paper at ICLR 2018
References
Peter L Bartlett, Michael I Jordan, and Jon D McAuliffe. Convexity, classification, and risk bounds.
Journal of the American Statistical Association,101(473):138-156, 2006.
Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse
problems. SIAM journal on imaging sciences, 2009.
Leon Bottou. Large-scale machine learning with stochastic gradient descent. In Proc. of COMP-
STAT, 2010s.
olivier Bousquet. Concentration Inequalities and Empirical Processes Theory Applied to the Anal-
ysis of Learning Algorithms. PhD thesis, Biologische Kybernetik, 2002.
Olivier Bousquet and Leon Bottou. The tradeoffs of large scale learning. NIPS, 2007.
Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.
Richard H. Byrd, Samantha L. Hansen, Jorge Nocedal, and Yoram Singer. A stochastic quasi-newton
method for large-scale optimization. SIAM J. Optim., 26(2), 10081031. (24 pages), 2015.
Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology, 2:27:1-27:27, 2011. Software available at
http://www.csie.ntu.edu.tw/~cjlin∕libsvm.
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method
with support for non-strongly convex composite objectives. NIPS, 2014.
Dmitriy Drusvyatskiy, Maryam Fazel, and Scott Roy. An optimal first order method based on
optimal quadratic averaging. arXiv preprint arXiv:1604.06543, 2016.
Mark Eisen, Aryan Mokhtari, and Alejandro Ribeiro. Large scale empirical risk minimization via
truncated adaptive newton method. arXiv preprint arXiv:1705.07957, 2017.
Xi He, Dheevatsa Mudigere, Mikhail Smelyanskiy, and Martin Takac. Large scale distributed
hessian-free optimization for deep neural network. arXiv preprint arXiv:1606.00511, 2016.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. NIPS, 2013.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Jakub Konecny and Peter Richtarik. Semi-stochastic gradient descent methods. Frontiers in Applied
Mathematics and Statistics, 2017.
Chenxin Ma and Martin Takac. Distributed inexact damped newton method: Data partitioning and
load-balancing. arXiv preprint arXiv:1603.05191, 2016.
Chenxin Ma, Naga Venkata C Gudapati, Majid Jahani, Rachael Tappenden, and Martin Takac. Un-
derestimate sequences via quadratic averaging. arXiv preprint arXiv:1710.03695, 2017.
Aryan Mokhtari and Alejandro Ribeiro. Global convergence of online limited memory bfgs. Journal
of Machine Learning Research, 16(1):3151-3181, 2015.
Aryan Mokhtari and Alejandro Ribeiro. Adaptive newton method for empirical risk minimization
to statistical accuracy. arXiv preprint arXiv:1605.07659, 2016.
Aryan Mokhtari and Alejandro Ribeiro. First-order adaptive sample size methods to reduce com-
plexity of empirical risk minimization. arXiv preprint arXiv:1709.00599, 2017.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2013.
Lam Nguyen, Jie Liu, Katya Scheinberg, and Martin Takac. Sarah: A novel method for machine
learning problems using stochastic recursive gradient. arXiv preprint arXiv:1703.00102, 2017.
9
Under review as a conference paper at ICLR 2018
Nicolas L Roux, Mark Schmidt, and Francis R Bach. A stochastic gradient method with an expo-
nential convergence _rate for finite training sets. In Advances in Neural Information Processing
Systems,pp. 2663-2671, 2012.
Nicol N Schraudolph, Jin Yu, and Simon Gunter. A stochastic quasi-newton method for online
convex optimization. In Artificial Intelligence and Statistics, pp. 436-443, 2007.
Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized
loss minimization. Journal of Machine Learning Research, 2013.
Virginia Smith, Simone Forte, Chenxin Ma, Martin Takac, Michael I Jordan, and Martin Jaggi.
Cocoa: A general framework for communication-efficient distributed optimization. arXiv preprint
arXiv:1611.02189, 2016.
Vladimir Vapnik. The nature of statistical learning theory. Springer, 2013.
Yuchen Zhang and Lin Xiao. Communication-efficient distributed optimization of self-concordant
empirical loss. arXiv preprint arXiv:1501.00263, 2015.
7 Appendix
Before talking about the main results, two following lemmas are used in our analysis.
Lemma 1. (Lemma 4 in Zhang & Xiao (2015)) Suppose Assumption 1 holds and ∣∣HHn -
▽2Rn(Wk)k ≤ μ. Then, Algorithm 2, after Tμ iterations calculates Vn such that ∣N2Rn(Wk)vn 一
▽Rn(Wk)k ≤ Ek, where
(17)
Lemma 2. (Proposition 5 in Mokhtari & Ribeiro (2016)) Consider the sample sets Sm with size
m and Sn with size n such that Sm ⊂ Sn. Let Wm is Vm -suboptimal solution of the risk Rm. If
assumptions 1 and2 hold, then the following is true:
Rn(Wm) - Rn(Wn ) ≤ Vm + 2⅛mi (Vn-m + Vm) + 2(Vm - Vn)+ C(VvVn) ∣∣W*k2, WhP (18)
If we consider Vn = O( n1γ) where Y ∈ [0.5,1], and assume that n = 2m (or a = 2), then (18) can
be written as:
Rn(Wm)- Rn(Wn) ≤ [3 + (1 — ⅛Y) (2 + C llW* k2) ] Vm.	(19)
7.1	Practical stopping criterion
As a result of Theorem 1 in the study Zhang & Xiao (2015), we have:
(1 一 β)∣∣Un(Wk)k ≤ IIvn(Wk)k ≤ (1 + β)∣∣Un(Wk)∣∣,	(20)
where β ≤ 击.Also, by the equation in (11), we know that ∣vn(^^k) k = δn(Wk). As it is discussed
in the section 9.6.3. of the study Boyd & Vandenberghe (2004), we have ω*(t) ≤ t2 for 0 ≤ t ≤
0.68.
According to Theorem4.1.13 in the study Nesterov (2013), if ∣Un(Wk)k < 1 we have:
ω(∣∣Un(Wk)k) ≤ Rn(Wk) - Rn(Wn) ≤ ω*(∣∣Un(Wk)k).	(21)
Therefore, if ∣Un(Wk)∣ ≤ 0.68, we have:
Rn(Wk ) - Rn(Wn) ≤ ω*(∣∣Un(Wk )∣) ≤ Ilun(Wk )∣2
≤ (1—β)2 kvn(Wk)Il2 = (I—β)2 δn(Wk)	(22)
Therefore, we can note that δn(Wk) ≤ (1 - β)√^ concludes that Rn(Wk) - Rn(Wn) ≤ Vn when
Vn ≤ 0.682 .
10
Under review as a conference paper at ICLR 2018
7.2	Proof of Theorem 1
According to the Theorem 1 in Zhang & Xiao (2015), we can derive the iteration complexity by
starting from wm as a good warm start, to reach wn which is Vn -suboptimal solution for the risk
Rn . By considering assumption 2, we can assume that Rn is a standard self-concordant function.
According to the Corollary 1 in Zhang & Xiao (2015), we can note that if we set k the same as (12),
after K iterations We reach the solution Wn such that Rn(Wn) - Rn(Wn) ≤ Vn where
K = [Rn(wm)-⅞(wn)] + [ log2( 2ωVn/6))].	(23)
Also, according to Lemma 1, we can note that the number of PCG steps needed to reach the approx-
imation of Newton direction with precision k is as following:
≡[ Q⅞) i0g2O ].
(24)
Therefore, we can note that when we start from Wm, which is Vm-suboptimal solution for the risk
Rm, after Tn PCG steps, where Tn ≥ KTμ, we reach the point Wn which is Vn-SUboPtimal solution
of the risk Rn , which follows (13).
Suppose the initial sample set contains m0 samples, and consider the set P =
{m0, αm0 , α2m0, . . . , N}, then after TN PCG steps, we reach VN -optimal solution for the whole
data set:
|P|	J_________
TN ≥ X(i Rp[i](wp[M)-R)p[i](wρ [i]) m+1 i0g2( 2⅛? )])iq1+⅛ i0g2( ¾L2)].
i=2
(25)
7.3	Proof of Corollary 1
The proof of the first part is trivial. According to Lemma 2, we can find the upper bound for
Rn(Wm) — Rn(Wn), and when α = 2,by utilizing the bound (19) we have:
κ=[Rn(wm)-⅞(wn)] + [log? (2ω詈)]
≤ 1(3+(1-2γ 2⅛2rk2))Vm m+1 i0g2(中
(26)
：=K
Therefore, we can note that when we start from Wm , which is Vm -suboptimal solution for the risk
Rm, after Tn PCG steps, where Tn ≥ KTU, TU is defined in (24), we reach the point Wn which is
Vn-suboptimal solution of the risk Rn, which follows (15).
Suppose the initial sample set contains m0 samples, and consider the set P =
{mo, 2mo, 4m0,..., N}, then the total number of PCG steps, TN, to reach VN-optimal solution
11
Under review as a conference paper at ICLR 2018
for the whole data set is as following:
X ∕Γ (3+(1-2Y )(2 + C kw*k2
⅛ω(i∕6)
i=2
上］+门总寸）DD（lQ
2μ )
CVP[i] '
噫（写I）D
(27)
12