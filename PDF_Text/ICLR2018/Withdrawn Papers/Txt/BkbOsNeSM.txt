Under review as a conference paper at ICLR 2018
FastNorm: Improving Numerical Stability
of Neural Network Training with Efficient
Normalization
Anonymous authors
Paper under double-blind review
Ab stract
We propose a modification to weight normalization techniques that provides the
same convergence benefits but requires fewer computational operations. The pro-
posed method, FastNorm, exploits the low-rank properties of weight updates and
infers the norms without explicitly calculating them, replacing an O(n2) computa-
tion with an O(n) one for a fully-connected layer. It improves numerical stability
and reduces accuracy variance enabling higher learning rate and offering better
convergence. We report experimental results that illustrate the advantage of the
proposed method.
1 Introduction
Achieving efficient convergence while training deep networks is a known challenge. One of the
reasons for this difficulty is the change in the distributions of layer inputs caused by the updates in
the parameter values in the course of training. This phenomenon, known as the internal covariate
shift (Shimodaira, 2000), skews activation distributions over time. This distortion gets amplified
along the depth of the network may in turn lead to the vanishing gradient problem (Hochreiter et al.,
2001) impeding or preventing convergence.
Normalization is a common approach to limit the internal covariate shift and to keep parameters
from reaching extreme values. In particular, batch normalization introduced by Ioffe & Szegedy
(2015) is very successful in stabilizing and accelerating the learning process whitening layer inputs
for each training mini-batch.
While very successful, batch normalization relies on sufficiently large batches to work effectively.
More recent techniques adapt the approach of batch normalization to smaller batches or to online
learning tasks. They include Batch Renormalization (Ioffe, 2017) and Layer Normalziation (Ba
et al., 2016).
Another class of algorithms normalize the weights rather than layer inputs. Weight Normallization
(Salimans & Kingma, 2016) and Normalization Propagation (Arpit et al., 2016) have convergence
properties comparable to batch normalization without relying on mini-batches to work. However,
normalizing a weight matrix directly is computationally expensive, as it requires processing each
entry of the matrix, taking the square root of the sum of squares, and finally updating each entry of
the matrix.
We propose FastNorm, a purely mathematical acceleration of weight normalization techniques. For
the fully connected case we derive a method to track the norm of the weights without computing
it and we apply those norms efficiently to avoid updating the weights explicitly. Because total
number of operations performed is reduced but the internal covariate shift is still mitigated, training
proceeds with more stability, and a higher learning rate can be used. In the convolutional case, we
can generalize the method to compute the norms in the standard manner but continue to apply them
more efficiently.
1
Under review as a conference paper at ICLR 2018
W
h
×
No normalization
W
Wi/kWik
Explicit normalization
Implicit normalization
1
W
WT
×
No normalization
Figure 1: Weight normalization: forward pass.
W T
WT
百
×
Explicit normalization
WT
×
Implicit normalization
×
t
δ
h
δ
δ
t
1
W
Figure 2:	Weight normalization: delta pass.
2 Implicit weight normalization
On each update weight normalization methods adjust the weight matrix to normalize its rows. For
a fully connected layer the application of the weight matrix W to the input h on the forward pass
becomes
zi = γi
k(wikhk)
VZPmWm
+ βi
Wih , β
YikWi∙k + βi
〜
(1)
=γiWi∙h + βi,
where Wi- and Wi are the i-th row of the original and normalized weight matrices and β and Y
are trainable rescaling parameters (Arpit et al., 2016). The gradient with respect to weight matrix
entries is
∂zi	_	IIWi∙khj- kWWijkWih
河=Yi	kWiτ
(2)
and once the corresponding error term δ is computed, the update for a row of the weight matrix with
learning rate α can be written as
Wi0 = Wi
S	IIWiIIhT -舟Wih
-iYi	IWT
(3)
As expected, the normalization procedure forces the gradient to be orthogonal to the matrix row Wi .
We can use it to compute the norm of the updated Wi0.
IIWi0II2=Wi0Wi0T
IIWiII2 + f IIWiII2hT h—WWh)2 + (Wih)2
IIWi II
∣∣w ∣∣2, α2δi2γi2 (hTh	(Wih)2ʌ
iiWiIT E Ih h-TWiM.
(4)
Note, that unlike explicit normalization of the weight matrix, computation of row norms using the
update expression (4) doesn’t require O(n2 ) operations. Instead, it requires to compute a single
2
Under review as a conference paper at ICLR 2018
>U2DUU< >USDUU<
80%-
60%-
4O%-
20%-
1
4
5
2	3
Epochs
0%-.
O
Figure 3:	Convergence of a 2-layer fully-connected network with MNIST data set. Validation accu-
racy was sampled every 100 iterations.
vector norm hTh amortized over all values ofi and reuses already computed values Wih brining the
total number of operations to O(n).
Instead of explicitly normalizing W at each step we can keep the matrix unnormalized but define
ti = 1/kWi k and maintain a vector t with these scalars. In this notation the forward pass (1)
becomes
zi = γiti (Wih) + βi .	(5)
Figure 1 shows a schematic comparison of explicit and implicit normalization (we removed the term
γ for simplicity). The update for the norm-tracking vector expressed from (4) can be written as
ti = (t2+α2γ2δ2t2(||训2-(Wih)2t2)).	⑹
Finally, we need to adjust the back propagation pass to include the effect of normalization. When
propagating the error using the unmodified weight matrix we need to apply normalization factors ti
to mimic the effect of multiplying by the scaled matrix.
We use δ0 to denote the δ passed to the next layer. Ordinarily, δ0 = WT (γ δ), where we use for
component-wise multiplication. Explicit normalization will follow the same patter with normalized
matrix W
WT (γ Θ δ)
As in the forward pass, for implicit normalization we use the original weight matrix W and absorb
normalization step in component-wise multiplication by t (Figure 2).
δ0=W0T(tΘγΘδ)	(7)
Equations (5,6,7) define FastNorm algorithm. We assume that the weight matrix is normalized after
initialization and subsequent implicit updates are tracked by scalars ti . In practice we can perform
3
Under review as a conference paper at ICLR 2018
infrequent explicit normalizations (are reset ti values to 1) to make sure that tracking scalars are
fully representable especially if the computation is performed in low precision. Our experiments
show that explicit normalization can be done approximately once per epoch for 16-bit computations
to keep tracking scalars within the representable range. Extra computational cost of such infrequent
normalizations is negligible.
3	Computational properties
3.1	Theoretical savings
For a fully connected layer with m inputs and n outputs both explicit weight normalization and pro-
posed method require m square roots and m inverse operations. In addition explicit normalization
requires 2n operations to compute the norm of each of m rows. It also requires mn operations to
rescale the weight values resulting in normalization cost of 3mn plus m divisions and m square
roots.
A normalization step performed by the proposed algorithm requires 2n operations to compute khk2,
then for each row it can reuse already computed value (Wih)ti to compute the difference term in
(6) in two operations with additional 8 operations to compute the rest of the expression in (6) plus a
division and a square root. The back propagation step adds another m operations to scale the result
with t. The total operation count for that FastNorm implicit normalization adds is n + 11m plus m
divisions and m square roots.
The computational complexity reduction is possible because at each step the weight matrix under-
goes a simple low rank update: without normalization it’s just the outer product of activation and
error vectors. Because of that we can recalculate updated norms of rows of the weight matrix without
explicit normalization.
3.2	Numerical stability
Computational savings have another positive effect: the reduction in the number of operations from
O(n2) to O(n) also reduces the accumulated rounding error leads to a more stable training process.
We used a two-layer fully connected network with the MNIST data set to compare NormProp and
FastNorm. Figure 3 compares the convergence behavior for this network for both techniques. Just
observing the shape of the curves we can conclude that applying FastNorm results in a more steady
increase in accuracy.
To quantify the variability over time, we compare the standard deviations of accuracy levels for both
methods (Figure 4). We observe close to five-fold reduction in variance when changing the explicit
normalization to the implicit scheme of FastNorm.
Because of the gained stability, we can use FastNorm with a slightly higher learning rate. We
observed that NormProp diverges with a learning rate of 0.07, but FastNorm can handle a learning
rate of 0.075. In one epoch, with a learning rate of 0.06, NormProp achieved 86.47% accuracy while
FastNorm achieved 90.97% accuracy with a learning rate of 0.075. Even when both networks were
run with the same learning rate of 0.03, FastNorm slightly outperformed NormProp in resulting
accuracy.
4	Generalizations of the method
The weight matrix update (3) uses the gradient computed from a single sample. More generally, the
Stochastic Gradient Descent method applies the update averaged over a mini-batch of samples. For
small batches that results in a low-rank update to the weight matrix and can be handled similarly to
the approach outlined in Section 2. As the batch size grows the method leads to diminishing returns
and ultimately becomes less efficient that explicit normalization.
Similarly, convolution layers require special handling. The derivation presented in Section 2 doesn’t
directly apply to convolutions because of the specific structure of corresponding computations.
4
Under review as a conference paper at ICLR 2018
0%
0
SPsP SP SP SP SP
9 9 9 9,9
6 5 4 3 2 1
uo>ɑ p-pus
1
2	3
Epochs
4
5
Figure 4: Standard deviations of accuracies (measured every 100 iterations) using a sliding window
of size 100 samples for two-layer network normalized with NormProp, and FastNorm.
An extension to the proposed method is a hybrid approach in which we compute the norms explicitly
but don’t apply them to the weights and use scalars ti to track the effects of normalization. This still
results in numerical savings: an O(n2) operation of weight matrix rescaling gets replaced with an
O(n) scheme of implicit normalization.
5	Conclusions
FastNorm provides a normalization method that is mathematically equivalent to explicit weight
normalization in infinite precision but provides computational savings and increased stability in the
network by implicitly normalizing weights. Because of that stability, FastNorm tolerates a higher
learning rate and as the result reaches convergence faster.
The approach or applying implicit or lightweight normalization can be extended to cases that include
batching and convolutions.
References
Devansh Arpit, Yingbo Zhou, Bhargava Urala Kota, and Venu Govindaraju. Normalization prop-
agation: A parametric technique for removing internal covariate shift in deep networks. CoRR,
abs/1603.01431, 2016.
Lei Jimmy Ba, Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR, abs/1607.06450,
2016.
Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jrgen Schmidhuber. Gradient flow in recurrent
nets: the difficulty of learning long-term dependencies, 2001.
Sergey Ioffe. Batch renormalization: Towards reducing minibatch dependence in batch-normalized
models. CoRR, abs/1702.03275, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. CoRR, abs/1502.03167, 2015.
Tim Salimans and Diederik P. Kingma. Weight normalization: A simple reparameterization to ac-
celerate training of deep neural networks. In Advances in Neural Information Processing Systems
29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016,
Barcelona, Spain, pp. 901, 2016.
Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-
likelihood function. Journal OfStatistical Planning and Inference, 90(2):227-244, October 2000.
5