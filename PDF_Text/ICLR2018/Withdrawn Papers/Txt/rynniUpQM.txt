Under review as a conference paper at ICLR 2018
Learning with Mental Imagery
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we propose deep convolutional generative adversarial networks (DC-
GAN) that learn to produce a “mental image” of the input image as internal rep-
resentation of a certain category of input data distribution. This mental image
is what the DCGAN ’imagines’ that the input image might look like under ideal
conditions. The mental image contains a version of the input that is iconic, with-
out any peculiarities that do not contribute to the ideal representation of the input
data distribution within a category. A DCGAN learns this association by training
an encoder to capture salient features from the original image and a decoder to
convert salient features into its associated mental image representation. Our new
approach, which we refer to as a Mental Image DCGAN (MIDCGAN), learns fea-
tures that are useful for recognizing entire classes of objects, and that this in turn
has the benefit of helping single and zero shot recognition. We demonstrate our
approach on object instance recognition and handwritten digit recognition tasks.
1	Introduction
Deep convolutional neural networks have had a revolutionary impact on machine learning and com-
puter vision, yet we still fall dramatically short when it comes to learning in a manner that is most
similar to people. Consider the way that children interact with objects when they are very young
(James et al., 2014; Yu et al., 2009): during their interaction, children look at objects from many
different perspectives. Eventually they build up a preference for certain viewpoints after examining
objects for a long period of time. In this paper, we consider the question of what would happen if we
were to train a deep convolutional generative adversarial network in the same manner. For this, we
provide the mental image (i.e., an ideal representation), and then provide samples of many different
variations of the input image. The Mental Image DCGAN (MIDCGAN) is trained to associate each
of these samples in a specific input distribution back to the mental image. This association is learned
using a GAN architecture (see Fig. 2) with a generator composed of an encoder and decoder. MID-
CGAN trains the encoder to learn salient bottleneck features for each class while the decoder learns
to generate a mental image from bottleneck features. We will show that MIDCGAN bottleneck fea-
tures are better suited for learning than those features that are generated without the benefit of using
a mental image.
Stated more formally, a typical learning task seeks to learn the data distribution, p(x) mapping to a
class or category label y or p(y|x). The diversity of samples in the training data are limiting in the
way the learner represents the category class internally. The MIDCGAN approach on the other hand
provides a mental image X as target to be learned and stored as representation of a category target
distribution. The learner maps the input data distribution, p(x), to this canonical representation
of the category, X, i.e p(X|x). During this mapping process, the MIDCGAN creates an internal
bottleneck feature vector that is best representative of the input distribution mapping to the mental
image.
We demonstrate the effectiveness of mental image DCGANs (MIDCGAN) on two different prob-
lems. First, we demonstrate this for handwritten digits as proof of concept. In this case, we assume
that a helpful tutor has provided an ideal representation of the digit, so the mental image is a stencil
(Fig. 3. When the MIDCGAN sees a digit, it is trained to think of how it might look ifit were look-
ing at the stencil. We mainly demonstrate the performance of MIDCGAN on instance based object
recognition. In this case, the helpful tutor provides the system with an iconic view of the object. The
MIDCGAN observes the objects from different viewpoints, but at each time it is trained to think of
how the object might look like if it were looking at the object from an ideal or iconic perspective.
1
Under review as a conference paper at ICLR 2018
Figure 1: MIDCGAN input (left) and generated mental image (right). The images are selected from
the BigBird database (Singh et al., 2014).
We evaluate this in three different ways. First, we evaluate quantitatively the usefulness of the bot-
tleneck features from MIDCGAN on learning tasks (comparing to DCGAN features trained without
a mental image). Second, we evaluate qualitatively MIDCGAN’s ability to generate mental images.
Finally, we evaluate MIDCGAN’s ability to perform few shot recognition on objects whose mental
image was not learned or transfer learning. More precisely how does MIDCGAN perform when
asked to imagine what an object in its frontal view, when it has never seen the object before.
2	Related work
Original GAN and Derivatives: After the introduction of the original generative adversarial net-
works (GAN) (Goodfellow et al., 2014a), several architectures and improvements on training GANs
were proposed such as improved GAN training (Salimans et al., 2016), categorical GAN Springen-
berg (2015), and InfoGAN (Chen et al., 2016).
Motivation to MIDCGAN: MIDCGAN is inspired partly by the work of image-to-image mapping
(Isola et al., 2016) and image editing (Wang & Gupta, 2016) and inpainting using GANs (Pathak
et al., 2016). Zhu et al. (Zhu et al., 2016b) also explored mapping 3D target from multiple 2D
projections. We propose mapping a canonical target image from a single viewpoint from various
inputs in terms of pose-invariance or style.
Partial Supervision with GAN for classification: GANs have been employed to learn expres-
sive features for classification in unsupervised (Radford et al., 2015) and semi-supervised manner
(Salimans et al., 2016) (Springenberg, 2015).
Rotation and Pose Invariance: Another closely related topic is invariance to Affine transforms.
Spatial transformer networks (Jaderberg et al., 2015) combine localization and transformation lay-
ers to provide invariance. However, MIDCGAN learns this invariance implicitly by learning to map
samples to canonical representation of the distribution. Pose invariance learning was also demon-
strated (LeCun et al., 2004) on the NORB dataset. MIDCGAN handles pose invariance learning
and exploits the canonical mapping to learn expressive features for either object class and instance
inference without any constraint on the view point of objects.
Single-Shot Recognition: Single shot learning is of particular importance where there is limited
training data. Held et al.(Held et al., 2016) proposed single-shot deep learning through multi-phase
supervised pre-training. We approach the problem from a different perspective, where we map
an instance of an object back to a canonical viewpoint using MIDCGAN. This allows single shot
recognition performance on par to the pre-trained ones without actually being pre-trained on a larger
similar dataset.
3	Methodology
DCGAN learns generative models using the joint adversarial training of a generator network and
discriminator network. The generator maps a noise vector Z to generated image using X = Gθg (Z)
that increasingly represents the underlying target distribution p(x) during training. The discrim-
2
Under review as a conference paper at ICLR 2018
Figure 2: The MIDCGAN architecture built with either feedforward convolutional or ResNet blocks.
inator predicts whether a sample is from the real target data distribution, p(x), or the generated
data distribution, p(X), and is represented by Djd (x, X) (Salimans et al., 2θ16). For the sake of
simplicity, we refer to the generator as G(z) and the discriminator as D(x).
DCGANs have been applied to a variety of domains including realistic image generation, unsuper-
vised and semi-supervised learning (Radford et al., 2015), and image-to-image translation (Isola
et al., 2016). We have built on the success of these applications with MIDCGAN, training the gen-
erative network using both l2 loss and the adversarial discriminative loss so that it learns to map
sample inputs from the training set to the associated mental image.
Another important basic distinction between regular DCGAN and MIDCGAN is that the generator
network is not generating samples from a random noise vector, z, rather they are derived from the
encoder of an autoencoder (Pathak et al., 2016). This enables MIDCGAN to encode important
features from the input distribution, p(χ), into a bottleneck feature vector denoted as Z This feature
vector is later used for learning as in 3.3.
3.1	Architecture
The MIDCGAN architecture is based on a GAN architecture with the generator replaced by a full
autoencoder (i.e., encoder-decoder) (Pathak et al., 2016). Given an input selected from a distribution
p(x) , the encoder network produces a representation of the object in the form of a bottleneck feature
vector, zZn, of length n. The decoder network then takes the output of the encoder, zZn, and produces
the a sample from the generated distribution, p(X). Here, X is the mental image of x.
The goal of MIDCGAN is to associate the input image back to the mental image. For this, the
encoder network focuses on emphasizing weights that are meaningful to map the input sample to
the mental image and in the process the bottleneck learns the essential features that are useful in
recognition tasks. The novel introduction of the mental image forces the network to focus on features
that are common to most samples of that specific category. The network learns features that could be
useful for classification implicitly without getting distracted to map the input peculiarities that are
not important for classification. In this, we take the semantics of the image that have been extracted
by the encoding function and reproduce what the image looks like from what an ideal perspective
(i.e., a mental image). To train, the generated mental image is given to the discriminator together
with the true mental images we want the network to produce to form the joint reconstruction and
adversarial loss discussed in Section 3.2.
Fig. 2 shows an overview of the MIDCGAN architecture. The convolutional blocks in all the three
components of MIDCGAN (encoder, decoder and discriminator) take two forms: either simple
convolutional blocks that contain a sequence of convolution-batch normalization-activation (we call
this simple network) or n residual blocks. We use n = 5 double convolutional BN-activation-
convolution residual units (He et al., 2016) in each of the convolution blocks. Our experiments
compare these different architectures. The discriminator is of two types: the regular AlexNet style
3
Under review as a conference paper at ICLR 2018
discriminator (similar to most DCGANs) and a discriminator that has a feature extraction pipeline
that is similar to the encoder with an added discriminator layer (we call this separate discriminator).
3.2	The Joint Autoencoder and Adversarial Loss
MIDCGAN is trained using joint adversarial and generator l2 losses. In a regular autoencoder, the
input and the target are the same and since there is no guidance for the encoder to select a specific
mode of the multimodal underlying data distribution, p(x). Therefore, the autoencoder tends to
average modes resulting in a blurry average image (Pathak et al., 2016). By adding the mental image
as a target in the l2 loss component, it forces the network to produce a specific mode of the target
distribution, p(X), instead (Goodfellow et al., 2014a; Pathak et al., 2016). Therefore, MIDCGAN
generates images that are sharper and closer to the target earlier in the training than an equivalent
regular DCGAN while at the same time learning useful classification features via its mapping of the
input sample to the mental image. The l2 loss is mainly responsible for the mapping of the input
to the canonical mental image while the adversarial loss contributes additional supervision of how
close the mapping is to the canonical image of that category. The mental image and the adversarial
loss together force the network to choose a particular mode (close to real mental image) of the target
distribution, p(X).
3.2.1	Autoencoder Reconstruction Loss
Here we used a normalized l2 loss similar to (Pathak et al., 2016) without the masking. We have
also experimented with both l1 and l2 losses and saw no significance difference. The l2 loss is given
by the squared difference between the target mental image, X, and the generated mental image,
G(z). Substituting encoded features from input data distribution, p(x), for z using the encoder,
E(x) results in Eq. 1
L12(x,X)=	,E L」|X - G(E(X))I∣2	(1)
x∈p(x),x∈p(x)
3.2.2	Adversarial Loss
According to (Goodfellow et al., 2014a), the regular adversarial two-player minmax game between
the discriminator, D(X), and the generator, G(z) is given by Eq. 2.
minmaX E [log(D(X))] + E [log(1 - D(G(z)))]	(2)
The discriminator maximizes both terms while the generator minimizes the second term. There are
several things to consider with respect to adversarial loss and MIDCGAN. First, the input distribu-
tion, P(X) and the target distribution, p(X), are not the same due to the mental image mapping in
MIDCGAN and hence the first term of Eq. 2 is maximized by the discriminator using samples from
the real target distribution or mental image, p(X). There is also no prior distribution of the bottle-
neck. In a regular GAN, Z is sampled from a prior noise distribution P(Z). In MIDCGAN, W comes
from the encoder and hence is learned. These modifications and the fact that the discriminator is
trying to maximize an objective results in an adversarial loss component of MIDCGAN as given by
Eq. 3 representing the encoder as E(X). In Eq. 3, P(X) represents the actual data distribution of the
dataset input to the encoder while P(X) represents the target canonical mental image distribution.
Ladv (x,X)= EJlOg(D(X))]+ E ∖log(.1 - D(G(E(X))))]	(3)
x∈p(x)	x∈p(x)
In practice (Goodfellow et al., 2014a)(Pathak et al., 2016), this loss is implemented by training the
discriminator and the encoder-generator pair using alternating SGD. The total joint loss used to train
the encoder-decoder generator pair is the weighted sum of the Eq. 1 and Eq. 3 and is given as Eq. 4.
Ltotal = λl2 Ll2 + λadv Ladv	(4)
4
Under review as a conference paper at ICLR 2018
We experimented with various λ weights and observed that it is dataset dependent. We found the set
(0.2, 0.8) worked well for MNIST while (1.0, 1.0) worked well for the remaining experiments.
The training of MIDCGAN involves three updates to the parameters of the three networks as shown
in Algorithm 1. The convergence of MIDCGAN happens often early due to the dual mode selection
because of the mental image and the adversarial loss and hence the discriminator could tell a real
target sample quickly and the real component of the adversarial loss does not change significantly
after a certain epoch as the generator tries to balance with the fake component of the adversarial
loss.
Algorithm 1 Training MIDCGAN
1:	θE, θG, θD J HeNormal He et al. (2016)	. initialize encoder, decoder/generator and
discriminator params
2:	repeat
3:	X, Y J shuffled mini-batch	. X is the input image while Y is the mental image
4:	Z J E ncoder (X)
5:	X J Decoder(Z)
ʌ
,	I	7 / τ~∖ / ~ττ- ∖ ∖ . 7 /r	τ~∖ / ~ττ~ ∖ ∖	z-x	. t	∙ -∣ -∣	r∙	ι t t` t
6:	Ladv J log(D(X)) + log(1 - D(X))	. Compute adversarial loss for real and fake
7:	θD J θD - OθD (Ladv) . Update Discriminator params with the adversarial loss gradients
..~	Q .. c
8:	l2 j ∣∣X - X∣∣2
9:	LT J λl2 l2 + λadvLadv	. total loss as fraction of adversarial and l2 losses
10:	θE J θE - OθE (LT)	. Update Encoder params with the total loss gradients
11:	θG J θG - OθG (LT)	. Update Decoder/Generator params with total loss gradients
12:	until number of epochs
3.3	B ottleneck Features for Classification
In our observations, the bottleneck features z represent important aspects of the input distribution,
p(x). We demonstrate how these features can be used for later classification. Once MIDCGAN
is trained, We need only the encoder to generate the bottleneck features z. To classify, We extract
features on training/testing samples from the dataset distribution, p(x). These features are then given
to an l2 SVM to perform classification.
4	Results and Discussion
In this section, We Will present and discuss the four major experiments. In the first tWo, MNIST
and SVHN are evaluated using stencils targets as mental images. In the final tWo, We evaluate
object instance recognition using the Big Berkeley Instance Recognition Database (BigBIRD) and
the University of Washington Kinects Objects Dataset. In the first experiment, We learn mental
images for each of the objects. In the second experiment, We use the features learned previously
to recognize an entirely neW database. In all experiments, We shoW that learning in this manner
outperforms features learned from a typical DCGAN architecture.
4.1	MNIST with Stencils as ’ Mental Images’
MIDCGAN maps every kind of handWritten digit in the MNIST dataset to stenciled digits as men-
tal images since they represent typical iconic representation of each digit. In this experiment, the
encoder, decoder and the separate discriminator Were all ResNet architecture With each convolu-
tion block replaced by 5 dual-convolutional residual units. Therefore, the separate discriminator is
much deeper and more expressive than the more commonly used AlexNet discriminator. In Table
1, We compare different architectures trained With a bottleneck size of 256 against the state-of-the-
art (Springenberg, 2015)(Makhzani et al., 2015)(Salimans et al., 2016) With a different number of
labeled training samples. MIDCGAN With simple feedforWard blocks and separate discriminator
(meaning the discriminator Weights are not shared With the encoder) outperforms others With feWer
training samples and comparable performance With more training samples. A discriminator With
5
Under review as a conference paper at ICLR 2018
Table 1: MNIST test error (%) with n labeled examples for different architectures at nBn=256. Note:
SS is simple feedforward CNN encoder/decoder/discriminator and separate discriminator and RS is
residual CNN encoder/decoder/discriminator and separate discriminator
Method/ Arch	Examples per Class					All
	10	20	50	100	200	
DCGAN Alexnet	53.39 ±	47.1 士	34.44 ±	24.55 ±	19.54 ±	8.17
	4.22	3.63	2.13	1.56	1.1	
Springenberg (2015)	-	-	-	1.39 ± 0.28	-	0.48
Makhzani et al. (2015)	-	-	1.90 ± 0.1	-	-	0.85
Salimans et al. (2016)		16.77 ±	2.21 ±	0.93 ±	0.9 ±	
	-	4.52	1.36	0.07	0.04	-
MIDCGAN SS	1.51 ±	1.22 ±	1.13 ±	1.07 ±	0.99 ±	0.82
	0.27	0.08	0.09	0.08	0.11	
MIDCGAN RS	4.58 ±	1.72 ±	1.48 ±	1.17 ±	1.03 ±	0.68
	2.65	0.69	0.43	0.13	0.07	
Figure 3: Reconstruction of stencil digits from SVHN samples at epochs 3(left) and 2499 (right).
Sample, target and generated, consecutively.
shared weights with the encoder performs slightly worse allowing a possibility of almost half pa-
rameter reduction with minimal performance impact. We finally evaluate results with all examples
labeled, and obtained our best results with a bottleneck size of 1024 with a separate discriminator
of 0.53% error. This is quite an improvement from 0.68% error reported in Table 1 with bottleneck
size of 256.
4.2	SVHN with Stencils as ’ Mental Image’
MIDCGAN with simple feedforward blocks and AlexNet style discriminator outperforms all other
architectures we experimented with for the SVHN dataset. As shown in Table 2, MIDCGAN pro-
duced the best performance among all the methods with a wide range of number of training samples.
Table 2: SVHN test error (%) with n labeled examples for different architectures at nBn=2048.
Method/
Arch
Examples per Class
10	20	50	100	200	500	1000	2000
Makhzani et al. (2015)
Salimans et al. (2016)
MIDCGAN SA
17.8 ±
4.11
17.70 ±
-	-	-	-	-	0.3	-
18.44	±	8.11	±	6.16	±
-	-	-	-	4.8	1.3	0.58
13.8 ±	9.1 ±	8.4 ±	7.89 ±	7.15 ±
1.8	0.6	0.68	0.39	-	0.26	-
6
Under review as a conference paper at ICLR 2018
Table 3: Object recognition accuracy using different databases and approaches.
Dataset	Method/ Arch	Pre- training	Examples per Class				
			50	25	10	5	1
BigBIRD	MIDCGAN	Self-	98.19 ±	95.45 ±	87.4 ±	79.8 ±	52.24 ±
		Supervised	0.14	0.39	0.59	0.89	0.64
BigBIRD	DCGAN	Unsupervised	65.55 ±	54.7 ±	43.4 ±	32.6 ±	14.1 ±
			0.42	0.45	0.54	0.73	0.89
RGBD	MIDCGAN	Transfer	61.7 士	59.7 ±	55.99 ±	51.4 ±	37.6 ±
			0.25	0.61	0.54	0.36	0.8
RGBD	DCGAN	Transfer	43.33 ±	40.34 ±	37.5 ±	32.1 ±	20 ±
			0.5	0.51	0.46	0.35	0.8
RGBD	CNN Held et al. (2016)	Supervised	-	-	-	-	63.9
4.3 Object Instance Recognition
In object instance recognition, rather than identifying a general class of objects (eg., a bottle), we
instead identify the specific instance of the object (e.g., a coke bottle). This problem is complicated
by subtle differences between different instances, which are also sometimes very dependent on pose
of the object. We evaluate results on two different datasets. The first is the Big Berkley Instance
Recognition Dataset (BigBIRD), which has 125 instances of different objects (Singh et al., 2014).
The objects include typical households items such as boxes of food, water bottles, shampoo, etc.
Many instances are highly similar in appearance. For example, there are 8 different types of cereal
bars made by the same company, some of which are extremely similar in packaging and appearance
(i.e., “big chewy chocolate chip” vs. “big chewy peanut butter”). The dataset shows objects from
5 different viewing angles and 360 degrees of rotation sampled at approximately every 3 degrees.
With MIDCGAN, recognition works as follows. For the sake of consistency, the mental image is the
first level of pose, first instance of the object that is seen. MIDCGAN is able to see the object from a
number of different poses, with each being mapped back to the mental image. When presented with
a test image, MIDCGAN generates a mental image of the frontal view of the object.
From the perspective of the linear SVM, The BigBIRD dataset is a 125 way classification problem,
where each class is a different object instance. We split the data with a stratified shuffle of all of the
data into a training set with 60,000 images, a test set with 7,500 images and a validation set with
7,500 images. A 125-way linear SVM is trained for object instance recognition using bottleneck
features. The results of this experiment are in the top of Table 3. In this case, we fix the bottleneck
size at 4096 and use the separate discriminator. The loss function assigns a weight of 0.2 to l2
loss and 0.8 to adversarial loss to capture the general details of the object without focusing on the
specifics of how the objects appear. It’s clear that the MIDCGAN features perform especially well at
object instance recognition. With only a single instance of each class (i.e., single shot recognition),
MIDCGAN recognizes objects 52.2% of the time, compared to 14% of the time for DCGAN. More
interestingly, in some cases only the back or the top of a box is used to train the SVM — it is still
able to recognize objects from dramatically different viewpoints!
A related question is how well do these features generalize to objects that have never been seen
before. That is, what if we were unable to train the MIDCGAN using the objects in question, but
instead only have a general knowledge of how mental images with objects of different classes. In
this rather challenging problem (sometimes referred to as transfer learning), as we have not seen
the objects a priori, but instead use the features learned on a different dataset. To evaluate in this
case, we make use of a second dataset, the University of Washington Kinect Object Dataset (hence
force referred to as RGBD) (Lai et al., 2011) (this dataset does have a depth component, but we
only make use of the color components). RGBD has 300 different object instances grouped into 51
classes. The dataset was collected at 3 different levels of pitch, and a full 360 degree pan, sampled
to generate 250 different poses per level of pan. It’s interesting to note that these objects are also
household type of objects, but this dataset contains a lot of images that are nowhere near the type
of images from BigBIRD. For example, this has images that are not present in BigBIRD such as
7
Under review as a conference paper at ICLR 2018
heads of garlic, bok choy, apples, etc. The transfer results are shown in the bottom of Table 3.
Here we follow the same general criteria established in (Held et al., 2016), which used one level
of pitch (30°) to train, and another (45°) to evaluate. Here, We see that the performance for single
shot recognition is not as good as it was in the previous experiment (37.9%), and with 10 labeled
images We recognize objects correctly 52% of the time). The difference in performance betWeen
MIDCGAN and DCGAN suggests that MIDCGAN learns features that are beneficial for learning
different classes of objects.
5 Conclusion
We have demonstrated the use of mental model autoencoder-based DCGAN (MIDCGAN) on digit
classification (MNIST), and object recognition on both the Kinects Objects Dataset (RGB-D) and
the Bigbird Dataset. Although MIDCGAN Was demonstrated on representative vieWpoints, the
selection of the mental target image could be arbitrary. In cases When MIDCGAN Was not sure
about the object, the generated image did not resemble the actual representative image, or important
details about the representative image Were omitted. In essence, it could potentially provide another
Way to determine the confidence in the prediction of the object class.
MIDCGAN can be used on robotics platform With real images in the Wild. Given the groWing inter-
est in active object manipulation and recognition in the robotics community (BroWatzki et al., 2012).
Incorporating real manipulation of objects as separate modality With MIDCGAN could permit robots
to learn about objects in their environment With minimal supervision. The mental images described
in this paper map very Well to the concept of prototype learning in the cognitive science community.
Several improvements could be made to MIDCGAN in the future. First, We have generally assumed
the availability ofa tutor to select a mental image. Autonomous selection of the mental images is an
inherent extension to alloW semi-supervised and fully unsupervised MIDCGAN training. Alterna-
tively, prototype images can be selected in some systematic manner using some heuristics related to
the objects (i.e., a cup holds liquid so prototype is face up, the handle is to be grabbed so that should
be visible).
References
H. Larochelle O. Winther A. Larsen, S. Sonderby. Autoencoding beyind pixels using a learned
similarity metric, 2016. URL https://arxiv.org/pdf/1512.09300v2.pdf.
AndreW Brock, Theodore Lim, JM Ritchie, and Nick Weston. Neural photo editing With introspec-
tive adversarial netWorks. arXiv preprint arXiv:1609.07093, 2016.
Bjom Browatzki, Vadim Tikhanoff, Giorgio Metta, Heinrich H Bulthoff, and Christian Wallraven.
Active object recognition on a humanoid robot. In Robotics and Automation (ICRA), 2012 IEEE
International Conference on,pp. 2021-2028. IEEE, 2012.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
Advances in Neural Information Processing Systems, pp. 2172-2180, 2016.
Antonia Creswell and Anil Anthony Bharath. Inverting the generator of a generative adversarial
network. arXiv preprint arXiv:1611.05644, 2016.
Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Alex Lamb, Martin Arjovsky, Olivier Mastropi-
etro, and Aaron Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704,
2016.
Ruohan Gao, Dinesh Jayaraman, and Kristen Grauman. Object-centric representation learning from
unlabeled videos. Asian Conference on Computer Vision (ACCV), 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014a.
8
Under review as a conference paper at ICLR 2018
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014b.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European Conference on Computer Vision, pp. 630-645. Springer, 2016.
David Held, Silvio Savarese, and Sebastian Thrun. Deep lerning for single-view instance recogni-
tion. IEEE International Conference on Robotics and Automation (ICRA), 2016.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with
conditional adversarial networks. arXiv preprint arXiv:1611.07004, 2016.
Max Jaderberg, Karen Simonyan, Andrew Zisserman, and Koray Kavukcuoglu. Spatial transformer
networks. In Advances in Neeural Information Processing Systems (NIPS), pp. 2017-2025, 2015.
Karin H James, Susan S Jones, Linda B Smith, and Shelley N Swain. Young children’s self-
generated object views and object recognition. Journal of Cognition and Development, 15(3):
393-401, 2014.
Kevin Lai, Liefeng Bo, Xiaofeng Ren, and Dieter Fox. A large-scale hierarchical multi-view rgb-d
object dataset. In Robotics and Automation (ICRA), 2011 IEEE International Conference on, pp.
1817-1824. IEEE, 2011.
Yann LeCun, Fu Jie Huang, and Leon Bottou. Learning methods for generic object recognition with
invariance to pose and lighting. In Computer Vision and Pattern Recognition, 2004. CVPR 2004.
Proceedings of the 2004 IEEE Computer Society Conference on, volume 2, pp. II-97. IEEE, 2004.
Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial
autoencoders. arXiv preprint arXiv:1511.05644, 2015.
Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High confi-
dence predictions for unrecognizable images. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pp. 427-436, 2015.
Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with auxil-
iary classifier gans. arXiv preprint arXiv:1610.09585, 2016.
Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context
encoders: Feature learning by inpainting. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 2536-2544, 2016.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. In Advances in Neural Information Processing Systems,
pp. 2226-2234, 2016.
Arjun Singh, James Sha, Karthik S Narayan, Tudor Achim, and Pieter Abbeel. Bigbird: A large-
scale 3d database of object instances. In 2014 IEEE International Conference on Robotics and
Automation (ICRA), pp. 509-516. IEEE, 2014.
Jost Tobias Springenberg. Unsupervised and semi-supervised learning with categorical generative
adversarial networks. arXiv preprint arXiv:1511.06390, 2015.
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and
composing robust features with denoising autoencoders. In Proceedings of the 25th international
conference on Machine learning, pp. 1096-1103. ACM, 2008.
Xiaolong Wang and Abhinav Gupta. Unsupervised learning of visual representations using videos.
In Proceedings of the IEEE International Conference on Computer Vision, pp. 2794-2802, 2015.
Xiaolong Wang and Abhinav Gupta. Generative image modeling using style and structure adversar-
ial networks. In European Conference on Computer Vision, pp. 318-335. Springer, 2016.
9
Under review as a conference paper at ICLR 2018
Raymond Yeh, Chen Chen, Teck Yian Lim, Mark Hasegawa-Johnson, and Minh N Do. Semantic
image inpainting with perceptual and contextual losses. arXiv preprint arXiv:1607.07539, 2016.
Chen Yu, Linda B Smith, Hongwei Shen, Alfredo F Pereira, and Thomas Smith. Active informa-
tion selection: Visual attention through the hands. IEEE Transactions on Autonomous Mental
Development ,1(2):141-151,2009.
Shuangfei Zhai, Yu Cheng, Rogerio Feris, and Zhongfei Zhang. Generative adversarial networks as
variational training of energy based models. arXiv preprint arXiv:1611.01799, 2016.
JUn-Yan Zhu, PhiliPP Krahenbuhl, Eli Shechtman, and Alexei A Efros. Generative visual manipu-
lation on the natural image manifold. In European Conference on Computer Vision, pp. 597-613.
SPringer, 2016a.
Zhuotun Zhu, Xinggang Wang, Song Bai, Cong Yao, and Xiang Bai. DeeP learning rePresentation
using autoencoder for 3d shaPe retrieval. Neurocomputing, 204:41 - 50, 2016b.
A	Supplemental Material
This section Presents the details of the MIDCGAN architecture, training Parameters and generated
images results for the different architectures, training conditions and datasets.
A. 1 Architectural Details
Both MIDCGAN and DCGAN used in this exPeriment had similar architectures excePt the targets
are different in each case. The architecture is comPosed of 3 networks, i.e. the encoder, the decoder
and the discriminator. The encoder and decoder together form the autoencoder embedded in the
DCGAN that serves as the generator. All the three networks (with the excePtion of the AlexNet
discriminator) are built around 3 major convolutional blocks. When these 3 major convolutional
blocks are single convolution followed by batch normalization and activation each, the resulting
architecture is referred to as simPle. This architecture is equivalent to most DCGANs that are based
on the AlexNet architecture for each network in the GAN. We then rePlaced these convolution blocks
with 3 residual units each and we referred to the resulting architecture as resnet. Each residual unit
in turn is double convolutional blocks Proceeded by batch normalization and activation and are
connected by a shortcut (residual) connection.
The encoder has single identity convolutions at the beginning and end for just channel transfor-
mation. The network also has a dense layer at the end that converts the sPatially flattened Pooled
features into a bottleneck of features suitable for classification. The bottleneck features were Param-
eterized by the bottleneck size of nBn. As Presented in the results section, we have exPerimented
with various bottleneck sizes such as 256, 512, 1024 and 2048.
The decoder has two dense layers that transform the inPut bottleneck feature size of nBn to N
that is suitable to transform back to sPatial Planes that were at the end of the encoder before fea-
ture transformation using the reshaPe layer. The 3 major convolutional blocks in the decoder are
imPlemented together with uPsamPling instead of converting them to transPosed convolution or de-
convolution because we wanted to keeP the 3 main convolution block the same for all the 3 networks
for consistency. The decoder also has a single identity convolution at the outPut for channel trans-
formation.
The discriminator is a bit different as there were three kinds of discriminators that were exPlored in
this PaPer. The seParate discriminator was essentially the same as the encoder excePt it adds binary
discrimination layer after the feature transformation. The shared discriminator went further and
shares weights with the discriminator with the excePtion of the last discrimination layer. The third
discriminator was the AlexNet discriminator that was similar to most discriminator architectures in
GAN literature. We added this discriminator for fair comParison with literature.
All the convolutions in all the architectures were 4 x 4 convolutions. DownsamPling is imPlemented
by a stride of size 2 at the beginning convolution of each of the 3 major convolutional blocks in
the encoder and the discriminator. Beginning and ending convolutions in each network were using
tanh activations while internal convolutions used LeakyReLU with Parameter of 0.2. The number
10
Under review as a conference paper at ICLR 2018
of filters in the internal convolutional blocks were all 32 for all the three networks for MNIST
training while there were 64 filters for BigBird training.
A.2 Training Parameters
The MNIST dataset were trained on the 60, 000 training images while the BigBird dataset was di-
vided into train and validation using 80% - 20% split. The 20% split was further divided equally into
validation and testing sets. We employed the Adam optimizer for both discriminator and generator
training. For MNIST the learning rate for both discriminator and generator were 0.002 with the
discriminator beta1 set at 0.5 and generator beta1 set at 0.9. For BigBird training, we used learn-
ing rate of 0.0001 and beta1 parameter of 0.9 for both discriminator and generator. The maximum
epochs for each training were 2500 with best model saved using the total validation generator loss.
However, in practice, we saw that most of the networks converged much earlier usually after few
hundred epochs.
A.3 Notes on Improvements for Convergence
Compared to Goodfellow et al.(Salimans et al., 2016), we have not applied advanced techniques
such as feature matching, minibatch discrimination, historical averaging and virtual batch normal-
ization to improve convergence and performance. We rather focused on basic techniques to stabilize
DCGAN and ResNet training. Training a DCGAN involves ideally finding the Nash equilibrium
point of the two models (discriminator and generator networks) that are competitive in adversarial
fashion. In practice, it is difficult to find the Nash equilibrium point for adversarial training where
the loss function is non-convex (Salimans et al., 2016). Therefore, several heuristic techniques are
employed to make the training of DCGANs converge using gradient descent back propagation.
We would like to emphasize that, in this work, we opt to not include the more complicated heuristics
suggested by (Salimans et al., 2016) to assess the performance of our networks in a fair manner so as
to not add confounding factors to the training. We believe that if we add all the improvements sug-
gested, it would be difficult to isolate the contribution of the architectural innovations and the main
idea of using mental imagery for the generative reasoning. It would be hard to tell if the performance
increase are from the network and mental image canonical learning or the improvements. Moreover,
it would also take away from the focus of this paper, which introduces learning from mental images.
We utilized mainly standard GAN training techniques such as one-sided label smoothing (Salimans
et al., 2016), regular batch normalization after each convolutional block in the case of the simple
architecture or after each residual block in the case of ResNet version of the networks. We also em-
ployed l2 regularization for the ResNet units inside the ResNet blocks to regularize the redundant
aggressive learning as the network gets deeper and deeper. We have not employed this regularization
to the shallower simple networks.
A.4 MNIST Additional Results
Generated samples from MNIST dataset are shown in Fig. 4 at epoch 7 (left) and epoch 1500 (right)
with a bottleneck of size 256. It is evident that at an earlier epoch, the generator confuses several
digits and outputs more than one mode in one generated image (the first digit being 7 overlaid on top
of 4. However, with more training epochs the generator learns to separate out only the target mode
(right).
Both the bottleneck size and the type of discriminator architecture have a direct impact on the de-
scriptiveness of the bottleneck features, as shown in Table 4. To explore this relationship, we chose
200 training images as this permits a fair comparison against (Salimans et al., 2016). The results
for this experiment are in Table 4. Our best result came with a bottleneck size of 1024 and a
more complicated ResNet discriminator. Note that in general, more complicated (and descriptive)
discriminators produce better results with larger bottleneck sizes whereas simpler discriminators
produce better results with smaller bottleneck sizes. We hypothesize that this is because simpler
discriminators produce simpler features that are easier to capture with fewer neurons.
In Table 5, we compare performance with different training sample sizes and different types of
discriminators. Goodfellow et al. (Salimans et al., 2016) showed classification results with as few as
20 training samples; we show results down to 10 samples per class (i.e., one sample per class). The
11
Under review as a conference paper at ICLR 2018
Figure 4: Reconstruction of stencil digits from MNIST samples at epochs 7(left) and 1500 (right).
Sample, target and generated, consecutively.
Table 4: Effect ofbottleneck size on performance and comparison with the state-of-the-art at n=200.
ArCh	nBn				
	256	512	1024	2048	4096
Separate	1.025	1.126	0.844	0.907	0.927
Alexnet	1.049	0.951	1.387	1.302	1.155
Salimans et al. (2016)	x	x	0.9	x	x
results indicated that the more descriptive discriminator (separate) dominated the simpler (AlexNet).
Although distinctive labels were not provided to MIDCGAN, clearly the features learned through
mental image generation are well suited for this task.
A.5 SVHN Additional Results
Table 6 shows the effect of bottleneck feature size on MIDCGAN performance using the SVHN
dataset.
Table 5: Test error (%) on MNIST at bottleneck size of 1024 for the two dicriminator types and
state-of-the-art.____________________________________________________________
ArCh	______________n______________
10	20	50	100
Separate	2.832^^1.209^^1.103^^1.017
Alexnet	8.045 4.473 2.426	1.727
Salimans et al. Salimans et al. (2016) x	16.77	2.21	0.93
12
Under review as a conference paper at ICLR 2018
Table 6: Effect of bottleneck size on performance and comparison with the state-of-the-art at
n=1000.
Method/	nBn
Arch	^56	512	1024~2048
Salisman et. al. MIDCGAN SS MIDCGAN SA	~~-	-	8.11 17.59	23.49	-	- 11.5	10.46	10.82	7.15
Figure 5: MIDCGAN generated images (every third column) for test set of MNIST after the first
epoch (left) and the last epoch (right). Every first column is the input, every second column is the
target and every third column is the generated image.
A.6 Additional Generated Images
In this supplementary material, we added additional generated images for qualitative comparisons
of MIDCGAN with the regular DCGAN counterpart. We are adding validation set reconstruction
images early on the training and after the training had finished to show the progression of quality of
the reconstructed images.
13

∙θSbuix pəiŋjəuəg əip sɪ uuɪnɪoɔ pjχιp ajəaə puŋ
jθSj∏ əip si uuɪnɪoɔ puoɔəs Xjθλθ ZndU! əip sɪ uuɪnɪoɔ IS叫 ∙(ιqSπ) qooda isŋɪ əip puŋ QJ/)
qoodb IS叫 əip mJe JLSINN JO ləs isəi joj (uuɪnɪoɔ pjχιp Xjθλθ) səsŋmɪ pəiŋjəuəg NVDDQ ：9
。/>/ /£<?W UJZr-JL∕S^1b 吟"夕夕 I 4
。/>/ /£<?W UJZr-JL∕S^1b 吟"夕夕 I 4
££27»£<r々1ZJ------------------------ 力 ∕βo ”6(7
Cfc Ct ns / £<r夕人，
h¾≤∙9$。夕 CA/
h¾≤∙9$。夕 CA/
L1 4，/
L1 4，/
b 4 / I /1 ^/b O S £u ʃ Oj
b 4 / I / t ^/b O S £u ʃ Oj
£ M OQ /6 b<?
£ M OQ /6 b<?
'λ
Λ
SrzN/，，，J3z^/Ag/4z-
SrzN/，，，J3z^/Ag/4z-
CΔL
CΔL
CIL
3
3 ⅛l‰
6 ∙⅛*J
£ J bH
£ J bH
αbh∕93z∙∖A
4<76h∕98z∙bA
4。益 b / 9，x4 b 4
匕 OΛ/ qb / / 〃卜人
匕 OΛ/ qb / / 〃卜人
J⅛Q⅛∕∙b∕ /夕卜，
,。4 — OF。e/G b IU-Ja
,。4 — OF。e/G b CU-Ja
fQ^ loτv-c∕cτ⅛Γ∙LX
r ObVJ/七夕Z6×snoλo
r ObVJ/七夕Z6×snoλo
rpb:∕r 夕4夕o⅜∙>Q-<o
∙6^∕45e∕i G 方 ∕βo "g(7
♦，口/5£/1g*∕j/Ga
9^so 9 0102 / ∕044-o
9^so 9 0102 / ∕044-o
ar<?SQ ∙9IG/
/∕CUJ4∕⅛SF∕∕y^x
/∕CUJ4∕⅛SF∕∕y^x
⅛¾fψw0ψs∕9Γ4-4z∕4->∙αhG<zτfQs4r
IN8h4，d\ct."5hc>gb SZG Γ'6∖σ)3
IN8h4，d\ct."5hc>gb SZG Γ'6∖σ)3
4/T£Jb404l1L4p3,0f.FgJa-F4，26.A‹h/
or4l47∙∕4 卜knhΓNZSOS
or4l47∙∕4 卜knhΓNZSOS


8IOZ HlDI18 ɪədŋd əɔuəjəjuoɔ ŋ sŋ mətaəj EPUn
Under review as a conference paper at ICLR 2018
6
15
2 2
5 5
4 8
7 7
3
2 2
6 6
3 3
3 8
1 1
5 5
6 3
5 5
222 2
"1 1
5 5
3 3 3
8 8
?2 2
■ 1 1
8 8
8 8
1 1
3 3
2 2
2 2
3 3
9 91
4 4
O O
9 9Q
2 2 2[
5k
IL5 53
ɪd 1
Q4 4
78

32 2 5
3 3
O O
3 8
6 6
2 2 2
O OD
4 4 :
Γ9 7
9 9
5 5
7 7
3
2
7
9
O
3
1
2
3
8
4
4
8
5
O
5
6
6
5
91J8 8
2 9 9
1, 6 6
7 2
2
Il
1 1
2 2
8 8图2 2
1 1
O O
4
2 2 ： O O
2 2 6 8
9 9
5 5
7 7J6 6
O 0B5 5
3 3
1 1
1 1U
4 8
4 4D
9 9
1
1
8 8
5 5
2 2
1 S
2 2 4
6 5E4
1 8 5
5 5H1
4 8 1
0 0⅛
7 7 i
7 7J1 1
1 1Li9 2
5 5 ∙ 6 6
1 1 2 2
3 3∏1 1
1 1 ∕j1 1B1 1
9 9E]8 8； 6 1
3 3 2 2 1 1
49❷2 2 3 3
3 3：
O 9以
3 3 H
1 1 L
1 1 8
4 41(1 3
2 2 6 6
9 1KH8 8
2 2u：3 5
8 8厘5 3
32165622131612443347921125
214972402922811475459595461
214972402922611473459595461
W
6
■ U Y
3 3
•一



2 id ιr
2 1121 1H
13 3*
3 4 8
1E4 4
6∣> 3 3∏
1 7 7
232 2S]
4 2 2 2 L
λ O 2
3 8 O
3 4 4SI
4 2 269
1H2 2 "
8
6
2
7
14
Figure 7: MIDCGAN generated images (every third column) for test set of SVHN. Every first
column is the input, every second column is the target and every third column is the generated
image.
15
Under review as a conference paper at ICLR 2018
Figure 8: DCGAN generated images (every third column) for validation set of BigBird after the first
epoch (last epoch is shown in the next figure). Every first column is the input, every second column
is the target and every third column is the generated image.
16
Under review as a conference paper at ICLR 2018
Figure 9: DCGAN generated images (every third column) for validation set of BigBird after the last
epoch (first epoch is shown in the previous figure). Every first column is the input, every second
column is the target and every third column is the generated image.
17
Under review as a conference paper at ICLR 2018
Figure 10: MIDCGAN generated images (every third column) for test set of BigBird after the last
epoch. Every first column is the input, every second column is the target and every third column is
the generated image.
18