Under review as a conference paper at ICLR 2018
A cluster-to-cluster framework for neural
MACHINE TRANSLATION
Anonymous authors
Paper under double-blind review
Ab stract
The quality of a machine translation system depends largely on the availability
of sizable parallel corpora. For the recently popular Neural Machine Translation
(NMT) framework, data sparsity problem can become even more severe. With
large amount of tunable parameters, the NMT model may overfit to the existing
language pairs while failing to understand the general diversity in language. In this
paper, we advocate to broadcast every sentence pair as two groups of similar sen-
tences to incorporate more diversity in language expressions, which we name as
parallel cluster. Then we define a more general cluster-to-cluster correspondence
score and train our model to maximize this score. Since direct maximization is
difficult, we derive its lower-bound as our surrogate objective, which is found
to generalize point-point Maximum Likelihood Estimation (MLE) and point-to-
cluster Reward Augmented Maximum Likelihood (RAML) algorithms as special
cases. Based on this novel objective function, we delineate four potential systems
to realize our cluster-to-cluster framework and test their performances in three
recognized translation tasks, each task with forward and reverse translation direc-
tions. In each of the six experiments, our proposed four parallel systems have
consistently proved to outperform the MLE baseline, RL (Reinforcement Learn-
ing) and RAML systems significantly. Finally, we have performed case study to
empirically analyze the strength of the cluster-to-cluster NMT framework.
1	Introduction
Recently, an encode-decoder neural architecture (Cho et al., 2014) has surged and gained its popu-
larity in machine translation. In this framework, the encoder builds up a representation of the source
sentence and the decoder uses its previous RNN hidden state and attention mechanism to generate
target translation. In order to better memorize the input information, an attention mechanism (Bah-
danau et al., 2014) has been exploited to further boost its performance. In order to train the attentive
encoder-decoder architecture, Maximum Likelihood Estimation (MLE) algorithm has been widely
used, which aims at maximizing the point-to-point (one sentence to one sentence) log-likelihood
of data pairs in a given dataset. However, this algorithm has severely suffered from data sparsity
problem, or in other word, maximizing only likelihood the existing language pairs might make the
model blind to all the non-existing similar sentence pairs. Thus, the large neural model might overfit
to certain prototypes existing in the training set while failing to generalize more unseen but similar
scenarios in test time.
Recently, different approaches (Zhang & Zong, 2016; Sennrich et al., 2015a; Ma et al., 2017;
Norouzi et al., 2016) have been proposed to tackle the data sparseness, which can be mainly classi-
fied into two categories: 1) Semi-Supervised Learning (Zhang & Zong, 2016; Sennrich et al., 2015a;
He et al., 2016; Cheng et al., 2016) resorts to external monolingual (unpaired) data to augment the
bilingual training pairs. 2) Pseudo-Learning (Ma et al., 2017; Norouzi et al., 2016; Ranzato et al.,
2015; Bahdanau et al., 2016) directly generates pseudo training samples from the existing dataset.
Though semi-supervised learning has achieved significant success in NMT, it’s still heavily restricted
by the quality and quantity of extern monolingual data. In this paper, we focus on pseudo-learning
strategy to enlarge training samples directly from the existing dataset. The existing pseudo-learning
algorithms can be categorized into two types: 1) Golden-Centroid Augmentation (RAML), Maet al.
(2017) and Norouzi et al. (2016) advocate to augment samples by sampling from a payoff distribu-
tion, which incorporates well-controlled modification to ground truth to enrich training data without
1
Under review as a conference paper at ICLR 2018
hurting its semantic meaning. 2) Model-Centroid Augmentation (RL), Bahdanau et al. (2016) and
Ranzato et al. (2015) leverage model-generated candidates as pseudo training samples, which are
weighted with rewards to enhance the model learning. By exploring self-generated candidates, the
model is able to understand the diversity in the output space. In pseudo-learning algorithms, both
Figure 1: Left figure shows the three pseudo-learning algorithms, right figure shows the details of
parallel cluster. X denotes the source language and Y denotes the target language.
RAML and RL can be interpreted as broadcasting a target ground truth as a cluster of analogues
while leaving the source input untouched, which though helps the model understand target diversity,
fails to capture the input diversity. In order to explore both sides’ diversity, we advocate a novel and
general cluster-to-cluster framework of pseudo learning, which first broadcasts both source and tar-
get sentence as clusters and then train the model to comprehend their correspondence, as described
in Figure 1.
In this paper, we first introduce the concept of parallel cluster, then design the cluster-to-cluster
correspondence score as our optimization objective, based on which, we derive its lower bound
KL-divergence as our surrogate objective for model training. In order to realize our proposed frame-
work, we design four parallel systems and apply them to three recognized machine translation tasks
with both forward and reverse translation directions, these four systems have all demonstrated their
advantages over the existing competing algorithms in six translation tasks. In the appendices, we
draw samples from the parallel clusters and further analyze their properties to verify our motivation.
The contributions of our paper can be summarized as follows: 1) We are the first to propose the
concept of cluster-to-cluster framework, which provides a novel perspective to current sequence-to-
sequence learning problems. 2) We delineate the framework and arrive in a novel KL-divergence
loss function and generalizes several existing algorithms as special cases, which provides a high-
level understanding about the previous algorithms.
2	Related Literature
2.1	Reinforcement Learning for Sequence-to-Sequence Model
Exposure bias and train-test loss discrepancy are two major issues in the training of sequence pre-
diction models. Many research works (Bahdanau et al., 2014; Shen et al., 2015; Ranzato et al.,
2015; Norouzi et al., 2016; Lamb et al., 2016) have attempted to tackle these issues by adding
reward-weighted samples drawn from model distribution into the training data via a Reinforcement
Learning (Sutton & Barto, 1998) framework. By exposing the model to its own distribution, these
methods are reported to achieve significant improvements. Bahdanau et al. (2016), Ranzato et al.
(2015) and Shen et al. (2015) advocate to optimize the sequence model as a stochastic policy to
maximize its expected task-level reward. Though RL is not initially designed to resolve data spar-
sity problem, the model-centroid training samples can indeed alleviate data sparseness by exposing
the sequence-to-sequence model to more unseen scenarios. One problem of the previous RL works
is that, the input information is still restricted to the dataset, which fails to teach model to compre-
hend source diversity. The cluster-to-cluster framework augments many similar input sentences to
account for source language diversity.
2
Under review as a conference paper at ICLR 2018
2.2	Reward Augmented Maximum Likelihood
One successful approach for data augmentation in neural machine translation system is Reward
Augmented Maximum Likelihood (RAML) (Norouzi et al., 2016), which proposes a novel payoff
distribution to augment training samples based on task-level reward (BLEU, Edit Distance, etc). In
order to sample from this intractable distribution, they further stratify the sampling process as first
sampling an edit distance, then performing random substitution/deletion operations. Following the
work of RAML, Ma et al. (2017) introduces a novel softmax Q-Distribution to reveal RAML’s
relation with Bayes decision rule, and they also propose an alternative sampling strategy - first
randomly replacing n-gram of the ground truth sentence and then using payoff distribution to com-
pute corresponding importance weight with local normalization. These two approaches augment the
target-side data by exposing the model to diverse scenarios and improve its robustness. We draw
our inspiration from RAML, but with a difference that, instead of based on task-level reward, a
learnable payoff function (cluster distribution) is used in our approach to take more latent structures
into account, such as semantic meaning, language fluency, etc. From the cluster distribution, we
can sample semantically and syntactically correct candidates to train the model. In addition, our
more generalized bilateral data augmentation strategy also empowers our model more capability to
generalize better.
2.3	Semi-supervised Learning in NMT
In order to utilize the large amount of monolingual data in current NMT framework, different strate-
gies have been designed, the most common methods can be concluded into these categories: 1)
using large monolingual data to train language model and integrates it to enhance language flu-
ency (Brants et al., 2007). 2) using self-learning method to transform the monolingual data into
bilingual form (Sennrich et al., 2015a; Zhang & Zong, 2016). 3) using reconstruction strategy to
leverage monolingual data to enhance NMT training (He et al., 2016; Cheng et al., 2016). Although
our motivation to augment training data is aligned with these semi-supervised algorithms, our pro-
posed framework has substantial differences from them: 1) we don’t rely on additional monolingual
data to boost NMT performance; 2) Though we jointly train forward and backward translation mod-
els as advocated in He et al. (2016) and Cheng et al. (2016), our joint algorithm doesn’t involve any
interactions between these two models (they can be trained independently).
3	Model
3.1	Parallel cluster
We define the parallel cluster as two groups of weighted sentences C(Y*) and C(X*), whose simi-
larities (BLEU, METEOR, etc) with Y * and X * are above certain threshold M.
C(Y *)
∪ {Y : p(Y|Y*)|R(Y,Y*) > M} C(X*) = ∪ {X : p(X|X*)|R(X, X*) > M}
Y∈Y	X∈X
(1)
Every sample X or Y is associated with a normalized weight p(X|X*) orp(Y|Y*) to denote how
much chance a sentence X orY is sampled from the corresponding cluster, here we draw a schematic
diagram to better visualize the parallel cluster in Figure 1. We will further talk about how we define
and compute the weights in the following sections.
3.2	Cluster-to-cluster Correspondence
Upon the definition of parallel cluster, we further design a cluster-to-cluster correspondence score
CRc→c(X*, Y*) as the log scaled expectation of likelihood ofa random sentence X in source clus-
ter C(X*) being translated to Y in target cluster C(Y*), which generally denotes the translatability
of two clusters, formally, we define the cluster-to-cluster correspondence score CRc→c(X*, Y*) as
below:
CRc→c(X*,Y*)
log
EE
X 〜C(X*)Y 〜C(Y *)
p(Y|X)
(2)
3
Under review as a conference paper at ICLR 2018
Algorithm	Source → NMT → Target	Objective
MLE(P2P)	σ(X|X*) → σ(Y|Y*)-	argminKL(σ(Y|Y*)|| PX σ(X|X*)p(Y|X))
RAML(P2C)	-σ(X|X*) → q(Y|Y*)-	argminKL(q(Y|Y*)|| PX σ(X∣X*)p(Y|X))
ClUSter-to-Cluster	P(X|X*) → P(Y|Y*)	argminKL(P(Y|Y*)|| PXP(X|X*)p(Y∣X))
Table 1: Our proposed framework can generalize MLE as point-to-point case, while RAML as
point-to-cluster case.
The higher correspondence score the more likely these two clusters correspond to each other. Note
that the cluster-to-cluster correspondence score can reflect both NMT’s and cluster’s quality, assum-
ing the cluster is ideal, then the correspondence score measures the translatability from a source
sentence X to a target sentence Y , while assuming the NMT is ideal, then the correspondence score
measures the quality of the cluster (the capability to rank paraphrases based on semantically simi-
larity).
3.3	Maximizing cluster-to-cluster Correspondence
Based on the definition of parallel cluster and cluster-to-cluster correspondence score, we further
design the cluster-to-cluster framework’s objective function as maximizing the empirical correspon-
dence score CRc→c(X*,Y*; D) with the regularization of target cluster's entropy H(P(Y|Y*)) in
a dataset D, as described below:
Objc→c =CRc→c (X *,Y *; D) + H (p(Y∣Y *))
Σ
(X*,Y *)∈D
log E	E	P(Y |X) + H (P(Y |Y *))
Y ~p(Y∣Y *)X ~p(X∣X *)
(3)
By applying Jensen’s inequality to the objective function Objc→c, we can further derive its lower-
bound as:
Objc→c =CRc→c(X*, Y *; D) + H(P(Y |Y *))
≥	E log E	P(Y|X) + H(P(Y|Y*))
(X*,Y*)∈dy ~p(γ∣γ *)	x~p(χ∣x*)
-	P(Y |Y *) log
(X*,Y*)∈D	Y
P(Y |Y *)
PX P(X |X *)p(Y |X)
(4)
-KL(P(Y|Y*)||	P(X|X*)P(Y|X))
(X*,Y*)∈D	X
X	-KL(P(Y|Y*)||P(Y|X*))
(X*,Y*)∈D
From this, we notice that the cluster-to-cluster objective is lower bounded by a negative KL-
divergence -KL(P(Y|Y*)||P(Y|X*)). Therefore, we can use this lower-bound to maximize cor-
respondence score, by changing the sign of this lower-bound function, we further define the loss
function as:
Losst = KL(P(Y|Y*)||P(Y|X*))	(5)
We theoretically verify that this lower bound KL-divergence can generalize Maximum Likelihood
(MLE) and Reward Augmented Maximum Likelihood (RAML) (Norouzi et al., 2016) as special
cases when we instantiate cluster distribution as Kronecker-Delta function δ(Y|Y*) and payoff dis-
eR(Y,γ *)∕τ
PY eR(Y,Y*)∕τ ,
tribution q(Y|Y*)
as shown in Table 1.
4 Optimization
4.1	NMT
In this section, we try to minimize the proposed KL-divergence KL(P(Y|Y*)||P(Y|X*)) so as to
raise the lower bound of the regularized cluster-to-cluster correspondence. We can write its deriva-
4
Under review as a conference paper at ICLR 2018
tives w.r.t to the NMT parameters in two forms, namely parallel sampling and NMT broadcasting
modes, which differ in their Monte-Carlo proposing distribution.
•	Parallel Sampling: sampling candidates independently from two clusters and then re-
weighted pairwise samples with a translation confidence W(Y|X, X*).
NLosst
Ep(YIY *)E
-p(X∣X *)p(Y∣X)
Pχo P(X 0IX *)p(Y∣x 0)
N logp(Y|X)
Ep(YIY*) EP(XIX*)--P(I (YIX Vlogp(YIχ)
Y	X	X0 〜ρ(x∣x *)p 1 J
(6)
EE
Y 〜p(Y∣Y *)X 〜p(X∣X*)
-P(Y IX)
p(Y X *)
VlogP(YIX)
EE
Y 〜p(Y∣Y *)X 〜p(X∣X*)
—W(Y X,X *)V log p(Y IX)
•	Translation Broadcasting: sampling candidates from one cluster and broadcasting them
through the NMT to construct its opponents, and re-weighted by cluster confidence
c(Y IY *,X *).
VLosst
Ep(YIY *) E
-p(XIX *)p(YIX)
PX 0 P(X 0IX *)p(YX 0)
VlogP(YIX)
Ep(XX *) E P(YIX) —Ep(IP(Y X 0)V log P(YIX)
X	Y	X0〜ρ(X∣X*) ' I '	(7)
EE
X 〜p(X∣X *)Y 〜p(Y∣X)
-P(Y IY*)
P(Y X *)
V logP(YIX)
EE
X 〜p(X∣X *)Y 〜p(Y∣X)
- c(YIY*, X*)V logP(YIX)
More specifically, translation broadcasting’s samples are more NMT-aware in the sense that it incor-
porates NMT’s knowledge to generate correspondents. The parallel sampling mode works like two-
sided RAML (Norouzi et al., 2016) while translation broadcasting works more like mixed RAML-
RL (Williams, 1992).
4.2	Cluster
In this paper, we design cluster distribution in two manners, namely inadaptive (pre-computed with-
out training) and adaptive (trained during optimization) cluster. Both cluster designs meet the crite-
rion of concentrating around the ground truth according to sentence similarity metric. In addition,
a cutoff criterion is also leveraged to reject samples whose task-level score is lower than certain
threshold M value as in Equation 1.
• Inadaptive Cluster: we use two non-parametric distributions q(XIX*) and q(Y IY *) to
denote source and target parallel clusters, based on the similarity score between sample
X/Y and the ground truth X*/Y *. We follow the payoff distribution (Norouzi et al.,
2016) to define our inadaptive cluster:
q(Y IY *)=P exe{RR,Y0* Y:1}=R(Y,Y *)	(8)
Y0 exp{R(Y , Y )// }
where R(Y, Y* ) denotes the task-level reward (BLEU, CIDEr, METEOR, etc) and
R(Y,Y*) denotes its normalization in the whole output space, T is the hyper-parameter
temperature to control the smoothness of the optimal distribution around correct target
Y * . Since the task-level reward only considers string-level matching (precision, recall,
etc) while ignoring semantic coherence, the generated samples though lexically similar,
prone to many semantical and syntactical mistakes, which might cause counter-effects to
the NMT model.
5
Under review as a conference paper at ICLR 2018
• Adaptive Cluster: We use two parametric models P(X|X*) and P(Y|Y*) to denote the
source and target adaptive cluster, which follow encoder-decoder neural architecture (Cho
et al., 2014) but take ground truth X*,Y* as inputs. Adaptive cluster is designed to fulfill
the following two requirements: 1) Proximity to ground truth: the randomly sampled can-
didates should have high similarity with the ground truth. 2) High correspondence score:
parallel cluster should be highly correlated and translatable. Combining these two goals
can guarantee mutual dependence between the source and target clusters and also retain its
similarity to the original ground truth. Formally, we write the optimization target of the
target cluster as:
Lossl
KL(P(Y|X*)||P(Y|Y*)) +
E
Y 〜p(Y∣Y *)
[-R(Y, Y *)]
(9)
During optimization, we fix the forward NMT P(Y|X) and target cluster P(X |X*) to up-
date source cluster P(Y |Y *), and we fix the parameters of backward NMT P(X |Y) and
source cluster P(Y |Y *) to update target cluster P(X|X*). Here we write target cluster’s
derivative as following:
VLOssl =	E	V log P(Y∣Y *)+	E	[-R(Y,Y *)Vlog p(Y |Y *)]	(10)
Y〜p(Y∣X* )	Y〜p(Y| Y*)
Due to the mutual dependence between adaptive clusters and translation models, we advo-
cate to alternately update the cluster and the translation models.
4.3 Joint Systems
In this section, we advocate to combine both forward and backward translation directions in a joint
system to simultaneously learn four models - forward NMT P(Y|X), backward NMT (X|Y),
source cluster P(X|X*) and target cluster P(Y |Y *). We exploit different scenarios to combine
these four models and then design four parallel systems, whose implementations are elaborated
in Table 2. System-A and B use inadaptive (non-parametric) cluster, thus require optimizing only
the two translation systems; system-A applies parallel sampling algorithm while B applies transla-
tion broadcasting algorithm. In contrast, system-C and D apply adaptive (parametric) cluster, thus
require simultaneous optimization of both NMT and cluster, system-C applies parallel sampling
while system-D applies translation broadcasting algorithm. These four systems exhibit different
characteristics which are shown in details as below:
System	NMT: P(X |Y) and P(Y|X)	Cluster: p(X∣X*) andP(Y|Y*)
-A-	Parallel Sampling	Inadaptive Model
B	Translation Broadcasting	Inadaptive Model
-C-	Parallel Sampling	Adaptive Model
D	Translation Broadcasting	AdaPtiVe Model	一
Table 2: Four systems with different cluster models and different training methods for NMTs.
In a slight abuse of notation, we will denote EYPn(X|Y)q(Y|Y*) as P(X|Y*) and
PXPY(Y|X)q(X|X*) asP(Y|X*).
System-A For system-A, we use inadaptive cluster with parallel sampling strategy to train the
NMT model, and the forward-backward joint objective functions is defined as:
LOssA(β, α) = KL(q(X|X*)|| £Pn(X|Y)q(Y∣Y*)) + KL(q(Y|Y*)|| £PY(Y|X)q(X∣X*))
Y	X	(11)
Formally, the derivative respect to η and γ are shown as:
Vn LOssA
EE
X 〜q(X∣X*)Y 〜q(Y∣Y *)
- w(X|Y, Y *)Vn log Pn(X|Y)
VY LOssA
EE
Y 〜q(Y∣Y *)X 〜q(X∣X*)
- w(Y |X, X*)VY log PY(Y |X)
(12)
Parallel candidates are sampled from source and target cluster distributions are leveraged by scaled
translation scores w(X|Y, Y*), w(Y |X, X*) during optimization.
6
Under review as a conference paper at ICLR 2018
System-B With the same loss function in system-A, translation broadcasting is leveraged to com-
pute derivatives in system-B, instead of parallel sampling, and the gradients is shown as:
▽n Loss B
Y『EY|Y*)X〜P昌Y*)-WXTVn logPn(XIy)
VγLossB
EE
Y〜pα(Y∣Y* )X〜P(X|Y*)
-C(X IX *,y*)Vn log Pn (X ∣Y)
x~PβEX∣x* )Y 〜P(EY∣X*) -RXXYxΓ VY log PY(YIX)
(13)
EE
X〜Pβ(X|X* )Y〜p(Y∣X* )
-c(Y∣Y*,X*)Vγ logPγ(Y∣X)
This system works similar as reinforcement Learning, where normalized environmental rewards
R(X, X*), R(Y, Y*) are leveraged to guide the model S policy search, and the gradients is mter-
preted as a form of Monte-Carlo Policy Gradient (Williams, 1992).
System-C Unlike System-A and system-B, two adaptive cluster distributions is used in system-C,
thus the NMT and cluster are jointly optimized during training, and the loss function is defined as:
LossC(η,γ,β,α) =KL(Pβ(XIX*)IIXPn(XIY)Pα(YIY*)) +	E	[-R(X, X*)]+
X 〜Pη (x∣y )
KL(Pα(YIY*)II XPY(YIX)Pβ(XIX*)) +	E	[-R(Y,Y*)]
Y〜FY (YIX)
(14)
we can get the derivatives as below:
VnLossC =	E	E	- w(XIY, Y *)Vn log Pn(XIY)
X 〜Pβ (X∣X*)Y 〜Pα(Y∣Y*)
VY LossC
EE
X 〜Pβ (X∣X*)Y 〜Pα(Y∣Y*)
- w(Y IX, X*)VY log PY(Y IX)
Ve LossC=Y 〜pE∣X* )V log Pa(YIY *)+ Y 〜PaM *) - R(Y，Y * )V log Pa(YIY *)
(15)
VaLossC =X -E1 γ* V log Pβ (XIX *) + χ E1	R(X，X *)V log Pβ (XIX * )
X 〜p(X∣Y * )	X 〜pβ (X IX * )
To train the NMT system, parallel sentence pairs (X， Y) are firstly sampled from two independent
cluster distributions and then translation confidence scores w(Y IX， X*)， w(XIY， Y*) are leveraged
to guide the training. The derivatives w.r.t the cluster contain two elements, candidates sampled
from translation system, and candidates sampled from cluster itself. The two components together
ensure parallel cluster’s translatability and the similarity to the ground truth.
System-D With the same loss function in system-C, translation broadcasting strategy is leveraged
to compute derivatives, instead of parallel sampling, and the gradients is shown as:
VnLossD =	E
Y 〜Pa(YY
*
)x~pEx∣y)-PWxΓVn logPn(XIY)
EE
Y 〜Pa(Y∣Y *)X 〜Pη(X∣Y)
- c(XIX*， Y*)Vn logPn(XIY)
VY LossD
EE
X 〜Pβ (X∣X*)Y 〜Pγ (Y∣X)
-Pa(YIY* )
P(Y IX *)
VY logPY(YIX)
(16)
EE
X 〜Pβ (X∣X*)Y 〜Pγ (Y∣X)
-c(YIY*，X*)VYlogPY(YIX)
Vβ LossD =Vβ LossC
VaLossD =VaLossC
System-D works quite similar as system-B but differs in that cluster confidence scores
c(XIX*， Y*)， c(YIY*， X*) are leveraged in training NMT, hence it is more abundant than task-
level rewards (R(X， X*) and R(Y， Y*)). System-D adopts the same gradient formulas in system-C
to update the clusters.
The details of the training algorithm for system-A,B,C,D are shown in Algorithm 1:
7
Under review as a conference paper at ICLR 2018
Algorithm 1 Cluster-to-Cluster Learning Framework
procedure PRE-TRAINING
InitializePY(Y|X), Pn(X|Y), Pe(X|X*), Pa(Y|Y*) with random weights η, γ, β and α
Pre-train thePY(Y|X) andPn(X|Y) via Maximum Likelihood Estimation
√-1	. rɪ-i	t	-∖7-	t W . 1	1	∕P7∣，广、	t
Generate Translations X and Y through Pn (X |Y) andPY(Y|X)
〜
〜
Pre-train pβ (X |X *) and Pa(Y |Y *) with data pair (X,X) and (Y, Y)
end procedure
procedure ALTERNATE ITERATIVE STRATEGY
while Not Converged do
Sample a random example (X*, Y*) from dataset D
if System-A then
Generate N sequence X, Y frompβ(X|X*) andPa(Y|Y*)
Evaluate X, Y with task-level reward and discard sequences (X, Y) below M
Compute gradient for both Pn (X|Y) and PY(Y|X) based on Equation 12
else if System-B then
Generate sequence X, Y fromPe(XX*) andPa(Y|Y*)
Evaluate X, Y with task-level reward and discard sequences (X, Y) below M
Translate corresponding sequence Y, X via translation system and sampled X, Y
Compute gradient for both Pn (X|Y) and PY(Y|X) based on Equation 13
else if System-C then
Generate sequence X, Y fromPe(X|X*) andPa(Y|Y*)
Evaluate X, Y with task-level reward and discard sequences (X, Y) below M
Compute gradientforPn(X|Y),PY(Y|X),Pe(X|X*),Pa(Y|Y*) based on Equation 15
else if System-D then
Generate N sequence X, Y fromPe(X|X*) and Pa(Y |Y *)
Evaluate X, Y with task-level reward and discard sequences (X, Y) below M
Translate corresponding sequence Y, X via translation system and sampled X, Y
Compute gradientforPn(X|Y),PY(Y|X),Pe(X|X*),Pa(Y|Y*) based on Equation 16
end if
Apply gradient descent: η = η - INnLoss and Y = Y - IZYLoss
if System-C or System-D then
Apply gradient descent: β = β - IrVeLoss and α = α - IrVaLoss
end if
end while
end procedure
5	Experiment
5.1 Machine Translation Results
To evaluate our cluster-to-cluster NMT framework on different-sized (small-data, medium-data and
large-data) and different-lingual (German-English and Chinese-English) translation tasks, we con-
duct experiments on three datasets (IWSLT, LDC, WMT). For more details about the datasets, please
refer to Appendix C. For comparability, we follow the existing papers to adopt similar network ar-
chitectures, and apply learning rate annealing strategy described in Wu et al. (2016) to further boost
our baseline NMT system. In our experiments, we design both the NMT and adaptive cluster mod-
els based on one-layered encoder-decoder network (Chung et al., 2014) with a maximum sentence
length of 62 for both the encoder and decoder. During training, ADADELTA (Zeiler, 2012) is
adopted with = 10-6 and ρ = 0.95 to separately optimize the NMT’s and adaptive cluster’s
parameters. During decoding, a beam size of 8 is used to approximate the full search space. We
compute the threshold similarity M via sentence-BLEU, some small-scaled experiments indicate
M = 0.5 yields best performance, so we simply stick to this setting throughout all the experi-
ments. To prevent too much hyper-parameter tuning in building the inadaptive cluster, we follow
Norouzi et al. (2016) to select the best temperature τ = 0.8 in all experiments. For comparison,
RAML and RL systems are also implemented with the same sequence-to-sequence attention model,
8
Under review as a conference paper at ICLR 2018
following Norouzi et al. (2016) and Williams (1992). For more details of our RL’s and RAML’s
implementations, please refer to Appendix A.
IWSLT2014 German-English We can see from Table 3 that our system-D achieves significant
improvements on both directions. Though our baseline system is already extremely strong, using
cluster-to-cluster framework can further boost the NMT system by over 1.0 BLEU point.
Direction	DE2EN		EN2DE	
Methods	Baseline	Model	Baseline	Model
MIXER (Ranzato et al., 2015)	20.10	21.81	-	-
BSO(Wiseman & Rush, 2016)	24.03	26.36	-	-
A-C (Bahdanau et al., 2016)	27.56	28.53	-	-
Softmax-Q (Ma et al., 2017)	27.66	28.77	-	-
Our implementation of RL (Williams, 1992)	29.10	29.70	24.40	24.75
Our implementation of RAML (NoroUzi et al., 2016)	29.10	29.47	24.40	24.86
cluster-to-cluster NMT (System-A)	29.10	30.08	24.40	25.15
cluster-to-cluster NMT (System-B)		30.10		25.20
cluster-to-cluster NMT (System-C)		30.30		25.35
cluster-to-cluster NMT (System-D)		30.50		25.46
Table 3: Experimental results on IWSLT-2014 German-English Machine Translation Task
LDC Chinese-English We can see from Table 4 that System-D achieves the best performance in
the CH2EN translation direction, while System-D achieves the best performance on most test data
sets in the EN2CH translation direction. The average improvement over the baseline systems for
both EN2CH and CH2EN direction are around 1.0 BLEU.
Direction	CH2EN (NIST03/05/06)		EN2CH (NIST03/05/06)		
Methods	Baseline	Model	Baseline	Model
Our RL	39.04/37.13/39.13	40.98/39.23/39.27	17.57/16.38/17.31	18.44/16.98/17.99
OUr RAML 一	39.04/37.13/39.13	40.28/37.28/37.20~	17.57/16.38/17.31	17.83/16.52/1679"
System-A	39.04/37.13/39.13	40.11/38.68/39.41-	17.57/16.38/17.31	18.81/17.32/1782"
SyStem-B		41.55/38.93/39.19		18.53/16.99/17.58
SyStem-C		41.69/39.05/39.17		18.50/17.36/17.88
SyStem-D-		41.78/39.15/3O8~		18.71/17.13/1777^
Table 4: Experimental results on NIST Chinese-English Machine Translation Task
WMT2014 German-English We can see from Table 5 that system-C achieves the strongest result
on both WMT14 EN-DE and DE-EN tasks, which outperforms the baseline system by over 1.1
BLEU points. It’s worth noting that our one-layer RNN model even outperforms the deep multi-
layer RNN model of Zhou et al. (2016) and Luong et al. (2015), which contain a stack of 4-7 LSTM
layers. By using cluster-to-cluster framework, our one-layer RNN model can fully exploit the dataset
and learn to generalize better.
5.2	Result Analysis
From the above 24 parallel cluster-to-cluster experiments, we observe general improvements over
the fine-tuned baseline systems as well as our implemented RL/RAML systems. To understand
the strength of our cluster-to-cluster framework, we give more detailed comparisons with existing
competing algorithms as below:
Comparison with RAML From the above three tables, we can observe general improvements
yielded by RAML algorithm on different tasks (except LDC Chinese-English), but RAML still suf-
fers from two problems: on one hand, RAML’s benefits is restricted by its neglect of the input
variabilities, and on the other hand, without considering semantic contexts and language fluency,
9
Under review as a conference paper at ICLR 2018
Direction	DE2EN		EN2DE	
Methods	Baseline	Model	Baseline	Model
Attention-NMT (BahdanaU et al., 2014)	-	-	16.46	18.79
Attention-NMT with LV (Jean et al., 2014)	-	-	16.95	19.40
Local-p Attention NMT (Luong et al., 2015)	-	-	19.0	20.90
DeeP-Att (ZhoU et al., 2016)	-	-	16.5	20.60
Our implementation of RL (Williams, 1992)	25.13	25.78	20.13	20.66
Our implementation of RAML (Norouzi et al., 2016)	25.13	25.54	20.12	20.54
cluster-to-cluster NMT (System-A)	25.13	25.94	20.12	20.90
cluster-to-cluster NMT (System-B)		25.94		20.71
cluster-to-cluster NMT (System-C)		26.26		21.30
cluster-to-cluster NMT (System-D)		26.09		20.06
Table 5: Experimental results on WMT-2014 German-English Machine Translation Task
RAML’s random replacement strategy may introduce noisy and wrong bilingual pairs to hurt the
translation performance (like in LDC Chinese-English translation task). Our adaptive cluster takes
into account more semantic contexts to enclose more rational paraphrases, and the bilateral augmen-
tation also empowers the model more chance to access various inputs.
Comparison with RL We can also observe prevalent improvements yielded by RL algorithm
Bahdanau et al. (2016); Ranzato et al. (2015). Exposing the model to self-generated translation can
improve the performance. Our methods inherit this merit and further enhance it with source and
target clusters, which can improve the model with more sampled bilingual pairs from both source
and target sides.
Comparison between four parallel systems Among our proposed four parallel systems, system-
C and D achieve better performances than A and B throughout different experiments, which confirms
the advantages of the adaptive clusters. The adaptive cluster is more flexible and target optimized
than inadaptive cluster. Unlike the payoff distribution used in inadaptive cluster which only takes
task-level reward into account, the adaptive cluster learns more sophisticated criterion and thus as-
signs more rational probability to sampled candidates. We give more detailed analysis and visual-
ization in the appendices to demonstrate how the source and target clusters look like.
5.3	Learning curve and Case studies
We demonstrate the learning curves of four systems and visualize some adaptive clusters in Ap-
pendix D and Appendix E, which give a more intuition about cluster-to-cluster learning.
6 Conclusion
In this paper, we propose a cluster-to-cluster learning framework and incorporate this concept into
neural machine translation. Our designed systems have proved to be efficient in helping current
NMT model to generalize in both source and target sides. In the cluster-to-cluster framework, the
cooperation of four agents can augment valuable samples and alleviate data sparsity, and achieve
significant improvement compared with strong baseline systems. We believe the concept of cluster-
to-cluster learning can be applicable to a wide range of natural language or computer vision tasks,
which will be explored in the future.
References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473, 2014. 1, 2, 10, 12, 14
Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron
Courville, and Yoshua Bengio. An actor-critic algorithm for sequence prediction. arXiv preprint
arXiv:1607.07086, 2016. 1, 2, 9, 10
10
Under review as a conference paper at ICLR 2018
Thorsten Brants, Ashok C Popat, Peng Xu, Franz J Och, and Jeffrey Dean. Large language models in
machine translation. In In Proceedings of the Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural Language Learning. Citeseer, 2007. 3
Yong Cheng, Wei Xu, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. Semi-
supervised learning for neural machine translation. arXiv preprint arXiv:1606.04596, 2016. 1,
3
KyUnghyUn Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014. 1, 6, 12
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of
gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014. 8
Junyoung Chung, Kyunghyun Cho, and Yoshua Bengio. A character-level decoder without explicit
segmentation for neural machine translation. arXiv preprint arXiv:1603.06147, 2016. 14
Di He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu, Tieyan Liu, and Wei-Ying Ma. Dual learning
for machine translation. In Advances in Neural Information Processing Systems, pp. 820-828,
2016. 1,3
Sebastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. On using very large target
vocabulary for neural machine translation. arXiv preprint arXiv:1412.2007, 2014. 10, 14
Alex M Lamb, Anirudh Goyal ALIAS PARTH GOYAL, Ying Zhang, Saizheng Zhang, Aaron C
Courville, and Yoshua Bengio. Professor forcing: A new algorithm for training recurrent net-
works. In Advances In Neural Information Processing Systems, pp. 4601-4609, 2016. 2
Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-
based neural machine translation. arXiv preprint arXiv:1508.04025, 2015. 9, 10, 14
Xuezhe Ma, Pengcheng Yin, Jingzhou Liu, Graham Neubig, and Eduard Hovy. Softmax q-
distribution estimation for structured prediction: A theoretical interpretation for raml. arXiv
preprint arXiv:1705.07136, 2017. 1, 3, 9
Mohammad Norouzi, Samy Bengio, Navdeep Jaitly, Mike Schuster, Yonghui Wu, Dale Schuurmans,
et al. Reward augmented maximum likelihood for neural structured prediction. In Advances In
Neural Information Processing Systems, pp. 1723-1731, 2016. 1, 2, 3, 4, 5, 8, 9, 10, 13
Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level train-
ing with recurrent neural networks. arXiv preprint arXiv:1511.06732, 2015. 1, 2, 9, 10
Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation models
with monolingual data. arXiv preprint arXiv:1511.06709, 2015a. 1, 3
Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with
subword units. arXiv preprint arXiv:1508.07909, 2015b. 14
Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. Minimum
risk training for neural machine translation. arXiv preprint arXiv:1512.02433, 2015. 2
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, volume 1. MIT
press Cambridge, 1998. 2
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229-256, 1992. 5,7, 9, 10
Sam Wiseman and Alexander M Rush. Sequence-to-sequence learning as beam-search optimization.
arXiv preprint arXiv:1606.02960, 2016. 9
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine trans-
lation system: Bridging the gap between human and machine translation. arXiv preprint
arXiv:1609.08144, 2016. 8, 14
11
Under review as a conference paper at ICLR 2018
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,
2012. 8
Jiajun Zhang and Chengqing Zong. Exploiting source-side monolingual data in neural machine
translation. In EMNLP, pp. 1535-1545, 2016. 1, 3
Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward
connections for neural machine translation. arXiv preprint arXiv:1606.04199, 2016. 9, 10
Appendices
A System-Design
Sequence to sequence problem (machine translation) can be considered to produce an output se-
quence Y = (yι, y2,..., yτ),yt ∈ A given an input X. Given input-target pairs (X, Y*), the
generated sequence Y on test is evaluated with task-specific score R(Y, Y*). Recurrent neural net-
works have been widely used in sequence to sequence prediction tasks. As proposed in Cho et al.
(2014) and Bahdanau et al. (2014), the basic idea is to first encode the input sequence as a variable-
length feature vectors, then apply attention mechanism to compute weighted average over the input
vectors and summarize a context vector, with which, previous hidden states and previous label are
fed into the decoder RNN to predict the next state and its label. In our approach, attention-based
encoder-decoder (Bahdanau et al., 2014; Cho et al., 2014) is leveraged for both the translation and
cluster models, shown as:
yt 〜g(st-ι,ct-ι)
st = f(st-1, ct-1, e(yt))
αt = β(st, (h1, . . . , hL))
L
ct =	αt,jhj
j=1
(17)
(18)
(19)
(20)
A.1 RL NMT
In order to train our RL system as well as adaptive cluster, we need to define a task-level reward as
driving signal. Instead of directly applying BLEU or other evaluation metric, we advocate to use a
surrogate n-gram match interpolation, as shown as:
R(Y, Y*) = 0.4 * N4 + 0.3 * N3 + 0.2 * N2 + 0.1 * Ni	(21)
where Nn denotes the number of n-gram match between Y and Y *. In order to alleviate sequence-
reward sparseness, we further split it as a series of local reward to drive model’s policy search at
every time step. Formally, we write the step-wise reward r(yt|y1:t-1, Y*) as following.
	1.0; 0.6;	0< N(Y*,yt-3:t) ≤ N(Y,yt-3:t) 0< N(Y*,yt-2:t) ≤ N(Y,yt-3:t)
r(yt |yi：t-i,Y *)= <	0.3;	0 <N (Y *,yt-i：t) ≤ N (Y,yt-3：t)	(22)
	0.1;	0 <N (Y*,yt) ≤ N (Y,yt-3：t)
	、0.0;	otherwise
1	"T∕1r W∖	. .1	Γ∙	W ♦	，广	∙ r∙ 11 ♦ C	.
where N(Y, Y) represents the occurrence of n-gram Y in sequence Y, specifically, if a certain n-
sequence yt-n+1:t appears in reference and it’s not repeating more than needed, then we assign a
corresponding matching score to yt, the policy gradient is described as:
Vθ = £r(yt|yi：t-i,Y *)Vθ log pθ (yt|yi：t-i,X)	(23)
t
12
Under review as a conference paper at ICLR 2018
A.2 RAML NMT
In order to sample from the intractable payoff distribution for system-A/B as well as our imple-
mented RAML system, we adopt stratified sampling technique described in Norouzi et al. (2016).
Given a sentence Y*, We first sample an edit distance m, and then randomly select m positions
to replace the original labels. For each sentence, we randomly sample four candidates to perform
RAML training.
Vθ =	E	V logpθ(YX)	(24)
Y ~q(Y∣Y *)
B Mathematical analysis
We optimize the model parameters of our cluster-to-cluster models by minimizing the loWer-bound
KL-divergence instead of maximizing the original correspondence score, to characterize the differ-
ence betWeen the tWo objective function, We analyze the relationships betWeen these tWo functions
beloW:
Proposition 1. For any two non-zero distributions P(Y|Y*) and P(Y|X*), we can construct an
auxiliary distribution P(Y|X* ,Y*) as P；(YY0*Y(Y〔X*∣X*)to get
CRc→c(X → Y) + H(P(Y|YI)=KL(P(Y|Y*)∣∣P(Y|X*)) + CE(P(Y|Y*),p(YX*,Y*))
(25)
This reveals the relationship between our regularized cluster-to-cluster correspondence objective and
the lower-bound KL-divergence, and their difference can be expressed as the cross-entropy between
P(Y|Y*) andP(Y|X*,Y*).
Proposition 2. The cross-entropy between P(Y|Y*) and P(Y|X*,Y*) achieves minimum if and
only ifP(Y|X* ) = U nif orm(Y):
argmin CE(P(Y|Y*),
p(Y|X*)
P(Y ∣Y*)p(Y |X *))
Py 0 P(Y 0|Y *)p(Y |X *)))
Uniform(Y)
(26)
According to the above two propositions, the gap between two objective functions decreases
as P(Y|X*) approaches uniform distribution, therefore, by introducing an entropy regularization
H(P(Y|X*)) during training, the gap can be further minimized. Although we have not yet in-
corporated this additional uniform regularization term into cluster-to-cluster framework, this is an
interesting research direction.
Here is the proof of proposition 1:
CRc→c(X → Y) + H(P(Y|Y*)) - KL(P(Y|Y*)||P(Y|X*))
=logXXP(X|X*)P(Y|Y*)P(Y|X) + H(P(Y|Y*)) - KL(P(Y|Y*)||P(Y|X*))
=logXP(Y|Y*)P(Y|X*) + H(P(Y|Y*)) - KL(P(Y|Y*)||P(Y|X*))
Y
=XP(Y|Y*)logXP(Y0|Y*)P(Y0|X*)+H(P(Y|Y*))-KL(P(Y|Y*)||P(Y|X*))
=Xp(Y|Y*)log PY0P(Y(OYXP(YOX*) + H(p(Y|Y*))
Y	P(Y |X )
=X P(Y 1Y *)iog PY0 P(YiY*)P(YOX *)
V ) g	P(YX*)p(y∣y*)
=CE(p(Y |Y *),P(Y X *,Y *))
Here is the proof of proposition 2:
argmin CE(p(Y |Y *),P(Y |X *,Y *)) = p(Y |Y *)
p(Y ∣X*,Y *)
(27)
(28)
13
Under review as a conference paper at ICLR 2018
which can be further written as:
therefore, we can derive:
p(Y∣Y*)p(Y∣X *)
Py 0 p(Y0∣Y *)p(Y0∣x *)
= p(Y |Y *)
p(Y|X*) =	p(Y0|Y*)p(Y0|X*)
Y0
C onstant
(29)
(30)
1
百
Since both cluster and translation confidence score c(Y |Y *, X*) and w(Y |X, X*) require comput-
ing the marginalized probability p(Y|X*) known to be intractable for variable-length sequences,
here we adopt different mechanisms to approximate them. In system-A and C, we simplify
PXpγ(YX)pβ(X|X*) as pγ(Y|X*) to approximate W(Y|X,X*) as ；*X)). In System-B and
D, since Y is broadcast through the translation system, the marginalized probability P(Y|X*) is
close to one, we discard this factor and approximate C(Y|Y*,X*) as R(Y, Y*)∕pα(Y|Y*).
C Dataset Description
IWSLT2014 Dataset The IWSLT2014 German-English training data set contains 153K sentences
while the validation data set contains 6,969 sentences pairs. The test set comprises dev2010,
dev2012, tst2010, tst2011 and tst2012, and the total amount are 6,750 sentences. We adopt 512
as the length of RNN hidden stats and 256 as embedding size. We use bidirectional encoder and
initialize both its own decoder states and coach’s hidden state with the learner’s last hidden state.
The experimental results for IWSLT2014 German-English and English-German Translation Task
are summarized in Table 3.
LDC Dataset The LDC Chinese-English training corpus consists of 1.25M parallel sentence,
27.9M Chinese words and 34.5M English words. We choose NIST 2003 as our development set
and evaluate our results on NIST 2005, NIST2006. We adopt a similar setting as IWSLT German-
English translation task, we use 512 as hidden size for GRU cell and 256 as embedding size. The
experimental results for LDC Chinese-English translation task are listed in Table 4.
WMT2014 Dataset The WMT2014 German-English training data consists of 4.5M sentences
pairs (116M English, 110M German words), we follow Sennrich et al. (2015b) to apply byte pair
encoding (BPE) to deal with the massive number of rare words in this translation task. Newstest2012
and newstest2013 are used as development set to select best hyperparameters. Translation perfor-
mance are reported in case-sensitive BLEU on newstest2014. Similar to Jean et al. (2014) and Bah-
danau et al. (2014), we use 2014 as length of RNN hidden units and 512 as embedding size. Unlike
the previous works (Luong et al., 2015; Chung et al., 2016; Wu et al., 2016), which uses multi-layer
stacking LSTM/GRU to boost the NMT performance, our model only use one-layer encoder-decoder
architecture. Thus, our results are not directly comparable with their results. The experimental re-
sults for WMT14 German-English Translation task are listed in Table 5.
D	Learning Curve
Here we draw the learning curve on WMT2014 German-English translation task in Figure 2 to
demonstrate a more intuitive comparison between system-A/B and system-C/D. From these curves,
we can observe steady improvements for system-C/D and unstable vibration for system-A/B.
Though system-C/D starts from a lower point, they gradually find better convergence points than
system-A/B. We attribute the smoothness to adaptive cluster’s capability to interact with the NMT
models in system-C and D, in other words, the adaptive cluster can accordingly update its distribu-
tion to stabilize NMT’s training.
We also demonstrate the learning curve of the adaptive cluster during the joint training in Figure 3.
We use cluster distribution to perform beam-search and find out its most-likely sampled candidates
and evaluate their BLEU against the ground truth for every iteration. We can observe that cluster’s
BLEU improvement is highly correlated with NMT’s BLEU improvements. When the translation
system becomes better, the adaptive cluster will concentrate more on the ground truth.
14
Under review as a conference paper at ICLR 2018
25.5
25.3
25.1
24.9
ɔ 24.7
m 24.5
24.3
24.1
23.9
23.7
Cluster-Cluster Learning Curve For System A/B
Cluster-Cluster Learning Curve For System C/D
25.5
25.3
25.1
24.9
n 24.7
24.5
24.3
24.1
23.9
23.7
K Batch
System-C DE-EN System-D DE-EN
K Batch
----System-A DE-EN -----System-B DE-EN
21.2
21
20.8
20.6
20.4
ɔ
匕 20.2
CQ 20
19.8
19.6
19.4
19.2
K Batch
21.2
21
20.8
20.6
口 20.4
匕 20.2
CQ 20
19.8
19.6
19.4
19.2
K Batch
System-C EN-DE System-D EN-DE
----System-A EN-DE -----System-B EN-DE
Figure 2: The trend of BLEU on WMT2014 DeV set for cluster-cluster system
----System-C EN-CH ------System-C CH-EN
Figure 3: AdaptiVe cluster’s BLEU trend on WMT2014 DeV set
E	Case S tudy
In order to giVe a more intuitiVe View about what the cluster distribution looks like, we draw some
samples from the well-trained cluster distribution in LDC Chinese-English Translation task as shown
in Table 6. we can obserVe that most of the paraphrases are based on three types of modification,
namely form changing, synonym replacement as well as simplification. Most of the modifications
does not alter the original meaning of the ground truth. Encompassing more expressions with close
meanings can ease the data sparseness problem, and enhance its generalization ability. We here
draw two samples from source and target clusters in Figure 4, which demonstrates how point-point
correspondence can be expanded into cluster-to-cluster correspondence.
15
Under review as a conference paper at ICLR 2018
Property	Synonym Replacement
Reference	taihsi natives seeking work in other parts of the country are given a thorough UNK before being hired, and later their colleagues maintain a healthy distance at first.
Cluster	taihsi natives seeking work in other parts of the country are given a thorough UNK before being employed, and later their colleagues maintain a healthy distance at first.
Property	Simplification
Reference	i once took mr tung chee - hwa to a squatter area where he found beyond imagination that a narrow alley could have accommodated so many people.
Cluster	i once took mr tung chee - hwa to a squatter area where he fo und beyond imagination that a narrow alley have a lot of people.
Table 6: Samples drawn from cluster distribution in LDC Chinese-English Translation task
alles hangt zusammen .
alle hangt zusammen .
alles hangt gemeinsam .
nichts hangt zusammen .
alles abhangt zusammen .
alles hangt miteinander.
alles bezieht zusammen .
alles hangen zusammen .
everything is linked together .
everything is connected together .
everything is analyzed together .
whole is linked together .
everything is connects together .
everything is connecting together .
everything is relate together .
everything is bringing together .
was bedeutet das ?
was bedeutet das ? das bedeutet das ?
WaS bedeutet das bedeutet das ?
was bedeutet das ? das ?
was heiβt das ?
wie bedeutet das ?
das bedeutet das ?
was bedeutete das ?
Figure 4: Cluster Visualization
what does that mean ?
why does that mean ?
how does that mean ?
what does that news ?
where does that mean ?
which does that mean ?
thing does that mean ?
what makes that mean ?
16