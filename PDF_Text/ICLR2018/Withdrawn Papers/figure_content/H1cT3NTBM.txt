Figure 1: An overview of the neural network architecture used in this paper. Before feeding theoutput of the convolutional layers to the global time-wise aggregation, the 3 - D feature map havingthe shape (# of channels, # of time frames, # of frequency bins) as their dimensions will be reshapedas 2-D matrices having the shape (# of time frames, # of channels× # of frequency bins)2.2	Global AggregationOriginally, the attention mechanism was introduced for sequence-to-sequence learning (Bahdanauet al., 2014) in an RNN architecture, that allows the prediction at each time-step to access informa-tion from every step in the input hidden sequence in a weighted way. Since the experiments donein this paper do not need sequence-to-sequence prediction, the feed-forward version of attentionproposed in (Raffel & Ellis, 2015; 2016) is used instead of the original one. The feed-forward at-tention is formulated as follows: Given the input matrix X ∈ RN ×D representing N frames of Ddimensional feature vectors, a weight vector σ ∈ RN over the time-steps is calculated byσ = softmax(f (X w + b))	(1)whereexmSoftmax(x)m =——E-------	(2)PnN=1 exnand f is a non-linear function (tanh for the experiments done in this paper), and w ∈ RD andb ∈ R are the learnable parameters, which can be learned by back-propagation. The output X of thefeed-forward attention layer is then calculated via
Figure 2: (a) is vanilla 2-layer CNN block, (b) is a ResNet 3-layer bottleneck block and can be seenas a ResNeXt block with cardinality of 1, and (c) is a 3-layer ResNeXt block with cardinality of 4.
Figure 3: Train/validation losses over epochs on selected subsets of experimented networks config-urations. The top shows results from two neural networks configurations with different aggregationmethods used. The bottom row depicts two sets of results that have either feed-forward attention oraverage aggregation with varying CNN blocks choices and depths.
Figure 4: t-SNE projections of the embedded performances compared to baseline handcrafted audiofeatures. The top row is colored by singer identities while the bottom is colored by song identities.
Figure 5: Bar plots of classification accuracies using k-nearest neighbor classification, on the em-beddings learned from the singing performance embedding experiment. ResNeXt configurationswith/without feed-forward attention and the handcrafted features (baseline) are and four k valuesare experimented. Left bar plot is for singer classification, while the right plot is for performed songclassification.
Figure 6: A detailed specification of one of the network configuration used (ResNeXt shallow forsinger classification). Numbers next to arrows are the dimensions of the feature maps, while thedescriptions in the boxes are operation specifics. Left plot is a flowchart from input to just beforeoutput layer, while the right plot is the detailed specifics for the RexNeXt configuration.
Figure 7: t-SNE projections of the embedded performance clips compared to baseline handcraftedaudio features. The top row is colored by singer identities while the bottom is colored by songidentities.
