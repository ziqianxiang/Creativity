Figure 1: We propose a non-parametric method to explain and modify the behavior of convolutionalnetworks, including those that classify pixels and those that generate images- For example, giventhe label mask on the top, why does a network generate strange gray border artifacts? Given theimage on the bottom, how is a network able to segment out the left-most telephone pole in shadow?We advocate an “explanation-by-example" approach to interpretation (Caruana et al., 1999). NeXtto the CNN output, we show the closest-matching training exemplar image. It appears to providecoarse explanations of such behaviors, though the quality of the output is still lacking (e.g., addi-tional cars are hallucinated in the bottom row). One the right, we show the output obtained througha compositional nearest-neighbor operation that simply (1) matches input patches to those in thetraining set and (2) returns the corresponding output label. This means that the output is created bycutting-and-pasting (composing) patches of training images. To ensure that inconsistent patches arenot composed together, one needs to match patches using an embedding that captures both globalsemantics (e.g., architectural styles) and local structure (e.g., windows versus doors). We demon-strate that local convolutional neighborhoods of feature activations produce such rich embedding.
Figure 2:	Overview of pipeline: Given an input label or image (top-left of each box), our approachextracts an embedding for each pixel. We visualize two pixels with a yellow and white dot. Theembedding captures both local and global context, which are crudely visualized with the surround-ing rectangular box. We then find the closest matching patches in the training set (with a nearestneighbor search), and then report back the corresponding pixel labels to generate the final output(bottom-left of each box). We visualize an example for label-to-image synthesis on the left, andimage-to-label prediction on the right.
Figure 3:	We visualize a non-parametric approach to computing activations from internal layers.
Figure 4: Adding composition by matching to later layers: We apply compositional nearest-neighbor matching to features extracted from different layers, starting with the bottleneck layerand progressing to the penultimate deconv2 layer. We match local neighborhoods of convolutionalembeddings, which naturally allows for more composition as We use later laters.
Figure 5:	Original labels v.s. self-supervised labels: Given the label input on the left, we show re-sults of Pix2Pix in the Convolutional Neural Networks column, and non-parametric matching to thetraining set using the original labels and the predicted “self-supervised” labels of the Pix2Pix net-work. Generating images with the predicted labels looks smoother, though the qualitative behaviorof the network is still explained by the original training labels. We quantify this in our experimentalresults, and include additional qualitative visualizations of the original and self-supervised labels inFigs. 13 and 14.
Figure 6:	Reconstruction: We can use our nonparametric matching framework to generate re-constructions by replacing the exemplar target label yn with the exemplar input image xn . Thiscan be done for both image generation and discrete label prediction. We find that, perhaps surpris-ingly, pixel embeddings contain enough local information to reconstruct the input pixel. We showadditional results in Fig. 10.
Figure 7: Semantic segmentation: The results of semantic segmentation on cityscape and CamViddataset. This result suggests the following observations. First, the difference between images gener-ated by generative networks (Pix2Pix and SegNet columns) and NN embedding (CompNN column)is surprisingly small. Thus, our method can perfectly interpret discriminative deep networks onpixel-level classification. Second, we can notice some noise edges with a high gradient (see columns5-8). This phenomenon can also be used to understand the difficulty of image segmentation task:Neural Network	Neural NetworkFigure 8: Image synthesis: The results suggest that Comp NN (our approach) can also explain theresults from Pix2Pix. We conclude this because our approach reproduces color and the structure ofthe Pix2Pix output, including a few artifacts (e.g., the image cuts in the 6th and 7th columns).
Figure 8: Image synthesis: The results suggest that Comp NN (our approach) can also explain theresults from Pix2Pix. We conclude this because our approach reproduces color and the structure ofthe Pix2Pix output, including a few artifacts (e.g., the image cuts in the 6th and 7th columns).
Figure 9: Bias modification: Given the same label input, we show different results obtainedby matching to different databases (using an embedding learned by Pix2Pix). By modifying thedatabase to include specific buildings from specific locations, one can introduce and remove im-plicit biases in the original network (e.g.,one can generate “European” facades versus “American”facades).
Figure 10: Reconstruction: Given the correspondences of NN features from penultimate the layer,we reconstruct the test input images (third column from left-to-right) by using the compositionalnearest-neighbor approach: copying and pasting corresponding image patches from the input imagesof the training set. The reconstructions using a compositional nearest-neighbor approach is shown inthe second column, while the reconstructions using a global nearest-neighbor approach is shown inthe first column. The learned embedding thus enables not only the reconstruction of the input image,but also of the output image (see the last two columns). These results suggest that the embeddingpossess not only the information relevant to a specific task, but also semantic information from theoriginal image. We can conclude then that CNNs understand an input image by finding the patchesfrom the training images that enable the composition of an image reproducing the input.
Figure 11: Correspondence map: Given the input label mask on the top left, we show the ground-truth image and the output of Pix2Pix below. Why does Pix2Pix synthesize the peculiar red awningfrom the input mask? To provide an explanation, we use CompNN to synthesize an image byexplicitly cutting-and-pasting (composing) patches from training images. We color code pixels inthe training images to denote correspondences. For example, CompNN copies doors from trainingimage A (blue) and the red awning from training image C (yellow).
Figure 12: Global NN v.s. Comp NN. We show synthesized images using our CompNN methods andfour global NN approaches (global nearest neighbor on bottleneck feature embedding and Decode2feature embedding using self-supervised labels and original labels respectively). We can observethat (1) compositional nearest neighbor outperforms other global nearest neighbor approaches, (2)using Decode2 features (the penultimate layer) sometimes can generate more similar structures (Seerow 1,4).
Figure 14: Synthesized images for pixel-wise prediction tasks with a Convolutional Neural Network,and Compositional Nearest Neighbors using self-supervised and original labels.
