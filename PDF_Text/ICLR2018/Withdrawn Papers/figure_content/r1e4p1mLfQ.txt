Figure 1: A typical RNN based encoder-decoder framework for S2S. The first layer in the decoderis used to compute the attention and the attentional context vectors are fed into every upper decoderlayer.
Figure 2: Illustration of our proposed DenseRNN for S2S learning. The figure is split by the purpledash line into two parts: 1) The left part indicates the dense encoder, where the red lines indicatedense connections, and the dense decoder has the similar architecture. 2) The right part indicates thedense attention mechanism, specified by the green and the yellow lines.
Figure 3: Performance on validation set with respect to training time in English-French translationtask. All the models are trained on two Nvidia Titan Xp GPU cards.
Figure 4: Performance on validation set with respect to training time in English-German translationtask. All the models are trained on a single Nvidia Titan Xp GPU card.
