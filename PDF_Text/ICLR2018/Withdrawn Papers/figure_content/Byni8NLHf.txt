Figure 1: Word Pair ExtractionThe first part in each word pair segment represents the relationship between two words, such as det,nsubj, advmod, etc. And the number after each word gives its position of in the original sentence.
Figure 2:Figure 2: Two Steps TF-IDF Flow ChartFirst, word pairs are generated and word-level TF-IDF is performed. The result of word level TF-IDF is used as a filter and a word pair is kept only if the TF-IDF scores of both words are higher thanthe threshold (0.01). After that, we treat each word pair as a single unit, and the TF-IDF algorithmis applied to the word pairs and further filter out word pairs that are either too common or too rare.
Figure 2: Two Steps TF-IDF Flow ChartFirst, word pairs are generated and word-level TF-IDF is performed. The result of word level TF-IDF is used as a filter and a word pair is kept only if the TF-IDF scores of both words are higher thanthe threshold (0.01). After that, we treat each word pair as a single unit, and the TF-IDF algorithmis applied to the word pairs and further filter out word pairs that are either too common or too rare.
Figure 3:	OMDb fix total feature number word/word pair performance evaluationTable 3: 20NewsGroup fix total feature number word/word pair performance evaluationmAPF = 10.5Kword word pairF=11K	F = 11.5K	F=12K	F = 12.5K	F=15Kword word pair word word pair word word pair word word pair word word pairmAP 1mAP 3mAP 5mAP 100.73736	0.771290.65227	0.689050.60861	0.646200.55103	0.589920.73375	0.760930.64848	0.680420.60548	0.637830.54812	0.580570.68720	0.75865
Figure 4:	Reuters fix total feature number word/word pair performance evaluationTable 4: Different K Values for OMDbmAP	Baseline	K=100	K = 300	K = 500	K = 800	K = 1000mAP 1	0.14134	0.14474	0.14318	0.14312	0.14212	0.14275mAP 3	0.09212	0.09410	0.09222	0.09324	0.09219	0.09223mAP 5	0.07312	0.07419	0.07313	0.07374	0.07293	0.07310mAP 10	0.05113	0.05341	0.05254	0.05320	0.05243	0.05274mAP score for Reuters dataset in original model is already very high almost all of them higher than0.9, it is hard to get the improvement as large as OMDb dataset. For the mAP 1, the most significantimprovement happens when K = 500, which is 0.31%. For the mAP 5, the mAP5 and the mAP 10,the most significant improvement shown in K = 800, about 0.50%, 0.38% and 0.42%.
Figure 5:	20NewsGroup fix total feature number word/word pair performance evaluationTable 5: Different K Values for ReutersmAP	Baseline	K=100	K = 300	K = 500	K = 800	K = 1000mAP 1	0.94692	0.94565	0.94860	0.94988	0.94955	0.94898mAP 3	0.92719	0.92149	0.92715	0.92987	0.93146	0.93179mAP 5	0.91740	0.90859	0.91577	0.91881	0.92064	0.92088mAP 10	0.90078	0.88975	0.89869	0.90179	0.90403	0.90460provements about 2.82%, 2.90%, 3.2% and 3.33% respectively, and they all happened when K =1000.
Figure 6:	Reuters dataset mAP score evaluationTable 6: Different K Values for 20NewsGroupmAP	Baseline	K=100	K = 300	K = 500	K = 800	K = 1000mAP 1	0.73582	0.68122	0.71698	0.72272	0.75207	0.75659mAP 3	0.65447	0.58838	0.62933	0.63745	0.66834	0.67343mAP 5	0.61153	0.54296	0.58551	0.59431	0.62492	0.63118mAP 10	0.55651	0.48424	0.52789	0.53714	0.56724	0.575054.3.3 Word Pair Generation PerformanceIn this experiment, we compare different word pair generation algorithms with the baseline. Similarto previous experiments, the baseline is the word-only RBM model whose input consists of the10000 most frequent words. The ”semantic” word pair generation is the method we proposed in thispaper. By applying the idea from the skip-gram Mikolov et al. (2013b) algorithm, we generate theword pairs from each word’s adjacent neighbor, and we call it ”N-gram” word pair generation. Andthe window size we used in here is N = 2. For the Non-K word pair generation, we use the samealgorithm as the semantic except that no K-means clustering is applied on the generated word pairs.
Figure 7:	20NewsGroup dataset mAP score evaluationof the semantic generation is slightly higher than the mAP score of the Non-K generation. This isbecause, although both Non-K and semantic techniques extract word pairs using natural languageprocessing, without the K-means clustering, semantically similar pairs will be considered separately.
