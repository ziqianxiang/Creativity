Figure 1: Pipeline diagrams for forward and backward propagation.
Figure 2: Feed-forward neural networkTable 1: Equations for deep network nodesNode Type		Function	A	Forward	h(l)	=Θ⑷ h(IT)A|	Backward	δ(l)	=Θ(I)I δ(1+1)Θ	Update	Θ(l)	= Θ0 - R0t h(l-1) × δ(l)dtΦ	Activation	h(l)	= φ(h(l-1))Φ0	Tangent	δ(l)	= φ0(h(l-1))	δ(l+1)X	Input*	x=	Xbt/aCY	Label	y(x)	L	Loss	L(h(D),y)		* {Xk 〜 X} is a sequence of random observa-tions of X. x(t) is piecewise constant.
Figure 3: Each processor implements layer dy-namics. Param values and states (e.g., momen-tum) reside locally in the node. Activations anddeltas stream through, modified by local params.
Figure 4:	Validation-sample accuracy during training5.2 Anchored-Delta versus Immediate-Delta RulesWe train the convolution network under two conditions, following the discussion from Section 4: theanchored-delta rule, which uses the original parameters; and the immediate-delta rule, which usesthe evolved parameters. The goal of this substudy is to compare the learning performance of the twomethods. The immediate rule allows us to avoid using extra memory to store the model parametervalues for multiple input vectors.
Figure 5:	Comparison of immediate- and anchored- delta rules.
