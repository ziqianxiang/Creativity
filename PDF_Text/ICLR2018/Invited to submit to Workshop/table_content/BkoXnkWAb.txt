Table 1: Penn Treebank validation errors (BPC)	dropout	#parameters	ReLU	BReLU	ELU	BELU4 x 760	0.1	〜4.75M	1.321	1.324	DNC	1.3188 x 540	0.075	〜4.75M	1.331	1.320	DNC	1.31716x385	0.075	〜4.75M	1.321	1.324	DNC	1.31724x 314	0.075	〜4.75M	1.349	1.334	DNC	1.31736 x 256	0.05	〜4.75M	1.353	1.319	DNC	1.31148 x 222	0.05	〜4.75M	-	-	-	1.320144 x 128	0.025	〜4.75M	-	-	-	1.402From Table 1 we can see that ReLU-RNN performed worse with increasing depth. With ELU-RNN,learning did not converge. The bipolar version of ELU avoids this problem, and its performance doesnot degrade with increasing depth up to 36 layers. Overall, the best validation BPC is achieved withthe 36 layer BELU-RNN. Figure 6 shows the training error curves of the 36-layer RNN with each ofthe activation functions, and shows that the bipolar variants see a faster drop in training error.
Table 2: Penn Treebank test errorNetwork	BPCTanh + Zoneout (Krueger et al., 2016) ReLU 1x2048 (Neyshabur et al., 2016) GRU + Zoneout (Krueger et al., 2016) MI-RNN 1x2000 (Wu et al., 2016) DOT(S)-RNN (Pascanu et al., 2013a) LSTM 1x1000 (Krueger et al., 2016) LSTM 1x1000 + Stoch. depth (Krueger et al., 2016) LSTM 1x1000 + Recurrent BN (Cooijmans et al., 2016) LSTM 1x1000 + Dropout (Ha et al., 2016) LSTM 1x1024 + Rec. dropout (Semeniuta et al., 2016) LSTM 1x1000 + Layer norm (Ha et al., 2016) LSTM 1x1000 + Zoneout (Krueger et al., 2016) Delta-RNN + Dropout (II et al., 2017) HM-LSTM 3x512 + Layer norm (Chung et al., 2016) HyperNetworks (Ha et al., 2016) BELU 36x256	1.52 1.47 1.41 1.39 1.386 1.356 1.343 1.32 1.312 1.301 1.267 1.252 1.251 1.24 1.233 1.2706Under review as a conference paper at ICLR 20184.2	Character-level Text8Text8 (Mahoney, 2011) is a simplified version of the Enwik8 corpus with a vocabulary of27 characters.
Table 3:	Text8 validation error [BPC]Network	ReLU BReLU ELU BELU36x474 RNN DNC	1.399	DNC 1.334We compare the result on the test set with reported results obtained with approximately the samenumber of parameters. From Table 4 we can see that the result for the 36 layer BELU-RNN improvesupon the best reported result for non-gated architectures (Skipping-RNN).
Table 4:	Text8 test errorNetworkMI-Tanh 1x2048 (Wu et al., 2016)LSTM 1x2048 (Wu et al., 2016)Skipping-RNN (Pachitariu & Sahani, 2013)MI-LSTM 1x2048 (Wu et al., 2016)LSTM 1x2000 (Cooijmans et al., 2016)LSTM 1x2000 (Krueger et al., 2016)mLSTM 1x1900 (Krause et al., 2016)LSTM 1x2000 + Recurrent BN (Cooijmans et al., 2016)LSTM 1x2000 + Stochastic depth (Krueger et al., 2016)LSTM 1x2000 + Zoneout (Krueger et al., 2016)Recurrent Highway Network (Zilly et al., 2016)HM-LSTM 3x1024 + Layer norm (Chung et al., 2016)BELU 36x474BPC1.521.511.481.44
Table 5:	CIFAR-10 test error with moderate data augmentation [%]Network	ReLU	BReLU	ELU	BELUOrientedResPonseNet-28 (no BN, 30% dropout)	9.20	4.91	9.03	5.35WideResNet-28 (no BN, 30% dropout)	9.78	6.03	7.69	6.125 ConclusionWe have introduced bipolar activation functions, as a way to pull the mean activation of a layertowards zero in deep neural networks. Through a series of experiments, we show that bipolarReLU and ELU units can improve trainability in deeply stacked, simple recurrent networks and inconvolutional networks.
