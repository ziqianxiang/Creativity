Table 1: Performance of different architectures and sentence representations on unsupervised similarity tasksusing dot product as the similarity measure. On each task, the highest performing setup for each encoder typeis highlighted in bold and the highest performing setup overall is underlined. All reported values indicatePearson/Spearman correlation coefficients for the task. RNN encoder: Unrolling the RNN decoders using theconcatenation of the decoder hidden states (RNN-concat) dramatically improves the performance across alltasks compared to using the raw encoder output (RNN-RNN), validating the theoretical justification presentedin Section 3.3. BOW encoder: Unrolling the RNN decoders improves performance overall, however, theimprovement is less drastic than that observed for the RNN encoder, which we discuss further in the main text.
Table 2: Performance of different architectures and sentence representations on supervised transfer tasks. Oneach task, the highest performing setup for each encoder type is highlighted in bold and the highest performingsetup overall is underlined. All reported values indicate test accuracy on the task, except for SICK-R where wereport the Pearson correlation with human-provided scores. Note that the analysis in Section 3 is not readilyapplicable here, as instead of using a similarity measure in the representation space directly, the supervisedtransfer tasks train an entirely new model on top the chosen representation.
Table 3: Performance of the SkipThought model, with and without layer normalisation (Kiros et al., 2015;Ba et al., 2016), compared against the RNN-RNN model used in our experimental setup. On each task,the highest performing model is highlighted in bold. For SICK-R, we report the Pearson correlation, andfor STS14 we report the Pearson/Spearman correlation with human-provided scores. For all other tasks, re-ported values indicate test accuracy. f indicates results taken from Conneau et al. (2017). ∣ indicates ourresults from running SentEval on the model downloaded from Ba et al. (2016)’s publicly available codebase(https://github.com/ryankiros/layer-norm). We attribute the discrepancies in performance todifferences in experimental setup or implementation. However, we expect our unrolling procedure to also boostSkipThought’s performance on unsupervised similarity tasks, as we show for RNN-RNN in our fair single-codebase comparisons in the main text.
Table 4: Performance of different architectures and sentence representations on unsupervised similarity tasksusing the cosine similarity between two vectors as the measure of their similarity. On each task, the highestperforming setup for each encoder type is highlighted in bold and the highest performing setup overall isunderlined. All reported values indicate Pearson/Spearman correlation coefficients for the task. RNN encoder:Using the raw encoder output (RNN-RNN) achieves the lowest performance across all tasks. Unrolling theRNN decoders dramatically improves the performance across all tasks compared to using the raw encoderRNN output, validating the theoretical justification presented in Section 3.3. BOW encoder: We do notobserve the same uplift in performance from unrolling the RNN encoder compared to the encoder output. Thisis consistent with our findings when using dot product (see Table 1).
Table 5: Performance of different architectures and sentence representations on unsupervised similarity tasksusing dot product as the similarity measure. On each task, the highest performing setup for each encoder typeis highlighted in bold and the highest performing setup overall is underlined. All reported values indicatePearson/Spearman correlation coefficients for the task.
Table 6: Performance of different architectures and sentence representations on supervised transfer tasks. Oneach task, the highest performing setup for each encoder type is highlighted in bold and the highest performingsetup overall is underlined. All reported values indicate test accuracy on the task, except for SICK-R where wereport the Pearson correlation with human-provided scores.
