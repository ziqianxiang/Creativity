Table 1: Classification performance on ModelNet40 and ModelNet10. All methods train a linearSVM with features derived in an unsupervised manner. Comparing to [1] Kazhdan et al. (2003),[2] Chen et al. (2003), [3] Girdhar et al. (2016a), [4] Sharma et al. (2016), [5] Wu et al. (2016).
Table 2: Evaluating 5 generators on train-split of chair dataset on epochs/models selected viaminimal JSD on the validation-split. We also compare against the volumetric approach of Wu et al.
Table 3: Evaluating 5 generators on test-split of chair dataset on epochs/models selected via minimalJSD on the validation-split. The reported scores are averages of 3 pseudo-random repetitions. GMM-32-F stands for a GMM with 32 Gaussian components with full covariances.
Table 4: MMD-CD measurements for l-WGANs stopped at the two-thousand epoch and trainedon the latent spaces of dedicated (left 5 columns) and multi-class EMD-AEs (right column). The“average” measurement is computed as the weighted average of the per-class values, using the numberof train resp. test examples for each class as weights.
Table 5: Training parameters of SVMs used in each dataset with each structural loss of the AE.
Table 6: Effect of loss-type for AE reconstructions. The EMD loss gives rise to reconstructions withsignificantly better JSD compared to Chamfer. MMD-measurements favor the AE that was trainedwith the same loss under which the MMD measurement is computed. (Tr: Train split, Te: Test split)F Applications of the Latent Space RepresentationFor shape editing applications, we use the embedding we learned with the AE-EMD trained acrossall 55 object classes, not separately per-category. This showcases its ability to encode features fordifferent shapes, and enables interesting applications involving different kinds of shapes.
Table 7: MMD and Coverage metrics evaluated on the output of voxel-based methods at resolutions323 and 643 and (oct-tree based) 1283, matched against the chair test set, using the same protocolas in Table 3 of the main paper. Our volumetric models use GMMs with full covariances and 32centers and 64 or 256-dimensional latent codes (for the 323, 643 and 1283 respectively). For themesh conversion we used the marching cubes algorithm ((Lewiner et al., 2003)) with an iso-surfacevalue of 0.5. The rightmost column shows the results with our point-cloud based GMM.
Table 8: Reconstruction quality statistics for our dense voxel-based AE and the one of Tatarchenkoet al. (2017) for the ShapeNetCars dataset. Both approaches use a 0.5 occupancy threshold and thetrain-test split of Tatarchenko et al. (2017). Reconstruction quality is measured by measuring theintersection-over-union between the input and synthesized voxel grids, namely the ratio between thevolume in the voxel grid that is 1 in both grids divided by the volume that is 1 in at least one grid.
Table 9: Quantitative results of a baseline sampling/memorizing model, for different sizes of setssampled from the training set and evaluated against the test split. The first three rows show results of amemorizing model, while the third row corresponds to our generative model. The first row shows theresults of memorizing the entire training chair dataset. The second and third rows show the averagesof three repetitions of the sub-sampling procedure with different random seeds.
Table 10: JSD-based comparison between Wu et al. (2016) and our generative models. Full GMM/32stands for a GM model trained on the latent space of our AE with the EMD structural loss. Note thatthe l-GAN here uses the same “vanilla” adversarial objective as Wu et al. (2016).
Table 11: EMD based MMD and Coverage comparison between Wu et al. (2016) and our generativemodel on the test split of each class. Full GMM/32 stands for a GM model trained on the latent spaceof our AE with the EMD structural loss. Note that Wu et al. used all models of each class for training.
Table 12: CD based MMD and Coverage comparison between Wu et al. (2016) and our generativemodel on the test split of each class. Full GMM/32 stands for a GM model trained on the latent spaceof our AE with the EMD structural loss. Note that Wu et al. used all models of each class for training.
Table 13: Evaluation of five generators on test-split of chair data on epochs/models that wereselected via minimal MMD-CD on the validation-split. The reported scores are averages of threepseudo-random repetitions. Compare this with Table 3. Note that the overall quality of the selectedmodels remains the same, irrespective of the metric used for the selection. GMM-40-F stands for aGMM with 40 Gaussian components with full covariances.
