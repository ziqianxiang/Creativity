Table 1: Complete comparison of the TCN to regularized recurrent architectures in various tasks.
Table 2: TCN parameter settings for experiments in Section. 4TCN Settings							Dataset/Task	Subtask	k	n	Hidden	Dropout	Grad Clip	Note	T = 200	^6^	7	27			The Adding Problem	T = 400	7	7	27	0.0	N/A		T = 600	8	8	24			Seq. MNIST	-	~1~ 6	8	25 20	0.0	N/A	Permuted MNIST	-	7 6	8 8	25 20	0.0	N/A		T = 500	~6~	~9~	10-			Copy Memory Task	T = 1000	8	8	10	0.05	1.0	RMSprop 5e-4	T = 2000	8	9	10			Music JSB Chorales	-	亍	F	150	05	04	Music Nottingham	-	^6^	4	150	0.2	0.4		PTB	亍	ɪ	600			Embed. size 600Word-level LM	Wiki-103	3	5	1000	0.4	0.3	Embed. size 400	LAMBADA	4	5	500			Embed. size 500Char-level LM	PTB text8	3 2	3 5	450 520	0.1	0.15	Embed. size 100As previously mentioned in Section 4, the number of hidden units was chosen based on k and nsuch that the model size is approximately at the same level as the recurrent models. In the tableabove, a gradient clip of N/A means no gradient clipping was applied. However, in larger tasks,
Table 3: LSTM parameter settings for experiments in Section 4.
Table 4: State of the art (SoTA) results for tasks in Section 4.
Table 5: TCN with Gating Mechanism within Residual Block.
