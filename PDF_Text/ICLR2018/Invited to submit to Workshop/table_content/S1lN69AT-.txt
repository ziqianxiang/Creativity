Table 1: Model size and accuracy tradeoff forsparse-InceptionV3Sparsity	NNZ params	Top-1 acc.	Top-5 acc.
Table 2: MobileNet sparse vs dense resultsWidth	Sparsity	NNZ params	Top-1 acc.	Top-5 acc.
Table 3: PTB sparse vs dense resultsModel	Sparsity	NNZ params	Per- plexitySmall	0%	4.6M	115.30Medium	0%	19.8M	83.37	80%	4.0M	83.87	85%	3.0M	85.17	90%	2.0M	87.86	95%	1.0M	96.30	97.5%	0.5M	113.6Large	0%	66M	78.45	80%	13.2M	77.52	85%	9.9M	78.31	90%	6.6M	80.24	95%	3.3M	87.83	97.5%	1.7M	103.20based on a form of factorized convolutions called depthwise separable convolution. Depthwiseseparable convolutions consist of a depthwise convolution followed by a 1x1 convolution calleda pointwise convolution. This factorization significantly reduces the number of parameters in themodel by filtering and combining input channels in two separate steps instead of together as in thestandard convolution. The MobileNet architecture consists of one standard convolution layer acting
Table 4: NMT sparse vs dense results# units	Sparsity	NNZ params	EN-DE BLEU score	DE-EN BLEU score256	0%	34M	23.52	26.52512	0%	81M	26.05	28.88768	0%	140M	26.63	29.411024	0%	211M	26.77	29.47	80%	44M	26.86	29.50	85%	33M	26.52	29.24	90%	23M	26.19	28.81layer, and 3 standard LSTM layers. The decoder has an embedding layer which maps the targetvocabulary of 36,548 words into a k-dimensional space, 4 LSTM layers with attention, and finallya softmax layer. For the dense baseline model with number of units k = 1024, there are 37.4Mparameters in each of the encoder embedding, decoder embedding, and softmax layers and 98.6Mparameters in all of the LSTM layers for a total of 211M parameters. We apply pruning to all of theLSTM layers, embedding layers, and softmax layers, but we do not prune the attention parametersof which there are relatively few. The other dense models were obtained by varying the number ofunits k. We use the WMT16 German and English dataset with news-test2013 as the dev set andnews-test2015 as the test set. The BLEU score is reported as a measure of the translation quality.
Table 5: Storage overheads associated with bit-mask and CSR(C) sparse matrix representations forsparse-MobileNetsSparsity	NNZ params	Bit-mask (MB)	CSR(C) (MB)0%	4.21M	N/A	N/A50%	2.13M	0.52	1.0675%	1.09M	0.52	0.5490%	0.46M	0.52	0.2395%	0.25M	0.52	0.13Table 6: Comparison of the performance of small-dense and large-sparse models. Model size calcu-lations include overhead for sparse matrix storage and assumes 32-bit (4 bytes) per nonzero element.
Table 6: Comparison of the performance of small-dense and large-sparse models. Model size calcu-lations include overhead for sparse matrix storage and assumes 32-bit (4 bytes) per nonzero element.
