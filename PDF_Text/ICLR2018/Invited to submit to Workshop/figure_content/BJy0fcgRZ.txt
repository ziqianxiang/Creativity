Figure 1: Deep MCMCP. A current state Z and proposal z* (top middle) are fed to a pretrained deepimage generator/deCoder network (top left). The corresponding decoded images X and x* for the twostates are presented to human raters on a computer screen (leftmost arrow and bottom left). Humanraters then view the images in an experiment (bottom middle arrow) and act as part of an MCMCsampling loop, choosing between the two states/images in accordance with the Barker acceptancefunction (bottom right). The chosen image can then be sent to the inference network (rightmostarrow) and decoded in order to select the state for the next trial, however this step is unnecessarywhen we know exactly which states corresponds to which images. The inference network will,however, be needed later to characterize new images that we did not generate.
Figure 2: Left: Fisher Linear Discriminant projections of all four MCMCP chains for each of thefour face categories. The four sets of chains overlap to some degree, but are also well-separatedoverall. Means of individual chains are closer to other means from the same class than to thoseof other classes. Right: Individual MCMCP chain means (4 × 4 grid) and overall category means(second to last) visualized as images (overall CI means also shown for comparison in the finalcolumn). MCMCP means are much more differentiated than CI means, and better resemble thecategory in question.
Figure 3: Human two-alternative forced-choice tasks reveal a strong preference for MCMCP meansas representations of a category, when twice as many trials are used for CI.
Figure 4: Most interpretable mixture component means (modes) taken from the top 40do this, we employ a bidirectional generative adversarial network (BiGAN; Donahue et al., 2016)trained on the entire 1.2 million-image ILSVRC 1 2 dataset (64 × 64 center-cropped). BiGAN includesan inference network, which regularizes the rest of the model and produces unconditional samplescompetitive with the state-of-the-art. This also allows for the later possibility of comparing humandistributions with other networks as well as assessing machine classification performance with newimages based on the granular human biases captured.
Figure 5: Fisher Linear Discriminant projections of A. Classification images comparisons for eachcategory of group 1, B. samples for MCMCP chains for category group 1, C. Classification imagescomparisons for each category of group 2, and D. samples for MCMCP chains for category group 2.
