Figure 1: Motivating example. (a) A simple platformer game. (b) The same game modified byre-rendering the textures. Despite the two games being structurally the same, human players tooktwice as long to finish the second game as the first one. In comparison, the performance of an RLagent was approximately the same for the two games.
Figure 2: Various game manipulations. (a) Original version of the game. (b) Game with maskedobjects to ablate semantics prior. (c) Game with reversed associations as an alternate way to ablatesemantics prior. (d) Game with masked objects and distractor objects to ablate the concept of object.
Figure 3: Quantifying the influence of various object priors. The blue bar shows average timetaken by humans (in minutes), orange bar shows the average number of deaths, and yellow bar showsthe number of unique states visited by players to solve the various games. For visualization purposes,the number of deaths is divided by 2, and the number of states is divided by 1000 respectively.
Figure 4: Change in behavior upon ablation of various priors. (a) Graph comparing number ofparticipants that reached the key before the door in the original version, game without semantics,and game without object prior. (b) Amount of time taken by participants to reach the door once theyobtained the key. (c) Average number of steps taken by participants to reach various vertical levelsin original version, game without affordance, and game without similarity.
Figure 5: Masking all object priors drastically affects human performance. (a) Original game.
Figure 6: Taxonomy of object priors. The earlier an object prior is obtained during childhood, themore critical that object prior is in human problem solving in video games.
Figure 7: Quantifying physics and motor control priors. Graph shows performance of partici-pants in original version, game with gravity reversed, and game with key controls reversed. Numberof deaths is divided by 2 and number of states is divided by 1000.
Figure 8: Quantifying the performance of RL agent. (a) Game without semantic information. (b)Game with masked and distractor objects to ablate concept of objects. (c) Game without affordanceinformation. (d) Game without similarity information. (e) Performance of RL agent on variousgame manipulations (steps shown in order of million). Error bars indicate standard error of meanfor the 5 random seeds. The RL agent performs similarly on all games except for the one withoutvisual similarity.
Figure 9: Prior information constrains human exploration. (Left) A very simple game withhidden rewards (shown in dashed yellow). (Right) Average rewards accumulated by human playersvs a random agent.
