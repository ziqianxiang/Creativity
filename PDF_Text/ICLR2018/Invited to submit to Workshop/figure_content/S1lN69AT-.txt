Figure 1:	Sparsity function used for gradualpruningniques aim to reduce the number of bits required to represent each parameter from 32-bit floats to 8bits or fewer. Different quantization techniques such as fixed-point quantization (Vanhoucke et al.,2011) or vector quantization (Gong et al., 2014) achieve different compression ratios and accura-cies but also require different software or hardware to support inference at runtime. Pruning canbe combined with quantization to achieve maximal compression (Han et al., 2015a). In addition,an emerging area of research is low precision networks where the parameters and/or activations arequantized to 4 bits or fewer (Courbariaux et al., 2015; Lin et al., 2015; Hubara et al., 2016; Rastegariet al., 2016; Zhu et al., 2016). Besides quantization, other potentially complementary approaches toreducing model size include low-rank matrix factorization (Denil et al., 2013; Denton et al., 2014;Jaderberg et al., 2014; Lebedev et al., 2014) and group sparsity regularization to arrive at an optimallayer size (Alvarez & Salzmann, 2016).
Figure 2:	(a) The gradual sparsity function and exponentially decaying learning rate used for trainingsparse-InceptionV3 models. (b) Evolution of the modelâ€™s accuracy during the training processFigure 3: MobileNet sparse vs dense resultsFigure 4: PTB sparse Vs dense resultsfrom the loss in accuracy caused by forcing the weights to zero. At the same time, pruning withtoo high of a learning rate may mean pruning weights when the weights have not yet converged to agood solution, so the pruning schedule should be chosen closely with the learning rate schedule.
Figure 3: MobileNet sparse vs dense resultsFigure 4: PTB sparse Vs dense resultsfrom the loss in accuracy caused by forcing the weights to zero. At the same time, pruning withtoo high of a learning rate may mean pruning weights when the weights have not yet converged to agood solution, so the pruning schedule should be chosen closely with the learning rate schedule.
Figure 4: PTB sparse Vs dense resultsfrom the loss in accuracy caused by forcing the weights to zero. At the same time, pruning withtoo high of a learning rate may mean pruning weights when the weights have not yet converged to agood solution, so the pruning schedule should be chosen closely with the learning rate schedule.
Figure 5: Comparison of sparse vs dense NMT models for English to German (EN-DE) and Germanto English (DE-EN) translationtion is the average negative log probability of the target words, and the perplexity is the exponentialof the loss function. The language model is composed of an embedding layer, 2 LSTM layers, anda softmax layer. The vocabulary size is 10,000, and the LSTM hidden layer size is 200 for the smallmodel, 650 for the medium model, and 1,500 for the large model. In the case of the large model,there are 15M parameters in the embedding layer, 18M parameters in each of the two LSTM layers,and 15M parameters in the softmax layer for a total of 66M parameters. Different hyperparametersare used to train the different-sized models. When pruning a model of a certain size, we use the samehyperparameters that were used for training the dense model of that size. We compare the perfor-mance of the dense models with sparse models pruned from medium and large to 80%, 85%, 90%,95%, and 97.5% sparsity in Figure 4 and Table 3. In this case, we see that sparse models are able tooutperform dense models which have significantly more parameters (note the log scale for the num-ber of parameters). The 90% sparse large model (which has 6.6 million parameters and a perplexityof 80.24) is able to outperform the dense medium model (which has 19.8 million parameters and aperplexity of 83.37), a model which has 3 times more parameters. Compared with MobileNet, prun-ing PTB model likely gives better results because the PTB model is larger with significantly moreparameters. Our results show that pruning works very well not only on the dense LSTM weights anddense softmax layer but also the dense embedding matrix. This suggests that during the optimizationprocedure the neural network can find a good sparse embedding for the words in the vocabulary that
