Figure 1: A simple causal convolution with filter size 3.
Figure 2: A dilated causal convolution with dilation factors d = 1, 2,4 and filter size k = 3. Thereceptive field is able to cover all values from the input sequence.
Figure 3: A visualization of the TCN residual block(ReLU) (Nair & Hinton, 2010). For normalization, we applied Weight Normalization (Salimans &Kingma, 2016) to the filters in the dilated convolution (where we note that the filters are essentiallyvectors of size k × 1). In addition, a 2-D dropout (Srivastava et al., 2014) layer was added aftereach dilated convolution for regularization: at each training step, a whole channel (in the widthdimension) is zeroed out.
Figure 4: Results of TCN vs. recurrent architectures on the adding problemA summary comparison of TCNs to the standard RNN architectures (LSTM, GRU, and vanillaRNN) is shown in Table 1. We will highlight many of these results below, and want to emphasizethat for several tasks the baseline RNN architectures are still far from the state of the art (see Table4), but in total the results make a strong case that the TCN architecture, as a generic sequence mod-eling framework, is often superior to generic RNN approaches. We now consider several of theseexperiments in detail, generally distinguishing between the “recurrent benchmark” tasks designedto show the limitations of networks for sequence modeling (adding problem, sequential & permutedMNIST, copy memory), and the “applied” tasks (polyphonic music and language modeling).
Figure 5: Results of TCN vs. recurrent architectures on the Sequential MNIST and P-MNIST(a) T = 500.
Figure 6: Result of TCN vs. recurrent architectures on the Copy Memory Task, for different T(c) T = 2000.
Figure 7: Accuracy on the copy memory taskfor varying sequence length T .
Figure 8: Controlled experiments that examine different components of the TCN modelIn this section we briefly study, via controlled experiments, the effect of filter size and residualblock on the TCN’s ability to model different sequential tasks. Figure 8 shows the results of thisablative analysis. We kept the model size and the depth of the networks exactly the same within eachexperiment so that dilation factor is controlled. We conducted the experiment on three very differenttasks: the copy memory task, permuted MNIST (P-MNIST), as well as word-level PTB languagemodeling.
