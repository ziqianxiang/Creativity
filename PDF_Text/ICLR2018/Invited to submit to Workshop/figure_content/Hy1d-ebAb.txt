Figure 1: Depiction of the steps taken during the generation process.
Figure 2: Training curves for the graph model and LSTM model on three sets.
Figure 3: Degree histogram for samplesgenerated by models trained on Barabasi-Albert Graphs. The histogram labeled“Ground Truth” shows the data distribu-LSTM baseline can be partly attributed to the ability to tion estimated from 10,000 examples.
Figure 5: Visualization of the molecule generation processes for graph model trained with fixed andrandom ordering. Solid lines represent single bonds, and dashed lines represent double bounds.
Figure 6: An example graph and two corresponding decision sequences.
Figure 7: Distribution of chemical properties for samples from different models and the training set.
Figure 8: Changing the faddnode and faddedge biases can affect the generated samples accordingly,therefore achieving a level of fine-grained control of sample generation process. nb<bias> andeb<bias> shows the bias values added to the logits.
Figure 9:	Step-by-step generation process visualization for a graph model trained with canonicalordering.
Figure 10:	Step-by-step generation process visualization for a graph model trained with permutedrandom ordering.
Figure 11: Histogram of negative log-likelihood log p(G, π) under different orderings π for one smallmolecule under a model trained with canonical ordering.
