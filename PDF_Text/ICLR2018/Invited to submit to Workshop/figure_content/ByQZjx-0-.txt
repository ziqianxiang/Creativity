Figure 1: The graph represents theentire search space while the red ar-rows define a model in the searchspace, which is decided by a con-troller. Here we assume that node1 is the input to the model whereasnodes 3, 5, and 6 are the outputs ofthe model.
Figure 2: An example of a recurrent cell in our search space with 4 computational nodes. Left: Thecomputational DAG that corresponds to the recurrent cell. The red edges represent the activatedpath. Middle: The recurrent cell. Right: The outputs of the controller RNN that result in the cellin the middle and the DAG on the left. Note that nodes 3 and 4 are never sampled by the RNN,so their results are averaged and are treated as the cell’s output.
Figure 3: Our parameter sharing scheme between convolutional models. Top: The network of Nlayers, where each layer has 6 channels as described. Bottom: A block of the controller network,which consists of 6 binary masks, followed by the steps that sample skip connections.
Figure 4: The organization of the convolution cell and reduction cell to form a network.
Figure 5: An example run of the controller for our convolutional cell search space. Left: thecontroller’s outputs. Note that in our search space for convolutional cells, node 1 and node 2respectively correspond to the output of the two previous layers, so the controller does not have tomake any decisions for them. Right: the convolutional cell according to the controller’s sample.
Figure 6: The best RNN cell discovered by ENASon the Penn Treebank dataset.
Figure 7: Top left: An architecture found by ENAS for CIFAR-10 for the search space over channels.
