Figure 1: Key metrics for architectures in their randomly initialized state evaluated on Gaussiannoise. The x axis in the left graph shows depth in terms of the number of linear layers counted fromthe input. Note: The curve for ReLU-layer is shadowed by tanh in the figure A and by ReLU infigure C.
Figure 3: Key metrics for ResNet architectures at various depths. Left and right graph only showvalues between skeip connections. The x axis shows depth in terms of the number of linear layerscounted from the input. Note: The curve for layer-tanh and batch-tanh is shadowed by SeLU in theright graph.
Figure 5: Key metrics for ResNet architectures trained on CIFAR10. The top left graph shows theestimated optimal relative update size in each layer according to the algorithm described in sectionI.3. Remaining graphs show results obtained from training with either those step sizes or a singlestep size, whichever achieved a lower error (see table 2). The top two rows are equivalent to graphsin figure 2. The bottom row shows pre-activation standard deviation and pre-activation sign diversity(see section I.2 for definition) of the highest nonlinearity layer as training progresses.
Figure 6: Key metrics for ReLU-based architectures with looks-linear initialization trained onCIFAR10. The top left graph shows the estimated optimal relative update size in each layer ac-cording to the algorithm described in section I.3. Remaining graphs show results obtained fromtraining with either those step sizes or a single step size, whichever achieved a lower error (see table2). The top two rows are equivalent to graphs in figure 2. The bottom row shows pre-activationstandard deviation and pre-activation sign diversity (see section I.2 for definition) of the highestnonlinearity layer as training progresses.
