Figure 1: Data flow within larger system contain-ing an IDK classifier (model). When the modelpredicts, the system outputs the model’s predic-tion; when the model says IDK, the system out-puts the decision-maker’s (DM’s) prediction. Re-jection learning considers the “IDK Model” to bethe system output.
Figure 2: Binary classification (one threshold) vs.
Figure 3: Comparing the performance of punting IDK and binary models, with and without a fair-ness regularizer. The figure illustrates the trade-off between accuracy (x-axis) and fairness (y-axis).
Figure 4: Comparison of DM-aware and -unaware learning (i.e., Section 5 vs. Section 6). Most ofthe results are the same as in Figure 3; the new results here show the performance of a DM-awaremodel (defer-fair), depicted by the green triangles. Of particular note is the improvement of thismodel relative to the punting model (the blue circles).
Figure 5: Results obtained with a highly biased DM (trained with α = -0.1). Note that DM-awarestill improves relative to DM-unaware learning in this case.
Figure 6: Comparison of IDK predictions be-tween deferring and punting model on COMPASdataset. Total IDKs are normalized to 1. A = 1 isthe protected group (black people). Y is the addi-tional information given to the DM (violent recidi-vism) - Y = 1 means violent recidivism occurred.
Figure 7:	Comparing model performance between expected loss training with oracle as DM to IDKtraining unaware of DM. At test time, same DM is used.
Figure 8:	Relationship of DI to α, the coefficient on the DI regularizer, 5 runs for each value of α(a) COMPAS dataset (b) Health datasetFigure 9:	Relationship of error rate to α, the coefficient on the DI regularizer, 5 runs for each valueof αC Dataset DetailsWe show results on two datasets. The first is the COMPAS recidivism dataset, made available byProPublica (Kirchner et al., 2016) 1. This dataset concerns recidivism: whether or not a criminaldefendant will commit a crime while on bail. The goal is to predict whether or not the person willrecidivate, and the sensitive variable is race (split into black and non-black). We used informationabout counts of prior charges, charge degree, sex, age, and charge type (e.g., robbery, drug posses-sion). We provide one extra bit of information to our DM - whether or not the defendant violentlyrecidivated. This clearly delineates between two groups in the data - one where the DM knows thecorrect answer (those who violently recidivated) and one where the DM has no extra information(those who did not recidivate, and those who recidivated non-violently). This simulates a real-worldscenario where a DM, unbeknownst to the model, may have extra information on a subset of thedata. The simulated DM had a 24% error rate, better than the baseline model’s 29% error rate. Wesplit the dataset into 7718 training examples and 3309 test examples.
Figure 9:	Relationship of error rate to α, the coefficient on the DI regularizer, 5 runs for each valueof αC Dataset DetailsWe show results on two datasets. The first is the COMPAS recidivism dataset, made available byProPublica (Kirchner et al., 2016) 1. This dataset concerns recidivism: whether or not a criminaldefendant will commit a crime while on bail. The goal is to predict whether or not the person willrecidivate, and the sensitive variable is race (split into black and non-black). We used informationabout counts of prior charges, charge degree, sex, age, and charge type (e.g., robbery, drug posses-sion). We provide one extra bit of information to our DM - whether or not the defendant violentlyrecidivated. This clearly delineates between two groups in the data - one where the DM knows thecorrect answer (those who violently recidivated) and one where the DM has no extra information(those who did not recidivate, and those who recidivated non-violently). This simulates a real-worldscenario where a DM, unbeknownst to the model, may have extra information on a subset of thedata. The simulated DM had a 24% error rate, better than the baseline model’s 29% error rate. Wesplit the dataset into 7718 training examples and 3309 test examples.
Figure 10:	Comparison of DM-aware and -unaware learning. Split into 3 bins, low, medium, andhigh deferral rate for each dataset. Bins are different between datasets due to the differing distribu-tions of deferral rate observed during hyperparameter search.
