Figure 1: Ordered eigenvalues at a random initial point, and at the final point of GD. x-axis is therank of the eigenvalue, and y-axis the value of the eigenvalue.
Figure 2: Ordered plot of eigenvalues with a visible gap. x-axis is the rank of the eigenvalue, andy-axis the value of the eigenvalue.
Figure 3: Right edge of the spectrum of the Hessian of the loss for a fully connected network onMNIST with increasing dimensions. x-axis is the rank of the eigenvalue, and y-axis the value of theeigenvalue.
Figure 4: Outlier eigenvalues ofLB (bs = 512) vs SB (bs = 10). x-axis is the rank of the eigenvalue,and y-axis the value of the eigenvalue.
Figure 5: Negative eigenvalues at the bottomwhen increasing the number of hidden nodes(compare with Figure 3).
Figure 6: Negative eigenvalues at the bottom forLB-SB experiment on MNIST (compare with Fig-ure 4).
Figure 8: Large batch training immediately followed by small batch training on the full dataset ofCIFAR10 with a raw version of AlexNet. The accuracy increases by about 1%: Part I and Part IIlocate solutions with different generalization properties.
Figure 9: Loss and accuracy evaluation on the straight line that contains LB and SB solutions. Theaccuracy of the SB solution is ã€œ1% better than the LB solution, but there is no barrier between thetwo points.
Figure 10: Spectrum of the logistic regression loss with tanh unit: when data has a single Gaussianblob (left), when data has two Gaussian blobs (right). In the latter case, the spectrum has outliereigenvalues at 454.4, 819.5, and 92.7 for alpha = 1, 2, 0.02, respectively.
