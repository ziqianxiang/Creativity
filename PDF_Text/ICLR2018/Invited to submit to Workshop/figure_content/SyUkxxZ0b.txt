Figure 1: Visualizing a 2d slice of the input space where the subspace is spanned by: 2 randomlychosen directions (left), 1 random and 1 "adversarial direction" (center), and 2 orthogonal "adversarialdirections" (right). The data manifold is indicated in black and the max margin boundary in red. Thegreen area indicates points which are classified by the ReLU network as being on the inner sphere. Inthe last plot, the projection of the entire outer sphere is misclassified despite the fact that the errorrate of the model is less than 1 in 10 million.
Figure 2: Left: We consider the ReLU net trained on 50 million samples from two 500 dimensionalspheres of radius 1.0 and 1.3. We evaluate the accuracy of this network on the entire space using atheoretical decision boundary of 1.15. For each norm considered we plot the accuracy among 10000random samples. We see the accuracy rapidly increases as we move away from the margin. As wemove far enough away we no longer observe errors on the random samples. However, we are able toadversarially find errors as far as norms .6 and 2.4. Right: We trained the same ReLU net on 100samples from the data distribution when d = 2. By visualizing predictions on a dense subset of theentire space it appears that the model makes no errors on either circle.
Figure 3: Left: The final distribution of Î±i when the quadratic network is trained on 100k examples.
Figure 4: We consider a classification model which only sees a projection of the input, size d, onto ak dimensional subspace. We then plot what k/d needs to be in order for the model to obtain a certainerror rate. We find that as the input dimension grows, the ratio k/d needed quickly decreases.
Figure 5: We compare the average distance to nearest error with error rate for 3 networks trained onthe sphere dataset. All errors are reported for the inner sphere. The 3 networks are trained with 5different training set sizes, and their performance are measured at various points during training (thenetworks eventually become too accurate to appear on this graph, as the error rate will be too small toestimate statistically). Amazingly, we observe that the trade off between the amount of error and theaverage distance to nearest error closely tracks what is optimal, as would be observed if all the errorswere concentrated near a pole of the sphere.
