Figure 1: Learning curves on copy memory problem for T =1000 and T =2000.
Figure 2: Results on adding problem for T =100, T =200, T =400 and T =750. KRU consistentlyoutperforms the baselines on all the settings with fewer parameters.
Figure 3:	Validation accuracy on pixel by pixel MNIST and permuted MNIST class prediction as thelearning progresses.
Figure 4:	Wall clock training time on JSB Chorales and Piano-midi data-set.
Figure 5: KRU and KRU-LSTM performs better than the baseline models with far less parametersin the recurrent weight matrix on the challenging TIMIT data-set (Garofolo et al., 1993). Thissignificantly bring down the training and inference time of RNNs. Both LSTM and KRU-LSTMconverged within 5 epochs whereas RNN and KRU took 20 epochs. A similar result was obtainedby (Graves & Schmidhuber, 2005) using RNN and LSTM with 4 times less parameters respectivelythan our models. However in their work the LSTM took 20 epochs to converge and the RNN took 70epochs. We have also experimented with the same model size as that of (Graves & Schmidhuber,2005) and have obtained very similar results as in the table but at the expense of longer training times.
Figure 6: Analysis of soft unitary constraints on three data-sets. First, second and the third columnpresents JSB Chorales, Piano-midi and TIMIT data-sets respectively.
