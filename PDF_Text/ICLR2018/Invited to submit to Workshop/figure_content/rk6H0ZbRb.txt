Figure 1: Test set adversarial error as a function of for models trained on ImageNet due to FGSMand step l.l. attack in (a) and (b), respectively. adv. tr. denotes models that are adversariallytrained (Kurakin et al., 2016b). In (b), we also show two of the power law fits with straight lines.
Figure 2: Points represent adversarial error as a function of for models trained on MNIST. Straightlines are power-law fits. (a) FGSM attack on fully-connected 3-layer network (FC), a linear model,and a convolutional network (b) FGSM attack on FC trained on randomly shuffled labels (evaluatedon the training set). (c) Different attacks on FC.
Figure 3: Linear approximation for the response of the logits to adversarial perturbation. (a) Thedynamics of logits to an adversarial perturbation as a function of size for a single training example.
Figure 4: (a) Distribution of ∆1j of NASNet-A trained on ImageNet. (b) Distribution of ∆1j forlogits that are sampled independently from a uniform random distribution for 5 million samples with10 classes. ∆1j of other models are in Appendix Fig. 14IOgIO(M)Given the large density of small ∆12 values, we study an entropy penalty regularizer to make modelsmore robust. Our proposed loss function can be written as: loss = old loss - λ in=1 pi log pi .
Figure 5: Step l.l. attack adversarial accuracy as a function of for CNN and permutation invariantMNIST in (a) and (b), respectively. Regular training (purple), entropy regularization (red), adver-sarial training (green), and adversarial training with entropy regularization (blue) have been imple-mented. Adversarial training was done using the step l.l. method. In (c), we show the PGD attackadversarial accuracy on permutation invariant MNIST trained with and without step l.l. adversarialtraining.
Figure 6: Distribution of ∆1j up toj = 5 for permutation invariant MNIST trained with and withoutentropy regularization in red and purple, respectively.
Figure 7: (a) step l.l. adversarial accuracy of NAS Baseline trained with and without step l.l. adver-sarial examples in green and red, respectively. Best model from Experiment 1 is shown in blue. (b)PGD adversarial accuracy of NAS Baseline trained with and without PGD adversarial examples ingreen and red, respectively. Best model from Experiment 2 is shown in blue.
Figure 8: Performance of child models on the validation set. FGSM adversarial accuracy at = 8 vs.
Figure 9: Adversarial error for hundreds of models trained on MNIST, including fully-connectedand convolutional models. We only show models with clean accuracy larger than 80%.
Figure 10: Points represent the adversarial error due to FGSM as a function of for a 32-layerResNet trained on CIFAR10. Straight line is a power law fit with an exponent of 0.99.
Figure 11: Effect of adding an entropy regularization (λ = 3.0): step l.l. adversarial accuracyof wide ResNet on CIFAR10, with and without entropy regularization. Both models have a cleanaccuracy of 94%. They were both trained for 100 epochs with the same hyperparameters.
Figure 12: Fully connected network trained on MNIST with hinge loss. (a) Distributions of logitdifferences. (b) Red dots represent the adversarial error when FGSM attack uses the same hinge lossfrom training. Blue dots represent the adversarial error when FGSM attack uses a cross-entropy lossto create the adversarial examples.The line is a power-law fit with an exponent of 0.98Figure 13: Fully connected network trained on MNIST with L2-norm loss. (a) Distributions of logitdifferences. (b) Black dots represent the adversarial error due to FGSM. The line is a power-law fitwith an exponent of 1.02We can compute the change to the logits of the network due to this perturbation. We find,h(x0) = h(x + cVχL∕∣∣VχL∣∣2)	(13)hβ ≈ hβ +	hvl∣K xx	∂Xαβ *+O©	(14)=h +	C X	dhβ dhδ	dL	(15)=β +	∣∣VxL∣∣2 勺	∂xα ∂x;	∂hδ	(5)where we have plugged in for eq. (11). Expressing the above equation in terms of the Jacobian, itfollows that we can write the effect of the adversarial perturbation on the logits by,h0JTJδh + e W(16)as postulated.
Figure 13: Fully connected network trained on MNIST with L2-norm loss. (a) Distributions of logitdifferences. (b) Black dots represent the adversarial error due to FGSM. The line is a power-law fitwith an exponent of 1.02We can compute the change to the logits of the network due to this perturbation. We find,h(x0) = h(x + cVχL∕∣∣VχL∣∣2)	(13)hβ ≈ hβ +	hvl∣K xx	∂Xαβ *+O©	(14)=h +	C X	dhβ dhδ	dL	(15)=β +	∣∣VxL∣∣2 勺	∂xα ∂x;	∂hδ	(5)where we have plugged in for eq. (11). Expressing the above equation in terms of the Jacobian, itfollows that we can write the effect of the adversarial perturbation on the logits by,h0JTJδh + e W(16)as postulated.
Figure 14: (a) Distribution of ∆1N for other ImageNet models.
Figure 15: (a) FGSM adversarial accuracy of NAS Baseline trained with and without step l.l. adver-sarial examples in green and red, respectively. Best model from Experiment 1 is shown in blue. (b)FGSM adversarial accuracy of NAS Baseline trained with and without PGD adversarial examplesin green and red, respectively. Best model from Experiment 2 is shown in blue.
Figure 16: Left: Best architecture from Experiment 1. Right: Architecture of NAS Baseline. Wenote that the architecture from Experiment 1 is “longer” and “narrower” than previous architecturesfound by NAS for higher clean accuracy (Zoph & Le, 2016; Zoph et al., 2017).
