Figure 1: Bipolar versions of popular activation functions. From left: Bipolar ELU, Bipolar LeakyReLU, Bipolar ReLU.
Figure 2: Iterations of Xi+ι = f (Wxi) for different activation functions f With xι 〜N(0,1)and W 〜N(0, σ2), with σ set by the LSUV procedure such that f (Wxι) has approximately unitvariance. The graphs shoW the mean and variance of xi , averaged over 50 separate runs, Where adifferent x1 and W was sampled each run.
Figure 3: Training and test error on CIFAR-10, with and without bipolar units, in a 28-layer OrientedResponse Network without batch normalization.
Figure 4: The L2 norm of the gradient on the output of each layer as it propagates back through time.
Figure 5: Skip connections help learning in deeply stacked LSUV-initialized RNNs. Left: Withoutskip connections. Right: With skip connections connecting groups of four layers. Both plots showtraining loss for vanilla RNNs with 256 bipolar ELU units per layer, LSUV-initialized, trained oncharacter-level Penn Treebank.
Figure 6: Training loss with various activation functions, in 36-layer RNNs on Penn Treebank. TheBReLU-RNN has a lower training error than ReLU-RNN at all times where the curves are comparable(until the learning rate is cut on the ReLU-RNN). The BELU-RNN has the lowest training error ofall. With the ELU-RNN, training diverged quickly.
