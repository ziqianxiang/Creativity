Figure 1: Training curVes on continuous control benchmarks. Note that SAC Performs consistently across alltasks attaining the highest score comPared to both on-Policy and off-Policy methods in all benchmark tasks.
Figure 2: Performance of individual randomseeds on the HalfCheetah-v1 benchmark.
Figure 3: We tested the importance of entropy maximization, Boltzmann exploration, and the separate valuenetwork. (a) The label of each curve denotes which entropy terms (KL divergence and/or value function update)were included in the policy and value objectives. (b) The blue curve corresponds to the unaltered SAC withBoltzmann exploration, the green and red curves were obtained by using OU exploration noise with the givenparameters. (c) Contribution of a separate value function target network in the TD updates and as a policygradient baseline.
Figure 4: We modified the SAC implementationto make it identical to DDPG by applying a se-quence of incremental modifications.
Figure 5: Sensitivity of soft actor-critic to selected hyperparameters in the HalfCheetah-v1 task. SeeAppendix D for more detailed analysis.
