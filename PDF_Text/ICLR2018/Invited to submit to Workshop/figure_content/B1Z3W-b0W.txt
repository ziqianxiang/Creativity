Figure 1: Optimization surface of L (in nats) for a 2-D latent Gaussian model and a particularMNIST data example. Shown on the plot are the MAP (optimal estimate), the output of a standardinference model (VAE), and an expectation step trajectory of variational EM using stochastic gradi-ent ascent. The plot on the right shows the estimates of each inference scheme near the optimum.
Figure 2: Plate notation for a latent variable model (solid lines) with each inference scheme (dashedlines). θ refers to the generative model (decoder) parameters. VλL denotes the gradients of theELBO w.r.t. the distribution parameters, λ, of the approximate posterior, q(z|x). Iterative inferencemodels learn to perform approximate inference optimization by using these gradients and a set ofinference model (encoder) parameters, φ. See Figure 8 for a similar set of diagrams with unrolledcomputational graphs.
Figure 3: Optimization trajectory along L (in nats) of an iterative inference model with a 2D la-tent Gaussian model for a particular MNIST test example. The iterative inference model learns toadaptively adjust inference update step sizes to iteratively refine the approximate posterior estimate.
Figure 4: Reconstructions over inference iterations (left to right) for test examples from (a) MNIST,(b) Omniglot, (c) Street View House Numbers, and (d) CIFAR-10. Corresponding data examples areshown on the far right of each panel. Empirically, reconstructions become gradually sharper as theiterative inference models traverse the optimization surface, remaining stable after many iterations.
Figure 5: Test performance on MNIST of standard and iterative inference models for (a) additionalsamples and (b) additional inference iterations during training. Iterative inference models improvesignificantly with both quantities. Lines are for visualization and do not imply interpolation.
Figure 6: Comparison of inference optimizationperformance on MNIST test set between iterativeinference models and conventional optimizationtechniques. Iterative inference models empiri-cally converge faster.
Figure 7: Plate notation for a hierarchical latent variable model consisting of L levels of latentvariables. Variables at higher levels provide empirical priors on variables at lower levels. Withdata-dependent priors, the model has more flexibility in representing the intricacies of each dataexample.
Figure 8: Computational graphs for variational inference with (a) Variational EM, (b) StandardInference Models, and (c) Iterative Inference Models.
Figure 10: Comparison of inference optimizationperformance on MNIST test set between iterativeinference models and conventional optimizationtechniques. Performances is plotted in terms ofwall-clock time. Iterative inference models stilloutperform other techniques.
Figure 9: Additional reconstructions over inference iterations (left to right) for test examples from(a) MNIST, (b) Omniglot, (c) Street View House Numbers, and (d) CIFAR-10. Corresponding dataexamples are shown on the far right of each panel.
Figure 11: Gradient magnitudes and ELBO inference improvement for an iterative inference modeltrained on the RCV1 data set. (a) The gradient magnitudes for the approximate posterior meandecrease over inference iterations, signifying reaching near-optimal approximate posterior estimates.
