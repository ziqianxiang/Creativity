Figure 1:	Three example worlds drawn from the task distribution. The agent must first perform anexploratory stage of system identification before exploiting. For example, in the leftmost grid theagent must first identify that the orange squares give +1 reward and the blue squares replenish itslimited supply of energy. Further, it will need to identify that the gold squares block progress andthe black square can only be passed by picking up the pink key. The agent may also want to identifythat the brown squares will kill it and that it will slide over the purple squares. The other center andright worlds show these assignments will change and need to be re-identified every time a new taskis drawn.
Figure 2:	A comparison of local and global observations for the Krazy World environment. In thelocal mode, the agent only views a 3 Ã— 3 grid centered about itself. In global mode, the agent viewsthe entire environment.
Figure 3: One example maze environment rendered in human readable format. The agent attemptsto find a goal within the maze.
Figure 4: Meta learning curves on Krazy World. We see that -RL2 is at times closest to the op-timal solution of 0.6 (obtained via value iteration), but has the highest initial variance. E-MAMLconverges faster than MAML, although both algorithms do manage to converge. RL2 has relativelypoor performance and high variance. A random agent achieves a score of around 0.05 on this task.
Figure 5: Meta learning curves on mazes. The environment gives the agent a penalty for hittingthe wall. Interestingly, E-RL2 and RL2 take much longer to learn how to avoid the wall. However,they also deliver better final performance. E-RL2 learns faster than RL2 and offers superior finalperformance. Since MAML and E-MAML utilize MLPs instead of RNNs and have no memory, theirworse returns are not surprising. The optimal return for this task would be around 1.1 (obtained viavalue iteration) and a random agent would achieve a return of around -2.2 (mostly from hitting thewall). Note that this figure appears somewhat busy due to having all four algorithms on one graph.
Figure 6: Looking at the gap between initial performance and performance after one update forRL2 , E-RL2 , MAML, and E-MAML. All algorithms show some level of improvement after oneupdate, which suggests the meta-learning is working. On Krazy World, the gap between the initialpolicy performance and updated policy performance is fairly large for all algorithms. Interestingly,the initial performance of all algorithms on gridworld is similar, converging to values around 0.1.
Figure 7: Three heuristic metrics for exploration on Krazy World: Fraction of tile types visitedduring test time, number of times killed at a death square during test time, and number of goalsquares visited. We see that E-MAML is consistently the most diligent algorithm at checking everytile type during test time. Beyond that, things are fuzzy with RL2 and MAML both checking amajority of tile types at test time and E-RL2 being sporadic in this regard. As for the number oftimes the death tile type was visited, we see that most algorithms start by dying in all three testepisodes, and subsequently decrease to between one and two by the time they have converged. Asmentioned above, RL2 suffers from finding one goal and exploiting it, whereas the other algorithmsregularly explore to find more goals. For the most part, the exploratory algorithms consistentlydeliver the best performance on these metrics. Performance on the death heuristic and the tile typesfound heuristic seem to indicate the meta learners are learning how to do at least some systemidentification.
Figure 8: MAML on the right and E-MAML on the left. A look at the number of gradient steps attest time vs reward on the Krazy World environment. Both MAML and E-MAML do not typicallybenefit from seeing more than one gradient step at test time. Hence, we only perform one gradientstep at test time for the experiments in this paper.
