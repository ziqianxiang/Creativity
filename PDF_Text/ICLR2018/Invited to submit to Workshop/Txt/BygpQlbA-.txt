Under review as a conference paper at ICLR 2018
Towards Provable Control for Unknown Lin-
ear Dynamical Systems
Anonymous authors
Paper under double-blind review
Ab stract
We study the control of symmetric linear dynamical systems with unknown dy-
namics and a hidden state. Using a recent spectral filtering technique for concisely
representing such systems in a linear basis, we formulate optimal control in this
setting as a convex program. This approach eliminates the need to solve the non-
convex problem of explicit identification of the system and its latent state, and
allows for provable optimality guarantees for the control signal. We give the first
efficient algorithm for finding the optimal control signal with an arbitrary time
horizon T, with sample complexity (number of training rollouts) polynomial only
in log T and other relevant parameters.
1	Introduction
Recent empirical successes of reinforcement learning involve using deep nets to represent the un-
derlying MDP and policy. However, we lack any supporting theory, and are far from developing
algorithms with provable guarantees for such settings. We can make progress by addressing simpler
setups, such as those provided by control theory.
Control theory concerns the control of dynamical systems, a non-trivial task even if the system is
fully specified and provable guarantees are not required. This is true even in the simplest setting of
a linear dynamical system (LDS) with quadratic costs, since the resulting optimization problems are
high-dimensional and sensitive to noise.
The task of controlling an unknown linear system is significantly more complex, often giving rise
to non-convex and high-dimensional optimization problems. The standard practice in the literature
is to first solve the non-convex problem of system identification—that is, recover a model that ac-
curately describes the system—and then apply standard robust control methods. The non-convex
problem of system identification is the main reason that we have essentially no provable algorithms
for controlling even the simplest linear dynamical systems with unknown latent states.
In this paper, we take the first step towards a provably efficient control algorithm for linear dynamical
systems. Despite the highly non-convex and high-dimensional formulation of the problem, we can
efficiently find the optimal control signal in polynomial time with optimal sample complexity. Our
method is based on wave-filtering, a recent spectral representation technique for symmetric LDSs
(Hazan et al., 2017).
1.1	Our results
A dynamical system converts input signals {x1, . . . , xT} ∈ Rn into output signals {y1, . . . , yT} ∈
Rm, incurring a sequence of costs c1 , . . . , cT ∈ R. We are interested in controlling unknown dy-
namical systems with hidden states (which can be thought of as being partially “observed”via the
output signals). A vast body of work focuses on linear dynamical systems with quadratic costs, in
which the {yt } and {ct} are governed by the following dynamics:
ht+1 = Aht + Bxt , yt = Cht + Dxt + ηt , ct = yt>Qyt + xt>Rxt ,
where h1, . . . , hT ∈ Rd is a sequence of hidden states starting with a fixed h1. Matrices
A, B, C, D, Q, R of appropriate dimension describe the system and cost objective; the {ηt } are
Gaussian noise vectors. All of these matrices, as well as the parameters of the Gaussian, can be
1
Under review as a conference paper at ICLR 2018
unknown. The most fundamental control problem involves controlling the system for some time
horizon T: find a signal x1, . . . , xT that minimizes the sum of these quadratic output costs Pt ct.
Clearly, any algorithm for doing so must first learn the system parameters in some form, and this is
often the source of computational intractability (meaning algorithms that take time exponential in
the number of system parameters).
Previously known algorithms are of two types. The first type tries to solve the non-convex problem,
with algorithms that lack provable guarantees and may take exponential time in the worst case: e.g.,
expectation-maximization (EM) or gradient-based methods (back-propagation through time, like the
training of RNNs) which identify both the hidden states and system parameters.
Algorithms of the second type rely upon regression, often used in time-series analysis. Since T -step
dynamics of the system involve the first T powers of A, the algorithm represents these powers as
new variables and learns the T -step dynamics via regression (e.g., the so-called VARX(p, s) model)
assuming that the system is well-conditioned; see Appendix A. This has moderate computational
complexity but high sample complexity since the number of parameters in the regression scales with
T , the number of time steps.
Our new method obtains the best of both results: few parameters to train resulting in low sample
complexity, as well as polynomial computational complexity. Table 1 below compares the different
methods, where we denote by |D| = max(kBkF, kCkF, kDkF, kQkop, m, n) the size of the system,
and by T the time horizon for planning.
Method	Sample complexity	Time complexity	Assumptions
System Identification	poly(e-i, |D|)	exponential	none
VARX(0,s)	poly(e-i, |D|, S)	Poly(T,|D|)	S = ω (τ-k⅛)or S = T
Ours	poly(e-i, ∣D∣,log T)	Poly(T,|D|)一	A symmetric
Table 1: Comparison of control algorithms.
In Section 2 we state the precise algorithm. The informal result is as follows.
Theorem 1.1 (Controlling an unknown LDS; informal). Let D be a linear dynamical system with
a symmetric transition matrix A and with size |D|. Then, for every ε > 0, Algorithm 1 produces a
sequence ofcontrols (Xi,... XT), ∣∣Xi∣∣2 ≤ 1 with ∣∣X1M∣2 ≤ L, Such that
E	1T	t T ECOStD(XI:t) t=1	≤	min	E x1:T ∈ B2T	1 T	T CoSttD (X1:t) t=1	+ ε.	(1)
			kx1:T k2 ≤ L				
Assuming i.i.d. Gaussian noise η 〜N (0, Σ) ,the algorithm samples O (poly(∣D∣, L, Tr Σ, 1∕ε))
trajectories from D, and runs in time polynomial in the same parameters.
1.2 Related work
The field of optimal control for dynamical systems is extremely broad and brings together litera-
ture from machine learning, statistical time-series analysis, dynamical system tracking and Kalman
filtering, system identification, and optimal control. For an extensive survey of the field, see e.g.
(Todorov, 2006; Bertsekas, 2000).
Tracking a known system. A less ambitions goal than control is tracking of a dynamical system,
or prediction of the output given a known input. For the special case of LDS, the well-known
Kalman filter (Kalman, 1960) is an optimal recursive least-squares solution for maximum likelihood
estimation (MLE) under Gaussian perturbations to a linear dynamical system.
System identification. When the underlying dynamical system is unknown, there are essentially
no provably efficient methods for recovering it. For various techniques used in practice, see the
2
Under review as a conference paper at ICLR 2018
classic survey (Ljung, 1998). Roweis & Ghahramani (1999) suggest using the EM algorithm to
learn the parameters of an LDS, nowadays widespread, but it is well-known that optimality is not
guaranteed. The recent result of Hardt et al. (2016) gives a polynomial time algorithm for system
recovery, although it applies only to the single-input-single-output case and makes various statistical
assumptions on the inputs.
Model-free tracking. Our methods depend crucially on a new algorithm for LDS sequence pre-
diction, at the heart of which is a new convex relaxation for the tracking formulation (Hazan et al.,
2017). In particular, this method circumvent the obstacle of explicit system identification. We detail
our usage of this result in Definition 2.3.
We note an intriguing connection to the recently widespread use of deep neural networks to represent
an unknown MDP in reinforcement learning: the main algorithm queries the unknown dynamical
system with exploration signals, and uses its responses to build a compact representation (denoted
by M in Algorithm 1) which estimates the behavior of the system.
Time-series analysis. One of the most common approaches to modeling dynamical systems is
the autoregressive-moving average (ARMA) model and its derivatives in the time-series analysis
literature (Hamilton, 1994; Box et al., 1994; Brockwell & Davis, 2009). At the heart of this method
is the autoregressive form ofa time series, namely,
k
xt =	Aixt-i + εt.
i=1
Using online learning techniques, it is possible to completely identify an autoregressive model, even
in the presence of adversarial noise (Anava et al., 2013). This technique lies at the heart ofa folklore
regression method for optimal control, given in the second row of table 1.
Optimal control. The most relevant and fundamental primitive from control theory, as applied to
the control of linear dynamical systems, is the linear-quadratic-Gaussian (LQG) problem. In this
setting, the system dynamics are assumed to be known, and the task is to find a sequence of inputs
which minimize a given quadratic cost. A common solution, the LQG controller, is to combine
Kalman filtering with a linear-quadratic regulator, a controller selected by solving the Bellman
equation for the problem. Such an approach admits theoretical guarantees under varied assumptions
on the system; see, for example, Dean et al. (2017).
Our setting also involves a linear dynamical system with quadratic costs, and thus can be seen as
a special case of the LQG setup, in which the process noise is zero, and the transition matrix is
assumed to be symmetric. However, our results are not analogous: our task also includes learning
the system’s dynamics. As such, our main algorithm for control takes a very different approach
than that of the standard LQR: rather than solving a recursive system of equations, we provide a
formulation of control as a one-shot convex program.
2 Statement of main theorem
First, we state the formal definitions of the key objects of interest.
Definition 2.1 (Dynamical system). A dynamical system D is a mapping that takes a sequence
of input vectors x1, . . . , xT ∈ B2 = {x ∈ Rn : kxk2 ≤ 1} to a sequence of output vectors
y1 , . . . , yT ∈ Rm and costs c1 , . . . , cT ∈ R. Denote xs:t = [xs ; . . . ; xt] as the concatenation of
all input vectors from time s to t, and write
D (x1:t ) = yt , costD (x1:t ) = ct .
Definition 2.2 (Linear dynamical system). A linear dynamical system (LDS) is a dynamical system
whose outputs and costs are defined by
ht+1 = Aht + Bxt,
yt = Cht + Dxt + ηt	with ηt 〜N(0, ∑),
ct = yt>Qyt + xt>Rxt,
3
Under review as a conference paper at ICLR 2018
where h1, . . . , hT ∈ Rd is a sequence of hidden states starting with fixed h1, and A, B, C, D, Q, R
are matrices (or vectors) of appropriate dimension. We assume kAkop ≤ 1, i.e., all singular values
of A are at most one, and that Q < 0, R < 0.
Our algorithm and its guarantees depend on the construction of a family of orthonormal vectors in
RT, which are interpreted as convolution filters on the input time series. We define the wave-filtering
matrix below; for more details, see Section 3 of Hazan et al. (2017).
Definition 2.3 (Wave-filtering matrix). Fix any n, T, and 1 ≤ k ≤ T. Let φj be the eigenvec-
tor corresponding to the j-th largest eigenvalue of the Hankel matrix ZT ∈ RT ×T, with entries
Zij = (i+j)3—(i+j). The wave-filtering matrix Φ ∈ Rnk×nT is defined by k vertically stacked
block matrices {Φ(j) ∈ Rn×nT}, defined by horizontally stacked multiples of the identity matrix:
Φ(j) d=ef φj (1) In φj(2)In ... φj(T)In .
Then, letting t range from 1 to T, Φχ^t-τ then gives a dimension-wise convolution of the input
time series by the filters {φj } of length T. Theorem 3.3 uses a structural result from Hazan et al.
(2017), which guarantees the existence of a concise representation of D in the basis of these filters.
The main theorem we prove is the following.
Theorem 2.4 (Controlling an unknown LDS). Let D be a LDS with a symmetric transition matrix
A and with kBkF, kCkF, kDkF , kQkop ≤ ρ, and with Q < λI. For every ε > 0, Algorithm 1,
with a choice of k = Ω (log2 Tlog (nρkk)) and Ω
(Lyn竺Ta田),produces。SequenCe °f
controls (Xι,...Xτ) ∈ BT, such that with probability at least 1 一 δ,
1
E T ECoStD(XI：t)≤
t=1
min E
x1:T ∈ B2T
kx1:T k ≤ L
1T
T ECoStD(XI：t)
t=1
+ ε,
(2)
assuming that
min E
x1:T ∈ B2T
kx1:T k ≤ L
1
T EcostDdt)
t=1
≤ Tr(QΣ) + c
(3)
Further, the algorithm samples Poly (ɪ, log (1) , log T, log ρ, 1 ,L,n,k, Tr(Σ)) trajeCtorieSfom
the dynamical system, and runs in time polynomial in the same parameters.
We remark on some of the conditions. λ is bounded away from 0 when we suffer loss in all
directions of yt. In condition (3), Tr(QΣ) is inevitable loss due to background noise, so (3) is an
assumption on the system’s controllability.
We set up some notation for the algorithm. Let Φ ∈ Rnk×nT be the wave-filtering matrix from Defi-
nition 2.3. Let Xi = 0 for i ≤ 0, and let Xt = Xt:t-T+1. Letρ = max(kBkF, kCkF, kDkF, kQkF)
be an upper bound for the matrices involved in generating the outputs and costs.
3 Proof of main theorem
To prove Theorem 2.4, we invoke Lemma 3.1 and Lemma 3.2, proved in Subsection 3.1 and Sub-
section 3.2, respectively.
Lemma 3.1 (Learning dynamics). For every ε > 0 and symmetric LDS D, with probability ≥ 1 一 δ,
setting k = Ω(log2 T log(nρ∕ε)) and S = Ω (L nk Tr：：)ln( δ)), SteP 7 in Algorithm 1 computes
a matrix M ∈ Rm×nk such that for every sequence of input signals (xι,..., XT) ∈ BT satisfying
kX1:T k2 ≤ L and 1 ≤ t ≤ T, we have that
kyt - Eytk ≤ ε	(5)
where y = MΦXt + Zt and y = Dy (xi：t).	(6)
4
Under review as a conference paper at ICLR 2018
Algorithm 1: Control with an LDS oracle
Input : Oracle access to LDS D, cost matrices Q, R, filter parameter k, sample count S.
Output: Control inputs Xi：t.
1 Run D with the all-zeros input S times, recording responses z1(s:T) := D1:T (0) for each
1	≤ s ≤ S.
2	Average the zero-impulse responses: zi：T := S PS=I Z(ST.
3	for 1 ≤ j ≤ nk do
4	RUn D with input φj S times, recording responses yTj := DT (φj) for each 1 ≤ S ≤ S.
5	Average the exploration responses: 抻 := 1 PS=I yT,j.
6	end
7	Let MM be the matrix whose j-th column is MMj := yT(j) - zT .
8	Solve the following convex program to obtain controls (Xi,... XT):
T
Xi：T :=	arg min X [(MMΦXt + zt)>Q(MMΦXt + Zt) + XTRxt] .	(4)
x1:T ∈ B2T	t=i
kx1:T k2 ≤ L
Lemma 3.2 (Robustness of control to uncertainty in dynamics). Let
T
Xi:T =	arg min X [Dt(xιt)>QDt(xi：t) + XTRxt],
x1:T ∈ B2T t=i
kx1:T k ≤ L
where M ∈ Rm×nk is such that for every Sequence of input signals (xi, ..., XT) ∈ BT with
IM：T∣∣2 ≤ L and t ≤ T, Equation (5) holds with yjt = Dt(Xi：t)∙ Assume (3). Then
1
E T ECoStD(XI：t)≤
t=i
min E
x1:T ∈ B2T
kx1:T k ≤ L
1T
T ECoStD(XLt)
t=i
(7)
Moreover, the minimization problem posed above is convex (for Q, R < 0), and can be solved to
within εopt acccuracy in Poly(T, ρ, 1∕ε0pt) time using the ellipsoid method.
Proof of Theorem 2.4. Use Lemma 3.1 with ε J O (,cρ) . Note that Xi：T = φj is a valid input
to the LDS because ∣∣φj∣∣2 = 1. Now use Lemma 3.2 on the conclusion of Lemma 3.1.	□
3.1	Learning the dynamics
To prove Lemma 3.1, we will use the following structural result from Hazan et al. (2017) restated to
match the setting in consideration.
Theorem 3.3. Let D be a symmetric LDS with the stated conditions from Section 2, with fixed hi .
Let ε > 0, and let k = Ω(log2 T log(nρ∕ε)). Let zi：t be the output if the input signal is 0. Let M0
be the matrix such that for all input-output pairs (Xi：T, yi：T), E(yT - ZT) = M 0XT. Then for all
Xi：T ∈ B2>, the matrix M = M 0Φ> satisfies
∣E(yt - Zt) -M ΦXt∣ ≤ε,	∀t ≤T.	(8)
Proof. This follows from Theorem 3b in Hazan et al. (2017) after noting four things.
1.	E(yt - Zt ) are the outputs when the system is started at hi = 0 with inputs Xi：t and no
noise. A linear relationship yt - Zt = M 0Xt holds by unfolding the recurrence relation.
5
Under review as a conference paper at ICLR 2018
2.	Examining the construction of M in the proof of Theorem 3b, the M is exactly the projec-
tion of M0 onto the subspace defined by Φ. (Note we are using the fact that the rows of Φ
are orthogonal.)
3.	Theorem 3b is stated in terms of the quantity Ly, which is bounded by ρ2 by Lemma F.5
in Hazan et al. (2017).
4.	We can modify the system so that D = O by replacing (A,B,C,D) with
((O o),(B ), (C D), O). This replaces P by O(max(ρ, √n)). Theorem 3b originally
had dependence of yt on both ΦXt and xt , but if D = O then the dependence is only on
ΦXt.	□
Proof of Lemma 3.1. Take S = Ω (
L2 nk Tr(Σ)ln( T )
. Letting M 0 be the matrix such that yt -
zt = M0Xt, and M = M0Φ> as in Theorem 3.3, we have that
E[yτ(j) - zτ] = M0φj = Mj .
(9)
ε^
Let 1 ≤ t ≤ T. We bound the error under controls x1:t ∈ Bt2, kx1:tk2 ≤ L using the triangle
inequality. Letting y1:t be the output under x1:t,
∣E[yt] - zt- MΦXt( ≤ kE[yt - zt] - MΦXtk2 + kE[zt] - ztk2 + IMΦXt - MΦXt(.
(10)
By Theorem 3.3, for k = Ω(log2 Tlog(nρ∕ε)), choosing constants appropriately, the first term is
≤ ε
≤ 4 .
To bound the second term in (10), we show zt concentrates around Ezt. We have
zt - Ezt 〜n,
n 〜N QB).
(11)
By concentration of sums of χ2 random variables (see Hsu et al. (2012), for example),
Pn〜N(o,s∑)(Hnk2 ≥ εO) ≤ δ0
as long as S ≥ -552 Tr(∑) log g).
(12)
Take δ0 = 2T and -0 = 4l√j and note S was chosen to satisfy (12). Use the union bound to get
that
P (∃t ∈ [1，T]，kzt- Eztk2 ≥ 4L⅛)≤ 2.
(13)
To bound the third term in (10), We first show that MM concentrates around M. We have
(14)
∀i ≤ nk
so
S
Mi = S X[yTs,i)] - zτ
S s=1
Mi - Mi =	1S S X[yTM)] - zτ -(EyT," - Ezτ) s=1		(15)
=	n+ (Ezτ - zτ),	n 〜N (o,S∑)	(16)
M - M =	n0 + (Ezτ 一 zτ)ι>,	η0 〜N (θ,SΣ出nk).	(17)
By χ2 concentration,	Pn〜N(o,S∑㊉nk)(旧1旧 ≥ 4L)	δ ≤ 2.	(18)
6
Under review as a conference paper at ICLR 2018
We also have Il(EzT - ZT) 1>∣∣f ≤ √nk IlEzT - ZT|卜.
With ≥ 1 - δ probability, We avoid both the bad events in (13) and (18) so Ilzt - Eztil2 ≤《工％^
for all 1 ≤ t ≤ T and
梃-MlF ≤ 竟 + √nk4L√nk = 2L	(19)
and for all Ix1:t I ≤ L, the third term of (10) is bounded by (note IΦIop = 1 because it has
orthogonal roWs)
IIM ΦXt	-MMΦXt∣∣ ≤ ∣∣m - M∣∣F∣ΦXt k2	(20)
	≤ ∣∣m - M∣∣F∣χt k2	(21)
	εε ≤ 2ll =2.	(22)
Thus by (10), ∣∣E[yt] - zt - MMΦXt∣	≤ ε With probability ≥ 1 - δ.	□
3.2	Robustness of Control to Uncertainty in Dynamics
To prove Lemma 3.2, We need the folloWing helpful lemma.
Lemma 3.4. For a symmetric LDS D with ρ = max(IBIF , ICIF , ID IF ,IP Iop) and A 4 I,
Q < λI, and an approximation Dt whose predictions yt satisfy the conclusion of Lemma 3.1, we
have that for every sequence of controls (x1, . . . , xT) ∈ B2T, and for every 1 ≤ t ≤ T,
T	__________________________
X [costD(xi：t) + Tr(Q∑) - E [costD(xi：t)]] ≤ 2ρε Jtmin (|攸上T∣∣2 , ||Eyi：T∣∣2) + Tρε2
t=1
(23)
where CostD5")= Dt(x±t)>QDt(xi：t) + x>Rxt.
Proof of Lemma 3.4. Let yt = Dy (xi：t) and yt = Dy (xi：t) = E[yt] + ηt. Using the assumption
that E[ηtηt>] = Σ,
I CostD (xi：t) + Tr(QΣ) - EIcostD (xi：t)]]	(24)
=y>Qyt + Tr(QΣ) - E [(E[yt] + ηt)>Q(E[yt] + ηt)] ∣	(25)
=y>Qyt- E[yt]>QE[yt]∣	(26)
≤2 llyt- Eyt kkQkop min(kytk JEytk) + IIQkop llyt- Eytk2 IT	I	(27)
X [costD(xi：t) + Tr(Q∑) - E [costD(xi：t)]] I t=1	I	(28)
≤ 2 IQIop εjτmin (X 帧『，X ∣EyT『)+ TkQkoP ≡2	(29)
using Cauchy-SchWarz.	□
Proof of Lemma 3.2. Define
T
XiT = arg min E X Dtc(x1:t)	(30)
x1:T ∈ B2T	t=1
kx1:T k ≤ L
T
Xi：T = arg min ED C(xi：t)	(31)
x1:T ∈ B2T t=1
kx1:T k ≤ L
7
Under review as a conference paper at ICLR 2018
Let V = PT=1 [E[costD(x；：J] - Tr(QΣ)], for inputs x£. By assumption (3), V ≤ cT. We have
T
V≥X(Eyt)TQ(Eyt) ≥ λ kEy1:T k2
t=1
=⇒ kEyi：TIl ≤ y y∙
λ
By Lemma 3.4 for inputs x；：t,
T	∕et T	/	I-
X CoStD(XI：J ≤ V + 2ρεγ -- + Tρε2 ≤ V + Tρε (2∖^-
t=1	λ	λ
(32)
(33)
(34)
Letting yi：T be the outputs under D under the control xi：T, note that similar to (32),
kyi：Tk	≤	y1 ^E	X costD(XLt)	≤	/J^E	X	costD(X;：J	(35)
F	ʌ	. ∙	1 ∕'	1 ʌ T	C / C ♦	. ʌ
because xi：t is optimal for D. By Lemma 3.4 for inputs xi：t,
T
X ICoStD(Xi：t) + Tr(QΣ) - E[costD(Xi：t)]] ≤ 2ρε√Tkyi：Tk + Tρε2.	(36)
t=i
Now by (34), (35), and (36),
E[costD(Xi：t)] - CoStD (x；：t) ≤ (costD Xi：t + Tr(Q∑) - CoStD (Xi：t))	(37)
+ (CoStD (xi：t) - CoStD (x1：t)) + (CoStD (xi：t) - Tr(Qς) - CoStD (x；：t))
using ε ≤ √λ ≤ 牟∙	口
4	Conclusion
We have presented an algorithm for finding the optimal control inputs for an unknown symmet-
ric linear dynamical system, which requires querying the system only a polylogarithmic number of
times in the number of such inputs T, while running in polynomial time. Deviating significantly
from previous approaches, we circumvent the non-convex optimization problem of system identifi-
cation by a new learned representation of the system. We see this as a first step towards provable,
efficient methods for the traditionally non-convex realm of control and reinforcement learning.
References
Oren Anava, Elad Hazan, Shie Mannor, and Ohad Shamir. Online learning for time series prediction.
In COLT 2013 - The 26th Annual Conference on Learning Theory, June 12-14, 2013, Princeton
University, NJ, USA, pp. 172-184, 2013.
Dimitri P. Bertsekas. Dynamic Programming and Optimal Control. Athena Scientific, 2nd edition,
2000. ISBN 1886529094.
G. Box, G. Jenkins, and G. Reinsel. Time Series Analysis: Forecasting and Control. Prentice-Hall,
3 edition, 1994.
8
Under review as a conference paper at ICLR 2018
P. Brockwell and R. Davis. Time Series: Theory and Methods. Springer, 2 edition, 2009.
Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu. On the sample com-
plexity of the linear quadratic regulator. arXiv preprint arXiv:1710.01688, 2017.
J. Hamilton. Time Series Analysis. Princeton Univ. Press, 1994.
Moritz Hardt, Tengyu Ma, and Benjamin Recht. Gradient descent learns linear dynamical systems.
arXiv preprint arXiv:1609.05191, 2016.
Elad Hazan, Karan Singh, and Cyril Zhang. Learning linear dynamical systems via spectral filtering.
In Advances in Neural Information Processing Systems, pp. 1-2, 2017.
Daniel Hsu, Sham Kakade, and Tong Zhang. A tail inequality for quadratic forms of subgaussian
random vectors. Electronic Communications in Probability, 17, 2012.
Rudolph Emil Kalman. A new approach to linear filtering and prediction problems. Journal of Basic
Engineering, 82.1:35-45, 1960.
Lennart Ljung. System identification: Theory for the User. Prentice Hall, Upper Saddle Riiver, NJ,
2 edition, 1998.
Sam Roweis and Zoubin Ghahramani. A unifying review of linear gaussian models. Neural compu-
tation, 11(2):305-345, 1999.
Emanuel Todorov. Optimal control theory. Bayesian brain: probabilistic approaches to neural
coding, pp. 269-298, 2006.
9
Under review as a conference paper at ICLR 2018
A Autoregressive Models as a Relaxation of LDS
In this section, we verify the statement made in Section 1 on the time and sample complexity of
approximating a linear dynamical system with an autoregressive model. Although this is well-
known (see, for example, Section 6 of Hardt et al. (2016)), we present a self-contained presentation
for convenience and to unify notation.
The vector autoregressive model with exogenous variables, or VARX(p, s), is a touchstone in time-
series analysis. Given a time series of inputs (sometimes known as biases) {ξt}, it generates the
time series of responses {yt } by the following recurrence:
p	s-1
yt =	Aiyt-i +	Bixt-i + ξt.
i=1	i=0
Here, p and s are memory parameters, the {Ai } and {Bi } are matrices of appropriate dimension,
and the {ξt } are noise vectors.
In the special case of p = 0, the problem can be solved efficiently with linear regression: in this
case, yt is a linear function of the concatenated inputs [xt; xt-1 ; . . . , xt-s+1].
A VARX(0, s) model is specified by M = [M(0), . . . , M (s-1)] ∈ Rm×ns and predicts yt =
Mxt:t-s+1.
We quantify the relationship between VARX(0, s) and linear dynamical systems, with a statement
analogous to Theorem 3.1:
Theorem A.1. Let D be an LDS with size L, fixed h1, and noise ηt = 0, producing outputs
{y1, . . . , yT} from inputs {x1, . . . , xT}. Suppose that the transition matrix of D has operator norm
at most a < L Then, for each ε > 0, there is a VARX(0, S) model with S = O(ι-1α log(L∕ε)),
specified by a matrix M, such that
kyt - M xt:t-s+1 k ≤ ε.
Proof. By the modification of D given in the proof of Theorem 3.3, we may assume without loss of
generality that D = O. Also, as in the discussion of Theorem 3.1, it can be assumed that the initial
hidden state h1 is zero.
Then, we construct the block of M corresponding to lag i as
M(i) = CAiB.
This is well-defined for all 1 ≤ i ≤ t. Note that when S ≥ t, the autoregressive model completely
specifies the system D, which is determined by its (infinite-horizon) impulse response function.
Furthermore, by definition of α, we have
kM(i)k2F ≤ αiL2.
Noting that
t-1
yt - M xt:t-s+1 = XM(i)xt-i,
i=s
we conclude that
t-1	αsL2
kyt — Mxt：t-s+ik ≤ EaiL2∣∣xιk2 ≤ I—,
i=s	1 - α
implying the claim by the stated choice of s.	□
VARX(0, S) only serves as a good approximation of an LDS whose hidden state decays on a time
scale shorter than S; when the system is ill-conditioned (α is close to 1), this can get arbitrarily large,
requiring the full time horizon S = T .
On the other hand, it is clear that both the time and sample complexity of learning a VARX(0, S)
model grows linearly in S. This verifies the claim in the introduction.
10