Workshop track - ICLR 2018
LSH-SAMPLING BREAKS THE COMPUTA-
TIONAL CHICKEN-AND-EGG LOOP IN ADAP-
TIVE STOCHASTIC GRADIENT ESTIMATION
Beidi Chen, Yingchen Xu & Anshumali Shrivastava
Department of Computer Science
Rice University
Houston, TX 77005, USA
{beidi.chen,yx26,anshumali}@rice.edu
Abstract
Stochastic Gradient Descent or SGD is the most popular optimization algorithm
for large-scale problems. SGD estimates the gradient by uniform sampling with
sample size one. There have been several other works that suggest faster epoch
wise convergence by using weighted non-uniform sampling for better gradient es-
timates. Unfortunately, the per-iteration cost of maintaining this adaptive distribu-
tion for gradient estimation is more than calculating the full gradient. As a result,
the false impression of faster convergence in iterations leads to slower conver-
gence in time, which we call a chicken-and-egg loop. In this paper, we break this
barrier by providing the first demonstration of a sampling scheme, which leads to
superior gradient estimation, while keeping the sampling cost per iteration similar
to that of the uniform sampling. Such an algorithm is possible due to the sam-
pling view of Locality Sensitive Hashing (LSH), which came to light recently. As
a consequence of superior and fast estimation, we reduce the running time of all
existing gradient descent algorithms. We demonstrate the benefits of our proposal
on both SGD and AdaGrad.
1	Motivation
Stochastic gradient descent or commonly known as SGD is the most popular choice of optimization
algorithm in large-scale setting for its computational efficiency. A typical interest in machine learn-
ing is to minimize the average loss function f over the training data, with respect to the parameters
θ, i.e., the objective function of interest is
1N
θ* = arg min F(θ) = arg min — VJ f (xi, θ).
θ	θN
(1)
i=1
Throughout the paper, our training data D = {xi, yi}iN=1 will have N instances with d dimensional
features xi ∈ Rd and labels yi . The labels can be continuous real valued for regression problems.
For classification problem, they will take value in a discrete set, i.e., yi ∈ {1, 2,…，K}. Typically,
the function f is a convex function. The least squares f (Xi, θ) = (θ ∙ Xi - yi)2, used in regression
setting is a classical example of f.
SGD (Bottou, 2010) samples an instance Xj uniformly from N instances, and performs the gradient
descent update:
θt = θt-ι- ηtvf (Xj ,θt-ι),
(2)
where ηt is the step size at the tth iteration. The gradient vf(Xj,θt-1) is only evaluated on Xj,
using the current θt-1.
1
Workshop track - ICLR 2018
It should be noted that a full gradient of the objective is given by the average -N PN=I Vf (xi, θt-ι).
Thus, a uniformly sampled gradient Vf (xj, θt-ι) is an unbiased estimator of the full gradient, i.e.,
1N
E(Vf(Xj, θt-ι)) = N E Vf (xi, θt-ι).	⑶
i=1
This is the key reason why, despite only using one sample, SGD still converges to the local min-
ima, analogously to full gradient descent, provided ηt is chosen properly (Robbins & Monro, 1951;
Bottou, 2010).
However, it is known that the convergence rate of SGD is slower than that of the full gradient
descent (Shamir & Zhang, 2013). Nevertheless, the cost of computing the full gradient requires
O(N) evaluations of Vf compared to just O(1) evaluation in SGD. Thus, with the cost of one
epoch of full gradient descent, SGD can perform O(N) epochs, which overcompensates the slow
convergence. Therefore, despite slow convergence rates, SGD is almost always the chosen algorithm
in large-scale settings as the calculation of the full gradient in every epoch is prohibitively slow.
Further improving SGD is still an active area of research. Any such improvement will directly speed
up almost all the state-of-the-art algorithms in machine learning.
The slower convergence of SGD is expected due to the poor estimation of the gradient (the average)
by only sampling a single instance uniformly. Clearly, the variance of the one sample estimator is
high. As a result, there have been several efforts in finding sampling strategies for better estimation
of the gradients (Zhao & Zhang, 2014; Needell et al., 2014; Zhao & Zhang, 2015; Alain et al., 2015).
The key idea behind these methods is to replace the uniform distribution with a weighted distribution
which leads tp a lower variance.
However, with all adaptive sampling methods for SGD, whenever the parameters and the gradients
change, the weighted distribution has to change. Unfortunately, as argued in (Gopal, 2016), all
of these methods suffer from what We call the Chicken-and-egg loop - adaptive sampling improves
stochastic estimation but maintaining the required adaptive distribution will cost O(N) per iteration,
which is also the cost of computing the full gradient exactly.
To the best of our knowledge, there does not exist any generic sampling scheme for adaptive gradient
estimation, where the cost of maintaining and updating the distribution, per iteration, is O(1) which
is comparable to SGD. Our work provides first such sampling scheme utilizing the recent advances
in sampling and unbiased estimation using Locality Sensitive Hashing (Spring & Shrivastava, 2017).
1.1	Adaptive Sampling for SGD
For non-uniform sampling, we can sample each xi with an associated weight wi . These wi ’s can
be tuned to minimize the variance. It was first shown in (Alain et al., 2015), that sampling xi with
probability in proportion to the L? norm of the gradient, i.e. || Vf (χi, θt-1)∣∣2, leads to the optimal
distribution that minimizes the variance. However, sampling xi with probability in proportion to
Wi = ||Vf(xi, θt-1)∣∣2, requires first computing all the wi's, which change in every iteration be-
cause θt-1 gets updated. Therefore, maintaining the values of wi ’s is even costlier than computing
the full gradient. (Gopal, 2016) proposed to mitigate this overhead partially by exploiting addi-
tional side information such as the cluster structure of the data. Prior to the realization of optimal
variance distribution, (Zhao & Zhang, 2014) and (Needell et al., 2014) proposed to sample a train-
ing instance with a probability proportional to the Lipschitz constant of the function f(xi,θt-1) or
Vf (xi, θt-1) respectively. Again, as argued, in (Gopal, 2016), the cost of maintaining the distribu-
tion is prohibitive.
It is worth mentioning that before these works, a similar idea was used in designing importance
sampling-based low-rank matrix approximation algorithms. The resulting sampling methods, known
as leverage score sampling, are again proportional to the squared Euclidean norms of rows and
columns of the underlying matrix (Drineas et al., 2012).
The Chicken-and-Egg Loop: In summary, to speed up the convergence of stochastic gradient
descent, we need non-uniform sampling for better estimates (low variance) of the full gradient. Any
interesting non-uniform sampling is dependent on the data and the parameter θt which changes
in every iteration. Thus, maintaining the non-uniform distribution for estimation requires O(N)
computations to calculate the weights wi , which is the same cost computing it exactly. It is not even
2
Workshop track - ICLR 2018
clear that there exists any sweet and adaptive distribution which breaks this computational chicken-
and-egg loop. We provide the first affirmative answer by giving an unusual distribution which is
derived from probabilistic indexing based on locality sensitive hashing.
Our Contributions: In this work, we propose a novel LSH-based samplers, that breaks the afore-
mentioned chicken-and-egg loop. Our algorithm, which we call LSD (LSH Sampled Stochastic
gradient Descent), are generated via hash lookups which have O(1) cost. Moreover, the probability
of selecting xi is provably adaptive. Therefore, the current gradient estimates have lower variance,
compared to a single sample SGD, while the computational complexity of sampling is constant and
of the order of SGD sampling cost. Furthermore, we demonstrate that LSD can be utilized to speed
up any existing gradient-based optimization algorithm such as AdaGrad (Duchi et al., 2011).
As a direct consequence, we obtain a generic and efficient gradient descent algorithm which con-
verges significantly faster than SGD, both in terms of epochs as well as running time. It should
be noted that rapid epoch wise convergence alone does not imply computational efficiency. For
instances, Newtons method converges faster, epoch wise, than any first-order gradient descent, but
it is prohibitively slow in practice. The wall clock time or the amount of floating point operations
performend to reach convergence should be the metric of consideration for useful conclusions.
Accuracy Vs Running Time: It is rare to see any fair (same computational setting) empirical
comparisons of SGD with existing adaptive SGD schemes, which compare the improvement in
accuracy with respect to running time on the same computational platform. Almost all methods
compare accuracy with the number of epochs, which is unfair to SGD which can complete O(N)
epochs at the computational cost (or running time) of one epoch for adaptive sampling schemes.
2	Background
We first describe a recent advancement in the theory of sampling and estimation using locality
sensitive hashing (LSH) (Indyk & Motwani, 1998) which will be heavily used in our proposal.
Before we get into details of sampling, let us revise the two-decade-old theory of LSH.
2.1	Locality Sensitive Hashing (LSH)
Locality-Sensitive Hashing (LSH) (Indyk & Motwani, 1998) is a popular, sub-linear time algorithm
for approximate nearest-neighbor search. The high-level idea is to place similar items into the same
bucket of a hash table with high probability. An LSH hash function maps an input data vector to an
integer key
h(x): RD 7→ [0, 1, 2,...,N].
A collision occurs when the hash values for two data vectors are equal: h(x) = h(y). The collision
probability of most LSH hash functions is generally a monotonic function of the similarity
Pr[h(x) = h(y)] = M(sim(x, y)),
where M is a monotonically increasing function. Essentially, similar items are more likely to collide
with each other under the same hash fingerprint.
The algorithm uses two parameters, (K, L). We construct L independent hash tables from the
collection C. Each hash table has a meta-hash function H that is formed by concatenating K random
independent hash functions from F. Given a query, we collect one bucket from each hash table
and return the union of L buckets. Intuitively, the meta-hash function makes the buckets sparse
and reduces the number of false positives, because only valid nearest-neighbor items are likely to
match all K hash values for a given query. The union of the L buckets decreases the number of
false negatives by increasing the number of potential buckets that could hold valid nearest-neighbor
items.
The candidate generation algorithm works in two phases [See (Spring & Shrivastava, 2017) for
details]:
1.	Pre-processing Phase: We construct L hash tables from the data by storing all elements
x ∈C. We only store pointers to the vector in the hash tables because storing whole data
vectors is very memory inefficient.
3
Workshop track - ICLR 2018
2.	Query Phase: Given a query Q; we will search for its nearest-neighbors. We report the
union from all of the buckets collected from the L hash tables. Note, we do not scan all the
elements in C , we only probe L different buckets, one bucket for each hash table.
After generating the set of potential candidates, the nearest-neighbor is computed by comparing the
distance between each item in the candidate set and the query.
2.2	LSH for Estimations and Sampling
An item returned as candidate from a (K, L)-parameterized LSH algorithm (section 3.2) is sampled
with probability 1 - (1 - pK)L, where p is the collision probability of LSH function. The LSH
family defines the precise form of p used to build the hash tables. This sampling view of LSH was
first utilized to perform adaptive sparsification of deep networks in near-constant time, leading to
efficient backpropagation algorithm (Spring & Shrivastava, 2016).
A year later, (Spring & Shrivastava, 2017) demonstrated the first theory of using these samples
for unbiased estimation of partition functions in log-linear models. More specifically, the authors
showed that since we know the precise probability of sampled elements 1 - (1 - pK)L, we could
design provably unbiased estimators using importance sampling type idea. This was the first demon-
stration that random sampling could be beaten with roughly the same computational cost as vanilla
sampling. (Luo & Shrivastava, 2017) used the same approach for unbiased estimation of anomaly
scoring function. (Charikar & Siminelakis) rigorously formalized these notions and showed prov-
able improvements in sample complexity of kernel density estimation problems. Recently, (Chen
et al., 2017) used the sampling in a very different context of connected component estimation for
unique entity counts.
2.2.1	MIPS Sampling
Recent advances in maximum inner product search (MIPS) using asymmetric locality sensitive hash-
ing has made it possible to sample large inner products.
For this paper, it is safe to assume that given a collection C of vectors and query vector Q, using
(K, L)-parameterized LSH algorithm with MIPS hashing (Shrivastava & Li, 2014), we get a candi-
date set S that every element xi ∈Cis sampled with probability pi ≤ 1, where pi is a monotonically
increasing function of Q ∙ Xi. Thus, We can Pay a one-time linear cost of preprocessing C into hash
tables, and any further adaptive sampling for query Q only requires few hash lookups. We can also
compute the probability of getting x.
Before getting into our main algorithm Where We use the above sampling process for estimation,
We Would like to cover some of its properties. To begin With, the sampling scheme is not a valid
distribution, i.e., Pxi∈C pi 6=1. In addition, given a query, the probability of sampling xi is not
independent of the probability of sampling xj (i 6= j). HoWever, We can still use it for unbiased
estimation. Details of such sampling are included in (Spring & Shrivastava, 2017). In fact, the
form of sampling probability Pi is quite unusual. Pi is a monotonic function of q ∙ Xi because
Pi = (1 - (1 - g(q ∙ Xi))K)L, where g(q ∙ Xi) is the collision probability.
3The LSD Algorithm
3.1 A Generic Framework for Efficient Gradient Estimation
Our algorithm leverages the efficient estimations using locality sensitive hashing, which usually
beats random sampling estimators while keeping the sampling cost near-constant. We first provide
the intuition of our proposal, and the analysis will follow. Consider least squares regression with
loss function -N PN=1(yi - θt ∙ Xi)2, where θt is the parameter in the tth iteration. The gradient is
just like a partition function. If we simply follow the procedures in (Spring & Shrivastava, 2017),
we can easily show a generic unbiased estimator via adaptive sampling. However, better sampling
alternatives are possible.
Observing that the gradient, with respect to θt concerning Xi, is given by 2(yi - θt ∙ Xi)Xi, the
L2 norm of the gradient can therefore be written as an absolute value of inner product. according
4
Workshop track - ICLR 2018
to (Alain et al., 2015), the L2 norm of the gradient is also the optimal sampling weight w* for xi.
∣∣Vf (xi, θt)k2 = ∣2(θt ∙Xi - yi)∣∣Xi∣∣2∣	(4)
=2∣hθt,-1i ∙ hχikχik2,yikχik2i∣,	(5)
where hθt, -1i is a vector concatenation of θ with -1. If the data is normalized then we should
sample Xi in proportion to wi* = ∣ <θt, -1)∙ Q%, y/1, i.e. large magnitude inner products should be
sampled with higher probability.
As argued, such sampling process is expensive because wi* changes with θt. We address this issue by
designing a sampling process that does not exactly sample with probability wi* but instead samples
from a different weighted distribution which is a monotonic function ofwi*. Specifically, we sample
from wilsh = f (wi*), where f is some monotonic function. Before we describe the efficient sampling
process, we first argue that a monotonic sampling is a good choice for gradient estimation.
Algorithm 1 LSH-Sampled Stochastic gradient Descent (LSD) Algorithm
Input： D = Xi, yi, N, θo, η
Input: LSH Family H, parameters K, L
Output: θ*
HT = Get preprocessed training data vectors Xlsh,ylsh and then put hXlish, ylishi into LSH Data
structure.
Get X0train, yt0rain from preprocessed data
t=0
while NotConverged do
Xlish,p= Sample(H,HT, K, hθt, -1i) (Algorithm 2)
00
Get corresponding Xitrain, ytirain from preprocessed data
i0
θt+ι := θt - ηt( f(p×N t)
end while
return θ *
For any monotonic function f, the weighted distribution wilsh = f(wi*) is still adaptive and changes
with θt. Also, due to monotonicity, if the optimal sampling prefers Xi over Xj i.e. wi* ≥ wj*, then
monotonic sampling will also have same preference, i.e., wilsh ≥ wjlsh.
The key insight is that there are two quantities in the inner product (equation 4), hθt, -1i and hXi,yii.
With successive iteration, hθt, -1i changes while hXi,yii is fixed. Thus, it is possible to preprocess
hXi,yii into hash tables (one time cost) and query with hθt, -1i for efficient and adaptive sampling.
With every iteration, only the query changes to hθt+1, -1i, but the hash tables remains the same.
Few hash lookups are sufficient to sample Xi for gradient estimation adaptively. Therefore, we only
pay one-time preprocessing cost of building hash tables and few hash lookups, typically just one, in
every iteration to get a sample for estimation.
There are few more technical subtleties due to the absolute value of inner product ∣hθt, -1) ∙(Xi ,yi) ∣,
rather than the inner product itself. However, the square of the absolute value of the inner product
∣(θt,-1) ∙ (xi,yi)∣2 = T((θt,-1)) ∙ T((xi,yii),
can also be written as an inner product as it is a quadratic kernel, and T is the corresponding fea-
ture expansion transformation. Again square is monotonic function, and therefore, our sampling
is still monotonic as composition of monotonic functions is monotonic. Thus, technically we hash
T(hXi,yi)) to create hash tables and the query at tth step is T (hθt, -1)).
Once an Xi is sampled via LSH sampling (Algorithm 2), we can precisely compute the probability
of its sampling, i.e., pi (See section 2). It is not difficult to show that our estimation of full gradient
is unbiased (Section 3.3).
3.2 Algorithmic Details
We first describe the detailed step of our gradient estimator in Algorithm 1. We also provide the
sampling algorithm 2 with detail. Assume that we have access to the right LSH function h, and
5
Workshop track - ICLR 2018
Algorithm 2 Sample
Input: H (Hash functions), HT [][] (L Hash Tables), K, Query
cp(x, Q) is the collision probability Pr(h(x)= h(Q)), under given LSH (known)
Output: sampled data x, probability of sampling p
l,	S =0
while true do
ti = random(1, L)
bucket = H (Query, ti) (table specific hash)
if HT[ti][bucket] = empty then
l++
continue;
end if
S = |HT [ti][bucket]| (size of bucket)
x = randomly pick one element from HT [ti][bucket]
break;
end while
P =(1 — (1 — cp(x, Query'K)l) X 1
return x, p
its collision probability expression cp(x, y) = Pr(h(x) = h(y)). For linear regression, we can
use signed random projections, simhash (Charikar, 2002), or MIPS hashing. With normalized data,
COS-1( || ∣x∙y || )
Simhash collision probability is cp(x,y) = 1---------llχll2llyll2 , which is monotonic in the inner
product. Furthermore, we centered the data we need to store in the LSH hash table to make the
simhash query more efficient.
3.2.1	Running Time of Sampling
The computational cost of SGD sampling is merely a single random number generator. The cost of
gradient update (equation 2) is one inner product, which is d multiplications. If we want to design an
adaptive sampling procedure that beats SGD, the sampling cost cannot be significantly larger than d
multiplications.
The cost of LSD sampling (Algorithm 2) is K × l hash computations followed by l +1 random
number generator, (1 extra for sampling from the bucket). Since the scheme works for any K, we can
always choose K small enough so that empty buckets are rare (see (Spring & Shrivastava, 2017)). In
all of our experiments, K = 5 for which l is almost always 1. Thus, we require K hash computations
and only two random number generations. If we use very sparse random projections, then K hash
computations only require a constant《 d multiplications. For example, in all our experiments We
only need 30 multiplication, in expectation, to get all the hashes using sparse projections. Therefore,
our sampling cost is significantly less than d multiplication which is the cost of gradient update.
Using fast hash computation is critical for our method to work in practice.
3.2.2	Near-Neighbor is Costlier than LSH-Sampling
It might be tempting to use approximate near-neighbor search with query θt to find xi . Near-
neighbor search has been used in past Dhillon et al. (2011) to speed up coordinate descent. However,
near-neighbor queries are expensive due to candidate generation and filtering. It is still sub-linear in
N (and not constant). Thus, even if we see epoch wise faster convergence, iterations with a near-
neighbor query would be orders of magnitude slower than a single SGD iteration. Moreover, the
sampling probability of x cannot be calculated for near-neighbor search which would cause bias in
the gradient estimates.
It is important to note that although LSH is heavily used for near-neighbor search, in our case, we use
it as a sampler. For efficient near neighbor search, K and L grow with N (Indyk & Motwani, 1998).
In contrast, the sampling works for any K and l 1 as small as one leading to only approximately 1.5
times the cost of SGD iteration (see section 4). Efficient unbiased estimation is the key difference
1L represents the number of hash tables but l represents the number of hash tables used in one query
6
Workshop track - ICLR 2018
(a) YearPredictionMSD
(b) Slice
(c) UJIIndoorLoc
(d) YearPredictionMSD
(e) Slice
(f) UJIIndoorLoc
Figure 1: In subplots (a)(b)(c), the comparisons of Wall clock training loss convergence are made
between plain LSD (red lines) and plain SGD (blue lines) separately in three datasets. We can
clearly see the big gap between them representing LSD converge faster than SGD even in time wise.
Subplots (c)(d)(e) shows the results for same comparisons but in epoch wise. We can see that LSD
converges even faster than SGD which is not surprising because LSD costs a bit more time than
SGD does in every iteration. Comparisons of testing loss are in supplemental material.
that makes sampling practical while near-neighbor query prohibitive. It is unlikely that a near-
neighbor query would beat SGD in time, while sampling would.
3.3 Variance Analysis
In this section, we first prove that our estimator of the gradient is unbiased with lower variance than
SGD for most real datasets. Define S as the bucket that contains the sample x from in Algorithm
2. For simplicity we denote the query as θt and pi =1- (1 - cp(xi,θt)K)l as the probability of
finding xi in bucket S.
Theorem 1.	The following expression is an unbiased estimator of the full gradient
Et _ 1XN	W") ∙∣s∣	(6)
st	N / √ ∙^xi∈S ∙ɪ(χi = xm I xi∈S)	. , (6)
N i=1	pi
1N
E[Est] = N E▽/(Xi,θt).	(7)
i=1
Theorem 2.	The Trace of the covariance of our estimator:
Tr(XEst)) = N X W(Xi；)k2 "S1 — Nkg Vf (Xi, θt))k∣	(8)
pi
The trace of the covariance of LSD is the total variance of the descent direction. The variance can
be minimized when the sampling probability of Xi is proportional to the L2-norm of the gradient we
mentioned in Section 1.1. The intuition of the advantage of LSD estimator comes from sampling Xi
under a distribution monotonic to the optimal one. We first make a simple comparison of the variance
of LSD with that of SGD theoretically and then in Section 4 and we would further empirically show
the drastic superiority of LSD over SGD.
7
Workshop track - ICLR 2018
Lemma 1. The Trace of the covariance of LSD’s estimator is smaller than that of SGD’s estimator
if
N X f^≡ < X W…,
N i=1	pi	i=1
YbarPredlCtlonMSD Ibalnlng Loss with Ada Grad
(a) YearPredictionMSD
(b) Slice
UjIIndoorLoc Italnlng Loss with Ada Grad
(c) UJIIndoorLoc
(d) YearPredictionMSD
(e) Slice
(f) UJIIndoorLoc
Figure 2: In subplots (a)(b)(c), the comparisons of Wall clock training loss convergence are made
between LSD+adaGrad (red lines) and SGD+adaGrad (blue lines) separately in three datasets. We
can again see the similar gap between them representing LSD converge faster than SGD in time
wise. Subplots (c)(d)(e) show the results for same comparisons but in epoch wise. We can see that
LSD converges even faster than SGD. Comparisons of testing loss are in supplemental material.
SGD would perform well if the data is uniformly distributed but it is unlikely in practice. Recall that
the collision probability pi =1- (1 - pK)l mentioned in Section 2.2. Note that l here according
to Algorithm 2 is the number of tables that have been utlized by the sampling process. In most
practical cases and also in our experiment, K and l are relatively small. L should be large to ensure
enough randomness but it does not show up in the sampling time (See Alg. 2). LSD can be efficient
and achieve a much smaller variance than SGD by setting small values of K and l. It is not difficult
to see that if several terms in the summation satisfy p|S^ ≤ 1, then the variance of our estimator is
better than random sampling. If the data is clustered nicely, i.e. a random pair has low similarity,by
tuning K, we can achieve the above inequality of |S |, pi and N. See Spring & Shrivastava (2017);
Charikar & Siminelakis for more details on when LSH sampling is better than random sampling.
4	Experiments
We examine the effectiveness of our algorithm on three large regression dataset, in the area of mu-
sical chronometry, clinical computed tomography, and WiFi-signal localization, respectively. The
dataset descriptions and our experiment results are as follows:
YearPredictionMSD: (Lichman, 2013) The dataset contains 515,345 instances subset of the Mil-
lion Song Dataset with dimension 90. We respect the original train/test split, first 463,715 examples
for training and the remaining 51,630 examples for testing, to avoid the ‘producer effect’ by making
sure no song from a given artist ends up in both the train and test set.
Slice: (Lichman, 2013) The data was retrieved from a set of 53,500 CT images from 74 different
patients. It contains 385 features. We use 42,800 instances as training set and the rest 10,700 in-
stances as the testing set.
8
Workshop track - ICLR 2018
(a) YearPredictionMSD
(b) Slice
(d) YearPredictionMSD
(e) Slice
Figure 3: Norm and cosine similarity comparisons of LSD and SGD gradient estimation. Subplots
(a)(b)(c) show the comparisons of the average (over number of samples) gradient L2 norm of the
points that LSD (red lines) and SGD sampled (blue lines). As argued before, LSD samples with
probability monotonic to L2 norm of the gradients while SGD samples uniformly. It matches with
the results shown in the plots that LSD queries points with larger gradient than SGD does. Sub-
plots (d)(e)(f) show the comparison of the cosine similarity between gradient estimated by LSD and
the true gradient and the cosine similarity between gradient estimated by SGD and the true gradi-
ent. Note that the variance of both norm and cosine similarity reduce when we average over more
samples.
UJIIndoorLoc: (Torres-Sospedra et al., 2014) The database covers three buildings of Universitat
Jaume I with 4 or more floors and almost 110,000 m2. It is a collection of 21,048 indoor location
information with 529 attributes containing the WiFi fingerprint, the coordinates where it was taken,
and other useful information. We equally split the total instances for training and testing.
All datasets were preprocessed as described in Section 3.2. Note that for all the experiments, the
choice of the gradient decent algorithm was the same. For both SGD and LSD, the only difference
in the gradient algorithm was the gradient estimator. For SGD, a random sampling estimator was
used, while for LSD, the estimator used the adaptive estimator. We used fixed values K =5
and L = 100 for all the datasets. l is the number of hash tables that have been searched before
landing in a non-empty bucket in a query. In our experiments l is almost always as low as 1. L
only affects preprocessing but not sampling. Our hash function was simhash (or signed random
projections) and We used sparse random projections with sparsity 击 for speed. We know that epoch
wise convergence is not a true indicator of speed as it hides per epoch computation. Our main focus
is convergence with running time, which is a better indicator of computational efficiency.
To the best of our knowledge, there is no other adaptive estimation baseline, where the cost of
sampling per iteration is less than linear O(N). Since our primary focus would be on wall clock
speedup, no O(N) estimation method would be able to outperform O(1) SGD (and LSD) estimates
on the same platform. From section 3.2.2, even methods requiring a near-neighbor query would be
too costly (orders of magnitude) to outperform SGD from computational perspective.
4.1	LSD vs. SGD
In the first experiment, we compare vanilla SGD with LSD, i.e., we use simple SGD with fixed
learning rate. This basic experiment aims to demonstrate the performance of pure LSD and SGD
9
Workshop track - ICLR 2018
without involving other factors like L1 /L2 regularizations on linear regression task. In such a way,
we can quantify the superiority of LSD.
Figure 1 shows the decrease in the squared loss error with epochs. Blue lines represent SGD and
red lines represent LSD. It is obvious that LSD converges much faster than SGD in both training
and testing loss comparisons. This is not surprising with the claims in Section 3.2.1 and theoretical
proof in Section 3.3. Since LSD uses slightly more computations per epoch than SGD does, it is
hard to defend if LSD gains enough benefits simply from the epoch wise comparisons. We therefore
also show the decrease in error with wall clock time also in figure 1. Wall clock time is the actual
quantification of speedups. Again, on every single dataset, LSD shows faster time-wise convergence
as well. (Plots for Testing loss are in the supplementary material.)
4.2	LSD+AdaGrad vs. SGD+AdaGrad
As argued in section 1.1, our LSD algorithm is complimentary to any gradient-based optimization
algorithm. We repeated the first experiment but using AdaGrad (Duchi et al., 2011) instead of plain
SGD. Again, other settings are fixed for both algorithms but the only change in the competing algo-
rithm is the gradient estimates per epoch. Figure 2 shows epoch wise and running time comparisons
on LSD and SGD convergence. The trends as expected are similar to those of LSD vs. SGD. LSD
with AdaGrad outperforms AdaGrad (SGD) estimates of gradients both epoch-wise and time-wise.
(Plots for Testing loss are in the supplementary.)
4.3	LSD, SGD vs. True Gradient:
In this section, as a sanity check, we first verify weather LSD samples data point with probability
monotonic to L2 norm of the gradient mentioned in section 3.1. In order to do that, we freeze the
optimization at an intermediate iteration and use the θ at that moment to sample data points with
LSD as well as SGD to compute gradient L2 norm separately. The upper three plots in Figure 3
show the comparison of the sampled gradient norm of LSD and SGD. X-axis represents the number
of samples that we averaged in the above process. It is obvious that LSD sampled points have larger
gradient norm than SGD ones consistently across all three datasets.
In addition, we also do a sanity check that if empirically, the chosen sample from LSD get bet-
ter estimation of the true gradient direction than that of SGD. Again, we freeze the program at an
intermediate iteration like the experiments above. Then we compute the angular similarity of full
gradient (average over the training data) direction with both LSD and SGD gradient direction, where,
cos-1	x∙y
Similarity = 1---------k∏k2kyk2 . From the bottom three plots in Figure 3, We can see that in aver-
age, LSD estimated gradient has smaller angle (more aligned) to true gradient than SGD estimated
gradient.The variance of both norm and cosine similarity reduce When We average them over more
samples as shoWn in plots.
5	Conclusion
In this paper, We proposed a novel LSH-based sampler With a reduction to the gradient estimation
variance. We achieved it by sampling With probability proportional to the L2 norm of the instances
gradients leading to an optimal distribution that minimizes the variance of estimation. More remark-
ably, LSD is as computationally efficient as SGD but achieves faster convergence not only epoch
Wise but also time Wise.
6	Acknowledgements
This Work Was supported by National Science Foundation IIS-1652131, RI-1718478, and a GPU
grant from NVIDIA.
References
Guillaume Alain, Alex Lamb, Chinnadhurai Sankar, Aaron Courville, and Yoshua Bengio. Variance
reduction in sgd by distributed importance sampling. arXiv preprint arXiv:1511.06481, 2015.
10
Workshop track - ICLR 2018
Leon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of
COMPSTAT,2010, pp. 177-186. Springer, 2010.
Moses Charikar and Paris Siminelakis. Hashing-based-estimators for kernel density in high dimen-
sions.
Moses S Charikar. Similarity estimation techniques from rounding algorithms. In Proceedings of
the thiry-fourth annual ACM symposium on Theory of computing, pp. 380-388. ACM, 2002.
Beidi Chen, Anshumali Shrivastava, and Rebecca C Steorts. Unique entity estimation with applica-
tion to the syrian conflict. arXiv preprint arXiv:1710.02690, 2017.
Inderjit S. Dhillon, Pradeep K. Ravikumar, and Ambuj Tewari. Nearest neighbor based greedy
coordinate descent. In J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q. Wein-
berger (eds.), Advances in Neural Information Processing Systems 24, pp. 2160-2168. Curran
Associates, Inc., 2011.
Petros Drineas, Malik Magdon-Ismail, Michael W Mahoney, and David P Woodruff. Fast approx-
imation of matrix coherence and statistical leverage. Journal of Machine Learning Research, 13
(Dec):3475-3506, 2012.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
Siddharth Gopal. Adaptive sampling for sgd by exploiting side information. In International Con-
ference on Machine Learning, pp. 364-372, 2016.
Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: towards removing the curse of
dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing,
pp. 604-613. ACM, 1998.
M. Lichman. UCI machine learning repository, 2013. URL http://archive.ics.uci.edu/
ml.
Chen Luo and Anshumali Shrivastava. Arrays of (locality-sensitive) count estimators (ace): High-
speed anomaly detection via cache lookups. arXiv preprint arXiv:1706.06664, 2017.
Deanna Needell, Rachel Ward, and Nati Srebro. Stochastic gradient descent, weighted sampling,
and the randomized kaczmarz algorithm. In Advances in Neural Information Processing Systems,
pp. 1017-1025, 2014.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathemati-
cal statistics, pp. 400-407, 1951.
Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization: Conver-
gence results and optimal averaging schemes. In International Conference on Machine Learning,
pp. 71-79, 2013.
Anshumali Shrivastava and Ping Li. Asymmetric lsh (alsh) for sublinear time maximum inner prod-
uct search (mips). In Advances in Neural Information Processing Systems, pp. 2321-2329, 2014.
Ryan Spring and Anshumali Shrivastava. Scalable and sustainable deep learning via randomized
hashing. arXiv preprint arXiv:1602.08194, 2016.
Ryan Spring and Anshumali Shrivastava. A new unbiased and efficient class of lsh-based sam-
plers and estimators for partition function computation in log-linear models. arXiv preprint
arXiv:1703.05160, 2017.
J. Torres-Sospedra, R. Montoliu, A. Martnez-Us, J. P. Avariento, T. J. Arnau, M. Benedito-
Bordonau, and J. Huerta. Ujiindoorloc: A new multi-building and multi-floor database for wlan
fingerprint-based indoor localization problems. In 2014 International Conference on Indoor Posi-
tioning and Indoor Navigation (IPIN), pp. 261-270, Oct 2014. doi: 10.1109/IPIN.2014.7275492.
Peilin Zhao and Tong Zhang. Accelerating minibatch stochastic gradient descent using stratified
sampling. arXiv preprint arXiv:1405.3080, 2014.
11
Workshop track - ICLR 2018
Peilin Zhao and Tong Zhang. Stochastic optimization with importance sampling for regularized
loss minimization. In Proceedings of the 32nd International Conference on Machine Learning
(ICML-15),pp.1-9, 2015.
A Epoch Plots and Proofs
Theorem 3.	Let S be the bucket that sample xm is chosen from in Algorithm 2. Let pm be the
sampling probability associated with sample xm . Suppose we query a sample with θt. Then we have
an unbiased estimator of the full gradient:
1 N
Est = Ν∑S 1Xi∈S Mxi = Xm∣Xi∈S)
N i=1
Vf (xi, θt) ∙∣S∣
pi
1N
EEsti = N EVf(Xi,θt)
i=1
Proof.
E[ lxi∈S ] = pi, and E[ lxi = xm∣xi∈S ] = ∣S∣.
Also note that
E[ = xi∈S=xi=xm∣xi∈S] = EHxi∈S]E[1 xi = xm∣xi∈S].
Then,
E[Est] = NE[X>xi∈slxi=xmki∈s Vf(X:θt) "S1 ]
N i=1	pi
=X EW xi ∈slxi = xm∣xi∈S] ∙ E[ Vf(Xi，%)IH ]
N i=1	pi
1 G 1 Vf(χi,θt) ∙∣S∣
=N i=1 pi .西—pi—
1N
=N EVf(Xi,θt)
i=1
□
Theorem 4.	The Trace of the covariance of our estimator is:
Tr(XEst)) = N X W",：)k2 .S - N (X kVf (Xi,θt )k2)2.
i=1	pi	i=1
Proof.
Tr(Σ(Est)=E[EstTEst] - E[Est]TE[Est]
12
Workshop track - ICLR 2018
▽f(χi, θt) ∙ Pf(Xj,θt) ∙∣s∣2
T	1	F	F	F
Est Est = N2 ) : lχi∈slχj ∈slχi = χm∣χi∈slχj =Xm∣Xj ∈S
i,j
Pi ∙ Pj
ι X
JN2 / y xi∈S χi = xm | xi∈S
i
(▽kf(xi, θt))k2 ∙∣SI2
E[EstTEst] = N X ("i72 ∙lSl
Tr(XEst)) = N X W"",：)k2 .S - N (X Vkf(Xi,θt)k2)
Pi
□
N
< XkVf(Xi,θt)k22	(10)
i=1
Lemma 2. The Trace of the covariance of LSD’s estimator is smaller than that of SGD’s estimator
if
ɪX kVf(Xi,θt)k2 ∙∣S∣
N i=1	pi
Proof. The trace of covariance of regular SGD is
1N	1N
Tr(∑(Est0))=而 E kVf (Xi, θt)k2 — - (E kVf(Xi, θt)k2)2.	(11)
N	N2
i	i=1
By (11) and (17), one can easily see that Tr(Σ(Est)) < Tr(Σ(Est0)) When (16) satisfies.	口
(a) YearPredictionMSD	(b) Slice
(c) UJIIndoorLoc
(d) YearPredictionMSD
(e) Slice
(f) UJIIndoorLoc
Figure 4:	Epoch Wise convergence comparisons of training and testing loss of LSD and SGD.
13
Workshop track -ICLR 2018
Hme (ms)
(c) UJIIndoorLoc
(a) YearPredictionMSD	(b) Slice
(d) YearPredictionMSD
(e) Slice
UJlInd09rLpc l⅛stinq LOSS With A^a Grad
Epoch
(f) UJIIndoorLoc
Figure 5:	Epoch wise convergence comparisons of training and testing loss of LSD+adaGrad and
SGD+adaGrad.
14