Under review as a conference paper at ICLR 2018
Regret Minimization for Partially Observable
Deep Reinforcement Learning
Anonymous authors
Paper under double-blind review
Ab stract
Deep reinforcement learning algorithms that estimate state and state-action value
functions have been shown to be effective in a variety of challenging domains,
including learning control strategies from raw image pixels. However, algorithms
that estimate state and state-action value functions typically assume a fully ob-
served state and must compensate for partial or non-Markovian observations by
using finite-length frame-history observations or recurrent networks. In this work,
we propose a new deep reinforcement learning algorithm based on counterfac-
tual regret minimization that iteratively updates an approximation to a cumulative
clipped advantage function and is robust to partially observed state. We demon-
strate that on several partially observed reinforcement learning tasks, this new
class of algorithms can substantially outperform strong baseline methods: on Pong
with single-frame observations, and on the challenging Doom (ViZDoom) and
Minecraft (Malmo) first-person navigation benchmarks.
1	Introduction
Many reinforcement learning problems of practical interest have the property of partial observability,
where observations of state are generally non-Markovian. Despite the importance of partial obser-
vation in the real world, value function-based methods such as Q-learning (Mnih et al., 2013; 2015)
generally assume a Markovian observation space. On the other hand, Monte Carlo policy gradient
methods do not assume Markovian observations, but many practical policy gradient methods such
as A3C (Mnih et al., 2016) introduce the Markov assumption when using a critic or state-dependent
baseline in order to improve sample efficiency.
Consider deep reinforcement learning methods that learn a state or state-action value function. One
common workaround for the problem of partial observation is to learn value functions on the space
of finite-length frame-history observations, under the assumption that frame-histories of sufficient
length will give the environment the approximate appearance of full observability. When learn-
ing to play Atari 2600 games from images, deep Q-learning algorithms (Mnih et al., 2013; 2015)
concatenate the last 4 observed frames of the video screen buffer as input to a state-action value
convolutional network. Not all non-Markovian tasks are amenable to finite-length frame-histories;
recurrent value functions can incorporate longer and potentially infinite histories (Hausknecht &
Stone, 2017; Foerster et al., 2016), but at the cost of solving a harder optimization problem. Can we
develop methods that learn a variant of the value function that is more robust to partial observability?
Our contribution is a new model-free deep reinforcement learning algorithm based on the principle
of regret minimization which does not require access to a Markovian state. Our method learns a
policy by estimating a cumulative clipped advantage function, which is an approximation to a type
of regret that is central to two partial information game-solving algorithms from which we draw our
primary inspiration: counterfactual regret minimization (CFR) (Zinkevich et al., 2007) and CFR+
(Tammelin, 2014). Hence we call our algorithm “advantage-based regret minimization” (ARM).
We evaluate our approach on three visual reinforcement learning domains: Pong with varying frame-
history lengths (Bellemare et al., 2013), and the first-person games Doom (Kempka et al., 2016)
and Minecraft (Johnson et al., 2016). Doom and Minecraft exhibit a first-person viewpoint in a 3-
dimensional environment and should appear non-Markovian even with frame-history observations.
We find that our method offers substantial improvement over prior methods in these partially observ-
1
Under review as a conference paper at ICLR 2018
able environments: on both Doom and Minecraft, our method can learn well-performing policies
within about 1 million simulator steps using only visual input frame-history observations.
2	Related Work
Deep reinforcement learning algorithms have been demonstrated to achieve excellent results on a
range of complex tasks, including playing games (Mnih et al., 2015; Oh et al., 2016) and continuous
control (Schulman et al., 2015; Lillicrap et al., 2016; Levine et al., 2016). Prior deep reinforce-
ment learning algorithms either learn state or state-action value functions (Mnih et al., 2013), learn
policies using policy gradients (Schulman et al., 2015), or perform a combination of the two using
actor-critic architectures (Mnih et al., 2016). Policy gradient methods typically do not need to as-
sume a Markovian state, but tend to suffer from poor sample complexity, due to their inability to use
off-policy data. Methods based on learning Q-functions can use replay buffers to include off-policy
data, accelerating learning (Lillicrap et al., 2016). However, learning Q-functions with Bellman er-
ror minimization typically requires a Markovian state space. When learning from observations such
as images, the inputs might not be Markovian. Prior methods have proposed to mitigate this issue
by using recurrent critics and Q-functions (Hausknecht & Stone, 2017; Oh et al., 2016; Mnih et al.,
2016; Heess et al., 2015), and learning Q-functions that depend on entire histories of observations.
Heuristics such as concatenation of short observation sequences have also been used (Mnih et al.,
2015). However, all of these changes increase the size of the input space, increasing variance, and
make the optimization problem more complex. Our method instead learns cumulative advantage
functions that depend only on the current state, but can still handle non-Markovian problems.
The form of our advantage function update resembles positive temporal difference methods (Peng
et al., 2016; van Hasselt & Wiering, 2007). Additionally, our update rule for a modified cumula-
tive Q-function resembles the average Q-function (Anschel et al., 2017) used for variance reduction
in Q-learning. In both cases, the theoretical foundations of our method are based on cumulative
regret minimization, and the motivation is substantively different. Previous work by Ross et al.
(2011); Ross & Bagnell (2014) has connected regret minimization to reinforcement learning, im-
itation learning, and structured prediction, although not with counterfactual regret minimization.
Regression regret matching (Waugh et al., 2015) is based on a closely related idea, which is to di-
rectly approximate the regret with a linear regression model, however the use of a linear model is
limited in representation compared to deep function approximation.
3	Advantage-based Regret Minimization
In this section, we provide background on CFR and CFR+, describe ARM in detail, and give some
intuition for why ARM works.
3.1	Counterfactual regret minimization (CFR)
In this section we review the algorithm of counterfactual regret minimization (Zinkevich et al.,
2007). We closely follow the version of CFR as described in the Supplementary Material of Bowling
et al. (2015), except that we try to use the notation of reinforcement learning where appropriate.
Consider the setting of an extensive game. There are N players numbered i = 1, . . . , N. An
additional player may be considered a “chance” player to simulate random events. At each time step
of the game, one player chooses an action a ∈ Ai . Define the following concepts and notation:
•	Sequences: A sequence specifically refers to a sequence of actions starting from an initial
game state. (It is assumed that a sequence of actions, including actions of the “chance”
player, is sufficient for defining state within the extensive game.) Let H be the space of all
sequences, and let Z be the space of terminal sequences.
•	Information sets: Let I be the space of information sets; that is, for each I ∈ I, I is a set
of sequences h ∈ I which are indistinguishable to the current player. Information sets are
a represention of partial observability.
•	Strategies: Let ∏i(a∣I) be the strategy of the i-th player, where ∏(a∖I) is a probability
distribution over action a conditioned on information set I. Let π = (π1 , . . . , πN) denote
2
Under review as a conference paper at ICLR 2018
the strategy profile for all players, and let π-i = (π1 , . . . , πi-1, πi+1, . . . , πN) denote the
strategy profile for all players except the i-th player.
•	Sequence probabilities: Let ρπ(h) be the probability of reaching the sequence h when all
players follow π. Additionally, let ρπ (h, h0) be the probability of reaching h0 conditioned
on h having already been reached. Similarly, define ρiπ and ρπ-i to contain the contributions
of respectively only the i-th player or of all players except the i-th.
•	Values: Let ui(z) be the value of a terminal sequence z to the i-th player. Let the expected
value of a strategy profile π to the i-th player be Ji (π) = Pz∈Z ρπ (z)ui (z).
Define the counterfactual value QCπF,i of all players following strategy π, except the i-th player plays
to reach information set I and to then take action a:
QCFi(I,a) = X X ρ-i(z)ρ∏lI→a(h,z)Ui(z).	(1)
h∈I z∈Z:h@z
The notation h @ h0 denotes that h is a prefix of h0, while π∣I → a denotes that action a is to be
performed when I is observed. The counterfactual value QπCF,i(I, a) is a calculation that assumes the
i-th player reaches any h ∈ I, and upon reaching any h ∈ I it always chooses a.
Consider a learning scenario where at the t-th iteration the players follow a strategy profile πt . The
i-th player's regret after T iterations is defined in terms of the i-th player's optimal strategy ∏*:
T-1
RT = X Ji((π1 ,...,πL1,πi,πi+1,...,πtN ))- JKnt)	⑵
t=0
The average regret is the average over learning iterations: (1/T)RiT. Now define the counterfactual
regret of the i-th player for taking action a at information set I:
(RiCF))T(I,a) = X1 (QCF,i(I,a) - X ∏t(a0∣I)Q∏F,i(I,a0))	⑶
t=0	a0∈A
=(RiCF))T-1(I,a)+ QnT-ι,i(I,a) - X ∏T-1 (a0∣I)Q∏Τ-1,。*	(4)
a0∈A
The counterfactual regret (Equation (3)) can be shown to majorize the regret (Equation (2)) (The-
orem 3, Zinkevich et al. (2007)). CFR can then be described as a learning algorithm where the
strategy is updated using regret matching (Hart & Mas-Colell, 2000) applied to the counterfactual
regret calculated in the most recent iteration:
∏T+i(a∣I )= / PaomUXRi∕+T 靠a0)) if Pa0∈A max(0, (RiCF))T "⑺) > °	(5)
ɪ 看	otherwise.
If all players follow the CFR regret matching strategy (Equation (5)), then at the T-th iteration the
players' average regrets are bounded by O(T -1/2) (Theorem 4, Zinkevich et al. (2007)).
3.2	CFR+
CFR+ (Tammelin, 2014) consists of a modification to CFR, in which instead of calculating the full
counterfactual regret as in (4), instead the counterfactual regret is recursively positively clipped to
yield the clipped counterfactual regret:
(Rig)T(I,a) = max(0,(RiCF+))T-1(I,a)) + QnT-/(/,a) - X ∏T-1(a0∣I)Q∏T』(").
a0∈A
(6)
Comparing Equation (4) with Equation (6), one can see that the only difference in CFR is that
the previous iteration's counterfactual regret is positively clipped in the recursion. The one-line
change of CFR+ turns out to yield a large practical improvement in the performance of the algorithm
(Bowling et al., 2015), and there is also an associated regret bound for CFR+ that is as strong as the
bound for CFR (Tammelin et al., 2015).
3
Under review as a conference paper at ICLR 2018
3.3 FROM CFR AND CFR+ TO ARM
CFR and CFR+ are formulated for imperfect information extensive-form games, so they are nat-
urally generalized to partially observed stochastic games since a stochastic game can always be
represented in extensive form. A 1-player partially observed stochastic game is simply a POMDP
with observation space O (Littman, 1994). By mapping information sets I ∈ I to observations
o ∈ O, we may rewrite the counterfactual value as a kind of stationary observation-action value
Qni (I, a) ≡ Qnto→a(o, a) that assumes the agent follows the policy ∏ except on observing o, after
which the action a is always performed (Bellemare et al., 2016). We posit that the approximation
Qnto→a(o, a) ≈ Q∏(o, a), where Q∏ is the usual action value function, is valid when observations
are rarely seen more than once in a trajectory. By approximating Qnto→Ο(o, a) ≈ Q∏(o, a), We get
a recurrence in terms of more familiar value functions (compare Equations (6) and (7)):
At (ok , ak ) = max(0, At-1 (ok , ak )) + Qnt (ok , ak ) -	πt (a |ok )Qnt (ok , a )	(7)
a0∈A
=max(0,A+-ι(θk,ak)) + Qnt (ok,a，k) - V∏t (ok)	(8)
= max(0, At-1 (ok , ak )) + Ant (ok , ak )	(9)
where A++ (o, a) is the cumulative clipped advantage function, and Ant (o, a) is the ordinary advan-
tage function evaluated at policy πt . Advantage-based regret minimization (ARM) is the resulting
reinforcement learning algorithm that updates the policy to regret match on the cumulative clipped
advantage function:
∏t+ι(ak [°k)
max(0,A+ (ok a))
Pa0∈A max(O,A+ (Ok,aO))
if Pao∈A max(0, A/+(ok,a0)) > 0
otherwise.
(10)
Equations (9) and (10) suggest the outline of a batch-mode deep reinforcement learning algorithm.
At the t-th sampling iteration, a batch of data is collected by sampling trajectories using the current
policy ∏t, followed by two processing steps: (a) fit A/+ using Equation (9), then (b) set the next
iteration’s policy πt+1 using Equation (10).
3.4 Implementation of ARM
To implement Equation (9) with deep function approximation, we define two value function approx-
imations, Vnt (ok； θt) and Q : ® ,ak; ωt), as well as a target value function V0(ok；夕)，where θt, ωt,
and 夕 are the learnable parameters. The cumulative clipped advantage function is represented as
A:(ok,ak) = Q ++(ok, ak; ωt) - Vnt (ok; θt). Within each sampling iteration, the value functions
are fitted using stochastic gradient descent by sampling minibatches and performing gradient steps.
The state-value function Vnt (ok; θt) is fit to minimize an n-step temporal difference loss with a
moving target V0(ok+n；夕)，essentially using the estimator of the deep deterministic policy gradient
(DDPG) (Lillicrap et al., 2016). In the same minibatch, Q + (ok,ak; θt) is fit to a similar loss, but
with an additional target reward bonus that incorporates the previous iteration’s cumulative clipped
advantage, max(0, J^f-ι(ok, ak)). The regression targets v(ok;夕)and q + (ok, ak;夕)are defined in
terms of the n-step returns gkn = Pkk0+=nk-1 γk0-krk0 :
V(Ok ； ψ) ,	gn + Y nV 0(ok+n； ψ)	(II)
q(ok, ak; ψ) ,	rk + Ygn-I + γnv0(ok+n； φ)	(12)
q+ (ok, ak； φ) ,	max(0, Q +-ι(ok, ak； ωt-i) - Vnt-I (ok； θt-i)) +	q(ok, ak； φ).	(13)
Altogether, each minibatch step of the optimization subproblem consists of the following three pa-
rameter updates in terms of the regression targets v(ok;夕)and q+(ok, ak；2)：
θ('+1) J θ(') - 2 Vθ(') (Kt (ok； θ(')) - v(ok； *))2	(14)
ω('+i) J ω(') - V (G(Q + (ok, ak； ω(')) - q + (ok, ak；2(')))2	(15)
2 ωt
d'+1) J 夕⑶+ T(θ('+1)-夕⑶).	(16)
4
Under review as a conference paper at ICLR 2018
Algorithm 1 Advantage-based regret minimization (ARM).
initialize ∏o J uniform, θ-1,ω-1 J arbitrary
for t in 0, . . . do
collect batch of trajectory data Dt 〜 ∏t
initialize θt J θt-ι, ωt J ωt-ι, φ J θt-i
for ` in 0, . . . do
sample transitions (ok ,ak,r,..., 0k+n-1, ak+n-ι ,rk+n-ι,θk+n)〜Dt
calculate n-step returns gkn = Pkk0+=nk-1 γk0-krk0
set δk+n J I[ok+n is terminal]
if t = 0 then
set φk J 0
else
set Φk J max(0,Qt-i(0k,ak； ωt-i) — V∏~] (ok； θt-i))
end if
set V(Ok) J gn + Yn(I — δk+n)V 0(ok+n； 0
set q+(θk,ak) J Φk + rk + Ygkn- + Yn(1 — δk+n)V0(ok+n； φ)
update θt with step size α and targets v(ok) (Equation (14))
update ωt with step size α and targets q+(θk ,ak) (Equation (15))
update 0 with moving average step size τ (Equation (16))
end for
set ∏t+ι(a∣o) H max(0, Q+ (o, a; ωt) — Vnt(o; θt))
end for
The overall advantage-based regret minimization algorithm is summarized in Algorithm 1.
We note that the mechanics of the ARM updates are similar to on-policy value function estimation,
but ARM learns a modified on-policy Q-function from transitions with the added reward bonus
max(0, ^4+-ι(θk ,ak)) (Equation (13)). This reward bonus can be thought of a kind of “optimism in
the face of uncertainty.”
3.5	ARM vs. existing policy gradient methods
In this section, we accentuate that ARM represents an inherently different update compared to ex-
isting policy gradient methods.
Recent work has shown that policy gradient methods and Q-learning methods are connected via
entropy regularization (O’Donoghue et al., 2017; Haarnoja et al., 2017; Nachum et al., 2017; Schul-
man et al., 2017; Anonymous, 2018). One perspective is from the soft policy iteration framework for
batch-mode reinforcement learning (Anonymous, 2018), where at each batch iteration the updated
policy is obtained by minimizing the average KL-divergence between the policy class Π and a target
policy f . Below is the soft policy iteration update, where the subscript t refers to the batch iteration:
∏t+ι J arg min Eo〜ρt [Dkl(∏∣∣/)]
π∈Π
=argmin Eo 〜ρt [Ea 〜∏(∙∣o) [log(∏(a∣o)) — log(f (α∣ο))]]∙
π∈Π
(17)
(18)
Using the connection between policy gradient methods and Q-learning, we define the policy gradient
target policy as the softmax distribution on the entropy regularized advantage function Aβ-soft:
fPG(a|o),
exp(βAβ-soft(ο, a))
Pao∈A exp(βAβ-soft(o,a0))
(19)
We note that it is more conventional in the literature to use the soft Q-function Qβ-soft(o, a) rather
than the soft advantage function Aβ-soft(o, a), however since they differ only by a function of o then
they both induce the same target softmax policy. Now, parameterizing the policy π in terms of
an explicit parameter θ, we obtain the expression for the existing policy gradient, where b(o) is a
baseline function:
∆θPG H Eo〜ρt [Ea〜∏(∙Mθ) [Vθ log(π(o∣a; θ))((1∕β)log(π(o∣a; θ)) — Arsoft(o, a) + b(o))]]. (20)
5
Under review as a conference paper at ICLR 2018
The classic policy gradient arises in the limit β → ∞.
Note that an alternative choice of target policy f will lead to a different kind of policy gradient
update. A policy gradient algorithm based on ARM instead proposes the following target policy
based on the regret-matching distribution:
fARM(a|o) ,
max(0, A+ (o, a))
Pa0∈A max(O,A+(o,aO))
(21)
Similarly, we can express the ARM-like policy gradient, where again b(o) is a baseline:
∆θARM = Eo〜°, [Ea〜∏(.gθ)[Vθ log(π(o∣a; θ))(log(π(o∣a; θ)) - log(max(0, A+ (o, a))) + b(o))]].
(22)
Comparing Equations (20) and (22), we see that the ARM-like policy gradient (Equation (22)) has
a logarithmic dependence on the advantage-like function A+, whereas the existing policy gradient
(Equation (20)) is only linearly dependent on the advantage function Aβ-soft. This difference in
logarithmic vs. linear dependence is responsible for a large part of the inherent distinction of ARM
from existing policy gradient methods. One consequence of the difference in logarithmic vs. linear
dependence is that the ARM-like update should be less sensitive to large positive advantages that
may result from overestimation compared to existing policy gradient methods.
We also see that for the existing policy gradient (Equation (20)), the (1∕β) log(∏(a∣o; θ)) term,
which is derived from the policy entropy, is vanishing for large β (e.g. β = 100 is a common choice
in practice). On the other hand, for the ARM-like policy gradient (Equation (22)), there is no similar
vanishing effect, suggesting that ARM may perform a kind of entropy regularization by default.
In practice we cannot implement an ARM-like policy gradient exactly as in Equation (22), as due
to the positive clipping max(0, A+) there can appear log(0). However We believe this is not an
intrinsic obstacle, leaving the issue of implementing an ARM-like policy gradient to future work.
3.6	Why does ARM work better in partially observable domains ?
In the previous Section 3.5, we showed that ARM and existing policy gradient methods can be
distinguished by their choices of target policy and the nature of their dependence on their respective
advantage-like functions. In this section, we argue that the convergence results of CFR and CFR+
suggest that ARM, to the degree that it inherits the properties of CFR/CFR+, ought to benefit from
greater partial observability compared to other methods.
We assume that regret bounds are a useful way to compare the convergence of different RL algo-
rithms, due to the interpretation of regret as “area over the learning curve (and under the optimal
expected value J")." Specifically, the regret bound of CFR and CFR+ is O(∣O∣√T) where |O| is
the size of the observation space (Zinkevich et al., 2007; Tammelin et al., 2015). The policy gradi-
ent method with a suitable baseline has a learning rate η-dependent regret bound derived from the
stochastic gradient method; assuming parameter norm bound B and gradient estimator second mo-
ments G2, by setting the learning rate η 8 T-1/2 policy gradient achieves a regret bound of O(√T)
with no explicit dependence on the observation space size |O| (Dick, 2015).
We argue that possessing a regret bound proportional to the observation space size |O| is beneficial
in highly partially observable domains. Let us fix an underlying state space S. Compare two RL
algorithms, where algorithm 1 (which is ARM-like) has a regret bound c1 |O| √T, whereas algorithm
2 (which is policy gradient-like) has a regret bound c2 √T; here, c1 and c2 are constants. Note that if
c1 |O| = c2 or equivalently |O| = c2∕c1, then the two RL algorithms possess the exact same regret
bound. If on the other hand |O| < c2∕c1, then the regret bound ofRL algorithm 1 is actually lower
than that of RL algorithm 2. Applying this intuition to CFR and hence ARM suggests that ARM can
benefit from greater partial observability if the degree of partial observability is above a threshold.
For Q-learning per se, we are not aware of any known regret bound. Szepesvari proved that the
convergence rate of Q-learning in the L∞-norm, assuming a fixed exploration strategy, depends
on a condition number C, which is the ratio of the minimum to maximum state-action occupation
frequencies (Szepesvari, 1998), and which describes how “balanced” the exploration strategy is. If
partial observability leads to imbalanced exploration due to confounding of states from perceptual
aliasing (McCallum, 1997), then Q-learning should be negatively affected.
6
Under review as a conference paper at ICLR 2018
We note that there remains a gap between ARM as implemented and the theory of CFR: the use of
(a) function approximation and sampling over tabular enumeration; (b) the “ordinary” Q-function
instead of the “stationary” Q-function; and (c) n-step bootstrapped values instead of full returns for
value function estimation. Waugh et al. (2015) address CFR with function approximation via a noisy
version of a generalized Blackwell’s condition (Cesa-Bianchi & Lugosi, 2003). Even the original
implementation of CFR used sampling in place of enumeration (Zinkevich et al., 2007). We refer
the reader to Bellemare et al. (2016) for a more in-depth discussion of the stationary Q-function.
Although only the full returns are guaranteed to be unbiased in non-Markovian settings, it is quite
common for practical RL algorithms to trade off strict unbiasedness in favor of lower variance by
using n-step returns or variations thereof (Schulman et al., 2016; Gu et al., 2017).
4	Experiments
Because we hypothesize that ARM should perform well in partially observable reinforcement learn-
ing environments, we conduct our experiments on visual domains that naturally provide partial ob-
servations of state. All of our evaluations use feedforward convnets with frame-history observations.
We are interested in comparing ARM with methods that assume Markovian observations, namely
double deep Q-learning (van Hasselt et al., 2016), as well as methods that can handle non-Markovian
observations, primarily TRPO (Schulman et al., 2015; 2016), and to a lesser extent A3C (Mnih et al.,
2016) whose critic assumes Markovian observations. We are also interested in controlling for the
advantage structure of ARM by comparing with other advantage-structured methods, which include
dueling networks (Wang et al., 2016), as well as policy gradient methods that estimate an empirical
advantage using a baseline state-value function or critic (e.g. TRPO, A3C).
4.1	Learning to play Pong with a single frame
Atari games consist of a small set of moving sprites with fixed shapes and palettes, and the motion
of sprites can be highly deterministic, so that with only 4 recently observed frames as input one
can predict hundreds of frames into the future on some games using only a feedforward model (Oh
et al., 2015). To increase the partial observability of Atari games, one may artificially limit the
amount of frame history fed as input to the networks (Hausknecht & Stone, 2017). As a proof of
concept of ARM, we trained agents to play Pong via the Arcade Learning Environment (Bellemare
et al., 2013) when the frame-history length is varied between 4 (the default) and 1. We found that
the performance of double deep Q-learning degraded noticeably when the frame-history length was
reduced from 4 to 1, whereas performance of ARM was not affected nearly as much. Our results on
Pong are summarized in Figure 1.
Pong (#frames=4)
u」na」① po-d ① CSE
O O O O o_
2 1 1 2
0.5 ι.c
sim steps le7
Pong (#frames=l)
Oooo Oo
2 1 1 2
- -
En」① pod ① CSE
0.5 ι.c
sim steps le7
Figure 1: Comparing double deep Q-learning (orange) and ARM (blue) on Pong.
4.2	Learning to navigate in ViZDoom
We evaluated ARM on the task of learning first-person navigation in the ViZDoom (Kempka et al.,
2016) domain based on the game of Doom. Doom is a substantially more complex domain than
Atari, featuring an egocentric viewpoint, 3D perspective, and complex visuals. We expect that
Doom exhibits a substantial degree of partial observability and therefore serves as a more difficult
evaluation of reinforcement learning algorithms’ effectiveness on partially observable domains. We
performed our evaluation on two standard ViZDoom navigation benchmarks, “HealthGathering”
7
Under review as a conference paper at ICLR 2018
and “MyWayHome.” In “HealthGathering,” the agent is placed in a toxic room and continually
loses life points, but can navigate toward healthkit objects to prolong its life; the goal is to survive
for as long as possible. In “MyWayHome,” the agent is randomly placed in a small maze and must
find a target object that has a fixed visual appearance and is in a fixed location in the maze; the goal
is to reach the target object before time runs out. Figure 2 (top row) shows example observations
from the two ViZDoom scenarios.
Unlike previous evaluations which augmented the raw pixel frames with extra information about
the game state, e.g. elapsed time ticks or remaining health (Kempka et al., 2016; Dosovitskiy &
Koltun, 2017), in our evaluation we forced all networks to learn using only visual input. Despite this
restriction, ARM is still able to quickly learn policies with minimal tuning of hyperparameters and
reach close to the maximum score in under 1 million steps. On “HealthGathering,” we observed that
ARM very quickly learns a policy that can achieve close to the maximum episode return of 2100.
Double deep Q-learning learns a more consistent policy on “HealthGathering” compared to ARM
and TRPO, but we believe this to be the result of evaluating double DQN’s -greedy policy with
small compared to the truly stochastic policies learned by ARM and TRPO. On “MyWayHome,”
we observed that ARM generally learned a well-performing policy more quickly than other methods.
Additionally, we found that ARM is able to take advantage of an off-policy replay memory when
learning on ViZDoom by storing the trajectories of previous sampling batches and applying an
importance sampling correction to the n-step returns; please see Section 6.2 in the Appendix for
details. Our Doom results are in Figure 6.
Figure 2: Top row: Doom screenshots from (left) “HealthGathering” and (right) “MyWayHome.”
Bottom row: Minecraft screenshots from (leftmost) “L1” through (rightmost) “L5”
DoomHeaIthGathenng
2000
1500
1000
O
O
5
出①POS-de croφE
0.5	1.0
sin∩ steps le6
-0.2
0
DoomiviyWayHome
1.0
-8-6-42∙0
Ooooo
出①POS-de croφE
1	2
sin∩ steps le6
Figure 3: Evaluating double deep Q-learning (orange), dueling double DQN (red), A3C (purple),
TRPO (green), ARM (blue), and ARM with off-policy data (cyan) on two ViZDoom scenarios.
8
Under review as a conference paper at ICLR 2018
4.3	Learning to navigate in Minecraft
We finally evaluated ARM on the task of learning first-person navigation in the Malmo domain
based on the game of Minecraft (Johnson et al., 2016). Minecraft has similar visual complexity
to Doom and should possess a comparable degree of partial observability, but Minecraft has the
potential to be more difficult than Doom due to the diversity of possible Minecraft environments
that can be generated. Our evaluation on Minecraft is adapted from the teacher-student curriculum
learning protocol (Matiisen et al., 2017), which consists of 5 consecutive “levels” that successively
increase the difficulty of completing the simple task of reaching a target block: the first level (“L1”)
consists of a single room; the intermediate levels (“L2”-“L4”) consist of a corridor with lava-bridge
and wall-gap obstacles; and the final level (“L5”) consists ofa 2 × 2 arrangement of rooms randomly
separated by lava-bridge or wall-gap obstacles. Figure 2 (bottom row) shows example observations
from the five Minecraft levels.
We performed our Minecraft experiments using fixed curriculum learning schedules to evaluate the
sample efficiency of different algorithms: the agent is initially placed in the first level (“L1”), and the
agent is advanced to the next level whenever a preselected number of simulator steps have elapsed,
until the agent reaches the last level (“L5”). We found that ARM and dueling double DQN both
were able to learn on an aggressive “fast” schedule of only 62500 simulator steps between levels.
TRPO required a “slow” schedule of 93750 simulator steps between levels to reliably learn. ARM
was able to consistently learn a well performing policy on all of the levels, whereas double DQN
learned more slowly on some of the intermediate levels. ARM also more consistently reached a high
score on the final, most difficult level (“L5”). Our Minecraft results are shown in Figure 4.
U」n4aj 3po-d3 ul°3E
-500
-1000
Ooo
O O
O 5
U」n4aj 3po-d3 ul°3E
Figure 4: Evaluating double deep Q-learning (orange), dueling double DQN (red), TRPO (green),
and ARM (blue) on a Minecraft curriculum learning protocol. The simulator step counts at which
each level begins are labeled and demarcated with dashed vertical lines.
5	Discussion
In this paper, we presented a novel deep reinforcement learning algorithm based on counterfactual
regret minimization (CFR). We call our method advantage-based regret minimization (ARM). Simi-
larly to prior methods that learn state or state-action value functions, our method learns a cumulative
clipped advantage function of observation and action. However, in contrast to these prior methods,
ARM is well suited to partially observed or non-Markovian environments, making it an appeal-
ing choice in a number of difficult domains. When compared to baseline methods, including deep
Q-learning and TRPO, on non-Markovian tasks such as the challenging ViZDoom and Malmo first-
person navigation benchmarks, ARM achieves substantially better results. This illustrates the value
of ARM for partially observable problems. In future work, we plan to further explore applications
of ARM to more complex tasks, including continuous action spaces.
References
Anonymous. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a
stochastic actor. International Conference on Learning Representations, 2018. URL https:
//openreview.net/forum?id=HJjvxl- Cb.
9
Under review as a conference paper at ICLR 2018
Oron Anschel, Nir Baram, and Nahum Shimkin. Averaged-DQN: Variance Reduction and Stabi-
lization for Deep Reinforcement Learning. In International Conference on Machine Learning,
pp.176-185, 2017.
Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The Arcade Learning Envi-
ronment: An evaluation platform for general agents. Journal of Artificial Intelligence Research,
47:253-279, 2013.
Mark G. Bellemare, Georg Ostrovski, Arthur Guez, Philip S. Thomas, and Remi Munos. Increasing
the Action Gap: New Operators for Reinforcement Learning. In AAAI, 2016.
Michael Bowling, Neil Burch, Michael Johanson, and Oskari Tammelin. Heads-up limit hold’em
poker is solved. Science, 347(6218):145-149, 2015.
Nicolo Cesa-Bianchi and Gabor Lugosi. Potential-Based Algorithms in On-Line Prediction and
Game Theory. Machine Learning, 51(3):239-261, 2003.
Travis Dick. Policy Gradient Reinforcement Learning Without Regret. Master’s thesis, University
of Alberta, 2015.
Alexey Dosovitskiy and Vladlen Koltun. Learning to Act by Predicting the Future. arXiv preprint
arXiv:1611.01779v2, 2017.
Jakob N. Foerster, Yannis M. Assael, Nando de Freitas, and Shimon Whiteson. Learning to Com-
municate with Deep Multi-Agent Reinforcement Learning. In Advances in Neural Information
Processing Systems 29, 2016.
Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard E. Turner, Bernhard Scholkopf, and
Sergey Levine. Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Esti-
mation for Deep Reinforcement Learning. arXiv preprint arXiv:1706.00387v1, 2017.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement Learning with
Deep Energy-Based Policies. arXiv preprint arXiv:1702.08165v2, 2017.
Sergiu Hart and Andreu Mas-Colell. A Simple Adaptive Procedure Leading to Correlated Equilib-
rium. Econometrica, 68(5):1127-1150, 2000.
Matthew Hausknecht and Peter Stone. Deep Recurrent Q-Learning for Partially Observable MDPs.
arXiv preprint arXiv:1507.06527v4, 2017.
Nicolas Heess, Jonathan J. Hunt, Timothy P. Lillicrap, and David Silver. Memory-based control
with recurrent neural networks. arXiv preprint arXiv:1512.04455v1, 2015.
Edward L Ionides. Truncated Importance Sampling. Journal of Computational and Graphical
Statistics, 17(2):295-311, 2008.
Matthew Johnson, Katja Hofmann, Tim Hutton, and David Bignell. The Malmo Platform for Arti-
ficial Intelligence Experimentation. In Proceedings of the 25th International Joint Conference on
Artificial Intelligence, 2016.
Michal Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech ja´kowski. ViZ-
Doom: A Doom-based AI Research Platform for Visual Reinforcement Learning. arXiv preprint
arXiv:1605.02097v2, 2016.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuo-
motor policies. The Journal of Machine Learning Research, 17(1):1334-1373, 2016.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971v5, 2016.
Michael L. Littman. Markov games as a framework for multi-agent reinforcement learning. In
Proceedings of the 11th International Conference on Machine Learning, pp. 157-163, 1994.
10
Under review as a conference paper at ICLR 2018
Tambet Matiisen, Avital Oliver, Taco Cohen, and John Schulman. Teacher-Student Curriculum
Learning. arXiv preprint arXiv:1707.00183v1, 2017.
Andrew Kachites McCallum. Efficient Exploration in Reinforcement Learning with Hidden State.
In AAAI Fall Symposium on Model-directed Autonomous Systems, 1997.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. Playing Atari with Deep Reinforcement Learning. arXiv preprint
arXiv:1312.5602v1, 2013.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International Conference on Machine Learning, pp. 1928-1937, 2016.
Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the Gap Between
Value and Policy Based Reinforcement Learning. In 31st Conference on Neural Information
Processing Systems, 2017.
Brendan O'Donoghue, Remi Munos, Koray KavUkcUoglu, and Volodymyr Mnih. Combining policy
gradient and Q-learning. arXiv preprint arXiv:1611.01626v3, 2017.
Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L Lewis, and Satinder Singh. Action-Conditional
Video Prediction using Deep Networks in Atari Games. In Advances in Neural Information Pro-
cessing Systems, pp. 2863-2871, 2015.
Junhyuk Oh, Valliappa Chockalingam, Satinder Singh, and Honglak Lee. Control of memory, active
perception, and action in minecraft. In Proceedings of The 33rd International Conference on
Machine Learning, pp. 2790-2799, 2016.
Xue Bin Peng, Glen Berseth, and Michiel van de Penne. Terrain-Adaptive Locomotion Skills Using
Deep Reinforcement Learning. ACM Transactions on Graphics, 35(4):81, 2016.
Stephane Ross and J. Andrew Bagnell. Reinforcement and Imitation Learning via Interactive No-
Regret Learning. arXiv preprint arXiv:1406.5979v1, 2014.
Stephane Ross, Geoffrey J. Gordon, and J. Andrew Bagnell. A Reduction of Imitation Learning and
Structured Prediction to No-Regret Online Learning. In Proceedings of the Fourteenth Interna-
tional Conference on Artificial Intelligence and Statistics, pp. 627-635, 2011.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In Proceedings of the 32nd International Conference on Machine Learning
(ICML-15), pp. 1889-1897, 2015.
John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and Pieter Abbeel. High-
Dimensional Continuous Control Using Generalized Advantage Estimation. arXiv preprint
arXiv:1506.02438v5, 2016.
John Schulman, Xi Chen, and Pieter Abbeel. Equivalence Between Policy Gradients and Soft Q-
Learning. arXiv preprint arXiv:1704.06440v1, 2017.
Csaba Szepesvari. The Asymptotic Convergence-Rate of Q-learning. In Advances in Neural Infor-
mation Processing Systems, pp. 1064-1070, 1998.
Oskari Tammelin. Solving Large Imperfect Information Games Using CFR+. arXiv preprint
arXiv:1407.5042v1, 2014.
Oskari Tammelin, Neil Burch, Michael Johanson, and Michael Bowling. Solving Heads-up Limit
Texas Hold’em. In Proceedings of the 24th International Joint Conference on Artificial Intelli-
gence, 2015.
11
Under review as a conference paper at ICLR 2018
Hado van Hasselt and Marco A. Wiering. Reinforcement Learning in Continuous Action Spaces.
In Proceedings of the 2007 IEEE Symposium on Approximate Dynamic Programming and Rein-
forcement Learning,pp. 272-279, 2007.
Hado van Hasselt, Arthur Guez, and David Silver. Deep Reinforcement Learning and Double Q-
Learning. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, 2016.
Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, and Nando de Freitas.
Dueling Network Architectures for Deep Reinforcement Learning. In Proceedings of the 33rd
International Conference on Machine Learning, pp. 1995-2003, 2016.
Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu,
and Nando de Freitas. Sample Efficient Actor-Critic with Experience Replay. arXiv preprint
arXiv:1611.01224v2, 2017.
Kevin Waugh, Dustin Morrill, J. Andrew Bagnell, and Michael Bowling. Solving Games with
Functional Regret Estimation. In Workshops at the Twenty-Ninth AAAI Conference on Artificial
Intelligence, 2015. Supplementary material in arXiv preprint arXiv:1411.7974v2.
Martin Zinkevich, Michael Johanson, Michael H. Bowling, and Carmelo Piccione. Regret Mini-
mization in Games with Incomplete Information. In Advances in Neural Information Processing
Systems 20, pp. 1729-1736, 2007.
12
Under review as a conference paper at ICLR 2018
6	Appendix
6.1	Experimental details
6.1.1	Pong (Arcade Learning Environment)
We use the preprocessing and convolutional network model of (Mnih et al., 2013). Specifically, we
view every 4th emulator frame, convert the raw frames to grayscale, and perform downsampling to
generate a single observed frame. The input observation of the convnet is a concatenation of the
most recent frames (either 4 frames or 1 frame). The convnet consists of an 8 × 8 convolution with
stride 4 and 16 filters followed by ReLU, a 4 × 4 convolution with stride 2 and 32 filters followed
by ReLU, a linear map with 256 filters followed by ReLU, and a linear map with |A| filters where
|A| is the action space cardinality (|A| = 6 for Pong).
We used Adam with a constant learning rate of α = 10-4, a minibatch size of 32, and the moment
decay rates set to their defaults β1 = 0.9 and β2 = 0.999. Our results on each method are averaged
across 3 random seeds.
We ran ARM with the hyperparameters: sampling batch size of 12500, 4000/3000 minibatches of
Adam for the first/subsequent sampling iterations respectively, and target update step size τ = 0.01.
Double DQN uses the tuned hyperparameters (van Hasselt et al., 2016). Note that our choice of
ARM hyperparameters yields an equivalent number of minibatch gradient updates per sample as
used by DQN and double DQN, i.e. 1 minibatch gradient update per 4 simulator steps.
6.1.2	Doom (ViZDoom)
We used a convolutional network architecture similar to those of (Kempka et al., 2016) and (Doso-
vitskiy & Koltun, 2017). The Doom screen was rendered at a resolution of 160 × 120 and downsized
to 84 × 84. Only every 4th frame was rendered, and the input observation to the convnet is a con-
catenation of the last 4 rendered RGB frames for a total of 12 input channels. The convnet contains
3 convolutions with 32 filters each: the first is size 8 × 8 with stride 4, the second is size 4 × 4 with
stride 2, and the third is size 3 × 3 with stride 1. The final convolution is followed by a linear map
with 1024 filters. A second linear map yields the output. Hidden activations are gated by ReLUs.
For “HealthGathering” only, we scaled rewards by a factor of 0.01. We did not scale rewards for
“MyWayHome.” We used Adam with a constant learning rate of α = 10-5 and a minibatch size
of 32 to train all networks (except TRPO). For “HealthGathering” we set β1 = 0.95, whereas for
“MyWayHome” we set β1 = 0.9. We set β2 = 0.999 for both scenarios. Our results on each method
are averaged across 3 random seeds.
Double DQN and dueling double DQN: n = 5 step returns; update interval 30000; 1 minibatch gra-
dient update per 4 simulator steps; replay memory uniform initialization size 50000; replay memory
maximum size 240000; exploration period 240000; with final exploration rate = 0.01.
A3C: 16 workers; n = 20 steps for “HealthGathering” and n = 40 steps for “MyWayHome”;
negentropy regularization β = 0.01; and gradient norm clip 5.
TRPO: sampling batch size 12500; KL-divergence step size δ = 0.01; 10 conjugate gradient itera-
tions; and Fisher information/Gauss-Newton damping coefficient λ = 0.1.
ARM: n = 5 step returns; sampling batch size 12500; 4000 Adam minibatches in the first sampling
iteration, 3000 Adam minibatches in all subsequent sampling iterations; target update step size
τ = 0.01. Again, our choice of ARM hyperparameters yields an equivalent number of minibatch
gradient updates per sample as used by DQN and double DQN. For “HealthGathering” only, because
ARM converges so quickly we annealed the Adam learning rate to α = 2.5 × 10-6 after 500000
elapsed simulator steps.
Off-policy ARM: n = 5 step returns; sampling batch size 1563, replay cache sample size 25000;
400 Adam minibatches per sampling iteration; target update step size τ = 0.01; and importance
sampling weight clip c = 1.
13
Under review as a conference paper at ICLR 2018
6.1.3	MINECRAFT (MALMO)
Our Minecraft tasks generally were the same as the ones used by Matiisen et al. (2017), with a few
differences. Instead of using a continuous action space, we used a discrete action space with 4 move
and turn actions. To aid learning on the last level (“L5”), we removed the reward penalty upon
episode timeout and we increased the timeout on “L5” from 45 seconds to 75 seconds due to the
larger size of the environment. We scaled rewards for all levels by 0.001.
We use the same convolutional network architecture for Minecraft as we used for ViZDoom in
Section 4.2. The Minecraft screen was rendered at a resolution of 320 × 240 and downsized to 84 ×
84. Only every 5th frame was rendered, and the input observation of the convnet is a concatenation
of the last 4 rendered RGB frames for a total of 12 input channels. We used Adam with constant
learning rate α = 10-5, moment decay rates β1 = 0.9 and β2 = 0.999, and minibatch size 32 to
train all networks (except TRPO). Our results on each method are averaged across 5 random seeds.
Double DQN and dueling double DQN: n = 5 step returns; update interval 12500; 1 minibatch gra-
dient update per 4 simulator steps; replay memory uniform initialization size 12500; replay memory
maximum size 62500; exploration period 62500; with final exploration rate = 0.01.
TRPO: sampling batch size 6250; KL-divergence step size δ = 0.01; 10 conjugate gradient itera-
tions; and Fisher information/Gauss-Newton damping coefficient λ = 0.1.
ARM: n = 5 step returns; sampling batch size 12500; 4000 Adam minibatches in the first sampling
iteration, 3000 Adam minibatches in all subsequent sampling iterations; target update step size
τ = 0.01.
6.2	Off-p olicy ARM via imp ortance sampling
Our current approach to running ARM with off-policy data consists of applying an importance
sampling correction directly to the n-step returns. Given the behavior policy μ under which the data
was sampled, the current policy πt under which we want to perform estimation, and an importance
sampling weight clip c for variance reduction, the corrected n-step return we use is:
k+n-1	k0
gn(μkπt) = X γk'-k I Y wμk∏t(a'lo') I rk0
k0 = k	∖'=k	)
where the truncated importance weight Wμ∣g (a|o) is defined (Ionides, 2008):
π I ʌ	π πt ⑷0八
wμk∏t ⑷。) = min 卜许＞
(23)
(24)
Our choice of c = 1 in our experiments was inspired by Wang et al. (2017). We found that c = 1
worked well but note other choices for c may also be reasonable.
When applying our importance sampling correction, we preserve all details of the ARM algorithm
except for two aspects: the transition sampling strategy (a finite memory of previous batches are
cached and uniformly sampled) and the regression targets for learning the value functions. Specif-
ically, the regression targets v(ok;夕)，q(ok,a，k；夕)，and q+(θk,ak；夕)(Equations (11)-(13)) are
modified to the following:
vμk∏t (Ok ； 6= gn(μkπt) + YnVO(Ok+n；夕)	(25)
qμk∏t (Ok,ak；2)=Irk + YWμk∏t(ak |ok )gn-1(μkπt) + γ nV 0(ok+n; φ)	(26)
KE (Ok ,ak ； ψ) = max(0,Q+-i (Ok,ak ； ωt-1) - V∏t-ι (Ok ； θt-1)) + 9μk∏t(ok ,ak； φ). (27)
Note that the target value function V0(Ok+n；夕)does not require an importance sampling correction
because V0 already approximates the on-policy value function Vπt (Ok+n； θt).
6.3 Additi onal experiments
6.3.1	Atari 2600 games
Although our primary interest is in partially observable reinforcement learning domains， we also
want to check that ARM works in nearly fully observable and Markovian environments， such as
14
Under review as a conference paper at ICLR 2018
Atari 2600 games. We consider two baselines: double deep Q-learning, and double deep fitted Q-
iteration which is a batch counterpart to double DQN. We find that double deep Q-learning is a
strong baseline for learning to play Atari games, although ARM still successfully learns interesting
policies. One major benefit of Q-learning-based methods is the ability to utilize a large off-policy
replay memory. Our results on a suite of Atari games are in Figure 5.
Beam Rider	Breakout
UJn-j3J BPO-dB CmVE
En-j3J BPO-dB CmVE
0O
IOOOO
8000
6000
4000
2000
Ooo
3 2 1
UJn3J BPOdB CmVE
6 6 6 6°
2 1 1 2
- -
UJaJ BPOMdB CmVE
steps ie7
Enl9J BDO-dB CmVE
DDQN
——DDFQI
ARM
Figure 5: Comparing double deep Q-learning (orange), double deep fitted Q-iteration (red), and
ARM (blue) on a suite of seven Atari games from the Arcade Learning Environment. For each
method, we plot the mean across 3 trials along with standard error bars.
6.3.2	Recurrence in Doom MyWayHome
We evaluated the effect of recurrent policy and value function estimation in the maze-like MyWay-
Home scenario of ViZDoom. We found that recurrence has a small positive effect on the convergence
of A2C (Mnih et al., 2016), but was much less significant than the choice of algorithm. Our hyper-
parameters were similar to those described for A3C in Section 6.1.2, except we used a learning rate
10-4 and gradient norm clip 0.5. For the recurrent policy and value function, we replaced the first
fully connected operation with an LSTM featuring an equivalent number of hidden units (1024).
1.0
o.o
Doom MyWayHome
8 6 4
000
u」b£ ① pos-d① Ue① IU
1	2
sim steps le6
Figure 6: Comparing A2C with a feedforward convolutional network (blue) and a recurrent
convolutional-LSTM network (orange) on the ViZDoom scenario MyWayHome.
15