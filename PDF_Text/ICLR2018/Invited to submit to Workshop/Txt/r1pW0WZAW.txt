Under review as a conference paper at ICLR 2018
Analyzing and Exploiting NARX
Recurrent Neural Networks
for Long-Term Dependencies
Anonymous authors
Paper under double-blind review
Ab stract
Recurrent neural networks (RNNs) have achieved state-of-the-art performance on
many diverse tasks, from machine translation to surgical activity recognition, yet
training RNNs to capture long-term dependencies remains difficult. To date, the
vast majority of successful RNN architectures alleviate this problem using nearly-
additive connections between states, as introduced by long short-term memory
(LSTM). We take an orthogonal approach and introduce MIST RNNs, a NARX
RNN architecture that allows direct connections from the very distant past. We
show that MIST RNNs 1) exhibit superior vanishing-gradient properties in com-
parison to LSTM and previously-proposed NARX RNNs; 2) are far more efficient
than previously-proposed NARX RNN architectures, requiring even fewer com-
putations than LSTM; and 3) improve performance substantially over LSTM and
Clockwork RNNs on tasks requiring very long-term dependencies.
1	Introduction
Recurrent neural networks (Rumelhart et al., 1986; Werbos, 1988; Williams & Zipser, 1989) are
a powerful class of neural networks that are naturally suited to modeling sequential data. For ex-
ample, in recent years alone, RNNs have achieved state-of-the-art performance on tasks as diverse
as machine translation (Wu et al., 2016), speech recognition (Miao et al., 2015), generative image
modeling (Oord et al., 2016), and surgical activity recognition (DiPietro et al., 2016).
These successes, and the vast majority of other RNN successes, rely on a mechanism introduced by
long short-term memory (Hochreiter & Schmidhuber, 1997; Gers et al., 2000), which was designed
to alleviate the so called vanishing gradient problem (Hochreiter, 1991; Bengio et al., 1994). The
problem is that gradient contributions from events at time t - τ to a loss at time t diminish exponen-
tially fast with τ, thus making it extremely difficult to learn from distant events (see Figures 1 and 2).
LSTM alleviates the problem using nearly-additive connections between adjacent states, which help
push the base of the exponential decay toward 1. However LSTM in no way solves the problem, and
in many cases still fails to learn long-term dependencies (see, e.g., (Arjovsky et al., 2016)).
NARX1 RNNs (Lin et al., 1996) offer an orthogonal mechanism for dealing with the vanishing gra-
dient problem, by allowing direct connections, or delays, from the distant past. However NARX
RNNs have received much less attention in literature than LSTM, which we believe is for two rea-
sons. First, as previously introduced, NARX RNNs have only a small effect on vanishing gradients,
as they reduce the exponent of the decay by only a factor of nd, the number of delays. Second, as
previously introduced, NARX RNNs are extremely inefficient, as both parameter counts and com-
putation counts grow by the same factor nd.
In this paper, we introduce MIxed hiSTory RNNs (MIST RNNs), a new NARX RNN architecture
which 1) exhibits superior vanishing-gradient properties in comparison to LSTM and previously-
proposed NARX RNNs; 2) improves performance substantially over LSTM on tasks requiring very
long-term dependencies; and 3) remains efficient in parameters and computation, requiring even
fewer than LSTM for a fixed number of hidden units. Importantly, MIST RNNs reduce the decay’s
exponent by a factor of 2nd-1; see Figure 2.
1The acronym NARX stems from Nonlinear AutoRegressive models with eXogeneous inputs.
1
Under review as a conference paper at ICLR 2018
O O
Z--. z--X z--X z-ʌ /~''、	/、	，--X z--X z-X ，--X z--X z-ʌ /~'、	，、	-->
OOOOOOOOOOOOOOOOO
(a)	Simple RNNs
›--：
OO
r∖ r\r\r\r\r\r\r\r\r\r\r\r\r\ r∖
OOOOOOOOOOOOOOOOO
(b)	LSTM
6 d'o
ʃ~~L~X'X /-X /ʌ 尸-S
OOOOOOOOOOOOOOOOO
(c)	Simple NARX RNNs (n = 2)
11	..,∙∙∙.[5≥⅝s,
d o o o o o o o d^ o o o d' o 6' b' o
OOOOOOOOOOOOOOOOO
(d)	NARX RNNs With exponential delays (nd = 5)
Figure 1: Direct connections (dashed) to a single time step t and example shortest paths (solid) from
time t 一 T to time t for various architectures. Typical RNN connections (blue) impede gradient flow
through matrix multiplications and nonlinearities. LSTM facilitates gradient flow through additional
paths between adjacent time steps with less resistance (orange). NARX RNNs facilitate gradient
flow through additional paths that span multiple time steps.
2	Background and Related Work
Recurrent neural networks, as commonly described in literature, take on the general form
ht = f (ht-1, xt, θ)	(1)
which compute a new state ht in terms of the previous state ht-1, the current input xt, and some
parameters θ (which are shared over time).
one of the earliest variants, now known to be especially vulnerable to the vanishing gradient prob-
lem, is that of simple RNNs (Elman, 1990), described by
ht = tanh(Whht-1 + Wxxt + b)	(2)
In this equation and elsewhere in this paper, all weight matrices W and biases b collectively form
the parameters θ to be learned, and tanh is always written explicitly2.
Long short-term memory (Hochreiter & Schmidhuber, 1997; Gers et al., 2000), the most widely-
used RNN architecture to date, was specifically introduced to address the vanishing gradient prob-
lem. The term LSTM is often overloaded; we refer to the variant with forget gates and without
peephole connections, which performs similarly to more complex variants (Greff et al., 2016):
ft =	σ(Wfhht-1 + Wfxxt + bf)	(3)
it =	σ(Wihht-1 + Wixxt + bi)	(4)
ot =	σ(Wohht-1 + Woxxt + bo)	(5)
Gt =	tanh(Wchht-I + WcxXt + b0)	(6)
Ct =	ft Θ ct-ι + it Θ Ct	(7)
ht =	ot tanh(ct)	(8)
Here σ(∙) denotes the element-wise sigmoid function and Θ denotes element-wise multiplication. f t,
it, and ot are referred as the forget, input, and output gates, which can be interpreted as controlling
how much we reset, write to, and read from the memory cell Ct. LSTM has better gradient properties
than simple RNNs (see Figure 2) because of the mechanism in Equation 7, which introduces a path
between Ct-1 and Ct which is modulated only by the forget gate. We also remark that gated recurrent
units (Cho et al., 2014) alleviate the vanishing gradient problem using this exact same idea.
NARX RNNs (Lin et al., 1996) also address the vanishing gradient problem, but using a mechanism
that is orthogonal to (and possibly complementary to) that of LSTM. This is done by allowing delays,
or direct connections from the past. NARX RNNs in their general form are described by
ht = f(ht-1,ht-2,.. .,Xt,Xt-1,...,θ)	(9)
2tanh is a common choice, but it is of course also possible to use other activation functions.
2
Under review as a conference paper at ICLR 2018
Figure 2: Gradient norms ∣∣ ∂h+lt k averaged over a batch of examples during permuted MNIST
∂ ht-τ
training. Unlike Clockwork RNNs and MIST RNNs, simple RNNs and LSTM capture essentially
no learning signal from inputs that are far from the loss.
but literature typically assumes the specific variant explored in (Lin et al., 1996),
ht = tanh	Xnd Wdht-d + Wxxt +b	(10)
which we refer to as simple NARX RNNs.
Note that simple NARX RNNs require approximately nd as much computation and nd as many
parameters as their simple-RNN counterpart (with nd = 1), which greatly hinders their applicability
in practice. To our knowledge, this drawback holds for all NARX RNN variants before MIST
RNNs. For example, in (Soltani & Jiang, 2016), higher-order recurrent neural networks (HORNNs)
are defined precisely as simple NARX RNNs, and every variant in the paper suffers from this exact
same problem. And, in (Zhang et al., 2016), a simple NARX RNN architecture is defined that is
limited to having precisely two delays with non-zero weights. This way, at the expense of having
fewer, longer paths to the past, parameter and computation counts are only doubled.
The previous work that is most similar to ours is that of Clockwork RNNs (Koutnik et al., 2014),
which split weights and hidden units into partitions, each with a distinct period. When it’s not a
partition’s time to tick, its hidden units are passed through unchanged, and so Clockwork RNNs
in some ways mimic NARX RNNs. However Clockwork RNNs differ in two key ways. First,
Clockwork RNNs sever high-frequency-to-low-frequency paths, thus making it difficult to learn
long-term behavior that must be detected at high frequency (for example, learning to depend on
quick motions from the past for activity recognition). Second, Clockwork RNNs require hidden
units to be partitioned a priori, which in practice is difficult to do in any meaningful way. NARX
RNNs (and in particular MIST RNNs) suffer from neither of these drawbacks.
Many other approaches have also been proposed to capture long-term dependencies. Notable ap-
proaches include maintaining a generative model over inputs and learning to process only unex-
pected inputs (Schmidhuber, 1992), operating explicitly at multiple time scales (El Hihi & Bengio,
1995), Hessian-free optimization (Martens & Sutskever, 2011), using associative or explicit mem-
ory (Plate, 1993; Danihelka et al., 2016; Graves et al., 2014; Weston et al., 2015), and initializing or
restricting weight matrices to be orthogonal (Arjovsky et al., 2016; Henaff et al., 2016).
3	The Vanishing Gradient Problem in the Context of NARX RNNs
In (Bengio et al., 1994; Pascanu et al., 2013), gradient decompositions and sufficient conditions for
vanishing gradients are presented for simple RNNs, which contain one path between times t - τ and
t. Here, we use the chain rule for ordered derivatives (Werbos, 1990) to connect gradient compo-
nents to paths and edges, which in turn provides a simple extension of the results from (Bengio et al.,
1994; Pascanu et al., 2013) to general NARX RNNs. We remark that we rely on slightly overloaded
notation for clarity, as otherwise notation becomes cumbersome (see (Werbos, 1989)).
We begin by disambiguating notation, as the symbol ∂f is routinely overloaded in literature. Con-
sider the Jacobian of f (x, U(X)) with respect to x. We let ∂f denote fx), a collection of full
3
Under review as a conference paper at ICLR 2018
derivatives, and We let ∂f denote df∂X,u), a collection of partial derivatives. This lets Us write the
ordinary chain rule as d+f = ⅞f d+x + ⅞f ⅞u. Note that this notation is consistent with (Werbos,
∂x ∂x ∂x ∂u ∂x
1989; 1990), but is the exact opposite of the convention used in (Pascanu et al., 2013).
3.1	The Chain Rule for Ordered Derivatives
Consider an ordered system of n vectors v1, v2, . . . , vn, where each is a function of all previous:
vi ≡ vi(vi-1, vi-2, . . . ,v1),	1 ≤ i ≤ n
(11)
The chain rule for ordered derivatives expresses the full derivatives d∂+Vi for any j < i in terms of
the full derivatives that relate vi to all previous vk :
∂+Vi = X ∂+Vi ∂vk , .
西一/j际dvj, J< "
(12)
3.2	Gradient Decomposition for General NARX RNNs
Consider NARX RNNs in their general form (Equation 9), which we remark encompasses other
RNNs such as LSTM as special cases. Also, for simplicity, consider the situation that is most often
encountered in practice, where the loss at time t is defined in terms of the current state ht and its
own parameters θl (which are independent of θ).
lt = fl(ht, θl)
(13)
(This is in not necessary, but we proceed this way to make the connection with RNNs in practice
evident. For example, fl may be a linear transformation with parameters θl followed by squared-
error loss.) Then the Jacobian (or transposed gradient) with respect to θ can be written as
∂+lt	∂fl ∂+ht
—:----=-------:--
∂θ	∂ht ∂θ
(14)
because the additional term f d+θl is 0. Now, by letting vι = θ, v = xι, v3 = x2, and so on in
Equations 11 and 12, we immediately obtain
∂+ht _ tX ∂+ht ∂ht-τ
∂θ =乙 ∂ht-τ	∂θ
τ=0
∂x
because all of the partials 焉 are 0.
(15)
Equations 14 and 15 extend Equations 3 and 4 of (Pascanu et al., 2013) to general NARX RNNs,
∂ + h
which encompass simple RNNs, LSTM, etc., as special cases. This decomposition breaks d∂ht into
∂ + h
its temporal components, making it clear that the spectral norm of 蔡广 plays a major role in how
ht-τ affects the final gradient d+lt . In particular, if the norm of d+ht is extremely small, then
∂ θ	∂ ht-τ
ht-τ has only a negligible effect on the final gradient, which in turn makes it extremely difficult to
learn from events that occurred at t - τ .
3.3	Connecting Gradient Components to Paths and Edges
Equations 14 and 15, along with the chain rule for ordered derivatives, let us connect gradient
components to paths and edges, which is useful for a) gaining insights into various architectures and
b) solidifying intuitions from backpropagation through time which suggest that short paths between
t - τ and t facilitate gradient flow. Here we provide an overview of the main idea; please see the
appendix for a full derivation.
∂ + h
By applying the chain rule for ordered derivatives to expand ∂∂hltht in Equation 15, we obtain a sum
over τ terms. However each term involves a partial derivative between ht and a prior hidden state,
and thus all of these terms are 0 with the exception of those states that share an edge with ht . Now,
4
Under review as a conference paper at ICLR 2018
for each term, we can repeat this process. This then yields non-zero terms only for hidden states
which can be connected to ht through two edges. We can then continue to apply the chain rule for
ordered derivatives repeatedly, until only partial derivatives remain.
Upon completion, we have a sum over gradient components, with each component corresponding
to exactly one path from t - τ to t and being a product over its path’s edges. The spectral norm
corresponding to any particular path (t 一 T → t0 → t00 → ∙ ∙ ∙ → t) can then bounded as
∂ ht
∂ htooo∙∙∙
∂hto
∂ht-τ
∂ht
∂htooo∙∙∙
∂hto
∂ ht-τ
≤ λne
(16)
≤
where λ is the maximum spectral norm of any factor and ne is the number of edges on the path.
Terms with λ < 1 diminish exponentially fast, and when all λ < 1, shortest paths dominate3.
4	Mixed History Recurrent Neural Networks
Viewing gradient components as paths, with each component being a product with one factor per
edge along the path, gives us useful insight into various RNN architectures. When relating a loss
at time t to events at time t 一 τ, simple RNNs and LSTM contain shortest paths of length τ, while
simple NARX RNNs contain shortest paths of length τ /nd, where nd is the number of delays.
One can envision many NARX RNN architectures with non-contiguous delays that reduce these
shortest paths further. In this section we introduce one such architecture using base-2 exponential
delays. In this case, for all τ ≤ 2nd-1, shortest paths exist with only log2 τ edges; and for τ >
2nd-1, shortest paths exist with only τ∕2nd-1 edges (see Figure 1). Finally We must avoid the
parameter and computation growth of simple NARX RNNs. We achieve this by sharing weights
over delays, instead using an attention-like mechanism (Bahdanau et al., 2015) over delays and a
reset mechanism from gated recurrent units (Cho et al., 2014).
The proposed architecture, which we call mixed history RNNs (MIST RNNs), is described by
at = softmax(Wahht-1 + Waxxt + ba)	(17)
rt = σ(Wrhht-1 + Wrxxt + br)	(18)
nd-1
ht = tanh Wh rt X atiht-2i +Wxxt+b	(19)
Here, at is a learned vector of nd convex-combination coefficients and rt is a reset gate. At each time
step, a convex combination of delayed states is formed according to at ; units of this combination
are reset according to rt ; and finally the typical linear layer and nonlinearity are applied.
5	Experiments
Here we compare MIST RNNs to simple RNNs, LSTM, and Clockwork RNNs. We begin with
the sequential permuted MNIST task and the copy problem, synthetic tasks that were introduced to
explicitly test RNNs for their ability to learn long-term dependencies (Hochreiter & Schmidhuber,
1997; Martens & Sutskever, 2011; Le et al., 2015; Arjovsky et al., 2016; Henaff et al., 2016; Dani-
helka et al., 2016). Next we move on to 3 tasks for which it is plausible that very long-term depen-
dencies play a role: recognizing surgical maneuvers from robot kinematics, recognizing phonemes
from speech, and classifying activities from smartphone motion data. We note that for all architec-
tures involved, many variations can be applied (variational dropout, layer normalization, zoneout,
etc.). We keep experiments manageable by comparing architectures without such variations.
5.1	Sequential pMNIST Classification
The sequential MNIST task (Le et al., 2015) consists of classifying 28x28 MNIST images (LeCun
et al., l998) as one of 10 digits, by scanning pixel by pixel - left to right, top to bottom - and emitting
3We remark that it is also possible for gradient contributions to explode exponentially fast, however this
problem can be remedied in practice with gradient clipping. None of the architectures discussed in this work,
including LSTM, address the exploding gradient problem.
5
Under review as a conference paper at ICLR 2018
Table 1: Test-set error rates for sequential pMNIST classification. Hidden unit counts (nh) vary to
match parameter counts with LSTM (approx. 42,000 parameters), except models marked with +
(which have more parameters). α* denotes the optimal learning rate according to validation error.
	nh	log10 α*	Error Rate (%)
Simple RNNs	198	-2.27 ± 0.10	12.9 ± 0.8
LSTM	100	-1.11 ± 0.11	10.4 ± 0.7
Clockwork RNNs	256	-1.91 ± 0.23	15.7 ± 1.2
MIST RNNs	139	-1.35 ± 0.08	5.5 ± 0.2
LSTM+	139	-0.90 ± 0.26	8.8 ± 0.6
LSTM+	512	-1.08 ± 0.18	7.6 ± 0.7
MIST RNNs+	512	-1.19 ± 0.13	4.5 ± 0.1
a label upon completion. Sequential pMNIST (Le et al., 2015) is a challenging variant where a
random permutation of pixels is chosen and applied to all images before classification. LSTM with
100 hidden units is used as a baseline, with hidden unit counts for other architectures chosen to
match the number of parameters. Means and standard deviations are computed using the top 5
randomized trials out of 50 (ranked according to performance on the validation set), with random
learning rates and initializations. Additional experimental details can be found in the appendix.
Test error rates are shown in Table 1. Here, MIST RNNs outperform simple RNNs, LSTM, and
Clockwork RNNs by a large margin. We remark that our LSTM error rates are consistent with best
previously-reported values, such as the error rates of 9.8% in (Cooijmans et al., 2016) and 12% in
(Arjovsky et al., 2016), which also use 100 hidden units. One may also wonder if the difference
in performance is due to hidden-unit counts. To test this we also increased the LSTM hidden unit
count to 139 (to match MIST RNNs), and continued to increase the capacity of each model further.
MIST RNNs significantly outperform LSTM in all cases.
We also used this task to visualize gradient magnitudes as a function of τ (the distance from the loss
which occurs at time t = 784). Gradient norms for all methods were averaged over a batch of 100
random examples early in training; see Figure 2. Here we can see that simple RNNs and LSTM
capture essentially no learning signal from steps that are far from the loss. To validate this claim
further, we repeated the 512-unit LSTM and MIST RNN experiments, but using only the last 200
permuted pixels (rather than all 784). LSTM performance remains the same (7.4% error, within 1
standard deviation) whereas MIST RNN performance drops by 15 standard deviations (6.0% error).
5.2	The Copy Problem
The copy problem is a synthetic task that explicitly challenges a network to store and reproduce
information from the past. Our setup follows (Arjovsky et al., 2016), which is in turn based on
03
-
O
B2H .ɪoɪɪ
20000	40000	60000	80000	100000
Iteration
aaH J0aaq
Simple RNN
--LSTM
Clockwork RNN
--MIST RNN
0.06
0.03
0.00
0	20000	40000	60000	80000	100000
Iteration
0.00
0
asH J0aκ
Iteration	Iteration
Figure 3: Validation curves for the copy problem with copy delays of 50 (upper left), 100 (upper
right), 200 (lower left), and 400 (lower right).
6
Under review as a conference paper at ICLR 2018
Table 2: Error rates for surgical maneuver recognition. Hyperparameters were copied from (DiPietro
et al., 2016), where they were tuned for LSTM with peephole connections. Our LSTM does not
include peephole connections (see Section 2).
	nh	Error Rate (%)
LSTM (DiPietro et al., 2016)	1024	12.2 ± 2.7
Simple RNNs	1024	38.0 ± 6.2
LSTM	1024	13.9 ± 3.0
Clockwork RNNs	1024	22.5 ± 5.0
MIST RNNs	1024	14.1 ± 3.9
(Hochreiter & Schmidhuber, 1997). An input sequence begins with L relevant symbols to be copied,
is followed by a delay of D - 1 special blank symbols and 1 special go symbol, and ends with L
additional blank symbols. The corresponding target sequence begins with L + D blank symbols
and ends with a copy of the relevant symbols from the inputs (in the same order). We run experi-
ments with copy delays of D = 50, 100, 200, and 400. LSTM with 100 hidden units is used as a
baseline, with hidden unit counts for other architectures chosen to match the number of parameters.
Additional experimental details can be found in the appendix.
Results are shown in Figure 3, showing validation curves of the top 5 randomized trials out of 50,
with random learning rates and initializations. With a short copy delay of D = 50, we can see
that all methods other than Clockwork RNNs can solve the task in a reasonable amount of time.
However, as the copy delay D is increased, we can see that simple RNNs and LSTM become unable
to learn a solution, whereas MIST RNNs are relatively unaffected. We also note that our LSTM
results are consistent with those in (Arjovsky et al., 2016; Henaff et al., 2016).
Note that Clockwork RNNs are expected to fail for large delays (for example, the second symbol can
only be seen by the highest-frequency partition, so learning to copy this symbol will fail for precisely
the same reason that simple RNNs fail). However, here they also fail for short delays, which is
surprising because the high-speed partition resembles a simple RNN. We hypothesized that this
failure is due to hidden unit counts / parameter counts: here, the high-frequency partition is allocated
only 256 / 8 = 32 hidden units. To test this hypothesis, we reran the Clockwork RNN experiments
with 1024 hidden units, so that 128 are allocated to the high-frequency partition. Indeed, under this
configuration (with 10x as many parameters), Clockwork RNNs do solve the task for a delay of
D = 50 and fail to solve the task for all higher delays, thus behaving like simple RNNs.
5.3	Surgical Maneuver Recognition
Here we consider the task of online surgical maneuver recognition using the MISTIC-SL dataset
(Gao et al., 2014; DiPietro et al., 2016). Maneuvers are fairly long, high-level activities; examples
include suture throw and knot tying. The dataset was collected using a da Vinci, and the goal is to
map robot kinematics over time (e.g., x, y, z) to gestures over time (which are densely labeled as
1 of 4 maneuvers on a per-frame basis). We follow (DiPietro et al., 2016), which achieves state-of-
the-art performance on this task, as closely as possible, using the same kinematic inputs, test setup,
and hyperparameters; details can be found in the original work or in the appendix. The primary
difference is that we replace their LSTM layer with our layers. Results are shown in Table 2. Here
MIST RNNs match LSTM performance (with half the number of parameters).
5.4	Phoneme Recognition
Here we consider the task of online framewise phoneme recognition using the TIMIT corpus (Garo-
folo et al., 1993). Each frame is originally labeled as 1 of61 phonemes. We follow common practice
and collapse these into a smaller set of 39 phonemes (Lee & Hon, 1989), and we include glottal stops
to yield 40 classes in total. We follow (Greff et al., 2016) for data preprocessing and (Halberstadt,
1998) for training, validation, and test splits. LSTM with 100 hidden units is used as a baseline,
with hidden unit counts for other architectures chosen to match the number of parameters. Means
and standard deviations are computed using the top 5 randomized trials out of 50 (ranked according
7
Under review as a conference paper at ICLR 2018
Table 3: Test-set error rates for TIMIT phoneme recognition. Hidden unit counts (nh) vary to match
parameter counts with LSTM (approx. 44,000 parameters). α* denotes the optimal learning rate
according to validation error.
	nh	log10 α*	Error Rate (%)
Simple RNNs	197	-1.09 ± 0.25	34.1 ± 0.3
LSTM	100	-0.63 ± 0.06	32.1 ± 0.2
Clockwork RNNs	248	-1.03 ± 0.31	38.2 ± 0.4
MIST RNNs	139	-0.91 ± 0.16	32.0 ± 0.3
Table 4: Test-set error rates for MobiAct activity classification. Hidden unit counts (nh) vary to
match parameter counts with LSTM (approx. 44,000 parameters), with the exception of LSTM+
(approx. 88,000 parameters). α* denotes the optimal learning rate according to validation error.
	nh	log10 α*	Error Rate (%)
Simple RNNs	203	-1.91 ± 0.18	49.2 ± 2.7
LSTM	100	-0.89 ± 0.12	38.8 ± 1.5
LSTM+	141	-0.45 ± 0.17	37.8 ± 2.1
Clockwork RNNs	256	-1.09 ± 0.31	40.3 ± 4.8
MIST RNNs	141	-1.39 ± 0.21	29.0 ± 5.2
to performance on the validation set), with random learning rates and initializations. Other experi-
mental details can be found in the appendix. Table 3 shows that LSTM and MIST RNNs perform
nearly identically, which both outperform simple RNNs and Clockwork RNNs.
5.5	Activity Recognition from Smartphones
Here we consider the task of sequence classification from smartphones using the MobiAct (v2.0)
dataset (Chatzaki et al., 2016). The goal is to classify each sequence as jogging, running, sitting
down, etc., using smartphone motion data over time. Approximately 3,200 sequences were collected
from 67 different subjects. We use the first 47 subjects for training, the next 10 for validation, and
the final 10 for testing. Means and standard deviations are computed using the top 5 randomized
trials out of 50 (ranked according to performance on the validation set), with random learning rates
and initializations. Other experimental details can be found in the appendix. Results are shown in
Table 4. Here, MIST RNNs outperform all other methods, including LSTM and LSTM+, a variant
with the same number of hidden units and twice as many parameters.
6	Conclusions and Future Work
In this work we analyzed NARX RNNs and introduced a variant which we call MIST RNNs, which
1) exhibit superior vanishing-gradient properties in comparison to LSTM and previously-proposed
NARX RNNs; 2) improve performance substantially over LSTM on tasks requiring very long-term
dependencies; and 3) require even fewer parameters and computation than LSTM. One obvious
direction for future work is the exploration of other NARX RNN architectures with non-contiguous
delays. In addition, many recent techniques that have focused on LSTM are immediately transferable
to NARX RNNs, such as variational dropout (Gal & Ghahramani, 2016), layer normalization (Ba
et al., 2016), and zoneout (Krueger et al., 2016), and it will be interesting to see if such enhancements
can improve MIST RNN performance further.
Acknowledgments
Removed for anonymity.
8
Under review as a conference paper at ICLR 2018
References
Martin Arjovsky, Shah Amar, and Yoshua Bengio. Unitary evolution recurrent neural networks.
International Conference on Machine Learning (ICML), 2016.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
Dzmitry Bahdanau, KyungHyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. ICLR, 2015.
Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient
descent is difficult. IEEE Transactions on Neural Networks, 5(2):157-166,1994.
Charikleia Chatzaki, Matthew Pediaditis, George Vavoulas, and Manolis Tsiknakis. Human daily
activity and fall recognition using a smartphone’s acceleration sensor. International Conference
on Information and Communication Technologies for Ageing Well and e-Health, 2016.
KyUnghyUn Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder
for statistical machine translation. EMNLP, 2014.
Tim Cooijmans, Nicolas Ballas, Cesar Laurent, and Aaron Courville. Recurrent batch normalization.
arXiv preprint arXiv:1603.09025, 2016.
Ivo Danihelka, Greg Wayne, Benigno Uria, Nal Kalchbrenner, and Alex Graves. Associative long
short-term memory. International Conference on Machine Learning (ICML), 2016.
Robert DiPietro, Colin Lea, Anand Malpani, Narges Ahmidi, S Swaroop Vedula, Gyusung I Lee,
Mija R Lee, and Gregory D Hager. Recognizing surgical activities with recurrent neural networks.
International Conference on Medical Image Computing and Computer-Assisted Intervention, pp.
551-558, 2016.
Salah El Hihi and Yoshua Bengio. Hierarchical recurrent neural networks for long-term dependen-
cies. Advances in neural information processing systems (NIPS), 1995.
Jeffrey L Elman. Finding structure in time. Cognitive science, 14(2):179-211, 1990.
Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent
neural networks. In Advances in Neural Information Processing Systems, pp. 1019-1027, 2016.
Yixin Gao, S. Swaroop Vedula, Carol E. Reiley, Narges Ahmidi, Balakrishnan Varadarajan,
Henry C. Lin, Lingling Tao, Luca Zappella, Benjamn Bejar, David D. Yuh, Chi Chiung Grace
Chen, Rene Vidal, Sanjeev Khudanpur, and Gregory D. Hager. Language of surgery: A surgi-
cal gesture dataset for human motion modeling. Modeling and Monitoring of Computer Assisted
Interventions (M2CAI) 2014, 2014.
John S Garofolo, Lori F Lamel, William M Fisher, Jonathon G Fiscus, and David S Pallett. DARPA
TIMIT acoustic-phonetic continous speech corpus CD-ROM. NIST speech disc 1-1.1. NASA
STI/Recon technical report, 1993.
Felix A Gers, Jurgen Schmidhuber, and Fred Cummins. Learning to forget: Continual prediction
with LSTM. Neural computation, 12(10):2451-2471, 2000.
Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint
arXiv:1410.5401, 2014.
K. Greff, R. K. Srivastava, J. Koutn´k, B. R. Steunebrink, and J. Schmidhuber. LSTM: A search
space odyssey. IEEE Transactions on Neural Networks and Learning Systems, 2016.
Andrew K Halberstadt. Heterogeneous acoustic measurements and multiple classifiers for speech
recognition. PhD thesis, Massachusetts Institute of Technology, 1998.
Mikael Henaff, Arthur Szlam, and Yann LeCun. Orthogonal RNNs and long-memory tasks. Inter-
national Conference on Machine Learning (ICML), 2016.
9
Under review as a conference paper at ICLR 2018
Sepp Hochreiter. Untersuchungen zu dynamischen neuronalen netzen. Diploma, Technische Uni-
VersitatMUnchen, pp. 91, 1991.
SePP Hochreiter and Jurgen Schmidhuber. Long short-term memory. NeUral computation, 9(8):
1735-1780,1997.
Rafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever. An empirical exploration of recurrent
network architectures. International Conference on Machine Learning (ICML), 2015.
Jan Koutnik, Klaus Greff, Faustino Gomez, and Juergen Schmidhuber. A clockwork RNN. Interna-
tional Conference on Machine Learning (ICML), pp. 1863-1871, 2014.
David Krueger, Tegan Maharaj, Janos Kramar, Mohammad Pezeshki, Nicolas Ballas, Nan Rose-
mary Ke, Anirudh Goyal, Yoshua Bengio, Hugo Larochelle, Aaron Courville, et al. Zoneout:
Regularizing rnns by randomly preserving hidden activations. arXiv preprint arXiv:1606.01305,
2016.
Quoc V Le, Navdeep Jaitly, and Geoffrey E Hinton. A simple way to initialize recurrent networks
of rectified linear units. arXiv preprint arXiv:1504.00941, 2015.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
K-F Lee and H-W Hon. Speaker-independent phone recognition using hidden Markov models. IEEE
Transactions on AcoUstics, Speech, and Signal Processing, 37(11):1641-1648, 1989.
Tsungnan Lin, Bill G Horne, Peter Tino, and C Lee Giles. Learning long-term dependencies in
NARX recurrent neural networks. IEEE Transactions on NeUral Networks, 7(6):1329-1338,
1996.
James Martens and Ilya Sutskever. Learning recurrent neural networks with hessian-free optimiza-
tion. In International Conference on Machine Learning (ICML), 2011.
Yajie Miao, Mohammad Gowayyed, and Florian Metze. EESEN: End-to-end speech recognition
using deep RNN models and WFST-based decoding. AUtomatic Speech Recognition and Under-
standing (ASRU), pp. 167-174, 2015.
Aaron Van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks.
International Conference on Machine Learning (ICML), 2016.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural
networks. International Conference on Machine Learning (ICML), 28:1310-1318, 2013.
Tony A. Plate. Holographic recurrent networks. Advances in neUral information processing systems
(NIPS), 1993.
David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-
propagating errors. NatUre, 323(6088):533-538, 1986.
Jurgen Schmidhuber. Learning complex, extended sequences using the principle of history com-
pression. NeUral CompUtation, 4(2):234-242, 1992.
Rohollah Soltani and Hui Jiang. Higher order recurrent neural networks. arXiv preprint
arXiv:1605.00064, 2016.
Paul J Werbos. Generalization of backpropagation with application to a recurrent gas market model.
NeUral networks, 1(4):339-356, 1988.
Paul J Werbos. Maximizing long-term gas industry profits in two minutes in lotus using neural
network methods. IEEE Transactions on Systems, Man, and Cybernetics, 19(2):315-333, 1989.
Paul J Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the
IEEE, 78(10):1550-1560, 1990.
10
Under review as a conference paper at ICLR 2018
Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. International Conference on
Learning Representations (ICLR), 2015.
Ronald J Williams and David Zipser. A learning algorithm for continually running fully recurrent
neural networks. Neural computation, 1(2):270-280, 1989.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine trans-
lation system: Bridging the gap between human and machine translation. arXiv preprint
arXiv:1609.08144, 2016.
Saizheng Zhang, Yuhuai Wu, Tong Che, Zhouhan Lin, Roland Memisevic, Ruslan Salakhutdinov,
and Yoshua Bengio. Architectural complexity measures of recurrent neural networks. Advances
in neural information processing systems (NIPS), 2016.
11
Under review as a conference paper at ICLR 2018
7 Appendix: Gradient Components as Paths
Here we will apply Equation 12 repeatedly to associate gradient components with paths connecting
t - τ to t, beginning with Equation 15 and handling simple RNNs and simple NARX RNNs in order.
∂+h
Applying Equation 12 to expand ∂hht , We obtain
∂+ht	∂+ht ∂hto
∂ht-τ 一针Z	∂hto ∂h-
t≥t0>t-τ
(20)
7.0. 1 Simple RNNs
For simple RNNs, by examining Equation 2, We can immediately see that all partials 瞪［ are 0
except for the one satisfying t0 = t - τ + 1. This yields
∂+ht =	∂+ht ∂ht-τ+ι
∂ ht-τ	∂ht-τ+ι ∂ht-τ
(21)
Now, by applying Equation 12 again to ah+h；】,and then to af+h；?, and so on, we trace out a path
from t - τ to t, as shoWn in Figure 1, finally resulting the single term
∂ht	∂ht-τ+2 ∂ht-τ +ι	2
∂ht-1 …dh「+1 dh「	( )
which is associated with the only path from t - τ to t, with one factor for each edge that is encoun-
tered along the path.
7.0.2 SIMPLE NARX RNNS AND GENERAL NARX RNNS
Next we consider simple NARX RNNs, again by expanding Equation 15. From Equation 10, we
can see that up to n partials are now nonzero, and that any particular partial 黑［ is nonzero if
and only if t0 > t - τ and t0 and t - τ share an edge. Collecting these t0 as the set Vt-τ = {t0 : t0 >
t - τ and (t - τ, t0) ∈ E}, we can write
∂+ht	∂+ht ∂hto
∂h-τ ɪ/	∂ht ∂h-τ
t ∈Vt-τ
(23)
a+h
We can then apply this exact same process to each ¾hht; by defining %， = {t00 : t00 >
t0 and (t0, t00) ∈ E} for all t0, we can write
∂+ht	= X	X	∂+ht	∂ht，。∂hto	(24)
∂ht-τ 乙	∂ht	dht，0	dht，∂ht-τ
t-τ	t0∈Vt-τ t00 ∈Vt0	t t	t-τ
By continuing this process until only partials remain, we obtain a summation over all possible paths
from t - τ to t. Each term in the sum is a product over factors, one per edge:
∂ht	∂ht00	∂ht，
---------- ∙ ---：-----
dht，00…	dht，∂ht-τ
(25)
The analysis is nearly identical for general NARX RNNs, with the only difference being the specific
sets of edges that are considered.
8 Appendix: Experimental Details
8.1	General Experimental Setup
Everything in this section holds for all experiments except surgical maneuver recognition, as in that
case we mimicked DiPietro et al. (2016) as closely as possible, as described above.
12
Under review as a conference paper at ICLR 2018
All weight matrices are initialized using a normal distribution with a mean of 0 and a standard
deviation of 1/√nh, where nh is the number of hidden units. All initial hidden states (for t < 1)
are initialized to 0. For optimization, gradients are computed using full backpropagation through
time, and we use stochastic gradient descent with a momentum of 0.9, with gradient clipping as
described by Pascanu et al. (2013) at 1, and with a minibatch size of 100. Biases are generally
initialized to 0, but we follow best practice for LSTM by initializing the forget-gate bias to 1 Gers
et al. (2000); Jozefowicz et al. (2015). For Clockwork RNNs, 8 exponential periods are used, as in
the original paper. For MIST RNNs, 8 delays are used. We avoid manual learning-rate tuning in
its entirety. Instead we run 50 trials for each experimental configuration. In each trial, the learning
rate is drawn uniformly at random in log space between 10-4 and 101, and initial weight matrices
are also redrawn at random. We report results over the top 10% of trials according to validation-
set error. (An alternative option is to report results over all trials. However, because the majority
of trials yields bad performance for all methods, this simply blurs comparisons. See for example
Figure 3 of Greff et al. (2016), which compares these two options.)
8.2	Sequential pMNIST Classification
Data preprocessing is kept minimal, with each input image individually shifted and scaled to have
mean 0 and variance 1. We split the official training set into two parts, the first 58,000 used for
training and the last 2,000 used for validation. Our test set is the same as the official test set,
consisting of 10,000 images. Training is carried out by minimizing cross-entropy loss.
8.3	Copy Problem: Experimental Details
In our experiments, the L relevant symbols are drawn at random (with replacement) from the set
{0, 1, . . . , 9}; D is always a multiple of 10; and L is chosen to be D/10. This way the simplest
baseline of always predicting the blank symbol yields a constant error rate for all experiments. No
input preprocessing of any kind is performed. In each case, we generate 100,000 examples for
training and 1,000 examples for validation. Training is carried out by minimizing cross-entropy
loss.
8.4	Surgical Activity Recognition: Experimental Details
We use the same experimental setup as DiPietro et al. (2016), which currently holds state-of-the-art
performance on these tasks. For kinematic inputs we use positions, velocities, and gripper angles for
both hands. We also use their leave-one-user-out teset setup, with 8 users in the case of JIGSAWS
and 15 users in the case of MISTIC-SL. Finally we use the same hyperparameters: 1 hidden layer
of 1024 units; dropout with p = 0.5; 80 epochs of training with a learning rate of 1.0 for the first 40
epochs and having the learning rate every 5 epochs for the rest of training. As mentioned in the main
paper, the primary difference is that we replaced their LSTM layer with our simple RNN, LSTM, or
MIST RNN layer. Training is carried out by minimizing cross-entropy loss.
8.5	Phoneme Recognition: Experimental Details
We follow Greff et al. (2016) and extract 12 mel frequency cepstral coefficients plus energy every
10ms using 25ms Hamming windows and a pre-emphasis coefficient of 0.97. However we do not use
derivatives, resulting in 13 inputs per frame. Each input sequence is individually shifted and scaled
to have mean 0 and variance 1 over each dimension. We form our splits according to Halberstadt
(1998), resulting in 3696 sequences for training, 400 sequences for validation, and 192 sequences
for testing. Training is carried out by minimizing cross-entropy loss. Means and standard deviations
are computed using the top 5 randomized trials out of 50 (ranked according to performance on the
validation set).
8.6	Activity Recognition from Smartphones
In (Chatzaki et al., 2016), emphasis was placed on hand-crafted features, and each subject was
included during both training and testing (with no official test set defined). We instead operate on
13
Under review as a conference paper at ICLR 2018
the raw sequence data, with no preprocessing other than sequence-wise centering and scaling of
inputs, and we define train, val, test splits so that subjects are disjoint among the three groups.
14