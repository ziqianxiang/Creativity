Learning Deep Models:
Critical Points and Local Openness
Anonymous authors
Paper under double-blind review
Ab stract
With the increasing interest in deeper understanding of the loss surface of many non-convex deep
models, this paper presents a unifying framework to study the local/global optima equivalence of the
optimization problems arising from training of such non-convex models. Using the local openness
property of the underlying training models, we provide simple sufficient conditions under which any
local optimum of the resulting optimization problem is globally optimal. We first completely charac-
terize the local openness of matrix multiplication mapping in its range. Then we use our characteri-
zation to: 1) show that every local optimum of two layer linear networks is globally optimal. Unlike
many existing results in the literature, our result requires no assumption on the target data matrix
Y , and input data matrix X. 2) develop almost complete characterization of the local/global optima
equivalence of multi-layer linear neural networks. We provide various counterexamples to show
the necessity of each of our assumptions. 3) show global/local optima equivalence of non-linear
deep models having certain pyramidal structure. Unlike some existing works, our result requires no
assumption on the differentiability of the activation functions and can go beyond “full-rank” cases.
1	Introduction
Deep learning models have recently led to significant practical successes in various fields ranging from computer
vision to natural language processing. Despite these significant empirical successes, the theoretical understanding
of the behavior of these models is still very limited. While some recent works have tried to explain these successes
through the lens of expressivity by showing the power of these models in learning large class of mappings, other works
find the root of the success in the generalizability of these models from learning perspective.
From optimization perspective, training deep models require solving non-convex optimization problems, where non-
convexity arises from the “deep” structure of the model. In fact, it has been shown by Blum & Rivest (1989) that
training neural networks to global optimality is NP-complete in the worst case even for the simple case of three node
networks. Despite this worst case barrier, the practical success of deep learning may suggest that most of the local
optimal points of these models are close to the global optimal points. In particular, Choromanska et al. (2015) uses spin
glass theory and empirical experiments to show that the local optima of deep neural network optimization problem are
close to the global optima.
In an effort to better understand the landscape of training deep neural networks, Kawaguchi (2016); Lu & Kawaguchi
(2017); Yun et al. (2017); Hardt & Ma (2016) studied the linear neural networks and provided sufficient conditions
under which critical points (or local optimal points) of the training optimization problems are globally optimal. For
non-linear neural networks, multiple works have shown that when the number of parameters of the model is larger
than the data dimension, local optima of the resulting optimization problems can be easily found using local search
procedures; see, e.g., Soltanolkotabi et al. (2017); Soudry & Carmon (2016); Nguyen & Hein (2017); Xie et al. (2017).
Despite the growing interest in studying the landscape of deep optimization problems, many of the results and mathe-
matical analyses are problem specific and cannot be generalized to other problems and network structures easily. As a
first step toward reaching a unifying theory for these results, we propose the use of open mappings for characterizing
the properties of the local optima of these “deep” optimization problems.
To study the landscape of shallow/deep models, we study the general optimization problem
minimize '(F(w)),
w∈W
(1)
where '(∙) is the loss function and F(∙) represents a statistical model with parameter W that needs to be learned by
solving the above optimization problem. A simple example is the popular linear regression problem
minimize kXw - y k22 ,
w
where y is a given constant response vector and X is a given constant feature matrix. In this example, the loss function
is the `2 loss, i.e., `(z) = kz - yk22, and the fitted model F is a linear model, i.e., F(w) = Xw. While this linear
regression problem is convex and easy, fitting many practical models, such as deep neural networks, requires solving
non-trivial non-convex optimization problems.
In this paper, we use the local openness of the mapping F to provide sufficient conditions under which every local
optimum of (1) is in fact global optimum. To proceed, let us define our notations that will be used throughout the paper.
We use Al,: and A:,l to denote the lth row and column of the matrix A, respectively. The notation Id ∈ Rd×d is used
to denote the d × d-dimensional identity matrix. Let kAk, N (A), C(A), rank(A) be respectively the Frobenius
norm, null-space, column-space, and the rank of the matrix A. Given subspaces U and V , we say U ⊥ V if U
is orthogonal to V , and U = V ⊥ if U is the orthogonal complement of V . We say matrix A ∈ Rd1 ×d0 is rank
deficient if rank(A) < min{d1, d0}, and full rank if rank(A) = min{d1, d0}. We call a point W = (Wh, . . . , W1),
with Wi ∈ Rdi×di-1, non-degenerate if rank(Wh •…Wi) = min°≤i≤h di, and degenerate if rank(Wh •…Wi) <
mino≤i≤h di. We also say a point W is a second order saddle point of an unconstrained optimization problem if
the gradient of the objective function is zero at W and the hessian of the objective function at W has a negative
eigenvalue. Let us start by briefly explaining the training problem of feedforward neural networks which will also be
used as a motivation for our analysis:
Example: Training Feedforward Neural Networks. Consider the following multiple layer feedforward neural
network optimization problem:
minWmize 1 kF%(W) - Y k2
where Fh is defined in a recursive manner:
Fk (W)，σk(Wk Fk-ι(W)), for k ∈ {2,...,h},
with
F1(W) , σ1(W1X).
Here h is the number of hidden units in our network; σk(∙) denotes the activation function of layer k; the matrix
Wk ∈ Rdk×dk-1 is the weight of layer k with W , (Wi)ih=1 being our optimization variable. The matrix X ∈ Rd0×n
is the input training data; and Y ∈ Rdh ×n is the target training data where n is the number of samples; see, e.g.
Goodfellow & Courville (2016). Notice that this problem is a special case of the optimization problem in (1) which
can be obtained simply by setting our loss function to the `2 loss, and setting F = Fh .
A special instance of this optimization problem was studied in Nguyen & Hein (2017) which considers the non-linear
neural network with pyramidal structure (i.e. di ≤ di-1 ∀ i = 1, . . . , h and d0 ≥ n). Note that this special network
structure does not allow wide intermediate layers. (Nguyen & Hein, 2017, Theorem 3.8) shows that under some
conditions, among which are the differentiability of the loss function '(∙) and the activation function σ(∙), if W is a
critical point with Wi’s being full row rank then itis a global minimum. In this paper, we will relax the differentiability
assumption on both '(∙) and σ(∙); and We will show any local optimum is a global optimum of the objective function.
Another special case is the linear feedforward network where the mapping σk (∙) is the identity map in all layers, which
leads to the optimization problem:
minimize 1 ∣∣Wh ∙∙∙ W1X - Y ∣∣2.	(2)
For this optimization problem, Lu & Kawaguchi (2017) showed that every local optimum of the objective function is
globally optimal under some assumptions. More precisely, by using perturbation analysis, (Lu & Kawaguchi, 2017,
Theorem 2.2) prove that when X and Y are full row rank, every local optimum in problem (2) is a local optimum of
the following problem:
minimum -∣ZX 一 Y∣∣2
Z ∈Rdh ×d0	2
subject to rank(Z) ≤ dp , min0≤i≤h di.
(3)
Moreover, they show that when X is full row rank, every local optimum of problem (3) is a global optimum. Thus,
with the sufficient condition that X and Y are both full row rank, every local optimum of problem (2) is a global
2
00
01
optimum. Another recent work Yun et al. (2017) shows the same result under similar set of assumptions. It is in fact
not hard to see that one cannot relax the full rankness assumption of Y due to the following simple counterexample:
X =I W3 = 10	, W2 = [0], W1 = [ 1 0 ], Y
It is not hard to check that the point W = (W1, W2, W3) is a local optimum of a 3-layer deep linear model problem
(2) with h = 3 that is not a global optimum. However, we will show that if a given local optimum is non-degenerate
(which is a simple checkable condition), the full rankness of Y can be relaxed. Moreover, for degenerate local optima,
we show that if there exist 1 ≤ p1 < p2 ≤ h- 1 with dh > dp2 and d0 > dp1, we can find Y and X such that problem
(2) has a local minimum that is not global. Otherwise, given any X and Y , we present a method for constructing a
descent direction from any given degenerate critical point that is not a global optimum; thus we show every degenerate
local minimum is global.
Other examples: Matrix Factorization and Matrix Completion. In addition to the training of deep neural networks,
the matrix completion problem also lies in the category of non-convex problems in (1). For the matrix completion
problem, Park et al. (2016) shows that the non-convex matrix factorization formulation of the non-square matrix
sensing problem has no spurious local optimum under restricted isometry property (RIP) conditions. Similar results
were obtained for the symmetric matrix multiplication problem by Ge et al. (2016), and the non-convex factorized
low-rank matrix recovery problem by Bhojanapalli et al. (2016). Like the analysis in Ge et al. (2016), we start with
the fully observed matrix completion scenario:
minimize	1 ∣∣W2W1 - Y ∣∣2.	(4)
W1 ∈Rd1 ×d0,W2 ∈Rd2 ×d1 2
This problem, which is also referred to as the low rank matrix estimation problem in Srebro & Jaakkola (2003), can
also be viewed as a 2-layer linear neural network optimization problem with the input data matrix X = I. Clearly, this
problem is much simpler than the general matrix completion problem and we only study it as a first step. Moreover,
this optimization problem is a special case of (1) with the loss function being the `2 loss, and the mapping F being
defined as F(W1, W2) = W2W1. In this paper, using our framework, we show that every critical point of (4) is
either a global minimum or a second-order saddle point. This result can be generalized to general loss function '(∙)
for degenerate critical points.
In addition to these results, one of our main contributions is the complete characterization of the local openness of
the matrix multiplication mapping in its range. These results could be used in many other optimization problems for
characterizing the local/global equivalence.
2	Mathematical Framework
As discussed in the previous section, we are interested in solving
minimize '(F(w)),	(5)
w∈W
where F : W 7→ S is a mapping and ` : S 7→ R is a loss function. Here we assume that the set W is closed and
the mapping F is continuous. In non-convex scenarios, this optimization problem can only be solved up to “local
optimality” by local search procedures; see Lee et al. (2016) for an example. To proceed, let us define the auxiliary
optimization problem
minimize `(s),	(6)
s∈S
where S is the range of the mapping F. Since problem (6) minimizes the function '(∙) over the range of the mapping F,
the global optimal objective values for problems (5) and (6) are the same. Moreover, there is a clear relation between
the global optima of the two optimization problem through the mapping F. However, the connection between the local
optima of the two optimization problems is not clear. This connection, in particular, is important when the local optima
of (6) are “nice” (e.g. globally optimal or close to optimal). In what follows, we establish the connection between the
local optima of the optimization problems (5) and (6) under some simple sufficient conditions. This connection is then
used to study the relation between local and global optima of (5) and (6) for various deep learning models. Let us first
define the following concepts, which will help us state our simple sufficient condition.
3
•	Open mapping: A mapping F : W → S is said to be open, if for every open set U ∈ W, F(U) is (relatively)
open in S .
•	Locally open mapping: A mapping F (∙) is said to be locally open at W if for every e > 0, there exists δ > 0
such that Bδ F(w) ⊆ F B(w) . Here Bδ(w) ⊆ W is an open ball with radius δ centered at w, and
B(F (w)) ⊆ S is the ball of radius centered at F (w).
By definition, openness ofa mapping is stronger than local openness. Furthermore, it is not hard to see that a mapping
is locally open everywhere if and only ifitis open. A useful property of (locally) open mappings is that the composition
of two (locally) open maps is (locally) open.
The following simple intuitive observation, which establishes a connection between the local optima of (5) and (6), is
a major building block of our analyses.
Observation 1. Suppose F(∙) is locally open at W. If W is a local minimum ofproblem (5), then S = F(W) is a local
minimum of problem (6).
Figure 1: Sketch of the Proof of Observation 1.
Proof. Let W be a local minimum of problem (5). Then there exists an e > 0 such that '(F(W)) ≤ '(F(w)), ∀w ∈
B(WS). By the definition of local openness,
∃ δ > 0 such that Bδ(S) ⊂ F(Be(W)).
where S = F (W). Therefore, '(s) ≤ '(s), ∀s ∈ Bδ(s), which implies S is a local minimum of problem (6).	□
The above observation can be used to map multiple local optima of the original problem (5) to one local optimum of the
auxiliary problem (6); and potentially make the problem easier to understand. This mapping is particularly interesting
in neural networks since permuting the weights in each layer does not change the objective function. Hence, by nature,
the optimization problem has multiple (disconnected) global optima; and hence it is non-convex. However, collapsing
these multiple local optima to one could potentially simplify the problem. In other words, instead of understanding
the problem in the original variables, we can analyze it in the space of the resulted mapping. Let us clarify this point
through the following simple examples:
Example 2.	Consider the optimization problem
minimize (w2 - 1)2 ,	(7)
w∈R
and its corresponding auxiliary problem
minimize (z - 1)2.	(8)
z≥0
Plots of these two problems can be found in Figure 2a and Figure 2b. Since F(x) , w2 is an open mapping in its
range, it follows from Observation 1 that every local minimum in problem (7) is a local minimum of problem (8). Thus
the two local minima w = -1 and w = +1 in (7) are mapped to a single local minimum z = 1 of problem (8).
Moreover, since the optimization problem (8) is convex, the local minimum is global; and hence the original local
optima w = -1 and w = +1 should be both global despite non-convexity of (7).
4
/(Z) = α — i)2
(b) Auxiliary Problem
Figure 2: Two local minima w = -1 and w = +1 in (a) are mapped to a single local minimum z = 1 in (b).
Wl
(a) Original Problem
(b) Auxiliary Problem
Figure 3: All the points in the set {(w1, w2) | w1w2 = 1} are local minima in (a) and are mapped to a single local
minimum z = 1 in (b).
Example 3.	Another example is related to the widely used matrix multiplication mapping Wi W2. Let (WW1, W2) be
a local minimum of the optimization problem
minimize	' (Wi W2).
W1 ∈Rm×k, W2∈Rk×n
Then, any point in the set S，{(WVjQi, Q2WW2) with Q1Q2 = I} is also a local minimum. Ifthe matrix product
Wi W2 is locally open at the point (WW 1, W2), then all points in S are mapped to a single local minimum Z = Wi W2
in the corresponding auxiliary problem. A simple one dimensional example is plotted in Figure 3a and Figure 3b.
This motivates us to study the local openness of the matrix multiplication mapping defined as
M : Rm×k × Rk×n 7→ RM with M(Wi,W2) , WiW2,	(9)
where RM , { Z ∈ Rm×n | rank(Z) ≤ min{m, n, k}} is the range of the mapping M.
Although matrix multiplication mappings M(Wi, W2) naturally appears in deep models and is widely used as a
non-convex factorization for rank constrained problems, see Wang et al. (2016); Bhojanapalli et al. (2016); Ge et al.
(2016); Srebro & Jaakkola (2003); Sun (2015), to our knowledge, the complete characterization of the local openness
of this mapping has not been studied in the optimization literature before.
While the classical open mapping theorem in Rudin (1973) states that surjective continuous linear operators are open,
this is not true in general for bilinear mappings such as matrix product. In fact, by providing a simple counterexample
5
of a bilinear mapping that is not open, Horowitz (1975) shows that the linear case cannot be generally extended to
multilinear maps. Several papers, see Balcerzak et al. (2013; 2005); Behrends (2011), investigate this bilinear mapping
and provide a characterization of the points where this mapping is open. Moreover, Behrends (2017) studies the
matrix multiplication mapping M which is a special example of bilinear mappings and provides an almost complete
characterization of the points where the mapping is locally open. However, the openness is studied in Rm×n ; while
the range of the mapping is RM ; and the (relative) local openness should be studied with respect to this range in our
framework. This, in particular causes trouble when Rm×n 6= RM , i.e., when k < min{m, n}.
For the above reason, we study the local openness of the mapping M in its range RM and characterize it completely.
An intuitive (and unofficial) definition of local openness of M(∙) at (W 1, W2) in RM is as follows. We say the
multiplication mapping is locally open at (W 1, W2) if for any small perturbation Z ∈ RM of Z = W1W2, there
exists a pair (Wι, W2),a small perturbation of (W 1, W2), such that Z = W∖ W2.
Notice that when k ≥ min{m, n}, we get RM = Rm×n. However, in the case where k < min{m, n} the mapping is
definitely not locally open in Rm×n, but can still be locally open in Rm. As a simple example, consider W1
and W2 = [ 1 1 ]. In this example there does not exist W1, W2 perturbations of W1 and W2 respectively such
that W1W2 = Z when Z is a full rank perturbation of Z = W1W2; however, for any rank 1 perturbation Z, We can
find a perturbed pair (W1, W2) such that Z = W1W2. Motivated by Observation 1, we study in the next section the
local openness/openness of the mapping M. We later use these results to analyze the behavior of local optima of deep
neural networks.
1
2
3 Local Openness of the Matrix Multiplication Mapping
When W1 ∈ Rm×k and W2 ∈ Rk×n with k ≥ min{m, n}, the range of the mapping M(W1, W2) = W1W2
is the entire space Rm×n. In this case, which we refer to as the full rank case, (Behrends, 2017, Theorem 2.5)
provides a complete characterization of the pairs (W1 , W2 ) for which the mapping is locally open. However, when
k < min{m, n}, which we refer to as the rank-deficient case, the characterization of the set of points for which the
mapping is locally open has not been resolved before. We settled this question in Theorem 5 by providing a complete
characterization of points (W1 , W2 ) for which the mapping M is locally open when k < min{m, n}. We start by
restating the main result in Behrends (2017):
Proposition 4. (Behrends, 2017, Theorem 2.5 Rephrased) Let M(W1, W2) = W1W2 denote the matrix multipli-
cation mapping with W1 ∈ Rm×k and W2 ∈ Rk×n. Assume k ≥ min{m, n}. Then the the following statements are
equivalent:
1. M(∙, ∙) is locally open at (W 1, W2).
∃ W1 ∈ Rm×k such that W1W2 = 0 and W1 + W1 isfull row rank.
or
∃ W2 ∈ Rk×n such that W1W2 = 0 and W2 + ^W? Isfull Column rank.
3.	dim (N(W 1) ∩ C(W2)) ≤ k - m or n - (rank(W2) - dim (N(W 1) ∩ C(W2)) ≤ k - rank(W 1).
The above proposition provides a checkable condition which completely characterizes the local openness of the map-
ping M at different points when the range of the mapping is the entire space. Now, let us state our result that
characterizes the local openness of the mapping M in its range when k < min{m, n}.
Theorem 5. Let M(W1, W2) = W1W2 denote the matrix multiplication mapping with W1 ∈ Rm×k and W2 ∈
Rk×n. Assume k < min{m,n}. Then if rank(W 1) = rank(W2), M(∙, ∙) is not locally open at (W 1, W2). Else, if
rank(W 1) = rank(W2), then the following statements are equivalent
_ rzn	_____一， 一 一 rzn	_	rzn ...	.
i)	∃ W1	∈ Rm×k	such that W1W2	= 0 and W1	+ W1	isfull column	rank.
6
—rzn _ 1_____ _ _ rzn	_	rzn ...
ii)	∃ W2 ∈ Rk×n such that W1W2 = 0 and W2 + W2 isfull row rank.
iii)	dim (N(W 1) ∩ C(W2)) =0.
iv)	dim (N(WT) ∩ C(WT)) = 0.
v)	M(∙, ∙) is locally open at (W 1, W2) in its range RM.
Note that the proof of Theorem 5, which can be found in the appendix section, is different than the proof of Propo-
sition 4, as in the former we need to work with the set of low rank matrices. Besides, the conditions in Theo-
rem 5 are different than the ones in Proposition 4. For example, while conditions i) and ii) are equivalent in the
rank-deficient case, they are not equivalent in the full-rank case. Moreover, unlike the full-rank case, the condition
rank(W 1) = rank(W2) is necessary for local openness in the low rank case.
How much perturbation is needed? As previously mentioned, local openness can be described in terms of pertur-
bation analysis. For example, M(∙, ∙) is locally open at (Wι, W2) if for a given e > 0, there exists δ > 0 such
that for any Z = Z + Rδ ∈ RM with ∣∣Rδk ≤ δ, there exists W1, W2 with ∣∣Wιk ≤ a ∣∣W2k ≤ e, such that
Z = (W1 + W1)(W2 + W2). As a perturbation bound on δ, we show that for any locally open pair (W1, W2), given
an > 0, the chosen δ is of order , i.e., δ = O(). The details of our analysis can be found in the proof of Theorem 5
in Appendix B.
Remark 11t follows from Theorem 5 that when W1 is full column rank, and W2 is full row rank, the mapping M (∙, ∙)
is locally open at (W1, W2). This result was observed in other works; see, e.g., (Sun, 2015, Proposition 4.2). Also
when k < min{m, n} if only one of the two matrices is full rank, then the mapping is not locally open. We have
showed this result in the proof of Theorem 5, and below is a simple example for this phenomenon:
Let
W1 =	11	,	W2	= [0, 0],	W1W2	=	00	00	,	Rδ	=	δ0	00	,
then W1W2 + Rδ is rank one and hence feasible perturbation. However, for any perturbation W1
e1 and W2 = [ €3 , €4 ], we have
, ~ , , ~
(W1 + W1)(W2 + W2)
(1 + €1)€3	(1 + €1)€4
(1 + €2)€3 (1 + €2)€4
Hence, in order for this perturbation to be equal to W1W2 + Rδ, we need €3 to be different from zero. But when €3 is
different from zero, for small enough €2, there does not exist such W1 and W2, or equivalently, M(∙, ∙) is not locally
openat(W1,W2).
In the next sections, we use our local openness result to characterize the cases where the local optima of various
training optimization problem of the form (5) are globally optimal.
4 Non-linear Deep Neural Network with a Pyramidal S tructure:
Consider the non-linear deep neural network optimization problem with a pyramidal structure
minimize '(Fh(W)) with F1(W)，σ1(W1X); Fk(W)，σi(WiF-(W)),	(10)
W
for i ∈ [2, h], where σi(∙) is the activation function applied component-wise to the entries of each layer, i.e., σi(A)=
[σi(Aij)]i,j with σi : R → R being continuous and strictly monotone. Here W = (Wi)h=1 where Wi ∈ Rdi×di-1
is the weight matrix of layer i, and X ∈ Rd0 ×n is the input training data. In this section, we consider the pyramidal
network structure with d0 > n and di ≤ di-1 for 1 ≤ i ≤ h; see Nguyen & Hein (2017) for more details on these
types of networks.
First notice that when X is full column rank and the functions σis are all continuous and strictly monotone, the image
of the mapping Fh is convex and hence every local optimum of the auxiliary optimization problem (6) is global. We
7
now show that when Wi’s are all full row rank and the functions σi are all strictly monotone, the mapping Fh is
locally open at W .
Lemma 6. Assume the functions σi(∙) : R → R are all continuous strictly monotone. Then the mapping Fh defined
in (10) is locally open at the point W = (W1, . . . , Wh) ifWi ’s are all full row rank.
Before proving this result, we would like to remark that many of the popular activation functions such as logsitic,
tangent hyperbolic, and leaky ReLu are strictly monotone and satisfy the assumptions of this lemma.
Proof. Let Us prove by induction. Since linear mappings are open, and since σι(∙) is strictly monotone; by using the
composition property of open maps, we get that F1 is open.
Assume Fk-ι ((Wi)k=J is locally open at (Wi)k=j, then using Proposition 4, due to the full row rankness of Wk,
the mapping WkFk-ι ((Wi)k=J is locally open at (Wk, (Wi)k=ι1). Using the composition property of open maps
and strict monotonicity of σk (∙), We get Fk ((Wi)k=, is locally open at (Wi)k=].	□
Thus, by Observation 1, if W is a local optimum of problem (10) with Wi's being full row rank, then Z = Fh(W)
is a local optimum of the corresponding auxiliary problem:
minimize '(Z)
Z∈Z
where Z is convex. Consequently, Z is a global optimum of problem (10) when the loss function '(∙) is convex.
Nguyen & Hein (2017) show that every critical point W of problem (10) with Wi ’s being full row rank is a global
optimum when both σ(∙) and '(∙) are differentiable. Our result relaxes the differentiability assumption on both the
activation and loss functions; however, we can only show all local optima are global. A popular activation function
that is strictly monotonic and not differentiable is the Leaky ReLU, for which our result follows. It is also worth
mentioning that Nguyen & Hein (2017) allow wide intermediate layers in parts of their result. It is not clear if this
result can be extended to non-differentiable activation functions as well or not.
5 Two-Layer Linear Neural Network
Consider the two layer linear neural network optimization problem
minimize 1 ∣∣W2W1X - Y ∣∣2	(11)
where W2 ∈ Rd2×d1 and W1 ∈ Rd1 ×d0 are weight matrices, X ∈ Rd0×n is the input data, and Y ∈ Rd2×n is the
target training data. Using our transformation, the corresponding auxiliary optimization problem can be written as
minimum	3∣ZX 一 Y||2
Z2
subject to rank(Z) ≤ min{d2, d1, d0}
(12)
(Lu & Kawaguchi, 2017, Theorem 2.2) shows that when X is full rank, every local minimum of problem (12) is
global. By using local openness, we first show that this result holds without any assumption on X or Y. The proof of
Lemma 7 can be found in Appendix A.3
Lemma 7. Every local minimum of problem (12) is global.
Lemma 7 uses local openness to simplify the proof of (Lu & Kawaguchi, 2017, Theorem 2.2) and relax the full
rankness assumption on X. In another related work, (Kawaguchi, 2016, Theorem 2.3) shows that when XXT and
YXT are full rank, d2 ≤ d0, and when YXT (XXT )-1XYT has d2 distinct eigenvalues, every local optimum
is global and all saddle points are second order saddles. While the local/global equivalence result holds for deeper
networks, the property that all saddles are second order does not hold in that case. Another result by (Yun et al., 2017,
Theorem 2.2) shows that when XXT, YXT, and YXT (XXT)-1XYT are full rank, every local optimum of a
linear deep network is global. Moreover, they provide necessary and sufficient conditions for a critical point to be a
8
global minimum. However, in their proof, the full rankness assumption of Y X T was not used in showing the result
for non-degenerate critical points and thus can be relaxed in that case. In this section, without any assumptions on
both X and Y , we reconstruct the proof that shows the latter result for 2-layer networks using local openness, and
then show a similar result for the degenerate case. The result for the degenerate case holds when replacing the square
loss error by a general convex loss function as we will see in Colorollary 9. The proofs of the theorem and corollary
stated below can be found in Appendices A.1 and A.2, respectively.
Theorem 8. Every local minimum of problem (11) is global. Moreover, every degenerate saddle point of problem (11)
is a second order saddle.
Corollary 9. Let the square loss error in(11)be replaced by a general convex loss function '(∙). Then every degenerate
critical point is either a global minimum or a second order saddle.
Baldi & Hornik (1989) and Srebro & Jaakkola (2003) show the same result when both X and Y are full row rank.
Theorem 8 generalizes their results by relaxing the assumptions on both X and Y .
6 Multi-Layer Linear Neural Network
Consider the training problem of multi-layer deep linear neural networks:
minimize 1 ∣∣Wh ∙∙∙ W1X - Y ∣∣2.	(13)
Here W =(Wi)h_v Wi ∈ Rdi × di-1 are the weight matrices, X ∈ Rd0 ×n is the input training data, and Y ∈ Rdh×n
is the target training data. Based on our general framework, the corresponding auxiliary optimization problem is given
by
minimum
Z∈Rdh×n
subject to
2l∣zx - YIl2
rank(Z) ≤ dp , min0≤i≤h di
(14)
Paper Lu & Kawaguchi (2017) showed that when X and Y are full row rank, every local minimum of (13) is global.
We now relax the full rankness assumption and reproduce similar results. However, as we will see, the local/global
equivalence does not always follow if we relax the full rankness. In such cases, we will provide detailed counter
examples. Before proceeding to the proof we define the following mapping:
Mi,j(Wi,...,Wj) : {Wi,...,Wj} → RMi,j
, {Z = Wi . . . Wj ∈ Rdi×dj-1 | rank(Z) ≤ min dl }
j-1≤l≤i
for i > j
Now we state Theorem 3.1 of Lu & Kawaguchi (2017) using our notation. The proof of the lemma stated below can
be found in Appendix A.4.
Lemma 10. If W is non-degenerate, then MhJ(W) = Wh …Wi is locally open at W.
We now demonstrate our main results for this optimization problem which shows that under a set of necessary condi-
tions, every local minimum of problem (13) is global. Although the result for the non-degenerate case directly follows
from (Yun et al., 2017, Theorem 2.2), we provide in Lemma 11 a more intuitive proof that uses local openness of M.
Moreover, Theorem 12 extends the result to degenerate critical points.
Lemma 11. Every non-degenerate local minimum of (13) is global minimum.
Proof. Suppose W = (Wh, . . . , W1) is a non-degenerate local minimum. Then it follows by Lemma 10 that Mh,1
is locally open at W. Then by Lemma 1, Z = Mh(Wh, . . . , W1) is a local optimum of problem (14) which is in
fact global by Lemma 7.	□
We now state the desired result for degenerate critical points.
Theorem 12. Let p；，argmin di and p22，argmin dj. If ʤ * < min{dh, do }, we can find a rank deficient Y such
0≤i≤h	j_Pi
that problem (30) has a local minimum that is not global. Otherwise, given any X and Y, every local minimum of
problem (30) is a global minimum.
The proof of Theorem 12 can be found in Appendix C .
9
References
M. Balcerzak, A. Wachowicz, and W. Wilczynski. Multiplying balls in the space of continuous functions on [0,1]. Studia Mathe-
matica,170:203-209, 2005.
M. Balcerzak, A. Majchrzycki, and A. Wachowicz. Openness of multiplication in some function spaces. Taiwanese J. Math, 17:
1115-1126, 2013.
P. Baldi and K. Hornik. Neural networks and principal component analysis: Learning from examples without local minima. Neural
networks, 2(1):53-58, 1989.
E. Behrends. Products of n open subsets in the space of continuous functions on [0, 1]. Studia Mathematica, 204:73-95, 2011.
E. Behrends. Where is matrix multiplication locally open? Linear Algebra and its Applications, 517:167-176, 2017.
S. Bhojanapalli, B. Neyshabur, and N. Srebro. Global optimality of local search for low rank matrix recovery. In Advances in
Neural Information Processing Systems, pp. 3873-3881, 2016.
A. Blum and R. L. Rivest. Training a 3-node neural network is np-complete. In Advances in neural information processing systems,
pp. 494-501, 1989.
A. Choromanska, M. Henaff, M. Mathieu, G. B. Arous, and Y. LeCun. The loss surfaces of multilayer networks. In Artificial
Intelligence and Statistics, pp. 192-204, 2015.
R. Ge, J. D. Lee, and T. Ma. Matrix completion has no spurious local minimum. In Advances in Neural Information Processing
Systems, pp. 2973-2981, 2016.
I.	Goodfellow and A. Courville. Deep learning. Book in preparation for MIT Press, Cambridge, 2016.
M. Hardt and T. Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231, 2016.
C. Horowitz. An elementary counterexample to the open mapping principle for bilinear maps. Proceedings of the American
Mathematical Society, 53(2):293-294, 1975.
K. Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information Processing Systems, pp. 586-594,
2016.
J.	D. Lee, M. Simchowitz, M. I. Jordan, and B. Recht. Gradient descent only converges to minimizers. In Conference on Learning
Theory, pp. 1246-1257, 2016.
H. Lu and K. Kawaguchi. Depth creates no bad local minima. arXiv preprint arXiv:1702.08580, 2017.
Q. Nguyen and M. Hein. The loss surface of deep and wide neural networks. arXiv preprint arXiv:1704.08045, 2017.
D. Park, A. Kyrillidis, C. Caramanis, and S. Sanghavi. Non-square matrix sensing without spurious local minima via the burer-
monteiro approach. arXiv preprint arXiv:1609.03240, 2016.
W. Rudin. Functional analysis, mcgraw-hill series in higher mathematics. 1973.
M.	Soltanolkotabi, A. Javanmard, and J. D. Lee. Theoretical insights into the optimization landscape of over-parameterized shallow
neural networks. arXiv preprint arXiv:1707.04926, 2017.
D. Soudry and Y. Carmon. No bad local minima: Data independent training error guarantees for multilayer neural networks. arXiv
preprint arXiv:1605.08361, 2016.
N.	Srebro and T. Jaakkola. Weighted low-rank approximations. In Proceedings of the 20th International Conference on Machine
Learning (ICML-03), pp. 720-727, 2003.
R. Sun. Matrix completion via nonconvex factorization: Algorithms and theory. PhD thesis, University of Minnesota, 2015.
L. Wang, X. Zhang, and Q. Gu. A unified computational and statistical framework for nonconvex low-rank matrix estimation. arXiv
preprint arXiv:1610.05275, 2016.
B.	Xie, Y. Liang, and L. Song. Diverse neural network learns true target functions. In Artificial Intelligence and Statistics, pp.
1216-1224, 2017.
C.	Yun, S. Sra, and A. Jadbabaie. Global optimality conditions for deep neural networks. arXiv preprint arXiv:1707.02444, 2017.
10
Appendix
A Proof of Theorem 8, Corollary 9 and Lemma 10
A.1 Proof of the Theorem 8
Proof. The proof for the degenerate case is done by constructing a descent direction if the point is critical but not
global. Let (匹, W1) be a degenerate critical point, i.e. rank(W2W1) < min{d2,d1 ,do}. Then, based on the
dimensions of d0, d1, and d2, we have one of the following cases:
d2 < di then ∃ b = 0 such that b ∈ N(W2)
do < di then ∃ b = 0 such that b ∈ N(WT)
di ≤ d2, and di ≤ do then either W2 is rank deficient and ∃ b = 0 such that b ∈ N(W2) or
Wi is rank deficient and ∃ b = 0 such that b ∈ N (WT)
So in all cases either N (W2) = 0 or N (WT) = 0. Also, let ∆ = W2 WiX - Y .If ∆X T = 0, then by convexity
of the square loss error function, the point (W2, Wi) is a global minimum of (11). Else, there exists (i,j) such that
Xi,:, ∆j,: 6= 0. We now use first and second order optimality conditions to construct a descent direction when the
current critical point is not global.
First order optimality condition: By considering perturbation in the directions A ∈ Rd2×d1 and B ∈ Rd1 ×d0 for
the optimization problem
minimize lk(W2 + tA)(Wi + tB)X — Y ∣∣2	(15)
t2
we obtain
〈AWiX + W2BX , ∆〉= 0, ∀A ∈ Rd2×d1, B ∈ Rd1×d0
Second order optimality condition:
2〈 ABX, ∆〉+ ∣AWiX + W2BXk2 ≥ 0 ∀A ∈ Rd2×d1, B ∈ Rd1 ×d0
ifl = i,
otherwise
B:,l,	0αb
Al,: ,
Suppose (W2, Wi) is a critical point and there exists b = 0, b ∈ N(W2). We define
bT	ifl=j,
0	otherwise
where α is a scalar constant. Then, using the second order optimality condition, for C = ∣ AWiX∣∣2, We get
α ∣b∣2〈Xi,：,Aj,J + C ≥ 0
|{z}|------{------'
6=o	6=o
Since this is true for every value of α, b should be zero which contradicts the assumption on the choice of b. Hence
N (W 2) = 0.
Similarly, suppose (W2, Wi) is a critical point and there exists aT = 0, aT ∈ N(WT). Let
A ,	αaT	if l = j,	B , a	if l = i,
l,:	0	otherwise	:,l 0	otherwise
where α is a constant.Then, for C = ∣W2BX∣∣2, we get
α ∣a∣2〈Xi,：, ∆j,J + c ≥ 0
|{z}|------{------'
6=o	6=o
Using the same argument, we can show that (W 2, W i) is a second order saddle point of problem (11).
We now show the result for the non-degenerate case. Let (W2, Wi) be a non-degenerate local minimum, i.e.
rank(W2Wi) _= min{d2,di, do}. Then it follows by Lemma 10 that the matrix multiplication M(∙, ∙) is locally
open at (W2, Wi). Then by Observation 1, Z = W2 Wi is a local optimum of problem (12) which is in fact global
by Lemma 7.
□
11
A.2 Proof of Corollary 9
Proof. We follow the same steps used in the proof of Theorem 8 to show the result.
First order optimality condition: By considering perturbation in the directions A ∈ Rd2×d1 and B ∈ Rd1 ×d0 for
the optimization problem
minimize '((W2 + tA)(Wι + tB)X - Y)	(16)
we obtain
〈AW ιX + W 2BX, ▽'(W 2 WV ιX - Y )〉=0	∀A ∈ Rd2×d1, B ∈ Rd1 ×d0
Second order optimality condition:
2〈 ABX, ▽'(W2WWιX — Y) i + h(AW 1X, W?BX, W2WW1X) ≥ 0 ∀A ∈ Rd2×d1, B ∈ Rd1 ×d0
where h(∙) is a function that has a tensor representation. But We only need to know that it is a function of
AWιX, W2BX, and W2W1X.
If ▽'(W2WιX — Y)XT, then by convexity of '(∙), (W2, W1) is a global minimum. Otherwise, there ex-
ists (i,j) such that〈Xi,：, (▽'(W2WWιX 一 Y))Q = 0. Using the same former argument in proof of Theorem 8,
we choose A and B such that h(AW 1X, W2BX, W2WWιX) is some constant that does not depend on α, and
(ABX, ▽'(W2W1X 一 Y) i = α(Xi,：, (▽'(W2WιX 一 Y))j ：〉= 0. Then by proper choice of α we show
、------------------{z---------------}
6=0
that the point (W2, W1) is a second order saddle point.
□
A.3 Proof of Lemma 7
Proof. Let rX = rank(X) and UX ΣX VXT with UX ∈ Rd0×d0, ΣX ∈ Rd0×n, VX ∈ Rn×n be a singular value
decomposition of X . Then
kZX - Yk2 = kZUχ [(∑X)∕rx∣θ] - YVxk2
：,1：rX	：,1：rX	：,rX +1：n k .
'--------V--------}
constant in problem (12)
Since UX ΣX is full column rank, then the linear mapping ZUX ΣX	is open, and
：,1：rX	：,1：rX
rank(ZUX (Σχ), 1	) ≤ min{rank(Z),rχ} = min{d2, d1,d0,rχ}.
：,1：rX
Consequently, every local minimum in problem (12) corresponds to a local minimum in problem
minimum WlZ 一 Yk2	Lf
Z ∈Rd2×rχ	2l1 _	11	(17)
subject to rank(Z) ≤ min{d2, d1,d0, rχ }
where Y = (YVX)	. The result follows using (LU & Kawaguchi, 2017, Theorem 2.2).
：,1：rX
□
A.4 Proof of Lemma 10
Proof. We construct a proof by induction on h to show the desired result. When h = 2, we either have d1 <
min{d2, d0} or d1 ≥ min{d2, d0}. In the first case,
d1 = rank(W2W1)	≤ rank(W1)	≤ d1 ⇔ rank(W1)	=	d1,	and	d1	= rank(W2 W1)	≤ rank(W2)	≤	d1	⇔ rank(W2)	=	d1.
12
Since Wi is full column rank and W2 is full column rank, then by Theorem 5, M2,1(∙) is locally open at (W2, Wι).
In the second case, either
d2 = rank(W2W1) ≤ rank(W2) ≤ d2 ⇔ rank(W2) = d2,
or
d0 = rank(W2W1) ≤ rank(W1) ≤ d0 ⇔ rank(W1) = d0
Thus, either W2 is full row rank or Wi is full row rank, then by Proposition 4, M2,1(∙) is locally open at (W2, Wi).
Now assume the result holds for the product of h matrices Mh,1 (W), we show it is true for Mh+1,1 (W).
Since
dp = rank(Wh . . .Wi) ≤ rank(Wp+iWp) ≤ dp ⇔ rank(Wp+iWp) = dp,
then using Proposition 4, we get Mp+ι,p(∙) is locally open at (Wp+i, Wp). So we can replace Wp+iWp
by a new matrix Zp with rank dp. Then by induction hypothesis, the product mapping Mh+i,i =
Wh+i ∙ ∙ ∙ Wp+2ZpWp-1 ∙ ∙ ∙ Wi is locally open at W. Since the composition of locally open maps is locally open,
the result follows.
□
B Proof of Theorem 5
In this section, we prove Theorem 5. This theorem provides a complete characterization of points (Wi, W2) ∈
Rm×k X Rk×n for which the matrix multiplication mapping M(∙, ∙) is locally open for the case of k < min{m, n}. In
particular, we show that if rank(Wi) = rank(W2), M(∙, ∙) is not locally open at (Wi, W2). Else, if rank(Wi)=
rank(W2), then the following statements are equivalent:
i)	∃ Wi ∈ Rm×k such that WiW2 = 0 and Wi + Wi is full column rank.
ii)	∃ W2 ∈ Rk×n such that Wiff 2 = 0 and W2 + W2 is full row rank.
iii)	dim (N(Wi) ∩ C(W2)) = 0.
iv)	dim (N(WT) ∩ C(WT)) = 0.
V) M(∙, ∙) is locally open at (Wi, W2) in its range RM = {Z ∈ Rm×n with rank(Z) ≤ min{m, k, n}}.
To prove this result, we first show that the local openness of M(∙, ∙) at (Wi, W2) is equivalent to the local openness
of M(∙, ∙) at (UTWi, W2 V) where the columns ofU ∈ Rm×m and the columns of V ∈ Rn×n are the left and
right singular vectors of the product Wi W2, respectively. This allows us to focus our study on the local openness
of the mapping M to matrix pairs whose product is a diagonal matrix. We then show in Lemma 16 that when
rank(Wi) = rank(W2), the statements i, ii, ii, and iv are equivalent. Finally, we show in Proposition 18, that these
conditions hold if and only if the mapping M (∙, ∙) is locally open at (Wi, W2). Before proceeding we state and prove
Lemma 13 that will be used later in the proof.
Lemma 13. Let V ∈ Rm×n be a matrix with rank(V) = r < m. Then there exist an index set B = {ii, . . . , ir} ⊆
{1, . . . , m} and a matrix A ∈ R(m-r)×r such that
kAk∞ = max |Aij | ≤ 2m-r-i and VBc = AVB,
i,j
where Vb ∈ Rr×n is a matrix with rows {Vi,-}i∈B and V⅛ ∈ R(m-r)×n is a matrix with rows {吟：}/万。.
Notice that in the above lemma, the bound on the norm of matrix A is independent of the dimension n and it also does
not depend on the choice of matrix V.
13
Proof. To ease the notation, we denote the ith row of V by vi. We use induction on m to show that there exists a basis
B = {iι,...,ir} and a vector aj ∈ Rr such that ∀ j ∈ Bc,
Vj = ^X aj,ivi	with ∣αj,/ ≤ 2m-r-1 ∀ i ∈ B.
i三B
• Induction Base Case m = r + 1: Without loss of generality, assume B = {1,...,r}. Since the case of vr+1 = 0
trivially holds, we consider vr+1 = 0. By the property of basis, there exists a non-zero vector αr+1 ∈ Rr such that
vr+1 = y?i~i αr+1,ivi.
Let i* = arg max |a『+i i∣. If |a『+i i* ∣ ≤ 1, then the induction hypothesis is true. Otherwise, when |a『+i i* ∣ > 1, we
i∈B	,	i , i —	,
have
r
1	ar+1,i
vi* = ------vr+1 -	>	-----— Vi
o+£}	i=i； i=i* o+z.
ar + 1,r + 1	ar + 1,i
ar+ a r+ι,ivi
i∈B*
where B* = (B U (r + 1})∖{i*},
i.e., we remove the item i* from B and include the item r + 1 instead. Since ∣ar+ι,i ∣ ≤ 1, the induction base holds.
• Inductive Step: Assume the induction hypothesis is true for m > r, we show it is also true for m + 1. Without loss
of generality we can assume that B = {1,...,r}. By induction hypothesis,
r
Vj= X aj,i Vi with ∣aj,i∣ ≤ 2m-r-1,	∀ j = {r + 1,...,m}.
i=1
Since the case of vm+ι = 0 trivially holds, we consider vm+ι = 0. Since B is a basis, there exists am+ι = 0 such that
vm+ι = PZi am+ι,i vi. Let i* = argmax ∣am+ι,i∣. If ∣am+ι,i* ∣ ≤ 2m-r, the induction step is done. Otherwise,
一	'	i∈B	'	'
for the case of ∣am+ι,i* ∣ > 2m-r, we have
r
1	am+1,i
vi* =---------vm+1 - y ----------------- Vi
am+1,i*	i=]. i=i* am+1,i*
{z	{z
a∙m+1,m + 1	a∙m + 1,i
=E am+1,iVi,
i∈B*
where B* = (B U {m + 1}) ∖{i*} and clearly ∣am+1,i∣ ≤ 1, ∀ i ∈ B* according to the definition of i*. For all
j ∈ {r +1,...,m}
r
Vj = E aj,i Vi + aj,i* Vi*
i=1; i=i*
r
Σ	aj,i*
aj,i vi T
i=1; i=i*	am+1,i*
r
Vm+1-	E
i=1; i=i
am+1,i aj,i*
------'∙~~JVi
am+1,i*
*
r
X	(aj,i- aj,i* am+1,i )Vi +
i=1； i=i* l___________am+1,i*	,
^^^^^^^^^^^^^∙^^^^^^^^^^^}
%,i
aj,i*
am+1,i*
^-{-----'
a j,m+1
vm+1
E j%.
i∈B*
14
It remains to show that ∖aj,i∖ ≤ 2m-r for all i ∈ B*, j ∈ {r +1,..., m}. Let us first consider i ∈ B*∖{m + 1} and
j ∈ {r +1, .. ., m}:
∖aj,i ∖ ≤ ∖aj,i ∖ + Iaj,i* am+1,i I by triangular inequality
a	am+1,i* '
≤ 2m~r-1 + 2m-r-1∣ am+1,i 1 by induction hypothesis
a am+1,i* '
≤ 2m-r	by definition of i*.
For i = m + 1,
proof.
∖aj,m+ι∖ = I j I≤I - I≤
a am+1,i* '	a am+1,i* '
2m-r. This concludes the inductive step and completes our
□
Lemma 14. Let Wi ∈ Rm×k and W2 ∈ RkXn. Assume further that Wi W2 = U∑Vt is a singular value
decomposition ofthe matrix product W1W2 with U ∈ Rmxm, V ∈ RnXn, and Σ ∈ Rm ×n. Then
M(∙, ∙) is locally open at (Wi, W2) ⇔ M(∙, ∙) is locally open at (UTWi, W2 V).
Proof. We first show the direction “ ⇒ ”. Suppose M(∙, ∙) is locally open at (Wi, W2), then by definition of local
openness, for any given e > 0, there exists δ > 0 such that
IBδ(WiW2) ∩ Rm ⊆ M(IBe(WI),IBe(W2)) = {(Wi + W；)(W2 + Wf) ∖ ||W；|| ≤ 3 ∣∣Wf∣∣ ≤ e}.	(18)
We now show that
IB6(UTWiW2V) ∩ Rm ⊆ M(IBe(UTWi),IBe(W2V)).
Consider Σ ∈ IB6(UT Wi W2 V) ∩ RM, i.e., Σ = UT Wi W2 V+Rδ Withrank(Σ) ≤ min{m, k, n} and	∣∣ ≤ δ.
Since U and V are unitary matrices, we get UΣ VT = WiW2 + UR6 VT with rank(UΣ VT) = rank(Σ) ≤
min{m, k,n} and ∣∣UR6 VT∣∣ = ∣∣R61∣ ≤ δ. According to (18), we have
UΣVT ∈ IBδ(WiW2) ∩ Rm C {(Wi + W；)(W2 + Wf) ∖ |W；|| ≤ e, ∣∣W2≡∣ ≤ e}.
which implies,
∑ ∈ {(UTWi + UTW；)(W2V + WfV) ∖∣∣WM ≤ e, ∣∣W2≡∣ ≤ e}
={(UTWi + UTWi≡)(W2V + WfV) ∖ ∣UTWM ≤ e, kWfVk ≤ 4
Since Σ was arbitrarily chosen, we get IB6(UTW1W2V) ∩ RM CM(IBe(UTWi), IBe(W2V)).
Proving the converse direction “ U ” is similar. Suppose M (∙, ∙) is locally open at (UT Wi, W2 V), then by definition
of local openness, for any given e > 0, there exists δ > 0 such that
IB6(UTW1W2V) ∩ Rm C M(IBe(UTWi),IBe(W2V))
={(UTWi + Wf)(W2V + W20 ∖∣Wfk ≤ e, ∣W2≡k ≤ e}.
(19)
We now show that
IB6(W1W2) ∩Rm CM(IBe(Wi),IBe(W2)).
Consider Z ∈ IB6(W1 W2) ∩ Rm, i.e. Z = W1W2 + R6 Withrank(Z) ≤ min{m,k,n} and |田61∣ ≤ δ. Since
U and V are unitary matrices we get UTZV = UTW1W2 V + UTR6V with rank(UTZV) = rank(Z) ≤
min{m,k,n} and ∣∣UTR6V∣∣ = ∣∣R61∣ ≤ δ . According to (19), we have
UTZV ∈ IB6(UTW1W2V) ∩ RM C M(IBe(UTWi),IBe(W2V)).
which implies,
Z ∈ {(Wi + UW；)(W2 + WfVT) ∖ ||W；k ≤ e, kW2ek ≤ e}
={(Wi + UW；)(W2 + W；eVT) ∖ ||UW；k ≤ e, ∣∣WieVTk ≤ e}.
15
Since Z was arbitrarily chosen, We get IBδ(Wi W2) ∩ RM ⊆ M (IBe(W1), IBe(W2)), which completes the proof.
□
Lemma 15. Let W1 ∈ Rm×k and W2 ∈ Rk×n. Assume further that W1W2 = UΣV T is a singular value
decomposition of the matrix product Wi W2 with U ∈ Rm×m, V ∈ Rn×n, and Σ ∈ Rm×n. Define W1，UT Wi
and W2，W2V. Then the condition (A) below holds true ifand only ifthe condition (B) is true. Similarly, condition
(C) is true if and only if condition (D) is true.
,.、	_ _2--_	___、,九 一 一 _2--__________ ______________ _2--_	...
(A)	∃ W1 ∈ Rm×k such that W1 W2 = 0 and W1 + W1 is full column rank.
(B)	∃ Wi ∈ Rm×k such that WiW2 = 0 and W1 + Wi Isfull column rank.
(C)	∃ W2 ∈ Rk×n such that WiW2 = 0 and W2 + W2 is full row rank.
(D)	∃ W2 ∈ Rk×n such that WiW2 = 0 and W2 + ^W? ISfull row rank.
Proof. Setting Wi
UTWc i and Wf 2
Wc2V leads
to the desired result.
□
Lemma 14 and Lemma 15 imply that for proving Theorem 5, without loss of generality, we can assume that the
product WiW2 is equal to a diagonal matrix. We next show in Lemma 16 that if k < min{m, n} and rank(Wi)=
rank(W2), then statements i, ii, iii, and iv in Theorem 5 are all equivalent.
Lemma 16. Let Wi ∈ Rm×k, W2 ∈ Rk×n with rank(Wi) = rank(W2) = r. Assume further that k < min{m, n}.
Then, the following conditions are equivalent
_ GC _______、，九	一 一 GC_______	____ GC	...	.
i)	∃ Wi	∈ Rm×k such that Wi W2	= 0 and Wi +	Wi	is full column	rank.
_ rzrc _ i__ 一 一 _______rzrc	___ rzrc ...
ii)	∃ W2 ∈ Rk×n such that WiW2 = 0 and W2 + W2 is full row rank.
iii)	dim (N(Wi) ∩ C(W2)) =0.
iv)	dim (N(WT) ∩ C(WT)) = 0.
Proof. To prove the desired result we show the equivalences ii ⇔ iii, and i ⇔ iv . Then we complete the proof by
showing iii ⇔ iv.
We first show the direction “ii ⇒ iii”. Consider Wi ∈ Rm×k, W2 ∈ Rk×n with both being rank r matrices.
Suppose ii holds, then
C(Wf2) ⊆N(Wi) ⇒ rank(Wf2) ≤ dim N(Wi) = k - r.	(20)
Also,
一	..__ 、	________ 一 ,rrc 、	一 ,rrc 、	一 一
k = rank(W2 + W2) ≤ rank(W2) + rank(W2) = r + rank(W2).	(21)
From inequalities (20) and (21), we get
k - r ≤ rank(W2) ≤ k - r ⇒ rank(W2) = k - r.
Note that dim C(Wf2) = dim N(Wi) andC(Wf2 ) ⊆N(Wi ),whichimpliesthatC(Wf2 ) =N(Wi).
Then since rank(W2 + W2) = rank(W2) + rank(W2), we get
0 = C(W2) ∩ C( W2) = N(Wi) ∩ C( W2) ⇒ dim(N(Wi) ∩ C(W )) =0.
16
We now show the other direction “ii U iii”. Without loss of generality, let W2 = (W20)k×rAr×n-r (W20)k×r
0	1	kr	k
where columns of W20 are linearly independent and let W2 = w11 ii), . . . , w1k-r, 0, . . . , 0 ∈ Rk×n be a rank k-r ma-
trix where w1i are unit basis ofN( W1 ) which yields C( W2 ) = N(W1 ). Then since dim N(W1) ∩ C(W2) = 0,
we get rank(W2 + W2) = k for generic choice of . This completes the proof.
Note that by setting W1 = W2T and W2 = W1T, the same proof can be used to show i ⇔ iv. Next, we will prove the
equivalence iii ⇔ iv . Notice that
dim(sPan(N(W ) ∪ C(W )))
=dim (N (Wi)) + dim(C( W2)) — dim (N (Wi) ∩C( W ))
=k — r + r — dim (N (W1) ∩ C (W2 ))
=k — dim (N (Wi) ∩C( W2)).
Thus,
dim (N (Wi) ∩C (W )) =0 ⇔ dim (span (N (Wi) ∪C( W ))) < k
⇔ ∃ a 6= 0 such that a ⊥ C( W2 ), and a ⊥ N( Wi )
⇔ ∃ a 6= 0 such that a ∈ N( W2T ), and a ∈ C( WiT )
⇔ dim(N(WT) ∩ C( WT )) =0,
which completes the proof.	□
The next result shows that if the conditions in Lemma 16 hold, then rank(Wi) = rank(W2). Moreover, for
r , rank (WiW2), the last n - r rows of UTWi and last n - r columns of W2V are all zeros, where the columns
of U ∈ Rm×m and the columns of V ∈ Rn×n are respectively the left and right singular vectors of the product
WiW2.
Lemma 17. Let Wi ∈ Rm×k, W2 ∈ Rk×n with k < min{m, n} and let r , rank(WiW2). Assume further that
WiW2 = UΣVT is an SVD decomposition of WiW2 with U ∈ Rm×m, andV ∈ Rn×n, andΣ ∈ Rm×n. If
i) ∃ Wi ∈ Rm×k such that Wi W2 = 0 and Wi + Wi is full column rank.
and
ii) ∃ W2 ∈ Rk×n such that WiW2 = 0 and W2 + W2 is full row rank.
then
rank(Wi) = rank(W2), (W2V):,r+i:n = 0, and (UTWi)r+i：n,: = 0.
Proof. Suppose that ii) holds, then
C( Wf2 ) ⊆ N( Wi ) ⇒ rank(Wf2) ≤ dim N( Wi ) = k - rank(Wi).	(22)
Also,
_	_ __ rτr2 、	_ ___ _ ,r≥c 、
k = rank(W2 + W2) ≤ rank(W2 ) + rank(W2 ).	(23)
From inequalities (22) and (23), we get
k - rank(W2) ≤ rank(W2) ≤ k - rank(Wi) ⇒ rank(W2) ≥ rank(Wi).	(24)
Similarly, condition i) implies
C (W T ) ⊆N (W T ) ⇒ rank(W i) ≤ dim (N ( WT )) = k — rank(W2).	(25)
17
Also,
k = rank(W1 + Wf1) ≤ rank(W1) + rank(Wf 1).	(26)
From inequalities (25) and (26), we get
k - rank(W1) ≤ rank(W1) ≤ k - rank(W2) ⇒ rank(W1) ≥ rank(W2).	(27)
From inequalities (24) and (27), we get rank(W1) = rank(W2), which combined with Lemma 16 implies
dim(N( Wi) ∩ C( W2)) = 0. Let r，rank(W1W2). It directly follows from the SVD decomposition of the
matrix W1W2, that UTW1(W2V)：,r+i：n = £：,r+i：n = 0, or equivalently W1(W2V),^n = 0. On the other
hand, since C(W2Z,r+im) ⊂ C(W2) and N(Wi) ∩ C(W2) = 0, we conclude that (W2V)：,r+i：n = 0.
Similarly, one can show that (UTWi)r+i：n,： = 0.	□
Proposition 18. Let M(Wi, W2) = WiW2 be the matrix product mapping with Wi ∈ Rm×k, W2 ∈ Rk×n,
and k < min{m, n}. Then, M(∙, ∙) is locally open in its range RM，{Z ∈ Rm×n : rank(Z) ≤ k} at the point
(W 1, W2) if and only ifthe following two conditions are satisfied:
i)	∃ Wi ∈ Rm×k such that WiW2 = 0 and W1 + Wi ISfull column rank.
and
ii)	∃ W2 such that WiW2 = 0 and W2 + W2 ISfUU row rank.
Proof. First of all, according to Lemma 14 and Lemma 15, without loss of generality we can assume that the matrix
product W i W2 is of diagonal form.
Let us start by first proving the “only if” direction. Notice that the result clearly holds when rank(Wi) =
rank(W2) = k by choosing Wi = W2 = 0. Moreover, the mapping M(∙j ∙) cannot be locally open if only
one of the matrices Wi or W2 is rank deficient. To see this, let US assume that Wiis full column rank, while W2 is
rank deficient. Assume further that the mapping M(∙, ∙) is locally open at (W1, W2), it follows from the definition
of openness that the mapping Mi(Wi, W%，WiW± is locally open at (Wɪ, W2) where W2，(W2)：,i：k only
contains the first k columns of W2. Since the range of the mapping Mi at (Wi, Wi) is the entire space Rm×k,
Proposition 4 implies that
{	_ rzn _ 一 rzn	_	r∑n .一一	一
∃	Wi such that WiW2 = 0 and Wi + Wi is full row rank.
or
∃	Wi such that Wi Wi = 0 and W∖ + Wi is full rank.
×k
Moreover, since Wi ∈ Rm×k and m > k,it is impossible for Wi + Wi to be full row rank. On the other hand, since
i	i	ii
Wi is full column rank, WiW2 = 0 implies that W^ = 0; and hence Wi + Wx is not full column rank. Hence
none of the above two conditions can hold and consequently, M(∙, ∙) cannot be open at the point (Wi, WJ) in this
case. Similarly, we can show thatwhen Wi is rank deficient and W2 is full row rank, the mapping M(∙, ∙) cannot be
locally open. Hence, if Wi and W2 are not both full rank, then they both should be rank deficient.
Assume that the matrices Wi and W2 are both rank deficient and M(∙, ∙) is locally open at (Wi, W2). It follows that
Mi(Wi, W2)，WiWi is locally open at (Wi, Wi). By Proposition 4, and since there does not exist Wi such
i	i	ii
that Wi + Wi is full row rank, there should exist Wa such that WiW2 = 0 and Wi + Wi is full rank. Defining
W2，	Wi I 0 , we satisfy the desired condition ii).
Similarly, by looking at the transpose of the mapping M, we can show that condition i) is true when M is locally open.
We now prove the “if” direction. Suppose i) and ii) hold, i.e.,
∃	W i such that W i W2 = 0 and W i + W i is full column rank.
and
∃ W2 such that W i W2 = 0 and W2 + W2 is full row rank.
18
and the last
Let Σ = Wι W2 = [ ∑-.,ι-,r 0 ] be a rank r matrix. Lemma 17 implies that rank(Wι) = rank(W2),
n - r columns of W2 are all zeros. We need to show that for any given e > 0, there exists δ > 0, such that
IBδ(WIW2) ∩Rm ⊆M(lBe(W1),IBe(W2))∙
Consider a perturbed matrix Σ ∈ IBδ(∑) ∩ Rm, we show that Σ ∈ M (IBe(W1), IBe(W2)). Without loss of
generality, and by permuting the columns of Σ if necessary, Σ can be expressed as
-	「∑Jr + Rδ	Rδ	(S：,1:r + Rδ)A1 + RjA2 -
Σ =	、----V----}	l{z}	|------------V----------/
m×r	m×(k-r)	m×(n-k)
Here Ai ∈ Rr×(n-k) and A2 ∈ R(k-r)×(n-k) exist since rank(Σ)≤ k. Moreover, ∣∣Σ — Σ∣∣ ≤ δ implies that the
perturbed matrix
Rδ，[ Ri I Rδ I (£：,i：r + R1)Aι + R1A2 ]
has norm less than or equal δ, i.e. ∣∣ ≤ δ.
kr	kr
Since rank(W2 +W2) = k, there exist a unitary basis set {w 1，∙ ∙ ∙, WIk r } for W2 such that span{w 1，∙ ∙ ∙, WIk r } ∩
C(W2) = 0. Define
€
n2n+i
k×(k-r)
W
1
2
(28)
and let us form the matrix W 1 ∈ Rk×k using the first k columns of W2. Since the last n — r columns of the matrix
W2 are zero, Wi + W1 is a full rank k × k matrix and W1 W^ = 0. Let us define
W0 , [ Ri I Rδ ] (W 1 + Wi)-i,
and
W1 I(W 1 + W 1)：,i：r Ai + (W 1 + W 1)：,r+i：k A2 卜
Using this definition, we have
(W1 + W 0)(W2 + W 0)
=[(W1 + W0)(W2 + W0)：,1：k I (W1 + W0)(W2 + Wo)：,k+1：„]
=[(W1 + W0)(W 1 + W2)∣ (W1 + W0)(W2 + W0)：,k+1：n]
=∑：,1：k + W1W} + [ R1 I R2 ] (W 1 + W 1)-1(W 1 + W1)	9
=0	m×(n-k)
+ J9 (W1 + W0) ][(W1 + Wi)：,1：r	(W 1 + Wi)：,r+1：k ] [ Ai ]]
m×k	_
=W1W2 + [ Rδ I R2 I (S：,1：r + Rδ)A1 + R2A2 ]
=W1W2 + Rδ
=Σ ∙	(29)
□
To complete the proof, it remains to show that for any € > 0, we can choose δ small enough such that ∣ W0∣ ≤ € and
∣W0∣ ≤ €. In other words, we will show Σ ∈ M (IBe(W1), IBe(W2)).
19
T . ~	*,177~'	F . 1	1 1'	A	1 ∙	. T	<c	1 1	∙1 1	. ∙	. 1	1	<-l	F
Let r, With k ≥ r ≥ r, be the rank of Σ. According to Lemma 13 and by possibly permuting the columns, Σ can be
expressed as
∑= [ ∑ι I ∑ιA ],
where Σ1 ∈ Rm×r is full column rank, and A has a bounded norm k Ak ≤ n2n-r-1. Notice that for given W0 and
W0 satisfying (29), permuting the columns of Σ corresponds to permuting the columns of (VW2 + W0). If we can
show that the first r columns are not among the permuted ones, then using the fact that VW2 has only its first r columns
non-zero, it follows that the permutation of the columns of Σ corresponds to the same permutation of the columns of
W0. Moreover, if the first r columns are not among the permuted ones, then without loss of generality we can express
the perturbed matrix
∑"Lr + Rδ
、----{z---}
m×r
∑
Rδ	(∑"Lr + Rδ)Al + R2A2
|{z}	1-----------V----------}
m×(k-r)	m×(n-k)
and the perturbation matrix

Rδ
m×r
Rδ	(∑"Lr + R1)Al + R2A2
|{z}	1-----------V----------}
m×(k-r)	m×(n-k)
where ^A	= A has a bounded norm.
We now show that the first r columns of Σ before permutation ∑:,1:r + Rδ ⊆ ∑ 1. Assume the contrary, then there
exists at least a column Σ=,j + Rδj)j j that is not a column of Σ1, which implies Σ=,j + Rδj)j j is a column of Σ1 A.
Without loss of generality let Σ=,j + (Rδ): j∙ = ∑ 1A：」. It follows that
∑j,j +(R1 )j,j = (∑ 1)j，：A：，1.
But since Σjj + (RI) j∙ j∙ is a non-zero perturbed singular value, and since elements of (Σ	are all of order δ, then
by choosing δ sufficiently small, we get k Ak > 2n-r-1, which contradicts the bound we have on A.
We now obtain an upper-bound on ∣∣ W01∣. Since the norm of A is bounded, the norm of A2 is also bounded by some
constant K，n2n > n2n-r-1. Hence,
δ ≥kRδ k ≥ k(∑jr + R1)A1 + R2A2k
≥k(Σjr + R1)A1k-kR2A2k
≥k(Σ"v + R1)A1∣∣- Kδ
≥ σ2inIAk- Kδ,
where σmi∏ is the minimum singular value of the full column rank matrix ∑：,1：r which is bounded away from zero.
σmin
Here, we have chosen δ <σmin∕2 SOthatk (£：/：『+ Rδ)A1k ≤ —k√L1 k. Rearranging the terms, we obtain
kA1k≤2(1+K)δ.
σmin
Thus, for some constant C，k W2 k,we obtain
20
kW0k2 ≤ kWf2k2 + kW 1k2 kA1k2 + kWf 1 k2 kA2k2
(by triangular inequality and Cauchy Shwarz)
≤
心	E2K2	δ2c2 22 + 2K
4n222n + 4n2 22n + y 0m加
E2	E2K2	2 2 2+2K
≤ 4K2+ w + δ C ∖mn~
≤ j/2 + δ2C2(3!2.
σmin
For a given E > 0, choose
E
δ ≤ min
1+max Ik(W 1 + Wi)-1k, √2C(2+^K ) I
σmin
σmin∕2).
This choice of δ leads to ∣∣WW0k ≤ e. Moreover,
kW 0k≤∣∣Rδ kk(W£ + W1)Tll
≤δk(W21+Wf21)-1k
j	-—■ j j
ek(W1 + W1)Tk
_ . -------------. .
1	+ k(Wl + W1 )-1k
≤ E,
which completes the proof.
We now use Proposition 18, Lemma 16, and Lemma 17 to complete the proof of Theorem 5 restated below.
Theorem 5 [Restated]: Let M(W1, W2) = W1W2 denote the matrix multiplication mapping with W1 ∈ Rm×k
and W2 ∈ Rk×n. Assume k < min{m, n}. Then if rank(W 1) = rank(W2), M(∙, ∙) is not locally open at
(W 1, WW2). Else, if rank(W 1) = rank(W2), then the following statements are equivalent:
i)	∃ W1 ∈ Rm ×k such that W 1W2 = 0 and W1 + W1 is full column rank.
ii)	∃ W2 ∈ Rk×n such that W1W2 = 0 and W2 + W? is full row rank.
iii)	dim (N(W 1) ∩ C(W2)) = 0.
iv)	dim (N(WT) ∩ C(WT)) = 0.
V) M(∙, ∙) is locally open at (W 1, W2) in its range RM = {Z ∈ Rm×n with rank(Z) ≤ min{m, k, n}}.
Proof. First of all, if M(∙, ∙) is locallyopen at (W 1, W 2), according to Proposition 18, the conditions i) and ii) must
hold; and hence rank(W 1) = rank(W2) due to Lemma 17. Thus, M(∙, ∙) cannot be locally open if rank(W 1)=
rank(W2). On the other hand, when rank(W 1) = rank(W2), the conditions i), ii), iii), and iv) are equivalent due
to Lemma 16. Moreover, these conditions imply local openness according to Proposition 18.
□
21
C Proof of Theorem 12
Consider the training problem of a multi-layer deep linear neural network:
minimize 1 kWh …W1X - Y k2.
W2
(30)
Here W = (Wi)h=], Wi ∈ Rdi × di-1 are the weight matrices, X ∈ Rd0 ×n is the input training data, and Y ∈ Rdh×n
is the target training data. Based on our general framework, the corresponding auxiliary optimization problem is given
by
minimum -||ZX - Y||2
Z ∈Rdh ×d0	2
subject to rank(Z) ≤ dp , min0≤i≤h di
(31)
Let Pi，argmin d% and PW，argmin dj. In this section We show that if dp2* < min{dh, d0}, We can find a rank
0≤i≤h	j=P；
deficient Y such that problem (30) has a local minimum that is not global. Otherwise, given any X and Y, every
local minimum of problem (30) is a global minimum. We start with a Lemma that will be essential in our main proof.
Lemma 19. Consider a degenerate point W = (Wh,..., W1) with N (W%) and N(WT) for h — 1 ≤ i ≤ 2 all
non-empty. If
N(Wh) is non-empty or N(WT) is non-empty,
then W is either a global minimum or a saddle point of problem (30).
Proof. Suppose that N(Wh) is non-empty. Let ∆ = Wh …WιX — Y. If ∆XT = 0, then by convexity of the
square loss error function, the point W = (Wh,..., Wι) is a global minimum of (30). Else, there exist (i,j) such
that〈 Xi,：, ∆Q = 0. We define the set K，{k | 3 ≤ k ≤ h, N(Wk) ⊥ N((W-Wk-…W2)T)}. We
split the rest of the proof into two cases that correspond to K being empty and non-empty.
Case a: Assume K is non-empty. We define k * , maximum k.
k∈K
By definition of the set K and choice of k*, the null space N(Wk*) is orthogonal to the null-space
N((Wk*-ι …W2)T). This implies there exists a non-zero b ∈ Rdk*-1 such that b ∈ N(Wk*) ∩
C(Wk*-ι •…W2). By considering perturbation in directions A = (Ah,..., Ai), Ai ∈ Rdi×di-1 for the opti-
mization problem
minimize g(t)，1 k(Wh + tAh)…(Wi + tA1)X — Y ∣∣2,	(32)
t2
we examine the optimality conditions for a specific direction A.
Let
(Ah)l,:,
αh pTh
ifl=j,
otherwise
ifl = i,
otherwise
if k* + 1 ≤ k ≤ h — 1
ifk= k*
if2 ≤ k ≤ k* —1,
where ah and αι are scalar constants, bi ∈ Rd1 such that Wk*-ι •…W2b1 = b, and
Pk ∈N ((W k-1 …W2)T), bk-1 ∈N (W k), and h Pk, bk-ιi=0 ∀ k* + 1 ≤ k ≤ h. (33)
Notice tha[such Pk and bk-i exist from the definition of K and choice of k*. For this particular choice of A =
(Ah,..., Ai), we obtain
Wk+iAk = 0 for k* ≤ k ≤ h — 1 and Jik W……W2 = 0 for k* + 1 ≤ k ≤ h.	(34)
We now show that (Ah,..., Ai) is in fact a descent direction. Before proceeding we define some notation that will
ease the expressions of the optimality conditions. Let V be an index set that is a subset of {1, . . . , h}. We define
the function f (AV, W-V) which is the matrix product attained from Wh …WiX by replacing matrices WV by
22
matrices Av for every v ∈ V. For instance, if h = 5 and V = {2,3,5}, then f (AV, W-V) = A5W4A3A2W1X.
We now determine index sets V, with |V| ≥ 1_, that correspond to non-zero f (AV, W-V). First note by definition
of A, if V ∩ {k* - 1,..., 2} = 0,jhen f (AV, W-V) = 0. Also by (34), for any k[≤ V ≤ h - 1, if V ∈ V
then either {k*,..., h} ∈ V or f (Av, W-V) = 0. This directly imply that Ah …Ak*Wfc*-1 ∙ ∙ ∙ W1X and
Ah …Ak* Wk*-ι …W2A1X are the only terms that can take non-zero values. Using the definition equation (32)
we obtain
g(t) = 2∣∣th-k*+1Ah …Ak*Wk*-ι …W1X + th-k*+2Ah …Ak*Wfc*-1 ∙∙∙ W2A1X + ∆∣∣2.
It follows that
and
∂h-k*+1ff(t)
∂th-k*+1
∂r g(t)
∂tr
=0
t=0
for all r ≤ h — k*.
(h - k* + 1)!( √Lh ∙∙∙ Ak* W k*-ι∙
∙ WIX, ∆).
t=0
If (Ah ∙ ∙ ∙ Ak* Wk*-ι ∙ ∙ ∙ W1X, ∆) =0, then by properly choosing the sign of αh such that
〈A% ∙∙∙ A®* W k*-ι ∙∙∙ W1X ,△〉 < 0, we get a descent direction. Otherwise, we examine
∂h-k*+2g(t)
∂th-k*+2
(h - k* +2)!〈 Ah∙∙∙ Ak* W k*-1 ∙∙∙ W 2A1X, △〉+ h(A% ∙∙∙ Ak* W k*-1 ∙∙∙ W1X).
t=0
where h(∙) is a function of Ah ∙ ∙ ∙ Ak* Wk*-1 ∙ ∙ ∙ W1X.
We now evaluate the term ( tL% ∙ ∙ ∙ Ak* Wk*-1 ∙ ∙ ∙ W2A1X, ∆). Since (A%)i,： = 0 for all l = j and (A。：,？ = 0
for all l = i, we only need to compute the (j, i) index (√Lh ∙ ∙ ∙ Ak*Wk*-1 ∙ ∙ ∙ W2AJ0 .)as all other indices are
zero. For some constant C = PT"-1pT-1bh-2 ∙ ∙ ∙ PT + 1bk* bτb, we obtain
(Ah∙∙∙ Ak * W k *-1 ∙∙∙ W 2A1)(川=αha1pT bh-IPLIbh-2 ∙∙∙pT*+1bk * bT W k *-1 ∙∙∙ W 2b1
=αha1pT bh-1pT-Ibh-2 ∙ ∙ ∙pT* + 1bk * bT b
=αh01C,
where C isjnon-zero by our choice of b, Pk and bk-1 for h ≤ k ≤ k* - 1 as defined in (33). For a fixed αh, = 0,
h(Ah ∙ ∙ ∙ Ak* Wk*-1 ∙ ∙ ∙ W1X) is a constant scalar we denote by cα. Then by properly choosing a such that
∣ah} α1 ICz} ( Xi,:, ʌj,: ) +cα < 0,
=0	=0 ' 篙 '
we get a descent direction. This completes the first case.
(Ah)l,: , {
Case b: Assume K is empty. We consider
ahpT	if l = j,	(A) , J。也	if l = i,	A △ J bkPT	if 3 ≤ k ≤ h - 1
0	otherwise	1 ：,l	0 0 otherwise	k b bkbT	ifk = 2,
where qh and a1 are scalar constants, b1 ∈ N(W2), and
Pk ∈ N((Wk-1 ∙ ∙ ∙ W2)T),	bk-1 ∈ N(Wk),	and〈pk, bk-1〉= 0 ∀ 3 ≤ k ≤ h.	(35)
For this particular choice of A = (Ah,..., A1),we obtain
W k+1Ak = 0 for 2 ≤ k ≤ h — 1 and Ak W k-1∙∙∙ W 2 = 0 for 3 ≤ k ≤ h.
23
We now determine index sets V, with IV∣ ≥ 1, that correspond to non-zero f (AV, WW-V). By (35), for any 2 ≤ V ≤
h 一 1, if v ∈ V then either {2,...,h} ∈ V or f (AV, W-V) = 0. This directly imply that Ah …A2WιX and
Ah •… AiX are the only terms that can take non-zero values. Using the definition of equation (32) we obtain
g(t) = 2kth-1√Lh …ANiX + thAh …AiX + ∆k2.
It follows that
∂rg(t)
∂tr
=0
t=0
for all r ≤ h 一 2.
and
∂h-ig(t)
∂th-i
=(h - 1)!( Ah …A2WiX,∆).
t=0
If〈 Ah …A2WiX, ∆〉= 0, then by properly choosing the sign of αh such that〈 Ah1…A2WiX, ∆)< 0, we
get a descent direction. Otherwise, we examine
∂hg(t)
∂th
(Ah …AiX, ∆〉+ h(Ah …A2W iX),
t=0
where h(∙) is a function of Ah …A2WiX. We now evaluate the term〈 Ah<…AiX, ∆). Since (A%)?,： = 0 for
all l = j and (Ai)：,? = 0 for all l = i, we only need to compute the (j, i) index (√ih •… AJ0 分)as all other indices
are zero. For some constant C = PTbh-iph-ibh-2 …PTb2bTbi, we obtain
(Ah …Ai)(W = αh,αiphbh-iPT-Ibh-2 …PTb2bTbi
= αhαic,
where c is non-zero by our choice of b, Pk and bk-i for 3 ≤ k ≤ h as defined in (35). For a fixed αh 6= 0,
h(Ah<…A2WiX) is a constant scalar we denote by Ca. Then by properly choosing αi such that
∣ah} ai Icz}〈 Xi,:, &j,： +cα < 0,
=0	=0 X	=Zo	Z
we get a descent direction. This completes the second case.
Now if N (WT) is non-empty, we define the set
K , {k 11 ≤ k ≤ h 一 2, N(Wh-i …Wk+i) ⊥ N(WT)},
and use a similar proof scheme to show the result. More specifically, we split the proof into two cases that correspond
to K being empty and non-empty.
Case a: Assume K is non-empty. We define k，minimum k.
k∈K
By definition of the set K and choice of k*, the null space N(WTJ is orthogonal to the null-space
N(Wh-i …Wk*+i). This implies there exists a non-zero P ∈ Rdk* such that P ∈ N(WT*) ∩
C( (Wh-i …Wk* + i)T). By considering perturbation in directions A = (Ah,..., Ai), Ai ∈ Rdi×di-1 for the
optimization problem
minimize g(t)，∣k(Wh + tAh)…(Wi + tAi)X 一 Y『，	(36)
we examine the optimality conditions for a specific direction A.
24
Let
（Ah）i,：，｛αhpT
ifl=j,
otherwise
α1b1 ifl =i,
0 otherwise
bkpkT
ppkT
0
if 2 ≤ k ≤ k - 1
if k = k
if k* + 1 ≤ k ≤ h - 1,
where αh and αι are scalar constants, Ph ∈ RdhT such that PT Wh_1 ∙∙∙ Wk* + ι = PT, and
Pk ∈ N (W J), bk-i ∈ N (W h—i ∙∙∙ W k), and h Pk, bk-ι〉=0 ∀ 2 ≤ k ≤ k*.	(37)
Notice tha[sUch Pk and bk-ι exist from the definition of K and choice of k*. For this particular choice of A =
(Ah,..., Ai), We obtain
AkWk-ι = 0 for 2 ≤ k ≤ k*	and Wh-i ∙∙∙ Wk+iAk = 0 for 1 ≤ k ≤ k* - 1.	(38)
The same argument used above can be used to show that (Ah,..., Ai) is actually a descent direction. This completes
the first case.
Case b: Assume K is empty. We consider
(A ∖ 企 JahPT if l = j,
h l,:	0 otherwise
，公、ʌ (αιbι ifl = i,
i :,l 0 otherwise
Ak, {PhpT
if2≤k≤h-2
ifk= h- 1,
where αh and αι are scalar constants, Ph ∈ N (W T-J, and
Pk ∈ N (W T-i),	bk-i ∈ N (W h—i …W k),	and h Pk, bk-i i=0 ∀ 2 ≤ k ≤ h - 1.	(39)
For this particular choice of A = (Ah,..., Ai),we obtain
√ikWk-i = 0 for 2 ≤ k ≤ h - 1 and Wh-i …Wk+i√ik = 0 for 1 ≤ k ≤ h - 2.	(40)
The same argument used above can be used to show that (Ah,..., Ai) is actually a descent direction. This completes
the first case and thus completes the proof.	□
Note that following the same steps of the proof in Lemma 19, we get the same result when replacing the square loss
error by a general convex and differentiable function '(∙). We are now ready to prove the main result restated below.
Theorem 12 Let pi* , argmin di and p2* , argmin dj. If dp2* < min(dh, d0), we can find a rank deficient Y such
0≤i≤h	j 6=p1*
that problem (30) has a local minimum that is not global. Otherwise, given any X and Y , every local minimum of
problem (30) is a global minimum.
Proof. Suppose dp* < min{dh , d0 }, we define
p2 , max(pi*, p2*) and pi , min(pi*, p*2).
Let X , I ,
Idk 0
Idk-1
0
ifdk ≤ dk-i,
ifdk > dk-i,
(Y)	, ʃ 1 if (i,j) = (dh,do)
(i,j)	0 otherwise
W k，
for k ∈	{h,...,p2	+	1}	∪	{pi,...,	1},	and Wk	= 0 for k ∈	{p2,...,pi +	1}.	Since Wh	…Wp2+i	and
Wpi …WI are both full rank, then using Lemma 10, the matrix products Mh,p2+i and Mp1,i are locally open
at (Wh,..., Wp2+i) and (Wp`,..., Wi), respectively. Moreover, using Proposition 4 and the composition property
25
of open maps, the matrix product mapping Mp2,p1+1 is locally open at (W p2,..., WW p1 + 1). Itfollows by Observation
1 that if W is a local minimum of	ɪ
minWmize 2kWh…WI- Yk .	(41)
then (Z3, Z2, Zι) is a local minimum of
minimize	UkZ3Z2Z1 - Y k2.	(42)
Z3∈Rdh×dp2 , Z2∈Rdp2 ×dp1 , Z1∈Rdp1 ×d0 2
where
Z3 = Wh …WP2+1	=	Idp2	,	Z2	= 0, and	Zi	= Wpi	…W1 = [	Idpι	0 ].
The point (Z3, Z2, Zi) is obviously not global, we show using optimality conditions that the point is a local minimum.
By considering perturbations in the directions A = (A3, A2, Ai) for the optimization problem
minimize g(t) ，2k(Z3 + tA3)(Z2 + tJɪ2)(Zi + tAi) - Yk2
(43)
=2kt(Z3 + tA3)A2(Z1 + tAi) - Yk2.
It follows that
∂g(t)
∂t
t=0
=(Z3A2Zi,-Y)
=-(z3A2zi)dh,do Ydh，do
= 0,
(44)
where the last equality holds since the last row (dh row) of Z3 is zero. Also,
∂2g(t)
∂t2
t=0
2〈 Z3A2Ai + A3A2Zi, -Y〉+ kZ3A2Zik2
-2(z3A2Ai)dh,d0 Ydh ,do - 2(A3A2zi)dh,d0 Ydh,d0 + kz3A2zik2
kA2k2,
(45)
where the last equality holds since the last row (dh row) of Z3 and the last column (d0h column) of Zi are both zeros.
Then for k A2 k = 0, it follows from the second-order optimalty condition that the point is a local minimum, and if
kA2k =0 we get
g(t)=2 kYk=2
which implies (Z3, Z2, Zi) is a local optimum that is not global.
Note that the same method used to construct the example above can be used to find a local minimum that is not global
whenever the rank(Y) ≤ min{dh -dp2, d0-dp1 }. When Y is full rank, we know from the results ofLu & Kawaguchi
(2017); Yun et al. (2017) that every local minimum is global. To have a complete characterization of problems for
which every local minimum is global, it remains to either prove or disprove the statement when Y is a rank deficient
matrix with rank(Y) > min{dh - dp2 , d0 - dp1 }. We now provide a counterexample that disproves the statement. In
particular, we construct a three layer network with input X and output Y with rank(Y) > min{dh - dp2, d0 - dp1 },
and then find a local minimum (W3, W2, Wi) that is not global. Let X = I,
10
Y,	0	4
-1	0
-1
0
1
1
W 3，	-1
1
-	11
-1 , W 2 ,	1 1 , and W i , W T.
Obviously (W3, W2, Wi) is not a global minimum. We define ∆，W3W2Wi - Y. Then we get
W T δ = ∆WT = 0.
(46)
26
∂g(t, A)
∂t
By considering perturbations in the directions A = (A3, A2, A1) for the optimization problem
minimize g(t, A)，1 k(W3 + tA3)(W3 + tA2)(W 1 + tA1) - Y k2,
it follows that
=(W 3 W 2A1 + W 3A2W1 + A3 W 2W1, ∆〉= 0,
t=0
where the last equality is directly implied from (46). Also
g⑵(0, A) , dg(2A)	= 2( A3A2W1 + A3W2A1 + W3A2A1, ∆〉+ kW3W2Ai + W3A2W1 + A3W2W 1k2
∂t2
t=0
=2( A3W 2A1, ∆〉+ IlW 3W 2A1 + W 3A2W1 + A3 W 2 WW 1k2.
which is a quadratic function of A we denote
fA , 1 aτHAa.
Here a ∈ R16×1 is a vectorization of matrices A3, A2, and A1, and HA is the hessian of fA. By computing the
eigenvalues of HA we get that HA	0 which directly implies
g(2)(0, A) ≥0 ∀A.
Moreover, let aopt be the optimal solution set of the problem
minimize fA .
a
Then aopt = {a | a ∈ N(Ha}. We notice that for any a ∈ a0pt, the corresponding direction A has
W3W2A1 + W3A2W1 + A3W2W1 = 0 and( A3A2A1, ∆〉=0.
Then, it follows that
g(3) (O, A) , a g(；34)	= 6( a3a2a1, δ =0,
t=0
and
g(4)(0, A), a"；；A)	= 12∣∣A3A2 W1 + A3 W 2A1 + W 3A2A1k2 ≥ 0.
t=0
If A3A2W1 + A3W2 A1 + W3A2 A1 = 0, then using the fourth order optimality conditions (W3, W2, W1) is a
local minimum. Otherwise, we get
g(5)(0, A) , d5gt5A)	=0,
t=0
and
g(6) (0, A),吗；5A)	= kA3A2 A1k2 ≥ 0,
t=0
which also implies that (W3, W2, W1) is a local minimum.
We now show that if dp* ≥ min{dh, d0}, every local minimum of (30) is global. In particular, We show that for any
X and Y, if W is not a global minimum, We can construct a descent direction.
First notice that if for some 1 ≤ i ≤ h 一 1, W i is full column rank, then using Proposition 4, Mi+1,i(∙) is locally open
at (W i+1, W i) and W i+1W i ∈ Rdi+1×di-1. Using Observation 1, we conclude that any local minimum of problem
(30) is a local minimum of the problem obtained by replacing Wi+1Wi by Zi+1,i ∈ Rdi+1×di-1. By a similar
27
argument, We conclude that if Wi is a full row rank forsome 2 ≤ i ≤ h, any local minimum of Problem(30) is a local
minimum of the problem obtained by replacing WiWi-ι by Zi,i-ι ∈ Rdi×di-2. Thus, if W= (Wh,..., W1)
is a local minimum of problem (30), the new point Z = (Zhz,..., Zj), where Zi ∈ Rdi×di-1 and h0 ≤ h, is a
local minimum of the problem attained by applying the replacements discussed above. If h0 = 1, we get the desired
result from Lemma 7. Else, if h0 = 2, the auxiliary problem becomes a two layer linear network for which Theorem
8 provides the desired result. When h0 > 2, examine d0h0 , d0h0 -1, d01 and d00. If d0h0 > d0h0 -1 and d00 > d01, then
dp* < min{dh, d0} which contradicts our assumption. It follows by construction of Zi, that either dh ≤ dh,-、and
Zho is not full row rank or d0 ≤ dj and Zj is not full column rank; thus at least one of the null spaces N((ZhJT),
N (Zj) is non empty. Moreover, Zi has non-empty right and left null spaces for 2 ≤ i ≤ h - 1. The result follows
using Lemma 19.
□
28