Workshop track - ICLR 2018
Learning Efficient Tensor Representations
with Ring Structure Networks
Qibin Zhao
RIKEN Center for Advanced Intelligence Project
Tokyo, Japan
qibin.zhao@riken.jp
Longhao Yuan
RIKEN Center for Advanced Intelligence Project
& Saitama Institute of Technology, Japan
longhao.yuan@riken.jp
Masashi Sugiyama
RIKEN Center for Advanced Intelligence Project
& The University of Tokyo
Tokyo, Japan
sugi@k.u-tokyo.ac.jp
Andrzej Cichocki
SKOLTECH, Moscow, Russia
& RIKEN BSI, Japan
a.cichocki@skoltech.ru
Ab stract
Tensor train (TT) decomposition is a powerful representation for high-order tensors,
which has been successfully applied to various machine learning tasks in recent
years. In this paper, we propose a more generalized tensor decomposition with ring
structure network by employing circular multilinear products over a sequence of
lower-order core tensors, which is termed as TR representation. Several learning
algorithms including blockwise ALS with adaptive tensor ranks and SGD with
high scalability are presented. Furthermore, the mathematical properties are inves-
tigated, which enables us to perform basic algebra operations in a computationally
efficiently way by using TR representations. Experimental results on synthetic
signals and real-world datasets demonstrate the effectiveness of TR model and
the learning algorithms. In particular, we show that the structure information and
high-order correlations within a 2D image can be captured efficiently by employing
an appropriate tensorization and TR decomposition.
1 Introduction
Tensor decompositions aim to represent a higher-order (or multi-dimensional) data as a multilinear
product of several latent factors, which attracted considerable attentions in machine learning (Yu &
Liu, 2016; Anandkumar et al., 2014; Romera-Paredes et al., 2013; Kanagawa et al., 2016; Yang et al.,
2017) and signal processing (Zhou et al., 2016) in recent years. For a dth-order tensor with “square”
core tensor of size r, standard tensor decompositions are the canonical polyadic (CP) decomposition
(Goulart et al., 2015) which represents data as a sum of rank-one tensors by O(dnr) parameters and
Tucker decomposition (De Lathauwer et al., 2000; Xu et al., 2012; Wu et al., 2014; Zhe et al., 2016)
which represents data as a core tensor and several factor matrices by O(dnr + rd) parameters. In
general, CP decomposition provides a compact representation but with difficulties in finding the
optimal solution, while Tucker decomposition is stable and flexible but its number of parameters
scales exponentially to the tensor order.
Recently, tensor networks have emerged as a powerful tool for analyzing very high-order tensors (Ci-
chocki et al., 2016). A powerful tensor network is tensor train / matrix product states (TT/MPS)
representation (Oseledets, 2011), which requires O(dnr2) parameters and avoid the curse of dimen-
sionality through a particular geometry of low-order contracted tensors. TT representation has been
applied to model weight parameters in deep neural network and nonlinear kernel learning (Novikov
et al., 2015; Stoudenmire & Schwab, 2016; Tsai et al., 2016), achieving a significant compression
factor and scalability. It also has been successfully used for feature learning and classification (Bengua
et al., 2015). To fully explore the advantages of tensor algebra, the key step is to efficiently represent
the real-world dataset by tensor networks, which is not well studied. In addition, there are some
limitations of TT including that i) the constraint on TT-ranks, i.e., r1 = rd+1 = 1, leads to the
1
Workshop track - ICLR 2018
Figure 1: The effects of noise corrupted tensor cores. From left to right, each figure shows noise
corruption by adding noise to one specific tensor core.
nd …
∖ /
n1- T -nk
zʌ
n …
n1---- Z1 )	( Zk ----nk
Figure 2: A graphical representation of tensor ring decomposition.
limited representation ability and flexibility; ii) TT-ranks are bounded by the rank of k-unfolding
matricization, which might not be optimal; iii) the permutation of data tensor will yield an inconsistent
solution, i.e., TT representations and TT-ranks are sensitive to the order of tensor dimensions. Hence,
finding the optimal permutation remains a challenging problem.
In this paper, we introduce a new structure of tensor networks, which can be considered as a
generalization of TT representations. First of all, we relax the condition over TT-ranks, i.e., r1 =
rd+1 = 1, leading to an enhanced representation ability. Secondly, the strict ordering of multilinear
products between cores should be alleviated. Third, the cores should be treated equivalently by
making the model symmetric. To this end, we add a new connection between the first and the last
core tensors, yielding a circular tensor products of a set of cores (see Fig. 2). More specifically, we
consider that each tensor element is approximated by performing a trace operation over the sequential
multilinear products of cores. Since the trace operation ensures a scalar output, r1 = rd+1 = 1 is
not necessary. In addition, the cores can be circularly shifted and treated equivalently due to the
properties of the trace operation. We call this model tensor ring (TR) decomposition and its cores
tensor ring (TR) representations. To learn TR representations, we firstly develop a non-iterative
TR-SVD algorithm that is similar to TT-SVD algorithm (Oseledets, 2011). To find the optimal lower
TR-ranks, a block-wise ALS algorithms is presented. Finally, we also propose a scalable algorithm
by using stochastic gradient descend, which can be applied to handling large-scale datasets.
Another interesting contribution is that we show the intrinsic structure or high order correlations
within a 2D image can be captured more efficiently than SVD by converting 2D matrix to a higher
order tensor. For example, given an image of size I × J, we can apply an appropriate tensorization
operation (see details in Sec. 5.2) to obtain a fourth order tensor, of which each mode controls one
specific scale of resolution. To demonstrate this, Fig. 1 shows the effects caused by noise corruption
of specific tensor cores. As we can see, the first mode corresponds to the small-scale patches, while
the 4th-mode corresponds to the large-scale partitions. We have shown in Sec. 5.2 that TR model can
represent the image more efficiently than the standard SVD. 2
2 Tensor Ring Decomposition
The TR decomposition aims to represent a high-order (or multi-dimensional) tensor by a sequence
of 3rd-order tensors that are multiplied circularly. Specifically, let T be a dth-order tensor of size
n1 X n2 ×∙ ∙ ∙× nd, denoted by T ∈ Rn1 乂…Xnd, TR representation is to decompose it into a sequence
of latent tensors Zk ∈ Rrk×nk×rk+1 , k = 1, 2, . . . , d, which can be expressed in an element-wise
form given by
T (iι ,i2 ,...,id) = Tr {Zι (iι )Z2(i2)…Zd (id)} = Tr
kY=d1Zk(ik)
(1)
2
Workshop track - ICLR 2018
T(i1, i2, . . . , id) denotes the (i1, i2, . . . , id)th element of the tensor. Zk(ik) denotes the ikth lateral
slice matrix of the latent tensor Zk, which is of size rk × rk+1. Note that any two adjacent latent
tensors, Zk and Zk+1, have a common dimension rk+1 on their corresponding modes. The last
latent tensor Zd is of size rd × nd × r1, i.e., rd+1 = r1, which ensures the product of these matrices is
a square matrix. These prerequisites play key roles in TR decomposition, resulting in some important
numerical properties. For simplicity, the latent tensor Zk can also be called the kth-core (or node).
The size of cores, rk, k = 1, 2, . . . , d, collected and denoted by a vector r = [r1, r2, . . . , rd]T, are
called TR-ranks. From (1), we can observe that T (i1, i2, . . . , id) is equivalent to the trace of a
sequential product of matrices {Zk(ik)}. Based on (1), we can also express TR decomposition in the
tensor form, given by
r1,...,rd
T=	zι(αι, α2) ◦ Z2(α2,α3) ◦••• ◦ Zd(αd,αι),
α1,...,αd=1
where the symbol ‘◦’ denotes the outer product of vectors and zk(αk, αk+1) ∈ Rnk denotes the
(αk, αk+1)th mode-2 fiber of tensor Zk. The number of parameters in TR representation is O(dnr2),
which is linear to the tensor order d as in TT representation.
The TR representation can also be illustrated graphically by a linear tensor network as shown in
Fig. 2. A node represents a tensor (including a matrix and a vector) whose order is denoted by the
number of edges. The number by an edge specifies the size of each mode (or dimension). The
connection between two nodes denotes a multilinear product operator between two tensors on a
specific mode. This is also called tensor contraction, which corresponds to the summation over the
indices of that mode. It should be noted that Zd is connected to Z 1 by the summation over the
index α1, which is equivalent to the trace operation. For simplicity, we denote TR decomposition by
T = <(Z1, Z2, . . . , Zd).
Theorem 1 (Circular dimensional permutation invariance). Let T ∈ Rn1 ×n2×...×nd be a dth-
order tensor and its TR decomposition is given by T = <(Z1, Z2, . . . , Zd). If we define T k ∈
Rnk+1×…×nd ×n1×…×nk as the circularly shifted version along the dimensions of T by k, then we
have Tk = <(Zk+ι,..., Zd, Z1,... Zk).
A proof of Theorem 1 is provided in Appendix B.1.
It should be noted that circular dimensional permutation invariance is an essential feature that
distinguishes TR decomposition from TT decomposition. For TT decomposition, the product of
matrices must keep a strictly sequential order, yielding that the tensor with a circular dimension
shifting does not correspond to the shifting of tensor cores.
3 Learning Algorithms
3.1	Sequential SVDs
We propose the first algorithm for computing the TR decomposition using d sequential SVDs. This
algorithm will be called the TR-SVD algorithm.
Theorem 2. Let us assume T can be represented by a TR decomposition. If the k-unfolding matrix
Thki has Rank(Thki) = Rk+1, then there exists a TR decomposition with TR-ranks r which satisfies
that ∃k, r1rk+1 ≤ Rk+1.
Proof. We can express TR decomposition in the form of k-unfolding matrix,
Thki (iT^ ,ik+1 …id ) = Tr] YY Zj (ij) YY Zj (ij (= 4ec( YY Zj (j) j, VeC(YZT (j))).
Ij=I	j = k + 1∖ ∖j=1	j	∖j=d	∕/
(2)
It Can also be rewritten as
Thki (i1 …ik, ik+1 ∙∙∙ id) = ɪ2 Z≤k (i1 ∙∙∙ ik, α1ak + l) Z>k (α1 αk+1, ik + 1 …，id),
α1αk+1
(3)
3
Workshop track - ICLR 2018
where We defined the subchain by merging multiple linked cores as Z<k (iι ∙∙∙ ik-ι) = Qk-I Zj (j)
and Z>k(ik+ι …id) = "；=k+1 Zj(j). Hence, we can obtain Thki = Z≤)(Z>k)T, where the
subchain Z(≤2k) is of size Qjk=1 nj × r1rk+1, and Z[>2]k is of size Qjd=k+1 nj × r1rk+1. Since the rank
of Thki is Rk+ι, we can obtain r1rk+1 ≤ Rk+ι.	□
According to (2) and (3), TR decomposition can be written as
TINi 1,il2 •一 id) = X Z≤1(iι,α02)Z>1(α1α2, i2 …id).
α1,α2
Since the low-rank approximation of Th1i can be obtained by the truncated SVD, which is Th1i =
UΣVT + E1, the first core Z1(i.e.,Z≤1) of size r1 × n1 × r2 can be obtained by the proper
reshaping and permutation of U and the subchain Z>1 of size r2 × Qjd=2 nj × r1 is obtained by the
proper reshaping and permutation of ΣVT, which corresponds to the remaining d - 1 dimensions
of T. Note that this algorithm use the similar strategy with TT-SVD (Oseledets, 2011), but the
reshaping and permutations are totally different between them. Subsequently, we can further reshape
the subchain Z>1 as a matrix Z>1 ∈ Rr2n2 ×Qjd=3 njr1 which thus can be written as
Z>1(α2i2, i3 …idα1) = ɪ2 Z2(α2i2, α3)Z>2(α3, i3 …idαl)∙
α3
By applying truncated SVD, i.e., Z>1 = UΣVT + E2, we can obtain the second core Z2 of size
(r2 × n2 × r3) by appropriately reshaping U and the subchain Z>2 by proper reshaping of ΣVT.
This procedure can be performed sequentially to obtain all d cores Zk,k = 1,. . . ,d.
As proved in (Oseledets, 2011), the approximation error by using such sequential SVDs is given by
kT -<(Z1,Z2,...,Zd)kF ≤ t
d-1
X kEkk2F.
k=1
Hence, given a prescribed relative error % the truncation threshold δ can be set to √d-τ∣∣T∣∣f.
However, considering that kE1 kF corresponds to two ranks including both r1 and r2, while
∣Ek ∣F,∀k > 1 correspond to only one rank rk+1. Therefore, we modify the truncation threshold as
J √2epkT∣F/√d k = 1,
1 ep ∣t∣f/√d	k > 1.
(4)
A pseudocode of the TR-SVD algorithm is summarized in Alg. 1. Note that the cores obtained by the
TR-SVD algorithm are left-orthogonal, which is ZkTh2iZkh2i = I for k = 2,. . . ,d - 1.
3.2	Block-Wise Alternating Least-Squares (ALS)
The ALS algorithm has been widely applied to various tensor decomposition models such as CP
and Tucker decompositions (Kolda & Bader, 2009; Holtz et al., 2012). The main concept of ALS is
optimizing one core while the other cores are fixed, and this procedure will be repeated until some
convergence criterion is satisfied. Given a dth-order tensor T, our goal is optimize the error function
as
min ∣T- <(Z1,. . . ,Zd)∣F.	(5)
Z1,...,Zd
According to the TR definition in (1), we have
T(iι, i2,. .., id)	= E	Z1(α1,	iι,	ɑ2)Z2(α2,	i2,	a3)	∙∙∙ Zd(ad,	id,	αι)
α1,...,αd
= X	Zk (αk, ik, ɑk+1)Z' (αk + 1, ik+1 ∙ ∙ ∙ idi1 ∙ ∙ ∙ ik-1, αk ) O ,
αk ,αk+1
4
Workshop track - ICLR 2018
where Z=k (ik+ι ∙ ∙ ∙ idiι... ik-ι) = Qd=k+ι Zj (j) Qk=I Zj (j) denotes a slice matrix of subchain
tensor by merging all cores except kth core Zk. Hence, the mode-k unfolding matrix of T can be
expressed by
T[k](ik,ik+ι ,…idiι ∙∙∙ ik-i) = X {Zk(ik,αk0k+l)Z=k(Okok+1,ik+ι •…idiι …ik=ι
αkαk+1
By applying different mode-k unfolding operations, we can obtain that T[k] = Zk(2) Z6[=2]k	, where
Z6=k is a subchain obtained by merging d - 1 cores.
The objective function in (5) can be optimized by solving d subproblems alternatively. More
specifically, having fixed all but one core, the problem reduces to a linear least squares problem,
which is
Zmin T[k] - Zk(2) Z6[=2]kTF, k= 1,...,d.
This optimization procedure must be repeated till the convergence, which is called TT-ALS.
Here, we propose a computationally efficient block-wise ALS (BALS) algorithm by utilizing truncated
SVD, which facilitates the self-adaptation of ranks. The main idea is to perform the blockwise
optimization followed by the separation of a block into individual cores. To achieve this, we consider
merging two linked cores, e.g., Zk, Zk+1, into a block (or subchain) Z(k,k+1) ∈ Rrk×nknk+1×rk+2.
Thus, the subchain Z (k,k+1) can be optimized while leaving all cores except Zk, Zk+1 fixed.
Subsequently, the subchain Z(k,k+1) Can be reshaped into Z(k,k+I) ∈ Rrknk ×nk+1rk+2 and separated
into a left-orthonormal core Zk and Zk+1 by a truncated SVD:
Z(k,k+1) = UΣVt = Zk ⑵ Zk+1 ⑴,
(6)
where Zkh2i ∈ Rrknk×rk+1 is the 2-unfolding matrix of core Zk, which can be set to U, while
Zk+1h1i ∈ Rrk+1 ×nk+1rk+2 is the 1-unfolding matrix of core Zk+1, which can be set to ΣVT.
This procedure thus moves on to optimize the next block cores Z(k+1,k+2) , . . . , Z(d-1,d), Z(d,1)
successively in the similar way. Note that since the TR model is circular, the dth core can also be
merged with the first core yielding the block core Z (d,1) .
The key advantage of our BALS algorithm is the rank adaptation ability which can be achieved simply
by separating the block core into two cores via truncated SVD, as shown in (6). The truncated rank
rk+1 can be chosen such that the approximation error is below a certain threshold. One possible
choice is to use the same threshold as in the TR-SVD algorithm, i.e., δk described in (4). However,
the empirical experience shows that this threshold often leads to overfitting and the truncated rank is
higher than the optimal rank. This is because the updated block Z (k,k+1) during ALS iterations is
not a closed form solution and many iterations are necessary for convergence. To relieve this problem,
we choose the truncation threshold based on both the current and the desired approximation errors,
which is
δ = max卜ITIf/√d, ep∣∣τ∣∣F/√d}.
A pseudo code of the BALS algorithm is described in Alg. 2.
3.3	Stochastic Gradient Descent
For large-scale dataset, the ALS algorithm is not scalable due to the cubic time complexity in
the target rank, while Stochastic Gradient Descent (SGD) shows high efficiency and scalability
for matrix/tensor factorization (Gemulla et al., 2011; Maehara et al., 2016; Wang & Anandkumar,
2016). In this section, we present a scalable and efficient TR decomposition by using SGD, which is
also suitable for online learning and tensor completion problems. To this end, we first provide the
element-wise loss function, which is
L(Z 1,Z2,...,Zd) = 2 X Tt(iι,i2,...,id)-Tr(YYZk(ik))] +2λkkZk(ik)k2, (7)
i1 ,...,id	k=1
5
Workshop track - ICLR 2018
where λk is the regularization parameters. The core idea of SGD is to randomly select one sample
T (i1, i2, . . . , id), then update the corresponding slice matrices Zk(ik), k = 1, . . . , d from each latent
core tensor Zk based on the noisy gradient estimates by scaling up just one of local gradients, i.e.
∀k = 1,. . . ,d,
∂L
∂Zk (ik)
-/T(i1, i2 , ∙ ∙ ∙ , id) - Tr (口 Zk (ik )) ] ( Y Zj (ij) J + λkZk (ik ),
k=1	j=1,j6=k
(8)
We employ Adaptive Moment Estimation (Adam) method to compute adaptive learning rates for each
parameter. Thus, the update rule for each core tensor is given by
Zk (ik )t = Zk-1(ik)- √vηη+; Mt- λk Zk-1(ik),
∀k = 1, . . . , d,
(9)
where Mt = β1Mt-1 + (1 - βι) ∂Ζ Lik)denotes an exponentially decaying average of past gradients
and Vt = β2Vt-i + (I - β2)( azdLik) )2 denotes exponentially decaying average of second moment
of the gradients.
The SGD algorithm can be naturally applied to tensor completion problem, when the data points are
sampled only from a sparse tensor. Furthermore, this also naturally gives an online TR decomposition.
The batched versions, in which multiple local losses are averaged, are also feasible but often have
inferior performance in practice. For each element T(i1, i2, . . . , id), the computational complexity
of SGD is O(d2r3). If we define N = Qkd=1 nk consecutive updates as one epoch of SGD, the
computational complexity per SGD epoch is thus O(N d2r3), which linearly scales to data size.
As compared to ALS, which needs O(N dr4 * + dr6), it is more efficient in terms of computational
complexity for one epoch. The memory cost of TR-SVD, ALS, and SGD are O(ndr2 + Qk nk),
O(ndr2+r2 Qk nk+r4), and O(ndr2+r2), respectively. Thus, TR-SGD requires much less memory.
The convergence condition of SGD algorithm follows other stochastic tensor decompositions (Ge
et al., 2015; Maehara et al., 2016).
In practice, how to choose the TR algorithms depends on the task and data at hand. TR-SVD is a
non-iterative algorithm leading to the fast computation. The expected approximation error is required
from user to choose TR-ranks automatically. However, the learned ranks is not necessary to be the
optimal in terms of compression rate. TR-ALS is an iterative algorithm in which the fixed TR-ranks
are required to be given by user. The computation time is highly related to TR-ranks, which is very
efficient only for small TR-ranks. TR-BALS is an iterative algorithm that can choose TR-ranks in
an adaptive manner given the expected approximation error. TR-SGD is an iterative algorithm that
is suitable for a large-scale tensor with given TR-ranks. In particular, the memory cost of TR-SGD
depends on only the size of core tensors rather than the size of data tensor.
4 Properties of TR Representation
By assuming that tensor data have been already represented as TR decompositions, i.e., a sequence of
third-order cores, we justify and demonstrate that the basic operations on tensors, such as the addition,
multilinear product, Hadamard product, inner product and Frobenius norm, can be performed
efficiently by the appropriate operations on each individual cores. We have the following theorems:
Property 1. Let T1 and T2 be dth-order tensors ofsize nι X …X n. IfTR decompositions ofthese
two tensors are T1 = <(Z1, . . . , Zd) where Zk ∈ Rrk×nk ×rk+1 andT2 = <(Y1, . . . , Yd) where
Yk ∈ Rsk×nk ×sk+1, then the addition of these two tensors, T3 = T1 + T2, can also be represented
in the TR format given by T3 = <(X 1, . . . , Xd), where Xk ∈ Rqk ×nk ×qk+1 and qk = rk + sk.
Each core X k can be computed by
Xk (ik) =	Zk0(ik)
Yk0(ik)
ik = 1, . . . , nk ,
k = 1, . . . , d.
(10)
A proof of Property 1 is provided in Appendix B.2. Note that the sizes of new cores are increased
and not optimal in general. This problem can be solved by the rounding procedure (Oseledets, 2011).
6
Workshop track - ICLR 2018
Figure 3: Highly oscillated functions. The left panel is f1 (x) = (x + 1) sin(100(x + 1)2). The
middle panel is Airy function: f2(x) = X-1 sin(3X3). The right panel is Chirp function f3(x)=
Sin 4 CoS(X2).
Property 2. Let T ∈ Rn1 X…Xnd be a dth-order tensor whose TR representation is T =
<(Z1, . . . , Zd) and uk ∈ Rnk , k = 1, . . . , d be a set of vectors, then the multilinear products,
denoted by C = T ×1 UT ×2 ∙∙∙×d UT, can be computed by the multilinear product on each cores,
which is
nk
c=<(X1,...,Xd)whereXk =	Zk(ik)uk(ik).	(11)
ik=1
A proof of Property 2 is provided in Appendix B.3. It should be noted that the computational
complexity in the original tensor form is O(dnd), while it reduces to O(dnr2 + dr3) that is linear to
tensor order d by using TR representation.
Property 3. Let T 1 and T2 be dth-order tensors of size n1 × ∙ ∙ ∙ × nd. Ifthe TR decompositions of
these two tensors are T1 = <(Z1, . . . , Zd) where Zk ∈ Rrk Xnk Xrk+1 and T2 = <(Y1, . . . , Yd)
where Yk ∈ RskXnk Xsk+1, then the Hadamard product of these two tensors, T3 = T1 ~ T2, can
also be represented in the TR format given by T3 = <(X1, . . . , Xd), where Xk ∈ RqkXnkXqk+1
and qk = rksk. Each core Xk can be computed by
Xk(ik) = Zk(ik)③ Yk(ik),	k = 1,...,d.	(12)
The inner product of two tensors can be computed by TR format hT1, T2i = <(V1, . . . , Vd) where
Vk = inkk=1 Zk (ik) 0 Yk (ik). Therefore, the Frobenius norm can be also computed by TRformat
using IlTlkF = PhT,Ti.
A proof of Property 3 is provided in Appendix B.4. In contrast to O(nd) in the original tensor form,
the computational complexity is equal to O(dnq2 +dq3) that is linear to dby using TR representation.
5 Experimental Results
5.1	Numerical Illustration
We consider highly oscillating functions that can be approximated perfectly by a low-rank TT
format (Khoromskij, 2015), as shown in Fig. 3. We firstly tensorize the functional vector resulting
in a dth-order tensor of size n1 X n2 × ∙ ∙ ∙ × nd, where isometric size is usually preferred, i.e.,
n1 = n2 = ∙ ∙ ∙ = nd = n, with the total number of elements denoted by N = nd. The error bound
(tolerance), denoted by p = 10-3, is given as the stopping criterion for all compared algorithms.
As shown in Table 1, TR-SVD and TR-BALS can obtain comparable results with TT-SVD in terms
of compression ability. However, when noise is involved, TR model significantly outperforms TT
model, indicating its more robustness to noises.
Table 1: The functional data f1(X), f2(X), f3(X) is tensorized to 10th-order tensor (4 × 4 × . . . × 4). In
the table, 3 r, Np denote relative error, average rank, and the total number of parameters, respectively.
			f1(x)				f2(χ)				f3(χ)		f1(X)+N(0,σ),SNR			= 60dB
		r	Np	Time (s)		rr	Np	Time (s)		rr	Np	Time (s)		rr	Np	Time (s)
TT-SVD	3e-4	4.4	1032	0.17	3e-4	5	1360	016	3e-4	3.7	680	0.16	1e-3	16.6	13064	0.5
TR-SVD	3e-4	4.4	1032	0.17	3e-4	5	1360	0.28	5e-4	3.6	668	0.15	1e-3	9.7	4644	0.4
TR-ALS	3e-4	4.4	1032	13.2	3e-4	5	1360	18.6	8e-4	3.6	668	4.0	1e-3	4.4	1032	11.8
TR-BALS	9e-4	4.3	1052	4.6	8e-4	4.9	1324	5.7	5e-4	3.7	728	3.4	1e-3	4.2	1000	6.1
7
Workshop track - ICLR 2018
Table 2: The results under different shifts of dimensions on functional data f2(x) with error bound
set at 10-3. For the 10th-order tensor, all 9 dimension shifts were considered and the average rank r
is compared.
r
	1	2	3	4	5	6	7	8	9
^TT-	~^2Γ	T8^	ɪ	-6T^	7^Γ~	T-	-8Γ^	14.6	TT
TR	5	4.9	5	4.9	4.9	5	5	4.8	4.9
Iter: 50%,	Iter: 250%,
RSE=0.2058	RSE=0.1430
Iter: 500%,	Iter: 5000%,
RSE=0.1270	RSE=0.1116
Figure 4: TR-SGD decomposition with TR-ranks of 12 on the 8th-order tensorization of an image.
Iter: 50% indicates that only 50% elements are sampled for learning its TR representation. RSE
indicates root relative square error kY - Y ∣∣f /kY ∣∣f .
It should be noted that TT representation has the property that ri = rd+ι = 1 and rk ,k = 2,...,d-1
are bounded by the rank of k-unfolding matrix of Thki , which limits its generalization ability
and consistency when the tensor modes have been shifted or permuted. To demonstrate this, we
consider shifting the dimensions of T of size ni ×∙∙∙× n by k times leading to Tk of size
nk+i ×∙∙∙× nd X ni ×∙∙∙× n .As shown in Table 2, the average TT-ranks are varied dramatically
along with the different shifts. In particular, when k = 8,尸材 becomes 14.6, resulting in a large
number of parameters Np = 10376. In contrast to TT, TR can obtain consistent and compact
representation with Np = 1360.
We also tested TR-ALS and TR-SGD algorithms on datasets which are generated by a TR model, in
which the core tensors are randomly drawn from N (0, 1). As shown in Table 3, TR-SGD can achieve
similar performance as TR-ALS in all cases. In particular, when data is relatively large-scale (108),
TR-SGD can achieve relative error = 0.01 by using 1% of data points only once.
Table 3: Results on synthetic data with fixed ranks ri = r = •一=2.
Tensor size	TR-ALS	TR-SGD
n=	10, d =	4	( =	二 0.01,Epoch= 19)	(	= 0.01, Epoch = 10)
n=	10, d =	6	( =	0.01, Epoch = 10)	(	0.01, Epoch = 0.4)
n=	10, d =	8	(	=0.05, Epoch = 9)	( =	0.01, Epoch = 0.01 )
5.2	Image Representation by Higher-order Tensor Decompositions
An image is naturally represented by a 2D matrix, on which SVD can provide the best low-rank
approximation. However, the intrinsic structure and high-order correlations within the image is not
well exploited by SVD. In this section, we show the tensorization of an image, yielding a higher-order
tensor, and TR decomposition enable us to represent the image more efficiently than SVD. Given
an image (e.g. ‘Peppers’) denoted by Y of size I × J, we can reshape it as Ii × I2 × . . . × Id ×
Ji × J2 × . . . × Jd followed by an appropriate permutation to Ii × Ji × I2 × J2 . . . × Id × Jd
and thus reshape it again to IiJi × I2J2 × . . . × IdJd, which is a dth-order tensor. The first mode
corresponds to small-scale patches of size Ii × Ji , while the dth-mode corresponds to large-scale
partition of whole image as Id × Jd . Based on this tensorization operations, TR decomposition is
able to capture the intrinsic structure information and provides a more compact representation. As
shown in Table 4, for 2D matrix case, SVD, TT and TR give exactly same results. In contrast, for
4th-order tensorization cases, TT needs only half number of parameters (2 times compression rate)
while TR achieves 3 times compression rate, given the same approximation error 0.1. It should be
8
Workshop track - ICLR 2018
2E」uoωs ①」dluoo
(a)	(b)
Figure 5: The comparisons of compression rate and approximation error (RSE) on CIFAR-10 dataset
by using TR and TT models.
noted that TR representation provides significantly high compression ability as compared to TT. In
addition, Fig. 4 shows TR-SGD results on ‘Lena’ image by sampling different fraction of data points.
Table 4: Image representation by using tensorization and TR decomposition. The number of
parameters is compared for SVD, TT and TR given the same approximation errors.
Data			e =	0.1	e =	0.01	e =	9e - 4	e =	2e - 15
n=	256, d	=2	SVD	TT/TR	SVD	TT/TR	SVD	TT/TR	SVD	TT/TR
			9.7e3	9.7e3	7.2e4	7.2e4	1.2e5	1.2e5	1.3e5	1.3e5
TarlCrVrir7。t∙i rm			e =	0.1	e =	0.01	e =	2e - 3	e =	1e-14
			TT	TR	TT	-^TR-	TT	TR	TT	TR
n	16, d	4	5.1e3	3.8e3	6.8e4	6.4e4	1.0e5	7.3e4	1.3e5	7.4e4
n	= 4, d =	8	4.8e3	4.3e3	7.8e4	7.8e4	1.1e5	9.8e4	1.3e5	1.0e5
n	2, d =	16	7.4e3	7.4e3	1.0e5	1.0e5	1.5e5	1.5e5	1.7e5	1.7e5
5.3	CIFAR- 1 0
The CIFAR-10 dataset consists of 60000 32 × 32 colour images. We randomly pick up 1000 images
for testing of TR decomposition algorithms. As shown in Fig. 5, TR model outperforms TT model in
terms of compression rate given the same approximation error, which is caused by strict limitation
that the mode-1 rank must be 1 for TT model. In addition, TR is a more generalized model, which
contains TT as a special case, thus yielding better low-rank approximation. Moreover, all other
TR algorithms can also achieve similar results. The detailed results for = 1e - 1 are shown in
Table 5. Note that TR-SGD can achieve the same performance as TR-ALS, which demonstrates its
effectiveness on real-world dataset. Due to high computational efficiency of TR-SGD per epoch, it
can be potentially applied to very large-scale dataset. For visualization, TR-SGD results after 10 and
100 epoch are shown in Fig. 6.
5.4	Tensorizing Neural Networks using TR representation
TT representations have been successfully applied to deep neural networks (Novikov et al., 2015),
which can significantly reduce the number of model parameters and improve computational efficiency.
To investigate the properties of TR representation, we applied TR framework to approximate the
weight matrix of a fully-connected layer and compared with TT representation. We run the experiment
on the MNIST dataset for the task of handwritten-digit recognition. The same setting of neural
network (two fully-connected layers with ReLU activation function) as in (Novikov et al., 2015) was
applied for comparisons. The input layer is tensorized to a 4th-order tensor of size 4 × 8 × 8 × 4, the
weight matrix of size 1024 × 625 is represented by a TR format of size 4 × 8 × 8 × 4 × 5 × 5 × 5 × 5.
Through deriving the gradients over each core tensor, all computations can be performed on small
core tensors instead of the dense weight matrix by using properties in Sec. 4, yielding the significant
improvements of computational efficiency. The experimental results are shown in Fig. 7. We observe
that the TR-layer provides much better flexibility than TT-layer, leading to much lower training and
9
Workshop track - ICLR 2018
Table 5: Results on CIFAR-10 images.
		Ranks	Np	Epoch	
TT-SVD	0.092	(1 7 79 67)	66099	NaN
TR-SVD	0.095	(5,3,49,58)	42710	NaN
TR-BALS	0.094	(61,13,3,6)	63278	23
TR-ALS	0.1076	(5,3,49,58)	42710	10
TR-SGD	0.1041	(5,3,49,58)	42710	100
Figure 6: The reconstructed images by using TR-SGD after 10 and 100 epochs.
testing errors under the same compression level (i.e., TT/TR ranks). In addition, TR can achieve
much better compression rate under the same level of test error. When r1 = . . . = r4 = 2, the
compression rate of dense weight matrix is up to 1300 times.
We tested the tensorizing neural networks with the same architecture on SVHN dataset
(http://ufldl.stanford.edu/housenumbers/). By setting all the TT-ranks in the network to 4, we
achieved the test error of 0.13 with compression rate of 444 times, while we can achieve the same test
error by setting all the TR-ranks to 3 with compression rate of 592 times. We can conclude that the
TR representation can obtain significantly higher compression rate under the same level of test error.
6 Conclusion
We have proposed a novel tensor decomposition model, which provides an efficient representation
for a very high-order tensor by a sequence of low-dimensional cores. The number of parameters in
2.4
口 TT-Iayer
o TR-Iayer
1.4
1.2∏
6
0.6
23456
0.8
ω <
0.4
2.3 - '
9
Ox
'—×,
J 2.2 -
E "一
ω
口 TT-Iayer
o TR-Iayer
1.9
23456
Ranks
Ranks
Figure 7: The classification performances of tensorizing neural networks by using TR representation.
10
Workshop track - ICLR 2018
our model scales only linearly to the tensor order. To optimize the latent cores, we have presented
several different algorithms: TR-SVD is a non-recursive algorithm that is stable and efficient, while
TR-BALS can learn a more compact representation with adaptive TR-ranks, TR-SGD is a scalable
algorithm which can be also used for tensor completion and online learning. Furthermore, we have
investigated the properties on how the basic multilinear algebra can be performed efficiently by
operations over TR representations (i.e., cores), which provides a powerful framework for processing
large-scale data. The experimental results verified the effectiveness of our proposed algorithms.
Acknowledgments
This work was partially supported by JSPS KAKENHI (Grant No. 17K00326) and JST CREST
(Grant No. JPMJCR1784), Japan and by the Ministry of Education and Science of the Russian
Federation (grant 14.756.31.0001).
References
Animashree Anandkumar, Rong Ge, Daniel J Hsu, Sham M Kakade, and Matus Telgarsky. Tensor
decompositions for learning latent variable models. Journal of Machine Learning Research, 15(1):
2773-2832, 2014.
J. A. Bengua, H. N. Phien, and H. D. Tuan. Optimal feature extraction and classification of tensors
via matrix product state decomposition. In 2015 IEEE International Congress on Big Data, pp.
669-672, June 2015. doi: 10.1109/BigDataCongress.2015.105.
A. Cichocki, N. Lee, I. Oseledets, A.-H. Phan, Q. Zhao, D. P Mandic, et al. Tensor networks for
dimensionality reduction and large-scale optimization: Part 1 low-rank tensor decompositions.
Foundations and Trends® in Machine Learning, 9(4-5):249-429, 2016.
L. De Lathauwer, B. De Moor, and J. Vandewalle. On the best rank-1 and rank-(R1,R2,. . .,RN)
approximation of higher-order tensors. SIAM J. Matrix Anal. Appl., 21:1324-1342, 2000. ISSN
0895-4798.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points?online stochastic
gradient for tensor decomposition. In Conference on Learning Theory, pp. 797-842, 2015.
Rainer Gemulla, Erik Nijkamp, Peter J Haas, and Yannis Sismanis. Large-scale matrix factorization
with distributed stochastic gradient descent. In Proceedings of the 17th ACM SIGKDD international
conference on Knowledge discovery and data mining, pp. 69-77. ACM, 2011.
J. Goulart, M. Boizard, R. Boyer, G. Favier, and P. Comon. Tensor cp decomposition with struc-
tured factor matrices: Algorithms and performance. IEEE Journal of Selected Topics in Signal
Processing, 2015.
S. Holtz, T. Rohwedder, and R. Schneider. The alternating linear scheme for tensor optimization in
the tensor train format. SIAM J. Scientific Computing, 34(2), 2012.
Heishiro Kanagawa, Taiji Suzuki, Hayato Kobayashi, Nobuyuki Shimizu, and Yukihiro Tagami.
Gaussian process nonparametric tensor estimator and its minimax optimality. In International
Conference on Machine Learning (ICML2016), pp. 1632-1641, 2016.
Boris N Khoromskij. Tensor numerical methods for multidimensional PDEs: theoretical analysis and
initial applications. ESAIM: Proceedings and Surveys, 48:1-28, 2015.
T.G. Kolda and B.W. Bader. Tensor decompositions and applications. SIAM Review, 51(3):455-500,
2009.
I. Laptev and T. Lindeberg. Local descriptors for spatio-temporal recognition. In Spatial Coherence
for Visual Motion Analysis, pp. 91-103. Springer, 2006.
Takanori Maehara, Kohei Hayashi, and Ken-ichi Kawarabayashi. Expected tensor decomposition
with stochastic gradient descent. In AAAI, pp. 1919-1925, 2016.
11
Workshop track - ICLR 2018
S Nayar, S Nene, and Hiroshi Murase. Columbia object image library (coil 100). Department of
Comp. Science, Columbia University, Tech. Rep. CUCS-006-96, 1996.
Alexander Novikov, Dmitrii Podoprikhin, Anton Osokin, and Dmitry P Vetrov. Tensorizing neural
networks. In Advances in Neural Information Processing Systems, pp. 442-450, 2015.
Ivan V Oseledets. Tensor-train decomposition. SIAM Journal on Scientific Computing, 33(5):
2295-2317, 2011.
Bernardino Romera-Paredes, Hane Aung, Nadia Bianchi-Berthouze, and Massimiliano Pontil. Mul-
tilinear multitask learning. In International Conference on Machine Learning, pp. 1444-1452,
2013.
Edwin Stoudenmire and David J Schwab. Supervised learning with tensor networks. In D. D. Lee,
M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information
Processing Systems 29, pp. 4799-4807. Curran Associates, Inc., 2016. URL http://papers.
nips.cc/paper/6211-supervised-learning-with-tensor-networks.pdf.
Chuan-Yung Tsai, Andrew M Saxe, and David Cox. Tensor switching networks. In Advances in
Neural Information Processing Systems, pp. 2038-2046, 2016.
Yining Wang and Anima Anandkumar. Online and differentially-private tensor decomposition. In
Advances in Neural Information Processing Systems, pp. 3531-3539, 2016.
Qiang Wu, Liqing Zhang, and Andrzej Cichocki. Multifactor sparse feature extraction using convolu-
tive nonnegative tucker decomposition. Neurocomputing, 129:17-24, 2014.
Zenglin Xu, Feng Yan, and Alan Qi. Infinite Tucker decomposition: Nonparametric Bayesian models
for multiway data analysis. In Proceedings of the 29th International Conference on Machine
Learning (ICML-12), pp. 1023-1030, 2012.
Yinchong Yang, Denis Krompass, and Volker Tresp. Tensor-train recurrent neural networks for video
classification. arXiv preprint arXiv:1707.01786, 2017.
Rose Yu and Yan Liu. Learning from multiway data: Simple and efficient tensor regression. In
International Conference on Machine Learning, pp. 373-381, 2016.
Shandian Zhe, Kai Zhang, Pengyuan Wang, Kuang-chih Lee, Zenglin Xu, Yuan Qi, and Zoubin
Ghahramani. Distributed flexible nonlinear tensor factorization. In Advances in Neural Information
Processing Systems, pp. 928-936, 2016.
G. Zhou, Q. Zhao, Y. Zhang, T. Adali, S. Xie, and A. Cichocki. Linked component analysis from
matrices to high-order tensors: Applications to biomedical data. Proceedings of the IEEE, 104(2):
310-331, 2016.
12
Workshop track - ICLR 2018
A Relation to Other Models
In this section, we discuss the relations between TR model and the classical tensor decompositions
including CPD, Tucker and TT models. All these tensor decompositions can be viewed as the
transformed representation of a given tensor. The number of parameters in CPD is O(dnr) that is
linear to tensor order, however, its optimization problem is difficult and convergence is slow. The
Tucker model is stable and can approximate an arbitrary tensor as close as possible, however, its
number of parameters is O(dnr + rd) that is exponential to tensor order. In contrast, TT and TR
decompositions have similar representation power to Tucker model, while their number of paramters
is O(dnr2) that is linear to tensor order.
A.1 CP decomposition
The cannonical polyadic decomposition (CPD) aims to represent a dth-order tensor T by a sum of
rank-one tensors, given by
r
T = X uO1)。…。UOd),	(13)
α=1
where each rank-one tensor is represented by an outer product of d vectors. It can be also written in
the element-wise form given by
T(i1,...,id)= Dui(11),...,ui(dd)E,	(14)
where h∙,…，•〉denotes an inner product of a set of vectors, i.e., Uik) ∈ Rr, k = 1,...,d.
By defining Vk (ik) = diag(ui(k)) which is a diagonal matrix for each fixed ik and k, where
k = 1, . . . , d, ik = 1, . . . , nk, we can rewrite (14) as
T (i1, ...,id) = Tr(vi(i1)v2(i2) ∙ ∙ ∙ Vd(id))∙	(15)
Hence, CPD can be viewed as a special case of TR decomposition T = <(V1, . . . , V d) where the
cores Vk, k = 1, . . . , d are of size r × nk × r and each lateral slice matrix Vk(ik) is a diagonal
matrix of size r × r.
A.2 Tucker decomposition
The Tucker decomposition aims to represent a dth-order tensor T by a multilinear product between a
core tensor G ∈ Rr1× ×rd and factor matrices U(k) ∈ Rnk ×rk, k = 1,...,d, which is expressed
by
T = G ×1 U(I) ×2 …×d U(d) = [G, U(I),…，U(d)]].	(16)
By assuming the core tensor G can be represented by a TR decomposition G = <(V1, . . . , Vd), the
Tucker decomposition (16) in the element-wise form can be rewritten as
T(i1,.. .,id)
=<(V 1,..., V d) ×1 U⑴T (i1) ×2 …×d Uid)T (id)
=Tr Yd	Xrk Vk(αk)u(k)(ik, αk)
k=1 αk=1
d
=Tr Y Vk ×2 U(k)T
k=1
(17)
where the second step is derived by applying Theorem 2. Hence, Tucker model can be represented as
a TR decomposition T = <(Z1, . . . , Zd) where the cores are computed by the multilinear products
between TR cores representing G and the factor matrices, respectively, which is
Zk=Vk ×2 U(k), k=1,...,d.
(18)
13
Workshop track - ICLR 2018
A.3 TT decomposition
The tensor train decomposition aims to represent a dth-order tensor T by a sequence of cores
Gk, k = 1, . . . , d, where the first core G1 ∈ Rn1 ×r2 and the last core Gd ∈ Rrd×nd are matrices
while the other cores Gk ∈ Rrk ×nk ×rk+1 , k = 2, . . . , d - 1 are 3rd-order tensors. Specifically, TT
decomposition in the element-wise form is expressed as
T(iι,...,id) = gι(iι)τG2(i2)…Gd-1(id-1)gd(id),	(19)
where g1(i1) is the i1th row vector of G1, gd(id) is the idth column vector of Gd, and Gk(ik), k =
2, . . . , d - 1 are the ikth lateral slice matrices of Gk.
According to the definition of TR decomposition in (1), it is obvious that TT decomposition is a
special case of TR decomposition where the first and the last cores are matrices, i.e., r1 = rd+1 = 1.
On the other hand, TR decomposition can be also rewritten as
T (iι,...,id)= Tr {Z1(i1)Z2(i2)…Zd(id)}
r1
E z1(a1, i1, :)TZ2(i2) ∙ ∙ ∙ Zd-I(id-1)zd(:, id, aI)
α1=1
(20)
wherez1(α1, i1, :) ∈ Rr2 is the α1th row vector of the matrix Z1(i1) and zd(:, id,α1) is the α1th
column vector of the matrix Zd(id). Therefore, TR decomposition can be interpreted as a sum of
TT representations. The number of TT representations is r1 and these TT representations have the
common cores Zk, for k = 2, . . . , d - 1. In general, TR outperforms TT in terms of representation
power due to the fact of linear combinations of a group of TT representations. Furthermore, given a
specific approximation level, TR representation requires smaller ranks than TT representation.
B	Proofs
B.1	Proof of Theorem 1
Proof. It is obvious that (1) can be rewritten as
T(i1,i2,...,id) = Tr(Z2(i2), Z3(i3),..., Zd(id), Z1(i1))
=…=Tr(Zd(id), Z1(i1),..., Zd-1(id-1)).
Therefore, We have Tk = <(Zk+ι,..., Zd, Z1,..., Zk).	□
B.2	Proof of Property 1
Proof. According to the definition of TR decomposition, and the cores shoWn in (10), the
(i1, . . . , id)th element of tensor T3 can be Written as
T3(i1,...,id)=TrQdk=10Zk(ik) Qdk=10Yk(ik)=TrkY=d1Zk(ik)!+TrkY=d1Yk(ik)!.
Hence, the addition of tensors in the TR format can be performed by merging of their cores. □
B.3 Proof of Property 2
Proof. The multilinear product betWeen a tensor and vectors can be expressed by
C = X T (iι,…，id)u1(i1)…ud(id) = X Tr (Y Zk (ik)) u1(i1)…Ud (id)
i1 ,...,id	i1 ,...,id	k=1
=Tr
Zk(ik)uk(ik)
))
Thus, it can be Written as a TR decomposition shoWn in (11) Where each core Xk ∈ Rrk×rk+1
becomes a matrix. The computational complexity is equal to O(dnr2).	□
14
Workshop track - ICLR 2018
Algorithm 1 TR-SVD
Input: A dth-order tensor T of size (nι ×∙∙∙× nd) and the prescribed relative error »
Output: Cores Zk, k = 1, . . . , d ofTR decomposition and the TR-ranks r.
1:	Compute truncation threshold δk for k = 1 and k > 1.
2:	Choose one mode as the start point (e.g., the first mode) and obtain the 1-unfolding matrix Th1i.
3:	Low-rank approximation by applying δ1-truncated SVD: Th1i = UΣVT + E1.
4:	Split ranks r1, r2 by minr1,r2	kr1 - r2 k, s. t. r1r2 = rankδ1 (Th1i).
5:	Zι — PermUte(reshape(U, [nι, ri, r2]), [2,1,3]).
6:	Z>1 — PermUte(reshape(ΣVT, [r1,r2, Qdd=2 nj]),[2,3,1]).
7:	for k = 2 to d - 1 do
8:	Z>k-i = reshape(Z>k-i, [rknk,nk+ι …ndr]).
9:	CompUte δk-trUncated SVD: Z>k-1 = UΣVT + Ek.
10:	rk+i — rankδk (Z>k-i).
11:	Zk — reshape(U, [rk,n√rk+ι]).
12:	Z>k J reshape(ΣVT, [rk+i, Qd=k+ι n, ri]).
13:	end for
B.4 Proof of Property 3
Proof. Each element in tensor T3 can be written as
T3(i1,...,id) =Tr (UZk (ik)) Tr (UYk("k)) = Tr{(Y1Zk(ik))㊈(UYk 血))}
=Tr
kY=i
Zk(ik) Z) Yk(ik)
Hence, T3 can be also represented as TR format with its cores compUted by (12), which costs
O(dnq2).
FUrthermore, one can compUte the inner product of two tensors in TR representations. For two
tensors Ti and T2, it is defined as hTi, T2i = Pi ,...,i T3(ii, . . . , id), where T3 = Ti ~ T2.
ThUs, the inner prodUct can be compUted by applying the Hadamard prodUct and then compUting
the multilinear product between T3 and vectors of all ones, i.e., T3 ×ι UT ×2 ∙∙∙×d UT, where
Uk = 1, k = 1,..., d, which can be computed efficiently by using Property 2.	□
C Algorithms
Pseudo codes of TR-SVD and TR-BALS are provided in Alg. 1 and Alg. 2, respectively.
D Additional Experimental Results
D.1 COIL- 1 00 dataset
The Columbia Object Image Libraries (COIL)-100 dataset (Nayar et al., 1996) contains 7200 color
images of 100 objects (72 images per object) with different reflectance and complex geometric
characteristics. Each image can be represented by a 3rd-order tensor of size 128 × 128 × 3 and then
is downsampled to 32 × 32 × 3. Hence, the dataset can be finally organized as a 4th-order tensor
of size 32 × 32 × 3 × 7200. The number of features is determined by r4 × ri, while the flexibility
of subspace bases is determined by r2, r3. Subsequently, we apply the K-nearest neighbor (KNN)
classifier with K=1 for classification. For detailed comparisons, we randomly select a certain ratio
ρ = 50% or ρ = 10% samples as the training set and the rest as the test set. The classification
performance is averaged over 10 times of random splitting. In Table 6, rmax of TR decompositions
is much smaller than that of TT-SVD. It should be noted that TR representation, as compared to TT,
can obtain more compact and discriminant representations. Fig. 8 shows the reconstructed images
under different approximation levels.
15
Workshop track - ICLR 2018
Algorithm 2 TR-BALS
Input: A d-dimensional tensor T of size (nι ×∙∙∙× nd) and the prescribed relative error e?.
Output: Cores Zk and TR-ranks rk, k = 1, . . . , d.
1:	Initialize rk = 1 for k = 1, . . . , d.
2:	Initialize Zk ∈ Rrk ×nk ×rk+1 for k = 1, . . . , d.
3:	repeat k ∈ circular{1, 2, . . . , d};
4:	Compute the subchain Z 6=(k,k+1) .
5:	Obtain the mode-2 unfolding matrix Z6[=2](k,k+1) of size Qjd=1 nj/(nknk+1) × rkrk+2.
(k,k+1)	(k,k+1)	6=(k,k+1)
6:	Z(2)	J arg min T[k] - Z(2)	(Z⑵	)	.
F
7:	Tensorization of mode-2 unfolding matrix
Z(k,k+1) J folding(Z((2k),k+1)).
8:	Reshape the block core by
Z(k,k+1) J reshape(Z(k,k+1), [rkn X nk+1rk+2]).
9:	Low-rank approximation by δ-truncated SVD Z(k,k+1) = U∑Vτ.
10:	Zk J reshape(U, [rk,nk,rk+1]).
11:	Zk+1 J reshape(ΣVT, [rk+1, nk+1, rk+2]).
12:	rk+1 J rankδ(Z(k,k+1)).
13:	k J k + 1.
14:	until The desired approximation accuracy is achieved, i.e., e ≤ ep.
Figure 8: The reconstruction of Coil-100 dataset by using TR-SVD. The top row shows the original
images, while the reconstructed images are shown from the second to sixth rows corresponding to
e=0.1, 0.2, 0.3, 0.4, 0.5, respectively.
D.2 KTH video dataset
We test the TR representation for KTH video database (Laptev & Lindeberg, 2006) containing
six types of human actions (walking, jogging, running, boxing, hand waving and hand clapping)
performed several times by 25 subjects in four different scenarios: outdoors, outdoors with scale
variation, outdoors with different clothes and indoors as illustrated in Fig. 9. There are 600 video
sequences for each combination of 25 subjects, 6 actions and 4 scenarios. Each video sequence was
downsampled to 20×20×32. Finally, we can organize the dataset as a tensor of size 20× 20×32×600.
For extensive comparisons, we choose different error bound ep ∈ {0.2, 0.3, 0.4}. In Table 7, we
can see that TR representations achieve better compression ratio reflected by smaller Irmaχ, r than
that of TT-SVD, while TT-SVD achieves better compression ratio than CP-ALS. For instance, when
16
Workshop track - ICLR 2018
Table 6: The comparisons of different algorithms on Coil-100 dataset. e, rmaχ,r denote relative error,
the maximum rank and the average rank, respectively.
	e	rmax	r	Acc. (%) (ρ = 50%)	Acc. (%) (ρ = 10%)
	0.19	67	47.3	99.05	89.11
	0.28	23	16.3	98.99	88.45
TT-SVD	0.37	8	6.3	96.29	86.02
	0.46	3	2.7	47.78	44.00
	0.19	23	12.0	99.14	89.29
	0.28	10	6.0	99.19	89.89
TR-SVD	0.36	5	3.5	98.51	88.10
	0.43	3	2.3	83.43	73.20
'		I		L * *	'∙	* ∙ ♦
		a	'	LUfli	心	MUU
«	TTTTF i	ɪ	I	I 5 i	A Λ	A k i
i i 11 i«i ⅞
T*****f* TTTt♦，，煮
Figure 9: Video dataset consists of six types of human actions performed by 25 subjects in four
different scenarios. From the top to bottom, six video examples corresponding to each type of actions
are shown.
e ≈ 0.2, CP-ALS requires rmax = 300, r = 300; TT-SVD requires rmax = 139, r = 78, while
TR-SVD only requires rmax = 99, r = 34.2. For classification performance, We observe that the best
accuracy (5 × 5-fold cross validation) achieved by CP-ALS, TT-SVD, TR-SVD are 80.8%, 84.8%,
87.7%, respectively. Note that these classification performances might not be the state-of-the-art
on this dataset, we mainly focus on the comparisons of representation ability among CP, TT, and
TR decomposition frameworks. To obtain the best performance, we may apply the powerful feature
extraction methods to TT or TR representations of dataset. It should be noted that TR decompositions
achieve the best classification accuracy when e = 0.29, while TT-SVD and CP-ALS achieve their
best classification accuracy when e = 0.2. This indicates that TR decomposition can preserve more
discriminant information even when the approximation error is relatively high. This experiment
demonstrates that TR decompositions are effective for unsupervised feature representation due to
their flexibility of TR-ranks and high compression ability.
Table 7: The comparisons of different algorithms on KTH dataset. e denotes the obtained relative
error; rm,a, denotes maximum rank; r denotes the average rank; and Acc. is the classification
accuracy.
	e	rmax	r	Acc. (5 × 5-fold)
	0.20	300	300	80.8 %
CP-ALS	0.30	40	40	79.3 %
	0.40	10	10	66.8 %
	0.20	139	78.0	84.8 %
TT-SVD	0.29	38	27.3	83.5 %
	0.38	14	9.3	67.8 %
	0.20	99	34.2	78.8 %
TR-SVD	0.29	27	12.0	87.7 %
	0.37	10	5.8	72.4 %
17