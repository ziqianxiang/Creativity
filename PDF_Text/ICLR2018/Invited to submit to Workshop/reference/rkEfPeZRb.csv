title,year,conference
 Sparse communication for distributed gradient descent,2017, In Proceedingsof the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP)
 Adam: A method for stochastic optimization,2015, In 3rd International Conferenceon Learning Representations (ICLR)
 Communication quantization for data-paralleltraining of deep neural networks,2016, In Proceedings of the Workshop on Machine Learning in HighPerformance Computing Environments (MLHPC â€™16)
 Deep residual learning for image recognition,2016, In IEEEConference on Computer Vision and Pattern Recognition (CVPR)
 Learning multiple layers of features from tiny images,2009, 2009
 1-bit stochastic gradient descent and application todata-parallel distributed training of speech DNNs,2014, In INTERSPEECH
 Very deep convolutional networks for large-scale image recogni-tion,2015, In 3rd International Conference on Learning Representations (ICLR)
 Going deeper with convolutions,2015, In IEEE Conference on Computer Vision and PatternRecognition (CVPR)
 TernGrad: Ternary gradients to re-duce communication in distributed deep learning,2017, In Advances in Neural Information ProcessingSystems 31 (NIPS)
