title,year,conference
 Training a 3-node neural network is np-complete,1989, In Advances inneural information processing Systems
 Entropy-sgd: Biasinggradient descent into wide valleys,2017, ICLR
 Theloss surfaces of multilayer networks,2015, In AISTATS
 Identifying and attacking the saddle point problem in high-dimensional non-convexoptimization,2014, In Advances in neural information processing systems
 Automated inference with adaptivebatches,2017, In Artificial Intelligence and Statistics
 Sharp minima can generalize fordeep nets,2017, ICLR
 Visualization of learning in multilayer perceptron networksusing principal component analysis,2003, IEEE Transactions on Systems
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the Thirteenth International Conference on Artificial Intelligence andStatistics
 Phasemax: Convex phase retrieval via basis pursuit,2016, arXivpreprint arXiv:1610
 Qualitatively characterizing neural networkoptimization problems,2015, ICLR
 Global optimality in neural network training,2017, In Proceedingsof the IEEE Conference on Computer Vision and Pattern Recognition
 Identity matters in deep learning,2017, ICLR
 Deep Residual Learning for ImageRecognition,2016, In CVPR
 Flat minima,1997, Neural Computation
 Densely connectedconvolutional networks,2017, CVPR
 An empirical analysis of deep network losssurfaces,2016, arXiv preprint arXiv:1612
 Batch Normalization: Accelerating Deep Network Training byReducing Internal Covariate Shift,2015, 2015
 Deep learning without poor local minima,2016, NIPS
 Generalization in deep learning,2017, arXivpreprint arXiv:1710
 On large-batch training for deep learning: Generalization gap and sharp minima,2017, ICLR
 Theory of deep learning ii: Landscape of the empirical risk in deeplearning,2017, arXiv preprint arXiv:1703
 Visualizing deep network training trajectories with pca,2016, ICML WorkshopWorkshop onVisualization for Deep Learning
 Depth creates no bad local minima,2017, arXiv preprintarXiv:1702
 The loss surface of deep and wide neural networks,2017, InternationalConference on Machine Learning
 On the quality of the initial basin in overspecified neural networks,2016, InInternational Conference on Machine Learning
 Very Deep Convolutional Networks for Large-Scale ImageRecognition,2015, In ICLR
 Theoretical insights into the optimizationlandscape of over-parameterized shallow neural networks,2017, arXiv preprint arXiv:1707
 Exponentially vanishing sub-optimal local minima in multilayerneural networks,2017, arXiv preprint arXiv:1702
 Local minima in training ofneural networks,2017, stat
 An analytical formula of population gradient for two-layered relu network and itsapplications in convergence and critical point analysis,2017, ICML
 Diverse neural network learns true target functions,2017, In ArtificialIntelligence and Statistics
 Understandingdeep learning requires rethinking generalization,2017, ICLR
