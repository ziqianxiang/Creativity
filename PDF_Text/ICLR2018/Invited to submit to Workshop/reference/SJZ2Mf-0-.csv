title,year,conference
 Conditional computation inneural networks for faster models,2015, arXiv preprint arXiv:1511
 Exploiting linearstructure within convolutional networks for efficient evaluation,2014, In Advances in Neural InformationProcessing Systems
 Learning factored representations in a deepmixture of experts,2013, arXiv preprint arXiv:1312
 Spatially adaptive computation time for residual networks,2016, arXiv preprintarXiv:1612
 Pruning recurrent neural networks for improved generalizationperformance,1994, IEEE transactions on neural networks
 Adaptive computation time for recurrent neural networks,2016, arXiv preprintarXiv:1603
 Neural turing machines,2014, arXiv preprintarXiv:1410
 Banishing the homunculus: makingworking memory work,2006, Neuroscience
 Tracking the worldstate with recurrent entity networks,2017, Proceedings of the International Conference on LearningRepresentations
 Learning graphical state transitions,2017, In ICLR
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Optimal brain damage,1989, In NIPS
 Gated graph sequence neuralnetworks,2015, arXiv preprint arXiv:1511
 Dynamic deep neural networks: Optimizing accuracy-efficiency trade-offsby selective execution,2017, arXiv preprint arXiv:1701
 Scaling memory-augmented neural networks with sparse reads andwrites,2016, In Advances in Neural Information Processing Systems
 Xnor-net: Imagenetclassification using binary convolutional neural networks,2016, In European Conference on ComputerVision
 Compression of neural machinetranslation models via pruning,2016, Proceedings of the SIGNLL Conference on Computational NaturalLanguage Learning (CoNLL)
 Outrageously large neural networks: The sparsely-gated mixture-of-experts layer,2017, arXivpreprint arXiv:1701
 Episodic and semantic memory,1972, Organization of memory
 Structuring mind: The nature of attention and how it shapes consciousness,2017, OxfordUniversity Press
 Memory networks,2014, arXiv preprintarXiv:1410
