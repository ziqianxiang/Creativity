title,year,conference
 Openness of multiplication in some function spaces,2013, Taiwanese J
 Neural networks and principal component analysis: Learning from examples without local minima,1989, Neuralnetworks
 Global optimality of local search for low rank matrix recovery,2016, In Advances inNeural Information Processing Systems
 Training a 3-node neural network is np-complete,1989, In Advances in neural information processing systems
 The loss surfaces of multilayer networks,2015, In ArtificialIntelligence and Statistics
 Matrix completion has no spurious local minimum,2016, In Advances in Neural Information ProcessingSystems
 Deep learning,2016, Book in preparation for MIT Press
 Identity matters in deep learning,2016, arXiv preprint arXiv:1611
 An elementary counterexample to the open mapping principle for bilinear maps,1975, Proceedings of the AmericanMathematical Society
 Deep learning without poor local minima,2016, In Advances in Neural Information Processing Systems
 Gradient descent only converges to minimizers,2016, In Conference on LearningTheory
 Depth creates no bad local minima,2017, arXiv preprint arXiv:1702
 The loss surface of deep and wide neural networks,2017, arXiv preprint arXiv:1704
 Non-square matrix sensing without spurious local minima via the burer-monteiro approach,2016, arXiv preprint arXiv:1609
 Theoretical insights into the optimization landscape of over-parameterized shallowneural networks,2017, arXiv preprint arXiv:1707
 No bad local minima: Data independent training error guarantees for multilayer neural networks,2016, arXivpreprint arXiv:1605
 Matrix completion via nonconvex factorization: Algorithms and theory,2015, PhD thesis
 A unified computational and statistical framework for nonconvex low-rank matrix estimation,2016, arXivpreprint arXiv:1610
 Global optimality conditions for deep neural networks,2017, arXiv preprint arXiv:1707
