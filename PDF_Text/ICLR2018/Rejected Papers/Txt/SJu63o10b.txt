Under review as a conference paper at ICLR 2018
Unsupervised Metric Learning via Nonlinear
Feature Space Transformations
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we propose a nonlinear unsupervised metric learning framework to
boost of the performance of clustering algorithms. Under our framework, nonlinear
distance metric learning and manifold embedding are integrated and conducted
simultaneously to increase the natural separations among data samples. The
metric learning component is implemented through feature space transformations,
regulated by a nonlinear deformable model called Coherent Point Drifting (CPD).
Driven by CPD, data points can get to a higher level of linear separability, which
is subsequently picked up by the manifold embedding component to generate
well-separable sample projections for clustering. Experimental results on synthetic
and benchmark datasets show the effectiveness of our proposed approach over the
state-of-the-art solutions in unsupervised metric learning.
1	Introduction
Cluster analysis has broad applications in various disciplines. Grouping data samples into categories
with similar features is an efficient way to summarize the data for further processing. In measuring
the similarities among data samples, the Euclidean distance is the most common choice in clustering
algorithms. Under Euclidean distance, feature components are assigned with the same weight,
which essentially assumes all features are equally important across the entire data space. In practice,
such setup is often not optimal. Learning a customized metric function from the data samples can
usually boost the performance of various machine learning algorithms (Bellet et al., 2013). While
metric learning has been extensively researched under supervised (Xing et al., 2003; Weinberger
& Saul, 2009; Wang et al., 2012; Noh et al., 2010) and semi-supervised settings (Peng et al., 2002;
Domeniconi et al., 2001; Zhang et al., 2016; Niu et al., 2014), unsupervised metric learning (UML)
remains a challenge, in part due to the absence of ground-truth label information to define a learning
optimality. In this paper, we focus on the problem of UML for clustering.
As the goal of clustering is to capture the natural separations among data samples, one common
practice in the existing UML solutions is to increase the data separability and make the separations
more identifiable for the ensuing clustering algorithm. Such separability gain can be achieved
by projecting data samples onto a carefully chosen low-dimensional manifold, where geometric
relationships, such as the pairwise distances, are preserved. The projections can be carried out linearly,
as through the Principle Component Analysis, or nonlinearly, as via manifold learning solutions.
Under the dimension-reduced space, clustering algorithms, such as K-means, can then be applied.
Recent years have seen the developments of UML solutions exploring different setups for the low-
dimensional manifolds. FME (Nie et al., 2010) relies on the learning of an optimum linear regression
function to specify the target low-dimensional space. Abou-Moustafa et al. (2013) model local
sample densities of the data to estimate a new metric space, and use the learned metric as the basis to
construct graphs for manifold learning. Application-specific manifolds, such as Grassmann space
(Huang et al., 2015) and Wasserstein geometry (Seguy & Cuturi, 2015), have also been studied. When
utilized as a separate preprocessing step, dimensionality reduction UML solutions are commonly
designed without considering the ensuing clustering algorithm and therefore cannot be fine-tuned
accordingly.
AML (Ye et al., 2007) takes a different approach, performing clustering and distance metric learning
simultaneously. The joint learning under AML is formulated as a trace maximization problem, and
numerically solved through an EM-like iterative procedure, where each iteration consists of a data
1
Under review as a conference paper at ICLR 2018
projection step, followed by a clustering step via kernel K-means. The projection is parameterized by
an orthogonal, dimension-reducing matrix. A kernelized extension of AML was proposed in (Chen
et al., 2007). As the projection models are built on linear transformations, their capabilities to deal
with complex nonlinear structures are limited.
UML solutions performing under the original input space have also been proposed. SSO (Jiang et al.,
2011) learns a global similarity metric through a diffusion procedure that propagates smooth metrics
through the data space. CPCM (Gupta et al., 2008) relies on the ratio of within cluster variance over
the total data variance to obtain a linear transformation, aiming to improved data separability. As the
original spaces are usually high-dimensional, UML solutions in this category tend to suffer from the
local minima problem.
In light of the aforementioned limitations and drawbacks, we propose a new nonlinear UML frame-
work in this paper. Our solution integrates nonlinear feature transformation and manifold embedding
together to improve the data separability for K-means clustering. Our model can be regarded as a
fully nonlinear generalization of AML, in which the transformation model is upgraded to a geometric
model called Coherent Point Drifting (CPD) (Myronenko & Song, 2010a). Data points are driven
by CPD to reach a higher level of linear separability, which will be subsequently picked up by
the manifold embedding component to generate well-separable sample projections. At the end,
K-means is applied on the transformed, dimension-reduced embeddings to produce label predictions.
The choice of CPD is with the consideration of its capability of generating high-order yet smooth
transformations. The main contributions of this paper include the following.
•	Our proposed fully nonlinear UML solution enhances data separability through the combi-
nation of CPD-driven deformation and spectral embeddings.
•	To the best of our knowledge, this is the first work that utilizes dense, spatial varying
deformations in unsupervised metric learning.
•	The CPD optimization has a closed-form solution, therefore can be efficiently computed.
•	Our model outperforms state-of-the-art UML methods on six benchmark databases, indicat-
ing promising performance in many real-world applications.
The rest of this paper is organized as follows. Section 2 describes our proposed method in detail. It
includes the description of CPD model, formulation of our CPD based UML, optimization strategy
and the approach to kernelize our model. Experimental results are presented in Section 3 to validate
our solutions with both synthetic and real-world datasets. Section 4 concludes this paper.
2	Unsupervised Metric Learning through CPD Transformations
(CPD-UML)
Many machine learning algorithms have certain assumption regarding the distribution of the data to
be processed. K-means always produces clustering boundaries of hyperplanes, working best for the
data set made of linearly separable groups. For data sets that are not linearly separable, even they
are otherwise well-separable, K-means will fail to deliver. Nonlinearly displacing the data samples
to make them linearly separable would provide a remedy, and learning such a transformation is the
goal of our design. The application of such a smooth nonlinear transformation throughout feature
space (either input space or kernel space) would change pairwise distances among samples, which is
equivalent to assigning spatially varying metrics in different areas of the data space.
In our framework, the CPD model is chosen to perform the transformation. Originally designed
for regulating points matching, CPD moves the points U towards the target V by estimating an
optimal continuous velocity function v(x) : Rd → Rd under Tikhonov regularization framework:
T[v] = 1 Pn=ι[vi - (ui + V(Ui))]2 + 1 λ∣∣Lv∣∣2, where n is the number of samples in the dataset,
and d is the data dimension. L represents a linear differentiation operator, and λ is the regularization
parameter. The regularization term in CPD is a Gaussian low-pass filter. The optimal solution v(x) to
matching U and V can be written in the matrix format as (Myronenko & Song, 2010b):
g(xi, x1)
V(Xi) = Ψ …	=ΨG(χi,X),	(1)
g(xi, xn)
2
Under review as a conference paper at ICLR 2018
where Ψ (size d × n) is the weight matrix for the Gaussian kernel functions, g(xi , xj ) = e
(Xi-Xj
-2σ2^
σ is the width of the Gaussian filter, which controls the smoothness level of the deformation field.

2.1	Formulation of CPD-UML
Let X = {xi| Xi ∈ Rd, i = 1, ∙∙∙ ,n} denote a dataset. K-means clustering aims to partition the
samples into K groups S = {S1, S2, ..., SK}, through the minimization of the following objective
function:
K
mSin J =	||xi - μc∣∣2 whereμc = £ Xi/nc	(2)
c=1 xi ∈Sc	xi ∈Sc
Sc is the set of data samples in the c-th cluster. nc is the number of data instances in cluster Sc , and
μc is the mean of Sc
Allowing samples to be moved, we intend to learn a spatial transformation to improve the performance
of K-means clustering by making groups more linearly separable, as well as by harnessing the updated
distance measure under the transformed feature space. Let Xi be the initial location of an instance.
Through the motion in Eqn. (1), Xi will be moved to a new position Xi1:
Xi1 = Xi + v(Xi) = Xi + ΨG(Xi, X)	(3)
With Eqn. (3), Eqn. (2) can be reformulated as:
K
min J =	∣∣x1 — μ1∣∣2 whereμ1 = ɪ2 £/n；	x1 = X + ΨG(x,X)	(4)
S ,Ψ	c=1 x∈Sc1	x1 ∈Sc1
Now S1 = {S1,S1,…，SK} is a partition of the transformed dataset. μ1 is the mean vector of the
instances in cluster Sc1 . Our proposed CPD based unsupervised metric learning (CPD-UML) is
designed to learn a spatial transformation Ψ and a clustering S 1 at the same time. Eqn. (4) can be
reformulated into a matrix format through the following steps. First, put the input dataset into a
d-by-n data matrix. Second, define a Gaussian kernel function matrix for the CPD deformation as:
G=G(X,X)={G(X1,X),G(X2,X),...,G(Xn,X)}	(5)
The size of G is n-by-n. Third, let p be a vector of dimension nc-by-1 with all elements equal to one,
then the mean of the data instances within a cluster Sc can be written as μ1 = Scp/n (Zha et al.,
2001). With these three formulations, and let E be a permutation matrix, Eqn. (4) can be rewritten as:
KK
Sm1inΨ	J=X||Sc1-μcpτ||F = X ||Sc - ScPPT/nc||F
S ,Ψ	c=1	c=1
(6)
Sc = EXc = E(X + ΨG(X, X))
where Xc is the transformed data matrix. Since ||A||2F = trace(AT A), Eqn. (6) can be written in the
form of the trace operation:
K
m1in J = Xtrace((Scc(I -PPT/nc))T(Scc(I -PPT/nc)))
K
= Xtrace(Scc(I - PPT/nc)(I -PPT/nc)T(Scc)T)
c=c
As trace(AB) = trace(BA), and PTP = nc, the J in Eqn. (7) can be further reformulated as:
K
J = X trace((I -PPT/nc)(I -PPT/nc)T(Scc)TScc)
c=c
K
=Xtrace((Sc)TSc -(PT/√nc)(Sc)τSc(p/Vnc))
c=c
(7)
(8)
3
Under review as a conference paper at ICLR 2018
Similar to (Zha et al., 2001), we define a n-by-k orthonormal matrix Y as the cluster indicator matrix:
Y = [Yi,Y2,...,Yk ]	where Yc = (0,...0,p, 0...0)T/√nC	(9)
With X1 = X + ΨG(X, X) and the cluster indicator matrix in Eqn. (9), Eqn. (8) can be written into
the following:
min J = trace((X + ΨG)T (X + ΨG)) -trace(YT(X+ΨG)T(X+ΨG)Y)	(10)
Y,Ψ
To reduce overfitting, we add the squared FrobeniUs norm λ∣∣Ψ∣∣F = λtrace(ΨTΨ), to penalize
any non-smoothness in the estimated transformations. λ is a regularization parameter. Finally, our
nonlinear CPD-UML solution is formulated as a trace minimization problem, parameterized by Y
and Ψ:
min J = trace((X + ΨG)T (X + ΨG)) - trace(Y T (X + ΨG)T (X + ΨG)Y) + λtrace(ΨT Ψ)
Y,Ψ
(11)
2.2 Optimization strategy
To search for the optimal solutions of Y and Ψ, an EM-like iterative minimization framework is
adopted to update Y and Ψ alternatingly. The transformation matrix Ψ is initialized with all 0
elements, and the cluster indicator is initialized with a K-means clustering result of the input data
samples.
Optimization for Y With Ψ fixed, Eqn. (11) reduces to a trace maximization problem:
max J = trace(YTXTXY)	(12)
Since Y is an orthonormal matrix: YTY = IK, the spectral relaxation technique (Zha et al., 2001)
can be adopted to compute the optimal Y. The solution is based on Ky Fan matrix inequalities below:
Theorem. (Ky Fan) If A be a symmetric matrix with eigenvalues {λ1 ≥ λ2 ≥ ... ≥ λn}. Let the
corresponding eigenvectors be {v1, v2, ...vn}, then
K
max
YTY=IK
trace(YTAY) =	λi
i=1
where the optimal Y * is given by Y * = [vι, v, ...vκ ]Q for any arbitrary orthogonal matrix Q.
This spectral relaxation solution can be regarded as a manifold learning method that projects data
samples from the original d-dimensional space to a new K-dimensional space. In our case, the A
matrix in Ky Fan Theorem takes the form of XTX . In implementation, we first compute the K
largest eigenvectors of XTX, and then apply the traditional K-means method, under the induced
K-dimensional space, to compute the cluster assignments.
Optimization for Ψ With the Y generated from Eqn. (12), Eqn. (11) becomes a trace minimization
problem w.r.t. Ψ:
min J = trace((X + ΨG)T (X + ΨG)) - trace(Y T (X + ΨG)T (X + ΨG)Y) + λtrace(ΨT Ψ)
Ψ
(13)
Through a careful investigation of the gradient and Hessian matrix of Eqn. (13), we found the J
could be proved to a smooth convex function, with its Hessian w.r.t. Ψ being positive definite (PD)
everywhere. Therefore, the only stationary point of J, where the gradient is evaluated to 0, locates
the global minimum, and provides the optimal Ψ*. The convexity proof is given as follows.
4
Under review as a conference paper at ICLR 2018
Convexity proof of J w.r.t. Ψ: Firstly, we update J in Eqn. (13), through several straightforward
derivation steps (the details are given in Appendix A), to an equivalent form:
J = trace(XTX) + 2trace(GTΨTX) + trace(ΨGGT ΨT) - trace(YTXTXY)
- 2trace(YTGTΨTXY) -trace(ΨGYYTGTΨT) + λtrace(ΨΨT)
The gradient of J w.r.t. Ψ can then be computed as:
J=2XGT- 2XYY T GT+2ψGGT- 2ψgyy t GT+2λψ
To facilitate the convexity proof, we rewrite this gradient equation as:
∂J
—=N + ΨM; where N = 2XGT - 2XYYTGT; M = 2(G(I - YYT)GT + λI)
(14)
(15)
(16)
N is a matrix of size d × n. M is a symmetric matrix of size n × n, which can be proved positive
definite, based on the theorem in (Horn & Johnson, 2012):
Theorem. "Suppose that A ∈ Mm,n and B ∈ Mn,m with m ≤ n. Then BA has the same
eigenvalues as AB, counting multiplicity, together with an additional n - m eigenvalues equal to 0."
We know YT * Y = IK, whose eigenvalues are all 1s. Then, according to this Theorem, the
eigenvalues of YYT are 1s (multiplicity is K), and 0 (multiplicity is n - K). In the matrix M of
Eqn. (16), I - YYT is a positive semidefinite matrix as it is symmetric and its eigenvalues are either
0 or 1. G is also positive definite because it is a kernel (Gram) matrix with the Gaussian kernel.
With G being symmetric PD and λ setting to be a positive number in our algorithm, the matrix M is
guaranteed to be a PD matrix.
Expanding the gradient formulated in Eqn. (16) to individual elements ofΨ, it can be further written
as:
∂J
dψij
n
Nij + X ΨiuMuj
u=1
i ∈ d;j ∈ n
(17)
With Eqn. (17), Eqn. (16) can be resized into a vector of size d × n. Then, the Hessian matrix ofJ
w.r.t. Ψ can be calculated as below:
M11 , M12, ..., M1n, 0, ..........................., 0
M21 , M22, ..., M2n, 0, ..........................., 0
.
.
Mn1, Mn2,..., Mnn, 0,.............................., 0
0,............., 0, M11, M12,..., M1n, 0,.........., 0
0, ............, 0, M21, M22, ..., M2n, 0, ........, 0
.
.
0,.............,0,Mn1,Mn2,...,Mnn,0,...............,0
0, ..............................., 0, M11, M12, ..., M1n
0, ..............................., 0, M21, M22, ..., M2n
.
.
0,................................, 0, Mn1, Mn2,..., Mnn
It is clear that H is a symmetric matrix with size (d * n) × (d * n). The diagonal of H is composed by
d repeating M matrices. Let ~z be any non-zero column vector with size (d * n) × 1. To prove H is a
5
Under review as a conference paper at ICLR 2018
PD matrix, we want to show that z~TH~z is always positive. To this end, we rewrite z~ as [~z1, ~z2, ..., ~zd],
where z~i is the sub-column of z~ with size n × 1. Then ~zTH~z can be computed as:
~zTHz~ = [~z1T M, z~2T M, ..., z~dT M]~z = z~1T M z~1 + z~2T M z~2 + ... + z~dT M z~d	(18)
As M has been proved to be a PD matrix, each item in Eqn. (18) is positive. Therefore, the summation
~zT H~z is also positive. Since ~z is an arbitrary non-zero column vector, this shows H is PD. With the
Hessian matrix H being PD everywhere, the objective function J is convex w.r.t. Ψ. As a result, the
stationary point of J makes the unique global minimum solution Ψ*. Let Eqn. (15) equal to 0, We get
Ψ* (G(I - YYT)GT + λI) = XYYTGT - XGT	(19)
The matrix M on the left is proved PD, thus invertible. The optimal solution of Ψ is given as:
Ψ* = (XYYTGt - XGt)(G(I - YYT)Gt + λI)-1	(20)
2.3	Main algorithm
Based on the description above, our proposed CPD-UML algorithm can be summarized as the
pseudo-code beloW:
Algorithm 1 Main Algorithm OfCPD-UML
Input: Samples X, cluster number K, regularization parameter λ, smoothness parameter σ in
CPD model, and threshold τ
Output: Transformation matrix Ψ and cluster indicator matrix Y
.......................................................................................
Initialize transformation matrix Ψ using zero values;
Compute the initial cluster indicator matrix Y using spectral relaxation;
while trace value changes > τ do
Update Y as in Section 2.2 (Ky Fan Theorem);
Update Ψ as in Eqn. (20);
Compute the trace value in Eqn. (11);
end (while)
Return Ψ and Y ;
2.4	Kernelization of CPD-UML
So far, We developed and applied our proposed CPD-UML under input feature spaces. HoWever, it
can be further kernelized to improve the clustering performance for more complicated data. A kernel
principal component analysis (KPCA) based frameWork (Zhang et al., 2010) is utilized in our Work.
After the input data instances are projected into kernel spaces introduced by KPCA, CPD-UML can
be applied under the kernel spaces to learn both deformation field and clustering result, in the same
manner as it is conducted under the original input spaces.
3	Experimental Results
We performed experiments on a synthetic dataset and six benchmark datasets. Comparisons are made
With state-of-the-art unsupervised metric learning solutions.
3.1	Experiments on synthetic dataset
The tWo-moon synthetic dataset 1 Was tested in the first set of experiments. It consists of tWo classes
With 100 examples in each class. (see Fig. 1). All the samples Were treated as unlabeled samples in
the experiments. Both linear and kernel versions of our CPD-UML Were tested.
1http://manifold.cs.uchicago.edu/manifold_regularization/data.html
6
Under review as a conference paper at ICLR 2018
Linear version CPD-UML In this experiment, our CPD-UML was applied in deforming the data
samples to achieve better separability under the input space. The effectiveness of our approach is
demonstrated by comparing with the base algorithm K-means.
(a)	( b)	(c)
(d)
Figure 1: (a) clustering result of K-means; (b) clustering result of CPD-UML; (c) and (d) show the
deformation field of (b). Color figures are best viewed on screen.
The clustering results of K-means and CPD-UML are shown in Fig. 1 (a) and 1 (b) respectively. The
sample labels are distinguished using blue and red colors. The clustering results are shown using
the decision boundary. It is obvious that K-means cannot cluster the two-moon data well due to
the data’s non-separability under the input space. Our CPD-UML, on the contrary, achieves a 99%
clustering accuracy by making the data samples linearly separable via space transformations. The
deformation field of Fig. 1 (b) in the input space is shown in Fig. 1 (c) and (d). It is evident that
our nonlinear metric learning model can deform feature spaces in a sophisticated yet smooth way to
improve the data separability.
Kernel version CPD-UML In this set of experiments, various RBF kernels were applied on the
two-moon dataset to simulate linearly non-separable cases under kernel spaces. The clustering results
of kernel K-means with different RBF kernels (width = 4, 8, 16, 32) are shown in Fig. 2 (a) - 2
(d). Colors and decision boundaries stand for the same meaning as those in Fig. 1. Obviously, the
performance of kernel K-means was getting worse with sub-optimal kernels, as in 2 (b), 2 (c)
and 2 (d). Searching for an optimal RBF kernel requires cross-validation among many candidates,
which could result in a large number of iterations. This procedure can be greatly eased by our kernel
CPD-UML. The CPD transformation under kernel spaces provides a supplementary force to the
kernelization to further improve the data separability, the same as it performs under the input space.
Fig. 2 (f) - 2 (h) demonstrate the effectiveness of our CPD-UML. Same RBF kernels as in Fig. 2 (b)
- 2 (d) were used, but better clustering results were obtained. The ability to work with sub-optimal
kernels should also be regarded as a computational advantage of our model.
3.2	Experiments on benchmark datasets
Experimental Setup In this section, we employ six benchmark datasets to evaluate the performance
of our CPD-UML. They are five UCI datasets 2 : Breast, Diabetes, Cars, Dermatology, E. Coli and
the USPS_20 handwritten data. Their basic information is summarized in Appendix B.
Both linear and kernel versions of our proposed approach were tested. For linear version, K-means
method was used as the baseline for comparison. In addition, three unsupervised metric learning
solutions, AML (Ye et al., 2007), RPCA-OM (Nie et al., 2014) and FME (Nie et al., 2010) were
utilized as the competing solutions. For kernel version, the baseline algorithm is kernel K-means.
NAML (Chen et al., 2007), the kernel version of AML is adopted. Since RPCA-OM and FME do not
have their kernel version, the same kernelization strategy in 2.4 was applied to kernelize these two
solutions. RBF kernels were applied for all kernel solutions.
Each dataset was partitioned into seen and unseen data randomly. Optimal cluster centers and
parameters are determined by the seen data. Clustering performance is evaluated via the unseen data,
which are labeled directly based on their distances away from the cluster centers. Similar setups have
been used in (Nie et al., 2011; Huang et al., 2015). In the experiments, we performed 3-fold cross
validation, in which two folds were used as seen data and one fold as unseen data. In the competing
2http://archive.ics.uci.edu/ml/
7
Under review as a conference paper at ICLR 2018
-10-5	0	5	10	15	20	25
(e)
Figure 2: First row: clustering results of kernel K-means with RBF kernels width = 4, 8, 16 and 32.
Second row: results of kernel version CPD-UML with RBF kernels width = 4, 8, 16 and 32.
solutions, the hyper-parameters were searched within the same range as in their publications. In
our proposed approach, the regularization parameter λ and smooth parameter σ were searched from
{100 〜1010} and {20 〜210}, respectively. The RBF kernel width for all kernel methods is chosen
from {2-5 〜210}. Since the performance of tested methods depends on the initialization clusters,
the clustering result of K-means was applied as the initialization clusters for all the competing
solutions in each run. The performance of each algorithm was calculated over 20 runs.
Results We measured the performance using the ground truth provided in all six benchmark datasets.
Three standard performance metrics were calculated: accuracy, normalized mutual information and
purity. To better compare the tested methods in statistic, we conducted a Student’s t-test with a
p-value 0.05 between each pair of solutions for each dataset. The solutions were ranked using a
scoring schema from (Wang et al., 2012). Compared with other methods, an algorithm scores 1 if it
performs significantly better than one opponent in statistic; 0.5 if there is no significant difference,
and 0 if it is worse.
Tables 1, 2 and 3 summarize the clustering performance and ranking scores. The best performance
is identified in Boldface for each dataset. It is evident that our CPD-UML outperforms other
competing solutions in all three standard measurements with significant margins. Highest ranking
scores in the performance tables are all achieved by our kernel version approach. In addition,
significant improvements have been obtained by our proposed approach compared with the baseline
algorithm K-means and kernel K-means. It is also noteworthy that, the linear CPD-UML achieved
comparable results with the other competing methods using RBF kernels, which further demonstrates
the effectiveness of our nonlinear feature space transformation.
4	Conclusions
The proposed CPD-UML model learns a nonlinear metric and the clusters for the given data simul-
taneously. The nonlinear metric is achieved by a globally smooth nonlinear transformation, which
improves the separability of given data during clustering. CPD is used as the transformation model
because of its capability in deforming feature space in sophisticated yet smooth manner. Evaluations
on synthetic and benchmark datasets demonstrate the effectiveness of our approach. Applying the
proposed approach to other computer vision and machine learning problems are in the direction of
our future research.
8
Under review as a conference paper at ICLR 2018
Table 1: Accuracy
Algorithms	Breast	Diabetes	Cars	Dermatology	E. Coli	USPS20	Total Score
K-means	96.39 ± 0.34 (2.5)	96.42 ± 0.12 (3.5)	42.67 ± 0.35 (0.0)	69.64 ± 7.73 (4.5)	57.11 ± 3.23 (6.0)	62.29 ± 3.12 (3.0)	19.5
AML	96.34 ± 0.35 (1.5)	96.49 ± 0.09 (4.5)	43.67 ± 0.50 (1.5)	63.17 ± 3.07 (0.0)	58.87 ± 3.27 (6.0)	64.36 ± 3.93 (4.0)	17.5
RPCA-OM	96.54 ± 0.49 (3.5)	96.71 ± 0.31 (6.0)	43.81 ± 0.78 (1.5)	71.15 ± 4.50 (5.0)	58.55 ± 3.28 (6.0)	62.64 ± 3.87 (3.5)	25.5
FME	96.36 ± 0.30 (1.5)	96.57 ± 0.35 (4.5)	48.86 ± 2.11 (4.5)	77.72 ± 3.68 (8.0)	58.59 ± 3.54 (6.0)	66.27 ± 4.09 (6.0)	30.5
CPD-UML	97.06 ± 0.44 (8.5)	97.52 ± 0.19 (8.0)	45.33 ± 0.73 (3.0)	68.67 ± 7.48 (4.0)	57.01 ± 3.67 (5.5)	64.78 ± 3.64 (4.5)	33.5
Kernel K-means	96.56 ± 0.29 (4.0)	95.57 ± 0.13 (1.0)	61.71 ± 0.08 (6.0)	64.98 ± 2.02 (1.5)	55.04 ± 2.49 (3.0)	56.14 ± 4.98 (0.5)	16.0
NAML	96.56 ± 0.30 (4.0)	95.57 ± 0.15 (1.0)	62.13 ± 0.54 (7.0)	66.60 ± 2.01 (3.5)	49.62 ± 2.82 (0.5)	57.76 ± 3.82 (0.5)	16.5
r-RPCA-OM	96.76 ± 0.27 (6.5)	95.57 ± 0.14 (1.0)	49.55 ± 2.53 (4.5)	65.69 ± 3.75 (2.5)	52.72 ± 5.26 (2.0)	67.44 ± 4.29 (7.5)	24.0
r-FME	96.73 ± 0.25 (5.0)	96.79 ± 0.14 (6.5)	63.38 ± 1.51 (8.5)	79.45 ± 3.86 (8.0)	50.78 ± 2.53 (1.0)	68.13 ± 5.47 (7.5))	36.5
r-CPD-UML	96.95 ± 0.41 (8.0)	97.75 ± 0.10 (9.0)	63.77 ± 1.52 (8.5)	80.37 ± 4.80 (8.0)	63.02 ± 2.74 (9.0)	68.58 ± 2.49 (8.0)	50.5
Table 2: Normalized MUtUal Information
Algorithms	Breast	Diabetes	Cars	Dermatology	E. Coli	USPS20	Total Score
K-means	76.10 ± 1.73 (2.5)	76.31 ± 0.63 (1.0)	18.17 ± 0.50 (5.5)	80.91 ± 4.96 (4.0)	56.77 ± 2.14 (7.5)	61.73 ± 1.49 (3.0)	23.5
AML	75.83 ± 1.80 (1.5)	76.58 ± 0.11 (1.5)	20.53 ± 0.77 (8.5)	79.15 ± 2.80 (3.0)	57.81 ± 2.07 (7.5)	62.51 ± 3.25 (3.5)	25.5
RPCA-OM	77.07 ± 2.52 (3.5)	77.88 ± 1.64 (5.5)	20.04 ± 0.64 (7.5)	82.27 ± 4.29 (6.0)	51.22 ± 2.42 (4.0)	60.78 ± 2.40 (2.5)	29.0
FME	75.93 ± 1.50 (1.5)	77.01 ± 1.77 (3.0)	19.45 ± 2.44 (8.0)	84.75 ± 3.58 (8.0)	56.73 ± 2.76 (7.5)	64.01 ± 2.73 (6.0)	34.0
CPD-UML	79.63 ± 2.35 (8.5)	83.31 ± 0.64 (8.0)	18.19 ± 0.51 (5.5)	79.46 ± 5.09 (4.0)	46.98 ± 8.29 (2.0)	61.12 ± 3.32 (2.5)	30.5
Kernel K-means	77.01 ± 1.58 (4.0)	77.06 ± 0.10 (3.5)	8.50 ± 0.05 (3.0)	74.65 ± 3.62 (0.0)	51.04 ± 1.76 (4.0)	59.48 ± 3.05 (1.5)	16.0
NAML	77.01 ± 1.58 (4.0)	77.06 ± 0.11 (3.5)	1.50 ± 0.95 (0.0)	79.86 ± 1.43 (3.0)	45.66 ± 4.03 (1.5)	60.87 ± 3.11 (2.5)	14.5
r-RPCA-OM	78.19 ± 1.50 (6.5)	77.08 ± 1.47 (3.5)	3.30 ± 1.90 (1.0)	79.84 ± 2.66 (3.5)	18.36 ± 1.85 (0.0)	66.12 ± 2.66 (8.0)	22.5
r-FME	77.95 ± 1.37 (5.0)	78.32 ± 0.79 (6.5)	6.38 ± 0.62 (2.0)	85.96 ± 3.17 (8.5)	49.74 ± 2.89 (3.5)	67.02 ± 3.76 (8.0)	33.5
r-CPD-UML	79.25 ± 2.43 (8.0)	85.76 ± 0.81 (9.0)	10.12 ± 2.26 (4.0)	81.46 ± 2.65 (5.0)	57.85 ± 2.18 (7.5)	65.34 ± 2.01 (7.5)	41.0
Table 3: PUrity
Algorithms	Breast	Diabetes	Cars	Dermatology	E. Coli	USPS20	Total Score
K-means	96.39 ± 0.34 (2.5)	96.42 ± 0.12 (3.5)	62.85 ± 0.36 (4.5)	81.36 ± 4.45 (4.0)	81.17 ± 1.58 (7.5)	69.78 ± 2.54 (3.5)	25.5
AML	96.34 ± 0.35 (1.5)	96.49 ± 0.09 (4.5)	62.91 ± 0.38 (4.5)	78.94 ± 1.81 (2.5)	81.39 ± 1.42 (7.5)	71.08 ± 4.32 (4.5)	25.0
RPCA-OM	96.54 ± 0.49 (3.5)	96.71 ± 0.31 (6.0)	63.85 ± 0.80 (7.0)	82.64 ± 3.85 (5.0)	74.50 ± 1.85 (2.5)	69.60 ± 3.58 (3.5)	27.5
FME	96.36 ± 0.30 (1.5)	96.57 ± 0.35 (4.5)	63.86 ± 1.07 (7.0)	85.27 ± 3.20 (8.0)	80.43 ± 2.11 (7.0)	72.68 ± 3.37 (7.0)	35.0
CPD-UML	97.06 ± 0.44 (8.5)	97.52 ± 0.19 (8.0)	63.10 ± 0.49 (4.5)	81.02 ± 4.45 (3.5)	74.88 ± 5.82 (3.5)	69.80 ± 3.61 (3.5)	31.5
Kernel K-means	96.56 ± 0.29 (4.0)	95.57 ± 0.13 (1.0)	62.16 ± 0.05 (1.0)	76.80 ± 1.94 (0.0)	76.03 ± 1.57 (4.0)	65.29 ± 4.29 (0.5)	10.5
NAML	96.56 ± 0.30 (4.0)	95.57 ± 0.15 (1.0)	62.21 ± 0.32 (1.0)	79.37 ± 1.02 (3.0)	70.90 ± 3.93 (1.0)	66.69 ± 3.38 (0.5)	10.5
r-RPCA-OM	96.76 ± 0.27 (6.5)	95.57 ± 0.14 (1.0)	62.29 ± 0.41 (1.0)	79.26 ± 2.16 (3.0)	52.84 ± 5.29 (0.0)	73.46 ± 2.95 (7.5)	19.0
r-FME	96.73 ± 0.25 (5.0)	96.79 ± 0.14 (6.5)	63.38 ± 1.76 (5.5)	85.44 ± 3.16 (8.0)	76.01 ± 2.62 (4.0)	72.77 ± 3.42 (7.0)	36.0
r-CPD-UML	96.95 ± 0.41 (8.0)	97.75 ± 0.10 (9.0)	64.86 ± 0.85 (9.0)	85.59 ± 2.56 (8.0)	82.09 ± 1.74 (8.0)	74.44 ± 2.14 (7.5)	49.5
9
Under review as a conference paper at ICLR 2018
References
Karim T Abou-Moustafa, Dale Schuurmans, and Frank P Ferrie. Learning a metric space for neighbourhood
topology estimation: Application to manifold learning. In ACML, pp. 341-356, 2013.
A. Bellet, A. Habrard, and M. Sebban. A survey on metric learning for feature vectors and structured data. arXiv
preprint arXiv:1306.6709, 2013.
Jianhui Chen, Zheng Zhao, Jieping Ye, and Huan Liu. Nonlinear adaptive distance metric learning for clustering.
In 13th ACM SIGKDD KDD, pp. 123-132. ACM, 2007.
Carlotta Domeniconi, Dimitrios Gunopulos, et al. Adaptive nearest neighbor classification using support vector
machines. In NIPS, pp. 665-672, 2001.
Abhishek A Gupta, Dean P Foster, and Lyle H Ungar. Unsupervised distance metric learning using predictability.
Technical Reports (CIS), pp. 885, 2008.
Roger A Horn and Charles R Johnson. Matrix analysis. Cambridge university press, 2012.
Zhiwu Huang, Ruiping Wang, Shiguang Shan, and Xilin Chen. Projection metric learning on grassmann
manifold with application to video based face recognition. In CVPR, pp. 140-149, 2015.
Jiayan Jiang, Bo Wang, and Zhuowen Tu. Unsupervised metric learning by self-smoothing operator. In ICCV,
pp. 794-801. IEEE, 2011.
A. Myronenko and Xubo Song. Point set registration: Coherent point drift. Pattern Analysis and Machine
Intelligence, IEEE Transactions on, 32(12):2262 -2275, dec. 2010a. ISSN 0162-8828. doi: 10.1109/TPAMI.
2010.46.
Andriy Myronenko and Xubo Song. Point set registration: Coherent point drift. Pattern Analysis and Machine
Intelligence, IEEE Transactions on, 32(12):2262-2275, 2010b.
Feiping Nie, Dong Xu, Ivor Wai-Hung Tsang, and Changshui Zhang. Flexible manifold embedding: A framework
for semi-supervised and unsupervised dimension reduction. IEEE Transactions on Image Processing, 19(7):
1921-1932, 2010.
Feiping Nie, Zinan Zeng, Ivor W Tsang, Dong Xu, and Changshui Zhang. Spectral embedded clustering: A
framework for in-sample and out-of-sample spectral clustering. IEEE Transactions on Neural Networks, 22
(11):1796-1808, 2011.
Feiping Nie, Jianjun Yuan, and Heng Huang. Optimal mean robust principal component analysis. In ICML, pp.
1062-1070, 2014.
Gang Niu, Bo Dai, Makoto Yamada, and Masashi Sugiyama. Information-theoretic semi-supervised metric
learning via entropy regularization. Neural computation, 26(8):1717-1762, 2014.
Yung-Kyun Noh, Byoung-Tak Zhang, and Daniel D Lee. Generative local metric learning for nearest neighbor
classification. In Advances in Neural Information Processing Systems, pp. 1822-1830, 2010.
Jing Peng, Douglas R Heisterkamp, and HK Dai. Adaptive kernel metric nearest neighbor classification. In
ICPR, volume 3, pp. 33-36. IEEE, 2002.
Vivien Seguy and Marco Cuturi. Principal geodesic analysis for probability measures under the optimal transport
metric. In NIPS, pp. 3312-3320, 2015.
Jun Wang, Alexandros Kalousis, and Adam Woznica. Parametric local metric learning for nearest neighbor
classification. In NIPS, pp. 1601-1609, 2012.
Kilian Q Weinberger and Lawrence K Saul. Distance metric learning for large margin nearest neighbor
classification. Journal of Machine Learning Research, 10(Feb):207-244, 2009.
Eric P Xing, Michael I Jordan, Stuart J Russell, and Andrew Y Ng. Distance metric learning with application to
clustering with side-information. In NIPS, pp. 521-528, 2003.
Jieping Ye, Zheng Zhao, and Huan Liu. Adaptive distance metric learning for clustering. In CVPR, pp. 1-7.
IEEE, 2007.
Hongyuan Zha, Xiaofeng He, Chris Ding, Ming Gu, and Horst D Simon. Spectral relaxation for k-means
clustering. In NIPS, pp. 1057-1064, 2001.
10
Under review as a conference paper at ICLR 2018
Changshui Zhang, Feiping Nie, and Shiming Xiang. A general kernelization framework for learning algorithms
based on kernel pca. Neurocomputing, 73(4):959-967, 2010.
Pin Zhang, Bibo Shi, Charles D Smith, and Jundong Liu. Nonlinear metric learning for semi-supervised learning
via coherent point drifting. In ICMLA, pp. 314-319. IEEE, 2016.
A	Appendix A
In section 2.2, we optimize the object function J in Eqn. (13) by computing its gradient and Hessian
matrix w.r.t. Ψ. The Hessian matrix can be proved to be positive definite. The derivations from
Eqn. (13) to Eqn. (14) is presented in this appendix. Eqn. (13) starts as:
min J = trace((X + ΨG)T (X + ΨG)) - trace(Y T (X + ΨG)T (X + ΨG)Y) + λtrace(ΨT Ψ)
It can be expanded to:
J = trace(XTX +XTΨG+ GTΨTX +GTΨTΨG)
-trace(YTXTXY +YTXTΨGY +YTGTΨTXY	(21)
+YTGTΨTΨGY) + λtrace(ΨT Ψ)
As trace(A) = trace(AT) for any matrix A, we get trace(XT ΨG) = trace(GT ΨT X) and
trace(Y TXTΨGY ) = trace(YTGTΨTXY ). With these two equations, Eqn. (21) becomes:
J = trace(XTX) + 2trace(GTΨTX) + trace(GT ΨT ΨG))
- trace(Y TXTXY ) - 2trace(Y T GT ΨT XY )	(22)
-trace(YTGTΨTΨGY) + λtrace(ΨT Ψ)
Through some simple matrix manipulations, as well as based on the fact that trace(AB) = trace(BA)
for any matrix A and B, Eqn. (22) can be updated to Eqn. (14):
J = trace(XTX) + 2trace(GTΨTX) + trace(ΨGGT ΨT) - trace(YTXTXY)
- 2trace(YTGTΨTXY) -trace(ΨGYYTGTΨT) + λtrace(ΨΨT)
B Appendix B
In section 3.2, we employ six benchmark datasets to evaluate the performance of our CPD-UML. They
are five UCI datasets: Breast, Diabetes, Cars, Dermatology, E. coli and the USPS_20 handwritten
data. Their basic information is summarized in Table 4.
Table 4: Six benchmark datasets used in experiments. Columns show the name, numbers of samples,
attributes and classes of each dataset.__________________________________
Datasets	# Samples	# Attributes	# Classes
Breast	683	10	2
Diabetes	768	8	2
Cars	392	8	3
Dermatology	366	34	6
EcoE	336	343	8
USPS20	1854	256 一	10
11