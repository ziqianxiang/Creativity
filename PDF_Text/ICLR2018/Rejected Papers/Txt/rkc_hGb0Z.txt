Under review as a conference paper at ICLR 2018
A dynamic game approach to training robust
deep policies.
Anonymous authors
Paper under double-blind review
Ab stract
We present a method for evaluating the sensitivity of deep reinforcement learning
(RL) policies. We also formulate a zero-sum dynamic game for designing robust
deep reinforcement learning policies. Our approach mitigates the brittleness of
policies when agents are trained in a simulated environment and are later exposed
to the real world where it is hazardous to employ RL policies. This framework for
training deep RL policies involve a zero-sum dynamic game against an adversarial
agent, where the goal is to drive the system dynamics to a saddle region. Using
a variant of the guided policy search algorithm, our agent learns to adopt robust
policies that require less samples for learning the dynamics and performs better
than the GPS algorithm. Without loss of generality, we demonstrate that deep
RL policies trained in this fashion will be maximally robust to a “worst” possible
adversarial disturbances.
1	Introduction
Deep reinforcement learning (RL) for complex agent behavior in realistic environments usually
combines function approximation techniques with learning-based control. A good RL controller
should guarantee fulfillment of performance specifications under external disturbances, or modeling
errors. Quite often in practice, however, this is not the case - with deep RL policies not often gen-
eralizing well to real-world scenarios. This can be attributed to the inherent differences between the
training and testing environments. Recently, there have been efforts at integrating function approx-
imation techniques with learning-based control, in an end-to-end fashion, in order to have systems
that optimize objectives while guaranteeing generalization to environmental uncertainties. Exam-
ples include trajectory-based optimization for known dynamics ([17, 26]), or trajectory optimization
for unknown dynamics such as guided policy search algorithms [1, 14, 16].
While these methods produce performance efficiency for agent tasks in the real world, there are
sensitivity questions of such policies that need to be addressed such as, how to guarantee maximally
robust deep RL policies in the presence of external disturbances, or modeling errors. A typical
approach employed in minimizing sample inefficiency is to engineer an agent’s policy in a simu-
lated environment, and later transfer such policies to physical environments. However, questions of
robustness persist in such scenarios as the agent often has to cope with modeling errors and new
sensory inputs from a different environment. For continuous control tasks, learned policies may
become brittle in the presence of external perturbations, or a slight change in the system dynamics
may significantly affect the performance of the learned controller [21] - defeating the purpose of
having a robust policy that is learned through environmental interaction .
The contribution of this paper is two-fold:
•	first, we provide a framework that demonstrates the brittleness of a state-of-the-art deep
RL policy; specifically, given a trained RL policy, we pose an adversarial agent against
the fixed trained policy; the goal is to perturb the parameter space of the learned policy.
We demonstrate that the most sophisticated deep policies fail in the presence of adversarial
perturbations.
•	second, we formulate an iterative dynamic zero-sum, two player game, where each agent
executes an opposite reaction to its pair: a concave-convex problem follows explicitly, and
1
Under review as a conference paper at ICLR 2018
our goal is to achieve a saddle point equilibrium, where the state is everywhere defined but
possibly infinite-valued).
Noting that lack of generalization of learned reward functions to the real-world can be thought of as
external disturbance that perturb the system dynamics, we formulate the learning of robust control
policies as a zero-sum two player Markov game - an iterative dynamic game (iDG) - that pits an
adversarial agent against a protagonist agent.
The controller aims to minimize a given cost while the second agent, an adversary aims to maximize
the given cost in the presence of an additive disturbance. We run the algorithm in finite episodic
settings and show a dynamic game approach aimed at generating policies that are maximally robust.
The content of this paper is thus organized: we review relevant literature to our contribution in
Sec. 2; we then provide an H∞ background in Sec. 3. This H∞ technical introduction will be
used in formulating the design of perturbation signals in Sec. 4. Without loss of generality, we
provide a formal treatment of the iDG algorithm within the guided policy search framework in
Sec. 5. Experimental evaluation on multiple robots is provided in Sec. 6 followed by conclusions in
Sec. 7.
2	Related Work
Robustness studies in classical control have witnessed the formalization of algorithms and computa-
tion necessary to carry out stable feedback control and dynamic game tasks (e.g. [2, 15, 19]). There
now exist closed-form and iterative-based algorithms to quantify the sensitivity of a control system
and design robust feedback controllers. These methods are well-studied in classical H∞ control the-
ory. While questions of robustness of policies have existed for long in connectionist RL settings[25],
only recently have researchers started addressing the question of incorporating robustness guarantees
into deep RL controllers.
Heess et. al [9] posit that rich, robust performance will emerge if an agent is simulated in a suffi-
ciently rich and diverse environment. [9] proposed a learning framework for agents in locomotion
tasks which involved choosing simple reward functions, but exposing the agent to various levels
of difficult environments as a way of achieving ostensibly sophisticated performance objectives.
Incorporating various levels of difficulty in obstacles, height and terrain smoothness to an agent’s
environment for every episodic task, they achieved robust behaviors for difficult locomotion tasks af-
ter many episodes ( ≈ 106) of training. However, this strategy defeats one of the primary objectives
of RL namely, to make an agent discover good policies with finite data based on little interaction
with the environment. An ideal robust RL controller must come from data-efficient samples or im-
itations. Furthermore, this approach takes a qualitative measure at building robust signals into the
reward function via means such as locomotion hurdles with variations in height, slopes, and slalom
walls. We reckon that building such physical barriers for an agent is expensive in the real-world and
learning such emergent locomotion behaviors takes a long training time.
Pinto et. al. [20], posed the learning of robust RL rewards in a zero-sum, two-player markov decision
process (MDP) defined by the standard RL tuple {S, A1, A2, P, R, γ, s0}, where A1 and A2 denote
the continuous action spaces of the two players. Both players share a joint transition probability
P and reward R. Pinto’s approach assumed a knowledge of the underlying dynamics so that an
adversarial policy, ∏θdv(ut |xt), can exploit the weakness in a protagonist agent's policy, ∏prot(ut |xt).
This relied on a minimax alternating optimization process: optimizing for one set of actions while
holding the other fixed, to the end of ensuring robustness of the learned reward function. While it
introduced H∞ control as a robustness measure for classical RL problems, it falls short of adapting
H∞ for complex agent tasks and policy optimizations. Moreover, there are no theoretical analyses of
saddle-/pareto-point or Nash equilibrium guarantees and the global optimum that assures maximal
robustness at ∏θrot(ut∣xt) = ∏θdv(ut∣xt) is left unaddressed.
Perhaps the closest formulation to this work is [10]’s neural fictitious self-play for large games
with imperfect state information whereby players select best responses to their opponents’ average
strategies. [10] showed that with deep reinforcement learning, self-play in imperfect-information
environments approached a Nash equilibrium where other reinforcement learning methods diverged.
2
Under review as a conference paper at ICLR 2018
From a methodical perspective, we formulate the robustness ofRL controllers within an H∞ frame-
work (see [19]) for deep robot motor tasks. Similar to a matrix game with two agents, we let both
agents play a zero-sum, two person game where each agent’s action strategy or security level never
falls below that of the other. The ordering according to which the players act so that each player acts
optimally in a “min max” fashion does not affect the convergence to saddle point in our formulation.
We consider the case where the security levels of both players coincide so that the strategy pair of
both agents constitute a saddle-point pure strategy [3, p. 19].
3	Background and Preliminaries
Reinforcement learning in robot control tasks consists of selecting control commands u from the
space of a control policy π (often parameterized by θ) that act on a high-dimensional state x. The
x typically composed of internal (e.g. joint angles and velocities) and external (e.g. object pose,
positional information in the world) components. For a stochastic policy ∏(ut∣xt) the commands
influence the state of the robot based on the transition distribution πθ (ut|xt, t). The state and action
pairs constitute a trajectory distribution T = (xι, uι, x2, u2, ∙∙∙, XT, UT).
The performance of the robot on an episodic motor task is evaluated by an accumulated reward
function R(τ ) defined as
T-1
R(τ) =	rt (xt, ut) + rtf (xtf)
t=0
for an instantaneous reward function, rt, and a final reward, rtf . Many tasks in robot learning
domains can be formulated as above, whereby we choose a locally optimal policy πθ? that optimizes
the expectation of the accumulated reward
'∏θ = E(R(T )∣∏θ) = / R(τ )p∏θ (T )dτ,
where pπθ (T ) denotes distribution over trajectories T and is defined as
T-1
p∏θ (τ) = p(xi) ɪɪ ∏θ(Ut∣xt)p(xt+1 ∣Xt, Ut).
t=0
p(xt+1|xt, Ut) above represents the robot’s dynamics and its environment.
Given the inadequacy of value function approximation methods in managing high-dimensional con-
tinuous state and action spaces as well as the difficulty of carrying out arbitrary exploration given
hardware constraints [7], we resolve to use policy search (PS) methods as they operate in the pa-
rameter space of parameterized policies. However, direct PS are often specialized algorithms that
produce optimal policies for a particular task (often using policy gradient methods), and they come
with the negative effects of not generalizing well to flexible trajectory optimizations and large rep-
resentations e.g. using neural network policies.
Guided policy search (GPS) algorithms [1, 14, 16] are able to guide the search for parameterized
policies from poor local minima using an alternating block coordinate ascent of the optimization
problem, made up of a C-Step and an S-Step. In the C-step, a well-posed cost function is minimized
with respect to the trajectory samples, generating guiding distributions pi(Ut|xt); and in the S-
step, the locally learned time-varying control laws, pi(Ut|xt), are parameterized by a nonlinear,
neural network policy using supervised learning. The S-step fits policies of the form ∏θ (ut∣xt)=
N(μπ(xt), Σπ(Xt)) to the local controllers Pi(Ut|xt), where μπ(xt), and Σπ(Xt) are functions that
are estimated.
In order to ensure the learned policy for a dynamical system is robust to external uncertainties,
modeling and transfer learning errors, we propose an iterative dynamic game consisting of an
agent within an environment, and an adversarial agent, interacting with the original agent in the
closed-loop environment E , over a finite horizon, T (it could also be extended to the infinite hori-
zon case). The adversary could represent a spoofing agent in the world or modeling errors be-
tween the plant and dynamics. The states evolve according to the following stochastic dynamics
p(Xt+1|Xt, Ut, vt), ∀t = 0, ..., T where Xt ∈ Xt is a markovian system state, Ut ∈ Ut is the ac-
tion taken by the agent (henceforth called the protagonist), vt ∈ Vt is the action taken by an
3
Under review as a conference paper at ICLR 2018
adversarial agent. The subscripts denote time steps t ∈ [1, T], allowing for a simpler structur-
ing of the individual policies per time step [7]. The problems we consider are control tasks with
complex dynamics, having continuous state and action spaces, and with trajectories defined as
T = {xι, uι, vι, x2, u2, V2,∙∙∙, Xtf, Utf, Vtf}. At time t, the system's controller visits states with
high rewards while the adversarial agent wants to visit states with low rewards. The solution to this
zero-sum game follows with an equilibrium at the origin - a saddle-point solution ensues.
The policies that govern the behavior of the agents are defined as ∏θ(ut∣xt) and ∏θ(Vk∣Xk) re-
spectively, and the learned local linear time-varying Gaussian controllers are defined as p(uk |xk),
p(Vk|xk). In the next subsection, we show that learned motor policies, p(uk |xk), are sensitive to
minute additive disturbances; we later on propose how to build robustness to such trained neural
network policies. This is important in learning tasks where the robustness margins of a trained
controller need to be known in advance before being introduced to a new execution environment.
Our goal is to select a suitable policy parameterization, so as to assure robustness and stability
guarantees [4]. In this paper, we specifically use convex variant of the mirror descent version of
GPS [16].
4	Case for Robustness in PS Algorithms
In this section, we show why guided policy search algorithms are non-robust to even the simplest
form of perturbations - additive disturbance. Before we proceed, we note that policy search algo-
rithms are popular among the RL tools available because they have a “modest” level of robustness
built into them e.g.
•	by requiring the learning controller to start the policy parameterization from multiple initial
states,
•	adding a Kullback-Leibler (KL) constraint term to the reward function e.g. [1, 14],
•	solving a motor task in multiple ways where more than one solution exist [7],
•	or by introducing additive noise into the system as white noise e.g. differential dynamic
programming (DDP) methods [11] or their iLQG variants[23].
A fundamental drawback of these robustness mechanisms, however, is that the learned policy can
only tolerate disturbance for slightly changing conditions; parametric uncertainty is not suitably
modeled as white noise, and treating the error as an extra input might be relative to the size of the
inputs (drawn from the environment) - necessitating the need for a formal treatment of robustness
in PS algorithms.
A methodical way of solving the robustness problem
in deep RL would be to consider techniques formal-
ized in H∞ control theory, where controller sensitiv-
ity and robustness are solved within the framework
of a differential game. We conjecture that the lack
of robustness of a RL trained policy arises from the
difference between a plant model and the real sys-
tem, or the difference in learning environments. If
we can measure the sensitivity of a system, γ, then
we can aim for policy robustness by ensuring that
γ is sufficiently small to reject disturbance arising
from the training environment or modeling errors if the gain of mapping from the error space to the
disturbance is less than γ-1[27]. The figure to the right depicts the standard H∞ control problem.
Suppose G in the left inset is a plant for which we can find an internally stabilizing controller, ΣK,
that ensures stable transfer of input u to measurement y, the H∞ control objective is to find the
“worst” possible disturbance, v, which produces an undesired output, z ; we want to minimize the
effect of z. In the right inset in the figure, we treat unmodeled dynamics, transfer errors and other
uncertainty as an additional feedback to which we would like to adapt with respect to the worst
possible disturbance in a prescribed range. Σ∆ in the right inset represents these uncertainties; our
goal is to find the closed-loop optimal policy for which the plant, G, will satisfy performance re-
quirements and maintain robustness for a large range of systems Σ∆. We focus on conditions under
4
Under review as a conference paper at ICLR 2018
Algorithm 1 Guided policy search: convex linear variant
1:
2:
3:
4:
for iteration k ∈ {1, . . . , K} do
C-step: Pi — argminpi Epi(T)
PtT=1 r(xt , ut)
such that DKL(pi(τ)kπθ(τ)) ≤
S-step: ∏θ J argminθ Ei DKL(Pi(T)k∏θ(T)) (from supervised learning)
end for
which we can make the H∞ norm of the system less than a given prior, γ . Specifically, we want to
design a controller ΣK that minimizes the H∞ norm of the closed-loop transfer function Tzv from
disturbance V to output Z defined as IlTzvk∞ = SuPv kV∣2.
From the small-gain theorem, the system in the right figure above will be stable for any stable
mapping ∆ : z → v for k∆k∞< γ-1 [19]. In a differential game setting, we can consider a min-
max solution to the H∞ problem for the plant G with dynamics given by X = f (x, u, W) so that we
solve an H∞ problem that satisfies the constraint k Tzvk ∞ = SuPv k⅛ ≤ γ2, or find a control input
v kvk2
U that satisfies the constraint V = Rt=0 (ZTZt - Y2 VTVt) dt ≤ 0 for all possible disturbances V for
which x0 = 0.
We consider a differential game for which the best control u that minimizes V, and the worst distur-
bance V that maximizes V are derived from
V? = min max
uv
ZT
t=0
- γ2VtT Vtdt .
(1)
The optimal value function is determined from the Hamilton-Jacobi-Isaacs (HJI) equation,
min max ZTZt - Y2VTVt + ”—f (x, u, v) = 0
u v	∂x
from which the optimal u and V can be computed. is adjusted based on the formulation in
[16]. The dynamics Pi(xk+1|xk, uk, vk) = N (fxkxk + fukuk + fvkvk,Fk) are fitted to samples
{xik+1, xik, uik, vik} using a mixture of Gaussian models to the generated samples {xik+1, xik, uik, vik}
at iteration i for all time k. Specifically, we incorporate a normal-inverse-Wishart prior on the Gaus-
sian model as described in [14, §A.3].
4.1	Sensitivity of a learned RL Policy
This section offers guidance on testing the sensitivity of a deep neural network policy for an agent.
We consider additive disturbance to a deep RL policy. Our goal is to study the degradation of perfor-
mance of a trained neural network policy in the presence of the “worst” possible disturbance in the
parameter space of the policy; if this disturbance cannot alter the performance of the trained policy,
we have some value for the policy parameters in the prescribed range that the decision strategy is
acceptable. We follow the model described above, where Σ∆ denotes the uncertainty injected by the
adversary. We arrive at the nominal system from u to y when the transfer matrix of Σ∆ is zero. We
call Σ∆ the adversary whose control, v’s effect on the output z is to be minimized. We quantify the
effect of v on z in closed loop using a suitable cost function as a min-max criteria. This can be seen
as an H∞ norm on the system. Suppose the local actions, P(uk|xk), of the controller belong to the
policy space π = [π0, ..., πT] that maximize the expected sum of rewards
maxE ['(τ)] s.t. P(Uk∣Xk)= ∏θ(ut∣xt) ∀ (xt, ut,t),	(2)
p,πθ
Therefore, the augmented reward for the closed-loop protagonist-adversary system becomes
min max E ['(T)] s.t. P(Uk|xk) = ∏θ(ut∣xt) ∀ (xt, Ut, vt,t),
pu,πθu pv,πθv
(3)
where '(τ) = r(xt, Ut,t) + r(xtf, Utf ,tf), and '(τ) = r(xt, Ut, vt,t)+ r(xtf, Utf, Vtf ,tf). Essen-
tially, '(T) = '(T) - Y2α(vt) where α(∙) can be chosen as a function of the adversarial disturbance
Vt1. We chose α as the L2 norm of the disturbance Vt in our implementation. γ is a sensitivity 1
1This formulation assumes that Vt is a vector space, though one can define nonnegative adversary input
penalty functions in other settings, e.g. when Vt is a finite set.
5
Under review as a conference paper at ICLR 2018
parameter that adjusts the strength of the adversary by increasing the penalty incurred by its actions.
In (2), we carry out the optimization procedure by first learning the optimal policy for the controller;
we then fix this optmal policy and carry out the minimization of the augmented reward function with
the adversary in closed-loop as in (3).
As γ → ∞ in (3), the optimal closed-loop policy is for the agent to do nothing, since any action
will incur a large penalty; as γ decreases, however, the adversary’s actions have a greater effect on
the state of the closed-loop system. The (inverse of the) lowest value of γ for which the adversary’s
policy causes unacceptable performance provides a measure of robustness of the control policy
∏θ (ut∣xt). For various values of γ, the state-of-the-art robot learning policies are non-robust to
small perturbations as we show in Sec. 6.
4.2	Robust zero-sum, two-person games
To learn robust policies, we run an alternating optimization algorithm that maximizes the cost func-
tion with respect to the adversarial controller (modeled with the worst possible disturbance) and
minimizes the cost function with respect to the protagonist’s policy. We consider a two-player,
zero-sum Markov game framework for simultaneously learning policies for the protagonist and the
adversary. We seek to learn saddle-point equilibrium strategies for the zero-sum game:
T-1
min max E V' 't(xt, ut, vt),	(4)
PUt ∈π(Xt) PVt ∈π(Xt)	t=0
where we have overloaded notation such that ∏(xt) = ∏θ(UtIXt) and ∏(xt) = ∏θ(vt∣xt);
'(xt, Ut, Vt) = r(xt, Ut) + γα(vt) is the stage cost. ∏(xt) denotes that the adversarial actions are
drawn from outside of the action space of the protagonist’s policy. Fixing a value ofγ is equivalent
to an assumption on the capability of the adversary or the magnitude of a worst possible distur-
bance. To validate this proposal, we develop locally robust controllers for a trajectory optimization
problem from multiple initial states using (4) as a guiding cost; a neural network function approxi-
mator is then used to parameterize these local controllers using supervised learning. We discuss this
procedure in the next section.
4.3	ROBUST GPS
GPS adds off-policy guiding samples to a sample set: this guides the policy toward spaces of high
rewards. If p(τ) is the trajectory distribution induced by the locally linear Gaussian controller
p(ut∣xt) and p(ut∣xt) denotes the previous local controller, GPS algorithms reduce the effect of
visiting regions of low entropy by minimizing the KL divergence of the current local policy from
the previous one as follows,
DKL (p(τ)kp(τ)) = E[-r(τ)] - ηH(p)	(5)
where H is the entropy term that favors broad distributions, η is a Lagrange multiplier and the first
term forces the actions p to be high in regions of high reward. The trajectory is optimized using
optimal control principles under linear quadratic Gaussian assumptions [23]. GPS minimizes the
expected cost, Eπθ(Xt,Ut)r(Xt, Ut) over the joint distribution of state and action pairs given by the
marginals πθ(τ) = p(X1) QtT=1 p(Xt+1 IXt, Ut). GPS algorithms optimize the cost J(θ) via a split
process of trajectory optimization of local control laws and a standard supervised learning to gen-
eralize to high-dimensional policy space settings. A generic GPS algorithm is shown in Algorithm
1. During the C-step, multiple local control laws, pi(UtIXt), are generated for different initial states
x1 〜p(xι). The supervised learning stage (S-step) regresses the global policy ∏θ(UtIXt) to all the
local actions computed in the C-step. For unknown dynamics, one can fit p(Xt+1 IXt, Ut) to sampled
trajectories from the trajectory distribution underP(T). To avoid divergence in dynamics, the differ-
ence between the current and previous trajectories are constrained by the KL divergence as in step 2
in algorithm 1.
The KL divergence P from P in (5) will not optimize for a robust policy in the presence of modeling
errors, changes in environment settings or disturbance as we show in the sensitivity section in sub-
section 4.1. To make the computed neural network policy robust to these uncertainties, we propose
a zero-sum, two-person dynamic game scenario in the next section.
6
Under review as a conference paper at ICLR 2018
Algorithm 2 Robust guided policy search: unknown nonlinear dynamics
1:
2:
3:
4:
5:
6:
7:
8:
9:
for iteration k ∈ {1, . . . , K } do
Generate samples Di = {τi,j } by running pi(uk |xk) and PiNk |xk) or ∏θi(u∣Xk) and ∏θi(v∣Xk)
Fit linear-Gaussian dynamics pi(xk+1 |xk, uk, vk) using samples in Di
Fit linearized protagonist policy πθi (uk |xk) using samples in Di
Regress global policies ∏θi(uk ∣Xk), ∏θi (vk∣Xk) With samples in Di
C-Step: Pi — arg minp% maxpvi
IEp(T) PK=II(Xk, uk, Vk)] s∙t∙ DKL(Pui (,τ)kπθui (t)) ≤ e
S-step: ∏θ — argminθ maxθ ∑k,i,j Dkl(∏θ(Uk∣Xk,i,j)kPu<Uk∣Xk,i,j)) (via supervised learning)
Adjust (see [16, §4.2])
end for
5 Two-Player Zero-Sum iterative dynamic GPS
To guarantee robust performance during the training of policies of a stochastic system, We introduce
the “Worst” disturbance in the H∞ paradigm to the search for a good guiding distribution problem.
We begin by augmenting the reWard function With a term that alloWs for Withstanding a disturbing
input
'(xt, Ut, Vt, t) = r(xt, Ut, Vt,t) + Y2vτv.	(6)
Where γ2vT v alloWs us to introduce a quadratic Weighting term in the disturbing input; γ denotes the
robustness parameter. A zero-sum game folloWs explicitly: the protagonist is guided toWard regions
of high reward regions while adversary pulls in its own favorite direction - yielding a saddle-point
solution.This frameWork facilitates learning control decision strategies that are robust in the presence
of disturbances and modeling errors - improving upon the generic optimal control policies that GPS
and indeed deep RL algorithms guarantee.
5.1	Two-Player Trajectory Optimization
We propose repeatedly solving an MPC-based finite-horizon trajectory optimization problem within
the framework of DDP. Specifically, we generalize a DDP variant - the iLQG algorithm of [22], to
a two-player, zero-sum dynamic game as follows:
•	we iteratively approximate the nonlinear dynamics, X = f (xt, Ut, Vt), starting with nominal
control, ∏t; t ∈ [to,tf ], and nominal adversarial input Vt; t ∈ [to,tf ] which are assumed to
be available.
•	we run the passive dynamics with & and Vt to generate a trajectory (Xt, ∏t, Vt)
•	discretizing time, we linearize the nonlinear system, Xt, about (Xk, Uk, Vk), so that the new
state and action pairs become
δXk = Xk - VXk, δUk = Uk - UVk, δVk = Vk - VVk
δXk, δUk, and δVk are measured w.r.t the nominal vectors XVk, UVk, VVk and are not necessarily small.
The LQG approximation to the original optimal control problem and reward become
δXk+1 ≈ fXkδXk + fUkδUk + fVkδVk
'(xk, Uk, Vk) ≈ δxT'χk + δUT'Uk - γδVT'vk + 1 δxT'χχkδx + 1 δUT'□UkδU + 2YSVvT'wkδV
+ Su'uTχkδx - γδVT'yχkδx + '(Xk, Vk, Vk) + E(wt).
where single and double subscripts in the augmented reward denote first-order and second-order
derivatives respectively, and fzk are the respective Jacobians e.g. fxk = fX)Ik and fxxk =
∂Xd∂f(∙)Ik at time k, E(Wt) is an additive random noise term (folded into Vt in our implementa-
tion); the value function is the cost-to-go given by the min-max of the control sequence
V (xk) = min max 'ij (Xk, Ui, Vj).
Ui	Vi
7
Under review as a conference paper at ICLR 2018
Setting V (xkf) = `kf (xkf), where kf is the final time step, the dynamic programming problem
transforms the min-max over an entire control sequence to a series of optimizations over a single
control, which proceeds backward in time as
V(Xk) = minmax['(xk, Uk, Vk) + V(f(xk+ι, Uk+ι, Vk+ι))].
puk pvk
The Hamiltonian, '(∙) + V(∙), can be considered as a function of perturbations around the tuple
{xk, uk, Vk}. Given the intractability of solving the Bellman partial differential equation above, we
restrict our attention to the local neighborhood of the nominal trajectory by expanding a power series
about the nominal, nonoptimal trajectory similar to [18]. We proceed as follows:
•	we maintain a second-order local model of the perturbed Q-coefficients of the LQR prob-
lem, (Qk, Qxk, Quk, QVk, Qxxk, Quxk, QVxk, Quuk, QVVk)2, defined thus
Q(δXk,δUk, δVk, k) ='(Xk + δXk, Uk + δUk, Vk + δVk) - '(Xk, UkVk) - V(f (Xk, UkVk))
+V (f (xk + δxk, uk + δuk, Vk + δVk)),
• a second-order Taylor approximation of Q(δXk, δUk, δVk, k) in the preceding equation
yields
1 ≈ — 2	-1 -	T	一 0	QTXk	QTUk	QTk-		-1 -	
	δXkT δUT		Qxk QUk	QXXk QUXk	QXUk QUUk	QXVk QUVk		δXk δUk	⑺
	δVkT		QVk	QVXk	QVUk	QVVk		δVk	
•	the best possible (protagonist) action and the worst possible (adversarial) action can be
found by performing the respective arg min and arg max operations
δu? = arg min Q(δxk,δuk,δvk), and δv? = arg max Q(δxk,δuk Mk)
δUk	δVk
so that we have the following linear controllers that minimize and maximize the quadratic
Q-function respectively:
δUk? = -QU-U1 k	QTUk +	QUXk δXk +	QUVkδVk	,	δVk?	=	-QV-V1k	QTVk	+ QVXk δXk	+ QVUk δUk	.
For nonlinear systems, the inverse of the 2nd partial derivatives of the Hamiltonian with respect
to the controls must be strictly positive definite. When the inverse of the Hessians above are non-
positive-definite, we can circumvent this bottleneck by adding a suitably large positive quantity to
QU-U1 k and QV-V1k [12, 5], by replacing the Hessian with an identity matrix (which gives the steepest
descent) [23], or by multiplying by lowest eigenvalue of the matrix. We find that the protagonist
and adversary in the above-equations have a local action containing a state feedback term, G, and
an open-loop term, g, given by
gUk = -QU-U1 k[QUk + QUVkδVk], GUk = -QU-U1 k QUXk,
gVk =	-QV-V1k	[QVk	+	QVUk δUk]	,	GVk	=	-QV-V1k QVXk.	(8)
respectively. The tuple {gU , GUk, gV , GVk} can be computed efficiently as shown in (17). We can
construct linear Gaussian controllers with mean given by the deterministic optimal solutions and the
covariance proportional to the curvatures of the respective Q functions:
P(Uk |xk ) = N(U + guk + Guk δxk, QUUk),
P(Vk |xk ) = N(v + gvk + Gvk δxk, QVVk).
[13] has shown that these types of distributions optimize an objective function with maximum en-
tropy given by
arg min	E['(τ)-H(P(T))] subject to	p(xt+ι∣xt, Ut)= N (xt+i； fxt Xt + ftUt, Ft)
P(T)∈N (T)
(9)
2where Qk = '(xk, Uk, Vk,k) + V(Xk+i, k + 1). Vector subscripts indicate partial derivatives.
8
Under review as a conference paper at ICLR 2018
while p(vk |xk) optimizes
arg maχ	E['(τ) - H(P(T))] subject to p(xt+ι |xt, Vt) = N(xt+ι; fxtXt + fvtVt, Ft)
P(T)∈N (T)
(10)
where T = (xt, uti, vt) is the system's trajectory evolution over all states, i, visited by both local
controllers, and H is the differential entropy. Equation (9) produces a trajectory that follows the
widest, highest-entropy distribution while minimizing the expected cost under linearized dynamics
and quadratic cost; (10) produces an opposing trajectory to what p(uk |xk) does by maximizing the
expected cost under locally linear quadratic assumptions about the dynamics.
Note that the open-loop control strategies in (8) depend on the action of the other player. Therefore,
equations (8) ensure we have a cooperative game in which the protagonist and the adversary alter-
nate between taking best possible and worst possible local actions during the trajectory optimization
phase. This helps maintain equilibrium around the system’s desired trajectory, while ensuring ro-
bustness in local policies. Substituting (8) into (7) and equating coefficients of δxk, δuk, δvk to those
of V(Xk + δxk ,k) = V(Xk, k) + Vχkδxk + ɪδx(Vxxkδxk, We obtain a quadratic value function at
time k, through the backward pass given by (19) in the appendix.
Say, the protagonist first implements its strategy, then transmits its information to the adversary, who
subsequently chooses its strategy; it follows that the adversary can choose a more favorable outcome
since it knows what the protagonist’s choice of strategy is. It becomes obvious that the best action
for the protagonist is to choose a control strategy that is an optimal response to the choice of the
adversary determined from
δvk = min '(δxk, δuk, δvk) = max min '(δxk, δuk, δvk).
pδvk	pδvk pδuk
Similarly, if the roles of the players are changed, the protagonist response to the adversary’s worst
choice will be
δuk = max '(δxk, δuk, δvk) = min max '(δxk, δuk, δvk).
δuk	pδuk pvk
Therefore, it does not matter that the order of play is predetermined. We end up with an iterative
dynamic game, where each agent’s strategy depends on its previous actions. The update rules for
the Q coefficients are determined using a Gauss-Newton approximation and is given in (15) in the
appendices.
In the forward pass, we integrate the state equation, X, compute the protagonist,s deterministic opti-
mal policy and update the trajectory as follows:
g(xk) = Uk + guk + GUk(xk - xk)
xι = xι, xk+1 = f (xk, Uk, Vk)
(11)
Compared to previous GPS algorithms, the local controllers not only produce locally linear Gaussian
controllers that favors the widest and highest entropy, they also have robustness to disturbance and
modeling errors built into them in the H∞ sense.
We arrive at a saddle point in the energy space of the cost function and we posit that the local con-
trollers generated during the trajectory optimization phase become robust to external perturbations,
modeling errors e.t.c. We arrive at a saddle point in the energy space of the cost function and we
posit that the local controllers generated during the trajectory optimization phase become robust to
external perturbations, modeling errors e.t.c. The next section shows how we generate the function
V(xk) that guarantees saddle-point equilibria for our examples.
5.2	Estimating Dynamics Distribution
The dynamics of the two player system is given by the tuple {xit, uit, vit, xit+1} and we fit the system
dynamics using piecewise linear functions in the form of a mixture of N Gaussians as proposed in [1]
and [14] over the vectors {xit, uit, vit, xit+1}T, where the ith index represents the i-th trajectory rollout
on the robot. We build a Gaussian Mixture Model (GMM) to fit piecewise linear dynamics so that
within each GMM cluster, ki, we represent a linear Gaussian dynamics model as ki(xit+1 |xit, uit, vit)
9
Under review as a conference paper at ICLR 2018
and the marginal ki(xit|uit, vit) represents the portion of the state-actions space where our Gaussian
model is valid.
In order to avoid the GMM not being a good separator of boundaries of complex modes, we follow
[1], and use the GMM to generate a prior for the regression phase. This enables us to obtain different
linear modes at separate time steps based on the observed transitions, even when the states are
dissimilar. The correct linear mode is obtained from the empirical covariance of {xt , ut , vt} with
xt+1 in the current samples at time t. As in [1] and [14], we improve sample efficiency by refitting
the GMM at each iteration to all of the samples at all time steps from the current iteration and
the previous 3 iterations and use this to construct a good prior for the dynamics. We then obtain
linear Gaussian dynamics by fitting Gaussian distributions to samples {xit, uit, vit, xit+1} which are
then conditioned on [xt, ut, vt]T. The prior allows us to build a normal-inverse Wishart prior on the
conditioned Gaussians so that the maximum a posteriori estimates for mean μ and covariance Σ are
given by
Nm
φ + N ςe + N + m (μe- μo)(μe- μO)
ς =-------------+m------------------, μ =
N+n0
where ∑e and μe are respectively the empirical covariance and
Φ,μo,m and no are prior parameters so chosen: Φ = no∑ and μo
mμo + noμe
m+ n0
mean of the dataset and
=μ. As in [14], We set
n0 = m = 1 in order to fit the prior to many samples than what is available at each time step.
5.3	Supervised learning of global neural network policies
The trajectories from the previous subsection are used to generate training data for global policies for
the controller and adversary. The local policies pτ (Uk |xk) and pτ (Vk |xk) will ideally be generated
for all possible initial states x1 〜p(xk). Since the iLQG-based linearized dynamics will only be
valid within a finite region of the state space; we used the KL-divergence constraint proposed in [16]
to ensure the current protagonist policy does not diverge too much from the previous policy.
The learning problem involves imposing KL constraints on the cost function such that the protag-
onist controller distribution agree with the global policy ∏θ (ut∣xt) by performing the following
alternating optimization between two steps at each iteration i:
K
Ep(T) El(Xk, Uk, Vk)	s.t. DKL(Pi(Uk∖Xk),∏i) ≤ e,
k=1
Πi+1 - arg min DKL(Pi(Uk ∣Xk),∏),
π∈ΠΘ
arg min max
pUi pVi
(12)
Essentially, P(Uk|xk) above generates robust local policies; The first step in (12) solves for a robust
local policy P(Uk |xk) via the min-max operation, by constraining P(Uk|xk) against its global policy
πi using the given KL divergence constraint; the second step projects the local linear Gaussian
controller distribution onto the constraint set ΠΘ, with respect to the divergence D(Pi, π). The local
policy that governs the agent’s dynamics is given by
P(Uk |xk ) = N(ɪɪ + guk + GUk δxk, QUUk).	(13)
Notice that the state is linearly dependent on the mean of the distribution P(Uk |xk) and the covariance
is independent of Vk ; we therefore end up with a linear Gaussian controller for the robust guided
policy search algorithm. For linear Gaussian dynamics and policies, the iterative KL constraint
during the S-step translates to minimizing the KL-divergence between policies i.e. ,
K
DKL(Pi(τ-)kπθ (T) = EEp(UkIXk )DKL(P(Uk IXk)kπθ (Uk IXk)).
k=1
For the nonlinear cases that we treat in this work, the KL-divergence term in the S-step above is
flipped as proposed in [16] so that Dkl(∏θ(UkIXk)kPi(UkIXk) minimizes the augmented stage cost
underPi(Uk∣Xk) w.r.t ∏θ(Uk∣Xk). Therefore, the S-step minimizes,
Epi(Xk) [DKL(πθ(UkIXk)kPi(UkIXk))] ≈
i,k
ɪ X
|Di&
DKL (πθ (Uk,i,j IXk)kPi(Uk,i,j IXk)),
10
Under review as a conference paper at ICLR 2018
(a) Peg Insertion Task	(b) Arm Swing-up Task
Figure 1: Simulation Experiments
where xk,i,j is the jth sample from pi(xk) obtained by running pi(uk|xk) on the real system, and
Di are the trajectory samples rolled out on the system. Our robust GPS algorithm is thus given in
algorithm 2. We follow the prior works in [1, 13, 14, 16] in computing the KL divergence term and
we refer readers to these works for a more detailed treatment.
6	Experimental Results
In this section, we present experiments to (i) confirm our hypothesis that guided policy search meth-
ods, with carefully engineered complex high-dimensional policies, fail when exposed to the simplest
of all perturbation signals; and (ii) answer the question of robustness using the trajectory optimiza-
tion scheme and the robust guided policy framework we have presented. We solve this under un-
known dynamics.
We answer both questions in this paper by using physics engines for policies that do not use vi-
sual features as feedback. Our validation examples are implemented in the MuJoCo physics engine
[24] and the pybox2d game engine [6], aided by the publicly available GPS codebase [8]. High-
dimensional policy experiments are implemented on a PR2 robot, while low-dimensional policy
experiments are implemented using a 2-DOF cart-pole swing-up experiment in the pybox2d game
engine. The perturbation signal we consider are those that enter additively through the reward func-
tions as described in subsection 4.1.
6.1	Sensitivity of an RL policy
We conducted simulated experiments demonstrating that guided policy search policies are sensitive
to disturbance introduced into the action space of their policies. The 7-DoF robot result presented
shortly previously appeared in our abstract that introduced robust GPS [21]. The states xk are the
joint angles, joint velocities, pose and velocity of the end effector as 3 points in 3-D. We assume the
initial velocity of the 2-link and robot arm are zero.
Experimental tasks. We simulated a 3D peg insertion task by a robot into a hole at the bottom of
the slot Fig. 1. The difficulty of this experiment stems from the discontinuity in dynamics from the
contact between the peg and the walls.
The 2-link arm swing-up experiment involves learning to balance the arm vertically about the origin
of the cart (see right inset of Fig. 1). The diffculty lies in the discontinuity of the dynamics along
the vertical axis of the arm when it is upright.
We initialized the linear-Gaussian controllers pi(uk|xk), pi(vk|xk) in the neighborhood of the initial
state x1 using a PD control law for both the inverted pendulum task and the peg insertion task.
Peg Insertion: We implement the sensitivity algorithm for the peg insertion task of [8] with a
robotic arm that requires dexterous manipulation. The robot has 12 states consisting of joint angles
11
Under review as a conference paper at ICLR 2018
Optimal adversarial costs vs. γ-penalty
Y-Penalty
Figure 2: Sensitivity Analysis for Peg Insertion Task
γ-penalty
and angular velocities with two controller states. We train the protagonist's policy using the GPS
algorithm. We then pit an adversarial disturbance against the trained policy so that the adversary
stays in closed-loop with the trained protagonist; The closed-loop cost function is given by
'(Xk, Uk, Vk)	=	1WUUTUk	+	Wp'12(dχk	-	d*)	-	YvTvk	(14)
where Y represents the disturbance term, dχk denotes the end effector,s (EE) position at state Xk
and d? denotes the EE,s position at the slot,s base. '12(Z) is a term that makes the peg reach the
target at the hole,s base, precisely given by ɪZtZ + (α + Z2)2. We set WU and WP to 10-6 and 1
respectively. For various values of γ, we check the sensitivity of the trained policy and its effect
on the task performance by maximizing the cost function above w.r.t vk. We run each sensitivity
experiment for a total of 10 iterations. Fig. 2 shows that the adversary causes a sharp degradation
in the protagonist,s performance for values of γ < 1.5. This corresponds to when the GPS-trained
policy gets destabilized and the arm struggles to reach the desired target. As values of Y ≥ 1.5,
however, we find that the adversary has a reduced effect on task performance: the adversary's effect
decreases as Y gets larger. Video of this result is available at https://goo.gl/YmmdhC.
Arm Swing-Up: Similar to the peg insertion task, we carry out a sensitivity evaluation procedure as
we did for the robot arm with the peg insertion experiment with a 2D arm. The goal is to balance
a 2D arm vertically about its origin. This agent has 7 states made up of two joint angles, two joint
angle velocities and a 3D end effector point. The action space has two dimensions. Contrary to
the example in [8] that uses the Bregman alternating direction method of multipliers algorithm, we
implement this experiment using the mirror descent GPS algorithm. We proceed as before: first,
we optimize the optimal global policy using GPS on the agent; we then fix the agent,s policy and
pit various adversarial disturbances, controlled by the Y robustness term in order to evaluate its
sensitivity. We use a similar cost function as the one used for the peg insertion task. Fig. 3 shows
the evolution of the cost function as we vary the values of y. We notice that the augmented reward
function gets larger as the adversary,s torque increases in magnitude and for lower values of Y,
the augmented cost is relatively low stays the same. The values of y < 1012 in Fig. 3 represent the
disturbance band where the protagonists learned policy becomes unstable and the arm never reaches
the vertical position (see videos here: https://goo.gl/52rKnt). This experiment further confirms that
the state-of-the-art reinforcement learning algorithms fail in the presence of additive disturbances
to their parameter space making them brittle when used in situations that call for robustness. To
mitigate these sensitivity errors, we implement the robust two-player, zero-sum game framework
provided in 5 in order to develop more robust deep RL controllers and mitigate modeling errors and
uncertainty.
12
Under review as a conference paper at ICLR 2018
Inverted pendulum task: adv. costs vs. γ-penalty
tsoc egareva
102.55
Robust GPS on Robot Peg Insertion Task
554
. 4.
.
20
101	104	107	1010	1013	1016
Iteration samples
43 2 1
0000
1111
tsoc lairasrevda lamitpo
10-8 10-4 100 104 108 1012
γ-penalty
Figure 3: [LEFT]: Cost of running the dynamic game-based robust guided policy search algoruithm
for various values of gamma for the robot peg insertion task. Our algorithm uses lesser number for
the Gaussian mixture models and requires fewer samples to generalize to the real-world. RIGHT:
Sensitivity Analysis for Arm Swing-up Task
6.2	Robust RL with GPS
As proposed in section 5, our goal is to improve the robustness of the controller’s policy in the pres-
ence of modeling errors and uncertainties and transfer errors. We follow the formulation in section 5
and generate vk from zero-mean, unit variance noise samples in every iteration. We employ various
values of γ as a robustness parameter and we run the dynamic game during the trajectory optimiza-
tion phase of the GPS algorithm. Specifically, for the values of γ that the erstwhile policies in the
previous subsection fail, we run the dynamic game algorithm to provide robustness in performnace
at test time compared against the GPS algorithm. We run experiments on the peg insertion task to
verify the algorithm. Figure 3 shows the cost of running the robust GPS algorithm on the 7-DoF
robot. We see that the policies that show achieve optimal performance behavior are now less costly
compared to vanilla GPS algorithm. For values of the sensitivity term γ that the algorithm erstwhile
fails in, we now see smoother execution of the trajectory in trying to achieve our goal. The model-
ing phase of the algorithm is also much less data consuming as our GMM algorithm now takes less
samples before generalizing to the global model.
7	Conclusion and future work
We have evaluated the sensitivity of select deep reinforcement learning algorithms and shown that
despite the most carefully designed policies, such policies implemented on real-world agents exhibit
a potential for disastrous performances when unexpected such as when there exist modeling errors
and discrepancy between training environment and real-world roll-outs (as evidenced by the results
from the two dynamics the agent faces in our sensitivity experiment). We then test the dynamic
trajectory optimization two-player algorithm on a robot motor task using Levine et al’s [14]’s guided
policy search algorithm. In our implementation of the dynamic game algorithm, we focus on the
robustness parameters that cause the robot’s policy to fail in the presence of the erstwhile sensy-
sensitivity parameter. We demonstrate that our two-player game framework allows agents operating
under nonlinear dynamics to learn the underlying dynamics under significantly more finite samples
than vanilla GPS algorithm does - thus improving upon the Gaussian model mixture method used
in [1] and [14].
Having agents that are robust to unmodeled nonlinearities, dynamics, and high frequency modes in
a nonlinear dynamical system has long been a fundamental question that control theory strives to
achieve. To the best of our knowledge, we are not aware of other works that addresses the robustness
of deep policies that are trained end-to-end from a maximal robustness perspective. In future work,
we hope to replace the crude Gaussian Mixture Model for the dynamics with a more sophisticated
nonlinear model, and evaluate how the agent behaves in the presence of unknown dynamics.
13
Under review as a conference paper at ICLR 2018
References
[1]	Learning Complex Neural Network Policies with Trajectory Optimization. Proceedings of the
31stInternational Conference on Machine Learning (ICML-14), 32:829-837, 2014. 1, 3, 4, 9,
10, 11, 13
[2]	Tamer BaSar and Pierre Bernhard. H-infinity Optimal Control And Related Minimax Design
Problems: A Dynamic Game Approach. Springer Science & Business Media, 2008. 2
[3]	Basar, Tamer and Olsder, Geert Jan. Dynamic Noncooperative Game Theory. Academic Press,
New York, 1999. 3
[4]	Dimitri Bertsekas. Dynamic Programming and Optimal Control. Athena Scientific, Nashua,
NH, USA, 2005. 4
[5]	T.E. Bullock and G.F. Franklin. IEEE Transactions on Automatic Control, pp. AC-12, 666,
1967. 8, 16
[6]	Erin Catto. Pybox2d. URL https://github.com/pybox2d/pybox2d. 11
[7]	Marc Peter Deisenroth, Gerhard Neumann, and Jan Peters. A Survey on Policy Search for
Robotics. Foundations and Trends in Robotics, 2(1):1-142, 2011. doi: 10.1561/2300000021.
3, 4
[8]	C. Finn, M. Zhang, J. Fu, X. Tan, Z. McCarthy, E. Scharff, and S. Levine. Guided Policy
Search Code Implementation, 2016. URL http://rll.berkeley.edu/gps. Software
available from rll.berkeley.edu/gps. 11, 12
[9]	Nicolas Heess, Dhruva Tb, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, Yuval
Tassa, Tom Erez, Ziyu Wang, S M Ali Eslami, Martin Riedmiller, and David Silver. Emergence
of Locomotion Behaviours in Rich Environments. 2017. 2
[10]	Johannes Heinrich and David Silver. Deep Reinforcement Learning from Self-Play in
Imperfect-Information Games. 2016. URL http://arxiv.org/abs/1603.01121. 2
[11]	David H. Jacobson and David Q. Mayne. Differential Dynamic Programming. American
Elsevier Publishing Company, Inc., New York, NY, 1970. 4
[12]	H.J. Kelley. AIAA Astrodynamics Specialists Conference, Yale University, August 1963. 8, 16
[13]	Sergey Levine and Vladlen Koltun. Guided Policy Search. Proceedings of the 30th Interna-
tional Conference on Machine Learning, 28:1-9, 2013. 8, 11
[14]	Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-End Training of Deep
Visuomotor Policies. Journal of Machine Learning Research, 17:1-40, 2016. ISSN 15337928.
doi: 10.1007/s13398-014-0173-7.2. 1, 3, 4, 5, 9, 10, 11, 13
[15]	Michael L Littman. Markov Games as a Framework for Multi-agent Reinforcement Learning.
In Proceedings of the Eleventh International Conference on Machine Learning, volume 157,
pp. 157-163, 1994. 2
[16]	William Montgomery and Sergey Levine. Guided Policy Search as Approximate Mirror De-
scent. arXiv preprint arXiv:1607.04614, 2016. 1, 3, 4, 5, 7, 10, 11
[17]	Igor Mordatch, Kendall Lowrey, Galen Andrew, Zoran Popovic, and Emanuel V Todorov.
Interactive control of diverse complex characters with neural networks. In Advances in Neural
Information Processing Systems, pp. 3132-3140, 2015. 1
[18]	J. Morimoto, G. Zeglin, and C.G. Atkeson. Minimax Differential Dynamic Programming:
Application to A Biped Walking Robot. Proceedings of the 2003 IEEE/RSJ International
Conference on Intelligent Robots and Systems., 2(October):1927-1932, 2003. doi: 10.1109/
IROS.2003.1248926. 8
[19]	Jun Morimoto and Kenji Doya. Robust Reinforcement Learning. Neural computation, 17(2):
335-359, 2005. 2, 3, 5
14
Under review as a conference paper at ICLR 2018
[20]	Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. Robust Adversarial
Reinforcement Learning. arXiv preprint arXiv:1703.02702, 2017. 2
[21]	Tyler Summers, Olalekan Ogunmolu, and Nicholas Gans. Robustness Margins and Robust
Guided Policy Search for Deep Reinforcement Learning. IEEE/RSJ International Conference
on Robots and Intelligent Systems, (Abstract Only Track), 2017. 1, 11
[22]	Yuval Tassa, Tom Erez, and Emanuel Todorov. Synthesis and Stabilization of Complex Be-
haviors through Online Trajectory Optimization. IEEE/RSJ International Conference on Intel-
ligent Robots and Systems, October 2012. doi: 10.1109/IROS.2012.6386025. 7
[23]	Emanuel Todorov and Weiwei Li. A Generalized Iterative Lqg Method For Locally-optimal
Feedback Control Of Constrained Nonlinear Stochastic Systems. 43rd IEEE Conference on
Decision and Control, 2004. 4, 6, 8
[24]	Emanuel Todorov, Tom Erez, and Yuval Tassa. MuJoCo: A Physics Engine for Model-based
Control. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference
on,pp. 5026-5033. IEEE, 2012. 11
[25]	Ronald J Williams. Simple Statistical Gradient-Following Algorithms for Connectionist Rein-
forcement Learning. Machine Learning, 8:229-256, 1992. 2
[26]	Tianhao Zhang, Gregory Kahn, Sergey Levine, and Pieter Abbeel. Learning deep control
policies for autonomous aerial vehicles with mpc-guided policy search. In Robotics and Au-
tomation (ICRA), 2016 IEEE International Conference on, pp. 528-535. IEEE, 2016. 1
[27]	Kemin Zhou, John Comstock Doyle, Keith Glover, et al. Robust and optimal control, vol-
ume 40. Prentice hall New Jersey, 1996. 4
APPENDIX I
The Q-coefficients are estimated using LQR as follows:
Qxk Qvk Quxk Quuk	二 'xk + fTkνxk + 1,	Quk = 'uk + fuTkVxk + 1 ' 'vk + fvTkνxk + 1	Qxxk = 'xxk + fxTkVxxk+1fxk	(⑸ 二 'uxk + fTk Vxxk+1fxk	Qvxk = 'vxk + fvk Vxxk + 1fxk `uuk + fuTkVxxk+1fuk	Qvvk = `vvk + fvTkVxxk+1fvk,
where the subscript terms denote the partial derivatives with respect to the given matrix.
APPENDIX II
The closed-form equations for the closed loop policy obtained after substituting (8) are given by
δuk = — Q-Uk	[QTk	+ QuXkδxk + Quvkδvk]	,	δvk	=	— Q-Vk	∣QTk + Qvxkδxk	+ Qvukδuj	. (16)
Solving the system of equations in (16), we find that
δuk = Quunk(I — QUukQuvkQ-VkQTvk)]	[(QuvkQ-VkQvxk — Quxk) δχk + (QuvkQ-vkQvk — QTk)]
(17)
δvk = [Quvk (I — Q-vkQvukQ-uk) Qvvk] 1 [(QvukQ-ukQuxk — Qvxk) δxk + (QvukQ-ukQTk — QTk)]
Suppose we let
1
Kuk
Kvk = [Qvvk (I — Q-vk Qvuk Q-uk Quvk)] ι,
15
Under review as a conference paper at ICLR 2018
so that
guk
gvk
Kuk(QUvk Qvvk Qvk — QTk), Guk = Kuk (QUvk Qvvk QvXk — QUXk) and
KVk(QvUk Q-UkQTk — QTk), Gvk = Kvk (Qvuk QUUk QUXk — Qvxk)
it follows that We can rewrite δU? and δv? as
δu? = gUk + Guk δχk,	δv? = gvk + GVk δxk.
(18)
Plugging δU? and δv? back into the Q function expansion in (7), we find that
Q(δXk ,δUk ,δvk,k) = QTk guk + QvkgVk + 1 gTcQuukguk + 1 gTkQvvk gvk
(QTXk + QTUk GUk + QTvk Gvk + gUTk QUUk GUk + gvTk Qvvk Gvk + gUTk QUXk
+ gvTk QvXk + gUTk QUvk Gvk + gvTk QTUvk GUk)δ Xk
+ 2 δxT(Qχχk + GTC Quuk Gu + GT Qvvk GV + 2GTc QUxk
+ 2GvTk QvXk + 2GUTk QUvk Gvk)δ Xk
Comparing coefficients, we obtain the following for the value function’s coefficients
Vk - Vk + 1 = QTkguk + QTkgVk + 2gTkQUUkguk + 2gTcQvvkgVk + gUkQUvkgvk
VXk = QXTk + QTUk GUk + QTvk Gvk + gUTk QUUk GUk + gvTk Qvvk Gvk + gUTk QUXk
(19)
+ gvTkQvxk + guTkQuvkGvk + gvTkQTuvkGuk
Vxxk = Qxxk + GuTk Quuk Guk + GvTk Qvvk Gvk + 2GuTk Quxk + 2GvTk Qvxk + 2GuTk Quvk Gvk
In practice, the Kuk and Kvk inverse terms above will result in numerical errors when the matrices are not
positive definite since the DDP algorithm does not guarantee that the inverse of the Q functions will be positive
definite. To guarantee numerical stability and preserve the concave-convex properties, we add to Kuk and
Kvk a sufficiently large positive quantity (greater than the lowest eigenvalue) in order to regularize the update
equations [12, 5].
16