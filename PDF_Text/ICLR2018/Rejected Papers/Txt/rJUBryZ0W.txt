Under review as a conference paper at ICLR 2018
Lifelong Learning by Adjusting Priors
Anonymous authors
Paper under double-blind review
Ab stract
In representational lifelong learning an agent aims to learn to solve novel tasks
while updating its representation in light of previous tasks. Under the assumption
that future tasks are ‘related’ to previous tasks, representations should be learned
in such a way that they capture the common structure across learned tasks, while
allowing the learner sufficient flexibility to adapt to novel aspects of a new task.
We develop a framework for lifelong learning in deep neural networks that is based
on generalization bounds, developed within the PAC-Bayes framework. Learning
takes place through the construction of a distribution over networks based on the
tasks seen so far, and its utilization for learning a new task. Thus, prior knowl-
edge is incorporated through setting a history-dependent prior for novel tasks. We
develop a gradient-based algorithm implementing these ideas, based on minimiz-
ing an objective function motivated by generalization bounds, and demonstrate its
effectiveness through numerical examples.
1	Introduction
Learning from examples is the process of inferring a general rule from a finite set of examples. It
is well known in statistics (e.g., Devroye et al. (1996)) that learning cannot take place without prior
assumptions. This idea has led in Machine Learning to the notion of inductive bias (Mitchell, 1980).
Recent work in deep neural networks has achieved significant success in using prior knowledge in
the implementation of structural constraints, e.g. the use of convolutions and weight sharing as
building blocks, capturing the translational invariance of image classification. However, in general
the relevant prior information for a given task is not always clear, and there is a need for building
prior knowledge through learning from previous interactions with the world.
Learning from previous experience can take several forms: Continual learning - a single model
is trained to solve a task which changes over time (and hopefully not ‘forget’ the knowledge from
previous times, (e.g., Kirkpatrick et al. (2017)). Multi-task learning - the goal is to learn how to
solve several observed tasks, while exploiting their shared structure. Domain adaptation - the goal
is to solve a ‘target’ learning task using a single ‘source’ learning task (both are observed, but usually
the target has mainly unlabeled data). Lifelong Learning / Meta-Learning / Learning-to-Learn -
the goal is to extract knowledge from several observed tasks to be used for future learning on new
(not yet observed) learning tasks. In contrast to multi-task learning, the performance is evaluated on
the new tasks.
We work within the framework of lifelong learning, where an agent learns through interacting with
the world, transferring the knowledge acquired along its path to any new task it encounters. This
notion has been formulated by Baxter (2000) in a clear and simple context of ‘task-environment’. In
analogy to the standard single-task learning in which data is sampled from an unknown distribution,
Baxter suggested to model a lifelong learning setting as if tasks are sampled from an unknown
task distribution (environment), so that knowledge acquired from previous tasks can be used in
order to improve performance on a novel task. Baxter’s work not only provided an interesting and
mathematically precise perspective for lifelong learning, but also provided generalization bounds
demonstrating the potential improvement in performance due to prior knowledge. Baxter’s seminal
work, has led to a large number of extensions and developments.
In this contribution we work within the framework formulated by Baxter (2000), and, following
the setup in Pentina & Lampert (2014), provide generalization error bounds within the PAC-Bayes
framework. These bounds are then used to develop a practical learning algorithm that is applied to
1
Under review as a conference paper at ICLR 2018
neural networks, demonstrating the utility of the approach. The main contributions of this work are
the following. (i) An improved and tighter bound in the theoretical framework of Pentina & Lampert
(2014) which can utilize different single-task PAC-Bayesian bounds. (ii) Developing a learning
algorithm within this general framework and its implementation using probabilistic feedforward
neural networks. This yields transfer of knowledge between tasks through constraining the prior
distribution on a learning network. (iii) Empirical demonstration of the performance enhancement
compared to naive approaches and recent methods in this field.
As noted above, Baxter (2000) provided a basic mathematical formulation and initial results for life-
long learning. While there have been many developments in this field since then (e.g., Andrychow-
icz et al. (2016); Edwards & Storkey (2016); Finn et al. (2017); Ravi & Larochelle (2016)), most
of them were not based on generalization error bounds which is the focus of the present work. An
elegant extension of generalization error bounds to lifelong learning was provided by Pentina &
Lampert (2014), mentioned above (more recently extended in Pentina & Lampert (2015)). Their
work, however, did not provide a practical algorithm applicable to deep neural networks. More re-
cently, Dziugaite & Roy (2017) developed a single-task algorithm based on PAC-Bayes bounds that
was demonstrated to yield good performance in simple classification tasks. Other recent theoretical
approaches to lifelong or multitask learning (e.g. Alquier et al. (2017); Maurer et al. (2016)) provide
increasingly general bounds but have not led directly to practical learning algorithms.
2	Background: PAC-Bayes learning
In the standard setting for supervised learning a set of (usually) independent pairs of input/output
samples S = {(xi, yi)}im=1 are given, each sample drawn from an unknown probability distribution
D, namely (xi, yi)〜D. We will use the notation S 〜Dm to denote the distribution over the full
sample. The usual learning goal is, based on S to find a function h ∈ H, where H is the so-called
hypothesis space, that minimizes the expected loss function E'(h, z), where Z = (x, y)1 and '(h, Z)
is a loss function bounded in [0, 1] . As the distribution D is unknown, learning consists of selecting
an appropriate h based on the sample S. In classification H is a space of classifiers mapping the
input space to a finite set of classes. As noted in the Introduction, an inductive bias is required for
effective learning. While in the standard approach to learning, described in the previous paragraph,
one usually selects a single classifier (e.g., the one minimizing the empirical error), the PAC-Bayes
framework, first formulated by McAllester (1999), considers the construction of a complete proba-
bility distribution over H, and the selection of a single hypothesis h ∈ H based on this distribution.
Since this distribution depends on the data it is referred to as a posterior distribution and will be
denoted by Q. We note that while the term ‘posterior’ has a Bayesian connotation, the framework
is not necessarily Bayesian, and the posterior does not need to be related to the prior through the
likelihood function as in standard Bayesian analysis. The PAC-Bayes framework has been widely
studied in recent years, and has given rise to significant flexibility in learning, and, more importantly,
to some of the best generalization bounds available Audibert (2010); McAllester (2013); Lever et al.
(2013). The framework has been recently extended to the lifelong learning setting by Pentina &
Lampert (2014), and will be extended and applied to neural networks in the present contribution.
2.1	Single-task problem formulation
Following the notation introduced above we define the generalization error and the empirical error
used in the standard learning setting,
1m
er (h,D)，E '(h,z) ; er (h,S)，— X' (h,Zi)	(h ∈ H) .	(1)
Z 〜D	m
j=1
Since the distribution D is unknown, er (h, D) cannot be directly computed.
1 Note that the framework is not limited to supervised learning and can also handle unsupervised learning.
2
Under review as a conference paper at ICLR 2018
PAC-Bayesian learning In the PAC-Bayesian setting the learner outputs a distribution over the
entire hypothesis space H, i.e, the goal is to provide a posterior distribution Q ∈ M, where M
denotes the set of distributions over H. The expected (over H) generalization error and empirical
error are then given in this setting by averaging (1) over the posterior distribution,
m
er(Q,D)，E E '(h,z)	;	er(Q,S)，E — X'(h,zj)	(Q ∈ M).	(2)
h〜Q Z〜D	h〜Q m / /
j=1
This average describes a Gibbs prediction procedure - first drawing a hypothesis h from Q then
applying it on the sample z. Similarly to (1), the generalization error er (Q, D) cannot be directly
computed since D is unknown.
2.2	PAC-Bayesian generalization bound
In this section we introduce a PAC-Bayesian bound for the single-task setting. The bound will also
serve us for the lifelong-learning setting in the next sections. PAC-Bayesian bounds are based on
specifying some reference distribution P ∈ M. P is called the ‘prior’ since it must not depend on
the observed data S. The distribution over hypotheses Q which is provided as an output from the
learning process is called the posterior (since it is allowed to depended on S). 2
The classical PAC-Bayes theorem for single-task learning was formulated by McAllester (1999).
Theorem 1 (McAllester’s single-task bound). Let P ∈ Mbe some prior distribution over H. Then
for any δ ∈ (0, 1],
Ps~Dm er( (Q, D) ≤ b(Q, S) + 2 m- -^y ^Dι κ l,^q^i∖p )+log δj~), ∀q ∈ M} ≥ 1 -δ,
(3)
where DKL(Q||P) is the Kullback-Leibler Divergence (KLD),
DKL(QIP) , h^ log II.	(4)
The bound (3) can be interpreted as stating that with high probability the expected error er (Q, D)
is upper bounded by the empirical error plus a complexity term. Since, with high probability, the
bound holds for all Q ∈ M (uniformly), we can choose Q after observing the data S. By choosing
Q that minimizes the bound we will get a learning algorithm with generalization guarantees. Note
that PAC-Bayesian bounds express a trade-off between fitting the data (empirical error) and a com-
plexity/regularization term (distance from prior) which encourages selecting a ‘simple’ hypothesis,
namely one similar to the prior. The contribution of the prior-dependent regularization term to the
objective is more significant for a smaller data set. For asymptotically large sample size m, the com-
plexity term converges to zero. The specific choice of P affects the bound’s tightness and so should
express prior knowledge about the problem. Generally, we want the prior to be close to posteriors
which can achieve low training error. For example, we may want to use a prior that prefers simpler
hypotheses (Occam’s razor).
In general, the bound might not be tight, and can even be vacuous, i.e., greater than the maximal
value of the loss. However, Dziugaite & Roy (2017) recently showed that a PAC-Bayesian bound
can achieve non-vacuous values with deep-networks and real data sets. In this work our focus is
on deriving an algorithm for lifelong-learning, rather than on actual calculation of the bound. We
expect that even if the numerical value of the bound is vacuous, it still captures the behavior of the
generalization error and so minimizing the bound is a good learning strategy.
In our experiments we will plug in positive and unbounded loss functions in the bound, in contrast to
the assumption on a bounded loss. Theoretically, we can still claim that we are bounding a variation
of the loss clipped to [0, 1]. Furthermore, empirically the loss function are almost always smaller
than one.
2As noted above, the terms ‘prior’ and ‘posterior’ might be misleading, since, this is not a Bayesian infer-
ence setting (the prior an posterior are not connected trough the Bayes rule). However, PAC-Bayes and Bayesian
analysis have interesting and practical connections, as we will see in the next sections (see also Germain et al.
(2016)).
3
Under review as a conference paper at ICLR 2018
3 PAC-Bayesian lifelong-learning
In this section we introduce the lifelong-learning setting. In this setting a lifelong-learning agent
observes several ‘training’ tasks from the same task environment. The lifelong-learner must extract
some common knowledge from these tasks, which will be used for learning new tasks from the same
environment. In the literature this setting is often called learning-to-learn, meta-learning or lifelong-
learning. We will formulate the problem and provide a generalization bound which will later lead to
a practical algorithm. Our work extends Pentina & Lampert (2014) and establishes a more general
bound. Furthermore, we will demonstrate how to apply this result practically in non-linear deep
models using stochastic learning.
3.1	Lifelong learning problem formulation
The lifelong learning problem formulation follows Pentina & Lampert (2014). We assume all tasks
share the sample space Z, hypothesis space H and loss function ` : H × Z → [0, 1]. The learning
tasks differ in the unknown sample distribution Dt associated with each task t. The lifelong-learning
agent observes the training sets S1, ..., Sn corresponding to n different tasks. The number of samples
in task i is denoted by mi . Each observed dataset Si is assumed to be generated from an unknown
sample distribution Si 〜 Dmi. As in Baxter (2000), We assume that the sample distributions Di are
generated i.i.d. from an unknown tasks distribution τ.
The goal of the lifelong-learner is to extract some knoWledge from the observed tasks that Will be
used as prior knoWledge for learning neW (yet unobserved) tasks from τ . The prior knoWledge
comes in the form of a distribution over hypotheses, P ∈ M. When learning aneW task, the learner
uses the observed task’s data S and the prior P to output a posterior distribution Q(S, P) over H.
We assume that all tasks are learned via the same learning process. Namely, for a given S and P
there is a specific output Q(S, P). Hence Q() is a function: Q : Zm× M → M. 3
The quality of a prior P is measured by the expected loss When using itto learn neW tasks, as defined
by,
er(P,τ) ,	E E E E `(h,z).
(D,m)~τ S~Dm h~Q(S,P) z~D
(5)
Since We Want to prove a PAC-Bayes style bound for lifelong-learning, We assume that the lifelong-
learner does not select a single prior P, but instead infers a distribution Q over all prior distributions
in M. 4. Since Q is inferred after observing the tasks, it is called the hyper-posterior distribution,
and serves as a prior for aneW task, i.e., When learning a neW task, the learner draWs a prior from Q
and then uses it for learning.
Ideally, the performance of the hyper-posterior Q is measured by the expected generalization loss of
learning neW tasks using priors generated from Q. This quantity is denoted as the transfer error
er (Q, τ) , E er (P, τ) .
(6)
While er (Q, τ) is not computable, We can hoWever evaluate the empirical multi-task error
1n
er(Q,Sι,...,Sn)，EC — Y^er(Q(Si,P),Si).	⑺
P~Q n
i=1
Although er (Q, τ ) cannot be evaluated, We Will prove a PAC-Bayes style upper bound on it, that
can be minimized over Q. It is important to emphasize that the hyper-posterior is evaluated on
neW, independent, tasks from the environment (and not on the observed tasks Which ae used for
meta-training).
In the single-task PAC-Bayes setting one selects a prior P ∈ M before seeing the data, and updates
itto a posterior Q ∈ M after observing the training data. In the present lifelong setup, folloWing the
frameWork in Pentina & Lampert (2014), one selects an initial hyper-prior distribution P, essentially
3In the next section We Will use stochastic optimization methods as learning algorithms, but We can assume
convergence to a same solution for any execution With a given S and P .
4After proving the bound, We Will simplify the lifelong-learning objective into choosing a single optimal
prior.
4
Under review as a conference paper at ICLR 2018
a distribution over prior distributions P, and, following the observation of the data from all tasks,
updates it to a hyper-posterior distribution Q. As a simple example, assume the initial prior P is
a Gaussian distribution over neural network weights, characterized by a mean and covariance. A
hyper distribution would correspond in this case to a distribution over the mean and covariance of
P.
3.2	Lifelong-learning PAC-Bayesian bound
In this section we present a novel bound on the transfer error in the lifelong learning setup. The
theorem is proved in the appendix 8.1.
Theorem 2 (Lifelong-learning PAC-Bayes bound). Let Q : Zm × M → M be a mapping (single-
task learning procedure), and let P be some predefined hyper-prior distribution. Then for any
δ ∈ (0, 1] the following inequality holds uniformly for all hyper-posteriors distributions Q with
probability of at least 1 - δ, 5
1n
er(Q,τ) ≤ - V Eλλ eri (Qi(Si,P),Si)+	⑻
n	P 〜Q
i=1
n XX S 2(m--i) (DKL(QIIP)+P EQ DKL(Q(Si，P )Hp)+log 2nm)+
S 2(n⅛i) (DKL (q||p )+lοg2n).
Notice that the transfer error (6) is bounded by the empirical multi-task error (7) plus two complex-
ity terms. The first is the average of the task-complexity terms of the observed tasks. This term
converges to zero in the limit of a large number of samples in each task (mi → ∞). The second is
an environment-complexity term. This term converges to zero if infinite number of tasks is observed
from the task environment (n → ∞). As in Pentina & Lampert (2014), our proof is based on two
main steps. The second step, similarly to Pentina & Lampert (2014), bounds the generalization er-
ror at the task-environment level (i.e, the error caused by observing only a finite number of tasks),
er (Q, τ), by the average generalization error in the observed tasks plus the environment-complexity
term.
The first step differs from Pentina & Lampert (2014). Instead of using a single joint bound on the
average generalization error, we use a single-task PAC-Bayes theorem to bound the generalization
error in each task separately (when learned using priors from the hyper-posterior), and then use a
union bound argument. By doing so our bound takes into account the specific number of samples
in each observed task (instead of their harmonic mean). Therefore our bound is better adjusted the
observed data set.
Another distinction is in the case in which an infinite number of tasks is observed, but each has
only a few samples. In contrast to Pentina & Lampert (2014), in Theorem 2 the hyper-prior still
has an effect on the bound. Intuitively, the prior knowledge we had before observing tasks (hyper-
prior) should still have an effect on the bound unless the observed tasks contain enough information
(samples).
Our proof technique can utilize different single-task bounds in each of the two steps. In section
8.1 we use McAllester’s bound (Theorem 1), which is tighter than the lemma used in Pentina &
Lampert (2014). Therefore, the complexity terms are in the form of
ʌ/ m1 DκL(Q∖∖p)
instead of
√= Dkl(Q∖∖P) as in Pentina & Lampert (2014). This means the bound is tighter (e.g. see Seldin
et al. (2012) Theorems 5 and 6). In section 8.2 we demonstrate how our technique can use other,
possibly tighter, single-task bounds. Finally, in the experiments section we empirically evaluate the
transfer risk obtained when using the bounds as learning objectives and show that our bound leads
to far better results.
5Theprobability is taken over sampling of (Di,mi)〜T and Si 〜Dmi,i = 1,…，n.
5
Under review as a conference paper at ICLR 2018
4	Lifelong-learning algorithm
As in the single-task case, the bound of Theorem 2 can be evaluated from the training data and so
can serve as a minimization objective for a principled lifelong-learning algorithm. Since the bound
holds uniformly for all Q, it is ensured to hold also for the inferred optimal Q*. In this section We
will derive a practical learning procedure that can applied to a large family of differentiable models,
including deep neural netWorks.
4.1	Hyper-Posterior Model
In this section We Will choose a specific form for the Hyper-posterior distribution Q, Which enables
practical implementation. Given a parametric family of priors Pθ : θ ∈ RNP , the space of hyper-
posteriors consists of all distributions over RNP . We Will limit our search to a certain family of
hyper-posteriors by choosing a Gaussian distribution in the space of prior parameters,
Qθp , N (θP, κQINp ×Np),	⑼
Where κQ > 0 is a predefined constant.
Notice that Q appears in the bound (8) in tWo forms (i) divergence from the hyper-prior DKL(Q||P)
and (ii) expectations over P 〜 Q.
First, by setting the hyper-prior as Gaussian, P = N 0, κ2P INP ×NP , Where κP > 0 is another
constant, We get a simple form for the KLD term,
DKL(QθpIIP) = 2⅛ kθpk2.	(10)
2κP
Note that the hyper-prior serves as a regularization term for learning the prior.
Second, the expectations can be easily approximated using by averaging several Monte-Carlo sam-
ples of P. Notice that sampling from QθP means adding Gaussian noise to the prior parameters θP
during training, θp = θp + εp,εp 〜N(0, KQINP ×Np). This means the learned parameters must
be robust to perturbations, Which encourages selecting solutions Which are less prone to over-fitting
and are expected to generalize better (Chaudhari et al., 2016; Keskar et al., 2016).
4.2	Joint optimization
The term appearing on the RHS of the lifelong learning bound in (8) can be compactly Written as
1n
J(θp) , - fJi(θp)+Υ(θp),
n i=1
Where We defined,
Ji(θP) ,	E ebri (Qi(Si, P), Si)+
P ~Qθp
∖L * 1 * (iii) n (DKL(QθpIIP)+p E DκL(Q(Si,P)I∣P) + iog2nmi
V 2(mi - 1) ∖	P〜Qθp	δ
and
γ(θp), ʌ/2(n-i) ^DκKL(QθP∣||P)+log2n).
(11)
(12)
(13)
Theorem 2 alloWs us to choose any single-task learning procedure Q(Si, P) : Zmi × M → M to
infer a posterior. We Will use a procedure Which minimizes Ji(θP) due to the folloWing advantages:
(i) It minimizes a bound on the generalization error of the observed task (see section 8.1). (ii) It uses
the prior knoWledge gained from the prior P to get a tighter bound and a better learning objective.
(iii) As Will be shoWn next, formulating the single task learning as an optimization problem enables
joint learning of the shared prior and the task posteriors.
6
Under review as a conference paper at ICLR 2018
To formulate the single-task learning as an optimization problem, we choose a parametric form for
the posterior of each task Qφi, φi ∈ RNQ (see section 4.3 for an explicit example). The single-task
learning algorithm can be formulated as φ* = argminφi Ji(θp, φi), where We abuse notation by
denoting the term Ji(θP) evaluated with posterior parameters φi as Ji(θP, φi).
The lifelong-learning problem of minimizing J(θP) over θP can now be written more explicitly,
)
min
θP,φ1,...,φn
{1 XX
i=1
Ji(θP, φi) + Υ(θP)
(14)
4.3	Distributions model
In this section we make the lifelong-learning optimization problem (14) more explicit by defining a
model for the posterior and prior distributions. First, we define the hypothesis class H as a family
of functions parameterized by a weight vector hw : w ∈ Rd . Given this parameterization, the
posterior and prior are distributions over Rd .
We will present an algorithm for any differentiable model 6, but our aim is to use neural network
(NN) architectures. In fact, we will use Stochastic NNs (Graves, 2011; Blundell et al., 2015) since in
our setting the weights are random and we are optimizing their posterior distribution. The techniques
presented next will be mostly based on Blundell et al. (2015).
Next we define the posteriors Qφi and the prior PθP as factorized Gaussian distributions7 8,
dd
Pθp(W) = ∏N(wk；μp,k,σP,k)	;	Qφi(W) = ∏N(Wk;μi,k,σi,k), i = 1,…,n,	(15)
k=1	k=1
where for each task, the posterior parameters vector φi = (μi,ρi) ∈ R2d is composed of the means
and log-variances of each weight, μ%,k and ρi,k = log σp k, k = 1,…,d.8 The shared prior vector
Θp = (μp,ρp) ∈ R2d has a similar structure. Since we aim to use deep models where d could be
in the order of millions, distributions with more parameters might be impractical.
Since Qφi and PθP are factorized Gaussian distributions the KLD takes a simple analytic form,
1 /c Ii D ʌ _ 1 ʃi σP,k , σi,,k + (Mi,k - μP,k) J	CQ
DKL(QΦi llPθp) = ɔ ʌ, lo log F------l	2------------1 ( .	(16)
2 k=1	σi,k	σP,k
4.4	Optimization technique
As an underlying optimization method, we will use stochastic gradient descent (SGD)9 . In each
iteration, the algorithm takes a parameter step in a direction of an estimated negative gradient. As
is well known, lower variance facilitates convergence and its speed. Recall that each single-task
bound is composed of an empirical error term and a complexity term (12). The complexity term is a
simple function of DκL(Qφi ∣∣Pθp) (16), which can easily be differentiated analytically. However,
evaluating the gradient of the empirical error term is more challenging.
Recall the definition of the empirical error, er (Qφi, Si) = Ew〜q力a (1/mi) Pm=I ' (hw, zi,j). This
term poses two major challenges. (i) The data set Si could be very large making it expensive to
cycle over all the mi samples. (ii) The term ` (hw, zj) might be highly non-linear in w, rendering
the expectation intractable. Still, we can get an unbiased and low variance estimate of the gradient.
First, instead of using all of the data for each gradient estimation we will use a randomly sampled
mini-batch Si ⊂ Si. Next, we require an estimate of a gradient of the form Vφ E f (w) which
i	寸w〜Qφ
6The only assumption on hw : w ∈ Rd is that the loss function `(hw, z) is differentiable w.r.t w.
7This choice makes optimization easier, but in principle we can use other distributions as long as the PDF
is differentiable w.r.t the parameters.
8 Note that we use ρ = log σ2 as a parameter in order to keep the parameters unconstrained (while σ2 =
exp(ρ) is guaranteed to be strictly positive).
9Or some other variant of SGD.
7
Under review as a conference paper at ICLR 2018
is a common problem in machine learning. We will use the ‘re-parametrization trick’ (Rezende
et al., 2014; Kingma & Welling, 2013) which is an efficient and low variance method 10 11 . The re-
parametrization trick is easily applicable in our setup since we are using Gaussian distributions. The
trick is based on describing the Gaussian distribution W 〜Qφi (15) as first drawing ε 〜N (0, Id×d)
and then applying the deterministic function w(φi, ε) = μ% + σi Θ ε (where Θ is an element-wise
multiplication).
Therefore, we can switch the order of gradient and expectation to get
Vφ E f(w) = Vφ	E	f(w(φi,ε)) = E	Jφf(w(Φi,εY).
W 〜Qφ	ε 〜N (O,Id×d)	ε 〜N (0,Id×d)
The expectation can be approximated by averaging a small number of Monte-Carlo samples with
reasonable accuracy. For a fixed sampled ε, the gradient Vφf (w(φi, ε)) is easily computable with
backpropagation.
In summary, the Lifelong learning by Adjusting Priors (LAP) algorithm is composed of two phases
In the first phase (Algorithm 1, termed “meta-training”) several observed “training tasks” are used to
learn a prior. In the second phase (Algorithm 2, termed “meta-testing”) the previously learned prior
is used for the learning of a new task (which was unobserved in the first phase). Note that the first
phase can be used independently as a multi-task learning method. Both algorithms are described in
pseudo-code in the appendix (section 8.4).
5	Toy Example Illustration
To illustrate the setup visually, we will consider a simple toy example of a 2D estimation problem.
In each task, the goal is to estimate the mean of the data generating distribution. In this setup, the
samples z are vectors in R2 . The hypothesis class is a the set of 2D vectors, h ∈ R2. As a loss
function we will use the Euclidean distance, `(h, z) , kh - zk22. We artificially create the data
of each task by generating 50 samples from the appropriate distribution: N (2, 1)>, 0.12I2×2 in
task 1, and N (4, 1)>, 0.12I2×2 in task 2. The prior and posteriors are 2D factorized Gaussian
distributions, P，N (μp, diag(σp)) and Qi，N (μ, diag(σ2)) , i = 1, 2.
We run Algorithm 1 (meta-training) with complexity terms according to Theorem 1. As seen in
Figure 1, the learned prior (namely, the prior learned from the two tasks) and single-task posteriors
can be understood intuitively. First, the posteriors are located close to the ground truth means of each
task, with relatively small uncertainty covariance. Second, the learned prior is located in the middle
between the two posteriors, and its covariance is larger in the first dimension. This is intuitively
reasonable since the prior learned that tasks are likely to have values of around 1 in dimension 2 and
values around 3 in the dimension 1, but with larger variance. Thus, new similar tasks can be learned
using this prior with fewer samples.
6	Experimental Results
In this section we demonstrate the performance of our transfer method with image classification
tasks solved by deep neural networks. 11.
In image classification, the data samples, z , (x, y), are pairs ofa an image, x, and a label, y. The
hypothesis class hw : w ∈ Rd is a the set of neural networks with a given architecture (which will
be specified later). As a loss function `(hw, z) we will use the cross-entropy loss.
We conduct an experiment with a task environment in which each task is created by a random
permutation of the labels of the MNIST dataset (LeCun, 1998). The meta-training set is composed
of 5 tasks from the environment, each with 60, 000 training examples. Following the meta-training
10In fact, we will use the ‘local re-parameterization trick’ (Kingma et al., 2015) in which we sample a
different ε for each data point in the batch, which reduces the variance of the estimate. To make the computation
more efficient with neural-networks, the random number generation is performed w.r.t the activations instead
of the weights (see Kingma et al. (2015) for more details.).
11Our implementation uses the PyTorch framework. The code for reproducing all the experiments can be
found in the GitHub repository: https://github.com/ML-Paper/lifelong-learning-pt.
8
Under review as a conference paper at ICLR 2018
phase, the learned prior is used to learn a new meta-test task with fewer training samples (2, 000).
The network architecture is a small CNN with 2 convolutional-layers, a linear hidden layer and a
linear output layer. See section 8.3 for more implementation details.
We compare the average generalization performance (test
error) in learning a new (meta-test) when using the fol-
lowing methods.
As a baseline, we measure the performance of learning
without transfer from the training-tasks:
•	Scratch-standard: standard learning from
scratch (non-stochastic network).
•	Scratch-stochastic:	stochastic learning
from scratch (stochastic network with no
prior/complexity term).
Other methods transfer knowledge from only one of the
train tasks:
•	Warm-start-transfer: Standard learning with
initial weights taken from the standard learning
of a single task from the meta-train set (with
60, 000 examples).
Figure 1: Toy example: the orange are
red dots are the samples of task 1 and
2, respectively, and the green and pur-
ple dots are the means of the posteriors
of task 1 and 2, respectively. The mean
of the prior is a blue dot. The ellipse
around each distribution’s mean repre-
sents the covariance matrix.
•	Oracle-transfer: Same as the previous method,
but all layers besides the output layer are frozen
(unchanged from their initial value), which is a
common practice for transfer learning in com-
puter vision (Razavian et al., 2014). Note that in this method we are manually inserting
prior knowledge based on our familiarity with the task environment. Therefore this method
can be considered an “oracle”.
Finally, we compare methods which transfer knowledge from all of the training tasks:
•	LAP-M: The objective is based on Theorem 2 - the lifelong-learning bound obtained using
Theorem 3 (McAllester’s single-task bound).
•	LAP-S: The objective is based on the lifelong-learning bound of eq. (23) in section 8.2.
This lifelong-learning bound is obtained using Theorem 4 (Seeger’s single-task bound).
•	LAP-PL: In this method we use the main theorem of Pentina & Lampert (2014) as an
objective for the algorithm, instead of Theorem 2.
•	LAP-KLD: Here we use a task-complexity term which is simply the KLD between the
sampled prior and the task posterior and an environment-complexity term which is the KLD
between the hyper-prior and hyper-posterior. This minimization problem is equivalent to
maximization of the Evidence-Lower-Bound (ELBO) when using a variational methods
to approximate the maximum-likelihood parameters of a hierarchical generative model 12.
Note that the ELBO can also be interpreted as an upper bound on the generalization error.
However the bound is looser than the one obtained using PAC-Bayesian methods 13.
•	Averaged-prior: Each of the training tasks is learned in a standard way to obtain a weights
vector, wi . The learned prior is set as an isotropic Gaussian with unit variances and a mean
vector which is the average of wi, i = 1, .., n. This prior is used for meta-testing as in
LAP-S.
•	MAML: The Model-Agnostic-Meta-Learning (MAML) algorithm by Finn et al. (2017)
finds an optimal initial weight for learning tasks from a given environment. We report
the best results obtained with all combinations of the following representative hyper-
parameters: 1-3 gradient steps in meta-training, 1-20 gradient steps in meta-testing and
α ∈ {0.01, 0.1, 0.4}.
12For example, see Edwards & Storkey (2016), equation 4.
13See discussion in section 3.2.
9
Under review as a conference paper at ICLR 2018
Table 1: Comparing the test error of different learning methods on 100 test tasks (average ± STD)
Method	Average STD
Scratch-standard	2.27%	0.16%
Scratch-stochastic	2.8%	0.16%
Warm-start-transfer	1.43%	0.12%
Oracle-transfer	0.802%	0.06%
LAP-M	1.04%	0.1%
LAP-S	0.856%	0.08%
LAP-PL	61.3%	15.9%
LAP-KLD	91.0%	6.06%
Averaged-Prior	2.89%	0.19%
MAML	0.931%	0.06%
As can be seen in Table 1, the best results are obtained with the “oracle” method. Recall that the
oracle method has the “unfair” advantage of a “hand-engineered” transfer technique which is based
on knowledge about the problem. In contrast, the other methods must automatically learn the task
environment by observing several tasks.
The LAP-M and LAP-S variants of the LAP algorithm improves considerably over learning from
scratch and over the naive warm-start transfer and are very close the the “oracle” method. As ex-
pected the the LAP-S variant preforms better since it uses a tighter bound (see section 8.2).
The other variants of the LAP algorithm with other objectives performed much worse. First, the
results for LAP-PL demonstrate the importance of the tight generalization bound developed in our
work. Second, the results for the LAP-KLD show that deriving objectives from variational-inference
techniques that maximize a lower bound on the model evidence, might be less a successful approach
than deriving objectives which minimize upper bounds on the generalization error.
The results for the “averaged-prior” method are about the same as learning from scratch. Due to the
high non-linearity of the problem, averaging weights was not expected to perform well .
The results of MAML are comparable to the results of the LAP algorithm. Note that MAML is
specifically suited for learning from many few-shot tasks, in which taking a small number of gradient
steps in each task is effective for learning. However, in our experiment, there are a few tasks but
more than a few samples. Still, the method performed quite well with several different sets of hyper-
parameters 14.
7	Discussion and Future Work
We have presented a framework for representational lifelong learning, motivated by PAC-Bayes
generalization bounds, and implemented through the adjustment of a learned prior, based on tasks
encountered so far. The framework bears conceptual similarity to the empirical Bayes method while
not being Bayesian, and is implemented at the level of tasks rather than samples. Combining the gen-
eral approach with the rich representational structure of deep neural networks, and learning through
gradient based methods leads to an efficient procedure for lifelong learning, as motivated theoreti-
cally and demonstrated empirically. While our experimental results are preliminary, we believe that
our work attests to the utility of using rigorous performance bounds to derive learning algorithms,
and demonstrates that tighter bounds indeed lead to improved performance.
There are several open issues to consider. First, the current version learns to solve all available tasks
in parallel, while a more useful procedure should be sequential in nature. This can be easily incor-
porated into our framework by updating the prior following each novel task. Second, our method
requires training stochastic models which is challenging due to the the high-variance gradients. We
we would like to develop new methods within our framework which have more stable convergence
and are easier to apply in larger scale problems. Third, there is much current effort in reinforcement
14The best results for MAML were obtained α = 0.01, 2 gradient steps in meta-training and 16 in meta-
testing.
10
Under review as a conference paper at ICLR 2018
learning to augment model free learning with model based components, where some aspects of the
latter are often formulated as supervised learning tasks. Incorporating our approach in such a context
would be a worthwhile challenge. In fact, a similar framework to ours was recently proposed within
an RL setting (Teh et al., 2017), although it was not motivated from performance guarantees as was
our approach, but rather from intuitive heuristic arguments.
References
Pierre Alquier, Massimiliano Pontil, et al. Regret bounds for lifelong learning. In Artificial Intelli-
gence and Statistics, pp. 261-269, 2017.
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul,
and Nando de Freitas. Learning to learn by gradient descent by gradient descent. In Advances in
Neural Information Processing Systems, pp. 3981-3989, 2016.
Jean-Yves Audibert. PAC-Bayesian aggregation and multi-armed bandits. PhD thesis, Universite
Paris-Est, 2010.
Jonathan Baxter. A model of inductive bias learning. J. Artif. Intell. Res.(JAIR), 12(149-198):3,
2000.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in
neural network. In International Conference on Machine Learning, pp. 1613-1622, 2015.
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, and Yann LeCun. Entropy-sgd: Biasing
gradient descent into wide valleys. In ICLR 2017, arXiv preprint arXiv:1611.01838, 2016.
Djork-Ame Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network
learning by exponential linear units (elus). ICLR 2016, arXiv preprint arXiv:1511.07289, 2015.
L Devroye, L Gyoorfi, and G Lugosi. A Probabilistic Thalamiceory of Pattern Recognition.
Springer, 1996.
Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. In UAI 2016,
arXiv preprint arXiv:1703.11008, 2017.
Harrison Edwards and Amos Storkey. Towards a neural statistician. In ICLR 2017, arXiv preprint
arXiv:1606.02185, 2016.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In ICML 2017, arXiv preprint arXiv:1703.03400, 2017.
Pascal Germain, Francis Bach, Alexandre Lacoste, and Simon Lacoste-Julien. Pac-bayesian theory
meets bayesian inference. In Advances In Neural Information Processing Systems, pp. 1876-
1884, 2016.
Alex Graves. Practical variational inference for neural networks. In Advances in Neural Information
Processing Systems, pp. 2348-2356, 2011.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv
preprint arXiv:1609.04836, 2016.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR 2015, arXiv
preprint arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In ICLR 2014, arXiv
preprint arXiv:1312.6114, 2013.
Diederik P Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparame-
terization trick. In Advances in Neural Information Processing Systems, pp. 2575-2583, 2015.
11
Under review as a conference paper at ICLR 2018
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcom-
ing catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences,
pp. 201611835, 2017.
Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.
GUy Lever, Francois Laviolette, and John Shawe-Taylor. Tighter pac-bayes bounds through
distribution-dependent priors. Theoretical Computer Science, 473:4-28, 2013.
Andreas Maurer. A note on the pac bayesian theorem. arXiv preprint cs/0411099, 2004.
Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. The benefit of multitask
representation learning. Journal of Machine Learning Research, 17(81):1-32, 2016.
David McAllester. A pac-bayesian tutorial with a dropout bound. arXiv preprint arXiv:1307.2118,
2013.
David A McAllester. Pac-bayesian model averaging. In Proceedings of the twelfth annual confer-
ence on Computational learning theory, pp. 164-170. ACM, 1999.
TomM Mitchell. The need for biases in learning generalizations. Department of Computer Science,
Laboratory for Computer Science Research, Rutgers Univ. New Jersey, 1980.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. A pac-
bayesian approach to spectrally-normalized margin bounds for neural networks. arXiv preprint
arXiv:1707.09564, 2017.
Anastasia Pentina and Christoph H Lampert. A pac-bayesian bound for lifelong learning. In ICML,
pp. 991-999, 2014.
Anastasia Pentina and Christoph H Lampert. Lifelong learning with non-iid tasks. In Advances in
Neural Information Processing Systems, pp. 1540-1548, 2015.
Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. 2016.
Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. Cnn features off-
the-shelf: an astounding baseline for recognition (2014). CoRR abs/1403.6382, 2014.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and ap-
proximate inference in deep generative models. In International Conference on Machine Learn-
ing, pp. 1278-1286, 2014.
Matthias Seeger. Pac-bayesian generalisation error bounds for gaussian process classification. Jour-
nal of machine learning research, 3(Oct):233-269, 2002.
Yevgeny Seldin, Francois Laviolette, Nicolo Cesa-Bianchi, John Shawe-Taylor, and Peter Auer.
Pac-bayesian inequalities for martingales. IEEE Transactions on Information Theory, 58(12):
7086-7093, 2012.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-
rithms. Cambridge university press, 2014.
Yee Whye Teh, Victor Bapst, Wojciech Marian Czarnecki, John Quan, James Kirkpatrick, Raia
Hadsell, Nicolas Heess, and Razvan Pascanu. Distral: Robust multitask reinforcement learning.
arXiv preprint arXiv:1707.04175, 2017.
Ilya O Tolstikhin and Yevgeny Seldin. Pac-bayes-empirical-bernstein inequality. In Advances in
Neural Information Processing Systems, pp. 109-117, 2013.
12
Under review as a conference paper at ICLR 2018
8	Appendix
8.1	Proof of the Lifelong-Learning Bound
In this section we prove Theorem 2. The proof is based on two steps, both use McAllaster’s classical
PAC-Bayes bound. In the first step we use it to bound the error which is caused due to observing
only a finite number of samples in each of the observed tasks. In the second step we use it again to
bound the generalization error due to observing a limited number of tasks from the environment.
We start by restating the classical PAC-Bayes bound (McAllester, 1999; Shalev-Shwartz & Ben-
David, 2014) using general notations.
Theorem 3 (Classical PAC-Bayes bound, general formulation). Let X be some ‘sample’ space and
X some distribution over X. Let F be some ‘hypothesis’ space. Define a ‘loss function’ g(f, X) :
F × X → [0, 1]. Let X1K , {X1, ..., XK} be a sequence of K independent random variables
distributed according to X. Let π be some prior distribution over F (which must not depend on the
samples X1 , ..., XK). For any δ ∈ (0, 1], the following bound holds uniformly for all ‘posterior’
distributions ρ over F (even sample dependent),
PXK 〜x[ E E g(f,X) ≤ ɪ XX E g(f,Xk)+
1	i.i.d [X〜X f〜P	K —f f 〜P
(17)
《2K-T) (DKL(P||n) + log IK),∀ρ} ≥ 1- δ∙
First step We use Theorem 3 to bound the generalization error in each of the observed tasks when
learning is done by an algorithm Q : Zmi × M → M which uses a prior and the samples to output
a distribution over hypothesis.
Let i ∈ 1, ..., n be the index of some observed task. We use Theorem 3 with the following sub-
stitutions. The samples are Xk , zi,j , K , mi , and their distribution is X , Di . We define a
‘tuple hypothesis’f = (P, h) where P ∈ M and h ∈ H. The ‘loss function’ is the regular loss
which uses only the h element in the tuple, g(f, X) , `(h, z). We define the ‘prior over hypothe-
sis’, π , (P, P), as some distribution over M × H in which we first sample P from P and then
sample h from P. According to Theorem 3, the ‘posterior over hypothesis’ can be any distribution
(even sample dependent), in particular, the bound will hold for the following family of distributions
over M × H, ρ , (Q, Q(Si, P)), in which we first sample P from Q and then sample h from
Q = Q(Si, P). 15
The KLD term is,
DKL(ρ∣∣∏) = E log ρ(f) = E E log Q(P)Q(Si,P)(h)
“U f	f 〜P	∏(f)	P 〜Q h 〜Q(S,P)	P (P )P (h)
Q(P)
E log 0一+ E E
2	P (P)	P〜Q h〜Q(S,P)
Q(Si,P )(h)
g	P(h
P〜Q
DKL(QIP ) + P EQ
DKL(Q(Si,P)||P)
Plugging in to (17) we obtain that for any δi > 0
PSi 〜Dm〈	E E E
i i [z〜Di P〜Q h〜Q(Si,P)
1 mi
'(h,z) ≤ ——^X E E	'(h,Zij)+	(18)
mi J P〜Q h〜Q(Si,P)	,j
j=1
S 2(m⅛ (dkl(q^p )+P eq DKL(Q(Si，p )HP)+log zδii-), ∀q} ≥1 - δi,
for all observed tasks i = 1, .., n.
15Recall that Q(Si , P ) is the posterior distribution which is the output of the learning algorithm Q() which
uses the data Si and the prior P .
13
Under review as a conference paper at ICLR 2018
Using the terms in section 2.1, we can write the above as,
PSi-Dm	PEQ e'⑼” P),Di) ≤ PEQ er(Q(Si,p),Si) +
(19)
f 2m⅛ (DKL(QIP) + PEQDKL(Q(Si,P)|1P) + log mi), ∀q} ≥ 1 -瓯
Second step Next we wish to bound the environment-level generalization (i.e, the error due to
observing only a finite number of tasks from the environment). We will use Theorem 3 again, with
the following substitutions. The i.i.d samples are (Di,mi, Si), i = 1, ..., n where (Di,mi) are
distributed according to the task-distribution T and Si 〜Dmi. The ‘hypothesis, are f，P and the
‘loss function’ is g(f, X) , E E `(h, z). Let π , P be some distribution over M, the
h-Q(S,P) z-D
bound will hold uniformly for all distributions ρ , Q over M.
For any δ0 > 0, the following holds (according to Theorem 3),
P(D m )-τ S -Dmi i=1 n E E E E E `(h, z) ≤	(20)
(Di,mi)-τ,Si-Di ,i=1,..,n (D,m)-τ S-Dm P-Q h-Q(S,P) z-D
1 XX E E E '(h,z) + ∖l. 1 1. DDklQQPP )+log -n∖ ∀q∖ ≥ 1 - δο.
n i=1 P-Q h-Q(Si,P) z-Di	2(n- 1) KL	δ0	0
Using the terms in section 3.1, we can write the above as,
1n
P(Di,mi)fSi 〜Dmi,i=1,..,n ( 2 7 ) ≤ P EQ n Eergi(Si P ),Di) +
i=1
(21)
f2n⅛ (DKL (QIIP )+log δ0), ∀q} ≥1 - δ0.
Finally, we will bound the probability of the event which is the intersection of the events in (19) and
(21) by using the union bound. For any δ > 0, set δo，2 and δi，ɪ for i = 1,…,n.
Using the union bound we finally get,
1n
P(Di,mi)5S 〜Dm”],..,/，3 ) ≤ n EP EQ bi (Qi(Si,P ),Si) +
i=1
n XX ∕2mi1-7(DKL麻％QDKLQS^P^+0⅛)+
j2⅛ (DKL(Q||P )+log2n)，∀q} ≥1 - δ∙
8.2	Lifelong learning bound based on alternative single-task bounds
Many PAC-Bayesian bounds for single-task learning have appeared in the literature. In this section
we demonstrate how our proof technique can be used with a different single-task bound to derive a
possibly tighter lifelong-learning bound.
Consider the following single-task bound by (Seeger, 2002; Maurer, 2004). 16
Theorem 4 (Seeger’s single-task bound). Under the same notations as Theorem 3, for any δ ∈ (0, 1]
we have,
PΧι,...,ΧK 鼠x{ XEX fEp g(f,X) ≤ er (ρ, XK) +2ε + ,2ε e∙(ρ,XK), ∀ρ} ≥ 1 - δ,
16Note that we used the slightly tighter version version by Maurer (2004) bound which requires K ≥ 8 .
14
Under review as a conference paper at ICLR 2018
where we define,
ε(κ,ρ,π,δ)=K (DKL(P忱+ι°gW
and,
1K
er(ρ,xK)=m£产 g(f,Xk).
K Mf ~ρ
Using the above theorem we get an alternative intra-task bound to (19),
Pim PEQ eMQ网，P),Di) ≤ Peq er (Q(Si，P),Si) +
(22)
2εi + √2εier (Q(Si,P),Si), ∀Q ≥ 1 -瓯
where,
εi
DKL(Q||P) + PEQ Dkl
(Q(Si, P)||P) + log
While the classical bound of Theorem 1 converges at a rate of about 1/√m (as in basic VC-like
bounds), the bound of Theorem 4 converges even faster (at a rate if mɪ-) if the empirical error eer (Q)
is negligibly small (compared to mlDkl(Q∖∖P)). Since this is commonly the case in modern deep
learning, we expect this bound to be tighter than others in this regime. 17
By utilizing the Theorem 4 in the first step of the proof in section 8.1 we can get a tighter bound for
lifelong-learning:
1n
P(Di ,mi)fSi 〜Dmi,i=ι,...,n{ er(Q,τ) ≤ n£ P EQ eri (Qi(Si,P ),Si)+	(23)
i=1
2εi + p∕2εier (Q(Si,P ),Si) + ^2(n - 1) (DKLj(q∖∖p ) + lθg ɪ ʃ ,∀q} ≥ 1 - δ,
where, ε% is defined in (22) (and δi = 金).
8.3	Classification example implementation details
The network architecture used for the permuted-labels experiment is a small CNN with 2
convolutional-layers of 10 and 20 filters, each with 5 × 5 kernels, a hidden linear layer with 50
units and a linear output layer. Each convolutional layer is followed by max pooling operation with
kernel of size 2. Dropout with p = 0.5 is performed before the output layer. In both networks we use
ELU (Clevert et al., 2015) (with α = 1) as an activation function. Both phases of the LAP algorithm
(algorithms 1 and 2) ran for 200 epochs, with batches of 128 samples in each task. We take only one
Monte-Carlo sample of the stochastic network output in each step. As optimizer we used ADAM
(Kingma & Ba, 2014) with learning rate of 10-3. The means of the weights (μ parameters) are
initialized randomly by N 0, 0.12 , while the log-var of the weights (ρ parameters) are initialized
byN -10, 0.12 . The hyper-prior and hyper-posterior parameters are κP = 2000 and κQ = 0.001
respectively and the confidence parameter was chosen to be δ = 0.1 .
To evaluate the trained network we used the maximum of the posterior for inference (i.e. we use
only the means the weights). 18
17 More recent works presented possibly tighter PAC-Bayesian bounds by taking into account the empirical
variance (Tolstikhin & Seldin, 2013) or by specializing the bound deep for neural networks (Neyshabur et al.,
2017). However, we leave the incorporation of these bounds for future work.
18Classifying using the the majority vote of several runs gave similar results in this experiment.
15
Under review as a conference paper at ICLR 2018
8.4	Pseudo code
Algorithm 1: LAP algorithm, meta-training phase (Iearning-to-learn)
Input : Data sets of observed tasks: Si,..., Sn
Output: Learned prior parameters θP
Initialize:
•	θp = (μp,pp) ∈ Rd X Rd
•	φi = (μi,pi) ∈ Rd × Rd,	for i = 1,…,n
while not done do
for each task i ∈ {1, ..n} 19 do
•	Sample a random mini-batch from the data Si ⊂ Si
•	Approximate Ji(θp, φi) (12) using Si and averaging Monte-Carlo draws
end
•	J 一 1 Pi∈{1,..n} Ji(θp ,Φi) + Υ(θp)
•	Evaluate the gradient of J w.r.t {θP, φ1, ..., φn} using backprop
•	Take an optimization step
end
Algorithm 2: LAP algorithm, meta-testing phase (learning a new task)
Input : Data set of a new task, S, and prior parameters, θP
Output: Posterior parameters φ0 which solve the new task
Initialize:
•	φ0 J θP
while not done do
•	Sample a random mini-batch from the data S0 ⊂ S
•	Approximate the empirical loss J (12) using S0 and averaging Monte-Carlo draws
•	Evaluate the gradient of J w.r.t φ0 using backprop
•	Take an optimization step
end
19For implementation considerations, when training with a large number of tasks we can sample a subset of
tasks in each iteration (“meta min-batch” ) to estimate J .
16