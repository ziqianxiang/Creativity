Under review as a conference paper at ICLR 2018
Dynamic Integration of Background
Knowledge in Neural NLU Systems
Anonymous authors
Paper under double-blind review
Ab stract
Common-sense or background knowledge is required to understand natural lan-
guage, but in most neural natural language understanding (NLU) systems, the
requisite background knowledge is indirectly acquired from static corpora. We
develop a new reading architecture for the dynamic integration of explicit back-
ground knowledge in NLU models. A new task-agnostic reading module provides
refined word representations to a task-specific NLU architecture by processing
background knowledge in the form of free-text statements, together with the task-
specific inputs. Strong performance on the tasks of document question answering
(DQA) and recognizing textual entailment (RTE) demonstrate the effectiveness
and flexibility of our approach. Analysis shows that our models learn to exploit
knowledge selectively and in a semantically appropriate way.
1	Introduction
Understanding natural language depends crucially on common-sense or background knowledge, for
example, knowledge about what concepts are expressed by the words being read (lexical knowl-
edge), and what relations hold between these concepts (relational knowledge). As a simple illus-
tration, if an agent needs to understand that the statement “King Farouk signed his abdication” is
entailed by “King Farouk was exiled to France in 1952, after signing his resignation”, it must know
(among other things) that abdication means resignation of a king.
In most neural natural language understanding (NLU) systems, the requisite background knowledge
is implicitly encoded in the models’ parameters. That is, what background knowledge is present
has been learned from task supervision and also by pre-training word embeddings (where distribu-
tional information reliably reflects certain kinds of useful background knowledge, such as semantic
relatedness). However, acquisition of background knowledge from static training corpora is limiting
for two reasons. First, we cannot expect that all background knowledge that could be important for
solving an NLU task can be extracted from a limited amount of training data. Second, as the world
changes, the facts that may influence how a text is understood will likewise change. In short: build-
ing suitably large corpora to capture all relevant information, and keeping the corpus and derived
models up to date with changes to the world would be impractical.
In this paper, we develop a new architecture for dynamically incorporating external background
knowledge in NLU models. Rather than relying only on static knowledge implicitly present in the
training data, supplementary knowledge is retrieved from a knowledge base to assist with under-
standing text inputs. Since NLU systems must necessarily read and understand text inputs, our
approach incorporates background knowledge by repurposing this reading machinery—that is, we
read the text being understood together with supplementary natural language statements that assert
facts (assertions) which are relevant to understanding the content (§2).
Our knowledge-augmented NLU systems operate in a series of phases. First, given the text input
that the system must understand, which we call the context, a set of relevant supporting assertions
is retrieved. While learning to retrieve relevant information for solving NLU tasks is an important
question (Nogueira & Cho, 2017; Narasimhan et al., 2016, inter alia), in this work, we focus on
learning how to incorporate retrieved information, and use simple heuristic retrieval methods to
identify plausibly relevant background from an external knowledge base. Once the supplementary
texts have been retrieved, we use a word embedding refinement strategy that incrementally reads the
context and retrieved assertions starting with context-independent word embeddings and building
1
Under review as a conference paper at ICLR 2018
successively refined embeddings of the words that ultimately reflect both the relevant supporting
assertions and the input context (§3). These contextually refined word embeddings, which serve
as dynamic memory to store newly incorporated knowledge, are used in any task-specific reading
architecture. The overall architecture is illustrated in Figure 1. Although we are incorporating a
new kind of information into the NLU pipeline, a strength of this approach is that the architecture
of the reading module is independent of the final NLU task—the only requirement is that the final
architecture use word embeddings.
We carry out experiments on several different datasets on the tasks of document question answer-
ing (DQA) and recognizing textual entailment (RTE) evaluating the impact of our proposed solu-
tion with both basic task architectures and a sophisticated task architecture for RTE (§4). We find
that our embedding refinement strategy is quite effective (§5). On four standard benchmarks, we
show that refinement helps—even refining the embeddings just using the context (and no additional
background information) can improve performance significantly, and adding background knowledge
helps further. Our results are very competitive, setting a new state-of-the-art on the recent TriviaQA
benchmarks which is remarkable considering the simplicity of the chosen task-specific architecture.
Finally, we provide a detailed analysis of how knowledge is being used by an RTE system (§6),
including experiments showing that our system is capable of making appropriate counterfactual in-
ferences when provided with “false knowledge”.
2	External Knowledge as S upplementary Text Inputs
Knowledge resources make information that could potentially be useful for improving NLU avail-
able in a variety different formats, such as (subject, predicate, object)-triples, relational databases,
and other structured formats. Rather than tailoring our solution to a particular structured repre-
sentation, we assume that all supplementary information either already exists in natural language
statements or can easily be recoded as natural language. In contrast to mapping from unstructured
to structured representations, the inverse problem is not terribly difficult. For example, given a triple
(monkey, isA, animal) we can construct the free-text assertion “a monkey is an animal” using a few
simple rules. Finally, the free-text format means that knowledge that exists only in unstructured text
form is usable by our system.
A major question that remains to be answered is: given some text that is to be understood, what sup-
plementary knowledge should be incorporated? The retrieval of contextually relevant information
from knowledge sources is a complex research topic by itself, and it is likewise crucially dependent
on the format of the underlying knowledge base. There are several statistical (Manning et al., 1999)
and more recently neural approaches (Mitra & Craswell, 2017) and approaches based on reinforce-
ment learning (Nogueira & Cho, 2017). In this work we make use of a simple heuristic from which
we almost exhaustively retrieve all potentially relevant assertions (see §4), and rely on our reading
architecture to learn to extract only relevant information.
In the next section, we turn to the question of how to leverage the retrieved supplementary knowledge
(encoded as text) in a NLU system.
3	Refining Word Embeddings by Reading
In order to incorporate information from retrieved input texts we propose to compute contextually
refined word representations prior to processing the NLU task at hand and pass them to the task
in the form of word embeddings. Word embeddings thus serve as a form of memory that not only
contains general-purpose knowledge (as in typical neural NLU systems) but also contextual infor-
mation (including retrieved background knowledge). Our incremental refinement process encodes
input texts followed by updates on the word embedding matrix using the encoded input in multiple
reading steps. Words are first represented non-contextually (i.e., standard word type embeddings),
which can be conceived of as the columns in an embedding matrix E0 . At each progressive reading
step ' ≥ 1, a new embedding matrix E' is constructed by refining the embeddings from the previ-
ous step E'-1 using (user-specified) contextual information X' for reading step ', which is a set of
natural language sequences (i.e., texts). An illustration of our incremental refinement strategy can
be found in Figure 1.
2
Under review as a conference paper at ICLR 2018
Figure 1: Illustration of our context-dependent, refinement strategy for word representations on an
example from the SNLI dataset comprising the premise, hypothesis and additional external infor-
mation in form of free-text assertions. The reading architecture constructs refinements of word rep-
resentations incrementally (conceptually represented as columns in a series of embedding matrices)
E' are incrementally refined by reading the input text and textual renderings of relevant background
knowledge before computing the representations used by the task model (in this figure, RTE).
In the following, we define this procedure formally. We denote the hidden dimensionality of our
model by n and a fully-connected layer by FC(z) = Wz + b, W ∈ Rn×m, b ∈ Rn, u ∈ Rm.
3.1	UNREFINED WORD EMBEDDINGS (E0)
The first representation level consists of non-contextual word representations, that is, word repre-
sentations that do not depend on any input; these can be conceived of as an embedding matrix E0
whose columns are indexed by words in Σ*. The non-contextual word representation eW for a single
word w is computed by using a gated combination of fixed, pre-trained word vectors epw ∈ Rn0 with
learned character-based embeddings ecwhar ∈ Rn. The formal definition of this combination is given
in Eq. 1:
epw0 = ReLU(FC(epw ))
gw = σ FC eechpwar
e0w = gw	epw0 + (1 - gw )	ecwhar
(1)
3
Under review as a conference paper at ICLR 2018
We compute ecwhar using a single-layer convolutional neural network using n convolutional filters
of width 5 followed by a max-pooling operation over time. Combining pre-trained with character
based word embeddings in such a way is common practice. Our approach follows (Seo et al., 2017;
Weissenborn et al., 2017).
3.2	CONTEXTUALLY REFINED WORD REPRESENTATIONS (E', ' ≥ 1)
In order to compute contextually refined word embeddings E' given prior representations E'-1 We
assume a given set of texts X' = {xf, xg,...} that are to be read at refinement iteration '. Each text
x' is a sequence of word tokens. We embed all tokens of every x' using the embedding matrix from
the previous layer, E'-1. To each word, we concatenate a one-hot vector of length L with position
` set to 1, indicating which layer is currently being processed.1 Stacking the vectors into a matrix,
we obtain a X' ∈ Rd×l%|. ThiS matrix is processed by a bidirectional recurrent neural network, a
BiLSTM (Hochreiter & Schmidhuber, 1997) in this work. The resulting output is further projected
to X' by a fully-connected layer followed by a ReLU non-linearity (Eq. 2).
X' = ReLU(FC(BiLSTM(X')))	(2)
To finally update the previous embedding e'w-1 of word w, we initially maxpool all representations
of occurrences matching the lemma of W in every X ∈ X' resulting in ^W (Eq. 3). Finally, we com-
bine the context-independent representation ew with ew to form a context-sensitive representation
e'w via a gated addition which lets the model determine how much to revise the embedding with the
newly read information (Eq. 5).
^W = max {Xk | x' ∈ X',lemma(xk) = lemma(w)}	(3)
Uw=σ (fc ([eew1]))	⑷
eW = Uw © eW-1 + (1 - Uw) © ew	⑸
Note that we soften the matching condition for w using lemmatization, lemma(w), during the pool-
ing operation of Eq. 3 because contextual information about certain words is usually independent of
the current word form w they appear in. As a consequence, this minor linguistic pre-processing step
allows for additional interaction between tokens of the same lemma.
The important difference between our contextual refinement step and conventional multi-layer
(RNN) architectures is the pooling operation that is performed over occurrences of tokens that share
the same lemma. This effectively connects different positions within and between different texts
with each other thereby mitigating the problems arising from long-distance dependencies. More
importantly, however, it allows models to make use of additional input such as relevant background
knowledge.
4	Experimental Setup
We run experiments on four benchmarks for two popular tasks, namely recognizing textual entail-
ment (RTE) and document question answering (DQA). In the following we describe different aspects
of our experimental setup in more detail.
Task-specific Models Our primary interest is to explore the value of our refinement strategy with
relatively generic task architectures. Therefore, we chose basic single-layer bidirectional LSTMs
(BiLSTMs) as encoders with a task-specific, feed-forward neural network on top for the final pre-
diction. Such models are common baselines for NLU tasks and can be considered general reading
architectures as opposed to the more highly tuned, task-specific NLU systems that are necessary
1Adding this one-hot feature lets the refinement model to learn update word embeddings differently in
different levels.
4
Under review as a conference paper at ICLR 2018
to achieve state-of-the art results. However, since such models frequently underperform more cus-
tomized architectures, we also add our refinement module to a reimplementation of a state-of-the-art
architecture for RTE called ESIM (Chen et al., 2017b).
All models are trained end-to-end jointly with the refinement module. For the DQA baseline system
we add a simple lemma-in-question feature (liq) as suggested in Weissenborn et al. (2017) when
encoding the context to compare against competitive baseline results. We provide the exact model
implementations for our BiLSTM baselines and general training details in Appendix A.
Question Answering We apply our DQA models on 2 recent DQA benchmark datasets, SQuAD
(Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017). The task is to predict an answer span
within a provided document p given a question q . Both datasets are large-scale, containing on the
order of 100k examples. Because TriviaQA is collected via distant supervision the test set is divided
into a large but noisy distant supervision part and a much smaller (on the order of hundreds) human
verified part. We report results on both. See Appendix A.1 for implementation details.
Recognizing Textual Entailment We test on the frequently used SNLI dataset (Bowman et al.,
2015), a collection of 570k sentence pairs, and the more recent MultiNLI dataset (433k sentence
pairs) (Williams et al., 2017). Given two sentences, a premise p and a hypothesis q, the task is to
determine whether p either entails, contradicts or is neutral to q. See Appendix A.2 for implemen-
tation details.
Knowledge Source We make use of ConceptNet2 (Speer & Havasi, 2012), a freely-available,
multi-lingual semantic network that originated from the Open Mind Common Sense project and
incorporates selected knowledge from various other knowledge sources, such as Wiktionary3 , Open
Multilingual WordNet4, OpenCyc and DBpedia5. It presents information in the form of relational
triples.6
Assertion Retrieval We would like to obtain information about the relations of words and phrases
between q and p from ConceptNet in order to strengthen the connection between the two se-
quences. Because assertions a in ConceptNet come in form of (subject, predicate, object)-triples
(s, r, o), we retrieve all assertions for which s appears in q and o appears in p, or vice versa.
Because still too many such assertions might be retrieved for an instance, we rank all retrievals
based on their respective subject and object. To this end we compute a ranking score which is
the inverse product of appearances of the subject and the object in the KB, that is score(a) =
(Pa，I(sa0 = Sa) ∙ Pa，1(。Ο，= Oa))-L Where I denotes the indicator function. This is very re-
lated to the popular idf score (inverted document frequency) from information retrieval which ranks
terms higher that appear less frequently across different documents. During training and evaluation
We only retain the top-k assertions Which We specify for the individual experiments separately. Note
that (although very rarely) it might happen that no assertions are retrieved at all.
Refinement Order When employing our embedding-refinement strategy, We first read the doc-
ument (p) folloWed by the question (q) in case of DQA, and the premise (p) folloWed by the hy-
pothesis (q) for RTE, that is, X 1 = {p} and X2 = {q}. Additional knoWledge in the form of a
set of assertions A is integrated after reading the task-specific input for both DQA and RTE, that
is, X3 = A. In preliminary experiments We found that the final performance is not significantly
sensitive to the order of presentation so We decided to fix our order as defined above.
5
Under review as a conference paper at ICLR 2018
SQuAD Dev TriviaQA Wiki Test	TriviaQA Web Test
Model	F1	Exact	F1	Exact	F1	Exact
BiLSTM + liq	74.3	63.7	50.9/55.1	45.2 / 48.8	50.9 / 60.0	44.2 / 54.7
+ reading	77.8	67.3	54.1 / 59.6	47.5 / 52.2	54.7 / 65.9	48.1 / 60.3
+ reading + knowledge (20)	79.2	69.0	54.6 / 60.7	48.4/ 55.1	56.9 / 67.3	50.6/61.9
+ reading + knowledge (50)	79.6	69.4	55.1 /59.9	48.6/53.4	56.7/ 68.0	50.6 / 63.2
SotA Results	81.81	73.21	46.9/55.83	43.2 / 49.33 4 5 6	48.3 / 57.63	44.3 / 53.33
	82.82	75.62	52.9 / 59.51	46.9 / 54.51	52.9 / 61.51	46.7 / 57.01
Table 1: Results on the SQuAD development set as well as TriviaQA-Wikipedia and -Web test
sets for n = 300-dimensional models. TriviaQA results are further divided by distant supervision
results (left) and human verified results (right). The model using external knowledge is trained with
the top-20/50 retrieved ConceptNet assertions. The liq-feature (lemma-in-question) is only used for
the baseline. 1Hu et al. (2017), 2Wang et al. (2017), 3Pan et al. (2017).
5	Results
5.1	Question Answering: S QuAD and TriviaQA
Table 1 presents our results on two question answering benchmarks. We report results on the SQuAD
development set7 and the two more challenging TriviaQA test sets which demonstrate that the intro-
duction of our reading architecture helps consistently with additional gains from using background
knowledge. Our systems do even outperform current state-of-the-art models on TriviaQA which is
surprising given the simplicity of our task-specific architecture and the complexity of the others. For
instance, the system of Hu et al. (2017) uses a complex multi-hop attention mechanism to achieve
their results. Even our baseline BiLSTM + liq system reaches very competitive results on TriviaQA
which is in line with findings of Weissenborn et al. (2017). To verify that it is not the additional
computation that gives the performance boosts when using only our reading architecture (without
knowledge), we also ran experiments with 2-layer BiLSTMs (+liq) for our baselines which exhibit
similar computational complexity to BiLSTM + reading. We found that the second layer even hurts
performance. This demonstrates that pooling over word/lemma occurrences in a given context be-
tween layers, which constitutes the main difference to conventional stacked RNNs, is a powerful,
yet simple technique. In any case, the most important finding of these experiments is that knowledge
actually helps considerably with up to 2.2/2.9% improvements on the F1/Exact measures.
5.2	Recognizing Textual Entailment: SNLI and MultiNLI
Table 2 shows the results of our RTE experiments. In general, the introduction of our refinement
strategy almost always helps, both with and without external knowledge. When providing additional
background knowledge from ConceptNet, our BiLSTM based models improve substantially, while
the ESIM-based models improve only on the more difficult MultiNLI dataset. Compared to previ-
ously published state-of-the-art systems, our models acquit themselves very well on the MultiNLI
benchmark, and competitively on the SNLI benchmark. In parallel to this work, Gong et al. (2017)
developed a novel task-specific architecture for RTE that achieves slightly better performance on
MultiNLI than our ESIM+reading+knowledge based models.8 It is worth observing that with our
knowledge-enhanced embedding architecture, our generic BiLSTM-based task model outperforms
ESIM on MultiNLI, which is architecturally much more complex and designed specifically for the
RTE task. Finally, we remark that despite careful tuning, our re-implementation of ESIM fails to
2http://conceptnet.io/
3http://wiktionary.org/
4http://compling.hss.ntu.edu.sg/omw/
5http://dbpedia.org/
6We exclude ConceptNet 4 assertions created by only one contributor and from Verbosity to reduce noise.
7Due to restrictions on code sharing, we are not able to use the public evaluation server to obtain test set
scores for SQuAD. However, for the remaining tasks we report both development accuracy and held-out test
set performance.
8Our reading+knowledge refinement architecture can be used of course with this new model.
6
Under review as a conference paper at ICLR 2018
Model	SNLI		MNLI Matched		MNLI Mismatched	
	Dev	Test	Dev	Test	Dev	Test
BiLSTM	84.4	83.4	70.0	69.8	70.2	69.4
+ reading	86.1	85.4	75.3	75.8	76.3	74.9
+ reading + knowledge	86.5	85.7	76.8	77.3	77.5	76.3
ESIM	88.2	87.2	76.8	76.3	77.3	75.8
+ reading	88.0	87.3	77.8	77.8	78.4	77.0
+ reading + knowledge	87.8	87.3	78.8	78.2	78.8	77.0
SotA Results	-	88.03	-	74.52	-	73.52
	-	88.61	-	78.83	-	77.83
Table 2: Results on the SNLI as well as MultiNLI-Matched and -Mismatched for n =
300-dimensional models. The model using external knowledge is trained with the top-20 retrieved
ConceptNet assertions. 1Chen et al. (2017b), 2Gong et al. (2017). 3Chen et al. (2017a).
Dim/Data	1/1k	3/3k	10∕10k	30/30k	100/100k	300/Full
SNLI						
ESIM	52.2	59.3	69.7	75.5	81.3	88.2
+ reading	60.9(+8.7)	66.0(+6.7)	71.0(+1.3)	75.0(-0.5)	80.6(-0.7)	88.0(-0.2)
+ reading +	knowledge 62.9(+2.0)	68.6(+2.6)	73.8(+2.8)	77.4(+2.4)	81.3(+0.7)	87.8(-0.2)
MultiNLI Matched + Mismatched						
ESIM	44.3	50.0	55.5	61.9	68.1	76.9
+ reading	51.8(+7.5)	55.8(+5.8)	60.1(+4.6)	65.0(+3.1)	70.7(+2.6)	78.1(+1.2)
+ reading +	knowledge 52.4(+0.6)	57.9(+2.1)	62.4(+2.3)	66.6(+1.6)	71.3(+0.6)	78.8(+0.7)
Table 3: Development set results when reducing training data and embedding dimsensionality with
PCA. In parenthesis we report the relative differences to the respective result directly above.
match the 88% reported in Chen et al. (2017b); however, with MultiNLI, we find that our implemen-
tation of ESIM performs considerably better (by approximately 5%). The instability of the results
suggests, as well as the failure of a custom RTE-architecture to consistently perform well suggests
that current SotA RTE models may be overfit to the SNLI dataset.
5.3	Reducing Training Data & Dimensionality of Pre-trained Word
Embeddings
We find that there is only little impact when using external knowledge on the RTE task when us-
ing a more sophisticated task model such as ESIM. We hypothesize that the attention mechanisms
within ESIM jointly with powerful, pre-trained word representations allow for the recovery of some
important lexical relations when trained on a large dataset. It follows that by reducing the number
of training data and impoverishing pre-trained word representations the impact of using external
knowledge should become larger.
To test this hypothesis, we gradually impoverish pre-trained word embeddings by reducing their
dimensionality with PCA while reducing the number of training instances at the same time.9 Our
joint data and dimensionality reduction results are presented in Table 3. They show that there is
indeed a slightly larger benefit when employing background knowledge in the more impoverished
settings with largest improvements over using only the novel reading architecture when using around
10k examples and reduced dimensionality to 10. However, we observe that the biggest overall
impact over the baseline ESIM model stems from our contextual refinement strategy (reading) which
is especially pronounced for the 1k and 3k experiments. This highlights once more the usefulness
of our refinement strategy even without the use of additional knowledge.
9Although reducing either embedding dimensionality or data individually exhibit similar (but less pro-
nounced) results we only report the joint reduction results here.
7
Under review as a conference paper at ICLR 2018
p: h:	His off-the-cuff style seems am- ateurish [...] He didn’t look like an amateur	the net cost of oper- ations. The gross cost.	but uh these guys [...] file their uh their final exams [...] These men filed their midterm exams [...]
a:	look like synonym seem	gross antonym net	midterm antonym final
→	contradiction	contradiction	contradiction
a:	look like antonym seem	gross synonym net	midterm synonym final
→	entailment	entailment	entailment
Table 4: Three examples for the antonym - synonym swapping experiment on MultiNLL P-Premise,
h-hypothesis, a-assertion, a-swapped assertion.
(a) BiLSTM on MultiNLI.
(c) BiLSTM on SQuAD.
(b) ESIM on MultiNLI.
Figure 2: Performance differences when ignoring certain types of knowledge, i.e., relation predicates
during evaluation. Normalized performance differences are measured on the subset of examples for
which an assertion of the respective relation predicate occurs.
6	Analysis of Knowledge Utilization
Is additional knowledge used? To verify whether and how our models make use of additional
knowledge, we conducted several experiments. First, we evaluated models trained with knowledge
on our tasks while not providing any knowledge at test time. This ablation drops performance by
3.7-3.9% accuracy on MultiNLL and by 4% F1 on SQuAD. This indicates the model is refining the
representations using the provided assertions in a useful way.
Are models sensitive to semantics of the provided knowledge? The previous result does not
show that the models utilize the provided assertions in any consistent way (it may just reflect a
mismatch of training and testing conditions). Therefore, to test our models sensitivity towards the
semantics of the assertions, we run an experiment in which we swap the synonym with the antonym
predicate in the provided assertions during test time. Because of our heuristic retrieval mechanism,
not all such “counterfactuals” will affect the truth of the inference, but we still expect to see a more
significant impact. The performance drop on MultiNLI examples for which either a synonym or an
antonym-assertion is retrieved is about 10% for both the BiLSTM and the ESIM model. This very
large drop clearly shows that our models are sensitive to the semantics of the provided knowledge.
Examples of prediction changes are presented in Table 4. They demonstrate that the system has
learned to trust the presented assertions to the point that it will make appropriate counterfactual
inferences—that is, the change in knowledge has caused the change in prediction.
What knowledge is used? After establishing that our models are somehow sensitive to semantics
we wanted to find out which type of knowledge is important for which task. For this analysis
we exclude assertions including the most prominent predicates in our knowledge base individually
when evaluating our models. The results are presented in Figure 2. They demonstrate that the
biggest performance drop in total (blue bars) stems from related to assertions. This very prominent
predicate appears much more frequently than other assertions and helps connecting related parts of
the 2 input sequences with each other. We believe that related to assertions offer benefits mainly
from a modeling perspective by strongly connecting the input sequences with each other and thus
8
Under review as a conference paper at ICLR 2018
bridging long-range dependencies similar to attention. Looking at the relative drops obtained by
normalizing the performance differences on the actually affected examples (green) we find that our
models depend highly on the presence of antonym and synonym assertions for all tasks as well as
partially on is a and derived from assertions. This is an interesting finding which shows that the
sensitivity of our models is selective wrt. the type of knowledge and task. The fact that the largest
relative impact stems from antonyms is very interesting because it is known that such information is
hard to capture with distributional semantics contained in pre-trained word embeddings.
7	Related Work
The role of background knowledge in natural language understanding has long been remarked on,
especially in the context of classical models of AI (Schank & Abelson, 1977; Minsky, 2000); how-
ever, it has only recently begun to play a role in neural network models of NLU (Ahn et al., 2016;
Xu et al., 2016; Long et al., 2017; Dhingra et al., 2017). However, previous efforts have focused on
specific tasks or certain kinds of knowledge, whereas we take a step towards a more general-purpose
solution for the integration of heterogeneous knowledge for NLU systems by providing a simple,
general-purpose reading architecture that can read background knowledge encoded in simple natural
language statements, e.g., “abdication is a type of resignation”.
Bahdanau et al. (2017) use textual word definitions as a source of information about the embeddings
of OOV words. In the area of visual question answering Wu et al. (2016) utilize external knowledge
in form of DBpedia comments (short abstracts/definitions) to improve the answering ability of a
model. Marino et al. (2017) explicitly incorporate knowledge graphs into an image classification
model. Xu et al. (2016) created a recall mechanism into a standard LSTM cell that retrieves pieces
of external knowledge encoded by a single representation for a conversation model. Concurrently,
Dhingra et al. (2017) exploit linguistic knowledge using MAGE-GRUs, an adapation of GRUs to
handle graphs, however, external knowledge has to be present in form of triples. The main differ-
ence to our approach is that we incorporate external knowledge in free text form on the word level
prior to processing the task at hand which constitutes a more flexible setup. Ahn et al. (2016) ex-
ploit knowledge base facts about mentioned entities for neural language models. Bahdanau et al.
(2017) and Long et al. (2017) create word embeddings on-the-fly by reading word definitions prior
to processing the task at hand. Pilehvar et al. (2017) seamlessly incorporate information about word
senses into their representations before solving the downstream NLU task, which is similar. We go
one step further by seamlessly integrating all kinds of fine-grained assertions about concepts that
might be relevant for the task at hand.
Another important aspect of our approach is the notion of dynamically updating word-
representations. Tracking and updating concepts, entities or sentences with dynamic memories is
a very active research direction (Kumar et al., 2016; Henaff et al., 2017; Ji et al., 2017; Kobayashi
et al., 2017). However, those works typically focus on particular tasks whereas our approach is task-
agnostic and most importantly allows for the integration of external background knowledge. Other
related work includes storing temporary information in weight matrices instead of explicit neural
activations (such as word representations) as a biologically more plausible alternative.
8	Conclusion
We have presented a novel task-agnostic reading architecture that allows for the dynamic integration
of background knowledge into neural NLU models. Our solution, which is based on the incremental
refinement of word representations by reading supplementary inputs, is flexible and be used with
virtually any existing NLU architecture that rely on word embeddings as input. Our results show that
embedding refinement using both the system’s text inputs, as well as supplementary texts encoding
background knowledge can yield large improvements. In particular, we have shown that relatively
simple task architectures (e.g., based on simple BiLSTM readers) can become competitive with
state-of-the-art, task-specific architectures when augmented with our reading architecture.
9
Under review as a conference paper at ICLR 2018
Acknowledgments
References
Sungjin Ahn, HeeyoUl Choi, Tanel Parnamaa, and Yoshua Bengio. A neural knowledge language
model. arXiv, 2016.
Dzmitry Bahdanau, Tom Bosc, StanisIaW Jastrzebski, Edward Grefenstette, Pascal Vincent, and
Yoshua Bengio. Learning to compute word embeddings on the fly. arXiv, 2017.
Samuel R Bowman, Gabor Angeli, Potts Christopher, and Christopher D Manning. A large anno-
tated corpus for learning natural language inference. In EMNLP. Association for Computational
Linguistics, 2015.
Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang, and Diana Inkpen. Recurrent neu-
ral network-based sentence encoder with gated attention for natural language inference. arXiv,
2017a.
Qian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, and Hui Jiang. Enhancing and Combining Se-
quential and Tree LSTM for Natural Language Inference. ACL, 2017b.
Bhuwan Dhingra, Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. Linguistic Knowledge
as Memory for Recurrent Neural Networks. arXiv, 2017.
Yichen Gong, Heng Luo, and Jian Zhang. Natural language inference over interaction space. arXiv,
2017.
Mikael Henaff, Jason Weston, Arthur Szlam, Antoine Bordes, and Yann LeCun. Tracking the world
state with recurrent entity networks. In ICLR, 2017.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 1997.
Minghao Hu, Yuxing Peng, and Xipeng Qiu. Reinforced mnemonic reader for machine comprehen-
sion. arXiv, 2017.
Yangfeng Ji, Chenhao Tan, Sebastian Martschat, Yejin Choi, and Noah A Smith. Dynamic entity
representations in neural language models. In EMNLP, 2017.
Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehension. In ACL, July 2017.
Diederik Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. ICLR, 2015.
Sosuke Kobayashi, Naoaki Okazaki, and Kentaro Inui. A neural language model for dynami-
cally representing the meanings of unknown words and entities in a discourse. arXiv preprint
arXiv:1709.01679, 2017.
Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor
Zhong, Romain Paulus, and Richard Socher. Ask me anything: Dynamic memory networks for
natural language processing. In ICML, pp. 1378-1387, 2016.
Teng Long, Emmanuel Bengio, Ryan Lowe, Jackie Chi Kit Cheung, and Doina Precup. World
knowledge for reading comprehension: Rare entity prediction with hierarchical lstms using ex-
ternal descriptions. In EMNLP, 2017.
Christopher D Manning, Hinrich Schutze, et al. Foundations Ofstatistical natural language Process-
ing, volume 999. MIT Press, 1999.
Kenneth Marino, Ruslan Salakhutdinov, and Abhinav Gupta. The more you know: Using knowledge
graphs for image classification. CVPR, 2017.
Marvin Minsky. Commonsense-based interfaces. Communications of the ACM, 2000.
Bhaskar Mitra and Nick Craswell. Neural models for information retrieval. arXiv preprint
arXiv:1705.01509, 2017.
10
Under review as a conference paper at ICLR 2018
Karthik Narasimhan, Adam Yala, and Regina Barzilay. Improving information extraction by acquir-
ing external evidence with reinforcement learning. In EMNLP, 2016.
Rodrigo Nogueira and Kyunghyun Cho. Task-oriented query reformulation with reinforcement
learning. In EMNLP, 2017.
Boyuan Pan, Hao Li, Zhou Zhao, Bin Cao, Deng Cai, and Xiaofei He. Memen: Multi-layer embed-
ding with memory networks for machine comprehension. arXiv, 2017.
Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word
representation. In EMNLP, 2014.
Mohammad Taher Pilehvar, Jose Camacho-Collados, Roberto Navigli, and Nigel Collier. Towards
a seamless integration of word senses into downstream nlp applications. In ACL, 2017.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. In SQuAD: 100,000+ Ques-
tions for Machine Comprehension of Text, 2016.
Tim RocktascheL Edward Grefenstette, Karl Moritz Hermann, Tomas Kocisky, and Phil Blunsom.
Reasoning about entailment with neural attention. ICLR, 2015.
Roger Schank and Robert Abelson. Scripts, Plans, Goals, and Understanding. Psychology Press,
1977.
Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hananneh Hajishirzi. Bi-Directional Attention
Flow for Machine Comprehension. In ICLR, 2017.
Robert Speer and Catherine Havasi. Representing General Relational Knowledge in ConceptNet 5.
In LREC, 2012.
Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang, and Ming Zhou. Gated self-matching networks
for reading comprehension and question answering. In ACL, 2017.
Dirk Weissenborn, Georg Wiese, and Laura Seiffe. Making Neural QA as Simple as Possible but
not Simpler. In CoNLL, 2017.
Adina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge corpus for
sentence understanding through inference. arXiv, 2017.
Qi Wu, Peng Wang, Chunhua Shen, Anton van den Hengel, and Anthony Dick. Ask Me Anything:
Free-form Visual Question Answering Based on Knowledge from External Sources. CVPR, 2016.
Zhen Xu, Bingquan Liu, Baoxun Wang, Chengjie Sun, and Xiaolong Wang. Incorporating Loose-
Structured Knowledge into LSTM with Recall Gate for Conversation Modeling. arXiv, 2016.
11
Under review as a conference paper at ICLR 2018
A Implementation Details
In the following we explain the detailed implementation of our two task-specific, baseline models.
We assume to have computed the contextually (un-)refined word representations depending on the
setup and embedded our input sequences q = (q1, ..., qLQ) and p = (p1, ..., pLP) to Q ∈ Rn×LQ
and P ∈ Rn×LP , respectively. The word representation update gate in Eq. 4 is initialized with
a bias of 1 to refine representations only slightly in the beginning of training. In the following
as before, we denote the hidden dimensionality of our model by n and a fully-connected layer by
FC(z) = Wz+b,W ∈ Rn×m,b ∈ Rn,u ∈ Rm.
A.1 Question Answering
Encoding In the DQA task q refers to the question andp to the supporting text. At first we process
both sequences by identical BiLSTMs in parallel (Eq. 6) followed by separate linear projections
(Eq. 7) .
Q = BiLSTM(Q)	P = BiLSTM(P)	Q ∈ R2n×LQ, P ∈ R2n×LP	(6)
Q = UQQ
~ ʌ
P = UP P
∈ Rn×LQ, P ∈ Rn×LP
(7)
UQ, UP ∈ Rn×2n are initialized by [I; I] where I ∈ Rn×n is the identity matrix.
Prediction Our prediction- or answer layer is the same as in Weissenborn et al. (2017). We first
compute a weighted, n-dimenSionaI representation q of the processed question Q (Eq. 8).
α
qq
softmax(vq Qq ) , vq ∈ Rn
αiqqi
(8)
The probability distribution ps/pe for the start/end location of the answer is computed by a 2-layer
MLP with a ReLU activated, hidden layer sj as follows:
sj = ReLU FCs
pqj
qq
pqj	qq
))
ej = ReLU FCe
pqj
qq
pqj	qq
))
Ps(j) H exp(vsSj)	Vs ∈ Rn
Pe(j) H exp(VeSj )	Ve ∈ Rn
(9)
The model is trained to minimize the cross-entropy loss of the predicted start and end positions,
respectively. During evaluation We extract the span (i, k) with the best span-score ps(i) ∙ Pe(k) of
maximum token length k - i = 16.
A.2 Recognizing Textual Entailment
Encoding Analogous to DQA we encode our input sequences by BiLSTMs, however, for RTE
we use conditional encoding (ROCktaSCheI et al., 2015) instead. Therefore, we initially process the
embedded hypothesis Q by a BiLSTM and use the respective end states of the forward and backward
LSTM as initial states for the forward and backward LSTM that processes the embedded premise
P.
Prediction We concatenate the outputs of the forward and backward LSTMs processing the
premise p, i.e., hpqtfw; pqtbwi ∈ R2n and run each of the resulting LP outputs through a fully-
connected layer with ReLU activation (ht) followed by a max-pooling operation over time resulting
in a hidden state h ∈ Rn . Finally, h is used to predict the RTE label as follows:
12
Under review as a conference paper at ICLR 2018
ht = ReLU (FC (]pfw D)
h = maxpool ht
t
p(c) H exp(vch)	, Vc ∈ Rn
(10)
The probability of choosing category c ∈ {entailment, contradiction, neutral} is defined in Eq. 10.
Finally, the model is trained to minimize the cross-entropy loss of the predicted category probability
distribution p.
A.3 Training
As pre-processing steps we lowercase all inputs and tokenize it. Additionally, we make use of
lemmatization as described §3.2 in which is necessary for matching. As pre-trained word repre-
sentations we use 300-dimensional word-embeddings from Glove (Pennington et al., 2014). We
employed ADAM (Kingma & Ba, 2015) for optimization with an initial learning-rate of 10-3 which
was halved whenever the F1 measure (DQA) or the accuracy (RTE) dropped on the development
set between 1000/2000 minibatches for DQA and RTE respectively. We used mini-batches of size
16 for DQA and 64 for RTE. Additionally, for regularization we make use of dropout with a rate of
0.2 on the computed non-contextual word representations ew defined in §3.1 with the same dropout
mask for all words in a batch. All our models were trained with 3 different random seeds and the
top performance is reported.
13