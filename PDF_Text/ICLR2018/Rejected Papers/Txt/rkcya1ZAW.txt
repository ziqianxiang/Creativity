Under review as a conference paper at ICLR 2018
Continuous-Time Flows for Efficient Infer-
ence and Density Estimation
Anonymous authors
Paper under double-blind review
Ab stract
Two fundamental problems in unsupervised learning are efficient inference for
latent-variable models and robust density estimation based on large amounts of
unlabeled data. For efficient inference, normalizing flows have been recently de-
veloped to approximate a target distribution arbitrarily well. In practice, however,
normalizing flows only consist of a finite number of deterministic transformations,
and thus they possess no guarantee on the approximation accuracy. For density
estimation, the generative adversarial network (GAN) has been advanced as an
appealing model, due to its often excellent performance in generating samples.
In this paper, we propose the concept of continuous-time flows (CTFs), a family
of diffusion-based methods that are able to asymptotically approach a target dis-
tribution. Distinct from normalizing flows and GANs, CTFs can be adopted to
achieve the above two goals in one framework, with theoretical guarantees. Our
framework includes distilling knowledge from a CTF for efficient inference, and
learning an explicit energy-based distribution with CTFs for density estimation.
Experiments on various tasks demonstrate promising performance of the proposed
CTF framework, compared to related techniques.
1	Introduction
Efficient inference and robust density estimation are two important goals in unsupervised learning. In
fact, inference and density estimation can be unified from the perspective of learning desired target
distributions. In inference problems, one is given an unnormalized distribution (e.g., the posterior
distribution), and the goal is to learn a distribution that is close to the unnormalized distribution. In
density estimation, one tries to learn an unknown data distribution given samples from it. It is also
helpful to make a distinction between two types of representations for learning distributions: explicit
and implicit methods (Mohamed & Lakshminarayanan, 2017). Explicit methods provide a prescribed
parametric form for the distribution, while implicit methods learn a stochastic procedure to directly
generate samples from the unknown distribution.
Existing deep generative models can easily be identified from this taxonomy. For example, the
standard variational autoencoder (VAE) (Kingma & Welling, 2014; Rezende et al., 2014) is an impor-
tant example of an explicit inference method. Within the inference arm (encoder) of a VAE, recent
research has focused on improving the accuracy of the approximation to the posterior distribution
on latent variables (codes) using normalizing flow (Rezende & Mohamed, 2015). Normalizing flow
is particularly interesting due to its ability to approximate the posterior distribution arbitrarily well,
while maintaining explicit parametric forms. On the other hand, Stein VAE (Feng et al., 2017; Pu
et al., 2017b) is an implicit inference method, as it only learns to draw samples to approximate
posteriors, without assuming an explicit form for the distribution.. For density estimation on observed
data, the generative adversarial network (GAN) can be regarded as an implicit density estimation
method (Ranganath et al., 2016; HUszS 2017; Mohamed & Lakshminarayanan, 2017), in the sense
that one may sample from the distribution (regarded as a representation of the unknown distribution),
bUt an explicit form for the distribUtion is not estimated. GAN has recently been aUgmented by
Flow-GAN (Grover et al., 2017) to incorporate a likelihood term for explicit density estimation.
FUrther, there also are some works trying to perform inference within the implicit density estimation
framework, e.g., the real-valUed non-volUme preserving (real NVP) transformations algorithm (Dinh
et al., 2017) was proposed as a tractable yet expressive approach to model high-dimensional data.
1
Under review as a conference paper at ICLR 2018
Some aforementioned methods rely on the concept of flows. A flow defines a series of transformations
for a random variable (RV), such that the distribution of the RV evolves from a simple distribution to
a more complex distribution. When the sequence of transformations are indexed on a discrete-time
domain (e.g., indexed with integers) with a finite number of transformations, this method is referred to
as a normalizing flow (Rezende & Mohamed, 2015). Various efficient implementations of normalizing
flows have been proposed, such as the planar, radial (Rezende & Mohamed, 2015), Householder
(Tomczak & Welling, 2016), and inverse autoregressive flows (Kingma et al., 2016). One theoretical
limitation of existing normalizing flows is that there is no guarantee on the approximation accuracy
due to the finite number of transformations.
By contrast, little work has explored the applicability of continuous-time flows (CTFs) in deep
generative models, where a sequence of transformations are indexed on a continuous-time domain
(e.g., indexed with real numbers). There are at least two reasons encouraging research in this direction:
i) CTFs are more general than traditional normalizing flows in terms of modeling flexibility, due
to the intrinsic infinite number of transformations; ii) CTFs are more theoretically grounded, in the
sense that they are guaranteed to approach a target distribution asymptotically (details provided in
Section 2.2). Unfortunately, these advantages also bring challenges for efficient learning, in that: i)
it is difficult to optimize over the variational lower bound in the inference framework, due to the
extra randomness introduced in CTFs; ii) it is difficult to design algorithms for efficient learning of
CTF-based models, due to the induced infinite number of transformations.
In this paper, we propose efficient ways to apply CTFs for the two motivating tasks. Based on the
CTF, our framework learns to drawn samples directly from desired distributions (e.g., the unknown
posterior and data distributions) for both inference and density estimation tasks. In addition, we are
able to learn an explicit form of the unknown data distribution for density estimation*. This shares a
similar flavor as Wang & Liu (2017); Feng et al. (2017). Specifically, i) for efficient inference, we
first show that optimizing the variational lower bound with CTFs can be achieved by decomposing
the optimization problem into a sequence of sub-optimization problems, based on a variational
formulation of the Fokker-Planck equations from statistical physics (Jordan et al., 1998). Based on
this decomposition, we derive bounds on the approximation errors when applying numerical methods
to solve a CTF. For computational efficiency, we generalize ideas from Gershman & Goodman (2014)
to distill knowledge of a CTF into an efficient inference network; ii) for density estimation, we
propose to use a flexible Gibbsian-style distribution (implemented by a deep neural network) to model
an unknown data distribution, whose samples can be drawn by learning a stochastic generator with
our CTF framework. The Gibbsian-style data distribution and the stochastic generator are learned
alternatively, leading to a learning procedure that is connected to the GAN framework (Goodfellow
et al., 2014), but that yields an explicit distribution for the data. We conduct a number of experiments
on real datasets, demonstrating excellent performance of the proposed framework, relative to existing
representative approaches.
2	Preliminaries
We first review related techniques of performing efficient inference and density estimation in the
machine learning literature. We then introduce the general concept of continuous-time flows.
2.1	Efficient inference and density estimation
Efficient inference with normalizing flows Consider a probabilistic generative model with ob-
servation X ∈ RD and latent variable Z ∈ RL such that X | Z 〜pθ(X | Z) with Z 〜p(z). For
efficient inference of z, the VAE (Kingma & Welling, 2014) introduces the concept of an inference
network (recognition model or encoder), qφ(Z | X), as a variational distribution in the VB frame-
work. An inference network is typically a stochastic (nonlinear) mapping from the input X to the
latent Z, with associated parameters φ. For example, one of the simplest inference networks is
defined as qφ(z | x) = N(z; μφ(x), diag(σφ(x))), where the mean function μφ(x) and the standard-
derivation function σφ(X) are specified via deep neural networks parameterized by φ. Parameters are
learned by minimizing the evidence lower bound (ELBO), i.e., the KL divergence between pθ (X, Z)
* Although the density is represented as an energy-based distribution with an intractable normalizer.
2
Under review as a conference paper at ICLR 2018
de0
∂Zk
and qφ(z | x): KL (qφ(z | x)kpθ(x, z)) , Eqφ(z | x) [log qφ (z | x) - log pθ (x, z)], via stochastic gradient
descent (Bottou, 2012).
One limitation of the VAE framework is that qφ (z | x) is often restricted to simple distributions for
feasibility, e.g., the normal distribution discussed above, and thus the gap between qφ(z | x) and
pθ(z | x) is typically large for complicated posterior distributions. Normalizing flows is a recently
proposed VB-based technique designed to mitigate this problem (Rezende & Mohamed, 2015). The
idea is to augment z via a sequence of deterministic invertible transformations {Tk : RL → RL}kK=1,
such that: zo 〜q0(∙∣ x), zι = T1(zo),…，ZK = Tk(zκ-ι).
Note the transformations {Tk} are typically endowed with different parameters, and we absorb
them into φ. Because the transformations are deterministic, the distribution of zK can be written as
q(zκ) = qφ(zo | x) QK=Jdet∂T ∣ via the change of variable formula. As a result, the ELBO for
normalizing flows becomes:
KL(qφ(zK | x)kpθ (x, z)) =	(1)
K
Eqφ(zo | x) [log qφ(ZO | x)] - Eqφ(zo | x) [logPθ(X, zK)] - Eqφ(z。| x) E log
k=1
Typically, transformations Tk of a simple parametric form are employed to make the computations
tractable (Rezende & Mohamed, 2015). Our method for inference generalizes these discrete-time
transformation to continuous-time transformations, ensuring convergence of the transformations to
the target distribution.
Density estimation overview There exist implicit and explicit density-estimation methods. Implicit
density models such as GAN provide a flexible way to draw samples directly from unknown data
distributions (via a deep neural network (DNN) called a generator with stochastic inputs) without
explicitly modeling their density forms; whereas explicit models such as the pixel RNN/CNN (van den
Oord et al., 2016) define and learn explicit forms of the unknown data distributions. This gives the
advantage that the likelihood for a test data point can be explicitly evaluated. However, the generation
of samples is typically time-consuming due to the sequential generation nature.
Similar to Wang & Liu (2017), our CTF-based approach in Section 4 provides an alternative way
for this problem, by simultaneously learning an explicit Gibbsian-style data distribution (estimated
density) and a generator whose generated samples match the learned Gibbsian distribution. This
not only gives us the advantage of explicit density modeling but also provides an efficient way to
generate samples.
2.2	Continuous-time flows
We notice two potential limitations with traditional normalizing flows: i) given specified transfor-
mations {Tk }, there is no guarantee that the distribution of ZK could exactly match pθ(x, Z); ii)
the randomness is only introduced in Z0 (from the inference network), limiting the representation
power. We specify CTFs where the transformations are indexed by real numbers, thus they could be
considered as consisting of an infinite number of transformations. Further, we consider stochastic
flows where randomness is injected in a continuous-time manner. In fact, the concept of CTFs
(such as the Hamiltonian flow) has been introduced in Rezende & Mohamed (2015), without further
development on efficient inference.
We consider a flow on Rl, defined as the mapping* T : RL X R → RL such that^ We have
T(Z, 0) = Z and T(T(Z, t), s) = T(Z, s + t), for all Z ∈ RL and s, t ∈ R. A typical example of
this family is defined as T(Z, t) = Zt , where Zt is driven by a diffusion of the form:
dZt=F(Zt)dt+V(Zt)dW.	(2)
tWe reuse the notation T as transformations from the discrete case above for simplicity, and use Z instead of
z (reserved for the discrete-time setting) to denote the random variable in the continuous-time setting.
^ Note we define continuous-time flows in terms of latent variable Z in order to incorporate it into the setting
of inference. However, the same description applies when we define the flow in data space, which is the setting
of density estimation in Section 4.
3
Under review as a conference paper at ICLR 2018
Here F : RL → RL , V : RL×L → RL are called the drift term and diffusion term, re-
spectively; W is the standard L-dimensional Brownian motion. In the context of inference, we
seek to make the stationary distribution of Zt approach pθ(z | x). One solution for this is to set
F(Zt) = 2Vz logpθ(x, Z = Zt) and V(Zt) = IL with IL the L X L identity matrix. The resulting
diffusion is called Langevin dynamics Welling & Teh (2011). Denoting the distribution of Zt as ρt, it
is well known Risken (1989) that ρt is characterized by the Fokker-Planck (FP) equation:
务=-Vz ∙(ρtF(Zt)) + VVz ： (PtV(Zt)V>(Zt)),
(3)
where a ∙ b，a> b for vectors a and b, A: B，trace(A> B) for matrices A and B.
For simplicity, we consider the flow defined by the Langevin dynamics specified above, though our
results generalize to other stochastic flows Dorogovtsev & Nishchenko (2014). In the following,
we specify the ELBO under a CTF, which can then be readily solved by a discretized numerical
scheme, based on the results from Jordan et al. (1998). An approximation error bound for the scheme
is also derived. We defer proofs of our theoretical results to the Supplementary Material (SM) for
conciseness.
3	Continuous-Time Flows for Inference
We first give an overview of our CTF-based method for efficient inference. We adopt the
VAE/normalizing-flow framework with an encoder-decoder structure. An important difference
is that instead of feeding data to an encoder and sampling a latent representation in the output as in
VAE, we concatenate the data with independent noise as input and directly generate output samples§.
These output samples are then driven by the CTF to approach the true posterior distribution. In
the learning process, the implicit transformations from the CTF are sequentially distilled into the
inference network by amortized learning, making the inference network flexible enough to represent
the true posterior distribution. In the following subsections, we specify our framework in detail.
3.1	The variational lower bound and discretized approximation
We first incorporate CTF into the normalizing-flow framework by writing out the corresponding
ELBO. Note that there are two steps in the inference process. First, an initial z0 is drawn from
the inference network q°(∙∣ x); second, z° is evolved via a diffusion such as (2) for time T (via the
transformation ZT = T (z0, T)). Consequently, the ELBO for CTF can be written as
F(x) = Eqφ(zo ∣χ)Eρτ 卜og PT — logPθ(x, ZT) + log detd|^]，Eqψ(zo | x) [Fι(x, zo)] . (4)
Note the term F1(x, z0) is intractable to calculate, in that i) ρT does not have an explicit form; ii)
the Jacobian d^zT- is generally infeasible. In the following, we propose an approximate solution for
problem i). Learning by avoiding problem ii) is presented in Section 3.2 via amortization.
For problem i), a reformulation of the results from Jordan et al. (1998) leads to a nice way to
approximate Pt in Lemma 1. Note in practice we adopt an implicit method which uses samples to
approximate the solution in Lemma 1 for feasibility, detailed in (6).
Lemma 1. Assume that log pθ (x, z) ≤ C1 is infinitely differentiable, and kVz logpθ(x, z)k ≤
C2 (1 + C1 - logpθ(x, z)) (∀x, z) for some constants {C1, C2}. Let T = hK (h is the stepsize
in discretization and K is the number of transformations), ρo ，qφ(zo | x), and {pk}K=ι be the
solution of the functional optimization problem:
Pk = arg minKL (p∣∣pe(x,z)) + ɪ W2 (pk-i,p) ,	(5)
ρ∈K	2h
where W2 (μι, μ2)，infp∈p(μ1,μ2) R ∣∣x 一 y∣∣2p(dx, dy), W2 (μι, μ2) is the 2nd-order Wasser-
Stein distance, with P (μ1,μ2) being the space ofjoint distributions on {μ1,μ2} ∙ K is the space of
probability distributions with the finite 2nd-order moment. Then PK converges to PT in the limit of
h → 0, i.e., limh→o Pκ = pt, where PT is the solution ofthe FP equation (3) at time T.
§Such structure can represent much more complex distributions than a parametric form, useful for following
procedures. We argue an explicit distribution form is not as important in inference as that in density estimation.
4
Under review as a conference paper at ICLR 2018
Lemma 1 reveals an interesting way to compute ρT via a sequence of functional optimization
problems. By comparing it with the objective of the traditional normalizing flow, which minimizes
the KL-divergence between ρK and pθ(x, z), at each sub-optimization-problem in Lemma 1, it
minimizes the KL-divergence between Pk andpe(x, z), plus a regularization term as the Wasserstein
distance between ρk-ι and Pk. The extra Wasserstein-distance term arises naturally due to the fact
that the Langevin diffusion can be explained as a gradient flow whose geometry is equipped with
the Wasserstein distance (Otto, 1998). From another point of view, it is known that the Wasserstein
distance is a better metric for probability distributions than the KL-divergence, especially in the
case of non-overlapping domains (Arjovsky & Bottou, 2017; Arjovsky et al., 2017). By using
the Wasserstein term as a regularizer, the CTF alleviates the issue in non-overlapping domains
by introducing the Brownian-motion (noise) term in the evolution (2). This relates to the idea in
(Arjovsky & Bottou, 2017), in which noise is added in parameter updates to alleviate the intrinsic
drawback of the KL-divergence metric.
The optimization problem in Lemma 1 is difficult to deal with directly. In practice, we instead
approximate the discretization in an equivalent way by simulation from the CTF. Starting from z0,
Zk (k = 0,…，K - 1) is fed into a transformation Tk (specified below), resulting in Zk+ι whose
distribution coincides with pk+ι in Lemma 1. The discretization procedure is illustrated in Figure 1.
We must specify the transformations Tk. For each k, let t = hk; we can conclude from Lemma 1 that
limh→o Pk = ρt. From FP theory, Pt is obtained by solving the diffusion (2) with initial condition
Z0 = z0 . It is thus reasonable to specify the transformation Tk as the k-th step of a numerical
integrator for (2). Specifically, we specify Tk as a stochastic transformation:
zk = Tk (zk-1 ) , zk-1 +F (zk-1 )h + V (zk-1 )ζk ,
(6)
where Zk ~ N(0, AIl) is drawn from an isotropic normal. Note the transformation defined here
is stochastic, thus we only get samples from PK at the end. A natural way to approximate PK is
to use the empirical sample distribution, i.e., PK ≈ K PK=I δzk，PT with δz a point mass at z.
Afterwards, PK (thus PT) will be used to approximate the true PT from (3).
Figure 1: Discretized approximation (right) of a continuous-time flow (left). Densities {Pk} of {zk}
evolve via transformations {Tk}, with Pk → Phk when h → 0 for each k due to Lemma 1.
Note that we use the simple sample averaging for the convenience of analysis, and the approximation
for pt is not necessarily optimal. Better approximation can be obtained by assigning more weights to
the more recent samples. However, this leads to more challenges in theoretical analysis, an interesting
future direction to pursue. In the following, we study how well PT approximates PT. Following
literature on numerical approximation for It6 diffusions (Vollmer et al., 2016; Chen et al., 2015), we
consider a 1-Lipschitz test function ψ : RL → R, and use the mean square error (MSE) bound to
measure the closeness of PT and PT, defined as: MSE(PT, ρT; ψ)，E (/ ψ(z)(ρT - ρT)(z)dZ),
where the expectation is taken over all the randomness in the construction of PT. Note that our goal
is related but different from the standard setup as in Vollmer et al. (2016); Chen et al. (2015), which
studies the closeness of PT to pθ(x, z). We need to adopt the assumptions from Vollmer et al. (2016);
Chen et al. (2015), which are described in the Supplementary Material (SM). The assumptions are
somewhat involved but essentially require coefficients of the diffusion (2) to be well-behaved. We
derive the following bound for the MSE of the sampled approximation, PT, and the true distribution.
5
Under review as a conference paper at ICLR 2018
Theorem 2. Under Assumption 1 in the SM, assume that ρT (z)pθ-1(x, z)dz < ∞ and there exists
a constant C such that dW2 (ρT,Pθ (x,z)) ≥ CW22 (PT ,pθ (x, z)) ,the MSE is bounded as
MSE(pτ, PT； Ψ) = O 1 ɪ + h2 + e-2ChK
hK
The last assumption in Theorem 2 requires PT to evolve fast through the FP equation, which
is a standard assumption used to establish convergence to equilibrium for FP equations (Bolley
et al., 2012). The MSE bound consists of three terms, the first two terms come from numerical
approximation of the continuous-time diffusion, whereas the third term comes from the convergence
bound of the FP equation in terms of the Wasserstein distance (Bolley et al., 2012). When the time
T = hK is large enough, the third term may be ignored due to its exponential-decay rate. Moreover,
in the infinite-time limit, the bound endows a bias proportional to h; this, however, can be removed by
adopting a decreasing-step-size scheme in the numerical method, as in standard stochastic gradient
MCMC methods (Teh et al., 2016; Chen et al., 2015).
Remark 3. To examine the optimal bound in Theorem 2, we drop out the term e-2ChK in the
long-time case (when hK is large enough) for simplicity because it is in a much lower order term
than the other terms. The optimal MSE bound (over h) decreases at a rate of O K -2/3 , meaning
that O -3/2 steps of transformations in Figure 1 (right) are needed to reach an -accurate
approximation, i.e., MSE ≤ . This is computationally expensive. An efficient way for inference is
thus imperative, developed in the next section.
3.2	Efficient inference via amortization
Even though We approximate PT with PT, it is still infeasible to directly apply it to the ELBO in (4)
as PT is discrete. To deal with this problem, we adopt the idea of “amortized learning” (Gershman &
Goodman, 2014) for efficient inference. The main idea is to optimize the two sets of parameters φ
and θ alternatively, based on different but related objective functions.
Updating φ To explain the idea, first note that the ELBO can be equivalently written as
F(x) = Eρ0,qφ(z0 | x)EρT[logP0 - logpθ(x, ZT)] .	(7)
When P0 = PT, it is easy to see that: F(x) = Eρ0 [log P0 - logpθ(ZT | x)] + log p(x) = log p(x),
which essentially makes the gap between qφ(z0 | x) andpθ(ZT | x) vanished. As a result, our goal
is to learn φ such that qφ(z0 | x) approaches pθ(ZT | x). As mentioned previously, we will learn
an implicit distribution of qφ(z0 | x) (i.e., learn how to draw samples from qφ(z0 | x) instead of
its explicit form), as it allows us to chose a candidate distribution from a much larger distribution
space, compared to explicitly defining q°t Consequently, q0(z0 | x) is implemented by a stochastic
generator (a DNN parameterized by φ) Qφ(z0 | x, ω) with input as the concatenation of x and ω,
where ω is a sample from an isotropic Gaussian distribution q0(ω). Our goal is now translated to
update the parameter φ of Qφ(z0 | x, ω) to φ0 such that the distribution of {z00 = Qφ0 (z00 | x, ω)}
with ω 〜qo(ω) matches that of zι in the original generating process with φ in Figure 1. In this way,
the generating process of zι via T is distilled into the parameterized generator Qφ(∙), eliminating
the need to do a specific transformation via T1 in testing, and thus is very efficient. Specifically, we
update φ0 such that
φ0 = arg mφin D {z00(i)},{z(1i)} ,	(8)
where {z0(i)}S=ι are a set of samples generated from qΦ，(z0 | x) via Qφ(∙), and {zf)}S=ι are samples
drawn by ωi 〜 qo(ω), Z0 = Qφ(∙∣ x,ωi),z1i) 〜 T1(Z0); D(∙, ∙) is a metric between samples such as
the simple Euclidean distance or the more advanced Wasserstein distance (Arjovsky et al., 2017). The
optimization is done by applying standard stochastic gradient descent (SGD). We call this procedure
distilling knowledge from T to Qφ(∙).
TThiS is distinct from our density-estimation framework described in the next section, where an explicit form
is assumed at the beginning for practical needs.
6
Under review as a conference paper at ICLR 2018
After distilling knowledge from T1 , we apply the same procedure for other transformations Tk (k >1)
sequentially. The final inference network, represented by qφ(∙∣ x), can then well approximate the
continuous-time flows, e.g., the distribution of zo 〜q0(∙∣ x) is close to PT from the CTF This
concept is illustrated in Figure 2. According to Theorem 2, the number of updates for φ in training
is still bounded by O(-3/2) for an -accurate MsE, however, inference in testing is significantly
boosted since we do not need to simulate a long-time transformations as shown in Figure 1 (right).
Updating θ Given φ, θ can be updated by simply optimizing the ELBo in (7), where PT is
approximated by PT from the discretized CTF. Specifically, the expectation w.r.t. PT in (7) is
approximated by a sample average from:
z0 〜qφ(ZO | X), z1 〜TI(ZO), z2 〜T2 (ZI),…，zK 〜TK (ZK-I).
!"(ZO一')
*+ ('一Z)
…step K
*+ ('一Z)
!"(ZO-')
P+ ('-Z)
→
句…m
Figure 2: Amortized learning of continuous-time flows for VAEs. From left to right: the initial
architecture with K-step transformations; For each step k, qφ(∙) is trained to match the distributin of
Zk in CTFs; In the end, the CTF is distilled into qφ(∙).
To sum up, there are three main steps in learning a CTF-based VAE:
1.	Generate a sample path (zo,…，ZK) according to qψ(zo | x) and the discretized flow with
transformations {Tk};
2.	Update φ according to (8);
3.	Optimize θ by minimizing the ELBO(7) with the generated sample path.
In testing, we use only the finally learned q0(z0 | x) for inference (into which the CTF has been
distilled), and hence testing is like the standard VAE. Since the discretized-CTF model is essentially
a Markov chain, we call our model Markov-chain-based VAE (MacVAE).
4	Continuous Time Flows for Explicit Density Estimation
We describe how to apply the proposed CTF framework to density estimation of the observed data.
We assume that the density of the observation x is characterized by a parametric Gibbsian-style
probability model pθ(x) = z(^pθ(x)，^^)eu(X⑻，where pθ(x) is an unnormalized version of
Pθ(x) with parameter θ, U(x; θ)，logpθ(x) is called the energy function (Zhao et al., 2017), and
Z(θ)，Jpθ (x)d X is the normalizer. Note this form of distributions constitutes a very large class
of distributions as long as the capacity of the energy function is large enough. This can be easily
achieved by adopting a DNN to implement U(x; θ), the setting we considered in this paper. Note our
model can be placed in between existing implicit and explicit density estimation methods, because
we model the data density with an explicit distribution form up to an intractable normalizer. such
distributions have been proved to be useful in real applications, e.g., Haarnoja et al. (2017) used them
to model policies in deep reinforcement learning.
our goal is to learn θ given observations {xi}iN=1 , which can be achieved via the standard maximum
likelihood estimator (MLE):
N
θ = arg max	logpθ(xi) , arg max M({xi}; θ)
θθ
i=1
This is usually optimized via sGD, with the following gradient formula:
∂M({xi}; θ) _ 1 XX ∂UM; θ) _ E
∂θ = N 乙—∂θ	pθ(X)
i=1
∂U(x; θ)
∂θ
(9)
7
Under review as a conference paper at ICLR 2018
Algorithm 1 CTFs for generative models at the k-th iteration. D(∙, ∙) is the same as (8).
Input: parameters from last step θ(k-1), φ(k-1)
Output: updated parameters θ(k) , φ(k)
1.	Generate samples {xι,s}S=ι via a discretized CTF: χo,s 〜qφ(k-i) (χo), χι,s 〜T(χo,s);
2.	Update the generator by minimizing ({x00,s}sS=1 are generated with the updated parameter φ(k)):
φ(k) = argmin D ({x1,s}, {x0,s}).
φ
3.	Update the energy-based model θk by maximum likelihood, with gradient as (9) except replacing
EX〜Pθ(x) With EX〜qφ(χ);
The gradient formula requires an integration over the model distribution pθ (x), which can be
approximated by Monte Carlo integration With samples. The sampling problem has been Well studied
for some particular energy-based distributions, for example, via contrastive divergence in restricted
Boltzmann machines (Hinton, 2002). HoWever, this does not fit into our setting directly. Here We
adopt the idea of CTFs and propose to use a DNN guided by a CTF, Which We call a generator, to
generate approximate samples from the original model pθ(x). Specifically, We require that samples
from the generator should Well approximate the target pθ (x). This can be done by adopting the CTF
idea above, i.e., distilling knoWledge of a CTF (Which approaches pθ(x)) to the generator. In testing,
instead of generating samples from pθ (x) via MCMC (Which is complicated and time consuming),
We generate samples from the generator directly. Furthermore, When evaluating the likelihood for
test data, the unknoWn constant Z(θ) ofpθ(x) can also be approximated by Monte Carlo integration
With samples draWn from the generator.
On the right side of (9), the first term is a model fit to observed data, and the (negative) second term
is a model fit to synthetic data draWn from pθ(X); this is similar to the critic/discriminator in GANs
(Arjovsky et al., 2017), but derived directly from the MLE. More connections are discussed beloW.
4.1	LEARNING VIA AMORTIZATION
Our goal is to learn a generator whose generated samples
match those from the original model pθ (x), by adopting the
amortization idea with CTF in the inference section above.
Similar to inference, the generator is learned implicitly.
However, we also learn an explicit density model for the
data by SGD, with samples from the implicit generator to
estimate gradients in (9). Note that in this case, the CTF is
performed directly on the data space, instead of on latent-
variable space as in previous sections. Specifically, the
sampling procedure from the generator plus a continuous-
time-flow transformation are written as:
X0 〜qφ(xo), XT 〜T(X0,T).
Figure 3: Learning a generator with
CTF. The goal is to match the samples
xo from qφ to those after a CTF (XT),
or equivalently samples from pθ.
Here T(∙, ∙) is the continuous-time flow; a sample xo from qφ(∙) is implemented by a deep neural
network (generator) Gφ(ω) with input ω 〜qo(ω), where qo is a simple distribution for a noise
random variable, e.g., the standard isotropic normal distribution. The procedure is illustrated in
Figure 3. Note the CTF cannot be replaced by standard normalizing flow (Rezende & Mohamed,
2015) in this model, because there is no objective function to guide the update of parameters in
normalizing flows, which is not necessary for CTFs.
Specifically, denote the parameters in the k-th step of our algorithm with subscript “(k)”. For efficient
sample generation, in the k-th step, we again adopt the amortization idea from Section 3.2 to update
φ(k-1) of the generator network Gφ(∙), such that samples from the updated generator match those
from the current generator followed by a one-step transformation 71 (∙). After that, θ is updated by
drawing samples from q°(∙) to estimate the expectation in (9). The detailed algorithm is presented in
Algorithm 1.
8
Under review as a conference paper at ICLR 2018
4.2	Connections to Wasserstein GAN (WGAN) and MLE
There is an interesting relation between our model and the WGAN framework (Arjovsky et al., 2017).
To see this, let pr be the data distribution. Substituting pθ(x) with qφ(x) for the expectation in the
gradient formula (9) and integrating out θ, we have that our objective is
maxEx〜pr [U(x; θ)] - Ex〜qΦ [U(x; θ)]	(10)
The objective is an instance of the general integral probability metrics (Arjovsky & Bottou, 2017).
When U is chosen to be 1-Lipschitz functions, it recovers WGAN. This connection motivates us to
introduce weight clipping (Arjovsky et al., 2017) or alternative regularizers (Gulrajani et al., 2017)
when updating θ for a better theoretical property. For this reason, we call our model Markov-chain-
based GAN (MacGAN).
Furthermore, it can be shown by Jensen’s inequality that the MLE is bounded by (detailed derivations
are provided in Section C of the SM)
1N
max N ElogPθ(Xi) ≤ maxEx〜p,[U(x; θ)] - Ex〜qφ [U(x； θ)] - Ex〜qφ [logqφ] .	(11)
N i=1
By inspecting (10) and (11), it is clear that: i) when learning the energy-based model parameters θ,
the objective can be interpreted as maximizing an upper bound of the MLE shown in (11); ii) when
optimizing the parameter φ of the inference network, we adopt the amortized learning procedure
presented in Algorithm 1, whose objective is minφ KL (qφkpθ), coinciding with the last two terms in
(11). In other words, both θ and φ are optimized by maximizing the same upper bound of the MLE,
guaranteeing convergence of the algorithm. Particularly, we can conclude that
Proposition 4. The optimal solution of MacGAN is the maximum likelihood estimator.
Note another difference between MacGAN and standard GAN framework is the way of learning the
generator qφ . We adopt the amortization idea, which directly guides qφ to approach pθ ; whereas in
GAN, the generator is optimized via a min-max procedure to make it approach the empirical data
distribution pr . By explicitly learning pθ , MacGAN is able to evaluate likelihood for test data (at
least up to a constant).
5	Related Work
Our framework extends the idea of normalizing flows (Rezende & Mohamed, 2015) to continuous-
time flows, by developing theoretical properties on the convergence behavior. Inference based on
CTFs has been studied in Salimans et al. (2015) based on the auxiliary-variable technique. However,
Salimans et al. (2015) directly uses discrete approximations for the flow, and the approximation
accuracy is unclear. Moreover, the inference network requires simulating a long Markov chain for the
auxiliary model, thus is less efficient than ours. Finally, the inference network is implemented as a
parametric distribution (e.g., the Gaussian distribution), limiting the representation power, a common
setting in existing auxiliary-variable based models (Tran et al., 2016). The idea of amortization
(Gershman & Goodman, 2014) has recently been explored in various research topics for Bayesian
inference such as in variational inference (Kingma & Welling, 2014; Rezende et al., 2014) and
Markov chain Monte Carlo (Wang & Liu, 2017; Li et al., 2017; Pu et al., 2017a). Both Wang & Liu
(2017) and Pu et al. (2017a) extend the idea of Stein variational gradient descent (Liu & Wang, 2016)
with amortized inference for a GAN-based and a VAE-based model, respectively, which resemble
our proposed MacVAE and MacGAN in concept. Li et al. (2017) applies amortization to distill
knowledge from MCMC to learn a student network. The ideas in Li et al. (2017) are similar to ours,
but the motivation and underlying theory are different from that developed here.
6	Experiments
We conduct experiments to test our CTF-based framework for efficient inference and density estima-
tion described above, and compared them with related methods. The implementation is based on the
excellent code for SteinGANk Wang & Liu (2017), where we adopt their default parameter setting.
k https://github.com/DartML/SteinGAN
9
Under review as a conference paper at ICLR 2018
qo(z)	P(Z)
Figure 4: Knowledge distillation from the CTF (left) and ELBO versus epochs on MNIST (right).
VAE with 80-layer NF is not included because it has much more parameters.
The discretization stepsize h is robust as long as it is set in a reasonable range, e.g., we set it the same
as the stepsize in SGD.
6.1	CTFs for inference
Synthetic experiment We examine our amortized learning framework with a toy experiment.
Following Rezende & Mohamed (2015), we use MacVAE to approximate samples from a two di-
mensional distribution on Z = {z1, z2}: P(Z) a e-U(Z) With U(Z)，ɪ (k Z0kj2 )2 - ln(e-2 [Zo-2]2 +
e- 1 [~θ+~]2). The inference network qφ is defined to be a 2-layer MLP with isotropic normal random
variables as input. Figure 4 (top) plots the densities estimated With the samples from transformations
{TK=100} (before optimizing φ), as well as with samples generated directly from qφ (after optimizing
φ). It is clear that the amortized learning is able to distill knowledge from the CTF to the inference
network.
MacVAE on MNIST Following Rezende & Mohamed (2015); Tomczak & Welling (2016), we
define the inference network as a deep neural network with two fully connected layers of size 300
with softplus activation functions. We compare MacVAE with the standard VAE and the VAE with
normalizing flow, where testing ELBOs are reported (Section D.1 of the SM describes how to calculate
the ELBO). We do not compare with other state-of-the-art methods such as the inverse autoregressive
flow (Kingma et al., 2016), because they typically endowed more complicated inference networks
(with more parameters), unfair for comparison. We use the same inference network architecture for all
the models. Figure 4 (bottom) plots the testing ELBO versus training epochs. MacVAE outperforms
VAE and normalizing flows with a better ELBO (around -85.62).
6.2	CTFs for density estimation
We test MacGAN on three datasets: MNIST, CIFAR-10 and CelabA. Following GAN-related
methods, the model is evaluated by observing its ability to draw samples from the learned data
distribution. Inspiring by Wang & Liu (2017), we define a parametric form of the energy-based
model as pθ(x) Z exp{- ∣∣x —DECθ (ENCθ(X))II2}, where ENCθ(∙) andDECθ(∙) are encoder and
decoder defined by using deep convolutional neural networks and deconvolutional neural networks,
respectively, parameterized by θ . For simplicity, we adopt the popular DCGAN architecture (Radford
et al., 2016) for the encoder and decoder. The generator Gφ is defined as a 3-layer convolutional
neural network with the ReLU activation function (except for the top layer which uses tanh as the
activation function, see SM D for details). Following Wang & Liu (2017), the stepsizes are set to
(m-；50lr, where e indexes the epoch, me is the total number of epochs, lr = 1e-4 when updating θ,
me -50
and lr = 1e-3 when updating φ. The stepsize in L1 is set to 1e-3.
We compare MacGAN with DCGAN (Radford et al., 2016), the improved WGAN (WGAN-I)
(Gulrajani et al., 2017) and SteinGAN (Wang & Liu, 2017). We plot images generated with MacGAN
and its most related method SteinGAN in Figure 5 for CelebA and CIFAR-10 datasets. More results
10
Under review as a conference paper at ICLR 2018
airplane
automobile
bird
Cat
deer
dog
frog
horse
ship
truck
Figure 5: Generated images for CIFAR-10 (top) and CelebA (middle) datasets with MacGAN (left)
and SteinGAN (right). The bottom are images generated by a random walk on the ω space for the
generator of MacGAN, i.e., ωt = ωt-ι + 0.03 X rand([-1,1]).
are provided in SM Section D. We observe that visually MacGAN is able to generate clear-looking
images. Following Wang & Liu (2017), we also plot the images generated by a random walk in the ω
space in Figure 5.
Qualitatively evaluating a GAN-like model is chal-
lenging. We follow literature and use the inception
score (Salimans et al., 2016) to measure the quantity
of the generated images. Figure 6 plots inception
scores versus training epochs for different models.
MacGAN obtains competitive inception scores with
the popular DCGAN model. Quantitatively, we get
a final inception score of 6.49 for MacGAN, com-
pared to 6.35 for SteinGAN, 6.25 for WGAN-I and
6.58 for DCGAN.
Figure 6: Inception score versus epochs for
different models.
11
Under review as a conference paper at ICLR 2018
7	Conclusion
We study the problem of applying CTFs for efficient inference and explicit density estimation in deep
generative models, two important tasks in unsupervised machine learning. Compared to discrete-time
normalized flows, CTFs are more general and flexible due to the fact that their stationary distributions
can be controlled without extra flow parameters. We develop theory on the approximation accuracy
when adopting a CTF to approximate a target distribution. We apply CTFs on two classes of deep
generative models, a variational autoencoder for efficient inference, and a GAN-like density estimator
for explicit density estimation and efficient data generation. Experiments show encouraging results
of our framework in both models compared to existing techniques. One interesting direction of future
work is to explore more efficient learning algorithms for the proposed CTF-based framework.
References
M. Arjovsky and L. Bottou. Towards principled methods for training generative adversarial networks.
In ICLR, 2017.
M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein GAN. Technical Report arXiv:1701.07875,
March 2017.
F. Bolley, I. Gentil, and A. Guillin. Convergence to equilibrium in wasserstein distance for
fokker-Planck equations. Journal of Functional Analysis, 263(8):2430-2457, 2012.
L. Bottou. Stochastic gradient descent tricks. Technical report, Microsoft Research, Redmond, WA,
2012.
C. Chen, N. Ding, and L. Carin. On the convergence of stochastic gradient MCMC algorithms with
high-order integrators. In NIPS, 2015.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvP. ICLR,
2017.
A. A. Dorogovtsev and I. I. Nishchenko. An analysis of stochastic flows. Communications on
Stochastic Analysis, 8(3):331-342, 2014.
Y. Feng, D. Wang, and Q. Liu. Learning to draw samPles with amortized Stein variational gradient
descent. In UAI, 2017.
S. J. Gershman and N. D. Goodman. Amortized inference in Probabilistic reasoning. In Annual
Conference of the Cognitive Science Society, 2014.
C. R. Givens and R. M. Shortt. A class of wasserstein metrics for Probability distributions. Michigan
Math. J., 31, 1984.
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and
Y. Bengio. Generative adversarial nets. In NIPS, 2014.
A. Grover, M. Dhar, and S. Ermon. Flow-GAN: Bridging imPlicit and Prescribed learning in
generative models. Technical RePort arXiv:1705.08868, 2017.
I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. Courville. ImProved training of Wasser-
stein GAN. Technical RePort arXiv:1704.00028, March 2017.
T. Haarnoja, H. Tang, P. Abbeel, and S. Levine. Reinforcement learning with deeP energy-based
Policies. In ICML, 2017.
G. E. Hinton. Training Products of exPerts by minimizing contrastive divergence. Neural Computation,
14(8):1771-1800, 2002.
F. Huszdr. Variational inference using implicit distributions. Technical Report arXiv:1702.08235,
2017.
R. Jordan, D. Kinderlehrer, and F. Otto. The variational formulation of the Fokker-Planck equation.
SIAM J. MATH. ANAL., 29(1):1-17, 1998.
12
Under review as a conference paper at ICLR 2018
D. Kingma, T. P. Salimans, and M. Welling. Improving variational inference with inverse autoregres-
sive flow. In NIPS, 2016.
D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In ICLR, 2014.
Y. Li, R. E. Turner, and Q. Liu. Approximate inference with amortised MCMC. Technical Report
arXiv:1702.08343, 2017.
Q. Liu and D. Wang. Stein variational gradient descent: A general purpose Bayesian inference
algorithm. In NIPS, 2016.
J. C. Mattingly, A. M. Stuart, and M. V. Tretyakov. Construction of numerical time-average and
stationary measures via Poisson equations. SIAM Journal on Numerical Analysis, 48(2):552-577,
2010.
S. Mohamed and B. Lakshminarayanan. Learning in implicit generative models. Technical Report
arXiv:1610.03483, 2017.
F. Otto. Dynamics of Labyrinthine pattern formation in magnetic fluids: A mean-field theory. Arch.
Rational Mech. Anal., pp. 63-103, 1998.
Y. Pu, Z. Gan, R. Henao, C. Li, S. Han, and L. Carin. Stein variational autoencoder. Technical Report
arXiv:1704.05155, 2017a.
Y. Pu, Z. Gan, R. Henao, C. Li, S. Han, and L. Carin. Vae learning via stein variational gradient
descent. In NIPS, 2017b.
A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional
generative adversarial networks. Technical Report arXiv:1511.06434, January 2016.
R. Ranganath, J. Altosaar, D. Tran, and D. M. Blei. Operator variational inference. In NIPS, 2016.
D. J. Rezende and S. Mohamed. Variational inference with normalizing flows. In ICML, 2015.
D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference
in deep generative models. In ICML, 2014.
H. Risken. The Fokker-Planck equation. Springer-Verlag, New York, 1989.
T. Salimans, D. P. Kingma, and M. Welling. Markov chain Monte Carlo and variational inference:
Bridging the gap. In ICML, 2015.
T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved techniques
for training GANs. Technical Report arXiv:1606.03498, June 2016.
Y. W. Teh, A. H. Thiery, and S. J. Vollmer. Consistency and fluctuations for stochastic gradient
Langevin dynamics. JMLR, 17(1):193-225, 2016.
J. M. Tomczak and M. Welling. Improving variational auto-encoders using Householder flow.
Technical Report arXiv:1611.09630, November 2016.
D. Tran, R. Ranganath, and D. M. Blei. The variational gaussian process. In ICLR, 2016.
A. van den Oord, N. Kalchbrenner, and K. Kavukcuoglu. Pixel recurrent neural networks. In ICML,
2016.
S. J. Vollmer, K. C. Zygalakis, and Y. W. Teh. (exploration of the (Non-)asymptotic bias and variance
of stochastic gradient Langevin dynamics. JMLR, 1:1-48, 2016.
D. Wang and Q. Liu. Learning to draw samples: With application to amortized MLE for generative
adversarial learning. In ICLR workshop, 2017.
M. Welling and Y. W. Teh. Bayesian learning via stochastic gradient Langevin dynamics. In ICML,
2011.
J. Zhao, M. Mathieu, and Y. LeCun. Energy-based generative adversarial networks. In ICLR, 2017.
13
Under review as a conference paper at ICLR 2018
Supplementary Material for:
Continuous-Time Flows for Efficient
Inference and Density Estimation
A	Assumptions of Theorem 2
First, let us define the infinitesimal generator of the diffusion (2). Formally, the generator L of the
diffusion (2) is defined for any compactly supported twice differentiable function f : RL → R, such
that,
Lf(Zt)，h→n+ EfNt+? - f(Zt) = (F(Zt) W + 1 (G(Zt)G(Zt)T): VVT) f (Zt),
where a ∙ b，aτ b, A: B，tr( AT B), h → 0+ means h approaches zero along the positive real
axis.
Given an ergodic diffusion (2) with an invariant measure ρ(Z), the posterior average is defined
as: ψ，/ ψ(Z)ρ(Z)d Z for some test function ψ(Z) of interest. For a given numerical method
with generated samples (Zk)K=ι, we use the sample average ψ defined as ψκ = K PK=I ψ(zk) to
_______________ ʃ—"
approximate ψ. We define a functional ψ that solves the following Poisson Equation:
ʃ—" _______________
Lψ(zk) = ψ(zk) - ψ	(12)
∙-v
We make the following assumptions on ψ.
Assumption 1. ψ exists, and its up to 4rd-order derivatives, Dk ψ, are bounded by a function V,
i.e., kDkψ∣∣ ≤ Ck VPk for k = (0,1, 2, 3, 4), Ck ,pk > 0. Furthermore, the expectation of V on
{zk} is bounded: supl EVp(zk) < ∞, and V is smooth such that sups∈(0,1) Vp (s z + (1 - s) y) ≤
C (Vp (z) + Vp (y)), ∀z, y,p ≤ max{2pk} for some C > 0.
B	Proofs for Section 3
Sketch Proof of Lemma 1. First note that (5) in Lemma 1 corresponds to eq.13 in Jordan et al. (1998),
where F(p) in Jordan et al. (1998) is in the form of KL(ρkpθ(x, z)) in our setting.
Proposition 4.1 in Jordan et al. (1998) then proves that (5) has a unique solution. Theorem 5.1 in
Jordan et al. (1998) then guarantees that the solution of (5) approach the solution of the Fokker-Planck
equation in (3), which is ρT in the limit of h → 0.
Since this is true for each k (thus each t in ρt), we conclude that Pk = Phk in the limit of h → 0. □
To prove Theorem 2, we first need a convergence result about convergence to equilibrium in Wasser-
stein distance for Fokker-Planck equations, which is presented in Bolley et al. (2012). Putting in our
setting, we can get the following lemma based on Corollary 2.4 in Bolley et al. (2012).
Lemma 5 (Bolley et al. (2012)). Let PT be the solution of the FP equation (3) at time T, pθ (x, z) be
the joint posterior distribution given x. Assume that PT (z)pθ-1 (x, z)dz < ∞ and there exists a
constant C such that dW2 STdpe(Xz) ≥ CWJ (PT,p§(x, Z)). Then
W2 (PT, p(x, z)) ≤ W2 (P0, p(x, z)) e-CT .	(13)
We further need to borrow convergence results from Mattingly et al. (2010); Vollmer et al. (2016);
Chen et al. (2015) to characterize error bounds of a numerical integrator for the diffusion (2).
Specifically, the goal is to evaluate the posterior average of a test function ψ(z), defined as ψ，
14
Under review as a conference paper at ICLR 2018
ψ(z)pθ(x, z)d z. When using a numerical integrator to solve (2) to get samples {zk}kK=1, the
sample average ψκ，PK=I ψ(zk) is Used to approximate the posterior average. The accuracy
is characterized by the mean square error (MSE) defined as: E (ψκ - ψ) . Lemma 6 derives the
bound for the MSE.
Lemma 6 (Vollmer et al. (2016)). Under Assumption 1, and for a 1st-order numerical intergrator,
the MSE is bounded, for a constant C independent of h and K, by
E (ψκ - ψ) ≤ C (hK + h2).
Furthermore, except for the 2nd-order Wasserstein distance defined in Lemma 1, we define the
1st-order Wasserstein distance between two probability measures μι and μ2 as
Wi (μ1,μ2)
inf
P∈p(μ1,μ2) J
kx-yk2p(dx,dy) .
(14)
According to the Kantorovich-Rubinstein duality Arjovsky et al. (2017), W1(μ1,μ2) is equivalently
represented as
Wi (μ1,μ2) = sup Ez~μι [f (z)] - Ez~μ2 [f(z)] ,	(15)
f∈L1
where Li is the space of 1-Lipschitz functions f : RL → R.
We have the following relation between Wι(μι, μ2) and W2(μi,μ2).
Lemma 7 (Givens & Shortt (1984)). We havefor any two distributions μι andμ? that Wi(μi,μ2) ≤
W2(μi,μ2).
Now it is ready to prove Theorem 2.
Proof of Theorem 2. The idea is to simply decompose the MSE into two parts, with one part charac-
tering the MSE of the numerical method, the other part charactering the MSE of ρT and pθ(x, z),
which consequentially can be bounded using Lemma 5 above.
Specifically, we have
MSE(PT,ρτ; ψ)，E (Z ψ(Z)(pτ - PT)(Z)dZ)
=E( K Xψ(ZQ- Z ψ(z)ρτ(Z)d z!
=Ef KX X ψ(ZQ-
(=i)E
(2)
≤E
(3)
≤E
k=i
1K
κ X ψ(Zk)-
1K
κ X ψ(Zk)-
1K
K X MZk) -
ψ(z)pθ(x, z)dz
ψ (Z)ρT (Z)d Z -	ψ (Z)pθ(x, Z)d Z
Z ψ (Z)pθ(x, Z)d Z	+ (Z ψ (Z)ρT (Z)d Z -Zψ(Z)pθ(x,Z)dZ
ψ (Z)pθ(x, Z)d Z	+ Wi2(ρT,pθ)
2
ψ (z)pθ(x, z)d z	+ W22(ρT,pθ)
(4)
≤ Ci
=O
A
hK
+ h2 + e-2ChK
15
Under review as a conference paper at ICLR 2018
Where "(1)” follows by the fact that E (春 Pk=I ψ(zk) - R Ψ(z)pθ(x, z)dZ) = 0 Chen et al.
(2015); “(2)” follows by the definition of W1(μ1 ,μ2) in (14) and the I-LiPsChitz assumption of the
test function ψ; “(3)” follows by Lemma 7; “(4)” follows by Lemma 5 and Lemma 6.
□
C Connection to WGAN
We derive the upper bound of the maximum likelihood estimator, which connects MacGAN to
WGAN. Let pr be the data distribution, rewrite our maximum likelihood objective as
NN
max N X log Pθ (xi)=max N X (U (xi； θ) - log / eU(x')d X
The above maximum likelihood estimator can be bounded with Jensen’s inequality as:
1 SN	e eu(X⑻
max N 4 log Pθ (Xi) ≤ max Ex~Pr [U(X; θ)]-叼 qφ(xω)qφ(X; ω)d X
≤ maxEx〜pr [U(x; θ)] - Ex〜qφ(x3)
-	eU (x；θ)
log qφχω) 一
= maxEx〜pr [U(x； θ)] - Ex〜qφ(xM [U(x； θ)] - Ex〜qφ(xM [log Qφ(x； ω)].
(16)
This results in the same objective form as WGAN except that our model does not restrict U(x； θ) to
be I-Lipschitz functions and the objective has an extra constant term Ex〜q0(x”)[log qφ(x； ω)] w,r.t.
θ.
Now we prove Proposition 4.
Proof of Proposition 4. First it is clear that the equality in (16) is achieved if and only if
qφ(x∙, ω) = Pθ(x) Y eU(xa .
From the description in Section 4 and (16), we know that θ and φ share the same objective function,
which is an upper bound of the MLE in (16).
Furthermore, based on the property of continuous-time flows (or formally Theorem 2), we know that
qφ is learned such that qφ → pθ in the limit of h → 0 (or alternatively, we could achieve this by
using a decreasing-step-size sequence in a numerical method, as proved in Chen et al. (2015)). When
qφ = pθ, the equality in (16) is achieved, leading to the MLE.	□
D	Additional Experiments
D. 1 Calculating the testing ELBO for MacVAE
We follow the method in Pu et al. (2017a) for calculating the ELBO for a test data x* . First, after
distilling the CTF into the inference network qφ , we have that the ELBO can be represented as
logp(x*) ≥ Eqφ [logpθ(x*,z*)] - Eqφ [logqφ] .
The expectation is approximated with samples {z*j}M11 with z*j = fφ(x*, Zj), and Zj 〜qo(Z) the
standard isotropic normal. Here fφ represents the deep neural network in the inference network.
Note qφ(z*) is not readily obtained. To evaluate it, we use the density transformation formula:
qφ(z*) = qo(Z) Idetdfφ∂x*,ζ)∣τ.
D.2 Network architecture
The architecture of the generator of MacGAN is given in Table 1.
16
Under review as a conference paper at ICLR 2018
Table 1: Architecture of generator in MacGAN
Output Size Architecture
100 ×	10 Linear, BN, ReLU
512 ×	4 × 4 deconv, 256 5	×	5	kernels,	ReLU,	strike 2,	BN
256 ×	8 × 8 deconv, 128 5	×	5	kernels,	ReLU,	strike 2,	BN
128 ×	16 × 16 deconv, 3 5	×	5	kernels,	Tanh, strike 2
100 × 1
256 × 8 × 8
128× 16× 16
3 × 32 × 32

∖3345∖9 79 4
Ia3丁歹。^J∕⅛Q 7
Ii Λ ¾ √ S 6*7¾c Cr
J623∕d6 7r
/N3d-b67Vq
,23"夕Gmq
Ol^3q6to7wq
0/23夕GS 7?夕
b∕23¼q Q 7gq
0∕r3∕Γ∖6
O/2 3斗 VlE 7Fq
Cj/23M5。r∙∕Q
c> O 4>0
//M
ZQ 2 2
333
4 4 y H
r S 55
，G Q 4
7 ¥ 7 ^7
8 84g
q q q q
O />3q54 7zq
。/2m√-677 4
Ol23M5G 7009
，— K ?454 7名q
o∕arcss4
P f λ ⅞ UJ 56 7幺7
0/25夕 5& Q?q
z>∕J3^6g 78亍
^，23“，6 7 8 q
o\2?q 56 7号，
C—23V5G 089
0/2-3，r，7g or
0'23 3r6 7 ʃ Z
0/2 5fc>Ogq
O,23,5Q 7gz
。//34r4 7,彳
0∕λmujγ6 7gq
O — 2 3√ 54 7 8，
6 12 3 q 56 7? q
4/23“567S 7
—Λ3,S6 7g9
Figure 7: Generated images for MNIST datasets with MacGAN (top) and SteinGAN (bottom).
D.3 Additional results
Additional experimental results are given in Figure 7 - 12.
D.4 Robustness of the discretization stepsize
To test the impact of the discretization stepsize h in (6), following SteinGAN Feng et al. (2017), we
test MacGAN on the MNIST dataset, where ee use a simple Gaussian-Bernoulli Restricted Boltzmann
Machines as the energy-based model. We adopt the annealed importance sampling method to evaluate
17
Under review as a conference paper at ICLR 2018
Figure 8: Generated images for CelebA datasets with MacGAN.
log-likelihoods Feng et al. (2017). Wevaryhin{6e-4,2.4e-3,3.6e-3,6e-3,1e-2,1.5e-2}.
The trend of log-likelihoods is plotted in Figure 13. We can see that log-likelihoods do not change a
lot within the chosen stepsize interval, demonstrating the robustness of h.
-750
-900
0
g-850
■g-800
O
£
Φ
X
0.005
0.01
0.015
h
Figure 13: Log-likelihoods vs discretization stepsize for MacGAN on MNIST.
18
Under review as a conference paper at ICLR 2018
Figure 9: Generated images for CIFAR-10 datasets with MacGAN.
Figure 10: Generated images for CelebA datasets with SteinGAN.
19
Under review as a conference paper at ICLR 2018
Figure 11: Generated images for CIFAR-10 datasets with SteinGAN.
20
Under review as a conference paper at ICLR 2018
Figure 12: Generated images with a random walk on the ω space for CelebA datasets with MacGAN,
ωt = ωt-1 + 0.02 × rand([-1, 1]).
21