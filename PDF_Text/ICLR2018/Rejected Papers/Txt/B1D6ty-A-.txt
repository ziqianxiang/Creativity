Under review as a conference paper at ICLR 2018
Training Autoencoders by Alternating Mini-
MIZATION
Anonymous authors
Paper under double-blind review
Ab stract
We present DANTE, a novel method for training neural networks, in particular
autoencoders, using the alternating minimization principle. DANTE provides a
distinct perspective in lieu of traditional gradient-based backpropagation techniques
commonly used to train deep networks. It utilizes an adaptation of quasi-convex
optimization techniques to cast autoencoder training as a bi-quasi-convex optimiza-
tion problem. We show that for autoencoder configurations with both differentiable
(e.g. sigmoid) and non-differentiable (e.g. ReLU) activation functions, we can
perform the alternations very effectively. DANTE effortlessly extends to networks
with multiple hidden layers and varying network configurations. In experiments on
standard datasets, autoencoders trained using the proposed method were found to
be very promising and competitive to traditional backpropagation techniques, both
in terms of quality of solution, as well as training speed.
1	Introduction
For much of the recent march of deep learning, gradient-based backpropagation methods, e.g.
Stochastic Gradient Descent (SGD) and its variants, have been the mainstay of practitioners. The use
of these methods, especially on vast amounts of data, has led to unprecedented progress in several
areas of artificial intelligence. On one hand, the intense focus on these techniques has led to an
intimate understanding of hardware requirements and code optimizations needed to execute these
routines on large datasets in a scalable manner. Today, myriad off-the-shelf and highly optimized
packages exist that can churn reasonably large datasets on GPU architectures with relatively mild
human involvement and little bootstrap effort.
However, this surge of success of backpropagation-based methods in recent years has somewhat
overshadowed the need to continue to look for options beyond backprogagation to train deep networks.
Despite several advancements in deep learning with respect to novel architectures such as encoder-
decoder networks and generative adversarial models, the reliance on backpropagation methods
remains. While reinforcement learning methods are becoming increasingly popular, their scope is
limited to a particular family of settings such as agent-based systems or reward-based learning. Recent
efforts have studied the limitations of SGD-based backpropagation, including parallelization of SGD-
based techniques that are inherently serial (Taylor et al. (2016)); vanishing gradients, especially for
certain activation functions (Hochreiter & Schmidhuber (1997)); convergence of stochastic techniques
to local optima (Anandkumar & Ge (2016)); and many more. For a well-referenced recent critique of
gradient-based methods, we point the reader to Taylor et al. (2016).
From another perspective, there has been marked progress in recent years in the area of non-convex
optimization (beyond deep learning), which has resulted in scalable methods such as iterated hard
thresholding (Blumensath & Davies (2009)) and alternating minimization (Jain et al. (2013)) as
methods of choice for solving large-scale sparse recovery, matrix completion, and tensor factorization
tasks. Several of these methods not only scale well to large problems, but also offer provably accurate
solutions. In this work, we investigate a non-backpropagation strategy to train neural networks,
leveraging recent advances in quasi-convex optimization. Our method is called DANTE (Deep
AlterNations for Training autoEncoders), and it offers an alternating minimization-based technique
for training neural networks - in particular, autoencoders.
DANTE is based on a simple but useful observation that the problem of training a single hidden-layer
autoencoder can be cast as a bi-quasiconvex optimization problem (described in Section 3.1). This
1
Under review as a conference paper at ICLR 2018
observation allows us to use an alternating optimization strategy to train the autoencoder, where each
step involves relatively simple quasi-convex problems. DANTE then uses efficient solvers for quasi-
convex problems including normalized gradient descent (Nesterov (1984)) and stochastic normalized
gradient descent (Hazan et al. (2015)) to train autoencoder networks. The key contributions of this
work are summarized below:
•	We show that viewing each layer of a neural network as applying an ensemble of generalized
linear transformations, allows the problem of training the network to be cast as a bi-quasi-
convex optimization problem (exact statement later).
•	We exploit this intuition by employing an alternating minimization strategy, DANTE, that
reduces the problem of training the layers to quasi-convex optimization problems.
•	We utilize the state-of-the-art Stochastic Normalized Gradient Descent (SNGD) technique
(Hazan et al. (2015)) for quasi-convex optimization to provide an efficient implementation of
DANTE for networks with sigmoidal activation functions. However, a limitation of SNGD
is its inability to handle non-differentiable link functions such as the ReLU.
•	To overcome this limitation, we introduce the generalized ReLU, a variant of the popular
ReLU activation function and show how SNGD may be applied with the generalized ReLU
function. This presents an augmentation in the state-of-the-art in quasi-convex optimization
and may be of independent interest. This allows DANTE to train AEs with both differentiable
and non-differentiable activation functions, including ReLUs and sigmoid.
•	We show that SNGD offers provably more rapid convergence with the generalized ReLU
function than it does even for the sigmoidal activation. This is corroborated in experiments
as well. A key advantage of our approach is that these theoretical results can be used to set
learning rates and batch sizes without finetuning/cross-validation.
•	We also show DANTE can be easily extended to train deep AEs with multiple hidden layers.
•	We empirically validate DANTE with both the generalized ReLU and sigmoid activations
and establish that DANTE provides competitive test errors, reconstructions and classification
performance (with the learned representations), when compared to an identical network
trained using standard mini-batch SGD-based backpropagation.
2	Related Work
Backpropagation-based techniques date back to the early days of neural network research (Rumelhart
et al. (1986); Chauvin & Rumelhart (1995)) but remain to this day, the most commonly used methods
for training a variety of neural networks including multi-layer perceptrons, convolutional neural
networks, autoencoders, recurrent networks and the like. Recent years have seen the development
of other methods, predominantly based on least-squares approaches, used to train neural networks.
Carreira-Perpinan and Wang (Carreira-Perpinan & Wang (2014)) proposed a least-squares based
method to train a neural network. In particular, they introduced the Method of Auxiliary Constraints
(MAC), and used quadratic penalties to enforce equality constraints. Patel et al. (Patel et al. (2015))
proposed an Expectation-Maximization (EM) approach derived from a hierarchical generative model
called the Deep Rendering Model (DRM), and also used least-squared parameter updates in each
of the EM steps. They showed that forward propagation in a convolutional neural network was
equivalent to the inference on their DRM. Unfortunately, neither of these methods has publicly
available implementations or published training results to compare against.
More recently, Taylor et al. proposed a method to train neural networks using the Alternating
Direction Method of Multipliers (ADMM) and Bregman iterations (Taylor et al. (2016)). The focus
of this method, however, was on scaling the training of neural networks to a distributed setting on
multiple cores across a computing cluster. Jaderberg also proposed the idea of ’synthetic gradients’ in
Jaderberg et al. (2016). While this approach is interesting, this work is more focused towards a more
efficient way to carry out gradient-based parameter updates in a neural network.
In our work, We focus on an entirely new approach to training neural networks - in particular,
autoencoders - using alternating optimization, quasi-convexity and SNGD, and show that this
approach shows promising results on the a range of datasets. Although alternating minimization
has found much appeal in areas such as matrix factorization (Jain et al. (2013)), to the best of our
knowledge, this is the first such effort in using alternating principles to train neural networks with
related performance guarantees.
2
Under review as a conference paper at ICLR 2018
3	DANTE: Deep AlterNations for Training autoEncoders
In this section, we will first set notation and establish the problem setting, then present details of the
DANTE method, including the SNGD algorithm. For sake of simplicity, we consider networks with
just a single hidden layer. We then offer some theoretical insight intro DANTE’s inner workings,
which also allow us to arrive at the generalized ReLU activation function, and finally describe how
DANTE can be extended to deep networks with multiple hidden layers.
3.1	Problem Formulation
Consider a neural network with L layers. Each layer l ∈ {1, 2, . . . , L} has nl nodes and is character-
ized by a linear operator Wl ∈ Rnl-1 ×nl and a non-linear activation function φl : Rnl → Rnl. The
activations generated by the layer l are denoted by al ∈ Rnl . We denote by a0, the input activations
and n0 to be the number of input activations i.e. a0 ∈ Rn0 . Each layer uses activations being fed
into it to compute its own activations as al = φlhWl, al-1i ∈ Rnl, where φh., .i denotes φ(h., .i)
for simplicity of notation. A multi-layer neural network is formed by nesting such layers to form a
composite function f given as follows:
f(W; X) = φLhWL,φL-l hWL-1, ∙∙∙ ,φlhWl, Xiii	(1)
where W = {Wl} is the collection of all the weights through the network, and x = a0 contains the
input activations for each training sample.
Given m data samples {(Xi, yi)}im=1 from some distribution D, the network is trained by tuning the
weights W to minimize a given loss function, J :
mwin E(x,y)〜D [J (f(W； x),y)]	⑵
Note that a multi-layer autoencoder is trained similarly, but with the loss function modified as below:
min Ex〜D[J(f (W; x), x)]	(3)
For purpose of simplicity and convenience, we first consider the case of a single-layer autoencoder,
represented as f(W; x) = φ2hW2, φ1 hW1, xii to describe our methodology. We describe in a later
section on how this idea can be extended to deep multi-layer autoencoders. (Note that our definition
of a single-layer autoencoder is equivalent to a two-layer neural network in a classification setting, by
nature of the autoencoder.)
A common loss function used to train autoencoders is the squared loss function which, in our
simplified setting, yields the following objective.
min Ex 〜D kf(w； χ) - χk2
W
(4)
Now denote
kf (W; x) - xk22 = kφ2hW2,φ1hW1,xii - xk22	(5)
An important observation here is that if we fix W1 , then Eqn (5) turns into a set of Generalized Linear
Model problems with φ2 as the activation function, i.e.
min Ex〜D∣∣φ2hW2, zi - x∣∣2
W2
where z = φ1 hW1, xi. We exploit this observation in this work. In particular, we leverage a recent
result by Hazan et al. (2015) that shows that GLMs with nice, differentiable link functions such as
sigmoid (or even a combination of sigmoids such as φw2 (∙)), satisfy a property the authors name
Strict Locally Quasi-Convexity (SLQC), which allows techniques such as SNGD to solve the GLM
problems effectively.
Similarly, fixing W2 turns the problem into yet another SLQC problem, this time with W1 as the
parameter (note that φw2〈•〉= φ2W2,φ1h))).
min Ex〜D∣∣φw2(Wi, Xi - x∣2.
W1
3
Under review as a conference paper at ICLR 2018
Algorithm 1: Stochastic Normalized Gradient Descent (SNGD)
Input :Number of iterations T, training data S = {(xi, yi)}im=1 ∈ Rd × R, learning rate η,
minibatch size b, Initialization parameters w0
1	for t = 1 to T do
2	Sample {(xi, yi)}b=ι 〜Uniform(S)	//Select a random mini-batch of training points
3	Let ft(W) = b Pb=ι(yi- φ(w, xii)2
4	Let gt = Vft(wt), and g(t)=氤
5	wt+ι = Wt — η ∙ gt
6	end
Output : Model given by WT
This is quite advantageous for us since it allows us to solve each sub-problem of the alternating
setup efficiently. In a subsequent section, we will show that GLMs with non-differentiable activation
一 in particular, a generalized Rectified Linear Unit (ReLU) 一 can also satisfy the SLQC property,
thus allowing us to extend the proposed alternating strategy, DANTE, to ReLU-based autoencoders
too. We note that while we have developed this idea to train autoencoders in this work (since our
approach relates closely to the greedy layer-wise training in autoencoders), DANTE can be used to
train standard multi-layer neural networks too (discussed in Section 5).
3.2 Methodology
We begin our presentation of the proposed method by briefly reviewing the Stochastic Normalized
Gradient Descent (SNGD) method, which is used to execute the inner steps of DANTE. We explain in
the next subsection, the rationale behind the choice of SNGD as the optimizer. We stress that although
DANTE does use stochastic gradient-style methods internally (such as the SNGD algorithm), the
overall strategy adopted by DANTE is not a descent-based strategy, rather an alternating-minimization
strategy.
Stochastic Normalized Gradient Descent (SNGD): Normalized Gradient Descent (NGD) is an
adaptation of traditional Gradient Descent where the updates in each iteration are purely based on
the direction of the gradients, while ignoring their magnitudes. This is achieved by normalizing
the gradients. SNGD is the stochastic version of NGD, where weight updates are performed using
individual (randomly chosen) training samples, instead of the complete set of samples. Mini-batch
SNGD generalizes this by applying updates to the parameters at the end of every mini-batch of
samples, as does mini-batch Stochastic Gradient Descent (SGD). In the remainder of this paper, we
refer to mini-batch SNGD as SNGD itself, as is common for SGD. Algorithm 1 describes the SNGD
methodology for a generic GLM problem.
DANTE: Given this background, Algorithm 2 outlines the proposed method, DANTE. Consider
the autoencoder problem below for a single hidden layer network:
min f(W1,W2) = Ex 〜D kΦ2W2,φ1W1, Xii- xk2
W
Upon fixing the parameters of the lower layer i.e. W1, it is easy to see that we are left with a set of
GLM problems:
min Ex〜DkΦ2hW2,zi — xk2,
where z = φ1 hW1, xi. DANTE solves this intermediate problem using SNGD steps by sampling
several mini-batches of data points and performing updates as dictated by Algorithm 1. Similarly,
fixing the parameters of the upper layer, i.e. W2, we are left with another set of problems:
mWn Ex〜DkΦw2 hW1, Xi — x∣∣2,
where φw2〈•〉= Φ2 hW2,Φ1〈•〉〉. This is once again solved by mini-batch SNGD, as before.
4
Under review as a conference paper at ICLR 2018
Algorithm 2: DANTE: Deep AlterNations for Training autoEncoders
Input : Stopping threshold G Number of iterations of alternating minimization TAM, Number of
iterations for SNGD TSNGD, initial values W10 , W20, learning rate η, minibatch size b
1	t := 1
2	while|f(W1t,W2t) -f(W1t-1,W2t-1)| ≥ ort<TAMdo
3	Wt — arg min Ex 〜D kΦ2hW, ΦιhWt-1, Xii- x∣∣2	//use SNGD
W
4	Wt — arg min Ex 〜D ||。2〈卬$,。1〈卬,Xii- x∣∣2	//use SNGD
W
5	t:=t+1
6	end
Output: W1t, W2t
3.3	Rationale
To describe the motivation for our alternating strategy in DANTE, we first define key terms and
results that are essential to our work. We present the notion of a locally quasi-convex function (as
introduced in Hazan et al. (2015)) and show that under certain realizability conditions, empirical
objective functions induced by Generalized Linear Models (GLMs) are locally quasi-convex. We
then introduce a new activation function, the generalized ReLU, and show that the GLM with the
generalized ReLU also satisfies this property. We cite a result that shows that SNGD converges to the
optimum solution provably for locally quasi-convex functions, and subsequently extend this result to
the newly introduced activation function. We also generalize the definition of locally quasi-convex to
functions on matrices, which allows us to relate these ideas to layers in neural networks.
Definition 3.1 (Local-Quasi-Convexity). Let X, z ∈ Rd , κ, > 0 and let f : Rd → R be a
differentiable function. Then f is said to be (, κ, z)-Strictly-Locally-Quasi-Convex (SLQC) in X, if
at least one of the following applies:
1.	f(X) - f(z) ≤
2.	INf(x)k > O, and∀y ∈ B (z, e∕κ), (Vf (x), y - x〉≤ 0
where B (z, /κ) refers to a ball centered atz with radius /κ.
We generalize this definition to functions on matrices in Appendix A.3.
Definition 3.2 (Idealized and Noisy Generalized Linear Model (GLM)). Given an (unknown) dis-
tribution D and an activation function φ : R → R, an idealized GLM is defined by the existence of a
w* ∈ Rd such that y% = φ(w*, Xii∀i ∈ [m] where w* is the global minimizer of the error function:
1m
err(w) = — X 3 - Φ(hw, Xii))2
m
i=1
Similarly, a noisy GLM is defined by the existence of a w* ∈ Rd such E(x,y)〜D [y| x] = φ(hw*, Xi)
which is the global minimizer of the error function:
err(w) = E(x,y)〜D Iy - φ(hw, Xii))2
Without any loss in generality, we use xi ∈ Bd, the unit d-dimensional ball.
(HaZan et al., 2015, Lemma 3.2) shows that if we draw m ≥ Ω (exp(2Jw k) log ±) samples of
{(xi, yi)}im=1 from a GLM with the sigmoid activation function, then with probability at least 1 - δ,
the empirical error function
m
f (w) = m X(yi - φhw, Xii)2
i=1
is (e, ekw* k2, w*)-SLQC in w. However, this result is restrictive, since its proof relies on properties
of the sigmoid function, which are not satisfied by other popular activation functions such as the
ReLU. We hence introduce a new generalized ReLU activation function to study the relevance of this
result in a broader setting (which has more use in practice).
5
Under review as a conference paper at ICLR 2018
Definition 3.3. (Generalized ReLU) The generalized ReLU function f : R → R, 0 < a < b,
a, b ∈ R is defined as:
f(x) =
ax
bx
x≤0
x>0
This function is differentiable at every point except 0. Note that this definition subsumes variants
of ReLU such as the leaky ReLU (Xu et al. (2015)). We define the function g that provides a valid
subgradient for the generalized ReLU at all x to be:
g(x) =	ba
x<0
x≥0
While SLQC is originally defined for differentiable functions, we now show that with the above
definition of the subgradient, the GLM with the generalized ReLU is also SLQC. This allows us to
use the SNGD as an effective optimizer for DANTE to train autoencoders with different kinds of
activation functions.
Th	eorem 3.4. In the idealized GLMWith generalized ReLU activation, assuming ∣∣w*∣∣ ≤ W,
err(w) is G 2b：W, w*) — SLQC in W forall w ∈ Bd(0, W).
Proof. Consider ∣∣w∣∣ ≤ W such that errm(w)=煮 Pm=13 一 Φhw,Xii)2 ≥ G where m is the
total number of samples. Also let V bea point e∕κ-close to minima w* with K = 2b3W. Let g be the
subgradient of the generalized ReLU activation and G be the subgradient of errm (w). (Note that as
before, gh., .i denotes g(h., .i)). Then:
hG(w), w — vi
2m
=一 ɪ2ghw, Xii(Φhw, Xii - yi)hXi, (W — v)i
m
i=1
2m
=一 Eghw,Xii (Φhw, Xii — Φhw ,Xii) [hxi, W — W i + hxi, W - vi]
m
i=1
(Step 1)
2m
≥ —X ghw, Xii [b-1 (Φhw, Xii — Φhw*,Xii)2 +(Φhw, Xii — Φhw*,Xii)(Xi, W* — vi]
m i=1
(Step 2)
m
≥ —X g(w, Xii(b-1 (φ(w, Xii — φ(w*,Xii)2 — ∣φ(w, Xii — φ(w*,Xii川 Xi∣∣∣∣w*
m
i=1
— v||
2m
≥ — Eab 1 (φ(w,Xii-φ(w ,Xii) — b∣φ(w,Xii-φ(w *)川Xi∣∣∣∣w — v|| (Step 3)
m i=1
2m
≥ m£abT (φ(w, Xii — φ(w*, Xii)2 — b2∣∣(w, Xii — (w*,Xii∣∣ - ∣∣Xi∣∣	(Step 4)
i=1
≥ 2ab-1e - bW ||(w, X，—(w*,Xii 川肉||	(SteP 5)
≥ ab-1e(2- WW||w - w*||||Xi||2)
≥ 0	□
In the above proof, we first use the fact (in Step 1) that in the GLM, there is some w* such that
φhw*, Xii = yi. Then, we use the fact (in Steps 2 and 4) that the generalized ReLU function is
b-Lipschitz, and the fact that the minimum value of the quasigradient of g is a (Step 3). Subsequently,
in Step 5, we simply use the given bounds on the variables Xi , w, w* due to the setup of the problem
(w ∈ Bd(0, W), and Xi ∈ Bd, the unit d-dimensional ball, as defined earlier in this section).
We also prove a similar result for the Noisy GLM below.
6
Under review as a conference paper at ICLR 2018
Th	eorem 3.5. In the noisy GLM with generalized ReLU activation, assuming ∣∣w*∣∣ ≤ W, given
W ∈ B(0, W) ,then with probability ≥ 1 一 δ after m ≥ 七乡？ IogQMm samples, err(w) is
G 2baW, w*) — SLQC in W.
The proof for Theorem 3.5 is included in Appendix A.1.
We connect the above results with a result from Hazan et al. (2015) (stated below) which shows that
SNGD provably converges to the optimum for SLQC functions, and hence, with very high probability,
for empirical objective functions induced by noisy GLM instances too.
Theorem 3.6 (Hazan et al. (2015)). Let e,δ,G,M,κ > 0, let f : Rd → R and w* =
arg minw f (w). Assume that for b ≥ b0(, δ, T), with probability ≥ 1 — δ, ft defined in Algorithm
1 is (c, κ, w*)-SLQC ∀w, and |ft| ≤ M∀t ∈ {1,…，T} . Ifwe run SNGD with T ≥ K」w；-W ||
and η = K, and b ≥ max { M I(Og2 δ), bo(e, δ, T)}, with probability 1 — 2δ, f (w) — f (w*) ≤ 3e.
The results so far show that SNGD provides provable convergence for idealized and noisy GLM
problems with both sigmoid and ReLU family of activation functions. We note that alternate activation
functions such as tanh (which is simply a rescaled sigmoid) and leaky ReLU (Xu et al. (2015)) are
variants of the aforementioned functions.
In Algorithm 2, it is evident that each node of the output layer presents a GLM problem (and hence,
SLQC) w.r.t. the corresponding weights from W2 . We show in Appendices A.2 and A.3 how the
entire layer is SLQC w.r.t. W2, by generalizing the definition of SLQC to matrices. In case of W1,
while the problem may not directly represent a GLM, we show in Appendix A.3 that our generalized
definition of SLQC to functions on matrices allows us to prove that Step 4 of Algorithm 2 is also
SLQC w.r.t. W1.
Thus, given a single-layer autoencoder with either sigmoid or ReLU activation functions, DANTE
provides an effective alternating minimization strategy that uses SNGD to solve SLQC problems
in each alternating step, each of which converges to its respective -suboptimal solution with high
probability, as shown above in Theorem 3.6. Importantly, note that the convergence rate of SNGD
depends on the κ parameter. Whereas the GLM error function with sigmoid activation has κ = eW
Hazan et al. (2015), We obtain K = 2baW (i.e. linear in W) for the generalized ReLU setting, which
is an exponential improvement. This is significant as in Theorem 3.6, the number of iterations T
depends on κ2. This shows that SNGD offers accelerated convergence with generalized ReLU GLMs
(introduced in this work) when compared to sigmoid GLMs.
3.4	Extending to a Multi-Layer Autoencoder
In the previous sections, we illustrated how a single hidden-layer autoencoder can be cast as a set of
SLQC problems and proposed an alternating minimization method, DANTE. This approach can be
generalized to deep autoencoders by considering the greedy layer-wise approach to training a neural
network (Bengio et al. (2007)). In this approach, each pair of layers of a deep stacked autoencoder is
successively trained in order to obtain the final representation. Each pair of layers considered in this
paradigm is a single hidden-layer autoencoder, which can be cast as pairs of SLQC problems that can
be trained using DANTE. Therefore, training a deep autoencoder using greedy layer-wise approach
can be modeled as a series of SLQC problem pairs. Algorithm 3 summarizes the proposed approach
to use DANTE for a deep autoencoder, and Figure 1 illustrates the approach.
Note that it may be possible to use other schemes to use DANTE for multi-layer autoencoders such
as a round-robin scheme, where each layer is trained separately one after the other in the sequence in
which the layers appear in the network.
4	Experiments and Results
We validated DANTE by training autoencoders on an expanded 32 × 32 variant of the standard MNIST
dataset (LeCun et al. (1998)) as well as other datasets from the UCI repository. We also conducted
experiments with multi-layer autoencoders, as well as studied with varying number of hidden neurons
7
Under review as a conference paper at ICLR 2018
(a) Phase - 1	(b) Phase - 2
Figure 1: An illustration of the proposed multi-layer DANTE (best viewed in color). In each training
phase, the outer pairs of weights (shaded in gold) are treated as a single-layer autoencoder to be
trained using single-layer DANTE, followed by the inner single-layer auroencoder (shaded in black).
These two phases are followed by a finetuning process that may be empirically determined, similar to
standard deep autoencoder training.
Algorithm 3: DANTE for a multi-layer autoencoder
Input : Encoder e with weights U, Decoder d with weights V, Number of hidden layers 2n - 1,
Learning rate η, Stopping threshold , Number of iterations of alternating minimization
TAM, initial values U0 , V0, minibatch size b
1	t := 1
2	for l = 1 to n do
3	while|f(Ut,Vt) -f(Ut-1,Vt-1)| ≥ ort<TAMdo
4	//Use SNGD for minimizations
Ui — argmin Ex~D (d(e(x, Uti：l-1] ∙ U Pt-1Ln-G, VtLlT] . Vt-I-G- X)
2
5	vtn-l J arg min Eχ~D (d(e(x, Uti：l] ∙ utl+hn-1]), Vti：n-l-1] ∙ v ∙ Vt--I+1:nT)-X)
t :=t+1
6	end
7	end
OUtpUt : U, V
on single-layer autoencoders. Our experiments on MNIST used the standard benchmarking setup of
the dataset1, with 60, 000 data samples used for training and 10, 000 samples for testing. Experiments
were conducted using Torch 7 ( Collobert et al. (2011)).
AUtoencoder with Sigmoid Activation: A single-layer autoencoder (equivalent to a neural net-
work with one hidden layer) with a sigmoid activation was trained using DANTE as well as standard
backprop-SGD (represented as SGD in the results, for convenience) using the standard Mean-Squared
Error loss function. The experiments considered 600 hidden units, a learning rate of 0.001, and a
minibatch size of 500 (same setup was maintained for SGD and the SNGD used inside DANTE for
fair comparison; one could optimize both SGD and SNGD to improve the absolute result values.) We
studied the performance by varying the number of hidden neurons, and show those results later in
this section. The results are shown in Figure 2a. The figure shows that while DANTE takes slightly
(negligibly) longer to reach a local minimum, it obtains a better solution than SGD. (We note that the
time taken for the iterations were comparable across both DANTE and backprop-SGD.)
AUtoencoder with ReLU Activation: Similar to the above experiment, a single-layer autoencoder
with a leaky ReLU activation was trained using DANTE and backprop-SGD using the Mean-Squared
Error loss function. Once again, the experiments considered 600 units in the hidden layer of the
1http://yann.lecun.com/exdb/mnist/
8
Under review as a conference paper at ICLR 2018
(a) Single-layer autoencoder with Sigmoid activation
(b) Single-layer autoencoder with Generalized ReLU
activation
Figure 2: Plots of training and test errors vs training iterations for single hidden-layer autoencoder
with Sigmoid (left) and Generalized ReLU (right) activations for both DANTE and SGD.
Figure 3: Reconstructions using autoencoder
models with ReLU activation. Top: Original
Images; Middle:Model trained using DANTE;
Bottom: Model trained using Backprop-SGD.
	DANTE	SGD
MNIST	93.6%	92.44%
ionosphere	92.45%	96.22%
SVmgUide4	87.65%	70.37%
USPS	90.43%	89.49%
vehicle	77.02%	74.80%
Table 1: Classification accuracies using
ReLU autoencoder features on different
datasets
autoencoder, a leakiness parameter of 0.01 for the leaky ReLU, a learning rate of 0.001, and a
minibatch size of 500. The results are shown in Figure 2b. The results for ReLU showed an
improvement, and DANTE was marginally better than back-prop SGD across the iterations (as shown
in the figure).
In Figure 3, we also show the reconstructions obtained by both trained models (DANTE and Backprop-
SGD) for the autoencoder with the Generalized ReLU activation. The model trained using DANTE
shows comparable performance as a model trained by SGD under the same settings, in this case. We
also conducted experiments to study the effectiveness of the feature representations learned using the
models trained using DANTE and SGD in the same setting. After training, we passed the dataset
through the autoencoder, extracted the hidden layer representations, and then trained a linear SVM.
The classification accuracy results using the hidden representations are given in Table 1. The table
clearly shows the competitive performance of DANTE on this task.
Experiments on other datasets: We also studied the performance of DANTE on other standard
datasets 2, viz. Ionosphere (34 dimensions, 351 datapoints), SVMGuide4 (10 dimensions, 300
datapoints), Vehicle (18 dimensions, 846 datapoints), and USPS (256 dimensions, 7291 datapoints).
2https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/
(a) ionosphere dataset
(b) svmguide4 dataset
(c) USPS dataset
(d) vehicle dataset
Figure 4: Comparison of DANTE vs Backprop-SGD on other datasets from the UCI repository. The
x-axis on all figures is the number of mini-batch iterations and y-axis denotes test error, which shows
the generalization performance. (Best viewed in color; DANTE = purple, SGD = green)
9
Under review as a conference paper at ICLR 2018
(a) 200 hidden neurons
(b) 300 hidden neurons
(c) 400 hidden neurons	(d) 600 hidden neurons
Figure 5: Plots of training and test error vs training iterations on a single-layer autoencoder with
generalized ReLU activation, with varying number of nodes in the hidden layer.
Figure 4 and Table 1 show the performance of the proposed method vs SGD on the abovementioned
datasets. It can be seen that DANTE once again demonstrates competitive performance across the
datasets, presenting its capability as a viable alternative for standard backprop-SGD.
Varying Number of Hidden Neurons: Given the decomposable nature of the proposed solution
to learning autoencoders, we also studied the effect of varying hyperparameters across the layers,
in particular, the number of hidden neurons in a single-layer autoencoder. The results of these
experiments are shown in Figure 5. The plots show that when the number of hidden neurons is low,
DANTE reaches its minumum value much sooner (considering this is a subgradient method, one can
always choose the best iterate over training) than SGD, although SGD finds a slightly better solution.
However, when the number of hidden neurons increases, DANTE starts getting consistently better.
This can be attributed to the fact that the subproblem is relatively more challenging for an alternating
optimization setting when the number of hidden neurons is lesser.
(a) Architecture: 1024->500->500->1024	(b) Architecture: 1024->750->500->750->1024
Figure 6: Plots of training error and test error vs training iterations for multi-layer autoencoders with
generalized (leaky) ReLU activations for both DANTE and SGD.
山£P」J_
Multi-Layer Autoencoder: We also studied the performance of the proposed multi-layer DANTE
method (Algorithm 3) for the MNIST dataset. Figure 6 shows the results obtained by stacking two
single-layer autoencoders, each with the generalized (leaky) ReLU activation (note that a two single-
layer autoencoder corresponds to 4 layers in the overall network, as mentioned in the architecture on
the figure). The figure shows promising performance for DANTE in this experiment. Note that Figure
6b shows two spikes: one when the training for the next pair of layers in the autoencoder begins, and
another when the end-to-end finetuning process is done. This is not present in Figure 6a, since the
500 → 500 layer in between is only randomly initialized, and is not trained using DANTE or SGD.
10
Under review as a conference paper at ICLR 2018
5 Conclusions and Future Work
In this work, we presented a novel methodology, Deep AlterNations for Training autoEncoders
(DANTE), to efficiently train autoencoders using alternating minimization, thus providing an effective
alternative to backpropagation. We formulated the task of training each layer of an autoencoder as
a Strictly Locally Quasi-Convex (SLQC) problem, and leveraged recent results to use Stochastic
Normalized Gradient Descent (SNGD) as an effective method to train each layer of the autoencoder.
While recent work was restricted to using sigmoidal activation functions, we introduced a new
generalized ReLU activation function, and showed that a GLM with this activation function also
satisfies the SLQC property, thus allowing us to expand the applicability of the proposed method to
autoencoders with both sigmoid and ReLU family of activation functions. In particular, we extended
the definitions of local quasi-convexity to use subgradients in order to prove that the GLM with
generalized ReLU activation is (g b3W, w*) - SLQC, which improves the convergence bound for
SLQC in the GLM with the generalized ReLU (as compared to a GLM with sigmoid). We also showed
how DANTE can be extended to train multi-layer autoencoders. We empirically validated DANTE
with both sigmoidal and ReLU activations on standard datasets as well as in a multi-layer setting, and
observed that it provides a competitive alternative to standard backprop-SGD, as evidenced in the
experimental results.
Future Work and Extensions. DANTE can not only be used to train autoencoders, but can be
extended to train standard multi-layer neural networks too. One could use DANTE to train a neural
network layer-wise in a round robin fashion, and then finetune end-to-end using backprop-SGD. In
case of autoencoders with tied weights, one could use DANTE to learn the weights of the required
layers, and then finetune end-to-end using a method such as SGD. Our future work will involve a
more careful study of the proposed method for deeper autoencoders, including the settings mentioned
above, as well as in studying performance bounds for the end-to-end alternating minimization strategy
for the proposed method.
References
Animashree Anandkumar and Rong Ge. Efficient Approaches for Escaping Higher Order Saddle
Points in Non-Convex Optimization. In 29th Conference on Learning Theory (COLT), 2016.
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise training of
deep networks. In Advances in neural information processing Systems, pp. 153-160, 2007.
Thomas Blumensath and Mike E. Davies. Iterative Hard Thresholding for Compressed Sensing.
Applied and Computational Harmonic Analysis, 27(3):265-274, 2009.
Miguel Carreira-Perpinan and Weiran Wang. Distributed Optimization of Deeply Nested Systems. In
17th International Conference on Artificial Intelligence and Statistics (AISTATS), pp. 10-19, 2014.
Yves Chauvin and David E Rumelhart. Backpropagation: Theory, Architectures, and Applications.
Psychology Press, 1995.
R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: A Matlab-like Environment for Machine
Learning. In NIPS 2011 Workshop on Algorithms, Systems, and Tools for Learning at Scale (Big
Learning), 2011.
Elad Hazan, Kfir Y. Levy, and Shai Shalev-Shwartz. Beyond Convexity: Stochastic Quasi-Convex
Optimization. In 29th Annual Conference on Neural Information Processing Systems (NIPS), pp.
1594-1602, 2015.
Sepp Hochreiter and Jurgen Schmidhuber. Long Short-term Memory. Neural Computation, 8(9):
1735-1780, 1997.
Max Jaderberg, Wojciech Marian Czarnecki, Simon Osindero, Oriol Vinyals, Alex Graves, and
Koray Kavukcuoglu. Decoupled neural interfaces using synthetic gradients. arXiv preprint
arXiv:1608.05343, 2016.
11
Under review as a conference paper at ICLR 2018
Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank Matrix Completion using Al-
ternating Minimization. In 45th Annual ACM Symposium on Theory of Computing (STOC),
2013.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324,1998.
Yurii E. Nesterov. Minimization methods for nonsmooth convex and quasiconvex functions. Matekon,
29:519-531, 1984.
Ankit B. Patel, Tan Nguyen, and Richard G. Baraniuk. A Probabilistic Theory of Deep Learning.
arXiv:1504.00641 [stat.ML], 2015.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning Internal Representation
by Back-propagating Errors. Nature, 323(9):533-536, 1986.
Gavin Taylor, Ryan Burmeister, Zheng Xu, Bharat Singh, Ankit Patel, and Tom Goldstein. Training
Neural Networks Without Gradients: A Scalable ADMM Approach. In 33rd International
Conference on Machine Learning (ICML), 2016.
Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical Evaluation of Rectified Activations in
Convolutional Network. arXiv:1505.00853v2 [cs.LG], 2015.
12
Under review as a conference paper at ICLR 2018
A	Appendices
A. 1 Noisy GLM with Generalized ReLU Activation Function
The theorem below is a continuation of the discussion in Section 3.3 (see Theorem 3.5). We prove
this result below.
Theorem A.1. In the noisy GLM with generaIized ReLU activation, assuming ||w* || ≤ W, given
W ∈ B(0, W), then with probability ≥ 1 一 δ after m ≥ a2(1-WWjzm log(1∕δ)∕e2 samples, err(w)
is G 2baW, w*) — SLQC in W.
Proof. Here, ∀i, yi ∈ [0, 1], the following holds:
y = Φ(w*, Xi + ξi	(6)
where {ξi}m=1 are zero mean, independent and bounded random variables, i.e. ∀i ∈ [m], ∣∣ξi || ≤ 1.
Then, errm(w) may be written as follows (expanding yi as in Eqn 6):
1m
errm(w)= 一 工3 — Φhw, Xi)i)2
m
i=1
m
=m (X(Φhw*, Xii — Φhw, Xii)2
i=1
mm
+ X 2ξi(Φhw*, Xii — Φhw, Xii) + X ξ2)
i=1	i=1
Therefore, we also have (by definition of noisy GLM in Defn 3.2):
1m
errm(w) — errm(w*) = 一 X(φ(w*, Xii — φ(w, Xii)2
m
i=1
m
+ m X 2ξi(φ(w*, Xii — φhw, Xii)
i=1
Consider ∣∣w∣∣ ≤ W such that errm(w) — errm(w*) ≥ e. Also, let V be a point e∕κ-close to minima
w* with K = 2baW. Let g be the subgradient of the generalized ReLU activation and G be the
subgradient of errm(w), as before. Then:
hG(w), w — vi
2m
=一 fghw, Xii (Φhw,Xii — yi)(Xi, (w — v)i
m
i=1
2m
=—g>w ghw, Xii (Φhw, Xii — Φhw , Xii — ξi)
m
i=1
[hXi,w — w*i + hXi,w* — vi]
2b1 m
≥ ----Eghw,Xii(Φhw*,Xii — Φhw,Xii)2
m
i=1
2m
——Eghw, Xiiξi(hw, Xii — hw*, Xii)
i=1
2m
+ — Eghw, Xii(Φhw, Xii — Φhw*, Xii — ξi)hw* — V, Xii)
m i=1
(Step 1)
(Step 2)
13
Under review as a conference paper at ICLR 2018
2b-1 m
≥ ----Eghw,Xii(Φhw*,Xii - Φhw,Xif
m
i=1
2 m	b2	1 m
—-Eghw,Xiiξi(hw,Xii - hw ,Xii) - 2—(||w - w*|| + — E∣ξi∣)
mi=1	κ	mi=1
2b-1 m
----Ea[(φ(w*,Xii - Φhw,Xii)2
m
i=1
-2ξi(Φhw,Xii - Φhw*,Xii)]
m
——X[ghw,Xii(ξi(hw,Xii - hw*,Xii))
m
i=1
b2	1 m
-2ab ξi(φhw, Xii - φhw , Xii)] - 2 — (||w - W*|| + m X |&D
i=1
b2	1 m	1 m
≥ 2ab E - 2------(IIW - w*|| +---^X lξiD+-------^X ξiλi(W)
κ	m i=1	m i=1
≥ 2ab-1E-ab-1W-1E(IIW-
mm
w*∣∣+m Xi” m χξiλi(W)
m i=1	m i=1
(Step 3)
(Step 4)
(Step 5)
(Step 6)
1m
≥ 2ab-1e - ab-1 e(1 + WT) +-^X ξiλi(w)	(Step 7)
m i=1
1m
≥ -ab-1eWT +-----^Xξiλi(w)	(Step 8)
i=1
Here, λi(w) =2g(w,Xii(hw, Xii - hw*, Xii) - 4ab-1(φhw, Xii - φ(w*, Xii),and
∣ξiλi(w)i ≤ 2b(ihw, Xii - hw*, XiiI +4ab-1 ∣φ(w, Xii- φ(w*, Xiii) ≤ 2b(3∣hw, Xi- 0*, Xiii) ≤
2b(6W) = 12bW
The above proof uses arguments similar to the proof for the idealized GLM (please see the lines after
the proof of Theorem 3.4, viz. the b-Lipschitzness of the generalized ReLU, and the problem setup).
Now, when
m
—X ξλi(w) ≥ ab-1WTE
m
i=1
our model is SLQC. By simply using the Hoeffding’s bound, we get that the theorem statement holds
for m ≥ 28a2：W4 log(1∕δ)∕E2.	□
A.2 Viewing the Outer Layer of an Autoencoder as a Set of GLMs
Given an (unknown) distribution D , let the layer be characterized by a linear operator W ∈ Rd×d0
and a non-linear activation function defined by φ : R → R. Let the layer output be defined by
φhW, Xi, where X ∈ Rd is the input, and φ is used element-wise in this function.
Consider the mean squared error loss, commonly used in autoencoders, given by:
min err(W) = min Ex〜D∣∣φhW, Xi- y∣∣2
d0
=min Ex〜Dk X ΦhW：,i, Xi- yi∣2
i=1
d0
=min XEx〜DkΦhW,i, Xi- yi∣2
i=1
d0
=Xmn Ex〜DkΦhW"i,χi - yik2
i=1
14
Under review as a conference paper at ICLR 2018
Each of these sub-problems above is a GLM, which can be solved effectively using SNGD as seen in
Theorem 3.6, which we leverage in this work.
A.3 Local Quasi-Convexity of the Autoencoder
In Algorithm 2, while it is evident that each of the problems in Step 3 is a GLM and hence, SLQC,
w.r.t. the corresponding parameters in W2, we show here that the complete layer in Step 3 is also
SLQC w.r.t. W2, as well as show that the problem in Step 4 is SLQC w.r.t. W1. We begin with the
definition of SLQC for matrices, which is defined using the Frobenius inner product.
Definition A.2 (Local-Quasi-Convexity for Matrices). Let x, z ∈ Rd×d0 , κ, > 0 and let f :
Rd×d0 → R be a differentiable function. Then f is said to be (, κ, z)-Strictly-Locally-Quasi-Convex
(SLQC) in x, if at least one of the following applies:
1.	f(x) - f(z) ≤
2.	∣Nf(x)k > 0, and∀y ∈ B (z,e∕κ), Tr(Vf (x)T(y - x)) ≤ 0
where B (z, /κ) refers to a ball centered at z with radius /κ.
We now prove that the err(W) of a multi-output single-layer neural network is indeed SLQC in W.
This corresponds to proving that the one-hidden layer autoencoder problem is SLQC in W2 . We then
go on to prove that a two layer single-output neural network is SLQC in the first layer W1, which
can be trivially extended using the basic idea seen in Theorem A.4 to show that the one hidden-layer
autoencoder problem is also SLQC in W1.
Theorem A.3. Let an idealized single-layer multi-output neural network be characterized by a linear
operator W ∈ Rd×d0 = [wι w2 •… Wd0]and a generalized ReLU activation function φ : R → R.
Let the output of the layer be φhW, xi where x ∈ Rd is the input, and φ is applied element-wise.
Assuming ||W* || ≤ C,err(W) is (e,答,W*) - SLQC in W for all W ∈ Bd(0, C).
Proof. Consider:
hG(W), W - ViF
d0
hG(wj),wj -vji
j=1
(By defn of Frobenius inner product)
m XX X (ΦhWj, Xij ) h d¾M , (Wj- Vj)iF
m i=1 j=1	wj
m d0
mɪɪg(Wj,Xi)(Ohwj,Xii -yij) Kxi,wj - wji + hχi,wj - VjiF]
m i=1 j=1
(Step 1)
(Step 2)
m d0
≥ m XX ghwj, xii [b-1 (φhwj, Xi)- φhwj, χii)2 +(φhwj, xii- φhw∫xii) hxi, w；- vji]
m i=1 j=1
(Step 3)
m d0
≥ m XX ghwj, xii [(b-1 (φhwj, xii- φhw5i,xii)2 τφhwj, xii- φhw∫,xi iιιιxiιιιιw∫- vj ||]
m d0
≥ m XX [ab-1 (φhwj, xii- φhwj, xii)2 -biφhwj, xii- φhw∫,xiiιιιxiιιιiw∫- vj||]
m i=1 j=1
(Step 4)
m d0
≥ —XX [ab-1 (Φhwj, xii - φhWj, xii)2 -b2∣∣hwj, xii - hw∫,xii∣∣-^∣∣xi∣∣]	(SteP 5)
m i=1 j=1	κ
15
Under review as a conference paper at ICLR 2018
≥ 2abTd0E----^llhw, Xii - hw*,xiillllxil1
bC
≥ abτd42 — k||w - w* mm XiIF)
C
≥0
(Step 6)
□
The remainder of the proof proceeds precisely as in Theorem 3.4.
Theorem A.4. Let an idealized two-layer neural network be characterized by a linear operator
w1 ∈ Rd×d0 , w2 ∈ Rd0 and generalized ReLU activation functions φ1 : Rd0 → Rd0 , φ2 : R → R
with a setting similar to Equation 5. Assuming ||w：∣∣ ≤ Wι, ||wg∣∣ ≤ W2, err(wι,w2) is
(3 2b W2 WI, wl) — SLQC in wι forall wι ∈ Bd(0, Wι).
Proof. Let ∣∣f (wi； w2; x) — x∣∣2 = kΦ2hw2, Φι(wι, Xii — x∣∣2 and。F be the Frobenius inner
product.
hG(w1), w1 — v1iF
=—X (Φ2hw2,Φ1hw1,Xiii - yi) hd(φ2hw2,,iφlhw1,Xiii), (wi - vi)〉F	(Step 1)
m	∂w1
i=1
Using chain rule, We can simplify d(φ2hw2∂W1hwι Kii) as
h ∂(φ2hw2,φ1hw1, Xii) i T = ∂(φ2 hw2,φ1hw1, Xii)	h ∂hw2,φ1hw1, Xii T
∂	∂wι	_!	∂(W2, φι hwι, Xii	〔	∂φ1hw1, Xi
∂φ1hw1, Xii tiT h∂hwι, Xi i T
∂(Wl, Xii	]	[ ∂wι	J
g2(wi, W2,x) ∙ gι(wi,x) ∙ W2 ∙ XT	(Let)
Continuing from Step 1:
2m
=- J^g2(wi, W2, Xi)(Φ2hW2,ΦlhWl, Xiii - Iyi)MiwTgl(wi, Xi)T, (wi - Vi)iF
m
i=i
2m
=一 fg2(wi, W2, Xi ) (Φ2hw2,Φlhwi, Xiii - yi) [hXiWT gl(wi, Xi)T, (wi - w； )〉F
m i=i	2	i (Step 2)
+ hXiw2Tgi(wi,Xi)T, (wi； - vi)iF]
2m
=一 y^g2(wi, W2, Xi)(Φ2hw2,Φlhwi, Xiii - yi) [Tr(gl(wi, Xi)W2X(wi - w；))
m
i=i
+ T r(gi(wi, Xi)w2XiT (wi； - vi)]
2m
=- V^g2(wi, W2, Xi)(Φ2hw2,Φlhwi, Xiii - yi) [Tr(gl(wi, Xi)W2Xwi)
m
i=i
- T r(gi(wi；, Xi)w2XiT wi；) + T r((gi(wi；, Xi) - gi(wi,Xi))w2XiTwi；)
+ T r(gi(wi, Xi)w2XiT (wi； - vi))]
(Step 3)
In order to convert the above terms into a more familiar form, We begin With the folloWing observation:
hw2, φihw, XiiF = T r(gi(w, X)w2XT (w))
Also, note that gi (w, X) is a diagonal d0 × d0 matrix consisting of a’s and b’s on the diagonal:
(hw2, φihwi,Xii - hw2, φihw, Xii) = T r(gi(wi, X)w2XT wi) - T r(gi(w, X)w2XT w)
16
Under review as a conference paper at ICLR 2018
Therefore, on setting W = wɪ and using the fact that the generalized ReLU is b-Lipschitz and
monotonically increasing, we have:
(Φ2hw2,φ1hw1,Xii - Φ2hw2,φιh W；, xii)2 ≤ b(φ2hw2,φ1hw1,Xii - Φ2hw2,φ1h w；, Xii)
(hw2,φlhwi, xii - hw2,φlhw;, Xii)
= b(φ2hw2, φ1hw1, Xii - φ2hw2,φ1h W1；,Xii)
• (Tr(gι(wι, x)w2xτWι) - Tr(gι(w,x)w2xτw；))
Plugging this result into Step 3:
2m
≥ 一 Y'g2(wi, W2, xi)[b-l(Φ2hw2,Φlhwl, Xiii- Φ2hw2,Φlh w1, Xiii)2
m
i=1
+ (φ2hw2, φ1hw1,Xiii - yi) • T r((g1(w1；, xi) - g1(w1,xi))w2xiTw1；)
+ (φ2hw2, φ1hw1,Xiii - yi) • T r(g1(w1, xi)w2xiT (w；1 - v1))]
2m
≥ 2ab-1e +——£g2(Wi, W2, xi) (Φ2hw2,Φ1 hwι, Xiii - yi)
i=1
• [Tr(g1(w1；,xi)w2xiTw1；) - T r(g1(w1, xi)w2xiT v1)]
≥
≥
2a 2 m
-be — - £g2(wi, w2,Xi) • I (Φ2hw2,Φ1hw1,Xiii - yi)川w2∣ll∣xTIl
m i=1
IbIIw1；II+ bIIv1II
2a 2 m
^b^e - m ^X b • | (φ2 hw2, φ1hw1, Xiii - yi) lW2 • 1 • [bllwl || + b||v1||]
i=1
2a	2 m
≥ Ke----EbW2 • I (Φ2hw2,φ1hw1,Xiii - yi) I • [b∣∣Wι∣∣ + b∣∣vι∣∣]
bm
i=1
2a	2 m
≥ -be - - b2Wb2W2 • ∣ (Φ2hw2,φ1hw1, Xiii - yi) ∣ • ∣∣Wι - Vill
m i=1
2a	2 m
=ɪe - m^b2W2 • IΦ2hw2,Φ1hw1, Xiii - Φ2hw2,Φ1hw1, Xiiil • I∣W1 - Vill
i=1
≥ -ae - 2b4W2 ∙∣∣Wi - w；ll，-
bκ
=la- - 2b4W2 2bsW2W1 llw1 - WM = be(2 - W llw1 - w；ll) ≥ 0
a
(Step 4)
(Step 5)
(Step 6)
(Step 7)
(Step 8)
(Step 9)
(Step 10)
(Step 11)
(Step 12)
We arrive at Step 6 by using a similar observation as used in Step 4. The remainder of the proof is
similar to that of Theorem 3.4.
□
17