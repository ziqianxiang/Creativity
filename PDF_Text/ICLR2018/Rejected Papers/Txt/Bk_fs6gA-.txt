Under review as a conference paper at ICLR 2018
Long Term Memory Network for Combinato-
rial Optimization Problems
Anonymous authors
Paper under double-blind review
Ab stract
This paper introduces a framework for solving combinatorial optimization prob-
lems by learning from input-output examples of optimization problems. We in-
troduce a new memory augmented neural model in which the memory is not re-
settable (i.e the information stored in the memory after processing an input exam-
ple is kept for the next seen examples). We used deep reinforcement learning to
train a memory controller agent to store useful memories. Our model was able to
outperform hand-crafted solver on Binary Linear Programming (Binary LP). The
proposed model is tested on different Binary LP instances with large number of
variables (up to 1000 variables) and constrains (up to 700 constrains).
1	Introduction
An intelligent agent with a long-term memory processes raw data (as images, speech and natural lan-
guage sentences) and then transfer these data streams into knowledge. The knowledge stored in the
long-term memory can be used later in inference either by retrieving segments of memory during re-
calling, matching stored concepts with new raw data (e.g. image classification tasks) or solving more
complex mathematical problems that require memorizing either the method of solving a problem or
simple steps during solving. For example, the addition of long-digit numbers requires memorizing
both the addition algorithm and the carries produced from the addition operations (Zaremba et al.,
2016).
In neural models, the weights connecting the layers are considered long term memories encoding
the algorithm that transform inputs to outputs. Other neural models as recurrent neural networks
(RNNs) introduce a short-term memory encoded as hidden states of previous inputs (Lipton et al.,
2015; Hochreiter & Schmidhuber, 1997).
In memory augmented neural networks (MANNs), a controller writes memories projected from its
hidden state to a memory bank (usually in the form of a matrix), the controller then reads from the
memory using some addressing mechanisms and generates a read vector which will be fed to the
controller in the next time step (Graves et al., 2014). The memory will contain information about
each of the input sequence tokens and the controller enriches its memory capacity by using the read
vector form the previous time step.
Unfortunately, In MANNs the memory is not a long-term memory and is re-settable when new
examples are processed, making it unable to capture general knowledge about the inputs domain.
In context of natural language processing, one will need general knowledge to answer open-ended
questions that do not rely on temporal information only but also on general knowledge from previous
input streams. In long-digits multiplication, it will be easier to store some intermediate multiplica-
tion steps as digit by digit multiplications and use them later when solving other instances than doing
the entire multiplication digit by digit each time from scratch.
Neural networks have a large capacity of memorizing, a long-term persistent memory will even
increase the network capacity to memorize but will decrease the need for learning coarse features of
the inputs that requires more depth.
Storing features of the inputs will create shortcut paths for the network to learn the correct targets.
Such a network will no longer need to depend on depth to learn good features of the inputs but
instead will depend on stored memory features. In other words a long-term memory can provide
1
Under review as a conference paper at ICLR 2018
intermediate answers to the network. Unlike regular MANNs and RNNs, a long-term memory can
provide shortcut connections to both inputs features and previous time steps inputs.
Consider when the memory contains the output of previous examples, the network would cheat from
the memory to provide answers. Training such a network will focus on two stages: (1) Learning to
find similarities between memory vectors and current input data, (2) learning to transform memory
vectors into meaningful representations for producing the final output.
The No Free Lunch Theorem of optimization (Wolpert & Macready, 1997) states that: any two
algorithms are equivalent when their performance is averaged across all possible problems, this
means that an algorithm that solve certain classes of problems efficiently will be incompetent in other
problems. In the setting of combinatorial optimization, there is no algorithm able to do better than a
random strategy in expectation. The only way an algorithm outperforms another is to be specialized
to a certain class of optimization problems (Andrychowicz et al., 2016). Learning optimization
algorithms from scratch using pairs of input-output examples is a way to outperform other algorithms
on certain classes. It is further interesting to investigate the ability of learned models to generate
better solutions than hand crafted solvers.
The focus of this paper is on designing neural models to solve Binary Linear Programming (or 0-
1 Integer Programming) which is a special case of Integer Linear Programming problems where
all decision variables are binary. The 0-1 integer programming is one of Krap’s 21 NP-complete
problems introduced in Karp (1972). The goal of Binary LP is to optimize a linear function under
certain constrains. It is proved by Cadoli (2001) that Binary LP expresses the complexity class NP
(i.e any problem in the complexity class NP can be modeled as Binary LP instance).
The standard form of a Binary LP problem is:
maximize cT x
subject to Ax ≤ b
x≥0
x ∈ {0, 1}
where c and b are vectors and A is a matrix.
We propose a general framework for long-term memory neural models that uses reinforcement learn-
ing to store memories from a neural network. A long-term memory is not resettable and may or may
not store hidden states from individual time steps. Instead a long term memory stores information
that is considered to be useful for solving similar instances. The controller that decides to write
memories follows a policy function that properly constructs the memory contents. We train and test
this framework on synthetic data set of Binary LP instances. We analyze the model capability of
generalization to more complex instances beyond the training data set.
2	Literature Review
A related line of work is learning to learn and meta-learning, Lake et al. (2016) argued that it is an
important building block in artificial intelligence. Younger et al. (2001) and Andrychowicz et al.
(2016) used recurrent neural networks to act as a gradient descent procedure to train other networks.
The application of neural networks to combinatorial optimization has a long distinguished history.
Most common approach is using Hopfield networks (Hopfield & Tank, 1985) for solving Travelling
Salesman Problem (TSP). Other neural approaches applied to Travelling Salesman Problem include
Elastic nets (Durbin, 1987) and Self-organized maps (Fort, 1988).
Pointer networks (Vinyals et al., 2015) an architecture similar to sequence-to-sequence neural net-
work (Sutskever et al., 2014) to solve combinatorial problems as TSP. Pointer networks solve class
of problems where the number of target classes in each step of the output depends on the input length
which is variable. Recently, Bello et al. (2016) introduced a pointer network for solving combinato-
rial problems which is optimized using policy gradient methods. This pointer network was applied
to TSP and Knapsack.
Our work is different from this recent approaches, we applied our model to a general mathematical
formulation of combinatorial optimization problems, instead of learning solutions from problem
2
Under review as a conference paper at ICLR 2018
dependent data. We introduce a memory neural network where the coupled memory is persistent
and is not resettable. The model is able to learn and store inputs features as neural memories and
utilizes them in solving Binary LP instances. The model can learn from minimal amount of data and
yet generate better solutions than handcrafted solver.
3	Long-Term Memory Network for B inary LP
Figure 1: The operation of an LTMN processing one Binary LP instance. First the encoder receives a
sequence of constrains coefficients and produces a memory vector via a linear projection of encoder
final hidden state. The memory controller receives this linear projection and decides whether to
store it in memory M or delete the current slot it points to. A control signal (sequence of costs) is
then passed to the decoder to produce un-normalized weights wt over the memory locations. A dot
product operation between wt and M Finally the output ot at each time step is generated using the
memory vector (or encoder final state), the decoder hidden state and the read vector.
A sequence-to-sequence approach to solve Binary LP requires formulating the entire problem inputs
(costs vector cT , bounds vector b and constrains matrix A) as one sequence and mapping it to an
output sequence. A naive linear programming solver constructs the set of feasible solutions from
the constrains matrix A, we denote this set as SF, then it iterates over SF using the cost function to
find the optimal solution. For a Binary LP instance, the number of possible feasible solutions is 2N
where N is the number of variables.
The model we describe works in in a similar way to the naive solver, an encoder encodes the entire
constrains matrix into one fixed vector va, this vector is a continuous representation of the set SF .
The vector va is then passed as the initial state of a decoder along with each cost ct that is used as
a control signal to read from memory. Figure 1 describes the operation of an LTMN for a single
Binary LP instance.
A general Long-Term Memory Network (LTMN) consists of three basic modules: a core network,
a memory bank in the form of a matrix and a memory controller. The core network has three main
functions: providing memory fragments that are useful information from input signals to be stored
in the memory, providing a reading mechanism to read from the memory, and finally producing an
output. The memory bank is a matrix R × C, were each row is addressed by a pointer PT R which
is a value in range [0, R - 1]. The pointer is provided to the memory controller to know the last
accessed memory slot. Clearing the memory contents is not allowed by the memory controller.
A memory controller is another module that uses reinforcement learning as a hard attention mech-
anism to store memory fragments from the core network as in Zaremba & Sutskever (2015). Upon
processing of each example, the controller basically chooses either to store or discard the memory
fragments. Writing to the memory in LTMN is totally a discrete process, the final computational
3
Under review as a conference paper at ICLR 2018
graph will not be fully differentiable. Thus it is essential to pass the memories to the output layer
for backpropagation to work.
The final output depends on both memory contents representation (a read vector) and inputs em-
bedding, this is similar to end-to-end memory networks (Sukhbaatar et al., 2015) that is used in
question answering tasks. In end-to-end memory networks the final answer depends on both the
output memory representation and question embedding.
For non-sequential data the core network can be a regular feed forward neural network and the
memories can be the hidden vectors from each layer, while for sequential data the core network can
be a recurrent neural network and memories can be its hidden states at each time step. The attention
mechanism (Bahdanau et al., 2014) used over to attend to memory locations will depend either on a
control signal or previous output.
3.1	Encoder
Let the sequence A = { a11, a12, ..., a1N, SEP ARAT OR, bi, SEP ARAT OR, ..., aMN} be the
sequence describing the constrains set, aij represents the coefficient of the jth variable in the ith
constrain. The SEP ARAT OR token separates between the left and right hand sides of the con-
strains inequality, the same token is used to separate between constrains in the sequence. Finally
bi is the bound for the ith constrain. The encoder module encodes the entire sequence A in to a
fixed vector. The encoder then produces a linear projection of this vector to be stored in memory.
Equations (1) and (2) describe the constrains encoder operation.
va = C(at, Θc)	(1)
The linear projection of va is:
hfa = Whvva + bh	(2)
where Whv and bh are weight matrix and bias vector respectively.
3.2	Decoder
The input to the decoder is a sequence of costs C = { c1, ...cN} . The decoder uses the fixed vector
va as initial hidden state. At each time step the decoder reads a cost ct of the costs vector C and
produces a hidden state vector:
htd = D(ct,Θd)	(3)
The reader uses the decoder hidden state htd to produce weights that are used to address the memory:
wt = R(htd; Θr)	(4)
A simple dot product operation between the memory and the weight vector produces a read vector:
rt = wt∙ M ⑴	(5)
where M(i) is the memory produced by the memory controller after processing instance i in the data
set.
The linear projection hfa, the cost embedding wt and the read vector rt are used to generate a final
output:
ot = O(ha, wt, rt; Θo)	(6)
The final output layer is a soft-max layer, and the whole model can be thought of estimating the
probability distribution P(ot|ha, wt, rt). Each cost is embedded via a linear layer first, then the
embedding is passed to recurrent layers. The reader is implemented as a recurrent layer, in this
sense the weights produced will depend on both the cost at time step t (the control signal) and the
weight from the previous time step wt-1.
The reader produces un-normalized weights over memory slots N in range [-1,1]. When the weight
for a slot i is 0 this means that the memory slot does not contribute at all to the read vector, when
it is 1 the whole memory slot is used. The interesting case is when the weight is -1 which means
the inverse of the current memory slot contents, when the weight is between between 0 and 1 the
4
Under review as a conference paper at ICLR 2018
information from the memory slot is preserved to a certain extent, instead a -1 weight transforms
the entire slot into a new representation. Hopefully, the reader weights learned by backpropgation
through the whole network will not only act as attention weights over memory slots but also trans-
form the memory contents in to useful representations.
3.3	Memory Controller
A typical memory controller is provided a memory fragment at a discrete time t and makes a decision
whether to store or discard this memory. Learning to store useful memories is descirbed as an
reinforcement learning (RL) framework. An agent senses the environment s, takes an action a from
an action space A using a policy function π and receives an immediate reward r. The total future
reward from point t is expressed by:
∞
Rt = X γlrt+l	(7)
l=0
where γ is the discount factor.
3.3.1	Environment and Actions
The environment is the memory bank at discrete time step t along with the current pointer PTRt
and a window of previous memories Wt that the controller has been subjected to. A window is
similar to a short term memory with limited size τ and the controller chooses to store one of the
contents of that memory. In all our experiments, the controller stores the last memory vector in the
window, in this sense the window gives insights to the controller about what memories have been
produced by the core network.
The basic actions a controller may perform are storing the current memory we refer to this as aSTR,
and not to store any thing at all we refer to this as aNO-OP . We include three more actions: delete
the current slot aDELETE, increment the pointer then store aSTRINC, and decrement the pointer
then store aST RDEC .
3.3.2	Reward function
An action taken by the controller is evaluated through a reward function. One can evaluate the
controller the same way we evaluate the core network, through common metrics as final loss or
accuracy, for each example the model solves correctly the controller gets a reward. In this way, the
rewards received will only account for how better the whole model gets on solving tasks. Instead we
want the controller to store useful memories that will be used in the next processed examples. The
reward function is a simple question: Did the current memory result in a better response than
empty memory ?. The reward function is:
1 if eval(A, C, Mt) = 1 & eval(A, C, MZero) = 0
0	otherwise
(8)
where MZero is the memory with zero entries, Mt is the memory produced by memory controller
and eval is an evaluation function for the core network. When the entries of a read vector are
all zeros the output will depend only on information coming from the control signal (costs ct in
case of Binary LP). The memory controller receives a reward only when the non-empty memory
results in solving an example correctly and empty memory results in incorrect response. An optimal
training procedure will make the core network depends only on non-empty memory to minimize a
loss function, while an optimal training procedure for the memory controller will result in useful
memories only.
4	Memory Controller Training
At the heart of the memory controller is an RL agent which interacts with the environment. The en-
vironment is basically the memories received per each example the LTMN solves. For each instance
the LTMN processes, the RL agent receives a state st and selects an action at following a policy
π(at |st) and receives a reward rt and the next state st+1 (Sutton & Barto, 1998).
5
Under review as a conference paper at ICLR 2018
To train the memory controller agent, we used deep Q-learning algorithm with experience replay
(Mnih et al., 2013) as described in algorithm 1. A neural network is used to estimate the action-
value function Q(s, a) (Tsitsiklis & Van Roy, 1996; Sallans & Hinton, 2004; Riedmiller, 2005). We
choose to implement the action-value neural network as a stacked LSTM which receives the memory
contents Mt, one slot each time step, followed by window contents Wt and the pointer PT R.
We draw the similarity between using deep Q-learning algorithm for playing Atari games and using
it for storing long-term memories. During each episode the core network is given a sequence of
instances form a training dataset Xcontroller and the memory agent should take proper actions on
memory to construct useful memory entries that helps solving theses instances. One can consider
the agent is learning to construct the memory as constructing a puzzle from pieces, where the pieces
come from a bucket. There will be no final state for the agent to reach, because of the fact that
one can not know exactly how the memory should be constructed. In the setting of Atari games
the agent will reach the final state (game over) quickly in the first few epochs as the agent will still
be struggling to learn an optimal action-value function. In the case of the memory agent, we have
to determine an episode length T that both simulates the agent failure in the earlier training phase
and the agent reaching a final state. The final state of the agent will be reached when the LTMN
processes the last example in an episode. Thus, there will be an infinite number of final states. We
can keep the episode length constant so the agent have a limited time to succeed in accumulating
rewards.
Both the reader and the memory controller agent should be trained jointly together. This procedure
is critical since both the reader and the memory controller depend on each other, the reader learns
to generate proper weights over memory slots that depend on a control signal and the memory
controller learns how to construct memories that are useful for the reader. For each action the
memory agent perform on the memory, one should train the core network. We sample a batch of
instances from training data Xtrain , run the memory agent using this batch for one session (memory
is not cleared between instances) and perform the backward pass on the core network.
A typical learning algorithm for neural networks as stochastic gradient descent, updates the network
parameters using data batches. The number of iterations (one forward pass followed by one back-
ward pass) is equal to dN/me where N is the size of dataset and m is the batch size. The number
of iterations can get large as the size of the dataset increases.
To effectively train the LTMN core network, the episode length should be as the same as the number
of iterations. In our experiments, we keep the episode length T as small as possible and then increase
it after K epochs. To avoid similar episode lengths and simulate various solving sessions (where the
memory controller agent has to store memories), each episode T is chosen randomly between [k,
bN/mc], where k is the minimum length of an episode.
5	Experiments
5.1	Data Generation
We generate two separate data sets: one for training the core network and the other for training
the memory controller. We generate 14k Binary LP instances as the core network training set,
3k instances as the memory controller training set and 3k as a validation/test set. We use mixed
curriculum learning strategy as suggested in Zaremba et al. (2016). For each instance, we generate
two random numbers n and m, where n is the number of variables and m is the number of constrains
for the instance. The maximum number of variables in an instance is N and maximum number of
constrains is M. In our experiments, n is chosen uniformly between [3,N] and m is between [1,M].
For our training dataset N is 10 and M is 5.
All the coefficients of the objective function and the constrains matrix A is generated between [-
99,99]. To ensure that the constrains matrix is sparse we generate a random number SL called
sparsity level, for each constrain in the problem we ensure that at most 1/3 of the coefficients are
zeros. To generate supervised targets for the problems we used a python interface to the COIN-OR
branch and cut solver (Lougee-Heimer, 2003) called PuLP (Mitchell & Dunning, 2011). We ensure
that all the generated problems have a feasible solution. We use denote the COIN-OR branch and
cut solver as the baseline solver and compare the solver results with the LTMN results.
6
Under review as a conference paper at ICLR 2018
Algorithm 1 LTMN Training Algorithm
1	: Initialize replay memory D to capacity N
2	: Initialize the stacked LSTM Q with random weights
3	: for episode = 1,M do
4	:	Select randomly an episode length T
5	:	for t = 1,T do
6	:	Select randomly an example xi from Xcontroller
7	:	Solve xi using Encoder to get memory vector hai
8	:	Store hai in to window Wt
9	:	With probability select a random action at
10	otherwise select at = maXaQ*(st, a; θ)
11	:	Execute action at on memory Mt observe the next memory state Mt+1 and reward rt
12	:	Set st = Mt, Wt, PTRt
13	:	Set st+1 = Mt+1, Wt, PTRt+1
14	:	Store transition (st, at, rt, st+1) in D
15	:	Sample random minibatch of transitions (st, at, rt, st+1) from D
16	:	Perform a backward pass on Q
17	:	Sample random minibatch of training data xb from Xtrain
18	:	Perform a forward pass of Q on xb and Set Mxb
19	:	Perform a backward pass on LTMN using xb and Mxb
20	:	end for
21	: end for
5.2	Experimental details
We implement both the encoder and decoder as a recurrent neural network with gated recurrent
units (GRU). We use three GRU layers each with 256 units followed by a linear layer with 128
units to project the memory vector of the constrains sequence. As suggested by Bello et al. (2016),
it is harder to learn from input-output examples of optimization problems due to subtle features
that model cannot figure out. We suggest a technique to learn more features by connecting the
1, 2, ..., L - 1 layers to layer L instead of connecting only the previous layer L - 1. In this way an
encoder can learn more features using combined features from the previous layers.
The decoder is implemented as two stacked GRU layers, the reader is implemented as one GRU
layer. The costs are first embedded using a linear layer of 64 units. The rest of the decoder has 256
units. All the weights are initialized uniformly in [-0.08,0,08]. We set the dropout ratio to 0.2 in all
GRU layers. We constrain the norm of the gradients to be no greater than 5 (gradient clipping). We
set the learning rate initially to 0.001 and drop it by factor 0.5 after each 10 epochs.
The memory controller agent is implemented as a stacked LSTM of2 layers each with 200 units. We
used batch normalization over the inputs to normalize the memory contents and window contents to
zero-mean and unit variance. We used Adam (Kingma & Ba, 2014) optimizer to train the agent. The
window size τ is 5 in all our experiments. We used generalization loss as an early stopping criteria
for the core network as suggested by Prechelt (2012). We then allow the memory controller agent to
train until a reasonable average total reward per episode is reached.
We compare the performance of LTMN to the baseline solver and the sequence-to-sequence ap-
proach. We trained a network of GRU units with comparable number of parameters (seq-to-seq has
1.3M parameters and LTMN has 1.9M parameters). The seq-to-seq network receives the input as
one long sequence.
We train the LTMN core network and seq-to-seq model using RMSprop algorithm (Tieleman &
Hinton, 2012) with cross entropy as a loss function.
7
Under review as a conference paper at ICLR 2018
Table 1: Average costs at different sampling temperatures T
average costs of the solver = 24.76		
Sampling Tempera- ture T	Average Costs	No of Instances with a better solution
06	51345	-375
1.0	42.938	546
1.2	43.056	565
1.5	36.646	544
	22		29.417	503	
Table 2: Average costs for the untrained model
average costs of the solver = 24.76	
Sampling Tempera- ture T	Average Costs
10	1.421
1.2	3.841
1.5	0.517
	22		-2.574	
5.3	Results
We define a metric to test the quality of solutions produced by the model, we define average costs
over N Binary LP instances:
PN Cost(i)
Average Costs =	n~~—	(9)
All the instances in training data and testing data are maximization problems, so the higher the
average costs the better. We sample only feasible solutions using the output probability distribution
with temperatures [0.6, 1.0, 1.2, 1.5, 2.2].
First we evaluate the LTMN model against the baseline solver, we generate a test set of 1000 in-
stances where all instances have 10 variables and the number of constrains is between 1 and 5. Ta-
ble 1 shows at different sampling temperatures the average costs and the number of instances where
the model outperformed the solver and found a better solution. The LTMN model outperformed the
baseline solver by large margin (average costs are nearly 51.7% higher).
To validate that the sampling technique is effective, we used an initial untrained model to sample
solutions. Table 2 shows that the untrained model performed poorly on the same test set, and hence
the random sampling is not generating solutions by chance and the trained model learned to generate
solutions effectively.
We also evaluate the LTMN against the seq-to-seq GRU model which fails to generate any feasible
solutions. While the number of instances is small (only 14K instances), other similar models as
pointer networks (Vinyals et al., 2015) used a training data of 1M instances for Travelling Salesman
Problem, however our model did not require large training dataset.
We test the generalization ability of LTMN to longer sequences beyond those in training set. Figure 2
shows the results on test sets of Binary LP instances (each test set contains 1000 instances) where
the number of variables is incremented while the maximum number of constrains is 5. LTMN
outperformed the baseline solver by large margin even when the number of variables is larger than
100, the LTMN was still able to generate better solutions.
We test LTMN generalization on very large instances. Table 3 shows that LTMN outperformed the
baseline solver even for very large instances with 1000 variables and 700 constrains.
8
Under review as a conference paper at ICLR 2018

600
500
400
300
200
100
0
20	40	60	80	100	120	140	160
Variables
(a) Average costs of LTMN vs baseline solver
100
40
0
90
80
70
60
50
Sti0u.|①s① q q-M S ①uuetiu- Jo %
20	40	60	80	100	120	140	160
Variables
(b) Percentage of instances with better costs (higher than baseline solver)
Figure 2: Evaluation of LTMN on test sets of Binary LP instances with number of constrains in
range [1, 5] and number of variables up to 150
5.4 Memory Tests
To understand the effectiveness of using an augmented long-term memory network for solving Bi-
nary LP instances, we conduct tests to prove that long-term memory improves model results. We
calculate the average costs for the same test set (10 variables and constrains between 1 and 5) but
9
Under review as a conference paper at ICLR 2018
Table 3: Objective values generated by baseline solver and object values calculated using the sam-
pled variables assignments from LTMN output probability distribution at different sampling temper-
atures for large Binary LP instances.
			LTMN objective values at different T				
^N	M∕Γ~	Solver	T=0.6	T=1.0	T=1.2	T=1.5	T=2.2
100	~00~	^441-	187-	^399	^462-	^420-	2266-
300	^T00^	1026	1513	^67-	^942-	1107	^T85-
500	^700^	1175	1866	1334	2062	2263	1754
1000	~350~	870	4189	3694	4247	4341	3309
Table 4: Average costs (memory is reset between examples)
average costs of the solver = 24.76	
Sampling Tempera- ture T	Average Costs
06	46.896
1.0	42.526
1.2	39.727
1.5	35.621
	22		28.063	
we reset the memory each time a new example is processed. Table 4 shows that the average costs is
slightly dropped when the memory is reset between examples.
We conduct a per-example test where we identify whether memory helped in generating good so-
lutions or not. Given two problems identified by their cost vectors and constrains matrices as [c1,
A1] and [c2, A2]. Let A1 6= A2 but both of them construct the same set of feasible solutions SF .
The encoder produces two memory vectors hA1 and hA2 for constrains A1 and A2 respectively. To
ensure that both ehA1 and ehA2 represent the same set of feasible solutions SF, we measure the cosine
similarity between these two memory vectors such that CosSim(ehA1 ,ehA2) ≥ S, where S is set to
0.8. We then enforce the controller to store ehA1 , and generate a solution for [c2, A2]
We define three metrics:
Memory Faults: number of examples where non-empty memory results in worse solution than
empty memory.
Memory Trues: number of examples where non-empty memory results in better solution than
empty memory.
Memory Equals: number of examples where both non-empty memory and empty memory result in
the same solution.
We generate 10K instances (with 10 variables and constrains in range of [1,5]), we compare the
feasible solutions of each two consecutive instances and calculate the cosine similarity between the
two memory vectors. We found 181 instances with the same feasible solutions. We record the
solutions of these instances using both empty memory and non-empty memory containing hA1 .
Table 5 shows high memory trues where non-empty memory helped generating better solutions than
empty memory. The table also shows high percentage of memory equals, for these 28 instances the
non-empty memory did not help much in generating a better solution, in fact both memory faults
and memory equals are similar metrics and can be thought of as the number of instances where
non-empty memory failed to generate a better solution. A good memory controller agent should
maximize the memory trues and minimize both the memory faults and equals at the same time.
We conclude that a long-term memory is quite effective in generating better solutions and the mem-
ory controller learns effectively how to store input features useful for longer interaction with a model.
10
Under review as a conference paper at ICLR 2018
Table 5: Memory faults, memory trues and memory equals
Metric Value Percentage
Memory faults	66	≈36.4%
Memory trues	87	≈48.0%
Memory equals	28	≈15.4%
6 conclusion
This paper introduced a long term memory coupled with a neural network, that is able to memorize
useful input features to solve similar instances. We applied LTMN model to solve Binary LP in-
stances. The LTMN was able to learn from supervised targets provided by a handcrafted solver, and
generate better solutions than the solver. The LTMN model was able to generalize to more complex
instances beyond those in the training set.
References
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul,
and Nando de Freitas. Learning to learn by gradient descent by gradient descent. In Advances in
Neural Information Processing Systems,pp. 3981-3989, 2016.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
Irwan Bello, Hieu Pham, Quoc V Le, Mohammad Norouzi, and Samy Bengio. Neural combinatorial
optimization with reinforcement learning. arXiv preprint arXiv:1611.09940, 2016.
Marco Cadoli. The expressive power of binary linear programming. 2001.
Richard Durbin. An analogue approach to the travelling salesman. Nature, 326(16):689-691, 1987.
JC Fort. Solving a combinatorial problem via self-organizing process: An application of the kohonen
algorithm to the traveling salesman problem. Biological cybernetics, 59(1):33-40, 1988.
Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint
arXiv:1410.5401, 2014.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
John J Hopfield and David W Tank. neural computation of decisions in optimization problems.
Biological cybernetics, 52(3):141-152, 1985.
Richard M Karp. Reducibility among combinatorial problems. In Complexity of computer compu-
tations, pp. 85-103. Springer, 1972.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building
machines that learn and think like people. Behavioral and Brain Sciences, pp. 1-101, 2016.
Zachary C Lipton, John Berkowitz, and Charles Elkan. A critical review of recurrent neural networks
for sequence learning. arXiv preprint arXiv:1506.00019, 2015.
Robin Lougee-Heimer. The common optimization interface for operations research: Promoting
open-source software in the operations research community. IBM Journal of Research and De-
velopment, 47(1):57-66, 2003.
Stuart Mitchell and Iain Dunning. Pulp: a linear programming toolkit for python. 2011.
11
Under review as a conference paper at ICLR 2018
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.
LUtz Prechelt. Early StoPPingbUt when? In Neural Networks: Tricks of the Trade, pp. 53-67.
Springer, 2012.
Martin Riedmiller. NeUral fitted q iteration-first experiences with a data efficient neUral reinforce-
ment learning method. In ECML, volUme 3720, pp. 317-328. Springer, 2005.
Brian Sallans and Geoffrey E Hinton. Reinforcement learning with factored states and actions.
Journal of Machine Learning Research, 5(AUg):1063-1088, 2004.
Sainbayar SUkhbaatar, Jason Weston, Rob FergUs, et al. End-to-end memory networks. In Advances
in neural information processing systems, pp. 2440-2448, 2015.
Ilya SUtskever, Oriol Vinyals, and QUoc V Le. SeqUence to seqUence learning with neUral networks.
In Advances in neural information processing systems, pp. 3104-3112, 2014.
Richard S SUtton and Andrew G Barto. Reinforcement learning: An introduction, volUme 1. MIT
press Cambridge, 1998.
T. Tieleman and G. Hinton. LectUre 6.5—RmsProp: Divide the gradient by a rUnning average of its
recent magnitUde. COURSERA: NeUral Networks for Machine Learning, 2012.
JN Tsitsiklis and B Van Roy. An analysis of temporal-difference learning with fUnction approxima-
tiontechnical. Technical report, Report LIDS-P-2322). Laboratory for Information and Decision
Systems, MassachUsetts InstitUte of Technology, 1996.
Oriol Vinyals, Meire FortUnato, and Navdeep Jaitly. Pointer networks. In Advances in Neural
Information Processing Systems, pp. 2692-2700, 2015.
David H Wolpert and William G Macready. No free lUnch theorems for optimization. IEEE trans-
actions on evolutionary computation, 1(1):67-82, 1997.
A Steven YoUnger, Sepp Hochreiter, and Peter R Conwell. Meta-learning with backpropagation. In
Neural Networks, 2001. Proceedings. IJCNN’01. International Joint Conference on, volUme 3.
IEEE, 2001.
Wojciech Zaremba and Ilya SUtskever. Reinforcement learning neUral tUring machines-revised.
arXiv preprint arXiv:1505.00521, 2015.
Wojciech Zaremba, Tomas Mikolov, Armand JoUlin, and Rob FergUs. Learning simple algorithms
from examples. In International Conference on Machine Learning, pp. 421-429, 2016.
12