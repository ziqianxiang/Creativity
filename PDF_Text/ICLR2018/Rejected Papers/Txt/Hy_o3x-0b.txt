Under review as a conference paper at ICLR 2018
Feature Map Variational Auto-Encoders
Anonymous authors
Paper under double-blind review
Ab stract
There have been multiple attempts with variational auto-encoders (VAE) to learn
powerful global representations of complex data using a combination of latent
stochastic variables and an autoregressive model over the dimensions of the data.
However, for the most challenging natural image tasks the purely autoregres-
sive model with stochastic variables still outperform the combined stochastic-
autoregressive models. In this paper, we present simple additions to the VAE
framework that generalize to natural images by embedding spatial information
in the stochastic layers. We significantly improve the state-of-the-art results on
MNIST, OMNIGLOT, CIFAR10 and ImageNet when the feature map parameteri-
zation of the stochastic variables are combined with the autoregressive PixelCNN
approach. Interestingly, we also observe close to state-of-the-art results without
the autoregressive part. This opens the possibility for high quality image genera-
tion with only one forward-pass.
1	Introduction
In representation learning the goal is to learn a posterior latent distribution that explains the ob-
served data well (Bengio et al., 2013). Learning good representations from data can be used for
various tasks such as generative modelling and semi-supervised learning (Kingma, 2013; Rezende
et al., 2014; Kingma et al., 2014; Rasmus et al., 2015; Maal0e et al., 2016). The decomposition of
variational auto-encoders (VAE) (Kingma, 2013; Rezende et al., 2014) provides the potential to dis-
entangle the internal representation of the input data from local to global features through a hierarchy
of stochastic latent variables. This makes the VAE an obvious candidate for learning good represen-
tations. However, in order to make inference tractable VAEs contain simplifying assumptions. This
limits their ability to learn a good posterior latent representation.
In complex data distributions with temporal dependencies (e.g. text, images and audio), the VAE
assumption on conditional independence in the input distribution limits the ability to learn local
structures. This has a significant impact on its generative performance, and thereby also the learned
representations. Additionally, the one-layered VAE model with aN(0, I) latent prior poses serious
constraints on the posterior complexity that the model is able to learn. A deep hierarchy of stochastic
latent variables should endow the model with more expressiveness, but the VAE has a tendency to
skip the learning of the higher representations since they pose a direct cost in its optimization term.
There have been several attempts to eliminate the limitations of the VAE. Some concern formulating
a more expressive variational distribution (Burda et al., 2015b; Rezende & Mohamed, 2015; Tran
et al., 2016; Maal0e et al., 2016) where other concerns learning a deeper hierarchy of latent variables
(S0nderby et al., 2016). These contributions have resulted in better performance, but are still limited
when modelling complex data distributions where a conditional independence does not apply. When
parameterizing the VAE decoder with recurrent neural networks (Krishnan et al., 2015; Bowman
et al., 2015; Fraccaro et al., 2016), the decoding architecture gets too powerful which results in
unused latent stochastic variables (Chen et al., 2017).
The limitations of the VAE have spawned interest towards other generative models such as Gen-
erative Adversarial Networks (GAN) (Goodfellow et al., 2014) and the autoregressive Pixel-
CNN/PixelRNN models (van den Oord et al., 2016b). These methods have proven powerful in
learning good generative models, but the lack of stochastic latent variables makes them less suitable
for representation learning purposes (Chen et al., 2017). Lately, we have seen several successful
attempts to combine VAEs with PixelCNNs (Gulrajani et al., 2016; Chen et al., 2017). This results
1
Under review as a conference paper at ICLR 2018
Figure 1: A visualization of FAME where the solid lines denote the variational approximation (infer-
ence/encoder/recognition) network and dashed lines denote the generative model (decoder) network
for training. When performing reconstructions during training, the input image is concatenated with
the output of the generative model (blue) and when generating the model follows a normal autore-
gressive sampling flow (red) while also using the stochastic latent variables z = z1, ..., zL. Both the
variational approximation and the generative model follow a top-down hierarchical structure which
enables precision weighted stochastic variables in the variational approximation.
in a model where the global structure of the data is learned in the stochastic latent variables of the
VAE and the local structure is learned in the PixelCNN. However, despite the additional complexity
and potential extra expressiveness, these models do not outperform a simple autoregressive model
(van den Oord et al., 2016a; Salimans et al., 2017).
In this paper we present the Feature Map Variational Auto-Encoder (FAME) that combines the
top-down variational approximation presented in the Ladder Variational Auto-Encoder (LVAE)
(S0nderby et al., 2016) with a spatial (feature map) representation of the stochastic latent variables
and an autoregressive decoder. We show that (i) FAME outperforms previously state-of-the-art log-
likelihood on MNIST, OMNIGLOT, CIFAR10 and ImageNet, (ii) FAME learns a deep hierarchy of
stochastic latent variables without inactivated latent units, (iii) by removing the autoregressive de-
coder FAME performs close to previous state-of-the-art log-likelihood suggesting that it is possible
to get good quality generation with just one forward pass.
2	Feature Map Variational Auto-Encoder
The VAE (Rezende et al., 2014; Kingma, 2013) is a generative model with a hierarchy of stochastic
latent variables:
L-1
Pθ(x, Z) = Pθ(x∣zι)Pθ(zL) ɪɪ Pθ(zi%+ι) ,	(1)
i=1
where z = z1 , ..., zL, θ denotes the parameters, and L denotes the number of stochastic latent
variable layers. The stochastic latent variables are usually modelled as conditionally independent
Gaussian distributions with a diagonal covariance:
Pθ(zi∣Zi+ι) = N(Zi； μθ,i(Zi+1 ),diag(σ2,i(zi+ι))),	Pθ(zl) = N(zl；0,I).	(2)
Since the posterior p(z∣x) often is intractable We introduce a variational approximation qφ(z|x) with
parameters φ. In the original VAE formulation qφ(z|x) is decomposed as a bottom-up inference path
through the hierarchy of the stochastic layers:
L
qφ(z∣x) = qφ(zι∣x) ɪɪ qφ(zi∣Zi-i) ,	(3)
i=2
qφ(zι∣x) = N(zι; μφ,ι(x),diag(σφ,ι(x))) ,	(4)
qφ(zi∣Zi-ι) = N(Zi； μφ,i(zi-ι),diag(σφ,i(Zi-i))) .	(5)
2
Under review as a conference paper at ICLR 2018
We optimize an evidence lower-bound (ELBO) to the log-likelihood log pθ (x) = log zpθ(x, z)dz.
Burda et al. (2015a) introduced the importance weighted bound:
log p(x) ≥ Eqφ (z1 |x) , ..., Eqφ (zK |x)
1 g Pθ (x,zk)
og k=1 qφ(zklx)
≡ LK(θ,φ; x)
(6)
and proved that LK(θ,φ; x) ≥ LL (θ, φ; x) for K > L. For K = 1 the bound co-incides with the
standard ELBO: L(θ,φ; x) = L1(θ, φ; x). The hierarchical structure of both the variational approx-
imation and generative model give the VAE the expressiveness to learn different representations of
the data throughout its stochastic variables, going from local (e.g. edges in images) to global fea-
tures (e.g. class specific information). However, We can apply as recursive argument Maal0e et al.
(2017) to show that when optimizing with respect to the parameters θ and φ the VAE is regularized
towards qφ(zL∣zL-ι) = pθ (ZL) = N (zl ; 0, I). This is evident if we rewrite Equation 6 for K = 1:
L(θ,φ; X) = %φ(zll-i∣x)
Pθ(X,Z1:L-1|ZL)
.qφ(zi.L-1 |x).
- Eqφ(zLL-1 |x) [KL(qφ(ZLIzL-l)"pθ (ZL))].
KL(qφ(zL∣ZL-1 )∣∣pθ(zl)) = 0 is a local maxima and learning a useful representation in zl can
therefore be disregarded throughout the remainder of the training. The same argumentation can be
used for all subsequent layers Z2:L, hence the VAE has a tendency to collapse towards not using
the full hierarchy of latent variables. There are different ways to get around this tendency, where
the simplest is to down-weight the KL-divergence with a temperature term (Bowman et al., 2015;
S0nderby et al., 2016). This term is applied during the initial phase of optimization and thereby
downscales the regularizing effect. However, this only works for a limited number of hierarchically
stacked latent variables (S0nderby et al., 2016).
Formulating a deep hierarchical VAE is not the only cause of inactive latent variables, it also occurs
when the parameterization of the decoder gets too powerful (Krishnan et al., 2015; Fraccaro et al.,
2016; Chen et al., 2017). This can be caused by using autoregressive models such as p(x, z) =
Qj p(xj |x<j, z)p(z). Chen et al. (2017) circumvent this by introducing the Variational Lossy Auto-
Encoder (VLAE) where they define the architecture for the VAE and autoregressive model such that
they capture global and local structures. They also utilize the power of more expressive posterior
approximations using inverse autoregressive flows (Rezende & Mohamed, 2015; Kingma et al.,
2016). In the PixelVAE, Gulrajani et al. (2016) takes a similar approach to defining the generative
model but makes a simpler factorizing decomposition in the variational approximation qφ(z∣x)=
QL qφ (zi|x), where the terms have some degree of parameter sharing. This formulation results in a
less flexible model.
In Kingma et al. (2016); Gulrajani et al. (2016); Chen et al. (2017) we have seen that VAEs with
simple decompositions of the stochastic latent variables and a powerful autoregressive decoder can
result in good generative performance and representation learning. However, despite the additional
cost of learning a VAE we only see improvement in the log-likelihood over the PixelCNN for small
gray-scale image datasets (Salimans et al., 2017). We propose FAME that extends the VAE with
a top-down variational approximation similar to the LVAE (S0nderby et al., 2016) combined with
spatial stochastic latent layers and an autoregressive decoder, so that we ensure expressive latent
stochastic variables learned in a deep hierarchy (cf. Figure 1).
2.1	Top-Down Variational Approximation
The LVAE (S0nderby et al., 2016) does not change the generative model but changes the varia-
tional distribution to be top-down like the generative model. Furthermore the variational distribution
shares parameters with the generative model which can be viewed as a precision-weighted (inverse
variance) combination of information from the prior and data distribution. The variational approxi-
mation is defined as:
L-1
qφ(z∣x) = qφ(zL∣x) ɪɪ qφ(zi∣Zi+ι,x) .	(7)
i=1
The stochastic latent variables are all fully factorized Gaussian distributions and are therefore mod-
elled by qφ(zi∣Zi+ι,x) = N (z∕μi, diag(σ2)) for layers i = 1,...,L. Instead of letting q and P have
3
Under review as a conference paper at ICLR 2018
separate parameters (as in the VAE), the LVAE let the mean and variance be defined in terms of a
function of x (the bottom-up data part) and the generative model (the top-down prior):
μi
σi
μφ,i σ-,i + μθ,iσθ,i
丁一2 I 丁一2
σφ,i + σθ,i
1
一2	一2 ,
σφ,i + σθ,i
(8)
(9)
where μφ,i = μφ,i(x) and μθ,i = μθ,i(zi+ι) and like-wise for the variance functions. This precision
weighted parameterization has previously yielded excellent results for densely connected networks
(S0nderby et al., 2016).
2.2	Convolutional Stochastic Layers
We have seen multiple contributions (e.g. Gulrajani et al. (2016)) where VAEs (and similar models)
have been parameterized with convolutions in the deterministic layers hij, for j = 1, ..., M, and
M is the number of layers connecting the stochastic latent variables zi . The size of the spatial
feature maps decreases towards higher latent representations and transposed convolutions are used
in the generative model. In FAME we propose to extend this notion, so that each of the stochastic
latent layers z* …,zl-i are also convolutional. This gives the model more expressiveness in the
latent layers, since it will keep track of the spatial composition of the data (and thereby learn better
representations). The top stochastic layer zL in FAME is a fully-connected dense layer, which makes
it simpler to condition on a non-informative N (0, I) prior and sample from a learned generative
model pθ(x, z). For the i = 1, ..., L - 1 stochastic latent variables, the architecture is as follows:
hM,i = CNN(h<M,i)
μφ∨θ,i = Linear(CONV(hM,i))
σφ∨θ,i = Softplus(CONV(hM,i)),
where CNN and CONV denote a convolutional neural network and convolutional layer respectively.
The top-most latent stochastic layer zL is computed by:
hM,L = Flatten(CNN(h<M,L))
μφ∨θ,L = Linear(Dense(Am,l))
σφ∨θ,L = Softplus(Dense(hM,L)) .
This new feature map parameterization of the stochastic layers should be viewed as a step towards a
better variational model where the test ELBO and the amount of activated stochastic units are direct
meaures hereof.
2.3	Autoregressive Decoding
From van den Oord et al. (2016b;a); Salimans et al. (2017) we have seen that the PixelCNN ar-
chitecture is very powerful in modelling a conditional distribution between pixels. In FAME we
introduce a PixelCNN in the input dimension of the generative model pθ (x|z) (cf. Figure 1). During
training we concatenate the input with the reconstruction data in the channel dimension and propa-
gate it through the PixelCNN, similarly to what is done in Gulrajani et al. (2016). When generating
samples we fix a sample from the stochastic latent variables and generate the image pixel by pixel
autoregressively.
3 Experiments
We test FAME on images from which we can compare with a wide range of generative models. First
we evaluate on gray-scaled image datasets: statically and dynamically binarized MNIST (LeCun
et al., 1998) and OMNIGLOT (Lake et al., 2013). The OMNIGLOT dataset is of particular interest
due to the large variance amongst samples. Secondly we evaluate our models on natural image
datasets: CIFAR10 (Krizhevsky, 2009) and 32x32 ImageNet1 (van den Oord et al., 2016b). When
1http://image-net.org/small/download.php
4
Under review as a conference paper at ICLR 2018
Figure 2: 10 randomly picked CIFAR10 images (left) and 200 random samples drawn from a
N(0, I) distribution and propagated through the generative model (right).
NLL
IWAE (BURDA ETAL., 2015A)	82.90				NLL
LVAE (S0NDERBY ET AL., 2016)	81.74	DRAW (GREGOR ET AL., 2015)	80.97
CAGEM (MAAL0E et al., 2017)	81.60	DVAE (RoLFE, 2017)	81.01
DVAE (ROLFE, 2017)	80.04	IAF VAE (KINGMA ET AL., 2016)	79.88
VGP (TRAN ET AL., 2016)	79.88	PixelRNN (van den Oord et al., 2016b)	79.20
IAF VAE Kingma et al. (2016)	79.10	VLAE (CHEN ET AL., 2017)	79.03
VLAE Chen ET al. (2017)	78.53	PixelVAE (Gulrajani et al., 2016)	79.02
FAME No Concatenation FAME	78.73 77.82	FAME	79.30
Table 2: Negative log-likelihood performance on dynamically (left) and statically (right) binarized
MNIST in nats. For the dynamically binarized MNIST results show the results for the FAME No
Concatenation that has no dependency on the input image. The evidence lower-bound is computed
with 5000 importance weighted samples L5000 (θ, φ; x).
modelling the gray-scaled images we assume a Bernoulli B distribution using a Sigmoid activation
function as the output and for the natural images we assume a Categorical distribution π by applying
the 256-way Softmax approach introduced in van den Oord et al. (2016b). We evaluate the gray-
scaled images with L5000 (cf. Equation 6) and due to runtime and space complexity we evaluate the
natural images with L1000.
We use a hierarchy of 5 stochastic latent variables. In case of gray-scaled images the stochastic
latent layers are dense with sizes 64, 32, 16, 8, 4 (equivalent to S0nderby et al. (2016)) and for
the natural images they are spatial (cf. Table 1). There was no significant difference when using
feature maps (as compared to dense layers) for modelling gray-scaled images. We apply batch-
normalization (Ioffe & Szegedy, 2015) and ReLU activation functions as the non-linearity between
all hidden layers hi,j and use a simple PixelCNN as in van den Oord et al. (2016b) with 4 residual
blocks.
Because of the concatenation in the autoregressive decoder (cf. Figure 1), generation is a cumber-
some process that scales linearly with the amount of pixels in the input image. Therefore we have
defined a slightly changed parameterization denoted FAME No Concatenation, where the concate-
nation with the input is omitted. The generation has no dependency on the input data distribution
and can therefore be performed in one forward-pass through the generative model.
For optimization we apply the Adam optimizer (Kingma & Ba, 2014) with a constant learning rate
of 0.0003. We use 1 importance weighted sample and temperature (S0nderby et al., 2016) scaling
from .3 to 1. during the initial 200 epochs for gray-scaled images and .01 to 1. during the first 400
epochs for natural images. All models are trained using the same optimization scheme.
3.1	Generative Performance on Gray- s caled Images
The MNIST dataset serves as a good sanity check and has a myriad of previously published gen-
erative modelling benchmarks. We experienced much faster convergence rate on FAME com-
pared to training a regular LVAE. On the dynamically binarized MNIST dataset we see a sig-
5
Under review as a conference paper at ICLR 2018
Gray- s caled images 28x28	Natural images 32x32
h:,1	1 X CONV f=5x5, k=32, s=2 1 X Conv f=3x3, k=64, s=1	2 x Conv f=3x3, k=96, s=1 1 x Conv f=3x3, k=96, s=2
z1	1 x Dense d=64 64 Feature VECTOR	1 x Conv f=3x3, k=8, s=1 16 x 16 x 8 Feature maps
h:,2	1 X CONV f=3x3, k=64, s=2 1 X CONV f=3x3, k=64, s=1	2 x Conv f=3x3, k=192, s=1 1 x CONV F=3x3, K=192, s=2
z2	1 x Dense d=32 32 Feature vector	1 x Conv f=3x3, k=16, s=1 8 x 8 x 16 Feature maps
h:,3	1 X CONV f=3x3, k=64, s=2 1 X CONV f=3x3, k=64, s=1	2 x CONV F=3x3, K=192, s=1 1 x Conv f=3x3, k=192, s=2
z3	1 x Dense d=16 16 Feature vector	1 x Conv f=3x3, k=16, s=1 4 x 4 x 16 Feature maps
h:,4	1 X CONV f=3x3, k=64, s=2 1 X CONV f=3x3, k=64, s=1	2 x Conv f=3x3, k=192, s=1 1 x Conv f=3x3, k=192, s=2
z4	1 x Dense d=8 8 Feature vector	1 x Conv f=3x3, k=16, s=1 2 x 2 x 16 Feature maps
h:,5	1 X CONV f=3x3, k=64, s=2 1 X CONV f=3x3, k=64, s=1	2 x Conv f=3x3, k=192, s=1 1 x CONV F=3x3, K=192, s=2
z5	1 x Dense d=4 4 Feature vector	1 x Dense d=64 64 Feature vector
Table 1: The convolutional layer (Conv), filter size (F), depth (K), stride (S), dense layer (Dense) and
dimensionality (D) used in defining FAME for gray-scaled and natural images. The architecture is
defined such that we ensure dimensionality reduction throughout the hierarchical stochastic layers.
The autoregressive decoder is a PixelCNN (van den Oord et al., 2016b) with a mask A convolution
F=7x7, K=64, S=1 followed by 4 residual blocks of convolutions with mask B, F=3x3, K=64, S=1.
Finally there are three non-residual layers of convolutions with mask B where the last is the output
layer with a Sigmoid activation for gray-scaled images and a 256-way Softmax for natural images.
nificant improvement (cf. Table 2). However, on the statically binarized MNIST, the parame-
terization and current optimization strategy was unsuccessful in achieving state-of-the-art results
(cf. Table 1). In Figure 4a we see random samples drawn from a N(0, I) distribution and prop-
agated through the decoder parameters θ. We also trained the FAME No Concatenation which
performs nearly on par with the previously state-of-the-art VLAE model (Chen et al., 2017) that
in comparison utilizes a skip-connection from the input distribution to the generative decoder:
plocal(x|z) = Qi p(xi|z, xWindowAround(i)). This proves that a better parameterization of the VAE
improves the performance without the need of tedious autoregressive generation. There was no sig-
nificant difference in the KL q(z|x)||p(z) between FAME and FAME No Concatenation. FAME
use 10.85 nats in average to encode images, whereas FAME No Concatenation use 12.29 nats.
OMNIGLOT consists of 50 alphabets of hand-
written characters, where each character has a
limited amount of samples. Each character has
high variance which makes it harder to fit a
good generative model compared to MNIST.
Table 3 presents the negative log-likelihood
of FAME for OMNIGLOT and demonstrates
significant improvement over previously pub-
lished state-of-the-art. Figure 4b shows gen-
erated samples from the learned θ parameter
NLL
IWAE (BURDA ET AL., 2015A)	103.38
LVAE (S0NDERBY ET AL., 2016)	102.11
RBM (BURDA ET AL., 2015B)	100.46
DVAE (ROLFE, 2017)	97.43
DRAW (GREGOR ET AL., 2015)	96.50
CONV DRAW (GREGOR ET AL., 2016)	91.00
VLAE Chen et al. (2017)	89.83
FAME	82.54
Figure 3: Negative log-likelihood performance on
OMNIGLOT in nats. The evidence lower-bound
is computed with 5000 importance weighted sam-
ples L5000 (θ, φ; x).
space.
From S0nderby et al. (2016) We have seen that
the LVAE is able to learn a much tighter L1
ELBO compared to the VAE. For the MNIST
experiments, the L1 ELBO is at 80.11 nats
compared to the L5000 77.82 nats. Similarly the OMNIGLOT L1 ELBO is 86.62 nats compared
to 82.54 nats. This shoWs significant improvements When using importance Weighted samples and
indicates that the parameterization of the FAME can be done in a Way so that the bound is even
6
Under review as a conference paper at ICLR 2018
Co产。构国-d口』
初一xfγg HgX① 3
QnH 限 QBBfcSEx
%R白RRQaAAn
d'¾gRM廿步R次上
BnΔJ⅛r 巾二巴 JIP
OBG;夕叫3阕√2
aπυHH卬攵4R怎口
^e÷dk 3 竹⅛9 七
g Ugd *9 A B 》H
J/Z& IO qa6
7-Γ^JH0o^97
3λ√i⅛？7⅞s9-∕5
9497^3^-00^00
7 O Oo O*s 7 S
CXq/?产2S4 7少
Ur*, Is 5.宁, IOo
8qW 3l 7/
√ ə 7/S 7/ 7，7
⅛i 73/ 74 ⅛* OQ ⅛∕5
(b)
(a)
Figure 4:	Random samples drawn fromaN(0, I) distribution and propagated through the generative
model of FAME for the dynamically binarized MNIST (a) and OMNIGLOT (b) dataset.
tighter. We also find that the top-most latent stochastic layer is not collapsing into its prior, since the
KL(q(z5∣x)∣∣p(z5)) is 5.04natsforMNISTand3.67natsfor OMNIGLOT.
另 4lluΓo 7/ 9
b飞 GZ4/夕 3 /G 4
4fs4lN / 4 2 弓?
Vqq7k:Tb371
，&0250 年 Yq4
ql473b7tMl
6。5。上邑 a LrΓ- 3
/夕G 3 4rq<rtJ/
2661 予 3 ∙O6So
70 7 316fc)∩∙r3
■用 a⅛ΓG7∙ff
♦ waF
<‰D1一丐m∙G 口
R≡αDSB≡⅞，千
Ms]即室T4S3 71
flu n⅛ fl⅞fl≡
70歹131 占□I7"I3
，5p 2≡,π■孑♦
 
eo。"上6,沅 q∙
(a)	(b)
Figure 5:	MNIST reconstructions when masking the output from the FAME stochastic variables (a)
and the concatenated input image (b) prior to feeding them to the autoregressive PixelCNN. It is
interesting to see how the edge information comes from the autoregressive dependency on the input
image.
In order to analyze the contribution from the autoregressive decoder we experimented on masking
the contribution from either the concatenated image or the output of the FAME decoder before
feeding it into the PixelCNN layers (cf. Figure 1). In Figure 5a we see the results of reconstructing
MNIST images when masking out the contribution from the stochastic variables and in Figure 5b
we mask out the contribution from the concatenated input image.
3.2 Generative Performance on Natural Images
We investigate the performance of FAME on two natural image datasets: CIFAR10 and ImageNet.
Learning a generative model on natural images is more challenging, which is also why there are
many tricks that can be done in regards to the autoregressive decoding (van den Oord et al., 2016a;
Salimans et al., 2017; Chen et al., 2017). However, since we are interested in the additional expres-
siveness ofa LVAE parameterization with convolutional stochastic latent variables, we have chosen
a suboptimal architecture for the autoregressive decoding (cf. Table 1) (van den Oord et al., 2016b).
An obvious improvement to the decoder would be to incorporate the PixelCNN++ (Salimans et al.,
2017), but by using the simpler architecture we ensure that the improvements in log-likelihood is
not a result ofa strong autoregressive model.
From Table 3 we see the performance from FAME and FAME No Concatenation on the CIFAR10
dataset. Similarly to the gray-scaled images, FAME outperforms current state-of-the-art results sig-
7
Under review as a conference paper at ICLR 2018
bits/dim
UNIFORM DISTRIBUTION (VAN DEN OORD ET AL., 2016b)	8.00
Deep Diffusion (Sohl-Dickstein et al., 2015)	5.40
Multivariate Gaussian (van den Oord et al., 2016b)	4.70
NICE (DINH ET AL., 2014)	4.48
Deep GMMs (van den Oord & Schrauwen, 2014)	4.00
Conv Draw (Gregor et al., 2016)	3.58
Real NVP (Dinh et al., 2016)	3.49
PixelCNN (van den Oord et al., 2016b)	3.14
IAF VAE Kingma et al. (201 6)	3.11
Gated PixelCNN (van den Oord et al., 2016a)	3.03
PixelRNN (van den Oord et al., 2016b)	3.00
VLAE (CHEN ET AL., 2017)	2.95
PIXELCNN++ (Salimans ET al., 2017)	2.92
FAME NO CONCATENATION	2.98
FAME	2.75
Table 3: Negative log-likelihood performance on CIFAR10 in bits/dim. The evidence lower-bound
is computed with 1000 importance weighted samples L1000 (θ, φ; x).
BITS/DIM
CONV DRAW 32x32 (GREGOR ET AL., 2016)	4.40
CONV DRAW 64X64 (GREGOR ET AL., 2016)	4.10
REAL NVP 32x32 (DINH ET AL., 2016)	4.28
REAL NVP 64x64 (DINH ET AL., 2016)	4.01
PIxELVAE 64x64 (GULRAJANI ET AL., 2016)	3.66
PIxELRNN 32x32 (VAN DEN OORD ET AL., 2016B)	3.86
PIxELRNN 64x64 (VAN DEN OORD ET AL., 2016B)	3.63
GATED PIxELCNN 32x32 (VAN DEN OORD ET AL., 2016A)	3.83
Gated PixelCNN 64x64 (van den OORD et al., 2016a)	3.57
FAME 32x32	323
Table 4: Negative log-likelihood performance on ImageNet in bits/dim. The evidence lower-bound
is computed with 1000 importance weighted samples L1000 (θ, φ; x).
nificantly. It is also interesting to see how FAME No Concatenation performs close to the previously
published state-of-the-art results. Especially in the image space, this could prove interesting, since
the FAME No Concatenation has no additional autoregressive runtime complexity. We only inves-
tigated the 32x32 ImageNet dataset, since the training time is significant and it outperformed the
64x64 models (cf. Table 4), whereas the previously published 64x64 ImageNet models consistently
outperform their 32x32 counterpart. In Figure 2 we show samples from FAME on the CIFAR10
dataset. Similarly to previously published results it is difficult to analyze the performance from the
samples. However, we can conclude that FAME is able to capture spatial correlations in the images
for generating sharp samples. It is also interesting to see how it captures the contours of objects in
the images.
4	Conclusion
We have presented FAME, an extension to the VAE that significantly improve state-of-the-art per-
formance on standard benchmark datasets. By introducing feature map representations in the latent
stochastic variables in addition to top-down inference we have shown that the model is able to
capture representations of complex image distributions while utilizing a powerful autoregressive
architecture as a decoder.
In order to analyze the contribution from the VAE as opposed to the autoregressive model, we have
presented results without concatenating the input image when reconstructing and generating. This
parameterization shows on par results with the previously state-of-the-art results without depending
on the time consuming autoregressive generation.
Further directions for FAME is to (i) test it on larger image datasets with images of a higher resolu-
tion, (ii) expand the model to capture other data modalities such as audio and text, (iii) combine the
model in a semi-supervised framework.
8
Under review as a conference paper at ICLR 2018
References
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8), 2013.
S	.R. Bowman, L. Vilnis, O. Vinyals, A.M. Dai, R. Jozefowicz, and S. Bengio. Generating sentences
from a continuous space. arXiv preprint arXiv:1511.06349, 2015.
Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance Weighted Autoencoders. arXiv
preprint arXiv:1509.00519, 2015a.
Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Accurate and conservative estimates of mrf
log-likelihood using reverse annealing. In Proceedings of the International Conference on Artifi-
cial Intelligence and Statistics, 2015b.
Xi Chen, Diederik P. Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya
Sutskever, and Pieter Abbeel. Variational Lossy Autoencoder. In International Conference on
Learning Representations, 2017.
Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components esti-
mation. arXiv preprint arXiv:1410.8516, 2014.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv
preprint arXiv:1605.08803, 2016.
Marco Fraccaro, S0ren Kaae S0nderby, Ulrich Paquet, and Ole Winther. Sequential neural models
with stochastic layers. In Advances in Neural Information Processing Systems. 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor-
mation Processing Systems. 2014.
Karol Gregor, Ivo Danihelka, Alex Graves, and Daan Wierstra. Draw: A recurrent neural network
for image generation. arXiv preprint arXiv:1502.04623, 2015.
Karol Gregor, Rezende Danilo Jimenez Besse, Fredric, Ivo Danihelka, and Daan Wierstra. Towards
conceptual compression. arXiv preprint arXiv:1604.08772, 2016.
Ishaan Gulrajani, Kundan Kumar, Faruk Ahmed, Adrien Ali Taiga, Francesco Visin, David Vazquez,
and Aaron Courville. PixelVAE: A latent variable model for natural images. arXiv e-prints,
1611.05013, November 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by re-
ducing internal covariate shift. In Proceedings of International Conference on Machine Learning,
2015.
Diederik Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. arXiv preprint
arXiv:1412.6980, 12 2014.
Diederik P. Kingma, Danilo Jimenez Rezende, Shakir Mohamed, and Max Welling. Semi-
Supervised Learning with Deep Generative Models. In Proceedings of the International Con-
ference on Machine Learning, 2014.
Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling.
Improved variational inference with inverse autoregressive flow. In Advances in Neural Informa-
tion Processing Systems. 2016.
Max Kingma, Diederik P; Welling. Auto-Encoding Variational Bayes. arXiv preprint
arXiv:1312.6114, 12 2013.
Rahul G Krishnan, Uri Shalit, and David Sontag. Deep Kalman filters. arXiv:1511.05121, 2015.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University
of Toronto, 2009.
9
Under review as a conference paper at ICLR 2018
Brenden M Lake, Ruslan R Salakhutdinov, and Josh Tenenbaum. One-shot learning by inverting a
compositional causal process. In Advances in Neural Information Processing Systems. 2013.
Yann LeCun, Lon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. In Proceedings of the IEEE Computer Society Conference on Computer
Vision and Pattern Recognition, pp. 2278-2324,1998.
Lars Maal0e, Casper K. S0nderby, S0ren K. S0nderby, and Ole Winther. Auxiliary Deep Generative
Models. In Proceedings of the International Conference on Machine Learning, 2016.
Lars Maal0e, Marco Fraccaro, and Ole Winther. Semi-supervised generation with cluster-aware
generative models. arXiv preprint arXiv:1704.00637, 2017.
Antti Rasmus, Mathias Berglund, Mikko Honkala, Harri Valpola, and Tapani Raiko. Semi-
supervised learning with ladder networks. In Advances in Neural Information Processing Systems,
2015.
Danilo J. Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic Backpropagation and Approx-
imate Inference in Deep Generative Models. arXiv preprint arXiv:1401.4082, 04 2014.
Danilo Jimenez Rezende and Shakir Mohamed. Variational Inference with Normalizing Flows. In
Proceedings of the International Conference on Machine Learning, 2015.
Jason Tyler Rolfe. Discrete variational autoencoders. In Proceedings of the International Conference
on Learning Representations, 2017.
Tim Salimans, Andrej Karparthy, Xi Chen, and Diederik P. Kingma. Pixelcnn++: Improv-
ing the pixelcnn with discretized logistic mixture likelihood and other modifications. arXiv
preprint:1701.05517, 2017, 2017.
Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper-
vised learning using nonequilibrium thermodynamics. arXiv preprint arXiv:1503.03585, 2015.
Casper Kaae S0nderby, Tapani Raiko, Lars Maal0e, S0ren Kaae S0nderby, and Ole Winther. Ladder
variational autoencoders. In Advances in Neural Information Processing Systems 29. 2016.
Dustin Tran, Rajesh Ranganath, and David M Blei. Variational Gaussian process. In Proceedings of
the International Conference on Learning Representations, 2016.
Aaron van den Oord and Benjamin Schrauwen. Factoring variations in natural images with deep
gaussian mixture models. In Advances in Neural Information Processing Systems, 2014.
Aaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Ko-
ray Kavukcuoglu. Conditional image generation with pixelcnn decoders. arXiv preprint
arXiv:1606.05328, 2016a.
Aaron van den Oord, Kalchbrenner Nal, and Koray Kavukcuoglu. Pixel recurrent neural networks.
arXiv preprint arXiv:1601.06759, 01 2016b.
10