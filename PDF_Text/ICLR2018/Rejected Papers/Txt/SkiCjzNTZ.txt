Under review as a conference paper at ICLR 2018
Spontaneous S ymmetry Breaking in Deep Neu-
ral Networks
Anonymous authors
Paper under double-blind review
Ab stract
We propose a framework to understand the unprecedented performance and ro-
bustness of deep neural networks using field theory. Correlations between the
weights within the same layer can be described by symmetries in that layer, and
networks generalize better if such symmetries are broken to reduce the redundan-
cies of the weights. Using a two parameter field theory, we find that the network
can break such symmetries itself towards the end of training in a process com-
monly known in physics as spontaneous symmetry breaking. This corresponds to
a network generalizing itself without any user input layers to break the symmetry,
but by communication with adjacent layers. In the layer decoupling limit applica-
ble to residual networks (He et al., 2015a), we show that the remnant symmetries
that survive the non-linear layers are spontaneously broken based on empirical re-
sults. The Lagrangian for the non-linear and weight layers together has striking
similarities with the one in quantum field theory of a scalar. Using results from
quantum field theory we show that our framework is able to explain many experi-
mentally observed phenomena, such as training on random labels with zero error
(Zhang et al., 2017), the information bottleneck and the phase transition out of it
and the following increase in gradient variances (Shwartz-Ziv & Tishby, 2017),
shattered gradients (Balduzzi et al., 2017), and many more.
1	Introduction
Deep neural networks have been used in image recognition tasks with great success. The first of its
kind, AlexNet (Krizhevsky et al., 2012), led to many other neural architectures have been proposed
to achieve start-of-the-art results in image processing at the time. Some of the notable architec-
tures include, VGG (Simonyan & Zisserman, 2015), Inception (Szegedy et al., 2015) and Residual
networks (ResNet)(He et al., 2015a).
Understanding the inner workings of deep neural networks remains a difficult task. It has been
discovered that the training process ceases when it goes through an information bottleneck (Shwartz-
Ziv & Tishby, 2017) until learning rate is decreased to a suitable amount then the network under
goes a phase transition. Deep networks appear to be able to regularize themselves and able to train
on randomly labeled data Zhang et al. (2017) with zero training error. The gradients in deep neural
networks behaves as white noise over the layers Balduzzi et al. (2017). And many other unexplained
phenomena.
A recent work (Anonymous, 2018) showed that the ensemble behavior and binomial path lengths
(Veit et al., 2016) of ResNets can be explained by just a Taylor series expansion to first order in the
decoupling limit. They found that the series approximation generates a symmetry breaking layer
that reduces the redundancy of weights, leading to a better generalization. Because the ResNet does
not contain such symmetry breaking layers in the architecture. They suggest that ResNets are able to
break the symmetry by the communication between the layers. Another recent work also employed
the Taylor expansion to investigate ResNets (Jastrzebski et al., 2017).
In statistical terms, a quantum theory describes errors from the mean of random variables. We
wish to study how error propagate through each layer in the network, layer by layer. In the limit
of a continuous sample space, the quantum theory becomes a quantum field theory. The effects of
sampling error and labelling error can then be investigated. It is well known in physics that a scalar
field can drive a phase transition. Using a scalar field theory we show that a phase transition must
1
Under review as a conference paper at ICLR 2018
exist towards the end of training based on empirical results. It is also responsible for the remarkable
performance of deep networks compared to other classical models. In Appendix D, We explain that
quantum field theory is likely one of the simplest model that can describe a deep network layer by
layer in the decoupling limit.
Much of the literature on neural network design focuses on different neural architecture that breaks
symmetry explicitly, rather than spontaneously. For instance, non-linear layers explicitly breaks the
symmetry of affine transformations. There is little discussion on spontaneous symmetry breaking.
In neural networks, the Goldstone theorem in field theory states that for every continuous symmetry
that is spontaneously broken, there exists a weight with zero Hessian eigenvalue at the loss mini-
mum. No such weights would appear if the symmetries are explicitly broken. It turns out that many
seemingly different experimental results can be explained by the presence of these zero eigenvalue
weights. In this work, we exploit the layer decoupling limit applicable to ResNets to approximate
the loss functions with a power series in symmetry invariant quantities and illustrate that sponta-
neous symmetry breaking of affine symmetries is the sufficient and necessary condition for a deep
network to attain its unprecedented power.
The organization of this paper is as follows. The background on deep neural networks and field
theory is given in Section 2. Section 3 shows that remnant symmetries can exist in a neural network
and that the weights can be approximated by a scalar field. Experimental results that confirm our
theory is given in Section 4. We summarize more evidence from other experiments in Appendix
A. A review of field theory is given in Appendix B. An explicit example of spontaneous symmetry
breaking is shown in Appendix C.
2	Background and Framework
In this section we introduce our frame work using a field theory based on Lagrangian mechanics.
2.1	Deep Neural Networks
A deep neural network consists of layers of neurons. Suppose that the first layer of neurons with
weight matrix W10 and bias b1 takes input x01 and outputs y1
y1 = W01x01 + b1 = W1x1 ,
where x = (x0, 1) and W1 = (W01 , b1), where W1 and b1 are real valued. Now suppose that
R1 denotes a nonlinear operator corresponding to a sigmoid or ReLU layer located after the weight
layer, so that
y1 = R1W1x1.
For a neural network with T repeating units, the output for layer t is
yt = RtWtyt-1
= RtWtRt-1Wt-1... R1W1x1
=	tY-1 Rt-nWt-n x1.	(1)
n=0
2.2	Symmetries in Neural Networks
We now show the necessary and sufficient conditions of preserving symmetry. We explicitly include
symmetry transformations in Equation (1) and investigate the effects caused by a symmetry trans-
formation of the input in subsequent layers. Suppose Qt ∈ G is a transformation matrix in some
Lie group G for all t. Note that the Qt are not parameters to be estimated. We write yt = yt(Qt),
where the dependence on Qt is obtained from a transformation on the input, xt (Qt) = Qtxt, and
the weights, Wt (Qt)
yt(Qt) = RtWt(Qt)xt(Qt).
If G is a symmetry group, then yt is covariant with xt, such that yt (Qt) = Qtyt. This requires two
conditions to be satisfied. First, Wt(Qt) = QtWtQt-1, where Qt-1Qt = I and the existence of the
inverse is trivial because G is a group and Qt ∈ G. The second is the commutativity between Rt and
2
Under review as a conference paper at ICLR 2018
Qt, such that RtQt = QtRt. For example, if gt ∈ Aff(D), the group of affine transformations, Rt
may not commute with gt. However, commutativity is satisfied when the transformation corresponds
to the 2D rotation of feature maps.
Including transformation matrices, the output at layer t is
yt(Qt) =	tY-1 Rt-nQt-nWt-nQt--1nQ1x1.
n=0
2.3	The Loss Functional
Statistical learning requires the loss function to be minimized. It can be written in the form of a mu-
tual information, training error, or the Kullback-Leibler divergence. In this section we approximate
the loss function in the continuum limit of samples and layers. Then we define the loss functional to
transition into Lagrangian mechanics and field theory. Let zi = (Xi, Yi) ∈ X be the i-th input sam-
ple in data set X, (Xi, Yi) are the features and the desired outputs, respectively, andi ∈ {1, . . . , N}.
The loss function is
1N
L = IN ELi(Xi, Yi, W, Q),
N i=1
where W = (W1, . . . , WT ), and Q = (Q1, . . . , QT), Qt ∈ G where G is a Lie group, and T is
the depth of the network. Taking the continuum limit,
L '	p(X, Y)Lx(X, Y, W, Q)dXdY,
X
where p(X, Y) is the joint distribution ofX and Y. Using the first fundamental theorem of calculus
and taking the continuous layers (t) limit, we write
Lx ' Lx(t = 0) +
0
T dLx(X, Y, W(t), Q(t))
dt
dt,
where Lx(t = 0) is the value of the loss before training. We let Lx,t = dLx/dt be the loss rate per
layer. The loss rate Lx,t is bounded from below. Therefore
minLx(X,Y,W,Q)=Lx(t=0)+ZTminLx,t(X,Y,W(t),Q(t))dt.
Minimizing the loss rate guarantees the minimization of the total loss. We require Lx,t to be invariant
under symmetry transformations. That is, if Q1 (t), Q2(t) ∈ G. Then
Lx,t(X, Y, W(t), Q1(t)) = Lx,t(X, Y, W(t), Q2(t)).
However if Q1(t) and Q2(t) do not belong in the same symmetry group, the above equality does
not necessarily hold. Now we define the loss functional for a deep neural network
P[W,Q] =	p(X, Y)Lx,t(X, Y, W(t), Q(t))dXdYdt.
X ×{0,T}
2.4	Lagrangian mechanics and Field theory
Having defined the loss functional, we can transition into Lagrangian dynamics to give a description
of the feature map flow at each layer. Let the minimizer of the loss rate be
W*(X, Y,Q(t),t) = argmWn p(X, Y)Lχ,t(X, Y, W(t),Q(t)).
From now on, We combine Z = (X, Y) as Y only appears in W* in this formalism, each Y
determines a trajactory for the representation flow determined by Lagrangian mechanics. Now we
define, for each i-th element of W(t), and a non-linear operator R(t) acting on W(t) such that the
loss minimum is centered at the origin,
wi(z,Q(t),t) = R(t)Wi(z,Q(t),t)- R(t)W*i(z,Q(t),t).
(2)
3
Under review as a conference paper at ICLR 2018
We now define the Lagrangian density,
L = L(z, w, ∂tw, ∂zw, Q(t))
and L = T - V , where T is the kinetic energy and V is the potential energy. We define the potential
energy to be
V = p(z)Lx,t(z, W(t), Q(t)),
The probability density p(z) and the loss rate Lx,t are invariant under symmetry transformations.
Therefore V is an invariant as well.
Definition: Orthogonal Group The orthogonal group O(D) is the group of all D × D matrices
such that OTO = I.
We now set up the conditions to obtain a series expansion of V around the minimum wi (Qt) = 0.
First, since V is an invariant. Each term in the series expansion must be an invariant such that
f(wi(Qt), wi(Qt)) = f(wi, wi) for all Qt ∈ G. Suppose G = O(D), the orthogonal group and
that wT (Qt) = wT QT and w(Qt) = Qtw. So wiwi is an invariant. Then f = wiwi is invariant
for all Qt where the Einstein summation convention was used
D
wiwi
wiwi = wTw,
i=1
Now we perform a Taylor series expansion about the minimum wi = 0 of the potential,
V = C + wiHjiwj + wiwj Λimj nwmwn + O((wiwi)6),
where Hji = ∂wi ∂wj V is the Hessian matrix, and similarly for Λimjn. The overall constant C can be
ignored without loss of generality. Because V is an even function in wi around the minimum, we
must have
D
wiHjiwj = X wiHiiwi .
i=1
The O(D) symmetry enforces that all weight Hessian eigenvalues to be Hii = m2 */2 for some
constant m2 . This can be seen in the O(2) case, with constants a, b, a 6= b, Q ∈ O(2) such that
w1(Q) = w2 and w2(Q) = w1,
aw1 (Q)2 + bw2 (Q)2 = aw22 + bw12,
this does not equal aw12 + bw22, so the O(2) symmetry implies a = b. This can be generalized to the
O(D) case. For the quartic term, the requirement that V be even around the minimum gives
D
wiwjΛimjnwmwn = X Λiijjwjwjwiwi .
i,j=1
Similarly the O(D) symmetry implies Nii = λ∕4 for some constant λ and zero for any other ele-
ments, the potential is
v=m Wiwi+4(wi wi)2,
where the numerical factors were added for convention. The power series is a good approximation
in the decoupling limit which may be applicable for Residual Networks.1 For the kinetic term T,
we expand in power series of the derivatives,
1 ∂wi ∂wi
2 ∂t ∂t
c2 ∂wi ∂wi
2 ∂z ∂ Z
+ O((∂tw)4, (∂zw)4),
1The output of a residual unit is yt	=	xt	+	Ft (xt , W1t ,	W2t),	with Ft	xt	and	Ft (xt , W1t , W2t) =
Rt2Wt2Rt1Wt1xt. After centering the loss minimum at the origin with Equation 2 and suitably normalizing,
we can rewrite this as Ft = awT w, with a 1 in the decoupling limit. See Peskin & Schroeder (1995) for
the normalization of a scalar field.
4
Under review as a conference paper at ICLR 2018
where the coefficient for (∂tw)2 is fixed by the Hamiltonian kinetic energy 2 (∂tw)2. Higher order
terms in (∂tw)2 are negligible in the decoupling limit. If the model is robust, then higher order terms
in (∂zw)2 can be neglected as well. 2 The Lagrangian density is 3
L=2(&W)2 - 2(dZw)2 - m w2 - 4(W2)2,
where we have set w2 = wiwi and absorbed c into z without loss of generality. This is precisely the
Lagrangian for a scalar field in field theory. Standard results for a scalar field theory can be found
in Appendix B. To account for the effect of the learning rate, we employ results from thermal field
theory (Kapusta & Gale, 2006) and we identify the temperature with the learning rate η. So that now
m2 = 一μ2 + 1 λη2, with μ2 > 0.
2.5 Spontaneous Symmetry Breaking
Spontaneous symmetry breaking describes a phase transition of a deep neural network. Consider
the following scalar field potential invariant under O(D0) transformations,
V(wi,η) = m 2(η wiwi + 4(wiwi)2,
where m2(η) = -μ2 + 1 λη2, μ2 > 0 and learning rate η. There exists a value of η = n such that
m2 = 0. In the first phase, η > ηc,the loss minimum is at w" = 0, where
w0i(η) = arg min V(wi,η).
wi
When the learning rate η drops sufficiently low, the symmetry is spontaneously broken and the phase
transition begins. The loss minimum bifurcates at η = ηc into
w*(η<ηc) = ±r-m≡.
This occurs when the Hessian eigenvalue becomes negative, m2 (η) < 0, when η < ηc.
This phenomenon has profound implications. It is responsible for phase transition in neural net-
works and generates long range correlation between representations and the desired output. Details
from field theory can be found in Appendix C. Figure 1 depicts the shape of the loss rate during
spontaneous symmetry breaking with a single weight w, and the orthogonal group O(D0) is reduced
to a reflection symmetry O(1) = {1, -1} such that w(Q) = ±w. At η > ηc, the loss rate has a
loss minima at point A. When the learning rate decreases, such that η < ηc, the critical point at A
becomes unstable and new minima with equal loss rate are generated. The weight must go through
B to get to the new minimum C. If the learning rate is too small, the weight will be stuck near
A. This explains why a cyclical learning rate can outperform a monotonic decreasing learning rate
(Smith & Topin, 2017).
Because the loss rate is invariant still to the sponteneously broken symmetry, any new minima gen-
erated from spontaneous symmetry breaking must have the same loss rate. If there is a unbroken
continuous symmetry remaining, there would be a connected loss rate surface corresponding to
the new minima generated by the unbroken symmetry. Spontaneous symmetry breaking splits the
weights into two sets, w → (π, σ). The direction along this degenerate minima in weight space
corresponds to π. And the direction in weight space orthogonal to π is σ . This has been shown
experimentally by Goodfellow et al. (2014) in Figure 19. We show the case for the breaking of O(3)
to O(2) in Figure 2.
2The kinetic term T is not invariant under transformation Q(t). To obtain invariance ∂twi is to be
replaced by the covariant derivative Dtwi so that (Dtwi)2 is invariant under Q(t)(Peskin & Schroeder,
1995). The covariant derivative is Dtwi (z, t, Q(t)) = ∂twi (z, t, Q(t)) + αB(z, t, Q(t))ij wj (z, t, Q(t)),
with B(z, t, Qt) = Q(t)B(z, t)Q(t)-1. The new fields B introduced for invariance is not responsible for
spontaneous symmetry breaking, the focus of this paper. So we will not consider them further.
3Formally, the ∂zw term should be part of the potential V, as T contains only ∂tw terms. However we
adhere to the field theory literature and put the ∂zw term in T with a minus sign.
5
Under review as a conference paper at ICLR 2018
Figure 1: The characteristics of the loss rate with spontaneous symmetry breaking. The dashed line
corresponds to the symmetric phase, while the solid line corresponds to the broken phase. Only at
w = 0 the reflection symmetry w(Q) = ±w is respected. Here, w = σ.
3	S ymmetries in Neural Networks
In this section we show that spontaneous symmetry breaking occurs in neural networks. First, we
show that learning by deep neural networks can be considered solely as breaking the symmetries in
the weights. Then we show that some non-linear layers can preserve symmetries across the non-
linear layers. Then we show that weight pairs in adjacent layers, but not within the same layer, is
approximately an invariant under the remnant symmetry leftover by the non-linearities. We assume
that the weights are scalar fields invariant under the affine Aff(D0) group for some D0 and find that
experimental results show that deep neural networks undergo spontaneous symmetry breaking.
Theorem 1: Deep feedforward networks learn by breaking symmetries Proof: Let Ai be an
operator representing any sequence of layers, and let a network formed by applying Ai repeatedly
such that xout = (QiM=1 Ai)xin. Suppose that Ai ∈ Aff(D), the symmetry group of all affine
transformations. We have L = QiD=1 Ai, where L ∈ Aff(D). Then xout = Lxin for some L ∈
Aff(D) and xout can be computed by a single affine transformation L. When Ai contains a non-
linearity for some i, this symmetry is explicitly broken by the nonlinearity and the layers learn a
more generalized representation of the input.
Now we show that ReLU preserves some continuous symmetries.
Theorem 2: ReLU reduces the symmetry of an Aff(D) invariant to some subgroup Aff(D0),
where D0 < D. Proof: Suppose R denotes the ReLU operator with output yt and Qt ∈ Aff(D)
acts on the input xt, where R(x) = max(0, x). Let xTx be an invariant under Aff(D) and let
xT = (γ, ν), ν < 0 and γ > 0. Let a = Rx = (γ, 0). Then aTa = xT RRx = γTγ. Then aTa
is an invariant under Aff(D0) where D0 = dim γ. Note that γi can be transformed into a negative
value as it has passed the ReLU already.
Corollary If there exists a group G that commutes with a nonlinear operator R, such that QR =
RQ, for all Q ∈ G, then R preserves the symmetry G.
6
Under review as a conference paper at ICLR 2018
Definition: Remnant Symmetry If Qt ∈ G commutes with a non-linear operator Rt for all Qt ,
then G is a remnant symmetry at layer t.
For the loss function Li(Xi, Yi, W, Q) to be invariant, we need the predicted output yT to be
covariant with xi. Similarly for an invariant loss rate Lx,t we require yt to be covariant with xt.
The following theorem shows that a pair of weights in adjacent layers can be considered an invariant
for power series expansion.
Theorem 3: Neural network weights in adjacent layers form an approximate invariant Sup-
pose a neural network consists of affine layers followed by a continuous non-linearity, Rt , and that
the weights at layer t, Wt(Qt) = QtWtQt-1, and that Qt ∈ H is a remnant symmetry such that
QtRt = RtQt. Then wtwt-1 can be considered as an invariant for the loss rate.
Proof: Consider x(Qt) = Qtxt, then
yt(Qt)	= RtWt(Qt)xt(Qt)
= RtQtWtQt-1Qtxt
= RtQtWtxt
= QtRtWtxt,
where in the last line	QtRt	=	RtQt	was used, so	yt (Qt)	=	Qtyt	is covariant with xt .	Now,
xt = Rt-1Wt-1xt-1, so that
yt(Qt) = QtRtWtRt-1Wt-1xt-1.
The pair (RtWt)(Rt-1Wt-1) can be considered an invariant under the ramnant symmetry at layer
t. Let Wt = RtWt - RtWz Therefore WtWt-I is an invariant. □
In the continuous layer limit, wtwt-1 tends to w(t)T w(t) such that w(t) is the first layer and
W(t)T corresponds to the one after. Therefore W(t) can be considered as D0 scalar fields under the
remnant symmetry. The remnant symmetry is not exact in general. For sigmoid functions it is only
an approximation. The crucial feature for the remnant symmetry is that it is continuous so that strong
correlation between inputs and outputs can be generated from spontaneous symmetry breaking. In
the following we will only consider exact remnant symmetries. We will state the Goldstone Theorem
from field theory without proof.
Theorem (Goldstone) For every spontaneously broken continuous symmetry, there exist a weight
π with zero eigenvalue in the Hessian m2π = 0. □
In any case, we will adhere to the case where the remnant symmetry is an orthogonal group O(D0).
Note that W is a D × D matrix and D0 < D. We choose a subset Γ ∈ RD0 of W such that ΓT Γ
is invariant under Aff(D0). Now that we have an invariant, we can write down the Lagrangian for a
deep feedforward network for the weights responsible for spontaneous symmetry breaking.
The Lagrangian for deep feedforward networks in the decoupling limit Let γi = R(Γi) -
R(Γ*i),
L = 2(∂tγi)2 - 1(∂zγi)2 - mηYiYi - 4(γiYi)2.	(3)
Now we can use standard field theory results and apply it to deep neural networks. A review for
field theory is given in Appendix B. The formalism for spontaneous symmetry breaking is given in
Appendix C.
4	Main Results
In this section we assume that the non-linear operator is a piecewise linear function such as ReLU
and set R = I to be the identity and restrict our attention to the symmetry preserving part of R (see
theorem 2). Our discussion also applies to other piecewise-linear activation functions. According
to the Goldstone theorem, spontaneous symmetry breaking splits the set of weight deviations Y into
two sets (σ, π) with different behaviors. Weights π with zero eigenvalues and a spectrum dominated
7
Under review as a conference paper at ICLR 2018
by small frequencies k in its correlation function.4 The other weights σ, have Hessian eigenvalues
μ2 as the weights before the symmetry is broken. In Appendix C, a standard calculation in field
theory shows that the correlation functions of the weights have the form
Pσ,∏ (t, k) = 2ω^ exp ( - iω°t)
(4)
where ωo = JIk|2 + mσ,∏, m27 = —* + 1 λη2 and m∏ = 4λη2. The correlation function of
π approaches infinity as k → 0, as m2π → 0. As the loss is minimized, this corresponds to large
correlation between representations and the desired output over all sample space and layers t.
Spontaneous symmetry breaking and the information bottleneck The neural network under-
goes a phase transition out of the information bottleneck via spontaneous symmetry breaking de-
scribed in Section 2.5. Before the phase transition, the weights γ have positive Hessian eigenvalues
m2. After the phase transition, weights π with zero Hessian eigenvalues are generated by sponta-
neous symmetry breaking. The correlation function for the π weights is concentrated around small
values of IkI, see Equation (4), with ω0 = IkI for any t. This corresponds to a highly correlated
representations across the sample (input) space and layers. Because the loss is minimized, the fea-
ture maps across the network is highly correlated with the desired output. And a large correlation
across the sample space means that the representations are independent of the input. This is shown
in Figure 2 of Shwartz-Ziv & Tishby (2017). After phase transition, I(Y ; T) ' 1 bit for all layers
T, and I(X; T) is small even for representations in early layers.
Gradient variance explosion It has been shown that the variance in weight gradients in the same
layer grow by an order of magnitude during the end of training (Shwartz-Ziv & Tishby, 2017). We
also connect this to spontaneous symmetry breaking. As two sets of weights, (σ, π) are generated
with different distributions. Considering them as the same object would result in a larger variance.
Robustness of deep neural networks We find that neural networks are resilient to overfitting.
Recall that the fluctuation in the weights can arise from sampling noise. Then (∂zwi)2 can be a
measure of model robustness. A small value denotes the weights’ resistance to sampling noise. If the
network were to overfit, the weights would be very sensitive to sampling error. After spontaneous
symmetry breaking, weights at the loss minimum with zero eigenvalues obey the Klein-Gordon
equation with m2π = 0,
(∂z∏)2 = (∂z∏*)(∂z∏) = ∣k∣2, π = exp(iωt — ik ∙ z).
The singularity in the correlation function suggests IkI2 ' 0. The zero eigenvalue weights provide
robustness to the model. Zhang et al. (2017) referred to this phenomenon as implicit regularization.
5 Concluding Remarks
In this work we solved one of the most puzzling mysteries of deep learning by showing that deep
neural networks undergo spontaneous symmetry breaking. This is a first attempt to describe a neural
network with a scalar quantum field theory. We have shed light on many unexplained phenomenon
observed in experiments, summarized in Appendix A.
One may wonder why our theoretical model works so well explaining the experimental results with
just two parameters. It is due to the decoupling limit such that a power series in the loss function
is a good approximation to the network. In our case, the two expansion coefficients are the lowest
number of possible parameters that is able to describe the phase transition observed near the end
of training, where the performance of the deep network improves drastically. It is no coincidence
that our model can explain the empirical observations after the phase transition. In fact, our model
can describe, at least qualitatively, the behaviors of phase transition in networks that the decoupling
limit may not apply to. This suggests that the interactions with nearby layers are responsible for the
phase transition.
4The correlation functions hπi(z, t)πi (z0, t0)i for i ∈ {1, . . . , D0}, is a measure of similarity of the weight
across at z and z0 . A singularity in frequency domain at |k| = 0 corresponds to a high correlation between the
weights across all sample space z = (X, Y) and layers t. This can be interpreted as the correlation length.
8
Under review as a conference paper at ICLR 2018
A	Validation of Proposed Framework for Neural Networks in
the Literature
In this section we summarize other experimental findings that can be explained by the proposed
field theory and the perspective of symmetry breaking. Here Q ∈ G acts on the the input and hidden
variables x, h, as Qx, Qh.
•	The shape of the loss function after spontaneous symmetry breaking has the same shape
observed by Goodfellow et al. (2014) towards the end of training, see Figure 1.
•	The training error typically drops drastically when learning rate is decreased. This oc-
curs when the learning rate drops below ηc, forcing a phase transition so that new minima
develop. See Figure 1.
•	A cyclical learning rate (Smith & Topin, 2017) helps to get to the new minimum faster, see
Section 2.5.
•	Stochasticity in gradient descent juggles the loss function such that the weights are no
longer at the local maximum of Figure 1. A gradient descent step is taken to further take
the weights towards the local minimum. Stochasticity helps the network to generalize
better.
•	When the learning rate is too small to move away from A in Figure 1. PReLU’s (He et al.,
2015b) could move the weight away from A through the training of the non-linearity. This
corresponds to breaking the symmetry explicitly in Theorem 1.
•	Results from Shwartz-Ziv & Tishby (2017) are due to spontaneous symmetry breaking, see
Section 4.
•	Deep neural networks can train on random labels with low training loss as feature maps
are highly correlated with their respective desired output. Zhang et al. (2017) observed that
a deep neural network can achieve zero training error on random labels. This shows that
small Hessian eigenvalues is not the only condition that determines robustness.
•	Identity mapping outperforms other skip connections (He et al., 2016) is a result of the
residual unit’s output being small. Then the residual units can be decoupled leading to
a small λ and so it is easier for spontaneous symmetry breaking to occur, from m2 =
-μ2 + 1 λη2.
•	Skip connection across residual units breaks additional symmetry. Suppose now an identity
skip connection connects x1 and the output ofF2. Now perform a symmetry transformation
on x1 and x2 , Q1 and Q2 ∈ G , respectively. Then the output after two residual untis is
Qx3 = Q1x1 + Q2x2 + Q2F2. Neither Q = Q1 nor Q = Q2 can satisfy the covariance
under G. This is observed by Orhan & Pitkow (2017).
•	The shattered gradient problem (Balduzzi et al., 2017). It is observed that the gradient in
deep (non-residual) networks is very close to white noise. This is reflected in the expo-
nential in Equation (7). This effect on ResNet is reduced because of the decoupling limit
λ → 0. This leads to the weight eigenvalues m2 being larger in non-residual networks
owing to m2 = -μ2 + ɪ λη2. And so a higher oscillation frequency in the correlation
function.
•	In recurrent neural networks, multiplicative gating (Yuhuai et al., 2016) combines the input
x and the hidden state h by an element-wise product. Their method outperforms the method
with an addition x+h because the multiplication gating breaks the covariance of the output.
A transformation Qx * Qh = Q(X * h), whereas for addition the output remains covariant
Qx + Qh = Q(x+ h).
B Review of Field Theory
In this section we state the relevant results in field theory without proof. We use Lagrangian mechan-
ics for fields w(x, t). Equations of motion for fields are the solution to the Euler-Lagrange equation,
which is a result from the principle of least action. The action, S, is
S[w] =
L(t, w, ∂tw)dt,
9
Under review as a conference paper at ICLR 2018
where L is the Lagrangian. Define the Lagrangian density
L(t) =	L(z, t, w, ∂z,tw)dz.
The action in term of the Lagrangian density is
S[w] =	L(z, t, w, ∂z,tw)dzdt.
The Lagrangian can be written as a kinetic term T, and a potential term V (loss function),
L=T -V
For a real scalar field w(x, t),
T=1 (∂t Y
—
1 (∂w) =2(dtW)2- I(Hzw)2
where we have set the constant c2 = 1 without loss of generality. The potential for a scalar field that
allows spontaneous symmetry breaking has the form
V = m2 w2 + λw4.
2	4
In the decoupling limit, λ → 0, the equation of motion for w is the Klein-Gordon Equation
[(∂t)2 - (∂z)2 - m2]w = 0.
In the limit of m2 → 0, the Klein-Gordon Equation reduces to the wave equation with solution
w(z,t) = ei(ωt-k∙z),
where i = √-1.
One can treat w as a random variable such that the probability distribution (a functional) of the scalar
field w(z, t) is p[w] = exp(-S[w])/Z, where Z is some normalizing factor. The distribution peaks
at the solution of the Klein-Gordon equation since it minimizes the action S. Now we can define the
correlation function between w(z1, t1) and w(z2, t2),
hw(zι,t1)w(z2,t2)i = ZZ /w(zi,ti)w(Z2,t2)e-S[w]Dw,
where Dw denotes the integral over all paths from (z1, t1) to (z2, t2). In the decoupling limit
λ → 0, it can be shown that
exp(-S[w]) = exp
w(∂t2 - ∂z2 + m2)w dzdt ,
where Stokes theorem was used and the term on the boundary of (sample) space is set to zero. The
above integral in the exponent is quadratic in w and the integral over Dw can be done in a similar
manner to Gaussian integrals. The correlation function of the fields across two points in space and
time is
hw(z1, t1)w(z2, t2)i = G(z1, t1, z2, t2),
where G(z1, t1, z2, t2) is the Green’s function to the Klein-Gordon equation, satisfying
(∂t2 - ∂z2 + m2)G(z1, t1, z2, t2) = δ(z1 - z2)δ(t1 - t2).
The Fourier transformation of the correlation function is
G(ω, k) = ω2 - |k|2 - m2 ,	m2 > 0.
An inverse transform over ω gives
G(t,k) = ^---ω0°t
2ω0
with ω02 = |k|2 + m2 .
10
Under review as a conference paper at ICLR 2018
Figure 2: The loss after spontaneous symmetry breaking from othogonal group O(3) to O(2). The
symmetry transformations of O(2), which are 1-D rotations, form the degenerate minima.
C S pontaneous S ymmetry B reaking in the Orthogonal Group
O(D0)
In this section We show that weights ∏ with small, near zero, eigenvalues m∏ = 1 λη2 are generated
by spontaneous symmetry breaking. Note that we can write the Lagrangian in Equation (3) as
L = T - V. Consider weights γ that transforms under O(D0), from Equation (3)
T =	1(∂tγi)2 - 2(∂zγi)2,
V = Tm YiYi+4(YiYi)2.
(5)
When m2 = -μ2 + 4 λη2 < 0, it can be shown that in this case the loss minimum is no longer at
γi = 0, but it has a degenerate minima on the surface such that Pi(γi)2 = v, where V = --r∕2∕λ.
Now we pick a point on this loss minima and expand around it. Write Yi = (πk, v + σ), where
k ∈ {1, . . . , D0 - 1}. Intuitively, the πk fields are in the subspace of degenerate minima and σ is
the field orthogonal to π. Then it can be shown that the Lagrangian can be written as
L = Tπ + Tσ - Vπ - Vσ - Vπσ,
where, in the weak coupling limit λ → 0,
Tπ
Tσ
Vπ
Vσ
Vπσ
2(∂t∏k)2 - 1(∂z ∏k )2,
2(dtσ)2 - 2(dzσ)2,
O(λ),
-2 m2σ2
O(λ),
(6)
11
Under review as a conference paper at ICLR 2018
the fields π and σ decouple from each other and can be treated separately. The σ fields satisfy
the Klein-Gordon Equation ( - m2)σ = 0, with = ∂t2 - ∂z2. The π fields satisfy the wave-
equation, π = 0. The correlation functions of the weights across sample space and layers, Pσ =
hσ(z0, t0)σ(z, t)i and Pπ = hπ(z0, t0)π(z, t)i are the Green’s functions of the respective equations
of motion. Fourier transforming the correlation functions give
Pσ,∏ (t, k) = d exP ( - i"0t)，	(7)
where ωo = JIkI2 + ∣m⅛,∏ I，and m∏ = 1 λη2 ` 0. The correlation function Pn is dominated by
values of |k| ' 0. Therefore hππi → ∞ as λη2 → 0. On the other hand, it can be shown that hσσi
is damped by the weight eigenvalues Im2I. The singularity in the correlation function means that
the value of the weights at the start of the layer is highly correlated with the ones in later layers.
In the language of group theory. The O(D) symmetry is broken down to O(D-1). Elements of O(D)
are the D × D orthogonal matrices, which have D(D- 1)/2 independent continous symmetries (e.g.
the Euler angles in D = 3). The number of continuous broken from O(D) to O(D - 1) is D - 1.
In the above example we showed that this corresponds to the D - 1 πk fields. Each of which have
zero Hessian eigenvalue.
Even though we formulated our field theory based on the decoupling limit of ResNets, the result of
infinite correlation is very general and can be applied even if the decoupling limit is not valid. It is
a direct result of spontaneous symmetry breaking. We state the Goldstone Theorem without proof.
Theorem (Goldstone): For every continuous symmetry that is spontaneously broken, a weight π
with zero Hessian eigenvalue is generated at zero temperature (learning rate η).
D Why Quantum Field Theory ?
In brief, the formalism for spontaneous symmetry breaking is mostly done in quantum field theory.
In terms of statistics, quantum mechanics is the study of errors. We also believe that it is a good
approximation to deep neural networks in the presence of the non-linear operators. The non-linear
operators quantizes the input. Let R denotes the opertor corresponding to a sigmoid, say, then the
output is R(W) ' {0, +1} for the most part. And the negative end of ReLU is zero.
Let us take a step back and go through the logical steps to understand that a scalar quantum field
theory is perhaps one of the simplest model one can consider to describe a neural network layer by
layer, in the decoupling limit. We wish to formulate a dynamical model to describe the weights layer
by layer,
1.	We know that the outputs of non-linearities are quantized. And they need to be quantized
to break the affine symmetry (see Theorem 1). This leads to quantum mechanics.
2.	Quantum mechanics does not admit spontaneous symmetry breaking (Zee, 2010).
3.	The decoupling limit allows spontaneous symmetry breaking in quantum mechanics.
4.	The decoupling limit and non-linearity together is quantum field theory.
Therefore, if one wishes to model the outputs of non-linearities in the decoupling limit. There is no
choice but to employ quantum field theory.
References
Anonymous. Decoupling the layers in residual networks. International Conference on Learning
Representations, 2018.
D. Balduzzi et al. The shattered gradients problem: If resnets are the answer, then what is the
question? 2017. https://arxiv.org/abs/1702.08591.
I. J. Goodfellow et al. Qualitatively characterizing neural network optimization problems. 2014.
https://arxiv.org/abs/1412.6544.
12
Under review as a conference paper at ICLR 2018
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. CoRR, 2015a.
https://arxiv.org/abs/1512.03385.
K. He et al. Delving deep into rectifiers: Surpassing human-level performance on imagenet classifi-
cation. 2015b. https://arxiv.org/abs/1502.01852.
K. He et al. Identity mappings in deep residual networks. European Conference on Computer Vision.
Springer International Publishing, 2016.
S. Jastrzebski et al. Residual connections encourage iterative inference. 2017. https://arxiv.
org/abs/1710.04773.
J. I. Kapusta and C. Gale. Finite-temperature field theory: principles and applications. Cambridge
University Press, 2nd edition, 2006.
A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural
networks. NIPS, 2012.
E. Orhan and X. Pitkow. Skip connections eliminate singularities. 2017. https://arxiv.org/
abs/1701.09175.
M Peskin and D. V. Schroeder. Introduction to quantum field theory. Westview Press, 1995.
R. Shwartz-Ziv and N. Tishby. Opening the black box of deep neural networks via information.
2017. https://arxiv.org/abs/1703.00810.
K.	Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recogni-
tion. ICLR, 2015.
L.	N. Smith and N. Topin. Super-convergence: Very fast training of residual networks using large
learning rates. 2017. https://arxiv.org/abs/1708.07120.
C. Szegedy et al. Going deeper with convolutions. CVPR, 2015.
A. Veit, Wilber M., and S. Belongie. Residual networks behave like ensembles of relatively shallow
networks. 2016. https://arXiv.org/abs/1605.06431.
W. Yuhuai et al. On multiplicative integration with recurrent neural networks. NIPS, 2016. https:
//arxiv.org/abs/1703.00810.
A. Zee. Quantum field theory in a nutshell. Princeton University Press, 2010.
C. Zhang et al. Understanding deep learning requires rethinking generalization. 2017.
13