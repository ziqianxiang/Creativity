Under review as a conference paper at ICLR 2018
Automatic Goal Generation for
Reinforcement Learning Agents
Anonymous authors
Paper under double-blind review
Ab stract
Reinforcement learning (RL) is a powerful technique to train an agent to perform
a task. However, an agent that is trained using RL is only capable of achieving
the single task that is specified via its reward function. Such an approach does not
scale well to settings in which an agent needs to perform a diverse set of tasks,
such as navigating to varying positions in a room or moving objects to varying
locations. Instead, we propose a method that allows an agent to automatically dis-
cover the range of tasks that it is capable of performing in its environment. We use
a generator network to propose tasks for the agent to try to accomplish, each task
being specified as reaching a certain parametrized subset of the state-space. The
generator network is optimized using adversarial training to produce tasks that are
always at the appropriate level of difficulty for the agent. Our method thus auto-
matically produces a curriculum of tasks for the agent to learn. We show that, by
using this framework, an agent can efficiently and automatically learn to perform
a wide set of tasks without requiring any prior knowledge of its environment1.
Our method can also learn to accomplish tasks with sparse rewards, which pose
significant challenges for traditional RL methods.
1	Introduction
Reinforcement learning (RL) can be used to train an agent to perform a task by optimizing a reward
function. Recently, a number of impressive results have been demonstrated by training agents using
RL: such agents have been trained to defeat a champion Go player (Silver et al., 2016), to outperform
humans in 49 Atari games (Guo et al., 2016; Mnih et al., 2015), and to perform a variety of difficult
robotics tasks (Lillicrap et al., 2015; Duan et al., 2016; Levine et al., 2016). In each of the above
cases, the agent is trained to optimize a single reward function in order to learn to perform a single
task. However, there are many real-world environments in which a robot will need to be able to
perform not a single task but a diverse set of tasks, such as navigating to varying positions in a room
or moving objects to varying locations. We consider the problem of maximizing the average success
rate of our agent over all possible goals, where success is defined as the probability of successfully
reaching each goal by the current policy.
In order to efficiently maximize this objective, the algorithm must intelligently choose which goals
to focus on at every training stage: goals should be at the appropriate level of difficulty for the
current policy. To do so, our algorithm allows an agent to generate its own reward functions, defined
with respect to target subsets of the state space, called goals. We generate such goals using a Goal
Generative Adversarial Network (Goal GAN), a variation of to the GANs introduced by Goodfellow
et al. (2014). A goal discriminator is trained to evaluate whether a goal is at the appropriate level
of difficulty for the current policy, and a goal generator is trained to generate goals that meet this
criteria. We show that such a framework allows an agent to quickly learn a policy that reaches all
feasible goals in its environment, with no prior knowledge about the environment or the tasks being
performed. Our method automatically creates a curriculum, in which, at each step, the generator
generates goals that are only slightly more difficult than the goals that the agent already knows how
to achieve.
In summary, our main contribution is a method for automatic curriculum generation that consid-
erably improves the sample efficiency of learning to reach all feasible goals in the environment.
1Videos and code available at: https://sites.google.com/view/goalgeneration4rl
1
Under review as a conference paper at ICLR 2018
Learning to reach multiple goals is useful for multi-task settings such as navigation or manipulation,
in which we want the agent to perform a wide range of tasks. Our method also naturally handles
sparse reward functions, without needing to manually modify the reward function for every task,
based on prior task knowledge. Instead, our method dynamically modifies the probability distribu-
tion from which goals are sampled to ensure that the generated goals are always at the appropriate
difficulty level, until the agent learns to reach all goals within the feasible goal space.
2	Related Work
The problem that we are exploring has been referred to as “multi-task policy search” (Deisenroth
et al., 2014) or “contextual policy search,” in which the task is viewed as the context for the pol-
icy (Deisenroth et al., 2013; Fabisch & Metzen, 2014). Unlike the work of Deisenroth et al. (2014),
our work uses a curriculum to perform efficient multi-task learning, even in sparse reward settings.
In contrast to Fabisch & Metzen (2014), which trains from a small number of discrete contexts /
tasks, our method generates a training curriculum directly in continuous task space.
Intrinsic Motivation: Intrinsic motivation involves learning with an intrinsically specified objec-
tive (Schmidhuber, 1991; 2010). Intrinsic motivation has also been studied extensively in the de-
velopmental robotics community, such as SAGG-RIAC (Baranes & Oudeyer, 2010; 2013a), which
has a similar goal of learning to explore a parameterized task space. However, our experiments with
SAGG-RIAC demonstrate that this approach do not explore the space as efficiently as ours. A re-
lated concept is that of competence-based intrinsic motivation (Baldassarre & Mirolli, 2012), which
uses a selector to select from a discrete set of experts. Recently there have been other formulations
of intrinsic motivation, relating to optimizing surprise (Houthooft et al., 2016; Achiam & Sastry,
2016) or surrogates of state-visitation counts (Bellemare et al., 2016; Tang et al., 2016). All these
approaches improve learning in sparse tasks where naive exploration performs poorly. However,
these formulations do not have a notion of which states are hard for the learner, and the intrinsic
motivation is independent of the current performance of the agent. In contrast, our formulation of
intrinsic motivation directly relates to our policy performance: the agent is motivated to train on
tasks that push the boundaries of its capabilities.
Curriculum Learning: The increasing interest on training single agents to perform multiple tasks
is leading to new developments on how to optimally present the tasks to the agent during learn-
ing. The idea of using a curriculum has been explored in many prior works on supervised learning
(Bengio et al., 2009; Zaremba & Sutskever, 2014; Bengio et al., 2015). However, these curricula
are usually hand-designed, using the expertise of the system designer. Another lines of work take
into explicit consideration which examples are hard for the current learner (Kumar et al., 2010;
Jiang et al., 2015); or use learning progress to build an automatic curriculum(Graves et al., 2017),
however both approaches have mainly been applied for supervised tasks. Most curriculum learn-
ing in RL still relies on fixed pre-specified sequences of tasks (Karpathy & Van De Panne, 2012).
Other recent work has proposed using a given baseline performance for several tasks to gauge which
tasks are the hardest and require more training (Sharma & Ravindran, 2017), but the framework can
only handle a finite set of tasks and cannot handle sparse rewards. Our method trains a policy that
generalizes to a set of continuously parameterized tasks and is shown to perform well even under
sparse rewards by not allocating training effort to tasks that are too hard for the current performance
of the agent. Finally, an interesting self-play strategy has been proposed that is concurrent to our
work (Sukhbaatar et al., 2017); however, they view their approach as simply providing an explo-
ration bonus for a single target task; in contrast, we focus on the problem of efficiently optimizing a
policy across a range of goals, as we explain below.
3	Problem Definition
3.1	Goal-parameterized Reward Functions
In the traditional RL framework, at each timestep t, the agent in state st ∈ S ⊆ Rn takes an action
at ∈ A ⊆ Rm, according to some policy π(at | st) that maps from the current state st to a probability
distribution over actions. Taking this action causes the agent to enter into a new state st+1 according
to a transition distribution p(st+1 |st, at), and receive a reward rt = r(st, at, st+1). The objective
2
Under review as a conference paper at ICLR 2018
of the agent is to find the policy π that maximizes the expected return, defined as the sum of rewards
R = PtT=0 rt, where T is a maximal time given to perform the task. The learned policy corresponds
to maximizing the expected return for a single reward function.
In our framework, instead of learning to optimize a single reward function, we consider a range of
reward functions rg indexed or parametrized by a goal g ∈ G . Each goal g corresponds to a set of
states Sg ⊂ S such that goal g is considered to be achieved when the agent is in any state st ∈ Sg .
Then the objective is to learn a policy that, given any goal g ∈ G, acts optimally with respect to rg .
We define a very simple reward function that measures whether the agent has reached the goal:
rg(st, at, st+1) = 1{st+1 ∈ Sg},	(1)
where 1 is the indicator function. In our case, We use Sg = {st : d(f (st),g) ≤ e}, where f (∙)
is a function that projects a state into goal space G, d(∙, ∙) is a distance metric in goal space, and
is the acceptable tolerance that determines when the goal is reached. However, our method can
handle generic binary rewards (as in Eq. (1)) and does not require a distance metric for learning.
Furthermore, we define our MDP such that each episode terminates when st ∈ Sg . Thus, the return
Rg = PtT=0 rtg is a binary random variable whose value indicates whether the agent has reached
the set Sg in at most T time-steps. Hence, the return of a trajectory s0, s1, . . . can be expressed as
Rg = 1{StT=0 st ∈ Sg}. Now, policies are also conditioned on the current goal g (as in Schaul et al.
(2015)), written as π(at | st, g). The expected return obtained when we take actions sampled from
the policy can then be expressed as the probability of succeeding on each goal within T timesteps,
as shown in Eq. (2).
Rg(∏) = E∏(∙ I st,g) 1{∃ t ∈ [1 ∙ ∙ ∙T]: St ∈ Sg} = p(∃ t ∈ [1.. .T]: St ∈ Sg I ∏,g^	⑵
The sparse indicator reward function of Eq. (1) is not only simple but also represents a property
of many real-world goal problems: in many settings, it may be difficult to tell whether the agent
is getting closer to achieving a goal, but easy to tell when a goal has been achieved. For example,
for a robot moving in a maze, taking actions that maximally reduce the straight-line distance from
the start to the goal is usually not a feasible approach for reaching the goal, due to the presence of
obstacles along the path. In theory, one could hand-engineer a meaningful distance function for each
task that could be used to create a dense reward function. Instead, we use the indicator function of
Eq. (1), which simply captures our objective by measuring whether the agent has reached the goal
state. We show that our method is able to learn even with such sparse rewards.
3.2	Overall Objective
We desire to find a policy π(at | St, g) that achieves a high reward for many goals g. We assume
that there is a test distribution of goals pg(g) that we would like to perform well on. For simplicity,
we assume that the test distribution samples goals uniformly from the set of goals G, although in
practice any distribution can be used. The overall objective is then to find a policy ∏* such that
∏ (at | St,g) = arg max Wg〜PgG) Rg(∏) ∙	(3)
Recall from Eq. (2) that Rg(π) is the probability of success for each goal g. Thus the objective of
Eq. (3) measures the average probability of success over all goals sampled from pg(g). We refer to
the objective in Eq. (3) as the coverage objective.
3.3	Assumptions
Similar to previous work (Schaul et al., 2015; Kupcsik et al., 2013; Fabisch & Metzen, 2014; Deisen-
roth et al., 2014) we need a continuous goal-space representation such that a goal-conditioned policy
can efficiently generalize over the goals. In particular, we assume that: 1 2 * *
1. A policy trained on a sufficient number of goals in some area of the goal-space will learn
to interpolate to other goals within that area.
2. A policy trained on some set of goals will provide a good initialization for learning to
extrapolate to close-by goals, meaning that the policy can occasionally reach them but
maybe not consistently so.
3
Under review as a conference paper at ICLR 2018
Furthermore, we assume that if a goal is reachable, there exists a policy that does so reliably. This
is a reasonable assumption for any practical robotics problem, and it will be key for our method, as
it strives to train on every goal until it is consistently reached.
4	Method
Our approach can be broken down into three parts: First, we label a set of goals based on whether
they are at the appropriate level of difficulty for the current policy. Second, using these labeled goals,
we construct and train a generator to output new goals that are at the appropriate level of difficulty.
Finally, we use these new goals to efficiently train the policy, improving its coverage objective. We
iterate through each of these steps until the policy converges.
4.1	Goal Labeling
As shown in our experiments, sampling goals from pg (g) directly, and training our policy on each
sampled goal may not be the most sample efficient way to optimize the coverage objective of Eq. (3).
Instead, we modify the distribution from which we sample goals during training: we wish to find
the set of goals g in the set Gi = {g : Rmin ≤ Rg (πi) ≤ Rmax} ⊆ G.
The justification for this is as follows: due to the sparsity of the reward function, for most goals
g, the current policy πi (at iteration i) obtains no reward. Instead, we wish to train our policy on
goals g for which πi is able to receive some minimum expected return Rg (πi) > Rmin such that
the agent receives enough reward signal for learning. On the other hand, if we only sample from
goals for which Rg (πi) > Rmin, we might sample repeatedly from a small set of already mastered
goals. To force our policy to train on goals that still need improvement, we train on the set of
goals g for which Rg (πi) ≤ Rmax, where Rmax is a hyperparameter setting a maximum level of
performance above which we prefer to concentrate on new goals. Thus, training our policy on goals
in Gi allows us to efficiently maximize the coverage objective of Eq. (3). Note that from Eq. (2),
Rmin and Rmax can be interpreted as a minimum and maximum probability of reaching a goal over
T timesteps. Given a set of goals sampled from some distribution pdata (g), we wish to estimate a
label yg ∈ {0, 1} for each goal g that indicates whether g ∈ Gi. These labels are obtained based on
the policy performance during the policy update step (Sec. 4.3); see Appendix C for details on this
procedure. In the next section we describe how we can generate more goals that also belong to Gi ,
in addition to the goals that we have labeled.
4.2	Adversarial Goal Generation
In order to sample new goals g uniformly from Gi , we introduce an adversarial training procedure
called “goal GAN”, which is a modification of the procedure used for training Generative Adversar-
ial Networks (GANs) (Goodfellow et al., 2014). The modification allows us to train the generative
model both with positive examples from the distribution we want to approximate and negative exam-
ples sampled from a distribution that does not share support with the desired one. This improves the
accuracy of the generative model despite being trained with very few positive samples. Our choice
of GANs for goal generation was motivated both from this potential to train from negative examples
as well as their ability to generate very high dimensional samples such as images (Goodfellow et al.,
2014) which is important for scaling up our approach to goal generation in high-dimensional goal
spaces. Other generative models like Stochastic Neural Networks (Tang & Salakhutdinov, 2013)
don’t accept negative examples and don’t have the potential to scale up to higher dimensions.
In our particular application, we use a “goal generator” neural network G(z) to generate goals g
from a noise vector z . We train the goal generator to uniformly output goals in Gi using a second
“goal discriminator” network D(g). The latter is trained to distinguish goals that are in Gi from
goals that are not in Gi. We optimize our G(z) and D(g) in a manner similar to that of the Least-
Squares GAN (LSGAN) (Mao et al., 2016), which we modify by introducing the binary label yg to
indicate whether g ∈ Gi (allowing us to train from “negative examples” when yg = 0):
mDn V (D)= Wg~Pdata (g) J% (D(O)- b)2 + (I- yg )(D(g) - a)2i + ^Z~Pz(z)[(D(G(Z))- a)2]
min V(G) = ^z~pz(z) [D(G(z)) - c)2]	(4)
4
Under review as a conference paper at ICLR 2018
Algorithm 1: Generative Goal Learning	
Input : Policy ∏o	
Output: Policy πN	
(G,D) - initialize_GAN()	
goalsoid - 0	
f	)r i J 1 to N do z J Sample_noise(pz(∙)); goals J G(z) ∪ sample(goalsold); ∏i J Update-policy(goals,∏i-ι); returns J evaluate_policy (goals, ∏i); labels J label_goals(returns) (G, D) J train_GAN(goals, labels, G, D); goalsoid J Update_replay(goals)
end	
We directly use the original hyperparameters reported in Mao et al. (2016) in all our experiments (a =
-1, b = 1, and c = 0). The LSGAN approach gives us a considerable improvement in training stability
over vanilla GAN, and it has a comparable performance to WGAN (Arjovsky et al., 2017). However,
unlike in the original LSGAN paper (Mao et al., 2016), we have three terms in our value function
V (D) rather than the original two. For goals g for which yg = 1, the second term disappears and
we are left with only the first and third terms, which are identical to that of the original LSGAN
framework. Viewed in this manner, the discriminator is trained to discriminate between goals from
pdata(g) with a label yg = 1 and the generated goals G(z). Looking at the second term, our
discriminator is also trained with “negative examples,” i.e. goals with a label yg = 0 which our
generator should not generate. The generator is trained to “fool” the discriminator, i.e. to output
goals that match the distribution of goals in pdata(g) for which yg = 1.
4.3	Policy Optimization
Our full algorithm for training a policy
π(at | st, g) to maximize the coverage objec-
tive in Eq. (3) is shown in Algorithm 1. At each
iteration i, we generate a set of goals by first
using Sample_noise to obtain a noise vector
Z from pz(∙) and then passing this noise to the
generator G.
We use these goals to train our policy us-
ing RL, with the reward function given by
Eq. (1) (Update_policy). The training can
be done with any RL algorithm; in our case
we use TRPO (Schulman et al., 2015a) with
GAE (Schulman et al., 2015b). Our pol-
icy’s emperical performance on these goals
(evaluate_policy) is used to determine
each goal,s label yg (label-goals), as de-
scribed in Section 4.1. Next, We use these labels
to train our goal generator and our goal discriminator (train_GAN), as described in Section 4.2.
The generated goals from the previous iteration are used to compute the Monte Carlo estimate of
the expectations With respect to the distribution pdata (g) in Eq. (4). By training on goals Within Gi
produced by the goal generator, our method efficiently finds a policy that optimizes the coverage
objective. For details on how we initialize the goal GAN (initialize_GAN), and how we use a
replay buffer to prevent “catastrophic forgetting” (Update_replay), see Appendix A.
The algorithm described above naturally creates a curriculum for our policy. The goal generator
generates goals in Gi , for which our policy obtains an intermediate level of return, and thus such
goals are at the appropriate level of difficulty for our current policy πi . As our policy improves,
the generator learns to generate goals in order of increasing difficulty. Hence, our method can be
viewed as a way to automatically generate a curriculum of goals. However, the curriculum occurs
as a by-product via our optimization, without requiring any prior knowledge of the environment or
the tasks that the agent must perform.
5	Experimental Results
In this section we provide the experimental results to answer the following questions:
•	Does our automatic curriculum yield faster maximization of the coverage objective?
•	Does our Goal GAN dynamically shift to sample goals of the appropriate difficulty?
•	Does it scale to a higher-dimensional state-space with a low-dimensional space of feasible goals?
To answer the first two questions, we demonstrate our method in two challenging robotic locomotion
tasks, where the goals are the (x, y) position of the Center of Mass (CoM) of a dynamically complex
quadruped agent. In the first experiment the agent has no constraints and in the second one the
agent is inside a U-maze. To answer the third question, we study how our method scales with the
5
Under review as a conference paper at ICLR 2018
dimension of the state-space in an environment where the feasible region is kept of approximately
constant volume in an embedding space that grows in dimension.
We compare our Goal GAN method against four baselines. Uniform Sampling is a method that
does not use a curriculum at all, training at every iteration on goals uniformly sampled from the
goal-space. To demonstrate that a straight-forward distance reward can be prone to local minima,
Uniform Sampling with L2 loss samples goals in the same fashion as the first baseline, but instead of
the indicator reward that our method uses, it receives the negative L2 distance to the goal as a reward
at every step. We have also adapted two methods from the literature to our setting: Asymmetric Self-
play (Sukhbaatar et al., 2017) and SAGG-RIAC (Baranes & Oudeyer, 2013b). Finally, we provide
an ablation and an oracle for our method to better understand the importance of sampling “good”
goals. The ablation GAN fit all consists on not training the GAN only on the “good” goals but rather
on every goal attempted in the previous iteration. Given the noise injected at the output of the GAN
this generates a gradually expanding set of goals - similar to any hand-designed curriculum. The
oracle consists in sampling goals uniformly from the feasible state-space, but only keeping them
if they satisfy the criterion defined in Section 4.1. This Rejection Sampling method is orders of
magnitude more expensive in terms of labeling, but it serves to estimate an upper-bound for our
method in terms of performance.
5.1 Ant Locomotion
We test our method in two challenging environments of a com-
plex robotic agent navigating either a free space (Free Ant) or a
U-shaped maze (Maze Ant). The latter is depicted in Fig. 1, where
the orange quadruped is the Ant, and a possible goal to reach is
drawn in red. Duan et al. (2016) describe the task of trying to reach
the other end of the U-turn and they show that standard RL methods
are unable to solve it. We further extend the task to ask to be able
to reach any given point within the maze, or the [-5, 5]2 square for
Free Ant. The reward is still a sparse indicator function which takes
the value 1 only when the (x, y) CoM of the Ant is within = 0.5
of the goal. Therefore the goal space is 2 dimensional, the state-
space is 41 dimensional and the action space is 8 dimensional (see
details in Appendix B.1).
Figure 1: Ant Maze
We first explore whether, by training on goals that are generated by our Goal GAN, we are able to
improve our policy’s training efficiency, compared to the baselines described above. In Figs. 2a-
Fig. 2b we see that our method leads to faster training compared to the baselines. The Uniform
Sampling baseline does very poorly because too many samples are wasted attempting to train on
goals that are infeasible or not reachable by the current policy - hence not receiving any learning
signal. If an L2 loss is added to try to guide the learning, the agent falls into a poor local optima
of not moving to avoid further negative rewards. The two other baselines that we compare against
perform better, but still do not surpass the performance of our method. In particular, Asymmetric
Self-play needs to train the goal-generating policy (Alice) at every outer iteration, with an amount
of rollouts equivalent to the ones used to train the goal-reaching policy. This additional burden is
not represented in the plots, being therefore at least half as sample-efficient as the plots indicate.
SAGG-RIAC maintains an ever-growing partition of the goal-space that becomes more and more
biased towards areas that already have more sub-regions, leading to reduced exploration and slowing
down the expansion of the policy’s capabilities. Details of our adaptation of these two methods to
our problem, as well as further study of their failure cases, is provided in the Appendices F.1 and
F.2.
To better understand the efficiency of our method, we analyze the goals generated by our automatic
curriculum. In these Ant navigation experiments, the goal space is two dimensional, allowing us
to study the shift in the probability distribution generated by the Goal GAN (Fig. 3) along with
the improvement of the policy coverage (Fig. 4). We have indicated the difficulty of reaching the
generated goals in Fig. 3. It can be observed in these figures that the location of the generated goals
shifts to different parts of the maze, concentrating on the area where the current policy is receiving
some learning signal but needs more improvement. The percentage of generated goals that are at the
appropriate level of difficulty (“good goals”) stays around 20% even as the policy improves. The
6
Under review as a conference paper at ICLR 2018
(a) Free Ant - Baselines
(b) Maze Ant - Baselines
(c) Free Ant - Variants	(d) Maze Ant - Variants
Figure 2: Learning curves comparing the training efficiency of our Goal GAN method and different
baselines (first row) and variants (second row), for the Free Ant (left column) and the Maze Ant
(right column). The y-axis indicates the average return over all feasible goals. The x-axis shows the
number of times that new goals have been sampled. All plots average over 10 random seeds.
goals in these figures include a mix of newly generated goals from the Goal GAN as well as goals
from previous iterations that we use to prevent our policy from “forgetting” (Appendix A.1). Overall
it is clear that our Goal GAN dynamically shift to sample goals of the appropriate difficulty. See
Appendix D for additional experiments.
☆ Start position
• ∙ ∙ Low rewards
• ∙ ∙ High rewards
• ∙ ∙ Good goals
(a) Iteration 5
(b) Iteration 90
(c) Iterartion 350
Figure 3: Goals that our algorithm trains on (200 sampled from the Goal GAN, 100 from the replay).
“High rewards” (green) are goals With RRg (πi) ≥ Rmax; “Good goals” (blue) have appropriate
difficulty for the current policy Rmin ≤ RRg (πi) ≤ RmaX. The red ones have Rmin ≥ Rg (πi)
同目■;
-2	0	2	4	6	-2	0	2	4	6	-2	0	2	4	6
°"	* Start position
(a)	Iteration 5:
Coverage = 0.20
(b)	Iteration 90:
Coverage = 0.48
(c)	Iteration 350:
Coverage = 0.71
Figure 4: Visualization of the policy performance (same policy training as in Fig. 3). For illustration
purposes, the feasible goal-space (i.e. the space within the maze) is divided into a grid, and a goal
location is selected from the center of each grid cell. Each grid cell is colored according to the
expected return achieved on this goal: Red indicates 100% success; blue indicates 0% success.
7
Under review as a conference paper at ICLR 2018
It is interesting to analyze the importance of generating “good goals” for efficient learning. This
is done in Figs. 2c-2d, where we first show an ablation of our method GAN fit all, that disregards
the labels. This method performs worse than ours, because the expansion of the goals is not related
to the current performance of the policy. Finally, we study the Rejection Sampling oracle. As
explained in Section 4.1, we wish to sample from the set of “good” goals Gi, which we approximate
by fitting a Goal GAN to the distribution of good goals observed in the previous policy optimization
step. We evaluate now how much this approximation affects learning by comparing the learning
performance of our Goal GAN to a policy trained on goals sampled uniformly from Gi by using
rejection sampling. This method is orders of magnitude more sample inefficient, but gives us an
upper bound on the performance of our method. Figs. 2c-2d demonstrate that our performance is
quite close to the performance of this much less efficient baseline.
5.2 N-dimensional Point Mass
Figure 5: Final goal coverage obtained after 200
outer iterations on the N-dim point mass environ-
ment. All plots average over 5 random seeds.
In most real-world RL problems, the set of fea-
sible states is a lower-dimensional subset of the
full state space, defined by the constraints of
the environment. For example, the kinematic
constraints of a robot limit the set of feasible
states that the robot can reach. Therefore, uni-
formly sampling goals from the full state-space
would yield very few achievable goals. In this
section we use an N-dimensional Point Mass to
explore this issue and demonstrate the perfor-
mance of our method as the embedding dimen-
sion increases.
In our experiments, the full state-space of the
N -dimensional Point Mass is the hypercube
[-5, 5]N . However, the Point Mass can only
move within a small subset of this state space.
In the two-dimensional case, the set of feasi-
ble states corresponds to the [-5, 5] × [-1, 1] rectangle, making up 20% of the full space. For
N > 2, the feasible space is the Cartesian product of this 2D strip with [-, ]N-2, where = 0.3.
In this higher-dimensional environment, our agent receives a reward of 1 when it moves within
CN = 0.3 √NN of the goal state, to account for the increase in average L2 distance between points in
higher dimensions. The ratio of the volume of the embedded space to the volume of the full state
space decreases as N increases, down to 0.00023:1 for 6 dimensions.
Fig. 5 shows the performance of our method compared to the other methods, as the number of di-
mensions increases. The uniform sampling baseline has very poor performance as the number of
dimensions increases because the fraction of feasible states within the full state space decreases as
the dimension increases. Thus, sampling uniformly results in sampling an increasing percentage of
unfeasible states, leading to poor learning signal. In contrast, the performance of our method does
not decay as much as the state space dimension increases, because our Goal GAN always generates
goals within the feasible portion of the state space (and at the appropriate level of difficulty). The
GAN fit all variation of our method suffers from the increase in dimension because it is not encour-
aged to track the narrow feasible region. Finally, the oracle and the baseline with an L2 distance
reward have perfect performance, which is expected in this simple task where the optimal policy
is just to go in a straight line towards the goal. Even without this prior knowledge, the Goal GAN
discovers the feasible subset of the goal space.
6 Conclusions and Future Work
We propose a new paradigm in RL where the objective is to train a single policy to succeed on a
variety of goals, under sparse rewards. To solve this problem we develop a method for automatic
curriculum generation that dynamically adapts to the current performance of the agent. The curricu-
lum is obtained without any prior knowledge of the environment or of the tasks being performed.
We use generative adversarial training to automatically generate goals for our policy that are always
8
Under review as a conference paper at ICLR 2018
at the appropriate level of difficulty (i.e. not too hard and not too easy). In the future we want to
combine our goal-proposing strategy with recent multi-goal approaches like HER (Andrychowicz
et al., 2017) that could greatly benefit from better ways to select the next goal to train on. Another
promising line of research is to build hierarchy on top of the multi-task policy that we obtain with
our method by training a higher-level policy that outputs the goal for the lower level multi-task pol-
icy (like in Heess et al. (2016) or in Florensa et al. (2017a)). The hierarchy could also be introduced
by replacing our current feed-forward neural network policy by an architecture that learns to build
implicit plans (Mnih et al., 2016; Tamar et al., 2016), or by leveraging expert demonstrations to
extract sub-goals (Zheng et al., 2016), although none of these approaches tackles yet the multi-task
learning problem formulated in this work.
References
Josh Achiam and Shankar Sastry. Surprise-based intrinsic motivation for deep reinforcement learn-
ing. In 2016 Deep Reinforcement Learning Workshop at NIPS, 2016.
Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob
McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. July
2017.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein GAN. January 2017.
Gianluca Baldassarre and Marco Mirolli. Temporal-difference competence-based intrinsic motiva-
tion (td-cb-im): A mechanism that uses the td-error as an intrinsic reinforcement for deciding
which skill to learn when. Intrinsically Motivated Learning in Natural and Artificial Systems, pp.
255-276, 2012.
Adrien Baranes and Pierre-Yves Oudeyer. Intrinsically motivated goal exploration for active motor
learning in robots: A case study. In Intelligent Robots and Systems (IROS), 2010 IEEE/RSJ
International Conference on, pp. 1766-1773. IEEE, 2010.
Adrien Baranes and Pierre-Yves Oudeyer. Active learning of inverse models with intrinsically mo-
tivated goal exploration in robots. Robotics and Autonomous Systems, 61(1):49-73, 2013a.
Adrien Baranes and Pierre-Yves Oudeyer. Active learning of inverse models with intrinsically mo-
tivated goal exploration in robots. Robotics and Autonomous Systems, 61(1), 2013b.
Marc G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi
Munos. Unifying count-based exploration and intrinsic motivation. Advances in Neural Informa-
tion Processing Systems, 2016.
Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence
prediction with recurrent neural networks. In Advances in Neural Information Processing Sys-
tems, 2015.
Yoshua Bengio, Jerome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In
International Conference on Machine Learning, pp. 41-48. ACM, 2009.
Marc Peter Deisenroth, Gerhard Neumann, Jan Peters, et al. A survey on policy search for robotics.
Foundations and Trends in Robotics, 2(1-2):1-142, 2013.
Marc Peter Deisenroth, Peter Englert, Jan Peters, and Dieter Fox. Multi-task policy search for
robotics. In Robotics and Automation (ICRA), 2014 IEEE International Conference on, pp. 3876-
3881. IEEE, 2014.
Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep
reinforcement learning for continuous control. International Conference on Machine Learning,
2016.
Alexander Fabisch and Jan Hendrik Metzen. Active contextual policy search. Journal of Machine
Learning Research, 15(1):3371-3399, 2014.
9
Under review as a conference paper at ICLR 2018
Carlos Florensa, Yan Duan, and Pieter Abbeel. Stochastic neural networks for hierarchical rein-
forcement learning. In International Conference in Learning Representations, 2017a.
Carlos Florensa, David Held, Markus Wulfmeier, and Pieter Abbeel. Reverse curriculum generation
for reinforcement learning. Conference on Robot Learning (CoRL), 2017b.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial networks. In Advances in neural
information processing systems,pp. 2672-2680, 2014.
Alex Graves, Marc G Bellemare, Jacob Menick, Remi Munos, and Koray Kavukcuoglu. Automated
Curriculum Learning for Neural Networks. arXiv preprint, arXiv:1704.03003, 2017.
Xiaoxiao Guo, Satinder Singh, Richard Lewis, and Honglak Lee. Deep learning for reward de-
sign to improve monte carlo tree search in atari games. Conference on Uncertainty in Artificial
Intelligence (UAI), 2016.
Nicolas Heess, Greg Wayne, Yuval Tassa, Timothy Lillicrap, Martin Riedmiller, and David Silver.
Learning and transfer of modulated locomotor controllers. 2016.
Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Variational
information maximizing exploration. Advances in Neural Information Processing Systems, 2016.
Lu Jiang, Deyu Meng, Qian Zhao, Shiguang Shan, and Alexander G Hauptmann. Self-paced cur-
riculum learning. In AAAI, volume 2, pp. 6, 2015.
Andrej Karpathy and Michiel Van De Panne. Curriculum learning for motor skills. In Canadian
Conference on Artificial Intelligence, pp. 325-330. Springer, 2012.
M. P. Kumar, Benjamin Packer, and Daphne Koller. Self-paced learning for latent variable models.
In Advances in Neural Information Processing Systems, pp. 1189-1197, 2010.
Andras Gabor Kupcsik, Marc Peter Deisenroth, Jan Peters, Gerhard Neumann, and Others. Data-
efficient generalization of robot skills with contextual policy search. In AAAI Conference on
Artificial Intelligence, pp. 1401-1407, 2013.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuo-
motor policies. Journal of Machine Learning Research, 17(39):1-40, 2016.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. 2015.
Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley.
Least squares generative adversarial networks. 2016.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.
Volodymyr Mnih, John Agapiou, Simon Osindero, Alex Graves, Oriol Vinyals, Koray Kavukcuoglu,
et al. Strategic attentive writer for learning macro-actions. 2016.
Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approxima-
tors. In International Conference on Machine Learning, pp. 1312-1320, 2015.
Jurgen Schmidhuber. Curious model-building control systems. In International Joint Conference
on Neural Networks, pp. 1458-1463. IEEE, 1991.
JUrgen Schmidhuber. Formal theory of creativity, fun, and intrinsic motivation (1990-2010). IEEE
Transactions on Autonomous Mental Development, 2(3):230-247, 2010.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning, pp. 1889-1897, 2015a.
10
Under review as a conference paper at ICLR 2018
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-
Dimensional continuous control using generalized advantage estimation. In International Con-
ference on Learning Representation, 2015b.
Sahil Sharma and Balaraman Ravindran. Online Multi-Task Learning Using Biased Sampling. arXiv
preprint arXiv: 1702.06053, 2017.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. Nature, 529(7587):484—489, 2016.
Sainbayar Sukhbaatar, Ilya Kostrikov, Arthur Szlam, and Rob Fergus. Intrinsic Motivation and
Automatic Curricula via Asymmetric Self-Play. arXiv preprint, arXiv: 1703.05407, 2017.
Aviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine, and Pieter Abbeel. Value iteration networks.
February 2016.
Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen, Yan Duan, John Schulman,
Filip De Turck, and Pieter Abbeel. #exploration: A study of count-based exploration for deep
reinforcement learning. CoRR, abs/1611.04717, 2016.
Yichuan Tang and Ruslan Salakhutdinov. Learning Stochastic Feedforward Neural Networks. Nips,
2:530-538, 2013.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In IEEE/RSJ International Conference on Intelligent Robots and Systems, 2012.
Wojciech Zaremba and Ilya Sutskever. Learning to execute. CoRR, abs/1410.4615, 2014.
Stephan Zheng, Yisong Yue, and Jennifer Hobbs. Generating long-term trajectories using deep
hierarchical networks. In D D Lee, M Sugiyama, U V Luxburg, I Guyon, and R Garnett (eds.),
Advances in Neural Information Processing Systems 29, pp. 1543-1551. Curran Associates, Inc.,
2016.
11
Under review as a conference paper at ICLR 2018
A	Implementation details
A. 1 Replay buffer
In addition to training our policy on the goals that were generated in the current iteration, we also
save a list (“regularized replay buffer”) of goals that were generated during previous iterations
(Update_replay). These goals are also used to train our policy, so that our policy does not
forget how to achieve goals that it has previously learned. When we generate goals for our policy to
train on, we sample two thirds of the goals from the Goal GAN and we sample the one third of the
goals uniformly from the replay buffer. To prevent the replay buffer from concentrating in a small
portion of goal space, we only insert new goals that are further away than from the goals already
in the buffer, where we chose the goal-space metric and to be the same as the ones introduced in
Section 3.1.
A.2 Goal GAN Initialization
In order to begin our training procedure, we need to initialize our goal generator to produce an initial
set of goals (initialize_GAN). If We initialize the goal generator randomly (or if We initialize it
to sample uniformly from the goal space), it is likely that, for most (or all) of the sampled goals, our
initial policy Would receives no reWard due to the sparsity of the reWard function. Thus We might
have that all of our initial goals g have Rg (∏o) < Rmin, leading to very slow training.
To avoid this problem, We initialize our goal generator to output a set of goals that our initial policy
is likely to be able to achieve with Rg (∏i) ≥ Rmin . To accomplish this, we run our initial policy
π0(at | st, g) With goals sampled uniformly from the goal space. We then observe the set of states
Sv that are visited by our initial policy. These are states that can be easily achieved with the initial
policy, π0, so the goals corresponding to such states will likely be contained within S0I . We then
train the goal generator to produce goals that match the state-visitation distribution pv (g), defined as
the uniform distribution over the set f(Sv). We can achieve this through traditional GAN training,
with pdata (g) = pv (g). This initialization of the generator allows us to bootstrap the Goal GAN
training process, and our policy is able to quickly improve its performance.
B Experimental details
B.1	Ant specifications
The ant is a quadruped with 8 actuated joints, 2 for each leg. The environment is implemented in
Mujoco (Todorov et al., 2012). Besides the coordinates of the center of mass, the joint angles and
joint velocities are also included in the observation of the agent. The high degrees of freedom make
navigation a quite complex task requiring motor coordination. More details can be found in Duan
et al. (2016), and the only difference is that in our goal-oriented version of the Ant we append the
observation with the goal, the vector from the CoM to the goal and the distance to the goal. For the
Free Ant experiments the objective is to reach any point in the square [-5m, 5m]2 on command.
The maximum time-steps given to reach the current goal are 500.
B.2	Ant Maze Environment
The agent is constrained to move within the maze environment, which has dimensions of 6m x 6m.
The full state-space has an area of size 10 m x 10 m, within which the maze is centered. To compute
the coverage objective, goals are sampled from within the maze according to a uniform grid on the
maze interior. The maximum time-steps given to reach the current goal are 500.
B.3	Point-mass specifications
For the N-dim point mass of Section 5.2, in each episode (rollout) the point-mass has 400 timesteps
to reach the goal, where each timestep is 0.02 seconds. The agent can accelerate in up to a rate of 5
m/s2 in each dimension (N = 2 for the maze). The observations of the agent are 2N dimensional,
including position and velocity of the point-mass.
12
Under review as a conference paper at ICLR 2018
(a) Point-mass 2D
(b) Point-mass 3D
Figure 6: Representation of the point-mass experiments with N = 2 and N = 3. The points
contained within the blue boundaries are the feasible goals, corresponding to the positions that can
be reached.
B.4	Goal GAN design and training
After the generator generates goals, we add noise to each dimension of the goal sampled from a
normal distribution with zero mean and unit variance. At each step of the algorithm, we train the
policy for5 iterations, each of which consists of 100 episodes. After 5 policy iterations, we then train
the GAN for 200 iterations, each of which consists of 1 iteration of training the discriminator and 1
iteration of training the generator. The generator receives as input 4 dimensional noise sampled from
the standard normal distribution. The goal generator consists of two hidden layers with 128 nodes,
and the goal discriminator consists of two hidden layers with 256 nodes, with relu nonlinearities.
B.5	Policy and optimization
The policy is defined by a neural network which receives as input the goal appended to the agent
observations described above. The inputs are sent to two hidden layers of size 32 with tanh non-
linearities. The final hidden layer is followed by a linear N -dimensional output, corresponding to
accelerations in the N dimensions. For policy optimization, we use a discount factor of 0.998 and
a GAE lambda of 0.995. The policy is trained with TRPO with Generalized Advantage Estimation
implemented in rllab (Schulman et al., 2015a;b; Duan et al., 2016). Every ”update_policy” consists
of 5 iterations of this algorithm.
C S tudy of GoalGAN “good” goals
To label a given goal (Section 4.1), we could empirically estimate the expected return for this goal
Rg(πi) by performing rollouts of our current policy πi. The label for this goal is then set to yg =
l{Rmin ≤ Rg(∏i) ≤ Rmax o
Nevertheless, having to execute additional rollouts just for labeling
is not sample efficient. Therefore, we instead use the rollouts that were used for the most recent
policy update. This is an approximation as the rollouts where performed under πi-1, but as we show
in Figs. 7a-7b, this small “delay” does not affect learning significantly. Indeed, using the true label
(estimated with three new rollouts from πi) yields the Goal GAN true label curves that are only
slightly better than what our method does.
In the same plots we also study another definition of “good” goals that has been previously used in
the literature: learning progress (Baranes & Oudeyer, 2013b; Graves et al., 2017). Given that we
work in a continuous goal-space, estimating the learning progress of a single goal requires estimat-
ing the performance of the policy on that goal before the policy update and after the policy update
(potentially being able to replace one of these estimations with the rollouts from the policy optimiza-
tion, but not both). Therefore the method does require more samples, but we deemed interesting to
compare how well the metric to automatically build a curriculum. We see in the Figs. 7a-7b that
the two metrics yield a very similar learning, at least in the case of Ant navigation tasks with sparse
rewards.
13
Under review as a conference paper at ICLR 2018
一 Goal GAN (ours)
— Goal GAN true label
o.8 -—— Goal GAN progress
——Goal GAN (ours)
— Goal GAN true label
o.8 .——Goal GAN progress
Outer Iterations
Outer Iterations
(a) Free Ant - Variants	(b) Maze Ant - Variants
Figure 7: Learning curves comparing the training efficiency of our method and different variants.
All plots are an average over 10 random seeds.
D Goal Generation for Free Ant
Similar to the experiments in Figures 3 and 4, here we show the goals that were generated for the
Free Ant experiment in which a robotic quadruped must learn to move to all points in free space.
Figures 8 and 9 show the results. As shown, our method produces a growing circle around the
origin; as the policy learns to move the ant to nearby points, the generator learns to generate goals
at increasingly distant positions.
☆ Start position
• ∙ ∙ Low rewards
• ∙ ∙ High rewards
• ∙ ∙ Good goals
(a) Iteration 10
(b) Iteration 10 0
(c) Iterartion 300
Figure 8: Goals that our algorithm trains on (200 sampled from the Goal GAN, 100 from the replay).
“High rewards” (green) are goals with Rg (πi) ≥ Rmax; “Good goals” (blue) have appropriate
difficulty for the current policy Rmin ≤ Rg (πi) ≤ RmaX. The red ones have Rmin ≥ Rg (πi)
(a) Iteration 10:
Coverage = 0.037
(b) Iteration 100:
Coverage = 0.4
Start position
(c) Iteration 300:
Coverage = 0.86
Figure 9: Visualization of the policy performance for different parts of the state space (same policy
training as in Fig. 8). For illustration purposes, the feasible state-space is divided into a grid, and a
goal location is selected from the center of each grid cell. Each grid cell is colored according to the
expected return achieved on this goal: Red indicates 100% success; blue indicates 0% success.
E	Multi-path point-mass maze
In this section we show that our Goal GAN method is efficient at tracking clearly multi-modal
distributions of good goals. To this end, we introduce a new maze environment with multiple paths,
14
Under review as a conference paper at ICLR 2018
as can be seen in Fig. 10. To keep the experiment simple we replace the Ant agent by a point-mass
environment (in orange), which actions are directly the velocity vector (2 dim). As in the other
experiments, our aim is to learn a policy that can reach any feasible goal corresponding to -balls in
state space like the one depicted in red.
Similar to the experiments in Figures 3 and 4, here we show the
goals that were generated for the Mutli-path point-mass maze ex-
periment. Figures 11 and 12 show the results. It can be observed
that our method produces a multi-modal distribution over goals,
tracking all the areas where goals are at the appropriate level of
difficulty. Note that the samples from the regularized replay buffer
are responsible for the trailing spread of “High Reward” goals and
the Goal GAN is responsible for the more concentrated nodes, as
can be seen in Fig. 13. A clear benefit of using our Goal GAN as a
generative model is that no prior knowledge about the distribution
Figure 10: Multi-path Point-
mass Maze
to fit is required (like the number of modes). Finally, note that the fact of having several possible
paths to reach a specific goal does not hinder the learning of our algorithm that consistently reaches
full coverage in this problem as seen in Fig. 14.
☆ Start position
► ∙ ∙ Low rewards
, ∙ ∙ High rewards
, ∙ ♦ Good goals
(a) Iteration 1
(b) Iteration 10
(c) Iteration 30
(d) Iteration 100
Figure 11: Goals that our algorithm trains on (200 sampled from the Goal GAN, 100 from the re-
play). “High rewards” (green) are goals with Rg(πi) ≥ Rmax; “Good goals” (blue) have appropriate
difficulty for the current policy Rmin ≤ Rg (πi) ≤ RmaX. The red ones have Rmin ≥ Rg (πi)
(a) Iteration 1:
Coverage = 0.014
(b) Iteration 10:	(c) Iteration 30:	(d) Iteration 100:
Coverage = 0.53	Coverage = 0.78	Coverage = 0.98
Figure 12: Visualization of the policy performance for different parts of the state space (same policy
training as in Fig. 8). For illustration purposes, the feasible state-space is divided into a grid, and a
goal location is selected from the center of each grid cell. Each grid cell is colored according to the
expected return achieved on this goal: Red indicates 100% success; blue indicates 0% success.
Figure 13: Iteration 10 Goal
GAN samples (Fig. 11b with-
out replay samples)
Figure 14: Learning curves of our algorithm on Multi-path Point-
mass Maze, consistently achieving full coverage
15
Under review as a conference paper at ICLR 2018
F Comparisons with other methods
F.1 Asymmetric self-play (Sukhbaatar et al., 2017)
Although not specifically designed for the problem presented in this paper, it is straight forward to
apply the method proposed by Sukhbaatar et al. (2017) to our problem. An interesting study of its
limitations in a similar setting can be found in (Florensa et al., 2017b).
F.2 SAGG-RIAC (BARANES & OUDEYER, 2013B)
In our implementation of this method, we use TRPO as the “Low-Level Goal-Directed Exploration
with Evolving Context”. We therefore implement the method as batch: at every iteration, we sample
Nnew new goals {yi}i=0...Nnew , then we collect rollouts of tmax steps trying to reach them, and
perform the optimization of the parameters using all the collected data. The detailed algorithm is
given in the following pseudo-code.
Algorithm 2: Generative Goal with Sagg-RIAC
Hyperparameters: window size Z, tolerance threshold ∈maχ, competence threshold EC,
maximum time horizon tmax, number of new goals Nnew, maximum number of goals gmax,
mode proportions (p1 , p2 , p3);
Input : Policy πθ0 (sstart, yg), goal bounds BY , reset position srest
Output: Policy πθN (sstart, yg)
R — {(Ro, Γro)} where Ro = Region(BY), Γr0 = 0;
for i J 1 to N do
goals J Self-generate Nnew goals： {yj}j=o…Nnew ；
paths = [ ];
while number_steps_in(paths) < batch_Size do
Reset s0 J srest;
yg J Uniform(goals);
yf, Γyg, path J Collect.rollout(∏θi(∙,yg), Sreset)；
paths.append(path);
UpdateRegions(R, yf, 0) ;
UpdateRegions(R, yg, Γyg);
end
πθi+1 J train πθi with TRPO on collected pathS;
end
UpdateRegions(R, yf , Γyf ) is exactly the Algorithm 2 described in the original paper, and Self-
generate is the ”Active Goal Self-Generation (high-level)” also described in the paper (Section
2.4.4 and Algorithm 1), but it’s repeated Nnew times to produce a batch of Nnew goals jointly.
As for the competence Γyg , we use the same formula as in their section 2.4.1 (use highest compe-
tence if reached close enough to the goal) and C(yg, yf) is computed with their equation (7). The
collect-rollout function resets the state so = Sreset and then applies actions following the
goal-conditioned policy ∏θ(∙,yg) until it reaches the goal or the maximum number of steps tmax has
been taken. The final state, transformed in goal space, yf is returned.
As hyperparameters, we have used the recommended ones in the paper, when available： p1 =
0.7, p2 = 0.2, p3 = 0.1. For the rest, the best performance in an hyperparameter sweep yields：
ζ = 100, gmax = 100. The noise for mode(3) is chosen to be Gaussian with variance 0.1, the same
as the tolerance threshold Emax and the competence threshold EC.
As other details, in our tasks there are no constraints to penalize for, so P = 0. Also, there are no
sub-goals. The reset value r is 1 as we reset to sstart after every reaching attempt. The number of
explorative movements q ∈ N has a less clear equivalence as we use a policy gradient update with a
stochastic policy πθ instead of a SSA-type algorithm.
16
Under review as a conference paper at ICLR 2018
(a) Iteration 2	(b) Iteration 20	(c) Iterartion 300
Figure 15: Goals sampled by SAGG-RIAC (same policy training as in Fig. 16). “High rewards” (in
green) are goals with Rg (πi) ≥ Rmax; “Good goals”(in blue) are those with the appropriate level
of difficulty for the current policy (Rmin ≤ Rg(πi) ≤ Rmax). The red ones have Rmin ≥ Rg (πi)
(a) Iteration 2:
Num. of Regions = 54
(b) Iteration 100:
Num. of Regions = 1442
(c) Iteration 300:
Num. of Regions = 15420
Figure 16: Visualization of the regions generated by the SAGG-RIAC algorithm
17