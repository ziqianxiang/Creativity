Under review as a conference paper at ICLR 2018
Stabilizing Gradients for Deep Neural Net-
works via Efficient SVD Parameterization
Anonymous authors
Paper under double-blind review
Ab stract
Vanishing and exploding gradients are two of the main obstacles in training deep
neural networks, especially in capturing long range dependencies in recurrent neu-
ral networks (RNNs). In this paper, we present an efficient parametrization of the
transition matrix of an RNN that allows us to stabilize the gradients that arise in its
training. Specifically, we parameterize the transition matrix by its singular value
decomposition (SVD), which allows us to explicitly track and control its singular
values. We attain efficiency by using tools that are common in numerical linear
algebra, namely Householder reflectors for representing the orthogonal matrices
that arise in the SVD. By explicitly controlling the singular values, our proposed
svdRNN method allows us to easily solve the exploding gradient problem and we
observe that it empirically solves the vanishing gradient issue to a large extent. We
note that the SVD parameterization can be used for any rectangular weight matrix,
hence it can be easily extended to any deep neural network, such as a multi-layer
perceptron. Theoretically, we demonstrate that our parameterization does not lose
any expressive power, and show how it potentially makes the optimization pro-
cess easier. Our extensive experimental results also demonstrate that the proposed
framework converges faster, and has good generalization, especially in capturing
long range dependencies, as shown on the synthetic addition and copying tasks.
1	Introduction
Deep neural networks have achieved great success in various fields, including computer vision,
speech recognition, natural language processing, etc. Despite their tremendous capacity to fit com-
plex functions, optimizing deep neural networks remains a contemporary challenge. Two main
obstacles are vanishing and exploding gradients, that become particularly problematic in Recurrent
Neural Networks (RNNs) since the transition matrix is identical at each layer, and any slight change
to it is amplified through recurrent layers (Bengio et al. (1994)).
Several methods have been proposed to solve the issue, for example, Long Short Term Mem-
ory (LSTM) (Hochreiter & Schmidhuber (1997)) and residual networks (He et al. (2016)). Another
recently proposed class of methods is designed to enforce orthogonality of the square transition
matrices, such as unitary and orthogonal RNNs (oRNN) (Arjovsky et al. (2016); Mhammedi et al.
(2017)). However, while these methods solve the exploding gradient problem, they limit the expres-
sivity of the network.
In this paper, we present an efficient parametrization of weight matrices that arise in a deep neural
network, thus allowing us to stabilize the gradients that arise in its training, while retaining the
desired expressive power of the network. In more detail we make the following contributions:
•	We propose a method to parameterize weight matrices through their singular value decom-
position (SVD). Inspired by (Mhammedi et al. (2017)), we attain efficiency by using tools
that are common in numerical linear algebra, namely Householder reflectors for represent-
ing the orthogonal matrices that arise in the SVD. The SVD parametrization allows us to
retain the desired expressive power of the network, while enabling us to explicitly track and
control singular values.
•	We apply our SVD parameterization to recurrent neural networks to exert spectral con-
straints on the RNN transition matrix. Our proposed svdRNN method enjoys similar space
and time complexity as the vanilla RNN. We empirically verify the superiority of svdRNN
over RNN/oRNN, in some case even LSTMs, over an exhaustive collection of time se-
ries classification tasks and the synthetic addition and copying tasks, especially when the
network depth is large.
1
Under review as a conference paper at ICLR 2018
•	Theoretically, we show how our proposed SVD parametrization can make the optimization
process easier. Specifically, under a simple setting, we show that there are no spurious local
minimum for the linear svdRNN in the population risk.
•	Our parameterization is general enough to eliminate the gradient vanishing/exploding prob-
lem not only in RNNs, but also in various deep networks. We illustrate this by applying
SVD parametrization to problems with non-square weight matrices, specifically multi-layer
perceptrons (MLPs) and residual networks.
We now present the outline of our paper. In Section 2, we discuss related work, while in Section 3
we introduce our SVD parametrization and demonstrate how it spans the whole parameter space and
does not limit expressivity. In Section 4 we propose the svdRNN model that is able to efficiently
control and track the singular values of the transition matrices, and we extend our parameterization
to non-square weight matrices and apply it to MLPs in Section 5. Section 6 provides the optimization
landscape of svdRNN by showing that linear svdRNN has no spurious local minimum. Experimental
results on MNIST and a popular time series archive are present in Section 7. Finally, we present our
conclusions and future work in Section 8.
2	Related Work
Numerous approaches have been proposed to address the vanishing and exploding gradient problem.
Long short-term memory (LSTM) (Hochreiter & Schmidhuber (1997)) attempts to address the van-
ishing gradient problem by adding additional memory gates. Residual networks (He et al. (2016))
pass the original input directly to the next layer in addition to the original layer output. Mikolov
(2012) performs gradient clipping, while Pascanu et al. (2013) applies spectral regularization to the
weight matrices. Other approaches include introducing L1 or L2 penalization on successive gradient
norm pairs in back propagation (Pascanu et al. (2013)).
Recently the idea of restricting transition matrices to be orthogonal has drawn some attention. Le
et al. (2015) proposed initializing recurrent transition matrices to be identity or orthogonal (IRNN).
This strategy shows better performance when compared to vanilla RNN and LSTM. However, there
is no guarantee that the transition matrix is close to orthogonal after a few iterations. The uni-
tary RNN (uRNN) algorithm proposed in Arjovsky et al. (2016) parameterizes the transition matrix
with reflection, diagonal and Fourier transform matrices. By construction, uRNN ensures that the
transition matrix is unitary at all times. Although this algorithm performs well on several small
tasks, Wisdom et al. (2016) showed that uRNN only covers a subset of possible unitary matrices and
thus detracts from the expressive power of RNN. An improvement over uRNN, the orthogonal RNN
(oRNN), was proposed by Mhammedi et al. (2017). oRNN uses products of Householder reflectors
to represent an orthogonal transition matrix, which is rich enough to span the entire space of orthog-
onal matrices. Meanwhile, Vorontsov et al. (2017) empirically demonstrate that the strong constraint
of orthogonality limits the model’s expressivity, thereby hindering its performance. Therefore, they
parameterize the transition matrix by its SVD, W = UΣV > (factorized RNN) and restrict Σ to be
in a range close to 1; however, the orthogonal matrices U and V are updated by geodesic gradi-
ent descent using the Cayley transform, thereby resulting in time complexity cubic in the number
of hidden nodes which is prohibitive for large scale problems. Motivated by the shortcomings of
the above methods, our work in this paper attempts to answer the following questions: Is there an
efficient way to solve the gradient vanishing/exploding problem without hurting expressive power?
As brought to wide notice in He et al. (2016), deep neural networks should be able to preserve
features that are already good. Hardt & Ma (2016) consolidate this point by showing that deep
linear residual networks have no spurious local optima. In our work, we broaden this concept and
bring it to the area of recurrent neural networks, showing that each layer is not necessarily near
identity, but being close to orthogonality suffices to get a similar result.
Generalization is a major concern in training deep neural networks. Bartlett et al. (2017) provide
a generalization bound for neural networks by a spectral Lipschitz constant, namely the product of
spectral norm of each layer. Thus, our scheme of restricting the spectral norm of weight matrices
reduces generalization error in the setting of Bartlett et al. (2017). As supported by the analysis in
Cisse et al. (2017), since our SVD parametrization allows us to develop an efficient way to constrain
the weight matrix to be a tight frame (Tropp et al. (2005)), we consequently are able to reduce the
sensitivity of the network to adversarial examples.
2
Under review as a conference paper at ICLR 2018
Hkn(u)
3	SVD parameterization
The SVD of the transition matrix W ∈ Rn×n of an RNN is given by W = UΣV T, where Σ is
the diagonal matrix of singular values, and U, V ∈ Rn×n are orthogonal matrices, i.e., UT U =
UUT = I and V TV = V V T = I (Trefethen & Bau III (1997)). During the training ofan RNN, our
proposal is to maintain the transition matrix in its SVD form. However, in order to do so efficiently,
we need to maintain the orthogonal matrices U and V in compact form, so that they can be easily
updated by forward and backward propagation. In order to do so, as in Mhammedi et al. (2017), we
use a tool that is commonly used in numerical linear algebra, namely Householder reflectors (which,
for example, are used in computing the QR decomposition of a matrix).
Given a vector u ∈ Rk, k ≤ n, the n × n Householder reflector Hkn(u) is defined as:
(nk	> )	, U = 0
Ik	Ik-2kUuk2)	⑴
In	, otherwise.
The Householder reflector is clearly a symmetric matrix, and it can be shown that it is orthogonal,
i.e., H2 = I (Householder (1958)). Further, when u 6= 0, it has n-1 eigenvalues that are 1, and one
eigenvalue which is -1 (hence the name that it is a reflector) . In practice, to store a Householder
reflector, we only need to store u ∈ Rk rather than the full matrix.
Given a series of vectors {ui }in=k where uk ∈ Rk, we define the map:
Mk : Rk × ... × Rn 7→ Rn×n
(uk, ..., un) 7→ Hn(un)...Hk (uk),	(2)
where the right hand side is a product of Householder reflectors, yielding an orthogonal matrix (to
make the notation less cumbersome, we remove the superscript from Hkn for the rest of this section).
Theorem 1.	The image of M1 is the set of all n × n orthogonal matrices.
The proof of Theorem 1 is an easy extension of the Householder QR factorization Theorem, and is
presented in Appendix A. Although we cannot express all n × n matrices with Mk, any W ∈ Rn×n
can be expressed as the product of two orthogonal matrices U, V and a diagonal matrix Σ, i.e. by
its SVD: W = UΣV >. Given σ ∈ Rn and {ui}in=k , {vi}in=k with ui , vi ∈ Ri, we finally define
our proposed SVD parametrization:
Mk1,k2 :Rk1 × ... × Rn × Rk2 × ... × Rn × Rn 7→ Rn×n
(uk1, ..., un, vk2, ..., vn, σ) 7→ Hn(un)...Hk1 (uk1)diag(σ)Hk2 (vk2)...Hn(vn).	(3)
Theorem 2.	The image of M1,1 is the set of n × n real matrices.
i	.e. Rn×n = M1,1 (R1 X ... X Rn X R1 X ... X Rn X Rn)
The proof of Theorem 2 is based on the singular value decomposition and Theorem 1, and is pre-
sented in Appendix A. The astute reader might note that M1,1 seemingly maps an input space of
n2 + 2n dimensions to a space of n2 dimensions; however, since Hkn (uk) is invariant to the norm of
uk, the input space also has exactly n2 dimensions. Although Theorems 1 and 2 are simple exten-
sions of well known linear algebra results, they ensure that our parameterization has the ability to
represent any matrix and so the full expressive power of the RNN is retained.
Theorem 3.	The image ofMk1,k2 includes the set of all orthogonal nXn matrices ifk1+k2 ≤ n+2.
Theorem 3 indicates that if the total number of reflectors is greater than n: (n - k1 + 1) + (n - k2 +
1) ≥ n, then the parameterization covers all orthogonal matrices. Note that when fixing σ = 1,
Mk1,k2 ({ui}in=k , {vi}in=k , 1) ∈ O(n), where O(n) is the set of n X n orthogonal matrices. Thus
when k1 + k2 ≤ n+2, we have O(n) = Mk1,k2 Rk1 X ... X Rn X Rk2 X ... X Rn X 1.
4	svd-RNN
In this section, we apply our SVD parameterization to RNNs and describe the resulting svdRNN
algorithm in detail. Given a hidden state vector from the previous step h(t-1) ∈ Rn and input
x(t-1) ∈ Rni, RNN computes the next hidden state h(t) and output vector o(t) ∈ Rno as:
h(t) = σ(Wh(t-1) + Mx(t-1) +b)	(4)
o(t) = Y h(t)	(5)
3
Under review as a conference paper at ICLR 2018
In svdRNN we parametrize the transition matrix W ∈ Rn×n using m1 + m2 Householder reflectors
as:
W = Mn-m1 +1,n-m2 +1 (un-m1 +1 , ..., un, vn-m2 +1 , ..., vn, σ)
Hn (un)...Hn-m1
+1 (un-m1+1)diag(σ)Hn
-m2 +1 (vn-m2 +1)...Hn (vn)
(6)
(7)
This parameterization gives us several advantages over the regular RNN. First, we can select the
number of reflectors m1 and m2 to balance expressive power versus time and space complexity. By
Theorem 2, the choice m1 = m2 = n gives us the same expressive power as vanilla RNN. Notice
oRNN could be considered a special case of our parametrization, since when we set m1 + m2 ≥ n
and σ = 1, we can represent all orthogonal matrices, as proven by Theorem 3. Most importantly,
we are able to explicitly control the singular values of the transition matrix. In most cases, we want
to constrain the singular values to be within a small interval near 1. The most intuitive method is to
clip the singular values that are out of range. Another approach would be to initialize all singular
values to 1, and add a penalty term kσ - 1k2 to the objective function. Here, we have applied
another parameterization of σ proposed in Vorontsov et al. (2017):
σi = 2r(f (σi) - 0.5) + σ*, i ∈ [n]	(8)
where f is the sigmoid function and &% is updated from u%, Vi via stochastic gradient descent. The
above allows us to constrain σi to be within [σ* 一 r, σ* + r]. In practice, σ* is usually set to 1 and
r 1. Note that we are not incurring more computation cost or memory for the parameterization.
For regular RNN, the number of parameters is (no + ni + n + 1)n, while for svdRNN it is (no +
m2 +m2 m m
n + mi + m2 + 2)n---------1---2≡——~2. In the extreme case where mi = m? = n, it becomes
(no + ni + n + 3)n. Later we will show that the computational cost of svdRNN is also of the same
order as RNN in the worst case.
4.1	Forward/backward propagation
In forward propagation, we need to iteratively evaluate h(t) from t = 0 to L using (4). The only
different aspect from a regular RNN in the forward propagation is the computation of Wh(t-i).
Note that in svdRNN, W is expressed as product of mi + m2 Householder matrices and a diagonal
matrix.Thus W h(t-i) can be computed iteratively using (mi + m2 ) inner products and vector
additions. Denoting Uk = (0u-k), we have:
Hk(Uk)h = (In ― 2ukuk ) h = h ― 2ukh-Uk	(9)
∖	ukuk J	UkUk
Thus, the total cost of computing W h(t-i) is O((mi + m2 )n) floating point operations (flops).
Detailed analysis can be found in Section 4.2. Let L({Ui}, {vi}, σ, M, Y, b) be the loss or objective
function, C(t)	w Wh(t), Σ = diag(σ). Given般t) ∂L _ " ∂C(t) #> ∂L =fcτ];				, we define: dL := 硬:=	∂C(t) #> ∂L 可]∂C(t);	(10)
	∂L _ ∂∑(t) :=	'∂C(t)' .∂∑(t).	>	∂L ∂C(t);	∂L _ Γ ∂Σ(t) :=[	∂∑(t) ]> ∂L ∂Σ(t) J	∂Σ(t);	(11)
	∂L	∂C(t)	-	> ∂L			(12)
	∂h(tτ) :=	.∂h(tτ).		∂C(t)			
Back propagation for SvdRNN requires d(t), dC(t), ∂C(t) and 贫t∖. These partial gradients
∂uk	∂vk	∂Σ(t)	∂h -
can also be computed iteratively by computing the gradient of each Householder matrix at a time.
∂L
We drop the superscript (t) now for ease ofexposιtιon. Given h = Hk (Uk )h and g =赤,we have
∂L ——= ∂h	■ _ ʌ - ∂ h ∂h	>	∂L —— ∂h	= In	2Uku> ) 〃 — 2	2 U>g ʌ ― -Σ>7_	g = g - 2 入> > Uk Uk Uk )	Uk Uk		(13)
∂L 		= ∂Uk	■ _ ʌ ∂ h ∂Uk	#>	∂L _ ʌ ∂h	=-2I	^ʃ-ln + -ɪ hU> +，^> h 2 UkU >n	>	k	> <Uk Uk	UkUk	(UkUk)2	k> g	(14)
	Q U>h -2 τ>τ- Uk Uk		-g —	2 Ukg 2 -ʌ > 入 Uk Uk	h + 4 Wh FUk >> Uk Uk Uk Uk		(15)
4
Under review as a conference paper at ICLR 2018
Details of forward and backward propagation can be found in Appendix (B). One thing worth notic-
ing is that the oRNN method in Mhammedi et al. (2017) actually omitted the last term in (15) by
assuming that kuk k are fixed. Although the scaling of uk in the Householder transform does not
affect the transform itself, it does produce different gradient update for uk even if it is scaled to norm
1 afterwards.
4.2	Complexity Analysis
Table 1 gives the time complexity of various algorithms. Hprod and Hgrad are defined in Algo-
rithm 2 3 (see Appendix (B)). Algorithm 2 needs 6k flops, while Algorithm 3 uses (3n + 10k) flops.
Since kuk k2 only needs to be computed once per iteration, we can further decrease the flops to 4k
and (3n + 8k). Also, in back propagation we can reuse α in forward propagation to save 2k flops.
flops
Hprod(h, Uk)	4k
H grad(h, uk, g)	3n + 6k
svdRNN-Local FP(n, m1, m2)	4n(m1 + m2) - 2m21 - 2m22 + O(n)
svdRNN-Local BP(n, m1, m2)	6n(m1 + m2) - 1.5m21 - 1.5m22 + O(n)
oRNN-Local FP(n, m)	4nm - m2 + O(n)
ORNN-Local BP(n, m)	7nm - 2m2 + O(n)
Table 1: Time complexity across algorithms
5 Extending SVD Parameterization to General Weight Matrices
In this section, we extend the parameterization to non-square matrices and use Multi-Layer Percep-
trons(MLP) as an example to illustrate its application to general deep networks. For any weight
matrix W ∈ Rm×n (without loss of generality m ≤ n), its reduced SVD can be written as:
W = U (∑∣0)(VL∣VR)> = U ∑V？,	(16)
where U ∈ Rm×m, Σ ∈ diag (Rm),VL ∈ Rn×m. There exist un, ..., uk1 and vn, ..., vk2 s.t. U =
Hmm(um)...Hkm (uk1 ), V = Hnn(vn)...Hkn (vk2), where k1 ∈ [m], k2 ∈ [n]. Thus we can extend the
SVD parameterization for any non-square matrix:
Mkm,nk	:Rk1	× ... ×	Rm	× Rk2	× ... ×	Rn	×	Rmin(m,n) 7→	Rm×n
k1,k2
(Uki ,…JUmjvk2 ,…,Vn,σ) → Hm (Um)…H^ (ukι)Σ H-k2(Vk2 )…Hn Qn)∙ (17)
where Σ = (diag(σ)∣0) f m < n and (diag(σ)∣0) 1 otherwise. Next we show that we only need
2 min(m, n) reflectors (rather than m + n) to parametrize any m × n matrix. By the definition of
Hkn, we have the following lemma:
Lemma 1. Given {vi}in=1, define V (k) = Hnn (vn)...Hkn (vk) fork ∈ [n]. We have:
V(,k1) = V(,k2), ∀kι, k2 ∈ [n], i ≤ min(n - kι,n - k2).
Here V*,i indicates the i th column of matrix V. According to Lemma 1, we only need at most first
m Householder vectors to express VL, which results in the following Theorem:
Theorem 4. Ifm ≤ n, the image of M1m,n,n-m+1 is the set of all m × n matrices; else the image of
Mnm-,nm+1,1 is the set of all m × n matrices.
Similarly if we constrain Ui, vi to have unit length, the input space dimensions of M1m,n,n-m+1 and
Mmm,-nn+1,1 are both mn, which matches the output dimension. Thus we extend Theorem 2 to the
non-square case, which enables us to apply SVD parameterization to not only the RNN transition
matrix, but also to general weight matrices in various deep learning models. For example, the
Multilayer perceptron (MLP) model is a class of feedforward neural network with fully connected
layers:
h(t) = f (W (t-1) h(t-1) +b(t-1))	(18)
Here h(t) ∈ Rnt, h(t-1) ∈ Rnt-1 and W(t) ∈ Rnt ×nt-1. Applying SVD parameterization to W(t)
say nt < nt-1, we have:
(t)	nt	nt	nt-1	nt 1
W	= Hnt (Unt)...H1 (U1)ΣHnt-1-nt+1(vnt-1-nt+1)...Hnt-1 (vnt-1).
We can use the same forward/backward propagation algorithm as described in Algorithm 1. Besides
RNN and MLP, SVD parameterization method also applies to more advanced frameworks, such as
Residual networks and LSTM, which we will not describe in detail here.
5
Under review as a conference paper at ICLR 2018
6 Theoretical Analysis
Since we can control and upper bound the singular values of the transition matrix in svdRNN, we can
clearly eliminate the exploding gradient problem. In this section, we now analytically illustrate the
advantages of svdRNN with lower-bounded singular values from the optimization perspective. For
the theoretical analysis in this section, we will limit ourselves to a linear recurrent neural network,
i.e., an RNN without any activation.
6.1	Representations of RNN without activation
Linear recurrent neural network. For simplicity, we follow a setting similar to Hardt & Ma (2016).
For compact presentation, we stack the input data as X ∈ Rn×t, where X = (x(0)|x⑴|•…∣x(t-1)),
and transition weights as W ∈ Rn×nt where W = (W|W2| …|Wt]. Then we can simplify the
output as:	t
o(t)(X)=Y(Wth(0)+XWi(Mx(t)+b))
i=1
By absorbing M and b in each data x(t) and assuming h(0) = 0, we further simplify the output as:
o(t)(X) = Y XWix(t-1)
i=1
Suppose the input data X 〜 D, and assume its underlying relation to the output is y = Avec(X)+η,
where A ∈ Rn×nt and residue η ∈ Rn satisfies EX〜D [η∣X] = 0.We consider the individual loss:
f(W; X, y) = ko(t)(X)-yk22 = kYWvec(X)-yk22.
Claim 1. With linear recurrent neural networks, the population risk
R[W] = EX〜D [f (W; X, y)] = k(YW - A)∑“kF + C,
where Σ = EX〜D[vec(X)vec(X)>], and C = E[kηk2]. Meanwhile
VWR[W] = (YW - A)Σ (Id|2W|3W2∣ …|tWt-1)>
6.2 All critical points are global minimum
Theorem 5. With linear recurrent neural networks, if transition matrix W satisfies σmin(W) ≥ e >
0, all critical points in the population risk are global minimum.
Proof. Write YW 一 A as (E1∣E21 …|Et), where each Ei ∈ Rd×d. By Claim 1,
kVwR[W]kF = k(YW 一 A)Σ (Id|2W>∣3(W>)2∣ …|t(W>)t-1)> kF
≥ σmm(∑)k(Y W 一 A) (Id|2W >∣3(W >)2∣∙∙∙ |t(W >)t-1)> kF
t
≥ σm2in(Σ)Xi2e2(i-1)kEik2F
i=1
≥ σm2 in(Σ) min {i2e2(i-1)}kY W 一 Ak2F
1≤i≤t
≥ 0mm(∑) Imin ɑ/iT)}(r(w)- r*)
1≤i≤t
Therefore when VW R[W] = 0 suffices R(W) = R*, meaning W reaches the global minimum. □
Theorem 5 potentially explains why our system is easier to optimize, since with our scheme of SVD
parametrization, we have the following corollary.
Corollary 1. With the update rule in (8), linear svdRNNs have no spurious local minimum.
While the above analysis lends further credence to our observed experimental results, we leave it to
future work to perform a similar analysis in the presence of non-linear activation functions.
7 Experimental Results
In this section, we provide empirical evidence that shows the advantages of SVD parameterization
in both RNNs and MLPs. For RNN models, we compare our svdRNN algorithm with (vanilla) RNN,
IRNN(Le et al. (2015)), oRNN(Mhammedi et al. (2017)) and LSTM (Hochreiter & Schmidhuber
(1997)). The transition matrix in IRNN is initialized to be orthogonal while other matrices are
initialized by sampling from a Gaussian distribution. For MLP models, we implemented vanilla
MLP, Residual Network (ResNet)(He et al. (2016)) and used SVD parameterization for both of them.
We used a residual block of two layers in ResNet. In most cases leaky_Relu is used as activation
6
Under review as a conference paper at ICLR 2018
function, except for LSTM, where Ieaky-Relu will drastically harm the performance. To train these
models, we applied Adam optimizer with stochastic gradient descent (Kingma & Ba (2014)). These
models are implemented with Theano (Al-Rfou et al. (2016)).1
7.1	Time Series Classification
In this experiment, we focus on the time series classification problem, where time series are fed
into RNN sequentially, which then tries to predict the right class upon receiving the sequence
end (HUsken & Stagge (2003)). The dataset we choose is the largest public collection of class-
labeled time-series with widely varying length, namely, the UCR time-series collection from Chen
et al. (2015)2. We present the test accuracy on 20 datasets with RNN, LSTM, oRNN and svdRNN
in Table 3(Appendix C) and Figure 1. In all experiments, we used hidden dimension nh = 32, and
chose total number of reflectors for oRNN and svdRNN to be m = 16 (for svdRNN m1 = m2 = 8).
We choose proper depth t as well as input size ni. Given sequence length L, since tni = L, we
choose ni to be the maximum divisor of L that satisfies depth ≤ LL. To have a fair comparison
(a)
(b)
Figure 1: Performance comparisons of the RNN based models on three UCR datasets.
Iteration
(c)
of how the proposed principle itself influences the training procedure, we did not use dropout in any
of these models. As illustrated in the optimization process in Figure 1, this resulted in some overfit-
ting (see (a) CBF), but on the other hand it shows that svdRNN is able to prevent overfitting. This
supports our claim that since generalization is bounded by the spectral norm of the weights Bartlett
et al. (2017), svdRNN will potentially generalize better than other schemes. This phenomenon is
more drastic when the depth is large (e.g. ArrowHead(251 layers) and FaceAll(131 layers)), since
regular RNN, and even LSTM, have no control over the spectral norms. Also note that there are
substantially fewer parameters in oRNN and svdRNN as compared to LSTM.
7.2	MNIST
In this experiment, we compare different models on the MNIST image dataset. The dataset was
split into a training set of 60000 instances and a test set of 10000 instances. The 28 × 28 MNIST
pixels are flattened into a vector and then traversed by the RNN models. Table 2 shows accuracy
scores across multiple We tested different models with different network depth as well as width.
Figure 2(a)(b) shows the test accuracy on networks with 28 and 112 layers (20 and 128 hidden
dimensions) respectively. It can be seen that the svdRNN algorithms have the best performance
and the choice of r (the amount that singular values are allowed to deviate from 1) does not have
much influence on the final precision. Also we explored the effect of different spectral constraints
and explicitly tracked the spectral margin (maxi ∣σi - 1|) of the transition matrix. Intuitively, the
influence of large spectral margin should increase as the network becomes deeper. Figure 2(d)
shows the spectral margin of different RNN models. Although IRNN has small spectral margin at
first few iterations, it quickly deviates from orthogonal and cannot match the performance of oRNN
and SVdRNN. Figure 2(e) shows the magnitude of first layer gradient ∣∣ ∂hL) k2. RNN suffers from
vanishing gradient at first 50k iterations while oRNN and svdRNN are much more stable. Note that
LSTM can perform relatively well even though it has exploding gradient in the first layer.
We also tested RNN and svdRNN with different amount of non-linearity, as shown in Figure 2(c).
This is achieved by adjusting the leak parameter in Ieaky-Relu: f (x) = max(leak ∙ x, x). With
leak = 1.0, it reduces to the identity map and when leak = 0 we are at the original Relu function.
From the figures, we show that svdRNN is resistant to different amount of non-linearity, namely
converge faster and achieve higher accuracy invariant to the amount of the leak factor. To explore
the reason underneath, we illustrate the gradient in Figure 2(f), and find out svdRNN could eliminate
the gradient vanishing problem on all circumstances, while RNN suffers from gradient vanishing
When non-linearity is higher_________
1we thank Mhammedi for providing their code for oRNN(Mhammedi et al. (2017))
2Details of the data sets, including how to split into train/vaildiation/test sets, are given in Appendix C
7
Under review as a conference paper at ICLR 2018
5000 IOOOO 15000 20000 25000 30000
# samples
(b)
50000：LOOOO(15000QOOOO(25000GoOoOO
# samoles
(a)
MNIST, L=28, ∩h = 20
(c)
MNlST L=28 3=20
Figure 2:	RNN models on MNIST
Models	Hidden dimension	Number of parameters	Test accuracy
svdRNN	256(m1, m2 = 16)	≈ 13k	97.6
oRNN(Mhammedi et al. (2017))	256(m = 32)	≈ 11k	97.2
RNN(Vorontsov et al. (2017))	128	≈ 35k	94.1
uRNN(Arjovsky et al. (2016))	512	≈ 16k	95.1
RC uRNN(Wisdom et al. (2016))	512	≈ 16k	97.5
FC uRNN(Wisdom et al. (2016))	116	≈ 16k	92.8
factorized RNN(Vorontsov et al. (2017))	128	≈ 32k	94.6
LSTM (Vorontsov et al. (2017))	128	≈ 64k	97.3
Table 2: Results for the pixel MNIST dataset across multiple algorithms.
For the MLP models, each instance is flattened to a vector of length 784 and fed to the input layer.
After the input layer there are 40 layers with hidden dimension 32 (Figure 3(a)) or 30 to 100 layers
with hidden dimension 128 (Figure 3(b)). On a 40-layer network, svdMLP and svdResNet achieve
similar performance as ResNet while MLP’s convergence is slower. However, when the network is
deeper, both MLP and ResNet start to fail. With nh = 128, MLP is not able to function with L > 35
and ResNet with L > 70. On the other hand, the SVD based methods are resilient to increasing
depth and thus achieve higher precision.
1.00
MNIST, ∏h = 32, L=40
MNISTm1=128
(a)	(b)
Figure 3:	MLP models on MNIST with L layers nh hidden dimension
8	Conclusions
In this paper, we have proposed an efficient SVD parametrization of various weight matrices in
deep neural networks, which allows us to explicitly track and control their singular values. This
parameterization does not restrict the network’s expressive power, while simultaneously allowing
fast forward as well as backward propagation. The method is easy to implement and has the same
time and space complexity as compared to original methods like RNN and MLP. The ability to
control singular values helps in avoiding the gradient vanishing and exploding problems, and as we
have empirically shown, gives good performance. Although we only showed examples in the RNN
and MLP framework, our method is applicable to many more deep networks, such as Convolutional
Networks etc. However, further experimentation is required to fully understand the influence of using
different number of reflectors in our SVD parameterization. Also, the underlying structures of the
image of Mk1,k2 when k1, k2 6= 1 is a subject worth investigating.
8
Under review as a conference paper at ICLR 2018
References
Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau, Nico-
las Ballas, Frederic Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, et al. Theano:
A python framework for fast computation of mathematical expressions. arXiv preprint, 2016.
Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In
International Conference on Machine Learning, pp. 1120-1128, 2016.
Peter Bartlett, Dylan J Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural
networks. arXiv preprint arXiv:1706.08498, 2017.
Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradi-
ent descent is difficult. IEEE transactions on neural networks, 5(2):157-166, 1994.
Yanping Chen, Eamonn Keogh, Bing Hu, Nurjahan Begum, Anthony Bagnall, Abdullah Mueen, and
Gustavo Batista. The UCR time series classification archive, July 2015. www.cs.ucr.edu/
~eamonn/time_series_data/.
Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval
networks: Improving robustness to adversarial examples. In International Conference on Machine
Learning, pp. 854-863, 2017.
Moritz Hardt and Tengyu Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231,
2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
SePP Hochreiter and Jurgen Schmidhuber Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Alston S Householder. Unitary triangularization of a nonsymmetric matrix. Journal of the ACM
(JACM), 5(4):339-342, 1958.
Michael Husken and Peter Stagge. Recurrent neural networksfor time series classification. NeUro-
computing, 50:223-235, 2003.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic oPtimization. arXiv preprint
arXiv:1412.6980, 2014.
Quoc V Le, NavdeeP Jaitly, and Geoffrey E Hinton. A simPle way to initialize recurrent networks of
rectified linear units. arXiv preprint arXiv:1504.00941, 2015.
Zakaria Mhammedi, Andrew Hellicar, Ashfaqur Rahman, and James Bailey. Efficient orthogonal
Parametrisation of recurrent neural networks using Householder reflections. In International
Conference on Machine Learning, PP. 2401-2409, 2017.
TOmas Mikolov. Statistical language models based on neural networks. Presentation at Google,
Mountain View, 2nd April, 2012.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural
networks. In International Conference on Machine Learning, PP. 1310-1318, 2013.
Lloyd N Trefethen and David Bau III. Numerical linear algebra, volume 50. SIAM, 1997.
Joel A TroPP, Inderjit S Dhillon, Robert W Heath, and Thomas Strohmer. Designing structured tight
frames via an alternating Projection method. IEEE Transactions on information theory, 51(1):
188-209, 2005.
Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. On orthogonality and learn-
ing recurrent networks with long term dePendencies. In International Conference on Machine
Learning, PP. 3570-3578, 2017.
Scott Wisdom, Thomas Powers, John Hershey, Jonathan Le Roux, and Les Atlas. Full-caPacity
unitary recurrent neural networks. In Advances in Neural Information Processing Systems, PP.
4880-4888, 2016.
9
Under review as a conference paper at ICLR 2018
Appendix A	Proofs
A.1 Proof of Proposition 1
Proposition 1. (Householder QR factorization) Let B ∈ Rn×n. Then there exists an upper tri-
angular matrix R with positive diagonal elements, and vectors {ui }in=1 with ui ∈ Ri, such that
B = Hnn(un)...H1n(u1)R. (Note that we allow ui = 0, in which case, Hin(ui) = In as in (1))
Proof of Proposition 1. For n = 1, note that H11(u1) = ±1. By setting u1 = 0 if B1,1 > 0 and
u1 6= 0 otherwise, we have the factorization desired.
Assume that the result holds for n = k, then for n = k + 1 set uk+1 = B1 - kB1 ke1. Here B1 is
the first column of B and e1 = (1, 0, ..., 0)>. Thus we have
Hk+1(uk+1)B = ( kB01k 瓦管+1 ),
where B ∈ Rk×k. Note that Hk+J(uk+ι) = Ik+ι when uk+ι = 0 and the above still holds. By
assumption we have B = Hk(Uk)…Hk(uι)R. Notice that HL(Ui) = (1 Hk(U)), so we have
that
Hk+1(u1)...Hk+1 (Uk)Hk+1(uk+ι)B = ( kB01k B1,Rk+1 ) = R
is an upper triangular matrix with positive diagonal elements. Thus the result holds for any n by the
theory of mathematical induction.	□
A.2 Proof of Theorem 1
Proof. Observe that the image of M1 is a subset of O(n), and we now show that the converse
is also true. Given A ∈ O(n), by Proposition 1, there exists an upper triangular matrix R with
positive diagonal elements, and an orthogonal matrix Q expressed as Q = Hnn(Un)...H1n(U1) for
some set of Householder vectors {Ui}in=1, such that A = QR. Since A is orthogonal, we have
A>A = AA> = In, thus:
A>A = R>Q>QR = R>R = In; Q>AA>Q = Q>QRR>Q>Q = RR> = In
Thus R is orthogonal and upper triangular matrix with positive diagonal elements. So R = In and
A = Q = Hn(Un)…Hn(UI).	□
A.3 Proof of Theorem 2
Proof. It is easy to see that the image of M1,1 is a subset ofRn×n. For any W ∈ Rn×n, we have its
SVD, W = UΣV >, where Σ = diag(σ). By Theorem 1, for any orthogonal matrix U, V ∈ Rn×n,
there exists {Ui}in=1{vi}in=1 such that U = M1(U1, ..., Un) and V = M1(v1, ..., vn), then we have:
W = Hnn(Un)...H1n(U1)ΣH1n(v1)...Hnn(vn)
= M1,1(U1, ..., Un, v1, ..., vn, σ)
□
A.4 Proof of Theorem 3
Proof. Let A ∈ Rn×n be an orthogonal matrix. By Theorem 1, there exist {ai}in=1, such that
A = M1(a1, ..., an). Since A> is also orthogonal, for the same reason, there exist {bi}in=1, such
that A> = M1(b1, ..., bn). Thus we have:
A = Hn(an)...H1(a1) = H1(b1)...Hn(bn)
Observe that one of k2 ≥ k1 - 1 and k1 ≥ k2 - 1 must be true. If k2 ≥ k1 - 1, set
Uk = ak , k = n, n - 1, ..., k1 ,
vk2+k1 -k-1 = ak, k = k1 - 1, ..., 1,	(19)
vt = 0, t = k2 + k1 - 2, ..., n,
and then we have:
Mk1 ,k2 (Uk1 , ..., Un , vk2 , ..., vn , 1) = Hn (Un )...Hk1 (Uk1 )In Hk2 (vk2 )...Hn (vn )
= Hn(an)...Hk1 (ak1)InHk1-1(ak1-1)...H1(a1)
= A	(20)
10
Under review as a conference paper at ICLR 2018
Else, assign:
vk = bk, k = n, n - 1, ..., k2,
uk2+k1-k-1 = bk, k = k2 - 1, ..., 1,	(21)
ut = 0, t = k2 + k1 - 2, ..., n,
and then we have:
Mk1,k2 (uk1, ..., un, vk2, ..., vn, 1) = H1(b1)...Hk2-1(bk2-1)InHk2(bk2)...Hn(bn)
= A	(22)
□
A.5 Proof of Theorem 4
Proof. It is easy to see that the image of Mmr is a subset of Rm×n. For any W ∈ Rm×n, We have
its SVD, W = UΣV >, where Σ is an m × n diagonal matrix. By Theorem 1, for any orthogonal
matrix U ∈ Rm×m , V ∈ Rn×n, there exists {ui}im=1{vi}in=1 such that U = Hmm (um )...H1m (u1)
and V = Hnn(vn)...H1n(v1). By Lemma 1, if m < n We have:
W=Hnm (un)...H1m (u1)ΣH1n(v1)...Hnn(vn)
=Hnm (un)...H1m (u1)ΣHnn-m+1(vn-m+1)...Hnn(vn).
Similarly, for n < m, We have:
W = Hnm (un)...H1m (u1)ΣH1n(v1)...Hnn(vn)
= Hnm (un)...Hmm-n+1(um-n+1)ΣH1n(v1)...Hnn(vn).
□
A.6 Proof of Claim 1
Proof.
R[W ] = EX〜D [f(W; X ,y)]
=EX〜D [k(Y W-A)VeC(X) - ηk2]
=E [tr ((YW - A)VeC(X)vec(X)>(YW - A)>)] + E[kηk2] - 2E [{(YW - A)VeC(X),力]
=tr ((YW - A)E [vec(X)vec(X)>] (YW - A)>) + C
=∣∣(y W-a)∑17∣F + c
For the deriVatiVe,
R[W + ∆W ] = k(Y (W + ∆W |(W + ∆W )2∣∙∙∙∣(W + ∆W )t) - A)£1/2kF + C
=R[W] + D(∆W|2W∆W| …|tWt-1∆W) Σ1/2, (YW - Aa/2E + o(∣∆W∣∣F)
=R[W] + (∆W, (YW - A)Σ (Id|2W|3W2| …他Wt-1 )>>
Therefore VR[W] = (YW - A)Σ (ld∣2W|3W2| …|tWt-1)>
Remark: here When W and ∆W are not CommutatiVe, eaCh Wi ∆W should instead be Written as
Pij=0 Wj ∆WWi-j. SinCe the Change of order doesn’t impaCt the analysis, We informally simplify
the expressions here.
□
11
Under review as a conference paper at ICLR 2018
Appendix B	Details of Forward and Backward Propagation
Algorithms
Algorithm 1 Local forward/backward propagation
Input： h(t-1), dCLt), U = (un|…|un-m1+1),
Σ, V = (vn |...|vn-m2+1)
Outut： C(t) = Wh(t-1) dL dL dL —dL-
OuIpuL C = Wh , ∂u , ∂v , ∂σ，∂h(t-i)
// Begin forward propagation
hv+ι - h(T)
for k = n, n - 1, ..., n - m2 + 1 do
hkv) J HProd(hv+ι,v)
end for
h(ku1)-1 J Σh(kv2)
for k = n - m1 + 1, ..., n do
h(ku) J Hprod(h(ku-)1, uk)
end for
C(t) J h(nu)
// Compute V>h
// Compute ∑V>h
^__^ -∣--
// Compute U∑V> h
Algorithm 2
h = H prod(h, uk)
Input： h, uk
Output： h = Hk (uk)h
// Compute h = (I 一 2ukUk )h
uk uk
α J k⅛ u> h
f ,	7
h J h 一 αuk
//Begin backward propagation
j dL
g J dC(t)
for k = n, n 一 1, ..., n 一 m1 + 1 do
g, Gun-k+ιJ Hgrad(hku),uk ,g)
end for
∑ J diag(g ◦ hkv)), g J Σg
g⑸ J d⅛F ◦ diag(∑)
for k = n 一 m2 + 1, ..., n do
g, Gvn-k+ιJ Hgrad(hku+ι,vk ,g)
end for
〃 ComPUte du-
// Compute ∂∑
// Compute ∂^
// Compute
∂L
∂vk
Algorithm 3
h,Uk = Hgrad(h,Uk ,g)
Input: h,uk,g = ∂L where
C=Hk(uk)h
OUtPUt： h = dL,uk = ∂L
α = k⅛ u>h
β = k⅛ u>g
h J g 一 βuk
Uk J--αg 一 βh + αβuk
∂Ul J G(U), ∂V J G(V), ∂L J g⑸,
dL J o
∂h(t-ι) J g
Appendix C	More Experimental Details
C.1 Details on the Time Series Classification Task
For the time series classification task, we use the training and testing sets directly from the UCR
time series archive http://www.cs.ucr.edu/~eamonn/time_series_data/, and ran-
domly choose 20% of the training set as validation data. We provide the statistical descriptions of
the datasets and experimental results in Table 3.
Datasets	Data Descriptions				Depth	RNN aCC (nparam)		LSTM acc (nparam)		oRNN acc (nparam)		svdRNN acc (nparam)	
	training/testing Size		length	#class									
50words	450	455	270	50	27	0.492	(3058)	0.598	(7218)	0.642	(2426)	0.651	(2850)
Adiac	390	391	176	37	16	0.552	(2694)	0.706	(6950)	0.668	(2062)	0.726	(2486)
ArrowHead	36	175	251	3	251	0.509	(1219)	0.537	(4515)	0.669	(587)	0.800	(1011)
Beef	30	30	470	5	47	0.600	(1606)	0.700	(5766)	0.733	(974)	0.733	(1398)
BeetleFly	20	20	512	2	32	0.950	(1699)	0.850	(6435)	0.900	(1067)	0.950	(1491)
CBF	30	900	128	3	16	0.702	(1476)	0.967	(5444)	0.881	(844)	0.948	(1268)
Coffee	28	28	286	2	22	1.000	(1570)	1.000	(6018)	1.000	(938)	1.000	(1362)
Cricket X	390	390	300	12	20	0.310	(1997)	0.456	(6637)	0.495	(1365)	0.500	(1789)
DistalPhalanxOutlineCorrect	276	600	80	2	10	0.790	(1410)	0.798	(5378)	0.830	(778)	0.840	(1202)
DistalPhalanxTW	154	399	80	6	10	0.815	(1641)	0.795	(5609)	0.807	(1009)	0.815	(1433)
ECG200	100	100	96	2	12	0.640	(1410)	0.640	(5378)	0.640	(778)	0.640	(1202)
ECG5000	500	4500	140	5	14	0.941	(1606)	0.936	(5766)	0.940	(974)	0.945	(1398)
ECGFiveDays	23	861	136	2	17	0.947	(1443)	0.790	(5411)	0.976	(811)	0.948	(1235)
FaceAll	560	1690	131	14	131	0.549	(1615)	0.455	(4911)	0.714	(983)	0.714	(1407)
FaceFour	24	88	350	4	25	0.625	(1701)	0.477	(6245)	0.511	(1069)	0.716	(1493)
FacesUCR	200	2050	131	14	131	0.449	(1615)	0.629	(4911)	0.710	(983)	0.727	(1407)
Gun Point	50	150	150	2	15	0.947	(1507)	0.920	(5667)	0.953	(875)	0.960	(1299)
InsectWingbeatSound	220	1980	256	11	16	0.534	(1996)	0.515	(6732)	0.598	(1364)	0.586	(1788)
ItalyPowerDemand	67	1029	24	2	6	0.970	(1315)	0.969	(4899)	0.972	(683)	0.973	(1107)
Lighting2	60	61	637	2	49	0.541	(1570)	0.541	(6018)	0.541	(938)	0.541	(1362)
MiddlePhalanxOutlineCorrect	291	600	80	2	10	0.793	(1410)	0.783	(5378)	0.712	(778)	0.820	(1202)
Table 3： Test accuracy (number of parameters) on UCR datasets. For each dataset, we present the
testing accuracy when reaching the smallest validation error. The highest precision is in bold, and
lowest two are colored gray.
12
Under review as a conference paper at ICLR 2018
C.2 Experimental Results on the Adding and Copying tasks
We tested RNN models on the Adding and Copying tasks with the same settings as Arjovsky et al.
(2016).
C.2. 1 Adding task
The Adding task requires the network to remember two marked numbers in a long sequence and add
them. Each input data includes two sequences: top sequence whose values are sampled uniformly
from [0, 1] and bottom sequence which is a binary sequence with only two 1’s. The network is asked
to output the sum of the two values. From the empirical results in Figure 4, we can see that when
the network is not deep (number of layers L=30 in (a)(d)), every model outperforms the baseline of
0.167 (always output 1 regardless of the input). Also, the first layer gradients do not vanish for all
models. However, on longer sequences (L=100 in (b)(e)), IRNN failed and LSTM converges much
slower than svdRNN and oRNN. If we further increase the sequence length (L=300 in (c)(f)), only
svdRNN and oRNN are able to beat the baseline within reasonable number of iterations. We can
also observe that the first layer gradient of oRNN/svdRNN does not vanish regardless of the depth,
while IRNN/LSTM’s gradient vanish as L becomes lager.
(f)
Figure 4: RNN models on the adding task with L layers and nh hidden dimension. The top plots
show the test MSE, while the bottom plots show the magnitude of the gradient at the first layer.
C.2.2 Copying task
Let A = {ai }i9=0 be the alphabet. The input data sequence x ∈ AT +20 where T is the time lag.
x1:10 are sampled uniformly from i{ai }i7=0 and xT +10 is set to a9. Rest of xi is set to a8. The
network is asked to output x1:10 after seeing a9. That is to copy x1:10 from the beginning to the end
with time lag T.
A baseline strategy is to predict a8 forT + 10 entrees and randomly sample from {ai}i7=1 for the last
10 digits. From the empirical results in Figure 5, svdRNN consistently outperforms all other models.
IRNN and LSTM models are not able to beat the baseline with large time lag. In fact, the loss of
RNN/LSTM is very close to the baseline (memoryless strategy) indicates that they do not memorize
any useful information throughout the time lag.
Figure 5: RNN models on the Copying task with T time lag and nh hidden dimension.
13