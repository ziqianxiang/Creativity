Under review as a conference paper at ICLR 2018
The Variational Homoencoder: learning to
infer high-capacity generative models from
FEW EXAMPLES
Anonymous authors
Paper under double-blind review
Ab stract
Hierarchical Bayesian methods have the potential to unify many related tasks (e.g.
k-shot classification, conditional, and unconditional generation) by framing each
as inference within a single generative model. We show that existing approaches
for learning such models can fail on expressive generative networks such as Pix-
elCNNs, by describing the global distribution with little reliance on latent vari-
ables. To address this, we develop a modification of the Variational Autoencoder
in which encoded observations are decoded to new elements from the same class;
the result, which we call a Variational Homoencoder (VHE), may be understood
as training a hierarchical latent variable model which better utilises latent vari-
ables in these cases. Using this framework enables us to train a hierarchical Pix-
elCNN for the Omniglot dataset, outperforming all existing models on test set
likelihood. With a single model we achieve both strong one-shot generation and
near human-level classification, competitive with state-of-the-art discriminative
classifiers. The VHE objective extends naturally to richer dataset structures such
as factorial or hierarchical categories, as we illustrate by training models to sep-
arate character content from simple variations in drawing style, and to generalise
the style of an alphabet to new characters.
1	Introduction
Learning from few examples is possible only with strong inductive biases. In machine learning such
biases can come from hand design, as in the parametrisation of a model, or can be the result of a
meta-learning algorithm. Furthermore they may be task-specific, as in discriminative modelling, or
may describe the world causally so as to be naturally reused across many tasks.
Recent work has approached one- and few-shot learning from all of these perspectives. Siamese
Networks (Koch, 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell
et al., 2017) and MANNs (Santoro et al., 2016) are all models discriminatively trained for few-shot
classification. Such models can achieve state-of-the-art performance at the task they were trained
for, but provide no principled means for transferring knowledge to other tasks.
Other work such as Rezende et al. (2016) has developed conditional generative models, which take
one or a few observations from a class as input, and return a distribution over new elements p(x|D).
These models may be used as classifiers despite not being explicitly trained for this purpose, by
comparing conditional likelihoods. They may also be used to generate full sets incrementally
as p(X) = Qi p(xi|x1, . . . , xi-1), as discussed in Generative Matching Networks (Bartunov &
Vetrov, 2016). However, such models are a more natural fit to sequences than sets as they typi-
cally lack exchangeability, and furthermore they do not expose any latent representation of shared
structure within a set.
Finally are hierarchical approaches that model shared structure through latent variables, as p(X) =
c p(c) Qip(xi|c)dc. For example, Lake et al. (2015) develop a compositional causal generative
model of handwritten characters, achieving human-level performance at a variety of related tasks.
Salakhutdinov et al. (2013) follow a similar Bayesian philosophy while employing a more general-
purpose architecture. Most recently the Neural Statistician (Edwards & Storkey, 2016) uses amor-
tised variational inference to support learning in a deep hierarchical generative model.
1
Under review as a conference paper at ICLR 2018
Figure 1: Single step of gradient training in various models. A VAE treats all datapoints as indepen-
dent, so only a single random element need be encoded and decoded each step. A Neural Statistician
instead feeds a full set of elements X through both encoder and decoder networks, in order to share
a latent variable c. In a VHE, we bound the full likelihood p(X) using only random subsamples D
and x for encoding/decoding. Optionally, p(x|c) may be defined through a local latent variable z.
In this work we propose the Variational Homoencoder (VHE), aiming to combine several advantages
of the models described above:
1.	Like conditional generative approaches, we train on a few-shot generation objective which
matches how our model may be used at test time. However, by introducing an encoding
cost, we simultaneously optimise a likelihood lower bound for a hierarchical generative
model, in which structure shared across elements is made explicit by shared latent variables.
2.	Previous work (Edwards & Storkey, 2016) has learned hierarchical Bayesian models by
applying Variational Autoencoders to sets, such as classes of images. However, their ap-
proach requires feeding a full set through the model per gradient step (Figure 1), rendering
it intractable to train on very large sets. In practice, they avoid computational limits by
sampling smaller subset as training data. In a VHE, we instead optimise a likelihood bound
for the complete dataset, while constructing this bound by subsampling. This approach
can not only improve generalisation, but also departs from previous work by extending to
models with richer latent structure, for which the joint likelihood cannot be factorised.
3.	As with a VAE, the VHE objective includes both an encoding- and reconstruction- cost.
However, by sharing latent variables across a large set of elements, the encoding cost per
element is reduced significantly. This facilitates use of powerful autoregressive decoders,
which otherwise often suffer from ignoring latent variables (Chen et al., 2016). We demon-
strate the significance of this by applying a VHE to the Omniglot dataset. Using a Pixel-
CNN decoder (Oord et al., 2016), our generative model is arguably the first with a general
purpose architecture to both attain near human-level one-shot classification performance
and produce high quality samples in one-shot generation.
2	Background
2.1	Variational Autoencoders
When dealing with latent variable models of the form p(x) = z p(z)p(x|z)dz, the integration is
necessary for both learning and inference but is often intractable to compute in closed form. Varia-
tional Autoencoders (VAEs, Kingma & Welling (2013)) provide a method for learning such models
by utilising neural-network based approximate posterior inference. Specifically, a VAE comprises
a generative network p(z)p(x|z) parametrised by θ, alongside a separate inference network q(z; x)
parameterised by φ. These are trained jointly to maximise a single objective:
LX (θ, φ) = X log pθ (x)
x∈X
-DKL qφ(z; X) k Pθ(ZIx)
Σ
x∈X
E log Pθ(x∣z) - DKL (qφ(z; x) k Pθ
q4(z;x)	'
(1)
(2)
2
Under review as a conference paper at ICLR 2018
As can be seen from Equation 1, this objective LX is a lower bound on the total log likelihood of the
dataset Px∈X log p(x), while q(z; x) is trained to approximate the true posterior p(z|x) as accu-
rately as possible. If it could match this distribution exactly then the bound would be tight so that the
VAE objective equals the true log likelihood of the data. In practice, the resulting model is typically
a compromise between two goals: pulling p towards a distribution that assigns high likelihood to
the data, but also towards one which allows accurate inference by q . Equation 2 provides a formula-
tion for the same objective which can be optimised stochastically, using Monte-Carlo integration to
approximate the expectation.
2.2	Variational Autoencoders Over Sets
The Neural Statistician (Edwards & Storkey, 2016) is a Variational Autoencoder in which each item
to be encoded is itself a set, such as the set X (i) of all images with a particular class label i:
X Ci) = {xιi),χ2i), ∙ ∙ ∙ ,xni)}
(3)
The generative model for sets, p(X), is described by introduction of a corresponding latent variable
c. Given c, individual x ∈ X are conditionally independent:
p(X) =
c
p(c)	p(x|c)dc
x∈X
(4)
This functional form is justified by de Finetti’s theorem under the assumption that elements within in
each set X are exchangeable. The likelihood is again intractable to compute, but it can be bounded
below via:
logp(X) ≥ Lχ = E ElOgp(x∣c) — DκL(q(c; X) k p(c))	(5)
q(c;X) x∈X
3	Variational Homoencoders
Unfortunately, calculating the variational lower bound for each set X requires evaluating both
q(c; X) and p(X|c), meaning that the entire set must be passed through both networks for each
gradient update. This can easily become intractable for classes with hundreds of examples. Indeed,
previous work (Edwards & Storkey, 2016) ensures that sets used for training are always of small
size by instead maximising a likelihood lower-bound for randomly sampled subsets.
In this work we instead replace the variational lower-bound in Equation 5 with a new objective, itself
constructed via sub-sampled datasets of reduced size. We use a constrained variational distribution
q(c; D), D ⊆ X for posterior inference and an unbiased stochastic approximation log p(x|c), x ∈ X
for the likelihood. In the following section we show that the resulting objective can be interpreted
as a lower-bound on the log-likelihood of the data.
This bound will typically be loose due to stochasticity in sampling D, and we view this as a reg-
ularization strategy: we aim to learn latent representations that are quickly inferable from a small
number of instances, and the VHE objective is tailored for this purpose.
3.1	Stochastic lower bound
We would like to learn a generative model for sets X of the form
p(X) =	p(c) Y p(x|c)dc	(6)
x∈X
We will refer our full dataset as a union of disjoint sets X = X1 t X2 t . . . t Xn , and use X(x) to
refer to the set Xi 3 x. Using the standard consequent of Jensen’s inequality, we can lower bound
the log-likelihood of each set X using an arbitrary distribution q. In particular, we give q as a fixed
function of arbitrary data.
log p(X) ≥ E logp(X|c) - DKL q(c; D) k p(c),	∀D ⊂ X	(7)
q(c;D)
3
Under review as a conference paper at ICLR 2018
Algorithm 1: Minibatch training for the Variational Homoencoder. Minibatches are of size M .
Stochastic inference network q uses subsets of size N .
initialize (θ, φ)	Parameters for p and q
repeat
sample (xk, ik) for k = 1, . . . , M Minibatch of elements with corresponding class labels
sample Dk ⊆ Xik for k = 1, . . . , M	where |Dk| = N
sample Ck 〜qφ(c; Dk) for k = 1,...,M
(optional) sample Zk 〜qφ(z; Ck,xk) for k = 1,...,M
g ≈ MM Ek VLθ,φ(xk; Dk, |Xik |)	Reparametrization gradient estimate using c, Z
(θ, φ)  (θ, φ) + λg	Gradient step, e.g SGD
until convergence of (θ, φ)
Splitting up individual likelihoods, we may rewrite
logp(X) ≥ E h X log p(x|C)i - DKL q(C;D) k p(C),
q(c;D) x∈X
=Xh E logP(XIC)- ɪDKL [q(c;D) k P(C)]],
x∈X q(c;D)	|X|
d=efXL(x;D,|X|),
x∈X
∀D ⊂ X	(8)
∀D ⊂ X	(9)
∀D ⊂ X	(10)
Finally, we can replace the universal quantification with an expectation under any distribution of D
(e.g. uniform sampling from X without replacement):
log P(X) ≥
E
D⊂X
L(x;D,|X|)=	E L(x; D, |X |)
x∈X	x∈X D⊂X
log P(X) ≥	E L(x; D, |X(x)|)
x∈X D⊂X(x)
(11)
(12)
This formulation suggests a simple modification to the VAE training procedure, as shown in Algo-
rithm 1. At each iteration we select an element x, use resampled elements D ⊂ X(x) to construct
the approximate posterior q(C; D), and rescale the encoding cost appropriately. If the generative
model P(x|C) also describes a separate latent variable z for each element, we may simply introduce
a second inference network q(z; C, x) in order to replace the exact reconstruction error of Equation
9 by a conditional VAE bound:
log P(x|C) ≥	E log P(x|C, z) -DKL q(z; C, x) k P(z|C)	(13)
q(z;c,x)
3.2	Application to structured datasets
The above derivation applies to a dataset partitioned into disjoint subsets X = X1t X2 t . . . t Xn ,
each with a corresponding latent variable Ci . However, many datasets offer a richer organisational
structure, such as the hierarchical grouping of characters into alphabets (Lake et al., 2015) or the
factorial categorisation of rendered faces by identity, pose and lighting (Kulkarni et al., 2015).
Provided that such organisational structure is known in advance, we may generalise the training
objective in Equation 12 to include a separate latent variable Ci for each group Xi within the dataset,
even when these groups overlap. To do this we first rewrite this bound in its most general form,
where c collects all latent variables:
log P(X ) ≥ E
Q(c;D)
X log P(x|c)
x∈X
-DKL Q(c; D) kP(c)
(14)
As shown in Figure 2, a separate Di ⊂ Xi may be subsampled for inference of each latent variable
Ci, so that Q(c) = Qi qi(Ci; Di). This leads to an analogous training objective (Equation 15), which
4
Under review as a conference paper at ICLR 2018
Figure 2: Application of VHE framework to hierarchical (left) and factorial (right) models. Given
an element x such that x ∈ X1 and x ∈ X2 , an approximate posterior is constructed for the
corresponding shared latent variables c1 , c2 using subsampled sets D1 ⊂ X1 , D2 ⊂ X2
may be applied to data with factorial or hierarchical category structure. For the hierarchical case,
this objective may be further modified to infer layers sequentially, as in Supplementary Material 6.2.
logp(X)≥ DEX
Di ⊂Xi
x∈X for each
i:x∈Xi
E	log P(XIc)- X 占DKL (qi(ci； Di) k P(Ci))
qi(ci;Di)	i X |Xi|
：x ∈
for each	i
i:x∈Xi
(15)
3.3	D iscussion
Powerful Decoder Models
As evident in Equation 9, the VHE objective provides a formal motivation for KL rescaling in the
variational objective (a common technique to increase use of latent variables in VAEs) by sharing
these variables across many elements. This is of particular importance when using autoregressive
decoder models, for which a common failure mode is to learn a decoder P(xIz) with no dependence
on the latent space, thus avoiding the encoding cost. In the context of VAEs, this particular issue has
been discussed by Chen et al. (2016) who suggest crippling the decoder as a potential remedy.
The same failure mode can occur when training a VAE for sets if the inference network q is not able
to reduce its approximation errorDKL[q(c; D) k P(cID)] to below the total correlation of D, either
because IDI is too small, or the inference network q is too weak. Variational Homoencoders suggest
a potential remedy to this, encouraging use of the latent space by reusing the same latent variables
across a large set X. This allows a VHE to learn useful representations even with IDI = 1, while at
the same time utilising a powerful decoder model to achieve highly accurate density estimation.
Constrained Posterior Approximation
In a VAE, use of a recognition network encourages learning of generative models whose structure
permits accurate amortised inference. In a VHE, this recognition network takes only a small subsam-
ple as input, which additionally encourages that the true posteriorP(cIX) can be well approximated
from only a few examples of X. For a subsample D ⊂ X, q(c; D) is implicitly trained to minimise
the KL divergence from this posterior in expectation over possible sets X consistent with D. For a
data distribution Pd we may equivalently describe the VHE objective (Equation 12) as
3Eχ hlogP(X)i - ∣χχ∣DKLhq(GD) k P(CIX)i	(16)
EE
pd(D) pd(X|D)
Note that the variational gap on the right side of this equation is itself bounded by:
E DKL
pd(X|D)
q(C; D) k P(CIX)
≥ DKL q(C; D) k
E
pd(X|D)
P(CIX)
≥0
(17)
The left inequality is tightest whenP(CIX) matches P(CID) well across all X consistent with D, and
exact only when these are equal. We view this aspect of the VHE loss as regulariser for constrained
posterior approximation, encouraging models for which the posterior P(CIX) can be well determined
by sampled subsets D ⊂ X . This reflects how we expect the model to be used at test time, and in
5
Under review as a conference paper at ICLR 2018
practice we have found this ‘loose’ bound to perform well in our experiments. In principle, the
bound may also be tightened by introducing an auxiliary inference network (see Supplementary
Material 6.1) which we leave as a direction for future research.
4	Experimental Results
4.1	Simple 1D Distributions
With a Neural Statistician model, under-utilisation of latent variables is expected to pose the greatest
difficulty either when |D | is too small, or the inference network q is insufficiently expressive. We
demonstrate on simple 1D distributions that a Variational Homoencoder can bring improvements
under these circumstances. For this we created five datasets as follows, each containing 100 classes
from a particular parametric family, and with 100 elements sampled from each class.
1.	Gaussian: Each class is Gaussian with μ drawn from a Gaussian hyperprior (fixed σ2).
2.	Mixture of Gaussians: Each class is an even mixture of two Gaussian distributions with
location drawn from a Gaussian hyperprior (fixed σ2 and separation).
3.	von Mises: Each class is Von Mises with μ drawn from a Uniform hyperprior (fixed κ).
4.	Gamma: Each class is Gamma with fixed β , and with α drawn from a Uniform hyperprior.
5.	Discrete: Each class is Uniform on a subset of {1,. . . ,8}, either 1-4, 5-8, odd or even.
For each dataset, we then trained models using a Variety of Values for |D |, restricting the inference
network q(c; D) to a simple linear map with Gaussian output. In each case the generatiVe model
p(x|c) was set to the correct parametric family, with parameters learned as a linear function ofc. All
models were built in Torch 7 (Collobert et al., 2011) and optimised using Adam (Kingma & Welling,
2013) for 200 epochs. To aid optimisation we used an additional 50 epochs for KL annealing, and
used training error to select the best parameters from 3 independent training runs.
Our results show that, when |D | is small, the Neural Statistician often places little to no information
in q(c; D) (Figure 3, top row). Our careful training suggests that this is not an optimisation difficulty,
but is core to the objectiVe as in Chen et al. (2016). In these cases a VHE better utilises the latent
space, leading to improVements in both few-shot generation (by conditional NLL) and classification.
Importantly, this is achieVed while retaining good likelihood of test-set classes, typically matching
or improVing upon that achieVed by a Neural Statistician (including a standard VAE, corresponding
to |D| = 1). Please see Supplement 6.3 for further comparison to alternatiVe objectiVes.
Neural Statistician
----Variational Homoencoder
Figure 3: Comparison of models trained using Neural Statistician and VHE objectiVes. |D | is the
number of encoder inputs during training. Top row: Mean encoded information DKL [q(c; D) k
p(c)]; Second row: ∣D∣-shot generation loss - Ec〜q(c;D) logp(x0∣c); Third row: |D∣-shot binary
classification error, classified by minimising conditional NLL Bottom row: Joint NLL (per element)
of full test set, calculated by importance weighting on 200 samples from q(c; X);
6
Under review as a conference paper at ICLR 2018
4.2	Handwritten Character Classes
To validate our claim that the VHE objective can facilitate learning with more expressive generative
networks, we trained a variety of models on the Omniglot dataset to explore the interaction between
model architecture and training objective. We consider two model architectures: a standard decon-
volutional network based on Edwards & Storkey (2016), and a hierarchical PixelCNN architecture
inspired by the recently proposed PixelVAE (Gulrajani et al., 2016). For each, we compare models
trained with the Variational Homoencoder objective against three alternative objectives.
For our hierarchical PixelCNN architecture (Figure 4) each character class is associated with a spa-
tial latent variable c (a character ‘template’) drawn from a learned PixelCNN prior. For each charac-
ter x, a sampled affine transform tx is applied to this template (Jaderberg et al., 2015) and the result is
used to condition a Gated PixelCNN (Oord et al., 2016). For q(t; x) we use a CNN with Batch Nor-
malisation (Ioffe & Szegedy, 2015). For q(c; D) we use a Spatial Transformer followed by a single
convolution, and then average over D to output parameters of a diagonal Gaussian distribution.
Using both PixelCNN and deconvolutional architectures, we trained models by several objectives
matched in the computational cost per gradient step of training. Firstly, we compare a VHE model
against a Neural Statistician baseline, with each trained on sampled subsets D ⊂ X with |D | = 5
(as in Edwards & Storkey (2016)). Secondly, since the VHE introduces both data-resampling and
KL-rescaling as modifications to this baseline, we separate the contributions of each using two
intermediate objectives:
Resample only:
E
D⊂X
x∈X
(E^)log P(XIc)- ɪDKL [q(C D) k P(C)]
ResCaIe only： E E log p(x∣c) - ^^DKLlq(c; D) k p(c)]
D⊂X q(c;D)	|X|
x∈D
(18)
(19)
All models were trained on a random sample of 1200 Omniglot classes using images scaled to 28x28
pixels, dynamically binarised, and augmented by 8 rotations/reflections to produce new classes.
We additionally used 20 small random affine transformations to create new instances within each
class. Models were optimised using Adam (Kingma & Welling, 2013), and we used training error
to select the best parameters from 5 independent training runs. This was necessary to ensure a fair
comparison with the Neural Statistician objective, which otherwise converges to a local optimum
with q(c; D) = P(c). We additionally implemented the ‘sample dropout’ trick of Edwards & Storkey
(2016), but found that this did not have any effect on model performance.
Figure 4: Autoregressive Variational
HomoenCoder for Omniglot characters.
	,∏r	n	6	友	ɪ	T
ʧ UT		5	∏ Γ7 ΓJ	P) 6 k	四%比	7 二 7	@ 丁丁
OT	τr	WrIn「R	BCn G	发反黑	7至？	T^t
	Ir	τr r r 勺	CD(I> 8	产发以	N工7	干TT
	Q	t∏τ	σ	T	P	L
	¾	Q WBt	9 U 9	,η^ z	PPP	U-匚
I	U	2 nʃ W Dr		η ττl 5	t7 ° P	匚二匚
		0 CrT bk	U ʧʃr	T]∏ξ L	必C1，	-c- -^- J=^
	Q	Jh	F		N	6
	G.	ɔ h Jq	声CK「		。5加	G⅛e
	Q	0 h ⅛ λ⅛	Γ∣ H甲	7 -J , ・、--		C匕G
	Q	o ⅛⅛ t1 心	n G再		L	
Figure 5:	One-shot same-class samples generated by
our model. Cues images were drawn at random from
previously unseen classes.
7
Under review as a conference paper at ICLR 2018
Table 1: Interaction of model archi- tecture and training objective on 5- shot, 20 way classification accuracy.			Table 2: Comparison of deep learning techniques for Omniglot classification		
			Accuracy (20-way)		
	KL/nats*	Accuracy			
			1-shot		5-shot
Deconvolutional Architecture			Generative models, log p(X)		
NS [7]	31.34	95.6%	Variational Memory Addressing [2]	77%	91 %
Resample	25.74	94.0%	Generative Matching Networks* [1]	77.0%	91.0%
Rescale	477.65	95.3%	Neural Statistician [7]	93.2%	98.1%
VHE	452.47	95.6%	Variational Homoencoder	95.2%	98.8%
PixelCNN Architecture			Discriminative models, log q(y|x, X, Y )		
NS	14.90	66.0%	Siamese Networks [16]	88.1%	97.0%
Resample	0.22	4.9%	Matching Networks [26]	93.8%	98.7%
Rescale	506.48	62.8%	Convnet with memory module [14]	95.0%	98.6%
VHE	268.37	98.8%	mAP-DLM [25]	95.4%	98.6%
*DKL	q(c; D) k p(c) , train set		Model-Agnostic Meta-learning [8] Prototypical Networks [24]	95.8% 96.0%	98.9% 98.9%
*Uses train/test split from Lake et al. (2015)
Table 1 collects classification results of models trained using each of the four alternative training
objectives, for both architectures. When using a standard deconvolutional architecture, we find
little difference in classification performance between all four training objectives, with the Neural
Statistician and VHE models achieving equally high accuracy.
For the hierarchical PixelCNN architecture, however, significant differences arise between training
objectives. In this case, a Neural Statistician learns an strong global distribution over images but
makes only minimal use of latent variables c. This means that, despite the use of a higher capacity
model, classification accuracy is much poorer (66%) than that achieved using a deconvolutional
architecture. For the same reason, conditional samples display an improved sharpness but are no
longer identifiable to the cue images on which they were conditioned (Figure 6). Our careful training
suggests that this is not an optimisation difficulty but is core to the objective, as discussed in Chen
et al. (2016).
By contrast, a VHE is able to gain a large benefit from the hierarchical PixelCNN architecture, with
a 3-fold reduction in classification error (5-shot accuracy 98.8%) and conditional samples which are
simultaneously sharp and identifiable (Figure 6). This improvement is in part achieved by increased
utilisation of the latent space, due to rescaling of the KL divergence term in the objective. However,
our results show that this common technique is insufficient when used alone, leading to overfitting
to cue images with an equally severe impairment of classification performance (accuracy 62.8%).
Rather, we find that KL-rescaling and data resampling must be used together in order to for the
benefit of the powerful PixelCNN architecture to be realised.
Table 2 lists the classification accuracy achieved by VHEs with both |D| = 1 and |D| = 5, as
compared to existing deep learning approaches. We find that both networks are not only state-of-the-
art amongst deep generative models, but are also competitive against the best discriminative models
trained directly for few-shot classification. Unlike these discriminative models, a VHE is also able
to generate new images of a character in one-shot, producing samples which are simultaneously
realistic and faithful to the class of the cue image (Figure 5).
8
Under review as a conference paper at ICLR 2018
PixelCNN Architecture					Deconvolutional Architecture		
≈⅛⅛⅛17	士.上触“	除加交•库	二 t> Z.6	t≥ Λ 42 h		⅛ CC C	t3 -h ⅛
⅛,⅛⅞	眨代力也	小府门与	M 总 即，D	ɪh ɪh ⅛3 ⅛	Ztɪ ti 幻	W匕04	
玲诏拈相	^⅞,∙⅛⅞ Ψ⅛ (f⅛		ZT ¾⅛ ⅛‰.	Itann #1	，h ∙⅛ j⅛ ti	QaCo	打匕力尢）
4⅞⅛ 为 h>	⅛松■后桁	TXUg	U ⅛ s. ⅛	均北htl	jɜ H tɪ ti	CR力C	-»Qt*七打
VHE	Rescale	Resample	NS	VHE	Rescale	Resample	NS
Figure 6:	5-shot samples generated by each model (more in Supplement 6.4.2). With a PixelCNN
architecture, both NS and Resample underutilise the latent space and so produce unfaithful samples.
As our goal is to model shared structure across images, we evaluate generative performance using
joint log likelihood of the entire Omniglot test set (rather than separately across images). From this
perspective, a single element VAE will perform poorly as it treats all datapoints as independent, op-
timising a sum over log likelihoods for each element. By sharing latent variables across all elements
of the same class, a VHE can improve upon this considerably.
Previous work which evaluates likelihood typically uses the train/test split of Burda et al. (2015).
However, our most appropriate comparison is with Generative Matching Networks (Bartunov &
Vetrov, 2016) as they also model dependencies within a class; thus, we trained models under the
same conditions as them, using the harder test split from Lake et al. (2015) with no data augmenta-
tion. We evaluate the joint log likelihood of full character classes from the test set, normalised by
the number of elements, using importance weighting with k=500 samples from q(c; X). As can be
seen in Tables 3 and 4, our hierarchical PixelCNN architecture is able to achieve state-of-the-art log
likelihood results only when trained using the full Variational Homoencoder objective.
Table 3: Joint NLL of Omniglot test set,
across architectures and objectives
Test NLL per image	
Deconvolutional Architecture	
NS [7]	102.84 nats
Resample	110.30 nats
Rescale	109.01 nats
VHE	104.67 nats
PiXelCNN Architecture	
NS	73.50 nats
Resample	66.42 nats
Rescale	71.37 nats
VHE	61.22 nats
Table 4: Comparison of deep generative models by
joint NLL of Omniglot test set
Test NLL per image	
Independent models	1 log QiP(Xi)
DRAW [9]	< 96.5 nats
Conv DRAW [10]	< 91.0 nats
VLAE [4]	89.83 nats
Conditional models	1 log QiP(xi|xi：i-i)
Variational Memory Addressing [2]	> 73.9 nats Generative Matching Networks [1]	62.42 nats1	
Shared-latent models	1 log Ep(C)Qi P(XiIC)
Variational Homoencoder	61.22 nats
4.3 Modelling richer category structure
To demonstrate how the VHE framework may apply to models with richer category structure, we
built both a hierarchical and a factorial VHE (Figure 2) using simple modifications to the above
architectures. For the hierarchical VHE, we extended the deconvolutional model with an extra latent
layer a using the same encoder and decoder architecture as c. This was used to encode alphabet
level structure for the Omniglot dataset, learning a generative model for alphabets of the form
p(A) =	p(a) Y	p(ci|a) Y p(xij|ci,a)dcida	(20)
Xi∈A	xij∈Xi
1We thank the authors of Bartunov & Vetrov (2016) for providing us with this comparison.
9
Under review as a conference paper at ICLR 2018
Again, we trained this model using a single objective, using separately resampled subsets Da and Dc
to infer each latent variable (Supplement 6.2). We then tested our model at both one-shot character
generation and 5-shot alphabet generation, using samples from previously unseen alphabets. As
shown in Figure 7, our single trained model is able to learn structure at both layers of abstraction.
For the factorial VHE, we extended the Omniglot dataset by assigning each image to one of 30
randomly generated styles (independent of its character class), modifying both the colour and pen
stroke of each image. We then extended the PixelCNN model to include a 6-dimensional latent
variable s to represent the style of an image, alongside the existing c to represent the character. We
used a CNN for style encoder q(s|Ds), and for each image location we condition the PixelCNN
decoder using the outer product S 0 cij∙.
We then test this model on a style transfer task by feeding separate images into the character encoder
q(c|Dc) and style encoder q(s|Ds), then rendering a new image from the inferred (c, s) pair. We
find that synthesised samples are faithful to the respective character and style of both cue images
(Figure 8), demonstrating ability of a factorial VHE to successfully disentangle these two image
factors using separate latent variables.
Figure 8: Previously unseen characters redrawn in
Figure 7: Conditional samples from both a style inferred from another image. Top two images
character (top) and alphabet (bottom) levels denote the content (left) and style (right).
of the same hierarchical model.
ɪɪ		5	DT	B		丹	■ S
UU U		乃y	e？ʃ UT σtf	6	6 B	CeC	FT
=UU		⅜石	WkW	G	6 6	r∖∏卜	胃，，
己"3	L日
e豆W"	UUr
W忑下	匚Uei
3"	
UUr
C G C?
UUU F 5		AΛΓ IA) (XI	θ	B B	n	n a
jWJWH讦		Q Y)		M F	h	1中
δΓ H R 刁 H	β	节t・	3)	0 C	U	。&
百々百耳百		，u y	0	t P		0 口
日 MmPcT	V	2 =	9	r C	P	π P
3彳B也力		9 1	W	同P	、	Q R
口 m 3 日 C3	1	^rJCl	Λ		G	DP
五 ι7 λ r i <4
# 4	顺4
u»U	*>∙ e
埠*d	命做力
制”	
q rr	ʌ 5
卜林
卜卜六
P "
a V
}Aλ
AA *
Λ∙人A
gqq
9 4ΓR
q<Qq
G，什
f t f
Al 1
Λ∕g
又 £4
A £4
VrTr月
qIr可
,牙IP
t>
Rl
TT
5
份由fr
7a®
γs⅜
33,
mam
△2
3-Qd
幺4公
编日修
Qaa
u<⅛假
%3,*e
AE/
4 4 4
寻sr
不■可
B
3
Δ
α
J尸£
√6 J
ʃ94
5 Conclusion and Future Work
We introduced the Variational Homoencoder : a deep hierarchical generative model learned by a
novel training procedure which resembles few-shot generation. This framework allows latent vari-
ables to be shared across a large number of elements in a dataset, encouraging them to be well
utilised even alongside highly expressive decoder networks. We demonstrate this by training a
hierarchical PixelCNN model on Omniglot dataset, and show that our novel training objective is
responsible for the state-of-the-art results it achieves. This model is arguably the first which uses
a general purpose architecture to both attain near human-level one-shot classification performance
and produce high quality samples in one-shot generation.
The VHE framework extends naturally to models with richer latent structure, as we demonstrate
with two examples: a hierarchical model which generalises the style of an alphabet to produce new
characters, and a factorial model which separates the content and drawing style of coloured character
images. In addition to these modelling extensions, our variational bound may also be tightened
by learning a subsampling procedure q(D; X), or by introducing an auxiliary inference network
r(D; c, X) as discussed in Supplementary Material 6.1. While such modifications were unnecessary
for our experiments on Omniglot character classes, we expect that they may yield improvements on
other datasets with greater intra-class diversity.
10
Under review as a conference paper at ICLR 2018
References
[1]	Sergey Bartunov and Dmitry P Vetrov. Fast adaptation in generative models with generative
matching networks. arXiv preprint arXiv:1612.02192, 2016.
[2]	Jorg Bornschein, Andriy Mnih, Daniel Zoran, and Danilo Jimenez Rezende. Variational memory
addressing in generative models. In Advances in Neural Information Processing Systems, pp.
3923-3932, 2017.
[3]	Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. arXiv
preprint arXiv:1509.00519, 2015.
[4]	Xi Chen, Diederik P Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schul-
man, Ilya Sutskever, and Pieter Abbeel. Variational lossy autoencoder. arXiv preprint
arXiv:1611.02731, 2016.
[5]	R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: A matlab-like environment for machine
learning. In BigLearn, NIPS Workshop, 2011.
[6]	Marco F Cusumano-Towner and Vikash K Mansinghka. Aide: An algorithm for measuring the
accuracy of probabilistic inference algorithms. arXiv preprint arXiv:1705.07224, 2017.
[7]	Harrison Edwards and Amos Storkey. Towards a neural statistician. arXiv preprint
arXiv:1606.02185, 2016.
[8]	Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adapta-
tion of deep networks. arXiv preprint arXiv:1703.03400, 2017.
[9]	Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, and Daan Wierstra. Draw:
A recurrent neural network for image generation. arXiv preprint arXiv:1502.04623, 2015.
[10]	Karol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka, and Daan Wierstra.
Towards conceptual compression. In Advances In Neural Information Processing Systems,
pp. 3549-3557, 2016.
[11]	Ishaan Gulrajani, Kundan Kumar, Faruk Ahmed, Adrien Ali Taiga, Francesco Visin, David
Vazquez, and Aaron Courville. Pixelvae: A latent variable model for natural images. arXiv
preprint arXiv:1611.05013, 2016.
[12]	Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
[13]	Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. In
Advances in Neural Information Processing Systems, pp. 2017-2025, 2015.
[14]	Eukasz Kaiser, Ofir Nachum, Aurko Roy, and Samy Bengio. Learning to remember rare events.
arXiv preprint arXiv:1703.03129, 2017.
[15]	Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
[16]	Gregory Koch. Siamese neural networks for one-shot image recognition. PhD thesis, University
of Toronto, 2015.
[17]	Tejas D Kulkarni, William F Whitney, Pushmeet Kohli, and Josh Tenenbaum. Deep convolu-
tional inverse graphics network. In Advances in Neural Information Processing Systems, pp.
2539-2547, 2015.
[18]	Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept
learning through probabilistic program induction. Science, 350(6266):1332-1338, 2015.
[19]	Aaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and
Koray Kavukcuoglu. Conditional image generation with pixelcnn decoders. arXiv preprint
arXiv:1606.05328, 2016.
11
Under review as a conference paper at ICLR 2018
[20]	Danilo Rezende, Ivo Danihelka, Karol Gregor, Daan Wierstra, et al. One-shot generalization
in deep generative models. In Proceedings of The 33rd International Conference on Machine
Learning ,pp.1521-1529, 2016.
[21]	Ruslan Salakhutdinov, Joshua B Tenenbaum, and Antonio Torralba. Learning with hierarchical-
deep models. IEEE transactions on pattern analysis and machine intelligence, 35(8):1958-
1971, 2013.
[22]	Tim Salimans, Diederik Kingma, and Max Welling. Markov chain monte carlo and variational
inference: Bridging the gap. In Proceedings of the 32nd International Conference on Machine
Learning (ICML-15), pp. 1218-1226, 2015.
[23]	Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lil-
licrap. One-shot learning with memory-augmented neural networks. arXiv preprint
arXiv:1605.06065, 2016.
[24]	Jake Snell, Kevin Swersky, and Richard S Zemel. Prototypical networks for few-shot learning.
arXiv preprint arXiv:1703.05175, 2017.
[25]	Eleni Triantafillou, Richard Zemel, and Raquel Urtasun. Few-shot learning through an infor-
mation retrieval lens. In Advances in Neural Information Processing Systems, pp. 2252-2262,
2017.
[26]	Oriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wierstra, et al. Matching networks for
one shot learning. In Advances in Neural Information Processing Systems, pp. 3630-3638,
2016.
12
Under review as a conference paper at ICLR 2018
6 Supplementary Material
6.1	Tightened variational bound
The likelihood lower bound in the VHE objective may also be tightened by introduction of an aux-
iliary network r(D; c, X), trained to infer which subset D ⊂ X was used in q. This meta-inference
approach was introduced in Salimans et al. (2015) to develop stochastic variational posteriors using
MCMC inference, and has recently been applied to approximate inference evaluation (Cusumano-
Towner & Mansinghka, 2017). Applied to Equation 12, this yields a modified bound for the VHE
objective
1	p(c)r(D; c, X(x) )
log P(X) ≥ X∈X q0(DEX(x)) log p(X|C)- 百log q0(D; X(χ))q(c; D)	(21)
q(c;D)
where q0(D; X) describes the stochastic sampling procedure for sampling D ⊂ X, which indeed
may itself be learned using policy gradients.
We have conducted preliminary experiments using fixed q0 and a simple functional form
r(D; c, X) = Qi r(di; c, X) “ Qi [fψ (c) ∙ ξdi], learning parameters ψ and embeddings {ξd :
d ∈ X}; however, on the Omniglot dataset we found no additional benefit over the strictly loose
bound (Equation 12). We attribute this to the already high similarity between elements of the same
Omniglot character class, allowing the approximate posterior q(c; D) to be relatively robust to dif-
ferent choices of D. However, we expect that the gain from using such a tightened objective may be
much greater for domains with lower intra-class similarity (e.g. natural images), and thus suggest
the tightened bound of Equation 21 as a direction for future research.
6.2 Variational Bound for Hierarchical Models
The resampling trick may be applied iteratively, to construct likelihood bounds over hierarchically
organised data. Expanding on Equation 12, suppose that we have collection of datasets
X = X1 t X2 t . . . t XN
(22)
For example, each X might be a different alphabet whose latent description a generates many char-
acter classes Xi , and for each of these a corresponding latent ci is used to generate many images
xij. From this perspective, we would like to learn a generative model for alphabets X of the form
p(X) =	p(a) Y	p(c|a) Y p(x|c, a)dcda
Xi⊂X	x∈Xi
(23)
Reapplying the same trick as before yields a bound taken over all elements x:
Σ
1
W
log p(X) ≥
x∈X
E	log p(x|c) -
qa(a|D1)
qc(c|D2,a)
DKL qa(a|Da) k p(a)
(24)
—
χl)|DKL(qc(CIDc,a) k P(CIa))
This suggests an analogous hierachical resampling procedure: Summing over every element x, we
can bound the log likelihood of the full hierarchy by resampling subsets Dc , Da , etc. at each level
to construct an approximate posterior. All networks are trained together by this single objective,
sampling x, Da and Dc for each gradient step. Note that this procedure need only require passing
sampled elements, rather than full classes, into the upper-level encoder qa.
13
Under review as a conference paper at ICLR 2018
6.3 Additional Results on Simple 1D distributions
Figure 9: Comparison of models trained simple 1D distributions using various alternate objec-
tives. |D | is the number of encoder inputs during training. Top row: Mean encoded information
DKL[q(c; D) k p(c)]; Second row: ∣D∣-shot generation loss — Ec〜q(c;D) log p(x0∣c); Third row:
|D |-shot binary classification error, by minimising conditional NLL Bottom row: Joint NLL (per
element) of full test set, calculated by importance weighting on 200 samples from q(c; X);
6.4 PixelCNN Omniglot Architecture
6.4. 1 Methodology
Our architecture uses a 8x28x28 latent variable c, with a full architecture detailed below. For our
classification experiments, we trained 5 models on each of the objectives (VHE, Rescale only, Re-
sample only, NS). Occasionally we found instability in optimisation, causing sudden large increases
in the training objective. When this happened, we halted and restarted training. All models were
trained for 100 epochs on 1000 characters from the training set (the remaining 200 have been used as
validation data for model selection). Finally, for each objective we selected the parameters achieving
the best training error.
Note that we did not optimise or select models based on classification performance, other than
through our development of our model’s architecture. However, we find that classification perfor-
mance is well correlated the generative training objective, as can be seen in the full table of results.
We perform classification by calculating the expected conditional likelihood under the variational
posterior: Eq(c;D) p(x|c). This is approximated using 20 samples for the outer expectation, and
importance sampling with k = 10 for the inner integral p(x|c) = Eq(t|x) qp备 P(Xlat)
To evaluate and compare log likelihood, we trained 5 more models with the same architecture, this
time on the canonical 30-20 alphabet split of Lake et al. We did not augment our training data.
Again, we split the background set into training data (25 alphabets) and validation data (5) but do
not use the validation set in training or evaluation for our final results. We estimate the total class
log likelihood by importance weighting, using k=20 importance samples of the class latent c and
k=10 importance samples of the transformation latent t for each instance.
14
Under review as a conference paper at ICLR 2018
6.4.2	Conditional Samples on Omniglot
Baseline Architecture
TT飞平午 6 6	6 & 6	H M Cf M m	皿 B V tπr k	攻	H沆次K	h.依t≡k¼粒	说初&双独
Neural Statistician ∣10] P θ fi θ e		LIlnmMCI	3 σκr k 叮 b	X	巩式”a	h公七∙h h	0 皿0 3 ⅜
、为，T二 6 θ	6 6 θ	n m ECM	叮 KBbar	A	X W W M	自幻冷启治	BViw 0 a
QeC飞飞中 θ e	θ θ θ	m M y H R	w w q<r vʧ °τ	»1	MUrfX	?5h力打加	Q_q st H ς⅛
二〉 ^t*t,*b θ 6	Bee	” tʌ M D	W (XT ⑻ k OET	Z	讯沉例,我	七七七力力	0 006 0
- 1 ” C 6 8	θ 8 θ	小 M	W or (JT CfT αr	H	讯E允h	b-Λ⅛大七	0 0s e 0
Resample Only							
TTTY <p g g	6 β G		IlrBB Q7 R	M	HHnH	h h R * k	M 0 05 q
KTPTT 6 G	GBG	CIrIBcIl	OT UT ∏Γ GCT ŋʃ	t∖h κ ∣rt M		心令刀CA	
午Tf 7T 6 6	6 G 6	h ∏ cl tn oɔ	Ur b OrR ∙r	出料5 n T		卜卜八	0 4 口 0 g
W)F ∙p l9 GG	6 e 6	CIGnGB	k CXr 07 B R	Fl		I? nt?	F gɑ0 ④
TTFy号 66	GGS	同 R cπ m R	BUraTPlIr		M KEtt	口 A8七口	g M acqe
Rescale Only							
TTTTV T 8 6	6 6 6	M M M Dl M	Or B* Qr στ ɑr	M	HKXH	ta ⅛	
TTTGT 6 θ	6 6 6	HHHMM	(XrRBcXrClr	H	M M M H	⅛ ʃtɪ ti +i	
T T TTT β 6	θ 6 B	H M M M M	ɑɪ CerBarQ	H	HMmH	卜：b ∣h> ti ta	
TTT TrP 6 θ	B θ 6	HMMCw	Or(Xrar BaT	M	MHKH	hH tɪ +i h	
TTT TT 6 6	6 6 6	Cl M M M	BBcXraerB	it	n n K M	⅛rλ ifl ti 幻；f"l	
Variational Homoencoder TTrVTR 6 6	6 6		W M Cl Cl «	k R w ger B	X	HKMH	tι h h Jj 冷	& & a 0 0
T 下 T TrV θ 6	6 θ B	W CT M M	wr CtT 3	03 {jj	H	H ttH H	心心心益公	aɑ && &
QTTTT 66	6 0 6	CT Cl H d 0	ClrBRar 3	H	乳式★ M	依心人⅛ι h	亩 0 0 8 0
飞TTTT 6 G	θ 6 6	Cl M Cl Pl tt	R BBtXrb	M	Y赳HM	^f3	& 0 & &&
∙prΓ TTT 6 6	6 6 6	M « d Cl ∏	ɑr B w B ou,		XXXx	力幻ita JUh	0 0 & & 0
TTTT> 6 6	6 & 6	PixelCNN Architecture 口抬仃门门	01ΓBVtπrk	发			HH次K	依ts¼粒	初W W忒”
Neural Statistician ①?」2力 4二 7 C Λ' UE 〉不叶年他 α r 甲 XrhtTr S q »子飞& a □百	/Ja， 而伊日 自3吟 0 c C	咫完越里也 力也M飞值 U时由旧尸 τ公Qj 四*3这3	七γsj] P B田 *¥)3 4 W 匕△何■乩」由 * P长《2 yg,4LB		A W97 井仃？W FYS乙优 小田■曲生 3 H上W	二匕A力b UE 2力 ZT知人Lh殍 U ⅛ ®</弟 。应血极，	母IU疑侬Le nɔ t÷Q 日 啊 W ^¾Γ F Qi 2 Q y 四 百 日飞多刀司
Resample Only							
EM <切小		0工3、小	，X），幻 U	也	可少承A	彳加选埠P	Q R 氯 9 ；
” cz心总0	4人	由《机	，…何T L	小必Q	=6		%府m①V	k 尺 tmS r
风T起宁）¾⅛	B 必 ɔtʃ.	Q沁.T?石3	帏S书 3佗	.例	心记Ea	口 也“ 必	5 ft.公▽ ∙∙°
国界j J N Q上	t≡> V£ ⅛	化品凶	Gkdβj，	D	y R占7	TXU在市	$ --工
2任V > g 匕"	'∈l & 甚	刍 Y ： 1/ 6	K03%。	I	大 S∙ D。	fj .-A ¾ B ⅛	* 甲△ hl 3
Rescale Only IATT二占 T B 9 R TTT⅛Ψ 806巴6 个””	，£ 0 & 6 ①飞= IJF 6 c C小 T?小D飞 & @笠5 £ Variational Homoencoder VDFe7 T T 6B&&6		Zl的n4朋 SUBCff )例匕值「 r ci m厅飞 « CiJI	σr Wr or (ɪr er σr~ b TO- I7Γ" 6 Vjr P、r ET"梦 BbW EJ- ffr 西Cirbk的 祈 eŋ" CT"	CV	岚 ιfi K & K H	上也敢M IiIMK 出 K曳岁K 於於*上	七.卜彼hh> 怜卜布自1 为卷抬起员 F除脂相益 A h,七Λ⅛性 ^7^⅛T>^⅛	门的。⅛K? 学 0∙Q Wa∙τS3∙ •^0。矶 α a<ι包或√ι ∙s⅛l 叫 ⅛⅛-tP 0 6 4.状哈0
QP1	6 C	β S 6	)M 9(Tl门	时	b στ b	It	双WHk	拈1⅞⅞穴4⅛论	&4孰。至
TT 6 G	6 G €	tι白m弓田	y PF B 及 B"	W	乳M X *	餐会振裕篇	河以a J0
TTT 牛 TL Re	6 C 6	B 札 GCIW	B 0 σσ, W* φfc	⅛	H H tt	看1寸二 h卜	μα钺-Q0
9 7T7T β S	6垃B	£ M %n风	TO OZr E CTlrCEr	凤	M扎片火	卜肉+k归和	认Q_8次X
15
Under review as a conference paper at ICLR 2018
6.4.3	Model Specification
[d] denotes a dimension d tensor. {t} denotes a set with elements of type t. Posteriors q are Gaussian.
P(C)
A PixelCNN with autoregressive weights along only the spatial (not depth) dimensions of c. We
use 2 layers of masked 64x3x3 convolutions, followed by a ReLU and two 8x1x1 convolutions
corresponding to the mean and log variance of a Gaussian posterior for the following pixel.
P(T)
t: [16] Normal(0, 1)
p(x—c,t)
c: [8x28x28], t: [16] 7→ x: [1x28x28]
Input	Operation	Output
t	Linear	t2: [6]
c,t2	Spatial Transformer	y1: [8x28x28]
y1	64x3x3 Conv; Relu; BatchNorm	y2: [64x28x28]
y2	64x3x3 Conv; Relu; BatchNorm	y3: [64x28x28]
y3	64x3x3 Conv; Relu; BatchNorm	y4: [64x28x28]
y4	64x3x3 Conv; Relu; BatchNorm	y5: [64x28x28]
y5	64x3x3 Conv; Relu; BatchNorm	y: [64x28x28]
y	PixelCNN	x: [1x28x28]
PixelCNN is gated by y, and is autoregressive along only the spatial (not depth) dimensions of c.
We use 2 layers of masked 64x3x3 convolutions, followed by a ReLU, a 2x1x1 convolution and a
softmax, corresponding to a Bernoulli distribution on the following pixel.
Q(C;D)
D: {[1x28x28]} 7→ c: [8x28x28]
Input	Operation	Output
D	STNq	Y: {[1x28x28]}
Y	Mean	y: [1x28x28]
y	16x28x28 Conv	mu: [8x28x28], logvar: [8x28x28]
Q(T;X)
x: [1x28x28] 7→t: [16]
Input	Operation	Output
x	32x3x3 Conv; 2x2 Max Pooling; ReLU; BatchNorm	y1: [32x15x15]
y1	32x3x3 Conv; 2x2 Max Pooling; ReLU; BatchNorm	y2: [32x8x8]
y2	32x3x3 Conv; 2x2 Max Pooling; ReLU; BatchNorm	y3: [32x4x4]
y3	32x3x3 Conv; 2x2 Max Pooling; ReLU; BatchNorm	y4: [32x2x2]
y4	32x3x3 Conv; 2x2 Max Pooling; ReLU; BatchNorm	y5: [32x1x1]
y5	Linear	mu: [16], logvar: [16]
16
Under review as a conference paper at ICLR 2018
Spatial Transformer STNq
x: [1x28x28] 7→y: [1x28x28]
Input	Operation	Output
x	16x3x3 Conv; 2x2 Max Pooling; ReLU; BatchNorm	y1: [16x15x15]
y1	16x3x3 Conv; 2x2 Max Pooling; ReLU; BatchNorm	y2: [16x8x8]
y2	16x3x3 Conv; 2x2 Max Pooling; ReLU; BatchNorm	y3: [16x4x4]
y3	16x3x3 Conv; 2x2 Max Pooling; ReLU; BatchNorm	y4: [16x2x2]
y4	16x3x3 Conv; 2x2 Max Pooling; ReLU; BatchNorm	y5: [16x1x1]
y5	Linear	y6: [6]
x, y6	Spatial Transformer Network	y: [1x28x28]
6.5 Hierarchical Omniglot Architecture
We extend the same architecture described in Appendix B of (Edwards & Storkey, 2016), with only
a simple modification: we introduce a new latent layer containing a 64-dimensional variable a, with
a Gaussian prior. We give p(c|a) the same functional form as p(z|c), and give q(a|Da) the same
functional form as q(c; Dc) using the shared encoder.
K) ^eɪ1 S X ≤ ʊ OT 3 ffʌ Z7 <εr HH a 6 打 H =T		ΛΛ	忙当 rʌ M		a β H ;	T I	0 7 O <4 B	个 D \/【		7/ Ih Q V		月ʌ H M b H » ɑ D V G；	7仃左 、乙三* 2 p 0				≡ Xh M HUqq 后丁 a & 4 0 口歹口口	
			: Q	Pr				g σ, . C P	sl 8 8 o 卜	g								
													Ad P『		二			
													α	√				
T	H T 口可																τ &	
^ɛ. -ɪ 1—1		今	反		仪		在加可			E	T	1 丁					r中	勺⅛
f H r±L XT J-L		M		¼	5	LU	πt	H	π	T	后	?】	LD			F	1 4	5 Ii
Frt≡-	■ X 〜/U	H		X	T	Z	忘「口			VS		0 d	V			O	已与	r X
eɔ ɑ £3 ZS		V		I	Q	A	E	σ	口仃口			I- n		d		0	X N	C? S
U	QdJXnl	X		0①			Sr	B	7	ŋ	师	/ b		Q		匕日口		D 7
ʃɪ 0 ɑj	W				D	H		Y	I	¥		T	4 4	Q	S			二 S	U P
ɔɪ	Q	X	>		5	E		飞	HTgT			EJ	Q 6	J	C		q	B N	t Q
Figure 10: 10-shot alphabet generation samples from the hierarchical model.
17
Under review as a conference paper at ICLR 2018
6.6 Conditional Samples on Silhouettes Dataset
We created a VHE using the same deconvolutional architecture as applied to omniglot, and trained
it on the Caltech-101 Silhouettes dataset. 10 object classes were held out as test data, which we use
to generate both 1-shot and 5-shot conditional samples.
忒 MrH*


Wf
^ff«ewir
W，*r
"k*Y
AA*、—
TfrTy
^T-rτ*
τ∙τ∙∙
vτ*rτ
*ττr t
τv∙rτr
?” r∙
θ√t-3A→9
得«OG
PARIF
y
—今
PK
・》心
≠<⅛
4»上


18