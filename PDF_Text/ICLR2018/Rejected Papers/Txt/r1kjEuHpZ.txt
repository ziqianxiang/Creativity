Under review as a conference paper at ICLR 2018
Learning Less-Overlapping Representations
Anonymous authors
Paper under double-blind review
Ab stract
In representation learning (RL), how to make the learned representations easy to
interpret and less overfitted to training data are two important but challenging is-
sues. To address these problems, we study a new type of regularization approach
that encourages the supports of weight vectors in RL models to have small over-
lap, by simultaneously promoting near-orthogonality among vectors and sparsity
of each vector. We apply the proposed regularizer to two models: neural networks
(NNs) and sparse coding (SC), and develop an efficient ADMM-based algorithm
for regularized SC. Experiments on various datasets demonstrate that weight vec-
tors learned under our regularizer are more interpretable and have better general-
ization performance.
1	Introduction
In representation learning (RL), two critical issues need to be considered. First, how to make the
learned representations more interpretable? Interpretability is a must in many applications. For in-
stance, in a clinical setting, when applying deep learning (DL) and machine learning (ML) models
to learn representations for patients and use the representations to assist clinical decision-making,
we need to explain the representations to physicians such that the decision-making process is trans-
parent, rather than being black-box. Second, how to avoid overfitting? It is often the case that the
learned representations yield good performance on the training data, but perform less well on the
testing data. How to improve the generalization performance on previously unseen data is important.
In this paper, we make an attempt towards addressing these two issues, via a unified approach.
DL/ML models designed for representation learning are typically parameterized with a collection of
weight vectors, each aiming at capturing a certain latent feature. For example, neural networks are
equipped with multiple layers of hidden units where each unit is parameterized by a weight vector.
In another representation learning model - sparse coding (OlshaUsen & Field, l997), a dictionary
of basis vectors are utilized to reconstruct the data. In the interpretation of RL models, a major part
is to interpret the learned weight vectors. Typically, elements of a weight vector have one-to-one
correspondence with observed featUres and a weight vector is oftentimes interpreted by examining
the top observed-featUres that correspond to the largest weights in this vector. For instance, when
applying SC to reconstrUct docUments that are represented with bag-of-words featUre vectors, each
dimension of a basis vector corresponds to one word in the vocabUlary. To visUalize/interpret a
basis vector, one can inspect the words corresponding to the large valUes in this vector. To achieve
better interpretability, varioUs constraints have been imposed on the weight vectors. Some notable
ones are: (1) Sparsity (Tibshirani, 1996) - which encoUrages most weights to be zero. Observed
featUres that have zeros weights are considered to be irrelevant and one can focUs on interpreting
a few non-zero weights. (2) Diversity (Wang et al., 2015) - which encoUrages different weight
vectors to be mUtUally “different” (e.g., having larger angles (Xie et al., 2015)). By doing this,
the redUndancy among weight vectors is redUced and cognitively one can map each weight vector
to a physical concept in a more UnambigUoUs way. (3) Non-negativeness (Lee & SeUng, 1999)
- which encoUrages the weights to be nonnegative since in certain scenarios (e.g., bag of words
representation of docUments), it is difficUlt to make sense of negative weights. In this paper, we
propose a new perspective of interpretability: less-overlapness, which encoUrages the weight vectors
to have small overlap in sUpports1 . By doing this, each weight vector is anchored on a UniqUe
sUbset of observed featUres withoUt being redUndant with other vectors, which greatly facilitates
interpretation. For example, if topic models (Blei et al., 2003) are learned in sUch a way, each topic
1The sUpport of a vector is the set of indices of nonzero entries in this vector.
1
Under review as a conference paper at ICLR 2018
(a)	(b)	(c)
Figure 1: (a) Under L1 regularization, the vectors are sparse, but their supports are overlapped; (b) Under LDD
regularization, the vectors are orthogonal, but their supports are overlapped; (c) Under LDD-L1 regularization,
the vectors are sparse and mutually orthogonal and their supports are not overlapped.
will be characterized by a few representative words and the representative words of different topics
are different. Such topics are more amenable for interpretation. Besides improving interpretability,
less-overlapness helps alleviate overfitting. It imposes a structural constraint over the weight vectors,
thus can effectively shrink the complexity of the function class induced by the RL models and
improve the generalization performance on unseen data.
To encourage less-overlapness, we propose a regularizer that simultaneously encourages different
weight vectors to be close to being orthogonal and each vector to be sparse, which jointly encourage
vectors’ supports to have small overlap. The major contributions of this work include:
•	We propose a new type of regularization approach which encourages less-overlapness, for
the sake of improving interpretability and reducing overfitting.
•	We apply the proposed regularizer to two models: neural networks and sparse coding (SC),
and derive an efficient ADMM-based algorithm for the regularized SC problem.
•	In experiments, we demonstrate the empirical effectiveness of this regularizer.
2	Methods
In this section, we propose a nonoverlapness-promoting regularizer and apply it to two models.
2.1	Nonoverlapness-Promoting Regularization
We assume the model is parameterized by m vectors W = {wi}im=1. For a vector w, its support
s(w) is defined as {i | wi = 0} - the indices of nonzero entries in w. We first define a score o(wi, Wj)
to measure the overlap between two vectors:
可 Wi , Wj )=
Is(Wi) ∩ S(Wj X
Is(Wi) ∪ S(Wj )|
(1)
which is the JaCCard index of their supports. The smaller o(Wi, Wj) is, the less overlapped the two
vectors are. For m vector, the overlap score is defined as the sum of pairwise scores
1m
O(W) = m(m-i) X oW Wj).	⑵
i6=j
This score function is not smooth, which will result in great difficulty for optimization if used as a
regularizer. Instead, we propose a smooth function that is motivated from O(Wi, Wj) and can achieve
a similar effect as o(W). The basic idea is: to encourage small overlap, we can encourage (1) each
vector has a small number of non-zero entries and (2) the intersection of supports among vectors is
small. To realize (1), we use an L1 regularizer to encourage the vectors to be sparse. To realize (2),
we encourage the vectors to be close to being orthogonal. For two sparse vectors, if they are close
to orthogonal, then their supports are landed on different positions. As a result, the intersection of
supports is small.
We follow the method proposed by Xie et al. (2017b) to promote orthogonality. To encourage two
vectors Wi and Wj to be close to being orthogonal, one can make their `2 norm kWi k2 , kWj k2 close
2
Under review as a conference paper at ICLR 2018
to one and their inner product wi>wj close to zero. Based on this, one can promote orthogonality
among a set of vectors by encouraging the Gram matrix G (Gij = wi>wj) of these vectors to be
close to an identity matrix I. Since wi>wj and zero are off the diagonal of G and I respectively, and
kwi k22 and one are on the diagonal of G and I respectively, encouraging G close to I essentially
makes wi> wj close to zero and kwi k2 close to one. As a result, wi and wj are encouraged to
be close to being orthogonal. In (Xie et al., 2017b), one way proposed to measure the “closeness”
between two matrices is to use the log-determinant divergence (LDD) (Kulis et al., 2009). The LDD
between two m × m positive definite matrices X and Y is defined as D(X, Y) = tr(XY-1) -
logdet(XYT) - m where tr(∙) denotes matrix trace. The closeness between G and I can be
achieved by encouraging their LDD D(G, I) = tr(G) - log det(G) - m to be small.
Combining the orthogonality-promoting LDD regularizer with the sparsity-promoting L1 regularizer
together, we obtain the following LDD-L1 regularizer
m
Ω(W) = tr(G) - Iogdet(G) + Y X 皿山	⑶
i=1
where γ is a tradeoff parameter between these two regularizers. As verified in experiments, this reg-
ularizer can effectively promote non-overlapness. The formal analysis of the relationship between
Eq.(3) and Eq.(2) will be left for future study. It is worth noting that either L1 or LDD alone is not
sufficient to reduce overlap. As illustrated in Figure 1(a) where only L1 is applied, though the two
vectors are sparse, their supports are completely overlapped. In Figure 1(b) where the LDD regu-
larizer is applied, though the two vectors are very close to orthogonal, their supports are completely
overlapped since they are dense. In Figure 1(c) where the LDD-L1 regularizer is used, the two
vectors are sparse and are close to being orthogonal. As a result, their supports are not overlapped.
2.2 Case Studies
In this section, we apply the LDD-L1 regularizer to two models.
Neural Networks In a neural network (NN) with L hidden layers, each hidden layer l is equipped
with m(l) units and each unit i is connected with all units in layer l - 1. Hidden unit i at layer
l is parameterized by a weight vector wi(l) . These hidden units aim at capturing latent features
underlying data. For m(l) weight vectors W(l) = {wi(l)}im=(1l) in each layer l, we apply the LDD-L1
regularizer to encourage them to have small overlap. An LDD-L1 regularized NN problem (LDD-
L1-NN) can be defined in the following way:
{牌L	L({W⑷}L=ι) + λPL=ι Ω(W(I))
where L({W(l)}lL=1) is the objective function of this NN.
Sparse Coding Given n data samples X ∈ Rd×n where d is the feature dimension, we aim to
use a dictionary of basis vectors W ∈ Rd×m to reconstruct X, where m is the number of basis
vectors. Each data sample x is reconstructed by taking a sparse linear combination of the basis
vectors x ≈ Pjm=1 αjwj, where {αj}jm=1 are the linear coefficients and most of them are zero. The
reconstruction error is measured using the squared L2 norm kx-Pjm=1 αjwj k22. To achieve sparsity
among the codes, L1 regularization is utilized: Pjm=I ∣αj|i. To avoid the degenerated case where
most coefficients are zero and the basis vectors are of large magnitude, L2 regularization is applied
to the basis vectors: kwj k22. We apply the LDD-L1 regularizer to encourage the supports of basis
vectors to have small overlap. Putting these pieces together, we obtain the LDD-L1 regularized SC
(LDD-L1-SC) problem
minw,A 1 kX - WAkF + λι∣A∣ι + ⅜2∣WkF + ⅛23(tr(W>W) - logdet(W>W)) + λ4∣W∣1
(4)
where A ∈ Rm×n denotes all the linear coefficients.
3
Under review as a conference paper at ICLR 2018
Algorithm 1 Algorithm for solving the LDD-L1-SC problem
Initialize W and A
repeat
Update A with W being fixed, by solving n Lasso problems defined in Eq.(6).
repeat
Update W by solving the Lasso problem defined in Eq.(10)
^	-~~■.
U — U + (W - W)
repeat
for i J 1 to n do
Update the ith column vector wi of W using Eq.(23)
end for
until convergence of the problem defined in Eq.(12)
until convergence of the problem defined in Eq.(9)
until convergence of the problem defined in Eq.(4)
3 Algorithm
For LDD-L1-NNs, a simple subgradient descent algorithm is applied to learn the weight parameters.
For LDD-L1-SC, we solve it by alternating between A and W: (1) updating A with W fixed; (2)
updating W with A fixed. These two steps alternate until convergence. With W fixed, the sub-
problem defined over A is
minA 2∣∣X - WAkF + λι∣A∣ι	(5)
which can be decomposed into n Lasso problems: for i = 1,… ,n
minai 1 ∣∣xi — Waiki + λι∣a∕ι	(6)
where ai is the coefficient vector of the i-th sample. Lasso can be solved by many algorithms, such
as proximal gradient descent (PGD) (Parikh & Boyd, 2014). Fixing A, the sub-problem defined
over W is:
minw 1 ∣X — WAkF + λ22 ∣W∣F + λ23 (tr(W>W) — logdet(W>W)) + λ4∣W∣1. (7)
We solve this problem using an ADMM-based algorithm. First, we write the problem into an equiv-
alent form
minw i∣X -WAkF + λi2kW∣F + λ3(tr(W>W) - logdet(W>W)) + λ4∣W|i
s.t. W = Wf
Then we write down the augmented Lagrangian function
ikX - WAkF + λi2kWkF + λ3(tr(W>W) - logdet(W>W)) + λ4∣W|i
+ hU, W - W i + P ∣w - W kF.	㈠
We minimize this Lagrangian function by alternating among W, U and W.
bT 一 U E， 1 一 … I √7;.
Update W The subproblem defined on W is
minw λ4∣W |i - (U, W)+ 2 k W - W kF	(10)
which is a Lasso problem and can be solved using PGD (Parikh & Boyd, 2014).
Update U The update equation of U is simple.
-~~^.
U = U + (W -Wf)	(11)
The subproblem defined on W is
minw 1 kX - WAkIF + ^IWkF + λ(tr(W>W) - logdet(W>W)) + (U, Wi
+P kW - W kF	( )
which can be solved using a coordinate descent algorithm. The derivation is given in the next
subsection.
4
Under review as a conference paper at ICLR 2018
3.1 COORDINATE DESCENT ALGORITHM FOR LEARNING W
In each iteration of the CD algorithm, one basis vector is chosen for update while the others are
fixed. Without loss of generality, we assume it is w1. The sub-problem defined over w1 is
n
minwι	2 P ∣∣Xi — Pm=2 aiiwι - aiiw"∣2 + λ⅛λ3∣∣wι∣∣2
i=1
-λ3logdet(W>W) + u>wι + 2∣∣wι - Wι∣2
(13)
To obtain the optimal solution, we take the derivative of the objective function and set it to zero.
First, we discuss how to compute the derivative of logdet(W>W) w.r.t w1. According to the chain
rule, we have
d logdet(WT W) = 2W(W>W)-11
∂w1	:,1
where (W>W)-ι1 denotes the first column of (W>W)-1. Let W-ι = [w2,…，Wm], then
w>w = [w>wι w>W-1
W W = ∣W>1w1 W>1W-1
According to the inverse of block matrix
A B -1	Ae	Be
C D = Ce De
(14)
(15)
(16)
where Ae = (A - BD-1C)-1, Be = -(A - BD-1C)-1BD-1, Ce = -D-1C(A - BD-1C)-1,
De = D-1 + D-1C(A - BD-1C)-1BD-1, we have (W>W):-,11 equals [a b>]> where
a = (w>w1 — w>W-1(W>1W-1)-1W>1w1)-1	(17)
b = -(W>1W-1)-1 W>1w1a	(18)
Then
W(W>W)-11 = [wι w-ι] [a] = WMMw7.	(19)
where
M = I — W-ι(W>1W-ι)-1W>1.	(20)
To this end, we obtain the full gradient of the objective function in Eq.(13):
Pn=I ai1(ai1w1 + Pm=2 ailwl - Xi) + (λ2 + λ3)w1 - λ3wMMww- + P(WI - WI) + u∙	(21)
Setting the gradient to zero, we get
((Pn=I a21 + λ2 + λ3 + P)I — λ3M∕(w>Mw1))w1 = Pn=I aii(xi — Pm=2 aiwι) - U + PWι.
(22)
Let γ = w1>Mw1, c = in=1 ai21 + λ2 + λ3 + P, b = in=1 ai1(Xi - ιm=2 aiιwι) - u + Pwe j,
then (CI — λγ3M)wι = b and wι = (cI 一 λ3M)Tb. Let UΣU> be the eigen decomposition of
M, we have
w1 = γU(γcI - λ3Σ)-1U>b.	(23)
Then
w1> Mw1
= γ2b>U(γcI - λ3Σ)-1U>UΣU>U(γcI - λ3Σ)-1U>b
= γ2b>U(γcI - λ3Σ)-1Σ(γcI - λ3Σ)-1U>b
=Y P (UT b)S ςSs =勺
=Y ʌ- (rc-λ3∑ss)2 = Y
s=1
(24)
The matrix A = W-I(W>1W-1)-1W>1 is idempotent, i.e., AA = A, and its rank is m - 1.
According to the property of idempotent matrix, the first m - 1 eigenvalues of A equal to one and
5
Under review as a conference paper at ICLR 2018
the rest equal to zero. Thereafter, the first m - 1 eigenvalues of M = I - A equal to zero and the
rest equal to one. Based on this property, Eq.(24) can be simplified as
d
γX
s=m
(U>b)2
(rc - λ3)2
(25)
1
After simplification, it is a quadratic function where γ has a closed form solution. Then we plug the
solution of γ into Eq.(23) to get the solution of w1.
4 Experiments
In these section, we present experimental results. The studies were performed on three mod-
els: sparse coding (SC) for document modeling, long short-term memory (LSTM) (Hochre-
iter & Schmidhuber, 1997) network for language modeling and convolutional neural network
(CNN) (Krizhevsky et al., 2012) for image classification.
4.1	Datasets
We used four datasets. The SC experiments were conducted on two text datasets: 20-Newsgroups2
(20-News) and Reuters Corpus3 Volume 1 (RCV1). The 20-News dataset contains newsgroup doc-
uments belonging to 20 categories, where 11314, 3766 and 3766 documents were used for training,
validation and testing respectively. The original RCV1 dataset contains documents belonging to
103 categories. Following (Cai & He, 2012), we chose the largest 4 categories which contain 9625
documents, to carry out the study. The number of training, validation and testing documents are
5775, 1925, 1925 respectively. For both datasets, stopwords were removed and all words were
changed into lower-case. Top 1000 words with the highest document frequency were selected to
form the vocabulary. We used tf-idf to represent documents and the feature vector of each document
is normalized to have unit L2 norm.
The LSTM experiments were conducted on the Penn Treebank (PTB) dataset (Marcus et al., 1993),
which consists of 923K training, 73K validation, and 82K test words. Following (Mikolov et al.),
top 10K words with highest frequency were selected to form the vocabulary. All other words are
replaced with a special token UNK.
The CNN experiments were performed on the CIFAR-10 dataset4. It consists of 32x32 color images
belonging to 10 categories, where 50,000 images were used for training and 10,000 for testing.
5000 training images were used as the validation set for hyperparameter tuning. We augmented
the dataset by first zero-padding the images with 4 pixels on each side, then randomly cropping the
padded images to reproduce 32x32 images.
4.2	LDD-L1 and Non-overlapness
First of all, we verify whether the LDD-L1 regularizer is able to promote non-overlapness. The
study is performed on the SC model and the 20-News dataset. The number of basis vectors was
set to 50. For 5 choices of the regularization parameter of LDD-L1: {10-4,10-3,…，1}, we ran
the LDD-L1-SC model until convergence and measured the overlap score (defined in Eq.2) of the
basis vectors. The tradeoff parameter γ inside LDD-L1 is set to 1. Figure 2 shows that the overlap
score consistently decreases as the regularization parameter of LDD-L1 increases, which implies
that LDD-L1 can effectively encourage non-overlapness. As a comparison, we replaced LDD-L1
with LDD-only and L1-only, and measured the overlap scores. As can be seen, for LDD-only, the
overlap score remains to be 1 when the regularization parameter increases, which indicates that
LDD alone is not able to reduce overlap. This is because under LDD-only, the vectors remain dense,
which renders their supports to be completely overlapped. Under the same regularization parameter,
LDD-L1 achieves lower overlap score than L1, which suggests that LDD-L1 is more effective in
promoting non-oveHaPness. Given that Y - the tradeoff parameter associated with the L1 norm in
2http://qwone.com/〜jason/20Newsgroups/
3http://www.daviddlewis.com/resources/testcollections/rcv1/
4https://www.cs.toronto.edu/~kriz/cifar.html
6
Under review as a conference paper at ICLR 2018
Figure 2: Overlap score versus the regularization parameter
Vector	RePresentative Words
1	crime, guns
2	faith, trust
3
4
5
6
7
8
9
worked, manager
weapons, citizens
board, Uiuc
application, performance, ideas
service, quality
bible, moral
christ, jews, land, faq
Table 1: Representative words of 9 exemplar basis vectors
LDD-LI - is set to 1, the same regularization parameter λ imposes the same level of sparsity for
both LDD-L1 and L1-only. Since LDD-L1 encourages the vectors to be mutually orthogonal, the
intersection between vectors’ supports is small, which consequently results in small overlap. This is
not the case for L1-only, which hence is less effective in reducing overlap.
4.3	Interpretability
In this section, we examine whether the weight vectors learned under LDD-L1 regularization are
more interpretable, using SC as a study case. For each basis vector w learned by LDD-L1-SC on
the 20-News dataset, we use the words (referred to as representative words) that correspond to the
supports of w to interpret w. Table 1 shows the representative words of 9 exemplar vectors. By
analyzing the representative words, we can see vector 1-9 represent the following semantics respec-
tively: crime, faith, job, war, university, research, service, religion and Jews. The representative
words of these vectors have no overlap. As a result, it is easy to associate each vector with a unique
concept, in other words, easy to interpret. Figure 3 visualizes the learned vectors where the black
dots denote vectors’ supports. As can be seen, the supports of different basis vectors are landed over
different words and their overlap is very small.
4.4	Reducing Overfitting
In this section, we verify whether LDD-L1 is able to reduce overfitting. The studies were performed
on SC, LSTM and CNN. In each experiment, the hyperparameters were tuned on the validation set.
Sparse Coding For 20-News, the number of basis vectors in LDD-L1-SC is set to 50. λ1, λ2,
λ3 and λ4 are set to 1, 1, 0.1 and 0.001 respectively. For RCV1, the number of basis vectors is
set to 200. λ1, λ2, λ3 and λ4 are set to 0.01, 1, 1 and 1 respectively. We compared LDD-L1 with
LDD-only and L1-only.
7
Under review as a conference paper at ICLR 2018
Figure 3: Visualization of basis vectors
Method		20-NeWS				RCV1	
-	Test	Gap between train and test	Test	Gap between train and test
SC	0.592	0∏9	0.872	0.009
LDD-SC	0.605	0Γ08	0.886	0.005
L1-SC	0.606	0.105	0.897	0.005
LDD-L1-SC	0.612		0.099		0.909		-0.015
Table 2: Classification accuracy on the test sets of 20-News and RCV1, and the gap between training
and test accuracy.
To evaluate the model performance quantitatively, we applied the dictionary learned on the training
data to infer the linear coefficients (A in Eq.4) of test documents, then performed k-nearest neigh-
bors (KNN) classification on A. Table 2 shows the classification accuracy on test sets of 20-News
and RCV1 and the gap5 between the accuracy on training and test sets. Without regularization,
SC achieves a test accuracy of 0.592 on 20-News, which is lower than the training accuracy by
0.119. This suggests that an overfitting to training data occurs. With LDD-L1 regularization, the
test accuracy is improved to 0.612 and the gap between training and test accuracy is reduced to
0.099, demonstrating the ability of LDD-L1 in alleviating overfitting. Though LDD alone and L1
alone improve test accuracy and reduce train/test gap, they perform less well than LDD-L1, which
indicates that for overfitting reduction, encouraging non-overlapness is more effective than solely
promoting orthogonality or solely promoting sparsity. Similar observations are made on the RCV1
dataset. Interestingly, the test accuracy achieved by LDD-L1-SC on RCV1 is even better than the
training accuracy.
LSTM for Language Modeling The LSTM network architecture follows the word language
model (PytorchTM) provided in Pytorch6. The number of hidden layers is set to 2. The embed-
ding size is 1500. The size of hidden state is 1500. The word embedding and softmax weights are
tied. The number of training epochs is 40. Dropout with 0.65 is used. The initial learning rate is
20. Gradient clipping threshold is 0.25. The size of mini-batch is 20. In LSTM training, the net-
work is unrolled for 35 iterations. Perplexity is used for evaluating language modeling performance
(lower is better). The weight parameters are initialized uniformly between [-0.1, 0.1]. The bias
parameters are initialized as 0. We compare with the following regularizers: (1) L1 regularizer; (2)
orthogonality-promoting regularizers based on cosine similarity (CS) (Yu et al., 2011), incoherence
(IC) (Bao et al., 2013), mutual angle (MA) (Xie et al., 2015), decorrelation (DC) (Cogswell et al.,
2015), angular constraint (AC) (Xie et al., 2017a) and LDD (Xie et al., 2017b).
Table 3 shows the perplexity on the PTB test set. Without regularization, PytorchLM achieves a
perplexity of 72.3. With LDD-L1 regularization, the perplexity is significantly reduced to 71.1.
This shows that LDD-L1 can effectively improve generalization performance. Compared with the
sparsity-promoting L1 regularizer and orthogonality-promoting regularizers, LDD-LI - which pro-
motes non-overlapness by simultaneously promoting sparsity and orthogonality - achieves lower
perplexity. For the convenience of readers, we also list the perplexity achieved by other state of
the art deep learning models. The LDD-L1 regularizer can be applied to these models as well to
potentially boost their performance.
5Training accuracy minus test accuracy.
6https://github.com/pytorch/examples/tree/master/word_language_model
8
Under review as a conference paper at ICLR 2018
Network	Test
RNN (Mikolov & Zweig, 2012)	124.7
RNN+LDA (Mikolov & Zweig, 2012)	113.7
Deep RNN (Pascanu et al., 2013)	107.5
Sum-Product Network (Cheng et al., 2014)	100.0
RNN+LDA+KN-5+Cache (Mikolov & Zweig, 2012)	92.0
LSTM (medium) (Zaremba et al., 2014)	82.7
CharCNN (Kim et al., 2016)	78.9
LSTM (large) (Zaremba et al., 2014)	78.4
Variational LSTM with MC Dropout (Gal & Ghahramani, 2016)	73.4
PytOrChLM	72.3
CS-PytorchLM (Yu et al., 2011)	71.8
IC-PytorchLM (Bao et al., 2013)	71.9
MA-PytorchLM (Xie et al., 2015)	72.0
DC-PytorchLM (Cogswell et al., 2015)	72.2
AC-PytorchLM (Xie et al., 2017a)	71.5
LDD-PytorchLM (Xie et al., 2017b)	71.6
L1-PytorchLM	71.8
LDD-LI-PytOrChLM		71.1
Pointer Sentinel LSTM (Merity et al., 2016)	70.9
Ensemble of 38 Large LSTMs (Zaremba et al., 2014)	68.7
Ensemble of 10 Large Variational LSTMs (Gal & Ghahramani, 2016)	68.7
Variational RHN (Zilly et al., 2016)	68.5
Variational LSTM +REAL (Inan et al., 2016)	68.5
Neural Architecture Search (Zoph & Le, 2016)	67.9
Variational RHN +RE (Inan et 1 2016; Zilly et al., 2016)	66.0
Variational RHN + WT (Zilly et al., 2016)	65.4
Variational RHN + WT with MC dropout (Zilly et al., 2016)	64.4
Neural Architecture Search + WTV1 (Zoph & Le, 2016)	64.0
Neural Architecture Search + WT V2 (Zoph & Le, 2016)	62.4
Table 3: Word-level perplexities on PTB test set
CNN for Image Classification The CNN architecture follows that of the wide residual network
(WideResNet) (Zagoruyko & Komodakis, 2016). The depth and width are set to 28 and 10 respec-
tively. The networks are trained using SGD, where the epoch number is 200, the learning rate is
set to 0.1 initially and is dropped by 0.2 at 60, 120 and 160 epochs, the minibatch size is 128 and
the Nesterov momentum is 0.9. The dropout probability is 0.3 and the L2 weight decay is 0.0005.
Model performance is measured using error rate, which is the median of 5 runs. We compared with
(1) L1 regularizer; (2) orthogonality-promoting regularizers including CS, IC, MA, DC, AC, LDD
and one based on locally constrained decorrelation (LCD) (Rodriguez et al., 2016).
Table 4 shows classification errors on CIFAR-10 test set. Compared with the unregularized
WideResNet which achieves an error rate of 3.89%, the proposed LDD-L1 regularizer greatly re-
duces the error to 3.60%. LDD-L1 outperforms the L1 regularizer and orthogonality-promoting
regularizers, demonstrating that encouraging non-overlapness is more effective than encouraging
sparsity alone or orthogonality alone in reducing overfitting. The error rates achieved by other state
of the art methods are also listed.
5	Related Works
The interpretation of representation learning models has been widely studied. Choi et al. (2016) de-
velop a two-level neural attention model that detects influential variables in a reverse time order and
use these variables to interpret predictions. Lipton (2016) discuss a taxonomy of both the desiderata
and methods in interpretability research. Koh & Liang (2017) propose to use influence functions
to trace a model’s prediction back to its training data and identify training examples that are most
relevant to a prediction. Dong et al. (2017) integrate topics extracted from human descriptions into
neural networks via an interpretive loss and then use a prediction-difference maximization algo-
9
Under review as a conference paper at ICLR 2018
Network	Error
Maxout (GoodfelloWetaL,2013)	9.38
NiN (Lin et al., 2013)	8.81
DSN (Lee et al., 2015)	7.97
Highway Network (Srivastava et al., 2015)	7.60
All-CNN (Springenberg et al., 2014)	7.25
ResNet (He et al., 2016)	6.61
ELU-Network (Clevert et al., 2015)	6.55
LSUV (Mishkin & Matas, 2015)	5.84
Fract. Max-Pooling (Graham, 2014)	4.50
WideResNet (HUang et al., 2016)	3.89
CS-WideResNet (Yu et al., 2011)	3.81
IC-WideResNet (Bao et al., 2013)	3.85
MA-WideResNet (Xie et al., 2015)	3.68
DC-WideResNet (Cogswell et al., 2015)	3.77
LCD-WideResNet (Rodriguez et al., 2016)	3.69
AC-WideResNet (Xie et al., 2017a)	3.63
LDD-WideResNet (Xie et al., 2017b)	3.65
L1-WideResNet	3.81
LDD-LI-WideResNet	3.60
ResNeXt (XieetaL,2016)	3.58
PyramidNet (Huang et al., 2016)	3.48
DenseNet (Huang et al., 2016)	3.46
PyramidSePDroP (Yamada et al., 2016)	3.31
Table 4: Classification error (%) on CIFAR-10 test set
rithm to interpret the learned features of each neuron. Our method is orthogonal to these existing
approaches and can be potentially used with them together to further improve interpretability.
6	Conclusions
In this paper, we propose a new type of regularization approach that encourages the weight vectors
to have less-overlapped supports. The proposed LDD-L1 regularizer simultaneously encourages the
weight vectors to be sparse and close to being orthogonal, which jointly produces the effects of less
overlap. We apply this regularizer to two models: neural networks and sparse coding (SC), and
derive an efficient ADMM-based algorithm for solving the regularized SC problem. Experiments
on various datasets demonstrate the effectiveness of this regularizer in alleviating overfitting and
improving interpretability.
References
Yebo Bao, Hui Jiang, Lirong Dai, and Cong Liu. Incoherent training of deep neural networks to
decorrelate bottleneck features for speech recognition. In 2013 IEEE International Conference
on Acoustics, Speech and Signal Processing, 2013.
David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. the Journal of
machine Learning research, 2003.
Deng Cai and Xiaofei He. Manifold adaptive experimental design for text categorization. IEEE
Transactions on Knowledge and Data Engineering, 24(4):707-719, 2012.
Wei-Chen Cheng, Stanley Kok, Hoai Vu Pham, Hai Leong Chieu, and Kian Ming A Chai. Language
modeling with sum-product networks. In Fifteenth Annual Conference of the International Speech
Communication Association, 2014.
Edward Choi, Mohammad Taha Bahadori, Jimeng Sun, Joshua Kulas, Andy Schuetz, and Walter
Stewart. Retain: An interpretable predictive model for healthcare using reverse time attention
mechanism. In Advances in Neural Information Processing Systems, pp. 3504-3512, 2016.
10
Under review as a conference paper at ICLR 2018
Djork-Ame Clevert, Thomas Unterthiner, and SePP Hochreiter. Fast and accurate deep network
learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.
Michael Cogswell, Faruk Ahmed, Ross Girshick, Larry Zitnick, and Dhruv Batra. Reducing overfit-
ting in deeP networks by decorrelating rePresentations. arXiv preprint arXiv:1511.06068, 2015.
YinPeng Dong, Hang Su, Jun Zhu, and Bo Zhang. ImProving interPretability of deeP neural net-
works with semantic information. arXiv preprint arXiv:1703.04096, 2017.
Yarin Gal and Zoubin Ghahramani. A theoretically grounded aPPlication of droPout in recurrent
neural networks. In Advances in Neural Information Processing Systems, pp. 1019-1027, 2016.
Ian J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout
networks. ICML, 2013.
Benjamin Graham. Fractional max-pooling. arXiv preprint arXiv:1412.6071, 2014.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
770-778, 2016.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected
convolutional networks. arXiv preprint arXiv:1608.06993, 2016.
Hakan Inan, Khashayar Khosravi, and Richard Socher. Tying word vectors and word classifiers: A
loss framework for language modeling. arXiv preprint arXiv:1611.01462, 2016.
Yoon Kim, Yacine Jernite, David Sontag, and Alexander M Rush. Character-aware neural language
models. In Thirtieth AAAI Conference on Artificial Intelligence, 2016.
Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. Pro-
ceedings of the 34th International Conference on Machine Learning, 2017.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, 2012.
Brian Kulis, MatyaS A Sustik, and Inderjit S Dhillon. Low-rank kernel learning with bregman matrix
divergences. Journal of Machine Learning Research, 10(Feb):341-376, 2009.
Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu. Deeply-
supervised nets. In Artificial Intelligence and Statistics, pp. 562-570, 2015.
Daniel D Lee and H Sebastian Seung. Learning the parts of objects by non-negative matrix factor-
ization. Nature, 401(6755):788-791, 1999.
Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400,
2013.
Zachary C Lipton. The mythos of model interpretability. arXiv preprint arXiv:1606.03490, 2016.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated
corpus of english: The penn treebank. Computational linguistics, 19(2):313-330, 1993.
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture
models. arXiv preprint arXiv:1609.07843, 2016.
Tomas Mikolov and Geoffrey Zweig. Context dependent recurrent neural network language model.
2012.
Tomas Mikolov, Martin Karafiat, and Lukas Burget. Recurrent neural network based language
model.
11
Under review as a conference paper at ICLR 2018
Dmytro Mishkin and Jiri Matas. All you need is a good init. arXiv preprint arXiv:1511.06422,
2015.
Bruno A Olshausen and David J Field. Sparse coding with an overcomplete basis set: A strategy
employed by v1? Vision research, 1997.
Neal Parikh and Stephen Boyd. Proximal algorithms. Foundations and Trends in Optimization, 1
(3):127-239, 2014.
Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. How to construct deep
recurrent neural networks. arXiv preprint arXiv:1312.6026, 2013.
PaU Rodr´guez, Jordi Gonzalez, Guillem Cucurull, Josep M Gonfaus, and Xavier Roca. Regularizing
cnns with locally constrained decorrelations. arXiv preprint arXiv:1611.01967, 2016.
Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for
simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014.
Rupesh Kumar Srivastava, Klaus Greff, and JUrgen Schmidhuber. Highway networks. arXivpreprint
arXiv:1505.00387, 2015.
Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
Society. Series B (Methodological), pp. 267-288, 1996.
Yichen Wang, Robert Chen, Joydeep Ghosh, Joshua C Denny, Abel Kho, You Chen, Bradley A
Malin, and Jimeng Sun. Rubik: Knowledge guided tensor factorization and completion for health
data analytics. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, pp. 1265-1274. ACM, 2015.
Pengtao Xie, Yuntian Deng, and Eric P. Xing. Diversifying restricted boltzmann machine for docu-
ment modeling. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2015.
Pengtao Xie, Yuntian Deng, Yi Zhou, Abhimanu Kumar, Yaoliang Yu, James Zou, and Eric P. Xing.
Learning latent space models with angular constraints. In Proceedings of the 34th International
Conference on Machine Learning, pp. 3799-3810, 2017a.
Pengtao Xie, Barnabas Poczos, and Eric P Xing. Near-orthogonality regularization in kernel meth-
ods. 2017b.
Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. Aggregated residual trans-
formations for deep neural networks. arXiv preprint arXiv:1611.05431, 2016.
Yoshihiro Yamada, Masakazu Iwamura, and Koichi Kise. Deep pyramidal residual networks with
separated stochastic depth. arXiv preprint arXiv:1612.01230, 2016.
Yang Yu, Yu-Feng Li, and Zhi-Hua Zhou. Diversity regularized machine. In International Joint
Conference on Artificial Intelligence. Citeseer, 2011.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint
arXiv:1605.07146, 2016.
Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization.
arXiv preprint arXiv:1409.2329, 2014.
Julian Georg Zilly, Rupesh Kumar Srivastava, Jan Koutnlk, and JUrgen Schmidhuber. Recurrent
highway networks. arXiv preprint arXiv:1607.03474, 2016.
Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint
arXiv:1611.01578, 2016.
12