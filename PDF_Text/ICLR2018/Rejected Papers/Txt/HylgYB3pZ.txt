Linearly Constrained Weights: Resolving the
Vanishing Gradient Problem by Reducing An-
gle Bias
Takuro Kutsuna
Toyota Central R&D Labs, Inc.
Nagakute, Aichi 480-1192, Japan
kutsuna@mosk.tytlabs.co.jp
Ab stract
In this paper, we first identify angle bias, a simple but remarkable phenomenon
that causes the vanishing gradient problem in a multilayer perceptron (MLP)
with sigmoid activation functions. We then propose linearly constrained weights
(LCW) to reduce the angle bias in a neural network, so as to train the network
under the constraints that the sum of the elements of each weight vector is zero. A
reparameterization technique is presented to efficiently train a model with LCW
by embedding the constraints on weight vectors into the structure of the network.
Interestingly, batch normalization (Ioffe & Szegedy, 2015) can be viewed as a
mechanism to correct angle bias. Preliminary experiments show that LCW helps
train a 100-layered MLP more efficiently than does batch normalization.
1 Introduction
Neural networks with a single hidden layer have been shown to be universal approximators (Hornik
et al., 1989; Irie & Miyake, 1988). However, an exponential number of neurons may be necessary
to approximate complex functions. A solution to this problem is to use more hidden layers. The
representation power of a network increases exponentially with the addition of layers (Telgarsky,
2016; Eldan & Shamir, 2016). A major obstacle in training deep nets, that is, neural networks with
many hidden layers, is the vanishing gradient problem. Various techniques have been proposed for
training deep nets, such as layer-wise pretraining (Hinton & Salakhutdinov, 2006), rectified linear
units (Nair & Hinton, 2010; Jarrett et al., 2009), variance-preserving initialization (Glorot & Bengio,
2010), and normalization layers (Ioffe & Szegedy, 2015; GuIcehre & Bengio, 2016).
In this paper, we first identify the angle bias that arises in the dot product of a nonzero vector and
a random vector. The mean of the dot product depends on the angle between the nonzero vector
and the mean vector of the random vector. We show that this simple phenomenon is a key cause of
the vanishing gradient in a multilayer perceptron (MLP) with sigmoid activation functions. We then
propose the use of so-called linearly constrained weights (LCW) to reduce the angle bias in a neural
network. LCW is a weight vector subject to the constraint that the sum of its elements is zero. A
reparameterization technique is presented to embed the constraints on weight vectors into the struc-
ture of a neural network. This enables us to train a neural network with LCW by using optimization
solvers for unconstrained problems, such as stochastic gradient descent. Preliminary experiments
show that we can train a 100-layered MLP with sigmoid activation functions by reducing the angle
bias in the network. Interestingly, batch normalization (Ioffe & Szegedy, 2015) can be viewed as a
mechanism to correct angle bias in a neural network, although it was originally developed to over-
come another problem, that is, the internal covariate shift problem. Preliminary experiments suggest
that LCW helps train deep MLPs more efficiently than does batch normalization.
In Section 2, we define angle bias and discuss its relation to the vanishing gradient problem. In
Section 3, we propose LCW as an approach to reduce angle bias in a neural network. We also
present a reparameterization technique to efficiently train a model with LCW and an initialization
method for LCW. In Section 4, we review related work; mainly, we examine existing normalization
techniques from the viewpoint of reducing the angle bias. In Section 5, we present empirical results
1
(a) Random matrix W .
column index
xc,pu- Mo」
o
20
40
60
80
0	20	40	60	80
column index
(b) Random matrix A.
204060β0
X3pu- MoJ
O
O 20	40	60	80
column index
(c) Multiplication W A.
Figure 1: Angle bias causes a horizontal stripe pattern in W A. (Best viewed in color)
that show that it is possible to efficiently train a 100-layered MLP by reducing the angle bias using
LCW. Finally, we conclude with a discussion of future works.
2 Angle bias
We introduce angle bias by using the simple example shown in Figure 1. Figure 1(a) is a heat map
representation of matrix W ∈ R100×100, each of whose elements is independently drawn from a
uniform random distribution in the range (-1, 1). Matrix A ∈ R100×100 is also generated randomly,
and its elements range from 0 to 1, as shown in Figure 1(b). We multiply W and A to obtain the
matrix shown in Figure 1(c). Unexpectedly, a horizontal stripe pattern appears in the heat map
of W A although both W and A are random matrices. This pattern is attributed to the angle bias
that is defined as follows:
Definition 1. Pγ is an m dimensional probability distribution whose expected value is γ1m,
where γ ∈ R and 1m is an m dimensional vector whose elements are all one.
Proposition 1. Let a bea random vector in Rm that follows Pγ. Given W ∈ Rm such that ∣ ∣w 11 > 0,
the expected value of W ∙ a is ∣7∣√m^w^ cos θw, where θw is the angle between W and 1 m.
Proof. It follows from E(w ∙ a) = w ∙ E(a) = w ∙ (Y 1 m) = IWll ∣∣γ 1 m∣∣ cosθw =
∣γ∣√m^w^ cos θw, where E (x) denotes the expected value of random variable x.	□
Definition 2. From Proposition 1, the expected value of W ∙ a depends on θw as long as γ = 0. The
distribution of W ∙ a is then biased depending on θw; this is called angle bias.
In Figure 1, if we denote the i-th row vector of W and the j-th column vector of A by wi and aj ,
respectively, aj follows Pγ with γ = 0.5. The i-th row of WA is biased according to the angle
between Wi and 1m, because the (i,j)-th element ofWA is the dot product ofWi and aj. Note that
if the random matrix A has both positive and negative elements, WA also shows a stripe pattern as
long as each column vector of A follows Pγ with γ 6= 0.
We can generalize Proposition 1 for any m dimensional distribution P, instead of PY, as follows:
Proposition 2. Let a be a random vector that follows an m dimensional probability distribution P
whose expected value is μ^ ∈ Rm. Given W ∈ Rm such that ∣w∣ > 0, it follows that
E(w ∙ a)
IlWIIIμIl cos θw
0
if∣∣μI > 0,
otherwise,
where θw is the angle between W and μ^.
Proof. The proof is the same as that of Proposition 1.
□
Proposition 2 states that the distribution of W ∙ a is biased according to θw unless IμIl = 0.
2
Figure 2: Activations in layers 1, 3, 5, 7, and 9 of the 10 layered MLP with sigmoid activation
functions. Inputs are randomly sampled from CIFAR-10.
2.1	Angle bias in a multilayer perceptron
We consider a standard MLP. For simplicity, the number of neurons m is assumed to be the same in
all layers. The activation vector in layer l is denoted by al = al1, . . . , alm > ∈ Rm. The weight
vector of the i-th neuron in layer l is denoted by wil ∈ Rm. It is generally assumed that kwil k > 0.
The activation of the i-th neuron in layer l is given by
ali = f (zil ,	(1)
Zi = wl ∙ al- 1 + bi	(2)
=kwilk kal-1k cos θil + bli,	(3)
where f is a nonlinear activation function, bli ∈ R is the bias term, zil ∈ R denotes the preactivation
value, and θil ∈ R means the angle between wil and al-1. We assume that a0 corresponds to
the input vector to the MLP. In Equations (1), (2), and (3), variables zil , ali , and θil are regarded as
random variables whose distributions are determined by the distribution of the input vector a0, given
the weight vectors and the bias terms.
From Proposition 2 and Equation (2), if a1- 1 follows P with the mean vector μl- 1 such
that kμl- 1 Il > 0, the preactivation Zi is biased according to the angle between wl and μl- 1. If f is
assumed to be the sigmoid activation function, al is then biased toward a specific region in (0, 1)m,
because each element of al have a different mean value from Equation (1). In the next layer, the
variance of θil+1 will shrink compared to that of θil, because al is biased in (0, 1)m. The variance
of ali+1 shrinks as the variance of θil+1 shrinks from Equations (1) and (3). Repeating the operations
through multiple layers, the variance of θil and ali will shrink to small values.
2.2	Visualizing effect of angle bias using CIFAR- 1 0 dataset
We illustrate the effect of angle bias in an MLP by using the CIFAR-10 dataset (Krizhevsky &
Hinton, 2009) that includes a set of 32 × 32 color (RGB) images. Each sample in CIFAR-10 is
considered an input vector with 32 × 32 × 3 = 3072 real values, in which each variable is scaled
into the range [-1, 1].
2.2	. 1 With sigmoid activation functions
We consider an MLP with sigmoid activation functions that has 10 hidden layers with m = 128
neurons in each layer. The weights of the MLP are initialized according to Glorot & Bengio (2010).
We randomly took 100 samples from the dataset and input them into the MLP. Figure 2 shows the
activation pattern in layers 1, 3, 5, 7, and 9 on the selected samples. Please note that the activation in
Layer 1 corresponds to ai1 in Equation (1), that is, Layer 1 is the layer after the input layer. We see
stripe patterns in the layers other than Layers 1 in Figure 2 that are caused by angle bias. In Layer 9,
the activation value of each neuron is almost constant regardless of the input. In contrast, no stripe
pattern appears in Layer 1, because each element of the input vector is scaled into the range [-1, 1]
and its mean value is near zero; this corresponds to the case in which ∣μ∣∣ ≈ 0 in Proposition 2.
3
Layed.
Layer3	Layer5	Layer7
92n90β98β
{33,!6ωp)φ-6u4
{αaJ6o≡Q-σc<
窜京P)-BU4
(3al6ωp)ω-6u4
aaJ6φ≡fi6u4
Layer9
Neuron ID	Neuron ID	Neuron ID	Neuron ID	Neuron ID
Figure 3: Boxplot summaries of θil in the MLP with sigmoid activation functions. Results for only
the first ten neurons in layers 1,3,5,7, and 9 are displayed. Samples shown in Figure 2 are used to
evaluate θil .
LayerlO (activation)	Layer20 (activation)
Layer30 (activation) Layer40 (activation)
Layerl (activation)
Sample ID
Sample ID
CI-UO-InωN
O O
O 5
Uo-InN
50	100
Sample ID
Figure 4: Activations in layers 1, 10, 20, 30, and 40 of the 50 layered MLP with ReLU activation
functions. Inputs are randomly sampled from CIFAR-10.
We calculated the angle between wil and al-1, that is, θil in Equation (3), for each sample1. Figure 3
shows boxplot summaries of θil on the first ten neurons in layers 1, 3, 5, 7, and 9, in which the 1%,
25%, 50%, 75%, and 99% quantiles are displayed as whiskers or boxes. We see the mean of θil are
biased according to the neurons in the layers other than Layer 1. We also see that the variance of θil
shrink through layers.
2.2.2 With ReLU activation functions
Next, we consider an MLP with ReLU activation functions that has 50 hidden layers with m = 128
neurons in each layer. The weights are initialized according to Glorot & Bengio (2010). Figure 4
shows the activation pattern in layers 1, 10, 20, 30, and 40 on the randomly selected samples. We
see stripe patterns in the layers other than Layer 1 that are caused by the angle bias.
Figure 5 shows boxplot summaries of θil on the first ten neurons in layers 1, 10, 20, 30, and 40.
We see that the mean of θil are biased according the neurons in the layers other than Layer 1. We
also see that the variance of θil shrink through layers, but the shrinking rate is much moderate com-
pared to that in Figure 3. This is because ReLU projects a preactivation vector into the unbounded
region [0, +∞)m and the activation vector is less likely to concentrate on a specific region.
2.3 Relation to vanishing gradient problem
Under the effect of angle bias, the activation of neurons in deeper layers are almost constant regard-
less of the input in an MLP with sigmoid activation functions, as shown in Figure 2. It indicates
that Na L = 0, where L is a loss function that is defined based on the output of the MLP and Na L
means the gradient with respect to the input vector a0. From Equation (2), we have
m
Nal-1L=XwilNzilL,	(4)
i=1
NwlL = al-1Nzl L	(i = 1, . . . , m) .	(5)
wi	zi
Assuming that wi1 (i = 1, . . . , m) are linearly independent, it follows that Nzi1 = 0 (i = 1, . . . , m)
from Equation (4), with l = 1, and Na0L = 0. Then, it holds that Nwi1 L = 0 (i = 1, . . . , m) from
1The angle is given by arccos ((WIl ∙ al-1 / (kwilk kal-1k
4
LayerI
Layen O
Layer20
TnT
工T
T?
工T
工T
TnT
工T
⅛1
- - -ul⅛-
009590B580
&al6(υp)<υ-6u4
ΛMT
TnHT
λul
TnMT
Tnul
TnMT
I-CT
TΛU1
ΛMT
-----A:H-
0500959085S0
s-」33P) ω-σc<
TInMTl
TnHTl
TlHUl
TiMMTl
Tlnu ,
TlHUIl
- ΠMT
- -^IHT1
一 i—cp—i 一
(<u3⅛φp) Φ-CTC<
Layer30
Layer40
ɪ
ɪɪ
ɪ-
ɪɪ
⅞
⅛
⅛
Λr
φ
- - -1⅜•
0095908580
&(u」6vp) φ-σc<
TT
f
⅞
⅞
工H
工¥
⅛
TPT
AT
• • •营•
00959085
(<U<U⅛QP) Φ-CTC<
Neuron ID	Neuron ID	Neuron ID	Neuron ID	Neuron ID
Figure 5: Boxplot summaries of θil in the MLP with ReLU activation functions. Results for only the
first ten neurons in layers 1, 10, 20, 30, and 40 are displayed. Samples shown in Figure 4 are used
to evaluate θil .
Equation (5), with l = 1, indicating that the gradients of weights in the first layer are vanished. From
Equation(1), with l = 1, we have NG L = f 0 (z1) V^ L. If f 0 (z1) = 0, it follows that V^ L = 0
from Nzι L = 0. This leads to Nw2 L = 0 from Equations (4) and (5), with l = 2, under the
zi	wi
assumption that wi2 (i = 1, . . . , m) are linearly independent. Consequently, we can derive NwlL =
wi
0 from Na0L = 0 so far as f0(zil) 6= 0 and wil (i = 1, . . . , m) being linearly independent.
If we use rectified linear activation instead of sigmoid activation, the gradients of weights are less
likely to vanish, because Na0L will seldom be exactly zero. However, the rate of each neuron being
active2 is biased, because the distribution of preactivation zil is biased. If a neuron is always active,
it behaves as an identity mapping. If a neuron is always inactive, it is worthless because its output is
always zero. Such a phenomenon is observed in deep layers in Figure 4. As discussed in Balduzzi
et al. (2017), the efficiency of the network decreases in this case. In this sense, angle bias may reduce
the efficiency of a network with rectified linear activation.
3 Linearly constrained weights
There are two approaches to reduce angle bias in a neural network. The first one is to somehow make
the expected value of the activation of each neuron near zero, because angle bias does not occur
if kμk = 0 from Proposition 2. The second one is to somehow regularize the angle between Wil
and E al-1 . In this section, we propose a method to reduce angle bias in a neural network by
using the latter approach. We introduce WLC as follows:
Definition 3. WLC is a subspace in Rm defined by
WLC := nw = (w1 , . . . , wm ) ∈ R	w1 + . . . + wm = 0o .
(6)
The following holds for w ∈ WLC:
Proposition 3. Let a be an m dimensional random variable that follows Pγ . Given w ∈ WLC such
that ∣Wk > 0, the expected value of W ∙ a is zero.
Proof. E(w ∙ a) = w ∙ E(a) = Y (w ∙ 1 m) = Y (W1 + ... + Wm) = 0.
□
From Proposition 3, if Wil ∈ WLC and al-1 follows Pγ, we can resolve angle bias in zil in Equa-
tion (2). Ifwe initialize bli = 0, the distribution of each of zil (i = 1, . . . , m) will likely be more sim-
ilar to each other. The activation vector in layer l, each of whose elements is given by Equation (1),
is then expected to follow Pγ. Therefore, if the input vector a0 follows Pγ , we can inductively
reduce the angle bias in each layer of an MLP by using weight vectors that are included in WLC. We
call weight vector wil in WLC linearly constrained weights (LCWs).
2A neuron with rectified linear activation is said to be active if its output value is positive.
5
Sample ID
Sample ID
Layer5 (activation)
Layer9 (activation)
0	50 ιoo
Sample ID
Figure 6: Activations in layers 1, 3, 5, 7, and 9 of the MLP with sigmoid activation functions with
LCW. The input samples are the same as those used in Figure 2.
Layerl	Layer3	Layers	Layer7
Figure 7: Boxplot summaries of θil on the first ten neurons in layers 1,3,5,7, and 9 of the MLP with
sigmoid activation functions with LCW.
Layer9
123456789 10
Neuron ID
3.1	Visualizing the effect of LCW
3.1.1	With sigmoid activation functions
We built an MLP with sigmoid activation functions of the same size as that used in Section 2.2.1,
but whose weight vectors are replaced with LCWs. We applied the minibatch-based initialization
described in Section 3.3. Figure 6 shows the activation pattern in layers 1, 3, 5, 7, and 9 of the
MLP with LCW on the randomly selected samples that are used in Figure 2. When compared with
Figure 2, we see no stripe pattern in Figure 6. The neurons in Layer 9 respond differently to each
input sample; this means that a change in the input leads to a different output. Therefore, the network
output changes if we adjust the weight vectors in Layer 1, that is, the gradients of weights in Layer 1
do not vanish in Figure 6.
Figure 7 shows boxplot summaries of θil on the first ten neurons in layers 1, 3, 5, 7, and 9 of the
MLP with LCW. We see that the angle distributes around 90o on each neuron in each layer. This
indicates that the angle bias is resolved in the calculation of zil by using LCW.
After 10 epochs training Figure 8 shows the activation pattern in layers of the MLP with LCW
after 10 epochs training. A slight stripe pattern is visible in Figure 8, but neurons in each layer
react differently to each input. Figure 9 shows boxplot summaries of θil of the MLP after 10 epochs
training. We see that the mean of θil is slightly biased according to the neurons. However, the
variance of θil do not shrink even in deeper layers.
3.1.2	With ReLU activation functions
We built an MLP with ReLU activation functions of the same size as that used in Section 2.2.2,
whose weight vectors are replaced with LCWs. We applied the minibatch-based initialization de-
scribed in Section 3.3. Figure 10 shows the activation pattern in layers 1, 10, 20, 30, and 40 of the
MLP with LCW. When compared with Figure 4, we see no stripe pattern in Figure 10. Figure 11
shows boxplot summaries of θil on the first ten neurons in layers 1, 10, 20, 30, and 40 of the MLP
with LCW. We can observe that the angle bias is resolved by using LCW in the MLP with ReLU
activation functions.
6
Layerl (activation) Layer3 (activation) Layers (activation) Layer7 (activation) Layer9 (activation)
Sample ID	Sample ID	Sample ID	Sample ID	Sample ID
Layerl
123456789 10
Neuron ID
Figure 8: Activations in layers of the MLP with sigmoid activation functions with LCW after 10
epochs training.
Figure 9: Boxplot summaries of θil of the MLP with sigmoid activation functions with LCW after
10 epochs training.
3.2	Learning LCW via reparameterization
A straightforward way to train a neural network with LCW is to solve a constrained optimization
problem, in which a loss function is minimized under the condition that each weight vector is in-
cluded in WLC. Although several methods are available to solve such constrained problems, for
example, the gradient projection method (Luenberger & Ye, 2015), it might be less efficient to solve
a constrained optimization problem than to solve an unconstrained one. We propose a reparam-
eterization technique that enables us to train a neural network with LCW by using a solver for
unconstrained optimization. We can embed the constraints on the weight vectors into the structure
of the neural network by reparameterization.
Reparameterization Let wil ∈ Rm be a weight vector in a neural network. We reparametrize wil
by using vector vil ∈ Rm-1 as wil = Bmvil, where Bm ∈ Rm×(m-1) is a basis of WLC, written as
a matrix of column vectors. For example, Bm is given by
Bm =	-I1m>m--11	∈ Rm×(m-1),	(7)
where Im-1 is the identity matrix of order (m - 1) × (m - 1).
It is obvious that wil = Bmvil ∈ WLC. We then solve the optimization problem in which vil
is considered as a new variable in place of wil . This optimization problem is unconstrained be-
cause vil ∈ Rm-1. We can search for wil ∈ WLC by exploring vil ∈ Rm-1. In the experiments in
Section 5, we used Bm in Equation (7). We also tried an orthonormal basis of WLC as Bm; however,
there was little difference in accuracy. It is worth noting that the proposed reparameterization can
be implemented easily and efficiently by using modern frameworks for deep learning using GPUs.
3.3	Initialization using minibatch statistics
By introducing LCW, we can reduce the angle bias in zil in Equation (2), which mainly affects the
expected value ofzil. Itis also important to regularize the variance ofzil, especially when the sigmoid
activation is used, because the output of the activation will likely saturate when the variance of zil is
too large. We apply an initialization method by which the variance of zil is regularized based on a
minibatch of samples. This type of initialization has also been used in previous studies Mishkin &
Matas (2016) and Salimans & Kingma (2016).
7
α- UOJnuN
Sample ID	Sample ID	Sample ID	Sample ID	Sample ID
Figure 10: Activations in layers 1, 10, 20, 30, and 40 of the MLP with ReLU activation functions
with LCW. The input samples are the same as those used in Figure 4.
Laye r30
Layerl
3456789 1。
Neuron ID
LayerlO
0 5 0 5 0
0 9 9 8 8
亩出 63p)-BU4
123456789 10
Neuron ID
Layer20
123456789 10
Neuron ID
TlMMT
TlnnTl
IJnT
- πu ,
-InHT
IHHT
-I-⊂.ΞF-H -
0095908580
φ(ugφΞΦ-CTC<
123456789 10
Neuron ID
Laye r40
009590β5β0
Bal6-5u4
123456789 10
Neuron ID
Figure 11: Boxplot summaries of θil on the first ten neurons in layers 1, 10, 20, 30, and 40 of the
MLP with ReLU activation functions with LCW.
First, We randomly generate a seed vector Wl ∈ Wlci, where Wlci := {w ∈ WLC IlIWll ≤ 1}.
Section A in the appendix describes the approach for sampling uniformly from WLC1. The scholar
variable ηl is then calculated in a layer-wise manner with the property that the standard deviation
of (ηlWl) ∙ a!-1 equals the target value σz, in which a1-1 is evaluated using a minibatch. Finally,
we use ηl Wll as the initial value of Wi. If we apply the reparameterization method described in the
previous section, the first m - 1 elements of ηlW l are used as the initial value of vl. We initialize
bias terms bli in Equation (2) to zero. 4
4 Related work
Ioffe & Szegedy (2015) proposed the batch normalization (BN) approach for accelerating the train-
ing of deep nets. BN was developed to address the problem of internal covariate shift, that is,
training deep nets is difficult because the distribution of the input of a layer changes as the weights
of the preceding layers change during training. The computation of the mean and standard deviation
of zil based on a minibatch is incorporated into the structure of the network, and zil is normalized by
using these statistics. GUICehre & Bengio (2016) proposed the standardization layer (SL) approach,
which is similar to BN. The main difference is that SL normalizes ali, whereas BN normalizes zil .
Interestingly, both BN and SL can be considered mechanisms for reducing the angle bias. SL re-
duces the angle bias by forcing ∣μ∣∣ = 0 in Proposition 2. On the other hand, BN reduces the angle
bias by normalizing zil for each neuron. A drawback of both BN and SL is that the model has to be
switched during inference to ensure that its output depends only on the input but not the minibatch.
By contrast, LCW proposed in this paper does not require any change in the model during inference.
Moreover, the computational overhead for training the LCW is lower than that of BN and SL, be-
cause LCW only requires the addition of layers that multiply Bm to vil, as described in Section 3.2,
whereas BN and SL need to compute both the mean and the standard deviation of zil or ali .
Salimans & Kingma (2016) proposed weight normalization (WN) to overcome the drawbacks of
BN, in which a weight vector Wil ∈ Rm is reparametrized as Wil = (gil/lvil l)vil, where gil ∈ R
and vil ∈ Rm are new parameters. By definition, WN does not have the property of reducing
the angle bias, because the degrees of freedom of Wil are unchanged by the reparameterization.
Salimans & Kingma (2016) also proposed a minibatch-based initialization by which weight vectors
are initialized so that zil has zero mean and unit variance, indicating that the angle bias is reduced
immediately after the initialization.
8
Ba et al. (2016) proposed layer normalization (LN) as a variant of BN. LN normalizes zil over the
neurons in a layer on a sample in the minibatch, whereas BN normalizes zil over the minibatch on
a neuron. From the viewpoint of reducing the angle bias, LN is not as direct as BN. Although LN
does not resolve the angle bias, it is expected to normalize the degree of bias in each layer.
5 Experiments
We conducted preliminary experiments using the CIFAR-10 dataset, the CIFAR-100
dataset (Krizhevsky & Hinton, 2009), and the SVHN dataset (Netzer et al., 2011). These experi-
ments are aimed not at achieving state-of-the-art results but at investigating whether we can train
a deep model by reducing the angle bias and empirically evaluating the performance of LCW in
comparison to that of BN and WN.
Network structure We used MLPs with the cross-entropy loss function. Each network has 32 ×
32 × 3 = 3072 input neurons and 10 output neurons, and it is followed by a softmax layer. We refer
to an MLP that has L hidden layers and M neurons in each hidden layer as MLP(L, M). Either
a sigmoid activation function or a rectified linear activation function was used. MLPLCW denotes
an MLP in which each weight vector is replaced by LCW. MLPBN denotes an MLP in which the
preactivation of each neuron is normalized by BN. MLPWN denotes an MLP whose weight vectors
are reparametrized by WN.
Initialization Plain MLP and MLPBN were initialized using the method proposed in Glorot &
Bengio (2010). MLPLCW was initialized using the minibatch-based method described in Section 3.3
with σz = 0.5. MLPWN was initialized according to Salimans & Kingma (2016).
Optimization MLPs were trained using a stochastic gradient descent with a minibatch size of 128
for 100 epochs. The learning rate starts from 0.1 and is multiplied by 0.95 after every two epochs.
Environment and implementation The experiments were performed on a system running Ubuntu
16.04 LTS with NVIDIAR TeslaR K80 GPUs. We implemented LCW using PyTorch version
0.1.12. We implemented BN using the torch.nn.BatchNorm1d module in PyTorch. We im-
plemented WN by ourselves using PyTorch3 4.
5.1	Experimental results: MLPs with sigmoid activation functions
We first consider MLPs with sigmoid activation functions. Figure 12 shows the convergence
and computation time for training MLPs with CIFAR-10 dataset. Figure 12(a)-(c) shows results
for MLP(100, 128), Figure 12(d)-(f) for MLP(50, 256), and Figure 12(g)-(i) for MLP(5, 512). Fig-
ure 12(a) shows that the training accuracy of the plain MLP(100, 128) is 10% throughout the train-
ing, because the MLP output is insensible to the input because of the angle bias, as mentioned in
Section 2.24. By contrast, MLPLCW or MLPBN is successfully trained, as shown in Figure 12(a),
indicating that the angle bias is a crucial obstacle to training deep MLPs with sigmoid activation
functions. MLPLCW achieves a higher rate of increase in the training accuracy compared to MLPBN
in Figure 12(a), (d), and (g). As described in Section 4, WN itself cannot reduce the angle bias, but
the bias is reduced immediately after the initialization of WN. From Figure 12(a) and (d), we see that
deep MLPs with WN are not trainable. These results suggest that starting with weight vectors that
do not incur angle bias is not sufficient to train deep nets. It is important to incorporate a mechanism
that reduces the angle bias during training, such as LCW or BN.
The computational overhead of training of MLPLCW(100, 128) is approximately 55% compared to
plain MLP(100, 128), as shown in Figure 12(b); this is much lower than that of MLPBN(100, 128).
The overhead of MLPWN is large compared to that of MLPBN, although it contradicts the claim of
Salimans & Kingma (2016). We think this is due to the implementation of these methods. The BN
3Although a module for WN is available in PyTorch version 0.2.0, we did not use it because the minibatch-
based initialization (Salimans & Kingma, 2016) is not implemented for this module.
4CIFAR-10 includes samples from 10 classes equally. The prediction accuracy is therefore 10% if we
predict all samples as the same class.
9
>UΞ3uura 6u 三一 e∙lh-
---MLP_LCW
---MLP_BN
——MLP_WN
...MLP
(b) Elapsed time (s).
O 20	40	60	80 IOO
Epoch
0.0 j-∣----1----1------1-----1-----γj
O	20	40	60	80	IOO
Epoch
(a) Training accuracy.
1.0
8 6 4 2 0
a0000
>UE3uura 6UW∙J1
O 20	40	60	80 IOO
Epoch
O 20	40	60	80 IOO
Epoch
(c) Test accuracy.
(d) Training accuracy.
Aue∙lnue 6u 三≡∙ll
O 20	40	60	80 IOO
Epoch
(g) Training accuracy.
(e) Elapsed time (s).
O 20	40	60	80 IOO
Epoch
(h) Elapsed time (s).
(f) Test accuracy.
O 20	40	60	80 IOO
Epoch
(i) Test accuracy.
Figure 12: Training accuracy, elapsed time, and test accuracy for CIFAR-10 dataset: (a-c) results
of MLP(100, 128), (d-f) results of MLP(50, 256), (g-i) results of MLP(5, 512). Sigmoid activation
functions are used in each model.
module we used in the experiments consists of a specialized function developed by GPU vendors,
whereas the WN module was developed by ourselves. In this sense, the overhead of LCW may be
improved by a more sophisticated implementation.
In terms of the test accuracy, MLPLCW has peaks around 20 epochs, as shown in Figure 12(c), (f),
and (i). We have no clear explanation for this finding, and further studies are needed to investigate
the generalizability of neural networks.
Experimental results with SVHN and CIFAR-100 datasets are reported in Section B in the appendix.
5.2	Experimental results: MLPs with rectified linear activation functions
We have experimented with MLPs with rectified linear activation functions. In our experiments,
we observed that the plain MLP with 20 layers and 256 neurons per layer was successfully trained.
However, the training of MLPLCW of the same size did not proceed at all, regardless of the dataset
used in our experiment; in fact, the output values of the network exploded after a few minibatch
updates. We have investigated the weight gradients of the plain MLP and MLPLCW. Figure 13 shows
boxplot summaries of the weight gradients in each layer of both models, in which the gradients are
evaluated by using a minibatch of CIFAR-10 immediately after the initialization. By comparing
Figure 13(a) and Figure 13(b), we find an exponential increase in the distributions of the weight
gradients of MLPLCW in contrast to the plain MLP. Because the learning rate was the same for every
layer in our experiments, this exponential increase of the gradients might hamper the learning of
MLPLCW. The gradients in a rectifier network are sums of path-weights over active paths (Balduzzi
et al., 2017). The exponential increase of the gradients therefore implies an exponential increase
10
∙*jUgP"j-16 1j=6'≡m
ɪɪ
:
善
S
⅛
ɪɪ
≠
ɪɪ
XT
ɪɪ
XT
T=T
XT
InY
TXHT
Tnul
TAUl
Tluun
Tm MTl
- πu ,
(a) MLP(20, 256).	(b) MLPLCW(20, 256).
Figure 13: Distributions of weight gradients in each layer of MLP(20, 256) or MLPLCW(20, 256)
with rectified linear activation functions.
of active paths. As discussed in Section 2.3, we can prevent neurons from being always inactive
by reducing the angle bias, which we think caused the exponential increase in active paths. We
need further studies to make MLPLCW with rectified linear activation functions trainable. Possible
directions are to apply layer-wise learning rates or to somehow regularize the distribution of the
weight gradients in each layer of MLPLCW, which we leave as future work.
6 Conclusions and future work
In this paper, we have first identified the angle bias that arises in the dot product of a nonzero vector
and a random vector. The mean of the dot product depends on the angle between the nonzero vector
and the mean vector of the random vector. In a neural network, the preactivation value of a neuron is
biased depending on the angle between the weight vector of the neuron and the mean of the activation
vector in the previous layer. We have shown that such biases cause a vanishing gradient in a neural
network with sigmoid activation functions. To overcome this problem, we have proposed linearly
constrained weights to reduce the angle bias in a neural network; these can be learned efficiently by
the reparameterization technique. Preliminary experiments suggest that reducing the angle bias is
essential to train deep MLPs with sigmoid activation functions.
We have observed that reducing the angle bias causes an unfavorable side effect in the training of
MLPs with rectified linear activation functions, in which the gradient exploding problem occurs.
We need further studies to make LCW applicable to rectifier networks. Future work also includes
investigating the applicability of LCW for other neural network structures, such as convolutional or
recurrent structures. The use of LCW in recurrent networks is of particular interest, for which batch
normalization is not straightforwardly applicable.
References
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. In NIPS 2016
Deep Learning Symposium, 2016.
David Balduzzi, Marcus Frean, Lennox Leary, J. P. Lewis, Kurt Wan-Duo Ma, and Brian
McWilliams. The shattered gradients problem: If resnets are the answer, then what is the ques-
tion? In Proceedings of the 34th International Conference on Machine Learning, pp. 342-350,
2017.
Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In 29th Annual
Conference on Learning Theory, volume 49 of Proceedings of Machine Learning Research, pp.
907-940, 2016.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence
and Statistics, pp. 249-256, 2010.
Caglar Gulcehre and Yoshua Bengio. Knowledge matters: Importance of prior information for
optimization. Journal of Machine Learning Research, 17(8):1-32, 2016.
11
Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural
networks. Science, 313(5786):504-507, 2006.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are uni-
versal approximators. Neural networks, 2(5):359-366, 1989.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In David Blei and Francis Bach (eds.), Proceedings of the 32nd
International Conference on Machine Learning, pp. 448-456. JMLR Workshop and Conference
Proceedings, 2015.
Bunpei Irie and Sei Miyake. Capabilities of three-layered perceptrons. In IEEE International Con-
ference on Neural Networks, volume 1, pp. 641-648, 1988.
Kevin Jarrett, Koray Kavukcuoglu, Marc’Aurelio Ranzato, and Yann LeCun. What is the best
multi-stage architecture for object recognition? In 2009 IEEE 12th International Conference on
Computer Vision, pp. 2146-2153, 2009.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-
nical report, University of Toronto, 2009.
David G. Luenberger and Yinyu Ye. Linear and Nonlinear Programming. Springer Publishing
Company, Incorporated, 2015.
Dmytro Mishkin and Jiri Matas. All you need is a good init. In International Conference on Learning
Representations, 2016.
Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines. In
Proceedings of the 27th International Conference on Machine Learning (ICML-10), pp. 807-814.
Omnipress, 2010.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading
digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning
and unsupervised feature learning, 2011.
PyTorch. http://pytorch.org/ (accessed: 2017/10/2).
Tim Salimans and Diederik P Kingma. Weight normalization: A simple reparameterization to accel-
erate training of deep neural networks. In Advances in Neural Information Processing Systems,
pp. 901-909, 2016.
Matus Telgarsky. Benefits of depth in neural networks. In 29th Annual Conference on Learning
Theory, volume 49 of Proceedings of Machine Learning Research, pp. 1517-1539, 2016.
A SAMPLING UNIFORMLY FROM WLC1
We can generate a sample w that is uniformly distributed in WLC1 as follows:
1.	w 〜N (0, Im): Generate a sample w from m-dimensional normal distribution N (0, Im).
2.	w J w — E(w): Subtract the mean of elements of w, which corresponds to the projection
of w onto WLC.
3.	w J w/kwk: Normalize w to a unit vector.
4.	ξ 〜U(0,1): Generate a sample ξ from a uniform distribution in (0,1).
5.	w J ξm- w: Shrink w in proportion to the hypersurface area of the m — 1 dimensional
hypersphere of radius ξ.
12
Q
L
>UΞ3uura 6u 三一 e∙l
(S)α,E'cpα,sdl0≡
Q'86 H~
LooCi <
Aue∙lnueJ.
(a) Training accuracy.	(b) Elapsed time (s).
(d) Training accuracy.
(c) Test accuracy.
(g) Training accuracy.
(e) Elapsed time (s).
(h) Elapsed time (s).
Ausnmttα,l
(i) Test accuracy.
Figure 14:	Training accuracy, elapsed time, and test accuracy for SVHN dataset: (a-c) results of
MLP(100, 128), (d-f) results of MLP(50, 256), (g-i) results of MLP(5, 512). Sigmoid activation
functions are used in each model.
B Additional graphs
The convergence and computation time for training MLPs with SVHN and CIFAR-100 datasets
are shown in Figure 14 and Figure 15, respectively. In each figure, (a)-(c) shows results
for MLP(100, 128), (d)-(f) for MLP(50, 256), and (g)-(i) for MLP(5, 512). We see that both LCW
and BN enable training deep models, such as MLP(100, 128) and MLP(50, 256). LCW achieves
faster convergence with respect to the training accuracy in most cases.
13
Aue∙lnuue 6u'≡'s∙ll
Ausnbe 6u 三≡∙ih
---MLP_LCW
--MLP_BN
---MLP_WN
...MLP
(a) Training accuracy.
(d) Training accuracy.
q∙86 4∙2
L0000
>υsɔuure 6u 三∙i
(g) Training accuracy.
(b) Elapsed time (s).
>UΞ3uuπttα,l
(c) Test accuracy.
Ooo
Ooo
6 4 2
(©Eq P3sd山
O 20	40	60	80 IOO
Epoch
(e) Elapsed time (s).
(-£=P3sd-山
(h) Elapsed time (s).
AUeJrDUett31
100
A0e,ln8ett31
5 O
1 1
(i) Test accuracy.
Figure 15:	Training accuracy, elapsed time, and test accuracy for CIFAR-100 dataset: (a-c) results
of MLP(100,128), (d-f) results of MLP(50, 256), (g-i) results of MLP(5, 512). Sigmoid activation
functions are used in each model.
14