Under review as a conference paper at ICLR 2018
Dependent Bidirectional RNN with
Extended-long Short-term Memory
Anonymous authors
Paper under double-blind review
Ab stract
In this work, we first conduct mathematical analysis on the memory, which is
defined as a function that maps an element in a sequence to the current output,
of three RNN cells; namely, the simple recurrent neural network (SRN), the long
short-term memory (LSTM) and the gated recurrent unit (GRU). Based on the
analysis, we propose a new design, called the extended-long short-term memory
(ELSTM), to extend the memory length of a cell. Next, we present a multi-task
RNN model that is robust to previous erroneous predictions, called the dependent
bidirectional recurrent neural network (DBRNN), for the sequence-in-sequence-
out (SISO) problem. Finally, the performance of the DBRNN model with the
ELSTM cell is demonstrated by experimental results.
1	Introduction
The recurrent neural network (RNN) has proved to be an effective solution for natural language
processing (NLP) through the advancement in the last three decades (Elman, 1990; Jordan, 1997;
Bayer et al., 2009; Barret & V. Le, 2016). At the cell level of a RNN, the long short-term memory
(LSTM) (Hochreiter & Schmidhuber, 1997) and the gated recurrent unit (GRU) (Cho et al., 2014)
are often adopted by a RNN as its low-level building cell. Being built upon these cells, various RNN
models have been proposed to solve the sequence-in-sequence-out (SISO) problem. To name a few,
there are the bidirectional RNN (BRNN) (Schuster & Paliwal, 1997), the encoder-decoder model
(Cho et al., 2014; Sutskever et al., 2014; Vinyals et al., 2015; Bahdanau et al., 2015) and the deep
RNN (Pascanu et al., 2014). Although the LSTM and the GRU were designed to enhance the mem-
ory length of RNNs and avoid the gradient vanishing/exploding issue (Hochreiter & Schmidhuber,
1997; Razvan et al., 2013; Bengio et al., 1994), a good understanding of their memory length is
still lacking. Here, we define the memory of a RNN model as a function that maps an element in a
sequence to current output. The first objective of this research is to analyze the memory length of
three RNN cells - the simple RNN (SRN)(Elman, 1990; Jordan, 1997), the long short-term mem-
ory (LSTM) and the gated recurrent unit (GRU). This will be conducted in Sec. 2. Such analysis
is different to the investigation of gradient vanishing/exploding problem in a sense that gradient
vanishing/exploding problem happens during the training process, the memory analysis is, however,
done on a trained RNN model. Based on the understanding from the memory analysis, we propose
a new design, called the extended-long short-term memory (ELSTM), to extend the memory length
of a cell in Sec.3.
As to the macro RNN model, one popular choice is the BRNN. Since the elements in BRNN output
sequences should be independent of each other (Schuster & Paliwal, 1997), the BRNN cannot be
used to solve dependent output sequence problem alone. Nevertheless, most language tasks do
involve dependent output sequences. The second choice is the encoder-decoder system, where the
attention mechanism has been introduced (Vinyals et al., 2015; Bahdanau et al., 2015) to improve
its performance furthermore. As shown later in this work, the encoder-decoder system is not an
efficient learner. Here, to take advantages of both the encoder-decoder and the BRNN and overcome
their drawbacks, we propose a new multitask model called the dependent bidirectional recurrent
neural network (DBRNN), which will be elaborated in Sec. 4. Furthermore, we conduct a series of
experiments on the part of speech (POS) tagging and the dependency parsing (DP) problems in Sec.
5 to demonstrate the performance of the DBRNN model with the ELSTM cell. Finally, concluding
remarks are given and future research direction is pointed out in Sec. 6.
1
Under review as a conference paper at ICLR 2018
2 Memory Analysis of SRN, LSTM and GRU
For a large number of NLP tasks, we are concerned with finding semantic patterns from the input
sequence. It was shown by Elman (1990) that the RNN builds an internal representation of semantic
patterns. The memory of a cell characterizes its ability to map an input sequence of certain length
into such a representation. More rigidly, we define the memory as a function that maps an element
in a sequence to the current output. So the memory capability of a RNN is not only about whether
an element can be mapped into current output, but also how this mapping takes place.
It was reported by Gers et al. (2000) that a SRN only memorized sequences of length between 3-5
units while a LSTM could memorize sequences of length longer than 1000 units. In this section, we
study the memory of the SRN, LSTM and GRU.
2.1 Memory of SRN
Here, for the ease of analysis, we use Elman’s SRN model (Elman, 1990) with linear hidden state
activation function and non-linear output activation function since such cell model is mathematically
tractable and performance-wise equivalent to Jordan (1997) and Tensorflow’s variations.
The SRN is described by the following two equations:
ct = Wcct-1 + WinXt,	(1)
ht = f(ct),	(2)
where subscript t is the index of the time unit, Wc ∈ RN ×N is the weight matrix for hidden state
vector ct-ι ∈ RN, Win ∈ RN×M is the weight matrix of input vector Xt ∈ RM, ht ∈ RN and f (∙)
is an element-wise non-linear function. Usually, f (∙) is a hyperbolic-tangent or a sigmoid function.
Throughout this paper, we omit the bias terms by putting them inside the corresponding weight
matrices.
By induction, ct can be rewritten as
t
ct = Wctc0+XWct-kWinXk,	(3)
k=1
where co is the initial internal state of the SRN. Typically, We set co = 0. Then, Eq. (3) becomes
t
ct = X Wct-kWinXk .	(4)
k=1
Let λmax be the largest singular value of Wc . Then, we have
||WrkXk|| ≤ ∣∣Wc∣∣lt-kl∣∣Xk|| = λ*l∣∣Xk||.	(5)
Here, we are only interested in the case of memory decay when λmax < 1. Hence, the contribution
of Xk, k < t, to ht decays at least in form of 1方-?.We conclude that SRN,s memory decays at
least exponentially with its memory length |t - k|.
2
Under review as a conference paper at ICLR 2018
2.2 Memory of LSTM
Figure 1: The diagram of a LSTM cell.
By following the work of Hochreiter & Schmidhuber (1997), we plot the diagram of a LSTM cell
in Fig. 1. In this figure, φ, σ and 0 denote the hyperbolic tangent function, the sigmoid function
and the multiplication operation, respectively. All of them operate in an element-wise fashion. The
LSTM has an input gate, an output gate, a forget gate and a constant error carousal (CEC) module.
Mathematically, the LSTM cell can be written as
ct = σ(Wf It)ct-1 +σ(WiIt)φ(WinIt),	(6)
ht = σ(WoIt)φ(ct),	(7)
where ct ∈ RN , column vector It ∈ R(M+N) is a concatenation of the current input, Xt ∈ RM ,
and the previous output, ht-1 ∈ RN (i.e., ItT = [XtT , htT-1]). Furthermore, Wf, Wi, Wo and Win
are weight matrices for the forget gate, the input gate, the output gate and the input, respectively.
Under the assumption co = 0, the hidden state vector of the LSTM can be derived by induction as
ct
tt
XY
σ(WfIj) σ(WiIk)φ(WinIk),
k=1 j=k+1
(8)
forget gate
By setting f (∙) in Eq. (2) to a hyperbolic-tangent function, We can compare outputs of the SRN and
the LSTM below:
SRN:
(9)
LSTM:	htLSTM = σ(WoIt)φXt	Yt	σ(WfIj) σ(WiIk)φ(WinIk).	(10)
k=1 j=k+1
、	一v—	J
forget gate
We see from the above that Wct-k and Qtj=k+1 σ(WfIj) play the same memory role for the SRN
and the LSTM, respectively.
IfWfinEq. (10) is selected such that
min ∣σ(WfIj)| ≥ λmaχ,	∀λmaχ ∈ [0,1),
then
t
Y σ(WfIj) ≥ λ∣m-χkl.	(11)
j=k+1
As given in Eqs. (5) and (11), the impact of input Ik on output htin the LSTM lasts longer than that
of input Xk in the SRN. This is the case if an appropriate Weight matrix, Wf , of the forget gate is
selected.
3
Under review as a conference paper at ICLR 2018
2.3 MEMORY OF GRU
The GRU was originally proposed for neural machine translation (Cho et al., 2014). It provides an
effective alternative for the LSTM. Its operations can be expressed by the following four equations:
zt = σ(WzXt + Uzht-1),
rt = σ(Wr Xt + Ur ht-1),
(12)
(13)
~ ..
ht = Φ(WXt + U(Tt 乳 ht-i)),
ht = Ztht-I + (I - zt)ht,
(14)
(15)
where Xt, ht, zt and rt denote the input vector, the hidden state vector, the update gate vector and
the reset gate vector, respectively, and Wz, Wr, W, are trainable weight matrices. Its hidden state is
also its output, which is given in Eq. (15). If we simplify the GRU by setting Uz, Ur and U to zero
matrices, then we can obtain the following simplified GRU system:
zt = σ(WzXt),
~
ht = Φ(WXt),
ht = Ztht-I + (1 — Zt)ht∙
(16)
(17)
(18)
For the simplified GRU with the initial rest condition, we can derive the following by induction:
t
ht = X
k=1
t
Y σ(WzXj )
j=k+1
、-------V-------}
update gate
(1 - σ(WzXk))φ(WXk).
(19)
By comparing Eqs. (8) and (19), we see that the update gate of the simplified GRU and the forget
gate of the LSTM play the same role. One can control the memory decay behavior of the GRU by
choosing the weight matrix, Wz, of the update gate carefully.
3	Extended-Long Short-Term Memory (ELSTM)
As discussed above, the LSTM and the GRU have longer memory by introducing the forget and
the update gates, respectively. However, from Eq. 10 and Eq. 11, it can be seen that the impact
of proceeding element to the current output at time step t still fades quickly due to the presence of
forget gate and update gate. And as we will show in the ELSTM design, this does not have to be the
case.
In this section, we attempt to design extended-long short-term memory (ELSTM) cells and propose
two new cell models:
•	ELSTM-I: the extended-long short-term memory (ELSTM) with trainable input weight
vector Si ∈ RN, i = 1, ∙∙∙ ,t - 1, where weights Si and Sj(With i = j) are independent.
•	ELSTM-II: the ELSTM-I with no forget gate.
These two cells are depicted in Figs. 2 (a) and (b), respectively.
4
Under review as a conference paper at ICLR 2018
(a)
(b)
Figure 2: The diagrams of (a) the ELSTM-I cell and (b) the ELSTM-II cell.
The ELSTM-I cell can be described by
ct = σ(WfIt)ct-1 +stσ(WiIt)φ(WinIt),	(20)
ht = σ(WoIt)φ(ct +b).	(21)
where b ∈ RN is a trainable bias vector. The ELSTM-II cell can be written as
ct = ct-1 + stσ(WiIt)φ(WinIt),	(22)
ht = σ(WoIt)φ(ct+b).	(23)
As shown above, We introduce scaling factor, si, i = 1, ∙∙∙ ,t -1, to the ELSTM-I and the ELSTM-
II to increase or decrease the impact of input Ii in the sequence.
To prove that the proposed ELSTM-I has longer memory than LSTM, we first derive the closed form
expression of ht , which is:
Xtt
sk Y σ(WfIj) σ(WiIk)φ(WinIk) +b
j=k+1
(24)
We then pick sk such that:
t
sk Y σ(Wf Ij)
j=k+1
t
≥ Y σ(WfIj)
j=k+1
(25)
Compare Eq. 25 with Eq. 11, we conclude that ELSTM-I has longer memory than LSTM. As
a matter of fact, sk plays a similarly role as the attention score in various attention models such
5
Under review as a conference paper at ICLR 2018
as Vinyals et al. (2015). The impact of proceeding elements to the current output can be adjusted
(either increase or decrease) by sk. The memory capability of ELSTM-II can be proven in a similarly
fashion, so even ELSTM-II does not have forget gate, it is capable in attending to or forgetting a
particular position of a sequence as ELSTM-I through the scaling factor.
The major difference between the ELSTM-I and the ELSTM-II is that fewer parameters are used in
the ELSTM-II than those in the ELSTM-I. The numbers of parameters used by different RNN cells
are compared in Table 1, where Xt ∈ RM, h ∈ RN and t = 1,…，T.
Table 1:	Comparison of Parameter Numbers.
Cell
Number of Parameters
LSTM
GRU
ELSTM-I
ELSTM-II
4N (M + N +1)
3N(M+N+1)
4N(M+N+ 1)+N(T+1)
3N(M+N+ 1)+N(T+1)
Although the number of parameters of ELSTM depends on the maximum length of a sequence in
practice, the memory overhead required is limited. ELSTM-II requires less number of parameters
than LSTM for typical lengthed sequence. From Table. 1, to double the number of parameters as
compare to an ordinary LSTM, the length of a sentence needs to be 4 times the size of the word
embedding size and number of cells put together. That is, in the case of Sutskever et al. (2014)
with 1000 word embedding and 1000 cells, the sentence length needs to be 4 × (1000 + 1000) =
8000! In practice, most NLP problems whose input involves sentences, the length will be typically
less than 100. In our experiment, sequence to sequence with attention (Vinyals et al., 2015) for
maximum sentence length 100 (other model settings please refer to Table 2), ELSTM-I parameters
uses 75M of memory, ELSTM-II uses 69.1M, LSTM uses 71.5M, and GRU uses 65.9M. Through
GPU parallelization, the computational time for all four cells are almost identical with 0.4 seconds
per step time on a GeForce GTX TITAN X GPU.
4	Proposed Dependent BRNN (DBRNN) Model
We investigate the macro RNN model and propose a multitask model called dependent BRNN
(DBRNN) in this section. The model is tasked to predict a output sequence {Yt}tT=0 1 (Yi ∈ RN)
based on the input sequence {Xt }tT=1 (Xi ∈ RM), where T and T0 are the length of the input and
output sequence respectively. Our proposal is inspired by the pros and cons of two RNN models -
the bidirectional RNN (BRNN) model (Schuster & Paliwal, 1997) and the encoder-decoder model
(Cho et al., 2014). In the following, we will first examine the BRNN and the encoder-decoder in
Sec. 4.1 and, then, propose the DBRNN in Sec. 4.2.
4.1	BRNN and Encoder-Decoder
BRNN is modeling the conditional probability density function: P(Yt|{Xi}iT=1). This output is
a combination of the output of a forward and a backward RNN. Due to this bidirectional design,
the BRNN can fully utilize the information of the entire input sequence to predict each individual
output element. On the other hand, the BRNN does not utilize the predicted output in predicting
Yt. This makes elements in the predicted sequence Yt = argmaxγ P(Yt∣{Xi}T=ι) independent of
each other.
This problem can be handled using the encoder-decoder model in form of Yt =
argmaxγt P(Yt∣{Yi}t=1, {Xi}T=ι). However, the encoder-decoder model is vulnerable to previ-
ous erroneous predictions in the forward path. Recently, the BRNN has been introduced in the
encoder by Bahdanau et al. (2015), yet this design still does not address the erroneous prediction
problem.
6
Under review as a conference paper at ICLR 2018
4.2	DBRNN Model and Training
Being motivated by observations in Sec. 4.1, we propose a multitask RNN model called DBRNN to
fulfill the following objectives:
pt = Wfptf + Wbptb	(26)
Yt = argmax pt	(27)
Yt
Y = argmax Pf,	(28)
Yt
Yb = argmax pb,	(29)
Yt
where Wf and Wb are trainable weights. Pt = P(跖{Xi}T=ι), Pf = P(跖{Xi}T=ι, {Yf }t=ι)
andpb = P(匕∣{Xi}τ=ι, {Yb}T=，. The DBRNN has three learning objectives: the target sequence
for the forward RNN prediction, the reversed target sequence for the backward RNN prediction,
and, finally, the target sequence for the bidirectional prediction.
The DBRNN model is shown in Fig. 3. It consists of a lower and an upper BRNN branches. At each
time step, the input to the forward and the backward parts of the upper BRNN is the concatenated
forward and backward outputs from the lower BRNN branch. The final bidirectional prediction is
the pooling of both the forward and backward predictions. We will show later that this design will
make DBRNN robust to previous erroneous predictions.
Figure 3: The DBRNN model.
Let F(∙) be the cell function. The input is fed into the forward and backward RNN of the lower
BRNN branch as
ht	= Fl	xt, cl(t-1),	htb =	Flb	xt, clb(t+1),	ht	=	htb	,	(30)
where c denotes the cell hidden state and l denotes the lower BRNN. The final output, ht , of the
lower BRNN is the concatenation of the output, htf , of the forward RNN and the output, htb, of the
backward RNN. Similarly, the upper BRNN generates the final output Pt as
Pt =	Fuf	ht, cu(t-1),	Pt	=	Fu	ht, cu(t+1),	Pt	=	WfPt	+ W	Pt ,	(31)
f
where u denotes the upper BRNN. To generate forward prediction Ytf and backward prediction
b
Ytb, the forward and backward paths of the upper BRNN branches are trained separately with the
target sequence and the reversed target sequence, respectively. The results of the upper forward and
backward RNN are then combined to generate the final result.
f	b 1
There are three errors: prediction error of Yt denoted by ef , prediction error of Ytb denoted by eb
and prediction error of Yt denoted by e. To train this network, ef is back propagated through time
7
Under review as a conference paper at ICLR 2018
to the upper forward RNN and the lower BRNN, eb is back propagated through time to the upper
backward RNN and the lower BRNN, and e is back propagated through time to the entire model.
To show that DBRNN is more robust to previous erroneous predictions than one-directional models,
we compare the cross entropy of them as follows:
K
l = -E Ptk bg(ptk),
k=1
(32)
where K is the total number of classes (e.g. the size of vocabulary for language tasks). Pt is the
ground truth distribution which is an one- hot vector such that: Ptk = II(Ptk = k0), ∀k ∈ 1,…，K,
where H is the indicator function, k0 is the ground truth label of the tth output. Pt is the predicted
distribution.
From Eq. 26, l can be further expressed as:
K
l = - XPtklog(WfPfk + Wk)Pbk),	(33)
k=1
=TOg(Wfo Pfko+ Wb PbkO),	(34)
We can pick WtJkO and WtbkO such that WfPfk, + W?P^bko is greater than PfkO and力)卜0.Which leads to
l < - PK=I log(Pf) and l < - Pk=I log(Pbk), which means DBRNN can have less cross entropy
than one-directional prediction by pooling expert opinions from that two predictions.
It is worthwhile to compare the DBRNN and the solution in Cheng et al. (2016). Both of them have
a bidirectional design for the output. However, there exist three main differences. First, the DBRNN
is a general design for the sequence-in-sequence-out (SISO) problem without being restricted to
dependency parsing. The target sequences in training Ytf, Ytb and Yt are the same for the DBRNN.
In contrast, the solution in Cheng et al. (2016) has different target sequences. Second, the attention
mechanism is used by Cheng et al. (2016) but not in the DBRNN. Third, The encoder-decoder
design is adopted by in Cheng et al. (2016) but not in the DBRNN.
5	Experiments
5.1	Experimental Setup
We conduct experiments on two problems: part of speech (POS) tagging and dependency parsing
(DP). The POS tagging task is an easy one which requires shorter memory while the DP task needs
much longer memory and has more complex relations between the input and the output.
In the experiments, we compare the performance of five RNN models under two scenarios: 1) It =
Xt, and 2) ItT = [XtT, htT-1]. The five RNN models are the basic one-directional RNN (basic RNN),
the BRNN, the sequence-to-sequence (a variation of encoder-decoder) RNN, sequence-to-sequence
with attention and the DBRNN with four cell designs (LSTM, GRU, ELSTM-I and ELSTM-II).
When It = Xt, we do not include the GRU cell since it inherently demands ItT = [XtT, htT-1]. For
the DBRNN, we show the results for Yt (denoted by “DBRNN combined”) and Ytf (denoted by
“DBRNN forward”), which is the prediction from the forward path of the upper BRNN branch. We
do not include the result for Ytb, which is the prediction from the backward path of the upper BRNN
branch since the performance of the backward RNN path of the upper BRNN branch is poorer.
The training dataset used for both problems are from the Universal Dependency 2.0 English branch
(UD-English). It contains 12543 sentences and 14985 unique tokens. The test dataset for both ex-
periments is from the test English branch (gold, en.conllu) of CoNLL 2017 shared task development
and test data.
In the experiment, the lengths of the input and the target sequences are fixed. Sequences longer than
the maximum length will be truncated. If the sequence is shorter than the fixed length, a special pad
8
Under review as a conference paper at ICLR 2018
symbol will be used to pad the sequence. Similar technique called bucketing is also used for some
popular models such as Sutskever et al. (2014). The input to the POS tagging and the DP problems
are the stemmed and lemmatized sequences (column 3 in CoNLL-U format). The target sequence
for POS tagging is the universal POS tag (column 4). The target sequence for DP is the interleaved
dependency relation to the headword (relation, column 8) and its position (column 7). As a result,
the length of the actual target sequence (rather than the preprocessed fixed-length sequence) for DP
is twice of the length of the actual input sequence.
Table 2:	Network and training details
InPUt/output sequence fixed length
Number of RNN layers
Embedding layer vector size
Number of RNN cells
Batch size
Training steps
Learning rate
Training optimizer
Maximum gradient norm
100/100
1
512
512
20
140000
0.5
AdaGrad(Duchi, 2011)
5
The input is first fed into a trainable embedding layer (Bengio et al., 2003) before it is sent to the
actual network. Table 2 shows the detailed network and training specifications. It is important
to point out that we do not finetune network parameters or apply any engineering trick for the
best possible performance since our main goal is to compare the performance of the LSTM, GRU,
ELSTM-I and ELSTM-II four cells under various macro-models.
5.2 Experimental Results
The results of the POS tagging problem with It = Xt and ItT = [XtT, htT-1] are shown in Tables
3 and 4, respectively. Among all possible combinations, the DBRNN with the LSTM cell has the
highest accuracy (89.16%) for It = Xt while the DBRNN with the GRU cell has the (89.74%) has
the highest accuracy for ItT = [XtT , htT-1].
Table 3: POS tagging test accuracy, It = Xt (%)
	LSTM	ELSTM-I	ELSTM-II
BASIC RNN	85.38	85.30	84.35
BRNN (Schuster & Paliwal, 1997)	88.49	82.84	79.14
Seq2seq (Sutskever et al., 2014)	25.83	24.87	31.43
Seq2seq with Attention (Vinyals et al., 2015)	27.97	78.98	42.05
DBRNN Combined	89.16	83.69	81.08
DBRNN Forward	88.93	83.54	81.08
Table 4: POS tagging test accuracy, ItT = [XtT , htT-1] (%)
	LSTM	GRU	ELSTM-I	ELSTM-II
BASIC RNN	86.98	87.09	85.57	85.56
BRNN	88.94	89.26	83.48	82.57
Seq2seq	24.73	33.79	34.09	52.96
Seq2seq with Attention	34.10	73.65	80.90	54.53
DBRNN Combined	89.67	89.74	84.25	84.41
DBRNN Forward	89.46	89.53	84.05	84.44
The results of the DP problem with It = Xt and ItT = [XtT, htT-1] are shown in Tables 5 and 6,
respectively. The ELSTM-I and ELSTM-II cells perform better than the LSTM and the GRU cells.
Among all possible combinations, the sequence-to-sequence with attention combined with ELSTM-
I has the best performance. It has an accuracy of 60.19% and 66.72% for the former and the latter,
9
Under review as a conference paper at ICLR 2018
respectively. Also, the basic RNN often outperforms BRNN for the DP problem as shown in Tables
5 and 6. This can be explained by that the basic RNN can access the entire input sequence when
predicting the latter half of the output sequence since the target sequence is twice as long as the
input. The other reason is that the BRNN can easily overfit when predicting the headword position.
Table 5: DP test accuracy, It = Xt (%)
	LSTM	ELSTM-I	ELSTM-II
BASIC RNN	15:14	38.36	42.52
BRNN	14.74	39.24	35.78
Seq2seq	24.37	30.04	35.67
Seq2seq with Attention	21.56	60.19	45.15
DBRNN Combined	25.26	54.39	53.80
DBRNN Forward	25.71	53.67	52.61
Table 6: DP test accuracy, ItT = [XtT , htT-1] (%)
	LSTM	GRU	ELSTM-I	ELSTM-II
BASIC RNN	44.12	47.49	54.52	56.02
BRNN	32.46	27.83	54.14	47.72
Seq2seq	27.67	29.94	40.85	48.73
Seq2seq with Attention	31.47	53.70	66.72	51.24
DBRNN Combined	56.89	51.32	60.30	58.28
DBRNN Forward	58.30	53.17	59.81	58.05
Bi-attention (Cheng et al., 2016) 1		61.29		
We see from Tables 3 - 6 that the two DBRNN models outperform both BRNN and sequence-to-
sequence (without attention) in both POS tagging and DP problems regardless of used cells. This
shows the superiority of introducing the expert opinion pooling from both the input and the predicted
output.
Furthermore, the proposed ELSTM-I and ELSTM-II outperform the LSTM and the GRU by a signif-
icant margin for complex language tasks. This demonstrates that the scaling factor in the ELSTM-I
and the ELSTM-II does help the network retain longer memory with better attention. ELSTMs even
outperform Cheng et al. (2016), which is designed specifically for DP. For the POS tagging problem,
the ELSTM-I and the ELSTM-II do not perform as well as the GRU or the LSTM. This is probably
due to the shorter memory requirement of this simple task. The ELSTM cells are over-parameterized
and, as a result, they converge slower and tend to overfit the training data.
The ELSTM-I and the ELSTM-II perform particularly well for sequence-to-sequence (with and
without attention) model. The hidden state ct of the ELSTMs is more expressive in representing
patterns over a longer distance. Since the sequence-to-sequence design relies on the expressive
power of the hidden state, the ELSTMs do have an advantage.
We compare the convergence behavior of It = Xt and ItT = [XtT , htT-1] with the LSTM, the
ELSTM-I and the ELSTM-II cells for the DP problem in Fig. 4. We see that the ELSTM-I and the
ELSTM-II do not behave very differently between It = Xt and ItT = [XtT , htT-1] as the LSTM
does. This shows the effectiveness of the ELSTM-I and the ELSTM-II design regardless of the
input. More performance comparison will be provided in the Appendix.
6 Conclusion and Future Work
The memory decay behavior of the LSTM and the GRU was investigated and explained by math-
ematical analysis. Although the memory of the LSTM and the GRU fades slower than that of the
SRN, it may not be long enough for complicated language tasks such as dependency parsing. To
1The result is generated by using exactly the same settings in Table. 2. We do not feed in the network with
information other than input sequence itself.
10
Under review as a conference paper at ICLR 2018
(a)	(b)	(c)
Figure 4: Training perplexity of the basic RNN with It = Xt and ItT = [XtT , htT-1] for the DP
problem.
enhance the memory length, two cells called the ELSTM-I and the ELSTM-II were proposed. Fur-
thermore, we introduced a new RNN model called the DBRNN that has the merits of both the BRNN
and the encoder-decoder. It was shown by experimental results that the ELSTM-I and ELSTM-II
outperforms other designs by a significant margin for complex language tasks. The DBRNN design
is superior to BRNN as well as sequence-to-sequence models for both simple and complex language
tasks. There are interesting issues to be further explored. For example, is the ELSTM cell also help-
ful in more sophisticated RNN models such as the deep RNN? Is it possible to make the DBRNN
deeper and better? They are left for future study.
References
Dzmitry Bahdanau, KyungHyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In Proceedings of The International Conference on Learning
Representations, 2015.
Zoph Barret and Quoc V. Le. Neural architecture search with reinforcement learning. arXiv preprint,
arXiv: 1611.01578, 2016.
Justin Bayer, Daan Wierstra, Julian Togelius, and Jurgen Schmidhuber. Evolving memory cell
structures for sequence learning. Artificial Neural Networks - ICANN, pp. 755-764, 2009.
Yoshua Bengio, Patrice Simard, and Paolo Frasoni. Learning long-term dependencies with gradient
descent is difficult. Neural Networks, 5:157-166, 1994.
Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic
language model. Journal of Machine Learning Research, pp. 1137-1155, 2003.
Hao Cheng, Hao Fang, Xiaodong He, Jianfeng Gao, and Li Deng. Bi-directional attention with
agreement for dependency parsing. In Proceedings of The Empirical Methods in Natural Lan-
guage Processing (EMNLP 2016), 2016.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoderdecoder for
statistical machine translation. In Proceedings of The Empirical Methods in Natural Language
Processing (EMNLP 2014), 2014.
Duchi. Adaptive subgradient methods for online learning and stochastic optimization. The Journal
of Machine Learning Research, pp. 2121-2159, 2011.
Jeffrey Elman. Finding structure in time. Cognitive Science, 14:179-211, 1990.
F. A. Gers, Jurgen Schmidhuber, and Fred Cummins. Learning to forget: Continual prediction with
lstm. Neural Computation, pp. 2451-2471, 2000.
11
Under review as a conference paper at ICLR 2018
SePP Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 9:1735-
1780, 1997.
Michael Jordan. Serial order: A Parallel distributed Processing aPProach. Advances in Psychology,
121:471-495, 1997.
Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. How to construct deeP
recurrent neural networks. arXiv:1312.6026, 2014.
Pascanu Razvan, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural
networks. In Proceedings of The International Conference on Machine Learning (ICML 2013),
PP. 1310-1318, 2013.
Mike Schuster and KuldiP K. Paliwal. Bidirectional recurrent neural networks. Signal Processing,
45:2673-2681, 1997.
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks.
Advances in Neural Information Processing Systems, PP. 3104-3112, 2014.
O. Vinyals, L. Kaiser, T. Koo, S. Petrov, I. Sutskever, and G. Hinton. Grammar as a foreign language.
Advances in Neural Information Processing Systems, PP. 2773-2781, 2015.
12
Under review as a conference paper at ICLR 2018
Appendix: More Experimental Results
In the appendix, we provide more experimental results to shed light on the convergence performance
in the training of various models with different cells for the POS tagging and the DP tasks. First,
we compare the training perplexity between It = Xt and ItT = [XtT , htT-1] for various models with
the LSTM, the ELSTM-I and the ELSTM-II cells in Figs. 5 - 8. Then, we examine the training
perplexity with ItT = [XtT, htT-1] for various models with different cells in Figs. 9 - 12.
DP: BRNN Architecture with LSTM Cell
DP: BRNN Architecture with ELSTM-I Cell
DP: BRNN Architecture with ELSTM-II Cell
(a)	(b)
(c)
Figure 5: The training perplexity of the BRNN model with It = Xt and ItT = [XtT , htT-1] for the
DP task.
-lt =[×t'
」It = Xt
DP: DBRNN Architecture with ELSTM-II Cell
(a)	(b)	(c)
Figure 6: The training perplexity of the DBRNN model with It = Xt and ItT = [XtT , htT-1] for the
DP task.
(a)	(b)	(c)
Figure 7: The training perplexity of the sequence-to-sequence model with It = Xt and ItT =
[XtT , htT-1] for the DP task.
13
Under review as a conference paper at ICLR 2018
(a)
Attention Architecture with ELSTM-II Cell
(b)	(c)
A±±xs-d-l Sd
-----Basic RNN
—∙—BRNN
---- Seq2Seq
.....Seq2Seq Attention
DBRNN
Figure 8: The training perplexity of the sequence-to-sequence model with It = Xt and ItT =
[XtT, htT-1] for the POS tagging task.
(b)
(a)
-----Basic RNN
—∙BRNN
——Seq2Seq
..Seq2Seq Attention
—A-DBRNN
(c)	(d)
Figure 9: The training perplexity of different models with the LSTM (top left), its zoom-in (top
right) and the GRU (bottom left), its zoom-in (bottom right) for the POS tagging.
14
Under review as a conference paper at ICLR 2018
-----Basic RNN
—•—BRNN
---- Seq2Seq
..Seq2Seq Attention
△ DBRNN
-----Basic RNN
—•—BRNN
---- Seq2Seq
..Seq2Seq Attention
△ DBRNN
50
45
40
35
50
45
40
35
(a)
(b)
Figure 10: The training perplexity of different models with the LSTM (left) and the GRU (right)
cells for the DP task.
DP: Basic RNN Architecture with Different Cells
DP: BRNN Architecture with Different Cells
20
18
16
------LSTM
GRU
-∙— ELSTM-I
A ELSTM-II
20
18
16
------LSTM
GRU
-∙— ELSTM-I
ʌ ELSTM-II
(a)
(b)
20
18
16
------LSTM
----GRU
—•— ELSTM-I
ʌ ELSTM-II
(c)
Figure 11: The training perplexity for the basic RNN (top left), the BRNN (top right), the sequence-
to-sequence (bottom left) and the DBRNN models (bottom right) for the DP task.
(d)
15
Under review as a conference paper at ICLR 2018
------LSTM
----GRU
—•— ELSTM-I
A ELSTM-II
------LSTM
----GRU
—•— ELSTM-I
ʌ ELSTM-II
18
16
(a)
(b)
Figure 12: The training perplexity of the sequence-to-sequence model with the ELSTM-II cell for
the POS tagging (left) and for the DP (right) tasks.
16