Under review as a conference paper at ICLR 2018
Unbiasing	Truncated Backpropagation
Through Time
Anonymous authors
Paper under double-blind review
Ab stract
Truncated Backpropagation Through Time (truncated BPTT, Jaeger (2005)) is a
widespread method for learning recurrent computational graphs. Truncated BPTT
keeps the computational benefits of Backpropagation Through Time (BPTT Werbos
(1990)) while relieving the need for a complete backtrack through the whole
data sequence at every step. However, truncation favors short-term dependencies:
the gradient estimate of truncated BPTT is biased, so that it does not benefit
from the convergence guarantees from stochastic gradient theory. We introduce
Anticipated Reweighted Truncated Backpropagation (ARTBP), an algorithm that
keeps the computational benefits of truncated BPTT, while providing unbiasedness.
ARTBP works by using variable truncation lengths together with carefully chosen
compensation factors in the backpropagation equation. We check the viability
of ARTBP on two tasks. First, a simple synthetic task where careful balancing
of temporal dependencies at different scales is needed: truncated BPTT displays
unreliable performance, and in worst case scenarios, divergence, while ARTBP
converges reliably. Second, on Penn Treebank character-level language modelling
Mikolov et al. (2012), ARTBP slightly outperforms truncated BPTT.
Backpropagation Through Time (BPTT) Werbos (1990) is the de facto standard for training recurrent
neural networks. However, BPTT has shortcomings when it comes to learning from very long
sequences: learning a recurrent network with BPTT requires unfolding the network through time
for as many timesteps as there are in the sequence. For long sequences this represents a heavy
computational and memory load. This shortcoming is often overcome heuristically, by arbitrarily
splitting the initial sequence into subsequences, and only backpropagating on the subsequences. The
resulting algorithm is often referred to as Truncated Backpropagation Through Time (truncated BPTT,
see for instance Jaeger (2005)). This comes at the cost of losing long term dependencies.
We introduce Anticipated Reweighted Truncated BackPropagation (ARTBP), a variation of truncated
BPTT designed to provide an unbiased gradient estimate, accounting for long term dependencies.
Like truncated BPTT, ARTBP splits the initial training sequence into subsequences, and only back-
propagates on those subsequences. However, unlike truncated BPTT, ARTBP splits the training
sequence into variable size subsequences, and suitably modifies the backpropagation equation to
obtain unbiased gradients.
Unbiasedness of gradient estimates is the key property that provides convergence to a local optimum
in stochastic gradient descent procedures. Stochastic gradient descent with biased estimates, such as
the one provided by truncated BPTT, can lead to divergence even in simple situations and even with
large truncation lengths (Fig. 3).
ARTBP is experimentally compared to truncated BPTT. On truncated BPTT failure cases, typically
when balancing of temporal dependencies is key, ARTBP achieves reliable convergence thanks to
unbiasedness. On small-scale but real world data, ARTBP slightly outperforms truncated BPTT on
the test case we examined.
ARTBP formalizes the idea that, on a day-to-day basis, we can perform short term optimization, but
must reflect on long-term effects once in a while; ARTBP turns this into a provably unbiased overall
gradient estimate. Notably, the many short subsequences allow for quick adaptation to the data, while
preserving overall balance.
1
Under review as a conference paper at ICLR 2018
1	Related Work
BPTT Werbos (1990) and its truncated counterpart Jaeger (2005) are nearly uncontested in the
recurrent learning field. Nevertheless, BPTT is hardly applicable to very long training sequences, as
it requires storing and backpropagating through a network with as many layers as there are timesteps
Sutskever (2013). Storage issues can be partially addressed as in Gruslys et al. (2016), but at an
increased computational cost. Backpropagating through very long sequences also implies performing
fewer gradient descent steps, which significantly slows down learning Sutskever (2013).
Truncated BPTT heuristically solves BPTT deficiencies by chopping the initial sequence into evenly
sized subsequences. Truncated BPTT truncates gradient flows between contiguous subsequences,
but maintains the recurrent hidden state of the network. Truncation biases gradients, removing any
theoretical convergence guarantee. Intuitively, truncated BPTT has trouble learning dependencies
above the range of truncation. 1
NoBackTrack Ollivier et al. (2015) and Unbiased Online Recurrent Optimization (UORO) Tallec &
Ollivier (2017) both scalably provide unbiased online recurrent learning algorithms. They take the
more extreme point of view of requiring memorylessness, thus forbidding truncation schemes and any
storage of past states. NoBackTrack and UORO’s fully online, streaming structure comes at the price
of noise injection into the gradient estimates via a random rank-one reduction. ARTBP’s approach to
unbiasedness is radically different: ARTBP is not memoryless but does not inject artificial noise into
the gradients, instead, compensating for the truncations directly inside the backpropagation equation.
2	Background on recurrent models
The goal of recurrent learning algorithms is to optimize a parametric dynamical system, so that its
output sequence, or predictions, is as close as possible to some target sequence, known a priori.
Formally, given a dynamical system with state s, inputs x, parameter θ, and transition function F ,
st+1 = F (xt+1, st, θ)	(1)
the aim is to find a θ minimizing a total loss with respect to target outputs OJ= at each time,
TT
Lτ = X't = X '(st,ot).	(2)
t=1	t=1
A typical case is that of a standard recurrent neural network (RNN). In this case, st = (ot, ht), where
ot are the activations of the output layer (encoding the predictions), and ht are the activations of the
hidden recurrent layer. For this simple RNN, the dynamical system takes the form
ht+1 = tanh(Wx xt+1 + Wh ht + b)
ot+1 = Wo ht+1
't+1 = '(Ot+1, o；+1)
with parameters θ = (Wx, Wh, b).
Commonly, θ is optimized via a gradient descent procedure, i.e. iterating
θ Jθ - ηdLτ
where η is the learning rate. The focus is then to efficiently compute ∂LT /∂θ.
(3)
(4)
(5)
(6)
Backpropagation through time is a method of choice to perform this computation. BPTT computes
the gradient by unfolding the dynamical system through time and backpropagating through it, with
each timestep corresponding to a layer. BPTT decomposes the gradient as a sum, over timesteps t, of
the effect of a change of parameter at time t on all subsequent losses. Formally,
T
^∂θT=X δ't 额 (χt,st-1,θ)
t=1
(7)
1 Still, as the hidden recurrent state is not reset between subsequences, it may contain hidden information
about the distant past, which can be exploited Sutskever (2013).
2
Under review as a conference paper at ICLR 2018
(a) BPTT	(b) Truncated BPTT
(c) ARTBP
Figure 1: Graphical representation of BPTT, truncated BPTT and ARTBP. Blue arrows represent
forward propagations, red arrows backpropagations. Dots represent either internal state resetting or
gradient resetting.
where δ' :=号? is computed backward iteratively according to the backpropagation equation
心一∂'
I δ'τ = ^7Γ(sT, OT)
∂s
∂F , 一 ∂'.	.
I δ't = δ't+1 W-(Xt+1, st, θ) + ~^~(st, Ot )∙
∂s	∂s
(8)
These backpropagation equations extend the classical ones Jaeger (2005), which deal with the case of
a simple RNN for F .
Unfortunately, BPTT requires processing the full sequence both forward and backward. This requires
maintaining the full unfolded network, or equivalently storing the full history of inputs and activations
(though see Gruslys et al. (2016)). This is impractical when very long sequences are processed with
large networks: processing the whole sequence at every gradient step slows down learning.
Practically, this is alleviated by truncating gradient flows after a fixed number of timesteps, or
equivalently, splitting the input sequence into subsequences of fixed length, and only backpropagating
through those subsequences. 2 This algorithm is referred to as Truncated BPTT. With truncation
length L < T, the corresponding equationsjust drop the recurrent term δ't+ι 箓(xt+ι, st, θ) every
L time steps, namely,
δ^ := ʃ ∂` (SE
t := 1 遂 ∂F.	n. , ∂',	*、
[δ't+1 击 (χt+1,st ,θ) + ∂s (St ,ot)
if t is a multiple of L
otherwise.
(9)
This also allows for online application: for instance, the gradient estimate from the first subsequence
t = 1 . . . , L does not depend on anything at time t > L.
However, this gradient estimation scheme is heuristic and provides biased gradient estimates. In gen-
eral the resulting gradient estimate can be quite far from the true gradient even with large truncations
L (Section 6). Undesired behavior, and, sometimes, divergence can follow when performing gradient
descent with truncated BPTT (Fig. 3).
3 Anticipated Reweighted Backpropagation Through Time:
UNBIASEDNESS THROUGH REWEIGHTED STOCHASTIC TRUNCATION
LENGTHS
Like truncated BPTT, ARTBP splits the initial sequence into subsequences, and only performs
backpropagation through time on subsequences. However, contrary to the latter, it does not split
the sequence evenly. The length of each subsequence is sampled according to a specific probability
distribution. Then the backpropagation equation is modified by introducing a suitable reweighting
factor at every step to ensure unbiasedness. Figure 1 demonstrates the difference between BPTT,
truncated BPTT and ARTBP.
Simply sampling arbitrarily long truncation lengths does not provide unbiasedness. Intuitively, it still
favors short term gradient terms over long term ones. When using full BPTT, gradient computations
flow back 3 from every timestep t to every timestep t0 < t. In truncated BPTT, gradients do not
2Usually the internal state st is maintained from one subsequence to the other, not reset to a default value.
3 Gradient flows between timesteps t and t0 if there are no truncations occuring between t and t0 .
3
Under review as a conference paper at ICLR 2018
flow from t to t0 if t - t0 exceeds the truncation length. In ARTBP, since random truncations are
introduced, gradient computations flow from t to t0 with a certain probability, decreasing with t - t0 .
To restore balance, ARTBP rescales gradient flows by their inverse probability. Informally, if a flow
has a probability P to occur, multiplication of the flow by P restores balance on average.
Formally, at each training epoch, ARTBP starts by sampling a random sequence of truncation points,
that is (Xt)1≤t≤T ∈ {0, 1}T. A truncation will occur at all points t such that Xt = 1. Here Xt may
have a probability law that depends on X1, . . . , Xt-1, and also on the sequence of states (st)1≤t≤T
of the system. The reweighting factors that ARTBP introduces in the backpropagation equation
depend on these truncation probabilities. (Unbiasedness is not obtained just by global importance
reweighting between the various truncated subsequences: indeed, the backpropagation equation
inside each subsequence has to be modified at every time step, see (11).)
The question of how to choose good probability distributions for the truncation points Xt is postponed
till Section 4. Actually, unbiasedness holds for any choice of truncation probabilities (Prop 1), but
different choices for Xt lead to different variances for the resulting gradient estimates.
Proposition 1. Let (Xt)t=1...T be any sequence of binary random variables, chosen according to
probabilities
ct := P(Xt = 1 | Xt-1, . . . , X1)	(10)
and assume ct 6= 1 for all t.
Define ARTBP to be backpropagation through time with a truncation between t and t + 1 iff Xt = 1,
and a compensation factor 1-^— when Xt = 0, namely:
-c
∂'.	*、
∂S (st,ot)
;-------δ't+ι -^-(Xt+1, st, θ) + Tr(St, ot)
1 - ct	∂s	∂s
if Xt = 1 or t = T
otherwise.
(11)
∙-v
Let g be the gradient estimate obtained by using δ't instead of δ't in ordinary BPTT (7), namely
g:= XX 3 ∂F (Xt,st-ι,θ)
t=1
(12)
Then, on average over the ARTBP truncations, this is an unbiased gradient estimate of the total loss:
Exi,...,Xt [g] = dLθT.
(13)
∙-v
The core of the proof is as follows: With probability Ct (truncation), δ't+ι does not contribute to
δ't. With probability 1 -Ct (no truncation), it contributes with a factor I-V. So on average, δ't+ι
∙-v
contributes to δ't with a factor 1, and ARTBP (11) reduces to standard, non-truncated BPTT (8) on
average. The detailed proof is given in Section 8.
While the ARTBP gradient estimate above is unbiased, some noise is introduced due to stochasticity
of the truncation points. It turns out that ARTBP trades off memory consumption (larger truncation
lengths) for variance, as we now discuss.
4	CHOICE OF ct AND MEMORY/VARIANCE TRADEOFF
ARTBP requires specifying the probability Ct of truncating at time t given previous truncations.
Intuitively the C’s regulate the average truncation lengths. For instance, with a constant Ct ≡ C, the
lengths of the subsequences between two truncations follow a geometric distribution, with average
truncation length C. Truncated BPTT with fixed truncation length L and ARTBP with fixed C = L
are thus comparable memorywise.
Small values of Ct will lead to long subsequences and gradients closer to the exact value, while large
values will lead to shorter subsequences but larger compensation factors and noisier estimates.
1-ct
4
Under review as a conference paper at ICLR 2018
In particular, the product of the =r factors inside a subsequence can grow quickly. For instance, a
1-ct
constant Ct leads to exponential growth of the cumulated factors when iterating (11).
1-ct
To mitigate this effect, we suggest to set ct to values such that the probability to have a subsequence
of length L decreases like L-α . The variance of the lengths of the subsequences will be finite if
α > 3. Moreover we might want to control the average truncation length L0. This is achieved via
α- 1
Ct = P(Xt = 1 | Xt-1, ...,X1) = 7_,) - —	(14)
(α - 2)L0 + δt
where δt is the time elapsed since the last truncation, δt = t - sup{s | s < t, Xs = 1}. Intuitively,
the more time spent without truncating, the lower the probability to truncate. This formula is chosen
such that the average truncation length is approximately L0, and the standard deviation from this
average length is finite. The parameter α controls the regularity of the distribution of truncation
lengths: all moments lower than α - 1 are finite, the others are infinite. With larger α, large lengths
will be less frequent, but the compensating factors ∙τ1~ will be larger.
1-ct
With this choice of Ct, the product of the 11— factors incurred by backpropagation inside each
1-ct
subsequence grows polynomially like Lα-1 in a subsequence of length L. If the dynamical system
has geometrically decaying memory, i.e., if the operator norm of the transition operator ∂F is less
than 1 - ε most of the time, then the value of δ' will stay controlled, since (1 - ε)L ∙ La stays
∙-v
bounded. On the other hand, using a constant Ct ≡ C provides bounded δ' only for small values
C < ε.
In the experiments below, we use the Ct from (14) with α = 4 or α = 6.
5	Online implementation
Importantly, ARTBP can be directly applied online, thus providing unbiased gradient estimates for
recurrent networks.
Indeed, not all truncation points have to be drawn in advance: ARTBP can be applied by sampling
the first truncation point, performing both forward and backward passes of BPTT up until this point,
and applying a partial gradient descent update based on the resulting gradient on this subsequence.
Then one moves to the next subsequence and the next truncation point, etc. (Fig. 1c).
6	Experimental validation
The experimental setup below aims both at illustrating the theoretical properties of ARTBP compared
to truncated BPTT, and at testing the soundness of ARTBP on real world data.
6.1	Influence balancing
The influence balancing experiment is a synthetic example demonstrating, in a very simple model,
the importance of being unbiased. Intuitively, a parameter has a positive short term influence, but a
negative long term one that surpasses the short term effect. Practically, we consider a row of agents,
numbered from left to right from 1 to p + n who, at each time step, are provided with a signal
depending on the parameter, and diffuse part of their current state to the agent directly to their left.
The p leftmost agents receive a positive signal at each time step, and the n rightmost agents a negative
signal. The training goal is to control the state of the leftmost agent. The first p agents contribute
positively to the first agent state, while the next n contribute negatively. However, agent 1 only feels
the contribution from agent k after k timesteps. If optimization is blind to dependencies above k, the
effect of k is never felt. A typical instantiation of such a problem would be that of a drug whose effect
varies after various delays; the parameter to be optimized is the quantity of drug to be used daily.
Such a model can be formalized as Tallec & Ollivier (2017)
st+1 = Ast + (θ, . . . , θ, -θ, . . . , -θ)>	(15)
with A a square matrix of size p + n with Ak,k = 1/2, Ak,k+1 = 1/2, and 0 elsewhere; stk
corresponds to the state of the k-th agent. θ ∈ R is a scalar parameter corresponding to the intensity
5
Under review as a conference paper at ICLR 2018
Figure 2: Influence balancing dynamics, 1 positive influence, 3 negative influences.
of the signal observed at each time step. The right-hand-side has p positive-θ entries and n negative-θ
entries. The loss considered is an arbitrary target on the leftmost agent s1,
't = 2 (s1 - 1)2.	(16)
The dynamics is illustrated schematically in Figure 2.
Fixed-truncation BPTT is experimentally compared with ARTBP for this problem. The setting is
online: starting at t = 1, a first truncation length L is selected (fixed for BPTT, variable for ARTBP),
forward and backward passes are performed on the subsequence t = 1, . . . , L, a vanilla gradient
step is performed with the resulting gradient estimate, then the procedure is repeated with the next
subsequence starting at t = L + 1, etc..
Our experiment uses p = 10 and n = 13, so that after 23 steps the signal should have had time to
travel through the network. Truncated BPTT is tested with various truncations L = 10, 100, 200.
(As the initial θ is fixed, truncated BPTT is deterministic in this experiment, thus we only provide
a single run for each L.) ARTBP is tested with the probabilities (14) using L0 = 16 (average
truncation length) and α = 6. ARTBP is stochastic: five random runs are provided to test reliability
of convergence.
The results are displayed in Fig. 3. We used decreasing learning rates η = √⅛ where no = 3 × 10-4
1+t
is the initial learning rate and t is the timestep. We plot the average loss over timesteps 1 to t, as a
function of t.
ssol detalumuC
30 25 20 15 10
. . . . . ‘ . ‘ 1
ARTBP
10-truncated BPTT
100-truncated BPTT
200-truncated BPTT
20000
40000	60000
Epoch
80000	100000
20000	40000	60000	80000	100000
Epoch
Figure 3: ARTBP and truncated BPTT on influence balancing, n = 13, p = 10. Note the log scale
on the y-axis.
Truncated BPTT diverges even for truncation ranges largely above the intrinsic temporal scale of the
system. This is an expected result: due to bias, truncated BPTT ill-balances temporal dependencies
and estimates the overall gradient with a wrong sign. In particular, reducing the learning rate will not
prevent divergence. On the other hand, ARTBP reliably converges on every run.
Note that for the largest truncation L = 200, truncated BPTT finally converges, and does so at a faster
rate than ARTBP. This is because this particular problem is deterministic, so that a deterministic
6
Under review as a conference paper at ICLR 2018
1.4
)retcarahc rep stib( ssol niarT
2	4	6	8	10	12	14	16	18	20
Training epoch
(a) Learning curves on Penn Treebank train set.
1.4
)retcarahc rep stib( ssol noitadilaV
2	4	6	8	10	12	14	16	18	20
Training epoch
(b) Learning curves on Penn Treebank validation set.
Figure 4: Results on Penn Treebank character-level language modelling.
gradient scheme will converge (if it does converge) geometrically like O(e-λt), whereas ARTBP is
stochastic due to randomization of truncations, and so will not converge faster than O(t-1/2). This
difference would disappear, for instance, with noisy targets or a noisy system.
Character-level Penn Treebank language model. We compare ARTBP to truncated BPTT on the
character-level version of the Penn Treebank dataset, a standard set of case-insensitive, punctuation-
free English text Marcus et al. (1993). Character-level language modelling is a common benchmark
for recurrent models.
The dataset is split into training, validation and test sets following Mikolov et al. (2012). Both ARTBP
and truncated BPTT are used to train an LSTM model Hochreiter & Schmidhuber (1997) with a
softmax classifier on its hidden state, on the character prediction task. The training set is batched into
64 subsets processed in parallel to increase computing speed. Before each full pass on the training
set, the batched training sequences are split into subsequences:
•	for truncated BPTT, of fixed size 50;
•	for ARTBP, at random following the scheme (14) with α = 4 and L0 = 50.
Truncated BPTT and ARTBP process these subsequences sequentially, 4 as in Fig. 1. The parameter
is updated after each subsequence, using the Adam Kingma & Ba (2014) stochastic gradient scheme,
with learning rate 10-4. The biases of the LSTM unit forget gates are set to 2, to prevent early
vanishing gradients Gers et al. (2000). Results (in bits per character, bpc) are displayed in Fig. 4. Six
randomly sampled runs are plotted, to test reliability.
In this test, ARTBP slightly outperforms truncated BPTT in terms of validation and test error, while
the reverse is true for the training error (Fig. 4).
Even with ordinary truncated BPTT, we could not reproduce reported state of the art results, and
do somewhat worse. We reach a test error of 1.43 bpc with standard truncated BPTT and 1.40 bpc
with ARTBP, while reported values with similar LSTM models range from 1.38 bpc Cooijmans et al.
(2016) to 1.26 bpc Graves (2013) (the latter with a different test/train split). This may be due to
differences in the experimental setup: we have applied truncated BPTT without subsequence shuffling
or gradient clipping Graves (2013) (incidentally, both would break unbiasedness). Arguably, the
numerical issues solved by gradient clipping are model specific, not algorithm specific, while the
point here was to compare ARTBP to truncated BPTT for a given model.
4 Subsequences are not shuffled, as we do not reset the internal state of the network between subsequences.
7
Under review as a conference paper at ICLR 2018
7 Conclusion
We have shown that the bias introduced by truncation in the backpropagation through time algorithm
can be compensated by the simple mathematical trick of randomizing the truncation points and
introducing compensation factors in the backpropagation equation. The algorithm is experimentally
viable, and provides proper balancing of the effects of different time scales when training recurrent
models.
8 Proof of Proposition 1
First, by backward induction, we show that for all t ≤ T, for all x1, . . . , xt-1 ∈ {0, 1},
E [S" | Xi：t-i = xi：t-ii = δ't
where δ' is the value obtained by ordinary BPTT (8). Here xi：k is short for (xι,...,Xk).
(17)
∙-v
For t = T, this holds by definition: δ'τ
∂' (sT ,oT) = δ'τ.
Assume that the induction hypothesis (17) holds at time t + 1. Note that the values st do not depend
on the random variables Xt, as they are computed during the forward pass of the algorithm. In
particular, the various derivatives of F and ` in (11) do not depend on X1:T.
Thus
E hδ⅛ | X1:t-1 = xLt-l]=
P(Xt = 1 | Xi：t-1 = Xi：t-1)E [δ2t | Xi：t-1 = xi：t-i,Xt = l] +
(18)
P(Xt = 0 | Xi：t-i = xi：t-i) E δ't | Xi：t-i = xi：t-i, Xt
ct E [δ^t | X1:t-1 = x1:t-1, Xt = 1i + (1 - ct) E [δ^t | X1:t-1
0
= x1:t-1 , Xt = 0
(19)
(20)
∙-v
If Xt = 1 then δ't
⅛∣(st,ot). If Xt = 0, then δ't = ∂'(st, ot) + ι⅛t δ't+ι d∂F(χt+ι, st, θ).
Therefore, substituting into (20),
E [解 1 Xi：t-i = xi：t-i] = ∂s (st, ot') + E [δ2t+ι | Xi：t-i = xi：t-i, Xt = 0i ^∂s^ (Xt+1, st,θ)
(21)
but by the induction hypothesis at time t +1, this is exactly ∂' (st, o⅛) + δ't+ι ∂∂S (xt+ι, St, θ), which
is δ't.
Therefore, E [δ"] = δ't unconditionally. Plugging the δ2's into (7), and averaging
T	∂F
EXI ,...,Xt [ g] = X EXt,...,Xτ [δ't] ∂θ (xt,st-1,θ)
t=1
T ∂F
=£阴而 (χt,st-ι,θ)
t=1
∂LT
=--—
∂θ
(22)
(23)
(24)
which ends the proof.
8
Under review as a conference paper at ICLR 2018
References
Tim Cooijmans, Nicolas Ballas, Cesar Laurent, and Aaron C. Courville. Recurrent batch normaliza-
tion. CoRR, abs/1603.09025, 2016. URL http://arxiv.org/abs/1603.09025.
Felix A. Gers, Jurgen A. Schmidhuber, and Fred A. Cummins. Learning to forget: Con-
tinual prediction with LSTM. Neural Comput., 12(10):2451-2471, October 2000. ISSN
0899-7667. doi: 10.1162/089976600300015015. URL http://dx.doi.org/10.1162/
089976600300015015.
Alex Graves. Generating sequences with recurrent neural networks. CoRR, abs/1308.0850, 2013.
URL http://arxiv.org/abs/1308.0850.
Audrunas Gruslys, Remi Munos, Ivo Danihelka, Marc Lanctot, and Alex Graves. Memory-
efficient backpropagation through time. In Daniel D. Lee, Masashi Sugiyama, Ulrike von
Luxburg, Isabelle Guyon, and Roman Garnett (eds.), NIPS, pp. 4125-4133, 2016. URL http:
//dblp.uni-trier.de/db/conf/nips/nips2016.html#GruslysMDLG16.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Comput., 9(9):1735-
1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL http://dx.
doi.org/10.1162/neco.1997.9.8.1735.
Herbert Jaeger. A tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF and the
"echo state network" approach. 2005. http://minds.jacobs-university.de/sites/
default/files/uploads/papers/ESNTutorialRev.pdf.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR,
abs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated
corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313-330, 1993.
Tomds Mikolov, Ilya Sutskever, Anoop Deoras, Le Hai-Son, Stefan Kombrink, and Jan Cernocky.
Subword language modeling with neural networks. 2012.
Yann Ollivier, Corentin Tallec, and Guillaume Charpiat. Training recurrent networks online without
backtracking. arXiv preprint arXiv:1507.07680, 2015.
Ilya Sutskever. Training Recurrent Neural Networks. PhD thesis, Toronto, Ont., Canada, Canada,
2013. AAINS22066.
Corentin Tallec and Yann Ollivier. Unbiased online recurrent optimization. arXiv preprint
arXiv:1702.05043, 2017.
P. Werbos. Backpropagation through time: what does it do and how to do it. In Proceedings of IEEE,
volume 78, pp. 1550-1560, 1990.
9