Under review as a conference paper at ICLR 2018
Bayesian Time Series Forecasting with Change
Point and Anomaly Detection
Anonymous authors
Paper under double-blind review
Ab stract
Time series forecasting plays a crucial role in marketing, finance and many other
quantitative fields. A large amount of methodologies has been developed on this
topic, including ARIMA, Holt-Winters, etc. However, their performance is eas-
ily undermined by the existence of change points and anomaly points, two struc-
tures commonly observed in real data, but rarely considered in the aforementioned
methods. In this paper, we propose a novel state space time series model, with
the capability to capture the structure of change points and anomaly points, as
well as trend and seasonality. To infer all the hidden variables, we develop a
Bayesian framework, which is able to obtain distributions and forecasting intervals
for time series forecasting, with provable theoretical properties. For implementa-
tion, an iterative algorithm with Markov chain Monte Carlo (MCMC), Kalman
filter and Kalman smoothing is proposed. In both synthetic data and real data ap-
plications, our methodology yields a better performance in time series forecasting
compared with existing methods, along with more accurate change point detection
and anomaly detection.
1	Introduction
Time series forecasting has a rich and luminous history, and is essentially important in most of
business operations nowadays. The main aim of time series forecasting is to carefully collect and
rigorously study the past observations of a time series to develop an appropriate model which could
describe the inherent structure of the series, in order to generate future values. For instance, the
internet companies are interested in the number of daily active users (DAU), say, what is DAU after
certain period of time, or when will reach their target DAU goal.
Time series forecasting is a fruitful research area with many existing methodologies. The most
popular and frequently used time series model might be the Autoregressive Integrated Moving Av-
erage (ARIMA) (Box et al., 2015; Zhang, 2003; Cochrane, 2005; Hipel & McLeod, 1994). Taking
seasonality into consideration, Box et al. (2015) proposed the Seasonal ARIMA. The Holt-Winters
method (Winters, 1960) is also very popular by using exponential smoothing. State space model
(Durbin & Koopman, 2012; Scott & Varian, 2014; Brodersen et al., 2015) also attracts much atten-
tion, which is a linear function of an underlying Markov process plus additive noise. Exponential
Smoothing State Space Model (ETS) (Hyndman et al., 2008) decomposes times series into error,
trend, seasonal that change over time. Recently, deep learning is applied for time-series trend learn-
ing using LSTM (Tao Lin, 2017), bidirectional dynamic Boltzmann machine (Osogami et al., 2017)
is applied for time-series long-term dependency learning, and coherent probabilistic forecast (Taieb
et al., 2017) is proposed for a hierarchy or an aggregation-level comprising a set of time series. Or-
thogonal to these works, this paper focuses on robust ways of time series forecasting in presence of
change points and anomalies.
In Internet time series forecasting, Google develops the Bayesian structure time series (BSTS) model
(Brodersen et al., 2015; Scott & Varian, 2014) to capture the trend, seasonality, and similar com-
ponents of the target series. Recently, Facebook proposes the Prophet approach (Taylor & Letham,
2017) based on a decomposable model with interpretable parameters that can be intuitively adjusted
by analyst.
1
Under review as a conference paper at ICLR 2018
However, as in the DAU example, some special events like Christmas Holiday or President Election,
newly launched apps or features, may cause short period or long-term change of DAU, leading to
weird forecasting of those traditional models. The aforementioned special cases are well known as
•	Anomaly points. The items, events or observations that don’t conform to an expected
pattern or other items in the dataset, leading to a sudden spike or decrease in the series.
•	Change points. A market intervention, such as a new product launch or the onset of an
advertising (or ad) campaign, may lead to the level change of the original series.
Time series forecasting without change/anomaly point detection and adjustment may also lead to
bizarre forecasting since these models might learn the abrupt changes in the past. There are litera-
tures on detecting anomaly or change points individually, examples can be found in Twitter (2017);
Netflix (2017); Barry & Hartigan (1993); Killick & Eckley (2014); twitter (2017). However, the
aforementioned change point detection models could not support detection in the presence of sea-
sonality, while the presence of trend/change point is not handled by the anomaly detection models.
Most importantly, there is a discrepancy between anomaly/change points detection and adjustment,
and commonly used manually adjustment might be a bit arbitrary. Unfortunately, the forecasting
gap caused by abnormal and change points, to the best of our knowledge, has not been given full
attention and no good solution has been found so far. This paper is strongly motivated by bridging
this gap.
In this paper, to overcome the limitations of the most (if not all) current models that the anomaly
points and change points are not properly considered, we develop a state space time series forecast-
ing model in the Bayesian framework that can simultaneously detect anomaly and change points and
perform forecasting. The learned structure information related to anomaly and change points is au-
tomatically incorporated into the forecasting process, which naturally enhances the model prediction
based on the feedback of state-space model. To solve the resultant optimization problem, an itera-
tive algorithm based on Bayesian approximate inference with Markov chain Monte Carlo (MCMC),
Kalman filter and Kalman smoothing is proposed. The novel model could explicitly capture the
structure of change points, anomaly points, trend and seasonality, as also provide the distributions
and forecasting intervals due to Bayesian forecasting framework. Both synthetic and real data sets
show the better performance of proposed model, in comparison with existing baseline. Moreover,
our proposed model outperforms state-of-the-art models in identifying anomaly and change points.
To summarize, our work has the following contributions.
•	We proposed a robust 1 Bayesian state-space time series forecasting model that is able to ex-
plicitly capture the structures of change points and anomalies (which are generally ignored
in most current models), and therefore automatically adapt for forecasting by incorporating
the prior information of trend, seasonality, as well as change points and anomalies using
state space modeling. Due to the enhancement of model description capability, the results
of model prediction and abnormal and change points detection are mutually improved.
•	To solve the resultant optimization problem, an effective algorithm based on approximate
inference using Markov chain Monte Carlo (MCMC) is proposed with theoretical guaran-
teed forecasting paths.
•	Our proposed method outperforms the state-of-the-art methods in time series forecasting
in presence of change points and anomalies, and detects change points and anomalies with
high accuracy and low false discovery rate on both tasks, outperforming popular change
point and anomaly detection methods. Our method is flexible to capture the structure of
time series under various scenarios with any component combinations of trend, seasonality,
change points and anomalies. Therefore our method can be applied in many settings in
practice.
1We call it “robust” because it can accommodate “noises” much better since change points and anomalies
can be viewed as a type of “noise”.
2
Under review as a conference paper at ICLR 2018
2	Model Overview
State space time series model (Hangos et al., 2014) has been one of the most popular models in time
series analysis. It is capable of fitting complicated time series structure including linear trend and
seasonality. However, times series observed in real life are almost all prevailed with outliers. Change
points, less in frequency but are still widely observed in real time series analysis. Unfortunately, both
structures are ignored in the classic state space time series model. In the section, we aim to address
this issue by introducing a novel state space time series model.
Residual
Observed Data
Trend
Seasonality
Change Points
Anomaly Points
Figure 1: Demostration of Decompositions.

0	100	200	300	400	500
Let y = (y1 , y2 , . . . , yn ) be a sequence of time series observations with length n. The ultimate goal
is to forecast (yn+1, yn+2, . . .). The accuracy in forecasting lies in a successful decomposition of
y into existing components. Apart from the residuals, we assume the time series is composed by
trend, seasonality, change points and anomaly points. In a nutshell, we have an additive model with
time series = trend + seasonality + change point + anomaly point + residual.
Figure 1 provides a demonstration of desired decomposition of time series. In Figure 1, the left
panel shows the observed time series. And it can be decomposed into the remaining five panels. The
shift in the change point panel shows where the change point lies. And the spikes in the last panel
reveals the anomaly points.
As the classical state space model, we have observation equation and transition equations to model
y and hidden variables. We use μ = (μ1,μ2,... ,μn) to model trend, and use Y = (γ1,γ2,..., γn)
to model seasonality. We use a binary vector za = (z1a, z2a, . . . , zna) to indicate anomaly points.
Then we have
t, if zta = 0
Observation equation: yt = μt + Yt + <	ba
ot, if zta = 1
(1)
The deviation between the observation yt and its “mean” μt + Yt is modeled by Et and ot, depending
on the value ofZta. If Zta = 1, then yt is an anomaly point; otherwise it is not. Distinguished from the
residues = (E1, E2, . . . , En), the anomaly is captured by o = (o1, o2, . . . , on) which has relative
large magnitude.
The hidden state variable μ and Y have intrinsic structures. There are two transition equations, for
trend and seasonality separately
Transition Equations:
ut , if ztc = 0
Trend： μt = μt-ι + δt-ι + <	C
rt, if ztc = 1
δt = δt-1 + vt ,
S-1
Seasonality: γt = -	γt-s + wt.
(2)
(3)
s=1
In Equation (2), δ = (δ1, δ2, . . . , δn) can be viewed as the “slope” of the trend, measuring how fast
the trend changes over time. The change point component is also incorporated in Equation (2) by
a binary vector Zc = (zf, zC,..., Zn). If Zc = 1, it means the t-th point is a change point, with μt
differs from μ1 + δ1 (which can be interpreted as the “momentum" from the previous status)
by rt; otherwise it is not a change point and they differ by ut. We model the change points in a way
such that r = (r1, r2, . . . , rn) have larger magnitude compared u = (u1, u2, . . . , un). The “slope”
part δ also has its own noise v = (v1, v2, . . . , vn).
A first look on Equation (2) may bring up with the question that it is not presented in an exactly the
same way as shown in Figure 1. In Figure 1, the change points component is a step function, and it
3
Under review as a conference paper at ICLR 2018
is one of the five additive components along with trend, seasonality, anomaly points and residuals.
Here we model the change point directly into the trend component. Though differing in formulation,
they are equivalent to each other. We choose to model in as in Equation (2) due to simplicity, and its
similarity with the definition of anomaly points in Equation (1).
The seasonality component is presented in Equation (3). Here S is the length of one season and
w = (w1, w2, . . . , wn) is the noise for seasonality. The seasonality component is assumed to have
almost zero average in each season.
The observation equation and transition equations (i.e., Equation (1,2,3)) define how y is generated
from all the hidden variables including change points and anomaly points. We continue to explore
this new model, under a Bayesian framework.
3 Bayesian Framework
Bayesian methods are widely used in many data analysis fields. Itis easy to implement and interpret,
and it also has the ability to produce posterior distribution. The Bayesian method on state space time
series model has been investigated in Scott & Varian (2014); Brodersen et al. (2015). In this section,
we also consider Bayesian framework for our novel state space time series model. We assume all
the noises are normally distributed
{et}=ι% N(0,σ2),	{θt}n=ιiid N(0,σ2),	{ut}=ι% N(0,σU),
n iid	2	n iid	2	n iid	2
{rt}t=1 〜N(0,σr ), {vt}t=1 〜N(0,σv), {wt}t=1 〜N(0,σw),
where σ , σo , σu , σr , σv , σw are parameters for standard deviation. As binary vectors, a natural
choice is to model anomaly point indicator za and change point indicator zc to the model them as
Bernoulli random variables
{za}n=ιiid Ber(Pa),	{zf\"驶 Ber(Pc),
where pa , pc are probabilities for each point to be an anomaly or change point.
Figure 2: Graphical presentation of our model. Note that y is observed, highlighted by gray back-
ground, distinguished from all the remaining ones that are hidden. Among the hidden ones, squares
indicate fixed parameters, and circles indicate random variables.
For simplicity, We denote αt = (μt, δt, γt, γt-1,..., Yt-(S-2)) to include the main hidden variables
(except zta and ztc) in the transition equations. All the αt are well defined and can be generated from
the previous status, except α1. We denote a1 to be the parameter for α1, Which can be interpreted
as the “mean” for α1.
With Bayesian frameWork, We are able to represent our model graphically as in Figure 2. As shoWn
in Figure 2, the only observations are y and all the others are hidden. In this paper, We assume
there is no additional information on all the hidden states. If We have some prior information, for
example, some points are more likely to be change points, then our model can be easily modified to
incorporate such information, by using proper prior.
4
Under review as a conference paper at ICLR 2018
In Figure 2, we use squares and circles to classify unknown variables. Despite all being unknown,
they actually behave differently according to their own functionality. For those in squares, they
behave like turning parameters. Once they are initialized or given, those in circles behaves like
latent variables. We call the former “parameters” and the latter “latent variable”, as listed in Table 1.
Table 1: TWo Categories for Hidden Variables
Category	Hidden Variable	Definition
Latent Variable	α = (α1,α2,... , an)	Trend and seasonality
	Z = (za, Zc) ~~	Anomaly and change points
Parameter	aι	The “mean” for the ini- tial trend and seasonal- ity
	P = (Pa,Pc)	Probabilities for each point to be anomaly or change point
	~σ = (σ e ,σo , σu,σr ,σv ,σw )-	Standard deviation
The discrepancy between these two categories is clearly captured by the joint likelihood function.
From Figure 2, the joint distribution (i.e., the likelihood function) can be Written doWn explicitly as
La1,p,σ (y, α, z)	(4)
= [ɪ	g(yt	- μt -	γt,σe) × [ɪ	g(yt	-	μt	- γt,σ°)	× [ɪ	g(μt - μt-i	- δt-i,σu)
{t:zta =0}	{t:zta =1}	{t:ztc =0}
n	n	S-1	n
× Y	g(μt - μt-i - δt-i,σr) × Y g(δt - δt-i,σv)×Yg(-Xγt-s, σv) × Y(pa)zta (1 - pa)1-zta (pc)ztc (1 - pc)1-ztc,
{t:ztc =1}	t=1	t=1	s=1	i=1
where g(x1,x2) = √∏^ exp (-x1∕(2x2)) is the density function for normal distribution
With mean x1 and standard deviation x2 . Here We slightly abuse the notation by using
μo, δo, γo, γ-ι,..., γ2-s, which are actually the corresponding coordinates of aι.
As long with other probabilistic graphical models, our model can also be viewed as a generative
model. Given the parameters a1 , p, σ, we are able to generate time series. We present the generative
procedure as follows.
Algorithm 1: Generative Procedure
Input: Parameters a1, σ = (σ, σo, σu, σr, σv, σw) andpa,pc, length of time series to generate m
Output: Time series y = (y1,y2, . . . , ym)
1	Generate the indexes where anomalies or change points occur
{zt}=ι 狎 Ber(pa),	{zCW=1 % Ber(p0);
2	Generate all the noises , o, u, r, v, w as independent normal random variables with mean zero and
standard deviation σ, σo, σu , σr, σv , σw respectively;
3	Generate {αt}tm=1 sequentially by the transition functions in Equation (2) and (3);
4	Generate time series {yt}tm=1 by the observation function in Equation (1).
4	Inference
This section is about inferring unknown variables from y, given the Bayesian setting described in the
previous section. The main framework here is to sequentially update each hidden variable by fixing
the remaining ones. As stated in the previous section, there are two different categories of unknown
variables. Different update schemes need to be used due to the difference in their functionality. For
the latent variables, we implement Markov chain Monte Carlo (MCMC) for inference. Particular,
we use Gibbs sampler. We will elaborate the details of updates in the following sections.
5
Under review as a conference paper at ICLR 2018
4.1	Updates on Trend and Seasonality
In this section, we focus on updating α assuming all the other hidden variables are given and
fixed. The essence of Gibbs sampler is to obtain posterior distributionPaι,p,σ(α∣y, z). This can be
achieved by a combination of Kalman filter, Kalman smoothing and the so-called “fake-path” trick.
We provide some intuitive explanation here and refer the readers to Durbin & Koopman (2012) for
detailed implementation.
Kalman filter and Kalman smoothing are classic algorithms in signal processing and pattern recol-
onization for Bayesian inference. It is well related to other algorithms especially message passing
algorithm. Kalman filter collects information forwards to obtain E(αt ∣y1, y2, . . . , yt); while Kalman
smoothing distribute information backwards to achieve E(αt ∣y).
However, the combination of Kalman filter and Kalman smoothing is not enough, as it only gives the
the expectations of marginal distributions {E(αt∣y)}tn=1, instead of the joint distribution required
for Gibbs sampler. To address this issue, we can use the “fake-path” trick described in Brodersen
et al. (2015); Durbin & Koopman (2012). The main idea underlying this trick lies on the fact that
the covariance structure of p(αt∣y) is not dependent on the means. If we are able to obtain the
covariance by some other way, then we can add it up with {E(αt∣y)}tn=1 to obtain a sample from
p(α∣y). This trick involves three steps. Note that all the other hidden variables z,p, σ are given.
1.	Pick some vector di, and generate a sequence of time series y from it by Algorithm 1. In
this way, We also observe <d.
2.	Obtain {E(αdt∣yd)}tn=1 from ydby Kalman filter and Kalman smoothing.
3.	We use {αdt - E(αdt∣yd) + E(αt∣y)}tn=1 as our sampling from the conditional distribution.
4.2	Change Point and Anomaly Detection
In this section, we update z by Gibbs sampler, assuming α, a1, p, σ are all given and fixed. We need
to obtain the conditional distribution pa1,p,σ (z ∣y, α). Note that in the graphical model described in
Section 2, {zta}tn=1 and {ztc}in=1 are all Bernoulli random variables and independent of each other.
Then the conditional distributionpa1,p,σ(z∣y, α) can also be decomposed into product of Bernoulli
density functions. In other words, conditioned on y, α, {zta}tn=1 and {ztc}in=1 are still independent
Bernoulli random variables, but possibly with different success probabilities. Thus, we can take the
calculation point by point. For example, for the anomaly detection for the t-th point, we have
Zt =0: yt - μt - Yt 〜N(0, σ2)
Za = 1 : yt - μt - γt 〜N(0,σO).
And the prior on zta is P(zta = 1) = pa and P(zta = 0) = p1. Let pta = P(zta = 1∣y, α). Directly
calculation leads to
____________σo eχp[Jyt-μσ-Yt)I
1-Pa αγn Γ	(yt-μt-Yt)2^∣ I Pa αγn Γ	(yt-μt-Yt)21
exp [	2σ2	+ σoexp [	2σo
This equality holds for all t = 1, 2, . . . , n. Similarly for change point detection, let ptc = P(Ztc
1∣y, α), and we have
Pc αγ 1~l h	(μt - μt-1 - δt -1) i
Pc = ________f σr	P1二	2σ2	J.	⑹
1-Pc PXn [	(μt-μt-i-δt-i) ] + Pc	(	(*t-μt-1-δt-I)2 ]
~^u~exp [	2σ2 J + σexp [	2σ J
As mentioned above, all the coordinates in z are still independent Bernoulli random variables con-
ditioned on y, α. Thus, for Gibbs sampler, we can generate z by sampling independently with
{zt}n=ι 〜Ber(pt),	{zf}t=ι 〜Ber(pc).
For change point detection here, we have an additional segment control step. After obtaining
{Ztc }tn=1 as mentioned above, we need to make sure that the change points detected satisfy some
additional requirement on the length of segment among two consecutive change points. This issue
6
Under review as a conference paper at ICLR 2018
arises from the ambiguity between the definitions of change point and anomaly points. For example,
consider a time series with value (0, 0, 0, 0, 1, 1, 1, 0, 0, 0). We can view it with two change points,
one increases the trend by 1 and the other decreases it by 1. Alternatively, we can also argue the
three 1s in this time series are anomalies, though next to each other. One way to address this ambi-
guity is by defining the minimum length of segment (denoted as `). In this toy example, if we set
the minimum length to be 4, then they are anomaly points; if we set it to be 3, then we regard them
to be change points. But a more complicated criterion is needed than using minimum length as the
time series usually own much more complex structure than this toy example. Consider time series
(0, 0, 0, 0, -1, -1, 1, 1, 1, 1) and the minimum time series parameter ` = 3. It is reasonable to view
it with one change point with increment 1, and the two -1s should be regarded as anomalies. As a
combination of all these factors, we propose the following segment control method. A default value
for the parameter ` is the length of seasonality, i.e., ` = S.
1
2
3
Algorithm 2: Segment control on change points
Input: change point binary vector zc,trend μ, standard deviation for outliers σrr, change point
minimum segment `
Output: change point binary vector zc
Denote t1 < t2 < . . . to be all the indexes such that ztc = 1;
while there exists i such that |ti+1 - ti | < ` do
Check if ∣μti-ι - μti+1+11 ≤ σrr/2. If so, exclude both them from change points by setting
ztc = ztc	= 0. Otherwise, randomly exclude one of them by setting the corresponding
coordinate in zc to be 0;
Update all the indexes of change points in zc .
end
4.3	Initialization and Updates on Parameters
The parameters σ , a1 and p need both initialization and update. We have different initializations
and update schemes for each of them.
For all the standard deviations, once we obtain α and z, we update them by taking the empirical
standard deviation correspondingly. For σδ and σγ , the calculation is straightforward as they only
involve δ and γ respectively. For σ , σo , σu and σr , it is a bit more involved due to z . Nevertheless,
we can obtain the following update equations for all of them:
σ =
t
X
{t:zta =0}
(yt - μt - γt)2
∣{t: Za = 0}∣
σo =
t
X
{t:zta=1}
(yt - μt - Yt)2
∣{t : Za = i}∣
σu =
t
X^	(μt - μt-ι - δt-ι)2
{t：ZC=o}	Rt: Zc = 0}1
σr
∖X	(μt - μt-ι - δt-ι)2
t {t⅛=1}	∣{t : Zc = 0}∣
u n	u n S-1
∖ - X(δt-δt-1)2,σγ = ʌ 1 X( X Yt-s)2.
n t=1	n t=1 s=0
(7)
(8)
Note that in some iterations, when there is no change point or anomaly detected in z, then the
updates above for σo , σr are not well-defined. In those cases, we simply let them remain the same.
To initialize σ, we let them all equal to the standard deviation of y.
For a1, we initialize it by letting its first coordinate to be equal to the average of y-, y2, . . . , yS, and
all the remaining coordinates to be equal to 0. Since a1 can be interpreted as the mean vector of α-,
in this way the trend is initialized to be matched up with average of the first season, and the slope
and seasonality are initialized to be equal to 0. We update a- by using information of α. We let the
first two coordinates (trend and slope) of a1 to be equal to those of α1, and we let the remaining
coordinates (seasonality) ofa1 to be equal to those of αS+1. The reason why we do not let a1 to be
equal to α1 entirely is due to the consideration on convergence and robustness. Since we initialize
the seasonality part in a1 as 0, it will remain 0 ifwe let a1 equals α1 entirely (due to the mechanism
how we update α1 as described in Section 4.1. We can avoid such trouble via using αS+1.
For p, we initialize them to be equal to 1/n. If we have additional information on the number of
change points or anomaly points, we can initiate them with different values, for example, 0.1/n, or
10/n. We can update p after obtaining z, but we choose not to, also for the sake of robustness. In
the early iterations when the algorithm is far from convergence, it is highly possible that za or zc
7
Under review as a conference paper at ICLR 2018
may turn out to be all 0. If we update p, say, by taking the proportion of change point or anomaly
points in z. Then pa or pc might be 0, and it may get stuck in 0 in the remaining iterations.
5	Forecasting
Once we infer all the latent variables α, z and tune all the parameters p, a1, σ, we are able to
forecast the future time series yfuture. From the graphical model described in Section 3, the future
forecasting only involves αn instead of the whole α. Note that we assume that there exists no change
point and anomaly point in the future. This is reasonable as in most cases we have no additional
information on the future time series. Given αn and σ we can use our predictive procedures (i.e.,
Algorithm 1) to generate future time series yfuture. We can further integrate out αn to have the
posterior predictive distribution as pσ (yfuture|y).
The forecasting on future time series is not deterministic. There are two sources for the randomness
in yfuture. One comes from the inference of αn (and also σ) from y. Under the Bayesian framework
in Section 3, we have a posterior distribution over αn rather than a single point estimation. The
second one comes from the forecasting function itself. The forecasting involves intrinsic noise like
t, ut, vt and wt. Thus, the predictive density function pσ (yfuture|y, αn) will lead to different path
even with fixed σ and αn . In this way we are able to obtain distribution and predictive interval for
forecasting. We also suggest to take the average of multiple forecasting paths, as the posterior mean
for the forecasting.
The average of multiple forecasting paths (denoted as yfuture), if the number of paths is large enough,
always takes the form as a combination of linear trend and seasonality. This can be observed in both
our synthesis data (Section 7) and real data analysis (Section 8). This seems to be surprising at the
first glance, but makes some sense intuitively. Under our assumption, we have no information on
the future, and thus a reliable way to forecast the future is to use the information collected at the
end of observed time series, i.e., trend μn, slope δn and seasonality structure. Theorem 1 gives
mathematical explanation of the linearity of yfuture, in both mean and standard deviation.
Theorem 1. Let N be the number of future time series paths we generate from Algorithm 1). Let m
be the number of points we are going to forecast. Denote {yn(1+) j}jm=1, {yn(2+) j}jm=1, . . . , {yn(N+)j}jm=1
to be thefuturepaths. Define 勺取取定=(yjn+1,yjn+2,..., yjn+m) to be the average such that
1N
yn+j = Nfyn+j.
i=1
Thenfor all j = 1,2,..., N, we have yjn+j as a normal distribution with mean and variance as
E[yn+j ] = μn + jδn + Yn-S+(j mod S)
Var [yn+j] = N (j (j + I)σV∕2 + j (σU + σW ) + σ2).
Consequently, for all j = 1, 2,..., m, E[yn+j] is in a Iinearform with respect to j, and the Standard
deviation of yn+j also takes a approximately linearform with respect to j.
Proof. Recall that αn , σ are given and fixed, and we assume there is no change point or anomaly
in the future time series. The Equation (2) leads to δn+j = δn + Plj=1 vn+l, which implies that
jj
μn+j = μn + jδn + ɪ2(j + 1 - Dvn+l + Σ um+l .
For the seasonality part, simple linear algebra together with Equation 3 leads to γn+j =
γn-S+(j mod S) +	lj=1 wn+l . Thus,
1N	j	j	j
yn+j = nΣ2	μn	+ jδn	+ γn-S+(j	mod	S)	+ Eej	+ 1 - Dv+1	+ E Um∖l	+ E w(n+1 +	^+j
i=1	l=1	l=1	l=1
Due to the independence and Gaussian distribution of all the noises, yn+j is also normally distributed
and its means and variance can be calculated accordingly.	□
8
Under review as a conference paper at ICLR 2018
6 Algorithm
Our proposed method can be divided into three parts: initialization, inference, and forecasting.
Section 4 and Section 5 provide detailed explanation and reasoning for each of them. We present a
whole picture of our proposed methodology in Algorithm 3.
Algorithm 3: Proposed Algorithm
Input: Observed time series y = (y1, y2, . . . , yn), seasonality length S, length of time series for
forecasting m, number of predictive paths N , change point minimum segment l
Output: Change point detection zc, anomaly points za, forecasting result
yfuture = (yn+1,yn+1, . . . ,yn+m) and its distribution or predictive intervals
1
2
3
4
5
6
7
8
9
10
11
Part I:	Initialization;
Initialize σ, σo, σu, σr, σv, σw all with the empirical standard deviation of y;
Initialize a1 such that its first coordinate equals to the average of (y1, y2, . . . , yS) and all the
remaining S coordinates with 0;
Initialize pa and pc by 1/n. Then generate za and zc as independent Bernoulli random variables
with success probability pa and pc respectively;
Part II:	Inference;
while the likelihood function La1,p,σ (y, α, z) not converges do
Infer α by Kalman filter, Kalman smoothing and “fake-path” trick described in Section 4.1;
Update za and zc by sampling from
{za}n=ι ~ Ber(pa),	{zC}n=ι ~ Ber(pc),
where the success probability {pta}tn=1 and {ptc}tn=1 are defined in Equation (5) and (6);
Segment control on zc by Algorithm 2;
Update σ by Equation (7) to (8);
Update a1 such that its first two coordinates equal to the those of α1 and the remaining
(S - 1) coordinates equals to those of αS+1;
Calculate the likelihood function La1 ,p,σ (y, α, z) given in Equation (4);
end
Part III:	Forecasting;
With an and σ , use the generate procedure in Algorithm 1 to generate future time series yfuture
with length m. Repeat the generative procedure to obtain multiple future paths
(1)	(2)	(N);
yfuture , yfuture , . . . , yfuture;
Combine all the predictive paths give the distribution for the future time series forecasting. If
needed, calculate the point-wise quantile to obtain predictive intervals. Use the point-wise average
as our final forecasting result.
It is worth mentioning that our proposed methodology is downward compatible with many simpler
state space time series models. By letting pc = 0, we assume there is no change point in the time
series. By letting pa = 0, we assume there is no anomaly point in the time series. If both pc and
pa are set to be 0, then our model is reduced to the classic state space time series model. Also, the
seasonality and slope can be removed from our model, if we know there exists no such structure in
the data.
7	Simulation
In this section, we study the synthetic data generated from our model. We let S = 7 and provide
values for σ and a1 . The change points and anomaly points are randomly generated. We use our
generative procedure (Algorithm 1) to generate time series with total length 500 by fixed parameters.
The first 350 points will be used as training set and the remaining 150 points will be used to evaluate
the performance of forecasting.
9
Under review as a conference paper at ICLR 2018
When generating, we let the time series have weekly seasonality with S = 7. For σ we have
σe = 0.1, σu = 0.1, σv = 0.0004, σw = 0.01, σr = 1,σ0 = 4. For aι We have value for μ as 20,
value for δ as 0, and value for seasonality as (1, 2, 4, -1, -3, -2)/10. For p we have pc = 4/350
and pa = 10/350. Despite that, to make sure that at least one change point is in existence, We
force z3c30 = 1 and r330 = 2. That is, for each time series We generate, its 330th point is a
change point With the mean shifted up by 3. Also to be consistence With our assumption, We force
zic = zia = 0, ∀351 ≤ i ≤ 500 so there exists no change point or anomaly point in the testing part.
The top panel of Figure 3 shoWs one example of synthesis data. The blue line marks the separation
betWeen training and testing set. The blue dashed line indicates the locations for the change point,
While the yelloW dots indicate the positions of anomaly points. Also see Figure 3 for illustration on
the results returned by implementing our proposed algorithm on the same dataset. The red line gives
the fitting results in the first 350 points and forecasting results in the last 150 points. The change
points detected are marked With vertical red dotted line, and the anomaly detected are flagged With
purple squares. Figure 3 shoWs that on this dataset, our proposed algorithm yields perfect detection
on both change points and anomaly points. In Figure 3, the gray part indicates the 90% predictive
interval for forecasting.
Figure 3: An example of synthesis data (left), and the result after applying our algorithm (right).
We run our generative model 100 times to produce 100 different time series, and implement mul-
tiply methods on each of them, and aggregate the results together for comparison. We include the
folloWing methodologies. For time series forecasting, We compare our method against Bayesian
Structural Time Series (BSTS) (Scott & Varian, 2014; Brodersen et al., 2015)), Seasonal Decom-
position of Time Series by Loess (STL) (Cleveland et al., 1990)), Seasonal ARIMA (Box et al.,
2015), Holt-Winters (Holt, 2004), Exponential Smoothing State Space Model (ETS)(Hyndman
et al., 2008)), and the Prophet R package by Taylor & Letham (2017). We evaluate the perfor-
mances by mean absolute percentage error (MAPE), mean square error (MSE) and mean absolute
error (MAE) on forecasting set. The mathematical definition of these three criterion is given as fol-
lows. Let χ1,χ2,...,χn be the true value and X1,X2,... ,Xn be the estimation or predictive values.
Then We have
MAPE
1n
n χ
n
i=1
|xi - Xi|
xi
MSE =
t
1n	1n
—X(Xi - Xi)2,MAE = - X ∣Xi - Xi|.
nn
i=1	i=1
The comparison of our proposed algorithm and the aforementioned algorithms are included below
in Table 2. As we mentioned in Section 6, our algorithm is downward compatible with the cases
ignoring the existence of change point or anomaly, by setting pc = 0 or pa = 0. We also run
proposed algorithm on the synthetic data with pc = 0 (no change point), or pa = 0 (no anomaly
point), orpc = pa = 0 (no change and anomaly point), for the purpose of numeric comparison.
From Table 2 it turns out that our proposed algorithm achieves the best performance compared to
other existing methods. Our proposed algorithm also performs better compared with the cases ignor-
ing change point or anomaly point. This is a convincing evidence on the importance of incorporating
both change point structure and anomaly point structure when modeling, for time series forecasting.
We also compare our proposed method with other existing change point detection methods and
anomaly detection algorithm with respect to the performance of detections. We evaluate the per-
formance by two criterions: True Positive Rate (TPR) and False Positive (FP). TPR measures the
percentage of change points or anomalies to be correctly detected. FP count the number of points
wrongly detected as change points or anomaly points. The mathematical definitions of TPR and FP
are as follows. Let (z1, z2, . . . , zn) be the true binary vector for change points or anomalies, and
10
Under review as a conference paper at ICLR 2018
Table 2: Comparison of methodologies on Forecasting
Methods	MAPE	MSE	MAE
Proposed	0.041 ± 0.027	1.03 ± 0.59	0.89 ± 0.53
Proposed (pa = 0)	0.069 ± 0.068	1.71 ± 1.61	1.49 ± 1.44
Proposed (pc = 0)	0.065 ± 0.058	1.67 ± 1.53	1.43 ± 1.35
Proposed (pa = 0, pc = 0)	0.084 ± 0.079	2.15 ± 2.00	1.87 ± 1.77
BSTS	0.162 ± 0.110	4.10 ± 2.81	3.59 ± 2.48
STL	0.047 ± 0.039	1.18 ± 1.06	1.03 ± 0.95
ARIMA	0.076 ± 0.050	1.88 ± 1.38	1.71 ± 1.24
Holt-Winters	0.093 ± 0.082	2.35 ± 2.06	2.05 ± 1.84
ETS	0.054 ± 0.042	1.37 ± 1.05	1.19 ± 0.94
Prophet	0.082 ± 0.055	2.06 ± 1.33	1.78 ± 1.16
(Z1,Z2,...,^n) are the estimated ones. Then
TPR =也；zi = 1∖ =1}|, FP = |{i ： Zi=0,^i = i}∣.
|{i : zi = 1}|
From the definition, we can see high TPR and low FP means the algorithm has better performance
in detection.
The comparison on change point detection is shown in Table 3. We compare our results against
three popular change point detection methods: Bayesian Change Point (BCP) (Barry & Hartigan,
1993), Change-Point (CP) (Killick & Eckley, 2014) and Breakout (twitter, 2017). From Table 3 our
proposed method outperforms the most of the others by both TPR and FP. We have smaller TPR
compared to CP, but we are better in FP.
Table 3: Comparison of Change Point Detection
Methods	TPR	FP
Proposed	0.41 ± 0.26	0.34 ± 0.57
Proposed (pa = 0)	0.14 ± 0.21	0.26 ± 0.60
BCP	0.58 ± 0.22	29.84 ± 8.13
CP	0.29 ± 0.22	1.71 ± 1.15
Breakout	0.01 ± 0.04	0.53 ± 0.86
Table 4: Comparison of Anomaly Detection
Methods	TPR	FP
Proposed	0.88 ± 0.12	0.58 ± 0.96
Proposed (pc = 0)	0.87 ± 0.12	2.56 ± 1.49
AnomalyDetection	0.32 ± 0.19	1.03 ± 1.94
RAD	0.88 ± 0.11	19.33 ± 3.58
tsoutlier	0.81 ± 0.14	4.76 ± 4.29
In Table 4, we also compare the performance of our algorithm on anomaly detection with three
existing common anomaly detection methods: the AnomalyDetection package by Twitter (2017),
RAD by Netflix (2017) and Tsoutlier by Chen & Liu (1993). The comparison is listed in Table 4.
We can see our method also outperforms most of the others with respect to anomaly detection, by
both TPR and FP. RAD has slightly better TPR but its FP is much worse compared with ours.
8	Real Data Analysis
In this section, we implement our proposed method on real-world datasets. We also compare its
performance against other existing time series forecasting methodologies. We consider two datasets,
one is a public data called Well-log dataset, and the other is an unpublished internet traffic dataset.
The bottom panels of Figure 4 and Figure 5 give the result of our proposed algorithms. The blue line
separates the training set and testing set. We use red line to show our fitting and forecasting result,
vertical red dashed line to indicate change points and purple dots to indicate anomaly points. The
gray part shows 90% predication interval.
8.1	Well-log Data
This dataset (Fearnhead & Clifford, 2003; JK & WJ, 1996) was collected when drilling a well. It
measures the nuclear magnetic response, which provides geophysical information to analyze the
structure of rock surrounding the well. This dataset is public and available online 2. It has 4050
2http://hips.seas.harvard.edu/files/well-log-2.dat
11
Under review as a conference paper at ICLR 2018
points in total. We split it such that the first 3000 points are used as training set and last 1000 points
are used to evaluate the forecasting performance.
Figure 4: Well-log Data (left). The result of implementing our proposed algorithm (right).
From Figure 4, it is obvious that there exists no seasonality or slope structure in the dataset. This
motivates us not to include these two components in our model. We implement our proposed algo-
rithm without seasonality and slope, and compare the forecasting performance with other methods
in Table 5. Our method outperforms BSTS, ARIMA, ETS and Prophet. However in Table 5 the per-
formance can be slightly improved if we ignore the existence of anomaly points by letting pa = 0.
This may be caused by model mis-specification as the data may not generated in a way not entirely
captured by our model. Nevertheless, the performances of our method considering anomaly points
or not, are comparable to each other.
Table 5: Comparison of Forecasting in Well-log Data
Methods	MAPE	MSE	MAE
Proposed	0.031	5296	3120
Proposed (pa = 0)	0.029	5252	2957
Proposed (pc = 0)	0.033	5434	3409
Proposed (Pa = 0,pc = 0)	0.038	5703	3908
BSTS	0.250	32030	27210
ARIMA	0.084	10480	8738
ETS	0.037	6071	3860
Prophet	0.159	19530	17480
In this dataset there is no ground-truth of change point and anomaly point on their locations or even
existence. However, from bottom panel of Figure 4, there are some obvious changes in the sequence
and they all successfully captured by our algorithm.
8.2	Internet Traffic Data
Our second real data is an Internet traffic data acquired from a major Tech company (see Figure 5).
It is a daily traffic data, with seasonality S = 7. We use the first 800 observations as training set and
evaluate the performance of forecasting on the remaining 265 points. The bottom panel of Figure 5
show the result from implementing our algorithm.
Figure 5: Internet Traffic Data (top); The result of implementing our proposed algorithm (bottom).
We also do the comparison of forecasting performance of our proposed algorithm together with
other existing methods, shown in Table 6. We can also see that our algorithm outperforms all the
other algorithms with respect to MAPE, MSE and MAE.
12
Under review as a conference paper at ICLR 2018
Table 6: Comparison of Forecasting in Internet traffic data
Methods	MAPE	MSE	MAE
Proposed	0.0837	0.1216	0.08414
Proposed (Pa = 0)	0.0838	0.1215	0.08320
Proposed (Pc = 0)	0.0934	0.1332	0.09296
Proposed (Pa =0,pc = 0)	0.0934	0.1366	0.09223
BSTS	0.2756	0.3087	0.27960
STL	0.1014	0.1258	0.09910
ARIMA	0.1409	0.1653	0.12580
Holt-Winters	0.2495	0.2739	0.25270
ETS	0.0893	0.1199	0.09362
Prophet	0.1015	0.1405	0.11450
From Figure 5 our proposed algorithm identifies one change point (the 576th point, indicated by
the vertical red dashed line), which can be confirmed that this is exactly the only one change point
existing in this time series caused by the change of counting methods, by some external information.
Thus, we give the perfect change point detection in this Internet traffic data.
For this Internet traffic dataset, since we have ground-truth for change point, we can compare the
performance of change point detection of different methodologies. BCP returns posterior distribu-
tion, which peaks in the the 576th point with posterior probability value 0.5. And it also returns
with many other points with posterior probability value around 0.1. CP returns 4 change points,
where the 576th point (the only true one) is one of them. Breakout returns 8 change points without
including the 576th point. To sum up, our proposed method achieves the best change point detection
in this real dataset.
9	Related Work
Parametric models are widely considered in econometric literature for time series forecasting,
e.g. Jalles (2009), Commandeur et al. (2011), Gould et al. (2008), Harvey & Peters (1990), Harvey
et al. (1998). The general procedure of decomposition method (using trend, seasonal and irregular
components) for univariate structural time series modeling is discussed in Harvey & Peters (1990);
a unified state space framework is proposed to handle any messy time series in Harvey et al. (1998);
and the explicit modeling of both additive and multiplicative seasonalities in Gould et al. (2008);
Jalles (2009). Although Kalman filter and MCMC-based approaches are used to sample posterior to
estimate hidden components, the changing points and anomalies are not considered and processed
in the above works. For example, the irregular component considered in Jalles (2009) is simply
the noises. Commandeur et al. (2011) discusses the statistical software for state-space modeling
which is designed for generic time series analytic and modeling, which cannot directly be used
when changing point and anomalies are in existence. Our proposed approach shares similarity with
the aforementioned papers as we have similar additive structure of components. However we are
able to incorporate the change points and anomalies, two common structure widely observed in real
data, into our model by using Bernoulli indicators. This is non-trivial, and cannot be handled by the
aforementioned papers or their variants.
Non-parametric approaches are used for extraction of components from quasi-periodic time-series,
e.g., the ensembles of weak detectors using non-parametric measurement are used in Artemov &
Burnaev (2016) to detect change-points and anomalies, and the online decomposition algorithm
based on per-component is adopted for change-point detection in Alexey Artemov (2015). Different
from the above works, to handle the structural brakes and change-points, this paper presents the
parametric approach for modeling anomalies and changing points by fitting them in the state-space
framework using approximate inference for forecasting path prediction.
Different Bayesian approaches are proposed for change-point detection, e.g., Adams & MacKay
(2007) performs Bayesian change point detection from online inference by generating the distribu-
tion estimation of the next unseen datum in the sequence given only data already observed, and the
Bayesian Online CPD (BOCPD) algorithm proposed by Turner et al. (2009) performs online pre-
diction using hidden variable given the underlying predictive model (UPM) and the hazard function.
13
Under review as a conference paper at ICLR 2018
Compared to the aforementioned models, our work differs in Bayesian modeling which samples
posterior to estimate hidden components given the independent Bernoulli priors of changing point
and anomalies.
10	Conclusion
We incorporate the change point structure and anomaly point structure into the classic space state
time series model. We provide a Bayesian scheme for inference and time series forecasting. We
compare the performance of our methodology and state-of-the-art methods on both synthetic data
and real datasets. Our method performs the best with respect to forecasting, change point detection,
and anomaly detection as well.
References
Ryan P. Adams and David J.C. MacKay. Bayesian online changepoint detection. Cambridge, UK,
2007.
Andrey Lokot Alexey Artemov, Evgeny Burnaev. Nonparametric decomposition of quasi-periodic
time series for change-point detection, 2015.
Alexey Artemov and Evgeny Burnaev. Detecting performance degradation of software-intensive
systems in the presence of trends and long-range dependence. In IEEE International Conference
on Data Mining Workshops, ICDM Workshops 2016, December 12-15, 2016, Barcelona, Spain.,
pp. 29-36, 2016.
Daniel Barry and John A Hartigan. A bayesian analysis for change point problems. Journal of the
American Statistical Association, 88(421):309-319, 1993.
George EP Box, Gwilym M Jenkins, Gregory C Reinsel, and Greta M Ljung. Time series analysis:
forecasting and control. John Wiley & Sons, 2015.
Kay H Brodersen, Fabian Gallusser, Jim Koehler, Nicolas Remy, Steven L Scott, et al. Inferring
causal impact using bayesian structural time-series models. The Annals of Applied Statistics, 9
(1):247-274, 2015.
Chung Chen and Lon-Mu Liu. Joint estimation of model parameters and outlier effects in time
series. Journal of the American Statistical Association, 88(421):284-297, 1993.
Robert B Cleveland, William S Cleveland, and Irma Terpenning. Stl: A seasonal-trend decomposi-
tion procedure based on loess. Journal of Official Statistics, 6(1):3, 1990.
John H Cochrane. Time series for macroeconomics and finance. 2005.
Jacques Commandeur, Siem Koopman, and Marius Ooms. Statistical software for state space meth-
ods. Journal of Statistical Software, Articles, 41(1):1-18, 2011. ISSN 1548-7660.
James Durbin and Siem Jan Koopman. Time series analysis by state space methods, volume 38.
OUP Oxford, 2012.
Paul Fearnhead and Peter Clifford. On-line inference for hidden markov models via particle filters.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 65(4):887-899, 2003.
Phillip G. Gould, Anne B. Koehler, J. Keith Ord, Ralph D. Snyder, Rob J. Hyndman, and Farshid
Vahid-Araghi. Forecasting time series with multiple seasonal patterns. European Journal of
Operational Research, 191:207-222, 2008.
Katalin Hangos, Jzsef Bokor, and G Szederknyi. Analysis and Control of Nonlinear Process Systems.
Advanced Textbooks in Control and Signal Processing. Springer-Verlag London, 2014. doi: 10.
1007/b97665.
A. C. Harvey and S. Peters. Estimation procedures for structural time series models. Journal of
Forecasting, 9(2):89-108, 1990. ISSN 1099-131X.
14
Under review as a conference paper at ICLR 2018
Andrew Harvey, Siem Jan Koopman, and J Penzer. Messy time series: A unified approach. pp.
103-143,13 1998.
Keith W Hipel and A Ian McLeod. Time series modelling of water resources and environmental
systems, volume 45. Elsevier, 1994.
Charles C Holt. Forecasting seasonals and trends by exponentially weighted moving averages.
International journal of forecasting, 20(1):5-10, 2004.
Rob Hyndman, Anne B Koehler, J Keith Ord, and Ralph D Snyder. Forecasting with exponential
smoothing: the state space approach. Springer Science & Business Media, 2008.
Joao Jalles. Structural time series models and the kalman filter: a concise review. Feunl working
paper series, Universidade Nova de Lisboa, Faculdade de Economia, 2009.
OR JK and F WJ. Numerical bayesian methods applied to signal processing, 1996.
Rebecca Killick and Idris Eckley. changepoint: An r package for changepoint analysis. Journal of
Statistical Software, 58(3):1-19, 2014.
Netflix. Rad: Time series anomaly detection. 2017.
Takayuki Osogami, Hiroshi Kajino, and Taro Sekiyama. Bidirectional learning for time-series
models with hidden units. In Proceedings of the 34th International Conference on Machine
Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pp. 2711-2720, 2017. URL
http://proceedings.mlr.press/v70/osogami17a.html.
Steven L Scott and Hal R Varian. Predicting the present with bayesian structural time series. Inter-
national Journal of Mathematical Modelling and Numerical Optimisation, 5(1-2):4-23, 2014.
Souhaib Ben Taieb, James W. Taylor, and Rob J. Hyndman. Coherent probabilistic forecasts
for hierarchical time series. In Proceedings of the 34th International Conference on Machine
Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pp. 3348-3357, 2017. URL
http://proceedings.mlr.press/v70/taieb17a.html.
Karl Aberer Tao Lin, Tian Guo. Hybrid neural networks for learning the trend in time series. In Pro-
ceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17,
pp. 2273-2279, 2017. doi: 10.24963/ijcai.2017/316. URL https://doi.org/10.24963/
ijcai.2017/316.
S. J. Taylor and Letham. Prophet: forecasting at scale. 2017.
Ryan Turner, Yunus Saatci, and Carl Edward Rasmussen. Adaptive sequential Bayesian change
point detection. In Advances in Neural Information Processing Systems (NIPS): Temporal Seg-
mentation Workshop, 2009.
Twitter. Anomalydetection: Anomaly detection with r. 2017.
twitter. Breakout detection via robust e-statistics. 2017.
Peter R Winters. Forecasting sales by exponentially weighted moving averages. Management sci-
ence, 6(3):324-342, 1960.
G Peter Zhang. Time series forecasting using a hybrid arima and neural network model. Neurocom-
puting, 50:159-175, 2003.
15