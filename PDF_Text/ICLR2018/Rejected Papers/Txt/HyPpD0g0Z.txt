Under review as a conference paper at ICLR 2018
GROUPING-BY-ID: GUARDING AGAINST ADVERSAR-
ial Domain Shifts
Anonymous authors
Paper under double-blind review
Ab stract
When training a deep neural network for supervised image classification, one can
broadly distinguish between two types of latent features of images that will drive
the classification of class Y . Following the notation of Gong et al. (2016), we can
divide features broadly into the classes of (i) ‘core’ or ‘conditionally invariant’
features Xci whose distribution P(Xci|Y ) does not change substantially across
domains and (ii) ‘style’ or ‘orthogonal’ features X⊥ whose distribution P(X⊥ |Y )
can change substantially across domains. These latter orthogonal features would
generally include features such as position, rotation, image quality or brightness
but also more complex ones like hair color or posture for images of persons. We
try to guard against future adversarial domain shifts by ideally just using the ‘con-
ditionally invariant’ features for classification. In contrast to previous work, we
assume that the domain itself is not observed and hence a latent variable. We
can hence not directly see the distributional change of features across different
domains.
We do assume, however, that we can sometimes observe a so-called identifier or
ID variable. We might know, for example, that two images show the same person,
with ID referring to the identity of the person. In data augmentation, we generate
several images from the same original image, with ID referring to the relevant
original image. The method requires only a small fraction of images to have an
ID variable.
We provide a causal framework for the problem by adding the ID variable to the
model of Gong et al. (2016). However, we are interested in settings where we
cannot observe the domain directly and we treat domain as a latent variable. If
two or more samples share the same class and identifier, (Y, ID) = (y, id), then
we treat those samples as counterfactuals under different style interventions on the
orthogonal or style features. Using this grouping-by-ID approach, we regularize
the network to provide near constant output across samples that share the same
ID by penalizing with an appropriate graph Laplacian. This is shown to substan-
tially improve performance in settings where domains change in terms of image
quality, brightness, color changes, and more complex changes such as changes in
movement and posture. We show links to questions of interpretability, fairness
and transfer learning.
1	Introduction
Deep neural networks (DNNs) have achieved outstanding performance on prediction tasks like vi-
sual object and speech recognition (Krizhevsky et al., 2012; Szegedy et al., 2015; He et al., 2015).
Issues can arise when the learned representations rely on dependencies that vanish in test distribu-
tions (e.g. see Csurka (2017) and references therein). Such domain shifts can be caused by changing
conditions, e.g. color, background or location changes arising when deploying the machine learn-
ing (ML) system in production. Predictive performance is then likely to degrade. For instance, the
“Russian tank legend” is an example where the training data was subject to sampling biases that
were not replicated in the real world. Concretely, the story relates how a machine learning system
was trained to distinguish between Russian and American tanks from photos. The accuracy was very
high but only due to the fact that all images of Russian tanks were of bad quality while the photos of
1
Under review as a conference paper at ICLR 2018
American tanks were not. The system learned to discriminate between images of different qualities
but would have failed badly in practice (Emspak, 2016)1.
Hidden confounding factors like in the example above between image quality and the origin of the
tank give rise to indirect associations. These are arguably one reason why deep learning requires
large sample sizes as large sample sizes tend to ensure that the effect of the confounding factors
averages out (although a large sample size is clearly not per se a guarantee that the confounding
effect will become weaker). A large sample size is also required if one is trying to achieve invariance
to known factors like translation, point of view, and rotation by using data augmentation. Another
related example where human and artificial cognition deviate strongly are adversarial examples—
imperceptibly but intentionally perturbed inputs that are misclassified by a ML model (Szegedy
et al., 2014; Goodfellow et al., 2015). Adversarial examples do not fool humans and in general we
only need to see one rotated example of the same object to achieve invariance to rotations in our
perception. Our starting point is the question whether we can in a simple way mimic the human
ability to learn desired invariances from a few instances of the same object and whether we can
better align the features DNNs exploit with human cognition.
Considerations of fairness and discrimination might be another reason why we are interested in
controlling that certain characteristics of the input data are not included in the learned representations
and thus have no impact on the resulting decisions (Barocas & Selbst, 2016; Kilbertus et al., 2017).
Unfortunately, existing biases in datasets used for training ML algorithms tend to be replicated in
the estimated models (Bolukbasi et al., 2016). For instance, in June 2015 Google’s photo app tagged
two non-white people as “gorillas”—most likely because the training examples for “people” were
mainly photos of white persons, making “color” predictive for the class label (Crawford, 2016;
Emspak, 2016). A human would not make the same mistake after only seeing one instance of a
non-white person.
Addressing the issues outlined above, we propose counterfactual regularization (CoRe) to control
what latent features an estimator extracts from the input data. Conceptually, we take a causal view
of the data generating process and categorize the latent data generating factors into ‘conditionally
invariant’ (core) and ‘orthogonal’ (style) features, as in (Gong et al., 2016). It is desirable that a
classifier uses only the core features as they pertain to the target of interest in a stable and coherent
fashion. CoRe yields an estimator which is invariant to factors of variation corresponding to style
features. Consequently, it is robust with respect to adversarial domain shifts, arising through arbi-
trarily strong interventions on the style features. CoRe relies on the fact that for certain datasets
we can observe “counterfactuals” in the sense that we observe the same object under different con-
ditions. Rather than pooling over all examples, CoRe exploits knowledge about this grouping, i.e.
that a number of instances relate to the same object.
The remainder of this manuscript is structured as follows: §2 starts with two motivating examples,
showing how CoRe can reduce the need for data augmentation and help predictive performance in
small sample size settings. In §3 we review related work and in §4 we formally introduce counterfac-
tual regularization, along with the CoRe estimator and theoretical insights for the logistic regression
setting. In §5 we further evaluate the performance of CORE in a variety of experiments.
2	Two motivating examples
2.1	Grouping photos of the same person: better predictive performance
The CelebA dataset (Liu et al., 2015) contains face images of celebrities. We consider the task of
classifying whether a person wears glasses. Several photos of the same person are available. We use
this grouping information and constrain the classification to yield the same prediction for all images
belonging to the same person and sharing the same class label. We call the additional instances
of the same person counterfactual (CF) observations. Figure 1a shows examples from the training
set. The standard approach would be to pool all examples. The only additional information we
exploit is that some observations can be grouped. We include n = 10 identities in the training set,
resulting in a total sample size m = 321 as there are approximately 30 images of each person2.
1A different version of this story can be found in Yudkowsky (2008).
2Additional results for n ranging from 10 to 160 can be found in Figure C.7b.
2
Under review as a conference paper at ICLR 2018
(a) Grouping-by-ID with ID=identity.
Figure 1: Examples from a) the subsampled CelebA dataset and b) the augmented MNIST dataset. Connected
images are counterfactual examples as they share the same realization of the ID which is the identity of the
person in a) and the original image used for data augmentation in b). The comparison is a training of exactly
the same network architecture that does not make use of the grouping information but using a standard ridge
penalty. In a) exploiting the grouping information reduces the test error by 32% compared to pooling over all
samples. In b) the test error on rotated digits is reduced by 50%.
(b) Grouping-by-ID with ID=original image.
Exploiting the group structure reduces the average test error from 24.76% to 16.89%, i.e. by approx.
32%, compared to the estimator which just pools all images and uses a standard ridge penalty for
the cofficients3.
2.2	Grouping augmented images by original: more sample efficient
A different use case of CoRe is to make data augmentation more efficient in terms of the required
samples. In data augmentation, one creates additional samples by modifying the original inputs, e.g.
by rotating, translating, or flipping the images (Scholkopf et al., 1996). In other words, additional
samples are generated by interventions on style features. Using this augmented data set for training
results in invariance of the estimator with respect to the transformations (style features) of interest.
For CoRe we can use the grouping information that the original and the augmented samples belong
to the same object. This enforces the invariance with respect to the style features more strongly
compared to normal data augmentation which just pools all samples. We assess this for the style
feature “rotation” on MNIST (LeCun & Cortes, 2010) and only include c = 100 augmented training
examples for n = 10000 original samples, resulting in a total sample size of m = 10100. The
degree of the rotations is sampled uniformly at random from [35, 70]. Figure 1b shows examples
from the training set. By using CoRe the average test error on rotated examples is reduced from
32.86% to 16.33%, around half its original value4.
3	Related work
Perhaps most similar to this work in terms of their goals are the work of Gong et al. (2016) and
Domain-Adversarial Neural Networks (DANN) proposed in Ganin et al. (2016), an approach moti-
vated by the work of Ben-David et al. (2007). While our approach requires grouped observations,
both of these works rely on unlabeled data from the target task being available.
The main idea of Ganin et al. (2016) is to learn a representation that contains no discriminative
information about the origin of the input (source or target domain). This is achieved by an adver-
sarial training procedure: the loss on domain classification is maximized while the loss of the target
prediction task is minimized simultaneously. In contrast, we do not assume that we have data from
different domains but just different realizations of the same object under different interventions.
The data generating process assumed in Gong et al. (2016) is similar to our model, introduced
in §4.2 where we detail the similarities and differences between the models (cf. Figure 2). Gong
et al. (2016) identify the conditionally independent features by adjusting a transformation of the
3Details on the architecture can be found in Table C.1. Using ImageNet pre-trained features from Inception
V3 does not yield lower error rates.
4Additional results for n ∈ {1000, 10000} and c ranging from 100 to 5000 can be found in Figure C.8.
3
Under review as a conference paper at ICLR 2018
variables to minimize the squared MMD distance between distributions in different domains5. The
fundamental difference to our approach is that we use a different data basis. The domain identifier is
explicitly observable in Gong et al. (2016), while it is latent in our approach. In contrast, we exploit
presence of an identifier variable ID to penalize the classifier using any latent features outside the
set of conditionally independent features.
Causal modeling has related aims to the setting of transfer learning and guarding against adversarial
domain shifts. Specifically, causal models have the defining advantage that the predictions will be
valid even under arbitrarily large interventions on all predictor variables (Haavelmo, 1944; Aldrich,
1989; Pearl, 2009; ScholkoPfet al., 2012; Peters et al., 2016; Zhang et al., 2013; 2015; X. Yu, 2017;
M. Rojas-Carulla, 2017; Magliacane et al., 2017). There are two difficulties in transferring these
results to the setting of adversarial domain changes in image classification. The first hurdle is that
the classification task is tyPically anti-causal since the image we use as a Predictor is a descendant
of the true class of the object we are interested in rather than the other way around. The second
challenge is that we do not want to guard against arbitrary interventions on any or all variables but
only would like to guard against a shift of the style features. It is hence not immediately obvious
how standard causal inference can be used to guard against large domain shifts.
Recently, various aPProaches have been ProPosed that leverage causal motivations for deeP learning
or use deeP learning for causal inference. In all of the following methods, the goals and the settings
are different from ours. SPecifically, the setting of anti-causal Prediction and non-ancestral interven-
tions on style variables is not considered. Various aPProaches focus on cause-effect inference where
the goal is to find the causal relation between two random variables, X and Y (LoPez-Paz et al.,
2017; LoPez-Paz & Oquab, 2017; Goudet et al., 2017). LoPez-Paz et al. (2017) ProPose the Neural
Causation Coefficient (NCC) to estimate the Probability of X causing Y and aPPly it to finding the
causal relations between image features. SPecifically, the NCC is used to distinguish between fea-
tures of objects and features of the objects’ contexts. LoPez-Paz & Oquab (2017) note the similarity
between structural equation modeling and CGANs (Mirza & Osindero, 2014). One CGAN is fitted
in the direction X → Y and another one is fitted for Y → X . Based on a two-samPle test statistic,
the estimated causal direction is returned. Goudet et al. (2017) use generative neural networks for
cause-effect inference, to identify v-structures and to orient the edges of a given graPh skeleton.
Bahadori et al. (2017) devise a regularizer that combines an `1 Penalty with weights corresPonding
to the estimated Probability of the resPective feature being causal for the target. The latter estimates
are obtained by causality detection networks or scores such as estimated by the NCC. Besserve
et al. (2017) draw connections between GANs and causal generative models, using a grouP theoretic
framework. Kocaoglu et al. (2017) ProPose causal imPlicit generative models to samPle from con-
ditional as well as interventional distributions, using a conditional GAN architecture (CausalGAN).
The generator structure needs to inherit its neural connections from the causal graPh, i.e. the causal
graPh structure must be known. Louizos et al. (2017) ProPose the use of deeP latent variable models
and Proxy variables to estimate individual treatment effects.
Kilbertus et al. (2017) exPloit causal reasoning to characterize fairness considerations in machine
learning. Distinguishing between the Protected attribute and its Proxies, they derive causal non-
discrimination criteria. The resulting algorithms avoiding Proxy discrimination require classifiers to
be constant as a function of the Proxy variables in the causal graPh, thereby bearing some structural
similarity to our style features.
Distinguishing between core and style features can be seen as some form of disentangling factors of
variation. Estimating disentangled factors of variation has gathered a lot of interested in the context
of generative modeling (Higgins et al., 2017; Chen et al., 2016; Bouchacourt et al., 2017). For exam-
Ple, Matsuo et al. (2017) ProPose a “Transform Invariant Autoencoder” where the goal is to reduce
the dePendence of the latent rePresentation on a sPecified transform of the object in the original im-
age. SPecifically, Matsuo et al. (2017) Predefine location as the orthogonal style feature X⊥ and the
goal is to learn a latent rePresentation that does not include X⊥ . Here, we do not Predefine which
features are in X⊥. It could be location but also image quality, Posture, brightness, background and
contextual information. Additionally, the aPProach in Matsuo et al. (2017) cannot effectively deal
5The distinction between ‘conditionally indePendent’ features and ‘conditionally transferable’ (which is the
former modulo location and scale transformations) is for our PurPoses not relevant as we never make a linearity
assumPtion and the core or conditionally indePendent features would then always refer to the identity of the
object and not its location, scale or any other style feature.
4
Under review as a conference paper at ICLR 2018
1 X
Figure 2: Left: data generating process for the considered model as in Gong et al. (2016), where the effect
of the domain on the orthogonal features X ⊥ is mediated via unobserved noise ∆. Right: our setting. The
domain itself is unobserved but we can now observe the ID variable we use for grouping.
with a confounding situation where the distribution of the style features differs conditional on the
class (this is a natural restriction as the class label is not even observed in the autoencoder setting).
As in CoRe, Bouchacourt et al. (2017) exploit grouped observations. In a variational autoencoder
framework, they aim to separate style and content—they assume that samples within a group share
a common but unknown value for one of the factors of variation while the style can differ. Here
we try to solve a classification task directly without estimating the latent factors explicitly as in a
generative framework.
4	Counterfactual regularization
We first describe the standard notation for classification before developing a causal graph that allows
us to compare the setting of adversarial domain shifts to transfer learning, domain adaptation and
adversarial examples.
4.1	Notation for standard classification
Let Y ∈ Y be a target of interest. Typically Y = R for regression or Y = {1, . . . , K} in classifica-
tion with K classes. LetX ∈ Rp be a predictor, for example thep pixels ofan image. The prediction
y for y, given X = x, is of the form fθ (x) for a suitable function fθ with parameters θ ∈ Rd, where
the parameters θ correspond to the weights in a DNN. For regression, fθ (x) ∈ R, whereas for clas-
sification fθ(x) corresponds to the conditional probability distribution ofY ∈ {1, . . . , K}. Let ` be
a suitable loss that maps y and y = fθ(x) to R+. A standard goal is to minimize the expected loss
or risk
L(θ) = E['(Y,fθ (X))].
Let (χi,yi) for i = 1,...,n be the samples that constitute the training data and y = fθ(Xi) the
prediction for yi . A standard approach to parameter estimation is penalized empirical risk mini-
mization, where we choose the weights or parameters as θ = argminθ Ln(θ), with the empirical
loss given by Ln(θ) = 1 P2ι '(yi, fθ(Xi)) + λ ∙ pen(θ), where the penalty pen(θ) could be a
ridge penalty or penalties that exploit underlying geometries such as the Laplacian regularized least
squares (Belkin et al., 2006).
4.2	Causal graph
The full structural model for all variables is shown in the right panel of Figure 2. The domain
variable D is latent, in contrast to Gong et al. (2016). We add the ID variable (identity ofa person,
for example), whose distribution can change conditional on class Y. The ID variable is used to
5
Under review as a conference paper at ICLR 2018
group observations, see Section 4.4, and can be assumed to be latent in the setting of Gong et al.
(2016).
The rest of the graph is in analogy to Gong et al. (2016). The prediction is anti-causal, that is the
predictors X that we use for Y are non-ancestral to Y . In other words, the class label is causal
for the image and not the other way around. The causal effect from the class label Y on the im-
age X is mediated via two types of latent variables: the so-called core or ‘conditionally invariant’
features Xci and the orthogonal or style features X⊥ . The distinguishing factor between the two is
that external interventions ∆ are possible on the style features but not on the core features. If the
interventions ∆ have different distributions in different domains, then the distribution P(Xci|Y )
is constant across domains while P(X⊥∣Y) can change across domains. The style features X⊥
and Y are confounded, in other words, by the latent domain D. In contrast, the core or ‘condi-
tionally invariant’ features satisfy Xci ⊥⊥ D|Y. The dimension of Xci is chosen maximally large
such that this conditional independence is still true. The style variable can include point of view,
image quality, resolution, rotations, color changes, body posture, movement etc. and will in general
be context-dependent6. The style intervention variable ∆ influences both the latent style X⊥, and
hence also the image X. In potential outcome notation, we let X⊥(∆ = δ) be the style under inter-
vention ∆ = δ, X(Y, ID, ∆ = δ) the image for class Y, identity ID and style intervention ∆ and
this sometimes abbreviated as X(∆ = δ) for notational simplicity. Finally, fθ (X(∆ = δ)) is the
prediction under the style intervention ∆ = δ. For a formal justification of using a causal graph and
potential outcome notation simultaneously see Richardson & Robins (2013).
4.3	Domain adaptation, adversarial examples and adversarial domain shifts
In this work, we are interested in guarding against adversarial domain shifts. We use the causal graph
to explain the related but not identical goals of domain adaptation, transfer learning and guarding
against adversarial examples.
(i)	Domain adaptation and transfer learning. Assume we have J different domains, each with
a new distribution Fj for the interventions ∆ (or more generally of the joint distribution of
(Y, ∆)). The shift of Fj for different domains j = 1, . . . , J causes a shift in both the distri-
bution of X and in the conditional distribution Y |X . If we consider domain adaptation and
transfer learning together, their goal is generally to give the best possible prediction Yj (x) in
each domain j = 1, . . . , J.
(ii)	Standard adversarial examples. The setting of adversarial examples in the sense of Szegedy
et al. (2014) and Goodfellow et al. (2015) can also be described by the causal graph above by
using X⊥ (∆) = ∆ and identifying X⊥ with pixel-by-pixel additive effects. The magnitude of
the intervention ∆ is then typically assumed to be within an e-ball in 'q -norm around the origin,
with q = ∞ or q = 2 for example. If the input dimension is large many imperceptible changes
in the coordinates of X can cause a large change in the output, leading to a misclassification
of the sample. The goal is to devise a classification in this graph that minimizes the adversarial
loss
E[ ∆∈Rm a∆kq ≤e'(Y,fθ(X ⑷))]，	⑴
where X(∆) is the image under the intervention ∆ and Y = fθ(X(∆)) is the estimated
conditional distribution of Y, given the image under the chosen interventions.
(iii)	Adversarial domain shifts. Here we are interested in arbitrarily strong interventions ∆ ∈ Rq
on the style features X⊥, which are not known explicitly in general. Analogously to (1), the
adversarial loss under arbitrarily large style interventions is
Ladv(θ) = E h maxq '①,fθ(X(△)))].	⑵
In contrast to (1) the interventions can be arbitrarily strong but we assume that the style features
X⊥ can only change certain aspects of the image, while other aspects of the image (mediated
by the core features) cannot be changed. In contrast to Ganin et al. (2016), we use the term
6The type of features we regard as style and which ones we regard as core features can conceivably change
depending on the circumstances—for instance, is the color “gray” an integral part of the object “elephant” or
can it be changed so that a colored elephant is still considered to be an elephant?
6
Under review as a conference paper at ICLR 2018
“adversarial” to refer to adversarial interventions on the style features, while the notion of
“adversarial” in domain adversarial neural networks describes the training procedure. Never-
theless, the motivation of Ganin et al. (2016) is equivalent to ours—that is, to protect against
shifts in the distribution(s) of test data which we characterize by distinguishing between core
and style features.
4.4	Counterfactual observations / grouping
The classical problem of causal inference is that we can never observe a counterfactual. For instance,
we can only see the health outcome Z if we take a medicine, T = 1, or not, T = 0, but we
can never see both health outcomes simultaneously. The counterfactual in this context would be
an observation where we change the treatment but hold all observed and unobserved confounders
constant. If the treatment T changes while all other variables are kept constant, we could just read
off the treatment effect as Z(T = 1) - Z(T = 0) if Z is the health outcome of interest. Observing
such counterfactuals is in general impossible as we can either observe the outcome under treatment
or under no treatment but not both.
Here, we use the term counterfactual for a situation where we keep class label Y and ID constant but
allow the value of the style intervention ∆ to change. The new value of ∆ could be a do-intervention
(as when explicitly rotating an image in data augmentation) or it could be a noise-intervention by
sampling a new realization of ∆. The style intervention ∆ takes the same role as the treatment T
in the previous medical example. In contrast to the medical example, however, counterfactuals are
conceivable for image analysis as we can see the same object (Y, ID) under different conditions
(‘treatments’) ∆.
As an example, if Y is the binary variable whether a person wears glasses and ID is the identity of
a person, then ∆ corresponds to all other variables that determine the different images of the same
person (either consistently wearing glasses or not) and includes background, posture, viewing angle,
image quality, etc.
In further contrast to the medical setting, we are not interested primarily in the ‘treatment effect’
of the style intervention ∆ but we merely use it to implicitly rule out parts of the feature space for
classification. We know that any ‘treatment effect’ of ∆ occurs in the space of the style or orthogonal
features X⊥ and not in the ‘conditionally invariant’ space Xci and we would thus like to penalize
any change in the classification under different style interventions ∆ but constant class and identity
(Y,ID).
Notationally, we have for sample i ∈ {1, . . . , n} with class label and identifier (Y, ID) = (yi, idi),
mi different images x(yi , idi , ∆i,j ) for j = 1, . . . , mi under different (unobserved) values of
∆i,1, . . . , ∆i,mi. Let m = Pi mi denote the total number of samples and c = m - n, the number
of counterfactual observations. Denote the j-th observation of sample i, by xi,j ∈ Rp. Typically
mi = 1 for most samples and occasionally mi ≥ 2.
4.4.1	S tandard approach: pooled estimator
The standard approach is to simply pool over all available observations, ignoring any grouping
information that might be available. The pooled estimator thus treats all examples identically by
summing over the loss as
n mi
θpool = argmi□θ ^XX 卜除 fθ(Xij))] + λ ∙ pen(θ),
m i=1 j=1
where pen(θ) could be a ridge penalty. The pooled estimator in all examples is always the ridge
estimator with a cross-validated choice of the penalty parameter. The adversarial loss of the pooled
estimator will in general be infinite; see §4.6 for a concrete example. Using Figure 2, one can show
that the pooled estimator will work well in terms of the adversarial loss Ladv if both (i) Y ⊥⊥ X |Xci
and (ii) Y 其 Xci|X⊥. The first condition (i) implies that if the estimator learns to extract XCi
from the image X , there is no further information in X that explains Y and, therefore, the direction
corresponding to X ⊥ is not required for predicting Y. The second condition (ii) is fulfilled if the
relations between Y, Xci , and X⊥ are not deterministic. Intuitively, it ensures that X⊥ cannot
replace Xci in the first condition. From (i) and (ii), we see that the pooled estimator will work well
7
Under review as a conference paper at ICLR 2018
in terms of the adversarial loss Ladv if (a) the edge from X⊥ to X is absent or if (b) both the edge
from D to X ⊥ and the edge from Y to X⊥ are absent (cf. Figure 2).
4.5	CoRe estimator
In order to minimize the adversarial loss (2) we have to ensure fθ (x(∆)) is as constant as possible
as a function of ∆ for all x ∈ Rp . Let I be the invariant parameter space
I := {θ : fθ(x(∆)) is constant as function of ∆ for all x ∈ Rp}
= {θ : fθ(x) = fθ(xci) is a function of the core features xci ∈ Rp only.}.
For all θ ∈ I , the adversarial loss (2) is identical to the loss under no interventions at all. More pre-
cisely, let X be a shorthand notation for X(∆ = 0), the images in absence of external interventions:
if θ ∈ I, then E[ max '(Y,fe(X(△)))] = E['(Y,fθ(X))].
The optimal predictor in the invariant space I is
θ* = argmi□θ E['(Y,fθ(X))]suchthat θ ∈ I.	(3)
Iffθ is only a function of the core features Xci, then θ ∈ I. The challenge is that the core features are
not directly observable and we have to infer the invariant space I from data. To get an approximation
to the optimal invariant parameter vector (3), we use empirical risk minimization:
1 n mi
θcore = argminθ n XX '(yi,fθ(xi,j)) such that θ ∈ In,	(4)
i=1 j=1
where the first part is the empirical version of the expectation in (3). The unknown invariant param-
eters space I is approximated by an empirically invariant space In , defined as
n
In := {θ : Xσi2(θ) ≤ τ},
i=1
where σi2(θ) is the variance of fθ (xi,j) when varying j = 1, . . . , mi for a fixed value ofi and τ ≥ 0
is a regularization constant. Setting τ = 0 is equivalent to demanding that the estimated predictions
for the class labels are identical across all mi counterfactuals of image i, while slightly larger values
of τ allow for some small degree of variations. For all values τ ≥ 0 the true invariant space I is a
subset of the empirically invariant subspace In , that is I ⊆ In . Under the right assumptions we get
In = I for n → ∞. We return to this question in §4.6. One can equally use the Lagrangian form of
the constrained optimization in (4), with a penalty parameter λ instead of a constraint τ to get
n mi
θcore = argmi□θ — XX S (Xij)) + λ ∙peniD(θ),	(5)
n i=1 j=1
n
where penID(θ)	=	fθtLIDfθ,	and fθ	∈	Rm	is the value of	fθ (xi,j) at all m =	i=1 mi	observa-
tions. The matrix LID is a graph Laplacian (Belkin et al., 2006), where the underlying graph has n
connectivity components as all samples that have the same ID are connected by an edge and form
fully connected connectivity components. The graph Laplacian regularization is identical to penal-
izing the sum over the variances σi2(θ). The graph for the underlying regularization is formed in the
sample space and induced by the identifier variable ID, in contrast to graphs formed in feature space
as in Sandler et al. (2009), where prior knowledge is used to form the graph by connecting features
that share similar characteristics.
We show in §C.1 that the outcome does not depend strongly on the chosen value of the penalty λ
and the experiments show that it is crucial to define the graph in terms of the identifier variable ID.
Other regularizations do not perform nearly as well when trying to guard against adversarial domain
shifts.
8
Under review as a conference paper at ICLR 2018
Figure 3: a) Examples from the stickmen training set. The first three images from the left have y ≡ child;
the remaining three images have y ≡ adult. Connected images are counterfactual examples. b) Misclassified
observations from test set 2. c) Misclassification rates for c = 50. Results for c ∈ {20, 500, 2000} can be
found in Figure C.10.
4.6 Theoretical results
In §A we analyze the adversarial loss, defined in Eq. (2), for the pooled and the CORE estimator
in a one-layer network for binary classification (logistic regression). Here, we briefly sketch the
result while all details are given in §A. Assume the structural equation for the image X ∈ Rp is
linear in the style features X ⊥ ∈ Rq (with generally p	q), the interventions are additive and
we use logistic regression to predict a class label Y ∈ {-1, 1}. Under suitable assumptions (cf.
Assumption 1), the pooled estimator has infinite adversarial loss while the adversarial loss of the
CORE estimator converges to the optimal adversarial loss as n → ∞.
5	Experiments
We perform an array of different experiments: in §5.1 and §5.2 we study how CORE can handle
confounded training data sets and changing style features in test distributions. For the assessment
we explicitly control the level of confounding. In §5.3, we consider classifying elephants and horses
where X⊥ ≡ color. In §B, we include two additional experiments: in the first one, Y ≡ gender
and X⊥ ≡ wearing glasses; in the second one, Y ≡ wearing glasses and X⊥ ≡ brightness.
Additional experimental results for the settings introduced in §2 can be found in §C.2 and §C.3. A
TensorFlow (Abadi et al., 2015) implementation of CoRe will be made available as well as further
code necessary to reproduce the experiments. In addition to the details provided below, information
on the employed architectures can be found in §C.7. An open question is how to set the value of
the tuning parameter τ or the penalty λ in Lagrangian form. We show in §C.1 that performance is
typically not very sensitive to the choice of λ.
5.1	Stickmen image-based age classification
In this example we consider synthetically generated stickmen images (cf. Figure 3a). The target
of interest is Y ∈ {adult, child} and Xci ≡ height. The class Y is causal for height and height
cannot be easily intervened on, so we consider it to be a core feature—it is a robust predictor for
differentiating between children and adults. Additionally, there is a dependence between age and
X ⊥ ≡ movement in the training dataset which arises through the hidden common cause D ≡
place of observation. The data generating process is illustrated in Figure C.9. For instance, the
images of children might mostly show children playing while the images of adults typically show
them in more “static” postures. If the learned model exploits this dependence for predicting Y , it
will fail when presented images of, say, dancing adults.
Figure 3a shows examples from the training set where large movements are associated with children
and small movements are associated with adults. Test set 1 follows the same distribution. In test
sets 2 and 3 X ⊥ is intervened on such that the edge from D to X ⊥ is removed and the dependence
between Y and X⊥ vanishes. In test sets 2 and 3 large movements are associated with both children
and adults, while the movements are heavier in test set 3 than in test set 2. Figure C.10 shows exam-
ples from all test sets. Figure 3c shows misclassification rates for CoRe and the pooled estimator
9
Under review as a conference paper at ICLR 2018
⑶ CF setting 1, μ = 30
(b) Examples of misclassified observations.
y ≡ no glasses
P core (no gl.) = 0.96
Ppool (nogl.) = 0
y ≡ glasses
P core (gl.) = 0.62
Ppool (gl.) = 0.02
y ≡ glasses
P core (gl.) = 0.99
Ppool (gl.) = 0.09
(c) Misclassification rates.
Figure 4: a) Examples from the CelebA image quality dataset. The first three images from the left have
y ≡ no glasses; the remaining three images have y ≡ glasses. Connected images are counterfactual examples.
b) MisClassified examples from the test sets. C) MisClassifiCation rates for μ = 30 and C = 5000. Results for
different CounterfaCtual settings and μ ∈ {30,40, 50} can be found in Figure C.12.
for c = 50 with a total sample size of m = 20000. For as few as 50 CounterfaCtual observations,
CoRe suCCeeds in aChieving good prediCtive performanCe on test sets 2 and 3 where the pooled
estimator fails (test errors > 40%). These results suggest that the learned representation of the
pooled estimator uses movement as a prediCtor for age while CoRe does not use this feature due to
the CounterfaCtual regularization. Importantly, inCluding more CounterfaCtual examples would not
improve the performanCe of the pooled estimator as these would be subjeCt to the same bias and
henCe also predominantly have examples of heavily moving Children and “statiC” adults (also see
Figure C.10 whiCh shows results for c ∈ {20, 500, 2000}).
5.2	Eyeglasses detection: Image quality intervention
As in §2.1, we use the CelebA dataset and Consider the problem of Classifying whether the person in
the image is wearing eyeglasses. Here, X ⊥ is the quality of the image whiCh differs Conditional on
Y 7—if the image shows a person wearing glasses, the image quality tends to be lower. This setting
mimiCs the Confounding that oCCurred in the Russian tank legend (Cf. §1). The strength of the image
quality intervention is governed by sampling the new image quality as a perCentage of the original
image,s quality from a Gaussian distribution N(μ = 30,σ = 1θ). Images of people without glasses
are not Changed. Thus, we only have CounterfaCtual observations for Y ≡ glasses. Figure 4a shows
examples from the training set. Here, we use as the CounterfaCtual observation the same image but
with a newly sampled image quality value from N (30, 10). We Call using the same image as a
CounterfaCtual “CF setting 1”. Two alternatives for ConstruCting CounterfaCtual observations for this
setting are disCussed in §B.2.1. Here, c = 5000 and m = 20000.
Figure 4C shows misClassifiCation rates for CoRe and the pooled estimator on different test sets.
Examples from all test sets Can be found in Figure C.11. Test set 1 follows the same distribution as
the training set. In test set 2 the Class of the quality intervention is reversed, i.e. the quality of images
showing people without glasses tends to be lower. In test set 3 all images are left unChanged and in
test set 4 the quality of all images is deCreased. First, we notiCe that the pooled estimator performs
better than CoRe on test set 1. This Can be explained by the faCt that it Can exploit the prediCtive
information Contained in an image’s quality while CoRe is restriCted not to do so. SeCond, we
observe that the pooled estimator does not perform well on test sets 2-4 as its learned representation
seems to use the image’s quality as a prediCtor for the target. In Contrast, the prediCtive performanCe
of CoRe is hardly affeCted by the Changing image quality distributions. More experimental details
are provided in §C.5. Results for quality interventions of different strengths (μ ∈ {30, 40, 50}) are
shown in Figure C.12.
7In §B.2 we additionally look at the Case where the brightness distribution differs Conditional on Y .
10
Under review as a conference paper at ICLR 2018
(a) Training examples.
(b) Examples of misclassified observations.
y ≡ horse	y ≡ horse
P c“e( horse) = 0.72 Pcore (horse) = 1.00
P pool (horse) = 0.01 P pool (horse) = 0.01
y ≡ elephant
PCOre (ele.) = 0.95
Ppool ( ele.) = 0.00
(c) Misclassification rates.
Figure 5: a) Examples from the subsampled and augmented AwA2 dataset. The first three images from the
left shows horses, the remaining three images show elephants. Connected images are counterfactual examples.
b) Misclassified examples from the test sets. c) Misclassification rates.
5.3	Elmer the Elephant
In this example, we want to assess whether invariance with respect to X⊥ ≡ color can be achieved.
In the children’s book “Elmer the elephant”8 one instance of a colored elephant suffices to recognize
it as being an elephant, making the color “gray” no longer an integral part of the object “elephant”.
Motivated by this process of concept formation, we would like to assess whether CoRe can exclude
“color” from its learned representation by including a few counterfactuals of different color.
We work with the “Animals with attributes 2” (AwA2) dataset (Xian et al., 2017) and consider
classifying images of horses and elephants. The data generating process is illustrated in Figure C.14.
We include counterfactual examples by adding grayscale images for c = 250 images of elephants,
i.e. counterfactuals are only available for one class and the shift in color is quite subtle. The total
sample size is 1850.
Figure 5a shows examples from the training set and Figure 5c shows misclassification rates for
CoRe and the pooled estimator on different test sets. Examples from all test sets can be found in
Figure C.13. Test set 1 contains original, colored images only. In test set 2 images of horses are in
grayscale and the colorspace of elephant images is modified, effectively changing the color gray to
red-brown. Test set 3 contains grayscale images only and in test set 4 the colorspace of all images
is shifted towards red. The details are given in §C.6 . We observe that the pooled estimator does not
perform well on test sets 2 and 3 as its learned representation seems to exploit the fact that “gray”
is predictive for the target in the training set. Using this information helps its predictive accuracy on
test set 1. In contrast, the predictive performance of CoRe is hardly affected by the changing color
distributions.
It is noteworthy that a colored elephant can be recognized as an elephant by adding a few examples
of a grayscale elephant to the very lightly colored pictures of natural elephants. If we just pool
over these examples, there is still a strong bias that elephants are gray. The CoRe estimator, in
contrast, demands invariance of the prediction for instances of the same elephant and we can learn
color invariance with a few added grayscale images.
While a thorough analysis in terms of fairness considerations is beyond the scope of this work, we
would like to draw the following connection. If “color” was a protected attribute or a proxy for one,
CoRe would satisfy fairness in the sense that it would not include it in its learned representation. In
contrast, there is no way to avoid that the pooled estimator extracts and uses “color” for its decisions.
6	Conclusion
Distinguishing the latent features in an image into core and style features, we have proposed coun-
terfactual regularization (CoRe) to achieve robustness with respect to arbitrarily large interventions
on the style or conditionally invariant features. The main idea of the CoRe estimator is to exploit the
fact that we often have instances of the same object in the training data. By demanding invariance of
8https://en.wikipedia.org/wiki/Elmer_the_Patchwork_Elephant
11
Under review as a conference paper at ICLR 2018
the classifier amongst a group of instances that relate to the same object, we can achieve invariance
of the classification performance with respect to adversarial interventions on style features such as
image quality, fashion type, color, or body posture. The training also works despite sampling biases
in the data.
There are two main applications areas. If the style features are known explicitly, we can achieve the
same classification performance as standard data augmentation approaches but using fewer instances
which, on top, do not have to be carefully balanced in the training data. Perhaps more interestingly,
if the style features are unknown, the regularization of CoRe avoids usage of them automatically by
penalizing features that vary strongly between different instances of the same object in the training
data.
An interesting line of work would be to use larger models such as Inception or large ResNet archi-
tectures (Szegedy et al., 2015; He et al., 2016). These models have been trained to be invariant to an
array of explicitly defined style features. In §B.1 we include results which show that using Inception
V3 features does not guard against interventions on more implicit style features. We would thus
like to assess what benefits CoRe can bring for training Inception-style models end-to-end, both in
terms of sample efficiency and in terms of generalization performance.
While we showed some examples where the necessary grouping information is available, an inter-
esting possible future direction would be to use video data since objects display temporal constancy
and the temporal information can hence be used for grouping and counterfactual regularization.
Potentially an analogous approach could also help to debias word embeddings.
12
Under review as a conference paper at ICLR 2018
References
M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean,
M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefow-
icz, L. Kaiser, M. Kudlur, J. Levenberg, D. Mane, R. Monga, S. Moore, D. Murray, C. Olah,
M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Va-
SUdevan, F. Viegas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng.
TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL https:
//www.tensorflow.org/. Software available from tensorflow.org.
J. Aldrich. Autonomy. Oxford Economic Papers,41:15-34,1989.
M. T. Bahadori, K. Chalupka, E. Choi, R. Chen, W. F. Stewart, and J. Sun. Causal regularization.
ArXiv e-prints, 2017. URL http://arxiv.org/abs/1702.02604.
S. Barocas and A. D. Selbst. Big Data’s Disparate Impact. 104 California Law Review 671, 2016.
M. Belkin, P. Niyogi, and V. Sindhwani. Manifold regularization: A geometric framework for
learning from labeled and unlabeled examples. Journal of machine learning research, 7(Nov):
2399-2434, 2006.
S.	Ben-David, J. Blitzer, K. Crammer, and F. Pereira. Analysis of representations for domain adap-
tation. In Advances in Neural Information Processing Systems 19. 2007.
M. Besserve, N. Shajarisales, B. Scholkopf, and D. Janzing. Group invariance principles for causal
generative models. ArXiv e-prints, 2017. URL http://arxiv.org/abs/1705.02212.
T.	Bolukbasi, K.-W. Chang, J. Y. Zou, V. Saligrama, and A. T. Kalai. Man is to computer program-
mer as woman is to homemaker? debiasing word embeddings. In Advances in Neural Information
Processing Systems 29. 2016.
D. Bouchacourt, R. Tomioka, and S. Nowozin. Multi-level variational autoencoder: Learning
disentangled representations from grouped observations. ArXiv e-prints, 2017. URL http:
//arxiv.org/abs/1705.08841.
X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel. InfoGAN: Interpretable
Representation Learning by Information Maximizing Generative Adversarial Nets. In Advances
in Neural Information Processing Systems 29. 2016.
K. Crawford. Artificial intelligence’s white guy problem. The New York Times, June 25
2016, 2016. URL https://www.nytimes.com/2016/06/26/opinion/sunday/
artificial- intelligences- white- guy- problem.html.
G. Csurka. A comprehensive survey on domain adaptation for visual applications. In Domain
Adaptation in Computer Vision Applications., pp. 1-35. 2017.
J.	Emspak. How a machine learns prejudice. Scientific American, December 29
2016,	2016.	URL https://www.scientificamerican.com/article/
how-a-machine-learns-prejudice/.
Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. Marchand, and
V. Lempitsky. Domain-adversarial training of neural networks. Journal of Machine Learning
Research, 17(1), 2016.
M. Gong, K. Zhang, T. Liu, D. Tao, C. Glymour, and B. Scholkopf. Domain adaptation with
conditional transferable components. In International Conference on Machine Learning, 2016.
I. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. In
International Conference on Learning Representations, 2015.
O. Goudet, D. Kalainathan, P. Caillou, D. Lopez-Paz, I. Guyon, M. Sebag, A. Tritas, and P. Tubaro.
Learning Functional Causal Models with Generative Neural Networks. ArXiv e-prints, 2017.
URL https://arxiv.org/abs/1709.05321.
13
Under review as a conference paper at ICLR 2018
T. Haavelmo. The probability approach in econometrics. Econometrica, 12:S1-S115 (supplement),
1944.
K.	He, X. Zhang, S. Ren, and J. Sun. Delving Deep into Rectifiers: Surpassing Human-Level
Performance on ImageNet Classification. ICCV, 2015.
K.	He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. CVPR, 2016.
I. Higgins, L. Matthey, A. Pal, C. Burges, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner.
beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework. Interna-
tional Conference on Learning Representations, 2017.
N. Kilbertus, M. Rojas-Carulla, G. Parascandolo, M. Hardt, D. Janzing, and B. Scholkopf. Avoiding
discrimination through causal reasoning. Advances in Neural Information Processing Systems,
2017.
D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. ICLR, 2015.
M. Kocaoglu, C. Snyder, A. G. Dimakis, and S. Vishwanath. CausalGAN: Learning Causal Implicit
Generative Models with Adversarial Training. ArXiv e-prints, 2017. URL https://arxiv.
org/abs/1709.02023.
A. Krizhevsky, I. Sutskever, and G. E Hinton. Imagenet classification with deep convolutional neural
networks. In Advances in Neural Information Processing Systems 25. 2012.
Y. LeCun and C. Cortes. MNIST handwritten digit database. 2010. URL http://yann.lecun.
com/exdb/mnist/.
Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face attributes in the wild. In Proceedings of
International Conference on Computer Vision (ICCV), 2015.
D. Lopez-Paz and M. Oquab. Revisiting Classifier Two-Sample Tests. International Conference on
Learning Representations (ICLR), 2017.
D. Lopez-Paz, R. Nishihara, S. Chintala, B. Scholkopf, and L. Bottou. Discovering causal signals
in images. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2017),
2017.
C. Louizos, U. Shalit, J. M. Mooij, D. Sontag, R. Zemel, and M. Welling. Causal effect inference
with deep latent-variable models. Advances in Neural Information Processing Systems, 2017.
R.	Turner J. Peters M. Rojas-Carulla, B. Scholkopf. Causal transfer in machine learning. ArXiv
e-prints, 2017. URL https://arxiv.org/abs/1507.05333.
S.	Magliacane, T. van Ommen, T. Claassen, S. Bongers, P. Versteeg, and J. M. Mooij. Causal transfer
learning. ArXiv e-prints, 2017. URL https://arxiv.org/abs/1707.06422.
T.	Matsuo, H. Fukuhara, and N. Shimada. Transform invariant auto-encoder. ArXiv e-prints, 2017.
URL http://arxiv.org/abs/1709.03754.
M.	Mirza and S. Osindero. Conditional Generative Adversarial Nets. ArXiv e-prints, 2014. URL
https://arxiv.org/abs/1411.1784.
J.	Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press, New York,
USA, 2nd edition, 2009.
J.	Peters, P. Buhlmann, andN. Meinshausen. Causal inference using invariant prediction: identifica-
tion and confidence intervals. Journal of the Royal Statistical Society, Series B (with discussion),
to appear, 2016.
T. Richardson and J. M. Robins. Single world intervention graphs (SWIGs): A unification of the
counterfactual and graphical approaches to causality. Center for the Statistics and the Social
Sciences, University of Washington Series. Working Paper 128, 30 April 2013, 2013.
14
Under review as a conference paper at ICLR 2018
Ted Sandler, John Blitzer, Partha P Talukdar, and Lyle H Ungar. Regularized learning with networks
of features. In Advances in neural information processing Systems, pp. 1401-1408, 2009.
B.	Scholkopf, D. Janzing, J. Peters, E. Sgouritsa, K. Zhang, and J. Mooij. On causal and anticausal
learning. In Proceedings of the 29th International Conference on Machine Learning (ICML), pp.
1255-1262, 2012.
B.	Scholkopf, C. Burges, and V. Vapnik. Incorporating invariances in support vector learning ma-
chines. pp. 47-52. Springer, 1996.
C.	Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing
properties of neural networks. In International Conference on Learning Representations, 2014.
C.	Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Ra-
binovich. Going deeper with convolutions. In Computer Vision and Pattern Recognition (CVPR),
2015.
M.	Gong K. Zhang D. Tao X. Yu, T. Liu. Transfer learning with label noise. ArXiv e-prints, 2017.
URL https://arxiv.org/abs/1707.09724.
Y. Xian, C. H. Lampert, B. Schiele, and Z. Akata. Zero-shot learning - A comprehensive evaluation
of the good, the bad and the ugly. ArXiv e-prints, 2017. URL http://arxiv.org/abs/
1707.00600.
E. Yudkowsky. Artificial intelligence as a positive and negative factor in global risk. Global catas-
trophic risks, 1, 2008.
K. Zhang, B. Scholkopf, K. Muandet, and Z. Wang. Domain adaptation under target and conditional
shift. In International Conference on Machine Learning, 2013.
K. Zhang, M. Gong, and B. Scholkopf. Multi-source domain adaptation: A causal view. In Pro-
ceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015.
15
Under review as a conference paper at ICLR 2018
S upplementary material
A Logistic regression
Assume the structural equation for the image X ∈ Rp is linear in the style features X⊥ ∈ Rq (with
generally p q) and we use logistic regression to predict a class label Y ∈ {-1, 1}. Let the inter-
ventions ∆ ∈ Rq act additively on the style features X ⊥ (this is only for notational convenience)
and let the style features X ⊥ act in a linear way on the image X via a matrix W ∈ Rp×q (this is an
important assumption without which results are more involved). The core or ‘conditionally invari-
ant’ features are Xci ∈ Rr, where in general r ≤ p but this is not important for the following. For
independent εY , εID, εX⊥ , εX in R, Rq, Rr, Rp respectively with positive density on their support
and continuously differentiable functions ky, kID, kX⊥ , kXci , kx,
class Y — ky (D,εγ)
identifier ID J kiD(Y, £id)
core or conditionally invariant features XCi J kχCi (Y, ID)
style or orthogonal features X⊥ J kX⊥ (D, Y, εX⊥ ) + ∆
image X J kX(Xci,εX) +WX⊥.
Of these, Y, X and ID are observed whereas D, Xci , ∆, X⊥ and the noise variables are latent.
We assume a logistic regression as a prediction ofY from the image data X:
fθ(x) :
exp(xtθ)
1 + exp(xt θ)
√-1 ∙	.	1 .	∙ .1	1	. ∙	. Zi ∙.) 久	1	1	1 ∙ . ∙ 1	C/	∖
Given training data With m samples, We estimate θ With θ and use here a logistic loss 'θ(yi, Xi)=
log(1 + exp(-yi(xitθ))) for training and testing. Some interesting expected losses on test data
include
L(θ) = E['(Y,fθ (X)))]
Ladv(θ)= Eh max '(Y,fθ(X(△)))],
where the X in the first loss is a shorthand notation for X(△ = 0), that is the images in absence
of interventions on the style variables. The first loss is thus a standard logistic loss in absence of
adversarial interventions. The second loss is the loss under adversarial style or domain interventions
as We alloW arbitrarily large interventions on X ⊥ here. The corresponding benchmarks are
L* = minL(θ), and Ladv = minLadv(θ)∙
The formulation of Theorem 1 relies on the folloWing assumptions.
Assumption 1. We require the following conditions to hold:
(A1) Assume △ is sampled from a distribution for training data in Rq with positive density on
an -ball in `2 -norm around the origin for some > 0.
(A2) Assume the matrix W has full rank q.
(A3) Assume c ≥ q, that is the number c = m - n of counterfactual examples in the samples is
at least as large as the dimension of the style variables.
Regarding (A3): the sampling process is as folloWs. We collect n independent samples (yi, idi, δi,1)
from a distribution of (Y, ID, △) that satisfies the constraints above. Then, for c = m - n of the
samples We select each time i ∈ {1, . . . , n} at random, keep (yi, idi) fixed (and hence also the
realization of X⊥ is fixed) and redraW a neW value of △ as δi,ui+1 if ui is the current number of
counterfactual examples for sample i. This leads to m samples in total With in general n distinct val-
ues of (yi, idi) and mi counterfactuals at each sample With corresponding xi,j With i ∈ {1, . . . , n}
and j ∈ {1, . . . , mi}.
16
Under review as a conference paper at ICLR 2018
Theorem 1. Under Assumption 1, with probability 1 with respect to the training data, the pooled
estimator has infinite adversarial loss
Ladv (θpθθl) = ∞.
For the CORE estimator, for n → ∞,
,ʌ____
Ladv(θ"re →p Ladv.
An equivalent results can be derived for misclassification loss instead of logistic loss (with infinity
replaced by 1).
Proof. First part. To show the first part, namely that with probability 1,
Ladv (θpθθl) = ∞,
We need to show that Wtθpo°l = 0 with probability 1. The reason this is sufficient is as follows: if
Wtθ 6= 0, then Ladv (θ) = ∞ as we can then find a v ∈ Rq such that γ := θtWv 6= 0. Setting
∆κ = κv for κ ∈ R, we get x(∆κ)tθ = x(∆ = 0)tθ + κγ. Hence log(1 + exp(-x(∆κ)tθ)) → ∞
for either κ → ∞ or κ → -∞.
To show that Wtθpo°l = 0 with probability 1, let θ* be the oracle estimator that is constrained to be
orthogonal to the column space of W :
1n
θ = argminθ:w t©=。Ln(θ) with Ln(θ) := nɪ^'(gi,fθ (xi(∆i))).	(6)
We show Wtθpo°l = 0 by contradiction. Assume hence that Wtθpo°l = 0. If this is indeed the case,
then the constraint Wtθ = 0 in (6) becomes non-active and we have θpoοl = θ*. This would imply
that taking the directional derivative of the training loss with respect to any δ ∈ Rp in the column
space of W should vanish at the solution θ*. Define ri (θ) := (yi + 1)/2 - f@*. For all i = 1,...,n
we have ri 6= 0. The derivative g(δ) of Ln(θ) in direction of δ is proportional to
n	mi
g(δ) = n X ri(θ*) X xt,jδ,	⑺
i=1	j=1
xi,j ∈ Rp is the j-th counterfactual for training sample i (with j ∈ {1, . . . , mi}). Let xi,j (0) =
xi,1(0) for i = 1, . . . , n be the counterfactual training data in absence of any interventions (∆i,j =
0). Since the interventions only have an effect on the column space of W in X, the oracle estimator
θ* is identical under the true training data and the counterfactual training data χ(0). Hence, for any
δ in Rp, the derivative g(δ) in (7) can also be written as
n	mi
g(δ) = - Xri(θ*) Xxi,k(0)tδ.	(8)
n i=1	j=1
Taking the difference between (7) and (8),
n	mi
—Xri(θ*)(X(xi,j -xi,j(0))tδ) =0.	(9)
n i=1	j=1
Now, by the model assumptions, xi,j - xi,j (0) = W∆i,j. Since δ is in the column-space of W,
there exists u ∈ Rq such that δ = Wu. then (9) can be written as
n	mi
-Xn(θ*) X∆t,jWtWu = 0	(10)
n i=1	j=1
From (A2) we have that the eigenvalues of WtW are all positive. Also %(θ*) is not a function of
the interventions ∆i,j since, as already argued above, the estimator θ* is identical whether trained
on the original data xi,j or on the counterfactual data xi,j (0). If we condition on (xi(0), yi) for
i = -, . . . , n (that is everything except for the random ∆i,j, i = -, . . . , n), then the interventions
17
Under review as a conference paper at ICLR 2018
∆i,j are by (A1) drawn from a continuous distribution. Hence the left hand side of (10) has a
continuous distribution, and the probability of the left hand side of (10) being not identically 0 is 1.
This completes the proof of the first part by contradiction.
C	1	j T-I . 1	1	r∙ . 1	. 1	.,1	F F ∙ 1 ∙. < 久 CCfQ
Second part. For the second part, we first show that with probability 1, θcore
O .	O .	.
θ* With θ* defined
as in (6). Note that the invariant space is for this model the linear subspace I = {θ : Wtθ = 0}.
Note that by their respective definitions,
1 n mi
θ* = argming 一 ΣΣ'(yi,fθ(xi,j)) such that θ ∈ I,
ʌ
θcore
i=1 j=1
1 n mi
argmi□θ 一 ΣΣ'(yi,fθ(xi,j)) such that θ ∈ In.
m i=1 j=1
By (A2) and (A3), With probability 1, In = {θ : Wtθ = 0} since the number of counterfactuals
examples is equal to or exceeds the rank q of W and X ⊥ has a linear influence on X . Hence With
probability 1, We have I = In and hence θcore = θ . We thus need to show that
_	, O , ,	_ ,
Ladv(θ*) →p Ladv.	(11)
Since θ* is in I, we have '(y, x(∆)) = '(y, x(0)), where x(0) are the previously discussed counter-
factual data in the absence of interventions. Hence
1 n mi
θ* = argminj 一 XX
'(yi,fθ(xi,j(0))) such that θ ∈ I,	(12)
that is the estimator is unchanged if we use the data without interventions (∆i = 0) as training data.
Define the population-optimal vector as
θ* = argming E [ max '(Y,fθ(X(△)))] such that θ ∈ I,
which can for the same reason be written as
θ* = argming E['(Y, fθ(X(△ = 0)))] such that θ ∈ I.	(13)
Hence (12) and (13) can be written as
n mi
θ*=argmi%θ∈ι Ln0)(θ)where Ln0)(θ) := m XX '(y ,fθ (Xij(O)))
θ* = argminθ,θ∈I L(O)(θ) where L⑼(θ) := E['(Y,fθ(X(△ = 0)))].
Comparing (12) and (13), by uniform convergence of L(n0) to the population loss L(0) under the
assumed sampling where n samples of (Y, ID) are drawn independently then c = m - n samples
are redrawn from this empirical sample at random, we have L(O) (θ*) →p L(O) (θ*).
By definition of I and θ* we have Ladv = Ladv(θ*) = L(0)(θ*). As θ* is in I, we also have
Ladv(θ*) = L(0)(θ*). Since, from above, L(0)(θ*) →p L(0)(θ*), this also implies Ladv(θ*) →p
Ladv(θ*) = Ladv. This completes the proof, using the previous fact that θcore = θ* with probabil-
ity 1 under (A3).
□
B	Additional experiments
B.1 Gender classification
We work with the CelebA dataset (Liu et al., 2015) and consider the problem of classifying whether
the person in the image is male or female. We create a confounding by including mostly images
of men wearing glasses while the images of women do not include photos of women with glasses.
As counterfactuals, we use an image of the same person without glasses if the person is male and
18
Under review as a conference paper at ICLR 2018
(a)	Training examples.
QMItIfi
(b)	Examples from test set 1.
(c) Examples from test set 2.
Figure B.1: Examples from the CelebA gender dataset.
with glasses if the person is female. We call using an image of the same person as counterfactual
“CF setting 2”. Examples from the training and test sets are shown in Figure B.2. Test set 1 follows
the same distribution as the training set. In test set 2 the association between gender and glasses is
flipped: women always wear glasses while men never wear glasses.
In this example, we would like to assess whether the results will differ when (a) training a four-
layer CNN (as detailed in Table C.1) end-to-end versus (b) using Inception V3 features and merely
retraining the softmax layer. Figure B.2 shows the results for varying numbers of m and c—in
the left column for training a four-layer CNN; in the right column for using Inception V3 features.
Overall, we see the same trends: As c increases, the performance difference between CORE and the
pooled estimator becomes smaller. This is due to the fact that X⊥ is binary in this example and,
therefore, including counterfactual examples corresponds to data augmentation. Interestingly, the
pooled estimator performs worse on test set 2 as m becomes larger. It thus seems to exploit X ⊥ to
a larger extent as m grows.
B.2 Eyeglasses detection: Brightness intervention
As in §5.2 we work with the CelebA dataset and consider the problem of classifying whether the
person in the image is wearing eyeglasses. Here we analyze a confounded setting that could arise
as follows. Say the hidden common cause of Y and X⊥, D indicates whether the image was taken
outdoors or indoors. If it was taken outdoors, then the person wears glasses and the image tends to
be brighter. If the image was taken indoors, then the person does not wear glasses and the image
tends to be darker. In other words, X⊥ ≡ brightness and the structure of the data generating process
is equivalent to the one shown in Figure C.9. Figure B.3a shows examples from the training set.
Here, we use as the counterfactual observation the same image (CF setting 1) but with a different
brightness. Two alternatives for constructing counterfactual observations in this setting are discussed
in §B.2.1. Weusec = 2000 andm = 20000.
For the brightness intervention, we sample the value for the magnitude of the brightness increase
resp. decrease from an exponential distribution with mean β = 20. Specifically, we use ImageMag-
ick9 to modify the brightness of each image. In the training set and test set 1, we sample the
brightness value as bi,j = 100 + yiei,j where ei,j 〜 ExP(β-1) and y% ∈ {-1,1}. y% = 1 corre-
sponds to yi ≡ glasses. We then apply the command convert -modulate bij,100,100
input.jpg output.jpg to the image. Importantly, since we sample from an exponential dis-
tribution, the brightness interventions are quite subtle in many cases as can be seen in Figure B.3a.
Figure B.3c shows misclassification rates for CoRe and the pooled estimator on different test sets.
Examples from all test sets can be found in Figure B.4. Test set 1 follows the same distribution as
the training set. In test set 2 the sign of the brightness intervention is reversed, i.e. images of people
with glasses tend to be darker; images of people without glasses tend to be brighter. In test set 3
all images are left unchanged and in test set 4 the brightness of all images is increased. First, we
notice that the pooled estimator performs better than CoRe on test set 1. This can be explained by
9https://www.imagemagick.org
19
Under review as a conference paper at ICLR 2018
(a)	m = 5000, 4-layer CNN
(b)	m = 5000, Inception V3
Method	COREl ： pooled
Method	COREi ： pooled
H
LLl
⅛
Q≤
ωω30ω--≥
50
45
40
35
30
25
20
15
10
5
0
LLl
⅛
Q≤
ωω30ω--≥
50
45
40
35
30
25
20
15
10
5
0
c: 100	c: 250	c: 500	c: 1000
05050505050
544332211
)% NI( ETAR .SSALCSI
Dataset
(C) m = 10000, 4-layer CNN
Tr Te1 Te2 Tr Te1 Te2 Tr Te1 Te2 Tr Te1 Te2
Dataset
(e)	m = 17000, 4-layer CNN
c: 100	c: 250	c: 500	c: 1000
ɪ	求50
H	M 45
_	40
lɪ
Tr Te1 Te2 Tr Te1 Te2 Tr Te1 Te2 Tr Te1 Te2
Dataset
05050505050
544332211
)% NI( ETAR .SSALCSI
H
LLl
⅛
Q≤
ωω30ω--≥
50
45
40
35
30
25
20
15
10
5
0
c: 100	c: 250	c: 500	c: 1000
Dataset
(d) m = 10000, Inception V3
皿
■ ^l
--
--
- -
--
1.
Tr Te1 Te2 Tr Te1 Te2 Tr Te1 Te2 Tr Te1 Te2
Dataset
(f)	m = 17000, Inception V3
c: 100	c: 250	c: 500	c: 1000
50505050
332211
ETAR .SSALCSI
Tr Te1 Te2 Tr Te1 Te2 Tr Te1 Te2 Tr Te1 Te2
Dataset

Figure B.2:	Misclassification rates for the CelebA gender datasets with varying numbers for m and c. The left
column shows results for training a four-layer CNN (cf. Table C.1) end-to-end, the right column shows results
for using Inception V3 features and retraining the softmax layer.
20
Under review as a conference paper at ICLR 2018
(b) Examples of misclassified observations.
y ≡ glasses
PCor (gl.) = 1.00
Ppool(gl.) = 0.21
y ≡ no glasses
PCoe (nogl.) = 0.84
Ppool (no gl.) = 0.13
y ≡ glasses
PMr (gl.) = 0.90
Ppool (gl.) = 0.14
(c) Misclassification rates.
Figure B.3:	a) Examples from the CelebA brightness dataset. The first three images from the left have
y ≡ no glasses; the remaining three images have y ≡ glasses. Connected images are counterfactual examples.
b) Misclassified examples from the test sets. c) Misclassification rates for β = 20 and c = 2000. Results for
different counterfactual settings, β ∈ {5, 10, 20} and c ∈ {200, 5000} can be found in Figure B.5.
the fact that it can exploit the predictive information contained in the brightness of an image while
CoRe is restricted not to do so. Second, we observe that the pooled estimator does not perform well
on test sets 2 and 4 as its learned representation seems to use the image’s brightness as a predictor
for the response which fails when the brightness distribution in the test set differs significantly
from the training set. In contrast, the predictive performance of CoRe is hardly affected by the
changing brightness distributions. Results for β ∈ {5, 10, 20} and c ∈ {200, 5000} can be found in
Figure B.5.
B.2. 1 Counterfactual settings 2 and 3
Above we used the same image to create a counterfactual observation by sampling a different value
for the brightness intervention. A plausible alternative is to use a different image of the same person
as counterfactual. We call this “CF setting 2”. For comparison, we also evaluate using an image of
a different person as counterfactual as a baseline (“CF setting 3”). Examples from the training sets
using CF setting 2 and 3 can be found in Figure B.4.
Results for all counterfactual settings, β ∈ {5, 10, 20} and c ∈ {200, 5000} can be found in Fig-
ure B.5. We see that using counterfactual setting 1 works best since we could explicitly control
that only X⊥ ≡ brightness varies between counterfactual examples. In counterfactual setting 2,
different images of the same person can vary in many factors, making it more challenging to isolate
brightness as the factor to be invariant against. Lastly, we see that even grouping images of different
persons can still help predictive performance to some degree.
C Experimental details and additional results for experiments
INTRODUCED IN §2 AND §5
C.1 CHOOSING THE TUNING PARAMETER λ
An open question is how to set the value of the tuning parameter τ in Eq. (4) or the penalty λ in
the Lagrangian form. Figure C.6 shows the misclassification rates of CoRe on the subsampled
and augmented AwA2 dataset as a function of the penalty λ. We see that performance is not very
sensitive to the choice of λ.
C.2 Grouping photos of the same person: better predictive performance
Here, we show further results for the experiment introduced in §2.1. We vary the number of identi-
ties included in the training data set n ∈ {10, 20, 40, 80, 160}. This results in total sample sizes m
ranging from 321 for n = 10 to 4386 for n = 160, implying that the average number of counter-
factual observations per person varies between 27 and 32. Figure C.7b shows the misclassification
21
Under review as a conference paper at ICLR 2018
Figure B.4: Examples from the CelebA brightness datasets, CounterfaCtual settings 1-3 With β ∈ {5,10, 20}.
In all rows, the first three images from the left have y ≡ no glasses; the remaining three images have y ≡
glasses. ConneCted images are CounterfaCtual examples. In panels (a)-(C), roW 1 shoWs examples from the
training set, roWs 2-4 Contain examples from test sets 2-4, respeCtively. Panels (d)-(i) shoW examples from the
respeCtive training sets.
rates for the test set WhiCh Consists of 5000 examples. We see that CoRe helps prediCtive perfor-
manCe Compared to the estimator WhiCh just pools all images, notably When n is very small. It thus
suCCessfully mitigates the effeCt of potential Confounders arising due to small sample sizes. As n
and m inCrease the performanCe of CORE and the pooled estimator beCome Comparable—the larger
sample sizes ensure that feWer Confounding faCtors are present in the training data and exploited by
the pooled estimator.
C.3 Grouping augmented images by original: more sample efficient
Here, We shoW further results for the experiment introduCed in §2.2. We vary the number of aug-
mented training examples c from 100 to 5000 for n = 10000 and c ∈ {100, 200, 500, 1000} for
n = 1000. The degree of the rotations is sampled uniformly at random from [35, 70]. Figure C.8
shoWs the misClassifiCation rates. Test set 1 Contains rotated digits only, test set2 is the usual MNIST
test set. We see that the misClassifiCation rates of CoRe are alWays loWer on test set 1, shoWing that
it makes data augmentation more effiCient. For n = 1000, it even turns out to be benefiCial for
performanCe on test set 2.
22
Under review as a conference paper at ICLR 2018
(a)	CF setting 1, c = 200
(b)	CF setting 1, c = 2000
Method I I CORE： !pooled
00000
4321
)% NI( ETAR .SSALCSIM
Dataset
(c) CF setting 2, C = 2000
00000
4321
)% NI( ETAR .SSALCSIM
Dataset
00000 00000
4321 4321
)% NI( ETAR .SSALCSIM )% NI( ETAR .SSALCSI
Method □ CORE ■: pooled
Dataset
(d)	CF setting 2, C = 5000
mean: 5	mean: 10	mean: 20
rr∏≡ ∏rr⅛0
∏ IT：
Tr Te1 Te2 Te3 Te4 Tr Te1 Te2 Te3 Te4 Tr Te1 Te2 Te3 Te4
Dataset
(e)	CF setting 3, C = 2000
mean: 5	mean: 10	mean: 20
00000
4321
)% NI( ETAR .SSALCSI
Tr Te1 Te2 Te3 Te4	Tr Te1 Te2 Te3 Te4 Tr Te1 Te2 Te3 Te4
Dataset
(f) CF setting 3, C = 5000
00000
4321
)% NI( ETAR .SSALCSI
Dataset
π
Figure B.5: Misclassification rates for the CelebA brightness datasets, CounterfaCtual settings 1-3 With C ∈
{200, 2000, 5000} and the mean of the exponential distribution β ∈ {5, 10, 20}.
(a) Test set 1.
0000
321
)% NI( ETAR .SSALCSI
3o∞1oo
(％ N_)山,IyQ∙mm410m一 W
10
25 50 75 100 125
Lambda
3o∞1oo
(％ N_)山,IyQ∙mm410m一 W
(c) Test set 3.
10 25 50 75 100 125
Lambda
(d) Test set 4.
3o∞1oo
(％ N_)山,lyfr∙mm<10m-w
Lambda
Figure C.6: Misclassification rates of CORE on the subsampled and augmented AWA2 dataset as a function
of the penalty λ. The outcome does not depend strongly on the chosen value.
23
Under review as a conference paper at ICLR 2018
(b) Misclassification rates.
(a) Training examples, grouped by identity.
n: 10 n: 20 n: 40 n: 80 n: 160
050
)% NI( ETAR .SSALCSI
Dataset
Method
□ core
::pooled
Figure C.7: a) Examples from the subsampled CelebA dataset. In each row, the first three images from the
left have y ≡ no glasses; the remaining three images have y ≡ glasses. Connected images are counterfactual
examples. b) Misclassification rates for different numbers of identities, included in the training data.
(a)	n = 1000
Method CORE「pooled
(b)	n = 10000
Method CORE「pooled
c: 100 c: 200 c: 500 c: 1000
00
)% NI( ETAR .SSALCSI
Dataset
c: 100 c: 200 c: 500 c: 1000 c: 2000 c: 5000
00
)% NI( ETAR .SSALCSI
Dataset
Figure C.8: Data augmentation setting: Misclassification rates for MNIST and S ≡ rotation. In test set 1 all
digits are rotated by a degree randomly sampled from [35, 70]. Test set 2 is the usual MNIST test set.
24
Under review as a conference paper at ICLR 2018
(a) Examples from test sets 1-3.
c: 20	c: 500	c: 2000
(b) Misclassification rates.
000000
54321
)% NI( ETAR .SSALCSI
Tr Te1 Te2 Te3 Tr Te1 Te2 Te3 Tr Te1 Te2 Te3
Dataset
Method
□ core
p : pooled
Figure C.10: a) Examples from the stickmen test set 1 (row 1), test set 2 (row 2) and test sets 3 (row 3). In
each row, the first three images from the left have y ≡ child; the remaining three images have y ≡ adult. Con-
nected images are counterfactual examples. b) Misclassification rates for different numbers of counterfactual
examples.
C.4 Stickmen image-based age classification
I place of observation D ∣
Figure C.9: Data generating process for the stickmen example.
Here, we show further results for the experiment introduced in §5.1. Figure C.10b shows results
for different numbers of counterfactual examples. For c = 20 the misclassification rate of CORE
estimator has a large variance. For c ∈ {50, 500, 2000}, the CORE estimator shows similar results.
Its performance is thus not sensitive to the number of counterfactual examples, once there are suf-
ficiently many counterfactual observations in the training set. The pooled estimator fails to achieve
good predictive performance on test sets 2 and 3 as it seems to use “movement” as a predictor for
“age”.
C.5 Eyeglasses detection: Image quality intervention
Here, we show further results for the experiment introduced in §5.2. Specifically, we consider inter-
ventions of different strengths by varying the mean of the quality intervention in μ ∈ {30,40,50}.
As in §B.2, we use ImageMagick, this time to modify the image quality. In the training set and
in test set 1, We sample the image quality value as qi,j 〜N(μ, σ = 10)and apply the command
convert -quality q_ij input.jpg output.jpg if y% ≡ glasses. If y% ≡ no glasses,
the image is not modified. In test set 2, the above command is applied if yi ≡ no glasses While
images With yi ≡ glasses are not changed. In test set 3 all images are left unchanged and in test set
4 the command is applied to all images, i.e. the quality of all images is reduced.
We run experiments for counterfactual settings 1-3 and for C = 5000. Figure C.11 shows examples
from the respective training and test sets and Figure C.12 shoWs the corresponding misclassification
rates. Again, we observe that counterfactual setting 1 works best while there are only small differ-
ences in predictive performance between counterfactual settings 2 and 3. Interestingly, there is a
25
Under review as a conference paper at ICLR 2018
(b) CF setting 1, μ
(a) CF setting 1, μ
(C) CF setting 1, μ
am®。日寸
IillKQl
BftHIKSI
金史日R恸金
Q国划©日才
l⅞B∣n
(d) CF setting 2, μ = 50
(g) CF setting 3, μ = 50

W
Figure C.11: Examples from the CelebA image quality datasets, CounterfaCtual settings 1-3 With μ ∈
{30, 40, 50}. In all rows, the first three images from the left have y ≡ no glasses; the remaining three images
have y ≡ glasses. ConneCted images are CounterfaCtual examples. In panels (a)-(C), roW 1 shoWs examples
from the training set, roWs 2-4 Contain examples from test sets 2-4, respeCtively. Panels (d)-(i) shoW examples
from the respeCtive training sets.
(a)	CF setting 1
(b)	CF setting 2
(C) CF setting 3
Method CORE pooled
mean: 30	mean: 40
mean: 50
Tx " "H
0000000
7654321
)% NI( ETAR .SSALCSIM
IJ
J:
ænhhhh^
mean: 30
mean: 40
mean: 50
Method CORE pooled
Tr Te1Te2Te3Te4 Tr Te1 Te2 Te3 Te4	Tr Te1 Te2 Te3 Te4
Dataset
0000000
7654321
)% NI( ETAR .SSALCSIM
Method CORE pooled
Tr Te1 Te2 Te3 Te4 Tr Te1 Te2 Te3 Te4 Tr Te1Te2Te3Te4
Dataset
Tr Te1Te2Te3Te4 Tr Te1Te2Te3Te4 Tr Te1Te2Te3Te4
Dataset
i
Figure C.12: MisClassifiCation rates for the CelebA image quality datasets, CounterfaCtual settings 1-3 With
C = 5000 and the mean of the Gaussian distribution μ ∈ {30,40, 50}.
large performance difference between μ = 40 and μ = 50 for the pooled estimator. Possibly, with
μ = 50 the image quality is not sufficiently predictive for the target.
C.6 Elmer the Elephant
The color interventions for the experiment introduced in §5.3 are created as follows. In the train-
ing set, if yi ≡ elephant we apply the following ImageMagick command only for the counterfac-
tual examples convert -modulate 100,0,100 input.jpg output.jpg, producing a
grayscale image. In test set 1, all images are left unchanged. In test set 2, the above command is
applied if yi ≡ horse; if yi ≡ elephant we sample Cij 〜N(μ = 20,σ = 1) and apply convert
-modulate 10 0,100,100-c_ij input.jpg output.jpg to the image. In test set4, the
26
Under review as a conference paper at ICLR 2018

用
Figure C.13: Examples from the subsampled and augmented AwA2 dataset. Row 1 shows examples from the
training set, rows 2-5 show examples from test sets 1-4, respectively.
latter command is applied to all images. It rotates the colors of the image, in a cyclic manner10. In
test set 3, all images are changed to grayscale.
Figure C.14: Data generating process for the Elmer the elephant example.
C.7 Network architectures
We implemented the considered models in TensorFlow (Abadi et al., 2015). The model architectures
used are detailed in Table C.1. CoRe and the pooled estimator thus use the same network archi-
tecture and training procedure; merely the loss function differs by the counterfactual regularization
term. In all experiments we use the Adam optimizer (Kingma & Ba, 2015).
All experimental results are based on training the respective model five times (using the same data)
to assess the variance due to the randomness in the training procedure.
In each epoch of the training, the training data Xi,∙,i = 1,...,n is randomly shuffled, keeping the
counterfactual observations xi,j, j = 1, . . . , mi together to ensure that mini batches will contain
counterfactual observations. In all experiments the mini batch size is set to 120. For small c this
implies that not all mini batches contain counterfactual observations, making the optimization more
challenging.
10For more details, see http://www.imagemagick.org/Usage/color_mods/#color_mods.
27
Under review as a conference paper at ICLR 2018
Dataset	Optimizer		Architecture	
MNIST	Adam	Input CNN	28 × 28 × 1 Conv 5 × 5 × 16, 5 × 5 × 32 (same padding, strides = 2, ReLu activation), fully connected, softmax layer	
Stickmen	Adam	Input CNN	64 × 64 × 1 Conv 5 × 5 × 16, 5 × 5 × 32, 5 × 5 × 64, 5 × 5 × (same padding, strides = 2, leaky ReLu activation), fully connected, softmax layer	128
CelebA	Adam	Input	64 × 48 × 3	
(all experiments using CelebA)		CNN	Conv 5 × 5 × 16, 5 × 5 × 32, 5 × 5 × 64, 5 × 5 × (same padding, strides = 2, leaky ReLu activation), fully connected, softmax layer	128
AwA2	Adam	Input CNN	32 × 32 × 3 Conv 5 × 5 × 16, 5 × 5 × 32, 5 × 5 × 64, 5 × 5 × (same padding, strides = 2, leaky ReLu activation), fully connected, Softmax layer	128
Table C.1: Details of the model architectures used.
28