Under review as a conference paper at ICLR 2018
Understanding GANs: the LQG Setting
Anonymous authors
Paper under double-blind review
Ab stract
Generative Adversarial Networks (GANs) have become a popular method to learn
a probability model from data. Many GAN architectures with different optimiza-
tion metrics have been introduced recently. Instead of proposing yet another ar-
chitecture, this paper aims to provide an understanding of some of the basic issues
surrounding GANs. First, we propose a natural way of specifying the loss func-
tion for GANs by drawing a connection with supervised learning. Second, we
shed light on the generalization peformance of GANs through the analysis of a
simple LQG setting: the generator is linear, the loss function is quadratic and
the data is drawn from a Gaussian distribution. We show that in this setting: 1)
the optimal GAN solution converges to population Principal Component Analy-
sis (PCA) as the number of training samples increases; 2) the number of samples
required scales exponentially with the dimension of the data; 3) the number of
samples scales almost linearly if the discriminator is constrained to be quadratic.
Thus, linear generators and quadratic discriminators provide a good balance for
fast learning.
1	Introduction
Learning a probability model from data is a fundamental problem in statistics and machine learn-
ing. Building off the success of deep learning methods, Generative Adversarial Networks (GANs)
(Goodfellow et al., 2014) have given this age-old problem a face-lift. In contrast to traditional meth-
ods of parameter fitting like maximum likelihood estimation, the GAN approach views the problem
as a game between a generator whose goal is to generate fake samples that are close to the real data
training samples and a discriminator whose goal is to distinguish between the real and fake samples.
The generator and the discriminators are typically implemented by deep neural networks. GANs
have achieved impressive performance in several domains (e.g., (Ledig et al., 2016; Reed et al.,
2016)). Since (Goodfellow et al., 2014), many variations of GANs have been developed, including
f -GAN (Nowozin et al., 2016), MMD-GAN (Dziugaite et al., 2015; Li et al., 2015), WGAN (Ar-
jovsky et al., 2017), improved WGAN (Gulrajani et al., 2017), relaxed WGAN (Guo et al., 2017),
Least-Squares GAN (Mao et al., 2016), Boundary equilibrium GAN (Berthelot et al., 2017), etc.
These GANs use different metrics in the optimization for training the generator and discriminator
networks (Liu et al., 2017).
The game theoretic formulation in GANs can be viewed as the dual of an optimization that mini-
mizes a distance measure between the empirical distributions of the fake and real samples. In the
population limit where there are infinite number of samples, this optimization minimizes the dis-
tance between the generated distribution and the true distribution from which the data is drawn. In
the original GAN framework, this distance measure is the Jenson Shannon divergence. However,
Arjovsky et al (Arjovsky et al., 2017) noted that this distance does not depend on the generated dis-
tribution whenever its dimension is smaller than that of the true distribution. In this typical case, the
Jenson Shannon divergence does not serve as a useful criterion in choosing the appropriate generated
distribution. To resolve this issue, (Arjovsky et al., 2017) proposed the Wasserstein GAN (WGAN)
which uses the first-order Wasserstein distance instead of Jensen-Shannon divergence. This distance
is meaningful even when the dimension of the generated distribution is less than the true distribution.
Nevertheless there are many other distance measures that satisfy this criterion and it is not clear how
to choose among them. This is responsible in part for the fact that there are so many different GAN
architectures. In fact, there is currently some confusion in the literature even on the basic issue of
how to specify the loss function for GANs. For example, while the ”Wasserstein” in Wasserstein
1
Under review as a conference paper at ICLR 2018
laciripme noitalupo
Unconstrained
Discriminator Solution
Quadratic
Discriminator Solution
(empirial PCA)
slow
(Result 2)
fast
(Result 3)
Figure 1: Summary of main results in the LQG setting. The population optimal GAN solution
is PCA when the discriminator is unconstrained and when the discriminator is constrained to be
quadratic. But the convergence to the population optimal is exponentially faster under a quadratic
constraint on the discriminator.
≠
GAN refers to the use of Wasserstein distance in the distance measure between the generated and
true distributions, the ”Least Squares” in Least-Squares GAN (Mao et al., 2016) refers to the use of
squared error in the discriminator optimization objective. These are two totally different types of
objects. The situation with GANs is in contrast to that in supervised learning, where how the loss
function is specified in the formulation is clear and quite universally accepted.
A central issue in any learning problem is generalization: how close a model learnt from a finite
amount of data is to the true distribution. Or, in statistical terms, how fast is the rate of convergence
of the learnt model to the true model as a function of number of samples? Arora et al (Arora et al.,
2017) have recently studied this problem for GANs. They observed that for Wasserstein GAN, if
there are no constraints on the generator or the discriminator, the number of samples required to
converge scales exponentially in the data dimension. They then showed that if the discriminator is
constrained to be in a parametric family, then one can achieve convergence almost linearly in the
number of parameters in that family (Theorem 3.1 in (Arora et al., 2017)). However, the convergence
is no longer measured in the Wasserstein distance but in a new distance measure they defined (the
neural network distance). The result is interesting as it highlights the role of the discriminator in
generalization, but it is somewhat unsatisfactory in that the distance measure needs to be modified
to tailor to the specific constraints on the discriminator. Moreover, the result requires the invention
of (yet) another family of distance measures for GANs.
In this paper, we first argue that there is a natural way to specify the loss function ` for GANs,
in an analogous way as in the supervised learning setup. The resulting optimal GAN minimizes
a generalized loss-function dependent Wasserstein distance between the generated distribution and
the true distribution, and the dual formulation of this generalized Wasserstein distance leads to a
loss-function dependent discriminator architecture. To study the impact of the constraints on the
generator and the discriminator on the generalization performance in this distance measure, we
focus on the case when the true data distribution is Gaussian. In this case, a natural loss function
to consider is quadratic, and a natural class of generators to consider is linear with a given rank
k. In this setting, the optimal GAN minimizes the second-order Wasserstein distance between the
generated distribution and the empirical data distribution among all linear generators of a given rank.
We show the following results:
1	In the population limit as the number of data samples grow, the optimal generated distribu-
tion is the rank k Gaussian distribution retaining exactly the top k principal components of
the true distribution, i.e. GAN performs PCA in the population limit.
2	The number of samples required for convergence in (second-order) Wasserstein distance
however scales exponentially with the dimension of the data distribution.
3	Under a further constraint that the discriminator is quadratic, GAN converges to the same
population-optimal PCA limit, but with the number of samples scaling almost linearly with
2
Under review as a conference paper at ICLR 2018
the dimension. The constrained GAN simply performs empirical PCA, and in the case
when the rank k of the generator is the same as the dimension of the data distribution,
GAN is equivalent to maximum likelihood estimation of the underlying Gaussian model.
These results are summarized in Figure 1. The GAN architecture with a linear generator and a
quadratic discriminator is shown in Figure 4.
(Arora et al., 2017) observed that the number of samples required to generalize for GAN is expo-
nential in the dimension of the data when there are no constraints on either the generator or the
discriminator. (They proved the result for first-order Wasserstein distance but a similar result holds
for second-order Wasserstein distance as well, see Lemma 2 in Section 3.) Result 2 above says that
even constraining the generator drastically to be linear cannot escape this exponential scaling. Result
3 says that this exponential scaling is not due to statistical limitation, but much better inference can
be obtained by constraining the discriminator appropriately. Similar to Theorem 3.1 in (Arora et al.,
2017), Result 3 points to the importance of constraining the discriminator. But there are two key
differences. First, the convergence in Result 3 is with respect to the original (second-order) Wasser-
stein distance, not another distance measure tailored to the constraint on the discriminator. Thus, the
original quadratic loss function is respected. Second, the population limit is the same as the PCA
limit achieved without constraints on the discriminator. Thus, by imposing a discriminator con-
straint, the rate of convergence is drastically improved without sacrificing the limiting performance.
There is no such guarantee in (Arora et al., 2017). Our results also provide concrete evidence that
an appropriate balance between the classes of generators and discriminators, i.e. linear generators
and quadratic discriminators, can provide fast training.
The Linear-Quadratic-Gaussian (LQG) setting, dating back to at least Gauss, has been widely used
across many fields, including statistics, machine learning, control, signal processing and communi-
cation. It has resulted in celebrated successes such as linear regression, the Wiener filter, the Kalman
filter, PCA, etc., and is often used to establish a baseline to understand more complex models. We
believe it serves a similar role here for GANs1. Indeed it allows us to make a clear connection be-
tween GAN and PCA, perhaps the most basic of unsupervised learning methods. Moreover, even
in this simple setting, the generalization issues in GAN are non-trivial, and understanding them in
this setting provides the foundation to tackle more complex data distributions and more complex
generators and discriminators such as deep nets.
The rest of the paper is organized as follows. In Section 2, we discuss a formulation of the GAN
problem for general loss functions. In Section 3, we specialize to the LQG setting and analyze the
generalization performance of GAN. In Section 4, we analyze the performance of GAN under a
quadratic constraint on the discriminator. In Section 5, we present some experimental results.
2	A General Formulation for GANs
Let {yi}in=1 be n observed data points in Rd drawn i.i.d. from the distribution PY . Let QYn be
the empirical distribution of these observed samples. Moreover, let PX be a normal distribution
N(0, Ik). GANs can be viewed as an optimization that minimizes a distance between the observed
empirical distribution QYn and the generated distribution Pg(X). The population GAN optimization
replaces QYn with PY . The question we ask in this section is: what is a natural way of specifying a
loss function ` for GANs and how it determines the distance?
2.1	WGAN Revisited
Let us start with the WGAN optimization (Arjovsky et al., 2017):
inf W1(PY, Pg(X)),	(1)
g(.)∈G
where G is the set of generator functions, and the p-th order Wasserstein distance between distribu-
tions PZ1 and PZ2 is defined as (Villani, 2008)
Wpp(PZ1,PZ2) := inf E [kZ1 -Z2kp],	(2)
PZ1 ,Z2
1The importance of baselines in machine learning was also expressed by Ben Recht (talk at Stanford, Oct 18
2017).
3
Under review as a conference paper at ICLR 2018
where the minimization is over all joint distributions with marginals fixed. Replacing (2) in (1), the
WGAN optimization can be re-written as
or equivalently:
inf
g(.)∈G
inf E[kY -g(X)k].
Pg(X),Y
PiXn,fY g(i.n)∈fGE[kY-g(X)k],
(3)
(4)
where the minimization is over all joint distributions PX,Y with fixed marginals PX and PY .
We now connect (4) to the supervised learning setup. In supervised learning, the joint distribution
PX,Y is fixed and the goal is to learn a relationship between the feature variable represented by
X ∈ Rk, and the target variable represented by Y ∈ Rd, according to the following optimization:
inf E [` (Y, g(X))] ,	(5)
g(.)∈G
where ` is the loss function. Assuming the marginal distribution of X is the same in both optimiza-
tions (4) and (5), We can connect the two optimization problems by setting '(y, y0) = ∣∣y - y0∣∣ in
optimization (5). Note that for every fixed PX,Y , the solution of the supervised learning problem (5)
yields a predictor g which is a feasible solution to the WGAN optimization problem (4). Therefore,
the WGAN optimization (3) can be re-interpreted as solving the easiest such supervised learning
problem, over all possible joint distributions PX,Y with fixed PX and PY .
2.2	From Supervised to Unsupervised Learning
GAN is a solution to an unsupervised learning problem. What we are establishing above is a general
connection between supervised and unsupervised learning problems: a good predictor g learnt in
a supervised learning problem can be used to generate samples of the target variable Y. Hence, to
solve an unsupervised learning problem for Y with distribution PY , one should solve the easiest
supervised learning problem PX,Y with given marginal PY (and PX, the randomness generating
distribution). This is in contrast to the traditional view of the unsupervised learning problem as
observing the feature variable X without the label Y . (Thus in this paper we break with tradition
and use Y to denote data and X as randomness for the generator in stating the GAN problem.)
This connection between supervised and unsupervised learning leads to a natural way of specifying
the loss function in GANs: we simply replace the `2 Euclidean norm in (3) with a general loss
function `:
inf
g(.)∈G
inf E [` (Y, g(X))].
Pg(X),Y
(6)
The inner optimization is the optimal transport problem between distributions of g(X) and Y (Vil-
lani, 2008) with general cost `. This is a linear programming problem for general cost, so there
is always a dual formulation (the Kantorovich dual (Villani, 2008)). The dual formulation can be
interpreted as a generalized discriminator optimization problem for the cost `. (For example, in the
case of ` being the Euclidean norm, we get the WGAN architecture; see Figure 2(a).) Hence, we
propose (6) as a formulation of GANs for general loss functions.
2.3	Quadratic Loss
The most widely used loss function in supervised learning is the quadratic loss: '(y,y0) = ∣∣y - y0∣2
(squared Euclidean norm). Across many fields its use had led to many important discoveries. With
the connection between supervised and unsupervised learning in mind, this loss function should be
a prime choice to consider in GANs as well. This choice of the loss function leads to the quadratic
GAN optimization:
g(i.n)∈fGW22(PY,Pg(X)).
(7)
Since Wasserstein distances are weakly continuous measures in the probability space (Villani, 2008),
similar to WGAN, the optimization of the quadratic GAN is well-defined even if k < d. The dual
4
Under review as a conference paper at ICLR 2018
formulation (discriminator) for W2 is shown in Figure 2(b). Note that in this dual, the discriminator
applies ψ to the real data and the convex conjugate ψ* to the generated (fake) data.
The empirical quadratic GAN optimization can be formulated by replacing PY with the empirical
distribution QYn of the data as follows:
inf W22(QYn, Pg(X)).	(8)
g(.)∈G
Note that while in practice one generates fake samples from X, we will keep the notations simpler
in this paper by assuming we can generate the exact distribution g(X), i.e. we can generate as many
fake samples as we wish. Almost all our results can be extended to the case when we have finite
number of samples from X comparable to the number of samples from Y .
For the rest of the paper, we will focus on the problem (8) for the particular case of Y Gaussian of
dimension d, and g linear of rank k ≤ d. This is the LQG setting for GANs.
3	GANs under the LQG setup
3.1	The Population GAN Optimization
First, we analyze the population GAN optimization under the LQG setup. We have the following
lemma:
Lemma 1 Let S be a k dimensional subspace in Rd. Let Y be a random variable whose support
lies in S. Then, Y*, the optimal solution Ofthe optimization
inf w22(Py,Pγ),	(9)
PY
is the projection of Y to S.
Proof 1 See Appendix B.2.
This Lemma holds even if PY is a non-Gaussian distribution. However, PY* cannot be generated as
g(X) when PX 〜N(0, Ik) and g(.) is restricted to be linear.
Using Lemma 1 and under the LQG setup, we show that the optimal solution for the population
GAN optimization is the same as the PCA solution. PCA is the most standard unsupervised learning
approach (Jolliffe, 2002). PCA computes an optimal linear mapping from Y to Y under the rank
constraint on the covariance matrix of Y (KY). We say Y is the k-PCA solution of Y if KY is
a rank k matrix whose top k eigenvalues and eigenvectors are the same as top k eigenvalues and
eigenvectors of the covariance matrix of Y (KY).
Theorem 1 Let Y 〜N (0, KY) where KY is full-rank. Let X 〜N (0, Ik) where k ≤ d. The
optimal population GAN solution of optimization (7) under linear G is the k-PCA solution of Y.
Proof 2 See Appendix B.3.
Lemma 1 holds if we replace W2 with W1 . However, the conclusion of Theorem 1 is tied to the W2
distance because the PCA optimization also considers the quadratic projection loss.
5
Under review as a conference paper at ICLR 2018
3.2	The Empirical GAN Optimization
In reality, one solves the GAN optimization over the empirical distribution of the data QYn , not
the population distribution PY . Thus, it is important to analyze how close optimal empirical and
population GAN solutions are in a given sample size n. This notion is captured in the generalization
error of the GAN optimization, defined as follows:
Definition 1 (Generalization of GANs) Let n be the number ofobserved SamPlesfom Y. Let g(.)
and g*(.) be the optimal generatorsfor empirical and population GANs respectively. Then,
dG(PY,QY) := W22(Pγ,Pg(X))- W22(PY,Pg*(x)),	(10)
is a random variable representing the excess error of g over g*, evaluated on the true distribution.
dG(PY, QYn ) can be viewed as a distance between PY and QYn which depends on G. To have a proper
generalization property, one needs to have dG(PY, QYn ) → 0 quickly as n → ∞. Before analyzing
the convergence rate of dG(PY, QYn ) for linear G, we characterize this rate for an unconstrained
G . For an unconstrained G, the second term of (10) is zero (this can be seen using a space filling
generator function (Cannon & Thurston, 1987)). Moreover, Pg(X) can be arbitrarily close to QY.
Thus, we have
Lemma 2 If G is unconstrained, we have
dG(PY,QYn) = W22(PY, QYn),	(11)
which goes to zero with high probability with the rate of O(n-2/d).
The approach described for the unconstrained G corresponds to the memorization of the empirical
distribution QYn using the trained model. Note that one can write
_ 2	_ 2 log(n)
n-d = 2	d —.
Thus, to have a small W22 (PY, QYn ), the number of samples n should be exponentially large in d
(Canas & Rosasco, 2012). It is possible that for some distributions PY, the convergence rate of
W22(PY, QYn ) is much faster than O(n-2/d). For example, (Weed & Bach, 2017) shows that if PY
is clusterable (i.e., Y lies in a fixed number of separate balls with fixed radii), then the convergence
of W22 (PY , QYn ) is fast. However, even in that case, one optimal strategy would be to memorize
observed samples, which does not capture what GANs do.
In supervised learning, constraining the predictor tobe from a small family improves generalization.
A natural question is whether constraining the family of generator functions G can improve the
generalization of GANs. In the LQG setting, we are constraining the generators to be linear. To
simplify calculations, we assume that Y 〜N(0, Id) and d = k. Under these assumptions, the GAN
optimization (8) can be re-written as
min W22(QY,N (μ, K)),	(12)
μ,κ
where K is the covariance matrix with the eigen decomposition K = UΣUt. The optimal popu-
lation solution of this optimization is μp°p = 0 and Kpop = I, which provides a zero Wasserstein
loss.
Theorem 2 Let μn and Kn be optimal solutions for optimization (12). Then, μn → 0 with the rate
of O(d∕n) and Tr(∑n) → d with the rate of O(n-2/d).
Proof 3 See Appendix B.4.
It turns out that Tr(∑n), which is a random variable, is strongly concentrated around its expec-
tation. Thus, Theorem 2 indicates that there is a significant bias in GAN’s estimation of the true
distribution which translates to the slow convergence of the generalization error. Note that in the
Wasserstein space, the empirical distribution QYn and the population distribution PY are far from
each other (the distance between them concentrates around n-2/d (Canas & Rosasco, 2012)). Thus,
if there exists another Gaussian distribution within the sphere around QYn with the radius of n-2/d,
6
Under review as a conference paper at ICLR 2018
the Wasserstein-based learning method will converge to the wrong Gaussian distribution. This phe-
nomenon causes a bias in estimating the true distribution.
Theorem 2 considers the regime where k = d. In practice, the dimension of the generated distribu-
tion is often much smaller than that of the true one (i.e., k d). In this case, GAN’s convergence
rate can be increased from O(n-2/d) to O(n-2/k). However, this faster convergence comes at the
expense of the increased bias term of the excess error (the second term of (10)). The trade-off is
favorable if Y is near low rank. Nevertheless even the convergence rate of O(n-2/k) is still slow.
In practice, however, GANs have demonstrated impressive performance. In the next section, we
show that by suitably constraining the GAN optimization, the convergence rate can be improved
exponentially.
4	GANs with Constrained Discriminators
In this section, first we review the min-max (dual) formulation of WGAN (Arjovsky et al., 2017).
Then, we characterize the min-max formulation of the quadratic GAN. Finally, we show that a
properly constrained quadratic GAN achieves the empirical PCA solution, which converges to the
population optimal with an exponentially faster rate of convergence compared to the case when the
discriminator is unconstrained.
Using the Kantorovich duality (Villani, 2008), the first-order Wasserstein distance W1(PY , Pg(X))
can be written as the following optimization (Arjovsky et al., 2017):
Wι(Pγ ,Pg(χ) )= SUp E hψ(Y) - ψ(Y)i,	(13)
ψ()1-Lip	L	」
where the function ψ(.) is restricted to be 1-Lipschitz. This dual formulation of W1 is then used in
optimization (1) to implement WGAN in a min-max architecture similar to the one of the original
GAN (Figure 2). In this architecture, ψ(.) is implemented by deep neural networks.
In a similar way, one can write the second-order Wasserstein distance W22(PY, Pg(X)) as the fol-
lowing optimization (Villani, 2008):
W22(Pγ,Pg(X))= E[kYk2]+ E[kg(X)k2]+2	SUp - E[Ψ(Y)] - E[Ψ*(g(X))],	(14)
ψ(.):ConVeX
where ψ*(y) := SUpv (Vty 一 ψ(v)) is the convex-conjugate of the function ψ(.). Similarly, this
dual formulation of W22 Can be used to implement the quadratiC GAN optimization (7) in a min-maX
architecture which can be interpreted as a game between optimizing two functions g(.) and ψ(.)
(Figure 2).
The following lemma characterizes the optimal solution of optimization (14) (Chernozhukov et al.,
2017):
Lemma 3 Let PY be absolutely continuous whose support contained in a convex set in Rd. For
a fixed g(.) ∈ G, let ψopt be the optimal solution of optimization (14). This solution is unique.
Moreover, we have
5ψopt(Y) d=istg(X),	(15)
where d=ist means matching distributions.
In the LQG setup, since g(X) is Gaussian, 5ψopt is a linear function. Thus, without loss of gen-
erality, ψ(.) in the discriminator optimization can be constrained to ψ(y) = ytAy/2 where A is
positive semidefinite. Therefore, we have
W22(Pγ ,Pg(x)) = E[kY k2] + E[kg(X )k2] + 2 SUp	— E[Ψ(Y)] — E[Ψ*(g(X))]
Ψ(y)=ytAy∕2,A占 0
(16)
=Tr(KY) + Tr(Kg(X)) + SUp — Tr(AKY) — Tr(AtKg(χ)),
A0
where At is the pseudo inverse of the matrix A.
7
Under review as a conference paper at ICLR 2018
TL T	1 . WF	1	♦ F 1	1	. 1	. 1	∙ ∙ 1 1 ∙ . ∙1	Λ-7><K) (-1 ∙ ∙ 1 1
Now let Y be a random variable whose distribution matches the empirical distribution QYn . Similarly
we can write:
W22(Pγ,Pg(X)) = E[kYk2]+ E[kg(X)k2]+2 SUp - E [ψ(Y)] - E[Ψ*(g(X))].	(17)
ψ(.):ConVeX
For WJ(PY, Pg(x)), however, We cannot restrict ψ to convex quadratic functions because Y is a
disCrete Variable while g(X) is Gaussian. Thus, Lemma 3 implies that 5ψopt for (17) Cannot be
linear. Nevertheless, by constraining to quadratic discriminators, we obtain a lower bound:
W22(Pγ, Pg(X)) > E[kY k2]+ E[kg(X )k2] + 2 SUp	- E[ψ(Y)] - E[Ψ*(g(X))]
Ψ(y)=ytAy∕2,A占 0	L 」
(18)
Tr(Kγ) + Tr(Kg(X)) + SUp - Tr(AKY) - Tr(AtKg(X))
A0
W22(PZ, Pg(X)),
where KY = E[YYt] (the empirical covariance matrix) and Z 〜N(0, KY) 2. Therefore, the
empirical constrained quadratic GAN solves the following optimization:
inf
g(.)∈G
W22(PZ,Pg(X)).
(19)
Using Theorem 1, the optimal g(X) to this problem is the empirical PCA solution, i.e. keeping the
top k principal components of the empirical covariance matrix.
Theorem 3 Under the LQG setup, the solution of the empirical constrained quadratic GAN opti-
mization is equivalent to the empirical PCA.
Consider the case where d = k (the case k < d is similar). The second term in the generalization
distance dG(PY, QYn ) (10) is zero. Therefore, we have
dg (PY,QY) = W22 (PY, Pz) = W22(N(。, KY),N(0, KY)) .	(20)
The W22 distance between two Gaussians depends only on the covariance matrices. More specifi-
cally:
W22 (N(0, Ky),N(0, KY)) = Tr(KY) + Tr(KY) - 2Tr ((KYZ2KYKY/2) 1/2) .	(21)
Hence, the convergence of this quantity only depends on the convergence of the empirical covariance
to the population covariance, together with smoothness property of this function of the covariance
matrices. The convergence has been established to be at a quick rate of O(,d/n) (RiPPl et al.,
2016).
Finally, note that if one has a finite number of X samples (replacing Pg(X) with Qg(X)), the con-
strained quadratic GAN would still have a fast convergence rate because only the empirical co-
variance matrix of g(X ) plays a role in its optimization which converges quickly to the population
covariance matrix.
5	Experimental Results
For experiments, we generate n i.i.d. samples from Pγ 〜N (0, Id), represented as QY. We then
fit a d dimensional Gaussian distribution N(μ, K) to QY using two methods: Maximum Likeli-
hood (ML) estimation, which computes the sample mean and the empirical covariance; and WGAN
(Arjovsky et al., 2017) with an affine generator function. Note that according to Theorem 3, ML
2Note that by considering ψ(y) = ytAy∕2 + hty where A ± 0, we would have obtained Z 〜N (μγ, KY)
where μγ is the sample mean. For simplicity, we ignore the affine terms.
8
Under review as a conference paper at ICLR 2018
Figure 3: Generalization errors of constrained quadratic GAN (ML) and WGAN under the LQG
setup.
is equivalent to the constrained quadratic GAN (19). Moreover, note that the WGAN implementa-
tion uses W1 and not W2 in its optimization. Although analyzing GANs with W2 is more tractable
than that of W1 , in practice we do not expect a significant difference between their performance.
Considering this and owing to the lack of an implementation of GANs with W2, we perform nu-
merical experiments using the WGAN implementation. Details of the experiments can be found in
Appendix A.
Let μ and 宜 be the estimated mean and the covariance. For evaluation, We compute
kμk2 + ki-宜 1/2kF,
(22)
which is the W22 distance between N (0, Id) and N (μ,宜)
(see Lemma 4 in Appendix B).
Figure 3 demonstrates the estimation error of the ML (constrained quadratic GAN) and WGAN
methods for d = 5 and d = 10 and in different sample sizes. These figures are consistent with The-
orem 2 and results of Section 3.2 which suggest that GAN’s convergence can be slow owing to a bias
in its optimal empirical solution with respect to the population one. Moreover, this figure shows that
the convergence of the constrained quadratic GAN (ML) is fast. Finally, in our experimental results
of Figure 3, one should take into the consideration practical issues of the WGAN implementation
such as the use of the stochastic gradient descent, convergence to bad locals, etc.
6	Discussions
From a broader perspective, the problem we addressed in this paper is that of finding a good gen-
erative model for data coming from a Gaussian ground-truth, Y 〜N(μ, K). This is an age-old
problem in statistics, and the baseline solution is using maximum likelihood estimation: one uses
the data to estimate the mean and covariance matrix of the Gaussian distribution, i.e. the empirical
mean μ and empirical covariance matrix K, and obtain a generative model Y 〜N(μ, K). And
when there is a desire to do dimensionality reduction, one can have a low-rank generative Gaussian
model retaining the top k principal components of K. This is the empirical PCA solution.
What we have shown in this paper is that there is a natural GAN architecture that can accomplish
exactly these tasks (Figure 4).
While this is certainly a complicated way of performing maximum likelihood Gaussian estimation
and PCA, we believe the result is interesting in several ways. First, it is not at all obvious that there
is a natural GAN architecture that can accomplish this task. Since Gaussian modeling is a basic task,
this is a good sanity check on GANs. Second, arriving at this GAN architecture requires us to make
several advances in our understanding of GANs. We needed to find a general way to specify the loss
function for GANs, and then specialize to the quadratic loss function for the Gaussian problem at
hand. This led to the use of the second-order Wasserstein distance for the generator optimization,
and to a general GAN architecture from the dual formulation of this optimization. We then needed
to find a proper way to constrain the class of generators and the class of discriminators in a balanced
9
Under review as a conference paper at ICLR 2018
Discriminator
Figure 4: The GAN architecture that achieves maximum likelihood estimation for the zero-mean
Gaussian model: a linear generator and a quadratic discriminator. On the training data, the generator
minimizes over G and the adversary maximizes over A.
way to achieve fast generalization. Indeed our goal was not to recover the maximum likelihood
solution but to overcome the slow generalization when there are no constraints on the generator and
the discriminator. That the final architecture with balanced generators and discriminators is also the
maximum likelihood solution gives this story a satisfactory ending.
References
Martin Arjovsky, SoUmith Chintala, and Leon Bottou. Wasserstein gan. arXiv preprint
arXiv:1701.07875, 2017.
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium
in generative adversarial nets (gans). arXiv preprint arXiv:1703.00573, 2017.
David Berthelot, Tom Schumm, and Luke Metz. Began: Boundary equilibrium generative adversar-
ial networks. arXiv preprint arXiv:1703.10717, 2017.
Guillermo Canas and Lorenzo Rosasco. Learning probability measures with respect to optimal
transport metrics. In Advances in Neural Information Processing Systems, pp. 2492-2500, 2012.
James W Cannon and William P Thurston. Group invariant peano curves. 1987.
Victor Chernozhukov, Alfred Galichon, Marc Hallin, Marc Henry, et al. Monge-kantorovich depth,
quantiles, ranks and signs. The Annals of Statistics, 45(1):223-256, 2017.
Gintare Karolina Dziugaite, Daniel M Roy, and Zoubin Ghahramani. Training generative neural
networks via maximum mean discrepancy optimization. arXiv preprint arXiv:1505.03906, 2015.
Clark R Givens, Rae Michael Shortt, et al. A class of wasserstein metrics for probability distribu-
tions. The Michigan Mathematical Journal, 31(2):231-240, 1984.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Im-
proved training of wasserstein gans. arXiv preprint arXiv:1704.00028, 2017.
Xin Guo, Johnny Hong, Tianyi Lin, and Nan Yang. Relaxed wasserstein with applications to gans.
arXiv preprint arXiv:1705.07164, 2017.
Ian Jolliffe. Principal Component Analysis. Wiley Online Library, 2002.
Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro
Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single
image super-resolution using a generative adversarial network. arXiv preprint arXiv:1609.04802,
2016.
10
Under review as a conference paper at ICLR 2018
Yujia Li, Kevin Swersky, and Rich Zemel. Generative moment matching networks. In Proceedings
ofthe 32nd International Conference on Machine Learning (ICML-15), pp.1718-1727, 2015.
Shuang Liu, Olivier Bousquet, and Kamalika Chaudhuri. Approximation and convergence proper-
ties of generative adversarial learning. arXiv preprint arXiv:1705.08991, 2017.
Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, and Zhen Wang. Multi-class generative
adversarial networks with the l2 loss function. arXiv preprint arXiv:1611.04076, 2016.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural sam-
plers using variational divergence minimization. In Advances in Neural Information Processing
Systems, pp. 271-279, 2016.
Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee.
Generative adversarial text to image synthesis. arXiv preprint arXiv:1605.05396, 2016.
Thomas Rippl, Axel Munk, and Anja Sturm. Limit laws of the empirical wasserstein distance:
Gaussian distributions. Journal of Multivariate Analysis, 151:90-109, 2016.
Cedric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media,
2008.
Jonathan Weed and Francis Bach. Sharp asymptotic and finite-sample rates of convergence of em-
pirical measures in wasserstein distance. arXiv preprint arXiv:1707.00087, 2017.
11
Under review as a conference paper at ICLR 2018
Appendix
A Details of Experiments
The WGAN is implemented in pytorch. Denote fully connected layer with the input dimension din
and the output dimension dout as FC(din, dout). The generator can be represented as FC(d, d); and
the discriminator can be represented as FC(d, nf) -ReLU-FC(nf, nf) -ReLU-FC(nf, nf)-
ReLU -FC(nf, 1). The model is trained 100k iterations with batch size 128 with Adam optimizer.
The learning rate is set to 2 × 10-4. As for hyper parameters, nf is set to 128, the ratio of iterations
between discriminator and generator is set to 5, and the weight clipping threshold is set to -0.02
and 0.02. Both ML and WGAN are repeated 10 times for each setting, and the mean and standard
deviation is calculated and plotted (68.3% confidence interval).
B Proofs
B.1 Notation and Preliminary Lemmas
For matrices we use bold-faced upper case letters, for vectors we use bold-faced lower case letters,
and for scalars we use regular lower case letters. For example, X represents a matrix, x represents a
vector, and x represents a scalar number. In is the identity matrix of size n × n. 1n1,n2 is the all one
matrix of size n1 × n2. When no confusion arises, we drop the subscripts. 1{x = y} is the indicator
function which is equal to one if x = y, otherwise it is zero. Tr(X) and Xt represent the trace and
the transpose of the matrix X, respectively. kxk2 = xtx is the second norm of the vector x. When
no confusion arises, we drop the subscript. kXk is the operator (spectral) norm of the matrix X.
< x, y > is the inner product between vectors X and y. At is the pseudo inverse of the matrix A.
The eigen decomposition of the matrix A ∈ Rn×n is denoted by A = Pin=1 λi(A)ui(A)ui(A)t,
where λi(A) is the i-th largest eigenvalue of the matrix A corresponding to the eigenvector ui(A).
We have λι(A) ≥ λ2(A) ≥ …. N(μ, K) is the Gaussian distribution with mean μ and the
covariance K. KL(PX, PY ) is the Kullback Leibler divergence between two distributions PX and
PY. O(d) is the same as O(dlog(d)).
Lemma 4 Let Y 〜N(0, KY) and Y 〜N(0, KY). Then,
W22(Py, PY) = Tr(KY) + Tr(KY) - 2Tr
KY KY
(23)
Tr(KY) + Tr(KY) - 2Tr
Proof 4 See reference (Givens et al., 1984).
B.2 Proof of Lemma 1
Let Y = YS0 + YS where YS represents the projection of Y onto the subspace S. Since
E [kY - Yk2i = E [kYS0 k2] + E hkYs — Yk2i	(24)
choosing Y = YS achieves the minimum of optimization (9).
B.3 Proof of Theorem 1
τ, CF	CIF	Γ∙	17 1	W 1 ∙	A	1 ∙	. T	Λ ∙ ɪ' W ♦	. ∙	1
Let S be a fixed subspace of rank k where Y lies on. According to Lemma 1, ifY is unconstrained,
the optimal Y* is
the projection of Y onto S (i.e., Y*
Ys). Moreover, since Y is Gaussian, Y
is also Gaussian. Therefore, there exists a linear g(.) such that PY* = Pg(X)where X 〜N(0, I).
Thus, the problem simplifies to choosing a subspace where E kYS k2 is maximized, which is the
same as the PCA optimization.
12
Under review as a conference paper at ICLR 2018
B.4 Proof of Theorem 2
Let yι,...,yn be n i.i.d. samples of PY. Let μ be the sample mean. Since PY is absolutely Contin-
uous, the optimal W2 coupling between QYn and PY is deterministic (Villani, 2008). Thus, every
Point y is coupled with an optimal transport vornoi region with the centroid Cyμ,K). Therefore, We
have
W22(N (μ, K) ,QY)	(25)
1N	2N
=kμk2 + Tr(E) + N X IMk2 - N Xytcyμ,K)
i=1	i=1
NN
=kμk2 + Tr(Σ) + N X kyik2 - N Xyt (u∑⑺Utcy0,1) + μ)
i=1	i=1
1N
=kμk2 - 2μμ + Tr(∑) + N E IMk2 - 2Tr(UE1/2UtA)
i=1
where
1N
A := N XCy,I)yt.	(26)
i=1
The first step in (25) follows from the definition of W2, the second step follows from the optimal
coupling between N(μ, K) and N(0, I), and the third step follows from the matrix trace equalities.
Therefore,
5μW2(N (μ, ∑), QY) = 2μ - 2μ,	(27)
which leads to μ↑b = μ. Moreover, each component of the sample mean is distributed according to
N(0,1/n). Thus, kμNk2 〜Xd/n which goes to zero with the rate of O(d∕n).
Let σi2 be the i-th diagonal element of Σ. Moreover, define B = UtAU. Therefore, we have
5σi W22(N (μ, ∑), QY) = 2σi- 2bi,i,	(28)
where b i is the i-th diagonal element of the matrix B. Thus, σ* = bi i and Tr(Σ*) = Tr(B)=
Tr(A).
Furthermore, we have
1N
W22(Py,QY) = d + N Ekyik2- 2Tr(A),	(29)
N i=1
which goes to zero with the rate of O(n-2/d) (Canas & Rosasco, 2012). Since N PN=Ikyi k2 goes
to d with the rate of O( ,d/n) (because it has a χ-squared distribution), Tr(A) goes to d with a rate
of O(n-2/d). Combining this result with Tr(Σ*) = Tr(A) completes the proof.
13