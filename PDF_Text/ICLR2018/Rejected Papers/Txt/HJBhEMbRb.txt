Under review as a conference paper at ICLR 2018
A Spectral Approach to Generalization and
Optimization in Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
The recent success of deep neural networks stems from their ability to generalize
well on real data; however, Zhang et al. (Zhang et al., 2016) have observed that
neural networks can easily overfit random labels. This observation demonstrates
that with the existing theory, we cannot adequately explain why gradient methods
can find generalizable solutions for neural networks. In this work, we use a Fourier-
based approach to study the generalization properties of gradient-based methods
over 2-layer neural networks with sinusoidal activation functions. We prove that
if the underlying distribution of data has nice spectral properties such as bandlim-
itedness, then the gradient descent method will converge to generalizable local
minima. We also establish a Fourier-based generalization bound for bandlimited
spaces, which generalizes to other activation functions. Our generalization bound
motivates a grouped version of path norms for measuring the complexity of 2-layer
neural networks with ReLU activation functions. We demonstrate numerically that
regularization of this group path norm results in neural network solutions that can
fit true labels without losing test accuracy while not overfitting random labels.
1	Introduction
Deep neural networks (DNNs) have achieved state-of-the-art performance on a wide array of diverse
tasks (LeCun et al., 2015). A given DNN architecture represents a highly rich space of hypotheses.
However, numerous empirical results have demonstrated that a simple stochastic gradient descent
(SGD) learner can efficiently search over this space to find a solution that achieves high performance
on both training and test data. Despite many successful applications of DNNs to practical tasks such
as computer vision (Krizhevsky et al., 2012), natural language processing (Collobert & Weston, 2008)
and speech recognition (Hinton et al., 2012), our basic understanding of the factors that drive DNN
generalization is still lacking.
Addressing generalization for DNNs is hard for two fundamental reasons: 1) Empirical risk mini-
mization for neural networks is a non-convex optimization problem with possibly many local minima,
and 2) Two different local minima with the same training performance can achieve significantly
different performance on test data. For these reasons, the neural network optimization method
plays an important role in the generalizability of the local minima found. For example, SGD has
been empirically shown to outperform large-batch gradient descent (Keskar et al., 2016). Also, the
performance of gradient methods can be improved upon by incorporating the geometry of observed
data (Duchi et al., 2011; Neyshabur et al., 2015a).
For DNNs, however, a good optimization method is not sufficient for guaranteeing good generalization.
Zhang et al. (Zhang et al., 2016) empirically demonstrate that a neural network trained by SGD can
easily overfit random labels on the CIFAR-10 (Krizhevsky & Hinton, 2009) data. Yet, the same neural
network fitted by the same SGD algorithm achieves good generalization performance for the original
CIFAR-10 labels. This observation challenges the ability of traditional learning theory to explain why
SGD learns generalizable hypotheses over neural networks. To shed light on this phenomenon, two
recent works have developed generalization bounds and complexity measures for neural networks
which can distinguish the local minima found for true and random labels. (Bartlett et al., 2017) proves
a margin-based generalization bound and shows how it correlates with the generalization risk of DNNs
when fitting true and random labels. (Neyshabur et al., 2017) explores different complexity scores
for DNNs and how they behave differently for true and random labels. The complexity measures
investigated in these works can effectively distinguish generalizable from poorly-generalizable local
1
Under review as a conference paper at ICLR 2018
Figure 1: (a) A 2-layer neural network with activation function φ, (b) Training and test accuracy
on CIFAR10 with true and random labels on a 2-layer neural network with 512 ReLU hidden units,
regularized with an additive penalty: (b1) no penalty, (b2) '2-norm, (b3) χ2-group path norm, (b4)
'ι-path norm. The χ2-group path norm and 'ι-path norm were successful to close the generalization
gap for both true and random labels.
b1.	Noreg.
250 500 750 1000
epoch
b3. Group path norm reg. (λ = 0.2)
250 500 750 1000
epoch
b2. L2 reg.(A = 0.01)
250 500	750 1000
epoch
b4. Ll path norm reg. (Λ = 0.01)
minima. They do not explain, however, why SGD converges to generalizable local minima when
there exist poorly-generalizable local minima which can also perfectly fit the training set.
To approach this question, one needs to understand the key characteristic of CIFAR-10’s original
labeling which differentiates it from random labels and how it is exploited by SGD to achieve good
generalization performance. In this work, we approach this problem in the Fourier domain where
non-random labeling schemes behave completely differently from random labeling schemes. While
signals recoverable from few measurements possess nice spectral properties such as bandlimitedness,
fully random stochastic processes are not bandlimited and not recoverable from any finite number of
measurements.
Using spectral analysis, we focus on characterizing spectral properties of an underlying distribution
which can be exploited by gradient-based methods to converge to generalizable local minima. We
address this problem for 2-layer neural networks (see Figure 1a) with sinusoidal activation functions,
where we show that if the underlying labeling scheme has limited bandwidth and Fourier '「norm
(i.e. "nice" Fourier properties), we expect a gradient-based method to achieve good generalization
performance. To arrive at this result, we first develop a Fourier-based generalization bound for
2-layer neural networks in terms of bandwidth and Fourier '1-norm. Next, we prove that the local
minima found by the gradient descent method over a 2-layer neural network with sine activation have
bandwidth and Fourier '1-norm bounded in terms of the spectral properties of the underlying labeling
scheme.
As a byproduct of our Fourier analysis, we derive generalization bounds for 2-layer neural networks
with general activation functions. For bandlimited activation functions with finite Fourier '1-norm,
such as sinusoidal or Gaussian activation1, our bound is tighter than the generalization bound obtained
using only the Lipschitz constant of the activation function. For ReLU-type activation functions, our
generalization bound is comparable to Lipschitz-based bounds; however, it leads to a grouped version
of the path norms developed in (Neyshabur et al., 2015a). We therefore call this capacity norm group
path norm which can be used as an additive penalty to regularize 2-layer neural networks with ReLU
activation. Our numerical experiments suggest that the generalization gap can be effectively tightened
by regularizing the group path norm. Figure 1b demonstrates how group path norm regularization
can help close the generalization gap for both true and random labels.
1Since a Gaussian function’s Fourier transform can be arbitrarily well approximated by bandlimited functions
in the `1 -norm sense, the results shown in this work are applicable to Gaussian activation.
2
Under review as a conference paper at ICLR 2018
2	Related work
Generalization has been a topic of central interest in statistical learning theory (Vapnik, 1999; Shalev-
Shwartz & Ben-David, 2014). Generalization bounds have been derived using the stability of a
learning algorithm (Bousquet & Elisseeff, 2002) and various complexity measures of a function space
such as VC-dimension (Vapnik, 2013) and Rademacher complexity (Bartlett & Mendelson, 2002).
(Hardt et al., 2015) develops a stability-based generalization result for SGD as the learning algorithm,
which holds for both convex and non-convex loss functions.
We note that spectral analysis has provided a powerful framework for studying neural networks.
(Barron, 1993) uses a Fourier-based approach to prove the universal approximation theorem for
2-layer neural networks. Similarly, (Lee et al., 2017) applies Fourier analysis to extend Barron’s
result to a general feedforward neural network. (Rippel et al., 2015) uses a spectral approach to model
and analyze convolutional neural networks (CNNs) and introduce the spectral pooling scheme for
CNNs. Also, our Fourier-based approach to analyze SGD’s performance for 2-layer neural networks
follows the same prinicples as the analysis performed in (Shamir, 2016) to prove the hardness of
fitting periodic labeling schemes via gradient-based methods. We should note that in this work we
use only periodic activation functions and not periodic labeling schemes. Therefore, the hardness
result shown in (Shamir, 2016) does not affect our numerical experiments.
In general, theoretical studies of neural networks can be categorized into three main categories: 1)
Approximation: Neural networks have been proven to be powerful in expressing a very rich class of
functions (Cybenko, 1989) and in general deeper networks need fewer neurons to express the same
class of functions (Eldan & Shamir, 2016; Liang & Srikant, 2016). 2) Generalization: Tight bounds
have been shown on the VC dimesnion of feedforward neural networks (Anthony & Bartlett, 2009;
Harvey et al., 2017). Also, norm-based Rademacher complexity bounds have been developed at
(Bartlett & Mendelson, 2002; Neyshabur et al., 2015b). Sharpness of local minima and its connection
to their generalizibility have been the focus of several recent works (Keskar et al., 2016; Dinh et al.,
2017; Neyshabur et al., 2017) 3) Optimization: theoretical studies have shown both positive (Andoni
et al., 2014; Daniely, 2017) and negative (Shalev-Shwartz et al., 2017) results about the performance
of gradient-based methods in training neural networks.
3	Preliminaries
3.1	Supervised learning and generalization
Suppose that we are given n samples (xi , yi)in=1 drawn i.i.d. from a population distribution PX,Y .
Here X denotes the random vector of features and Y denotes the target variable. Using these n
samples, the goal of a supervised learner is to find a prediction rule f from a function space F which
can predict Y for an unseen test sample X. Therefore, given loss function ` the supervised learner
wants to find f * ∈ F minimizing the population risk, defined as E ['(f (X),Y)] averaged under the
population distribution.
However, the supervised learner does not know the population distribution PX,Y and has only
access to the n training samples. The supervised learner can minimize the empirical risk, defined
as 1/n Pin=1 ` f(xi), yi and find fnemp. Since we only observe a limited number of samples, the
empirical risk would be different from the population risk. The generalization risk, defined for f ∈ F
as E['(f (X),Y)] - * Pn=ι '(f (xi), yi), is the difference among the population risk and empirical
risk for f. Studying the behavior of fnemp’s generalization risk for different function spaces and
learning algorithms is a topic of central interest in statistical learning theory.
3.2	Fourier Transform and Bandlimited Functions
Consider a real-valued function f : Rk → R. The Fourier transform of this function, which we
denote by f, is defined as
fb(ξ) =	f(x) exp
-2πiξT x
dx.
(1)
Some important examples of Fourier transform are:
3
Under review as a conference paper at ICLR 2018
• Sinusoidal function: f (x) = exp(2πiωT x), then fb(ξ) = δ(ξ - ω) where δ denotes the
Dirac delta function, which also implies
-f (x) = cos(2πωTx), then f (ξ) = 1/2 [ δ(ξ + ω) + δ(ξ — ω)].
-f (x) = sin(2πωTx), then f (ξ) = i/2 [ δ(ξ + ω) — δ(ξ — ω)].
•	Gaussianfunction: f(x) = (√2∏σ)k exp(-∣∣xk2∕2σ2), then f(ξ) = exp(—σ2kξk2∕2).
Thus, the Fourier transform of a Gaussian function preserves the Gaussian shape.
A function f is called B-bandlimited if f(ξ) = 0 for every ξ where kξk2 > B. The smallest B for
which this property holds is called the bandwidth of f. We use B(f) to denote the bandwidth of
function f. We also use ∣∣f kι to denote the 'ι-norm of f s Fourier transform,
ʌ f ʌ
kfkι = J lf(ξ)ldξ
(2)
which We call the Fourier 'ι -norm of f. Fourier 'ι-norm can be interpreted as the absolute volume
under f S Fourier transform, and is an approximate measure of f s sparsity. Fourier 'ι-norm is both
scale and shift invariant, i.e. if we define g(x) = f(Wx + b) for a real-valued f and W ∈ Rr×k
and b ∈ Rr for some r ≤ k, then ∣gb∣1 = ∣f∣1. Some other useful properties of Fourier transform
are:
•	Synthesis: f (x) = R f(ξ) exp(2πiξτx) dξ, which also implies ∣∣f ∣∣ι = f (0) if f is real
and non-negative.
•	Shift: fbb(ξ) = exp(2πibT ξ)fb(ξ) wherefb(x) := f(x—b), which implies ∣fcb∣1 = ∣fb∣1
andB(fb)=B(f).
•	Derivative: Vf (ξ) = 2∏i f (ξ) ξ, where Vf denotes the gradient of f.
•	Isometry: / f (x)g(x) dx = / f (ξ)g(ξ) dξ where Z denotes the complex conjugate of z.
•	Convolution: fg = f ? gb where ? denotes the convolution operator i.e. f ? gb(ξ) :=
R fb(η)gb(ξ — η) dη. Therefore, B(f g) ≤ B(f) + B(g) and ∣fcg∣1 ≤ ∣fb∣1∣gb∣1.
4 A Fourier-based Generalization B ound
Consider a supervised learning task with n training samples xi , yi in=1 and function space F. We
are interested in uniform convergence bounds on the generalization risk. A standard approach to
bound the generalization risk is based on the notion of Rademacher complexity. Given samples
xi, yi in=1, the empirical Rademacher complexity ofF is defined as
1n
RnmP(F):= Eσ sup — Vσif(xi)	(3)
f∈F n i=1
where σ∕s are i.i.d. random variables uniformly distributed over {-1, +1}. In fact, the Rademacher
complexity of F measures how well F can fit some random labels over input xi ’s. The following
result shows how to bound the generalization risk over F through its Rademacher complexity.
Theorem 1 (Bartlett & Mendelson (2002)). Consider a ρ-Lipschitz loss function `(f (x), y) bounded
as |'(z, y)| ≤ c. Then, for any δ > 0, with probability at least 1 — δ
∀ f ∈F :	E[ '(f(X),Y) ] — n XX '(f(xi),yi) ≤ 2ρRnmp(F ) + 4c J2l*4∕δ).	(4)
i=1
Since the Rademacher complexity of norm-bounded linear functions can be appropriately bounded
(Kakade et al., 2009), one can effectively apply Theorem 1 to bound generalization risk over norm-
bounded linear functions. To use Theorem 1 in the Fourier domain, here we provide a Rademacher
complexity bound for bandlimited functions with bounded Fourier `1 -norm. We apply the following
Rademacher complexity bound to bound generalization risk for 2-layer neural networks in Section 5,
and also to analyze the performance of gradient-based methods with sinusoidal activation functions
in Section 6.
4
Under review as a conference paper at ICLR 2018
Theorem 2.	Consider function space F = { f : Rk → R s.t. B(f) ≤ B, kf∖∖ι ≤ V} of B -
bandlimited functions with V-bounded Fourier '1 -norm. Then, the empirical Rademacher complexity
for samples (xi, yi)in=1 is bounded as

Renmp(F) ≤ V
4k log 64 nB maxi ∖xi ∖2
n
(5)
Proof. We defer the proof to the Appendix.
□
Corollary 1. Assume that ∖X∖2 ≤ C holds almost surely and the loss function ` is ρ-Lipschitz.
Then, for any δ > 0 with probability at least 1 - δ the following generalization bound holds for any
B -bandlimited function f with V-bounded Fourier '1 -norm:
E['(f(X),Y) ] - n XX'(f(xi),yi) ≤ O(PVJk^gnBC/δ)).	(6)
Proof. The Corrollary is a direct result of applying the bound in Theorem 2 to Theorem 1.	□
The above corollary bounds the generalization risk uniformly over all bandlimited f’s such that
S/ 八 ，7~»	1 Il Wl ，T7	,	1.1	1	1,	, Cl	1,1
B(f) ≤ B and ∖f∖1 ≤ V. Next, we apply the above results to 2-layer neural networks.
5 Application of Theorem 2 to 2 - layer Neural Networks
Consider a 2-layer neural network including d neurons with activation function φ in the hidden layer
(See Figure 1a). The output of this neural network is
fa,W,b(x) = aT φ(Wx + b).	(7)
If φ has bounded bandwidth and Fourier '1 -norm, we can apply Theorem 2 to bound the Rademacher
complexity and hence generalization risk over the 2-layer neural network. Here, we use ∖W∖2,∞ to
denote the maximum '2-norm ∖wi∖2 among all rows ofW.
Corollary2. Let	Fφ	=	f (x)	= aT φ(Wx + b)	:	∖W∖2,∞ ≤ W,	∖a∖1 ≤ A} be the class
of 2-layer neural networks where B(φ) = B and ∖φ∖1 = V. Then, the empirical Rademacher
complexity of Fφ for samples (xi, yi)in=1 is bounded as follows
Rnmp(Fφ) ≤ O(AVjkSWmax⅞≡).
(8)
Proof. We defer the proof to the Appendix.
□
Notice that for bandlimited activation functions with bounded Fourier '1 -norm, the above generaliza-
tion bound is increasing logarithmically with ∖W∖2,∞. For example, this result holds for sinusoidal
activation φ(x) = sin(2πx) where ∖φ∖1 = 1, B(φ) = 1. On the other hand, the existing Rademacher
complexity bounds which use only the Lipschitz constant of the activation function are linear in W’s
norm (Bartlett & Mendelson, 2002). Therefore, by exploiting the spectral properties of φ, Corollary
2 results in a tighter generalization bound than the bounds using only the Lipschitz constant of φ.
However, an unbounded function such as ReLU φ(x) = max(x, 0) has an infinite Fourier '1-norm.
Therefore, Corollary 2 does not directly apply to these functions. The following theorem uses a
boundedness assumption on input X to apply Theorem 2 to ReLU-type activation functions. Although
the following bound is growing faster than logarithmically with W’s norm, it introduces new capacity
norms for 2-layer ReLU-based networks.
Theorem 3.	Suppose that φα(x) = max{x, αx} where α ∈ [0, 1] is an arbitrary constant. Consider
the pair of dual norms (∖ ∙ ∣∣p, ∖ ∙ ∣∣q) where 1 ≤ p,q ≤ ∞ and 1/p + 1/q = 1. Assume that
IIXiIlp ≤ C holdsforall Xi ,s. Then,for Fφa = {fa,w(X)= aτφα(Wx): pd=ι ∣ai∣∣Wikq ≤ V}
Rnlp(FΦa) ≤ OW^klognkC
(9)
5
Under review as a conference paper at ICLR 2018
Proof. We relegate the proof to the Appendix.	□
The above bound uses the complexity score Pid=1 |ai|kwi kq for each fa,W(x) = aTφα(Wx). We
can rewrite this complexity score in the following way, which is an 'ι,q-group norm on the product
of weights for each path from the input nodes to the output node of the 2-layer neural network,
Xq (fa,w) = X(X(|ai||wi,j |)q)	.	(IO)
i=1 j=1
Here wi,j denotes the weight on the link from the jth node of the input layer to the ith node of
the hidden layer. Based on the path-norm function defined at (Neyshabur et al., 2015a), we call
Xq (fa,w) the group path norm. For q = 1, χι-group path norm leads to the 'ι-path norm for 2-layer
neural networks. We can use group path norms as an additive regularization penalty to learn over
2-layer neural networks. In our numerical experiments, we test the performance of X2-group path
norm and `1 -path norm regularization to control the generalization risk over 2-layer neural networks.
6	Fourier Analysis of Gradient-based methods for 2-layer Neural
Networks with Sine Activation
In this section, we apply Fourier analysis for a 2-layer neural network with sinusoidal activation. We
aim to understand the connection between generalizibility of local minima found by gradient-based
methods and spectral properties of the population distribution PX,Y . As a simplifying assumption,
let’s assume that target variable Y is a deterministic function Y (x) of input X, which we call the
labeling scheme. In our analysis, we consider the squared-error loss '(y, y0) = (y - y0)2.
We specifically ask this question: how can spectral properties of labeling scheme Y (x) and population
density function PX(x) affect the generalization performance of a gradient-based method? To address
this question, we use a similar strategy to the analysis performed in (Mei et al., 2016) by establishing
generalization results for both the empirical risk and the gradient of empirical risk. First, we show
that the bandwidth and Fourier `1 -norm for the local minima of the population risk can be bounded in
terms of the bandwidth and Fourier 'ι-norm of Y(x) and PX(x). Next, we establish a generalization
result for the gradient of the empirical risk, proving that the gradient of empirical risk would stay
close to the gradient of population risk given that Y(x) has limited bandwidth and Fourier 'ι-norm.
These two results show that by assuming a labeling scheme with limited bandwidth and Fourier
`1 -norm, the local minima found by the gradient descent (in general large-batch gradient descent)
method will generalize well.
6.1	Population Risk with Sinusoidal Activation
Consider fa,w,b(x) = Pjd=1 aj sin(2πwjT x + bj ) coming from a 2-layer neural network with d
sinusoidal hidden units. Given the labeling scheme Y(x) the population risk will be
d
Epχ ['(fa,w,b(x), Y(x))] = EPX [(Y(X)- Xaj sin(2πwTX + bj) )2 ],	(11)
j=1
where the expectation is according to the population density function PX(X).
Lemma 1. Consider the population risk in (11). Assume wj satisfies ∀i 6= j : min kwi -
wj k2, kwi +wj k2 > B(PX). Then, if (a, W, b) is assumed to be a local minimum of the population
risk,
|aj | ≤ 2 Yb ? PcX (wj).	(12)
Proof. We defer the proof to the Appendix.	□
Lemma 1 says that if the component aj sin(2πwjT X) becomes isolated for a local minimum, by
which we mean there are no other component ai sin(2πwiTX) with min{kwi - wj k2, kwi + wj k2}
less than PX’s bandwidth, then the value of aj for that local minimum is nicely bounded in terms of
the population distribution. This result leads to the following Theorem which describes the Fourier
properties of the local minima of the population risk.
6
Under review as a conference paper at ICLR 2018
Theorem 4. Consider the minimization problem of the population risk (11). If a local minimum
(a*, W*, b*) satisfies the isolated Components Condition, i.e. for any two different i,j we have
min{kw* — w*k2, kw* + w*k2} > 2 B(Pχ), then for the local minimum function fa* ,w* ”*
•	B(fa*,w*,b*) ≤B(Y) + B(PX),
..^ .. ..ʌ,.
•	kfba*,W*,b* k1 ≤ 2 kYb k1.
Proof. We defer the proof to the Appendix.	□
Theorem 4 implies that the bandwidth of the local minima of the population risk is less than the
sum of bandwidths for Y and Pχ. Also, the Fourier 'ι-norm for the local minima of the population
distribution is bounded by twice the Fourier `1 -norm of Y .
Remark 1. To apply Theorem 4, the bandwidth ofPX needs to be smaller than half the distance
among w*，s. For example, suppose that X 〜N(μ, σ2Ik×k) has a multivariate Gaussian distribution
with mean μ and diagonal covariance matrix with standard deviation σ. Then, the above theorem
shows that ifforany i,j we have min{∣∣w* — w*∣∣2, ∣∣w* + w*∣∣2} > 2C∕σ for some constant C,
then
•	B(fa*,w*,b*) ≤ B(Y) + O(√k∕σ),
•	kfba*,W*,b*k1 ≤ 2(1+dexp(—C2/2))kYbk1.
Proof. See the proof of Theorem 4 in the Appendix.
□
6.2	Generalization to the Empirical Risk
Theorem 4 characterizes the Fourier properties of the local minima for the population risk. However,
we want to investigate the generalization performance of the local minima of the empirical risk
defined for training samples (xi, Y(xi))in=1 as
1n
-E '(fa,W,b(Xi) , Y(Xi))
n i=1
-n	d	2
"( Y(Xi)- ΣSaJ sin(2πwTχi + bj))
(13)
To address this question, note that the bandwidth and Fourier '「norm of the loss,s gradient with
respect to each a§ are bounded in terms of the bandwidth and Fourier 'ι-norm of Y (x) as
M	z ------- .	.	. J	^.........
IlVaj '( fa,W,b(x),Y(X)III ≤ kYk1 + ∣∣ak1,
B( Vaj '( fa,W,b(x),Y(x))) ≤ B(Y) + 2∣W∣2,∞ .
(14)
(15)
We can apply Corollary 1 to show that not only the empirical risk uniformly converges to the
population risk but also the gradient of the empirical risk will stay close to the gradient of the
population risk.
Corollary 3. Consider fa,W,b(x) = Pjd=1 aj sin(wjT x + bj ) and squared error loss `. Then, given
that ∣X∣2 ≤ C, for any δ > 0 with probability at least - δ we have
∀j,a,W,b s.t. ∣a∣1+∣Yb∣1 ≤V, 2∣W∣2,∞ + B(Y) ≤B :
-n
E[Vaj'(fa,W,b (X),Y (X)))] — — £[V“j '(fa,W,b(Xi),Y(Xi)))]
n i=1
(16)
≤ O(VSkbg(n5*)).
Proof. The corollary is a direct result of Corollary (1) given (14) and (15). Note that the generalization
bound holds with probability - — δfor the derivative with respect to all aj ’s, since the bounds in (14)
and (15) hold for all j,s.	□
We emphasize that to prove Theorem 4 we need to analyze the risk function’s derivative only with
respect to aj ’s. Hence, generalization of the empirical risk’s gradient with respect to aj ’s, which is
7
Under review as a conference paper at ICLR 2018
Generalization risk
Generalization risk
I
Figure 2:	Training an test performance on cat and airplane CIFAR10 images with true and random
labels. Sine activation and mean-squared-error loss were used.
b.
Figure 3:	Training and test performance on cat and airplane CIFAR10 images with true and random
labels. ReLU activation and cross-entropy loss were used.
shown in the above corollary under certain assumptions, is sufficient to apply an approximate version
of Theorem 4 in section 8.6 to a local minimum (a*, W*, b) satisfying the isolated components
assumption and found by the gradient descent approach initialized at a low ∣∣akι and ∣∣W∣∣2,∞. We
can conclude that with probability at least 1 一 δ the 'ι-norm of fa*,w*,b* 's Fourier transform outside
the bandwidth B(Y) + B(PX) is bounded by
O(dV Jk log(nBc⑹)
,and also
kfa*,w*,b*kι ≤ 2 ∣Ykι + θ(dV^klog(nBcδ).
Based on the above discussion, if a large-batch gradient descent method starts learning from fa,w,b
with low ∣∣a∣ι and ∣∣W∣∣2,∞ and also we assume that the bandwidth and the Fourier 'ι-normfor Y (x)
are properly bounded, Theorem 4 combined with Corollary 1 will guarantee good generalization
performance for the local minima found by the gradient descent method.
7	Numerical Experiments
For all experiments described in this section, we implemented and trained the two-layer neural
network described in Figure 1a using TensorFlow 1.3.0. We used SGD to train the model for 2000
epochs with an initial learning rate of 0.01. The learning rate decayed slightly each epoch at a rate
of 0.95 every 390 epochs. We used h = 512 hidden units and a batch size of 128. When working
with CIFAR10 data, we preprocessed the data as described in (Zhang et al., 2016), resulting in each
training sample having dimension d = 2352. Initial weights from the first layer were sampled from
N(0,0.01/d) and initial weights from the second layer were sampled from N(0,0.01/h).
8
Under review as a conference paper at ICLR 2018
7.1	SGD GRADUALLY LEARNS HIGHER FOURIER `1 -NORM, BANDWIDTH HYPOTHESES
We first numerically demonstrate that how Fourier '1 -norm and bandwidth both increases during
training via SGD. Motivated by the analysis from Section 6, we use the squared-error as our loss
function and sine as our activation function. Our samples consist of cats and airplanes from the
CIFAR10 dataset with the labels mapped to -1 and 1. We use 5000 and 2000 samples from each
category for training and test, respectively. We arbitrarily chose two of the ten classes to accommodate
our choice of loss function. We evaluate the network’s performance for both random and true labels.
Figure 2a shows that without regularization, SGD learns to perfectly fit both the true and random
labels, which is consistent with the results from Zhang et al. (2016). Additionally, the random labels
are harder to learn, requiring more epochs before achieving a perfect fit. Figures 2b and 2c confirm
that both Fourier `1 -norm and bandwidth consistently increase with training, highlighting how SGD
gradually finds more complex hypotheses in order to fit the data. Finally, we see in figures 2d and 2e
how both Fourier `1 -norm and bandwidth increase with generalization risk (the difference between
test mean squared-error (MSE) and training MSE) with almost perfect correlation. This suggests
that, as implied by the theory above, regularizing Fourier `1 -norm and bandwidth could improve
generalizability of the final learned model.
7.2	Group path norm regularization for ReLU activation
We regularize group path norm for ReLU activation as motivated by Theorem 3. Although χ2-group
path norm is not convex, it is differentiable and we can use it as an additive penalty and find a local
minimum via SGD. Using the same experimental setup as from section 7.1, we swap sine for ReLU
and test the network’s performance for both random and true labels.
Figure 3a confirms that, like before, the network can fit both true and random labels. The generaliza-
tion gap, however, remains large for random labels. By regularizing the `2 -norm of all the weights,
we see that the generalization gap closes for both the true labels and the random labels without
compromising test accuracy significantly (Figure 3b). This result is further improved when we use
the χ2-group path norm and 'ι-path norm (Figure 3c and 3d), demonstrating that direct regularization
of Fourier `1 -norm leads to better generalization.
We cross-validated the value of λ for each regularization technique, and we chose the λ that resulted
in the smallest generalization gap with comparable validation performance. To fairly compare
different regularization strategies, we tested five lambda values for each strategy and then reported
the performance on the test set for the lambda value that resulted in the best performance on the
validation set.
We repeated the experiment using all 50000 CIFAR10 training samples (and 10000 test samples). We
included all 10 classes and switched to cross-entropy loss. The results are shown in Figure 1b. Again,
we see that while all regularization techniques give similar test performance, the generalization gap is
closed significantly for the X -group path norm and '「path norm.
References
Alexandr Andoni, Rina Panigrahy, Gregory Valiant, and Li Zhang. Learning polynomials with neural networks.
In International Conference on Machine Learning, pp. 1908-1916, 2014.
Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations. cambridge university
press, 2009.
Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE Transac-
tions on Information theory, 39(3):930-945, 1993.
Peter Bartlett, Dylan J Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural networks.
arXiv preprint arXiv:1706.08498, 2017.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural
results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.
Olivier Bousquet and Andr6 Elisseeff. Stability and generalization. Journal ofMachine Learning Research, 2
(Mar):499-526, 2002.
9
Under review as a conference paper at ICLR 2018
Ronan Collobert and Jason Weston. A unified architecture for natural language processing: Deep neural
networks with multitask learning. In Proceedings of the 25th international conference on Machine learning,
pp.160-167, 2008.
George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals,
and Systems (MCSS), 2(4):303-314, 1989.
Amit Daniely. Sgd learns the conjugate kernel class of the network. arXiv preprint arXiv:1702.08503, 2017.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep nets.
arXiv preprint arXiv:1703.04933, 2017.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic
optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In Conference on Learning
Theory, pp. 907-940, 2016.
Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient
descent. arXiv preprint arXiv:1509.01240, 2015.
Nick Harvey, Chris Liaw, and Abbas Mehrabian. Nearly-tight vc-dimension bounds for piecewise linear neural
networks. arXiv preprint arXiv:1703.02930, 2017.
Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior,
Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep neural networks for acoustic modeling in
speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine, 29(6):
82-97, 2012.
Sham M Kakade, Karthik Sridharan, and Ambuj Tewari. On the complexity of linear prediction: Risk bounds,
margin bounds, and regularization. In Advances in neural information processing systems, pp. 793-800, 2009.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On
large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836,
2016.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural
networks. In Advances in neural information processing systems, pp. 1097-1105, 2012.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436-444, 2015.
Holden Lee, Rong Ge, Andrej Risteski, Tengyu Ma, and Sanjeev Arora. On the ability of neural nets to express
distributions. arXiv preprint arXiv:1702.07028, 2017.
Shiyu Liang and R Srikant. Why deep neural networks? arXiv preprint arXiv:1610.04161, 2016.
Song Mei, Yu Bai, and Andrea Montanari. The landscape of empirical risk for non-convex losses. arXiv preprint
arXiv:1607.06534, 2016.
Behnam Neyshabur, Ruslan R Salakhutdinov, and Nati Srebro. Path-sgd: Path-normalized optimization in deep
neural networks. In Advances in Neural Information Processing Systems, pp. 2413-2421, 2015a.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural networks. In
COLT, pp. 1376-1401, 2015b.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. Exploring generalization in
deep learning. arXiv preprint arXiv:1706.08947, 2017.
Oren Rippel, Jasper Snoek, and Ryan P Adams. Spectral representations for convolutional neural networks. In
Advances in Neural Information Processing Systems, pp. 2449-2457, 2015.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms.
Cambridge university press, 2014.
Shai Shalev-Shwartz, Ohad Shamir, and Shaked Shammah. Failures of gradient-based deep learning. In
International Conference on Machine Learning, pp. 3067-3075, 2017.
10
Under review as a conference paper at ICLR 2018
Ohad Shamir. Distribution-specific hardness of learning neural networks. arXiv preprint arXiv:1609.01037,
2016.
Vladimir Vapnik. The nature of statistical learning theory. Springer science & business media, 2013.
Vladimir N Vapnik. An overview of statistical learning theory. IEEE transactions on neural networks, 10(5):
988-999,1999.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning
requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
8	Appendix
8.1	Proof of Theorem 2
We use a high-dimensional grid in the Fourier domain to approximate the Fourier transform of a B-bandlimited
function. Consider the ball {ξ : kξ∣∣2 ≤ B}. Using the bounds on the covering number for '2-norm, for any
0 <	< B we can find a set of points {ξj : 1 ≤ j ≤ (3B/)k} such that for any ξ with kξk2 ≤ B, there
exists some ξj with kξ - ξj k2 ≤ .
Let Sj = {ξ : kξ - ξj k2 ≤ } for each 1 ≤ j ≤ (3B/)k. Note that {ξ : kξk2 ≤ B} ⊂ ∪jSj. We then
define Sj0 = Sj \ ∪jt=-11 St to have a group of disjoint sets Sj0 covering {ξ : kξk2 ≤ B}. Since any f ∈ F is
assumed to be B-bandlimited, for f ∈ F
f(x) =	fb(ξ) exp(2πiξT x) dξ
(3B/)k
= X	fb(ξ) exp(2πiξT x) dξ.
j=1	ξ∈Sj0
(17)
Then, for any f ∈ F = {f : B(f) ≤ B, kfbk1 ≤ V } we have
f(x) -
(3B/)k
X exp(2πiξjT x)
j=1	ξ∈Sj0
fb(ξ) dξ
(a)
(b)
≤
(3B/)k
Xj=1 Zξ∈Sj0
(3B/)k
Xj=1 Zξ∈Sj0
(3B/)k
Xj=1 Zξ∈Sj0
fb(ξ)exp(2πiξT x) - exp(2πiξjT x) dξ
fb(ξ)exp(2πiξT x) - exp(2πiξjT x) dξ
fb(ξ)2πkxk2kξ-ξjk2 dξ
≤
(3B/)k
≤ 2πkxk2	X	fb(ξ)kξ-ξjk2 dξ
j=1	ξ∈Sj0
(c)	(3B/)k
≤ 2π kxk2	X	fb(ξ) dξ
j=1	ξ∈Sj0
=2πkxk2Z fb(ξ) dξ
≤ 2π kxk2V.
Here, (a) is a direct application of (17). (b) holds as exp(ibz) = cos(bz) + i sin(bz) is b-Lipschitz as a
function of z ∈ R for any real number b > 0. (c) holds because according to our definitions Sj0 ⊆ Sj and
Sj={ξ: kξ-ξjk2≤}.
Therefore, the following function space F can approximate any f ∈ F = {f : B(f) ≤ B, ∣∣jb∣∣ι ≤ V} within
2πCV accuracy for any kxk2 ≤ C. Here a is, in general, a vector of complex numbers, and kak1 := Pj |aj |
11
UlIderreVieW as a COnferenCe PaPer at ICLR 2018
Whereɪdenotes rhe abs-se ValUe Of COm-ex number N
(XB∕e)k
U - T(X) H M Pj exp(2TΓx)二一 a=l ≤ 二 (18)
L E J
Then- Fe is The SPaCe Of 01—norm bounded linear flmcfnsTerms OffhepVeCsr Lexp(27Γ2X)-「Now"
We Can apply a WeIknown bound (ShaleV Ishwarrz 涛 Ben—DaVida 2014) On rhe RademaCher ComPleXiryof
Nl—norm bounded linear SPaCe J⅛n∙l = {> i 京 S∙L T(X) H aTX二一 a = l ≤ 4 } as
力3≡p(⅛≡-l) ≤ AmaX =X=(19)
Applying rhe above bound- We Can bound rhe RademaCherCOmexiry Of Fe as
力比p(sie) ≤ V」2 三Og-6B∕a. (2。)
SCe for each j 平 F〔here exisf m Fm SUeh rhar< =x=2 ≤ Q 二 j(X)I f (X)-≤ 2TΓC
「一时 1
力Q) H FSUP — Y QI(Xm)
' 一 ? 〕 (21)
≤q sup —UQ，(Xm) + 2τTCymax=X一 2
=7^()+27Γamax=x2∙
i
Finally. combining (20) and (21) We Obtam
<evo:力3l(⅛i) ≤ VJ2l⅜log(6B/C) + 2aey max =x=2. (22)
Nn i
If We ChOOSe the VaIUe C= J I= :〔hen We g
2τrri max=X=2 O
力 empq) Λ F2A⅜(127mBmax=xm = 2) - I)
A V」B-OgaZTmBmaXi = X一2) + 2/n (23)
I H,
4A; log (64 τιB maxm =X2)
n
Where rhe IaSrequay follows from rhe farhar 1 ≤ k"n. TherefOre- rhe PrOOfiS COmpler
8∙2 PRooF OF CoRoLLARY 2
FirSL We PrOVe rhe following Iemma∙
Lemma 2∙ GiVenjiInCtn j .. 定R J 毋 OndmalriXW m 京 k×Wg dg g(x) = j(wxThen.
∙≤ =W = 2(f) Wim =W = 2 denoting Zhe specnorm MW7
•亘lul∙
proof. FrOm rhe PrOPerHeS Ofrhe FoUrier Kansfonn We know
6sU H⅛≠W5C∙ (24)
12
Under review as a conference paper at ICLR 2018
Therefore, b(WTξ0) = Pet(wπf(ξ') and if kξ0∣∣2 ≤ B(f), then ∣∣Wtξ0∣∣2 ≤ k W∣∣2B(f) gives an UPPer-
bound on B(g). Also,
kgbk1 =	gb(ξ) dξ
∕∣det1WTf(W-Tξ^dξ
∣d⅛√f(w-Tξ)∣dξ
(25)
1
∣det(W)∣
/f(ξ0))∣ ∣ det(W-T)∣ dξ0
fb(ξ0) dξ0
∙^∙
kfbk1.
□
It can be seen that this resUlt remains valid even if W is not an invertible matrix, which will comPlete the Proof
for Corollary 2. However, we continUe Proving Corollary 2 withoUt Using this fact.
As shown in the above lemma, FoUrier `1 -norm and bandwidth are invariant to an orthonormal transformation
W. Given fi (x) = aiφ(wiTx), we define gi (x) = fi (Aix) where Ai is an orthonormal matrix with wi
as an eigenvector. Note that kfi k1 = kgbi k1 and B(fi) = B(gi). However, gi (x) is a fUnction of only
one of the coordinates, which we can assUme, withoUt loss of generality, to be the first coordinate. Hence,
gi(x) = ai φ (k Wi k 2 x 1) forthe first coordinate xι, implying gi(ξ) = kwa⅛ b( ɪw^ ).δ2(ξ2)... δk(ξk) where
δj is the Dirac delta fUnction across the jth dimension. Hence, we can Use the above lemma in the 1-dimensional
case to show kgbik1 = ∣ai∣kφk1 and B(gi) = kwik2B(φ). As a resUlt,
kfbik1 = ∣ai∣kφbk1,	B(fi) ≤ kwik2B(φ).	(26)
Hence, for f (x) = aT φ(Wx + b) = Pid=1 aiφ(wiTx + bi) we have
kfbk1 ≤ kak1kφbk1,	B(f) ≤ kWk2,∞B(φ).	(27)
The corollary is then a direct aPPlication of Theorem 2.
8.3	Proof of Theorem 3
Given a ReLU-tyPe activation fUnction φα (z) = max{z, αz},
φα(wT X)= IHlq Φα(( Rj- )T x)∙	(28)
kwkq
Since ∣∣ 后"Iq = 1, if ∣∣x∣p ≤ C, then ∣( kwp)Tx∣ ≤ C and hence the input to φα in the R.H.S. of (28) is
always between -C and C.
Suppose that function ψα satisfies ψα (z) = φα (z) for z ∈ [-C, C]. Then, based on the above discussion,
we can bound the Rademacher complexity of Fφα by finding a bound on the Rademacher complexity of
Fψα = {fv,U (x) = VT ψα(Ux) : ∣M∣1 ≤ V, ∀i : ∣Ui∣q = 1 }.
To find a good candidate for ψα, we use a symmetrization trick to define
ψα(z)
-αC
φα(z)
φα (2C - z)
-αC
if z < -C,
if - C ≤ z < C,
if C ≤ z < 3C,
if 3C ≤ z.
(29)
Note that ψα(z) = (1 — α)Ch(Z-C) + 2aCh(Z-CC) — αC where h(z) = max{0,1 — ∣z∣}. It can be seen
that h(ξ) = (Sinn* )2 which is real and positive everywhere. Therefore, ∣h∣ι = h(0) = 1 which means that
∣ψcα ∣1 ≤ C(1 + 2α) ≤ 3C.
Since ∣b(ξ)∣ ≤ ξ2, we have ∣ψα(ξ)∣ ≤ 表
following Fourier transform:
For B > 0, we let the B-filtered ψα,B be a function with the
5λ^'/ʌʌ
ψα,B (ξ)
( ψcα(ξ) if ∣ξ∣ ≤ B
0 otherwise.
(30)
13
Under review as a conference paper at ICLR 2018
Then, since Ivc (ξ)∣ ≤ ξ2 We have
∀ Z ∈ R :	∣ψα(z)- Ψα,B (z) | ≤ /	∣Vjα(ξ)∣ dξ ≤	∙	(31)
J∣ξ∣≥B	B
Thus, for any B > 0 the defined ψα,B approximates φa with a maximum error of B uniformly over [—C, C].
ψα,B also satisfies kψα,B k1 ≤ 3C and B(ψα,B) = B. Applying Corollary 2, we get
∀ B> 0:	Rnmp (Fφ.) ≤ OaC Sk Sg(HB max kxik2) )+ B.	(32)
Here we can bound maxi ∣∣xi∣∣2 ≤ VZkmaxi Ilxil∣∞ ≤ ∖!~kC, and choose B = n to get
Rnmp (Fφɑ) ≤ O (VCs k lθg(HkC) + 1) ,	(33)
nn
which completes the proof.
8.4	Proof of Lemma 1
Note that
Raj EPX ['(fa,W,b(X),Y (X) )] = EPX [Vaj ' (fa,W ,b (X) ,Y (X))]
=EPX [Vaj (fa,W,b(X) — Y(X))2]
d
=EPX [2 sin(2πwTX + bj) (^X at sin(2πwTX + bt) — Y(X))]
t=1
= EPX [2aj sin2(2πwjT X + bj)] — EPX [2 sin(2πwjTX + bj)Y(X)]
+ EPX [X 2at sin(2πwtTX + bt) sin(2πwjT X + bj)]
t6=j
=aj — 2 [cos(bj) Im{Y ? PX(Wj)} + sin(bj) Re{Y ? PX(Wj)}].
(34)
To show the last equality, we use the isolatedness assumption for Wj, i.e. ∀t 6= j : min ∣Wt — Wj ∣2 , ∣Wt +
Wj ∣2 } > B(PX), and also ∣Wj ∣2 ≥ B(PX)/2. Then, for each t
EPX [2 sin(2πWtT X +bt)sin(2πWjTX + bj)] = 2	PX (x) sin(2πWtT x + bt) sin(2πWjT x + bj ) dx
= ∕pX(x) cos(2π(wt - Wj)Tx + bt — bj)
—cos(2π(wt + Wj )t x + bt + bj) dx
,_ .. ≤^-. .
= 0.5 exp(j (bt — bj))PcX(Wt — Wj)
+ 0.5 exp(j (bj — bt))PX(Wj — Wt)
—	0.5 exp(j (bt + bj))PX (Wt + Wj)
—	0.5 exp(—j(bt + bj))PX (—Wt — Wj)
0 if t 6= j,
=	1 if t = j.
(35)
Also, by applying the convolution property of Fourier transform we can show
EPX[sin(2πWjTX+bj)Y(X)]=Z PX(x)Y(x)sin(2πWjTx+bj)dx
=Z (PX×Y)(x)[cos(bj)sin(2πWjTX)+sin(bj)cos(2πWjTX)]dx
= cos(bj) ImYb ? PcX (Wj)} + sin(bj) ReYb ? PcX (Wj)}.
Finally	if (a, W, b) is a	local minimum for the population risk, for all t’s	we	have
Vat EPX ` fa,W,b (X), Y(X)	= 0. Therefore, due to the isolatedness assumption of Wj we have
.	.	.	_	^	,	、、	. _	,c	,	. 、	1 ^	,	. ∣
|aj | = 2 ∣cos(bj)	Im	Yb ? PcX (Wj) + sin(bj) Re	Yb ? PcX (Wj ) ∣ ≤ 2∣Yb ? PcX (Wj)∣.	(36)
14
Under review as a conference paper at ICLR 2018
8.5 Proof of Theorem 4
Since the isolatedness assumption holds for all j ’s, by Lemma 1,
∀j :	|a*| ≤ 2∣Y ?Pcx(w*)|.
If kWta k2 > B(Y) + B(PX) holds for some t , (37) implies
(37)
|a：| ≤ 2∣Y ?Pcx(w：)| =0.	(38)
Hence, ata will be 0, implying there will be no component in fa* ,W* ,b* with kWta k2 > B(Y) + B(PX). This
discussion proves the first part of Theorem, i.e. B(fa* ,W* ,b* ) ≤ B(Y) + B(PX).
To show the second part, note that
...—-- .. ..
kfa* ,W* ,b
,*
k1 = kaa
(a)
≤2
k1
d
X∣Y ?pcx(w：)|
t=1
2Xd ∣∣∣∣Z Yb (ξ)PcX(Wta - ξ)dξ
d∣	∣
≤2
≤^-
E 八Y(ξ)pcχ(w: - ξ) dξ
t=1
d
2X	∣∣Yb (ξ)∣∣ ∣∣PcX(Wta - ξ)∣∣ dξ
(39)
d
y∣Y (ξ)∣]XlPcX(Wa - ξ)∣
dξ
t=1
(≤b) 2 Xd Z ∣∣Yb (ξ)∣∣ dξ
∙^∙
= 2kYb k1.
Here, (a) comes from Lemma 1. Also, since min kWta - Wra k2 , kWta +
t 6= r, for any ξ at most one element in PcX(Wta - ξ)td=1
≤r- ,	_	.	___
Wra k2 > 2 B(PX ) is assumed for any
.≤^-. .、
can be nonzero. Because if both PX (Wta - ξ) and
PcX(Wra - ξ) are nonzero for r 6= t, then kWta - ξk ≤ B(PX) and also kWra - ξk ≤ B(PX) which results in
kWta - Wra k ≤ 2B(PX) which is a contradiction. Hence,
d
X∣∣PcX(Wta - ξ)∣∣ ≤ mξa0 x ∣∣PcX(ξ0)∣∣ ≤	∣∣PX(x)∣∣ dx = 1,
(40)
which proves (b) and completes the proof.
8.5.1 APPLYING THEOREM 4 TO MULTIVARIATE GAUSSIAN X
Assume X 〜N(μ, σ2Ik×k) has a multivariate Gaussian distribution With mean μ and standard deviation σ.
Then, the Fourier transform PK has a Gaussian shape with mean 0 and standard deviation 1 /σ. Hence, if for any
i,j we have min{ ∣∣w* — w* k2, kw* + w； k2 } > 2C/σ for some constant C, the approximation error term
which should be added to the upperbound in Equation (39) is 2kYb k1 d exp(-C 2 /2). Also, given any > 0 the
Fourier '1 norm outside the bandwidth O( VZklog(1 /e)/σ) is at most e. Therefore, Theorem 4 implies
•	B(fa* ,W* ,b*)≤B(Y) + O(√k∕σ),
•	kfba*,W* ,b* k1 ≤ 2(1 + dexp(-C2/2)) kYb k1.
8.6 Approximate version of Theorem 4
Here we show an approximate version of Theorem 4 which applies to approximate population local minima.
Theorem 5. Consider minimizing the population risk (11). Consider an approximate local minimum
(a*, W*, b*) where |Vaj E ['(fa* ,w* ,b* (X), Y (X)))] | ≤ e for all j's. If for any two different i,j we
2
d
15
Under review as a conference paper at ICLR 2018
have min{ ∣∣w⅛ — w:∣∣2, ∣∣w 吃 + w* k2 } > 2 B(PX), then the Fourier '1 -norm of fa*,w*,b* outside the
bandwidth B(Y ) + B(PX ) is bounded by d and
∣fba* ,W* ,b* ∣1 ≤ 2 ∣Yb ∣1 + d .
Proof. Since the isolated components condition holds, we can apply Lemma 1’s proof to show under the above
assumptions
∀ j :	|a*| ≤ 2|Y> *Pχ(w*)∣ + e.
Then, a simple modification of Theorem 4’s proof according to the above inequality proves the above theorem.
□
8.7 Proof of Theorem 4 without the is olated components assumption
What happens if a component wi is not isolated from the other components which has been assumed in Theorem
4? As a simplifying assumption, we assume that b = 0 and PcX is real. We can write
d
∀ i ： Rai EPX ['(fa,w (x),Y (x))] = X[ aj PX(Wi — Wj) ] — Im{Yb ? PX(Wi)}
j=1
d
≈X
j=1
aj —
ξ∈S
≤^-
Im{Yb (ξ)} dξ PX (wi — wj )
wj
(41)
^
^
Here Swj ’s, which are centered around wj ’s, are disjoint sets covering the bandwidth region for Y , i.e.
{ξ : ∣ξ∣2 ≤ B(Y )} ⊆ Sj Swj . Note that in (41) we have approximated the convolution integral as
^	, . f
Yb ? PcX (Wi) =
ξ: kξk2≤B(Y)
^
≤^-
PX(wi — ξ)Y (ξ) dξ
d
Xj=1Zξ∈S
≤^-
PX (wi — ξ)Yb (ξ) dξ
wj
^
d
≈ XPcX(Wi — Wj)	Yb (ξ) dξ.
j=1	ξ∈Swj
Letting the gradient element in (41) be zero for all ai’s at a local minimum (a* , W*) of the population risk, the
following approximation holds in general case:
∀j : aj* ≈	Im{Yb (ξ)} dξ,
ξ∈Swj
(42)
Here, the matrix [PcX(Wi — Wj)]1≤i,j≤d is positive-definite and hence invertible, because PcX is the Fourier
transform of PX and due to Bochner’s theorem a positive-definite kernel function. Therefore, the system of linear
***
equations	PX(Wi*	—	Wj*)	aj*	—	ξ∈S	Im{Y	(ξ)} dξ	≈ 0 would imply (42). This discussion indicates that
the result of Theorem 4 would remain valid even if the isolated components condition does not hold.
16