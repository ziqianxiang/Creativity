Under review as a conference paper at ICLR 2018
Learning non-linear transform with discrim-
INATIVE AND MINIMUM INFORMATION LOSS PRIORS
Anonymous authors
Paper under double-blind review
Ab stract
This paper proposes a novel approach for learning discriminative and sparse rep-
resentations. It consists of utilizing two different models. A predefined number
of non-linear transform models are used in the learning stage, and one sparsify-
ing transform model is used at test time. The non-linear transform models have
discriminative and minimum information loss priors. A novel measure related to
the discriminative prior is proposed and defined on the support intersection for the
transform representations. The minimum information loss prior is expressed as a
constraint on the conditioning and the expected coherence of the transform matrix.
An equivalence between the non-linear models and the sparsifying model is shown
only when the measure that is used to define the discriminative prior goes to zero.
An approximation of the measure used in the discriminative prior is addressed,
connecting it to a similarity concentration. To quantify the discriminative proper-
ties of the transform representation, we introduce another measure and present its
bounds. Reflecting the discriminative quality of the transform representation we
name it as discrimination power.
To support and validate the theoretical analysis a practical learning algorithm is
presented. We evaluate the advantages and the potential of the proposed algo-
rithm by a computer simulation. A favorable performance is shown considering
the execution time, the quality of the representation, measured by the discrimina-
tion power and the recognition accuracy in comparison with the state-of-the-art
methods of the same category.
1	Introduction
Learning a transform that provides sparse and discriminative representation is an active domain of
research in various areas, some of which are data processing, pattern recognition, image processing,
language modeling, text analysis and gene separation. A class of algorithms proposed by Kreutz-
Delgado et al. (2003); Mairal et al. (2009); Bengio et al. (2012); Gangeh et al. (2015); Mairal et al.
(2008); Jiang et al. (2011); Guo et al. (2012); Cai et al. (2014) and Liu et al. (2016) for learning
discriminative sparse representations have been shown to perform well across various learning tasks.
A subclass of them known as discriminative dictionary learning (DDL) Guo et al. (2012); Jiang
et al. (2013); Cai et al. (2014); Shekhar et al. (2014); Xu et al. (2015); Liu et al. (2016); Bengio et al.
(2012); Gangeh et al. (2015); Jiang et al. (2016) and Vu & Monga (2016) addreses the estimate of
the dictionary in a supervised manner such that the representation w.r.t. words (vectors) from the
resulting dictionary (vector set) is discriminative.
Most of the DDL methods synthesize the data sample k from class c, i.e, xc,k ∈ <N as an approxima-
tion by a linear combination yc,k ∈ <M (referred to as a sparse data representation kyc,k k0 << M)
of a few words (vectors dm), from a dictionary (vector set) D ∈ <N×M, i.e., xc,k = Dyc,k + vc,k,
vc,k ∈ <N, with vc,k denoting the approximation error. It is important to highlight that with the
synthesis model approach the data reconstruction is addressed.
The differences between the DDL methods Guo et al. (2012); Jiang et al. (2013); Cai et al. (2014);
Shekhar et al. (2014); Gangeh et al. (2015); Xu et al. (2015); Liu et al. (2016); Bengio et al. (2012);
Jiang et al. (2016) and Vu & Monga (2016) are determined by the prior defined on the sparse rep-
resentation and the prior defined for the relations between the sparse representations for the data
samples from the same/different classes. The discrimination is enforced by replacing the prior with
1
Under review as a conference paper at ICLR 2018
a structural constraint on the dictionary or imposing a discriminative term on the sparse representa-
tions. Additionally, some works by Mairal et al. (2008); Guo et al. (2012) and Taalimi et al. (2015)
consider even a joint estimation/learning of a dictionary, sparse representation, and classifier by us-
ing iterative alternating minimization strategy. The manuscripts by Bengio et al. (2012); Cai et al.
(2014) and Gangeh et al. (2015) give comprehensive overview covering different approaches.
1.1	Open issues
The general open issue for DDL methods is the computational complexity w.r.t. the optimal dic-
tionary/transform learning and the discriminative encoding, since the sparse representation in the
synthesis model is a solution to an inverse problem.
An additional open issue with most of the proposed approaches Guo et al. (2012); Jiang et al. (2013);
Cai et al. (2014); Gangeh et al. (2015); Liu et al. (2016), Bengio et al. (2012); Jiang et al. (2016); Vu
& Monga (2016) is that there is no formal notion to measure the discriminative properties. Therefore,
there are no means that provide a quantitative evaluation of the quality of the representation, other
than the performance of a classifier used on top of the representation.
Concerning the specifics in the discriminative constraints, Yang et al. (2011b) proposed a synthesis
model with a discriminative fidelity term and Fisher discriminant constraints, where the within-class
scatter and the between-class scatter of the representation is minimized and maximized, respectively.
The authors Vu & Monga (2016) proposed an extension considering a low-rank constraint on the dic-
tionary. An approach by Guo et al. (2013) used a synthesis model with a constraint on the pair-wise
relation between the sparse representation expressed by `2 distance metric. The methods reported
by Yang et al. (2011b) Vu & Monga (2016) and Guo et al. (2013) take into account assumption on
the metric by defining the scatter and the pair-wise relations. Therefore, they constrain the space of
the representation, which essentially is determined by the dictionary. However, these works do not
consider whether the used metric is optimal w.r.t. the sparse representation.
The method proposed by Liu et al. (2016) finds a dictionary under which the representation of a data
sample from the same class c have a common sparse structure by minimizing the size of the support
overlap for the representation from different classes. Assuming yc1,k1 ∈ <M and yc2,k2 ∈ <M are
two sparse representations for two data samples xc1,k1 ∈ <N and xc2,k2 ∈ <N, from two classes c1
and c2, they proposed a similarity measure defined by empirical expectation on kyc1,k1 yc2,k2 k0,
where represents the Hadamar product. Note that two transform data samples yc1,k2 and yc2,k2
that have small support overlap kyc1,k1 yc2,k2k0 = s, s << M, might not necessarily be similar
or dissimilar, i.e., yc1,k1 = yc2,k2 and yc1,k1 = -yc2,k2 with kyc1,k1 k0 = kyc2,k2k0 = s and s
small.
1.2	Approach and motivations
Model Instead of addressing a synthesis model where the data reconstruction is targeted and the
estimation of the discriminative representation is an inverse problem1 we present a novel, alternative
approach. That is we propose non-linear transform models in the learning stage and a sparsifying
transform model Rubinstein et al. (2010), Rubinstein et al. (2013), Rubinstein & Elad (2014) and
Ravishankar & Bresler (2014) for testing. The sparsifying transform model assumes that the data
sample xc,k is approximately sparsifiable under a linear transform A ∈ <M×N, i.e., Axc,k =
yc,k + zc,k, zc,k ∈ <M, where yc,k is sparse kyc,k k0 << M. It also represents a generalization
of the analysis model Rubinstein et al. (2010; 2013); Ravishankar & Bresler (2014); Rubinstein &
Elad (2014).
The proposed non-linear transform model is an extension to the sparsifying transform model that
considers additional assumptions. Both of the models used in this paper address a direct problem,
where the estimation of the discriminative representation represents a low complexity constrained
projection problem. Additionally, since these models have no restrictions on the transform represen-
tation to be in the column space of the dictionary (transform matrix A), they allow more freedom in
modeling and imposing constraints on the transform representation 2.
1 Note that a solution to an inverse problem has a high computational complexity if the dimensionality of
the dictionary (transform matrix) or the data dimensionality is high.
2In fact it allows modeling other non-linearity also, i.e., ReLu can be modeled as a transform representation.
2
Under review as a conference paper at ICLR 2018
产(XGQ
a)	b)	C)
Figure 1: a) Data samples X c , C ∈ {1,2,3,4} from four different classes, b) given a k-th data sample
Xc,k from class c, the non-linear transform is represented as two step operation: linear mapping
Axc,k (step 1) followed by an element wise thresholding function yck = TPc (Axc,k) (SteP 2), c)
the transform data samples Yc, C ∈ {1, 2,3,4}, d) two transform representations yci,ki and yc2,k2,
e) the resulting Hadamar products y+ι ki Θ y(⅛ k2 and y-1 ki ® y-2 k2 on the support intersection for
the similarity contribution and f) the resulting Hadamar products yc+1,k1 yc-2,k2 and yc-1,k1 yc+2,k2
on the support intersection for the dissimilarity contribution between yc1,k1 and yc2,k2.
similarity	dissimilarity
I yci,⅛ι ^∣y⅛,fc2 I I I I	y啊
yci,j⅛ι 0 yc2,⅛21		I ∣yci,*ι ® y⅛,fc2
	
d)	e)	f)
Prior and its measure In the learning stage, central in the non-linear transform models is the novel
parametric measure for a discriminative prior. It is defined on the support intersection for the trans-
form representations. The first motivation behind the used measure is that the support intersec-
tion of the transform data allows more freedom in imposing regularization on the discriminative
properties without taking into account any additional assumptions. Second, by approximating the
parametric mesure with an non-parametric one the focus of the regularization is directly put on
the contributing components for similarity/dissimilarity. Consider the measure (yc+1,k1)T yc+2,k2 +
(yc-1,k1)Tyc-2,k2 between two transform representations yc1,k1 and yc2,k2 where yc1,k1 = yc+1,k1 -
yc-1,k1 where yc+1,k1 = max(yc1,k1,0) and yc-1,k1 = max(-yc1,k1,0). Note that (yc+1,k1)T yc+2,k2 +
(yc-1,k1)Tyc-2,k2 = kyc+1,k1 yc+2,k2k1 + kyc-1,k1 yc-2,k2k1 captures the only contribution for the
similarity (whereas kyc+1,k1 yc-2,k2 k1 + kyc-1,k1 yc+2,k2k1 captures the only contribution for the
dissimilarity) between the vectors yc1,k1 and yc2,k2. Moreover, ycT1,k1yc2,k2 = kyc1,k1 yc2,k2k1,
only if the dissimilarity contribution -(yc-1,k1)T yc+2,k2 - (yc+1,k1)T yc-2,k2 for the vectors yc1,k1
and yc2,k2 is 0. That is ycT1,k1yc2,k2 = (yc+1,k1)T yc+2,k2 + (yc-1,k1)T yc-2,k2 - (yc-1,k1)T yc+2,k2 -
(yc+1,k1)Tyc-2,k2 ≤ (yc+1,k1)T yc+2,k2 + (yc-1,k1)T yc-2,k2, now if -(yc-1,k1)T yc+2,k2 - (yc+1,k1)T yc-2,k2
is zero then kyc1,k1 yc2,k2k1 = (yc+1,k1)T yc+2,k2 + (yc-1,k1)T yc-2,k2 = ycT1,k1yc2,k2. Third, the
expectation E[kyc-1,k1 yc-2,k2k1 + kyc+1,k1 yc+2,k2 k1] captures the concentration of similarity.
Therefore, it provides the possibility to define a formal notion that quantifies the discriminative
properties. Fourth, (yc+1,k1)T yc+2,k2 + (yc-1,k1)T yc-2,k2 is not ambiguous w.r.t. a notion for similar-
ity/dissimilarity between two sparse representations yc1,k1 and yc2,k2. This is because the support
intersections for the positive and the negative components (yc+1,k1)T yc+2,k2 = k(yc+1,k1) yc+2,k2k1
and (yc-1,k1)T yc-2,k2 = k(yc+1,k1)T yc+2,k2k1, respectively, are considered separately. In addition,
taken into account is the strength on the support intersection, defined as kyc1,k1 yc2,k2 k22 . Its em-
pirical expectation Pc,k ∣∣yd,ki Θyc2,k2∣∣2 〜E[kyci,ki Θyc2,k2∣∣2] captures the expected strength
on the support intersection for that set. A schematic diagram of the transform and the main idea
behind the proposed concept are shown in Figure 1, a), b) and C). On Figure 1 d), e) and f) are given
illustrative examples for the support intersections between the positive and negative component of
two vectors yc1,k1 and yc2,k2 in the transform domain.
A learning algorithm is presented using the proposed model with discriminative and minimum in-
formation loss priors. To quantify the discriminative properties, we introduce a measure named as
discrimination power, which reflects the discriminate properties of the representation for a dataset.
In addition, we present its lower and upper bounds, which depend on the parameters of the transform.
On the practical side, the advantages and the potential of the proposed algorithm are demonstrated
by a numerical experiments using the Extended YALE B Georghiades et al. (2001), AR Martlnez &
Benavente (1998), Norb LeCun et al. (2004), Coil-20 Nene et al. (1996), Clatech101 LeCun et al.
(2008), UKB NiSter & SteWeniUS (2006) and MNIST Lecun & Cortes datasets.
3
Under review as a conference paper at ICLR 2018
1.3	Notations
A scalar variable is denoted using x, a vector is denoted by a bold, low caps symbols x, a matrix
by bold, upper cap symbol A. A single element from a vector (or matrix) is denoted as x(n) (or
A(m, n)). A set is denoted by a calligraphic symbol, i.e, S. The `p-norm is denoted as k.kp and
the nuclear norm as k/*. The Θ symbol represents the Hadamard product. Throughout the paper
it is assumed that a set of data samples X = [X1, X2, ..., XC] ∈ <N×L, L = CK from C classes
is given and that every class c ∈ C = {1, 2, 3, ..., C} has K samples, Xc = [xc,1, xc,2, ..., xc,K] ∈
<N×K, xc,k ∈ <N, ∀c ∈ C, ∀k ∈ K = {1, 2, ..., K}. We denote the transform data as Y =
[Y1,Y2, ..., YC] ∈ <M×L, where Yc = [yc,1,yc,2, ..., yc,K] ∈ <M×K and yc,k ∈ <M. We
denote X∖{k∈c} = [Xi, X ,∙.X°,∖k,…X。] ∈ <N X(L-I) as the matrix that has all the columns
of X, except the column xc,k ∈ <N, where Xc,\k = [xc,1, xc,2, ..., xc,k-1, xc,k+1, ..., xc,K] ∈
<N×(K-1) is a matrix that has all the columns of block Xc, except the column xc,k, ∀c ∈ C and
∀k ∈ K. WeletN = {1,2,...,N}andM = {1,2,...,M}.
2	Learning non-linear transform with dis criminative and
MINIMUM INFORMATION LOSS PRIORS
The proposed approach has two operational modes: learning and test. It considers two different
models. A predefined number of non-linear transforms are used in the learning mode and one
sparsifying transform is used for test time.
2.1	The parametric non-linear transform modeling
Considering the learning mode, we assume that for every class c ∈ C there exist one non-linear
transform defined by a set of parameters Pc = {A ∈ <M×N, τc ∈ <M}, ∀c ∈ C3. All nonlinear
transforms described by {P1, ..., PC} share the linear map A and have distinct parameters τc. One
Pc with τc is related to only one class c. It is assumed that the parameters τc are spread far apart
in the transform domain. In addition, when all the non-linear transforms are applied to the corre-
sponding class samples then the transform data samples are separable w.r.t. the different classes, in
the transform domain.
As far as the non-linear transform we focus on transforms expressible by a two-steps operation,
consisting of a linear mapping (step 1) followed by an element-wise non-linearity (step 2):
A	Hτc (.)
xc,k ---→ Axc,k -----→ yc,k ,	(1)
step 1	step 2
where ∀wc,k = Axc,k ∈ <M, Hτc (wc,k) = sign(wc,k) Θ max(|wc,k| - τc, 0) : <M → <M,
represents a non-linear thresholding function with parameters τc ∈ <M . We also have to mention
that the thresholding is done with different thresholding parameters τc (m) for the corresponding
different transform dimensions m ∈ M.
At testing time we use one, common sparsifying transform defined by a set of parameters
P = {A ∈ <M×N, τ1 ∈ <M} for all data samples, with a constant tresholding parameter τ. The
transform matrix A is one and same for the C training and the testing models.
In the following section we describe the non-linear transform models, the proposed discriminative
prior and its mesure, together with the main reason behind this particular use of non-linear transform
models for learning and a sparsifying transform model for testing.
2.2	The non-linear transform model with a discriminative prior
The learning model This paper defines a compact description of the non-linear transform (1) by a
non-linear transform model as follows:
Axc,k = yc,k + zc,k, yc,k = TPc(xc,k),	(2)
3That is the number of non-linear transforms equals the number of classes.
4
Under review as a conference paper at ICLR 2018
where TPc (.) : <N → <M is a parametric non-linear function that gives yc,k, by using the set
of parameters Pc. The term zc,k = Axc,k - yc,k is the non-linear transform error vector that
represents the deviation of Axc,k from the targeted transform representation yc,k = TPc(xc,k) in
the transform domain. Since the transform representation yc,k = TPc (xc,k) takes into acount a
non-linearity, Axc,k is only seen as its linear approximation. Knowing something in advance about
the difference between yc,k - Axc,k can be used in our model. However, since in advance we do
not have any prior we assume that it is Gausssian like distributed. Therefore, the prior on zc,k is
modeled as p(xc,k∣yc,k, A) 8 exp(- "Ax'k-y'kk2), where βo is a scaling parameter. Assuming
additionally that the non-linear function TPc (xc,k ) gives sparse yc,k, then we have the improper
prior on yck, defined as p(yc,k) 8 exp(- kyβkk1) where βι is a scaling parameter. This paper
models the joint probability p(τ1, τ2,  , τC, yc,k) as
p(τι, T2,….，Tc, yc,k) H exp(--1 min D(yc,k； τd))p(yc,k),	(3)
β2 1≤c1≤C
and assumes that (Aiid) : p(τ1, τ2,  , τC) = Qcp(τc), where β2 is a scaling parameter. By
the assumption in subsection 2.1, τc1 are spread far apart in the transform domain. Therefore, a
minimum over D(yc,k; τc1) ensures that yc,k in the transform domain will be located to the closest
τc1 w.r.t. to the mesure D(yc,k; τc1). Moreover, given the class label c using (3) and (Aiid) the
model for the discriminative prior reduces to p(τjyc,k) h exp(一 "(丫言Tc)) where D(yc,k TC) is
a parametric measure with parameter τc. Assuming that D(yc,k; τc) is determined by a relation on
the support intersection between yc,k and Tc we propose the following definition of measure:
D(yc,k; Tc) = kyc+,k	Tc+k1 + kyc-,k	Tc-k1 + kyc,k	Tck22,	(4)
where kyc+,k Tc+ k1 + kyc-,k Tc- k1 measures the similarity contribution on the support intersec-
tion using the positive and the negative components, yc+,k, Tc+ and yc-,k, Tc- , respectively, of yc,k
and Tc and kyc,k Tc k22 measures the strength of the support intersection between yc,k and Tc.
The true p(Tc ) and Tc are not known and instead of estimating them explicitly, an approximation to
D(yc,k; Tc) is considered based only on the concentrations of the similarity on the support intersec-
tion and the expected strength of the support intersection for the transform data.
Non-parametric approximation We propose an approximation by sum of two expectations. The
first one is the expected similarity on the support intersection for the positive and negative component
between all yc,k and the coresponding sets of the transform representations Y\c that come from
all classes c1 different from c, i.e., c 6= c1. The second is the expected strength on the support
intersection between yc,k and the set of transform representations Y\c that come from all classes c1
different from c, c 6= c1. We define the approximation as:
X D(yc,k； TC)〜DP(X) + SP (X) where
c,k
CK	K
DP (X) = XX X X (ky++k 0 y+ι,kikι + kyc-,k	yc-1,k1k1 ),	(5)
c=1 k=1 c1∈{{1,2,...,C}\c} k1=1
CK	K
SP (X) = XX X X kyc,k 0 yc1,k1k22,
c=1 k=1 c1∈{{1,2,...,C}\c} k1=1
We highlight that the transform represntations yck used in the approximation DP (X)
and SP (X) are the result of applying the sparsifying transform with parameter set P to
the data samples xc,k. The m-th element yc+,k (m) of yc+,k is defined as yc+,k (m)	=
max(yc,k(m),0) and similarly, y-k(m) = max(-yc,k(m),0), ∀m ∈ M. We also de-
fine the expected similarity using the positive and negative components of all yc,k across
the transform representations Y\c that come from the same classes C as DP C(X) =
Pc PkK=1 P k1∈{{1,2,...,K}\k} kyc+,k 0y+kikι + ky-k0y-kikι)∙ If the measure DP(X) is
not used then the approximation (5) is most similar to the one proposed in Liu et al. (2016).
5
Under review as a conference paper at ICLR 2018
By (5) a link is established between the non-linear transform models and the sparsify-
ing transform model, or more generaly, assuming A is known, between a parametric and
a non-Parametric modeling view. The terms ((C-I)K)(CK) P C1∈{{1,2,…,C}∖c} PK1=1 y;ι,ki,
((CI)K)(CK) P c1∈{{1,2,…,C}∖c} Pk1=1 y+l,k1 and ((CI)K)(CK) Pc1,c1=c Pk1 yc1,k1 ® yc1,k1 are
seen as finite sample estimates 4 5 of the positive, negative components and the Hadamard square,
τc- , τc+ and τc τc, respectivly, for the unknown variable τc, ∀c ∈ C.
Note that the Fisher discriminate constraint Yang et al. (2011b), the pairwise constraint Guo et al.
(2013) and the support intersection constraint Liu et al. (2016) are all approximations of a discrim-
inative prior. However, they all have specific assumptions on the distribution of the data represen-
tation in the transform domain. The advantage of using (5) is that the approximation is without
any prior to the probability distributions p(τc) and without any explicit assumption about the met-
ric/measure, or space/manifold in the transform domain.
Given a training set X, the learning perspective w.r.t. a discriminative property is to estimate
a non-linear transform models Pc = {A ∈ <M ×N, τc ∈ <M} that minimize the empirical ex-
Pectation CK Pc,k miηι≤c≤C D(yc,k; Tc)〜 Emini≤c≤c D(yc,k; τj]. Moreover, if the corre-
sponding class labels for the training set X are given6 then Pc,k min1≤c≤C D(yc,k; τc) equals to
Pc k D(yc,k; τc) and exactly matches the empirical expectation DP (X) + SP (X).
The testing model Assume that the transform matrix A and the Parameters τ1, τ2, ..., τC are known,
then given any data sample xc,k the transform representaton yc,k is estimated as:
min
yc,k
kAxc,k - yc,k k22 + λ0 (
min
1≤c1≤C
D(yc,k; τc1)) + λ1kyc,kk1,
(6)
which is euqialent to min1≤c1≤C minyc,k kAxc,k - yc,kk22 + λ0D(yc,k; τc1) + λ1kyc,kk1 . Fur-
theremore, if we assume that the measures D(yc,k; τc1) are zero, i.e., D(yc,k; τc1) = 0, then the
discriminative prior is non-informative, in a sence that it has no influence in the models. Only then
do the non-linear transforms reduce to the sparsifying transform model, since (6) reduce to:
min kAxc,k - yc,kk22 + λ1kyc,kk1.
yc,k
(7)
Considering the testing stage, we note that the result (5) sheds light on another view. Namely, the
sparsifying transform model P = {A ∈ <M×N, λ1 ∈ <M} is also seen as an approximation to the
models represented by a set of parameters Pc = {A ∈ <M×N, τc ∈ <M} with expected loss in the
discriminative properties of the transform representations expressed by the similarity concentration
measure DP (X) + SP (X). At the same time this measure can also be considered as an empirical
risk Vapnik (1995) w.r.t. the discriminative properties, related to the generalization capabilities
Vapnik (1995) and Mark (2010) of the sparsifying transform model. Note that the same model is
also the simplest that approximates the non-linear transform models used in the learning stage.
2.3	The learning algorithm
In summary, the used priors are:
p(xc,k ∣yc,k, A) Y exp(-k AXc,kβ- yc,k k2)
p(τc, yc,k) = p(τc∣yc,k)p(yc,k) Y exp(-D(yc,k; Tc)) eχp(-kyc,kk1).
β2	β1
(8)
4Since	Pc1∈{{1,2,...,C}\c} Pk1(kyc+,k	yc+1,k1k1 + kyc-,k	yc-1,k1k1)	=
k Pc1∈{{1,2,...,C}\c} Pk1 yc+1,k1	yc+,kk1 + k Pc1∈{{1,2,...,C}\c} Pki y-ι,ki) ® y-kkι, τ)〜
((C-I)K)(CK) Ec1∈{{1,2,…，C}∖c} Σki yci,ki and τc+ 〜((C-I)K)(CK) Ec1∈{{1,2,…，C}∖c} k1 yc+1,k1.
5Since Pc1∈{{1,2,...,C}\c} Pk1 kyc,k	yc1,k1 k22 =	Pc1∈{{1,2,...,C}\c} Pk1 yc1,k1	yc1,k1
(yc,k ° yc,k), τc ° Tc ~ ((C-I)K)(CK) Pc1∈{{1,2,…,C}∖c} Pki yc1,k1 ° yc1,k1.
6 Note that if the labels are not given then the unsupervised case can also be addressed by using a likelihood
measure between a sample and the rest of the available samples, with the possibility to be defined in the original
or in the transform domain.
6
Under review as a conference paper at ICLR 2018
Additionally, we have a prior on A that penalizes the information loss in order to avoid trivially
unwanted matrices A, i.e., matrices that have repeated or zero rows. The prior is defined as:
P(A) IX exp(-Ω(A)) = exp f-(-1 ∣∣A∣∣F + -1 ∣∣AAT — IkF - -1 log | det ATA|)) ,	(9)
β3	β4	β5
where the ∣A∣F penalty helps regularize the scale ambiguity, the log | det (AT A)| and ∣A∣2F are
functions of the singular values of A and together help regularize the conditioning of A. Assuming
that the expected coherence μ2 (A) between the rows am, of A (i.e., AT = [aι, a2,..., aM]) is
definedas μ2 (A)= M (M-1)pm1=m2 |ami aTl2 |2, ∀m1,m2 ∈ {1, 2,..,M}. Then IlAAT - IkF
measures the expected coherence μ2 (A) and the '2 norm for the rows of A.
Note that the joint probability can be expressed as:
p(xc,k, yc,k, Tc, A) = p(xc,k, yc,k, Tc∣A)p(A),	(10)
where p(x0,k, yck ,τ∕A) = p(x°,k ∣yc,k, A)P(TcJyc,k)p(yc,k), since p(xc,k 卜 c,k ,τc, A)=
p(xc,k|yc,k, A). Given the available training data set X, maximizing p(xc,k, yc,k, τc, A) over Y
and A is same as minimizing the following problem:
min IIax - y∣2 +	λ0D(yc,k; TC) + λ1kyc,k∣1 + ω(A),
,	c,k
(11)
where {λ0, λ1} are inversely proportional to the scaling parameters {β2, β1}. Note that the solution
to (11) is not equivalent to the maximum a priory (MAP) solution, which is difficult to compute, as it
involves integrating over the vectors yc,k. Considering the optimization perspective, the problem is
not convex in the variables (Y, A). The proposed solution here is obtained by iteratively, marginally
maximizing the probability P(xc,k, yc,k, Tc, A) over Y and A which is equivalent to maximizing
the conditional densities P(yc,k |xc,k, Tc, A) and P(A|xc,k, yc,k, Tc), respectively. Meaning that at
one iterating step one of the variables Y or A is fixed and w.r.t. the other the problem (11) is mini-
mized. The following describes the iterating steps that consist of linear map estimation (maximizing
P(A|xc,k, yc,k, Tc)) and discriminative encoding (maximizing P(yc,k |xc,k, Tc, A)).
Linear map estimation: Given the available data samples X and the corresponding transform rep-
resentations Y the linear map A estimation problem reduces to:
min ∣AX - Y∣2 + λ2k AkF + λ3∣ AAT - IkF - λ4 log | det ATA|,	(12)
A	22
where {λ2 , λ3, λ4 } are inversely proportional to the scaling parameters {β3, β4 , β5 } and we use the
-close closed form solution estimated as follows:
Proposition 1 (-close closed form solution): Given Y ∈ <M×CK, ∀X ∈ <N×CK and M ≥ N,
∀λ2 ≥ 0, λ3 ≥ 0 and λ4 ≥ 0 let the eigen value decomposition UX ΣX VXT of XXT + λ2I and
the singular value decomposition UUXXY ΣUXXYVUT XY of UTXXYT exist, then if and only if
σX (n) > 0, ∀n ∈ N = {1, 2, 3, ..., N}, (12) has -close approximative solution as:
A=VUXXYUTUXXYΣAΣ-X1UTX,	(13)
where ΣA is diagonal matrix, ΣA (n, n) = σA (n) ≥ 0 , and σA(n) are solutions to quartic polyno-
mials with global minimums (the proof is given in Appendix A.2).
Discriminative encoding: Given the available data samples X and the current estimate of the
transform A the discriminative representation estimation problem is formulated as (PDR) :
minY IAX - YI2F + λ0 Pc,k D(yc,k; Tc) + λ1Iyc,kI. (PDR). Even not knowing P(Tc) or the
model variables Tc we show that by the approximation (5), (PDR) has an efficient solution. Assum-
ing that Y\c is given, using the approximation (5), then for any sample k ∈ K from any class c ∈ C,
problem (PDR ) reduces to a constrained projection problem:
min l∣Axc,k — y0,k∣∣2 + λo (gT卜c,k| + ST(yc,k Θ y°,k)) + λι1T卜°,&|,	(14)
yc,k
and has a closed form solution as:
Yc,k =Sign(AXc,k) Θ max(∣Axc,k∣ 一 λogc — λι1, 0) 0 (1 + 2λ°Sc),	(15)
7
Under review as a conference paper at ICLR 2018
where we abuse notation to denote |yc,k | as the vector having as elements the absolute values of
the corresponding elements in yc,k,	is a Haddamard division, gc = sign(max(Axc,k, 0))
dc+	+ sign(max(-Axc,k,0))	dc-,	dc+	= P	c1	Pk1 yc+1,k1,	dc-	= P c1	Pk1	yc-1,k1	and
c16=c	c16=c
sc = P c1 Pk1 yc1,k1	yc1,k1 (the proof is given in Appendix B).
c16=c
We note that at convergence (which we do not prove here) we can only claim that a joint local max-
imum in (Y, A) of p(xc,k, yc,k, τc, A) has been reached, even if, as in this case, each optimization
step achieves the (marginal) -close and the global optimal solution, respectively.
2.4	A measure for the discriminative properties and its bounds
This paper proposes a notion for the discriminative properties of a data set under a non-linear trans-
form named as discrimination power, based on a measure for the relations between the concentra-
tions DP,c(X) and DP (X).
Proposition 2: The discrimination power for any dataset Y ∈ <M ×CK under a non-linear trans-
form with parameter set P is defined as:
It = log(D黑(Y))- log(DBM(Y) + e) = log(DP,c(X)) - log(DP (X) + e),	(16)
where BM = {At ∈ <M×M, 0 ∈ <M}, At = I and > 0 is a small positive constant.
Remark 1: The advantage of this measure is that it logarithmically signifies the difference between
DP,c(X) and DP (X)7.
The definition about the discrimination power of the original data set X, but, now under a model
with a parameter set BN = {Ao ∈ <N×N, 0 ∈ <N} where Ao = I is equivalent to the one defined
for It. We denote it as Io. The bound on the discrimination power is given by the following result.
Theorem 1: The discrimination power for any data set X ∈ <N×CK under any non-linear trans-
form with parameter set P is bounded as:
log(λmin(ATA)) + log
Tr{
∂DBNC(χ)
^Ao
|Ao=I}
、DaM(AX)+ C
≤I t ≤ log (D 黑(AX)) - log C.
(17)
The proof is given in Appendix D8.
At first the resulting bounds might look counterintuitive since the loss of information seems to
increase the discrimination power. This fact is true, however, up to a certain limit. Therefore, it is
important to distinguish two main conclusions. First, for any model with a set of parameters P for
which there is no loss of information, that is, no thresholding, the only condition for the increase in
the discrimination power is DBN(X) ≥ DBM(AX) and D^NC(X) ≤ DBMC(AX). Second, in the
rest of the cases for which DBN(X) ≥ DBM (Y) and D^NC(X) ≤ DBMc(Y) holds true it will be
possible to increase the discrimination power. Moreover, there is a trade-off between the increase
in discrimination power as a result of the loss of information as consequence of the non-linear
thresholding operation.
7	kτc+yc+kk1+kτc-yc-kk1
'Assume we have a discriminative prior defined as p(τc∣yc,k) (X exp(----------Ck-瓦---------Ck-) and
ρ(υc∣yc,k) (X exp(一 '% ©丫…狂+'~。丫、”1), where Tc and Uc are unknown parameters. Then the differ-
ence Dp,c(X) — DP (X) between Dp,c(X) and DP (X) actually represents a finite sample approximation
to a discriminative density since it approximates the density log (P(TcIyc,k)), ie, Dp,c(X) — DP (X)〜
Tog (P(Ucyc：k))and - log (P(Tc，£)) = D(Tc,yc,k) - D(Uc,yc,k) = kτc+ ® y+kkι + ∣∣τc- ®
y-kkι — (kυc+ Θ y+kkι + kυc- Θ y-kkι).
8	In Appendix Cwe also provide a sensitivity analysis that complements our result since itis related to the no-
tion about the discriminative quality of the representation. The result in Appendix C also gives an information-
theoretic interpretation and information-geometric perspective about the model and the similarity concentration
measure without the need of strict conditions for regularity, i.e., smoothness of the manifolds.
8
Under review as a conference paper at ICLR 2018
Number of iterations
Number of iterations
Iteration number
Number of iterations
Figure 2: The evolution of the similarity concentrations C1 = DBMc(Y) and C2 = DBM (Y), their
ratio C1/C2 and the discrimination power log(C 1/C 2) = It during the learning of the non-linear
transform with transform dimension M = 19000.
	Cn(A)	μ(A)	te [min]	IO	IRT	I ST *	I NT
D1	2.21	0.03	5.10	^Ο3	^Ο8	-Ο8	1.98
D2	1.80	0.02	5.45	0.02	0.10	1.30	1.79
D3	2.12	0.02	6.55	0.00	0.01	0.71	1.61
D4	0.08	0.02	8.92	0.08	0.61	0.89	1.89
D5	6.01	0.01	12.8	0.01	0.16	1.02	2.12
D6	33.1	0.02	30.1	0.06	0.53	1.36	3.36
D7	1.60	0.02	5.00	0.13	0.63	1.06	1.96
Table 1: The conditioning number Cn(A) = λmax and the expected mutual coherence μ(A) for
the learned transform A. The execution time te[min] in minutes of the proposed algorithm for 28
iterations at the transform domain dimensionality M = 19000.
3 Numerical experiments
The numerical experiments are summarized in two different parts. In the first series of the experi-
ments the properties of the learned map A for the proposed algorithm are investigated. We evaluate
the computational efficiency, as run time te[min], the conditioning number Cn(A) = λmax, the ex-
pected mutual coherence μ(A) and the discrimination power across several databases for a learned
non-linear transforms having different dimensionality. A comparison between the discrimination
power uder different transforms is presented. The discriminatation power is estimated in the orig-
inal domain, after transform by a random matrix (having Gaussian random samples as entries and
transform dimension of M = 19000) and after a learned non-linear transform having transform
dimension M = 19000 without and with discriminative prior, denoted as 10, IRT, I ST * and I NT,
respectively. The second part evaluates a comparison of the discrimination power between the pro-
posed algorithm and different DDL methods Ramirez et al. (2010), Yang et al. (2011a), Vu et al.
(2015) and Vu & Monga (2016). This comparison considers a setup where the used data sets are
divided into a training and test set. Moreover, the learning is performed on the training set and the
evaluation is performed on the test set. In the same series of experiments the recognition accuracy
for the two data sets is also computed and compared.
Data sets and algorithms set up The used data sets are Extended YALE B (D1)Georghiades et al.
(2001), AR (D2) Martlnez & Benavente (1998), Norb (D3) LeCun et al. (2004), Coil-20 (D4)
9
Under review as a conference paper at ICLR 2018
Transform dimension (M)
Figure 3: The conditioning number Cn(A) = λmax and the expected mutual coherence μ(A) for
the learned linear transform A at different dimensionality M ∈ Q.
Figure 4: The similarity concentrations C1 = DBMc(Y) and C2 = DBM (Y), their ratio C1/C2
and the discrimination power log(C1/C2) = It on a subset of the transform data using learned
non-linear transform at different dimensionality M ∈ Q.
Nene et al. (1996), ClatechIOI (D5) LeCun et al. (2008), UKB (D6) Nister & SteWeniUs (2006)
and MNIST (D7) Lecun & Cortes. All the images from the respective datasets were downscaled
to resolutions 21 × 21, 32 × 28, 24 × 24, 20 × 25, 21 × 21, 20 × 25, 28 × 28, respectively, and
are normalized to unit variance. Considering the used implementation of the algorithm We note that
the singular value decomposition for a large matrix has high computational complexity. HoWever,
A - A, Where A is estimated as a solution in the transform update step, can be considered as an
proximal operator Parikh & Boyd (2014) for the gradient of the objective (12). Additionally, instead
of using all of the available data samples X, a subset of them might be used. Therefore, one simple
on-line variant for the update of A W.r.t. a subset of the available training set has the form At+1 =
At - P(At - At) with P a predefined step size. In the numerical experiments we use the on-line
variant of the algorithm (the convergence analysis for this variant of the algorithm is left for future
work) were we used a baches of sizes equal to 10%-12% of the total amount of the available training
data. The parameters λ0 and λ1 are set such that the resulting non-linear transform representation
has a very small number of non-zeros w.r.t. the transform dimension. In the experiments this number
is set to be 15. The rest of the parameters are set as {λ2, λ3, λ4} = {1000000, 1000000, 1000000}.
The algorithm is initialized with a random matrix having i.i.d. Gaussian (zero mean, variance one)
entries and is terminated after the 28th iteration. The results are obtained as the average of 3 runs. An
implementation presented in Vu & Monga (2016) was used to learn the dictionaries and estimate the
10
Under review as a conference paper at ICLR 2018
	D1	D7	D1 Acc. [%]	D7 Acc. [%]
IDLSI	0.71	0.67 DLSI-	965	DLSI-	9874
IFDDL	0.87	0.63 FDDL	97.5	FDDL	96.31
ICOPAR	0.57	0.54 COPAR	98.3	COPAR	96.41
ILRSDL	0.42	0.40 LRSDL	98.7	LRSDL	-
INT	0.98	0.81 NT	99.7	NT	99.02
	a)	b)	c)
Table 2: a) The discrimination power for the methods DLSIRamirez et al. (2010), FDDL Yang
et al. (2011a), COPAR Vu et al. (2015) and LRSDL Vu & Monga (2016) and the proposed method
NT , b) and c) The recognition results on the Extended Yale B and MNIST database.
0 8 6 4 2 0
0 9 9 9 9 9
AoE-noo‹
88
0.2	0.4	0.6	0.8	1
Q'8'6'4
0 9 9 9
AoE-noo4
1.2
Discriminative power
M	100	500	1.5K	4K
Acc. [%]	89.1	94.3	97.4	99.7
It	0.21	0.73	0.93	1.23
92
1.2	1.3	1.4	1.5
Discriminative power
M	1K	4K	6K	12K
Acc. [%]	92.49	94.24	96.01	99.02
It	1.18	1.35	1.42	1.55
Figure 5: The recognition results and the discrimination power on the Extended Yale B and MNIST
databases, respectively, using a non-linear transform with different dimensionality M and linear
SVM classifier on top of the transform representation.
Figure 6: The expected loss E[k ZMk∣∣2] = E[ kAxc,Myc,kk2 ] and the discrimination power on the
Extended Yale B and MNIST databases, respectively, on the transform representation Y, obtained
by using a non-linear transform TP at different dimensionality M .
sparse codes for the respective supervised dictionary learning methods DDL Ramirez et al. (2010),
Yang et al. (2011a), Vu et al. (2015) and Vu & Monga (2016).
Linear map properties, the similarity concentrations and the discrimination power The condi-
tioning number and the expected coherence for the learned transforms are shown on Table 1. The
learned transforms for all the databases have good conditioning numbers and low expected coher-
ence. The running time te, measured in minutes, and the number of used dimensions, denoted as M
are also shown in Table 1. The learned transforms for all the data sets have relatively low execution
time, regardless of the very high transform dimension M = 19000. The discrimination power is
significantly increased in the transform domain INT compared to the one in the original domain IO
and is higher than I ST * and IRT .The evolution of the similarity concentrations C1 = DB C(Y)
and C2 = DBM (Y), their ratio C1/C2 and the discrimination power log(C 1/C2) = It for subsets
of the used databases after applying a non-linear transform with transform dimension M = 19000
is shown in Figure 2. It is important to note that the similarity concentrations C1 = DB C(Y) and
11
Under review as a conference paper at ICLR 2018
C2 = DBM (Y) are decreasing, meaning that there is a loss of information. However how this
loss effects the resulting similarity concentration is crucial for the discrimination properties. As
shown in Figure 2, the slope of decrease for C2 is stronger. Therefore, the discrimination power
increases per iteration. For the Coil-20 (D4) database there is a fluctuation. This is explained by
the fact that during learning we used a small number of data samples from the same database and
that in the data there is high variability. The conditioning number and the expected coherence
for the learned transforms for all the databases at different transform dimensions M ∈ Q =
{100, 1150, 2200, 3250, 4300, 5350, 6400, 7450, 8500, 9550, 10600, 11650, 12700, 13750, 14800,
15850, 16900, 17950, 19000} are shown in Figure 3. We see that the value of both the condi-
tioning number and the coherence is reducing and it converges to common values. This confirms
the effectiveness of the conditioning and the coherence constraints. The similarity concentra-
tions C1 = DBMc(Y) and C2 = DBM(Y), their ratio C1/C2 and the discrimination power
log(C1/C2) = It for a subsets of the used databases after applying a non-linear transform having
transform dimensions M ∈ Q is shown in Figure 4. We can see similar behavior as previous,
that is, C 1 and C 1 are decreasing, but, the slope of decrease for C2 is stronger. Therefore, the
discrimination power increases as the transform dimension increases.
NT vs DDL discrimination power and recognition performance The proposed method is com-
pared with DLSIRamirez et al. (2010), FDDL Yang et al. (2011a), COPAR Vu et al. (2015)
and LRSDL Vu & Monga (2016). Half of the data samples from the data set Extended YALE B,
sampled at random are used for learning and the remaining other half are used for evaluation. Con-
sidering the MNIST database the training set is used for learning and the test set is used for both eval-
uating the discrimination power and the recognition accuracy. We compute both the discrimination
power and the recognition accuracy on a subsets from the test sets. The dictionary size (transform
dimension M) is set to be equal to {150, 75, 1515, 3825, 570, 150, 300}for the used databases, re-
spectivly, in all of the comparing algorithms. The discrimination power of the comparing methods is
denoted as IDLSI, IFDDL, ICOPAR and ILRSDL. The recognition results for the methods DLSI,
F DDL, COP AR and LRSDL on the data sets Extended YALE B and MNIST were not computed
here, rather we use the best reported result form the respective papers Ramirez et al. (2010), Yang
et al. (2011a), Vu et al. (2015) and Vu & Monga (2016). Considering the proposed algorithm the
non-linear transform was learned for the transform dimensions M = {100, 500, 1500, 4000} and
M = {1000, 4000, 6000, 12000}, respectively, for the used data sets. After the transform was
learned, the transform data samples were computed for the respective training and test sets. Then,
the transform training data samples were used as features to learn a linear SVM classifier in one-
against-all regime. The results are shown in Table 2 a), b) and c). The discrimination power of the
proposed non-linear transform is higher that the discrimination power of the comparing methods.
The recognition accuracy is higher for high dimensionality of the proposed method and outper-
forms the DDL methods at dimensionality 4000 and 12000. In Figure 5 and Figure 6 are shown the
recognition accuracy and the expected loss measured as E[k ZMk∣∣2] = E[ kAx'My'kk2 ] as a linear
function of the discrimination power evaluated at transform dimension M = {100, 500, 1500, 4000}
and M = {1000, 4000, 6000, 12000}. It is interesting to highlight that as the discrimination power
at different transform dimension increases it also increases the accuracy of recognition. Moreover,
the results on these two data sets show that this increase is approximately linear. On the other hand
the expected loss decreases as the discrimination power at different transform dimensions increases.
4 Conclusion
This paper presented a novel approach for learning discriminative and sparse representations. A
novel discriminative prior was proposed and the properties of the models with the prior were in-
vestigated. A low complexity learning algorithm was presented. The preliminary results w.r.t. the
introduced measures and the recognition accuracy on the used databases showed promising perfor-
mance. We showed that it is possible to increase the discrimination power with information loss.
Moreover, we highlight that when expanding to high dimensional space with non-linear transforms
how the loss of information reflects the similarity concentrations is crucial for the discriminative
properties. A study on the recognition capabilities for other databases are our next steps. An ex-
tention considering the sufficient conditions for increase in discrimination power in the transform
domain, under supervised and unsupervised case, together with an analysis for a deep architecture
where per single layer we have a non-linear transforms are left for our future work.
12
Under review as a conference paper at ICLR 2018
Appendix A.
A.1 The global optimal solution
Given X and the curent estimate of Y, the estimate of the transform is a solution to the following
problem
min kAX - Y∣∣2 + λ22k A∣∣F + λ23k AAT - IkF - λ4 log | det ATA|.	(18)
Theorem 2 (golobal optimal solution) Given X ∈ <N ×CK and Y ∈ <M ×CK, if and only if the
joint decomposition
XXT = UXΣ2XUTX
XYT = UXΣXYVTXY ,
(19)
exists, where UX ∈ <N ×N is orthonormal, VXY ∈ <M ×N is per columns orthonormal and
ΣX, ΣXY ∈ <N×N are diagonal matrices with positive diagonal elements, then (18) has a global
minimum as
A=VXYΣAΣ-X1UTX,	(20)
ΣA(n, n) = σA(n), ∀n, σA(n) ≥ 0 and σA(n) are positive solutions to
λ3
σX (n)
σA(n)+σXσχ-2λ3σAS)- σσXχYn)σA(n) - 2λ4 log σXn)=0.	QI)
Proof of Theorem 2 Consider the equvalent trace form of (18)
minTr{(AX - Y)TAX - Y} + λ2Tr{ATA}+
A	(22)
λ3Τr{(AAT - I)T(AAT - I)} - λ4 log |ATA|.
Note that since λ2 ≥ 0, XXT + λ2I is a symetric positive definite matrix whit all eigenvalues
non-negative, therfore it decomposes as
UXΣ2XUTX = UXΣXUTXUXΣXUTX = XXT+λ2I.
Let
A = BD,D = UXΣ-1UTX,
Define
g1 = BDXYT,g2 = BBT,	g3 = (BDDT BT)(BDDT BT)T
g4 = (BDDT BT),	g5 =log|detBDDTBT|,
Then (18) equvalently is
min-Tr{g1} + T r{g2} + λ3Tr{g3 - g4} -λ4g5.
B
(23)
(24)
(25)
(26)
Asumme that B decomposes as
UB ΣBVBT	(27)
where ΣB is a diagonal matrix with positive diagonal elements, UB is column orthogonal and VB
is orthogonal square matrix. Moreover, let the following decomposition on XYT exists
XYT = UXΣXY VXT Y , and substitute as UB = VXY , VB = UX ,	(28)
then
T r{g1} = Tr{UBΣBVTBUXΣ-X1UTXXYT} = Tr{ΣBΣ-X1ΣXY}.	(29)
The term
Tr{g2} = {BBT} = Tr{(UBΣBVBT)(UBΣBVBT)T} = Tr{Σ2B},	(30)
and
Tr{g4} = {BDDT BT} = Tr{ΣBΣ-X1Σ-X1ΣB} =Tr{Σ-X2Σ2B}.	(31)
T r{g3} ={(BDDT BT)(BDDT BT)T} = T r{Σ-X4Σ4B },	(32)
13
Under review as a conference paper at ICLR 2018
g5 =log | det AAT | = log | det DT BT BD| =
log | det UχΣχ1∑B Σχ1UX | = log | det Σχ2∑B |
Finnaly, (18) is reduced to
(33)
minσB(n) X 4λ3 ʌ σB (n) + QX (n)	"3 σB (n) - σXY(n) σp (n) - 2λ4 log σX(n), (34)
B	n=1 σX4 * (n) B	σX2 (n)	B σX (n)	σA(n)
equalling to zero the first order derivative of the objective (33) w.r.t. σB (n) and multiplaing by
σB (n) gives
4 4λ3 ʌ σB (n) + 2 σX X(n	2λ3 σB (n) - QXY(n) Qb (n) - 2λ4 = 0	(35)
σX4 (n) B	σX2 (n)	B σX(n)
A closed form solution to (34) exists and depends on the discriminint of the quartic polynomial.
Moreover, since 4 σ4λ3n) is positive a global minimum to (18) exists if and only if the decompsition
(19) exists
A.2 THE -CLOSE CLOSED FORM APPROXIMATION
Consider the equvalent trace form (26) of (18) and note that since λ2 ≥ 0, XXT + λ2I is a symetric
positive definite matrix whit all eigenvalues non-negative, therfore it decomposes as
UX Σ2XUTX = UX ΣXUTXUXΣXUTX = XXT + λ2I.	(36)
Let the following decomposition exists
UUXXYΣUXXYVTUXXY = UXXYT .	(37)
Define
A = BD, where D = UXΣ-X1UTX	(38)
Assume that B decomposes as UB ΣBVBT = B, where ΣB is a diagonal matrix with positive
diagonal elements, UB is column orthogonal and VB is orthogonal square matrix and let
UB =(UUXXYVUTXXY)T,VB =UX,	(39)
then
Tr{AXYT } =Tr{BUXΣ-X1UTXXYT} =	(40)
Tr{VUXXYUTUXXYΣBΣ-X1UUXXYUUXXYΣTVXXY}.
Consider the decomposition UB ΣB VTB of B, use Mirsky (1959) and Neumann (1937) and note
that
min max Tr{UBΣBVBT UXΣ-X1UTXXYT} ≤ min T r{ΣBΣ-X1ΣΓ},	(41)
ΣB UB,VB	X	ΣB	X
where ΣΓ is a diagonal matrix, having diagonal elements ΣΓ (n, n) = QΓ (n) = T (n, n), ∀n ∈ N
and T = UUXXYΣUXXYVUTXXY.
Note that the term T r{(AX)(AX)T } = T r{BBT } = T r{Σ2B } and as in the Apendix
subsection A.1 T r{AAT} = T r{BDDT BT} = T r{Σ2BΣ-X2}, T r{(AAT)(AAT)T} =
T r{(BDDT BT)(BDDT BT)} = T r{Σ4B Σ-X4} andlog|detAAT| = log|detDTBTBD| =
log | det UX ∑χ1∑B ∑χ1uχ | = log | det Σχ2∑B |.
Finally, the aproximation of (18) using the bound (41) is reduced to
N λ3	4 QX2 (n) - 2λ3 2 QΓ (n)	QX (n)
minσB(n)2-1 QX(n)σB(n) +	σX (n)	σB(n) - QXn)σB(n) - 2λ4 log QAH,	(42)
equalling to zero the first order derivative of the objective (42) w.r.t. QB (n) and multiplaing by
QB (n) gives
4 4λ3 ʌ σB (n)+2 QX (n)	2λ3 σB (n) - σr(n) qB (n) - 2λ4 = 0.	(43)
QX4 (n)	QX2 (n)	QX(n)
14
Under review as a conference paper at ICLR 2018
A closed form solution to (43) exists and depends on the discriminint of the quartic polynomial.
Moreover, since 4σλ3n) is positive a global minimum to (42) exists. Therfore, having the decom-
position UB ΣB VTB = B, the substitutions UB = (UUXXYVUT XY )T and VB = UX with the
solution of (42) gives the -close closed form approximative solution to problem (18) as
A=VUXXYUTUXXYΣBΣ-X1UTX,	(44)
where the bound (41) implies that the -close closed form approximative solution is a lower bound
to the solution of (18)
Appendix B.
Let yCι,ki = yCι,ki + y-t,ki, yCι,ki ∈ <M and y-iM ∈ <M. COnSiderthemeaSUre DP(X)
DP (X)= ∑
c1,c2
c16=c2
X
c1,c2
c16=c2
kyc+1,k1	yc+2,k2k1 +	kyc-1,k1	yc-2,k2k1
k1,k2	c1,c2 k1,k2
c16=c2
X |yc+1,k1|T |yc+2,k2| + X X |yc-1,k1|T|yc-2,k2|.
k1,k2	c1,c2 k1,k2
c16=c2
(45)
Let A and Y\{c1,k1} be given then problem PDP has only one varible yc1,k1. Conseqently in (45),
yci,ki is releted with only a part of the transform representations in DP (X), the rest are constants
for the reduced problem PDP , in particulary we have
|yc+1,k1|T	|yc+2,k2| + |yc-1,k1|T	|yc-2,k2| = |yc+1,k1|T dc+ + |yc-1,k1|T dc-,
c2 k2
c26=c1
where dc+ = P c2,
c26=c1
c2	k2
c26=c1
(46)
Pk2 |yc+2,k2|, dc- = P c2 Pk2 |yc-2,k2| and we abuse notation by denot-
c26=c1
ing |yc1,k1 | as the vector whose elements are the absolute values of the elements in yc1,k1.
Note that
SP (X)= E
c1,c2
c16=c2
X
c1,c2
c16=c2
simmilary as in (46) we have
kyc1,k1	yc2,k2k22
k1,k2
(yc1,k1	yc1,k1)T (yc2,k2	yc2,k2).
k1,k2
(47)
(yc1,k1	yc1,k1)T (yc2,k2	yc2,k2) =
c2	k2
c26=c1
(yc1,k1	yc1,k1)T ( Σ Σ yc2,k2	yc2,k2) = (yc1,k1	yc1,k1)T sc,
c2	k2
c26=c1
(48)
where sc =	c2	k2 yc2,k2	yc2,k2. Denote qc1,k1 = Axc1,k1 and consider the problem
c26=c1
minyc1,k1kqc1,k1 - yc1,k1k22+	(
λ0((yc+1,k1)T dc+ + (yc-1,k1)Tdc- + (yc1,k1	yc1,k1)T sc) + λ1kyc1,k1k1,
by taking the first order derivative w.r.t. yc1,k1 we have that
(yc1,k1 - qc1,k1) + λ0(sign(yc+1,k1)	dc+ + sign(yc-1,k1)	dc- + yc1,k1	sc)	(
λ1sign(yc1,k1) = 0,
take sign magnitude decomposition of yc1,k1 = sign(yc1,k1)	|yc1,k1 | then we have
sign(yc1,k1)	|yc1,k1 |	(1 + 2λ0sc) - sign(qc1,k1)	|qc1,k1|+
λ0(sign(yc+1,k1)	dc+ + sign(yc-1,k1)	dc-) + λ1sign(yc1,k1) = 0.
(49)
(50)
(51)
15
Under review as a conference paper at ICLR 2018
X
Y
log
J
DBNc(X)
DBN (X)+e
J
Io
J
D"c(X)	DBMc(Y)
1	1 ,1	1.,
log D'1 (X)+e = log DBM (Y) + e
J
It
Figure 7:	The relation for the definition of the discrimination power in the original and the transform
domain under the base models BN and BM .
Let the sign of yc1,k1, i.e. sign(yc1,k1) be equal to the sign of sign(qc1,k1), and Hadamar multiply
from the left side by sign(qc1,k1) then we have
|yc1,k1|	(1 + 2λ0sc) - |qc1,k1| + λ0(sign(qc1,k1)	sign(qc+1,k1)	dc++
sign(qc1,k1)	sign(qc-1,k1)	dc-) + λ11 = 0,
(52)
note that sign(qc1,k1)	sign(qc+1,k1) = sign(qc+1,k1) and that sign(qc1,k1)	sign(qc-1,k1)
sign(-qc-1,k1), theretofore we have
|yc1,k1 |	(1 + 2λ0sc) = |qc1,k1| - λ0(sign(qc+1,k1)	dc+ + sign(-qc-1,k1)	dc-)-
λ11,
(53)
since the magnitude might be only positive we have that |yc1,k1 | (1 + 2λ0sc) = max(|qc1,k1 | -
λ0 (sign(qc+1,k1) dc+ + sign(-qc-1,k1) dc-) - λ11,0). Denote gc = (sign(qc+1,k1) dc+ +
sign(-qc-1,k1) dc-) then the closed form solution to (49) is:
yc1,k1 =sign(Axc1,k1)	max(|Axc1,k1| - λ0gc - λ11,0)	(1 + λ0sc),	(54)
which completes the proof
Appendix C. Sensitivity analysis and interpretations
The similarity concentration measure provides possibility to measure the discriminative properties,
their deviation, increase (or decrease) and the corresponding relations between different non-linear
transform models across one domain or different domains, thereby quantifying their quality w.r.t.
the discriminative properties.
C.1 Sensitivity analysis w.r.t. the similarity concentration measures
An illustration about the definition of discriminative power given by a diagram is shown in Figure
7.
To measure the ability for an increase in discriminative properties by a non-linear transform9 we
first have to define a notion for the discriminative properties on a data set under different non-
linear transform models. Therefore, first we introduce the ”special” base models and then analyze
the properties of the similarity concentration measures under the change in model parameter and
the relation between the base model and the proposed non-linear transform model defined by a
parameter set P = {A ∈ <M×N, τ ∈ <M}.
Any data set X in the original domain might have a transform model with parameters BN = {Ao ∈
<N ×N, τ = 0 ∈ <+N }, if Ao = I ∈ D+N we refer to it as a base original model. Similarly as in
the original domain, any data set Y in the transform domain might have a transform model with
parameters BM = {At ∈ <M×M, τ = 0 ∈ <+M}, if At = I ∈ D+M we refer to it as a base
transform model. Any base model, defined ether in the original domain BN or in the transform
domain BM, has domain equal to the co-domain, since xc,k = TBN (xc,k) and yc,k = TBM (yc,k)
holds trivially, for the respective sets of parameters BN = {Ao = I ∈ D+N , τ = 0 ∈ <+N} and
BM = {At = I ∈ D+M , τ = 0 ∈ <+M }. This is illustrated with a diagram shown in Figure 8.
9Instead of using the term non-liner transform model as defined by (1) or (2) for short we just use the term
model.
16
Under review as a conference paper at ICLR 2018
original domain
X
N
lTBN
X
transform domain
transform domain
-T-→P	Y
lTBM
Y
transform domain
Figure 8:	The original and the transform domains under a non-linear transforms with a set of param-
eters BN, P and BM, note that for TBN and TBM the original and the transform domains are the
same.
A base original model provides a possibility to compare it with any other non-linear transform model
with parameters P = {A ∈ <M×N, τ ∈ <+M}. Additionally, note that for BM = {At = I ∈
DM, τ = 0 ∈ <M} We have that DBMc(Y) = DP C(X) and that DBM (Y) = DP (X). It implies
that the similarity concentrations can be analyzed as a function in the original domain under model
P or in the transform domain under model BM . The main relations considering the preservation
of change in the similarity concentration betWeen tWo models, defined not necessary in the same
domain are stated by Lemma 1.
Lemma 1: The non-linear transform model (2) totally preserves the information in the change for
the similarity concentration for a data set X w.r.t. a small change in the parameters of the models
BN, BM and P f ∣∣δo∣∣* = 0 and ∣∣δtk* = 0 as
(R1) : A
dDC(X)l	AdDB (X)
∂Ao	IAo=I+	d Ao	lAo=I
dDP,c(X)
d A
+ 以F + δo
dA o
(R2) : A
<∂DP,c(X)
(d A
l ∂DP (X) ʌ T	dD然(Y)I	, ∂DB1μ (Y)i
+ ^"∂^	=	∂At	IAt=I+	∂At	IAt=I
(55)
(R3) : AδoT
dD 黑(Z)
IAt=I +
∂DB1M (Z)
∂At
IAt=I + δtT,
where
dD 黑(Y)
∂DP,c(X)	∂DP (X)
∂A	+
dA
ΣΣyc1,k1xcT,k +yc,kxcT1,k1
c,c1 k,k1
k6=k1
IAt=I +
∂DB (Y)
dAt
IAt=I
ΣΣyc1,k1ycT,k + yc,kycT1,k1
c,c1 k,k1
k6=k1
∂D 黑(Z)
IAt=I +
dD片(Z)
IAt=I =ΣΣzc1,k1zc,k + zc,k zc1,k1
c,c1 k,k1
k6=k1
∂D 黑(X)
∂Ao 1A
=I =ΣΣxc,kxcT,k1 + xc,k1xcT,k
c k,k1
(56)
o
∂DB (X),
∂Ao	1A
o
=I =	xc,k xc1,k1 + xc1,k1xc,k
c,c1,c6=c1 k,k1
δo =ΣΣzc1,k1xc,k + zc,k xc1,k1
c,c1 k,k1
k6=k1
δt =ΣΣzc1,k1yc,k + zc,k yc1,k1
c,c1 k,k1
k6=k1
17
Under review as a conference paper at ICLR 2018
The proof is given in Appendix C.2.
The terms zc,k represent the non-linear transform error vectors that appear in the model Axc,k =
yc,k + zc,k as a result of applying an element-Wise non-liner operation Hτ to Axc,k, i.e., yc,k =
Hτ (Axc,k). As an example in the sparsifying transform model zc,k is the ”loss of information”, that
is the information about the values of the elements in Axc,k that are discarded. The terms δo and δt
correlate the errors zc,k With the original data xc1,k1 and transform data yc1,k1, respectively. Note
that if there is no loss of information (in the earlier example it means that there is no thresholding
and just a simple linear transform model is used) then δo = 0 and δt = 0. Moreover, δo and δt bear
important information about the discriminative properties in the transform domain.
1	∂dP c(X)	, ∂dP (x)	1	…	..∣ .	I .
The terms ——∂A—— and 一∂A	represent the change of the similarity concentrations under m-
finitesimally small change of the parameter A from the model P . The terms
∂DBNC(X)
|Ao=I and
aAO
∂DBn (X) .	1	1一	.	. Jt	TllL	「 F
—∂A~~- ∣ao=i have dual interpretation. Assuming metric Ao = I, then the first one is considered
as a change of the similarity concentrations under infinitesimally small change of the space metric,
or equivalently under small metric perturbation. Conversely, assuming the data samples are dis-
tributed under a Gaussian distribution With parameters identity covariance matrix and zero mean,
V	L 丁、 I	∂DB (X) ,	FdDBN (X) .	1	1	1
i.e. Xc,k 〜N(μ = 0, Σ = I), then —^A——-1 ao=i and —∂a^^-1 ao=I represent the change of the
similarity concentrations under small change in the assumption aWay from a Gaussian distribution.
Equation (R1) relates the base transform for the original domain BN With any arbitrary transform
defined in the original domain P. The relation (R2) is a result about the preservation of change
in the similarity concentration betWeen tWo models BM and P defined on tWo different domains.
Whereas (R3) gives the preservation of change in the similarity concentration betWeen the error in
the transform domain.
The next result highlights the relation between: the linear projection (by the linear map A that
appears in the model P) of the change in the similarity concentration under the model BN in the
original domain and the change of the similarity concentration under the model BM in the transform
domain.
Tk 1 +∙	∙ t ∙4	,ι F	∂DBNc(X)	∂DBN (X)
This relation exists independently for —∂A---∣ao=i and —∂A——∣ao=i, nevertheless, We Win
define the summarized and the independent versions. Therefore, first we define
∂J'∖ (X)
∂DB1Nc(X)	∂DBN (X)	」
-∂Ao- IAO=I +	∂AO- IAO=I and
rewrite(RI) asA JAo) |AO=I - δo =
∂J'∖ (Y)
∂At
d%c
∂A
aAO
|Ao=I
.	∂DBNXY)I	∂DB (Y)I	F
IAt = I =	-∂At- IAt=I +	∂At- IAt=I and
(X)	∂DP (X) 1	1	∂DP c(X)	∂DP (X)
—+ -ɪ2, then replace	'∂A	+ ~∂^
in (R2) by the same term in (R1), use (R3), reorder and we have the following result.
Lemma 2: For fixed τ any non-linear transform model (2) preservers the information in the change
of similarity concentrations w.r.t. a small change in A by
∂ J (V)
∂ J (Y)
(R心 AdJ'1 (x)∣	AT
(R4) :A^∂A^ IAO=IA
|At=I
|At=I + ξc + ξ,
(R5) :A
∂D'ι,c(X)
(R6) :A
∂Ao
∂D'ι (X)
∂ Ao
|Ao=IAT
|Ao=IAT
∂D'ι (V)
∂At
∂D'ι (V)
|At=I
∂D'ι,c(Y)
∂At
|At=I
∂At
∂D'ι (Y)
|At=I + ξc,
|At=I + ξ,
(57)
where
V =AX
ξc + ξ =
∂J (Z)
∂At
|At=I +
∂ J (Y; Z)
∂J'1 (Z)
|At=I
∂D¾Z)
|At=I +
∂At
∂dB (Z)
|At=I
|At=I
(58)
∂J (Y; Z)
|At=I = δt + δtT
18
Under review as a conference paper at ICLR 2018
The expressions (R4) actually relate the metric in the original domain under the model BN
to the induced metric in the transform domain for the model BM, with induction done by the
model P with parameter set {A ∈ <M×N, τ ∈ <+M}. Moreover, the model P might describe
a transform domain with a non-smooth manifold. Since the manifolds of the original and the
transform domain under the models BN and BM are smooth the analysis of their relations re-
veals insights about the relation between the manifolds under the models BN and P . The terms
JA(Z) ∣At=ι and JdAYZ) ∣a==i carry out the information about the breaks and the discontinu-
ities of the regularity and smoothness in the manifold induced by the model P . Also we note
J ， ∂nJ'1 (X) I	Adn-IJ (X) I	二	∂n J'λ (Y)I	Adn-IJ'、(Y)I	J JC
that dnA:	∣Ao=ι	= 4 dn-ιA:	∣Ao=ι	and gnAt ∣At=ι	= 4 gn—At	∣At=i	therefore,
∂J'1 (X) I 0	ι d2J'1 (X) I °	. dJ'1 (Y) I	ι d2J'1 (Y) I	∙ ml ∙ +
dAO |Ao = I = 4 d2AO |Ao=I and dAt |At = I = 4 d?At |At=I might be interpreted as
Fisher information matrices evaluated at Ao = I ∈ DN and At = I ∈ DM . Furthermore, if
k JA(Z) ∣At=ιk* = 0 and k JdAYz)履=曰心 =0, then (R4) in information geometry is seen as
change of coordinates on a manifold, where the intrinsic properties of curvature remain unchanged
under different parametrization.
Appendix C.2
Note that for the model Pt we have that
y =T (Ax) = max(Ax - τ, 0) - max(-Ax - τ, 0),
q =T (Ax) = max(Ag - τ, 0) - max(-Ag - τ, 0),
(59)
since
sign(a) max(|a| - b, 0) = max(a - b, 0) - max(-a - b, 0),	(60)
The first order derivative of the divergence DPt (x; g) w.r.t. the parameter A is:
dDP(X; g)=
dA	(61)
∂(max(Ax - τ, 0)T max(Ag - τ, 0))	∂(max(-Ax - τ, 0)T max(-Ag - τ, 0))
∂A	+	∂A
we assume that the threshold parameter τ is chosen such that the vector |Ax| - τ (or for any other
q, the vector |Aq| - τ ) has least one non-zero element, then
d(max(AX - T, 0)t maχ(Ag -T, O)) = ma*	-T, o)χT + maχ(Aχ-T, 0)gT,	(62)
∂A
and
∂(max(-AX - T, 0)T max(-Ag - T, 0))
∂A	=	(63)
- max(-Ag - T, 0)XT - max(-AX - T, 0)gT,
combining (62) and (63) we have that
	dDP(x; g)= ∂A max(Ag - T, 0)XT + max(AX - T, 0)gT - (max(-Ag + T, 0)xT + max(-Ax - T, 0)gT) = qxT + ygT	(64)
where	y(m) = sign(aTmx) max(|aTmx| - τ (m), 0) q(m) = sign(aTmg) max(|aTmg| - τ (m), 0),	(65)
∀m ∈ M.
19
Under review as a conference paper at ICLR 2018
Similarity, note that for the model P0o = {Ao , τ = 0} we have that
xo =T P0o (Aox) =
sign(Aox)	max(|Aox| - 0, 0) =
max(Aox - 0, 0) - max(-Aox - 0, 0),
go =T P0o (Aog) =
sign(Aog)	max(|Aog| - g,0) =
max(Aog - 0, 0) - max(-Aog - 0, 0).
(66)
The first order derivative of the divergence DP0 (x; g) w.r.t Ao is:
∂DP0 (x; g)
∂ Ao
xogT + goxT ,
(67)
note that at Ao = I, P0t = BN and we have that
dDB(x； g),	tl t
∂ Ao IAO=I = xg + gx
(68)
Also for the model P0t = {At , τ = 0} we have that
yt =T P0t (Aty) =
sign(Aty) max(|Aty| - 0,0) =
max(At y - 0, 0) - max(-Aty - 0, 0),
qt =T P0t (Atq) =
sign(Atq) max(|Atq| - q, 0) =
max(At q - 0, 0) - max(-Atq - 0, 0).
(69)
The first order derivative of the divergence D' 0 (y; q) w.r.t At is:
∂At
T
ytqT
+ qtyT,
(70)
note that at At = I, P0t = BM and we have that
Consider the following
yqT + qy
(71)
T
Ag = q + z1/xT → AxgT = ygT + z1gT
Ax = y + z2/gT AgxT = qxT + z2xT	(72)
A(xgT +gxT) = ygT+qxT+z1gT+z2xT
where
z1 = Ax - sign(Ax) max(|Ax| - τ1, 0)
z2 = Ag - sign(Ag) max(|Ag| - τ1, 0)
(73)
A closer look at (72) reveals us that
A dDBN(X； g) |	_ = dDP (X； g) +
∂Ao IAO=I=	∂A + o
20
Under review as a conference paper at ICLR 2018
where
δoz1,z2 = Z1gT + Z2xT
(75)
By similar construction applied to the rest of the pairs of data samples we have the result:
A ∂ dD 黑(X) 1	+ dDB (X) |
[∂Ao	IAo=I+	∂ Ao	lAo=I
∂DPι,c(X)	∂DPι (X)
一∂A — +
∂A
+ δo
(76)
Note that
Ax = y + Z1/qT → AxqT = yqT + Z1qT
Ag = q + Z2/yT AgyT = qyT + Z2yT	(77)
A(xyT +qgT) = yqT +qyT +Z1qT +Z2yT,
where
Z1= Ax - sign(Ax) max(IAxI - τ1, 0)
Z2 = Ag - sign(Ag) max(IAgI - τ1, 0)
(78)
A closer look at (77) reveals us
where
(79)
(80)
By similar construction applied to the rest of the pairs of data samples we have the result:
A ∂dDPtc(χ) , ∂Dp(X)
[∂ A	+	∂A
∂ ∂D 黑(Y)
〈∂At
IAt=I +
∂DBM (Y)
∂ At	IAt=I
+ δt
(81)
Note that
Ax = y + z1/
Ag = q + z2/
T2T
ZZ
T2T
ZZ
xg
AA
yz2T + z1z2T
qz1T + z2z1T
(82)
→
where
A(xz2T + qz1T) = yz2T + qz1T + z1z2T + z2z1T ,
z1 = Ax - sign(Ax) max(|Ax| - τ1, 0)
z2 = Ag - sign(Ag) max(|Ag| - τ1, 0)
(83)
A closer look at (82) reveals us
A(δzι,z2)T = dD1Mf；Z2)∣At=I + (δZ1,z2)T.	(84)
∂At
where
δtz1 ,z2 = Z1qT + Z2yT	(85)
By similar construction applied to the rest of the pairs of data samples we have the result:
AδoT=( ∂D⅛kt=I + ∂D⅛ IAt=I) + δT □	(86)
∂At	∂At
21
Under review as a conference paper at ICLR 2018
§6)6。一
Figure 9: The ratio C1/C2 of the similarity concentrations C1 = DB C(Y) and C2 = DB (Y)
and the discrimination power log(C1/C2) = It for randomly chosen subsets from all of the used
databases under a non-linear transform with transform dimension M = 19000 and varying thresh-
olding parameter τ = λ1.
(eO二。)6。一
30
20
30
70
Appendix D
The result in by Lemma 2 (58) decomposes on the contribution components for simmilarity and on
the contributiong components for dissimilarity
∂J'1 (AX)I	∂ J (AX)	∂ J (AX)
∂A|At = I =	∂A|At=I|s-------∂A|At=I|d + ξcL -&|d + ξl - ξ3
t	t	t	(87)
Moreover, w.r.t. the simmilarity concentrations we have the following decompositions
Tr{ "BAAX) "=1} =D二(Y) + Tr{ξc∣s} = D^(X) + Tr{ξ,}
∂DB∂MA(At X)	M	(88)
Tr{	'∂At	∣At=ι} =DBi (Y) + Tr{ξ∣s} = DP(X) + "长鼾，
therfore we have the following bounds
a : Tr{A %(X) ∣Ao=iAt }≤ D^(X) ≤ Tr{ '写C(AX) "=ι∣s} = D 黑(AX)
∂Ao	∂At
∂DBN (X)	∂DBM (AX)	M
b ： Tr{A 1 ' ∣Ao=iAt} ≤ DP(X) ≤ Tr{∣a,=i∣s} = DB (AX)
∂Ao	∂At
(89)
Additionaly, we have that c
:λmin(ATA)Tr{dDA(X)∣Ao=ι} ≤ Tr{Ad¾X)∣a0=iAt},
where λmin (ATA) is the minimum singlular value of the matrix ATA. Taking the logarithm of the
.DP C(X)	, 1	,	，.， ILl
ratio p14∖ using the bounds a, b and C We arrive at the desired result □
D'i (X)+ '
Appendix E
The exact steps of the proposed non-linear transform learning are described by Algorithm 1.
Appendix F
The ratio C1/C2 between the similarity concentrations C1 = DBI(Y) and C2 = DBM (Y) and
the discrimination poWer log(C1/C2) = It on subsets of the used databases after applying a non-
linear transform with transform dimension M = 19000 and varying the thresholding parameter
τ = λ1 is shown in Figure 9. We used 70 different values for the parameter λ, sampled uniformly
from the interval 0, (max1≤C≤C,1≤k≤K max1≤m≤M |aTmxC,k|) . The results were obtained using
22
Under review as a conference paper at ICLR 2018
Algorithm 1 Non-linear transform learning algorithm
Input χ, λ0, λι, λ2, λ3, λ4
A — iniCialize
repeat
DISCRIMINATIVE ENCODING closed form solution per data sample
Y — AX
repeat
for ∀c ∈ C do
d- - P ci PkI ycι,ki, d+ - P ci PkI y+ι,ki
c16=c	c16=c
sc -E ci ∑ki yc1,ki Θ yc1,ki
ci6=c
end for
and
for ∀c ∈ C and ∀k ∈ K do
g - sign(max(Axc,k, 0))	dc+ + sign(max(-Axc,k, 0)) dc-
yc,k - sign(Axc,k)	max (Axc,k - λ0g+λi1,0)	(1 + 2λ0sc)
end for
until convergence
TRANSFORM UPDATE -close closed form solution
UX Σ2X UTX - XXT + λ2I and UUXXY ΣUXXYVUTXXY - UTX XYT
minσA(n) σX4σA(n) + (σXX-T3) σA(n) - σXnσA(n) - 2λ4 log σX(n)
where σΓ(n) - T(n, n), T - UUXXY ΣUXXY UTUXXY , ∀n ∈ N
A -VUXXYUTUXXY ΣAΣ-XiUTX
until convergence
Output A, Y
O
sδ
sδ
20
4|)
60
20
4I)
60
O-2-4
(eo∖8)6。一
-6
0	20	、40	60
λ
0 5 0
- 1
(§8)6。一 一
0	20 λ 40	60
Figure 10: The ratio of the similarity concentrations similarity concentrations C1 = DBMc(Y)
and C2 = DBM (Y) and the discrimination power log(C 1 /C2) = It for the Extended Yale B and
MNIST databases under non-linear transforms having different transform dimension M and varying
thresholding parameter τ = λ1.
a non-linear transform learned with one value for parameter λ for all the databases. Since all the
databases have different variabilities and the amount of available data is different, this result suggest
that per different database there should be different optimal values for the parameter λ.
23
Under review as a conference paper at ICLR 2018
The results about the ratio C1/C2 between the similarity concentrations C1 = DBMc(Y) and
C2 = DBM (Y) and the discrimination power log(C 1/C2) = It on the Extanded-Yale-B and
the MNIST data sets after applying a non-linear transform with transform dimensions M =
{100, 500, 1500, 4000} and M = {1000, 4000, 6000, 12000}, and varying the thresholding param-
eter τ = λ1 are shown in Figure 10. We again used 70 different values for the parameter λ,
sampled uniformly from the interval 0, (max1≤c≤C,1≤k≤K max1≤m≤M |aTmxc,k|) . The results
were obtained using a non-linear transforms learned with optimally choose values (by using cross-
validation) of the parameter λ for the two different databases. As expected, we can see that the
extreme points of the ratio between the '*-norms C1 and C2 of the similarity concentrations and
the discrimination power is around the optimal values of the parameter τ = λ1.
References
Yoshua Bengio, Aaron C. Courville, and Pascal Vincent. Unsupervised feature learning and deep
learning: A review and new perspectives. CoRR, abs/1206.5538, 2012. URL http://arxiv.
org/abs/1206.5538.
Sijia Cai, Wangmeng Zuo, Lei Zhang, Xiangchu Feng, and Ping Wang. Support vector guided
dictionary learning. In Computer Vision - ECCV 2014 - 13th European Conference, Zurich,
Switzerland, September 6-12, 2014, Proceedings, PartIV, pp. 624-639, 2014.
Mehrdad J. Gangeh, Ahmed K. Farahat, Ali Ghodsi, and Mohamed S. Kamel. Supervised dic-
tionary learning and sparse representation-a review. CoRR, abs/1502.05928, 2015. URL
http://arxiv.org/abs/1502.05928.
A. S. Georghiades, P. N. Belhumeur, and D. J. Kriegman. From few to many: Illumination cone
models for face recognition under variable lighting and pose. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 23:643-660, 2001.
Huimin Guo, Zhuolin Jiang, and Larry S. Davis. Discriminative dictionary learning with pairwise
constraints. In Computer Vision - ACCV 2012 - 11th Asian Conference on Computer Vision,
Daejeon, Korea, November 5-9, 2012, Revised Selected Papers, Part I, pp. 328-342, 2012.
Huimin Guo, Zhuolin Jiang, and Larry S. Davis. Discriminative Dictionary Learning with Pairwise
Constraints, pp. 328-342. Springer Berlin Heidelberg, Berlin, Heidelberg, 2013. ISBN 978-3-
642-37331-2. doi: 10.1007/978-3-642-37331-2_25. URL https://doi.org/10.1007/
978-3-642-37331-2_25.
Rui Jiang, Hong Qiao, and Bo Zhang. Efficient fisher discrimination dictionary learning. Signal
Process., 128(C):28-39, November 2016. ISSN 0165-1684. doi: 10.1016/j.sigpro.2016.03.013.
URL http://dx.doi.org/10.1016/j.sigpro.2016.03.013.
Zhuolin Jiang, Zhe Lin, and L. S. Davis. Learning a discriminative dictionary for sparse coding
via label consistent k-svd. In Proceedings of the 2011 IEEE Conference on Computer Vision and
Pattern Recognition, CVPR ’11, pp. 1697-1704, Washington, DC, USA, 2011. IEEE Computer
Society. ISBN 978-1-4577-0394-2. doi: 10.1109/CVPR.2011.5995354. URL http://dx.
doi.org/10.1109/CVPR.2011.5995354.
Zhuolin Jiang, Zhe Lin, and Larry S. Davis. Label consistent K-SVD: learning a discriminative
dictionary for recognition. IEEE Trans. Pattern Anal. Mach. Intell., 35(11):2651-2664, 2013.
doi: 10.1109/TPAMI.2013.88. URL http://dx.doi.org/10.1109/TPAMI.2013.88.
Kenneth Kreutz-Delgado, Joseph F. Murray, Bhaskar D. Rao, Kjersti Engan, Te-Won Lee, and Ter-
rence J. Sejnowski. Dictionary learning algorithms for sparse representation. Neural Comput.,
15(2):349-396, February 2003. ISSN 0899-7667. doi: 10.1162/089976603762552951. URL
http://dx.doi.org/10.1162/089976603762552951.
Yann Lecun and Corinna Cortes. The MNIST database of handwritten digits. URL http://
yann.lecun.com/exdb/mnist/.
24
Under review as a conference paper at ICLR 2018
Yann LeCun, FU Jie Huang, and Leon Bottou. Learning methods for generic object recognition with
invariance to pose and lighting. In Proceedings of the 2004 IEEE Computer Society Conference
on Computer Vision and Pattern Recognition, CVPR’04, pp. 97-104, Washington, DC, USA,
2004. IEEE Computer Society.
Yann LeCun, David G. Lowe, Jitendra Malik, Jim Mutch, Pietro Perona, and Tomaso Poggio. Object
Recognition, Computer Vision, and the Caltech 101: A Response to Pinto et al. Technical report,
March 2008.
Yang Liu, Wei Chen, Qingchao Chen, and Ian J. Wassell. Support discrimination dictionary learn-
ing for image classification. In Computer Vision - ECCV 2016 - 14th European Conference,
Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II, pp. 375-390, 2016.
Julien Mairal, Francis R. Bach, Jean Ponce, Guillermo Sapiro, and Andrew Zisserman. Supervised
dictionary learning. In Advances in Neural Information Processing Systems 21, Proceedings of
the Twenty-Second Annual Conference on Neural Information Processing Systems, Vancouver,
British Columbia, Canada, December 8-11, 2008, pp. 1033-1040, 2008.
Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro. Online dictionary learning for sparse
coding. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML
’09, pp. 689-696, New York, NY, USA, 2009. ACM. ISBN 978-1-60558-516-1. doi: 10.1145/
1553374.1553463. URL http://doi.acm.org/10.1145/1553374.1553463.
Reid Mark. Generalization Bounds, pp. 447-454. Springer US, Boston, MA, 2010.
A. Martinez and R. Benavente. The ar face database. Technical Report 24, Computer Vision
Center, Jun 1998. URL "http://www.cat.uab.cat/Public/Publications/1998/
MaB1998".
L. Mirsky. On the trace of matrix products. 20(3-6):171-174, 1959. ISSN 2167-3888.
Sameer A. Nene, Shree K. Nayar, and Hiroshi Murase. Columbia object image library (coil-20.
Technical report, 1996.
J. Von Neumann. Some matrix-inequalities and metrization of matrix-space. Tomskii Univ. Rev., 1:
286-300, 1937.
D. NiSter and H. Stewenius. Scalable recognition with a vocabulary tree. In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), volume 2, pp. 2161-2168, June 2006. oral
presentation.
Neal Parikh and Stephen Boyd. Proximal algorithms. Found. Trends Optim., 1(3):127-239, January
2014. ISSN 2167-3888.
I. Ramirez, P. Sprechmann, and G. Sapiro. Classification and clustering via dictionary learning
with structured incoherence and shared features. In 2010 IEEE Computer Society Conference on
Computer Vision and Pattern Recognition, pp. 3501-3508, June 2010. doi: 10.1109/CVPR.2010.
5539964.
Saiprasad Ravishankar and Yoram Bresler. Doubly sparse transform learning with convergence guar-
antees. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP
2014, Florence, Italy, May 4-9, 2014, pp. 5262-5266. IEEE, 2014. doi: 10.1109/ICASSP.2014.
6854607. URL http://dx.doi.org/10.1109/ICASSP.2014.6854607.
Ron Rubinstein and Michael Elad. Dictionary learning for analysis-synthesis thresholding. IEEE
Trans. Signal Processing, 62(22):5962-5972, 2014.
Ron Rubinstein, Alfred M. Bruckstein, and Michael Elad. Dictionaries for sparse representation
modeling. Proceedings of the IEEE, 98(6):1045-1057, 2010.
Ron Rubinstein, Tomer Peleg, and Michael Elad. Analysis K-SVD: A dictionary-learning algorithm
for the analysis sparse model. IEEE Trans. Signal Processing, 61(3):661-677, 2013.
25
Under review as a conference paper at ICLR 2018
Sumit Shekhar, Vishal M. Patel, and Rama Chellappa. Analysis sparse coding models for image-
based classification. In 2014 IEEE International Conference on Image Processing, ICIP 2014,
Paris, France, October 27-30, 2014, pp. 5207-5211. IEEE, 2014. ISBN 978-1-4799-5751-4.
doi: 10.1109/ICIP.2014.7026054. URL http://dx.doi.org/10.1109/ICIP.2014.
7026054.
Ali Taalimi, Shahab Ensafi, Hairong Qi, Shijian Lu, Ashraf A. Kassim, and Chew Lim Tan. Mul-
timodal dictionary learning and joint sparse representation for hep-2 cell classification. pp. 308-
315, 2015.
Vladimir N. Vapnik. The Nature of Statistical Learning Theory. Springer-Verlag New York, Inc.,
New York, NY, USA, 1995. ISBN 0-387-94559-8.
T. H. Vu, H. S. Mousavi, V. Monga, U. K. A. Rao, and G. Rao. Dfdl: Discriminative feature-oriented
dictionary learning for histopathological image classification. In 2015 IEEE 12th International
Symposium on Biomedical Imaging (ISBI), pp. 990-994, April 2015. doi: 10.1109/ISBI.2015.
7164037.
Tiep Huu Vu and Vishal Monga. Fast low-rank shared dictionary learning for image classification.
CoRR, abs/1610.08606, 2016. URL http://arxiv.org/abs/1610.08606.
Yong Xu, Yuping Sun, Yuhui Quan, and Bo Zheng. Discriminative structured dictionary learning
with hierarchical group sparsity. Comput. Vis. Image Underst., 136(C):59-68, July 2015. ISSN
1077-3142. doi: 10.1016/j.cviu.2015.01.006. URL http://dx.doi.org/10.1016/j.
cviu.2015.01.006.
M. Yang, L. Zhang, X. Feng, and D. Zhang. Fisher discrimination dictionary learning for sparse
representation. In 2011 International Conference on Computer Vision, pp. 543-550, Nov 2011a.
doi: 10.1109/ICCV.2011.6126286.
Meng Yang, Lei Zhang, Xiangchu Feng, and David Zhang. Fisher discrimination dictionary learning
for sparse representation. In Proceedings of the 2011 International Conference on Computer
Vision, ICCV ’11, pp. 543-550, Washington, DC, USA, 2011b. IEEE Computer Society. ISBN
978-1-4577-1101-5. doi: 10.1109/ICCV.2011.6126286. URL http://dx.doi.org/10.
1109/ICCV.2011.6126286.
26