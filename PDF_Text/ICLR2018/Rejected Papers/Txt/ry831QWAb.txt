Under review as a conference paper at ICLR 2018
Block-Normalized Gradient Method: An Em-
pirical S tudy for Training Deep Neural Net-
WORK
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we propose a generic and simple strategy for utilizing stochastic
gradient information in optimization. The technique essentially contains two con-
secutive steps in each iteration: 1) computing and normalizing each block (layer)
of the mini-batch stochastic gradient; 2) selecting appropriate step size to update
the decision variable (parameter) towards the negative of the block-normalized
gradient. We conduct extensive empirical studies on various non-convex neu-
ral network optimization problems, including multi layer perceptron, convolution
neural networks and recurrent neural networks. The results indicate the block-
normalized gradient can help accelerate the training of neural networks. In partic-
ular, we observe that the normalized gradient methods having constant step size
with occasionally decay, such as SGD with momentum, have better performance
in the deep convolution neural networks, while those with adaptive step sizes, such
as Adam, perform better in recurrent neural networks. Besides, we also observe
this line of methods can lead to solutions with better generalization properties,
which is confirmed by the performance improvement over strong baselines.
1	Introduction
Continuous optimization is a core technique for training non-convex sophisticated machine learning
models such as deep neural networks (Bengio, 2009). Compared to convex optimization where a
global optimal solution is expected, non-convex optimization usually aims to find a stationary point
or a local optimal solution of an objective function by iterative algorithms. Among a large volume
of optimization algorithms, first-order methods, which only iterate with the gradient information
of objective functions, are widely used due to its relatively low requirement on memory space and
computation time, compared to higher order algorithms. In many machine learning scenarios with
large amount of data, the full gradient is still expensive to obtain, and hence the unbiased stochastic
version will be adopted, as it is even more computationally efficient.
In this paper, we are particularly interested in solving the deep neural network training problems
with stochastic first order methods. Compared to other non-convex problems, deep neural network
training additionally has the following challenge: gradient may be vanishing and/or exploding. More
specifically, due to the chain rule (a.k.a. backpropagation), the original gradient in the low layers
will become very small or very large because of the multiplicative effect of the gradients from the
upper layers, which is usually all smaller or larger than 1. As the number of layers in the neural
network increases, the phenomenon of vanishing or exploding gradients becomes more severe such
that the iterative solution will converge slowly or diverge quickly.
We aim to alleviate this problem by block-wise stochastic gradient normalization, which is con-
structed via dividing the stochastic gradient by its norm. Here, each block essentially contains the
variables of one layer in a neural network, so it can also be interpreted as layer-wise gradient normal-
ization. Compared to the regular gradient, normalized gradient only provides an updating direction
but does not incorporate the local steepness of the objective through its magnitude, which helps to
control the change of the solution through a well-designed step length. Intuitively, as it constrains
the magnitude of the gradient to be 1, it should to some extent prevent the gradient vanishing or
exploding phenomenon. In fact, as showed in (Hazan et al., 2015; Levy, 2016), normalized gra-
1
Under review as a conference paper at ICLR 2018
dient descent (NGD) methods are more numerically stable and have better theoretical convergence
properties than the regular gradient descent method in non-convex optimization.
Once the updating direction is determined, step size (learning rate) is the next important compo-
nent in the design of first-order methods. While for convex problems there are some well studied
strategies to find a stepsize ensuring convergence, for non-convex optimization, the choice of step
size is more difficult and critical as it may either enlarge or reduce the impact of the aforementioned
vanishing or exploding gradients.
Among different choices of step sizes, the constant or adaptive feature-dependent step sizes are
widely adopted. On one hand, stochastic gradient descent (SGD) + momentum + constant step size
has become the standard choice for training feed-forward networks such as Convolution Neural Net-
works (CNN). Ad-hoc strategies like decreasing the step size when the validation curve plateaus are
well adopted to further improve the generalization quality. On the other hand, different from the
standard step-size rule which multiplies a same number to each coordinate of gradient, the adap-
tive feature-dependent step-size rule multiplies different numbers to coordinates of gradient so that
different parameters in the learning model can be updated in different paces. For example, the adap-
tive step size invented by (Duchi et al., 2011) is constructed by aggregating each coordinates of
historical gradients. As discussed by (Duchi et al., 2011), this method can dynamically incorporate
the frequency of features in the step size so that frequently occurring coordinates will have a small
step sizes while infrequent features have long ones. The similar adaptive step size is proposed in
(Kingma & Ba, 2014) but the historical gradients are integrated into feature-dependent step size by
a different weighting scheme.
In this paper, we propose a generic framework using the mini-batch stochastic normalized gradi-
ent as the updating direction (like (Hazan et al., 2015; Levy, 2016)) and the step size is either
constant or adaptive to each coordinate as in (Duchi et al., 2011; Kingma & Ba, 2014). Our frame-
work starts with computing regular mini-batch stochastic gradient, which is immediately normalized
layer-wisely. The normalized version is then plugged in the constant stepsize with occasional decay,
such as SGD+momentum, or the adaptive step size methods, such as Adam (Kingma & Ba, 2014)
and AdaGrad (Duchi et al., 2011). The numerical results shows that normalized gradient always
helps to improve the performance of the original methods especially when the network structure is
deep. It seems to be the first thorough empirical study on various types of neural networks with this
normalized gradient idea. Besides, although we focus our empirical studies on deep learning where
the objective is highly non-convex, we also provide a convergence proof under this framework when
the problem is convex and the stepsize is adaptive in the appendix. This convergence under the
non-convex case will be a very interesting and important future work.
The rest of the paper is organized as follows. In Section 2, we briefly go through the previous work
that are related to ours. In Section 3, we formalize the problem to solve and propose the generic
algorithm framework. In Section 4, we conduct comprehensive experimental studies to compare the
performance of different algorithms on various neural network structures. We conclude the paper
in Section 5. Finally, in the appendix, we provide a concrete example of this type of algorithm and
show its convergence property under the convex setting.
2	Related Work
A pioneering work on normalized gradient descent (NGD) method was by Nesterov (Nesterov,
1984) where it was shown that NGD can find a e-optimal solution within O(±) iterations when the
objective function is differentiable and quasi-convex. Kiwiel (Kiwiel, 2001) and Hazan et al (Hazan
et al., 2015) extended NGD for upper semi-continuous (but not necessarily differentiable) quasi-
convex objective functions and local-quasi-convex objective functions, respectively, and achieved
the same iteration complexity. Moreover, Hazan et al (Hazan et al., 2015) showed that NGD’s
iteration complexity can be reduced to O(ɪ) if the objective function is local-quasi-convex and
locally-smooth. A stochastic NGD algorithm is also proposed by Hazan et al (Hazan et al., 2015)
which, if a mini-batch is used to construct the stochastic normalized gradient in each iteration, finds
e-optimal solution with a high probability for loCany-quasi-convex functions within O(±)itera-
tions. Levy (Levy, 2016) proposed a Saddle-Normalized Gradient Descent (Saddle-NGD) method,
which adds a zero-mean Gaussian random noise to the stochastic normalized gradient periodically.
When applied to strict-saddle functions with some additional assumption, it is shown (Levy, 2016)
2
Under review as a conference paper at ICLR 2018
that Saddle-NGD can evade the saddle points and find a local minimum point approximately with a
high probability.
Analogous yet orthogonal to the gradient normalization ideas have been proposed for the deep neu-
ral network training. For example, batch normalization (Ioffe & Szegedy, 2015) is used to address
the internal covariate shift phenomenon in the during deep learning training. It benefits from making
normalization a part of the model architecture and performing the normalization for each training
mini-batch. Weight normalization (Salimans & Kingma, 2016), on the other hand, aims at a repa-
rameterization of the weight vectors that decouples the length of those weight vectors from their
direction. Recently (Neyshabur et al., 2015) proposes to use path normalization, an approximate
path-regularized steepest descent with respect to a path-wise regularizer related to max-norm regu-
larization to achieve better convergence than vanilla SGD and AdaGrad. Perhaps the most related
idea to ours is Gradient clipping. It is proposed in (Pascanu et al., 2013) to avoid the gradient explo-
sion, by pulling the magnitude of a large gradient to a certain level. However, this method does not
do anything when the magnitude of the gradient is small.
Adaptive step size has been studied for years in the optimization community. The most celebrated
method is the line search scheme. However, while the exact line search is usually computational
infeasible, the inexact line search also involves a lot of full gradient evaluation. Hence, they are not
suitable for the deep learning setting. Recently, algorithms with adaptive step sizes start to be applied
to the non-convex neural network training, such as AdaGrad (Duchi et al., 2012), Adam (Kingma
& Ba, 2014) and RMSProp (Hinton et al.). However they directly use the unnormalized gradient,
which is different from our framework. Singh et al. (2015) recently proposes to apply layer-wise
specific step sizes, which differs from ours in that it essentially adds a term to the gradient rather
than normalizing it. Recently Wilson et al. (2017) finds the methods with adaptive step size might
converge to a solution with worse generalization. However, this is orthogonal to our focus in this
paper.
3	Algorithm Framework
In this section, we present our algorithm framework to solve the following general problem:
min f(x) = E(F (x, ξ)),	(1)
x∈Rd
where x = (x1, x2, . . . , xB) ∈ Rd with xi ∈ Rdi and PiB=1 di = d, ξ is a random variable
following some distribution P, F(∙,ξ) is a loss function for each ξ and the expectation E is taken
over ξ. In the case where (1) models an empirical risk minimization problem, the distribution P can
be the empirical distribution over training samples such that the objective function in (1) becomes a
finite-sum function. Now our goal is to minimize the objective function f over x, where x can be
the parameters of a machine learning model when (1) corresponds to a training problem. Here, the
parameters are partitioned into B blocks. The problem of training a neural network is an important
instance of (1), where each block of parameters xi can be viewed as the parameters associated to the
ith layer in the network.
We propose the generic optimization framework in Algorithm 1. In iteration t, it firstly computes
the partial (sub)gradient Fi0(xt, ξt) of F with respect to xi for i = 1, 2, . . . , at x = xt with a
mini-batch data ξt , and then normalizes it to get a partial direction gti
__Fi (xt,ξt)  ^^ve define
kF0(χt,ξt)k2. Wedefine
gt = (gt1 , gt2, . . . , gtB). The next is to find d adaptive step sizes τt ∈ Rd with each coordinate
of τt corresponding to a coordinate of x. We also partition τt in the same way as x so that τt =
(τt1 , τt2 , . . . , τtB ) ∈ RB with τti ∈ Rdi . We use τt as step sizes to update xt to xt+1 as xt+1 =
xt - τt ◦ gt, where ◦ represents coordinate-wise (Hadamard) product. In fact, our framework can be
customized to most of existing first order methods with fixed or adaptive step sizes, such as SGD,
AdaGrad(Duchi et al., 2011), RMSProp (Hinton et al.) and Adam(Kingma & Ba, 2014), by adopting
their step size rules respectively.
4	Numerical Experiments
Basic Experiment Setup In this section, we conduct comprehensive numerical experiments on
different types of neural networks. The algorithms we are testing are SGD with Momentum
3
Under review as a conference paper at ICLR 2018
Algorithm 1 Generic Block-Normalized Gradient (BNG) Descent
1:
2:
3:
4:
5:
6:
Choose x1 ∈ Rd .
for t = 1, 2, ..., do
Sample a mini-batch of data ξt and compute the partial stochastic gradient gti
Let gt = (gt1 , gt2 , . . . , gtB) and choose step sizes τt ∈ Rd.
Fi (Xt ,ξt)
kF0(xt,ξt)k2
xt+1 = xt - τt ◦ gt
end for
(SGDM), AdaGrad (Duchi et al., 2013), Adam (Kingma & Ba, 2014) and their block-normalized
gradient counterparts, which are denoted with suffix “NG”. Specifically, we partition the parameters
into block as x = (x1 , x2 , . . . , xB ) such that xi corresponds to the vector of parameters (including
the weight matrix and the bias/intercept coefficients) used in the ith layer in the network.
Our experiments are on four diverse tasks, ranging from image classification to natural language
processing. The neural network structures under investigation include multi layer perceptron, long-
short term memory and convolution neural networks.
To exclude the potential effect that might be introduced by advanced techniques, in all the exper-
iments, we only adopt the basic version of the neural networks, unless otherwise stated. The loss
functions for classifications are cross entropy, while the one for language modeling is log perplex-
ity. Since the computational time is proportional to the epochs, we only show the performance
versus epochs. Those with running time are similar so we omit them for brevity. For all the al-
gorithms, we use their default settings. More specifically, for Adam/AdamNG, the initial step size
scale α = 0.001, first order momentum β1 = 0.9, second order momentum β2 = 0.999, the pa-
rameter to avoid division of zero = 1e-8; for AdaGrad/AdaGradNG, the initial step size scale is
0.01.
4.1	Multi Layer Perceptron for MNIST Image Classification
The first network structure we are going to test upon is the Multi Layer Perceptron (MLP). We will
adopt the handwritten digit recognition data set MNIST1Lecun et al. (1998), in which, each data
is an image of hand written digits from {1, 2, 3, 4, 5, 6, 7, 8, 9, 0}. There are 60k training and 10k
testing examples and the task is to tell the right number contained in the test image. Our approach
is applying MLP to learn an end-to-end classifier, where the input is the raw 28 × 28 images and the
output is the label probability. The predicted label is the one with the largest probability. In each
middle layer of the MLP, the hidden unit number are 100, and the first and last layer respectively
contain 784 and 10 units. The activation functions between layers are all sigmoid and the batch size
is 100 for all the algorithms.
We choose different numbers of layer from {6, 12, 18}. The results are shown in Figure 1. Each
column of the figures corresponds to the training and testing objective curves of the MLP with a
given layer number. From left to right, the layer numbers are respectively 6, 12 and 18. We can
see that, when the network is as shallow as containing 6 layers, the normalized stochastic gradient
descent can outperform its unnormalized counterpart, while the Adam and AdaGrad are on par with
or even slightly better than their unnormalized versions. As the networks become deeper, the ac-
celeration brought by the gradient normalization turns more significant. For example, starting from
the second column, AdamNG outperforms Adam in terms of both training and testing convergence.
In fact, when the network depth is 18, the AdamNG can still converge to a small objective value
while Adam gets stuck from the very beginning. We can observe the similar trend in the comparison
between AdaGrad (resp. SGDM) and AdaGradNG (resp. SGDMNG). On the other hand, the algo-
rithms with adaptive step sizes can usually generate a stable learning curve. For example, we can
see from the last two column that SGDNG causes significant fluctuation in both training and testing
curves. Finally, under any setting, AdamNG is always the best algorithm in terms of convergence
performance.
1http://yann.lecun.com/exdb/mnist/
4
Under review as a conference paper at ICLR 2018
sso—lU-EJI
10	12	14	16	18	20
Epoch
5	10	15	H	25	30
Epoch
5	10	15H 25	30	35	40	45,
Epoch
10	12	14	16	18	H
Epoch
SSOls31
5	10	15	H	25	R
Epoch
5	10	15H>253>354□4555I
Epoch
Figure 1: The training and testing objective curves on MNIST dataset with multi layer perceptron.
From left to right, the layer numbers are 6, 12 and 18 respectively. The first row is the training curve
and the second is testing.
4.2	Residual Network on CIFAR 1 0 and CIFAR 1 00
Datasets In this section, we benchmark the methods on CIFAR (both CIFAR10 and CIFAR100)
datasets with the residual networks He et al. (2016a), which consist mainly of convolution layers
and each layer comes with batch normalization (Ioffe & Szegedy, 2015). CIFAR10 consists of
50,000 training images and 10,000 test images from 10 classes, while CIFAR100 from 100 classes.
Each input image consists of 32 × 32 pixels. The dataset is preprocessed as described in He et al.
(2016a) by subtracting the means and dividing the variance for each channel. We follow the same
data augmentation in He et al. (2016a) that 4 pixels are padded on each side, and a 32 × 32 crop is
randomly sampled from the padded image or its horizontal flip.
Algorithms We adopt two types of optimization frameworks, namely SGD and Adam2, which
respectively represent the constant step size and adaptive step size methods. We compare the perfor-
mances of their original version and the layer-normalized gradient counterpart. We also investigate
how the performance changes if the normalization is relaxed to not be strictly 1. In particular, we
find that if the normalized gradient is scaled by its variable norm with a ratio, which we call NGadap
and defined as follows,
NGadap := NG × Norm of variable × α = Grad ×
Norm of variable
Norm of grad
× α,
we can get lower testing error. The subscript “adap” is short for “adaptive”, as the resulting norm of
the gradient is adaptive to its variable norm, while α is the constant ratio. Finally, we also compare
with the gradient clipping trick that rescales the gradient norm to a certain value if it is larger than
that threshold. Those methods are with suffix “CLIP”.
Parameters In the following, whenever we need to tune the parameter, we search the space with
a holdout validation set containing 5000 examples.
For SGD+Momentum method, we follow exactly the same experimental protocol as described in
He et al. (2016a) and adopt the publicly available Torch implementation3 for residual network. In
2We also tried AdaGrad, but it has significantly worse performance than SGD and Adam, so we do not
report its result here.
3https://github.com/facebook/fb.resnet.torch
5
Under review as a conference paper at ICLR 2018
particular, SGD is used with momentum of 0.9, weight decay of 0.0001 and mini-batch size of 128.
The initial learning rate is 0.1 and dropped by a factor of 0.1 at 80, 120 with a total training of 160
epochs. The weight initialization is the same as He et al. (2015).
For Adam, we search the initial learning rate in range {0.0005, 0.001, 0.005, 0.01} with the base
algorithm Adam. We then use the best learning rate, i.e., 0.001, for all the related methods Adam-
CLIP, AdamNG, and AdamNGadap. Other setups are the same as the SGD. In particular, we also
adopt the manually learning rate decay here, since, otherwise, the performance will be much worse.
We adopt the residual network architectures with depths L = {20, 32, 44, 56, 110} on both CIFAR-
10 and CIFAR100. For the extra hyper-parameters, i.e., threshold of gradient clipping and scale
ratio of NGadap, i.e., α, we choose the best hyper-parameter from the 56-layer residual network. In
particular, for clipping, the searched values are {0.05, 0.1, 0.5, 1, 5}, and the best value is 0.1. For
the ratio α, the searched values are {0.01, 0.02, 0.05} and the best is 0.02.
Results For each network structure, we make 5 runs with random initialization. We report the
training and testing curves on CIFAR10 and CIFAR100 datasets with deepest network Res-110 in
Figure 2. We can see that the normalized gradient methods (with suffix “NG”) converge the fastest
in training, compared to the non-normalized counterparts. While the adaptive version NGadap is not
as fast as NG in training, however, it can always converge to a solution with lower testing error,
which can be seen more clearly in Table 1 and 2.
For further quantitative analysis, we report the means and variances of the final test errors on both
datasets in Table 1 and 2, respectively. Both figures convey the following two messages. Firstly,
on both datasets with ResNet, SGD+Momentum is uniformly better than Adam in that it always
converges to a solution with lower test error. Such advantage can be immediately seen by compar-
ing the two row blocks within each column of both tables. It is also consistent with the common
wisdom (Wilson et al., 2017). Secondly, for both Adam and SGD+Momentum, the NGadap version
has the best generalization performance (which is mark bold in each column), while the gradient
clipping is inferior to all the remaining variants. While the normalized SGD+Momentum is better
than the vanilla version, Adam slightly outperforms its normalized counterpart. Those observations
are consistent across networks with variant depths.
Figure 2: The training and testing curves on CIFAR10 and CIFAR100 datasets with Resnet-110.
Left: CIFAR10; Right: CIFAR100; Upper: SGD+Momentum; Lower: Adam. The thick curves are
the training while the thin are testing.
6
Under review as a conference paper at ICLR 2018
Algorithm	ResNet-20	ReSNet-32 一	ResNet-44	ResNet-56	ResNet-110
Adam					
Adam	9.14 ± 0.07	8.33 ± 0.17	7.794 ± 0.22	7.33 ± 0.19	6.75 ± 0.30
AdamCLIP	10.18± 0.16	9.18± 0.06	8.89 ± 0.14	9.24 ± 0.19	9.96± 0.29
AdamNG	9.42 ± 0.20	8.50 ± 0.17	8.06± 0.20	7.69 ± 0.19	7.29 ± 0.08
AdamNGadap	8.52± 0.16	7.62± 0.25	7.28± 0.18	7.04± 0.27	6.71± 0.17
SGD+Momentum					
SGDM*	8.75	7.51	7.17	6.97	6.61± 0.16
SGDM	7.93 ± 0.15	7.15 ± 0.20	7.09 ± 0.21	7.34 ± 0.52	7.07 ± 0.65
SGDMCLIP	9.03 ± 0.15	8.44 ± 0.14	8.55 ± 0.20	8.30± 0.08	8.35± 0.25
SGDMNG	7.82 ± 0.26	7.09 ± 0.13	6.60± 0.21	6.59 ± 0.23	6.28 ± 0.22
SGDMNGadap	7.71 ± 0.18	6.90± 0.11	6.43± 0.03	6.19± 0.11	5.87± 0.10
Table 1: Error rates of ResNets with different depths on CIFAR 10. SGDM* indicates the results
reported in He et al. (2015) with the same experimental setups as ours, where only ResNet-110 has
multiple runs.
Algorithm	ResNet-20	ReSNet-32一	ResNet-44	ResNet-56	ResNet-110
Adam					
Adam	34.44 ± 0.33	32.94 ± 0.16	31.53 ± 0.13	30.80 ± 0.30	28.20 ± 0.14
AdamCLIP	38.10 ± 0.48	35.78 ± 0.20	35.41± 0.19	35.62± 0.39	39.10± 0.35
AdamNG	35.06 ± 0.39	33.78 ± 0.07	32.26± 0.29	31.86 ± 0.21	29.87 ± 0.49
AdamNGadap	32.98± 0.52	31.74± 0.07	30.75± 0.60	29.92± 0.26	28.09± 0.46
SGD+Momentum					
SGDM	32.28 ± 0.16	30.62 ± 0.36	29.96 ± 0.66	29.07 ± 0.41	28.79 ± 0.63
SGDMCLIP	35.06 ± 0.37	34.49± 0.49	33.36± 0.36	34.00± 0.96	33.38± 0.73
SGDMNG	32.46 ± 0.37	31.16 ± 0.37	30.05 ± 0.29	29.42 ± 0.51	27.49 ± 0.25
SGDMNGadap	31.43 ± 0.35	29.56 ± 0.25	28.92 ± 0.28	28.48 ± 0.19	26.72 ± 0.39
Table 2: Error rates of ResNets with different depths on CIFAR 100. Note that He et al. (2015) did
not run experiment on CIFAR 100.
4.3	Residual Network for ImageNet Classification
In this section, we further test our methods on ImageNet 2012 classification challenges, which con-
sists of more than 1.2M images from 1,000 classes. We use the given 1.28M labeled images for train-
ing and the validation set with 50k images for testing. We employ the validation set as the test set,
and evaluate the classification performance based on top-1 and top-5 error. The pre-activation (He
et al., 2016b) version of ResNet is adopted in our experiments to perform the classification task.
Like the previous experiment, we again compare the performance on SGD+Momentum and Adam.
We run our experiments on one GPU and use single scale and single crop test for simplifying discus-
sion. We keep all the experiments settings the same as the publicly available Torch implementation
4. That is, we apply stochastic gradient descent with momentum of 0.9, weight decay of 0.0001, and
set the initial learning rate to 0.1. The exception is that we use mini-batch size of64 and 50 training
epochs considering the GPU memory limitations and training time costs. Regarding learning rate
annealing, we use 0.001 exponential decay.
As for Adam, we search the initial learning rate in range {0.0005, 0.001, 0.005, 0.01}. Other setups
are the same as the SGD optimization framework. Due to the time-consuming nature of training
the networks (which usually takes one week) in this experiment, we only test on a 34-layer ResNet
and compare SGD and Adam with our default NG method on the testing error of the classification.
From Table 3, we can see normalized gradient has a non-trivial improvement on the testing error
over the baselines SGD and Adam. Besides, the SGD+Momentum again outperforms Adam, which
4We again use the public Torch implementation: https://github.com/facebook/fb.resnet.
torch
7
Under review as a conference paper at ICLR 2018
is consistent with both the common wisdom (Wilson et al., 2017) and also the findings in previous
section.
method	Top-1	Top-5
Adam	35.6	14.09
AdamNG	30.17	10.51
SGDM	29.05	9.95
SGDMNG	28.43	9.57
Table 3: Top-1 and Top 5 error rates of ResNet on ImageNet classification with different algorithms.
4.4	Language Modeling with Recurrent Neural Network
Now we move on to test the algorithms on Recurrent Neural Networks (RNN). In this section, we test
the performance of the proposed algorithm on the word-level language modeling task with a popular
type of RNN, i.e. single directional Long-Short Term Memory (LSTM) networks (Hochreiter &
Schmidhuber, 1997). The data set under use is Penn Tree Bank (PTB) (Marcus et al., 1993) data,
which, after preprocessed, contains 929k training words, 73k validation and 82k test words. The
vocabulary size is about 10k. The LSTM has 2 layers, each containing 200 hidden units. The
word embedding has 200 dimensions which is trained from scratch. The batch size is 100. We
vary the length of the backprop through time (BPTT) within the range {40, 400, 1000}. To prevent
overfitting, we add a dropout regularization with rate 0.5 under all the settings.
The results are shown in Figure 3. The conclusions drawn from those figures are again similar to
those in the last two experiments. However, the slightly different yet cheering observations is that
the AdamNG is uniformly better than all the other competitors with any training sequence length.
The superiority in terms of convergence speedup exists in both training and testing.
SSOIU_e」J_
SSOls3
0	5	10	15 H 25	30	35	40	45	50
EpoCh
5
5	10	15H25R:5945a
Epoch



. . . . . . . . .
0	5	10	15 H 25	3 0	35	40	45	,
Epoch
Figure 3: The training and testing objective curves on Penn Tree Bank dataset with LSTM recurrent
neural networks. The first row is the training objective while the second is the testing. From left to
right, the training sequence (BPTT) length are respectively 40, 400 and 1000. Dropout with 0.5 is
imposed.
8
Under review as a conference paper at ICLR 2018
4.5	Sentiment Analysis with Convolution Neural Network
The task in this section is the sentiment analysis with convolution neural network. The dataset under
use is Rotten Tomatoes5 Pang & Lee (2005), a movie review dataset containing 10,662 documents,
with half positive and half negative. We randomly select around 90% for training and 10% for
validation. The model is a single layer convolution neural network that follows the setup of (Kim,
2014). The word embedding under use is randomly initialized and of 128-dimension.
For each algorithm, we run 150 epochs on the training data, and report the best validation accuracy in
Table 4. The messages conveyed by the table is three-fold. Firstly, the algorithms using normalized
gradient achieve much better validation accuracy than their unnormalized versions. Secondly, those
with adaptive stepsize always obtain better accuracy than those without. This is easily seen by the
comparison between Adam and SGDM. The last point is the direct conclusion from the previous two
that the algorithm using normalized gradient with adaptive step sizes, namely AdamNG, outperforms
all the remaining competitors.
Algorithm	AdamNG	Adam	AdaGradNG	AdaGrad	SGDMNG	SGDM
Validation Accuracy	77.11%	74.02%	71.95% —	69.89%	71.95%	64.35%
Table 4: The Best validation accuracy achieved by the different algorithms.
5 Conclusion
In this paper, we propose a generic algorithm framework for first order optimization. Itis particularly
effective for addressing the vanishing and exploding gradient challenge in training with non-convex
loss functions, such as in the context of convolutional and recurrent neural networks. Our method is
based on normalizing the gradient to establish the descending direction regardless of its magnitude,
and then separately estimating the ideal step size adaptively or constantly. This method is quite
general and may be applied to different types of networks and various architectures. Although the
primary application of the algorithm is deep neural network training, we provide a convergence for
the new method under the convex setting in the appendix.
Empirically, the proposed method exhibits very promising performance in training different types of
networks (convolutional, recurrent) across multiple well-known data sets (image classification, nat-
ural language processing, sentiment analysis, etc.). In general, the positive performance differential
compared to the baselines is most striking for very deep networks, as shown in our comprehensive
experimental study.
References
Yoshua Bengio. Learning deep architectures for ai. Foundations and trendsR in Machine Learning,
2(1):1-127, 2009.
John Duchi, Michael I Jordan, and Brendan McMahan. Estimation, optimization, and parallelism
when data is sparse. In NIPS, pp. 2832-2840, 2013.
John C. Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine Learning Research, 12:2121-2159, 2011.
John C Duchi, Alekh Agarwal, and Martin J Wainwright. Dual averaging for distributed optimiza-
tion: convergence analysis and network scaling. Automatic Control, IEEE Transactions on, 57
(3):592-606, 2012.
Elad Hazan, Kfir Y. Levy, and Shai Shalev-Shwartz. Beyond convexity: Stochastic quasi-convex
optimization. In NIPS, pp. 1594-1602, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In ICCV, 2015.
5http://www.cs.cornell.edu/people/pabo/movie-review-data/
9
Under review as a conference paper at ICLR 2018
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. CoRR, abs/1603.05027, 2016b.
Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Lecture 6a: Overview of mini-batch gradi-
ent descent. Neural Networks for Machine Learning.
SePP Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780,1997.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In ICML, PP. 448-456, 2015.
Yoon Kim. Convolutional neural networks for sentence classification. arXiv preprint
arXiv:1408.5882, 2014.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic oPtimization. CoRR,
abs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980.
Krzysztof C Kiwiel. Convergence and efficiency of subgradient methods for quasiconvex minimiza-
tion. Mathematical programming, 90(1):1-25, 2001.
Yann Lecun, Leon Bottou, Yoshua Bengio, and Patrick Haffner? Gradient-based learning aPPlied
to document recognition. In Proceedings of the IEEE, PP. 2278-2324, 1998.
Kfir Y. Levy. The Power of normalization: Faster evasion of saddle Points. CoRR, abs/1611.04831,
2016. URL http://arxiv.org/abs/1611.04831.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated
corPus of english: The Penn treebank. Computational Linguistics, 19(2):313-330, 1993.
Nesterov. Minimization methods for nonsmooth convex and quasiconvex functions. Matekon, 29:
519-531, 1984.
Behnam Neyshabur, Ruslan Salakhutdinov, and Nathan Srebro. Path-sgd: Path-normalized oPti-
mization in deeP neural networks. In NIPS, PP. 2422-2430, 2015.
Bo Pang and Lillian Lee. Seeing stars: ExPloiting class relationshiPs for sentiment categorization
with resPect to rating scales. In Proceedings of the 43rd annual meeting on association for com-
putational linguistics, PP. 115-124. Association for ComPutational Linguistics, 2005.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural
networks. In ICML, PP. 1310-1318, 2013.
Tim Salimans and Diederik P. Kingma. Weight normalization: A simPle reParameterization to
accelerate training of deeP neural networks. In NIPS, 2016.
Bharat Singh, Soham De, Yangmuzi Zhang, Thomas Goldstein, and Gavin Taylor. Layer-sPecific
adaPtive learning rates for deeP networks. CoRR, abs/1510.04609, 2015.
Ashia C. Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal
value of adaPtive gradient methods in machine learning. In Advances in Neural Information
Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9
December 2017, Long Beach, CA, USA, PP. 4151-4161, 2017.
10
Under review as a conference paper at ICLR 2018
Appendix
As a concrete example for Algorithm 1, we present a simple modification of AdaGrad using block-
wise normalized stochastic gradient in Algorithm 2, where g1:t is a matrix created by stacking g1,
g2,... and gt in columns and g1:t,j ∈ Rt represents the jth row of g1:t forj = 1, 2, . . . , d. Assuming
the function F is convex over x for any ξ and following the analysis in (Duchi et al., 2011), it is
straightforward to show a O(方)convergence rate of this modification. In the following, We denote
∣∣χkw := √χ>Wχ as the the Mahalanobis norm associated to a d X d positive definite matrix W.
The convergence property of Block-Normalized AdaGrad is presented in the following theorem.
Algorithm 2 AdaGrad with Block-Normalized Gradient (AdaGradBNG)
1
2
3
4
5
6
7
8
Choose x1 ∈ Rd , δ > 0 and η > 0.
for t = 1, 2, ..., do
Sample a mini-batch data ξt and compute the stochastic partial gradient gti
Let	gt	=	(gt , gt	, . . . , gt ),	g1:t	=	[g1,	g2 , .	. . , gt]
(IlgI:t,1k2, ∣∣gLt,2∣∣2,..., kgl:t,dl^
Partition st = (st1, st2, . . . , stB) in the same way as gt.
Let τt = (τt1,τt2, . . .,τtB) with6 τti = η∣Fi0(xt,ξt)∣2(δ1di + sit)-1.
xt+1 = xt - τt ◦ gt
end for
_ Fi (Xt,£t)
- kF0(xt,ξt)k2
and st
Theorem 1 Suppose F is convex over x, ∣F0(xt,ξt)∣2 ≤ Mi and ∣∣xt 一 x*∣∣∞ ≤ D∞ for all t for
some constants Mi and D∞ > 0 in Algorithm 2. Let Ht = δId+diag(st) and Hti = δIdi + diag(sit)
for t = 1, 2,... and XT := T PT=I Xt∙ Algorithm 2 guarantees
E[f(Xτ) — f(x*)]	≤
≤
∣M-χ*kH1	d∞√Bd X ηE [M2Pd=+d++2++d+di-1+11"Tjk2]
2ηT	+ 2η√T + =	T
kχι - χ*llHι + D∞√Bd + X ηM2 √di
—2ηT —	2η√T	=	√T	.
Proof: It is easy to see that Ht is a positive definite and diagonal matrix. According to the updating
scheme of Xt+1 and the definitions of Ht, τt and gt, we have
I∣xt+1 一 x*kHt = (Xt 一 Tt ◦ gt 一 x*)>Ht(Xt 一 Tt ◦ gt 一 X*)
=kXt	— X*kHt	— MXt-吟, Ht(Tt ◦ gt)	+	IlTt ◦ gt∣∣Ht
≤	llXt	— X*llHt	— 2n(Xt- x*)>f'(Xt,ξt)	+	IlTt ◦ gt∣∣Ht.
The inequality above and the convexity of F(X, ξ) in X imply
尸(勺,8)一 F(X*,ξt)	≤
归一X*kHt	llXt+i — 门员"I” ◦ gt∣∣Ht
一+
2η	2η	2η
iiXt - 浊民	iiXt+i - X*IHt G ^37(j,832ngi^Hi)-1,
―2	2	+ 工	2	(2)
Taking expectation over ξt for t = 1, 2, . . . and averaging the above inequality give
E[f(XT) — f(X*)]
≤
1T
指X
t=1
-EkXt-X*怆 EkXt+1-汕信:
----------------------
2η	2η
T B ηE IFi0(Xt,ξt)I22IgtiI(2Hi)-1
+ X X -~2T~」
≤
ɪX ΓEkXt -X*llHt 一 园|勺+1 — X*iiHt
T L [	2η	2η
TB
+XX
t=1 i=1
ηMi2E Ugtk2Hi)-J
2T
(3)
11
Under review as a conference paper at ICLR 2018
where we use the fact that kFi0 (xt , ξt)k22 ≤ Mi2 in the second inequality.
According to the equation (24) in the proof of Lemma 4 in (Duchi et al., 2011), we have
T	T	d1 +d2+Hdi	2	d1+d2+Hdi
Xkgik2Hi)-ι = X, X	δ+⅛⅛≤	X	2k"∙k2.	(4)
t=1	t=1 j = dl +d2 +-+di —1 + 1	,	j = dι+d2+-+di-1 + 1
Following the analysis in the proof of Theorem 5 in (Duchi et al., 2011), we show that
kxt+1 - x* kHt+1 - kxt+1 - x* IIHt = hx* - xt+1, diag(st+1 - St)(X* - xt+1)i
≤ D∞2 kst+1 - stk1 = D∞2 hst+1 - st, 1i .	(5)
After applying (4) and (5) to (3) and reorganizing terms, we have
E[f(xT) - f (x*)]
∣∣X1 - x*kH1	D2oE hsT, 1〉	G ηE hMi2 Pj=+：++?++”+ di-1 + 1 kg1:Tjk2i
≤	1 +∞+
一	2ηΤ	2ηT	+ 乙	T
∣X1 - x*kH	D2√Bd	g ηE hMi2 Pd=+d++2^d+ di-1+1 kgi:Tjk21
≤	^I +∞	+
≤	2ηT	+ 2η√T + =	T	,
where the second inequality is becausehsT, 1)= Pd=I ||gi：Tjk2 ≤ TTdd which holds due to
Cauchy-Schwarz inequality and the fact that kgti k2 = 1. Then, we obtain the first inequality in the
conclusion of the theorem. To obtain the second inequality, we only need to observe that
d1+d2+-Hdi
X	kgi:Tj∣2 ≤ pTdi	(6)
j = d1+d2+-+di-1 +1
which holds because of CaUchy-SchWarz inequality and the fact that kgi k2 = 1.	■
Remark 1 When d = 1, namely, the normalization is applied to the full gradient instead of different
blocks of the gradient, the inequality in Theorem 1 becomes
E[f(XT) — f(x*)]	≤
kxι - x*kH1 + D∞√d + ηM2√d
2ηT	+ 2η√T 十 √T
where M is a constant such that kF0(xt, ξt)k2 ≤M. Note that the right hand side of this inequality
can be larger than that of the inequality in Theorem 1 with d > 1. We use d = 2 as an example.
Suppose F10 dominates in the norm of F0, i.e., M2 M1 ≈M and d1	d2 ≈ d, we can have
PB=I M2√di = O(M2√d1 + M2√d) which can be much smaller than the factor M2√d in the
inequality above, especially whenM and d are both large. Hence, the optimal value for d is not
necessarily one.
12