Under review as a conference paper at ICLR 2018
Small Coresets to Represent Large Training
Data for Support Vector Machines
Anonymous authors
Paper under double-blind review
Ab stract
Despite their popularity, even efficient implementations of Support Vector Ma-
chines (SVMs) have proven to be computationally expensive to train at a large-
scale, especially in streaming settings. In this paper, we propose a coreset con-
struction algorithm for efficiently generating compact representations of massive
data sets to speed up SVM training. A coreset is a weighted subset of the orig-
inal data points such that SVMs trained on the coreset are provably competitive
with those trained on the original (massive) data set. We provide lower and upper
bounds on the number of samples required to obtain accurate approximations to
the SVM problem as a function of the complexity of the input data. Our analy-
sis also establishes sufficient conditions on the existence of sufficiently compact
and representative coresets for the SVM problem. We empirically evaluate the
practical effectiveness of our algorithm against synthetic and real-world data sets.
1	Introduction
Popular machine learning algorithms are computationally expensive, or worse yet, intractable to
train on Big Data. The notion of using coresets (Feldman & Langberg, 2011; Braverman et al.,
2016; Bachem et al., 2017), small weighted subsets of the input points that provably approximate
the original data set, has shown promise in accelerating machine learning algorithms such as k-
means clustering (Feldman & Langberg, 2011), mixture model training (Feldman et al., 2011; Lucic
et al., 2017), and logistic regression (Huggins et al., 2016).
Coreset constructions were originally introduced in the context of computational geometry (Agar-
wal et al., 2005) and subsequently generalized for applications to other problems (Langberg &
Schulman, 2010; Feldman & Langberg, 2011). Coresets provide a compact representation of the
structure of static and streaming data, with provable approximation guarantees with respect to spe-
cific algorithms. For instance, a data set consisting of K clusters would yield a coreset of size K,
with each cluster represented by one coreset point. Even if the data has no structure (e.g., uniformly
distributed), coresets will correctly down sample the data to within prescribed error bounds. For
domains where the data has structure, the coreset representation has the potential to greatly and ef-
fectively reduce the time required to manually label data for training and the computation time for
training, while at the same time providing a mechanism of supporting machine learning systems for
applications with streaming data.
Coresets are constructed by approximating the relative importance of each data point in the original
data set to define a sampling distribution and sampling sufficiently many points in accordance with
this distribution. This construction scheme suggests that beyond providing a means of conducting
provably fast and accurate inference, coresets also serve as efficient representations of the full data
set and may be used to automate laborious representation tasks, such as automatically generating
semantic video representations or detecting outliers in data (Lucic et al., 2016).
The representative power and provable guarantees provided by coresets also motivate their use in
training of one of the most popular algorithms for classification and regression analysis: Support
Vector Machines (SVMs). Despite their popularity, SVMs are computationally expensive to train on
massive data sets, which has proven to be computationally problematic with the rising availability of
Big Data. In this paper, we present a novel coreset construction algorithm for efficient, large-scale
Support Vector Machine training.
1
Under review as a conference paper at ICLR 2018
In particular, this paper contributes the following:
1.	A practical coreset construction algorithm for accelerating SVM training based on an effi-
cient importance evaluation scheme for approximating the importance of each point.
2.	An analysis proving lower bounds on the number of samples required by any coreset con-
struction algorithm to approximately represent the data.
3.	An analysis proving the efficiency and theoretical guarantees of our algorithm and charac-
terizing the family of data sets for which applications of coresets are most suited.
4.	Evaluations against synthetic and real-world data sets that demonstrate the practical effec-
tiveness of our algorithm for large-scale SVM training.
2	Related Work
Training a canonical Support Vector Machine (SVM) requires O(n3) time and O(n2) space where
n is the number of training points (Tsang et al., 2005). Work by Tsang et al. (2005) introduced
Core Vector Machines (CVMs) that reformulated the SVM problem as the Minimum Enclosing Ball
(MEB) problem and used existing coreset methods for MEB to compress the data. The authors
proposed a method that generates a (1 + ε)2 approximation to the two-class L2-SVM in O(n∕ε2 +
1∕ε4) time, when certain assumptions about the kernel used are satisfied. However, CVM's accuracy
and convergence properties have been noted to be at times inferior to the performance of existing
SVM implementations (Loosli & Canu, 2007). Similar geometric approaches including extensions
to the MEB formulation, those based on convex hulls, and extreme points, among others, were also
investigated by Rai et al. (2009); Agarwal & Sharathkumar (2010); Har-Peled et al. (2007); Nandan
et al. (2014).
Since the SVM problem is inherently a quadratic optimization problem, prior work has investigated
approximations to the quadratic programming problem using the Frank-Wolfe algorithm or Gilbert’s
algorithm (Clarkson, 2010). Another line of research has been in reducing the problem of polytope
distance to solve the SVM problem (Gartner & Jaggi, 2009). The authors establish lower and upper
bounds for the polytope distance problem and use Gilbert’s algorithm to train an SVM in linear time.
A variety of prior approaches were based on randomized algorithms with the property that they
generated accurate approximations with high probability. Most notable are the works of Clarkson
et al. (2012) Hazan et al. (2011). Hazan et al. (2011) used a primal-dual approach combined with
Stochastic Gradient Descent (SGD) in order to train linear SVMs in sub-linear time. They proposed
the SVM-SIMBA approach and proved that it generates an ε-approximate solution with probability
at least 1∕2 to the SVM problem that uses hinge loss as the objective function. The key idea in their
method is to access single features of the training vectors rather than the entire vectors themselves.
Their method is nondeterministic and returns the correct ε-approximation with probability greater
than a constant probability, similar to the probabilistic guarantees of coresets.
Clarkson et al. (2012) present sub-linear-time (in the size of the input) approximation algorithms for
some optimization problems such as training linear classifiers (e.g., perceptron) and finding MEB.
They introduce a technique that is originally applied to the perceptron algorithm, but extend it to the
related problems of MEB and SVM in the hard margin or L2-SVM formulations. Shalev-Shwartz
et al. (2011) introduce Pegasos, a stochastic sub-gradient algorithm for approximately solving the
SVM optimization problem, that runs in O(dnC∕ε) time for a linear kernel, where C is the SVM
regularization parameter and d is the dimensionality of the input data points. These works offer
probabilistic guarantees, similar to those provided by coresets, and have been noted to perform
well empirically; however, unlike coresets, SGD-based approaches cannot be trivially extended to
streaming settings since each new arriving data point in the stream results in a change of the gradient.
Joachims (2006) presents an alternative approach to training SVMs in linear time based on the cut-
ting plane method that hinges on an alternative formulation of the SVM optimization problem. He
shows that the Cutting-Plane algorithm can be leveraged to train SVMs in O(sn) time for classifica-
tion and O(sn log n) time for ordinal regression where s is the average number of non-zero features.
Har-Peled et al. (2007) constructs coresets to approximate the maximum margin separation, i.e., a
hyperplane that separates all of the input data with margin larger than (1 - ε)ρ*, where ρ* is the best
achievable margin.
2
Under review as a conference paper at ICLR 2018
3	Problem Definition
3.1	Soft-margin SVM
We assume that we are given a set of weighted training points P = {(xi, yi)}in=1 with the corre-
sponding weight function u : P → R≥0, such that for every i ∈ [n], xi ∈ Rd, yi ∈ {-1, 1}, and
u(pi) corresponds to the weight of point pi . For simplicity, we assume that the bias term is embed-
ded into the feature space by defining Xi = (x%, 1) for each point and W = (w, 1) for each query.
Thus, We henceforth assume that We are dealing with d +1 dimensional points and refer to W and Xi
as just w and xi respectively. Under this setting, the hinge loss of a point pi = (xi, yi) with respect
to the separating hyperplane W is defined as h(pi, W) = [1 - yihXi, Wi]+, Where [X]+ = max{0, X}.
For any subset of points, P0 ⊆ P, define H(P0, W) = Pp∈P0 u(p)h(p, W) as the sum of the hinge
losses and u(P0) = Pp∈P0 u(p) as the sum of the Weights of points in set P0. To clearly depict the
contribution of each point to the objective value of the SVM problem, We present the SVM objective
function, f(P, W), as the sum of per-point objective function evaluations, Which We formally define
beloW.
Definition 1 (Per-point Objective Function). Define the function f : (Rd+1 × {-1, 1}) × Rd+1 →
R≥0 such that for every i ∈ [n]
f (Pi, w) = kw1(P)2 + Ch(Pi, w),
W1:d denotes the entries 1 : d of W, h(pi, W) is the corresponding hinge loss, and C ∈ [0, 1] is the
regularization parameter.
Definition 2 (Soft-margin SVM Problem). Given a set of d + 1 dimensional weighted points P
with weight function u : P → R≥0 the primal of the SVM problem is expressed by the following
quadratic program
min f((P, u), w),	(1)
w∈Q
where f is the evaluation of the weighted point setP with weight function u : P → R≥0,
f((P,u),w) = X U(Pi)f(Pi,w) = "'d"2 + CH(P,w).	⑵
i∈[n]
When the set of points P and the corresponding Weight function u are clear from context, We Will
henceforth denote f((P, u), w) by f(P, w) for notational convenience.
3.2	Coresets
Coresets can be seen as a compact representation of the full data set that approximate the SVM
cost function (2) uniformly over all queries w ∈ Q. Thus, rather than introducing an entirely
neW algorithm for solving the SVM problem, our approach is to reduce the runtime of standard
SVM algorithms by compressing the size of the input points from n to a compact set Whose size is
sublinear (ideally, polylogarithmic) in n.
Definition 3 (ε-coreset). Let ε ∈ (0, 1/2) andP ⊂ Rd+1 × {-1, 1} be a set of n weighted points
with weight function u : P → R≥0. The weighted subset (S, v), where S ⊂ P with corresponding
weight function v : S → R≥0 is an ε-coreset if for any query w ∈ Q, (S, v) satisfies the coreset-
property
|f((P,u),w) - f ((S, v), w)| ≤ εf((P,u),w) (Coreset Property).	(3)
Our overarching goal is to efficiently construct an ε-coreset, (S, v), such that the size of S is suffi-
cient small in comparison to the original number of points n.
3
Under review as a conference paper at ICLR 2018
4	Method
4.1	Method Overview
Our coreset construction scheme is based on the unified framework of Langberg & Schulman (2010);
Feldman & Langberg (2011) and is shown as Alg. 1. The crux of our algorithm lies in generating
the importance sampling distribution via efficiently computable upper bounds (proved in Sec. 5)
on the importance of each point (Lines 1-6). Sufficiently many points are then sampled from this
distribution and each point is given a weight that is inversely proportional to its sample probability
(Lines 7-9). The number of points required to generate an ε-coreset with probability at least 1 - δ
is a function of the desired accuracy ε, failure probability δ, and complexity of the data set (t from
Theorem 9). Under mild assumptions on the problem at hand (see Sec. 5.2), the required sample
size is polylogarithmic in n.
Intuitively, our algorithm can be seen as an importance sampling procedure that first generates a
judicious sampling distribution based on the structure of the input points and samples sufficiently
many points from the original data set. The resulting weighted set of points, (S, v), serves as an
unbiased estimator for f(P, w) for any query w ∈ Q, i.e., E[f ((S, v), w)] = f(P, w). Although
sampling points uniformly with appropriate weights can also generate such an unbiased estimator,
it turns out that the variance of this estimation is minimized if the points are sampled according to
the distribution defined by the ratio between each point’s sensitivity and the sum of sensitivities, i.e.,
Y(Pi)/t on Line 9 (Bachem et al., 2017).
4.2	Chicken and the egg phenomena
Coresets are intended to provide efficient and provable approximations to the optimal SVM solution,
however, the very first line of our algorithm entails computing the optimal solution to the SVM prob-
lem. This seemingly eerie phenomenon is explained by the merge-and-reduce technique (Har-Peled
& Mazumdar, 2004) that ensures that our coreset algorithm is only run against small partitions of
the original data set (Har-Peled & Mazumdar, 2004; Braverman et al., 2016; Lucic et al., 2017). The
merge-and-reduce approach leverages the fact that coresets are composable and reduces the coreset
construction problem for a (large) set of n points into the problem of computing coresets for 2ns∣
points, where 2|S| is the minimum size of input set that can be reduced to half using Alg. 1 (Braver-
man et al., 2016). Assuming that the sufficient conditions for obtaining polylogarithmic size coresets
implied by Theorem 9 hold, the overall time required for coreset construction is nearly linear in n,
Oε,δ(d3n)1. This follows from the fact that 2|S| = Oδ,ε(d) by Theorem 9, that the Interior Point
Method runs in time O(∣S∣3) = Oδ,ε(d3) for an input set of size 21S|, and that the merge-and-reduce
tree has height at most dlog ne, meaning that an accuracy parameter of ε0 = ε/ log n has to be used
in the intermediate coreset constructions to account for the compounded error over all levels of the
tree (Braverman et al., 2016).
4.3	Extensions
We briefly remark on a straightforward extension that can be made to our algorithm to accelerate
performance and applicability. In particular, the computation of the optimal solution to the SVM
problem in line 1 can be replaced by an efficient gradient-based method, such as Pegasos (Shalev-
Shwartz et al., 2011), to compute an approximately ξ optimal solution in O (dnC∕ξ) time, which
is particularly suited to scenarios with C small. We give this result as Lemma 11, an extension of
Lemma 7. We also note that based on our analytical results (Lemmas 7 and 11), any SVM solver,
either exact or approximate, can be used in Line 1 as a replacement for the Interior Point Method.
5	Analysis
In this section, we prove upper and lower bounds on the sensitivity of a point in terms of the com-
plexity of the given data set. Our main result is Theorem 9, which establishes sufficient conditions
1Oeε,δ notation suppresses ε, δ and polylog(n) factors.
4
Under review as a conference paper at ICLR 2018
1
2
3
4
5
6
7
8
9
10
11
12
Algorithm 1: CORESET(P, u, ε, δ)
Input:	A set of training points P ⊆ Rd+1 × {-1, 1} containing n points,
a weight function u : P → R≥0,
an error parameter ε ∈ (0, 1), and failure probability δ ∈ (0, 1).
Output:	An ε-coreset (S, v) with probability at least 1 - δ.
// Compute the optimal solution using an Interior Point Method.
w* — InteriorPointMethod(P, u, C)
Kyi 一 U(Pyi)/ (2U(P) ∙ U(Pyi)) for each yi ∈ {-1,1};
// Compute an upper bound for the sensitivity of each point
according to Eqn.(5).
for i ∈ [n] do
P∆ 一 (Pyi - Pi) where Pyi = U(Pyi) Pq∈Pyi u(q) q
Y(Pi) -	U(P⅛ + f Pwr	^r(hw*,P∆i-	Kyi-)	+2f(p,w*)	kP∆k2 * *	+	hw*,P∆i	-
t J Pi∈[n] Y(Pi)
Let
m J Ω f εt2 (dlogt + log(1∕δ))
(K1,..., Kn) ~ Multinomial (m, ∏ = Y(Pi)/t ∀i ∈ [n])
S J {Pi ∈ P : Ki > 0}
// Compute the weights v : S → R≥0 for every point Pi ∈ S .
for i ∈ [n] do
I v(p.) J tKiu(Pi)
I V(Pi) J γ(Pi)∣s∣
return (S, v )
for the existence of small coresets depending on the properties of the data. Our theoretical results
also highlight the influence of the regularization parameter, C, in the size of the coreset.
Definition 4 (Sensitivity (Braverman et al., 2016)). The sensitivity of an arbitrary point P ∈ P,
P = (x, y) is defined as
u(P)f (P, w)
S(P) = SUp 干-----ΓΓ77——v
w∈Q	q∈P u(q)f (q, w)
where u : P → R≥0 is the weight function as before.
(4)
5.1 Sensitivity Lower B ound
We first prove the existence of a hard point set for which the sum of sensitivities is approximately
Ω(nC), ignoring d factors, which suggests that if the regularization parameter is too large, then the
required number of samples for property (3) to hold is Ω(n).
Lemma 5. There exists a set of n points P such that the sensitivity of each point Pi is bounded
below by Ω (n(2+nC)) and the sum of sensitivities is bounded below by Ω (dy+nC).
The same hard point set from Lemma 5 can be used to also prove a bound that is nearly exponential
in the dimension, d.
Corollary 6. There exists a set of n points P such that the total sensitivity is bounded below by
Ω ( d2 + c(2d∕√d)∖
ω I	d2+c	.∙
We next prove upper bounds on the sensitivity of each data point with respect to the complexity of
the input data. Despite the non-existence results established above, our upper bounds shed light into
the class of data sets for which coresets of sufficiently small size exist, and thus have potential to
significantly speed up SVM training.
5
Under review as a conference paper at ICLR 2018
5.2 Sensitivity Upper Bound
For any arbitrary point p = (xi , yi) ∈ P, let Pyi ⊂ P denote the set of points of label yi, let
Pyi = P \ Pyi be its complement, and let w* denote the optimal solution to the SVM problem
(2). We assume that the points are normalized to have a Euclidean norm of at most one, i.e.,
∀(x, y) ∈ P kx1:d k2 ≤ 1, where x1:d refers to original input point, without the bias embedding.
Lemma 7. The sensitivity of any point pi ∈ P is bounded above by
s(Pi) ≤ u^ + 2"∖ (S(hw*,P∆i- Ki)2+2f(P,w*)kP∆k2 + hw*,P∆i - Kyi
u(Pyi )	2f(P, w*)	C	2	C
= γ(pi),	(5)
where pδ = Pyi — Pi and Kyi = U(Pyi)/ (2U(P) ∙ U(Pyi)).
Let P+ = P1 ⊂ P and P- = P \ P1 denote the set of points with positive and negative labels
respectively. Let 夕+ and P- denote the weighted mean of the positive and labeled points respectively,
and for any Pi ∈ P+ let p∆i = P+ — Pi andp∆i = P— - Pi.
Lemma 8. The sum of sensitivities over all points P is bounded by
S(P) ≤ 2 + C(%r(P+) + Var(P-))
Pf (P,w*)
t,
(6)
where f(P, w*) is the optimal value of the SVM problem, and Var(P+) and Var(P-) denote the
total deviation of positive and negative labeled points from their corresponding label-specific mean
Var(P+) =	E	U(Pi)Ilp∆i∣∣2 =	E U(Pi)	I∣p+	- Pik2
pi∈P+	pi∈P+
Var (P-) =	X	U(Pi)I∣P∆i Il2 =	X U(Pi)	kP-	- Pik2.
pi∈P-	pi∈P-
Theorem 9. Given any ε ∈ (0, 1/2), δ ∈ (0, 1) and a weighted data set P with corresponding
weight function U, with probability greater than 1 - δ, Algorithm 1 generates an ε-coreset, i.e., a
weighted set (S, v), of size
S ∈ Ω (εt2(dlogt + log(1∕δ)))
in O(n3) time, where t is the upper bound on the sum of sensitivities from Lemma 8,
C (%r(P+) + %r(P-))
t = 2+-----------,	=-------.
√f (P ,w*)
For any subset T ⊆ P, let wT* denote the optimal separating hyperplane with respect to the set of
points in T . The following corollary immediately follows from Theorem 9 and implies that training
an SVM on an ε-coreset, (S, v), to obtain wS* yields a solution that is provably competitive with the
optimal solution on the full data-set, wP* = w*.
Corollary 10. Given any ε ∈ (0, 1/2), δ ∈ (0, 1) and a weighted data set (P, U), the weighted set
of points (S, v) generated by Alg. 1 satisfies
f((P,U),wS*) ≤ (1+4ε)f((P,U),wP*),
with probability greater than 1 - δ.
Sufficient Conditions Theorem 9 immediately implies that, for reasonable ε and δ, coresets of
polylogarithmic (in n) size can be obtained ifd = O(polylog(n)), which is usually the case in our
target applications, and if
C (Var(P+)+ Var(P-))
O(polylog(n)).
For example, a value of C ≤ Ionnn for the regularization parameter C satisfies the sufficient condition
for all data sets with normalized points.
6
Under review as a conference paper at ICLR 2018
Interpretation of Bounds Our approximation of the sensitivity of a point pi ∈ P, i.e., its relative
importance, is a function of the following highly intuitive variables.
1.	Relative weight with respect to the weights of points of the same label (u(pi)/u(Pyi)): the
sensitivity increases as this ratio increases.
2.	Distance to the label-specific mean point (kPyi - Pi k 2): points that are considered outliers
with respect to the label-specific cluster are assigned higher importance.
3.	Distance to the optimal hyperplane (hw*,P∆i): importance increases as distance of the
difference vector p∆ = Pyi — Pi to the optimal hyperplane increases.
Note that the sum of sensitivities, which dictates how many samples are necessary to obtain an ε-
coreset with probability at least 1 - δ and in a sense measures the difficulty of the problem, increases
monotonically with the sum of distances of the points from their label-specific means.
5.3 Extensions
We conclude our analysis with an extension of Lemma 7 to the case where only an approximately
optimal solution to the SVM problem is available.
Lemma 11. Consider the case where only a ξ-approximate solution W is available such that
f(P,W) ≤ f(P,w*) + ξ ,for ξ ∈ (0,f (P,w*)∕2). Then,, the sensitivity of any arbitrary point
Pi ∈ P is bounded above by
u(Pi)
S(Pi) ≤ u(pyi)+
Cu(Pi)
+ 4(f(P,W)- 2ξ) kp∆k2 + hW,p∆i-
2(f(P,W)- 2ξ)
where pδ = Pyi — Pi and Kyi = U(PyJ/ (2U(P) ∙ U(Pyiy) as in Lemma 7.
6 Results
We evaluate the performance of our coreset construction algorithm against synthetic and real-world,
publicly available data sets (Lichman, 2013). We compare the effectiveness of our method to uni-
form subsampling on a wide variety of data sets and also to Pegasos, one of the most popular
Stochastic-Gradient Descent based algorithm for SVM training (Shalev-Shwartz et al., 2011). For
each data set of size N, we selected a set of M = 10 subsample sizes S1, . . . , SM ⊂ [N] and
ran each coreset construction algorithm to construct and evaluate the accuracy subsamples sizes
S1, . . . , SM. The results were averaged across 100 trials for each subsample size. Our results of
relative error and sampling variance are shown as Figures 1 and 3. The computation time required
for each sample size and approach can be found in the Appendix (Fig. 4). Our experiments were
implemented in Python and performed on a 3.2GHz i7-6900K (8 cores total) machine with 16GB
RAM. We considered the following data sets for evaluation.
•	Pathological — 1, 000 points in two dimensional space describing two clusters distant from
each other of different labels, as well as two points of different labels which are close to
each other. We note that uniform sampling performs particularly poorly against this data
set due to the presence of outliers.
•	Synthetic & Synthetic100K— The Synthetic and Synthetic100K are datasets with
6, 000, 100, 000 points, each consisting of 3 and 4 dimensions respectively. The datasets
describe two blocks of mirrored nested rings of points, each of different labels such that
Gaussian noise has been added to them.
•	HTRU2 — 17, 898 radio emissions of Pulsar (rare type of Neutron star) each consisting of
9 features.
•	CreditCard 3— 30, 000 client entries each consisting of 24 features that include education,
age, and gender among other factors.
2https://archive.ics.uci.edu/ml/datasets/HTRU2
3https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients
7
Under review as a conference paper at ICLR 2018
•	Skin4 — 245, 057 random samples of B,G,R from face images consisting of 4 dimensions.
Evaluation We computed the relative error of the sampling-based algorithms with respect to the
cost of the optimal solution to the SVM problem, f (P, WP) and the approximate cost generated by
the subsample, f ((S, V),wS). We have also evaluated against Pegasos, running Pegasos the amount
of time needed to construct the coreset and comparing the resulted error, applying 128 repetitions as
presented at Figure 2. Furthermore, we have run our coreset constructing under streaming setting,
where subsamples are used as leaf size and half of the leaf’s size is then used to set the subsample
for our sampling approach. In addition, we also compared our coreset construction’s related error to
CVM’s related error with respect to the cost of the optimal solution to the SVM problem, as function
of subsample sizes. Finally, we have evaluated the variance of the estimators for the sampling-based
approaches and observed that the variances of the estimates generated by our coreset were lower
than those of uniform subsampling.
Pathological
Synthetic
-------- Uniform Sampling
--------Our Coreset
CreditCard
----Uniform Sampling
----Our Coreset
----All Data
0.0
200	300	400	500	600
Subsample Size
700	800
WOO 1250
Subsample Size
Uniform Sampling
Our Coreset
Uniform Sampling
⅛jrCoreset
All Data
Pathological
k--------Ξ
970	980
Running Time(msecs)
CreditCard
normal
0.10
5 0 5
1 1 O
0.0.0.
UΓ)山 φ>ωφ比
4000	4050
Running Time(msecs)
—Pegasos
—Our Coreset
—A l Data
500	1000	1500	2000	2500	3000
Subsample Size
Figure 1: The relative error of query evaluations with respect uniform and coreset subsamples for
the 4 data sets.
5 4 3 2
(R)」0t山 θ>∙sφB
290	300	310	320
Running Time(msecs)
t Pegasos Avg. normal
Pegasos
Our Coreset
All Data
1200.0 1202.5 1205.0 1207.5 1210.0 1212.5
Running Time(msecs)
----Our Coreset
----All Data
2000	2100	2200	2300
Running Time(msecs)
Figure 2: The relative error of query evaluations with respect to coreset running time and Pegasos
running time for the 4 data sets.
4https://archive.ics.uci.edu/ml/datasets/Skin+Segmentation/
8
Under review as a conference paper at ICLR 2018
7	Conclusion
We presented an efficient coreset construction algorithm for generating compact representations of
the input data points that provide provably accurate inference. We presented both lower and upper
bounds on the number of samples required to obtain accurate approximations to the SVM problem as
a function of input data complexity and established sufficient conditions for the existence of compact
representations. Our experimental results demonstrate the effectiveness of our approach in speeding
up SVM training when compared to uniform sub-sampling
The method presented in this paper is also applicable to streaming settings, using the merge-and-
reduce technique from coresets literature (Braverman et al., 2016).We conjecture that our coreset
construction method can be extended to significantly speed up SVM training for nonlinear kernels
as well as other popular machine learning algorithms, such as deep learning.
8	Appendix
500
ω 0.75
0.50
200	300	400
Coreset Size
SynthetidOOk
3 Uniform Sampling
Our Coreset
All Data
1000	1500	2000	2500
Coreset Size
Pathological
Synthetic
200	400	600	800
Coreset Size
----Uniform Sampling
----Our Coreset
All Data
All Data
----Uniform Sampling
----Our Coreset
CreditCard
W 0.03
0.02
0.00------i-----」——Lr
200	300	400	500	600	700	800
Coreset Size
--------------------!------!------U 0.000--------l-
500	750	1000 1250 1500 1750	500
Coreset Size
----Uniform Sampling
-----Θαr Coreset
----All Data
----Uniform Sampling
----Our Coreset
----Uniform Sampling
----Our Coreset
⅞ 0.004
Figure 3: The estimator variance of query evaluations. We note that due to the use of a judicious
sampling distribution based on the points’ sensitivities, the variance of our coreset estimator is lower
than that of uniform sampling for all data sets.
8.1	Proof of Lemma 5
Proof. Following Yang et al. (2017) we define the set of n points P ⊆ Rd+1 × {-1, 1}, each point
p ∈ P with weight u(p) = 1, such that for each i ∈ [n], among the first d entries of pi, exactly d/2
entries are equivalent to γ :
where R is the normalization factor (typically R = 1), the remaining d/2 entries among the first d
are set to 0, and pi(d+1) = yi as before. For each i ∈ [n], define the set of non-zero entries of pi as
the set
Bi = {j ∈ [d + 1] : pij 6= 0}.
Now, for bounding the sensitivity of point pi , consider the normal to the margin wi with entries
defined as
0 if j ∈ Bi ,
∀j ∈ [d + 1] wij =
1 /γ otherwise.
Note that for R = 1, ∣∣wi∣∣2 = (d∕2)(1∕γ2) = (d∕2)d∕(R + 1) = d2/4. We also have that
h(wi,pi) = 1 since pi ∙ Wi = fl∈& Pij Wij = (d∕2)(0) = 0. To bound the sum of hinge losses
9
Under review as a conference paper at ICLR 2018
Synthetic
Uniform Sampling
Our Corese
Uniform Sampling
Our Coreset
All Data
U n iform SamphM
Our Coreset
All Data
(SPU8①S)① UJLL 6u⊂ro二 IΛI>S + UOQon4≡suoo Baloo
HTRU
1.0-------Ufttform-Sam-pHng-
-----Our Coreset
0.8 ——All Data
0.6
0.4________________________
0.2
Subsample Size
0.0
250	500	750	1000 1250 1500
(R)」0」」山θ>QBφα
Pathological
----Uniform Sampling
----Our Coreset
--------All Data
CreditCard
600	800	1000	1200
Subsample Size
Uniform Sampling
Our Coreset
HTRU
Figure 4: Computation + Training Time vs. Sample Size
Uniform Sampling
Our Coreset
All Data
SynthetidOOk
---- Uniform Sampling
----Our Coreset
----All Data
---- Uniform Sampling
----Our Coreset
----All Data
250	500	750 1000 1250 1500 1750
Subsample Size
Figure 5: The relative error of query evaluations with respect uniform and coreset subsamples for
the 5 data sets in a streaming setting where the input points arrive one-by-one.
contributed by other points j ∈ [n], j = i note that Bi \ Bj = 0, thus:
hwi ,Pji = 工	WilPjl ≥ Y(1/Y) = 1,
l∈Bi∖Bj
which implies that h(pj, wi) = 0. Thus, it follows that H(wi) =	j∈[n] h(pj, wi) = 1.
Putting it all together, we have for the sensitivity of any arbitrary i ∈ [n]:
f (pi , w)
S(Pi) = sup ------------77------V
w∈Q j∈[n] f (pj , w)
≥ 82 + Ch(Pi,Wi)
一	||wi ||2 + c
dn + C
j.
10
Under review as a conference paper at ICLR 2018
Synthetic
----Uniform Sampling
----Our Coreset
----Core Vector Machine (CVM)
----All Data
200	300	400
Subsample Size
Figure 6: The relative error of query evaluations with respect
Machine (CVM) (Tsang et al., 2005) subsamples for the 4 data
750	1000	1250	1500	1750
Subsample Size
2
O
1OOOO 12000
SynthetidOOk
-Uniform Sampling
-Our Coreset
-Core Vector Machine (CVM)
■ All Data
400	500	600	700	800
Subsample Size
Skin
Uniform Sampling
Our Coreset
Core Vector Machine (CVM)
All Data
14000	16000	18000	20000
Subsample Size
uniform, coreset, and Core Vector
sets.
Moreover, we have for the sum of sensitivities that
s(pi) ≥
i∈[n]
d8—+ nC	( d2 + nC λ
ɪ+C	^2+C
□
8.2	Proof of Corollary 6
Proof. Consider the set of points P from the proof of Lemma 5 and note that
n=
2d (1 - 1/d)
√∏d∕2)
2d
≥ √d
= Ω(2d ∕√d).
□
8.3	Proof of Lemma 7
Proof. Consider any arbitrary point pi ∈ P and let p = xiyi for brevity when the point pi =
(xi, yi) is clear from the context. We proceed to bound s(pi)∕u(pi) by first leveraging the Lipschitz
11
Under review as a conference paper at ICLR 2018
continuity of hinge loss and convexity of f
S(Pi)= sup f(pi,w)
u(pi) w∈Q(P) f(P, w)
≤ sup
w∈Q
f(pyi ,w) + C [hw,pyi - Pi] +
f(P,w)
Lipschitz Continuity
∑q∈Pyi u(q)f (q,w)	C [hw,pyi - pi]+
≤ WsuQ	U(Pyi)f(P,w)	+ —fpw)一
=Su (f (Pyi,w) + f (Pyi ,w)) - f (Pyi,w) + C [hw,pyi - Pi] +
W∈Q	U(Pyi )f (P,w)	f(P, W)
__	1	C [hw,pyi - Pi]+	f(Pyi ,w)
=Uw) + WsuQ —T(Pw	U(Pyi )f (P,w)
Jensen’s Inequality
1	C [hw,pyi - Pi]+ -U(Pyi)/(2U(P) ∙ U(Pyi))
UPG+WsuQ	TPw
(7)
where the last inequality follows from the fact that for any w ∈ Q, kw1:dk2 ≥ 1 since the points are
normalized, and thus,
f(P；i,w)〉kwi：dk2 U(Pyi)
2U(Pyi) ≥ 2U(P )U(Pyi)
≥ U(Pyi)/ (2U(P) ∙ U(Pyi)).
Now consider the expression involving the supremum from (7) and let
Kyi= U(Pyi)/ (2U(P) ∙ U(Pyi))	(8)
be the constant independent of W in the numerator, let pδ = Pyi - P, and let wδ = W - w* for
notational convenience. We observe that by λ-strong convexity of the SVM objective function for
λ = 1, we have the inequality T(P, w* ) + λ kw - w* k22 /2 ≤ T(P, w) for any w ∈ Q. Thus, the
expression containing the supremum from (7) is bounded by
sup CKWM- Pi]、+- Kyi = sup C[hw iw M -力+ hw*,Pyi- Pi]+- Kyi
w∈Q	f (P, w)	w∈Q	f (P, w)
C[hw∆,P∆i + hw*, P∆i]+ - Kyi
≤ sup ---------------------/-------
-W∆∈β	f (P,w*) + λ kw∆k2 /2
[hw∆, P∆i + a]+ + b
≤ sup -----------2—ɪ;——,
W∆∈Q	ckw∆k2 + d
(9)
where a = hw*, P∆i, b = -Kyi /C, c = 1/(2C), and d = f(P, w*)/C are constants independent
ofw∆. Analytical optimization based on the gradient of the term above yields the optimal solution,
w∆ = 2cz, where
_ hw∆,P∆i + a + b
C kw∆l∣2 + d
We resolve the interdependency on w∆* by observing that
(10)
*	P∆	lP∆ l
hw∆ si = L吟=F
and
lw∆* l22
hw∆* , w∆* i
kP∆ k2
(2cz)2,
which implies that |w&∣∣2 = hw∆,pδi/(2cz). Putting it all together, We obtain
ʌ/(a + b)2 + dkpɪɪ2 + a + b
Z ≤ —-----------:------------
—	2d
=2"PC *、qq(hw*,P∆i - KyiIC)2 + 2f (P,w*) kP∆k2 + hw*,P∆i - Kyi6 ,
2f(P, w )
and the sensitivity bound follows.	□
12
Under review as a conference paper at ICLR 2018
8.4	Proof of Lemma 8
Proof. Let P+ = P1 ⊂ P and P- = P\P1 denote the set of points with positive and negative labels
respectively and let K+ and K- denote the corresponding constants defined by (8). Let p+ and P-
denote the weighted mean of the positive and labeled points respectively, and for any pi ∈ P+ let
P∆i = P+ - Pi and P∆i = P- - Pi
Since the sensitivity can be decomposed into sum over the two disjoint sets, i.e., S(P) =
Pp∈P s(P) = Pp∈P1 s(P) + Pp∈P- s(P) = S(P1) + S(P-), we consider first bounding S(P1).
Invoking Lemma 7 yields
S(PI) ≤ 1+ X fP⅛ (j(hw*,P∆ii-k+∕c )2 + 2f (P ,w*)∣HJ2 + hw-,P∆i i- K
≤ 1+ X fP¾(q(hw*,P∆ii-k+∕c)2+2f(P,w*)∣Hj2)
≤ 1+ f(PCW*) X u(Pi) QHJ2 P(P,wD)
f ,w pi∈P+
=1 + Pfc) pX+u(Pi)HJ2 2
where the second equality follows by the definition of K+ and the fact that
E U(Pi) hw*,pΔ) = E U(Pi) hw*,P+i - E U(Pi) hw* ,Pii
pi ∈P+	pi ∈P+	pi ∈P+
=hw*,ρ+i E U(Pi)- E u(Pi)hw*,pii = 0,
pi ∈P+	pi ∈P+
and the third by noting that (hw*,p∆) - K+/C)2 ≤ 2f(P,w*) ∣∣p∆. ∣∣2 since by Cauchy-Schwarz
and definition of f (P, w*) We have
hw*,P∆ii- K+/C ≤ I∣w*k2 ∣p∆i∣∣2 ≤ P2f(P,w*) ∣∣p∆i∣∣2 ∙
Using the same argument as above, an analogous bound holds for S(P-), thus we have
S(P) ≤2+ Pf(C,w*) (X U(Pi)∣∣p∆i∣∣2+ X U(Pi)∣∣P∆i∣∣2)
C (Var(P+)+ Var(P-))
=2 +--------,	==---- = t.
Pf(P ,w*)
□
8.5	Proof of Theorem 9
Proof. By Lemma 8 and Theorem 5.5 of Braverman et al. (2016) we have that the coreset con-
structed by our algorithm is an ε-coreset with probability at least 1 - δ if
|S| ≥ Ω (εt2(dlogt + log(1∕δ))),
where we used the fact that the VC dimension of a separating hyperplane in the case of a linear
kernel is bounded dim(F) ≤d + 1 = O(d) (Vapnik & Vapnik, 1998). Moreover, note that the
computation time of our algorithm is dominated by computing the optimal solution of the SVM
problem using interior-point Method which takes O(d3L) = O(n3) time (Nesterov & Nemirovskii,
1994), where L is the bit length of the input data.	□
13
Under review as a conference paper at ICLR 2018
8.6	Proof of Corollary 10
Proof. By theorem 9, (S, v) is an ε-coreset for (P, u) with probability at least 1 - δ, thus we have
f((P,u),wP) ≤ f((P,u),wS) ≤ f((S,v),wS) ≤ (1 + ε)f((P,U),Wp)
1-ε	1-ε
≤ (1 + 4ε)f((P,u),wp).
□
8.7	Proof of Lemma 11
Proof. The proof is almost identical to that of Lemma 7, thus we use the same techniques to arrive
at the following inequality with W instead of w* and with wδ = W - W:
S(Pi) =	f(Pi,w)
U(Pi)	w∈Q(P) f (P,w)
1	CKW,Pyi- Pi]+ - U(Pyi)/(2U(P) ∙ U(Pyi))
≤ U(Py^)+ WuQ	fPW
≤	1	+ Su C [hw∆,P∆i + hw,P∆i]+ - Kyi
u U(Pyi)	W∆∈Q	f(P, W*) + ∣∣W - W*k2 /2
Now, consider the supremum term and note that (i) f (P, w*) ≥ f (P, W) - ξ by definition of W and
(ii) ∣w - w*∣2 /2 ≥ ∣∣w - W∣2 /4 - ξ, since by the CaUchy-SchWarz inequality we have
∣w - w∣2 ≤2 (∣w - w*∣2 + i∣w* - w∣2)
≤2∣W-W*∣22+4ξ,
where the last inequality follows by strong convexity of f and by definition of our approximation:
∣w - w*∣2 V ffτ, 7 ffτ, *、V e
----2—2 ≤ f (P, W) - f (P, W ) ≤ ξ.
Combining (i) and (ii) yields for the supremum term
C [hW^mai + hw,Pδi]+ - Kyi
SuP
w∆∈Q	f(P, W*) + ∣W - W*∣2 /2
C ∖hW∆,p∆ + hW,PAi]+ — Kyi
≤ Sup
^w∆∈β	f(P,W)- 2ξ + ∣W∆k2 /4
[hW∆, P∆i + a]+ + b
≤ SuP	;；2,
w∆∈Q	c ∣W∆∣2 + d
where a = hW,pδ>, b = -KyJC, c = 1/(4C), and d = (f (P,W) - 2ξ) /C are constants inde-
pendent of W∆ .
This is the exact same expression as in the proof of Lemma 7, with the exception of slightly different
values for the constants above. Thus, analytical optimization yields the same optimal solution as
before and thus we have
s(pi) ≤ ι + qa+b)2+dkpjk2+a+b
U(Pi) — U(Pyi)	2d	,
and the lemma follows.	口
References
Pankaj K Agarwal and R Sharathkumar. Streaming algorithms for extent problems in high dimen-
sions. In Proceedings of the twenty-first annual ACM-SIAM symposium on Discrete Algorithms,
pp. 1481-1489. Society for Industrial and Applied Mathematics, 2010.
14
Under review as a conference paper at ICLR 2018
Pankaj K Agarwal, Sariel Har-Peled, and Kasturi R Varadarajan. Geometric approximation via
coresets. Combinatorial and computational geometry, 52:1-30, 2005.
Olivier Bachem, Mario Lucic, and Andreas Krause. Practical coreset constructions for machine
learning. arXiv preprint arXiv:1703.06476, 2017.
Vladimir Braverman, Dan Feldman, and Harry Lang. New frameworks for offline and streaming
coreset constructions. arXiv preprint arXiv:1612.00889, 2016.
Kenneth L Clarkson. Coresets, sparse greedy approximation, and the frank-wolfe algorithm. ACM
Transactions on Algorithms (TALG), 6(4):63, 2010.
Kenneth L Clarkson, Elad Hazan, and David P Woodruff. Sublinear optimization for machine
learning. Journal of the ACM (JACM), 59(5):23, 2012.
Dan Feldman and Michael Langberg. A unified framework for approximating and clustering data.
In Proceedings of the forty-third annual ACM symposium on Theory of computing, pp. 569-578.
ACM, 2011.
Dan Feldman, Matthew Faulkner, and Andreas Krause. Scalable training of mixture models via
coresets. In Advances in neural information processing systems, pp. 2142-2150, 2011.
Bemd Gartner and Martin Jaggi. Coresets for polytope distance. In Proceedings of the twenty-fifth
annual symposium on Computational geometry, pp. 33-42. ACM, 2009.
Sariel Har-Peled and Soham Mazumdar. On coresets for k-means and k-median clustering. In
Proceedings of the thirty-sixth annual ACM symposium on Theory of computing, pp. 291-300.
ACM, 2004.
Sariel Har-Peled, Dan Roth, and Dav Zimak. Maximum margin coresets for active and noise tolerant
learning. In IJCAI, pp. 836-841, 2007.
Elad Hazan, Tomer Koren, and Nati Srebro. Beating sgd: Learning svms in sublinear time. In
Advances in Neural Information Processing Systems, pp. 1233-1241, 2011.
Jonathan H Huggins, Trevor Campbell, and Tamara Broderick. Coresets for scalable bayesian lo-
gistic regression. arXiv preprint arXiv:1605.06423, 2016.
Thorsten Joachims. Training linear svms in linear time. In Proceedings of the 12th ACM SIGKDD
international conference on Knowledge discovery and data mining, pp. 217-226. ACM, 2006.
Michael Langberg and Leonard J Schulman. Universal ε-approximators for integrals. In Proceedings
of the twenty-first annual ACM-SIAM symposium on Discrete Algorithms, pp. 598-607. SIAM,
2010.
M. Lichman. UCI machine learning repository, 2013. URL http://archive.ics.uci.edu/
ml.
Gaelle Loosli and Stephane Canu. Comments on the “Core Vector Machines: Fast SVM Training
on Very Large Data Sets. Journal of Machine Learning Research, 8(Feb):291-301, 2007.
Mario Lucic, Olivier Bachem, and Andreas Krause. Linear-time outlier detection via sensitivity.
arXiv preprint arXiv:1605.00519, 2016.
Mario Lucic, Matthew Faulkner, Andreas Krause, and Dan Feldman. Training mixture models at
scale via coresets. arXiv preprint arXiv:1703.08110, 2017.
Manu Nandan, Pramod P Khargonekar, and Sachin S Talathi. Fast svm training using approximate
extreme points. Journal of Machine Learning Research, 15(1):59-98, 2014.
Yurii Nesterov and Arkadii Nemirovskii. Interior-point polynomial algorithms in convex program-
ming. SIAM, 1994.
Piyush Rai, Hal DaUme III, and Suresh Venkatasubramanian. Streamed learning: one-pass svms.
arXiv preprint arXiv:0908.0572, 2009.
15
Under review as a conference paper at ICLR 2018
Shai Shalev-Shwartz, Yoram Singer, Nathan Srebro, and Andrew Cotter. Pegasos: Primal estimated
sub-gradient solver for svm. Mathematical programming, 127(1):3-30, 2011.
Ivor W Tsang, James T Kwok, and Pak-Ming Cheung. Core vector machines: Fast svm training on
very large data sets. Journal of Machine Learning Research, 6(Apr):363-392, 2005.
Vladimir Naumovich Vapnik and Vlamimir Vapnik. Statistical learning theory, volume 1. Wiley
New York, 1998.
Jiyan Yang, Yin-Lam Chow, Christopher Re, and Michael W Mahoney. Weighted Sgd for \ell_p
regression with randomized preconditioning. arXiv preprint arXiv:1502.03571, 2017.
16