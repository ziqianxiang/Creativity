Under review as a conference paper at ICLR 2018
Theoretical properties of the global
optimizer of two-layer neural network
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we study the problem of optimizing a two-layer artificial neural
network that best fits a training dataset. We look at this problem in the setting
where the number of parameters is greater than the number of sampled points. We
show that for a wide class of differentiable activation functions (this class involves
most nonlinear functions and excludes piecewise linear functions), we have that
arbitrary first-order optimal solutions satisfy global optimality provided the hid-
den layer is non-singular. We essentially show that these non-singular hidden
layer matrix satisfy a “good” property for these big class of activation functions.
Techniques involved in proving this result inspire us to look at a new algorithmic,
where in between two gradient step of hidden layer, we add a stochastic gradi-
ent descent (SGD) step of the output layer. In this new algorithmic framework,
we extend our earlier result and show that for all finite iterations the hidden layer
satisfies the“good” property mentioned earlier therefore partially explaining suc-
cess of noisy gradient methods and addressing the issue of data independency
of our earlier result. Both of these results are easily extended to hidden layers
given by a flat matrix from that of a square matrix. Results are applicable even
if network has more than one hidden layer provided all inner hidden layers are
arbitrary, satisfy non-singularity, all activations are from the given class of differ-
entiable functions and optimization is only with respect to the outermost hidden
layer. Separately, we also study the smoothness properties of the objective func-
tion and show that it is actually Lipschitz smooth, i.e., its gradients do not change
sharply. We use smoothness properties to guarantee asymptotic convergence of
O(1/number of iterations) to a first-order optimal solution.
1	Introduction
Neural networks architecture has recently emerged as a powerful tool for a wide variety of applica-
tions. In fact, they have led to breakthrough performance in many problems such as visual object
classification (Krizhevsky et al., 2012), natural language processing (Collobert & Weston, 2008)
and speech recognition (Mohamed et al., 2012). Despite the wide variety of applications using neu-
ral networks with empirical success, mathematical understanding behind these methods remains a
puzzle. Even though there is good understanding of the representation power of neural networks
(Barron, 1994), training these networks is hard. In fact, training neural networks was shown to
be NP-ComPlete for single hidden layer, two node and sgn(∙) activation function (Blum & RivesL
1988). The main bottleneck in the optimization problem comes from non-convexity of the problem.
Hence it is not clear how to train them to global oPtimality with Provable guarantees.
Neural networks have been around for decades now. A sudden resurgence in the use of these meth-
ods is because of the following: DesPite the worst case result by Blum & Rivest (1988), first-order
methods such as gradient descent and stochastic gradient descent have been surPrisingly successful
in training these networks to global oPtimality. For examPle, Zhang et al. (2016) emPirically showed
that sufficiently over-Parametrized networks can be trained to global oPtimality with stochastic gra-
dient descent.
Neural networks with zero hidden layers are relatively well understood in theory. In fact, several au-
thors have shown that for such neural networks with monotone activations, gradient based methods
will converge to the global oPtimum for different assumPtions and settings (Mei et al., 2017; Hazan
et al., 2015; Kakade et al., 2011; Kalai & Sastry, 2009).
1
Under review as a conference paper at ICLR 2018
Despite the hardness of training the single hidden layer (or two-layer) problem, enough literature is
available which tries to reduce the hardness by making different assumptions. E.g., Choromanska
et al. (2014) made a few assumptions to show that every local minimum of the simplified objective
is close to the global minimum. They also require some independent activations assumption which
may not be satisfied in practice. For the same shallow networks with (leaky) ReLU activations, it
was shown in Soudry & Carmon (2016) that all local minimum are global minimum of the modified
loss function, instead of the original objective function. Under the same setting, Xie et al. (2016)
showed that critical points with large “diversity” are near global optimal. But ensuring such condi-
tions algorithmically is difficult.
All the theoretical studies have been largely focussed on ReLU activation but other activations have
been mostly ignored. In our understanding, this is the first time a theoretical result will be pre-
sented which shows that for almost all nonlinear activation functions including softplus, an arbitrary
first-order optimal solution is also the global optimal provided certain “simple” properties of hidden
layer. Moreover, we show that a stochastic gradient descent type algorithm will give us those re-
quired properties for free for all finite number of iterations hence even if the hidden layer variables
are data dependent, we still get required properties. Our assumption on data distribution is very
general and can be reasonable for practitioners. This comes at two costs: First is that the hidden
layer of our network can not be wider than the dimension of the input data, say d. Since we also
look at this problem in over-parametrized setting (where there is hope to achieve global optimality),
this constraint on width puts a direct upper-bound of d2 on the number of data points that can be
trained. Even though this is a strong upper bound, recent results from margin bounds (Neyshabur
et al., 2017) show that if optimal network is closer to origin then we can get an upper bound on
number of samples independent of dimension of the problem which will ensure closeness of popu-
lation objective and training objective. Second drawback of this general setting is that we can prove
good properties of the optimization variables (hidden layer weights) for only finite iterations of the
SGD type algorithm. But as it is commonly known, stochastic gradient descent converges to first
order point asymptotically so ideally we would like to prove these properties for infinitely many
iterations. We compare our results to some of the prior work of Xie et al. (2016) and Soudry &
Carmon (2016). Both of these papers use similar ideas to examine first order conditions but give
quite different results from ours. They give results for ReLU or Leaky ReLU activations. We, on
the other hand, give results for most other nonlinear activations, which can be more challenging. We
discuss this in section 3 in more detail.
We also formally show that even though the objective function for training neural networks is non-
convex, it is Lipschitz smooth meaning that gradient of the objective function does not change a
lot with small changes in underlying variable. To the best of our knowledge, there is no such re-
sult formally stated in the literature. Soltanolkotabi et al. (2017) discuss similar results, but there
constant itself depends locally on wmax , a hidden layer matrix element, which is variable of the
the optimization function. Moreover, there result is probabilistic. Our result is deterministic, global
and computable. This allows us to show convergence results for the gradient descent algorithm,
enabling us to establish an upper bound on the number of iterations for finding an ε-approximate
first-order optimal solution (kVf Ok ≤ ε). Therefore our algorithm will generate an ε-approximate
first-order optimal solution which satisfies aforementioned properties of the hidden layer. Note that
this does not mean that the algorithm will reach the global optimal point asymptotically. As men-
tioned before, when number of iterations tend to infinity, we could not establish “good” properties.
We discuss technical difficulties to prove such a conjecture in more detail in section 5 which details
our convergence results.
At this point we would also like to point that there is good amount of work happening on shallow
neural networks. In this literature, we see variety of modelling assumptions, different objective func-
tions and local convergence results. Li & Yuan (2017) focuses on a class of neural networks which
have special structure called “Identity mapping”. They show that if the input follows from Gaus-
sian distribution then SGD will converge to global optimal for population objective of the “identity
mapping” network. Brutzkus & Globerson (2017) show that for isotropic Gaussian inputs, with one
hidden layer ReLU network and single non-overlapping convolutional filter, all local minimizers are
global hence gradient descent will reach global optimal in polynomial time for the population ob-
jective. For the same problem, after relaxing the constraint of isotropic Gaussian inputs, they show
that the problem is NP-complete via reduction from a variant of set splitting problem. In both of
these studies, the objective function is a population objective which is significantly different from
training objective in over parametrized domain. In over-parametrized regime, Soltanolkotabi et al.
(2017) shows that for the training objective with data coming from isotropic Gaussian distribution,
2
Under review as a conference paper at ICLR 2018
provided that we start close to the true solution and know maximum singular value of optimal hidden
layer then corresponding gradient descent will converge to the optimal solution. This is one of its
kind of result where local convergence properties of the neural network training objective function
have studied in great detail.
Our result differ from available current literature in variety of ways. First of all, we study the training
problem in the over-parametrized regime. In that regime, the training objective can be significantly
different from population objective. Moreover, we study the optimization problem for many general
non-linear activation functions. Our result can be extended to deeper networks when considering the
optimization problem with respect to outermost hidden layer. We also prove that stochastic noise
helps in keeping the aforementioned properties of hidden layer. This result, in essence, provides
justification for using stochastic gradient descent.
Another line of study looks at the effect of over-parametrization in the training of neural networks
(Haeffele & Vidal, 2015; Nguyen & Hein, 2017). These result are not for the same problem as they
require huge amount of over-parametrization. In essence, they require the width of the hidden layer
to be greater than number of data points which is unreasonable in many settings. These result work
for fairly general activations as do our results but we require a moderate over-parametrization, width
× dimension ≥ number of data population, much more reasonable in practice as pointed before from
margin bound results. They also work for deeper neural network as do our results when optimization
is with respect to outermost hidden layer (and aforementioned technical properties are satisfied for
all hidden layers).
2	Notation and Problem of Interest
We define set [q] := {1, . . . , q}. For any matrix A ∈ Ra×b, we write vect(A) ∈ Rab×1 as vector
form of the matrix A. For any vector z ∈ Rk, we denote h(z) := h(z[1]), . . . , h(z[k])T, where
z[i] is the i-th element in vector z. Bi(r) represents a li-ball of radius r, centred at origin. We define
component-wise product of two vectors with operator .
We say that a collection of vectors, {vi}iN=1 ∈ Rd, is full rank if rank v1 . . . vN =
min{d, N}. Similarly, we say that collection of matrices, {Mi}iN=1 ∈ Rn×d, is full rank if
rank [vect(M1) . . . vect(Mk)] = min{N, nd}.
A fully connected two-layer neural network has three parameters: hidden layer W, output layer θ
and activation function h. For a given activation function, h, we define neural network function as
φW,θ (u) := θT h(W u).
In the above equation, W ∈ Rn×d is hidden layer matrix, θ ∈ Rn is the output layer. Finally
h : R → R is an activation function.
The main problem of interest in this paper is the two-layer neural network problem given by
min
W ∈Rn×d
θ∈Rn
1N
f (W,θ) ：=2N X(Vi
i=1
- φW,θ(ui))2.
(2.1)
In this paper, we assume that (ui, vi) ∈ Rd × R, i ∈ [N] are independently distributed data point
and each ui is sampled from a d-dimensional Lebesgue measure.
3	The basic idea and the Algorithm
First-order optimality condition for the problem defined in (2.1), with respect to W[j, k] (j-th row,
k-th column element of matrix W) ∀ j ∈ [n], ∀ k ∈ [d] is
1N
VWf (W,θ)[j, k] = N Y{vi - θτh(Wui)}h0(W[j, :]ui)θ[j]ui[k] = 0.	(3.1)
Equation (3.1) is equivalent to
N
X{vi - θτh(Wui)}(h0(Wui) Θ θ)uiT = 0.	(3.2)
i=1
3
Under review as a conference paper at ICLR 2018
(3.1) can also be written in a matrix vector product form:
Ds = 0,
(3.3)
where
"h0(W[1, Ru1)θ[1]u1
D :二	.
.
h0(W[d, Ju1)θ[d]u1
h0(W[1, :]uN)θ[1]uN
.	and s :
.
h0(W[d,:]UN)θ[d]uN
v1 - θTh(Wu1)
.
.
.
vN - θTh(WuN)
Notice that if matrix D ∈ Rnd×N is of full column rank (which implies nd ≥ N, i.e., number of
samples is less than number of parameters) then it immediately gives us that s = 0 which means
such a stationary point is global optimal. This motivates us to investigate properties of h under
which we can provably keep matrix D full column rank and develop algorithmic methods to help
maintain such properties of matrix D.
Note that similar approach was explored in the works of Soudry & Carmon (2016) and Xie et al.
(2016). To get the full rank property for matrix D, Soudry & Carmon (2016) use leaky ReLu
function. Basically this leaky activation function adds noise to entries of matrix D which allows
them to show matrix D is full rank and hence all local minimums are global minimums. So this
is essentially a change of model. We, on the other hand, do not change model of the problem.
Moreover, we look at the algorithmic process of finding W differently. We show that SGD will
achieve full rank property of matrix D with probability 1 for all finite iterations. So this is essentially
a property of the algorithm and not of the model. Even if that is the case, to show global optimality,
we need to prove that matrix D is full column rank in asymptotic sense.
That question was partly answered in Xie et al. (2016). They show that matrix D is full column
rank by achieving a lower bound on smallest singular value of matrix D. But to get this, they
need two facts. First, the activation function has to be ReLu so that they can find the spectrum of
corresponding kernel matrix. Second, they require a bound on discrepancy of weights W. These
conditions are strong in the sense that they restrict the analysis to a non-differentiable activation
function and finding an algorithm satisfying discrepancy constraint on Wcan be a difficult task. On
other hand, our results are proved for a simple SGD type algorithm which is easy to implement. But
we do not get a lower bound on singular value of D in asymptotic sense. There are obvious pluses
and minuses for both types of results.
For the rest of the discussion, we will assume that n = d (our results can be extended to case n ≤ d
easily) and hence W is a square matrix. In this setting, we develop the following algorithm whose
output is a provable first-order approximate solution. Here we present the algorithm and in next
sections we will discuss conditions that are required to satisfy full rank property of matrix D as well
as convergence properties of the algorithm.
In Algorithm 1, we use techniques inspired from alternating minimization to minimize with respect
to θ and W. For minimization with respect to θ, we add gaussian noise to the gradient information.
This will be useful to prove convergence of this algorithm. We use randomness in θ to ensure some
“nice” properties of W which help us in proving that matrix D generated along the trajectory of
Algorithm 1 is full column rank. More details will follow in next section.
The algorithm has two loops. An outer loop implements a single gradient step with respect to hidden
layer, W . For each outer loop iteration, there is an inner loop which optimizes objective function
with respect to θ using a stochastic gradient descent algorithm. In the stochastic gradient descent,
We generate a noisy estimated of Vθf (W, θ) as explained below.
Let ξ ∈ Rd be a vector whose elements are i.i.d. Gaussian random variable with zero mean. Then
for a given value of W we define stochastic gradient w.r.t. θ as follows:
GW (θ, ξ) =Vθf(W,θ)+ξ.	(3.4)
Then we know that
E[GW (θ, ξ)] =Vθf(W,θ).
We can choose a constant σ > 0 such that following holds
EhGW (θ, ξ) - Vθf (W, θ)2i ≤σ2.	(3.5)
Moreover, in the algorithm we consider a case where θ ∈ R. Note that R can be kept equal to Rd
but that will make parameter selection complicated. In our convergence analysis, we will use
R := B2(R/2),	(3.6)
4
Under review as a conference paper at ICLR 2018
for some constant R, to make parameter selection simpler. We use prox-mapping Px : Rd → R as
follows:
Pχ(y) = argmin hy,z - Xi + ɔkz - x∣∣2∙	(3.7)
z∈R	2
In case R is a ball centred at origin, solution of (3.7) is just projection of x - y on that ball. For case
where R = Rd then the solution is quantity x - y itself.
Algorithm 1 SGD-GD Algorithm
procedure
Wo J Random d X d matrix
θo J Random d vector
Initialize No to predefined iteration count for outer ietaration
Initialize Ni to predefined iteration count for inner iteration
Begin outer iteration:
for k = 0,1, 2,..., No do
θ1 J θk
Begin inner iteration:
for i = 1,2,. ..,N do _
Θi+1 J Pθi(βiGwk(θi,ξk))
ii
θi+1 = ( P βτ)	P βτθτ +1
τ=1	τ=1
end for _
θk+1 J θNi + 1
Wk + 1 J Wk — YkVwf (Wk, θk + l)
end for
return {WNo+1; θNo+1}
end procedure
Notice that the problem of minimization with respect to θ is a convex minimization problem. So
we can implement many procedures developed in the Stochastic optimization literature to get the
convergence to optimal value (Nemirovski et al., 2009).
In the analysis, we note that one does not even need to implement complete inner iteration as we
can skip the stochastic gradient descent suboptimally given that we improve the objective value with
respect to where we started, i.e.,
f(Wk,θk+1) ≤f(Wk,θk).	(3.8)
In essence, if evaluation of f for every iteration is not costly then one might break out of inner iter-
ations before running Ni iterations. If it is costly to evaluate function values then we can implement
the whole SGD for convex problem with respect to θ as specified in inner iteration of the algorithm
above. In each outer iteration, we take one gradient decent step with respect to variable W . We
have total of No outer iterations. So essentially We evaluate V f (W, ∙) a total of NoNi times and
VWf (∙,θ) total of No times.
Overall, this algorithm is neW form of alternate minimization, Where one iteration can be potentially
left suboptimally and other one is only one gradient step.
4	Arbitrary Point satisfying First order optimality conditions
We prove in this section that arbitrary first order optimal points are globally optimal. One does not
expect to have an arbitrary first order optimal point because it has to depend on data. We still Would
like to put our analysis here because that inspires us to consider a neW algorithmic frameWork in
Section 3 providing similar results for all finite iterations of the algorithm.
We say that h : R → R satisfy the condition “C1” if
• ∀ interval (a, b), @ {c1, c2, c3} ∈ R3 s.t.
{h0(x) = c1, ∀x ∈ (a, b)} or
{(x + c2)h0(x) + h(x) = c3, ∀x ∈ (a, b)}.
5
Under review as a conference paper at ICLR 2018
One can easily notice that most activation functions used in practice e.g.,
•	(Softplus) h(x) := ln(1 + ex),
•	(Sigmoid) h(x):=耳I-X,
-x
•	(Sigmoid symmetric) h(x) := 1+∣-x ,
•	(Gaussian) h(x) := e-x2,
•	(Gaussian Symmetric) h(x) := 2e-x2 - 1,
•	(Elliot) h(x) := 2(i+∣χ∣) +0.5,
•	(Elliot Symmetric) h(x) := j+χ^,
•	(Erf h(x):= -2∏ R e-t2/2dt,
0
•	(Hyperbolic tangent) h(x) := tanh(x),
satisfy the condition C1. Note that h0(x) also satisfy condition C1 for all of them. In fact, except
for very small class of functions (which includes linear functions), none of the continuously differ-
entiable functions satisfy condition C1.
We first prove a lemma which establishes that columns of the matrix D (each column is a vector
form of d × d matrix itself) are linearly independent when W = Id and h0 satisfies condition C1. We
later generalise it to any full rank W using a simple corollary. The statement of following lemma is
intuitive but its proof is technical.
Lemma 4.1 Suppose xi ∈ Rd are independently chosen vectors from any d-dimensional Lebesgue
measure and let h : R → R be any function that satisfies condition C1 then collection of matrices
h(xi)xiT , i ∈ [N] are full rank with measure 1.
Now Lemma 4.1 gives us a simple corollary:
Corollary 4.2 IfW isa nonsingular square matrix and ui ∈ Rd is independently sampled from
a
N
Lebesgue measure then the collection of matrices
h(Wui)uiT
i=1
is full rank with measure 1.
This means that ifui in the Problem (2.1) are coming from a Lebesgue measure then by corollary 4.2
we have h(Wui)uiT will be a full rank collection given that we have maintained full rank property
of W . Now note that in the first-order condition, given in (3.3), row of matrix D are scaled by
constant factors θ[j]'s, j ∈ [d]. Notice that We may assume θ[j] = 0 because otherwise there is
no contribution of corresponding j -th row of W to the Problem (2.1) and we might as well drop
it entirely from the optimization problem. Hence we can rescale rows of matrix D by factor j
without changing the rank. In essence, corollary 4.2 implies that matrix D is full rank when W is
full rank. So by our discussion in earlier section, we show that satisfying first-order optimality is
enough to show global optimality under condition C1 for data independent W.
Remark 4.3 Due to Lemma 4.1 and corollary 4.2 then, rank of collection h(ui)uiT is invariant
under any rotation.
Remark 4.4 As a result of corollary above one can see that the collection of vectors h(Wxi) is
full rank under the assumption that W is non-singular, xi ∈ Rd are independently sampled from
Lebesgue measure and h satisfies condition C1.
Remark 4.5 Since collection h(Wui) is also full rank, we can say that zi := h(W1 ui) are inde-
pendent and sampled from a Lebesgue measure for a non-singular matrix W1. Applying the Lemma
i	i iT
to zi, we have collection ofmatricesg(W2zi)zi are full rank with measure 1for non-singular W2
and g satisfying condition C1. So we see that for multiple hidden layers satisfying non-singularity,
we can apply full rank property for collection of gradients with respect to outermost hidden layer.
6
Under review as a conference paper at ICLR 2018
Remark 4.6 If W ∈ Rn×d is such that n ≤ d and W is full row rank, then we can extend its
basis to create W0 and apply corollary 4.2 to get that h(W0ui)uiT is full rank with measure 1. So
this implies that h(W ui)uiT must have been full rank with probability 1 otherwise we will have
contradiction.
Remark 4.7 We can extend corollary 4.2 to a general result that h(W ui)uiT has rank
min{rank(W)d, N} with measure 1 by removing dependent rows and using remark 4.6.
5 Convergence results for Stochastic Gradient Descent
Even though We have proved that collection {h(Wui)uiT }N=1 is full rank in the previous section,
we need such W to be independent of data. In general, any algorithm will use data to find W and it
appears that results in previous section are not meaningful in practice.
HoWever, the analysis of Lemma 4.1 motivates the idea that stochastic noise of θ might help in
obtaining the required properties of W and D along the trajectory of Algorithm 1. In this section We
first prove that by using random noise in stochastic gradient on θ gives a non-singular Wk in every
iteration. Then using this fact, We prove that matrix D generated along the algorithm is also full
rank. The proof techniques are very similar to proof of Lemma 4.1.
Later on, We Will also shoW that overall algorithm Will converge to approximate first-order optimal
solution to Problem (2.1) by using smoothness properties. It should be noted hoWever that this can
not guarantee convergence to a global optimal solution. To prove such a result, one needs to analyze
the smallest singular value of random matrix D, defined in (3.3). More specifically, We have to shoW
that σmin(D) decreases at the rate sloWer than the first-order convergence rate of the algorithm so
that the overall algorithm converges to the global optimal solution. Even ifitis very difficult to prove
such a result in theory, We think that such an assumption about σmin (D) is reasonable in practice.
NoW We analyze the algorithm. For the sake of simplicity of notation, let us define
ξ[k] := {ξ[1Ni],...,ξ[kNi]}	(5.1)
and
ξ[jNi] = {ξ1j ... ξNji},	(5.2)
Where Ni is the inner iteration count in Algorithm 1. Essentially ξ [k] contains the record of all
random samples used until the k-th outer iteration in Algorithm 1 and ξ[jN ] contains record of all
random samples used in the inner iterations of j-th outer iteration.
Lemma 5.1 Pr{∃ v such that Wkv = 0ξ[k-1] } = 0, ∀ k ≥ 0, where Wk are matrices generated
by Algorithm 1 and measure Pr{.ξ[k-1]} is w.r.t. random variables ξ[kN ].
NoW that We have proved that Wk ’s generated by the algorithm are full rank, We shoW that matrix
D generated along the trajectory of the algorithm is full rank for any finite number of iterations. We
use techniques inspired from Lemma 4.1 but this time We use Lebesgue measure over Θ rather than
data. Over randomness of Θ, We can shoW that our algorithm Will not produce any W such that
corresponding matrix D is rank deficient. Since Θ is essentially designed to be independent of data
so We Will not produce rank deficient D throughout the process of randomized algorithm.
Lemma 5.2 Suppose W = W0 + DvZ where Dv := diag(v[i], i ∈ [d]) and v is a random vector
with Lebesgue measure in Rd. W0 , Z ∈ Rd×d and Z 6= 0. Let h be a function which follows condi-
tion C1. Also assume that W is full rank with measure 1 over randomness of v. Then h(Wui)uiT is
full rank with measure 1 over randomness of v.
Lemma 5.3 Collection of matrices h0(Wk+1ui)uiT are full rank with measure 1, where the measure
is over randomness of ξ[kN+1]
Proof. We knoW that
N
Wk+1 = Wk + γkΘk+1 X h0(Wkuj)ujT (vi - θTh(Wkuj)).
j=1
7
Under review as a conference paper at ICLR 2018
Now apply Lemma 5.2 to obtain the required result.
Note that Lemma 5.3 is very similar to the result in section 4. Some remarks are in order.
Remark 5.4 As a result of Lemma 5.3 above, one can see that collection of vectors h(Wk ui) is full
rank for all finite iterations of Algorithm 1.
Remark 5.5 If we have a neural network with multiple hidden layer, we can assume that inner lay-
ers are initialized to arbitrary full rank matrices and we are optimizing w.r.t. outermost hidden layer.
Corollary 4.2 and Remark 4.4 give us that input to outermost hidden layer are independent vectors
sampled from some lebesgue measure. Then applying Algorithm 1 to optimize w.r.t. outermost
hidden layer will give us similar results as mentioned in Lemma 5.3.
Hence we showed that algorithm will generate full rank matrix D for any finite iteration.
Now to prove convergence of the algorithm, we need to analyze the function f (defined in (2.1))
itself. We show that f is a Lipschitz smooth function for any given instance of data {ui, vi}iN=1.
This will give us a handle to estimate convergence rates for the given algorithm.
Lemma 5.6 Assuming that h : R → R is such that its gradients, hessian as well as values are
bounded and data {ui , vi}iN=1 is given then there exists a constant L such that
IlVWf(Wι,θ) -Vwf(W2,θ)∣∣F ≤ LllWI- W2∣∣F.	(5.3)
Moreover, a possible upper bound on L can be as follows:
N	N
L ≤ N θmax(Lh0 (X kuik2∣vil) + √2dLhh0 kθk2(X kuik2))
i=1	i=1
Remark 5.7 Before staing the proof, we should stress that assumptions on h is satisfied by most
activation functions e.g., sigmoid, sigmoid symmetric, gaussian, gaussian symmetric, elliot, elliot
symmetric, tanh, Erf.
Remark 5.8 Note that one can easily estimate value of L given data and θ. Moreover, if we put
constraints on ∣θ∣2 then L is constant in every iteration of the algorithm 1. As mentioned in section
3, this will provide an easier way to analyze the algorithm.
Lemma 5.9 Assuming that scalarfunction h is such that ∣h(∙)∣ ≤ U then there exists Lθ s.t.
∣∣Vwf(W,θ1) -Vwf(W,θ2)∣∣2 ≤Lθ∣∣θ1 -θ2∣∣2	(5.4)
Notice that Lemma 5.9 gives us value of Lθ irrespective of value of W or data. Also observe that
f (W, ∙) is convex function since hessian
1N
V2f (W,θ) = N N h(Wui)h(Wui)T,
which is the sum of positive Semidefinite matrices. By Lemma 5.9, We know that f (W, ∙) is smooth
as well. So we can use following convergence result provided by Lan (2012) for stochastic composite
optimization. A simplified proof can be found in appendix.
Theorem 5.10 Assume that stepsizes βi satisfy 0 < βi ≤ 1∕2Lθ, ∀ i ≥ 1. Let {θa+1}i≥1 be the
sequence computed according to Algorithm 1. Then we have,
E[f (Wk,θf+Vι) - f (Wk,θWk)] ≤ Ko(i), ∀ i ≥ 1,∀ k ≥ 0,	(5.5)
(i 、- 1	_	i	i
P βτ)	∣∣θι-θWk∣∣2 + σ2 P β2
τ=1	τ=1
iteration and σ is defined in (3.5).
where θι is the starting point for inner
8
Under review as a conference paper at ICLR 2018
Now we look at a possible strategy of selecting stepsize βi . Suppose we adopt a constant stepsize
policy then we have βi = β, ∀ i ∈ [Ni]. Then we have
E[f(Wk,θNVi+ι) - f(Wk,θWfc)] ≤
H-θWJ2 + σ2β
Niβ	+ σ e.
Now if We choose	____
β=min n 2Lθ ,∕N1^ o，	(5.6)
we get
Ef(Wk,θNi+1) - f(Wk,θWfc)] ≤ ∣∣θ1-θwfc∣∣2h^Nθ + √Ni + √N=.
By Lemma 5.6, the objective function for neural networks is Lipschitz-smooth with respect to the
hidden layer, i.e., it satisfies eq (5.3). Notice that it is equivalent to saying
f(W2,w) - f(W1,w) -(Vwf(W1,w),W2 - Wii I ≤ 2∣∣W1 - W2∣∣2,	∀ W1,W2 ∈ Rd×d.
(5.7)
Since we have a handle on the smoothness of objective function, we can provide a convergence
result for the overall algorithm.
Theorem 5.11 Suppose Yk < L then we have
No	Ni	-1	2	Ni	k2 2
「	N	f(W0,θ0)+P(Pβk)i 与+p舟σ两
E kminNUVfW(Wk,θk+i)∣∣F ≤ -----------kʒrθ=1——L~T=..............-，
L ,...,	」	P (Yk-L∕2γk)
k=0
(5.8)
where R∕2 is the radius of origin centred ball, R in algorithm, defined as R := {r ∈ Rd : krk2 ≤
打
In view of Theorem 5.11, we can derive a possible way of choosing γk, σ and Ni to obtain a conver-
gence result. More specifically, if Ni = N0,σ = √= ,γk = L and βk is chosen according to (5.6)
then we have
∣	∣2	2Lf(W0,θ0)+R2(Lθ+ 1∕2) + 1
E Ik*,NnVW f(Wk ,θk + i"l ≤ ----------------No----------------
Note that in the algorithm 1, we have proved that having a stochastic noise helps keeping matrix D
full rank for all finite iterations. Then in Theorem 5.11, we showed a methodical way of achieving
approximate first order optimal. So essentially at the end of the finitiely many steps of algorithm 1,
we have a point W which satisfies full rank property ofD and is approximately first order optimal.
We think this kind of result can be extended to variety of different first order methods developed
for Lipschitz-smooth non-convex optimization problems. More specifically, accelerated gradient
method such as unified accelerated method proposed by Ghadimi et al. (2015) or accelerated gradi-
ent method by Ghadimi & Lan (2016) can be applied in outer iteration. We can also use stochastic
gradient descent method for outer iteration. For this, we need a stochastic algorithm that is de-
signed for non-convex and Lipschitz smooth function optimization. Randomized stochastic gradi-
ent method, proposed by Ghadimi & Lan (2013), Stochastic variance reduction gradient method
(SVRG) by Reddi et al. or Simplified SVRG by Allen-Zhu & Hazan can be employed in outer itera-
tion. Convergence of these new algorithms will follow immediately from the convergence results of
respective studies. Some work may be needed to prove that they hold matrix D full rank. We leave
that for the future work. We also leave the problem of proving a bound on singular value for future.
This will close the gap between empirical results and theory.
Value of Lipschitz constant, L, puts a significant impact on the running time of the algorithm. Notice
that if L increases then correspondingly No and Ni increase linearly with L. So we need methods
by which we can reduce the value of the estimate of L. One possible idea would be to use l1 -ball
for feasible region of θ. More specifically, if R = B1 (R∕2) then we can possibly enforce sparsity
on θ which will allow us to put better bound on L.
9
Under review as a conference paper at ICLR 2018
References
Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization. In Pro-
ceedings of the 33rd International Conference on International Conference on Machine Learning
-Volume 48,ICML'16,pp. 699-707.
Andrew R. Barron. Approximation and estimation bounds for artificial neural networks. Machine
Learning, pp. 115-133, 1994.
Avrim Blum and Ronald L. Rivest. Training a 3-node neural network is np-complete. In Proceedings
of the First Annual Workshop on Computational Learning Theory, COLT ’88, pp. 9-18, 1988.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian
inputs. CoRR, 2017.
Anna Choromanska, Mikael Henaff, Michael Mathieu, Gerard Ben Arous, and Yann LeCun. The
loss surface of multilayer networks. 2014.
Ronan Collobert and Jason Weston. A unified architecture for natural language processing: Deep
neural networks with multitask learning. In Proceedings of the 25th International Conference on
Machine Learning, ICML ’08, pp. 160-167, 2008.
Saeed Ghadimi and Guanghui Lan. Stochastic first- and zeroth-order methods for non-convex
stochastic programming. SIAM Journal on Optimization, pp. 2341-2368, 2013.
Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for nonconvex nonlinear and
stochastic programming. Math. Program., 156:59-99, 2016.
Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang. Generalized uniformly optimal methods for
nonlinear programming. CoRR, 2015.
Benjamin D. Haeffele and Rene Vidal. Global optimality in tensor factorization, deep learning, and
beyond. CoRR, 2015.
Elad Hazan, Kfir Y. Levy, and Shai Shalev-Shwartz. Beyond convexity: Stochastic quasi-convex
optimization. In Proceedings of the 28th International Conference on Neural Information Pro-
cessing Systems - Volume 1, pp. 1594-1602, 2015.
Sham Kakade, Adam Tauman Kalai, Varun Kanade, and Ohad Shamir. Efficient learning of gener-
alized linear and single index models with isotonic regression. CoRR, 2011.
Adam Kalai and Ravi Sastry. The isotron algorithm: High-dimensional isotonic regression. 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convo-
lutional neural networks. In Proceedings of the 25th International Conference on Neural Infor-
mation Processing Systems - Volume 1, NIPS’12, pp. 1097-1105, 2012.
Guanghui Lan. An optimal method for stochastic composite optimization. Math. Program., 133
(1-2):365-397, 2012.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation.
CoRR, 2017.
Song Mei, Yu Bai, and Andrea Montanari. The landscape of empirical risk for non-convex losses.
2017.
A. Mohamed, G. E. Dahl, and G. Hinton. Acoustic modeling using deep belief networks. Trans.
Audio, Speech and Lang. Proc., pp. 14-22, 2012.
A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to
stochastic programming. SIAM J. on Optimization, 19:1574-1609, 2009.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. A pac-bayesian
approach to spectrally-normalized margin bounds for neural networks. CoRR, 2017.
10
Under review as a conference paper at ICLR 2018
Quynh N. Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. CoRR,
2017.
Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola. Stochastic variance
reduction for nonconvex optimization. In Proceedings of the 33rd International Conference on
International Conference on Machine Learning - Volume 48, ICML’16, pp. 314-323.
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D. Lee. Theoretical insights into the optimization
landscape of over-parametrized shallow neural networks. CoRR, 2017.
Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees
for multilayer neural networks. CoRR, 2016.
Bo Xie, Yingyu Liang, and Le Song. Diversity leads to generalization in neural networks. CoRR,
2016.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. CoRR, 2016.
A Proofs of Auxiliary Results
In this appendix, we provide proofs for auxiliary results.
A.1 Proof of Lemma 4.1
The result is trivially true for d =1, we will show this using induction on d.
Define vi := vect(h(xi)xiT), i ∈ [N]. Note that it suffices to prove independence of vector vi, i ∈
[N] for N ≤ d2.
Now for sake of contradiction assume that vi , i ∈ [N], are linearly dependent with positive joint
measure on xi , i ∈ [N] which is equivalent to positive measure on individual xi , ∀ i ∈ [N] due to
independence of vectors xi .
Since xi ’s are sampled from Lebesgue measure so positive measure on xi , ∀i ∈ [N], implies there
exists a d-dimensional volume for each xi such that corresponding vi are linearly dependent. We
can assume volume to be d-dimensional hyper-cuboid Zi := {x ∈ Rd : ai < x < bi}, ∀ i ∈ [N]
(otherwise we can inscribe a hyper-cuboid in that volume). Notice that since Zi is a d-dimensional
hyper-cuboid so ai [k] < bi[k], ∀ i ∈ [N], ∀ k ∈ [d]. Moreover, for any collection satisfying xi ∈ Zi,
corresponding collection of vector vi are linearly dependent, i.e.,
v1 = μ2v2 + …+ μNVN,	such that ∀i ∈ [N], Xi ∈ Zi.	(A.1)
Noticing the definition of Z1, we can choose > 0 s.t. xe1 := x1 + e1 ∈ Z1. Since we ensure that
xe1 ∈ Z1 then by (A.1) we have
e1 := vect( h(e1)e1τ) = μ2 v2 + …+ μN vN.	(A.2)
So using (A.1) and (A.2) we get
e1 — v1 = λ2v2 + …+ λN VN.	(A.3)
Since h(xi)xiT [j, k] = h(xi [j])xi [k], we have h(x1)x1T [j, k] = h(xe1)xe1T [j, k], ∀j ∈
{2, . . . , d}, k ∈ {2, . . . , d}. So we have (d — 1)2 components of ve1 — V1 are zero. Let us de-
fine:
(x1[1] + )h(x1[1] + ) — x1 [1]h(x1 [1])
h(x1[2])
.
.
.
wι =	eh(X1[d])
x1[2](h(x1[1] + e) — h(x1[1]))
「0])
> (2d -I)	z1 = . f (d — 1)2
’	LoJ
X1[d](h(X1 [1] + e) — h(X1[1]))
11
Under review as a conference paper at ICLR 2018
and notice that ve1 - v1 = wz1 . Since > 0, w1 6= 0 with measure 1.
Let yi := xi [2 : d] then last (d - 1)2 equations in (A.3) gives us
λ2h(y2)y2T +------+ λN h(yN YyNT = z1 = 0	(A.4)
By definition we have yi ∈ Rd-1 are independently sampled from (d - 1)-dimensional Lebesgue
measure. So by inductive hypothesis, rank of collection of matrices h(yi)yiT , i ∈ {2, . . . , N} =
min{(d - 1)2, N 一 1}. Soif N 一 1 ≤ (d 一 1)2 then λ2=…=Xn =0 With measure 1, then by
(A.3) we have w1 = 0 with measure 1, which is contradiction to the fact that w1 6= 0 with measure
1. This gives us
N > (d - 1)2 + 1	(A.5)
Notice that (A.4) in its matrix form can be Written as linear system
λ2
卜ect(h(y2)y2T)	...	vect(h(yN )yN T)]	.	= 0	(A.6)
λN
By (A.6), we have that vector of λ's lies in the null space of the matrix. Finally by inductive
hypothesis and (A.5) We conclude that the dimension of that space is N 一 1 一 (d 一 1)2{> 0}. Let
u1, . . . , uN-1-(d-1)2 ∈ RN-1 be the basis of that null space i.e.
hvect(h(y2)y2T) . . . vect(h(yN)yNT)i uj = 0,	∀ j ∈ {1, N 一 1 一 (d 一 1)2}
Define ti ∈ R2d-1 as:
^xi[1]h(xi[1])^
.
.
.
ti .= Xi[1]h(xi[d])
t := xi[2]h(xi[1])
.
.
.
xi[d]h(xi[1])
then we can rewrite (A.3) as
w1	t2	...	tN
z1	vect(h(y2)y2T ) . . .	vect(h(yN)yNT )
λN-(d-1)2
(A.7)
which implies that
w1 = b2b2 + …+ bN-(d-i)2 VN TdT)2	(A.8)
where vbi = t2 . . . tN ui-1, i = 2, . . . , N 一 (d 一 1)2 and z1 part of the equation is already
satisfied due to selection of null space.
Since N ≤ d2 ⇒ N 一 1 一 (d 一 1)2 ≤ 2d 一 2 then 2d 一 1 equations specified in (A.8) are
consistent in ≤ (2d 一 2) variables. Hence we get linearly dependent equations ∀x11 ∈ (a11, b11) and
small enough. Since x2 , . . . , xN are kept constant, v2, . . . , vN are constant. So t2 , . . . , tN are
constants and we can choose the same basis of null space u1, . . . , uN-1-(d-1)2 . Hence we have
vb2, . . ., vb(N-(d-1)2)
are constant. Let us define the set S to be the index set of linearly independent
rows of matrix [vb2 . . . vbN -(d-1)2] and every other row is a linear combination of rows in S. Since
(A.8) is consistent so the same combination must be valid for the rows of w1.
Now if N ≤ d2 -1 then number of variables in (A.8) is ≤ 2d - 3 but number of equations is 2d - 1,
therefore at least two equations are linearly dependent on other equation. This implies last (2d 一 2)
equations then function must be dependent on each other:
dd
Xαjh(x(1)[j]) + h(x(1)[1] + ) - h(x(1)[1]) Xβjx(1)[j] = 0
j=2	j=2
12
Under review as a conference paper at ICLR 2018
for some fixed combination αj , βj . If we divide above equation by and take the limit as → 0
then we see that h satisfies following differential equation on interval (a11, b11):
h0(x) = c1
which is a contradiction to the condition C1!
Clearly this leaves only one case i.e. N = d2 and (2d - 1) equations must satisfy dependency of
the following form for all x(11) ∈ (a(11) , b(11)):
(x(1)[1] + )h(x(1)[1] + ) - x(1)[1]h(x(1)[1])
dd
= Xαjh(x(1)[j]) + h(x(1)[1] + ) - h(x(1)[1]) Xβjx(1)[j]
j=2	j=2
Again by similar arguments, the combination is fixed. Let H(x) = xh(x) then dividing above
equation by and taking the limit as → 0, we can see that h satisfies following differential
equation:
H0 (x) = c1 + c2 h0 (x) ⇒ (x - c2)h0(x) + h(x) = c1	(A.9)
which is again a contradiction to the condition C1
So we conclude that for N ≤ d2 there does not exist hyper-cuboids Zi such that vol(Zi) > 0 and
for all xi ∈ Zi, corresponding vi are linearly dependent. So we get rank of collection {vi}iN=1 =
min{N, d2} with measure 1.
A.2 Proof of corollary 4.2
Let us define x := Wu be another random variable. Since W is full rank and u has Lebesgue
measure ⇒ x has Lebesgue measure.
Now we claim that the collection h(Wui)uiT is full rank iff the collection h(xi)xiT is full rank.
This can observed as follows:
NN
X λih(xi)xiT = 0⇔	X λih(Wui)uiT WT =0
i=1	i=1
N
XT
λih(Wui)uiT = 0
i=1
Here the second statement follows from the fact W is a non-singular matrix.
Now by Lemma 4.1 we have that collection h(xi)xiT is linearly independent with measure 1. So
h(Wui)uiT is linearly independent with measure 1.
Since any rotation is U is a full rank matrix so we have the result.
A.3 Proof of Lemma 5.1
This is true for k = 0 trivially since we are randomly sampling matrix W0 . We now show this by
induction on k.
N
Recall that gradient of f(W, θ) with respect to W can be written as P {vi-θT h(W ui)} h0(W ui)
i=1
θ uiT . Notice that in effect, we are multiplying i-th row of the rank one matrix h0(Wui)uiT by i-th
element of vector θ. So this can be rewritten as a matrix product
N
X{vi - θT h(W ui)}Θh0(W ui)uiT,
i=1
where Θ := diag{θ[i], i = 1, . . . , d}. So iterative update of the algorithm can be given as
Wk+1 = Wk — YkΘk+ιVwf (Wk, θk+ι),	∀ k ≥ 0.
Notice that given ξ[k] , vector θk+1 and corresponding diagonal matrix Θk+1 are found by SGD
in the inner loop so θk+1 is a random vector. More specifically, since {ξik+1}iN=i1 is sequence of
13
Under review as a conference paper at ICLR 2018
independent d-dimensional isotropic Gaussian vectors. Hence the distribution of ξk+1 = {ξik+1}iN=i1
induces a Lebesgue measure on random variable {θk+1 ξ[k]}
Given ξ[k] then Wk is deterministic quantity.
For the sake of contradiction, take any vector v that is supposed to be in the null space of Wk+1
with positive probability.
Wk+1 = Wk — YkVwf (Wk, θk+ι)
N
=Wk - γk X Θk+1(vi -θkT+1h(Wkui))h0(Wkui)uiT.
i=1
N
⇒ Wk+1v = Wkv - γk X Θk+1(vi - θkT+1h(W ui))h0(Wkui)uiT v = 0.
i=1
N
⇒ Wkv = Θk+1 X(λivi - riT θk+1)h0(Wkui)	setting λi = γk(vT ui), ri = λih(Wkui)
i=1
N
λivih0(Wkui) - Xh0(Wkui)riT θk+1 .
i=1
Now the last equation is of the form
b = Θk+1 [w - Mθk+1],	(A.10)
N	NT
where b = Wkv, w = P λivih0(Wkui), M = P h0(Wkui)ri .
i=1	i=1
Suppose we can find such θ with positive probability. Then we can find hypercuboid Z := {x ∈
Rd|a < x < b} such that any θk+1 in given hypercuboid can solve equation (A.10). By induction
we have b 6= 0. We may assume b[1] 6= 0. Then to get contradiction on existence of Z, we observe
that first equation in (A.10) is:
d
b[1] = θk+1 [1] w[1] - X M[1, j]θk+1[j] - M[1, 1]θk+1 [1]2, ∀ θk+1 ∈ (a, b).	(A.11)
j=2
Hence ifwe fix θk+1 [i] ∈ (a[i], b[i]), i = 2, . . . , d then (A.11) holds for all θk+1 [1] ∈ (a[1], b[1]). So
d
we conclude that b[1] = w[1] + P M[1, j]θk+1[j] = M[1, 1] = 0. But b[1] can not be 0. Hence we
j=2
arrive at a contradiction to the assumption that there existed a hypercuboid Z containing solutions
of (A.10).
Since measure on θk+1 was induced by {ξik+1}iN=i1 so we conclude that Pr{∃ v such that Wk+1v =
0ξ[k]} = 0,∀k ≥ 0.
A.4 Proof of Lemma 5.2
We use induction on d. For d = 1 this is trivially true. Now assume this is true for d - 1. We will
show this for d.
Let zi := Wui = W0ui + DvZui. For simplicity of notation define ti := Zui. Due to simple linear
algebraic fact provided by full rank property of W we have rank of collection (h(Wui)uiT = rank
of collection h(zi)ziT . For the sake of contradiction, say the collection is rank deficient with positive
probability then there exists d-dimensional volume V such that for all v ∈ V, we have h(Wui)uiT
is not full rank where W := W(v) = W0 + DvZ. Without loss of generality, we may assume
d-dimensional volume to be a hypercuboid V := {x ∈ Rd|a < x < b} (if not then we can inscribe
a hypercuboid in that volume). Let us take v ∈ V and ε small enough such that vb := v + εe1 ∈ V.
Correspondingly we have zi and zbi. Note that zbi = zi + εti[1]. So in essence, a small ε change in
v[1] causes εti[1] change in vector zi[1].
14
Under review as a conference paper at ICLR 2018
Let vi = vect(h(zi)ziT). Similarly, vbi = vect(h(zbi)zbiT). So we can divide vi
ci ∈ R2d-1 and gi ∈ R(d-1)2 . Here
-h(zi[1])zi[1]-
h(zi[2])zi[l]
ci := h(zl[d])zi[1]
h(zi[l])zi[2]
h(zi [1])zi [d]
Similarly we also have vbi
cbi
gbi
T
gi := vect(h(yi)yi ),	yi := zi[2 : d]
ci
gi
such
Now by the act that v, vb corresponding to z, zb are in V, and our
assumption of linear dependence for all v ∈ V we get
v1 = μ2v2 +----+ μN VN
b1 = b2b2 +----+ bN vN
(A.12)
(A.13)
NoW notice that yi = ybi, ∀ i ∈ [N]. So gi = gbi, ∀ i ∈ [N]. Also by induction on d - 1, We have that
the rank of collection g2, . . . , gN ≥ (d - 1)2. So We can reWrite matrix [g2 . . . gN] := [G Ge] such
that G ∈ R(d-1)2×(d-1)2 is an invertible matrix and reWrite one part of equation (A.12) as g1 =
Γ~π
[G G] μ


.Hence We can replace e = G-1(g1 - Gμ) = G-1g1 - G-1Gμ. Essentially the vector
[μ] is completely defined by parameter μ ∈ RN-1-(d-1)2. Similarly we have e = GTgI - Gb,
so vector ]b is completely defined by b ∈ RN-1-(d-1)2. So essentially we have satisfied one part
of equations (A.12) and (A.13). Notice that since we are moving only one coordinate of random
vector v i.e. v[1] ∈ (a[1], b[1]) (by ε incremental changes) keeping all other elements of v constant
so we will have yi as constants which implies gi, G, G are constant. So for the sake of simplicity of
notation we define l := G-1g1 ∈ R(d-1)2 and R := G-1Ge ∈ R(d-1)2 ×(N -1-(d-1)2)
Now, we look at the remaining part of two equation (A.12),(A.13):
c1 = μ2c2 + . 一 + μN CN,
b1 = b2b2 +-----+ bN bN,
which can be rewritten as
c1 = [C C] l - Rμ = Cl - CRμ + Cμ,
L L μ」
b1 = [C C] l -μRb = Cl - CRb + Cμ.
After (A.15) - (A.14), we have
^ ^
(C - C)l - (C - C)Rμ - CR(b - μ) + (C - C)μ + C(b - μ) = C1 - c1.
(A.14)
(A.15)
(A.16)
Now note that (A.16), characterizes incremental changes in C, C, μ due to ε. So taking the limit as
ε → 0, we have
0
C1 = C0l - C0Rμ - CRμ0 + C0μ + Cμ0.
[c10 C 0] -l = (-CR + C)μ0 + (-C 0R + C0)μ.
⇒ [c1 C] -l = (-CR + C)μ.
(A.17)
15
Under review as a conference paper at ICLR 2018
Here, last equation is due to product rule in calculus. In (A.17), we see that we have 2d- 1 equations
and N - 1 - (d - 1)2 unknowns at every point. If N ≤ d2 then N - 1 - (d - 1)2 ≤ 2d - 2. So
at least one equation should depend on others. But as we have shown earlier, h satisfying condition
C1 does not have row dependence. So we arrive at the required contradiction for N ≤ d2. That
completes the proof.
A.5 Proof of Lemma 5.6
Assume that all the gradients in this proof are w.r.t. W then we know that
1N
-Vf (W,θ)[j, k] = N- £{vi — θTh(Wui)}h0(W[j, :]ui)θ[j]ui[k]
N i=1
Notice that kWkF = kvect(W)k2. Also notice that if W = abT then kWkF = kak2.kbk2
Let us define vector ai s.t. ai [j] = θ[j]h0(W [j, :]ui)(vi - θTh(Wui)) so we have
1N
-(Vf(WI)- Vf(W2))jk = NN Euk(a1[j] - a2[j])
i=1
1N	T
⇒ -(Vf(WI)- Vf (W2)) = NN £(a1 - a)ui
N i=1
1N
⇒ kVf(W1) - Vf (W2)kF ≤ INf Ilull2∙∣∣a1 - a2∣∣2,
i=1
where the last inequality follows from Cauchy-Schwarz inequality.
So if we can show Lipschitz constant Li on lai1 - ai2 l2, ∀ i then we are done.
Let θmaχ := max ∣θj |, then
j
(A.18)
∣(a1 — a2)[j]∣ = ∣θj|. h0(W1[j, :]ui)(vi - θTh(Wιui)) - h0(W2[j, :]ui)(vi - θτh(W2ui))
≤ θmax h0(W1[j, :]ui)(vi - θTh(W1ui)) - h0(W2[j, :]ui)(vi - θτh(W2ui))
⇒	llai1	-ai2	ll2 ≤	θmax	lllvi	h0(W1ui)	- h0(W2ui)	-	h(W1ui)h0(W1ui)T -	h(W2ui)h0(W2ui)T	θ
≤ θmax lllvih0(W1ui) -h0(W2ui)lll
+ lll(h0(W1ui)h(W1ui)T - h0(W2ui)h(W2ui)T)θlll	.
Suppose the Lipschitz constants for the first and second term are Li,L and Li,R respectively. Then
N
Li = θmaχ(Li,L + LiR and possible upper bound on value of L would become N P Iluik2Li.
i=1
We now analyse existence of Li,L
Since the Hessian of scalar function h(∙) is bounded so we have h0(x) is LiPschitz continuous with
constant Lh0. Let r1, r2 be two row vectors then we claim kh0(r1x) - h0(r2x)k2 ≤ Lh0 lxl2.lr1 -
r2 l2, ∀ r1, r2 because:
kh0(r1x) - h0(r2x)k2 ≤ Lh0 ∣r1 x -r2x∣ ≤ Lh0kxk2kr1 - r2k2
From the relation above we have the following:
2
d2
llh0(W1ui)-h0(W2ui)ll22 =X h0(W1[j,:]ui)-h0(W2[j,:]ui)
j=1
d
≤ L2h0	lluill22X	llW1[j,:] - W2[j,:]ll22	=	L2h0	lluill22llW1-W2	ll2F
j=1
⇒ Li,L = Lh0kuik2|vi|.
(A.19)
16
Under review as a conference paper at ICLR 2018
Now we focus our attention to second term. Notice the simple fact that
kW1 -W2k2 ≤ kW1 -W2kF = kvect(W1 -W2)k2.	(A.20)
Define v := W1ui , u := W2ui, then we have
kv-uk2 =	(W1	-W2)ui	≤	W1	-W2	.ui	≤	ui	.vect(W1	-W2)	,	(A.21)
and
h0(W1ui)h(W1ui)T - h0(W2ui)h(W2ui)T θ
≤ θ .h0(W1ui)h(W1ui)T - h0(W2ui)h(W2ui)T
= θ .h0(v)h(v)T - h0(u)h(u)T
≤ θ .h0(v)h(v)T - h0(u)h(u)T	.
The latter inequality implies that
(h0(W1ui)h(W1ui)T - h0(W2ui)h(W2ui)T)θ2
d2
≤ θ2	h0(v[i])h(v[j]) - h0(u[i])h(u[j])	.
i,j=1
Now let us define a 2-D function H(χ1,χ2) = h(χ1)h0(χ2). Then VH(χ1,χ2)
so under given assumptions, ∣∣VH(∙) k 2 is bounded. Let that bound be Lh%，.
Now by mean value theorem, we have
h0(x1)h0(x2)
h(x1)h00(x2)
H(x1, x2) - H(y1,y2) = VH(ξ)T {(x1, x2) - (y1,y2)}
⇒ H(x1,x2) - H(y1,y2)	≤ VH(ξ) .n(x1 - y1)2 + (x2 - y2)2o
≤ L2hh0 n(x1 - y1)2 + (x2 - y2)2o
So nh0(W1ui)h(W1ui)T - h0(W2ui)h(W2ui)T oθ2
d2
≤ θ 2	h0(v[i])h(v[j]) - h0(u[i])h(u[j])
i,j=1
d
≤ ∣∣θ∣∣2 X LhhO ((V[i] - u[i])2 +(V[j] -UjD2)
i,j=1
=2dL2hh0∣∣θ∣∣22kV-Uk22	(A.22)
It then follows from (A.20),(A.21) and (A.22) that
∣∣(h0(W1ui)h(W1ui)T - h0(W2ui)h(W2ui)T)θ∣∣
≤√2dLhh0∣∣θ∣∣2.∣∣ui∣∣2.∣∣wι- W2∣∣F
So you get that Li,R = √2dLhh0 kθk2kuik2
Finally, using (A.18), (A.19) and (A.22), we get a possible finite upper bound on the value of L:
N	N
L ≤ N θmax(Lh0 (X kuik2 |vil) + √2dLhh0 kθk2( X kuik2
N	i=1	i=1
Also note that this bound is valid even if W is not a square matrix.
17
Under review as a conference paper at ICLR 2018
A.6 Proof of Lemma 5.9
Noting that
1N
-Vθf (W,θ) = NN £{vi - θTh(Wui)}h(Wui),
N i=1
we have
Vθf(W,θ1)-Vwf(W,θ2)2
= ɪ X [{vi - θTh(Wui)}h(Wui) - {vi - θTh(Wui)}h(Wui)i U
1 N
—X [{-h(Wui)h(Wui)Tθ1 + h(Wui)h(Wui)Tθ2} J]
i=1	2
1
NN ]Th(Wui)h(Wui)t(θ2 - θι)
i=1	2
≤
1N
_£h(Wui)h(Wui )t
N i=1
.Uθ1 - θ2U2
2
N
=NFUX h(wui)h(wui)TU2.Uθι - θ2U2
i=1
1N
=NN λmax( X h(Wui)h(Wui)T) .∣∣θ1 - θ2∣∣2
i=1
1N
≤ NN { X λmaχ (h(Wui)h(Wui)T) }∣∣θ1 - θ2∣∣2
=N [X∣∣h(Wui)∣∣2h∣θι-θ2∣∣2
i=1
≤ u2d∣∣θ1 - θ2 ∣∣2
∙.∙ Weyrs Inequality
where u1 and u2 are upper bounds on scalar functions ∣h(∙)∣ and ∣h0(∙)∣ respectively and d is row-
dimension of W.
A.7 Proof of Theorem 5.11
We know by Lemma 5.6 that f (∙, θ) is a LiPschitz smooth function. So using (5.7) we have
f (Wk+ 1,θk+1) ≤ f (Wk, θk +1) + 2 ∣∣vect(Wk+1 - Wk )(
+ vect(VW f (Wk, θk+1)), vect(W1k+1 -W1k)
=f (Wk, θk +I)-(Yk- 2γ2) ∣∣vect(vwf (Wk ,θk +I))II
Ni	Γ1	Ni	_
≤ f (Wk,θk) + (E βτ)-1 D∣∣θk-θ*j∣2+Eβτ 心,θwk - θk)
τ=1	τ=1
-(γk - 2γ2) ∣∣vect(vWG(Wk, θk + 1)) ∣∣ ,	(A.23)
+ X βτ ∣∣ξτ ∣∣
+ T=1 2(1 - Lθβτ)
where the last inequality follows from equation (A.28) and (A.29).
From (3.6), we have kθk ≤ R/2 so L is constant for each outer iteration. Summing (A.23) from
18
Under review as a conference paper at ICLR 2018
No
k = 0 to No and dividing both side by E(Yk - LL γk), We get
k=0
k=min,JvWf (Wk ,θk+ι)L
No (γk -
≤
k=0
LL Y2“ VeCt(VWIf(Wk ,θk+1))
2
k=0
≤
No Ni	1
f (Wo,θo)+ p (P βτ)
k=0 τ=1
Rr+P 卜 k Dξτ ,θwk- θτ E+
βk2∣∣ξk∣∣2
2(1-Lθβτ)
No
P (Yk - L/2Yk2 )
k=0
NoW taking expeCtation With respeCt to ξ[No] (WhiCh is defined in (5.1)), We have
E 心,θWk-θk E ∣ξ[k-1] ∪ ξ[kτ-1]	=0,
WhiCh implies Eξ[No
0. We also have Eξ[No]
hξτk2i ≤ σ2, and henCe
U	f(W0,θ0) + P(PeT )-1]R22 + Nii 2⅛¾)
Ehk=minUvWf(Wk,θk/u ≤ ------------kɪ~----------------------
(Yk - L/2Yk2)
k=0
A.8 Proof of Theorem 5.10
For sake of simplicity of notation, we define f (∙) := f (Wk, ∙), g(∙) := Vf (∙) = Vθf (Wk, ∙) and
GWk (θτ,ξT) ：= GT. Then from (3.4) and Algorithm 1 we get
GT = g(θτ)+ ξT.	(A.24)
Also define d「:= θτ +ι - θτ.
Notice that θτ+ι is optimal solution to the problem
min βτ<Gτ,u - θτ〉+ 2 ∣∣u - θτ ∣∣2,	(A.25)
by simply writing first order necessary condition for problem (A.25). Also we note that objective
function in (A.25) is strongly convex with parameter 1. Then we have
βτ hGT, dτ i + 2∣l dT ∣∣2 + 2∣lu - θτ +11∣2 ≤ βτ〈GT, U - θτ〉+ 2∣lu - θτ∣∣2∙	(A.26)
We will use eq (A.26) along with smoothness and convexity of the function f to get the final conver-
gence result. Notice that due to smoothness, we have
βTf (θT +1) ≤ βT [f (θT ) + <g(θT ), dT)+ -2- IldTIl2 ]
=β"f (Θt ) + (g(θT ),dT〉] + 2 IdT k2 - (I- Lβ ) IdT I2.
then due to (A.24) we have,
Bt f (Θt+1) ≤ Bt [f (Θt ) +(Gt ,dT i] - Bt^T ,dT〉+ 11 &t I 2 - (I-	^ ) I &t I 2
≤ βτ [f (θτ ) + (Gt ,dτ i] +1 Idτ I2 - (I- Lθ BT) IdτI2 + BτIξTI∙IdτI
≤ BT f (θτ )+hβτ hGT ,dτ i + 1 IdT I2i +2BT-L3 .
19
Under review as a conference paper at ICLR 2018
By (A.26) we have
βτ f (θτ+1) ≤βτf (θτ)+…,u- θτ〉+2 ku - θτk2 - 2 ku- θτ+1k2+2(β-ξτθlβτ)
=[βτ f (θτ ) + βτ (g(θτ ), u - θτ〉i + βτ * ,u - θτ〉
+ 2ku - θτk2 - 2ku - θτ+1k2 +
kβTξTk2
2(I - Lθ βτ )
≤ βτf (U) + βτ <ξT, U - θτ〉+ 2 ku - θτ k2 - 2 ku - θτ +1k2 + 2(1T- "β ) .
τ (A.27)
Last equation is due to convexity of function f. So using (A.27) we have
X βτ [f (θτ+1 ) - f (θWfc )] ≤ 1 kθ1 - θWfc k2 + X [βτ K ,θWfc - θτ〉+ 2(β-K ) i. (A.28)
Note that from convexity of f, we get
ii
f (θa+1)-f (θw*) ≤ (X βτ 厂1 X [βτ [f (θτ+1)-f (θw*)]].	(a.29)
τ=1	τ=1
Moreover, noting the definition of ξ[kτ] in (5.2) so we have,
E[<ξT ,θW*-θτ〉昨-1]] =0,	(A.30)
and from (3.5) we get E [kξT k2] ≤ σ2. Hence using this relation and noting 1 - Lθβτ ≥ 11, (A.28),
(A.29) and (A.30) we prove the result.
20