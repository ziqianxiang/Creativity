Under review as a conference paper at ICLR 2018
Sparse Regularized Deep Neural Networks
For Efficient Embedded Learning
Anonymous authors
Paper under double-blind review
Abstract
Deep learning is becoming more widespread in its application due to its
power in solving complex classification problems. However, deep learning
models often require large memory and energy consumption, which may
prevent them from being deployed effectively on embedded platforms, limit-
ing their applications. This work addresses the problem by proposing meth-
ods Weight Reduction Quantisation for compressing the memory footprint
of the models, including reducing the number of weights and the number of
bits to store each weight. Beside, applying with sparsity-inducing regular-
ization, our work focuses on speeding up stochastic variance reduced gradi-
ents (SVRG) optimization on non-convex problem. Our method that mini-
batch SVRG with £ 1 regularization on non-convex problem has faster and
smoother convergence rates than SGD by using adaptive learning rates. Ex-
perimental evaluation of our approach uses MNIST and CIFAR-10 datasets
on LeNet-300-100 and LeNet-5 models, showing our approach can reduce
the memory requirements both in the convolutional and fully connected
layers by up to 60× without affecting their test accuracy.
1 Introduction
Artificial intelligence is finding wider application across a number of domains where com-
putational resources can vary from large data centres to mobile devices. However, state-of-
the-art techniques such as deep learning (LeCun et al., 2015) require significant resources,
including large memory requirements and energy consumption. Reducing the size of the
deep learning model to a compact model that has small memory footprint without compro-
mising its performance is a desirable research aim to address the challenges for deploying
these leading approaches on mobile devices. £ 1 regularization can be used as a penalty to
train models to prevent the model from over-fitting the training data. As well as providing,
£ 1 regularization is a powerful compression techniques to penalize some weights to be zero.
As the results, our research focus on improving the method based on £ 1 regularization to re-
duce memory requirements. Moreover, as deep neural network optimization is a non-convex
problem, the optimization can be stuck in local-minimal, which can reduce the performance.
To address the problem, we improve SGD optimization for non-convex function to enhancing
sparse representations obtained with £ 1 regularization. In this paper, We propose our com-
pression method Weight Reduction Quantisation which reduces both the number of weights
and bits-depth of model without sacrificing accuracy. To reduces the number of weights, our
method employs sparsity-inducing £ 1 regularization to encourage many connections in both
convolutional and fully connected layers to be zero during the training process. Formally,
in this paper we consider the following unconstrained minimization problem, Given training
labels y1, y2, ..., yN as correct outputs for input data x1, x2, ..., xN, the optimization problem
to estimate the weights in all layers, W, is defined by
1N
mwnN 工 L(yi，f(xi； W)) + λr(W),
(1)
i=1
where λ is a hyper-parameter controlling the degree of regularization and the weights in
all layers is given by W. The problem 1 can be strongly convex or possibly non-convex
1
Under review as a conference paper at ICLR 2018
(Allen-Zhu & Yuan, 2016). Following update rule, the mini-batch SGD method with £ 1
regularization is a popular approach for performing the optimization, and the weight update
rule is given by
∂	1B	λM
wk+1 = wk—ηk两(B 工 La,/⑴；W)) + M ^ |吗 | J,	⑵
where each weight of network can be represented by wj ,the total number of weights is M. k
is the iteration counter and ηk is the learning rate and B is mini-batch size (1 < B < N) used
to approximate the full gradient. However, SGD optimization with £ 1 regularization has two
challenges: firstly, it inefficiently encourages weight to be zero due to fluctuations generated
by SGD (Tsuruoka et al., 2009). Secondly, SGD optimization slowing down convergence
rate due to the high variance of gradients. The two methods of cumulative £1 regularization
and SVRG can solve the two challenges respectively:
Cumulative £ 1 regularization Tsuruoka et al. (2009) proposed a method cumulating
the £ 1 penalties to resolve the problem. The method clips regularization at zero, which
avoids the derivative
∂W £Mi(M ∣Wj∣) being non-differentiable when Wj = 0 and provides a
more stable convergence for the weights. Moreover, the cumulative penalty can reduce the
weight to zero more quickly.
Mini-batch SVRG As SGD optimization has slow convergence asymptotically due to
noise, Johnson & Zhang (2013) proposed SVRG that can efficiently decrease the noise of
SGD by reducing the variance of gradients by:
k+1 k
Wj	=Wj - ηk
(1 ^(Vψi(wk) - Vψi(Wj)) + Iij),
(3)
where R is the average gradient of sub-optimal weights Wj which is the weight after every
m SGD iterations
N
S ∂L(yi,f(xi; W))
μj ^N 乙	∂Wj
i=1
1N
=N £VWi(Wj),
i=1
(4)
where W is the sub-optimal weights after m SGD iterations in all layers. For succinctness
we also write Vψi(Wk) = dL(yi∂WxX"W. They determined that reduction of variance helps
initial weights W0 close to global minima at the beginning in order to boost the convergence
rate of SGD in strongly convex problems. Johnson & Zhang (2013) further prove that the
performance of SGD degrades with mini-batching by the theoretical result of complexity.
Specifically, for batch size of B, SGD has a 1Λ∕B dependence on the batch size. In contrast,
SVRG in a parallel setting has 1/B dependence on the batch size which is much better than
SGD. Hence, SVRG allows more efficient mini-batching. However, for non-strongly convex
problems, global minimization of non-convex function is NP-hard(Allen Zhu & Hazan, 2016).
Johnson & Zhang (2013) have a assumption that SVRG can also be applied in neural
networks to accelerate the local convergence rate of SGD. Further, Allen Zhu & Hazan
(2016) prove non-asymptotic rates of convergence of SVRG for non-convex optimization and
proposed improved SVRG that is provably faster than SGD. Hence, a promising approach
is to use mini-batch SVRG instead of SGD with cumulative £ 1 regularization.
Main Contributions We summarize our main contributions below:
1.	Reducing memory requirements:
1.1	We analyse a method that combines SVRG with cumulative £ 1 regular-
ization to reduce the number of weights, and propose our method Delicate-SVRG-
Cumulative-£ 1 which can significantly reduce the number of weights by up to 25×
2
Under review as a conference paper at ICLR 2018
without affecting their test accuracy. To our knowledge, ours is the first work that
to combine mini-batch SVRG with cumulative £ 1 regularization for non-convex op-
timization.
1.2	To further reduce the memory requirements of models, we aim to reduces
the number of bits to store each weight. Compression method Weight Reduction
Quantisation, including both reducing number of weights and bit-depth, can reduce
the memory footprints up to 60× without affecting accuracy.
2.	Accelerating convergence rates:
2.1	We analyse non-convex stochastic variance reduced gradient (SVRG). Based
on the results from (Reddi et al., 2016), we provide the condition when SVRG has
faster rates of convergence than SGD.
2.2	We empirically show that modified SVRG in our method have faster rates
of convergence than ordinary SVRG and SGD.
2 Related Works
Different methods have been proposed to remove redundancy in deep learning models.
Sparse representation is a good approach to reduce the number of parameters. Han et al.
mainly explored pruning which is a direct approach to remove small values of connection and
focuses on the important connections with large weight values in all layers of the network.
However, a disadvantage is that after pruning the needs networks to be retrained. One idea
from matrix factorization can be applied to compressed parameters in models by finding
a low rank approximation of the weight matrix Denton et al. (2014). However, in prac-
tice whilst it improves computation performance, it dose not significantly reduce memory
requirements.
Weight sharing aims to approximate weights by a single weight. Chen et al. proposed
HashedNets binning network connections into hash buckets uniformly at random by a hash
function. As part of a three stage compression pipeline, Han et al. use k-means clustering
to identify the shared weights for each layer of a trained network.
Weight quantization for reducing the bit-width to store each weight is an other approach to
reduce memory requirements of models. Gysel et al. can successfully condense CaffeNet and
SqueezeNet to 8 bits with only slight accuracy loss. Han et al. quantizes the sparse weights
matrix to be an index which encodes in 8-bit for convolutional layers and 5-bit for fully
connected layers. Rastegari et al. used binary operations to find the best approximations
of the convolutions, in which the bit-size can be reduced to 1-bit.
Another type of approach uses regularization to induce sparsity. Hinton et al. proposed
"dropout" that refers to dropping out neurons that are from visible and hidden layers in
neural network during training, which can be shown to be a kind of regularization. Collins
& Kohli applied £ 1 regularization and shrinkage operators in the training process. However,
it only reduced the weights by only 4× with inferior accuracy. Tsuruoka et al. improved on
this with £ 1 regularization with superior compression, but the methods use SGD and has
slow asymptotical convergence due to the inherent variance Johnson & Zhang (2013).
3 Mini-batch Non-convex SVRG
For Problem 1, a stochastic iterative learning algorithm estimate a stationary point x and
achieve ε-accuracy in finite iterations satisfying || R f (ɪ)||2 ≤ ε, which is termed of the ε-
accurate solution. For a non-convex problem, the goal is to find a reasonable local minimum.
However, the challenge is that gradients are easy to be stuck into saddle-point or a local
minimum. As a result, such an algorithm aims to help gradients escape from saddle-point or
local-minimal, e.g.(Ge et al., 2015) demonstrated that adding additional noise can help the
algorithm escape from saddle points. To our best knowledge, there is no theoretically proof
that can guarantee SVRG has faster rates of convergence than SGD. (Reddi et al., 2016)
compared the Incremental First-order Oracle (IFO) complexity Agarwal & Bottou (2015)
of SGD and SVRG on non-convex problem, O (l∕ε2) and O (n + (n2∕ε)) respectively. For
3
Under review as a conference paper at ICLR 2018
our analysis, whether non-convex SVRG can be efficiently close to reasonable optimal local
minimum depends on the number of training samples. Suppose fi is non-convex for i ∈ [n]
and f has ε-bounded gradients, the IFO complexity of mini-batch SGD with a adaptive
learning rate is O (l∕ε2) and for mini-batch SVRG with a fixed learning rate O (n + (n2∕ε)).
If the value of ε is constant, the speed of convergence rates of SVRG depends on the number
of training samples: when n is small, SVRG is faster than SGD for non-convex optimization
and vice versa. Our experiment results showed in Figure1 and Figure5can support our view.
3.1	Mini-batch Non-convex SVRG on Sparse Representation
In our case, SVRG is applied on sparse representation. However, if directly combining mini-
batch non-convex SVRG with cumulative £ 1 regularization (called SVRG-CumUlative-01):
let Uk be the average value of the total £ 1 penalty given by
λ S
Uk=M Σηt.	⑸
t=1
At each training sample, weights that are used in current sample can be updated as
wk+2 = wk - ηk (1 "vψi(Wk)- vψi(wj)) + j
(6)
C k+1	,
if Wj 2〉0 then
wk+1 = max(0, wk+2 -(Uk + qk-1)),
k+1	⑺
else if Wj 2 < 0 then
wk+1 = min(0, wk+2 + (uk - q：-1)),
where, qk is the total difference of two weights between the SGD update and the £ 1 regu-
larization update,
qk =工(wj+1-wj+2),	⑻
t=1
where t is an index to calculate cumulative value of q, the algorithm has two problems: (1)
As we mentioned, SVRG on sparse representation cannot guarantee to be faster than SGD.
Figure 1 shows that for small dataset (e.g. MNIST) the convergence of SVRG is faster
than SGD but slower than SGD using a larger dataset (e.g. CIFAR-10), (2) The trade-off
(a) MNIST dataset on LeNet-300-100 model
(b) CIFAR-10 dataset on LeNet-5 model
Figure 1: With cumulative £ 1 regularization, we compare the convergence rates of SGD and
SVRG. SVRG-Cumulative-£ 1 has faster convergence rate in Figure 1(a). However, in Figure
1(b), SGD-Cumulative-£ 1 can significantly converge into lower loss than SVRG-Cumulative-£ 1
when compression rate equal 50% and 90%.
of SVRG-Cumulative-£ 1 in the variance reduction versus the sparsity of the cumulative £ 1
regularization. After the variance of the gradient is reduced by SVRG in Equation 6, the ab-
k+1
solute value of the updated weight Wj 2 is higher than that using SGD, which causes SVRG
4
Under review as a conference paper at ICLR 2018
to have an adverse effect on the sparsity of £ 1-regularization. Compared to ordinary SVRG,
(Reddi et al., 2016) proposed an extension of SVRG: MSVRG that introduces adapts the
learning rate, which guarantee that their method has equal or better than SGD. Therefore,
similar to the method MSVRG, our method provides separate adaptive learning rates for
SVRG-CUmUlative-£ 1, which empirically demonstrates that it has faster convergence rate
than SGD.
3.2	Delicate-SVRG-cumulative-£ 1
To reduce the number of weights, we introduce our compression method Delicate-SVRG-
CUmUlative-£1 that have two main improvements :
(1)	Separate Adaptive Learning Rate Learning rates play an important rule in effect-
ing the convergence rate of optimization during the training process which must be chosen
carefully to ensure that the convergence rate is fast, but not too aggressive in which case the
algorithms may become unstable. Reddi et al. believe that adaptive learning rates can be
applied with reduced variance to provide faster convergence rates on nonconvex optimiza-
tion. As a result, the convergence rate of the algorithms can be improved if the learning rate
is adaptively updated. Our algorithm includes three parameters to provide greater fidelity
in controlling the convergence of gradients for implementation of the £ 1 regularization.
Firstly, the learning rate γk is chosen based on the learning rate from Collins et al. shown
as,
= no
Yk= 1 + π(k∕N)
(9)
where η0 is an initial learning rate with large value. Our experiments determined the param-
eters in three learning rates are over range of values, and a value of π=0.6 as determined to
be efficient. The learning rate schedule can emphasis the large distance between the gradient
in the current optimization iteration and the sub-optimal solutions after every m iteration
in the beginning, which avoids the current gradient being stuck in a local minimum at the
start. It has a fast convergence rate to start with which decreases over time to minima local
station.
The second learning rate, βk, that reduces the variance of the SVRG-CUmUlative-£1 and
better balances the trade-off in both of SVRG and cumulative £ 1 regularization. βk is
chosen such that βk > γk with slower convergence as
βk = 1 + α(k∕N)q,	(10)
here βk =0.75, and the results of experiment is the best when q = 3 that can keep relatively
large penalty of average gradients. During updating weight, it is efficient to prevent the
absolute value of weight from being increased by SVRG, which can reduce the bad effect of
£ 1 regularization, and sparsity.
We retain the same learning rate nk for cumulative £ 1 regularization Tsuruoka et al. (2009)
shown as,
nk = noakN	(11)
The exponential decay ensures that the learning rates dose not drop too fast at the beginning
and too slowly at the end.
(2)	Bias-based Pruning To further reduce the number of weights, we add a bias-based
pruning b after the £ 1 regularization in each iteration. The pruning rule is based on following
heuristic Fonseca & Fleming (1995): connections (weights) in each layer will be removed
if their value is smaller than the network’s minimal bias. If the absolute value of weight
connections are smaller than the absolute value of the smallest bias of the entire network in
each batch, these connections have least contribution to the node, which can be removed.
In practice, bias-based pruning has no effect on train and test loss.
5
Under review as a conference paper at ICLR 2018
Consequently, Delicate-SVRG-CUmUlative-01 that incorporates the adaptive learning rate
schedules and bias-based pruning as,
k+1
Wj 2
=wjk
N
-(N S
(▽Wi(Wk)- Vψi(Wj)) +
- Ir-I-1	_
if Wj 2〉0 then
w；+1 = max(0, wk+2 -(Uk + qjk-1 + b)),
- 一 k+1	. .
else if Wj 2 V 0 then
w；+1 = min(0, Wk+2 + (Uk - qjk-1 - b)).
(12)
The pseudo code of our method is illustrated as Algorithm 1 in the Appendix.
4	Weight Quantization for Bit-depth Reduction
To further compress the model, weight quantization can significantly reduce memory re-
quirement by reducing bit precision. We quantize to 3-bit after reducing by Delicate-
SVRG-cumulative-01 for convolutional layers and encode 5 bits for fully connected layers.
Consequently, we propose our final compression method Weight RedUction QUantisation.
Table 1: Comparison of the compression results of the pruning method from Han et al. (2016)
and our method in each layer. Using MNIST dataset train and test on LeNet-300-100 1(a)
and LeNet-5 model1(b). D is Delicate-SVRG-cumulative-01 and Q is weight quantization.
(a) MNIST dataset with LeNet-300-100 model.
Layer	Original network	#Weights (D)	Memory (D+Q)	Compress rate (D)	Compress rate (D+Q)	Deep compression Han et al. (2016) Compress rate
ip1	235K(940KB)	-80K-	14.36KB	3%	163%	2.32%
ip2	30K(120KB)	2.5K	3.392KB	8.3%	2.82%	3.04%
ip3	1K(4KB)	0.3K	0.308KB	30%	7.7%	12.70%
Total	266K(1070KB)	10.8K	18.06KB	4%(25×)	1.68%(60×)	2.49%(40×)
Top-1 Error	1.64%	-	-	1.58%	1.57%	1.58%
(b) MNIST dataset with LeNet-5 model.
Layer	Original network	D-SVRG-C-LI (D)	Memory (D+Q)	Compress rate (D)	Compress rate (D+Q)	Deep compression Han et al. (2016) Compress rate
conv1	-0.5K(2KB)-	0.33K	1.16KB	78%	58%	67.85%
conv2	25K(100KB)	3K	2.42KB	12%	2.42%	5.28%
ip1	400k(i600KB)	32K	24KB	3.7%	1.5%	2.45%
ip2	5K(40KB)	0.95K	2.112KB	17%	5.28%	6.13%
Total	431k(i720KB)	35K	30KB	4.5%(22×)	1.8%(57×)	2.55%(39×)
Top-1 Error	0.80%	-	-	0.74%	0.737%	0.74%
5	Experiments
In order to estimate and compare the effect of our compression method on different topolo-
gies, e.g. fully connected networks and convolutional networks, we select deep neural net-
works (DNNs) and convolutional neural networks (CNNs). The DNN chosen is LeNet-300-
100 which has two fully connected layers as hidden layers with 300 and 100 neurons respec-
tively. The CNN chosen is LeNet-5 which has two convolutional layers and two fully con-
nected layers. We evaluate the performance of our new compression method using MNIST,
6
Under review as a conference paper at ICLR 2018
and CIFAR as benchmarks. MNIST (LeCun et al., 2001) is a set of handwritten digits which
is a commonly used dataset in machine learning. It has 60,000 training examples and 10,000
test samples. Each image is grey-scale with 28×28 pixels. CIFAR-10 is a dataset that has
10 classes with 5,000 training images and 1,000 test images in each class. In total, it con-
tains 50,000 training images and 10,000 test images with 32×32 pixels. CIFAR-10 images
are RGB. Two types of error rate are used to measure the performance of models, which
are top-1 and top-5 error rate. Here, we consider top-1 error on MNIST, while top-5 error
on CIFAR-10 because many images in CIFAR are small and ambiguous. Our compression
method was implemented using Caffe1 .
5.1	Comparison with leading results
Applying Weight Reduction Quantisation to the MNIST dataset, we choose the results
with the best combination of compression and error rate for comparison. Our method can
reduce 98% of the memory requirements with a 1.57% test error rate on the LeNet-300-
100 model and 98% of the parameters with a 0.74% test error on the LeNet-5 model. In
Table 1, the compression pipeline is summarised with weight statistics in comparison to
the method from Han et al. (2016). In our first stage Delicate-SVRG-CUmUlative-01 that
focus on reducing the number of weights, we compare the results of pruning method from
Han et al. (2016) that is the first stage of their compression method. The two tables show
that both Delicate-SVRG-cumulative-01 and Han et al. pruning method can significantly
remove many weights in the fully connected layers. For LeNet-300-100 models, the number
of weights in the first fully connected layers (ip1) contains about 88% of the total number of
weights and this can be compressed by 97% by Delicate-SVRG-cumulative-01. Furthermore,
both Delicate-SVRG-cumulative-01 and pruning method have very similar compression rate
in convolutional layers (conv1 and conv2) in reducing the number of weights in LeNet-5
model, but Delicate-SVRG-cumulative-01 is more effective to reduce the number of weights
of the two fully connected layers (ip1 and ip2) sparse. Both Delicate-SVRG-cumulative-01
and Han et al. pruning method can achieve lower test error than that of uncompressed
models, whilst delivering overall compression rates up to 25× and 12× respectively. The
(a) MNIST dataset on LeNet-300-100 model (b) CIFAR-10 dataset on LeNet-300-100 model
(left) and LeNet-5 (right): error rate is top-1 er- (left) and LeNet-5 (right): error rate is top-5 er-
ror	ror
Figure 2:	Four 01 regularization compression methods experiment on two deep learning
models, including LeNet-300-100 and LeNet-5 using MNIST datasets and CIFAR-10.
second stage is to further compress the model by bit-depth reduction, and Table1 shows
our method Weight Reduction Quantisation that combines Delicate-SVRG-cumulative-01
with bit-depth reduction can achieve 1.56% error rate on MNIST, and 0.737% error rate on
CIFAR-10, where the two errors are all lower than that of original model. The compression
rates are up to 60× in LeNet-300-100 and up to 57× in LeNet-5 model.
1 Caffe is a deep learning framework.
http://caffe.berkeleyvision.org
Source code can be download:
7
Under review as a conference paper at ICLR 2018
5.2	Evaluation the Trade-off Between Memory Requirements and
Performance
Focusing on Delicate-SVRG-CUmUlative-£ 1 to examine the performance of method at differ-
ent compression rates controlled by threshold λ, we compare the performance of different
model-compression based on £ 1 regularization over the range of memory requirements. Fig-
ure 2 shows how the test error rate and weight sparsity vary as the regularization parameter
λ is adjusted. Where pareto fronts are not available for comparison, we compare with a
single trade-off and determine the related performance by the side of the pareto front that
the point lies.
LeNet on MNIST Figure 2(a) shows LeNet on MNIST. Compared with SVRG-
cumulative-£1, SGD-cumulative-£ 1 has the better ability of compression, but the error rate
is higher due to the variance generated by SGD optimiser. Replacing SGD with SVRG,
SVRG-Cumulative-£1 reduces the test error but the compression ability is also reduced. The
Delicate-SVRG-cumulative-。1 method has the least number of weights and the best perfor-
mance having the lowest test error for almost every compression value. Its performance is
similar with the method without bias-based pruning, which means that adding bias-based
pruning can further reduce the number of weights without side-effect on the performance.
The pink box on 2(a) showed that the results within the box is better than pink point.
LeNet on CIFAR-10 Figure 2(b) shows LeNet on CIFAR-10 dataset that is a larger
and more complicated dataset than MNIST. SVR G-cumulative-£ 1 has chances to achieve
lower test error than SGD-cumulative £1 but can not guarantee that the performance is
always better than SGD-cumulative-£ 1. Delicate-SVRG-cumulative-£ 1 method has better
performance than the other methods. Its performance is further enhanced by adding bias-
based pruning. Consequently,Delicate-SVRG-cumulative-£ 1 can be effectively applied in
LeNet-300-100 and LeNet-5 models without accuracy loss when applied to MNIST and
CIFAR-10.
5.3	Combining Delicate-SVRG-cumulative-£ 1 and Weight Quantization
Figure3 shows the test error at different compression rates for Delicate-SVRG-cumulative-£ 1
and weight Quantization. Individually, weight quantization can reduce more memory before
the test error increases significantly in Delicate-SVRG-cumulative-£ 1 using MNIST dataset,
but the reverse results applied on CIFAR-10 dataset. However, if combining together, the
approach consistently outperforms.
Ts。1
:w
m
I D D+Q
I ON LeNet-300-100
-
.
Hi i
∖ ■
0.065
0.064
0.063
0.062
g 0.061
I 0.06
0.059
0.058
0.057
0.056
5% 7% 10%	30%	50% 70%
=l=
≡∣ ⅛∣≡≡≡
≡≡≡⅛≡
! ∖ ∣≡i
≡≡⅛1≡
■■
Compression Rate



(a) MNIST dataset on LeNet-300-100 model (b) CIFAR-10 dataset on LeNet-300-100 model
(left) and LeNet-5 (right).	(left) and LeNet-5 (right).
Figure 3:	The test error with compression rate under different compression methods. D
is Delicate-SVRG-Cumulative-£ 1, Q is weight quantization. Combining Delicate-SVRG-
CumulatiVe-£ 1 with weight quantization can achieve the best performance.
8
Under review as a conference paper at ICLR 2018
5.4 Comparison of Convergence Rates
To confirm the theoretical insights that our method has no bad effect on convergence rate
to achieve similar fast convergence with SGD-CUmUlative-01 or SVRG-CUmUlative-01, We
calculate the training loss of two LeNet models on MNIST and CIFAR datasets during
increasing iterations. In Figure 4(a), all methods have similar convergence rates in LeNet-
300-100. In all of our experiments, DeliCate-SVRG-CUmUlative-01 has same or lower training
loss and faster convergence rate than other methods, meaning that adaptive learning rate can
help SVRG with cumulative 01 regularization to escape the local minimum in the beginning
and quickly converge to a good local minimum within finite training iterations. Moreover,
DeliCate-SVRG-CUmUlative-01 without bias-based pruning has a similar train loss, which
illustrates that adding bias-based pruning in 01 regularization has no obvious bad effect
on the convergence of weights. Consequently, applying adaptive learning rates, DeliCate-
SVRG-CUmUlative-01 is a efficient compression method for neural network problems.
6	Discussion
In this paper, we proposed Weight RedUCtion QUantisation that efficiently compressed neural
networks without scarifying accuracy. Our method has two stages that reduce the number
of weights and reduce the number of bits to store each weight. We show that SVRG and
cumulative 01 regularization can improve over SGD and 01-regularization. By combining
them, we have presented a new compression method DeliCate-SVRG-CUmUlative-01 that can
efficiently reduce the number of parameters by the separate adaptive learning rates. The
three adaptive learning rates are applied on SVRG and cumulative 01 penalty, which provides
a high accuracy and reduced number of weights. Besides, our method improved SVRG that
can be used on non-convex problem with fast convergence rate. In our experiments on
LeNet-300-100 and LeNet-5, our method can significantly reduce the memory requirements
up to 60× without accuracy loss. After compression by our method, a compact deep neural
network can be efficiently deployed on an embedded device with performance of the original
model.
References
Alekh Agarwal and Leon Bottou. A lower bound for the optimization of finite sums. In
David Blei and Francis Bach (eds.), ProCeedings of the 32nd International ConferenCe on
MaChine Learning (ICML-15), pp. 78—86. JMLR Workshop and Conference Proceedings,
2015. URL http://jmlr.org/proceedings/papers/v37/agarwal15.pdf.
Zeyuan Allen Zhu and Elad Hazan. Variance reduction for faster non-convex optimiza-
tion. In ProCeedings of the 33nd International ConferenCe on MaChine Learning, ICML
2016, New York City, NY, USA, June 19-24, 2016, pp. 699—707, 2016. URL http:
//jmlr.org/proceedings/papers/v48/allen-zhua16.html.
Zeyuan Allen-Zhu and Yang Yuan. Improved svrg for non-strongly-convex or sum-of-non-
convex objectives. In ProCeedings of the 33rd International ConferenCe on International
Conference on Machine Learning - Volume 48, ICML’16, pp. 1080-1089. JMLR.org, 2016.
URL http://dl.acm.org/citation.cfm?id=3045390.3045505.
Wenlin Chen, James T. Wilson, Stephen Tyree, Kilian Q. Weinberger, and Yixin Chen.
Compressing neural networks with the hashing trick. CoRR, abs/1504.04788, 2015. URL
http://arxiv.org/abs/1504.04788.
Maxwell D. Collins and Pushmeet Kohli. Memory bounded deep convolutional networks.
CoRR, abs/1412.1442, 2014. URL http://arxiv.org/abs/1412.1442.
Michael Collins, Amir Globerson, Terry Koo, Xavier Carreras, and Peter L. Bartlett. Ex-
ponentiated gradient algorithms for conditional random fields and max-margin markov
networks. JoUrnal of MaChine Learning ResearCh, 9:1775-1822, 2008. URL http:
//jmlr.csail.mit.edu/papers/v9/collins08a.html.
9
Under review as a conference paper at ICLR 2018
Emily Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. Ex-
ploiting linear structure within convolutional networks for efficient evaluation. CoRR,
abs/1404.0736, 2014. URL http://arxiv.org/abs/1404.0736.
Carlos M. Fonseca and Peter J. Fleming. An overview of evolutionary algorithms in mul-
tiobjeCtive optimization. Evol. CompUt, 3(1):1-16, March 1995. ISSN 1063-6560. doi:
10.1162/evco.1995.3.1.1. URL http://dx.doi.org/10.1162/evco.1995.3.1.1.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points - online
stochastic gradient for tensor decomposition. CoRR, abs/1503.02101, 2015. URL http:
//arxiv.org/abs/1503.02101.
Philipp Gysel, Mohammad Motamedi, and Soheil Ghiasi. Hardware-oriented approxi-
mation of convolutional neural networks. CoRR, abs/1604.03168, 2016. URL http:
//arxiv.org/abs/1604.03168.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural
networks with pruning, trained quantization and huffman coding. International Confer-
ence on Learning Representations (ICLR), 2016.
Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-
dinov. Improving neural networks by preventing co-adaptation of feature detectors. CoRR,
abs/1207.0580, 2012. URL http://arxiv.org/abs/1207.0580.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive
variance reduction. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q.
Weinberger (eds.), Advances in Neural Information Processing Systems 26, pp. 315-323.
Curran Associates, Inc., 2013.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to docu-
ment recognition. In Intel ligent Signal Processing, pp. 306-351. IEEE Press, 2001.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):
436-444, 05 2015. URL http://dx.doi.org/10.1038/nature14539.
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Im-
agenet classification using binary convolutional neural networks. CoRR, abs/1603.05279,
2016a. URL http://arxiv.org/abs/1603.05279.
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Im-
agenet classification using binary convolutional neural networks. CoRR, abs/1603.05279,
2016b. URL http://arxiv.org/abs/1603.05279.
Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola. Stochas-
tic variance reduction for nonconvex optimization. In Proceedings of the 33rd Inter-
national Conference on International Conference on Machine Learning - Volume 48,
ICML’16, pp. 314-323. JMLR.org, 2016. URL http://dl.acm.org/citation.cfm?id=
3045390.3045425.
Yoshimasa Tsuruoka, Jun’ichi Tsujii, and Sophia Ananiadou. Stochastic gradient descent
training for l1-regularized log-linear models with cumulative penalty. In Proceedings
of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Processing of the AFNLP: Volume 1 - Vol-
ume 1, ACL ’09, pp. 477-485, Stroudsburg, PA, USA, 2009. Association for Computa-
tional Linguistics. ISBN 978-1-932432-45-9. URL http://dl.acm.org/citation.cfm?
id=1687878.1687946.
10
Under review as a conference paper at ICLR 2018
Algorithm 1 Delicate-SVRG-Cumulative-£ 1: Stochastic descent training with cumulative
£ 1 penalty
procedure Train( λ)
uJ0
μ J 0
Initial wj and qj with zero for all number of weights M
for k=0 to Maximal Iterations do
no
1 + π(k∕N)
η0
Y J
βJ
1 + α(k∕N)3
for t=0 to k do
η J noat/N
end for
U J u + nλ∕M
end for
for j ∈ features used in sample i do
randomly select m features from train samples
Wj J Wj- (N ∑Nz1(Vψi(wj)- Vψi(wj)) + βk□)
▽Wi(Wj) = Vψi(Wj)
if Wj and Wj converge to the same weights then
μ = 0
end if
μ j μ + N vψi(W)
end for
end procedure
procedure Apply Penalty(i)
zJWj
b is minimal bias in all layers.
if Wj > 0 then
Wj ∈ max(0, Wj - (U + qj + b)),
else
if Wj < 0 then
Wj ∈ min(0, Wj + (U - qj - b)),
end if
end if
qj J qj + (Wj -z)
end procedure
7	Appendix
7.1	The algorithm of Delicate-SVRG-cumulative-£ 1
7.2	Comparison of the convergence rates of between our method and
SVRG AND SGD WHEN COMBINING WITH £ 1 REGULARIZATION.
The results showed in Figure4.
7.3	UsING MuLTIpLE INITIALIZATIoNs To coMpARE THE pERfoRMANcE of ouR
METHoD AND oTHER THREE METHoDs.
The experiments were run with multiple initializations and there was some small variability
in the results. However, the relative performance of the our method is always better than
SVRG and SGD combining with cumulative £ 1 regularization. The results showed in Figure5
11
Under review as a conference paper at ICLR 2018
(a) The train loss: MNIST dataset on LeNet-5 (left) and CIFAR-10 dataset on LeNet-300-100
(middle) and LeNet-5 (right)
(b) The test loss: MNIST dataset on LeNet-5 (left) and CIFAR-10 dataset on LeNet-300-100
(middle) and LeNet-5 (right)
Figure 4: Estimate the convergence rate when using four compression methods, includ-
ing our method Delicate-SVRG-Cumulative-01, Delicate-SVRG-Cumulative-01 (without Bi-
asPruning) that without bias-based pruning in 01 regularization, SVRG-Cumulative-01 and
SGD-Cumulative-01, on LeNet-300-100 and LeNet-5 models with MNIST and CIFAR-10
datasets. Here we choose the compression rate that equal 90% to observe training and
test loss. For MNIST dataset, we did not notice subtle difference train and test loss on
LeNet-300-100 mo del generated by four methods.
12
Under review as a conference paper at ICLR 2018
』0」」山ω31
#Weights
#Weights
#Weights
(a)	MNIST dataset on LeNet-300-100
#WeightS	乂出
SVRG-C-LI
D-SVRG-C-LI
T -D-SVRG-C-L1(wo BiasPruning)
S -4-SGD-C-LI	'______________

Li...
#WeightS
(b)	MNIST dataset on LeNet-5
13
Under review as a conference paper at ICLR 2018
105
#Weights
0.082
0.08
0.078
0.076
0.074
0.072
0.07
0.068
0.066
0.064
0.062
#W*ghts
0.08
0.078
0.076
0.074
0.072
0.07
0.068
0.066
0.064
0.062
106
0.1
0.09
104
#Weights
CIFAR-10 dataset on LeNet-300-100
0.07
0.05
0.03
0.3
0.25
0.19
0.17
0.15
0.13
■ SVRG-C-LI
♦ D-SVRG-C-LI
*D-SVRG-C-L1(wo BiasPruning)
-÷ SGD-C-L1
0.1
0.07
0.05
0.3
0.25
0.19
0.17
0.15
0.13
104
#Weights
0.4
0.03
■ SVRG-C-L1
♦ D-SVRG-C-LI
*D-SVRG-C-L1(wo BiasPruning)
-4-SGD-C-LI
#Weights
(d) CIFAR-10 dataset on LeNet-5
Figure 5: Using three types of initial weights, we compare our method with other three
methods. D-SVRG-C-L1 and D-SVRG-C-L1(wo BiasPruning) are always better than other
two methods. This experiment also can verify the our view that the performance of SVRG
is better or worse than SGD that depends on the number of training samples. In our
experiment, if choosing small dataset (e.g. MNIST), SVRG is better than SGD. Otherwise,
if choosing relatively large dataset (e.g. CIFAR-10), SVRG is worse than SGD.
14