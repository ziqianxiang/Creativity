Under review as a conference paper at ICLR 2018
Training Deep	AutoEncoders	for Recom-
mender Systems
Anonymous authors
Paper under double-blind review
Ab stract
This paper proposes a new model for the rating prediction task in recommender
systems which significantly outperforms previous state-of-the art models on a
time-split Netflix data set. Our model is based on deep autoencoder with 6 lay-
ers and is trained end-to-end without any layer-wise pre-training. We empirically
demonstrate that: a) deep autoencoder models generalize much better than the
shallow ones, b) non-linear activation functions with negative parts are crucial
for training deep models, and c) heavy use of regularization techniques such as
dropout is necessary to prevent overfitting. We also propose a new training al-
gorithm based on iterative output re-feeding to overcome natural sparseness of
collaborate filtering. The new algorithm significantly speeds up training and im-
proves model performance. 1
1	Introduction
Sites like Amazon, Netflix and Spotify use recommender systems to suggest items to users. Recom-
mender systems can be divided into two categories: context-based and personalized recommenda-
tions.
Context based recommendations take into account contextual factors such as location, date and time
(Adomavicius & Tuzhilin, 2011). Personalized recommendations typically suggest items to users
using the collaborative filtering (CF) approach. In this approach the user’s interests are predicted
based on the analysis of tastes and preference of other users in the system and implicitly inferring
“similarity” between them. The underlying assumption is that two people who have similar tastes,
have a higher likelihood of having the same opinion on an item than two randomly chosen people.
In designing recommender systems, the goal is to improve the accuracy of predictions. The Netflix
Prize contest provides the most famous example of this problem (Bennett et al., 2007): Netflix held
the Netflix Prize to substantially improve the accuracy of the algorithm to predict user ratings for
films. This is a classic CF problem: Infer the missing entries in an mxn matrix, R, whose (i, j)
entry describes the ratings given by the ith user to the jth item. The performance is then measured
using Root Mean Squared Error (RMSE).
Training very deep autoencoders is non trivial both from optimization and regularization points of
view. Early works on training auto-enocoders adapted layer-wise pre-training to solve optimiza-
tion issues (Hinton & Salakhutdinov, 2006). In this work, we empirically show that optimization
difficulties of training deep autoencoders can be solved by using scaled exponential linear units
(SELUs)(Klambauer et al., 2017). This enables training without any layer-wise pre-training or resid-
ual connections. Since publicly available data sets for CF are relatively small, sufficiently large
models can easily overfit. To prevent overfitting we employ heavy dropout with drop probability as
high as 0.8. We also introduce a new output re-feeding training algorithm which helps to bypass
the natural sparseness of updates in collaborative filtering and helps to further improve the model
performance.
1Our code is available at https://github.com/Anonymous
1
Under review as a conference paper at ICLR 2018
1.1	Related work
Deep learning (LeCun et al., 2015) has led to breakthroughs in image recognition, natural language
understanding, and reinforcement learning. Naturally, these successes fuel an interest for using
deep learning in recommender systems. First attempts at using deep learning for recommender
systems involved restricted Boltzman machines (RBM) (Salakhutdinov et al., 2007). Several re-
cent approaches use autoencoders (Sedhain et al., 2015; Strub & Mary, 2015), feed-forward neural
networks (He et al., 2017), neural autoregressive architectures (Zheng et al., 2016) and recurrent
recommender networks (Wu et al., 2017). Many popular matrix factorization techniques can be
thought of as a form of dimensionality reduction. It is, therefore, natural to adapt deep autoencoders
for this task as well. I-AutoRec (item-based autoencoder) and U-AutoRec (user-based autoencoder)
are first successful attempts to do so Sedhain et al. (2015). Stacked de-noising autoencoders has
been sucesfully used on this task as well (Li et al., 2015; Wang et al., 2015).
There are many non deep learning types of approaches to collaborative filtering (CF) (Breese et al.,
1998; Ricci et al., 2011). Matrix factorization techniques, such as alternating least squares (ALS)
(Kim & Park, 2008; Koren et al., 2009) and probabilistic matrix factorization (Mnih & Salakhutdi-
nov, 2008) are particularly popular. The most robust systems may incorporate several ideas together
such as the winning solution to the Netflix Prize competition (Koren, 2009). Note that Netflix Prize
data also includes temporal signal - time when each rating has been made. Thus, several classic
CF approaches has been extended to incorporate temporal information such as TimeSVD++ Koren
(2010), as well as more recent RNN-based techniques such as recurrent recommender networks Wu
et al. (2017).
2	Model
Our model is inspired by U-AutoRec approach with several important distinctions. We train much
deeper models. To enable this without any pre-training, we: a) use “scaled exponential linear units”
(SELUs) Klambauer et al. (2017), b) use high dropout rates, and d) use iterative output re-feeding
during training.
An autoencoder is a network which implements two transformations - encoder encode(x) : Rn →
Rd and decoder(z) : Rd → Rn. The “goal” of autoenoder is to obtain d dimensional representation
of data such that an error measure between x and f (x) = decode(encode(x)) is minimized Hinton
& Zemel (1994). Figure 1 depicts typical 4-layer autoencoder network. If noise is added to the
data during encoding step, the autoencoder is called de-noising. Autoencoder is an excellent tool
for dimensionality reduction and can be thought of as a strict generalization of principle component
analysis (PCA) Hinton & Salakhutdinov (2006). An autoencoder without non-linear activations
and only with “code” layer should be able to learn PCA transformation in the encoder if trained to
optimize mean squared error (MSE) loss.
In our model, both encoder and decoder parts of the autoencoder consist of feed-forward neural
networks with classical fully connected layers computing l = f (W * X + b), where f is some non-
linear activation function. If range of the activation function is smaller than that of data, the last layer
of the decoder should be kept linear. We found it to be very important for activation function f in
hidden layers to contain non-zero negative part, and we use SELU units in most of our experiments
(see Section 3.2 for details).
If decoder mirrors encoder architecture (as it does in our model), then one can constrain decoder’s
weights Wdl to be equal to transposed encoder weights Wel from the corresponding layer l. Such
autoencoder is called constrained or tied and has almost two times less free parameters than uncon-
strained one.
Forward pass and inference. During forward pass (and inference) the model takes user represented
by his vector of ratings from the training set x ∈ Rn, where n is number of items. Note that x is
very sparse, while the output of the decoder, f(x) ∈ Rn is dense and contains rating predictions for
all items in the corpus.
2
Under review as a conference paper at ICLR 2018
2.1	Loss function
Since it doesn’t make sense to predict zeros in user’s representation vector x, we follow the approach
from Sedhain et al. (2015) and optimize Masked Mean Squared Error loss:
MMSE
mi * (r - yi)2
∑i=n
i=0 mi
(1)
where ri is actual rating, yi is reconstructed, or predicted rating, and mi is a mask function such that
mi = 1 if ri = 0 else mi = 0. Note that there is a straightforward relation between RMSE score
and MMSE score: RMSE = √MMSE.
2.2	Dense re-feeding
During training and inference, an input x ∈ Rn is very sparse because no user can realistically rate
but a tiny fractions of all items. This poses problem for model training. Bayesian approches can
be used to overcome this issue (Wang et al., 2015). On the other hand, autoencoder’s output f(x)
is dense. Lets consider an idealized scenario with a perfect f. Then f (x)i = xi, ∀i : xi 6= 0 and
f (x)i accurately predicts all user’s future ratings for items i : xi = 0. This means that if user rates
new item k (thereby creating a new vector x0) then f (x)k = x0k and f(x) = f(x0). Hence, in this
idealized scenario, y = f(x) should be a fixed point ofa well trained autoencoder: f(y) = y.
To explicitly enforce fixed-point constraint and to be able to perform dense training updates, we
augment every optimization iteration with an iterative dense re-feeding steps (3 and 4 below) as
follows:
1.	Given sparse x, compute dense f(x) and loss using equation 1 (forward pass)
2.	Compute gradients and perform weight update (backward pass)
3.	Treat f(x) as a new example and compute f (f (x)). Now both f(x) and f (f (x)) are dense
and the loss from equation 1 has all m as non-zeros. (second forward pass)
4.	Compute gradients and perform weight update (second backward pass)
Steps (3) and (4) can be also performed more than once for every iteration.
3	Experiments and Results
3.1	Experiment setup
For the rating prediction task, it is often most relevant to predict future ratings given the past ones
instead of predicting ratings missing at random. For evaluation purposes we followed Wu et al.
(2017) exactly by splitting the original Netflix Prize Bennett et al. (2007) training set into several
training and testing intervals based on time. Training interval contains ratings which came in earlier
than the ones from testing interval. Testing interval is then randomly split into Test and Validation
subsets so that each rating from testing interval has a 50% chance of appearing in either subset.
Users and items that do not appear in the training set are removed from both test and validation
subsets. Table 1 provides details on the data sets.2
For most of our experiments we uses a batch size of 128, trained using SGD with momentum of 0.9
and learning rate of 0.001. We used xavier initialization to initialize parameters. Note, that unlike
Strub & Mary (2015) we did not use any layer-wise pre-training. We believe that we were able to
do so successfully because of choosing the right activation function (see Section 3.2).
3.2	Effects of the activation types
To explore the effects of using different activation functions, we tested some of the most popular
choices in deep learning : sigmoid, “rectified linear units” (RELU), max(relu(x), 6) or RELU6,
2Note, that while checking our data set statistics with first author of (Wu et al., 2017) it was determined that
their publication contained the following typo: “Netflix 6m” should be “Netflix 3m”.
3
Under review as a conference paper at ICLR 2018
Figure 1: AutoEncoder consists of two neural networks, encoder and decoder, fused together on
the “representation” layer z. Encoder has 2 layers e1 and e2 and decoder has 2 layers d1 and d2 .
Dropout may be applied to coding layer z .
Table 1: Subsets of Netflix Prize training set used in our experiments. We made sure that these splits
match exactly the ones used in (Wu et al., 2017).
Full 3 months 6 months 1 year
Training	12/99-11/05	09/05-11/05	06/05-11/05	06/04-05/05
Users	477,412	311,315	390,795	345,855
Ratings	98,074,901	13,675,402	29,179,009	41,451,832
Testing	12/05	12/05	12/05	06/05
Users	173,482	160,906	169,541	197,951
Ratings	2,250,481	2,082,559	2,175,535	3,888,684
hyperbolic tangent (TANH), “exponential linear units” (ELU) (Clevert et al., 2015), leaky relu
(LRELU) (Xu et al., 2015) , “self-gated activation function” (SWISH) (Ramachandran et al., 2017),
and “scaled exponential linear units” (Klambauer et al., 2017) (SELU) on the 4 layer autoencoder
with 128 units in each hidden layer. Because ratings are on the scale from 1 to 5, we keep last layer
of the decoder linear for sigmoid and tanh-based models. In all other models activation function is
applied in all layers.
We found that on this task ELU, SELU and LRELU perform much better than SIGMOID, RELU,
RELU6, TANH and SWISH. Figure 2 clearly demonstrates this. There are two properties which
seems to separate activations which perform well from those which do not: a) non-zero negative
part and b) unbounded positive part. Hence, we conclude, that in this setting these properties are im-
portant for successful training. Thus, we use SELU activation units and tune SELU-based networks
for performance.
4
Under review as a conference paper at ICLR 2018
Figure 2: Training RMSE per mini-batch. All lines correspond to 4-layers autoencoder (2 layer
encoder and 2 layer decoder) with hidden unit dimensions of 128. Different line colors correspond
to different activation functions. TANH and SIGMOID lines are very similar as well as lines for
ELU and SELU. The best performing activation functions are ELU and SELU.
3.3	Over-fitting the data
The largest data set we use for training, “Netflix Full” from Table 1, contains 98M ratings given by
477K users. Number of movies (e.g. items) in this set is n = 17, 768. Therefore, the first layer of
encoder Win have d * n + d weights, where d is number of units in the layer.
For modern deep learning algorithms and hardware this is relatively small task. If we start with
single layer encoders and decoders we can quickly overfit to the training data even for d as small as
512. Figure 3 clearly demonstrates this. Switching from unconstrained autoencoder to constrained
reduces over-fitting, but does not completely solve the problem.
Figure 3: Single layer autoencoder with 128, 256, 512 and 1024 hidden units in the coding layer. A:
training RMSE per epoch; B: evaluation RMSE per epoch.
3.4	Going deeper
While making layers wider helps bring training loss down, adding more layers is often correlated
with a network’s ability to generalize. In this set of experiments we show that this is indeed the
case here. We choose small enough dimensionality (d = 128) for all hidden layers to easily avoid
over-fitting and start adding more layers. Table 2 shows that there is a positive correlation between
the number of layers and the evaluation accuracy.
5
Under review as a conference paper at ICLR 2018
Table 2: Depth helps generalization. Evaluation RMSE of the models with different number of
layers. In all cases the hidden layer dimension is 128.
Number of layers	Evaluation RMSE	params
2	1.146	4,566,504
4	0.9615	4,599,528
6	0.9378	4,632,552
8	0.9364	4,665,576
10	0.9340	4,698,600
12	0.9328	4,731,624
Going from one layer in encoder and decoder to three layers in both provides good improvement in
evaluation RMSE (from 1.146 to 0.9378). After that, blindly adding more layers does help, however
it provides diminishing returns. Note that the model with single d = 256 layer in encoder and
decoder has 9,115,240 parameters which is almost two times more than any of these deep models
while having much worse evauation RMSE (above 1.0).
3.5	Dropout
Section 3.4 shows us that adding too many small layers eventually hits diminishing returns. Thus, we
start experimenting with model architecture and hyper-parameters more broadly. Our most promis-
ing model has the following architecture: n, 512, 512, 1024, 512, 512, n, which means 3 layers in
encoder (512,512,1024), coding layer of 1024 and 3 layers in decoder of size 512,512,n. This
model, however, quickly over-fits if trained with no regularization. To regularize it, we tried several
dropout values and, interestingly, very high values of drop probability (e.g. 0.8) turned out to be
the best. See Figure 4 for evaluation RMSE. We apply dropout on the encoder output only, e.g.
f (x) = decode(dropout(encode(x))). We tried applying dropout after every layer of the model
but that stifled training convergence and did not improve generalization.
Figure 4: Effects of dropout. Y-axis: evaluation RMSE, X-axis: epoch number. Model with no
dropout (Drop Prob 0.0) clearly over-fits. Model with drop probability of 0.5 over-fits as well (but
much slowly). Models with drop probabilities of 0.65 and 0.8 result in RMSEs of 0.9192 and 0.9183
correspondingly.
3.6	Dense re-feeding
Iterative dense re-feeding (see Section 2.2) provides us with additional improvement in evaluation
accuracy for our 6-layer-model: n, 512, 512, 1024, dp(0.8), 512, 512, n (referred to as Baseline be-
low). Here each parameter denotes the number of inputs, hidden units, or outputs and dp(0.8) is a
dropout layer with a drop probability of 0.8. Just applying output re-feeding did not have significant
6
Under review as a conference paper at ICLR 2018
Table 3: Test RMSE of different models. I-AR, U-AR and RRN numbers are taken from (Wu et al.,
2017)	DataSet	I-AR	U-AR	RRN	DeepRec Netflix 3 months	0.9778	0.9836	0.9427	0.9373 Netfix Full	0.9364	0.9647	0.9224	0.9099
impact on the model performance. However, in conjunction with the higher learning rate, it did
significantly increase the model performance. Note, that with this higher learning rate (0.005) but
without dense re-feeding, the model started to diverge. See Figure 5 for details.
Figure 5: Effects of dense re-feeding. Y-axis: evaluation RMSE, X-axis: epoch number. Baseline
model was trained with learning rate of 0.001. Applying re-feeding step with the same learning
rate almost did not help (Baseline RF). Learning rate of 0.005 (Baseline LR 0.005) is too big for
baseline model without re-feeding. However, increasing both learning rate and applying re-feeding
step clearly helps (Baseline LR 0.005 RF).
Applying dense re-feeding and increasing the learning rate, allowed us to further improve the evalua-
tion RMSE from 0.9167 to 0.9100. Picking a checkpoint with best evaluation RMSE and computing
test RMSE gives as 0.9099, which we believe is significantly better than other methods.
3.7	Comparison with other methods
We compare our best model with Recurrent Recommender Network from Wu et al. (2017) which
has been shown to outperform PMF (Mnih & Salakhutdinov, 2008), T-SVD (Koren, 2010) and I/U-
AR (Sedhain et al., 2015) on the data we use (see Table 1 for data description). Note, that unlike
T-SVD and RRN, our method does not explicitly take into account temporal dynamics of ratings.
Yet, Table 3 shows that it is still capable of outperforming these methods on future rating prediction
task. We train each model using only the training set and compute evaluation RMSE for 100 epochs.
Then the checkpoint with the highest evaluation RMSE is tested on the test set.
“Netflix 3 months” has 7 times less training data compared to “Netflix full”, it is therefore, not
surprising that the model’s performance is significantly worse if trained on this data alone (0.9373
vs 0.9099). In fact, the model that performs best on “Netflix full” over-fits on this set, and we had
to reduce the model’s complexity accordingly (see Table 4 for details).
7
Under review as a conference paper at ICLR 2018
Table 4: Test RMSE achieved by DeepRec on different Netflix subsets. All models are trained with
one iterative output re-feeding step per each iteration.
DataSet
Netflix 3 months 0.9373
Netflix 6 months 0.9207
Netflix 1 year 0.9225
Netfix Full 0.9099
Model Architecture
n, 128, 256, 256, dp(0.65), 256, 128, n
n, 256, 256, 512, dp(0.8), 256, 256, n
n, 256, 256, 512, dp(0.8), 256, 256, n
n, 512, 512, 1024, dp(0.8), 512, 512, n
4	Conclusion
Deep learning has revolutionized many areas of machine learning, and it is poised do so with recom-
mender systems as well. In this paper we demonstrated how very deep autoencoders can be success-
fully trained even on relatively small amounts of data by using both well established (dropout) and
relatively recent (“scaled exponential linear units”) deep learning techniques. Further, we introduced
iterative output re-feeding - a technique which allowed us to perform dense updates in collaborative
filtering, increase learning rate and further improve generalization performance of our model. On
the task of future rating prediction, our model outperforms other approaches even without using
additional temporal signals.
While our code supports item-based model (such as I-AutoRec) we argue that this approach is less
practical than user-based model (U-AutoRec). This is because in real-world recommender systems,
there are usually much more users then items. Finally, when building personalized recommender
system and faced with scaling problems, it can be acceptable to sample items but not users.
Acknowledgments
We thank the author of (Wu et al., 2017), Chao-Yuan Wu, for fruitfull discussion and help validating
our data sets.
References
Gediminas Adomavicius and Alexander Tuzhilin. Context-aware recommender systems. In Recom-
mender systems handbook, pp. 217-253. Springer, 2011.
James Bennett, Stan Lanning, and Netflix Netflix. The netflix prize. In In KDD Cup and Workshop
in conjunction with KDD, 2007.
John S. Breese, David Heckerman, and Carl Kadie. Empirical analysis of predictive algorithms for
collaborative filtering. In Proceedings of the Fourteenth Conference on Uncertainty in Artificial
Intelligence, UAI’98, pp. 43-52, San Francisco, CA, USA, 1998. Morgan Kaufmann Publishers
Inc. ISBN 1-55860-555-X. URL http://dl.acm.org/citation.cfm?id=2074094.
2074100.
Djork-Ame Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network
learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.
Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. Neural col-
laborative filtering. In Proceedings of the 26th International Conference on World Wide Web, pp.
173-182. International World Wide Web Conferences Steering Committee, 2017.
Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural
networks. science, 313(5786):504-507, 2006.
Geoffrey E Hinton and Richard S. Zemel. Autoencoders, minimum description
length and helmholtz free energy. In J. D. Cowan, G. Tesauro, and J. Al-
8
Under review as a conference paper at ICLR 2018
SPector (eds.), Advances in Neural Information Processing Systems 6, pp. 3-
10. Morgan-Kaufmann, 1994.	URL http://papers.nips.cc/paper/
798-autoencoders-minimum-description-length-and-helmholtz-free-energy.
pdf.
Hyunsoo Kim and Haesun Park. Nonnegative matrix factorization based on alternating nonneg-
ativity constrained least squares and active set method. SIAM journal on matrix analysis and
applications, 30(2):713-730, 2008.
Gunter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing
neural networks. arXiv preprint arXiv:1706.02515, 2017.
Yehuda Koren. The bellkor solution to the netflix grand prize. Netflix prize documentation, 81:1-10,
2009.
Yehuda Koren. Collaborative filtering with temporal dynamics. Communications of the ACM, 53
(4):89-97, 2010.
Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recommender
systems. Computer, 42(8), 2009.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436-444,
2015.
Sheng Li, Jaya Kawale, and Yun Fu. Deep collaborative filtering via marginalized denoising auto-
encoder. In Proceedings of the 24th ACM International on Conference on Information and Knowl-
edge Management, pp. 811-820. ACM, 2015.
Andriy Mnih and Ruslan R Salakhutdinov. Probabilistic matrix factorization. In Advances in neural
information processing systems, pp. 1257-1264, 2008.
Prajit Ramachandran, Barret Zoph, and Quoc V Le. Swish: a self-gated activation function. arXiv
preprint arXiv:1710.05941, 2017.
Francesco Ricci, Lior Rokach, and Bracha Shapira. Introduction to recommender systems hand-
book. In Recommender systems handbook, pp. 1-35. Springer, 2011.
Ruslan Salakhutdinov, Andriy Mnih, and Geoffrey Hinton. Restricted boltzmann machines for
collaborative filtering. In Proceedings of the 24th international conference on Machine learning,
pp. 791-798. ACM, 2007.
Suvash Sedhain, Aditya Krishna Menon, Scott Sanner, and Lexing Xie. Autorec: Autoencoders
meet collaborative filtering. In Proceedings of the 24th International Conference on World Wide
Web, pp. 111-112. ACM, 2015.
Florian Strub and Jeremie Mary. Collaborative filtering with stacked denoising autoencoders and
sparse inputs. In NIPS workshop on machine learning for eCommerce, 2015.
Hao Wang, Naiyan Wang, and Dit-Yan Yeung. Collaborative deep learning for recommender sys-
tems. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discov-
ery and Data Mining, pp. 1235-1244. ACM, 2015.
Chao-Yuan Wu, Amr Ahmed, Alex Beutel, Alexander J. Smola, and How Jing. Recurrent recom-
mender networks. In Proceedings of the Tenth ACM International Conference on Web Search
and Data Mining, WSDM ’17, pp. 495-503, New York, NY, USA, 2017. ACM. ISBN 978-
1-4503-4675-7. doi: 10.1145/3018661.3018689. URL http://doi.acm.org/10.1145/
3018661.3018689.
Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical evaluation of rectified activations in
convolutional network. arXiv preprint arXiv:1505.00853, 2015.
Yin Zheng, Bangsheng Tang, Wenkui Ding, and Hanning Zhou. A neural autoregressive approach
to collaborative filtering. arXiv preprint arXiv:1605.09477, 2016.
9