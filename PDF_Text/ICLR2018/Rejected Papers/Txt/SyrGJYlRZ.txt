Under review as a conference paper at ICLR 2018
YellowFin and the Art of Momentum Tuning
Anonymous authors
Paper under double-blind review
Abstract
Hyperparameter tuning is one of the most time-consuming workloads in deep
learning. State-of-the-art optimizers, such as AdaGrad, RMSProp and Adam, re-
duce this labor by adaptively tuning an individual learning rate for each variable.
Recently researchers have shown renewed interest in simpler methods like momen-
tum SGD as they may yield better results. Motivated by this trend, we ask: can
simple adaptive methods, based on SGD perform as well or better? We revisit the
momentum SGD algorithm and show that hand-tuning a single learning rate and
momentum makes it competitive with Adam. We then analyze its robustness to
learning rate misspecification and objective curvature variation. Based on these
insights, we design YellowFin, an automatic tuner for momentum and learning
rate in SGD. YellowFin optionally uses a negative-feedback loop to compensate
for the momentum dynamics in asynchronous settings on the fly. We empirically
show YellowFin can converge in fewer iterations than Adam on ResNets and
LSTMs for image recognition, language modeling and constituency parsing, with a
speedup of up to 3.28x in synchronous and up to 2.69x in asynchronous settings.
1 Introduction
Accelerated forms of stochastic gradient descent (SGD), pioneered by Polyak (1964) and Nesterov
(1983), are the de-facto training algorithms for deep learning. Their use requires a sane choice for
their hyperparameters: typically a learning rate and momentum parameter (Sutskever et al., 2013).
However, tuning hyperparameters is arguably the most time-consuming part of deep learning, with
many papers outlining best tuning practices written (Bengio, 2012; Orr and Muller, 2003; Bengio
et al., 2012; Bottou, 2012). Deep learning researchers have proposed a number of methods to deal
with hyperparameter optimization, ranging from grid-search and smart black-box methods (Bergstra
and Bengio, 2012; Snoek et al., 2012) to adaptive optimizers. Adaptive optimizers aim to eliminate
hyperparameter search by tuning on the fly for a single training run: algorithms like AdaGrad (Duchi
et al., 2011), RMSProp (Tieleman and Hinton, 2012) and Adam (Kingma and Ba, 2014) use the
magnitude of gradient elements to tune learning rates individually for each variable and have been
largely successful in relieving practitioners of tuning the learning rate.
Recently some researchers have started favoring
simple momentum SGD over the previously men-
tioned adaptive methods (Chen et al., 2016; Gehring
et al., 2017), often reporting better test scores (Wil-
son et al., 2017). Motivated by this trend, we ask the
question: can simpler adaptive methods, based on
momentum SGD perform as well or better? We em-
pirically show that, with hand-tuned learning rate,
Polyak’s momentum SGD achieves faster conver-
gence than Adam for a large class of models. We
then formulate the optimization update as a dynam-
ical system and study certain robustness properties
of the momentum operator. Building on our analysis, we design YellowFin, an automatic hy-
perparameter tuner for momentum SGD. YellowFin simultaneously tunes the learning rate and
momentum on the fly, and can handle the complex dynamics of asynchronous execution. Specifically:
•	In Section 2, we show that momentum presents convergence robust to learning rate misspecifica-
tion and curvature variation in a class of non-convex objectives; this robustness is desirable for
101 i
100 •……
Figure 1: YellowFin in comparison to Adam
on a ResNet (CIFAR100, cf. Section 5).
Asynchronous training
T→ Adam
.....O-O-YellowFin ..
YellowFin ....
0k 30k	60k	90k	120k
Iterations
1
Under review as a conference paper at ICLR 2018
deep learning. They stem from a known but obscure fact: the momentum operator’s spectral
radius is constant in a large subset of the hyperparameter space.
•	In Section 3, we use these robustness insights and a simple quadratic model analysis to design
YellowFin, an automatic tuner for momentum SGD. YellowFin uses on-the-fly measure-
ments from the gradients to tune both a single learning rate and momentum.
•	In Section 3.3, we discuss common stability concerns related to the phenomenon of exploding
gradients (Pascanu et al., 2013). We present a natural extension to our basic tuner, using adaptive
gradient clipping, to stabilize training for objectives with exploding gradients.
•	In Section 4 we present closed-loop YELLOWFIN, suited for asynchronous training. It uses
a novel component for measuring the total momentum in a running system, including any
asynchrony-induced momentum, a phenomenon described in Mitliagkas et al. (2016). This
measurement is used in a negative feedback loop to control the value of algorithmic momentum.
We provide a thorough evaluation of the performance and stability of our tuner. In Section 5, we
demonstrate empirically that on ResNets and LSTMs YellowFin can converge in fewer iterations
compared to: (i) hand-tuned momentum SGD (up to 1.75x speedup); and (ii) default Adam (0.8x
to 3.3x speedup). Under asynchrony, the closed-loop control architecture speeds up YellowFin,
making it up to 2.69x faster than Adam. Our experiments include runs on 7 different models,
randomized over at least 5 different random seeds. YellowFin is stable and achieves consistent
performance: the normalized sample standard deviation of test metrics varies from 0.05% to 0.6%.
We released PyTorch and TensorFlow implementations, that can be used as drop-in replacements for
any optimizer. YellowFin has also been implemented in other various packages. Its large-scale
deployment in industry has taught us important lessons about stability; we discuss those challenges
and our solution in Section 3.3. We conclude with related work and discussion in Section 6 and 7.
2The momentum operator
In this section we identify the main technical insights guiding the design of YellowFin. We show
that momentum presents convergence robust to learning rate misspecification and curvature variation
for a class of non-convex objectives; these robustness properties are desirable for deep learning.
2.1	Preliminaries
We aim to minimize some objective f (x). In machine learning, x is referred to as the model and
the objective is some loss function. A low loss implies a well-fit model. Gradient descent-based
procedures use the gradient of the objective function, Vf (χ),to update the model iteratively. Polyak's
momentum gradient descent update (Polyak, 1964) is given by
xt+1 = Xt 一 αVf (Xt) + μ(xt — xt-i),	(1)
where α denotes the learning rate and μ the value of momentum used. Momentum's main appeal
is its established ability to accelerate convergence (Polyak, 1964). On a strongly convex smooth
function with condition number κ, the optimal convergence rate of gradient descent (μ = 0) is
O(KK+∣) (Nesterov, 2013). On the other hand, for certain classes of strongly convex and smooth
functions, like quadratics, the optimal momentum value,
〃* = ( √K÷1 )2，	⑵
yields the optimal accelerated rate O( √K-1). 1 ThiS is the smallest value of momentum that ensures
the same rate of convergence along all directions. This fact is often hidden away in proofs. We
shed light on some of its previously unknown implications in Section 2.2.
2.2	Robustness properties of the momentum operator
In this section we analyze the dynamics of momentum on a simple class of one dimensional, non-
convex objectives. We first introduce the notion of generalized curvature and use it to describe the
momentum operator. Then we discuss some properties of the momentum operator.
1This guarantee does not generalize to arbitrary strongly convex functions (Lessard et al., 2016). Nonetheless,
acceleration is often observed in practice (cf. Section 2.2).
2
Under review as a conference paper at ICLR 2018
Definition 1 (Generalized curvature). The derivative of f(x):R → R, can be written as
f0(x) = h(x)(x — x*)	(3)
for some h(x) ∈ R, where x* is the global minimum of f (x). We call h(x) the generalized curvature.
The generalized curvature describes, in some sense, curvature with respect to the optimum, x*. For
quadratic objectives, it coincides with the standard definition of curvature, and is the sole quantity
related to the objective that influences the dynamics of gradient descent. For example, the contraction
of a gradient descent step is 1 — αh(xt). Let At denote the momentum operator at time t. Using a
state-space augmentation, we can express the momentum update as
Xxt+ι — x*∖ _ 1 — αh(x, + μ
xt — x*	=	1
-μ
0
xt — x*	xt — x*
xt-t1 —x* ,At xt-t1 —x* .
(4)
Lemma 2 (Robustness of the momentum operator). As proven in Appendix A, if the generalized
curvature, h, and hyperparameters α,μ are in the robust region, that is:
(1 — √μ)2 ≤ αh(xt) ≤ (1 + √μ)2,	(5)
then the spectral radius ofthe momentum operator only depends on momentum: P(At) = √μ.
We explain Lemma 2 as robust convergence with respect to learning rate and to variations in curvature.
Momentum is robust to learning rate misspecification For a
one dimensional strongly convex quadratic objective, we get h(x) =
h for all X and Lemma 2 suggests that P(At) = √μ as long as
(1 — √μ)2∕h ≤ α ≤ (1 + √μ)2∕h.	(6)
In Figure 2, we plot P(At) for different a and μ. As we increase the
value of momentum, the optimal rate of convergence √μ is achieved
by an ever-widening range of learning rates. Furthermore, for objec-
tives with large condition number, higher values of momentum are
both faster and more robust. This property influences the design
of our tuner: as long as the learning rate satisfies (6), we are in the
robust region and expect the same asymptotics, e.g. a convergence
rate of √μ for quadratics, independent of the learning rate. Having
established that, we can just focus on optimally tuning momentum.
Figure 2: Momentum operator
on scalar quadratic.
Momentum is robust to curvature variation As discussed in Section 2.1, the intuition hidden in
classic results is that for strongly convex smooth objectives, the momentum value in (2) guarantees
the same rate of convergence along all directions. We extend this intuition to certain non-convex
functions. Lemma 2 guarantees a constant, time-homogeneous spectral radius for the momentum
operators (At)t if (5) is satisfied at every step. This motivates an extension of the condition number.
Definition 3 (Generalized condition number). We define the generalized condition number (GCN) of
a scalar function, f(x):R → R, to be the dynamic range of its generalized curvature, h(x):
inf x∈dom(f) h(x)
(7)
The GCN captures variations in generalized curvature along a scalar slice. From Lemma 2 we get
μ*=✓ √V+4 )2，
(1 - √μ)2
infx∈dom(f) h(X)
(8)
as the optimal hyperparameters. Specifically, μ* is the smallest momentum value that guarantees a
homogeneous spectral radius of √μ* for all (At)t. The spectral radius of an operator describes its
asymptotic convergence behavior. However, the product of a sequence of operators At …Ai all with
spectral radius √μ does not always follow the asymptotics of √μt. In other words, we do not provide
a convergence rate guarantee. Instead, we provide evidence in support of this intuition. For example,
the non-convex objective in Figure 3(a), composed of two quadratics with curvatures 1 and 1000, has
a GCN of 1000. Using the tuning rule of (8), and running the momentum algorithm (Figure 3(b))
yields a practically constant rate of convergence throughout. In Figures 3(c,d) we demonstrate that
for an LSTM, the majority of variables follow a √μ convergence rate. This property influences the
design of our tuner: in the next section we use the tuning rules of (8) in YELLOWFIN, generalized
appropriately to handle SGD noise.
ν
3
Under review as a conference paper at ICLR 2018
-2-1-10-5 0 5 10 15 20
X
-I-L 2 2 1 1 01. 1 1 2 2 -I-L 4 Pt 61. 7 7
Oooo................................
Iiiiooooooo
1111111
xunxudo XUoJJ əɔueasɑ
100 200 300 400 500
Iterations
0 50 100 150 200 250 300
Iterations
0 50 100 150 200 250 300
Iterations
(a)	(b)	(c)	(d)
Figure 3: (a) Non-convex toy example; (b) constant convergence rate achieved empirically on the
objective of (a) tuned according to (8); (c,d) LSTM on MNIST: as momentum increases, more
variables (shown in grey) fall in the robust region and follow the robust rate, √μ.
3The YellowFin tuner
In this section we describe YellowFin, our tuner for momentum SGD. We introduce a noisy
quadratic model and work on a local quadratic approximation of f(x) to apply the tuning rule of (8)
to SGD on an arbitrary objective. YellowFin is our implementation of that rule.
Noisy quadratic model We consider minimizing the one-dimensional quadratic
f(x) = 1 hχ2 + C =1X1 h(x - ci)2,1X fi(x),	X ci = 0.	⑼
2	n2	n
i	ii
The objective is defined as the average of n component functions, fi . This is a common model
for SGD, where we use only a single data point (or a mini-batch) drawn uniformly at random,
St 〜Uni([n]) to compute a noisy gradient, Vfst (x), for step t. Here, C = 2n Pi hc2 denotes the
gradient variance. This scalar model is sufficient to study an arbitrary local quadratic approximation:
optimization on quadratics decomposes trivially into scalar problems along the principal eigenvectors
of the Hessian. Next we get an exact expression for the mean square error after running momentum
SGD on a scalar quadratic for t steps.
Lemma 4. Let f(x) be defined as in (9), x1 = x0 and xt follow the momentum update (1) with
stochastic gradients VfSt (xt-1) fort ≥ 2. Let e1 =[1, 0]T, the expectation of squared distance to
the optimum x* is
E(xt+1 - x*)2 = (e>At[x1 - x*,x0 - x*]>)2 + α2Ce>(I - Bt)(I - B)-1 e 1, (10)
where the first and second term correspond to squared bias and variance, and their corresponding
momentum dynamics are captured by operators
1 — αh + μ —μ
10
(1 — αh + μ)2 μ2
B= 10
1 — αh + μ 0
—2μ(1 — αh + μ)
0
-μ
(11)
Even though it is possible to numerically work on (10) directly, we use a scalar, asymptotic surrogate
based in (12) on the spectral radii of operators to simplify analysis and expose insights. This decision
is supported by our findings in Section 2: the spectral radii can capture empirical convergence speed.
2
E(xt+1- x*)2 ≈ρ(A)2t(xo- x*)2 + (1- P(B)t) -- (12)
1 - ρ(B)
One of our design decisions for YellowFin is to always work in the robust region of Lemma 2. We
know that this implies a spectral radius √μ of the momentum operator, A, for the bias. Lemma 5
shows that under the exact same condition, the variance operator B has spectral radius μ.
Lemma 5. The spectral radius ofthe variance operator, B is μ, if (1 — √μ)2 ≤ -h ≤ (1 + √μ)2.
As a result, the surrogate objective of (12), takes the following form in the robust region.
-2C
E(xt+1 - x )2 ≈ μt(xo - X )2 + (1 - μt)-----	(13)
1 — μ
We use this surrogate objective to extract a noisy tuning rule for YellowFin.
4
Under review as a conference paper at ICLR 2018
3.1	Tuning rule
Based on the surrogate in (13), we present YellowFin (Algorithm 1). Let D denote an estimate
of the current model’s distance to a local quadratic approximation’s minimum, and C denote an
estimate for gradient variance. Also, let hmax and hmin denote estimates for the largest and smallest
generalized curvature respectively. The extremal curvatures hmin and hmax are meant to capture both
curvature variation along different directions (like the classic condition number) and also variation
that occurs as the landscape evolves. At each iteration, we solve the following SingleStep problem.
(SingleStep)
μt, at
s.t.
arg min μD2 + α2C
μ
μ ≥
α=
P pPhmax/hmin - 1 ∖
y hhmax/hmin + 1 y∕
(i-√μ)2
hmin
Algorithm 1 YellowFin
function YELLOWFIN(gradient gt , β)
hmax,hmin — CURVATURERANGE(gt, β)
C — VARIANCE(gt, β)
D J DISTANCE(gt, β)
μt, at J SINGLESTEP(C,D,hmaχ,hmin)
return μt, αt
end function
SingleStep minimizes the surrogate for the expected squared distance from the optimum of a
local quadratic approximation (13) after a single step (t =1), while keeping all directions in the
robust region (5). This is the SGD version of the noiseless tuning rule in (8). It can be solved in
closed form; we refer to Appendix D for discussion on the closed form solution. YellowFin uses
functions CURVATURERANGE, VARIANCE and DISTANCE to measure quantities hmax, hmin, C and
D respectively. These measurement functions can be designed in different ways. We present the
implementations we used for our experiments, based completely on gradients, in Section 3.2.
3.2	Measurement functions in YellowFin
This section describes our implementation of the measurement oracles used by YellowFin: Cur-
vatureRange, Variance, and Distance. We design the measurement functions with the
assumption of a negative log-probability objective; this is in line with typical losses in machine
learning, e.g. cross-entropy for neural nets and maximum likelihood estimation in general. Under this
assumption, the Fisher information matrix—i.e. the expected outer product of noisy gradients—equals
the Hessian of the objective (Duchi, 2016; Pascanu and Bengio, 2013). This allows for measurements
purely from minibatch gradients with overhead linear to model dimensionality. These implementa-
tions are not guaranteed to give accurate measurements. Nonetheless, their use in our experiments in
Section 5 shows that they are sufficient for YellowFin to outperform the state of the art on a variety
of objectives. We also refer to Appendix E for implementation details on zero-debias (Kingma and
Ba, 2014), slow start (Schaul et al., 2013) and smoothing for curvature range estimation.
Algorithm 3 Gradient variance Algorithm 4 Distance to opt.
Algorithm 2 Curvature range
state: hmax , hmin , hi , ∀i ∈ {1, 2, 3,...}	state: g2 — 0, g — 0	state: ∣∣gk J 0, h J 0
function CURVATURERANGE(gradient gt , β) ht — ∣∣gt ∣∣2	function VARIANCE(gradient gt , β)	function DISTANCE(gradient gt, β)
hmax,t^-maχ hi, hmin,t^- min hi	g2 J β ∙ g2 + (1 — β) ∙ gt Θ gt	≡J β ∙ ≡ +(1 — β) ∙kgtk
t-w≤i≤t	t-w≤i≤t		h J β ∙ h + (1 — β)∙ ∣gtk2
hmax 士- β ∙ hmax + (I - β) ∙ hmax,t hmin J β ∙ hmin + (I - β) ∙ hmin,t return hmax, hmin	g J β ∙ g + (I — β) ∙ gt return ∣g2 — g2∣ι	D J β ∙ D + (1 - β)∙ M/h return D
end function	end function	end function
Curvature range Let gt be a noisy gradient, we estimate the range of curvatures in Algorithm 2.
We notice that the outer product of gt and gtT has an eigenvalue ht = kgt k2 with eigenvector gt . Thus
under our negative log-likelihood assumption, we use ht to approximate the curvature of Hessian
along gradient direction gt . Specifically, we maintain hmin and hmax as running averages of extreme
curvature hmin,t and hmax,t, from a sliding window of width 20. As gradient directions evolve, we
explore curvatures along different directions. Thus hmin and hmax capture the curvature variations.
Gradient variance To estimate the gradient variance in Algorithm 3, We use running averages
g and g2 to keep track of gt and gt Θ gt, the first and second order moment of the gradient. As
Var(gt) = Eg2 — (Egt)2, we estimate the gradient variance C in (14) using C = ∣∣g2 一 g2∣∣ι.
5
Under review as a conference paper at ICLR 2018
—Without clipping ：
■ — With CliPPing
—Clippingthresh.
	Loss	BLEU4
Default w/o clip.	diverge	
Default w/ clip.	2.86	30.75
YF	2.75	31.59
Table 1: German-English transla-
tion validation performance using
convolutional seq-to-seq learning.
105
103
100
10-3
0k 1k 2k 3k
Iterations
Figure 4: A variation of the LSTM architecture in (Zhu et al.,
2016) exhibits exploding gradients. The proposed adaptive
gradient clipping threshold (blue) stabilizes the training loss.
Distance to optimum In Algorithm 4, we estimate the distance to the optimum of the local
quadratic approximation. Inspired by the fact that kVf (X )k ≤ ∣∣H k∣∣x - X ?k for a quadratic f (x)
with Hessian H and minimizer X*,we first maintain h and ∣∣g∣ as running averages of curvature ht
and gradient norm ∣∣gt∣. Then the distance is approximated using ∣∣g∣∕h.
3.3	Stability on non-smooth objectives
Neural network objectives can involve arbitrary non-linearities, and large Lipschitz constants (Szegedy
et al., 2013). Furthermore, the process of training them is inherently non-stationary, with the landscape
abruptly switching from flat to steep areas. In particular, the objective functions of RNNs with hidden
units can exhibit occasional but very steep slopes (Pascanu et al., 2013). To deal with this issue, we
use adaptive gradient clipping heuristics as a very natural addition to our basic tuner. It is discussed
with extensive details in Appendix F. In Figure 4, we present an example of an LSTM that exhibits
the ’exploding gradient’ issue. The proposed adaptive clipping can stabilize the training process
using YellowFin and prevent large catastrophic loss spikes.
We validate the proposed adaptive clipping on the convolutional sequence to sequence learning model
(Gehring et al., 2017) for IWSLT 2014 German-English translation. The default optimizer (Gehring
et al., 2017) uses learning rate 0.25 and Nesterov’s momentum 0.99, diverging to loss overflow due
to ’exploding gradient’. It requires, as in Gehring et al. (2017), strict manually set gradient norm
threshold 0.1 to stabilize. In Table 1, we can see YellowFin, with adaptive clipping, outperforms the
default optimizer using manually set clipping, with 0.84 higher validation BLEU4 after 120 epochs.
4	Closed-loop YellowFin
Asynchrony is a parallelization technique that avoids synchronization barriers (Niu et al., 2011).
It yields better hardware efficiency, i.e. faster steps, but can increase the number of iterations to a
given metric, i.e. statistical efficiency, as a tradeoff (Zhang and R6, 2014). Mitliagkas et al. (2016)
interpret asynchrony as added momentum dynamics. We design closed-loop YellowFin, a variant
of YellowFin to automatically control algorithmic momentum, compensate for asynchrony and
accelerate convergence. We use the formula in (14) to model the dynamics in the system, where the
total momentum, μτ, includes both asynchrony-induced and algorithmic momentum, μ, in (1).
E[xt+1 - xt] = μτE[xt - xt-1 ] - αEVf (xt)	(14)
We first use (14) to design an robust estimator μτ for the value of total momentum at every iteration.
Then we use a simple negative feedback control loop to adjust the value of algorithmic momentum
so that μτ matches the target momentum decided by YELLOWFIN in Algorithm 1. In Figure 5, we
demonstrate momentum dynamics in an asynchronous training system. As directly using the target
value as algorithmic momentum, YellowFin (middle) presents total momentum μτ strictly larger
than the target momentum, due to asynchrony-induced momentum. Closed-loop YellowFin (right)
automatically brings down algorithmic momentum, match measured total momentum μτ to target
value and, as we will see, significantly speeds up convergence comparing to YellowFin. We refer
to Appendix G for details on estimator μτ and Closed-loop YellowFin in Algorithm 5.
5	Experiments
In this section, we empirically validate the importance of momentum tuning and evaluate YellowFin
in both synchronous (single-node) and asynchronous settings. In synchronous settings, we first
demonstrate that, with hand-tuning, momentum SGD is competitive with Adam, a state-of-the-art
6
Under review as a conference paper at ICLR 2018
Figure 5:	When running YellowFin, total momentum μt equals algorithmic value in synchronous
settings (left); μt is greater than algorithmic value on 16 asynchronous workers (middle). Closed-loop
YellowFin automatically lowers algorithmic momentum and brings total momentum to match the
target value (right). Red dots are measured μt at every step with red line as its running average.
adaptive method. Then, we evaluate YellowFin without any hand tuning in comparison to hand-
tuned Adam and momentum SGD. In asynchronous settings, we show that closed-loop YellowFin
accelerates with momentum closed-loop control, performing significantly better than Adam.
We evaluate on convolutional neural networks (CNN) and recurrent neural networks (RNN). For CNN,
we train ResNet (He et al., 2016) for image recognition on CIFAR10 and CIFAR100 (Krizhevsky et al.,
2014). For RNN, we train LSTMs for character-level language modeling with the TinyShakespeare
(TS) dataset (Karpathy et al., 2015), word-level language modeling with the Penn TreeBank (PTB)
(Marcus et al., 1993), and constituency parsing on the Wall Street Journal (WSJ) dataset (Choe and
Charniak). We refer to Table 3 in Appendix H for model specifications. To eliminate influences of
a specific random seed, in our synchronous and asynchronous experiments, the training loss and
validation metrics are averaged from 3 runs using different random seeds.
5.1 Synchronous experiments
We tune Adam and momentum SGD on learning rate grids with prescribed momentum 0.9 for SGD.
We fix the parameters of Algorithm 1 in all experiments, i.e. YellowFin runs without any hand
tuning. We provide full specifications, including the learning rate (grid) and the number of iterations
we train on each model in Appendix I. For visualization purposes, we smooth training losses with
a uniform window of width 1000. For Adam and momentum SGD on each model, we pick the
configuration achieving the lowest averaged smoothed loss. To compare two algorithms, we record
the lowest smoothed loss achieved by both. Then the speedup is reported as the ratio of iterations to
achieve this loss. We use this setup to validate our claims.
Momentum SGD is competitive with adaptive methods
In Table 2, we compare tuned momentum SGD and tuned
Adam on ResNets with training losses shown in Figure 9 in
Appendix J. We can observe that momentum SGD achieves
1.71x and 1.87x speedup to tuned Adam on CIFAR10 and
CIFAR100 respectively. In Figure 6 and Table 2, with the
exception of PTB LSTM, momentum SGD also produces
better training loss, as well as better validation perplexity
in language modeling and validation F1 in parsing. For the
parsing task, we also compare with tuned Vanilla SGD and
Table 2: Speedup of YellowFin and
tuned mom. SGD over tuned Adam.
		Adam mom.SGD		YF
CIFAR10	1x	1.71x	1.93x
CIFAR100	1x	1.87x	1.38x
PTB	1x	0.88x	0.77x
TS	1x	2.49x	3.28x
WSJ	1x	1.33x	2.33x
AdaGrad, which are used in the NLP community. Figure 6 (right) shows that fixed momentum 0.9
can already speedup Vanilla SGD by 2.73x, achieving observably better validation F1. We refer to
Appendix J.2 for further discussion on the importance of momentum adaptivity in YellowFin.
YELLOWFIN can match hand-tuned momentum SGD and can outperform hand-tuned Adam
In our experiments, YellowFin, without any hand-tuning, yields training loss matching hand-tuned
momentum SGD for all the ResNet and LSTM models in Figure 6 and 9. When comparing to tuned
Adam in Table 2, except being slightly slower on PTB LSTM, YellowFin achieves 1.38x to 3.28x
speedups in training losses on the other four models. More importantly, YellowFin consistently
shows better validation metrics than tuned Adam in Figure 6. It demonstrates that YellowFin
can match tuned momentum SGD and outperform tuned state-of-the-art adaptive optimizers. In
Appendix J.4, we show YellowFin further speeding up with finer-grain manual learning rate tuning.
7
Under review as a conference paper at ICLR 2018
SSOIωUIUIt0JI
■ Momentum SGD
Adam
0-0 YellOwFin
Momentum SGD
▼ Adam
0-0 YellowFin
SSOlωUIU-sJI
5k 10k	15k	20k
Iterations
SSOIωτπτπt0JI
0k 30k	60k	90k	120k
Iterations
0k 5k 10k	15k 20k 25k 30k
AEXgdJed Uo-EPIp3>
■ Momentum SGD
Adam
0-0 YellOWFin
0	10	20	30	40	50
omentum SG
Adam
Q-O YellowFin
A-A AdaGrad
h□ Vanilla SGD
5k 10k	15k	20k
Iterations
Iterations
Figure 6:	Training loss and test metrics on word-level language modeling with PTB (left), character-
level language modeling with TS (middle) and constituency parsing on WSJ (right). Note the
validation metrics are monotonic as we report the best values up to each specific number of iterations.
5.2 Asynchronous experiments
In this section, we evaluate closed-loop YellowFin with focus
on the number of iterations to reach a certain solution. To that end,
we run 16 asynchronous workers on a single machine and force
them to update the model in a round-robin fashion, i.e. the gradient
is delayed for 15 iterations. Figure 7 presents training losses on
the CIFAR100 ResNet, using YellowFin in Algorithm 1, closed-
loop YellowFin in Algorithm 5 and Adam with the learning rate
achieving the best smoothed loss in Section 5.1. We can observe
closed-loop YellowFin achieves 20.1x speedup to YellowFin,
and consequently a 2.69x speedup to Adam. This demonstrates
Figure 7: Asynchronous perfor-
mance on CIFAR100 ResNet.
that (1) closed-loop YellowFin accelerates by reducing algorithmic momentum to compensate for
asynchrony and (2) can converge in less iterations than Adam in asynchronous-parallel training.
6	Related work
Many techniques have been proposed on tuning hyperparameters for optimizers. General hyper-
parameter tuning approaches, such as random search (Bergstra and Bengio, 2012) and Bayesian
approaches (Snoek et al., 2012; Hutter et al., 2011), directly applies to optimizer tuning. As another
trend, adaptive methods, including AdaGrad (Duchi et al., 2011), RMSProp (Tieleman and Hinton,
2012) and Adam (Chilimbi et al., 2014), uses per-dimension learning rate. Schaul et al. (2013)
use a noisy quadratic model similar to ours to extract learning rate tuning rule in Vanilla SGD.
However they do not use momentum which is essential in training modern neural networks. Existing
adaptive momentum approach either consider the deterministic setting (Graepel and Schraudolph,
2002; Rehman and Nawi, 2011; Hameed et al., 2016; Swanston et al., 1994; Ampazis and Perantonis,
2000; Qiu et al., 1992) or only analyze stochasticity with O(1/t) learning rate (Leen and Orr, 1994).
In the contrast, we aim at practical momentum adaptivity for stochastically training neural networks.
7	Discussion
We presented YellowFin, the first optimization method that automatically tunes momentum as
well as the learning rate of momentum SGD. YellowFin outperforms the state-of-the-art adaptive
optimizers on a large class of models both in synchronous and asynchronous settings. It estimates
statistics purely from the gradients of a running system, and then tunes the hyperparameters of
momentum SGD based on noisy, local quadratic approximations. As future work, we believe that
more accurate curvature estimation methods, like the bbprop method (Martens et al., 2012) can
further improve YellowFin. We also believe that our closed-loop momentum control mechanism in
Section 4 could accelerate convergence for other adaptive methods in asynchronous-parallel settings.
8
Under review as a conference paper at ICLR 2018
References
Boris T Polyak. Some methods of speeding up the convergence of iteration methods. USSR
Computational Mathematics and Mathematical Physics, 4(5):1-17,1964.
Yurii Nesterov. A method of solving a convex programming problem with convergence rate o (1/k2).
In Soviet Mathematics Doklady, volume 27, pages 372-376, 1983.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization
and momentum in deep learning. In Proceedings of the 30th international conference on machine
learning (ICML-13), pages 1139-1147, 2013.
Yoshua Bengio. Practical recommendations for gradient-based training of deep architectures. In
Neural networks: Tricks of the trade, pages 437-478. Springer, 2012.
Genevieve B Orr and KlaUs-Robert Muller. Neural networks: tricks of the trade. Springer, 2003.
Yoshua Bengio et al. Deep learning of representations for unsupervised and transfer learning. ICML
Unsupervised and Transfer Learning, 27:17-36, 2012.
LConBottou. Stochastic gradient descent tricks. In Neural networks: Tricks of the trade, pages
421-436. Springer, 2012.
James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of
Machine Learning Research, 13(Feb):281-305, 2012.
Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine
learning algorithms. In Advances in neural information processing systems, pages 2951-2959,
2012.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2), 2012.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Danqi Chen, Jason Bolton, and Christopher D Manning. A thorough examination of the cnn/daily
mail reading comprehension task. arXiv preprint arXiv:1606.02858, 2016.
Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional
sequence to sequence learning. arXiv preprint arXiv:1705.03122, 2017.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. arXiv preprint arXiv:1705.08292, 2017.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural
networks. In International Conference on Machine Learning, pages 1310-1318, 2013.
Ioannis Mitliagkas, Ce Zhang, Stefan Hadjis, and Christopher RC. Asynchrony begets momentum,
with an application to deep learning. arXiv preprint arXiv:1605.09774, 2016.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2013.
Laurent Lessard, Benjamin Recht, and Andrew Packard. Analysis and design of optimization
algorithms via integral quadratic constraints. SIAM Journal on Optimization, 26(1):57-95, 2016.
John Duchi. Fisher information., 2016. URL https://web.stanford.edu/class/
stats311/Lectures/lec- 09.pdf.
Razvan Pascanu and Yoshua Bengio. Revisiting natural gradient for deep networks. arXiv preprint
arXiv:1301.3584, 2013.
9
Under review as a conference paper at ICLR 2018
Tom SchaUL Sixin Zhang, and Yann LeCun. No more pesky learning rates. ICML (3), 28:343-351,
2013.
Chenzhuo Zhu, Song Han, Huizi Mao, and William J Dally. Trained ternary quantization. arXiv
preprint arXiv:1612.01064, 2016.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Feng Niu, Benjamin Recht, Christopher Re, and Stephen Wright. Hogwild: A lock-free approach to
parallelizing stochastic gradient descent. In Advances in Neural Information Processing Systems,
pages 693-701, 2011.
Ce Zhang and Christopher R6. Dimmwitted: A study of main-memory statistical analytics. PVLDB,
7(12):1283-1294, 2014. URL http://www.vldb.org/pvldb/vol7/p1283-zhang.
pdf.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pages 770-778, 2016.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. The cifar-10 dataset, 2014.
Andrej Karpathy, Justin Johnson, and Li Fei-Fei. Visualizing and understanding recurrent networks.
arXiv preprint arXiv:1506.02078, 2015.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated
corpus of english: The penn treebank. Computational linguistics, 19(2):313-330, 1993.
Do Kook Choe and Eugene Charniak. Parsing as language modeling.
Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Sequential model-based optimization for
general algorithm configuration. LION, 5:507-523, 2011.
Trishul M Chilimbi, Yutaka Suzue, Johnson Apacible, and Karthik Kalyanaraman. Project adam:
Building an efficient and scalable deep learning training system. In OSDI, volume 14, pages
571-582, 2014.
Thore Graepel and Nicol N Schraudolph. Stable adaptive momentum for rapid online learning in
nonlinear systems. In International Conference on Artificial Neural Networks, pages 450-455.
Springer, 2002.
Mohammad Zubair Rehman and Nazri Mohd Nawi. The effect of adaptive momentum in improving
the accuracy of gradient descent back propagation algorithm on classification problems. In Inter-
national Conference on Software Engineering and Computer Systems, pages 380-390. Springer,
2011.
Alaa Ali Hameed, Bekir Karlik, and Mohammad Shukri Salman. Back-propagation algorithm with
variable adaptive momentum. Knowledge-Based Systems, 114:79-87, 2016.
DJ Swanston, JM Bishop, and Richard James Mitchell. Simple adaptive momentum: new algorithm
for training multilayer perceptrons. Electronics Letters, 30(18):1498-1500, 1994.
Nikolaos Ampazis and Stavros J Perantonis. Levenberg-marquardt algorithm with adaptive momen-
tum for the efficient training of feedforward networks. In Neural Networks, 2000. IJCNN 2000,
Proceedings of the IEEE-INNS-ENNS International Joint Conference on, volume 1, pages 126-131.
IEEE, 2000.
G Qiu, MR Varley, and TJ Terrell. Accelerated training of backpropagation networks by using
adaptive momentum step. Electronics letters, 28(4):377-379, 1992.
Todd K Leen and Genevieve B Orr. Optimal stochastic search and adaptive momentum. In Advances
in neural information processing systems, pages 477-484, 1994.
10
Under review as a conference paper at ICLR 2018
James Martens, Ilya Sutskever, and Kevin Swersky. Estimating the hessian by back-propagating
curvature. arXiv preprint arXiv:1206.6464, 2012.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http:
//www.deeplearningbook.org.
Stefan Hadjis, Ce Zhang, Ioannis Mitliagkas, Dan Iter, and Christopher R6. Omnivore: An optimizer
for multi-device deep learning on cpus and gpus. arXiv preprint arXiv:1606.04487, 2016.
Saining Xie, Ross Girshick, Piotr Doll念 ZhUOWen Tu, and Kaiming He. Aggregated residual
transformations for deep neural networks. arXiv preprint arXiv:1611.05431, 2016.
Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint
arXiv:1608.05859, 2016.
11
Under review as a conference paper at ICLR 2018
A Proof of Lemma 2
To prove Lemma 2, we first prove a more generalized version in Lemma 6. By restricting f to be a one
dimensional quadratics function, the generalized curvature ht itself is the only eigenvalue. We can
prove Lemma 2 as a straight-forward corollary. Lemma 6 also implies, in the multiple dimensional
correspondence of (4), the spectral radius P(At) = √μ if the curvature on all eigenvector directions
(eigenvalue) satisfies (5).
Lemma 6. Let the gradients of a function f be described by
▽f (Xt) = H(Xt)(Xt- X*), (15)
with H(xt) ∈ Rn 7→ Rn×n. Then the momentum update can be expressed as a linear operator:
(yy+ι) = (I-αHIxt) + μI -μ1 Xyy-I) = Aet)	(16)
where yt，Xt — X*. Now, assume that thefollowing condition holdsfor all eigenvalues λ(H(Xt))
of H(xt):
(1 - √μ)2 ≤ λ(H(Xt)) ≤ (1 + √μ)2 . (17)
αα
then the spectral radius of At is controlled by momentum with P(At) = √μ.
Proof. Let λt be an eigenvalue of matrix At, it gives det (At - λtI) = 0. We define the blocks in
At as C = I - αHt + μI - %I, D = -μI, E = I and F = -%I which gives
det(At - λtI) = det F det (C - DFTE) = 0
assuming generally F is invertible. Note we use Ht , H(Xt) for simplicity in writing. The equation
det (C - DFTE) = 0 implies that
det (λ2I — λtMt + μI) = 0 (18)
with Mt = (I - αHt + μI). In other words, λt satisfied that λ2 - λtλ(Mt) + μ = 0 with λ(Mt)
being one eigenvalue of Mt. I.e.
λt
λ(Mt) ± ,λ(Mt)2 - 4μ
2
(19)
On the other hand, (17) guarantees that (1 - αλ(Ht) + μ)2 ≤ 4μ. We know both Ht and I - αHt +
μI are symmetric. Thus for all eigenvalues λ(M t) of Mt, we have λ(M t)2 = (1-αλ(Ht)+μ)2 ≤
4μ which guarantees ∣λt∣ = √μ for all λt. As the spectral radius is equal to the magnitude of the
largest eigenvalue of At, we have the spectral radius of At being √μ.
□
B	Proof of Lemma 4
We first prove Lemma 7 and Lemma 8 as preparation for the proof of Lemma 4. After the proof for
one dimensional case, we discuss the trivial generalization to multiple dimensional case.
Lemma 7. Let the h be the curvature of a one dimensional quadratic function f and Xt = Ext. We
assume, without loss of generality, the optimum point of f is x? =0. Then we have the following
recurrence
C⅛1)=(1-T+μ -μ)'(xι)	(20)
Proof. From the recurrence of momentum SGD, we have
Ext+1 =E[xt - aVfst (xt) + μ(xt - xt-1)]
=Eχt[xt - αEst VfSt (xt) + μ(xt - xt-ι)]
=Ext [xt - αhxt + μ(xt - xt-1)]
=(1 — αh + μ)Xt — μxt-ι
12
UlIderreVieW as a COnferenCe PaPer at ICLR 2018
By PUttmg the equat5'IImtO matriXfOnn" (20) is a Straight—forward result from UlIr-Hng the
recurrence for E times∙ NOte as We set1 U。With no UIlCertaiIlty in momentum SGDy we have
可。7」HIL □
Lemma∙LsUtU IE(V Ihi-)2 and K HH( V Ihi-)(P——1 — M——1) With being -he expectation
OfX 可 Fbr quadratic function j() Wiih crvalre h m I⅛ have IhefolIoWing recurrence
'F 二 325
¾17ibt)(7ib)二二 (21)

((1 — Rh + 七)2 ∕χ2 —2七(1 — Rh + 七)
1 O O
1 — ah + 七 O —
(22)
IQH E(VjS-(P)I Vj(P))2 i:heVarianCe Ofgradienl On minibalch Sr
Proof We ProVe by HrSt deriving the recurrence for Ut and K respectively and COmbis∙ng them in to
a matriXfOnn∙ For UtNe have
s+1 U(v+l — v+l)2
UlE(V — QVjSe(V) +p — P——1) — (1 — αh + + 舄 t——2
口岸与 — αVj (岁)+岁—岁——一) — (1 — αh+G + KG——1 +Wf(Xt- I Vjse(与)))2
UlE((I — αh ÷(P I Hf)——p——l —t1))2 ÷ α2( Vj(P) — Vjse(V)) 2
U(I — ah + 七)2(K —-)2 — 2七(1 — ah + ʌi)(K —-)(-——1 —-——1)
+ ∕Z2(P——1 — p——l)2 +
(23)
Where the CrOSS terms CanCeIS due to the factSe -Vj(P) I WfSt (P)- UOin -he third equality.
For We CaIl similarIy derive
K UlE(V —-)(p——l — 刻——1)
UlE((I — ah+(P——1 — M——1) —p——2 it2)+Vj(P) — Vj(P)))(P——1 — 刻——1)
U(Ilah +IE(V——1 —τl)2 — 七IE(P——1 — 刻——1)(P——2 —t2)
(24)
Agaithe term involving Vj(V) I VjSe(V) CanCeISm the third equality as a results Of
SJVj(V) IVjSe(岁H 0∙ (23) and (24) CaIl be jointly expressed in the following matriX
form
(U阜 1∖ (U;c∕ :c∕ 'U二
E" B≡+ΞiM二二 +⅛11m g，二.
(25)
NOte the SeCOIld term in the SeCOIld equality is ZerO because 七。and 21 are deterministic, ThUS
UlUuOUF□
According to Lemma 7 and-We have 犀豹—H*)2 U (e-4-Hla-τ)2 ^nd IE(Plhi-)2 U
α2c(7—to。(7—to) L 1 Wherel m用如 has1 Zero entries but the HrSt dimensiop COmbining
these two teπnwe prove Lemma 4∙ ThOUgh the proof here is for OlledimeIISIIaI quadraticit
trivialIy generalizes to multiple dimensional quadratics，SPeCifiCIw We Can decomposee quadratics
along the eigenvector directionand then apply Lemma 4 to each eigenvector direction USing the
COrreSPOnding CUrVatUre h (eigenvalueBy SUmming quantities in (IO) for all eigenvector direction
We Can achieve the multiple dimensional COrreSPOlIdenCe Of (IO
13
Under review as a conference paper at ICLR 2018
C Proof of Lemma 5
Again we first present a proof of a multiple dimensional generalized version of Lemma 5. The proof
of Lemma 5 is a one dimensional special case of Lemma 9. Lemma 9 also implies that for multiple
dimension quadratics, the corresponding spectral radius P(B) = μ if (1-了 ≤ h ≤ (1+α√μ) on
all the eigenvector directions with h being the eigenvalue (curvature).
Lemma 9. Let H ∈ Rn×n be a symmetric matrix and ρ(B) be the spectral radius of matrix
((I - aH + μI)>(I - aH + μI) μ2I -2μ(I - aH + μI八
B = I00(26)
∖ I - aH + μI0 -μI )
We have P(B) = μ if all eigenvalues λ(H) of H satisfies
(1 -√μ)2	, 、	(1 + √μ)2
ʌ——≤λ- ≤ λ(H) ≤ -——^-.	(27)
αα
Proof. Let λ be an eigenvalue of matrix B, it gives det (B - λI)=0which can be alternatively
expressed as
det (B — λI) = det F det (C — DFTE) = 0 (28)
assuming F is invertible, i.e. λ + μ = 0, where the blocks in B
C= ✓ M>M -λI -λ∣ ) , D = ✓ -2μM ) , E =✓ M ) , F = -μI -λI
with M = I - aH + μI.(28) can be transformed using straight-forward algebra as
det ✓ (λ - μ)M>M - (λ + μ)λI
<	(λ + μ)I
(λ + μ)μ2I ) = o
-(λ + μ)λI J
(29)
Using similar simplification technique as in (28), we can further simplify into
(λ - μ) det ((λ + μ)2I - λM>M)= 0 (30)
if λ = μ, as (λ + μ)2I - λM>M is diagonalizable, we have (λ + μ)2 - λλ(M)2 = 0 with
λ(M) being an eigenvalue of symmetric M. The analytic solution to the equation can be explicitly
expressed as
λ(M)2 - 2μ ± P(λ(M)2- 2μ)2- 4μ2
λ —	.	(31)
When the condition in (27) holds, we have λ(M)2 = (1 - αλ(H) + μ)2 ≤ 4μ. One can verify that
(λ(M)2 - 2μ)2 - 4μ2 = (λ(M)2 - 4μ)λ(M)2
=((1 - αρ(H) + μ)2 - 4μ) λ(M)2	(32)
≤0
Thus the roots in (31) are conjugate with ∣λ∣ = μ. In conclusion, the condition in (27) can guarantee
all the eigenvalues of B has magnitude μ. Thus the spectral radius of B is controlled by μ.	□
D	Analytical solution to (14)
The problem in (14) does not need iterative solver but has an analytical solution. Substituting only the
second constraint, the objective becomes p(x) = x2D2 + (1 -/)4/月言由。with X = √μ ∈ [0,1). By
setting the gradient of p(χ) to 0, we can get a cubic equation whose root X = √μp can be computed
in closed form using Vieta’s substitution. As p(x) is uni-modal in [0, 1), the optimizer for (14) is
exactly the maximum of μ? and (yJhmax/hmin - 1)2/( VZhmax/hmin + 1)2, the right hand-side of
the first constraint in (14).
14
Under review as a conference paper at ICLR 2018
SSOl MUIUIE』H
--- YF with clipping
—YF without clipping
— YF With clipping
— YF without clipping
SSoImu's's.!H
Iterations
Iterations
Figure 8: Training losses on PTB LSTM (left) and CIFAR10 ResNet (right) for YellowFin with and
without adaptive clipping.
E Practical implementation
In Section 3.2, we discuss estimators for learning rate and momentum tuning in YellowFin. In our
experiment practice, we have identified a few practical implementation details which are important
for improving estimators. Zero-debias is proposed by Kingma and Ba (2014), which accelerates
the process where exponential average adapts to the level of original quantity in the beginning. We
applied zero-debias to all the exponential average quantities involved in our estimators. In some
LSTM models, we observe that our estimated curvature may decrease quickly along the optimization
process. In order to better estimate extremal curvature hmax and hmin with fast decreasing trend, we
apply zero-debias exponential average on the logarithmic of hmax,t and hmin,t, instead of directly
on hmax,t and hmin,t. Except from the above two techniques, we also implemented the slow start
heuristic proposed by (SchaUl et al., 2013). More specifically, We use a = min{αt, t ∙ αt∕(10 ∙ w)}
as our learning rate with w as the size of our sliding window in hmax and hmin estimation. It discount
the learning rate in the first 10 ∙ W steps and helps to keep the learning rate small in the beginning
When the exponential averaged quantities are not accurate enough.
F	Adaptive gradient clipping in YellowFin
Gradient clipping has been established in literature as a standard—almost necessary—tool for training
such objectives (Pascanu et al., 2013; GoodfelloW et al., 2016; Gehring et al., 2017). HoWever, the
classic tradeoff betWeen adaptivity and stability applies: setting a clipping threshold that is too loW
can hurt performance; setting it to be high, can compromise stability. YellowFin, keeps running
estimates of extremal gradient magnitude squares, hmax and hmin in order to estimate a generalized
condition number. We posit that √hmx is an ideal gradient norm threshold for adaptive clipping.
In order to ensure robustness to extreme gradient spikes, like the ones in Figure 4, We also limit the
groWth rate of the envelope hmax in Algorithm 2 as folloWs:
hmax《-β ∙ hmax + (I - β) ∙ min {hmax,t, 100 ∙ hmax}	(33)
Our heuristics folloWs along the lines of classic recipes like Pascanu et al. (2013). HoWever, instead
of using the average gradient norm to clip, it uses a running estimate of the maximum norm hmax .
In Section 3.3, We saW that adaptive clipping stabilizes the training on objectives that exhibit exploding
gradients. In Figure 8, We demonstrate that the adaptive clipping does not hurt performance on models
that do not exhibit instabilities Without clipping. Specifically, for both PTB LSTM and CIFAR10
ResNet, the difference betWeen YellowFin With and Without adaptive clipping diminishes quickly.
G Closed-loop YellowFin for asynchronous training
In Section 4, We briefly discuss the closed-loop momentum control mechanism in closed-loop
YellowFin. In this section, after presenting more preliminaries on asynchrony, We shoW With
15
Under review as a conference paper at ICLR 2018
details on the mechanism: it measures the dynamics on a running system and controls momentum
with a negative feedback loop.
Preliminaries Asynchrony is a popular parallelization technique (Niu et al., 2011) that avoids
synchronization barriers. When training on M asynchronous workers, staleness (the number of model
updates between a worker’s read and write operations) is on average τ = M - 1, i.e., the gradient
in the SGD update is delayed by T iterations as Vfst-τ (χt-τ). Asynchrony yields faster steps, but
can increase the number of iterations to achieve the same solution, a tradeoff between hardware and
statistical efficiency (Zhang and R6, 2014). MitliagkaS et al. (2016) interpret asynchrony as added
momentum dynamics. Experiments in Hadjis et al. (2016) support this finding, and demonstrate
that reducing algorithmic momentum can compensate for asynchrony-induced momentum and
significantly reduce the number of iterations for convergence. Motivated by that result, we use the
model in (34), where the total momentum, μτ, includes both asynchrony-induced and algorithmic
momentum, μ, in (1).
E[xt+1 - xt] = μτE[xt - xt-ι] - αEVf (xt)	(34)
We will use this expression to design an estimator for the value of total momentum, μτ. This estimator
is a basic building block of closed-loop YellowFin, that removes the need to manually compensate
for the effects of asynchrony.
Measuring the momentum dynamics Closed-loop YELLOWFIN estimates total momentum μτ
on a running system and uses a negative feedback loop to adjust algorithmic momentum accordingly.
Equation (14) gives an estimate of μτ on a system with staleness T, based on (14).
xt-τ - xt-τ -1 + αVSt-τ-1 f(xt-τ-1)
μτ = median -------------------------------------- (35)
xt-τ -1 - xt-τ -2
We use T -stale model values to match the staleness of the gradient, and perform all operations in
an elementwise fashion. This way we get a total momentum measurement from each variable; the
median combines them into a more robust estimate.
Closing the asynchrony loop Given a reliable measurement of μτ, we can use it to adjust the
value of algorithmic momentum so that the total momentum matches the target momentum as decided
by YellowFin in Algorithm 1. Closed-loop YellowFin in Algorithm 5 uses a simple negative
feedback loop to achieve the adjustment.
Algorithm 5 Closed-loop YELLOWFIN
1 2 3 4 5 6 7	Input: μ — 0, α — 0.0001, Y - 0.01, T (staleness) for t — 1 to T do Xt — Xt-1 + μ(xt-i - Xt-2)- αVstf(xt-τ-i) μ*, α - YELLOWFIN(VStf(Xt-τ-i), β) μτ - median (xt-τ xt-X-1+：^：：-T-2f(xt-T-I))	. Measuring total momentum μ J μ + γ ∙ (μ* - μτ)	. Closing the loop : end for
H Model specification
The model specification is shown in Table 3 for all the experiments in Section 5. CIRAR10 ResNet
uses the regular ResNet units while CIFAR100 ResNet uses the bottleneck units. Only the convolu-
tional layers are shown with filter size, filter number as well as the repeating count of the units. The
layer counting for ResNets also includes batch normalization and Relu layers. The LSTM models are
also diversified for different tasks with different vocabulary sizes, word embedding dimensions and
number of layers.
I Specification for synchronous experiments
In Section 5.1, we demonstrate the synchronous experiments with extensive discussions. For the
reproducibility, we provide here the specification of learning rate grids. The number of iterations as
16
Under review as a conference paper at ICLR 2018
Table 3: Specification of ResNet and LSTM model architectures.
network # layers	Conv 0	Unit 1s	Unit 2s	Unit 3s
CIFAR10ReSNet	110 Γ 3 × 3 4 1 3 × 3, 4 × 6 3 × 3,	8 × 6 3 × 3,	16 × 6
CIFAR10 ResNet 110 3 × 3, 4 × 6 × 6 × 6
3 × 3, 4 3 × 3,	8 3 × 3,	16
CIFAR100 ResNet	164	[3 × 3,	41	-1 × 1,		16 ' 16	×6	■ 1 × 1,		32 ' 32	×6	■ 1 × 1, 3 × 3,	64 ' 64	×6
				3	× 3,			3×	3,					
				1	× 1,	64		1×	1,	128		1 × 1,	256	
network	# layers	Word Embed.	Layer 1	Layer 2	Layer 3
TS LSTM	2	[65 vocab, 128 dim]	128 hidden units	128 hidden units	-
PTB LSTM	2	[10000 vocab, 200 dim]	200 hidden units	200 hidden units	-
WSJ LSTM	3	[6922 vocab, 500 dim]	500 hidden units	500 hidden units	500 hidden units
well aS epochS, i.e. the number of paSSeS over the full training SetS, are alSo liSted for completeneSS.
For YellowFin in all the experimentS in Section 5, we uniformly uSe Sliding window Size 20 for
extremal curvature eStimation and β = 0.999 for Smoothing. For momentum SGD and Adam, we
uSe the following configurationS.
• CIFAR10 ReSNet
-	40k iterations (〜114 epochs)
-	Momentum SGD learning rates {0.001,0.01(best), 0.1,1.0}, momentum 0.9
-	Adam learning rates {0.0001, 0.001(best), 0.01, 0.1}
• CIFAR100 ResNet
-	120k iterations (〜341 epochs)
-	Momentum SGD learning rates {0.001, 0.01(best), 0.1, 1.0}, momentum 0.9
-	Adam learning rates {0.00001, 0.0001(best), 0.001, 0.01}
• PTB LSTM
-	30k iterations (〜13 epochs)
-	Momentum SGD learning rates {0.01, 0.1, 1.0(best), 10.0}, momentum 0.9
-	Adam learning rates {0.0001, 0.001(best), 0.01, 0.1}
• TS LSTM
-	〜21k iterations (50 epochs)
-	Momentum SGD learning rates {0.05, 0.1, 0.5, 1.0(best), 5.0}, momentum 0.9
-	Adam learning rates {0.0005, 0.001, 0.005(best), 0.01, 0.05}
-	Decrease learning rate by factor 0.97 every epoch for all optimizers, following the
design by Karpathy et al. (2015).
• WSJ LSTM
-	〜120k iterations (50 epochs)
-	Momentum SGD learning rates {0.05, 0.1, 0.5(best), 1.0, 5.0}, momentum 0.9
-	Adam learning rates {0.0001, 0.0005, 0.001(best), 0.005, 0.01}
-	Vanilla SGD learning rates {0.05, 0.1, 0.5, 1.0(best), 5.0}
-	Adagrad learning rates {0.05, 0.1, 0.5(best), 1.0, 5.0}
-	Decrease learning rate by factor 0.9 every epochs after 14 epochs for all optimizers,
following the design by Choe and Charniak.
J Additional experiment results
J.1 Training losses on CIFAR10 and CIFAR100 ResNet
In Figure 9, we demonstrate the training loss on CIFAR10 ResNet and CIFAR100 ResNet. Specifi-
cally, YellowFin can match the performance of hand-tuned momentum SGD, and achieves 1.93x
and 1.38x speedup comparing to hand-tuned Adam respectively on CIFAR10 and CIFAR100 ResNet.
17
Under review as a conference paper at ICLR 2018
1 O
-
W
W
SSOIb°uμl-3告
10k	20k	30k	40k
Iterations
Figure 9: Training loss for ResNet on 100-layer CIFAR10 ResNet (left) and 164-layer CIFAR100
bottleneck ResNet.
^.5--------------lok
1
SSoluμIP2』
0-0 YelloWFin
YFmom.=0.0 天
YFmom.=0.9
5k 10k	15k	20k
Iterations
Figure 10: Training loss comparison between YellowFin with adaptive momentum and Yel-
lowFin with fixed momentum value.
J.2 Importance of momentum adaptivity
To further emphasize the importance of momentum adaptivity in YellowFin, we run YF on
CIFAR100 ResNet and TS LSTM. In the experiments, YellowFin tunes the learning rate. Instead
of also using the momentum tuned by YF, we continuously feed prescribed momentum value 0.0
and 0.9 to the underlying momentum SGD optimizer which YF is tuning. In Figure 10, when
comparing to YellowFin with prescribed momentum 0.0 or 0.9, YellowFin with adaptively
tuned momentum achieves observably faster convergence on both TS LSTM and CIFAR100 ResNet.
It empirically demonstrates the essential role of momentum adaptivity in YellowFin.
J.3 Tuning momentum can improve Adam in asynchronous-parallel setting
We conduct experiments on PTB LSTM with 16 asynchronous
workers using Adam using the same protocol as in Section 5.2.
Fixing the learning rate to the value achieving the lowest
smoothed loss in Section 5.1, we sweep the smoothing parameter
β1 (Kingma and Ba, 2014) of the first order moment estimate
in grid {-0.2, 0.0, 0.3, 0.5, 0.7, 0.9}. β1 serves the same role
as momentum in SGD and we call it the momentum in Adam.
Figure 11 shows tuning momentum for Adam under asynchrony
gives measurably better training loss. This result emphasizes
the importance of momentum tuning in asynchronous settings
and suggests that state-of-the-art adaptive methods can perform
sub-optimally when using prescribed momentum.
Ss-Muiuibai
0k 5k 10k	15k	20k	25k	30k
Iterations
Figure 11: Hand-tuning Adam's
momentum under asynchrony.
18
Under review as a conference paper at ICLR 2018
Figure 12: Validation perplexity on Tied LSTM and validation accuracy on ResNext. Learning
rate fine-tuning using grid-searched factor can further improve the performance of YellowFin
in Algorithm 1. YellowFin with learning factor search can outperform hand-tuned Adam on
validation metrics on both models.
AORmOOF UoEEPHBʌ
YellowFin
Adam default
YF searched
Adam searched
50	100	150	200
Epochs
J.4 Accelerating YellowFin with finer grain learning rate tuning
As an adaptive tuner, YellowFin does not involve manual tuning. It can present faster development
iterations on model architectures than grid search on optimizer hyperparameters. In deep learning
practice for computer vision and natural language processing, after fixing the model architecture,
extensive optimizer tuning (e.g. grid search or random search) can further improve the performance of
a model. A natural question to ask is can we also slightly tune YellowFin to accelerate convergence
and improve the model performance. Specifically, we can manually multiply a positive number, the
learning rate factor, to the auto-tuned learning rate in YellowFin to further accelerate.
In this section, we empirically demonstrate the effectiveness of learning rate factor on a 29-
layer ResNext (2x64d) (Xie et al., 2016) on CIFAR10 and a Tied LSTM model (Press and
Wolf, 2016) with 650 dimensions for word embedding and two hidden units layers on the PTB
dataset. When running YellowFin, we search for the optimal learning rate factor in grid
{1, 0.5,1,2(best for ResNext), 3(best for Tied LSTM), 10}. Similarly, We search the same learn-
ing rate factor grid for Adam, multiplying the factor to its default learning rate 0.001. To further
strength the performance of Adam as a baseline, We also run it on conventional logarithmic learning
rate grid {5e-5, 1e-4, 5e-4, 1e-3, 5e-3} for ResNext and {1e-4, 5e-4, 1e-3, 5e-3, 1e-2} for Tied
LSTM. We report the best metric from searching the union of learning rate factor grid and logarithmic
learning rate grid as searched Adam results. Empirically, learning factor 3 and 1.0 works best for
Adam respectively on ResNext and Tied LSTM.
As shown in Figure 12, with the searched best learning rate factor, YellowFin can improve
validation perplexity on Tied LSTM from 88.7 to 80.5, an improvement of more than 9%. Similarly,
the searched learning rate factor can improve test accuracy from 92.63 to 94.75 on ResNext. More
importantly, we can observe, with learning rate factor search on the two models, YellowFin can
achieve better validation metric than the searched Adam results. It demonstrates that finer-grain
learning rate tuning, i.e. the learning rate factor search, can be effectively applied on YellowFin to
improve the performance of deep learning models.
19