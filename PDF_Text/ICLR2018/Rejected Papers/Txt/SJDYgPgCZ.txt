Under review as a conference paper at ICLR 2018
Understanding Local Minima in Neural Net-
works by Loss Surface Decomposition
Anonymous authors
Paper under double-blind review
Ab stract
To provide principled ways of designing proper Deep Neural Network (DNN)
models, it is essential to understand the loss surface of DNNs under realistic
assumptions. We introduce interesting aspects for understanding the local minima
and overall structure of the loss surface. The parameter domain of the loss surface
can be decomposed into regions in which activation values (zero or one for rectified
linear units) are consistent. We found that, in each region, the loss surface have
properties similar to that of linear neural networks where every local minimum is a
global minimum. This means that every differentiable local minimum is the global
minimum of the corresponding region. We prove that for a neural network with
one hidden layer using rectified linear units under realistic assumptions. There are
poor regions that lead to poor local minima, and we explain why such regions exist
even in the overparameterized DNNs.
1	Introduction
Deep Neural Networks (DNNs) have achieved state-of-the-art performances in computer vision,
natural language processing, and other areas of machine learning (LeCun et al., 2015). One of the
most promising features of DNNs is its significant expressive power. The expressiveness of DNNs
even surpass shallow networks as a network with few layers need exponential number of nodes to have
similar expressive power (Telgarsky, 2016). The DNNs are getting even deeper after the vanishing
gradient problem has been solved by using rectified linear units (ReLUs) (Nair & Hinton, 2010).
Nowadays, RELU has become the most popular activation function for hidden layers. Leveraging this
kind of activation functions, depth of DNNs has increased to more than 100 layers (He et al., 2016).
Another problem of training DNNs is that parameters can encounter pathological curvatures of the
loss surfaces prolonging training time. Some of the pathological curvatures such as narrow valleys
would cause unnecessary vibrations. To avoid these obstacles, various optimization methods were
introduced (Tieleman & Hinton, 2012; Kingma & Ba, 2015). These methods utilize the first and
second order moments of the gradients to preserve the historical trends. The gradient descent methods
also have a problem of getting stuck in a poor local minimum. The poor local minima do exist
(Swirszcz et al., 2016) in DNNs, but recent works showed that errors at the local minima are as low
as that of global minima with high probability (Dauphin et al., 2014; Choromanska et al., 2015;
Kawaguchi, 2016; Safran & Shamir, 2016; Soudry & Hoffer, 2017).
In case of linear DNNs in which activation function does not exist, every local minimum is a global
minimum and other critical points are saddle points (Kawaguchi, 2016). Although these beneficial
properties do not hold in general DNNs, we conjecture that it holds in each region of parameters
where the activation values for each data point are the same as shown in Figure 1. We prove this for a
simple network. The activation values of a node can be different between data points as shown in
Figure 1, so it is hard to apply proof techniques used for linear DNNs. The whole parameter space is
a disjoint union of these regions, so we call it loss surface decomposition.
Using the concepts of loss surface decomposition, we explain why poor local minima do exist even
in large networks. There are poor local minima where gradient flow disappears when using the ReLU
(Swirszcz et al., 2016). We introduce another kind of poor local minima where the loss is same as
that of linear regression. To be more general, we prove that for each local minimum in a network,
there exists a local minimum of the same loss in the larger network that is constructed by adding a
node to that network.
1
Under review as a conference paper at ICLR 2018
Figure 1: A simple example of the activation regions for a dataset x1 = [-1, 1]T , x2 = [2, 1]T , x3 =
[1, 3]T . In each region, activation values are the same. There are six nonempty regions. The
parameters on the boundaries hit the non-differentiable point of the rectified linear unit.
2	Loss surface decomposition
Loss surface of deep linear networks have the following interesting properties: 1) the function is
non-convex and non-concave, 2) every local minimum is a global minimum, 3) every critical point
that is not a global minimum is a saddle point (Kawaguchi, 2016). This means that there is no poor
local minima problem when using gradient descent methods, but such properties do not hold for
nonlinear networks. We conjecture that these properties hold if activation values are fixed, and we
prove it for a simple network. The loss surface of DNNs can be decomposed into regions in terms of
activation values as illustrated in Figure 1.
2.1	Model and activation regions
Let D be a dataset {(x1, y1), (x2, y2), ..., (xN, yN)} with xi ∈ Rn and yi ∈ R. We define a network
with one hidden layer as follows:
f(xi, θ) = vT σ(W xi + b) + c.	(1)
The model parameters are W ∈ Rh×n, v ∈ Rh, b ∈ Rh, and c ∈ R where h is the number of
hidden nodes. Let θ = [vec(W), v, b, c]T collectively denote vectorized form of all the model
parameters. The activation function σ(x) = max(x, 0) is a rectified linear unit, and we abuse
notation by generalizing it as an element-wise function for multidimensional inputs. Alternatively,
the network can be expressed in terms of the activation values:
gA(xi, θ) = vT diag(ai)(W xi + b) + c,	(2)
where ai = [ai1, ai2, ..., aih]T is a vector of the binary activation values aij ∈ {0, 1} of i-th data
point xi, and A = (a1, a2, ..., aN) is a collection of all activation values for a given dataset D. We
fix the activation values of the function gA(xi, θ) regardless of real activation values to find out the
interesting properties. The real model f(xi, θ) agrees with gA(xi, θ) only if A is same as the real
activation values in the model.
Before we introduce a definition of the activation region, we denote wjT as a j-th row of W, and let
1(∙) be an indicator function.
Definition 2.1 An activation region RA of an activation values A is a set of parameters satisfying
aij = I(WT Xi + bj > 0) for all i,j, where i is an index of data points and j is an index of hidden
nodes.
2
Under review as a conference paper at ICLR 2018
Figure 2: A simple example of a non-differentiable local minimum for a dataset x1 = -2, y1 =
-3, x2 = +2, y2 = -1. In this example, a network is defined by f(x) = w2σ(w1x) and w1 is fixed
to one. The non-differentiable local minima exist in a line w1 = 0 which is a boundary of the two
regions. Note that if wι = 0, then Vw2 Lf = 0.
We consider a general loss function called squared error loss:
1N
Lf (θ) = 2∑(f (Xi,θ) - y，)2.	(3)
i=1
The following lemma state that the local curvatures of LgA (θ) and Lf (θ) agree in the differentiable
part of RA.
Lemma 2.2 For any differentiable point θ ∈ RA, the θ is a local minimum (saddle point) in Lf (θ)
if and only if it is a local minimum (saddle point) in LgA (θ).
2.2	Function of fixed activation values
The function gA (xi , θ) of fixed activation values A has properties similar to that of linear neural
networks. If all activation values are one, then the function gA(xi, θ) is identical to a linear neural
network. In other cases, some of the parameters are inactive. The proof becomes tricky since inactive
parameters are different for each data point. In case of the simple network gA (xi , θ), we can convert
it into a convex function in terms of other variables.
h
gA(xi , θ) =	aij (pjT xi + qj) + c,	(4)
where pj = vj wj and qj = vj bj . The vj is a j-th scalar value of the vector v and aij is an activation
value on a j -th hidden node of a i-th data point.
Lemma 2.3 The function LgA (θ) is a convex function in terms of pj, qj, and c.
Note that for any pj and qj , there exist θ that forms them, so the following lemma holds.
Lemma 2.4 Thefunction LgA (θ) is minimized if and only if the gradients Rpj LgA, Rqj LgA, and
▽cLgA are zeros for all j.
Now we introduce the following theorem describing the important properties of the function LgA (θ).
Theorem 2.5 The function LgA (θ) has following properties: 1) it is non-convex and non-concave
except for the case that activation values are all zeros, 2) every local minimum is a global minimum,
3) every critical point that is not a global minimum is a saddle point.
3
Under review as a conference paper at ICLR 2018
Sketch of proof. A function f(x, y) = (xy - 1)2 is not convex, since it has a saddle point at
x = y = 0. Similarly, the LgA (θ) is a quadratic function of vjbj, so it is non-convex and non-
concave. If activation values are all zeros, then LgA (θ) is a convex function 2 PN=I(C - yi)2 with
respect to c. If ▽ PjLgA = 0 and ▽ qjLgA = 0, then ▽ WjLgA = 0, ▽ VjLgA = 0 and RbjLgA = 0,
so the global minima are critical points. In other critical points, at least one of the gradients along pj
or qj is not zero. If a critical point satisfies RpjL 6= 0 (or RqjL 6= 0), then it is a saddle point with
respect to wjT and vj (or bj and vj ). The detailed proof is in the appendix.
To distinguish between the global minimum of LgA (θ) and Lf (θ), we introduce subglobal minimum:
Definition 2.6 A subglobal minimum of A is a global minimum of LgA (θ).
Some of the subglobal minima may not exist in the real loss surface Lf (θ). For this kind of regions,
there only exist saddle points and the parameter would move to another region by gradient descent
methods without getting stuck into local minima. Since the parameter space is a disjoint union of the
activation regions, the real loss surface Lf (θ) is a piecewise combination of LgA (θ). Using Lemma
2.2 and Theorem 2.5, we conclude as follows:
Corollary 2.7 The function Lf (θ) has following properties: 1) it is non-convex and non-concave,
2) every differentiable local minimum is a subglobal minimum, 3) every critical point that is not a
subglobal minimum is a saddle point.
We explicitly distinguish differentiable and non-differentiable local minima. The non-differentiable
local minima can exist as shown in Figure 2.
3	Existence of poor local minima
In this section, we answer why poor local minima do exist even in large networks. There are parameter
points where all the activation values are zeros eliminating gradient flow (Swirszcz et al., 2016).
This is a well-known region that forms poor and flat local minima. We introduce another kind of
poor region called linear region and show that it always forms poor local minima when a dataset
is nonlinear. In a more general setting, we prove that a network has every local minimum of the
narrower networks of the same number of layers.
3.1	Linear region
There always exists a linear region where all activation values are one, and its subglobal minima
stay in that region. This subglobal minimum results in an error which is same as that of linear
regression, so if given dataset is nonlinear the error would be poor. We can easily spot a linear region
by manipulating biases to satisfy wjT xi + bj > 0. One way of achieving this is by selecting bj as:
bj = - min wjT xi + 1.	(5)
i
To say that the model can get stuck in the linear region, it is necessary to find the subglobal minima
in that region. If f(xi, θ) is linear, then it is of form uTxi + d. Let vT W = uT , c = -vT b + d, and
bj be same as Equation 5, then the Equation 1 is equal to uTxi + d and the parameters are inside
the linear region. Thus the subglobal minima do exist in the linear region. This trick is generally
applicable to multilayer neural networks.
3.2	Poor local minima arises from the smaller network
Consider a multilayer neural network f that has some local minima. Let f0 be a network constructed
from f by adding a single hidden node. If activation values of the new node are all zeros, then the loss
values of local minima in f0 and f are equivalent since the new parameters in f0 does not change the
outputs of f. We can easily find such regions by constraining the new bias to be bj = - maxi wjT hi
where hi is a hidden layer of i-th data point xi . We numerically checked this property by identifying
all subglobal minima as shown in Figure 3.
4
Under review as a conference paper at ICLR 2018
# of hidden nodes = 1
25
# of hidden nodes = 2
U. L ∏
Xi - (TI),为二1
χ2 — ( 2j 1 ), y2 ~ 2
χ3 — ( 1/ ɜ )^3z3 — ɜ
Oooooooo
5 0 5 0 5 0 5
3 3 2 2 1 1
elu⊂-lu -eqo-6qns JO #
Oooooooo
Ooooooo
5 0 5 0 5 0 5
3 3 2 2 1 1
elu∙≡一E -eqo-6qns jo #
12345678
Loss
Figure 3: The histograms of subglobal minima for a dataset x1 = [-1, 1]T , y1 = 1, x2 =
[2, 1]T , y2 = 2, x3 = [1, 3]T , y3 = 3. The network has one hidden layer and no biases. We
increased the number of hidden nodes from one to four.
4	Poor regions in large networks
The ratio of the poor regions decreases as the size of the network grows. We show it numerically by
identifying all subglobal minima of a simple network. For the MNIST (LeCun, 1998), we estimated
subglobal minima of randomly selected activation values and compared with the rich regions.
4.1	Identifying all subglobal minima
Training a neural network is known to be NP-Complete (Blum & Rivest, 1989), due to the non-
convexity and infinite parameter space of DNNs. The number of possible combination of activation
values has the complexity of O(2Nh) for f(xi, θ), so we restricted the experiments to a small size of
hidden layers and datasets to find all subglobal minima in a reasonable time.
Consider the Equation 4 again. The subglobal minimum is a solution of the convex optimization
for LgA (θ). To compute optimal parameters, We need to solve linear equations Vpj LgA = 0,
Rqj LgA = 0, and VCLgA = 0. For simplicity, We assume that biases are removed, then the gradient
V L is as folloWs:
pj gA
N	NN
∀j, Vpj LgA	= XaijxixiT	pj + X	XaijaikxixiT	pk - Xaijyixi	= 0.	(6)
i=1	k6=j	i=1	i=1
Let Φj = PiN=1 aijxixiT, Λjk = PiN=1 aijaikxixiT, and Yj = PiN=1 aijyixi. As a result, the linear
equation to solve is as folloWs:
Φ1	Λ12
Λ21	Φ2
..
..
..
Λh1	Λh2
(7)
5
Under review as a conference paper at ICLR 2018
Table 1: The accuracy (%) of estimated SUbglobal minima for MNIST.
	k=16	k = 32	k = 64	k= 128	k=256
Random subglobal minima	82.59±0.32	92∙84±0.24	98.19±0.27	99.61±0.07	99.60±0.06
Rich subglobal minima	98.10±0.20	99.41±0.i3	99.80±0.06	99.88±0.04	99.91±0.03
The leftmost matrix in the Equation 7 is a square matrix. If it is not full rank, we compute a particular
solution. Figure 3 shows four histograms of the poor subglobal minima for the different number of
hidden nodes. As shown in the histograms, gradient descent based methods are more likely to avoid
poor subglobal minima in larger networks. It also shows that the subglobal minima arise from the
smaller networks. Intuitively speaking, adding a node provides a downhill path to the previous poor
subglobal minima without hurting the rich subglobal minima in most cases.
4.2	Experiments on MNIST
For more realistic networks and datasets, we conducted experiments on MNIST. We used networks
of two hidden layers consisting of 2k and k nodes respectively. The networks use biases, softmax
outputs, cross entropy loss, mini-batch size of 100, and Adam Optimizer (Kingma & Ba, 2015).
Assuming that the Corollary 2.7 holds for multilayer networks, the subglobal minima can be estimated
by gradient descent methods. It is impossible to compute all of them, so we randomly selected various
combinations of activation values with P(a = 1) = P(a = 0) = 0.5. Then we removed rectified
linear units and multiplied the fixed activation values as follows:
hA(xi, θ) = diag(ai2)(W2diag(ai1)(W1xi + b1) + b2),	(8)
where hA is the output of the second hidden layer. The rich subglobal minima were estimated by
optimizing the real networks since it would end up in one of the subglobal minima that exist in the
real loss surface. The experiments were repeated for 100 times, and then we computed mean and
standard deviation. The results are shown in Table 1 and it implies that most of the regions in the
large networks are rich, whereas the small networks have few rich regions. In other words, it is more
likely to end up in a rich subglobal minimum in larger networks.
5	Related Works
(Baldi & Hornik, 1989) proved that linear networks with one hidden layer have the properties of the
Theorem 2.5 under minimal assumptions. Recently, (Kawaguchi, 2016) proved that it also holds for
deep linear networks. Assuming that the activation values are drawn from independent Bernoulli
distribution, a DNN can be mapped to a spin-glass Ising model in which the number of local minima
far from the global minima diminishes exponentially with the size of the network (Choromanska et al.,
2015). Under same assumptions in (Choromanska et al., 2015), the effect of nonlinear activation
values disappears by taking expectation, so nonlinear networks satisfy the same properties of linear
networks (Kawaguchi, 2016).
Nonlinear DNNs usually do not encounter any significant obstacles on a single smooth slope path
(Goodfellow et al., 2014), and (Dauphin et al., 2014) explained that the training error at local
minima seems to be similar to the error at the global minimum which can be understood via random
matrix theory. The volume of differentiable sub-optimal local minima is exponentially vanishing in
comparison with the same volume of global minima under infinite data points (Soudry & Hoffer,
2017). Although a number of specific example of local minima can be found in DNNs (Swirszcz
et al., 2016), it seems plausible to state that most of the local minima are near optimal.
As the network width increases, we are more likely to meet a random starting point from which there
is a continuous, strictly monotonically decreasing path to a global minimum (Safran & Shamir, 2016).
Similarly, the starting point of the DNNs approximate a rich family of hypotheses precisely (Daniely
et al., 2016). Another explanation is that the level sets of the loss become connected as the network is
increasingly overparameterized (Freeman & Bruna, 2015). These works are analogous to our results
6
Under review as a conference paper at ICLR 2018
showing that the parameters would end up in one of the subglobal minima which are similar to the
global minima.
6	Discussion and conclusion
We conjecture that the loss surface is a disjoint union of activation regions where every local minimum
is a subglobal minimum. Using the concept of loss surface decomposition, we studied the existence
of poor local minima and experimentally investigated losses of subglobal minima. However, the
structure of non-differentiable local minima is not yet well understood yet. These non-differentiable
points exist within the boundaries of the activation regions which can be obstacles when using
gradient descent methods. Further work is needed to extend knowledge about the local minima,
activation regions, their boundaries.
References
Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from
examples without local minima. Neural networks, 2(1):53-58, l989.
Avrim Blum and Ronald L Rivest. Training a 3-node neural network is np-complete. In Advances in
neural information processing systems, pp. 494-501, 1989.
Anna Choromanska, Mikael Henaff, Michael Mathieu, Gerard Ben Arous, and Yann LeCun. The
loss surfaces of multilayer networks. In Artificial Intelligence and Statistics, pp. 192-204, 2015.
Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks:
The power of initialization and a dual view on expressivity. In Advances In Neural Information
Processing Systems, pp. 2253-2261, 2016.
Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua
Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex
optimization. In Advances in neural information processing systems, pp. 2933-2941, 2014.
C Daniel Freeman and Joan Bruna. Topology and geometry of half-rectified network optimization.
In Proceedings of the International Conference on Learning Representations, 2015.
Ian J Goodfellow, Oriol Vinyals, and Andrew M Saxe. Qualitatively characterizing neural network
optimization problems. arXiv preprint arXiv:1412.6544, 2014.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information
Processing Systems, pp. 586-594, 2016.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of
the International Conference on Learning Representations, 2015.
Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436-444,
2015.
Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In
Proceedings of the 27th international conference on machine learning (ICML-10), pp. 807-814,
2010.
Blaine Rister and Daniel L. Rubin. Piecewise convexity of artificial neural networks. Neural Networks,
94(Supplement C):34 - 45, 2017.
Itay Safran and Ohad Shamir. On the quality of the initial basin in overspecified neural networks. In
International Conference on Machine Learning, pp. 774-782, 2016.
7
Under review as a conference paper at ICLR 2018
Daniel Soudry and Elad Hoffer. Exponentially vanishing sub-optimal local minima in multilayer
neural networks. arXiv preprint arXiv:1702.05777, 2017.
Grzegorz Swirszcz, Wojciech Marian Czarnecki, and Razvan Pascanu. Local minima in training of
deep networks. arXiv preprint arXiv:1611.06310, 2016.
Matus Telgarsky. benefits of depth in neural networks. In 29th Annual Conference on Learning
Theory, volume 49 of Proceedings ofMachine Learning Research ,pp.1517-1539. PMLR, 23-26
Jun 2016.
T. Tieleman and G. Hinton. Lecture 6.5—RmsProp: Divide the gradient by a running average of its
recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.
8
Under review as a conference paper at ICLR 2018
Appendix A Proofs of lemmas and theorem
A.1 Proof of Lemma 2.2
Let θ ∈ RA be a differentiable point, so it is not in the boundaries of the activation regions. This
implies that wjT xi + bj 6= 0 for all parameters. Without loss of generality, we assume wjT xi + bj < 0.
Then there exist > 0 such that wjT xi +bj + < 0. This implies that small changes in the parameters
for any direction does not change the activation region. Since Lf (θ) and LgA (θ) are equivalent in the
region RA, the local curvatures of these two function around the θ are also the same. Thus, the θ is a
local minimum (saddle point) in Lf (θ) if and only if it is a local minimum (saddle point) in LgA (θ).
A.2 Proof of Lemma 2.3
Clearly, (gA(xi, θ) - yi)2 is convex in terms of gA(xi, θ). Since gA(xi, θ) is a linear transformation
of pj , qj , and c, the (gA (xi , θ) - yi)2 is convex in terms of pj , qj , and c. Summation of convex
functions is convex, so the lemma holds.
A.3 Proof of Theorem 2.5
(1) Assume that activation values are not all zeros, and then consider the following Hessian matrix
evaluated from vj and bj for some non-zero activation values aij > 0:
∂2 L
U LgA
vV3- ,bj LgA (θ) =	∂⅞jA
∂vj ∂bj
2L	N
∂VgA = X aij (WTXi + bj)2
vj	i=1
2L	N
LgA — X aV
∂b2	= 2^aij V
j i=1
(9)
dLgA
∂Vj∂bj
N
aij (vj (WjT xi + bj) + gA(xi, θ) - yi)
i=1
Let vj = 0 and bj = 0, then two eigenvalues of the Hessian matrix are as follows:
2λ
d2LgA
± j(EY +4( jY
(10)
There exist c > 0 such that g∕(xi, θ) > y% for all i. IfWe choose such c, then UVLgA > 0 which
implies that two eigenvalues are positive and negative. Since the Hessian matrix is not positive
semidefinite nor negative semidefinite, the function LgA (θ) is non-convex and non-concave.
9
Under review as a conference paper at ICLR 2018
(2, 3) We organize some of the gradients as follows:
N
RbjLgA (θ) = Vj yy ^(gA(xi, θ) - yi)aij
i=1
N
Rwj LgA (θ) = vj	(gA(xi , θ) - yi )aij xi
i=1
N
RvjLgA(θ) =X(gA(xi,θ) -yi)aij(wjTxi+bj)	(11)
i=1
N
Rpj LgA (θ) =	(gA(xi, θ) -yi)aijxi
i=1
N
Rqj LgA (θ) =	(gA(xi, θ) - yi)aij.
i=1
We select a critical point θ* where RwjLgA(θ*) = 0, RvjLgA(θ*) = 0, RbjLgA(θ*) = 0, and
NcLgA(θ*)=0 forall j.	‘	'	'
Case 1)	Assume that RpjLgA(θ*) = 0 and RcjjLgA(θ*) = 0 for all j. These points are global
minima, since RcLgA (θ*) = 0 and LgA (θ) is convex in terms of Pj, qj, and c.
Case 2)	Assume that there exist j such that RpjLgA(θ*) = 0. Since NwjLgA(θ*)=
VjNpjLgA(θ*) = 0, the Vj is zero. If Vj = 0, then:
RWj LgA (θ*)=0
N
Rvj Rwj LgA (θ ) =	((gA(xi , θ) - yi ) + Vj (wj xi + bj ))aij xi
i=1
N
=	(gA(xi , θ) - yi )aij xi
i=1	(12)
= Rpj LgA (θ*)
6= 0
N
R2vjLgA(θ*) = X(wjTxi+bj)2aij
i=1
≥ 0.
There exist an element w* in Wj such that Rvj Rw* LgA (θ*) = 0. Consider a Hessian matrix
evaluated from w* and Vj . Analogous to the proof of (1), this matrix is not positive semidefinite nor
negative semidefinite. Thus θ* is a saddle point.
Case 3)	Assume that there exist j such that Rqj LgA (θ*) 6= 0. Since Rbj LgA (θ*) =
Vj Rqj LgA (θ*) = 0, the Vj is zero. Analogous to the Case 2, a Hessian matrix evaluated from
bj and Vj is not positive semidefinite nor negative semidefinite. Thus θ* is a saddle point.
As a result, every critical point is a global minimum or a saddle point. Since LgA (θ) is a differentiable
function, every local minimum is a critical point. Thus every local minimum is a global minimum.
10