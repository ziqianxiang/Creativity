Under review as a conference paper at ICLR 2018
Variance Regularized Counterfactual Risk
Minimization via Variational Divergence Min-
IMIZATION
Anonymous authors
Paper under double-blind review
Ab stract
Off-policy learning, the task of evaluating and improving policies using historic
data collected from a logging policy, is important because on-policy evaluation is
usually expensive and has adverse impacts. One of the major challenge of off-
policy learning is to derive counterfactual estimators that also has low variance
and thus low generalization error.
In this work, inspired by learning bounds for importance sampling problems, we
present a new counterfactual learning principle for off-policy learning with bandit
feedbacks. Our method regularizes the generalization error by minimizing the dis-
tribution divergence between the logging policy and the new policy, and removes
the need for iterating through all training samples to compute sample variance
regularization in prior work. With neural network policies, our end-to-end train-
ing algorithms using variational divergence minimization showed significant im-
provement over conventional baseline algorithms and is also consistent with our
theoretical results.
1	Introduction
Off-policy learning refers to evaluating and improving a deterministic policy using historic data
collected from a stationary policy, which is important because in real-world scenarios on-policy
evaluation is oftentimes expensive and has adverse impacts. For instance, evaluating anew treatment
option, a clinical policy, by administering it to patients requires rigorous human clinical trials, in
which patients are exposed to risks of serious side effects. As another example, an online advertising
A/B testing can incur high cost for advertisers and bring them few gains. Therefore, we need to
utilize historic data to perform off-policy evaluation and learning that can enable safe exploration of
the hypothesis space of policies before deploying them.
There has been extensive studies on off-policy learning in the context of reinforcement learning and
contextual bandits, including various methods such as Q learning (Sutton & Barto (1998)), doubly
robust estimator (Dudik et al. (2014)), self-normalized (Swaminathan & Joachims (2015b)), etc. A
recently emerging direction of off-policy learning involves the use of logged interaction data with
bandit feedback. However, in this setting, we can only observe limited feedback, often in the form
of a scalar reward or loss, for every action; a larger amount of information about other possibilities
is never revealed, such as what reward we could have obtained had we taken another action, the
best action we should have take, and the relationship between the change in policy and the change
in reward. For example, after an item is suggested to a user by an online recommendation system,
although we can observe the user’s subsequent interactions with this particular item, we cannot
anticipate the user’s reaction to other items that could have been the better options.
Using historic data to perform off-policy learning in bandit feedback case faces a common challenge
in counterfactual inference: How do we handle the distribution mismatch between the logging pol-
icy and a new policy and the induced generalization error? To answer this question, Swaminathan
& Joachims (2015a) derived the new counterfactual risk minimization framework, that added the
sample variance as a regularization term into conventional empirical risk minimization objective.
However, the parametrization of policies in their work as linear stochastic models has limited rep-
resentation power, and the computation of sample variance regularization requires iterating through
1
Under review as a conference paper at ICLR 2018
all training samples. Although a first-order approximation technique was proposed in the paper,
deriving accurate and efficient end-to-end training algorithms under this framework still remains a
challenging task.
Our contribution in this paper is three-fold:
1.	By drawing a connection to the generalization error bound of importance sampling (Cortes
et al. (2010)), we propose a new learning principle for off-policy learning with bandit feed-
back. We explicitly regularize the generalization error of the new policy by minimizing the
distribution divergence between it and the logging policy. The proposed learning objective
automatically trade off between emipircal risk and sample variance.
2.	To enable end-to-end training, we propose to parametrize the policy as a neural network,
and solves the divergence minimization problem using recent work on variational diver-
gence minimization (Nowozin et al. (2016)) and Gumbel soft-max (Jang et al. (2016))
sampling.
3.	Our experiment evaluation on benchmark datasets shows significant improvement in per-
formance over conventional baselines, and case studies also corroborates the soundness of
our theoretical proofs.
2	Background
2.1	Problem Framework
We first review the framework of off-policy learning with logged bandit feedback introduced in
Swaminathan & Joachims (2015a). A policy maps an input x ∈ X to a structured (discrete) output
y ∈ Y . For example, the input x can be profiles of users, and we recommend movies of relevance
to the users as the output y; or in the reinforcement learning setting, the input is the trajectory of
the agent, and the output is the action the agent should take in the next time point. We use a family
of stochastic policies, where each policy defines a posterior distribution over the output space given
the input x, parametrized by some θ, i.e., hθ(Y|x). Note that here a distribution which has all its
probability density mass on one action corresponds to a deterministic policy. With the distribution
h(Y|x), we take actions by sampling from it, and each action y has a probability of h(y|x) being
selected. In the discussion later, we will use h and h(y|x) interchangeably when there will not create
any confusion.
In online systems, we observe feedbacks δ(x, y; y*) for the action y sampled from h(Y|x) by com-
paring it to some underlying 'best' y* that was not revealed to the system. For example, in rec-
ommendation system, We can use a scalar loss function δ(x, y; y*) → [0, L], with smaller values
indicating higher satisfaction with recommended items.
The expected risk ofa policy h(Y|x) is defined as
R(A)= Ex 〜P(X ),y 〜h(Y∣x)[δ(X,y)]
, and the goal of off-policy learning is to find a policy with minimum expected risk on test data.
In the off-line logged learning setting, we only have data collected from a logging policy h0(Y|x),
and we aim to find an improved policy h(Y|x) that has lower expected risks R(h) < R(h0). Specif-
ically, the data we will use will be
D = {xi , yi , δi = δi (xi , yi), pi = h0 (yi |xi )}, i = 1, ..., N
, where δi and pi are the observed loss feedback and the logging probability (also called propensity
score), and N is the number of training samples.
Two main challenges are associated with this task: 1) If the distribution ofa logging policy is skewed
towards a specific region of the whole space, and doesn’t have support everywhere, feedbacks of
certain actions cannot be obtained and improvement for these actions is not possible as a result. 2)
since we cannot compute the expectation exactly, we need to resort to empirical estimation using
finite samples, which creates generalization error and needs additional regularization.
A vanilla approach to solve the problem is propensity scoring approach using importance sam-
pling (Rosenbaum & Rubin (1983)), by accounting for the distribution mismatch between h and
2
Under review as a conference paper at ICLR 2018
h0. Specifically, we can rewrite the expected risk w.r.t h as the risk w.r.t h0 using an importance
reweighting:
R(h)
Ex〜P(X),y〜h(y∣x) [δ(x, y)]
Ex~P(X),y~ho(y|x) [
h(y∣χ)
ho(y∣χ)
δ(x, y)]
(1)
5TJ1 .1	11	.	1 1 ∙	.	∙	1	.	. /TA	. ∙	.	. 1	♦ ♦	1	♦ 1 A / 7 ∖	1	.	A / 7 ∖
With the collected historic dataset D, we can estimate the empirical risk RD(h), short as R(h)
R(h)=N X MS …)
(2)
2.2	Counterfactual Risk Minimization
Swaminathan & Joachims (2015a) pointed out several flaws with the vanilla approach, namely, not
being invariant to loss scaling, large and potentially unbounded variance. To regularize the variance,
the authors proposed a regularization term for sample variance derived from empirical Bernstein
bounds.
The modified objective function to minimize is now:
A,7、	1	ʌ IVar(U)
R㈤=NEu + λy^u	⑶
i=1
h(y |x )	1 N
,where Ui = 九3；))δi, U= 齐 P Ui is the average of {ui} obtained from training data, and
0 i i	i=1
Var(U) is the sample variance of {ui}.
As the variance term is dependent on the whole dataset, stochastic training is difficult, the authors
approximated the regularization term via first-order Taylor expansion and obtained a stochastic op-
timization algorithm. Despite its simplicity, such first-order approximation neglects the non-linear
terms from second-order and above, and introduces approximation errors while trying to reduce the
sample variance.
3	Variance Regularization Objective
3.1	Theoretical Motivation
Instead of estimating variance empirically from the samples, which prohibits direct stochastic train-
ing, the fact that we have a parametrized version of the policy h(Y|x) motivates us to think: can we
derive a variance bound directly from the parametrized distribution?
We first note that the empirical risk term R(h) is the average loss reweigthed by importance sampling
function 彳(工?), and a general learning bound exist for importance sampling weights.
Let Z be a random variable and the importance sampling weight w(z) = Pp(Zzy, where P and po are
two probability density functions, the following identity holds
Lemma 1. (Cortes et al. (2010)) For a random variable z, let p(z) and p0(z) be two distribuion
density function defined for z, and l(z) be a loss function of z bounded in [0, 1]. Let w = w(z) =
p(z)/p0 (z) be the importance sampling weight, the following identity holds:
E[w] = 1, E[w2] = d2(p||p0) = 2D2(p||p0)
V ar(w) = E[w2] - E[w] = d2(p||p0) - 1
Ez〜po(z)[w2(z)l2(z)] ≤ d2(p∣∣P0)	(4)
,where D? is the Renyi divergence Da (Renyi et al. (1961)) with α = 2, i.e. squared Chi-2 diver-
gence.
3
Under review as a conference paper at ICLR 2018
Based on this lemma, we can derive an upper bound for the second moment of the weighted loss
Theorem 1.	Let X be a random variable distributed according to distribution P with density
p(x), Y be a random variable, and δ(x, y) be a loss function over (x, y) that is bounded in [0, L].
For two sampling distributions of y, h(y|x) and h0(y|x), define their conditional divergence as
d2(h(y|x)||h0(y|x); P (x)), we have
Ex〜P(X),y〜ho(y∣x)[w2⑶X)δ2(X,y)] ≤ L2d2(h(y|X)IIMy⑻;P(X))	⑸
The bound is similar to Eq. (4) with the difference that we are now working with a joint distribution
over X, y . Detailed proofs can be found in Appendix 1.
From the above theorem, we are able to derive a generalization bound between the expected risk
R(h) and empirical risk R(h) using the distribution divergence function as
Theorem 2.	Let Rh be the expected risk of the new policy on loss function δ, and Rh be the emprical
risk. We additionally assume the divergence is bounded by M, i.e., d2(hIIh0) ≤ d∞ (hIIh0) = M
Then with probability at least 1 - η,
R(h) ≤ R(h) + 2LM” + Lr2Var(Rlog1∕η
The proof of this theorem is an application of Bernstein inequality and the second moment bound,
and detailed proof is in Appendix 7. This result highlights the bias-variance trade-offs as seen
in empirical risk minimization (ERM) problems, where Rh approximates the emipircal risk/ bias,
and the third term characterize the variance of the solution with distribution divergence (Recall
V ar(w) = d2(hIIh0) - 1). It thus motivates us that in bandit learning setting, instead of directly
optimizing the reweighed loss and suffer huge variance in test setting, we can try to minimize the
variance regularized objectives as
min
h=h(Y |x)
ʌ , .
R(h) + λ
4 N (d2(h||h0; P (X))))一
(6)
1
λ = /2L2 log1∕η is a model hyper-parameter controlling the trade-off between empirical risk and
model variance, but we are still faced with the challenge of setting λ empirically and the difficuty
in optimizing the objective (See Appendix for a comparison). Thus, in light of the recent success
of distributionally robust learning, we explore an alternative formulation of the above regularized
ERM in the next subsection.
3.2 Robustly Regularized Formulation
Instead of solving a ‘loss + regularizer’ objective function, we here study a closely related con-
trained optimizationf formulation, whose intuition comes from the method of Langaragian mutli-
plier for constrained optimization.
The new formulation is:
min
h
1 m h(yiIXi)
NrF^ δi
i=1	pi
s.t. d2(hIIh0;P(X)) ≤ ρ
, where ρ is a pre-determined constant as the regularization hyper-parameter.
By applying Theorem , for a policy h, we have
R(h) ≤ R(h) + 2LMlog1加 + Lr2Var(Rlog1加
v ^	2, L , 2LMlog 1/n	r,∕2(ρ- 1)logUη
≤ Rd(h∣∣ho≤ρ)(h) +----3N----+ LV--------N-------
(7)
(8)
4
Under review as a conference paper at ICLR 2018
This inequality shows that the robust objective Rd(h∣∣ho≤ρ)(h) is also a good surrogate of the true
risk R(h), with their difference bouned by the regularization hyper-parameter ρ and approaches 0
when N → ∞.
At first glance, the new objective function removes the needs to compute the sample variance in
existing bounds (3), but when we have a parametrized distribution of h(y|x), and finite samples
{xi, yi}iN=1 from h0(yi|xi), estimating the divergence function is not an easy task. In the next
subsection, we will present how recent f-gan networks for variational divergence minimization
(Nowozin et al. (2016)) and Gumbel soft-max sampling (Jang et al. (2016)) can help solve the
task.
Discussion: Possibility of Counterfactual Learning: One interesting aspect of our bounds also
stresses the need for the stochasticity of the logging policy (Langford et al. (2008)). For a determin-
istic logging policy, if the corresponding probability distribution can only have some peaked masses,
and zeros elsewhere in its domain, our intution suggests that learning will be difficult, as those re-
gions are never explored. Our theory well reflects this intuition in the calculation of the divergence
term, the integral of form y h2(y|x)/h0(y|x)dy. A deterministic policy has a non-zero measure
region of h0(Y|x) with probability density of h0(y|x) = 0, while the corresponding h(y|x) can
have finite values in the region. The resulting integral results is thus unbounded, and in turn induces
an unbounded generalization bound, making counterfactual learning in this case not possible.
4 Adversarial Training Algorithm
4.1	Adversarial Learning of the Divergence
The derived variance regularized objective (6) requires us to minimize the square root of the condi-
tional divergence, d2(h∣∣h0; P(X)) = ExRy hdy.
For simplicity, we can examine the term inside the expectation operation first. With simple calcula-
tion, we have
Z h(τ^dy = Z h0(y∣χ)[(^yxy)2 - 1 + 1] = Df (h(Y∣χ)l∣ho(Y∣χ)) + 1
y h0(y|x)	y	h0(y|x)
, where f(t) = t2 - 1 is a convex function in the domain {t : t ≥ 0} with f(1) = 0. Combining
with the expectation operator gives a minimization objective of Df(h||h0; P(X)) (+1 omitted as
constant).
The above calculation draws connection between our divergence and the f-divergence measure
(Nguyen et al. (2010)). Follow the f-GAN for variational divergence minimization method proposed
in Nowozin et al. (2016), we can reach a lower bound of the above objective as
Df (h(Y|x)||ho(Y|x); P (X)) = Ex[∕f( 7h(yxv)dho(y∣χ)]
y	h0(y|x)
=Ex[sUp{Ey 〜h(y∣x)[T ⑻]-Ey 〜h0(y∣x)[f * (T 3))])}	(9)
T
=SUP{ExEy〜h(y∣x)[T(x,y)] - ExEy〜ho(y∣x)[f (T(x,y))]}
T
≥ sup {Ex Ey 〜h(y∣x)[T (x,y)] - ExEy 〜ho(y∣χ)[f *(T (χ,y))]}
T∈T
=sup{Ex,y〜h(y∣x)T(x,u) - Ex,y〜h0(y∣x)f*(T(χ,y))} (10)
T∈T
, F(T,h)
(11)
For the second equality, as f is a convex function and applying Fenchel convex duality (f * =
supu{u0v - f (u)}) gives the dual formulation. Because the expectation is taken w.r.t to x while
the supreme is taken w.r.t. all functions T , we can safely swap the two operators. We note that the
bound is tight when T0(x) = f0(h/h0), where f0 is the first order derivative of f as f0 (t) = 2t
(Nguyen et al. (2010)).
5
Under review as a conference paper at ICLR 2018
The third inequality follows because we restrict T to a family of functions instead of all functions.
Luckily, the universal approximation theorem of neural networks Hornik et al. (1989) states that
neural networks with arbitrary number of hidden units can approximate continous functions on a
compact set with any desired precision. Thus, by choosing the family of T to be the family of neural
networks, the equality condition of the second equality can be satisfied theoretically.
The final objective (10) is a saddle point of a function T(x, y) : X × Y → R that maps input pairs
to a scalar value, and the policy we want to learn h(Y|x) acts as a sampling distribution. Although
being a lower bound with achievable equality conditions, theoretically, this saddle point trained with
mini-batch estimation is a consistent estimator of the true divergence.
We use Df = SuPT JTdhdX - f f *(T)dh°dx to denote the true divergence, and
Df SuPT∈t / T(Xi, yi)dhdx — f f *T(xj, yj)dh°dx the empirical estimator We use, where h and
ho are the emipircal distribution obtained by sampling from the two distribution respectively.
Propoistion 1. Df is a consistent estimator of Df.
Proof. Let’s start by decomposing the estimation error. Let e0 = (Df - SuPT∈TE[ Tdh -
R f*(T)dho]), and eι = SupT∈t| R T(x,y)d(h - h) - R f *(T(x,y))d(ho - ho)|
|Df - Df | ≤ eo + eι	(12)
, where the first term of error comes from restricting the parametric family of T to a family of
neural networks, and the second term of error involves the approximation error of an emipirical
mean estimation to the true distribution. By the universal approximation theorem, we have e0 = 0,
and that ∃T ∈ T, such that T = T0 .
For the second term e1 , we plug in T0 and have it rewritten as
≤
+
SupT∈T |/ T (x,y)d(h - h)-/
| sup[∕(T - To)(dh - 2h)] - j
T∈T
,, .. ʌ ..
f*(T(x,y))d(ho - ho)|
, ,, ...................... ʌ ..
(f*(T) - f *(T0))(dh0- dho)∣
, —,__ 、 _______________ , ,
| SUP T0(dh — dh) — f (T0)(dh0 — dh0)∣
T∈T
(13)
(14)
(15)
For the first term, | SUPT∈t To(dh - dh) - f *(T0)(dho - dho)∣, we can see that this is the diffrence
between an empirical distribution and the underlying population distribution. We can verify that the
strong law of large numbers (SLLN) applies. By optimality condition, To = 彳(案),where both h
and h0 are probability density functions. By the bounded loss assumption, the ratio is integrable.
Similarly, f *(To) = 2To - 1 is also integrable. Thus, we can apply SLLN and conclude the term
→ 0. For the second term, we can apply Theroem 5 from [] and also obtain it → a.s.0.	口
Again, a generative-adversarial approach (Goodfellow et al. (2014)) can be applied. Toward this
end, we represent the T function as a discriminator network parametrized as Tw (x, y). We then
parametrize the distribution of our policy h(y∣x) as another generator neural network hθ (y|x) map-
ping x to the probability of sampling y. For structured output problems with discrete values of y, to
allow the gradients of samples obtained from sampling backpropagated to all other parameters, we
6
Under review as a conference paper at ICLR 2018
use the Gumbel soft-max sampling (Jang et al. (2016)) methods for differential sampling from the
distribution h(y|x). We list the complete training procedure Alg. 1 for completeness.
Data: D = {xi, yi}iN=1 sampled from logging policy h0; a predefined threshold D0; an initial
generator distribution hθ0 (y|x); an initial discriminator function Tw0 (x, y)
;max iteration I Result: An optimized generator hθ* (y|x) distribution that has minimum
divergence to h0
initialization;
_ α ，_ ，， _	_ _____ 一	— _
while DDf (h∣∣ho; P(X)) > Do or iter < I do
Sample a mini-batch ‘real’ samples (xi, yi) from D ;
Sample a mini-batch X from D, and construct ‘fake' samples (χi, yi) by sampling y from
hθt (y|x) with Gubmel soft-max ;
Update wt+1 = wt +ηw∂F(Tw,hθ)(10) ;
Update θt+1 = θt - ηθ∂F(Tw, hθ)(10) ;
end
Algorithm 1: Variational Minimizing Df (h||h0; P(X))
For our purpose of minimizing the variance regularization term, we can similarly derive a training
algorithm, as the gradient of t → √t + 1 can also be backpropagated.
4.2	Training algorithm
With the above two components, we are now ready to present the full treatment of our end-to-end
learning for counterfactual risk minimization from logged data. The following algorithm solve the
robust regularized formulation and for completeness, training for the original ERM formulation in
Sec. 3.1 (referred to co-training version in the later experiment sections) is included in Appendix 7.
Data: D = {xi, yi, pi, δi}iN=0 sampled from h0; regularization hyper-parameter ρ, and maximum
iteration of divergence minimization steps I, and max epochs for the whole algorithm MAX
Result: An optimized generator h^(y∣x) that is an approximate minimizer of R(W)
initialization;
while epoch < MAX do
/* Update θ to minimize the reweighted loss	*/
Sample a mini-batch ofm samples from D ;
Estimate the reweighted loss as Rt = ml P h (yi|Xi)方分 and get the gradient as gι = ∂θ Rt ;
i=1	pi
Update θt+1 = θt - ηθg1 ;
/* Update discriminator and generator for divergence
minimization	*/
Call Algorithm 1 to minimize the divergence D2(h||h0; P(X)) with threshold = ρ, and max
iter set to I ;
end
Algorithm 2: Minimizing Variance Regularized Risk - Separate Training
The algorithm works in two seperate training steps: 1) update the parameters of the policy h to
minimize the reweighed loss 2) update the parameters of the policy/ generator and the discriminator
to regularize the variance thus to improve the generalization performance of the new policy.
5	Related Work
Exploiting historic data is an important problem in multi-armed bandit and its variants such as con-
textual bandit and has wide applications (Strehl et al. (2010); Shivaswamy & Joachims (2012);
Beygelzimer & Langford (2009)). Approaches such as doubly robust estimators (DUdIk et al.
(2014)) have been proposed, and recent theoretical study explored the finite-time minimax risk
lower bound of the problem (Li et al. (2015)), and an adaptive learning algorithm (Wang et al.
(2017)) using the theoretical analysis.
7
Under review as a conference paper at ICLR 2018
Bandits problems can be interpreted as a single-state reinforcement learning (RL) problems, and
techniques including doubly robust estimators (Jiang & Li (2015); Thomas & Brunskill (2016);
Munos et al. (2016)) have also been extended to RL domains. Conventional techniques such as
Q function learning, and temporal difference learning (Sutton & Barto (1998)) are alternatives for
off-policy learning in RL by accounting for the Markov property of the decision process. Recent
works in deep RL studies have also addressed off-policy updates by methods such as multi-step
bootstrapping (Mahmood et al. (2017)), off-policy training of Q functions (Gu et al. (2017)).
Learning from logs traces backs to Horvitz & Thompson (1952) and Rosenbaum & Rubin (1983) ,
where propensity scores are applied to evaluate candidate policies. In statistics, the problem is also
described as treatment effect estimation (Imbens (2004)), where the focus is to estimate the effect of
an intervention from observational studies that are collected by a different intervention. Bottou et al.
(2013) derived unbiased counterfactual estimators to study an example of computational advertising;
another set of techniques reduce the bandit learning to a weighted supervised learning problems
(Zadrozny et al. (2003)), but is shown to have poor generalization performance (Beygelzimer &
Langford (2009)).
Although our variance regularization aims at off-policy learning with bandit feedback, part of the
proof comes from the study of generalization bounds in importance sampling problems (Cortes et al.
(2010)), where the original purpose was to account for the distribution mismatch between training
data and testing distribution, also called covariate shift, in supervised learning. Duchi & Namkoong
(2016) also discussed variance regularized empirical risk minimization for supervised learning with
a convex objective function, which has connections to distributionally robust optimization problem
(Bertsimas et al. (2011)). It will be of further interest to study how our divergence minimization
technique can be applied to supervised learning and domain adaptation (Sugiyama et al. (2007);
Gretton et al. (2009)) problems as an alternative to address the distribution match issue.
Regularization for our objective function has close connection to the distributionally robust opti-
mization techniques (Bertsimas et al. (2013)), where instead of minizing the emiprical risk to learn
a classifier, we minimize the supreme the emipirical risk over an ellipsoid uncertainty set. Wasser-
stein distance between emipircal distribution and test distribution is one of the most well studied
contraint and is proven to achieve robust generalization performance (Esfahani & Kuhn (2015); Gao
& Kleywegt (2016)). Distributionally robust optimization is also connected to machine learning,
specifically generalization error (Xu et al. (2009); Shafieezadeh-Abadeh et al. (2015)). We refer
interested readers to a more comprehensive overview of the subject in Gabrel et al. (2014).
6	Experiments
6.1	Experiment Setups
For empirical evaluation of our proposed algorithms, we follow the conversion from supervised
learning to bandit feedback method (Agarwal et al. (2014)). For a given supervised dataset D* =
{(xi, y*)}N=ι, We first construct a logging policy ho(Y∣χ), and then for each sample Xi, We sample
a prediction yi 〜ho(y∣Xi), and collect the feedback as δ(yi,yi). For the purpose of benchmarks,
we also use the conditional random field (CRF) policy trained on 5% of D* as the logging policy
ho , and use hamming loss, the number of incorrectly misclassified labels between yi and yi, as
the loss function δ (Swaminathan & Joachims (2015a)). To create bandit feedback datasets D =
{xi, yi, δi,pi}, each of the samples xi were passed four times to the logging policy h0 and sampled
actions yi were recorded along with the loss value δi and the propensity score pi = h0(yi|xi).
In evaluation, we use two type of evaluation metrics for the probabilistic policy h(Y|x). The first is
the expected loss (referred to as ‘EXP' later) R(h) = NI^ Pi Ey^h(y∣χi)δ(y*,y), adirect measure
of the generalization performance of the learned policy. The second is the average hamming loss
of maximum a posteriori probability (MAP) prediction yMAP = arg max h(y|x) derived from the
learned policy, as MAP is a faster way to generate predictions without the need for sampling in
practice. However, since MAP predictions only depend on the regions with highest probability, and
doesn’t take into account the diverse of predictions, two policies with same MAP performance could
have very different generalization performance. Thus, a model with high MAP performance but low
EXP performance might be over-fitting, as it may be centering most of its probability masses in the
regions where h0 policy obtained good performance.
8
Under review as a conference paper at ICLR 2018
Table 1: Benchmark Comparison Results.
Dataset	Scene		Yeast		TMC		LYRL	
Evaluation Metrics	MAP	EXP	MAP	EXP	MAP	EXP	MAP	EXP
Logging Policy h0 (5% data CRF)	1.069	1.887	3.255	5.485	4.995	5.053	1.047	1.949
NN-NoReg	1.465	1.981	3.223	4.705	1.706	1.724	0.247	0.263
NN-Hard	1.303	1.463	3.047	3.788	1.694	1.720	0.248	0.255
NN-Soft	1.347	1.457	3.097	3.789	1.683	1.707	0.247	0.255
IPS	1.350	1.350	4.256	4.521	4.601	4.416	1.240	1.240
POEM	1.169	1.169	4.238	4.508	4.611	4.505	1.169	1.306
IPS(Stochastic)	1.291	1.291	4.090	4.605	2.812	2.737	1.149	1.479
POEM(Stochastic)	1.322	1.323	4.140	4.570	3.601	3.561	1.237	1.237
Supervised Learning (NN)	0.943	2.238	3.101	4.300	1.530	3.786	0.217	0.519
Supervised Learning (CRF)	1.110	1.423	2.807	4.047	1.344	1.241	0.240	0.437
6.2	Benchmark Comparison
Baselines Vanilla importance sampling algorithms using inverse propensity score (IPS), and
the counterfactual risk minimization algorithm from Swaminathan & Joachims (2015a) (POEM)
are compared, with both L-BFGS optimization and stochastic optimization solvers. The hyper-
parameters are selected by performance on validation set and more details of their methods can be
found in the original paper (Swaminathan & Joachims (2015a)).
Neural network policies without divergence regularization (short as ”NN-NoReg” in later discus-
sions) is also compared as baselines, to verify the effectiveness of variance regularization.
Dataset We use four multi-label classification dataset collected in the UCI machine learning repo
(Asuncion & Newman (2007)), and perform the supervised to bandit conversion. We report the
statistics in Table 2 in the Appendix.
For these datasets, we choose a three-layer feed-forward neural network for our policy distribution,
and a two or three layer feed-forward neural network as the discriminator for divergence minimiza-
tion. Detailed configurations can be found in the Appendix 7.
For benchmark comparison, we use the separate training version 2 as it has faster convergence and
better performance (See Sec. 6.5 for an empirical comparison). The networks are trained with Adam
(Kingma & Ba (2014)) of learning rate 0.001 and 0.01 respectively for the reweighted loss and the
divergence minimization part. We used PyTorch to implement the pipelines and trained networks
with Nvidia K80 GPU cards. Codes for reproducing the results as well as preprocessed data can be
downloaded with the link 1
Results by an average of 10 experiment runs are obtained and we report the two evaluation metrics
in Table 1. We report the regularized neural network policies with two Gumbel-softmax sampling
schemes, soft Gumbel soft-max (NN-Soft), and straight-through Gumbel soft-max (NN-Hard).
As we can see from the result, by introducing a neural network parametrization of the polices, we
are able to improve the test performance by a large margin compared to the baseline CRF policies,
as the representation power of networks are often reported to be stronger than other models. The
introduction of additional variance regularization term (comparing NN-Hard/Soft to NN-NoReg),
we can observe an additional improvement in both testing loss and MAP prediction loss. We observe
no significant difference between the two Gumbel soft-max sampling schemes.
1https://www.dropbox.com/sh/etuc8dnxyope1xh/AAAjFJ06cFyJeYr8YN8z996Ta?
dl=0. We will make them publicly available on GitHub after the anonymous review periods.
9
Under review as a conference paper at ICLR 2018
6.3	Effect of Variance Regularization
To study the effectiveness of variance regularization quantitatively, we vary the maximum number
of iterations (I in Alg. 2) we take in each divergence minimization sub loop. For example, ‘NN-
Hard-10’ indicates that we use ST Gubmel soft-max and set the maximum number of iterations to
10. Here we set the thresholds for divergence slightly larger so maximum iterations are executed so
that results are more comparable. We plot the expected loss in test sets against the epochs average
over 10 runs with error bars using the dataset yeast.
0.0	25	5.0	7.S	10.0	12.S	15.0	17.S	0.0	2.5	5.0	7J5	10.0	12.S	15.0	17.S
Test Epochs	Test Epochs
(a) Test Hamming Loss with Expected Loss	(b) Test Hamming Loss with MAP Predictions
Figure 1:	Stronger regularization can help obtain faster convergence and better test performance.
As we can see from the figure, models with no regularization (gray lines in the figure) have higher
loss, and slower convergence rate. As the number of maximum iterations for divergence minimiza-
tion increases, the test loss decreased faster and the final test loss is also lower. This behavior sug-
gests that by adding the regularization term, our learned policies are able to generalize better to test
sets, and the stronger the regularization we impose by taking more divergence minimization steps,
the better the test performance is. The regularization also helps the training algorithm to converge
faster, as shown by the trend.
6.4 Generalization Performance
Our theoretical bounds implies that the generalization performance of our algorithm improves as the
number of training samples increases. We vary the number of passes of training data x was passed
to the logging policy to sample an action y, and vary it in the range 2[1,2,...,8] with log scales.
2°	21	22	23	2*	25 2β 2， 2β
Replay Count
3.3 -r
…3.2 -
d
§
«3.1 -
8
-I
6
•g 3.0 -
E
e
5 ≡∙9 -
φ
2.8 -
2°	21	22	23	2*	25 2β 2， 2β
Replay Count
(a) Test Hamming Loss with Expected Loss	(b) Test Hamming Loss with MAP Predictions
Figure 2:	Neural network policies both have increasing performance with increasing number of
training data, while models with regularization have faster convergence rate and better performance.
10
Under review as a conference paper at ICLR 2018
When the number of training samples in the bandit dataset increases, both models with and without
regularization have an increasing test performance in the expected loss and reaches a relatively stable
level in the end. Moreover, regularized policies have a better generalization performance compared
to the model without regularization constantly. This matches our theoretical intuitions that explicitly
regularizing the variance can help improve the generalization ability, and that stronger regularization
induces better generalization performance. But as indicated by the MAP performance, after the
replay of training samples are more than 24 , MAP prediction performance starts to decrease, which
suggests the models may be starting over-fitting already.
6.5	Training Schemes
In this section, we use some experiments to present the difference in two training schemes: co-
training in Alg. 3 and the easier version Alg. 2. For the second algorithm, we also compare the two
Gumbel-softmax sampling schemes in addition, denoted as Gumbel-softmax, and Straight-Through
(ST) Gumbel-softmax respectively.
6 5
S8sso-∣ 6u-EEeH
0	5	10	15	20	25	30	35	40
Test Epochs
(b) Test Hamming Loss with MAP Predictions
6 5 .
S8SS0, 6u-EEeH
O 5	10	15	20	25	30	35	40
Test Epochs
(a) Test Hamming Loss with Expected Loss
Figure 3: Results from different training schemes suggest that separately minimizing reweighted
loss and divergence is better compared to training the two losses together.
The figures suggest that blending the weighted loss and distribution divergence performs slightly
better than the model without regularization, however, the training is much more difficult compared
to the separate training scheme, as it’s hard to balance the gradient of the two parts of the objective
function. We also observe no significant performance difference between the two sampling schemes
of the Gumbel-softmax.
6.6 Effect of logging policy vs results
In this section, we discuss how the effect of logging policies, in terms of stochasticity and quality,
will affect the learning performance and additional visualizations of other metrics can be found in
the Appendix 7.
As discussed before, the ability of our algorithm to learn an improved policy relies on the stochastic-
ity of the logging policy. To test how this stochasticity affects our learning, we modify the parameter
of h0 by introducing a temperature multiplier α. For CRF logging policies, the prediction is made
by normalizing values ofwTφ(x, y), where w is the model parameter and can be modified by α with
w → αw. As α becomes higher, h0 will have a more peaked distribution, and ultimately become a
deterministic policy with α → ∞.
We varied α in the range of 2[-1,1,...,8], and report the average ratio of expected test loss to the log-
ging policy loss of our algorithms (Y-axis in Fig 4a, where smaller values indicate a larger improve-
ment). We can see that NN polices are performing better than logging policy when the stochasticity
of h0 is sufficient, while after the temperature parameter increases greater than 23 , it’s much harder
and even impossible (ratio Z 1) to learn improved NN policies. We also note here that the Stochas-
ticity doesn’t affect the expected loss values themselves, and the drop in the ratios mainly resulted
11
Under review as a conference paper at ICLR 2018
from the decreased loss of the logging policy h0 . In addition, comparing within NN policies, poli-
cies with stronger regularization have slight better performance against models with weaker ones,
which in some extent shows the robustness of our learning principle.
(a) The effect of stochasticity of h0 vs ratio of expected
test loss
(b) The effect of quality of h0 vs ratio of expected test
loss
Figure 4: a) The decreasing stochasticity of h0 makes it harder to obtain an improved NN policy,
and our regularization can help the model be more robust and achieve better generalization perfor-
mance. b) As h0 improves, the models constantly outperform the baselines, however, the difficulty
is increasing with the quality of h0. Note: more visualizations of other metrics can be found in the
appendix 7.
Finally, we discusses the impact of logging policies to the our learned improved policies. Intuitively,
a better policy that has lower hamming loss can produce bandit datasets with more correct predic-
tions, however, it’s also possible that the sampling biases introduced by the logging policy is larger,
and such that some predictions might not be available for feedbacks. To study the trade-off between
better policy accuracy and the sampling biases, we vary the proportion of training data points used
to train the logging policy from 0.05 to 1, and compare the performance of our improved policies
obtained by in Fig. 4b. We can see that as the logging policy improves gradually, both NN and
NN-Reg policies are outperforming the logging policy, indicating that they are able to address the
sampling biases. The increasing ratios of test expected loss to h0 performance, as a proxy for relative
policy improvement, also matches our intuition that h0 with better quality is harder to beat.
7 Conclusion
In this paper, we started from an intuition that explicitly regularizing variance can help improve the
generalization performance of off-policy learning for logged bandit datasets, and proposed a new
training principle inspired by learning bounds for importance sampling problems. The theoretical
discussion guided us to a training objective as the combination of importance reweighted loss and a
regularization term of distribution divergence measuring the distribution match between the logging
policy and the policy we are learning. By applying variational divergence minimization and Gumbel
soft-max sampling techniques, we are able to train neural network policies end-to-end to minimize
the variance regularized objective. Evaluations on benchmark datasets proved the effectiveness of
our learning principle and training algorithm, and further case studies also verified our theoretical
discussion.
Limitations of the work mainly lies in the need for the propensity scores (the probability an action
is taken by the logging policy), which may not always be available. Learning to estimate propensity
scores and plug the estimation into our training framework will increase the applicability of our algo-
rithms. For example, as suggested by Cortes et al. (2010), directly learning importance weights (the
ratio between new policy probability to the logging policy probability) has comparable theoretical
guarantees, which might be a good extension for the proposed algorithm.
Although the work focuses on off-policy from logged data, the techniques and theorems may be ex-
tended to general supervised learning and reinforcement learning. It will be interesting to study how
12
Under review as a conference paper at ICLR 2018
this training algorithm can work for empirical risk minimization and what generalization bounds it
may have as the future direction of research.
Acknowledgements
We thank Dr. Adith Swaminathan for publishing their codes and answering the authors’ questions.
We’d also like to thank XXX, YYY, ZZZ for their precious comments and TTT for their funding
support.
References
Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming
the monster: A fast and simple algorithm for contextual bandits. In International Conference on
Machine Learning,pp. 1638-1646, 2014.
Arthur Asuncion and David Newman. Uci machine learning repository, 2007.
Dimitris Bertsimas, David B Brown, and Constantine Caramanis. Theory and applications of robust
optimization. SIAM review, 53(3):464-501, 2011.
Dimitris Bertsimas, Vishal Gupta, and Nathan Kallus. Data-driven robust optimization. Mathemat-
ical Programming, pp. 1-58, 2013.
Alina Beygelzimer and John Langford. The offset tree for learning with partial labels. In Pro-
ceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data
mining, pp. 129-138. ACM, 2009.
Leon Bottou, Jonas Peters, JoaqUin QUinonero-Cande山,Denis X Charles, D Max Chickering, Elon
Portugaly, Dipankar Ray, Patrice Simard, and Ed Snelson. Counterfactual reasoning and learning
systems: The example of computational advertising. The Journal of Machine Learning Research,
14(1):3207-3260, 2013.
Corinna Cortes, Yishay Mansour, and Mehryar Mohri. Learning bounds for importance weighting.
In Advances in neural information processing systems, pp. 442-450, 2010.
John Duchi and Hongseok Namkoong. Variance-based regularization with convex objectives. arXiv
preprint arXiv:1610.02581, 2016.
Miroslav Dudk DUmitrU Erhan, John Langford, Lihong Li, et al. Doubly robust policy evaluation
and optimization. Statistical Science, 29(4):485-511, 2014.
Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven distributionally robust optimization
using the wasserstein metric: Performance guarantees and tractable reformulations. arXiv preprint
arXiv:1505.05116, 2015.
Virginie Gabrel, Cecile Murat, and Aurelie Thiele. Recent advances in robust optimization: An
overview. European journal of operational research, 235(3):471-483, 2014.
Rui Gao and Anton J Kleywegt. Distributionally robust stochastic optimization with wasserstein
distance. arXiv preprint arXiv:1604.02199, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Arthur Gretton, Alexander J Smola, Jiayuan Huang, Marcel Schmittfull, Karsten M Borgwardt, and
Bernhard Scholkopf. Covariate shift by kernel mean matching. 2009.
Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. Deep reinforcement learning for
robotic manipulation with asynchronous off-policy updates. In Robotics and Automation (ICRA),
2017 IEEE International Conference on, pp. 3389-3396. IEEE, 2017.
13
Under review as a conference paper at ICLR 2018
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are uni-
Versal approximators. Neural networks, 2(5):359-366, 1989.
Daniel G Horvitz and Donovan J Thompson. A generalization of sampling without replacement
from a finite universe. Journal of the American statistical Association, 47(260):663-685, 1952.
Guido W Imbens. Nonparametric estimation of average treatment effects under exogeneity: A
review. The review of Economics and Statistics, 86(1):4-29, 2004.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv
preprint arXiv:1611.01144, 2016.
Nan Jiang and Lihong Li. Doubly robust off-policy value evaluation for reinforcement learning.
arXiv preprint arXiv:1511.03722, 2015.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
John Langford, Alexander Strehl, and Jennifer Wortman. Exploration scavenging. In Proceedings
of the 25th international conference on Machine learning, pp. 528-535. ACM, 2008.
Lihong Li, Remi Munos, and Csaba Szepesvari. Toward minimax off-policy value estimation. 2015.
Ashique Rupam Mahmood, Huizhen Yu, and Richard S Sutton. Multi-step off-policy learning with-
out importance sampling ratios. arXiv preprint arXiv:1702.03006, 2017.
Remi Munos, Tom StePIeton, Anna Harutyunyan, and Marc Bellemare. Safe and efficient off-policy
reinforcement learning. In Advances in Neural Information Processing Systems, pp. 1054-1062,
2016.
XuanLong Nguyen, Martin J Wainwright, and Michael I Jordan. Estimating divergence functionals
and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory,
56(11):5847-5861, 2010.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural sam-
plers using variational divergence minimization. In Advances in Neural Information Processing
Systems, pp. 271-279, 2016.
Alfred Renyi et al. On measures of entropy and information. In Proceedings ofthe Fourth Berkeley
Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of
Statistics. The Regents of the University of California, 1961.
Paul R Rosenbaum and Donald B Rubin. The central role of the propensity score in observational
studies for causal effects. Biometrika, 70(1):41-55, 1983.
Soroosh Shafieezadeh-Abadeh, Peyman Mohajerin Esfahani, and Daniel Kuhn. Distributionally
robust logistic regression. In Advances in Neural Information Processing Systems, pp. 1576-
1584, 2015.
Pannagadatta Shivaswamy and Thorsten Joachims. Multi-armed bandit problems with history. In
Artificial Intelligence and Statistics, pp. 1046-1054, 2012.
Alex Strehl, John Langford, Lihong Li, and Sham M Kakade. Learning from logged implicit explo-
ration data. In Advances in Neural Information Processing Systems, pp. 2217-2225, 2010.
Masashi Sugiyama, Matthias Krauledat, and Klaus-Robert MAzller. Covariate shift adaptation by
importance weighted cross validation. Journal of Machine Learning Research, 8(May):985-1005,
2007.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, volume 1. MIT
press Cambridge, 1998.
Adith Swaminathan and Thorsten Joachims. Counterfactual risk minimization: Learning from
logged bandit feedback. In International Conference on Machine Learning, pp. 814-823, 2015a.
14
Under review as a conference paper at ICLR 2018
Adith Swaminathan and Thorsten Joachims. The self-normalized estimator for counterfactual learn-
ing. In Advances in Neural Information Processing Systems, pp. 3231-3239, 2015b.
Philip Thomas and Emma Brunskill. Data-efficient off-policy policy evaluation for reinforcement
learning. In International Conference on Machine Learning, pp. 2139-2148, 2016.
Yu-Xiang Wang, Alekh Agarwal, and Miroslav Dudik. Optimal and adaptive off-policy evaluation
in contextual bandits. In International Conference on Machine Learning, pp. 3589-3597, 2017.
Huan Xu, Constantine Caramanis, and Shie Mannor. Robust regression and lasso. In Advances in
Neural Information Processing Systems, pp. 1801-1808, 2009.
Bianca Zadrozny, John Langford, and Naoki Abe. Cost-sensitive learning by cost-proportionate
example weighting. In Data Mining, 2003. ICDM 2003. Third IEEE International Conference
on, pp. 435-442. IEEE, 2003.
A. Proofs
Proof for Theorem 1
Proof. Let z = (x, y), p(z) = P (x)h(y|x), p0(z) = P(x)h0(y|x), and l(z) = δ(x, y) ∈ [0, L]. We
apply Lemma 1 to z, importance sampling weight function w(z) = p(z)/p0 (z) = h(y|x)/h0(y|x),
and loss l(z)/L, we have
Ex〜P(X),y〜ho(y∣x)[w (y|X)δ (X, y)/L ]
=Ez 〜po(z)[w2(z)l2(z)∕L2]
≤ d2(p(z)||p0(z))
=	Z -p(z)p(z)dz
z p0 (z )
= h h(yR h(y∣χ)P (x)dxdy
x,yh0(y|X)
= d2(h(y|X)||h0(y|X))P(X)dX
x
= d2(h(y|X)||h0(y|X);P(X))
Thus, we have
Ez〜po(z)[w2(z)l2(z)] ≤ L2d2(p∣∣P0)
□
Proof for Theorem 3.2
Proof. For a single hypothesis denoted as δ with values δi = δ(zi), let Z = wh(z)l(z) - R(h), then
|Z| ≤ LM. By Lemma 1, the variance can be bounded using Reni divergence as
Var(Z)= Ez〜p0(z)[w2(z)l2(z)] - Rh ≤ L2d2(p∣∣P0) = Rh
Applying Bernstein’s concentration bounds we have
-m2/2
Pr(R(h0) - R(h) ")≤ 呻(i1lM∕3
Let η = exp( σ2(zm∕M/3), We can obtain that with probability at least 1 - η, the following bounds
for importance sampling of bandit learning holds
Rh0
LM log 1∕η	L2M2 log2 1∕η
Rh + —3m — + V ―9m—
+ 2L2 Var(Z)log1∕η
Rh +
2LM log1∕η
3m
2L2V ar(Z) log 1∕η
,where the second inequality comes from the fact that √a + b ≤ √α + √b.
(16)
□
≤
≤
+
m
m
15
Under review as a conference paper at ICLR 2018
Training Algorithm for Original ERM Formulation
Data: D = {xi, yi}iN=1 sampled from logging policy h0; regularization hyper-parameter λ
Result: An optimized generator h^(y∣x) that is an approximate minimizer of R(W)
initialization;
while Not Converged do
/* Update discriminator	*/
Sample a mini-batch of ‘fake' samples (xi, yi) with Xi from D and yi 〜h# (y|xi);
Sample a mini-batch of ‘real’ samples (xi, yi) from D ;
Update wt+1 = wt + ηw∂F(Tw, hθ )(10) ;
/* Update generator	*/
Sample a mini-batch ofm samples from D ;
Estimate the reweighted loss as RR = 1- P hθ (yilxi) δi and get the gradient as g1 = ∂q Rt ;
m	pi
i=1
Sample a mini-batch ofm1 ‘fake’ samples ;
Estimate the generator gradient as g2 = F(Tw, hθ )(10) ;
Update θt+1 = θt - ηθ (g1 + λg2) ;
end
Algorithm 3: Minimizing Variance Regularized Risk - Co-Training Version
A.	Statistics of Benchmark Datasets
We report the statistics of the datasets as in the following table. For the latter two datasets TMC,
Table 2: Dataset Statistics
Name	# Features	# Labels	# Train	# Test
Yeast	103	14	1500	-917-
Scene	294	6	1211	1196
TMC	30438	22	21519	7077
LYRL	47236	4	23149	781265
LYRL, as they have sparse features with high dimension of features, we first reduced their feature
dimensions to 1000 via truncated singular value decomposition (latent semantic analysis).
B.	Network Structures
For policy networks, we use the following simple network structure
Linear -> BatchNorm -> ReLU -> Linear -> BatchNorm
-> ReLU -> Linear -> Sigmoid
, mainly because of features used in the datasets don’t invole imaging or original text features. For
the discriminator, a three-layer design is
Linear -> BatchNorm -> ReLU -> Linear -> BatchNorm -> ReLU -> Linear
C.	Supplement Figures to Section 6.6
16
Under review as a conference paper at ICLR 2018
6 5 5 4 4a,a
(dxa) SSO-I βuμ∣uIeHISel
2,	22	23
DetBnrinisUdtyoffte，
2,	22	2,
Deterministic® of ∕⅛ ,
2,	2,
2^'	20	2'	22	2,	2*	25
Detsnnnsticity of ∕⅛ ,
(a) The effect of stochasticity of h0 (b) The effect of stochasticity of h0 (c) The effect of stochasticity of h0
vs expected test loss	vs test loss with MAP predictions vs ratio of test loss with MAP
Figure 5: As the logging policy becomes more deterministic, NN policies are still able to find
improvement over h0 in a) expected loss and b) loss with MAP predictions. c) We cannot observe
a clear trend in terms of the performance of MAP predictions. We hypothesize it results from that
h0 policy already has good MAP prediction performance by centering some of the masses. While
NN policies can easily pick up the patterns, it will be difficult to beat the baselines. We believe this
phenomenon worth further investigation.
(a) The quality of h0 vs ratio of ex- (b) The quality of h0 vs ratio of ex- (c) The quality of h0 vs ratio of ex-
pected test loss	pected test loss	pected test loss with MAP
Figure 6: a) As the quality of the logging policy increases, NN policies are still able to find improve-
ment over h0 in expected loss. and b) c) For MAP predictions, however, it will be really difficult for
NN policies to beat if the logging policy was already exposed to full training data and trained in a
supervised fashion.
17