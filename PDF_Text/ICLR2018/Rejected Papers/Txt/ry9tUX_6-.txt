Entropy-SGD optimizes the prior
of a PAC-Bayes bound: Data-dependent PAC-
Bayes priors via differential privacy
Anonymous authors
Paper under double-blind review
Ab stract
We show that Entropy-SGD (Chaudhari et al., 2017), when viewed as a learning
algorithm, optimizes a PAC-Bayes bound on the risk of a Gibbs (posterior) clas-
sifier, i.e., a randomized classifier obtained by a risk-sensitive perturbation of the
weights of a learned classifier. Entropy-SGD works by optimizing the bound’s
prior, violating the hypothesis of the PAC-Bayes theorem that the prior is chosen
independently of the data. Indeed, available implementations of Entropy-SGD
rapidly obtain zero training error on random labels and the same holds of the
Gibbs posterior. In order to obtain a valid generalization bound, we show that
an ε-differentially private prior yields a valid PAC-Bayes bound, a straightfor-
ward consequence of results connecting generalization with differential privacy.
Using stochastic gradient Langevin dynamics (SGLD) to approximate the well-
known exponential release mechanism, we observe that generalization error on
MNIST (measured on held out data) falls within the (empirically nonvacuous)
bounds computed under the assumption that SGLD produces perfect samples. In
particular, Entropy-SGLD can be configured to yield relatively tight generaliza-
tion bounds and still fit real labels, although these same settings do not obtain
state-of-the-art performance.
1	Introduction
Optimization is central to much of machine learning, but generalization is the ultimate goal. Despite
this, the generalization properties of many optimization-based learning algorithms are poorly under-
stood. The standard example is stochastic gradient descent (SGD), one of the workhorses of deep
learning, which has good generalization performance in many settings, even under overparametriza-
tion (Neyshabur, Tomioka, and Srebro, 2014), but rapidly overfits in others (Zhang et al., 2017). Can
we develop high performance learning algorithms with provably strong generalization guarantees?
Or is their a limit?
In this work, we study an optimization algorithm called Entropy-SGD (Chaudhari et al., 2017),
which was designed to outperform SGD in terms of generalization error when optimizing an empir-
ical risk. Entropy-SGD minimizes an objective f : Rp → R indirectly by performing (approximate)
stochastic gradient ascent on the so-called local entropy
F (W) =f C + log Eξ 〜N 怙-f(w+ξ)] =ef C + log ∕exp(-f(w + X)) N (dɪ),	(1)
where C is a constant andN denotes a zero-mean isotropic multivariate normal distribution on Rp.
Our first contribution is connecting Entropy-SGD to results in statistical learning theory, show-
ing that maximizing the local entropy corresponds to minimizing a PAC-Bayes bound (McAllester,
1999) on the risk of the so-called Gibbs posterior. The distribution of w + ξ is the PAC-Bayesian
“prior”, and so optimizing the local entropy optimizes the bound’s prior. This connection between
local entropy and PAC-Bayes follows from a result due to Catoni (2007, Lem. 1.1.3) in the case of
bounded risk. (See Theorem 4.1.) In the special case where f is the empirical cross entropy, the
local entropy is literally a Bayesian log marginal density. The connection between minimizing PAC-
Bayes bounds under log loss and maximizing log marginal densities is the subject of recent work
Under review as a conference paper at ICLR 2018
by Germain et al. (2016). Similar connections have been made by Zhang (2006a); Zhang (2006b);
Grunwald (2012); GrnnWaId and Mehta (2016).
Despite the connection to PAC-Bayes, as well as theoretical results by Chaudhari et al. suggesting
that Entropy-SGD may be more stable than SGD, we demonstrate that Entropy-SGD (and its corre-
sponding Gibbs posterior) can rapidly overfit, just like SGD. We identify two changes, motivated by
theoretical analysis, that suffice to control generalization error, and thus prevent overfitting.
The first change relates to the stability of optimizing the prior mean. The PAC-Bayes theorem
requires that the prior be independent of the data, and so by optimizing the prior mean, Entropy-
SGD invalidates the bound. Indeed, the bound does not hold empirically. While a PAC-Bayes prior
may not be chosen based on the data, it can depend on the data distribution. This suggests that if the
prior depends only weakly on the data, it may be possible to derive a valid bound.
We formalize this intuition using differential privacy (Dwork, 2006; Dwork et al., 2015b). By modi-
fying the cross entropy loss to be bounded and replacing SGD with stochastic gradient Langevin
dynamics (SGLD; Welling and Teh, 2011), the data-dependent prior mean can be shown to be
(ε, δ)-differentially private (Wang, Fienberg, and Smola, 2015; Minami et al., 2016). We refer
to the SGLD variant as Entropy-SGLD. Using results connecting statistical validity and differential
privacy (Dwork et al., 2015b, Thm. 11), we show that an ε -differentially private prior mean yields a
valid, though looser, generalization bound using the PAC-Bayes theorem. (See Theorem 5.4.)
A gap remains between pure and approximate differential privacy. Under some technical condi-
tions, in the limit as the number of iterations diverges, the distribution of SGLD’s output is known to
converge weakly to the corresponding stationary distribution, which is the well-known exponential
mechanism in differential privacy (Teh, Thiery, and Vollmer, 2016, Thm. 7). Weak convergence,
however, falls short of implying that SGLD achieves pure ε -differential privacy. We proceed un-
der the approximation that SGLD enjoys the same privacy as the exponential release mechanism,
and apply our ε -differentially private PAC-Bayes bound. We find that the corresponding 95% con-
fidence intervals are reasonably tight but still conservative in our experiments. While the validity
of our bounds are subject to our approximation, the bounds give us a view as to the limitations
of combining differential privacy with PAC-Bayes bounds: when the privacy of Entropy-SGLD is
tuned to contribute no more than 2ε2 × 100 ≈ 0.2% to the generalization error, the test error of
the learned network is 3-8%, which is approximately 5-10 times higher than the state of the art,
which for MNIST is between 0.2-1%, although the community has almost certainly overfit its net-
works/learning rates/loss functions/optimizers to MNIST. We return to these points in the discussion.
The second change pertains to the stability of the stochastic gradient estimate made on each iteration
of Entropy-SGD. This estimate is made using SGLD. (Hence Entropy-SGD is SGLD within SGD.)
Chaudhari et al. make a subtle but critical modification to the noise term in SGLD update: the noise
is divided by a factor that ranges from 103 to 104. (This factor was ostensibly tuned to produce good
empirical results.) Our analysis shows that, as a result of this modification, the Lipschitz constant of
the objective function is approximately 106-108 times larger, and the conclusion that the Entropy-
SGD objective is smoother than the original risk surface no longer stands. This change to the noise
also negatively impacts the differential privacy of the prior mean. Working backwards from the
desire to obtain tight generalization bounds, we are led to divide the SGLD noise by a factor of
only √m,, where m is the number of data points. (For MNIST, √m ≈ 16.) The resulting bounds are
nonvacuous and tighter than those recently published by Dziugaite and Roy (2017), although it must
be emphasized that the bound presented here hold subject to the approximation concerning privacy
of the prior mean, which is certainly violated but to an unknown degree.
We begin with a review of some related work, before introducing sufficient background so that we
can make a formal connection between local entropy and PAC-Bayes bounds. We then introduce
a differentially private PAC-Bayes bound. In Section 6, we present experiments on MNIST which
provide evidence for our theoretical analysis. (Empirical validation is required in order to address
the aforementioned gap between pure and approximate differential privacy.) We close with a short
discussion.
2
Under review as a conference paper at ICLR 2018
2	Related work
This work was inspired in part by Zhang et al. (2017), who highlight empirical properties of SGD
that were not widely appreciated within the theory community, and propose a simple linear model to
explain the phenomenon. They observe that, without regularization, SGD can achieve zero training
error on MNIST and CIFAR, even if the labels are chosen uniformly at random. At the same time,
SGD obtains weights with very small generalization error with the original labels. The first obser-
vation is strong evidence that the set of classifier accessible to SGD within a reasonable number of
iterations is extremely rich. Indeed, with probability almost indistinguishable from one, fitting ran-
dom labels on a large data set implies that the Rademacher complexity of this effective hypothesis
class is essentially the maximum possible (Bartlett and Mendelson, 2003, Thm. 11).
The second observation suggests that SGD is performing some sort of capacity control. Zhang
et al. show that SGD obtains the minimum norm solution for a linear model, and thus performs
implicit regularization. They suggest a similar phenomenon may occur when using SGD to training
neural networks. Indeed, earlier work by Neyshabur, Tomioka, and Srebro (2014) observed similar
phenomena and argued for the same point: implicit regularization underlies the ability of SGD to
generalize, even under massive overparametrization. Subsequent work by Neyshabur, Tomioka, and
Srebro (2015) introduced “path” norms as a better measure of the complexity of ReLU networks.
Despite progress, these new norms have not yet lead to nonvacuous generalization bounds (Dziugaite
and Roy, 2017, App. D).
There has been recent progress: Dziugaite and Roy (2017) describe PAC-Bayes bounds, built by
perturbing the weights learned by SGD. (The authors were motivated in part by Entropy-SGD and
empirical findings relating to “flat minima”.) Their bounds are controlled by 1) the “flatness” of
empirical risk surface near the SGD solution and 2) the L2 distance between the learned weights
and the random initialization. The bounds are also found to be numerically nonvacuous. (We re-
turn to this aspect below.) Similar bounds are studied in further depth by Neyshabur et al. (2017b).
Recent advances have also identified new spectral norm bounds that correlate closely with general-
ization error and distinguish between true and random labels (Bartlett, Foster, and Telgarsky, 2017;
Neyshabur et al., 2017a).
Our work and Entropy-SGD both connect to early work by Hinton and Camp (1993) and Hochreiter
and Schmidhuber (1997), which introduced regularization schemes based on information-theoretic
principles. These ideas, now referred to as “flat minima”, were related to minimizing PAC-Bayes
bounds by Dziugaite and Roy (2017), although these bounds are minimized with respect to the pos-
terior, not the prior, as is done by Entropy-SGD. Achille and Soatto (2017) provide an information-
theoretic argument for a generalization of the objective of Hinton and Camp. Their objective takes
the form of regularized empirical cross entropy
R S (Q) + β KL( Q Il P),	⑵
where Q and P are the prior and posterior on the weights, respectively. For an appropriate range of
β , linear PAC-Bayes bounds are exactly of this form. In Achille and Soatto (2017) they empirically
observe that varying β correlates with a degree of overfitting on a random label dataset. Achille
and Soatto (2017) also highlight the connections with variational inference (Kingma, Salimans, and
Welling, 2015).
Our work also relates to renewed interest in nonvacuous generalization bounds (Langford, 2002;
Langford and Caruana, 2002), i.e., bounds on the numerical difference between the unknown classi-
fication error and the training error that are (much) tighter than the tautological upper bound of one.
Recently, Dziugaite and Roy (2017) demonstrated nonvacuous generalization bounds for random
perturbations of SGD solutions using PAC-Bayes bounds for networks with millions of weights.
(The algorithm can be viewed as variational dropout (Kingma, Salimans, and Welling, 2015), with
a proper data-dependent prior but without local reparametrization.) Their work builds on the core
insight demonstrated nearly 15 years ago by Langford and Caruana (2002), who computed nonvac-
uous bounds for neural networks five orders of magnitude smaller.
A key aspect of our analysis relies on the stability of a data-dependent prior. Stability has long
been understood to relate to generalization (Bousquet and Elisseeff, 2002). Our analysis of Entropy-
SGLD rests on results in differential privacy (see (Dwork, 2008) for a survey) and its connection to
generalization (Dwork et al., 2015b; Dwork et al., 2015a; Bassily et al., 2016; Oneto, Ridella, and
3
Under review as a conference paper at ICLR 2018
Anguita, 2017), which can be viewed as a particularly stringent notion of stability. Entropy-SGLD
is an instance of differentially private empirical risk minimization, which is well studied, both in
the abstract (Chaudhuri, Monteleoni, and Sarwate, 2011; Kifer, Smith, and Thakurta, 2012; Bassily,
Smith, and Thakurta, 2014) and in the particular setting of private training via SGD (Bassily, Smith,
and Thakurta, 2014; Abadi et al., 2016). Our analysis also relates to the differential privacy of
Bayesian and Gibbs posteriors, and approximate sampling algorithms (Mir, 2013; Bassily, Smith,
and Thakurta, 2014; Dimitrakakis et al., 2014; Wang, Fienberg, and Smola, 2015; Minami et al.,
2016).
In effect, our differentially private PAC-Bayes bound uses a data-distribution-dependent prior, which
are permitted in the PAC-Bayesian framework. (Priors must be independent of the data sample,
however. Differential privacy allows us to extract information about the distribution from a sample
while maintaining statistical validity (Dwork et al., 2015b).)
There is a growing body of work in the PAC-Bayes literature on data-distribution-dependent priors.
Write S for a data sample and Q(S) for a data-dependent PAC-Bayesian posterior (i.e., Q : Zm →
M1(Rp) is a fixed learning algorithm for a randomized classifier). Catoni (2007) makes an extensive
study of data-distribution-dependent priors of the form P * = P *( Q) =f E S 〜D m [ Q (S)]. While SUchPri-
ors were known to minimize the KL term in expectation, Catoni was the first to derive PAC-Bayes
excess risk bounds using these priors: focusing on Gibbs posteriors Q(S) = QP(S) = PexP(-τRS)
for some fixed measure P, Catoni derives bounds on the complexity term KL(QP(S)||P*(QP)) that
hold uniformly over all possible data distributions D. Catoni calls such priors and bounds “lo-
cal”. Lever, Laviolette, and Shawe-Taylor (2013) extend this approach to generalization bounds and
consider both data-independent and data-dependent choices for P. In the later case, P = P(S) and
the generalization bound uses the local prior P*(QP) = ES〜Dm [QP(S)(S)]. In our work, We make
a data-dependent but private choice of the prior P = P(S), and then use our differentially private
PAC-Bayes generalization bound to control the generalization error of the associated Gibbs posterior
QP(S) in terms of KL(QP(S)||P). We also evaluated differentially private versions of local bounds,
where the complexity term is a uniform bound on KL(QP(S)||P*(QP)). The bounds were virtually
indistinguishable, and so we do not report them here.
3	Preliminaries: Supervised learning, Entropy- S GD, and
PAC-Bayes
Let Z be a measurable space, let D be an unknown distribution on Z, and consider the batch su-
pervised learning setting under a loss function bounded below: having observed S 〜Dm, i.e., m
independent and identically distributed samples from D, we aim to choose a predictor, parameter-
ized by weight vector w ∈ Rp , with minimal risk
RD (w) d=ef E (`(w,z)),
Z ZD
(3)
where ` : Rp × Z → R is measurable and bounded below. (We ignore the possibility of constraints
on the weight vector for simplicity.) We will also consider randomized predictors, represented by
probability measures Q ∈ M1 (Rp) on Rp, whose risks are defined via averaging,
[RD(Q) d=ef E (RD(w)) = E E (`(w,z)) ,
wZQ	zZD wZQ
(4)
where the second equality follows from Fubini’s theorem and the fact that ` is bounded below.
Let S = (Z ι,..., Zm) and let D = mm ∑=ι δZ/ be the empirical distribution. Given a weight distribution
Q, such as that chosen by a learning algorithm on the basis of data S, its empirical risk
1m
RS( q)=R D( q ) = mm ∑ wE Q C(W, Z))，
(5)
will be studied as a stand-in for its risk, which we cannot compute. While RS (Q) is easily seen
to be an unbiased estimate of RD (Q) when Q is independent of S, our goal is to characterize the
(one-sided) generalization error RD (Q) 一 RS (Q) when Q is random and dependent on S.
One of our focuses will be on classification, where Z = X × K, with K a finite set of classes/labels.
A product measurable (in practice, continuous) function f : Rp × X → K maps weight vectors w
4
Under review as a conference paper at ICLR 2018
to classifiers f (w, ∙) : X → K. The loss function is given by '(w, (ɪ, y)) = g(f (w, X), y) for some
g : K × K → R. In this setting, 0-1 loss corresponds to g(y', y) = 1 if and only if y' = y. In binary
classification, we take K = {0, 1}.
We will also consider parametric families of probability-distribution-valued classifiers f : Rp × X →
[0, 1]K. For every input X ∈ X, the output f(w, X) specifies a probability distribution on K. In this
setting, `(w, (X, y)) = g(f(w, X), y) for some g : [0, 1]K × K → R. The standard loss is then the cross
entropy, given by g((p1, . . . , pK),y) = -logpy. (Under cross entropy loss, the empirical risk is, up
to a multiplicative constant, a negative log likelihood.) In the special case of binary classification,
the output can be represented simply by an element of [0, 1], i.e., the probability the label is one.
The binary cross entropy, 'bce, is given by g(P,y) = -ylog(P) - (1 - y) log(1 - P). Note that cross
entropy loss is merely bounded below. We will consider bounded modifications in Appendix B.2.
We will sometimes refer to elements of RP and M1 (RP) as classifiers and randomized classifiers,
respectively. Likewise, we will often refer to the (empirical) risk as the (empirical) error.
3.1	ENTROPY-SGD
Entropy-SGD is a gradient-based learning algorithm proposed by Chaudhari et al. (2017) as an
alternative to stochastic gradient descent on the empirical risk surface RS. The authors argue that
Entropy-SGD has better generalization performance and provide some empirical evidence. Part of
that argument is a theoretical analysis of the smoothness of the local entropy surface that Entropy-
SGD optimizes in place of the empirical risk surface, as well as a uniform stability argument that
they admit rests on assumptions that are violated, but to a small degree empirically. As we have
mentioned in the introduction, Entropy-SGD’s modifications to the noise term in SGLD result in
much worse smoothness. We will modify Entropy-SGD in order to stabilize its learning and, up to
some approximations, provably control overfitting.
Entropy-SGD is stochastic gradient ascent applied to the optimization problem:
arg max Fγ,τ(w;S), where F,τ(w;S) = log	exp(-τRS(w0) - TY∣∣w0- w∣∣2)dw0.	(6)
w∈RP	RP	2
The objective FY,τ (∙ ； S) is known as the local entropy, and can be viewed as the log partition function
of the unnormalized probability density function
w0→ exp(-τRS(w0) - TY∣∣w0- w∣2).
(We will denote the corresponding distribution by Gγw,,τS.) Assuming that one can exchange differen-
tiation and integration, it is straightforward to verify that
VW Fγ,τ(w; S) =	EW S(TY (W - w0)),	(8)
w0〜G WS
and then the local entropy FY,τ (∙ ； S) is even differentiable, even if the empirical risk RS is not. Indeed,
Chaudhari et al. show that the local entropy and its derivative are Lipschitz. Chaudhari et al. argue
informally that maximizing the local entropy leads to “flat minima” in the empirical risk surface,
which several authors (Hinton and Camp, 1993; Hochreiter and Schmidhuber, 1997; Baldassi et al.,
2015; Baldassi et al., 2016) have argued is tied to good generalization performance (though none of
these papers gives generalization bounds, vacuous or otherwise).1
Chaudhari et al. propose a Monte Carlo estimate of the gradient,
VWFY,τ(w;S) ≈	TY(w - μL),	with μι	=	wι	and μj +1	=	αWj +	(1 -	α)μj,	(9)
1 The local entropy should not be confused with the smoothed risk surface obtained by convolution with
a Gaussian kernel: in that case, every point on this surface represents the average risk of a network obtained
by perturbing the network parameters according to a Gaussian distribution. The local entropy also relates
to a perturbation, but the perturbation is either accepted or rejected based upon its relative performance (as
measured by the exponentiated loss) compared with typical perturbations. Thus the local entropy perturbation
concentrates on regions of weight space with low empirical risk, provided they have sufficient probability
mass under the distribution of the random perturbation. Section 4 yields further insight into the local entropy
function.
5
Under review as a conference paper at ICLR 2018
Algorithm 1 One step of Entropy-SG(L)D along the local entropy gradient
Input:
w ∈ Rp
S ∈ Zm
. Current weight
. Data
':RP X Z → R
. Loss
τ,γ,η,η0,L,K
. Parameters
Output: Weight vector w moved along stochastic gradient
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
procedure ENTROPY-SG(L)D-STEP
w0, μ J W
for i ∈ {1, ..., L} do
η i J η 0/i
(zj1 , . . . , zjK ) J sample size K minibatch from S
dw0 J K ∑K=ι Vw0'(w0, zji) 一 γτ(w0 — w)
w0 J w0- 2 η，dw0 + p∕ηfN(0, Ip)
μ J (1 — α)μ + αw0
W J W — 2 ητγ(w — μ) + η /β N(0, Ip)	i
. Run SGLD for L iterations.
return w
X-------V--------}
Entropy-SGLD only
.Step along stochastic local entropy V
where W01, W02, . . . are (approximately) i.i.d. samples from GγW,,τS and α ∈ (0, 1) defines a weighted
average. Obtaining samples from GγW,,τS may be difficult when the dimensionality of the weight
vector is large. Chaudhari et al. use Stochastic Gradient Langevin Dynamics (SGLD; Welling and
Teh, 2011), which generates an exact sample in the limit of infinite computation and requires that
the empirical risk be differentiable.2 The final output of Entropy-SGD is the deterministic predictor
corresponding to the final weights w* achieved by several epochs of optimization.
Algorithm 1 gives a complete description of the stochastic gradient step performed by Entropy-SGD.
If We rescale the learning rate, η0 J 2η0τ, lines 6 and 7 are equivalent to
6:	dw0 J K ∑K=I Vw0'(w0, Zji) — Y(w0 — w)
7:	w0 J w0 — η idw + ,2η0∕τ N (o, ip)
Notice that the noise term is multiplied by a factor of ,2∕τ. This follows from the definition of the
local entropy. A multiplicative factor ε —called the “thermal noise”, but playing exactly the same
role as ,2∕τ here—appears in the original description of the Entropy-SGD algorithm given by
Chaudhari et al. However, ε does not appear in the definition of local entropy used in their stability
analysis. Our derivations highlights that the scaling the noise term in SGLD update has a profound
effect: the thermal noise exponentiates the density that defines the local entropy. The smoothness
analysis of Entropy-SGD does not take into consideration the role of ε, which is critical because
Chaudhari et al. take ε to be as small as 10—3 and 10—4. Indeed, the conclusion that the local
entropy surface is smoother no longer holds. We will see that τ controls the differential privacy and
thus the generalization error of Entropy-SGD.
3.2 KL divergence and the PAC -Bayes theorem
Let Q, P be probability measures defined on Rp , assume Q is absolutely continuous with respect to
P, and write dQ : RP → R+ ∪ {∞} for some Radon-Nikodym derivative of Q with respect to P. Then
the Kullback-Liebler divergence (or relative entropy) of P from Q is defined to be
KL( Q Il P) =f ∕logd! d Q.	(10)
For p,q ∈ [0, 1], we will abuse notation and define
KL( q Il P) =KL(B( q )∣∣B( P))= q log q + (1 — q) log1 ~q,	(11)
p	1—p
2 Chaudhari et al. take L = 20 steps of SLGD, using a constant step size η0j = 0.2 on iteration j, and
weighting α = 0.75. It seems unlikely that these settings produce high quality samples.
6
Under review as a conference paper at ICLR 2018
where B(p) denotes the Bernoulli distribution on {0, 1} with mean p.
We now present a PAC-Bayes theorem, first established by McAllester (1999). We focus on the
setting of bounding the generalization error of a (randomized) classifier on a finite discrete set of
labels K. The following variation is due to Langford and Seeger (2001) for 0-1 loss (See also
(Langford, 2002) and (Catoni, 2007).)
Theorem 3.1 (PAC-BayeS (McAllester, 1999; Langford and Seeger, 2001)). Under 0-1 loss, for
every δ > 0, m ∈ N, distribution D on Rk × K, and distribution P on Rp,
PIJI((VQ) KL(RS(Q)||RD(Q)) ≤ KL(Q)，+“除)≥ 1 - δ.	(12)
SZDm	m — 1
We will also use the following variation of a PAC-Bayes bound, where we consider any bounded
loss function.
Theorem 3.2 (Linear PAC-Bayes Bound (McAllester, 2013; Catoni, 2007)). Fix λ > 1/2 and as-
sume the loss takes values in an interval of length Lmax. For every δ > 0, m ∈ N, distribution D on
Rk × K, and distribution P on Rp,
Pm((∀Q) RD(Q) ≤ -1r (R^S(Q) + λLmax(KL(Q||P)+ log1))) ≥ 1 -δ.	(13)
SZD ∖	1 — 2. ∖	mi	δ j /
We introduce several additional generalization bounds when we introduce differential entropy.
4 Maximizing local entropy minimizes a PAC-Bayes bound
We now present our first contribution, a connection between the local entropy and PAC-Bayes
bounds. We begin with some notation for Gibbs distributions. For a measure P on Rp and function
g : Rp → R, let P[g] denote the expectation R g(h)P(dh) and, provided P[g] < ∞, let Pg denote the
probability measure on Rp , absolutely continuous with respect to P, with Radon-Nikodym deriva-
tive dp (h) = gP[g^. A distribution of the form Pexp(-τg) is generally referred to as a GibbS distribution.
In the special case where P is a probability measure, We call Pexp(-τRS) a “Gibbs posterior”.
Theorem 4.1 (Maximizing local entropy optimizes a PAC-Bayes bound’s prior). Assume the loss
function takes values in an interval of length Lmαx, let T = .m^ for Some λ > 1/2, and let P be a
multivariate normal distribution with mean w and covariance matrix (τγ)-1Ip. Then maximizing
the local entropy FY,τ (w; S) with respect to W is equivalent to minimizing a linear PAC-Bayes bound
(Theorem 3.2) on the risk RD(GWS) of the Gibbs posterior GWS = Pexp(-τRS), where the bound is
optimized with respect to the mean W ofP.
Proof. Let m, δ, D, and P be as in Theorem 3.1 and let S Z Dm. The linear PAC-Bayes bound
(Theorem 3.2) ensures that for any fixed . > 1/2 and bounded loss function, with probability at
least 1 - δ over the choice of S, the bound
(1- 2λ) λ mRD(Q) ≤ λ mRS(Q)+KL(QIIP)+g(δ).	(14)
2.	.Lm
ax	.Lm
ax
holds for all Q ∈ M1 (Rp). Minimizing the upper bound on the risk RD (Q) of the randomized
classifier Q is equivalent to the program
infQ E (r(h)) + KL(QIIP)	(15)
hZQ
with r(h) = λm;RS(h). By (Catoni, 2007, Lem. 1.1.3), for all Q ∈ M1(RP) with KL(Q∣∣P) < ∞,
—log P [exp(—r)] = E (r (h)) + KL( Q ∣∣ P) — KL( Q ∣∣ Pexp(-r)).	(16)
hZQ
Using Eq. (16), we may reexpress Eq. (15) as
infQ KL(QIIPexp(—r)) — log P[exp(—r)].	(17)
7
Under review as a conference paper at ICLR 2018
By the nonnegativity of the KUllback-Liebler divergence, the infimum is achieved when the KL
term is zero, i.e., when Q = Pexp(-r) . Then
(1 -次)4Lm RD(%xp(-r)) ≤ -logP[exp(-r)]+ g(δ).	(18)
Finally, it is plain to see that FY ,τ(w; S) = C + log P [exp(- r)] when C = ∖ P log(2π (τγ)-1) is a
constant, T = jLm^, and P = N(w, (τγ)-1 IP) is a multivariate normal with mean W and covariance
matrix (τγ)-1 I.	□
The analysis falls short when the loss function is unbounded, because the PAC-Bayes bound we
have used applies only to bounded loss functions. Germain et al. (2016) described PAC-Bayes
generalization bounds for unbounded loss functions. (See Grunwald and Mehta (2016) for related
work on excess risk bounds and further references). For their bounds to be evaluated on the negative
log likelihood loss, one needs some knowledge of the data distribution in order to approximate
certain statistics of the deviation of the empirical risk RS(W) from true risk RD(W).
5	Data-dependent PAC-Bayes priors via differential privacy
Theorem 4.1 reveals that Entropy-SGD is optimizing a PAC-Bayes bound with respect to the prior.
As a result, the prior P depends on the sample S, and the hypotheses of the PAC-Bayes theorem
(Theorem 3.1) are not met. Naively, it would seem that this interpretation of Entropy-SGD cannot
explain its ability to generalize. Using tools from differential privacy (Dwork, 2006), we show that
if the prior term is optimized in a differentially private way, then a PAC-Bayes theorem still holds,
at the cost of a slightly looser bound. We will assume basic familiarity with differential privacy, but
give basic definitions and results in Appendix A. We use the notation A : Z T for a (randomized)
algorithm that takes as input an element in Z and produces an output in T .
The key result we will employ is due to Dwork et al. (2015b, Thm. 11).
Theorem 5.1. Let m ∈ N, let A : Zm T, let D be a distribution over Z, let β ∈ (0, 1), and, for
each t ∈ T, fix a set R (t) ⊆ Zm such that P S ZDm (S ∈ R (t)) ≤ β. If A is ε-differentially private for
ε ≤ pln(1∕β)/(2m), then PSZDm(S ∈ R(A(S))) ≤ 3pβ.
Using Theorem 5.1, one can compute tail bounds on the generalization error of fixed classifiers,
and then, provided that a classifier is learned from data in a differentially private way, the tail bound
holds on the classifier, with less confidence. The following two tail bounds are examples of this idea.
The first is a simple variant of (Dwork et al., 2015b, Thm. 9) due to Oneto, Ridella, and Anguita
(2017, Lem. 2).
Theorem 5.2. Let m ∈ N and let A : Zm	RP be ε-differentially Private. Then
P (| R D (A (S)) - R S (A (S ))∣≥ε + p/m) ≤ 3 e - m ε 2.
SZDm
Theorem 5.3 ((Oneto, Ridella, and Anguita, 2017, Lem. 3)). Let m ∈ N and let A : Zm	RP be
ε -differentially Private. Then
sP m (R D (A (S)) - R S (A (S ))| ≥16R S (A( S)) (ε + p1/m) + 6(ε 2 + 1/m)) ≤ 3 e- m ε2.
5.1	AN ε -DIFFERENTIALLY PRIVATE PAC-BAYES BOUND
The PAC-Bayes theorem allows one to choose the prior based on the data-generating distribution
D, but not on the data S Z Dm. Using differential privacy, we can consider a data-dependent prior
P(S).
Theorem 5.4. Under 0-1 loss,for every δ > 0, m ∈ N, distribution D on Rk X K, and ε-differentially
Private data-dePendent Prior P : Zm	M1 (RP),
KL(Q||P(S)) + ln2m + 2max{ln3, mε2}、
PIJI((VQ) KL(RS(Q)||RD(Q)) ≤	δ2_-) ≥ 1 -δ. (19)
SZDm	m - 1
8
Under review as a conference paper at ICLR 2018
Proof. Fix a distribution D on Rk × K. For every distribution P on Rp, let
R(P) = {s∈ Zm ∙. (∃Q) KL(RS(Q)||RD(Q)) ≥ (m- 1)-1(KL(Q||P)+ ln2m + ln(1∕β))}. (20)
It follows from the PAC-Bayes theorem (Theorem 3.1) that PS〜Dm(S ∈ R(P)) ≤ 0. Theorem 5.1
implies that the bound holds with P replaced by P (S), provided that we inflate the probability of
failure.
In particular, let δ = 3pβ. Thenln(1/0) = 2ln(3∕δ). By Theorem 5.1,provided2mε2 ≤ ln(1∕0),
then Ps〜Dm(S ∈ R(P(S))) ≤ δ. It follows that, with probability no more than δ over S 〜Dm, there
exists a distribution Q on Rp such that
KL(RS(Q)||RD(Q)) ≥ KL(Q11P(S))+ ln2m + m喇2m£2，ln(1/e)} .	(21)
m-1
The bound stated in Eq. (19) follows immediately.	□
Note that the bound holds for any posterior Q, including one obtained by optimizing a different
PAC-Bayes bound. We have chosen to present a differentially private version of Theorem 3.1 rather
than Theorem 3.2, because the former tends to be tighter numerically. Giving a differentially private
version of Theorem 3.2, or any other PAC-Bayes bound, should be straightforward: one merely
needs to decide how to incorporate the constraint between ε, β, and m in Theorem 5.1. We have
chosen to deal with the constraint via a max operation affecting the width of the confidence interval.
Note that, in realistic scenarios, δ is large enough relative to ε that an ε-differentially private prior
P(S) contributes 2ε2 to the generalization error. Therefore, ε must be much less than one to not
contribute a nontrivial amount to the generalization error. In order to match the m-1 rate by which
the KL term decays, one must have ε ∈ O(m-1/2). Our empirical studies use this rate.
5.2	Differentially private data-dependent priors
We have already explained that the weights learned by Entropy-SGD can be viewed as the mean of
a data-dependent prior P (S). By Theorem 5.4 and the fact that post-processing does not decrease
privacy, it would suffice to establish that the mean is ε-differentially private in order to obtain a risk
bound on the corresponding Gibbs posterior classifier.
Entropy-SGD can be viewed as stochastic gradient ascent on the negative local entropy, but with
biased gradient estimates. The bias comes from the use of SGLD to compute the expectation in
Eq. (8). Putting aside this issue, existing privacy analyses of SGD worsen after every iteration.
For the number of iterations necessary to obtain reasonable weights, known upper bounds on the
differential privacy of SGD yield vacuous generalization bounds.
The standard (if idealized) approach for optimizing a data-dependent objective in a private way is
to use the exponential mechanism (McSherry and Talwar, 2007). In the context of maximizing the
local entropy, the exponential mechanism correspond to sampling exactly from the “local entropy
(Gibbs) distribution”
Pexp(β Fγ,τ(∙;S)),	(22)
where β > 0 and P is some measure on Rp . (It is natural to take P to be Lebesgue measure, or a
multivariate normal distribution, which would correspond to L2 regularization of the local entropy.)
The following result establishes the privacy of a sample from the local entropy distribution:
Theorem 5.5. Let γ, τ > 0, and assume the range of the loss is contained in an interval of length
Lmax. One SamPlefrOm the local entropy distribution PexP(βFyτ(,；s)), is *LmXT-differentially private.
Proof. The result follows immediately from the following two lemmas.	□
Lemma 5.6 ((McSherry and Talwar, 2007, Thm. 6)). Let q : Zm × Rp → R be measurable, let
P be a measure on Rp, let β > 0, and assume P[exp(-β q(S, ∙))] < ∞ for all S ∈ Zm. Let
∆q =def supS,S0 supw∈Rp |q(S, w) - q(S0, w)|, where the first supremum ranges over pairs S,S0 ∈ Zm
that disagree on no more than one coordinate. Let A : Zm Rp, on input S ∈ Zm, output a sample
from the Gibbs distribution PexP(-βq(s,∙)). Then A is 2β∆q-differentially private.
9
Under review as a conference paper at ICLR 2018
Lemma 5.7. Let Fγ,τ (w; S) be defined as Eq. (6), assume the range of the loss is contained in
an interval of length Lmax, and define q(S, w) = -Fγ,τ (w; S). Then ∆q =def supS,S0 supw∈Rp |q(S, h) -
q(Ss,h)| ≤ LmmXτ.
Proof. The proof essentially mirrors that of (McSherry and Talwar, 2007, Thm. 6).	□
There are two obvious obstructions to using the exponential mechanism to pick a prior mean: first,
cross-entropy loss can change in an unbounded way when swapping a single data point; second,
sampling from the local entropy distribution exactly is hard in general. To sidestep the first ob-
struction, we modify the underlying cross-entropy loss to be bounded by rescaling the probabilities
output by the classifier to be bounded away from zero and one, allowing us to invoke Lemma 5.7.
(Details of our modification of the cross entropy are described in Appendix B.2.1.)
There is no simple way to sidestep the second obstruction. Instead, we once again use SGLD to
generate an approximate sample from the local entropy distribution. In summary, to optimize the
local entropy FY,τ(∙; S) ina private way to obtain the prior mean w, We repeatedly perform the SGLD
update
W — W - 1 η g(w) + p∕ηTβ N(0,Ip),	(23)
where at each round g(w) is an estimate of the gradient VWFY,τ(w;S). (Recall the identity Eq. (8).)
As in Entropy-SGD, we construct biased gradient estimates via an inner loop of SGLD. In summary,
the only change to Entropy-SGD is the addition of noise in the outer loop. We call the resulting
algorithm Entropy-SGLD. (See Algorithm 1. Note that we take β = 1 in our experiments.)
There have been a number of privacy analyses of SGLD (Mir, 2013; Bassily, Smith, and Thakurta,
2014; Dimitrakakis et al., 2014; Wang, Fienberg, and Smola, 2015; Minami et al., 2016). Most of
these analyses deliver (ε, δ)-differential privacy, but none of them take advantage of the fact that
SGLD mixes in the limit as it converges weakly to the Gibbs distributions, under certain technical
conditions (Teh, Thiery, and Vollmer, 2016, Thm. 7). In our analysis and bound calculations, we
therefore make the approximation that SGLD has the same privacy as its limiting invariant measure,
the exponential mechanism. Building a less conservative model of the privacy of SGLD is an open
problem. However, by making this approximation, we may see the potential/limits of combining
differentially private optimization and PAC-Bayesian bounds. We return to the issues again in light
of our empirical findings (Section 6) and in our discussion (Section 7).
6	Numerical evaluations on MNIST
The generalization bounds that we have devised are data-dependent and so the question of their
utility is an empirical one that requires data. In this section, we perform an empirical study of SGD,
SGLD, Entropy-SGD, and Entropy-SGLD on the MNIST data set, on both convolutional and fully
connected architectures, and compare our generalization bounds to estimates based on held-out data.
Under our privacy approximation, SGLD and Entropy-SGLD are ε-differentially private and we
take advantage of this fact to apply differentially private versions of two tail bounds and our PAC-
Bayes bound. The degree ε of privacy is determined by the τ parameter of the local entropy (C.f.
thermal noise ,2∕τ), and then, in turn, ε contributes to our bounds on the generalization error. As
theory predicts, τ affects the degree of overfitting empirically, and no bound we compute is violated
too frequently. Of course, the validity of our generalization bounds rests on the degree to which
our privacy approximation is violated.3 We reflect on our approximation in light of our empirical
results, and then return to this point in the discussion.
The weights learned by SGD, SGLD, and Entropy-SGD are treated differently from those learned
by Entropy-SGLD. In the former case, the weights parametrize a neural network as usual, and the
training and test error are computed using these weights. In the latter case, the weights are taken
to be the mean of a multivariate normal prior, and we evaluate the training and test error of the
3A number of factors might influence the degree to which our privacy approximation is invalid: step size,
batch size, use of batch normalization, number of iterations, use of Metropolis-Hastings corrections. How-
ever, when we are operating far from the worst case, we suspect these details are not relevant to observable
phenomena, and so our theoretical analysis and empirical findings should serve as a useful guide.
10
0。IXJoHə 70
True Labels
2 0 8 6 4
1 1
001 × rorre 1-0
× ×
-∙- Thermal noise = 0.1
Thermal noise = 0.05
Thermal noise = 0.01
-A- Thermal noise = 0.005
—I- Thermal noise = 0.0001
-A- SGD
Random Labels
Oooooo
6 5 4 3 2 1
SGD and SGLD
20
30
50
-∙- Thermal noise = 0.1
Thermal noise = 0.05
Thermal noise = 0.01
Thermal noise = 0.005
―I- Thermal noise = 0.0001
Entropy-SGD
0。IXJotə 70
0	50	100	150	200	250	300	350
Epochs ÷ L
30-
20-
10-
Test (mean)
Train (mean)
-H- PAC bound (Gibbs)
--Test (Gibbs)
-⅛- Train (Gibbs)
H-bound
—I— C-bound
Entropy-SGLD
0	100	200	300	400	500	600	700	800
Epochs ÷ L
W
Figure 1: Results on the CONV network on two-class MNIST. (top) Training error (under 0-1
loss) for SGLD on the empirical risk -TRS under a variety of thermal noise ,2∕τ settings. SGD
corresponds to zero thermal noise. (top-left) The large markers on the right indicate test error. The
gap is an estimate of the generalization error. On true labels, SGLD finds classifiers with relatively
small generalization error. At low thermal noise settings, SGLD (and its zero limit, SGD), achieve
small empirical risk. As we increase the thermal noise, the empirical 0-1 error increases, but the
generalization error decreases. At 0.1 thermal noise, risk is close to 50%. (top-right) On random
labels, SGLD has high generalization error for thermal noise values 0.01 and below. (True error is
50%). (middle-left) On true labels, Entropy-SGD, like SGD and SGLD, has small generalization
error. For the same settings of thermal noise, empirical risk is lower. (middle-right) On random
labels, Entropy-SGD overfits for thermal noise values 0.005 and below. Thermal noise 0.01 produces
good performance on both true and random labels. (bottom row) Entropy-SGLD is configured to
be ε-differentially private with ε ≈ 0.0327 by setting T = √m, where m is the number of training
samples. (bottom-left) On true labels, the generalization error for networks learned by Entropy-
SGLD is close to zero. Generalization bounds are relatively tight. (bottom-right) On random label,
Entropy-SGLD does not overfit. See Fig. 3 for SGLD bounds at same privacy setting.
Under review as a conference paper at ICLR 2018
associated Gibbs posterior (i.e., a randomized classifier). We also report the performance of the
(deterministic) network parametrized by these weights (called the “mean” classifier) in order to give
a coarse statistic summarizing the local empirical risk surface.
Following Zhang et al. (2017), we study these algorithms on MNIST with its original (“true”) labels,
as well as on random labels. Parameter τ that performs very well in one setting often does not
perform well in the other. Random labels mimic data where the Bayes error rate is high, and where
overfitting can have severe consequences.
6.1	Details
We use a two-class variant of MNIST (LeCun, Cortes, and Burges, 2010).4 (See Fig. 4 and Ap-
pendix C for our experiments on the standard multiclass MNIST dataset. They yield similar insight.)
Some experiments involve random labels, i.e., labels drawn independently and uniformly at random
at the start of training. We study three network architectures, abbreviated FC600, FC1200, and
CONV. Both FC600 and FC1200 are 3-layer fully connected networks, with 600 and 1200 units per
hidden layer, respectively. CONV is a convolutional architecture. All three network architectures
are taken from the MNIST experiments by Chaudhari et al. (2017), but adapted to our two-class
version of MNIST.5 Let S and Stst denote the training and test sets, respectively. For all learning
algorithms we track
(i)	Rs(W) and RStst (w), i.e., the training and test error for w.
We also track
(ii)	estimates of RS(GWS) and RStSt(GW'S)，i.e., the mean training and test error of the local
Gibbs distribution, viewed as a randomized classifier (“Gibbs”)
and, using the differential privacy bounds in Theorem 5.5, compute
(iii)	a PAC-Bayes bound on RD (Gγw,,τS) using Theorem 5.4 (“PAC-bound”);
(iv)	the mean of a Hoeffding-StyIe bound on RD(w0), where w0 〜Pexp(Fγτ(,；§)),, using Theo-
rem 5.2 (“H-bound”);	,
(v)	an upper bound on the mean of a Chernoff-style bound on RD(w0), where w0 〜
Pexp(Fγ,τ(.;s)),, using Theorem 5.3 ("C-bound").
We also compute H- and C- bounds for SGLD, viewed as a sampler for w0 〜 Pexp(-τRS), where P
here is Lebesgue measure.
In order for SGLD and Entropy-SGLD to be private, we modify the cross entropy loss function to
be bounded. We achieve this by an affine transformation of the neural networks output that prevents
extreme probability (se Appendix B.2.1). With the choice of τ = √m, and the loss function taking
values in an interval of length Lmax = 4, Entropy-SGLD is ε-differentially private, with ε ≈ 0.0327.
See Appendix B.2 for additional details. Note that, in the calculation of (iii), we do not account for
Monte Carlo error in our estimate of RS (w). The effect is small, given the large number of iterations
of SGLD performed for each point in the plot. Recall that
RD(Gγw,,τS) = 0 Ew,S(RD(w0)),	(24)
w	γ,τ
and so we may interpret the bounds in terms of the performance of a randomized classifier or the
mean performance of a randomly chosen classifier.
4 The MNIST handwritten digits dataset (LeCun, Cortes, and Burges, 2010) consists of 60000 training set
images and 10000 test set images, labeled 0-9. We transformed MNIST to a two-class (i.e., binary) classifica-
tion task by mapping digits 0-4 to label 1 and 5-9 to label -1.
5 We adapt the code provided by Chaudhari et al., with some modifications to the training procedure and
straightforward changes necessary for our binary classification task.
12
Under review as a conference paper at ICLR 2018
6.2	Results
Key results for the convolutional architecture (CONV) appear in Fig. 1. Results for FC600 and
FC1200 appear in Fig. 2 of Appendix B. (Training the CONV network produces the lowest train-
ing/test errors and tightest generalization bounds. Results and bounds for FC600 are nearly identical
to those for FC1200, despite FC1200 having three times as many parameters.)
The top row of Fig. 1 presents the performance of SGLD for various levels of thermal noise ,2∕τ
under both true and random labels. (Under our privacy approximation, we may also use SGLD to
directly perform a private optimization of the empirical risk surface. The level of thermal noise
determines the differential privacy of SGLD and so we expect to see a tradeoff between empirical
risk and generalization error. Note that SGD is the same as SGLD with zero thermal noise.) SGD
achieves the smallest training and test error on true labels, but overfits the worst on random labels. In
comparison, SGLD’s generalization performance improves with higher thermal noise, while its risk
performance worsens. At 0.05 thermal noise, SGLD achieves reasonable but relatively large risk but
almost zero generalization error on both true and random labels. Other thermal noise settings have
either much worse risk or generalization performance.
The middle row of Fig. 1 presents the performance of Entropy-SGD for various levels of thermal
noise ,2∕τ under both true and random labels. As with SGD, EntrOPy-SGD's generalization per-
formance improves with higher thermal noise, while its risk performance worsens. At the same
levels of thermal noise, Entropy-SGD outperforms the risk and generalization error of SGD. At 0.01
thermal noise, Entropy-SGD achieves good risk and low generalization error on both true and ran-
dom labels. However, the test-set performance of Entropy-SGD at 0.01 thermal noise is still worse
than that of SGD. Whether this difference is due to SGD overfitting to the MNIST test set is unclear
and deserves further study.
The bottom row of Fig. 1 presents the performance of Entropy-SGLD with τ = √m on true and ran-
dom labels. (This corresponds to approximately 0.09 thermal noise.) On true lables, both the mean
and Gibbs classifier learned by Entropy-SGLD have approximately 2% test error and essentially
zero generalization error, which is less than predicted by our bounds. Our PAC-Bayes risk bounds
are roughly 3%. As expected by the theory, Entropy-SGLD does not overfit on random labels, even
after thousands of epochs.
We find that our PAC-Bayes bounds are generally tighter than the H- and C-bounds. All bounds are
nonvacuous, though still loose. The error bounds reported here are tighter than those reported by
Dziugaite and Roy (2017). However, the validity of all three privacy-based bounds that we report
rests on the privacy approximation regarding SGLD, and so interpreting these bounds requires some
subtlety. We achieve much tighter generalization bounds than previously reported, and better test
error, but we are still far from the performance of SGD. This is despite making a strong approxi-
mation, and so we might view these results as telling us the limits of combining differential privacy
and PAC-Bayes bounds. Weaker notions of stability/privacy may be necessary to achieve further
improvement in generalization error and test error. Despite the coarse privacy approximation, no
bound is ever violated: possible explanations include the bounds simply being loose and/or the data
being far from worst case. Note that, given the number of experiments, we might even expect a
violation for tight bounds. Indeed, our performance on random labels supports the hypothesis that
the privacy of (Entropy-)SGLD does not degrade over time, at least not in a way that can be detected
by our experiments.
7	Discussion
Our work reveals that Entropy-SGD can be understood as optimizing a PAC-Bayes generalization
bound in terms of the bound’s prior. Because the prior must be independent of the data, the bound is
invalid, and, indeed, we observe ovefitting in our experiments with Entropy-SGD when the thermal
noise ,2∕τ is set to 0.0001 as suggested by Chaudhari et al. for MNIST.
PAC-Bayes priors can, however, depend on the data distribution. This flexibility seems wasted,
since the data sample is typically viewed as one’s only view onto the data distribution. However,
using differential privacy, we can span this gap. By performing a private computation on the data,
we can extract information about the underlying distribution, without undermining the statistical
13
Under review as a conference paper at ICLR 2018
validity of a subsequent PAC-Bayes bound. Our PAC-Bayes bound based on a differentially private
prior is made looser by the use of a private data-dependent prior, but the gains in choosing a data-
distribution-dependent prior more than make up for the expansion of the bound due to the privacy.
(The gains come from the KL term being much smaller on the account of the prior being better
matched to the posterior.) Understanding how our approach compares to local PAC-Bayes priors
(Catoni, 2007) is an important open problem.
The most elegant way to make Entropy-SGD private is to replace SGD with a sample from the Gibbs
distribution (known as the exponential mechanism in the differential privacy literature). However,
generating an exact sample is intractable, and so practicioners use SGLD to generate an approximate
sample, relying on the fact that SGLD converges weakly to the exponential mechanism under certain
technical conditions. Our privacy approximation allows us to proceed with a theoretical analysis by
assuming that SGLD achieves the same privacy as the exponential mechanism. On the one hand, we
do not find overt evidence that our approximation is grossly violated. On the other, we likely do not
require such strong privacy in order to control generalization error.
We might view our privacy-based bounds as being optimistic and representing the bounds we might
be able to achieve rigorously should there be a major advance in private optimization. (No analysis
of the privacy of SGLD takes advantage of the fact that it mixes weakly.) On the account of using
private data-dependent priors, our bounds are significantly tighter than those reported by Dziugaite
and Roy (2017). However, despite our bounds potentially being optimistic, the test set error we
are able to achieve is still 5-10 times that of SGD. Differential privacy may be too conservative for
our purposes, leading us to underfit. Indeed, we think it is unlikely that Entropy-SGD has strong
differential privacy, yet we are able to achieve good generalization on both true and random labels
under 0.01 thermal noise. Identifying the appropriate notion of privacy/stability to combine with
PAC-Bayes bounds is an important problem.
Despite our progress on building learning algorithms with strong generalization performance, and
identifying a path to much tighter PAC-Bayes bounds, Entropy-SGLD learns much more slowly than
Entropy-SGD, the risk of Entropy-SGLD is far from state of the art, and our PAC-Bayes bounds
are loose. It seems likely that there is a fundamental tradeoff between the speed of learning, the
excess risk, and the ability to produce a certificate of one’s generalization error via a rigorous bound.
Characterizing the relationship between these quantities is an important open problem.
References
Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar,
and Li Zhang (2016). “Deep Learning with Differential Privacy”. Proceedings of the 2016 ACM
SIGSAC Conference on Computer and Communications Security. CCS ’16. Vienna, Austria:
ACM, pp. 308-318. doi: 10.1145/2976749.2978318.
Alessandro Achille and Stefano Soatto (2017). “On the Emergence of Invariance and Disentangling
in Deep Representations”. CoRR abs/1706.01350. arXiv: 1706.01350.
Carlo Baldassi, Alessandro Ingrosso, Carlo Lucibello, Luca Saglietti, and Riccardo Zecchina (2015).
“Subdominant Dense Clusters Allow for Simple Learning and High Computational Performance
in Neural Networks with Discrete Synapses”. Phys. Rev. Lett. 115 (12), p. 128101. DOI: 10 .
1103/PhysRevLett.115.128101.
Carlo Baldassi, Christian Borgs, Jennifer T. Chayes, Alessandro Ingrosso, Carlo Lucibello, Luca
Saglietti, and Riccardo Zecchina (2016). “Unreasonable effectiveness of learning neural net-
works: From accessible states and robust ensembles to basic algorithmic schemes”. Proceedings
of the National Academy of Sciences 113.48, E7655-E7662. DOI: 10.1073/pnas.1608103113.
eprint: http://www.pnas.org/content/113/48/E7655.full.pdf.
Peter Bartlett, Dylan J. Foster, and Matus Telgarsky (2017). “Spectrally-normalized margin bounds
for neural networks”. CoRR abs/1706.08498. arXiv: 1706.08498.
Peter L. Bartlett and Shahar Mendelson (2003). “Rademacher and Gaussian Complexities: Risk
Bounds and Structural Results”. J. Mach. Learn. Res. 3, pp. 463-482.
14
Under review as a conference paper at ICLR 2018
Raef Bassily, Adam Smith, and Abhradeep Thakurta (2014). “Differentially private empirical risk
minimization: Efficient algorithms and tight error bounds”. arXiv preprint arXiv:1405.7085.
Raef Bassily, Kobbi Nissim, Adam Smith, Thomas Steinke, Uri Stemmer, and Jonathan Ullman
(2016). “Algorithmic stability for adaptive data analysis”. Proceedings of the 48th Annual ACM
SIGACT Symposium on Theory ofComputing. ACM,pp. 1046-1059.
Olivier BoUsqUet and Andre Elisseeff (2002). “Stability and generalization”. Journal of Machine
Learning Research 2.Mar, pp. 499-526.
Olivier Catoni (2007). “PAC-Bayesian sUpervised classification: the thermodynamics of statistical
learning”. arXiv preprint arXiv:0712.0248.
Pratik ChaUdhari, Anna Choromanska, Stefano Soatto, Yann LeCUn, Carlo Baldassi, Christian
Borgs, Jennifer Chayes, Levent SagUn, and Riccardo Zecchina (2017). “Entropy-SGD: Biasing
Gradient Descent Into Wide Valleys”. International Conference on Learning Representations
(ICLR). arXiv: 1611.01838v4 [cs.LG].
Kamalika ChaUdhUri, Claire Monteleoni, and Anand D Sarwate (2011). “Differentially private em-
pirical risk minimization”. Journal of Machine Learning Research 12.Mar, pp. 1069-1109.
Christos Dimitrakakis, Blaine Nelson, Aikaterini Mitrokotsa, and Benjamin IP RUbinstein (2014).
“RobUst and private Bayesian inference”. International Conference on Algorithmic Learning The-
ory. Springer, pp. 291-305.
Cynthia Dwork (2006). “Differential Privacy”. Automata, Languages and Programming: 33rd In-
ternational Colloquium, ICALP 2006, Venice, Italy, July 10-14, 2006, Proceedings, Part II. Ed.
by Michele BUgliesi, Bart Preneel, Vladimiro Sassone, and Ingo Wegener. Berlin, Heidelberg:
Springer Berlin Heidelberg, pp. 1-12. doi: 10.1007/11787006_1.
-	(2008). “Differential privacy: A sUrvey of resUlts”. International Conference on Theory and Ap-
plications of Models of Computation. Springer, pp. 1-19.
Cynthia Dwork, Aaron Roth, et al. (2014). “The algorithmic foUndations of differential privacy”.
Foundations and Trends in Theoretical Computer Science 9.3-4, pp. 211-407.
Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toni Pitassi, Omer Reingold, and Aaron Roth
(2015a). “Generalization in adaptive data analysis and holdoUt reUse”. Advances in Neural In-
formation Processing Systems, pp. 2350-2358.
Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Aaron Leon
Roth (2015b). “Preserving statistical validity in adaptive data analysis”. Proceedings of the Forty-
Seventh Annual ACM on Symposium on Theory of Computing. ACM, pp. 117-126.
Gintare Karolina DziUgaite and Daniel M. Roy (2017). “CompUting NonvacUoUs Generalization
BoUnds for Deep (Stochastic) NeUral Networks with Many More Parameters than Training Data”.
arXiv preprint arXiv:1703.11008.
Pascal Germain, Francis Bach, Alexandre Lacoste, and Simon Lacoste-JUlien (2016). “PAC-
Bayesian Theory Meets Bayesian Inference”. Advances in Neural Information Processing Sys-
tems, pp. 1884-1892.
Peter Grunwald (2012). “The Safe Bayesian-Learning the Learning Rate via the Mixability Gap.”
ALT. Springer, pp. 169-183.
Peter D. Grunwald and Nishant A. Mehta (2016). “Fast Rates with UnboUnded Losses”. CoRR
abs/1605.00252. arXiv: 1605.00252.
Geoffrey E. Hinton and Drew van Camp (1993). “Keeping the NeUral Networks Simple by Mini-
mizing the Description Length of the Weights”. Proceedings of the Sixth Annual Conference on
Computational Learning Theory. COLT ’93. Santa CrUz, California, USA: ACM, pp. 5-13. DOI:
10.1145/168304.168306.
Sepp Hochreiter and Jurgen SchmidhUber (1997). “Flat Minima”. Neural Comput. 9.1, pp. 1-42.
doi: 10.1162/neco.1997.9.1.1.
15
Under review as a conference paper at ICLR 2018
Daniel Kifer, Adam Smith, and Abhradeep Thakurta (2012). “Private convex empirical risk mini-
mization and high-dimensional regression”. Journal of Machine Learning Research 1.41, pp. 1-
40.
Diederik P Kingma, Tim Salimans, and Max Welling (2015). “Variational Dropout and the Lo-
cal Reparameterization Trick”. Advances in Neural Information Processing Systems 28. Ed. by
C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett. Curran Associates, Inc.,
pp. 2575-2583.
John Langford (2002). “Quantitatively tight sample complexity bounds”. PhD thesis. Carnegie Mel-
lon University.
John Langford and Rich Caruana (2002). “(Not) Bounding the True Error”. Advances in Neural
Information Processing Systems 14. Ed. by T. G. Dietterich, S. Becker, and Z. Ghahramani. MIT
Press, pp. 809-816.
John Langford and Matthias Seeger (2001). Bounds for Averaging Classifiers. Tech. rep. CMU-CS-
01-102. Carnegie Mellon University.
Yann LeCun, Corinna Cortes, and Christopher J. C. Burges (2010). MNIST handwritten digit
database. http://yann.lecun.com/exdb/mnist/.
GUy Lever, Frangois Laviolette, and John ShaWe-TayIor (2013). “Tighter PAC-Bayes bounds
through distribution-dependent priors”. Theoretical Computer Science 473, pp. 4-28. DOI: 10 .
1016/j.tcs.2012.10.013.
David A. McAllester (1999). “PAC-Bayesian Model Averaging”. Proceedings of the Twelfth Annual
Conference on Computational Learning Theory. COLT ’99. Santa Cruz, California, USA: ACM,
pp. 164-170. doi: 10.1145/307400.307435.
-	(2013). “A PAC-Bayesian Tutorial With A Dropout Bound”. CoRR abs/1307.2118.
Frank McSherry and Kunal TalWar (2007). “Mechanism Design via Differential Privacy”. Proceed-
ings of the 48th Annual IEEE Symposium on Foundations of Computer Science. FOCS ’07. Wash-
ington, DC, USA: IEEE Computer Society, pp. 94-103. doi: 10.1109/FOCS.2007.41.
Kentaro Minami, Hitomi Arai, Issei Sato, and Hiroshi NakagaWa (2016). “Differential Privacy With-
out Sensitivity”. Advances in Neural Information Processing Systems 29. Ed. by D. D. Lee, M.
Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett. Curran Associates, Inc., pp. 956-964.
Darakhshan J Mir (2013). “Differential privacy: an exploration of the privacy-utility landscape”.
PhD thesis. Rutgers University.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro (2014). In Search of the Real Inductive
Bias: On the Role of Implicit Regularization in Deep Learning. Workshop track poster at ICLR
2015. arXiv: 1412.6614v4 [cs.LG].
-	(2015). “Norm-based capacity control in neural netWorks”. Proceedings of the Eighth Annual
Conference on Learning Theory. COLT 2016, pp. 1376-1401. arXiv: 1503.00036v2 [cs.LG].
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro (2017a). “A
PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural NetWorks”. CoRR
abs/1707.09564. arXiv: 1707.09564.
-	(2017b). “Exploring Generalization in Deep Learning”. CoRR abs/1706.08947. arXiv: 1706 .
08947.
Luca Oneto, Sandro Ridella, and Davide Anguita (2017). “Differential privacy and generalization:
Sharper bounds With applications”. Pattern Recognition Letters 89, pp. 31-38. DOI: 10.1016/j.
patrec.2017.02.006.
Yee Whye Teh, Alexandre H Thiery, and Sebastian J Vollmer (2016). “Consistency and fluctuations
for stochastic gradient Langevin dynamics”. Journal of Machine Learning Research 17, pp. 1-33.
Yu-Xiang Wang, Stephen E. Fienberg, and Alexander J. Smola (2015). “Privacy for Free: Posterior
Sampling and Stochastic Gradient Monte Carlo”. Proceedings of the 32nd International Confer-
16
Under review as a conference paper at ICLR 2018
ence on International Conference on Machine Learning - Volume 37. ICML’15. Lille, France:
JMLR.org, pp. 2493-2502.
Max Welling and Yee W Teh (2011). “Bayesian learning via stochastic gradient Langevin dynam-
ics”. Proceedings of the 28th International Conference on Machine Learning (ICML-11), pp. 681-
688.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals (2017). “Under-
standing deep learning requires rethinking generalization”. International Conference on Repre-
sentation Learning (ICLR). arXiv: 1611.03530v2 [cs.LG].
Tong Zhang (2006a). “From ε -entropy to KL-entropy: Analysis of minimum information com-
plexity density estimation”. The Annals of Statistics 34.5, pp. 2180-2210. DOI: 10 . 1214 /
009053606000000704.
-	(2006b). “Information-theoretic upper and lower bounds for statistical estimation”. IEEE Trans-
actions on Information Theory 52.4, pp. 1307-1321.
17
Under review as a conference paper at ICLR 2018
A	Background: Differential privacy
Here we formally define some of the differential privacy related terms used in the main text. (See
(Dwork, 2006; Dwork and Roth, 2014) for more details.)
Let U, U1, U2, . . . be independent uniform (0, 1) random variables, independent also of any ran-
dom variables introduced by P and E, and let π : N × [0, 1] → [0, 1] satisfy (π (1, U), . . . , π(k, U)) =d
(U1,..., Uk) for all k ∈ N. Write ∏k for π(k, ∙).
Definition A.1. A randomized algorithm A from R to T, denoted A : R T, is a measurable map
A : [0, 1] × R → T. Associated to A is a (measurable) collection of random variables {Ar : r ∈ R}
that satisfy Ar = A (U, r). When there is no risk of confusion, we will write A (r) for Ar.
Definition A.2. A randomized algorithm A : Zm T is (ε, δ)-differentially private if, for all pairs
S,S0 ∈ Zm that differ at only one coordinate, and all measurable subsets B ⊆ T, we have P(A (S) ∈
B) ≤ eε P(A (S0) ∈B)+δ.
We will write ε-differentially private to mean (ε, 0)-differentially private algorithm.
Definition A.3. Let A : R T and A 0 : T T0. The composition A0 ◦ A : R T0 is given by
(A0 ◦A )(u, r) = A0(π2(u),A (π1(u), r)).
Lemma A.4 (post-processing). Let A : Zm T be (ε, δ)-differentially private and let F : T T0
be arbitrary. Then F ◦A is (ε, δ)-differentially private.
B	Two-class MNIST experiments
B.1	Architecture
We studied three architectures: CONV, FC600, and FC1200.
CONV was a convolutional neural network, whose architecture was the same as that used by Chaud-
hari et al. (2017) for multiclass MNIST classification, except modified to produce a single probabil-
ity output for our two-class variant of MNIST. In particular, CONV has two convolutional layers, a
fully connected ReLU layer, and a sigmoidal output layer, yielding 126, 711 parameters in total.
FC600 and FC1200 are fully connected 3-layer neural networks, with 600 and 1200 hidden units,
respectively, yielding 834, 601 and 2, 385, 185 parameters in total, respectively. We used ReLU
activations for all but the last layer, which was sigmoidal to produce an output in [0, 1].
In their MNIST experiments, Chaudhari et al. (2017) use dropout and batch normalization. We did
not use dropout. The bounds we achieved with and without batch norm were very similar. Without
batch norm, however, it was necessary to tune the learning rates. Understanding the combination of
SGLD and batch norm and the limiting invariant distribution, if any, is an important open problem.
B.2	Training objective and hyperparameters for Entropy- S GLD
B.2.1	Objective
All networks are trained to minimize a bounded variant of empirical cross entropy loss. The change
involves replacing g(p,y) = - log p with g(p,y) = - log ψ(p), where
ψ (p) = eLmax + (1 - 2e-Lmax ) p	(25)
is an affine transformation that maps to [0, 1] to [eLmax, 1 - eLmax], removing extreme probability
values. As a result, the binary cross entropy loss 'bce is contained in an interval of length Lmax∙ In
particular, g(p,y) ∈ [0, Lmax]. We take Lmax = 4 in our experiments.
B.2.2	Epochs
Ordinarily, an epoch implies one pass through the entire data set. For SGD, each stochastic gradient
step processes a minibatch of size K = 128. Therefore, an epoch is m/K = 468 steps of SGD. An
epoch for Entropy-SGD and Entropy-SGLD is defined as follows: each iteration of the inner SGLD
18
B.2 Training objective and hyperparameters for Entropy-SGLD
19
0。IXJotə 70
FC600
FC1200
True Labels
0	50	100	150	200	250	300	350	0	50	100	150	200	250	300	350
Epochs ÷ L	Epochs ÷ L
Figure 2: Fully connected networks trained on binarized MNIST with a differentially private
Entropy-SGLD algorithm. (left) Entropy-SGLD applied to FC600 network trained on true labels.
(right) Entropy-SGLD applied to FC1200 network trained on true labels. Both training error and
generalization error are similar for both network architectures. T he true generalization gap is close
to zero, since the test and train error overlaps. All the computed bounds on the test error are loose
but nonvacuous.
108 6 4
OolxJauə II。
50-
40 -
30-
20-
--Test
Train
H-bound
—I- C-bound
≠H + ∣*H 附将⅛∣ 田中HW卅Ma)HH■卅⅛h山IWHli⅛∣∣∣再由1∣由HIllR■
~'-~AA∙-f'-^∙∙-∙—r.-∖∙^-∙∕n-4ς∙^-r∙4∙→-yv∙*Γ*rt—M-*“
忖,,汕浮阳 叭沪⅞ 岬咿HwTw
SGLD
10-
Epochs
Figure 3: Results on CONV architecture, running SGLD configured to have the same differential
privacy as EntroPy-SGLD with T = √m. On true labels, SGLD learns a network with approximately
3% higher training and test error than the mean and Gibbs networks learned by Entropy-SGLD.
SGLD does not overfit on random labels, as predicted by theory. The C-bound on the true error of
this network is around 8%, which is worse than the roughly 4% C-bound on the mean classifier.
Test
Train
H-bound
—I- C-bound
IOOO 2000	3000	4000
Epochs
Under review as a conference paper at ICLR 2018
loop processes a minibatch of size K = 128, and the inner loop runs for L = 20 steps. Therefore,
an epoch is m/(LK) steps of the outer loop. In concrete terms, there are 20 steps of SGD per every
one step of Entropy-SG(L)D. Concretely, the x-axis of our plots measure epochs divided by L. This
choice, used also by Chaudhari et al. (2017), ensures that the wall-clock time of Entropy-SG(L)D
and SGD align.
B.2.3	SGLD parameters: step sizes and weighted averages
The step sizes for SGLD must be square summable but not summable. The step sizes for the outer
SGLD loop are of the form ηt = ηt-0.6, with η = 0006. The step sizes for the inner SGLD loop are
of the form ηt = η t-1, with η = γT.
The estimate produced by the inner SGLD loop is computed using a weighted average (line 8)
with α = 0.75. We use SGLD again when computing the PAC-Bayes generalization bound (Ap-
pendix B.3.2). In this case, SGLD is used to sample from the local Gibbs distribution when estimat-
ing the Gibbs risk and the KL term. We run SGLD for 1000 epochs to obtain our estimate. Again,
we use weighted averages, but with α = 0.005, in order to average over a larger number of samples
and better control the variance.
B.2.4	Gibbs classifier parameters
We set Y = 1 and τ = √m and keep the values fixed during optimization. By Theorem 5.5, the
value of τ, Lmax, and β determine the differential privacy of Entropy-SGLD. In turn, the differential
privacy parameter ε and confidence parameter δ contribute
max{ln 3, mε2}
2	m-1
(26)
to the bound on the KL-generalization error KL(RS(Q)||RD(Q)) in our differentially private PAC-
Bayes bound (Theorem 5.4). Choosing T = √m, implies that the contribution coming from differ-
ential privacy decays at a rate of 1/m. Numerically, given Lmax = 4 and β = 1, this contribution is
0.002.
B.3 Evaluating the PAC-Bayes bound
B.3.1	INVERTING KL(q|| p)
When the empirical error is close to zero, the KL version of the PAC-Bayes bound Theorem 3.1 is
considerably tighter than the Hoeffding-style bound first described by McAllester (1999). However,
using this relative entropy bound requires one to be able to compute the largest value p such that
KL(q||p) ≤ c. There does not appear to be a simple formula for this value. In practice, however,
the value can be efficiently numerically approximated using, e.g., Newton’s method. See (Dziugaite
and Roy, 2017, §2.2 and App. B).
B.3.2	Estimating the KL divergence
Let '(w) = τRs(w). By (Catoni, 2007, Lem. 1.1.3),
KUPexP(-')||P) =	JE [—']-logp[eχp(-')].
W 〜pxp(-')
Both terms have obvious Monte Carlo estimates:
1 k0
E [-'(w)] ≈ -k0 ∑ 以w0)
w~PeXP(-')	k i=1
(27)
(28)
where w：,..., Wk, are taken from a Markov chain targeting PeXP(-`), such as SGLD run for k，》1
steps (which is how we computed our bounds), and
log P [exp(-')]= log / exp{-'(w)} P (dw)
(29)
1k
' log k ∑eχp{-'(wi)}.	(30)
k i=1
20
Under review as a conference paper at ICLR 2018
where h1, . . . ,hk are i.i.d. P (which is a multivariate Gaussian in this case). In the latter case, due to
the concavity of log, the estimate is a lower bound with high probability, yielding a high probability
upper bound on the KL term.
C Multiclas s MNIST experiments
We evaluate the same generalization bounds on the standard MNIST classification task as in the
MNIST binary labelling case. All the details of the network architectures and parameters are as
stated in Appendix B.2, with two exception: following Chaudhari et al. (2017), we use a fully
connected network with 1024 hidden units per layer, denoted FC1024.
C.1 Objective
The neural network produces a probability vector ( p1 , . . . , pK ) via a soft-max operation. Ordinarily,
we then apply the cross entropy loss corresponding to g((p1, . . . ,pK),y) = -logpy. When training
privately, we use a bounded variant of the cross entropy loss, where the function g above is replaced
by g((p1, . . . , pK),y) = -log ψ(py), and ψ is defined as in Eq. (25).
21
C.1 Objective
22
True Labels
OOI × johə 70

O	IOO	200	300	400
Epochs ÷ L
Figure 4: (?) Local here refers to the mean classifier. Entropy-SGLD results on MNIST. (top-left)
FC1024 network trained on true labels. The train and test error suggest that the generalization gap
is close to zero, while all three bounds exceed the test error by slightly more than 3%. (bottom-
left) CONV network trained on true labels. Both the train and the test errors are lower than those
achieved by the FC1024 network. We still do not observe overfitting. The C-bound and PAC-Bayes
bounds exceed the test error by ≈ 3%. (top-right) FC1024 network trained on random labels. After
approximately 1000 epochs, we notice overfitting by ≈ 2%. Running Entropy-SGLD further does
not cause an additional overfitting. Theory suggests that our choice of τ prevents overfitting via
differential privacy. (bottom-right) CONV network trained on random labels. We observe almost
no overfitting (less than 1%). Both training and test error coincide and remain close to the guessing
rate (90%).
Random Labels
IOO-
98-
96-
94-
92-
90-
88-
86-
6
500 IOOO 1500	2000	2500	3000	3500
Epochs ÷ L