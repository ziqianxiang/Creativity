Under review as a conference paper at ICLR 2018
Tensor Contraction & Regression Networks
Anonymous authors
Paper under double-blind review
Ab stract
Convolution neural networks typically consist of many convolutional layers fol-
lowed by several fully-connected layers. While convolutional layers map between
high-order activation tensors, the fully-connected layers operate on flattened ac-
tivation vectors. Despite its success, this approach has notable drawbacks. Flat-
tening discards the multi-dimensional structure of the activations, and the fully-
connected layers require a large number of parameters. We present two new tech-
niques to address these problems. First, we introduce tensor contraction layers
which can replace the ordinary fully-connected layers in a neural network. Sec-
ond, we introduce tensor regression layers, which express the output of a neural
network as a low-rank multi-linear mapping from a high-order activation tensor
to the softmax layer. Both the contraction and regression weights are learned
end-to-end by backpropagation. By imposing low rank on both, we use signifi-
cantly fewer parameters. Experiments on the ImageNet dataset show that applied
to the popular VGG and ResNet architectures, our methods significantly reduce
the number of parameters in the fully connected layers (about 65% space savings)
while negligibly impacting accuracy.
1	Introduction
Many natural datasets exhibit pronounced multi-modal structure. We represent audio spectrograms
as 2nd-order tensors (matrices) with modes corresponding to frequency and time. We represent
images as third-order tensors with modes corresponding to width, height and the color channels.
Videos are expressed as 4th-order tensors, and the signal processed by an array of video sensors can
be described as a 5th-order tensor. A broad array of multi-modal data can be naturally encoded as
tensors. Tensor methods extend linear algebra to higher order tensors and are promising tools for
manipulating and analyzing such data.
The mathematical properties of tensors have long been the subject of theoretical study. Previously,
in machine learning, data points were typically assumed to be vectors and datasets to be matrices.
Hence, spectral methods, such as matrix decompositions, have been popular in machine learning.
Recently, tensor methods, which generalize these techniques to higher-order tensors, have gained
prominence. One class of broadly useful techniques within tensor methods are tensor decomposi-
tions, which have been studied for learning latent variables (Anandkumar et al., 2014).
Deep Neural Networks (DNNs) frequently manipulate high-order tensors: in a standard deep con-
volutional Neural Network (CNN) for image recognition, the inputs and the activations of convo-
lutional layers are 3rd-order tensors. And yet, to wit, most architectures output predictions by first
flattening the activations tensors and then connecting to the output neurons via one or more fully-
connected layers. This approach presents several issues: we lose multi-modal information during
the flattening process and the fully-connected layers require a large number of parameters.
In this paper, we propose Tensor Contraction Layers (TCLs) and Tensor Regression Layers (TRLs)
as end-to-end trainable components of neural networks. In doing so, we exploit multilinear structure
without giving up the power and flexibility offered by modern deep learning methods. By replacing
fully-connected layers with tensor contractions, we can aggregate long-range spatial information
while preserving multi-modal structure. Moreover, by enforcing low rank, we can significantly
reduce the number of parameters needed with minimal impact on accuracy.
Our proposed TRL represent the regression weights through the factors of a low-rank tensor de-
composition. The TRL obviates the need for flattening when generating output. By combining
1
Under review as a conference paper at ICLR 2018
tensor regression with tensor contraction, we further increase efficiency. Augmenting the VGG and
ResNet architectures, we demonstrate improved performance on the ImageNet dataset despite sig-
nificantly reducing the number of parameters (almost by 65%). This is the first paper that presents
an end-to-end trainable architecture that retains the multi-dimensional tensor structure throughout
the network.
Related work: Several recent papers apply tensor decomposition to deep learning. Lebedev et al.
(2014) propose using CP decomposition to speed up convolutional layers. Kim et al. (2015) take
a pre-trained network and apply tensor (Tucker) decomposition on the convolutional kernel tensors
and then fine-tune the resulting network. Yang & Hospedales (2016) propose weight sharing in
multi-task learning and Chen et al. (2017) propose sharing residual units. These contributions are
orthogonal to ours and can be applied together.
Novikov et al. (2015) use the Tensor-Train (TT) format to impose low-rank tensor structure on
weights. However, they still retain the fully-connected layers for the output, while we present an
end-to-end tensorized network architecture.
Despite the success of DNNs, many open questions remain as to why they work so well and whether
they really need so many parameters. Tensor methods have emerged as promising tools of analysis
to address these questions and to better understand the success of deep neural networks. Cohen
et al. (2015), for example, use tensor methods as tools of analysis to study the expressive power
of CNNs. Haeffele & Vidal (2015) derive sufficient conditions for global optimality and optimiza-
tion of non-convex factorization problems, including tensor factorization and deep neural network
training. Other papers investigate tensor methods as tools for devising neural network learning al-
gorithms with theoretical guarantees of convergence (Sedghi & Anandkumar, 2016; Janzamin et al.,
2015a;b).
Several prior papers address the power of tensor regression to preserve natural multi-modal structure
and learn compact predictive models (Guo et al., 2012; Rabusseau & Kadri, 2016; Zhou et al.,
2013; Yu & Liu, 2016). However, these works typically rely on analytical solutions and require
manipulating large tensors containing the data. They are usually used for small dataset or require
to downsampled datasets or extract compact features prior to fitting the model, and do not scale to
large datasets such as ImageNet.
To our knowledge, no prior work combines tensor contraction or tensor regression with deep learning
in an end-to-end trainable fashion.
2	Mathematical background
Notation: Throughout the paper, we define tensors as multidimensional arrays, with indexing
starting at 0. First order tensors are vectors, denoted v. Second order tensors are matrices, denoted
M and Id is the identity matrix. We denote X tensors of order 3 or greater. For a third order tensor
X , we denote its element (i, j, k) as Xi1,i2,i3. A colon is used to denote all elements of a mode e.g.
the mode-1 fibers of X are denoted as X:,i2,i3. The transpose of M is denoted M> and its pseudo-
inverse ML Finally, for any i,j ∈ N, [i .. j] denotes the set of integers {i,i + 1,… ，j - 1,j}.
Tensor unfolding: Given a tensor, X ∈ RI0 ×I1 × ×IN, its mode-n unfolding is a matrix X[n] ∈
RIn,IM, with M = QN=0, Ik and is defined by the mapping from element (io,iι,…，iN) to (in,j),
k6=n
with j = PkN=0, ik × QNm=k+1, Im.
k6=n	m6=n
Tensor vectorization: Given a tensor, X ∈ RI0 ×I1 × ×IN ,we can flatten it into a vector Vec(X)
of size (Io ×∙∙∙× IN) defined by the mapping from element (io, iι,…,iN) of X to element j of
VeC(X), with j = PN=O ik × Qm=k+1 Im.
n-mode product: For a tensor X ∈ RI0 × I1 ×∙∙∙× IN and a matrix M ∈ RR×In, the n-mode
product of a tensor is a tensor of size (Io X … X In-ι X R X In+ι X ∙ X IN) and can be expressed
2
Under review as a conference paper at ICLR 2018
using unfolding of X and the classical dot product as:
X Xn M = MX[n] ∈ RI0×…XIn-GRXIn+G,XIN	(1)
Generalized inner-product For two tensors X, Y ∈ RI0 × I1 ×∙∙∙× IN of Same size, their inner
PrOdUCt is defined as hX, Yi = PIO-I PII-I …PIN=-I Xi0,iι,…，in Yi0,i1,…,in FOr two tensors
X ∈ RDx ×I1 ×I2 × ×IN and Y ∈ r1i×12× ×1ν×Dy sharing N modes of same size, we similarly
defined the generalized inner product along the N last (respectively first) modes of X (respectively
I1 -1	I1 -1	IN-1	I I
Y)as hX, YiN = Ei1=0 Σi2=0 …ΣX = 0 X：,il,i2,…,in Yil,i2,…,in,: With (X, Y) N ∈	∙
Tucker decomposition: Given a tensor X ∈ RI0×I1× ×IN, We Can decompose it into a low
rank core G ∈ rr°×ri×∙∙∙×rn by projecting along each of its modes with projection factors
(U⑼，…，U(N)),with U(k) ∈ RRk，Ik,k ∈ (0,…，N).
In other words, we can write:
X = C ×oU(0) ×1 U⑵ ×∙∙∙×N U(N) = JC; U(0),…，U(N)K	(2)
Typically, the factors and core of the decomposition are obtained by solving a least squares prob-
lem. In particular, closed form solutions can be obtained for the factor by considering the n-mode
unfolding of X that can be expressed as:
X[n] = U(n)G[n] (U(O) ㊈…U(nT) ㊈ U(n+1) ㊈•一㊈ U(N))T	(3)
Similarly, we can optimize the core in a straightforward manner by isolating it using the equivalent
rewriting of the above equality:
Vec(X) =(U(0)㊈一 •㊈ U(N)) Vec(G)	(4)
The interested reader is referred to the thorough review of the literature on tensor decompositions
by Kolda & Bader (2009).
3 Tensor Contraction and Tensor Regression
In this section, we explain how to incorporate tensor contractions and tensor regressions into neural
networks as differentiable layers.
3.1	Tensor Contraction
One natural way to incorporate tensor operations into a neural network is to apply tensor contraction
to an activation tensor in order to obtain a low-dimensional representation. We call this technique
the Tensor Contraction layer (TCL). Compared to performing a similar rank reduction with a fully-
connected layer, TCLs require fewer parameters and less computation.
Tensor contraction layers Given an activa-
tion tensor X of size (S0,D0,D1,…,Dn),
the TCL will produce a compact core tensor C
of smaller size (So,R0,R1, •一,Rn) defined
as:
X0 = X ×o V(0) × 1 V(I) ×∙∙∙×n V(N) (5)
with V(k) ∈ RRk,Ik , k ∈ [0 . . N]. Note that
the projections start at the second mode because
the first mode S0 corresponds to the batch.
The projection factors (V(k))代口 …N] are
learned end-to-end with the rest of the network
by gradient backpropagation. In the rest of this
Figure 1: A representation of the Tensor Contrac-
tion Layer (TCL) on a tensor of order 3. The input
tensor X is contracted into a low rank core X0 .
paper, we denote size-(Ro,…，RN) TCL, or TCL-(Ro,…，RN) a TCL that produces a compact
core of dimension (Ro,…，Rn ).
3
Under review as a conference paper at ICLR 2018
τr~>∙	C ɪ ，	1 1 ∕rx τ-κ τ “ ι	∙	， ∙∖> ∙ n	ι ι,ι	ι,	c∙ it	1 ι ι
Figure 2: In standard CNNs, the input X is flattened and then passed to a fully-connected layer,
where it is multiplied by a weight matrix W.
Gradient back-propagation In the case of the TCL, we simply need to take the gradients with
respect to the factors V(k) for each k ∈ 0,…，N ofthe tensor contraction. Specifically, We compute
∂X0 _ ∂X ×o V(O) ×ι V(I) ×∙∙∙×n V(N)
∂v(k) 一	∂v(k)	( )
By reWriting the previous equality in terms of unfolded tensors, We get an equivalent reWriting Where
We have isolated the considered factor:
∂X0[k]	dV(k)X[k] (Id ⑤ V(O)⑤…V(k-1)⑤ V(k+1)⑤…⑤ V(N))T
∂V(k) 一	∂ V(k)	⑺
Model complexity Considering an activation tensor X of Size (So,Do,Dι, ∙∙∙ , Dn), a size一
(Ro,Rι,…，Rn ) Tensor Contraction Layer will have a total of PN=O Dk X Rk parameters.
3.2 Low-Rank Tensor Regression
In order to generate outputs, CNNs typically either flatten the activations or apply a spatial pool-
ing operation. In either case, the discard all multimodal structure, and subsequently apply a full-
connected output layer. Instead, we propose leveraging the spatial structure in the activation tensor
and formulate the output as lying in a low-rank subspace that jointly models the input and the output.
We do this by means of a low-rank tensor regression, where we enforce a low multilinear rank of
the regression weight tensor.
Tensor regression as a layer Let US denote by X ∈ rs,1o×1i×∙∙∙×1n the input activation ten-
sor corresponding to S samples(Xi,…，XS) and Y ∈ RS,O the O corresponding labels for
each sample. We are interested in the problem of estimating the regression weight tensor W ∈
RI0 ×I1 X-XINXO under some fixed low rank (Ro, •-，RN ,Rn+1), such that, Y =〈X, W〉n + b,
i.e.
Y =〈X, W)N + b
subject to W= JC; U(O),…，U(N), U(N+1)K	(8)
With 〈X, WiN = X[O] × W[N +1] the contraction of X by W along their N last (respectively first)
modes, C ∈ RROx…XRNXRN+1, u(k) ∈ RIk XRk for each k in [0 ..N ] and U(N+1) ∈ ROXRN+1.
Previously, this problem has been studied as a standalone one where the input data is directly mapped
to the output, and solved analytically. However, this requires pre-processing the data to extract
(hand-crafted) features to feed the model. In addition, the analytical solution is prohibitive in terms
of computation and memory usage for large datasets.
In this work, we incorporate tensor regressions as trainable layers in neural networks. We do so by
replacing the traditional flattening + fully-connected layers with a tensor regression applied directly
to the high-order input and enforcing low rank constraints on the weights of the regression. We
call our layer the Tensor Regression Layer (TRL). Intuitively, the advantage of the TRL comes from
leveraging the multi-modal structure in the data and expressing the solution as lying on a low rank
manifold encompassing both the data and the associated outputs.
4
Under review as a conference paper at ICLR 2018
Figure 3: We propose to first reduce the dimensionality of the activation tensor by applying tensor
contraction before performing tensor regression. We then replace flattening operators and fully-
connected layers by a TRL. The output is a product between the activation tensor and a low-rank
weight tensor W . For clarity, we illustrate the case of a binary classification, where y is a scalar. For
multi-class, y becomes a vector and the regression weights would become a 4th order tensor.
Gradient backpropagation The gradients of the regression weights and the core with respect to
each factor can be obtained by writing:
∂W _ ∂G ×0 U(O) ×1 U(I) X …XN+1 U(N+1)
∂ U(k)
∂ U(k)
(9)
Using the unfolded expression of the regression weights, we obtain the equivalent formulation:
∂W[k] _ ∂U⑻G[k] (U(O)③…U(I)⑤ U(k+1)⑤…⑤ U(N+I))T
∂ U(k)
∂ U(k)
(10)
Similarly, we can obtain the gradient with respect to the core by considering the vectorized expres-
sions:
∂Vec(W) _ ∂(U(O)⑤…⑤ U(N+1)) Vec(G)
∂VeC(G)
, ,≈.
∂VeC(G)
(11)
Model analysis We consider as input an activation tensor X ∈ rs,1o×1i×∙∙∙×1n, and a rank-
(Ro,Ri,…，Rn,Rn+i) tensor regression layer, where, typically, Rk ≤ Ik. Let’s assume the
N
output is n-dimensional. A fully-connected layer taking X as input will haVe nFC = n ×	k=O Ik
parameters.
By comparison, the TRL has a number of parameters nTRL, with:
N
N+1
nTRL =	Rk +	Rk × Ik + RN+1 × n
(12)
k=0
k=0
4	Experiments
We empirically demonstrate the effectiVeness of preserVing the tensor structure through tensor con-
traction and tensor regression by integrating it into state-of-the-art architectures and demonstrating
similar performance on the popular ImageNet dataset. In particular, we empirically Verify the effec-
tiVeness of the TCL on VGG-19 (Simonyan & Zisserman, 2014) and conduct thorough experiment
with the tensor regression on ResNet-50 and ResNet-101 (He et al., 2015).
4.1	Experimental setting
Synthetic data To illustrate the effectiVeness of the low-rank tensor regression, we first apply it
to synthetic data y = VeC(X) X W where each sample X ∈ R(64) follows a Gaussian distribution
5
Under review as a conference paper at ICLR 2018
(a)	(b)
Figure 4: Empirical comparison (4a) of the TRL against regression with a fully-connected layer. We
plot the weight matrix of both the TRL and a fully-connected layer. Due to its low-rank weights, the
TRL better captures the structure in the weights and is more robust to noise. Evolution of the RMSE
as a function of the training set size (4b) for both the TRL and fully-connected regression
Table 1: Results obtained on ImageNet by adding a TCL to a VGG-19 architecture. We reduce the
number of hidden units proportionally to the reduction in size of the activation tensor following the
tensor contraction. Doing so allows more than 65% space savings over all three fully-connected
layers (i.e. 99.8% space saving over the fully-connected layer replaced by the TCL) with no corre-
sponding decrease in performance (comparing to the standard VGG network as a baseline).
Method		Accuracy		Space SavingS (%)
TCL-SiZe	Hidden UnitS	Top-1 (%)	Top-5 (%)	
baseline	4096	68.7	88	0
(512, 7, 7)	4096	69.4	88.3	-0.21
(384, 5, 5)	3072	68.3	87.8	65.87
A f f C∖ Cx ∖ TAT ∙	Γ∙ 1	j ∙	1.1	11	1	/ -15∖ . . TAT IpT ,1	,	♦	,1
N(0, 3). W is a fixed matrix and the labels are generated as y = vec(X ) × W. We then train the
data on X + E, where E is added Gaussian noise sampled from N (0, 3). We compare i) a TRL with
squared loss and ii) a fully-connected layer with a squared loss. In Figure 4a, we show the trained
weight of both a linear regression based on a fully-connected layer and a TRL with various ranks,
both obtained in the same setting. As can be observed in Figure 5b, the TRL is easier to train on
small datasets and less prone to over-fitting, due to the low rank structure of its regression weights,
as opposed to typical Fully Connected based Linear Regression.
ImageNet Dataset We ran our experiments on the widely-used ImageNet-1K dataset, using sev-
eral widely-popular network architectures. The ILSVRC dataset (ImageNet) is composed of 1.2
million images for training and 50, 000 for validation, all labeled for 1,000 classes. Following
(Huang et al., 2016a; He et al., 2015; Huang et al., 2016b; He et al., 2016), we report results on the
validation set in terms of Top-1 accuracy and Top-5 accuracy across all 1000 classes. Specifically,
we evaluate the classification error on single 224 × 224 single center crop from the raw input images.
Training the TCL + TRL When experimenting with the tensor regression layer, we did not retrain
the whole network each time but started from a pre-trained ResNet. We experimented with two
settings: i) We replaced the last average pooling, flattening and fully-connected layer by either a
TRL or a combination of TCL + TRL and trained these from scratch while keeping the rest of the
network fixed. ii) We investigate replacing the pooling and fully-connected layers with a TRL that
jointly learns the spatial pooling as part of the tensor regression. In that setting, we also explore
initializing the TRL by performing a Tucker decomposition on the weights of the fully-connected
layer.
6
Under review as a conference paper at ICLR 2018
Implementation details We implemented all models using the MXNet library (Chen et al., 2015)
and ran all experiments training with data parallelism across multiple GPUs on Amazon Web Ser-
vices, with 4 NVIDIA k80 GPUs. For training, we adopt the same data augmentation procedure as
in the original Residual Networks (ResNets) paper (He et al., 2015).
When training the layers from scratch, we found it useful to add a batch normalization layer (Ioffe
& Szegedy, 2015) before and after the TCL/TRL to avoid vanishing or exploding gradients, and to
make the layers more robust to changes in the initialization of the factors. In addition we constrain
the weights of the tensor regression by applying `2 normalization (Salimans & Kingma, 2016) to the
factors of the Tucker decomposition.
4.2	Results
Table 2: Results obtained with ResNet-50 on ImageNet. The first row corresponds to the standard
ResNet. Rows 2 and 3 present the results obtained by replacing the last average pooling, flattening
and fully-connected layers with a TRL. In the last row, we have also added a TCL.
Method			Accuracy	
Architecture	TCL-size	TRL rank	Top-1 (%)	Top-5 (%)
Resnet-50	baseline with spatial pooling		74.58	92.06
	-	(1000, 2048, 7, 7)	73.6	91.3
	-	(500, 1024, 3,3)	72.16	90.44
	(1024, 3, 3)	(1000, 1024, 3, 3)	73.43	91.3
Resnet-101	baseline with spatial pooling		77.1	93.4
	-	(1000, 2048, 7, 7)	76.45	92.9
	-	(500, 1024, 3,3)	76.7	92.9
	(1024, 3, 3)	(1000,1024, 3, 3)	76.56	93
Impact of the tensor contraction layer We first investigate the effectiveness of the TCL using
a VGG-19 network architecture (Simonyan & Zisserman, 2014). This network is especially well-
suited for out methods because of its 138, 357, 544 parameters, 119, 545, 856 of which (more than
80% of the total number of parameters) are contained in the fully-connected layers. By adding TCL
to contract the activation tensor prior to the fully-connected layers we can achieve large space saving.
We can express the space saving of a model M with nM total parameters in its fully-connected layers
with respect to a reference model R with nR total parameters in its fully-connected layers as 1 - InM
(bias excluded).
Table 1 presents the accuracy obtained by the different combinations of TCL in terms of top-1 and
top-5 accuracy as well as space saving. By adding a TCL that preserves the size of its input we are
able to obtain slightly higher performance with little impact on the space saving (0.21% of space
loss) while by decreasing the size of the TCL we got more than 65% space saving with almost no
performance deterioration.
Overcomplete TRL We first tested the TRL with a ResNet-50 and a ResNet-101 architectures on
ImageNet, removing the average pooling layer to preserve the spatial information in the tensor. The
full activation tensor is directly passed on to a TRL which produces the outputs on which we apply
softmax to get the final predictions. This results in more parameters as the spatial dimensions are
preserved. To reduce the computational burden but preserve the multi-dimensional information, we
alternatively insert a TCL before the TRL. In Table 2, we present results obtained in this setting on
ImageNet for various configurations of the network architecture. In each case, we report the size of
the TCL (i.e. the dimension of the contracted tensor) and the rank of the TRL (i.e. the dimension of
the core of the regression weights).
Joint spatial pooling and low-rank regression Alternatively, we can learn the spatial pooling as
part of the tensor regression. In this case, we remove the average pooling layer and feed the tensor
7
Under review as a conference paper at ICLR 2018
Table 3: Results obtained with a ResNet-101 architecture on ImageNet, learning spatial pooling as
part of the TRL.
TRL rank	Performance (%)		
	Top-1	Top-5	Space savings
baseline	77.1	934	0
(200, 1, 1, 200)	77.1	93.2	68.2
(150, 1, 1, 150)	76	92.9	76.6
(100, 1, 1, 100)	74.6	91.7	84.6
(50, 1, 1, 50)	73.6	91	92.4
(a) Accuracy as a function of the core size
Figure 5: 5a shows the Top-1 accuracy (in %) as we vary the size of the core along the number of
outputs and number of channels (the TRL does spatial pooling along the spatial dimensions, i.e., the
core has rank 1 along these dimensions).
(b) Accuracy as a function of space savings
of size (batch size, number of channels, height, width) to the TRL, while imposing a rank of 1 on
the spatial dimensions of the core tensor of the regression. Effectively, this setting simultaneously
learns weights for the multi-linear spatial pooling as well as the regression.
In practice, to initialize the weights of the TRL in this setting, we consider the weight of fully-
connected layer from a pre-trained model as a tensor of size (batch size, number of channels, 1,
1, number of classes) and apply a partial tucker decomposition to it by keeping the first dimension
(batch-size) untouched. The core and factors of the decomposition then give us the initialization
of the TRL. The projection vectors over the spatial dimension are then initialize to heg^ and Width，
respectively. The Tucker decomposition was performed using TensorLy (Kossaifi et al., 2016). In
this setting, we show that we can drastically decrease the number of parameters with little impact on
performance. In Figure 5, we show the change of the Top-1 and Top-5 accuracy as we decrease the
size of the core tensor of the TRL and also the space savings.
5	Conclusions
Unlike fully-connected layers, TCLs and TRLs obviate the need to flatten input tensors. Our exper-
iments demonstrate that by imposing a low-rank constraint on the weights of the regression, we can
learn a low-rank manifold on which both the data and the labels lie. The result is a compact network,
that achieves similar accuracies with many fewer parameters. Going forward, we plan to apply the
TCL and TRL to more network architectures. We also plan to leverage recent work (Shi et al., 2016)
on extending BLAS primitives to avoid transpositions needed when computing tensor contractions.
8
Under review as a conference paper at ICLR 2018
References
Animashree Anandkumar, Rong Ge, Daniel J Hsu, Sham M Kakade, and Matus Telgarsky. Tensor
decompositions for learning latent variable models. Journal of Machine Learning Research, 15
(1):2773-2832, 2014.
Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu,
Chiyuan Zhang, and Zheng Zhang. Mxnet: A flexible and efficient machine learning library for
heterogeneous distributed systems. CoRR, abs/1512.01274, 2015.
Yunpeng Chen, Xiaojie Jin, Bingyi Kang, Jiashi Feng, and Shuicheng Yan. Sharing residual units
through collective tensor factorization in deep neural networks. 2017.
Nadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A tensor
analysis. CoRR, abs/1509.05009, 2015.
W. Guo, I. Kotsia, and I. Patras. Tensor learning for regression. IEEE Transactions on Image
Processing, 21(2):816-827, Feb 2012.
Benjamin D. Haeffele and Rene Vidal. Global optimality in tensor factorization, deep learning, and
beyond. CoRR, abs/1506.07540, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. CoRR, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. CoRR, abs/1603.05027, 2016.
Gao Huang, Zhuang Liu, and Kilian Q. Weinberger. Densely connected convolutional networks.
CoRR, abs/1608.06993, 2016a.
Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q. Weinberger. Deep networks with
stochastic depth. CoRR, abs/1603.09382, 2016b.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. CoRR, abs/1502.03167, 2015.
Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Generalization bounds for neural networks
through tensor factorization. CoRR, abs/1506.08473, 2015a.
Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of non-convexity: Guar-
anteed training of neural networks using tensor methods. CoRR, 2015b.
Yong-Deok Kim, Eunhyeok Park, Sungjoo Yoo, Taelim Choi, Lu Yang, and Dongjun Shin. Com-
pression of deep convolutional neural networks for fast and low power mobile applications. CoRR,
abs/1511.06530, 2015.
Tamara G. Kolda and Brett W. Bader. Tensor decompositions and applications. SIAM REVIEW, 51
(3):455-500, 2009.
Jean Kossaifi, Yannis Panagakis, and Maja Pantic. Tensorly: Tensor learning in python. ArXiv
e-print, 2016.
Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan V. Oseledets, and Victor S. Lempit-
sky. Speeding-up convolutional neural networks using fine-tuned cp-decomposition. CoRR,
abs/1412.6553, 2014.
Alexander Novikov, Dmitry Podoprikhin, Anton Osokin, and Dmitry Vetrov. Tensorizing neural
networks. In Proceedings of the 28th International Conference on Neural Information Processing
Systems, NIPS’15, pp. 442-450, 2015.
Guillaume Rabusseau and Hachem Kadri. Low-rank regression with tensor responses. In D. D. Lee,
M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), NIPS, pp. 1867-1875. 2016.
9
Under review as a conference paper at ICLR 2018
Tim Salimans and Diederik P Kingma. Weight normalization: A simple reparameterization to ac-
celerate training of deep neural networks. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon,
and R. Garnett (eds.), NIPS,pp. 901-909. 2016.
Hanie Sedghi and Anima Anandkumar. Training input-output recurrent neural networks through
spectral methods. CoRR, abs/1603.00954, 2016.
Y. Shi, U. N. Niranjan, A. Anandkumar, and C. Cecka. Tensor contractions with extended blas
kernels on cpu and gpu. In 2016 IEEE 23rd International Conference on High Performance
Computing (HiPC), pp. 193-202, Dec 2016.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. CoRR, abs/1409.1556, 2014.
Yongxin Yang and Timothy M. Hospedales. Deep multi-task representation learning: A tensor
factorisation approach. CoRR, abs/1605.06391, 2016.
Qi Rose Yu and Yan Liu. Learning from multiway data: Simple and efficient tensor regression.
CoRR, abs/1607.02535, 2016.
Hua Zhou, Lexin Li, and Hongtu Zhu. Tensor regression with applications in neuroimaging data
analysis. Journal of the American Statistical Association, 108(502):540-552, 2013.
10