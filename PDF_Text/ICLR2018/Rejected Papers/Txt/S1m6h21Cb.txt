Under review as a conference paper at ICLR 2018
THE Cramer Distance AS A Solution to Biased
Wasserstein Gradients
Anonymous authors
Paper under double-blind review
Ab stract
The Wasserstein probability metric has received much attention from the machine
learning community. Unlike the Kullback-Leibler divergence, which strictly mea-
sures change in probability, the Wasserstein metric reflects the underlying geom-
etry between outcomes. The value of being sensitive to this geometry has been
demonstrated, among others, in ordinal regression and generative modelling, and
most recently in reinforcement learning. in this paper we describe three natural
properties of probability divergences that we believe reflect requirements from
machine learning: sum invariance, scale sensitivity, and unbiased sample gradi-
ents. The Wasserstein metric possesses the first two properties but, unlike the
Kullback-Leibler divergence, does not possess the third. We provide empirical
evidence suggesting this is a serious issue in practice. Leveraging insights from
probabilistic forecasting we propose an alternative to the Wasserstein metric, the
Cramer distance. We show that the Cramer distance possesses all three desired
properties, combining the best of the Wasserstein and Kullback-Leibler diver-
gences. We give empirical results on a number of domains comparing these three
divergences. To illustrate the practical relevance of the Cramer distance we design
a new algorithm, the Cramer Generative Adversarial Network (GAN), and show
that it has a number of desirable properties over the related Wasserstein GAN.
1	Introduction
in machine learning, the Kullback-Leibler (KL) divergence is perhaps the most common way of as-
sessing how well a probabilistic model explains observed data. Among the reasons for its popularity
is that it is directly related to maximum likelihood estimation and is easily optimized. However,
the KL divergence suffers from a significant limitation: it does not take into account how close two
outcomes might be, but only their relative probability. This closeness can matter a great deal: in
image modelling, for example, perceptual similarity is key (Rubner et al., 2000; Gao & Kleywegt,
2016). Put another way, the KL divergence cannot reward a model that “gets it almost right”.
To address this limitation, researchers have turned to the Wasserstein metric, which does incorporate
the underlying geometry between outcomes. The Wasserstein metric can be applied to distributions
with non-overlapping supports, and has good out-of-sample performance (Esfahani & Kuhn, 2015).
Yet, practical applications of the Wasserstein distance, especially in deep learning, remain tentative.
in this paper we provide a clue as to why that might be: estimating the Wasserstein metric from
samples yields biased gradients, and may actually lead to the wrong minimum. This precludes using
stochastic gradient descent (SGD) and SGD-like methods, whose fundamental mode of operation is
sample-based, when optimizing for this metric.
As a replacement we propose the Cramer distance (Szekely, 2002; Rizzo & Szekely, 2016), also
known as the continuous ranked probability score in the probabilistic forecasting literature (Gneiting
& Raftery, 2007). The Cramer distance, like the Wasserstein metric, respects the underlying geom-
etry but also has unbiased sample gradients. To underscore our theoretical findings, we demonstrate
a significant quantitative difference between the two metrics when employed in typical machine
learning scenarios: categorical distribution estimation, regression, and finally image generation. in
the latter case, we use a multivariate generalization of the Cramer distance, the energy distance
(Szekely, 2002), itself an instantiation of the MMD family of metrics (Gretton et al., 2012).
1
Under review as a conference paper at ICLR 2018
2	Probability Divergences and Metrics
In this section we provide the notation to mathematically distinguish the Wasserstein metric (and
later, the Cramer distance) from the KUllback-Leibler divergence and probability distances such as
the total variation.
Let P bea probability distribution over R. When P is continuous, We will assume it has density μP.
The expectation of a function f : R → R with respect to P is
XEPf(X)= Zlf 3P(dx) =
f f (x)μp(x)dx if P is continuous, and
Pf (x)P (x)	ifP is discrete.
We will suppose all expectations and integrals under consideration are finite. We will often associate
P to a random variable X, such that for a subset of the reals A ⊆ R, we have Pr{X ∈ A} = P (A).
The (cumulative) distribution function of P is then
FP (x) := Pr{X ≤ x}
r
-∞
P (dx).
Finally, the inverse distribution function of P, defined over the interval (0, 1], is
FP-1 (u) := inf {x : FP (x) = u}.
2.1	Divergences and Metrics
Consider two probability distributions P and Q over R. A divergence d is a mapping (P, Q) 7→ R+
with d(P, Q) = 0 if and only if P = Q almost everywhere. A popular choice is the Kullback-
Leibler (KL) divergence
KL(P k Q) := Γ log P^P(dx),
-∞	Q(dx)
with KL(P k Q) = ∞ if P is not absolutely continuous w.r.t. Q. The KL divergence, also called
relative entropy, measures the amount of information needed to encode the change in probability
from Q to P (Cover & Thomas, 1991).
A probability metric is a divergence which is also symmetric (d(P, Q) = d(Q, P)) and respects
the triangle inequality: for any distribution R, d(P, Q) ≤ d(P, R) + d(R, Q). We will use the
term probability distance to mean a symmetric divergence satisfying the relaxed triangle inequality
d(P, Q) ≤ c [d(P, R) + d(R, Q)] for some c ≥ 1.
We will first study the p-Wasserstein metrics wp (Dudley, 2002). For 1 ≤ p < ∞, a practical
definition is through the inverse distribution functions ofP and Q:
wp(P,Q) := Z1FP-1(u)-FQ-1(u)pdu1/p.	(1)
We will sometimes find it convenient to deal with the pth power of the metric, which we will denote
by wpp ; note that wpp is not a metric proper, but is a probability distance.
We will be chiefly concerned with the 1-Wasserstein metric, which is most commonly used in prac-
tice. The 1-Wasserstein metric has a dual form which is theoretically convenient and which we
mention here for completeness. Define F∞ to be the class of 1-Lipschitz functions. Then
w1 (P, Q) := sup	Ef (x) - Ef (x).	(2)
f ∈f∞ "〜P	X〜Q	'
This is a special case of the celebrated Monge-Kantorovich duality (Rachev et al., 2013), and is
the integral probability metric (IPM) with function class F∞ (Muller, 1997). We invite the curious
reader to consult these two sources as a starting point on this rich topic.
2.2	Properties of a Divergence
As noted in the introduction, the fundamental difference between the KL divergence and the Wasser-
stein metric is that the latter is sensitive not only to change in probability but also to the geometry of
possible outcomes. To capture this notion we now introduce the concept of an ideal divergence.
2
Under review as a conference paper at ICLR 2018
Consider a divergence d, and for two random variables X, Y with distributions P, Q write
d(X, Y ) := d(P, Q). We say that d is scale sensitive (of order β), i.e. it has property (S), if
there exists a β > 0 such that for all X, Y , and a real value c > 0,
d(cX,cY) ≤ ∣c∣βd(X,Y).	(S)
A divergence d has property (I), i.e. it is sum invariant, if whenever A is independent from X, Y
d(A+X,A+Y) ≤d(X,Y).	(I)
Following Zolotarev (1976), an ideal divergence d is one that possesses both (S) and (I).1
We can illustrate the sensitivity of ideal divergences to the value of outcomes by considering Dirac
functions δx at different values of x. If d is scale sensitive of order β = 1 then the divergence
d(δo, δ1∕2) can be no more than half the divergence d(δ0,δι). If d is sum invariant, then the
divergence of δ0 to δ1 is equal to the divergence of the same distributions shifted by a constant c, i.e.
of δc to δ1+c. As a concrete example of the importance of these properties, Bellemare et al. (2017)
recently demonstrated the importance of ideal metrics in reinforcement learning, specifically their
role in providing the contraction property of the distributional Bellman operator. In particular, the
contraction modulus is γβ, where γ ∈ [0, 1) is a discount factor and β is the scale sensitivity order.
In machine learning we often view the divergence d as a loss function. Specifically, let Qθ be some
distribution parametrized by θ, and consider the loss θ 7→ d(P, Qθ). We are interested in minimizing
this loss, that is finding θ* := argmin& d(P, Qθ). We now describe a third property based on this
loss, which we call unbiased sample gradients.
Let Xm := X1, X2, . . . , Xm be independent samples from P and define the empirical distribution
Pm := Pm(Xm) := * Pm=I δχi (note that Pm is a random quantity). From this, define the sample
ʌ
loss θ 7→ d(Pm, Qθ). We say that d has unbiased sample gradients when the expected gradient of
the sample loss equals the gradient of the true loss for all P and m:
__ ___ ʌ _____________
E Vθd(Pm, Qθ) = Vθd(P, Qθ).	(U)
Xm〜P
The notion of unbiased sample gradients is ubiquitous in machine learning and in particular in deep
learning. Specifically, if a divergence d does not possess (U) then minimizing it with stochastic
gradient descent may not converge, or it may converge to the wrong minimum. Conversely, if d
possesses (U) then we can guarantee that the distribution which minimizes the expected sample loss
is Q = P. In the probabilistic forecasting literature, this makes d a proper scoring rule (Gneiting &
Raftery, 2007).
We now characterize the KL divergence and the Wasserstein metric in terms of these properties. As
it turns out, neither simultaneously possesses both (U) and (S).
Proposition 1. The KL divergence has unbiased sample gradients (U), but is not scale sensitive (S).
Proposition 2. The Wasserstein metric is ideal (I, S), but does not have unbiased sample gradients.
We will provide a proof of the bias in the sample Wasserstein gradients just below; the proof of the
rest and later results are provided in the appendix.
3	Bias in the S ample Gradient Estimates of the Wasserstein
Distance
In this section we give theoretical evidence of serious issues with gradients of the sample Wasserstein
loss. We will consider a simple Bernoulli distribution P with parameter θ* ∈ (0,1), which we would
like to estimate from samples. Our model is Qθ, a Bernoulli distribution with parameter θ. We study
the behaviour of stochastic gradient descent w.r.t. θ over the sample Wasserstein loss, specifically
using the pth power of the metric (as is commonly done to avoid fractional exponents). Our results
build on the example given by Bellemare et al. (2017), whose result is for θ* = 2 and m = 1.
1Properties (S) and (I) are called regularity and homogeneity by Zolotarev; we believe our choice of terms
is more machine learning-friendly.
3
Under review as a conference paper at ICLR 2018
Consider the estimate VθWp(Pm, Qθ) of the gradient VθWp(P) Qθ). We now show that even in this
simplest of settings, this estimate is biased, and we exhibit a lower bound on the bias for any value
of m. Hence the Wasserstein metric does not have property (U). More worrisome still, we show
that the minimum of the expected empirical Wasserstein loss θ → EXm [wp(Pm, Qθ)] is not the
minimum of the Wasserstein loss θ 7→ Wpp(P, Qθ). We then conclude that minimizing the sample
Wasserstein loss by stochastic gradient descent may in general fail to converge to the minimum of
the true loss.
Theorem 1.	Let Pm = + Pi=I δχi be the empirical distribution derived from m independent
samples Xm = X1, . . . , Xm drawn from a Bernoulli distribution P. Then for all 1 ≤ p < ∞,
•	Non-vanishing minimax bias of the sample gradient. For any m ≥ 1 there exists a pair of
Bernoulli distributions P, Qθ for which
I E [VθWp(Pm,Qθ)] -Vθwp(P,Qθ)1 ≥ 2e-2;
I Xm〜P L	尸	一	厂	I
∙-v
•	Wrong minimum of the sample Wasserstein loss. The minimum of the expected sample loss θ =
argmi□θ EXm [wp(Pm, Qθ )] is in general different from the minimum of the true Wasserstein
loss θ* = argminj Wp(P) Qθ).
•	Deterministic solutions to stochastic problems. For any m ≥ 1, there exists a distribution P
with nonzero entropy whose sample loss is minimized by a distribution Qg with zero entropy.
Taken as a whole, Theorem 1 states that we cannot in general minimize the Wasserstein loss using
naive stochastic gradient descent methods. Although our result does not imply the lack ofa stochas-
tic optimization procedure for this loss,2 we believe our result to be cause for concern. We leave as
an open question whether an unbiased optimization procedure exists and is practical.
Wasserstein Bias in the Literature
Our result is surprising given the prevalence of the Wasserstein metric in empirical studies. We
hypothesize that this bias exists in published results and is an underlying cause of learning instability
and poor convergence often remedied to by heuristic means. For example, Frogner et al. (2015) and
Montavon et al. (2016) reported the need for a mixed KL-Wasserstein loss to obtain good empirical
results, with the latter explicitly discussing the issue of wrong minima when using Wasserstein
gradients.
We remark that our result also applies to the dual (2), since the losses are the same. This dual
was recently considered by Arjovsky et al. (2017) as an alternative loss to the primal (1). The
adversarial procedure proposed by the authors is a two time-scale process which first maximizes (2)
w.r.t f ∈ F∞ using m samples, then takes a single stochastic gradient step w.r.t. θ. Interestingly,
this approach does seem to provide unbiased gradients as m → ∞. However, the cost of a single
gradient is now significantly higher, and for a fixed m we conjecture that the minimax bias remains.
4 THE Cramer Distance
We are now ready to describe an alternative to the Wasserstein metric, the Cramer distance (Szekely,
2002; Rizzo & Szekely, 2016). As we shall see, the Cramer distance has the same appealing prop-
erties as the Wasserstein metric, but also provides us with unbiased sample gradients. As a result,
we believe this underappreciated distance is an appealing alternative to the Wasserstein metric for
many machine learning applications.
4.1 Definition and Analysis
Recall that for two distributions P and Q over R, their (cumulative) distribution functions are re-
spectively FP and FQ. The (squared) Cramer distance between P and Q is
Z∞
∞
(FP (x) - FQ (x))2dx.
2For example, if P has finite support keeping track of the empirical distribution suffices.
4
Under review as a conference paper at ICLR 2018
Figure 1: Leftmost. Target distribution. One outcome (10) is significantly more distant than the
two others (0, 1). Rest. Distributions minimizing the divergences discussed in this paper, under the
constraint Q(1) = Q(10). Both Wasserstein metric and Cramer distance underemphasize Q(0) to
better match the cumulative distribution function. The sample Wasserstein loss result is for m = 1.
The Cramer distance is a Bregman divergence, and is a member of the lp family of divergences
lp(P, Q) := Z	|FP (x) - FQ(x)|pdx	.
The lp and Wasserstein metrics are identical at p = 1, but are otherwise distinct. As the following
theorem shows, the Cramer distance possesses unique properties.
Theorem 2.	Consider two random variables X, Y, a random variable A independent of X, Y, and
a real value c > 0. Then for 1 ≤ p ≤ ∞,
(I) lp(A+X,A+Y) ≤ lp(X, Y)	(S) lp(cX, cY) ≤ |c|1/plp(X, Y).
Furthermore, the Cramer distance has unbiased sample gradients. That is, given Xm :=
Xi,..., Xm drawn from a distribution P, the empirical distribution Pm := m Pm=I δχi, and a
distribution Qθ,
E	Vθ∕2(Pm,Qθ ) = Vθ l2(P,Qθ),
Xm〜P
and of all the lp distances, only the Cramer (p = 2) has this property.
We conclude that the Cramer distance enjoys both the benefits of the Wasserstein metric and the
SGD-friendliness of the KL divergence. Given the close similarity of the Wasserstein and lp metrics,
it is truly remarkable that only the Cramer distance has unbiased sample gradients.
4.2 Comparison to the 1 -Wasserstein Metric
To illustrate how the Cramer distance compares to the 1-Wasserstein metric, we consider modelling
the discrete distribution P depicted in Figure 1 (left). Since the trade-offs between metrics are only
apparent when using an approximate model, we use an underparametrized discrete distribution Qθ
which assigns the same probability to x = 1 and x = 10. That is,
1	eθ
Qθ(O)= Qθ{x = 0} = 1 + 2eθ Qθ⑴=Qθ(Io) = 1 + 2eθ .
Figure 1 depicts the distributions minimizing the various divergences under this parametrization. In
particular, the Cramer solution is relatively close to the 1-Wasserstein solution. Furthermore, the
minimizer of the sample Wasserstein loss (m = 1) clearly provides abad solution (most of the mass
is on 0). Note that, as implied by Theorem 1, the bias shown here would arise even if the distribution
could be exactly represented.
To further show the impact of the Wasserstein bias we used gradient descent to minimize either the
true or sample losses with a fixed step-size (α = 0.001). In the stochastic setting, at each step we
ʌ
construct the empirical distribution Pm from m samples (a Dirac when m = 1), and take a gradient
step. We measure the performance of each method in terms of the true 1-Wasserstein loss.
Figure 2 (left) plots the resulting training curves in the 1-Wasserstein regime, with the KL and
Cramer solutions indicated for reference. We first note that, compared to the KL solution, the
Cramer solution has significantly smaller Wasserstein distance to the target distribution. Second, for
5
Under review as a conference paper at ICLR 2018
15	20
Training Epochs
WaSSerStein
KL
φuu5ssf-s,lφSSOM
Training Epochs
Figure 2: Left. Wasserstein distance in terms of SGD updates, minimizing the true or sample
WaSSerStein losses. Also shown are the distances for the KL and Cramer solutions. Results are
averaged over 10 random initializations, with error-bands indicating one standard deviation. Center.
Ordinal regression on the Year Prediction MSD dataset. Learning curves report RMSE on test set.
Right. The same in terms of sample Wasserstein loss.
small sample sizes stochastic gradient descent fails to find reasonable solutions, and for m = 1 even
converges to a solution worse than the KL minimizer. This small experiment highlights the cost
incurred from minimizing the sample Wasserstein loss, and shows that increasing the sample size
may not be sufficient to guarantee good behaviour.
Ordinal Regression
We next trained a neural network in an ordinal regression task using either of the three divergences.
The task we consider is the Year Prediction MSD dataset (Lichman, 2013). In this task, the model
must predict the year a song was written (from 1922 to 2011) given a 90-dimensional feature rep-
resentation. In our setting, this prediction takes the form of a probability distribution. We measure
each method’s performance on the test set (Figure 2) in two ways: root mean squared error (RMSE)
一the metric minimized by Hernandez-Lobato & Adams (2015) — and the sample Wasserstein loss.
Full details on the experiment may be found in the appendix.
The results show that minimizing the sample Wasserstein loss results in significantly worse perfor-
mance. By contrast, minimizing the Cramer distance yields the lowest RMSE and Wasserstein loss,
confirming the practical importance of having unbiased sample gradients. Naturally, minimizing
for one loss trades off performance with respect to the others, and minimizing the Cramer distance
results in slightly higher negative log likelihood than when minimizing the KL divergence (Figure 7
in appendix). We conclude that, in the context of ordinal regression where outcome similarity plays
an important role, the Cramer distance should be preferred over either KL or the Wasserstein metric.
5 Multivariate Distributions
The energy distance (Szekely, 2002) is a natural extension of the Cramer distance to the multivariate
case. Let P, Q be probability distributions over Rd and let X, X 0 and Y, Y 0 be independent random
variables distributed according to P and Q, respectively. The energy distance (sometimes called the
squared energy distance, see e.g. Rizzo & Szekely, 2016) is
E(P,Q) :=E(X,Y) :=2EkX-Yk2-EkX-X0k2-EkY -Y0k2.	(3)
Szekely showed that, in the univariate case, 12 (P, Q) = 2E(P,Q). Interestingly enough, the energy
distance can also be written in terms of a difference of expectations. For
f (x) ：= E kx — Y0k2- E kx - X0k2 ,
we find that
E (X,Y )= E f*(X) - E f (Y).	(4)
The energy distance is closely related to the distances known as maximum mean discrepancies
(MMDs; Gretton et al., 2012); in particular, Sejdinovic et al. (2013) showed that the energy distance
is equivalent to the squared MMD with kernel k(x, y) = kxk2 + kyk2 - kx - yk2. Finally, we
remark that E also possesses properties (I), (S), and (U) (proof in the appendix).
6
Under review as a conference paper at ICLR 2018
given left halves are from CelebA 64x64 validation set (Liu et al., 2015).
Algorithm 1: Cramer GAN Losses.	2
Parameter. Gradient penalty coefficient λ.
Sample Xr 〜P, Xg, xg 〜Q, e 〜Uniform(0,1).
Interpolate real and generated samples:
X = exr + (1 — e)xg
Sample generator loss (12):
ʌ
Lg = kh(Xr) -h(Xg)k2+kh(Xr)-h(X0g)k2
-kh(Xg) -h(X0g)k2
Sample surrogate generator loss (13) and critic loss:
Ls (u, v) = kh(Xr) - h(v)k2 - kh(Xr)k2
- kh(u) - h(v)k2 + kh(u)k2
Ls = 1 L S(Xg ,χg) + Ls(Xg, Xg)]
f(X) = kh(X) -h(Xg)k2 - kh(X) -h(Xr)k2
LCritiC = -Ls + λ(kVχf(X)k2 - I)2
IO1
φou2sδu's-s,lφSSDM
10°
0	20000	40000	60000	80000100000
Generator Steps
Figure 4: Approximate Wasserstein
distances between CelebA test set and
the generators. Nu is the number
critic updates per generator update.
5.1	CRAMERGAN
We now consider the Generative Adversarial Networks (GAN) framework (Goodfellow et al., 2014),
in particular issues arising in the Wasserstein GAN (Arjovsky et al., 2017), and propose a better GAN
based on the Cramer distance. A GAN is composed of a generative model Q (in our experiments,
over images), called the generator, a target source P, and a trainable loss function called a dis-
criminator or critic. GANs are particularly interesting because we can establish a direct comparison
between the two distances. Our choice of name reflects this fact, and We prefer Cramer GAN to the
perhaps more technically correct, but less palatable Energy Distance GAN. In theory, the Wasser-
stein GAN algorithm requires training the critic until convergence, but this is rarely achievable: we
would require a critic that is a very powerful network to approximate the Wasserstein distance well
(Arora et al., 2017). Simultaneously, training this critic to convergence would overfit the empirical
distribution of the training set, which is undesirable.
Our proposed loss function allows for useful learning with imperfect critics by combining the energy
distance with a transformation function h : Rd → Rk, where d is the input dimensionality and
k = 256 in our experiments. The generator then seeks to minimize the energy distance of the
transformed variables E(h(X), h(Y )), where X is a real sample and Y is a generated sample. The
critic itself seeks to maximize this same distance by changing the parameters of h, subject to a soft
constraint (the gradient penalty used by Gulrajani et al., 2017). Specifically, the critic maximizes a
surrogate loss whose gradient can be estimated from a single real sample. The Cramer GAN losses
are summarized in Algorithm 1, with additional design choices detailed in Appendix C.
7
Under review as a conference paper at ICLR 2018
We note that MMDs such as the energy distance have in the last year become an appealing tool
for training GANs. Among others, the squared MMD is used within Generative Moment Matching
Networks (Li et al., 2015; Dziugaite et al., 2015); Bouchacourt et al. (2016) trained a model to
minimize the energy distance for hand pose estimation. Our use of the tranformation h(x) reflects
our anecdotal finding that the direct minimization of the energy distance over raw images does not
work well (see Figure 10 in appendix). Similar findings can be found in the work of Mroueh et al.
(2017) and the independently developed MMD GAN (Li et al., 2017), which additionally uses an
auto-encoder loss to make the transformation injective.
The Cramer GAN We present here complements our comparison of the Wasserstein and Cramer
distance from previous sections. At the same time, our experiments also provide novel GAN-related
contributions, including the ability to perform conditional modelling using a surrogate generator
loss, Which lets us train the critic even When only one independent sample from P is available. We
note also that in our experiments, kx - yk2 distances Were more stable than distances generated by
Gaussian or Laplacian kernels.
5.2	CRAMER GAN Experiments
We noW shoW that, compared to the improved Wasserstein GAN (WGAN-GP) of Gulrajani et al.
(2017), the Cramer GAN leads to more stable learning and increased diversity in the generated
samples. In both cases We train generative models that predict the right half of an image given the
left half; samples from unconditional models are provided in the appendix (Figure 10). The dataset
We use here is the CelebA 64 × 64 dataset (Liu et al., 2015) of celebrity faces.
Increased diversity. In our first experiment, We compare the qualitative diversity of completed faces
by shoWing three sample completions generated by either model given the left half of a validation
set image (Figure 3). We observe that the completions produced by WGAN-GP are almost deter-
ministic. Our findings echo those of Isola et al. (2016), Who observed that “the generator simply
learned to ignore the noise.” By contrast, the completions produced by Cramer GAN are fairly di-
verse, including different hairstyles, accessories, and backgrounds. We vieW this lack of diversity in
WGAN-GP as undesirable given that the main requirement of a generative model is that it should
provide a variety of outputs.
Theorem 1 provides a clue as to What may be happening here. We knoW that minimizing the sample
Wasserstein loss Will find the Wrong minimum. In particular, When the target distribution has loW
entropy, the sample Wasserstein minimizer may actually be a deterministic distribution. But a good
generative model of images must lie in this “almost deterministic” regime, since the space of natural
images makes up but a fraction of all possible pixel combinations and hence there is little per-
pixel entropy. We hypothesize that the increased diversity in the Cramer GAN comes exactly from
learning these almost deterministic predictions.
More stable learning. In a second experiment, We varied the number of critic updates (Nu ) per
generator update. To compare performance betWeen the tWo architectures, We measured the loss
computed by an independent WGAN-GP critic trained on the validation set, folloWing a similar
evaluation previously done by Danihelka et al. (2017). Figure 4 shoWs the independent Wasserstein
critic distance betWeen each generator and the test set during the course of training. Echoing our
results With the toy experiment and ordinal regression, the plot shoWs that When a single critic update
is used, WGAN-GP performs particularly poorly. We note that additional critic updates also improve
Cramer GAN. This indicates that it is helpful to keep adapting the h(x) transformation.
6 Conclusion
There are many situations in Which the KL divergence, Which is commonly used as a loss function
in machine learning, is not suitable. The desirable alternatives, as We have explored, are the di-
vergences that are ideal and alloW for unbiased estimators: they alloW geometric information to be
incorporated into the optimization problem; because they are scale-sensitive and sum-invariant, they
possess the convergence properties We require for efficient learning; and the correctness of their
sample gradients means We can deploy them in large-scale optimization problems. Among open
questions, We mention deriving an unbiased estimator that minimizes the Wasserstein distance, and
variance analysis and reduction of the Cramer distance gradient estimate.
8
Under review as a conference paper at ICLR 2018
References
Martin Arjovsky, Soumith Chintala, andL6on Bottou. Wasserstein generative adversarial networks.
In Proceedings of the International Conference on Machine Learning, 2017.
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium
in generative adversarial nets (GANs). arXiv preprint arXiv:1703.00573, 2017.
Marc G. Bellemare, Will Dabney, and Remi Munos. A distributional perspective on reinforcement
learning. In Proceedings of the International Conference on Machine Learning, 2017.
Peter J. Bickel and David A. Freedman. Some asymptotic theory for the bootstrap. The Annals of
Statistics ,pp.1196-1217,1981.
Diane Bouchacourt, Pawan K Mudigonda, and Sebastian Nowozin. DISCO Nets: DISsimilarity
COefficients Networks. In Advances in Neural Information Processing Systems, pp. 352-360,
2016.
Kun-Jen Chung and Matthew J Sobel. Discounted MDP’s: Distribution functions and exponential
utility maximization. SIAM Journal on Control and Optimization, 25(1):49-62, 1987.
Thomas M. Cover and Joy A. Thomas. Elements of information theory. John Wiley & Sons, 1991.
Ivo Danihelka, Balaji Lakshminarayanan, Benigno Uria, Daan Wierstra, and Peter Dayan. Com-
parison of Maximum Likelihood and GAN-based training of Real NVPs. arXiv preprint
arXiv:1705.05263, 2017.
J6r6me Dedecker and Florence Merlevede. The empirical distribution function for dependent vari-
ables: asymptotic and nonasymptotic results in Lp. ESAIM: Probability and Statistics, 11:102-
114, 2007.
Richard M Dudley. Real analysis and probability, volume 74. Cambridge University Press, 2002.
Gintare Karolina Dziugaite, Daniel M Roy, and Zoubin Ghahramani. Training generative neural
networks via maximum mean discrepancy optimization. In Proceedings of the Conference on
Uncertainty in Artificial Intelligence, 2015.
Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven distributionally robust optimization us-
ing the Wasserstein metric: Performance guarantees and tractable reformulations. Mathematical
Programming, 2015.
Charlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya, and Tomaso A Poggio. Learn-
ing with a Wasserstein loss. In Advances in Neural Information Processing Systems, 2015.
Rui Gao and Anton J Kleywegt. Distributionally robust stochastic optimization with Wasserstein
distance. arXiv preprint arXiv:1604.02199, 2016.
Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and estimation.
Journal of the American Statistical Association, 102(477):359-378, 2007.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor-
mation Processing Systems, 2014.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scholkopf, and Alexander Smola.
A kernel two-sample test. Journal of Machine Learning Research, 13:723-773, 2012.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Im-
proved training of Wasserstein GANs. arXiv preprint arXiv:1704.00028, 2017.
Jose Miguel Herndndez-Lobato and Ryan P Adams. Probabilistic backpropagation for scalable
learning of Bayesian neural networks. In Proceedings of the International Conference on Machine
Learning, 2015.
9
Under review as a conference paper at ICLR 2018
Martin HeUseL HUbert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Gunter Klambauer, and
Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a Nash equilibrium.
arXiv preprint arXiv:1706.08500, 2017.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with
conditional adversarial networks. In Proceedings of the Conference on Computer Vision and
Pattern Recognition, 2016.
Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. Proceedings of the Inter-
national Conference on Learning Representations, 2014.
C.-L. Li, W.-C. Chang, Y. Cheng, Y. Yang, andB. P6czos. MMD GAN: Towards deeper understand-
ing of moment matching network. In Proceedings of the Neural Information Processing Systems,
2017.
Yujia Li, Kevin Swersky, and Rich Zemel. Generative moment matching networks. In Proceedings
of the International Conference on Machine Learning, 2015.
M. Lichman. UCI machine learning repository, 2013. URL http://archive.ics.uci.edu/
ml.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of International Conference on Computer Vision, 2015.
Gregoire Montavon, Klaus-Robert Muller, and Marco Cuturi. Wasserstein training of restricted
Boltzmann machines. In Advances in Neural Information Processing Systems, 2016.
Youssef Mroueh, Tom Sercu, and Vaibhava Goel. McGan: Mean and covariance feature matching
GAN. In Proceedings of the International Conference on Machine Learning, 2017.
Alfred Muller. Integral probability metrics and their generating classes of functions. Advances in
AppliedProbability, 29(2):429-443,1997.
Svetlozar T. Rachev, Lev Klebanov, Stoyan V. Stoyanov, and Frank Fabozzi. The methods of dis-
tances in the theory of probability and statistics. Springer, 2013.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Maria L Rizzo and Ggbor J Szekely. Energy distance. Wiley Interdisciplinary Reviews: Computa-
tional Statistics, 8(1):27-38, 2016.
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomed-
ical image segmentation. In International Conference on Medical Image Computing and
Computer-Assisted Intervention, 2015.
Yossi Rubner, Carlo Tomasi, and Leonidas J Guibas. The earth mover’s distance as a metric for
image retrieval. International journal of computer vision, 40(2):99-121, 2000.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training GANs. In Advances in Neural Information Processing Systems,
pp. 2234-2242, 2016.
Dino Sejdinovic, Bharath Sriperumbudur, Arthur Gretton, Kenji Fukumizu, et al. Equivalence of
distance-based and RKHS-based statistics in hypothesis testing. The Annals of Statistics, 41(5):
2263-2291, 2013.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethink-
ing the inception architecture for computer vision. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 2818-2826, 2016.
Gabor J. Szekely. E-statistics: The energy of statistical samples. Technical Report 02-16, Bowling
Green State University, Department of Mathematics and Statistics, 2002.
10
Under review as a conference paper at ICLR 2018
Gdbor J Szekely and Maria L Rizzo. Energy statistics: A class of statistics based on distances.
Journal of statistical planning and inference ,143(8):1249-1272, 2013.
Aaron Van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks.
In Proceedings of the International Conference on Machine Learning, 2016.
Mark Veraar. On Khintchine inequalities with a weight. Proceedings of the American Mathematical
Society, 138(11):4119-4121, 2010.
Vladimir M. Zolotarev. Metric distances in spaces of random variables and their distributions.
Sbornik: Mathematics, 30(3):373-401, 1976.
11
Under review as a conference paper at ICLR 2018
A Proofs
A. 1 Properties of a Divergence
Proof (Proposition 1 and 2). The statement regarding (U) for the KL divergence is well-known, and
forms the basis of most stochastic gradient algorithms for classification. Chung & Sobel (1987) have
shown that the total variation does not have property (S); by Pinsker’s inequality, it follows that the
same holds for the KL divergence. A proof of (I) and (S) for the Wasserstein metric is given by
Bickel & Freedman (1981), while the lack of (U) is shown in the proof of Theorem 1.	□
A.2 Biased estimate
Proof(Theoreml). Minimax bias: Consider P = B(θ*), a Bernoulli distribution of parameter
θ* and Qθ = B(θ) a Bernoulli of parameter θ. The empirical distribution Pm is a Bernoulli with
parameter θ := mm Pm=I Xi. Note that with P and Qθ both Bernoulli distributions, the Pth powers
of the p-Wasserstein metrics are equal, i.e. w1(P, Qθ) = wpp(P, Qθ). This gives us an easy way to
prove the stronger result that all p-Wasserstein metrics have biased sample gradients. The gradient
of the loss Wp(P, Qθ ) is, for θ = θ*,
g := Vwp(P, Qθ) = v[∣θ* - θ∣i = sgn(θ - θ*),
ʌ
and similarly, the gradient of the sample loss is, for θ 6= θ,
g ：= Vwp(Pm, Qθ) = v[∣θ - θ∣i = sgn(θ - θ).
Notice that this estimate is biased for any m ≥ 1 since
ʌ
Eg = 2Pr{θ <θ}- 1,	(5)
which is different from g for any θ* ∈ (0,1). In particular for m = 1, EP g = 1 一 2θ* does
not depend on θ, thus a gradient descent using a one-sample gradient estimate has no chance of
minimizing the Wasserstein loss as it will converge to either 1 or 0 instead of θ*.
Now observe that for m ≥ 2, and any θ > m-1,
ʌ
Pr{θ < θ} = Pr{∃i s.t. Xi = 0} = 1 — (θ*)m,
and therefore
E g= 1 - 2(θ*)m.
Taking θ* = m-1, we find that
g - Eg = 1 - [1 - 2(θ*)m] = 2(1 - 口 ≥ 2e-2.
m
Thus for any m, there exists P = B(θ*) and Qθ = B(θ) with θ* = m-1 < θ < 1 such that the bias
g 一 E g is lower-bounded by a numerical constant. Thus the minimax bias does not vanish with the
number of samples m.
Notice that a similar argument holds for θ * and θ being close to 0. In both situations where θ* is
close to 0 or 1, the bias is non vanishing when ∣θ* 一 θ∣ is of order 煮.However this is even worse
when θ* is away from the boundaries. For example chosing θ* = 2, we can prove that the bias is
non vanishing even when ∣θ* 一 θ∣ is (only) of order √=.
Indeed, using the anti-concentration result of Veraar (2010) (Proposition 2), we have that for a
sequence Y1, . . . , Ym of Rademacher random variables (i.e. +/ - 1 with equal probability),
m
Pr (- X Yi ≥ e) ≥ (1 - me2)2∕3.
n i=1
This means that for samples Xi,..., Xm drawn from a Bernoulli B(θ* = 1) (i.e.,匕=2Xi - 1
are Rademacher), we have
Pr (θ ≥ θ* + 〃2)≥ (1 - me2)2∕3,
12
Under review as a conference paper at ICLR 2018
'DUU2∙-P∙≡'Dti-SSSDM
Figure 5: Wasserstein loss (black curve) θ → ∣θ* - θ∣ versus expected sample Wasserstein loss (red
curve) θ → E[∣θ - θ∣], for different values of m and θ* and P = 1. Left: m = 1, θ* = 0.6. A
stochastic gradient using a one-sample Wasserstein gradient estimate will converge to 1 instead of
θ*. Middle: m = 6, θ* =0.6. The minimum of the expected sample Wasserstein loss is the median
of θ which is here θ = 2 = θ* = 0.6. Right: m = 5, P = 0.9. The minimum of the expected
sample Wasserstein is θ = 1 and not θ* = 0.9.
thus for 1/2 = θ* <θ<θ* + 1/ √8m We have the following lower bound on the bias:
g - E^ = 2Pr (θ ≥ θ) ≥ 1/6.
Thus the bias is lower-bounded by a constant (independent of m) when θ* = 2 and ∣θ* - θ∣ =
O(1∕√m).
Wrong minimum: From (5), we deduce that a stochastic gradient descent algorithm based on the
sample Wasserstein gradient will converge to a θ such that Pr{θ < θ} = 1, i.e., θ is the median of
ʌ ʌ
the distribution over θ, whereas θ* is the mean of that distribution. Since θ follows a (normalized)
binomial distribution with parameters m and θ*, we know that the median θ and the mean θ* do not
necessarily coincide, and can actually be as far as 2m -away from each other. For example for any
odd m and any θ* ∈ 2，, 2 - 2m)the median is θ* - 2m.
It follows that the minimum of the expected sample Wasserstein loss (the fixed point of the stochastic
gradient descent using the sample Wasserstein gradient) is different from the minimum of the true
Wasserstein loss:
ʌ
argθminE[wp(Pm Qθ)] = argθmin[wp(P, Qθ)].
This is illustrated in Figure 5.
Notice that the fact that the minima of these losses differ is worrisome as it means that minimizing
the sample Wasserstein loss using (finite) samples will not converge to the correct solution.
Deterministic solutions: Consider the specific case where (1/2)1/n < θ* < 1 (illustrated in the
right plot of Figure 5). Then the expected sample gradient V E[wp(Pm,Qθ*)] = Eg = 1-2(θ*)n <
0 for any θ, so a gradient descent algorithm will converge to 1 instead of θ*. Notice that a symmetric
argument applies for θ* close to 0.
In this simple example, minimizing the sample Wasserstein loss may lead to degenerate solutions
(i.e., deterministic) when our target distributions have low (but not zero) entropy.	□
A.3 CONSISTENCY OF THE SAMPLE 1 -WASSERSTEIN GRADIENT
We provide an additional result here showing that the sample 1-Wasserstein gradient converges to
the true gradient as m → ∞.
Theorem 3. Let P and Qθ be probability distributions, with Qθ parametrized by θ. Assume that
the set x ∈ X, such that FP (x) = FQθ (x) has measure zero, and that for any x ∈ X, the map
θ → Fq/(x) is differentiable in a neighborhood V(θ) of θ with a uniformly bounded derivative
13
Under review as a conference paper at ICLR 2018
(for θ ∈ V (θ) and X ∈ X). Let Pm =+ P m=1 δχi be the empirical distribution derived from m
independent samples X1 , . . . , Xm drawn from P . Then
lim Vwι(JPm, Qθ) = Vwι(P, Qθ), almost surely.
m→∞
We note that the measure requirement is strictly to keep the proof simple, and does not subtract from
the generality of the result.
Proof. Let V := Vθ. Since p = 1 the Wasserstein distance w1(P, Q) measures the area between the
curves defined by the distribution function of P and Q, thus w1 (P, Q) = l1 (P, Q) = FP (x) -
FQ(x)dx and
Vwι(P,Qθ) = lim WI(P,qθ+δ)-WI(P,Qθ)
∆→0	∆
=∆im0 / ~∆ (IFP(X)- fQθ+δ (X)IFP(X)- fQθ (X)I) dx.
Now since we have assumed that for any X ∈ X, the map θ 7→ FQθ (X) is differentiable in a
neighborhood V(θ) of θ and its derivative is uniformly (over V(θ) and X) bounded by M, we have
∆]IfP(X)- FQθ+∆ (X)I-IFP(X)- FQθ (x)I I ≤	∆IFQθ+∆ (X)- FQθ (X)I ≤ M.
Thus the dominated convergence theorem applies and
VWI(P,QΘ) = / ∆im0 ~∆ (IFP(X)-FQθ+∆ (X)ITFP(X)-FQθ(X)I)dX
=	VIIFP (X) - FQθ (X)IIdX
=Jsgn(FP (x) - FQθ (X))VFQθ (X)dX,
since we have assumed that the set of X ∈ X such that FP (X) = FQθ (X) has measure zero.
ʌ
Now, using the same argument for W1(Pm, Qθ) we deduce that
NWI(Pm,Qθ) = l lim ɪ
∆→0 ∆
J '_____
(IFPm (X)- FQθ+∆ (X)ITFPm (X)- FQθ (X)I) dX.
{z^^
A(x)
}
Let us decompose this integral over X as the sum of two integrals, one over X \ Ωm, and the other
one over Ωm, where Ωm = {x ∈ X, FP (x) = Fq§ (x)}. We have
A(X)dX =
J X'∖Ωm	J X'∖Ωm
sgn(FPm(x) - FQθ (X))VFQθ (X)dχ,
and
A(X)dX
≤ ∆ ∆im0 4 (ifQθ+δ (x) - FQθ (X) I)dx
J Qm
≤ M∣Ωm∣.
Now from the strong law of large numbers, we have that for any X, the empirical cumulative distri-
bution function FP (x) converges to the cumulative distribution FP (x) almost surely. We deduce
that Ωm converges to the set {χ, FP(x) = FQ§ (x)} which has measure zero, thus ∣Ωm, | → 0 and
lim VWI(Pm,Qθ) = lim
m→∞	m→∞
/ sgn(FPm (x) - FQθ (X))VFQθ(X)dχ.
Xm
14
Under review as a conference paper at ICLR 2018
Now, since ∣VFqθ (x) | ≤ M, we can use once more the dominated convergence theorem to deduce
that
lim NWI(Pm, Qθ)= / lim Sgn(FP (x) - Fq@(X))VFq@(x)dx
m→∞	X m→∞	m
=I Sgn(FP (X)- FQθ (X))VFQθ (X) dx
X
= Vw1(P, Qθ).
□
The following lemma will be useful in proving that the Cramer distance has property (U).
Lemma 1. Let Xm := Xi,..., Xm be independent samples from P, and let Pm := mm Pi δχi.
Then
E	F^ (x) = FP(x).
Xm 〜P Pm
Proof. Because the Xi ’s are independent,
FPm (X)= Z	Pm(S = mm X I [Xi ≤ x].
-∞	i=1
Now, taking the expectation w.r.t. Xm,
m
E FP (x) = E 工 XI [Xi ≤ x]
Xm 〜P Pm	Xm 〜PmJ 一
i=1
m
=m∙X XiEPI [Xi ≤ χ]
i=1
m
=m X Pr{Xi ≤ x}
i=1
= FP (X),
since the Xi are identically distributed according to P.
□
Proof (Theorem 2). Like the Wasserstein metrics, the lp metrics have dual forms as integral proba-
bility metrics (see Dedecker & Merlevede, 2007, for a proof):
lp(P, Q) = sup	E f(X) - E f(X),
f ∈Fq X〜P	X 〜Q
(6)
where Fq := {f : f is absolutely continuous, ^ d∣ ∣∣ ≤ 1} and q is the conjugate exponent of p, i.e.
p-1 + q-1 = 1.3 We will use this dual form below.
We will prove that lp has properties (I) and (S) forp ∈ [1, ∞); the case p = ∞ follows by a similar
argument. Begin by observing that
FcX (X) = Pr{cX ≤ X}
=Pr nx ≤ C o
=FX (C).
Then we may rewrite lpp(CX, CY ) as
lp(cX，cY )=∕∞pχ(C)- Fγ(C )「dX
(=a) C Z∞ FX (z) -FY(z)pdz,
-∞
3This relationship is the reason for the notation F∞ in the definition the dual of the 1-Wasserstein (2).
15
Under review as a conference paper at ICLR 2018
where (a) uses a change of variables z = x/c. Taking both sides to the power 1/p proves that the lp
metric possesses property (S) of order 1/p. For (I), we use the IPM formulation (6):
lp(A+X,A+Y) = sup E f(x)- E f(y)
f∈Fq A+X	A+Y
(=a) sup EA EX f (x + a) - EA EY f (y + a)
f∈Fq
= sup EA EX f(x + a) - EY f(y + a)
f∈Fq
(b)
≤ EA sup EX f(x + a) - EY f(y + a),
f∈Fq
where (a) is by independence of A and X, Y , and (b) is by Jensen’s inequality. Next, recall that
Fq is the set of absolutely continuous functions whose derivative has bounded Lq norm. Hence if
f ∈ Fq, then also for all a the translate ga(x) := f(x + a) is also in Fq. Therefore,
lp(A+X,A+Y) ≤EA sup EXf(x+a)-EY f(y+a)
f∈Fq
= EA sup EX g(x) - EY g(y)
g∈Fq
= sup EX g(x) - EY g(y)
g∈Fq
= lp(X, Y).
□
Now, to prove (U). Here we make use of the introductory requirement that “all expectations under
consideration are finite.” Specifically, We require that the mean under P, Ex〜P [x], is well-defined
and finite, and similarly for Qθ . In this case,
E [X] = Z (1 - FP (X))dX - Z	FP (X)dX.
X〜P	0o	J-∞
(7)
This mild requirement guarantees that the tails of the distribution function FP are light enough to
avoid infinite Cramer distances and expected gradients (a similar condition was set by Dedecker &
Merlevede (2007)). Now, by definition,
Vθl2(P,Qθ) = Vθ Γ (Fqθ(X)- FP(x))2 dx
-∞
(=a)	∞ Vθ(FQθ(x) - FP (x))2 dx
=	2 (FQθ (x) - FP (x)) VθFQθ (x)dx
-∞
=)/	2(fQθ (X)- EXm FPm (X)) vθFQθ(X)dx
=/	2 EXm(FQθ (X)- FPm (X)) vθFQθ (x)dx
=EXm /	2 (FQθ(X)- FPm (X)) vθFQθ (X)dX
=EXm Vθ l2(Pm,Qθ ),
where (a) follows from the hypothesis (7) (the convergence of the squares follows from the conver-
gence of the ordinary values), (b) follows from Lemma 1 and (c) follows from Fubini’s theorem,
again invoking (7).
Finally, we prove that of all the lp distances (1 ≤ P ≤ ∞) only the Cramer distance, l2, has the
(U) property.
16
Under review as a conference paper at ICLR 2018
Without loss of generality, let Us suppose P is not a Dirac, and further suppose that for any Xm 〜P,
FQθ (x) ≥ Fp (x) everywhere. For example, when Qθ has bounded support we can take P to be a
sufficiently translated version of Qθ, such that the two distributions’ supports do not overlap.
We have already established that the 1-Wasserstein does not have the (U) property, and is equivalent
to lpp for p = 1. We will thus assume that p > 1, and also that p < ∞, the latter being recovered
through standard limit arguments. Begin with the gradient for lpp(P, Qθ),
Z∞
FQθ(x) - FP(x)pdx
∞
=) P I	(FQθ (X)- FP (X))PT VθFQθ (χ)dχ
-∞
= p	φp(FQθ (X) - FP(X))VθFQθ (X)dX
-∞
=P / φp( EXm (FQθ(X)- FPm (X)))VθFQθ(X)dx,
-∞
for φp(z) = zp-1; in (a) we used the same argument as in Theorem 3.
Now, φp is convex on [0, ∞) when P ≥ 2 and concave on the same interval when 1 < P < 2.
From Jensen’s inequality we know that for a convex (concave) function φ and a random variable Z,
E φ(Z) is greater than (less than) or equal to φ(E Z), with equality if and only if φ is linear or Z
is deterministic. By our first assumption we have ruled out the latter. By our second assumption
Fqθ (x) ≥ FPm (x), We can apply Jensen,s inequality at every X to deduce that
EXm	[Vθip(Pm, Qθ)]	<	Vθip (P,	Qθ),	if 1 < p <	2,
Exm	[Vθip(Pm, Qθ)i	>	Vθip(P,	Qθ),	if P > 2,
Exm	[VθIP(Pm, Qθ)i	=	Vθip(P,	Qθ),	if P = 2.
We conclude that of the lp distances, only the Cramer distance has unbiased sample gradients. □
Proposition 3. The energy distance E(P, Q) has properties (I), (S), and (U).
Proof. As before, write E(X, Y ) := E(P, Q). Recall that
E(X,Y)=2EkX-Yk2-EkX-X0k2-EkY -Y0k2.
Consider a random variable A independent of X and Y . First, we want to prove property (I):
E(A+X,A+Y) ≤ E(X,Y).
We will use Proposition 2 from Szekely & Rizzo (2013) to express the energy distance in terms of
characteristic functions φX, φY of d-dimensional random variables X and Y :
E(X,Y)」Z Jφx¾+Y≡dt
cd Rd	|t|d+1
where
∏(d+1)∕2
Γ(d+1).
17
Under review as a conference paper at ICLR 2018
The proof then uses properties of characteristic functions (∣φa(t) | ≤ 1 and Φa+x (t) = φa(t)φχ (t)
for independent variables A and X) to show:
E (A + X, A + Y)=—[
cd Rd
J Z
cd Rd
=LZ
cd Rd
≤ ɪ Z
cd Rd
lφA+X (t) - φA+Y (t)|2
∣t∣d+1
dt
∣φA(t)Φx (t) - φA(t)φγ (t)|2
∣t∣d+1
lφx『(t)|2 IΦA(t)l2dt
|t|
lφX (t) - φY (t)|2
∣t∣d+1
dt
E(X,Y).
This proves (I). Next, consider a real value c > 0. We have
E(cX, cY) = 2EkcX -cYk2 -EkcX - cX0k2 -EkcY-cY0k2
=2cEkX-Yk2-cEkX-X0k2-cEkY-Y0k2
= cE(X, Y).
This proves (S). Finally, suppose that Y is distributed according to Qθ parametrized by θ. Let
Xm = Xi,..., Xm be drawn from P, and let PmL := 煮 Pm=I δχ%. Let X be the random variable
ʌ ʌ ʌ
distributed according to Pm , and X0 an independent copy of X . Then
E (Pm,Qθ ) = E (X ,Y ) = 2 EIlX - Y∣∣2 - EIlX - X 0II2 - E∣∣Y - Y 0II2.
The gradient of the true loss w.r.t. θ is
VθE(X,Y) = 2VθEIIx - YII2 -VθEIIy- Y0II2.	⑻
Now, taking the gradient of the sample loss w.r.t. θ,
VθE(X,Y) = 2VθEIIX - YII2 -VθEIIy- Y0II2.	⑼
Since the second terms of the gradients match, all we need to show is that the first terms are equal,
in expectation. Assuming that Vθ and the expectation over Xm commute, we write
F vθeIIx-YII2 = 口事 EUX-YII2
Xm	Xm
= Vθ E E E IIx - Y II2 ,
Xm X 〜Pm
by independence of X and Y. But now we know that the expected empirical distribution is P, that
is
F 号 EIIx-YII2 = xepEIIx-YII2 = EIIX-γII2.
Xm X〜Pm	χ~p
It follows that the first terms of (8) and (9) are also equal, in expectation w.r.t. Xm . Hence we
conclude that the energy distance has property (U), that is
ʌ
E Vθ E (Pm,Qθ ) = Vθ E (P,Qθ).
Xm〜P
□
B Comparison with the Wasserstein Distance
Figure 2 (left) provides learning curves for the toy experiment described in Section 4.2.
18
Under review as a conference paper at ICLR 2018
φuu2∙-α U-史S-SSSDM
Figure 6: WaSSerStein while training to minimize different loss functions (Wasserstein, KL, Cramer).
Averaged over 10 random initializations. Error-bands indicate one standard deviation. Note the
different y-axes.
Figure 7: Ordinal regression on the year prediction MSD dataset. Each loss function trained with
various minibatch sizes. Training progress shown in terms of: Left. RMSE, Middle. Wasserstein
distance, Right. Negative log-likelihood.
B.1	Ordinal Regression
We compare the different losses on an ordinal regression task using the Year Prediction MSD dataset
from (Lichman, 2013). The task is to predict the year ofa song (taking on values from 1922 to 2011),
from 90-dimensional feature representation of the song.4 Previous work has used this dataset for
benchmarking regression performance (Hernandez-Lobato & Adams, 2015), treating the target as a
continuous value. Following Hernandez-Lobato & Adams (2015), We train a network with a single
hidden layer with 100 units and ReLU non-linearity, using SGD with 40 passes through the training
data, using the standard train-test split for this dataset (Lichman, 2013). Unlike (Hernandez-Lobato
& Adams, 2015), the network outputs a probability distribution over the years (90 possible years
from 1922-2011).
We train models using either the 1-Wasserstein loss, the Cramer loss, or the KL loss, the latter of
which reduces the ordinal regression problem to a classification problem. In all cases, we compare
performance for three different minibatch sizes, i.e. the number of input-target pairs per gradient
step. Note that the minibatch size only affects the gradient estimation, but has otherwise no direct
relation to the number of samples m previously discussed, since each sample corresponds to a dif-
ferent input vector. We report results as a function of number of passes over the training data so that
our results are comparable with previous work, but note that smaller batch sizes get more updates.
The results are shown in Figure 2. Training using the Cramer loss results in the lowest root mean
squared error (RMSE) and the final RMSE value of 8.89 is comparable to regression (Hernandez-
Lobato & Adams, 2015) which directly optimizes for MSE. We further observe that minimizing
the Wasserstein loss trains relatively slowly and leads to significantly higher KL loss. Interestingly,
larger minibatch sizes do seem to improve the performance of the Wasserstein-based method some-
what, suggesting that there might be some beneficial bias reduction from combining similar inputs.
By contrast, using with the Cramer loss trains significantly faster and is more robust to choice of
minibatch size.
4We refer to https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD for the
details.
19
Under review as a conference paper at ICLR 2018
Figure 8: Left, middle. Sample Wasserstein and cross-entropy loss curves on the CelebA validation
data set. Right. Test loss at the end of training, in function of loss minimized (see text for details).
Figure 9: Generated right halves for WGAN-GP (left) and Cramer GAN (right) for left halves from
the validation set of Downsampled ImageNet 64x64 (Van den Oord et al., 2016). The low diversity
in WGAN-GP samples is consistent with the observations of Isola et al. (2016): “the generator
simply learned to ignore the noise.”
B.2	Image Modelling with PixelCNN
As additional supporting material, we provide here the results of experiments on learning a proba-
bilistic generative model on images using either the 1-Wasserstein, Cramer, or KL loss. We trained
a PixelCNN model (Van den Oord et al., 2016) on the CelebA 32x32 dataset (Liu et al., 2015),
which is constituted of 202,599 images of celebrity faces. At a high level, probabilistic image mod-
elling involves defining a joint probability Qθ over the space of images. PixelCNN forms this joint
probability autoregressively, by predicting each pixel using a histogram distribution conditional on
a probability-respecting subset of its neighbours. This kind of modelling task is a perfect setting
to study Wasserstein-type losses, as there is a natural ordering on pixel intensities. This is also a
setting in which full distributions are almost never available, because each prediction is conditioned
on very different context; and hence we require a loss that can be optimized from single samples.
Here the true losses are not available. Instead we report the sample Wasserstein loss, which is an
upper bounds on the true loss Bellemare et al. (proof is provided by 2017). For the KL divergence
we report the cross-entropy loss, as is typically done; the KL divergence itself corresponds to the
expected cross-entropy loss minus the real distribution’s (unknown) entropy.
Figure 8 shows, as in the toy example, that minimizing the Wasserstein distance by means of stochas-
tic gradient fails. The Cramer distance, on the other hand, is as easily minimized as the KL and in
fact achieves lower Wasserstein and Cramer loss. We note that the resulting KL loss is higher than
when directly minimizing the KL, reflecting the very real trade-off of using one loss over another.
We conclude that in the context of learning an autoregressive image model, the Cramer should be
preferred to the Wasserstein metric.
20
Under review as a conference paper at ICLR 2018
C Cramer GAN
C.1 Loss Function Details
Our critic has a special form:
f(x) =	0E	kh(x)	-	h(Y0)k2	-	0E	kh(x) -	h(X0)k2
Y〜Q	X〜P
where Q is the generator and P is the target distribution. The critic has trainable parameters only
inside the deep network used for the transformation h. From (4), we define the generator loss to be
Lg(X,Y)= E [f(X)]- E [f (Y)],
X〜P	Y〜Q
(10)
as in Wasserstein GAN, except that no maxf operator is present and we can obtain unbiased sample
gradients. At the same time, to provide helpful gradients for the generator, we train the transforma-
tion h to maximize the generator loss. Concretely, the critic seeks to maximize the generator loss
while minimizing a gradient penalty:
Lcritic (X, Y) =-Lg(X,Y)+λGP
(11)
where GP is the gradient penalty from the original WGAN-GP algorithm (Gulrajani et al., 2017)
(the penalty is given in Algorithm 1). The gradient penalty bounds the critic’s outputs without using
a saturating function. We chose λ = 10 from a short parameter sweep. Our training is otherwise
similar to the improved training of Wasserstein GAN (Gulrajani et al., 2017).
In the next two sections, we describe how to practically compute gradients of these losses with
respect to the generator and transformation parameters, respectively.
C.2 Gradient Estimates for the Generator
Recall that the energy distance is:
E(X,Y) = 2XEPkX - Yk2 - XEP kX - X0k2 - YIEQ kY - Y0k2
Y 〜Q	X0eP	Y 0EQ
If Y is generated from the standard normal noise Z 〜N(0,1) by a differentiable generator Y =
G(Z) and the generator has an integrable gradient, we can use the reparametrization trick (Kingma
& Welling, 2014) to compute the gradient with respect to the generator parameters:
ReG E (X,Y)
2E
XP
Z EN (0,1)
RθG kX - G(Z)k2 - E	RθG kG(Z) - G(Z)0k2 .
Z EN (0,1)
Z0EN(0,1)
We see that we only need one real sample X to estimate the gradient, because the kX - X0 k term
does not depend on the generator parameters. This allows us to define a generator loss usable for
situations with only one real sample (e.g., for conditional modeling):
ʌ .. ...	.. ...........
Lg(X,Y) = 2 YEfJh(X) - h(Y)k2- VEC kh(Y) - h(Y0)k2
X EP	Y EQ
Y EQ	Y0EQ
(12)
C.3 Gradient Estimates for the Transformation
As shown in the previous section, we can obtain an unbiased gradient estimate of the generator loss
(12) from three samples: two from the generator, and one from the target distribution. However,
to estimate the gradient of the Cramer GAN loss with respect to the transformation parameters We
need four independent samples: two from the generator and two from the target distribution. In
many circumstances, for example when learning conditional densities, we do not have access to two
independent target samples. We will instead define a surrogate objective for the critic. The surrogate
critic will have the following form:
fs(x) = 0E kh(x) - h(Y0)k2 - kh(x)k2
Y0EQ
21
Under review as a conference paper at ICLR 2018

仔 annas
Figure 10: Left. Generated images from a generator trained to minimize the energy distance of raw
images, E(X, Y). Right. Generated images if minimizing the Cramer GAN loss, E(h(X), h(Y)).
Both generators had the same DCGAN architecture (Radford et al., 2015).
which we use to define a surrogate loss Ls (X, Y) similar to (10):
Ls(X,Y)= E [fs(X)]- E [fs (Y)]
X〜P	Y〜Q
= E kh(X)-h(Y0)k2- E kh(X)k2
X〜P	X〜P
Y 0〜Q
—
E kh(Y)-h(Y0)k2+ E kh(Y)k2
Y〜Q	Y〜Q
Y 0〜Q
(13)
The surrogate loss emulates an integral probability metric (IPM) (Muller, 1997) and can be used to
train the critic. The maximization of this loss will force E kh(X) - h(Y0)k2 andE kh(Y) - h(Y0)k2
to be informative about the underlying distributions.
The generator can be then trained to minimize the energy distance Lg (12) of the transformed vari-
ables. Itis also possible to obtain training more similar to Wasserstein GAN by training the generator
to minimize the surrogate loss (13). We recommend trying both possibilities, because they were both
stable and produced diverse conditional samples. The whole training procedure is summarized as
Algorithm 1.
Finally, when estimating the losses in Algorithm 1, we use two independent samples xg, x0g from the
generator. However, in constructing the surrogate loss L§, an asymmetry arises. We reduce variance
by averaging the two losses Ls(xg, x0g) and Ls(x0g, xg).
C.4 Generator Architecture
The generator architecture is the U-Net (Ronneberger et al., 2015) previously used for Image-to-
Image translation (Isola et al., 2016). We used no batch normalization and no dropout in the genera-
tor and in the critic. The network conditioned on the left half of the image and on extra 12 channels
with Gaussian noise. We generated two independent samples for a given image to compute the
Cramer GAN loss. To be computationally fair to WGAN-GP, we trained WGAN-GP with twice the
minibatch size (i.e., the Cramer GAN minibatch size was 64, while the WGAN-GP minibatch size
was 128).
C.5 Critic Architecture
Our h(x) transformation is a deep network with 256 outputs (more is better). The network has the
traditional deep convolutional architecture (Radford et al., 2015). We do not use batch normaliza-
tion, as it would conflict with the gradient penalty.
C.6 Performance Evaluation
We report the Inception score (Salimans et al., 2016) and the Frechet Inception Distance (FID)
(Heusel et al., 2017) in Figure 11 (left), which are commonly used measures of evaluation for GANs.
22
Under review as a conference paper at ICLR 2018
Model	Inception	FID
Training set	HT	0.0
WGAN-GP	6.5	36.4
Cramer GAN	6.7	33.6
Surrogate GAN	6.6	34.1
Figure 11: Left. Inception score and FID on CIFAR-10. The Surrogate GAN is a Cramer GAN
with the generator trained to minimize the surrogate loss (13). Right. Inception Energy Distance
on conditional CIFAR-10. The network conditioned on the left half of the CIFAR-10 images. The
shaded area denotes the standard deviation from 3 runs.
These evaluation measures have the disadvantage that they are not able to detecting overfitting and
account for diversity in generated conditional samples. For example, a mixture model that overfits
to the training set would get a better Inception score and FID than the trained GANs.
We propose a new evaluation for conditional GANs that uses data from the validation set and that is
able to detect overfitting. Our Inception Energy Distance (IED) measures a difference, similar to the
genererator loss (12), between features of completed image and features of the corresponding real
image. An unbiased estimator of the IED is:
IED = kin(xr) - in(xg)k2 + in(xr) - in(x0g)2 - in(xg) - in (x0g)2	(14)
where xr is a real sample and xg , x0g are two independent generated samples. in(x) are the features
for image x, and is the is the output of the pretrained Inception network5 (Szegedy et al., 2016),
specifically the output layer pool_3:0 with 2048 features. The pretrained Inception network al-
lows to objectively compare different GANs. Our performance measure is similar to the FID, but
can be computed with one real sample and monitored online.
We use the Inception Energy Distance only to detect underfitting and overfitting. Figure 11 (right)
shows that WGAN-GP is not minimizing IED on the training set. WGAN-GP produces very deter-
ministic completions and this is detected by the in(xg) - in(x0g)2 term in the IED. We also see
that the Cramer GAN is overfitting the training set. The Cramer GAN is progressively learning the
distribution of the training set and obtains a worse IED on the validation set. This suggests that our
optimization is able to successfully train the generator, and that with more data and regularization
methods, we will be able to overcome this overfitting. For example, future work can train on large
video datasets and try to minimize the IED directly.
5http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz
23