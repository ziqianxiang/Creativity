Parametric Information B ottleneck to
Optimize Stochastic Neural Networks
Thanh T. Nguyen & Jaesik Choi
Department of Computer Engineering
Ulsan National Institute of Science and Technology
50 UNIST, Ulsan 44919, Republic of Korea
{thanhnt,jaesik}@unist.ac.kr
Ab stract
In this paper, we present a layer-wise learning of stochastic neural networks
(SNNs) in an information-theoretic perspective. In each layer of an SNN, the
compression and the relevance are defined to quantify the amount of information
that the layer contains about the input space and the target space, respectively. We
jointly optimize the compression and the relevance of all parameters in an SNN
to better exploit the neural network’s representation. Previously, the Information
Bottleneck (IB) framework (Tishby et al. (1999)) extracts relevant information for
a target variable. Here, we propose Parametric Information Bottleneck (PIB) for a
neural network by utilizing (only) its model parameters explicitly to approximate
the compression and the relevance. We show that, the PIB framework can be con-
sidered as an extension of the maximum likelihood estimate (MLE) principle to
every layer level. We also show that, as compared to the MLE principle, PIB : (i)
improves the generalization of neural networks in classification tasks, (ii) is more
efficient to exploit a neural network’s representation by pushing it closer to the
optimal information-theoretical representation in a faster manner.
1	Introduction
Deep neural networks (DNNs) have demonstrated competitive performance in several learning tasks
including image recognition (e.g., Krizhevsky et al. (2012), Szegedy et al. (2015)), natural language
translation (e.g., Cho et al. (2014), Bahdanau et al. (2014)) and game playing (e.g., Silver et al.
(2016)). Specifically in supervised learning contexts, a common practice to achieve good perfor-
mance is to train DNNs with the maximum likelihood estimate (MLE) principle along with various
techniques such as data-specific design of network architecture (e.g., convolutional neural network
architecture), regularizations (e.g., early stopping, weight decay, dropout (Srivastava et al. (2014)),
and batch normalization (Ioffe & Szegedy (2015))), and optimizations (e.g., Kingma & Ba (2014)).
The learning principle in DNNs has therefore attributed to the MLE principle as a standard one for
guiding the learning toward a beneficial direction. However, the MLE principle is very generic that
is not specially tailored for neural networks. Thus, a reasonable question is does the MLE principle
effectively and sufficiently exploit a neural network’s representative power and is there any better
alternative? As an attempt to address this important question, this work investigates the learning of
DNNs from the information-theoretic perspective.
An alternative principle is the Information Bottleneck (IB) framework (Tishby et al. (1999)) which
extracts relevant information in an input variable X about a target variable Y . More specifically,
the IB framework constructs a bottleneck variable Z = Z(X) that is compressed version of X
but preserves as much relevant information in X about Y as possible. In this information-theoretic
perspective, I(Z, X) 1, the mutual information of Z and X, captures the compression of Z about X
and I(Z, Y ) represents the relevance of Z to Y . The optimal representation Z is determined via the
minimization of the following Lagrangian:
LIB[p(z|x)] =I(Z,X)-βI(Z,Y)
(1)
1In this work, we use comma rather than semicolon to separate variables inside the mutual information
operator I(., .).
1
where β is the positive Lagrangian multiplier that controls the trade-off between the complexity
of the representation, I(Z, X), and the amount of relevant information in Z, I(Z, Y ). The exact
solution to the minimization problem above is found (Tishby et al. (1999)) with the implicit self-
consistent equations:
p(z |x)
p(z)
p(y|z)
P(Z)
Z (x;β)
exp(-βDKL [p(y|x)kp(y|z)])
p(z|x)p(x)dx
p(y|x)p(x|z)dx
(2)
where Z(x; β) is the normalization function, and DKL[.k.] is the Kullback - Leibler (KL) divergence
(Kullback & Leibler (1951)). Unfortunately, the self-consistent equations are highly non-linear
and still non-analytic for most practical cases of interest. Furthermore, the general IB framework
assumes that the joint distribution p(X, Y ) is known and does not specify concrete models.
On the other hand, the goal of the MLE principle is to match the model distribution pmodel as close
to the empirical data distribution PD as possible (e.g., see Appendix I.B). The MLE principle treats
the neural network model p(x; θ) as a whole without explicitly considering the contribution of its
internal structures (e.g., hidden layers and hidden neurons). As a result, a neural network with
redundant information in hidden layers may have a good distribution match in a training set but
show a poor generalization in test sets. In the MLE principle, we only need empirical samples of
the joint distribution to maximize the likelihood function of the model given the data. The MLE
principle is proved to be mathematically equivalent to the IB principle for the multinomial mixture
model for clustering problem when the input distribution X is uniform or has a large sample size
(Slonim & Weiss (2002)). However in general the two principles are not obviously related.
In this work, we leverage neural networks and the IB principle by viewing neural networks as a set
of encoders that sequentially modify the original data space. We then propose a new generalized
IB-based objective that takes into account the compression and relevance of all layers in the network
as an explicit goal for guiding the encodings in a beneficial manner. Since the objective is designed
to optimize all parameters of neural networks and is mainly motivated by the IB principle for deep
learning (Tishby & Zaslavsky (2015)), we name this method the Parametric Information Bottleneck
(PIB). Because the generalized IB objective in PIB is intractable, we approximate it using variational
methods and Monte Carlo estimation. We propose re-using the existing neural network architecture
as variational decoders for each hidden layers. The approximate generalized IB objective in turn
presents interesting connections with the MLE principle. We show that our PIBs have a better
generalization and better exploit the neural network’s representation by pushing it closer to the
information-theoretical optimal representation as compared to the MLE principle.
2	Related Work
Originally, the general IB framework is proposed in Tishby et al. (1999). The framework provides
a principled way of extracting the relevant information in one variable X about another variable Y .
The authors represent the exact solution to the IB problem in highly-nonlinear self-consistent equa-
tions and propose the iterative Blahut Arimoto algorithm to optimize the objective. However, the
algorithm is not applicable to neural networks. In practice, the IB problem can be solved efficiently
in the following two cases only: (1) X, Y and Z are all discrete (Tishby et al. (1999)); or (2) X, Y
and Z are mutually joint Gaussian (Chechik et al. (2005)) where Z is a bottleneck variable.
Recently, the IB principle has been applied to DNNs (Tishby & Zaslavsky (2015)). This work
proposes using mutual information of a hidden layer with the input layer and the output layer to
quantify the performance of DNNs. By analyzing these measures with the IB principle, the authors
establish an information-theoretic learning principle for DNNs. In theory, one can optimize the
neural network by pushing up the network and all its hidden layers to the IB optimal limit in a layer-
wise manner. Although the analysis offers a new perspective about optimality in neural networks,
it proposes general analysis of optimality rather than a practical optimization criteria. Furthermore,
estimating mutual information between the variables transformed by network layers and the data
variables poses several computational challenges in practice that the authors did not address in the
work. A small change in a multi-layered neural network could greatly modify the entropy of the
input variables. Thus, it is hard to analytically capture such modifications.
2
The recent work Alemi et al. (2016) also uses variational methods to approximate the mutual infor-
mation as an attempt to apply the IB principle to neural networks. Their approach however considers
one single bottleneck and parameterizes the encoder p(z|x; θ) by an entire neural network. The en-
coder maps the input variable x to a single bottleneck variable z that is not a part of the considered
neural network architecture. Therefore, their approach still treats a neural network as a whole rather
than optimizing it layer-wise. Furthermore, the work imposes a variational prior distribution in the
code space to approximate its actual marginal distribution. However, the variational approximate
distribution for the code space may be too loose while the actual marginal distribution can be sam-
pled easily.
Our work, on the other hand, focuses on better exploiting intermediate representations of a neural
network architecture using the IB principle. More specifically, our work proposes an optimization
IB criteria for an existing neural network architecture in an effort to better learn the layers’ represen-
tation to their IB optimality. In estimating mutual information, we adopted the variational method as
in Alemi et al. (2016) for I(Z, Y ) but use empirical estimation for I(Z, X). Furthermore, we exploit
the existing network architecture as variational decoders rather than resort to variational decoders
that are not part of the neural network architecture.
3	Parametric Information B ottleneck
This section presents an information-theoretic perspective of neural networks and then defines our
PIB framework. This perspective paves a way for the soundness of constraining the compression-
relevance trade-off into a neural network.
We denote X, Y as the input and the target (label) variables of the data, respectively; Zl as a stochas-
tic variable represented by the lth hidden layer of a neural network where 1 ≤ l ≤ L, L is the number
of hidden layers. We extend the notations of Zl by using the convention Zo := X and Z-1 := 0.
The space of X, Y and Zl are denoted as X , Y and Zl, respectively. Each respective space is asso-
ciated with the corresponding probability measures pD (x), pD (y) and p(zl) where pD (.) indicates
the underlying probability distribution of the data and p(.) denotes model distributions. Each Zl is
stochastically mapped from the previous stochastic variable Zl-1 via an encoder p(zl|zl-1). We
name Zl , 1 ≤ l ≤ L as a (information) bottleneck or code variable of the network. In this work,
we focus on binary bottlenecks where Zl ∈ {0, 1}nl and ni is the dimensionality of the bottleneck
space.
3.1	Neural Networks as Sequential Quantization
An encoder p(z|x) introduces a soft partitioning of the space X into anew space Z whose probabil-
ity measure is determined as p(z) = p(z|x)pD(x)dx. The encoding can modify the information
content of the original space possibly including its dimensionality and topological structure. On
average, 2H(X |Z) elements of X are mapped to the same code in Z. Thus, the average volume of a
partitioning of X is 2H(X)/2H(X|Z) = 2I(X,Z). The mutual information I(Z, X) which measures
the amount of information that Z contains about X can therefore quantify the quality of the encod-
ing p(z|x). A smaller mutual information I(Z, X) implies a more compressed representation Z in
terms of X .
Since the original data space is continuous, it requires infinite precision to represent it precisely.
However, only some set of underlying explanatory factors in the the data space would be benefi-
cial for a certain task. Therefore, lossy representation is often more helpful (and of course more
efficient) than a precise representation. In this aspect, we view the hidden layers of a multi-layered
neural network as a lossy representation of the data space. The neural network in this perspective
consists of a series of stochastic encoders that sequentially encode the original data space X into
the intermediate code spaces Zl. These code spaces are lossy representations of the data space as it
follows from the data-processing inequality (DPI) (Cover & Thomas (2006)) that
H(X) ≥ I(X,Zl) ≥ I(X,Zl+1)	(3)
where we assume that Y, X, Zl and Zl+1 form a Markov chain in that order, i.e.,
Y → X → Xl → Xl+1	(4)
3
Figure 1: A directed graphical representation of a PIB of two bottlenecks. The neural network
parameters θ = (θ1 , θ2, θ3). The dashed blue arrows do not denote variable dependencies but
the relevance decoders for each bottleneck. The relevance decoder ptrue(y|zi), which is uniquely
determined given the encoder pθ(z∕x) and the joint distribution PD(x,y), is intractable. We use
Pθ(y ∣Zi) as a variational approximation to each intractable relevance decoder Ptrue(y∣Zi).
A learning principle should compress irrelevant information and preserve relevant information in
the lossy intermediate code spaces. In the next subsection, we describe in details how a sequential
series of encoders, compression and relevance are defined in a neural network.
3.2	PIB Framework
Our PIB framework is an extension of the IB framework to optimize all paramters of neural net-
works. In neural networks, intermediate representations represent a hierarchy of information bottle-
necks that sequentially extract relevant information for a target from the input data space. Existing
IB framework for DNNs specifies a single bottleneck while our PIB preserves hierarchical represen-
tations which a neural network’s expressiveness comes from. Our PIB also gives neural networks
an information-theoretic interpretation both in network structure and model learning. In PIBs, we
utilize only neural network parameters θ for defining encoders and variational relevance decoders at
every level, therefore the name Parametric Information Bottleneck. Our PIB is also a standard step
towards better exploiting representational power of more expressive neural network models such as
Convolutional Neural Networks (LeCun et al. (1998)) and ResNet (He et al. (2016)).
3.2.1	Stochasticity
In this paper, we focus on binary bottlenecks in which the encoder p(zl |zl-1) is defined as
nl
p(zl|zl-1) =	p(zl,i |zl-1)	(5)
i=1
where
p(zι,i = I∣Z1-1) = σ (a((I)) = σ(wi;:1) zi-i + b(l)),	(6)
σ(.) is the sigmoid function, and W(l) is the weights connecting the lth layer to the (l + 1)th
layer. Depending on the structure of the target space Y, we can use an appropriate model for
output distributions as follows: (1) For classification, we model the output distribution with soft-
max function, p(Y = i|zL) = softmax(Wi(:L+1)zL + bi(L+1)); (2) For binary output vectors Y ,
we use a product of Bernoulli distributions, p(y|zL) = Qi p(yi |zL) where p(Yi = 1|zL) =
σ(wi>L+r1 zl + b(L+1)); (3) For real-valued output vectors Y, We use Gaussian distribution,
4
P(Y∣zl) = N(y;μ = W(L+1)ZL + b(L+1),σ2). The conditional distribution p(y∣x) from the
model is computed using the Bayes’ rule and the Markov assumption (Equation 4) in PIBs 2:
ZL+1
p(y, z|x)dz =	p(y|z)p(z|x)dz = p(zl|zl-1)dz
(7)
where z = (z1, z2, ..., zL) is the entire sequence of hidden layers in the neural network. Note that
fora given joint distribution pD (x, y), the relevance decoder ptrue(y|zl) is uniquely determined if an
encoding function p(zl|x) is defined. Specifically, the relevance decoder is determined as follows:
PM(M = IPD (x,y) Tdx
(8)
It is also important to note that many stochastic neural networks have been proposed before (e.g.,
Neal (1990), Neal (1992), Tang & Salakhutdinov (2013), Raiko et al. (2014), Dauphin & Grangier
(2016)). However, our motivation for this stochasticity is that it enables bottleneck sampling given
the data variables (X, Y). The generated bottleneck samples are then used to estimate mutual in-
formation. Thus, our framework does not depend on a specific stochastic model. For deterministic
neural networks, we only have one sample of hidden variables given one data point. Thus, estimat-
ing mutual information for hidden variables in this case is as hard as estimating mutual information
for the data variables themselves.
3.2.2 Learning principle
Since the neural network is a lossy representation of the original data space, a learning principle
should make this loss in a beneficial manner. Specifically in PIBs, we propose to jointly compress
the network’s intermediate spaces and preserve relevant information simultaneously at all layers of
the network. For the lth-level bottleneck Zl , the compression is defined as the mutual information
between Zl and the previous-level bottleneck Zl-1 while the relevance is specified as its mutual
information with the target variable Y. We explicitly define the learning objective for PIB as:
L
LPIB (Z) := LPIB(θ) :=X βl-1I(Zl, Zl-1) -I(Zl,Y)	(9)
l=0
where the layer-specific Lagrangian multiplier βl-1 controls the tradeoff between relevance and com-
pression in each bottleneck, and the concept of compression and relevance is taken to the extreme
when l = 0 (with convention that I(Zo, Z-ι) = I(X, 0) = H(X) = constant). Here We prefer
to this extreme, i.e., the 0th level, as the super level. While the lth level for 1 ≤ l ≤ L indicates a
specific hidden layer l, the super level represents the entire neural network as a whole.
The objective LPIB can be considered as a joint version of the theoretical IB analysis for DNNs
in Tishby & Zaslavsky (2015). However, minimizing LPIB has an intuitive interpretation as tight-
ening the “information knots” of a neural network architecture simultaneously at every layer level
(including the super level). Optimizing PIBs now becomes the minimization of LPIB (Z) which at-
tempts to decrease I(Zl, Zl-1) and increase I(Zl, Y) simultaneously. The decrease of I(Zl, Zl-1)
makes the representation at the lth-level more compressed while the increase of I(Zl, Y) promotes
the preservation of relevant information in Zl about Y. In optimization’s aspect, the minimization of
LPIB is much harder than the minimization of LIB since LPIB involves inter-dependent terms that
even the self-consistent equations of the IB framework are not applicable to this case. Furthermore,
LPIB is intractable since the bottleneck spaces are usually high-dimensional and the relevance en-
coders Ptrue (y|zl) (computed by Equation 8) are intractable. In the following section, we present
our approximation to LPIB which fully utilizes the existing architecture without resorting to any
model that is not part of the considered neural network. The approximation then leads to effective
gradient-based training of PIBs.
3.3 Approximate learning
Here, we present our approximations to the relevance and the compression terms in the PIB objective
LPIB.
2Here we use integral even for discrete-valued variables instead of sum P for denotation simplicity.
5
3.3.1 Approximate relevance
Since the relevance decoder ptrue(y|zl) (Equation 8) is intractable, we use a variational relevance
decoder pv(y|zl) to approximate it. Firstly, we decompose the mutual information into a difference
of two entropies:
I(Zl,Y)=H(Y)-H(Y|Zl)	(10)
where H(Y ) = constant can be ignored in the minimization of L(Z), and
H(Y |Zl) = -	ptrue(y|zl)p(zl) log ptrue(y|zl)dydzl	(11)
= -	pD (x, y)p(zl |x) logptrue(y|zl)dzldxdy	(12)
= -	pD (x, y)p(zl |x) log pv(y|zl)dzldxdy
-	p(zl)DKL[ptrue(y|zl)||pv(y|zl)]dzl	(13)
≤ -	pD (x, y)p(zl |x) log pv(y|zl)dzldxdy	(14)
=-EpD (x,y) [Ep(zι∣x) [log Pv (y∣Zl)]] =: H(Y |Zl)	(15)
where the equality in Equation 12 holds due to the Markov assumption (Equation 4). In PIBs, we
propose to use the higher-level part of the existing network architecture at each layer to define the
variational relevance encoder for that layer, i.e., pv(y|zl) = p(y |zl) where p(y |zl) is determined by
the network architecture. In this case, we have:
L+1
pv(y|zl) = p(y|zl) =	p(zi+1 |zi)dzL...dzl+1 = Ep(zL |zl) [p(y|zL)]	(16)
i=l
We will refer to H (Y|Zl) as the variational conditional relevance (VCR) for the lth-level bottleneck
variable Zl for the rest of this work. In the following, we present two important results which
indicate that the relevance terms in our objective is closely and mutually related to the concept of
the MLE principle.
Proposition 3.1. The VCR at the super level (i.e., l = 0) equals the negative log-likelihood (NLL)
function.
Proposition 3.2. The VCR at the highest-level bottleneck variable ZL equals the VCR for the entire
compositional bottleneck variable Z = (Z1, Z2, ..., ZL) which is an upper bound on the NLL. That
is,
~ , . ~ , - .,
H(Y|Zl) = H(Y|Z) ≥ -EpD(χ,y) [logp(y∣x)]	(17)
While the Proposition 3.1 is a direct result of Equation 16, the Proposition 3.2 holds due to Jensen’s
inequality (its detail derivation in Appendix I.A).
In PIB’s terms, the MLE principle can be interpreted as increasing the VCR of the network as a
whole while the PIB objective takes into account the VCR at every level of the network. In turn,
the VCR can also be interpreted in terms of the MLE principle as follows. It follows from Equation
15 and 16 that the VCR for layer l (including l = 0) is the NLL function of p(y|zl ). Therefore,
increasing the Relevance parts of JPIB is equivalent to performing the MLE principle for every layer
level instead of the only super level as in the standard MLE. Another interpretation is that our PIB
framework encourages forwarding explicit information from all layer levels for better exploitation
during learning while the MLE principle performs an implicit information forwarding by using only
information from the super level. Finally, the VCR for a multivariate y can be decomposed into the
sum of that for each component of y (see Appendix I.C).
6
3.3.2 Approximate Compression
The compression terms in LPIB involve computing mutual information between two consecutive
bottlenecks. For simplicity, we present the derivation of I(Z1, Z0) only 3. For the compression, we
decompose the mutual information as follows:
I(Z1,Z0) =H(Z1)-H(Z1|Z0)	(18)
which consists of the entropy and conditional entropy term. The conditional entropy can be further
rewritten as:
H(Z1|Z0) =
p(z0)H(Z1|Z0
z0)dz0
N1
p(z0) X
H(Z1,i|Z0 = z0)dz0
N1
= Ep(z0) XH(Z1,i|Z0=z0)	(19)
i=1
where Z1 = (Z1,i)iN=11 andH(Z1,i|Z0 = z0) = -qlogq - (1 - q)log(1 - q) where q = p(Z1,i =
1|Z0 = z0). For the entropy term H(Z1), we resort to empirical samples of z1 generated by Monte
Carlo sampling to estimate the entropy:
1M
H(Zι) = -Ep(zι)[logp(zι)] ≈ -M ElOgp(z1k)) =: HMLE(Zι)	(20)
k=1
where z，) 〜p(zι) = Ep(z0)[p(z1∣z0)]. This estimator is also known as the maximum likelihood
estimator or ‘plug-in’ estimator (Antos & Kontoyiannis (2001)). The larger number of samples M
guarantees the better plug-in entropy by the following bias bound (Paninski (2003))
|Z1 | - 1
|E[HMLE(Zι)]- H(Zi)I ≤ log(1 + LM—)	(21)
where |Z1 | denotes the cardinality of the space of variable Z1. In practice, logp(z1) may be nu-
merically unstable for large cardinality |Z1 |. In the large space of Z1 , the probability of a single
pointp(z1) may become very small that logp(z1) becomes numerically unstable. To overcome this
problem, we propose an upper bound on the entropy using Jensen’s inequality:
logp(z1) = log Ep(z0)[p(z1|z0)] ≥ Ep(z0) [logp(z1|z0)]	(22)
Thus,
H(Zi) ≤ -Ep(zι) [Ep(zo) [logp(z1∣z0)]] =： H(Zi)	(23)
The upper bound H(Zi) is numerically stable because the conditional distribution p(zi |z0) is fac-
torized into Qip(zi,i|z0), therefore, logp(zi |z0) = Pi logp(zi,i |z0) which is more stable. The
upper bound H(Zi) can then be estimated using Monte Carlo sampling for z0 and zi.
3.3.3 Approximate Gradients via B inary Bottlenecks
Discrete-valued variables in PIBs make standard back-propagation not straightforward. Fortunately,
one can estimate the gradient in this case. The authors in Tang & Salakhutdinov (2013) used a
Generalized EM algorithm while Bengio et al. (2013) proposed to resort to reinforcement learning.
However, these estimators have high variance. In this work, we use the gradient estimator inspired
by Raiko et al. (2014) for binary bottlenecks because it has low variance despite of being biased.
Specifically, a bottleneck z = (zi, z2, ..., znl ) can be rewritten as being continuous by zi = σ(ai) +
i where
1 - σ(ai) with probability σ(ai)
i =
-σ(ai) with probability 1 - σ(ai)
The bottleneck component zi defined as above still gets value of either 0 or 1 but it is decomposed
into the sum of a deterministic term and a noise term. The gradient is then propagated only through
the deterministic term and ignored in the noise term. A detail of gradient-based training of PIB is
presented in Algorithm 1. One advantage of GRAD-PIB algorithm is that it requires only a single
forward pass to estimate all the information terms in LPIB since the generated samples are re-used
to compute the information terms at each layer level.
3The extension at the other levels is straightforward from the derivation of I(Z1, Z0).
7
Algorithm 1 Minibatch version of training PIB, we use M = 16 for training (and M = 32 for
testing).
1:	procedure GRAD-PIB
2:	Input: Labeled training dataset SD
3:	θ J Initialize parameters
4:	repeat:
5:	(xi, yi)iN=1 J Random minibatch of N samples drawn from SD
6:	Generate M samples of zi per each sample of zi-1 for 1 ≤ i ≤ L
7:	Use the generated samples above and Equations 15 and 23 to approximate LPIB(θ)
8:	g J 互 LPIB (θ) using Raiko estimator
∂θ
9:	θ J Update parameters using the approximate gradients g and SGD
10:	until convergence of parameters θ
11:	Output: θ
12:	end procedure
4	Experiments
We used the same architectures for PIBs and Stochastic Feed-forward Neural Networks (SFNNs)
(e.g., Tang & Salakhutdinov (2013)) and trained them on the MNIST dataset (LeCun et al. (1998))
for image classification, odd-even decision problem and multi-modal learning. Here, a SFNN sim-
ply prefers to feed-forward neural network models following the MLE principle for learning model
parameters. Each hidden layer in SFNNs is also considered as a stochastic variable. The afore-
mentioned tasks are to evaluate PIBs, as compared to SFNNs, in terms of generalization, learning
dynamics, and capability of modeling complicated output structures, respectively. All models are
implemented using Theano framework (Al-Rfou et al. (2016)).
4.1	MNIST Classification
In this experiment, we compare PIBs with SFNNs and deterministic neural networks in the classifi-
cation task. For comparisons, we trained PIBs and five additional models. The first model (Model
A) is a deterministic neural network. In Model D, we used the weight trained in Model A to perform
stochastic prediction at test time. Model E is SFNN and Model B is Model C with deterministic
prediction during test phase. Model C uses the weighted trained in PIB but we report deterministic
prediction instead of stochastic prediction for test performance.
	Model	Mean (%)	Std dev.
deterministic	deterministic (A)	1.73	-
	SFNN as deterministic (B)	1.88	-
	PIB as deterministic (C)	1.46	-
stochastic	deterministic as stochastic (D)	2.30	-0:07-
	SFNN (E)	1.94	0.036
	PIB	1.47	0.034
Table 1: The MNIST classification results of various models.
The MNIST dataset (LeCun (1998)) contains a standard split of 60000, and 10000 examples of
handwritten digit images for training and test, respectively in which each image is grayscale of size
28 × 28 pixels. We used the last 10000 images of the training set as a holdout set for tuning hyper-
parameters. The best configuration chosen from the holdout set is used to retrain the models from
scratch in the full training set. The result in the test set is then reported (for stochastic prediction,
we report mean and standard deviation). We scaled the images to [0, 1] and do not perform any
other data augmentation. These base configurations are applied to all six models we use in this
experiment.
The base architecture is a fully-connected, sigmoid activation neural network with two hidden layers
and 512 units per layer. Weights are initialized using Xavier initialization (Glorot & Bengio (2010)).
8
Figure 2: A comparison of Monte-Carlo averaging and deterministic prediction of PIB.
Models were optimized with stochastic gradient descent with a constant learning rate of 0.1 and a
batch size of 8. For stochastic sampling, we generate M = 16 samples per point during training and
M = 32 samples per point during testing. For stochastic prediction, we run the prediction 10 times
and report its mean and deviation standard. For PIBs, we set βl = β, ∀1 ≤ l ≤ L. We tuned β from
{0} ∪ {10-i : 1 ≤ i ≤ 7}, and found β-1 = 10-4 works best.
Table 1 provides the results in the MNIST classification error in the test set for PIB and the com-
parative models (A), (B), (C), (D), and (E). As can be seen from the table, PIB and Model C gives
nearly the same performance which outperform deterministic neural networks and SFNNs, and their
stochastic and deterministic version.
It is interesting to empirically see that the deterministic version of PIB at test time (Model C) gives a
slightly better result than PIB. This also empirically holds for the case of SFNN. To investigate more
in this, we compute the test error for various values of the number of samples used for Monte-Carlo
averaging, M (Figure 2). As we can see from the figure, the Monte-Carlo averaging of PIB obtains
its good approximation around M = 30 and the deterministic prediction roughly places a lower
bound on the Monte-Carlo averaging at test time. For visualization of learned filters of PIB, see
Appendix II.A.
4.2	Learning dynamics
One way to visualize the learning dynamic of each layer of a neural network is to plot the layers in the
information plane (Tishby et al. (1999), Slonim (2003)). The information plane is an information-
theoretic plane that characterizes any representation Z = Z(X) in terms of (I(Z, Y ), I(Z, X))
given the joint distribution I(X, Y ). The plane has I(Z, X) and I(Z, Y ) as its horizontal axis
and its vertical axis, respectively. In the general IB framework, each value of β specifies a unique
point of Z in the information plane. As β varies from 0 to ∞, Z traces a concave curve, known
as information curve for representation Z, with a slope of β-1. The information-theoretic goal of
learning a representation Z = Z(X) is therefore to push Z as closer to its corresponding optimal
point in the information curve as possible. For multi-layered neural networks, each hidden layer Zl
is a representation that can also be quantified in the information plane.
In this experiment, we considered an odd-even decision problem in the MNIST dataset in which
the task is to determine if the digit in an image is odd or even. We used the same neural network
architecture of 784-10-10-10-1 for PIB and SFNN and trained them with SGD with constant learning
rate of 0.01 in the first 50000 training samples. We used three different randomly initialized neural
9
Figure 3: The learning dynamic of PIB (left) and SFNN (right) in a decision problem are presented
in the information plane. Each point represents a hidden layer while the color indicate epochs.
Because of the Markov assumption (Equation 4), we have H(X) ≥ I(Zi, X) ≥ I(Zi+1, X) and
I(X,Y) ≥I(Zl,Y) ≥I(Zl+1,Y).
networks and averaged the mutual informations. For PIB, we used βl-1 = β-1 = 10-4. Since
the network architecture is small, we can compute mutual information Ix := I(Zi, X) and Iy :=
I(Zi, Y ) precisely and plot them over training epochs.
As indicated by Figure 3, both PIB and SFNN enable the network to gradually encode more infor-
mation into their hidden layers at the beginning as I(Zi , X) increases. The encoded information at
the beginning also contains some relevant information for the target variable as I(Zi, Y ) increases
as well. However, information encoding in the PIB is more selective as it quickly encodes more rel-
evant information (it reaches higher I(Z, Y ) but in lesser number of epochs) while keeps the layers
concise at higher epochs. The SFNN, on the other hand, encodes information in a way that matches
the model distribution to the empirical data distribution. As a result, it may encode irrelevant infor-
mation that hurts the generalization.
For additional visualization, an empirical architecture analysis of PIB and SFNN is presented in
Appendix II.B.
4.3	Multi-modal learning
As PIB and SFNN are stochastic neural networks, they can model structured output space in which
a one-to-many mapping is required. A binary stochastic variable zl of dimensionality nl can take on
2nl different states each of which would give a different y . This is the reason why the conditional
distribution p(y|x) in stochastic neural networks is multi-modal.
In this experiment, we followed Raiko et al. (2014) and predicted the lower half of the MNIST digits
using the upper half as inputs. We used the same neural network architecture of 392-512-512-392
for PIB and SFNN and trained them with SGD with constant learning rate of 0.01. We trained the
models in the full training set of 60000 images and tested in the test set. For PIB, we also used
βl-1 = β-1 = 10-4. The visualization in Figure 4 indicates that PIB models the structured output
space better and faster (using lesser number of epochs) than SFNN. The samples generated by PIB
is totally recognizable while the samples generated by SFNN shows some discontinuity (e.g., digit
2, 4, 5, 7) and confusion (e.g., digit 3 confuses with number 8, digit 5 is unrecognizable or confuses
with number 6, digit 8 and 9 are unrecognizable).
5	Conclusion
In this paper we introduced an information-theoretic learning framework to better exploit a neural
network’s representation. We have also proposed an approximation that fully utilizes all parameters
in a neural network and does not resort to any extra models. Our learning framework offers a
principled way of interpreting and learning all layers of neural networks and encourages a more
10
<o∕ 乙¾夕(ʃs
-O/乙¾V(ΓS 7pe.
ð/ 乙¾qσb7f<γ.
O /乙¾夕-Gs 7? a
0∕2.¾ζrσs 7pq
C /乙¾0--sb7pq
O/乙¾夕(ʃs 7P0
C /乙¾4cs 7pq
O/乙¾夕 S & 7f⅛
6∕ζ,¾夕 Fb 7 P a
⅛⅛⅛∙∙∙⅛A∙∙
。/乙¾qcfo7S Q
0⅛6 06060^000
QqqqUnqQq
G痴GeGCGG匕弓匚匚
Q&Qsssssssss
T7 777777777
夕一"????????
一。MAqd % aαq
Figure 4: Samples drawn from the prediction of the lower half of the MNIST test data digits based
on the upper half for PIB (left, after 60 epochs) and SFNN (right, after 200 epochs). The leftmost
column is the original MNIST test digit followed by the masked out digits and nine samples. The
rightmost column is obtained by averaging over all generated samples of bottlenecks drawn from
the prediction. The figures illustrate the capability of modeling structured output space using PIB
and SFNN.
informative yet compressed representation, which is supported by qualitative empirical results. One
limitation is that we consider here fully-connected feed-forward architecture with binary hidden
layers. Since we used generated samples to estimate mutual information, we can potentially extend
the learning framework to larger and more complicated neural network architectures. This work
is our first step toward exploiting expressive power of large neural networks using information-
theoretic perspective that is not yet fully utilized.
References
Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau,
Nicolas Ballas, Frederic Bastien, JUstin Bayer, Anatoly Belikov, Alexander Belopolsky, Yoshua
Bengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, Nicolas
Bouchard, Nicolas BoUIanger-LeWandoWski, Xavier Bouthillier, Alexandre de BrebiSson, Olivier
Breuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooij-
mans, Marc-Alexandre C6te, Myriam Cote, Aaron Courville, Yann N. Dauphin, Olivier Delal-
leau, Julien Demouth, Guillaume Desjardins, Sander Dieleman, Laurent Dinh, MeIanie Ducoffe,
Vincent Dumoulin, Samira Ebrahimi Kahou, Dumitru Erhan, Ziye Fan, Orhan Firat, Mathieu
Germain, Xavier Glorot, Ian GoodfelloW, Matt Graham, Caglar Gulcehre, Philippe Hamel, Iban
Harlouchet, Jean-Philippe Heng, BaIazS Hidasi, Sina Honari, Arjun Jain, SebaStien Jean, Kai
Jia, Mikhail Korobov, Vivek Kulkarni, Alex Lamb, Pascal Lamblin, Eric Larsen, CeSar Lau-
rent, Sean Lee, Simon Lefrancois, Simon Lemieux, Nicholas Leonard, Zhouhan Lin, Jesse A.
Livezey, Cory Lorenz, Jeremiah LoWin, Qianli Ma, Pierre-Antoine Manzagol, Olivier Mastropi-
etro, Robert T. McGibbon, Roland Memisevic, Bart van Merrienboer, Vincent Michalski, Mehdi
Mirza, Alberto Orlandi, Christopher Pal, Razvan Pascanu, Mohammad Pezeshki, Colin Raffel,
Daniel RenshaW, MattheW Rocklin, Adriana Romero, Markus Roth, Peter SadoWski, John Sal-
vatier, Francois Savard, Jan Schluter, John Schulman, Gabriel Schwartz, Iulian Vlad Serban,
Dmitriy Serdyuk, Samira Shabanian, Etienne Simon, Sigurd Spieckermann, S. Ramana Subra-
manyam, Jakub Sygnowski, Jeremie Tanguay, Gijs van Tulder, Joseph Turian, Sebastian Urban,
Pascal Vincent, Francesco Visin, Harm de Vries, David Warde-Farley, Dustin J. Webb, MattheW
Willson, Kelvin Xu, Lijun Xue, Li Yao, Saizheng Zhang, and Ying Zhang. Theano: A Python
framework for fast computation of mathematical expressions. arXiv e-prints, abs/1605.02688,
May 2016. URL http://arxiv.org/abs/1605.02688.
Alexander A. Alemi, Ian Fischer, Joshua V. Dillon, and Kevin Murphy. Deep variational information
bottleneck. CoRR, abs/1612.00410, 2016. URL http://arxiv.org/abs/1612.00410.
11
A. Antos and I. Kontoyiannis. Convergence properties of functional estimates for discrete distribu-
tions. pp. 163193, 2001.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
Yoshua Bengio, Nicholas Leonard, and Aaron C. Courville. Estimating or propagating gradients
through stochastic neurons for conditional computation. CoRR, abs/1308.3432, 2013. URL
http://arxiv.org/abs/1308.3432.
Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss. Information bottleneck for gaus-
sian variables. Journal of Machine Learning Research, 6:165-188, 2005. URL http://www.
jmlr.org/papers/v6/chechik05a.html.
KyUnghyUn Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.
Thomas M. Cover and Joy A. Thomas. Elements of information theory. Wiley Series in Telecom-
munications and Signal Processing, 2006.
Yann N Dauphin and David Grangier. Predicting distributions with linearizing belief networks.
ICLR,, 2016.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and
Statistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, pp. 249-256,
2010. URL http://www.jmlr.org/proceedings/papers/v9/glorot10a.html.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016,
Las Vegas, NV, USA, June 27-30, 2016, pp. 770-778, 2016. doi: 10.1109/CVPR.2016.90. URL
https://doi.org/10.1109/CVPR.2016.90.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In Proceedings of the 32nd International Conference on Machine
Learning, ICML 2015, Lille, France, 6-11 July 2015, pp. 448-456, 2015. URL http://jmlr.
org/proceedings/papers/v37/ioffe15.html.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR,
abs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with
deep convolutional neural networks. In Advances in Neural Information Process-
ing Systems 25: 26th Annual Conference on Neural Information Processing Sys-
tems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,
United States., pp. 1106-1114, 2012. URL http://papers.nips.cc/paper/
4824-imagenet-classification-with-deep-convolutional-neural-networks.
S. Kullback and R.A. Leibler. On information and sufficiency. Annals of Mathematical Statistics,
22 (1):7986, 1951.
Bottou L eon Bengio Yoshua & Haffner Patrick LeCun, Yann. Gradient-based learning applied to
document recognition. pp. 22782324, 1998.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
R. M. Neal. Learning stochastic feedforward networks. 1990.
R. M. Neal. Connectionist learning of belief networks. pp. 71-113, 1992.
Liam Paninski. Estimation of entropy and mutual information. Neural Computation, 15(6):1191-
1253, 2003. doi: 10.1162/089976603321780272. URL https://doi.org/10.1162/
089976603321780272.
12
Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh. Techniques for learning
binary stochastic feedforward neural networks. CoRR, abs/1406.2989, 2014. URL http://
arxiv.org/abs/1406.2989.
David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driess-
che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander
Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lilli-
crap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering
the game of go with deep neural networks and tree search. Nature, 529(7587):484-489, 2016.
doi: 10.1038/nature16961. URL https://doi.org/10.1038/nature16961.
N. Slonim. Information bottleneck theory and applications. PhD thesis, Hebrew University of
Jerusalem, 2003.
Noam Slonim and Yair Weiss.	Maximum likelihood and the information bottle-
neck. In Advances in Neural Information Processing Systems 15 [Neural Infor-
mation Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, British
Columbia, Canada], pp. 335-342, 2002. URL http://papers.nips.cc/paper/
2214-maximum-likelihood-and-the-information-bottleneck.
Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learn-
ing Research, 15(1):1929-1958, 2014. URL http://dl.acm.org/citation.cfm?id=
2670313.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed, Dragomir Anguelov,
Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions.
In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA,
USA, June 7-12, 2015, pp. 1-9, 2015. doi: 10.1109/CVPR.2015.7298594. URL https://
doi.org/10.1109/CVPR.2015.7298594.
Yichuan Tang and Ruslan Salakhutdinov. Learning stochastic feedforward neural networks. In Ad-
vances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Infor-
mation Processing Systems 2013. Proceedings ofa meeting held December 5-8, 2013, Lake Tahoe,
Nevada, United States., pp. 530-538, 2013. URL http://papers.nips.cc/paper/
5026-learning-stochastic-feedforward-neural-networks.
N. Tishby and N. Zaslavsky. Deep learning and the information bottleneck principle. pp. 1-
5, 2015. doi: 10.1109/ITW.2015.7133169. URL http://dx.doi.org/10.1109/ITW.
2015.7133169.
N. Tishby, Fernando C. Pereira, and William Bialek. The information bottleneck method. In Proc.
of the 37-th Annual Allerton Conference on Communication, Control and Computing, 1999.
13
Appendix I
A.	Proof of the Prepositions
Proof of the Preposition 3.2: It follows from the Markov chain assumption (4) that p(y|z)
p(y|zL, zL-1, ..., z1) = p(y|zL) and from Jensen’s inequality that
p(z|x) logp(y|z)dz ≤ log
p(z |x)p(y|z)dz
log p(y|x)
Hence, the variational compositional relevance H(Y |Z) equals the variational relevance for the last
bottleneck and is an upper bound on the negative log-likelihood as well (Q.E.D).
B.	MLE as distribution matching
The purpose of the MLE principle can be interpreted as matching the model distribution to the em-
pirical data distribution using the KL divergence as a measure of their discrepancy. Rigorously,
given a set of samples X = {x1, x2, ..., xN} i.i.d. drawn from some underlying data distribution
pD(x), a parametric model pmodel (x; θ) attempts to map any data sample x to a real number that es-
timates the true probability pD(x). The MLE principle maximizes the likelihood function under the
empirical data distribution. This in turn can be interpreted as matching the model distribution pmodel
with the data distribution pD by minimizing their KL divergence to find the maximum likelihood
(point) estimator for θ:
Θml = arg max Ex〜PD(x) [logPmodeI(X θ)]	(24)
θ
=arg min [-Ex 〜pd(x) [log PmodeI(X θ)]+ Ex 〜PD (x) [log PD (x； θ)]]	(25)
θ
= arg min DKL [PD(x)kPmodel(x; θ)]	(26)
θ
N
≈ arg max	log Pmodel (xi; θ)	(27)
θ i=1
where expression (27) is an empirical estimation of expression (24) for N datapoints.
C.	Variational relevance for multivariate target variable
The VCR at level l (defined by (15), (16)) for a multivariate variable y can be decomposed into the
VCRs for each of its components. Indeed, without loss of generality, we assume bivariate target
variable y = (y1, y2). It follows from the fact that the neurons within a layer are independent given
some previous layer that we have:
H(Y|Z1) = -EPD(X,y1,y2) [Ep(zι∣χ) [logP(y1,y2∣z1)]]	(28)
= -EPD(x)PD(y1,y2|x) EP(zl |x) [log P(y1 |zl) +logP(y2|zl)]	(29)
=	-EPD(x)PD (yi |x) [EP(zl |x) [log P(yi |zl )]	(30)
i
=X H(YjZl)	(31)
i
14