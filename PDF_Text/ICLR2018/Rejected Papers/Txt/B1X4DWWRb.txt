Under review as a conference paper at ICLR 2018
Learning Weighted Representations for Gen-
eralization Across Designs
Anonymous authors
Paper under double-blind review
Ab stract
Predictive models that generalize well under distributional shift are often desir-
able and sometimes crucial to machine learning applications. One example is the
estimation of treatment effects from observational data, where a subtask is to pre-
dict the effect of a treatment on subjects that are systematically different from
those who received the treatment in the data. A related kind of distributional shift
appears in unsupervised domain adaptation, where we are tasked with generaliz-
ing to a distribution of inputs that is different from the one in which we observe
labels. We pose both of these problems as prediction under a shift in design.
Popular methods for overcoming distributional shift are often heuristic or rely on
assumptions that are rarely true in practice, such as having a well-specified model
or knowing the policy that gave rise to the observed data. Other methods are
hindered by their need for a pre-specified metric for comparing observations, or
by poor asymptotic properties. In this work, we devise a bound on the general-
ization error under design shift, based on integral probability metrics and sample
re-weighting. We combine this idea with representation learning, generalizing and
tightening existing results in this space. Finally, we propose an algorithmic frame-
work inspired by our bound and verify is effectiveness in causal effect estimation.
1	Introduction
A long-term goal in artificial intelligence is for agents to learn how to act. This endeavor relies
on accurately predicting and optimizing for the outcomes of actions, and fundamentally involves
estimating counterfactuals—what would have happened if the agent acted differently? In many ap-
plications, such as the treatment of patients in hospitals, experimentation is infeasible or impractical,
and we are forced to learn from biased, observational data. Doing so requires adjusting for the dis-
tributional shift between groups of patients that received different treatments. A related kind of
distributional shift arises in unsupervised domain adaptation, the goal of which is to learn predictive
models for a target domain, observing ground truth only in a source domain.
In this work, we pose both domain adaptation and treatment effect estimation as special cases of
prediction across shifting designs, referring to changes in both action policy and feature domain.
We separate policy from domain as we wish to make causal statements about the policy, but not
about the domain. Learning from observational data to predict the counterfactual outcome under
treatment B for a patient who received treatment A, one must adjust for the fact that treatment A
was systematically given to patients of different characteristics from those who received treatment
B. We call this predicting under a shift in policy. Furthermore, if all of our observational data comes
from hospital P, but we wish to predict counterfactuals for patients in hospital Q, with a population
that differs from P , an additional source of distributional shift is at play. We call this a shift in
domain. Together, we refer to the combination of domain and policy as the design. The design for
which we observe ground truth is called the source, and the design of interest the target.
The two most common approaches for addressing distributional shift are to learn shift-invariant rep-
resentations of the data (Ajakan et al., 2014) or to perform sample re-weighting or matching (Shi-
modaira, 2000; Kallus, 2016). Representation learning approaches attempt to extract only informa-
tion from the input that is invariant to a change in design and predictive of the variable of interest.
Such representations are typically learned by fitting deep neural networks in which activations of
deeper layers are regularized to be distributionally similar across designs (Ajakan et al., 2014; Long
1
Under review as a conference paper at ICLR 2018
et al., 2015). Although representation learning can be shown to reduce the error associated to dis-
tributional shift (Long et al., 2015) in some cases, standard approaches are biased, even in the limit
of infinite data, as they penalize the use also of predictive information. In contrast, re-weighting
methods correct for distributional shift by assigning higher weight to samples from the source de-
sign that are representative of the target design, often using importance sampling. This idea has
been well studied in, for example, the causal inference (Rosenbaum & Rubin, 1983), domain adap-
tation (Shimodaira, 2000) and reinforcement learning (Precup et al., 2001) literature. For example,
in causal effect estimation, importance sampling is equivalent to re-weighting units by the inverse
probability of observed treatments (treatment propensity). Re-weighting with knowledge of impor-
tance sampling weights often leads to asymptotically unbiased estimators of the target outcome, but
may suffer from high variance in finite samples (Swaminathan & Joachims, 2015).
A significant hurdle in applying re-weighting methods is that optimal weights are rarely known
in practice. There are a variety of methods to learn these weights. Weights can be estimated as
the inverse of estimated feature or treatment densities (Rosenbaum & Rubin, 1983; Freedman &
Berk, 2008) but this plug-in approach can lead to highly unstable estimates. More stable methods
learn weights by minimizing distributional distance metrics (Gretton et al., 2009; Kallus, 2016;
2017; Zubizarreta, 2015). Closely related, matching (Stuart, 2010) produces weights by finding
units in the source design that are close in some metric to units in the target design. Specifying a
distributional or unit-wise metric is challenging, especially if the input space is high-dimensional
where no metric incorporating all features can ever be made small. This has inspired heuristics such
as first performing variable selection and then finding a matching in the selected covariates.
Our key algorithmic contribution is to show how to combine the intuition behind shift-invariant
representation learning and re-weighting methods by jointly learning a representation Φ of the in-
put space and a weighting function w(Φ) to minimize a) the re-weighted empirical risk and b) a
re-weighted measure of distributional shift between designs. This is useful also for the identity
representation Φ(x) = x, as it allows for principled control of the variance of estimators through
regularization of the re-weighting function w(x), mitigating the issues of exact importance sam-
pling methods. Further, this allows us to evaluate w on hold-out samples to select hyperparameters
or do early stopping. Finally, letting w depend on Φ alleviates the problem of choosing a metric by
which to optimize sample weights, as Φ is trained to extract information predictive of the outcome.
We capture these ideas in an upper bound on the generalization error under a shift in design and
specialize it to the case of treatment effect estimation.
Main contributions We bring together two techniques used to overcome distributional shift be-
tween designs—re-weighting and representation learning, with complementary robustness proper-
ties, generalizing existing methods based on either technique. We give finite-sample generalization
bounds for prediction under design shift, without assuming access to importance sampling weights
or to a well-specified model, and develop an algorithmic framework to minimize these bounds. We
propose a neural network architecture that jointly learns a representation of the input and a weight-
ing function to improve balance across changing settings. Finally, we apply our proposed algorithm
to the task of predicting causal effects from observational data, achieving state-of-the art results on
a widely used benchmark.
2	Predicting outcomes under design shift
The goal of this work is to accurately predict outcomes of interventions T ∈ T in contexts X ∈ X
drawn from a target design pπ (X, T ). The outcome of intervening with t ∈ T is the potential out-
Come Y(t) ∈ Y (Imbens & Rubin, 2015, Ch. 1-2), which has a stationary distributionPt(Y | X)
given context X . Assuming a stationary outcome is akin to the covariate shift assumption (Shi-
modaira, 2000), often used in domain adaptation.1 For example, in the classical binary setting,
Y(1) represents the outcome under treatment and Y(0) the outcome under control. The target de-
sign consists of two components: the target policy pπ(T | X), which describes how one intends
to map observations of contexts (such as patient prognostics) to interventions (such as pharmaco-
logical treatments) and the target domain pπ (X), which describes the population of contexts to
which the policy will be applied. The target design is known to us only through m unlabeled sam-
1Equivalently, we may write p∏ (Y (t) | X) = Pμ(Y (t) | X).
2
Under review as a conference paper at ICLR 2018
ples (x01, t01), . . . , (x0m, t0m) from pπ(X, T). Outcomes are only available to us in labeled samples
from a source domain: (x1, t1, y1), . . . , (xn, tn, yn), where (xi, ti) are draws from a source design
Pμ(X,T) and yi = yi (t) is a draw from PT(Y | X), corresponding only to the factual outcome
Y(T) of the treatment administered. Like the target design, the source design consists of a domain of
contexts for which we have data and a policy, which describes the (unknown) historical administra-
tion of treatment in the data. Only the factual outcomes of the treatments administered are observed,
while the counterfactual outcomes yi(t) for t 6= ti are, naturally, unobserved.
Our focus is the observational or off-policy setting, in which interventions in the source design are
performed non-randomly as a function of X, Pμ(T | X) = Pμ(T). This encapsulates both the
covariate shift often observed between treated and control populations in observational studies and
the covariate shift between the domain of the study and the domain of an eventual wider interven-
tion. Examples of this problem are plentiful: in addition to the example given in the introduction,
consider predicting the return of an advertising policy based on the historical results of a different
policy, applied to a different population of customers. We stress that we are interested in the causal
effect of an intervention T on Y, conditioned on X . As such, we cannot think of X and T as a sin-
gle variable. Without additional assumptions, it is impossible to deduce the effect ofan intervention
based on observational data alone (Pearl, 2009), as it amounts disentangling correlation and causa-
tion. Crucially, for any unit i, we can observe the potential outcome yi(t) of at most one intervention
t. In our analysis, we make the following standard assumptions.
Assumption 1 (Consistency, ignorability and overlap). For any unit i, assigned to intervention ti,
we observe Yi = Y(ti). Further, {Y(t)}t∈τ and the data-generating process pμ(X,T,Y) satisfy
strong ignorability: {Y(t)}t∈τ ⊥ T | X and overlap: Prpn (p*(T | X) > 0) = 1.
Assumption 1 is a sufficient condition for causal identifiability (Rosenbaum & Rubin, 1983). Ig-
norability is also known as the no hidden confounders assumption, indicating that all variables that
cause both T and Y are assumed to be measured. Under ignorability therefore, any domain shift
in p(X ) cannot be due to variables that causally influence T and Y, other than through X . Under
Assumption 1, potential outcomes equal conditional expectations: E[Y (t) | X = x] = E[Y | X =
x, T = t], and we may predict Y(t) by regression. We further assume common domain support,
∀x ∈ X : p∏(X = x) > 0 ⇒ Pμ(X = x) > 0. Finally, we adopt the notation p(x) := p(X = x).
2.1	Re-weighted risk minimization
We attempt to learn predictors f : X × T → Y such that f(x, t) approximates E[Y | X =
x, T = t]. Recall that under Assumption 1, this conditional expectation is equal to the (possibly
counterfactual) potential outcome Y (t), conditioned on X. Our goal is to ensure that hypotheses
f are accurate under a design p∏ that deviates from the data-generating process, Pμ. This is unlike
standard supervised learning for which p∏ = pμ. We measure the (in)ability of f to predict outcomes
under π, using the expected risk,
Rπ(f)
:= Eχ,t,y~p∏ [`f (x, t, y)]	(1)
based on a sample from μ, Dn = {(x"i, y' 〜p*; i = 1,..., n}. Here, 'f (x, t, y) := L(f (x,t),y)
is an appropriate loss function, such as the squared loss, L(y, y0) := (y - y0)2 or the log-loss,
depending on application. As outcomes under the target design pπ are not observed, even through
a finite sample, we cannot directly estimate (1) using the empirical risk under pπ. A common way
to resolve this is to use importance sampling (Shimodaira, 2000)——the observation that if Pμ and p∏
have common support, with w*(x, t) = p∏(x, t)∕pμ(x, t),
Rw*(f ):= Eχ,t,y〜pμ[w*(X,t)'f(X,t,y)] = Rn(f).	⑵
Hence, with access to w*, an unbiased estimator of Rπ(f) may be obtained by re-weighting the
(factual) empirical risk under μ,
*	1n
Rμ (f) := - Ew(Xi,ti)'f (Xi,ti,yi) .	(3)
n i=1
Unfortunately, importance sampling weights can be very large when p∏ is large and Pμ small, re-
*
sulting in large variance in Rw (f) (Swaminathan & JOaChims, 2015). More importantly, pμ (x, t)
is rarely known in practice, and neither is w* . In principle, however, any re-weighting function w
with the following property yields a valid risk under the re-weighted distribution Pw.
3
Under review as a conference paper at ICLR 2018
Definition 1. A functionW : X ×T → R+ is a valid re-weighting of pμ if
Eχ,t〜pμ[w(x,t)] = 1 and p*(x,t) > 0 ⇒ w(x,t) > 0.
We denote the re-weighted density Pw (x,t) := w(x,t)p*(x,t).
A natural candidate in place of w* is an estimate W* based on estimating densities p∏ (χ,t) and
Pμ(χ, t). In this work, We adopt a different strategy, learning parameteric re-weighting functions W
from observational data, that minimize an upper bound on the risk under pπ .
2.2 Conditional treatment effect estimation
An important special case of our setting is when treatments are binary, T ∈ {0, 1}, often inter-
preted as treating (T = 1) or not treating (T = 0) a unit, and the domain is fixed across de-
signs, pμ(X) = p∏(X). This is the classical setting for estimating treatment effects——the effect
of choosing one intervention over another (Morgan & Winship, 2014).2 The effect of an inter-
vention T = 1 in context X , is measured by the conditional average treatment effect (CATE),
τ(x) = E [Y (1) - Y (0) | X = x]. Predicting τ for unobserved units typically involves prediction
of both potential outcomes3. In a clinical setting, knowledge of τ is necessary to assess which
medication should be administered to a certain individual. Historically, the (population) average
treatment effect, ArE = Ex〜P [τ(x)], has received comparatively much more attention (Rosenbaum
& Rubin, 1983), but is inadequate for personalized decision making. Using predictors f (x, t) of
potential outcomes Y(t) in contexts X = x, we can estimate the CATE by τ(x) = f (x, 1) - f (x, 0)
and measure the quality using the mean squared error (MSE),
MSE(T) = Ep [(τ(x) - T(x))2]	(4)
In Section 4, we argue that estimating CATE from observational data requires overcoming distribu-
tional shift with respect to the treat-all and treat-none policies, in predicting each respective potential
outcome, and show how this can be used to derive generalization bounds for CATE.
3	Related work
A large body of work has shown that under assumptions of ignorability and having a well-
specified model, various regression methods for counterfactual estimation are asymptotically consis-
tent (Chernozhukov et al., 2017; Athey & Imbens, 2016; Belloni et al., 2014). However, consistency
results like these provide little insight into the case of model misspecification. Under model misspec-
ification, regression methods may suffer from additional bias when generalizing across designs due
to distributional shift. A common way to alleviate this is importance sampling, see Section 2. This
idea is used in propensity-score methods (Austin, 2011), that use treatment assignment probabilities
(propensities) to re-weight samples for causal effect estimation, and more generally in re-weighted
regression, see e.g. (Swaminathan & Joachims, 2015). A major drawback of these methods is the
assumption that the design density is known. To address this, others (Gretton et al., 2009; Kallus,
2016), have proposed learning sample weights W to minimize the distributional distance between
samples under p∏ and pw, but rely on specifying the data representation a priori, without regard for
which aspects of the data actually matter for outcome prediction and policy estimation.
On the other hand, Johansson et al. (2016); Shalit et al. (2017) proposed learning representations
for counterfactual inference, inspired by work in unsupervised domain adaptation (Mansour et al.,
2009). The drawback of this line of work is that the generalization bounds of Shalit et al. (2017)
and Long et al. (2015) are loose——even if infinite samples are available, they are not guaranteed to
converge to the lowest possible error. Moreover, these approaches do not make use of important
information that can be estimated from data: the treatment/domain assignment probabilities.
4	Generalization under design shift
We give a bound on the risk in predicting outcomes under a target design pπ (T, X) based on un-
labeled samples from p∏ and labeled samples from a source design pμ(T,X). Our result com-
bines representation learning, distribution matching and re-weighting, resulting in a tighter bound
2Notions of causal effects exist also for the non-binary case, but these are not considered here.
3This is sufficient but not necessary.
4
Under review as a conference paper at ICLR 2018
than the closest related work, Shalit et al. (2017). The predictors we consider are compositions
f (x, t) = h(Φ(x), t) where Φ is a representation of x and h an hypothesis. We first give an up-
per bound on the risk in the general design shift setting, then show how this result can be used to
bound the error in prediction of treatment effects. In Section 5 we give a result about the asymptotic
properties of the minimizers of this upper bound.
Risk under distributional shift Our bounds on the risk under a target design capture the intuition
that if either a) the target design ∏ and source design μ are close, or b) the true outcome is a simple
function of x and t, the gap between the target risk and the re-weighted source risk is small. These
notions can be formalized using integral probability metrics (IPM) (Sriperumbudur et al., 2009) that
measure distance between distributions w.r.t. a normed vector space of functions H.
Definition 2. The integral probability metric (IPM) distance, associated with a normed vector space
of functions H, between distributions p and q is, IPMH(p, q) := suph∈H |Ep[h] - Eq [h]|.
Important examples of IPMs include the Wasserstein distance, for which H is the family of functions
with Lipschitz constant at most 1, and the Maximum Mean Discrepancy for which H are functions
in the norm-1 ball in a reproducing kernel Hilbert space. Using definitions 1-2, and the definition of
re-weighted risk, see (2), we can state the following result (see Appendix A.2 for a proof).
Lemma 1. For hypotheses f with loss 'f such that 'f/k'f ∣∣h ∈ H, and Pμ,p∏ with common
support, there exists a valid re-weighting W of pμ, see Definition 1, such that,
Rn(f) ≤ Rw(f) + k'f ∣hIPMh(p∏,pW) ≤ Rμ(f) + k'f ∣hIPMh(p∏,Pμ).	(5)
The first inequality is tight for importance sampling weights, w(x,t) = p∏ (x,t)∕pμ(x,t). The
second inequality is not tightfor general f, even if 'f/∣'f ∣∣h ∈ H, unlessp∏ = pμ.
The bound of Lemma 1 is tighter if pμ and p∏ are close (the IPM is smaller), and if the loss lives
in a small family of functions H (the supremum is taken over a smaller set). Lemma 1 also implies
that there exist weighting functions w(x, t) that achieve a tighter bound than the uniform weighting
w(x, t) = 1, implicitly used by Shalit et al. (2017). While importance sampling weights result
in a tight bound in expectation, neither the design densities nor their ratio are known in general.
Moreover, exact importance weights often result in large variance in finite samples (Cortes et al.,
2010). Here, we will search for a weighting function w, that minimizes a finite-sample version of
(5), trading off bias and variance. We examine the empirical value of this idea alone in Section 6.1.
We now introduce the notion of representation learning to combat distributional shift.
Representation learning The idea of learning representations that reduce distributional shift
in the induced space, and thus the source-target error gap, has been applied in domain adapta-
tion (Ajakan et al., 2014), algorithmic fairness (Zemel et al., 2013) and counterfactual predic-
tion (Shalit et al., 2017). The hope of these approaches is to learn predictors that predominantly
exploit information that is common to both source and target distributions. For example, a face
detector should be able to recognize the structure of human features even under highly variable en-
vironment conditions, by ignoring background, lighting etc. We argue that re-weighting (e.g. impor-
tance sampling) should also only be done with respect to features that are predictive of the outcome.
Hence, in Section 5, we propose using re-weightings that are functions of learned representations.
We follow the setup of Shalit et al. (2017), and consider learning twice-differentiable, invertible
representations Φ : X → Z, where Z is the representation space, and Ψ : Z → X is the inverse
representation, such that Ψ(Φ(x)) = x for all x. LetE denote space of such representation functions.
For a design π, we let pπ,Φ (z, t) be the distribution induced by Φ over Z × T, with pπw,Φ(z, t) :=
P∏,φ(z,t)w(Ψ(z),t) its re-weighted form and pwφ its re-weighted empirical form, following our
previous notation. Finally, we let G ⊆ {h : Z × T → Y} denote a set of hypotheses h(Φ, t)
operating on the representation Φ and let F the space of all compositions, F = {f = h(Φ(x), t) :
h ∈ G, Φ ∈ E}. We can now relate the expected target risk Rπ(f) to the re-weighted empirical
ʌ...,
source risk Rw (f).
Theorem 1. Given is a labeled sample (xι,tι,yι),…，(xn,tn,yn) from pμ, and an unlabeled
sample (x1,t；),..., (xm,tm) from p∏, with corresponding empirical measures pμ and p∏. SUP-
pose that Φ is a twice-differentiable, invertible representation, that h(Φ, t) is an hypothesis, and
f =	h(Φ(x),t) ∈	F.	Define mt(x) =	Eγ[Y	| X = x,T =	t],	let	'h,φ(Ψ(z),t):二
5
Under review as a conference paper at ICLR 2018
L(h(z, t), mt(Ψ(z))) where L is the squared loss, L(y, y0) = (y - y0)2, and assume that there
exists a constant Bφ > 0 such that 'h,φ∕Bφ ∈H⊆{h : Z×T→ Y}, where H is a repro-
ducing kernel Hilbert space of a kernel, k such that k((z, t), (z, t)) < ∞. Finally, let w be a valid
re-weighting of pμ,φ. Then with probability at least 1 一 2δ,
R∏(f) ≤ Rw(f) + BΦIPMH(Pn◎pw^)+Vμ(w, `f)nn/8+dφ,h (√m + √n) + σY (6)
where CnF,δ is a function of the pseudo-dimension ofF, DmH,n,δ a function of the kernel norm ofH,
both only with logarithmic dependence on n and m, σY2 is the expected variance in Y , and
Vμ(w,'f) = max (，Ep“[w2(x,t)'f (x,t)],，Ep“[w2(x,t)'f(x,t)]).
A similar bound exists where H is the family of functions Lipschitz constant at most 1, and IPMH
the Wasserstein distance, but with worse sample complexity.
See Appendix A.2 for a proof of Theorem 1 that involves applying finite-sample generalization
bounds to Lemma 1, as well as moving to the space induced by the representation Φ.
Theorem 1 has several implications: non-identity feature representations, non-uniform sample
weights, and variance control of these weights can all contribute to a lower bound. Using uni-
form weights w(x, t) = 1 in (6), results in a bound similar to that of Shalit et al. (2017) and Long
et al. (2015). When π = μ, minimizing uniform-weight bounds results in biased hypotheses, even
in the asymptotical limit, as the IPM term does not vanish when the sample size increases. This is
an undesirable property, as even k-nearest-neighbor classifiers are consistent in the limit of infinite
samples. We consider minimizing (6) with respect to w, improving the tightness of the bound.
Theorem 1 indicates that even though importance sampling weights w* yield estimators with small
bias, they can suffer from high variance, as captured by the factor Vμ(w, 'f). The factor Bφ is
not known in general as it depends on the true outcome, and is determined by k`f kH as well as
the determinant of the Jacobian of Ψ, see Appendix A.2. Qualitatively, BΦ measures the joint
complexity of Φ and 'f and is sensitive to the scale of Φ——as the scale of Φ vanishes, Bφ blows
up. To prevent this in practice, we normalize Φ. As BΦ is unknown, Shalit et al. (2017) substituted
a hyperparameter α for BΦ, but discussed the difficulties of selecting its value without access to
counterfactual labels. In our experiments, we explore a heuristic for adaptively choosing α, based
on measures of complexity of the observed held-out loss as a function of the input. Finally, the
term CnF,δ follows from standard learning theory results (Cortes et al., 2010) and F, and DδΦ,H from
concentration results for estimating IPMs (Sriperumbudur et al., 2012), see Appendix A.2.
Theorem 1 is immediately applicable to the case of unsupervised domain adaptation in which there
is only a single potential outcome of interest, T = {0}. In this case, Pμ(T | X) = p∏ (T | X) = 1.
Conditional average treatment effects A simple argument shows that the error in predicting
the conditional average treatment effect, MSE(T) can be bounded by the sum of risks under the
constant treat-all and treat-none policies. As in Section 2.2, we consider the case of a fixed domain
p∏(X) = pμ(X) and binary treatment T = {0,1}. Let Rnt (f) denote the risk under the constant
policy πt such that ∀x ∈ X : pπt (T = t | X = x) = 1.
Proposition 1. We have with MSE(T) as in (4) and Rnt (f) the risk under the constant policy ∏t,
MSE(T) ≤ 2(R∏ι (f) + R∏o(f)) - 4σ2	⑺
where σ is such that ∀t ∈ T, x ∈ X, σY (x, t) ≥ σ and σY (x, t) is standard deviation of Y (t)
conditioned on X = x.
The proof involves the relaxed triangle inequality and the law of total probability. By Proposition 1,
we can apply Theorem 1 to Rn1 and Rn0 separately, to obtain a bound on MSE(T). For brevity, we
refrain from stating the full result, but emphasize that it follows from Theorem 1. In Section 6.2, we
evaluate our framework in treatment effect estimation, minimizing this bound.
5	Joint learning of representations and sample weights
Motivated by the theoretical insights of Section 4, we propose to jointly learn a representation Φ(x),
a re-weighting w(x, t) and an hypothesis h(Φ, t) by minimizing a bound on the risk under the target
6
Under review as a conference paper at ICLR 2018
design, see (6). This approach improves on previous work in two ways: it alleviates the bias of
Shalit et al. (2017) when sample sizes are large, see Section 4, and it increases the flexibility of the
balancing method of Gretton et al. (2009) by learning the representation to balance.
For notational brevity, We let Wi = w(χi,ti). Recall that Pw ① is the re-weighted empirical distribu-
tion of representations Φ under pπ. The training objective of our algorithm is the RHS of (6), with
hyperparameters β = (α, λh, λw) substituted for model (and representation) complexity terms,
Lπ(h, Φ, w; β)
1n
1X
n i=1
wi'h(φ(xi),ti) +---7=
n
{^^^^^^^^^^^™
Ln 5,φ,wiD,α,λQ
R(h) + α IPMG(Pn,Φ,pw,φ) + λw
,、---------------{--------
'	Lw (®w；D,a6w )
kwk2
(8)
where R(h) is a regularizer of h, such as `2 -regularization. We can show the following result.
Theorem 2. Suppose H is a reproducing kernel Hilbert space given by a bounded kernel. Suppose
weak overlap holds in that E[(p∏(x, t)∕pμ(x, t))2] < ∞. Then,
min L∏(h, Φ, w; β) ≤ min R∏(f) + Op(1∕√n + 1∕√m).
h,Φ,w	f∈F
Consequently, under the assumptions of Thm. 1, for sufficiently large α and λw,
R∏(fn) ≤ minR∏(f) + Op(1∕n3/6 * 8 + 1∕√m).
f∈F
In words, the minimizers of (8) converge to the representation and hypothesis that minimize the
counterfactual risk, in the limit of infinite samples.
Implementation Minimization of Lπ (h, Φ, w; β) over h, Φ and w is, while motivated by Theo-
rem 2, a difficult optimization problem to solve in practice. For example, adjusting w to minimize
the empirical risk term may result in overemphasizing “easy” training examples, resulting in a poor
local minimum. Perhaps more importantly, ensuring invertibility of Φ is challenging for many rep-
resentation learning frameworks, such as deep neural networks. In our implementation, we deviate
from theory on these points, by fitting the re-weighting w based only on imbalance and variance
terms, and don’t explicitly enforce invertibility. As a heuristic, we split the objective, see (8), in two
and use only the IPM term and regularizer to learn w . In short, we adopt the following alternating
procedure.
hk, Φk = arg min Lπh (h, Φ, w; D, α, λh), wk+1 = arg min Lπw (Φk, w; D, α, λw)	(9)
h,Φ	w
The re-weighting function w(x, t) could be represented by one free parameter per training point, as
it is only used to learn the model, not for prediction. However, we propose to let w be a parametric
function of Φ(x). Doing so ensures that information predictive of the outcome is used for balancing,
and lets us compute weights on a hold-out set, to perform early stopping or select hyperparameters.
This is not possible with existing re-weighting methods such as Gretton et al. (2009); Kallus (2016).
An example architecture for the treatment effect estimation setting is presented in Figure 1. By
Proposition 1, estimating treatment effects involves predicting under the two constant policies—
treat-everyone and treat-no-one. In Section 6, we evaluate our method in this task.
As noted by Shalit et al. (2017), choosing hyperparameters for counterfactual prediction is fun-
damentally difficult, as we cannot observe ground truth for counterfactuals. In this work, we ex-
plore setting the balance parameter α adaptively. α is used in (8) in place of BΦ, a factor mea-
suring the complexity of the loss and representation function as functions of the input, a quantity
that changes during training. As a heuristic, we use an approximation of the Lipschitz constant
of 'f, with f = h(Φ(x),t), based on observed examples: α%,φ = maxi,j∈[n] |'f (xi,ti,yi)-
' f(χj,tj,yj) | / k Xi - Xjk 2. We use a moving average over batches to improve stability.
6 Experiments
6.1 Synthetic experiments for domain adaptation
We create a synthetic domain adaptation experiment to highlight the benefit of using a learned re-
weighting function to minimize weighted risk over using importance sampling weights w*(x)=
7
Under review as a conference paper at ICLR 2018
Context Repres. Hypothesis Weighted risk Treatment
Weighting	Imbalance
Figure 1: Architecture for predicting out-
comes under design shift. A re-weighting
function w is fit jointly with a representa-
tion Φ and hypothesis h of the potential out-
comes, to minimize a bound on the target
risk. Dashed lines are not back-propagated
through. Regularization penalties not shown.
Table 1: Causal effect estimation on IHDP. CATE
error RMSE(τ), target prediction error R∏ (f) and
std errors. Lower is better.
	RMSE(T)	Rn (f)
OLS	2.3 ± .11	1.1 ±.05
OLS-IPW	2.4 ±.11	1.2 ±.05
Random For.	6.6 ±.30	4.1 ± .18
Causal For.	3.8 ±.18	1.8 ±.08
BART	2.3 ±.10	1.7 ±.07
IPM-WNN	1.2 ±.12	.65 ±.02
CFRW	.76 ± .02	.46± .01
RCFR Oracle α, w = 1	.81 ± .07	.47± .03
RCFR Oracle α	.65 ±.04	.38 ±.01
RCFR Adapt. α	.67 ± .05	.37 ±.01
p∏(χ)∕pμ(χ) for small sample sizes. We observe n labeled source samples, distributed according
to Pμ(χ) = N(x; mμ, Id) and predict for n unlabeled target samples drawn according to p∏(x)=
N(x; m∏, Id) where Id is the d-dimensional identity matrix, m* = b1d∕2, m∏ = -b1d∕2 and
1d is the d-dimensional vector of all 1:s, here with b = 1, d = 10. We let β 〜N(0d, 1.5Id)
and C 〜 N(0,1) and let y = σ(β>x + C) where σ(z) = 1/(1 + e-z). Importance sampling
weights, w*(x) = p∏(x)∕pμ(x), are known. In experiments, We vary n from 10 to 600. We fit
(misspecified) linear models4 f(x) = β>x + γ to the logistic outcome, and compare minimizing a
weighted source risk by a) parameterizing sample weights as a small feed-forward neural network to
minimize (8) (ours) b) using importance sampling weights (baseline), both using gradient descent.
For our method, we add a small variance penalty, λw = 10-3, to the learned weights, use MMD
with an RBF-kernel of σ = 1.0 as IPM, and let α = 10. We compare to exact importance sampling
weights (IS) as well as clipped IS weights (ISC), wM (x) = min(w(x), M) for M ∈ {5, 10}, a
common way of reducing variance of re-weighting methods (Swaminathan & Joachims, 2015).
In Figure 2a, we see that our proposed method behaves well at small sample sizes compared to
importance sampling methods. The poor performance of exact IS weights is expected at smaller
samples, as single samples are given very large weight, resulting in hypotheses that are highly sen-
sitive to the training set. While clipped weights alleviates this issue, they do not preserve relevance
ordering of high-weight samples, as many are given the truncation value M, in contrast to the re-
weighting learned by our method. True domain densities are known only to IS methods.
6.2 Conditional average treatment effects — IHDP
We evaluate our framework in the CATE estimation setting, see Section 2.2. Our task is to predict
the expected difference between potential outcomes conditioned on pre-treatment variables, for a
held-out sample of the population. We compare our results to ordinary least squares (OLS) (with
one regressor per outcome), OLS-IPW (re-weighted OLS according to a logistic regression estimate
of propensities), Random Forests, Causal Forests (Wager & Athey, 2017), BART (Chipman et al.,
2010), and CFRW (Shalit et al., 2017) (with Wasserstein penalty). Finally, we use as baseline (IPM-
WNN): first weights are found by IPM minimization in the input space (Gretton et al., 2009; Kallus,
2016), then used in a re-weighted neural net regression, with the same architecture as our method.
Our implementation, dubbed RCFR for Re-weighted CounterFactual Regression, parameterizes
representations Φ(x), weighting functions w(Φ, t) and hypotheses h(Φ, t) using neural networks,
trained by minimizing (8). We use the RBF-kernel maximum mean discrepancy as the IPM (Gret-
ton et al., 2012). For a description of the architecture, training procedure and hyperparameters, see
Appendix B. We compare results using uniform w = 1 and learned weights, setting the balance
parameter α either fixed, by an oracle (test-set error), or adaptively using the heuristic described in
Section 5. To pick other hyperparameters, we split training sets into one part used for function fitting
and one used for early stopping and hyperparameter selection. Hyperparameters for regularization
are chosen based on the empirical loss on a held-out source (factual) sample.
4The identity representation Φ(x) = x is used for both approaches.
8
Under review as a conference paper at ICLR 2018
3 2
田SW茗J国
0∙
101
Number of samples
(a) Target prediction error on synthetic domain
adaptation experiment, comparing learned re-
weighting (RCFR) and exact/clipped importance
sampling weights (IS/ISC). The high variance of
IS hurts performance for small sample sizes.
6 4 2 0 8
. . . . .
1 1 1 1 O
(fc田S≡H 二OJ由田Iy°
a a a a a a
二.
=0.0001
=0.01
=1
= 100
= 100(
10-4
Re-weighting regularization λw (uniformity)
(b) For small imbalance penalties α, re-weighting
(low λw) has no effect. For moderate α, less uni-
form re-weighting (smaller λw) improves the error,
c) for large α, weighting helps, but overall error in-
creases. Best viewed in color.
Figure 2: Results for domain adaptation and causal effect estimation experiments.
The Infant Health and Development Program (IHDP) dataset is a semi-synthetic binary-treatment
benchmark (Hill, 2011), split into training and test sets by Shalit et al. (2017). IHDP has a set of
d = 25 real-world continuous and binary features describing n = 747 children and their mothers,
a real-world binary treatment made non-randomized through biased subsampling by Hill (2011),
and a synthesized continuous outcome that can be used to compute the ground-truth CATE error.
Average results over 100 different realizations/settings of the outcome are presented in Table 1.
We see that our proposed method achieves state-of-the-art results, and that adaptively choosing α
does not hurt performance much. Furthermore, we see a substantial improvement from using non-
uniform sample weights. In Figure 2b we take a closer look at the behavior of our model as we
vary its hyperparameters on the IHDP dataset. Between the two plots we can draw the following
conclusions: a) For moderate to large α ∈ [10, 100], we observe a marginal gain from using the IPM
penalty. This is consistent with the observations of Shalit et al. (2017). b) For large α ∈ [10, 1000],
we see a large gain from using a non-uniform re-weighting (small λw ). c) While large α makes the
factual error more representative of the counterfactual error, using it without re-weighting results in
higher absolute error. We believe that the moderate sample size of this dataset is one of the reasons
for the usefulness of our method. See Appendix C.2 for a complementary view of these results.
7 Discussion
We have proposed a theory and an algorithmic framework for learning to predict outcomes of in-
terventions under shifts in design—changes in both intervention policy and feature domain. The
framework combines representation learning and sample re-weighting to balance source and tar-
get designs, emphasizing information from the source sample relevant for the target. Existing re-
weighting methods either use pre-defined weights or learn weights based on a measure of distribu-
tional distance in the input space. These approaches are highly sensitive to the choice of metric used
to measure balance, as the input may be high-dimensional and contain information that is not pre-
dictive of the outcome. In contrast, by learning weights to achieve balance in representation space,
we base our re-weighting only on information that is predictive of the outcome. In this work, we
apply this framework to causal effect estimation, but emphasize that joint representation learning
and re-weighting is a general idea that could be applied in many applications with design shift.
Our work suggests that distributional shift should be measured and adjusted for in a representa-
tion space relevant to the task at hand. Joint learning of this space and the associated re-weighting
is attractive, but several challenges remain, including optimization of the full objective and relax-
ing the invertibility constraint on representations. For example, variable selection methods are not
covered by our current theory, as they induce a non-ivertible representation, but a similar intuition
holds there—only predictive attributes should be used when measuring imbalance. We believe that
addressing these limitations is a fruitful path forward for future work.
9
Under review as a conference paper at ICLR 2018
References
Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois Laviolette, and Mario Marchand.
Domain-adversarial neural networks. arXiv preprint arXiv:1412.4446, 2014.
Susan Athey and Guido Imbens. Recursive partitioning for heterogeneous causal effects. Proceed-
ings of the National Academy of Sciences ,113(27):7353-7360, 2016.
Peter C Austin. An introduction to propensity score methods for reducing the effects of confounding
in observational studies. Multivariate behavioral research, 46(3):399-424, 2011.
Alexandre Belloni, Victor Chernozhukov, and Christian Hansen. Inference on treatment effects after
selection among high-dimensional controls. The Review of Economic Studies, 81(2):608-650,
2014.
Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney
Newey, James Robins, et al. Double/debiased machine learning for treatment and causal parame-
ters. Technical report, 2017.
Hugh A Chipman, Edward I George, Robert E McCulloch, et al. Bart: Bayesian additive regression
trees. The Annals of Applied Statistics, 4(1):266-298, 2010.
Corinna Cortes, Yishay Mansour, and Mehryar Mohri. Learning bounds for importance weighting.
In Advances in neural information processing systems, pp. 442-450, 2010.
David A Freedman and Richard A Berk. Weighting regressions by propensity scores. Evaluation
Review, 32(4):392-409, 2008.
Arthur Gretton, Alexander J Smola, Jiayuan Huang, Marcel Schmittfull, Karsten M Borgwardt, and
Bernhard Scholkopf. Covariate shift by kernel mean matching. 2009.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scholkopf, and Alexander Smola.
A kernel two-sample test. Journal of Machine Learning Research, 13(Mar):723-773, 2012.
Jennifer L Hill. Bayesian nonparametric modeling for causal inference. Journal of Computational
and Graphical Statistics, 20(1), 2011.
Guido W Imbens and Donald B Rubin. Causal inference in statistics, social, and biomedical sci-
ences. Cambridge University Press, 2015.
Fredrik Johansson, Uri Shalit, and David Sontag. Learning representations for counterfactual infer-
ence. In International Conference on Machine Learning, pp. 3020-3029, 2016.
Nathan Kallus. Generalized optimal matching methods for causal inference. arXiv preprint
arXiv:1612.08321, 2016.
Nathan Kallus. Optimal a priori balance in the design of controlled experiments. Journal of the
Royal Statistical Society: Series B (Statistical Methodology), 2017. doi: 10.1111/rssb.12240.
Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features with
deep adaptation networks. In International Conference on Machine Learning, pp. 97-105, 2015.
Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds
and algorithms. arXiv preprint arXiv:0902.3430, 2009.
Stephen L Morgan and Christopher Winship. Counterfactuals and causal inference. Cambridge
University Press, 2014.
Judea Pearl. Causality. Cambridge university press, 2009.
Doina Precup, Richard S Sutton, and Sanjoy Dasgupta. Off-policy temporal-difference learning
with function approximation. In ICML, pp. 417-424, 2001.
Paul R Rosenbaum and Donald B Rubin. The central role of the propensity score in observational
studies for causal effects. Biometrika, 70(1):41-55, 1983.
10
Under review as a conference paper at ICLR 2018
Uri Shalit, Fredrik Johansson, and David Sontag. Estimating individual treatment effect: general-
ization bounds and algorithms. In International Conference on Machine Learning, 2017.
Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-
likelihood function. Journal of statistical planning and inference, 90(2):227-244, 2000.
Bharath K SriPerUmbUdur, Kenji Fukumizu, Arthur Gretton, Bernhard Scholkopf, and Gert RG
Lanckriet. On integral probability metrics,\phi-divergences and binary classification. arXiv
preprint arXiv:0901.2698, 2009.
Bharath K Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Scholkopf, Gert RG Lanck-
riet, et al. On the empirical estimation of integral probability metrics. Electronic Journal of
Statistics, 6:1550-1599, 2012.
Elizabeth A Stuart. Matching methods for causal inference: A review anda look forward. Statistical
science: a review journal of the Institute of Mathematical Statistics, 25(1):1, 2010.
Adith Swaminathan and Thorsten Joachims. Counterfactual risk minimization: Learning from
logged bandit feedback. In International Conference on Machine Learning, pp. 814-823, 2015.
Stefan Wager and Susan Athey. Estimation and inference of heterogeneous treatment effects using
random forests. Journal of the American Statistical Association, (just-accepted), 2017.
Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations.
In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pp. 325-
333, 2013.
Jose R Zubizarreta. Stable weights that balance covariates for estimation with incomplete outcome
data. Journal of the American Statistical Association, 110(511):910-922, 2015.
11
Under review as a conference paper at ICLR 2018
Appendix
A Proofs
A.1 Definitions
Distribution re-weighting
Definition 1 (Restated). Afunction W : X ×T → R+ is a valid re-weighting of pμ if
Eχ,t〜Pμ[w(x,t)] = 1 and Pμ(x,t) > 0 ⇒ w(x,t) > 0.
We denote the re-weighted density Pw (x,t) := w(x,t)p*(x,t).
Expected & empirical risk We let the (expected) risk of f measured by 'h under pμ be denoted
Rμ(h) = Epμ [lh(x, t)]
where lh is an appropriate loss function, and the empirical risk over a sample Dμ =
{ (x1, t1, yI)…，(xn, tn, yn ) from pμ
1n
Rμ(f) = — f v If(Xi, ti, yi) .
n i=1
We use the superscript w to denote the re-weighted risks
1n
Rw (f) = E[w(χ,t)lf (χ,t)]	Rw (f) = — Ew(Xi,ti)lh(χi,ti,yi)
n i=1
Definition A1 (Importance sampling). For two distributions p, q on Z, of common support, ∀z ∈
Z : p(z) > 0 ^⇒ q(z) > 0, we call
wIS (z)
q(Z)
p(z)
the importance sampling weights ofp and q.
Definition 2 (Restated). The integral probability metric (IPM) distance, associated with the func-
tion family H, between distributions p and q is defined by
IPMH(p,q) :=	sup |Ep[h] - Eq[h]|
h:khkH=1
A.2 Learning bounds
We begin by bounding the expected risk under a distribution pπ in terms of the expected risk under
Pμ and a measure of the discrepancy between p∏ and pμ. Using definition 2 We can show the
following result.
Lemma 1 (Restated). For hypotheses f with loss 'f such that 'f∕k'f IIH ∈ H, and Pμ,p∏ with
common support, there exists a valid re-weighting w of pμ, see Definition 1, such that,
Rn(f) ≤ Rw(f) + k'f IhIPMh(p∏,Pw) ≤ Rμ(f) + k'f IhIPMh(p∏,Pμ).	(10)
The first inequality is tight for importance sampling weights, w(x, t) = Pn (x,t)∕pμ(x,t). The
SeCOnd inequality is not tightfor general f, even if 'f ∈ H, unless Pn = pμ.
Proof. The results follows immediately from the definition of IPM.
Rn (f) — Rw (f) = En ['f (X,t)] — Eμ[w(x,t)'f (X,t)]
≤ sup |En [h(x,t)] — Eμ[w(x,t)h(x,t)]∣
h∈H'
=IPMH` (Pn ,Pw )
12
Under review as a conference paper at ICLR 2018
Further, for importance sampling weights WIS(x, t) = [(；；：), for any h ∈ H,
E∏[h(x,t)] - Eμ[wIs(x,t)h(x,t)] = E∏ [h(x,t)] - Eμ["(t; x) h(x,t)]
μ^ (t; ^X)
=0
and the LHS is tight.
□
We could apply Lemma 1 to bound the loss under a distribution q based on the weighted loss under
p. Unfortunately, bounding the expected risk in terms of another expectation is not enough to reason
about generalization from an empirical sample. To do that we use Corollary 2 of Cortes et al. (2010),
restated as a Theorem below.
Theorem A1 (Generalization error of re-weighted loss (Cortes et al., 2010)). Fora loss function `h
of any hypothesis h ∈ H ⊆ {h0 : X → R}, such that d = Pdim ({'h : h ∈ H}) where Pdim is the
pseudo-dimension, and a weighting function w(x) such that Ep[w] = 1, with probability 1 - δ over
a sample (xι,…，xn), with empirical distribution P,
Rp(h) ≤ Rw(h) + 25/4Vp,p[w(x)l(x)]
d log ⅞e + log 4
n
3/8
with Vp,p[w(x)l(x)] = maχ(，Ep[w2(x)'h(x)],，Ep[w2(x)'h(x)]). With
CH = 25/4 (d log2ne +log 418
n	dδ
we get the simpler form
CH
Rw(h) ≤ Rw(h) + vp,^[w(x)i(x)]nn8 .
We will also need the following result about estimating IPMs from finite samples from Sriperum-
budur et al. (2009).
Theorem A2 (Estimation of IPMs from empirical samples (Sriperumbudur et al., 2009)). Let M be
a measurable space. Suppose k is measurable kernel such that supx∈M k(x, x) ≤ C ≤ ∞ and H
the reproducing kernel Hilbert space induced by k, with ν := supx∈M,f ∈H f(x) < ∞. Then, with
p, q the empirical distributions of p, q from m and n samples respectively, and with probability at
least 1 - δ,
∣IPMH(p,q) - IPMH(P,q)∣≤
118ν2 log δc
We consider learning twice-differentiable, invertible representations Φ : X → Z, where Z is the
representation space, and Ψ : Z → X is the inverse representation, such that Ψ(Φ(x)) = x for
all x. Let E denote space of such representation functions. For a design π, we let Pπ,Φ (z, t) be
the distribution induced by Φ over Z × T, with Pπw,Φ(z, t) := Pπ,Φ(z, t)w(Ψ(z), t) its re-weighted
form and Pw φ its re-weighted empirical form, following our previous notation. Note that We do
not include t in the representation itself, although this could be done in principle. Let G ⊆ {h :
Z × T → Y} denote a set of hypotheses h(Φ, t) operating on the representation Φ and let F denote
the space of all compositions, F = {f = h(Φ(x), t) : h ∈ G, Φ ∈ E}. We now restate and prove
Theorem 1.
Theorem 1 (Restated). Given is a labeled sample Dμ = {(xι,tι,yι),…，(xn,tn,yn)} from Pμ,
and an unlabeled sample Dπ = {(x01, t01), ..., (x0m, t0m)} from Pπ , with corresponding empirical
measures pμ andp∏. Suppose that Φ is a twice-differentiable, invertible representation, that h(Φ, t)
is an hypothesis, and f = h(Φ(x), t) ∈ F. Define mt(x) = EY [Y | X = x, T = t], let
'h,φ(Ψ(z), t) ：= L(h(z, t), mt(Ψ(z))) where L is the squared loss, L(y,y0) = (y — y0)2, and
assume that there exists a COnStant Bφ > 0 such that 'h,φ∕Bφ ∈H⊆{h : ZXT → Y}, where
13
Under review as a conference paper at ICLR 2018
H is a reproducing kernel Hilbert space of a kernel, k such that k((z, t), (z, t)) < ∞. Finally, let w
be a valid re-weighting of pμ,φ. Then with probability at least 1 一 2δ,
CFδ
Rn(f) ≤ Rw(f) + BφIPMh(P∏,φ,PW,φ) + Vμ(w,'f)nn⅛
+ σY2
(11)
where CnF,δ measures the capacity ofF and has only logarithmic dependence onn, DmH,n,δ measures
the capacity of H, σY2 is the expected variance in potential outcomes, and
Vμ(w, 'f) = max( JEpμ[w2(x,t)'f (x,t)], JEp”[w2(x,t)'f(x,t)]).
A similar bound exists where H is the family of functions Lipschitz constant at most 1, but with worse
sample complexity.
Proof. We have by definition
Rn(f) 一 Rw(f) = En['f(χ,t,y)] — Eμ[w(χ,t)'f(χ,t,y)]
`f (x, t, y)p(y |
t,x)(p∏(x,t) — Pw (x, t))dxdtdy
Define 'h,φ(x, t) = L(h(Φ(x), t), mt(x)) where mt(x) := E[Y | T = t,X = x]). Then, with L,
the squared loss, L(y, y0) = (y 一 y0)2, we have,
En ['h,φ(x,t,y)] = En ['h,φ(x,t)] + σ∏
where σ2 = Epπ [(Y — mt(x))2], and analogously for μ. We get that
Rn (f) — Rw (f) = [	'h,φ(x,t)(Pn (x,t) — Pw (x,t))dxdt + σ + σμ
x∈X,t∈T
=I	'h,φ(Ψ(z),t)(Pn,φ(z,t) — pμwφ(z,t))∣Jψ(z)∣dzdt + σ + σμ
z∈Z,t∈T
≤ Aφ /	'h,φ(Ψ(z),t)(pn(z,t) — pμw(z,t))dzdt + σ + σμ
z∈Z,t∈T
≤ σ2 + σ2 + Aφk'h,φ∣∣H sup I /	h(Ψ(z),t) (pπφ(rz,t) — Pwφ(z,t)) dzdt
h∈H zt∈∈TZ	,
=bφ TPMH (p∏,φ,pw,φ)+σ2+σμ
where Jψ(z) is the Jacobian matrix of Ψ evaluated at Z and Aφ ≥ | Jψ(z)∣ for all Z ∈ Z, where | J|
is the absolute determinant of J. By application of Theorem A1 we have with probability at least
1 — δ,
CHδ
RW(f) ≤ RW(f) + Vμ(w,'nnδ .
and by applying Theorem A2, we have with probability at least 1 — δ,
IIPMH(Pn,φ,pW,φ) — IPMH(Pn,φ,PW,φ)∣ ≤ y18ν2 log；。
We let σY2 = σ 2 + σ 2 and
Y n μ
-1	4Γ
18ν2 log-C
δ
Combining these results, observing that (1 — δ)2 ≥ 1 — 2δ, we obtain the desired result.	□
14
Under review as a conference paper at ICLR 2018
A.3 Asymptotics
Theorem 2 (Restated). Suppose H isa reproducing kernel Hilbert space given bya bounded kernel.
Suppose weak overlap holds in that E[(p∏ (x,t)∕pμ(x,t))2] < ∞. Then,
min L∏(h, Φ, w; β)] ≤ min Rn (f) + O(1∕√n + 1∕√m).
h,Φ,w	f∈F
Proof. Let f = Φ* ◦ h ∈ arg minf∈f R∏(f) and let w*(x, t) = p∏,φ(Φ*(x),t)∕pμ,φ(Φ*(x), t).
Since minh,φ,w L∏(h, Φ, w; β) ≤ L∏(h*, Φ*,w*; β), it suffices to show that L∏(h*, Φ*,w*; β)=
Rn(f *) + O(1∕√n + 1∕√m). We will work term by term:
1n
L∏(h*, Φ*,w*; β) = -]T
wi'h (φ(xi")+λh Rf+α IPMG(如pφk)+λw 号.
济 © 击
}
For term ©,letting Wi = w* (χi,ti), we have that by weak overlap
© 2=n × n X(Wi)2=Op(1/n),
i=1
so that (D = Op(1∕√n). For term ®, under ignorability, each term in the sum in the first term
has expectation equal to Rn(fi) and so, so by weak overlap and bounded second moments of loss,
We have (A = Rn(f *) + Op(1∕√n). For term ⑻，since h* is fixed We have deterministically that
(B =OQ/√n).
Finally, We address term ©, which when expanded can be written as
m
sup (一 X h(Φ*(xi),ti) —
khk≤1 m i=1
-n
n EWih(Φ*(χi),ti)).
i=1
Let x0i0, t0i0 for i = -, . . . , m and x0i00, t0i00 for i = -, . . . , n be new iid replicates of x01 , t01 , i.e., new
ghost samples drawn from the target design. By Jensen’s inequality,
mn
E©2] = E[ sup (— X h(Φ*(xi),ti) - - X w*h(Φ*(xi),ti))2]
khk≤1 mi=1	ni=1
-m
=E[ sup (— X(h(Φ*(χi),ti) - E[h(Φ*(χi0),ti0)])
khk≤1 m i=1
-n
- ~J2(w*h(Φ*(xi),ti) - E[h(Φ*(xi,,),ti,,)]))2]
m
≤ E[ sup (— X(h(Φ*(χi),ti) - h(Φ*(xi0),ti0))
khk≤1 m i=1	i i	i i
-n
--E(w*h(Φ*(xi),ti) - h(Φ*(xi00),ti00)))2]
n i=1
m
≤ 2E[ sup (— X(h(Φ*(xi),ti) - h(Φ*(xi,),ti,)))2]
khk≤1 m i=1
-n
+ 2E[ sup (-]T(wih(Φ*(xi),ti) - h(Φ*(χi00),ti00)))2]
khk≤1 n i=1
Let ξi(h) = h(Φi(x0i), t0i) - h(Φi(X0iq) and let ζi(h) = wiih(Φi(xi),ti) - h(Φi(x0i00),t0i00). Note
that for every h, E[ζi (h)] = E[ξi (h)] = 0. Moreover, E[kζi k2] ≤ 4E[K(Φi (x0i), t0i, Φi (x0i), t0i)] ≤
15
Under review as a conference paper at ICLR 2018
M. Similarly, E[∣∣ξik2] ≤ 2E[(w*)2]M + 2M ≤ M0 < ∞ because of weak overlap. Let Z0 for
i = 1, . . . , n be iid replicates of ζi (ghost sample) and let i be iid Rademacher random variables.
Because H is a Hilbert space, we have that supkhk≤1(A(h))2 = kAk2 = hA, Ai. Therefore, by
Jensen’s inequality,
1n
E[ sup (— E(Wih(Φ*E),ti) - h(Φ*(χi00),ti00)))2]
khk≤1 n i=1
1n
=E[ sup (— EZi(h))2]
khk≤1 n i=1
1n
=E[ sup (-∑(Zi(h) — E[Zi(h)]))2]
khk≤1 n i=1
-n
≤ E[ sup (-∑(Zi(h) — Z0(h)))2]
khk≤1 n i=1
-n
=E[ sup (— ∑>(Zi(h)-Z0(h)))2]
khk≤1 n i=1
≤ ~EE[ SuP (X GZi(h))2]
n2 khk≤1 i=1
n
=nE[k XCik2]
i=1
4n
=n E[ X ij hζi, ζji]
i,j=1
=n42 E[χx kZik2]
n
i=1
n
=n X E[kZik2]
n i=1
4M0
≤ —
n
An analogous argument can be made of ξi,s, showing that E[©2] = O(l∕n) and hence © =
O(1 /√n) by Markov,s inequality.	□
B	Implementation
We implemented all neural network models (IPM-WNN, RCFR) in TensorFlow as feed-forward
fully-connected networks with ELU activations. All architectures have a representation with two
hidden layers of 32 and 16 hidden units, and hypotheses (one for each outcome) of 1 layer of 16
hidden units. The networks were trained using stochastic gradient descent with the ADAM optimizer
with a learning rate of-0-3. The batch size was 128. Representations were normalized by dividing
by the norm. Weight functions were implemented as 2 hidden layers of 32 units each, as functions
of the representation Φ. σ in the RBF kernel was set to 1.0. λw was set to 0.1 and λh to-0-4.
C Experiments
C.1 Synthetic
We use a two-layer MLP with ELU units and layer sizes 10, 10 as parameterization of the sample
weights. Weights are normalized by dividing by the mean.
16
Under review as a conference paper at ICLR 2018
08
α =0.0001
α =0.01
α =1
α =10
α =100
10-
10-0	102	∞
Re-weighting regularization λw (uniformity)
12
1O
≡田 S≡H 二 0JaM⅛o
∞
α
三三
50505050
33221100
. . . . . . . .
¾^∕(田SWU 匕BPSmjeJJuno
Figure 3: Error in CATE estimation on IHDP as a function of re-weighting regularization strength
λw (left) and source prediction error (right). We see in the left-hand plot that a) for small imbalance
penalties α, re-weighting (low λw) has no effect, b) for moderate α, less uniform re-weighting
(smaller λw) improves the error, c) for large α, weighting helps, but overall error increases. In the
right-hand plot, we compare the ratio of CATE error to source error. Color represents α (see left)
and size λw. We see that for large α, the source error is more representative of CATE error, but does
not improve in absolute value without weighting. Here, α was fixed. Best viewed in color.
C.2 IHDP
In Figure 3, we see two different views of the IHDP results.
17