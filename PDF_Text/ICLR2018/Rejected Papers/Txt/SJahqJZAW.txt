Under review as a conference paper at ICLR 2018
Stabilizing GAN Training with
Multiple Random Projections
Anonymous authors
Paper under double-blind review
Abstract
Training generative adversarial networks is unstable in high-dimensions as
the true data distribution tends to be concentrated in a small fraction of the
ambient space. The discriminator is then quickly able to classify nearly all
generated samples as fake, leaving the generator without meaningful gra-
dients and causing it to deteriorate after a point in training. In this work,
we propose training a single generator simultaneously against an array of
discriminators, each of which looks at a different random low-dimensional
projection of the data. Individual discriminators, now provided with re-
stricted views of the input, are unable to reject generated samples perfectly
and continue to provide meaningful gradients to the generator throughout
training. Meanwhile, the generator learns to produce samples consistent
with the full data distribution to satisfy all discriminators simultaneously.
We demonstrate the practical utility of this approach experimentally, and
show that it is able to produce image samples with higher quality than
traditional training with a single discriminator.
1	Introduction
Generative adversarial networks (GANs), introduced by Goodfellow et al. (2014), endow
neural networks with the ability to express distributional outputs. The framework includes
a generator network that is tasked with producing samples from some target distribution,
given as input a (typically low dimensional) noise vector drawn from a simple known dis-
tribution, and possibly conditional side information. The generator learns to generate such
samples, not by directly looking at the data, but through adversarial training with a discrim-
inator network that seeks to differentiate real data from those generated by the generator.
To satisfy the ob jective of “fooling” the discriminator, the generator eventually learns to
produce samples with statistics that match those of real data.
In regression tasks where the true output is ambiguous, GANs provide a means to simply
produce an output that is plausible (with a single sample), or to explicitly model that ambi-
guity (through multiple samples). In the latter case, they provide an attractive alternative to
fitting distributions to parametric forms during training, and employing expensive sampling
techniques at the test time. In particular, conditional variants of GANs have shown to be
useful for tasks such as in-painting (Denton et al., 2016), and super-resolution (Ledig et al.,
2016). Recently, Isola et al. (2016) demonstrated that GANs can be used to produce plau-
sible mappings between a variety of domains—including sketches and photographs, maps
and aerial views, segmentation masks and images, etc. GANs have also found uses as a
means of un-supervised learning, with latent noise vectors and hidden-layer activations of
the discriminators proving to be useful features for various tasks (Denton et al., 2016; Chen
et al., 2016; Radford et al., 2016).
Despite their success, training GANs to generate high-dimensional data (such as large im-
ages) is challenging. Adversarial training between the generator and discriminator involves
optimizing a min-max objective. This is typically carried out by gradient-based updates
to both networks, and the generator is prone to divergence and mode-collapse as the dis-
criminator begins to successfully distinguish real data from generated samples with high
confidence. Researchers have tried to address this instability and train better generators
1
Under review as a conference paper at ICLR 2018
Figure 1: Overview of our approach. We train a single generator against an array of
discriminators, each of which receives lower-dimensional projections—chosen randomly prior
to training—as input. Individually, these discriminators are unable to perfectly separate
real and generated samples, and thus provide stable gradients to the generator throughout
training. In turn, by trying to fool all the discriminators simultaneously, the generator
learns to match the true full data distribution.
through several techniques. Denton et al. (2015) proposed generating an image by explic-
itly factorizing the task into a sequence of conditional generations of levels of a Laplacian
pyramid, while Radford et al. (2016) demonstrated that specific architecture choices and
parameter settings led to higher-quality samples. Other techniques include providing addi-
tional supervision (Salimans et al., 2016), adding noise to the discriminator input (Arjovsky
& Bottou, 2016), as well as modifying or adding regularization to the training ob jective
functions (Nowozin et al., 2016; Arjovsky et al., 2017; Zhao et al., 2016).
We propose a different approach to address this instability, where the generator is trained
against an array of discriminators, each of which looks at a different, randomly-chosen,
low-dimensional projection of the data. Each discriminator is unable to perfectly separate
real and generated samples since it only gets a partial view of these samples. At the same
time, to satisfy its ob jective of fooling all discriminators, the generator learns to match the
true full data distribution. We describe a realization of this approach for training image
generators, and discuss the intuition behind why we expect this training to be stable—i.e.,
the gradients to the generator to be meaningful throughout training—and consistent—i.e.,
for the generator to learn to match the full real data distribution. Despite its simplicity,
we find this approach to be surprisingly effective in practice. We demonstrate this efficacy
by using it to train generators on standard image datasets, and find that these produce
higher-quality samples than generators trained against a single discriminator.
1.1	Related Work
Researchers have explored several approaches to improve the stability of GANs for train-
ing on higher dimensional images. Instead of optimizing Jensen-Shannon divergence as
suggested in the original GAN framework, Energy based GAN (Zhao et al., 2016) and
Wasserstein GAN (Arjovsky et al., 2017) show improvement in the stability by optimizing
total variation distance and Earth mover distance respectively, together with regularizing
the discriminator to limit the discriminator capacity. Nowozin et al. (2016) further extended
GAN training to any choice of f -divergence as objective. Salimans et al. (2016) proposed
several heuristics to improve the stability of training. These include modifying the ob jective
function, virtual batch-normalization, historical averaging of parameters, semi-supervised
learning, etc.. All of these methods are designed to improve the quality of gradients, and
provide additional supervision to the generator. They are therefore complementary to, and
can likely be used in combination with, our approach.
2
Under review as a conference paper at ICLR 2018
Note that prior methods have involved training ensembles of GANs (Wang et al., 2016), or
ensembles of discriminators (Durugkar et al., 2016). However, their goal is different from
ours. In our framework, each discriminator is shown only a low-dimensional pro jection of the
data, with the goal of preventing it from being able to perfectly reject generated samples.
Crucially, we do not combine the outputs of discriminators directly, but rather compute
losses on individual discriminators and average these losses.
Indeed, the idea of combining multiple simple classifiers, e.g., with boosting (Freund et al.,
1996), or mixture-of-experts Jacobs (1995), has a rich history in machine learning, including
recent work where the simple classifiers act on random lower-dimensional projections of their
inputs (Cannings & Samworth, 2017) (additionally benefiting from the regularization effect
of such projections (Durrant & Kaban, 2015)). While these combinations have been aimed
at achieving high classification accuracy (i.e., achieve accurate discrimination), our ob jective
is to maintain a flow of gradients from individual discriminators to the generator. It is also
worth noting here the work of Bonneel et al. (2015), who also use low-dimensional pro jec-
tions to deal with high-dimensional probability distributions. They use such projections to
efficiently compute Wasserstein distances and optimal transport between two distributions,
while we seek to enable stable GAN training in high dimensions.
2	Training with Multiple Random Projections
GANs (Goodfellow et al., 2014) traditionally comprise of a generator G that learns to
generate samples from a data distribution Px , through adversarial training against a single
discriminator D as:
min max V(DG= Ex〜p, [log D(x)]+ Ez〜Pz[log(1 - D(G(Z)))],	⑴
for X ∈ Rd, Px : Rd → R+, J Px = 1, and with Pz a fixed distribution (typically uniform
or Gaussian) for noise vectors Z ∈ Rd ,d' ≪ d. The optimization in (1) is carried out
using stochastic gradient descent (SGD), with alternating updates to the generator and the
discriminator. While this approach works surprisingly well, instability is common during
training, especially when generating high-dimensional data.
Theoretically, the optimal stationary point for (1) is one where the generator matches the
data distribution perfectly, and the discriminator is forced to always output 0.5 (Goodfellow
et al., 2014). But usually in practice, the discriminator tends to “win” and the cost in (1)
saturates at a high value. While the loss keeps increasing, the gradients received by the
generator are informative during the early stages of training. However, once the loss reaches
a high enough value, the gradients are dominated by noise, at which point generator quality
stops improving and in fact begins to deteriorate. Training must therefore be stopped early
by manually inspecting sample quality, and this caps the number of iterations over which
the generator is able to improve.
Arjovsky & Bottou (2016) discuss one possible source of this instability, suggesting that
it is because natural data distributions Px often have very limited support in the ambient
domain of x and G(Z). Then, the generator G is unable to learn quickly enough to generate
samples from distributions that have significant overlap with this support. This in turn
makes it easier for the discriminator D to perfectly separate the generator’s samples, at
which point the latter is left without useful gradients for further training.
We propose to ameliorate this problem by training a single generator against an array of
discriminators. Each discriminator operates on a different low-dimensional linear projection,
that is set randomly prior to training. Formally, we train a generator G against multiple
discriminators {Dk }kK=1 as:
KK
min max EV(Dk,G) = EEx〜p。[log Dk(WTx)] + Ez〜Pz[log(1 - Dk(WTG(Z)))], (2)
G {Dk} i=k	i=k
where Wk, k ∈ {1, •…,K} is a randomly chosen matrix in Rd×m with m < d. Therefore,
instead of a single discriminator looking at the full input (be it real or fake) in (1), each of
3
Under review as a conference paper at ICLR 2018
the K discriminators in (2) sees a different low-dimensional pro jection. Each discriminator
tries to maximize its accuracy at detecting generated samples from its own projected version,
while the generator tries to ensure that each sample it generates simultaneously fools all
discriminators for all projected versions of that sample.
When the true data X and generated samples G(z) are images, prior work (Radford et al.,
2016) has shown that it is key that both the generator and discriminator have convolutional
architectures. While individual discriminators in our framework see projected inputs that
are lower-dimensional than the full image, their dimensionality is still large enough (a very
small m would require a large number of discriminators) to make it hard to train discrim-
inators with only fully-connected layers. Therefore, it is desirable to employ convolutional
architectures for each discriminator. To do so, the pro jection matrices WkT must be chosen
to produce “image-like” data. Accordingly, we use strided convolutions with random filters
to embody the projections WkT (as illustrated in Fig. 1). The elements of these convolution
filters are drawn i.i.d. from a Gaussian distribution, and the filters are then scaled to have
unit I2 norm. To promote more “mixing” of the input coordinates, We choose filter sizes
that are larger than the stride (e.g., we use 8 × 8 filters when using a stride of 2). While
still random, this essentially imposes a block-Toeplitz structure on WkT .
3	Motivation
In this section, We motivate our approach, and provide the reader With some intuition for Why
We expect our approach to improve the stability of training While maintaining consistency.
Stability. Each discriminators Dk in our frameWork in (2) Works With loW dimensional
projections of the data and can do no better than the traditional full discriminator D in (1).
Let y ∈ Rd and l ∈ {0, 1} denote the input and ideal output of the original discriminator
D , Where l = 1 if y is real, and 0 if generated. HoWever, each Dk is provided only a loWer
dimensional projection WkTy. Since WkTy is a deterministic and non-invertible function of
y, it can contain no more information regarding l than Y itself, i.e., I(l; Y ) ≥ I(l; WkTY ),
Where I(x; y) denotes the mutual information betWeen the random variables x and y. In
other Words, taking a loW-dimensional pro jection introduces an information bottleneck Which
interferes With the ability of the discriminator to determine the real/fake label l.
While strictly possible, it is intuitively unlikely that all the information required for clas-
sification is present in the WkTy (for all, or even most of the different WkT ), especially in
the adversarial setting. We already knoW that GAN training is far more stable in loWer
dimensions, and We expect that each of the discriminators Dk Will perform similarly to a
traditional discriminator training on loWer (i.e., m-) dimensional data. Moreover, Arjovsky
& Bottou (2016) suggest that instability in higher-dimensions is caused by the true distribu-
tion Px being concentrated in a small fraction of the ambient space—i.e., the support of Px
occupies a loW volume relative to the range of possible values of x. Again, We can intuitively
expect this to be ameliorated by performing a random loWer-dimensional projection (and
provide analysis for a simplistic Px in the supplementary).
When the discriminators Dk are not perfectly saturated, Dk (WkT G(z)) Will have some varia-
tion in the neighborhood of a generated sample G(z), Which can provide meaningful gradients
to the generator. But note that gradients from each individual discriminator Dk to G(z)
Will lie entirely in the loWer dimensional sub-space spanned by WkT .
Consistency. While impeding the discriminator’s ability benefits stability, it could also
have trivially been achieved by other means—e.g., by severely limiting its architecture or
excessive regularization. HoWever, these Would also limit the ability of the discriminator to
encode the true data distribution and pass it on to the generator. We begin by considering
the folloWing modified version of Theorem 1 in GoodfelloW et al. (2014):
Theorem 3.1. Let Pg denote the distribution of the generator outputs G(z), where Z 〜Pz,
and let PWT be the marginals of Pg along Wk . For fixed G, the optimal {Dk} are given by
kg
Dk(y) = PWTx(y)/ (PWTx(y) + PWTg(y)),	⑶
for all k ∈ {1,…，K}. The optimal G under (2) is achieved iff PWT X = PWT g, ∀k.
4
Under review as a conference paper at ICLR 2018
5 4 3
sso—l」OlRlφuφ0
Figure 2: Training Stability. We plot the evolution of the “generator loss” across training—
against a traditional single discriminator (DC-GAN), and the average and individual losses
against multiple discriminators (K = 48) in our setting. For the traditional single dis-
criminator, this loss rises quickly to high value, indicating that the discriminator saturates
to rejecting generated samples with very high confidence. In contrast, the loss in our case
remains lower, allowing our discriminators to provide meaningful gradients to the generator.
Iter. 5k	Iter. 10k	Iter. 15k	Iter. 20k	Iter. 30k	Iter. 4Ck	Iter. 60k	Iter. 80k	Iter. 100k
Figure 3: Evolution of sample quality across training iterations. With our approach, the
generator improves the visual quality of its samples quickly and throughout training. Mean-
while, the generator trained in the traditional setting with a single discriminator shows
slower improvement, and indeed quality begins to deteriorate after a point.
This result, proved in the supplementary, implies that under ideal conditions the generator
will produce samples from a distribution whose marginals along each of the projections Wk
match those of the true distribution. Thus, each discriminator adds an additional constraint
on the generator, forcing it to match Px along a different marginal. As we show in the sup-
plementary, matching along a sufficiently high number of such marginals—under smoothness
assumptions on the true and generator distributions Px and Pg —guarantees that the full
joint distributions will be close (note it isn’t sufficient for the set of projection matrices
{Wk } to simply span Rd for this). Therefore, even though viewing the data through ran-
dom pro jections limits the ability of individual discriminators, with enough discriminators
acting in concert, the generator learns to match the full joint distribution of real data in Rd .
4 Experimental Results
We now evaluate our approach with experiments comparing it to generators trained against
a single traditional discriminator, and demonstrate that it leads to higher stability during
training, and ultimately yields generators that produce higher quality samples.
Dataset and Architectures. For evaluation, we primarily use the dataset of celebrity
faces collected by Liu et al. (2015)—we use the cropped and aligned 64 × 64-size version of
the images—and the DC-GAN (Radford et al., 2016) architectures for the generator and
discriminator. We make two minor modifications to the DC-GAN implementation that we
find empirically to yield improved results (for both the standard single discriminator setting,
as well as our multiple discriminator setting). First, we use different batches of generated
5
Under review as a conference paper at ICLR 2018
(_eu&*00 二理 1< XO 寸-8-14
IIsB~Eμosδ里 6~s) z<0lOQ

(Moow=-) po≡φ≡PSSOdaId
Figure 4: Random sets of generated samples from traditional DC-GAN and the proposed
framework. For DC-GAN, we show results from the model both at 40k iterations (when
the samples are qualitatively the best) and at the end of training (100k iterations). For
our setting, we show samples from the end of training for generator models trained with
K = 12, 24, 48 pro jections. Our generator produces qualitatively better samples, with finer
detail and fewer distortions. Quality is worse with subtle high-frequency noise when K is
smaller, but these decrease with increasing K to 24 and 48.
images to update the discriminator and generator—this increases computation per iteration,
but yields better quality results. Second, we employ batch normalization in the generator
but not in the discriminator (the original implementation normalized real and generated
batches separately, which we found yielded poorer generators).
For our approach, we train a generator against K discriminators, each operating on a dif-
ferent single-channel 32 × 32 projected version of the input, i.e., d/m = 12. The pro jected
images are generated through convolution with 8 × 8 filters and a stride of two. The filters
are generated randomly and kept constant throughout training. We compare this to the
standard DC-GAN setting of a single discriminator that looks at the full-resolution 64 × 64
color image. We use identical generator architectures in both settings—that map a 100 di-
mensional uniformly distributed noise vector to a full resolution image. The discriminators
also have similar architectures—but each of the discriminator in our setting has one less layer
as it operates on a lower resolution input (we map the number of channels in the first layer
in our setting to those of the second layer of the full-resolution single-discriminator, thus
matching the size of the final feature vector used for classification). As is standard practice,
we compute generator gradients in both settings with respect to minimizing the “incorrect”
classification loss of the discriminator-in our setting, this is given by 一表 Ek log Dk(G(z)).
As suggested in (Radford et al., 2016), we use Adam (Kingma & Ba, 2014) with learning
rate 2 × 10-4, β1 = 0.5, and a batch size of 64.
Stability. We begin by analyzing the evolution of generators in both settings through
training. Figure 2 shows the generator training loss for traditional DC-GAN with a single
discriminator, and compares it the proposed framework with K = 48 discriminators. In both
6
Under review as a conference paper at ICLR 2018
a=O.Q Q=0.1	a=Q.2	α=0.3	Q=O.4	CH=0.5	Qt=0.6	α=0.7	O!=0.8	C⅛=0.9	Qf=1.0
G (QZl + (1- Q)Z2)
Figure 5: Interpolating in latent space. For selected pairs of generated faces (with K = 48),
we generate samples using different convex combinations of their corresponding noise vectors.
Every combination generates a plausible face, and these appear to smoothly interpolate
between various facial attributes—age, gender, expression, hair, etc. Note that the α = 0.5
sample always corresponds to an individual clearly distinct from the original pair.
settings, the generator losses increase through much of training after decreasing in the initial
iterations (i.e., the discriminators eventually “win”). However, DC-GAN’s generator loss
rises quickly and remains higher than ours in absolute terms throughout training. Figure 3
includes examples of generated samples from both generators across iterations (from the
same noise vectors). We observe that DC-GAN’s samples improve mostly in the initial
iterations while the training loss is still low, in line with our intuition that generator gradients
become less informative as discriminators get stronger. Indeed, the quality of samples from
traditional DC-GAN actually begins to deteriorate after around 40k iterations. In contrast,
the generator trained in our framework improves continually throughout training.
Consistency. Beyond stability, Fig. 3 also demonstrates the consistency of our framework.
While the average loss in our framework is lower, we see this does not impede our generator’s
ability to learn the data distribution quickly as it collates feedback from multiple discrim-
inators. Indeed, our generator produces higher-quality samples than traditional DC-GAN
even in early iterations. Figure 4 includes a larger number of (random) samples from gener-
ators trained with traditional DC-GAN and our setting. For DC-GAN, we include samples
from both roughly the end (100k iterations) of training, as well as from roughly the middle
(40k iterations) where the sample quality are approximately the best. For our approach,
we show results from training with different numbers of discriminators with K = 12, 24,
and 48—selecting the generator models from the end of training for all. We see that our
generators produce face images with higher-quality detail and far fewer distortions than tra-
ditional DC-GAN. We also note the effect of the number of discriminators on sample quality.
Specifically, we find that setting K to be equal only to the projection ratio d/m = 12 leads
to subtle high-frequency noise in the generator samples, suggesting these many pro jections
do not sufficiently constrain the generator to learn the full data distribution. Increasing K
diminishes these artifacts, and K = 24 and 48 both yield similar, high-quality samples.
Training Time. Note that the improved generator comes at the expense of increased
computation during training. Traditional DC-GAN with a single discriminator takes only
0.6s per training iteration (on an NVIDIA Titan X), but this goes up to 3.2s for K = 12,
5.8s for K = 24, and 11.2s for K = 48 in our framework. Note that once trained, all
generators produce samples at the same speed as they have identical architectures.
7
Under review as a conference paper at ICLR 2018
Figure 6: Examples from training on canine images from Imagenet. We show manually
selected examples of 128 × 128 images produced by a generator trained on various canine
classes from Imagenet. Although not globally plausible, the samples contain realistic low-
level textures, and reproduce rough high-level composition.
Latent Embedding. Next, we explore the quality of the embedding induced by our
generator (K = 48) in the latent space of noise vectors z. We consider selected pairs of
randomly generated faces, and show samples generated by linear interpolation between their
corresponding noise vectors in Fig. 5. Each of these generated samples is also a plausible
face image, which confirms that our generator is not simply memorizing training samples,
and that it is densely packing the latent space with face images. We also find that the
generated samples smoothly interpolate between semantically meaningful face attributes—
gender, age, hair-style, expression, and so on. Note that in all rows, the sample for α = 0.5
appears to be a clearly different individual than the ones represented by α = 0 and α = 1.
Results on Imagenet-Canines. Finally, we show results on training a generator on a
subset of the Imagenet-1K database (Deng et al., 2009). We use 128 × 128 crops (formed by
scaling the smaller side to 128, and taking a random crop along the other dimension) of 160k
images from a subset of Imagenet classes (ids 152 to 281) of different canines. We use similar
settings as for faces, but feed a higher 200-dimensional noise vector to the generator, which
also begins by mapping this to a feature vector that is twice as large (2048), and which has
an extra transpose-convolution layer to go upto the 128 × 128 resolution. We again use 8 × 8
convolutional filters with stride two to form {WkT }—in this case, these produce 64 × 64 single
channel images. We use only K = 12 discriminators, each of which has an additional layer
because of the higher-resolution—beginning with fewer channels in the first layer so that
the final feature vector used for classification is the same length as for faces. Figure 6 shows
manually selected samples after 100k iterations of training (see supplementary material for
a larger random set). We see that since it is trained on a more diverse and unaligned image
content, the generated images are not globally plausible photographs. Nevertheless, we
find that the produced images are sharp, and that generator learns to reproduce realistic
low-level textures as well as some high-level composition.
5 Conclusion
In this paper, we proposed a new framework to training GANs for high-dimensional outputs.
Our approach employs multiple discriminators on random low-dimensional pro jections of
the data to stabilize training, with enough projections to ensure that the generator learns
the true data distribution. Experimental results demonstrate that this approaches leads to
more stable training, with generators continuing to improve for longer to ultimately produce
higher-quality samples. Source code and trained models for our implementation is available
at the pro ject page [anonymized for review].
In our current framework, the number of discriminators is limited by computational cost.
In future work, we plan to investigate training with a much larger set of discriminators,
employing only a small subset of them at each iteration, or every set of iterations. We
are also interested in using multiple discriminators with modified and regularized objectives
(e.g., (Nowozin et al., 2016; Arjovsky et al., 2017; Zhao et al., 2016)). Such modifications
are complementary to our approach, and deploying them together will likely be beneficial.
8
Under review as a conference paper at ICLR 2018
References
Martin Arjovsky and Leon Bottou. Towards principled methods for training generative
adversarial networks. In NIPS Workshop on Adversarial Training, 2016.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein gan. arXiv:1701.07875
[stat.ML], 2017.
Nicolas Bonneel, Julien Rabin, Gabriel Peyre, and Hanspeter Pfister. Sliced and radon
wasserstein barycenters of measures. Journal of Mathematical Imaging and Vision, 2015.
Timothy I Cannings and Richard J Samworth. Random-pro jection ensemble classification.
Journal of the Royal Statistical Society B, 2017.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel.
Infogan: Interpretable representation learning by information maximizing generative ad-
versarial nets. In NIPS, 2016.
Sanjoy Dasgupta. Learning mixtures of gaussians. In Foundations of Computer Science.
IEEE, 1999.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-
scale hierarchical image database. In CVPR, 2009.
Emily Denton, Sam Gross, and Rob Fergus. Semi-supervised learning with context-
conditional generative adversarial networks. arXiv:1611.06430 [cs.CV], 2016.
Emily L Denton, Soumith Chintala, Rob Fergus, et al. Deep generative image models using
a laplacian pyramid of adversarial networks. In NIPS, 2015.
Robert J Durrant and Ata Kaban. Random projections as regularizers: learning a linear
discriminant from fewer observations than dimensions. Machine Learning, 2015.
Ishan Durugkar, Ian Gemp, and Sridhar Mahadevan. Generative multi-adversarial networks.
arXiv:1611.01673 [cs.LG], 2016.
Yoav Freund, Robert E Schapire, et al. Experiments with a new boosting algorithm. In
ICML, 1996.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation
with conditional adversarial networks. arXiv:1611.07004 [cs.CV], 2016.
Robert A Jacobs. Methods for combining experts’ probability assessments. Neural compu-
tation, 1995.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv
preprint arXiv:1412.6980, 2014.
Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejan-
dro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, and Wenzhe
Shi. Photo-realistic single image super-resolution using a generative adversarial network.
arXiv:1609.04802 [cs.CV], 2016.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in
the wild. In ICCV, 2015.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural
samplers using variational divergence minimization. In NIPS, 2016.
Alec Radford, Luke Metz, and Soumit Chintala. Unsupervised representation learning with
deep convolutional generative adversarial networks. In ICLR, 2016.
9
Under review as a conference paper at ICLR 2018
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and
Xi Chen. Improved techniques for training gans. In NIPS, 2016.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices.
arXiv:1011.3027 [math.PR], 2010.
Yaxing Wang, Lichao Zhang, and Joost van de Weijer. Ensembles of generative adversarial
networks. arXiv:1612.00991 [cs.CV], 2016.
Junbo Zhao, Michael Mathieu, and Yann LeCun. Energy-based generative adversarial net-
work. arXiv:1609.03126 [cs.LG], 2016.
10
Under review as a conference paper at ICLR 2018
Supplementary Material
A Theory
In this section we provide additional theoretical results showing the benefits of random
projections, for some particular choice of distributions, for both improving stability and
guaranteeing consistency of GAN training.
Stability
Here, we consider several simplifying assumptions on the distribution of x, to gain intuition
as to how a random low-dimensional projection affects the “relative” volume of the support
of Px . Let us assume that the range of x is the d-dimensional ball Bd of radius 1 centered
at 0. We define the support supp(Px) ⊂ Bd of a distribution Px to be the set where the
density is greater than some small threshold e. Assume that the projection W ∈ Rd×m is
entry-wise random Gaussian (rather than corresponding to a random convolution). Denote
as BWd the projection of the range on W, and PWTx as the marginal of Px along W.
Theorem A.1. Assume Px = Ej TjN(x∣μj, Σj) is a mixture of Gaussians, such that the
individual components are sufficiently well separated (in the sense that there is no overlap
between their supports or the projections thereof, see (Dasgupta, 1999)). If supp(Px) ⊂ Bd
and Vol(supp(Px)) > 0, then Vol(supp(PW T x))/Vol(BWd ) > Vol(supp(Px))/Vol(B d) with
high probability.
This result implies that the projection of the support of Px , under this simplified assump-
tions, occupies a higher fraction of the volume of the pro jection of the range of x. This aids
in stability because it makes it more likely that a larger fraction of the generator’s sam-
ples (which also lie within the range of x) will, after projection, overlap with the pro jected
support supp(Px), and can not be rejected absolutely by the discriminator.
Proof of Theorem A.1. We first show that we can assume that the columns of the
projection W are orthonormal. Since W ∈ Rd×m is entry-wise Gaussian distributed, it
has rank m with high probability. Then, there exists a square invertible matrix A such
that W' = AW where W' is orthonormal. In that case, Vol(SuPP(PWTx))/Vol(BW)=
Vol(supp(PW ′ T ))/Vol(BWd ′ ) because the numerator and denominator terms for both can
be related by det(A) for the change of variables, which cancels out. Note that under this
orthonormal assumPtion, BWd = Bm .
Next, We consider the case of an individual Gaussian distribution Px = N(x∣μ, Σ), and prove
that the ratio of supports (defined with respect to a threshold E) does not decrease with the
projection. The expression for these ratios is given by:
Vol(supp(Px)) = Vol(Bd) X det(Σ) X log ɪ 一 dlog 2π 一 logdet(Σ)
E2
⇒ VOl(SuPPdPx)) = det(Σ) X log ^2 一 dlog 2π 一 logdet(Σ) .	(4)
Vol(Bd )	E2
Vol(supp(Pw T x))
-Vol(BW)-
det(WTΣW) χ logɪ 一 m log 2π — log det(WTΣW).
E2
(5)
For sufficiently small E, the volume ratio of a single Gaussian will increase with projection
if det(WTΣW) > det(Σ). Note that all eigenvalues of Σ ≤ 1, with at-least one eigenvalue
strictly < 1 (since supp(Px) ⊂ Bd). First, we consider the case when Σ is not strictly positive
definite and one of the eigenvalues is 0. Then, Vol(supp(Px)) = 0 and Vol(supp(PW T x)) ≥ 0,
i.e., the volume ratio either stays the same or increases.
11
Under review as a conference paper at ICLR 2018
For the case when all eigenvalues are strictly positive, consider a co-ordinate transform
where the first m co-ordinates of x correspond to the column vectors of W , such that
ςW	ςWW '	心
Σ = ΣT Σ ,	(6)
_ ςWW'	ςW' _
where ΣW = WTΣW. Then,
det(Σ) = det(∑W )det(∑W ‛— ∑TWW' ∑W1 ∑ww ')
≤ det(ΣW) det(ΣW ′),
⇒ det(ΣW) ≥ det(Σ)/ det(ΣW ′).	(7)
Note that det(ΣW ′ ) ≤ 1, since all eigenvalues of Σ are ≤ 1, with equality only when W is
completely orthogonal to the single eigenvector whose eigenvalue is strictly < 1, which has
probability zero under the distribution for W . So, we have that det(ΣW ′ ) < 1, and
det(WTΣW) = det(Σw) > det(Σ).	(8)
The above result shows that the volume ratio of individual components never decrease, and
always increase when their co-variance matrices are full rank (no zero eigenvalue). Now,
we consider the case of the Gaussian mixture. Note that the volume ratio of the mixture
equals the sum of the ratios of individual components, since the denominator Vol(B m) is
the same, where the support volume in these ratios for component j is defined with respect
to a threshold e∕τj. Also, note that since mixture distribution has non-zero volume, at least
one of the Gaussian components must have all non-zero eigenvalues. So, the volume ratios
of Px and PWTx are both sums of individual Gaussian component terms, and each term for
PWTx is greater than or equal to the corresponding term for Px, and at least one term is
strictly greater. Thus, the support volume ratio of PWTx is strictly greater than that of Px.
Consistency
Proof of Theorem 3.1. The proof follows along the same steps as that of Theorem 1
in Goodfellow et al. (2014).
V(Dk,G)
=Eχ~Pχ[log Dk (WT x)] + Eχ~Pg [log(1 — Dk(Wk x))]
=EY~pwt [logDk(y)]+ Ey~PWτ [log(1 — Dk(y))].	(9)
Wk x	Wk g
For any point y ∈ supp(PWkT x) ∪ supp(PWkT g), differentiating V (Dk, G) w.r.t. Dk and setting
to 0 gives us:
PWT x (y )
Dk(S = p_/WkC .	(10)
PWkT x (y) + PWkT g (y)
Notice we can rewrite V (Dk , G) as
V(Dk,G) = -2log(2) + KL (PWTxllPWTx+ PWTg)
PWTx + PWTg
+ KL (PWTgIl —k-^-kɪJ . (11)
Here KL is the Kullback Leibler divergence, and it is easy to see that the above expression
achieves the minimum value when PWkT x = PWkT g .
Next, we present a result that shows that given enough random projections K, the full
distribution Pg of generated samples will closely match the full true distribution Px .
12
Under review as a conference paper at ICLR 2018
Def: A function f : Rd → R+ is L-Lipschitz, if∀x1,x2 ∈ Rd, |f(xι)-f (x2)∣ ≤ L∙d(xι,x2).
Theorem A.2. Let Px and Pg be two compact distributions with support of radius B, such
that PWTX = PWTg,∀k ∈ {1,…,K}. Let {Wk} be entrywise random Gaussian matrices in
Rd×m . Let R = Px - Pg be the residual function of the difference of densities. Then, with
high probability, for any x ∈ Rd,
|R(x)|
…-Pg(X)ι≤ O(KLm),
where LR is the Lipschitz constant of R.
This theorem captures how much two probability densities can differ if they match along
K marginals of dimension m, and how the error decays with increasing number of discrim-
inators K. In particular, if we have a smooth residual function R (with small Lipschitz
constant LR), then at any point ∈ Rd , it is constrained to have small values—smaller with
increasing number of discriminators K, and higher dimension m. The dependence on LR
suggests that the residual can take larger values if it is not smooth—which can result from
either the true density Px or the generator density Pg , or both, being not smooth.
Again, this result gives us rough intuition about how we may expect the error between the
true and generator distribution to change with changes to the values of K and m. In practice,
the empirical performance will depend on the nature of the true data distribution, the
parametric form / network architecture for the generator, and the ability of the optimization
algorithm to get near the true optimum in Theorem 3.1.
Proof of Theorem A.2. Let R = Px - Pg, be the residual function that captures the
difference between the two distributions. R satisfies the following properties:
/ R(x)dx = x(Px(x) - Pg(x))dx = 1-1=0,
and for any set S,
R(x)dx ≤	Px(x)dx ≤ 1.
x∈S	x∈S
(12)
(13)
Further, since both the distributions have same marginals along k different directions {Wk },
we have, for any x,
RWkT y (x) =	R(y)
y|x=WkT y
0.
We first prove this result for discrete distributions supported on a compact set S with γ
∙-v
points along each dimension. Let P denote such a discretization of a distribution P and
∙-v	∙-v	∙-v
R = Px - Pg.
Each of the marginal equation PWTx = PWT (RWTU = 0) is equivalent to Ym linear
k kg ky
∙-v	∙-v	∙-v
equations of the distribution Px of the form, x:WkT x=y Px(x) = PWkT g (y). Note that we
have γd choices for x and γm choices for y. Let Ak ∈ Rγm×γd denote the coefficient matrix
AkPx = PWkTg, such that Ak (i, j) = 1 if WkT xi = yj, and 0 otherwise.
The rows of Ak for different values of yj are clearly orthogonal. Further, since different Wk
are independent Gaussian matrices, rows of Ak corresponding to different Wk are linearly
independent. In particular let A ∈ RγmK ×γd denote the vertical concatenation of Ak . Then,
A has full row rank of Ym ∙ K with probability ≥ 1 — C∙ m ∙ e-d (Vershynin, 2010), with C some
arbitrary positive constant. Since Px is a Yd dimensional vector, Yd linearly independent
∙-v	∙-v	∙-v
equations determine it uniquely. Hence Ym ∙ K ≥ γd, guarantees that Px = Pg or R = 0.
Now we extend the results to the continuous setting. Without loss of generality, let the
compact support S of the distributions be contained in a sphere of radius B. Let NL be
an LR net of S, with Yd points (see Lemma 5.2 in Vershynin (2010)), where Y = 2B ∙ Lr/j
Then for every point Xi ∈ S, there exists a x2 ∈ NlR such that, d(x1,x2) ≤ LR.
13
Under review as a conference paper at ICLR 2018
Further for any x1,x2 with d(xι, x2) ≤ L—, if R is LR LiPschitz We know that,
LR
IR(XI)- R(χ2)1 ≤ LR ∙ -— = €.	(14)
LR
∙-v
Finally, notice that the marginal constraints do not guarantee that the distributions PWkT x
and PWTg match exactly on the €-net (or that R is 0), but only that they are equal upto
an additive factor of €. Hence, combining this with equation 14 we get, |R(x)| = |Px(x) -
Pg(x)∣ ≤ O(e), for any X with probability ≥ 1 — C ∙ m ∙ K ∙ e-d. Since we have γd-m =
(等)d-m = O(Kk), we get € = O(9).
K d-m
14
Under review as a conference paper at ICLR 2018
B Additional Experimental Results
Face Images: Proposed Method (K = 48)
15
Under review as a conference paper at ICLR 2018
Face Images: Proposed Method (K = 24)
16
Under review as a conference paper at ICLR 2018
Face Images: Proposed Method (K = 12)
17
Under review as a conference paper at ICLR 2018
Face Images: Traditional DC-GAN (Iter. 40k)
18
Under review as a conference paper at ICLR 2018
Face Images: Traditional DC-GAN (Iter. 100k)
19
Under review as a conference paper at ICLR 2018
Random Imagenet-Canine Images: Proposed Method
20