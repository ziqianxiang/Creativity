Under review as a conference paper at ICLR 2018
Gaussian Process Neurons
Anonymous authors
Paper under double-blind review
Ab stract
We propose a method to learn stochastic activation functions for use in probabilis-
tic neural networks. First, we develop a framework to embed stochastic activation
functions based on Gaussian processes in probabilistic neural networks. Second,
we analytically derive expressions for the propagation of means and covariances in
such a network, thus allowing for an efficient implementation and training without
the need for sampling. Third, we show how to apply variational Bayesian infer-
ence to regularize and efficiently train this model. The resulting model can deal
with uncertain inputs and implicitly provides an estimate of the confidence of its
predictions. Like a conventional neural network it can scale to datasets of arbitrary
size and be extended with convolutional and recurrent connections, if desired.
1	Introduction
The popularity of deep learning and the implied race for better accuracy and performance has lead
to new research of the fundamentals of neural networks. Finding an optimal architecture often
focusses on a hyperparameter search over the network architecture, regularization parameters, and
one of a few standard activation functions: tanh, ReLU (Glorot et al. (2011)), maxout (Goodfellow
et al. (2013)) . . . Focussing on the latter, looking into activation functions has only taken off since
Nair & Hinton (2010) introduced the rectified linear unit (ReLU), which were shown to produce
significantly better results on image recognition tasks Krizhevsky et al. (2012). Maas et al. (2013)
then introduced the leaky ReLU, which has a very small, but non-zero, slope for negative values.
He et al. (2015) proposed the parameterized ReLU, by making the slope of the negative part of
the leaky ReLU adaptable. It was trained as an additional parameter for each neuron alongside
the weights of the neural network using stochastic gradient descent. Thus, the activation function
was not treated as a fixed hyper-parameter anymore but as adaptable to training data. While the
parameterized ReLU only has one parameter, this was generalized in Agostinelli et al. (2014a) to
piecewise linear activation functions that can have an arbitrary (but fixed) number of points where
the function changes it slope. This can be interpreted as a different parameterization of a Maxout
network (Goodfellow et al. (2013)), in which each neuron takes the maximum over a set of different
linear combinations of its inputs.
Instead of having a fixed parameter for the negative slope of the ReLU, Xu et al. (2015) introduced
stochasticity into the activation function by sampling the value for the slope with each training itera-
tion from a fixed uniform distribution. Clevert et al. (2015) and Klambauer et al. (2017) replaced the
negative part of ReLUs with a scaled exponential function and showed that, under certain conditions,
this leads to automatic renormalization of the inputs to the following layer and thereby simplifies
the training of the neural networks, leading to an improvement in accuracy in various tasks.
Nearly fully adaptable activation functions have been proposed by Eisenach et al. (2016). The au-
thors use a Fourier basis expansion to represent the activation function; thus with enough coefficients
any (periodic) activation function can be represented. The coefficients of this expansion are trained
as network parameters using stochastic gradient descent or extensions thereof.
Promoting a more general approach, Agostinelli et al. (2014b) proposed to learn the activation
functions alongside the layer weights. Their adaptive piecewise linear units consist of a sum of
hinge-shaped functions with parameters to control the hinges and the slopes of the linear segments.
However, by construction the derivative of these activation functions is not continuous at the joints
between two linear segments, which often leads to non-optimal optimizer performance.
1
Under review as a conference paper at ICLR 2018
To our knowledge, previous research on learning activation functions took place in a fully deter-
ministic setting, i.e. deterministic activation functions were parameterized and included in the op-
timization of a conventional neural network. Here instead, we explore the setting of probabilistic
activation functions embedded in a graphical model of random variables resembling the structure
of a neural network. We develop the theory of Gaussian-process neurons and subsequently derive
a lower-bound approximation using variational inference, in order to develop a computationally
efficient version of the Gaussian Process neuron.
Notation
To define the model we will need to slice matrices along rows and columns. Given a matrix X, we
will write Xi? to select all elements of the i-th row and X?j to select all elements of the j -th column.
2	Gaussian Processes
Gaussian Processes (GPs) are nonparametric models that provide flexible probabilistic approaches
for function estimation. A Gaussian Process (Rasmussen & Williams, 2006) defines a distribution
over a function f (x) 〜GP(m(x), k(x, x0)) where m is called the mean function and k is the
covariance function. For S inputs x ∈ RS×N of N dimensions the corresponding function values
f with fi , f(Xi?) follow a multivariate normal distribution1
f 〜N(m,K(X,X))
with mean vector mi , m(Xi?) and K(X, X) is the covariance matrix defined by (K(X, X))ij ,
k(Xi?, Xj?). In this work we use the zero mean function m(x) = 0 and the squared exponential
(SE) covariance function with scalar inputs,
,f	0、	( (X-X0)2∖
k(x, x ) = exp I---2ν2— ) ,	(1)
where ν is called the length-scale and determines how similar function values of nearby inputs
are according to the GP distribution. Since this covariance function is infinitely differentiable, all
function samples from a GP using it are smooth functions.
3	Gaussian Process Neurons
We will first describe the fundamental, non-parametric model, which will be approximated in the
following sections for efficient training and inference. Let the input to the l-th layer of Gaussian
Process neurons (GPNs) be denoted by Xl-1 ∈ RS×Nl-1 where S is the number of data points
(samples) and Nl-1 is the number of input dimensions. A layer l ∈ {1, . . . , L} ofNl GPNs indexed
by n ∈ {1, . . . Nl} is defined by the joint probability
P(X?ln, F?ln | Xl-1) = P(F?ln | Xl-1) P(X?ln | F?ln)	(2)
with the GP prior Fl conditioned on the layer inputs Xl-1 multiplied with the weights Wl,
F?n | Xl-1 〜N(0, Kl(Xl-1Wln, Xl-1 Wln)),	⑶
and an additive Gaussian noise distribution,
X?ln|F?ln 〜 N (F?ln, (σnl)21).	(4)
This corresponds to a probabilistic activation function Fsln = fnl (Xsl-? 1 W?ln) with
fnl(z) 〜 GP(0, k(z, z0)).	(5)
This GP has scalar inputs and uses the standard squared exponential covariance function. Analogous
to standard neural networks, GPN layers can be stacked to form a multi-layer feed-forward network.
The joint probability of such a stack is
L
P(X{l},F{l})=P(X0) Y P(X?ln, F?ln | Xl-1).	(6)
l=1
1We omit writing P (•).
2
Under review as a conference paper at ICLR 2018
U?ln	V?ln
Figure 1: The auxiliary parametric representation of a GPN using virtual observation inducing
points V? and targets U? .
All input samples Xs0?, s ∈ {1, . . . , S}, are assumed to be normally distributed with known mean
and covariance,
S
P(X0) = Y N(X0? I μsn?, ∑sn??).	⑺
s=1
To obtain predictions P(XL | X0), all latent variables in eq. (6) would need to be marginalized out;
unfortunately due to the occurrence of X l in the covariance matrix in eq. (3) analytic integration is
intractable.2
3.1	Auxiliary Parametric Representation
The path to obtain a tractable training objective is to temporarily parameterize the activation func-
tion #(Z) of each GPN using virtual observations (originally proposed by QUinonero-CandeIa &
Rasmussen (2005) for sparse approximations of GPs) of inputs and outputs of the function. These
virtual observations are only introduced as an auxiliary device and will be marginalized out later.
Each virtual observation r consists of a scalar inducing point Vrln and corresponding target Urln .
Under these assumptions on fdl, the GP prior P(F?ln | Xl-1) is replaced with
F?d I X j,Uln 〜N(μFn, ∑Fln)	⑻
where the mean and variance are those obtained by using the virtual observations as “training” points
for a GP regression evaluated at the “test” points X lW?ld,
μF = K(Xl-1w?n, VI)(Kn??)T U?n	(9)
∑L= K(Xl-1Wln, Xl-1Wln) - K(Xl-1Wln, 丫?公(良心 T K(VL Xl-1 W?„) .	(10)
where Kenl rt = K(Vrln,Vtln).
Given enough inducing points that lie densely between the layer’s activations Al?n = XlT1W?ln , the
shape of the activation function becomes predominantly determined by the corresponding targets
of these inducing points. Consequently, the inter-sample correlation in eq. (8) becomes negligible,
allowing us to further approximate this conditional by factorizing it over the samples; thus we have
P(F?ln IXl,U?ln)=QsP(Fsln IXslT?1,U?ln)with
Fsn I Xs- 1,U?n 〜N("FL ∑FSn) ∙	(")
We now marginalize eq. (4) over F?ln and get P(X?ln I XlT1, U?ln) = Qs P(Xsln I XslT? 1, U?ln) with
Xsn I Xs- 1, U?n 〜"MF1, ∑Fln +	)2) ∙	(12)
2 The inverse of the covariance matrix appears in the PDF of a normal distribution. Thus, the dependency
on Xl is highly non-linear and analytic calculation of the integral is not feasible.
3
Under review as a conference paper at ICLR 2018
(b) X
X3
Figure 2: A GPN feed-forward network prior distribution (a) for three layers and the approximation
of its posterior obtained using variational inference and the central limit theorem on the activations
(b). Each node corresponds to all samples and GPN units within a layer. Dotted circles represent
variational parameters.
Although we now have a distribution for Xl that is conditionally normal given the values of the
previous layer, the marginals P(X?l n), l ∈ {1, . . . , L}, will, in general, not be normally distributed,
because the input from the previous layer Xl-1 appears non-linearly through the kernel function in
the mean eq. (9).
By putting a GP prior on the distribution of the virtual observation targets Ul as shown in fig. 1.,
Uln -N(0,K (V?n ,V?n)) ,	(13)
it can easily be verified that the marginal distribution of the response,
P(F?ln|Xl)=	P(F?ln | Xl, U?ln) P(U?ln) dU?ln = N (F?ln | 0, K(Xl-1W?ln, Xl-1W?ln)), (14)
recovers the original, non-parametric GPN response distribution given by (3). The first use of this
prior-restoring technique was presented in Titsias (2009) for finding inducing points of sparse GP
regression using variational methods.
3.2	Approximate Bayesian Inference for GPN Networks
The joint distribution of a GPN feed-forward network is given by
L
P({X}1L,{A}1L,{U}1L,{F}1L |X0) = YP(Al|Xl-1)P(Ul)P(Fl|Al,Ul)P(Xl|Fl), (15)
l=1
where to notation {•}1L should be read as {•1, •2, . . . , •L}. A graphical model corresponding to that
distribution for three layers is shown in fig. 2a. Since exact marginalization over the latent variables
is infeasible we apply the technique of variational inference Wainwright et al. (2008) to approximate
the posterior of the model given some training data by a variational distribution Q.
The information about the activation functions learned from the training data is mediated via the
virtual observation targets Ul, thus their variational posterior must be adaptable in order to store that
information. Hence, we choose a normal distribution factorized over the GPN units within a layer
with free mean and covariance for the approximative posterior of Ul,
Nl
Q(Ul)= Y Q(Uln) ,	Q(Uln) = N(U?n I bUl, ∑En) ∙	(⑹
n=1
This allows the inducing targets of a GPN to be correlated, but the covariance matrix can be con-
strained to be diagonal, if it is desired to reduce the number of variational parameters. We keep the
4
Under review as a conference paper at ICLR 2018
rest of the model distribution unchanged from the prior; thus the overall approximating posterior is
L
Q({U}1L,{X}1L-1,{A}1L,{F}1L)=YP(Al|Xl-1)Q(Ul)P(Fl|Al,Ul)P(Xl|Fl).	(17)
l=1
Estimating the variational parameters μul and ΣUl requires maximizing the evidence lower bound
(ELBO) given by
L /	L「XL-1「A】L 7]L)2Q({U}L,{χ}L-1,{A}L,{F}L)
J-J …J Q({U }1, {X }1	,⑷1, {F}1 )log P({U }L,{X }L,{A}L,{F}L | X 0) ∙
d{U}1Ld{X}1Ld{A}1Ld{F}1L.	(18)
Substituting the distributions into this equation results inE = -Ereg + 乜Pred With
Lreg = XJ Q(UI)IOg P(U 1) dUl，	(19)
EPred =	Q(FL) logP(XL|FL)dFL.	(20)
The term Ereg can be identified as the sum of the KL-divergences between the GP prior on the
virtual observation targets and their approximative posterior Q(Ul ). Since this term enters E with
a negative sign, its purpose is to keep the approximative posterior close to the prior; thus it can be
understood as a regularization term. Evaluating using the formula for the KL-divergence between
two normal distributions gives
L	L Nl
Ereg= X KL(Q(Ul ) || P(Ul )) = XX KL(Q(U?n) || P(U[n))	(21)
l=1	l=1 n=1
(X 2 X X (tr(κ (V?ln，Wn)T ∑ U?n) + (μU )T KM”,匕了⑶ + log fnllf/
l=1 n=1 ∖	I，Y?n I	/
The term EPred cannot be evaluated yet because the exact marginal Q(FL) is still intractable.
3.2.1	Central Limit Distribution of Activations
Due to the central limit theorem the activation Al (weighted sum of inputs) of each GPN will con-
verge toa normal distribution, if the number of incoming connections is sufficiently large (≥ 50) and
the weights Wl have a sufficiently random distribution. For standard feed forward neural networks
Wang & Manning (2013) experimentally showed that even after training the weights are sufficiently
random for this assumption to hold. Hence we postulate that the same is true for GPNs and assume
that the marginal distributions Q(Al), l ∈ {1, . . . , L}, can be written as
Q(Al) = YQ(AS?),	Q(AS?) ≈N(AS*∣μAl,∑f?) .	(22)
s=1
A graphical model corresponding to this approximate posterior is shown in fig. 2b. This allows the
moments of Q(Al) to be calculated exactly and propagated analytically from layer to layer. For this
purpose we need to evaluate the conditional distributions
Q(Xl | Xl-1)
Q(Fl | Al = Wl Xl-1) P(Xl | Fl)dFl,
Nl
Q(Fl|Al)=Y
n=1
Q(U?ln) P(F?ln | Al?n, U?ln) dU?ln.
(23)
(24)
Since Q(Fl | Al) is the conditional of a GP with normally distributed observations, the joint distri-
bution Q(F?ln, U?ln | Al?n) = Q(U?ln) P(F?ln | Al?n,U?ln) must itself be normal,
Q(F?ln, U?ln | Al?n) = N
Fl
?n
Ul
?n
Fl
μ*n
μUn
ΣUF
ΣbFl
乙？？九
(25)

5
Under review as a conference paper at ICLR 2018
and We can find the values for the unknown parameters b?Fl, ∑ Fln (and ∑ FU = Σe TU F) by equating
the moments of its conditional distribution Q(F?ln | U?ln, Al?n) with P(F?ln | U?ln, Al?n). Thus by
solving the resulting equations we obtain for eq. (24),
Nl
Q(Fl I Al) = Y N(FL I μFn, ∑Fln).	(26)
n=1
where
bFl = K(AIn, Hn) K Mn, VL)-I 或	⑪
Σ Mn = K(A*n, AIn)- K(A*n, VL ) KUn K Mn, A*n)	(28)
with
KbIUIln ,K(VIln,VIln)-1-K(VIln,VIln)-1ΣbIUIlnK(VIln,VIln)-1.	(29)
For deterministic observations, that is Σb IUIln = 0, we obtain KbIUIln = K(VIln, VIln)-1 and thus
recover the standard GP regression distribution as expected. If Ul follows its prior, that is b?Un = 0
and Σb IUIln = K(VIln, VIln), we obtain KbIUIln = 0 and thus recover the GP prior on Fl. In that case
the virtual observations behave as if they were not present.
Having Q(Fl I Al) immediately allows us to evaluate eq. (23) since P(Xl I Fl) just provides additive
Gaussian noise; thus we obtain
Nl
Q(XlI Xl-1) = Y N(FIn I bFl, ∑Fln + (σn)21).
n=1
Returning to 乜Pred from (20) and writing it as
(30)
七Pred =川 Q(AL) Q(UL) P(FLI AL, UL) log P(XLIFL) dAL dUL dFL .	(31)
shows that we first need to obtain the distribution Q(AL). This is done by iteratively calculating the
marginals Q(Al) for l ∈ {1, . . . , L}.
3.2.2	Propagation of Uncertainty
For l ≥ 1 the marginal distribution of the activations is
Q(Al+1) =
Q(Al)Q(Fl IAl)P(Xl I Fl) P(Al+1 I Xl) dAl dFl dXl
(32)
where Q(Fl I Al ) is given by (26). We first evaluate the mean and covariance of the marginal
Q(Fl) = Q(Al)Q(FlIAl)dAl.
EQ(As?) [bFn]
we obtain
For the marginal mean of the response eFn
EQ(Fl) Fsln
eFn = EQ(As?) [K(AISn, Vln)]T K(Vt Wn)T μUnl = W K(Vln Wn)-I μU	(33)
with
ψstn , EQ(Asn) [K(ASn, VlJ = Sν2 +∑Ann eXP(-2V + Σnn) ! ,
which was calculated by expressing the squared exponential kernel as a normal PDF and applying
the product formula for Gaussian PDFs (Bromiley, 2003). For the marginal covariances of the
response ΣesFIlI we obtain by applying the law of total expectation
ςFnn0 , Cov(FSn, FsnO)= EQ(As?) [EQ(Fsn,Fsn0 | Al)[Fsn FSn0]] - TeFn 试n0	(34)
For the elements representing the variance, i.e. the diagonal n = n0, this becomes
ΣFnn= EQ(As*) h∑Fln + (bFn )2i - (eFn )2
=1 - Tr([KUn- βIn(βIn)T] H**) - Tr(ψS*n(ψS*n)T βIn(βIn)T)	(35)
6
Under review as a conference paper at ICLR 2018
With βl*n，K(Vln, V?n )-1 bUn and, using the same method as above,
ΩStrn，EQ(As?) [K(ASn, Vn) K(Alsn, Vln)]
i
ν2
V2 +2∑Ann exp
For off-diagonal elements, n
Vtn+VrnY	Mn- Vln)2
ν2 + 2∑Aln
4ν2
(36)
\ /
6= n0 , we observe that Fsln and Fsln0 are conditionally independent
given Al because the activation functions of GPNs n and n0 are represented by two different GPs.
Hence we have
ΣesFnln0 = EQ(Als?) [bFl 现]--eFn eF∣0 = (Xn)T AS**nn0
BL
(37)
/
—
—
—
—
where
Λl
srtnn
, EQ(Als?)K(Alsn,Vtln)K(Alsn0,Vrln0)
ν2 exp(Aslrtnn0/Bslnn0 )
W+∑Ain][ν2+∑Ak, ] -(∑nn O )2
0
with
l
srtnn0
Bsl nn0
,(VInO- μAn0 )2 [ν2 + £snn] + (Vn- 4sn T [ν2 + 工AlOnO] +
2 (Vtn- μA) (μAn0 - VInO) EAmO
, 2 [ν2 + ΣsAnln] [ν2 + ΣsAnlOnO] - (ΣsAnlOn)2	.
This concludes the calculation of the moments of Fl. We can now state how the activation distribu-
tion propagates from a layer to the next. The marginal distribution of Al+1 is given by
Q(Al+1) = n(aS+1∣ eA?+1, ∑ A?+1)
(38)
with
(39)
∑A?+1 = Wl+1 (∑F：* + diag(σl)2) (Wl+1)T .	(40)
Thus Q(AL) in (31) can be calculated by iterating the application of eqs. (33), (35), (37), (39)
and (40) over the layers l. To save computational poWer only the variances can be propagated by
assuming that Σe sF?l? is diagonal and therefore ignoring (37).
Now Epred can be identified as the expected log-probability of the observations under the marginal
distribution Q(FL) and thus We can expand it as folloWs,
EPred = EQ(FL) [logP(XLI FL)] H EQ(FL)
Nl
-SXlogσnL -
n=1
S NL
2 XX
s=1 n=1
(Xsn - Fsn)2 #
(σL)2
NL
-S	log σnL -
n=1
ιX X (Xsn)2- 2 Xsn EQ(FL) [Fsn]+EQ(F L)[(Fsn)2]
2 s=1 ⅛	可
(41)
where S is the number of training samples and XL are the training targets. The distribution Q(FL)
is of arbitrary form, but only its first and second moments are required to evaluate Epred. For the first
moment we obtain EQ(Fl) [Fln] = eFn with eFn given by (33) and the second moment evaluates
to
EQ(Fl) [(FSn)2] = VarQ(FL)(Fsn) + EQ(FL) [Fsn] = eFnn + (μFnn)
=1 - tr h(KUn - β*n(β*n)T) ΩL**ni ,	(42)
With β*n , K(Vn,Vn)T bUL and ωl from (36).
This concludes the calculation of all terms of the variational lower bound (18). The resulting ob-
jective is a fully deterministic function of the parameters. Training of the model is performed by
maximizing E w.r.t. to the variational parameters bUl, ΣUl and the model parameters σl, Wl and
Vl . This can be performed using any gradient-descent based algorithm. The necessary derivatives
are not derived here and it is assumed that this can be performed automatically using symbolic or
automatic differentiation in an appropriate framework.
7
Under review as a conference paper at ICLR 2018
3.3	Computational and Model Complexity
The activation function are represented using 2R variational parameters per GPN, where R is the
number of inducing points and targets. It can be shown that R = 10 linearly spaced inducing
points are enough to represent the most commonly used activation functions (sigmoid, tanh, soft
ReLU) with very high accuracy. The number of required parameters can be reduced by sharing
the same activation function within groups of neurons or even across whole layers of neurons. If
the inducing points V l are fixed (for example by equally distributing them in the interval [-1, 1]),
the kernel matrices K(V l, V l) and their inverses can be precomputed since they are constant. The
number of parameters and the computational complexity of propagating the means and covariances
only depend on R and are therefore independent of the number of training samples. Thus, like a
conventional neural network, a GPN network can inherently be trained on datasets of unlimited size.
4	Conclusion
We have presented a non-parametric model based on GPs for learning of activation functions in a
multi-layer neural network. We then successively applied variational to make fully Bayesian infer-
ence feasible and efficient while keeping its probabilistic nature and providing not only best guess
predictions but also confidence estimations in our predictions. Although we employ GPs, our para-
metric approximation allows our model to scale to datasets of unlimited size like conventional neural
networks do.
We have validated networks of Gaussian Process Neurons in a set of experiments, the details of
which we submit in a subsequent publication. In those experiments, our model shows to be signifi-
cantly less prone to overfitting than a traditional feed-forward network of same size, despite having
more parameters.
References
Forest Agostinelli, Matthew Hoffman, Peter Sadowski, and Pierre Baldi. Learning activation func-
tions to improve deep neural networks. arXiv preprint arXiv:1412.6830, 2014a.
Forest Agostinelli, Matthew D. Hoffman, Peter J. Sadowski, and Pierre Baldi. Learning activation
functions to improve deep neural networks. CoRR, abs/1412.6830, 2014b.
Paul Bromiley. Products and convolutions of Gaussian probability density functions. Tina-Vision
Memo, 3, 2003.
Djork-Ame Clevert, Thomas Unterthiner, and SePP Hochreiter. Fast and accurate deep network
learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.
Carson Eisenach, Zhaoran Wang, and Han Liu. NonParametrically learning activation functions in
deeP neural nets. 2016.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio. DeeP sParse rectifier neural networks. In Pro-
ceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS),
2011.
I.	J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and Y. Bengio. Maxout Networks. ArXiv
e-prints, February 2013.
Ian J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout
networks. arXiv preprint arXiv:1302.4389, 2013.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deeP into rectifiers: SurPassing
human-level Performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
Gunter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing
neural networks. arXiv preprint arXiv:1706.02515, 2017.
8
Under review as a conference paper at ICLR 2018
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (eds.),
Advances in Neural Information Processing Systems 25, pp. 1097-1105. Curran Associates, Inc.,
2012.
Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectifier nonlinearities improve neural net-
work acoustic models. In Proc. ICML, volume 30, 2013.
Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines.
In Proceedings of the 27th International Conference on International Conference on Machine
Learning, ICML’10, pp. 807-814, USA, 2010. Omnipress. ISBN 978-1-60558-907-7.
Joaquin Quinonero-Candela and Carl Edward Rasmussen. A unifying view of sparse approximate
Gaussian process regression. Journal of Machine Learning Research, 6(Dec):1939-1959, 2005.
Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning.
The MIT Press, 2006.
Michalis K Titsias. Variational learning of inducing variables in sparse gaussian processes. In
International Conference on Artificial Intelligence and Statistics, pp. 567-574, 2009.
Martin J Wainwright, Michael I Jordan, et al. Graphical models, exponential families, and variational
inference. Foundations and TrendsR in Machine Learning, 1(1-2):1-305, 2008.
Sida I Wang and Christopher D Manning. Fast dropout training. In ICML (2), pp. 118-126, 2013.
Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical evaluation of rectified activations in
convolutional network. arXiv preprint arXiv:1505.00853, 2015.
9