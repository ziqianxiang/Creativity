Under review as a conference paper at ICLR 2018
Learning temporal evolution of probability
distribution with Recurrent Neural Network
Anonymous authors
Paper under double-blind review
Ab stract
We propose to tackle a time series regression problem by computing temporal
evolution of a probability density function to provide a probabilistic forecast. A
Recurrent Neural Network (RNN) based model is employed to learn a nonlinear
operator for temporal evolution of a probability density function. We use a soft-
max layer for a numerical discretization of a smooth probability density functions,
which transforms a function approximation problem to a classification task. Ex-
plicit and implicit regularization strategies are introduced to impose a smoothness
condition on the estimated probability distribution. A Monte Carlo procedure to
compute the temporal evolution of the distribution for a multiple-step forecast is
presented. The evaluation of the proposed algorithm on three synthetic and two
real data sets shows advantage over the compared baselines.
1	Introduction
Application of the deep learning for manufacturing processes has attracted a great attention as one
of the core technologies in Industry 4.0 (Lasi et al., 2014). In many manufacturing processes, e.g.
blast furnace, smelter, and milling, the complexity of the overall system makes it almost impossible
or impractical to develop a simulation model from the first principles. Hence, system identification
from sensor observations has been a long-standing research topic (Wang et al., 2016). Still, when
the observation is noisy and there is no prior knowledge on the underlying dynamics, there is only a
very limited number of methods for the reconstruction of nonlinear dynamics.
In this work, we consider the following class of problems, where the system is driven by a complex
underlying dynamical system, e.g.,
d∂y = F(y(t),y(t - T), UW).
(1)
Here, y(t) is a continuous process, F is a nonlinear operator, τ is a delay-time parameter, and
u(t) is an exogenous forcing, such as control parameters. At time step t, we then observe a noisy
measurement ofy(t) which can be defined by the following noise model
y = y(t)νt + et,
(2)
where νt is a multiplicative and t is an additive noise process. In (1) and (2), we place no assumption
on function F, do not assume any distributional properties of noises νt and t , but assume the
knowledge of the control parameters u(t).
Since the noise components, Vt and j, are stochastic processes, the observation yt is a random
variable. In this work, we are interested in computing temporal evolution of the probability den-
Sity function (PDF) of y, given the observations UP to time step t, i.e., p(yt+n|Y0：t, U^t+n-ι)
for n ≥ 1, where Y0：t = (y0, ∙∙∙ ,yt) is a trajectory of the past observations and U0t+n-1 =
(u0,…,ut+n-ι) consists of the history of the known control actions, Uo：t—i, and a future control
scenario, Ut:t+n-1. We show, in Section 3, a class of problems, where simple regression problem
of forecasting the value of yt+n is not sufficient or not possible, e.g., chaotic systems. Note that the
computation of time evolution of a PDF has been a long-standing topic in statistical physics. For
a simple Markov process, there are well-established theories based on the Fokker-Planck equation.
However, it is very difficult to extend those theories to a more general problem, such as delay-time
dynamical systems, or apply it to complex nonlinear systems.
1
Under review as a conference paper at ICLR 2018
Modeling of the system (1) has been extensively studied in the past, in particular, under the linearity
assumptions on F and certain noise models, e.g., Gaussian t and νt = 1 in (2). The approaches
based on auto-regressive processes (LutkePohL 2005) and Kalman filter (Harvey, 1990) are good
examples. Although these methods do estimate the predictive probability distribution and enable the
computation of the forecast uncertainty, the assumptions on the noise and linearity in many cases
make it challenging to model real nonlinear dynamical systems.
Recently, a nonlinear state-space model based on the Gaussian process, called the Gaussian Process
State Space Model (GPSSM), has been extended for the identification of nonlinear system (Frigola
et al., 2013; Eleftheriadis et al., 2017). GPSSM is capable of representing a nonlinear system and
is particularly advantageous when the size of the data set is relatively small that it is difficult to
train a deep learning model. However, the joint Gaussian assumption of GPSSM may restrict the
representation capability for a complex non-Gaussian noise.
A recent success of deep learning created a flurry of new approaches for time series modeling and
prediction. The ability of deep neural networks, such as RNN, to learn complex nonlinear spatio-
temporal relationships in the data enabled these methods to outperform the classical time series
approaches. For example, in the recent works of Qin et al. (2017); Hsu (2017); Dasgupta & Osogami
(2017), the authors proposed different variants of the RNN-based algorithms to perform time series
predictions and showed their advantage over the traditional methods. Although encouraging, these
approaches lack the ability to estimate the probability distribution of the predictions since RNN is a
deterministic model and unable to fully capture the stochastic nature of the data.
To enable RNN to model the stochastic properties of the data, Chung et al. (2015) augmented RNN
with a latent random variable included in the hidden state and proposed to estimate the resulting
model using variational inference. In a similar vein, the works of Archer et al. (2015); Krishnan
et al. (2017) extend the traditional Kalman filter to handle nonlinear dynamics when the inference
becomes intractable. Their approach is based on formulating the variational lower bound and opti-
mizing it under the assumption of Gaussian posterior.
Another recent line of works enabled stochasticity in the RNN-based models by drawing a connec-
tion between Bayesian variation inference and a dropout technique. In particular, Gal & Ghahramani
(2016) showed that the model parameter uncertainty (which then leads to uncertainty in model pre-
dictions), that traditionally was estimated using variational inference, can be approximated using
a dropout method (a random removal of some connections in the network structure). The predic-
tion uncertainty is then estimated by evaluating the model outputs at different realizations of the
dropout weights. Following the ideas of Gal & Ghahramani (2016), Zhu & Laptev (2017) proposed
additional ways (besides modeling the parameter uncertainty) to quantify the forecast uncertainty in
RNN, which included the model mis-specification error and the inherent noise of the data.
1.1	Overview of the proposed work
We propose an RNN-model to compute the temporal evolution of a PDF, p(yt+n|Y0：t, U1t+n-1).
To avoid the difficulties in directly estimating the continuous function, we use a numerical dis-
cretization technique, which converts the function approximation problem to a classification task
(see Section 2.2). We note that the use of the traditional cross-entropy (CE) loss in our formulated
classification problem can be problematic since it is oblivious to the class ordering. To address this,
we additionally propose two regularizations for CE to account for a geometric proximity between
the classes (see Sections 2.2.1 and 2.2.2). The probability distribution of one-step-ahead prediction,
p(yt+1|Y0：t, U0:t) can now be simply estimated from the output softmax layer of RNN (see Section
2.2), while to propagate the probability distribution further in time, for a multiple-step forecast, we
propose a sequential Monte Carlo (SMC) method (see Section 2.4). For clarity, we present most
derivations for univariate time series but also show the extension to multivariate data in Section 2.3.
We empirically show that the proposed modeling approach enables us to represent a continuous PDF
of any arbitrary shape, including the ability to handle the multiplicative data noises in (2). Since the
probability distribution is computed, the RNN-model can also be used for a regression task by com-
puting the expectation (see Section 2.4). Hereafter, we use DE-RNN for the proposed RNN model,
considering the similarity with the density-estimation task.
In summary, the contributions of this work are as follows: (i) formulate the classical regression
problem for time series prediction as a predictive density-estimation problem, which can be solved
2
Under review as a conference paper at ICLR 2018
by a classification task (ii) propose an approach to compute the time evolution of probability distri-
bution using SMC on the distributions from DE-RNN (iii) proposed two regularizations for CE loss
to capture the ordering of the classes in the discretized PDF. We evaluated the proposed algorithm
on three synthetic and two real datasets, showing its advantage over the baselines. Note that DE-
RNN has a direct relevance to a wide range of problems in physics and engineering, in particular,
for uncertainty quantification and propagation (Zhang & Karniadakis, 2017).
2	LSTM for Noisy Dynamical System
In this Section we present the details of the proposed algorithm using a specific form of RNN,
called Long Short-Term Memory (LSTM) network. Although in the following presentation and
experiments we used LSTM, other networks, e.g., GRU (Chung et al., 2014), can be used instead.
2.1	Review of Long Short-Term Memory network
The Long Short-Term Memory network (LSTM) (Hochreiter & Schmidhuber, 1997; Gers et al.,
2000) consists of a set of nonlinear transformations of input variables zt ∈ Rm ;
Gating functions: Gi,f,o =4S ◦ L(Zt))	(3)
Internal state:	St	=	(1 — Gf) Θ	st-i +	Gi Θ (φτ	◦ L(Zt)),	(4)
Output:	ht	=	Go st .	(5)
Here, φS and φT , respectively, denote the sigmoid and hyperbolic tangent functions, L is a linear
layer, which includes a bias, st ∈ RNc is the internal state, ht ∈ RNc is the output of the LSTM
network, Nc is the number of the LSTM units, and a Θ b denote a component-wise multiplication.
Interesting observation can be made about equation (4). We can re-write equation (4) as
st+1 = [1 — f(Zt)]st + g(Zt),	(6)
for some functions f and g. With a simple re-scaling, this equation can be interpreted as a first-order
Euler scheme for a linear dynamical system,
ds
= = —f(z)s + g(z).	(7)
dt
Thus, LSTM can be understood as a series expansion, where a complex nonlinear dynamical system
is approximated by a combination of many simpler dynamical systems.
Usually, LSTM network is supplemented by feed-forward neural networks, e.g.,
Zt = Fin(xt, ht-1), Pt+1 = Fout(ht),	(8)
in which xt is the input feature. Using (5), we can denote by Ψe and Ψd a collection of the operators
from input to internal state (encoder) and from internal state to the output P (decoder):
st = Ψe(xt, st-1), Pt+1 = Ψd(st).	(9)
2.2	Discrete Approximation of Probability Density Function
T .1 ∙ 1-1	r'	∙ t .t	-I 1	C	t 1∙	.ι	t∙ .∙	1 l xiʌ 1	/ ʌ	I -Ct- t t ∖
In this Section We first consider the problem of modeling the conditional PDF, p(yt+i|Y0：t, Uo：t).
Although yt+i has a dependence on the past trajectories of both y and u, using the “state space”
LSTM model argument in Section 2.1, the conditional PDF can be modeled as a Markov process
,. .ʌ ________. ,. . . . .. .
p(yt+i|Y0：t, U0：t) = p(yt+ι |yt, ut, st-i) = p(yt+i|st).	(IO)
Hence, to simplify the problem, We consider a task of estimating the PDF of a random variable
y, given an input x, i.e., p(y|x). The obtained results can then be directly applied to the original
problem of estimating p(yt+ι∣st).
Let α = (αo, ∙ ∙ ∙ , ɑκ) denote a set of real numbers, such that a—i < α% for i = 1,…，K, which
defines K disjoint intervals, Ii = (αi-i, αi). Then, a discrete probability distribution can be defined
p(k∣x) = / p(y∣x)dy, for k = 1,...,K,
Ik
(11)
3
Under review as a conference paper at ICLR 2018
where it is clear that p(k∣x) is a numerical discretization of the continuous PDF, p(y∣x). Using the
LSTM from Section 2.1, the discrete probability p(k|x) can be modeled by the softmax layer (P) as
an output of Ψd in (9) such that
p(k|x) = Pk, for k = 1, . . . , K.	(12)
Thus, the original problem of estimating a smooth function, p(y∣x), is transformed into a classifi-
cation problem of estimating p(k|x) in a discrete space. Obviously, the size of the bin, |Ij |, affects
the fidelity of the approximation. The effects of the bin size are presented in Section 3.1. There is a
similarity between the discretization and the idea of Lin et al. (2007). However, it should be noted
that the same discretization technique, often called “finite volume method”, has been widely used in
the numerical simulations of partial differential equations for a long time.
The discretization naturally leads to the conventional cross-entropy (CE) minimization. Suppose we
have a data set, DR = {(yi, χ/ yi ∈ R,Xi ∈ R, and i = 1,..., N}. We can define a mapping
C : R → N+ such that C(y) = k, if y ∈I. Then, DR can be easily converted to a new data set for
target labels, DC = {(ci, yi, xj Ci ∈ N+, yi ∈ R,Xi ∈ R, and i = 1,..., N}, where Ci = C(yi).
DC provides a training data set for the following CE minimization problem,
NK	N
CE = -	δcnk log Pkn = -	log Pcnn.	(13)
n=2 k=1	n=2
Note, however, that the CE minimization does not explicitly guarantee the smoothness of the esti-
mated distribution. Since CE loss function depends only on Pi of a correct label, δcnk, as a result,
in the optimization problem every element Pi, except for the one corresponding to the correct label,
Pcn , is penalized in the same way, which is natural in the conventional classification tasks where a
geometric proximity between the classes is not relevant. In the present study, however, the softmax
layer, or class probability, is used as a discrete approximation to a smooth function. Hence, it is
expected that Pcn and Pcn ±1 (i.e., the nearby classes) should be close to each other. To address this
issue, in the following Sections 2.2.1 and 2.2.2, we propose two types of regularization to impose
the class proximity structure in the CE loss.
2.2.1	Explicit Regularization of Cross-Entropy Loss
To explicitly impose the smoothness between the classes, we propose to use a regularized cross-
entropy (RCE) minimization, defined by the following loss function
N
RCE=
n=2
-δcnklogPkn + λ (LPn)T LPn ,
(14)
where λ is a penalty parameter and the Laplacian matrix L ∈ RK-2,K is
1	-2	1	0	…0-
0	1	-2	1	…0
(15)
0 …0	1 -2 1
RCE is analogous to the penalized maximum likelihood solution for density estimation (Silverman,
1986). Assuming a uniform bin size, |Io| = …=|Ik | = δy, the Laplacian of a distribution
can be approximated by a Taylor expansion p00(y∣χ)∣y=ɑi-1∕2 ' (Pi-ι - 2pi + Pi+ι)∕δy2, where
ai-1∕2 = 0.5(αi-ι + αi). Then, it is clear that
(LPnT LPn 〜/ [P00(y∣χ)]2 dy.	(16)
In other words, RCE aims to smooth out the distribution by penalizing local minima or maxima.
2.2.2	Implicit Regularization of Cross-Entropy Loss
Alternative to adding an explicit regularization to CE, the smoothness can be achieved by enforcing
a spatial correlation in the network output. Here, we use an one-dimensional convolution layer to
4
Under review as a conference paper at ICLR 2018
enforce smoothness. Let oe ∈ RK denote the last layer of DE-RNN, which was the input to the
softmax layer.	We can add a convolution layer, o ∈ RK, on top of oe, such that
_ V`λ 1	1 ii - j V 〜 F . _ 1	N	z1-7λ
oi = ʌ, hexp - 2 I 九 I oj, for i = 1,…，K,	(17)
j=1
where the parameter h determines the smoothness of the estimated distribution. Then, o is supplied
to the softmax layer. Using (17), DE-RNN can now be trained by the standard CE. The implicit
regularization, here we call convolution CE (CCE), is analogous to a kernel density estimation.
2.3	Multivariate Time Series
In the modeling of multivariate time series, it is usually assumed that the noise is independent, i.e.,
the covariance of the noise is a diagonal matrix. In this case, it is straightforward to extend DE-
RNN, by using multiple softmax layers as the output of DE-RNN. However, such an independent
noise assumption significantly limits the representative capability of an RNN. Here, we propose to
use a set of independently trained DE-RNNs to compute the joint PDF of a multivariate time series.
Let ^t be a l-dimensional multivariate time series; yt = (y(1), ∙ ∙ ∙ , y(l)). The joint PDF can be
represented by a product rule,
P(yt+1) = P (y(l)ιly(+ι1), …,y(+)ι) P (y(l-L1)ly(l-L2), …,y(+)ι)…P (y(+)ιly(+)ι) P (y(+)ι),
where the dependency on the past trajectory (Y0:t, U0:t) is omitted in the notation for simplicity.
Directly learning thejomt PDF, p(yt+i|Y0：t, Uιt), in a tensor product space is not scalable. Instead,
a set of DE-RNN is trained to represent the conditional PDFs shown on the right hand side of the
above expression. Then, the joint PDF can be computed by a product of the Softmax outputs of the
DE-RNNs. Note that, although it requires training l DE-RNNs to compute the full joint PDF, there is
no dependency between the DE-RNNs in the training phase. So, the set of DE-RNNs can be trained
in parallel, which can greatly reduce the training time. The details of the multivariate DE-RNN are
explained in Appendix A.
2.4	Computing Time Evolution of Probability Distribution
The inputs to a DE-RNN are (yt, ut), and the output is the probability distribution,
Pt+1 = Ψd(st) = Ψd ◦ Ψe(yt, Ut, St-1).
Note that DC is used only in the training stage. Then, the moments of the predictive distribution
can be easily evaluated, e.g.,
E[yt+1∣Ylt, U0:t] = αT∕2Pt+1, Var[yt+1 |Y0:t, U0:t] = (α2∕2)τPt+1 - E[yt+1∣Ylt, U0:t]2,
(18)
where a” = (α1∕2,α1+1∕2,…，ακ7评,α2∕2 = a“ Θ a”，and a”" = 0.5(αi-ι + α0.
Next, we consider a multiple-step forecast, which corresponds to computing a temporal evolution of
the probability distribution, i.e., p(yt+n|Y0：t, U0∙,t+n-1) for n > 1. For simplicity, the multiple-step
forecast is shown only for a univariate time series. An extension to a multivariate time series is
straightforward (Appendix A).
Applying the results of Section 2.2, once the distribution of yt+1 in (10) is computed, the distri-
bution of yt+2 can be similarly obtained as p(yt+2∣St+1). Observe that St+1 is computed from a
deterministic function of st, ut+ι, and yt+1, i.e.,
st+1 = ψe(yt+1, ut+1, st).
Here, ut+1 and St are already known, while yt+1 is arandom variable, whose distributionp(yt+ι |st)
is computed from the deterministic function Ψd(st). Then, st+1 is also a random variable. The
distribution, p(st+1 |st, ut+1), can be obtained by applying a change of variables on p(yt+1∣st) with
5
Under review as a conference paper at ICLR 2018
a nonlinear mapping Ψe. Repeating this process, the multiple-step-ahead predictive distribution can
therefore be computed as
n-1
∙∙∙ I p(yt+n |st+n-1) ɪ ɪ p(st+i |st+i-1, ut+i) dst+i ∙	(19)
i=1
Since the high dimensional integration in (19) is intractable, we propose to approximate it by a
sequential Monte Carlo method. The Monte Carlo procedure is outlined in Algorithm 1.
Algorithm 1 Sequential Monte Carlo method for LSTM multi-step-ahead prediction
Input: Y0:t, U0:t, number of Monte Carlo samples, Ns , and forecast horizon n
Output： p(yt+n|Y0：t, Ust+n-1) (density estimation from ^t+n)
Initialization: Set LSTM states to s0 = h0 = 0
Perform a sequential update of LSTM up to time t from the noisy observations (Y0:t).
Si = Ψe(yi, Ui, Si-1) for i = 1,…，t.
Make Ns replicas of the internal state, s1 =…=SNs = st.
repeat
Compute the predictive distribution of yi+ι for each sample
Pti+1 = Ψd(si), for i = 1, ∙∙∙ ,Ns.
Sample the target variable at t + 1, yi+ι, from the distribution
1.	Sample the class label from the discrete distribution: Ci 〜 Pti+ι
2.	Sample yi+ι inIci yi+ι ~U(。。「"。”)
Update the internal state of LSTM
si+1 = ψe(yi+1, ut+1, Si).
until (all ^t+n are sampled)
3	Experiments
In this section, DE-RNN is tested against three synthetic and two real data sets. The LSTM archi-
tecture used in all of the numerical experiments is identical. Two feed-forward networks are used
before and after the LSTM;
Zt = L WT ◦ L(yt, Ut) + ht-1),	Pt+1 =中SM ◦ L(ψt ◦ L(ψsp ◦ L(ht))),	(20)
in which WSP and 夕SM denote the softplus and softmax functions, respectively. The size of the bins
is kept uniform, i.e., |Ii| = •… =|Ik | = δy. The LSTM is trained by using ADAM (Kingma &
Ba, 2015) with a minibath size of 20 and a learning rate of η = 10-3.
3.1	Cox-Ingersoll-Ross process
First, we consider a modified Cox-Ingersoll-Ross (CIR) process, which is represented by the fol-
lowing stochastic differential equation,
dy(t) = -0.5y(t)dt +，0.5 + |y(t)| dW,
(21)
in which W is the Weiner process. The original CIR process is used to model the valuation of
interest rate derivatives in finance. Equation (21) is solved by the forward Euler method with the
time step size δt = 0.1. The simulation is performed for T = (0, 160000]δt to generate the training
data and T = (160000, 162000]δt is used for the testing. Note that the noise component of CIR is
multiplicative, which depends on y(t).
6
Under review as a conference paper at ICLR 2018
Figure 1: NRMSE of the next-step prediction of the CIR process by RCE (a,b) and CCE (c,d). The
bin size is (a,c) δy = 0.08 and (b,d) 0.04. The hollow circles (◦) denote NRMSE in the expectation
(eμ) and the solid circles (•) are the standard deviation (eσ).
Table 1: NRMSE of the mean and standard deviation of the next-step prediction. The DE-RNN re- sults are compared with the first-order autoregressive model (AR), Kalman filter (KF), and Gaussian process (GP).						
	CE	RCE	CCE	AR(1)	KF	GP
eμ	0.238	0.0549	0.149	0.029	0.029	0.831
eσ	0.066	0.017	0.038	0.228	0.228	0.095
The experiments are performed for two different bin sizes, dy = 0.08 and 0.04. The DE-RNN has
64 LSTM cells. Figure 1 shows the errors in the expectation and the standard deviation with respect
to the analytical solution;
Ey~pτ [yt+ι |yt ] = yt eχp(-0.5δt),	Sdy 〜PT[yt+1 |yt ] = √(0.5+ ∣yt∣)δt.	(22)
Here, pT denotes the true distribution of the CIR process. The normalized root mean-square errors
(NRMSE) are defined as
h(Ey
~PL [yt+1 |yt] - Ey〜PT [yt+1|yt])2i1/2
h(yt- Ey 〜PT [yt+1 |yt])2 i1/2
h(sdy
~Pl [yt+ι|yt] - Sdy〜PT[yt+1|yt])2i1/2
sd[y]
(24)
in which〈•〉denotes an average over the testing data, PL is the distribution from the LSTM, and
Sd[y] denotes the standard deviation of the data. The error in the expectation is normalized against
a zeroth-order prediction, which assumes yt+1 = yt .
In Figure 1, it is clearly shown that the prediction results are improved when a regularization is used
to impose a smoothness condition. Comparing Figures 1 (a) and (b), for RCE, eμ and eσ become
smaller when a smaller δy is used. As expected, eσ increases when λ is large. But, for the smaller
bin size, δy = 0.04, both eμ and eσ are not so sensitive to λ. Similar to RCE, eμ and eσ for CCE
decrease at first as the penalty parameter h increases. However, in general, RCE provides a better
prediction compared to CCE.
NRMEs are listed in Table 1. For a comparison, the predictions by AR(1) and KF are shown. The
CIR process is essentially a first-order autoregressive process. So, it is not surprising to see that
AR(1) and KF, which are designed for the first-order AR process, outperforms DE-RNN for the
prediction of the expectation. However, eσ of AR(1) and KF are much larger than that of DE-RNN,
because those models assume an additive noise. The Gaussian process (GP) model has a relatively
large eμ. But, GP outperforms AR(1) and KF in the prediction of the noise (eσ). Still, eσ of RCE
and CCE are less than 4%, while that of GP is about 10%, indicating that DE-RNN can model the
complex noise process much better.
In Figure 2, a 200-step forecast by DE-RNN is compared with a Monte-Carlo solution of equation
(21). DE-RNN is trained with δy = 0.04 and λ = 200. For the DE-RNN forecast, the testing
data is supplied to DE-RNN for the first 100 time steps, i.e., for t = -10 to t = 0, and the SMC
multiple-step forecast is performed for the next 200 time steps with 20,000 samples. It is shown that
the multiple-step forecast by DE-RNN agrees very well with the MC solution of the CIR process.
Note that, in Figure 2 (b), the noise process, as reflected in Sd[yt], is a function of yt, and hence the
multi-step forecast of the noise increases rapidly first and then decreases before reaching a plateau.
7
Under review as a conference paper at ICLR 2018
Figure 2: 200-step forecast of (a) expectation and (b) standard deviation of the CIR process. The
circles denote the solution of Eqn (21) from a Monte Carlo method with 107 samples.
Figure 3: (a) Next-step prediction (•) and the noisy observation (◦) for the Mackey-Glass equation.
The solid line denotes the ground truth, y(t). (b) The next-step probability distribution, p(yt+1 ∣yt),
from the standard CE (◦) and CCE (•) with h = 5.
The SMC forecast can accurately capture the behavior. Such kind of behavior can not be represented
if a simple additive noise is assumed.
3.2	Mackey-Glass time series
For the next test, we applied DE-RNN for a time series generated from the Mackey-Galss equation
(Mackey & Glass, 1977);
dy = α (t-T) -7 n(t)
dt 1+ yβ (t- T) γy(t)∙
(25)
We use the parameters adopted from Gers (2001), α= 0.2, β = 10, γ = 0.1, and τ = 17.
The Mackey-Glass equation is solved by using a third-order Adams-Bashforth method with a time
step size of 0.02. The time series is generated by down-sampling, such that the time interval between
consecutive data is δt = 1. A noisy observation is made by adding a white noise;
yt = yt + 以.
t is a zero-mean Gaussian random variable with the noise level sd[t] = 0.3sd[y]. A time series of
the length 1.6 × 105δt is generated for the model trainig and another time series of length 2 × 103δt
is made for the validation. DE-RNN is trained for δy = 0.04sd[y] and consists of 128 LSTM cells.
Figure 3 (a) shows the noisy observation and the expectation of the next-step prediction, E [yt+1 ∣yt],
in a phase space. It is shown that DE-RNN can filter out the noise and reconstruct the original dy-
namics accurately. Even though the noisy data are used as an input, E [yt+1 ∣yt] accurately represents
the original attractor of the chaotic system, indicating a strong de-noising capability of DE-RNN.
The estimated probability distribution is shown in Figure 3 (b). Without a regularization, the stan-
dard CE results in a noisy distribution, while the distribution from CCE shows a smooth Gaussian
shape.
8
Under review as a conference paper at ICLR 2018
Table 2: NRMSEs of the Mackey-Galss time series. DE-RNN results are compared with autoregres-
SiVe integrated moving average (ARIMA), Kalman filter (KF), and GauSSian process (GP) models.
	λ=0	λ=50	RCE λ=100	λ=200	CCE h = 5 h = 10		ARIMA	KF	GP
eμ	0.198	0.196	0.198	0.196	0.185	0.198	0.662	0.903	0.327
eσ	0.032	0.023	0.027	0.038	0.013	0.020	0.191	0.351	0.048
Figure 4: (a) 500-Step forecast by a regression LSTM (◦) and the ground truth (—). (b) The color
contours denote a 500-step forecast of the probability distribution, p(yn+s ∣ys), and the dashed lines
are 95%-CL The ground truth is shown as the solid line (一).
The prediction errors are shown in table 2. NRMSEs are defined as,
h(E 血+ι 咧-yt+ι 1 2'/
h(yt - yt+ι)2i1/2
sd[yt+1 |yt] - 1
sd[et ]
(26)
NRMSES are computed with respect to the ground truth. Again, eμ compares the prediction error
to the zeroth-order prediction. In this example, the errors are not so sensitive to the regularization
parameters. The best result is achieved by CCE. DE-RNN can make a very good estimation of the
noise. The error in the noise component, eσ, is only 2% 〜5%. Unlike the CIR process, NRMSEs
from KF and ARIMA are much larger than those of DE-RNN. Because the underlying process is
a delay-time nonlinear dynamical system, those linear models can not accurately approximate the
complex dynamics. Since GP is capable of representing a nonlinear behavior of data, GP outper-
forms KF and ARIMA both in eμ and eσ. In particular, eσ of GP is similar to that of DE-RNn.
However, eμ of GP is about 1.5 times larger than DE-RNN.
A multiple-step forecast of the Mackey-Glass time series is shown in Figure 4. In the validation time
series, the observations in t ∈ [1, 100]δt are supplied to the DE-RNN to develop the internal state,
and a 500-step forecast is made for t ∈ [101, 600]δt. In Figure 4 (a), it is shown that a multiple-step
forecast by a standard regression LSTM approximates y(t) very well initially, e.g, for t < 80δt, but
eventually diverges for larger t. Because of the Mackey-Glass time series is chaotic, theoretically
it is impossible to make a long time forecast. But, in the DE-RNN forecast, y(t) is bounded by the
95%-confidence interval (CI) even for the 500-step forecast. Note that the uncertainty, denoted by
95%-CI grows only at a very mild rate in time. In fact, it is observed that CI is not a monotonic
function of time. In DE-RNN, the 95%-CI may grow or decrease following the dynamics of the
system, while for the conventional time series models, such as ARIMA and KF, the uncertainty is a
non-decreasing function of time.
3.3	Mauna Loa CO2 observation
In this experiments, DE-RNN is tested against the atmospheric CO2 observation at Mauna Loa Ob-
servatory, Hawaii (Keeling et al., 2001). The CO2 data set consists of weekly-average atmospheric
CO2 concentration from Mar-29-1958 to Sep-23-2017 (Figure 5 a). The data from Mar-29-1958 to
Apr-01-2000 is used to train DE-RNN and a 17-year forecast is made from Apr-01-2000 to Sep-23-
2017. This CO2 data has been used in previous studies (Gal & Ghahramani, 2016; Rasmussen &
Williams, 2006). In DE-RNN, 64 LSTM cells and δy = 0.1sd[dyt], in which dyt = yt+1 - yt, are
used.
9
Under review as a conference paper at ICLR 2018
Figure 5: (a) Mauna Loa CO2 observation. The vertical line denotes the boundary of the training
and testing data. (b) 17-year forecast of the CO2 concentration by DE-RNN: Apr-01-2000 Sep-23-
2017. (c) 17-year forecast by Gaussian processes. The solid and dashed lines denote the expectation
and 95%-CI, respectively. The observation is shown as the solid circles (•) and a regression LSTM
is shown as the hollow circles (◦) in (b). The time unit is a week.
Table 3: l∞ error for RCE, CCE and regression LSTM in ° C.
RCE CCE LSTM
IIE[yn+s∣ys]- yn+sk∞	4.61	2.70	9.48
The 17-year DE-RNN forecast, with 1,000 MC samples, is shown in Figure 5 (b). DE-RNN well
represents the growing trend and the oscillatory patten of the CO2 data. The CO2 data is non-
stationary, where the rate of increase of CO2 is an increasing function of time. Since DE-RNN
is trained against the history data, where the rate of CO2 increase is smaller than the current, it
is expected that the forecast Win underestimate the future CO2. E [yn+s ∣ys ] agrees very Wen with
the observation for the first 200 weeks, but eventually underestimates CO2 concentration. It is
interesting to observe that the upper bound of the 95%-CI groWs more rapidly than the expectation
and provides a good approximation of the observation. For a comparison, the forecast by a regression
LSTM is also shoWn. Similar to the chaotic Mackey-Glass time series, the regression LSTM makes a
good prediction for a short time, e.g., t < 100 Weeks, but eventually diverges from the observation.
Note that the loWer bound of 95%-CI encompasses the regression LSTM. Figure 5 (c) shoWs a
forecast made by GP, folloWing setup suggested by Rasmussen & Williams (2005). For a short-
term forecast (< 50 Weeks), GP provides a sharper estimate of the uncertainty, i.e., a smaller 95%-
CI interval. However, even for a mid-term forecast, 100 〜600 weeks, the ground truth is close
or slightly above the upper bound of 95%-CI. Because of the different behaviors, it is difficult to
conclude which method, DE-RNN or GP, provides a better forecast. But, it should be noted that
the GP is based on a combination of handcrafted kernels specifically designed for this particular
problem, while such careful tuning is not required for DE-RNN.
3.4	CPU temperature forecast
In the last experiment, IBM Power System S822LC and NAS Parallel Benchmark (NPB) are used to
generate the temperature trace. Figure 6 (a) shows the temperature ofa CPU. The temperature sensor
generates a discrete data, which has a resolution of 1°C. The CPU temperature is controlled by three
major parameters; CPU frequency, CPU utilization, and cooling fan speed. In this experiment, we
have randomized the CPU frequencies and job arrival time to mimic the real workload behavior,
while the fan speed is fixed to 3300RPM. The time step size is δt = 2 seconds. Accurate forecast
of CPU temperature for a future workload scenario is essential in developing an energy-efficient
control strategy for the thermal management of a cloud system.
Figure 6 (c) and (d) show multiple-step forecasts of the CPU temperature by RCE and a regres-
sion LSTM, respectively. The bin size is δy = 0.18°C, which is smaller than the sensor resolu-
tion. In the forecast, the future control parameters are given to DE-RNN. In other words, DE-RNN
predicts the probability distribution of future temperature with respect to a control scenario, i.e.,
.—-
p(yt+n |Y0:t, U0:t+n-1). The forecast is made by using 5,000 Monte Carlo samples. Here, 1800-step
forecast is made, t = 0 〜3, 600 sec. and only the results in t ∈ (50,1800) sec. is shown. While
the regression LSTM makes a very noisy prediction near the local peak temperature at t ' 500,
RCE provides a much more stable forecast. Table 3 shows the l∞-errors, i.e., maximum absolute
10
Under review as a conference paper at ICLR 2018
-25000	-20∞0	-15000	-10000	-5∞0	O	5000
time (sec.)
-25000	-200∞	-15000	-10000	-5000	0	5000
time (sec.)
Figure 6: (a) Temperature of a CPU in ◦C. (b) Control parameters; CPU utilization (black) and
Clock speed (blue). Multiple-step forecasts by (c) RCE and (d) regression LSTM. The solid and
dashed lines in (C) denote E[yn+s∣ys] and 95%-CL respectively. The circles are the observations.
difference. The maximum error is observed near the peak temperature at t ' 500. ARIMA, KF,
and GP are also tested for the multiple-step forecast, but the results are not shown because their
performance is much worse than the LSTMs. The changes in the temperature are associated with
step changes of some control parameters. Such abrupt transitions seem to cause the large oscillation
in the regression LSTM prediction. But, for RCE or CCE, the prediction is made from an ensem-
ble of Monte Carlo samples, which makes it more robust to such abrupt changes. Also, note that
for t < 200 sec., RCE prediction (' 53.4°C) is in between the two discrete integer levels, 53°C
and 54° C, which correctly reflects the uncertainty in the measurement, while the regression LSTM
(' 53.1°C) more closely follows one of the two levels.
3.5	Lorenz Time Series
Finally, in this experiment we evaluate the performance of the multivariate DE-RNN, for which we
used a noisy multivariate time series generated by the Lorenz equations (Lorenz, 1963)
dy(1)
F = α1
(y(2) - y(1)),
Cddr = y ⑴ 3 - y ⑶)-y ⑵,
dy7^ = y ⑴ y ⑵-α3 y ⑶.
dt
(27)
We used the coefficients from Lorenz (1963), which are α1 = 10, α2 = 8/3, and α3 = 28. The
system of equations (27) is solved by using a third-order Adams-Bashforth method with a time step
size of 0.001 and a time series data set is generated by downsampling, such that δt = 0.02. A
multivariate joint normal distribution is added to the ground truth to generate a noisy time series,
i.e.,
y(t)^∖	∕y(1∖	(σ2 0∙25σι σ2	-0.25σι σ3 ʌ
y(2)I = I y(2)I +。, J = N(0, ∑), and Σ =	σ2	-0.25σ2σ3 .	(28)
W)/	U"	V	σ2 )
Here, the noise level is set to σ1 = 0.2sd[y (1)], σ2 = 0.3sd[y (2)], and σ3 = 0.2sd[y (3)]. Three
DE-RNNS are trained to model p(y(+)ι|Y^o：t), p(y(+)ι∣y(+)ι,凡力)，and p(y(+)ι∣y(+)ι, y(+)ι,凡力)，re-
11
Under review as a conference paper at ICLR 2018
Table 4: Normalized errors of the Lorenz time series. DE-RNN results are compared with vector
autoregressive (VAR) and Gaussian process (GP) models.
	eμ	eσ	e∑12	eΣ13	eΣ23
CE	0.19	0.027	0.146	-0.012	0.024
RCE	0.20	0.030	0.155	-0.010	0.026
CCE	0.20	0.018	0.123	-0.009	0.022
VAR	1.00	0.318	1.485	0.483	0.649
GP	0.58	0.296	-	-	-
Figure 7: Multiple-step forecast of the Lorenz time series by DE-RNN trained with CCE. The solid
and dashed lines denote the expectation and 95%-CI, respectively. The ground truth is shown as
sold circles.
SPeCtiVely, with the grid spaces δy(i) = 0.02sd[y(i)] for i = 1, 2, 3. Each DE-RNN has 128 LSTM
cells. The penalty parameters for RCE and CCE are λ = 50 and h = 5, respectively.
Figure 4 shows normalized errors for the next-step prediction. The normalized errors are defined as
1 X h(E[y(+ι] - y(+1)2i	1 X Var[y(+ι]	1	Cov[yt+-ι, y(j) 1]	1
eμ = t iʌ丁池一百丁，eσ = t iʌ -浮--------------------1, 3j =------之--------1.
\ 3 4 i=ι	h(yt - yt+1)2i	\ 3 i=ι	σi	ςij
(29)
The moments of the joint PDF are computed by the Monte Carlo method with a sample size of 5 ×
103 . It is shown that DE-RNN makes a very good prediction of both expectations and covariances.
The error in the covariance is less than 4% except for Σ12 . It is shown that DE-RNN is able to
make correct predictions of not only the magnitude, but also the signs of the covariance. A vector
autoregressive model (VAR) also predicts the signs of the covariance correctly. But, the errors in the
expectation and covariances are much larger than DE-RNN. The GP used in the experiment assumes
an independent noise, i.e. Σ = ρ2I. Hence, eΣij is not evaluated for GP. Similar to the Mackey-
Glass time series, GP outperforms VAR, but the errors are larger than DE-RNN. The error in eσ is
about 10 times larger than DE-RNN, while eμ of GP is about 3 times larger than DE-RNN.
Figure 7 shows 300-step forecasts of the Lorenz time series. The expectations from DE-RNNs make
a good prediction of the ground truth up to t - s < 1.5. Then, the expectations start to diverge
from the ground truth, which is accompanied by a sudden increase in the 95%-CI. For a longer-time
forecast, e.g., t - s > 4, the 95%-CI exhibits an oscillatory patten depending on the dynamics of the
Lorenz system and well captures the oscillations in the ground truth.
4	Concluding remarks
We present DE-RNN to compute the time evolution of a probability distribution for complex time
series data. DE-RNN employs LSTM to learn multiscale, nonlinear dynamics from the noisy ob-
servations, which is supplemented by a softmax layer to approximate a probability density function.
To assign probability to the softmax output, we use a mapping from R to N+ , which leads to a
cross-entropy minimization problem. To impose a geometric structure in the distribution, two regu-
larization strategies are proposed. The regularized cross-entropy method is analogous to the penal-
ized maximum likelihood estimate for the density estimation, while the convolution cross-entropy
method is motivated by the kernel density estimation. The proposed algorithm is validated against
12
Under review as a conference paper at ICLR 2018
three synthetic data set, for which we can compare with the analytical solutions, and two real data
sets.
References
E. Archer, M. Park, L. Buesing, J. Cunningham, and L. Paninski. Black box variational inference
for state space models. arXiv preprint arXiv:1511.07367, 2015.
J. Chung, C. Gulcehre, K. Cho, and Y. Bengio. Empirical evaluation of gated recurrent neural
networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.
J. Chung, K. Kastner, L. Dinh, K. Goel, A. Courville C, and Y. Bengio. A recurrent latent variable
model for sequential data. In Advances in Neural Information Processing Systems, pp. 2980-
2988, 2015.
S. Dasgupta and T. Osogami. Nonlinear dynamic boltzmann machines for time-series prediction. In
AAAI Conference on Artificial Intelligence, 2017.
S. Eleftheriadis, T. F. W. Nicholson, M. P. Deisenroth, and J. Hensman. Identification of Gaussian
process state space models. In Advances in Neural Information Processing Systems, 2017.
R. Frigola, F. Lindsten, T. B. Schon, and C. E. Rasmussen. Bayesian inference and learning in
Gaussian process state-space models with particle MCMC. In Advances in Neural Information
Processing Systems, pp. 3156-3164, 2013.
Y. Gal and Z. Ghahramani. Dropout as a Bayesian approximation: Representing model uncertainty
in deep learning. In Proceedings of the 33rd International Conference on Machine Learning, pp.
1050-1059, 2016.
F. A. Gers. Ph.D Thesis. EPFL, 2001.
F. A. Gers, J. Schmidhuber, and F. Cummins. Learning to forget: Continual prediction with LSTM.
Neural Comput., 12:2451 - 2471, 2000.
A. C. Harvey. Forecasting, structural time series models and the Kalman filter. Cambridge university
press, 1990.
S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Comput., 9:1735 - 1780, 1997.
D. Hsu. Time series forecasting based on augmented long short-term memory. arXiv preprint
arXiv:1707.00666, 2017.
C.	D. Keeling, S. C. Piper, R. B. Bacastow, M. Wahlen, T. P. Whorf, M. Heimann, and H. A.
Meijer. Exchanges of atmospheric CO2 and 13CO2 with the terrestrial biosphere and oceans from
1978 to 2000. I. Global aspects. In SIO Reference Series, pp. No. 01-06. Scripps Institution of
Oceanography, 2001.
D.	P. Kingma and J. L. Ba. ADAM: A method for stochastic optimization. In 3rd International
Conference on Learning Representation, San Diego, CA, USA, 2015.
R.	Krishnan, U. Shalit, and D. Sontag. Structured inference networks for nonlinear state space
models. In AAAI Conference on Artificial Intelligence, 2017.
H.	Lasi, P. Fettke, H. Kemper, T. Feldand, and M. Hoffmann. Industry 4.0. Business & Information
Systems Engineering, 6(4):239-242, 2014.
J. Lin, E. Keogh, L. Wei, and S. Lonardi. Experiencing SAX: a novel symbolic representation of
time series. Data Mining and knowledge discovery, 15(2):107-144, 2007.
E. N. Lorenz. Deterministic Nonperiodic Flow. J. Atmos. Sci., 20:130-148, 1963.
H. Lutkepohl. New introduction to multiple time series analysis. Springer Science & Business
Media, 2005.
13
Under review as a conference paper at ICLR 2018
M. Mackey and L. Glass. Oscillation and chaos in physiological control systems. Science, 197:
287-289,1977.
Y. Qin, D. Song, H. Chen, W. Cheng, G. Jiang, and G. W. Cottrell. A dual-stage attention-based
recurrent neural network for time series prediction. In Proceedings of the Twenty-Sixth Interna-
tional Joint Conference on Artificial Intelligence, IJCAI-17, pp. 2627-2633, 2017.
C.	E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. 2005.
C.	E. Rasmussen and K. I. Williams. Gaussian Processes for Machine Learning. MIT Press, 2006.
B. W. Silverman. Density estimation for statistics and data analysis. Chapman & Hall, 1986.
W.-X. Wang, Y.-C. Lai, and C. Grebogi. Data based identification and prediction of nonlinear and
complex dynamical systems. Phys. Reports, 644:1-76, 2016.
D.	Xiu and J. S. Hesthaven. High-order collocation methods for differential equations with random
inputs. SIAM J. Sci. Comput., 27:1118-1139, 2005.
Z. Zhang and G. E. Karniadakis. Numerical methods for stochastic partial differential equations
with white noise. Springer, 2017.
L. Zhu and N. Laptev. Deep and confident prediction for time series at Uber. arXiv preprint
arXiv:1709.01907, 2017.
14
Under review as a conference paper at ICLR 2018
Appendix A	Multivariate DE-RNN
Recall the product rule from Section 2.3
P(yt+ι) = P (y(+ιly(+ι1), …,y(+)ι) P (y(+ιI)Iy(+i2),…,y(+)ι)…P (y(+)ιly(+)ι) P (y(+)ι).
The extension of DE-RNN algorithm for univariate data to an l-dimensional multivariate time series
is straightforward. First, define the discretization grid points, α(i) = (α0i),…，ɑg), for every
variable, i.e., i = 1,…，l. Here, Ki is the number of the discretization intervals for the i-th variable.
Then, we can define the discretization intervals, Ij(i) = (αj(i-) 1, αj(i)), and the mapping functions,
C(i) (y(i)), for the l variables.
The first component of the product rule is the marginal PDF, p(y(+∖). Hereafter, the obvious de-
pendency on Y0:t , U0:t in the notation is omitted for simplicity. The marginal PDF can be com-
puted by the same method as for the univariate time series. To train DE-RNNs for the condi-
tional PDFs for the i-th variable, p(y(+∕y(i-LI), .…y(+)ι), the original time series data of length
N, DR = {yt; ryt ∈ Rl, and t = 1,...,N}, is discretized by using C(i), which gives us
DCi = {(c(i), yt); Cf) ∈ N+, yt ∈ Rl, and t = 1,...,N}, where Cf) = C(i)(y(i)). Then the
output softmax layer, Pt(+i)1, is computed by an LSTM as
Pt+)1 = Ψd ◦ Ψe (y(+11),…，y(+)L, yt, Sai)	(30)
in which s(ti) is the internal state of the i-th variable. In other words, in the training of the DE-RNN
for p(y(+ι∣y(i-LI), .…y(+)ι), the input vector is the variables at the current time step, ^t, combined
with the conditioning variables in the next step, (y(+∖,…,y(i-L1)), and the target is the class label,
(i)
Ct+L, in the next time step. The DE-RNN can be trained by minimizing RCE or CCE as described in
Section 2.2. Observe that during the training phase, each DE-RNN is independent from each other.
Therefore, all the DE-RNNs can be trained in parallel, significantly improving the computational
efficiency, enabling the method to scale only linearly with the number of dimensions l.
Once all the conditional PDFs are obtained, the joint PDF can be computed by a product of the
DE-RNN outputs. For a demonstration, the covariance ofa bivariate time series can be computed as
Cov (y(+)ι, y(+)ι) = ZZ y(+)ι yt++ι P (y(+)i|y(+)i, Y0：t) P (y(+)ιlY0:) dy(+)ι dy(+)ι
Ki (	K2	)
'X α(-)ι∕2 Pi(I) (yt) X aj22ι∕2 Pj⑵(α(-)ι∕2,%,(31)
i=1 I	j=1
where the time index, (t+ 1), is omitted in the notation of the softmax output, Pj(i), and the subscript
j denotes the j-th element of the softmax layer. Note that, although there is no dependency between
DE-RNNs during training, in the prediction phase of computing the joint PDF, there is a hierarchical
dependency between all the DE-RNNs. This kind of direct numerical integration does not scale well
for number of dimensions l 1. For a high dimensional system, a sparse grid (Xiu & Hesthaven,
2005) or a Monte Carlo method can be used to speed up the numerical integration. We outline a
Monte Carlo procedure in Algorithm 2. Comparing with Algorithm 1, an extension of Algorithm 2
for a multiple-step forecast ofa multivariate time series is straightforward.
Appendix B	DE-RNN achitecture
In this Section we present a few extra details of the proposed DE-RNN algorithm. Figure 8 shows
the architecture of DE-RNN as was used during the experiments in Section 3. In Figure 9 we show
the process of computing one-step-ahead predictions of univariate time series as was presented in
Section 2.2. Note that since DE-RNN estimates approximation of the predictive probability distribu-
tion, the predicted value, e.g., for time step t +1, is the discrete approximation of E[yt+ι |Yo：t, Uo：t],
i.e., the expectation of yt+ι given all the observations and control inputs up to time t. Finally, in
15
Under review as a conference paper at ICLR 2018
Algorithm 2 Monte Carlo method for the computation of the joint PDF
Input: Yb0:t, and number of Monte Carlo samples, Ns
Output: p(yt+ι |Y0：t) (density estimation from yt+ι)
Initialization: Set LSTM states to s0 = h0 = 0
Perform a sequential update of LSTM up to time t - 1 from the noisy observations (Y0:t):
for i = 1, t - 1 do
S(I)= ψe(yi, s(-)I)
for j = 2, l do
Sij)= ψe(y(+)i,…,y(+-1), yi, s(j)I)
end for
end for
Monte Carlo sampling forp(yt+i|Y0：t):
Compute the predictive distribution of y；；、
Pt+1 = ψd ◦ ψe(yt, st-)i)
for n = 1, Ns do
Draw a sample, y(+),n, from P；1.
for j = 2, l do
Compute the conditional distribution for y(j)ι for each sample
Pt+11n = Ψd ◦ Ψe(y(+)ιn,…,y(+Jn, yt, St-ι)
Draw a sample, y(j)1n, from Pjn
end for
end for
Figure 10 we show the details of the multi-step forecast for univariate time series as was presented
in Algorithm 1, in Section 2.4. Using sequential Monte Carlo method, the discrete approximation
of the predictive distribution p(yt；n|Y0：t, Uo：t；n—i) is estimated using Ns samples.
16
Under review as a conference paper at ICLR 2018
discrete approximation of ʌ
predictive distribution p(%+i|K：t, Uo：t)
Pt+1 /
y	Ut
Figure 8: Architecture of the DE-RNN algorithm.
E 仅 t+ιMκ U0：t]	E[yt+2∣Yy0:t+1, U0:t+l]
α 2~i
ɑ2
— K
---► X αk-1 Pt+1(k)
k=1
↑
Pt+1
一 ɑo
α 1，，
α,
一 K
----► X αk-1 Pt+2(k)
- k=1	▲
-LaK
Pt+2
↑ ↑ ↑ ↑
yt	Ut	yt+ι ut+ι
Figure 9: Details of the computation for the one-step-ahead predictions. At a given step the model
computes a discrete approximation of the expectation for the next step observation.
17
Under review as a conference paper at ICLR 2018
replicate hidden
states NstimeS
T 3

DE-RNN
y1+3 .
density estimation ʌ
----------► P(yt+3 | Y0:t, U0:t+2 )
DE-RNN
y1+3
sample within
selected bin (uniformly)
-H-M÷H
↑αK
sample over
all bins (using p；，
pt+ι
T
h1
y1+2
↑
HH-I-H-
αo
0K
DE-RNN
αo	[	aK
Pt+2	Pt1+3
□ J □ I I	□ I I
]↑ _______↑ ↑
y1+1 ut+1	y1+2 ut+2
Figure 10: Visualization of the multi-step-ahead forecast made by DE-RNN. The shown illustration
is for computing 3-step-ahead predictive probability distributionp(yt+3|Y0：t, U0：t+2). Ns indepen-
dent DE-RNN instances, i = 1, . . . , Ns, are initialized identically with the hidden states from the
time step of the last observation. Each DE-RNN instance i uses a two-stage sampling procedure to
propagate its own value of yi in time. At the end, Ns samples of yi+3 are used to compute a discrete
approximation of the predictive distribution p(yt+3|Y0：t, U0∙.t+2)
18