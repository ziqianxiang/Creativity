Under review as a conference paper at ICLR 2018
Loss Functions for Multiset Prediction
Anonymous authors
Paper under double-blind review
Ab stract
We study the problem of multiset prediction. The goal of multiset prediction is
to train a predictor that maps an input to a multiset consisting of multiple items.
Unlike existing problems in supervised learning, such as classification, ranking
and sequence generation, there is no known order among items in a target multi-
set, and each item in the multiset may appear more than once, making this prob-
lem extremely challenging. In this paper, we propose a novel multiset loss func-
tion by viewing this problem from the perspective of sequential decision making.
The proposed multiset loss function is empirically evaluated on two families of
datasets, one synthetic and the other real, with varying levels of difficulty, against
various baseline loss functions including reinforcement learning, sequence, and
aggregated distribution matching loss functions. The experiments reveal the ef-
fectiveness of the proposed loss function over the others.
1 Introduction
A relatively less studied problem in machine learning, particularly supervised learning, is the prob-
lem of multiset prediction. The goal of this problem is to learn a mapping from an arbitrary input
to a multiset1 of items. This problem appears in a variety of contexts. For instance, in the con-
text of high-energy physics, one of the important problems in a particle physics data analysis is to
count how many physics objects, such as electrons, muons, photons, taus, and jets, are in a collision
event (Ehrenfeld et al., 2011). In computer vision, automatic alt-text, such as the one available on
Facebook,2 is a representative example of multiset prediction (Welleck et al., 2017; Lempitsky &
Zisserman, 2010).3
In multiset prediction, a learner is presented with an arbitrary input and the associated multiset of
items. It is assumed that there is no predefined order among the items, and that there are no further
annotations containing information about the relationship between the input and each of the items
in the multiset. These properties make the problem of multiset prediction unique from other well-
studied problems. It is different from sequence prediction, because there is no known order among
the items. It is not a ranking problem, since each item may appear more than once. It cannot be
transformed into classification, because the number of possible multisets grows exponentially with
respect to the maximum multiset size.
In this paper, we view multiset prediction as a sequential decision making process. Under this view,
the problem reduces to finding a policy that sequentially predicts one item at a time, while the
outcome is still evaluated based on the aggregate multiset of the predicted items. We first propose an
oracle policy that assigns non-zero probabilities only to prediction sequences that result exactly in
the target, ground-truth multiset given an input. This oracle is optimal in the sense that its prediction
never decreases the precision and recall regardless of previous predictions. That is, its decision is
optimal in any state (i.e., prediction prefix). We then propose a novel multiset loss which minimizes
the KL divergence between the oracle policy and a parametrized policy at every point in a decision
trajectory of the parametrized policy.
1A set that allows multiple instances, e.g. {x, y, x}. See Appendix A for a detailed definition.
2 https://newsroom.fb.com/news/2016/04/using-artificial- intelligence-
to-help-blind-people-see-facebook/
3 We however note that such a multiset prediction problem in computer vision can also be solved as seg-
mentation, if fine-grained annotation is available. See, e.g., (He et al., 2017).
1
Under review as a conference paper at ICLR 2018
We compare the proposed multiset loss against an extensive set of baselines. They include a sequen-
tial loss with an arbitrary rank function, sequential loss with an input-dependent rank function, and
an aggregated distribution matching loss and its one-step variant. We also test policy gradient, as
was done by Welleck et al. (2017) recently for multiset prediction. Our evaluation is conducted on
two sets of datasets with varying difficulties and properties. According to the experiments, we find
that the proposed multiset loss outperforms all the other loss functions.
The paper is structured as follows. We first define multiset prediction at the beginning of Section 2,
and compare itto existing problems in supervised learning in 2.1. Then we propose the multiset loss
in Section 2.2, followed by alternative baseline losses in Section 3. The multiset loss and baselines
are then empirically evaluated in Section 4.
2 Multiset Prediction
A multiset prediction problem is a generalization of classification, where a target is not a single
class but a multiset of classes. The goal is to find a mapping from an input x to a multiset Y =
y1 , . . . , y|Y| , where yk ∈ C. Some of the core properties of multiset prediction are
1.	the input x is an arbitrary vector.
2.	there is no predefined order among the items yi in the target multiset Y .
3.	the size of Y may vary depending on the input x.
4.	each item in the class set C may appear more than once in Y .
Refer to Appendix A for definitions related to multiset prediction.
As is typical in supervised learning, in multiset prediction a model fθ (x) is evaluated by forming a
test set {(xi, Yi)}n=ι and computing evaluation metrics m(∙) that compare the predicted and target
multisets, n Pn=ι m(Yi, Yi), where Yi = fθ(Xi) denotes a predicted multiset. Here, F1 score and
exact match (defined in Appendix A), are used as evaluation metrics.
2.1	Related Problems in Supervised Learning
Variants of this multiset prediction problem have been extensively studied. However, they differ
from our definition of the problem. Here, we go over each variant and discuss how it differs from
our definition of multiset prediction.
Power Multiset Classification Perhaps the most naive approach to multiset prediction is to trans-
form the class set C into a set M (C) of all possible multisets. This transformation, or the size of
M(C), is not well defined unless some constraints are put in place. If the maximum size of a target
multiset is set to K, the number of all possible multisets is
K
X
k=1
(|C| + k - 1)!
k!(∣C∣- 1)!
With some constant |C |, we notice that this grows exponentially in the maximum size of the target
multiset.
Once the class set C is transformed, we can train a multi-class classifier π that maps an input x
to one of the elements in M(C). However, this is infeasible in practice and generally intractable.
For instance, for the COCO Medium dataset used later in the experiments (see section 4.1), M(C)
has roughly 20 thousand elements while the dataset only contains roughly 40 thousand training
examples. For the full MS COCO dataset, |M(C)| is on the order of 1049, making it infeasible to
learn a classifier using this method.
Ranking A ranking problem can be considered as learning a mapping from a pair of input x and
one of the items c ∈ C to its score s(x, c). All the items in the class set are then sorted according
to the score, and this sorted order determines the rank of each item. By taking the top-K items
from this sorted list, we can turn this problem of ranking into set prediction. Similarly to multiset
2
Under review as a conference paper at ICLR 2018
prediction, the input x is arbitrary, and the target is a set without any prespecific order. However,
ranking differs from multiset prediction in that it is unable to handle multiple occurrences of a single
item in the target set.
Aggregated Distribution Matching Instead of considering the target multiset as an actual multi-
set, one can convert it into a distribution by computing the frequency of each item from the class set
in the target multiset. That is,
p(y|x)
Eya∈Y Iyi=y
-|Y|-
where I is an indicator function. Then, We can simply minimize a divergence between this distribu-
tion and the predictive distribution from a model. This loss function works only when the conditional
distribution p(y|x) substantially differs from the marginal distribution p(y), since the model would
resort to a trivial solution of predicting the marginal distribution regardless of the input x.
We describe this approach in more detail in Sec. 3.1, and test it against our proposal in the experi-
ments.
Sequence prediction A sequence prediction problem is characterized as finding a mapping from
an input x to a sequence of classes Y = y1 , . . . , y|Y| . Representative examples of sequence
prediction include machine translation, automatic speech recognition and other tagging problems,
such as part-of-speech tagging, in natural language processing. Similarly to multiset prediction, the
input x is arbitrary, and an item in the class set C may appear more than once in the target sequence.
It is, however, different from multiset prediction in that there is a clear, predetermined order of items
in the target sequence. We detail this sequence prediction approach later in Sec. 3.2.
2.2	Multiset Loss Function for Multiset Prediction
In this paper, we propose a novel loss function, called multiset loss, for the problem of multiset
prediction. This loss function is best motivated by treating the multiset prediction problem as a
sequential decision making process with a model being considered a policy π. This policy takes as
input the input X and all the previously predicted classes y<t at time t, and outputs the distribution
over the next class to be predicted. That is, ∏θ(yt∣y<t, x). This policy is parametrized with a set θ
of parameters.
We first define a free label multiset at time t as
Definition 1 (Free Label Multiset).
Yt — Yt-1 \ {yt-1}
yt-ι is the prediction made by the policy at time t - 1.
This free label multiset Yt contains all the items that remain to be predicted after t - 1 predictions
by the policy.
We then construct an oracle policy ∏*. This oracle policy takes as input a sequence of predicted
labels y<t, the input x, and the free label multiset with respect to its predictions, Yt = Y\ {y<t}. It
outputs a distribution whose entire probability (1) is evenly distributed over all the items in the free
label multiset Yt. In other words,
Definition 2 (Oracle).
∏*(yt∣y<t,χ, Yt) =[产1, ifyt ∈Yt
[0,	otherwise
An interesting and important property of this oracle is that it is optimal given any prefix y<t with
respect to both precision and recall. This is intuitively clear by noticing that the oracle policy allows
only a correct item to be selected. We call this property the optimality of the oracle.
Remark 1. Given an arbitrary prefix y<t,
Prec(y<t, Y) ≤ Prec(y<t ∪ y, Y) and Rec(y<t, Y) ≤ Rec(y<t ∪ y, Y),
for any y 〜∏≠(y<t,x, Yt).
3
Under review as a conference paper at ICLR 2018
The proof is given in Appendix B. See Appendix A for definitions of precision and recall for
multisets.
From the remark above, it follows that the oracle policy is an optimal solution to the problem of
multiset prediction in terms of precision and recall.
Remark 2.
Prec(y≤∣γ∣,Y) = 1 and Rec(y≤∣γ∣, Y) = 1,
for all y≤∣γ∣ 〜πKy"χ, YI)n*(y2|yi,x, Y2) ∙ ∙ ∙ π*(y∣γjy<∣γ∣,x, YIYI).
The proof can be found in Appendix C.
It is trivial to show that sampling from such an oracle policy would never result in an incorrect
prediction. That is, this oracle policy assigns zero probability to any sequence of predictions that is
not a permutation of the target multiset.
Remark 3.
IYI
U∏*(yt∣y<t,x) = 0, ifmultiset(yι, ...,y∣γ∣) = Y,
t=1
where multiset equality refers to exact match, defined in Appendix A. In short, this oracle policy
tells us at each time step t which of all the items in the class set C must be selected. This optimality
allows Us to consider a step-wise loss between a parametrized policy ∏θ and the oracle policy ∏*,
because the oracle policy provides us with an optimal decision regardless of the quality of the prefix
generated so far. We thUs propose to minimize the KL divergence from the oracle policy to the
parametrized policy at each step separately. This divergence is defined as
KL(πtkπt) = H(πt) - X ∣4τlogπθ(yjly<t,χ),	(I)
'{z}	∣ |Yt |
const. w.r.t. θ yj ∈IYt I
where Yt is formed using predictions y<t from ∏, and H(πtt) is the entropy of the oracle policy
at time step t. This entropy term can be safely ignored when learning πθ, since it is constant with
respect to θ. We define
Lt (x,Y ,y<t,θ)= KL(∏t IInt )-H(∏t)	⑵
and call it a per-step loss function. We note that it is indeed possible to use another divergence in
the place of the KL divergence.
It is intractable to minimize the per-step loss from Eq. (2) for every possible state (y<t, x), since
the size of the state space grows exponentially with respect to the size of a target multiset. We
thus propose here to minimize the per-step loss only for the state, defined as a pair of the input x
and the prefix y<t, visited by the parametrized policy ∏θ. That is, We generate an entire trajectory
(yι,...,yτ) by executing the parametrized policy until either all the items in the target multiset
have been predicted or the predefined maximum number of steps have passed. Then, we compute
the loss function at each time t based on (x, y<t), for all t = 1,...,T. The final loss function is
then the sum of all these per-step loss functions.
Definition 3 (Multiset Loss Function).
L(x, Y, θ)
T1
-Σ ∣Yξ∣ Σ logπθ(yj|y<t,x),
where T is the smaller of the smallest t for which Yt = 0 and the predefined maximum number of
steps allowed.
Note that as a consequence of Remarks 2 and 3, minimizing the multiset loss function results in
maximizing F1 and exact match.
As was shown by Ross et al. (2011), the use of the parametrized policy πθ instead of the oracle policy
πt allows the upper bound on the learned policy’s error to be linear with respect to the size of the
target multiset. If the oracle policy had been used, the upper bound would have grown quadratically
with respect to the size of the target multiset. To confirm this empirically, we test the following three
alternative strategies for executing the parametrized policy πθ in the experiments:
4
Under review as a conference paper at ICLR 2018
1.	Greedy search: yt = argmaxy log∏θ(y∣y<t,x)
2.	Stochastic sampling: yt 〜∏θ(y∣y<t, x)
3.	Oracle sampling: y 〜∏*(y∣y<t,x, Yt)
Once the proposed multiset loss is minimized, we evaluate the learned policy by greedily selecting
each item from the policy.
2.3 Variable-Sized Target Multiset
We have defined the proposed loss function for multiset prediction while assuming that the size of
the target multiset was known. However, this is a major limitation, and we introduce two different
methods for relaxing this constraint.
Termination Policy The termination policy πs outputs a stop distribution given the predicted se-
quence of items y<t and the input x. Because the size of the target multiset is known during training,
we simply train this termination policy in a supervised way using a binary cross-entropy loss. At
evaluation time, we simply threshold the predicted stop probability at a predefined threshold (0.5).
Special Class An alternative strategy is to introduce a special item to the class set, called hENDi,
and add it to the final free label multiset Y|Y |+1 = {hENDi}. Thus, the parametrized policy is trained
to predict this special item hENDi once all the items in the target multiset have been predicted. This
is analogous to NLP sequence models which predict an end of sentence token (Sutskever et al., 2014;
Bahdanau et al., 2014), and was used in Welleck et al. (2017) to predict variable-sized multisets.
3 Other Loss Functions
In addition to the proposed multiset loss function, we propose three more loss functions for multiset
prediction. They serve as baselines in our experiments later.
3.1	Aggregated Distribution Matching
In the case of distribution matching, we consider the target multiset Y as a set of samples from a sin-
gle, underlying distribution q* over the class set C . This underlying distribution can be empirically
estimated by counting the number of occurrences of each item c ∈ C in Y. That is,
q*
⅛ XYIL
where I is the indicator function as before.
Similarly, we can construct an aggregated distribution computed by the parametrized policy πθ .
As with the proposed multiset loss in Def. 3, We first execute ∏θ to predict a multiset Y. This is
converted into an aggregated distribution qθ in the same way as we turned the target multiset into
the oracle aggregate distribution.
Learning is equivalent to minimizing the divergence between these two distributions. In this paper,
we test two types of divergences. The first one is from a family of Lp distances defined as
Ldpm(x,Y, θ) = kq* - qθkp,
where q* and q are the vectors representing the corresponding categorical distributions. The other is
a usual KL divergence defined earlier in Eq. (1):
LKL(χ, Y ,θ) = £q*(c|x)iog qθ (c|x).
c∈C
One major issue with this approach is that minimizing the divergence between the aggregated dis-
tributions does not necessarily result in the optimal policy (see the oracle policy in Def. 2.) That is,
a policy that minimizes this loss function may assign non-zero probability to an incorrect sequence
5
Under review as a conference paper at ICLR 2018
of predictions, unlike the oracle policy. This is due to the invariance of the aggregated distribution
to the order of predictions. Later when analyzing this loss function, we empirically notice that a
learned policy often has a different behaviour from the oracle policy, for instance, reflected by the
increasing entropy of the action distribution over time.
One-Step Variant We can train an one-step predictor with this aggregate distribution matching
criterion, instead of learning a policy ∏. That is, a predictor outputs both a point qθ(∙∣x) in a |C|-
dimensional simplex and the size lθ (x) of the target multiset. Then, for each unique item c ∈ C, the
number of its occurrences in the predicted multiset Y is
c
#(C) = round(qc(x) ∙ l(x)).
The corresponding loss function is then
LI-SteP(χ, Y,θ) = £q*(c|x)iogqθ(c∣χ) + λ(lθ(x) - ∣γ∣)2,
c∈C
where λ > 0 is a coefficient for balancing the contributions from the two terms.
A major weakness of this one-step variant, compared to the approaches based on sequential decision
making, is the lack of modelling dependencies among the items in the predicted multiset. We test
this approach in the experiments later and observe this lack of output dependency modelling results
in substantially worse prediction accuracy.
3.2	Sequence prediction with a predefined order
All the loss functions defined so far have not relied on the availability of an existing order of items
in a target multiset. However, by turning the problem of multiset prediction into sequential decision
making, minimizing such a loss function is equivalent to capturing an order of items in the target
multiset implicitly. Here, we instead describe an approach based on explicitly defining an order in
advance. This will serve as a baseline later in the experiments.
We first define a rank function r that maps from one of the unique items in the class set c ∈ C to a
unique integer. That is, r : C → Z. This function assigns the rank of each item and is used to order
items yi in a target multiset Y. This results in a sequence S = (s1, . . . , s|Y| ), where r(si) ≥ r(sj)
for all j > i, and si ∈ Y. With this target sequence S created from Y using the rank function r, we
define a sequence loss function as
|S|
LSeq(x, S, θ) = - Elog ∏θ (st∣S<t,x).
t=1
Minimizing this loss function is equivalent to maximizing the conditional log-probability of the
sequence S given x.
This sequence loss function has two clear disadvantages. First, it does not take into account the ac-
tual behaviour of the policy ∏θ (see, e.g., Bengio et al., 2015; DaUme In & Marcu, 2005; Ross et al.,
2011). This makes a learned policy potentially vulnerable to cascading error at test time. Second
and more importantly, this loss function requires a pre-specified rank function r. Because multiset
prediction does not come with such a rank function by definition, we must design an arbitrary rank
function, and the final performance varies significantly based on the choice. We demonstrate this
variation in section 4.3.
Input-Dependent Rank Function When the input x has a well-known structure, and an object
within the input for each item in the target multiset is annotated, it is possible to devise a rank
function per input. A representative example is an image input with bounding box annotations.
Here, we present two input-dependent rank functions in such a case.
First, a spatial rank function rspatial assigns an integer rank to each item in a given target multiset Y
such that
rspatial(yi|x) < rspatial(yj|x), if posx (xi) < posx (xj) and posy (xi) < posy(xj),
6
Under review as a conference paper at ICLR 2018
where xi and xj are the objects corresponding to the items yi and yj .
Second, an area rank function rarea decides the rank of each label in a target multiset according to
the size of the corresponding object inside the input image:
rarea(yi|x) < rarea (yj |x), if area(xi) < area(xj).
The area may be determined based on the size of a bounding box or the number of pixels, depending
on the level of annotation.
We test these two image-specific input-dependent rank functions against a random rank function in
the experiments.
3.3	Reinforcement Learning
In (Welleck et al., 2017), an approach based on reinforcement learning was proposed for multiset
prediction. Instead of assuming the existence of an oracle policy, this approach solely relies on a
reward function r designed specifically for multiset prediction. The reward function is defined as
r(yt,Yt) = { -1
if yt ∈ Yt
otherwise
The goal is then to maximize the sum of rewards over a trajectory of predictions from a parametrized
policy πθ . The final loss function is
T
LRL = -Ey〜∏θ Xr(y<t,Yt) - λH(∏θ(y<t,x))
t=1
(3)
where the second term inside the expectation is the negative entropy multiplied with a regularization
coefficient λ. The second term encourages the exploration during training. As in (Welleck et al.,
2017), we use REINFORCE (Williams, 1992) to stochastically minimize the loss function above
with respect to πθ .
This loss function is optimal in that the return, i.e., the sum of the step-wise rewards, is maximized
when both the precision and recall are maximal (= 1). In other words, the oracle policy, defined in
Def. 2, maximizes the expected return.
However, this approach of reinforcement learning is known to be difficult, with a high variance (Pe-
ters & Schaal, 2008). This is especially true here, as the size of the state space grows exponentially
with respect to the size of the target multiset, and the action space of each step is as large as the
number of unique items in the class set.
4	Experiments and Analysis
In this section, we extensively evaluate the proposed multiset loss function against various baseline
loss functions presented throughout this paper. More specifically, we focus on its applicability and
performance on image-based multiset prediction.
4.1	Datasets
MNIST Multi MNIST Multi is a class of synthetic datasets. Each dataset consists of multiple
100x100 images, each of which contains a varying number of digits from the original MNIST (Le-
Cun et al., 1998). We vary the size of each digit and also add clutters. In the experiments, we
consider the following variants of MNIST Multi:
•	MNIST Multi (4): |Y| = 4, 20-50 pixel digits
•	MNIST Multi (1-4): |Y| ∈ {1,..., 4}, 20-50 pixel digits
•	MNIST Multi (10): |Y| = 10, 20 pixel digits
Each dataset has a training set with 70,000 examples and a test set with 10,000 examples. We
randomly sample 7,000 examples from the training set to use as a validation set, and train with the
remaining 63,000 examples.
7
Under review as a conference paper at ICLR 2018
MS COCO As a real-world dataset, we use Microsoft COCO (Lin et al., 2014) which includes
natural images with multiple objects. Compared to MNIST Multi, each image in MS COCO has
objects of more varying sizes and shapes, and there is a large variation in the number of object
instances per image which spans from 1 to 91. The problem is made even more challenging with
many overlapping and occluded objects.
To control the difficulty in order to better study the loss functions, we create the following two
variants:
•	COCO Easy: |Y | = 2, 10,230 training examples, 24 classes
•	COCO Medium: |Y| ∈ {1, . . . , 4}, 44,121 training examples, 23 classes
In both of the variants, we only include images whose |Y | objects are large and of common classes.
An object is defined to be large if the object’s area is above the 40-th percentile across the train set
of MS COCO. After reducing the dataset to have |Y | large objects per image, we remove images
containing only objects of rare classes. A class is considered rare if its frequency is less than 吉,
where C is the class set. These two stages ensure that only images with a proper number of large
objects are kept. We do not use fine-grained annotation (pixel-level segmentation and bounding
boxes) except for creating input-dependent rank functions from Sec. 3.2.
For each variant, we hold out a randomly sampled 15% of the training examples as a validation set.
We form separate test sets by applying the same filters to the COCO validation set. The test set sizes
are 5,107 for COCO Easy and 21,944 for COCO Medium.
4.2	Models
MNIST Multi We use three convolutional layers of channel sizes 10, 10 and 32, followed by
a convolutional long short-term memory (LSTM) layer (Xingjian et al., 2015). At each step, the
feature map from the convolutional LSTM layer is average-pooled spatially and fed to a softmax
classifier. In the case of the one-step variant of aggregate distribution matching, the LSTM layer is
skipped.
MS COCO We use a ResNet-34 (He et al., 2016) pretrained on ImageNet (Deng et al., 2009) as a
feature extractor. The final feature map from this ResNet-34 is fed to a convolutional LSTM layer,
as described for MNIST Multi above. We do not finetune the ResNet-34 based feature extractor.
In all experiments, for predicting variable-sized multisets we use the termination policy approach
since it is easily applicable to all of the baselines, thus ensuring a fair comparison. Conversely, it is
unclear how to extend the special class approach to the distribution matching baselines.
Training and evaluation For each loss, a
model was trained for 200 epochs (350 for
MNIST Multi 10). After each epoch, exact
match was computed on the validation set. The
model state with the highest validation exact
match was used for evaluation on the test set.
MNIST Multi (4) COCO Easy
Table 1: Influence of the choice of a rank function
on the sequence prediction loss function
	EM	F1	EM	F1
Random	0.920	0.977	0.721	0.779
Area	0.529	0.830	0.700	0.763
Spatial	0.917	0.976	0.675	0.738
When evaluating a trained policy, we use
greedy decoding and the termination policy for
determining the size of a predicted multiset.
Each predicted multiset is compared against the
ground-truth target multiset, and we report both the accuracy based on the exact match (EM) and
F-1 score (F1), as defined in Appendix A.
More details about the model architectures and training are in Appendix D.
4.3	Experiment 1: Influence of a Rank Function on Sequence Prediction
8
Under review as a conference paper at ICLR 2018
Table 3: Loss Function Comparison on the variants of MNIST Multi
	MNIST Multi (4)		MNIST Multi (1-4)		MNIST Multi (10)	
	EM	F1	EM	F1	EM	F1
Proposed L	0.950	0.987	0.953	0.981	0.920	0.992
LRL	0.912	0.977	0.945	0.980	0.665	0.970
Ld1m	0.921	0.978	0.918	0.969	0.239	0.714
LKL dm	0.908	0.974	0.908	0.962	0.256	0.874
L seq	0.906	0.973	0.891	0.952	0.592	0.946
L1-step	0.210	0.676	0.055	0.598	0.032	0.854
First, we investigate the sequence loss function Lseq from Sec. 3.2, while varying a rank function.
We test three alternatives: a random rank function4 r and two input-dependent rank functions rspatial
and rarea. We compare these rank functions on MNIST Multi (4) and COCO Easy validation sets.
We present the results in Table 1. It is clear from the results that the performance of the se-	Table 2: Selection Strategies				
quence prediction loss function is dependent on the choice of a rank function. In the case		MNIST Multi (10)		COCO Easy	
of MNIST Multi, the area-based rank function		EM	F1	EM	F1
Was far Worse than the other choices. HoWever,	Greedy	0.920	0.992	0.702	0.788
this Was not true on COCO Easy, Where the spa-	Stochastic 0.907		0.990	0.700	0.790
tial rank function Was Worst among the three. In	Oracle	0.875	0.986	0.696	0.780
both cases, We have observed that the random
rank function performed best, and from here on, we use the random rank function in the remaining
experiments. This set of experiments firmly suggests the need of an order-invariant multiset loss
function, such as the multiset loss function proposed in this paper.
4.4	Experiment 2: Execution S trategies for the Multiset Loss Function
In this set of experiments, We compare the three execution strategies for the proposed multiset loss
function, illustrated in Sec. 3. They are greedy decoding, stochastic sampling and oracle sampling.
We test them on MNIST Multi (10) and COCO Easy.
As shoWn in Table 2, greedy decoding and stochastic sampling, both of Which consider states that are
likely to be visited by the parametrized policy, outperform the oracle sampling. This is consistent
With the theory by Ross et al. (2011). Although the first tWo strategies perform comparably to
each other, across both of the datasets and the tWo evaluation metrics, greedy decoding tends to
outperform stochastic sampling. We conjecture this is due to better matching betWeen training and
testing in the case of greedy decoding. Thus, from here on, We use greedy decoding When training a
model With the proposed multiset loss function.
4.5	Experiment 3: Loss Function Comparison
We noW compare the proposed multiset loss function against the five baseline loss functions: rein-
forcement learning Lrl, aggregate distribution matching-Llm and LKm-, its one-step variant LI-Step,
and sequence prediction Lseq.
MNIST Multi We present the results on the MNIST Multi variants in Table 3. On all three variants
and according to both metrics, the proposed multiset loss function outperforms all the others. The
reinforcement learning based approach closely folloWs behind. Its performance, hoWever, drops as
the number of items in a target multiset increases. This is understandable, as the variance of policy
gradient groWs as the length of an episode groWs. A similar behaviour Was observed With sequence
prediction as Well as aggregate distribution matching. We Were not able to train any decent models
With the one-step variant of aggregate distribution matching. This Was true especially in terms
4The random rank function is generated before training and held fixed. We verified that generating a neW
random rank function for each batch significantly decreased performance.
9
Under review as a conference paper at ICLR 2018
of exact match (EM), which we attribute to the one-step variant not being capable of modelling
dependencies among the predicted items.
MS COCO Similarly to the results on the
variants of MNIST Multi, the proposed mul-
tiset loss function matches or outperforms all
the others on the two variants of MS COCO,
as presented in Table 4. On COCO Easy, with
only two objects to predict per example, both
aggregated distribution matching (with KL di-
vergence) and the sequence loss functions are
as competitive as the proposed multiset loss.
The other loss functions significantly underper-
form these three loss functions, as they did on
MNIST Multi.
Table 4: Loss function comparison on the variants
of MS COCO
	COCO Easy		COCO Medium	
	EM	F1	EM	F1
Proposed L	0.702	0.788	0.481	0.639
LRL	0.672	0.746	0.425	0.564
L1 dm	0.533	0.614	0.221	0.085
LKL dm	0.714	0.763	0.444	0.591
Lseq	0.709	0.774	0.457	0.592
L1-step	0.552	0.664	0.000	0.446
The performance gap between the proposed
loss and the others, however, grows substantially on the more challenging COCO Medium, which
has more objects per example. The proposed multiset loss outperforms the aggregated distribution
matching with KL divergence by 3.7 percentage points on exact match and 4.8 on F1. This is anal-
ogous to the experiments on the MNIST Multi variants, where the performance gap increased when
moving from four to ten digits.
4.6	Analysis: Entropy Evolution
One property of the oracle policy defined in Sec. 2.2 is
that the entropy of the predictive distribution strictly de-
creases over time, i.e., H∏* (y∣y<t,x) > H∏ (y∣y≤t,x).
This is a natural consequence from the fact that there is
no pre-specified rank function, because the oracle policy
cannot prefer any item from the others in a free label mul-
tiset. Hence, we examine here how the policy learned
based on each loss function compares to the oracle pol-
icy in terms of per-step entropy. We consider the poli-
cies trained on MNIST Multi (10), where the differences
among them were most clear.
As shown in Fig. 1, the policy trained on MNIST Multi
(10) using the proposed multiset loss closely follows the
oracle policy. The entropy decreases as the predictions
are made. The decreases can be interpreted as concen-
trating probability mass on progressively smaller free la-
bels sets. The variance is quite small, indicating that this
strategy is uniformly applied for any input.
Figure 1: Comparison of per-step en-
tropies of predictive distributions com-
pared over the validation set.
The policy trained with reinforcement learning retains a relatively low entropy across steps, with a
decreasing trend in the second half. We carefully suspect the low entropy in the earlier steps is due
to the greedy nature of policy gradient. The policy receives a high reward more easily by choosing
one of many possible choices in an earlier step than in a later step. This effectively discourages the
policy from exploring all possible trajectories during training.
On the other hand, the policy found by aggregated distribution matching (LdKmL) has the opposite
behaviour. The entropy in general grows as more predictions are made. To see why this is sub-
optimal, consider the final (1θth) step. Assuming the first nine predictions {yι,..., y9} were correct
(i.e. they form a subset of Y), there is only one correct class left for the final prediction y10. The
high entropy, however, indicates that the model is placing a significant amount of probability on
incorrect sequences. We believe such a policy is found by minimizing the aggregated distribution
matching loss function because it cannot properly distinguish between policies with increasing and
decreasing entropies.
10
Under review as a conference paper at ICLR 2018
The increasing entropy also indicates that the policy has learned a rank function implicitly and is
fully relying on it. Given some unknown free label multiset, inferred from the input, this policy uses
the implicitly learned rank function to choose one item from this set. We conjecture this reliance
on an inferred rank function, which is by definition sub-optimal,5 resulted in lower performance of
aggregate distribution matching.
5	Conclusion
We have extensively investigated the problem of multiset prediction in this paper. We rigorously
defined the problem, and proposed to approach it from the perspective of sequential decision mak-
ing. In doing so, an oracle policy was defined and shown to be optimal, and a new loss function,
called multiset loss, was introduced as a means to train a parametrized policy for multiset prediction.
The experiments on two families of datasets, MNIST Multi variants and MS COCO variants, have
revealed the effectiveness of the proposed loss function over other loss functions including rein-
forcement learning, sequence, and aggregated distribution matching loss functions. The success of
the proposed multiset loss brings in new opportunities of applying machine learning to various new
domains, including high-energy physics.
References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence
prediction with recurrent neural networks. In Advances in Neural Information Processing Sys-
tems,pp.1171-1179, 2015.
Hal Daume In and Daniel Marcu. Learning as search optimization: Approximate large margin meth-
ods for structured prediction. In Proceedings of the 22nd international conference on Machine
learning, pp. 169-176. ACM, 2005.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009.
IEEE Conference on, pp. 248-255. IEEE, 2009.
W Ehrenfeld, R Buckingham, J Cranshaw, T Cuhadar Donszelmann, T Doherty, E Gallas, J Hrivnac,
D Malon, M Nowak, M Slater, F Viegas, E Vinek, Q Zhang, and the ATLAS Collaboration. Using
tags to speed up the atlas analysis process. Journal of Physics: Conference Series, 331(3):032007,
2011. URL http://stacks.iop.org/1742-6596/331/i=3/a=032007.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask R-CNN. arXiv preprint
arXiv:1703.06870, 2017.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Yann LeCun, Leon Bottou, YoShUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
V. Lempitsky and A. Zisserman. Learning to count objects in images. In Advances in Neural
Information Processing Systems, 2010.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European
conference on computer vision, pp. 740-755. Springer, 2014.
5 Note that this synthetic data was created without any specific rank function.
11
Under review as a conference paper at ICLR 2018
J. Peters and S. Schaal. Reinforcement learning of motor skills with policy gradients. Neural
Networks, 21(4):682-697, May 2008.
StePhane Ross, Geoffrey J Gordon, and Drew Bagnell. A reduction of imitation learning and StrUc-
tured prediction to no-regret online learning. In International Conference on Artificial Intelligence
and Statistics, PP. 627-635, 2011.
D. Singh, A. M. Ibrahim, T. Yohanna, and J.N Singh. An overview of the aPPlications of multisets.
Novi Sad Journal of Mathematics, 37(3):73-92, 2007.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
In Advances in neural information processing systems, PP. 3104-3112, 2014.
APostolos SyroPoulos. Mathematics of Multisets, PP. 347-358. SPringer, Berlin, Heidelberg, 2001.
URL https://doi.org/10.1007/3- 540-45523-X_17.
Sean Welleck, Kyunghyun Cho, and Zheng Zhang. Saliency-based sequential image attention with
multiset Prediction. In Advances in neural information processing systems, 2017. to aPPear.
Ronald J Williams. SimPle statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229-256, 1992.
SHI Xingjian, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo.
Convolutional lstm network: A machine learning aPProach for PreciPitation nowcasting. In Ad-
vances in neural information processing systems, PP. 802-810, 2015.
A Definitions
We review definitions of multiset and exact match, and Present multiset versions of Precision, recall,
and F1. For a comPrehensive overview of multisets, refer to SyroPoulos (2001); Singh et al. (2007).
Multiset A multiset is a set that allows for multiPle instances of elements. Multisets are unordered,
i.e. {x, x, y} and {x, y, x} are equal. We now introduce the formal definition and convenient ways
of rePresenting a multiset.
Formally, a multiset is apair Y = (C, μ), where C = {cι,…，Cp} is a ground set, and μ : C → N≥o
is a multiplicity function that maPs each ci ∈ C to the number of times it occurs in the multiset. The
multiset cardinality is defined as |Y| = £氏。μ(c).
Notationally, a multiset can be enumerated by numbering each element instance and representing
the multiset as a size |Y| set: Y = {c11),c12),…,c,(ci)), c21),…,c2μ(c2)),..., cp,…,cP"(cp))}. This
allows for notation such as Pc∈Y .
An additional compact notation is Y = {y1, y2, ..., y|Y| }, where each yi is an auxiliary variable
referring to an underlying element c ∈ C of the ground set.
For instance, the multiset Y = {cat, cat, dog} can be defined as Y = (C,μ), where C =
{cι = cat, c2 = dog, c3 = fish}, μ(cat) = 2, μ(dog) = 1, μ(fish) = 0, and can be written as
Y = {c(11) = cat, c(12) = cat, c(21) = dog} orY = {y1 = cat, y2 = cat, y3 = dog}.
For additional multiset representations, multiset analogues of common set operations (e.g. union,
intersection, difference), and the notion ofa subset, see Syropoulos (2001); Singh et al. (2007).
Exact Match (EM) Two multisets exactly match when their elements and multiplicities are the
same. For example, {x, y, x} exactly matches {y, x, x}, while {x, y, x} does not exactly match
{z, y, z} or {x, y}.
Formally, let Y = (C,μγ), Y = (C,μγ) be multisets over a common ground set C. Then Y and
Y exactly match if and only if μγ (C) = μγ (C) for all C ∈ C. The evaluation metric EM(Y, Y) is 1
when Y and Y exactly match, and 0 otherwise.
Note that exact match is the same as multiset equality, i.e. Y = Y, as defined in Singh et al. (2007).
12
Under review as a conference paper at ICLR 2018
Precision Precision gives the ratio of correctly predicted elements to the number of predicted
elements. Specifically, let Y= (C,μγ), Y = (C,μγ) be multisets. Then
PreC(YJ) = Py;J∈Y .
|Y |
The summation and membership are done by enumerating the multiset. For example, the multisets
Y = {a,a,b} and Y = {a,b} are enumerated as Y = {a(1),a(2),b⑴} and Y = {a(1), b(1)},
respectively. Then clearly a(1) ∈ Y but a(2) 6∈ Y.
Formally, precision can be defined as
PreC(Y, Y) = ι - Imax(〃Y,(c) - μγ(C)，0)
1Y1
where the summation is now over the ground set C. Intuitively, precision decreases by 由 each time
an extra class label is predicted.
Recall Recall gives the ratio of correctly predicted elements to the number of ground-truth ele-
ments. Recall is defined analogously to precision, as:
Rec(Y,Y )= Py∈Y Iy∈Y .
|Y |
Formally,
Rec(Y, Y) = 1 - Pi max(飞c)-μγ(C)，0).
|Y |
Intuitively, recall decreases by 由 each time an element of Y is not predicted.
F1 The F1 score is the harmonic mean of precision and recall:
,ʌ , ʌ
F _2 PreC(Y, Y) ∙ Rec(Y, Y)
F1 (Y, Y )	2 ∙	.
PreC(Y, Y) + Rec(Y, Y)
B Proof of Remark 1
Proof. Note that the precision with y<t is defined as
Prec(y<t, Y) = Py∈y<t Iy∈Y .
|y<t|
Because y 〜∏≠(y<t,χ, Yt) ∈ Yt,
Prec(y≤t, Y)=1 + Py∈y<t Iy∈Y .
—	1 + |y<t|
Then,
Prec(y≤t, Y) - Prec(y<t, Y) = 1_Pre；®：,Y) ≥ 0,
—	1 + ∣y<t∣
because 0 ≤ Prec(y<t, Y) ≤ 1 and |y<t| ≥ 0. The equality holds when Prec(y<t, Y) = 1.
Similarly, we start with the definition of the recall:
Rec(y<t, Y) = Py∈ygly∈Y.
|Y |
13
Under review as a conference paper at ICLR 2018
Because y ~ π*(y<t,x, Yt) ∈ Yt,
Rec(y≤t, Y) = 1 + EyY^<t Iy∈Y .
Since the denominator is identical,
Rec(y≤t, Y) - Rec(y<t, Y) = ɪ ≥ 0.
C Proof of Remark 2
Proof. When t = 1,
Prec(y≤ι, Y) = 1,
because y1 ~ π*(0, x, Y1) ∈ Y. From Remark 1, We know that
Prec(y≤t, Y) = Prec(y<t, Y),
when Prec(y<t, Y) = 1. By induction, Prec(y≤∣γ∣, Y) = L
From the proof of Remark 1, we know that the recall increases by Yy each time, and we also know
that
ReC®i ,Y) = ɪ,
when t = 1. After |Y| - 1 steps of executing the oracle policy, the recall becomes
1	|Y|	1
ReC(y≤lYl, Y)= m + 且 M = 1.
D Model Descriptions
Figure 2: Graphical illustration of a predictor used throughout the experiments.
Model An input x is first processed by a tower of convolutional layers, resulting in a feature
volume of size w0 × h0 with d feature maps, i.e., H = φ(x) ∈ Rw0×h0×d. At each time step t,
we resize the previous prediction,s embedding emb(yt-1) ∈ R(WO)(h0) to be a W0 X h0 tensor and
concatenate it with H, resulting in H ∈ Rw ×h ×(d+1). This feature volume is then fed into a stack
of convolutional LSTM layers. The output from the final convolutional LSTM layer C ∈ Rw0×h0×q
w0	h0
is spatially average-pooled, i.e., C = w^ Ei=I Ej=I Ci,j,∙ ∈ Rq. ThiS feature vector C is then
14
Under review as a conference paper at ICLR 2018
turned into a conditional distribution over the next item after affine transformation followed by a
softmax function. When the one-step variant of aggregated distribution matching is used, we skip
the convolutional LSTM layers, i.e., C = w1^ Pw=I Ph=I Hj ∈ Rd.
See Fig. 2 for the graphical illustration of the entire network. See Table 5 for the details of the
network for each dataset.
TabIe 5: NetWork ArchitecmreS
Data	CNN	emb(yt-ι)	ConvLSTM
MNIST Multi	conv 5 X	5 max-pool 2	X	2 feat	10 conv 5 x	5 max-pool 2	X	2 feat	10 conv 5 x	5 max-pool 2	X	2 feat	32	81	conv 3 x 3 feat 32 conv 3 x 3 feat 32
MS COCO	ResNet-34	361	conv 3 x 3 feat 512 conv 3 X 3 feat 512
Preprocessing For MNIST Multi, we do not preprocess the input at all. In the case of MS COCO,
input images are of different sizes. Each image is first resized so that its larger dimension has
600 pixels, then along its other dimension is zero-padded to 600 pixels and centered, resulting in a
600x600 image.
Training The model is trained end-to-end, except ResNet-34 Which remains fixed after being
pretrained on ImageNet. For all the experiments, We train a neural netWork using Adam (Kingma
& Ba, 2014) With a fixed learning rate of 0.001, β of (0.9, 0.999) and of 1e-8. The learning rate
Was selected based on the validation performance during the preliminary experiments, and the other
parameters are the default values. For MNIST Multi, the batch size Was 64, and for COCO Was 32.
Feedforward Alternative While We use a recurrent model in the experiments, the multiset loss
can be used With a feedforWard model as folloWs. A key use of the recurrent hidden state is to
retain the previously predicted labels, i.e. to remember the full conditioning set y1,..., yt-1 in
p(yt∣yι,..., yt-ι). Therefore, the proposed loss can be used in a feedforward model by encoding
yι,…，yt-ι in the input xt, and running the feedforward model for |Y| steps, where |Y| is deter-
mined with a method from section 2.3. Note that compared to the recurrent model, this approach
involves additional feature engineering.
15