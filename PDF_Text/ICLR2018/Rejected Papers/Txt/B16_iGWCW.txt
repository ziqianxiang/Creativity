Under review as a conference paper at ICLR 2018
Deep Boosting of Diverse Experts
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, a deep boosting algorithm is developed to learn more discrimina-
tive ensemble classifier by seamlessly combining a set of base deep CNNs (base
experts) with diverse capabilities, e.g., these base deep CNNs are sequentially
trained to recognize a set of object classes in an easy-to-hard way according to
their learning complexities. Our experimental results have demonstrated that our
deep boosting algorithm can significantly improve the accuracy rates on large-
scale visual recognition.
1	Introduction
The rapid growth of computational powers of GPUs has provided good opportunities for us to de-
velop scalable learning algorithms to leverage massive digital images to train more discriminative
classifiers for large-scale visual recognition applications, and deep learning (Simonyan & Zisserman,
2015; Szegedy et al., 2015; He et al., 2016) has demonstrated its outstanding performance because
highly invariant and discriminant features and multi-way softmax classifier are learned jointly in an
end-to-end fashion.
Before deep learning becomes so popular, boosting has achieved good success on visual recognition
(Viola & Jones, 2004). By embedding multiple weak learners to construct an ensemble one, boost-
ing (Schapire, 1999) can significantly improve the performance by sequentially training multiple
weak learners with respect to a weighted error function which assigns larger weights to the samples
misclassified by the previous weak learners. Thus it is very attractive to invest whether boosting can
be integrated with deep learning to achieve higher accuracy rates on large-scale visual recognition.
By using neural networks to replace the traditional weak learners in the boosting frameworks, boost-
ing of neural networks has received enough attentions (Zhou & Lai, 2009; Mosca & Magoulas,
2017a; Kuznetsov et al., 2014; Moghimi et al., 2016). All these existing deep boosting algorithms
simply use the weighted error function (proposed by Adaboost (Schapire, 1999)) to replace the soft-
max error function (used in deep learning ) that treats all the errors equally. Because different object
classes may have different learning complexities, it is more attractive to invest new deep boosting
algorithm that can use different weights over various object classes rather than over different training
samples.
Motivated by this observation, a deep boosting algorithm is developed to generate more discrim-
inative ensemble classifier by combining a set of base deep CNNs with diverse capabilities, e.g.,
all these base deep CNNs (base experts) are sequentially trained to recognize different subsets of
object classes in an easy-to-hard way according to their learning complexities. The rest of the paper
is organized as: Section 2 briefly reviews the related work; Section 3 introduce our deep boosting
algorithm; Section 4 reports our experimental results; and we conclude this paper at Section 5.
2	Related Work
In this section, we briefly review the most relevant researches on deep learning and boosting.
Even deep learning has demonstrated its outstanding abilities on large-scale visual recognition
(Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; Szegedy et al., 2015; He et al., 2016; Huang
et al., 2016), it still has room to improve: all the object classes are arbitrarily assumed to share sim-
ilar learning complexities and a multi-way softmax is used to treat them equally. For recognizing
large numbers of object classes, there may have significant differences on their learning complexi-
1
Under review as a conference paper at ICLR 2018
ties, e.g., some object classes may be harder to be recognized than others. Thus learning their deep
CNNs jointly may not be able to achieve the global optimum effectively because the gradients of
their joint objective function are not uniform for all the object classes and such joint learning process
may distract on discerning some object classes that are hard to be discriminated. For recognizing
large numbers of object classes with diverse learning complexities, it is very important to organize
them in an easy-to-hard way according to their learning complexities and learn their deep CNNs
sequentially. By assigning different weights to the training samples adaptively, boosting (Schapire,
1999; Freund & Schapire, 1997; Schapire et al., 1998) has provided an easy-to-hard approach to
train a set of weak learners sequentially. Thus it is very attractive to invest whether we can lever-
age boosting to learn a set of base deep CNNs sequentially for recognizing large numbers of object
classes in an easy-to-hard way.
Some deep boosting algorithms have been developed by seamlessly integrating boosting with deep
neural networks to improve the performance in practice. Schwenk & Bengio (1997; 2000) proposed
the first work to integrate Adaboost with neural networks for online character recognition applica-
tion. Zhou & Lai (2009) extended the Adaboosting neural networks algorithm for credit scoring.
Recently, Mosca & Magoulas (2017b) developed a deep incremental boosting method which in-
creases the size of neural network at each round by adding new layers at the end of the network.
Moreover, Mosca & Magoulas (2017a) integrated residual networks with incremental boosting and
built an ensemble of residual networks via adding one more residual block to the previous residual
network at each round of boosting. All these methods combine the merits of boosting and neural
networks; they train each base network either using a different training set by resampling with a
probability distribution derived from the error weight, or directly using the weighted cost function
for the base network.
Alternatively, Saberian & Vasconcelos (2011) proposed a margin enforcing loss for multi-class
boosting and presented two ways to minimize the resulting risk: the one is coordinate descent ap-
proach which updates one predictor component at a time, the other way is based on directional
functional derivative and updates all components jointly. By applying the first way, i.e., coordinate
descent, Cortes et al. (2014) designed ensemble learning algorithm for binary-class classification
using deep decision trees as base classifiers and gave the data-dependent learning bound of convex
ensembles, and Kuznetsov et al. (2014) furthermore extended it to multi-class version. By applying
the second way, i.e., directional derivative descent, Moghimi et al. (2016) developed an algorithm for
boosting deep convolutional neural networks (CNNs) based on least squares between weights and
directional derivatives, which differs from the original method based on inner product of weights
and directional derivative in (Saberian & Vasconcelos, 2011). All above algorithms focus on seek-
ing the optimal ensemble predictor via changing the error weights of samples; they either update
one component of the predictor per boosting iteration, or update all components simultaneously.
On the other hand, our deep boosting algorithm focuses on combining a set of base deep CNNs
with diverse capabilities: (1) large numbers of object classes are automatically organized in an easy-
to-hard way according to their learning complexities; (2) all these base deep CNNs (base experts)
are sequentially learned to recognize different subsets of object classes; and (3) these base deep
CNNs with diverse capabilities are seamlessly combined to generate more discriminative ensemble
classifier.
3	Deep B oosting of Diverse Experts
In this paper, a deep boosting algorithm is developed by seamlessly combining a set of base deep
CNNs with various capabilities, e.g., all these base deep CNNs are sequentially trained to recognize
different subsets of object classes in an easy-to-hard way according to their learning complexities.
Our deep boosting algorithm uses the base deep CNNs as its weak learners, and many well-designed
deep networks (such as AlexNet (Krizhevsky et al., 2012), VGG (Simonyan & Zisserman, 2015),
ResNet (He et al., 2016), and huang2016densely), can be used as its base deep CNNs. It is worth
noting that all these well-designed deep networks [] optimize their structures (i.e., numbers of layers
and units in each layer), their node weights and their softmax jointly in an end-to-end manner for
recognizing the same set of object classes. Thus our deep boosting algorithm is firstly implemented
for recognizing 1,00 object classes, however, it is straightward to extend our current implementation
2
Under review as a conference paper at ICLR 2018
Algorithm 1 Deep Boosting of Diverse Experts
Require: Training set from C classes: {(xi, yi) | yi ∈ {1, ..., C}, i = 1, ..., N}; Initial significance
distribution over categories: [D1(1), ..., D1(C)]; Number of base deep CNNs: T.
1:	for t = 1, . . . , T do
2:	Normalization: De t (l) = P，，l, (l = 1 ,...,C)
j=1 Dt (j)
3:	Training the tth base deep CNNs ft(x) via Losst with respect to the importance distribution
over C categories [Dt(1), ..., Dt(C)];
4:	Calculating the error per category for ft(x): εt(l), (l = 1, ..., C);
5:	Computing the weighted error for ft(x): εt = PlC=1 Det(l)εt(l);
6：	Setting βt = -εεt ；
7:	Updating Dt+1(l) as Dt+1(l) = Dt(l)βt1-λεt(l), (l = 1, ..., C), so that hard object classes
misclassified by ft(x) can receive larger weights (importances) when training the (t + 1)th base
deep CNNs at the next round;
8:	end for
9：	Ensembling: g(x) = 1 PT=I log (表)ft(x)
when huge deep networks (with larger capacities) are available in the future and being used as the
base deep CNNs.
3.1 Algorithm
As illustrated in Algorithm 1, our deep boosting algorithm contains the following key components:
(a) Training the tth base deep CNNs (base expert) ft(x) by focusing on achieving higher accuracy
rates for some particular object classes; (b) Estimating the weighted error function for the tth base
deep CNNs ft(x) according to the distribution of importances Dt for C object classes; (c) Updating
the distribution of importances Dt+1 for C object classes to train the (t + 1)th base deep CNNs by
spending more efforts on distinguishing the hard object classes which are not classified very well by
the previous base deep CNNs; (d) Such iterative training process stops when the maximum number
of iterations is reached or a certain level of the accuracy rates is achieved.
For the tth base expert ft(x), we firstly employ deep CNNs to map x into more separable fea-
ture space ht(x; θt), followed by a fully connected discriminant layer and a C-way softmax
layer. The output of the tth base expert is the predicted multi-class distribution, denoted as
ft(x) = [pt(1|x), ..., pt(C|x)]>, whose each component pt(l|x) is the probability score of x as-
signed to the object class l, (l = 1, ..., C):
pt(l|x)
£/p{ W[ ht(x; θt)}
PC=1 exp{w>tht(x； θ))}
(1)
where θt and Wlt, (l = 1, ..., C) are the model parameters for the tth base expert ft(x). Based on the
above probability score, the category label of x can be predicted by the tth base expert as follows:
yt = argm ax Pt (l∣ x)	(2)
Suppose that training set consists of N labeled samples from C classes:
{(xi, yi) | yi ∈ {1, ..., C}}iN=1. To train the tth base expert ft(x), the model parameters can
be learned by maximizing the objective function in the form of weighted margin as follows:
C
Ot(θt,{Wlt}lC=1) =	Det(l)ξlt	(3)
l=1
where
1N	1 N
ξιt = N E1( yi = l) log Pt(ll X i) - n - n E 1( yi = l) log Pt(ll X i)	⑷
l i=1	l i=1
3
Under review as a conference paper at ICLR 2018
Herein the indicator function 1(yi = l) is equal to 1 if yi = l; otherwise zero. Nl denotes the
number of samples belonging to the lth object class. Dt (l) is the normalized importance score for
class l in the tth base expert ft(x). By using the distribution of importances [Dt(1), ..., Dt(C)] to
approximate the learning complexities for C object classes, our deep boosting algorithm can push
the current base deep CNNs to focus on distinguishing the object classes which are hard classified
by the previous base deep CNNs, thus it can support an easy-to-hard solution for large-scale visual
recognition.
ξlt measures the margin between the average confidence on correctly classified examples and the
average confidence on misclassified examples for the lth object class. If the second item in Eq.(4) is
small enough and negligible, ξιt ≈ N PN=11(yi = l) logPt(l∣Xi), then maximizing the objective
function in Eq.(3) is equivalent to maximizing the weighted likelihood.
For the tth base expert ft(X), the classification error rate over the training samples in lth object class
is as follows:
q (l) = 2 X {i( y∙ = ι )1- pN(ll xi) +1( y。= l) N-XN]	(5)
i=1	ι	ι
This error rate is used to update category weight and the loss function of the next weak learner,
and above definition encourages predictors with large margin to improve the discrimination between
correct class and incorrect classe competing with it. Error rate calculated by Eq.(6) is in soft decision
with probability; alternatively, we can also simply compute the error rate in hard decision as t (l) =
~Nl Pn=i 1(y。= l ∧ yi = l). The criterion for hard object classes is et(l)〉表 where the hyper-
parameter λ controls the threshold, and We constrain λ > 2 (i.e.,我 V 1 ) SUCh that the threshold
makes sense. The larger the hyper-parameter λ is, the more strict the precision requirement becomes.
We then compute the weighted error rate εt over all classes for ft(X) such that hard object classes
are focused on by the next base expert.
C
εt =	Det(l)t(l)
ι=1
(6)
The distribution of importances is initialized equally for all C object classes: D 1(l) = C1,(l =
1, ..., C), and it is updated along the iterative learning process by emphasizing the object classes
which are heavily misclassified by the previous base deep CNNs:
Dt+1 (l) = Dt (l)βt1-λt(ι)	(7)
where βt should be an increasing function of εt, and its range should be 0 V βt V 1. It should be
pointed out that λt(l) denotes the product of λ and t(l). Such update of distribution encourages
the next base network focusing on the categories that are hard to classify. As shown in Section
4, to guarantee the upper boundary of ratio (the number of heavily misclassified categories over the
number of all classes) to be minimized, we set
λεt
1 - λεt
(8)
Normalization of the updated importances distribution can be easily carried out:
Det+1(l)
Dt +ι(l)
P C=IDt +1( j),
(l= 1,..., C)
(9)
The distribution of importances is used to: (a) separate the hard object classes (heavily misclassified
by the previous base deep CNNs) from the easy object classes (which have been classified correctly
by the previous base deep CNNs); (b) estimate the weighted error function for the (t + 1)th base
deep CNNs ft+1(X), so that it can spend more efforts on distinguishing the hard object classes
misclassified by the previous base deep CNNs.
4
Under review as a conference paper at ICLR 2018
After T iterations, We can obtain T base deep CNNs (base experts) {fι, ∙∙∙ , ft,…,fτ}, which
are sequentially trained to recognize different subsets of C object classes in an easy-to-hard way
according to their learning complexities. All these T base deep CNNs are seamlessly combined to
generate more discriminative ensemble classifier g(x) for recognizing C object classes:
g(X)=ZX ιog G) ft(X)
(10)
where Z = PT=Jog (卷)is a normalization factor. By diversifying a set of base deep CNNs on
their capabilities (i.e., they are trained to recognize different subsets of C object classes in an easy-
to-hard way), our deep boosting algorithm can obtain more discriminative ensemble classifier g(X)
to significantly improve the accuracy rates on large-scale visual recognition.
To apply such ensembled classifier for recognition, for a given test sample Xtest, it firstly goes
through all these base deep CNNs to obtain T deep representations {hi, ∙∙∙ , h T} and then its final
probability score p(l|Xtest) to be assigned into the lth object class is calculated as follows:
p (l∣ X test) = 1 X log C )	HP{ W W> h t (X Xes Q ) }
Z t =1	∖βt) Pj=i exp{WJtht(xtest; θt)}
(11)
3.2 SELECTION OFβt
In our deep boosting algorithm, βt is selected to be an increasing function of error rate εt, with its
range [0, 1]. βt is employed in two folds: (i) As seen in Eq.(7), βt helps to update the importance
of different categories such that hard object classes are emphasized; (ii) As seen in Eq.(10) and
Eq.(11), reciprocals ofβt are the combination coefficients for the final ensemble classifier such that
those base experts with low error rate have large weight.
The criterion of hard object classes for the tth expert is q (l) > 击∙ Denote emin (l) ，
min{ei(l),…,eτ(l)}. If	(l) > 看,then “(l) > 2λ for each t, (t = 1, ...,T); it implies
that the lth object class is hard for all T experts.
Let ]{l : Emin(l) > 2λ} denote the the number of hard object classes for all T experts. Inspired by
Freund & Schapire (1997), we now show that the selection ofβt as in Eq.(8) guarantees the upper
boundary of ratio (the number of heavily misclassified categories over the number of all classes) to
be minimized.
It can be shown that for 0 < x < 1 and 0 < α < 1, we have xα ≤ 1 - (1 - x)α. According to
Eq.(7):
Dt+1(l) = Dt(l)βt1-λt(l),
we get
CC	C
Dt+1(l) =	Dt(l)βt1-λt(l) ≤	Dt(l)(1 - (1 - βt)(1 - λEt(l)))
l=1	l=1	l=1
= (XC Dt(l))(1-(1-βt))+λ(1-βt)XC Dt(l)Et(l)
l=1	l=1
According to Eq.(6) and Eq.(9), we get:
CC
Dt(l)Et(l) = (	Dt(l))εt
l=1	l=1
Therefore
CC	C
Dt+1(l) ≤	Dt(l)(1-(1-βt))+λ(1-βt)(	Dt(l))εt
l=1	l=1	l=1
C
=(	Dt(l))[1 - (1 - βt)(1 - λεt)]
l=1
(12)
(13)
5
Under review as a conference paper at ICLR 2018
Since PlC=1 D1(l) = 1, we have
CC
X D2(l) ≤ (X D1(l))[1 - (1 - β1)(1 - λε1)] = 1 - (1 - β1)(1 - λε1)
Thus
C
X DT+1 (l) ≤ ΠtT=1 [1 - (1 - βt)(1 - λεt)]
l=1
By substituting Eq.(7) into Eq.(14), we get
CC
ΠtT=1[1-(1-βt)(1-λεt)] ≥	DT+1(l)=	(D1(l)ΠtT=1βt1-λt(l))
l=1	l=1
C
=CC XcCIβt	t(l)) ≥ C X	CCIβt-λet(l))
l	— 1	l: ^min (l ) > 2λ
(14)
(15)
Due to Emin (l) > =,it holds that a (l) > = and 1 一 λet (l) V ɪ for all l. Recall the constraint that
0 < βt < 1, thus
CC X	(∏T=1 βt-λet(l)) ≥ CC X	(∏T=1 βt)
l:	emin (l ) > 2λ	l: emin (l ) > 2λ
H{l : Emin Cl) > 长} ∏T β2
(16)
Combining Eq.(15) with Eq.(16), we get
]{l : Emin Cl) > 2λ }
C
πT=1l1 - (1 - βt)(1 一 λεt)]
1
∏ T=1 βt
1 - (1 一 βt)(1 一 λεt)
1	1
(17)
To minimize the rightside, we set its partial derivative with respect to βt to zero:
1 一 (1 一 βt)(1 一 λεt)
1
βt
)=0
Since βt only exists in the tth factor, above equation is equivalent to
∂ 1 - (1 一 βt)(1 一 λεt)
β
Solving it, we find that βt can be optimally selected as:
βt = ⅛
3.3 SELECTION OF λ
We substitute βt = 1-黑 into Eq.(17), and get the upper boundary of ratio (the number of hard
object categories over the number of all classes):
]{l : Emin (l) > 2λ }
C
≤ 2tπLIPλε(I 一 λε)
(18)
看⑴T=I
Now we discuss the range for the hyper-parameter λ. Recall that the criterion of hard object classes
for the tth expert is Et (l) > = and λ controls the threshold. When We are to raise the precision
6
Under review as a conference paper at ICLR 2018
requirements, λ can be set large. We constrain λ > 2 such that ɪ < 1 and the threshold of error
rate makes sense. On the other hand, the range of βt = iλ^λε ɪs 0 < βt < 1, so it is required that
λεt < 2, i.e., λ < 贵∙ To conclude, λ should be selected between the interval [ 11,七].
From the relation between λεt and λεt(1 - λεt), as illustrated in Fig.1, we can see the effect of λ
on the upper boundary of ratio (the number of hard object categories over the number of all classes)
in Eq.(18).
•	In the yellow shaded region, λ ∈ [ 1, 土], i.e., εt < λεt < 1, the condition 0 < βt < 1
is satisfied, and the upper boundary of hard category percentage in Eq.(18) increases with
λ increasing, the reason for which is that when λ increases, the precision requirement
increases, thus the number of hard categories increases too.
•	On the right side of the yellow shaded region, λ > 七，i.e., λεt > 2. In this case, the
condition 0 < βt = --tit < 1 is not satisfied, thus the update of importance distribution
in Eq.(7) can not effectively emphasize the object classes which are heavily misclassified
by the previous experts. In hard classification task, large error rates εt tend to result in λεt
larger than or approaching 2, and βt larger than or approaching 1. The value of λ should
be set smaller to alleviate large εt such that λεt < - and 0 < βt < 1.
•	On the left side of the yellow shaded region, λ < 1, i.e., 2λ > 1, then it can not be used as
the the criterion of hard object classes that et (l) > 2λ.
Figure 1: The relation between λεt and λεt(1 - λεt). The domain with respect to λ is 2 < λ < 2^,
so εt < λεt < 1 (yellow shaded region).
3.4	Back Propagation
The procedure of learning the tth base expert repeatedly adjusts the parameters of the corresponding
deep network so as to maximize the objective function Ot in Eq.(3). To maximize Ot, it is necessary
to calculate its gradients with respect to all parameters, including the weights {wlt}lC=1 and the set
of model parameters θt .
For clearance, we denote
zlt(x) = wl>tht(x; θt) , zlt(ht, wlt)
Thus, the probability score ofx assigned to the object class l, (l = 1, ..., C), in Eq.(1) can be written
as
ezlt(x)
pt(llX) = C^c	, = pt(Z11,…，ZCt)
jC=1 ezjt(x)	t
Then, the objective function in Eq.(3) can be denoted as
Ot(θt, {wlt}lc=1),	Ot(pt1, ..., ptc)
From above presentations, it can be more clearly seen that the objective is a composite function.
Ot is firstly a function of multiple variables pt1, ..., ptc, whose each component plt is a function of
7
Under review as a conference paper at ICLR 2018
multiple variables z1t, ..., zCt, and each zlt is a function of multiple variables ht, wlt, furthermore,
each ht is a function of multiple variables θt. By chain rule, the gradients for the objective function
Ot with respect to {wlt }lC=1 and θt are computed from the top layer to the bottom one in backward
pass.
∂Ot = XX ∂O1 ∂pj ∂zL	∂O1 = X ∂O1 ∂pj 也也
∂WIt	M ∂pj ∂zit ∂Wit,	∂θt j∣=ι ∂pj ∂zit ∂ht ∂θt
where
∂O	1 N	1	1 N
嬴=D t (j)[ Nj 3 1( yi = j) pj) - N-Nj ∑ 1( yi = j) log pt (jl X i)]
∂pj = J Plt(I- plt) if j = l
∂zιt - -PItPj if j = l
and
∂zit
∂ W it
∂zit
∂ h t
Wit,
∂ h t
E
Herein, J is Jacobi matrix. Such gradients are back-propagated [] through the tth base deep CNNs
to fine-tune the weights {Wit}iC=1 and the set of model parameters θt simultaneously.
3.5	Generalization Error B ound
Denote X as the instance space, denote Ω as the distribution over X, and denote S as a training set
of N examples chosen i.i.d according to Ω. We are to investigate the gap between the generalization
error on Ω and the empirical error on S.
Suppose that F is the set from which the base deep experts are chosen, and let G = X 7→
Pf∈F af f (X)|af > 0, Pf∈F af = 1 . The combination coefficients A = [a1, ..., af, ...] can
be viewed as a distribution over F. Define G= {x ∣→ 1 PJ=1 ft(x) |ft ∈ F}, and each ft ∈ F
may appear multiple times in the sum. For any g ∈ G, there exists a distribution A = [a1, ..., af, ...],
so we can select the base deep experts from F for Γ times independently according to A and obtain
ʌ
g ∈G, denote g ^ A.
Note that g is a C-dim vetor, and each component of g is the category confidence, i.e., gy (X) =
p(y|X), (y = 1, ..., C). Based on Eq.(11), the category label of test sample can be predicted by
arg maxy gy (x) = P(y∣x). The ensembled classifier g predicts wrong if gy(x) ≤ maxy=y gy(x).
The generalization error rate for the final ensembled classifier can be measured by the probability
Pω[gy(x) ≤ maxy=y gy(x)]∙
According to probability theory, for any events Bι and B2, P(B1) ≤ P(B2) + P(B2 ∣B 1), therefore
ξ
Pω[gy(X) ≤ maxgy(x)] ≤Pω,g^A[gy(x) ≤ max^y(x) + ɔ] +
y=y	y=y	2	，1Q.
ξ	(19)
Pω ,g ^A [ g y (X) > max gy(X) + ∣∣gy (X) ≤ max gy(X)]
y=y	2	y=y
where ξ > 0 measures the margin between the confidences from ground-truth and incorrect cate-
gories. Using Chernoff bound (Schapire et al., 1998), the the second term in the right side of Eq.(19)
is bounded as:
PQ …[ gy (χ) > maxg y(χ)+2 ∣gy (χ) ≤ m=y gy^χ^] ≤ e-γ 128	(20)
8
Under review as a conference paper at ICLR 2018
Assume that the base-classifier space F is with VC-dimension d, which can be approximately es-
timated by the number of neurons ν and the number of weigths ω in the base deep network, i.e.,
d = O(νω). Recall that S is a sample set of N examples from C categories. Then the effective
number of hypotheses for F over S is at most Pd=ι (CN) = (eNdC)d. Thus, the effective number
of hypotheses over S for G = {x ∣→ γ1 P；=i ft (X) ∣ft ∈ F∣ is at most (eNdC )γd.
Applying Devroye Lemma as in (Schapire et al., 1998), it holds with probability at least 1 - δΓ that
ξξ
Pω,g^A[gy(x) ≤ maxgy(x) + ʒ] ≤ PS,g^A[gy(x) ≤ maxgy(x) + -] + ∆r	(21)
y=y	2	y=y	2
where ∆r = J2N [Γd log eNC + log 44"1) ].
Likewise, in probability theory for any events B1 and B2, P(B1) ≤ P(B2) + P(B1 ∣B2), thus
ξ
PS,g^A[gy(x) ≤ maxgy(x) + ɔ] ≤PS[gy(x) ≤ maxgy(x) + ξ] +
y=y	2	y=y
ξ
PS,g^A[gy(x) ≤ max^y(x) + -g'∖g↑∕(x) > maxgy(x) + ξ]
y=y	2	y=y
(22)
Because
ξ
PS,g〜A[gy(x) ≤ maxgy(x) + -g]∖9↑l(x) > maxgy(x) + ξ]
y=y	2	y=y
≤ X PS,g〜A[gy (x) ≤ gy(x) + 2 ∣gy (x) > gy(x) + ξ] ≤ (C — 1)eTξ2/8
y=y
So, combining Eq.(19-23) together, it can be derived that
Pω[gy(x) ≤ maxgy(x)] ≤ PS[gy(x) ≤ maxgy(x) + ξ] + CeTξ2/8 + ∆γ
y=y	y=y
(23)
(24)
As can be seen from above, large margin ξ over the training set corresponds to narrow gap between
the generalization error on Ω and the empirical error on S, which leads to the better upper bound of
generalization error.
4 Experimental Results
In this section we evaluate the proposed algorithms on three real world datasets MNIST (LeCun,
1998), CIFAR-100 (Krizhevsky & Hinton, 2009), and ImageNet (Russakovsky et al., 2015). For
MNIST and CIFAR-100, we train all networks from scrach in each AdaBoost iteration stage. On
ImageNet, we use the pretrained model as the result of iteration #1 and then train weighted models
sequentially. The pretrained model is available in TorchVision1. In each iteration, we adopt the
weight initialization menthod proposed by He et al. (2015). All the networks are trained using
stochastic gradient descent (SGD) with the weight decay 0f 10-4 and the momentum of 0.9 in the
experiments.
Experimental Results on MNIST
MNIST dataset consists of 60,000 training and 10,000 test handwritten digit samples. Schwenk
& Bengio (2000) showed the accuracy improvement of MLP via AdaBoost on MNIST dataset by
updating sample weights according to classification errors. For fair comparison, we firstly use the
similar network architecture (MLP) as the base experts in experiments. We train two sets of networks
with the only difference that one updates weights w.r.t the class errors on training datasets while the
other one updates weights w.r.t the sample errors on training datasets. The former is the proposed
1https://github.com/pytorch/vision
9
Under review as a conference paper at ICLR 2018
method in this paper, and the latter is the traditional AdaBoost method. In the two sets of weak
learners, we share the same weak learner in iteration #1 and train other two weak learners seperately.
For data pre-processing, we normalize data via subtracting means and dividing standard deviations.
In the experiment on MNIST, we simply train the network with learning rate 0.01 through out the
whole 120 epoches.
Method	Iteration #1	Iteration #2	Iteration #3
update weights w.r.t sample errors	473	253	223
update weights w.r.t class errors	4.73	2.22	1.87
Table 1: Comparison of test error rate(%) with boosted model on MNIST datasets.
With our proposed method, the top 1 error on test datasets decreases from 4.73% to 1.87 % after
three iterations (table 1). After the interation #1, the top 1 error of our method drops more quickly
than the method which update weights w.r.t sample errors. Our method, which updates weights w.r.t
the class errors, leverages the idea that different class should have different learning comlexity and
should not be treated equally. Through the iterations, our method trains a set of classifiers in an
easy-to-hard way.
Class APs vary from each weak learner in each iteration to others, increasing for marjor weighted
classes while decreasing for minor weighted classes(fig. 2-left). Therefore, in each iteration, the
weighted learner classifier behaves like a expert different from the classfier in the previous iteration.
Though some APs for certain classes may decrease in some degree with each weak learner, the
boosting models improve the accuracy for hard classes while preservering the accuracy for easy
classes (fig. 2-right). Our method cordinates the set of weak learners trained sequeentially with
diversified capabilities to improve the classfication capability of boosting model.
(a) Individual Expert Performance
Figure 2: The comparison of class AP(average precision) on MNIST with MLP. The x-axis (class
ID) is sorted according to the test APs (average pricision) per class in the weak learner #1.
AP(average pricision) on the left plot is calculated on validation darasets with single model. AP
(average pricision) on the right plot is calcucalted on test datasets with boosted model as in eq. (11)
with t={1,2,3} respectively.
(b) Ensembled Experts Performance
Experimental Results on CIFAR- 1 00
We also carry out experiments on CIFAR-100 dataset. CIFAR-100 dataset consists of 60,000 images
from 100 classes. There are 500 training images and 100 testing images per class. We adopt padding,
mirroring, shifting for data augumentation and normalization as in (He et al., 2016; Huang et al.,
2016). In training stage, we hold out 5,000 training images for validation and leave 45,000 for
training. Because the error per class on training datasets approaches zero and training errors could
be all zeros with even simple networks (Zhang et al., 2016), we update the category distribution w.r.t
10
Under review as a conference paper at ICLR 2018
the class errors on validation datasets. We do not use any sample of validation datasets to update
parameters of the networks itself. When training networks on CIFAR-100, the initial learning rate
is set to 0.1 and divided by 0.1 at epoch [150, 225]. Similar to (He et al., 2015; Huang et al.,
2016), we train the network for 300 epoches. We show the results with various models including
ResNet56(λ = 0.7) and DenseNet-BC(k=12)(Huang et al., 2016) on test set. The performances of
emsembled classifier with different number of base networks are shown in the middle two rows of
(table 2).
As illustrated in section 3.3, λ controls the weight differences among classes. In comparison, we
use λ={0.7, 0.5, 0.1}. As shown in fig. 3-left, with smaller lambda, the weitht differences become
bigger. We use ResNet model in (He et al., 2016) with 56 layers on CIFAR-100 datasets.
Figure 3: Comparison of lambda selection on CIFAR-100 datsets with ResNet56 model. Class IDs
are sorted w.r.t the class APs of iteration #1. Top 1 errors (left plot) are generated on test datasets.
Overall, the models with lambda=0.7 performs the best, resulting in 24.15% test error after four iter-
ations. Comparing with lambda=0.5 and lambda=0.7, we find that both model performs well in the
initial several iterations, but the model with lambda=0.7 would converge to a better optimal(fig. 3-
right). However, with lambda=0.1 which weights classes more discriminately, top 1 error fluctuates
along the iterations (fig. 3-right). We conclude that lambda should be merely used to insure that the
value of β is below 0.5 and may harm the performance in the ensemble models if set to a low value.
Figure 4: The change of validation APs from the classes with low APs in the iteration # 1 for
ResNet56 (left) and DenseNet-BC(k=12) (right) on CIFAR-100. APs are all calculated from valida-
tion sets which would be used for the update of weights in the proceeding iteration. Class IDs are
sorted by the APs of weak learner #1 and selected for those with small class IDs.
In fig. 4, we show the comparison of weak leaner #1 and weak learner #2 without boosting. Though
with minor exeptions, most classes with low APs improve their class APs in the proceeding weak
11
Under review as a conference paper at ICLR 2018
learner. Our method is based on the motivation that different classes should have different learning
comlexity. Thus, those classes with higher learning complexity should be paid more attention along
the iterations. Based on the class AP result of the privious iteration, we suppose those classes
with lower APs should have higher learning complexity and be paid for attention in the subsequent
iterations.
Experimental Results on ImageNet
We furthermore carry out experiments on ILSVRC2012 Classification dataset(Russakovsky et al.,
2015) which consists of 1.2 million images for training, and 50,000 for validation. There are 1,000
classes in the dataset. For data augmentation and normalization, we adopt scaling, ramdom crop-
ping and horizontal flipping as in (He et al., 2016; Huang et al., 2016). Similar to the experiments
on CIFAR-100, the error per class on training datasets approaches zero, we update the category
distribution w.r.t the class errors on validation datasets. Since the test dataset of ImageNet are not
available, we just report the results on the validation sets, following He et al. (2016); Huang et al.
(2016) for ImageNet. When we train ResNet50 networks on ImageNet, the initial learning rates
are set to 0.1 and divided by 0.1 at epoch [30, 60]. Similar to (He et al., 2015; Huang et al., 2016)
again, we train the network for 90 epoches. The performances of emsembled classifier with different
number of base networks are shown in the bottom rows of (table 2). These base ResNet networks
with diverse capabilities are combined to generate more discriminative ensemble classifier.
Datasets	Network	T=1	T=2	T=3	T=4
MNIST	MLP	-475-	-222-	1.87	-186-
CIFAR-100	ResNet56(He et al.,2016)*	-29.53-	-26.65-	24.97	-24.15-
	DenseNet-BC(k=12)(Huang etal., 2016)*	30.78	28.95	27.60	26.64
ImageNet	ResNet50 (He etal., 2015)	24.18(7.49)	23.28(6.98)	22.96(6.81)	22.96(6.79)
Table 2: Single crop test error rate(%) along iterations with boosted models. * indicates updating
weights with λ = 0.7, while the others λ = 1. Blue indicates the use of pre-trained model from
TorchVision. () indicates top 5 error rate.
Conclusions
In this paper, we develop a deep boosting algorithm is to learn more discriminative ensemble clas-
sifier by combining a set of base experts with diverse capabilities. The base experts are from the
family of deep CNNs and they are sequentially trained to recognize a set of object classes in an
easy-to-hard way according to their learning complexities. As for the future network, we would like
to investigate the performance of heterogeneous base deep networks from different families.
Acknowledgments
References
Corinna Cortes, Mehryar Mohri, and Umar Syed. Deep boosting. In Proceedings of the 31st Inter-
national Conference on Machine Learning (ICML-14) ,pp.1179-1187, 2014.
Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an
application to boosting. Journal of Computer and System Sciences, 55(1):119-139, 1997.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected
convolutional networks. arXiv preprint arXiv:1608.06993, 2016.
12
Under review as a conference paper at ICLR 2018
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
Iutional neural networks. In Advances in neural information processing Systems, pp. 1097-1105,
2012.
Vitaly Kuznetsov, Mehryar Mohri, and Umar Syed. Multi-class deep boosting. In Advances in
Neural Information Processing Systems, pp. 2501-2509, 2014.
Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.
Mohammad Moghimi, Serge J Belongie, Mohammad J Saberian, Jian Yang, Nuno Vasconcelos, and
Li-Jia Li. Boosted convolutional neural networks. In BMVC, 2016.
Alan Mosca and George D Magoulas. Boosted residual networks. In International Conference on
Engineering Applications of Neural Networks, pp. 137-148. Springer, 2017a.
Alan Mosca and George D Magoulas. Deep incremental boosting. arXiv preprint arXiv:1708.03704,
2017b.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision
(IJCV), 115(3):211-252, 2015. doi: 10.1007/s11263-015-0816-y.
Mohammad J Saberian and Nuno Vasconcelos. Multiclass boosting: Theory and algorithms. In
Advances in Neural Information Processing Systems, pp. 2124-2132, 2011.
Robert E Schapire. A brief introduction to boosting. In Ijcai, volume 99, pp. 1401-1406, 1999.
Robert E Schapire, Yoav Freund, Peter Bartlett, Wee Sun Lee, et al. Boosting the margin: A new
explanation for the effectiveness of voting methods. The annals of statistics, 26(5):1651-1686,
1998.
Holger Schwenk and Yoshua Bengio. Adaboosting neural networks: Application to on-line character
recognition. In International Conference on Artificial Neural Networks, pp. 967-972. Springer,
1997.
Holger Schwenk and Yoshua Bengio. Boosting neural networks. Neural Computation, 12(8):1869-
1887, 2000.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. ICLR, 2015.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1-9, 2015.
Paul Viola and Michael J Jones. Robust real-time face detection. International journal of computer
vision, 57(2):137-154, 2004.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
Ligang Zhou and Kin Lai. Adaboosting neural networks for credit scoring. In The Sixth International
Symposium on Neural Networks (ISNN 2009), pp. 875-884. Springer, 2009.
13