Under review as a conference paper at ICLR 2018
Combining Model-based and Model-free RL
via Multi-step Control Variates
Anonymous authors
Paper under double-blind review
Ab stract
Model-free deep reinforcement learning algorithms are able to successfully solve
a wide range of continuous control tasks, but typically require many on-policy
samples to achieve good performance. Model-based RL algorithms are sample-
efficient on the other hand, while learning accurate global models of complex
dynamic environments has turned out to be tricky in practice, which leads to the
unsatisfactory performance of the learned policies. In this work, we combine the
sample-efficiency of model-based algorithms and the accuracy of model-free al-
gorithms. We leverage multi-step neural network based predictive models by em-
bedding real trajectories into imaginary rollouts of the model, and use the imag-
inary cumulative rewards as control variates for model-free algorithms. In this
way, we achieved the strengths of both sides and derived an estimator which is not
only sample-efficient, but also unbiased and of very low variance. We present our
evaluation on the MuJoCo and OpenAI Gym benchmarks.
1	Introduction
Reinforcement learning algorithms are usually divided as two broad classes: model-based algo-
rithms which try to learn forward dynamics models first (and derive a planning-based policy from
it), and model-free algorithms which do not explicitly train any dynamics models but instead di-
rectly learn a policy. Model-free algorithms have been proven effective in learning sophisticated
controllers in a wide range of applications, ranging from playing video games from raw pixels Mnih
et al. (2013); Oh et al. (2016) to solving locomotion and robotic tasks Schulman et al. (2015). Model-
based algorithms on the other hand are widely used in robot learning and continuous control tasks
Deisenroth & Rasmussen (2011).
Both approaches have their strengths as well as limitations. Although model-free RL algorithms
can achieve very good performance, they usually require ten thousands of samples for each iteration
(Schulman et al., 2015), which is a very high sample intensity. A fundamental advantage of model-
based RL is that knowledge about the environment can be acquired in an unsupervised setting,
even in trajectories where no rewards are available. While model-based algorithms are usually
considered as more sample-efficient (Deisenroth et al., 2013), currently the most successful model-
based algorithms rely on simple functional approximators Lioutikov et al. (2014) to learn local
models over-fitting a few samples, usually one mini-batch. Thus ideas that rely on a global neural
network model for planning often do not perform very well (Mishra et al., 2017; Gu et al., 2016b)
due to the bias of the learned models and the limitations of the planning algorithms, e.g. MCTS
(Chaslot, 2010). As a result, although theoretically global models should be more sample efficient
because models can be trained on off-policy data, they are seldom used in real problems.
In this paper, we attempt to combine the benefits of model-based and model-free reinforcement
learning and reduce the drawbacks on both sides. Our algorithm can generally be viewed as using
a multi-step forward model as a control variate (MSCV) for model-free RL algorithms. It works as
follows: given a batch of on-policy trajectory samples, instead of directly performing on-policy gra-
dient update based on the return, we first embed a portion of the trajectory into the dynamics model,
yielding an imaginary trajectory which is usually very close to the real trajectory thus we call it
a trajectory ”embedding” to the dynamics model. From the imaginary trajectory, we can get low
variance gradients via the reparameterization trick Kingma & Welling (2013) and backpropagation-
through-time (BPTT) Werbos (1990). However, such gradients are in general biased. The funda-
1
Under review as a conference paper at ICLR 2018
mental reason for this is that in real circumstances, the global dynamics is highly complicated while
the model is usually biased. Thus afterwards, we use the return from the real trajectory to correct
the model-based gradient. This way, we effectively combine the strength of both model-based and
model-free RL while avoiding their shortcomings. We evaluated MSCV on OpenAI Gym Brockman
et al. (2016) and MuJoCo Todorov et al. (2012) benchmarks.
The contributions of our work are the following: (1) We show how to train efficient multi-step value
prediction models by utilizing techniques such as replay memory and importance sampling on off-
policy data. (2) We demonstrate that the model-based path derivative methods can be combined with
model-free RL algorithms to produce low variance, unbiased, and sample efficient policy gradient
estimates. (3) We show that techniques such as latent space trajectory embedding and dynamic
unfolding can significantly boost the performance of the model based control variates.
2	Related Works
Model-based and model-free RL are two broad classes of reinforcement learning algorithms with
different weakness and strength. Thus no doubt there are many works trying to combine the strength
of both sides and avoid their weakness. (Nagabandi et al., 2017) tries to train a neural network
based global model and use model predictive control to initialize the policy for model free fine
tuning. In this way, their algorithm can be significantly more sample-efficient than purely model
free approaches. (Chebotar et al., 2017) combines local model based algorithms such as LQR to
a model-free framework called path integral policy improvement. Local models such as LQR are
sample efficient but they need to refit simple local models for each iteration, so they cannot work
with high dimensional state spaces such as images.
QProp (Gu et al., 2016a) proposed to use a critic trained over the amortized deterministic policy
using DDPG (Lillicrap et al., 2015) as a control variate. While this has brought good performance
gain in terms of sample complexity, our new control variate provides several attractive properties
over it. First, the Q function Q(s, a) fitted off-policy using DDPG is usually very different from the
real on-policy expected return Qμ(s, a) (because the trajectory in the replay buffer are generated by
different policies, they have different value estimates). As we’ve seen in our experiments, sometimes
the gap between these two values is of orders of magnitude. However, in our algorithm, since their
is no inconsistency of the objectives, our control variate is usually much more accurate than QProp.
See e.g. Section 5. Second, Our control variate takes into consideration not only the current policy
step, but also several future steps. This approach have brought two main gains to our algorithm: (1)
our control variate can be fitted much closer to the actual return, resulting in less variance. (2) we
can use backpropagation through time to optimize multiple steps in the imaginary trajectory jointly,
this makes our algorithm be able to look further into the future. This is very useful when the reward
cannot be observed very often.
Our algorithm is also closely related to stochastic value gradient (Heess et al., 2015). In fact, if
the control variate choose to unroll our multi-step model for one step, then the resulting algorithm
is equivalent to use SVG(1) algorithm in (Heess et al., 2015) as a control variate for a model-free
algorithm. However, our algorithm makes use of multi-step prediction and dynamic rollout which
SVG did not take advantage of.
3	Preliminaries
Reinforcement learning aims at learning a policy that maximizes the sum of future rewards (Naga-
bandi et al., 2017). Formally, in an MDP environment (Thie, 1983) (S, A, f, r), suppose that at
time t the agent is in state st ∈ S and it executes one action at sampled according to its policy
∏θ(at∣st), then receives a reward r(st, at) from the environment. Following this, the environment
makes a transition to state st+1 according to an unknown dynamics function f(st, at, η) where η is
a random noise variable sampled from a fixed distribution, for example η 〜 N(0,1). The trajectory
{si , ai , ri }iN=1 is the resulting sequence of state-action-reward records produced by the policy and
the environment. The goal is to maximize the discounted total sum of reward Pt+0=∞t γt0-tr(st0, at0),
where γ is a discounting factor that prioritizes near-term rewards. In MSCV, to select the best ac-
tions, We train a stochastic policy a = μ(s, E)〜 ∏(a|s), where E 〜 N(0,1) is another random
noise variable.
2
Under review as a conference paper at ICLR 2018
In model-based reinforcement learning, a dynamics model is learned as an approximation of the
real unknown dynamics f . In MSCV, we are learning a multi-step dynamics model f predicting the
next state Sk+ι = f (si, aι, ηι •…ak, ηk). f is modeled as a RNN so that it can be unrolled for an
arbitrary number of time steps k. We also need to fit a reward predictor r(st, at) (Otherwise we can
also assume the reward function r is available to us.). Besides these two state and reward predictive
models, we also learn an on-policy value function V π(s) from off-policy data, which can be viewed
as an approximation of the real expected future return V π(s) of state s.
4	Multi-step Models as Control Variates
Assume that we have a multi-step model as specified above. Given a real trajectory T =
{si , ai, ri, ηi , i}iN=t collected using the current policy πθ, we define the empirical k-step return
as Q π (st, at) = rt + •…+ γk-1rt+k-1 + Yk V π(St+k ).
Given the sub-trajectory Tk = {si, ai, ri, ηi, i}it=+tk, we can ”embed” Tk to an imaginary trajectory
E(T) = {s0i, a0i, ri0, ηi, i}it=+tk where E(T) satisfies three conditions: (1) s0t = st, (2) {s0i+1 =
f(si, ai, ηi)}t+k (3) {ai = μ(si, Ei)}t=k. One can easily verify f and μθ's effectiveness by seeing
that when f = f, the imaginary trajectory E[T] perfectly matches the real trajectory T.
T .<	∕' 11	i'	.	. ∙	1	∙	1 ∙	∙ .	.1	1	1	∙	f	1 . 1
In the following, for notational simplicity we assume the unknown dynamics f and the model f are
deterministic. The stochastic cases are similar except that one need an extra inference network to
infer the latent variables of a given generation. We can represent the k-step imaginary empirical
return as:
Q(St, μθ (St, Et), et+i ∙ ∙ ∙ et+k-i) = Q(st, at …at+k-i)	(I)
=r(st, at) + …+ YkTr(St+k-i, at+k-i) + YkVπ(st+k),
where a0 = μθ(s0, e). Assume all {ei}t+k are sampled from fixed prior distribution P = N(0,1),
we have the following policy gradient formula:
VJ (θ) = Eρ∏,p [Vθ log π(at∣st)Q k (st,at)]	(2)
π
=Eρπ,p[vθ log π(at Ist)[Qk (St, at) - Q(St, at, Et+1 …Et+k-1)]]	(3)
+ Eρπ,P∣vθ log n(at| St)Q(St, at, Et+1 …Et+k-1)]	(4)
We assume in the above equation that the imaginary rollout to compute Q is an embedding of the
real rollout to compute Q. The second term can be expressed via the reparameterization trick:
Eρ∏,p[Vθ log∏(at∣st)Q(st,at,且+1 •…6t+k-ι)]
VtθEρπ,P
a
[∏θ(a∣st)Q(st,a, Q+1 ∙∙∙ 6t+k-ι)]da
Vθ / Eρ∏,p[∏θ(a∣st)Q(st,a,且十i ∙∙∙ Q+k-ι)]da
a
VθEρπ,p[Q(st, μθ(st, €t), €t+i ∙ ∙ ∙ et+k-ι)],	(5)
where the operator Vtθ stands for the partial derivative with respect to the θ of time step t only,
namely we treat policy πθ occurring before and after time step t as fixed. This can be easily done
by detaching the node from the computational graph with any modern deep learning framework
exploiting automatic differentiation. Through this way, the variance of the second term G2 can
be reduced significantly since we cut off its dependency among former and latter timesteps. The
algorithm we use for calculating the partial derivative of G2 is backpropagation through time of
RNN.
There are several interesting properties of the derived gradient estimator. First, imaging the extreme
cases in which f, r are perfect, then Gi is always zero. On the other hand, G? is reparameterized as
a differentiable function from a fixed random noise, whose gradient is of low variance. In practice,
one can replace k-step cumulative return Qk with the real return Q of the entire trajectory. In this
case, the estimator will be unbiased at the expenses of a little more variance.
Dynamic rollout. There is a trade-off on the model rollout number k. If k is too small, the model
bias is small but the training procedure is not able to take advantage of the multi-step trajectory opti-
mization and the accurate reward predictions. Moreover, more burden would be put on the accuracy
3
Under review as a conference paper at ICLR 2018
i' ɪʌTrTΓ T i' I ∙	1	.1	1 1 F ♦	∙11 . 1	1 . 1	♦	.	∙11 1 ∙	∙ .1
of V π . If k is too large, the model bias will take over, and the imaginary trajectory will diverge with
the real trajectory, but the bias in the value function is relieved because of the discounting. A proper
k will help us balance the biases from the forward model and the value function.
In our algorithm, we perform a dynamic model rollout, following the similar technique with QProp.
Namely, let A = Q - V be the advantage estimation of the real trajectory, and A = A - V be the
advantage estimation of the generated trajectory. We perform a short line search on k, such that k is
the largest number such that Cov(A, A) > 0.
Algorithm 1 k-step Actor-Critic with Model-based Control Variate
μ(s, e): Policy network parameterized by θμ
V (s): Value network. parameterized by θv
f (s, a): Forward model, parameterized by θf
T : time step horizon for TRPO update.
L:	Maximum episode length for data collection.
k: number of time steps to evaluate return.
M:	the replay memory.
Initialize μ, V randomly, initialize f to pretrained model, initialize environment states S.
for μ not converge do
Collect batch B of trajectories {si,e, g© ai,e 〜μθ, ri,e}T=o：eNi With {so,e} from S.
if episode ends or length > L then
reset S to initial states.
end if
Add B to M.
for n iterations do
Sample batch Bf of trajectories of length k from M and train model f on Bf.
Sample batch Bv = {si, ai, ri, s0i}iN=1 of transitions from M, train V With the algorithm
specified in section 5.
end for
dθμ = 0.
for each t = 0,1, •…T 一 k do
Take a batch of sub-trajectories Bt = {sj,e, aj,e, ej,e, rj,e}tj+=kt,-e=1,1N from B
Unroll the model for k steps, get Bt0 = {s0j,e, a0j,e, ej,e, rj0,e}tj+=kt,-e=1,1N such that s0t,e = st,e
and fix the latent variables e as in Bt .
Calculate gradient Gi = N Pe[Vθ log π(at∣st)[Q ∏ (St ,at) 一 Q(st,at,et+ι ∙ ∙ ∙ et+k-ι)].
Estimate the gradient G? = VθE°∏,p[Q(st, μθ(st, Q, et+i …et+k-1)] by setting initial
state to be St,e and perform multiple random model roll-outs and backpropagate through time.
Accumulate gradients: dθμ = dθμ + Gi + G2
end for
Update θμ with dθμ with TRPO.
end for
5	Off-Policy Learning of Multi-step Models and Value
FUNCTIONS
Dynamics model. We now present how to off-policy learn the multi-step model and the value func-
tion. We use a replay buffer to train both of them. It is relatively straight forward to train the multi-
step model f given a batch of length k trajectories. We use similar techniques with (Nagabandi et al.,
2017) to predict the difference between consecutive states. Therefore, we have s0t+i = f(st, a) + st,
which gives rise to a esNet like structure. This is a suitable structure because it fits the prior of MDP
that the environment need to know the information of this state to generate the next state. In this
work, we assume the reward function r is accessible. Then the overall objective is
s0t+i 一 st)2 + α(rt 一 r(st, s0t+i, at)2
where α is the weight for reward loss. In the experiment, we use α = 10.
4
Under review as a conference paper at ICLR 2018
TaSk	Threshold	MSCV	TRPO
Swimmer	90	21	30
Reacher	-7	34	42
Walker	2500	167	279
Hopper	2000	35	57
Table 1: Iteration Required for Every Task
Figure 1: Reacher
Value function. The value function Vπ can be conveniently fitted off-policy. Different from DDPG,
the gradient estimator for Vπ is fitted using importance sampling, thus the objective is consistent.
More precisely, if a transition (s, a, r, s) sampled from the replay buffer is generated by policy
πθ0(a∣s), the current policy is πθ (a|s), We compute an importance weight Wt = ∏ (at^stʌ, and We
minimize the weighted bellman error: Wt(V(s, θ) - y)2, where y = r + V0(s, θ).
Algorithm 2 Fitting Value Function Vπ
Given experience replay memory M
Given value function V(∙, θ), outer loop time t
θ=θ0
for m = 0 do
Sample(sk, ak, rk, sk+1) from M(k < t).
ym = rk + YV(sk+1; θ0)
p(ak|sk)
W = p(ak∣sk).
2
∆ = NVnew W (Om — V(Sk ； θ)).
Apply gradient-based update to θ using ∆.
θ0 = σθ0 + (1 - σ)θ
end for
6	Experiment
We evaluate MSCV on the MuJoCo and OpenAI Gym benchmarks against TRPO. We perform
experiments on Reacher, Hopper, Swimmer and Walker tasks. The batch size is 5000 for the first
three tasks and 25000 for Walker. For all experiments we set the number of steps k to be 2. Although
we tried other values of k, we found k = 2 is most effective. We set the discounting factor to be
0.995 and α = 10 for all experiments.
6.1	Multi-step Dynamics Model
In this section, we evaluate the dynamics model’s capacity. We use the similar architecture with that
in (Nagabandi et al., 2017) to predict the difference between the next states and current states.
Figure 3 shows our predicted rewards in Swimmer task. We randomly choose a starting state from
a trajectory and roll out for 15 steps. Nevertheless, it is not enough to have accurate reward in our
algorithm, because we would backpropogate through the trajectory as well. In Figure 2, we compare
the L2-norm of the actual states and imaginary states from the dynamic model. We also plot the norm
of difference between two states and the cosine similarity. The results show that the dynamics model
is able to predict the future states and rewards, which proves the fundamental assumptions in our
algorithm.
5
Under review as a conference paper at ICLR 2018
Figure 2: State Prediction
Figure 4: Swimmer
Figure 3: Reward Prediction
6.2	Evaluation Across Domains
In this section, we present the comparison between MSCV and TRPO. For all tasks, both policy and
value networks are two-layer neural networks with hidden size [64, 64]. We pre-train the dynamics
model for each task using the technique described in section 5.
We summarize the results in Table 1. We set a threshold for each task and recorded the number of
iteration needed in order to reach the threshold. Table 1 shows that MSCV outperformed TRPO in
every task. The most significant improvement is in Walker, where the setting is more complex than
the other three. It shows that by combining model-free and model-based algorithms, we can learn to
improve the policy more efficiently in the complex dynamics.
Figure 4, 5, and 1 shows the improvement of policy after each iteration. It shows MSCV is able to
converge faster and learn more efficiently than TRPO at each iteration. The improvement in Figure
1 is least significant, and we believe that it is because of the simple dynamic in this task that TRPO
is already able to learn efficiently.
7	Conclusion
In this paper we presented the algorithm MSCV which combines the advantages of model-free
and model-based reinforcement learning algorithms while alleviates the shortcomings of both sides.
The resulting algorithm utilizes BPTT and multi-step models as a control variate for model free
algorithms.
6
Under review as a conference paper at ICLR 2018
Our algorithm can be viewed as a multi-step model-based generalization of QProp, in which, instead
of training a Q function using unstable techniques such as DDPG, we use off-policy data to stably
train a multi-step dynamics model, a reward predictor, and an on-policy value function.
Our experiments on MuJoCo and OpenAI Gym benchmarks not only prove the footstone assump-
tions of MSCV, but show that MSCV outperforms the baseline method significantly in various cir-
cumstances.
References
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Guillaume Chaslot. Monte-carlo tree search. Maastricht: Universiteit Maastricht, 2010.
Yevgen Chebotar, Karol Hausman, Marvin Zhang, Gaurav Sukhatme, Stefan Schaal, and Sergey
Levine. Combining model-based and model-free updates for trajectory-centric reinforcement
learning. arXiv preprint arXiv:1703.03078, 2017.
Marc Deisenroth and Carl E Rasmussen. Pilco: A model-based and data-efficient approach to policy
search. In Proceedings of the 28th International Conference on machine learning (ICML-11), pp.
465-472, 2011.
Marc Peter Deisenroth, Gerhard Neumann, Jan Peters, et al. A survey on policy search for robotics.
Foundations and TrendsR in Robotics, 2(1-2):1-142, 2013.
Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard E Turner, and Sergey Levine. Q-
prop: Sample-efficient policy gradient with an off-policy critic. arXiv preprint arXiv:1611.02247,
2016a.
Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and Sergey Levine. Continuous deep q-learning
with model-based acceleration. In International Conference on Machine Learning, pp. 2829-
2838, 2016b.
Nicolas Heess, Gregory Wayne, David Silver, Tim Lillicrap, Tom Erez, and Yuval Tassa. Learning
continuous control policies by stochastic value gradients. In Advances in Neural Information
Processing Systems, pp. 2944-2952, 2015.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
Rudolf Lioutikov, Alexandros Paraschos, Jan Peters, and Gerhard Neumann. Sample-based
informationl-theoretic stochastic optimal control. In Robotics and Automation (ICRA), 2014 IEEE
International Conference on, pp. 3896-3902. IEEE, 2014.
Nikhil Mishra, Pieter Abbeel, and Igor Mordatch. Prediction and control with temporal segment
models. arXiv preprint arXiv:1703.04070, 2017.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.
Anusha Nagabandi, Gregory Kahn, Ronald S Fearing, and Sergey Levine. Neural network dy-
namics for model-based deep reinforcement learning with model-free fine-tuning. arXiv preprint
arXiv:1708.02596, 2017.
Junhyuk Oh, Valliappa Chockalingam, Satinder Singh, and Honglak Lee. Control of memory, active
perception, and action in minecraft. arXiv preprint arXiv:1605.09128, 2016.
7
Under review as a conference paper at ICLR 2018
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In Proceedings of the 32nd International Conference on Machine Learning
(ICML-15),pp.1889-1897, 2015.
Paul R Thie. Markov decision processes. Comap, Incorporated, 1983.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026-
5033. IEEE, 2012.
PaulJ Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the
IEEE, 78(10):1550-1560, 1990.
8