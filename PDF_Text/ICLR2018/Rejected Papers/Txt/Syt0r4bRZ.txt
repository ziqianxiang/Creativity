Under review as a conference paper at ICLR 2018
Tree2Tree Learning with Memory Unit
Anonymous authors
Paper under double-blind review
Ab stract
Traditional recurrent neural network (RNN) or convolutional neural net-
work (CNN) based sequence-to-sequence model can not handle tree structural
data well. To alleviate this problem, in this paper, we propose a tree-to-tree model
with specially designed encoder unit and decoder unit, which recursively encodes
tree inputs into highly folded tree embeddings and decodes the embeddings into
tree outputs. Our model could represent the complex information of a tree while
also restore a tree from embeddings.
We evaluate our model in random tree recovery task and neural machine transla-
tion task. Experiments show that our model outperforms the baseline model.
1	Introduction
In the past several years, Sequence-to-Sequence (Seq2Seq) model based on neural networks has
made significant progress. It is widely used in various tasks, such as time series analysis(Lipton
et al., 2015), speech recognition(Graves et al., 2013) and machine translation (Bahdanau et al.,
2014). However, such model only captures sequential information while structural information is
also of great importance in many tasks. On the one hand, data of many tasks is in hierarchical
structure, such as interpersonal relationships in social network. Sequential data may contain latent
structural information, on the other hand. For instance, natural language can be parsed into a tree
with hierarchical structure, which represents the semantic dependency relationship between words
(Chen & Manning, 2014). Dyer et al. (2016) has shown that introducing structural information into
sentence embedding could lead to an improvement to neural machine translation.
General Seq2Seq model can be regarded as a transduction from sequential inputs to sequential out-
puts, which limits the data structure of applications. But there are a lot of data hierarchical structure
that can be represented in tree instead of sequence format. Compared with sequence based model,
tree based model is more difficult to construct for the following two reasons: first, the number of
child nodes is usually uncertain; second, the structural information of a tree is difficult to repre-
sent and restore. Thus, constructing a more informative embedding in tree based model remains a
challenging problem. Lots of efforts have been made to tackle this problem. Socher et al. (2011b)
designed a recursive tree encoder to encode both content and structure information. Tai et al. (2015)
introduced a LSTM tree encoder to calculate an embedding for classification tasks. However, few
works are devoted to recovering structural information from a embedding of tree. Socher et al.
(2011a) proposed a tree-based model which applied hierarchical perceptron in both encoding and
decoding process as a recursive auto-encoder. But its cell cannot handle the long-term dependency.
To address this issue, we propose a novel tree-to-tree model that recursively encodes tree inputs
into highly folded tree embeddings and decodes the embeddings into tree outputs. Our model could
represent the complex information of a tree while also restore a tree from embeddings. Furthermore,
inspired by the architecture of long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997),
we proposed two memory units for encoder and decoder, respectively. Units of encoder contain
an output gate, an input gate and forget gates, and units of decoder have an output gate as well
as memory gates. Concretely, during encoding process, information from memory units of child
nodes is combined in an organic way with the label of parent node, which is assigned to memory
units of parent nodes. After getting the embedding, the decoder transduce it into a tree by splitting
information into several parts and passing them to generate child nodes.
Besides, the similarity of two trees is hard to define. To alleviate this issue, we take both content
and structure similarity into consideration and design a novel objective function. The content loss
1
Under review as a conference paper at ICLR 2018
reflects whether the corresponding nodes of two trees share the same label, while the structure loss
measures the difference of child number for corresponding nodes. The final objective function is the
weighted sum of content loss and structure loss.
We design a random tree recovery experiment to evaluate the performance of the tree-to-tree model
to represent and restore tree structure data. Experiment result in Section 4 shows, our model can
recover nearly 90% of node labels and nearly all structural information, which significantly outper-
form the previous work. To evaluate our model in real-world problems, we perform experiments
in neural machine translation task. Our model shows an improvement on both performance and
robustness.
Our main contributions are three-fold:
•	We propose a tree-to-tree model with specially designed encoder unit and decoder unit.
•	We design a novel objective function with content loss and structure loss for tree.
•	Experiments on both tree recovery evaluation and machine translation shows a great im-
provement.
The complete training and testing code are available at http://www.panda.com
2	Related Work
Tree-based models are widely used in recent AI research thanks to its ability to restore structure
information which has been proved to help raise up performance in practice, like Wang et al. (2011)
in image retrieval and Mou et al. (2015) in Heuristic Matching.
Recursive Neural Network(RNN) models are a popular family of tree-based deep learning models
especially for NLP research. The main difference among these models lies in how the tree structure
is composited, i.e., how to obtain the representation for a phrase or sentence with the representa-
tions of words it consists of. The Socher et al. (2011b) propose a basic form of RNN models, which
represent phrases and words as vectors. It employ matrices and tensors as operators to perform com-
positions in a bottom-to-up way and finally reach a sentence embedding vector. Recently, there have
been several studies addressing more semantic composition for RNNs. RNTN (Socher et al., 2013b)
employs a tensor to model quadratic form interactions rather than basic matrix multiplication. MV-
RNN (Socher et al., 2012) assigns each word with a vector and a matrix specifically. Also there have
been studies on untied parameter tree composition including Socher et al. (2013a) and Hermann &
Blunsom (2013) which utilize different composition functions according to the combined nodes’
syntactic tags according to parsing results. Recently an adaptive framework were proposed (Dong
et al., 2016) which selects composition function based on only the input vectors’ value of their chid
nodes.
As a combination of recursive neural network and LSTM, Tree-LSTM was first proposed in Zhu
et al. (2015). It utilizes a memory cell to reflect the history memories of multiple child cells or mul-
tiple descendant cells recursively. Following its step, it is applied in NLP tasks and outperform the
state-of-art result including language inference in Chen et al. (2017; 2016), sentiment classification
and paraphrase analysis (Tai et al., 2015). In the above models, the Tree-LSTM structure is applied
in the sentence encoding process. For decoding and tree generation process (Zhang et al., 2015)
applies top-down Tree-LSTM to generate dependency parse tree.
As the aspect of tree language model, Brychcin (2016) proposes a model that construct a tree struc-
ture for different sentences without utilizing syntax information like parsing trees. Bechet et al.
(2001) and Richardson et al. (2016) derive a tree language model dependent on dependent parsing
trees. As far as we know, we are the first to take consistency parsing information into consideration
when constructing a tree-based language model.
3	Model
In this paper, we introduce a novel tree-to-tree architecture, which first encodes the source tree as a
context vector by a carefully designed encoder and to decode the vector to a target tree by a decoder.
The overall workflow is shown in Table 1.
2
Under review as a conference paper at ICLR 2018
Encode Cell
Decode Cell
Split Unit
Figure 1: Tree2Tree Structure. Encoder cell get information from input and child nodes. Decoder
uses state of root node of encoder(I5) as the state of root node of decoder(lι). For nodes in decoder,
We first decide the number of child cells by split units. For example lι and l3 have 3 and 2 child
nodes, while other nodes have none. Then nodes split information for their children.
3.1	Tree Memory Encoder(TME)
As intuitively showed in Figure 2(a), our encoder is improved from Socher et al. (2011a), which
ensures that important information from nodes of lower level can be transmitted to the root node
without much decay. In order to reconstruct trees from representation vectors generated by encoder,
we designed a novel tree LSTM decoder. At each node of the decoder, information is properly
devided into several parts and passed to its child nodes. The structure of Tree2Tree LSTM is shown
in Figure 1.
While Socher et al. (2011a) utilize LSTM encoder to encode trees into a representation vectors con-
taining corresponding node and structure information, we employ a recursive decoder to transform
the representation vectors back to trees.
We define tree-encoder as a collection of recursively organized encoder cells. Given the tree struc-
ture and the inputs of the leaf nodes, denoted as χ1,χ2, ∙ ∙ ∙ ,Xn each represents an N-dimension
vector. Each time a parent vector P is computed from its child nodes c1,c2 ,∙一with equation .
P = TME([cι; C2；…])
Here [ci； c2;…]denotes the concatenation of the children. From bottom to up, all the nodes are
computed recursively.
Concretely, our TME unit is shown as follows.
n
it =σ(XW((ki))ht-1(k)+u(i)It+b(i))
k=1
n
ft(p) =σ(XW((kf))(p)ht-1(k)+u(f)(p)It+b(f)(p))
k=1
n
ct = X ft(k)	ct-1(k) + it	It
k=1
ot = σ(W (o)ct + b(o))
ht = ct	ot
(1)
ct(i), ht(i), (i = 1,2,…，n) are the memory and output vectors of the child units. It the input.
ft(i), (i = 1, 2,…，n) are forget gates determining how much information from ct(i) will be dis-
carded. it is the input gate deciding which part ofI(i) can get into ct. Since the value of gates vary
for different cell, information can be maintained for a long time.
3
Under review as a conference paper at ICLR 2018
(a) The Structure of Encoder cell. Ct-ι(k) is values of
child node,s memory units. It is the input. After passing
through forget gate f(k) and input gate it, information
from Ct-ι (k) and It are combined to generate new cell
state and hidden state, Ct and ht.
(b) Generation process for
one of the child units. Ct-ι
and ht-ι are cell state and
hidden state of parent cell.
After passing through mem-
ory gate mt(k) for the k-
th child node, a specific part
of information from Ct-ι is
transmitted to the k-th child
node.
Figure 2: Units of Our Model
3.2	Tree Memory Decoder(TMD)
While tree-encoder is natural extention of sequence LSTM cell, there is a large difference between
tree-decoder and sequence LSTM decoder. Sequence LSTM encoder only only need to recover
content information, but tree-decoder should also resolve structure information. That is to say, a
tree-decoder cell need to decide what to output, as well as the number child cells. So the structure
of tree-decoder is much much complicated that sequence-LSTM cell. The structure of decoder is
shown in Figure 2(b).
nu Jprobt = softmax(WCnu^ Ct + Whnu) ht + b(nu))
nut = argmax(nu_Probt)
mt(k) = σ(Wc(m)(k)ct +Wn(m)(k)ht +b(m)(k))
ct (k) = mt(p)	ct
nt(k) = σ(W(o)ct(k) + b(o))
ht (k) = ct (k) ot(k)
(2)
where nut is a gate decide how much child nodes to generate.
mt(i)(i = 1,…，nut) are memery-keep cells determining how information is splited from
ct,nt(i)(i = 1,…，nut) are output gates of child nodes, and ht(i), (i = 1,…，nut) are the
corresponding outputs. At each step cells decide what to output, how many children to generate and
the proportion of remaining information to pass to a specific child node. According to the spilt num-
ber nut determined by our decoding unit at each time, we generate the child nodes c1,c2,…,CnUt
through the parent node with equation .
[C1； C2； •一；Cnut] = TMD(P)
(3)
3.3	Object function
There are two criterion to assess the ability of autoencoder - the correctness of structure and precision
of content recovery. So we need to minimize structure loss L1 and content loss L2 at the same time.
We define L1 , L2 as the cross entropy of generating the right number of child nodes and output the
4
Under review as a conference paper at ICLR 2018
right token. We use L = L1 + λL2 as our training loss. And we use Adam(Kingma & Ba, 2014) as
the optimizer, which is more efficient and convenient to use than SGD.
For the target tree Tr and the output tree To,
Lcontent = -E (I(i,T) ∙ (£ (h(i,j)log(labelTr⑴∙ label(T°(j))))))	(4)
i∈Tr	j∈To
Lstructure = -E (I(i,T) ∙ (E (h(i,j)log(ChildnumTr(i) ∙ ChildnumTo(j)y))	(5)
i∈Tr	j∈To
where,
I(i, T)
1,if∃j ∈ T, the position ofiis same asj,
0,otherwise
H(i,j)
1,if the position ofi, j are same,
0,otherwise
labelTr (i), Childnum means the label and child number of node i in tree Tr. Node i and node j have
the same position if and only if the position of the parent and left brother of i, j are same.
4	Experiment
To demonstrate the effectiveness of our model, we test our model on two tasks, which includes the
tree reconstruction task and machine translation task.
4.1	Experiment Setup
Dataset
For for tree reconstruction task, we choose the mathematics genealogy dataset 1. Each tree represents
a teacher-student relationship of mathematicians. A node indicate a person with its child nodes as
the person’s direct students. Each node is represented by an integer label. For nodes with more than
2 child nodes, we split the child nodes with each cluster sized 2, and construct corresponding trees.
Finally we got 10000 binary trees. We split all these data into 3 parts, with 9000, 500, 500 each for
training/validation/test.
For the machine translation task, our data set consists of 350,000 sentences from United Nations
Parallel Corpus (Ziemski et al., 2016). We choose the first 50,000, 100,000, 340,000 sentence pairs
as training set at each time, the validation set and test set both consist of 5,000 sentences. For
Chinese corpus, Jieba 2 is applied in tokenization. We use Stanford parser (Klein & Manning, 2003;
Levy & Manning, 2003) to get parsing tree of each sentence.
Implementation Details
All our experiments are run on GPUs in tensorflow framework. The autoencoder is trained on our
model and the baseline separately with the training set. All parameters in our model are initialized
with a uniform distribution within [-0.01, 0.01].
For machine translation tasks, our dictionary consists of all the words occurring in the training set,
with each word vector initialized via Glove (Pennington et al., 2014) method in 300 dimensions and
set to be joint trained along the model weights.
4.2	Experiment Results
Tree Reconstruction
In this part, we design a self-to-self task to evaluate our model’s recovery ability on arbitrary trees.
Our model is compared with recursive Auto-Encoder (RAE) (Socher et al., 2011a).
1https://genealogy.math.ndsu.nodak.edu/
2https://pypi.python.org/pypi/jieba/
5
Under review as a conference paper at ICLR 2018
We evaluate the performance of recovery on label accuracy and structure accuracy. The label accu-
racy reflects how many nodes are correctly predicted, and the structure accuracy denotes the percent-
age of trees whose predicted structure is the same as the reference. Our Tree2Tree model reaches
85.9% in labeled accuracy and 100% in structure annuracy, far higher than the baseline which is
73.4% and 79.6%. The exciting result reflects that our tree2tree perfectly maintain almost all of the
structured information When it is utilized as an autoencoder.
Figure 4.2 shows a example of tree reconstruction result. we can observe that our model reconstructs
the original tree much better than the baseline in both node level and structure level.
Euler
Lagrange PoiSSon
DiriChIet
Euler
Lagrange ChaSleS
Chasles
DiriChlet Chasles
Kronecker Lipschitz
Lerch Mertens
KroneCker Lipschitz
Kronecker Mertens
(c) RAE
(a)	Origin Genealogy
(b)	Tree2Tree
Figure 3: A example of tree reconstruction result. (a) is the reference tree. (b) and (c) denote
the reconstruction result of our Tree2Tree model and the baseline model (Socher et al., 2011a),
respectively.
Machine Translation
In this part, we compare our model against the lstm baseline on English to Chinese machine trans-
lation task.
Table 1 shows Bleu (Papineni et al., 2002) scores in test set of our model and LSTM of English to
Chinese translation task. Here the loss function in our model is defined as the cross entropy between
prediction and target. We report results of both our model and LSTM with the same random seed
and pick the one with best validation perplexity for final Bleu evaluation. Translation are generated
via beam search with beam size 5. We can see that our tree2tree model outperforms the baseline for
each data scale, with obvious difference in small dataset.
	5w	10w	35w				BLEU1 Reduction(%)	BLEU2 Reduction(%)
LSTM	0~(Γ~	T59-	28.9	LSTM	K	325
Tree2Tree LSTM	13.4	18.7	29.3	Tree2Tree LSTM		18.9			30.9	
Table 1:	Reslut of Translation
Table 2:	Robustness Analysis
We also test the robustness of our model against the baseline. For each sentence, we replace one
word randomly in the original text with a word randomly chosen in the dictionary, and evaluate the
difference of the output to test the robustness. We use the reduction proportion of Bleu1 and Bleu2
scores to evaluate the robustness of translation. The result is shown in Table 2, from which we
can see that Tree2Tree LSTM are more stable than LSTM. We will prove this property in theory in
Section 5.
5	Analysis
In this section, we intruduce a Neural Tree Generation Model as a theoretical analysis which pro-
vides an explanation for our Tree2Tree in view of probability.
For tree T , we assume that the generation probability of node wi ∈ T satisfies the conditional
independent property as follows.
∀node wj 6= wi, wj ⊥wi |par(wi)
Then, we can get the generation probability of the tree in the Figure 4(c).
P(T) = P(r3|r1)P(r4|r1)P(r1|r0)P(r2|r0)P(r0)
6
Under review as a conference paper at ICLR 2018
Table 3: Examples of Robustness
Original Text
consideration of reports submitted by states parties under article 16 of the convention
financing of the united nations preventive deployment force
second periodic reports due in 1996
these figures show that womens participation in politics continues to be limited
the group of experts had before it the following documents
Reference
审议缔约国在盟约第16条下提交的报告
联合国预防性部署部队经费的筹措
应于1996年提交的第二次报告
这些数字表明妇女参与政治生活继续受到限制
专家组收到下列文件
LSTM
审议缔约国提交的报告提交的国家间工作组的报告
联合国预防性部署部队联预部队
1996年11月10日
这些数字表明妇女继续在这些部门中占主导地位
专家组的以下文件是的的的工作
Tree2Tree LSTM
审议缔约国在公约第16条下提交的报告
联合国预防性部署部队经费的筹措
应于1996年第二次定期定期报告
这些数字表明认为参与妇女人数仍然有限
专家组有以下文件
Formally, we can define the generate probability of a tree by equation 6.
P (tree) =	P (wi |par(wi))	(6)
wi node:wi
Considering the tree structure as a Bayesian Network and every node as a random vector, we can get
the set of Independence assertions of the tree.
I (tree) = {wi ⊥ N onDescendantwi |par(wi)}	(7)
Where N onDescendantwi denotes the nodes in the tree that are not descendants of wi.
According to equation 7, we can find that for two nodes with different parent nodes, the probability
of two nodes is independent when parent nodes of two nodes are given. Formally, the tree structure
can be regarded as a Bayesian Network(Koller & Friedman, 2009), which leads to the independen-
cies in the tree-structure using D-separation. For instance, Figure 4(c) provides an intuitive example
that (r3 ⊥ r2|r1).
In NLP tasks, the model above can be regarded as a neural tree-structure language model. For a
sentence (w1,w2,…,wn), We define the probability of P(w1,w2,…,Wn) as follows.
P (w1,w2, ∙∙∙ ,Wn) = P (parsing tree of (w1,w2, ∙ ∙ ∙ ,Wn))	(8)
7
Under review as a conference paper at ICLR 2018
(ROOT S)
/ X
(NP ri)	(VP r2)
I	∕∖
(PRP I) (VBP love) (VP r3)
I
(NNS pandas)
(a) Parsing Tree of 'I
love pandas’
Figure 4: Parsing Tree of Example Sentence
(c) A Tree Example
where Par(Wi) denotes the parent node of Wi in the parsing tree of the sentence.
Take 'I love pandas' for instance, the parsing tree of the sentence is shown in Figure 4, the probability
of the sentence is computed as follows(by equation 9).
P(010,0 love0,0pandas') = P(0I0∣r1 )P(0love0∣r2)P(0pandas'∣r2)P(r1∣r0)P(r2∣r0)P(r0)	(9)
The parsing tree of a sentence is constructed by the grammar of the sentence, which shows the
syntactic structure of the sentence. This is consistent with the independency we get from Bayesian
Network.
Besides, neural tree-structure language model have an another advantage. LSTM regards a sentence
with a length of n as a sequence of n words. We define the path length of a word as the number of
units the word goes through before reaching the end node of sentence embedding. For LSTM, the
end node is the last word of sentence, and for tree-structured model, the end node is the root node of
a tree. The AVE(average path length) of LSTM is O(n), but the AVE of tree LSTM is O(log(n)).
So tree LSTM usually has smaller AVE.
6 Conclusion and future work
In this paper, we introduce a novel Tree2Tree LSTM which contains a tree-structure encoder and
a tree-structure decoder. The model can apply to many fancy tasks, such as machine translation,
tree-structure generation, etc. Based on our model, some experiments and theroy analysis are done
to prove the strength of our model. In the future work, we will try to append tree-based attention to
our model to improve the performance of our model.
References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
Frederic Bechet, Yannick Esteve, and Renato De Mori. Tree-based language model dedicated to
natural spoken dialog systems. In ISCA Tutorial and Research Workshop (ITRW) on Adaptation
Methods for Speech Recognition, 2001.
Tomas Brychcin. Latent tree language model. arXiv preprint arXiv:1607.07057, 2016.
Danqi Chen and Christopher Manning. A fast and accurate dependency parser using neural net-
works. In Proceedings of the 2014 conference on empirical methods in natural language process-
ing (EMNLP), pp. 740-750, 2014.
Qian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, and Hui Jiang. Enhancing and combining sequen-
tial and tree lstm for natural language inference. arXiv preprint arXiv:1609.06038, 2016.
Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang, and Diana Inkpen. Enhanced lstm for
natural language inference. In Proc. ACL, 2017.
8
Under review as a conference paper at ICLR 2018
Li Dong, Furu Wei, Ke Xu, Shixia Liu, and Ming Zhou. Adaptive multi-compositionality for recur-
sive neural network models. IEEE/ACM Transactions on Audio, Speech and Language Processing
(TASLP), 24(3):422-431, 2016.
Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A Smith. Recurrent neural network
grammars. arXiv preprint arXiv:1602.07776, 2016.
Alex Graves, Navdeep Jaitly, and Abdel-rahman Mohamed. Hybrid speech recognition with deep
bidirectional lstm. In Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE
Workshop on, pp. 273-278. IEEE, 2013.
Karl Moritz Hermann and Phil Blunsom. The role of syntax in vector space models of compositional
semantics. In Proceedings of the 51st Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, 2013.
SePP Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic oPtimization. arXiv preprint
arXiv:1412.6980, 2014.
Dan Klein and ChristoPher D Manning. Accurate unlexicalized Parsing. In Proceedings of the 41st
Annual Meeting on Association for Computational Linguistics-Volume 1, PP. 423-430. Associa-
tion for ComPutational Linguistics, 2003.
DaPhne Koller and Nir Friedman. Probabilistic graphical models: principles and techniques. MIT
Press, 2009.
Roger Levy and ChristoPher Manning. Is it harder to Parse chinese, or the chinese treebank? In
Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,
PP. 439-446. Association for ComPutational Linguistics, 2003.
Zachary C LiPton, David C Kale, and Randall C Wetzell. PhenotyPing of clinical time series with
lstm recurrent neural networks. arXiv preprint arXiv:1510.07641, 2015.
Lili Mou, Rui Men, Ge Li, Yan Xu, Lu Zhang, Rui Yan, and Zhi Jin. Natural language inference by
tree-based convolution and heuristic matching. arXiv preprint arXiv:1512.08422, 2015.
Kishore PaPineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of the 40th annual meeting on association for
computational linguistics, PP. 311-318. Association for ComPutational Linguistics, 2002.
Jeffrey Pennington, Richard Socher, and ChristoPher Manning. Glove: Global vectors for word
rePresentation. In Proceedings of the 2014 conference on empirical methods in natural language
processing (EMNLP), PP. 1532-1543, 2014.
John Richardson, Taku Kudo, Hideto Kazawa, and Sadao Kurohashi. A generalized dePendency
tree language model for smt. Information and Media Technologies, 11:213-235, 2016.
Richard Socher, Eric H Huang, Jeffrey Pennin, ChristoPher D Manning, and Andrew Y Ng. Dy-
namic Pooling and unfolding recursive autoencoders for ParaPhrase detection. In Advances in
Neural Information Processing Systems, PP. 801-809, 2011a.
Richard Socher, Cliff C Lin, Chris Manning, and Andrew Y Ng. Parsing natural scenes and natural
language with recursive neural networks. In Proceedings of the 28th international conference on
machine learning (ICML-11), PP. 129-136, 2011b.
Richard Socher, Brody Huval, ChristoPher D Manning, and Andrew Y Ng. Semantic comPosi-
tionality through recursive matrix-vector sPaces. In Proceedings of the 2012 joint conference on
empirical methods in natural language processing and computational natural language learning,
PP. 1201-1211. Association for ComPutational Linguistics, 2012.
Richard Socher, John Bauer, ChristoPher D Manning, et al. Parsing with comPositional vector
grammars. In Proceedings of the 51st Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), volume 1, PP. 455-465, 2013a.
9
Under review as a conference paper at ICLR 2018
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng,
and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment
treebank. In Proceedings of the 2013 conference on empirical methods in natural language pro-
cessing ,pp.1631-1642, 2013b.
Kai Sheng Tai, Richard Socher, and Christopher D Manning. Improved semantic representations
from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075, 2015.
Xiaoyu Wang, Ming Yang, Timothee Cour, Shenghuo Zhu, Kai Yu, and Tony X Han. Contextual
weighting for vocabulary tree based image retrieval. In Computer Vision (ICCV), 2011 IEEE
International Conference on, pp. 209-216. IEEE, 2011.
Xingxing Zhang, Liang Lu, and Mirella Lapata. Top-down tree long short-term memory networks.
arXiv preprint arXiv:1511.00060, 2015.
Xiaodan Zhu, Parinaz Sobihani, and Hongyu Guo. Long short-term memory over recursive struc-
tures. In International Conference on Machine Learning, pp. 1604-1612, 2015.
Michal Ziemski, Marcin Junczys-Dowmunt, and Bruno Pouliquen. The united nations parallel cor-
pus v1. 0. In LREC, 2016.
10