Under review as a conference paper at ICLR 2018
Understanding Deep Learning Generalization
by Maximum Entropy
Anonymous authors
Paper under double-blind review
Ab stract
Deep learning achieves remarkable generalization capability with overwhelming
number of model parameters. Theoretical understanding of deep learning gener-
alization receives recent attention yet remains not fully explored. This paper at-
tempts to provide an alternative understanding from the perspective of maximum
entropy. We first derive two feature conditions that softmax regression strictly
apply maximum entropy principle. DNN is then regarded as approximating the
feature conditions with multilayer feature learning, and proved to be a recursive
solution towards maximum entropy principle. The connection between DNN and
maximum entropy well explains why typical designs such as shortcut and regular-
ization improves model generalization, and provides instructions for future model
development.
1	Introduction
Deep learning has achieved significant success in various application areas. Its success has been
widely ascribed to the remarkable generalization ability. Recent study shows that with very lim-
ited training data, a 12-layer fully connected neural network still generalizes well while kernel
ridge regression easily overfits with polynomial kernels of more than 6 orders (Wu et al., 2017).
Classical statistical learning theories like Vapnik-Chervonenkis (VC) dimension (Maass, 1994) and
Rademacher complexity (Neyshabur et al., 2015) evaluate generalization based on the complexity
of the target function class. It is suggested that the models with good generalization capability are
expected to have low function complexity. However, most successful deep neural networks already
have over 100 hidden layers, e.g., ResNet (He et al., 2016) and DenseNet (Huang et al., 2016) for
image recognition. The number of model parameters in these cases is even larger than the number
of training samples. Statistical learning theory cannot well explain the generalization capability of
deep learning models (Zhang et al., 2017).
Maximum Entropy (ME) is a general principle for designing machine learning models. Models
fulfilling the principle of ME make least hypothesis beyond the stated prior data, and thus lead to
least biased estimate possible on the given information (Jaynes, 1957). Appropriate feature func-
tions are critical in applying ME principle and largely decide the model generalization capabili-
ty (Berger et al., 1996). Different selections of feature functions lead to different instantiations of
maximum entropy models (Malouf, 2002; Yusuke & Jun’ichi, 2002). The most simple and well-
known instantiation is that ME principle invents identical formulation of softmax regression by se-
lecting certain feature functions and treating data as conditionally independent (Manning & Klein,
2003). It is obvious that softmax regression has no guaranty of generalization, indicating that in-
appropriate feature functions and data hypothesis violates ME principle and undermines the model
performance. It remains not fully studied how to select feature functions to maximally fulfill ME
principle and guarantee the generalization capability of ME models. Maximum entropy provides a
potential but not-ready way to understand deep learning generalization.
This paper is motivated to improve the theory behind applying ME principle and use it to understand
deep learning generalization. We research on the feature conditions to equivalently apply ME princi-
ple, and indicates that deep neural networks (DNN) is essentially a recursive solution to approximate
the feature conditions and thus maximally fulfill ME principle.
1
Under review as a conference paper at ICLR 2018
•	In Section 2, we first revisit the relation between generalization and ME principle, and con-
clude that models well fulfilling ME principle requires least data hypothesis so to possess
good generalization capability. One general guideline for feature function selection is to
transfer the hypothesis on input data to the constrain on model features 1. This demonstrates
the role of feature learning in designing ME models.
•	Section 3 addresses what features to learn. Specifically, we derive two feature conditions to
make softmax regression strictly equivalent to the original ME model (denoted as Maximum
Entropy Equivalence Theorem). That is, if the utilized features meet the two conditions,
simple softmax regression model can fulfill ME principle and guarantee generalization.
These two conditions actually specify the goal of feature learning.
•	Section 4 resolves how to meet the feature conditions and connects DNN with ME. Based
on Maximum Entropy Equivalence Theorem, viewing the output supervision layer as soft-
max regression, the DNN hidden layers before the output layer can be regarded as learning
features to meet the feature conditions. Since the feature conditions are difficult to be direct-
ly satisfied, they are optimized and recursively decomposed to a sequence of manageable
problems. It is proved that, standard DNN uses the composition of multilayer non-linear
functions to realize the recursive decomposition and uses back propagation to solve the
corresponding optimization problem.
•	Section 5 employs the above ME interpretation to explain some generalization-related
observations of DNN. Specifically, from the perspective of ME, we provide an alterna-
tive way to understand the connection between deep learning and Information Bottle-
neck (Shwartz-Ziv & Tishby, 2017). Theoretical explanations on typical generalization
design of DNN, e.g., shortcut, regularization, are also provided at last.
The contributions are summarized in three-fold:
1.	We derive the feature conditions that softmax regression strictly apply maximum entropy
principle. This helps understanding the relation between generalization and ME models,
and provides theoretical guidelines for feature learning in these models.
2.	We introduce a recursive decomposition solution for applying ME principle. It is proved
that DNN maximally fulfills maximum entropy principle by multilayer feature learning and
softmax regression, which guarantees the model generalization performance.
3.	Based on the ME understanding of DNN, we provide explanations to the information bot-
tleneck phenomenon in DNN and typical DNN designs for generalization improvement.
2	Revisiting Generalization and Maximum Entropy
In machine learning, one common task is to fit a model to a set of training data. If the derived
model makes reliable predictions on unseen testing data, we think the model has good generalization
capability. Traditionally, overfitting refers to a model that fits the training data too well but generalize
poor to testing data, while underfitting refers to a model that can neither fits the training data nor
generalize to testing data (Vapnik & Vapnik, 1998).
As a criterion for learning machine learning models, ME principle makes null hypothesis beyond the
stated prior data (X, Y ) where X, Y denote the original sample representation and label respectively.
To facilitate the discussion between generalization and maximum entropy, we revisit generalization,
overfitting and underfitting by how much data hypothesis is assumed by the model:
•	Underfitting: Underfitting occurs when the model’s data hypothesis is not satisfied by the
training data.
•	Overfitting: Overfitting occurs when the model’s data hypothesis is satisfied by the training
data, but not satisfied by the testing data.
1 Here “feature” refers to the variable directly used by machine learning models like softmax regression.
To avoid confusion, from now on, we will refer to the feature function in ME models as “predicate func-
tion” (Jeon & Manmatha, 2004).
2
Under review as a conference paper at ICLR 2018
•	Generalization: According to ME principle, a model with good generalization capability
is expected to have as less extra hypothesis on data (X, Y ) as possible.
The above interpretation of underfitting and overfitting can be illustrated with the toy example in
Fig. 1(left). The underfitting model in solid line assumes linear relation on (X, Y ), which is not
satisfied by the training data. The model in dot dash line assumes 5-order polynomial relation
on (X, Y ), which perfectly fits to the training data. However, it is obvious that the hypothesis
generalizes poorly to testing data and the 5-order polynomial model tends to overfitting. A coarse
conclusion reaches that, introducing extra data hypothesis, whether or not fitting well to the training
data, will lead to degradation of model generalization capability.
Figure 1: (Left) fitting 5 data points with linear (solid line) and 5-order (dot dash line) polynomials.
(Right) illustration of different model settings with/without data hypothesis; from top to bottom:
non-ME model with data hypothesis, original ME model without data hypothesis, simple model
with feature constraint (equivalent to original ME).
One question arises: why ME models cannot guarantee good generalization? Continuing the discus-
sion in Introduction, to enable the enumeration of predicate states, most ME models explicitly or im-
plicitly introduce extra data hypothesis, e.g., softmax regression assumes independent observations
when applying ME principle. Imposing extra data hypothesis actually violates the ME principle and
degrades the model to non-ME (Maximum Entropy) model. The dilemma is: generalization requires
no extra data hypothesis, but it is difficult to derive simple models without data hypothesis. Is there
solution to apply ME principle without imposing hypothesis on the original data?
While the input original data (X, Y ) is fixed and maybe not compatible with the hypothesis, we can
introduce model feature T sufficient to represent data, and transfer the data hypothesis to feature
constraint. Ideally the model defined on feature T is a simple ME model (e.g., softmax regression),
so that we can easily apply ME principle without imposing extra data hypothesis. In this case, the
simple model plus feature constraint constitutes an equivalent implementation to ME principle and
possesses good generalization capability. Fig. 1(right) illustrates these model settings with/without
data hypothesis. It is easy to see that, from the perspective of applying ME, feature learning works
between data and feature, with goal to realizing the feature constraints.
3	Feature Conditions for Maximum Entropy Equivalence
According to the above discussions, when applying ME principle, the problem becomes how to
identify the equivalent feature constraints and simple models. Since the output layer of DNN is
usually softmax regression, this section will explore what feature constraints can make softmax
regression equivalent to the original ME model.
3.1	Review of Original ME Model & Feature-based Softmax Model
We first review the definition of original ME model and feature-based softmax model in this subsec-
tion. Note that in defining the original ME model, instead of using predicate functions as most ME
3
Under review as a conference paper at ICLR 2018
models, we deliver the constraints of (X, Y ) with joint distribution equality 2. Before defining the
softmax model, to facilitate the transfer of data hypothesis to feature constraint, we first provide the
definition of feature T over input data X , and then derive the general formulation of feature-based
ME model. Feature-based softmax model can be seen as a special case of feature-based ME model.
Definition 1. (Original Maximum Entropy Model) Supposing the dataset has input X and label
Y, the task is to find a good prediction of Y using X. The prediction Y needs to maximize the
ʌ
conditional entropy H(Y|X) while preserving the same distribution with data (X, Y). This is
formulated as:
ʌ
min -H(Y∣X)	(1)
s.t. P(X,Y) = P(X,Y), E P(Y∣X) = 1
This optimization question can be solve by lagrangian multiplier method :
L = X P(X)P(Y∣X)log(P(Y∣X)) + ω0 11 - XP(Y∣x) I
XY	∖ Y J
+ X 3i (P(X, Y) - P(X)P(Y = Y∣X))
X,Y
The above equation can be equivalently written with the original defined predicate function
in (Berger et al., 1996):
L = EP(X)P(Y∣x)log(P(Y∣x)) + 3o U-EP(Y∣x)I
XY	∖ Y	/
+ X 3i(X P(X, Y) fi(X, Y) - X P(X)P(Y∣X) f (x,Y)
i	XY,Y	X^Y
where fi(X, Y) is predicate function, which equalizes 1 when (X, Y) satisfies a certain status:
fi(X,Y)= 10,
X = xi , Y = yi
others
(2)
The solution to the above problem is:
ʌ
Pω (Y = y∣X = χ)=
1
Gexp
Xi 3ifi (x, y)
(3)
Zω (x) =	exp i 3ifi (x, y)	(4)
Definition 2. (Feature-based Maximum Entropy Model) T is defined as a set of features over
input X: T is only related to X, and ti denotes the confidence of feature Ti happening in input
status x. In other words, ti(x) = P(Ti = 1∣X = x), and P(Ti = 0∣X = x) = 1 - ti(x).
According to the above definition of feature T, feature-based maximum entropy model can be for-
mulated as :
ʌ
min -H(YT∣T)	(5)
s∙t. EP(TY)= EP(TY), ^XP(^Y∣T) = 1
2 In the early paper introducing ME models, predicate functions are used to instantiate ME principle for
model derivation. It is easily to find that according to the original predicate function defined in (Berger et al.,
1996), the expectation over predicate functions is exactly joint distribution.
4
Under review as a conference paper at ICLR 2018
Definition 3. (Feature-based Softmax Model) Feature-based softmax model is a special case of
feature-based ME model with manageable predicate functions. The loglinear solution of feature-
based softmax regression model based on feature T is:
P(Y = y∣x = χ) = Zxexp 卜(y) + X λi(y)ti(X)
(6)
Z(x) =	exp b(y) + i λi (y)ti (x)	(7)
where λi(y) andb(y) denote functions of y: λi (y) is weight for feature Ti, and b(y) is the bias term
in softmax regression.
3.2 Maximum Entropy Equivalence Theorem
From Eqn. (3) and Eqn. (4), we find it impossible to traverse all status of (X, Y ), making the original
ME problem difficult to solve. To address this, many studies are devoted to designing special kind
of predicate functions to make the problem solvable. However, recalling the discussion on ME
and generalization in Section 2, if extra data hypothesis is imposed on (X, Y ), the generalization
capability of the derived ME model will be undermined. An alternative solution is to design the
predicate function by imposing constraints on intermediate feature T instead of directly on input
data (X, Y ).
On imposing the feature constraints, two issues need to be considered: (1) not arbitrary T makes the
feature-based ME model equivalent to the original ME model; (2) under the premise of equivalence,
T should make the derived ME model solvable (like the softmax regression). Based on these consid-
erations, we prove and derive two necessary and sufficient feature conditions to make feature-based
softmax regression (Definition 3) strictly equivalent to the original ME model (Definition 1). The
theorem is denoted as Maximum Entropy Equivalence Theorem.
Theorem (Maximum Entropy Equivalence Theorem). Given the input data X, Y and feature T,
the necessary and sufficient conditions of feature-based softmax model equivalent to the original
maximum entropy model are:
<condition 1>: X and Y are conditionally independent given T;
〈condition 2>: allfeatures of T: {Tι1,…，Ti,…，Tn} are conditionally independent given Y.
The proof to the theorem is given in Section A in the Appendix. The first condition ensures that
feature-based ME model is equivalent to the original ME model, and thus be denoted as equivalent
condition. The second condition makes feature-based ME model solvable and converted as feature-
based softmax regression problem. We denote the second condition as solvable condition. This
theorem on one hand derives operable feature constraints that softmax regression is equivalent to the
original ME model, on the other hand provides theoretical guidance to feature learning with goal of
improving model generalization.
4	Modeling DNN as Maximum Entropy
Based on the derived Maximum Entropy Equivalence Theorem, the original ME model is equivalent
to a feature-based softmax model with two feature constraints. In this way, from the perspective of
maximum entropy, if DNN uses softmax as the output layer, the previous latent layers can be seen as
the process of feature learning to approach these constraints. However, these feature constraints are
difficult to be satisfied directly, and therefore being decomposed to many smaller and manageable
problems for approximation. This section claims that DNN actually uses the composition of mul-
tilayer non-linear functions to realize a recursive decomposition towards these feature constraints.
In the following we will first introduce the recursive decomposition operation to difficult problem,
and then prove that DNN with sigmoid-activated hidden layers and softmax output layer is exactly
a recursive decomposition solution towards the original ME model.
5
Under review as a conference paper at ICLR 2018
4.1	Recursive Decomposition
A common way to solve a difficult problem is relaxing the problem to an easier one, like majorize-
minimize algorithms (Hunter & Lange, 2004). Inspired by this, we introduce a special case of relax-
ation solution to difficult problem: recursive decomposition. Decomposable problem is first defined
as follows:
Definition 4. (Decomposable problem) If a difficult optimization problem is equivalent to a man-
ageable problem, but with additional constraints only related to extra added parameters, this prob-
lem is a decomposable problem.
Obviously, according to Maximum Entropy Equivalence Theorem, the original ME problem is such
a decomposable problem. If the original problem P is decomposable, and P is equivalent to a
manageable problem P1 with additional constraints C1 , we denote it as P = P1 + C1 . In this case,
we can solve P1 + C1 instead of directly solving P .
Since P1 is easy to solve, it remains to satisfy the constraint C1. The constrain C1 can be approxi-
mately satisfied by an optimization problem p1 as its upper bound. From Definition 4, we know that
p1 is only related to the extra added parameters. Now, we have P = P1 + p1.
Ifp1 is solvable, we can use an algorithm similar to EM to solve P1 + p1:
(1)	fix parameters in p1 and optimize P1;
(2)	fix parameters in P1 and optimize p1 ;
(3)	iterate (1) and (2) until convergence.
However, sometimes p1 is still difficult to solve but decomposable. In this case, we need further
decompose p1 to a manageable problem P2 with smaller problem p2 under condition that p1 =
P2 + p2 . The problem transfers to solve P = P1 + P2 + p2 in a similar iterative way. If p2 is still
difficult, We can repeat this process to get P3 ,p4,…，pl until PL is small enough that PL ≈ PL and
PL is manageable. Since this constitutes a recursive process, we denote this way of relaxation as
recursive decomposition.
The optimization process of recursive decomposition is also recursive. Given the decomposition of
difficult problem P = Pi +---+ Pl + + PL, We have the following optimization process:
⑴	fix parameters in P2, ∙∙∙ ,Pl and optimize Pi;
(2)	fix parameters in P1,P3,…，Pl and optimize P2;
...
(L) fix parameters in Pi,P2,…，Pl- 1 and optimize PL;
(L+1) iterate (1), (2), ∙∙∙ ,(L) until convergence.
The premise behind this method is that, if We change the constraints of problem to a minimum
problem ofits upper bound, the neW problem is still a better approximation than the original problem
Without constraint.
4.2	DNN is Recursive Decomposition Solution towards ME
This subsection Will explain that DNN is actually a recursive decomposition solution toWards max-
imum entropy, and the back propagation algorithm is a realization of parameter optimization to the
model. According to Maximum Entropy Equivalence Theorem, the original ME model is equivalent
to softmax model With tWo feature constraints, Which is a typical decomposable problem. In the
folloWing We employ the above introduced recursive decomposition method to solve it: the original
ME problem is the difficult problem P, softmax model is the manageable problem Pi , and the tWo
conditions constitutes the constraints Ci related only to feature T .
While the feature constraints Ci are still difficult to be satisfied, We relax the constraints to smaller
problems using the folloWing Feature Constraint Relaxation Theorem.
Theorem (Feature Constraint Relaxation Theorem). The constraints in Maximum Entropy Equiv-
alence Theorem onfeature T = T1 ,T, ∙∙∙ , Tn：
6
Under review as a conference paper at ICLR 2018
(1)	mutual information I(X; Y |T) = 0
(2)	conditional mutual information I(Ti, Tj|Y ) = 0 , for all i 6= j
can be relaxed to the following optimization problem:
min —岁 λiH (Ti∣X)
s	.t. EP (X,Y) = EP (X,S(T))
where S(T) denotes the output of softmax model if input is T.
(8)
This theorem is proved in Section B in the Appendix. The above relaxed minimization problem
constitutes p1, which optimizes feature T = T1 , T2, ..., Tn. Using the derivation from the proof
for the above theorem, We know that minimization of 一 P- λiH(局X) leads to I(Ti; Tj ∣Y)=
0 for all i 6= j . The fact that Ti, Tj is independent allows to split p1 further to n smaller problems
P11,… ,p ι i, ∙∙∙ ,p ι n, where P ι - is an optimization problem with the same formulation as Eqn. (8)
but defined over T- .
Note that the new optimization problems P1- are still difficult ME problems, which need to be
decomposed and relaxed recursively till problem PL- ≈ PL- where PL- is manageable. According
to Maximum Entropy Equivalence Theorem, each decomposed manageable problem Pl- is realized
by a softmax regression. Since feature T- is binary random variable, the models for feature learning
change to logistic regression. For a L-depth recursive decomposition, the original ME model is
approximated by PlL=1 nl logistic regression models and one softmax regression model (nl denotes
the number of features at the l-th recursion depth). It is easy to find that this structure perfectly
matches a basic DNN model: the depth of recursion corresponds to the network hidden layer (but
in opposite index, i.e., the L-th depth recursion corresponds to the 1st hidden layer), the number of
features at each recursion correspond to the number of hidden neurons at each layer, and the logistic
regression corresponds to one layer of linear regression with sigmoid activation function.
Therefore, we reach a conclusion that DNN is a recursive decomposition solution towards maximum
entropy. The generalization capability is thus guaranteed under the ME principle. This explains why
DNN is designed as composition of multilayer non-linear functions. Moreover, the model learning
technique, backpropagation, actually follows the same spirits as the optimization process in recursive
decomposition for DNN parameter optimization.
5	Explaining DNN via Maximum Entropy
After modeling DNN as a recursive decomposition solution towards ME, in this section, we use the
ME theory to explain some generalization-related phenomenon about DNN and provide interpreta-
tions on DNN structure design. Specifically, Section 5.1 explains why Information Bottleneck exists
in DNN, and Section 5.2 explains why certain DNN structure design can improve generalization.
5.1	Explaining Information Bottleneck
In the Information Bottleneck (IB) theory (Tishby et al., 1999), given data (X, Y), the optimiza-
tion target is to minimize mutual information I(X; T) while T is a sufficient statistic satisfying
I(T; Y) = I(X; Y). Shwartz-Ziv & Tishby (2017) designed an experiment about DNN and found
that the intermediate feature T of DNN meets the IB theory: maximize I(T; Y) while minimizing
I(X;T).
Now, we prove that the output of constraint problem in ME model is sufficient to satisfy the Infor-
mation Bottleneck theory. In other words, basic DNN model with softmax output fulfills IB theory.
Corollary (Corollary of ME’s interpretation on Information Bottleneck). The output of maximum
entropy problem
mτin 一 y^λ-H(Ti1X) s.t. EP(x,γ) = EP(X,S(T))
-
is sufficient condition to the IB optimization problem:
mTin I(X; T) s.t.I(T; Y) = I(X; Y)
7
Under review as a conference paper at ICLR 2018
The proof of this corollary is available in Section C in the Appendix. Since DNN is an approximation
towards ME, this result explains why DNN tends to increase I(T; Y ) while reduce I(X; T) and the
Information Bottleneck phenomenon in DNN.
5.2	Explaining DNN Design for Generalization
DNN has some typical generalization designs, e.g., shortcut, regularization, etc. This subsection
explains why these designs can improve model generalization capability.
Shortcut is widely used in many CNN framework. The traditional explanation is that shortcut makes
information flow more convenient, so we can train deeper networks (He et al., 2016). But this cannot
explain why shortcut contributes to a better performance. According to the above modeling of DNN
as ME, CNN is a special kind of DNN where we use part of input X at each layer to construct
the model. The actual input of CNN is related to the size of corresponding convolution kernel, and
receives only part of X within its receptive field. Shortcut enriches different size of receptive fields
and thus reserve more information from X during problem decomposition in the recursion process.
The regularization in DNN can be seen as playing similar role as the feature conditions in Maximum
Entropy Equivalence Theorem. Achille & Soatto (2017) demonstrated that the regularization design,
like sgd, L2-Norm, dropout, is equal to minimizing the mutual information I(X; T). Since we have
proved that I(X; T) ≥ I(Ti; Tj) ≥ I(Ti; Tj|Y ) in Appendix C, these regularization designs thus
help to minimize the upper bounds of I(Ti; Tj|Y ) and approximate the solvable condition.
The ME modeling of DNN also sheds some light on the role of network depth in generalization
performance. Following the recursive decomposition discussion, it seems network with more layers
leads to deeper recursion and thus closer approximation towards ME. However, it is noted that
we are using relaxed optimization to replace the original constraints. Considering the continuous
minimization of upper bound, simple DNN with too many hidden layers may not always guarantees
the performance. We emphasize that for those CNNs with good architecture, more hidden layers
bring richer receptive fields and less loss of information in X. In this case, increasing network depth
will contribute to generalization improvement.
6	Conclusion and Future Work
This paper regards DNN as a solution to recursively decomposing the original maximum entropy
problem. From the perspective of maximum entropy, we ascribe the remarkable generalization
capability of DNN to the introduction of least extra data hypothesis. The future work goes in two
directions: (1) first efforts will be payed to identifying connections with other generalization theories
and explaining more DNN observations like the role of ReLu activation and redundant features; (2)
the second direction is to improve and exploit the new theory to provide instructions for future model
development of traditional machine learning as well as deep learning methods.
References
Alessandro Achille and Stefano Soatto. On the emergence of invariance and disentangling in deep
representations. 2017.
Adam L Berger, Vincent J Della Pietra, and Stephen A Della Pietra. A maximum entropy approach
to natural language processing. Computational linguistics, 22(1):39-71, 1996.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected
convolutional networks. arXiv preprint arXiv:1608.06993, 2016.
David R Hunter and Kenneth Lange. A tutorial on mm algorithms. The American Statistician, 58
(1):30-37, 2004.
Edwin T Jaynes. Information theory and statistical mechanics. Physical review, 106(4):620, 1957.
8
Under review as a conference paper at ICLR 2018
Jiwoon Jeon and R Manmatha. Using maximum entropy for automatic image annotation. In Inter-
national Conference on Image and Video Retrieval, pp. 24-32. Springer, 2004.
Wolfgang Maass. Neural nets with superlinear vc-dimension. Neural Computation, 6(5):877-884,
1994.
Robert Malouf. A comparison of algorithms for maximum entropy parameter estimation. In pro-
ceedings of the 6th conference on Natural language learning-Volume 20, pp. 1-7, 2002.
Christopher Manning and Dan Klein. Optimization, maxent models, and conditional estimation
without magic. In Proceedings of the 2003 Conference of the North American Chapter of the
Association for Computational Linguistics on Human Language Technology: Tutorials-Volume 5,
pp. 8-8. Association for Computational Linguistics, 2003.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Conference on Learning Theory, pp. 1376-1401, 2015.
Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via informa-
tion. arXiv preprint arXiv:1703.00810, 2017.
Naftali Tishby, Fernando C. Pereira, and William Bialek. The information bottleneck method. Uni-
versity of Illinois, 411(29-30):368-377, 1999.
Vladimir Naumovich Vapnik and Vlamimir Vapnik. Statistical learning theory, volume 1. Wiley
New York, 1998.
Lei Wu, Zhanxing Zhu, et al. Towards understanding generalization of deep learning: Perspective
of loss landscapes. arXiv preprint arXiv:1706.10239, 2017.
Miyao Yusuke and Tsujii Jun’ichi. Maximum entropy estimation for feature forests. In Proceedings
of the second international conference on Human Language Technology Research, pp. 292-297,
2002.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In Proceedings of the International Conference
on Learning Representations (ICLR), 2017.
APPENDIX
A Proof of Maximum Entropy Equivalence Theorem
The two feature conditions can be separately proved. Firstly, we prove the necessity and sufficiency
of condition 1 (equivalent condition) for equivalence of feature-based ME model and original ME
model. Secondly, condition 2 (solvable condition) guarantees the solution of feature-based ME
model in a manageable form (i.e., softmax regression).
To prove this theorem, we first prove the following three Lemmas.
Lemma 1. If T is a set of random variables only related to X, and T satisfies condition 1, i.e.,
mutual information I(X; Y |T) = 0, then
ʌ
P (X,Y)= P(X, Y) ⇔ EP(TY) = EP(TY)
Proof. Since T is a set of random variables only related to X, it is obvious to have
ʌ
P (X,Y) = P(X,Y) ⇒ EP (TY) = EP (TY)
(9)
So the task leaves to prove
ʌ
P (X,Y) = P(X,Y) U EP(T,Y)
E
EP (T,yY)
9
Under review as a conference paper at ICLR 2018
Recall that T is a set of random variables only related to X , then
ʌ ʌ
P(X, Y) = P(X, Y) ⇔ P (X,T, Y) = P (X, T, Y)
We further have T satisfying condition 1:
I(X; Y|T) = 0 ⇒ P(X, Y|T) = P(X|T)P(Y|T)
⇒ P(X, T, Y) = P(T)P(X, Y|T) = P(T)P(X|T)P(Y|T) = P(X |T)P(T, Y)
Similarly, X → T → Y is Marcov chain, hence We have:
ʌ ʌ ʌ
I(X; Y∣T) = 0 ⇒ P(X, T, Y) = P(X ∣T)P(T, Y)
T is defined feature function on X, so P(X|T) is a constant. We further have:
EP(t,y) = EP(t,Y) ⇒ EP(x,t,y) = EP(x,tY∖ ⇔ EP(XX) = EP(x^Y)	(IO)
Note that EP(χ,γ)= EP(X Y) indicates that the predicate functions satisfy Eqn. (2) in the definition
ʌ
of original ME model, and thus is equivalent to P(X, Y) = P(X, Y).
With Eqn. (9) and Eqn. (10), We finally have:
EP(t,y) = EP(t,Y) ⇔ EP(X∙Y) = EP(x,Yr)	(II)
□
Lemma 2. If T is a set of random variables only related to X, and P (X, Y) = P (X, ~Y) ⇔
EP(τ,γ) = EP(t γ), then: T satisfies condition 1.
Proof. Since T is a set of random variables only related to X, We have
EP(XX) = EP(X^Y) ⇔ EP(x,t,y) = EP(XTY	(12)
X → T → Y is Marcov chain, We have:
ʌ ʌ ʌ
I(X; Y∣T) = 0 ⇒ P(X, T, Y) = P(X ∣T)P(T, Y)
Additionally ,
EP(t,y) = EP(TY) ⇔ EP(XX) = EP(XY) ⇔ EP(XhX) = EP(XTY)
So We can derive:
P(X, T, Y) = P(X|T)P(T,Y) ⇒ I(X; Y|T) = 0
.∙. T meets condition 1	□
Lemma 3. If T is a set of random variables only related to X that satisfies condition 1, and
EP(TY) = EP(t γ), then:
ʌ ʌ
min -H(Y|X) ⇔ min -H(Y∣T)
Proof. T is a set of random variables only related to X :
Y → X → T is Marcov chain ⇒ I(T; Y) 6 I(X; Y)	(13)
T satisfies condition 1, so:
Y → T → X is Marcov chain ⇒ I(T; Y) > I(X; Y)	(14)
With Eqn. (13) and Eqn. (14), We have: I(T; Y) = I(X;Y)
Further using Lemma1, We can derive:
EP(t,y) = EP(TY) ⇔ EP(x,y) = EP(XY) ⇔ EP(XhX) = EP(XTY)	(15)
Therefore, We get
ʌ ʌ ʌ ʌ
I(T; Y) = I(X; Y) ⇒ H(Y|X) = H(Y|X)	(16)
ʌ ʌ
.∙. min -H(Y|X) ⇔ min -H(Y∣T)	□
10
Under review as a conference paper at ICLR 2018
Theorem (Maximum Entropy Equivalence Theorem). Given the input data X, Y and feature T,
the necessary and sufficient conditions of feature-based softmax model equivalent to the original
maximum entropy model are:
<condition 1>: X and Y are conditionally independent given T;
〈condition 2>: allfeatures of T: {Tι1,…，Ti,…，Tn} are conditionally independent given Y.
Proof. With Lemma1, Lemma2 and Lemma3, we derive that condition 1 is necessary and sufficient
for the equivalence of original ME model and the following feature-based ME model:
ʌ
min -H(Y∣T)
s.t. EP(T,Y) = EP(T,Y), X P(Y 1T) = 1
Y
The above optimization problem can be solved with the following solution:
Pω (Y = y∣T) = 71-exp (X 3ifi(T,y))	(17)
Zω = X exp Xi ωifi(T, y)	(18)
However, this solution is too complex to apply. With n features T = {T1 , T2, ..., Tn} and m different
classes of Y, there will be m * 2n different f (T, Y). Condition 2 assumes the conditional indepen-
dence among feature (Ti, Tj), which derives that thejoint distribution equation P(T, Y) = P(T, Y)
ʌ
is equivalent to its marginal distribution version P(Ti, Y) = P(Ti, Y), i = 1...n. In this case, there
leaves only 2 * m * n different fi (T, Y).
According to definition, for each Ti , we have P(Ti = 1|X = x) = ti(x) and P(Ti = 0|X = x) =
1 - ti(x). Therefore, under condition 2, the predicate functions will be:
ti (x),	X = x, Y = y
fi1(Ti=1,y)=	i ,	,
0,	others
1 - ti(x),	X = x, Y = y
fi0(Ti=0,y)=	1 - ti(x),	X=x,Y=y
0,	others
We then have:
3ifi (Ti , y) = 3i0fi0 (Ti = 0, y) + 3i1 fi1 (Ti = 1, y) = 3i0 + (3i1 - 3i0)ti (x)
where 3 denotes variable about y.
We further define b(y) = Pi 3i0 and λi(y) = (3i1 - 3i0), then the solution of Eqn. (17) and
Eqn. (18) change to:
P(Y = y∣x = χ) = z1x) exp (b(y) + X λi(y)t(X))	(19)
Z (x) = X exp b(y) + Xi λi(y)ti(x)	(20)
This is the identical formulation to the general softmax regression model as in Definition 3. It also
explains why we have bias term in the softmax model. Note that ti(x) need not to be in range [0, 1]
when We use the softmax model, as We can change λ and b to achieve translation and scaling. □
11
Under review as a conference paper at ICLR 2018
B Proof of Feature Constraint Relaxation Theorem
Theorem (Feature Constraint Relaxation Theorem). The constraints in Maximum Entropy Equiv-
alence Theorem on feature T = T1 , T2, ..., Tn:
(1)I(X;Y|T)=0
(2) I(Ti,Tj|Y) = 0 , for all i 6= j
can be relaxed to the following optimization problem:
min —岁 λiH (Ti∣X)
s.t. EP (X,Y) = EP (X,S(T))
where S(T) denotes the output of softmax model if input is T.
(21)
Proof. Since T is only related to X , Ti→ X → Tj is Marcov chain, and
I(Ti； Tj) 6 I(Ti； X) ⇒ X MMT; Tj) 6 X λij(T; X)
We can relax the minimization problem to minimize its upper bound instead, so
min X λij I (Ti; Tj) U min X % I (Ti∣X) U min X % (H (X) - H(TiIX))
Additionally, we have I(Ti; Tj |Y ) 6 I(Ti; Tj), hence:
min X MI(Ti; Tj ∣Y) U min X λijI(Ti; Tj)
Since H(X) is constant, so
I(Ti ； Tj IY )=0 for all i = j U min - X 入声(Ti∣X)
i
Note that Y = S(T):
X → T → Y is M arcov chain
Recall that Y is solution to the problem Pi, and Pi has constraint EP(χ,γ)= EP(X Y). Same as
EP(X,Y) = EP (X,S(T)), we have
X → T → Y → Y is Marcov chain
⇒ I(X; YIT) =0
.∙. the solution to the optimization problem in Eqn. (21) is sufficient to satisfy the constraints of T in
Maximum Entropy Equivalence Theorem.	□
C Proof of Corollary of ME’s Interpretation on Information
B ottleneck
Lemma 4.
min -	λiH(T iIX) ⇒ min I(X; T)
i
Proof.
min - X λiH(TiIX) ⇒ min -H(TIX) ⇒ min I(X;T)
i
□
12
Under review as a conference paper at ICLR 2018
Lemma 5. T is only related to X, then
EP (X,Y ) = EP (X,S(T)) ⇒ I (T; Y ) = I (X; Y)
Proof.
EP (X,Y) = EP (X,S(T)) ⇒ Y → T → X is M arcov Chain ⇒ I(X; Y) 6 I(T; Y)
T is only related to X , so
Y → X → Tis M arcov Chain ⇒ I(T; Y) 6 I(X; Y)
・•.I (T; Y ) = I (X; Y)
□
Corollary (Corollary of ME’s interpretation on Information Bottleneck). The output of maximum
entropy problem
mTin 一	λiH(Ti1X) s.t. EP(X,Y) = EP(X,S(T))
i
is sufficient condition to the IB optimization problem:
mTin I(X; T) s.t.I(T; Y) = I(X; Y)
Proof. Summing up Lemma4 and Lemma5, the output of the constraint problem is sufficient to
solving the IB optimization problem .	□
13