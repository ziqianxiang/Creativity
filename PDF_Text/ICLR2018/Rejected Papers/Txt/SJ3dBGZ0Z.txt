Under review as a conference paper at ICLR 2018
LSH S oftmax:
Sub-Linear Learning and Inference of the
Softmax Layer in Deep Architectures
Anonymous authors
Paper under double-blind review
Ab stract
Log-linear models models are widely used in machine learning, and in particular
are ubiquitous in deep learning architectures in the form of the softmax. While
exact inference and learning of these requires linear time, it can be done approxi-
mately in sub-linear time with strong concentrations guarantees. In this work, we
present LSH Softmax, a method to perform sub-linear learning and inference of
the softmax layer in the deep learning setting. Our method relies on the popular
Locality-Sensitive Hashing to build a well-concentrated gradient estimator, using
nearest neighbors and uniform samples. We also present an inference scheme in
sub-linear time for LSH Softmax using the Gumbel distribution. On language
modeling, we show that Recurrent Neural Networks trained with LSH Softmax
perform on-par with computing the exact softmax while requiring sub-linear com-
putations.
1	Introduction
Deep neural networks have achieved impressive successes in tasks spanning vision (He et al., 2016;
Krizhevsky et al., 2012), language (Bahdanau et al., 2014), speech (Graves et al., 2013; Oord et al.,
2016) and videos (Abu-El-Haija et al., 2016). While these models can vastly differ in architecture,
activation functions, and presence of recurrence, they (almost) all share a common trait: the soft-
max layer. The softmax layer, or log-linear model, is a widely used model in machine learning
and statistics that transforms a feature vector into a distribution over the output space, modeling
log-probabilities as a linear function of the feature vector. For example, in object classification, the
softmax layer at the end of a deep convolutional network transforms a feature vector into a proba-
bility distribution over classes for the image; in language modeling using recurrent neural networks,
it maps the hidden state to a distribution over next words.
While parameterizing for logits offers modeling flexibility, inference and learning have linear run-
time in the number of classes. Indeed, both of these require computing the un-normalized probability
for every class to compute the partition function and retrieve an actual probability distribution. Prob-
lems with large output spaces arise naturally in many areas like natural language processing (NLP),
where the output space is a language’s vocabulary and can be on the order of hundreds of thousands
of elements Jozefowicz et al. (2016); Jean et al. (2014). This can also occur in computer vision
(Joulin et al., 2016) when attempting tag prediction on massive, weakly-labeled datasets such as
Flickr100M (Thomee et al., 2015).
Many solutions have been proposed to address this bottleneck, all revolving around two themes:
approximation of the softmax probabilities or computation of exact probabilities for an approximate
model. Canonical examples of the former are importance sampling (IS) or noise contrastive esti-
mation (NCE; Gutmann & Hyvarinen (2012)). Instead of computing probabilities over the whole
output space, these methods compute the softmax over a smaller, sampled vocabulary and re-weight
the probabilities, providing an unbiased estimator. An illustration of the latter is Hierarchical Soft-
max (Morin & Bengio, 2005), where the output classes are first clustered such that you only need
to compute the softmax over a smaller output space. While the former is an unbiased estimate, it
comes with no concentration guarantees, and it is often more art than science to craft proposal dis-
tributions which will provide low-variance estimators. The latter, while efficient, requires carefully
1
Under review as a conference paper at ICLR 2018
hand-crafted clustering of the output space, at the risk of making mistakes from which there is no
recovery.
More recently, estimators based on nearest neighbor search have been proposed for inference and
learning in log-linear models (Mussmann & Ermon, 2016; Mussmann et al., 2017). These estima-
tors hinge on Maximum Inner Product Search using Locality-Sensitive to retrieve the largest logits
of the distribution and account for the tail with uniformly sampled classes. They boast strong the-
oretical guarantees and well-established concentration bounds. However, they were constrained to
toy settings and not directly applicable to real-world, large-scale, machine learning. In this work, we
build upon these estimators to make them amenable to deep learning practitioners, without losing
any theoretical guarantees. We first show how they can be extended to be usable within training
of deep learning models, then present our efficient implementation, adapted to deep learning hard-
ware and frameworks. Finally, we show the applicability and efficiency of our method by evaluating
on a real-world task: language modeling. We show significant perplexity gains against competing
methods with significant speed-ups.
Our contributions are as follows:
•	We present a new deep learning layer, LSH Softmax, an efficient replacement for the soft-
max layer based on Locality-Sensitive Hashing and the Gumbel distribution, for any deep
learning architecture, with strong theoretical guarantees for sub-linear learning and infer-
ence.
•	We provide details for efficient implementation on deep learning hardware (GPUs) and
modern deep learning frameworks (Abadi et al., 2016; Maclaurin et al.)).
•	Empirically, we show, on several datasets, that training and sampling from LSH Softmax
performs similarly to an exact softmax while requiring significantly less FLOPS.
2	Background
In this section, we first provide a quick overview of Neural Networks and the most popular classifi-
cation layer, the softmax layer. We then present the Gumbel distribution (Gumbel & Lieblein, 1954)
and introduce Locality-Sensitive Hashing (Indyk & Motwani, 1998), both of which our estimator is
built upon for inference and learning. Notationally, X is the input space, e.g. X , Rd and Y is a
discrete output space: Y , {1, . . . , C}.
2.1	Neural Networks
Feedforward Networks Neural networks models are built hierarchically by applying linear and
non-linear transformations in alternating fashion. Formally, given input x ∈ X, an m-layer neural
network with σ(∙) non-linearity transforms X into h defined as:
h = σ(Wm ∙ σ(... ∙ σ(Wι ∙ x + bi) + ...)+ bm).	(1)
{Wi}i≤m and {bi}i≤m are learned weights of the network. σ(∙) denotes an element-wise non-
linearity such as ReLU (max(∙, 0)) or sigmoid ((1 + exp(-∙))-i).
Recurrent Networks Recurrent Neural Networks (RNN) are an extension of the previous setting
to arbitrarily long sequences by keeping an internal state ht . Formally, given an input sequence
(x1, . . . , xT), it can be written as a dynamical system of the form:
h0 = 0;ht = σ(Uht-1 +Vxt).	(2)
where U and V are learnable weight matrices. In practice, this parametrization is not well-
conditioned for optimization as it can be subject to vanishing or exploding gradients and in practice
the Longer Short Term Memory (LSTM; Hochreiter & Schmidhuber (1997)) is preferred.
2
Under review as a conference paper at ICLR 2018
In both cases, these outputs are then given as input to a softmax layer which produces a distribution
over the output space Y . In the rest of this work, we denote by φ the parameters of the neural
network.
2.2	Softmax
The softmax layer is the common name given to a log-linear model for multi-classification at the
end of a neural network. Let us consider the multi-classification setting with inputs in X and outputs
in Y. Given a feature vector ψ(x) and C weight vectors {θc}c≤C, the softmax layer parameterizes
the following distribution:
P(Y = c|x; θ) H exp(ψ(x)Tθc)
In the particular case of neural networks, p(y|x; θ,φ) h exp(hTθi). {hTθi}i≤c are called the
logits. It is important to note that computing the distribution over the output space, for inference
or learning, requires O(C) operations. For the rest of this work, θ denotes the parameters of the
softmax whereas φ denotes the parameters of the neural network (producing the feature).
2.3	Gumbel distribution
First introduced by Gumbel & Lieblein (1954), the Gumbel distribution is defined by the following
cumulative distribution function: p(G < s) = exp(- exp(-s)). More practically, one can sample
from the GUmbeI distribution by first sampling U 〜 U[0,1] and returning G = 一 log(- log(U)).
This distribution is particularly useful as it casts sampling as optimization.
Theorem 1 (Maddison et al. (2014)). Let {yi}i≤C be un-normalized probabilities (or logits) over
Y and let {Gi}i≤C be i.i.d Gumbel variables. Then:
arg max{yi + Gi}
i≤C
~ Categorical { 'yi}
2.4	MIPS and Locality-Sensitive Hashing
Nearest neighbor search is a task that arises in many fields, such as information retrieval. Given a
fixed set of vectors S and a distance, this task consists of, given any incoming query q, returning the
vectors closest to the query according to the specified distance. In this work, we will be interested in
the Maximum Inner Product Search (MIPS) task. Let S = {s1, . . . , sN} be a subset of Rd. Given
a query q ∈ Rd, MIPS aims at retrieving arg maxs∈S qTs.
This requires Θ(N) operations as one has to compute the dot-product of q with all elements of S.
In the case where we assume that, for a given set S, it is needed to retrieve the nearest neighbor for
a large numbers of queries, we can achieve amortized sub-linear time.
This problem is commonly addressed with space partitioning techniques, such as Locality-Sensitive
Hashing (LSH; Indyk & Motwani (1998)). LSH leverages hashing to reduce the number of candidate
vectors to evaluate, based on the idea that similar vectors will hash in the same bucket. We have the
following result:
Theorem 2 (Indyk & Motwani (1998)). Given a set S of size N, a similarity measure d(∙, ∙) and a
family of hash functions H s.t. for S > T andp > q:
•	∀x, y ∈	S, d(x, y)	≥ S ⇒ p [h(x)	=	h(y)]	≥ p.
•	∀x, y ∈	S, d(x, y)	≤ T ⇒ p [h(x)	=	h(y)]	≤ q.
we can construct a data structure s.t. given an incoming query q, a nearest neighbor can be re-
trieved, with high probability, in sub-linear time O(NP log N) with P，Iogp < L
Recent work builds on top of LSH to either reduce the number of tables (Lv et al., 2007), or utilize
more expressive hash functions (Andoni et al., 2015). A common family of hash is the hyperplane
hash, i.e. for V 〜 N(0, I), h(x) = sign (VTx), also called Signed Random Projections (Charikar,
2002). For the rest of this work, we denote b the number of hashing bits (equivalently, the number
of random vectors) per table, and L the number of tables.
3
Under review as a conference paper at ICLR 2018
3	Learning
In this section, we show how we can apply Theorem 3.5 of (Mussmann et al., 2017) to enable sub-
linear learning of softmax parameters in the context of deep models, i.e. where both weights and
inputs can change. This is crucial for real-world use.
Deep learning models for both classification (Krizhevsky et al., 2012) and generation (Mikolov,
2012) are often trained with a maximum-likelihood objective. Formally, given a training pair
(x, y) ∈ X × Y, one aims at maximizing logp(y|x; θ, φ), where θ ∈ Θ and φ ∈ Φ are respec-
tively the parameters of the softmax and of the neural network. To optimize this model, the usual
method is to use back-propagation (Rumelhart et al., 1988) to differentiate and then perform stochas-
tic gradient descent (SGD; LeCun et al. (1998)) on θ and φ.
Let’s denote by f(x; φ) , h the feature vector given as input to the softmax. Given our notation, the
objective is written as '(x, y,θ, φ)，logp(y∣x; θ, φ) = hτθy - log Pi≤c exp(hTθi). For back-
propagation, We need to compute the gradient of ' w.r.t to both θ and h - the gradient w.r.t. h is then
passed down to compute the gradient w.r.t. φ.
Vh' = θy - Ei〜p(∙∣χRφ) [θi]
Vθi' = 1i=y h - h
exp(hT θi )
(3)
Zθ(h)
with Zθ(h) , Pi exp(hTθi). Computing these gradients clearly requires O(|Y |) operations. In
practice, this constitutes a major bottleneck for large output spaces. Mussmann et al. (2017) shows
how to compute expectation in in sub-linear time, with a well-concentrated estimator using an LSH
structure. Intuitively, we can build a good estimate of the partition function by retrieving the largest
logits (using LSH) and accounting for the tail with uniform samples. Applying this result, we can
compute the expectations necessary to compute the softmax gradients in sub-linear time. This is
described in Theorem 3.
Theorem 3 (LSH Softmax for Learning). Let h = f (x; φ) be input to a softmax layer with pa-
rameters {θc}c≤c and define '(x, y, θ, φ) as previously. Given S, the k-nearest neighbors of h in
{θc}c≤C and T, l uniform samples from {1, . . . , C} - S, let us define:
Ck
Zθ(h)，EeXp(hTθi) +-----1— EeXp(hTθi)	(4)
i∈S	l i∈T
ʌ , h _ C , C - k ι ʌ h exp(hτθi)
gθi , h1i=y	I 1i∈S +	1	1i∈T I ht	今 /7、	(5)
l	Zθ(h)
gh，θy 八	^X θi eχp(hTθi) +-----^X θi eχp(hTθi)	(6)
Zθ(h) i∈S	l	i∈T
These estimators are well concentrated: i.e. for e, δ > 0, if k = l = O (n2 ∣y ɪj, then with
probability greater than 1 - δ:
lZθIh)- Zθ(h)| ≤ e;∀i ≤ C, llvθi' - gθJ ≤ e; ||Vh' - ghIl ≤ E
While Theorem 3 provides computation of the gradients in sub-linear time, it is only usable in a
setting where the weights ({θi}i≤C) are not updated. Indeed, querying nearest neighbors in sub-
linear time assumes that an appropriate data structure (here LSH) was built in advance. However,
when training deep models, we are required to update the weights at every training step. This
necessitates online updating of the LSH structure. To maintain the sub-linear runtime, we perform
these updates in a sparse manner. We describe in Algorithm 1 how this estimator can be used in a
training loop, with weight updating and sparse LSH updates.
Proposition 4. The softmax computations described in Algorithm 1 run in sub-linear time.
4
Under review as a conference paper at ICLR 2018
Algorithm 1 Fast Training of the Softmax layer
Inputs: Dataset D = {(x(i), y(i)}i≤N ⊂ X × Y, k, l, niters number of training iterations.
Initialize θ and φ
Initialize the MIPS structure with {θi}i≤∣v∣.
for j ≤ niters do
Sample an example (x, y) from D.
∆θ - 0
Compute h J f (x; φ)
Find S, k-nearest-neighbors of h using the MIPS.
Define T as l indexes uniformly sampled from Y - S .
Zθ(h) J Pi∈s exp(hTθi) + Y- Pi∈τ exp(hTθi)	. Partition function estimate
Output ` = hTθy - log Zθ (h)
δ% J δ% + h1i=y - (1i∈S + lYl k 1i∈T) heχ^hhθi)
gh J θy - ZIh [Pi∈S θi exp(hTθi) + IYl k Pi∈T θi exp(hTθi)i
Pass down gh for back-propagation.
Re-hash the updated vectors (at most (k + l)) into the right buckets.
end for
Proof. The softmax computations can be split into three parts: retrieving nearest neighbors, com-
puting the forward/backward passes, and rehashing updated vectors. With a sub-linear MIPS such
as LSH, the first part is guaranteed to be sub-linear. For the second part, computing the parti-
tion function and the entire gradient estimator requires computing a finite number of sums over
2
O(k + V) = O(n3) terms, which is sub-linear. The third part consists of re-hashing updated vectors.
Re-hashing a vector is a constant operation (consisting of b × L dot-products) and thus, given that
only a sub-linear number of vectors are updated, re-hashing is sub-linear.	□
4	Inference
In the last section, we presented a method to speed-up training time based on an LSH data structure.
In addition to these training time gains, LSH Softmax can be utilized for computational gains at in-
ference time as well. While MAP inference can be easily derived from the MIPS structure, sampling
from the conditional distribution is often required (e.g. to generate diverse sentences in language
modeling or machine translation). These gains can be crucial for large-scale deployment. This is
a direct application of (Mussmann et al., 2017) that once again leverages a MIPS structure and the
Gumbel distribution. By lazily evaluating Gumbel noise, once can devise an inference scheme which
allows to sample from log-linear models in sub-linear time.
Theorem 5 (LSH Softmax for Inference). We reuse the same notations as the once in Theorem 3.
We define t , - log(- log(1 - l/C)). Let {Gi}i≤k be k samples from the Gumbel distribution.
We then proceed to sample m 〜Binomial(C, l/C), and Sample T, m points from Y-S with
associated Gumbels {G0i}i≤m s.t. each G0i are larger than t. Let us define:
y，argmax{hτθi + Gi, i ∈ S} [{hτθi + Gi, i ∈ T}.
Let , δ > 0, we then have the two following results:
1.	For k = l ≥ ʌ/log 1, y is a Samplefrom p(y∣x; θ, φ) with probability greater than 1 — δ.
2.	This inference scheme runs in sub-linear time.
Proof. (Mussmann et al., 2017)
□
We denote by PGUmbel(∙∣h; θ) the implicit distribution over Y provided by this inference scheme.
While we can sample from pGumbel, we note that the likelihood is intractable. We also emphasize
that this scheme can be utilized for any softmax model, regardless of the training method.
5
Under review as a conference paper at ICLR 2018
5	Efficient Implementation
Recent successes of deep neural networks hinge on their efficient implementation on specialized
hardware: Graphics Processor Units (GPU), which enables training of large models in reasonable
time. Often, methods with theoretically faster runtime are dismissed by practitioners because of
their incompatibility with the hardware, rendering them hard to implement efficiently and ultimately
not widely used. In this section, we first detail how our method is indeed amenable to GPU im-
plementation and can amount to wall-clock gains in practice, and explain why LSH Softmax is
easy to implement in the context of modern deep learning frameworks who often provide a gradient
computation API.
GPU Implementation Standard LSH implementations consist of three steps:
1.	Hashing: Given a query q ∈ Rd, hash q into L tables i.e. computing b × L dot-product
with (random) hyperplanes.
2.	Look-up: Given L signatures in {0, 1}b, retrieve candidates in each of the L tables. Let us
denote Cq the number of candidates retrieved.
3.	Distances: Given those candidates {x1, . . . , xCq } ⊂ Rd, compute the distances
{qT xi}i≤Cq and only return the closest one.
It is also important to note that deep learning models are often trained using minibatch optimization;
let us describe how each of these steps can be computed efficiently and in the minibatch setting.
The first step is amenable to the GPU setting; a batch of queries {qi}i≤m ⊂ Rd can be repre-
sented by Q ∈ Rm×d. Given that the hyperplanes are similarly presented in matrix form i.e.
H ∈ Rd×(b×L), the hashing step is equivalent to sign (Q ∙ H) ∈ {0,1}m×(b×L). This is the type of
operations that GPUs excel at: matrix-multiply followed by element-wise function.
The second step, while not as compatible with GPU, is still massively parallelizable using multi-
threading on CPU. Given the computed signatures, one can run parallelism at the query level (i.e.
each thread retrieves candidates for a given query), rendering that step efficient. It also allows for
more memory-efficient look-up such as (Lv et al., 2007).
The last operation is, once again, very amenable to GPU. It simply consists of a gather (i.e. building
a matrix with the appropriate indexes from the candidates) into a 3-d tensor. Indeed, after the
previous step, the LSH structure returns m lists of s candidates, and the gather step returns the
appropriate vectors from the vocabulary into a 3-d tensor of shape Rm×s×d . As the batched queries
can be also seen as a 3-d tensor Rm×d×1, computing the exact distances then reduces to a batch
matrix-multiply which is a very efficient operation on GPU.
Software Implementation Another crucial point for practitioners is the ability to rely on frame-
works automatically providing gradients, such as (Abadi et al., 2016; Maclaurin et al.), to implement
deep learning models; this abstracts away the need to write down the exact gradients which can be
both cumbersome and error-prone. An additional advantage of our estimator is that it can be effort-
lessly implemented in these frameworks. Indeed, given logits computed over the nearest-neighbors
and the additional uniformly sampled indexes, one can compute the estimate of the partition function
and thus an estimate of the loss. Computing the gradient estimators now reduces to differentiating
this loss, which can be very simply done using the framework’s differentiation API.
6	Experiments
After having presented our new layer LSH Softmax, we now proceed to show its applicability and
efficiency in a real-world setting for deep learning practitioners, specifically towards language mod-
eling. We first show that our method significantly outperforms approximate softmax baselines while
performing within 20% of the performance of the exact softmax. We then provide a computational
comparison. While we evaluate our method on NLP tasks, we want to emphasize that it is directly
applicable to other domains, such as vision. However, public vision benchmark datasets with large
output spaces require significantly more computational resources (e.g. 98 GPU nodes for 8 days for
Flickr100M (Thomee et al., 2015)) which is outside the scope of this paper.
6
Under review as a conference paper at ICLR 2018
6.1	Language Modeling
Language modeling is the task of, given a sequence of words (w1, . . . , wT) in a vocabulary V,
estimating p(w1, . . . , wT) = Qt≤T p(wt|w<t). Substantial work has been done to model these
distributions using non-parametric n-gram counts with additional smoothing techniques, but can
fail to model long histories because of an exponential number of sequences. Recently, parametric
models using RNNs have shown impressive success on these tasks (Mikolov, 2012). In this setting,
large output spaces arise naturally, as the vocabulary size can range from 104 to 106. We first
describe our experimental protocol, and then report perplexity (ppl) of LSH Softmax against a set of
baselines on this task for several datasets.
Datasets We evaluate our method on three standard datasets for Language Modeling with varying
number of characters and vocabulary size:
•	Penn TreeBank (PTB): We follow the pre-processing described by (Mikolov, 2012), which
results in 929k training tokens, 73k validation and 82k test tokens with a 10k vocabulary
size.
•	Text8 is a dataset consisting of the first 100 millions characters of Wikipedia, and has
a vocabulary size of 44k. This dataset has been used recently in the context of language
modeling (Xie et al., 2017). We use the 90M first words for training and split the remaining
between the validation and test set.
•	Wikitext-2. First introduced in Merity et al. (2016), this is a selected corpus of Wikipedia
articles. It has a vocabulary size of 33k and contains 217k tokens. As previously, we split
between a training, validation and testing set.
Baselines We evaluate the performance of models trained with (1) exact softmax i.e. computed
over the entire output space, (2) Biased Importance Sampled softmax (BIS), as presented in (Jean
et al., 2014), which consists of sub-sampling the vocabulary according to a proposal distribution
based on unigram counts, and (3) Negative Sampling (NS), proposed in Mikolov et al. (2013),
equivalent to (BIS) with a uniform distribution, (4) standard Importance Sampling (Jozefowicz et al.,
2016) and (5) Noise-Contrastive Estimation (NCE; Gutmann & Hyvarinen (2θ12)). These baselines
are what practitioners canonically use to circumvent the bottleneck of large output spaces.
Implementation Details Our architecture is a 2-layer RNN with LSTM cells and 650 hidden
units. Weights are initialized uniformly within [-0.1, 0.1]. Our models are trained using SGD
using gradient clipping, with an initial learning rate of 20. This learning rate is annealed when the
validation perplexity plateaus. Our models are trained for 40 epochs for PTB, 3 epochs for Text8, 25
epochs for Wikitext-2. With the notations of Theorem 3, for LSH Softmax, We choose k = 10∖∕∖V∖
and l = ∖∕∖V∖. For the IS and NS baselines, we choose to sample k + l classes from the output
space for a fair comparison. We choose the number of bits per signature b , log2 ∖V∖ and choose L,
number of tables, to have sufficient recall for the MIPS task.
6.1.1 Learning
We report perplexity for a fixed architecture but comparing different softmax evaluations; we present
both learning curves and perplexity on each set. We report the perplexity of all trained models using
the exact probabilities i.e. the full softmax. Perplexities are reported in Table 1 and learning curves
in Figure 1. We see that LSH Softmax consistently outperforms the approximate baselines by a fair
margin while performing a similar number of operations, showcasing the strength of this estimator.
We also observe from the training curves that approximate methods’ performances tend to plateau,
as IS and NS cannot target the proper classes to push down. In constrast, LSH Softmax does not.
6.2 Computational Comparison
Having established that the proposed estimator performs very well on real-world tasks, we now
proceed to evaluate the computation gains. It is important to note that for models with large output
spaces, the softmax computation can amount to about 80% of the total computation (Joulin et al.,
7
Under review as a conference paper at ICLR 2018
Method	PTB		Wikitext-2					Text8	
	Train	Val	Test	Train	Val	Test	Train	Val	Test
Exact	29.67	83.52	79.80	38.05	101.88	95.06	164.68	151.92	189.67
BIS	48.76	133.26	135.51	65.57	214.9	205.65	—	—	—
NS	32.12	103.26	101.48	42.66	142.82	136.30	255.62	234.02	281.11
IS	—	—	114.33	—	—	128.38	—	—	205.94
NCE	—	—	115.30	—	—	122.04	—	—	386.87
Ours	25.68	97.45	92.91	63.60	124.51	115.11	206.20	178.86	224.42
Table 1: LSH Softmax performs closest to the exact softmax and handily outperforms importance
sampling based methods with no concentration guarantees.
Figure 1: LSH Softmax converges faster than compared baselines on all three datasets. IS is not
reported for Text8 as the results were order of magnitude worse than compared method.
2016; Ji et al., 2015); we thus choose to only evaluate computational gains in the softmax layer.
We evaluate our method in CPU, with a batch size of 1, to have an accurate estimation of the
ratio of FLOPS. We report both speed-up and validation perplexity (ppl) relative difference with the
exact softmax for LSH Softmax and NS. Note that NS requires the same number of operations as
importance sampling (IS) but outperforms it in all tasks. Additionally, we show the speed-ups one
can achieve on the One Billion Word dataset (Chelba et al., 2013), whose ppl was not evaluated
due to computational constraints. We report the results in Table 2. We observe that, while faster,
NS performs significantly worse than LSH Softmax. Furthermore, its performance deteriorates
significantly when increasing the size of the output space, contrary to LSH Softmax which always
performs in the same relative range.
Method	PTB		Wikitext-2		Text8		Billion Word
	Speed-up	∆ ppl	Speed-up	∆ PPl	SPeed-uP	∆ PPl	SPeed-uP
NS	2.8×	23.6%	3.7×	40.2%	3.1×	54.0%	5.7×
Ours	1.6×	16.7%	2.4×	22.2%	2.3×	17.8%	4.1×
Table 2: LSH Softmax performs closest to the exact softmax and handily outperforms importance
sampling based methods with no concentration guarantees.
7	Related Work
In recent years, MIPS-based estimators for log-linear models have been explored in the literature.
Vijayanarasimhan et al. (2014) propose retrieving the largest logits using LSH and estimating the
Softmax using only those classes. Their method is encompassed in ours by simply setting l to
0. However, we note that not accounting for the tail can lead to highly biased gradients. Indeed,
Mussmann et al. (2017) show that, using only the top-k largest values leads to significantly worse
8
Under review as a conference paper at ICLR 2018
performance. In a similar direction, Spring & Shrivastava (2017b) propose using LSH at each layer
and only retaining the largest activations which can be viewed as a form of adaptive dropout. This
work differs with ours in two ways: first of all, their paper provides no theoretical guarantees and
secondly, they focus on reducing memory footprint which is not the aim of our work. Finally, Spring
& Shrivastava (2017a) proposed using the LSH structure as a proposal distribution to evaluate the
Softmax. While unbiased and efficient, their method does not offer any concentration guarantees
and the estimator can have arbitrarily bad variance.
8	Conclusion
In this work, we presented LSH Softmax, a softmax approximation layer for large output spaces with
sub-linear learning and inference cost (in the number of states) and strong theoretical guarantees. We
showcased both its applicability and efficiency by evaluating LSH on a common NLP task, language
modeling. On several datasets for this task, we report perplexity closest to exact training among all
baselines, as well as significant speed-ups. Our hope is that, for any architecture, this layer could be
chosen in lieu of softmax, when the output space is sufficiently large to warrant the approximation.
To that end, we plan to release source-code with the camera-ready version.
References
Martin Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado,
Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine learning on heteroge-
neous distributed systems. arXiv preprint arXiv:1603.04467, 2016.
Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul Natsev, George Toderici, Balakrishnan Varadarajan,
and Sudheendra Vijayanarasimhan. Youtube-8m: A large-scale video classification benchmark. arXiv
preprint arXiv:1609.08675, 2016.
Alexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya Razenshteyn, and Ludwig Schmidt. Practical and optimal
lsh for angular distance. In Advances in Neural Information Processing Systems, pp. 1225-1233, 2015.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to
align and translate. arXiv preprint arXiv:1409.0473, 2014.
Moses S Charikar. Similarity estimation techniques from rounding algorithms. In Proceedings of the thiry-
fourth annual ACM symposium on Theory of computing, pp. 380-388. ACM, 2002.
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robin-
son. One billion word benchmark for measuring progress in statistical language modeling. arXiv preprint
arXiv:1312.3005, 2013.
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent neural
networks. In Acoustics, speech and signal processing (icassp), 2013 ieee international conference on, pp.
6645-6649. IEEE, 2013.
Emil Julius Gumbel and Julius Lieblein. Statistical theory of extreme values and some practical applications:
a series of lectures. 1954.
Michael U Gutmann and Aapo Hyvarinen. Noise-contrastive estimation of unnormalized statistical models,
with applications to natural image statistics. Journal of Machine Learning Research, 13(Feb):307-361,
2012.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Computer Vision and Pattern Recognition (CVPR), 2016 IEEE Conference on, 2016.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural ComPutation, 9(8):1735-1780,
1997.
Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: towards removing the curse of dimension-
ality. In Proceedings of the thirtieth annual ACM symPosium on Theory of comPuting, pp. 604-613. ACM,
1998.
Sebastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. On using very large target vocabulary
for neural machine translation. arXiv PrePrint arXiv:1412.2007, 2014.
9
Under review as a conference paper at ICLR 2018
Shihao Ji, SVN Vishwanathan, Nadathur Satish, Michael J Anderson, and Pradeep Dubey. Blackout: Speeding
up recurrent neural network language models with very large vocabularies. arXiv preprint arXiv:1511.06909,
2015.
Armand Joulin, Laurens van der Maaten, Allan Jabri, and Nicolas Vasilache. Learning visual features from
large weakly supervised data. In European Conference on Computer Vision, pp. 67-84. Springer, 2016.
Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of
language modeling. arXiv preprint arXiv:1602.02410, 2016.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural
networks. In Advances in neural information processing systems, pp. 1097-1105, 2012.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Qin Lv, William Josephson, Zhe Wang, Moses Charikar, and Kai Li. Multi-probe lsh: efficient indexing for
high-dimensional similarity search. In Proceedings of the 33rd international conference on Very large data
bases, pp. 950-961. VLDB Endowment, 2007.
Dougal Maclaurin, David Duvenaud, Matthew Johnson, and Ryan P Adams. Autograd: Reverse-mode differ-
entiation of native python, 2015. URL http://github. com/HIPS/autograd.
Chris J Maddison, Daniel Tarlow, and Tom Minka. A* sampling. In Advances in Neural Information Processing
Systems, pp. 3086-3094, 2014.
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv
preprint arXiv:1609.07843, 2016.
Tomas Mikolov. Statistical language models based on neural networks. Presentation at Google, Mountain
View, 2nd April, 2012.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of
words and phrases and their compositionality. In Advances in neural information processing systems, pp.
3111-3119, 2013.
Frederic Morin and Yoshua Bengio. Hierarchical probabilistic neural network language model. In Aistats,
volume 5, pp. 246-252, 2005.
Stephen Mussmann and Stefano Ermon. Learning and inference via maximum inner product search. In Pro-
ceedings of The 33rd International Conference on Machine Learning, pp. 2587-2596, 2016.
Stephen Mussmann, Daniel Levy, and Stefano Ermon. Fast amortized inference and learning in log-linear
models with randomly perturbed nearest neighbor search. arXiv preprint arXiv:1707.03372, 2017.
Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalch-
brenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. arXiv
preprint arXiv:1609.03499, 2016.
David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. Learning representations by back-propagating
errors. Cognitive modeling, 5(3):1, 1988.
Ryan Spring and Anshumali Shrivastava. A new unbiased and efficient class of lsh-based samplers and estima-
tors for partition function computation in log-linear models. arXiv preprint arXiv:1703.05160, 2017a.
Ryan Spring and Anshumali Shrivastava. Scalable and sustainable deep learning via randomized hashing.
In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, pp. 445-454. ACM, 2017b.
Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian
Borth, and Li-Jia Li. The new data and new challenges in multimedia research. arXiv preprint
arXiv:1503.01817, 1(8), 2015.
Sudheendra Vijayanarasimhan, Jonathon Shlens, Rajat Monga, and Jay Yagnik. Deep networks with large
output spaces. arXiv preprint arXiv:1412.7479, 2014.
Ziang Xie, Sida I Wang, Jiwei Li, Daniel Levy, Aiming Nie, Dan Jurafsky, and Andrew Y Ng. Data noising as
smoothing in neural network language models. arXiv preprint arXiv:1703.02573, 2017.
10