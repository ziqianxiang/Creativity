Under review as a conference paper at ICLR 2018
On Batch Adaptive Training for Deep Learn-
ing: Lower Loss and Larger Step Size
Anonymous authors
Paper under double-blind review
Ab stract
Mini-batch gradient descent and its variants are commonly used in deep learning.
The principle of mini-batch gradient descent is to use noisy gradient calculated
on a batch to estimate the real gradient, thus balancing the computation cost per
iteration and the uncertainty of noisy gradient. However, its batch size is a fixed
hyper parameter requiring manual setting before training the neural network. Yin
et al. (2017) proposed a batch adaptive stochastic gradient descent (BA-SGD)
that can dynamically choose a proper batch size as learning proceeds. We extend
the BA-SGD to momentum algorithm, and evaluate both the BA-SGD and the
batch adaptive momentum (BA-Momentum) on two deep learning tasks from nat-
ural language processing to image classification. Experiments confirm that batch
adaptive methods can achieve a lower loss compared with mini-batch methods and
methods that manually increase the batch size, after scanning the same epochs of
data. Furthermore, our BA-Momentum is more robust against larger step sizes,
in that it can dynamically enlarge the batch size to reduce the larger uncertainty
brought by larger step sizes. We also identified an interesting phenomenon, batch
size boom. The code implementing the batch adaptive framework is now open
source, applicable to any gradient-based optimization problems.
1 Introduction
Efficiency of training large neural networks becomes increasingly important as deep neural networks
tend to have more parameters and require more training data to achieve the state-of-the-art perfor-
mance on a wide variety of tasks (Goodfellow et al., 2015). For training deep neural networks,
stochastic gradient descent (SGD) (Robbins & Monro, 1951) and its variants, including momen-
tum, which utilizes past updates with an exponential decay (Qian, 1999), and other methods that
can adapt different learning rates for each dimension, such as ADAGRAD (Duchi et al., 2010),
ADADELTA (Zeiler, 2012) and ADAM (Kingma & Ba, 2014), are commonly used.
SGD approximates the gradient by only using a single data instance in each iteration, which may lead
to uncertainty of approximation. This uncertainty can be reduced by adopting a batch of instances
to do the approximation. In mini-batch SGD, the batch size is a fixed hyper parameter requiring
manual setting before training the neural network. Setting the batch size typically involves a tuning
procedure in which the best batch size is chosen by a series of attempts.
Yin et al. (2017) has developed a batch adaptive stochastic gradient descent (BA-SGD) that can
dynamically choose a proper batch size as learning proceeds. BA-SGD models the decrease of
objective value as a Gaussian random walk game with rebound on the basis of Taylor extension and
central limit theorem. Its core idea is to only update the parameters when the ratio between the
expected decrease of objective value and the current batch size is large enough, otherwise enlarge
the batch size to better approximate the gradient. It claimed that by smartly choosing the batch
size, the BA-SGD not only conserves the fast convergence of SGD algorithm but also avoids too
frequent model updates, and compared with mini-batch SGD, its objective value decreases more,
after scanning the same amount of data.
However, the experiment in Yin et al. (2017) was only conducted on some simple classification
tasks using fully connected neural network with one input layer, one output layer and two hidden
layers. What about the evaluation on some complex neural networks, such as convolutional neural
network (CNN) and recurrent neural network (RNN)? How well would the batch adaptive algorithm
1
Under review as a conference paper at ICLR 2018
perform on other complicated tasks related to natural language processing and computer vision?
Furthermore, empirical studies reveal that SGD usually performs not so well in some deep and
complex neural networks (Sutskever et al., 2013). Can this batch adaptive framework be extended
to other gradient based optimization algorithms except SGD?
Therefore, in this paper we extend the batch adaptive framework to momentum algorithm, and eval-
uate both the batch adaptive SGD (BA-SGD) and the batch adaptive momentum (BA-Momentum)
on two deep learning tasks from natural language processing to image classification. These two
tasks use RNN and CNN respectively, which cover most of the deep learning models.
In our experiments, we have the following observations. First, for batch adaptive methods, their
loss functions converge to lower values after scanning same epoches of data, compared with fixed-
batch-size methods. Second, BA-Momentum is more robust against large step sizes by dynamically
enlarging the batch size to counteract with the larger noise brought by larger step sizes. Third,
we observed a batch size boom, a concentrated period where the batch size frequently increases to
larger values, in the training of BA-Momentum. The batch size boom is of significance in that it
always appears at the point where mini-batch method starts to reach its lowest possible loss and it
helps BA-Momentum keep dropping to even lower loss. More details on these observations and
their analysis can be found in Section 4. The code implementing the batch adaptive framework
using Theano (AlR) is now open source 1, which is applicable to any gradient-based optimization
problems.
This paper is organized as follows. In Section 2, we briefly introduce the batch adaptive frame-
work proposed by Yin et al. (2017). In Section 3, we extend the batch adaptive framework to
momentum algorithm. In Section 4, we demonstrate the performance of BA-M and BA-SGD on
Fashion-MNIST (Xiao et al., 2017) and relation extraction task, and then reveal the robustness of
BA-Momentum against large step sizes. In Section 5, we discuss some efficiency issue concerned
with implementing this batch adaptive framework, and also propose several promising applications
based on this framework.
2 Prerequisites
In this section we briefly summarize the batch adaptive stochastic gradient descent proposed by Yin
et al. (2017).
2.1	Notations
We use X and Y to respectively denote the training data set and its random subset. The vector of
model parameters is denoted by θ and is subscripted by t to denote an iteration. F is used to denote
objective function, while fθ~ is the partial derivative of function F with model parameters θ (i.e. the
gradient, computed over the whole data set).
2.2	Batch Adaptive Framework
T , 3	1	.	..	1	.	..	^	「	，	，1	♦	「	「 一 ，
Let ξj denote the difference between the approximate gradient computed on an individual instance
j
gj~ and the real gradient fθ~, thus, fθ~, the approximate gradient computed on a batch Y can be written
as:
j~
^	_ Eyj ∈Y g _ ʃ , Eyj ∈Y ξj
~ = 一|Y| — = ~ + 一|Y|一
(1)
Py ∈Y ξ~j
The term ——jγ∣——can be viewed as a random variable determined by the randomly sampled batch
Y . On the basis of Central Limit Theorem (CLT), it should satisfy a multi-dimension normal distri-
bution N(0, ∑), where Σ is the covariance matrix of ~j. f~ is the real gradient, computed on the
whole data set and treated as a constant given the parameters θ. Thus We have f~ 〜N(f~,奇).
1The code implementing the batch adaptive framework using Theano can be found in our GitHub repository:
https://github.com/thomasyao3096/Batch_Adaptive_Framework
2
Under review as a conference paper at ICLR 2018
After modelling the estimation of gradient on a batch as a normally distributed random variable,
Yin et al. (2017) uses first-order Taylor extension to approximate the objective function F(θ~) at any
parameter configuration. The equation is shown below.
F(~)= F(~o) + fT0∙(~-~o)+ h~o (~)
(2)
where the function hθ~ (θ~) is the remainder term satisfying limθ~→θ~ hθ~ (θ~) = 0.
If SGD optimization algorithm is adopted to update the parameters, with Equation (1) and Equation
(2), the decrease of objective value can be expressed as:
~
△F(~o) =	F(~o- ηf~O)-F(~O)	≈ -ηfT0	∙	(f~0	+	yjYY	') = -η∙	f~0f~o	- η ∙	fT0	∙	~y	⑶
|Y|
where η is the learning rate, and the noise term satisfies εγ 〜N(0,育)，then εT ∙ f~θ can be
viewed as a weighted sum of each dimension of vector ~εY, thus satisfying a one-dimension Gaussian
F	fT Nf	T
distribution, i.e. εT ∙ f~ 〜N(0, 0 γ-0). Then ∆F(~o) also satisfies a Gaussian distribution,
9 f~ ∙∑ ∙f ~	ʌ
,η2 0 ∣γ∣~~0). In practice, they use the approximate gradient f~θ to
calculate the mean and variance of ∆F(θ~0)
For the covariance matrix of ~εY,
Σ
Σ, its unbiased estimation Σ is:
一Pyj ∈Y (g~o- f~o)(g~0- f~o )T
|Y|-1
(4)
After modelling the decrease of objective value as a normally distributed random variable, Yin et al.
(2017) abstract the process of objective value change as a random walk game with a Gaussian dice.
In the game, the objective value is regarded as a state. For the simplicity of notation, we define a
state st as below.
st
∆F
η
(5)
—
The decrease of objective value, namely the transfer from the current state to the next state is deter-
mined by a Gaussian dice, of which the mean solely depend on the current state and the variance is
controlled by the state and the batch to be chosen. They define the domain of game state as a half
closed set of real numbers [S*, +∞), where S * is the minimum objective value that the learning can
possibly achieve. The game starts with a random state and the goal is to move as close as possible
to S* . There are two ways of state transfer: one directly decreasing from Si to Sj , another first
reaching minimum point S* and then rebounding to Sj .
Formally, for state st, the moving step 4st is generated by a Gaussian dice N(μt, σm), where
μt =耳f~t ,σ2 = fT ς %,m = |Y|, denoting the batch size. The state transition equation is
presented below.
st+1 = |st - S* - η4st∣ + S* = SStq- η4st,	ifSt - η4st ≥ S*
2S* + η4st - st, otherwise
(6)
LetPm(4st) denote the probability density function for a random moving step 4st 〜N (μt,
The expected value of next state can be expressed as follows.
Z+∞
Pm(4st)(∣st - S* - η ∙ 4st| + S*)d4st
∞
(St - S* - ημt) {φ(a) - φ(-a)} + ησ^∖ ee
mπ

a2	c*
丁 + S*
(7)
st - S* - ημt /—
where a =-------------√m
ησt
3
Under review as a conference paper at ICLR 2018
To decide the best batch size, one should consider both the variance of estimated gradients and
computation cost. Large batch can reduce the variance and therefore make more accurate updates,
whereas it also requires more computations. Yin et al. (2017) then define a utility function to find
a balance. Maximizing the utility means achieving the largest expected decrease of loss per data
instance.
u(m,st)= St- Em(st"	(8)
m
m* — arg max u(m,st)	(9)
m
where m* is the best batch size for the (t + 1)-th iteration.
For more specifications on the BA-SGD algorithm, see Yin et al. (2017).
3	Batch Adaptive Momentum
The main appeal of momentum is its ability to reduce oscillations and accelerate convergence (Goh,
2017). The idea behind momentum is that it accelerates learning along dimensions where gradient
continues pointing to the same direction, and slows down those where sign of gradient constantly
changes (Zeiler, 2012). Recent work on momentum, called YellowFin, an automatic tuner for both
momentum and learning rate, helps momentum optimizer converge in even fewer iterations than
ADAM on large ResNet and LSTM models (Zhang et al., 2017). Since it is both powerful and
popular, we would like to also apply the batch adaptive framework to momentum optimizer.
3.1	Momentum
Momentum utilizes past parameter updates with an exponential decay. Its way of update is given by
θ~t+1 = θ~t - mt	(10)
mt = ρmt-1 + ηfθ~t	(11)
where mt denotes momentum at the t-th iteration and ρ is the rate controlling the decay of the
previous parameter updates. Equation 11 can also be written in the following form.
t
mt = η X ρt-τ f~τ	(12)
τ=0
3.2	Derivation
Referring to Equation 3, we can analogically estimate the decrease of objective value in the following
expression.
∆Ft = F(θ~t+1) - F(θ~t) = fθT~ (θ~t+1 - θ~t) = -fθ~T mt
tt
= FTtX Pt-τ fθτ = -吗X Pt-τ f~τ+ εT)
τ=0	τ=0
~
where εT = yjγγ∣τ ' is the noise term which represents the difference between the real gradi-
ent and the estimated gradient calculated on a batch Yτ chosen at the τ -th iteration. Though the
estimated gradient from the previous iterations (i.e. τ = 0...t - 1) has noise, their batches which
respectively determine their noise have already been selected, thus their noise is no longer a ran-
dom variable but a constant. However, for the t-th iteration, we have not decided which batch to be
sampled, therefore ~εt is indeed a random variable, and on the basis of CLT we know it is normally
distributed, i.e.耳 〜N(0,岛).Based on this and the fact that real gradients, f~ (T = 0…t), are
all constants for the t-th iteration, we then have the decrease of objective value, ∆Ft, satisfying a
one-dimensional Gaussian distribution, which is also experimentally verified in Appendix B.
4
Under review as a conference paper at ICLR 2018
TTT	1	.	1	1	.	. 1	1	♦	∕' Λ	t	1'	.	.	1 -I -I . 1	1'
We need to calculate the mean and variance of ∆Ft , but we prefer not to record all the fθ~ from
previous iterations. Therefore, we construct a recurrence formula to avoid the trouble.
Let Pt = Ptτ=0 ρt-τ (fθ~ + ~ετ), then we have the following recurrence formula.
Pt = (fθ~t + ε~t) + ρPt-1	(14)
Thus the mean and variance of Pt can be calculated in the following forms.
μpt = ρμpt-ι + fθt	(15)
ς _ Σ t
ςPt = IYtI
(16)
Now We have ∆Ft 〜N(-ηfT μpt ,η2fT Σpt£~). In the Gaussian walk game with rebound illus-
trated in Section 2, the Gaussian dice here satisfies 4st 〜N (fT μpt, fT ςPt fθt).
For simplicity, we let σ2 = fT Σtf~, μt = fTμpt, m = |Yt|, thus 4st 〜N(μt,今).Then the
expected value of the next state shares the same expression with Equation 7, though the mean of
4st is different.
For batch adaptive momentum algorithm, we also adopt the utility function in Equation 8, and the
best batch size is the one that maximizes the utility function, calculated through Equation 9.
3.3	Algorithm
At the end of this section, we would like to summarize how the batch adaptive momentum algorithm
is implemented by presenting the pseudo code below. In the pseudo code, the M stands for the
total budget, i.e. the total number of instances used for training, the m0 means sampling step, the
smallest batch increment. The rest symbols have been introduced before. In this pseudo code, we
aim to calculate the optimal batch size for each update step. When an optimal size is determined and
it is larger than the current batch size, we need to add more instances to enlarge the batch. However,
st, μt, σt will change every time we add more instances, leading to a different optimal size. Thus
Algorithm 1 Batch-Adaptive Momentum
1 2 3 4 5 6 7 8	: procedure BA-MOMENTUM(X , θ, η, ρ, M, m0) :	while M > 0 do Yt J0 :	repeat :	random sample Z from X - Yt with IZ I = m0 Yt JYtS Z 八 calculate f ~, Σt with Yt	. Equation (1) and (4) calculate μpt with μp- and f ~	. Equation (15)
9 10 11 12 13 14 15 16 17 18	st J F(~IYt),μt J fTμPt ,σt J JfTςtf~ m* J arg maxm u(m, St) where S * is user-specified until |Yt| ≥ min{m*, ∣X∣} :	mt = ρmt-1 + ηfθ~t :	θ~ J θ~ - mt :	M J M - IYt I :	t=t+1 :	end while :	return θ : end procedure
in practice, we can only gradually increase the batch size until it becomes larger than or equal to a
running estimate of the optimal batch size. Lastly, when implementing the algorithm, one may note
that it is time-consuming to calculate the covariance matrix, Σt . We will discuss a tradeoff and the
computation cost of this algorithm in Appendix C.
5
Under review as a conference paper at ICLR 2018
4	Experiment
In this section, we present the learning efficiency of different learning algorithms on two deep
learning models. One is the CNN for image classification on the dataset of Fashion MNIST (X-
iao et al., 2017), and the other is a more complex RNN model for relation extraction on financial
documents (Miwa & Bansal, 2016). To evaluate our learning efficiency regardless of the size of the
data set, we use epoch as the unit of scanned data and one epoch means all instances in the whole
training set. We calculate the training loss on the whole training set each time when the model has
scanned one more epoch of data.
4.1	Model and Dataset
Fashion MNIST is a data set of Zalando’s article images, which consists of 60000 training samples
and 10000 test samples. Each sample is a 28 × 28 grayscale image, associated with a label from 10
classes. We design a CNN model with 3 convolutional layers for this experiment.
Another model we use for relation extraction task is a bi-directional LSTM RNN (Gers, 2001). To
train this model, we use 3855 training instances. A detailed description on the specific task and
architecture of networks for the two experiments can be found in Appendix A.
4.2	Lower Loss with Budget Limit
In both experiments, 12 different optimization algorithms are used. They are BA-Momentum, BA-
SGD, mini-batch momentum and mini-batch SGD with a fixed size of 32, 64, 128, 256 respec-
tively, and manually adjusted mini-batch momentum and SGD (denoted as Manual-Momentum and
Manual-SGD). Smith et al. (2017) proposed this manually adjusted mini-batch method that increas-
es the batch size after every manually set number of epochs to reduce the noise scale of estimated
gradients during training. Here we let “Manual” methods start with batch sizes of 32 and double
their sizes every 25 epochs for image classification task (100 epochs as total budget) and every 75
epochs for relation extraction task (300 epochs as total budget), eventually their sizes will reach 256.
For simplicity, we denote mini-batch momentum with a fixed size of 32 as “Mini-32-Momentum”.
The same rule applies to “Mini-256-SGD” etc. We choose “Manual” methods and mini-batch meth-
ods with different sizes ranging from 32 to 256 as baselines. This is because with a budget limit,
the small batch method can have more but less accurate updates, while the large batch method can
make fewer but more accurate updates. More updates and accurate updates both can to some extent
help the model achieve a lower loss, thus we use these small and large batch methods, together with
“Manual” methods as baselines to make the comparative test with “BA” methods more convincing.
For “BA” methods, the smallest batch increment, m0 , is 32.
The result of image classification task on Fashion MNIST is plotted in Figure 1 and Figure 3a. The
observation shows that for six different momentum-based optimizations, BA-Momentum achieves
a lower loss than the rest five methods after 100 epochs of training. Furthermore, BA-Momentum
has fewer fluctuations in later training stage than most of the methods with fixed batch sizes. For the
SGD-based optimizations, BA-SGD achieves a second lowest loss while the lowest one is achieved
by Mini-32-SGD.
Now we take a look at the batch size change per iteration, plotted in Figure 3a. For both BA-
Momentum and BA-SGD, the batch size keeps almost constant at 32 in an early stage, while it more
frequently increases to larger sizes in later training stage. The tendency is especially evident for
BA-SGD. The lines in Figure 3a for BA-SGD gets denser as iterations increase, indicating the batch
size rises more frequently from 32 to larger sizes. Also, the largest possible batch size for BA-SGD
enlarges from 64 to almost 500 as learning proceeds.
We display the result of relation extraction task in Figure 2 and Figure 3b. BA-Momentum still
achieves the lowest loss. For SGD-based methods, BA-SGD has a similar curve with Mini-32-SGD,
while BA-SGD fluctuates much less in later training stage and eventually reach a lower training
loss than Mini-32-SGD does. The batch size change for BA-SGD on relation extraction task have
the similar trend observed in the image classification result but much less frequent, and the batch
size of BA-Momentum stays nearly constant at 32. This might be because each training instance
in the relation extraction task is a sentence containing dozens of possible quadruples for binary
6
Under review as a conference paper at ICLR 2018
classification, and a batch of 32 instances is already a relatively large batch in terms of number of
binary classifications. Furthermore, the test accuracies in Appendix D show that for this task the
larger the batch size is, the worse the model will generalize, thus it is smart for BA-SGD and BA-
Momentum to choose smaller sizes. In addition, one may note that there is a sudden rise of loss
from 0.05 to 0.30 at the 224th epoch in Figure 2b during the training of BA-SGD and the loss soon
drops back after this epoch. We presume it is due to encountering some very inaccurate estimates of
gradients.
From Figure 3, we can see that BA-SGD tends to have much larger batch sizes than BA-Momentum
does on both tasks. This might be because momentum-based method is better at reducing the un-
certainty of updates by taking into account previous updates, thus it does not need that large a batch
which SGD requires, to further reduce the uncertainty.
Above all, for the four figures below, the proposed “BA” methods reach the lowest loss in three cases
and always perform better than the “Manual” methods. As for test accuracies, we present them in
Appendix D.
Here we analyze why “BA” methods can achieve the lowest loss in most cases. The “BA” methods
adopt a smaller batch in an early stage, which allows them to have more iterations per epoch, then
they dynamically enlarge their batch size in later stage to reduce the noise of estimated gradients,
because the noise has a larger impact on the accuracy of parameter update in later training stage.
Therefore “BA” methods can conserve the fast decrease of Mini-32 methods in early stage and
meanwhile keep decreasing rather than severely fluctuate like Mini-32 methods do in later stage.
SSoI ou-u-e」1-
12 3
- - -
Ooo
111
SSoI oe」
0	20	40	60	80	100
Epoch
(b)	SGD-based methods
O 20	40	60	80 IOO
Epoch
(a)	Momentum-based methods
O 1 2
O - - _
1 O O -
1 1 -
SSO- blne」
IO-4
0	50	100	150	200	250	300
Epoch
(a)	Momentum-based methods
Figure 1: Training loss per epoch on Fashion MNIST.
SSo- 6u三一。一1-
0	50	100	150	200	250	300
Epoch
(b)	SGD-based methods
Figure 2: Training loss per epoch on relation extraction.
7
Under review as a conference paper at ICLR 2018
(a) Fashion MNIST
(b) Relation extraction
Figure 3: Batch size per iteration on Fashion MNIST and relation extraction.
4.3	Robustness to Large Step Size
In the fine tuning process of BA-Momentum, we find that BA-Momentum is robust against large
step sizes. Specifically, under a certain range of learning rates, when we tune the learning rate to a
higher value, BA-Momentum can still achieve the same or even lower loss while the performance
of other mini-batch methods will be degraded, converging to higher loss and fluctuating more. For
simplicity, we only choose Mini-32-Momentum as baseline. We choose four different learning rates
to test the robustness of BA-Momentum. They are 0.005, 0.01, 0.02, and 0.05.
The result is displayed in Figure 4. As shown in Figure 4a, when learning rate rises from 0.005 to
0.01 and then 0.02, BA-Momentum ends up with lower loss from 1.13e-4 to 1.69e-5 and then 5.77e-
6. In contrast, Mini-32-Momentum fluctuates more when the learning rate rises from 0.005 to 0.01,
and it ends up with much higher loss when the learning rate increases from 0.01 to 0.02. However,
when the learning rate is tuned to 0.05, both BA-Momentum and Mini-32-Momentum suffer from
a very high loss. These observations confirm that BA-Momentum is more robust against larger step
sizes within a certain reasonable range.
We offer an explanation on the robustness of BA-Momentum. As the density of lines in Figure
4b suggests, the higher the learning rate is given, the larger our batch size tends to become. BA-
Momentum enlarges the batch size to counteract with the larger uncertainty of parameter updates
brought by higher learning rate. In this way, it successfully avoids the fluctuations and meanwhile
benefits from the larger step size to converge faster.
SSo- 6££ro」H
(a) Training loss per epoch
(b) Batch size per iteration
Figure 4: Effect of different learning rates on BA-Momentum.
8
Under review as a conference paper at ICLR 2018
4.4	Batch Size Boom
We find an intriguing phenomenon when conducting experiments testing the robustness of BA-
Momentum. In the process of learning, the batch size of BA-Momentum will experience a batch
size boom, a concentrated period where the batch size frequently increases to larger values. For
example, the batch size boom in Figure 5a is from 80000 iterations to 110000 iterations, while the
batch size boom in Figure 5b is from 60000 iterations to 80000 iterations. In the process of learning,
when a batch size boom appears, the curve of BA-Momentum starts to diverge with the curve of
Mini-32-Momentum. This makes sense because using a larger batch can help BA-Momentum make
more accurate updates and thus decrease to a lower loss. Interestingly, the batch size boom always
appears at the point in which Mini-32-Momentum reaches its lowest possible loss and after which
it fluctuates around that loss. This can be observed in all three plots in Figure 5. As learning rate
increases, Mini-32-Momentum reaches its lowest loss earlier, and the batch size boom also appears
earlier, helping BA-Momentum keep decreasing to lower loss.
(a) Learning rate = 0.005
(b) Learning rate = 0.01
Figure 5: Batch size boom on different learning rates.
(c) Learning rate = 0.02
5	Conclusion and Discussion
In this work we developed BA-Momentum algorithm, an extension of the BA-SGD proposed by
Yin et al. (2017). We also evaluate the two algorithms on natural language processing and image
classification tasks using RNN and CNN respectively. The experiments show that in most cases both
batch adaptive methods can achieve lower loss than mini-batch methods after scanning same epochs
of data. Furthermore, we also confirm that within a certain range of step sizes, BA-Momentum is
more robust against large step size compared with mini-batch methods.
In the experiments, we did not evaluate the decrease of training loss with respect to training time.
This is because, in the BA-SGD and BA-Momentum algorithm, we have to calculate the derivatives
of the loss of each instance from a batch with respect to parameters, and then derive a covariance
matrix through Equation 4 from the derivatives. Computing derivatives by backpropagation is time
consuming, and now we have to compute all the derivatives of every instance in a batch. However,
in mini-batch gradient descent, it is a common practice to calculate an average loss from a batch and
then the derivative of this average loss, which requires less time. A feasible approach to reduce the
computation cost might be to modify the way Theano do the derivation for a batch of instances and
return the square sum of the derivatives, which we plan to study in future work.
The batch adaptive framework can have many important applications. It can be adapted to accelerate
distributed deep learning. For distributed deep learning, communication cost for synchronizing
gradients and parameters among workers and parameter server is its well-known bottleneck (Li et al.,
2014a;b; Wen et al., 2017). A larger batch may help make more accurate updates, thus reducing the
total number of iterations needed to converge, lowering the communication cost. However, a larger
batch also causes a higher computation cost per iteration. In this update-costly environment, the
batch adaptive framework may be modified to take both the computation and communication cost
into consideration when deciding a proper batch size, which is worth further exploring.
Another application is that the batch adaptive framework may help remedy the generalization degra-
dation of using large batch studied by Keskar et al. (2016). They provided solid numeric evidence
9
Under review as a conference paper at ICLR 2018
suggesting that using a larger batch will degrade the quality of the model, as measured by its abil-
ity to generalize. They also studied the cause for this generalization drop and presented evidence
supporting the view that large-batch methods tend to converge to sharp minimizers of the training
and testing functions, which causes this generalization drop. Several strategies to help large-batch
methods eliminate this generalization drop was proposed in their work. The most promising one is
to warm-start with certain epochs of small-batch regime, and then use large batch for the rest of the
training. However, the number of epochs needed to warm start with small batch varies for different
data sets, thus a batch adaptive method that can dynamically change the batch size against the char-
acteristics of data is the key to solving this problem. The batch adaptive framework sheds light on
this issue. Difficulty lies in how to identify a sharp minima accurately and efficiently in the process
of learning and limit the batch size when encountering a sharp minima, which we plan to study in
future work.
References
Theano: A python framework for fast computation of mathematical expressions.
John C. Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning
and stochastic optimization. In COLT, 2010.
Felix Gers. Long short-term memory in recurrent neural networks. Unpublished PhD dissertation,
Ecole PoIytechnique Federale de Lausanne, Lausanne, Switzerland, 2001.
Gabriel Goh. Why momentum really works. Distill, 2017. doi: 10.23915/distill.00006. URL
http://distill.pub/2017/momentum.
Ian J. Goodfellow, Yoshua Bengio, Aaron C. Courville, and Geoffrey E. Hinton. Deep learning.
Scholarpedia, 10:32832, 2015.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. CoRR,
abs/1609.04836, 2016.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, ab-
s/1412.6980, 2014.
Mu Li, David G. Andersen, Alexander J. Smola, and Kai Yu. Communication efficient distributed
machine learning with the parameter server. In NIPS, 2014a.
Mu Li, Tong Zhang, Yuqiang Chen, and Alexander J. Smola. Efficient mini-batch training for
stochastic optimization. In KDD, 2014b.
M. Lichman. UCI machine learning repository. http://archive.ics.uci.edu/ml, 2013.
Makoto Miwa and Mohit Bansal. End-to-end relation extraction using lstms on sequences and tree
structures. CoRR, abs/1601.00770, 2016.
Ning Qian. On the momentum term in gradient descent learning algorithms. Neural networks, 12
(1):145-151,1999.
Herbert Robbins and Sutton Monro. A stochastic approximation method. Ann. Math. Statist., 22(3):
400-407, 09 1951. doi: 10.1214/aoms/1177729586. URL https://doi.org/10.1214/
aoms/1177729586.
Samuel L. Smith, Pieter-Jan Kindermans, and Quoc V. Le. Don’t decay the learning rate, increase the
batch size. CoRR, abs/1711.00489, 2017. URL http://arxiv.org/abs/1711.00489.
Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning
Research, 15:1929-1958, 2014.
Ilya Sutskever, James Martens, George E. Dahl, and Geoffrey E. Hinton. On the importance of
initialization and momentum in deep learning. In ICML, 2013.
10
Under review as a conference paper at ICLR 2018
Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Terngrad:
Ternary gradients to reduce communication in distributed deep learning. CoRR, abs/1705.07878,
2017.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms, 2017.
Peifeng Yin, Ping Luo, and Taiga Nakamura. Small batch or large batch?: Gaussian walk with
rebound can teach. In KDD, 2017.
Matthew D. Zeiler. Adadelta: An adaptive learning rate method. CoRR, abs/1212.5701, 2012.
Jian Zhang, Ioannis Mitliagkas, and Christopher Re. YelloWfin and the art of momentum tuning.
CoRR, abs/1706.03471, 2017. URL http://arxiv.org/abs/1706.03471.
A Architecture of Networks
For the classification task on Fashion MNIST, We use 3 convolutional layers, Where each layer has
32, 64, 128 filters respectively, With MaxPool after each convolutional layer. For all layers, We use
ReLU activations and Dropout (Srivastava et al., 2014).
The relation extraction task aims to extract quadruples in Which the elements are correctly matched
With each other. The input training instance consists of a sequence representing a sentence from
financial documents, and all possible quadruples in this sequence. For the model, a bi-directional
LSTM RNN is used. There are tWo layers of bi-directional LSTM, and a softmax outputs the prob-
ability of each quadruple being correct.
B	Model Verification
In our BA-Momentum method, We model the decrease of objective value as a random variable
satisfying a one-dimensional Gaussian distribution in Equation 13. This derivation is on the basis
of first order Taylor extension and central limit theorem, involving some approximation. After
modelling the decrease of objective value as normally distributed, We then estimated its mean and
variance in Equation 15 and Equation 16. Here We Would like to use real data from Fashion-MNIST
to verify that the decrease of objective value truly satisfies a Gaussian distribution and our estimation
is close to its real mean and variance.
We compute the decrease of objective value on the Whole data set after 10 iterations for 500 times
With batches randomly sampled and size fixed at 100. The result is plotted in Figure 6. The ob-
servation confirms that the decrease of objective value distributes normally. Its mean and standard
deviation are -7.94253e-10 and 3.48414e-11 respectively, While its estimated mean and standard de-
viation are -8.58165e-10 and 4.58062e-12. Therefore, the decrease of objective value in our BA-M
algorithm satisfies a Gaussian distribution and our estimation for its mean and variance is close to
the real ones.
C Implementation Issues
When calculating the covariance matrix in Equation 4, one should note that it takes a space of O(d2)
With d denoting the dimension of the model parameter vector to store the covariance matrix. The
cost is quite high for complex models With a large amount of parameters. Yin et al. (2017) has
proposed a practical tradeoff assuming that model parameters are independent of each other, then
the covariance matrix becomes a diagonal one, greatly reducing the space and time cost. Here We
use a small data set from UCI machine learning repository (Lichman, 2013) to test Whether the
tradeoff Will affect the performance of BA-Momentum.
We use tWo different BA-Momentum. One calculates the exact covariance matrix, While the other
calculates its estimated diagonal matrix. The task is a classification task With 3 classes and 42
features. The training set is composed of around 60000 data instances. After training for 14 epochs,
We yield the folloWing result. The Figure 7a shoW the batch size change as learning proceeds for
11
Under review as a conference paper at ICLR 2018
J 6∙,
50C
3三qeqo」d
0.2
0⅛
-9.0	-8.5
-8.0
-7.5	-7.0	-6.5
Decrease of Objective Value le-1°
Figure 6:	Verification on the distribution of decrease of objective value
matrix method and diagonal method respectively. The Figure 7b demonstrates different training
loss per epoch for two different methods. From the figures we can see that the trend of batch size
change for two methods are almost the same, therefore they perform equally well on the training
loss descent. This indicates estimating the covariance matrix by its diagonal matrix will not cause
degradation to the performance of BA-Momentum.
Diagonal
Iterations
6 × 10~1
5 ×10-1
SSo- ct⊂-⊂-2h
0	2	4	6	8	10	12	14
Epoch
(a)	Batch size per iteration
(b)	Training loss per epoch
Figure 7:	Effects of calculating exact covariance matrix and its estimated diagonal.
As for the cost of computing the optimal batch size, on average it takes up 1.03% and 0.61% of
the total computing time per iteration for BA-Momentum and BA-SGD respectively on the image
classification task, and the percentage on the relation extraction task is 1.31% and 0.92% for BA-
Momentum and BA-SGD respectively. Computing the optimal batch size involves calculating some
means and variances, and a binary search to find the m* that maximizes the utility function. Both
operations take little time.
D	Test Accuracies
We show the test accuracies of different methods on the image classification task and relation ex-
traction task in Table 1. For the relation extraction task, we would like to mention that its accuracy
is an intersection over union, specifically defined as the ratio of the intersection of the predicted
quadruples and the ground truth quadruples to their union. For the evaluation, we record the test
accuracies of each method after every epoch’s training and present below the best test accuracies
achieved by each method during their whole budget’s training (100 epochs for image classification
task and 300 epochs for relation extraction task).
12
Under review as a conference paper at ICLR 2018
The proposed “BA” methods achieve two best test accuracies in the following four cases (SGD-based
on Fashion MNIST and SGD-based on relation extraction). In the other two cases, the test accura-
cies achieved by “BA” methods, 91.33% and 88.46%, are still very close to the best ones, 91.64%
and 89.02% respectively. What’s more, these results are realized in a self-adaptive way and require
no fine tuning, while the best accuracies in the two momentum-based cases are achieved by totally
different fixed batch sizes, indicating a tuning process. In contrast with “BA” methods, “Manual”
methods which manually increase the batch size performs well on Fashion MNIST, whereas they
ends up with the fourth highest test accuracies on the relation extraction task, indicating that “Man-
ual” methods also need fine tuning to realize a satisfactory generalization.
Table 1: Best test accuracies achieved by different methods on Fashion MNIST and relation extrac-
tion
	Fashion MNIST		Relation extraction	
	Momentum	SGD	Momentum	SGD
Mini-32	-90.93%-	90.87%	89.02%-	75.13%-
Mini-64	91.17%	90.64%	87.12%	58.01%
Mini-128	91.64%	90.37%	85.33%	31.21%
Mini-256	90.76%	90.16%	79.37%	28.11%
Manual	91.35%	90.84%	86.31%	50.02%
BA	91.33%	91.01%	88.46%	75.57%
13