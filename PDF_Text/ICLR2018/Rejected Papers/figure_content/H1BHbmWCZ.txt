Figure 1: The diagram of the 3-stage object detector. The input image is passed to the First stagefor feature extraction. The second stage takes the features as input and reconstructs and upscalesthem to the size of the image using Deconvolution layers to form masks. The input image is thenelement-wise multiplied with the masks, and passed on to the Third stage as a new input image. TheThird stage is just a standard CNN with the softmax output. The max value of the softmax functionis used as a ’score’. The closer the score is to 1 the better becuase it reflects a higher confidence inthe soft attention mechanism(a) An example input image featuring a soc- duced by a VGG16 network pre-trained oncer ball	ImageNetFigure 2: The figure shows the original input and the respective activation maps. the masking effectdescribed can be observed, especially in the middle left mapperformance of the model. That is, if for example we retrieve the maps of one of the introductorylayers then we will skip the potential of entirely masking out the object of interest. That is because3Under review as a conference paper at ICLR 2018the introductory layers in CNNs detect features like edges and curves. Similarly, if we choose thedeepest layers’ activation maps, then the maps may be too abstract to mask out desired object.
Figure 2: The figure shows the original input and the respective activation maps. the masking effectdescribed can be observed, especially in the middle left mapperformance of the model. That is, if for example we retrieve the maps of one of the introductorylayers then we will skip the potential of entirely masking out the object of interest. That is because3Under review as a conference paper at ICLR 2018the introductory layers in CNNs detect features like edges and curves. Similarly, if we choose thedeepest layers’ activation maps, then the maps may be too abstract to mask out desired object.
Figure 4: A diagram of the Location Network. Essentially, it is a VGG16 CNN feature extractorfollowed by a few convolutional layers of somewhat large filters. The large filters constitute thebasis of our masking schemeTherefore, we worked on the autoencoder architecture described here. The rationale was simple: wewanted to understand the problem more fundamentally. Was the CNN was suffering from a mappingproblem, i.e. mapping the instances to the correct labels in its high-dimensional space? Or was theinformation of object plurality get lost, if it was picked up at all in the first place?A dataset was compiled of more than 800 images, over 5 classes from ”One” to ”Five”. The imagesfeature the corresponding number of objects, whether homogenous or heterogeneous. The diagramof the autoencoder model is given in Fig. 54	Related WorksOur work criss-crosses many fields though in itself is relatively considered a niche. We will startour discussion by presenting work from the area of Developmental Robotics. First comes our mainsource of inspiration, the work in (Sigaud & Droniou, 2016). It served as the plateau for the develop-ment and synthesis of our research efforts, despite the work itself being without an implementation.
Figure 5: A diagram of the Plurality Autoencoder Network. The encoder is a ResNet50 CNN pre-trained on ImageNet. The last activation layer is taken as the final encoding. The encoded image isthen deconvolved over 6 stages to reconstruct the image. Loss is calculated as a factor of the originalimagethey use the CNN activation maps to estimate object saliency. Furthermore, researchers in Wei et al.
Figure 6: The figure shows the results of the Soft Attention mechanism. The results are of differentconfigurations of Stage Two. It can be seen that correct masking is observable as most of the outeredges are ignored. However, still some of the background is present in the pictureFrom Fig. 6 it can be seen that there is some selection or focus given to the object of most interest inthe input image. We provided different examples to highlight the different configurations and theirresults.
Figure 7: Training behavior graphs of the Object Location net5.3	Plurality Autoencoder NetworkWe ran our model with Binary Crossentropy loss, with the Adadelta optimizer and 80 epochs. Thenetwork is relatively slow to learn and converge, taking more than 50 epochs to start noticing learn-ings transfer to the validation set. Eventually, however, the network saturates at a certain loss. Thetraining behavior is shown in Fig. 8.
Figure 8: Training behavior graphs of the Plurality Autoencoder netDespite the relatively high loss, upon checking the results over sample inputs as shown in Fig. 9,we noticed the network does understand multiplicity. For example we can see that the networkproduces 5 spoon-like shapes for an input image of 5 spoons. What this tells us is that the networkhas encoded information about the number of interesting objects in the image, and was able tosuccessfully recover that information. This is further verified by checking that an image of a singleobject, such as the book shown in the figure, is reconstructed also as a single object.
Figure 9: The figure shows sample inputs to the Plurality Autoencoder net and their respectivereconstructions. We can see that the network creates the appropriate number of ’blobs’ per eachimageWe recognize the limitation of only using images with a single object against a plain background, asthe real-world environment is cluttered. We perceive this can be tackled by creating more specializedCNNs for feature extraction that would be immune to noisy backgrounds.
