Figure 1: Sketch of code to draw an image (left) and the generated image (right). Boxes are drawnaround the many adjustable parameters in the code such as the number of iterations and offsets forthe shapes.
Figure 2: The selected subset of pixel examples (left). The neural network’s estimation of the image(middle). The recovered parameters from running the solver on the selected subset (right).
Figure 3: Our neural network architecture resembles a feed-forward auto-encoder with explicitlyenumerated input and output neuronsThat is to say, the training task of the neural network is to predict the output values for allthe possible input values in dom(x) while given only a subset of input-output values for D’ ={(x(i), F(x(i); s))|x(i) ∈ X0}, the rest (where x0 ∈/ X0) are encoded as unknowns. This is simi-lar to a data completion task in Boltzmann machines (Ackley et al., 1985), with the difference thatwe directly compute the completion process rather than performing a gradient search for the mostprobable completion configuration.
Figure 4: The average time taken for each step of the algorithm (upper left). The spread of total timetaken for each algorithm (upper right). The number of examples used in each algorithm (bottom).
