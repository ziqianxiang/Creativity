Figure 1: Performance results of pre-defined sparsity for (a)-(C) CIFAR10, and (d)-(f) CIFAR100,trained for 30 epochs using different network densities and varying number of CLs. (a),(b),(d),(e)Validation accuracy across epochs. (c),(f) Best validation accuracies after 1, 5 and 30 epochs.
Figure 2: (a)-(b) Performance results of pre-defined sparsity on an MNIST conv network withdifferent densities, each trained for 30 epochs. (c) Performance vs. connection density for differentMNIST CL only networks, each trained for 100 epochs.
Figure 3: (a) Performance vs. connection density for a Morse CL only 2 junction network. (b) and(c) Performance results by varying individual junction densities while overall density is fixed at (b)25% (c) 50%. All cases trained for 30 epochs.
Figure 4: An example of adjacency matrices and equivalent junctions.
Figure 5: (a) Example of 16 2D windows for an MNIST input image. (b) Example of 3D windowswhen the output from a layer also includes features. (c) Construction of window adjacency matrices.
Figure 6: Window adjacency matrices and scatter. Green neurons indicate ideal connectivity. Thehidden layer is split into 2 to show separate constructions of A1w1r and A2w2levery neuron on the opposite side. Note that these matrices can also be constructed for multiplejunctions, i.e. AwXX:Yl and AwXY:Yr , by multiplying matrices for individual junctions. See AppendixSection 5.2 for more discussion.
Figure 7: Network performance vs. scatter for CL only networks of (a) Morse (b) MNIST, andconvolutional network with 2 CL junctions of (c) CIFAR10. All minimum values that need to beconsidered to differentiate between connection patterns are bolded.
