Figure 1: The red line represents the discriminator’s output, the blue and green lines represent thereal and fake distributions, respectively. Top: When the discriminator is near-perfect, it’s unnormal-ized output distribution is peaked. Bottom: Variance in discriminator output within the data and/orgenerated distribution will result in variance in its unnormalized output distribution. Note that thediscriminator’s output need not be in the range (0, 1), we provide just an example here.
Figure 2: Generated CIFAR-10 samples using the standard GAN, GAN with - log D loss, LeastSquares GAN, Wasserstein GAN, and two versions of VRAL (with and without subtracting means)over various training ratios. The top row shows results from training each method at a 1:1 trainingratio. Likewise, the second, third, and fourth shows show results from 10:1, 25:1 and 50:1 train-ing. The standard and least squares GANs are not robust to such large training ratios while otherscontinue to generate sharp images.
Figure 3: Histograms depicting the discriminator’s unnormalized output distribution for the standardGAN, GAN with - log D loss, Least Squares GAN, Wasserstein GAN and our proposed methodwhen trained with various training ratios. In descending order, the results in each row correspond to1:1, 10:1, 25:1 and 50:1 training ratios.
Figure 4: The discriminator’s gradient norms w.r.t. generated samples during training for eachmethod. Note that values are averaged over 50 batches.
Figure 5: Samples and discriminator output histograms from training at various training ratios onCelebA.
Figure 6: Generated images on CelebA by forcing the discriminator to be 1-Lipschitz using a singlemeta-discriminator.
