Figure 1: YellowFin in comparison to Adamon a ResNet (CIFAR100, cf. Section 5).
Figure 2: Momentum operatoron scalar quadratic.
Figure 3: (a) Non-convex toy example; (b) constant convergence rate achieved empirically on theobjective of (a) tuned according to (8); (c,d) LSTM on MNIST: as momentum increases, morevariables (shown in grey) fall in the robust region and follow the robust rate, √μ.
Figure 4: A variation of the LSTM architecture in (Zhu et al.,2016) exhibits exploding gradients. The proposed adaptivegradient clipping threshold (blue) stabilizes the training loss.
Figure 5:	When running YellowFin, total momentum μt equals algorithmic value in synchronoussettings (left); μt is greater than algorithmic value on 16 asynchronous workers (middle). Closed-loopYellowFin automatically lowers algorithmic momentum and brings total momentum to match thetarget value (right). Red dots are measured μt at every step with red line as its running average.
Figure 6:	Training loss and test metrics on word-level language modeling with PTB (left), character-level language modeling with TS (middle) and constituency parsing on WSJ (right). Note thevalidation metrics are monotonic as we report the best values up to each specific number of iterations.
Figure 7: Asynchronous perfor-mance on CIFAR100 ResNet.
Figure 8: Training losses on PTB LSTM (left) and CIFAR10 ResNet (right) for YellowFin with andwithout adaptive clipping.
Figure 9: Training loss for ResNet on 100-layer CIFAR10 ResNet (left) and 164-layer CIFAR100bottleneck ResNet.
Figure 10: Training loss comparison between YellowFin with adaptive momentum and Yel-lowFin with fixed momentum value.
Figure 11: Hand-tuning Adam'smomentum under asynchrony.
Figure 12: Validation perplexity on Tied LSTM and validation accuracy on ResNext. Learningrate fine-tuning using grid-searched factor can further improve the performance of YellowFinin Algorithm 1. YellowFin with learning factor search can outperform hand-tuned Adam onvalidation metrics on both models.
