Figure 1: An illustration of the proposed multi-layer DANTE (best viewed in color). In each trainingphase, the outer pairs of weights (shaded in gold) are treated as a single-layer autoencoder to betrained using single-layer DANTE, followed by the inner single-layer auroencoder (shaded in black).
Figure 2: Plots of training and test errors vs training iterations for single hidden-layer autoencoderwith Sigmoid (left) and Generalized ReLU (right) activations for both DANTE and SGD.
Figure 3: Reconstructions using autoencodermodels with ReLU activation. Top: OriginalImages; Middle:Model trained using DANTE;Bottom: Model trained using Backprop-SGD.
Figure 4: Comparison of DANTE vs Backprop-SGD on other datasets from the UCI repository. Thex-axis on all figures is the number of mini-batch iterations and y-axis denotes test error, which showsthe generalization performance. (Best viewed in color; DANTE = purple, SGD = green)9Under review as a conference paper at ICLR 2018(a) 200 hidden neurons(b) 300 hidden neurons(c) 400 hidden neurons	(d) 600 hidden neuronsFigure 5: Plots of training and test error vs training iterations on a single-layer autoencoder withgeneralized ReLU activation, with varying number of nodes in the hidden layer.
Figure 5: Plots of training and test error vs training iterations on a single-layer autoencoder withgeneralized ReLU activation, with varying number of nodes in the hidden layer.
Figure 6: Plots of training error and test error vs training iterations for multi-layer autoencoders withgeneralized (leaky) ReLU activations for both DANTE and SGD.
