Figure 1: Example NeuralNetwork. We attempt to provethe property that the networkoutput is always greater than -5Table 1: Evolution of the Reluplex algorithm. Red cells cor-responds to value violating Linear constraints, and orange cellscorresponds to value violating ReLU constraints. Resolution ofviolation of linear constraints are prioritised.
Figure 3: Quality of the linear approximation, depending on the size of the domain. We computethe value of the lower bound on a given domain, centred around the global minimum and repeatedlyshrink the size of the domain. Rebuilding completely the linear approximation at each step allowsto create tighter lower-bounds thanks to better la and ua , as opposed to using the same constraintsand only changing the bounds on input variables. This effect is even more significant on deepernetworks.
Figure 4: Impact of the various hyperparameters over the runtimes of the different solvers. Thebase network has 10 inputs and 4 layers of 25 hidden units, and the property to prove is True with amargin of 1000. Each of the subgraph correspond to a variation of one of this parameters. MIPtuneis omitted as the curves was almost identical to MIP.
