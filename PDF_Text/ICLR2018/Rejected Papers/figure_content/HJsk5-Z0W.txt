Figure 1: Network architectures for factorization models. The white clouds g) represent one ormore factorized deep layers, and the dark clouds (∙) softens the dot product constraints.
Figure 2: Comparison of how factorization may use item descriptions features.
Figure 3: Feature extraction network used for labelling tasks. We use f=1000 convolutional filterseach of width 3 (words)Here we describe the details of the feature extraction function φ used in our experiments for labellingin §4.2.1 and §4.2.2. An overview of the network is given in Fig. 3. We choose the most popularwords of each dataset to build a vocabulary of size n, and convert the words of each document to asequence of length t of one-hot encodings of the input words. If the input text is shorter than t, thenwe pad it with zeros; if the text is longer, we truncate it by discarding the trailing words. Therefore,for a vocabulary size n, the input has dimensions t × n, and this matrix is then passed through thefollowing layers:1.	We use an embedding layer to assign a d-dimensional vector to each word in the inputpassage of text. This is done through a d × n-dimensional lookup table, which results in ant × d matrix.
