Figure 1: Our bidirectional network with a style memory in the output layer. Here, x denotesthe input (x ∈ X), while Convi and F Ci denote convolutional layer and fully connected layer i,respectively. Lastly, y denotes output label (y ∈ Y ), and m denotes the style memory (m ∈ M).
Figure 2: The “unrolled” network. Learning consists of training the network as a deep autoencoder,where hi denotes the hidden layer representation of layer i.
Figure 3: Reconstruction of MNIST digits using the network’s predictions and style memories.
Figure 4: Reconstruction of EMNIST letters using the network’s predictions and style memories.
Figure 5: Comparison of MNIST digit reconstruction using the prediction from the network versusground truth label. The top row shows the original images from the MNIST test set that the networkmisclassified. The middle row shows the reconstruction of the images, along with the incorrect classand confidence score. The bottom row shows the reconstructions using the corrected one-hot labels.
Figure 6: Comparison of EMNIST letter reconstruction using the prediction from the network versusground truth label.
Figure 7: Nearest neighbours in image space and style-memory space. (a) and (c) show the 97 digitimages closest to the image in the top-left, as well as their corresponding style-memories. (b) and (d)show the 97 style memories closest to the style memory in the top-left, as well as their correspondingdigit images. The order of elements (across rows, then down) indicate increasing Euclidean distance.
Figure 8: Nearest neighbours in image space and style-memory space of EMNIST dataset.
Figure 9:	Two different styles of digits form the endpoints for the style interpolation experiment.
Figure 10:	Two different styles of letters form the endpoints for the style interpolation experiment.
Figure 11:	Image reconstruction with style memory interpolation between digits and letters shownin Fig. 9 and Fig. 10, where λ was increasing from 0.1 to 1.0 with a step of 0.1 from top to bottom.
