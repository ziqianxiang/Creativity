Figure 1: Training forward models on latent space.(a) Prediction structure of A3C model. (b) Predictthe immediate next latent representation ht+ι by ht and at and directly minimizes ∣∣ ht+ι - ht+ι ∣∣;(C) Predict ht+ι so that ∏t+ι matches with ∏t+ι, the policy distribution from the true latent stateht+ι; (d) Predict ht+ι so that the greedy action from ∏t+ι matches With at+ι, the action predictedfrom policy distribution ∏t+ι; (e) Multiple hop predictions. (f) Only predict long-hop.
Figure 2: Training curves. Each experiment is repeated 3 times. Each iteration contains 250 mini-batches with batchsize 128. The win rate is computed by sampling actions from the current policy.
Figure 3: Statistics of 10K games played by AI trained With different input information. (a) Gamelength statistics for Won/lost games. (b) Value estimation When game progresses. (c) Histogram ofsurprise metric (Eqn. 5). Complete information gives less surprise.
Figure 5: Histogram of averaged number of distinct opponent units seen-and-hidden per decisionpoint from 10K games of PrevSeen (left) and BuildHistory (right). PrevSeen explores more.
Figure 6: Normalized reconstruction accuracy of each channel.
Figure 7: Win rate from the decisions made by latent state h that is predicted by forward models.
Figure 8: Win rate against AI .SIMPLE with frame skip 20 on 1000 games using latent space Monte-Carlo Tree Search. All results uses 100 rollouts (5 threads, each thread with 20 rollouts). The pinkline is the baseline of a random agent that picks the 9 actions uniformly at each step.
