Figure 1: (a) Reservoir architecture with input state at time t denoted by u(t), reservoir state by r(t)# „and output state by s(t). (b) shows one image from the MNIST data split vertically and fed into thereservoir in columns of 1 pixel width, shown to be larger here for ease of visualization.
Figure 2: Pairs of images that are representative of the transformations classified into five labels: (a)very similar, (b) rotated, (c) zoomed, (d) blurred and (e) different.
Figure 3: Fraction correct against (a) training set size (b) reservoir size and (c) spectral radius γ. (a)γ = 0.5, reservoir size=1000 nodes (b) training size=250 pairs, γ = 0.5 and (c) reservoir size = 1000nodes, training size=250 pairs. Training data: digits 0-5, testing data: digits 6-9, sparsity = 0.9.
Figure 4: (a), (b), (c), (d), (e) show the differential reservoir activity of 200 nodes over 28 timestepsfor input transformations very similar, rotated, zoomed, blurred and different respectively. (f) showsthe output weight matrix(W out) for 50 reservoir nodes. (g) shows activity of a random node for alloutput labels over 28 timesteps. Reservoir size: 1000 nodes. γ = 0.5, sparsity= 0.9.
Figure 5: Label probability for images that are (a) both rotated and blurred, (b) rotated only. γ=0.8,reservoir size=1000. Training digits: 0-5, testing digits: 6-9. Fraction correct: (a) 0.986 (b) 0.989.
Figure 6: (a,b) show performance on MNIST for an SNN with 128 nodes in each layer for (a)8layers and a training size of 600 image pairs and (b) for 40 epochs each. (c,d) compare performance(fraction correct) between a RCN and an equivalent SNN for rotated or not rotated MNIST imagesand very similar or different depth maps.
