Figure 1: Illustration on how we modify the deep classification model by introducing Switch layersand Binary classifier. A Switch Gate layer is cascaded between lth layer and its next layer. Each unitin the Switch Gate layer works as an ON/OFF gate, by which the output of each feature map in thelth layer is multiplied with 0/1. At the rightmost, the binary classifier transforms multi-dimensionalprediction scores into a scalar within range of [0, 1].
Figure 2: Value curves of the function of a switch gate in the switch Gate layer (a) and the penaltyfunction for continuous relaxation (b).
Figure 3: Structures of two classifiers used for experiments. (a) Structure of the MNIST classi-fier, which consists of 2 convolutional layers, each followed by 1 max-pooling layer, and 2 fully-connected layers. (b) Structure of the CIFAR10 classifier, which contains 2 convolutional layers,each followed with 1 max-pooling layer and 1 local response normalization layer, and 3 fully-connected layers.
Figure 4: Positions of units selected in max1 and max2 layers of MNIST classifier. For each timeof optimization, the positions of units obtained are marked with light blue color. The position withorange color represents appearing for all five times. The layer max1 has 32 feature maps; resultsof layer max1 are listed in Figure (a). The layer max2 has 64 feature maps; the results are listed inFigure (b).
Figure 5: Visualization of informative features in pixel-space for the MNIST classifier. Informativefeatures for class 0 to 4 (from left to right) are listed in the first row, and those for class 5 to 9 (fromleft to right) are listed in the second row. The highlight points in figures are those most contributedto the prediction of corresponding class.
Figure 6: The prediction holding rate vs number of units. (a): After obtaining the number andpositions of units for class 0 in max1 layer in the MNSIT classifier, we modify the original modelwith all other units in max1 layer masked and get a model with prediction holding rate near 99.5%.
