Figure 1: Cubic Regularization (CR) applied to Stochastic Gradient Descent (SGD); initial learningrate = 0.01, σ = 100(a) Error (SGD in blue, SGD with CR in red) (b) Learning rate (calculated rate in red, period-100moving average in green)Figure 2: Cubic Regularization (CR) applied to Stochastic Gradient Descent (SGD); initial learningrate = 0.02, σ = 100plateaus. We would expect CR to provide the greatest benefit when the optimization gets stuck ona plateau - having information about the objective function curvature would enable the algorithmto increase the learning rate while on the plateau and then return it to a more typical value once itleaves the plateau. To test this, we deliberately initialized our weights so that they lay on a plateau:the objective function is very flat near the origin, and we found that setting the network weights torandom values uniformly sampled between 0.1 and -0.1 was sufficient.
Figure 2: Cubic Regularization (CR) applied to Stochastic Gradient Descent (SGD); initial learningrate = 0.02, σ = 100plateaus. We would expect CR to provide the greatest benefit when the optimization gets stuck ona plateau - having information about the objective function curvature would enable the algorithmto increase the learning rate while on the plateau and then return it to a more typical value once itleaves the plateau. To test this, we deliberately initialized our weights so that they lay on a plateau:the objective function is very flat near the origin, and we found that setting the network weights torandom values uniformly sampled between 0.1 and -0.1 was sufficient.
Figure 3: Cubic Regularization (CR) applied to Stochastic Gradient Descent (SGD) on the CIFAR-10 Dataset; initial learning rate = 0.01, σ = 10006Under review as a conference paper at ICLR 2018Figure 2 shows the results of SGD with and without CR when stuck on a plateau. There, we seea hundred-fold increase in the learning rate while the optimization is on the plateau, but this ratedrops rapidly as the optimization exits the plateau, and once it returns to a more normal descent,the learning rate also returns to an average of about 0.05 as before. The CR calculation enables thetraining process to recognize the flat space and take significantly larger steps as a result. ApplyingCR to SGD when training on CIFAR-10 (Figure 3) produced results similar to those seen on MNIST.
Figure 4: Cubic Regularization (CR) applied to Adagrad; initial learning rate = 0.1, σ = 1000(a) Error (Adadelta in blue, Adadelta with CR in red)(b) Learning rate (calculated rate in red, period-100moving average in green)Figure 5: Cubic Regularization (CR) applied to Adadelta; initial learning rate = 1.0, σ = 10004	DiscussionWe see this CR approach as an addition to, not a replacement for, existing training methods. Itcould potentially replace existing methods, but it does not have to in order to be used. Because ofthe low-rank structure of the Hessian, we can use CR to supplement existing optimizers that do notexplicitly leverage second order information. The CR technique used here is most useful when theoptimization is stuck on a plateau prior to convergence: CR makes it possible to determine whether7Under review as a conference paper at ICLR 2018the optimization has converged (perhaps to a local minimum) or is simply bogged down in a flatregion. It may eventually be possible to calculate a search direction as well as a step length, whichwould likely be a significant advancement, but this would be a completely separate algorithm.
Figure 5: Cubic Regularization (CR) applied to Adadelta; initial learning rate = 1.0, σ = 10004	DiscussionWe see this CR approach as an addition to, not a replacement for, existing training methods. Itcould potentially replace existing methods, but it does not have to in order to be used. Because ofthe low-rank structure of the Hessian, we can use CR to supplement existing optimizers that do notexplicitly leverage second order information. The CR technique used here is most useful when theoptimization is stuck on a plateau prior to convergence: CR makes it possible to determine whether7Under review as a conference paper at ICLR 2018the optimization has converged (perhaps to a local minimum) or is simply bogged down in a flatregion. It may eventually be possible to calculate a search direction as well as a step length, whichwould likely be a significant advancement, but this would be a completely separate algorithm.
