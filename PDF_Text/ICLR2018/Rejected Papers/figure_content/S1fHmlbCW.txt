Figure 1: Decomposition and recomposition principle for depth-wise separable convolutions. Wedenote the Moore-Penrose inverse of the full column rank matrix A by A+ .
Figure 2: For a ratio of compression of 2, the learned wavelet basis auto-encoder outperforms anLSTM-based auto-encoder with the same number of parameters.
Figure 3:	Top: An LSTM model trained on a wavelet-transformed YouTube-8M data-set achievescomparable results against the baseline while using half the number of parameters. Bottom: Tableevaluating the wavelet-transformed LSTM model against the results from Abu-El-Haija et al. (2016)on held-out data.
Figure 4:	Predicting trade volume of a 15 minute window given the previous 15 minutes of observa-tions. We evaluate the performance of neural networks against a simple averaging model. Even withL1 regularization, the LSTM does not have the same predictive power as the neural network specifiedin Theorem 2.2 with L = 2 or L = 4.
Figure 5: Baseline architecture from (Abu-El-Haija et al., 2016).
Figure 6: The architecture we propose for the Youtube video classification task that leverages amulti-resolution approximation computed by a wavelet convolution stack.
