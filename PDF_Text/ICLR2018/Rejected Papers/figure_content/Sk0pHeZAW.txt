Figure 1: With cumulative £ 1 regularization, we compare the convergence rates of SGD andSVRG. SVRG-Cumulative-£ 1 has faster convergence rate in Figure 1(a). However, in Figure1(b), SGD-Cumulative-£ 1 can significantly converge into lower loss than SVRG-Cumulative-£ 1when compression rate equal 50% and 90%.
Figure 2:	Four 01 regularization compression methods experiment on two deep learningmodels, including LeNet-300-100 and LeNet-5 using MNIST datasets and CIFAR-10.
Figure 3:	The test error with compression rate under different compression methods. Dis Delicate-SVRG-Cumulative-£ 1, Q is weight quantization. Combining Delicate-SVRG-CumulatiVe-£ 1 with weight quantization can achieve the best performance.
Figure 4: Estimate the convergence rate when using four compression methods, includ-ing our method Delicate-SVRG-Cumulative-01, Delicate-SVRG-Cumulative-01 (without Bi-asPruning) that without bias-based pruning in 01 regularization, SVRG-Cumulative-01 andSGD-Cumulative-01, on LeNet-300-100 and LeNet-5 models with MNIST and CIFAR-10datasets. Here we choose the compression rate that equal 90% to observe training andtest loss. For MNIST dataset, we did not notice subtle difference train and test loss onLeNet-300-100 mo del generated by four methods.
Figure 5: Using three types of initial weights, we compare our method with other threemethods. D-SVRG-C-L1 and D-SVRG-C-L1(wo BiasPruning) are always better than othertwo methods. This experiment also can verify the our view that the performance of SVRGis better or worse than SGD that depends on the number of training samples. In ourexperiment, if choosing small dataset (e.g. MNIST), SVRG is better than SGD. Otherwise,if choosing relatively large dataset (e.g. CIFAR-10), SVRG is worse than SGD.
