Figure 1: We propose CyCADA, an adversarial unsupervised adaptation algorithm which uses cycleand semantic consistency to perform adaptation at multiple levels in a deep network. Our modelprovides significant performance improvements over source model baselines.
Figure 2: Cycle-consistent adversarial adaptation overview. By directly remapping source trainingdata into the target domain, we remove the low-level differences between the domains, ensuringthat our task model is well-conditioned on target data. We depict here the image-level adaptationas composed of the pixel GAN loss (green), the source cycle loss (red), and the source and targetsemantic consistency losses (black dashed) - used when needed to prevent label flipping. For claritythe target cycle is omitted. The feature-level adaptation is depicted as the feature GAN loss (orange)and the source task loss (purple).
Figure 3: Ablation: Effect of Semantic or Cycle Consistency Examples of translation failureswithout the semantic consistency loss. Each triple contains the original SVHN image (left), the imagetranslated into MNIST style (middle), and the image reconstructed back into SVHN (right). (a)Without semantic loss, both the GAN and cycle constraints are satisfied (translated image matchesMNIST style and reconstructed image matches original), but the image translated to the target domainlacks the proper semantics. (b) Without cycle loss, the reconstruction is not satisfied and though thesemantic consistency leads to some successful semantic translations (top) there are still cases of labelflipping (bottom).
Figure 4: GTA5 to CityScapes Semantic Segmentation. Each test CityScapes image (a) along withthe corresponding predictions from the source only model (b) and our CyCADA model (c) are shownand may be compared against the ground truth annotation (d).
Figure 5: GTA5 to CityScapes Image Translation. Example images from the GTA5 (a) andCityscapes (c) datasets, alongside their image-space conversions to the opposite domain, (b) and (d),respectively. Our model achieves highly realistic domain conversions.
Figure 6: Network architectures used for digit experiments. We show here the task net (f), discrimi-nator for feature level adaptation (Df eat), discriminator for image level adaptation (Dimage), andgenerator for source to target (G) - same network used for target to source.
Figure 7: GTA5 to CityScapes Image Translation. Example images from the GTA5 (a) andCityscapes (c) datasets, alongside their image-space conversions to the opposite domain, (b) and (d),respectively. Our model achieves highly realistic domain conversions.
Figure 8: Cross Season Image Translation. Example image-space conversions for the SYNTHIAseasons adaptation setting. We shoW real samples from each domain (Fall and Winter) alongsideconversions to the opposite domain.
Figure 10: Confusion matrices for SVHN → MNIST experiment.
Figure 9: Image transformation results from Shrivastava et al. (2017) applied to GTA to CityScapestransformation. We demonstrate results using three different settings for λ.
