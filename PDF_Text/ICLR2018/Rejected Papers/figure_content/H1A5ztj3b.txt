Figure 1: Examples of super-convergence with Resnet-56 on Cifar-10.
Figure 2: Comparison of learning rate range test results.
Figure 3: The 3-D visualizations from Goodfellow et al. (2014). The z axis represents the losspotential.
Figure 4: Evidence of regularization with large learning rates: decreasing generalization error as thelearning rate increases from 0.3 to 1.5.
Figure 5: Estimated learning rate from the simplified Hessian-free optimization (see text for additionalinformation).
Figure 6: Comparisons of super-convergence to typical training outcome with piecewise constantlearning rate schedule.
Figure 7: CompariSonS of Super-convergence to over a range of batch SizeS. TheSe reSultS Show that alarge batch Size iS more effective than a Small batch Size for Super-convergence training.
Figure 8: Comparisons for Cifar-100, Resnet-56 of super-convergence to typical (piecewise constant)training regime.
Figure 9: Comparisons for Cifar-10,(b) Comparison of test accuracies With Nesterovmethod.
Figure 10: Comparisons of super-convergence with and without dropout (dropout ratio=0.2).
Figure 11: Comparisons for Cifar-10, Resnet-56 of super-convergence to typical training regime.
Figure 12: Comparison of learning rate range test results on Cifar-10 with alternate architectures.
