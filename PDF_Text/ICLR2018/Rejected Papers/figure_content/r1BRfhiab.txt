Figure 1: The Principle of Logit Separation. Left: when training with the cross-entropy loss, thelogit values for the class ‘Cat’ can be the same for two examples, one where it is the true class(blue) and one where it is not (red). Therefore, at test-time, a logit with the same value for the class‘Cat’ does not indicate whether the example belongs to this class. Right: With a loss function thatis aligned with the Principle of Logit Separation, all true logits are greater than all false logits attraining time. Hence, at test time, a single logit can indicate the correctness of its respective class.
