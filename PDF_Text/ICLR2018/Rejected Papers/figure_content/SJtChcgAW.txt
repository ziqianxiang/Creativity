Figure 1: Time Complexity4.2	Classification Accuracy4.2.1	Sparse RegularisationIn this section we perform experiments on the proposed compression scheme with feedforwardneural networks. We compare the original full-precision network (without compression) with thefollowing compressed networks: (i) FeTa1 with Ω(U) = ||U||1 (ii) FeTa2 with Ω(U) = ||U||1(iii) Net-Trim (vi) LOBS (v) Hard Thresholding. We refer to the respective papers for Net-Trimand LOBS. Hard Thresholding is defined as F(x) = x I (|x| > t), where I is the elementwiseindicator function, is the Hadamard product and t is a positive constant.
Figure 2: Accuracy vs SparsityTable 1: Test accuracy rates (%) prune only first fully connected layer.
Figure 3: ACCuraCy vs CRIn the above given U ∈ Rd1 ×d2 the CommPreSSion Ratio (CR) is defined as CR = (k * d1 + k +k * d2)∕(d1 * d2). The results are in line with the l1 regularisation, with significant degredation inClassifiCation aCCuraCy for Hard Thresholding above 85% CR.
Figure 4: Layer RobustnessWe now aim to see how well our bound captures this exponential behaviour. We take two net-works ga pruned at layer 3 and gb pruned at layers 2 and 3 and make a number of simplify-ing assumptions.
