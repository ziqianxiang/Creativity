Figure 1: Left: Schematic agent architecture. Right: An example of the word learning environmentcommon to all experiments in this paper. The agent observes two 3D rotating objects and a languageinstruction and must select the object that matches the instruction. In this case the instruction isa shape word (chair). The confounding object (a refrigerator) and the colours of both objects areselected at random and will vary across the agent’s experience of the word chair.
Figure 2: Degrees of shape bias for different training regimes: An agent that is trained onlyon shape words (right) more readily presumes that ambiguous words refer to object shape than toobject colour. This tendency is measured across all combinations of known and confounding objectsand labels and represented by the blue line. The magnitude of the bias on the scale [-10, 10] is themean ‘score’ (10 for the object matching the instruction in shape and 10 for the object matching incolour) over 1000 random test episodes. In contrast, an agent trained only on colour words (left)exhibits a colour bias. Interestingly, an agent trained on 8 colour and 8 shape words (middle) alsoexhibits a colour bias. Data (in this and proceeding figures) show mean and standard error acrossfive fastest-learning agents of 16 different hyperparameter settings, sampled at random from rangesspecified in in supplementary material 6.1.
Figure 3: The problem of learning negation in language learning agents: The agent must beexposed to negative instructions in a sufficiently diverse range of contexts in order to learn a usefulgeneralisable notion of negation. If trained to interpret positive commands involving 12 terms andnegative commands involving 6 of those 12 terms (left, colour terms, middle, shape terms), theagent does not effectively interpret negative commands involving the remaining 6 terms. Whenexposed to 40 shape terms and trained to interpret negative commands involving 20 of those terms, theagent generalises the negation operation more effectively, but still not perfectly. When the two-wordnegative instructions are encoded with an LSTM rather than additive BOW encoder, an almostidentical pattern of gradually improving generalisation is observed.
Figure 4: Curriculum training expedites vocabulary growth: An agent that is presented withstimuli sampled uniformly from a set S of 40 shape words (red line) learns more slowly than onewhose stimuli are constrained to a two-word subset S1, S1 ⊂ S, until the agent learns both words,then extended to a 5-word subset S2, S1 ⊂ S2 ⊂ S, then a 10-word subset S3, S2 ⊂ S3 ⊂ S. Thisstrong effect of ‘curriculum learning’ can be observed both when comparing average reward whenagents in the two conditions are learning words sampled from S (left - note that the agent in thecurriculum condition begins reporting performance on S after prior training on the restricted subsets)and by measuring vocabulary size as a function of training episodes (right).5As shown in Figure 4, an agent that followed the curriculum (i.e. in the second condition) learnednotably faster overall than one presented directly with a large group of new words. This result furthercorroborates the importance of training curricula for grounded language learning agents. Moreover,unlike the effect observed by Hermann et al. (2017), which focused on tasks requiring the agent toexplore a large maze, the present simulation demonstrates strong curriculum effects simply whenlearning to associate objects with words. Thus, the development of core linguistic and semanticknowledge in situated or grounded agents can be clearly expedited by starting small and easy andslowly increasing the language learning challenge faced by the agent.
Figure 5: Words from different semantic classes are learned at different speeds: In the fixedclass-size condition (left), the agent learns two words from each class. In the variable class-sizecondition (right) each class has a different number of members, as per supplementary material 6.2.
Figure 6: Representation and processing differences between colour and shape words. ‘Dash-boards’ for interpreting processing in an agent with layerwise attention. The large pane at the top leftof the dashboard shows the input to the agent. The bar chart on the bottom left shows the attentiondistribution over all 520 ‘locations’ from the agent’s visual representations. 400 red bars show theattention on the (20 × 20) locations output from the lowest layer of the convnet, 81 green bars showthe attention on the (9 × 9) locations from the middle layer and 49 blue bars show the attention onthe (7 × 7) locations from the top layer. The small windows on the right side illustrate these attentionweights (grouped by layer) propagated back to and superimposed over a greyscale copy of the inputimage, as described by Simonyan et al. (2014). An agent trained exclusively on colour words (A)relies more on the first and second layers of the convnet than an agent trained exclusively on shapewords (B), which uses second and upper layer visual features. C: A schematic of layerwise attentionin the agent architecture. D: A 2D (t-SNE) visualisation of the space of the word embeddings weightsin the language module (L) of an agent trained on different word types, illustrating that words clusternaturally according to semantic classes in the linguistic memory of the agent.
