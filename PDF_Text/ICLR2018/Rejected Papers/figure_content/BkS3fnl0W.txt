Figure 1: The basic architecture of the CorGAN(a) The deep deconvolutional architecture of the GeneratorCl: feature map 64x14x14	C2: feature map 128x7x7	F3: layer 6272 F4: layer 1024is trained to minimise the cost U(G):min U(G) = Ez〜Pz (Z) lθg(∣ anew - D(G(Z) )|)	⑵The CorGAN model is updated via back-propagation algorithm. If the D is overly optimised withoutupdating the G, it will result in overfitting problem. TheD and the G will be updated simultaneouslyor alternately to avoid the problem, e.g. k steps of optimizing the D and one step of optimizing theG. The traditional GANs reach Nash equilibrium after several training epochs. The new objectiveof the G of CorGAN breaks Nash equilibrium of the training process, which causes that the G cankeep generating outlier samples.
Figure 2: Comparison between the generated data and the training data: The images of handwrittendigit nine are training data. After several training epochs, the generated images and the training dataare visualised in the figure.
Figure 3: Area Under the Curve of inlier accepted fraction: The figure describes the relationshipbetween the inlier accepted fraction and the specified threshould. Given the specified threshold 0.7,the point P in the curve corresponds to the accepted fraction of inliers 0.68. Since no outlier isavailable, the area under this curve (positively biased AUC) is a good measure to select the nearoptimal model.
Figure 4: The figures show the ROC curves of all models on three differenct test datasets. Thearea under the ROC curve represents the overall performance of a one-class classifier. The modelCorGAN3 shows robust performance on all the three datasets.
