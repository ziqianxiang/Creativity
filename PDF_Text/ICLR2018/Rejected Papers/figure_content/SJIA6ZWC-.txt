Figure 1: Training and validation lossof a neural net, estimated by cross-validation (crosses) or by a hypernet(lines), which outputs 7, 850-dimensionalnetwork weights. The training and valida-tion loss can be cheaply evaluated at anyhyperparameter value using a hypernet.
Figure 2: A visualization of exact (blue) and approximate (red) optimal weights as a function ofgiven hyperparameters. Left: The training loss surface. Right: The validation loss surface. Theapproximately optimal weights w@* are output by a linear model fit at A. The true optimal hyperpa-rameter is λ*, while the hyperparameter estimated using approximately optimal weights is nearby atλφ* .
Figure 3: A comparison of standard hyperparameter optimization, and our first algorithm. Instead ofupdating weights W using the loss gradient dL(W)/∂w, We update hypernet weights φ using the chainrule: ©LWW? dWWφφ. Also, instead of returning the best hyperparameters from a fixed set, our methoduses gradient-based hyperparameter optimization. Here, hyperopt refers to a generic hyperparameteroptimization.
Figure 4: A side-by-side comparison of two variants of hyper-training. Algorithm 3 fuses the hy-pernet training and hyperparameter optimization into a single loop of SGD.
Figure 5: The training and validationlosses of a neural network, estimated bycross-validation (crosses) or a linear hy-pernet (lines). The hypernet’s limited ca-pacity makes it only accurate where hy-perparameter distribution put mass.
Figure 6: Validation and test losses during hyperparameter optimization with a separate L2 weightdecay applied to each weight in the model. Thus, models with more parameters have more hy-perparameters. Left: The 7, 850 dimensional hyperparameter optimization problem from havinga linear model is solved with multiple algorithms. Hypernetwork-based optimization convergesfaster than unrolled optimization from Maclaurin et al. (2015a) but to a sub-optimal solution. Right:Hyper-training is applied different layer configurations in the model. The hand-tuned regulariza-tion parameters on the 784-10, 784-100-10, and 784-100-100-10 models have a validation losses of0.434, 0.157 and 0.206 respectively.
Figure 7: Comparing three approaches to predicting validation loss. First row: A Gaussian pro-cess, fit on a small set of hyperparameters and the corresponding validation losses. Second row: Ahypernet, fit on the same small set of hyperparameters and the corresponding optimized weights.
Figure 8: Validation and test losses during hyperparameter optimization. A separate L2 weightdecay is applied to the weights of each digit class, resulting in 10 hyperparameters. The weightswφ* are output by the hypernet for current hyperparameter λ, while random losses are for the bestresult of a random search. Hypernetwork-based optimization converges faster than random searchor Bayesian optimization. We also observe significant overfitting of the hyperparameters on thevalidation set, which may be reduced be introducing hyperhyperparameters (parameters of the hy-perparameter prior). The runtime includes the inner optimization for gradient-free approaches sothat equal cumulative computational time is compared for each method.
