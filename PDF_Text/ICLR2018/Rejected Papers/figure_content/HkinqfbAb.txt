Figure 1: Joint histograms of parameters before and after training, with without (left) and withan additional k-means loss (soft-tying APT). The parameters are initialized with scaled uniformdistributions proposed in (Glorot & Bengio, 2010) and K = 8.
Figure 2: Sparsity versus accuracy trade-off ofsparse APT for LeNet-300-100, shown as thePareto frontier of typical hyper-parameter searchresults.
Figure 3: Comparison of training with APT (first 20000 iterations soft-tying, last 20000 iterationshard-tying) vs. without regularization on LeNet-300-100, using the same initialization/learning rate.
Figure 4:	Evolution of the clusters in the first APT experiment with LeNet-300-100.
Figure 5:	Effect of varying K and t on learning outcome with APT.
Figure 6: Visualization of the first conv layer in LeNet-5, achieving 1% test error and 99.5% sparsity.
Figure 7: Comparing the the number of input units pruned by `2, `1, and sparse APT, on LeNet-300-100.
Figure 8: First layer weight matrix of LeNet-300-100 learned with `2, APT, `1, and sparse APT.
