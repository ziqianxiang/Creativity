Figure 1: An example of how we deal with out-of-vocabulary words (indicated in bold). We obtainrepresentations by retrieving external information (e.g. a dictionary definition) and embedding it,for example, with another LSTM-RNN, instead of using a catch-all “UNK” representation forout-of-vocabulary items.
Figure 2: The attention maps AC of the models with (on the left) and without the dictionary (on theright). The rows correspond to words of the context and the columns to the words of the question. Onecan see how with the help of the dictionary the model starts considering “overseas“ as a candidateanswer to “where“.
Figure 3: (a) t-SNE projection of word embeddings computed on the fly. (b) Prediction accuracy ofdifferent ESIM models trained on SNLI, as a function of the mean log rank of words in the input. Asexpected, the dictionary-enabled model clearly outperforms the baseline on sentences contaning rarewords.
