Figure 1: The video predici-ton model is trained on 17,000video sequences collected au-tonomously by a Sawyer robotinteracting with 140 differentevery-day objects.
Figure 3: Our network architecture. Our network uses a U-Net (Ronneberger et al., 2015) With recurrentconnections in internal layers (SChmidhuber, 1987). As proposed by Finn et al. (2016), the network predicts(1) a set of convolution kernels to produce a set of transformed input images (2) synthesized pixels at the inputresolution and (3) a compositing mask. Using the mask, the network can choose how to composite together theset of warped pixels, the first frame, previous frame, and synthesized pixels.
Figure 4: Visual realism (left) and Predicted flow accuracy error (right) through time. For visual realism(higher is better), we show that our architecture, trained with the same `2 loss (Ours (no adv)) yields morerealistic results than previous state-of-the-art (CDNA, SNA). Adding an adversarial loss (Ours(adv)) greatlyimproves visual realism. For flow accuracy error (lower is better), our architecture produces much more accu-rate flows than baselines, and adding the adversarial loss slightly improves accuracy.
Figure 5: Qualitative examples of visual quality. We show selected examples of predicted frames. The lefttwo columns show the initial ground truth frame, as well as a frame in the future. Note that for each method,a prediction is made at each time step. We show the result from a single frame for visual clarity clarity. Pleasesee our supplemental material for a complete set of video results. The examples shown here vary from 6 to 14steps in the future. The next two columns show results from previous methods CDNA (Finn et al., 2016) andSNA (Ebert et al., 2017), respectively. These methods produce large blurriness in the gripper and objects thegripper may have come into contact with. The second to last column shows our method, without the adversarialloss. As seen in the first three rows, our improved architecture is able to produce a sharper image, even withan `2 regression loss. The final column shows our full method (Ours), with the adversarial loss. Note theincreased sharpness on the gripper in the first three rows, along with the blue objects in the first and secondrow, relative to Ours (no adv). The last row shows a failure case, where our method is unable to recover highfrequencies and produces a blurry, visually implausible, result.
Figure 6: Predicted frames, pixel distributions, and flows. None of the model are trained using optical flowsupervision - only supervision from raw pixels.
