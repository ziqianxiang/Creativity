Figure 1: Gradient Normalization. Imbalanced gradient norms (left) result in suboptimal trainingwithin a multitask network, so we implement a novel gradient loss Lgrad (right) which detects suchimbalances in gradient norms amongst tasks and tunes the weights in the loss function to compen-sate. We illustrate here a simplified case where such balancing results in equalized gradient norms,but in general some tasks may need higher or lower gradient norms relative to other tasks for optimaltask balancing (discussed further in Section 3).
Figure 2: Gradient Normalization on a toy 2-task (top) and 10-task (bottom) system. Diagramsof the network structure with loss scales are on the left, traces of wi (t) during training in the middle,and task-normalized test loss curves on the right. α = 0.12 for all runs.
Figure 3: Test and training loss curves for all tasks in expanded NYUv2, VGG16 backbone.
Figure 4: Gridsearch performance for random task weights, expanded NYUv2. Average changein performance across three tasks for a static network with weights wistatic is plotted against theL2 distance between wistatic and our GradNorm network’s time-averaged weights, Et [wi (t)]. Allcomparisons are made at 15000 steps of training.
Figure 5: Visualizations at inference time. Expanded NYUv2 with room layout labels is shown onthe left, while downsampled NYUv2 with semantic segmentation labels is shown on the right.
Figure 6: Weights wi (t) during training, expanded NYUv2. Traces of how the task weights wi(t)change during training for two different values of α. A larger value of α pushes weights fartherapart, leading to less symmetry between tasks.
