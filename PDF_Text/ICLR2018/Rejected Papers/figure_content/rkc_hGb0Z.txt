Figure 1: Simulation Experimentswhere xk,i,j is the jth sample from pi(xk) obtained by running pi(uk|xk) on the real system, andDi are the trajectory samples rolled out on the system. Our robust GPS algorithm is thus given inalgorithm 2. We follow the prior works in [1, 13, 14, 16] in computing the KL divergence term andwe refer readers to these works for a more detailed treatment.
Figure 2: Sensitivity Analysis for Peg Insertion Taskγ-penaltyand angular velocities with two controller states. We train the protagonist's policy using the GPSalgorithm. We then pit an adversarial disturbance against the trained policy so that the adversarystays in closed-loop with the trained protagonist; The closed-loop cost function is given by'(Xk, Uk, Vk)	=	1WUUTUk	+	Wp'12(dχk	-	d*)	-	YvTvk	(14)where Y represents the disturbance term, dχk denotes the end effector,s (EE) position at state Xkand d? denotes the EE,s position at the slot,s base. '12(Z) is a term that makes the peg reach thetarget at the hole,s base, precisely given by ɪZtZ + (α + Z2)2. We set WU and WP to 10-6 and 1respectively. For various values of γ, we check the sensitivity of the trained policy and its effecton the task performance by maximizing the cost function above w.r.t vk. We run each sensitivityexperiment for a total of 10 iterations. Fig. 2 shows that the adversary causes a sharp degradationin the protagonist,s performance for values of γ < 1.5. This corresponds to when the GPS-trainedpolicy gets destabilized and the arm struggles to reach the desired target. As values of Y ≥ 1.5,however, we find that the adversary has a reduced effect on task performance: the adversary's effectdecreases as Y gets larger. Video of this result is available at https://goo.gl/YmmdhC.
Figure 3: [LEFT]: Cost of running the dynamic game-based robust guided policy search algoruithmfor various values of gamma for the robot peg insertion task. Our algorithm uses lesser number forthe Gaussian mixture models and requires fewer samples to generalize to the real-world. RIGHT:Sensitivity Analysis for Arm Swing-up Task6.2	Robust RL with GPSAs proposed in section 5, our goal is to improve the robustness of the controller’s policy in the pres-ence of modeling errors and uncertainties and transfer errors. We follow the formulation in section 5and generate vk from zero-mean, unit variance noise samples in every iteration. We employ variousvalues of γ as a robustness parameter and we run the dynamic game during the trajectory optimiza-tion phase of the GPS algorithm. Specifically, for the values of γ that the erstwhile policies in theprevious subsection fail, we run the dynamic game algorithm to provide robustness in performnaceat test time compared against the GPS algorithm. We run experiments on the peg insertion task toverify the algorithm. Figure 3 shows the cost of running the robust GPS algorithm on the 7-DoFrobot. We see that the policies that show achieve optimal performance behavior are now less costlycompared to vanilla GPS algorithm. For values of the sensitivity term γ that the algorithm erstwhilefails in, we now see smoother execution of the trajectory in trying to achieve our goal. The model-ing phase of the algorithm is also much less data consuming as our GMM algorithm now takes lesssamples before generalizing to the global model.
