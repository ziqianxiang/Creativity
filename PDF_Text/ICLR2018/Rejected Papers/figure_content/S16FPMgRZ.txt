Figure 1: A representation of the Tensor Contrac-tion Layer (TCL) on a tensor of order 3. The inputtensor X is contracted into a low rank core X0 .
Figure 2: In standard CNNs, the input X is flattened and then passed to a fully-connected layer,where it is multiplied by a weight matrix W.
Figure 3: We propose to first reduce the dimensionality of the activation tensor by applying tensorcontraction before performing tensor regression. We then replace flattening operators and fully-connected layers by a TRL. The output is a product between the activation tensor and a low-rankweight tensor W . For clarity, we illustrate the case of a binary classification, where y is a scalar. Formulti-class, y becomes a vector and the regression weights would become a 4th order tensor.
Figure 4: Empirical comparison (4a) of the TRL against regression with a fully-connected layer. Weplot the weight matrix of both the TRL and a fully-connected layer. Due to its low-rank weights, theTRL better captures the structure in the weights and is more robust to noise. Evolution of the RMSEas a function of the training set size (4b) for both the TRL and fully-connected regressionTable 1: Results obtained on ImageNet by adding a TCL to a VGG-19 architecture. We reduce thenumber of hidden units proportionally to the reduction in size of the activation tensor following thetensor contraction. Doing so allows more than 65% space savings over all three fully-connectedlayers (i.e. 99.8% space saving over the fully-connected layer replaced by the TCL) with no corre-sponding decrease in performance (comparing to the standard VGG network as a baseline).
Figure 5: 5a shows the Top-1 accuracy (in %) as we vary the size of the core along the number ofoutputs and number of channels (the TRL does spatial pooling along the spatial dimensions, i.e., thecore has rank 1 along these dimensions).
