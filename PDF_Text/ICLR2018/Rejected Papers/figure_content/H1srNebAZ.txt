Figure 1: The figures show the histograms of the average sign of partial derivatives of the losswith respect to activation of samples, as collected over training for a random neuron in ten differentlayers. An average derivative sign of 1 means that the derivative of the activation of this samplewas positive in all the recordings performed during training. For layers close enough to the output,we clearly observe two distinct categories: some sample activations should always go up, othersalways down. This reveals that the neuron receives consistent information about how to affect theactivation of a sample, allowing it to act as a binary classifier. As detailed in Section 3, the layersfrom the first two rows are part of a network trained on MNIST (with ReLU and sigmoid activationfunctions respectively), the third and fourth row on CIFAR-10 (with ReLU and no activation functionrespectively).
Figure 2: Evolution of the pre-activation distributions across training. Plots correspond to oneneuron from dense2-relu (first row), dense2-sigmoid (second row), stage3layer2-relu (third row) andstage3layer2-linear (fourth row). Pre-activations are separated in two categories, high and low, basedon the average partial derivative sign over training of their corresponding activation (see Figure 1).
Figure 3: Quantization experiment: measuring test accuracy when pre-activations of a layer arequantized to two values per neuron, based on their percentile rank. Quantization is performed on asingle layer at a time, using a range of percentile ranks as quantization thresholds. Except for conv1(very first layer of ResNet50), the networks are astonishingly robust to quantization, suggesting thatneurons provide a binary signal to the next layers. The average percentile rank of the zero pre-activation (which corresponds to ReLU’s and sigmoid’s threshold) is also provided. As detailed inSection 3, the layers from the first two rows are part of a network trained on MNIST (with ReLUand sigmoid activation functions respectively), the third and fourth row on CIFAR-10 (with ReLUand no activation function respectively) and the fifth row on ImageNet (with ReLU activation).
Figure 4: Sliding window binarization experiment: pre-activations inside a window with a widthof percentile rank 10 are mapped to 1, pre-activations outside of it to 0. Information that remainsin the signal is only the fact that the pre-activation was inside or outside the window. Observingif a new network can use this information for classification reveals structure about the encoding:which window positions provide the most important information for a classifier? The results showa clear pattern across all layers and networks that confirms an encoding based on a fuzzy, binarypartition of the inputs in two categories of nearly equal size. As detailed in Section 3, the layersfrom the first two rows are part of a network trained on MNIST (with ReLU and sigmoid activationfunctions respectively), the third and fourth row on CIFAR-10 (with ReLU and no activation functionrespectively) and the fifth row on ImageNet (with ReLU activation).
Figure 5: The figures show the histograms of the average sign of partial derivatives of the loss withrespect to sample activations, as collected over training for all neurons in ten different layers. Anaverage derivative sign of 1 means that the derivative of the activation of a neuron for this sample waspositive in all the recordings performed during training. The histograms represent the statistics onall (neuron, sample) pairs of the layer. For layers close enough to the output, we clearly observe twodistinct categories: some sample activations should always go up, others always down. This revealsthat the neuron receives consistent information about how to affect the activation of a sample. Theneuron-wise histograms in Figure 1 moreover show that more or less half of the input samples havenegative derivatives, and the other positive ones, allowing a neuron to act as a binary classifier. Asdetailed in Section 3, the layers from the first two rows are part of a network trained on MNIST(with ReLU and sigmoid activation functions respectively), the third and fourth row on CIFAR-10(with ReLU and no activation function respectively).
Figure 6: Evolution of the pre-activation distributions across training. Each line correspondsto the dynamics of a different neuron. Plots correspond to neurons from dense2-relu (row 1-3) and from dense2-sigmoid (row 4-6). Pre-activations are separated in two categories, highand low, based on the average partial derivative sign over training of their corresponding acti-vation. We can see that both categories are being separated during training. The final highestpre-activations of the high category are highlighted to show that it is not a simple translation.
Figure 7: Evolution of the pre-activation distributions across training. Each line corresponds to thedynamics of a different neuron. Plots correspond to neurons from stage3layer2-relu (row 1-3) andfrom stage3layer2-linear (row 4-6). Each line corresponds to the dynamics of a different neuron.
Figure 8: Histogram showing the consistency between the class of a sample and its belonging to thelow category of a neuron (samples whose activation should be decreased) in dense2-relu. In mostcases, nearly all the elements of a class are in the same category, as can be seen by the two peaks at0 and 100%.
