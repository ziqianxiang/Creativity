Figure 1: Example of mapping session data to user actions. The session data comprises of sequenceof logs, each log comprises of search query, filters applied (content type), offset field and interactionperformed by the user (such as search, click etc)photography and digital asset marketplace which contain information on queries made by real users,the corresponding clicks and other interactions with the assets. This information has been usedto generate a user which simulates human behavior while searching and converses with the agentduring search episode. We map every record in the query log to one of the user actions as depictedin Table 3. Figure 1 shows an example mapping from session data to user action. To model ourvirtual user, we used the query and session log data of approximately 20 days.
Figure 2: A3C architecture for predicting policy pt and value V (st). Current search state st isprocessed by a LSTM followed by a fully connected layer. The cell state ct and hidden state ht ofLSTM from previous time step is retained while processing the next state during an episode. Thesame fully connected layer is used for prediction at different time steps in an episode. The episodeterminates at time step T .
Figure 3: Plot of average validation reward against number of training episodes for A3C agent. Thesize of LSTM is 250 for each plot with varying discount factor; γ = 0.90 (left), γ = 0.80 (middle)and γ = 0.60 (right). It can be observed that a lower γ value results in higher variance in the rewardsresulting in a greedy (less exploratory) policy.
Figure 4: Plot of mean of state values observed in an episode for A3C agent. Different curves corre-spond to different LSTM size. The discount value is γ = 0.90 for each curve. Better states (higheraverage state values) are observed with larger LSTM size since it enables the agent to remembermore context while performing actions.
Figure 5: Plots of mean of state values in an episode for A3C agent with γ = 0.90 to analyzeadvantage of history in state representation. The 3 plots correspond to different LSTM sizes - 250(left), 150 (middle) and 100 (right). Not including history does not effect the state values for largerLSTM but results in lower average value of the states observed for relatively smaller LSTM size.
Figure 6: Plot of average reward observed in validation episodes with Q-agent (left) with γ = 0.70and = 0.90) and A3C agent (right) with γ = 0.90 and LSTM size = 250. The average rewardvalue at convergence is larger for A3C agent than Q-agent.
Figure 7: Architecture Diagram(left) and Chat-Search Interface(right)14Under review as a conference paper at ICLR 20186.1.1	Chat InterfaceChat interface comprises of a two-pane window, one for text dialogues and other for viewing searchresults. The chat interface allows the user to convey queries in form of dialogue in an unrestrictedformat. The user message is parsed by NLU which deduces user action from the input message. TheNLU additionally obtains and redirects the query to search engine if the user action is new queryor refine query. User action and search engine output are forwarded to RL agent by NLU. TheRL agent performs an action according to the learned policy based on which a formatted responseis displayed on chat interface by the NLU. In addition to inputting the message, the chat interfaceprovides functionality for other user actions such as liking a search result, adding assets to cart etc.
