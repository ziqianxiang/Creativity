Figure 1: A digit from anomaly class (‘7’) distorted to appear like a digit from the normal class (‘9’)(d) sattack = 0.8The adversary picks a target xit for each xi to be distorted and moves it towards the target by someamount. Choosing xit for each xi optimally requires a significant level of computational effort anda thorough knowledge about the distribution of the data. The attacker, similar to Zhou et al. (2012),uses the centroid of the normal data cloud in the training set as the target point for all anomaly datapoints that he/she intends to distort. A data point sampled from the normal class or an artificial datapoint generated from the estimated normal class distribution could be used as alternatives.
Figure 2: Training data distribution and separating hyperplane of a toy problem under differentattack severities. ‘o’ denotes the undistorted data points and ‘x’ denotes the data points distorted bythe adversary. The OCSVM is trained using the entire (unlabeled) dataset as normal.
Figure 3: The top row shows the performance of OCSVMs under adversarial conditions when thetraining takes place in different dimensional spaces. It compares the evaluation performance ofOCSVMs trained on trainC and trainD against the two test sets: testc and testD. The bottom rowshows the false positive rate of the OCSVMs under an integrity attack (i.e., trained on trainD andevaluated using testD).
