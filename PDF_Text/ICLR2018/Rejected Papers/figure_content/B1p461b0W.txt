Figure 1:	Performance onMNIST as different amounts ofnoisy labels are added to a fixedtraining set of clean labels. Wecompare a perceptron, MLPswith 1, 2, and 4 hidden layers,and a 4-layer ConvNet. Evenwith 100 noisy labels for everyclean label the ConvNet still at-tains a performance of 91%.
Figure 2:	Performance onCIFAR-10 as different amountsof noisy labels are added to afixed training set of clean la-bels. We tested ConvNets with4 and 6 layers, and a ResNetwith 101 layers. Even with 10noisy labels for every clean la-bel the ResNet still attains a per-formance of 85%.
Figure 3:	Performance on Im-ageNet as different amounts ofnoisy labels are added to afixed training set of clean la-bels. Even with 5 noisy labelsfor every clean label, the 18-layer ResNet still attains a per-formance of 70%.
Figure 4: Illustration of uniform and structured noise models. In the case of structured noise, theorder of false labels is important; we tested decreasing order of confusion, increasing order of con-fusion, and random order. The parameter δ parameterizes the degree of structure in the noise. Itdefines how much more likely the second most likely class is over chance.
Figure 5: Performance on MNIST with fixedα = 20 noisy labels per clean label. Noise isdrawn from three types of structured distribu-tion: (1) “confusing order” (highest probabilityfor the most confusing label), (2) “reverse con-fusing order”, and (3) random order. We inter-polate between uniform noise, δ = 0, and noiseso highly skewed that the most common falselabel is as likely as the correct label, δ = 1.
Figure 6: Performance on CIFAR-10 for vary-ing amounts of noisy labels. Noisy training ex-amples are drawn from (1) CIFAR-10 itself, butmislabeled uniformly at random, (2) CIFAR-100, with uniformly random labels, and (3)white noise with mean and variance chosen tomatch those of CIFAR-10. Noise drawn fromCIFAR-100 resulted in only half the drop in per-formance observed with noise from CIFAR-10itself, while white noise examples did not ap-preciable affect performance.
Figure 7: Comparison of the effect of reusingimages vs. using novel images as noisy exam-ples. Essentially no difference is observed be-tween the two types of noisy examples, support-ing the use of repeated examples in our experi-ments.
Figure 9: Performance on MNIST for varyingbatch size as a function of noise level. Higherbatch size gives better performance. We approx-imate the limit of infinite batch size by trainingwithout noisy labels, but using the noisy lossfunction Hα .
Figure 10: Performance on CIFAR-10 for vary-ing learning rate as a function of noise level.
