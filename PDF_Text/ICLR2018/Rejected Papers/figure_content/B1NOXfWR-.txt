Figure 1: Example task and our agentâ€™s trajectory. The agent is required to execute subtasks in the optimal orderto maximize the reward within a time limit. The task graph describes subtasks with the corresponding rewards(e.g. subtask F gives 0.3 reward) and dependencies between subtasks through AND and OR nodes. For instance,in order to execute subtask F, the agent needs to satisfy its precondition: OR(AND(A, B), AND(B, C, NOT(D)))).
Figure 2: Neural task graph solver architecture. The task module encodes the task graph through a bottom-upand top-down process, and outputs the reward score (Preward). The observation module encodes observationusing CNN and outputs the cost score (Pcost). The final policy is a softmax policy over the sum of two scores.
Figure 3: Visualization of OR, OR,AND, and AgND operations with threeinputs (a,b,c).
Figure 4: Learning curves. NTS-RProp isdistilled from RProp until 120 epochs andfine-tuned through actor-critic after that.
Figure 8: Performance of MCTS+NTS and MCTS on D2 (see Table 1) per the number of simulated episodes.
