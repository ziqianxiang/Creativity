Figure 1: A schematic layout of the typical approach for sample based GAN evaluation methods.
Figure 2:	Distinguishing a set of real images (in the training set) from a mixed set of real images and GANgenerated images. For the metric to be discriminative, its score should increase as the fraction of generatedsamples in the mix increases. RIS and RMS fail as they decrease with the fraction of generated samples in Sg onLSUN. Wasserstein and 1-NN accuracy (real) fail in pixel space as they do not increase.
Figure 3:	Experiment on simulated mode collapsing. A metric score should increase to reflect the mismatchbetween true distribution and generated distribution as more modes are collapsed towards their cluster center. Allmetrics respond correctly in convolutional space. In pixel space, both Wasserstein distance and 1-NN accuracy(real) fail as they decrease in response to more collapsed clusters.
Figure 4: Experiment on simulated mode dropping. A metric score should increase to reflect the mismatchbetween true distribution and generated distribution as more modes are dropped. All metrics except RIS andRMS respond correctly, as they only increase slightly in value even when almost all modes are dropped.
Figure 5: Experiment on robustness of each metric to small transformations (rotations and translations). Allmetrics should remain constant across all mixes of real and transformed real samples, since the transformationsdo not alter semantics of the image. All metrics respond correctly in convolutional space, but behave incorrectlyin pixel space. This experiment illustrates the unsuitability of distances in pixel space.
Figure 7: Measurement of wall-clock timefor computing various metrics as a functionof the number of samples. All metrics arepractical to compute for a sample of size2000, but Wasserstein distance does notscale to large sample sizes.
Figure 6: The score of various metrics as a function of the number of samples. An ideal metric should result in alarge gap between the real-real (R-R; P(Sr, Sr)) and real-fake (R-G; P(Sr, Sg)) curves in order to distinguishbetween real and fake distributions using as few samples as possible. Compared with Wasserstein distance,MMD and 1-NN accuracy require much fewer samples to discriminate real and generated images, while RIStotally fails on LSUN as it scores generated images even better (lower) than real images.
Figure 8: Experiment on detecting overfitting of generated samples. As more generated samples overlap with realsamples from the training set, the gap between validation and training score should increase to signal overfitting.
Figure 9: Comparison of all metrics in different feature spaces. When using different trained networks, the trendsof all metrics are very similar. Most metrics work well even in a random network, but Wasserstein distance hasvery high variance and the magnitude of increase for 1-NN accuracy is small.
Figure 10: Using features extracted from a ResNet trained on CIFAR-10 (right plot) to evaluate a GAN modeltrained on the same dataset. Compared to using an extractor trained on ImageNet, the metrics appear to havelower variance. However, this may due to the feature dimensionality being smaller for CIFAR-10.
Figure 11: Training curves of DCGAN and WGAN on a large (left two panels), small (middle two panels) andtiny (right two panels) subsets of CelebA. Note that for the first four plots, blue (yellow) curves almost overlapwith the red (green) curves, indicating no overfitting detected by the two metrics. Overfitting only observed onthe tiny training set, with MMD score and 1-NN accuracy significantly worse (higher) on the validation set.
