Figure 1: The global flow of ResBinNet binary training. Residual binarization learns a multi-levelrepresentation for feature maps. Temperature adjustment performs a change of variable over thetrainable weights and gradually pushes them towards binary values during the training phase.
Figure 2: Schematic flow for computing 3 levels of residual binary estimates e. As we go deeper inlevels, the estimation becomes more accurate.
Figure 3: Illustration of binarized activation function. (a) Conventional 1-level binarization. (b)Residual binarization with two levels.
Figure 4: An example change of variable with Tanh nonlinearity. (a) The effect of the temperatureparameter: higher α values provide better soft-binary estimations. (b) The effect of the boundingparameter: γ is a trainable value for each weight matrix W . (c) The effect of the temperatureparameter a on the gradient filtering term -∂w.
Figure 5: Histogram of the elements of θ in a certain layer of the neural network during training.
Figure 6: Hardware architecture of the baseline (top) and our modified (bottom) binary CNN layer.
Figure 7: (a) Resource utilization overhead of ResBinNet with different residual levels versus base-line design( Umuroglu et al. (2017)) implemented on Xilinx ZC706 Evaluation Kit. (b) Latency-accuracy trade-off offered by ResBinNet with different residual levels.
