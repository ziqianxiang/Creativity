Figure 1: Performance of BitNet in comparison to LeNet32 and baselines on the MNIST and CIFAR-10 datasets. The left panel shows the test error % and the right panel shows the training error overtraining iterations. The learning rate Î¼ is halved after every 200 iterations.
Figure 2: Number of bits learned by BitNet for representing the parameters of each layer of the CNNfor MNIST and CIFAR-10.
Figure 3: Sensitivity of BitNet to Learning Rate.
Figure 4: Sensitivity of the Test Error and Compression Ratio to the hyperparameters for BitNet for(a) MNIST and (b) CIFAR-10 datasets6 ConclusionThe deployment of deep networks in real world applications is limited by their compute and memoryrequirements. In this paper, we have developed a flexible tool for training compact deep neuralnetworks given an indirect specification of the total number of bits available on the target device.
