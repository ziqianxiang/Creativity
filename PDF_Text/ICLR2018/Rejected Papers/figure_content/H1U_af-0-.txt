Figure 1: Relative error of approximation of kernel matrix with 95% confidence interval dependingon the scaling factor, Gaussian kernel was used. In this experiment for each scaling factor α weconstruct approximate kernel matrix Kb and the exact kernel matrix K using scaled input vectorsX = x∕α. To plot the confidence interval We run each experiment 10 times each time generatingnew weights. Experiment was conducted for 3 different input dimensions: d = 1O, 1OO, 5OO.
Figure 2: Kernel approximation error across three kernels (columns: arc-cosine 0, arc-cosine 1,Gaussian) on three datasets: LETTER (d = 16), MNIST (d = 784), CIFAR100 (d = 3072) andLEUKEMIA(d = 7129). Lower is better. The x-axis represents the factor to which we extend theoriginal feature space, n =哥片,where d is the dimensionality of the original feature space, D isthe dimensionality of the new feature space.
Figure 3: Accuracy/R2 score using embeddings with three kernels (columns: arc-cosine 0, arc-cosine 1, Gaussian) on three datasets (rows: Powerplant, LETTER, USPS). Higher is better. Thex-axis represents the factor to which We extend the original feature space, n = 2(指),where d isthe dimensionality of the original feature space, D is the dimensionality of the new feature space.
Figure 4: (a) Butterfly orthogonal matrix factors for d = 16. (b) Sparsity pattern for BPBPBP(left) and B (right), d = 15.
Figure 5: Kernel approximation error across three kernels (columns: arc-cosine 0, arc-cosine 1,Gaussian) on three datasets: Powerplant (d = 4), LETTER (d = 16), USPS (d = 256), MNIST(d = 784). Lower is better. The x-axis represents the factor to which we extend the originalfeature space, n = 2(dD+i), where d is the dimensionality of the original feature space, D is thedimensionality of the new feature space.
Figure 6: Kernel approximation error for Gaussian kernel on LETTER dataset. Subsampled densegrid method (denoted GQ) from Dao et al. (2017) show very similar performance to the baselinerandom Fourier features (denoted G on the picture).
Figure 7: Walltime experiment measures the time spent on applying the explicit mapping acrossdifferent methods. The x-axis represents the 5 datasets with increasing input number of features:LETTER (d = 16), USPS (d = 256), MNIST (d = 784), CIFAR100 (d = 3072) and LEUKEMIA(d = 7129). Thanks to structured explicit mapping of the proposed method B, it is favorable forhigher dimensional data.
