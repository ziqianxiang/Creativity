Figure 1: This figure illustrates the forward pass in SAB for the configuration ktop = 3, katt = 1,ktrunc = 2. This involves Microstate Selection (§ 3.2), Summarization of microstates(§ 3.3), and incorporation into next microstate (§ 3.4). Red arrows depict how atten-tion weights at(t{)-8,-7,-6,-5,-4,-3,-2,-1} are evaluated, first by broadcasting the currentprovisional hidden state h(t) against the macrostate (which, in the presented case ofkatt = 1, consists of all past hidden states), concatenating, then passing the result to anMLP. The attention weights are then run through the sparsifier which selects the ktop = 3attention weights, while the others are zeroed out. Black arrows show the microstatescorresponding to the non-zero sparse attention weights {a(-)6, α(-)3,々巴}, namely{m(t-6) = h(t-6), m(t-3) = h(t-3), m(t-2) = h(t-2)}, being weighted, summed, thenincorporated into h(t) to compute the current final hidden state h(t).
Figure 2: This figure illustrates the backward pass in SAB for the configuration ktop = 3, katt = 1,ktrunc = 2. The gradients are passed to the microstates selected in the forward pass anda local truncated backprop is performed around those microstates. Blue arrows show thegradient flow in the backward pass. Red crosses indicate TBPTT truncation points, wherethe gradient stops being backpropagated.
Figure 3: This figure shows how attention weights change over time for the Copying Task of copylength 200. The vertical axis is the time step attending from timestep 210 to timestep220. The horizontal axis is the time step being attended. The top most subfigure A is theattention plot for iteration 400 of epoch 0, subfigure B is for iteration 800 of epoch 0 andsubfigure C is for iteration 3000 of epoch 0.
