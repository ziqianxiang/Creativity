Figure 1: (a) Simple DNN with two inputs and three hidden layers with 2 activation units each. (b),(c), and (d) Visualization of the hyperplanes from the first, second, and third hidden layers respec-tively partitioning the input space into several linear regions. The arrows indicate the directionsin which the corresponding neurons are activated. (e), (f), and (g) Visualization of the hyperplanesfrom the first, second, and third hidden layers in the space given by the outputs of their respectiveprevious layers.
Figure 2: (a) A network with one input x1 and three activation units a, b, and c. (b) We show thehyperplanes x1 = 0 and -x1 + 1 = 0 corresponding to the two activation units in the first hiddenlayer. In other words, the activation units are given by ha = max{0, x1} and hb = max{0, -x1 +1}. (c) The activation unit in the third layer is given by hc = max{0, 4ha + 2hb - 3}. (d) Theactivation boundary for neuron c is disconnected.
Figure 3: Bounds from Theorem 1: (a) is in semilog scale, has input dimension n0 = 32, and thewidth of the first five layers is 16 - 2k, 16 - k, 16, 16 + k, 16 + 2k; (b) is in linear scale, evenlydistributes 60 neurons in 1 to 6 layers (the single-layer case is exact), and the input dimension varies.
Figure 4: Total number of regions classifying each digit (different colors for 0-9) of MNIST alone astraining progresses, each plot corresponding to a different number of hidden layers.
Figure 5: (a) Total number of linear regions classifying a single digit of MNIST as training pro-gresses, each plot corresponding to a different number of hidden layers. (b) Comparison of upperboundsfrom Montufaret al. (2014), Montufar (2017), andfrom Theorem 1 with the total number oflinear regions of a network with two hidden layers totaling 22 neurons.
Figure 6: (a) The 1D construction from Montufar et al. (2014). All units point to the right, leavinga region with dimension zero before the origin. (b) The 1D construction described in this section.
Figure 7: A function with a zigzag pattern composed with itself. Note that the entire function isreplicated within each linear region, up to a scaling factor.
Figure 8: Contrast of cross-entropy along training with number of regions identifying a single digitin the first experiment: (a) shows training error in green; (b) shows validation error in purple.
Figure 9: Contrast of final errors with number of regions and bound in the second experiment: (a)shows training error in green and validation error in purple; (b) shows accuracy in red.
Figure 10: Bounds from Theorem 1 in semilog scale for no = 60 as the total number of neuronsincrease by evenly distributing such neurons in 1 to 4 layers: (a) actual values showing overallimpact of more depth; and (b) ratio by sum over all layers showing local impact of particular depths.
