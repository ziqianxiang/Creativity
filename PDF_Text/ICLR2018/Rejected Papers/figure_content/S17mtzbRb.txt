Figure 1: An RNN model with two fully-connected layers for binary classification of time series2	Related WorkIn the past few years, a substantial amount of work has been dedicated to learning a better repre-sentation of the input data that can be either used in downstream tasks, such as KMeans clustering,or to improve generalizability or performance of the model. In general, these works can be dividedinto three categories: (1) approaches that introduce a new loss component that can be easily appliedto an arbitrary cost function (discriminative models), (2) approaches that require a complicated orcumbersome training procedure (discriminative models), and (3) probabilistic generative and/or ad-versarial models.
Figure 2: Number of samples for which the neurons on the y axis were active the most in a binaryclassification task on MNIST strokes sequences dataset. The classes 0-4 have the label 0, and theclasses 5-9 have the label 1. See the subsection 4.1 and subsection 5.2 for details.
Figure 3: Number of samples for which the neurons on the y axis were active the most in a binaryclassification task on the CIFAR-10 dataset. See the subsection 4.2 for details.
Figure 4: A CNN model used in the CIFAR-10 experiments3.2	Multilayer lossNote that the loss component Lsingle affects only the weights of the specific layer, as it operatesnot on the outputs of the layer but directly on its weights, similar to, for example, `2 regularization.
Figure 5: PCA visualizations of the learned representations on the MNIST strokes sequences dataset.
Figure 6: Number of clusters and the corresponding AMI score on the CIFAR-10 dataset6	ConclusionIn this paper, we propose two novel loss components that substantially improve the quality ofKMeans clustering, which uses representations of the input data learned by a given model. We per-formed a comprehensive set of experiments using two popular neural network architectures (RNNsand CNNs), and different modalities of data (image and text). Our results demonstrate that the pro-posed loss components consistently increase the Mutual Information scores by a significant margin,and outperform previously proposed methods. In addition, we qualitatively analyzed the represen-tations learned by the network by visualizing the activation patterns and relative positions of thesamples in the learned space, showing that the proposed loss components indeed force the networkto learn diverged representations.
