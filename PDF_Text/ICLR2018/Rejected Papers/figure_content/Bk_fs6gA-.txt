Figure 1: The operation of an LTMN processing one Binary LP instance. First the encoder receives asequence of constrains coefficients and produces a memory vector via a linear projection of encoderfinal hidden state. The memory controller receives this linear projection and decides whether tostore it in memory M or delete the current slot it points to. A control signal (sequence of costs) isthen passed to the decoder to produce un-normalized weights wt over the memory locations. A dotproduct operation between wt and M Finally the output ot at each time step is generated using thememory vector (or encoder final state), the decoder hidden state and the read vector.
Figure 2: Evaluation of LTMN on test sets of Binary LP instances with number of constrains inrange [1, 5] and number of variables up to 1505.4 Memory TestsTo understand the effectiveness of using an augmented long-term memory network for solving Bi-nary LP instances, we conduct tests to prove that long-term memory improves model results. Wecalculate the average costs for the same test set (10 variables and constrains between 1 and 5) but9Under review as a conference paper at ICLR 2018Table 3: Objective values generated by baseline solver and object values calculated using the sam-pled variables assignments from LTMN output probability distribution at different sampling temper-atures for large Binary LP instances.
