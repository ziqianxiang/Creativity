Figure 1: The ProblemCitationDBLPresponds to an edge between nodes i and j). Reordering the vertices (figure (b) below) gives a muchmore structured image for the same subgraph as in (a). Now, the potential to learn distinguishingproperties of the subgraph is evident. We propose to exploit this very observation to solve a basicgraph problem (see Figure 1). The datasets mentioned in Figure 1 are discussed in Section 2.4.
Figure 2:	An image of a dog and a structured image of a Facebook graph sample vs their corre-sponding maximally specific classification vectors returned by Caffegives a description which doesnâ€™t really have intuitive meaning (Figure 2b). We map the Caffe-descriptions to vectors. This allows us to compute similarity between network images using thesimilarity between Caffe description-vectors (see Section 2).
Figure 3:	Classification of structured image embeddings using Deep LearningWe compared performance with several methods, including graph kernel classifiers and classifiersbased on standard topological features of the graph. Our image representation performs best.
Figure 4:	Classification of structured images using the classification vectors obtained from CaffeImages, not graphs, are passed through the Caffe deep neural network, and as we shall show, onecan get good performance from as little as 10% of the training data used in the ab initio machinelearning approach. It is quite stunning that such little training data together with un-tweaked transferlearning from a completely unrelated domain can perform so well. The reason is that our image rep-resentation provides very structured images (human-recognizable) for real world networks. Thoughthese images are not traditional images like those used in training Caffe, Caffe still maps the differ-ent structured images to different distributions over its known classes, hence we are able to transferthis knowledge from Caffe to graph classification.
Figure 5: Structured images and the top principal component. (a) Citation; (b) Facebook; (c) RoadNetwork; (d) Web; (e) Wikipedia; (f) Amazon; (g) DBLP; (h) Terrorist Network; (i) Gowalla3	Experimental Setup and ResultsIn this section we will describe our experimental setup and present the results we obtained from thetwo approaches we have described in the earlier sections.
Figure 6: Classification accuracy increases as n increasesWe would like to make a note about DCNN. Out of the 4 neural network classification models wehave used in this work, DCNN is the only one that takes a graph as an input instead of images.
Figure 7: Accuracy of transfer learning. (a)-i: Terrorist Net. vs Facebook, (a)-ii: Citation vs DBLPand (a)-iii: Web vs Wiki and (b): multi-way classification.
Figure 8: Performance on hybrid datasetsNodes	Case 1	Case 2	Case 3	Case4^^8-	25%	10%	40%	10%16	25%	20%	30%	40%32	25%	30%	20%	40%64	25%	40%	10%	10%A.4 Transfer Learning Results Contd.
Figure 9: Classification between similarly themed networksTable below shows that our approach does not doquite as well when it comes to multi-way classifica-tion. While multi-way classification is challengingin general, our particular approach of using the ma-jority rule may be somewhat more impacted sincenow the correct class has to outnumber several otherclasses.
Figure 10: Accuracy vs nbhood. size (k)Data	Prc.	Rec.	F1Wiki	0.71	0.79	-075Web	0.66	0.73	0.69DBLP	0.57	0.59	0.58Terrorist Net.	0.48	0.70	0.57Citations	0.36	0.18	0.24Facebook	0.84	0.66	0.74Accuracy		61%	Figure 10 shows the variation of accuracy results when k is varied in steps of 8 from k = 7 tok = 47, with the base case k = 15 used for the tabulated results above included for comparison.
