Figure 2: Structured latent variable model of the multi-agent policy (actor, left) and instance of themulti-agent actor-critic interacting in the environment E (right). The joint policy contains two stackedlayers of stochastic latent variables (red), and deterministically receives states and computes actions(green). Global variables λ are shared across agents. On the right, a neural network instance of theactor-critic uses the reparametrization trick and receives the environment state, samples actions fromthe policy for all agents and computes value functions V • .
Figure 3: Train-time cumulative terminal reward for N agents in (N, M, 1, T) Hare-Hunters(upper, T = 2000) and Stag-Hunters (lower, T = 1000) on a 50 × 50 gridworld, for 10-vs-10or 20-vs-20 agents; randomly moving or fixed preys. Average, minimal and maximal rewards for thebest 5 runs for each model are shown. MACE accumulates increasingly higher rewards compared tothe baselines, by 1) achieving higher terminal reward per episode and 2) finishing episodes faster(see Figure 4). For 10-10 Stag-Hunters with frozen prey, average reward per-episode is 4.64(Cloned), 6.22 (Shared), 6.61 (MACE) after 1 million samples. For more, see the Appendix.
Figure 4: Train-time episode lengths during 1 million steps for 10-vs-10 Hare-Hunters (left) andStag-Hunters (right), with fixed preys. MACE (orange) finishes an episode successfully beforethe time limit more often than the baselines (Cloned (blue) and Shared (yellow)).
Figure 5: Predators (red) and prey (green) during training for 2v2 Hare-Hunters for 100 episodes.
Figure 6: Visualization of MACE for a (10, 10, 1, 1000) Hare-Hunters game in a 30 × 30 world.
Figure 7: Train-time cumulative terminal reward in (N, M, 1, 1000) Hare-Hunters (upper) andStag-Hunters (lower) on a 50 × 50 gridworld, for 10-vs-10 or 20-vs-20 agents; randomly movingor fixed preys. Average, minimal and maximal rewards for the best 5 runs for each model are shown.
