Figure 1: Pruning of complete features for the GFCNN architecture trained on MNIST (top panel)and CIFAR100 (bottom panel). Top row shows sorted feature importance values for every layeraccording to three different metrics at the end of training. Bottom row illustrates accuracy loss whenremoving feature by feature in ascending order of feature importance.
Figure 2: Exemplary GFCNN network expansion on MNIST and CIFAR100. Top panel shows onearchitectureâ€™s individual layer expansion; bottom panel shows the evolution of total parameters forfive runs. Itis observable how different experiments converge to similar network capacity on slightlydifferent time-scales and how network capacity systematically varies with complexity of the dataset.
Figure 3: Mean and standard deviation of topologies as evolved from the expansion algorithm fora VGG-E and VGG-E all-convolutional architecture run five times on MNIST, CIFAR10 and CI-FAR100 datasets respectively. Top panels show the reference architecture, whereas bottom showsautomatically expanded architecture alternatives. Expanded architectures vary in capacity withdataset complexity and topologically differ from their reference counterparts.
Figure 4: Left to right: Loss, train and validation curves for a GFCNN-all-conv trained on theCIFAR100 dataset. The dashed curve presents a reference architecture implementation whereas thesolid lines correspond to the behavior of an expanded network (Note that we have omitted the non-stable part of the expansion and only show the 200 stable epochs). It can be observed that lossand training accuracy improve by a large margin for the expanded architecture, whereas validationaccuracy benefits only slightly.
Figure 5: Mean and standard deviation of topologies as evolved from the expansion algorithm forthe shallow networks run five times on MNIST, CIFAR10 and CIFAR100 datasets respectively.
Figure 6: Mean and standard deviation of topologies as evolved from the expansion algorithm forthe VGG-A style networks run five times on MNIST, CIFAR10 and CIFAR100 datasets respectively.
Figure 7: Mean and standard deviation of topologies as evolved from the expansion algorithm forthe WRN-28 networks run five times on MNIST, CIFAR10 and CIFAR100 datasets respectively.
Figure 8: Architecture topologies for the two all-convolutional Alexnets of table 2 as evolved fromthe expansion algorithm on ImageNet.
