Figure 1: Our proposal of a principled robot learning pipeline. We used 3D motion controllersto collect human demonstrations of a task. Our reinforcement and imitation learning model lever-aged these demonstrations to facilitate learning in a simulated physical engine. We then performedsim2real transfer to deploy the learned visuomotor policy to a real robot.
Figure 2: Model overview. The core of our model is the deep VisUomotor policy, which takes thecamera observation and the proprioceptive feature as input and produces the next joint velocities.
Figure 3: Visualizations of the six manipulation tasks in our experiments. The left column showsRGB images of all six tasks in the simulated environments. These images correspond to the actualpixel observations as input to the visuomotor policies. The right column shows the two tasks withcolor blocks on the real robot.
Figure 4: Learning efficiency of our reinforcement and imitation model against baselines. Theplots are averaged over 5 runs with different random seeds. All the policies use the same networkarchitecture and the same hyperparameters (except λ).
Figure 5: Model analysis in the stacking task. On the left we investigate the impact on performanceby removing each individual component from the full model. On the right we investigate the model’ssensitivity to the hyperparameter λ that moderates the contribution of reinforcement and imitation.
Figure 6: Tiles show the representative range of diversity seen in the domain-randomized variationsof the colors, lighting, background, etc.
