Figure 1: Illustration of LSTM-based method3.1	The bi-LSTM LayerIn this research, we take advantage of bi-LSTM (bi-directional long short-term memory), to en-hance the ordinary RNN performance considering both forward and backward information and solvethe problem of the long-term dependencies. The updates rules of LSTM for each sequential inputx1, x2, ..., xt, ..., xT could be express as:it = sigmoid(Wixt + Uiht-1 + bi)	(1)ft = sigmoid(Wf xt + Uf ht-1 + bf)	(2)Ct = tanh(Wcxt + Ucht-I + bc)	(3)Ct = it Θ Ct + ft Θ ct-i	(4)ot = sigmoid(Woxt + Uoht-1 + bo)	(5)ht = ot Θ tanh(ct)	(6)where ht-1 is the hidden layer value of the previous states and the sigmoid and tanh functions in theabove equations are also used as activation functions:Sigmoid(x) = 1+eXp(-x)	⑺2tanh(x) =----------------- — 1	(8)()	1 + eχp(-2x)	()The weights (i.e. parameters) we need to train include Wi, Wf , Wc, Wo, Ui, Uf , Uc, Uo and biasvectors bi , bf , bc , bo. In this layer, there are four LSTM modules, constructing two bi-LSTM struc-
Figure 2: Illustration of an evaluation procedures using ranks and TOP-N index4.2	Baseline: Siamese LSTM with Google-translationAs Siamese LSTM is one of the deep learning-based models with the art-of-state performance on thesemantic text similarity problems. In this research, we make this model be a baseline by extendingthis model from monolingual domain to cross-lingual domain with the help of the Google Transla-tion services. We first translate all Japanese text into English version on both test and training databy using the google translate service 2 . Then we implement the Siamese LSTM model as describedin the original paper for Siamese LSTM Mueller & Thyagarajan (2016) with the help of the opensource code on the Github 3. The illustration of this baseline method regarding a two cross-lingualinput, we first translate the Japanese input into an English sentence using Google Translation ser-vice. Then, we can consider the cross-lingual task as monolingual one using so that we can applythe Siamese LSTM model for training as a baseline.
Figure 3: TOP-N along with Epoches for TEST-1S, TEST-2S, Short data, LSTM-based modelFigure 4: TOP-N along with Epoches for TEST-1L, TEST-2L, Long data, LSTM-based model8Under review as a conference paper at ICLR 2018ReferencesEneko Agirre, Carmen Banea, Daniel M Cer, Mona T Diab, Aitor Gonzalez-Agirre, Rada Mihalcea,German Rigau, and Janyce Wiebe. Semeval-2016 task 1: Semantic textual similarity, monolingualand cross-lingual evaluation. In SemEval@ NAACL-HLT,pp. 497-511, 2016.
Figure 4: TOP-N along with Epoches for TEST-1L, TEST-2L, Long data, LSTM-based model8Under review as a conference paper at ICLR 2018ReferencesEneko Agirre, Carmen Banea, Daniel M Cer, Mona T Diab, Aitor Gonzalez-Agirre, Rada Mihalcea,German Rigau, and Janyce Wiebe. Semeval-2016 task 1: Semantic textual similarity, monolingualand cross-lingual evaluation. In SemEval@ NAACL-HLT,pp. 497-511, 2016.
