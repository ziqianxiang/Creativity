Figure 1: Performance comparisons of the RNN based models on three UCR datasets.
Figure 2:	RNN models on MNISTModels	Hidden dimension	Number of parameters	Test accuracysvdRNN	256(m1, m2 = 16)	≈ 13k	97.6oRNN(Mhammedi et al. (2017))	256(m = 32)	≈ 11k	97.2RNN(Vorontsov et al. (2017))	128	≈ 35k	94.1uRNN(Arjovsky et al. (2016))	512	≈ 16k	95.1RC uRNN(Wisdom et al. (2016))	512	≈ 16k	97.5FC uRNN(Wisdom et al. (2016))	116	≈ 16k	92.8factorized RNN(Vorontsov et al. (2017))	128	≈ 32k	94.6LSTM (Vorontsov et al. (2017))	128	≈ 64k	97.3Table 2: Results for the pixel MNIST dataset across multiple algorithms.
Figure 3:	MLP models on MNIST with L layers nh hidden dimension8	ConclusionsIn this paper, we have proposed an efficient SVD parametrization of various weight matrices indeep neural networks, which allows us to explicitly track and control their singular values. Thisparameterization does not restrict the network’s expressive power, while simultaneously allowingfast forward as well as backward propagation. The method is easy to implement and has the sametime and space complexity as compared to original methods like RNN and MLP. The ability tocontrol singular values helps in avoiding the gradient vanishing and exploding problems, and as wehave empirically shown, gives good performance. Although we only showed examples in the RNNand MLP framework, our method is applicable to many more deep networks, such as ConvolutionalNetworks etc. However, further experimentation is required to fully understand the influence of usingdifferent number of reflectors in our SVD parameterization. Also, the underlying structures of theimage of Mk1,k2 when k1, k2 6= 1 is a subject worth investigating.
Figure 4: RNN models on the adding task with L layers and nh hidden dimension. The top plotsshow the test MSE, while the bottom plots show the magnitude of the gradient at the first layer.
Figure 5: RNN models on the Copying task with T time lag and nh hidden dimension.
