Figure 1: Left: A comparison of the relationship between average reconstruction L1 distance onthe MNIST training set and sample compression for both continuous latent variable and categoricallatent variable autoencoders. Right: Comparison of generative transfer learning performance usinga CNN teacher and student model on MNIST while using code sampling and recollection buffersampling.
Figure 2: An example illustration of our proposed three module architecture. First, the input obser-vation is encoded to a latent code that is stored in a recollection buffer. Later, a code is selected fromthe buffer and sent through the decoder to provide a recollection for which the reasoning modulemakes a prediction.
Figure 3: Left: Average test set L1 reconstruction distortion on incremental CIFAR-100 using anautoencoder with an effective incremental storage buffer size of 200. Right: Average test set L1reconstruction distortion on incremental CIFAR-100 using an autoencoder with 76 2d categoricallatent variables.
Figure 4: Retention of performance on CIFAR-100 after prolonged training on CIFAR-10. Wecompare lossy and lossless replay buffer strategies listed by their effective incremental buffer size.
Figure 5: An illustration of how our proposed recollection generator is used to produce recollectionsto transfer knowledge from a teacher neural network to a student neural network.
Figure 6: An illustration of how our proposed recollection generator is used to produce recollectionsthat are interleaved with real incoming examples for training continual lifelong learning models.
