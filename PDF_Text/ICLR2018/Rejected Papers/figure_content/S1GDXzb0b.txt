Figure 1: Showing the overall architecture of the proposed method. Firstly, model-free policy isupdated using reward estimation from next state mismatch which storing the samples in replaybuffer. These are used to update the differentiable dynamics model of the environment, whichprovides the error gradients for end-to-end model-based policy update from state trajectoriesΦ(^t+ι)Dynamics -Modelp(st+1∣st,at)(st>st+l} ~ τEModel based policy learning2.2	Model-based imitation learning from state trajectoriesConsider there are m sets of expert trajectory episode each consisting of T states, given as τE ={s0, s1 , ..., sn}, where n = mT . We assume that the trajectories in each episode are independentof each other. For the imitation learning problem, we wish to imitate an expert agent πE fromstate trajectories τE . we formulate a maximum likelihood problem for state trajectories given aparameterized model of the agent policy, given asθ* = arg min [-XX log P(St+1 |si：t ； θ)],	Q)θ	j=1 t=1where θ represents the parameter of the model. We assume that the random variables state(st),action(at) and next state st+1 form a directed graphical model as shown in figure 2 (b). Following
Figure 2: (a) Showing the toy obstacle avoidance environment used in our experiments, (b) Thedirected graphical model, which shows that next is dependent on current state and action, whilecurrent action is just dependent of the current state3.1	Robotics arm reacher in 2DWe use roboschool reacher(OpenAI (2017); Brockman et al. (2016) environment to simulate two-link robotic arm that can move in two-dimensional space. The desired task is to learn reaching agiven target from random starting configurations. The arm is controlled using angular torque valuesof both joints. We use state values consisting of angular position, angular velocity, the end effectorlocation of the robotic link and the position of the target. The robotic arm angles and angularvelocities were used as φ(st), which is the portion of state dependent on action. In this experiment,we assume that the true reward function is known and in addition, we have some state trajectoriesfrom the expert policy. The goal is to leverage these existing state trajectories to learn a betterpolicy in less number of steps. Reward signal, consisting of distance potential, electricity cost andthe penalty for stuck joint, which is the default reward specified for the environment, was used.
Figure 3: (a) Comparison of proposed method with model-free reinforcement learning methods,behavior cloning and proposed model-based method for Flappy birds. The model-based methodis shown to surpass the model-free method in test reward performance with much less iterations,although being upper bounded by behavior cloning results (b) Comparison of the proposed methodwith continuous model-free methods on roboschool reacher showing better convergence perfor-mance with less iterationsFor expert demonstration, we implemented a manually engineered policy that always avoids theobstacle. We used 1000 number of demonstrations containing only state trajectories to reach a targetwhile avoiding a single obstacle. Out of 1000 demonstrations, 800 are used for training and 200 forvalidation. We first learn the time series prediction of the next state, used to compute the heuristicreward based using prediction error as discussed in section 2.1. For model-based policy, we use aMLP with (64, 64) hidden units. We use the same policy network for both model-based and model-free policy. The dynamics model is also modeled as a neural network with a single hidden layer of8 units for both state and action input. We used a switching frequency of 5 between the model-freeand model-based updates. Using these setting for the proposed algorithm, we get a model-basedpolicy and the dynamics model as output.
