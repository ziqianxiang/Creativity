Figure 1:	Network architectures for factorization models. The white clouds (c≥>) represent deeplayers, for example a convolutional network for text features.
Figure 2:	Comparison of how factorization may use item descriptions features.
Figure 3: RMSE in Ratings Task as a Function of α1sampling, to see what effect this hyperparameter has on our performance. Intuitively, a low α1 willgreatly improve the quality of the ratings embeddings learned, since it has relatively few parametersand is otherwise sampled infrequently. At the same time, with low α1 the director feature will besampled less since itis one of the most complex features to learn, so the learned director embeddingsmay be of poorer quality. Figure 3 displays the results of our experiment, benchmarked against theperformance of Word2Vec’s CBOW algorithm in the prediction task. We also show as a baseline theRMSE of a random uniform variable over the range of possible ratings (0 to 10). As is evident fromthe plot, CBOW performs a bit better than a random prediction, but is also handily outperformedby Feat2Vec across all hyper-parameter settings. The algorithm’s performance does not seem verysensitive to the hyperparameter choice.
