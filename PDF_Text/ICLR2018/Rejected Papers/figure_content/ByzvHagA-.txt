Figure 1: When data is distributed along non-linear manifolds, a linear model cannot describe thedata well (left). However, with a non-linear model (right), it is possible to capture the variations ofthe data in a more reasonable way and unfold it into a compact orthogonal representation space.
Figure 2: In this figure we compare the amount of residual linear correlation after training the modelwith LΣ and L1 regularization respectively, measured in MAPC (left) and CVR (right). The firstpoint on each curve corresponds to λ = 0, i.e. no regularization, followed by 8 points logarithmicallyspaced between 0.001 and 1. All scores are averaged over 10 experiments using a different randomprojection (Ω).
Figure 3: The resulting dimensionality the coding layer after training the model with LΣ and L1regularization respectively, measured in TdV (left) and UD90% (right). The first point on each curvecorresponds to λ = 0, i.e. no regularization, followed by 8 points logarithmically spaced between0.001 and 1.All scores are averaged over 10 experiments using a different random projection (Ω).
Figure 4: Covariance matrix (left) and spectrum (right) of the hidden layers of a feed forward neuralnetwork trained with LΣ regularization to solve the XOR problem. Layer one (top) has learned toutilize unit zero and three while keeping the rest constant, and in layer two only unit two is utilized.
Figure 5: Covariance matrix (left) and spectrum (right) of the hidden layers of a feed forward neuralnetwork trained without regularization to solve the XOR problem.
Figure 6: Results from the convolutional autoencoder experiments on CIFAR-10: Left: CVR plottedagainst MSE on the CIFAR-10 test set, using LΣ regularization and L1 regularization, respectively.
