Figure 1: MSL image showing damage to the wheels from small rocks.
Figure 2: Information flow diagram for our architecture.
Figure 3: Value Iteration Moduleto the value iteration algorithm requires that this be a max pooling operation, however, in the nextsection we propose an alternative approach.
Figure 4: Four example maps and paths from our rock-world dataset. The star shows the goalposition with one example path to that goal.
Figure 5: Training curves show accuracy and loss on the test set against gradient step on the rock-world dataset. The model in orange uses a standard hard action model, the light blue uses ourproposed soft action model.
Figure 6: Vector fields show optimal policy planning results on our rock-world dataset. Learnedpolicy actions are shown in red. A few green arrows show places where the optimal actions from thetraining data deviated from our learned (red) policy. Note how in the map on the right some regionsdo not show valid policies that converge to the goal (bottom-right and bottom-left corners). This isan artifact of fixing the number of iterations (i.e., k) in VI. When k is too small, information aboutthe goal location cannot propagate to the whole map.
Figure 7: (left) An example rock-world obstacle map. (middle) The corresponding reward map aftertraining. The goal position is the brightest square on the map. (right) Value map produced by theSVIN algorithm.
Figure 8: A selection of path segments from our MSL drive dataset.
Figure 9: Accuracy training curve on the MSL dataset.
