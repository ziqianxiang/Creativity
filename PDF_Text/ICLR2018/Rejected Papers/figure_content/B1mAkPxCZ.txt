Figure 1: Overview of our framework. We extract image feature and project it to the semanticspace using the encoder. After augmenting data in semantic space, we use the decoder to obtain thecorresponding image feature for augmented semantic feature. Both real and augmented data are usedto train the classification model.
Figure 2: (a) Auto-encoder Network. We use fully connected layers and the number indicatesthe amount of neurons. Each fully connected layer is followed by non-linear kernel RELU. (b)Augmenting data in semantic space using auto-encoder. The training image (in blue box) featureis mapped to the semantic space (blue text), where nearest neighbor (red text) are searched. Theneighboring semantic features are then projected back to the image feature domain as the augmentedfeatures (red box). Noted that for purpose of visualization, we demonstrate with images and wordswhich are actually feature vectors.
Figure 3: Example images from ShapeNet Multiview dataset. As a reflection of the real indoorscene, the categories are chosen to cover various types of furniture, whereas including fine-grainedcategories as well.
Figure 4: Visualization of the classification results. Each training image is followed by threetesting images that are misclassified as the categories below before the augmentation, which are latercorrectly classified after the augmeantation. These testing images are visually different with the onlytraining data, ambiguous with other categories, and thus hard to recognize. Our augmentation canhelp to correctly recognize them.
Figure 5: (a) Confusion matrix on AWA before (top) and after (bottom) data augmentation.
