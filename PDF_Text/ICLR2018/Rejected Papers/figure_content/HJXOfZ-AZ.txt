Figure 1: Examples of interpretable local codes found in a distributed network. Left: a selectivelyon unit with a selectivity of 〜+0.12; Right: selectively off unit with a selectivity of 〜一0.2. Redcircles belong to a single category, blue stars are all the members of all other categories, the x-axisis the activation of a hidden layer neuron (HLN) and points are jittered randomly around 1 on they-axis for ease of viewing. There is a clear separation between activations for the class depicted inred (A) and all other activations (not-A), thus examination of the activations of these units wouldreveal the presence or absence of the red class.
Figure 2: Schematic for building a random code with known properties. Black circles representones, white circles represent zeros. Class prototypes are made (P1, P2, and P3) with length LP; thenumber of prototypes are np , which is three in this example, their weight is four, with a sparsenessnumber, Sp of 3. Random vectors, Rχ, are made, as shown, these have length LR and there arenR of them; they have weight, WR of two and sparseness number, Sr, of 1. To assemble a newcodeword, a prototype is chosen, this example, P2, and ‘perturbation’ errors are applied, in thisexample, the perturbation rate is 1, soa single one is turned to a zero in the modified prototype (P2).
Figure 3: Input data that is few in number and sparse exhibits more local codes. Left: the number oflocal codes (l.C.s) against the number of HLNs, n∏LN for different numbers of training examples(nx). Right: The effect of changing the sparseness of the random blocks of the vector, Sr. Asthe prototype vector in all these cases has a sparseness of 1 /10, the weight of the random WR andprototype, WP, parts of the codeword and the codewords are 500bits long, note that purple: SX =0.2, wr=50, wp = 50; green: SX = 0.3, wr=100, WP = 50; black dashed: SX = 0.4, wr=150,wp = 50 Gray dashed and dot-dashed lines are drawn at n∏LN=500 and 1000 respectfully.
Figure 4: Network architecture parameters can drastically effect the emergence of local codes. Left:Switching from sigmoidal HLNs with a sigmoidal activation function to a rectified linear units(ReLU) neurons; Right: Switching from a distributed to a 1-hot output encoding. Note that theblack dashed data is the same as in figure 3. Switching to ReLU gives slightly more LCs, likely dueto the fact that ReLUs train quicker and all experiments were stopped after 45,000 epochs. Using a1-hot output code drastically reduces the need for local codes to emerge in the hidden layer.
Figure 5: Perturbing the prototype part of the code word decreases the drive to learn local codes.
Figure 6: Increasing dropout increases the number of local codes. Probability density function(PDF), left, and cumulative probability density function (CDF), right, of the number of local codesthat emerge in repeated neural networks trained with dropout, percentage of the hidden layer neuronsdropped out given in the legend. As the dropout percentage increases, generally, the mean andrange of local codes found increases, suggesting that localized encoding by the network offers someadvantage against noise.
