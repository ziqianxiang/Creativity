Figure 1: An illustration of word2net. (a) In word2net, each term V is represented by a neural networkwith weights Èôê.The word network predicts the probability of a target word (increase, shownin blue) from its context (green). The input is the sum of the context vectors and the output is theoccurrence probability of the target. (b) Word2net can incorporate syntactic information by sharing anentire layer (orange) between words with the same pos tag (noun in this case). (c) The fitted word2netwith pos sharing can be queried for semantic similarities of the word networks for each word/tag pair.
