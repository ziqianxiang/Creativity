Figure 1: Illustration of BHNs (second and third) and a traditional non-Bayesian DNN (first) on the toyproblem from Blundell et al. (2015). In the second subplot, we place a prior on the scaling factor g and inferthe posterior distribution using a BHN, while in the third subplot the hypernet is used to generate the wholeweight matrices of the primary net. Each shaded region represents half a standard deviation in the posterior onthe predictive mean. The red crosses are 50 examples from the training dataset.
Figure 2: Learning the identity function With an overparametrized network: y = a ∙ b ∙ x. This Parametriza-tion results in symmetries shown by the dashed red lines, and the Bayesian hypernetwork assigns significantprobability mass to both modes of the posterior (a = b = 1 and a = b = -1).
Figure 3: Box plot of performance across10 trials. Bayesian hypernets (BHNs) withinverse autoregressive flows (IAF) consis-tently outperform the other methods.
Figure 4: Active learning: Bayesian hypernets outperform other approaches after sufficient acqui-sitions When Warm-starting (left), for both random acquisition function (top) and BALD acquisitionfunction (bottom). Warm-starting improves stability for all methods, but hurts performance for otherapproaches, compared With randomly re-initializing parameters as in Gal et al. (2017) (right). Wealso note that the baseline model (no dropout) is competitive With MCdropout, and outperforms theDropout baseline used by (Gal et al., 2017).9 These curves are the average of three experiments.
Figure 5: Adversary detection: Horizontal axis is the step size of the FGS algorithm. While accuracy dropswhen more perturbation is added to the data (left), uncertainty measures also increase (first row). In particular,the BALD and Mean STD scores, which measure epistemic uncertainty, are strongly increasing for BHNs, butnot for dropout. The second row and third row plots show results for adversary detection and error detection(respectively) in terms of the AUC of ROC (y-axis) with increasing perturbation along the x-axis. Gradientdirection is estimated with one Monte Carlo sample of the weights/dropout mask.
Figure 6: Histogram of Pearson correlation coefficient p-values (left) and a scatter matrix (right) of sam-ples from a hypernet approximate posterior. We see that the hypernet posterior includes correlations betweendifferent parameters. Many of the p-values of the Pearson correlation test are below .05.
Figure 7: Adversary detection with 32-sample estimate of gradient.
