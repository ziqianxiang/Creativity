Figure 1: Percent change in perplexity on 10 test sentences containing new word, plotted vs. thenumber of training sentences, across four different words, comparing optimizing from three differentstarting points to centroid and training with the word baselines. Averages across 10 permutations areshown in the dark lines, the individual results are shown in light lines. (Note that the full trainingwith the word was only run once, with a single permutation of all 10 training sentences.)Approach・ Centroid.Optimizingfrom current.Optimizingfrom centroid.Optimizing■ from zero.Full trainingwith the word2.1	Task, Model, and ApproachThe framework we have described for updating embeddings could be applied very generally, but forthe sake of this paper we ground it in a simple task: predicting the next word of a sentence based onthe previous words, on the Penn Treebank dataset (Marcus et al., 1993). Specifically, we will use the(large) model architecture and approach of Zaremba et al. (2014), see Appendix B.1 for details.
Figure 2: Comparing full training with the word, centroid, and optimizing from the centroid ap-proaches on both the new word dataset and the full test corpus (to assess interference), while using100 negatively sampled sentences for replay. When using a replay buffer, learning new words doesnot interfere substantially with prior knowledge.
Figure 3: Comparing change in perplexity on the new word test set when optimizing the inputembedding, output embedding, or both on either 1 or 10 sentences containing the new word. Lightlines are 10 independent runs, dark lines are averages.
Figure 4: Percent change in perplexity on 100 new words from applying the centroid, optimizingfrom the centroid, and full training with all words. Ten sentences containing the new word were usedin training and 10 were used in testing. Large solid dots indicate the change in the mean, smaller dotsindicate the change for individual words.
Figure 5: Interference With prior knowledge caused by naive use of our approach11Under review as a conference paper at ICLR 20182- -1 O(％) EIEPIS ①一 mH∩- UoAI×①-Red U - SUEqOCentroid	Optimizing	Full trainingfrom centroid	with all wordsApproachFigure 6: Percent change in perplexity on new word test data - comparing 10-Shot learning across100 different words.
Figure 6: Percent change in perplexity on new word test data - comparing 10-Shot learning across100 different words.
Figure 7: Similarity of representational similarity: how correlated are the similarity structuresgenerated by the different methods with the similarity structure produced by training with the word?E	Other related workA commenter on a draft of this paper also noted that Herbelot & Baroni (2017) pursued relatedquestions and independently came to some of the same conclusions we reached. However, webelieve our results improve upon their work in several important ways. First, while they wereonly able to show benefits on training from definitions of a word, we show benefits of learningfrom a word in context. Furthermore, they only evaluated on embedding similarity, while we showbehaviorally relevant improvements. This is important, because one conclusion from our results ofour embedding similarity analyses in Appendix D show that dissimilar embeddings may neverthelessoffer comparable performance. Finally, we explore in depth the effects of varying data parameterslike the number of training sentences, and analyze where the improvements are occurring, both fromthe perspectives of parameters and outputs.
