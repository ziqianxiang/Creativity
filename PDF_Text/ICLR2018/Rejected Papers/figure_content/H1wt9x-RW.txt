Figure 1: A visualization of the interaction between T and S. At each step T takes in the true concept and S'slast estimate of the concept and outputs an example for S. Then S outputs its new estimate of the concept.
Figure 2: Rule-based concepts. The black rectangle is the ground-truth concept and the blue dashed rectangleis student's output after each example. Left: Thejoint optimization has no clear interpretable strategy. Right:Under BR optimization T learns to give opposite corners of the rectangle.
Figure 3: Probabilistic concepts. T picks examples at different modes more consistently than the randompolicy, which picks examples near the same mode half of the time. Example are visualized by length of lines.
Figure 4:	Rule-based concepts. T’s examples arecloser to oppposite corners of the rectangles than ran-domly generated or jointly trained examples.
Figure 5:	Probabilistic concepts. T’s examples arecloser to the two modes than randomly generated orjointly trained examples.
Figure 6: Boolean ConcePts. Possible example images.
Figure 7: Boolean concepts. Examples for theconcept “red”. Left: The concept “red with bor-der” and “red” are consistent with the randomexamples. Right: Only the true concept “red” isconsistent with T's examples.
Figure 8: Boolean concepts. T matches the intu-itive strategy 87% of the time, compared to 36% forrandom, and 0% forjoint.
Figure 9:	Hierarchical concepts. An example sub-tree. T's strategy is to give two nodes whose lowestcommon ancestor is the target concept. For example,to teach ape T could choose to give an orangutanimage and a siamang image.
Figure 10:	Hierarchical concepts. T learns toperfectly match the intuitive strategy for hierarchicalconcepts, but the joint optimization matches theintuitive strategy less than random examples.
Figure 11: Probabilistic concepts. Humans learnedthe correct distribution over concepts better than hu-mans given random examples.
Figure 12: Boolean concepts. Humans learned toclassify test images better through examples from T.
