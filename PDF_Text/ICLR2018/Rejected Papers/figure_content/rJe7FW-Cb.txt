Figure 1: The proposed mechanism. Feature maps at different levels are processed to generatespatial attention masks and use them to output a class hypothesis based on local information and aconfidence score (C). The final prediction consists of the average of all the hypotheses weighted bythe normalized confidence scores.
Figure 2: Scheme of the submodules in the proposed mechanism. (a) depicts the attention heads, (b)shows a single output head.
Figure 3: Attention masks for each dataset: (a) dogs, (b) cars, (c) gender, (d) birds, (e) age, (f)food. As it can be seen, the masks help to focus on the foreground object. In (c), the attention maskfocuses on ears for gender recognition, possibly looking for earrings.
Figure 4: Ablation experiments on Cluttered Translated MNIST.
Figure 5: Samples from the five fine-grained datasets.
Figure 6: Test accuracy logs for the five fine-grained datasets. As it can be seen, the augmentedmodels (WRNA) achieve higher accuracy at similar convergence rates. For the sake of space weonly show one of the five folds of the Adience dataset.
