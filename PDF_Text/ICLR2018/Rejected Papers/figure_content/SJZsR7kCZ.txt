Figure 1: Three stage pipeline: pruning, weight sharing and quantization.
Figure 2: Weight distribution of two different dense layers of LeNet architecture.
Figure 3: Initial cluster centers by random andlinear centroid initializationsFigure 4: Linear initialization of clusters centersbefore and after codebook pruningWeight Sharing limits the number of effective weights to store by finding the weight values thatcould be shared across multiple connections without affecting the accuracy of the network. Weightscan be shared, first, only within a layer, that is finding shared weights among the multiple connec-tions within a layer and second, across all the layers, that is, finding shared weights among multipleconnections across all the layers. This can be done using clustering. The idea is that all the weightsthat belong to one cluster share the same weight value, partitioning the n original weights into kclusters such that n >> k. The performance of the network depends upon the quality of the clus-tering algorithm. The value of the cluster’s centroid is assigned to all the weight values within thatcluster. So at the end we need to store only the centroid values to represent the weight values of thenetwork instead of storing all the weight values individually. We examine two different clustering al-gorithms: mean shift clustering and k-means. K-means is very sensitive to the initial position of thecluster centers (Celebi et al., 2013), so we examine two different initialization methods: random andlinear initialization (Han et al., 2015). The stepwise algorithm for k-means with linear initializationis illustrated in algorithm 3.
Figure 4: Linear initialization of clusters centersbefore and after codebook pruningWeight Sharing limits the number of effective weights to store by finding the weight values thatcould be shared across multiple connections without affecting the accuracy of the network. Weightscan be shared, first, only within a layer, that is finding shared weights among the multiple connec-tions within a layer and second, across all the layers, that is, finding shared weights among multipleconnections across all the layers. This can be done using clustering. The idea is that all the weightsthat belong to one cluster share the same weight value, partitioning the n original weights into kclusters such that n >> k. The performance of the network depends upon the quality of the clus-tering algorithm. The value of the cluster’s centroid is assigned to all the weight values within thatcluster. So at the end we need to store only the centroid values to represent the weight values of thenetwork instead of storing all the weight values individually. We examine two different clustering al-gorithms: mean shift clustering and k-means. K-means is very sensitive to the initial position of thecluster centers (Celebi et al., 2013), so we examine two different initialization methods: random andlinear initialization (Han et al., 2015). The stepwise algorithm for k-means with linear initializationis illustrated in algorithm 3.
Figure 5: Weight distribution of second convolutional layer of LeNet before pruning (in the left) andafter pruning (in the right)We iteratively perform pruning and retraining using the same optimizer and learning rate on allnetwork weights until the current validation error of the network becomes less than the defined errortolerance. Our pruning runs for 42 iterations in total for the whole network. Figure 6 depicts theconvergence of error rate and accuracy rate after each iteration at initialized sparsity level. As itcan be seen, after a few iterations the accuracy of the network is not improving anymore so wedecreased the sparsity level by 1% in each layer and again start iteratively pruning and retrainingat new decreased sparsity levels. In table 2 we compare layer by layer pruning results between ourmethod and Han et al. (2015).
Figure 6: Convergence of error rate (a) and accuracy rate (b) after each iteration of pruning andtraining40 - 1/	-30 ------------1------------1-----------1-----------1-0	5	10	15	20Iterations(b) Convergence of accuracy rate after each iterationof pruning and trainingTable 2: Comparison of our pruning results on LeNet-5 with that of Han et al. (2015)Model	Layer	Percentage of remaining parameters [ours]	Number of remaining parameters [ours]	Percentage of remaining parameters (Han et al., 2015)	Number of remaining parameters (Han et al., 2015)LeNet-5	F2	5%	250	19%	950	^Tr1-	6%	-24000-	8%	32000	C2	15%	-3750-	12%	3000	C1	50%	90	66%	330Total		≈ 6.5% 一	28090	≈ 8%	36280	—Accuracy			99.27%				99.26 %		Storage Requirement		6 KB		44 KB	4.2 Results for FCN on cityscapesWe conducted second experiment on FCN (Yang et al., 2016a) performing semantic segmentationtask on the Cityscapes dataset. Our trained FCN8 has achieved the baseline IU of 64.75% and
Figure 7: Convergence of accuracy rate (a) and error rate (b) after each iteration of pruning andtraining5wa4>Ee-ledWelght.distributionSJZ3UJfflJedWelght—distributionFigure 8: Weight distribution of first convolutional layer of FCN before pruning (in the left) andafter pruning (in the right)Weight sharing To find the shared weights, we applied k-means clustering with linear initial-ization as it achieved substantially better results in the previous experiment. To find the optimumnumber of clusters, we evaluate the number of clusters ranging from 10 to 1200. Figure 9 depictsthe achieved IOU score corresponding to number of clusters found in this range. It can been seen9Under review as a conference paper at ICLR 2018Table 5: FCN compression results after pruningLayers of the network to be pruned	Total parameters before pruning	Achieved sparsity in each layer	Total non-zero parameters after pruning	Remaining non-zero weights after pruning in % (P)C1	1792	49%	913	5Γ%C2	36928	84%	5908	16%C3	73856	69%	22895	31%C4	147584	69%	45751	31%
Figure 8: Weight distribution of first convolutional layer of FCN before pruning (in the left) andafter pruning (in the right)Weight sharing To find the shared weights, we applied k-means clustering with linear initial-ization as it achieved substantially better results in the previous experiment. To find the optimumnumber of clusters, we evaluate the number of clusters ranging from 10 to 1200. Figure 9 depictsthe achieved IOU score corresponding to number of clusters found in this range. It can been seen9Under review as a conference paper at ICLR 2018Table 5: FCN compression results after pruningLayers of the network to be pruned	Total parameters before pruning	Achieved sparsity in each layer	Total non-zero parameters after pruning	Remaining non-zero weights after pruning in % (P)C1	1792	49%	913	5Γ%C2	36928	84%	5908	16%C3	73856	69%	22895	31%C4	147584	69%	45751	31%C5	295168	69%	91502	31%C6	590080	70%	177024	30%C7	590080	69%	182924	31%C8	1180160-	79%	247833	21%C9	-2359808	81%	448363	19%C10	-2359808	84%	377569	16%
Figure 9: Number of clusters vs IOU scorethat as the number of clusters increases the IOU score is improving, however, the improvement isvery slow. Due to time constraints, we did not continue with weight sharing and directly appliedquantization to the non-zero weights after pruning.
