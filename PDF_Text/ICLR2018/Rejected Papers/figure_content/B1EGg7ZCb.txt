Figure 1: Example environment with agents and customersTable 1: Event Reward StructureEvent	RewardAgent picks up customer	+1Agent collides with obstacle or travels outside the map -1A customers wait time runs out	-1 to all agents in vicinityStandard Movement (E.g. left, right)	-0.1Agent chooses Stay in charging location (Charging)	+0.1Agent chooses Stay in open road (Parking)	-0.05Agents energy reaches 0	-10essential to incentive agents to find the quickest path to the customers. Another important rewardstructure decision was the small positive reward for charging. We found that there was not enoughsignal to just have a large negative penalty for losing all of its energy. We had to incentivize being inthat charging station space without giving a strong enough reward to detract from the agents pickingup the customers.
Figure 2: Customers Fulfilled vs. Game LengthWe can see that as time (game length) increases the models reward begins to diverge from thebaseline. This shows that the policies that the two model agents are performing more efficientlythan the baselines. We inspected the individual games that were resulting and found that the agentsdid indeed perform a dived and conquer technique. When randomly placed on the same side of themap, one agent would take the initiative and travel to the other side of the map. When both agentsstarted on different sides, they would just stay there and pick up their respective customers. This6Under review as a conference paper at ICLR 2018simple example, successfully demonstrates that an agent can create a policy based on the other agentin the environment and the map structure to improve its overall expected future reward.
