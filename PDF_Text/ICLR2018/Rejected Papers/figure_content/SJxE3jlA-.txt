Figure 1: Pathological Example of Cosine Sim-ilarity: Although memory vector 1 is identical tothe completed (non-zero) portion of the key vec-tor, cosine similarity judges memory vector 2 tobe more similar to the key vector.
Figure 2: The three RL agent architectures evaluated on Concentration.
Figure 3: The MEM agent effectively utilizes episodic memory to solve the Concentration task:During training, counters keep track of the number of cards flipped by all 16 worker agents, as wellas the number of card-pair matches obtained by the agents. After every 100, 000 training steps thesecounts are used to calculate the matches per flip for that period, then reset to zero. This produces atrailing estimate (including exploratory actions) of the agentâ€™s performance on the task.
Figure 4: Multiple-copy task, 1 copy round. Five random runs where memory reuse is not required.
Figure 5: Multiple-copy task, 2 copy rounds. Five random runs where memory reuse is required.
Figure 6: Mini-SHRDLU results. The DNC results were copied from (Graves et al., 2016) . TheLSTM-A3C results were achieved by an RL agent with no external memory that read the constraintsfrom a simple array at its own pace. After training on a lesson, the LSTM agent was tested on thatlesson using 10,000 random problems. Direct numeric comparisons between these two performancecurves are not meaningful for various reasons: (1) The LSTM agent was advanced to the nextlesson only after its performance on the previous lesson had plateaued, which gave it more trainingproblems than used by DNC. (2) The LSTM agent was always given problems using the full 6 blockson the first 6 lessons, which is why those results are not shown. (3) Twelve of the subsequent lessonswere actually skipped during training, and the LSTM agent was tested on those lessons using themodel from the next trained lesson.
