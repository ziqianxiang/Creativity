Figure 1: Graphical representation of the process for producing the token embedding for the wordcat. The token embedding for the word corresponds to single row of the embeddings matrix, whichis then presented to a fully connected layer.
Figure 2: A graphical representation of the process for embedding the word cat using spellingembeddings. The position aware and position agnostic embeddings are selected and averaged fromtheir respective matrices. The results are then concatenated before being presented to an MLP.
Figure 3: Per-word perplexity on each test set after each training epoch. Comparing both the embed-ding style (token vs spelling) and whether or not dropout was included on the final embedding layer.
Figure 4: For each experiment, displaying the word embeddings of3 randomly selected words. The embeddings have 400 nodes whichhave been reshaped to 20 Ã— 20 and visualized as a heat map. Whiteindicates the most highly activated node, black the least.
Figure 5: For each experiment, shows the distribution of sparsity mea-surements over all trained word embeddings in the vocabulary.
