Figure 1: Illustration of the networks of P-DQN and DDPG (Hausknecht & Stone, 2016). P-DQNselects the discrete action type by maximizing Q values explicitly; while in DDPG, the discrete actionwith largest f , which can be seen as a continuous parameterization of K discrete action types, ischosen. Also in P-DQN the state and action parameters are feed into the Q-network which outputs Kaction values for each action type; while in DDPG, the continuous parameterization f, instead of theactual action k taken, is feed into the Q-network.
Figure 2: (a) An illustration of the map of a MOBA game, where there are three lanes connectingtwo bases, with three towers on each lane for each side. (b). The map of a solo game of King ofGlory, where only the middle lane is active. (c). A screenshot of a solo game of King of Glory,where the unit under a blue bar is a hero controlled by our algorithm and the rest of the units are thecomputer-controlled units.
Figure 3: Comparison of P-DQN and DDPG for solo games with the same hero Lu Ban. The learningcurves for different training workers are plotted in different colors. We further smooth the originalnoisy curves (plotted in light colors) to their running average (plotted in dark colors). In the 3 rows,we plot the average of episode lengths, reward sum averaged for each episode in training, and rewardsum averaged for each episode in validation, for the two algorithms respectively. Usually a positivereward sum indicates a winning game, and vice versa. We can see that the proposed algorithm P-DQNlearns much faster than its precedent work in our setting. (a) Performance of P-DQN. (b) Performanceof DDPGare sampled with probability of 0.05 each and the action “Retreat” with probability 0.005. For actionswith additional parameters, since the parameters are in bounded sets, we draw these parameters froma uniform distribution. Moreover, if the sampled action is infeasible, we execute the greedy policyfrom the feasible ones, so the effective exploration rate is less than . We uses 48 parallel workerswith constant learning rate 0.001 in training and 1 worker with deterministic sampling in validation.
