Figure 1: Example of a sequence-to-sequencetranslation model. The encoder receives the in-put characters ["g","o"]. Its internal state is passedto the decoder, which outputs the translation, i.e.,the characters of the word "aller".
Figure 2: Encoder modelFirst, each element xi of the input set X is mapped to a memory slot mi ∈ Rl using a mappingfunction finp (Eq. 1). We use a linear mapping as finp, the same for all i3 . Then, an LSTMnetwork (Hochreiter & Schmidhuber, 1997; Gers & Schmidhuber, 2000) with l cells performs nsteps of calculation. In each step, it calculates its new cell state ct ∈ Rl and hidden state ht ∈ Rlusing the previous cell- and hidden state ct-1 and ht-1, as well as the previous read vector rt-1,all of which are initialized to zero in the first step. The new read vector rt is then calculated asweighted combination of all memory locations, using the attention mechanism (Eq. 5). For eachmemory location i, the attention mechanism calculates a scalar value ei,t which increases based onthe similarity between the memory value mi and the hidden state ht , which is interpreted as a queryto the memory (Eq. 3). We set fdot to be a dot product. Then, the normalized weighting ai for allmemory locations is calculated using a softmax (Eq. 4). After n steps, the concatenation of cn,hnand rn can be seen as a fixed-size embedding of X4. Note that permuting the elements of X has noeffect on the embedding, since the memory locations mi are weighted by content, and the sum in Eq.
Figure 3: Decoder modelIn each step, the decoder LSTM calculates its internal cell state Ct and its hidden state ht (Eq. 6). Thecell- and hidden state are initialized with the cell- and hidden state from the embedding, producedby the encoder (Eq. 7 and Eq. 8). In the first step, the decoder also gets an additional input ro,which is set to be the last read vector of the encoder (Eq. 9). In all following steps, ^ is a vector ofall zeros. We calculate the decoder’s output ot at step t by using the linear function fout (Eq. 10).
Figure 4: Examples of sets inshapes data set(Besl & McKay, 1992) find closest points between two sets, and find a transformation that alignsthe second set to the first. Since we are only interested in the correspondence step, we notice itssimilarity to the stable marriage problem (Gusfield & Irving, 1989): We want to find matchingpairs Pi = {mani, womani} of two sets of men and women, such that there are no two pairs Pi,Pj where element mani prefers womanj over womani , and, at the same time, womanj prefersmani over manj .5 To solve this problem greedily, we can use the Gale-Shapely algorithm (Gale &Shapley, 1962), which has a run time complexity of O(n2) (see Algorithm 1)6. Since its solution ispermutation invariant in the set that proposes first (Gusfield & Irving, 1989)(p. 10), we consider theinput elements xi to be the men, and let them propose first. After the stable marriage step, wij = 1 ifxi is “engaged” to oj .
Figure 6: Smoothness of embeddings for sets with a sin-gle 2d-element. Each vertical slice through both graphsrepresents data for a single set. The top shows differentpositions of the element in in a 1 × 1 plane, the bottomthe corresponding embeddings. E.g., the first point fromthe left in the top diagram represents a set of size 1, witha single element at the coordinates [0, -0.5]. The pointsdirectly below visualize the values of the embeddingcorresponding to this set. Best viewed in color.
Figure 5: Meanreconstruction errorof reconstructed ele-ments in sets of dif-ferent size.
Figure 7: Heat maps of the location of the ith element in sets of size 8 with 2d-elements (decoderoutput). Darker shadings indicate more points at these coordinates.
Figure 8: Heat maps of the location of thefirst element in sets of various sizes n with 2d-elements (decoder output). Darker shadingsindicate more points at these coordinates.
