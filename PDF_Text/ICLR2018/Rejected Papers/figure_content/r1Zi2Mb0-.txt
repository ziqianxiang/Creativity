Figure 1: Tree search space for recurrent cells - shown is an illustration of a tree search spacespecifically designed for searching over LSTM-inspired cells. The figure was obtained from (Zoph& Le, 2017) with permission. Left: the tree that defines the computation steps to be predicted bycontroller. Center: an example set of predictions made by the controller for each computation stepin the tree. Right: the computation graph of the recurrent cell constructed from example predictionsof the controller.
Figure 2: General stack search space - shown is an illustration of how the stack changes overa sequence of operations. In this example, the controller predicts (copy0, linear, sigmoid, add) asa sequence of operations to be applied. The stack has a vector x as an initial input and producesx + Ïƒ (Wx) as an output.
Figure 3: Attention Mechanism - example of an attention-based NMT system as described in(Luong et al., 2015a). We highlight in detail the first step of the attention computation.
Figure 4: Stack Search Space for Attention - shown is the bilinear scoring function (Luong et al.,2015a) as an instance of the stack search space for attention. The controller predicts (linear, mul)as ops. All scoring functions end with a reduce op that turns vectors into scalars.
Figure 5: Effects of optimizers for NMT models - shown are training plots comparing two recur-rent cell searches, which are different in terms of optimizers used for NMT models, adam or sgd.
Figure 6: Effects of reward functions for attention searches - shown are similar plots to those inFigure 5 with two attention searches that differ in terms of reward functions used: one based oneBLEU while the other is based on perplexity (pp). For brevity, we only show plot for dev BLEU andtotal entropy.
