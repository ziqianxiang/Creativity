Figure 1: ReacherValue function. The value function Vπ can be conveniently fitted off-policy. Different from DDPG,the gradient estimator for Vπ is fitted using importance sampling, thus the objective is consistent.
Figure 2: State PredictionFigure 4: SwimmerFigure 3: Reward Prediction6.2	Evaluation Across DomainsIn this section, we present the comparison between MSCV and TRPO. For all tasks, both policy andvalue networks are two-layer neural networks with hidden size [64, 64]. We pre-train the dynamicsmodel for each task using the technique described in section 5.
Figure 4: SwimmerFigure 3: Reward Prediction6.2	Evaluation Across DomainsIn this section, we present the comparison between MSCV and TRPO. For all tasks, both policy andvalue networks are two-layer neural networks with hidden size [64, 64]. We pre-train the dynamicsmodel for each task using the technique described in section 5.
Figure 3: Reward Prediction6.2	Evaluation Across DomainsIn this section, we present the comparison between MSCV and TRPO. For all tasks, both policy andvalue networks are two-layer neural networks with hidden size [64, 64]. We pre-train the dynamicsmodel for each task using the technique described in section 5.
