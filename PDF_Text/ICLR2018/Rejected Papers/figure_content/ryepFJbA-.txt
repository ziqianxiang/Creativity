Figure 1: One hidden layer networks as G and D (MNIST). On the left, we plot inception scoreagainst time for vanilla GAN training and on the right, we plot the squared norm of discriminator’sgradients around real data points for the same experiment. Notice how this quantity changes before,during and after mode collapse events.
Figure 2: One hidden layer networks as G and D (MNIST). On the left, losses for both the playersare shown for vanilla GAN training and on the right, we added a regularization term to penalize thegradients of D(x) around real data points. Notice the improved stability.
Figure 3: One hidden layer networks as G and D (MNIST). On the left, inception score plot is shownfor vanilla GAN training and on the right, we added a regularization term to penalize the gradients ofD(x) around real data points. Notice how mode collapse is mitigated.
Figure 4: Swissroll experiment (different phases of training) - Vanilla GAN (top), WGAN-GP(middle), and DRAGAN (bottom). Real samples are marked orange and generated samples are green.
Figure 5: Comparison of modeling performance on CIFAR10Algorithm	ScoreWGAN	3.25WGAN-GP	5.99DRAGANg	6.11DRAGANd	6.90Vanilla GANg	6.3Vanilla GANd	6.99(b) Inception scoresTable 1: Summary of inception score statistics across 100 architecturesAlgorithm	Final score		Area under curve		QUaL score	Mean	Std	Mean	Std	TotalVanilla GAN	2.91	1.44	277.72	126.09	^9Σ5DRAGAN-	3.70	1.71	312.15	135.35	157.5WGAN-GP~~	3.49	1.30	300.09	100.96	-To demonstrate that our algorithm performs better compared to vanilla GAN training and WGAN-GP,we created 100 such instances of hard games. Each instance is trained using these algorithms onCIFAR-10 (under similar conditions for a fixed number of generator iterations, which gives a slightadvantage to WGAN-GP) and we plot how inception score changes over time. For each algorithm,we calculated the average of final inception scores and area under the curve (AUC) over all 100
Figure 6: Inception score plots for two divergence measures, demonstrating superior stability for ouralgorithm.
Figure 8: Modeling CIFAR-10 using DCGAN architecture.
Figure 9: Latent space walk of the model learned on MNIST using DRAGANFigure 10: Latent space walk of the model learned on CelebA using DRAGAN5.2 Additional Experiments5.2.1	One hidden layer network to model MNISTWe design a simple experiment where G and D are both fully connected networks with just onehidden layer. Vanilla GAN performs poorly even in this simple case and we observe severe modecollapses. In contrast, our algorithm is stable throughout and obtains decent quality samples despitethe constrained setup.
Figure 10: Latent space walk of the model learned on CelebA using DRAGAN5.2 Additional Experiments5.2.1	One hidden layer network to model MNISTWe design a simple experiment where G and D are both fully connected networks with just onehidden layer. Vanilla GAN performs poorly even in this simple case and we observe severe modecollapses. In contrast, our algorithm is stable throughout and obtains decent quality samples despitethe constrained setup.
Figure 11:	One hidden layer network to model MNIST - Inception score plots(a) Vanilla GAN(b) DRAGANFigure 12:	One hidden layer network to model MNIST - Samples5.2.2	8-Gaussians ExperimentWe analyze the performance of WGAN-GP and DRAGAN on the 8-Gaussians dataset. As it can beseen in Figure 13, both of them approximately converge to the real distribution but notice that inthe case of WGAN-GP, Dθ (x) seems overly constrained in the data space. In contrast, DRAGAN’sdiscriminator is more flexible.
Figure 12:	One hidden layer network to model MNIST - Samples5.2.2	8-Gaussians ExperimentWe analyze the performance of WGAN-GP and DRAGAN on the 8-Gaussians dataset. As it can beseen in Figure 13, both of them approximately converge to the real distribution but notice that inthe case of WGAN-GP, Dθ (x) seems overly constrained in the data space. In contrast, DRAGAN’sdiscriminator is more flexible.
Figure 13:	Comparing the performance of WGAN-GP and DRAGAN on the 8-Gaussians dataset.
Figure 14: Comparing performance of DRAGAN and Vanilla GAN training in the hard variations ofDCGAN architecture.
Figure 15: Comparing performance of DRAGAN and Vanilla GAN training using different objectivefunctions.
