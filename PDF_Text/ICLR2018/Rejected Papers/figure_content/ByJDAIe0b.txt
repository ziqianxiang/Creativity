Figure 1: Episodic memory architecture, each grey circle represents a neural network module. Inputstate (S) is given separately to the query (q), write (w), value (V) and policy (π) networks at eachtime step. The query network outputs a vector of size equal to the input state size which is used(via equation 1) to choose a past state from the memory (m1 ,m2 or m3 in the above diagram) tocondition the policy. The write network assigns a weight to each new state determining how likely itis to stay in memory. The policy network assigns probabilities to each action conditioned on currentstate and recalled state. The value network estimates expected return (value) from the current state.
Figure 2: (a) shows an instance of the secret informant problem with 3 actions (A ={up, f orward, down}) and 2 decision states. The start state uniquely contains all zeros. In thefinal 2 states of an episode (which we call decision states), the agent must select the right sequenceof actions to receive a +1 reward, any other action sequence gives reward 0. At all other states theforward action leads forward along the chain while other actions keep the agent in the same state.
Figure 3: Experiment with environment length 10, 1 decision and a 1 state memory. In the 1 statememory case the query module is unnecessary. (a) shows average write weight assigned to informa-tive states (•) and uninformative states (). (b) shows average return with episodic memory learner(•) and recurrent baseline ().
Figure 4: Experiment with environment length 10, 1 decision and a 3 state memory. In this case thequery module is necessarily. (a) shows average write weight assigned to informative states (•) anduninformative states(). (b)shows average return with episodic memory learner (•) and recurrentbaseline (). (c) shows the value of several relevant query vector elements in the decision state: theuninformative indicator (N), the informative indicator()and the first decision state identifier (•)(unnecessary here since there is only one decision state, but included for uniformity).
Figure 5: Experiment with environment length 10, 2 decisions and a 3 state memory. (a) showsaverage write weight assigned to informative states (•) and uninformative states (). (b) showsaverage return with episodic memory learner (•) and recurrent baseline (). (c) shows the value ofseveral relevant query vector elements in the first decision state: the uninformative indicator (N),the informative indicator(),the first decision state identifier (•), and the second decision stateidentifier (H). (d) shows the same thing but for the query generated in the second decision state.
Figure 6: Experiment with environment length 20, 2 decisions and a 3 state memory. (a) shows aver-age write weight assigned to informative states (•) and uninformative states (). (b) shows averagereturn with episodic memory learner (•), we did not train a recurrent baseline for this problem. (c)shows the value of several relevant query vector elements in the decision state. N corresponds tothe uninformative indicator, to the informative indicator, • to the first decision state identifier, Hto the second decision state identifier. (d) shows the same thing but for the query generated in thesecond decision state.
