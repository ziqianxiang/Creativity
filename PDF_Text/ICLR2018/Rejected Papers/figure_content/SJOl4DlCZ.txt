Figure 1: Outline of Classifier-to-Generator (C2G) Attack. The publisher trains classifier f fromtraining data Dtr and publishes f to the adversary. However, the publisher does not wish to leaktraining data Dtr and sample generating distribution ρt by publishing f. The goal of the adversary isto learn the publisher's private distribution ρt* for any t^ ∈ Ttr specified by the adversary providedmodel f, target label t and (unlabeled) auxiliary samples Dauχ.
Figure 2: Inference on the space of y. Here, we suppose the adversary has auxiliary samples labeledwith alphabets only and a probabilistic classification model that takes an image of a number andoutputs the corresponding number. The axis corresponds to an element of the probabilistic vectoroutputted by the classification model. For example, y9 = (f (x))9 denotes the probability that themodel discriminates the input image as ”9”. The green region in the figure describes the spannedby auxiliary samples in Daux. Daux does not contain images of numbers classified so that y9 = 1 ory8 = 1 whereas PreImageGAN generates samples close to ”9” by interpolating latent variables ofimages that are close to ”9” such as ”Q” and ”g”.
Figure 3: C2G attack against an alphanumeric classifier with changing the richness of the back-ground knowledge of the adversary targeting lowercase letters. The samples in the bottom row (”y:random”) are generated when y is randomly drawn from empirically estimated dYaux .
Figure 4: C2G attack against a numeric classification model with changing the richness of thebackground knowledge of the adversary targeting numeric letters. The samples in the bottom row(”y: random”) are images generated when y is randomly drawn from empirically estimated dYaux .
Figure 5: FaceScrub: The results of the C2G attack against face recognition in the mutuallyexclusive setting. We trained a face recognition model of 100 people (including Brad Pitt, KeanuReeves, Nicolas Cage and Marg Helgenberger), and evaluated the C2G attack where the classifica-tion model for the 100 people is given to the adversary while no face images of the 100 people arenot given. Generated samples are randomly selected, and we did not cherry-pick ”good” samples.
Figure 7: Images generated by the C2G attack When the target label is set as t^ = 0, 1, 2 anduniformly generated noise images are used as the auxiliary dataset. We used an alphanumeriC letterclassifier (label num:62) described in Sec. 5.2 as f for this experiment.
Figure 8: Interpolated images using PreImageGAN.
