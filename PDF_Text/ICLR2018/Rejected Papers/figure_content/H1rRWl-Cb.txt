Figure 1: Schematic representation of thephase diagram in the RD-plane. The distor-tion (D) axis measures the reconstruction er-ror of the samples in the training set. The rate(R) axis measures the relative KL divergencebetween the encoder and our own marginalapproximation. The thick black lines denotethe feasible boundary in the infinite modelcapacity limit.
Figure 2: Toy Model illustrating the difference between fitting a model by maximizing ELBO (middle column)vs minimizing distortion for a fixed rate (right column). Top: Three distributions in data space: the true datadistribution, p* (x), the model,s generative distribution, g(x) = Pz m(z)d(x∣z), and the empirical data recon-struction distribution, d(x) = pχθ Pzp(x0)e(z∣x0)d(x∣z). Middle: Four distributions in latent space: thelearned (or computed) marginal m(z), the empirical induced marginal e(z) = Px p(x)e(z∣x), the empiricaldistribution over z values for data vectors in the set X0 = {xn : zn = 0}, which we denote by e(z0) in purple,and the empirical distribution over z values for data vectors in the set X1 = {xn : zn = 1}, which we denoteby e(z1) in yellow. Bottom: Three K × K distributions: e(z|x), d(x|z) and p(x0|x) = Pz e(z|x)d(x0 |z).
Figure 3: Results on MNIST. (a) The best achieved rate distortion value for each run plotted on the RD-plane. We denote the particular model combination by the tuple (+/-, +/-, +/ - /v), depending on whetherwe use a simple (-) or complex (+) (or (v) VampPrior) version for the (encoder, decoder, marginal) respec-tively. (b) The same data, but on the skew axes of ELBO = R + D versus R.
Figure 4: We can smoothly move between pure autodecoding and autoencoding behavior in a single modelfamily by tuning β. (a) Sampled reconstructions from the -+v model family trained at given β values. Pairsof columns show a single reconstruction and the mean of 5 reconstructions. The first column shows the inputsamples. (b) Generated images from the same set of models. The pairs of columns are single samples and themean of 5 samples. See text for discussion.
Figure 5: Results on Omniglot. Otherwise same description as Figure 3. (a) Rate-distortion curves. (b) Thesame data, but on the skew axes of ELBO = R + D versus R.
Figure 6: We can smoothly move between pure autodecoding and autoencoding behavior in a single modelfamily by tuning β. (a) Sampled reconstructions from the -+v model family trained at given β values. Pairsof columns show a single reconstruction and the mean of 5 reconstructions. The first column shows the inputsamples. (b) Generated images from the same set of models. The pairs of columns are single samples and themean of 5 samples. See text for discussion.
