Figure 1: The fragility of feature-importance maps. We generate feature-importance scores, alsocalled saliency maps, using three popular interpretation methods: simple gradient (a), DeepLIFT(b) and integrated gradient (c). The top row shows the the original images and their saliency mapsand the bottom row shows the perturbed images (using the center attack with = 8, as describedin Section 3) and the corresponding saliency maps. In all three images, the predicted label has notchanged due to perturbation; in fact the network’s (SqueezeNet) confidence in the prediction hasactually increased. However, the saliency maps of the perturbed images are meaningless.
Figure 2: Intuition for why interpretation is fragile. Consider a test example xt ∈ R2 (blackdot) that is slightly perturbed to a new position xt + δ in input space (gray dot). The contoursand decision boundary corresponding to a loss function (L) for a two-class classification task arealso shown, allowing one to see the direction of the gradient of the loss with respect to the inputspace. Neural networks with many parameters have decision boundaries that are roughly piecewiselinear with many transitions. We illustrate that points near the transitions are especially fragile tointerpretability-based analysis. A small perturbation to the input changes the direction of NxL frombeing in the direction of x1 to being in the direction of x2, directly affecting feature-importanceanalyses. Similarly, a small perturbation to the test image changes which training image, whenup-weighted, has the largest influence on L, directly affecting exemplar-based analysis.
Figure 3:	Evaluation metrics vs subjective change We generate snapshots of the perturbed imageand its simple gradient saliency maps along with iterations of mass-center attack to visualize thegradual change in saliency map with its corresponding the rank-correlation and top-1000 intersectionmetrics.
Figure 4:	Comparison of adversarial attack algorithms on feature-importance methods. Across512 correctly-classified ImageNet images, we find that the top-k and center attacks perform similarlyin top-1000 intersection and rank correlation measures, and are far more effective than the randomsign perturbation at demonstrating the fragility of interpretability, as characterized through top-1000intersection (top) as well as rank order correlation (bottom). This is true for (a) the simple gradientmethod, (b) DeepLift, and (c) the integrated gradients method.
Figure 5: Gradient Sign attack on influence functions. An imperceptible perturbation to a testimage can significantly affect exemplar-based interpretability. The original test image is that of asunflower that is classified correctly in a rose vs. sunflower classification task. The top 3 trainingimages identified by influence functions are shown in the top row. Using the gradient sign attack, Weperturb the test image (with e = 8) to produce the leftmost image in the second row. Although theimage is even more confidently predicted as a sunflower, influence functions suggest very differenttraining images by means of explanation: instead of the sunflowers and yellow petals that resemblethe input image, the most influential images are pink/red roses. The plot on the right shows the in-fluence of each training image before and after perturbation. The 3 most influential images (targetedby the attack) have decreased in influence, but the influences of other images have also changed.
Figure 6:	Comparison of random and targeted perturbations on influence functions. Here, weshow the averaged results of applying random (green) and gradient sign-based (orange) perturbationsto 200 test images on the flower classification task. While random attacks affect interpretability, theeffect is small and generally doesn’t affect the most influential images. On the other hard, a targetedattack can significantly affect (a) the rank correlation and (b) even change the make-up of the 5 mostinfluential images. Even at the maximal level of noise, the changes to the perturbed images werevisually imperceptible, and prediction confidence was not significantly changed (the mean changewas < 1% for random attacks and < 5% for targeted attacks at the highest level of noise).
Figure 7:	Orthogonality of Predictionand Interpretation Fragile Directions (a)The histogram of the angle between thesteepest direction of change in feature im-portance and the steepest score change di-rection. (b) The distribution of the anglebetween the gradient of the loss functionand the steepest direction of change of in-fluence of the most influential image.
Figure 8: All of the images are classified as a airedale.
Figure 9: All of the images are classified as a damselfly.
Figure 10:All of theimages are classified as a lighter.
Figure 11:	Evaluation metrics vs subjective change in saliency maps To have a better sense ofhow rank order correlation and top-1000 intersection metrics are related to changes in saliency maps,snapshots of the iterations of mass-center attack are depicted.
Figure 12:	Center-shift results for three feature importance methods on ImageNet: As dis-cussed in the paper, among our three measurements, center-shift measure was the most correlatedmeasure with the subjective perception of change in saliency maps. The results in Appendix B alsoshow that the center attack which resulted in largest average center-shift, also results in the mostsignificant subjective change in saliency maps. Random sign perturbations, on the other side, didnot substantially change the global shape of the saliency maps, though local pockets of saliencyare sensitive. Just like rank correlation and top-1000 intersection measures, the integrated gradientsmethod is the most robust method against adversarial attacks in the center-shift measure .
Figure 13:	Results for adversarial attacks against CIFAR10 feature importance methods: ForCIFAR10 the mass-center attack and top-k attack with k=100 achieve similar results for rank cor-relation and top-100 intersection measurements and both are stronger than random perturbations.
Figure 14:	Further examples of gradient-sign attacks on influence functions. (a) Here we see arepresentative example of the most influential training images before and after a perturbation to thetest image. The most influential image before the attack is one of the least influential afterwards.
