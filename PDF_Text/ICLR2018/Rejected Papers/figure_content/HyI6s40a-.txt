Figure 1: (a) In this example, data points (denoted by green and blue squares) can be easily sepa-rated in one-dimensional space. Having extra dimensions adds ambiguity in choosing the pertinentdecision boundaries. For instance, all the shown boundaries (dashed lines) are sufficient to classifythe raw data with full accuracy in two-dimensional space but are not equivalent in terms of robust-ness to noise. (b) The rarely explored space (region specified by diagonal striped) in a learningmodel leaves room for adversaries to manipulate the nuisance (non-critical) variables and misleadthe model by crossing the decision boundaries. (c) In PCL methodology, a set of defender (check-point) modules is trained to characterize the data density distribution in the space spanned by thevictim model. The defender modules are then used in parallel to checkpoint the reliability of theultimate prediction and raise an alarm flag for risky samples.
Figure 2: Block diagram of the training procedure for devising parallel checkpointing modules.
Figure 3: (a) Illustration of the optimization objective in each defender module. (b) The distanceof legitimate (blue) and adversarial (red) samples from the corresponding centers Ci before, and(c) after realignment of data samples. In this example, we consider the LeNet3 model (LeCunet al. (1998a)) trained on MNIST dataset (the checkpoint is inserted in the second-to-last layer) andadversarial samples are generated by FGS attack with different perturbation levels.
Figure 4: (a) Illustration of the effect of security parameter (SP) on the detection policy. A high SPleads to a tight boundary which treats most samples as adversarial examples. (b) Example featuresamples in the second-to-last layer of LeNet3 trained for classifying MNIST data. The three axisshow the first three Eigenvectors corresponding to the PCA of the data. The first three dimensions areused for visualization purposes only, whereas, the actual data points belong to higher dimensionalspaces. (c) Latent feature samples of the same layer in the defender module after data realignment.
Figure 5: An input defender module is devised based on robust dictionary learning techniques toautomatically filter out test samples that highly deviate from the typical PSNR of data points withinthe corresponding predicted class (output of victim model).
Figure 6: Adversarial detection rate of the latent and input defender modules as a function ofthe perturbation level for (a) SP = 0.1%, (b) SP = 1%, and (c) SP = 5%. In this experiment, theFGS attack is used to generate adversarial samples and the perturbation is adjusted by changing itsspecific attack parameter Îµ .
Figure 7: Impact of security parameter on the ultimate performance of MRR module. The falsepositive rate is defined as the ratio of legitimate test samples that are mistaken for adversarial samplesby the defender modules. The true positive rate is defined as the ratio of adversarial samples correctlyclassified as malicious data point over the total number of malicious samples. Note that the scales forfalse positive and true positive axis are different. The false positive rate is computed by consideringlegitimate samples that are correctly classified by the victim model.
Figure 8: ROC performance curve of PCL methodology against FGS, JSMA, Deepfool, and Car-lini&WagnerL2 attacks. The diagonal line indicates the trajectory obtained by a random prediction.
Figure 9: Example adversarial confusion matrix (a) without PCL defense mechanism, and (b) withPCL defense and a security parameter of (1%). (c) Example adversarial samples for which accuratedetection is hard due to the closeness of decision boundaries for the corresponding classes.
Figure 10: Leveraging multiple parallel checkpoint modules can significantly improves the DLmodel prediction reliability while minimizing the number of false alarms.
Figure 11: True positive versus false positive rates in CIFAR10 (top row) and ImageNet (bottomrow) benchmarks for adversarial samples generated by FGS (Goodfellow et al. (2014)), JSMA (Pa-pernot et al. (2016a)), Deepfool (Moosavi-Dezfooli et al. (2016)), and Carlini&WagnerL2 (Carlini& Wagner (2017b)) attacks.
