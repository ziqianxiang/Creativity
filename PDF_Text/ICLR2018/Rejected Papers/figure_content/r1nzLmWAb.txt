Figure 1: An overview of video action segmentation problems with our proposed methodology, withexample video and action labels from GTEA Fathi et al. (2011) dataset.
Figure 2: The overall framework of our proposed TricorNet. The encoder network consists of ahierarchy of temporal convolutional kernels that are effective at capturing local motion changes. Thedecoder network consists of a hierarchy of Bi-LSTMs that model the long-term action dependencies.
Figure 3: Model variants of TricorNet.
Figure 4: Top: Example images in a sample testing video from 50 Salads dataset. Middle: Groundtruth and predictions from different models. Bottom: Two typical mistakes caused by visual similarity(left - peeled cucumber is similar to lettuce in color; right - hands will cover object when placing);TricorNet avoids the mistakes by learning long-range dependencies of different actions.
Figure 5: Left: Example images in a sample testing video from GTEA dataset, with ground truth andpredictions from different models. Right: Example images in a sample testing video from JIGSAWSdataset, with ground truth and predictions from different models. Our proposed TricorNet avoidssome classification mistakes when it can still perform a precise segmentation between actions.
