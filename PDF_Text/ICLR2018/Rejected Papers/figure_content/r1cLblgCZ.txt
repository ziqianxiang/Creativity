Figure 1: Recurrent auto-encoder model. Both the encoder and decoder are made up of multilayeredRNN. Arrows indicate the direction of information flow.
Figure 2: Effects of relaxing dimensionality of the output sequence on the training and validationMSE losses. They contain same number of layers in the RNN encoder and decoder respectively. Allhidden layers contain same number of LSTM neurons with hyperbolic tangent activation.
Figure 3: A heatmap showing eight randomly selected output sequences in the held-out validationset. Colour represents magnitude of sensor measurements in normalised scale.
Figure 4: A correlation matrix showing the pairwise correlation of all context vectors. Notice thenarrow band around the diagonal always has stronger correlation.
Figure 5: The first example. On the left, the context vectors were projected into two-dimensionalspace using PCA. The black solid line on the left joins all consecutive context vectors together asa trajectory. Different number of clusters were identified using simple K -means algorithm. Clusterassignment and the SVM decision boundaries are coloured in the charts. On the right, output dimen-sions are visualised on a shared time axis. The black solid line demarcates the training set (70%)and validation sets (30%). The line segments are colour-coded to match the corresponding clusters.
Figure 6: The second example. The sensor data is drawn from the same time period as the previousexample, only the output dimension has been changed to K = 2 where another set of gas pressuresensors were selected.
Figure 7: A simplified process diagram of the two-stage centrifugal compression train which islocated at a natural gas terminal.
Figure 8: Locations of key components around the centrifugal compressor.
