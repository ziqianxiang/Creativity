Figure 1: Proposed model. Given long text, the generator produces a shorter text as a summary.
Figure 2: Architecture of proposed model. The generator network and reconstructor network are aseq2seq hybrid pointer-generator network, but for simplicity, we omit the pointer and the attentionparts.
Figure 3: When the second arrested appears, the discriminator determines that this example camefrom the generator. Hence, after this time-step, it outputs low scores.
Figure 4:	Real example from our model in English Gigaword. The proposed method generatessummaries that capture the core idea of the article.
Figure 5:	In part (C-3), some words in the summary sentences are arranged in incorrect order.
Figure 6:	Semi-supervised results in English Gigaword. With the same amount of labeled data, theperformances of semi-supervised training are always better than supervised training.
Figure 7:	Real example from our model in English Gigaword. In part (E-2), due to transfer learning,the summary sentence begins with word he, which never appears in the English Gigaword summarysentences.
Figure 8:	Real example from our model in English Gigaword. The sentence grammar in part (C-3)is correct, but the semantics are incorrect.
Figure 9:	Real example from our model in English Gigaword.
Figure 10:	Real example from our model in English Gigaword.
Figure 11:	Real example from our model in English Gigaword.
Figure 12:	Real example from our model in English Gigaword.
