Figure 1: Graphical description of our proposed variational Bi-LSTM model during train phase(left) and inference phase (right). During training, each step t is composed of an encoder whichreceives both the past and future summary via ht-1 and bt respectively, and a decoder that generatesht-ι and bt which are forced to be close enough to ht-ι and bt using two auxiliary reconstructioncosts (dashed lines). This dependence between backward and forward LSTM through the latentrandom variable encourages the forward LSTM to learn a richer representation. During inference,the backward LSTM is removed. In this case, zt is sampled from the prior as in a typical VAE,which in our case, is defined as a function of ht-1.
Figure 2: Evolution of the average of log-likelihood during training of Variational Bi-LSTMs withand without using skip gradient and auxiliary costs on PTB and Blizzard.
Figure 3: Evolution of the bits per character on PTB validation with sampling latent variables zfrom N (0, I) during training or using a fixed vector which we set to be the mean of latent variables.
