Figure 1: Here we show a simple dependency tree. For the sake of understanding this paper, wedraw the reader’s eyes to two distinct classes of dependency: semantic roles (verb ordered →subject/object boy, ramen) and syntactic rules (noun girls → determiner the).
Figure 2: Break down the computation of the proposed models.
Figure 3: A pictorial illustration of having two separate attention (3b) and shared attention (3a) fromthe decoder to the encoder. The blue text represents the content vectors of the sentence and the purpletext represents the syntactic vectors. The number corresponding to each word is the probability massfrom decoder-to-encoder attention layer(s). Note, the reallocation of mass to both the subject andobject.
Figure 4: A visualization of attention distributions over head words. y-axis shows the head words.
Figure 5: Trees induced by the SA-NMT models. Roots and punctuations are ingored.
Figure 6: Visualization of gate norm. Best viewed in color.
