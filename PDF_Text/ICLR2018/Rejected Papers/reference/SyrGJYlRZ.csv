title,year,conference
 Some methods of speeding up the convergence of iteration methods,1964, USSRComputational Mathematics and Mathematical Physics
 Practical recommendations for gradient-based training of deep architectures,2012, InNeural networks: Tricks of the trade
 Neural networks: tricks of the trade,2003, Springer
 Deep learning of representations for unsupervised and transfer learning,2012, ICMLUnsupervised and Transfer Learning
 Stochastic gradient descent tricks,2012, In Neural networks: Tricks of the trade
 Practical bayesian optimization of machinelearning algorithms,2012, In Advances in neural information processing systems
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Convolutionalsequence to sequence learning,2017, arXiv preprint arXiv:1705
 The marginalvalue of adaptive gradient methods in machine learning,2017, arXiv preprint arXiv:1705
 On the difficulty of training recurrent neuralnetworks,2013, In International Conference on Machine Learning
 Fisher information,2016,
 Revisiting natural gradient for deep networks,2013, arXiv preprintarXiv:1301
 No more pesky learning rates,2013, ICML (3)
 Trained ternary quantization,2016, arXivpreprint arXiv:1612
 Intriguing properties of neural networks,2013, arXiv preprint arXiv:1312
 Hogwild: A lock-free approach toparallelizing stochastic gradient descent,2011, In Advances in Neural Information Processing Systems
 Dimmwitted: A study of main-memory statistical analytics,2014, PVLDB
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Building a large annotatedcorpus of english: The penn treebank,1993, Computational linguistics
 Sequential model-based optimization forgeneral algorithm configuration,2011, LION
 Project adam:Building an efficient and scalable deep learning training system,2014, In OSDI
 Simple adaptive momentum: new algorithmfor training multilayer perceptrons,1994, Electronics Letters
 Levenberg-marquardt algorithm with adaptive momen-tum for the efficient training of feedforward networks,2000, In Neural Networks
 Accelerated training of backpropagation networks by usingadaptive momentum step,1992, Electronics letters
 Optimal stochastic search and adaptive momentum,1994, In Advancesin neural information processing systems
 Estimating the hessian by back-propagatingcurvature,2012, arXiv preprint arXiv:1206
 Deep Learning,2016, MIT Press
 Omnivore: An optimizerfor multi-device deep learning on cpus and gpus,2016, arXiv preprint arXiv:1606
 Aggregated residualtransformations for deep neural networks,2016, arXiv preprint arXiv:1611
 Using the output embedding to improve language models,2016, arXiv preprintarXiv:1608
