title,year,conference
 Training a 3-node neural network is np-complete,1989, In Advances inneural information processing systems
 Theloss surfaces of multilayer networks,2015, In Artificial Intelligence and Statistics
 Identifying and attacking the saddle point problem in high-dimensional non-convexoptimization,2014, In Advances in neural information processing systems
 Qualitatively characterizing neural networkoptimization problems,2014, arXiv preprint arXiv:1412
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Deep learning without poor local minima,2016, In Advances in Neural InformationProcessing Systems
 Adam: A method for stochastic optimization,2015, In Proceedings ofthe International Conference on Learning Representations
 Deep learning,2015, Nature
 Piecewise convexity of artificial neural networks,2017, Neural Networks
 On the quality of the initial basin in overspecified neural networks,2016, InInternational Conference on Machine Learning
 Exponentially vanishing sub-optimal local minima in multilayerneural networks,2017, arXiv preprint arXiv:1702
 Local minima in training ofdeep networks,2016, arXiv preprint arXiv:1611
 benefits of depth in neural networks,2016, In 29th Annual Conference on LearningTheory
 Lecture 6,2012,5â€”RmsProp: Divide the gradient by a running average of itsrecent magnitude
