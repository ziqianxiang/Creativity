title,year,conference
 Adaptive subgradient methods for online learningand stochastic optimization,2010, In COLT
 Long short-term memory in recurrent neural networks,2001, Unpublished PhD dissertation
 Why momentum really works,2017, Distill
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, CoRR
 Adam: A method for stochastic optimization,2014, CoRR
 Communication efficient distributedmachine learning with the parameter server,2014, In NIPS
 Efficient mini-batch training forstochastic optimization,2014, In KDD
 End-to-end relation extraction using lstms on sequences and treestructures,2016, CoRR
 A stochastic approximation method,1951, Ann
 On the importance ofinitialization and momentum in deep learning,2013, In ICML
 Terngrad:Ternary gradients to reduce communication in distributed deep learning,2017, CoRR
 Small batch or large batch?: Gaussian walk withrebound can teach,2017, In KDD
 Adadelta: An adaptive learning rate method,2012, CoRR
