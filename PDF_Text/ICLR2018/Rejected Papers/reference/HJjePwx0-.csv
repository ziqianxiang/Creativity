title,year,conference
 Convergence rate analysis ofa stochastic trust region method for nonconvex optimization,2016, arXiv preprint arXiv:1609
 Practical gauss-newton optimisation for deeplearning,2017, arXiv preprint arXiv:1706
 Convex optimization,2004, Cambridge university press
 Gradient descent efficiently finds the cubic-regularized non-convexnewton step,2016, arXiv preprint arXiv:1612
 Entropy-sgd: Biasinggradient descent into wide valleys,2016, arXiv preprint arXiv:1611
 Stochastic optimization using a trust-regionmethod and random models,2015, Mathematical Programming
 Trust region methods,2000, SIAM
 Big batch sgd: Automated inferenceusing adaptive batch sizes,2016, arXiv preprint arXiv:1610
 Gradientdescent can take exponential time to escape saddle points,2017, arXiv preprint arXiv:1705
 Escaping from saddle pointsonline stochasticgradient for tensor decomposition,2015, In Conference on Learning Theory
 The modification of newtons method for unconstrained optimization by bound-ing cubic terms,1981, Technical report
 A linear-time algorithm for trust region problems,2016, MathematicalProgramming
 A fast learning algorithm for deep beliefnets,2006, Neural computation
 Mobilenets: Efficient convolutional neural networks formobile vision applications,2017, arXiv preprint arXiv:1704
 How to escapesaddle points efficiently,2017, arXiv preprint arXiv:1703
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Training neural networks with stochastic hessian-free optimization,2013, arXiv preprintarXiv:1301
 Gradient descent only con-verges to minimizers,2016, In Vitaly Feldman
 Fast exact multiplication by the hessian,1994, Neural computation
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Numerical optimization,1999, Springer Science
 Second-order optimization for non-convex machine learning: An empirical study,2017, arXiv preprint arXiv:1708
 Newton-type methods for non-convex optimization under inexact hessian information,2017, arXiv preprint arXiv:1708
 100-epoch imagenet training withalexnet in 24 minutes,2017, arXiv preprint arXiv:1709
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
