title,year,conference
 Dataset augmentation in feature space,2017, In Proc
 Improved regularization of convolutional neural networkswith cutout,2017, arXiv preprint arXiv:1708
 EraseReLU: A simple way to ease thetraining of deep convolution neural networks,2017, arXiv preprint 1709
 Coupled ensembles of neural networks,2017, arXivpreprint 1709
 Shake-shake regularization,2017, arXiv preprint arXiv:1705
 Deep pyramidal residual networks,2017, In Proc
 Deep pyramidal residual networks,2017, arXiv preprintarXiv:1610
 Delving deep into rectifiers: Surpassinghuman-level performance on ImageNet classification,2015, In Proc
 Deep residual learning for image recog-nition,2016, In Proc
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In Francis Bach and David Blei (eds
 Learning multiple layers of features from tiny images,2009, Technical report
 Sgdr: Stochastic gradient descent with warm restarts,2016, arXivpreprint arXiv:1608
 Learning representations byback-propagating errors,1986, Nature
 Residual networks behave like ensembles ofrelatively shallow networks,2016, Advances in Neural Information Processing Systems 29
 Aggregated residual trans-formations for deep neural networks,2017, In Proc
 Deep pyramidal residual netWorks Withseparated stochastic depth,2016, arXiv preprint arXiv:1612
 Wide residual netWorks,2016, In Proc
 Random erasing data augmen-tation,2017, arXiv preprint arXiv:1708
 2 shows change of training loss,1800, Fig
