title,year,conference
 Simulated annealing and boltzmann machines,1988, 1988
 A closer look atmemorization in deep netWorks,2017, arXiv preprint arXiv:1706
 Layer normalization,2016, arXiv preprintarXiv:1607
 Curriculum learning,2009, InProceedings ofthe 26th annual international conference on machine learning
 Optimization methods for large-scale machinelearning,2016, arXiv preprint arXiv:1606
 Entropy-sgd: Biasinggradient descent into wide valleys,2016, arXiv preprint arXiv:1611
 Deep learning,2016, MIT Press
 Qualitatively characterizing neural networkoptimization problems,2014, arXiv preprint arXiv:1412
 Adasecant: robust adaptive secant methodfor stochastic gradient,2014, arXiv preprint arXiv:1412
 A robust adaptive stochasticgradient method for deep learning,2017, arXiv preprint arXiv:1703
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Flat minima,1997, Neural Computation
 An empirical analysis of deep network losssurfaces,2016, arXiv preprint arXiv:1612
 Three factors influencing minima in sgd,2017, arXiv preprint arXiv:1711
 Generalization in deep learning,2017, arXivpreprint arXiv:1710
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Streaming normalization: Towards simplerand more biologically-plausible normalizations for online and recurrent learning,2016, arXiv preprintarXiv:1610
 Sgdr: stochastic gradient descent with restarts,2016, arXiv preprintarXiv:1608
 Adding gradient noise improves learning for very deep networks,2015, arXiv preprintarXiv:1511
 No more pesky learning rates,2013, ICML (3)
 No more pesky learning rate guessing games,2015, arXiv preprint arXiv:1506
 Cyclical learning rates for training neural networks,2017, In Proceedings of the IEEEWinter Conference on Applied Computer Vision
 Understanding generalization and stochastic gradient descent,2017, arXivpreprint arXiv:1710
 On the importance ofinitialization and momentum in deep learning,2013, ICML (3)
 Towards understanding generalization of deep learning: Perspective ofloss landscapes,2017, arXiv preprint arXiv:1706
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
