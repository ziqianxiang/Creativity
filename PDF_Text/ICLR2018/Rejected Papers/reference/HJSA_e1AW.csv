title,year,conference
 A closer lookat memorization in deep netWorks,2017, arXiv preprint arXiv:1706
 Weight uncertainty inneural netWorks,2015, In ICML 2015
 Freezeout: Accelerate training byprogressively freezing layers,2017, arXiv preprint arXiv:1706
 Algorithms for manifold learning,2005, Univ
 Shake-shake regularization of 3-branch residual netWorks,2017, In ICLR 2017 Workshop
 Beyond convexity: Stochastic quasi-convex opti-mization,2015, In Advances in Neural Information Processing Systems
 Flat minima,1997, Neural Computation
 Squeeze-and-excitation netWorks,2017, arXiv preprint arXiv:1709
 Batch normalization: Accelerating deep netWork training byreducing internal covariate shift,2015, In International Conference on Machine Learning
 On large-batch training for deep learning: Generalization gap and sharp minima,2017, ICLR2017
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Learning multiple layers of features from tiny images,2009, 2009
 Sgdr: stochastic gradient descent With restarts,2016, arXiv preprintarXiv:1608
 Sample complexity of testing the manifold hypothesis,2010, InAdvances in Neural Information Processing Systems
 The manifoldtangent classifier,2011, In Advances in Neural Information Processing Systems
 Weight normalization: A simple reparameterization to accel-erate training of deep neural networks,2016, In Advances in Neural Information Processing Systems
 Going deeper with convolutions,2015, InProceedings of the IEEE conference on computer vision and pattern recognition
 Lecture 6,2012,5â€”RmsProp: Divide the gradient by a runningaverage of its recent magnitude
 Normalized gradient withadaptive stepsize method for deep neural network training,2017, arXiv preprint arXiv:1707
 Wide residual networks,2016, arXiv preprintarXiv:1605
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
 Understandingdeep learning requires rethinking generalization,2017, In ICLR 2017
