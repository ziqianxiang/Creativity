title,year,conference
 Learning phrase representations using RNN encoder-decoder for statistical ma-chine translation,2014, CoRR
 Empirical evaluation ofgated recUrrent neUral networks on seqUence modeling,2014, CoRR
 Capacity and trainability in recur-rent neural networks,2017, In 5th International Conference on Learning Representations
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Yee Whye Teh and Mike Titterington (eds
 Highway and residual networkslearn unrolled iterative estimation,2016, CoRR
 Deep residual learning for image recog-nition,2015, CoRR
 A fast learning algorithm for deep beliefnets,0899, Neural Comput
 Long short-term memory,0899, Neural Computation
 Gated Orthogonal Recurrent Units: On Learning to Forget,2017, CoRR
 Adam: A method for stochastic optimization,2014, In Proceedingsof the 3rd International Conference on Learning Representations (ICLR)
 A simple way to initialize recurrent networksof rectified linear units,2015, CoRR
 Gradient-based learning applied todocument recognition,0018, Proceedings of the IEEE
 Learning recurrent neural networks with hessian-free optimiza-tion,2011, In Proceedings of the 28th International Conference on Machine Learning
 All you need is a good init,2015, CoRR
 HoW to construct deeprecurrent neural networks,2013, CoRR
 Exact solutions to the nonlinear dy-namics of learning in deep linear neural netWorks,2013, CoRR
 Training very deep netWorks,2015, InC
 Lecture 6,2012,5â€”RmsProp: Divide the gradient by a runningaverage of its recent magnitude
 Residual netWorks are exponential ensem-bles of relatively shalloW netWorks,2016, CoRR
 Extracting andcomposing robust features With denoising autoencoders,2008, In Proceedings of the 25th InternationalConference on Machine Learning
 ToWards ai-complete questionansWering: A set of prerequisite toy tasks,2015, CoRR
 DiracNets: Training Very Deep Neural NetWorks WithoutSkip-Connections,2017, CoRR
