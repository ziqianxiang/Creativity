Table 1: Results on the Omniglot dataset 3Model	Fine Tune	5-way Acc		20-way Acc			1-shot	5-shot	1-shot	5-shotBASELINE CLASSIFIER	N	80.0%	95.0%	69.5%	89.1%BASELINE CLASSIFIER	Y	86.0%	97.6%	72.9%	92.3%CONVOLUTIONAL SIAMESE NET	N	96.7%	98.4%	88.0%	96.5%CONVOLUTIONAL SIAMESE NET	Y	97.3%	98.4%	88.1%	97.0%MATCHING NET	N	98.1%	98.9%	93.8%	98.5%MATCHING NET	Y	97.9%	98.7%	93.5%	98.7%NEURAL STATISTICIAN Edwards & Storkey (2016)	N	98.1%	99.5%	93.2%	98.1%TCML Mishra et al. (2017)	N	99.0%	99.8%	97.6%	99.4%PROTOTYPICAL NETWORKS Snell et al. (2017)	N	98.8%	99.7%	96.0%	98.9%METANET Munkhdalai & Yu (2017)	N	99.0%	98.7%	97.1%	97.0%SIAMESE KERNEL SVM	N	98.5%	99.3%	94.0%	98.0%SIAMESE KERNEL SVM	Y	98.4%	99.2%	94.1%	98.2%4.2	TIMITThe task of one-shot learning is poorly defined on audio data because one-shot can be 1 second oreven 5 seconds as well, therefore it is required to redefine the task. In this paper k-sec learning isdefined so that the length of the training data is k seconds regardless of the sample rate. Eventually,2The figure originally published in Koch et al. (2015), this version contains minor modifications.
Table 2: Accuracies on the TIMIT datasetModel	Window length	5-way Acc		20-way Acc			1-sec	5-sec	1-sec	5-secBASELINE MODEL	0.3 sec	45.9%	71.6	24.3%	48.2%(SVM-MFCC-LPCC)	3.0 sec	-	86.7%	-	66.3%BASELINE MODEL	0.8 sec	48.7%	55.8%	19.1%	28.2%(CNN-transfer learning)	3.0 sec	-	59.9%	-	28.1%SIAMESE KERNEL SVM	0.8 sec	86.1%	95.3%	60.4%	84.2%SIAMESE KERNEL SVM	3.0 sec	-	99.0%	-	95.9%sults confirmed that classical neural network requires a lot of data, so can’t take advantageof longer audio slices, if it causes fewer data point. The sliding window’s offset here is 0.1sec as well and the length of the window can be seen in Table 2.
