Table 1: Comparisons of CNN, LSTM and SWEM architectures. Columnscorrespond to the number of compositional parameters, computational com-plexity and sequential operations, respectively.
Table 2: Test error rates on (long) document classification tasks, in percentage. Results marked With * arereported in Zhang et al. (2015), with f are reported in Dai & Le (2015), and with ∣ are reported in Conneauet al. (2016).
Table 3: Top five words with the largest values w.r.t. a give word embeddings’ dimension (each columncorresponds to a dimension). The first row shows the topic for words in each column.
Table 4: Speed & Parameters on Ya-hoo! Answer dataset.
Table 5: Performance of different models on matching natural language sentences. Results With * are forBidirectional LSTM, reported in Williams et al. (2017). Our reported results on MultiNLI are only trainedMultiNLI training set (Without training data from SNLI). For MSRP dataset, We folloW the setups in Hu et al.
Table 6: Test accuracies with different compositional functions on (short) sentence classifications.
Table 8:	The test samples from Yelp Polarity dataset that LSTM gives wrong predictions with shuffled trainingdata, but predicts correctly with the original training set. Therefore, word order should be relatively importantin these cases for predicting the corresponding sentiment (the first column shows the ground truth labels).
Table 7: Test accuracy for LSTM model trainedon original/shuffled training set.
Table 9:	Test accuracy of SWEM on Yahoo dataset with a wide range ofword embedding dimensions.
Table 10: Data Statistics. Where #w, #c and Train denote the average number of words, the numberof classes and the size of training set, respectively. For sentence matching datasets, #w stands forthe average length for the two corresponding sentences.
