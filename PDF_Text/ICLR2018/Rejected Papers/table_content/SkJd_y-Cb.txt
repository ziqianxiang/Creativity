Table 1: Summary of the two corpora analyzed in Section 3.
Table 2: Word2net outperforms existing word embedding models (skip-gram and b-emb/cbow)in terms of test log-likelihood on the Wikipedia data, both with and without pos tags. We comparemodels with the same context dimension K and the same total number of parameters p= V for differentcontext sizes (cs). (Results on more configurations are in Appendix A.) For word2net, we studydifferent parameter sharing schemes, and the color coding indicates which layer is shared and how, asin Figure 1. Parameter sharing improves the performance of word2net, especially with pos tags.
Table 3: The word networks fitted using word2net capture semantic similarities. We compare the top3 similar words to several query words (shaded in gray) for cbow/b-emb and word2net, trained onthe Wikipedia dataset. The numbers in parenthesis indicate the frequency of the query words.
Table 4: Word2net learns better semantic representations by exploiting syntactic information. Thetop 3 similar words to several queries are listed for different models fitted to the Senate speeches.
Table 5: Comparison of the test log-likelihood across different models on the Wikipedia dataset. Wecompare models with the same context dimension K and the same total number of parameters p= Vfor different context sizes (“cs”). For word2net, we explore different parameter sharing schemes. Thecolor coding of the parameter sharing (same as Figure 1) indicates which layer is shared and how.
Table 6: Comparison of the test log-likelihood across different models on the Senate speeches. Wecompare models with the same context dimension K and the same total number of parameters p= Vfor different context sizes (“cs”). For word2net, we explore different parameter sharing schemes. Thecolor coding of the parameter sharing (same as Figure 1) indicates which layer is shared and how.
Table 7: Universal pos tagset.
