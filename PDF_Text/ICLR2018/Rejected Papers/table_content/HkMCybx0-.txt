Table 1: Computational complexity of various filter sizesConvolution	FP	FP	Cycles per output Multiplies	Adds	element(CPE)5x5 3x3 3x1, 1x3 Inception-v3, -v4	25	24	~4.25 9	8	~1.53 3	2	~0.51Due to all of these reductions in convolution computational complexity, activation function perfor-mance is now a greater part of overall learning performance.
Table 2: CPU performance on vector inverse square root, Exp, Tanh (x86).
Table 3: Vector ISRLU, ISRU, ELU, and ReLU performance on AVX2 (Intel Core i7-7700 Processor[3.60 GHz “Kaby Lake”] ).
Table 4: Architecture 1 on MNIST with test accuracy and cross-entropy loss with different activationfunctions.
Table 5: Architecture 2 on MNIST with test accuracy and cross-entropy loss with different activationfunctions.
