Table 1: Coupled Ensembles of DenseNet-BCs (e = 4) with different “fuse layer” combinationsversus a single branch model. Performance is given as the top-1 error rate (mean±standard deviationfor the individual branches) on the CIFAR-100 test set. Columns “L” and “k” denote the “elementblock” architecture, “e” is the number of branches. Column “Avg.” indicates the type of “fuse layer”during training (section 3); “none” for separate trainings. Column “Individual” is error of eachbranch; Columns “FC” and “SM” give the performance for “fuse layer” choices during inference.
Table 2: Different number of branches, e, for a parameter budget. The models are trained on CIFAR-100 with standard data augmentation. See table 1 caption for the meaning of row and column labels.
Table 3: Classification error comparison with the state of the art, for single model training.
Table 4: Classification error comparison with the state of the art, multiple model trainings.
Table 5: Coupled Ensembles of two DenseNet-BCs (e = 2) versus a single model of comparablecomplexity and study of training / prediction fusion combinations.
Table 6: Different number of branches e while varying also the depth L and the growth rate k for anapproximately fixed parameter count.
Table 7: Classification error comparison with learnt architectures.
Table 8: Performance measurement and reproducibility issues. Statistics on 10 runs.
Table 9: Performance versus training time and network size (GTX 1080Ti), statistics on 5 runs.
Table 10: Results with low training data.
Table 11: Preliminary results on ImageNet.
