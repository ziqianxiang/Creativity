Table 1: Efficiency at solving the word analogy problem. Comparing the efficacy of the currentapproach with that obtained using word vectors trained on 1) the same dataset and 2) on the entireWikipedia 2014 corpus, which contains more than 10 times the data. Performance on the Wikipedia2014 dataset are taken from Levy & Goldberg (2014a)Category	current approach	Word2vec (same dataset)	Word2vec (full wiki 2014)common capitals	81.59 %	-63.44 %-	-90.51 %all capitals	78.3 %	23.80 %	77.61 %currencies	0.7 %	6.91 %	14.55 %city in state	59.3 %	23.79 %	56.95 %the more likely it describes the relationship between A and B . Since the relationship between A andB need not be unique, I assume that the set, NAB, comprising the k most frequent nouns in L areequally likely candidates for the relationship between A and B . Algorithm 1 shows how NAB canbe derived from A, B and k, for a given value of s.
Table 2: Frequency of co-occurence matters. Accuracy of the current approach at solving the wordanalogy problem for different values of k. Increasing the value of k produces diminishing returnsbeyond k = 5; the improvements on going from k = 10 to k = 20 are nominal.
Table 3: Category wise MLEs The 5 most frequently appearing MLEs for the value D in each of thecategories, listed by their frequency of occurrence. The MLEs for ‘common capitals’ demonstratesthat a model may not be ‘learning’ the relationship between a pair of words the way a human does.
Table 4: Median counts Median number of times A and B appear in the dataset for each of thecategories. The median counts for the frequencies are far less than that for any other group.
Table 5: Derived relationship between countries and their capitals. The five most commonly co-occurring nouns with countries and their capitals. The countries chosen in this list are the same asthose considered in figure 2 of Mikolov et al. (2013c). The model appears to be learning informationabout the wars that affected the capital city of these countries.
Table 6: Word relationship ambiguities Ambiguities associated with determining the relationshipbetween a word pair.
Table 7: Phrase information and common information Relationship between A and B when con-sidered as part of a phrase (algorithm 3), and as two separate words (algorithm 1). The cases wherethe information provided by the two algorithms disagree are marked in red.
