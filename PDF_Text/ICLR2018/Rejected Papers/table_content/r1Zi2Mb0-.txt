Table 1: Neural architectural searches for translation - shown are translation performances inBLEU for various neural architecture searches (NAS) at the cell and attention levels. Searches areperformed on either the small IWSLT or large WMT translation setups with reward functions beingeither ppl (perplexity) or BLEU. For each NAS search, we report results on both translation setups.
Table 2: Attention functions for SQuAD systems4.3	Transferability to Reading ComprehensionFor the reading comprehension setting, we evaluate on the Stanford Question Answering dataset asdiscussed in 2.3. We report results in F1 which measures the portion of overlap tokens between thepredicted answer and groundtruth.
