Table 1: The result of oracle test. PG, R denote the policy gradient and reward function. The topis the model trained with only MLE. The second top is the original SeqGAN model. The scorein parentheses is the one SeqGAN originally reported. The temperature τ of the proposed trainingmethod is set to be 1.5 for the long-term reward function (Long R), and 0.001 for the short-termreward function (Short R).
Table 2: The output of long-term rewardfunction (Long R) and short-term rewardfunction(Short R). Fake sequence Y 0 isproduced from Y by changing y5 to a ran-dom token. The reward is the average re-ward of 100 samples.
Table 3: The performance of PG_S_exp andPG_L_exp With different T value.
Table 4: The result of text generation. Although PG_S_exp scores the best BLEU-3, its outputsentence lacks coherence. We can see that models of PG_SL_exp balance the coherence as well aspartial correctness. To make it easy to see the comparison, sentence example is generated from thefirst word “according”.
Table 5: The examples of generated sentences.
Table 6: Window size and kernel numbers for reward functionsWe set window size and kernel numbers as Table 6 in both experiments.
