Table 1: Time complexity across algorithms5 Extending SVD Parameterization to General Weight MatricesIn this section, we extend the parameterization to non-square matrices and use Multi-Layer Percep-trons(MLP) as an example to illustrate its application to general deep networks. For any weightmatrix W ∈ Rm×n (without loss of generality m ≤ n), its reduced SVD can be written as:W = U (∑∣0)(VL∣VR)> = U ∑V？,	(16)where U ∈ Rm×m, Σ ∈ diag (Rm),VL ∈ Rn×m. There exist un, ..., uk1 and vn, ..., vk2 s.t. U =Hmm(um)...Hkm (uk1 ), V = Hnn(vn)...Hkn (vk2), where k1 ∈ [m], k2 ∈ [n]. Thus we can extend theSVD parameterization for any non-square matrix:Mkm,nk	:Rk1	× ... ×	Rm	× Rk2	× ... ×	Rn	×	Rmin(m,n) 7→	Rm×nk1,k2(Uki ,…JUmjvk2 ,…,Vn,σ) → Hm (Um)…H^ (ukι)Σ H-k2(Vk2 )…Hn Qn)∙ (17)where Σ = (diag(σ)∣0) f m < n and (diag(σ)∣0) 1 otherwise. Next we show that we only need2 min(m, n) reflectors (rather than m + n) to parametrize any m × n matrix. By the definition ofHkn, we have the following lemma:Lemma 1. Given {vi}in=1, define V (k) = Hnn (vn)...Hkn (vk) fork ∈ [n]. We have:V(,k1) = V(,k2), ∀kι, k2 ∈ [n], i ≤ min(n - kι,n - k2).
Table 2: Results for the pixel MNIST dataset across multiple algorithms.
