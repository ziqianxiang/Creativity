Table 1: Comparison with related works.
Table 2: Error rate (%), FLOPs and # of parameters of DenseNet on the CIFAR-10/100 (C10/C100)and ImageNet datasets. Bold number indicates the best performance among the networks withsimilar complexity.
Table 3: Effective FLOPs ratio of wider and deeper networks (wider/deeper) at the same FLOPs Effective FLOPS =GPU RUnOPng Time.		Configuration	Effective FLOPs RatioReSNet-32-sc (w = 1.3125)/ ReSNet-50-sc (w = 1.0) ReSNet-110-Sc (w = 1.3125) / ReSNet-194-Sc (w = 1.0) ReSNet-164-Sc (w = 1.3125) / ReSNet-290-Sc (w = 1.0)	1.085 1.080 1.079The values larger than 1 mean wider networks achieve higher effective FLOPs.	network configuration can be more efficient than the deeper network. Since by having more convo-lutional kernels, we also extend the dimension of feature maps for exploring better feature represen-tation at each layer rather than searching better feature representation at deeper layers.
Table 4: Error rate (%) among networks on the CIFAR-10/100 (C10/C100) datasets. Bold numberindicates the best performance among the networks with similar complexity. w of all models are setto 1.3125.
Table 5: GPU speed comparison of SW-SC and regular convolution under Similiar computations.
Table 6: Error rate (%), FLOPs and # of parameters among networks on ImageNet. Single cropvalidation errors on a 224 × 224 crop is reported. Bold number indicates the best performanceamong the networks with similar complexity.
Table 7: Error rate (%), FLOPs and # of parameters among networks on ImageNet. Single cropvalidation errors on a 224 × 224 crop is reported.
Table 8: Error rate (%), FLOPs and # of parameters among networks on the CIFAR-10/100(C10/C100) datasets. Bold number indicates the best performance among the networks with similarcomplexity.
Table 9: GPU speed up of SW-SC convolution to conventional convolution.			Configuration	Input tensor N X C X H X W *	Convolutional layer k X k, Kt	SpeedupA	32 x 256 x 28 X 28	3 X 3, 256	1.22XB	32 X 256 X 28 X 28	3 X 3, 512	1.40XC	32X512X 14X 14	3 X 3, 256	1.48XD	32X512X 14X 14	3 X 3, 512	1.77XE	32 X 1024 X 7 X 7	3 X 3, 512	1.68XF	32 X 1024 X 7 X 7	3 X 3, 1024	1.98X*: N: batch size, C: channels, H: height, W: width. 1: k: kernel size, K: number ofkernels.			6.2	Experimental Results on ImageNetTable 10 shoWs the details on the experiments for ImageNet.
Table 10: Error rate (%), FLOPs and # of parameters among netWorks on ImageNet. Single cropvalidation errors on a 224 × 224 crop is reported. Bold number indicates the best performanceamong the networks with similar complexity.
