Table 1: AlexNet and AlexNet-BN trained for 100 epcohs: B=4K and B=8K. BatchNorm makes itpossible to use larger learning rates, but training with large batch still results in lower accuracy.
Table 2: AlexNet: The ratio of norm of weights to norm of gradients for different layers at 1stiteration. The maximum learning rate is limited by the layer with smallest ratio.
Table 3: Alexnet and Alexnet-BN training with LARS: the top-1 accuracy as function of batch size(a) AlexNet with LARSBatch	LR	accuracy,%512	"ɪ	5874K		5858K	~0Q~	58216K	~4A~	55.032K	~1Γ~	46.9(b) AlexNet-BN with LARSBatch	LR	accuracy,%512	"ɪ	6024K		6048K	14-	60116K	^^3^	59332K	~^Γ	57.8	—Figure 3: AlexNet-BN, B=16K and 32k: Accuracy as function of LRDuring testing we used one model and 1 central crop. The baseline (B=256) accuracy is 73.8% forminimal augmentation. To match the state-of-the art accuracy from Goyal et al. (2017) and Cho et al.
Table 4: ResNet50 with LARS: top-1 accuracy as function of batch size for training with minimaland extended data augmentationBatch	Y	warm-up	min aug, accuracy,%	max aug, accuracy, %256	1-	N/A	73.8	75.81K	"ɪ	5	73.3	75.48K	^30^	5	735	75216K	^33^	12	729	74432K	^40^	12	72.5	72.5The accuracy with B=16K is 0.7-1.4% less than baseline. This gap is related to smaller number ofsteps. We will show in the next section that one can recover the accuracy by training for more epochs.
Table 5: Accuracy vs Training duration(b) AlexNet-BN, B=32KEpochs	accuracy,%-100-	578-125-	59.2-150-	59.5-175-	59.5200	59.9	—(c) ResNet-50, B=16KEpochs	accuracy,%-100-	744-125-	741-150-	74.6-175-	74.8200	75.5 一In general we found that we have to increase the training duration to keep the accuracy. Considerfor example Googlenet [Szegedy et al. (2015)]. As a baseline we trained BVLC googlenet 8with batch=256 for 100 epoch. The top-1 accuracy of this model is 69.2%. Googlenet is deep,so in original paper authors used auxiliary losses to accelerate SGD. We used LARS to solveoptimization difficulties so we don’t need these auxiliary losses. The original model also has no Batch
Table 6: Googlenet training with LARSBatch	Y	epochs	warm-up	accuracy,%256 (no LARS)	0.02	-100-	-	69.2256 QARS)	4	-100-	5	70.31K	-^6^^	100	10	69.72K	7	-150-	25	Tn4K	^T0-	200	30	69.98K	~~L0~	250	60	69.016K	~T0-	350	110	67.27	ConclusionLarge batch is a key for scaling up training of convolutional networks. The existing approach forlarge-batch training, based on using large learning rates, leads to divergence, especially during the8https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet7Under review as a conference paper at ICLR 2018initial phase, even with learning rate warm-up. To solve these difficulties we proposed the newoptimization algorithm, which adapts the learning rate for each layer (LARS) proportional to the ratiobetween the norm of weights and norm of gradients. With LARS the magnitude of the update foreach layer doesn’t depend on the magnitude of the gradient anymore, so it helps with vanishing andexploding gradients. But even with LARS and warm-up we couldn’t increase LR farther for very
