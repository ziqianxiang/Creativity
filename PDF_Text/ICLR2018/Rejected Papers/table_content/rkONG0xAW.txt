Table 1: Comparisons of weight information usage in BNNs and RBNNSteps	BNN	Proposed RBNNMAC in forward prop.	Sign bits of weights	Sign bits of weightsMAC in back prop.	Sign bits of weights	Sign bits of weightsWeight update	All bits of weights	All bits of weightsRecursive recycling	N/A	Keep sign bits and recycle storages of the other bits for more plastic weights(Courbariaux et al. (2015)). After that, we discard all bits except the sign bit in each weight(binarization), resulting in a 1 X 2 X 2 X 1 trained network having binary weights (trained上NN.
Table 2: Detail comparisons of RBNNs and BNNs	Ri	R2	R3	Bi	B2	B3Initial hidden neurons	200	1oo	100	800	4o0	-200~Final hidden neurons	800	700	400	800	4o0	-200-Final synaptic weights	635,200	555,800	317,600	-635,200-	-317,600-	155,600Initial weight bit-width	16	16	12	12	12	16Storage requirement	4	228	3	12	12	16Test error (%)	256	265	276	261	280	-360-Arithm., training	2,223,200	2,779,000	1,111,600	1,270,400	-635,200-	317,600ShifvMultiPly/Add	635,200	555,800	317,600	635,200	317,600	158,800	2,223,200	2,779,000	1,111,600	1,270,400	635,200	317,600Arithm., inference	635,200	555,800	317,600	-635,200-	-317,600-	158,800ShiftAdd	635,200	555,800	317,600	635,200	317,600	158,800Storage for weights	-310kB-	-155kB-	-116kB-	-930kB-	-465kB-	114kBTotal Train Energy (nJ)	2,715.18	2,459.41	1,004.58	231,197.91	115,304.86	655.05Arithm.	365.24	402.95	123.56	175.67	87.84	67.49Data Access	2,350.24	2,056.46	881.02	231,022.24	115,217.02	587.56Table 3: Energy table for 45nm CMOS processOPeration(int)	12-bit ADD/ SHIFT	12-bit MULT	12-bit SRAM	12-bit DRAM	16-bit ADD/ SHIFT	16-bit MULT	16-bit SRAM	16-bit DRAMEnergy [pJ]	0.0375	0.126	1.387	-240~	0.05	0.225	-1.85	-320~
Table 3: Energy table for 45nm CMOS processOPeration(int)	12-bit ADD/ SHIFT	12-bit MULT	12-bit SRAM	12-bit DRAM	16-bit ADD/ SHIFT	16-bit MULT	16-bit SRAM	16-bit DRAMEnergy [pJ]	0.0375	0.126	1.387	-240~	0.05	0.225	-1.85	-320~Relative Cost	1	-34-	-37-	6400	-13-	6	49.3	8533and B1; R3 and B2), RBNN requires around twice as many add and shift operations as conventionalBNN does. On the other hand, RBNN and BNN have the same amount of multiply operations. Sincethe complexity of multiplication is much higher than add and shift, it is important not to increase the8Under review as a conference paper at ICLR 2018number of multiplications. For inference, both RBNN and BNN have the same amount of shift andadd operations. Inference requires no multiplication since MAC uses binary information of weights.
Table 4: Accuracy and data storage size comparison of the RBNN and the conventional BNN onVAD benchmarkScenario	RBNN			BNN			Data storage savings	Weight bit-width	Hidden neurons inital/final	Test accuracy(%)	Weight bit-width	Hidden neurons	Test accuracy(%)	bus	16	100/500	5*27	12	-900~	59	-67×~cafe		100/400	8.8		-1100-	8.71	8.25 ×-park		100/600	7.94		-1200-	821	9×river		100/500	8.15		-1000-	8.12	-75×-traffic		100/600	8.05		-900-	807	6.75 ×-D RBNN in Fully-CONNECTED DNN SystemsIn Sec. 3.1, we have the RBNN to train a tiled feedforward DNN. In this section, we experiment totrain a fully-connected DNN using the proposed RBNN. Note that the fully-connected DNN is onlyone way of many other possible approaches on how to recycle the data storage to expand a neuralnetwork. Figure 7 illustrates the training process. It starts with an exemplary DNN whose initial12Under review as a conference paper at ICLR 2018size is 1×2×2×1. Each weight is n bits. As shown in the first two sub-figures, we first train a tiledDNN as we did in Sec. 3.1. Then, we start to add the weights that connect between tiles (the last twosub-figures), again by recycling the data storage from the binarization in each recursive iteration.
