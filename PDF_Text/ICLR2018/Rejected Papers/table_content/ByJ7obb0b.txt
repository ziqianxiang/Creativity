Table 1: Nomenclature - FormulationQuantity	Descriptionn	Number of hidden layersXi	Vector of inputs for a single samplev(k),j	Vector output of layer kw(k),j	Matrix of weights for layer kA(∙)	Activation functionj Ui	Matrix of output layer weightsPj	Vector of intermediate variables for the output layeryl	Vector of outputs for a single sampleyl	Vector of labels for a single samplef	Scalar objective function value for a single sampleF	Scalar objective functionEquations 10-16 define a generic feedforward network with ReLU activation functions in the hiddenlayers, n hidden layers, a softmax at the output layer, and categorical cross-entropy as the objectivefunction.
Table 2: Variable-Specific Learning Rate CalculationsLearning Rate	gtVf	gT Hg	kgk3ji (w，k)，i αj	dF v(n)，j	dF v(n)，jv(n)，t	1 ∂pi v	∂pi∂pSvv	1 ∂F Umη""v(kf3	∂p⅛ umη(n点mV(I)"做")，。V(I)，t	1B.2 Convolutional and Recurrent LayersConvolutional and recurrent layers preserve the low-rank derivative structure of the fully connectedfeedforward layers considered above, and we will show this in the following sections. Because weare only considering a single layer of each, we calculate the derivatives of the layer outputs withrespect to the layer inputs - in a larger network, those derivatives will be necessary for calculatingtotal derivatives via back-propagation.
