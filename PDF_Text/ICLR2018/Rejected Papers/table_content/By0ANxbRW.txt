Table 1: Baseline modelsDataset	Model	Model Size	Error RateMNIST	LeNet-5 (LeCun et al.,1998a)	12Mb	0.65%CIFAR-10	(Hubara et al., 2016)	612Mb	13.07%SVHN	(Hubara et al., 2016)	110Mb	2.73%5Under review as a conference paper at ICLR 20185	ExperimentsIn this section, we experimentally study the convergence of the proposed algorithm and show that itachieves state-of-the-art compression rates. We confirm that the parameters of the compressed modelcluster about the learnt centroids and evaluate the effect of this clustering on the accuracy. We thenpresent a more in-depth study of the trade-off between accuracy and the compressed model size onour benchmarks. Finally, we compare the optimal model obtained through the trade-off analysis withthe previous techniques.
Table 2: Baseline modelsDataset	Compression Technique	Compression Ratio	Accuracy Loss	Constrained Compression	112	0.53MNIST	Deep Compression	39	-0.06	Iterative ECSQ	49	-0.02	Soft Weight-Sharing	64	-0.05CIFAR-10	Constrained Compression	42	0.78SVHN	Constrained Compression	128	2.376 ConclusionIn this paper, we presented a method for compressing trained neural network models that directlyminimizes its complexity while maintaining its accuracy. For simplicity of calculations, we chose torepresent the complexity using the k-means objective which is frequently used in previous works.
