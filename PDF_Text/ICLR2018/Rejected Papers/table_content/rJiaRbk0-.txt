Table 1: Performance comparison on language model (perplexity)Model		Size	Valid	TeStExisting results			Unregularzed LSTM	7M	120.7	114.5NR-dropout (Zaremba et al., 2014)	66M	82.2	78.4ZoneoUt (Krueger et al., 2016)	66M	-	77.4Variational LSTM (Gal & Ghahramani, 2016)	19M	-	73.4CharCNN (Kim et al., 2016)	21M	72.4	78.9Pointer Sentinel-LSTM (Merity et al., 2016)	51M	-	70.9LSTM + continuous cache pointer (Grave et al., 2016)	-	-	72.1Variational LSTM + augmented loss (Inan et al., 2016)	51M	71.1	68.5Variational RHN (Zilly et al., 2016)	23M	67.9	65.4NAS Cell (Zoph & Le, 2016)	54M	-	62.44-layer skip connection LSTM (Melis et al., 2017)	24M	60.9	58.3AWD-LSTM w/o finetune (Merity et al., 2017)	24M	60.7	58.8AWD-LSTM (BaSeline)(Merity et al., 2017)		24M	60.0	57.3Our System			Sharpened Sigmoid AWD-LSTM w/o finetune	24M	61.6	59.4Sharpened Sigmoid AWD-LSTM	24M	59.9	57.5G2-LSTM w/o finetune	24M	60.4	58.2
Table 2: Performance comparison on machine translation (BLEU)English→German task	BLEU	German→English task	BLEUExisting end-to-end system			RNNSearch-LV (Jean et al., 2015) MRT (Shen etal., 2015) Global-att (Luong et al., 2015) GNMT (WU et al., 2016)		19.40 20.45 20.90 24.61	BSO (Wiseman & Rush, 2016b) NMPT (Huang et al.) NMPT+LM (Huang et al.) ActorCritic (Bahdanau et al., 2016)	26.36 28.96 29.16 28.53Our end-to-end system			Baseline Sharpened Sigmoid G2-LSTM		21.89 21.64 22.43	- - -	31.00 29.73 31.95conditioned on previous words. A model is evaluated by the prediction perplexity: smaller theperplexity, better the prediction.
Table 3: Model compression results on Penn Tree Bank dataset	Original	Round	Round & clip	SVD (rank = 128)	SVD (rank = 64)Baseline	52.8	53.2 (+0.4)	53.6 (+0.8) =	56.6 (+3.8)	65.5 (+12.7)Sharpened Sigmoid	-532-	53.5(+0.3)	53.6 (+0.4)	54.6 (+1.4)	60.0 (+6.8)G2-LSTM	52.1	52.2 (+0.1)	52.8 (+0.7) 一	53.3 (+1.2)	56.0 (+3.9)Table 4: Model compression results on IWSLT German→English dataset	Original	Round	Round & clip	SVD (rank = 32)	SVD (rank = 16)Baseline	31.00	28.65 (-2.35f	21.97 (-9.03)Z=	30.52 (-0.48)	29.56 (-1.44)Sharpened Sigmoid	29.73	27.08 (-2.65)	25.14 (-4.59)	29.17 (-0.53)	-28.82 (-0.91)-G2-LSTM	31.95	31.44 (-0.51)—	31.44 (-0.51)一	31.62 (-0.33)	31.28 (-0.67)~~Table 5: Model compression results on WMT English→German dataset	Original	Round	Round & clip	SVD (rank = 32)	SVD (rank = 16)Baseline	21.89	16.22 (-5.67f	16.03 (-5.86)Z=	21.15 (-0.74)	19.99 (-1.90)Sharpened Sigmoid	21.64	16.85 (-4.79)	16.72 (-4.92)	20.98 (-0.66)	-19.87 (-1.77)-G2-LSTM	22.43	20.15 (-2.28)—	20.29 (-2.14)一	22.16 (-0.27)	21.84 (-0.51)~~4.3	Sensitivity AnalysisWe conducted a set of experiments to test how sensitive our learnt models were if their gate param-eters were compressed. We considered two ways of parameter compression.
Table 4: Model compression results on IWSLT German→English dataset	Original	Round	Round & clip	SVD (rank = 32)	SVD (rank = 16)Baseline	31.00	28.65 (-2.35f	21.97 (-9.03)Z=	30.52 (-0.48)	29.56 (-1.44)Sharpened Sigmoid	29.73	27.08 (-2.65)	25.14 (-4.59)	29.17 (-0.53)	-28.82 (-0.91)-G2-LSTM	31.95	31.44 (-0.51)—	31.44 (-0.51)一	31.62 (-0.33)	31.28 (-0.67)~~Table 5: Model compression results on WMT English→German dataset	Original	Round	Round & clip	SVD (rank = 32)	SVD (rank = 16)Baseline	21.89	16.22 (-5.67f	16.03 (-5.86)Z=	21.15 (-0.74)	19.99 (-1.90)Sharpened Sigmoid	21.64	16.85 (-4.79)	16.72 (-4.92)	20.98 (-0.66)	-19.87 (-1.77)-G2-LSTM	22.43	20.15 (-2.28)—	20.29 (-2.14)一	22.16 (-0.27)	21.84 (-0.51)~~4.3	Sensitivity AnalysisWe conducted a set of experiments to test how sensitive our learnt models were if their gate param-eters were compressed. We considered two ways of parameter compression.
Table 5: Model compression results on WMT English→German dataset	Original	Round	Round & clip	SVD (rank = 32)	SVD (rank = 16)Baseline	21.89	16.22 (-5.67f	16.03 (-5.86)Z=	21.15 (-0.74)	19.99 (-1.90)Sharpened Sigmoid	21.64	16.85 (-4.79)	16.72 (-4.92)	20.98 (-0.66)	-19.87 (-1.77)-G2-LSTM	22.43	20.15 (-2.28)—	20.29 (-2.14)一	22.16 (-0.27)	21.84 (-0.51)~~4.3	Sensitivity AnalysisWe conducted a set of experiments to test how sensitive our learnt models were if their gate param-eters were compressed. We considered two ways of parameter compression.
Table 6: Low precision compression results on Penn Tree Bank dataset	Original	C = 0.20,r = 0.10	c = 0.40,r = 0.20	c = 0.60,r = 0.30	c = 0.80,r = 0.40LSTM	52.8	58.5(+5.7)	53.6 (+0.8)	54.2 (+1.4)	57.7 (+4.9)Sharpened Sigmoid	-53.2-	54.6 (+1.4)	53.6 (+0.4)	54.1 (+0.9)	57.8 (+4.6)G2-LSTM	52.1	54.5 (+2.4)	52.8 (+0.7)	53.2 (+1.1	55.0 (+2.9)Table 7: Low rank compression results on Penn Tree Bank dataset	Original	rank = 128	rank = 64	rank = 32	rank = 16LSTM	52.8	56.6 (+3.8)二	65.5(+12.7)二	83.1 (+30.3)二	111.6 (+58.8)Z=Sharpened Sigmoid	-53.2-	54.6 (+1.4)	60.0 (+6.8)	72.8 (+19.6)	100.9 (+47.7)G2 -LSTM	52.1	53.3 (+1.2) 一	56.0 (+3.9) 一	62.8 (+10.7) 一	75.9 (+23.8)一B Examples14Under review as a conference paper at ICLR 2018G^2-LSTMdnput-gateG^2-LSTM-forget-gateLSTM-input-gateLSTM-forget-gateFigure 6: The gate value visualization in German→English task.
Table 7: Low rank compression results on Penn Tree Bank dataset	Original	rank = 128	rank = 64	rank = 32	rank = 16LSTM	52.8	56.6 (+3.8)二	65.5(+12.7)二	83.1 (+30.3)二	111.6 (+58.8)Z=Sharpened Sigmoid	-53.2-	54.6 (+1.4)	60.0 (+6.8)	72.8 (+19.6)	100.9 (+47.7)G2 -LSTM	52.1	53.3 (+1.2) 一	56.0 (+3.9) 一	62.8 (+10.7) 一	75.9 (+23.8)一B Examples14Under review as a conference paper at ICLR 2018G^2-LSTMdnput-gateG^2-LSTM-forget-gateLSTM-input-gateLSTM-forget-gateFigure 6: The gate value visualization in German→English task.
