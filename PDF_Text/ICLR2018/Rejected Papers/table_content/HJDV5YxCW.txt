Table 1: Accuracy of related binarization work and our results using AlexNet architectureModel		Binarization (Inputs / Weights)	Top-1	Top-5	Binarized weights with floating point activations			1	SQ-BWN (Dong et al., 2017)	full precision / 1 bit	51.2%	75.1%2	SQ-TWN (Dong et al., 2017)	full precision / 2 bit	55.3%	78.6%3	TWN (our implementation)	full precision / 1 bit	48.3%	71.4%4	TWN	full precision / 2 bit	54.2%	77.9%5	HBNN (our results)	full precision / 1.4 bit	55.2%	78.4%Binarized weights and activations excluding first and last layers6	BNN (Courbariaux et al., 2015)	1 bit / 1-bit	27.9%	50.4%7	Xnor-Net (Rastegari et al., 2016)	1 bit / 1 bit	44.2%	69.2%8	DoReFaNet (Zhou et al., 2016)	2 bit / 1 bit	50.7%	72.6%9	QNN (Hubara et al., 2016)	2 bit / 1 bit	51.0%	73.7%10	AlexNet (our implementation)	2 bit / 2 bit	52.2%	74.5%11	AlexNet	3 bit / 3 bit	54.2%	78.1%12	HBNN	1.4 bit / 1.4 bit	53.2%	77.1%13	HBNN	1 bit / 1.4 bit	49.4%	72.1%14	HBNN	1.4 bit/ 1 bit	51.5%	74.2%15	HBNN	2 bit / 1.4 bit	52.0%	74.5%	Unbinarized (our implementation)			
