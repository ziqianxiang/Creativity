Table 1: Various quantities associated with the distribution of local codes (LCs) in with dropout(given as a percentage) applied during trainingNo. of LCs	0%	20 %	50%	70%	90%Minimum	8	4	7	15	0Maximum	34	29	41	57	125Mean	18.44	16.13	20.65	34.54	73.80Standard deviation	4.55	4.19	4.96	8.05	24.53expected (mean) and maximum number roughly increases with an increasing percentage of droppedout neurons. Generally, dropout percentages in the range of 20-50% are used in training, and theseprobability distribution functions (PDFs), like 0% peak, are also joint peaks, with the 20% datahaving more of the lower solution and the 50% having more of the higher one. Droping out morethan 50% of the network is not generally used as it slows down training. However, with these highervalues, the range of solutions is much higher (as evidence by a higher variance and range of thenumber of local codes), which is expected as dropout forces the network to adopt a range of solutionsub-networks, the increase in local codes suggest that localised encoding offers some protectionagainst noise. At first glance, this might seem unlikely, as distributed patterns are claimed to bemore resilient against failure. However, say we had a 20% dropout rate, a fully distributed encoding,would be affected by dropout 100% of the time, losing 20% of its information, whereas a localisedencoding would be unaffected 80% of the time (although 20% of the time it would lose all data), andfurther resilience can be provided if duplicate local codes were used for the same class. Note that, as
