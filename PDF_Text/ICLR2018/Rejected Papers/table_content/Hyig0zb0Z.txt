Table 1: Architectures details. “#conv.” is the number of convolutional layers. Dropout amplitude,“#hu” (number of output hidden units) and “kw” (convolution kernel width) are provided for the firstand last layer (all are linearly increased with layer depth). The size of the final layer is also provided.
Table 2: Comparison in LER and WER of variants of our model on LibriSpeech. When not specified,decoding is performed with the logadd(∙) operation to aggregate similar hypothesis (see Section 2.4).
Table 3: Comparison of different ASR systems. We report the type of acoustic model used for varioussystems, as well as the type of sub-word unit. HMM stands for Hidden Markov Model, CNN forConvNet; when not specified, CNNs are 1D (also called Time-Delay Neural Networks - TDNN - inthe literature). pNorm is a particular non-linearity (Waibel, 1989). We also report extra information(besides word transcriptions) which might be used by each system, including speaker adaptation, orany other domain-specific data.
Table 4: Comparison in WER of our model with other systems on LibriSpeech.
