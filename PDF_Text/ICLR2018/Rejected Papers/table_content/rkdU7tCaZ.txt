Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
Table 2: WikiText-2 perplexities.
Table 3: text8 (word-level) perplexities7.2	Medium scale word-level language modellingWe benchmark the performance of dynamic evaluation against static evaluation and the neural cacheon the larger text8 dataset. Like WikiText-2, text8 is derived from Wikipedia text. Text8 wasintroduced for word level language modelling by Mikolov et al. (2014), which preprocessed the databy mapping rare words to an ‘<unk>’ token, resulting in a vocab of 44k and 17M training tokens. Weuse the same test set as in Mikolov et al. (2014), but also hold out the final 100k training tokens asa validation set to allow for fair hyper-parameter tuning (the original task did not have a validationset). We trained an AWD-LSTM with 52M parameters using the implementation from Merity et al.
Table 4: Hutter Prize test set error in bits/char.
Table 5: text8 (char-level) test set error in bits/char.
