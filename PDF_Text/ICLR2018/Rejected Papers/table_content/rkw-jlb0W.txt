Table 1: log-likelihood of various approaches forMNIST dataset in natspractice, we have taken 16K samples from the trained generator and report the mean log-likelihoodin Table 5. Even though KDE is not an excellent measure and has high variance, we compared thelog-likelihoods of variations approaches. As observed, tanh with Lipschitz network outperformstanh with no weight normalization (corresponding to the total variation). The performance of thebounded linear function with regularization that is proposed in the paper has the best log-likelihood6Under review as a conference paper at ICLR 2018(a) Linear(b) tanh(c)Clipped(d) Grad ClippingFigure 3: Weight distributions of the last layer of the generator when using weight clipping compare to othercases. As observed, the variance of the weights in the linear and is larger. This implies more diversity inthe images produced. In the gradient clipping case where the weight has a very small variance our approachdiverges.
Table 2: The discriminator for image size 32 × 32. nc is the number of channels which is 1 forMNIST and 3 for colored images.
Table 3: The generator for image size 32 × 32. nc is the number of channels which is 1 for MNISTand 3 for colored images.
