Table 1: Results for unpermuted and permuted pixel-by-pixel MNIST experiments. Evaluationaccuracies are based on the best test accuracy at the end of every epoch.
Table 2: Results for the TIMIT speech dataset. Evaluation based on MSE and various audio metricsModel	n	# params	Valid. MSE	Eval. MSE	Model	n	# params	Valid. MSE	Eval. MSEscoRNN	224	≈ 83k	9.26	8.50	Rest. URNN二	158	≈ 83k	15.57	18.51scoRNN	322	≈ 135k	8.48	7.82	Rest. uRNN	256	≈ 135k	15.90	15.31scoRNN	425	≈ 200k	7.97	7.36	Rest. uRNN	378	≈ 200k	16.00	15.15LSTM	84	≈ 83k	18.43	17.18	FUll URNN	128	≈ 83k	15.07	14.58LSTM	120	≈ 135k	17.05	15.91	Full uRNN	192	≈ 135k	15.10	14.50LSTM	158	≈ 200k	16.33	16.06	FUll URNN	256	≈ 200k	14.96	14.696 ConclusionThere have been recent breakthroughs with RNN architectures using unitary recurrent weight ma-trices to address the vanishing/exploding gradient problem. These unitary RNNs are implementedwith complex valued matrices and require additional complexity in computation. Unlike unitaryRNNs, the scoRNN developed in this paper uses real valued orthogonal recurrent weight matri-ces with a simpler implementation scheme by parametrizing with a skew-symmetric matrix. Theresulting model’s additive update step is in the direction of steepest descent with respect to thisparametrization, and maintains the orthogonality of the recurrent weight matrix in the presence ofroundoff errors. Results from our experiments show that scoRNN can achieve superior performanceto unitary RNNs, in some cases with many fewer trainable parameters than other models.
Table 3: Timing results for the unpermuted MNIST dataset.
