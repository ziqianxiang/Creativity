Table 1: Average classification accuracies on Mini-ImageNet with 95% confidence intervals. Eval-uations of Baseline-finetune and Baseline-nearest-neighbor are from Ravi & Larochelle (2017).
Table 2: Average classification accuracies on Mini-ImageNet with 95% confidence intervals underrandomly selected few-shot settings. All models used here and in Table 1 are the same set of modelstrained only once on the training set. Like the last column of Table 1, we omit those models whichcannot be directly applied to various few-shot settings.
Table 3: Average classification accuracies on PASCAL3D+. At the top is the group of baselinemethods including nearest neighbor and Exemplar-SVM based on Pool-4 features from the sameVGG-16 used in our methods. In the middle are our factorizable likelihood models using differentnumber of VCs. At the bottom are our VCs-based nearest neighbor models. Marked in bold are thebest results within each group for each scenario.
Table 1: VC models using distances and VC-Encoding on Mini-ImageNet. VC-distance stands for anearest neighbor model using distances regarding VCs and the cosine distance metric. VC-nearest-neighbor and VC-likelihood are our proposed models based on VC-Encoding. Pool3-nearest-neighbor is a nearest neighbor model using original CNN features. Marked in bold are the bestresult for each few-shot set-up.
Table 2: Different clustering for each model we proposed on the 5-shot 5-category setting on Mini-ImageNet. Likelihood refers to the Factorizable Likelihood Model. Marked in bold are the bestresult for each model.
Table 3: Results of using feature from different layers of VGG-13 on Mini-ImageNet. Originalrefers the nearest neighbor model based on the original VGG features. Nearest Neighbor denotesthe our nearest neighbor model based on VC-Encoding. Likelihood is our Factorizable LikelihoodModel.
Table 4: Distribution of VCs over objeCt Categories. The objeCt Categories are from PASCAL3D+.
