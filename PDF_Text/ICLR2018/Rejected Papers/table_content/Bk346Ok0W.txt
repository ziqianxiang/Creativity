Table 1: Models for the TIDIGITS evaluation, with ’G’=GRU and ’D’=dense unitName	Architecture	Audio sensors	Transformation Module	Attention Module	Classification Module	ParametersSingle Audio	Baseline	1	Identity	None	(150,100,12)-G,G,D	162KDouble Audio STAN	STAN	2	Identity	(20,1)-G,D	(150,100,12)-G,G,D	170KTriple Audio STAN	STAN	3	Identity	(20,1)-G,D	(150,100,12)-G,G,D	173KDouble Audio Concat	Concatenation	2	Identity	None	(150,100,12)-G,G,D	180KTriple Audio Concat	Concatenation	3	Identity	None	(150,100,12)-G,G,D	197Kthe output of a particular sensor might temporarily be too noisy for a network to solve a given taskwell, therefore, attending to less noisy sensors is beneficial. We refer to this noise type as randomwalk noise and elaborate our implementation in Appendix A. The actual noise generation processis rather arbitrary, but sufficient if the noise level varies enough to learn the attentional mechanismduring training. In all synthetic noise experiments, a Gaussian noise source with μ = 0 and σ between[0, 3] was added to zero-mean and unit-variance normalized samples during training. We further referto σ as the noise level.
Table 2: Models for the GRID evaluation, with ’G’=GRU, ’bG’=bidirect. GRU and ’D’= dense unitName	Architecture	Video sensors	Transformation Module	Attention Module	Classification Module	ParametersSingle Video	Baseline	1	CNN	None	(200,200,52)-bG,bG,D	1.06MDouble Video STAN	STAN	2	CNN	(150,1)-G,D	(200,200,52)-bG,bG,D	1.09Mthe concatenation models with an equivalent number of sensors, achieving around half the WER.
Table 3: WER [%] in environments BUS, CAF, PED, STR and by average on ’et05_real’, CHiME-3.
Table 4: WER [%] when removing a single channel or multiple channels at a time on CHiME-3Single channel removed	Multiple channels removedModel	Base	CH1	CH2	CH3	CH4	CH5	CH6	CH1/2	CH1/2/3	CH1/2/3/4	CH1/2/3/4/5STAN-default	31.5	32.1	30.9	31.9	32.6	33.3	32	31.6	32.2	34.1	39.7STAN-shared	32.1	32.5	31.2	32.4	33	33.3	33.2	31.8	32.3	33.9	39.9of a STAN variant, the model seems capable of exploiting the remaining channels with acceptableperformance. Removing channel 2 (the backward channel) decreased the WER by relatively 2%on both STAN variants. In a second phase, multiple channels are removed in a sequential manner,starting with channels 1/2. For up to three removed channels (CH1/2/3), the WER remains stablewithin 2% of the 6-channel STANs. With five removed channels (CH1/2/3/4/5), the WER increasesrelatively by up to 26%. While this is a clear deterioration of performance, the performance still doesnot fully collapse. When removing sensors, we observed that the standard deviation of the mergedrepresentation increased with the number of removed sensors from around σ = 0.85 (all channelsactive) to σ = 1 (one channel active), which could push the network out of its preferred operatingrange and consequently cause the performance loss.
Table 5: Sample keys and corrupted channels (based on visual inspection and listening tests), orderedby the number of corrupted channels. All samples are from the ’et05_real’ evaluation set, CHiME-3Sample key	Corrupted channels	FigureM06_440C0209_CAF	None	Figure 8M06_443C020P_BUS	2	Figure 9F06_446C0210_BUS	5	Figure 10F06_442C020N_STR	2/6	Figure 11M05_440C2010_PED	2/3/4	Figure 12M05_443C020Q_BUS	1/2/4	Figure 1312Under review as a conference paper at ICLR 2018FramesFigure 8: Corrupted channels - none. This sample shows the native attention response when nochannel is corrupted, that could otherwise be interpreted as the bias towards channels. STAN-sharedseems to express no channel preference, while STAN-default prefers channels ch4 and ch5. Themerged representations appear smoother than the single channels.
