Table 1: The statistics of the training, dev and test sets of CLOTH-M (middle school questions),CLOTH-H (high school questions) and CLOTHexamine students knowledge in vocabulary, logic or grammar. Then, they replace the words withblanks and prepare three incorrect but confusing candidate options to make the test non-trivial. Asample passage is presented in Table 2.
Table 2: A Sample passage from our dataset. The correct answers are highlighted.
Table 3: The question type statistics of 3000 sampled questions. Grammar and short-term-reasoningquestions can both be solved with a short context, while we need longer context to solve long-term-reasoning and matching/paraphrasing.
Table 4: Model and human’s performance on CLOTH. Attention model does not leads to perfor-mance improvement compared to vanilla LSTM. Language model outperforms LSTM since it re-ceives more supervisions in learning to predict each word. Training on large external corpus furthersignificantly enhances the accuracy.
Table 5: Error analysis of 1-billion-language-model with three sentences as the context. The ques-tions are sampled from the sample passage shown in Table 2. The correct answer is in bold text. Theincorrectly selected options are in italics.
Table 6: Human’s performance compared with 1-billion-language-modelAssuming the majority of question type labels is reliable, we verify the strengths and weaknessesof models and human by studying the performance of models and human on different questioncategories. The comparison is shown in Figure 1.
Table 7: We train a model on α percent of automatically generated data and 100 - α percent ofhuman-designed data and test it on human-designed data and automatically generated data respec-tively.
Table 8: Overall results on CLOTH. The “representativeness” means weighted averaging the lossof each question using the predicted representativeness. “equal-average” means to equally averagelosses of questions.
