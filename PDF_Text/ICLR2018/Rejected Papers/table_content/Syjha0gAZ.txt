Table 1: Influence of the choice of a rank functionon the sequence prediction loss function	EM	F1	EM	F1Random	0.920	0.977	0.721	0.779Area	0.529	0.830	0.700	0.763Spatial	0.917	0.976	0.675	0.738When evaluating a trained policy, we usegreedy decoding and the termination policy fordetermining the size of a predicted multiset.
Table 3: Loss Function Comparison on the variants of MNIST Multi	MNIST Multi (4)		MNIST Multi (1-4)		MNIST Multi (10)		EM	F1	EM	F1	EM	F1Proposed L	0.950	0.987	0.953	0.981	0.920	0.992LRL	0.912	0.977	0.945	0.980	0.665	0.970Ld1m	0.921	0.978	0.918	0.969	0.239	0.714LKL dm	0.908	0.974	0.908	0.962	0.256	0.874L seq	0.906	0.973	0.891	0.952	0.592	0.946L1-step	0.210	0.676	0.055	0.598	0.032	0.854First, we investigate the sequence loss function Lseq from Sec. 3.2, while varying a rank function.
Table 4: Loss function comparison on the variantsof MS COCO	COCO Easy		COCO Medium		EM	F1	EM	F1Proposed L	0.702	0.788	0.481	0.639LRL	0.672	0.746	0.425	0.564L1 dm	0.533	0.614	0.221	0.085LKL dm	0.714	0.763	0.444	0.591Lseq	0.709	0.774	0.457	0.592L1-step	0.552	0.664	0.000	0.446The performance gap between the proposedloss and the others, however, grows substantially on the more challenging COCO Medium, whichhas more objects per example. The proposed multiset loss outperforms the aggregated distributionmatching with KL divergence by 3.7 percentage points on exact match and 4.8 on F1. This is anal-ogous to the experiments on the MNIST Multi variants, where the performance gap increased whenmoving from four to ten digits.
