Table 1: Generalization results on MNIST and CIFAR10 for BHNs with different numbers of RealNVP cou-pling layers (#), and comparison methods (dropout / maximum likelihood (MLE)). Bayes-by-backprop (Blun-dell et al., 2015) (*) models each parameter as an independent Gaussian, which is equivalent to using a hypernetwith 0 coupling layers. We achieved a better result outputting a distribution over scaling factors (only). MNIST5000 (A) and (B) are generalization results on subset (5,000 training data) of MNIST, (A) MLP with 800 hiddennodes. (B) MLP With 1,200 hidden nodes.
Table 2: Anomaly detection on MNIST. Since we use the same datasets as Hendrycks & Gimpel (2016), wehave the same base error rates, and refer the reader to that work.
Table 3: Anomaly detection on MNIST with unseen classes. The first column indicates the missing classlabel in the training set. Top-most block: ROC score; middle: positive precision-recall; bottom: negativeprecision-recall.
