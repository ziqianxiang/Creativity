Table 1: The models here all have a bi-directional GRU as the encoder (dimensionality 300 in eachdirection). The default way of producing the representation is a concatenation of outputs from a globalmean-pooling and a global max-pooling, while “∙ -Max” refers to the model with only global max-pooling. Bold numbers are the best results among all presented models. We found that 1) inputtingcorrect words to an autoregressive decoder is not necessary; 2) predict-all-words decoders workroughly the same as autoregressive decoders; 3) mean+max pooling provides stronger transferabilitythan the max-pooling alone does. The table supports our choice of the predict-all-words CNN decoderand the way of producing vector representations from the bi-directional RNN encoder.
Table 2: Related Work and Comparison. As presented in the table, our designed asymmetricRNN-CNN model has strong transferability, and is overall better than existing unsupervised models interms of fast training speed and good performance on evaluation tasks. The table presents the modelcomparison. “t”s refer to our models, and “small/large” refers to the dimension of representation as1200/4800. “*” indicates that DiscSent model was trained with additional data from Wikipedia andthe Gutenberg project. Bold numbers are the best ones among the models with same training andtransferring setting, and underlined numbers are best results among all unsupervised representationlearning models. For STS14, the performance measures are Pearson’s and Spearman’s score. ForMSRP, the performance measures are accuracy and F1 score.
Table 3: We implemented the same classifier asmentioned in Vendrov et al. (2015) on top of thefeatures computed by our model. Our proposedRNN-CNN model gets similar result on SNLI asSkip-thought, but with much less training time.
Table 1: Architecture Comparison. As shown in the table, our designed asymmetric RNN-CNNmodel (row 1,9, and 12) works better than other asymmetric models (CNN-LSTM, row 11), andmodels with symmetric structure (RNN-RNN, row 5 and 10). In addition, with larger encoder size,our model demonstrates stronger transferability. The default setting for our CNN decoder is thatit learns to reconstruct 30 words right next to every input sentence. “CNN(10)” represents a CNNdecoder with the length of outputs as 10, and “CNN(50)” represents it with the length of outputsas 50. “t” indicates that the CNN decoder learns to reconstruct next sentence. “*” indicates theresults reported in Gan et al. as future predictor. The CNN encoder in our experiment, noted as“§”, was based on AdaSent in Zhao et al. and Conneau et al.. Bold numbers are best results amongmodels at same dimension, and underlined numbers are best results among all models. For STS14,the performance measures are Pearson’s and Spearman’s score. For MSRP, the performance measuresare accuracy and F1 score.
