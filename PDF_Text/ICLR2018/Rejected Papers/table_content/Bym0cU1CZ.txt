Table 1: Definition of dialogue acts.
Table 2: Statistics of the experimental data sets	train	Val	test# dialogues	9M	90k	1000Min. # turns per dialogue	-3-	-5-	5Max. # turns per dialogue	-50-	^^0-	50Avg. # turns Per dialogue	7.68	7.67	7.66Avg. # words per utterance	15.81	15.89	15.74For dialogue act learning, we randomly sample 500 dialogues from the training set and recruit 3native speakers to label dialogue acts3 for each utterance according to the definitions in Table 1.
Table 3: An example of dialogue with labeled acts.
Table 4: Evaluation Results(a) Human annotations. Ratios are calculated bycombining labels from the three judges.
Table 5: Automatic evaluation results. Numbers in bold mean that improvement from the model onthat metric is statistically significant over the baseline methods (t-test, p-value < 0.01).
Table 6: An example of response generation. Utterances in the context are split by "⇒”.
Table 7: Characteristics of the generated responses from different dialogue acts.
Table 8: Dull responses for learning RL-S2S.
Table 9: More examples of response generation. Utterances in the context are split by "⇒''.
Table 10:RL-S2SComparison of simulated dialogues from different models.
Table 11: Example 1 of human-machine conversation. “M” means a machine turn, and “H” meansa human turn.
Table 12: Example 2 of human-machine conversation. “M” means a machine turn, and “H” meansa human turn.
