Table 1: Feature value thresholdsf t+ and f t- found by comput-ing the Kolmogrov-Smirnov statis-tic of the distributions exemplifiedin Figure 2.
Table 2: Properties of all datasets computedDataset	#Images	#Classes	#Images (train)	#Images (test)	#Images per class	#Images per class (train)	#Images per class (test)mit67	6,700	67	5,360	1,340	100	77 - 83	17-23cub200	11,788	200	5,994	5,794	41 - 60	29 - 30	12-30flowers102	8,189	102	2,040	6,149	40 - 258	20	20 - 238cats-dogs	7,349	37	3,680	3,669	184 - 200	93 - 100	88 - 100sdogs	20,580	120	12,000	8,580	150 - 200	100	50- 100caltech101	9,146	101	3,060	2,995	31 - 800	30	1 - 50food101	25,250	101	20,200	5,050	250	200	50textures	5,640	47	3,760	1,880	120	80	40wood	438	7	350	88	14- 179	10- 142	3 - 37characteristic by presence set correspond to activations which are particularly high. We obtain theft- and ft+ values through the Kolmogrov-Smirnov statistic, which provides the maximum gapbetween two empirical distributions. Vertical dashed lines of Figure 2 indicate these optimal thresh-olds for the mit67 dataset, the rest are shown in Table 1. To obtain a parameter free methodology,and considering the stable behavior of the ft+ and ft- thresholds, we chose to set ft+ = 0.15 andft- = -0.25 in all our experiments. Thus, after the step of feature standardization, we discretizethe values above 0.15 to 1, the values below -0.25 to -1, and the rest to 0.
Table 3: Classification results in % of average per-class accuracy for the baselines, for the full-network embedding, and for the current state-of-the-art (SotA). ED: SotA uses external data, FT:SotA performs fine-tuning of the network. SotA citeation for each dataset: mit67 (Ge & Yu, 2017),cub200 (Krause et al., 2016), flowers102 (Ge & Yu, 2017), cats-dogs (Simon & Rodner, 2015), sdogs(Ge & Yu, 2017), caltech101 (He et al., 2014), food101 (Liu et al., 2016) and textures (Cimpoi et al.,2015).
Table 4: Mean per-class accuracy in % obtained by the full-network embedding, when not per-forming feature discretization (FS), and when only discretizing the values between thresholds({-v, 0, v}).
Table 5: Classification results in % average per-class accuracy of the baseline and the full-networkembedding when using a network pre-trained on ImageNet 2012 for mit67 and on Places2 for therest.
