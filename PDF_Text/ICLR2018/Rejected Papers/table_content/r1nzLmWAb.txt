Table 1: Experimental results on University of Dundee 50 Salads dataset. We use frame-wiseaccuracy (Acc.), segmental edit score (Edit) and overlap F1 score with thresholds (F1@10, 25, 50)for evaluation. The top-two results for each metric are in boldface; the same applies for other tables.
Table 2: Experimental results on Georgia Tech Ego-centric Activities dataset. We use frame-wise accu-racy (Acc.) and overlap F1 score with thresholds(F1@10,25,50) for evaluation. (See citations in Ta-ble 1 and text.)__________________________GTEA	Acc.	F1@{10,25, 50}SPatial CNN	54.1	41.8, 36.0,25.1ST-CNN	60.6	58.7, 54.4,41.9Bi-LSTM	55.5	66.5, 59.0, 43.6EgoNet+TDD	68.5	-Dilated TCN	58.3	58.8, 52.2, 42.2ED-TCN	64.0	72.2, 69.3, 56.0TricorNet (high)	62.4	75.2, 71.3, 58.0TricorNet (low)	64.7	77.3, 73.4, 62.9TricorNet	64.8	76.0,71.1, 59.2Table 3: Experimental results on JHU-ISIGesture and Skill Assessment WorkingSet dataset. We use frame-wise accuracy(Acc.) and segmental edit score (Edit) forevaluation. (See citations in Table 1 and
Table 3: Experimental results on JHU-ISIGesture and Skill Assessment WorkingSet dataset. We use frame-wise accuracy(Acc.) and segmental edit score (Edit) forevaluation. (See citations in Table 1 andtext.)_____________________________JIGSAWS	Acc.	EditMSM-CRF	71.7	-Spatial CNN	74.0	37.7ST-CNN	77.7	68.0TCN	81.4	83.1TricorNet (high)	79.4	83.3TricorNet (low)	82.2	84.9TricorNet	82.9	86.8Figure 4: Top: Example images in a sample testing video from 50 Salads dataset. Middle: Groundtruth and predictions from different models. Bottom: Two typical mistakes caused by visual similarity(left - peeled cucumber is similar to lettuce in color; right - hands will cover object when placing);TricorNet avoids the mistakes by learning long-range dependencies of different actions.
