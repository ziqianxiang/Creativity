Table 1: Interaction of model archi- tecture and training objective on 5- shot, 20 way classification accuracy.			Table 2: Comparison of deep learning techniques for Omniglot classification					Accuracy (20-way)			KL/nats*	Accuracy						1-shot		5-shotDeconvolutional Architecture			Generative models, log p(X)		NS [7]	31.34	95.6%	Variational Memory Addressing [2]	77%	91 %Resample	25.74	94.0%	Generative Matching Networks* [1]	77.0%	91.0%Rescale	477.65	95.3%	Neural Statistician [7]	93.2%	98.1%VHE	452.47	95.6%	Variational Homoencoder	95.2%	98.8%PixelCNN Architecture			Discriminative models, log q(y|x, X, Y )		NS	14.90	66.0%	Siamese Networks [16]	88.1%	97.0%Resample	0.22	4.9%	Matching Networks [26]	93.8%	98.7%Rescale	506.48	62.8%	Convnet with memory module [14]	95.0%	98.6%VHE	268.37	98.8%	mAP-DLM [25]	95.4%	98.6%*DKL	q(c; D) k p(c) , train set		Model-Agnostic Meta-learning [8] Prototypical Networks [24]	95.8% 96.0%	98.9% 98.9%*Uses train/test split from Lake et al. (2015)Table 1 collects classification results of models trained using each of the four alternative trainingobjectives, for both architectures. When using a standard deconvolutional architecture, we findlittle difference in classification performance between all four training objectives, with the NeuralStatistician and VHE models achieving equally high accuracy.
Table 3: Joint NLL of Omniglot test set,across architectures and objectivesTest NLL per image	Deconvolutional Architecture	NS [7]	102.84 natsResample	110.30 natsRescale	109.01 natsVHE	104.67 natsPiXelCNN Architecture	NS	73.50 natsResample	66.42 natsRescale	71.37 natsVHE	61.22 natsTable 4: Comparison of deep generative models byjoint NLL of Omniglot test setTest NLL per image	Independent models	1 log QiP(Xi)DRAW [9]	< 96.5 natsConv DRAW [10]	< 91.0 natsVLAE [4]	89.83 nats
Table 4: Comparison of deep generative models byjoint NLL of Omniglot test setTest NLL per image	Independent models	1 log QiP(Xi)DRAW [9]	< 96.5 natsConv DRAW [10]	< 91.0 natsVLAE [4]	89.83 natsConditional models	1 log QiP(xi|xi：i-i)Variational Memory Addressing [2]	> 73.9 nats Generative Matching Networks [1]	62.42 nats1	Shared-latent models	1 log Ep(C)Qi P(XiIC)Variational Homoencoder	61.22 nats4.3 Modelling richer category structureTo demonstrate how the VHE framework may apply to models with richer category structure, webuilt both a hierarchical and a factorial VHE (Figure 2) using simple modifications to the abovearchitectures. For the hierarchical VHE, we extended the deconvolutional model with an extra latentlayer a using the same encoder and decoder architecture as c. This was used to encode alphabetlevel structure for the Omniglot dataset, learning a generative model for alphabets of the formp(A) =	p(a) Y	p(ci|a) Y p(xij|ci,a)dcida	(20)Xi∈A	xij∈Xi1We thank the authors of Bartunov & Vetrov (2016) for providing us with this comparison.
