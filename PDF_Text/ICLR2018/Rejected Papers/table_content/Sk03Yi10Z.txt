Table 1: Characteristics of retrieved and generated replies in two different conversational systems.
Table 2: Statistics of our datasets.
Table 3: Results of our ensemble and competing methods in terms of average human scores andBLEUs. Inter-annotator agreement for human annotation: Fleiss’ κ = 0.2932 (Fleiss (1971)), std= 0.3926, indicating moderate agreement. While the agreement is comparable to previous results,e.g., 0.2-0.4 reported in Shang et al. (2015)To train our neural models, we implement code based on dl4mt-tutorial6, and follow Shang et al.
Table 4: Examples of retrieved and generated ones. “/'indicates the reply selected by the re-ranker.
