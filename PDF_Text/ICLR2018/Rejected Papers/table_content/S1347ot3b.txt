Table 1: ROUGE-1 Results on the DUC 2004 dataset. ROUGE-2 results in parentheses. All combi-nations which do not perform significantly better than random chance (p < .05, using a paired t-test)are replaced with ’NA’ for clarity. SIF Average with either Max Similarity or Brute Force were in-cluded, despite having p=.051. In addition, one combination (Max Similarity with Skipthought Vec-tors) are not computed, but are not expected to perform better than chance. Selector Functions areroughly organized according to the vector functions with which they are effective. For Skipthoughtvectors, docvec-avg is used (Section 6.5)5.1	Experimental EvaluationBecause evaluation of summarization is fundamentally a subjective task, human evaluations areideal. However, human evaluations are often expensive and time-consuming to obtain. Luckily,some metrics of automatically evaluating summaries, by comparison to a human-written summary,have been developed. Traditionally, various forms of the ROUGE metric, which compare sharedn-grams, have been used.(Lin, 2004). ROUGE has been shown to correlate strongly with humanjudgments(Rankel et al., 2013), and is our primary metric for evaluating summaries1. We reportROUGE-1 and ROUGE-2 statistics, which correspond to unigrams and bigrams, respectively.
Table 2: A comparison of document vector methods. Numbers represent the difference in ROUGE-1scores between document vector methods. Positive numbers represent a gain when using docvec-avg. Selectors which do not use the document vector have been omitted.
