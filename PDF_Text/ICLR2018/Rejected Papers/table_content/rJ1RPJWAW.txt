Table 1: Learnability comparison of shallow networks on CIFAR-10 dataset with a batchsize of 64averaged across five independent runs.
Table 2: Learnability comparison of popular neural network architectures on CIFAR 10 dataset witha batchsize of 64 averaged across five independent runs.
Table 3: Learnability comparison of MLPs (Multi Layer Perceptrons) of fixed hidden unit size 64and varying depth on CIFAR-10 dataset with a batchsize of 64 averaged across five independentruns.
Table 4: Learnability comparison of shallow CNNs on CIFAR-100 dataset with a batchsize of 64averaged across five independent runs.
Table 5: Class wise Percentage distribution for N1 predictions on D2 for CIFAR-10 Dataset. Shal-low 16 refers to a single layer ConvNet with 16 number of filters.
Table 6: N1 : Shallow net with 1024 filters,N2: Shallow net with 1024 filters; in percent-agePPP^ PLP TLP^PPɪ	0	10	25.84	19.831	15.29	39.04Table 7: N1: Shallow net with 16 filters, N2:Shallow net with 16 filters; in percentage5Under review as a conference paper at ICLR 2018PPP^ PLP TLP^PPɪ	0	10	14.58.	12.931	-	11.94	60.55Table 8: N1: VGG11 and N2: VGG11; inpercentagePPP^ PLP TLP^PPɪ	0	10	11.21	9.971	10.23	68.59Table 9: N1 : GoogleNet, N2 : GoogleNet; inpercentage
Table 7: N1: Shallow net with 16 filters, N2:Shallow net with 16 filters; in percentage5Under review as a conference paper at ICLR 2018PPP^ PLP TLP^PPɪ	0	10	14.58.	12.931	-	11.94	60.55Table 8: N1: VGG11 and N2: VGG11; inpercentagePPP^ PLP TLP^PPɪ	0	10	11.21	9.971	10.23	68.59Table 9: N1 : GoogleNet, N2 : GoogleNet; inpercentagefour different possibilities of TLP and PLP for shallow networks while Tables 8 and 9 present theseresults for VGG-11 and GoogleNet. The key point we would like to point out from these tables isthat if we focus on those examples where N1 does not predict the true label correctly i.e., TLP = 0or the first row in the tables, we see that approximately half of these examples are still learnedcorrectly by N2 . Contrast this with the learnability values of N1 learned with random data whichare all less than 20%. This suggests that networks learned on true data make simpler predictions
Table 8: N1: VGG11 and N2: VGG11; inpercentagePPP^ PLP TLP^PPɪ	0	10	11.21	9.971	10.23	68.59Table 9: N1 : GoogleNet, N2 : GoogleNet; inpercentagefour different possibilities of TLP and PLP for shallow networks while Tables 8 and 9 present theseresults for VGG-11 and GoogleNet. The key point we would like to point out from these tables isthat if we focus on those examples where N1 does not predict the true label correctly i.e., TLP = 0or the first row in the tables, we see that approximately half of these examples are still learnedcorrectly by N2 . Contrast this with the learnability values of N1 learned with random data whichare all less than 20%. This suggests that networks learned on true data make simpler predictionseven on examples which they misclassify.
Table 9: N1 : GoogleNet, N2 : GoogleNet; inpercentagefour different possibilities of TLP and PLP for shallow networks while Tables 8 and 9 present theseresults for VGG-11 and GoogleNet. The key point we would like to point out from these tables isthat if we focus on those examples where N1 does not predict the true label correctly i.e., TLP = 0or the first row in the tables, we see that approximately half of these examples are still learnedcorrectly by N2 . Contrast this with the learnability values of N1 learned with random data whichare all less than 20%. This suggests that networks learned on true data make simpler predictionseven on examples which they misclassify.
Table 10: Learnability values for shallow 2-layer CNNs of various sizes. Values in the first columnrepresent the number of filters in N1 and values in the header row represent the number of filters inN2.
Table 11: Learnability values for shallow MLPs of various sizes. Values in the first column representthe depth ofN1 and values in the header row represent the depth ofN2. Each MLP layer had a hiddenunit size of 64 followed by a ReLUPPP	N NI ^PP	# of Layers, Params	Test Accuracy	VGG11	ResNet18	GoogLeNet	DenseNet121DPN26	89-11574842	69.84±0.27	68.54±0.97	69.84±0.89	72.33±0.52	72.47±0.15ResNet18	62-11173962	69.98±0.55	68.79±0.68	69.94±0.55	71.76±0.24	73.33±0.29ResNet34	110-21282122	71.88±0.41	71.51±0.41	71.14±0.01	72.22±0.69	73.86±0.09VGG11	34-9231114	72.93±0.36	73.47±0.39	69.46±0.57	72.39±0.11	73.42±0.03VGG13	42-9416010	75.01±0.56	74.87±0.67	70.83±0.08	73.84±0.83	74.65±0.01VGG16	54-14728266	75.78±0.60	74.23±1.04	72.28±0.14	74.40±0.66	74.76±0.39VGG19	66-20040522	76.10±0.28	74.66±0.10	71.84±0.15	74.29±0.93	76.75±0.12GoogLeNet	258-6166250	78.58±0.31	70.75±0.18	71.24±1.55	78.55±1.58	77.74±0.05DenseNet121	362-6956298	79.47±0.67	73.41±0.37	73.95±0.56	78.61±0.01	79.46±0.01Table 12: Learnability values for various popular architectures. The first column gives the architec-ture of N1 and the header row shows the architecture of N2. See text for discussion.
Table 12: Learnability values for various popular architectures. The first column gives the architec-ture of N1 and the header row shows the architecture of N2. See text for discussion.
Table 13: Learnability comparison of network architectures on CIFAR-10 dataset with varying batchsizes. For this experiment we fixed N2 to be VGG11 with batch size of 64. GoogleNet and DenseNetarchitectures ran out of memory for batch size of 128 and 256.
Table 14: Learnability and Accuracy comparison of single layer MLP with varying hidden unit sizefor N1 on MNIST dataset averaged across five independent runs. For all of the above results wefixed N2 to a single layer MLP with hidden unit size of 4.
