Table 1: DatasetsNAME	ARTICLE	PARAGRAPH	A	A TRAIN	TEST	TRAIN	TEST	LANGUAGEenwiki	7,634,438	850,457	41,256,261	4,583,893	Englishhudong	1,618,817	180,278	53,675,117	5,999,920	Chineseargiga	3,011,403	334,764	27,989,646	3,116,719	Arabicengiga	8,887,583	988,513	116,456,520	12,969,170	Englishzhgiga	5,097,198	567,179	38,094,390	4,237,643	Chineseallgiga	16,996,184	1,89,0456	182,540,556	20,323,532	Multi-lingualenwiki. This dataset contains paragraphs from the English Wikipedia 1, constructed from the dumpon June 1st, 2016. We were able to obtain 8,484,895 articles, and then split our 7,634,438 fortraining and 850,457 for testing. The number of paragraphs for training and testing are therefore41,256,261 and 4,583,893 respectively.				hudong. This dataset contains pragraphs from the Chinese encyclopedia website	Table 2: Training and testing byte-level			errorsbaike.com 1 2. We crawled 1,799,095 ar- ticle entries from it and used 1,618,817 for training and 180,278 for testing. The num- ber of paragraphs for training and testing are 53,675,117 and 5,999,920.	DATASET	LANGUAGE	TRAIN	TEST	enwiki	English	3.34%	3.34%	hudong	Chinese	3.21%	3.16%argiga. This dataset contains paragraphs	argiga	Arabic	3.08%	3.09%from the Arabic Gigaword Fifth Edition	engiga	English	2.09%	2.08%release (Parker et al., 2011a), which is a	zhgiga	Chinese	5.11%	5.24%collection of Arabic newswire articles. In	allgiga	Multi-lingual	2.48%	2.50%
Table 3: Byte-level errors for long short-term memory(LSTM) recurrent networkDATASET	LANGUAGE	TRAIN	TESTenwiki	English	67.71%	67.80%fg hudong	Chinese	64.47%	64.56%argiga	Arabic	61.23%	61.29%engiga	English	70.47%	70.45%zhgiga	Chinese	75.91%	75.90%allgiga	Multi-lingual	72.39%	72.44%We only back-propagate through valid bytes in the output. Note that each sample contains a end-of-sequence byte (“null” byte) by design.
Table 4: Byte-level errors for dif-ferent pooling layersPOOL TRAIN TEST4.4 Sample LengthmaxaverageL23.34%7.91%6.85%3.34%7.98%6.77%We also conducted experiments to show how does the byte-level errors vary with respect to the sample length. Figure 7shows the histogram of sample lengths for all datasets. It indi-cates that a majority of paragraph samples can be well modeledunder 1024 bytes. Figure 6 shows the byte-level error of our models with respect to the length ofsamples. This figure is produced by testing 1,000,000 samples from each of training and testingsubsets of enwiki dataset. Each bin in the histogram represent a range of64 with the indicated upper
Table 5: Byte-level errors for re-cursive and static modelsMODEL TRAIN TESTrecursivestatic3.34%	3.34%8.01%	8.05%4.5	Pooling LayersThis section details an experiment in studying how do thetraining and testing errors vary with the choice of pooling layers in the encoder network. The exper-iments are conducted on the aforementioned model with n = 8, and replacing the max-pooling layerin the encoder with average-pooling or L2-pooling layers. Table 4 details the result. The numbersstrongly indicate that max-pooling is the best choice. Max-pooling selects the largest values in itsfield of view, helping the network to achieve better optima (Boureau et al., 2010).
