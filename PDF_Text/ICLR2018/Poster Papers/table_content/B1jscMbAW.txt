Table 1: ConvHull test accuracy results with the baseline PtrNet and different setups of the DiCoNet. The scale J has been set to 3 for n=100 and 4 for n=200. At row 2 we observe that when thesplit block is not trained we get worse performance than the baseline, however, the generalizationerror shrinks faster on the baseline. When both blocks are trained jointly, we clearly outperformthe baseline. In Row 3 the split is only trained with REINFORCE, and row 4 when we add thecomputational regularization term (See Supplementary) enforcing shallower trees.
Table 2: We have used n = 20 ∙ k points for the Gaussian dataset and n = 500 for the CIFAR-10patches. The baseline performs better than the DiCoNet when the number of clusters is small butDiCoNet scales better with the number of clusters. When Lloyd’s performs much better than itsrecursive version ("Gaussian" with d = 10), we observe that DiCoNet performance is between thetwo. This shows that although having a recursive structure, DiCoNet is acting like a mixture of bothalgorithms, in other words, it is doing better than applying binary clustering at each scale. DiCoNetachieves the best results in the CIFAR-10 patches dataset, where Lloyd’s and its recursive versionperform similarly with respect to the k-means cost.
Table 3: Performance Ratios of different models trained with n = 50 (and using 3 splits in theDiCoNet ) and tested for n ∈ {50, 100, 200} (and different number of splits). We report the numberof splits that give better performances at each n for the DiCoNet . Note that for n = 50 the model doesbest with 3 splits, the same as in training, but with larger n more splits give better solutions, as wouldbe desired. Observe that even for n = 50, the DiCoNet architecture significatively outperforms thenon-recursive model, highlighting the highly constrained nature of the problem, in which decisionsover an element are highly dependent on previously chosen elements. Although the DiCoNet clearlyoutperforms the baseline and the Dantzig algorithm for n ≤ 100, its performance eventually degradesat n = 200; see text.
Table 4: DiCoNet has been trained for n = 20 nodes and only one scale (J = 1). We used thepre-trained baseline for n = 10 as model on both leaves. BS1 and BS2 correspond to the baselinetrained for n = 10 and n = 20 nodes respectively. Although for small n, both baselines outperformthe DiCoNet , the scale invariance prior of the DiCoNet is leveraged at larger scales resulting in betterresults and scalability.
