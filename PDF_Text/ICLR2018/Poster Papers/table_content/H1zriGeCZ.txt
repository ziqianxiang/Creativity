Table 1: 60 options used in Section 5Option Name	Description01. Weight initialization	Use standard initializations or other initializations?02. Weight initialization (Detail 1)	Xavier Glorot (Glorot & Bengio, 2010), Kaiming (He et al., 2015), 1/n, or 1/n2?03. Optimization method	SGD or ADAM? (Kingma & Ba, 2014)04. Initial learning rate	≥ 0.01 or < 0.01?05. Initial learning rate (Detail 1)	≥ 0.1, < 0.1, ≥ 0.001,or < 0.001?06. Initial learning rate (Detail 2)	0.3, 0.1, 0.03, 0.01 0.003, 0.001, 0.0003, or 0.0001?	一07. Learning rate drop	Do We need to decrease learning rate as We train? Yes or No?08. Learning rate first drop time	If drop learning rate, when is the first time to drop by 1/10? Epoch 40 or Epoch 60?09. Learning rate second drop time	If drop learning rate, when is the second time to drop by 1/100? Epoch 80 orEpoch 100?	10. Use momentum (Sutskever et al., 2013)		YeS or No?11. Momentum rate	If use momentum, rate is 0.9 or 0.99?12. Initial residual link weight	What is the initial residual link weight? All constant 1 or a random number in [0,1]?	13. Tune residual link weight	Do we want to use back propagation to tune the weight of residual links? YeS orNo?	14. Tune time of residual link weight	When do we start to tune residual link weight? At the first epoch or epoch 10?15. Resblock first activation	Do we want to add activation layer after the first convolution? Yes or No?	16. Resblock second activation	Do we want to add activation layer after the second convolution? Yes or No?17. Resblock third activation	Do we want to add activation layer after adding the residual link? Yes or No?18. Convolution bias	Do we want to have bias term in convolutional layers? Yes or No?
Table 2: Important featuresStage	Feature Name	Weights1-1	24. Batch norm	-805-1-2	19. Activation	-347-1-3	04. Initial learning rate * 05. Initial learning rate (Detail 1)	-332-1-4	19. Activation * 24. Batch norm	-2.551-5	04. Initial learning rate	-2.341-6	28. Weight decay	-1.901-7	24. Batch norm * 28. Weight decay	-1:79-1-8	34. Optnet * 35. Share gradInput * 52. Dummy 13	134-2-1	03. Optimization method	-4.222-2	03. Optimization method * 10. Use momentum	-3.022-3	15. Resblock first activation	-280-2-4	10. Use momentum	-2Γ9-2-5	15. Resblock first activation * 17. Resblock third activation	-1:68-2-6	01. Good initialization	-1.262-7	01. Good initialization * 10. Use momentum	-1.122-8	01. Good initialization * 03. Optimization method	-067-3-1	29. Weight decay parameter	-0.493-2	28. Weight decay	-0.26
Table 3: Stable ranges for parameters in LassoParameter	Stage 1	Stage 2	Stage 3λ	[0.01,4.5]	[。1, 2.5]	[05,1.1]#SamPles	≥ 250	≥ 180	≥ 150C.3 Generalizing from small networks to big networksIn our experiments, Harmonica first runs on a small network to extract important features and thenuses these features to do fine tuning on a big network. Since Harmonica finds significantly bettersolutions, it is natural to ask whether other algorithms can also exploit this strategy to improveperformance.
