Table 1: Measurement on the approximation of different quantization methods, e.g., Uniform (Hubaraet al., 2016b), Balanced (Zhou et al., 2017), Greedy (Guo et al., 2017), Refined (Guo et al., 2017),and our Alternating method, see Section 2. We apply these methods to quantize the full precisionpre-trained weight of LSTM on the PTB dataset. The best values are in bold. W-bits represents thenumber of weight bits and FP denotes full precision.
Table 2: Quantization on the full precision pre-trained weight of GRU on the PTB dataset.
Table 3: Testing PPW of multi-bit quantized LSTM and GRU on the PTB dataset. W-Bits and A-Bitsrepresent the number of weight and activation bits, respectively.
Table 4: Testing PPW of multi-bit quantized LSTM and GRU on the WikiText-2 dataset.
Table 5: Testing PPW of multi-bit quantized LSTM and GRU on the Text8 dataset.
Table 6: Computing time of the binary matrix vector multiplication in CPUs, where Quant representsthe cost to execute our alternating quantization on-line.
Table 7: Testing error rate of LSTM on MNIST with 1-bit input, 2-bit weight, and 2-bit activation.	Methods	Testing Error RateFull Precision	1.10 %Refined (GUo et al., 2017)	1.39 %Alternating (ours)	1.19 %Table 8: Testing error rate of MLP on MNIST with 2-bit inpUt, 2-bit weight, and 1-bit activation.	Methods	Testing Error RateFUll Precision	0.97 %Greedy (reported in (Li et al., 2017))	1.25 %Refined (GUo et al., 2017)	1.22 %Alternating (oUrs)	1.13 %Table 9: Testing error rate of CNN on CIFAR-10 with 2-bit weight and 1-bit activation.	Methods	Testing Error RateFUll Precision (reported in (HoU et al., 2017))	11.90 %XNOR-Net (1-bit weight & activation, reported in (HoU et al., 2017))	12.62 %Refined (GUo et al., 2017)	12.08 %Alternating (oUrs)	11.70 %used. We also use ADAM (Kingma & Ba, 2015) with an exponentially decaying learning rate andBatch Normalization (Ioffe & Szegedy, 2015) with a batch size 100. The testing error rates for 2-bitinput, 2-bit weight, and 1-bit activation are shown in Table 8. Among all the compared multi-bit
Table 8: Testing error rate of MLP on MNIST with 2-bit inpUt, 2-bit weight, and 1-bit activation.	Methods	Testing Error RateFUll Precision	0.97 %Greedy (reported in (Li et al., 2017))	1.25 %Refined (GUo et al., 2017)	1.22 %Alternating (oUrs)	1.13 %Table 9: Testing error rate of CNN on CIFAR-10 with 2-bit weight and 1-bit activation.	Methods	Testing Error RateFUll Precision (reported in (HoU et al., 2017))	11.90 %XNOR-Net (1-bit weight & activation, reported in (HoU et al., 2017))	12.62 %Refined (GUo et al., 2017)	12.08 %Alternating (oUrs)	11.70 %used. We also use ADAM (Kingma & Ba, 2015) with an exponentially decaying learning rate andBatch Normalization (Ioffe & Szegedy, 2015) with a batch size 100. The testing error rates for 2-bitinput, 2-bit weight, and 1-bit activation are shown in Table 8. Among all the compared multi-bitquantization methods, our alternating one achieves the lowest testing error.
Table 9: Testing error rate of CNN on CIFAR-10 with 2-bit weight and 1-bit activation.	Methods	Testing Error RateFUll Precision (reported in (HoU et al., 2017))	11.90 %XNOR-Net (1-bit weight & activation, reported in (HoU et al., 2017))	12.62 %Refined (GUo et al., 2017)	12.08 %Alternating (oUrs)	11.70 %used. We also use ADAM (Kingma & Ba, 2015) with an exponentially decaying learning rate andBatch Normalization (Ioffe & Szegedy, 2015) with a batch size 100. The testing error rates for 2-bitinput, 2-bit weight, and 1-bit activation are shown in Table 8. Among all the compared multi-bitquantization methods, our alternating one achieves the lowest testing error.
