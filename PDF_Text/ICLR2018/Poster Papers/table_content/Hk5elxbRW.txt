Table 1: Testing performance on CIFAR-100 with different levels of label noise. With noisy labels,L5,1 consistently outperforms CEon both top-5 and top-1 accuracies, with improvements increasinglysignificant with the level of noise. For reference, a model making random predictions would obtain1% top-1 accuracy and 5% top-5 accuracy.
Table 2: Top-5 accuracy (%) on ImageNet using training sets of various sizes. Results are reportedon the official validation set, which we use as our test set.
Table 3: Execution time (s) of the forward pass. The Divide and Conquer (DC) algorithm offersnearly logarithmic scaling with n in practice, thanks to its parallelization. In contrast, the runtime ofthe Summation Algorithm (SA) scales linearly with n.
Table 4: Execution time (s) of the backward pass. Our Custom Backward (CB) is faster than AutomaticDifferentiation (AD).
Table 5: Stability on forward pass. A setting is considered stable if no overflow has occurred.
Table 6: Influence of the temperature parameter on the training accuracy and testing accuracy.
Table 7:	Influence of the margin parameter on top-1 performance.
Table 8:	Influence of the margin parameter on top-5 performance.
Table 9: Testing performance on CIFAR-100 with different levels of label noise. We indicate the meanand standard deviation (in parenthesis) for each score.
