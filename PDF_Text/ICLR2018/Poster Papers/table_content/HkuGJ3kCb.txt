Table 1: A detailed description for the embeddings in this paper.
Table 2: Before-After on the measure of isotropy.
Table 3: Before-After results (x100) on Wordsimilarity task on seven datasets.
Table 4: Before-After results (x100) on thecategorization task.
Table 5: Before-After results (x100) on the word We use the analogy dataset introduced in (Mikolovanalogy task.	et al., 2013). The dataset can be divided into twoparts: (a) the semantic part containing around 9kquestions, focusing on the latent semantic relation between pairs of words (for example, what isto Chicago as Texas is to Houston); and (b) the syntatic one containing roughly 10.5k questions,focusing on the latent syntatic relation between pairs of words (for example, what is to “amazing” as“apprently” is to “apparent”).
Table 6: Before-After results (x100) on thesemantic textual similarity tasks.
Table 7: Before-After results (x100) on the text classification task using CNN (Kim, 2014) and vanillaRNN, GRU-RNN and LSTM-RNN.
Table 8: A detailed description for the TSCCA embeddings in this paper.
Table 9: Before-After results (x100) on the word similarity task in multiple languages.
Table 10: Before-After results (x100) on the word similarity task on seven datasets.
Table 11: Before-After results (x100) on the categorization task.
Table 12: Before-After results (x100) on the word analogy task.
Table 13: Before-After results (x100) on the semantic textual similarity tasks.
Table 14: Statistics on word representation of dimensions 300, 400, ..., and 1000 using the skip-grammodel.
Table 15: Before-After results (x100) on word similarity task on seven datasets.
Table 16: Before-After results (x100) on the categorization task.
Table 17: Before-After results (x100) on the word analogy task.
Table 18: Before-After results (x100) on the semantic textual similarity tasks.
Table 19: Before-After results (x100) on the word analogy task.
Table 20: Before-After results (x100) on the semantic textual similarity tasks.
Table 21: Statistics for the five datasets after tokenization: c represents the number of classes; lrepresents the average sentence length; Train represents the size of the training set; and Test representthe size of the test set.
