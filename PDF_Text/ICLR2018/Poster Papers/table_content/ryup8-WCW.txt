Table 1: Measured dint90 on various supervised and reinforcement learning problems.
Table S2: dint90 required to memorize shuffled MNIST labels. As dataset size grows, memorizationbecomes more efficient, suggesting a form of “generalization” from one part of the training set toanother, even though labels are random.
Table S3: Hyperparameters used in training RL tasks using ES. σ refers to the parameter perturbationnoise used in ES. Default Adam parameters of β1 = 0.9, β2 = 0.999, = 1 × 10-7 were used.
Table S4: Comparison of theoretical complexity and average duration of a forward+backward passthrough M (in seconds). d was fixed to 1% of D in each measurement. D = 100k is approximatelythe size ofan MNIST fully-connected network, and D = 60M is approximately the size of AlexNet.
Table S5: Intrinsic dimension of different objective landscapes, determined by dataset and network.
