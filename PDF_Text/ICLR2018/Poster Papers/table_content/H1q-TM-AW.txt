Table 1: Test set accuracy on visual domain adaptation benchmarks. In all settings, both VADA andDIRT-T achieve state-of-the-art performance in all settings.
Table 2: Results of the domain adaptation experiments on Wi-Fi Activity Recognition TaskWi-Fi Activity Recognition. To evaluate the performance of our models on a non-visual domainadaptation task, we applied VADA and DIRT-T to the Wi-Fi Activity Recognition Dataset (Yousefiet al., 2017). The Wi-Fi Activity Recognition Dataset is a classification task that takes the Wi-Fi Channel State Information (CSI) data stream as input x to predict motion activity within anindoor area as output y . Domain adaptation is necessary when the training and testing data arecollected from different rooms, which we denote as Rooms A and B. Table 2 shows that VADAsignificantly improves classification accuracy compared to Source-Only and DANN by 17.3% and15% respectively. However, DIRT-T does not lead to further improvements on this dataset. Weperform experiments in Appendix F which suggests that VADA already achieves strong clustering inthe target domain for this dataset, and therefore DIRT-T is not expected to yield further performanceimprovement.
Table 3: Additional comparison of the margin of improvement computed by taking the reportedperformance of each model and subtracting the reported source-only performance in the respectivepapers. W.I.N.I. indicates “with instance-normalized input.”Overall. We achieve state-of-the-art results across all tasks. For a fairer comparison against ATTand the Π-model, Table 3 provides the improvement margin over the respective source-only per-formance reported in each paper. In four of the tasks (MNIST → MNIST-M, SVHN → MNIST,MNIST → SVHN, STL → CIFAR), we achieve substantial margin of improvement compared toprevious models. In the remaining three tasks, our improvement margin over the source-only model8Published as a conference paper at ICLR 2018is competitive against previous models. Our closest competitor is the Π-model. However, unlike theΠ-model, we do not perform data augmentation.
Table 4: Test set accuracy in ablation experiments, starting from the DANN model. The “no-vat”subscript denote models where the virtual adversarial training component is removed.
Table 5: Comparison of model behavior when domain adversarial training is applied to variouslayers. We denote the very last (simplex) layer of the neural network as L and ablatively domainadversarial training to the last eight layers. A lower bound on the Jensen-Shannon Divergence iscomputed by training a logistic regression model to predict domain origin when given the layerembeddings.
Table 6: Small and Large CNN architectures. Leaky ReLU parameter a = 0.1. All convolutionaland dense layers in the classifier are pre-activation batch-normalized. All images are resized to 32 ×32 × 3. Note the use of additive Gaussian noise: this addition was motivated by initial experimentsin which we observed that domain adversarial training appears to contract the feature space.
Table 7: Domain discriminator architecture.
Table 8: Hyperparameters for each task, both with and without instance-normalized input. Theonly exception is MNIST → SVHN without instance-normalized input. In this specific case, dH∆His sufficiently large that conditional entropy minimization quickly finds a degenerate solution inthe target domain. To counter this, we remove conditional entropy minimization (but keep thetarget-side virtual adversarial training) only during VADA. We apply target-side conditional entropyminimization and virtual adversarial training during DIRT-T. To compensate, we use a lower βduring the DIRT-T phase to allow for larger natural gradient steps.
