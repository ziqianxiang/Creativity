Table 1: Results for the adding task, displayed as mean ± std over four different runs. The taskis considered to be solved if the MSE is at least two orders of magnitude below the variance of theoutput distribution.
Table 2: Accuracy, used samples and average FLOPs per sequence at inference on the test set ofMNIST after 600 epochs of training. Results are displayed as mean ± std over four different runs.
Table 3: Mean Average Precision (mAP), used samples and average FLOPs per sequence at infer-ence on the validation set of Charades. The number of state updates is displayed as mean ± stdover all the videos in the validation set.
Table 4: Results for the frequency discrimination task, displayed as mean ± std over four differentruns. The task is considered to be solved if the classification accuracy is over 99%. Models withthe same cost per sample (λ > 0) converge to a similar number of used samples under differentsampling conditions.
Table 5: Accuracy and used samples on the test set of IMDB for different sequence lengths. Resultsare displayed as mean ± std over four different runs.
Table 6: Accuracy, used samples and average FLOPs per sequence at inference on the validation setof UCF-101 (split 1).
