Table 1: Attention error counts for single-speaker Deep Voice 3 models on the 100-sentence testset, given in Appendix E. One or more mispronunciations, skips, and repeats count as a singlemistake per utterance. “Phonemes & Characters” refers to the model trained with a joint characterand phoneme representation, as discussed in Section 3.2. We did not include phoneme-only modelsbecause the test set contains out-of-vocabulary words. All models use Griffin-Lim as their vocoder.
Table 2: Mean Opinion Score (MOS) ratings with 95% confidence intervals using different wave-form synthesis methods. We use the crowdMOS toolkit (Ribeiro et al., 2011); batches of samplesfrom these models were presented to raters on Mechanical Turk. Since batches contained samplesfrom all models, the experiment naturally induces a comparison between the models.
Table 3: MOS ratings with 95% confidence intervals for audio clips from neural TTS systems onmulti-speaker datasets. We also use crowdMOS toolkit; batches of samples including ground truthwere presented to human raters. Multi-speaker Tacotron implementation and hyperparameters arebased on Arik et al. (2017), which is a Proof-of-concept implementation. Deep Voice 2 and Tacotronsystems were not trained for the LibriSpeech dataset due to prohibitively long time required tooptimize hyperparameters.
Table 4: Hyperparameters used for best models for the three datasets used in the paper.
