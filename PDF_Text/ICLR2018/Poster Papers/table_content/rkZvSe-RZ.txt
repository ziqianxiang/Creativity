Table 1: Error rates (in %) of adversarial examples transferred between models. We use Step-LL with = 16/256 for 10,000 random test inputs. Diagonal elements represent a white-box attack.
Table 2: Error rates (in %) for Step-LL, R+Step-LL and a two-step Iter-LL on ImageNet. Weuse = 16/256, α = /2 on 10,000 random test inputs. R+FGSM results on MNIST are in Table 7.
Table 3: Models used for Ensemble Adversarial Training on ImageNet. The ResNets (He et al.,2016) use either 50 or 101 layers. IncRes stands for Inception ResNet (Szegedy et al., 2016a).
Table 4: Error rates (in %) for Ensemble Adversarial Training on ImageNet. Error rates onclean data are computed over the full test set. For 10,000 random test set inputs, and = 16/256, wereport error rates on white-box Step-LL and the worst-case error over a series of black-box attacks(Step-LL, R+Step-LL, FGSM, I-FGSM, PGD) transferred from the holdout models in Table 3. Forboth architectures, we mark methods tied for best in bold (based on 95% confidence).
Table 5: Neural network architectures used in this work for the MNIST dataset. Conv: convo-lutional layer, FC: fully connected layer.
Table 6: Approximation ratio between optimal loss and loss induced by single-step attack onMNIST. Architecture B’ is the same as B without the input dropout layer.
Table 7: White-box and black-box attacks against standard and adversarially trained models.
Table 8: Ensemble Adversarial Training on MNIST. For black-box robustness, we report themaximum and average error rate over a suite of 12 attacks, comprised of the FGSM, I-FGSM andPGD (Madry et al., 2017) attacks applied to models A,B,C and D. We use = 16 in all cases. Foreach model architecture, we mark the models tied for best (at a 95% confidence level) in bold.
Table 9: Error rates (in %) of randomized single-step attacks transferred between models onImageNet. We use R+Step-LL with = 16/256, α = /2 for 10,000 random test set samples. Thewhite-box attack always outperforms black-box attacks.
