Table 1: Parallel kernel speedup on m features (minibatch size = 1)Sequence Length	m=4	m = 32	m= 12816	0.06	0.06	0.05256	0.22	0.22	0.864,096	1.02	2.94	3.3665,536	38.5	41.8	17.5Table 2: Parallel kernel speedup for a variety of LS-RNNs, implemented as two stacked RNNlayers with 256 hidden units. We keep the GPU memory usage constant by fixing bT = 65, 536 forminibatch size b and sequence length TSequence Length	SRU	QRNN (filter size 2)	QRNN (filter size 10)	GILR-LSTM16	0.28	0.38	0.78	0.61256	0.84	0.86	0.99	0.914,096	1.38	1.18	1.05	0.9865,536	9.21	6.68	2.05	1.41kernels, one which evaluates the parallel linear recurrence described in algorithm 2, and one whichevaluates the same linear recurrence on GPU in serial over sequence length and in parallel overfeatures and minibatch. The performance of each kernel depends on two factors: the sequence lengthand the product of number of features and minibatch size. The performance measurements for thisexperiment are made directly at the kernel level, avoiding any overhead from TensorFlow. We findthat the parallel kernel has a distinct advantage at long sequence lengths with a speedup factor of up
Table 2: Parallel kernel speedup for a variety of LS-RNNs, implemented as two stacked RNNlayers with 256 hidden units. We keep the GPU memory usage constant by fixing bT = 65, 536 forminibatch size b and sequence length TSequence Length	SRU	QRNN (filter size 2)	QRNN (filter size 10)	GILR-LSTM16	0.28	0.38	0.78	0.61256	0.84	0.86	0.99	0.914,096	1.38	1.18	1.05	0.9865,536	9.21	6.68	2.05	1.41kernels, one which evaluates the parallel linear recurrence described in algorithm 2, and one whichevaluates the same linear recurrence on GPU in serial over sequence length and in parallel overfeatures and minibatch. The performance of each kernel depends on two factors: the sequence lengthand the product of number of features and minibatch size. The performance measurements for thisexperiment are made directly at the kernel level, avoiding any overhead from TensorFlow. We findthat the parallel kernel has a distinct advantage at long sequence lengths with a speedup factor of upto 40x, as shown in table 1. The parallel kernel does not perform well at short sequence lengths dueto the overhead of multiple passes over data and communication between processors.
Table 3: Performance of the GILR-LSTM compared to the CuDNN LSTM on problem 2b fromHochreiter and SchmidhUber (1997).
