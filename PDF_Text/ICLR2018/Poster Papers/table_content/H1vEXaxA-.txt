Table 1: Test BLEU scores for each model and dataset. The best performing (unaligned) model for each datasetis shown in bold. We show the results of baselines from (Nakayama & Nishida, 2017) using two differentloss functions (Appendix A). Pretrained denotes initializing the speaker modules and image encoders withpretrained image captioning models. Fixed denotes fixing the parameters of either the speaker module or theimage encoder.
Table 2: Multi30k Task 1 Test BLEU scores. Results should be compared with the first two columns in Table 1.
Table 3: Nearest neighbors of foreign word embeddings learned from communication, along with a sampleimage for each concept in the dataset. The English word embeddings were learned by the German agent andvice versa.
Table 4: Sample DE-EN translations from Multi30k Task 2 test set. Images were not used to aid translationand are only shown for references. We show the source sentence as Src and one of the five target sentencesas Ref. The outputs from the nearest neighbor baseline are shown as NN, NMT baseline with neighbor pairsas NMT, the 3-way, both decoder model from (Nakayama & Nishida, 2017) as N&N, and our (pretrained, spkfixed) model as Model.
Table 5: Sample translations from WMT’15 DE-EN validation and test set.
Table 6: Sample JA-DE translationsSrc	黄色㈠亍二入术一儿在打^返寸 女性	黄色C々、G 7口＞卜部分仁 取4付I十bH^夕A^一仁 黄色口）:人环映n P □马		木 G 枝 ^ 止⅛oPU⅞ 目 G 周 ^^足讲赤㈠舄HyP	eine tennisspielerin in weiβ und grunem oberteil spielt tennis	ein gelber bus steht geparkt vor einem nicht fertigen gebaude .	ein vogel sitzt auf einem ast .
Table 7: Sample JA-DE translations17Published as a conference paper at ICLR 2018G	Alien Language TranslationTo demonstrate our models’ ability to learn to translate only with monolingual captions, we ex-periment with a language for which no parallel corpus exists, nor the knowledge of the languageitself: Klingon. As no image captions are available in Klingon, we translate 15k English captionsin Multi30k Task 1 into Klingon pIqaD6 using Bing Translator.7 We tokenize the Klingon captionsand discard words occurring less than 5 times in the training data. We then train our base model(no pretraining) on English and Klingon communication. In Tables 9 and 10, the source sentence inEnglish is shown as src, the Klingon model output in hyp, and the English translation of the outputin hyp (en) (using Bing Translator). Although the Klingon training data is noisy and imperfect, weobserve that our model learns to translate only With 15k KIingon captions. This example illustrateshow we can learn to translate even if there is no knowledge of the other language, and where aprofessional translator would take a long time to first acquire the other language.
