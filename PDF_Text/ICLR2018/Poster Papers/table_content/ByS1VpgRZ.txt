Table 1: Inception score and intra FIDs on ImageNet.
Table 2: Inception accuracy and MS-SSIM on different super-resolution methods. We picked updataset images from the validation set.
Table 3: The performance of class conditional image generation on CIFAR-10 (C10) and CIFAR-100 (C100).
Table 4: Inception score and intra FIDs on ImageNet with a pretrained model on classification tasksfor ILSVRC2012 dataset. iNguyen et al. (2017)Method	Inception Score	Intra FIDPPGNs*	47.4	N/Aprojection(finetuned)	210	54.2B Objective function with an auxiliary classifier costIn this experiment, we followed the footsteps of Plug and Play Generative model (PPGNs) (Nguyenet al., 2017) and augmented the original generator loss with an additional auxiliary classifier loss. Inparticular, we used the losses given by :L GD ,ppre(y∣x)) = -Eq(y) [Ep(z) [D(G(z ,y),y) - LC (Ppre(y∣G(z ,y)))ii ,	(11)where Ppre(y|x) is the fixed model pretrained for ILSVRC2012 classification task. For the actualexperiment, we trained the generator with the original adversarial loss for the first 400K updates,and used the augmented loss for the last 50K updates. For the learning rate hyper parameter, weadopted the same values as other experiments we described above. For the pretrained classifier,we used ResNet50 model used in He et al. (2016a). Figure 12 compares the results generated byvanilla objective function and the results generated by the augmented objective function. As wecan see in Table 4, we were able to significantly outperform PPGNs in terms of inception score.
