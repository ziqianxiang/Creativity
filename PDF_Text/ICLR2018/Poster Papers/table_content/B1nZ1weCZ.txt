Table 1: Comparison of the performance our MTAs to BA3C MTA based on qam	MT1	MT2	MT3	MT4	MT5	MT6	MT7|T|	6	6	6	8	12	12	21A5C	0.799	0.601	0.646	0.915	0.650	0.741	0.607UA4C	0.779	0.506	0.685	0.938	0.673	0.756	0.375EA4C	0.779	0.585	0.591	0.963	0.587	0.730	0.648BA3C	0.316	0.398	0.337	0.295	0.260	0.286	0.3087Published as a conference paper at ICLR 2018higher performance to the fact that there are many hyper-parameters to tune in the UA4C and theEA4C methods unlike A5C where only the temperature hyperparameter had to be tuned. We tuned allthe important hyperparameters for UA4C and EA4C. However, our granularity of tuning was perhapsnot very fine. This could be the reason for the slightly lower performance. The UA4C agent, however,generalizes better than A5C agent on the larger MTIs (MT5 & MT6). Also, the performance obtainedby EA4C is close to that of A5C and UA4C in all the multitasking instances. The MTI MT4 has beentaken from (Parisotto et al., 2016). On MT4, many of our agents are consistently able to obtain aperformance close to qam = 0.9. It is to be noted that Actor Mimic networks are only able to obtainqam = 0.79 on the same MTI. The most important test of generalization is the 21-task instance(MT7). EA4C is by far the best performing method for this instance. This clearly demonstrates thehierarchy of generalization capabilities demonstrated by our proposed methods. At the first level, the
Table 2: Comparison of the performance DUA4C agent to BA3C based on qam	MT1	MT2	MT4	MT5|T|	6	6	8	12DUA4C	0.661	0.533	0.576	0.509BA3C	0.316	0.398	0.295	0.2608Published as a conference paper at ICLR 2018Figure 4: Training curve for DUA4C wherein doubling of target estimates is performed and thus notarget scores whatsoever are used.
Table 3: Descriptions of the seven multi-tasking instances.
Table 4: Comparison of the performance of our agents based on qam								MT1	MT2	MT3	MT4	MT5	MT6	MT7|T|	6	6	6	8	12	12	21A5C	0.799	0.601	0.646	0.915	0.650	0.741	0.607UA4C	0.779	0.506	0.685	0.938	0.673	0.756	0.375EA4C	0.779	0.585	0.591	0.963	0.587	0.730	0.648FA4C	0.795	0.463	0.551	0.822	0.128	0.294	0.287BA3C	0.316	0.398	0.337	0.295	0.260	0.286	0.308Table 5 demonstrates the need for the evaluation metrics that we have proposed. specifically, itcan be seen that in case of MT4, the non-clipped average performance is best for BA3C. However,this method is certainly not a good MTL algorithm. This happens because the uniform samplingensures that the agent trains on the task of Enduro a lot (can be seen in the corresponding trainingcurves). Owing to high performance on a single task, pam ends up concluding that BA3C is the bestmulti-tasking network.
Table 5: Comparison of the performance of our agents based on pam	MT1	MT2	MT3	MT4	MT5	MT6	MT7|T|	6	6	6	8	12	12	21A5C	0.978	0.601	0.819	3.506	0.733	1.312	2.030UA4C	0.942	0.506	0.685	38.242	0.810	1.451	1.409EA4C	1.079	0.585	0.658	36.085	0.743	1.231	2.010FA4C	1.054	0.470	0.892	3.067	0.128	0.536	1.213BA3C	0.316	0.398	0.669	132.003	0.260	0.286	1.64330Published as a conference paper at ICLR 2018We defined the qgm and qhm metrics because in some sense, the qam metric can still get away withbeing good on only a few tasks and not performing well on all the tasks. In this limited sense, qhm isprobably the best choice of metric to understand the multi-tasking performance of an agent. We canobserve that while A5C performance was slightly better than EA4C performance for MT4 accordingto the qam metric, the agents are much more head to head as evaluated by the qhm metric.
Table 6: Comparison of the performance of our agents based on qgm	MT1	MT2	MT3	MT4	MT5	MT6	MT7|T|	6	6	6	8	12	12	21A5C	0.782	0.580	0.617	0.902	0.633	0.870	0.547UA4C	0.749	0.466	0.649	0.934	0.650	0.733	0.144EA4C	0.753	0.565	0.561	0.958	0.553	0.697	0.616FA4C	0.777	0.418	0.380	0.722	0.091	0.071	0.133BA3C	0.151	0.343	0.047	0.345	0.125	0.113	0.097Table 7: Comparison of the performance of our agents based on qhm								MT1	MT2	MT3	MT4	MT5	MT6	MT7|T|	6	6	6	8	12	12	21A5C	0.678	0.515	0.536	0.798	0.590	0.650	0.480UA4C	0.641	0.420	0.553	0.833	0.602	0.670	0.059EA4C	0.647	0.505	0.491	0.851	0.502	0.628	0.576FA4C	0.675	0.381	0.215	0.508	0.070	0.016	0.052BA3C	0.060	0.297	0.008	7.99E-7	0.038	0.028	0.02331Published as a conference paper at ICLR 2018Appendix F:	Demonstrative visualization of our methodsFigure 1 demonstrates a general visual depiction of our method. This appendix contains visualizations
Table 7: Comparison of the performance of our agents based on qhm								MT1	MT2	MT3	MT4	MT5	MT6	MT7|T|	6	6	6	8	12	12	21A5C	0.678	0.515	0.536	0.798	0.590	0.650	0.480UA4C	0.641	0.420	0.553	0.833	0.602	0.670	0.059EA4C	0.647	0.505	0.491	0.851	0.502	0.628	0.576FA4C	0.675	0.381	0.215	0.508	0.070	0.016	0.052BA3C	0.060	0.297	0.008	7.99E-7	0.038	0.028	0.02331Published as a conference paper at ICLR 2018Appendix F:	Demonstrative visualization of our methodsFigure 1 demonstrates a general visual depiction of our method. This appendix contains visualizationsspecific to each of the methods.
Table 8: Evaluation of the HUA4C agent on MT1 using all of our proposed evaluation metrics.
Table 9: Comparison of our agents between the case of using usual target scores and double targetscores.
Table 10: Comparison of performance of DA5C agents to DBA3C agents.
Table 11: Table of target scores used for the tasksTask	TargetSpace Invaders	1200Seaquest	2700Asterix	2400Alien	2700Assault	1900Time Pilot	9000Bank Heist	1700Crazy Climber	170000Demon Attack	27000Gopher	9400Name This Game	12100Star Gunner	40000TutanKham	260Amidar	1030Chopper Command	4970Breakout	560Beam Rider	2200Bowling	17
Table 12: Comparison of the performance DUA4C agent to BA3C based on q&m	MT1	MT2	MT4	MT5|T|	6	6	8	12DUA4C	0.661	0.533	0.576	0.509BA3C	0.316	0.398	0.295	0.260Table 13: Comparison of the performance DUA4C agent to BA3C based on Pam	MT1	MT2	MT4	MT5|T|	6	6	8	12DUA4C	0.739	0.549	134.939	0.577BA3C	0.316	0.398	132.003	0.260Table 14: Comparison of the performance DUA4C agent to BA3C based on qgm	MT1	MT2	MT4	MT5|T|	6	6	8	12DUA4C	0.452	0.463	0.398	0.381BA3C	0.151	0.343	0.345	0.125Table 15: Comparison of the performance DUA4C agent to BA3C based on qhm	MT1	MT2	MT4	MT5|T|	6	6	8	12DUA4C	0.175	0.371	0.217	0.218BA3C	0.060	0.297	7.99E-7	0.038
Table 13: Comparison of the performance DUA4C agent to BA3C based on Pam	MT1	MT2	MT4	MT5|T|	6	6	8	12DUA4C	0.739	0.549	134.939	0.577BA3C	0.316	0.398	132.003	0.260Table 14: Comparison of the performance DUA4C agent to BA3C based on qgm	MT1	MT2	MT4	MT5|T|	6	6	8	12DUA4C	0.452	0.463	0.398	0.381BA3C	0.151	0.343	0.345	0.125Table 15: Comparison of the performance DUA4C agent to BA3C based on qhm	MT1	MT2	MT4	MT5|T|	6	6	8	12DUA4C	0.175	0.371	0.217	0.218BA3C	0.060	0.297	7.99E-7	0.03843Published as a conference paper at ICLR 2018Training Curves for different MTIsMulti-tasking instance 1 (MT1)Figure 26: Training Curves for the DUA4C agent on MT1 (6 tasks). The horizontal line represents
Table 14: Comparison of the performance DUA4C agent to BA3C based on qgm	MT1	MT2	MT4	MT5|T|	6	6	8	12DUA4C	0.452	0.463	0.398	0.381BA3C	0.151	0.343	0.345	0.125Table 15: Comparison of the performance DUA4C agent to BA3C based on qhm	MT1	MT2	MT4	MT5|T|	6	6	8	12DUA4C	0.175	0.371	0.217	0.218BA3C	0.060	0.297	7.99E-7	0.03843Published as a conference paper at ICLR 2018Training Curves for different MTIsMulti-tasking instance 1 (MT1)Figure 26: Training Curves for the DUA4C agent on MT1 (6 tasks). The horizontal line representsthe Single Task Agent’s score. Agents in these experiments were trained for 300 million time stepsand required half the data and computation that would be required to train the task-specific agents(STA3C) for all the tasks.
Table 15: Comparison of the performance DUA4C agent to BA3C based on qhm	MT1	MT2	MT4	MT5|T|	6	6	8	12DUA4C	0.175	0.371	0.217	0.218BA3C	0.060	0.297	7.99E-7	0.03843Published as a conference paper at ICLR 2018Training Curves for different MTIsMulti-tasking instance 1 (MT1)Figure 26: Training Curves for the DUA4C agent on MT1 (6 tasks). The horizontal line representsthe Single Task Agent’s score. Agents in these experiments were trained for 300 million time stepsand required half the data and computation that would be required to train the task-specific agents(STA3C) for all the tasks.
