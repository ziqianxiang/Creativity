Table 1: Results on LeNet-5 (MNIST), showing validation error, percentage of non-pruned weightsand bit-precision per parameter. Original is our pre-trained LeNet-5. We show results after VNQtraining (without pruning and quantization, denoted by “no P&Q”) where weights were determin-istically replaced by the full-precision means θ and for VNQ training with subsequent pruning andquantization (denoted by “P&Q”). “random init.” denotes training with random weight initializa-tion (Glorot). We also show results of non-ternary or pruning-only methods (P): Deep Compression(Han et al., 2016), Soft weight-sharing (Ullrich et al., 2017), Sparse VD (Molchanov et al., 2017),Bayesian Compression (Louizos et al., 2017) and Stuctured Bayesian Pruning (Neklyudov et al.,2017).
Table 2: Results on DenseNet (CIFAR-10), showing the error on the validation set, the percentage ofnon-pruned weights and the bit-precision per weight. Original denotes the pre-trained network. Weshow results after VNQ training without pruning and quantization (weights were deterministicallyreplaced by the full-precision means θ) denoted by “no P&Q”, and VNQ with subsequent pruningand quantization denoted by “P&Q” (in the condition “(w/o 1)” we use full-precision means for theweights in the first layer and do not prune and quantize this layer).
Table 3: Comparing the effects of local reparameterization and naive MC approximation of the KLdivergence. “func. KL approx” denotes our functional approximation of the KL divergence given byEq. (16). “naive MC approx” denotes a naive Monte Carlo approximation that uses a single sampleonly. The first column of results shows the validation error after training, but without pruning andquantization (no P&Q), the next column shows results after pruning and quantization (results inbrackets correspond to the validation error without pruning and quantizing the first layer).
