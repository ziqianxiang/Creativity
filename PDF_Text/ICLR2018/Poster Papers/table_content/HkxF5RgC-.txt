Table 1: A naive implementation has limited performance; our optimizations are necessary to achievegood results. (Layer size = 1152, batch size = 4, density = 10%, #timesteps = 256.)Configuration	Speedup (vs. dense GEMM)	Bank Conflict PenaltyNaive	253×	1.3Wide Memory Load	4.28×	1.0Bank-Aware Layout	4.56×	0.3Lamport Timestamps		5.44×		0.35 ExperimentsIn this section, we describe the setup and experiments performed to show the benefits of our sparsepersistent RNN technique.
Table 2: BLEU Scores and Execution Times of Various ConfigurationsNetwork	Layer Size	Density	Eff. Size	BLEU	ms (GEMM)	ms (Persistent)	256	100%	256	21.97	1.74 二	1.11	=	362	100%	362	23.06	2.05	1.14Dense	512	100%	512	23.81	3.00	1.12	768	100%	768	24.62	3.55	1.26	1024	100%	1024	24.96	4.34	3.65	1448	100%	1448	25.18	7.21	一	256	12.5%	90	21.15	2.44	033	512	6.25%	128	23.21	2.21	0.37Sparse	768	4.17%	156	24.19	2.19	0.48	1024	4.7%	222	24.60	1.87	0.55	1024	12.5%	362	24.67	3.19	0.63	1448	4.7%	322	24.78	3.39	0.78E.	Performance ResultsWhile these accuracy results are not necessarily a surprise, a missing part of most treatments ofthis behavior is the throughput of the network on a given architecture. To show that a large, sparsenetwork is not only a good tradeoff for accuracy but also for performance, we compare differentlayers’ performance with all state-of-the-art algorithms that support each particular layer size. Inparticular, we use a dense GEMM (cuBLAS), dense persistent GEMM (cuDNN), sparse GEMM
Table 3: CER and Execution Times of Various Deep Speech 2 NetworksNetwork	Layer Size	Density	Ef. Size	CER	ms (GEMM)	ms (Persistent)	704	100%	704	14.50	3.02	0.74 =Dense	1760	100%	1760	10.67	10.80	1.11	2560	100%	2560	9.43	14.30	—	1760-	-12%-	-6l0-	12.88	3.95	072Sparse	2560	12%	887	10.59	4.70	0.89	3072	12%	1064	10.25	6.09	0.94We see the familiar pattern: for a given effective layer size, larger and sparse layers lead to moreaccurate models than small and dense layers. However, Figure 7 reveals that the dense persistentapproach is along the pareto-optimal curve in the absence of our technique. In other words, sparsemethods, though they may be faster than a dense GEMM (especially for a batch size of 1), can notout-perform a dense persistent approach. If speed is a metric of importance, then dense persistentkernels are the answer. If accuracy is the most important metric, then past work has not shown that asparse network can outperform any dense network.
