Table 1: Test performance on SQuAD. The papers are as follows: rnet (Microsoft Asia NaturalLanguage Computing Group, 2017), SEDT (Liu et al., 2017), BiDAF (Seo et al., 2017), DCN w/CoVe (McCann et al., 2017), ReasoNet (Shen et al., 2017), Document Reader (Chen et al., 2017),FastQA (Weissenborn et al., 2017), DCN (Xiong et al., 2017). The CoVe authors did not submittheir model, which we use as our baseline, for SQuAD test evaluation.
Table 2: Ablation study on the development set of SQuAD.
