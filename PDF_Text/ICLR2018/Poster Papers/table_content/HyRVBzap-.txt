Table 1: CIFAR10 test results (%) under black box at-tacks for =16. Source networks share the same ini-tialization which is different from the target networks.
Table 2: MNIST test results (%) for 20-layer ResNet models ( = 0.3*255 at test time). { R20M:standard training, R20MK: Kurakin’s adversarial training, R20MB: Bidirectional loss, R20MP :P ivot loss.} CW L∞ attack is performed with 100 test samples (10 samples per each class) and thenumber of adversarial examples with > 0.3*255 is reported. Additional details for CW attack canbe found in Appendix FModel	clean	step」l	SteP-FGSM	iter」l	iter_FGSM	CWR20M	99.6	9.7	10.3	0.0	0.0	0R20MK	99.6	96.7	94.5	89.0	60.2	46R20MB (Ours)	99.5	97.3	96.2	97.2	88.5	81R20MP (Ours)	99.5	97.1	95.7	96.9	88.9	82We tried N = 1, 2 and found not much difference between the two. We report the results with N =2 for the rest of the paper otherwise noted. When N = 2, Ldist becomes L2 loss.
Table 3: CIFAR10 test results (%) for 110-layer ResNet models. CW L∞ attack is performedwith 100 test samples (10 samples per each class) and the number of adversarial examples with> 2 or 4 is reported. {R110K: Kurakin’s, R110P : P ivot loss, R110E: Ensemble training,R110K,C: Kurakin’s and Cascade training, R110P,E: P ivot loss and Ensemble training, andR110P,C: P ivot loss and Cascade training}Model	clean	step」l		SteP-FGSM		iter_FGSM		CW			=2	e=16	€=2	€二16	€=2	€二4	€二2	€=4R110K	92.3	88.3	90.7	86.0	95.2	59.4	9.2	25	4R110P (Ours)	92.3	86.0	89.4	81.6	91.6	64.1	20.9	32	7R110E	92.3	86.3	74.3	84.1	72.9	63.5	21.1	24	6R110K,C (Ours)	92.3	86.2	72.8	82.6	66.7	69.3	33.4	20	5R110P,E (Ours)	91.3	84.0	65.7	77.6	54.5	66.8	38.3	38	16R110P,C (Ours)	91.5	85.7	76.4	82.4	69.1	73.5	42.5	27	15lation between iter_FGSM noises from networks with the same initialization. Correlation betweeniter_FGSM noises from the networks with different initialization, however, becomes lower as thenetwork is deeper as shown in figure 6 (b). Since the degree of freedom increases as the networksize increases, adversarially trained networks prone to end up with different states, thus, makingtransfer rate lower. To maximize the benefit of the cascade adversarial training, we propose to usethe same initialization for a cascade network and a source network used for iterative adversarialexamples generation.
Table 4: CIFAR10 test results (%) for 110-layer ResNet models under black box attacks (=16).
Table 5: Model descriptionsDataset	ResNet	Initialization Group	Training	ModelMNIST	20-layer	A	standard training Kurakin’s Bidirection loss P ivot loss	R20M R20MK R20MB R20MP	20-layer	B	standard training Kurakin’s Ensemble training Bidirection loss P ivot loss Kurakin’s & Cascade training P ivot loss & Ensemble training P ivot loss & Cascade training	R20 R20K R20E R20B R20P R20K,C R20P,E R20P,C		C	standard training Kurakin’s P ivot loss	R202 R20K2 R20P 2		D	standard training	R203		E	standard training	R204CIFAR10	56-layer	F	Kurakin’s P ivot loss	R56K R56P		G	Kurakin’s	R56K2		H	standard training Kurakin’s P ivot loss Ensemble training Kurakin’s & Cascade training P ivot loss & Ensemble training P ivot loss & Cascade training	R110 R110K R110P R110E R110K,C R110P,E R110P,C	110-layer	I	standard training Kurakin’s Ensemble training P ivot loss Kurakin’s & Cascade training P ivot loss & Ensemble training P ivot loss & Cascade training	R1102 R110K2 R110E2 R110P 2 R110K,C2 R110P,E2 R110P,C2		J	standard training	R1103		K	standard training	R1104Table 6: Ensemble model descriptionEnsemble models	Pre-trained modelsR20e, R20p,e, R110e, R110p,e	R203, R1103R110E2, R110P,E2	R204, R1104Table 7:	Cascade model descriptionCascade models Pre-trained modelR20k,c, R20p,c	R20P
Table 6: Ensemble model descriptionEnsemble models	Pre-trained modelsR20e, R20p,e, R110e, R110p,e	R203, R1103R110E2, R110P,E2	R204, R1104Table 7:	Cascade model descriptionCascade models Pre-trained modelR20k,c, R20p,c	R20PR110K,C , R110P,C	R110PR110K,C2, R110P,C2	R110P 212Published as a conference paper at ICLR 2018C Alternative Visualization on EmbeddingsXeaJ4=OS Ol IUωαjn≡,eXeaJ4=OS Ol IUωUJn≡,e20	40	60(a) R20, step_llXeaJ4=OS Ol IUωαjn≡,e(d) R20, step_FGSM20	40	60— True class I
Table 7:	Cascade model descriptionCascade models Pre-trained modelR20k,c, R20p,c	R20PR110K,C , R110P,C	R110PR110K,C2, R110P,C2	R110P 212Published as a conference paper at ICLR 2018C Alternative Visualization on EmbeddingsXeaJ4=OS Ol IUωαjn≡,eXeaJ4=OS Ol IUωUJn≡,e20	40	60(a) R20, step_llXeaJ4=OS Ol IUωαjn≡,e(d) R20, step_FGSM20	40	60— True class I-False class(g)	R20, random sign5Γd5b& 5,Ol-US2 2 11 -
Table 8:	CIFAR10 test results (%) under black box attacks between the network with the sameinitialization (=16)}Target	Source: StePGSM	SoUrce: iter^GSM	R20	R20k	R20p	R20	R20k	R20PR20	12.2	27.4	27.5	0.0	45.9	44.7R20K	65.7	81.5	81.8	51.5	0.0	18.2R20P	58.1	89.3	91.7	48.9	13.4	0.0Table 8 shows that black box attack between trained networks with the same initialization tends tobe more successful than that between networks with different initialization as exPlained in (Kurakinet al., 2017).
Table 9: CIFAR10 test results (%) under black box attacks for =16. {Target and Source networksare switched from the table 1}Target	Source: StePIGSM	Source: ite∏GSM	R20	R20k	R20p	R20	R20k	R20PR202	17.9	33.9	34.5	4.1	54.8	54.3R20K2	65.0	84.6	84.5	61.2	25.3	30.4R20P 2	66.4	88.2	87.2	61.6	27.7	36.1In table 9, our method (R20P 2) iS alwayS better at one-SteP and iterative black box attack fromdefended networkS (R20K, R20P) and undefended network (R20) than Kurakin’S method (R20B2).
Table 10: CIFAR10 teSt reSultS (%) for caScade networkS under black box attackS for =16. {Targetand Source: PleaSe See the model deScriPtionS in APPendix B.}T ,	Source: iter_FGSMTarget	_____________________________________	R110	R110K	R110e	R110p	R110K,C	R110P,E	R110P,CR110K2	80.5	72.7	49.3	68.0	49.6	41.0	67.9R110E2	82.7	59.1	39.5	59.6	51.5	40.3	69.6R110P 2 (Ours)	80.3	75.9	54.2	72.2	54.9	44.3	72.4R110K,C2 (Ours)	62.1	74.7	61.5	72.3	46.5	39.0	67.9R110P,E2 (Ours)	81.5	79.0	50.0	77.3	56.9	45.5	75.2R110P,C2 (Ours)	72.2	76.4	60.6	73.8	51.0	40.9	72.0In table 10, we Show black box attack accuracieS with the Source and the target networkS Switchedfrom the table 4. We alSo obServe that networkS trained with both low-level Similarity learning andcaScade/enSemble adverSarial training (R110P,C2, R110P,E2) Show better worst-case Performancethan other networks. Overall, iter_FGSM images crafted from ensemble model families (R110e,R110P,E) remain Strong on the defended networkS.
