Table 1: Baseline modelsDataset	Model	Model Size	Error RateMNIST	LeNet-5 (Lecun et al., 1998)	12Mb	0.65%CIFAR-10	(Courbariaux & Bengio, 2016)	612Mb	9.08%SVHN	(Courbariaux & Bengio, 2016)	110Mb	7.26%3.2	Execution TimeThe key contributors to the computational load of the proposed technique are back propagation (tocalculate gradients) and quantization (Algorithm 2). Both these operations can be completed inO(n) complexity. We implement the proposed quantization on Intel Core i7 CPU (3.5 GHz) withTitan X GPU performing training and quantization. The timing results of the algorithm have beensummarized in Table 2.
Table 2: Timing results for training and quantization of the benchmark models in secondsDataset	Training	QuantizationMNIST	120	300CIFAR-10	4560	4320SVHN	1920	25803.3	Quantization ResultsWe evaluate the performance of the quantization algorithm by analyzing the compression rate ofthe models and their respective error rates after each pass of quantization excluding retraining. Asdiscussed in section 2, the quantization algorithm will try to reduce parameter precisions whilemaintaining a minimum classification accuracy. Here we present the results of these experiments.
