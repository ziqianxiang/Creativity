Table 1: Validation accuracy results on learned vs. fixed classifierNetwork	Dataset	Learned	Fixed	# Params	% Fixed paramsResnet56 (He et al., 2016)	Cifar10	93.03%	93.14%	855,770	0.07%DenseNet(k=12)(Huang et al., 2017)	Cifar100	77.73%	77.67%	800,032	4.2%Resnet50 (He et al., 2016)	ImageNet	75.3%	75.3%	25,557,032	8.01%DenseNet169(Huang et al., 2017)	ImageNet	76.2%	76%	14,149,480	11.76%ShuffleNet(Zhang et al., 2017b)	ImageNet	65.9%	65.4%	1,826,555	52.56%3.1	CIFAR10/100We used the well known Cifar10 and Cifar100 datasets by Krizhevsky (2009) as an initial test-bed toexplore the idea ofa fixed classifier. Cifar10 is an image classification benchmark dataset containing50, 000 training images and 10, 000 test images. The images are in color and contain 32 Ã— 32 pixels.
Table 2: Validation perplexity resultsNetwork	Dataset	Learned	Fixed	# Params	% Fixed params2-layer LSTM (h=512)	WikiText-2	74.1	81.2	38,312,446	88.94%4	Discussion4.1	Implications to future DNN models and use casesIn the last couple of years a we observe a rapid growth in the number of classes benchmark datasetscontain, for example: Cifar100 (Krizhevsky, 2009), ImageNet1K, ImageNet22k (Deng et al., 2009)and language modeling (Merity et al., 2016). Therefore the computational demands of the finalclassifier will increase as well and should be considered no less than the architecture chosen. Weuse the work by Sun et al. (2017) as our use case, which introduced JFT-300M - an internal Googledataset with over 18K different classes. Using a Resnet50 (He et al., 2016), with a 2048 sizedrepresentation, this led to a model with over 36M parameters. This means that over 60% of themodel parameters reside in the final classification layer.
