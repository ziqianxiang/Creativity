Table 1: Results on a subset of the ALE environments in comparison to baselines taken from (Belle-mare et al., 2017)Game/Agent	Human	DQN	Prior. Dueling	C51	MPOPong	14.6	19.5	20.9	20.9	20.9Breakout	30.5	385.5	366.0	748	360.5Q*bert	13,455.0	13,117.3	18,760.3	23,784	10,317.0Tennis	-8.3	12.2	0.0	23.1	22.2Boxing	12.1	88.0	98.9	97.8	82.0Finally, using (24), we deduce thatJ (θi+1 , qi+1 )	=	Vαqi+1,πi+1(x0) Vαqi,πi(x0) +δx0(I - γPπi)-1(Tαqi+1,πi+1Vπi -Tαqi,πiVπi) + O(β2)≥	J (θi, qi) + ηgi + O(β2)≥	J (θi,qi) + 2 ηgi,for small enough η .	□B Additional Experiment: Discrete controlAs a proof of concept - showcasing the robustness of our algorithm and its hyperparameters - Weperformed an experiment on a subset of the games contained contained in the "Arcade LearningEnvironment" (ALE). For this experiment we used the same hyperparameter settings for the KLconstraints as for the continuous control experiments as well as the same learning rate and merelyaltered the network architecture to the standard network structure used by DQN Mnih et al. (2015)- and created a seperate network with the same architecture, but predicting the parameters of the
Table 2: Parameters for non-parametric variational distributionHyperparameters for MPO with parametric variational distribution were as follows,17Published as a conference paper at ICLR 2018Hyperparameter	control suite tasks	humanoidPolicy net	100-100	200-200Q function net	200-200	300-300eμ	0.1	"e∑	0.0001	"Discount factor (Y)	0.99	"Adam learning rate	0.0005	"Table 3: Parameters for parametric variational distributionD Derivation of update rules for a Gaussian PolicyFor continuous control we assume that the policy is given by a Gaussian distribution with a fullcovariance matrix, i.e, π(a∣s, θ) = N (μ, Σ). Our neural network outputs the mean μ = μ(s) andCholesky factor A = A(s), such that Σ = AAT. The lower triagular factor A has positive diagonalelements enforced by the softplus transform Aii J log(1 + exp(Aii)).
Table 3: Parameters for parametric variational distributionD Derivation of update rules for a Gaussian PolicyFor continuous control we assume that the policy is given by a Gaussian distribution with a fullcovariance matrix, i.e, π(a∣s, θ) = N (μ, Σ). Our neural network outputs the mean μ = μ(s) andCholesky factor A = A(s), such that Σ = AAT. The lower triagular factor A has positive diagonalelements enforced by the softplus transform Aii J log(1 + exp(Aii)).
