Table 1: Statistics of the datasets. #q represents the number of questions for training (not countingthe questions that don’t have ground-truth answer in the corresponding passages for training set),development, and testing datasets. #p is the number of passages for each question. For TriviaQA,we split the raw documents into sentence level passages and select the top 100 passages based on theits overlaps with the corresponding question. #p(golden) means the number of passages that containthe ground-truth answer in average. #p(aggregated) is the number of passages we aggregated inaverage for top 10 candidate answers provided by RC model.
Table 2: Experiment results on three open-domain QA test datasets: Quasar-T, SearchQA and Trivi-aQA (open-domain setting). EM: Exact Match. Full Re-ranker is the combination of three differentre-rankers.
Table 3: The upper bound (recall) of the Top-K answer candidates generated by the baseline R3system (on dev set), which indicates the potential of the coverage-based re-ranker.
Table 4: Results of running coverage-based re-ranker on different number of the top-K answercandidates on Quasar-T (dev set).
Table 5: Results of running strength-based re-ranker (counting) on different number of top-K answercandidates on Quasar-T (dev set).
Table 6: An example from Quasar-T dataset. The ground-truth answer is ”Sesame Street”. Q:question, A: answer, P: passages containing corresponding answer.
