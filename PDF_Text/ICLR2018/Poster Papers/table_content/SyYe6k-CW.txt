Table 1: Cumulative regret incurred by the algorithms in Section 3 on the bandits described inSection A. Results are relative to the cumulative regret of the Uniform algorithm. We report the meanand standard error of the mean over 50 trials.
Table 2: Detailed description of the algorithms in the experiments. Unless otherwise stated, algorithmsuse ts = 20 (mini-batches per training period), and tf = 20 (one training period every tf contexts).
Table 3: Detailed description of the algorithms in the linear experiments. Unless otherwise stated,algorithms use ts tf contexts).	= 100 (mini-batches per training period), and tf = 20 (one training period everyAlgorithm	DescriptionAlpha Divergences Alpha Divergences (1) Alpha Divergences (2) Alpha Divergences (3) BBBN BBBN2 BBBN3 BBBN4 Bootstrapped NN Bootstrapped NN2 Bootstrapped NN3 Dropout (RMS3) Dropout (RMS2) RMS1 RMS2 RMS2b RMS3 SGFS ConstSGD EpsGreedy (RMS1) EpsGreedy (RMS2) EpsGreedy (RMS3) LinDiagPost LinDiagPrecPost LinGreedy LinGreedy (eps = 0.01) LinGreedy (eps = 0.05) LinPost LinFullDiagPost LinFullDiagPrecPost LinFullPost Param-Noise Param-Noise2 Uniform	BB α-divergence with α	=	0.1, noise σ	=	0.1, K	=	10,	prior var σ02	=	0.1.	(ts	=	100, first	100 times	linear	decay from	ts	=	10000). BB α-divergence with α	=	0.5, noise σ	=	0.1, K	=	10,	prior var σ02	=	0.1.	(ts	=	100, first	100 times	linear	decay from	ts	=	10000). BB α-divergence with α	=	1.0, noise σ	=	0.1, K	=	10,	prior var σ02	=	0.1.	(ts	=	100, first	100 times	linear	decay from	ts	=	10000). BB α-divergence with α	=	0.5, noise σ	=	0.1, K	=	10,	prior var σ02	=	1.0.	(ts	=	100, first	100 times	linear	decay from	ts	=	10000). BayesByBackprop with noise σ = 0.1. (ts = 100, first 100 times linear decay from ts = 10000). BayesByBackprop with noise σ = 0.5. (ts = 100, first 100 times linear decay from ts = 10000). BayesByBackprop with noise σ = 0.75. (ts = 100, first 100 times linear decay from ts = 10000). BayesByBackprop with noise σ = 1.0. (ts = 100, first 100 times linear decay from ts = 10000). Bootstrapped with q = 5 models, and p = 0.85. Based on RMS3 net. Bootstrapped with q = 5 models, and p = 1.0. Based on RMS3 net. Bootstrapped with q = 10 models, and p = 1.0. Based on RMS3 net. Dropout with probability p = 0.8. Based on RMS3 net. Dropout with probability p = 0.8. Based on RMS2 net. Greedy NN approach, fixed learning rate (γ = 0.01). Learning rate decays, and it is reset every training period. Similar to RMS2, but training for longer (ts = 800). Learning rate decays, and it is not reset at all. Starts at γ = 1. Burning = 500, learning rate γ = 0.014, EMA decay = 0.9, noise σ = 0.75. Burning = 500, EMA decay = 0.9, noise σ = 0.5. Initial	=	0.01. Multiplied by	0.999	after every	context.	Based on RMS1	net. Initial	=	0.01. Multiplied by	0.999	after every	context.	Based on RMS2	net. Initial	=	0.01. Multiplied by	0.999	after every	context.	Based on RMS3	net. Σ in Eq. 1	is diagonalized. Ridge prior λ = 0.25.	Assumed noise level σ2	= 0.25. Σ-1 in Eq. 1 is diagonalized. Ridge prior λ = 0.25. Assumed noise level σ2 = 0.25. Takes action with highest predicted reward for Ridge regression, λ = 0.25. Noise level σ2 = 0.25. linGreedy that selects action uniformly at random with prob p = 0.01. linGreedy that selects action uniformly at random with prob p = 0.05. Ridge prior λ = 0.25. Assumed noise level σ2 = 0.25. Σ in Eq. 1 is diagonalized. Noise prior a0 = 6, b0 = 6. Ridge prior λ = 0.25. Σ-1 in Eq. 1 is diagonalized. Noise prior a0 = 6, b0 = 6. Ridge prior λ = 0.25. Noise prior a0 = 6, b0 = 6. Ridge prior λ = 0.25. Initial noise σ = 0.01, and level = 0.01. Based on RMS3 net. Initial noise σ = 0.01, and level = 0.01. Based on RMS3 net. Trained for longer: ts = 800. Takes each action at random with equal probability.
Table 4: Cumulative regret incurred by linear models using algorithms in Section 3 on the bandits described in Section A. Values reported are the mean over 50independent trials with standard error of the mean.
Table 5: Simple regret incurred by linear models using algorithms in Section 3 on the bandits described in Section A. Simple regret was approximated by averagingthe regret over the final 500 steps. Values reported are the mean over 50 independent trials with standard error of the mean.
Table 6: Cumulative regret incurred by models using algorithms in Section 3 on the bandits described in Section A. Values reported are the mean over 50 independenttrials with standard error of the mean. Normalized with respect to the performance of Uniform.
Table 7: Simple regret incurred by models using algorithms in Section 3 on the bandits described in Section A. Simple regret was approximated by averaging theregret over the final 500 steps. Values reported are the mean over 50 independent trials with standard error of the mean. Normalized with respect to the performanceof Uniform.
Table 8: Elapsed time for algorithms in Section 3 on the bandits described in Section A. Values reported are the mean over 50 independent trials with standard errorof the mean. Normalized with respect to the elapsed time required by RMS (which uses ts = 100 and tf = 20).
Table 9: Cumulative regret incurred on the Wheel Bandit problem with increasing values of δ. Values reported are the mean over 50 independent trials with standarderror of the mean. Normalized with respect to the performance of Uniform.
Table 10: Simple regret incurred on the Wheel Bandit problem with increasing values of δ. Simple regret was approximated by averaging the regret over the final 500steps. Values reported are the mean over 50 independent trials with standard error of the mean. Normalized with respect to the performance of Uniform.
