Table 1: Sparsity, parameter sharing, and compression rate results on MNIST. Baseline model istrained with weight decay and we do not enforce parameter sharing for baseline model. We traineach model for 5 times and report the average values together with their standard deviations.
Table 2: Sparsity (S1) and Parameter Sharing (S2) of VGG-16 on CIFAR-10. Layers marked by *are regularized. We report the averaged results over 5 runs.
Table 3: Network statistics of VGG-16.
Table 4: VGG: Clustering rows over different preference values for running the affinity propagationalgorithm (Algorithm 3). For each experiment, we report clustering accuracy (A), compression rate(C), and parameter sharing (S) of layers 9-14. For each regularizer, we use different preferencevalues to run Algorithm 3 to cluster the rows at the end of initial training process. Then we retrainthe neural network correspondingly. The results are reported as the averages over 5 training andretraining runs.
