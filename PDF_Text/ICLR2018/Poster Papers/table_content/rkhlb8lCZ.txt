Table 1: MNIST Performance of Pooling Methods4.2	CIFAR- 1 0We run two sets of experiments with the pooling methods. The first is a regular network structurewith no dropout layers. We use this network to observe each pooling method without extra regu-larization. The second uses dropout and batch normalization, and performs over 30 more epochsto observe the effects of these changes. Figure 9 shows our network structure for the CIFAR-10experiments:The input training and test data come from the CIFAR-10 dataset. The full training set of 50,000images is used, as well as the full testing set of 10,000 images. For both cases, with no dropout,and with dropout, Table 2 and Table 3 show our proposed method has the second highest accuracy.
Table 2: CIFAR-10 Performance of Pooling Methods	Average	Max	Mixed	Stochastic	WaveletI Accuracy (%)	81.15	80.30	79.21	80.09	80.28Table 3: CIFAR-10 Performance of Pooling Methods + Dropout4.3	SHVNWe run two sets of experiments with the pooling methods. The first is a regular network structurewith no dropout layers. We use this network to observe each pooling method without extra regular-ization. The second uses dropout to observe the effects of this change. Figure 11 shows our networkstructure for the SHVN experiments:The input training and test data come from the SHVN dataset. For the case with no dropout, we use55,000 images from the training set. For the case with dropout, we use the full training set of 73,257images, a validation set of 30,000 images we extract from the extra training set of 531,131 images,as well as the full testing set of 26,032 images. For both cases, with no dropout, and with dropout,7Published as a conference paper at ICLR 20183x 1	Batch (733)	——►	Convolution1 5x5 kernel 64 features2	―►	LR Normalization	―►	ReLU	→	Pooling 2×2 Window	—►	Dropout 0.1 rate		Convolution 4×4 kernel 128 features		Notes: 1	.) Dropout (0.1) prior on 3rd run 2	.) 128 features on 3rd run ** Remove Dropout layers for the structure of the original network			*	Predictions	.——	Softmax	——	Convolution Ixl kernel 10 features	W——	Dropout 0.2 rate		ReLU	Figure 11: CNN SHVN Structure Block DiagramTable 4 and Table 5 show our proposed method has the second lowest accuracy. Max and wavelet
Table 3: CIFAR-10 Performance of Pooling Methods + Dropout4.3	SHVNWe run two sets of experiments with the pooling methods. The first is a regular network structurewith no dropout layers. We use this network to observe each pooling method without extra regular-ization. The second uses dropout to observe the effects of this change. Figure 11 shows our networkstructure for the SHVN experiments:The input training and test data come from the SHVN dataset. For the case with no dropout, we use55,000 images from the training set. For the case with dropout, we use the full training set of 73,257images, a validation set of 30,000 images we extract from the extra training set of 531,131 images,as well as the full testing set of 26,032 images. For both cases, with no dropout, and with dropout,7Published as a conference paper at ICLR 20183x 1	Batch (733)	——►	Convolution1 5x5 kernel 64 features2	―►	LR Normalization	―►	ReLU	→	Pooling 2×2 Window	—►	Dropout 0.1 rate		Convolution 4×4 kernel 128 features		Notes: 1	.) Dropout (0.1) prior on 3rd run 2	.) 128 features on 3rd run ** Remove Dropout layers for the structure of the original network			*	Predictions	.——	Softmax	——	Convolution Ixl kernel 10 features	W——	Dropout 0.2 rate		ReLU	Figure 11: CNN SHVN Structure Block DiagramTable 4 and Table 5 show our proposed method has the second lowest accuracy. Max and waveletpooling both slightly overfit the data. Our method follows the path of max pooling, but performsslightly better in maintaining some stability. Mixed, stochastic, and average pooling maintain a slowprogression of learning, and their validation sets trend at near identical rates. Figure 12 shows the
Table 4: SHVN Performance of Pooling Methods	Average	Max	Mixed	Stochastic	WaveletI Accuracy (%)	92.80	92.18	92.13	91.04	91.10Table 5: SHVN Performance of Pooling Methods + Dropout4.4 KDEFWe run one set of experiments with the pooling methods that includes dropout. Figure 13 shows ournetwork structure for the KDEF experiments:The input training and test data come from the KDEF dataset. This dataset contains 4,900 imagesof 35 people displaying seven basic emotions (afraid, angry, disgusted, happy, neutral, sad, andsurprised) using facial expressions. They display emotions at five poses (full left and right profiles,half left and right profiles, and straight).
Table 5: SHVN Performance of Pooling Methods + Dropout4.4 KDEFWe run one set of experiments with the pooling methods that includes dropout. Figure 13 shows ournetwork structure for the KDEF experiments:The input training and test data come from the KDEF dataset. This dataset contains 4,900 imagesof 35 people displaying seven basic emotions (afraid, angry, disgusted, happy, neutral, sad, andsurprised) using facial expressions. They display emotions at five poses (full left and right profiles,half left and right profiles, and straight).
Table 6: KDEF Performance of Pooling Methods + Dropout4.5 Computational ComplexityOur construction and implementation of wavelet pooling is not efficient. We present this proposedmethods as a proof-of-concept, to show its potential and validity, and also to be open to massiveimprovements. The main area of improvement is computational efficiency. As a proof-of-concept,the code written to implement this method is not at its peak form. Additionally, we did not have9Published as a conference paper at ICLR 2018the time, space, or resources to optimize the code. We view the accuracy results and novelty as astarting point to spawn improvements, both from our own research as well as other researchers.
Table 7: Number of Mathematical Operations for Each Method According to DatasetNonetheless, by implementing our method through good coding practices (vectorization, architec-ture, etc.), GPUs, and an improved FTW algorithm, this method can prove to be a viable option.
