Table 1: Word translation retrieval P@1 for our released vocabularies in various language pairs. Weconsider 1,500 source test queries, and 200k target words for each language pair. We use fastText embeddingstrained on Wikipedia. NN: nearest neighbors. ISF: inverted softmax. (’en’ is English, ’fr’ is French, ’de’ isGerman, ’ru’ is Russian, ’zh’ is classical Chinese and ’eo’ is Esperanto)6Published as a conference paper at ICLR 2018I English to Italian Italian to EnglishI P@1 P@5 P@10 I P@1 P@5 P@10Methods with cross-lingual supervision (WaCky)Mikolov et al. (2013b) t	33.8	48.3	53.9	24.9	41.0	47.4Dinu et al. (2015)t	38.5	56.4	63.9	24.6	45.4	54.1CCAt	36.1	52.7	58.1	31.0	49.9	57.0Artetxe et al. (2017)	39.7	54.7	60.5	33.8	52.4	59.1Smith et al. (2017)t	43.1	60.7	66.4	38.0	58.5	63.6ProCrustes - CSLS	44.9	61.8	66.6	38.5	57.2	63.0Methods without cross-lingual supervision (WaCky)Adv-Refine-CSLS ∣ 45.1 6θ.7 65.1 ∣ 38.3 57.8 62.8Methods with cross-lingual supervision (Wiki)ProCrustes-CSLS	∣ 63.7 78.6	81.1	∣ 56.3	76.2	80.6Methods without cross-lingual SUPerviSiOn (Wiki)
Table 2: English-Italian wordtranslation average preCisions (@1,@5, @10) from 1.5k sourCe wordqueries using 200k target words. Re-Sults marked With the symbol t arefrom Smith et al. (2017). Wikimeans the embeddings Were trainedon Wikipedia using fastText. Notethat the method used by Artetxe et al.
Table 3: English-Italian sentencetranslation retrieval. We reportthe average P@k from 2,000 sourcequeries using 200,000 target sen-tences. We use the same embeddingsas in Smith et al. (2017). Their re-Sults are marked With the symbol ’.
Table 5: BLEU score on English-Esperanto.
Table 4: Cross-lingual wordsim task. NASARI(Camacho-Collados et al. (2016)) refers to the officialSemEval2017 baseline. We report Pearson correlation.
Table 6: Esperanto-English. Examples of fully unsuperVised word-by-word translations. The translationsreflect the meaning of the source sentences, and could potentially be improVed using a simple language model.
