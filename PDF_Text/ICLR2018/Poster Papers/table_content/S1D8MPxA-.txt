Table 1: MNIST test error and comparator threshold values with gradual pruning. Pruning is per-formed at the 50th epoch (〜50% target pruning rate), 100th epoch (〜70% target pruning rate),and 150th epoch (final). 40 VD outputs are used for FC1, while 8 VD outputs for the others.
Table 2: Sparse matrix comparison with MNIST using magnitude-based pruning (Han et al., 2015)and our proposed Viterbi-based pruning. We assume that 16 bits are used for the non-zero valuesand index for magnitude-based pruning.
Table 3: Pruning and sparse matrix size comparison for AlexNet on ImageNet using magnitude-based pruning (Han et al., 2015) and our proposed Viterbi-based pruning. We assume that 16 bitsare used for the non-zero values and index for magnitude-based pruning.
Table 4: Various configurations of the number of XOR taps, the minimum Hamming distance, thenumber of FFs, and the number of VD outputs.
Table 5: Weight size and VD parameters per layer for Viterbi-based pruning.
Table 6: Sparse matrix comparison with LeNet-300-100 on MNIST using Variational Dropout-basedpruning (Molchanov et al., 2017a) and our proposed Viterbi-based pruning. We assume that non-zero values and CSR index use 16 bits.
Table 7: Sparse matrix comparison with LeNet-5-Caffe on MNIST using Variational Dropout-basedpruning (Molchanov et al., 2017a) and our propoSed Viterbi-baSed pruning. We aSSume that non-zero valueS and CSR index uSe 16 bitS.
