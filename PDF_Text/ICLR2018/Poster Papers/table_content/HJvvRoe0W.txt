Table 1: Network Architecture. The output size is given asheight, width, channels.
Table 2: DNA DatasetsName	]Samples	Description^^3	14965	H3 occupancyH4	14601	H4 occupancyH3K9ac	27782	H3K9 acetylation relative to H3H3K14ac	33048	H3K14 acetylation relative to H3H4ac	34095	H4 acetylation relative to H3H3K4me1	31677	H3K4 monomethy- lation relative to H3H3K4me2	30683	H3K4 dimethylation relative to H3H3K4me3	36799	H3K4 trimethylation relative to H3H3K36me3	34880	H3K36 trimethylation relative to H3H3K79me3	28837	H3K79 trimethylation relative to H3Splice	3190	Splice-junction Gene Sequences3	ExperimentsWe test the performance of our approach using ten publicly available datasets from Pokholok et al.
Table 3: Prediction accuracy obtained with an SVM-based method, Seq-CNN from Nguyen et al. (2016),LSTM, seq-HCNN and HCNN. The results for SVM are taken from Table 12 in Higashihara et al. (2008).
Table 4: Training times, presented as min:sec.
Table 5: Recall, Precision, area under precision-recall curve (AP) and area under ROC curve (AUC) forseq-HCNN and HCNN. The reported values are the means over ten folds.
Table 6: Γ(C) for four space-filling curves,evaluated in sequences of varying lengths.
Table 7: The model architecture of seq-CNN (Nguyen et al.,	Table 8: Bir-Direction LSTM2016)Layer	DescriptionEmbedding	Conv 1	1-by-3 convolution layer with 32 filter size	activation function is RELUMax pooling	1 X 2 max pooling layerBir-LSTM 1	100 unitsBir-LSTM 2	128 unitsDropout	0.3 dropout rateClassifier	sigmoidArchitecture	# Parameters-Seq-CNN-	1ΓΓm-biLSTM-	455KHilbert-CNV	961K	—Table 9: Table with the number of network parametersD Hyperparameter optimizationAccuracy is one of the most intuitive performance measurements in deep learning and machinelearning. We therefore optimized the hyperparameters such as the network architecture and learn-ing rate based on maximum accuracy. The hyperparameters are optimized through random search
Table 9: Table with the number of network parametersD Hyperparameter optimizationAccuracy is one of the most intuitive performance measurements in deep learning and machinelearning. We therefore optimized the hyperparameters such as the network architecture and learn-ing rate based on maximum accuracy. The hyperparameters are optimized through random search(Bergstra & Bengio, 2012) and using general principles from successful deep learning strategies andthe following intuition. First, as our main goal was to capture long-term interactions, we chose alarge kernel size for the first layer, for which various values were attempted and 7x7 gave the bestperformance. As is common practice in deep learning, we then opted for a smaller kernel size in thefollowing layer. Second, in order to limit the number of parameters, we made use of residual blocksinspired by ResNet (He et al., 2015) and Inception (Szegedy et al., 2015). Finally, we applied batchnormalization to prevent overfitting.
