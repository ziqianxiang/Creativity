Table 1: Comparison between answers in original sentence and paraphrased sentence.
Table 2: The performances of different models on SQuAD dataset.
Table 3: Speed comparison between our model and RNN-based models on SQuAD dataset, all withbatch size 32. RNN-x-y indicates an RNN with x layers each containing y hidden units. Here, weuse bidirectional LSTM as the RNN. The speed is measured by batches/second, so higher is faster.
Table 4: Speed comparison between our model and BiDAF (Seo et al., 2016) on SQuAD dataset.
Table 5: An ablation study of data augmentation and other aspects of our model. The reported resultsare obtained on the development set. For rows containing entry “data augmentation"，“x N" meansthe data is enhanced to N times as large as the original size, while the ratio in the bracket indicatesthe sampling ratio among the original, English-French-English and English-German-English dataduring training.
Table 6: The F1 scores on the adversarial SQuAD test set.
Table 7: The development set performances of different single-paragraph reading models on theWikiPedia domain of TriviaQA dataset. Note that * indicates the result on test set.
Table 8: SPeed comParison between the ProPosed model and RNN-based models on TriviaQAWikiPedia dataset, all with batch size 32. RNN-x-y indicates an RNN with x layers each containingy hidden units. The RNNs used here are bidirectional LSTM. The Processing sPeed is measured bybatches/second, so higher is faster.
