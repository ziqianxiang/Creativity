Table 1: The components of the VoiceLooP modelSymbolDescriptionComputed as:soIqEμυΛsdR∈∈∈∈StutEzRl Rdp Rdo∈∈∈αtctobuffer at time tnew representation for the bufferembedding of the input sequenceembedding of the current speakerattention model parametersattention GMM parametersattention vector at time t
Table 2: Single Speaker MOS Scores (Mean ± SD)Method	LJ	Blizzard 2011	Blizzard 2013Tacotron (re-impl)	2.06 ± 1.02	2.15 ± 1.10	N/AChar2wav	3.42 ± 1.14	3.33 ± 1.06	2.03 ± 1.16VoiceLoop	3.69 ± 1.04	3.38 ± 1.00	3.40 ± 1.03Ground truth	4.60 ± 0.71	4.56 ± 0.67	4.80 ± 0.50Table 3: Single Speaker MCD Scores (Mean ± SD; lower is better)Method	LJ	Blizzard 2011	Blizzard 2013Tacotron (re-impl)	12.82 ± 1.41	14.60 ± 7.02	N/AChar2wav	19.41 ± 5.15	13.97 ± 4.93	18.72 ± 6.41VoiceLoop	14.42 ± 1.39	8.86 ± 1.22	8.67 ± 1.264.2	Multi- S peaker experimentsMulti-speaker experiments were performed on the VCTK dataset (Veaux et al., 2017). The 109speakers were divided into four different nested subsets: 22 North American speakers, both male andfemales; and 65, 85 and 101 random selection of speakers, where the remaining eight speakers wereleft out for validation. Each subset was shuffled into train and test sets. Different models were trainedto each of the subsets. Qualitatively, the models provide distinguished voices, and as can be seen inFig. 3, the generated voice samples display a different dynamic behavior for different speakers.
Table 3: Single Speaker MCD Scores (Mean ± SD; lower is better)Method	LJ	Blizzard 2011	Blizzard 2013Tacotron (re-impl)	12.82 ± 1.41	14.60 ± 7.02	N/AChar2wav	19.41 ± 5.15	13.97 ± 4.93	18.72 ± 6.41VoiceLoop	14.42 ± 1.39	8.86 ± 1.22	8.67 ± 1.264.2	Multi- S peaker experimentsMulti-speaker experiments were performed on the VCTK dataset (Veaux et al., 2017). The 109speakers were divided into four different nested subsets: 22 North American speakers, both male andfemales; and 65, 85 and 101 random selection of speakers, where the remaining eight speakers wereleft out for validation. Each subset was shuffled into train and test sets. Different models were trainedto each of the subsets. Qualitatively, the models provide distinguished voices, and as can be seen inFig. 3, the generated voice samples display a different dynamic behavior for different speakers.
Table 4: Multi-speaker MOS scores (Mean ± SE)Method	VCTK22	VCTK65	VCTK85	VCTK101Char2wav	2.84 ± 1.20	2.85 ± 1.19	2.76 ± 1.19	2.66 ± 1.16VoiceLoop	3.57 ± 1.08	3.40 ± 1.00	3.13 ± 1.17	3.33 ± 1.10GT	4.61 ± 0.75	4.59 ± 0.72	4.64 ± 0.64	4.63 ± 0.66In our experiments, we employ the author’s implementation of Char2Wav mentioned above asbaseline. Note that while the Char2Wav paper did not present multi-speaker results, the openimplementation is more general and includes this option.
Table 5: Multi-speaker MCD scores (Mean ± SE; lower is better)Method	VCTK22	VCTK65	VCTK85	VCTK101Char2wav	15.71 ± 1.82	15.1 ± 1.45	15.23 ± 1.49	15.06 ± 1.32VoiceLoop	13.74 ± 0.98	14.1 ± 0.94	14.16 ± 0.87	14.22 ± 0.88Table 6: Multi-Speaker Identification Top-1 Accuracy (%)Method	VCTK85	VCTK101VCTKteSt SPlit	98.25	97.16Char2Wav on test split sentences	75.70	81.63VoiceLoop on test split sentences	100	99.764.3	New speaker fitting experimentsOur system is the only published system that is capable of post-training fitting of new speakers. Inorder to experiment with this capability, we employ the VoiceLoop model trained on VCTK85 andexperiment on the remaining 16 speakers one by one, where only the speaker embedding z getsupdated. While TTS systems typically require several hours of data to model a single speaker (Zenet al., 2009), our fitting set contains only 23.65 minutes per speaker on average.
Table 6: Multi-Speaker Identification Top-1 Accuracy (%)Method	VCTK85	VCTK101VCTKteSt SPlit	98.25	97.16Char2Wav on test split sentences	75.70	81.63VoiceLoop on test split sentences	100	99.764.3	New speaker fitting experimentsOur system is the only published system that is capable of post-training fitting of new speakers. Inorder to experiment with this capability, we employ the VoiceLoop model trained on VCTK85 andexperiment on the remaining 16 speakers one by one, where only the speaker embedding z getsupdated. While TTS systems typically require several hours of data to model a single speaker (Zenet al., 2009), our fitting set contains only 23.65 minutes per speaker on average.
