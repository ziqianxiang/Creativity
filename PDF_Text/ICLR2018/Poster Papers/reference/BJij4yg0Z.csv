title,year,conference
 On the emergence of invariance and disentangling in deeprepresentations,2017, arXiv preprint arXiv:1706
 Sample size selection in opti-mization methods for machine learning,2012, Mathematical programming
 Entropy-SGD: Biasinggradient descent into wide valleys,2016, arXiv preprint arXiv:1611
 Automated inference with adaptivebatches,2017, In Artificial Intelligence and Statistics
 Sharp minima can generalizefor deep nets,2017, arXiv preprint arXiv:1703
 PAC-bayesian theorymeets bayesian inference,2016, In Advances in Neural Information Processing Systems
 Flat minima,1997, Neural Computation
 Three factors influencing minima in SGD,2017, arXiv preprintarXiv:1711
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Deep nets donâ€™t learn via mem-orization,2017, ICLR Workshop
 Stochastic modified equations and adaptive stochasticgradient algorithms,2017, In International Conference on Machine Learning
 Stochastic gradient descent as approximatebayesian inference,2017, arXiv preprint arXiv:1704
 Opening the black box of deep neural networks via informa-tion,2017, arXiv preprint arXiv:1703
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
