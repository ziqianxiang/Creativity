title,year,conference
 Vapnik-chervonenkis dimension of neural nets,2003, The handbook of brain theory andneural networks
 The loss surfaces of multilayer networks,2015, InAISTATS
 Open problem: The landscape of the loss surfaces of multilayernetworks,2015, In COLT
 A unified architecture for natural language processing: Deep neural networks withmultitask learning,2008, In ICML
 Identifying and attacking the saddlepoint problem in high-dimensional non-convex optimization,2014, In NIPS
 Escaping from saddle pointsâ€”online stochastic gradient for tensordecomposition,2015, In COLT
 Fast rates for empirical risk minimization of strict saddle problems,2017, COLT
 Speech recognition with deep recurrent neural networks,2013, In ICASSP
 On differentiable functions with isolated critical points,1969, Topology
 Deep residual learning for image recognition,2016, In CVPR
 A fast learning algorithm for deep belief nets,2006, Neural Computation
 Deep learning without poor local minima,2016, In NIPS
 Regularized m-estimators with nonconvexity: Statistical and algorithmic theoryfor local optima,2015, JMLR
 The landscape of empirical risk for non-convex losses,2017, Annals of Statistics
 A unified framework for high-dimensional analysis ofM-estimators with decomposable regularizers,2009, In NIPS
 A unified framework for high-dimensional analysis ofm-estimators with decomposable regularizers,2011, In NIPS
 The loss surface of deep and wide neural networks,2017, In ICML
 Understanding machine learning: From theory to algorithms,2014, CambridgeUniv
 Understanding machine learning: From theory to algorithms,2014, Cambridgeuniversity press
 Failures of deep learning,2017, ICML
 No bad local minima: Data independent training error guarantees for multilayerneural networks,2016, arXiv preprint arXiv:1605
 An analytical formula of population gradient for two-layered relu network and its applications inconvergence and critical point analysis,2017, ICML
 Robustness and generalization,2012, Machine Learning
 `1 -regularized neural netWorks are improperly learnable in polynomial time,2016, InICML
 Convexified convolutional neural netWorks,2017, ICML
1 Proof of Lemma 13Lemma 15,2015, (Rigollet
4 Proof of Lemmas 19 and 20Lemma 25,2015, (Alessandro
