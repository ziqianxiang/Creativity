title,year,conference
 Second order stochastic optimization in lineartime,2016, Optimization Methods for the Next Generation of Machine Learning workshop
 Optimization methods for large-scale machinelearning,2016, arXiv preprint arXiv:1606
 Gradient descent efficiently finds the cubic-regularized non-convexnewton step,2016, arXiv preprint arXiv:1612
 Entropy-sgd: Bias-ing gradient descent into wide valleys,2016, CoRR
 Identifying and attacking the saddle point problem in high-dimensional non-convex op-timization,2014, In Advances in neural information processing systems
 Saga: A fast incremental gradient methodwith support for non-strongly convex composite objectives,2014, In Advances in Neural InformationProcessing Systems
 A kronecker-factored aPProximate fisher matrix for convolutionlayers,2016, In International Conference on Machine Learning
 Identity mappings in deep residualnetworks,2016, In European Conference on Computer Vision
 Rmsprop: Divide the gradient by a running average of itsrecent magnitude,2012, Neural networks for machine learning
 Accelerating stochastic gradient descent using predictive variancereduction,2013, In Advances in neural information processing systems
 adaqn: An adaptive quasi-newton algorithm for train-ing rnns,2016, In Joint European Conference on Machine Learning and Knowledge Discovery inDatabases
 Adam: A method for stochastic optimization,2015, InternationalConference for Learning Representations
 Deep learning,2015, Nature
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, In International Conference on Machine Learning
 Training deep and recurrent networks with hessian-free opti-mization,2012, In Neural networks: Tricks of the trade
 Cubic regularization of newton method and its global perfor-mance,2006, Mathematical Programming
 Fast exact multiplication by the hessian,1994, Neural computation
 Eigenvalues of the hessian in deep learning: Singu-larity and beyond,2016, 2016
 A stochastic quasi-newton method for onlineconvex optimization,2007, In Artificial Intelligence and Statistics
 Fast large-scale optimization by unifyingstochastic gradient and quasi-newton methods,2014, In International Conference on Machine Learning
 Rethink-ing the inception architecture for computer vision,2016, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 Krylov subspace descent for deep learning,2012, In Artificial Intelligenceand Statistics
 Tacotron: Towards end-to-endspeech synthesis,2017, Interspeech
 Imagenet training in 24minutes,2017, arXiv preprint arXiv:1709
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
