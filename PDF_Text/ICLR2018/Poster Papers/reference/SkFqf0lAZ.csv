title,year,conference
 Frustratingly shortattention spans in neural language modeling,2017, In Proc
 A theoretically grounded application of dropout in recurrentneural networks,2016, In Proc
 Improving neural language models with acontinuous cache,2017, In Proc
 Adaptive computation time for recurrent neural networks,2017, arXiv preprint
 Learning totransduce with unbounded memory,2015, In Proc
 Long short-term memory,1997, Neural Computation
 Tying word vectors and word classifiers: Aloss framework for language modeling,2017, In Proc
 Variable computation inrecurrent neural networks,2017, In Proc
 Inferring algorithmic patterns with stack-augmented recurrentnets,2015, arXiv preprint
 Dynamic evaluation of neuralsequence models,2017, arXiv preprint
 Assessing the ability of lstms to learn syntax-sensitive dependencies,2016, Transactions of the Association for Computational Linguistics
 On the state of the art of evaluation in neural languagemodels,2017, arXiv preprint
 Regularizing and optimizing lstmlanguage models,2017, arXiv preprint
 Pointer sentinel mixturemodels,2017, In Proc
 In Proc,2017, ofICLR
