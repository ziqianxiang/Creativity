title,year,conference
 Natural gradient works efficiently in learning,1998, Neural COmputatiOn
 Adam: A method for stochastic optimization,2015, In ICLR
 Distributed second-order optimization usingkronecker-factored approximations,2017, In InternatiOnal COnference On Learning RepresentatiOns(ICLR’2017)
 Layer normalization,2016, arXiv preprintarXiv:1607
 Metric-free natu-ral gradient for joint-training of boltzmann machines,2013, In InternatiOnal COnference On LearningRepresentatiOns (ICLR’2013)
 Natural neuralnetworks,2015, In Advances in Neural InfOrmatiOn PrOcessing Systems
 Early stopping as nonparametric variationalinference,2016, In Artificial Intelligence and Statistics
 Hybrid computing using a neural network with dynamic external memory,2016, Nature
 Scaling up natural gradient by factorizing fisher informa-tion,2015, In PrOceedings Of the 32nd InternatiOnal COnference On Machine Learning (ICML)
 On “natural” learning and pruning in multilayered perceptrons,2000, Neural COmputatiOn
 Long short-term memory,1997, Neural COmputatiOn
 Batch normalization: Accelerating deep network training by re-ducing internal covariate shift,2015, In Proceedings of The 32nd International Conference on MachineLearning
 Improving generalization performance by switching fromadam to sgd,2017, arXiv preprint arXiv:1712
 Topmoumoute online natural gra-dient algorithm,2008, In Advances in Neural Information Processing Systems 20
 Practical riemannian neural networks,2016, 2016
 Building a large annotatedcorpus of english: The penn treebank,1993, Computational linguistics
 Deep learning via Hessian-free optimization,2010, In Proceedings of the 27th InternationalConference on Machine Learning (ICML)
 Learning recurrent neural networks with Hessian-free optimization,2011, InProceedings of the 28th International Conference on Machine Learning (ICML)
 New insights and perspectives on the natural gradient method,2014, 2014
 SUbWord lan-guage modeling with neural networks,2012, 2012
 Numerical optimization,2006, Springer
 Revisiting natural gradient for deep networks,2014, In InternationalConference on Learning Representations (ICLR)
 Parallel training of DNNs with naturalgradient and parameter averaging,2015, In International Conference on Learning Representations:Workshop track
 Centering neural network gradient factors,1998, In Genevieve B
 Lecture 6,2012,5—RmsProp: Divide the gradient by a running average of itsrecent magnitude
 Pushing stochastic gradient towardssecond-order methods - backpropagation learning with transformations in nonlinearities,2013, 2013
 Memory networks,2014, arXiv preprintarXiv:1410
