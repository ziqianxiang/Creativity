title,year,conference
 Combining labeled and unlabeled data with co-training,1998, In Proceedingsof the Eleventh Annual Conference on Computational Learning Theory
 Semi-Supervised Learning,2006, The MITPress
 Adversarial feature learning,2017, In ICLR2017
 Classification in the presence oflabel noise: a survey,2014, IEEEtransactions on neural networks and learning systems
 Training deep neural-networks using a noise adaptationlayer,2017, In ICLR2017
 Global distant supervision for relation extraction,2016, In AAAI’16
 Scalable variational gaussianprocess classification,2015, In Proceedings of AISTATS
 Distilling the knowledge in a neural network,2014, In NIPS2014 Deep Learning Workshop
 A fast learning algorithm for deep beliefnets,2006, Neural Comput
 Adam: A method for stochastic optimization,2015, In ICLR
 Pseudo-label: The simple and efficient semi-supervised learning method for deepneural networks,2013, In Workshop on Challenges in Representation Learning
 Unifying distillation andprivileged information,2016, In ICLR’16
 Decoupling” when to update” from” how to update”,2017, InNIPS2017
 Distributed Representationsof Words and Phrases and their Compositionality,2013, In NIPS ’13
 Distant supervision for relation extractionwithout labeled data,2009, In ACL
 Unsupervised learning of visual representations by solving jigsawpuzzles,2016, In European Conference on Computer Vision
 Learning a deep hybrid model for semi-supervised text classification,2015, In Proceedings of the 2015 Conference on Empirical Methods inNatural Language Processing (EMNLP)
 Semi-supervisedknowledge transfer for deep learning from private training data,2017, In ICLR
 A picture of search,2006, In InfoScale ’06
 Making neuralnetworks robust to label noise: a loss correction approach,2017, In CVPR
 Estimation from indirect supervisionwith linear moments,2016, In International Conference on Machine Learning
 Holoclean: Holistic data repairswith probabilistic inference,2017, PVLDB
 Response-based learning for grounded machinetranslation,2014, In ACL (1)
 Semi-supervised self-training of objectdetection models,2005, In Seventh IEEE Workshop on Applications of Computer Vision
 Incidental supervision: Moving beyond supervised learning,2017, In AAAI
 Twitter sentiment analysis with deep convolutionalneural networks,2015, In Proceedings of the 38th International ACM SIGIR Conference on Research andDevelopment in Information Retrieval
 Unitn: Training deep convolutional neural network fortwitter sentiment classification,2015, In Proceedings of the 9th International Workshop on SemanticEvaluation (SemEval 2015)
 Fast gaussian process regression using kd-trees,2006, InAdvances in neural information processing systems
 Label-free supervision of neural networks with physics anddomain knowledge,2017, In AAAI
 Trainingconvolutional networks with noisy labels,2015, In Workshop contribution at ICLR 2015
 Variational learning of inducing variables in sparse gaussian processes,2009, InInternational Conference on Artificial Intelligence and Statistics
 Socraticlearning: Correcting misspecified generative models using discriminative models,2017, arXiv preprintarXiv:1610
 Learningfrom noisy large-scale datasets with minimal supervision,2017, In The Conference on Computer Visionand Pattern Recognition
 Privacy aware learning,2012, In Advances inNeural Information Processing Systems
 Deep learning via semi-supervised embedding,2012, In Neural Networks: Tricks of the Trade
 FWL makes use of information from a small setof correctly labeled data to improve the performance of a semi-supervised learning algorithm,1998, The main ideabehind LUPI comes from the fact that humans learn much faster than machines
