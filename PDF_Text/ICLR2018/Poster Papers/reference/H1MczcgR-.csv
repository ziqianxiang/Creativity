title,year,conference
 Natural gradient works efficiently in learning,0899, Neural Computation
 Learning to learn by gradient descent by gradientdescent,2016, In Advances in Neural Information Processing Systems 29: Annual Conference onNeural Information Processing Systems 2016
 Model-agnostic meta-learning for fast adaptationof deep networks,2017, In ICML
 Practical variational inference for neural networks,2011, In Advances in Neural InformationProcessing Systems 24: 25th Annual Conference on Neural Information Processing Systems 2011
 Caffe: Convolutional architecture for fast featureembedding,2014, In Proceedings of the 22Nd ACM International Conference on Multimedia
 Adam: A method for stochastic optimization,2015, In 3thInternational Conference on Learning Representations
 Understanding black-box predictions via influence functions,2017, InInternational Conference on Machine Learning (ICML)
 Learning multiple layers of features from tiny images,2009, Technical report
 Gradient-based learning applied todocument recognition,0018, Proceedings of the IEEE
 Learning to optimize,2017, In 5th International Conference on LearningRepresentations
 Sharpness in rates of convergence for CG and symmetric lanczos methods,2005, Technicalreport
 Learning gradient descent: Better generalization and longerhorizons,2017, In Proceedings of the 34th International Conference on Machine Learning
 A practical bayesian framework for backpropagation networks,0899, NeuralComput
 Gradient-based hyperparameteroptimization through reversible learning,2015, In Proceedings of the 32nd International Conference onMachine Learning
 New insights and perspectives on the natural gradient method,2014, arXiv:1412
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, In ICML
 Unrolled generative adversarialnetworks,2017, In 5th International Conference on Learning Representations
 Fast exact multiplication by the hessian,0899, Neural Computation
 Optimization as a model for few-shot learning,2017, In 5thInternational Conference on Learning Representations
 No more pesky learning rates,2013, In Proceedings of the30th International Conference on Machine Learning
 Local gain adaptation in stochastic gradient descent,1999, In 1999 NinthInternational Conference on Artificial Neural Networks ICANN 99
 Fast curvature matrix-vector products for second-order gradient de-scent,0899, Neural Computation
 Cyclical learning rates for training neural networks,2017, In 2017 IEEE Winter Conferenceon Applications of Computer Vision (WACV)
 Bayesian learning via stochastic gradient langevin dynamics,2011, InProceedings of the 28th International Conference on Machine Learning
 Backpropagation: past and future,1988, IEEE 1988 International Conference on NeuralNetworks
 Learned optimizers that scale andgeneralize,2017, In Proceedings of the 34th International Conference on Machine Learning
 Noisy natural gradientas variational inference,2017, CoRR
