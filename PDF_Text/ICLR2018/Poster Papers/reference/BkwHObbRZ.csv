title,year,conference
 Provable bounds for learning some deep represen-tations,2014, In International Conference on Machine Learning
 Neural networks and principal component analysis: Learning from exampleswithout local minima,1989, Neural networks
 On the low-rank approach for semidefiniteprograms arising in synchronization and community detection,2016, arXiv preprint arXiv:1602
 Global optimality of local search for low rankmatrix recovery,2016, In Advances in Neural Information Processing Systems
 Escaping from saddle pointsonline stochastic gradient fortensor decomposition,2015, In Conference on Learning Theory
 Matrix completion has no spurious local minimum,2016, Advances inNeural Information Processing Systems (NIPS)
 No spurious local minima in nonconvex low rank problems: A unifiedgeometric analysis,2017, arXiv preprint arXiv:1704
 Identity matters in deep learning,2017, In 5th International Conference on LearningRepresentations (ICLR 2017)
 Gradient descent learns linear dynamical systems,2016, CoRR
 Deep residual learning for image recognition,2016, InProceedings of the IEEE conference on computer vision and pattern recognition
 Identity mappings in deep residual networks,2016, InEuropean Conference on Computer Vision
 Batch normalization: Accelerating deep network training by reducinginternal covariate shift,2015, In International Conference on Machine Learning
 Beating the perils of non-convexity: Guaranteedtraining of neural networks using tensor methods,2015, arXiv preprint arXiv:1506
 How to escape saddle pointsefficiently,2017, arXiv preprint arXiv:1703
 Deep learning without poor local minima,2016, In Advances in Neural Information ProcessingSystems
 Convergence analysis of two-layer neural networks with relu activation,2017, arXivpreprint arXiv:1705
 The landscape of empirical risk for non-convex losses,2016, arXivpreprint arXiv:1607
 Analysis of boolean functions,2014, Cambridge University Press
 No bad local minima: Data independent training error guarantees for multi-layer neural networks,2016, arXiv preprint arXiv:1605
 Weighted low-rank approximations,2013, In ICML
 An analytical formula of population gradient for two-layered relu network and its applicationsin convergence and critical point analysis,2017, arXiv preprint arXiv:1703
 Electron-proton dynamics in deep learning,2017, CoRR
 l1-regularized neural networks are improperly learnable inpolynomial time,2016, In International Conference on Machine Learning
 On the learnability of fully-connectedneural networks,2017, In Artificial Intelligence and Statistics
 Recovery guarantees for one-hidden-layer neural networks,2017, arXiv preprint arXiv:1706
 In the setting of Proposition B,2018,6
