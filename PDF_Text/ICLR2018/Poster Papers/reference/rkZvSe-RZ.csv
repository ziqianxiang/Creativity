title,year,conference
 Con-crete problems in ai safety,2016, arXiv preprint arXiv:1606
 Obfuscated gradients give a false sense ofsecurity: Circumventing defenses to adversarial examples,2018, arXiv preprint arXiv:1802
 Adversarial transformation networks: Learning to generate adver-sarial examples,2017, arXiv preprint arXiv:1703
 Comment on” biologically inspired protection of deep net-works from adversarial attacks”,2017, arXiv preprint arXiv:1704
 Thermometer encoding: One hotway to resist adversarial examples,2018, In International Conference on Learning Representations
 Towards evaluating the robustness of neural networks,2017, In IEEESymposium on Security and Privacy
 Adversarial examples are not easily detected: Bypassing tendetection methods,2017, arXiv preprint arXiv:1705
 Zoo: Zeroth order opti-mization based black-box attacks to deep neural networks without training substitute models,2017, InProceedings ofthe 10th ACM Workshop on Artificial Intelligence and Security
 Parsevalnetworks: Improving robustness to adversarial examples,2017, arXiv preprint arXiv:1704
 CRC handbook of combinatorial designs,2010, CRC press
 ImageNet: A Large-Scale HierarchicalImage Database,2009, In CVPR09
 A rotation and atranslation suffice: Fooling cnns with simple transformations,2017, arXiv preprint arXiv:1712
 Explaining and harnessing adversarialexamples,2014, arXiv preprint arXiv:1412
 Towards deep neural network architectures robust to adversarialexamples,2014, arXiv preprint arXiv:1412
 Provable defenses against adversarial examples via the convex outeradversarial polytope,2017, arXiv preprint arXiv:1711
 Adversarial examples in the physical world,2017, InICLR
 Adversarial machine learning at scale,2017, In ICLR
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Delving into transferable adversarial exam-ples and black-box attacks,2017, In ICLR
 Foveation-based mechanismsalleviate adversarial examples,2015, arXiv preprint arXiv:1511
 Domain adaptation: Learning boundsand algorithms,2009, arXiv preprint arXiv:0902
 Systematic evaluation of convolution neuralnetwork advances on the imagenet,2017, Computer Vision and Image Understanding
 Biologically inspired protection of deep networks from adversarialattacks,2017, arXiv preprint arXiv:1703
 Towards the science ofsecurity and privacy in machine learning,2016, arXiv preprint arXiv:1611
 Distillation as adefense to adversarial perturbations against deep neural networks,2016, In Security and Privacy (SP)
 Practical black-box attacks against machine learning,2017, In Asia Conference on Computerand Communications Security (ASIACCS)
 Certified defenses against adversarialexamples,2018, In International Conference on Learning Representations
 Certifiable distributional robustness withprincipled adversarial training,2018, In International Conference on Learning Representations
 Intriguing properties of neural networks,2013, arXiv preprint arXiv:1312
 Rethinkingthe inception architecture for computer vision,2016, In CVPR
 Stealing machinelearning models via prediction apis,2016, In Usenix Security
 The spaceof transferable adversarial examples,2017, arXiv preprint arXiv:1704
 Mitigating adversarialeffects through randomization,2018, In International Conference on Learning Representations
 Generalization bounds for domain adaptation,2012, In Advancesin neural information processing Systems
