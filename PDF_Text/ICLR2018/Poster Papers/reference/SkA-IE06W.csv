title,year,conference
 Training a 3-node neural network is np-complete,1989, In Advancesin neural information processing Systems
 Globally optimal gradient descent for a convnet with gaussianinputs,2017, arXiv preprint arXiv:1702
 Theloss surfaces of multilayer networks,2015, In Artificial Intelligence and Statistics
 Language modeling with gatedconvolutional networks,2016, arXiv preprint arXiv:1612
 Gradientdescent can take exponential time to escape saddle points,2017, arXiv preprint arXiv:1705
 Globally optimal training of generalizedpolynomial neural networks with nonlinear spectral methods,2016, In Advances in Neural InformationProcessing Systems
 Escaping from saddle pointsonline stochasticgradient for tensor decomposition,2015, In Proceedings of The 28th Conference on Learning Theory
 Learning depth-three neural networks in polynomial time,2017, arXivpreprint arXiv:1709
 Reliably learning the relu in polyno-mial time,2016, arXiv preprint arXiv:1611
 Identity matters in deep learning,2016, arXiv preprint arXiv:1611
 Beating the perils of non-convexity: Guar-anteed training of neural networks using tensor methods,2015, arXiv preprint arXiv:1506
 How to escapesaddle points efficiently,2017, arXiv preprint arXiv:1703
 Deep learning without poor local minima,2016, In Advances in Neural InformationProcessing Systems
 Imagenet classification with deep convo-Iutional neural networks,2012, In Advances in neural information processing Systems
 Gradient descent onlyconverges to minimizers,2016, In Conference on Learning Theory
 Network in network,2013, arXiv preprint arXiv:1312
 An adaptive accelerated proximal gradient method and its homotopycontinuation for sparse optimization,2014, In International Conference on Machine Learning
 On the computational efficiency of trainingneural networks,2014, In Advances in Neural Information Processing Systems
 V-net: Fully convolutional neural net-works for volumetric medical image segmentation,2016, In 3D Vision (3DV)
 The loss surface of deep and wide neural networks,2017, arXivpreprint arXiv:1704
 Provable methods for training neural networks with sparseconnectivity,2014, arXiv preprint arXiv:1412
 Weight sharing is crucial to succesfuloptimization,2017, arXiv preprint arXiv:1706
 Distribution-specific hardness of learning neural networks,2016, arXiv preprintarXiv:1609
 Masteringthe game of go with deep neural networks and tree search,2016, Nature
 Training a single sigmoidal neuron is hard,2002, Neural Computation
 Learning relus via gradient descent,2017, arXiv preprint arXiv:1705
 Theoretical insights into the optimiza-tion landscape of over-parameterized shallow neural networks,2017, arXiv preprint arXiv:1707
 An analytical formula of population gradient for two-layered relu network and itsapplications in convergence and critical point analysis,2017, arXiv preprint arXiv:1703
 Learning halfspaces andneural networks with random initialization,2015, arXiv preprint arXiv:1511
 l1-regularized neural networks are improperlylearnable in polynomial time,2016, In International Conference on Machine Learning
 Recovery guaranteesfor one-hidden-layer neural networks,2017, arXiv preprint arXiv:1706
 The landscape of deep learning algorithms,2017, arXiv preprintarXiv:1705
