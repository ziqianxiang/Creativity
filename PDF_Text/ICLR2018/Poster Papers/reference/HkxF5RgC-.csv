title,year,conference
 Deep speech 2: End-to-end speech recognition inEnglish and Mandarin,2015, arXiv preprint arXiv:1512
 Optimizing performance of recurrent neural networks onGPUs,2016, CoRR
 Neural machine translation by jointly learning toalign and translate,2014, CoRR
 Compressing neuralnetworks with the hashing trick,2015, arXiv preprint arXiv:1504
 Empirical evaluation of gatedrecurrent neural networks on sequence modeling,2014, CoRR
 Compressing deep convolutional networks usingvector quantization,2014, arXiv preprint arXiv:1412
 Dynamic network surgery for efficient dnns,2016, In Advances In NeuralInformation Processing Systems
 ESE: Efficient speech recognition engine with compressedLSTM on FPGA,2016, CoRR
 EIE:Efficient inference engine on compressed deep neural network,2016, In Proceedings of the 43rd InternationalSymposium on Computer Architecture
 DSD: Regularizing deep neural networks with dense-sparse-dense training flow,2017, InInternational Conference on Learning Representations
 Deep speech: Scaling up end-to-end speechrecognition,2014, arXiv
 Long short-term memory,1997, Neural computation
 Deep visual-semantic alignments for generating image descriptions,2014, CoRR
 A new solution of Dijkstraâ€™s concurrent programming problem,0001, Commun
 Optimal brain damage,1990, In Advances in Neural InformationProcessing Systems
 Effective approaches to attention-based neuralmachine translation,2015, arXiv preprint arXiv:1508
 Recurrent neuralnetwork based language model,2010, In INTERSPEECH
 Exploring sparsity in recurrent neuralnetworks,2017, In International Conference on Learning Representations
 Compression of neural machine translationmodels via pruning,2016, CoRR
 Learning structured sparsity in deep neuralnetworks,2016, In Advances in Neural Information Processing Systems
 See et al,2018, (2016) found that different11Published as a conference paper at ICLR 2018pruning methods affect accuracy differently
