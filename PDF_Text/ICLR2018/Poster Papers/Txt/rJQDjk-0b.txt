Published as a conference paper at ICLR 2018
Unbiased Online Recurrent Optimization
Corentin Tallec
Laboratoire de Recherche en Informatique
Universite Paris SUd
Gif-sur-Yvette, 91190, France
corentin.tallec@u-psud.fr
Yann Ollivier
Laboratoire de Recherche en InformatiqUe
Universite Paris SUd
Gif-sUr-Yvette, 91190, France
yann@yann-ollivier.org
Ab stract
The novel Unbiased Online Recurrent Optimization (UORO) algorithm allows for
online learning of general recUrrent compUtational graphs sUch as recUrrent net-
work models. It works in a streaming fashion and avoids backtracking throUgh
past activations and inpUts. UORO is compUtationally as costly as Truncated
Backpropagation Through Time (trUncated BPTT), a widespread algorithm for
online learning of recUrrent networks Jaeger (2002). UORO is a modification of
NoBackTrack Ollivier et al. (2015) that bypasses the need for model sparsity and
makes implementation easy in cUrrent deep learning frameworks, even for com-
plex models. Like NoBackTrack, UORO provides Unbiased gradient estimates;
Unbiasedness is the core hypothesis in stochastic gradient descent theory, withoUt
which convergence to a local optimUm is not gUaranteed. On the contrary, trUn-
cated BPTT does not provide this property, leading to possible divergence. On
synthetic tasks where trUncated BPTT is shown to diverge, UORO converges. For
instance, when a parameter has a positive short-term bUt negative long-term in-
flUence, trUncated BPTT diverges Unless the trUncation span is very significantly
longer than the intrinsic temporal range of the interactions, while UORO performs
well thanks to the Unbiasedness of its gradients.
CUrrent recUrrent network learning algorithms are ill-sUited to online learning via a single pass
throUgh long seqUences of temporal data. Backpropagation Through Time (BPTT Jaeger (2002)), the
cUrrent standard for training recUrrent architectUres, is well sUited to many short training seqUences.
Treating long seqUences with BPTT reqUires either storing all past inpUts in memory and waiting
for a long time between each learning step, or arbitrarily splitting the inpUt seqUence into smaller
seqUences, and applying BPTT to each of those short seqUences, at the cost of losing long term
dependencies.
This paper introdUces Unbiased Online Recurrent Optimization (UORO), an online and memoryless
learning algorithm for recUrrent architectUres: UORO processes and learns from data samples se-
qUentially, one sample at a time. Contrary to BPTT, UORO does not maintain a history of previoUs
inpUts and activations. Moreover, UORO is scalable: processing data samples with UORO comes
at a similar compUtational and memory cost as jUst rUnning the recUrrent model on those data.
Like most neUral network training algorithms, UORO relies on stochastic gradient optimization. The
theory of stochastic gradient crUcially relies on the Unbiasedness of gradient estimates to provide
convergence to a local optimUm. To this end, in the footsteps of NoBackTrack (NBT) Ollivier et al.
(2015), UORO provides provably unbiased gradient estimates, in a scalable, streaming fashion.
Unlike NBT, thoUgh, UORO can be easily implemented in a black-box fashion on top of an existing
recUrrent model in cUrrent machine learning software, withoUt delving into the strUctUre and code of
the model.
The framework for recUrrent optimization and UORO is introdUced in Section 2. The final algorithm
is reasonably simple (Alg. 1), bUt its derivation (Section 3) is more complex. In Section 6, UORO
is shown to provide convergence on a set of synthetic experiments where trUncated BPTT fails to
display reliable convergence. An implementation of UORO is provided as sUpplementary material.
1
Published as a conference paper at ICLR 2018
1	Related work
A widespread approach to online learning of recurrent neural networks is Truncated Backpropaga-
tion Through Time (truncated BPTT) Jaeger (2002), which mimics Backpropagation Through Time,
but zeroes gradient flows after a fixed number of timesteps. This truncation makes gradient esti-
mates biased; consequently, truncated BPTT does not provide any convergence guarantee. Learning
is biased towards short-time dependencies. 1. Storage of some past inputs and states is required.
Online, exact gradient computation methods have long been known (Real Time Recurrent Learning
(RTRL) Williams & Zipser (1989); Pearlmutter (1995)), but their computational cost discards them
for reasonably-sized networks.
NoBackTrack (NBT) Ollivier et al. (2015) also provides unbiased gradient estimates for recurrent
neural networks. However, contrary to UORO, NBT cannot be applied in a blackbox fashion, mak-
ing it extremely tedious to implement for complex architectures.
Other previous attempts to introduce generic online learning algorithms with a reasonable computa-
tional cost all result in biased gradient estimates. Echo State Networks (ESNs) Jaeger (2002); Jaeger
et al. (2007) simply set to 0 the gradients of recurrent parameters. Others, e.g., Maass et al. (2002);
Steil (2004), introduce approaches resembling ESNs, but keep a partial estimate of the recurrent gra-
dients. The original Long Short Term Memory algorithm Hochreiter & Schmidhuber (1997) (LSTM
now refers to a particular architecture) cuts gradient flows going out of gating units to make gradient
computation tractable. Decoupled Neural Interfaces Jaderberg et al. (2016) bootstrap truncated gra-
dient estimates using synthetic gradients generated by feedforward neural networks. The algorithm
in Movellan et al. (2002) provides zeroth-order estimates of recurrent gradients via diffusion net-
works; it could arguably be turned online by running randomized alternative trajectories. Generally
these approaches lack a strong theoretical backing, except arguably ESNs.
2	Background
UORO is a learning algorithm for recurrent computational graphs. Formally, the aim is to optimize
θ, a parameter controlling the evolution of a dynamical system
st+1 = Fstate (xt+1 , st, θ)
ot+1 = Fout (xt+1 , st , θ)
(1)
(2)
in order to minimize a total loss L := E 't(ot, ot), where OJ= is a target output at time t.
0≤t≤T
For instance, a standard recurrent neural network, with hidden state st (preactivation values) and
output ot at time t, is described with the update equations Fstate(xt+1, st, θ) := Wx xt+1 +
Wstanh(st) + b and Fout(xt+1, st, θ) := Wotanh(Fstate(xt+1, st, θ)) + bo; here the parameter
is θ = (Wx, Ws, b, Wo, bo), and a typical loss might be `s(os, os=) := (os - os=)2.
Optimization by gradient descent is standard for neural networks. In the spirit of stochastic gradient
descent, we can optimize the total loss L = P `t (ot, ot= ) one term at a time and update the
0≤t≤T
parameter online at each time step via
(3)
where ηt is a scalar learning rate at time t. (Other gradient-based optimizers can also be used, once
喻 is known.) The focus is then to compute, or approximate,端.
BPTT computes 焉 by unfolding the network through time, and backpropagating through the Un-
folded network, each timestep corresponding to a layer. BPTT thus requires maintaining the full
unfolded network, or, equivalently, the history of past inputs and activations. 2 Truncated BPTT
1 Arguably, truncated BPTT might still learn some dependencies beyond its truncation range, by a mecha-
nism similar to Echo State Networks Jaeger (2002). However, truncated BPTT’s gradient estimate has a marked
bias towards short-term rather than long-term dependencies, as shown in the first experiment of Section 6.
2Storage of past activations can be reduced, e.g. Gruslys et al. (2016). However, storage of all past inputs is
necessary.
2
Published as a conference paper at ICLR 2018
only unfolds the network for a fixed number of timesteps, reducing computational cost in online
settings Jaeger (2002). This comes at the cost of biased gradients, and can prevent convergence of
the gradient descent even for large truncations, as clearly exemplified in Fig. 2a.
3	Unbiased Online Recurrent Optimization
Unbiased Online Recurrent Optimization is built on top of a forward computation of the gradi-
ents, rather than backpropagation. Forward gradient computation for neural networks (RTRL) is
described in Williams & Zipser (1989) and we review it in Section 3.1. The derivation of UORO
follows in Section 3.2. Implementation details are given in Section 3.3. UORO’s derivation is
strongly connected to Ollivier et al. (2015) but differs in one critical aspect: the sparsity hypothesis
made in the latter is relieved, resulting in reduced implementation complexity without any model
restriction. The proof of UORO's convergence to a local optimum can be found in Masse (2θ17).
3.1	Forward computation of the gradient
Forward computation of the gradient for a recurrent model (RTRL) is directly obtained by applying
the chain rule to both the loss function and the state equation (1), as follows.
Direct differentiation and application of the chain rule to 't+ι yields
∂'t+1 - ∂'t+ι (	* ) ∕∂Fout (	空 A (	θ 八	⑷
∂θ = ∂o (ot+1,0t+1) ∙ V ∂s (xt+1,st,θ) ∂θ + ∂θ (xt+1,st,θ)J .	⑷
Here, the term ∂st∕∂θ represents the effect on the state at time t of a change of parameter during
the whole past trajectory. This term can be computed inductively from time t to t + 1. Intuitively,
looking at the update equation (1), there are two contributions to ∂st+ι∕∂θ:
•	The direct effect ofa change of θ on the computation of st+1, given st.
•	The past effect of θ on st via the whole past trajectory.
With this in mind, differentiating (1) with respect to θ yields
∂st+1	∂Fstate	∂Fstate	∂st
~~∂θ~ = ∂θ (xt+1,st,θ)+ ∂s (xt+1,st,θ) 既.	⑸
This gives a way to compute the derivative of the instantaneous loss without storing past history: at
each time step, update ∂st∕∂θ from ∂st-ι∕∂θ, then use this quantity to directly compute ∂'t+ι∕∂θ.
This is how RTRL Williams & Zipser (1989) proceeds.
A huge disadvantage of RTRL is that ∂st∕∂θ is of size dim(state) X dim(params). For instance,
with a fully connected standard recurrent network with n units, ∂st∕∂θ scales as n3. This makes
RTRL impractical for reasonably sized networks.
UORO modifies RTRL by only maintaining a scalable, rank-one, provably unbiased approximation
∙-v
of ∂st/∂θ, to reduce the memory and computational cost. This approximation takes the form St 0 θt,
where St is a column vector of the same dimension as st, St is a row vector of the same dimension
as θ>, and 0 denotes the outer product. The resulting quantity is thus a matrix of the same size as
∙-v
∂st∕∂θ. The memory cost of storing St and θt scales as dim( state) + dim (params). Thus UORO is
as memory costly as simply running the network itself (which indeed requires to store the current
∙-v
state and parameters). The following section details how sSt and θt are built to provide unbiasedness.
3.2	Rank-one trick: from RTRL to UORO
Given an unbiased estimation of ∂st/∂θ, namely, a stochastic matrix Gt such that E Gt = ∂st/∂θ,
unbiased estimates of ∂'t+ι∕∂θ and ∂st+ι∕∂θ can be derived by plugging Gt in (4) and (5). Unbi-
asedness is preserved thanks to linearity of the mean, because both (4) and (5) are affine in ∂st/∂θ.
3
Published as a conference paper at ICLR 2018
Thus, assuming the existence of a rank-one unbiased approximation Gt = St 0 θt at time t, We can
plug it in (5) to obtain an unbiased approximation Gt+1 at time t + 1
Gt+1 = %tate (xt+1, st,θ) + 'gtate (xt+1, St,θ) St 0 St.	(6)
∂θ	∂s
HoWever, in general this is no longer rank-one.
To transform Gt+1 into Gt+1, a rank-one unbiased approximation, the folloWing rank-one trick,
introduced in Ollivier et al. (2015) is used:
Proposition 1. Let A be a real matrix that decomposes as
k
A =	vi 0 wi.	(7)
i=1
Let ν be a vector of k independent random signs, and ρ a vector of k positive numbers. Consider
the rank-one matrix
A := X ρiνivi 0
∙-v	∙-v
Then A is an unbiased rank-one approximation of A: EνA = A.
The rank-one trick can be applied for any ρ. The choice of ρ influences the variance of the approxi-
mation; choosing
ρi = VWTM
minimizes the variance of the approximation, E kA - ASk22 Ollivier et al. (2015).
(9)
The UORO update is obtained by applying the rank-one trick twice to (6). First, AFre (/t+匕 st,θ)
is reduced to a rank one matrix, Without variance minimization. 3 Namely, let ν be a vector of
independant random signs; then,
纥tate (xt+1, St, θ) = EV V 0 V> 纥tate (xt+ι,st,θ) .	(10)
∂θ	∂θ
This results in a rank-two, unbiased estimate of ∂st+1T∂θ by substituting (10) into (6)
dFstate (xt+1, St,θ) St 0 St + V 0 (V> dFstate (xt+1, St, θ)) .	(11)
∂s	∂θ
Applying Prop. 1 again to this rank-two estimate, with variance minimization, yields UORO’s esti-
∙-v
mate Gt+1
Gt+1 = (ρo'qtate (xt+ι, st, θ) st + Pi V) 0 t — + -- agtate (xt+ι, st, θ)]	(12)
∂S	ρ0	ρ1	∂θ
∙-v
which satisfies that Eν Gt+1 is equal to (6). (By elementary algebra, some random signs that should
appear in (12) cancel out.) Here
P = S	kθt k	— P = Jk v> 5 (χt+ι,Zθ1
0 V k d⅛te(xt+ι,st,θ) Stk,	1 V	kVk
minimizes variance of the second reduction.
The unbiased estimation (12) is rank-one and can be rewritten as Gt+1 = sSt+1 0 θt+1 with the
update
St+1 - P0 "dsaae (xt+1, St,θ) St + Pi V	(14)
>
S	θt	V ∂Fstate
θt+ι4-----1---------(χt—(xt+ι, st, θ).	(15)
P0	Pi ∂θ
3 Variance minimization is not used at this step, since computing
for every i is not scalable.
4
Published as a conference paper at ICLR 2018
Initially, ∂s°∕∂θ = 0, thus so = 0, θo = 0 yield an unbiased estimate at time 0. Using this initial
estimate and the update rules (14)-(15), an estimate of ∂st∕∂θ is obtained at all subsequent times,
allowing for online estimation of ∂'t∕∂θ. Thanks to the construction above, by induction all these
estimates are unbiased. 4
We are left to demonstrate that these update rules are scalably implementable.
3.3	Implementation
Implementing UORO requires maintaining the rank-one approximation and the corresponding gra-
dient loss estimate. UORO's estimate of the loss gradient ∂'t+ι∕∂θ at time t + 1 is expressed by
plugging into (4) the rank-one approximation ∂st∕∂θ ≈ st 0 瓦,which results in
(∂'t+ι	* . ∂Fout	θ ~} θ ɪ ∂'t+ι	*	∂Fout
I ∂o	(ot+1,0t+1)	∂s	(xt+1,	st,θ)	∙ st )	θt	+-∂o~ (ot+1,0t+1)	∂θ	(xt+1,	st, θ)'	(16)
Backpropagating ∂'t+ι∕∂ot+ι once through FOut returns
(∂'t+ι∕∂ot+ι ∙ ∂Fout∕∂xt+ι, ∂'t+ι∕∂ot+ι ∙ ∂Fout∕∂st, ∂'t+ι∕∂ot+ι ∙ ∂Fout∕∂θ), thus provid-
ing all necessary terms to compute (16).
∙-v
Updating ss and θ requires applying (14)-(15) at each step. Backpropagating the vector of random
signs V once through Fstate returns (一一 ν> ∂FState(xt+ι, st,θ)∕∂θ), providing for(15).
Updating S via (14) requires computing (∂Fstate∕∂st) ∙ st. This is computable numerically through
∂Fstate	Fstate (xt+1 , st + ε sst , θ) - Fstate (xt+1 , st , θ)
~^τ~(Xt+1, st,θ) ∙ St = ε→o------------------------ε---------------------
(17)
computable through two applications of Fstate. This operation is referred to as tangent forward
propagation Simard et al. (1991) and can also often be computed algebraically.
This allows for complete implementation of one step of UORO (Alg. 1). The cost of UORO (includ-
ing running the model itself) is three applications of Fstate, one application of Fout, one backprop-
agation through Fout andFstate, and a few elementwise operations on vectors and scalar products.
The resulting algorithm is detailed in Alg. 1. F.forward(v) denotes pointwise application of
F at point v, F.backprop(v, δo) backpropagation of row vector δo through F at point v, and
F.forwarddiff(v, δv ) tangent forward propagation of column vector δv through F at point v. No-
tably, F.backprop(v, δo) has the same dimension as v>, e.g. Fout.backprop((xt+1, st, θ), δot+1)
has three components, of the same dimensions as xt>+1, st> and θ> .
The proposed update rule for stochastic gradient descent (3) can be directly adapted to other op-
timizers, e.g. Adaptative Momentum (Adam) Kingma & Ba (2014) or Adaptative Gradient Duchi
et al. (2010). Vanilla stochastic gradient descent (SGD) and Adam are used hereafter. In Alg. 1,
such optimizers are denoted by SGDOpt and the corresponding parameter update given current
parameter θ, gradient estimate gt and learning rate ηt is denoted SGDOpt.update(gt, ηt, θ).
3.4	MEMORY-T UORO AND RANK-k UORO
The unbiased gradient estimates of UORO injects noise via ν, thus requiring smaller learning rates.
To reduce noise, UORO can be used on top of truncated BPTT so that recent gradients are computed
exactly.
Formally, this just requires applying Algorithm 1 to a new transition function F T which is just T
consecutive steps of the original model F . Then the backpropagation operation in Algorithm 1
becomes a backpropagation over the last T steps, as in truncated BPTT. The loss of one step ofF T
t+T
is the sum of the losses of the last T steps of F, namely 't+T := P 'k. Likewise, the forward
k=t+1
tangent propagation is performed through FT . This way, we obtain an unbiased gradient estimate
in which the gradients from the last T steps are computed exactly and incur no noise. The resulting
algorithm is referred to as memory-T UORO. Its scaling inT is similar to T-truncated BPTT, both in
4 In practice, since θ changes during learning, unbiasedness only holds exactly in the limit of small learning
rates. This is not specific to UORO as it also affects RTRL.
5
Published as a conference paper at ICLR 2018
Algorithm 1 — One step of UORO (from time t to t +1)
Inputs:
-	xt+i, o；+「St and θ: input, target, previous recurrent state, and parameters
-	St column vector of size state, θt row vector of size params such that E St0^t = ∂st∕∂θ
-	SGDOpt and ηt+1: stochastic optimizer and its learning rate
Outputs:
-	't+ι, st+i and θ: loss, new recurrent state, and updated parameters
-	st+i and St+i such that ESt+i 0 St+i = ∂st+ι∕∂θ
-	gt+ι such that E gt+i = ∂'t+ι∕∂θ
/* compute next state and loss */
st+i J Fstate.forward(xt+i, st, θ),	ot+i J FOut.forward(xt+i, st, θ)
't+i J '(ot+i, ot+i)
/* compute gradient estimate */
(_, δs, δθ) J F0ut.backprop ((xt+i, st,θ), ∂'t+i^
∙-v
gt+i J (δs ∙ St) θt + δθ
/* prepare for reduction */
Draw ν, column vector of random signs ±1 of size state
sSt+i J Fstate.forwarddiff((xt+i, st,θ), (0, sSt,0))
(一一 δθg) J Fstate.backprop((xt+i, st, θ),ν丁)
/* compute normalizers */
P0 J Srk[I + ε, Pi J S k lkδθgl1 + ε with ε = 10-7
ksSt+ik +ε	kνk +ε
/* reduce */
∙-v
S	θSt	δθg
gt+i J Po st+i + Pi ν, θt+i j------1-----
ρ0	ρi
/* update θ */
SGDOpt.update(gSt+i, ηt+i, θ)
terms of memory and computation. In the experiments below, memory-T UORO reduced variance
early on, but did not significantly impact later performance.
The noise in UORO can also be reduced by using higher-rank gradient estimates (rank-r instead of
∙-v
rank-1), which amounts to maintaining r distinct values of sS and θ in Algorithm 1 and averaging the
resulting values of gS. We did not exploit this possibility in the experiments below, although r = 2
visibly reduced variance in preliminary tests.
4	UORO’s variance is stable as time goes by
Gradient-based sequential learning on an unbounded data stream requires that the variance of the
gradient estimate does not explode through time. UORO is specifically built to provide an unbiased
estimate whose variance does not explode over time.
A precise statement regarding UORO’s convergence and boundedness of the variance of gradients is
provided in MaSSe (2017). Informally, when the largest eigenvalue of the differential transition op-
erator ∂Fstate∕∂s is uniformly bounded by a constant δ < 1 (which characterizes stable dynamical
systems), the normalizing factors in (14) and (15) enforce that the influence of previous V,s decrease
exponentially with time.
We hereby provide an experimental validation of the boundedness of UORO’s variance in Fig. 1a.
To monitor the variance of UORO’s estimate over time, a 64-unit GRU recurrent network is trained
on the first 107 characters of the full works of Shakespeare using UORO. The network is then rerun
6
Published as a conference paper at ICLR 2018
100 50
100
Time step
(a)
1000
.λλa-∙- *--,,∙-*∙-a-0
10000	16
80
20
32	64	128
NumVw of IliddQl ɪuɪɪts
(b)
⅛s.s-τ .∙G∙∙s』*£'
0.6
100
1000
10000	100000	1 × 106	1 × 107	1 × 108
I⅛i.⅛1 i;roɑssoj
(c)
Figure 1:	(a) The relative variance of UORO gradient estimates does not significantly increase with
time. Note the logarithmic scale on the time axis. (b) The relative variance of UORO gradient
estimates significantly increases with network size. Note the logarithmic scale on number of units.
(c) Variance of larger networks affects learning on a small range copy task.
100 times on the 10000 first characters of the text, and gradients estimates at each time steps are
computed, but not applied. The gradient relative variance, that is
E[kgt- E[g川 12]
kE [gt] k2
(18)
is computed, where the average is taken with respect to runs. This quantity appears to be stationary
over time (Fig. 1a).
5	UORO’s variance increases with the number of hidden units
As the number of hidden units in the recurrent network increases, the rank one approximation that is
used to provide an unbiased gradient estimate becomes coarser. Consequently, the relative variance,
as defined in (18), should increase as the number of hidden units increases.
This increase is experimentally verified in Fig. 1b. Untrained GRU networks with various number
of units are run for 10 timesteps, 100 times for each size, and the UORO gradient estimate after
these 10 timesteps is computed (but not applied). The relative variance of these gradients over the
100 runs is evaluated, for each network size. As shown in the figure, the relative variance increases
with the number of units. Note the horizontal log scale.
The increase of the variance of the estimate with network size underlines the need for smaller learn-
ing rates when training large networks with UORO, compared to truncated backpropagation. This
can imply slower learning for the kind of dependencies that truncated backpropagation can learn.
The need for lower learning rates with larger networks is exemplified in Fig. 1c. GRU networks of
various hidden sizes are trained with UORO on a simple copy task, as presented in Hochreiter &
Schmidhuber (1997), with a lag of T = 5. The networks are all trained with the same decreasing
-4
learning rate, η = ι+3⅛-3t. For all network sizes except the largest, the error decreases slowly
but steadily. For the largest network, the variance is too large compared to the learning rate, and the
error jumps sharply midway through.
6	Experiments illustrating truncation bias
The set of experiments below aims at displaying specific cases where the biases from truncated
BPTT are likely to prevent convergence of learning. On this test set, UORO’s unbiasedness pro-
vides steady convergence, highlighting the importance of unbiased estimates for general recurrent
learning.
Influence balancing. The first test case exemplifies learning of a scalar parameter θ which has a
positive influence in the short term, but a negative one in the long run. Short-sightedness of truncated
algorithms results in abrupt failure, with the parameter exploding in the wrong direction, even with
truncation lengths exceeding the temporal dependency range by a factor of 10 or so.
7
Published as a conference paper at ICLR 2018
1 × 106
1 × 10-12
0	20000
1 × 104
1 × 102
1 × 100
1 × 10-2
1 × 10-4
1 × 10-6
1 × 10-8
1 × 10-10
40000	60000	80000	100000
Epoch
(a)
2.5
2.4
1.8
1.7
10000
)retcarahc rep stib( ssol-gol egarev
100000	1 × 106	1 × 107
Epoch
(b)
Figure 2:	(a)Results for influence balancing with 23 units and 13 minus; note the vertical log scale.
(b)Learning curves on distant brackets (1, 5, 10).
(a) Influence balancing, 4 units, 3 minus.
[a]eecbe[a]
[j]fbfjd[j]
[c]bgddc[c]
[d]gjhai[d]
[e]iaghb[e]
[h]bigaj[h]
(b) Distant brackets (1, 5, 10).
aaaaaa
bbbbbb
aaaaaaaaaaaaaaaa
bbbbbbbbbbbbbbbb
aaaaaaaa
bbbbbbbb
(c) anbn(1, 32).
Figure 3:	Datasets.
Consider the linear dynamics
st+1 = Ast + (θ, . . . , θ, -θ, . . . , -θ)>
(19)
with A a square matrix of size n with Ai,i = 1/2, Ai,i+1 = 1/2, and 0 elsewhere; θ ∈ R is a
scalar parameter. The second term has p positive-θ entries and n - p negative-θ entries. Intuitively,
the effect of θ on a unit diffuses to shallower units over time (Fig. 3a). Unit i only feels the effect
of θ from unit i + n after n time steps, so the intrinsic time scale of the system is ≈ n. The loss
considered is a target on the shallowest unit s1,
't = 2 (St- 1)2.	(20)
Learning is performed online with vanilla SGD, using gradient estimates either from UORO or T -
truncated BPTT with various T. Learning rates are of the form η = ι+√, for suitable values of
η.
As shown in Fig. 2a, UORO solves the problem while T -truncated BPTT fails to converge for any
learning rate, even for truncations T largely above n. Failure is caused by ill balancing of time
dependencies: the influence of θ on the loss is estimated with the wrong sign due to truncation. For
n = 23 units, with 13 minus signs, truncated BPTT requires a truncation T ≥ 200 to converge.
Next-character prediction. The next experiment is character-level synthetic text prediction: the
goal is to train a recurrent model to predict the t + 1-th character of a text given the first t online,
with a single pass on the data sequence.
A single layer of 64 units, either GRU or LSTM, is used to output a probability vector for the
next character. The cross entropy criterion is used to compute the loss.At each time t we plot the
cumulated loss per character on the first t characters, ɪ PS=1 '§. (Losses for individual characters
are quite noisy, as not all characters in the sequence are equally difficult to predict.) This would be
the compression rate in bits per character if the models were used as online compression algorithms
on the first t characters. In addition, in Table 1 we report a “recent” loss on the last 100, 000
characters, which is more representative of the model at the end of learning.
Optimization was performed using Adam with the default setting β1 = 0.9 and β2 = 0.999, and a
decreasing learning rate η =	：√, With t the number of characters processed. As convergence of
8
Published as a conference paper at ICLR 2018
0.2
)retcarahc rep stib( ssol-gol egarevA
100	1000	10000	100000	1 × 106	1 × 107
Epoch
.9
.8 .7 6. .5 .4
0. 0. .0 0. 0.
)retcarahc rep stib( ssol-gol egarevA
100	1000	10000	100000	1 × 106	1 × 107
Epoch
Figure 4: Learning curves on anb(n1,32)
UORO requires smaller learning rates than truncated BPTT, this favors UORO. Indeed UORO can
fail to converge with non-decreasing learning rates, due to its stochastic nature.
DISTANT BRACKETS DATASET (s, k, a). The distant brackets dataset is generated by repeatedly
outputting a left bracket, generating s random characters from an alphabet of size a, outputting a
right bracket, generating k random characters from the same alphabet, repeating the same first s
characters between brackets and finally outputting a line break. A sample is shown in Fig. 3b.
UORO is compared to 4-truncated BPTT. Truncation is deliberately shorter than the inherent time
range of the data, to illustrate how bias can penalize learning if the inherent time range is unknown a
priori. The results are given in Fig. 2b (with learning rates using α = 0.015 andγ = 10-3). UORO
beats 4-truncated BPTT in the long run, and succeeds in reaching near optimal behaviour both with
GRUs and LSTMs. Truncated BPTT remains stuck near a memoryless optimum with LSTMs; with
GRUs it keeps learning, but at a slow rate. Still, truncated BPTT displays faster early convergence.
anbn (k, l) DATASET. The anbn (k, l) dataset tests memory and counting Gers & Schmidhuber
(2001); it is generated by repeatedly picking a random number n between k and l, outputting a
string of n a’s, a line break, n b’s, and a line break (see Fig. 3c). The difficulty lies in matching the
number of a’s and b’s.
Table 1: Averaged loss on the 105 last iterations on anbn(1, 32).
	Truncation	LSTM	GRU
	No memory (default)	0.147	0.155
UORO	Memory-2	0.149	0.174
	Memory-16	0.154	0.149
	T	0.178	0.231
Truncated BPTT	2	0.149	0.285
	16	0.144	0.207
Plots for a few setups are given in Fig. 4. The learning rates used α = 0.03 and γ = 10-3.
Numerical results at the end of training are given in Table 1. For reference, the true entropy rate is
0.14 bpc, while the entropy rate of a model that does not understand that the numbers of a’s and b’s
coincide is double, 0.28 bpc.
Here, in every setup, UORO reliably converges and reaches near optimal performance. Increasing
UORO’s range does not significantly improve results: providing an unbiased estimate is enough
to provide reliable convergence in this case. Meanwhile, truncated BPTT performs inconsistently.
Notably, with GRUs, it either converges to a poor local optimum corresponding to no understanding
of the temporal structure, or exhibits gradient reascent in the long run. Remarkably, with LSTMs
rather than GRUs, 16-truncated BPTT reliably reaches optimal behavior on this problem even with
biased gradient estimates.
Conclusion
We introduced UORO, an algorithm for training recurrent neural networks in a streaming, memo-
ryless fashion. UORO is easy to implement, and requires as little computation time as truncated
9
Published as a conference paper at ICLR 2018
BPTT, at the cost of noise injection. Importantly, contrary to most other approaches, UORO scal-
ably provides unbiasedness of gradient estimates. Unbiasedness is of paramount importance in the
current theory of stochastic gradient descent. Furthermore, UORO is experimentally shown to ben-
efit from its unbiasedness, converging even in cases where truncated BPTT fails to reliably achieve
good results or diverges pathologically.
10
Published as a conference paper at ICLR 2018
References
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning
and stochastic optimization. Technical Report UCB/EECS-2010-24, EECS Department, Univer-
sity of California, Berkeley, Mar 2010. URL http://www2.eecs.berkeley.edu/Pubs/
TechRpts/2010/EECS-2010-24.html.
Felix A Gers and Jurgen Schmidhuber. Long short-term memory learns context free and context
sensitive languages. In Artificial Neural Nets and Genetic Algorithms, pp. 134-137. Springer,
2001.
Audrunas Gruslys, Remi Munos, Ivo Danihelka, Marc Lanctot, and Alex Graves. Memory-efficient
backpropagation through time. CoRR, abs/1606.03401, 2016. URL http://arxiv.org/
abs/1606.03401.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735-
1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL http://dx.
doi.org/10.1162/neco.1997.9.8.1735.
Max Jaderberg, Wojciech Marian Czarnecki, Simon Osindero, Oriol Vinyals, Alex Graves, and Ko-
ray Kavukcuoglu. Decoupled neural interfaces using synthetic gradients. CoRR, abs/1608.05343,
2016. URL http://arxiv.org/abs/1608.05343.
Herbert Jaeger. Tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF and the
“echo state network” approach, 2002.
Herbert Jaeger, Mantas Lukosevicius, Dan Popovici, and Udo Siewert. Optimization and AP-
plications of Echo State Networks with Leaky-Integrator Neurons. Neural Networks, 20(3):
335-352, April 2007. ISSN 08936080. doi: 10.1016/j.neunet.2007.04.016. URL http:
//www.sciencedirect.com/science/article/pii/S089360800700041X.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR,
abs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980.
Wolfgang Maass, Thomas Natschlager, and Henry Markram. Real-time computing without stable
states: A new framework for neural computation based on perturbations. Neural Comput., 14
(11):2531-2560, November 2002. ISSN 0899-7667. doi: 10.1162/089976602760407955. URL
http://dx.doi.org/10.1162/089976602760407955.
Pierre-Yves Masse. Autour de l’Usage des gradients en apprentissage statistique. PhD thesis, 2017.
URL https://hal.archives-ouvertes.fr/tel-01665478.
Javier R. Movellan, Paul Mineiro, and R. J. Williams. A Monte Carlo EM approach for partially
observable diffusion processes: Theory and applications to neural networks. Neural Comput., 14
(7):1507-1544, July 2002. ISSN 0899-7667. doi: 10.1162/08997660260028593. URL http:
//dx.doi.org/10.1162/08997660260028593.
Yann Ollivier, Corentin Tallec, and Guillaume Charpiat. Training recurrent networks online with-
out backtracking. CoRR, abs/1507.07680, 2015. URL http://arxiv.org/abs/1507.
07680.
Barak A Pearlmutter. Gradient calculations for dynamic recurrent neural networks: A survey. IEEE
Transactions on Neural networks, 6(5):1212-1228, 1995.
Patrice Y. Simard, Bernard Victorri, Yann LeCun, and John S. Denker. Tangent prop - a
formalism for specifying selected invariances in an adaptive network. In John E. Moody,
Stephen Jose Hanson, and Richard Lippmann (eds.), NIPS, pp. 895-903. Morgan Kaufmann,
1991. ISBN 1-55860-222-4. URL http://dblp.uni-trier.de/db/conf/nips/
nips1991.html#SimardVLD91.
Jochen J. Steil. Backpropagation-decorrelation: online recurrent learning with O(N) complexity. In
Neural Networks, 2004. Proceedings. 2004 IEEE International Joint Conference on, volume 2,
pp. 843-848 vol.2. IEEE, July 2004. ISBN 0-7803-8359-1. doi: 10.1109/ijcnn.2004.1380039.
URL http://dx.doi.org/10.1109/ijcnn.2004.1380039.
11
Published as a conference paper at ICLR 2018
Ronald J. Williams and David Zipser. A learning algorithm for continually running fully recurrent
neural networks. NeuralComPut, 1(2):270-280, June 1989. ISSN 0899-7667. doi: 10.1162/
neco.1989.1.2.270. URL http://dx.doi.org/10.1162/neco.1989.1.2.270.
12