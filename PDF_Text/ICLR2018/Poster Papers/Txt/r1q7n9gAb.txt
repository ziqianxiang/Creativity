Published as a conference paper at ICLR 2018
The Implicit Bias of Gradient Descent on Sepa-
rable Data
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson Nathan Srebro
Department of Electrical Engineering,Technion Toyota Technological Institute at Chicago
Haifa, 320003, Israel	Chicago, Illinois 60637, USA
daniel.soudry@gmail.com	nati@ttic.edu
elad.hoffer@gmail.com
mor.shpigel@gmail.com
Ab stract
We show that gradient descent on an unregularized logistic regression problem, for
almost all separable datasets, converges to the same direction as the max-margin
solution. The result generalizes also to other monotone decreasing loss functions
with an infimum at infinity, and we also discuss a multi-class generalizations to the
cross entropy loss. Furthermore, we show this convergence is very slow, and only
logarithmic in the convergence of the loss itself. This can help explain the benefit
of continuing to optimize the logistic or cross-entropy loss even after the training
error is zero and the training loss is extremely small, and, as we show, even if the
validation loss increases. Our methodology can also aid in understanding implicit
regularization in more complex models and with other optimization methods.
1	Introduction
It is becoming increasingly clear that implicit biases introduced by the optimization algorithm play
a crucial role in deep learning and in the generalization ability of the learned models (Neyshabur
et al., 2014; 2015; Zhang et al., 2017; Keskar et al., 2017; Neyshabur et al., 2017; Wilson et al.,
2017). In particular, minimizing the training error, without any explicit regularization, over models
with more parameters and more capacity then the number of training examples, often yields good
generalization, despite the empirical optimization problem being highly underdetermined. That is,
there are many global minima of the training objective, most of which will not generalize well, but
the optimization algorithm (e.g. gradient descent) biases us toward a particular minimum that does
generalize well. Unfortunately, we still do not have a good understanding of the biases introduced by
different optimization algorithms in different situations.
We do have a decent understanding of the implicit regularization introduced by early stopping of
stochastic methods or, at an extreme, of one-pass (no repetition) stochastic optimization. However,
as discussed above, in deep learning we often benefit from implicit bias even when optimizing the
(unregularized) training error to convergence, using stochastic or batch methods. For loss functions
with attainable, finite, minimizers, such as the squared loss, we have some understanding of this:
In particular, when minimizing an underdetermined least squares problem using gradient descent
starting from the origin, we know we will converge to the minimum Euclidean norm solution. But
the logistic loss, and its generalization the cross-entropy loss which is often used in deep learning, do
not admit a finite minimizer on separable problems. Instead, to drive the loss toward zero and thus
minimize it, the predictor must diverge toward infinity.
Do we still benefit from implicit regularization when minimizing the logistic loss on separable data?
Clearly the norm of the predictor itself is not minimized, since it grows to infinity. However, for
prediction, only the direction of the predictor, i.e. the normalized w(t)/ kw(t)k, is important. How
does w(t)/ kw(t)k behave as t → ∞ when we minimize the logistic (or similar) loss using gradient
descent on separable data, i.e., when it is possible to get zero misclassification error and thus drive
the loss to zero?
In this paper, we show that even without any explicit regularization, for all most all datasets (except a
zero measure set), when minimizing linearly separable logistic regression problems using gradient
1
Published as a conference paper at ICLR 2018
descent, we have that w(t)/ kw(t)k converges to the L2 maximum margin separator, i.e. to the
solution of the hard margin SVM. This happens even though the norm kwk, nor the margin constraint,
are in no way part of the objective nor explicitly introduced into optimization. More generally,
we show the same behavior for generalized linear problems with any smooth, monotone strictly
decreasing, lower bounded loss with an exponential tail. Furthermore, we characterize the rate of this
convergence, and show that it is rather slow, with the distance to the max-margin predictor decreasing
only as O(1/ log(t)). This explains why the predictor continues to improve even when the training
loss is already extremely small. We emphasize and demonstrate that this bias is specific to gradient
descent, and changing the optimization algorithm, e.g. using adaptive learning rate methods such as
ADAM Kingma & Ba (2015), changes this implicit bias.
2	Main Results
Consider a dataset {xn, yn}nN=1, with binary labels yn ∈ {-1, 1}. We analyze learning by minimiz-
ing an empirical loss of the form
N
L (W) = X ' (ynW>Xn) .	(2.1)
n=1
where W ∈ Rd is the weight vector. A bias term could be added in the usual way, extending xn by an
additional ’1’ component. To simplify notation, we assume that ∀n : yn = 1 — this is true without
loss of generality, since we can always re-define ynxn as xn .
We are particularly interested in problems that are linearly separable, and the loss is smooth monotone
strictly decreasing and non-negative:
Assumption 1. The dataset is strictly linearly separable: ∃w* such that ∀n : w>Xn > 0 .
Assumption 2. ` (u) is a positive, differentiable, monotonically decreasing to zero1, (so ∀u : ` (u) >
0, `0 (u) < 0 and limu→∞ ` (u) = limu→∞ `0 (u) = 0) and a β-smooth function, i.e. its derivative
is β-Lipshitz.
Many common loss functions, including the logistic, exp-loss, probit and sigmoidal losses, follow
Assumption 2. Assumption 2 also straightforwardly implies that L (w) is a βσm2 ax (X )-smooth
function, where the columns of X are all samples, and σmax (X ) is the maximal singular value of X.
Under these conditions, the infimum of the optimization problem is zero, but it is not attained at any
finite w. Furthermore, no finite critical point w exist. We consider minimizing eq. 2.1 using Gradient
Descent (GD) with a fixed learning rate η, i.e., with steps of the form:
N
W (t + 1)= W (t) - ηVL (w(t)) = W (t) - η X '0 (w (t)> Xn) Xn.	(2.2)
n=1
We do not require convexity. Under Assumptions 1 and 2, gradient descent converges to the global
minimum (i.e. to zero loss) even without it:
Lemma 1. Let W (t) be the iterates of gradient descent (eq. 2.2) with η < 2β-1σm-a2x (X ) and
any starting point W(0). Under Assumptions 1 and 2, we have: (1) limt→∞ L (W (t)) = 0, (2)
limt→∞ kW (t)k = ∞, and (3) ∀n : limt→∞ W (t)> Xn = ∞.
Proof. Since the data is strictly linearly separable, ∃w* which linearly separates the data, and
therefore
N
w> VL (w) = X ' (w>Xn) W>Xn.
n=1
For any finite w, this sum cannot be equal to zero, as a sum of negative terms, since ∀n : w>Xn > 0
and ∀u : `0 (u) < 0. Therefore, there are no finite critical points w, for which VL (w) = 0. But
1The requirement of nonnegativity and that the loss asymptotes to zero is purely for convenience. It is enough
to require the loss is monotone decreasing and bounded from below. Any such loss asymptotes to some constant,
and is thus equivalent to one that satisfies this assumption, up to a shift by that constant.
2
Published as a conference paper at ICLR 2018
gradient descent on a smooth loss with an appropriate stepsize is always guaranteed to converge to a
critical point: VL (W (t)) → 0 (see, e.g. Lemma 5 in Appendix A.4, slightly adapted from Ganti
(2015), Theorem 2). This necessarily implies that kw (t)k → ∞ while ∀n : w (t)> xn > 0 for large
enough t—since only then `0 W (t)> xn → 0. Therefore, L (W) → 0, so GD converges to the
global minimum.	□
The main question we ask is: can we characterize the direction in which W(t) diverges? That is, does
the limit limt→∞ W (t) / kW (t)k always exists, and if so, what is it?
In order to analyze this limit, we will need to make a further assumption on the tail of the loss
function:
Definition 2. A function f (u) has a “tight exponential tail”, if there exist positive constants
c, a, μ+, μ-,u+ and U- such that
∀u > u+ :f (u) ≤ C (1 + exp (—μ+ U)) e-au
∀u > u- :f (u) ≥ c (1 — exp (—μ-u)) e-au .
Assumption 3. The negative loss derivative —'0 (u) has a tight exponential tail (Definition 2).
For example, the exponential loss ` (u) = e-u and the commonly used logistic loss ` (u) =
log (1 + e-u) both follow this assumption with a = c = 1. We will assume a = c = 1 — without
loss of generality, since these constants can be always absorbed by re-scaling xn and η.
We are now ready to state our main result:
Theorem 3.	For almost all datasets (i.e., except for a measure zero) which are strictly linearly
separable (Assumption 1) and given a β-smooth decreasing loss function (Assumption 2) with an
exponential tail (Assumption 3), gradient descent (as in eq. 2.2) with stepsize η < 2β -1 σm-2ax (X )
and any starting point W(0) will behave as:
w (t) = Wlog t + P (t) ,	(2.3)
where the residual ρ (t) is bounded and so
w (t)	W
lim -----——=--------
t→∞ kw (t)k	kWk
where W is the L? max margin vector (the solution to the hard margin SVM):
W = argmin ∣∣Wk2 s.t. W>Xn ≥ 1.	(2.4)
w∈Rd
Since the theorem holds for almost all datasets, in particular, it holds with probability 1 if {xn }nN=1
are sampled from an absolutely continuous distribution.
Proof Sketch We first understand intuitively why an exponential tail of the loss entail asymptotic
convergence to the max margin vector: Assume for simplicity that ` (u) = e-u exactly, and examine
the asymptotic regime of gradient descent in which ∀n : W (t)> xn → ∞, as is guaranteed by Lemma
1. If W (t) / kW (t)k converges to some limit W∞, then we can write W (t) = g (t) W∞ + ρ (t) such
that g (t) → ∞, ∀n :x>W∞ > 0, and limt→∞ P (t) /g (t) = 0. The gradient can then be written as:
NN
—VL (w) = X exp ( —W (t)> Xn) Xn = X exp ( —g (t) W∞Xn) exp (—P (t)> Xn) Xn .
n=1	n=1
(2.5)
As g(t) → ∞ and the exponents become more negative, only those samples with the largest (i.e.,
least negative) exponents will contribute to the gradient. These are precisely the samples with the
smallest margin argminnW>∞Xn, aka the “support vectors”. The negative gradient (eq. 2.5) would
then asymptotically become a non-negative linear combination of support vectors. The limit W∞ will
then be dominated by these gradients, since any initial conditions become negligible as kW (t)k → ∞
3
Published as a conference paper at ICLR 2018
(from Lemma 1). Therefore, w∞ will also be non-negative linear combination of support vectors,
and so will its scaling w^ = w∞/ (mi□n w∞Xn). We therefore have:
N
W = X anXn	∀n (an ≥ 0 and W>Xn = 1)OR (an = 0 and W>Xn > 1)	(2.6)
n=1
These are precisely the KKT condition for the SVM problem (eq. 2.4) and we can conclude that W is
indeed its solution and W∞ is thus proportional to it.
To prove Theorem 3 rigorously, we need to show that W (t) / kW (t)k has a limit, that g (t) = log (t)
and to bound the effect of various residual errors, such as gradients of non-support vectors and
the fact that the loss is only approximately exponential. To do so, we substitute eq. 2.3 into the
gradient descent dynamics (eq. 2.2), with w∞ = W being the max margin vector and g(t) = log t.
We then show that the increment in the norm of ρ (t) is bounded by C1t-ν for some C1 > 0 and
ν > 1, which is a converging series. This happens because the increment in the max margin term,
W [log (t + 1) - log (t)] ≈ Wt-1, cancels out the dominant t-1 term in the gradient -VL (w (t))
(eq. 2.5 with g (t) = log (t) and W∞> Xn = 1). A complete proof can be found in Appendix A.
More refined analysis: characterizing the residual We can furthermore characterize the asymp-
totic behavior of ρ (t). To do so, we need to refer to the KKT conditions (eq. 2.6) of the SVM
problem (eq. 2.4) and the associated support vectors S = argminnW>Xn. The following refinement
of Theorem 3 is also proved in Appendix A:
Theorem 4.	Under the conditions and notation of Theorem 3, if, in addition the support vectors span
the data (i.e. rank (XS) = rank (X) where the columns of X are all samples and of XS are the
support vectors), then limt→∞ P (t) = W, where W is unique, g^ven W (0), and a solution to
∀n ∈ S : η exp (—x>W)= an	(2.7)
Note these equations are well-defined for almost all datasets, since (see Lemma 8 in Appendix F)
then there are at most d support vectors, an are unique and ∀n ∈ S : an 6= 0.
3 Implications: Rates of convergence
The solution in eq. 2.3 implies that W (t) / kW (t)k converges to the normalized max margin vec-
tor W/ ∣∣Wk. Moreover, this convergence is very slow— logarithmic in the number of iterations.
Specifically, in Appendix B we show that Theorem 3 implies the following tight rates of convergence:
The normalized weight vector converges to normalized max margin vector in L2 norm
and in angle
and the margin converges as
W (t) W
---------------------
kW ⑴k-----kWk
W (t)> W
1 -----------
kW ⑴kkWk
O (1/log t) ,
O (1/log2 t),
1	minn Xn> W (t)
-----------------
kWk	kW ⑴k
O (1/log t) ;
this slow convergence is in sharp contrast to the convergence of the (training) loss:
L(W ⑴)=O (t-1).
(3.1)
(3.2)
(3.3)
(3.4)
A simple construction (also in Appendix B) shows that the rates in the above equations are tight.
Thus, the convergence of W(t) to the max-margin W can be logarithmic in the loss itself, and we
might need to wait until the loss is exponentially small in order to be close to the max-margin solution.
This can help explain why continuing to optimize the training loss, even after the training error is
zero and the training loss is extremely small, still improves generalization performance—our results
suggests that the margin could still be improving significantly in this regime.
4
Published as a conference paper at ICLR 2018
8
6
4
2
100	1 01	102	1 03	1 04	1 05	1 06
t
(C)00
---GD
---GDMO
10-5
10-10
Figure 1:	Visualization of or main results on a synthetic dataset in which the L2 max margin vector
w^ is precisely known. (A) The dataset (positive and negatives samples (y = ±1) are respectively
denoted by 0+0 and 0◦/), max margin separating hyperplane (black line), and the asymptotic solution
of GD (dashed blue). For both GD and GD with momentum (GDMO), we show: (B) The norm of
w (t), normalized so it would equal to 1 at the last iteration, to facilitate comparison. As expected
(eq. 2.3), the norm increases logarithmically; (C) the training loss. As expected, it decreases as t-1
(eq. 3.4); and (D&E) the angle and margin gap of W (t) from w^ (eqs. 3.2 and 3.3). As expected,
these are logarithmically decreasing to zero. Implementation details: The dataset includes four
support vectors: x1 = (0.5, 1.5) , x2 = (1.5, 0.5) with y1 = y2 = 1, and x3 = -x1, x4 = -x2 with
y3 = y4 = -1 (the L? normalized max margin vector is then W = (1,1) /√2 with margin equal
to √2), and 12 other random datapoints (6 from each class), that are not on the margin. We used
a learning rate η = 1∕σmaχ (X), where °m&x (X) is the maximal singular value of X, momentum
γ = 0.9 for GDMO, and initialized at the origin.
A numerical illustration of the convergence is depicted in Figure 1. As predicted by the theory,
the norm kW(t)k grows logarithmically (note the semi-log scaling), and W(t) converges to the
max-margin separator, but only logarithmically, while the loss itself decreases very rapidly (note the
log-log scaling).
An important practical consequence of our theory, is that although the margin of W(t) keeps improving,
and so we can expect the population (or test) misclassification error of W(t) to improve for many
datasets, the same cannot be said about the expected population loss (or test loss)! At the limit, the
direction of w(t) will converge toward the max margin predictor W. Although W has zero training
error, it will not generally have zero misclassification error on the population, or on a test or a
validation set. Since the norm of W(t) will increase, if we use the logistic loss or any other convex
loss, the loss incurred on those misclassified points will also increase. More formally, consider the
logistic loss '(u) = log(1 + e-u) and define also the hinge-at-zero loss h(u) = max(0, -u). Since W
classifies all training points correctly, we have that on the training set PnN=I h(W>xn) = 0. However,
on the population we would expect some errors and so E[h(W>x)] > 0. Since W(t) ≈ Wlog t and
'(au) → ɑh(u) as a → ∞, we have:
E['(W(t)>x)] ≈ E['((logt)W>x)] ≈ (logt)E[h(W>x)] = Ω(logt).	(3.5)
That is, the population loss increases logarithmically while the margin and the population misclassifi-
cation error improve. Roughly speaking, the improvement in misclassification does not out-weight
the increase in the loss of those points still misclassified.
The increase in the test loss is practically important because the loss on a validation set is frequently
used to monitor progress and decide on stopping. Similar to the population loss, the validation loss
Lval (W (t)) = Px∈V ` W (t)> x calculated on an independent validation set V, will increase
logarithmically with t (since we would not expect zero validation error), which might cause us to
think we are over-fitting or otherwise encourage us to stop the optimization. But this increase does
not actually represent the model getting worse, merely kW(t)k getting larger, and in fact the model
might be getting better (with larger margin and possibly smaller error rate).
5
Published as a conference paper at ICLR 2018
⅛0.15-
oo
9 0.1
oo
O 0.05 -
100	1 01	102	1 03	1 04	1 05	1 06
t
100	1 01	102	1 03	1 04	1 05	1 06
t
Figure 2:	Same as Fig. 1, except we multiplied all x2 values in the dastaset by 20, and also train
using ADAM. The final weight vector produced after 2 ∙ 106 epochs of optimization using ADAM
(red dashed line) does not converge to L2 max margin solution (black line), in contrast to GD (blue
dashed line), or GDMO.
4	Extensions
We discuss several possible extensions of our results.
4.1	Multi-Class Classification with Cross-Entropy Loss
So far, we have discussed the problem of binary classification. For multi-class problems commonly
encountered, we frequently learn a predictor wk for each class, and use the cross-entropy loss with a
softmax output, which is a generalization of the logistic loss. What do the linear predictors wk (t)
converge to if we minimize the cross-entropy loss by gradient descent on the predictors? In Appendix
C we analyze this problem for separable data, and show that again, the predictors diverge to infinity
and the loss converges to zero. Furthermore, we show that, generically, the loss converges to a logistic
loss for transformed data, for which our theorems hold. This strongly suggests that gradient descent
converges to a scaling of the K-class SVM solution:
K
arg min	kwkk2 s.t. ∀n, ∀k 6= yn : wy> xn ≥ wk>xn + 1	(4.1)
w1 ,...,wK
k=1
We believe this can also be established rigorously and for generic exponential tailed multi-class loss.
4.2	Other optimization methods
In this paper we examined the implicit bias of gradient descent. Different optimization algorithms
exhibit different biases, and understanding these biases and how they differ is crucial to understanding
and constructing learning methods attuned to the inductive biases we expect. Can we characterize the
implicit bias and convergence rate in other optimization methods?
In Figure 1 we see that adding momentum does not qualitatively affects the bias induced by gradient
descent. In Figure 4 in Appendix E we also repeat the experiment using stochastic gradient descent,
and observe a similar bias. This is consistent with the fact that momentum, acceleration and
stochasticity do not change the bias when using gradient descent to optimize an under determined
least squares problems. It would be beneficial, though, to rigorously understand how much we can
generalize our result to gradient descent variants, and how the convergence rates might change in
these cases.
Employing adaptive methods, such as AdaGrad (Duchi et al., 2011) and ADAM (Kingma & Ba,
2015), does significantly affect the bias. In Figure 2 we show the predictors obtained by ADAM and
by gradient descent on a simple data set. Both methods converge to zero training error solutions.
But although gradient descent converges to the L2 max margin predictor, as predicted by our theory,
ADAM does not. The implicit bias of adaptive method has been a recent topic of interest, with Hoffer
et al. (2017) and Wilson et al. (2017) suggesting they lead to worse generalization.
6
Published as a conference paper at ICLR 2018
Figure 3: Training of a convolutional neural network on CIFAR10 using stochastic gradient descent
with constant learning rate and momentum, softmax output and a cross entropy loss, where we
achieve 8.3% final validation error. We observe that, approximately: (1) The training loss decays as a
t-1, (2) the L2 norm of last weight layer increases logarithmically, (3) after a while, the validation
loss starts to increase, and (4) in contrast, the validation (classification) error slowly improves.
Wilson et al. discuss the limit of AdaGrad on lest square problems, but fall short of providing an
actual characterization of the limit. This is not surprising, as the limit of AdaGrad on least square
problems is fragile and depends on the choice of stepsize and other parameters, and thus complicated
to characterize. We expect our methodology could be used to precisely characterize the implicit bias
of such methods on logistic regression problems. The asymptotic nature of the analysis is appealing
here, as it is insensitive to the initial point, initial conditioning matrix, and large initial steps.
More broadly, it would be interesting to study the behavior of mirror descent and natural gradient
descent, and relate the bias they induce to the potential function or divergence underlying them. A
reasonable conjecture, which we have not yet investigated, is that for any potential function Ψ(w),
these methods converge to the maximum Ψ-margin solution arg minw Ψ(w)s.t.∀n : w>xn ≥ 1.
Since mirror descent can be viewed as regularizing progress using Ψ(w), it is worth noting the results
of Rosset et al. (2004b): they considered the regularization path wλ = arg min L(w) + λ kwkpp for
similar loss function as we do, and showed that limλ→0 wλ/ kwλ kp is proportional to the maximum
Lp margin solution. Rosset et al. do not consider the effect of the optimization algorithm, and instead
add explicit regularization—here we are specifically interested in the bias implied by the algorithm
not by adding (even infinitesimal) explicit regularization.
Our analysis also covers the exp-loss used in boosting, as its tail is similar to that of the logistic loss.
However, boosting is a coordinate descent procedure, and not a gradient descent procedure. Indeed,
the coordinate descent interpretation of AdaBoost shows that coordinate descent on the exp-loss for a
linearly separable problem is related to finding the maximum L1 margin solution (Schapire et al.,
1998; Rosset et al., 2004a; Shalev-Shwartz & Singer, 2010).
4.3	Deep networks
In this paper, we only consider linear prediction. Naturally, it is desirable to generalize our results
also to non-linear models and especially multi-layer neural networks.
Even without a formal extension and description of the precise bias, our results already shed light on
how minimizing the cross-entropy loss with gradient descent can have a margin maximizing effect,
how the margin might improve only logarithmically slow, and why it might continue improving
even as the validation loss increases. These effects are demonstrated in Figure 3 and Table 1 which
portray typical training of a convolutional neural network using unregularized gradient descent2. As
can be seen, the norm of the weight increases, but the validation error continues decreasing, albeit
very slowly (as predicted by the theory), even after the training error is zero and the training loss is
extremely small. We can now understand how even though the loss is already extremely small, some
sort of margin might be gradually improving as we continue optimizing. We can also observe how
the validation loss increases despite the validation error decreasing, as discussed in Section 3.
As an initial advance toward tackling deep network, we can point out that for two special cases, our
results may be directly applied to multi-layered networks. First, our results may be applied exactly,
2Code available here: https://github.com/paper-submissions/MaxMargin
7
Published as a conference paper at ICLR 2018
Epoch	50	100	200	400	2000	4000
L2 norm	13.6	16.5	19.6	20.3	25.9	27.54
Train loss	o~QΛ	0.03	0.02	0.002	10-4	3∙10-5
Train error	~^%	1.2%	0.6%	0.07%	0%	-0%-
Validation loss	0.52	0.55	0.77	0.77	1.01	-1ΓT8-
Validation error	12.4%	10.4%	11.1%	9.1%	8.92%	8.9%
Table 1: Sample values from various epochs in the experiment depicted in Fig. 3.
as we show in Appendix D, if only a single weight layer is being optimized, and furthermore, after
a sufficient number of iterations, the activation units stop switching and the training error goes to
zero. Second, our results may also be applied directly to the last weight layer if the last hidden layer
becomes fixed and linearly separable after a certain number of iterations. This can become true, either
approximately, if the input to the last hidden layer is normalized (e.g., using batch norm), or exactly,
if the last hidden layer is quantized (Hubara et al., 2016).
4.4	Matrix Factorization
With multi-layered neural networks in mind, Gunasekar et al. (2017) recently embarked on a study
of the implicit bias of under-determined matrix factorization problems, where we minimize the
squared loss of linear observation of a matrix by gradient descent on its factorization. Since a matrix
factorization can be viewed as a two-layer network with linear activations, this is perhaps the simplest
deep model one can study in full, and can thus provide insight and direction to studying more complex
neural networks. Gunasekar et al. conjectured, and provided theoretical and empirical evidence, that
gradient descent on the factorization for an under-determined problem converges to the minimum
nuclear norm solution, but only if the initialization is infinitesimally close to zero and the step-sizes
are infinitesimally small. With finite step-sizes or finite initialization, Gunasekar et al. could not
characterize the bias. It would be interesting to study the same problem with a logistic loss instead
of squared loss. Beyond the practical relevance of the logistic loss, taking our approach has the
advantage that because of its asymptotic nature, it does not depend on the initialization and step-size.
It thus might prove easier to analyze logistic regression on a matrix factorization instead of the
least square problem, providing significant insight into the implicit biases of gradient descent on
non-convex multi-layered optimization.
5	Summary
We characterized the implicit bias induced by gradient descent when minimizing smooth monotone
loss functions with an exponential tail. This is the type of loss commonly being minimized in deep
learning. We can now rigorously understand:
1.	How gradient descent, without early stopping, induces implicit L2 regularization and
converges to the maximum L2 margin solution, when minimizing the logistic loss, or exp-
loss, or any other monotone decreasing loss with appropriate tail. In particular, the non-tail
part does not affect the bias and so the logistic loss and the exp-loss, although very different
on non-separable problems, behave the same for separable problems. The bias is also
independent of the step-size used (as long as it is small enough to ensure convergence) and
(unlike for least square problem) is also independent on the initialization.
2.	This convergence is very slow. This explains why it is worthwhile continuing to optimize
long after we have zero training error, and even when the loss itself is already extremely
small.
3.	We should not rely on slow decrease of the training loss, or on no decrease of the validation
loss, to decide when to stop. We might improve the validation, and test, errors even when
the validation loss increases and even when the decrease in the training loss is tiny.
Perhaps that gradient descent leads to a max L2 margin solution is not a big surprise to those for
whom the connection between L2 regularization and gradient descent is natural. Nevertheless, we are
not familiar with any prior study or mention of this fact, let alone a rigorous analysis and study of
8
Published as a conference paper at ICLR 2018
how this bias is exact and independent of the initial point and the step-size. Furthermore, we also
analyze the rate at which this happens, leading to the novel observations discussed above. Perhaps
even more importantly, we hope that our analysis can open the door to further analysis of different
optimization methods or in different models, including deep networks, where implicit regularization
is not well understood even for least square problems, or where we do not have such a natural guess
as for gradient descent on linear problems. Analyzing gradient descent on logistic/cross-entropy loss
is not only arguably more relevant than the least square loss, but might also be technically easier.
Acknowledgments
The authors are grateful to S. Gunasekar, J. Lee, and C. Zeno for helpful comments on the manuscript.
The research of DS was supported by the Taub foundation.
References
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal ofMachine Learning Research, 12(Jul):2121-2159, 2011. 4.2
RadhaKrishna Ganti. EE6151, Convex optimization algorithms. Unconstrained minimization: Gradi-
ent descent algorithm, 2015. 2, A.4
Suriya Gunasekar, Blake Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nathan Srebro.
Implicit Regularization in Matrix Factorization. arXiv, pp. 1-10, 2017. 4.4
Elad Hoffer, Itay Hubara, and D. Soudry. Train longer, generalize better: closing the generalization
gap in large batch training of neural networks. In NIPS (oral presentation), pp. 1-13, may 2017.
4.2
I Hubara, M Courbariaux, D. Soudry, R El-yaniv, and Y Bengio. Quantized Neural Networks:
Training Neural Networks with Low Precision Weights and Activations. Accepted to JMLR, 2016.
4.3
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter
Tang. On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima. ICLR,
pp. 1-16, 2017. doi: 10.1227/01.NEU.0000255452.20602.C9. 1
Diederik P Kingma and Jimmy Lei Ba. Adam: a Method for Stochastic Optimization. In ICLR, pp.
1-13, 2015. 1, 4.2
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the
role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014. 1
Behnam Neyshabur, Ruslan R Salakhutdinov, and Nati Srebro. Path-sgd: Path-normalized opti-
mization in deep neural networks. In Advances in Neural Information Processing Systems, 2015.
1
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. Exploring Gener-
alization in Deep Learning. arXiv, jun 2017. 1
Saharon Rosset, Ji Zhu, and Trevor Hastie. Boosting as a regularized path to a maximum margin
classifier. Journal of Machine Learning Research, 5(Aug):941-973, 2004a. 4.2
Saharon Rosset, Ji Zhu, and Trevor J Hastie. Margin Maximizing Loss Functions. In NIPS, pp.
1237-1244, 2004b. ISBN 0-262-20152-6. 4.2
Robert E Schapire, Yoav Freund, Peter Bartlett, Wee Sun Lee, et al. Boosting the margin: A new
explanation for the effectiveness of voting methods. The annals of statistics, 26(5):1651-1686,
1998. 4.2
Shai Shalev-Shwartz and Yoram Singer. On the equivalence of weak learnability and linear separabil-
ity: new relaxations and efficient boosting algorithms. Machine Learning, 80(2):141-163, Sep
2010. ISSN 1573-0565. doi: 10.1007/s10994-010-5173-z. 4.2
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, and Benjamin Recht. The Marginal
Value of Adaptive Gradient Methods in Machine Learning. arXiv, pp. 1-14, 2017. 1, 4.2
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In ICLR, 2017. 1
9
Published as a conference paper at ICLR 2018
Appendix
A Proof of main results
In the following proofs, for any solution w (t), we define
r (t) = W (t) — W log t — W,
where W and W follow the conditions of Theorems 3 and 4, that is W is the L? is the max margin
vector, which satisfies eq. 2.4:
W = argmin ∣∣Wk2 s.t. ∀n : w>Xn ≥ 1,
w∈Rd
and W is a vector which satisfies eq. 2.7:
∀n ∈ S : η exp (—x>W) = an ,	(A.1)
where we recall that we denoted XS ∈ Rd×lSl as the matrix whose columns are the support vectors,
a subset S ⊂ {1, . . . , N} of the columns of X = [X1, . . . , XN] ∈ Rd×N.
In Lemma 8 (Appendix F) we prove that for almost every dataset α is uniquely defined, there are no
more then d support vectors and αn 6= 0, ∀n ∈ S. Therefore, eq. A.1 is well-defined in those cases.
If the support vectors do not span the data, then the solution W to eq. A.1 might not be unique. In
this case, we can use any such solution in the proof.
We furthermore denote
θ = min x> W > 1,	(A.2)
n∈S n
and by Ci ,i ,ti (i ∈ N) various positive constants which are independent of t. Lastly, we define
P1 ∈ Rd×d as the orthogonal projection matrix3 to the subspace spanned by the support vectors (the
columns of XS), and P2 = I — P1 as the complementary projection (to the left nullspace of XS).
A.1 Simple proof of Theorem 3 for a special case
In this section we first examine the special case that ` (u) = e-u and take the continuous time limit
of gradient descent: η → 0 , so
W (t) = —VL (w (t)).
The proof in this case is rather short and self-contained (i.e., does not rely on any previous results),
and so it helps to clarify the main ideas of the general (more complicated) proof which we will give
in the next sections.
Recall we defined
r (t) = w (t) — log (t) W — W .	(A.3)
Our goal is to show that ∣∣r (t) ∣∣ is bounded, and therefore P (t) = r (t) + W is bounded. Eq. A.3
implies that
r (t) = W (t) — ；W = —VL (w (t)) — ；W	(A.4)
and therefore
1 -d kr (t)k2 =r >(t) r (t)
N1
EeXP (—x>w ⑻ χ>r (t) — -W>r (t)
n=1
X exp (— log (t) W>Xn — W>Xn — x>r (t)) x>r (t) — ；W>r (t)
n∈S
+ EeXP (— log (t) W>Xn — W>Xn — x>r(t)) x>r(t) ,	(A.5)
n/S
3This matrix can be written as P1 = XSXS+, where M+ is the Moore-Penrose pseudoinverse of M.
10
Published as a conference paper at ICLR 2018
where in the last equality we used eq. A.3 and decomposed the sum over support vectors S and
non-support vectors. We examine both bracketed terms.
Recall that w^>Xn = 1 for n ∈ S, and that We defined (in eq. A.1) W so that
Pn∈s exp (-W>Xn) Xn = W^. Thus, the first bracketed term in eq. A.5 can be written as
1 X exp (-W>Xn - x>r (t)) x>r (t) - 1 X exp (-W>Xn) Xn
n∈S	n∈S
=1 X exp (-W>Xn) (exp (-x>r (t)) - 1) x>r (t) ≤ 0	(A.6)
n∈S
since Z (e-z - 1) ≤ 0. Furthermore, since exp (-z) Z ≤ 1 and θ = argmi□n∈sx>W (eq. A.2), the
second bracketed term in eq. A.5 can be upper bounded by
X exp (- log(t) W>Xn - W>Xn) ≤ tθ X exp (-W>Xn) .	(A.7)
n/S	n/S
Substituting eq. A.6 and A.7 into eq. A.5 and integrating, we obtain, that ∃C, C0 such that
∀t1,∀t > t1 : kr (t)k2 - ||r(t1)||2 ≤ C
Zt
t1
≤ C0 < ∞ ,
since θ > 1 (eq. A.2). Thus, we showed that r(t) is bounded, which completes the proof for the
special case.
A.2 Proof of Theorem 3 (General case)
Next, we give the proof for the general case (discrete time, and exponentially-tailed functions).
Though it is based on a similar analysis as in the special case we examined in the previous section, it
is somewhat more involved since we have to bound additional terms.
First, we state two auxilary Lemmata, which are proven below in appendix sections A.4 and A.5:
Lemma 5. Let L (W) be a β-smooth non-negative objective. If η < 2β-1, then, for any W(0), with
the GD sequence
W (t +1) = W (t) — ηVL(W(t))	(A.8)
we have that P∞=0 ∣∣VL (w (u))k2 < ∞ and therefore limt→∞ ∣∣VL (w (t))∣∣2 = 0.
Lemma 6. We have
∃C1,t1 : ∀t>tι : (r (t + 1) - r (t))> r (t) ≤ Cγt-min(θ,-1-1.5μ+,-1-0.5μ-) .	(A.9)
Additionally, ∀1 > 0 , ∃C2 , t2, such that ∀t > t2, if
∣P1r (t)∣ ≥ 1,	(A.10)
then the following improved bound holds
(r (t + 1) - r (t))> r (t) ≤ -C2t-1 < 0 .	(A.11)
Our goal is to show that ∣r (t)∣ is bounded, and therefore P (t) = r (t) + W is bounded. To show
this, we will upper bound the following equation
∣r(t+1)∣2=∣r(t+1)-r(t)∣2+2(r(t+1)-r(t))>r(t)+∣r(t)∣2	(A.12)
First, we note that first term in this equation can be upper-bounded by
∣r(t+1)-r(t)∣2
=) ∣∣w (t + 1) — Wlog (t + 1) — W — w (t) + Wlog (t) + W∣∣2
=k-ηVL (w (t)) - W[log(t + 1) - log(t)]k2
=η2 ∣∣VL (w (t))∣2 + kW∣2 log2 (1 +1-1) + 2ηW>VL (w (t))log (1 +1-1)
(3)
≤ η2∣VL(w(t))∣∣2 + ∣∣w∣∣21-2	(a.13)
11
Published as a conference paper at ICLR 2018
where in (1) we used eq. 2.3, in (2) we used eq. 2.2, and in (3) we used ∀x > 0 : x ≥ log (1 + x) >
0, and also that
N
W> VL (w (t)) = ^X '0 (w (t)> Xn) W>Xn ≤ 0,	(A.14)
n=1
since W>Xn ≥ 1 (from the definition of W) and '0(u) ≤ 0.
Also, from Lemma 5 we know that
∞
kVL(W(t))k2=o(1)and X kVL (W (t))k2< ∞ .	(A.15)
t=0
Substituting eq. A.15 into eq. A.13, and recalling that a t-ν power series converges for any ν > 1,
we can find C0 such that
∞
kr (t + 1) - r (t)k2 = o (1) and X kr (t + 1) - r (t)k2 = C0 < ∞ .	(A.16)
Note that this equation also implies that ∀0
∃t0 : ∀t > t0 : |kr (t + 1)k - kr (t)k| < 0 .	(A.17)
Next, we would like to bound the second term in eq. A.12. From eq. A.9 in Lemma 6, we can find
t1, C1 such that ∀t > t1:
(r(t +1) — r (t))> r (t) ≤ gt- min(θ,-1-1.5μ+,-1-0.5μ-)
Thus, by combining eqs. A.18 and A.16 into eq. A.12, we find
kr (t)k2 - kr (t1)k2
t-1
= X kr(u+1)k2 - kr (u)k2
u=t1
t-1
≤ C0 + 2 X C1u-min(θ,-1-1.5μ+,-1-0.5μ-)
u=t1
(A.18)
which is a bounded, since θ > 1 (eq. A.2). Therefore, kr (t)k is bounded.
A.3 Proof of Theorem 4
All that remains now is to show that ∣∣r (t)k → 0 if rank (XS) = rank (X), and that W is unique
given W (0). To do so, this proof will continue where the proof of Theorem 3 stopped, using notations
and equations from that proof.
Since r (t) has a bounded norm, its two orthogonal components r (t) = P1r (t) + P2r (t) also have
bounded norms (recall that P1, P2 were defined in the beginning of appendix section A). From eq.
2.2, VL (W) is spanned by the columns of X. If rank (XS) = rank (X), then it is also spanned
by the columns of XS, and so P2VL (W) = 0. Therefore, P2r (t) is not updated during GD, and
remains constant. Since W in eq. 2.3 is also bounded, We can absorb this constant P2r (t) into W
without affecting eq. 2.7 (since ∀n ∈ S : Xn>P2r (t) = 0). Thus, without loss of generality, we can
assume that r (t) = P1r (t).
Now, recall eq. A.11 in Lemma 6
∃C2, t2 : ∀t > t2 : (r (t + 1) - r (t))> r (t) ≤ -C2t-1 < 0 .
Combining this with eqs. A.12 and A.16, implies that ∃t3 > max [t2, t0] such that ∀t > t3 such that
∣r (t)∣ > 1, we have that ∣r (t + 1)∣2 is a decreasing function since then
∣r(t+1)∣2-∣r(t)∣2 ≤ -C3t-1 <0.	(A.19)
12
Published as a conference paper at ICLR 2018
Additionally, this result also implies that we cannot have kr (t)k > 1 ∀t > t3 , since then we arrive
to the contradiction.
t-1	t-1
kr (t)k2 - kr (t3)k2 = X kr(u+1)k2-kr(u)k2 ≤-XC3u-1→-∞,
u=t3	u=t3
Therefore, ∃t4 > t3 such that kr (t4)k ≤ 1. Recall also that kr (t)k is a decreasing function
whenever kr (t)k ≥ 1 (eq. A.19). Also, recall that t4 > t0, so from eq. A.17, we have that
∀t > t4,|kr (t + 1)k - kr (t)k| < 0. Combining these three facts we conclude that ∀t > t4 :
kr (t)k ≤ 1 + 0. Since this reasoning holds ∀1, 0, this implies that kr (t)k → 0.
Lastly, We note that since P2r (t) is not updated during GD, We have that P2 (W - W (0)) = 0. This
sets W uniquely, together with eq. 2.7. ■
A.4 Proof of Lemma 5
Lemma 5. Let L (W) be a β-smooth non-negative objective. If η < 2β-1, then, for any W(0), with
the GD sequence
W (t + 1) = W (t) - ηVL (W(t))
we have that Pu∞=0 kVL (W (u))k2 < ∞ and therefore limt→∞ kVL (W (t))k2 = 0.
(A.8)
This proof is a slightly modified version of the proof of Theorem 2 in (Ganti, 2015). Recall a
Well-knoWn property of β-smooth functions:
f (X) - f (y) - vf(y)> (χ - y)∣ ≤ βk kχ - yk2 .
(A.20)
From the β-smoothness of L (W)
L (w (t	+ 1)) ≤ L	(w	(t)) + VL (w (t))> (w	(t + 1)	- w (t))	+ β ∣∣w (t	+ 1) - W (t)∣∣2
=L	(w	(t)) - η ∣VL (w (t))k2	+ βη2	∣VL (w	(t))k2
=L (w (t)) - η (1- β2η)kVL(w (t))k2
Thus, We have
L (W (t)) - L (W (t + 1))
≥ kVL (w (t))k2
Which implies
t
EkVL(W (u))k2 旺
L (W (u)) - L (W (u + 1)) L (W (0)) - L (W (t + 1))
u=0
u=0
t
The right hand side is upper bounded by a finite constant, since L (W (0)) < ∞ and 0 ≤
L (W (t + 1)). This implies
∞
XkVL(W(u))k2<∞,
u=0
and therefore kVL (W (t))k2 → 0.
A.5 Proof of Lemma 6
Recall that we defined r (t) = W (t) - Wlog t - W, with W and W follow the conditions of the
Theorems 3 and 4, i.e. W is the L? max margin vector and (eq. 2.4), and eq. 2.7 holds
∀n ∈ S : η exp (-x>W) = an .
13
Published as a conference paper at ICLR 2018
Lemma 6. We have
∃Cι,tι ： ∀t>tι ： (r (t + 1) - r (t))> r (t) ≤ C1t-min(θ,- 1 — 1.5"+,-l-0.5u) .	(a.9)
Additionally, ∀∈ι > 0, ∃C2,t2, SUCh that∀t > t2, if
∣∣P1r(t)∣∣≥ j	(A.10)
then thefollowing improved bound holds
(r (t + 1) - r (t))> r (t) ≤ -C2t-1 < 0 .	(A.11)
From Lemma 1, ∀n : limt→∞ W (t)> Xn = ∞. In addition, from assumption 3 the negative loss
derivative —' (u) has an exponential tail e-u (recall We assume a = C = 1 without loss of generality).
Combining both facts, we have positive constants μ-, μ+, t- and t+ such that ∀n
∀t > t+ ： —' (W (t)> Xn)
∀t > t- ： —' (W (t)> Xn)
≤ (1+exp (—μ+W (t)> Xn) ) exp (—W (t)> Xn
≥ (1 — exp (一μ-W (t)> Xn) ) exp (—W (t)> Xn
(A.21)
(A.22)
Next, we examine the expression we wish to bound, recalling that r (t) = W (t) — W log t — W:
(r (t +1)— r (t))T r (t)
=(—ηVL (w (t)) — W [log (t + 1) — log (t)])> r (t)
N
=-η X 0 (W (t)T Xn) XTr (t) — Wτr (t) log (1 + t-1)
n=1
=Wτr (t) [t-1 — log (1 +1-1)] — η X ' (w (t)TXn) XTr (t)
n∈S
—η X [t-1 exp (—WTXn) + a (w (t)> Xn)] X>r (t)
n∈S
where in last line we used eqs. 2.6 and 2.7 to obtain
W= E QnXn = η £ exp (-WTXn) Xn .
n∈S	n∈S
We examine the three terms in eq. A.23. The first term can be upper bounded by
WTr (t) [t-1 — log (1 + t-1)]
≤ max[WTr (t), 0] [t-1 — log (1+1-1)]
≤ max [WTPIr (t), 0] t-2
(≤) [∣∣W∣∣e1t-2	, if ∣Rr (t)∣∣≤ q
—∖o(t-1)	, if IRr (t)∣∣>e1
(A.23)
(A.24)
where in (1) we used that P2W = P2XSα = 0 from eq. 2.6, and in (2) we used that WT r (t) = o (t),
since
WTr (t) = WT (W (0) — η ^X VL (w (u)) — W log (t) — W)
u = 0
≤ WT (w (0) — W — W log (t)) — ηt min WtVL (w (u)) = o (t)
—	0≤u≤t
where in the last line we used that VL (w (t)) = o (1), from Lemma 5.
14
Published as a conference paper at ICLR 2018
Next, We upper bound the second term in eq. A.23, ∀t > t+:
—η X 40 (W (t)τ Xn) x>r (t)
n∈S
≤-η X ' (w ⑴ TXn) XTr ⑴
n/S: x>r(t)≥0
Vη	X (1 + exp (-μ+w (t)T Xn)) exp (-w (t)T Xn) XTr (t)
n/S: x>r(t)≥0
Vη	X (1 + t-"+x>应 exp (-μ+WTXn - 〃+*口 (t))) t-x>w exp (-WTXn - XTr (t)) XTr (t)
n/S: x>r(t)≥0
≤)η	X (1 + t-μ+x> 应 exp(-μ+WT Xn))
n/S: x>r(t)≥0
t-x>w exp (-WTXn)
VηN (1 + [t-θ exp (- min WTXn)] *+)exp (- minWTXn) t-θ
(5)
≤ 2ηN exp
T
一min W
n
t-θ ,∀t >t1,
(A.25)
where in (1) we used eq. A.21, in (2) we used W (t) = Wlog t + W + r (t), in (3) we used
xe-x ≤ 1 and XTr (t) ≥ 0, in (4) we used θ > 1, from eq. A.2 and in (5) we defined tl=
max [t+, exp (minn WTXn)].
Lastly, we will aim to bound the sum in the third term in eq. A.23
-η X 卜TeXP (-WTXn) + '0 (W (t)T Xn)] XTr (t) .	(A.26)
n∈S
We examine each term k in this sum, and divide into two cases, depending on the sign of XTr (t).
First, if XTr (t) ≥ 0, then term k in eq. A.26 can be upper bounded ∀t > t+, using eq. A.21, by
ηt-1 exp (-WTXk) [(1 + t-μ+ exp (-μ+WTXk)) exp (-乂门⑴)-1] XTr (t)	(A.27)
We further divide into cases:
1.	IflXTr(t) I ≤ C0t-0∙5μ+, then we can upper bound eq. A.27 with
ηexp (- (1 + μ+) min WTXn) C0t-1-1-5μ+ .	(A.28)
2.	If ∣xTr(t) ∣ > C¾t-0∙5μ+, then we can find 及 > t[ to upper bound eq. A.27 ∀t > 坟:
ηt-1e-wTXk [(1+ t-μ+ e-"+wTXk) exp (-00t-0∙5μ+ ) - 1] XTr (t)
≤ηt-1e-λwTXk [(1+ t-μ+ e-"+wTXk) (1 - 00t-0∙5μ+ + C0⅛-μ+ ) - 1] XTr (t)
≤ηt-1e-wTXk [(1 - 00t-0∙5μ+ + C0⅛-μ+ ) e-μ+ min“应TXnt-μ+ - 00t-0∙5μ+ + C0⅛-μ+] XTr (t)
(2)
≤ 0, ∀t>t+	(A.29)
where in (1) we used the fact that e-x ≤ 1 - X + x2 for X ≥ 0 and in (2) we defined t] so
that the previous expression is negative ——this is possible since t-0∙5μ+ decreases slower
then t-μ+.
3.	If ∣ XTr(t) ∣	≥	62, then we define t+0	>	t] such that t+0	>
exp (minn WTXn) [e0^5e2 — 1] ""+,	and therefore ∀t	>	t++0,	we have
(1 + t-μ+ exp (-μ+WTXn)) e-e2 < e-0-5e2 .
This implies that ∀t > t0+00 we can upper bound eq. A.27 by
-η exp (- max WTXn) (1 — e-0-5e2)0t-1.	(A.30)
15
Published as a conference paper at ICLR 2018
Second, if x>r(t) < 0, We again further divide into cases:
1.
If I x>r(t) I ≤ C0t-0∙5μ-, then, since —' (W (t)> Xn)
eq. A.26 with
> 0, we can upper bound term k in
ηt-1 exp (-WTXk) ∣ x>r (t) ∣ ≤ ηexp (一 min WTXn) C0t-l-0∙5μ-	(A.31)
2.	If ∣ XTr (t) ∣ > C0t-0∙5μ- , then, using eq. A.22 we upper bound term k in eq. A.26 with
η [-t-1e-wTXk- 20 (W (t)τ χk)] χ[r (t)
≤η [—t-1e-wTXk + (1 - exp (-μ-W (t)T Xk)) exp (-w (t)T Xk)] x>r (t)
=ηt-1e-wTXk [1-exp (-rT (t) Xk)(1 - [t-1e-wTXk exp (-rT (t) Xk)『)]∣ XTr (t) ∣
(A.32)
Next, we will show that ∃t- > t_ such that the last expression is strictly negative ∀t > t-.
Let M > 1 be some arbitrary constant. Then, since [t-1e-wTXk exp (-rT (t) Xk)] ”
exp (-μ-W (t)T Xk) → 0 from Lemma 1, ∃t^ > t_ such that ∀t > t^, t > Me-WT
and if exp (-rT (t) Xk) ≥ M > 1 then
exp (-rT (t) Xk)(1 - [t-1e-wTXk exp (-rT (t) Xk)『)≥ MZ > 1.	(A.33)
Furthermore, if ∃t > tM such that exp (-rT (t) Xk) < M, then
exp (-rT (t) Xk) (1 -卜-1e-wTXk exp (-rT (t) Xk)『)
> exp (-rT (t) Xk) (1 - [t-1e-^TXkM]*-) .	(A.34)
which is lower bounded by
(1 + C0t-0∙5μ -) (1 - t-μ- [e-®TXkM『一)
≥1 + C0t-0∙5μ- - t-μ- [e-WTXkM]μ- - t-1∙5μ- [e-®TXkM]" C0
since ∣ XTr (t) ∣ > C0t-0∙5μ-, XTr (t) < 0 and ex ≥ 1 + x. In this case last line is strictly
larger then 1 for sufficiently large t. Therefore, after we substitute eqs. A.33 and A.34 into
A.32, we find that ∃t- > tM > t_ such that ∀t > t_, term k in eq. A.26 is strictly negative
η [-t-1e-^TXk- 'z (w (t)T Xk)] XTr (t) < 0	(A.35)
3.	If ∣ xTr(t) ∣ ≥ €2 , which is a special case of the previous case (∣ XTr (t) ∣ > C¾t-0∙5μ-)
then ∀t > t-, either eq. A.33 or A.34 holds. Furthermore, in this case, ∃t- > t_ and
Mzz > 1 such that ∀t > t_ eq. A.34 can be lower bounded by
exp(e2) (1 - [t-1e-应TXk M]μ- ) > MZZ > 1.
Substituting this, together with eq. A.33, into eq. A.32, we can find C0 > 0 such we can
upper bound term k in eq. A.26 with
-C01-1, ∀t>t-.	(A.36)
To conclude, we choose t° = max [t?，t-]:
1.	IfkPIr (t)k ≥ €1 (as in Eq. A.10), we have that
max ∣ XTr (t) ∣ 2 ≥ ∙∩^X∣ XTP1r(t) ∣ 2 = -ɪ- ∣∣XtP1 r (t) ∣∣2 ≥ ɪσ!i (XS) €2
n∈S I n vy∣ — |S| / 」In 1 vy∣ |S| Il S 1 '	一 |S| min ∖ s) 1
1 1 n∈S	1 1	1 1
(A.37)
16
Published as a conference paper at ICLR 2018
where in (1) we used P1>xn = xn ∀n ∈ S, in (2) we denoted by σmin (XS), the minimal
non-zero singular value of XS and used eq. A.10. Therefore, for some k, xk>r ≥ 2 ,
|S |-1 σm2 in (XS) 12 . In this case, we denote C000 as the minimum between C00 (eq. A.36)
and ηexp (- maxn W>Xn)(1 - e-0.5e2) €2 (eq. A.30). Then We find that eq. A.26 can
be upper bounded by -C000t-1 + o t-1 , ∀t > t0, given eq. A.10. Substituting this result,
together With eqs. A.24 and A.25 into eq. A.23, We obtain ∀t > t0
(r (t +1) - r (t))> r (t) ≤ -C0zt-1 + o (t-1).
This implies that ∃C2 < C000 and ∃t2 > t0 such that eq. A.11 holds. This implies also that
eq. A.9 holds for kP1r (t)k ≥ €1.
2.	OtherWise, if kP1r (t)k < €1, We find that ∀t > t0 , each term in eq. A.26 can be upper
bounded by either zero (eqs. A.29 and A.35), or terms proportional to t-1-1-5μ+ (eq, A.28)
or t-1-0.5μ-, (eq. A.31). Combining this together with eqs. A.24, A.25 into eq. A.23 we
obtain (for some positive constants C3, C4, C5 , and C6)
(r (t + 1) - r (t))> r (t) ≤ C3t-1-1.5μ+ + C4t-1-0.5μ- + C5t-2 + C6t-θ .
Therefore, ∃t1 > t0 and C1 such that eq. A.9 holds.
B Calculation of convergence rates
From Theorem 3, we can write W (t) = Wlog t + P (t), where P (t) has bounded norm.
Calculation of normalized weight vector (eq. 3.1):
W (t)
kW (t)k
P (t) + Wlog t
JP (t)> P (t) + W>Wlog21 + 2p (t)> Wlog t
P (t) / log t + W
kWk J1 + 2P (t)> W/ (kWk2 logt) + kρ (t)k2 / (kWk2log2 t
⅛ (P (t)l⅛ + w
W
-----+
kWk +
1	P (t)> W ,
1-------+------r
kWI『 log t
P (t) W P (t)> W
----------------7T-
kWk kWk kWk2
3 (2 修!2
kρ (t)k2
2 kWk2
■—2—+ O
log2 t
(B.1)
∣⅛ + O
, and in the last line we used the
=jw_ + (I	WW> ʌ p^)ɪ + O
kwk	∖	∣Wk2J kwk logt
where to obtain eq. B.1 we used √1+χ = 1 - 2x + 3x
fact that P (t) has a bounded norm.
We use eq. B.1 to calculate the angle (eq. 3.2):
w (t)> W
kw (t)kkwk
∣⅛ (P (t)l⅛ + w )(1-
=1∣2 kP (t)k2
=+ ^ihT
>2
P (t) w ∖	1
kWkkρ (t)k
1 ρ (t)> W
1
4 log21
log t IIWk2
—
kρ (t)k2
2 kWk2
2—2—+ O
log2 t
—
—
17
Published as a conference paper at ICLR 2018
Calculation of margin (eq. 3.3):
min x>W (t)
n
>
min xn
n
W P (t)> Wʌ _1_ + O
kwk kw∣2 l logt
----+
kWk十
V∣^w k
1
----+
kWk十
ρ⅛w^)ι⅛+O
(B.2)
W
where in eq. B.2 we used eq. A.2.
Calculation of the training loss (eq. 3.4):
N
L (W (t)) ≤ X (1 + exp (—μ+w (t)> Xn)) exp (—w (t)> Xn)
N
=X (1 + exp (一μ+ (ρ (t) + Wlog t)> Xn)) exp (一 (ρ (t) + Wlog t)> Xn
n=1
N
=X (1 + t-μ+wTxn exp (—μ+ρ (t)>Xn)) exp (—ρ (t)> xn) t-wTxn
n=1
=1 ^X e-ρ(t)>xn + O (t-maχ^,1+μ+).
n∈S
Next, we give an example demonstrating the bounds above are strict. Consider optimization with
and exponential loss ' (U) = e-u, and a single data point X = (1,0). In this case W = (1,0) and
∣W k = 1. We take the limit η → 0, and obtain the continuous time version of GD:
W1 (t) = exp (—w (t)) ; W2 (t) = 0.
We can analytically integrate these equations to obtain
W1 (t) = log (t + exp (W1 (0))) ; W2 (t) = W2 (0) .
Using this example with W2 (0) > 0, it is easy to see that the above upper bounds are strict.
Lastly, recall that V is a set of indices for validation set samples. We calculate of the validation
loss for logistic loss, if the error of the L2 max margin vector has some classification errors on the
validation, i.e., ∃k ∈ V : W>Xk < 0:
Lval (W (t)) = X log 1 + exp —W (t)> Xn
n∈V
≥ log 1 + exp —W (t)> Xk
=log (1 + exp (— (P (t) + W log t)> Xk))
=log (exp (— (p (t) + Wlogt)> Xk) (1 + exp ((ρ (t) + Wlogt)> Xk)))
≥ — (ρ (t) + Wlogt)> Xk + log (1 + exp ((ρ (t) + Wlogt)> Xk))
≥ — log tW>Xk + P (t)> Xk
C S oftmax output with cross-entropy loss
We examine multiclass classification. In the case the labels are the class index yn ∈ {1, . . . , K} and
we have a weight matrix W ∈ RK ×d with Wk being the k-th row of W.
18
Published as a conference paper at ICLR 2018
Furthermore, we define w = vec W> , a basis vector ek ∈ RK so that(ek)i = δki, and the matrix
Ak ∈ RdK ×d so that Ak = ek 0 Id, where 0 is the Kronecker product and Id is the d-dimension
identity matrix. Note that Ak>w = wk.
Consider the cross entropy loss with softmax output
L(W)
-XX log (	exP (w>nXn)
n=1	∖PK=1 exP(w>xn)
Using our notation, this loss can be re-written as
exp (w>Ayn Xn)
PkK=1 exp (w>Akxn)
NK
Elog (EeXp (WT(Ak- Ayn) xn)
n=1	k=1
(C.1)
Therefore
N
VL(W) = X
PK=I exP (WT(Ak- Ayn) Xn) (Ak- Ayn ) χn
n=1
NK
XX
n=1 k=1
PrK=1 exp (W> (Ar - Ayn)xn)
(Ak - Ayn)xn.
1
If, again, make the assumption that the data is strictly linearly separable, i.e., in our notation
Assumption 4. ∃w* such that WT (Ak — Ayn) Xn < 0 ∀k = yn1.
then the expression
NK	T
W>VL (W) = XX K w* (Ak- Ayn)Xn_
n=1 七 Pr=I exP (WT (Ar- Ak) Xn)
is strictly negative for any finite W. However, from Lemma 5, in gradient descent with learning
rate η > 2β-1, we have that VL	(W (t)) → 0.	This implies that:	kW (t)k →	∞, and ∀k	6=
yn, ∃r : W (t)T (Ar - Ak) Xn →	∞, which implies ∀k	6= yn,maxk	W (t)T (Ak	- Ayn) Xn	→
-∞. Examining the loss (eq. C.1)	we find that L	(W (t))	→ 0 in this	case. Thus,	we arrive to	an
equivalent Lemma to Lemma 1, for	this case:
Lemma 7. Let W (t) be the iterates of gradient descent (eq. 2.2) with η < 2β-1, for cross-
entropy loss operating on a softmax output, under the assumption of strict linear separability
(Assumption 4), then: (1) limt→∞ L (W (t)) = 0, (2) limt→∞ kW (t)k = ∞, and (3) ∀n, k 6= yn :
limt→∞ W (t)T (Ayn - Ak) Xn = ∞.
Therefore, since
L (W (t)) = Xlog XexP W (t)T (Ak -Ayn)Xn
N
≈ X log 1 + max exP W (t)T (Ak - Ayn) Xn	,
n=1	yn
(C.2)
where in the last line we assumed for simplicity that W (t)T (Ak - Ayn ) Xn has a unique minimum
in k, since then the other exponential terms inside the log become negligible. If
argmax exP W (t)T (Ak - Ayn) Xn
k6=yn
19
Published as a conference paper at ICLR 2018
1 + exp -W
has a limit kn, then We define Xn = (Ayn — Akn) Xn, so eq. C.2 is transformed to the standard
logistic regression loss
N
X log
n=1
to Which our Theorems directly apply.
Therefore, W (t) / ∣∣w (t)k → W where
W = argmin ∣∣w∣2 s.t. ∀n : w>Xn ≥ 1
w
Recalling that Ak>W = Wk, we can re-write this as
K
arg min ∣Wk ∣2 s.t. ∀n, ∀k 6= yn : Wy>n Xn ≥ Wk>Xn + 1
w1 ,...,wK
k=1
D	Deep networks, if only a single layer is optimized
We examine a deep neural network (DNN) with m = 1, . . . , L layers, piecewise linear activation
functions fl and loss function ` following assumption 3, parameterized by weights matrices Wl .
Since f are piecewise linear, we can write for almost every u: fι (u) = Vfi (u) Θ U (an element-wise
product). Given an input sample Xn, for each layer l the input un,l and output vn,l are calculated
sequentially in a “forward propagation”
un,l =Wlvn,l-1 ; vn,l = fl (un,l )	(D.1)
initialized by vn,0 = Xn. Then, given the DNN output un,L and target yn ∈ {-1, 1} the loss
` (ynun,L) can be calculated.
During training, the gradients of the loss are calculated using the chain rule in a “back-propagation”
δn,l-1 = [Vfl (un,l)]Wl>δn,l	(D.2)
initialized by δn,L = 1. Finally, the weights are updated with GD. The basic update (without weight
sharing) is
N∂
Wi (t +1) - Wl (t) = -η £ dW' (ynUn,L)	(D.3)
n=1 ∂Wl
N
=-η〉： yn' (ynun,L) δn,lvn,i-1	(D.4)
n=1
N
=-η X yn' (ynδ>,iwivn,l-1 δn,lvn,l-1 ,	(D.5)
n=1
where in the last line we used
∀l : un,L =WL LY-1 Vfl (un,m) Wm Xn = δn>,lWlvn,l-1.
m=1
Denoting Xn,ι = ynδn,ι 0 Vn,ι-ι and Wi = Vec(W>) we obtain
N
Wl (t + 1) — Wl (t) = —η X '0 (w>Xn,l) Xn,l .
n=1
We got the same update as in eq. 2.2. Thus, if Xn,l does not change between iterations and becomes
linearly separable so the training error can go to zero, we can apply Theorem 3. This can happen if
we only optimize Wl, and the activation units stop crossing their thresholds, after a sufficient number
of iterations.
20
Published as a conference paper at ICLR 2018
E An experiment with stochastic gradient descent
100	101	102	103	104	1 05
t
%Io-510-,°— >0X)6≡ ≡
)M)电 卷 UgJen
Figure 4: Same as Fig. 1, except stochastic gradient decent is used (with mini-batch of size 4), instead
of GD.
F	Generic solutions of the KKT conditions in eq. 2.6
Lemma 8. For almost all datasets there is a unique α which satisfies the KKT conditions (eq. 2.6):
N
W = ɪ2 αnXn	∀n (an ≥ 0 and W>Xn = 1)OR (an = 0 and W>Xn > 1)
n=1
Furthermore, in this solution an = 0 if W>Xn = 1, i.e., Xn is a support vector (n ∈ S), and there
are at most d such support vectors.
Proof. For almost every set X, no more than d points Xn can be on the same hyperplane. Therefore,
since all support vectors must lie on the same hyperplane, there can be at most d support vectors, for
almost every X.
Given the set of support vectors, S, the KKT conditions of eq. 2.6 entail that an = 0 ifn ∈/ S and
1 = X>W = X>Xsas ,	(F.1)
where we denoted αS as α restricted to the support vector components. For almost every set X,
since d ≥ |S|, X>Xs ∈ RlSl×lSl is invertible. Therefore, as has the unique solution
(X> XS) 1 1 = as .	(F.2)
This implies that ∀n ∈ S, an is equal to a rational function in the components of XS, i.e., an =
pn (Xs) /qn (Xs), where pn and qn are polynomials in the components of XS. Therefore, if an = 0,
then pn (Xs ) = 0, so the components of Xs must be at a root of the polynomial pn . The roots of the
polynomial pn have measure zero, unless ∀Xs : pn (Xs) = 0. However, pn cannot be identically
equal to zero, since, for example, if X> = [l∣s∣×∣s∣, O∣s∣×(d-∣s∣)], then X>Xs = I∣s∣×∣s∣, and so
in this case ∀n ∈ S, an = 1 6= 0, from eq. F.2.
Therefore, for a given S, the event that "eq. F.1 has a solution with a zero component" has a zero
measure. Moreover, the union of these events, for all possible S, also has zero measure, as a finite
union of zero measures sets (there are only finitely many possible sets S ⊂ {1, . . . , N} ). This
implies that, for almost all datasets X, an = 0 only if n ∈/ S. Furthermore, for almost all datasets
the solution a is unique: for each dataset, S is uniquely deteremined, and given S , the solution eq.
F.1 is uniquely given by eq. F.2.	□
21