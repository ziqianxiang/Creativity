Published as a conference paper at ICLR 2018
Training Generative Adversarial Networks
via Primal-Dual Subgradient Methods: A
Lagrangian Perspective on GAN
tXu Chen , Jiang WangjHao Ge *
t Department of EECS, Northwestern University, Evanston, IL, USA
^ Google Inc.
{chenx,haoge2013}@u.northwestern.edu
wangjiangb@gmail.com
Ab stract
We relate the minimax game of generative adversarial networks (GANs) to finding
the saddle points of the Lagrangian function for a convex optimization problem,
where the discriminator outputs and the distribution of generator outputs play the
roles of primal variables and dual variables, respectively. This formulation shows
the connection between the standard GAN training process and the primal-dual
subgradient methods for convex optimization. The inherent connection does not
only provide a theoretical convergence proof for training GANs in the function
space, but also inspires a novel objective function for training. The modified
objective function forces the distribution of generator outputs to be updated along
the direction according to the primal-dual subgradient methods. A toy example
shows that the proposed method is able to resolve mode collapse, which in this
case cannot be avoided by the standard GAN or Wasserstein GAN. Experiments on
both Gaussian mixture synthetic data and real-world image datasets demonstrate
the performance of the proposed method on generating diverse samples.
1	Introduction
Generative adversarial networks (GANs) are a class of game theoretical methods for learning data
distributions. It trains the generative model by maintaining two deep neural networks, namely the
discriminator network D and the generator network G. The generator aims to produce samples
resembling real data samples, while the discriminator aims to distinguish the generated samples and
real data samples.
The standard GAN training procedure is formulated as the following minimax game:
minmax Ex〜pd(χ){log D(x)} + Ez〜pz(z){log(1 - D(G(Z)))},	(1)
where pd(x) is the data distribution and pz(z) is the noise distribution. The generated samples G(z)
induces a generated distribution pg(x). Theoretically, the optimal solution to (1) is p*g = pd and
D*(x) = 1/2 for all x in the support of data distribution.
In practice, the discriminator network and the generator network are parameterized by θd and θg,
respectively. The neural network parameters are updated iteratively according to gradient descent.
In particular, the discriminator is first updated either with multiple gradient descent steps until
convergence or with a single gradient descent step, then the generator is updated with a single descent
step. However, the analysis of the convergence properties on the training approaches is challenging,
as noted by Ian Goodfellow in (Goodfellow, 2016), “For GANs, there is no theoretical prediction as
to whether simultaneous gradient descent should converge or not. Settling this theoretical question,
and developing algorithms guaranteed to converge, remain important open research problems.".
There have been some recent studies on the convergence behaviours of GAN training (Nowozin
et al., 2016; Li et al., 2017b; Heusel et al., 2017; Nagarajan & Kolter, 2017; Mescheder et al., 2017).
*The first two authors have equal contributions.
1
Published as a conference paper at ICLR 2018
The simultaneous gradient descent method is proved to converge assuming the objective function
is convex-concave in the network parameters (Nowozin et al., 2016). The local stability property is
established in (Heusel et al., 2017; Nagarajan & Kolter, 2017).
One notable inconvergence issue with GAN training is referred to as mode collapse, where the
generator characterizes only a few modes of the true data distribution (Goodfellow et al., 2014; Li
et al., 2017b). Various methods have been proposed to alleviate the mode collapse problem. Feature
matching for intermediate layers of the discriminator has been proposed in (Salimans et al., 2016). In
(Metz et al., 2016), the generator is updated based on a sequence of previous unrolled discriminators.
A mixture of neural networks are used to generate diverse samples (Tolstikhin et al., 2017; Hoang
et al., 2017; Arora et al., 2017). In (Arjovsky & Bottou, 2017), it was proposed that adding noise
perturbation on the inputs to the discriminator can alleviate the mode collapse problem. It is shown
that this training-with-noise technique is equivalent to adding a regularizer on the gradient norm of
the discriminator (Roth et al., 2017). The Wasserstein divergence is proposed to resolve the problem
of incontinuous divergence when the generated distribution and the data distribution have disjoint
supports (Arjovsky et al., 2017; Gulrajani et al., 2017). Mode regularization is used in the loss
function to penalize the missing modes (Che et al., 2016; Srivastava et al., 2017). The regularization
is usually based on heuristics, which tries to minimize the distance between the data samples and the
generated samples, but lacks theoretical convergence guarantee.
In this paper, we formulate the minimax optimization for GAN training (1) as finding the saddle
points of the Lagrangian function for a convex optimization problem. In the convex optimization
problem, the discriminator function D(∙) and the probabilities of generator outputs Pg(∙) Play the
roles of the primal variables and dual variables, respectively. This connection not only provides
important insights in understanding the convergence of GAN training, but also enables us to leverage
the primal-dual subgradient methods to design a novel objective function that helps to alleviate mode
collapse. A toy example reveals that for some cases when standard GAN or WGAN inevitably leads
to mode collapse, our proposed method can effectively avoid mode collapse and converge to the
optimal point.
In this paper, we do not aim at achieving superior performance over other GANs, but rather provide a
new perspective of understanding GANs, and propose an improved training technique that can be
applied on top of existing GANs. The contributions of the paper are as follows:
•	The standard training of GANs in the function space is formulated as primal-dual subgradient
methods for solving convex optimizations.
•	This formulation enables us to show that with a proper gradient descent step size, updating
the discriminator and generator probabilities according to the primal-dual algorithms will
provably converge to the optimal point.
•	This formulation results in a novel training objective for the generator. With the proposed
objective function, the generator is updated such that the probabilities of generator
outputs are pushed to the optimal update direction derived by the primal-dual algorithms.
Experiments have shown that this simple objective function can effectively alleviate mode
collapse in GAN training.
•	The convex optimization framework incorporates different variants of GANs including the
family of f -GAN (Nowozin et al., 2016) and an approximate variant of WGAN. For all these
variants, the training objective can be improved by including the optimal update direction of
the generated probabilities.
2	Primal-Dual S ub gradient Methods for Convex Optimization
In this section, we first describe the primal-dual subgradient methods for convex optimization. Later,
we explicitly construct a convex optimization and relate the subgradient methods to standard GAN
training. Consider the following convex optimization problem:
maximize f0(x)
subject to fi(x) ≥ 0,i = 1, ∙∙∙,'
x ∈ X,
(2a)
(2b)
(2c)
2
Published as a conference paper at ICLR 2018
where X ∈ Rk is a length-k vector, X is a convex set, and fi(x), i = 0 ∙∙∙ ,', are concave functions
mapping from Rk to R. The Lagrangian function is calculated as
`
L(x, λ) = f0(x) + Xλifi(x).	(3)
i=1
In the optimization problem, the variables X ∈ Rk and λ ∈ R； are referred to as primal variables
and dual variables, respectively. The primal-dual pair (x*,λ*) is a saddle-point of the Lagrangian
fuction, if it satisfies:
L(x*,λ*) = minmax L(x,λ).	(4)
λ≥0 x∈X
Primal-dual subgradient methods have been widely used to solve the convex optimization problems,
where the primal and dual variables are updated iteratively, and converge to a saddle point (Nedic &
Ozdaglar, 2009; Komodakis & Pesquet, 2015).
There are two forms of algorithms, namely dual-driven algorithm and primal-dual-driven algorithm.
For both approaches, the dual variables are updated according to the subgradient of L(X(t), λ(t))
with respect to λ(t) at each iteration t. For the dual-driven algorithm, the primal variables are updated
to achieve maximum of L(X, λ(t)) over X. For the primal-dual-driven algorithm, the primal variables
are updated according to the subgradient of L(X(t), λ(t)) with respect to X(t). The iterative update
process is summarized as follows:
X(t+1)
arg maxx∈X L (X, λ(t))
PX [X(t) + α(t)∂xL (X(t), λ(t))]
(dual-driven algorithm)
(primal-dual-driven algorithm)
λ(t + 1) = [λ(t) - α(t)∂λL (X(t), λ(t))]+,
(5)
(6)
where PX (∙) denotes the projection on set X and (x)； = max(χ, 0).
The following theorem proves that the primal-dual subgradient methods will make the primal and
dual variables converge to the optimal solution of the convex optimization problem.
Theorem 1 Consider the convex optimization (2). Assume the set of saddle points is compact.
Suppose f0(X) is a strictly concave function over X ∈ X and the subgradient at each step is bounded.
There exists some step size α(t) such that both the dual-driven algorithm and the primal-dual-driven
algorithm yield x(t) → x* and λ(t) → λ*, where x* is the solution to (2), and λ* satisfies
L(x*,λ*)=max L (x,λ*).	(7)
x∈X
Proof: See Appendix 7.1.
3	Training GAN via Primal-Dual Subgradient Methods
3.1	GAN as a convex optimization
We explicitly construct a convex optimization problem and relate it to the minimax game of GANs.
We assume that the source data and generated samples belong to a finite set {xι, ∙∙∙ , Xn} of arbitrary
size n. The extension to uncountable sets can be derived in a similar manner (Luenberger, 1997).
The finite case is of particular interest, because any real-world data has a finite size, albeit the size
could be arbitrarily large.
We construct the following convex optimization problem:
n
maximize	pd(xi) log(Di)	(8a)
i=1
subject to log(1 - Di) ≥ log(1∕2),i = 1,…，n	(8b)
D ∈ D,	(8c)
3
Published as a conference paper at ICLR 2018
where D is some convex set. The primal variables are D = (Di,…，Dn), where Di is defined as
Di = D(xi). Let Pg = (Pg(xi), ∙∙∙ ,Pg(xn)), wherePg(Xi) is the Lagrangian dual associated with
the i-th constraint. The Lagrangian function is thus
nn
L(D,pg) = XPd(xi) log(Di) + X Pg (xi) log(2(1 - Di)),D ∈ D.	(9)
i=1	i=1
When D = {D : 0 ≤ Di ≤ 1, ∀i}, finding the saddle points for the Lagrangian function is exactly
equivalent to solving the GAN minimax problem(1). This inherent connection enables us to utilize the
primal-dual subgradient methods to design update rules for D(x) andPg(x) such that they converge
to the saddle points. The following theorem provides a theoretical guideline for the training of GANs.
Theorem 2 Consider the Lagrangian function given by (9) with D = {D : ≤ Di ≤ 1 - , ∀i},
where 0 < < 1/2. If the discriminator and generator have enough capacity, and the discriminator
output and the generated distribution are updated according to the primal-dual update rules (5) and
(6) with (x,λ) = (D, Pg), thenPg(∙) converges to Pd(∙).
Proof: The optimization problem (8) is a particularized form of (2), where fo(∙) =
Pn=iPd(Xi)Iog(Di), ∕i(∙) = log(1 - Di) and X = [e, 1 - e]n. The objective function is strictly
concave over D. Moreover, since D is projected onto the compact set [, 1 - ] at each iteration t,
the subgradients ∂fi(D(t)) are bounded. The assumptions of Theorem 1 are satisfied.
Since the constraint (8b) gives an upper bound of Di ≤ 1/2, the solution to the above convex
optimization is obviously Di = 1/2, for all i = 1, ∙∙∙ ,n. Since the problem is convex, the optimal
primal solution is the primal saddle point of the Lagrangian function (Bertsekas, 1999, Chapter 5).
Moreover, any primal-dual saddle point (Di, Pgi) satisfies L(Di, Pgi) = maxD∈D L D,Pgi . Since
Di is strictly inside D, we have ∂DL(Di, Pgi) = 0. Since ∂Di L(Di, Pig) = 2Pd(Xi) - 2Pgi (Xi),
we have Pgi = Pd, and the saddle point is unique. By Theorem 1, the primal-dual update rules will
guarantee convergence of D(t), P(gt) to the primal-dual saddle point (Di,Pgi).
It can be seen that the standard training of GAN corresponds to either dual-driven algorithm (Nowozin
et al., 2016) or primal-dual-driven algorithm (Arjovsky et al., 2017; Goodfellow et al., 2014). A
natural question arises: Why does the standard training fail to converge and lead to mode collapse?
As will be shown later, the underlying reason is that standard training of GANs in some cases do
not update the generated distribution according to (6). Theorem 2 inspires us to propose a training
algorithm to tackle this issue.
3.2	Algorithm Description
First, we present our training algorithm. Later, we will use a toy example to give intuitions of why
our algorithm is effective to avoid mode collapse.
The algorithm is described in Algorithm 1. The maximum step of discriminator update is k0 . In the
context of primal-dual-driven algorithms, k0 = 1. In the context of dual-driven algorithms, k0 is
some large constant, such that the discriminator is updated till convergence at each training epoch.
The update of the discriminator is the same as standard GAN training. The main difference is the
modified loss function for the generator update (13). The intuition is that when the generated samples
have disjoint support from the data, the generated distribution at the data support may not be updated
using standard training. This is exactly one source of mode collapse. Ideally, the modified loss
function will always update the generated probabilities at the data support along the optimal direction.
The generated probability mass at X is Pg(x) = m Pmm=I 1{G(zi) = x}, where 1{∙} is the indicator
function. The indicator function is not differentiable, so we use a continuous kernel to approximate it.
Define
，/ 、	-∣∣x∣∣2	,…
kσ (x) = e σ2 ,	(14)
where σ is some positive constant. The constant σ is also called bandwidth for kernel density
estimation. The empirical generated distribution is thus approximately calculated as (17). There
4
Published as a conference paper at ICLR 2018
Algorithm 1 Training GAN via Primal-Dual Subgradient Methods
Initialization: Choose the objective function fo(∙) and constraint function fι(∙) according to the
GAN realization. For the original GAN based on Jensen-Shannon divergence, f0(D) = log (D)
and f1(D) = log(2(1 - D)).
while the stopping criterion is not met do
Sample minibatch mi data samples xι, ∙∙∙ , Xmi.
Sample minibatch m2 noise samples zi, ∙∙∙ , Zm?.
for k = 1,…，ko do
Update the discriminator parameters with gradient ascent:
Oθd ɪ X fo(D(xi)) + ɪ X fl (D (G (Zj)))
m1	m2
i=1	j=1
end for
Update the target generated distribution as:
Pg(Xi) = Pg(Xi) - αfi(D(xi)),i = 1, ∙∙∙ ,mi,
where α is some step size and
1	m2
Pg (Xi) = ~~ k k kσ (G(Zj ) - Xi).
With Pg (Xi) fixed, update the generator parameters with gradient descent:
O
1 m2	1 m1	1 m2
m2 X fl	(D (G (Zj ))) + 有	X	Pg (Xi)	— m2	X kσ (G(Zj )	-Xi)
2	j=1	1	i=1	2	j=1
(10)
(11)
(12)
(13)
end while
are different bandwidth selection methods (Botev et al., 2010; Hall et al., 1991). It can be seen that
as σ → 0, kσ(X - y) tends to the indicator function, but it will not give large enough gradients to
far areas that experience mode collapse. A larger σ implies a coarser quantization of the space in
approximating the distribution. In practical training, the kernel bandwidth can be set larger at first
and gradually decreases as the iteration continues.
By the dual update rule (6) , the generated probability of every Xi should be updated as
…idL(D, Pg )
pg(Xi)= pg(Xi) - α ∂ρg(Xi)
= Pg(Xi) - α log(2(1 - D(Xi))).
(15)
(16)
This motivates us to add the second term of (13) in the loss function, such that the generated
distribution is pushed towards the target distribution (15).
Although having good convergence guarantee in theory, the non-parametric kernel density estimation
of the generated distribution may suffer from the curse of dimension. Previous works combining
kernel learning and the GAN framework have proposed methods to scale the algorithms to deal with
high-dimensional data, and the performances are promising (Li et al., 2015; 2017a; Sinn & Rawat,
2017). One common method is to project the data onto a low dimensional space using an autoencoder
or a bottleneck layer of a pretrained neurual network, and then apply the kernel-based estimates on
the feature space. Using this approach, the estimated probability of Xi becomes
m2
Pg (Xi ) = m X kσ (fφ(G(Zj )) - fφ(Xi)),
2 j=i
(17)
where fφ(.) is the projection of the data to a low dimensional space. We will leave the work of
generating high-resolution images using this approach as future work.
5
Published as a conference paper at ICLR 2018
3.3 Intuition of avoiding mode collapse
Mode collapse occurs when the generated samples have a very small probability to overlap with some
families of the data samples, and the discriminator D(∙) is locally constant around the region of the
generated samples. We use a toy example to show that the standard training of GAN and Wasserstein
may fail to avoid mode collapse, while our proposed method can succeed.
Claim 1 Suppose the data distribution is pd(x) = 1{x = 1}, and the initial generated distribution
is pg (x) = 1{x = 0}. The discriminator output D(x) is some function that is equal to zero for
|x - 0| ≤ δ and is equal to one for |x - 1| ≤ δ, where 0 < δ < 1/2. Standard training of GAN and
WGAN leads to mode collapse.
Proof: We first show that the discriminator is not updated, and then show that the generator is not
updated during the standard training process.
In standard training of GAN and WGAN, the discriminator is updated according to the gradient of
(10). For GAN, since 0 ≤ D(x) ≤ 1, the objective funtion for the discriminator is at most zero, i.e.,
Epd log (D (x)) + Epg log (1 - D (x)) = log(D(1)) + log(1 - D(0)) ≤ 0,	(18)
which is achieved by the current D(x) by assumption.
For WGAN, the optimal discrminator output D(x) is some 1-Lipschitz function such that
Epd {D(x)} - Epg {D(x)} is maximized. Since
Epd {D(x)} - Epg{D(x)} = D(1) - D(0) ≤ 1,	(19)
where (19) is due to the Lipschitz condition |D(1) - D(0)| ≤ 1. The current D(x) is obviously
optimal. Thus, for both GAN and WGAN, the gradient of the loss function with respect to θd is zero
and the discriminator parameters are not updated.
On the other hand, in standard training, the generator parameters θg are updated with only the first
term of (13). By the chain rule,
dθg log (1 - D(G (Zi))) = - 1- D 1G (Z )) dxD(切 X=G(Zi)dθg G(Zi)	(20)
= 0,	(21)
where (21) is due to the assumption that D(x) is locally constant for x = 0. Therefore, the generator
and the discriminator reach a local optimum point. The generated samples are all zeros.
In our proposed training method, when x = 1, the optimal update direction is given by (11), where
Pg is a large value because D(1) = 1. Therefore, by(13), the second term in the loss function is very
large, which forces the generator to generate samples at G(Z) = 1. As the iteration continues, the
generated distribution gradually converges to data distribution, and D(x) gradually converges to 1/2,
which makes ∂pg(x)L(D(x), pg(x)) = log(2(1 - D(x))) become zero. The experiment in Section 5
demonstrates this training dynamic.
In this paper, the standard training of GANs in function space has been formulated as primal-dual
updates for convex optimization. However, the training is optimized over the network parameters in
practice, which typically yields a non-convex non-concave problem. Theorem 2 tells us that as long
as the discriminator output and the generated distribution are updated according to the primal-dual
update rule, mode collapse should not occur. This insight leads to the addition of the second term in
the modified loss function for the generator (13). In Section 5, experiments on the above-mentioned
toy example and real-world datasets show that the proposed training technique can greatly improve
the baseline performance.
4	Variants of GANs
Consider the following optimization problem:
n
maximize	pd(xi)f0(Di)	(22a)
i=1
subject to fι(Di) ≥ 0, i = 1,…，n,	(22b)
6
Published as a conference paper at ICLR 2018
Table 1: Variants of GANs under the convex optimization framework.
Divergence metric	fo(Di)	fι(Di)	D二
Kullback-Leibler	Iog(Di)	1 — Di	Pd(xi) 	Pg(xi)	
Reverse KL	-Di	log Di	Pg(Xi) Pd(xi)
Pearson χ2	Di	-4D2 - Di	2(Pd(Xi)-Pg(Xi))- 	Pg(Xi) _
Squared Hellinger χ2	1 — Di	1 - 1/Di	/Pg (Xi) V Pd(Xi)
Jensen-Shannon	Iog(Di)	log(1 - Di)- log(1∕2)	Pd(Xi) Pd(Xi)+Pg (Xi)
Approximate WGAN	Di- eD2	-Di	Pd(Xi )-?g (Xi) ?eP、d(xi)
Other metric	-2 D2 + Di	Di- 2	Pd(Xi)+Pg (Xi) 	Pd(Xi)	
where fo(∙) and fι(∙) are concave functions. Compared with the generic convex optimization
problem (2), the number of constraint functions is set to be the variable alphabet size, and the
constraint functions are fi(D) = fι(Di), i = 1,… ,n.
The objective and constraint functions in (22) can be tailored to produce different GAN variants. For
example, Table 1 shows the large family of f -GAN (Nowozin et al., 2016). The last row of Table 1
gives a new realization of GAN with a unique saddle point of D* (x) = 2 andPg (x) = pd(x).
We also derive a GAN variant similar to WGAN, which is named “Approximate WGAN". As shown
in Table 1, the objective and constraint functions yield the following minimax problem:
min max Ex〜pd(x) {D(x) - eD2(x)} - Ex〜Pg(x) {D(x)} ,	(23)
where is an arbitrary positive constant. The augmented term D2 (x) is to make the objective
function strictly concave, without changing the original solution. It can be seen that this problem
has a unique saddle point pg(x) = Pd(x). As E tends to 0, the training objective function becomes
identical to WGAN. The optimal D(x) for WGAN is some Lipschitz function that maximizes
Ex〜pd(x) {D(x)} - Ex〜Pg(x) {D(χ)}, while for our problem is D*(x) = 0. Weight clipping can
still be applied, but serves as a regularizer to make the training more robust (Merolla et al., 2016).
The training algorithms for these variants of GANs follow by simply changing the objective function
fo(∙) and constraint function fι(∙) accordingly in Algorithm 1.
5	Experiments
5.1	Synthetic data
Fig. 1 shows the training performance for a toy example. The data distribution is pg(x) = 1{x = 1}.
The inital generated samples are concentrated around x = -3.0. The details of the neural network
parameters can be seen in Appendix 7.3. Fig. 1a shows the generated samples in the 90 quantile as
the training iterates. After 8000 iterations, the generated samples from standard training of GAN
and WGAN are still concentrated around x = -3.0. As shown in Fig. 1c and 1d, the discrminators
hardly have any updates throughout the training process. Using the proposed training approach, the
generated samples gradually converge to the data distribution and the discriminator output converges
to the optimal solution with D(1) = 1/2.
Fig. 2 shows the performance of the proposed method for a mixture of 8 Gaussain data on a circle.
While the original GANs experience mode collapse (Nguyen et al., 2017; Metz et al., 2016), our
proposed method is able to generate samples over all 8 modes. In the training process, the bandwidth
of the Gaussian kernel (14) is inialized to be σ2 = 0.1 and decreases at a rate of 0.820≡, where t is
the iteration number. The generated samples are dispersed initially, and then gradually converge to
the Gaussian data samples. Note that our proposed method involves a low complexity with a simple
regularization term added in the loss function for the generator update.
7
Published as a conference paper at ICLR 2018
(a)
(b)
x
(c)
x
(d)
Figure 1: Performance of a toy example. Figure (a) shows the generated samples for different GANs.
Figure (b) shows the discriminator output D(x) for GANs trained using the proposed method. Figure
(c) and (d) show the discriminator output D(x) for standard GANs and WGANs.
Iteration 0	Iteration 10k Iteration 20k Iteration 30k Iteration 40k
Figure 2: Performance of the proposed algorithm on 2D mixture of Gaussian data. The data samples
are marked in blue and the generated samples are marked in orange.
5.2	Real-world datasets
We also evaluate the performance of the proposed method on two real-world datasets: MNIST and
CIFAR-10. Please refer to the appendix for detailed architectures. Inception score (Salimans et al.,
2016) is employed to evaluate the proposed method. It applies a pretrained inception model to every
generated image to get the conditional label distribution p(y|x). The Inception score is calculated as
exp (Ex {KL(p(y|x) k p(y)}). It measures the quality and diversity of the generated images.
5.2.1	MNIST
The MNIST dataset contains 60000 labeled images of 28 × 28 grayscale digits. We train a simple
LeNet-5 convolutional neural network classifier on MNIST dataset that achieves 98.9% test accuracy,
and use it to compute the inception score. The proposed method achieves an inception score of 9.8,
while the baseline method achieves an inception score of 8.8. The examples of generated images are
shown in Fig. 3. The generated images are almost indistinguishable from real images.
We further evaluated our algorithm on an augmented 1000-class MNIST dataset to further demonstrate
the robustness of the proposed algorithm against mode collapse problem. More details of the
experimental results can be found in the Appendix.
5.2.2	CIFAR- 1 0
CIFAR is a natural scene dataset of 32 × 32. We use this dataset to evaluate the visual quality of
the generated samples. Table 2 shows the inception scores of different GAN models on CIFAR-10
dataset. The inception score of the proposed model is much better than the baseline method WGAN
8
Published as a conference paper at ICLR 2018
+ 7¾G 3gsQ3号
27oz 7∖%rκ
Λzsw∕∕5 I&
1斗 q#2su
Oos∙ηp/
4/f-o¾×∆5
W5g∕qz2
Xa 7 9彳孑39H
30llo7g∕⅛4 夕
6 / 34:To a qe a
CIFAR
MNIST
Figure 3: Examples of generated images using MNIST and CIFAR dataset.
Method	Score
Real data	11.24 ± 0.16
WGAN (Arjovsky et al.,2017)	3.82 ± 0.06
MIX + WGAN (Arora et al., 2017)	4.04 ± 0.07
Improved-GAN (Salimans et al., 2016)	4.36 ± 0.04
ALI (Dumoulin et al., 2016)	5.34 ± 0.05
DCGAN (Radford et al., 2015)	6.40 ± 0.05
Proposed method	4.53 ± 0.04
Table 2: Inception scores on CIFAR-10 dataset.
that uses similar network architecture and training method. Note that although DCGGAN achieves
a better score, it uses a more complex network architecture. Examples of the generated images are
shown in Fig. 3.
6 Conclusion
In this paper, we propose a primal-dual formulation for generative adversarial learning. This
formulation interprets GANs from the perspective of convex optimization, and gives the optimal
update of the discriminator and the generated distribution with convergence guarantee. By framing
different variants of GANs under the convex optimization framework, the corresponding training
algorithms can all be improved by pushing the generated distribution along the optimal direction.
Experiments on two synthetic datasets demonstrate that the proposed formulation can effectively
avoid mode collapse. It also achieves competitive quantitative evaluation scores on two benchmark
real-world image datasets.
References
Martin Arjovsky and Leon Bottou. Towards principled methods for training generative adversarial
networks. arXiv preprint arXiv:1701.04862, 2017.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In International Conference on Machine Learning, pp. 214-223, 2017.
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium
in generative adversarial nets (GANs). arXiv preprint arXiv:1703.00573, 2017.
Dimitri P Bertsekas. Nonlinear programming. Athena scientific Belmont, 1999.
Dimitri P Bertsekas and John N Tsitsiklis. Parallel and distributed computation: numerical methods.
Prentice-Hall, Inc., 1989.
9
Published as a conference paper at ICLR 2018
Zdravko I Botev, Joseph F Grotowski, Dirk P Kroese, et al. Kernel density estimation via diffusion.
TheAnnals ofStatistics, 38(5):2916-2957, 2010.
Tong Che, Yanran Li, Athul Paul Jacob, Yoshua Bengio, and Wenjie Li. Mode regularized generative
adversarial networks. arXiv preprint arXiv:1612.02136, 2016.
Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Alex Lamb, Martin Arjovsky, Olivier Mastropietro,
and Aaron Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704, 2016.
Diego Feijer and Fernando Paganini. Stability of primal-dual gradient dynamics and applications to
network optimization. Automatica, 46(12):1974-1981, 2010.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and
Statistics, pp. 249-256, 2010.
Ian Goodfellow. Nips 2016 tutorial: Generative adversarial networks. arXiv preprint
arXiv:1701.00160, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural
information processing systems, pp. 2672-2680, 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved
training of Wasserstein GANs. arXiv preprint arXiv:1704.00028, 2017.
Peter Hall, Simon J Sheather, MC Jones, and James Stephen Marron. On optimal data-based
bandwidth selection in kernel density estimation. Biometrika, 78(2):263-269, 1991.
Martin HeUseL HUbert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Gunter Klambauer, and
Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a nash equilibrium.
arXiv preprint arXiv:1706.08500, 2017.
Quan Hoang, Tu Dinh Nguyen, Trung Le, and Dinh Phung. Multi-generator gernerative adversarial
nets. arXiv preprint arXiv:1708.02556, 2017.
Nikos Komodakis and Jean-Christophe Pesquet. Playing with duality: An overview of recent primal
dual approaches for solving large-scale optimization problems. IEEE Signal Processing Magazine,
32(6):31-54, 2015.
ChUn-Liang Li, Wei-Cheng Chang, YU Cheng, Yiming Yang, and Barnabds P6czos. Mmd gan:
Towards deeper understanding of moment matching network. arXiv preprint arXiv:1705.08584,
2017a.
Jerry Li, Aleksander Madry, John Peebles, and LUdwig Schmidt. Towards Understanding the dynamics
of generative adversarial networks. arXiv preprint arXiv:1706.09884, 2017b.
YUjia Li, Kevin Swersky, and Rich Zemel. Generative moment matching networks. In Proceedings
of the 32nd International Conference on Machine Learning (ICML-15), pp. 1718-1727, 2015.
David G LUenberger. Optimization by vector space methods. John Wiley & Sons, 1997.
PaUl Merolla, RathinakUmar AppUswamy, John ArthUr, Steve K Esser, and Dharmendra Modha.
Deep neUral networks are robUst to weight binarization and other non-linear distortions. arXiv
preprint arXiv:1606.01981, 2016.
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The nUmerics of GANs. arXiv preprint
arXiv:1705.10461, 2017.
LUke Metz, Ben Poole, David PfaU, and Jascha Sohl-Dickstein. Unrolled generative adversarial
networks. arXiv preprint arXiv:1611.02163, 2016.
Vaishnavh Nagarajan and J Zico Kolter. Gradient descent gan optimization is locally stable. arXiv
preprint arXiv:1706.04156, 2017.
10
Published as a conference paper at ICLR 2018
Angelia Nedic and AsUman Ozdaglar. SUbgradient methods for saddle-point problems. Journal of
optimization theory and applications,142(1):205-228, 2009.
TU Dinh NgUyen, TrUng Le, HUng VU, and Dinh PhUng. DUal discriminator generative adversarial
nets. arXiv preprint arXiv:1709.03831, 2017.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-GAN: Training generative neUral samplers
Using variational divergence minimization. In Advances in Neural Information Processing Systems,
pp. 271-279, 2016.
Alec Radford, LUke Metz, and SoUmith Chintala. UnsUpervised representation learning with deep
convolUtional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Kevin Roth, AUrelien LUcchi, Sebastian Nowozin, and Thomas Hofmann. Stabilizing training of
generative adversarial networks throUgh regUlarization. arXiv preprint arXiv:1705.09367, 2017.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki CheUng, Alec Radford, and Xi Chen.
Improved techniqUes for training GANs. In Advances in Neural Information Processing Systems,
pp. 2234-2242, 2016.
MathieU Sinn and Ambrish Rawat. Towards consistency of adversarial training for generative models.
arXiv preprint arXiv:1705.09199, 2017.
Akash Srivastava, Lazar Valkov, Chris RUssell, Michael GUtmann, and Charles SUtton. VEEGAN:
RedUcing mode collapse in GANs Using implicit variational learning. arXiv preprint
arXiv:1705.07761, 2017.
IlyaTolstikhin, Sylvain Gelly, Olivier Bousquet, Carl-Johann Simon-Gabriel, and Bernhard Scholkopf.
Adagan: Boosting generative models. arXiv preprint arXiv:1701.02386, 2017.
7 Appendix
7.1 Proof of Theorem 1
The proof of convergence for dual-driven algorithms can be found in (Bertsekas & Tsitsiklis, 1989,
Chapter 3).
The primal-dual-driven algorithm for continuous time update has been studied in (Feijer & Paganini,
2010). Here, we show the convergence for the discrete-time case.
We choose a step size α(t) that satisfies
∞∞
α(t) > 0, Xα(t) = ∞, X α2 (t) < ∞.	(24)
t=1	t=1
Let z(t) = [x(t), λ(t)]T be a vector consisting of the primal and dual variables at the t-th iteration.
The primal-dual-driven update can be expressed as:
z(t + 1) = z(t) + α(t)T (t),
(25)
where
T(t)= [ dxL(x(t),λ(t)) ] = Γ ∂fo(x(t)) + P'=1 λi∂fi (x(t)) ]	(26)
T(t) =	-∂λL(x(t), λ(t))	=	-F (xi=(1t))	,	(26)
and
f1 (X)
F (X)=	.	.	(27)
f'(x)
Since the subgradient is bounded by assumption, there exists M > 0 such that ||T(∙)∣∣2 < M, where
||.||2 stands for the L2 norm.
11
Published as a conference paper at ICLR 2018
Let x* be the unique saddle point and Φ be the set of saddle points of λ. For any λ* ∈ Φ, it satisfies
L(x,λ*) ≤ L(x*,λ*) ≤ L(x*,λ).	(28)
for all x and λ.
For any saddle point (x*, λ*), we have
||x(t + 1) - x*||22 = ||PX [x(t) + α(t)∂xL(x(t), λ(t))] - x*||22	(29)
≤ ||x(t) + α(t)∂xL(x(t), λ(t)) - x*||22	(30)
= ||x(t) - x*||22 + 2α(t)∂xL(x(t), λ(t))(x(t) - x*)
+ α2(t)∣∣∂χL(x(t),λ(t))∣∣2	(31)
≤ ||x(t) - x* ||22 + 2α(t)∂xL(x(t), λ(t))(x(t) - x*) + α2(t)M	(32)
where (30) is due to the nonexpansive projection lemma for any X that contains x* (Bertsekas &
Tsitsiklis, 1989, Chapter 3), and (32) is due to the assumption that the subgradients are upper bounded
by M.
Similarly, we have
∣∣λ(t +1)-λ* ||2 ≤ ∣∣λ(t) - λ*∣∣2 - 2α(t)F (X)T (λ(t) - λ*) + α2(t)M.	(33)
Let λ*(t) = argmi□λ*∈φ ∣∣λ(t) - λ*∣∣2. Define z*(t) = [x*,λ*(t)]. Since (32) and (33) hold for
any λ* ∈ Φ, we have
||z(t + 1) - z*(t + 1)I∣2 = ||x(t + 1) - X*I∣2 + l∣λ(t + 1) - λ*(t + 1)∣∣2	(34)
≤l∣x(t +1) - X*I∣2 + l∣λ(t + 1) - λ*(t)∣∣2	(35)
≤ ||z(t) - z*(t)||22 + 2α(t)TT(t)(z(t) - z*(t)) + 2α2 (t)M.	(36)
Next we will show that (x(t), λ(t)) converges to a saddle point. The intuition is that for large t, the
second term (36) is less than zero and dominates over the third term, thus z(t) will be driven to the
set of saddle points.
Since L(x, λ) is concave in x and convex in λ, we have
∂xL(x(t),λ(t))(x(t)-x*) ≤ L(x(t), λ(t)) - L(x*, λ(t))	(37)
∂λL(x(t),λ(t))(λ(t)-λ*) ≥ L(x(t),λ(t)) - L(x(t),λ*).	(38)
Therefore,
TT(t)(z(t) - z*(t)) ≤ L(x(t), λ(t)) - L(x*, λ(t)) + L(x(t), λ*(t)) - L(x(t), λ(t))	(39)
= L(x*,λ*(t)) - L(x*, λ(t)) +L(x(t),λ*(t)) - L(x*,λ*(t))	(40)
≤ 0,	(41)
where the last step is due to the definition of saddle point (28). Combining (36) and (41), we have
||z(t +1) - z*(t + 1)I∣2 ≤ ||z(t) - z*(t)∣∣2 + 2α2(t)M.	(42)
Summing (42) over 1 ≤ t ≤ n - 1, we have
n-1
l∣z(n) - z*(n)ll2 ≤ ∣∣z(1) - z*(1)∣∣2 + X 2α2(t)M.	(43)
t=1
Since the saddle points are bounded by assumption, the initial point ||z(1)||2 is bounded and
Pt∞=1 α2(t) is bounded, ||z(n)||2 must be bounded.
Give any > 0, define a neighbor of the saddle points as
Ae = {z :||Z - (x*,λ*)∣∣2 <e, ∃λ* ∈ Φ}.	(44)
We first show that there must be infinitely many points of z(t) that are in A. Suppose this does not
hold, then there exists some N0 > 0, such that for every t ≥ N0, zt ∈/ Ae . By the continuity of
12
Published as a conference paper at ICLR 2018
function L(∙, ∙), (40) implies that there exists some δ > 0 such that for every t ≥ N0, T T (t)(z(t) -
z*(t)) < -δ. In this case, by summing (36) over No ≤ t ≤ n - 1, we have
n-1	n-1
∣∣z(n)-z*(n)∣∣2 ≤ ∣∣z(N0) - z*(N°)∣∣2 - 2 X α(t)δ + X 2α2(t)M.	(45)
t=N0	t=N0
Note that ||z(No) - z*(No)∣∣2 is bounded. By the choice of the step size (24), we have ||z(n)-
z*(n)||2 tends to -∞, which is contradicted with the fact that ||z(n) - z*(n)||2 ≥ 0. Therefore,
there are infinitely many z(t) in A.
Consequently, we can find a large enough N1 such that 2M Pt∞=N α2 (t) ≤ and ||z(N1) -
z*(Ni)II ≤ U Summing (42) over Ni ≤ t ≤ n - 1 we have
n-1
IIz(n)-z*(n)II22 ≤ IIz(Ni)-z*(Ni)II22+2M X α2(t)	(46)
t=N1
≤ 2U.	(47)
In other words, for any U> 0, there exists Ni > 0 such that for all t ≥ Ni, IIz(t) - z*(t)II2 ≤ 2U.
Since it holds for all U, it implies that there are infinitely many z(t) that belong to the saddle points.
The set of saddle points is compact by assumption. By the Bolzano-Weierstrass theorem, there must
exist a subsequence of {z(tn)} that converges to a saddle point z0. For such subsequence, there
exists some large enough n0 such that tn0 ≥ Ni and IIz(tn) - z0II22 ≤ U, for every n ≥ n0. Since
(42) holds for any saddle point z*(t), we replace z*(t) by z0 and sum (42) over tn0 ≤ t ≤ n - 1 to
obtain
n-i
IIz(n) - z0II22 ≤ IIz(tn0) - z0II22 + 2M X α2(t)	(48)
t=tn0
n-i
≤ IIz(tn0)-z0II22+2M X α2(t)	(49)
t=N1
≤ 2U.	(50)
This means that for any U, there exists N2 = tn0 such that for every t ≥ N2, IIz(t) - z0II22 ≤ U. That
concludes the proof that z(t) converges to a saddle point.
7.2	1000 Class MNIST dataset
We use an augmented version of MNIST dataset similar to the experiment conducted in (Che et al.,
2016; Metz et al., 2016). Each image in this dataset is created by randomly choosing three letter
images from MNIST dataset. The three images are stacked as the R,G, and B channels into a color
image. This dataset has 1000 distinct modes, corresponding to each combination of the ten MNIST
classes in each channel.
We train a GAN and a classifier on this dataset. For each generated image, we apply the classifier to
determine its label. We compute two metrics on this dataset. the number of modes the GAN generates,
and the inception score computed using the classifier. We use the same architecture as (Metz et al.,
2016) in our experiment. The result is shown in Table 7.2.
We find that the proposed method achieves a much better performance than unrolled GAN with 5
steps and comparable performance to unrolled GAN with 10 steps in terms of the number of predicted
modes. However, since our method does not involve unrolling step, it is much more computationally
efficient. Notice that although (Che et al., 2016) generates much more modes, it uses a more complex
architecture, and such architecture is known to contribute to mode collapse avoidance on the 1000
Class MNIST dataset (Metz et al., 2016). Compared to the baseline that does not use the second
regularization term in (13), the proposed method achieves better inception score, and it generates
more modes.
13
Published as a conference paper at ICLR 2018
Table 3: Performance comparison on augmented MNIST dataset.
	Method		Modes generated	Inception Score
(Metz et al., 2016) 5 steps	732	NA
(Metz et al., 2016) 10 steps	817	NA
(Che etal., 2016)	969	NA
Baseline	526	87.15
Proposed	827	155.6
7.3	Toy Example Training Details
In the toy example, both the generator and the discriminator has only one ReLU hidden layer with
64 neurons. The output activation is sigmoid function for GAN and ReLU for WGAN. For WGAN,
the parameters are clipped in between [-1, 1], and the networks are trained with Root Mean Square
Propagation (RMSProp) with a learning rate of 1e-4. For GAN, the networks are trained with Adam
with a learning rate of 1e-4. The minibatch size is 32. The bandwidth parameter for the Gaussian
kernel is initialized to be σ = 0.5 and then is changed to 0.1 after 2000 iterations.
7.4	2D Mixture Gaussian Data Training Details
We use the network structure in (Metz et al., 2016) to evaluate the performance of our proposed
method. The data is sampled from a mixture of 8 Gaussians of standard deviation of 0.02 uniformly
located on a circle of radius 2. The noise samples are a vector of 256 independent and identically
distributed (i.i.d.) Gaussian variables with mean zero and standard deviation of 1.
The generator has two hidden layers of size 128 with ReLU activation. The last layer is a linear
projection to two dimensions. The discriminator has one hidden layer of size 128 with ReLU
activation followed by a fully connected network to a sigmoid activation. All the biases are initialized
to be zeros and the weights are initalilzed via the “Xavier” initialization (Glorot & Bengio, 2010).
The training follows the primal-dual-driven algorithm, where both the generator and the discriminator
are updated once at each iteration. The Adam optimizer is used to train the discriminator with 8e-4
learning rate and the generator with 4e-4 learning rate. The minibatch sample number is 64.
7.5	MNIST Training Details
For MNIST dataset, the generator network is a deconvolutional neural network. It has two fully
connected layer with hidden size 1024 and 7 × ×7 × 128, two deconvolutional layers with number
of units 64, 32, stride 2 and deconvolutional kernel size 4 × 4 for each layer, respectively, and a final
convolutional layer with number of hidden unit 1 and convolutional kernel 4 × 4.. The discriminator
network is a two layer convolutional neural network with number of units 64, 32 followed by two
fully connected layer of hidden size 1024 and 1. The input noise dimension is 64.
We employ ADAM optimization algorithm with initial learning rate 0.01 and β = 0.5.
7.6	CIFAR Traing Details
For CIFAR dataset, the generator is a 4 layer deconvolutional neural network, and the discriminator is
a 4 layer convolutional neural network. The number of units for discriminator is [64, 128, 256, 1024],
and the number of units for generator is [1024, 256, 128, 64]. The stride for each deconvolutional and
convolutional layer is two.
We employ RMSProp optimization algorithm with initial learning rate of 0.0001, decay rate 0.95,
and momentum 0.1.
14