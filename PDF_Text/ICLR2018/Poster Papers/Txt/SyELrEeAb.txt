Under review as a conference paper at ICLR 2018
Implicit Causal Models for
Genome-wide Association Studies
Anonymous authors
Paper under double-blind review
Ab stract
Progress in probabilistic generative models has accelerated, developing richer
models with neural architectures, implicit densities, and with scalable algorithms
for their Bayesian inference. However, there has been limited progress in mod-
els that capture causal relationships, for example, how individual genetic factors
cause major human diseases. In this work, we focus on two challenges in partic-
ular: How do we build richer causal models, which can capture highly nonlinear
relationships and interactions between multiple causes? How do we adjust for la-
tent confounders, which are variables influencing both cause and effect and which
prevent learning of causal relationships? To address these challenges, we syn-
thesize ideas from causality and modern probabilistic modeling. For the first, we
describe implicit causal models, a class of causal models that leverages neural ar-
chitectures with an implicit density. For the second, we describe an implicit causal
model that adjusts for confounders by sharing strength across examples. In exper-
iments, we scale Bayesian inference on up to a billion genetic measurements. We
achieve state of the art accuracy for identifying causal factors: we significantly
outperform existing genetics methods by an absolute difference of 15-45.3%.
1	Introduction
Probabilistic models provide a language for specifying rich and flexible generative processes (Pearl,
1988; Murphy, 2012). Recent advances expand this language with neural architectures, implicit den-
sities, and with scalable algorithms for their Bayesian inference (Rezende et al., 2014; Tran et al.,
2017). However, there has been limited progress in models that capture high-dimensional causal
relationships (Pearl, 2000; Spirtes et al., 1993; Imbens & Rubin, 2015). Unlike models which learn
statistical relationships, causal models let us manipulate the generative process and make counter-
factual statements, that is, what would have happened if the distributions changed.
As the running example in this work, consider genome-wide association studies (gwas) (Yu et al.,
2005; Price et al., 2006; Kang et al., 2010). The goal of gwas is to understand how genetic factors,
i.e., single nucleotide polymorphisms (snps), cause traits to appear in individuals. Understanding
this causation both lets us predict whether an individual has a genetic predisposition to a disease and
also understand how to cure the disease by targeting the individual snps that cause it.
With this example in mind, we focus on two challenges to combining modern probabilistic models
and causality. The first is to develop richer, more expressive causal models. Probabilistic causal
models represent variables as deterministic functions of noise and other variables, and existing work
usually focuses on additive noise models (Hoyer et al., 2009) such as linear mixed models (Kang
et al., 2010). These models apply simple nonlinearities such as polynomials, hand-engineered low
order interactions between inputs, and assume additive interaction with, e.g., Gaussian noise. In
gwas, strong evidence suggests that susceptibility to common diseases is influenced by epistasis
(the interaction between multiple genes) (Culverhouse et al., 2002; McKinney et al., 2006). We
would like to capture and discover such interactions. This requires models with nonlinear, learnable
interactions among the inputs and the noise.
The second challenge is how to address latent population-based confounders. In gwas, both la-
tent population structure, i.e., subgroups in the population with ancestry differences, and relatedness
among sample individuals produce spurious correlations among snps to the trait of interest. Exist-
ing methods correct for this correlation in two stages (Yu et al., 2005; Price et al., 2006; Kang et al.,
1
Under review as a conference paper at ICLR 2018
2010): first, estimate the confounder given data; then, run standard causal inferences given the esti-
mated confounder. These methods are effective in some settings, but they are difficult to understand
as principled causal models, and they cannot easily accommodate complex latent structure.
To address these challenges, we synthesize ideas from causality and modern probabilistic modeling.
For the first challenge, we develop implicit causal models, a class of causal models that leverages
neural architectures with an implicit density. With gwas, implicit causal models generalize previous
methods to capture important nonlinearities, such as gene-gene and gene-population interaction.
Building on this, for the second challenge, we describe an implicit causal model that adjusts for
population-confounders by sharing strength across examples (genes). We derive conditions that
prove the model consistently estimates the causal relationship. This theoretically justifies existing
methods and generalizes them to more complex latent variable models of the confounder.
In experiments, we scale Bayesian inference on implicit causal models on up to a billion genetic
measurements. Validating these results are not possible for observational data (Pearl, 2000), so we
first perform an extensive simulation study of 11 configurations of 100,000 snps and 940 to 5,000
individuals. We achieve state of the art accuracy for identifying causal factors: we significantly
outperform existing genetics methods by an absolute difference of 15-45.3%. In a real-world gwas,
we also show our model discovers real causal relationships—identifying similar snps as previous
state of the art—while being more principled as a causal model.
1.1	Related work
There has been growing work on richer causal models. Louizos et al. (2017) develop variational
auto-encoders for causality and address local confounders via proxy variables. Our work is com-
plementary: we develop implicit models for causality and address global confounders by sharing
strength across examples. In other work, Mooij et al. (2010) propose a Gaussian process over
causal mechanisms, and Zhang & HyVarinen (2009) study post-nonlinear models, which apply a
nonlinearity after adding noise. These models typically focus on the task of causal discovery, and
they assume fixed nonlinearities (post-nonlinear models) or impose strong smoothness assumptions
(Gaussian processes) which we relax using neural networks. In the potential outcomes literature,
much recent work has considered decision trees and neural networks (e.g., Hill (2011); Wager &
Athey (2015); Johansson et al. (2016)). These methods tackle a related but different problem of
balancing coVariates across treatments.
Causality with population-confounders has primarily been studied for genome-wide association
studies (gwas). A popular approach is to first, estimate the confounder using the top principal
components of the genotype matrix of indiViduals-by-snps; then, linearly regress the trait of interest
onto the genetic factors and these components (Price et al., 2006; Astle & Balding, 2009). Another
approach is to first, estimate the confounder Via a “kinship matrix” on the genotypes; then, fit a
linear mixed model of the trait giVen genetic factors, and where the coVariance of random effects is
the kinship matrix (Yu et al., 2005; Kang et al., 2010). Other work adjusts for the confounder Via
admixture models and factor analysis (Song et al., 2015; Hao et al., 2016). This paper builds on
all these methods, proViding a theoretical understanding about when causal inferences can succeed
while adjusting for latent confounders. We also deVelop a new causal model with nonlinear, learn-
able gene-gene and gene-population interactions; and we describe a Bayesian inference algorithm
that justifies the two-stage estimation.
The problem of epistasis, that is, nonadditiVe interactions between multiple genes, dates back to clas-
sical work on epigenetics by Bateson and R.A. Fisher (Fisher, 1918). Primary methods for captur-
ing epistasis include adding interactions within a linear model, permutation tests, exhaustiVe search,
and multifactor dimensionality reduction (Cordell, 2009). These methods require hand-engineering
oVer all possible interactions, which grows exponentially in the number of genetic factors. Neural
networks haVe been applied to address epistasis for epigenomic data, such as to predict sequence
specificities of protein bindings giVen DNA sequences (Alipanahi et al., 2015). These methods use
discriminatiVe neural networks (unlike neural networks within a generatiVe model), and they focus
on prediction rather than causality.
2	Implicit Causal Models
We describe the framework of probabilistic causal models. We then describe implicit causal models,
an extension of implicit models for encoding complex, nonlinear causal relationships.
2
Under review as a conference paper at ICLR 2018
Figure 1: Probabilistic causal model. (left) Variable x causes y coupled with a shared variable β.
(right) A more explicit diagram where variables (denoted with a square) are a deterministic function
of other variables and noise (denoted with a triangle).
2.1	Probabilistic Causal Models
Probabilistic causal models (Pearl, 2000), or structural equation models, represent variables as de-
terministic functions of noise and other variables. As illustration, consider the causal diagram in
Figure 1. It represents a causal model where there is a global variable
β = fβ (β),
and for each data point n = 1, . . . , N,
xn
fx (x,n, β),
yn
fy (y
n,
xn, β),
eβ 〜s(∙),
eχ,n 〜s(∙)
ey,n 〜s(∙).
(1)
The noise e are background variables, representing unknown external quantities which are jointly in-
dependent. Each variable β, x, y is a function of other variables and its background variable.
We are interested in estimating the causal mechanism fy . It lets us calculate quantities such as
the causal effect p(y | do(X = x), β), the probability of an outcome y given that we force X to
a specific value x and under fixed global structure β. This quantity differs from the conditional
p(y | x, β). The conditional takes the model and filters to the subpopulation where X = x; in
general, the processes which set X to that value may also have influenced Y . Thus the conditional
is not the same as if we had manipulated X directly (Pearl, 2000).
Under the causal graph of Figure 1, the adjustment formula says that p(y | do(x), β) = p(y | x, β).
This means we can estimate fy from observational data {(xn, yn)}, assuming we observe the global
structure β. For example, an additive noise model (Hoyer et al., 2009) posits
yn = f(Xn,β | θ) + en,	e 〜s(∙),
where f (∙) might be a linear function of the concatenated inputs, f (∙) = [xn, β]>θ, or it might use
spline functions for nonlinearities. If s(∙) is standard normal, the induced density for y is normal
with unit variance. Placing a prior over parameters p(θ), Bayesian inference yields
p(θ I χ,y,β) Yp(θ)p(y Iχ, θβ)∙
(2)
The right hand side is a joint density whose individual components can be calculated. We can use
standard algorithms, such as variational inference or MCMC (Murphy, 2012).
A limitation in additive noise models that they typically apply simple nonlinearities such as polyno-
mials, hand-engineered low-order interactions between inputs, and assume additive interaction with,
e.g., Gaussian noise. Next we describe how to build richer causal models which relax these restric-
tions. (An additional problem is that we typically don’t observe β; we address this in § 3.)
2.2	Implicit Causal Models
Implicit models capture an unknown distribution by hypothesizing about its generative process (Dig-
gle & Gratton, 1984; Tran et al., 2017). For a distribution p(x) of observations x, recent advances
define a function g that takes in noise e 〜s(∙) and outputs X given parameters θ,
x = g(e I θ),
e 〜s(∙).
(3)
3
Under review as a conference paper at ICLR 2018
Figure 2: (left) Causal graph for GWAS. The population structure of SNPs for each individual (zn)
confounds inference of how each SNP (xnm) causes a trait of interest (yn). (right) Implicit causal
model for gwas (described in § 3.2). Its structure is the same as the causal graph but also places
priors over parameters φ and θ and with a latent variable wm per SNP.
Unlike models which assume additive noise, setting g to be a neural network enables multilayer,
nonlinear interactions. Implicit models also separate randomness from the transformation; this imi-
tates the structural invariance of causal models (Equation 1).
To enforce causality, we define an implicit causal model as a probabilistic causal model where the
functions g form structural equations, that is, causal relations among variables. Implicit causal
models extend implicit models in the same way that causal networks extend Bayesian networks
(Pearl & Verma, 1991) and path analysis extends regression analysis (Wright, 1921). They are
nonparametric structural equation models where the functional forms are themselves learned.
A natural question is the representational capacity of implicit causal models. In general, they are
universal approximators: we can use a fully connected network with a sufficiently large number of
hidden units to approximate each causal mechanism. We describe this formally.
Theorem (Universal Approximation Theorem). Let the tuple (E, V, F, s(E)) denote a probabilistic
causal model, where E represent the set of background variables with probability s(E), V the set of
endogenous variables, and F the causal mechanisms. Assume each causal mechanism is a continu-
ous function on the m-dimensional unit cube f ∈ C([0, 1]m). Let σ be a nonconstant, bounded, and
monotonically-increasing continuous function.
For each causal mechanism f and any error δ > 0, there exist parameters θ = {v, w, b}, for real
constants vi, bi ∈ R and real vectors wi ∈ Rm fori = 1, . . . , H and fixed H, such that the following
function approximates f:
H
g(x | θ) = ^X Viσ (WTX + bi) ,	|g(x | θ) — f(x)∣ < δ for all X ∈ [0,1]m.
i=1
The implicit model defined by the collection of functions g and same noise distributions universally
approximates the true causal model.
(This directly follows from the approximator theorem of, e.g., Cybenko (1989).) A key aspect is
that implicit causal models are not only universal approximators, but that we can use fast algo-
rithms for their Bayesian inference (to calculate Equation 2). In particular, variational methods both
scale to massive data and provide accurate posterior approximations (§ 4). This lets us obtain good
performance in practice with finite-sized neural networks; § 5 describes such experiments.
3	Implicit Causal Models with Latent Confounders
We described implicit causal models, a rich class of models that can capture arbitrary causal re-
lations. For simplicity, we assumed that the global structure is observed; this enables standard
inference methods. We now consider the typical setting when it is unobserved.
3.1	Causal Inference with a Latent Confounder
Consider the running example of genome-wide association studies (gwas) (Figure 2). There are
N data points (individuals). Each data point consists of an input vector of length M (measured
SNPs), Xn = [Xn1, . . . , XnM] and a scalar outcome yn (the trait of interest). Typically, the number of
4
Under review as a conference paper at ICLR 2018
measured SNPs M ranges from 100,000 to 1 million and the number of individuals N ranges from
500 to 10,000.
We are interested in how changes to each SNP Xm cause changes to the trait Y . Formally, this is the
causal effect p(y | do(xm), x-m), which is the probability of an outcome y given that we force SNP
Xm = xm and consider fixed remaining SNPs x-m. Standard inference methods are confounded by
the unobserved population structure of snps for each individual, as well as the individual’s cryptic
relatedness to other samples in the data set. This confounder is represented as a latent variable zn ,
which influences xnm and yn for each data index n; see Figure 2. Because we do not observe the
zn’s, the causal effect p(y | do(xm), x-m) is unidentifiable (Spirtes et al., 1993).
Building on previous gwas methods (Price et al., 2006; Yu et al., 2005; Astle & Balding, 2009), we
build a model that jointly captures zn’s and the mechanisms for Xm → Y . Consider the implicit
causal model where for each data point n = 1, . . . , N and for each SNP m = 1, . . . , M,
Zn = gz (eZn ),	eZn 〜s(∙),
Xnm = gXm (eXnm , zn 1 Wm)，	eXnm 〜s(')，	(4)
yn = gy (eyn , xn,1:M, zn | θ),	eyn 〜s(∙).
The function gz(∙) for the ConfoUnder is fixed. Each function gχm (∙ | Wm) per SNP depends on the
ConfoUnder and has parameters wm,. The function gy (∙ | θ) for the trait depends on the confounder
and all SNPs, and it has parameters θ. We place priors over the parameters p(Wm) and p(θ).
Figure 2 (right) visualizes the model. It is a model over the full causal graph (Figure 2 (left)) and
differs from the unconfounded case: Equation 2 only requires a model from X → Y , and the rest of
the graph is “ignorable” (Imbens & Rubin, 2015).
To estimate the mechanism fy, we calculate the posterior of the outcome parameters θ,
p(θ | x,y)
p(z | x, y)p(θ |x,y,z)dz.
(5)
Note how this accounts for the unobserved confounders: it assumes that p(z | x, y) accurately re-
flects the latent structure. In doing so, we perform inferences for p(θ | x, y, z), averaged over poste-
rior samples from p(z | x, y).
In general, causal inference with latent confounders can be dangerous: it uses the data twice (once
to estimate the confounder; another to estimate the mechanism), and thus it may bias our estimates
of each arrow Xm → Y . Why is this justified? We answer this below.
Proposition 1. Assume the causal graph of Figure 2 (left) is correct and that the true distribution
resides in some configuration of the parameters of the causal model (Figure 2 (right)). Then the
posterior p(θ | x, y) provides a consistent estimator of the causal mechanism fy.
(See Appendix A for the proof.) Proposition 1 rigorizes previous methods in the framework of
probabilistic causal models. The intuition is that as more SNPs arrive (“M → ∞, N fixed”), the
posterior concentrates at the true confounders zn , and thus we can estimate the causal mechanism
given each data point’s confounder zn . As more data points arrive (“N → ∞, M fixed”), we can
estimate the causal mechanism given any confounder zn as there are infinity of them.
Connecting to Two-Stage Estimation. Existing GWAS methods adjust for latent population struc-
ture using two stages (Astle & Balding, 2009): first, estimate the confounders z1:N ; second, infer the
outcome parameters θ given the data set and the estimate of the confounders. To incorporate uncer-
tainty, a Bayesian version would not use a point estimate of z1:N but the full posterior p(z1:N | x, y);
then it would infer θ given posterior samples ofz1:N. Following Equation 5, this is the same as joint
posterior inference. Thus the two stage approach is justified as a Bayesian approximation that uses
a point estimate of the posterior.
3.2	Implicit Causal Model with a Latent Confounder
Above, we outlined how to specify an implicit causal model for gwas. We now specify in detail the
functions and priors for the confounders zn, the SNPs xnm, and the traits yn (Equation 4). Figure 2
(right) visualizes the model we describe below. Appendix B provides an example implementation
in the Edward probabilistic programming language (Tran et al., 2016).
5
Under review as a conference paper at ICLR 2018
Generative Process of Confounders zn. We use standard normal noise and set the confounder
function gz(∙) to the identity. This implies the distribution of ConfoUnders p(zn,) is standard normal.
Their dimension zn ∈ RK is a hyperparameter. The dimension K should be set to the highest value
such that the latent space most closely approximates the true population structure but smaller than
the total number of snps to avoid overfitting.
Generative Process of SNPs xnm. Designing nonlinear processes that return matrices is an ongoing
research direction (e.g., Lawrence (2005); Lloyd et al. (2012)). To design one for gwas (the snp
matrix), we build on an implicit modeling formulation of factor analysis; it has been successful in
GWAS applications (Price et al., 2006; Song et al., 2015). Let each SNP be encoded as a 0, 1, or 2 to
denote the three possible genotypes. This is unphased data, where 0 indicates two major alleles; 1
indicates one major and one minor allele; and 2 indicates two minor alleles. Set
logit πnm = Zn wm,	Xnm = I[e1 > πnm] + I[e2 > πnm],	^1,^2 〜Uniform(0, 1).
This defines a Binomial(2, πnm) distribution on xnm. Analogous to generalized linear models, the
Binomial’s logit probability is linear with respect to zn . We then sum up two Bernoulli trials: they
are represented as indicator functions of whether a uniform sample is greater than the probability.
(The uniform noises are newly drawn for each index n and m.)
Assuming a standard normal prior on the variables wm , this generative process is equivalent to
logistic factor analysis. The variables wm act as “principal components,” embedding the M -many
SNPs within a subspace of lower dimension K.
Logistic factor analysis makes strong assumptions: linear dependence on the confounder and that
one parameter per dimension has sufficient representational capacity. We relax these assumptions
using a neural network over concatenated inputs,
logit πnm = NN([zn, wm] | φ).
Similar to the above, the variables wm serve as principal components. The neural network takes an
input of dimension 2K and outputs a scalar real value; its weights and biases φ are shared across
SNPs m and individuals n. This enables learning of nonlinear interactions between zn and wm ,
preserves the model’s conditional independence assumptions, and avoids the complexity ofa neural
net that outputs the full N × M matrix. We place a standard normal prior over φ.
Generative Process of Traits yn. To specify the traits, we build on an implicit modeling formula-
tion of linear regression. It is the mainstay tool in gwas applications (Price et al., 2006; Song et al.,
2015). Formally, for real-valued y ∈ R, we model each observed trait as
yn = [xn,1:M, zn] θ + ^n,	Cn 〜Normal(0, 1),
This process assumes linear dependence on snps, no gene-gene and gene-population interaction,
and additive noise. We generalize this model using a neural network over the same inputs,
yn = NN([xn,i:M, Zn, c] | θ), Cn 〜Normal(0, 1).
The neural net takes an input of dimension M+K+ 1 and outputs a scalar real value; for categorical
outcomes, the output is discretized over equally spaced cutpoints. We also place a group Lasso prior
on weights connecting a snp to a hidden layer. This encourages sparse inputs: we suspect few snps
affect the trait (Yuan & Lin, 2006). We use standard normal for other weights and biases.
4	Likelihood-Free Variational Inference
We described a rich causal model for how snps cause traits and that can adjust for latent population-
confounders. Given GWAS data, we aim to infer the posterior of outcome parameters θ (Equation 5).
Calculating this posterior reduces to calculating the joint posterior of confounders Zn, SNP parame-
ters wm and φ, and trait parameters θ,
p(zi：N, wi：M, Φ,θ | χ, y) X p(Φ)p(θ)
N
Y p(Zn)p(yn |
n=1
xn
M
1:M, Zn, θ)	p(wm)p(xnm
m=1
| Zn , wm,
φ) .
This means we can use typical inference algorithms on the joint posterior. We then collapse variables
to obtain the marginal posterior of θ. (For Monte Carlo methods, we drop the auxiliary samples; for
variational methods, it is given if the variational family follows the posterior’s factorization.)
6
Under review as a conference paper at ICLR 2018
One difficulty is that with implicit models, evaluating the density is intractable: it requires integrat-
ing over a nonlinear function with respect to a high-dimensional noise (Equation 3). Thus we require
likelihood-free methods, which assume that one can only sample from the model’s likelihood (Marin
et al., 2012; Tran et al., 2017). Here we apply likelihood-free variational inference (lfvi), which
we scale to billions of genetic measurements (Tran et al., 2017).
As with all variational methods, lfvi posits a family of distributions over the latent variables and
then optimizes to find the member closest to the posterior. For the variational family, we specify nor-
mal distributions with diagonal covariance for the SNP components wm and confounder zn ,
q(wm) = Normal(wm； μww.,。W.I),	q(zn) = Normal(zn； μz.,。名.I).
We specify a point mass for the variational family on both neural network parameters φ and θ. (This
is equivalent to point estimation in a variational EM setting.)
For lfvi to scale to massive gwas data, we use stochastic optimization by subsampling snps
(Gopalan et al., 2016). At a high level, the algorithm proceeds in two stages. In the first stage,
lfvi cycles through the following steps:
1.	Sample SNP location m and collect the observations at that location from all individuals.
2.	Use the observations and current estimate of the confounders z1:N to update the mth SNP
component wm and SNP neural network parameters φ.
3.	Use the observations and current estimate of SNP components w1:M to update the con-
founders z1:N .
This first stage infers the posterior distribution of confounders zn and SNPs parameters wm and φ.
Each step’s computation is independent of the number of snps, allowing us to scale to millions of
genetic factors. In experiments, the algorithm converges while scanning over the full set of snps
only once or twice.
In the second stage, we infer the posterior of outcome parameters θ given the inferred confounders
from the first stage. Appendix C describes the algorithm in more detail; it expands on the lfvi
implementation in Edward (Tran et al., 2016).
5	Empirical S tudy
We described implicit causal models, how to adjust for latent population-based confounders, and
how to perform scalable variational inference. In general, validating causal inferences on observa-
tional data is not possible (Pearl, 2000). Therefore to validate our work, we perform an extensive
simulation study on 100,000 snps, 940 to 5,000 individuals, and across 100 replications of 11 set-
tings. The study indicates that our model is significantly more robust to spurious associations, with
a state-of-the-art gain of 15-45.3% in accuracy. We also apply our model to a real-world gwas of
Northern Finland Birth Cohorts; our model indeed captures real causal relationships—identifying
similar snps as previous state of the art.
We compare against three methods that are currently state of the art: PCA with linear regression
(Price et al., 2006) (“PCA”); a linear mixed model (with the EMMAX software) (Kang et al., 2010)
(“LMM”); and logistic factor analysis with inverse regression (Song et al., 2015) (“GCAT”). In all
experiments, we use Adam with a initial step-size of 0.005, initialize neural network parameters
uniformly with He variance scaling (He et al., 2015), and specify the neural networks for traits and
snps as fully connected with two hidden layers, ReLU activation, and batch normalization (hidden
layer sizes described below). For the trait model’s neural network, we found that including latent
variables as input to the final output layer improves information flow in the network.
5.1	Simulation Study: Robustness to Spurious Associations
We analyze 11 simulation configurations, where each configuration uses 100,000 SNPs and 940 to
5,000 individuals. We simulate 100 gwas data sets per configuration for a grand total of 4,400 fitted
models (4 methods of comparison). Each configuration employs a true model to generate the snps
and traits based on real genomic data. Following Hao et al. (2016), we use the Balding-Nichols
model based on the HapMap dataset (Balding & Nichols, 1995; Gibbs et al., 2003); PCA based on
7
Under review as a conference paper at ICLR 2018
Trait		ICM	PCA (Price+06)	LMM (Kang+10)	GCAT (Song+10)
HapMap		99.2	34.8	30.7	99.2
TGP		85.6	2.7	43.3	70.3
HGDP		91.8	6.8	40.2	72.3
PSD (a =	1)	97.0	80.4	92.3	95.3
PSD (a =	0.5)	94.3	79.5	90.1	93.6
PSD (a =	0.1)	92.2	38.1	38.6	90.4
PSD (a =	0.01)	92.7	24.2	35.1	90.7
Spatial (a	=1)	90.9	56.4	60.0	75.2
Spatial (a	= 0.5)	86.2	50.5	46.6	72.5
Spatial (a	= 0.1)	80.9	2.4	26.6	35.6
Spatial (a	= 0.01)	75.5	1.8	15.3	30.2
Table 1: Precision accuracy over an extensive set of configurations and methods; we average over
100 simulations for a grand total of 4,400 fitted models. The setting a in PSD and Spatial determines
the amount of sparsity in the latent population structure: lower a means higher sparsity. ICM is
significantly more robust to spurious associations, outperforming other methods by up to 45.3%.
the 1000 Genomes Project (TGP) (Consortium et al., 2010); PCA based on the Human Genome Di-
versity project (HGDP) (Rosenberg et al., 2002); four variations of the Pritchard-Stephens-Donelly
model (PSD) based on HGDP (Pritchard et al., 2000); and four variations of a configuration where
population structure is determined by a latent spatial position of individuals. Only 10 of the 100,000
snps are set to be causal. Appendix D provides more detail.
Table 1 displays the precision for predicting causal factors across methods. When failing to ac-
count for population structure, “spurious associations” occur between genetic markers and the trait
of interest, despite the fact that there is no biological connection. Precision is the fraction of the
number of true positives over the number of true and false positives. This measures a method’s
robustness to spurious associations: higher precision means fewer false positives and thus more
robustness.1
Table 1 shows that our method achieves state of the art across all configurations. Our method espe-
cially dominates in difficult tasks with sparse (small a), spatial (Spatial), and/or mixed membership
structure (PSD): there is over a 15% margin in difference to the second best in general, and up to a
45.3% margin on the Spatial (a = 0.01) configuration. For simpler configurations, such as a mixture
model (HapMap), our method has comparable performance.
5.2	Northern Finland B irth Cohort Data
We analyze a real-world gwas of Northern Finland Birth Cohorts (Sabatti et al., 2009), which
measure several metabolic traits and height and which contain 324,160 snps and 5,027 individuals.
We separately fitted 10 implicit causal models, each of which models the effect of snps on one of
ten traits. To specify the implicit causal models, we set the latent dimension of confounders to be
6 (following Song et al. (2015)). We use 512 units in both hidden layers of the snp neural network
and use 32 and 256 units for the trait neural network’s first and second hidden layers respectively.
Appendix E provides more detail.
Table 2 compares the number of identified causal snps across methods, with an additional “uncor-
rected” baseline, which does not adjust for any latent population structure. Each method is per-
formed with a subsequent correction as measured by the genomic control inflation factor (Sabatti
et al., 2009). Our models identify similar causal snps as previous methods. Interestingly, our model
tends to agree with Song et al. (2015), identifying a total of 15 significant loci (Song et al. (2015)
identified 16; others identified 11-14 loci). This makes sense intuitively, as Song et al. (2015) uses
logistic factor analysis which, compared to all methods, most resembles our model.
1We also measured recall. All true-positives were found across all methods (like a real-world experiment,
only few (10) snps are causal). Rarely did a method deviate and if so, it missed at most one.
8
Under review as a conference paper at ICLR 2018
Trait	ICM	GCAT	LMM	PCA	Uncorrected
Body mass index	0	0	0	0	0
C-reactive protein	2	2	2	2	2
Diastolic blood pressure	0	0	0	0	0
Glucose levels	3	3	2	2	2
HDL cholesterol levels	4	4	4	2	4
Height	1	1	0	0	0
Insulin levels	0	0	0	0	0
LDL cholesterol levels	3	4	3	3	3
Systolic blood pressure	0	0	0	0	0
Triglyceride levels	2	2	3	2	2
Table 2: Number of significant loci at genome-wide significance (p < 7.2 × 10-8) for each of the
ten traits from NFBC data. The counts for GCAT are obtained from Song et al. (2015, Table 1);
counts for LMM, PCA, and uncorrected are obtained from Kang et al. (2010, Table 2). The implicit
causal model (ICM) captures causal relationships comparable to previously work.
6	Discussion
We described implicit causal models, a rich class of models that can capture high-dimensional,
nonlinear causal relationships. With genome-wide association studies, implicit causal models gen-
eralize previous successful methods to capture important nonlinearities, such as gene-gene and
gene-population interaction. In addition, we described an implicit causal model that adjusts for
confounders by sharing strength across examples. Our model achieves state-of-the-art accuracy,
significantly outperforming existing genetics methods by 15-45.3%.
There are several limitations to learning true causal associations. For example, alleles at different
loci typically exhibit linkage disequilibrium, which is a local non-random association influenced by
factors such as the rate of recombination, mutation, and genetic drift. The implicit causal model
might be extended with variables shared across subsets of snps to model the recombination process.
Another limitation involves the data, where granularity of sequenced loci may lose signal or attribute
causation to a region involving multiple snps. Better technology, and accounting for mishaps in the
sequencing process in the model, can help.
While we focused on gwas applications in this paper, we also believe implicit causal models have
significant potential in other sciences: for example, to design new dynamical theories in high en-
ergy physics; and to accurately model structural equations of discrete choices in economics. We’re
excited about applications to these new domains, leveraging modern probabilistic modeling and
causality to drive new scientific understanding.
References
Babak Alipanahi, Andrew Delong, Matthew T Weirauch, and Brendan J Frey. Predicting the se-
quence specificities of dna- and rna-binding proteins by deep learning. Nature Biotechnology, 33
(8):831-838,2015.
William Astle and David J Balding. Population structure and cryptic relatedness in genetic associa-
tion studies. Statistical Science, 24(4):451-471, 2009.
David J Balding and Richard A Nichols. A method for quantifying differentiation between popula-
tions at multi-allelic loci and its implications for investigating identity and paternity. In Human
identification: The use of DNA markers, pp. 3-12. 1995.
1000 Genomes Project Consortium et al. A map of human genome variation from population scale
sequencing. Nature, 467(7319):1061, 2010.
Heather J Cordell. Detecting gene-gene interactions that underlie human diseases. Nature Reviews
Genetics, 10(6):392-404, 2009.
9
Under review as a conference paper at ICLR 2018
Thomas M Cover and Joy A Thomas. Elements of Information Theory. Wiley Series in Telecom-
munications and Signal Processing, 1991.
Robert Culverhouse, Brian K Suarez, Jennifer Lin, and Theodore Reich. A perspective on epistasis:
Limits of models displaying no main effect. The American Journal of Human Genetics, 70(2):
461-471,2002.
G Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control,
Signals and Systems, 2(4):303-314, 1989.
P J Diggle and R J Gratton. Monte Carlo methods of inference for implicit statistical models. Journal
of the Royal Statistical Society Series B, 1984.
Ronald A Fisher. The correlation between relatives on the supposition of mendelian inheritance.
Philosophical Transactions of the Royal Society of Edinburgh, 52:399-433, 1918.
Richard A Gibbs, John W Belmont, Paul Hardenbol, Thomas D Willis, FL Yu, HM Yang, Lan-Yang
Ch’ang, Wei Huang, Bin Liu, Yan Shen, et al. The international hapmap project. 2003.
P Gopalan, W Hao, D M Blei, and J D Storey. Scaling probabilistic models of genetic variation to
millions of humans. Nature Genetics, 2016.
W Hao, M Song, and J D Storey. Probabilistic models of genetic variation in structured populations
applied to global human studies. Bioinformatics, 2016.
Kaiming He, X Zhang, S Ren, and J Sun. Delving deep into rectifiers: Surpassing human-level
performance on imagenet classification. In International Conference on Computer Vision. IEEE,
2015.
Jennifer L Hill. Bayesian nonparametric modeling for causal inference. Journal of Computational
and Graphical Statistics, 20(1):217-240, 2011.
Patrik O Hoyer, Dominik Janzing, Joris M Mooij, Jonas Peters, and Prof Bernhard Scholkopf. Non-
linear causal discovery with additive noise models. In Neural Information Processing Systems,
2009.
Guido Imbens and Donald B Rubin. Causal Inference. Cambridge University Press, 2015.
Fredrik D Johansson, Uri Shalit, and David Sontag. Learning representations for counterfactual
inference. In International Conference on Machine Learning, 2016.
Hyun Min Kang, Jae Hoon Sul, Susan K Service, Noah A Zaitlen, Sit-yee Kong, Nelson B Freimer,
Chiara Sabatti, and Eleazar Eskin. Variance component model to account for sample structure in
genome-wide association studies. Nature Genetics, 42(4):348-354, 2010.
Neil D Lawrence. Probabilistic Non-linear Principal Component Analysis with Gaussian Process
Latent Variable Models. The Journal of Machine Learning Research, 6:1783-1816, 2005.
James Robert Lloyd, Peter Orbanz, Zoubin Ghahramani, and Daniel M Roy. Random function priors
for exchangeable arrays with applications to graphs and relational data. In Neural Information
Processing Systems, 2012.
Christos Louizos, Uri Shalit, Joris Mooij, David Sontag, Richard Zemel, and Max Welling. Causal
effect inference with deep latent-variable models. 2017.
Jean-Michel Marin, Pierre Pudlo, Christian P Robert, and Robin J Ryder. Approximate bayesian
computational methods. Statistics and Computing, 22(6):1167-1180, 2012.
Brett A McKinney, David M Reif, Marylyn D Ritchie, and Jason H Moore. Machine learning for
detecting gene-gene interactions. Applied Bioinformatics, 5(2):77-88, 2006.
Joris M Mooij, Oliver Stegle, Dominik Janzing, Kun Zhang, and Bernhard Schoelkopf. Probabilis-
tic latent variable models for distinguishing between cause and effect. In Neural Information
Processing Systems, 2010.
Kevin P Murphy. Machine Learning: A Probabilistic Perspective. MIT press, 2012.
J Pearl. Causality. Cambridge University Press, 2000.
10
Under review as a conference paper at ICLR 2018
Judea Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference. Morgan
Kaufmann, 1988.
Judea Pearl and Thomas S Verma. A theory of inferred causation. In Proceedings of the Second
International Conference on Principles of Knowledge Representation and Reasoning, 1991.
Alkes L Price, Nick J Patterson, Robert M Plenge, Michael E Weinblatt, Nancy A Shadick, and
David Reich. Principal components analysis corrects for stratification in genome-wide association
studies. Nature Genetics, 38(8):904-909, 2006.
Jonathan K Pritchard, Matthew Stephens, and Peter Donnelly. Inference of population structure
using multilocus genotype data. Genetics, 155(2):945-959, 2000.
Danilo J Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approx-
imate inference in deep generative models. In International Conference on Machine Learning,
2014.
Noah A Rosenberg, Jonathan K Pritchard, James L Weber, Howard M Cann, Kenneth K Kidd,
Lev A Zhivotovsky, and Marcus W Feldman. Genetic structure of human populations. Science,
298(5602):2381-2385, 2002.
Chiara Sabatti, Susan K Service, Anna-Liisa Hartikainen, Anneli Pouta, Samuli Ripatti, Jae Brod-
sky, Chris G Jones, Noah A Zaitlen, Teppo Varilo, Marika Kaakinen, et al. Genome-wide associ-
ation analysis of metabolic traits in a birth cohort from a founder population. Nature genetics, 41
(1):35-46, 2009.
Minsun Song, Wei Hao, and John D Storey. Testing for genetic associations in arbitrarily structured
populations. Nature, 47(5):550-554, 2015.
Peter Spirtes, Clark Glymour, and Richard Scheines. Causation, prediction, and search. Springer-
Verlag, 1993.
Dustin Tran, Alp Kucukelbir, Adji B Dieng, Maja Rudolph, Dawen Liang, and David M
Blei. Edward: A library for probabilistic modeling, inference, and criticism. arXiv preprint
arXiv:1610.09787, 2016.
Dustin Tran, Rajesh Ranganath, and David M Blei. Hierarchical implicit models and likelihood-free
variational inference. In Neural Information Processing Systems, 2017.
Stefan Wager and Susan Athey. Estimation and inference of heterogeneous treatment effects using
random forests. arXiv preprint arXiv:1510.04342, 2015.
Bruce S Weir andC Clark Cockerham. Estimating f-statistics for the analysis of population structure.
Evolution, 38(6):1358-1370, 1984.
Sewall Wright. Correlation and causation. Journal of agricultural research, 20(7):557-585, 1921.
Jianming Yu, Gael Pressoir, William H Briggs, Irie Vroh Bi, Masanori Yamasaki, John F Doebley,
Michael D McMullen, Brandon S Gaut, Dahlia M Nielsen, James B Holland, Stephen Kresovich,
and Edward S Buckler. A unified mixed-model method for association mapping that accounts for
multiple levels of relatedness. Nature Genetics, 38(2):203-208, 2005.
Ming Yuan and Yi Lin. Model selection and estimation in regression with grouped variables. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 68(1):49-67, 2006.
KUn Zhang and AaPo Hyvarinen. On the identifiability of the post-nonlinear causal model. In
Uncertainty in Artificial Intelligence, pp. 647-655, 2009.
11
Under review as a conference paper at ICLR 2018
A Consistency
Consider the simplest setting in § 2, where the causal graph is as shown with a global confounder
with finite dimension and where we observe the data set {(xn, yn)}. Assume the specified model
family over the causal graph includes the true data generating process.
First consider an atomic intevention do(X = x) and let β* be the true structural value that generated
our observations. The probability of a new outcome given the intervention and global structure
is
p*(y | do(X = x),β*)) = p*(y |x,β*)∙
This follows from the backdoor criterion on the empty set. By Bernstein von-Mises, the poste-
rior for our model p(β | x, y) concentrates at β*. Thus, similarly, our posterior for θ given β*
concentrates to the true functional mechanism fy . This implies we have a consistent estimate of
p*(y I do(∙),β*).
This simple proof rigorizes the ideas behind learning and fixing population structure, a common
heuristic in gwas methods. Moreover, it lets us understand how to extend them to more complex
latent variable models of the confounder and also provide uncertainty estimates of the latent structure
become important as we apply these methods to finite data in practice.
B Implicit Causal Model in Edward
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
We provide an example of an implicit causal model written in the Edward language below. It writes
neural net weights and biases as model parameters. It does not include priors on the weights or
biases; we add these as penalties to the objective function during training.
import edward as ed
import tensorflow as tf
from edward.models import Binomial, Normal
N = 5000 # number of individuals
M = 100000 # number of SNPs
K = 25 # latent dimension
def snp_neural_network(z, w):
z_tile = tf.tile(tf.reshape(z, [N, 1, K]), [1, M, 1])
w_tile = tf.tile(tf.reshape(w, [1, M, K]), [N, 1, 1])
h = tf.concat([z_tile, w_tile], 2)
h = tf.layers.dense(h, 512, activation=tf.nn.relu)
h = tf.layers.dense(h, 512, activation=tf.nn.relu)
h = tf.layers.dense(h, 1, activation=None)
return tf.reshape(h, [N, M])
def trait_neural_network(z, x):
eps = Normal(loc=0.0, scale=1.0, sample_shape=[N, 1])
h = tf.concat([z, x, eps], 1)
h = tf.layers.dense(h, 32, activation=tf.nn.relu)
h = tf.layers.dense(h, 256, activation=tf.nn.relu)
h = tf.concat([z, h], 1)	# include connection to z for output layer
h = tf.layers.dense(h, 1, activation=None)
return tf.reshape(h, [N])
z = Normal(loc=0.0, scale=1.0, sample_shape=[N, K])
w = Normal(loc=0.0, scale=1.0, sample_shape=[M, K])
logits = snp_neural_network(z, w)
x = Binomial(total_count=2.0, logits=logits)
#	Note: Sometimes it’s preferable to have neural net parameterize a
#	distribution for tractable density such as for binary-valued traits.
#	To do this, specify, e.g., ‘y = Bernoulli(logits=trait_neural_network(z, x))‘
y = trait_neural_network(z, x)
12
Under review as a conference paper at ICLR 2018
C Likelihood-Free Variational Inference for GWAS
C.1 Variational Objective
The log-likelihood per-individual and per-snp is
logp(xnm,yn | wm, zn, θ, φ) = log p(yn | xn,1:M, zn, θ) + log p(xnm | wm, zn, φ).
There are local priors p(wm), p(zn) and global priors p(θ), p(φ). The posterior factorizes as
p(z1:N, w1:M, φ, θ | x,y)
MN
=p(Φ | χ)p(θ | χ, y) ɪɪ P(Wm | xi：n,m, Φ)	p(zn |xn,1:M, yn, θ, φ, w1:M).
m=1	n=1
Let the variational family follow the posterior’s factorization above. For notational convenience, we
drop the data dependence in q, q(φ) = q(φ | x), q(θ) = q(θ | x, y) q(wm | φ) = q(wm |
x1:N,m, φ),
q(zn | θ, φ, w1:M) = q(zn | xn,1:M, yn, θ, φ, w1:M). We write the evidence lower bound and decom-
pose the model’s log joint density and the variational density,
MN
L=XXEq(φ)q(wm | φ)q(θ)q(zn | θ,φ,w∖∙.M) [ log P(Xnm | wm, zn, φR +
m=1 n=1
N
X Eq(φ)q(wLM | φ)q(θ)q(zn | θ,φ,wi:M) log P(yn | xn,1:M, zn, θ) +
n=1
M
X Eq(φ)q(wm | φ) logP(wm) - log q(wm | φ) +
m=1
N
〉：Eq(Φ)q(wi:M | Φ)q(θ)q(zn | θ,Φ,wi:M) [ logP(Zn) ― log q(Zn | θ, φ, W1：MR +
n=1
Eq(φ)[log P(φ) - log q(φ)] + Eq(θ)[log P(θ) - log q(θ)].
Assume that the variational family for Zn and wm are independent of other variables, q(Zn) and
q(wm). Also assume delta point masses for q(θ) = I[θ-θ0] parameterized by θ0 and q(φ) = I[φ-φ0]
parameterized by φ0 . This simplifies the objective, reducing to
MN
L=
Eq(wm)q(zn)
m=1 n=1
N
log P(xnm |	wm,	Zn,	φ0)	+	XEq(zn)	log P(yn	|	xn,1:M, Zn, θ0)	+
n=1
MN
X Eq(wm) logP(wm) - log q(wm) +	X Eq(zn) logP(Zn) - logq(Zn) +
m=1	n=1
logP(φ0) + log P(θ0).
Each expectation can be unbiasedly estimated with Monte Carlo. For gradient-based optimization,
we use reparameterization gradients (Rezende et al., 2014). We describe them next.
C.2 First Stage: Learning the Confounder
We provide details for the gradients. Let λwm parameterize q(wm; λwm) and λzn parameterize
q(Zn; λzn). We are interested in training the parameters λwm, λzn, θ0, φ0. Subsample a SNP location
m ∈ {1,..., M}. Draw a sample Zn 〜q(zn； λzn) for n = 1,...,N and Wm 〜q(wm; λwm)
for m = 1, . . . , M, where the samples are reparameterizable (see Rezende et al. (2014) for de-
tails).
The gradient with respect to parameters λzn is unbiasedly estimated by
Vλzn L ≈ Vλzn [ logP(Xnm | Wm, Zn, φ0) + logPkyn | Xn,1:M, Zn ,θ0) + logP(Zn ) - log q(z!n )] ∙
13
Under review as a conference paper at ICLR 2018
This gradient scales linearly with the number of SNPs M. This is undesirable as the number of SNPs
ranges from the hundreds of thousands to millions. We prevent this scaling by observing that for
large M, the information in xn,1:M will influence the posterior far more than the single scalar yn. In
math, p(zn | xn,1:M, yn, θ, φ, w1:M) ≈ p(zn | xn,1:M, θ, φ, w1:M). This is a tacit assumption in all
gwas methods that adjust for the confounder (Yu et al., 2005; Price et al., 2006; Astle & Balding,
2009; Kang et al., 2010).
The gradient with respect to parameters λzn simplifies to
Vλzn L ≈ vλzn [ log P(Xnm | Wm ,z'n, φ') + log P(Zn ) - log q(Zn )],
which scales to massive numbers of snps.
The gradients with respect to parameters λwm and φ' are unbiasedly estimated by
N
vλwm L ≈ X [Vλwm log P(Xnm | Wm ,zn, φ')] + NXwm log P(Wm ) - NXwm log 9(Wm ),
n=1
N
Nφ0L ≈	Nφ0 log P(Xnm | Wm' ,Zn' , φ').
n=1
Note how none of this depends on the trait y or trait parameters θ. We can thus perform inference to
first approximate the posterior P(Z1:N, W1:M, φ | x, y). In a second stage, we can then perform infer-
ence to approximate the posterior P(θ | Z1:N, W1:M, x, y). The computational savings is significant
not only within task but across tasks: when modelling many traits of interest (for example, § 5.2),
inference over the snp confounders only needs to be done once and can be re-used. We perform
stochastic gradient ascent using these gradients to maximize the variational objective.
C.3 Second Stage: Learning the Trait
Above we described the first stage of an algorithm which performs stochastic gradient ascent to
optimize parameters so that q(zn)q(wiM)q(θ') ≈ p(zi：N, wi：M, φ | x, y). Given these parameters,
we are interested in training θ'. Dropping constants with respect to θ in the objective, we have
L Z
N
X Eq(zn) log P(yn | Xn,1:M , Zn, θ )i
n=1
+ log P(θ').
We maximize this objective using stochastic gradients with a single sample Zn 〜q(zn),
N
Nθ0L ≈ X Nθ0 log P(yn | Xn,1:M, Zn' , θ') + Nθ0 logP(θ')
n=1
This corresponds to Monte Carlo EM. Its primary computation per-iteration is the backward pass
of the trait’s neural network. Unlike in the first stage, we do not subsample snps as the likelihood
depends on all snps.
C.4 Second Stage: Handling Likelihood-Free Traits
In general, the density ofyn is intractable: we exploit its tractable density if yn is discrete (it induces
a categorical likelihood); otherwise for real-valued traits, we perform likelihood-free inference with
respect to yn. Following LFVI (Tran et al., 2017), define q(y) to be the empirical distribution over
observed data {yn }. Then subtract it as a constant to the objective, so
N
L Z X Eq(zn) log P(yn | Xn,1:M, Zn, θ') - log q(yn) + log P(θ').
n=1
We approximate this log-ratio with a ratio estimator, r(yn, Xn,1:M, Zn, θ'; λr). It is a function of all
inputs in the log-ratio and is parameterized by λr .
We train the ratio estimator by minimizing a loss function with respect to its parameters,
Ep(yn | Xn,iM/n,θ')[- log σ(r("n,xn, 1:M,zn,%λr ))]+Eq(yn)[- Iog(I-σ(r3n,xn, 1:M,zn,θ'; λr )))]∙
14
Under review as a conference paper at ICLR 2018
The global minima of this objective with respect to the ratio estimator is the desired log-ratio,
r*(yn, Xn,1:M, Zn,θ0) = logP("n | Xn,1:M,，n, θ0) - log q("n)∙
Unfortunately, the ratio estimator has inputs of many dimensions. In particular, it has the prob-
lematic property of scaling with the number of snps, which can be on the order of hundreds of
thousands.
We can efficiently parameterize the ratio estimator by studying two extreme cases with respect to
computational efficiency and statistical efficiency. In one extreme, suppose yn has a tractable Gaus-
sian density with mean given by the outcome model’s neural network and unit variance (that is, the
neural net is parameterized to apply a location-shift on the noise input, yn, = NN(∙) + eQ. UP to
additive and multiplicative constants, the optimal log-ratio is
r*(yn,Xn,1M ,Zn,θ0) H (y∏ - NN([Xn,1:M, Zn] | θ0))2∙
This implies the ratio estimator must relearn the neural network’s forward pass in order to estimate
the optimal log-ratio. This is computationally redundant and can lead to unstable training. On
another extreme, suppose we parameterize r as
r(yn, xn,1:M, zn, θ0; λr) = r(yn, NN([xn,1:M, zn,e] | θ0); λr).
This dramatically reduces r’s input dimensions, from hundreds of thousands to just two. However,
while computationally efficient, this is a poor statistical approximation: there is only a single dimen-
sion to preserve information about xn,1:M, zn, en, and θ0 relevant to yn; this dimension is lossy for
even Gaussian densities.
As a middleground, we use the neural net’s first hidden layer as input into the ratio estimator,
r(yn, xn,1:M, zn, θ ; λr) = r(yn, hn; λr).
This reduces the ratio estimator’s inputs to be the trait yn and first hidden layer’s units hn. This
hidden layer has much fewer dimensions than the raw inputs, such as 32 units (making it compu-
tationally efficient). Moreover, under the data processing inequality (Cover & Thomas, 1991), hn
preserves more information relevant to yn than subsequent layers of the neural network (making
it statistically efficient). For all experiments, we parameterized r with two fully connected hidden
layers with equal number of hidden units.
The gradient with respect to parameters θ is estimated by
N
Vθ, L ≈ E Vθ0 r(yn, Xn,1:M ,Zn ,θ0; λr ) + log p(θ0).
n=1
This substitutes in the ratio estimator as a proxy to the intractable likelihood.
We minimize the auxiliary loss function in order to train the ratio estimator. Sample y'n 〜
p(yn | Xn,i：M, Zn, θ0) and subsample a data point ynj 〜q(yn). The gradient is estimated by
Vλr ∙ ≈ V", [ - log σ(r(y'n, hn; λr))- log(1 - σ(r(yn,hn, λr)))].
This corresponds to maximum likelihood, balanced with an adversarial objective to estimate the
likelihood, and is relatively fast. We perform stochastic gradient ascent, alternating between these
two sets of gradients.
D	Simulation S tudy
We provide more detail to § 5.1. Implicit causal models can not only represent many causal struc-
tures but, more importantly, learn them from data. To demonstrate this, we simulate data from a
comprehensive collection of popular models in gwas and analyze how well the fitted model can
capture them. These configurations exactly follow Hao et al. (2016) with same hyperparameters,
which we describe below.
For each of the 11 simulation configurations, we generate 100 independent data sets. Each data set
consists ofa M × N matrix of genotypes X and vector ofN traits y. Each individual n has M SNPs
and one trait.
15
Under review as a conference paper at ICLR 2018
D. 1 Genotype Matrix
To simulate the M X N matrix of genotypes X, We draw Xmn 〜Binomial(2,∏mn) for m =
1, . . . , M SNPs and n = 1, . . . , N individuals. The probabilities πmn can be encoded under a
real-valued M × N matrix of allele frequencies F where πmn = [logit(F)]mn.
Many models in GWAS can be described under the factorization F = ΓS, where Γ is a M × K matrix
and S is aK×N matrix for a fixed rank K ≤ N. This includes principal components analysis (Price
et al., 2006), the Balding-Nichols model (Balding & Nichols, 1995), and the Pritchard-Stephens-
Donnelly model (Pritchard et al., 2000). The M × K matrix Γ describes how structure manifests
in the allele frequencies across SNPs. The K × N matrix S encapsulates the genetic population
structure across individuals.
We describe how we form Γ and S for each of the 11 simulation configurations.
Balding-Nichols Model (BN) + HapMap. The BN model describes individuals according to a
discrete mixture of ancestral subpopulations (Balding & Nichols, 1995). The HapMap data set was
collected from three discrete populations (Gibbs et al., 2003), which allows us to populate each
row m of Γ with three i.i.d. draws from the Balding-Nichols model: Γm,k 〜BN(pm,, Fm), where
k ∈ {1, 2, 3}. Each Γmk is interpreted to be the allele frequency for subpopulation k at SNP m. The
pairs (pm , Fm ) are computed by randomly selecting a SNP in the HapMap data set, calculating its
observed allele frequency, and estimating its FST value using the estimator of Weir & Cockerham
(1984). The columns of S are populated with indicator vectors such that each individual is as-
signed to one of the three subpopulations. The subpopulation assignments are drawn independently
with probabilities 60/210, 60/210, and 90/210, which reflect the subpopulation proportions in the
HapMap data set. The simulated data has M = 100, 000 SNPs and N = 5000 individuals.
1000 Genomes Project (TGP). TGP is a project that comprehensively catalogs human genetic vari-
ation by producing complete genome sequences of well over 1000 individuals of diverse ancestries
(Consortium et al., 2010). To form Γ, we sample Γmk 〜0.9Uniform(0,1/2) for k = 1,2 and
set Γm3 = 0.05. To form S, we compute the first two principal components of the TGP genotype
matrix after mean centering each snp. We then transform each principal component to be between
(0, 1) and set the first two rows of S to be the transformed principal components. The third row of
S is set to 1 as an intercept. The simulated data has M = 100, 000 SNPs and N = 1500 individuals
(the total number of individuals in the TGP data set).
Human Genome Diversity Project (HGDP). HGDP is an international project that has genotyped
a large collection of DNA samples from individuals distributed around the world, aiming to assess
worldwide genetic diversity at the genomic level (Rosenberg et al., 2002). We followed the same
scheme as for TGP above. The simulated data has M = 100, 000 SNPs and N = 940 individuals
(the total number of individuals in the HGDP data set).
Pritchard-Stephens-Donnelly (PSD) + HGDP. The PSD model describes individuals according
to an admixture of ancestral subpopulations (Pritchard et al., 2000). The rows of Γ are drawn from
three i.i.d. draws from the Balding-Nichols model: Γmk 〜BN(pm, Fm), where k ∈ {1, 2, 3}. The
pairs (pm , Fm ) are computed by randomly selecting a SNP in the HGDP data set, calculating its
observed allele frequency, and estimating its FST value using the estimator of Weir & Cockerham
(1984). The estimator requires each individual to be assigned to a subpopulation, which are made
according to the K = 5 subpopulations from the analysis in Rosenberg et al. (2002). The columns
of S are sampled (sin, s2n s3n)〜Dirichlet(α = (a, a, a)) for n = 1,...,N. We apply four PSD
configurations with hyperparameter settings ofa = 0.01, 0.1, 0.5, 1. Varying a from 1 to 0 varies the
level of sparsity as individuals are placed from uniformly to corners of the simplex. The simulated
data has M = 100, 000 SNPs and N = 5000 individuals.
Spatial. In this setting, we simulate genotypes such that the population structure relates to the
spatial position of individuals. The matrix Γ is populated by sampling Γmk 〜0.9 Uniform(0,1/2)
for k = 1, 2 and setting Γm3 = 0.05. The first two rows of S correspond to coordinates for each
individual on the unit square and are set to be independent and identically distributed samples from
Beta(a, a), while the third row of S is set to 1 as an intercept. We apply four spatial configura-
tions with hyperparameter settings of a = 0.01, 0.1, 0.5, 1. As with the Dirichlet distribution in
the PSD model, varying a from 1 to 0 varies the level of sparsity as individuals are placed from
16
Under review as a conference paper at ICLR 2018
uniformly to corners of the unit square. The simulated data has M = 100, 000 SNPs and N = 5000
individuals.
D.2 Traits of Interest
To simulate traits y, we simulate from a linear model: for each individual n’s SNPs {xmn},
M
yn =	βm
xmn + λn + €n,	En 〜Normal(0, σn).
m=1
Each trait is real-valued and is determined by a linear combination of snps, a per-individual offset,
and heteroskedastic noise. Below we describe how we set {βm}, {λn}, and {σn}.
Without loss of generality, we set the first 10 SNPs to be true alternative SNPs (βm 6= 0), where
βm 〜 Normal(0,0.5) for m = 1, 2,..., 10; βm = 0 for m > 10. In order to simulate λj and Ej so
that they are also influenced by the latent variables z1, . . . , zn, we performed the following:
1.	Run K-means clustering on the columns of S with K = 3 using Euclidean distance. This
assigns each individual j to one of three partitions S1, S2, S3 where Sk ⊂ 1, 2, . . . , n.
2.	Set λj = k for all j ∈ Sk for each k = 1, 2, 3.
3.	Draw τ2,τ2,τ3 〜 InverseGamma(3, 1). Set σj = τk for all j ∈ Sk.
Following Song et al. (2015),
This strategy simulates non-genetic effects and random variation that manifest among K discrete
groups over a more continuous population genetic structure defined by S. This is meant to
emulate the fact that environment (specifically lifestyle) may partition among individuals in a
manner distinct from, but highly related to population structure.
We apply this strategy for each of the 11 configurations where each involved up to M = 100, 000
SNPs and N = 5000 individuals. For each configuration, we simulated 100 independent data sets,
thus requiring a total of 1100 fitted models per method of comparison.
D.3 Other Details
For testing, we obtain one p-value for each SNP, which is the probability that the SNP’s effect on the
trait of interest is zero. To do so, we apply a likelihood ratio test statistic for each SNP m,
T(x, y) = 2( max logp(y | x; θ) - max logp(y | x; θ-m,θm = 0))
θ	θ-m
where θm corresponds to weights connecting xm to hidden units in the trait neural networks’ first
layer. When θm = 0 in the model, following Wilks’ theorem, the null distribution of this test
statistic is χ2H where H is the number of first layer hidden units, i.e., the test statistic converges
to this distribution as the number of individuals N → ∞. This statistic and null distribution is a
simple extension of Song et al. (2015), which applies a linear model. (See also their proof for the
null distribution.)2
In summary, the testing procedure works as follows:
1.	Formulate and estimate a model of population structure that provides well-behaved esti-
mates of the posterior p(z | x, y).
2.	First, perform a regression of the trait given SNPs and estimated posterior p(z | x, y). Sec-
ond, for each snp, perform a regression of the trait given snps and estimated posterior
p(z | x, y), but with θm fixed at 0.
3.	Calculate a p-value for each SNP. Use the test statistic above, which follows a χ2H null
distribution for large sample sizes.
2The test only holds approximately for two reasons. First, it assumes asymptotically large sample size (as
all frequentist tests). Second, given a multi-layered neural network, there exist permutations of the weights and
biases beyond the first layer weights which set the effect of xm on y to 0.
17
Under review as a conference paper at ICLR 2018
The p-value threshold is fixed to t = 0.0025 across all methods (Song et al., 2015). To calculate the
number of observed positives, we count the number of p-values for that are less than or equal to t.
The true positives are the subset of p-values associated with causal SNPs; false positives are those
associated with null snps.
Spurious associations occur when p-values corresponding to null SNPs are artificially small. Namely,
false positives are spurious associations. In general, we expect there to be m0 × t false positives
among the m0 p-values corresponding to null SNPs; in our setting, this corresponds to (100, 000 -
10) ∙ 0.0025 ≈ 250 SNPs. A method properly accounts for structure when the average excess is no
more than this number. Our precision count involved only the number of false positives higher than
this calculation (which depends on the number of snps in that setting).
To specify the implicit causal model in our experiments, we set the latent dimension of confounders
equal to 3 or 5. We use 512 units in both hidden layers of the snp neural network and use 32 and
256 units for the trait neural network’s first and second hidden layers respectively.
E	Northern Finland Birth Cohort Data
We provide more detail to § 5.2. The data was obtained from the database of Genotypes and
Phenotypes (dbGaP) (phs000276.v1.p1). We follow the same preprocessing as Song et al.
(2015),
Individuals were filtered for completeness (maximum 1% missing genotypes) and pregnancy.
(Pregnant women were excluded because We did not receive IRB approval for these individuals.)
SNPS were first filtered for completeness (maximum 5% missing genotypes) and minor allele
frequency (minimum 1% minor allele frequency), then tested for Hardy-Weinberg equilibrium
(p-value < 1/328348). The final dimensions of the genotype matrix are m = 324,160 SNPS and
n = 5027 individuals.
A Box-Cox transform was applied to each trait, where the parameter was chosen such that the
values in the median 95% value of the trait was as close to the normal distribution as possible.
Indicators for sex, oral contraception, and fasting status were added as adjustment variables. For
glucose, the individual with the minimum value was removed from the analysis as an extreme
outlier.
No additional changes were made to the data.
After fitting each model, we follow the same procedure as in the simulation study for predicting
causal factors. We set the p-value threshold to be the genome-wide threshold of 7.2 × 10-8 following
Kang et al. (2010).
18