Published as a conference paper at ICLR 2018
Gaussian Process B ehaviour in
Wide Deep Neural Networks
Alexander G. de G. Matthews
University of Cambridge
am554@cam.ac.uk
Jiri Hron
University of Cambridge
jh2084@cam.ac.uk
Mark Rowland
University of Cambridge
mr504@cam.ac.uk
Richard E. Turner
University of Cambridge
ret26@cam.ac.uk
Zoubin Ghahramani
University of Cambridge, Uber AI Labs
zoubin@eng.cam.ac.uk
Ab stract
Whilst deep neural networks have shown great empirical success, there is still
much work to be done to understand their theoretical properties. In this paper, we
study the relationship between Gaussian processes with a recursive kernel defini-
tion and random wide fully connected feedforward networks with more than one
hidden layer. We exhibit limiting procedures under which finite deep networks
will converge in distribution to the corresponding Gaussian process. To evalu-
ate convergence rates empirically, we use maximum mean discrepancy. We then
exhibit situations where existing Bayesian deep networks are close to Gaussian
processes in terms of the key quantities of interest. Any Gaussian process has a
flat representation. Since this behaviour may be undesirable in certain situations
we discuss ways in which it might be prevented. 1
1	Introduction
Deep feedforward neural networks have emerged as an essential component of modern machine
learning. As such there has been significant research effort in trying to understand the theoretical
properties of such models. One important branch of such research is the study of random networks.
By assuming a probability distribution on the network parameters, a distribution is induced on the
input to output function that such networks encode. This has proved important in the study of
initialisation and learning dynamics (Schoenholz et al., 2017) and expressivity (Poole et al., 2016).
It is, of course, essential in the study of Bayesian priors on networks (Neal, 1996). The Bayesian
approach makes little sense if prior assumptions are not understood, and distributional knowledge
can be essential in finding good posterior approximations.
Since we typically want our networks to have high modelling capacity, it is natural to consider limit
distributions of networks as they become large. Whilst distributions on deep networks are generally
challenging to work with exactly, the limiting behaviour can lead to more insight. Further, as we
shall see, networks used in the literature may be very close to this behaviour.
The seminal work in this area is that of Neal (1996), which showed that under certain conditions
random neural networks with one hidden layer converge to a Gaussian process. The question of
the type of convergence is non-trivial and part of our discussion. Historically this result was a
significant one because it provided a connection between flexible Bayesian neural networks and
Gaussian processes (Williams, 1998; Rasmussen & Williams, 2006)
1Code for the experiments in the paper can be found at https://github.com/
widedeepnetworks/widedeepnetworks
1
Published as a conference paper at ICLR 2018
1.1	Our contributions
We extend the theoretical understanding of random fully connected networks and their relationship
to Gaussian processes. In particular, we prove a rigorous result (Theorem 1) on the convergence of
certain finite networks with more than one hidden layer to Gaussian processes.
Further, we empirically study the distance between finite networks and their Gaussian process ana-
logues by using maximum mean discrepancy (Gretton et al., 2012) as a distance measure. We find
that Bayesian deep networks from the literature can exhibit predictions that are close to Gaussian
processes. To demonstrate this, we systematically compare exact Gaussian process inference with
‘gold standard’ MCMC inference for Bayesian neural networks.
Our work is of relevance to the theoretical understanding of neural network initialisation and dynam-
ics. It is also important in the area of Bayesian deep networks because it demonstrates that Gaussian
process behaviour can arise in more situations of practical interest than previously thought. If this
behaviour is desired then Gaussian process inference (exact and approximate) should also be consid-
ered. In some scenarios, the behaviour may not be desired because it implies a lack of a hierarchical
representation. We therefore highlight promising ideas from the literature to prevent such behaviour.
1.2	Related work
The case of random neural networks with one hidden layer was studied by Neal (1996). Cho & Saul
(2009) provided analytic expressions for single layer kernels including those corresponding to a
rectified linear unit (ReLU). They also studied recursive kernels designed to ‘mimic computation in
large, multilayer neural nets’. As discussed in Section 3 they arrived at the correct kernel recursion
through an erroneous argument. Such recursive kernels were later used with empirical success in the
Gaussian process literature (Krauth et al., 2017), with a similar justification to that of Cho and Saul.
The first case we are aware of using a Gaussian process construction with more than one hidden
layer is the work of Hazan & Jaakkola (2015). Their contribution is similar in content to Lemma
1 discussed here, and the work has had increasing interest from the kernel community (Mitrovic
et al., 2017). Recent work from Daniely et al. (2016) uses the concept of ‘computational skeletons’
to give concentration bounds on the difference in the second order moments of large finite networks
and their kernel analogue, with strong assumptions on the inputs. The Gaussian process view given
here, without strong input assumptions, is related but concerns not just the first two moments of a
random network but the full distribution. As such the theorems we obtain are distinct. A less obvious
connection is to the recent series of papers studying deep networks using a mean field approximation
(Poole et al., 2016; Schoenholz et al., 2017). In those papers a second order approximation gives
equivalent behaviour to the kernel recursion. By contrast, in this paper the claim is that the behaviour
emerges as a consequence of increasing width and is therefore something that needs to be proved.
Another surprising connection is to the analysis of self-normalizing neural networks (Klambauer
et al., 2017). In their analysis the authors assume that the hidden layers are wide in order to invoke
the central limit theorem. The premise of the central limit theorem will only hold approximately
in layers after the first one and this theoretical barrier is something we discuss here. An area that
is less related than might be expected is that of ‘Deep Gaussian Processes’ (DGPs) (Damianou &
Lawrence, 2013). As will be discussed in Section 6, narrow intermediate representations mean that
the marginal behaviour is not close to that of a Gaussian process. Duvenaud et al. (2014) offer an
analysis that largely applies to DGPs though they also study the Cho and Saul recursion with the
motivating argument from the original paper.
2	The deep wide limit
We consider a fully connected network as shown in Figure 1. The inputs and outputs will be real
valued vectors of dimension M and L respectively. The network is fully connected. The initial step
and recursion are standard. The initial step is:
M
fi(1)(x) = Xwi(,1j)xj + bi(1) .	(1)
j=1
2
Published as a conference paper at ICLR 2018
Inputs
Activations Activities
11
Activations Activities
22
Output
Figure 1: In this paper we consider fully connected feedforward networks with more than one
hidden layer. We call the pre-nonlinearity an activation and post-nonlinearity an activity. As the
network becomes increasingly wide the distribution of the marginal distributions of the activations
at each layer and of the output will become close to a Gaussian process in a sense described in the
text.
We make the functional dependence on x explicit in our notation as it will help clarify what follows.
For a network with D hidden layers the recursion is, for each μ = 1,...,D,
g(μ)(x) = Φ(f(μ)(x))	(2)
Hμ
fi(μ+I)(X) = X w(μ+1)gjμ)(x) + b(μ+1),	(3)
j=1
so that f (D+1)(x) is the output of the network given input x. φ denotes the non-linearity. In all cases
the equations hold for each value of i; i ranges between 1 and Hμ in Equation (2), and between 1
and Hμ+ι in Equation (3) except in the case of the final activation where the top value is L. The
network could of course be modified to be probability simplex-valued by adding a softmax at the
end.
A distribution on the parameters of the network will be assumed. Conditional on the inputs, this
induces a distribution on the activations and activities. In particular we will assume independent
normal distributions on the weights and biases
w(j)〜N(0,Cμ)) indep	(4)
b(μ) 〜N(0, C((μ)) indep.	(5)
We will be interested in the behaviour of this network as the widths Hμ becomes large. The weight
variances for μ ≥ 2 will be scaled according to the width of the network to avoid a divergence in the
variance of the activities in this limit. As will become apparent, the appropriate scaling is
Cw
Cw
F
(6)
μ ≥ 2 .
The assumption is that Cw\ will remain fixed as we take the limit. Neal (1996) analysed this problem
for D = 1, showing that as H1 → ∞, the values of fi(2)(x), the output of the network in this case,
converge to a certain multi-output Gaussian process if the activities have bounded variance.
Since our approach relies on the multivariate central limit theorem we will arrange the relevant terms
into (column) vectors to make the linear algebra clearer. Consider any two inputs x and x0 and all
3
Published as a conference paper at ICLR 2018
output functions ranging over the index i. We define the vector f (2) (x) of length L whose elements
are the numbers f(2)(x). We define f (2)(χ0) similarly. For the weight matrices defined by Wiμj) for
fixed μ We use a 'placeholder' index • to return column and row vectors from the weight matrices.
In particular wj∙? denotes row j of the weight matrix at depth 1. Similarly, w(,j) denotes column j
at depth 2. The biases are given as column vectors b(1) and b(2). Finally we concatenate the two
vectors f(2) (x) and f(2)(x0) into a single column vector F(2) of size 2L. The vector in question
takes the form
(f ⑵(x) ʌ = (b(2∖ + XX ww(2φ(wj?X + bjI))
V (2)(x0)J ∖b(2)√ M [w(,j)θ(w!?/0 + b(j1))
(7)
The benefit of writing the relation in this form is that the applicability of the multivariate central limit
theorem is immediately apparent. Each of the vector terms on this right hand side is independent
and identically distributed conditional on the inputs x and x0 . By assumption, the activities have
bounded variance. The scaling we have chosen on the variances is precisely that required to ensure
the applicability of the theorem. Therefore as H becomes large F (2) converges in distribution to a
multivariate normal distribution. The limiting normal distribution is fully specified by its first two
moments. Defining Y 〜NaCbI),SN(0, Cw(1)IM), the moments in question are:
E fi(2) (x) = 0	(8)
E [fi(2) (XM2) (x0)i = δi,j [CW2)Ee,γ [φ(eTx + Y)φ(eTx0 + Y)] + *]	(9)
Note that we could have taken a larger set of input points to give a larger vector F and again we
would conclude that this vector converged in distribution to a multivariate normal distribution. More
formally, we can consider the set of possible inputs as an index set. A set of consistent finite dimen-
sional Gaussian distributions on an index set corresponds to a Gaussian process by the Kolmogorov
extension theorem. The Gaussian process in question is a distribution over functions defined on the
product σ-algebra, which has the relevant finite dimensional distributions as its marginals.
In the case of a multivariate normal distribution a set of variables having a covariance of zero implies
that the variables are mutually independent. Looking at Equation (9), we see that the limiting distri-
bution has independence between different components i, j of the output. Combining this with the
recursion (2), we might intuitively suggest that the next layer also converges to a multivariate normal
distribution in the limit of large Hμ. Indeed we state the following lemma, which we attribute to
Hazan & Jaakkola (2015):
Lemma 1 (Normal recursion). If the activations of a previous layer are normally distributed with
moments:
E [fi(μ-1)(x)] =0
EhfyT) (x)fj(μT)(x0)i = δi,j K (x,x0),
(10)
(11)
Then under the recursion (2) and as H → ∞ the activations of the next layer converge in distribution
to a normal distribution with moments
E [fi(μ)(x)] =0	(12)
Ehfy)(X)铲(x0)i =汨[CWμ)E(q,e2)〜N(0,k)[Φ(e1)Φ(e2)]+ Cbμ)i	(13)
where K is a 2 × 2 matrix containing the input covariances.
4
Published as a conference paper at ICLR 2018
Unfortunately the lemma is not sufficient to show that the joint distribution of the activations of
higher layers converge in distribution to a multivariate normals. This is because for finite H the
input activations do not have a multivariate normal distribution - this is only attained (weakly or in
distribution) in the limit. It could be the case that the rate at which the limit distribution is attained
affects the distribution in subsequent layers. We are able to offer the following theorem rigorously:
Theorem 1. Consider a Bayesian deep neural network of the form in Equations (1) and (2) using
ReLU activation functions. Then there exist strictly increasing width functions hμ : N → N such
that H1 = h1(n), . . . , HD = hD (n), and for any countable input set (x[i])i∞=1, the distribution of
the output of the network converges in distribution to a Gaussian process as n → ∞.
A proof is included in the appendix. We conjecture that a more general theorem will hold. In
particular We expect that the width functions hμ can be taken to be the identity and that the non-
linearity can be extended to monotone functions with well behaved tails. Our conjecture is based on
the intuition from Lemma 1 and from our experiments, in which we always take the width function
to be the identity.
3	Specific kernels under recursion
Cho & Saul (2009) suggest a family of kernels based on a recurrence designed to ‘mimic compu-
tation in large, multilayer neural nets’. It is therefore of interest to see how this relates to deep
wide Gaussian processes. A kernel may be associated with a feature mapping Φ(x) such that
K(x, x0) = Φ(x) • Φ(x0). Cho and Saul define a recursive kernel through a new feature map-
ping by compositions such as Φ(Φ(x)). However this cannot be a legitimate way to create a kernel
because such a composition represents a type error. There is no reason to think the output dimension
of the function Φ matches the input dimension and indeed the output dimension may well be infi-
nite. Nevertheless, the paper provides an elegant solution to a different task: it derives closed form
solution to the recursion from Lemma 1 (Hazan & Jaakkola, 2015) for the special case
φ(u) = Θ(u)ur for r = 0,1,2,3,	(14)
where Θ is the Heaviside step function. Specifically, the recursive approach of Cho & Saul (2009)
Can be adapted by using the fact that u>z for Z 〜N(0,LL>) is equivalent in distribution to
(L>u)>ε with ε 〜N(0, I), and by optionally augmenting U to incorporate the bias. Since r = 1
corresponds to rectified linear units we apply this analytic kernel recursion in all of our experiments.
4	Measuring convergence using maximum mean discrepancy
In this section we use the kernel based two sample tests of Gretton et al. (2012) to empirically
measure the similarity of finite random neural networks to their Gaussian process analogues. The
maximum mean discrepancy (MMD) between two distributions P and Q is defined as:
MMD(P, Q, H) := sup	EP [h] - EQ[h]	(15)
l∣h∣∣H≤ι L	」
where H denotes a reproducing kernel Hilbert space and || • ||H denotes the corresponding norm. It
gives the biggest possible difference between expectations of a function under the two distributions
under the constraint that the function has Hilbert space norm less than or equal to one. We used the
unbiased estimator of squared MMD given in Equation (3) of Gretton et al. (2012).
In this experiment and all those that follow we take weight variance parameters Cwμ) = 0.8 and bias
variance Cb = 0.2. We took 10 standard normal input points in 4 dimensions and pass them through
2000 independent random neural networks drawn from the distribution discussed in this paper. This
was then compared to 2000 samples drawn from the corresponding Gaussian process distribution.
The experiment was performed with different numbers of hidden layers and numbers of units per
hidden layer. We repeated each experiment 20 times which allows us to reduce variance in our results
and give a simple estimate of measurement error. The experiments use an RBF kernel for the MMD
5
Published as a conference paper at ICLR 2018
Number of hidden units per layer
Figure 2: A comparison of finite random neural networks to their corresponding Gaussian process
analogue using an (RBF) kernel estimator of the squared maximum mean discrepancy (MMD). The
results are consistent with the emergence of Gaussian process behaviour as the networks become
wide. The red dashed line is for calibration and denotes the squared MMD between two Gaussian
processes with isotropic RBF kernels and length scales l and 2l where l = √8 is the characteristic
length scale of the input space (see text).
estimate with lengthscale 1/2. In order to help give an intuitive sense of the distances involved
we also include a comparison between two Gaussian processes with isotropic RBF kernels using
the same MMD distance measure. The kernel length scales for this pair of ‘calibration, Gaussian
processes are taken to be l and 2l, where the characteristic length scale l = √8 is chosen to be
sensible for the standard Normal input distribution on the four dimensional space.
The results of the experiment are shown in Figure 2. We see that for each fixed depth the network
converges towards the corresponding Gaussian process as the width increases. For the same number
of hidden units per layer, the MMD distance between the networks and their Gaussian process
analogue becomes higher as depth increases. The rate of convergence to the Gaussian process is
slower as the number of hidden layers is increased.
5	Comparing Bayesian Deep Networks to Gaus s ian Processes
In this section we compare the behaviour of finite Bayesian deep networks of the form considered
in this paper with their Gaussian process analogues. If we make the networks wide enough the
agreement will be very close. It is also of interest, however, to consider the behaviour of networks
actually used in the literature, so we use 3 hidden layers and 50 hidden units which is typical of the
networks used by Hernandez-Lobato & Adams (2015). Fully connected Bayesian deep networks
with finite variance priors on the weights have also been considered in other works (Graves, 2011;
Hernandez-Lobato et al., 2016; Blundell et al., 2015), though the specific details vary. We use rec-
tified linear units and correct the variances to avoid a loss of prior variance as depth is increased.
Our general strategy was to compare exact Gaussian process inference against expensive ‘gold stan-
dard’ Markov Chain Monte Carlo (MCMC) methods. We choose the latter because used correctly it
works well enough to largely remove questions of posterior approximation quality from the calculus
of comparison. It does mean however that our empirical study does not extend to datasets which
are large in terms of number of data points or dimensionality, where such inference is challenging.
We therefore sound a note of caution about extrapolating our empirical finite network conclusions
too confidently to this domain. On the other hand, lower dimensional, prior dominated problems are
generally regarded as an area of strength for Bayesian approaches and in this context our results are
directly relevant.
We computed the posterior moments by the two different methods on some example datasets. For the
MCMC we used Hamiltonian Monte Carlo (HMC) (Neal, 2010) updates interleaved with elliptical
6
Published as a conference paper at ICLR 2018
slice sampling (Murray et al., 2010). We considered a simple one dimensional problem and a two
dimensional real valued embedding of the four data point XOR problem. We see in Figures 3 and 4
(left) that the agreement in the posterior moments between the Gaussian process and the Bayesian
deep network is very close.
A key quantity of interest in Bayesian machine learning is the marginal likelihood. It is the normal-
ising constant of the posterior distribution and gives a measure of the model fit to the data. For a
Bayesian neural network, it is generally very difficult to compute, but with care and computational
time it can be approximated using Hamiltonian annealed importance sampling (Sohl-Dickstein &
Culpepper, 2012). The log-importance weights attained in this way constitute a stochastic lower
bound on the marginal likelihood (Grosse et al., 2015). Figure 4 (right) shows the result of such an
experiment compared against the (extremely cheap) Gaussian process marginal likelihood compu-
tation on the XOR problem. The value of the log-marginal likelihood computed in the two different
ways agree to within a single nat which is negligible from a model selection perspective (Grosse
et al., 2015).
Predictive log-likelihood is a measure of the quality of probabilistic predictions given by a Bayesian
regression method on a test point. To compare the two models we sampled 10 standard normal train
and test points in 4 dimensions and passed them through a random network of the type under study
to get regression targets. We then discarded the true network parameters and compared the predic-
tions of posterior inference between the two methods. We also compared the marginal predictive
distributions of a latent function value. Figure 5 shows the results. We see that the correspondence
in predictive log-likelihood is close but not exact. Similarly the marginal function values are close
to those of a Gaussian process but are slightly more concentrated.
Figure 3: A comparison between Bayesian posterior inference in a Bayesian deep neural network
and posterior inference in the analogous Gaussian process. The neural network has 3 hidden layers
and 50 units per layer. The lines show the posterior mean and two σ credible intervals.
Log marginal likelihood
2	1 O -1-2	2	1 O -1	-2
Figure 4: A comparison between posterior inference for a Gaussian process and a Bayesian deep
network for a real value embedding of the XOR function. Left and centre: The two posterior means.
The mean absolute different between the two posterior estimate grids is 0.027. Right: Kernel den-
sity estimate of the log weights from annealed importance sampling on a Bayesian deep network
compared to the analogous Gaussian process marginal likelihood shown by the vertical line. The
neural network has 3 hidden layers and 50 units per layer.
7
Published as a conference paper at ICLR 2018
3 一u,ulup 6o-*.IOM4luu -e.lnluN
Figure 5: A comparison of the predictive distributions of a Bayesian deep network and a Gaussian
process on a randomly generated test case. Left: the per point log-densities of the two models.
Right: a randomly selected predictive marginal distribution for the latent function on a randomly
selected test point.
X4-u,uluα
Gaussian process log density
0.0
-3	-2	-1	0	1
Function value
6	Avoiding Gaussian process behaviour
When using deep Bayesian neural networks as priors, the emergence of Gaussian priors raises im-
portant questions in the cases where itis applicable, even if one sets aside questions of computational
tractability. It has been argued in the literature that there are important cases where kernel machines
with local kernels will perform badly (Bengio et al., 2005). The analysis applies to the posterior
mean of a Gaussian process. The emergent kernels in our case are hyperparameter free. Although
they do not meet the strict definition of what could be considered ‘local’ the fact remains that any
Gaussian process with a fixed kernel does not use a learnt hierarchical representation. Such rep-
resentations are widely regarded to be essential to the success of deep learning. There is relevant
literature here on learning the representation of a standard, usually structured, network composed
with a Gaussian process (Wilson et al., 2016a;b; Al-Shedivat et al., 2017). This differs from the
assumed paradigm of this paper, where all model complexity is specified probabilistically and we
do not assume convolutional, recurrent or other problem specific structure.
Within this paradigm, the question therefore arises as to what can be done to avoid marginal Gaus-
sian process behaviour if it is not desired. Speaking loosely, to stop the onset of the central limit
theorem and the approximate analogues discussed in this paper one needs to make sure that one
or more of its conditions is far from being met. Since the chief conditions on the summands are
independence, bounded variance and many terms, violating these assumptions will remove Gaus-
sian process behaviour. Deep Gaussian processes (Damianou & Lawrence, 2013) are not close to
standard Gaussian processes marginally because they are typically used with narrow intermediate
layers. It can be challenging to choose the precise nature of these narrow layers a priori. Neal
(1996) suggests using networks with infinite variance in the activities. With a single hidden layer
and correctly scaled, these networks become alpha stable processes in the wide limit. Neal also
discusses variants that destroy independence by coupling weights. Our results about the emergence
of Gaussian processes even with more than one hidden layer mean these ideas are of considerable
interest going forward.
7	Conclusions
Studying the limiting behaviour of distributions on feedforward networks has been a fruitful avenue
for understanding these models historically. In this paper we have extended the state of knowledge
about the wide limit, including for networks with more than one hidden layer. In particular, we
have exhibited limit sequences of networks that converge in distribution to Gaussian processes with
a certain recursively defined kernel. Our empirical study using MMD suggests that this behaviour
8
Published as a conference paper at ICLR 2018
is exhibited in a variety of models of size comparable to networks used in the literature. This led
us to juxtapose finite Bayesian neural networks with their Gaussian process analogues, finding that
the agreement in terms of key predictors is close empirically. If this Gaussian process behaviour
is desired then exact and approximate inference using the analytic properties of Gaussian processes
should be considered as an alternative to neural network inference. Since Gaussian processes have
an equivalent flat representation then in the context of deep learning the behaviour may well not be
desired and steps should be taken to avoid it.
We view these results as a new opportunity to further the understanding of neural networks in the
work that follows. Initialisation and learning dynamics are crucial topics of study in modern deep
learning which require that we understand random networks. Bayesian neural networks should offer
a principled approach to generalisation but this relies on successfully approximating a clearly un-
derstood prior. In illustrating the continued importance of Gaussian processes as limit distributions,
we hope that our results will further research in these broader areas.
8	Acknowledgements
We wish to thank Neil Lawrence for helpful conversations. We also thank the anonymous reviewers
for their insights. Alexander Matthews and Zoubin Ghahramani acknowledge the support of EPSRC
Grant EP/N014162/1 and EPSRC Grant EP/N510129/1 (The Alan Turing Institute). Jiri Hron holds
a Nokia CASE Studentship. Mark Rowland acknowledges support by EPSRC grant EP/L016516/1
for the Cambridge Centre for Analysis. Richard E. Turner is supported by Google as well as EPSRC
grants EP/M0269571 and EP/L000776/1.
References
Maruan Al-Shedivat, Andrew G. Wilson, Yunus Saatchi, Zhiting Hu, and Eric P. Xing. Learning
Scalable Deep Kernels with Recurrent Structure. Journal of Machine Learning Research (JMLR),
2017.
Yoshua Bengio, Olivier Delalleau, and Nicolas Le Roux. The Curse of Dimensionality for
Local Kernel Machines. Technical Report 1258, DePartement d'informatique et recherche
operationnelle, Universite de Montreal, 2005.
Vidmantas K. Bentkus. On the Dependence of the Berry-Esseen bound on Dimension. Journal of
Statistical Planning and Inference, 2003.
Patrick Billingsley. Convergence of Probability Measures. John Wiley & Sons Inc., Second edition,
1999.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight Uncertainty in
Neural Networks. International Conference on Machine Learning (ICML), 2015.
Youngmin Cho and Lawrence K. Saul. Kernel Methods for Deep Learning. Advances in Neural
Information Processing Systems (NIPS), 2009.
Andreas C. Damianou and Neil D. Lawrence. Deep Gaussian Processes. International Conference
on Artificial Intelligence and Statistics (AISTATS), 2013.
Amit Daniely, Roy Frostig, and Yoram Singer. Toward Deeper Understanding of Neural Networks:
The Power of Initialization and a Dual View on Expressivity. Advances in Neural Information
Processing Systems (NIPS), 2016.
David Duvenaud, Oren Rippel, Ryan P. Adams, and Zoubin Ghahramani. Avoiding Pathologies in
very Deep Networks. International Conference on Artificial Intelligence and Statistics (AISTATS),
2014.
Alex Graves. Practical Variational Inference for Neural Networks. Advances in Neural Information
Processing Systems (NIPS), 2011.
Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Scholkopf, and Alexander Smola.
A Kernel Two-sample test. Journal of Machine Learning Research (JMLR), 2012.
9
Published as a conference paper at ICLR 2018
Roger B. Grosse, Zoubin Ghahramani, and Ryan P. Adams. Sandwiching the marginal likelihood
using bidirectional Monte Carlo. ArXiv e-prints, November 2015.
Tamir Hazan and Tommi Jaakkola. Steps Toward Deep Kernel Methods from Infinite Neural Net-
works. ArXiv e-prints, August 2015.
Jose M. Hemandez-Lobato and Ryan P. Adams. Probabilistic BackProPagation for Scalable Learn-
ing of Bayesian Neural Networks. International Conference on Machine Learning (ICML), 2015.
Jose M. Hernandez-Lobato, Yingzhen Li, Mark Rowland, Thang Bui, Daniel Hernandez-Lobato,
and Richard E. Turner. Black-box alPha divergence minimization. International Conference on
Machine Learning (ICML), 2016.
Gunter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-Normalizing
Neural Networks. CoRR, abs/1706.02515, 2017.
Karl Krauth, Edwin V. Bonilla, Kurt Cutajar, and Maurizio Filippone. AutoGP: Exploring the
capabilities and limitations of Gaussian Process models. Conference on Uncertainty in Artificial
Intelligence (UAI), 2017.
Jovana Mitrovic, Dino Sejdinovic, and Yee Whye Teh. Deep Kernel Machines via the Kernel
Reparametrization Trick. In International Conference on Learning Representations (ICLR) Work-
shop Track, 2017.
Iain Murray, Ryan P. Adams, and David J. C. MacKay. Elliptical Slice Sampling. International
Conference on Artificial Intelligence and Statistics (AISTATS), 2010.
Radford M. Neal. Bayesian Learning for Neural Networks. Springer, 1996.
Radford M. Neal. MCMC using Hamiltonian Dynamics. Handbook of Markov Chain Monte Carlo,
2010.
Ben Poole, Subhaneil Lahiri, Maithreyi Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Ex-
ponential expressivity in Deep Neural Networks through Transient Chaos. Advances in Neural
Information Processing Systems (NIPS), 2016.
Carl E. Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning. The
MIT Press, 2006.
Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep Information
Propagation. International Conference on Learning Representations (ICLR), 2017.
Jascha Sohl-Dickstein and Benjamin J. Culpepper. Hamiltonian Annealed Importance Sampling for
partition function estimation. CoRR, abs/1205.1925, 2012.
Christopher K. I. Williams. Computing with Infinite Networks. Advances in Neural Information
Processing Systems (NIPS), 1998.
Andrew G. Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P. Xing. Deep Kernel Learning.
International Conference on Artificial Intelligence and Statistics (AISTATS), 2016a.
Andrew G. Wilson, Zhiting Hu, Ruslan R. Salakhutdinov, and Eric P. Xing. Stochastic Variational
Deep Kernel Learning. Advances in Neural Information Processing Systems (NIPS), 2016b.
10
Published as a conference paper at ICLR 2018
A Proof of Main Theorem
A. 1 S tatement of theorem and notation
In this section, we provide a proof of the main theorem of the paper, which we begin by recalling.
Theorem 1. Consider a Bayesian deep neural network of the form in Equations (1) and (2) using
ReLU activation functions. Then there exist strictly increasing width functions hμ : N → N such
that H1 = h1(n), . . . , HD = hD (n), and for any countable input set (x[i])i∞=1, the distribution of
the output of the network converges in distribution to a Gaussian process as n → ∞.
The theorem is proven via use of the propositions that follow below. The broad structure of the proof
is to use a particular variant of the Berry-Esseen inequality to upper bound how far each layer is from
a multivariate normal distribution, and then to inductively propagate these inequalities through the
network, leading to a bound on the distance between the output of the network for a collection of
input points, and a multivariate Gaussian distribution. These notions will be made precise below.
We begin in Section A.2 by stating the propositions that will be used in the proof of Theorem 1, but
first establish notation that will be used in the remainder of the appendix.
Given a finite set of inputs x[1], . . . , x[n] ∈ RM, we will write:
•	f (μ)(x) for the random variables (f (μ)(x[i]))n=ι collectively taking values in RnHμ;
•	fjμ(x) for the random variables (f(μ)(x[i]))n=ι collectively taking values in Rn;
•	g(μ)(x) for the random variables (g(μ)(χ[i]))n=ι collectively taking values in RnHμ;
•	gj")(x) for the random variables (gj")(x[i]))n=ι collectively taking values in Rn;
Throughout, if U1 , U2 are random variables taking in values in some Euclidean space Rd , we will
define
d(U1,U2)= sup ∣P(Uι ∈ A) - P(U2 ∈ A)| .
A⊆Rd
A convex
Note that convergence of a sequence of random variables in this metric implies convergence in
distribution.
We will also consider multivariate normal distributions (Zjμ) (x[i])∣j = 1,...,Hμ , i = 1,...,n)
with covariance matrices of block diagonal form, such that
Cov(Z(μ)(x[a]), Z(μ)(x[b])) = 0 for distinct k,l ∈ {1,..., Hμ} , for all x[a],x[b].
To avoid writing this in full every time it is required, we will refer to this condition as blockwise
independence with respect to the index j . We will avoid specification of all covariance values,
deferring to the expression (13) given in the main paper. Finally, to simplify notation, we will
assume that the network output is one-dimensional. Our proof trivially extends to arbitrary finite
output dimension where the limiting distribution is a coordinate-wise independent multivariate GP.
A.2 Supporting results
Proposition 1. Let ε > 0, and x[1],..., x[n] ∈ RM. Let μ ∈ {2,...,D + 1}, and let Hk = 2Hk+1
for k = 1, . . . , D - 1. Then for HD sufficiently large, suppose the condition
d(f (μ-D(x), Z(μ-D(x)) ≤ 2-((D+1)-WT))-nPD=μ-ι Hkε,
holds, where Z(μT)(X) = (Z(μ-1)(x[i])∣j = 1,..., Hμ-ι, i = 1,...,n) is mean-zero multivari-
ate normal, with blockwise independence with respect to the index j. Then we have
d(f(μ)(x),Z(μ)(x)) ≤ 2-((D+1)-μ)-nPD=μHkε,
where Z(μ)(x) = (Zj*)(x[i])∣j = 1,..., Hμ , i = 1,...,n) is mean-zero multivariate normal,
with blockwise independence with respect to the index j .
11
Published as a conference paper at ICLR 2018
Proposition 2. Let ε > 0, and x[1], . . . , x[n] ∈ RM. If Hk = 2Hk2+1 for k = 1, . . . , D - 1, then for
HD sufficiently large, we have
d(f(D+1)(x), Z(x)) ≤ε,
where Z(x) is a mean-zero multivariate normal random variable.
In establishing the two propositions above, the following three lemmas will be useful.
Lemma 2. Let ε > 0, and let Z (μ-1)(x) = (zjμ-1)(χ[i])∣j = 1,...,Hμ-ι, i = 1,...,n)
be mean-zero multivariate normal, with blockwise independence with respect to the index j . Let
e(μ-1)(x) = φ(Z(μ-1)(χ)), and let f(μ)(x) be given by
Hμ-1
f(μ)(χ[i]) = X w(,μ)ejμτ)(χ[i]) + b(μ),
j=1
for i = 1, . . . , n. Then given ε > 0, if Hk = 2Hk2+1 for k = 1, . . . , D - 1, then for all sufficiently
large HD we have:
d(f(μ)(x),Z(μ)(x)) ≤ 2-((D+DTμT))-nPD=μ Hkε,
where Z (μ)(x) = (Zj*)(x[i])∣j = 1,...,Hμ , i = 1,...,n) is mean-zero multivariate normal,
with blockwise independence with respect to the index j .
Lemma 3. Let Z(μT)(X) = (Z(μ-1)(x[i])∣j = 1,..., Hμ-ι, 1,...,n) be mean-zero multivariate
normal, with blockwise independence with respect to the index j , such that for some ε > 0,
d(Z(μT)(X),f(μT)(X)) ≤ ε.
Then, defining f (μ)(x) by
〜	Hμ
f(μ)(x[i]) = fw.,j φ(zjμT)(x[i])) + b(μ),
j=1
in the particular case where φ is the elementwise ReLU function, we have
d(f(μ)(x),f(μ)(x)) ≤ 2nHμ-1 ε.
Lemma 4. Let Xi,..., XHμ-ι be iid random variables of the form Xj = 犷T)(X) 0 W(,μ),
where 0 denotes the Kronecker product, ej-μ-1)(x) is defined as in Lemma 2, and W* is a
multivariate normal variable taking values in RHμ with mean vector 0, and covariance Cwf) I.
We denote the variance of Xj by Σ蜜 and its Schur decomposition as Σ& = Q0Λ0Q0. Then
β = E 卜3人”以Xjk3] ≤ CHμ,n, WhereCHμ,n
dent of Hμ-ι. Further, we have C%,n = O(Hμ2n2).
∈ R depends on Hμ and n, but is indepen-
A.3 Proofs
Proof of Lemma 2. We use a straightforward variant of a particular Berry-Esseen inequality de-
scribed in Bentkus (2003). We first state this result from the literature, and then derive a straightfor-
ward variation that we will use in the sequel.
Theorem 2 (From Bentkus (2003)). Let X1, . . . , Xn be iid random variables taking values in Rd,
with mean vector 0, identity covariance matrix, and β = E [∣∣Xik3] < ∞. Let Sn = √n 5∑n=ι Xi,
and let Y be a standard d-dimensional multivariate normal random vector. Then we have
SUp ∣P(Sn ∈ A)- P(Y ∈ A) | ≤ 40√4β
A⊆Rd	n
A convex
12
Published as a conference paper at ICLR 2018
We need a mildly modified version of this theorem to deal with iid random vectors X1 , . . . , Xn with
non-identity covariance matrices. To this end, suppose that Σ is the (full-rank) covariance matrix
of each Xi, with decomposition Σ = RR>, for some invertible matrix R; R can be obtained, for
example, by using Cholesky or Schur decomposition. The random variables R-1X1, . . . , R-1Xn
are then iid, mean zero and with identity covariance matrices, so we may apply Theorem 2 to obtain:
SUp ∣P(R-1Sn ∈ A) - P(Y ∈ A)| ≤ 400√Tβ ,
A⊆Rd	n
A convex
where β = E kR-1Xi k3 . Now note that this is equivalent to
SUp ∣P(Sn ∈ RA) - P(RY ∈ RA)| ≤ 400√^4β ,
A⊆Rd	n
A convex
noting that RY 〜N(0, Σ).
Since R is invertible, and recalling the definition of the distance d above, this is exactly equivalent
to:
a® RY)≤ 400√n4β
(16)
which is the variant of Bentkus’ result we will require in the sequel.
We apply this bound to the sum
Hμ-1
X ejμT)(X) 0 w(,μ)
j=1
Noting that the summands indexed by j are iid by assumption, with the expected third moment norm
featuring in the Berry-Esseen inequality upper-bounded by β ≤ CHμ,n, for some constant C/ ,n
depending on Hμ and n, but independent of Hμ-ι (finiteness of CHμ,n follows from Lemma 4).
As a consequence, we have the following bound:
d ( X ej"T)(X) 0 w(,μ),Z0(x)) ≤ 400CH”,n(nHμ)”∕√H-;,
where Z0(x) = (Zj(x[i])∣j = 1,...,Hμ , i = 1,...,n) is mean-zero multivariate normal, with
blockwise independence with respect to the index j . We wish to demonstrate that this is less than or
equal to 2-(D-(μ-1))-n PD=μ Hkε when HD is sufficiently large. This is equivalent to showing that
400CH”,n(nHμ)1∕42((D+1)-(μT))+npD=" Hk∕PH-1 ≤ ε
for all sufficiently large HD. But note that with Hk-ι = 2Hk for k = μ,...,D — 1, the left-hand
side converges to 0 as HD increases (using the bound obtained for CHμ,n in Lemma 4), so for all
HD sufficiently large, we obtain
d (XI ej"T)(X) 0 w(μj),Z 0(x)j ≤ 2-((D+1)-(“T))-n PD=μ Hk ε,
as required.
Adding the independent bias vector b(μ) immediately yields
d (ln 0 b(μ) + X ejμT)(X) 0 w(,μ),Z (x)j ≤ 2-((D+1)-("T))-n PD=“ Hk ε,
where Z(x) is mean-zero multivariate normal, with the same block-diagonal covariance structure as
described for Z0(x) above, and 1n ∈ Rn is a vector of 1's.	□
13
Published as a conference paper at ICLR 2018
ProofofLemma 3. Let A ⊆ RnHμ be an arbitrary convex set. First, observe that We have
P (小乳 b(μ) + X φ(fe(μT)(X))㊈ w(,μ) ∈ Aj
=E P (卜㊈ b(μ) + E φ(fjμ-1)(x)) 0 w(,;)j ∈ Aw⑷,b(μ)jj
=E P ((X。(欧T)(X)) 0 w(μ)j ∈ A — 1n 0 b(μ) w(μ) ,b(μ)jj	(17)
Now, note that for fixed w(μ) and b(μ), the event
{(X φ(f?T)(X)) 0 w(,μ) ∈ A — 1n 0 b(μ) w(μ),b(μ)
is exactly that the vector φ(f(μT)(X)) lies in the preimage of the convex set A 一 1n 0 b(μ) under
the linear map w(μ), which is again a convex set. Secondly, observe that for the specific ReLU non-
linearity φ, if C is an arbitrary convex set, then {(f (μ-1)(x)∣φ(f (μ-1)(x)) ∈ C} may be written as
the disjoint union of at most 2nHμ-1 convex sets:
{f(μ-D(x)IΦ(f(μ-D(x)) ∈ C}
=(φ-1(C) ∩{t ∈ RnHμ-1∣ti ≥ 0 ∀i})∪
[	{t	∈ RnHμ-1	∣tι	< 0,	∃y	∈ C s.t.	y/c	= t/c ,yι =	0}.
I⊆{1,...,nHμ-ι}
I=Q
Applying the assumed bound in the statement of the lemma to each of these sets, we obtain
∣P(φ(f (μ-D(x)) ∈ C) - P(φ(fe(μT)(X)) ∈ C)| ≤ 2nHμ-1 ε.
Substituting this bound into the conditional probability (17) yields
|P(f (μ)(x) ∈ A) 一 P(fe(μ)(x) ∈ A)| ≤ 2nHμ-1 ε.
Since A was an arbitrary convex set, the proof is complete.	□
ProofofLemma 4. Note that by independence of ej*-1)(x) from w(j) we have that each Xj has
mean zero and covariance Σ0 = Σ 0 Cw)I where Σ is the covariance matrix of gj*-1)(x). By
standard properties of the Kronecker product, the Schur decomposition of Σ0 is (QΛQt) 0 (Cw)I)
where QΛQT is the Schur decomposition of Σ. Simple algebraic manipulation yields:
E [l3Λ-1∕以(犷T)(X) 0 w*)k3] = E [k(QΛ-1∕2Qτ7j“τ)(x)) 0 (。俨尸/利泞户]
=E hkQΛT∕2QT7j“τ)(x)k3i E [k(C俨)T∕2w.(μ)k3i .
Notice that the random variable (CW) -1∕2w(μ) follows the RHμ -dimensional standard normal dis-
tribution, and thus its squared norm follows the chi-squared distribution with Hμ degrees of freedom,
which is also known as the Gamma(Hμ∕2,1/2) distribution. Exponentiating to the power of 3/2
and taking the expectation, we obtain:
E卜伍8厂⑺觉）k3]
23∕2 Γ((Hμ + 3)/2)
Γ(Hμ∕2)
14
Published as a conference paper at ICLR 2018
Finally, kQA-1/2QTgj-μ-1)(x)k3 ≤ kej*T)(X)113/13/2 where λmin is the smallest value on the di-
agonal of Λ. If the activation φ does not increase the norm of the input vector (as is the case for
rectified linear), we have ∣∣e(μ-1)(x)k3 ≤ kzj*-1)(x)k3 almost surely, where zj*-1)(x) follows
the known n-dimensional normal distribution with mean zero and covariance matrix whose Schur
decomposition will be denoted as UΨUT. Using standard Gaussian identities, we can write
E [kzjμT)(x)k3i = E h∣UΨ14k3i = E [kΨ14k3i ≤ 23/2@maXr((Γ(+∕3))∕2) ,
where ε 〜N(0, In) and ψmax is the highest entry on the diagonal of Ψ. Putting it all together, We
arrive at the desired upper bound C/n
EhIg …Q T X ∣∣3i < (λ ψmax Y" γ((H + 3)∕2H((n + 3)/2)
ElkQA Mk ]≤(4r(Hμ∕2)--Tnk.
Because ψmax and λmi∩ are derived from the distribution of the limiting variable e(μ-1)(x) =
φ(zj*-1)(x)), which only depends on μ, the bound only depends on Hμ and n as desired. Fur-
ther, noting that Γ((x + 3)∕2)∕Γ(x∕2) = O(x2), we have that CHμ,n = O(Hμ2n2), as required. □
Proof of Proposition 1. We first apply Lemma 3 to the assumed inequality
d(f (μτ)(x), ZWT)(X)) ≤ 2T(D+DTμT))-nPD=μ-ι Hkε,
to obtain
d(f W)(X),fW)(X)) ≤ 2-((D+I)T*T))-n Pk=μ Hkε .
We apply Lemma 2 so that for HD sufficiently large, we have
d(f(μ)(x),Z(μ)(x)) ≤ 2-((D+1)-("T))-nPD=μ Hkε.
Applying the triangle inequality then yields
d(f (μ)(x),Z(μ)(x)) ≤ 2-((D+1)-μ)-n PD=μ Hkε,
as required.	□
Proof of Proposition 2. The idea of the proof is to chain Proposition 1 together across the layers
of the network. We fix ε > 0, and apply Proposition 1 to each layer of the network, yielding the
following set of implications for HD sufficiently large:
d(f (μ-1)(x),Z("T)(X)) ≤ 2-((D+1)-("T))-nPD=μ-ι Hkε
=⇒ d(f (μ)(x),Z(μ)(x)) ≤ 2-((D+1)-μ)-npD=μ Hkε,
for μ ∈ {2,...,D + 1}. Finally, note that from the definition of the network, the distri-
bution of f(1) (x) is exactly multivariate normal with the required covariance structure, so that
d(f (I)(X),Z(I)(X)) = 0, completing the proof.	□
Proof of Theorem 1. To prove that (f(D+1)(x[i]))i∞=1 converges weakly to a Gaussian process with
respect to the metric ρ on RN given by:
∞
ρ(v, v0) = X 2-i min(1, |vi - vi0 |)	∀v, v0 ∈ RN ,
i=1
it is sufficient (Billingsley, 1999, p. 19) to prove weak convergence of the finite-dimensional
marginals of the process to multivariate Gaussian random variables, with covariance matrix match-
ing that specified by the kernel of the proposed Gaussian process.
To this end, let I be a finite subset of N, and consider the inputs (x[i])i∈I. We may now apply
Proposition 2 to obtain weak convergence of the joint distribution of the output variables of the
network, (f (D+1) (x[i]))i∈I to a multivariate Gaussian with the correct covariance matrix. As the
finite subset of inputs was arbitrary, we are done.	□
15