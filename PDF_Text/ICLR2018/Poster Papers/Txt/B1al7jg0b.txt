Published as a conference paper at ICLR 2018
Overcoming Catastrophic Interference using
Conceptor-Aided Backpropagation
Xu He, Herbert Jaeger
Department of Computer Science and Electrical Engineering
Jacobs University Bremen
Bremen, 28759, Germany
{x.he,h.jaeger}@jacobs-university.de
Ab stract
Catastrophic interference has been a major roadblock in the research of con-
tinual learning. Here we propose a variant of the back-propagation algorithm,
“conceptor-aided backprop” (CAB), in which gradients are shielded by concep-
tors against degradation of previously learned tasks. Conceptors have their ori-
gin in reservoir computing, where they have been previously shown to overcome
catastrophic forgetting. CAB extends these results to deep feedforward networks.
On the disjoint and permuted MNIST tasks, CAB outperforms two other methods
for coping with catastrophic interference that have recently been proposed.
1	Introduction
Agents with general artificial intelligence are supposed to learn and perform well on multiple tasks.
Continual learning refers to the scenarios where a machine learning system can retain previously
acquired skills while learning new ones. However, when trained on a sequence of tasks, neural
networks usually forget about previous tasks after their weights are adjusted for a new task. This
notorious problem known as catastrophic interference (CI) (McCloskey & Cohen, 1989; Ratcliff,
1990; French, 1999; Kumaran et al., 2016) poses a serious challenge towards continual learning.
Many approaches have been proposed to overcome or mitigate the problem of CI in the last three
decades (Hinton & Plaut, 1987; French, 1991; Ans & Rousset, 1997; French, 1997; Srivastava et al.,
2014). Especially recently, an avalanche of new methods in the deep learning field has brought
about dramatic improvements in continual learning in neural networks. Kirkpatrick et al. (2017)
introduced a regularization-based method called elastic weight consolidation (EWC), which uses
the posterior distribution of parameters for the old tasks as a prior for the new task. They approx-
imated the posterior by a Gaussian distribution with the parameters for old tasks as the mean and
the inverse diagonal of the Fisher information matrix as the variance. Lee et al. (2017) introduced
two incremental moment matching (IMM) methods called mean-IMM and mode-IMM. Mean-IMM
approximates the distribution of parameters for both old and new tasks by a Gaussian distribution,
which is estimated by minimizing its KL-divergence from the mixture of two Gaussian posteriors,
one for the old task and the other one for the new task. Mode-IMM estimates the mode of this
mixture of two Gaussians and uses it as the optimal parameters for both tasks.
In the field of Reservoir Computing (Jaeger, 2001; Maass et al., 2002), an effective solution to CI
using conceptors was proposed by Jaeger (2014) to incrementally train a recurrent neural network
to generate spatial-temporal signals. Conceptors are a general-purpose neuro-computational mecha-
nism that can be used in a diversity of neural information processing tasks including temporal pattern
classification, one-shot learning, human motion pattern generation, de-noising and signal separation
(Jaeger, 2017). In this paper, we adopt and extend the method introduced in Jaeger (2014) and
propose a conceptor-aided backpropagation (CAB) algorithm to train feed-forward networks. For
each layer of a network, CAB computes a conceptor to characterize the linear subspace spanned by
the neural activations in that layer that have appeared in already learned tasks. When the network
is trained on a new task, CAB uses the conceptor to adjust the gradients given by backpropagation
so that the linear transformation restricted to the characterized subspace will be preserved after the
1
Published as a conference paper at ICLR 2018
gradient descent procedure. Experiment results of two benchmark tests showed highly competitive
performance of CAB.
The rest of this paper is structured as follows. Section 2 introduces conceptors and their application
to incremental learning by ridge regression. Section 3 extends the method to stochastic gradient
descent and describes the CAB algorithm. Section 4 compares its performance on the permuted and
disjoint MNIST tasks to recent methods that address the same problem. Finally we conclude our
paper in Section 5.
2	Incremental Ridge Regression by Conceptors
This section reviews the basics of conceptor theory and its application to incrementally training
linear readouts of recurrent neural networks as used in reservoir computing. A comprehensive treat-
ment can be found in (Jaeger, 2014).
2.1	Conceptors
Figure 1: 3D point clouds (black dots) and their corresponding conceptors, represented by ellipsoids
whose axes are the singular vectors of conceptors and the lengths of these axes match the singular
values of conceptors. Each edge of the plot boxes range from -1 to +1 admitted by neural dynamics
with a tanh nonlinearity; conceptor ellipsiods lie inside the unit sphere.
In brief, a matrix conceptor C for some vector-valued random variable x ∈ RN is defined as a linear
transformation that minimizes the following loss function.
Ex |||x- Cx||2 ]+ α-2 ||C ||^	⑴
where α is a control parameter called aperture and || ∙ ∣∣fro is the Frobenius norm. This optimization
problem has a closed-form solution
C = R(R + α-2I)-1	(2)
where R = Ex [xx>] is the N × N correlation matrix of x, and I is the N × N identity matrix. This
result given in (2) can be understood by studying the singular value decomposition (SVD) of C. If
R = UΣU> is the SVD of R, then the SVD of C is given as USU>, where the singular values
Si of C can be written in terms of the singular values σi of R: Si = σi/(σi + α-2) ∈ [0,1). In
intuitive terms, C is a soft projection matrix on the linear subspace where the samples of x lie. For
a vector y in this subspace, C acts like the identity: Cy ≈ y, and when some noise orthogonal to
the subspace is added to y, C de-noises: C(y + ) ≈ y. Figure 1 shows the ellipsoids corresponding
to three sets ofR3 points. We define the quota Q(C) of a conceptor to be the mean singular values:
Q(C) := N PN=I si. Intuitively, the quota measures the fraction of the total dimensions of the
entire vector space that is claimed by C .
Moreover, logic operations that satisfy most laws of Boolean logic can be defined on matrix concep-
tors as the following:
C :=I-C,	(3)
Ci ∨ Cj :=(Ri + Rj)(Ri + Rj + α-2I)-1	(4)
Ci ∧ Cj :=」(-Ci ∨-Cj)	(5)
2
Published as a conference paper at ICLR 2018
where C softly projects onto a linear subspace that can be roughly understood as the orthogonal
complement of the subspace characterized by C . Ci ∨ Cj is the conceptor computed from the union
of the two sets of sample points from which Ci and Cj are computed. It describes a space that is
approximately the sum of linear subspaces characterized by Ci and Cj , respectively. The definition
of Ci ∧ Cj reflects de Morgan’s law. Figure 2 illustrates the geometry of these operations.
Figure 2: Geometry of Boolean operations on 2-dimensional conceptors. The OR (resp. AND) op-
eration gives a conceptor whose ellipsoid approximately is the smallest (largest) ellipsoid enclosing
(contained in) the argument conceptor’s ellipsoids.
2.2	Incremental Ridge Regression
This subsection explains how conceptors can be applied to master continual learning in a simple
linear model trained on a supervised task by ridge regression. The training is done sequentially on
multiple input-to-output mapping tasks. This simplified scenario illustrates the working principle of
continual learning with conceptors and will later be used repeatedly as a sub-procedure in the CAB
algorithm for training multilayer feed-forward networks.
Consider a sequence of m incoming tasks indexed by j . We denote the training dataset for the j -th
task by {(xj1, yj), ∙ ∙ ∙ , (Xn, yn)}, where Xj ∈ RN are input vectors and yj ∈ RM their CorreSPond-
ing target outputs. Whenever the training dataset for a new task is available, the incremental learning
method will compute a matrix conceptor Cj for the input variable of the new task using Equation
2 and update the linear model, resulting in a sequence of linear models W1, . . . Wm such that Wj
solves not only the j-th task but also all previous tasks: for k ≤ j, yk ≈ WjXk. The conceptor Cj is
a soft projection matrix onto the linear subspace spanned by input patterns from the j -th task. Then,
AjT = C1 ∨ …∨ CjT characterizes the memory space already claimed by the tasks 1, . . . , j - 1
and Fj = Aj-1, the orthogonal complement of Aj - 1, represents the memory space still free for
the j -th task. Here “memory space” refers to the linear space of input vectors. In detail, this method
proceeds in the following way:
•	Initialization (no task trained yet): W0 = 0M×N,A0 = 0N×N.
•	Incremental task learning: For tasks j = 1, . . . , m do:
1.	Store the input vectors from the j -th training dataset of size n into a N × n sized input
collection matrix Xj, and store the output vectors into a M × n sized output collection
matrix Y j .
2.	Compute the conceptor for this task by Cj = Rj (Rj + α-2I)-1, where Rj =
1X jX j >
n
3.	Train an increment matrix Wijnc (to be added to Wj-1, yielding Wj), with the crucial
aid of a helper conceptor Fj :
(a)	Fj := Aj-1 (comment: this conceptor characterizes the “still disposable”
memory space for the j -th task),
(b)	T := Yj - (Wj-1Xj) (comment: this matrix consists of target values fora linear
regression to compute Wijnc),
(c)	S := FjXj (comment: this matrix consists of input arguments for the linear
regression),
3
Published as a conference paper at ICLR 2018
(d)	Wjnc = ((SS>∕n + λ-2I)-1ST>/n)> (comment: carry out the regression,
regularized by λ-2),
4.	Update Wj : Wj = Wj-1 + Wijnc.
5.	Update A : Aj = Aj-1 ∨ Cj (comment: this is possible due to the associativity of the
∨ operation on conceptors)
The weight increment Wijnc does not interfere much with the previously learned weights Wj-1
because the regularization in step 3(d) constrains the row space of Wijnc to be only the linear sub-
space spanned by input arguments defined in 3(c), which are inside the kernel of Wj-1 due to the
projection by Fj . Intuitively speaking, when learning a new task, this algorithm exploits only the
components of input vectors in the still unused space (kernel of W j-1, characterized by Fj) to
compensate errors for the new task and leaves the directions in the already used memory space (row
space of W j-1, characterized by Aj-1) intact.
3	Conceptor-Aided SGD and Back-prop
In this section, we first derive a stochastic gradient descent version of the algorithm described in the
previous section, then present the procedure of CAB.
3.1	SGD
In the algorithm introduced in the previous section, Wijnc is computed by ridge regression, which
offers a closed-form solution to minimize the following cost function
J(Wjj= E[∣WjncS- t|2] + λ-2∣Wjd2ro	(6)
where t = yj - Wj-1xj, s = Fjxj. One can also minimize this cost function by stochastic
gradient descent (SGD), which starts from an initial guess of Wijnc and repeatedly performs the
following update
Winc 一 wl - η^wj J(Winc)	(7)
inc
where η is the learning rate and the gradient is given by:
VWj J(WjJ = 2E[(Wjncs - t)s>]+2λ-2Wjnc	(8)
inc
Substituting t by yj - Wj-1xj and s by Fjxj = (I - Aj-1)xj in (8), we get
VWj	J(Wijnc)=2E[(Wijnc(I-Aj-1)xj	-	yj	+	W j-1xj)s>] +	2λ-2Wijnc	(9)
inc
= 2E[(-WijncAj-1xj + (W j-1 + Wijnc)xj - yj)s>] + 2λ-2Wijnc	(10)
Due to the regularization term in the cost function, as the optimization goes on, eventually Winc
will null the input components that are not inside the linear subspace characterized by Fj , hence
WijncAj-1xj will converge to 0 as the algorithm proceeds. In addition, since Wj = Wj-1 + Wijnc,
(10) can be simplified to
VWijncJ(Wijnc) =2E[(Wjxj-yj)s>]+2λ-2Wijnc	(11)
Adding Wj-1 to both sides of (7), we obtain the update rule for Wj:
Wj — Wj - 2ηE[es>] + 2ηλ-2Wjnc	(12)
where e := Wjxj - yj. In practice, at every iteration, the expected value can be approximated by a
mini-batch of size nB, indexed by iB:
1L	1L
E[es>] = — X (WjXjB-yiB)(FjXiB)> = — X (WjXjB-yiB)xj>Fj	(13)
nB	nB
iB=0	iB=0
where the transpose for Fj can be dropped since it is symmetric.
4
Published as a conference paper at ICLR 2018
Ifwe only trainthej-th task without considering the previous tasks, the update rule given by normal
SGD is
Wj 一 Wj - 2ηE[exj>] + 2ηλ-2Wj	(14)
Comparing this to the update rule in (12), we notice two modifications when a conceptor is adopted
to avoid CI: first, the gradient of weights are calculated using the conceptor-projected input vector
s = F j xj instead of the original input vector xj ; second, regularization is done on the weight
increment Wijnc rather than the final weight Wj . These two modifications lead to our design of the
conceptor-aided algorithm for training multilayer feed-forward networks.
3.2 Backprop
The basic idea of CAB is to guide the gradients of the loss function on every linear component
of the network by a matrix conceptor computed from previous tasks during error back-propagation
(Rumelhart et al., 1986), repeatedly applying the conceptor-aided SGD technique introduced in the
previous section in every layer.
Consider a feed-forward network with L + 1 layers, indexed by l = 0, . . . L, such that the 0-th and
the L-th layers are the input and output layers respectively. W(l) represents the linear connections
between the (l - 1)-th and the l-th layer, where we refer to the former as the pre-synaptic layer with
respect to W(l), and to the latter as the post-synaptic layer. We denote by N(l) the size of the l-th
layer (excluding the bias unit) and A(l)j a conceptor characterizing the memory space in the l-th
layer used UP by the first j tasks. Let σ(∙) be the activation function of the nonlinear neurons and
θ all the parameters of the network to be trained. Then the incremental training method with CAB
Proceeds as follows:
•	Initialization (no task trained yet): ∀l = 0, . . . , L - 1, A(l) := 0(N (l) +1)×(N (l) +1), and
randomly initialize W(l+1) 0 to be a matrix of size N (l+1) × (N (l) + 1).
•	Incremental task learning: For j = 1, . . . , m do:
1.	∀l = 0, . . . ,L-1,F(l)j = A	. (This conceptor characterizes the still dispos-
able vector space in layer l for learning task j )
2.	UPdate the network Parameters θ(j-1) obtained after training the first j - 1 tasks to
θj by stochastic gradient descent, where the gradients are comPuted by CAB instead
of the classical backProP. Algorithms 1 and 2 detail the forward and backward Pass
of CAB, resPectively. Different from classical backProP, the gradients are guided by a
matrix concePtor F(l)j, such that in each layer only the activity in the still disPosable
memory sPace will contribute to the gradient. Note that the concePtors remain the
same until convergence of the network for task j.
3.	After training on the j-th task, run the forward Procedure again on a batch ofnB inPut
vectors, indexed by iB, taken from thej-th training dataset, to collect activations hi(l)j
of each layer into a N(l) × nB sized matrix H(l)j, and set the correlation matrix
R(I)j =h H(I)j(H (l)j )>. 4 5
4. ComPute a concePtor on the l-th layer for the j-th Pattern by C(l)j = R(l)j(R(l)j +
α-2IN(l) ×N(l) )-1, ∀l = 0, . . . , L - 1. Finding an oPtimal aPerture can be done by a
cross-validation search1.
5. UPdate the concePtor for already used sPace in every layer: A(l)j = A(l)j ∨
C(l)j, ∀l = 0,. . . ,L - 1.
5
Published as a conference paper at ICLR 2018
Algorithm 1 The forward procedure of conceptor-aided backprop, adapted from the traditional
backprop. Input vectors are passed through a feed-forward network to compute the cost function.
L(yj,yj) denotes the loss for the j-th task, to which a regularize] Ω(θjnc) = Ω(θj - θj-1)=
l∣θj - θjτ∣K is added to obtain the total cost J , where θ contains all the weights (biases are
considered as weights connected to the bias units). The increment of parameters rather than the
parameters themselves are regularized, similar to the conceptor-aided SGD.
Require: Network depth, l
Require: W(l)j, l ∈ {1, . . . , L}, the weight matrices of the network
Require: xj, one input vector ofthej-th task
Require: yj, the target output for xj
1:	h(0) = xj
2:	for l = 1, . . . L do
3:	b(l) = [h(l-1)>, 1]>, include the bias unit
4:	a(l) = W(l)jb(l)
5:	h(l) = σ(a(l))
6:	end for
7： yj = h(I)
8: J = L(yj,yj) + λΩ(θM)
Algorithm 2 The backward procedure of conceptor-aided backprop for the j -th task, adapted from
the traditional backprop. The gradient g of the loss function L on the activations a(l) represents
the error for the linear transformation W(l)j between the (l - 1)-th and the l-th layers. In the
standard backprop algorithm, the gradient of L on W(l)j is computed as an outer product of the
post-synaptic errors g and the pre-synaptic activities h(l-1). This resembles the computation of the
gradient in the linear SGD algorithm, which motivates us to apply conceptors in a similar fashion as
in the ConcePtor-aided SGD. Specifically, We project the gradient Vw(i)j L by the matrix ConcePtor
F (l-1) j that indicates the free memory space on the pre-synaptic layer.
^1:
g — VyJ = vyL(y,y)
2: for l = L, L - 1, . . . , 1 do
3:	Convert the gradient on the layer’s output into a gradient on the pre-nonlinearity activation
( denotes element-wise multiplication):
g — Va(i) J = g Θ σ0(a(l 2 3 4 5 6))
4:	Compute the gradient of weights, project it by F(l-1)j, and add it to the regularization term
on the increment:
VW (i)j J =g(F (T)j b(T))> + λVw (i)j Ω(θjnc) = gb(IT)>F (IT)j + 2λ%ncj
=gb(l-1)>F (l-1)j + 2λ(W(l)j - W (l)j-1)
5:	Propagate the gradients w.r.t. the next lower-level hidden layers activations:
j>
g 一 Vh(l-1) J = W(I) j g
6: end for
6
Published as a conference paper at ICLR 2018
Figure 3: Average performance across already learned permuted MNIST tasks using CAB or EWC
4	Experiments
4.1	Permuted MNIST Experiment
To test the performance of CAB, we evaluated it on the permuted MNIST experiment (Srivastava
et al., 2013; Goodfellow et al., 2014; Kirkpatrick et al., 2017; Lee et al., 2017), where a sequence of
pattern recognition tasks are created from the MNIST dataset (LeCun et al., 1998). For each task, a
random permutation of input image pixels is generated and applied to all images in MNIST to obtain
a new shuffled dataset, equally difficult to recognize as the original one, the objective of each task is
to recognize these images with shuffled pixels.
For a proof-of-concept demonstration, we trained a simple but sufficient feed-forward network with
[784-100-10] of neurons to classify 10 permuted MNIST datasets. The network has logistic sigmoid
neurons in both hidden and output layers, and is trained with mean squared error as the cost function.
Vanilla SGD was used in all experiments to optimize the cost function. Learning rate and aperture
were set to 0.1 and 4, respectively. For comparison, we also tested EWC on the same task with the
same network architecture, based on the implementation by Seff (2017). The parameters chosen
for the EWC algorithm were 0.01 for the learning rate and 15 for the weight of the Fisher penalty
term. Figure 3 shows the performance of CAB on this task, the average testing accuracy is 95.2%
after learning all 10 tasks sequentially. Although a fair amount of effort was spent on searching for
optimal parameters for EWC, the accuracies shown here might still not reflect its best performance.
However, the same experiment with EWC was also conducted in Kemker et al. (2017), where the
authors reimplemented EWC on a network with higher capacity (2 hidden layers and 400 ReLU
neurons per layer) and the resulting average accuracy after learning 10 tasks sequentially was shown
to be around 93%.
Since all tasks are generated by permuting the same dataset, the portion of the input space occupied
by each of them should have the same size. However, as more tasks are learned, the chance that the
space of a new task will overlap with the already used input space increases. Figure 4 shows the
singular value spectra and quota of the input and hidden layer conceptors every time after a new task
is learned. As the incremental learning proceeds, it becomes less likely for a new task to be in the
free space. For example, the second task increases the quota of the input layer memory space by 0.1,
whereas the 10th task increases it by only 0.03. However, CAB still manages to make the network
learn new tasks based on their input components in the non-overlapping space.
1Jaeger (2014) proposes a number of methods for analytical aperture optimization. It remains for future
work to determine how these methods transfer to our situation.
7
Published as a conference paper at ICLR 2018
一 taskɪ, quota： 0.12
—— task2, quota： 0.22
—task3, quota： 0.32
task4, quota： 0.40
task5, quota： 0.48
task6, quota: 0.54
task7. quota： 0.60
task8, quota： 0.64
—task9, quota： 0.69
—task!Or quota： 0.72
(a)	Singular value spectra of conceptors A(O) j on the input layer.
一 taskɪ, quota： 0.26
—— task2. quota： 0 43
—task3, quota: 0.55
task4, quota： 0.63
task5, quota： 0.69
task6, quota： 0.74
task7, quota： 0.78
—tasks, quota： 0.80
—task% quota： 0.83
—— task!Or quota： 0.85
(b)	Singular value spectra of conceptors A(1)j on the hidden layer.
Figure 4: The development of singular value spectra of conceptors for “used-up” space on the input
layer and hidden layer during incremental learning of 10 permuted MNIST tasks. Quota of these
conceptors are displayed in the legends.
8
Published as a conference paper at ICLR 2018
4.2	Disjoint MNIST Experiment
We then applied CAB to categorize the disjoint MNIST datasets into 10 classes (Srivastava et al.,
2013; Lee et al., 2017). In this experiment, the original MNIST dataset is divided into two disjoint
datasets with the first one consisting of data for the first five digits (0 to 4), and the second one
of the remaining five digits (5 to 9). This task requires a network to learn these two datasets one
after the other, then examines its performance of classifying the entire MNIST testing images into
10 classes. The current state-of-the-art accuracy on this task, averaged over 10 learning trials, is
94.12(±0.27)%, achieved by Lee et al. (2017) using IMM. They also tested EWC on the same task
and the average accuracy was 52.72(±1.36)%.
To test our method, we trained a feed-forward network with [784-800-10] neurons. Logistic sigmoid
nonlinearities were used in both hidden and output layers, and the network was trained with vanilla
SGD to minimize mean squared errors. The aperture α = 9 was used for all conceptors on all layers,
learning rate η and regularization coefficient λ were chosen to be 0.1 and 0.005 respectively. The
accuracy of CAB on this task, measured by repeating the experiment 10 times, is 94.91(±0.30)%.
It is worth mentioning that the network used by Lee et al. (2017) for testing IMM and EWC had
[784-800-800-10] rectified linear units (ReLU), so CAB achieved better performance with fewer
layers and neurons.
4.3	Computational Cost
If a conceptor is computed by ridge regression, the time complexity is O(nN2 + N3) when the
design matrix is dense, where n is the number of samples and N the number of features. In terms of
wall time measures, the time taken to compute a conceptor from the entire MNIST training set (in
this case, n = 55000 images and N = 784 pixels, corresponding to the input layer in our networks)
is 0.42 seconds of standard notebook CPU time on average. Although we did not implement it in
these experiments, incremental online adaptation of conceptors by gradient descent is also possible
in principle and would come at a cost of O(N2) per update.
5	Conclusion
In this work, we first reviewed the conceptor-based incremental ridge regression algorithm, intro-
duced in section 3.11 of Jaeger (2014) for memory management in recurrent neural networks. Then
we derived its stochastic gradient descent version for optimizing the same objective. Finally we
designed a conceptor-aided backprop algorithm by applying a conceptor to every linear layer of a
feed-forward network. This method uses conceptors to guide gradients of parameters during the
backpropagation procedure. As a result, learning a new task interferes only minimally with pre-
viously learned tasks, and the amount of already used network capacity can be monitored via the
singular value spectra and quota of conceptors.
In Jaeger (2014), different scenarios for continual learning are investigated in a reservoir computing
setting. Two extreme cases are obtained when (i) the involved learning tasks are entirely unrelated
to each other, versus (ii) all tasks come from the same parametric family of learning tasks. The two
cases differ conspicuously with regards to the geometry of involved conceptors, and with regards
to opportunities to re-use previously acquired functionality in subsequent learning episodes. The
permuted MNIST task is an example of (i) while the disjoint MNIST task rather is of type (ii).
Conceptors provide an analytical tool to discuss the “family relatedness” and enabling/disabling
conditions for continual learning in geometrical terms. Ongoing and future research is devoted to
a comprehensive mathematical analysis of these phenomena which in our view lie at the heart of
understanding continual learning.
Acknowledgments
The work reported in this article was partly funded through the European H2020 collaborative
project NeuRAM3 (grant Nr 687299).
9
Published as a conference paper at ICLR 2018
References
Bernard Ans and StePhane Rousset. Avoiding catastrophic forgetting by coupling two reverberating
neural networks. Comptes Rendus de l,Academie des Sciences-Series In-Sciences de la Vie, 320
(12):989-997,1997.
Robert M French. Using semi-distributed representations to overcome catastrophic forgetting in
connectionist networks. Proceedings of the 13th Annual Cognitive Science Society Conference,
pp. 173178, 1991.
Robert M French. Pseudo-recurrent connectionist networks: An approach to the ‘sensitivity-
stability’ dilemma. Connection Science, 9(4):353-380, 1997.
Robert M French. Catastrophic forgetting in connectionist networks. Trends in Cognitive Sciences,
3(4):128-135, 1999.
Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An empirical inves-
tigation of catastrophic forgetting in gradient-based neural networks. International Conference
on Learning Representations, 2014.
Geoffrey E Hinton and David C Plaut. Using fast weights to deblur old memories. In Proceedings of
the Ninth Annual Conference of the Cognitive Science Society, pp. 177-186. Lawrence Erlbaum
Associates, 1987.
Herbert Jaeger. The echo state approach to analysing and training recurrent neural networks-with
an erratum note. German National Research Center for Information Technology GMD Technical
Report, 148(34):13, 2001.
Herbert Jaeger. Controlling recurrent neural networks by conceptors. Jacobs University Technical
Reports, (31), 2014. https://arxiv.org/abs/1403.3369.
Herbert Jaeger. Using conceptors to manage neural long-term memories for temporal patterns. Jour-
nal of Machine Learning Research, 18(13):1-43, 2017. URL http://jmlr.org/papers/
v18/15-449.html.
Ronald Kemker, Angelina Abitino, Marc McClure, and Christopher Kanan. Measuring catastrophic
forgetting in neural networks. Computing Research Repository, abs/1708.02072, 2017. http:
//arxiv.org/abs/1708.02072.
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A.
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hass-
abis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting
in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521, 2017.
Dharshan Kumaran, Demis Hassabis, and James L McClelland. What learning systems do intelligent
agents need? complementary learning systems theory updated. Trends in Cognitive Sciences, 20
(7):512-534, 2016.
Yann LeCun, Corinna Cortes, and Christopher JC Burges. The MNIST database of handwritten
digits. 1998. http://yann.lecun.com/exdb/mnist/.
Sang-Woo Lee, Jin-Hwa Kim, JungWoo Ha, and Byoung-Tak Zhang. Overcoming catastrophic
forgetting by incremental moment matching. Computing Research Repository, abs/1703.08475,
2017. http://arxiv.org/abs/1703.08475.
Wolfgang Maass, Thomas Natschlager, and Henry Markram. Real-time computing without stable
states: A new framework for neural computation based on perturbations. Neural Computation,
14(11):2531-2560, 2002.
Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The
sequential learning problem. Psychology of Learning and Motivation, 24:109-165, 1989.
Roger Ratcliff. Connectionist models of recognition memory: Constraints imposed by learning and
forgetting functions. Psychological Review, 97(2):285-308, 1990.
10
Published as a conference paper at ICLR 2018
David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-
propagating errors. Nature, 323:533-535, 1986.
Ari Seff. Implementation of overcoming catastrophic forgetting in neural networks
in tensorflow. GitHub Repository, 2017. https://github.com/ariseff/
overcoming- catastrophic.
RUPesh K Srivastava, Jonathan Masci, Sohrob Kazerounian, FaUstino Gomez, and Jurgen Schmid-
huber. Compete to compute. In Advances in Neural Information Processing Systems, pp. 2310-
2318, 2013. http://papers.nips.cc/paper/5059-compete-to-compute.pdf.
Vipin Srivastava, Suchitra Sampath, and David J Parker. Overcoming catastrophic interference in
connectionist networks using Gram-Schmidt orthogonalization. PloS ONE, 9(9):e105619, 2014.
11