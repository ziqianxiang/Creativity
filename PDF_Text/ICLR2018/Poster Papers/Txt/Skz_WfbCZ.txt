Published as a conference paper at ICLR 2018
A PAC-Bayesian Approach to
Spectrally-Normalized Margin Bounds
for Neural Networks
Behnam Neyshabur, Srinadh Bhojanapalli, Nathan Srebro
Toyota Technological Institute at Chicago
{bneyshabur, srinadh, nati}@ttic.edu
Ab stract
We present a generalization bound for feedforward neural networks with ReLU
activations in terms of the product of the spectral norm of the layers and the
Frobenius norm of the weights. The key ingredient is a bound on the changes
in the output of a network with respect to perturbation of its weights, thereby
bounding the sharpness of the network. We combine this perturbation bound with
the PAC-Bayes analysis to derive the generalization bound.
1	Introduction
Learning with deep neural networks has enjoyed great success across a wide variety of tasks. Even
though learning neural networks is a hard problem, even for one hidden layer (Blum & Rivest, 1993),
simple optimization methods such as stochastic gradient descent (SGD) have been able to minimize
the training error. More surprisingly, solutions found this way also have small test error, even though
the networks used have more parameters than the number of training samples, and have the capacity
to easily fit random labels (Zhang et al., 2017).
Harvey et al. (2017) provided a generalization error bound by showing that the VC dimension of
d-layer networks is depth times the number parameters improving over earlier bounds by Bartlett
et al. (1999). Such VC bounds which depend on the number of parameters of the networks, cannot
explain the generalization behavior in the over parametrized settings, where the number of samples is
much smaller than the number of parameters.
For linear classifiers we know that the generalization behavior depends on the norm and margin of
the classifier and not on the actual number of parameters. Hence, a generalization bound for neural
networks that only depends on the norms of its layers, and not the actual number of parameters, can
explain the good generalization behavior of the over parametrized networks.
Bartlett & Mendelson (2002) showed generalization bounds for feedforward networks in terms of
unit-wise `1 norm with exponential dependence on depth. In a more recent work, Neyshabur et al.
(2015a) provided generalization bounds for general class of group norms including Frobenius norm
with same exponential dependence on the depth and showed that the exponential dependence is
unavoidable in the worst case.
Bartlett et al. (2017a) showed a margin based generalization bound that depends on spectral norm
and `1 norm of the layers of the networks. They show this bound using a complex covering number
argument. This bound does not depend directly on the number of parameters of the network but
depends on the norms of its layers. However the `1 term has high dependence on the number of hidden
units if the weights are dense, which is typically the case for modern deep learning architectures.
Keskar et al. (2016) suggested a sharpness based measure to predict the difference in generalization
behavior of networks trained with different batch size SGD. However sharpness is not a scale invariant
measure and cannot predict the generalization behavior (Neyshabur et al., 2017). Instead sharpness
when combined with the norms of the network can predict the generalization behavior according to
the PAC-Bayes framework (McAllester, 1999). Dziugaite & Roy (2017) numerically evaluated a
generalization error bound from the PAC-Bayes framework showing that it can predict the difference
in generalization behavior of networks trained on true vs random labels. This generalization result is
1
Published as a conference paper at ICLR 2018
more of computational in nature and gives much tighter (non-vacuous) bounds compared with the
ones in either (Bartlett et al., 2017a) or the ones in this paper.
In this paper we present and prove a margin based generalization bound for feedforward neural
networks with ReLU activations, that depends on the product of the spectral norm of the weights in
each layer, as well as the Frobenius norm of the weights.
Our generalization bound shares much similarity with a margin based generalization bound recently
presented by Bartlett et al. (2017a). Both bounds depend similarly on the product of the spectral
norms of each layer, multiplied by a factor that is additive across layers. In addition, Bartlett et al.
(2017a) bound depends on the elementwise 'ι-norm of the weights in each layer, while our bound
depends on the Frobenius (elementwise '2) norm of the weights in each layer, with an additional
multiplicative dependence on the “width”. The two bounds are thus not directly comparable, and as
we discuss in Section 3, each one dominates in a different regime, roughly depending on the sparsity
of the weights. We also discuss in what regimes each of these bounds could dominate a VC-bound
based on the overall number of weights.
More importantly, our proof technique is entirely different, and arguably simpler, than that of Bartlett
et al. (2017a). We derive our bound using PAC-Bayes analysis, and more specifically a generic PAC-
Bayes margin bound (Lemma 1). The main ingredient is a perturbation bound (Lemma 2), bounding
the changes in the output of a network when the weights are perturbed, thereby its sharpness, in terms
of the product of the spectral norm of the layers. This is an entirely different analysis approach from
the covering number analysis of Bartlett et al. (2017a). We hope our analysis can give more direct
intuition into the different ingredients in the bound and will allow modifying the analysis, e.g. by
using different prior and perturbation distributions in the PAC-Bayes bound, to obtain tighter bounds,
perhaps with dependence on different layer-wise norms.
We note that other prior bounds in terms of elementwise or unit-wise norms (such as the Frobenius
norm and elementwise `1 norms of layers), without a spectral norm dependence, all have a multi-
plicative dependence across layers or exponential dependence on depth (Bartlett & Mendelson, 2002;
Neyshabur et al., 2015b), or are for constant depth networks (Bartlett, 1998). Here only the spectral
norm is multiplied across layers, and thus if the spectral norms are close to one, the exponential
dependence on depth can be avoided.
After the initial preprint of this paper, Bartlett et al. (2017b) presented an improved bound that
replaces the dependency on `1 norm of the layers (Bartlett et al., 2017a) with `2,1 norm. This new
generalization bound is strictly better than our Frobenius norm bound, and improves over existing
results.
1.1	Preliminaries
Consider the classification task with input domain XB,n = x ∈ Rn | | Pin=1 xi2 ≤ B2 and output
domain Rk where the output of the model is a score for each class and the class with the maximum
score will be selected as the predicted label. Let fw (x) : XB,n → Rk be the function computed
by a d layer feed-forward network for the classification task with parameters w = vec {Wi }id=1 ,
fw(x) = Wd φ(Wd-1 φ( φ(W1x))), here φ is the ReLU activation function. Let fwi (x) denote
the output of layer i before activation and h be an upper bound on the number of output units in each
layer. We can then define fully connected feedforward networks recursively: fw1 (x) = W1x and
fwi (x) = Wiφ(fwi-1(x)). Let k.kF, k.k1 and k.k2 denote the Frobenius norm, the element-wise `1
norm and the spectral norm respectively. We further denote the `p norm of a vector by |.|p.
Margin Loss. For any distribution D and margin γ > 0, we define the expected margin loss as
follows:
Ll(fw) = P(X,y)~D fw(x)[y] ≤ γ + m=x fw(x)[j]
(1)
T .	/ P ∖ -I	.1	∙	∙	1	.∙	.	∕' .1	F	1	1	〜	C
Let Lγ (fw) be the empirical estimate of the above expected margin loss. Since setting γ = 0
corresponds to the classification loss, we will use L0(fw) and L0(fw) to refer to the expected risk
and the training error. The loss Lγ defined this way is bounded between 0 and 1.
2
Published as a conference paper at ICLR 2018
1.2	PAC-Bayesian framework
The PAC-Bayesian framework (McAllester, 1998; 1999) provides generalization guarantees for
randomized predictors, drawn form a learned distribution Q (as opposed to a learned single predictor)
that depends on the training data. In particular, let fw be any predictor (not necessarily a neural
network) learned from the training data and parametrized by w. We consider the distribution Q over
predictors of the form fw+u, where u is a random variable whose distribution may also depend on
the training data. Given a “prior” distribution P over the set of predictors that is independent of the
training data, the PAC-Bayes theorem states that with probability at least 1 - δ over the draw of the
training data, the expected error of fw+u can be bounded as follows (McAllester, 2003):
Eu[L0(fw+u)] ≤ Eu[Lo(fw+u)] + 2∖F (KL (W+ 川? + 1互.
m-1
(2)
To get a bound on the expected risk L0(fw) for a single predictor fw, we need to relate the expected
perturbed loss, Eu[L0(fw+u)] in the above equation with L0(fw). Toward this we use the following
lemma that gives a margin-based generalization bound derived from the PAC-Bayesian bound (2):
Lemma 1. Let fw (x) : X → Rk be any predictor (not necessarily a neural network) with parameters
W, and P be any distribution on the parameters that is independent of the training data. Then, for
any γ, δ > 0, with probability ≥ 1 - δ over the training set of size m, for any W, and any random
perturbation U s.t. Pu Imaxx∈χ fw+u(x) — fw(x)∣∞ < 4] ≥ 1, we have:
L0(fw) ≤ LY (fw ) + 4∖ 卜…UkP )+ln 竽.
m—1
In the above expression the KL is evaluated for a fixed W and only U is random, i.e. the distribution
of W + U is the distribution of U shifted by W. The lemma is analogous to similar analysis of
Langford & Shawe-Taylor (2003) and McAllester (2003) obtaining PAC-Bayes margin bounds for
linear predictors, and the proof, presented in Section 4, is essentially the same. As we state the lemma,
it is not specific to linear separators, nor neural networks, and holds generally for any real-valued
predictor.
We next show how to utilize the above general PAC-Bayes bound to prove generalization guarantees
for feedforward networks based on the spectral norm of its layers.
2 Generalization B ound
In this section we present our generalization bound for feedfoward networks with ReLU activations,
derived using the PAC-Bayesian framework. Langford & Caruana (2001), and more recently Dziugaite
& Roy (2017) and Neyshabur et al. (2017), used PAC-Bayes bounds to analyze generalization behavior
in neural networks, evaluating the KL-divergence, “perturbation error” L[fw+u] — L[fw], or the
entire bound numerically. Here, we use the PAC-Bayes framework as a tool to analytically derive a
margin-based bound in terms of norms of the weights. As we saw in Lemma 1, the key to doing so is
bounding the change in the output of the network when the weights are perturbed. In the following
lemma, we bound this change in terms of the spectral norm of the layers:
Lemma 2 (Perturbation Bound). For any B, d > 0, let fw : XB,n → Rk be a d-layer neural network
with ReLU activations. Then for any W, and x ∈ XB,n, and any perturbation U = vec {Ui}id=1
such that ∣∣Uik2 ≤ d ∣∣Wik2 ,the change in the output of the network can be bounded as follows:
|fw+u(x) - fw(x)|2 ≤ eB
dd
Y kWik2 X
i=1	i=1
kUik2
kWik2.
This lemma characterizes the change in the output of a network with respect to perturbation of its
weights, thereby bounding the sharpness of the network as defined in Keskar et al. (2016).
The proof of this lemma is presented in Section 4. Next we use the above perturbation bound and the
PAC-Bayes result (Lemma 1) to derive the following generalization guarantee.
3
Published as a conference paper at ICLR 2018
Theorem 1 (Generalization Bound). For any B, d, h > 0, let fw : XB,n → Rk be a d-layer
feedforward network with ReLU activations. Then, for any δ, γ > 0, with probability ≥ 1 - δ over a
training set of size m, for any w, we have:
(
Lo(fw) ≤ LY (fw) + O t
B2d2h ln(dh)∏=IkWik 2 P?= IWl +ln dm '
γ2 m
/
Proof. The proof involves mainly two steps. In the first step we calculate what is the maximum
allowed perturbation of parameters to satisfy a given margin condition γ, using Lemma 2. In the
second step we calculate the KL term in the PAC-Bayes bound in Lemma 1, for this value of the
perturbation.
Let β = (Qd=IkWik2) / and consider a network with the normalized weights Wi =宿宁 W
Due to the homogeneity of the ReLU, we have that for feedforward networks with ReLU activations
fwe = fw, and so the (empirical and expected) loss (including margin loss) is the same for w and we .
We can also verify that (Qd=I ∣∣Wik2) =(Qd=1 ∣∣ffi∣∣2) and kkWF =制1,and so the excess
error in the Theorem statement is also invariant to this transformation. It is therefore sufficient to
prove the Theorem only for the normalized weights W, and hence We assume w.l.o.g. that the spectral
norm is equal across layers, i.e. for any layer i, kWik2 = β.
Choose the distribution of the prior P to be N (0, σ2I), and consider the random perturbation
U 〜N(0, σ2I), with the same σ, which We will set later according to β. More precisely, since the
prior cannot depend on the learned predictor w or its norm, we will set σ based on an approximation
/-ɪ 1 '	1	1	,' %	1 ,	♦ 1	∙ 1	∙1 1	, ,1 IxAXl lʌ	1	1	, 1 1 ∙ 1 ∙
β . For each value of β on a pre-determined grid, we will compute the PAC-Bayes bound, establishing
the generalization guarantee for all W for which ∣β 一 β∣ ≤ dβ, and ensuring that each relevant value
of β is covered by some β on the grid. We will then take a union bound over all β on the grid. For now,
we will consider a fixed β and the W for which ∣β — β∣ ≤ dβ, and hence1 ɪβd-1 ≤ fβd-1 ≤ eβd-1.
Since U 〜N(0, σ2I), we get the following bound for the spectral norm of Ui (Tropp, 2012):
PUi〜N(0,σ2I) [kUik2 > t] ≤ 2he-t /2hσ .
Taking a union bond over the layers, we get that, with probability ≥ 2, the spectral norm of the
perturbation Ui in each layer is bounded by σ，2h ln(4dh). Plugging this spectral norm bound into
Lemma 2 we have that with probability at least 2,
max lfw+u(X)- fw (X) |2 ≤ eBed X IUI2
x∈XB,n	β
i
=eBβd-1 X ∣∣Uik2 ≤ e2dBβd-1σP2hln(4dh) ≤ Y,	(3)
i
where we choose σ =--------γ, to get the last inequality. Hence, the perturbation U with
42dBβd-1 ʌ/h ln(4hd)	&	， F
the above value of σ satisfies the assumptions of the Lemma 1.
We now calculate the KL-term in Lemma 1 with the chosen distributions for P and U, for the above
value of σ.
KL(w+u∣∣P) ≤ 察=422解炉%2hln(4hd) XX k WikF ≤ O "2d2hln(dh)βZ XX 岭脸
2σ	2γ	i=1	γ i=1	β
≤O
B2d2hln(dh)πd=1 k2Wik2 XX
γ	i=1
kWikF
kWik2
1(1 + d)d-1 ≤ (1 + d)d≤ e, as 1 + X
O- d )d-1.
ex, for all x. Similarly
d-1
1 + d-1)
≤ e, gives 1 ≤
≤
4
Published as a conference paper at ICLR 2018
T T	Γ∙	/-ɪ	∙ . ∖	F F ∙ 1 ∙ .	¢-	1 Γ∙	11	1,1, IC	六 I， IC	1
Hence, for any β, With probability ≥ 1 - δ and for all W such that, ∣β - β∣ ≤ dβ, we have:
/
L0 (fw) ≤ Lbγ (fw) + O
∖
B2d2hln(dh)∏=IkWik2 Pd=1 * +ln 等 '
γ2 m
(4)

∖
Finally we need to take a union bound over different choices of β. Let us see how many choices
r∙ ,r>	ι .	ι	ι	/ɔ ∙ .ι	∙ ι .	∖ ^r> cl, ICTyT ι	ι .	∙ ι
of β We need to ensure We always have β in the grid s.t. ∣β - β∣ ≤ Mβ. We only need to consider
values of β in the range (2B) 1/d ≤ β ≤ (γ√mm)	. For β outside this range the theorem statement
holds trivially: Recall that the LHS of the theorem statement, Lo(fw) is always bounded by 1. If
βd < 2B, then for any x, |fw (x)| ≤ βdB ≤ γ∕2 and therefore LY = 1. Alternately, if βd > γ√mm,
then the second term in equation 2 is greater than one. Hence, we only need to consider values of β
in the range discussed above. ∣β - β| ≤ d (余)1/d is a sufficient condition to satisfy the required
1
condition that ∣β - β∣ ≤ dβ in the above range, thus we can use a cover of Size dm匆.Taking a
union bound over the choices of β in this cover and using the bound in equation (4) gives us the
theorem statement.
3 Comparison to Existing Generalization Bounds
In this section we will compare the bound of Theorem 1 with a similar spectral norm based margin
bound recently obtained by Bartlett et al. (2017a;b), as well as examine whether and when these
bounds can improve over VC-based generalization guarantees.
The VC-dimension of fully connected feedforward neural networks with ReLU activation with d
layers and h units per layer is Θ(d2h2) (Harvey et al. (2017)), yielding a generalization guarantee of
the form:
ʌ ~
Lo(fw) ≤ Lo(fw) + O
(5)
where here and throughout this section we ignore logarithmic factors that depend on - the failure
probability δ, the sample size m, the depth d and the number of units h.
Bartlett et al. (2017a) showed a generalization bound for neural networks based on the spectral norm
of its layers, using a different proof approach based on covering number arguments. For feedforward
depth-d networks with ReLU activations, and when inputs are in XB,n, i.e. are of norm bounded
by |x|2 ≤ B, their generalization guarantee, ignoring logarithmic factors, ensures that, with high
probability, for any W,
/
L0 (fW) ≤ LY (Zw) + (O
∖
B2πd=ιkWik2(pd=ι (∣W⅜ )2/3)3,
(6)
γ2 m
/
∖
Comparing our Theorem 1 and the Bartlett et al. (2017a) bound (6), the factor O (γ⅛B2Πd=ι ||用||2)
appears in both bounds. The main difference is in the multiplicative factors, d2 h Pid=1 kWik2F∕kWik22
in Theorem 1 compared to(Pd=i (kWaki/kWilb)2/3) in (6). To get a sense of how these two
bounds compare, we will consider the case where the norms of the weight matrices are uniform across
layers—this is a reasonable situation as we already saw that the bounds are invariant to re-balancing
the norm between the layers. But for the sake of comparison, we further assume that not only is
the spectral norm equal across layers (this we can assume w.l.o.g.) but also the Frobenius kWi kF
and element-wise `1 norm kWi k1 are uniform across layers (we acknowledge that this setting is
somewhat favorable to our bound compared to (6)). In this case the numerator in the generalization
5
Published as a conference paper at ICLR 2018
bound of Theorem 1 scales as:
O (#h kWkF ʌ
Ol "ilW,
while numerator in (6) scales as:
O(3 kWik2 ʌ
OldkWik”.
(7)
(8)
Comparing between the bounds thus boils down to comparing √h IlWiIlF With k 用 k 1. RecaIling
that Wi is at most a h × h matrix, we have that kWikF ≤ kWik1 ≤ h kWikF. When the weights
are fairly dense and are of uniform magnitude, the second inequality Will be tight, and We will have
√h IlWillF《kWiki, and Theorem 1 will dominate. When the weights are sparse with roughly a
constant number of significant weights per unit (i.e. weight matrix with sparsity Θ(h)), the bounds
will be similar. Bartlett et al. (2017a) bound will dominate when the weights are extremely sparse,
with much fewer significant weights than units, i.e. when most units do not have any incoming or
outgoing weights of significant magnitude.
It is also insightful to ask in what regime each bound could potentially improve over the VC-bound
(5) and thus provide a non-trivial guarantee. To this end, we consider the most “optimistic” scenario
where B∏d=ι∣Wi∣2 = Θ(1) (it certainly cannot be lower than one if We have a non-trivial margin
loss). As before, we also take the norms of the weight matrices to be uniform across layers, yielding
the multiplicative factors in (7) and (8), which we must compare to the VC-dimension Θ(d2h2). We
get that the bound of Theorem 1 is smaller than the VC bound if
∣Wi∣F = o (ph/d kWik2) .	(9)
We always have IlWiIlF ≤ h∣ IWik2, and this is tight only for orthogonal matrices, where all
eigenvalues are equal. Satisfying (9), and thus having Theorem 1 potentially improving over the
VC-bound, thus only requires fairly mild eigenvalue concentration (i.e. having multiple units be
similar to each other), reduced rank or row-level sparsity in the weight matrices. Note that we cannot
expect to improve over the VC-bound for unstructured “random” weight matrices—we can only
expect norm-based guarantees to improve over the VC bound if there is some specific degenerate
structure in the weights, and as we indeed see is the case here.
A similar comparison with Bartlett et al. (2017a) bound (6) and its multiplicative factor (8), yields
the following condition for improving over the VC bound:
IWiIi = o ((h∕√d) IWiI2).	(10)
Since IWiI1 can be as large as h1.5 IWiI2, in some sense more structure is required here in order
to satisfy (10), such as elementwise sparsity combined with low-rank row structure. As discussed
above, Theorem 1 and Bartlett et al. (2017a) bound can each be better in different regimes. Also in
terms of comparison to the VC-bound, it is possible for either one to improve over the VC bound
while the other doesn’t (i.e. for either (9) or (10) to be satisfied without the other one being satisfied),
depending on the sparsity structure in the weights.
After the initial preprint of this paper, Bartlett et al. (2017b) presented an improved bound replacing
the '1 norm term in the bound Bartlett et al. (2017a) with '2,1 norm (sum of '2 norms of each
unit). This new bound depending on IWiI2,i is always better than the bound based on IWiIi and
our bound based on 海||呜|旧.They match when each hidden unit has the same norm, making
IWiI2,1 ≈√hIWiIF.
4 Proofs of Lemmas
In this section we present the proofs of Lemmas 1 and 2.
Proof of Lemma 1. Let w0 = w + u. Let Sw be the set of perturbations with the following property:
Sw ⊆ {w0 χmax lfw0 (X)- fw (X)l∞ <4
6
Published as a conference paper at ICLR 2018
Let q be the probability density function over the parameters w0 . We construct a new distribution Q
over predictors fw where W is restricted to SW With the probability density function:
_ 1 ʃ q(W) W ∈ Sw
q(w) — z ɪo otherwise.
Here Z is a normalizing constant and by the lemma assumption Z = P [w0 ∈ SW] ≥ 1. By the
definition of Q, we have:
γ
XmXax lfw(X) - fw(X)l∞ < 4.
Therefore, the perturbation can change the margin between two output units of fw by at most 2; i.e.
for any perturbed parameters W drawn from Q:
rmaxv	|(|fw(x)[i] - fW(X)[j]|) - (IfW(X)[i] - fw(X)[j]|)| < Y
i,j∈[k],x∈XB,n	2
Since the above bound holds for any X in the domain XB,n, we can get the following a.s.:
Lo(fw) ≤ L2(fw)
L2 (fw) ≤ LY(fw)
Now using the above inequalities together with the equation (2), with probability 1 - δ over the
training set we have:
Lo(fw) ≤ EW hL2(fw)]
≤ Ew hL 2 (fw )i+2S2(KL(WmP)1+ιn 亨
≤ LY(fw)+2sS2(KL (WkP丁n 再
m-1
Nb “ —/ / KL (W0kP ) + ln 噜
≤ ly(fw) + 4\ ------------；-------,
m-1
The last inequality follows from the following calculation.
Let SW denote the complement set of SW and qc denote the density function q restricted to SW and
normalized. Then,
KL(q∣∣p)= ZKL(q∣∣p) + (1 - Z)KL(qc∣∣p) - H(Z),
where H(Z) = -Zln Z - (1 - Z) ln(1 - Z) ≤ 1 is the binary entropy function. Since KL is always
positive, we get,
KL⑷|p) = 1[KL(q∣∣P)+ H(Z))-(1- Z)KL(qc∣∣p)] ≤ 2(KL(q∣∣p) + 1).	□
Z
Proof of Lemma 2. Let ∆i = fwi +u (X) - fwi (X)2. We will prove using induction that for any
i ≥ 0:
△i ≤ (1+d)i(γpWjk2)∣χ∣2 X 林.
The above inequality together with 1 +d)d ≤ e proves the lemma statement. The induction base
clearly holds since ∆0 = |X - X|2 = 0. For any i ≥ 1, we have the following:
∆i+1 = (Wi+1 + Ui+1) φi(fwi +u(X)) -Wi+1φi(fwi (X))2
=∣(Wi+ι + a+。(Φi(fW+u(x)) - Φi(fW (X))) + Ui+1Φi(fW (x))∣2
≤ (kWi+1k2 + kUi+1k2)φi(fwi+u(X))-φi(fwi (X))2 + kUi+1k2φi(fwi (X))2
≤ (kWi+1k2 + kUi+1k2)∣∣fwi+u(X)-fwi (X)∣∣2 + kUi+1k2∣∣fwi (X)∣∣2
= ∆i (kWi+1k2 + kUi+1k2)+ kUi+1k2∣∣fwi (X)∣∣2,
7
Published as a conference paper at ICLR 2018
where the last inequality is by the Lipschitz property of the activation function and using φ(0) = 0.
The `2 norm of outputs of layer i is bounded by |x|2 Πij=1 kWj k2 and by the lemma assumption we
have k Ui+ι k 2 ≤ d k Wi+ι k 2. Therefore, using the induction step, We get the following bound:
∆i+1 ≤ δW (1+ d) ∣∣Wi+1 ∣∣2 + ∣∣Ui+1k2 |x|2 Y kWjk 2
≤ (1 + d
≤ (1 + d
j=1
i+1m 帽儿卜12 X j+⅛⅛∣χ∣2 DkWik2
i+1mkWjkW X 修.
□
5 Conclusion
In this paper, we presented new perturbation bounds for neural networks thereby giving a bound on
its sharpness. We also discussed how PAC-Bayes framework can be used to derive generalization
bounds based on the sharpness of a model class. Applying this to the feedforward networks, we
showed that a tighter generalization bound can be achieved based on the spectral norm and Frobenius
norm of the layers. The simplicity of the proof compared to that of covering number arguments in
Bartlett et al. (2017a) suggest that the PAC-Bayes framework might be an important tool in analyzing
the generalization behavior of neural networks.
References
Peter Bartlett, Dylan J Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural
networks. arXiv preprint arXiv:1706.08498v1, 2017a.
Peter Bartlett, Dylan J Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural
networks. arXiv preprint arXiv:1706.08498v2, 2017b.
Peter L Bartlett. The sample complexity of pattern classification with neural networks: the size of the
weights is more important than the size of the network. IEEE transactions on Information Theory,
44(2):525-536,1998.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.
Peter L Bartlett, Vitaly Maiorov, and Ron Meir. Almost linear vc dimension bounds for piecewise
polynomial networks. In Advances in Neural Information Processing Systems, pp. 190-196, 1999.
Avrim L Blum and Ronald L Rivest. Training a 3-node neural network is np-complete. In Machine
learning: From theory to applications, pp. 9-28. Springer, 1993.
Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. arXiv preprint
arXiv:1703.11008, 2017.
Nick Harvey, Chris Liaw, and Abbas Mehrabian. Nearly-tight vc-dimension bounds for piecewise
linear neural networks. arXiv preprint arXiv:1703.02930, 2017.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter
Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv
preprint arXiv:1609.04836, 2016.
John Langford and Rich Caruana. (not) bounding the true error. In Proceedings of the 14th
International Conference on Neural Information Processing Systems: Natural and Synthetic, pp.
809-816. MIT Press, 2001.
John Langford and John Shawe-Taylor. Pac-bayes & margins. In Advances in neural information
processing systems, pp. 439-446, 2003.
8
Published as a conference paper at ICLR 2018
David McAllester. Simplified pac-bayesian margin bounds. Lecture notes in computer science, pp.
203-215, 2003.
David A McAllester. Some PAC-Bayesian theorems. In Proceedings of the eleventh annual conference
on Computational learning theory, pp. 230-234. ACM, 1998.
David A McAllester. PAC-Bayesian model averaging. In Proceedings of the twelfth annual conference
on Computational learning theory, pp. 164-170. ACM, 1999.
Behnam Neyshabur, Ruslan Salakhutdinov, and Nathan Srebro. Path-SGD: Path-normalized opti-
mization in deep neural networks. In Advanced in Neural Information Processsing Systems (NIPS),
2015a.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Proceeding of the 28th Conference on Learning Theory (COLT), 2015b.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. Exploring general-
ization in deep learning. arXiv preprint arXiv:1706.08947, 2017.
Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational
mathematics, 12(4):389-434, 2012.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understand-
ing deep learning requires rethinking generalization. In International Conference on Learning
Representations, 2017.
9