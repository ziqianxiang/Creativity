Published as a conference paper at ICLR 2018
Hierarchical and Interpretable Skill Acqui-
sition in Multi-task Reinforcement Learning
Tianmin Shu*
University of California, Los Angeles
tianmin.shu@ucla.edu
Caiming XiongN Richard Socher
Salesforce Research
{cxiong, rsocher}@salesforce.com
Ab stract
Learning policies for complex tasks that require multiple different skills is a major
challenge in reinforcement learning (RL). It is also a requirement for its deploy-
ment in real-world scenarios. This paper proposes a novel framework for efficient
multi-task reinforcement learning. Our framework trains agents to employ hier-
archical policies that decide when to use a previously learned policy and when to
learn a new skill. This enables agents to continually acquire new skills during dif-
ferent stages of training. Each learned task corresponds to a human language de-
scription. Because agents can only access previously learned skills through these
descriptions, the agent can always provide a human-interpretable description of
its choices. In order to help the agent learn the complex temporal dependencies
necessary for the hierarchical policy, we provide it with a stochastic temporal
grammar that modulates when to rely on previously learned skills and when to
execute new skills. We validate our approach on Minecraft games designed to
explicitly test the ability to reuse previously learned skills while simultaneously
learning new skills.
1	Introduction
Deep reinforcement learning has demonstrated success in policy search for tasks in domains like
game playing (Mnih et al., 2015; Silver et al., 2016; 2017; Kempka et al., 2016; Mirowski et al.,
2017) and robotic control (Levine et al., 2016a;b; Pinto & Gupta, 2016). However, it is very difficult
to accumulate multiple skills using just one policy network Teh et al. (2017). Knowledge transfer
techniques like distillation (Bengio, 2012; Rusu et al., 2016; Parisotto et al., 2016; Teh et al., 2017)
have been applied to train a policy network both to learn new skills while preserving previously
learned skill as well as to combine single-task policies into a multi-task policy. Existing approaches
usually treat all tasks independently. This often prevents full exploration of the underlying relations
between different tasks. They also typically assume that all policies share the same state space and
action space. This precludes transfer of previously learned simple skills to a new policy defined over
a space with differing states or actions.
When humans learn new skills, we often take advantage of our existing skills and build new ca-
pacities by composing or combining simpler ones. For instance, learning multi-digit multiplication
relies on the knowledge of single-digit multiplication; learning how to properly prepare individual
ingredients facilitates cooking dishes based on complex recipes.
Inspired by this observation, we propose a hierarchical policy network which can reuse previously
learned skills alongside and as subcomponents of new skills. It achieves this by discovering the
underlying relations between skills.
To represent the skills and their relations in an interpretable way, we also encode all tasks using
human instructions such as “put down.” This allows the agent to communicate its policy and generate
plans using human language. Figure 1 illustrates an example: given the instruction “Stack blue,” our
hierarchical policy learns to compose instructions and take multiple actions through a multi-level
hierarchy in order to stack two blue blocks together. Steps from the top-level policy π3 (i.e., the red
*This work was done when the author was an intern at Salesforce Research.
,Corresponding author
1
Published as a conference paper at ICLR 2018
Task: “Stack blue”
▲ uo-a-nEnooe *sel.
-6u-p-42un A。=。"
Time t = 1
t = 4	t = 5	t = 6
Figure 1: Example of our multi-level hierarchical policy for a given task - stacking two blue blocks.
Each arrow represents one step generated by a certain policy and the colors of arrows indicate the
source policies. Note that at each step, a policy either utters an instruction for the lower-level policy
or directly takes an action.
Blue block None
t = 9	t = 10
branches) outline a learned high-level plan - “Get blue → Find blue → Put blue.” In addition, from
lower level policies, we may also clearly see composed plans for other tasks. Based on policy π2,
for instance, the task “Get blue” has two steps - “Find blue → action: turn left,” whereas “Put blue”
can be executed by a single action “put down” according to π3 . Through this hierarchical model, we
may i) accumulate tasks progressively from a terminal policy to a top-level policy and ii) unfold the
global policy from top-level to basic actions.
In order to better track temporal relationships between tasks, we train a stochastic temporal grammar
(STG) model on the sequence of policy selections (previously learned skill or new skill) for positive
episodes. The STG focuses on modeling priorities of tasks: for example, it is necessary to obtain an
object before putting it down. Integrating the STG into the hierarchical policy boosts efficiency and
accuracy by explicitly modeling such commonsense world knowledge.
We validated our approach by testing it on object manipulation tasks implemented in a Minecraft
world. Our experimental results demonstrate that this framework can (i) efficiently learn hierarchi-
cal policies and representations for multi-task RL; (ii) learn to utter human instructions to deploy
pretrained policies, improve their explainability and reuse skills; and (iii) learn a stochastic temporal
grammar via self-supervision to predict future actions.
2	Related Work
Multi-task Reinforcement Learning. Previous work on multi-task reinforcement learning mainly
falls into two families: knowledge transfer through distillation (Rusu et al., 2016; Parisotto et al.,
2016; Teh et al., 2017; Tessler et al., 2017) or modular policy design through 2-layer hierarchical
policy (Andreas et al., 2017). Our multi-level policy is more similar to the latter approach. The
main differences between our model and the one in Andreas et al. (2017) are two-fold: i) we do
not assume that a global task can be executed by only performing predefined sub-tasks; ii) in our
multi-level policy, global tasks at a lower-level layer may also be used as sub-tasks by global tasks
carried out at higher-levels.
Hierarchical Reinforcement Learning. Complex policies often require the modeling of longer
temporal dependencies than what standard Markov decision processes (MDPs) can capture. To
combat this, hierarchical reinforcement learning was introduced to extend MDPs to semi-MDPs
(Sutton et al., 1999), where options (or macro actions) are introduced on top of primitive actions
to decompose the goal of a task into multiple subgoals. In hierarchical RL, two sets of policies
are trained: local policies that map states to primitive actions for achieving subgoals, and a global
policy that initiates suitable subgoals in a sequence to achieve the final goal of a task (Bacon &
Precup, 2015; Kulkarni et al., 2016; Vezhnevets et al., 2016; Tessler et al., 2017; Andreas et al.,
2
Published as a conference paper at ICLR 2018
2017). This two-layer hierarchical policy design significantly improves the ability of discovering
complex policies which can not be learned by flat policies. However, it also often makes some strict
assumptions that limit its flexibility: i) a task’s global policy cannot use a simpler task’s policy as
part of its base policies; ii) a global policy is assumed to be executable by only using local policies
over specific options, e.g., (Kulkarni et al., 2016; Andreas et al., 2017). In this work, we aim to
learn a multi-level global policy which does not have these two assumptions. In addition, previous
work usually uses a latent variable to represent a task. In our work, we encode a task by a human
instruction to learn a task-oriented language grounding as well as to improve the interpretability of
plans composed by our hierarchical policies.
Language grounding via reinforcement learning. Recently, there has been work on grounding hu-
man language in 3D game environments (Hermann et al., 2017; Chaplot et al., 2017) or in text-based
games (Narasimhan et al., 2015) via reinforcement learning. In these games agents are instructed to
pick up an item described by a sentence. Besides visual grounding, Andreas et al. (2017) grounded
instructions (not necessarily using human language) to local policies in hierarchical reinforcement
learning. Our approach not only learns the language grounding for both visual knowledge and poli-
cies, but is also trained to utter human instructions as an explicit explanation of its decisions to
humans. To our knowledge, this is the first model that learns to compose plans for complex tasks
based on simpler ones which have human descriptions.
3	Model
In this section, we discuss our multi-task RL setting, hierarchical policy, stochastic temporal gram-
mar, and how interaction of these components can achieve plan composition.
3.1	Multitask RL Setting
Let G be a task set, where each task g is uniquely described by a human instruction. For sim-
plicity, we assume a two-word tuple template consisting of a skill and an item for such a phrase,
i.e., huskill, uitemi. Each tuple describes an object manipulation task. In this paper, we define
g = huskill, uitem i by default, thus tasks and instructions are treated as interchangeable concepts.
For each task, we define a Markov decision process (MDP) represented by states s ∈ S and primitive
actions a ∈ A. Rewards are specified for goals of different tasks, thus we use a function R(s, g) to
signal the reward when performing any given task g.
We assume that as a starting point, we have a terminal policy π0 (as shown in Figure 2a) trained for
a set of basic tasks (i.e., a terminal task set G0). The task set is then progressively increased as the
agent is instructed to do more tasks by humans at multiple stages, such that Go ⊂ Gi ⊂ …⊂ GK,
which results in life-long learning of polices from π0 for G0 to πK for GK as illustrated by the “task
accumulation” direction in Figure 1. At stage k > 0, Gk-1 is defined as the base task set of Gk. The
tasks in Gk-1 are named as base tasks at this stage and πk-1 becomes the base policy of πk. Here,
we utilize weak supervision from humans to define what tasks shall be augmented to the previous
task set at each new stage.
3.2	Hierarchical Policy
One of our key ideas is that anew task in current task set Gk may be decomposed into several simpler
subtasks, some of which can be base tasks in Gk-1 executable by base policy πk-1. Therefore,
instead of using a flat policy (Figure 2a) as π0 that directly maps state and human instruction to
a primitive action, we propose a hierarchical design (Figure 2b) with the ability to reuse the base
policy (i.e., πk-1) for performing base tasks as subtasks. Namely, at stage k, the global policy πk
is defined by a hierarchical policy. This hierarchy consists of four sub-policies: a base policy for
executing previously learned tasks, an instruction policy that manages communication between the
global policy and the base policy, an augmented flat policy which allows the global policy to directly
execute actions, and a switch policy that decides whether the global policy will primarily rely on the
base policy or the augmented flat policy.
The base policy is defined to be the global policy at the previous stage k - 1. The instruction policy
maps state s and task g ∈ Gk to a base task g0 ∈ Gk-1. The purpose of this policy is to inform base
3
Published as a conference paper at ICLR 2018
Actions
(a) Flat policy.
Actions
(b) Hierarchical design of global policy.
Figure 2: Flat and hierarchical policy architectures. V (s, g) and V SW(s, e, g) are value functions
defined in Section 4.1.
policy πk-1 which base tasks it needs to execute. Since an instruction is represented by two words,
we define the instruction policy using two conditionally independent distributions, i.e., πkinst(g0 =
〈"skill, Uitemils, g) = Pkall(uskill∣s, g)pkem(uitem∣s, g). An augmented flat policy, ∏kug(a∣s, g), maps
state s and task g to a primitive action a for ensuring that the global policy is able to perform novel
tasks in Gk that can not be achieved by only reusing the base policy. To determine whether to perform
a base task or directly perform a primitive action at each step, the global policy further includes a
switch policy, ∏kw(e∣s, g), where e is a binary variable indicating the selection of the branches, π[st
(e = 0) or πkaug (e = 1).
Note that the above description of the hierarchical policy does not account for an STG. The instruc-
tion policy and switch policy introduced here are simplified from the ones in the full model (see
Section 3.3).
At each time step, we first sample et from our switch policy πksw to decide whether the global policy
πk will rely on the base policy πk-1 or the augmented flat policy πkaug. We also sample a new
instruction gt0 from our instruction policy πkinst in order to sample actions from the base policy. This
can be summarized as:
et 〜∏kw(et∣st,g),	(1)
g0 〜∏inst(g0∣st,g),	⑵
and finally
at 〜∏k(at∣st,g) = ∏k-i(at∣st,g0)(1-e"∏kug(at∣st,g)et,	(3)
where πk and πk-1 are the global policies at stage k and k - 1 respectively. After each step, we will
also obtain a reward rt = R(st, g).
3.3	Stochastic Temporal Grammar
Different tasks may have temporal relations. For instance, to move an object, one needs to first find
and pick up that object. There has been previous research (Si et al., 2011; Pirsiavash & Ramanan,
2014) using stochastic grammar models to capture such temporal relations. Inspired by this, we
summarize temporal transitions between various tasks with a stochastic temporal grammar (STG). In
our full model, the STG interacts with the hierarchical policy described above through the modified
switch policy and instruction policy by using the STG as a prior. This amounts to treating the past
history of switches and instructions in positive episodes as a guidance on whether the hierarchical
policy should defer to the base policy to execute a specific base task or employ its own augmented
flat policy to take a primitive action.
4
Published as a conference paper at ICLR 2018
In an episode, the temporal sequence of et and gt0, i.e., {het, gt0i; t ≥ 0}, can be seen as a finite
state Markov chain (Baum & Petrie, 1966). Note that the state here is referred to the tuple het, gt0i,
which is not the state of the game st ∈ S defined in Section 3.1. Consequently, at each level k > 0,
we may define an STG of a task g by i) transition probabilities, ρk(et, gt0 |et-1, gt0-1, g), and ii) the
distribution of he0, g00 i, qk(e0, g00 |g), all of which follow categorical distributions.
With the estimated probabilities, we sample et and gt0 in an episode at level k > 0 w.r.t. to reshaped
policies πksw0 and πkinst0 respectively:
•	If t = 0,
eo 〜πkw0(e0lst,g) Y πkw(e0lst,g) X qk(eo,g0|g),	(4)
g0∈Gk-1
g0 〜∏knst0(g0Mg) Y ∏inst(g0∣st,g)qk(eo = 0,g0|g);	⑸
•	Otherwise,
et 〜πkw0(et∣et-i,g0-i, st,g) Y ∏kw(et∣st,g) E Pk(et, g0∣et-i, gt-i, g),⑹
g0 ∈Gk-1
g0 〜πinst0(gtlet-i, g-t-i, st, g) Y πinst(gtlst,g)Pk(et = O,g't let-i,g0-i,g).	⑺
Note that primitive action sampling is not affected by the STG.
3.4	Plan Composition
Combined with our hierarchical policy and STG defined above, we are able to run an episode to
compose a plan for a task specified by a human instruction. Algorithm 1 in Appendix A summarized
this procedure with respect to the policy and STG at level k. Note that to fully utilize the base policy,
we assume that once triggered, a base policy will play to the end before the global policy considers
the next move.
4	Learning
The learning algorithm is outlined in Algorithm 2 in Appendix A. We learn our final hierarchical
policy through k stages of skill acquisition. Each of these stages is broken down into a base skill
acquisition phase and a novel skill acquisition phase in a 2-phase curriculum learning.
In the base skill acquisition phase, we only sample tasks from the base task set Gk-1. This ensures
that the global policy learns how to use previously learned skills by issuing instructions to the base
policy. In other words, this phase teaches the agent how to connect its instruction policy to its base
policy. Once the average reward for all base tasks exceeds a certain threshold, we proceed to the
next phase.
In the novel skill acquisition phase, we sample tasks from the full task set, Gk, for the k-th stage of
skill acquisition. It is in this phase that the agent can learn when to rely on the base policy and when
to rely on the augmented flat policy for executing novel tasks.
In each of these phases, all policies are trained with advantage actor-critic (A2C) (Section 4.1) and
distributions in the STG are estimated based on accumulated positive episodes (Section 4.2).
4.1	Policy Optimization by Advantage Actor-Critic
We use advantage actor-critic (A2C) for policy optimization with off-policy learning (Su et al.,
2017). Here, we only consider the gradient for global policies (i.e., k > O) as we assume the
terminal policy has been trained as initial condition. Let Vk(st, g) be a value function indicating the
expected return given state st and task g. To reflect the nature of the branch switching in our model,
we introduce another value function Vksw(st, et, g) to represent the expected return given state st,
task g and current branch selection et .
5
Published as a conference paper at ICLR 2018
Thus, given a trajectory Γ = {<st,et,g0 ,at,r, μkw(∙∣sj μ^st(∙∣st, g), μaug(∙∣st, g), gi : t =
0,1,…，T} generated by old policies μkw(∙∣st), μpst(∙∣st,g), and μθug(∙∣st,g), the policy gradi-
ent reweighted by importance sampling can be formulated as
+
+
ωswVθsw log ∏kw(et∣st,g)A(st,g, e。
、	—一一	J
1st term: switch policy gradient
(1 - et)ωtnstVθinst log π⅛nst(g0 ∣st, g)A(st, g, et,g0)
(8)
2nd term: instruction policy gradient
etωaugVθaug log ∏kug(at∣st,g)A(st,g,et, aj
3rd term: augmented policy gradient
where ωsw	= RetftR,	ωtnst	= 嘴夕0|st,g),	and a；*	= "/(atFtR	are importance sampling
t μ'k (et lst,g), t	μ⅛nst(g0 ∣st,g),	t	μfcg(at∣st,g)
weights for the three terms respectively; A(st, g, et), A(st, g, et, gt0), and A(st, g, et, at) are esti-
mates of advantage functions, which have multiple possible definitions. In this paper, we define
them by the difference between empirical return and value function estimation: A(st, g, et) =
Pτ∞=0 γτ R(st+τ, g) - Vk(st, g), A(st,g,et,gt0) = A(st, g, et,at) = Pτ∞=0 γτ R(st+τ, g) -
Vksw(st, g, et), where γ is the discounted coefficient.
Finally, the value functions can be updated using the following gradient:
vθv 2
∞
XγτR(st+τ,g) - Vk(st, g)
τ=0
21
+ vθsw 2
∞
X γτR(st+τ, g) - Vksw(st, et, g)
τ=0
(9)
2
To increase the episode efficiency, after running an episode, we conduct n mini-batch updates where
n is sampled from a Poisson distribution with λ = 4, similar to Wang et al. (2017). Note that one
can also apply other common policy optimization methods, e.g., A3C (Mnih et al., 2016), to our
model. We leave this as future work to evaluate the efficiency of different methods when using our
model.
Optimizing all three sub-policies together leads to unstable learning. To avoid this, we apply a
simple alternating update procedure. For each set of M iterations, we keep two of the sub-policies
fixed and train only the single policy that remains. When we reach M iterations, we switch the
policy that is trained. For all experiments in this paper, we use M = 500. This alternating update
procedure is used within both phases of curriculum learning.
4.2	Learning an STG
If at any point in the aforementioned training process the agent receives a positive reward after an
episode, we update the stochastic temporal grammar. ρk and qk of the STG are both initialized to
be uniform distributions. Since the STG is a finite state Markov chain over tuples het, gt0i, we use
maximum likelihood estimation (MLE) to update the distributions (Baum & Petrie, 1966). As the
training progresses, the STG starts to guide the exploration.
To avoid falling into local minima in the early stages of training, itis important to encourage random
exploration in early episodes. Based on our experiments, we find that using -greedy suffices.
5	Experiments
5.1	Game Environment and Task Specifications
Figure 3 (left) shows the two room environment in Minecraft that we created using the Malmo
platform (Johnson et al., 2016). In each episode, an arbitrary number of blocks with different colors
(totaling 6 colors in our experiments) are randomly placed in one of the two rooms. The agent is
initially placed in the same room with the items. We consider five sets of tasks: i) G(0) = {“Find x”},
walking to the front of a block with color x, ii) G(1) = {“Get x”}, picking up a block with color
x, iii) G(2) = {“Put x”}, putting down a block with color x, iv) G(3) = {“Stack x”}, stacking two
blocks with color x together, and v) G(4) = {’Put x on y’}, putting a block with color x on top of
6
Published as a conference paper at ICLR 2018
Training environment Unseen environment
Figure 3: Room layout. Left: training environment; right: unseen rooms for testing.
a block with a different color y. In total, there are 54 tasks. An agent can perform the following
actions: “move forward,” “move backward,” “move left,” “move right,” “turn left,” “turn right,”
“pick up,” “put down.”
Without loss of generality, we assume the following skill acquisition order: Gk = ∪kκ=1G(κ), ∀k =
0, 1, 2, 3, 4, which is a natural way to increase skill sets. One may also alter the order, and the main
conclusions shall still hold. This results in policies {πk : k = 0, 1, 2, 3, 4} for these four task sets.
For the last task set, we hold out 6 tasks out of all 30 tasks (i.e., 3 pairs of colors out of 15 color
combinations) for testing and the agent will not be trained on these 6 tasks.
We adopt a sparse reward function: when reaching the goal of a task, the agent gets a +1 reward;
when generating an instruction g0 that is not executable in current game (e.g., trying to find an object
that does not exist in the environment), we give a -0.5 reward; otherwise, no reward will be given.
Whenever a non-zero reward is given, the game terminates. Note that the negative reward is only
given during training.
5.2	Implementation Details
We specify the architecture of the modules in our model in Appendix B, where the visual and in-
struction encoding modules have the same architectures as the ones in Hermann et al. (2017). We
train the network with RMSProp (Tieleman & Hinton, 2012) with a learning rate of 0.0001. We set
the batch size to be 36 and clip the gradient to a unit norm. For all tasks, the discounted coefficient
is γ = 0.95. For the 2-phase curriculum learning, we set the average reward threshold to be 0.9
(average rewards are estimated from the most recent 200 episodes of each task).
To encourage random exploration, we apply -greedy to the decision sampling for the global policy
(i.e., only at the top level k at each stage k > 0), where gradually decreases from 0.1 to 0.
Episodes
(a) Learning curves for π1 .
(b) Learning phase 2 for π3 .
Figure 4: Comparison of learning efficiency on two task sets: (a) G1 for global policy π1 and (b) G3
for global policy π3 respectively.
5.3	Learning Efficiency
To evaluate the learning efficiency, we compare our full model with 1) a flat policy (Figure 2a) as
in Hermann et al. (2017) fine-tuned on the terminal policy π0, 2) H-DRLN (Tessler et al., 2017)
and variants of our approach: 3) ours without STG, 4) ours without alternating policy optimization,
and 5) ours without Vksw (s, e, g) (replaced by Vk (s, g) instead). Note that all the rewards have been
converted to the same range, i.e., [0, 1] for the sake of fair comparison.
7
Published as a conference paper at ICLR 2018
Figure 5: Effects of different training protocols.
Table 1: Success rates in different game settings including scenarios seen during training, new
environments and new tasks. All policies are trained in the same environment in seen scenarios.
Method	Seen scenarios					Unseen environments				Unseen tasks
	FindX	Get x	Put X	Stack X	Put Xony	Findx	Get X	Put X	Stack X	Put Xony
Full model	0.995	0.970	1.00	0.955	0.873	0.723	0.648	1.00	0.613	0.792
Flat policy	0.980	0.965	1.00	0	0	0.515	0.450	1.00	0	0
In Figure 4a, we use various methods to train policy π1 for the task set G1 based on the same base
policy π0 . The large dip in the reward indicates that the curriculum learning switches from phase
1 to phase 2. From Figure 4a, we may clearly see that our full model and variants can all converge
within 22,000 episodes, whereas the average reward of the flat policy is still below 0.8 given the
same amount of episodes. In addition, our full model finishes phase 1 significantly faster than other
methods and its curve of average reward maintains notably higher than the remaining ones.
To further examine the learning efficiency during phase 2 when new tasks are added into the training
process, we first pretrain π3 using our full model following our definition of phase 1 in the cur-
riculum learning. We then proceed to learning phase 2 using different approaches all based on this
pretrained policy. As shown in Figure 4b, our full model has the fastest convergence and the highest
average reward upon convergence. By comparing Figure 4a and Figure 4b, we further show that our
full model has a bigger advantage when learning more complex tasks.
Since we have a large number of previously learned tasks, H-DRLN is clearly not able to learn a
descent policy according the results. Note that an H-DRLN can only learn one task at a time, each
of its curves in Figure 4 is for a single task (i.e., “Get white” and “Stack white” respectively).
To demonstrate the effects of our 2-phase curriculum learning and the -0.5 penalty on the training
efficiency, we visualize the learning curves of our model trained without the curriculum learning or
without the penalty along with the one trained with the full protocol in Figure 5. According to the
results, the curriculum learning indeed helps accelerate the convergence, which empirically proves
the importance of encouraging a global policy to reuse relevant skills learned by its base policy. It
also appears that adding the penalty is an insignificant factor on learning efficiency except that it
helps shorten the episode lengths as an episode ends whenever a penalty is given.
5.4	Policy Generalization
Finally, we evaluate how the hierarchical design and encoding tasks by human instructions benefit
the generalization of learned policies in the following three ways.
First, we train π1 in a simpler setting where in each episode, only one item (i.e, the target item of
the given task) is present. We then test the policy π1 for “Get x” tasks in a room where there will be
multiple items serving as distraction and the agent must interact with the correct one. Both the flat
policy and the hierarchical policy can achieve near perfect testing success rate in the simple setting.
However, in the more complex setting, flat policy can not differentiate the target item from other
8
Published as a conference paper at ICLR 2018
items that are also placed in the room (the success rate drops to 29%), whereas our hierarchical
policy still maintains a high success rate (94%). This finding suggests that the hierarchical policy
not only picks up the concept of “find” and “get” skills as the flat policy does, but also inherits the
concept of items from the base policy by learning to utter correct instructions to deploy “find” skill
in the base policy.
Second, we reconfigure the room layout in Figure 3 (left) and test the flat policy and our full model
in the new rooms shown in Figure 3 (right) for various tasks. Both policies are trained in the same
environment. There are multiple items in a room for both training and testing cases. The success
rates are summarized in Table 1. Using the flat policy results in a much bigger drop in the testing
success rate compared to using out full model. This is mainly because that our global policy will
repeatedly call its base policy to execute the same task until the agent finally achieves the goal even
though the trained agent is unable to reach the goal by just one shot due to the simplicity of the
training environment.
Third, we evaluate the learned policy on the 6 unseen tasks for the “Put x on y” tasks as a zero-
short evaluation. The success rate reported in Table 1 suggests that our model is able to learn
the decomposition of human instructions and generate correct hierarchical plans to perform unseen
tasks.
5.5	Policy Interpretability
We visualize typical hierarchical plans of several tasks generated by global policies learned by our
full model in Appendix C (Figure 6 and Figure 7)1. It can been seen from the examples that our
global policies adjust the composed plans in different scenarios. For instance, in the second plan on
the first row, π1 did not deploy base policy π0 as the agent was already in front of the target item at
the beginning of the episode, whereas in the plan on the second row, π1 deployed π0 for the “Find
x” base task twice consecutively, as it did not finish the base task in the first call.
6	Conclusion
In this work, we have proposed a hierarchal policy modulated by a stochastic temporal grammar as
a novel framework for efficient multi-task reinforcement learning through multiple training stages.
Each task in our settings is described by a human instruction. The resulting global policy is able
to reuse previously learned skills for new tasks by generating corresponding human instructions to
inform base policies to execute relevant base tasks. We evaluate this framework in Minecraft games
and have shown that our full model i) has a significantly higher learning efficiency than a flat policy
does, ii) generalizes well in unseen environments, and iii) is capable of composing hierarchical plans
in an interpretable manner.
Currently, we rely on weak supervision from humans to define what skills to be learned in each
training stage. In the future, we plan to automatically discover the optimal training procedures to
increase the task set.
References
Jacob Andreas, Dan Klein, and Sergey Levine. Modular multitask reinforcement learning with
policy sketches. In International Conference on Machine Learning (ICML), 2017.
Pierre-Luc Bacon and Doina Precup. The option-critic architecture. In NIPS Deep Reinforcement
Learning Workshop, 2015.
Leonard E. Baum and Ted Petrie. Statistical inference for probabilistic functions of finite state
markov chains. TheAnnals OfMathematical Statistics, 37(6):1554-1563, 1966.
Yoshua Bengio. Deep learning of representations for unsupervised and transfer learning. In JMLR:
Workshop on Unsupervised and Transfer Learning, 2012.
1A video demo is available at https://youtu.be/pOv2YiV-2XI
9
Published as a conference paper at ICLR 2018
Devendra Singh Chaplot, Kanthashree Mysore Sathyendra, Rama Kumar Pasumarthi, Dheeraj Ra-
jagopal, and Ruslan Salakhutdinov. Gated-attention architectures for task-oriented language
grounding. arXiv preprint arXiv:1706.0723, 2017.
Karl Moritz Hermann, Felix Hill, Simon Green, Fumin Wang, Ryan Faulkner, Hubert Soyer, David
Szepesvari, Wojtek Czarnecki, Max Jaderberg, Denis Teplyashin, Marcus Wainwright, Chris
Apps, Demis Hassabis, and Phil Blunsom. Grounded language learning in a simulated 3d world.
arXiv preprint arXiv:1706.06551, 2017.
Matthew Johnson, Katja Hofmann, Tim Hutton, and David Bignell. The malmo platform for ar-
tificial intelligence experimentation. In International Joint Conference on Artificial Intelligence
(IJCAI), 2016.
MichaI KemPka, Marek Wydmuch, Grzegorz Runc, Jakub Tocze, and WCjciech ja´kowski. Viz-
doom: A doom-based ai research platform for visual reinforcement learning. In IEEE Conference
on Computational Intelligence and Games (CIG), 2016.
Tejas D. Kulkarni, Karthik Narasimhan, Ardavan Saeedi, , and Josh Tenenbaum. Hierarchical deeP
reinforcement learning: Integrating temPoral abstraction and intrinsic motivation. In Advances in
Neural Information Processing Systems (NIPS), 2016.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deeP visuo-
motor policies. Journal of Machine Learning Research ,17(39):1-40, 2016a.
Sergey Levine, Peter Pastor, Alex Krizhevsky, Julian Ibarz, , and Deirdre Quillen. Learning hand-
eye coordination for robotic grasping with deep learning and large-scale data collection. In ”
International Symposium on Experimental Robotics (ISER), 2016b.
Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andy Ballard, Andrea Banino, Misha
Denil, Ross Goroshin, Laurent Sifre, Koray Kavukcuoglu, Dharshan Kumaran, and Raia Had-
sell. Learning to navigate in complex environments. In International Conference on Learning
Representations (ICLR), 2017.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature, 518(7540):529-533, 2015.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International Conference on Machine Learning (ICML), 2016.
Karthik Narasimhan, Tejas Kulkarni, and Regina Barzilay. Language understanding for text-based
games using deep reinforcement learning. In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP), 2015.
Emilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov. Actor-mimic: Deep multitask and trans-
fer reinforcement learning. In International Conference on Learning Representations (ICLR),
2016.
Lerrel Pinto and Abhinav Gupta. Supersizing self-supervision: Learning to grasp from 50k tries and
700 robot hours. In IEEE Conference on Robotics and Automation (ICRA), 2016.
Hamed Pirsiavash and Deva Ramanan. Parsing videos of actions with segmental grammars. In IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 2014.
Andrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins, James Kirk-
patrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell. Policy distil-
lation. In International Conference on Learning Representations (ICLR), 2016.
Zhangzhang Si, Mingtao Pei, Benjamin Yao, and Song-Chun Zhu. Unsupervised learning of event
and-or grammar and semantics from video. In IEEE International Conference on Computer Vision
(ICCV), 2011.
10
Published as a conference paper at ICLR 2018
David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George Van Den Driess-
che, and Julian Schrittwieser et al. Mastering the game of go with deep neural networks and tree
search. Nature, 529(7587):484-489, 2016.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan
Hui, Laurent Sifre, George Van Den Driessche, Thore Graepel, and Demis Hassabis. Mastering
the game of go without human knowledge. Nature, 550(7676):354-359, 2017.
Pei-Hao Su, Pawel Budzianowski, Stefan Ultes, Milica Gasic, and Steve Young. Sample-efficient
actor-critic reinforcement learning with supervised data for dialogue management. In The 18th
Annual SIGdial Meeting on Discourse and Dialogue (SIGDIAL), 2017.
Richard S. Sutton, Doina Precup, and Satinder Singh. Between mdps anad semi-mdps: A framework
fro temporal abstraction in reinforcement learning. Artificial Intelligence, 112:181-211, 1999.
Yee Whye Teh, Victor Bapst, Wojciech Marian Czarnecki, John Quan, James Kirkpatrick, Raia
Hadsell, Nicolas Heess, and Razvan Pascanu. Distral: Robust multitask reinforcement learning.
In International Conference on Learning Representations (ICLR), 2017.
Chen Tessler, Shahar Givony, Tom Zahavy, Daniel J. Mankowitz, and Shie Mannor. A deep hierar-
chical approach to lifelong learning in Minecraft. In AAAI Conference on Artificial Intelligence
(AAAI), 2017.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5—RmsProp: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.
Alexander (Sasha) Vezhnevets, Volodymyr Mnih, John Agapiou, Simon Osindero, Alex Graves,
Orial Vinyals, and Koray Kavukcuoglu. Strategic attentive writer for learning macro-actions. In
Advances in Neural Information Processing Systems (NIPS), 2016.
Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu,
and Nando de Freitas. Sample efficient actor-critic with experience replay. In International
Conference on Learning Representations (ICLR), 2017.
11
Published as a conference paper at ICLR 2018
A		Pseudo Code of Our Algorithms
Algorithm 1 RUN(k, g)		
In O 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24	put: Policy level k, task g ∈ Gk utput: Episode trajectory Γ at the top level policy t — 0 Γ = 0 : Get initial state s0 : repeat :	if k == 1 then Sample at 〜∏k (∙∣st ,g) and execute at :	Get current state st+1 rt - R(st+ι,g) Add hst,αt,rt,∏k(∙∖st,g),g> to Γ :	else :	Sample et and gt0 as in Section 3.3 for using STG as guidance 2:	Sample at 〜∏kug(∙∣st,g) :	if et = 0 then :	// Execute base policy πk-1 by giving instruction gt0 :	RUN(k - 1, gt0) :	else :	Execute at :	end if :	Get current state st+1 ):	rt — R(st+ι,g) :	Add hst,et,g0,αt,rt,∏kw(∙∣st),πinst(∙∣st,g),∏kug(∙∣st,g),gi to Γ :	end if M	t ― t + 1 : until t > T or rt 6= 0	
		
Algorithm 2 Learning global policy and STG at stage k > 0		
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29	: : : : : : : : : : : : : : : : : : : :	Specify λ, maximum training iterations N, alternating update rotation frequency M, and reward threshold Rmin Initialize total replay memory D — 0 and its subset for positive episodes D+ — 0 Initialize current iteration id i — 0 and set current updating term to be τ — 1 Initialize parameters of policies and value functions Θ = hθsw , θinst, θaug , θv , θvswi Initialize distributions of the STG as uniform distributions repeat Determine current learning phase by comparing average rewards of tasks in Gk-1 with Rmin if in curriculum learning phase 1 then Sample a task g from base task set Gk-1 else Sample a task g from global task set Gk end if //Run an episode Γ — RUN(k, g) D ― D ∪ Γ if the maximum reward in Γ is +1 then D+ — D+ ∪ Γ Re-estimate the distributions of the STG based on updated D+ by MLE end if Sample n 〜Possion(λ) for j ∈ {1, ∙∙∙ ,n} do Sample a mini-batch S from D Update Θ based on (9) and the τ-th term in (8) i—i+1 if i%M = 0 then τ — τ%3 + 1 end if end for until i ≥ N
12
Published as a conference paper at ICLR 2018
B Architectures of Modules
The architecture designs of all modules in our model shown in Figure 2 are as follows:
Visual Encoder extracts feature maps from an input RGB frame with the size of 84 × 84 through
three convolutional layers: i) the first layer has 32 filters with kernel size of 8 × 8 and stride of 4; ii)
the second layer has 64 filters with kernel size of 4 × 4 and stride of 2; iii) the last layer includes 64
filters with kernel size of 3 × 3 and stride of 1. The feature maps are flatten into a 3136-dim vector.
We reduce the dimension of this vector to 256 by a fully connected (FC) layer resulting a 256-dim
visual feature as the final output of this module.
Instruction Encoder first embeds each word into a 128-dim vector and combines them into a single
vector by bag-of-words (BOW). Thus the output of this module is a 128-dim vector. For more
complex instructions such as “Put x on y”, we replace BOW by a GRU with 128 hidden units.
Fusion layer simply concatenates the encoded visual and language representations together and
outputs 384-dim fused representation. We then feed this 384-dim vector into an LSTM with 256
hidden units. The hidden layer output of the LSTM is served as the input of all policy modules and
value function modules.
Switch Policy module has a FC layer with output dimension of 2 and a softmax activation to
get ∏kw(e∣s,g). Instruction Policy module has two separate FC layers, both of which are acti-
vated by softmax to output the distribution of skill, pskkill(uskill|s, g), and the distribution of item,
Pkem(Uitem|s,g), respectively. Augmented Policy module outputs ∏aug(α∣s,g) also through a FC
layer and softmax activation. The two Value Function modules, V (s, g) and V sw(s, e, g), all have
a scalar output through a FC layer.
Finally, the Selector module selects the action sampled from Augmented Policy module or Base
Policy module based on the switching decision sampled from the Switch Policy module.
C Composed Hierarchical Plans
Figure 6 and Figure 7 show several plans for different tasks composed by executing our hierarchical
policies.
13
Published as a conference paper at ICLR 2018
Task: Tind x”
Tind x”
÷TA
Item in hands None
None
Task: *tPut xh
“Put x”
2
V
Action:
put down
move forward
Action:
pick up
Egocentric view
Item in hands
Egocentric view
Item in hands
pick up
Figure 6: Samples of typical hierarchical plans for different tasks composed by our global policies.
Note that all tasks must start from the top-level policy. The branches are ordered from left to right
in time indicating consecutive steps carried out by a policy. We also show the egocentric view and
the item in hands at critical moments for a real episode example.
14
Published as a conference paper at ICLR 2018
Egocentric view
Item in hands
Egocentric view
Item in hands
put down
Figure 7: Hierarchical plans for “Put x on y” tasks. Top: an example of performing trained tasks;
bottom: an example of generalizing the plan composition to unseen tasks.
Action:
pick up
Action:
pick up
Action:
put down
15