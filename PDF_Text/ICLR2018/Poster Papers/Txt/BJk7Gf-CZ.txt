Published as a conference paper at ICLR 2018
Global optimality conditions for deep neural
NETWORKS
Chulhee Yun, Suvrit Sra & Ali Jadbabaie
Massachusetts Institute of Technology
Cambridge, MA 02139, USA
{chulheey,suvrit,jadbabai}@mit.edu
Ab stract
We study the error landscape of deep linear and nonlinear neural networks with
the squared error loss. Minimizing the loss of a deep linear neural network is a
nonconvex problem, and despite recent progress, our understanding of this loss
surface is still incomplete. For deep linear networks, we present necessary and
sufficient conditions for a critical point of the risk function to be a global mini-
mum. Surprisingly, our conditions provide an efficiently checkable test for global
optimality, while such tests are typically intractable in nonconvex optimization.
We further extend these results to deep nonlinear neural networks and prove sim-
ilar sufficient conditions for global optimality, albeit in a more limited function
space setting.
1	Introduction
Since the advent of AlexNet (Krizhevsky et al., 2012), deep neural networks have surged in pop-
ularity, and have redefined the state-of-the-art across many application areas of machine learning
and artificial intelligence, such as computer vision, speech recognition, and natural language pro-
cessing. However, a concrete theoretical understanding of why deep neural networks work well in
practice remains elusive. From the perspective of optimization, a significant barrier is imposed by
the nonconvexity of training neural networks. Moreover, it was proved by Blum & Rivest (1988)
that training even a 3-node neural network to global optimality is NP-Hard in the worst case, so
there is little hope that neural networks have properties that make global optimization tractable.
Despite the difficulties of optimizing weights in neural networks, the empirical successes suggest
that the local minima of their loss surfaces could be close to global minima; and several papers have
recently appeared in the literature attempting to provide a theoretical justification for the success
of these models. For example, by relating neural networks to spherical spin-glass models from
statistical physics, Choromanska et al. (2015) provided some empirical evidence that the increase of
size of neural networks makes local minima close to global minima.
Another line of results (Yu & Chen, 1995; Soudry & Carmon, 2016; Xie et al., 2016; Nguyen & Hein,
2017) provides conditions under which a critical point of the empirical risk is a global minimum.
Such results roughly involve proving that if full rank conditions of certain matrices (as well as some
additional technical conditions) are satisfied, derivative of the risk being zero implies loss being
zero. However, these results are obtained under restrictive assumptions; for example, Nguyen &
Hein (2017) require the width of one of the hidden layers to be as large as the number of training
examples. Soudry & Carmon (2016) and Xie et al. (2016) require the product of widths of two
adjacent layers to be at least as large as the number of training examples, meaning that the number
of parameters in the model must grow rapidly as we have more training data available. Another
recent paper (Haeffele & Vidal, 2017) provides a sufficient condition for global optimality when the
neural network is composed of subnetworks with identical architectures connected in parallel and a
regularizer is designed to control the number of parallel architectures.
Towards obtaining a more precise characterization of the loss-surfaces, a valuable conceptual simpli-
fication of deep nonlinear networks is deep linear neural networks, in which all activation functions
are linear and the output of the entire network is a chained product of weight matrices with the
input vector. Although at first sight a deep linear model may appear overly simplistic, even its opti-
1
Published as a conference paper at ICLR 2018
mization is nonconvex, and only recently theoretical results on this problem have started emerging.
Interestingly, already in 1989, Baldi & Hornik (1989) showed that some shallow linear neural net-
works have no local minima. More recently, Kawaguchi (2016) extended this result to deep linear
networks and proved that any local minimum is also global while any other critical point is a saddle
point. Subsequently, Lu & Kawaguchi (2017) provided a simpler proof that any local minimum
is also global, with fewer assumptions than (Kawaguchi, 2016). Motivated by the success of deep
residual networks (He et al., 2016a;b), Hardt & Ma (2017) investigated loss surfaces of deep linear
residual networks and showed every critical point is a global minimum in a near-identity region;
subsequently, Bartlett et al. (2017) extended this result to a nonlinear function space setting.
1.1	Our contributions
Inspired by this recent line of work, we study deep linear and nonlinear networks, in settings either
similar to or more general than existing work. We summarize our main contributions below.
•	We provide both necessary and sufficient conditions for a critical point of the empirical risk to be a
global minimum. Specifically, Theorem 2.1 shows that if the hidden layers are wide enough, then
a critical point of the risk function is a global minimum if and only if the product of all parameter
matrices is full-rank. In Theorem 2.2, we consider the case where some hidden layers have smaller
width than both the input and output layers, and again provide necessary and sufficient conditions
for global optimality. In comparison, Kawaguchi (2016) only proves that every critical point of the
risk is either a global minimum or a saddle; it is an “existence” result without any computational
implication. In contrast, we present efficiently checkable conditions for distinguishing the two
different types of critical points; we can even use these conditions while running optimization to
test whether the critical points we encounter are saddle points or not, if desired. It is also worth
noting that such tests are intractable for general nonconvex optimization (Murty & Kabadi, 1987).
•	Under the same assumption as (Hardt & Ma, 2017) on the data distribution, namely, a linear model
with Gaussian noise, we can modify Theorem 2.1 to handle the population risk. As a corollary,
we not only recover Theorem 2.2 in (Hardt & Ma, 2017), but also extend it to a strictly larger set,
while removing their assumption that the true underlying linear model has a positive determinant.
•	Motivated by (Bartlett et al., 2017), we extend our results on deep linear networks to obtain suf-
ficient conditions for global optimality in deep nonlinear networks, although only via a function
space view; these are presented in Theorems 4.1 and 4.2.
2	Global optimality conditions for deep linear neural networks
In this section, we describe the problem formulation and notations for deep linear neural networks,
state main results (Theorems 2.1 and 2.2), and explain their implication.
2.1	Problem formulation and notation
Suppose we have m input-output pairs, where the inputs are of dimension dx and outputs of dimen-
sion dy. Let X ∈ Rdx×m be the data matrix and Y ∈ Rdy×m be the output matrix. Suppose we
have H hidden layers in the network, each having width d1 , . . . , dH . For notational simplicity we
let d0 = dx and dH+1 = dy . The weights between adjacent layers can be represented as matri-
ces Wk ∈ Rdk×dk-1, for k = 1, . . . , H + 1, and the output of the network can be written as the
product of weight matrices Wh+i,...,Wi and data matrix X: Wh+iWh •…WiX. We consider
minimizing the summation of squared error loss over all data points (i.e. empirical risk),
minimize L(W):= 1 ∣∣WH+1WH …W1X - YkF ,	(1)
where W is a shorthand notation for the tuple (W1, . . . , WH+1).
Assumptions. We assume that dx ≤ m and dy ≤ m, and that XXT and YXT have full ranks.
These assumptions are common when we consider supervised learning problems with deep neural
networks (e.g. Kawaguchi (2016)). We also assume that the singular values ofYXT(XXT)-1X are
all distinct, which is made for notational simplicity and can be relaxed without too much difficulty.
Notation. Given a matrix A, let σmax(A) and σmin (A) denote the largest and smallest singular
values ofA, respectively. Let row(A), col(A), null(A), rank(A), and kAkF be respectively the row
2
Published as a conference paper at ICLR 2018
space, column space, null space, rank, and Frobenius norm of matrix A. Given a subspace V of Rn ,
we denote V ⊥ as its orthogonal complement. Given a set V , let V c denote the complement of V .
Let us denote k := mini∈{0,...,H +1} di, and define p ∈ argmini∈{0,...,H+1} di. That is, p is any
layer with the smallest width, and k = dp is the width of that layer. Here, p might not be unique,
but our results hold for any layer P with smallest width. Notice also that the product Wh+i •…Wi
can have rank at most k. Let YXT (XXT)-1X = UΣVT be the singular value decomposition of
YXT(XXT)-1X ∈ Rdy ×dx. Let U ∈ Rdy ×k be a matrix consisting of the first k columns of U.
2.2	Necessary and sufficient conditions for global optimality
We now present two main theorems for deep linear neural networks. The theorems describe two
sets, one for the case k = min{dx , dy} and the other for k < min{dx , dy}, inside which every
critical point of L(W) is a global minimum. Moreover, the sets have another remarkable property
that every critical point outside of these sets is a saddle point. Previous works (Kawaguchi, 2016;
Lu & Kawaguchi, 2017) showed that any critical point is either a global minimum or a saddle point,
without providing any condition to distinguish between the two; here, we take a step further and
partition the domain of L(W) into two sets clearly delineating one set which only contains global
minima and the other set with only saddle points.
Theorem 2.1.	Ifk = min{dx , dy}, define the following set
Vi ：= {(Wi,..., WH+i)：rank(WH+i …Wi) = k}.
Then, every critical point of L(W) in Vi is a global minimum. Moreover, every critical point of
L(W) in Vic is a saddle point.
Theorem 2.2.	Ifk < min{dx, dy}, define the following set
V1 ：= {(Wi,...,Wh+i) : rank(WH+i ∙∙∙ Wi) = k,Col(WH+i …Wp+i) = col(U)}.
Then, every critical point of L(W) in V1 is a global minimum. Moreover, every critical point of
L(W) in V1c is a saddle point.
Theorems 2.1 and 2.2 provide necessary and sufficient conditions for a critical point of L(W) to be
globally optimal. From an algorithmic perspective, they provide easily checkable conditions, which
we can use to determine if the critical point the algorithm encountered is a global optimum or not.
Given that L(W) is nonconvex, it is interesting to have such efficient tests for global optimality,
which is not possible in general (Murty & Kabadi, 1987).
In Hardt & Ma (2017), the authors consider minimizing population risk of linear residual networks:
minimize 1 Eχ,y [∣∣(I + Wh+i)…(I + Wi)x — y∣∣F],
where dx = d1
dH = dy = d. They assume that x is drawn from a zero-mean distribution
with a fixed covariance matrix, and y = Rx + ξ where ξ is iid standard Gaussian noise and R is
the true underlying matrix with det(R) > 0. With these assumptions they prove that whenever
σmax(Wi) < 1 for all i, any critical point is a global minimum (Hardt & Ma, 2017, Theorem 2.2).
Under the same assumptions on data distribution, we can slightly modify Theorem 2.1 to derive
a population risk counterpart, and in fact notice that the result proved in Hardt & Ma (2017) is a
corollary of this modification because having σmax(Wi) < 1 for all i is a sufficient condition for
(I + Wh+i)…(I + Wi) having full rank. Moreover, notice that We can remove the assumption
det(R) > 0 which was required by Hardt & Ma (2017). We state this special case as a corollary:
Corollary 2.3 (Theorem 2.2 of Hardt & Ma (2017)). Under assumptions on data distribution as
described above, any critical point of 11 Eχ,y [k(I + WH+i)…(I + Wi)x 一 yk，is a global min-
imum ifσmax(Wi) < 1 for all i.
We also note in passing that the classical problem of matrix factorization minU,V UV T 一 Y 1F is
a special case of deep linear neural networks, so our theorems can also be directly applied.
Remarks. The previous result (Kawaguchi, 2016) assumed dy ≤ dx and showed that: 1) every
local minimum is a global minimum, and 2) any other critical point is a saddle point. A subsequent
paper by Lu & Kawaguchi (2017) proved 1) without the assumption dy ≤ dx , but as far as we
know there is no result showing 2) in the case of dy > dx . We provide the proof for this case
in Lemma B.1. In fact, we propose an alternative proof technique for handling degenerate critical
points, which is much simpler than the technique presented by Kawaguchi (2016).
3
Published as a conference paper at ICLR 2018
3 Analysis of deep linear networks
In this section, we provide proofs for Theorems 2.1 and 2.2.
3.1	Solutions of the relaxed problem
We first analyze the globally optimal solution of a “relaxation” of L(W), which turns out to be very
useful while proving Theorems 2.1 and 2.2. Consider the relaxed risk function
LO(R) = 2 IlRX - YIIF,
where R ∈ Rdy ×dx and rank(R) ≤ k. For any W, the product WH+1 Wh •…Wi has rank at most
k and setting R to be this product gives the same loss values: Lo(Wh+iWh …Wi) = L(W).
Therefore, L0 is a relaxation of L and
inf	L0(R) ≤ inf L(W).
Rιank(R)≤k	W
This means that if there exists W such that L(W) = infRmnk(R)≤k Lo(R), then W is a global
minimum of the function L. This observation is very important in proofs; we will show that inside
certain sets, any critical point W of L(W) must satisfy R = Wh+i •…Wi, where R* is a global
optimum of Lo(R). This proves that L(W) = L0(R*) = infR=rank(R)≤k Lo(R), thus showing that
W is a global minimum of L.
By restating this observation as an optimization problem, the solution of problem in (1) is bounded
below by the minimum value of the following:
minimize 2 ∣∣RX 一 Y∣∣F
sub ject to rank(R) ≤ k.
(2)
In case where k = min{dx , dy}, (2) is actually an unconstrained optimization problem. Note that
L0 is a convex function of R, so any critical point is a global minimum. By differentiating and
setting the derivative to zero, we can easily get the unique globally optimal solution
R* = YX T (XX T )-i.	(3)
In case of k < min{dx , dy }, the problem becomes non-convex because of the rank constraint, but
its exact solution can still be computed easily. We present the solution of this case as a proposition
and defer the proof to Appendix C due to its technicalities.
Proposition 3.1. Suppose k < min{dx , dy}. Then the optimal solution to (2) is
R* = U^ U^ T YX T (XX T )-i,	(4)
which is the orthogonal projection of YX T (XX T )-i onto the column space of U^.
3.2	PARTIAL DERIVATIVES OF L(W)
By simple matrix calculus, we can calculate the derivatives of L(W) with respect to Wi, for i =
1, . . . , H + 1. We present the result as the following lemma, and defer the details to Appendix C.
Lemma 3.2. The partial derivative ofL(W) with respect to Wi is given as
∂L
浙=Wi+i ∙ ∙ ∙ WH +i(Wh+iWh …WiX 一 Y)XTWT …W乙，	(5)
∂Wi
for i = 1, . . . , H + 1.
This result will be used throughout the proof of Theorems 2.1 and 2.2. For clarity in notation, note
that when i = 1, WT •…W(T is just an identity matrix in Rdχ×dx. Similarly, when i = H + 1,
wH+2 …WH+i is an identity matrix in Rdy ×dy .
We also state an elementary lemma which proves useful in our proofs, whose proof we defer to
Appendix C.
Lemma 3.3. 1. For any A ∈ Rm×n and B ∈ Rn×l where m ≥ n,
∣AB∣2F ≥ σm2 in(A) ∣B∣2F.
2. For any A ∈ Rm×n and B ∈ Rn×l where n ≤ l,
∣AB∣2F ≥ σm2 in(B) ∣A∣2F.
4
Published as a conference paper at ICLR 2018
3.3 Proof of Theorem 2.1
We prove Theorem 2.1, which addresses the case k = min{dx, dy}. First, recall that the set defined
in Theorem 2.1 is
VI= {(W1, ... , WH+1) : rank(WH+ι ∙ ∙ ∙ WI) = k} .
As seen in (3), the unique minimum point of L0 has rank k. So, no point W ∈ V1c can be a global
minimum of L. Therefore, by Kawaguchi (2016, Theorem 2.3.(iii)) and Lemma B.1, any critical
point in V1c must be a saddle point.
For the rest of our proof, we need to consider two cases: dy ≤ dx and dx ≤ dy . If dx = dy , both
cases work. The outline of the proof is as follows: we define a new set W, show that any critical
point in the set W is a global minimum, and then show that every W ∈ V1 is also in W for some
> 0. This proves that any critical point of L(W) in V1 is also a critical point in W for some
> 0, hence a global minimum.
The following proposition proves the first step:
Proposition 3.4. Assume that k = min{dx, dy}. For any > 0, define the following set:
W = ( {(Wl, . . . , WH +1) : σmin(WH+ι …W2) ≥ e} , if dy ≤ dχ,
J= [{(Wi,∙∙∙,Wh +l): σmin(WH …Wι) ≥ f} , if dχ ≤ dy.
Then any critical point of L(W) in W is a global minimum point.
Proof. (If dy ≤ dχ) Consider (5) in the case of i = 1. We can observe that WT •…WH十]∈ Rd1 ×dy
and that d1 ≥ dy. Then by Lemma 3.3.1,
≥ σ2ιin(WH+1 …W2) Il (WH+1WH …WIX - Y)XTllF
F
∂L
∂W1
≥ e2 ∣∣(Wh+1Wh …W1X — Y)XT∣∣F .
By the above inequality, any critical point in W satisfies
F ∂L
∀i,∂Wi
0 ⇒ (Wh+1Wh …W1X - Y)XT = 0,
which means that Wh+↑Wh •…W1 = YXT (XXT )-1. The product is the unique globally optimal
solution (3) of the relaxed problem in (2), so W is a global minimum point of L.
∂L
∂WH+1
(If dχ ≤ dy) Consider (5) for i = H + 1. We can observe that WT •…WH ∈ Rdx×dH and that
dx ≤ dH . Then by Lemma 3.3.2,
≥ e2 ∣∣(Wh+1Wh …W1X — Y)Xt∣∣F ,
F
and the rest of the proof flows in a similar way as the previous case.	□
The next proposition proves the theorem:
Proposition 3.5. For any point W ∈ V1, there exists an > 0 such that W ∈ W.
Proof. Define a new set W, a “limit” version (as → 0) of W , as
W = ({(W1,..., Wh+1) : rank(WH+1 ∙ ∙ ∙ W2) = dy} , if dy ≤ dχ,
:= l{(W1,..., Wh+1 ):rank(WH …W1) = dχ} ,	if dχ ≤ dy.
We show that V1 ⊂ W by showing that Wc ⊂ V1c. Consider
WC= ({(W1,..., Wh+1) : rank(WH+1 …W?) < dy} , if dy ≤ dχ,
=[{(W1,...,WH+Jrank( WH …W。< dχ} ,	if dχ ≤ dy.
Then any W ∈ WC must have rank(WH+1 •…WQ < min{dχ, dy} = k, so W ∈ Vf. Thus, any
W ∈ V1 is also in W, so either rank(WH+1 •…W2) = dy or rank(WH .…WQ = dχ, depending
on the cases. Then, we can set
e = (σmin(WH+1 …W2), if dy ≤ dx,
e [σmin(WH …W1),	if dχ ≤ dy.
We always have e > 0 because the matrices are full rank, and We can see that W ∈ We.	□
5
Published as a conference paper at ICLR 2018
3.4 Proof of Theorem 2.2
In this section we prove Theorem 2.2, which tackles the case k < min{dx , dy }. Note that this
assumption also implies that 1 ≤ p ≤ H.
As for the proof of Theorem 2.1, define
Vi ：= {(Wι,..., WH+i)：rank(WH+i …Wi) = k}.
The globally optimal point of the relaxed problem (2) has rank k, as seen in (4). Thus, any point
outside of Vi cannot be a global minimum. Then, by Kawaguchi (2016, Theorem 2.3.(iii)) and
Lemma B.1, it follows that any critical point in Vic must be a saddle point. The remaining proof
considers points in Vi .
For this section, let us introduce some additional notations to ease presentation. Define
E := (WH+1 …W1X - Y)XT ∈ Rdy ×dχ,
Ai :=	Wi+1	• —	WH +1	∈	Rdi×dy,	Bi ：= WT	…WTLI	∈	Rdχ×di-1,	i = 1,...,H + 1,
So that ∂WL = AiEBi. Notice that AH+1 and B1 are identity matrices.
Now consider any tuple W ∈ Vi. Since the full product Wh+i ∙ ∙ ∙ Wi has rank k, any partial
products Ai and Bi must have rank(Ai) ≥ k and rank(Bi) ≥ k, for all i. Then, consider Ap ∈
Rk×dy and Bp+i ∈ Rdx×k. Since rank(Ap) ≤ k and rank(Bp+i) ≤ k, we can see that rank(Ap) =
rank(Bp+i) = k. Also, notice that Ai = Wi+iAi+i and Bi+i = BiWi, so that
rank(Ai) ≤ rank(A2) ≤ ∙∙∙ ≤ rank(Ap) and rank(BH+1) ≤ rank(BH) ≤ ∙∙∙ ≤ rank(Bp+i).
However, we have k ≤ rank(Ai) and k ≤ rank(BH+i), so the ranks are all identically k. Also,
rοw(Ai) ⊂ r0w(A2) ⊂ ∙∙∙ ⊂ rοw(Ap) and Col(BH+i) ⊂ Col(BH) ⊂ ∙∙∙ ⊂ Col(Bp+i),
but it was just shown that the these spaces have the same dimensions, which equals k, meaning
row(Ai) = row(A2) = .…=row(Ap) and Col(BH+i) = Col(BH) = .…=Col(Bp+i).
Using this observation, we can now state a proposition showing necessary and sufficient conditions
for a tuple W ∈ Vi to be a critical point of L(W).
Proposition 3.6. A tuple W ∈ Vi is a critical point of L if and only if ApE = 0 and E Bp+i = 0.
Proof. (If part) ApE = 0 implies that Col(E) ⊂ row(Ap)⊥ = … =row(Ai)⊥, so ∂∂-L =
AiEBi = 0 ∙ Bi = 0, for i = 1,...,ρ. Similarly, EBp+i = 0 implies row(E) ⊂ Col(Bp+i)⊥ =
∙∙∙ = Col(BH+i)⊥, so ∂WL = AiEBi = Ai ∙ 0 = 0 for i = P + 1,...,H + 1.
(Only if part) We have ∂WL = AiEBi = 0 for all i. This means that
Col(E Bi) ⊂ row(Ai)⊥ = row(Ap)⊥ for i = 1, . . . ,p
row(AiE) ⊂ Col(Bi)⊥ = Col(Bp+i)⊥ for i = p + 1, . . . , H + 1.
Now recall that Bi and AH+i are identity matrices, so Col(E) ⊂ row(Ap)⊥ and row(E) ⊂
Col(Bp+i)⊥, which proves ApE = 0 and EBp+i = 0.	□
Now we present a proposition that specifies the necessary and sufficient condition in which a critical
point of L(W) in Vi is a global minimum. Recall that when we take the SVD YXT(XXT)-iX =
UΣVT, U ∈ Rdy ×k is defined to be a matrix consisting of the first k columns of U.
Proposition 3.7. A critical point W ∈ Vi of L(W) is a global minimum point if and only if
Col(WH+i •…Wp+i) = row(Ap) = Col(U).
Proof. Since W is a critical point, by Proposition 3.6 we have ApE = 0. Also note from the
definitions of Ai’s and Bi’s that WH+i •…Wi = ATBp+i, so
ApE = Ap(ApTBpT+iX - Y)XT = ApApTBpT+iXXT - ApYXT = 0.
6
Published as a conference paper at ICLR 2018
Because rank(Ap) = k, and ApApT ∈ Rk×k is invertible, so Bp+1 is determined uniquely as
BpT+1 = (ApApT)-1ApYXT(XXT)-1,
thus
Wh+i ∙∙∙ Wi = AT BT+ι = AT (ApAT )-1ApYXT (XX T)-
Comparing this with (4), W is a global minimum solution if and only if
UUtYXt(XXt)-i = Wh+1 …Wi = AT(ApAT)-1ApYXT(XXT)-1.
This equation holds if and only if AT(ApAT)-iAp = UUT, meaning that they are projecting
YXT(XXT)-i onto the same subspace. The projection matrix ApT(ApApT)-iAp is onto row(Ap),
while UUT is onto col(U7). From this, We conclude that W is a global minimum point if and only if
row(Ap) = Col(U).	□
From Proposition 3.7, we can define the set V2 that appeared in Theorem 2.2, and conclude that
every critical point of L(W) in V2 is a global minimum, and any other critical points are saddle
points.
4 Extension to deep nonlinear neural networks
In this section, we present some sufficient conditions for global optimality for deep nonlinear neu-
ral networks via a function space view. Given a smooth nonlinear function h that maps input to
output, Bartlett et al. (2017) described a method to decompose it into a number of smooth nonlinear
functions h = hH+i ◦ ∙∙∙ ◦ hi where his are close to identity. Using FreChet derivatives of the
population risk with respect to each function hi , they showed that when all hi ’s are close to identity,
any critical point of the population risk is a global minimum. One can see that these results are direct
generalization of Theorems 2.1 and 2.2 of Hardt & Ma (2017) to nonlinear networks and utilize the
classical “small gain” arguments often used in nonlinear analysis and control (Khalil, 1996; Zames,
1966). Motivated by this result, we extended Theorem 2.1 to deep nonlinear neural networks and
obtained sufficient conditions for global optimality in function space.
4.1	Problem formulation and notation
Suppose the data X ∈ Rdx and its corresponding label Y ∈ Rdy are drawn from some distribution.
Notice that in this section, X and Y are random vectors instead of matrices. We want to predict Y
given X with a deep nonlinear neural network that has H hidden layers. We express each layer of
the network as functions hi : Rdi-1 → Rdi, so the entire network can be expressed as a composition
of functions: hH+i ◦ hH ◦•••◦ hi. Our goal is to obtain functions hi,..., hH+i that minimize the
population risk functional:
L(h) = L(hi,…，hH+i) := 2E [∣∣hH+i。…。hi(X) - Yk2i ,
where h is a shorthand notation for (hi, . . . , hH+i). It is well-known that the minimizer of squared
error risk is the conditional expectation of Y given X, which we will denote h*(x) = E [Y | X = x].
With this, we can separate the risk functional into two terms
L(h) = 1E hkhH+ i。…。hi (X )—h* (X )k2 i + C,
where the constant C denotes the variance that is independent of hi, . . . , hH+i. Note that if hH+i 。
…。hi = h* almost surely, the first term in L(h) vanishes and the optimal value L* of L(K) is C.
Assumptions. Define the function spaces as the following:
F := hh : Rdx → Rdy | h is differentiable, h(0) = 0, and sup k,(x)k2 < ∞ I
x kxk2
Fi := hh : Rdi-I → Rdi | h is differentiable, h(0) = 0, and sup k，(x)k2 < ∞
x kxk2
7
Published as a conference paper at ICLR 2018
where Fi are defined for all i = 1,..., H + 1. Assume that h ∈ F, and that We are optimizing
L(h) with h1 ∈ F1, . . . , hH+1 ∈ FH+1. In other words, the functions in F, F1, . . . , FH+1 are
differentiable and show sublinear growth starting from 0. Notice that h*ι ◦•••◦ hi ∈ F, because
a composition of differentiable functions is also differentiable, and a composition of sublinear func-
tions is also sublinear. We also assume that di ≥ min{dx, dy} for all i = 1, . . . , H + 1, which is
identical to the assumption k = min{dx, dy} in Theorem 2.1.
Notation. To simplify multiple composition of functions, we denote hi.j = h ◦ hi-ι ◦•••◦ hj+i ◦
hj. As in the matrix case, h0:1 and hH+1:H+2 mean identity maps in Rdx and Rdy, respectively.
Given a function f, let J [f](x) be the Jacobian matrix of function f evaluated at x. Let Dhi [L(h)]
be the FreChet derivative of L(h) with respect to h evaluated at h. The FreChet derivative D^ [L(h)]
is a linear functional that maps a function (direction) η ∈ Fi to a real number (directional derivative).
4.2	Sufficient conditions for global optimality
Here, we present two theorems which give sufficient conditions for a critical point (Dhi [L(h)] = 0
for all i) in the function space to be a global optimum. The proofs are deferred to Appendix A.
Theorem 4.1.	Consider the case dx ≥ dy. If there exists > 0 such that
1.	J[hH+i：2](z) ∈ Rdy ×d1 has σmin( J[hH+i：2](z)) ≥ E for all Z ∈ Rd1,
2.	hH+i:2 (z) is twice-differentiable,
then any critical point of L(h), in terms of Dh1 [L(h)], . . . , DhH+1 [L(h)], is a global minimum.
Theorem 4.2.	Consider the case dx ≤ dy. Assume that there exists some j ∈ {1, . . . , H + 1} such
that dx = dj-i and dy ≤ dj. If there exist Ei, E2 > 0 such that
1.	hj-i:i : Rdx → Rdj-1 = Rdx is invertible,
2.	hj-i:i satisfies khj-i:i(u)k2 ≥ Ei kuk2 for all u ∈ Rdx,
3.	J[hH+i：j+i](z) ∈ Rdy ×dj has σmin( J[hH+1j+1](z)) ≥ ⑦ forall Z ∈ Rdj,
4.	hH+i:j+i(z) is twice-differentiable,
then any critical point of L(h), in terms of Dh1 [L(h)], . . . , DhH+1 [L(h)], is a global minimum.
Note that these theorems give sufficient conditions, whereas Theorems 2.1 and 2.2 provide necessary
and sufficient conditions. So, if the sets we are describing in Theorems 4.1 and 4.2 do not contain
any critical point, the claims would be vacuous. We ensure that there are critical points in the sets,
by presenting the following proposition, whose proof is also deferred to Appendix A.
Proposition 4.3. For each of Theorems 4.1 and 4.2, there exists at least one global minimum solution
of L(h) satisfying the conditions of the theorem.
Discussion and Future work. Theorems 4.1 and 4.2 state that in certain sets of (hi, . . . , hH+i),
any critical point in function space a global minimum. However, this does not imply that any critical
point for a fixed sigmoid or arctan network is a global minimum. As noted in (Bartlett et al., 2017),
there is a downhill direction in function space at any suboptimal point, but this direction might
be orthogonal to the function space represented by a fixed network, and may hence result in local
minima in the parameter space of the fixed architecture.
Understanding the connection between the function space and parameter space of commonly used
architectures is an open direction for future research, and we believe that these results can be good
initial steps from the theoretical point of view. For example, we can see that one of the sufficient
conditions for global optimality is the Jacobian matrix being full rank. Given that a nonlinear func-
tion can locally be linearly approximated using Jacobians, this connection is already interesting. An
extension of the function space viewpoint to cover different architectures or design new architectures
(that have “better” properties when viewed via the function space view) should also be possible and
worth studying.
8
Published as a conference paper at ICLR 2018
Acknowledgments
This research project was supported in parts by DARPA DSO’s Fundamental Limits of Learning
program.
References
Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from
examples without local minima. Neural networks, 2(1):53-58, l989.
Peter Bartlett, Steve Evans, and Phil Long. Deep residual networks: Representation and optimization
properties, 2017. Talk by Peter Bartlett at the Computational Challenges in Machine Learning
Workshop at Simons Institute for the Theory of Computing, Berkeley, CA, USA.
Avrim Blum and Ronald L Rivest. Training a 3-node neural network is NP-complete. In Proceedings
of the 1st International Conference on Neural Information Processing Systems, pp. 494-501. MIT
Press, 1988.
Anna Choromanska, Mikael Henaff, Michael Mathieu, Gerard Ben Arous, and Yann LeCun. The
loss surfaces of multilayer networks. In Artificial Intelligence and Statistics, pp. 192-204, 2015.
Benjamin D Haeffele and Rene Vidal. Global optimality in neural network training. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7331-7339, 2017.
Moritz Hardt and Tengyu Ma. Identity matters in deep learning. In International Conference on
Learning Representations, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
770-778, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European Conference on Computer Vision, pp. 630-645. Springer, 2016b.
Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information
Processing Systems, pp. 586-594, 2016.
Hassan K Khalil. Noninear Systems. Prentice-Hall, New Jersey, 1996.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Haihao Lu and Kenji Kawaguchi. Depth creates no bad local minima. arXiv preprint
arXiv:1702.08580, 2017.
Katta G Murty and Santosh N Kabadi. Some NP-complete problems in quadratic and nonlinear
programming. Mathematical programming, 39(2):117-129, 1987.
Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. In Pro-
ceedings of the 34th International Conference on Machine Learning, volume 70, pp. 2603-2612,
2017.
Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees
for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.
Bo Xie, Yingyu Liang, and Le Song. Diverse neural network learns true target functions. arXiv
preprint arXiv:1611.03131, 2016.
Xiao-Hu Yu and Guo-An Chen. On the local minima free condition of backpropagation learning.
IEEE Transactions on Neural Networks, 6(5):1300-1303, 1995.
George Zames. On the input-output stability of time-varying nonlinear feedback systems part one:
Conditions derived using concepts of loop gain, conicity, and positivity. IEEE transactions on
automatic control, 11(2):228-238, 1966.
9
Published as a conference paper at ICLR 2018
A	Analysis of deep nonlinear networks
A. 1 Notation
In this section, we introduce additional notation that is used in the proofs. To emphasize that
the Frechet derivative DhJL(h)] is a linear functional that outputs a real number, We will write
Dhi [L(h)](η) in an inner-product form hDhi [L(h)], ηi. This notation also helps avoiding confusion
coming from multiple parentheses and square brackets.
There are many different kinds of norms that appear in the proofs. Given a finite-dimensional real
vector v, kvk2 denotes its `2 norm. For a matrix A, its operator norm is defined as kAkop =
SuPx kAAχk2. Let h ∈ F. Then define a “generalized” induced norm for nonlinear functions with
sublinear growth: |向|最=SuPx 卜早2, where the subscript nl is used to emphasize that this norm
is for nonlinear functions. The norm |卜||最 is defined in the same way for Fi,s. Now, given a
linear functional G that maps a function f ∈ Fi to a real number hG, fi, define the operator norm
kGkop =SUPf ∈Fi fl.
A.2 FRECHET DERIVATIVES
By definition OfFrechet derivatives, we have
hDhi [L(h)],ηi = lim LhI …，h + 3,…,h,H+1 - L(h),
→0
where η ∈ Fi is the direction of perturbation and hDhi [L(h)], ηi is the directional derivative along
that direction η. From the definition of L(h),
L(h1, ..., hi + η, ..., hH+1)
=2E hkhH+1:i+1 ◦ (hi + eη) ◦ hi-i：i(X) - h*(X)|同 + C
=2E hkhH+i：i+i(hi：i(X) + eη(hi-rι(X))) - h*(X)|同 + C
=2E h∣∣h∏+1:1(X) + J[hH+i：i+i](hi：i(X))η(hiτι(X)) + O(e2) - h*(X)|同 + C
=L(h) + eE [(hn+i：i(X) - h*(X))tJ[hH+i：i+i](hi：i(X))n(hi-i：i(X))] + O(e2).
Therefore,
hDhi[L(h)],ηi = E [(h∏+i：i(X) - h*(X))T J[hH+i：i+i](hi：i(X))n(hi-i：i(X))] .	(6)
This equation (6) will be used in the proof of Theorems 4.1 and 4.2.
A.3 Proof of Theorem 4.1
From (6), consider Dh1 [L(h)]. For any η ∈ F1,
hDhi [L(h)],ηi = E [(hH+1:1(X) - h*(X))tJ[hH+i：2](hi(X))η(X)].
Let A(X) = J[hH+1:2](h1(X)). Since A(X) has full row rank by assumption, A(X)A(X)T is
invertible. Then define a particular direction
η(X) = A(X)t(A(X)A(X)t)-1(hH+i：i(X) - h*(X)),
so that
hDhi [L(h)],ni = E [khH+1:1(X )-h*(X )k2].
It remains to check if η ∈ Fι. It is easily checked that η(0) = 0 because hH+i：i(0) - h*(0) = 0.
Since J[hH+1:2] is differentiable by assumption and h1 ∈ F1, A(X) is differentiable and A(X)T,
(A(X)A(X)t)-1 are differentiable functions. Also, hn+i：i - h ∈ F, so we can conclude that η
is differentiable.
10
Published as a conference paper at ICLR 2018
Moreover, ifwe decompose A(X) with SVD, A(X) = UΣVT, Σ is of the form Σ = [Σ1 0] and
A(X)T (A(X)A(X)T)-1 = VΣTUT(UΣVTVΣTUT)-1 = VΣTUT(UΣ12UT)-1
= VΣTUTUΣ1-2UT = V Σ01-1 UT,
from which we can see that
A(X)T(A(X)A(X)T)-1op = σmax(A(X)T(A(X)A(X)T)-1) ≤ 1/,
by our assumption. Note that, for any X ∈ Rdx ,
kη(x)k2 = IlA(X)T(A(X)A(X)T)-1(hH+i：i(x) - h*(X))∣∣2
≤ ∣∣A(X)T(A(X)A(X)T)-1∣∣op khH+i：i(X) - h*(X)k2
≤ ∣∣A(X)T(A(X)A(X)T)-1∣∣op Mh+i：i - h*kni kXk2.
Since this holds for any X, we have
kηknl ≤ ∣∣A(X )T (A(X )A(X )T )-1∣∣op khH+1:1 - h*knl ≤ ' h" +1:1 -"knl
which ensures that η ∈ F1. Finally,
lln 「「小川I	、hDhjL(h)],ηi
kDhi[L(h)]kop ≥ kηkni
eE UhH+1:1(X) - h*(X川；]
IIhH+1:1 - h*knl
e(L(h) — L*)
IlhH+1:1 - h*knl
which yields
kDhi [L(h)] kop khH+1：1 - h*knl ≥ e(L(h) - L*)∙
From this we can see that if we have a critical point of L(h), then IDh1 [L(h)]Iop = 0 implies
L(h) = L*, which means that the critical point is a global minimum of L(h).
A.4 Proof of Theorem 4.2
Recall that by assumption we have j ∈ {1, . . . , H + 1} such that dx = dj-1 and dy ≤ dj. Consider
Dhj [L(h)], then for any η ∈ Fj,
(Dhj[L(h)],η} = E [(hH+1：1(X) - h(X))TJ[hH+1j+1 ](hj：1(X))n(hj-1：1(X))].
As done in the previous theorem, for any w ∈ Rdj-i, let A(w) = J [hH+1:j+1](hj (w)). Since A(w)
has full row rank by assumption, A(w)A(w)T is invertible. Then define
η(w) = A(W)T(A(W)A(W)T)-1(hH+1：1 - h*) ◦ hj21:1(w),
so that
(Dhj[L(h)],n)= E hkhH+1:1(X )-h*(X )k2i.
We need to check if η ∈ Fj. It is easily checked that η(0) = 0. Since J[hH+1j+1] is differentiable
by assumption and hj ∈ Fj, A(W) is differentiable, and so are A(W)T and (A(W)A(W)T)-1. The
inverse function of a differentiable and invertible function is also differentiable, so (hH+1：1 一 h*) ◦
h-11∙1 is differentiable. Hence, We can conclude that η is differentiable.
As seen in the previous section,
∣∣A(W)T (A(W)A(W)T)-1 ∣∣op = σmax(A(W)T (A(W)A(W)T)-1) ≤ 1/e2.
By the assumption that hj-1:1 is invertible and khj-1:1(u)k2 ≥ e1 kuk2,
kvk2 ≥ e1 ∣∣h-M1(V)∣∣2,
11
Published as a conference paper at ICLR 2018
for all v ∈ Rdj-1. From this, we can see that hj--11:1 nl ≤ 1/1. For any w ∈ Rdj-1,
kη(w)k2 = IIA(W)T(A(W)A(W)T)-1(hH+1：1 - h*) ◦ h-Ml(W)Il2
≤ ∣lA(W)T(A(W)A(W)T)-1∣∣op ||(hH+1：1 - h*)。h—，i：1(W)II2
≤ IIA(W)T(A(w)A(w)t)-1∣∣op khH+1:1 - h*kni ∣∣h⅛1(W)∣∣2
≤ ∣∣A(W)T(A(W)A(W)T)-1∣∣op khH+i：i — h*kni ∣∣h⅛ι∣∣nl kWk2 ∙
From this, we have
kηkni ≤ ∣∣A(W)T(A(W)A(W)T)-1∣∣op khH+i：i - h*kni IIh--LiIIni ≤ khH+;,-h*knl.
12
Finally,
IID [L(h)]∣∣	≥ (Dhj[L(h)],η> ≥ e1 e2E "hH”1(X)-h*(X)k2] = eg(L(h) - L*)
II hj[ ( )]IIoP≥ kηknl ≥	khH+L1-h*knl	= k 〃附1：1 - h* k nl ,
which yields
∣∣Dhj[L(h)]∣Iop khH+1:1 - h*knl ≥ eie2(L(h) - L)
A.5 Proof of Proposition 4.3
(Theorem 4.1) By assumption, We have di ≥ dy. Set hi(x) = (h*(x), 0,..., 0) where for every
X ∈ Rdx, the first dy components of hi (x) are identical to h* (x), and all other components are zero.
For the rest of hi’s, define hi : Rdi-1 → Rdi to be
hi(W) = (W1,...,Wdi),
(W1, . . . , Wdi-1 , 0, . . . , 0),
ifdi ≤ di-1,
ifdi > di-1,
(7)
for all W ∈ Rdi-I. Since di ≥ dy for all i, we can check that hH+i。•…。hi = h*, and hi ∈
Fi for all i. Moreover, for all z ∈ Rd1, J[hH+1:2](z) is all 0 except 1’s in diagonal entries, so
σmin( J[hH+ie](Z)) ≥ 1 and hH+i：2(z) is twice-differentiable.
(Theorem 4.2) It is given that we have j ∈ {1, . . . , H + 1} such that dx = dj-i and dy ≤ dj. Set
hj(x) = (h*(x), 0,..., 0), where the first dy components are h*(x) and the rest are zero. All the
rest of hi are set as in (7). Then, it can be easily checked that hi ∈ Fi for all i and all the conditions
of the theorem are satisfied.
B	Deferred Lemma
Lemma B.1. Suppose we are given a data matrix X ∈ Rdx ×m and an output matrix Y ∈ Rdy×m,
where dx < dy. Assume XXT and Y XT have full ranks. Consider minimizing the empirical
squared error risk:
L(Wi,..., Wh+i) := 1 ∣Wh+iWh …WiX - YkF ,
where Wk ∈ Rdk ×dk-1, k = 1, . . . , H + 1 are weight matrices of the linear neural network, and
d0 = dx and dH+i = dy for simplicity in notation. Also let W denote the tuple (Wi, . . . , WH+i).
Then, any critical point of L(W) that is not a local minimum is a saddle point.
Proof. For this lemma, we separate the proof into two cases: WH •…Wi = 0 and WH •…Wi = 0.
The crux of the proof is to show that any critical point cannot be a local maximum. Then, any critical
point is either a local minimum or a saddle point, so the conclusion of this lemma follows.
In case of WH …Wi = 0, we use some of the results in Kawaguchi (2016) and examine the
Hessian of L(W) with respect to vec(WHT+i), where vec(A) denotes vectorization of matrix A.
12
Published as a conference paper at ICLR 2018
Let Dvec(W T )L(W) be the partial derivative of L(W) with respect to vec(WHT+1) in numerator
layout. It was shown by Kawaguchi (2016, Lemma 4.3) that the Hessian matrix
H(W )= Dvec(WH +1) (Dvec(WH +1)L(W ))T =(I ㊈(WH …WiX )(Wh …Wi X )T)
=(10 WH …WiXX T WT …WH),
where 0 denotes the Kronecker product of two matrices. Notice that H(W) is positive Semidefinite.
Since XXT is full rank, whenever WH •…Wi = 0 there exists a strictly positive eigenvalue in
H(W), which means that there exists an increasing direction. So W cannot be a local maximum.
The case where WH …Wi = 0 requires a bit more careful treatment. Note that this case corre-
sponds to where we have degenerate critical points, which are in many cases much harder to handle.
For any arbitrary > 0, we describe a procedure that perturbs the matrices Wi , . . . , WH+i by
perturbations sampled from Frobenius norm balls of radius centered at 0, which we will denote as
Bi(), i = 1, . . . , H+ 1. LetU(Bi()) be the uniform distribution over the ball Bi(). The algorithm
goes as the following:
1.	For i ∈ {1, . . . , H + 1}
1.1.	Sample ∆% 〜U(Bi(E)), and define Vi = Wi + ∆%.
1.2.	If Wh+i …Wi+ι Vi …Vi = 0, stop and return i* = i.
First, recall that the set of rank-deficient matrices have Lebesgue measure zero, so for any sample
△i 〜U(Bi(E)), Vi = Wi + ∆i has full rank with probability 1. If we proceed the for loop until
i = H +1,we have a full-rank VH+i •…Vi with probability 1, which means that the algorithm must
return i* ∈ {1,..., H + 1} with probability 1. Notice that before and after the i*-th iteration, we
have
Wh+i …Wi* Vi*-i …H =0,
Wh+i …Wi* + iVi* ∙∙∙ Vi = WH+i …Wi* + i(Wi* + Ai* )Vi*-i …Vi =0.
This means that if we define △ = Wh+i •…Wi*+i∆i* Vi*-i •…Vi, then △ = 0. Also, notice that
WH+i …Wi*+i(Wi* — ∆i*)Vi*-i …Vi = -△.
Now, define two points
U(i) = (Vi,...,Vi*-i,Wi* +△i*,Wi*+i,...,WH+i),
U(2) = (Vi,...,Vi*-i,Wi* -△i*,Wi*+i,...,WH+i),
and notice that they are all in the neighborhood of W, that is, the Cartesian product of E-radius balls
centered at Wi , . . . , WH+i . Moreover, we have
L(W ) = 2 k0 ∙ X - YkF = 1 kY kF,
L(U ⑴ )=1 卜 X- Y BF=1 kY kF+2 卜 XIF-DA X,Y E，
L(U ⑵ )=1B-A X- Y BF=2 kYkF+2 心 X BF+DA X，Y E，
from which we can see that at least one of L(W) < L(U(i)) or L(W) < L(U(2)) must hold. This
shows that for any E > 0, there is a point U in E-neighborhood of W with a strictly greater function
value L(U). This proves that W cannot be a local maximum.	□
13
Published as a conference paper at ICLR 2018
C Deferred Proofs
C.1 Proof of Proposition 3.1
In case of k < min{dx , dy}, we can decompose the loss function in the following way:
kRX - Y k2F = RX - YXT(XXT)-1X +YXT(XXT)-1X - Y 2F
= RX-YXT(XXT)-1X2F+ YXT(XXT)-1X-Y2F
+2tr((YXT(XXT)-1X-Y)(RX-YXT(XXT)-1X)T).
Let us take a close look into the last term in the RHS. Note that YXT (XXT)-1X is the orthogonal
projection of Y onto row(X), so each row of Y XT(XXT)-1X - Y must be in null(X). Also,
(RX - YXT(XXT)-1X)T =XT(RT - (XXT)-1XYT).
It is XT right-multiplied with some matrix, so its columns must lie in col(X T ) = row(X). By the
fact that null(X)⊥ = row(X),
(YXT(XXT)-1X - Y)(RX - YXT(XXT)-1X)T = 0,
thus
Lo(R) = 1 IIRX - YXT(XXT)-1 XlIF + 1 IlYXT(XXT)-1χ - YllF
holds.
Now, (2) becomes a problem of minimizing IIRX - YXT(XXT)-1XII2F subject to the rank con-
straint rank(R) ≤ k. The optimal solution for this is obtained when RX is the k-rank approxi-
mation of YXT(XXT)-1X. Then, k-rank approximation of YXT(XXT)-1X can be expressed
as U UT YXT (XXT )-1X, where U is unique due to our assumption that all singular values are
distinct. Therefore,
R* = U U T YX T (XX T )-1
is the unique global minimum solution of (2) when k < min{dx , dy}.
C.2 Proof of Lemma 3.2
L(W1,...,Wi-1,Wi+∆i,Wi+1,...,WH+1)
=2 IlWH+1 …Wi+1 (Wi + δJWi-I …WIX - YIIF
=2 k Wh+i …WiX — Y + Wh+i …Wi+ι∆iWi-ι …WIXkF
=L(W) + tr((WH+1 …Wi+ι∆iW-…WiX)T(Wh+i …WiX — Y)) + O(IAkF)
=L(W) + tr(Wi+1 …WH +1(Wh+1 …W" - Y)XTWT …Wi-QT) + O(IAkF).
From this, we can conclude that
∂L
= Wi+1 …wh+1(wh+1 …WIX — Y)X WI …Wi-1.
∂Wi
C.3 Proof of Lemma 3.3
1.	Since ATA	σm2 in(A)I, BTATAB	σm2in(A)BTB. Then
IABI2F = tr(BT AT AB) ≥ σm2 in(A) tr(BT B) = σm2 in(A) IBI2F.
2.	SinceBBT	σm2in(B)I, ABBTAT	σm2in(B)AAT. Then
IABI2F = tr(BT AT AB) = tr(ABBT AT) ≥ σm2 in(B) tr(AAT) = σm2 in(B) IAI2F.
14