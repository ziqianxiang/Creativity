Learning how to explain neural networks:
PatternNet and PatternAttribution
Pieter-Jan Kindermans* *
Google Brain
pikinder@google.com
Kristof T. Schutt & Maximilian Alber
TU Berlin
{kristof.schuett,maximilian.alber}@tu-berlin.de
Klaus-Robert MuHert
TU Berlin
klaus-robert.mueller@tu-berlin.de
Dumitru Erhan & Been Kim
Google Brain
{dumitru,beenkim}@google.com
Sven Dahne^
TU Berlin
sven.daehne@tu-berlin.de
Ab stract
DeConvNet, Guided BackProp, LRP, were invented to better understand deep neu-
ral networks. We show that these methods do not produce the theoretically correct
explanation for a linear model. Yet they are used on multi-layer networks with
millions of parameters. This is a cause for concern since linear models are simple
neural networks. We argue that explanation methods for neural nets should work
reliably in the limit of simplicity, the linear models. Based on our analysis of
linear models we propose a generalization that yields two explanation techniques
(PatternNet and PatternAttribution) that are theoretically sound for linear models
and produce improved explanations for deep networks.
1	Introduction
Deep learning made a huge impact on a wide variety of applications (Krizhevsky et al., 2012;
Sutskever et al., 2014; LeCun et al., 2015; Schmidhuber, 2015; Mnih et al., 2015; Silver et al.,
2016) and recent neural network classifiers have become excellent at detecting relevant signals (e.g.,
the presence of a cat) contained in input data points such as images by filtering out all other, non-
relevant and distracting components also present in the data. This separation of signal and distractors
is achieved by passing the input through many layers with millions of parameters and nonlinear ac-
tivation functions in between, until finally at the output layer, these models yield a highly condensed
version of the signal, e.g. a single number indicating the probability of a cat being in the image.
While deep neural networks learn efficient and powerful representations, they are often considered a
‘black-box’. In order to better understand classifier decisions and to gain insight into how these mod-
els operate, a variety techniques have been proposed (Simonyan et al., 2014; Yosinski et al., 2015;
Nguyen et al., 2016; Baehrens et al., 2010; Bach et al., 2015; Montavon et al., 2017; Zeiler & Fer-
gus, 2014; Springenberg et al., 2015; Zintgraf et al., 2017; Sundararajan et al., 2017; Smilkov et al.,
2017). These methods for explaining classifier decisions operate under the assumption that it is pos-
sible to propagate the condensed output signal back through the classifier to arrive at something that
shows how the relevant signal was encoded in the input and thereby explains the classifier decision.
Simply put, if the classifier detected a cat, the visualization should point to the cat-relevant aspects
of the input image from the perspective of the network. Techniques that are based on this princi-
ple include saliency maps from network gradients (Baehrens et al., 2010; Simonyan et al., 2014),
DeConvNet (Zeiler & Fergus, 2014, DCN), Guided BackProp (Springenberg et al., 2015, GBP),
*Part of this work was done at TU Berlin, part of the work was part of the Google Brain Residency program.
*KRM is also with Korea University and Max PlanCk Institute for Informatics, Saarbrucken, Germany
^ Sven Dahne is now at Amazon
1
PatternNet
PatternAttribution
Forward pass
DeconvNet
(Zeiler βt al)
Gradient
(Baehrena et al, Simonyan Bt ≡Γ
Guided Backprop
(Springeriieraetal)
B
Linear Neuron
Foiward ReLU
(deactivated)
Example
VGG-16 classification
BacKward ReLU	Backward RBLU
(activated)	(deactivated)
Figure 1: Illustration of explanation approaches. Function and signal approximators visualize the
explanation using the original color channels. The attribution is visualized as a heat map of pixel-
wise contributions to the output
OForward ReLU
(activated)
Layer-wise Relevance Propagation (Bach et al., 2015, LRP) and the Deep Taylor Decomposition
(Montavon et al., 2017, DTD), Integrated Gradients (Sundararajan et al., 2017) and SmoothGrad
(Smilkov et al., 2017).
The merit of explanation methods is often demonstrated by applying them to state-of-the-art deep
learning models in the context of high dimensional real world data, such as ImageNet, where the pro-
vided explanation is intuitive to humans. Unfortunately, theoretical analysis as well as quantitative
empirical evaluations of these methods are lacking.
Deep neural networks are essentially a composition of linear transformations connected with non-
linear activation functions. Since approaches, such as DeConvNet, Guided BackProp, and LRP,
back-propagate the explanations in a layer-wise fashion, it is crucial that the individual linear layers
are handled correctly. In this work we show that these gradient-based methods fail to recover the
signal even for a single-layer architecture, i.e. a linear model. We argue that therefore they cannot
be expected to reliably explain a deep neural network and demonstrate this with quantitative and
qualitative experiments. In particular, we provide the following key contributions:
•	We analyze the performance of existing explanation approaches in the controlled setting of
a linear model (Sections 2 and 3).
•	We categorize explanation methods into three groups - functions, signals and attribution
(See Fig. 1) - that require fundamentally different interpretations and are complementary
in terms of information about the neural network (Section 3).
•	We propose two novel explanation methods - PatternNet and PatternAttribution - that alle-
viate shortcomings of current approaches, as discovered during our analysis, and improve
explanations in real-world deep neural networks visually and quantitatively (Sections 4 and
5).
This presents a step towards a thorough analysis of explanation methods and suggests qualitatively
and measurably improved explanations. These are crucial requirements for reliable explanation
techniques, in particular in domains, where explanations are not necessarily intuitive, e.g. in health
and the sciences Schutt et al. (2017).
Notation and scope Scalars are lowercase letters (i), column vectors are bold (u), element-wise
multiplication is (). The covariance between u and v is cov[u, v], the covariance of u and i is
cov[u, i]. The variance of a scalar random variable i is σi2 . Estimates of random variables will
have a hat (u). We analyze neural networks excluding the final soft-max output layer. To allow
for analytical treatment, we only consider networks with linear neurons optionally followed by a
rectified linear unit (ReLU), max-pooling or soft-max. We analyze linear neurons and nonlinearities
independently such that every neuron has its own weight vector. These restrictions are similar to
those in the saliency map (Simonyan et al., 2014), DCN (Zeiler & Fergus, 2014), GBP (Springenberg
2
Figure 2: For linear models, i.e., a simple neural network, the weight vector does not explain the
signal it detects Haufe et al. (2014). The data x = yas + ad is color-coded w.r.t. the output
y = wTx. Only the signal s = yas contributes to y. The weight vector w does not agree with the
signal direction, since its primary objective is canceling the distractor. Therefore, rotations of the
basis vector ad of the distractor with constant signal s lead to rotations of the weight vector (right).
et al., 2015), LRP (Bach et al., 2015) and DTD (Montavon et al., 2017). Without loss of generality,
biases are considered constant neurons to enhance clarity.
2	Understanding linear models
In this section, we analyze explanation methods for deep neural network, starting with the simplest
neural network setting: a purely linear model and data sampled from a linear generative model. This
setup allows us to (i) fully control how signal and distractor components are encoded in the input
data and (ii) analytically track how the resulting explanation relates to the known signal component.
This analysis allows us then to highlight shortcomings of current explanation approaches that carry
over to deep neural networks.
Consider the following toy example (see Fig. 2) where we generate data x as:
x = s + d	s = asy,	with as = (1, 0)T , y ∈ [-1, 1]
d = ad6,	with ad = (1, I)T , e 〜N (μ, σ2).
We train a linear regression model to extract y from x. By construction, s is the signal in our data,
i.e., the part of x containing information about y. Using the terminology of Haufe et al. (2014) the
distractor d obfuscates the signal making the detection task more difficult. To optimally extract y,
our model has to be able to filter out the distractor d. This is why the weight vector is also called the
filter. In the example, w = [1, -1]T fulfills this convex task.
From this example, we can make several observations: The optimal weight vector w does not align,
in general, with the signal direction as, but tries to filter the contribution of the distractor (see
Fig. 2). This is optimally solved when the weight vector is orthogonal to the distractor wT d = 0.
Therefore, when the direction of the distractor ad changes, w must follow, as illustrated on the right
hand side of the figure. On the other hand, a change in signal direction as can be compensated for
by a change in sign and magnitude of w such that wTas = 1, but the direction stays constant.
The fact that the direction of the weight vector in a linear model is largely determined by the dis-
tractor implies that given only the weight vector, we cannot know what part of the input produces
the output y. On the contrary, the direction as must be learned from data.
Now assume that we have additive isotropic Gaussian noise. The mean of the noise can easily be
compensated for with a bias change. Therefore, we only have to consider the zero-mean case. Since
isotropic Gaussian noise does not contain any correlations or structure, the only way to remove it is
by averaging over different measurements. It is not possible to cancel it out effectively by using a
well-chosen weight vector. However, it is well known that adding Gaussian noise shrinks the weight
vector and corresponds to L2 regularization. In the absence of a structured distractor, the smallest
weight vector w such that wT as = 1 is the one in the direction of the signal. Therefore in practice
both these effects influence the actual weight vector.
3
As already indicated above, deep neural networks are essentially a composition of linear layers and
non-linear activation functions. In the next section, we will show that gradient-based methods, e.g.,
DeConvNet, Guided BackProp, and LRP, are not able to distinguish signal from distractor in a linear
model and therefore back-propagate sub-optimal explanations in deeper networks. This analysis
allows us to develop improved layer-wise explanation techniques and to demonstrate quantitative
and qualitative better explanations for deep neural networks.
Terminology Throughout this manuscript we will use the following terminology: The filter w
tells us how to extract the output y optimally from data x. The pattern as is the direction in the data
along which the desired output y varies. Both constitute the signal s = asy, i.e., the contributing
part of x. The distractor d is the component of the data that does not contain information about the
desired output.
3	Overview of explanation approaches and their behavior
In this section, we take a look at a subset of explanation methods for individual classifier decisions
and discuss how they are connected to our analysis of linear models in the previous section. Fig. 1
gives an overview of the different types of explanation methods which can be divided into function,
signal and attribution visualizations. These three groups all present different information about the
network and complement each other.
Functions - gradients, Saliency map Explaining the function in input space corresponds to de-
scribing the operations the model uses to extract y from x. Since deep neural networks are highly
nonlinear, this can only be approximated. The saliency map estimates how moving along a particu-
lar direction in input space influences y (i.e., sensitivity analysis) where the direction is given by the
model gradient (Baehrens et al., 2010; Simonyan et al., 2014). In case of a linear model y = wTx,
the Saliency map reduces to analyzing the weights ∂y∕∂x = w. Since it is mostly determined by
the distractor, as demonstrated above, it is not representing the signal. It tells us how to extract the
signal, not what the signal is in a deep neural network.
Signal - DeConvNet, Guided BackProp, PatternNet The signal s detected by the neural net-
work is the component of the data that caused the networks activations. Zeiler & Fergus (2014)
formulated the goal of these methods as ”[...] to map these activities back to the input pixel space,
showing what input pattern originally caused a given activation in the feature maps”.
In a linear model, the signal corresponds to s = asy. The pattern as contains the signal direction,
i.e., it tells us where a change of the output variable is expected to be measurable in the input
(Haufe et al., 2014). Attempts to visualize the signal for deep neural networks were made using
DeConvNet (Zeiler & Fergus, 2014) and Guided BackProp (Springenberg et al., 2015). These use
the same algorithm as the saliency map, but treat the rectifiers differently (see Fig. 1): DeConvNet
leaves out the rectifiers from the forward pass, but adds additional ReLUs after each deconvolution,
while Guided BackProp uses the ReLUs from the forward pass as well as additional ones. The
back-projections for the linear components of the network correspond to a superposition of what are
assumed to be the signal directions of each neuron. For this reason, these projections must be seen
as an approximation of the features that activated the higher layer neuron. It is not a reconstruction
in input space (Zeiler & Fergus, 2014).
For the simplest of neural networks - the linear model - these visualizations reduce to the gradient1.
They show the filter w and neither the pattern as, nor the signal s. Hence, DeConvNet and Guided
BackProp do not guarantee to produce the detected signal for a linear model, which is proven by our
toy example in Fig. 2. Since they do produce compelling visualizations, we will later investigate
whether the direction of the filter w coincides with the direction of the signal s. We will show that
this is not the case and propose a new approach, PatternNet (see Fig. 1), to estimate the correct
direction that improves upon the DeConvNet and Guided BackProp visualizations.
1In tensorflow terminoloy: linear model on MNIST can be seen as a convolutional neural network with
VALID padding and a 28 by 28 filter size.
4
Attribution - LRP, Deep Taylor Decomposition, PatternAttribution Finally, We can look at
how much the signal dimensions contribute to the output through the layers. This will be referred
to as the attribution. For a linear model, the optimal attribution Would be obtained by element-Wise
multiplying the signal With the Weight vector: rinput = w ay, With the element-Wise multipli-
cation. Bach et al. (2015) introduced layer-wise relevance propagation (LRP) as a decomposition
of pixel-Wise contributions (called relevances). Montavon et al. (2017) extended this idea and pro-
posed the deep Taylor decomposition (DTD). The key idea of DTD is to decompose the activation
of a neuron in terms of contributions from its inputs. This is achieved using a first-order Taylor
expansion around a root point x0 With wTx0 = 0. The relevance of the selected output neuron
i is initialized With its output from the forWard pass. The relevance from neuron i in layer l is
re-distributed toWards its input as:
『output = y	『output = o	yi-i,i = W G (X - XO) r
To obtain the relevance for neuron i in layer l-1 the incoming relevances from all connected neurons
j in layer l are summed
l-1	X l-1,j
『i =	『i	.
j
Here We can safely assume that wT X > 0 because a non-active ReLU unit from the forWard pass
stops the re-distribution in the backWard pass. This is identical to hoW a ReLU stops the propagation
of the gradient. The difficulty in the application of the deep Taylor decomposition is the choice of
the root point X0, for Which many options are available. It is important to recognize at this point that
selecting a root point for the DTD corresponds to estimating the distractor X0 = d and, by that, the
signal S = X — xo. PatternAttribution is a DTD extension that learns from data how to set the root
point.
Summarizing, the function extracts the signal from the data by removing the distractor. The attri-
bution of output values to input dimensions shows how much an individual component of the signal
contributes to the output, which is what LRP calls relevance.
4	Learning to estimate the signal
Visualizing the function has proven to be straightforward (Baehrens et al., 2010; Simonyan et al.,
2014). In contrast, visualizing the signal (Haufe et al., 2014; Zeiler & Fergus, 2014; Springenberg
et al., 2015) and the attribution (Bach et al., 2015; Montavon et al., 2017; Sundararajan et al., 2017)
is more difficult. It requires a good estimate of what is the signal and what is the distractor. In the
following section we first propose a quality measure for neuron-wise signal estimators. This allows
us to evaluate existing approaches and, finally, derive signal estimators that optimize this criterion.
These estimators will then be used to explain the signal (PatternNet) and the attribution (Patter-
nAttribution). All mentioned techniques as well as our proposed signal estimators treat neurons
independently, i.e., the full explanation will be a superposition of neuron-wise explanations.
4.1	Quality criterion for signal estimators
Recall that the input data X comprises both signal and distractor: X = s + d, and that the signal con-
tributes to the output but the distractor does not. Assuming the filter w has been trained sufficiently
well to extract y, we have
wTX = y, wTs = y,	wTd = 0.
Note that estimating the signal based on these conditions alone is an ill-posed problem. We could
limit ourselves to linear estimators of the form S = U(WTU)Ty, with U a random vector such that
WTu = 0. For such an estimator, the signal estimate S = U (WTU) 1 y satisfies wtS = y. This
implies the existence of an infinite number of possible rules for the DTD as well as infinitely many
back-projections for the DeConvNet family.
To alleviate this issue, we introduce the following quality measure P for a signal estimator S(X) = S
that will be written with explicit variances and covariances using the shorthands d = X - S(X) and
5
y = wTx:
P(S) = 1 — max Corr (WTx, VT (X — S(x))) = 1 — max V Cov",=y∙
V	V	∖∕σ2τ^σ
vTd y
(1)
This criterion introduces an additional constraint by measuring how much information about y can
be reconstructed from the residuals X — S using a linear projection. The best signal estimators
remove most of the information in the residuals and thus yield large ρ(S). Since the correlation is
invariant to scaling, We constrain vtd to have variance。：『金=σj. Finding the optimal V for a
fixed S(X) amounts to a least-squares regression from d to y. This enables us to assess the quality
of signal estimators efficiently.
4.2	Existing Signal Estimators
Let us noW discuss tWo signal estimators that have been used in previous approaches.
Sx - the identity estimator The naive approach to signal estimation is to assume the entire data
is signal and there are no distractors:
Sx (X) = X.
With this being plugged into the deep Taylor frameWork, We obtain the z-rule (Montavon et al.,
2017) Which is equivalent to LRP (Bach et al., 2015). For a linear model, this corresponds to
r = w X as the attribution. It can be shoWn that for ReLU and max-pooling netWorks, the z-rule
reduces to the element-Wise multiplication of the input and the saliency map (Shrikumar et al., 2016;
Kindermans et al., 2016). This means that for a Whole netWork, the assumed signal is simply the
original input image. It also implies that, if there are distractors present in the data, they are included
in the attribution:
r = w X = w s + w	d.
When moving through the layers by applying the filters w during the forWard pass, the contributions
from the distractor d are cancelled out. HoWever, they cannot be cancelled in the backWard pass by
the element-Wise multiplication. The distractor contributions w d that are included in the LRP
explanation cause the noisy nature of the visualizations based on the z-rule.
Sw - the filter based estimator The implicit assumption made by DeConvNet and Guided Back-
Prop is that the detected signal varies in the direction of the Weight vector w. This Weight vector has
to be normalized in order to be a valid signal estimator. In the deep Taylor decomposition frameWork
this corresponds to the w2 -rule and results in the folloWing signal estimator:
SW (x)	τ W x.
For a linear model, this produces an attribution of the form W⅛Wy. This estimator does not recon-
struct the proper signal in the toy example of section 2. Empirically it is also sub-optimal in our
experiment in Fig. 3.
4.3	PatternNet and PatternAttribution
We suggest to learn the signal estimator S from data by optimizing the previously established crite-
rion. A signal estimator S is optimal With respect to Eq. (1) if the correlation is zero for all possible
V: ∀V, cov[y, d]V = 0. This is the case When there is no covariance betWeen y and d. Because of
linearity of the covariance and since d = x — S(x) the above condition leads to
cov[y, d] = 0 ⇒ cov[x, y] = cov[S(x), y].	(2)
It is important to recognize that the covariance is a summarizing statistic and consequently the
problem can still be solved in multiple Ways. We Will present tWo possible solutions to this problem.
Note that When optimizing the estimator, the contribution from the bias neuron Will be considered 0
since it does not covary With the output y.
6
Sa - The linear estimator A linear neuron can only extract linear signals S from its input x.
Therefore, we could assume a linear dependency between s and y, yielding a signal estimator:
Sa (x) = awT x.	(3)
Plugging this into Eq. (2) and optimising for a yields
T	cov[x, y]
cov[x, y] = cov[aw x, y] = acov[y, y] ⇒ a =-------——.	(4)
σ2
σy
Note that this solution is equivalent to the approach commonly used in neuro-imaging (Haufe et al.,
2014) despite different derivation. With this approach we can recover the signal of our toy example
in section 2. It is equivalent to the filter-based approach only if the distractors are orthogonal to the
signal. We found that the linear estimator works well for the convolutional layers. However, when
using this signal estimator with ReLUs in the dense layers, there is still a considerable correlation
left in the distractor component (see Fig. 3).
Sa+- - The two-component estimator To move beyond the linear signal estimator, it is crucial to
understand how the rectifier influences the training. Since the gate of the ReLU closes for negative
activations, the weights only need to filter the distractor component of neurons with y > 0. Since this
allows the neural network to apply filters locally, we cannot assume a global distractor component.
We rather need to distinguish between the positive and negative regime:
s+ + d+ ify > 0
s- + d- otherwise
Even though signal and distractor of the negative regime are canceled by the following ReLU, we
still need to make this distinction in order to approximate the signal. Otherwise, information about
whether a neuron fired would be retained in the distractor. Thus, we propose the two-component
signal estimator:
Sa+-(x)
a+wT x,
a-wT x,
if wTx > 0
otherwise
(5)
Next, we derive expressions for the patterns a+ and a-. We denote expectations over x within the
positive and negative regime with E+ [x] and E- [x], respectively. Let π+ be the expected ratio of
inputs x with wT x > 0. The covariance of data/signal and output become:
cov[x,y] =	π+	(E+	[xy]	-E+	[x]	E [y])	+	(1 - π+)	(E- [xy]	-E- [x]E[y])	(6)
cov[s,y] =	π+	(E+	[sy]	- E+ [s]	E [y])	+	(1 -π+)	(E- [sy]	- E- [s]E[y])	(7)
Assuming both covariances are equal, we can treat the positive and negative regime separately using
Eq. (2) to optimize the signal estimator:
E+ [xy] -E+ [x] E [y] = E+ [sy] -E+ [s] E [y]
Plugging in Eq. (5) and solving for a+ yields the required parameter (a- analogous).
a =	E+ [χy] — E+[x] E [y]	(8)
+	WT E+ [xy] — WT E+ [x] E [y]
The solution for Sa+- reduces to the linear estimator when the relation between input and output is
linear. Therefore, it solves our introductory linear example correctly.
PatternNet and PatternAttribution Based on the presented analysis, we propose PatternNet and
PatternAttribution as illustrated in Fig. 1. PatternNet yields a layer-wise back-projection of the esti-
mated signal to input space. The signal estimator is approximated as a superposition of neuron-wise,
nonlinear signal estimators Sa+- in each layer. It is equal to the computation of the gradient where
during the backward pass the weights of the network are replaced by the informative directions. In
Fig. 1, a visual improvement over DeConvNet and Guided Backprop is apparent.
PatternAttribution exposes the attribution Wa+ and improves upon the layer-wise relevance prop-
agation (LRP) framework (Bach et al., 2015). It can be seen as a root point estimator for the Deep-
Taylor Decomposition (DTD). Here, the explanation consists of neuron-wise contributions of the
estimated signal to the classification score. By ignoring the distractor, PatternAttribution can reduce
the noise and produces much clearer heat maps. By working out the back-projection steps in the
Deep-Taylor Decomposition with the proposed root point selection method, it becomes obvious that
PatternAttribution is also analogous to the backpropagation operation. In this case, the weights are
replaced during the backward pass by W a+ .
7
1.0
Figure 3: Evaluating ρ(S) for VGG-16 on ImageNet. Higher values are better. The gradient (Sw),
linear estimator (Sa) and nonlinear estimator (Sa+- ) are compared. An estimator using random
directions is the baseline. The network has 5 blocks with 2/3 convolutional layers and 1 max-pooling
layer each, followed by 3 dense layers.
皂-- qeqojd 4nano -i3e一 SSe-ɔ
20	40	60	80
number of patches modified
100
Figure 4: Image degradation experiment on all 50.000 images in the ImageNet validation set. The
effect on the classifier output is measured. A steeper decrease is better.
5 Experiments and discussion
To evaluate the quality of the explanations, we focus on the task of image classification. Neverthe-
less, our method is not restricted to networks operating on image inputs. We used Theano (Bergstra
et al., 2010) and Lasagne (Dieleman et al., 2015) for our implementation. We restrict the analysis to
the well-known ImageNet dataset (Russakovsky et al., 2015) using the pre-trained VGG-16 model
(Simonyan & Zisserman, 2015). Images were rescaled and cropped to 224x224 pixels. The signal
estimators are trained on the first half of the training dataset.
The vector v, used to measure the quality of the signal estimator ρ(x) in Eq. (1), is optimized on the
second half of the training dataset. This enables us to test the signal estimators for generalization.
All the results presented here were obtained using the official validation set of 50000 samples. The
validation set was not used for training the signal estimators, nor for training the vector v to measure
the quality. Consequently our results are obtained on previously unseen data.
The linear and the two component signal estimators are obtained by solving their respective closed
form solutions (Eq. (4) and Eq. (8)). With a highly parallelized implementation using 4 GPUs this
could be done in 3-4 hours. This can be considered reasonable given that several days are required
to train the actual network. The quality of a signal estimator is assessed with Eq. (1). Solving it with
the closed form solution is computationally prohibitive since it must be repeated for every single
weight vector in the network. Therefore we optimize the equivalent least-squares problem using
stochastic mini-batch gradient descent with ADAM Kingma & Ba (2015) until convergence. This
was implemented on a NVIDIA Tesla K40 and took about 24 hours per optimized signal estimator.
After learning to explain, individual explanations are computationally cheap since they can be imple-
mented as a back-propagation pass with a modified weight vector. As a result, our method produces
explanations at least as fast as the work by Dabkowski & Gal (2017) on real time saliency. However,
our method has the advantage that it is not only applicable to image models but is a generalization
of the theory commonly used in neuroimaging Haufe et al. (2014).
8
Sx
Sa
Sa+ -
horn — horn (0.98)
Figure 5: Top: signal. Bottom: attribution. For the trivial estimator Sx the original input is the
signal. This is not informative w.r.t. how the network operates.
Measuring the quality of signal estimators In Fig. 3 We present the results from the correlation
measure ρ(x), where higher values are better. We use random directions as baseline signal estima-
tors. Clearly, this approach removes almost no correlation. The filter-based estimator Sw succeeds
in removing some of the information in the first layer. This indicates that the filters are similar to
the patterns in this layer. However, the gradient removes much less information in the higher layers.
Overall, it does not perform much better than the random estimator. This implies that the weights do
not correspond to the detected stimulus in a neural network. Hence the implicit assumptions about
the signal made by DeConvNet and Guided BackProp is not valid. The optimized estimators remove
much more of the correlations across the board. For convolutional layers, Sa and Sa+- perform
comparably in all but one layer. The two component estimator Sa+- is best in the dense layers.
Image degradation The first experiment was a direct measurement of the quality of the signal
estimators of individual neurons. The second one is an indirect measurement of the quality, but it
considers the whole network. We measure how the prediction (after the soft-max) for the initially
selected class changes as a function of corrupting more and more patches based on the ordering
assigned by the attribution (see Samek et al., 2016). This is also related to the work by Zintgraf
et al. (2017). In this experiment, we split the image in non-overlapping patches of 9x9 pixels. We
compute the attribution and sum all the values within a patch. We sort the patches in decreasing
order based on the aggregate heat map value. In step n = 1..100 we replace the first n patches with
the their mean per color channel to remove the information in this patch. Then, we measure how
this influences the classifiers output. We use the estimators from the previous experiment to obtain
the function-signal attribution heat maps for evaluation. A steeper decay indicates a better heat map.
Results are shown in Fig. 4. The baseline, in which the patches are randomly ordered, performs
worst. The linear optimized estimator Sa performs quite poorly, followed by the filter-based estima-
tor Sw . The trivial signal estimator Sx performs just slightly better. However, the two component
model Sa+- leads to the fastest decrease in confidence in the original prediction by a large margin.
Its excellent quantitative performance is also backed up by the visualizations discussed next.
Qualitative evaluation In Fig. 5, we compare all signal estimators on a single input image. For
the trivial estimator Sx , the signal is by definition the original input image and, thus, includes the
distractor. Therefore, its noisy attribution heat map shows contributions that cancel each other in
the neural network. The Sw estimator captures some of the structure. The optimized estimator Sa
results in slightly more structure but struggles on color information and produces dense heat maps.
The two component model Sa+- on the right captures the original input during signal estimation
and produces a crisp heat map of the attribution.
Fig. 6 shows the visualizations for six randomly selected images from ImageNet. PatternNet is
able to recover a signal close to the original without having to resort to the inclusion of additional
rectifiers in contrast to DeConvNet and Guided BackProp. We argue that this is due to the fact
that the optimization of the pattern allows for capturing the important directions in input space.
This contrasts with the commonly used methods DeConvNet, Guided BackProp, LRP and DTD,
for which the correlation experiment indicates that their implicit signal estimator cannot capture
9
Figure 6: Visualization of random images from ImageNet (validation set). In the leftmost shows
column the ground truth, the predicted label and the classifier’s confidence. Methods should only be
compared within their group. PatternNet, Guided Backprop, DeConvNet and the Gradient (saliency
map) are back-projections to input space with the original color channels. They are normalized using
XnOrm = 2mXχ∣x∣ + 1 to maximize contrast. LRP and PatternAttribution are heat maps showing
pixel-wise contributions.
the true signal in the data. Overall, the proposed approach produces the most crisp visualization in
addition to being measurably better, as shown in the previous section. Additonally, we also contrast
our methods to the prediction-differences analysis by Zintgraf et al. (2017) in the supplementary
material.
Relation to previous methods Our method can be thought of as a generalization of the work
by Haufe et al. (2014), making it applicable on deep neural networks. Remarkably, our proposed
approach can solve the toy example in section 2 optimally while none of the previously published
methods for deep learning are able to solve this (Bach et al., 2015; Montavon et al., 2017; Smilkov
et al., 2017; Sundararajan et al., 2017; Zintgraf et al., 2017; Dabkowski & Gal, 2017; Zeiler & Fer-
gus, 2014; Springenberg et al., 2015). Our method shares the idea that to explain a model properly
one has to learn how to explain it with Zintgraf et al. (2017) and Dabkowski & Gal (2017). Fur-
thermore, since our approach is after training just as expensive as a single back-propagation step, it
can be applied in a real-time context, which is also possible for the work done by Dabkowski & Gal
(2017) but not for Zintgraf et al. (2017).
6 Conclusion
Understanding and explaining nonlinear methods is an important challenge in machine learning.
Algorithms for visualizing nonlinear models have emerged but theoretical contributions are scarce.
We have shown that the direction of the model gradient does not necessarily provide an estimate for
the signal in the data. Instead it reflects the relation between the signal direction and the distracting
noise contributions ( Fig. 2). This implies that popular explanation approaches for neural networks
(DeConvNet, Guided BackProp, LRP) do not provide the correct explanation, even for a simple
linear model. Our reasoning can be extended to nonlinear models. We have proposed an objective
function for neuron-wise explanations. This can be optimized to correct the signal visualizations
(PatternNet) and the decomposition methods (PatternAttribution) by taking the data distribution into
10
account. We have demonstrated that our methods constitute a theoretical, qualitative and quantitative
improvement towards understanding deep neural networks.
Acknowledgments
This project has received funding from the European Union’s Horizon 2020 research and innova-
tion programme under the Marie Sklodowska-Curie grant agreement NO 657679, the BMBF for
the Berlin Big Data Center BBDC (01IS14013A), a hardware donation from NVIDIA. We thank
Sander Dieleman, Jonas Degraeve, Ira Korshunova, Stefan Chmiela, Malte Esders, Sarah Hooker,
Vincent Vanhoucke for their comments to improve this manuscript. We are grateful to Chris Olah
and Gregoire Montavon for the valuable discussions.
References
Sebastian Bach, Alexander Binder, Gregoire Montavon, Frederick Klauschen, Klaus-Robert Muller,
and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise
relevance propagation. PloS one, 10(7):e0130140, 2015.
David Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja Hansen, and Klaus-
Robert Muller. HoW to explain individual classification decisions. Journal of Machine Learning
Research,11(Jun):1803-1831, 2010.
James Bergstra, Olivier Breuleux, Frederic Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume
Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. Theano: A cpu and gpu
math compiler in python. In Proc. 9th Python in Science Conf, pp. 1-7, 2010.
Piotr DabkoWski and Yarin Gal. Real time image saliency for black box classifiers. In NIPS 2017,
2017.
Sander Dieleman, Jan Schluter, Colin Raffel, Eben Olson, S0ren Kaae S0nderby, Daniel Nouri,
Daniel Maturana, Martin Thoma, Eric Battenberg, Jack Kelly, et al. Lasagne: First release.
Zenodo: Geneva, Switzerland, 2015.
Stefan Haufe, Frank Meinecke, Kai Gorgen, Sven Dahne, John-Dylan Haynes, Benjamin Blankertz,
and Felix Bieβmann. On the interpretation of weight vectors of linear models in multivariate
neuroimaging. Neuroimage, 87:96-110, 2014.
Pieter-Jan Kindermans, Kristof Schutt, Klaus-Robert Muller, and Sven Dahne. Investigating the
influence of noise and distractors on the interpretation of neural networks. arXiv preprint
arXiv:1611.07270, 2016.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436-444,
2015. doi: 10.1038/nature14539.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.
GregOire Montavon, Sebastian Lapuschkin, Alexander Binder, Wojciech Samek, and Klaus-Robert
Muller. Explaining nonlinear classification decisions with deep taylor decomposition. Pattern
Recognition, 65:211-222, 2017.
Anh Nguyen, Alexey Dosovitskiy, Jason Yosinski, Thomas Brox, and Jeff Clune. Synthesizing the
preferred inputs for neurons in neural networks via deep generator networks. In Advances in
Neural Information Processing Systems, pp. 3387-3395, 2016.
11
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual
recognition challenge. International Journal ofComputer Vision, 115(3):211-252, 2015.
Wojciech Samek, Alexander Binder, Gregoire Montavon, Sebastian Lapuschkin, and Klaus-Robert
Muller. Evaluating the visualization of what a deep neural network has learned. IEEE Transac-
tions on Neural Networks and Learning Systems, 2016. doi: 10.1109/TNNLS.2016.2599820.
Jurgen Schmidhuber. Deep learning in neural networks: An overview. Neural networks, 61:85-117,
2015.
Kristof T Schutt, Farhad Arbabzadah, Stefan Chmiela, Klaus-Robert Muller, and Alexandre
Tkatchenko. Quantum-chemical insights from deep tensor neural networks. Nature Comm., 8:
13890, 2017. doi: 10.1038/ncomms13890.
Avanti Shrikumar, Peyton Greenside, Anna Shcherbina, and Anshul Kundaje. Not just a black box:
Learning important features through propagating activation differences. CoRR, abs/1605.01713,
2016. URL http://arxiv.org/abs/1605.01713.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. Nature, 529(7587):484-489, 2016.
doi: 10.1038/nature16961.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In ICLR, 2015.
Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks:
Visualising image classification models and saliency maps. In ICLR, 2014.
Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Viegas, and Martin Wattenberg. Smoothgrad:
removing noise by adding noise. arXiv preprint arXiv:1706.03825, 2017.
Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for
simplicity: The all convolutional net. In ICLR, 2015.
Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In
ICML 2017, 2017.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
In Advances in neural information processing systems, pp. 3104-3112, 2014.
Jason Yosinski, Jeff Clune, Thomas Fuchs, and Hod Lipson. Understanding neural networks through
deep visualization. In ICML Workshop on Deep Learning, 2015.
Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In
European Conference on Computer Vision, pp. 818-833. Springer, 2014.
Luisa M Zintgraf, Taco S Cohen, Tameem Adel, and Max Welling. Visualizing deep neural network
decisions: Prediction difference analysis. In ICLR, 2017.
A	Algorithms
In this section we will give an overview of the visualization algorithms to clarify their actual im-
plementation for ReLu networks. This shows the similarities and the differences between all ap-
proaches. For all visualization approaches, the back-projection through a max-pooling layer is only
through the path that was active in the forward pass.
12
A. 1 Function visualisation
A.1.1 Gradient with respect to the input
Initialization To compute the gradient of an output neuron w.r.t. to the input, we can initialize the
gradient at the output neuron with a single 1, for the non-selected output neurons we initialize it at
0.
output	output
gi	= y,	gj6=i	= 0.
Linear or convolutional layer If it is a linear layer, the gradients from neuron i in layer l can be
projected back to its input along the weight vector
gl-1,i = wgil .
The gradient of neuron i at layer l - 1 is the sum of all incoming gradients from the layer above:
gil-1 =Xgil-1,j.
j
ReLu layer To propagate through a ReLu, the propagation is blocked if the ReLu was not active
in the forward pass.
gl-1 = gil xli > 0
i 0 otherwise
A.2 Signal visualization
A.2.1 DeConvNet
The DeConvNet visualization is highly similar to the computation of the gradient. The key difference
is how the ReLu’s are treated.
Initialization To compute the visualization of the signal of an output neuron w.r.t. to the input, we
can initialize it at the output neuron with the output value y, for the non-selected output neurons we
initialize it at 0.
output	output
si	= y,	sj 6=i	= 0.
Linear or convolutional layer If it is a linear layer, the signal from neuron i in layer l can be
projected back to its input along the weight vector.
sl-1,i = wsli.
The signal of neuron i at layer l - 1 is the sum of all incoming signals from the layer above:
sli-1 =Xsli-1,j.
j
ReLu layer To propagate through a ReLu, the propagation is blocked if the signal is negative
sl-1 = sli sli > 0
i 0 otherwise
A.2.2 Guided Backpropagation
The Guided Backpropagation visualization is highly similar to the computation of the gradient. and
to DeConvNet. The only difference is how the ReLu’s are treated.
Initialization To compute the visualization of the signal of an output neuron w.r.t. to the input, we
can initialize it at the output neuron with the output value y, for the non-selected output neurons we
initialize it at 0.
output	output
si	= y,	sj 6=i	= 0.
13
Linear or convolutional layer If it is a linear layer, the signal from neuron i in layer l can be
projected back to its input along the weight vector
sl-1,i = wsli.
The signal of neuron i at layer l - 1 is the sum of all incoming signals from the layer above:
sli-1 =Xsli-1,j.
j
ReLu layer To propagate through a ReLu, the propagation is blocked if the signal is negative or if
the neuron was not active in the forward pass
l-1	sli sli > 0 and xli > 0
s- = i	i	i
i 0 otherwise
A.2.3 PatternNet
The PatternNet computation is analogous to the gradient computation. The key difference is that
during the backward pass patterns are used instead of weights.
Initialization To compute the visualization of the signal of an output neuron w.r.t. to the input, we
can initialize it at the output neuron with the output value y, for the non-selected output neurons we
initialize it at 0.
output	output
si	= y,	sj 6=i	= 0.
Linear or convolutional layer If it is a linear layer, the signal from neuron i in layer l can be
projected back to its input along the pattern vector.
sl-1,i = asli .
In our case where we use the Sa+- (x) estimator, this vector is always the positive components a+
since non-active ReLu’s block the propagation.
The signal of neuron i at layer l - 1 is the sum of all incoming signals from the layer above:
sli-1 = Xsli-1,j.
ii
j
ReLu layer To propagate through a ReLu, the propagation is blocked if the neuron was not active
in the forward pass
sl-1 = sli xli > 0
i 0 otherwise
A.3 Attribution visualization
A.3.1 Deep-Taylor Decomposition
Initialization To compute the attribution of an output neuron, we can initialize it at the output
neuron with the output value y, for the non-selected output neurons we initialize it at 0.
output	output
ri = y,	rj6=i	= 0.
Linear or convolutional layer If it is a linear layer, the signal from neuron i in layer l can be
projected back to its input w.r.t. a reference point x0
rl-1,i = W。(X - xC rl
WT x i"
In this work we use the Sa+- (x) estimator. Since a non-active ReLu blocks the propagation, we
only have to use the positive component a+ .
The attribution of neuron i at layer l - 1 is the sum of all incoming attributions from the layer above:
l-1 X l-1,j
ri =	ri	.
j
14
ReLu layer To propagate through a ReLu, the propagation is blocked if the ReLu was not active
in the forward pass.
rl-1 = ril xli > 0
i 0 otherwise
A.3.2 LRP
LRP corresponds to x0 = 0. This allows us to re-write the distribution rule for the linear and
convolutional layers as:
r1-1,i = W Θ X rι
=WT x i.
Division by zero is not an issue since the propagation is blocked at the ReLu if wTx <= 0.
A.3.3 PATTERNATTRIBUTION WITH Sa+-(x)
PatternAttributions corresponds to x0 = x - Sa+- (x). Because propagation is blocked at the ReLu
if wTx <= 0, we can always use x0 = x - a+wT x.
This allows us to re-write the distribution rule for the linear and convolutional layers as:
w x - x + a+wTx
rl-1,i
wTx
ril.
This can be simplified to
rl-1,i = w	a+ril
since division by zero is not an issue because the propagation is blocked at the ReLu if wTx <= 0.
15
B Qualitative comparison to Prediction-Differences analysis
To create the Predictive-Differences analysis visualizations Zintgraf et al. (2017), we used the open-
source code provided by the authors with the default parameter settings provided for VGG.
rapeseed
rapeseed
(1-00)
mongoose
mongoose
(0.93)
mailbox
mailbox
(0.71)
goldfish
goldfish
(1.00)
Kerry Nue terrier
giant schnauzer
(0.47)
Figure 7: Visualization of random images from ImageNet (validation set). In the leftmost shows
column the ground truth, the predicted label and the classifier’s confidence. Comparison between
the proposed methods PatternNet and PatternAttribution to the Prediction-Differences approach by
Zintgraf et al. (2017).
16