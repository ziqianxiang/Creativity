Published as a conference paper at ICLR 2018
Trust-PCL: An Off-Policy
Trust Region Method for Continuous Control
Ofir Nachum, Mohammad Norouzi, Kelvin Xu, & Dale Schuurmans*
{ofirnachum,mnorouzi,kelvinxx,schuurmans}@google.com
Google Brain
Ab stract
Trust region methods, such as TRPO, are often used to stabilize policy optimiza-
tion algorithms in reinforcement learning (RL). While current trust region strate-
gies are effective for continuous control, they typically require a large amount
of on-policy interaction with the environment. To address this problem, we pro-
pose an off-policy trust region method, Trust-PCL, which exploits an observation
that the optimal policy and state values of a maximum reward objective with a
relative-entropy regularizer satisfy a set of multi-step pathwise consistencies along
any path. The introduction of relative entropy regularization allows Trust-PCL to
maintain optimization stability while exploiting off-policy data to improve sample
efficiency. When evaluated on a number of continuous control tasks, Trust-PCL
significantly improves the solution quality and sample efficiency of TRPO.* 1
1	Introduction
The goal of model-free reinforcement learning (RL) is to optimize an agent’s behavior policy
through trial and error interaction with a black box environment. Value-based RL algorithms such
as Q-learning (Watkins, 1989) and policy-based algorithms such as actor-critic (Konda & Tsitsiklis,
2000) have achieved well-known successes in environments with enumerable action spaces and pre-
dictable but possibly complex dynamics, e.g., as in Atari games (Mnih et al., 2013; Van Hasselt et al.,
2016; Mnih et al., 2016). However, when applied to environments with more sophisticated action
spaces and dynamics (e.g., continuous control and robotics), success has been far more limited.
In an attempt to improve the applicability of Q-learning to continuous control, Silver et al. (2014)
and Lillicrap et al. (2015) developed an off-policy algorithm DDPG, leading to promising results
on continuous control environments. That said, current off-policy methods including DDPG often
improve data efficiency at the cost of optimization stability. The behaviour of DDPG is known to
be highly dependent on hyperparameter selection and initialization (Metz et al., 2017); even when
using optimal hyperparameters, individual training runs can display highly varying outcomes.
On the other hand, in an attempt to improve the stability and convergence speed of policy-based
RL methods, Kakade (2002) developed a natural policy gradient algorithm based on Amari (1998),
which subsequently led to the development of trust region policy optimization (TRPO) (Schulman
et al., 2015). TRPO has shown strong empirical performance on difficult continuous control tasks
often outperforming value-based methods like DDPG. However, a major drawback is that such meth-
ods are not able to exploit off-policy data and thus require a large amount of on-policy interaction
with the environment, making them impractical for solving challenging real-world problems.
Efforts at combining the stability of trust region policy-based methods with the sample efficiency of
value-based methods have focused on using off-policy data to better train a value estimate, which
can be used as a control variate for variance reduction (Gu et al., 2017a;b).
In this paper, we investigate an alternative approach to improving the sample efficiency of trust
region policy-based RL methods. We exploit the key fact that, under entropy regularization, the
* Also at the Department of Computing Science, University of Alberta, daes@Ualberta.ca
1An implementation of Trust-PCL is available at https://github.com/tensorflow/models/
tree/master/research/pcl_rl
1
Published as a conference paper at ICLR 2018
optimal policy and value function satisfy a set of pathwise consistency properties along any sam-
pled path (Nachum et al., 2017), which allows both on and off-policy data to be incorporated in
an actor-critic algorithm, PCL. The original PCL algorithm optimized an entropy regularized max-
imum reward objective and was evaluated on relatively simple tasks. Here we extend the ideas of
PCL to achieve strong results on standard, challenging continuous control benchmarks. The main
observation is that by alternatively augmenting the maximum reward objective with a relative en-
tropy regularizer, the optimal policy and values still satisfy a certain set of pathwise consistencies
along any sampled trajectory. The resulting objective is equivalent to maximizing expected reward
subject to a penalty-based constraint on divergence from a reference (i.e., previous) policy.
We exploit this observation to propose a new off-policy trust region algorithm, Trust-PCL, that is
able to exploit off-policy data to train policy and value estimates. Moreover, we present a simple
method for determining the coefficient on the relative entropy regularizer to remain agnostic to
reward scale, hence ameliorating the task of hyperparameter tuning. We find that the incorporation
of a relative entropy regularizer is crucial for good and stable performance. We evaluate Trust-
PCL against TRPO, and observe that Trust-PCL is able to solve difficult continuous control tasks,
while improving the performance of TRPO both in terms of the final reward achieved as well as
sample-efficiency.
2	Related Work
Trust Region Methods. Gradient descent is the predominant optimization method for neural
networks. A gradient descent step is equivalent to solving a trust region constrained optimization,
minimize '(θ + dθ) ≈ '(θ) + V'(θ)Tdθ s. t.	dθTdθ ≤ e ,	(1)
which yields the locally optimal update dθ = -ηV'(θ) such that η = √e∕kV'(θ)k; hence by
considering a Euclidean ball, gradient descent assumes the parameters lie in a Euclidean space.
However, in machine learning, particularly in the context of multi-layer neural network training,
Euclidean geometry is not necessarily the best way to characterize proximity in parameter space.
It is often more effective to define an appropriate Riemannian metric that respects the loss surface
(Amari, 2012), which allows much steeper descent directions to be identified within a local neigh-
borhood (e.g., Amari (1998); Martens & Grosse (2015)). Whenever the loss is defined in terms
of a Bregman divergence between an (unknown) optimal parameter θ* and model parameter θ,
i.e., '(θ) ≡ Df(Θ*, θ), it is natural to use the same divergence to form the trust region:
minimize Df(Θ*, θ + dθ)	s. t.	DF(θ, θ + dθ) ≤ e .	(2)
The natural gradient (Amari, 1998) is a generalization of gradient descent where the Fisher informa-
tion matrix F(θ) is used to define the local geometry of the parameter space around θ. If a parameter
update is constrained by dθTF(θ)dθ ≤ e, a descent direction of dθ ≡ -ηF(θ)-1V'(θ) is obtained.
This geometry is especially effective for optimizing the log-likelihood of a conditional probabilistic
model, where the objective is in fact the KL divergence Dkl(θ*, θ). The local optimization is,
minimize DKL(θ*, θ + dθ) s. t. DKL(θ, θ + dθ) ≈ dθτF(θ)dθ ≤ e .	(3)
Thus, natural gradient approximates the trust region by DKL(a, b) ≈ (a - b)TF(a)(a - b), which
is accurate up to a second order Taylor approximation. Previous work (Kakade, 2002; Bagnell &
Schneider, 2003; Peters & Schaal, 2008; Schulman et al., 2015) has applied natural gradient to policy
optimization, locally improving expected reward subject to variants of dθTF (θ)dθ ≤ . Recently,
TRPO (Schulman et al., 2015; 2016) has achieved state-of-the-art results in continuous control by
adding several approximations to the natural gradient to make nonlinear policy optimization feasible.
Another approach to trust region optimization is given by proximal gradient methods (Parikh et al.,
2014). The class of proximal gradient methods most similar to our work are those that replace the
hard constraint in (2) with a penalty added to the objective. These techniques have recently become
popular in RL (Wang et al., 2016; Heess et al., 2017; Schulman et al., 2017b), although in terms
of final reward performance on continuous control benchmarks, TRPO is still considered to be the
state-of-the-art.
2
Published as a conference paper at ICLR 2018
Norouzi et al. (2016) make the observation that entropy regularized expected reward may be ex-
pressed as a reversed KL divergence DKL (θ, θ*), which suggests that an alternative to the constraint
in (3) should be used when such regularization is present:
minimize DKL(θ + dθ, θ*) s. t. DKL(θ + dθ, θ) ≈ dθTF(θ + dθ)dθ ≤ e .	(4)
Unfortunately, this update requires computing the Fisher matrix at the endpoint of the update. The
use of F(θ) in previous work can be considered to be an approximation when entropy regularization
is present, but it is not ideal, particularly if dθ is large. In this paper, by contrast, we demonstrate that
the optimal dθ under the reverse KL constraint DKL (θ + dθ, θ) ≤ can indeed be characterized.
Defining the constraint in this way appears to be more natural and effective than that of TRPO.
Softmax Consistency. To comply with the information geometry over policy parameters, previous
work has used the relative entropy (i.e., KL divergence) to regularize policy optimization; resulting
in a softmax relationship between the optimal policy and state values (Peters et al., 2010; Azar et al.,
2012; 2011; Fox et al., 2016; Rawlik et al., 2013) under single-step rollouts. Our work is unique in
that we leverage consistencies over multi-step rollouts.
The existence of multi-step softmax consistencies has been noted by prior work—first by Nachum
et al. (2017) in the presence of entropy regularization. The existence of the same consistencies with
relative entropy has been noted by Schulman et al. (2017a). Our work presents multi-step con-
sistency relations for a hybrid relative entropy plus entropy regularized expected reward objective,
interpreting relative entropy regularization as a trust region constraint. This work is also distinct
from prior work in that the coefficient of relative entropy can be automatically determined, which
we have found to be especially crucial in cases where the reward distribution changes dramatically
during training.
Most previous work on softmax consistency (e.g., Fox et al. (2016); Azar et al. (2012); Nachum et al.
(2017)) have only been evaluated on relatively simple tasks, including grid-world and discrete algo-
rithmic environments. Rawlik et al. (2013) conducted evaluations on simple variants of the CartPole
and Pendulum continuous control tasks. More recently, Haarnoja et al. (2017) showed that soft Q-
learning (a single-step special case of PCL) can succeed on more challenging environments, such as
a variant of the Swimmer task we consider below. By contrast, this paper presents a successful appli-
cation of the softmax consistency concept to difficult and standard continuous-control benchmarks,
resulting in performance that is competitive with and in some cases beats the state-of-the-art.
3	Notation & Background
We model an agent’s behavior by a policy distribution π(a | s) over a set of actions (possibly discrete
or continuous). At iteration t, the agent encounters a state st and performs an action at sampled from
π(a | st). The environment then returns a scalar reward r 〜 r(st, at) and transitions to the next
state st+ι 〜ρ(st,at). When formulating expectations over actions, rewards, and state transitions
we will often omit the sampling distributions, π, r, and ρ, respectively.
Maximizing Expected Reward. The standard objective in RL is to maximize expected future
discounted reward. We formulate this objective on a per-state basis recursively as
OER(s, π) = Ea,r,s0 [r + γOER(s0, π)] .	(5)
The overall, state-agnostic objective is the expected per-state objective when states are sampled from
interactions with the environment:
OER(π) =Es[OER(s,π)].	(6)
Most policy-based algorithms, including REINFORCE (Williams & Peng, 1991) and actor-
critic (Konda & Tsitsiklis, 2000), aim to optimize OER given a parameterized policy.
Path Consistency Learning (PCL). Inspired by Williams & Peng (1991), Nachum et al. (2017)
augment the objective OER in (5) with a discounted entropy regularizer to derive an objective,
OENT(s, π) = OER(s, π) + τ H(s, π) ,	(7)
where τ ≥ 0 is a user-specified temperature parameter that controls the degree of entropy regular-
ization, and the discounted entropy H(s, π) is recursively defined as
H(s,π) = Ea,s0 [- log π(a | s) + γH(s0, π)] .	(8)
3
Published as a conference paper at ICLR 2018
Note that the objective OENT (s, π) can then be re-expressed recursively as,
OENT(s,π) = Ea,r,s0 [r - τ logπ(a | s) + γOENT(s0,π)] .
(9)
Nachum et al. (2017) show that the optimal policy ∏ for OENT and V*(s)=OENT(s, ∏*) mutually
satisfy a softmax temporal consistency constraint along any sequence of states s0, . . . , sd starting at
s0 and a corresponding sequence of actions a0, . . . , ad-1:
V *(s0)= E
ri,si
d-1
YdV*(sd) + X Yi(ri - T log∏*(ai∣Si)) ∙
i=0
(10)
This observation led to the development of the PCL algorithm, which attempts to minimize squared
error between the LHS and RHS of (10) to simultaneously optimize parameterized πθ and Vφ . Im-
portantly, PCL is applicable to both on-policy and off-policy trajectories.
Trust Region Policy Optimization (TRPO). As noted, standard policy-based algorithms for max-
imizing OER can be unstable and require small learning rates for training. To alleviate this issue,
Schulman et al. (2015) proposed to perform an iterative trust region optimization to maximize OER.
At each step, a prior policy ∏ is used to sample a large batch of trajectories, then ∏ is subsequently
optimized to maximize OER while remaining within a constraint defined by the average per-state
KL-divergence with ∏. That is, at each iteration TRPO solves the constrained optimization problem,
maximize Oer(π)	s. t.	Es〜∏ ρ[KL (∏(-∣s) ∣∣ π(-∣s)) ] ≤ e.	(11)
π,
The prior policy is then replaced with the new policy π, and the process is repeated.
4	Method
To enable more stable training and better exploit the natural information geometry of the parameter
space, we propose to augment the entropy regularized expected reward objective OENT in (7) with a
discounted relative entropy trust region around a prior policy ∏,
maximize Es[Οent(∏)]	s. t.	Es[G(s,π,Π)] ≤ e ,	(12)
π
where the discounted relative entropy is recursively defined as
G(s,π,∏) = Ea,s0 [logπ(a∣s) - log∏(a∣s) + γG(s0,π,Π)].	(13)
This objective attempts to maximize entropy regularized expected reward while maintaining natural
proximity to the previous policy. Although previous work has separately proposed to use relative
entropy and entropy regularization, we find that the two components serve different purposes, each
of which is beneficial: entropy regularization helps improve exploration, while the relative entropy
improves stability and allows for a faster learning rate. This combination is a key novelty.
Using the method of Lagrange multipliers, we cast the constrained optimization problem in (13) into
maximization of the following objective,
ORELENT(s,∏)=OENT(S,∏) - λG(s,Π,Π) .	(14)
Again, the environment-wide objective is the expected per-state objective when states are sampled
from interactions with the environment,
ORELENT (π) = Es [ORELENT(s, π)].	(15)
4.1	Path Consistency with Relative Entropy
A key technical observation is that the ORELENT objective has a similar decomposition structure to
OENT, and one can cast ORELENT as an entropy regularized expected reward objective with a set of
transformed rewards, i.e.,
〜
ORELENT(s, π) = OER(s, π) + (τ + λ)H(s, π),	(16)
4
Published as a conference paper at ICLR 2018
〜
where OER(s, π) is an expected reward objective on a transformed reward distribution function
r(s, a) = r(s, a) + λlog∏(a∣s). Thus, in What follows, We derive a corresponding form of the
multi-step path consistency in (10).
Let ∏ denote the optimal policy, defined as ∏ = argmax∏ ORELENT (∏). As in PCL (Nachum et al.,
2017), this optimal policy may be expressed as
π*(at∣st) = exp { ErtHst，at)，st+1 [rt]：*("+1)] - V*(St) } ,	(17)
where V * are the softmax state values defined recursively as
V *(st) = (τ + λ)log / exp { Ert~r(St,a),s; [rt： YV *(st+1)] }da.	(18)
We may re-arrange (17) to yield
V *(St) = EFt 〜r(st,at),st+ι [rt - (T + λ) log n*(at|st) + YV *(st+1)]	(19)
=Ert,st+1 [rt - (τ + λ) logπ*(at∣St) + λlog∏(at+i∣st+i) + γV*(st+ι)]. (20)
This is a single-step temporal consistency which may be extended to multiple steps by further ex-
panding V* (St+1) on the RHS using the same identity. Thus, in general we have the following
softmax temporal consistency constraint along any sequence of states defined by a starting state St
and a sequence of actions at, . . . , at+d-1:
d-1
V * (st) = E	YdV *(st+d) + X Yi (rt+i - (T + λ) log ∏*(at+i∣st+i) + λ log ∏(at+i∣st+i))
rt+i,st+i	i=0
(21)
4.2	TRUST-PCL
We propose to train a parameterized policy πθ and value estimate Vφ to satisfy the multi-step con-
sistencies in (21). Thus, we define a consistency error for a sequence of states, actions, and rewards
St:t+d ≡ (St, at, rt, . . . , St+d-1, at+d-1,rt+d-1, St+d) sampled from the environment as
C (St:t+d, θ, φ) = - Vφ(St) + YdVφ(St+d) +
d-1	(22)
EYi (rt+i — (τ + λ) log∏θ(at+i∣st+i) + λlog∏j(at+i∣st+i)).
i=0
We aim to minimize the squared consistency error on every sub-trajectory of length d. That is, the
loss for a given batch of episodes (or sub-episodes) S = {S(0k:T) }kB=1 is
B Tk -1
L(S,θ,φ)=X X C(S(t:kt)+d, θ, φ)2.	(23)
k=1 t=0
We perform gradient descent on θ and φ to minimize this loss. In practice, we have found that it
is beneficial to learn the parameter φ at least as fast as θ, and accordingly, given a mini-batch of
episodes we perform a single gradient update on θ and possibly multiple gradient updates on φ (see
Appendix for details).
In principle, the mini-batch S may be taken from either on-policy or off-policy trajectories. In our
implementation, we utilized a replay buffer prioritized by recency. As episodes (or sub-episodes)
are sampled from the environment they are placed in a replay buffer and a priority p(S0:T) is given
to a trajectory S0:T equivalent to the current training step. Then, to sample a batch for training, B
episodes are sampled from the replay buffer proportional to exponentiated priority exp{βp(s0:T)}
for some hyperparameter β ≥ 0.
For the prior policy πθF, we use a lagged geometric mean of the parameters. At each training step,
we update θ J αθ + (1 — α)θ. Thus on average our training scheme attempts to maximize entropy
regularized expected reward while penalizing divergence from a policy roughly 1/(1 - α) training
steps in the past.
5
Published as a conference paper at ICLR 2018
4.3	AUTOMATIC TUNING OF THE LAGRANGE MULTIPLIER λ
The use of a relative entropy regularizer as a penalty rather than a constraint introduces several
difficulties. The hyperparameter λ must necessarily adapt to the distribution of rewards. Thus, λ
must be tuned not only to each environment but also during training on a single environment, since
the observed reward distribution changes as the agent’s behavior policy improves. Using a constraint
form of the regularizer is more desirable, and others have advocated its use in practice (Schulman
et al., 2015) specifically to robustly allow larger updates during training.
To this end, we propose to redirect the hyperparameter tuning from λ to . Specifically, we present a
method which, given a desired hard constraint on the relative entropy defined by , approximates the
equivalent penalty coefficient λ(). This is a key novelty of our work and is distinct from previous
attempts at automatically tuning a regularizing coefficient, which iteratively increase and decrease
the coefficient based on observed training behavior (Schulman et al., 2017b; Heess et al., 2017).
We restrict our analysis to the undiscounted setting γ = 1 with entropy regularizer τ = 0. Addi-
tionally, we assume deterministic, finite-horizon environment dynamics. An additional assumption
we make is that the expected KL-divergence over states is well-approximated by the KL-divergence
starting from the unique initial state s0 . Although in our experiments these restrictive assumptions
are not met, we still found our method to perform well for adapting λ during training.
In this setting the optimal policy of (14) is proportional to exponentiated scaled reward. Specifically,
for a full episode s0:T = (s0, a0, r0, . . . , sT-1, aT-1, rT-1, sT), we have
Π*(s0:T )
H Π(s0:T)exp
f R(s0：T) }
(24)
where n(so：T) = QT=o1 ∏(a∕si) and R(so：T) = PT=o1 r. The normalization factor of ∏* is
Z = Eso：T〜π exp { R(sλ0Tl}[	(25)
We would like to approximate the trajectory-wide KL-divergence between ∏ and ∏. We may ex-
press the KL-divergence analytically:
KL(π*∣∣∏) = Eso：t〜∏* [log ππ*(s0:T)}]	(26)
L	"(so：T) 力
=ES0:T〜π* —( ；T) - log Z	(27)
λ
lcσ∙7j!P	∣^R(S0：T) π*(S0：T )1	OG
= -log Z + Eso：T〜∏ -----ʌ--- ∙ FT---U	(28)
L λ	∏(S0:T)」
=-log Z + Es0T〜∏ R(弋:T) exp{R(so:T)∕λ - log Z} .	(29)
λ
Since all expectations are with respect to ∏, this quantity is tractable to approximate given episodes
sampled from ∏
Therefore, in Trust-PCL, given a set of episodes sampled from the prior policy ∏ and a desired
maximum divergence , we can perform a simple line search to find a suitable λ() which yields
KL(∏^∖∖∏ρ) as close as possible to U
The preceding analysis provided a method to determine λ() given a desired maximum divergence
U. However, there is still a question of whether U should change during training. Indeed, as episodes
may possibly increase in length, KL(∏*∖∖Π) naturally increases when compared to the average per-
state KL(Π(-∖s)∖∖∏(-∖s)), and vice versa for decreasing length. Thus, in practice, given an E and
a set of sampled episodes S = {S(0k：T) }kN=1, we approximate the best λ which yields a maximum
divergence of N PN=I Tk. This makes it so that E corresponds more to a constraint on the length-
averaged KL-divergence.
To avoid incurring a prohibitively large number of interactions with the environment for each pa-
rameter update, in practice we use the last 100 episodes as the set of sampled episodes S. While
6
Published as a conference paper at ICLR 2018
this is not exactly the same as sampling episodes from ∏q, it is not too far off since ∏ is a lagged
version of the online policy πθ . Moreover, we observed this protocol to work well in practice. A
more sophisticated and accurate protocol may be derived by weighting the episodes according to the
importance weights corresponding to their true sampling distribution.
5	Experiments
We evaluate Trust-PCL against TRPO on a number of benchmark tasks. We choose TRPO as a base-
line since it is a standard algorithm known to achieve state-of-the-art performance on the continuous
control tasks we consider (see e.g., leaderboard results on the OpenAI Gym website (Brockman
et al., 2016)). We find that Trust-PCL can match or improve upon TRPO’s performance in terms of
both average reward and sample efficiency.
5.1	Setup
We chose a number of control tasks available from OpenAI Gym (Brockman et al., 2016). The
first task, Acrobot, is a discrete-control task, while the remaining tasks (HalfCheetah, Swimmer,
Hopper, Walker2d, and Ant) are well-known continuous-control tasks utilizing the MuJoCo envi-
ronment (Todorov et al., 2012).
For TRPO we trained using batches of Q = 25, 000 steps (12, 500 for Acrobot), which is the
approximate batch size used by other implementations (Duan et al., 2016; Schulman, 2017). Thus,
at each training iteration, TRPO samples 25,000 steps using the policy ∏ and then takes a single
step within a KL-ball to yield a new πθ .
Trust-PCL is off-policy, so to evaluate its performance we alternate between collecting experience
and training on batches of experience sampled from the replay buffer. Specifically, we alternate
between collecting P = 10 steps from the environment and performing a single gradient step based
on a batch of size Q = 64 sub-episodes of length P from the replay buffer, with a recency weight of
β = 0.001 on the sampling distribution of the replay buffer. To maintain stability we use α = 0.99
and we modified the loss from squared loss to Huber loss on the consistency error. Since our policy
is parameterized by a unimodal Gaussian, it is impossible for it to satisfy all path consistencies, and
so we found this crucial for stability.
For each of the variants and for each environment, we performed a hyperparameter search to find
the best hyperparameters. The plots presented here show the reward achieved during training on the
best hyperparameters averaged over the best 4 seeds of 5 randomly seeded training runs. Note that
this reward is based on greedy actions (rather than random sampling).
Experiments were performed using Tensorflow (Abadi et al., 2016). Although each training step of
Trust-PCL (a simple gradient step) is considerably faster than TRPO, we found that this does not
have an overall effect on the run time of our implementation, due to a combination of the fact that
each environment step is used in multiple training steps of Trust-PCL and that a majority of the run
time is spent interacting with the environment. A detailed description of our implementation and
hyperparameter search is available in the Appendix.
5.2	Results
We present the reward over training of Trust-PCL and TRPO in Figure 1. We find that Trust-PCL can
match or beat the performance of TRPO across all environments in terms of both final reward and
sample efficiency. These results are especially significant on the harder tasks (Walker2d and Ant).
We additionally present our results compared to other published results in Table 1. We find that even
when comparing across different implementations, Trust-PCL can match or beat the state-of-the-art.
5.2.1	Hyperparameter Analysis
The most important hyperparameter in our method is , which determines the size of the trust region
and thus has a critical role in the stability of the algorithm. To showcase this effect, we present the
reward during training for several different values of in Figure 2. As increases, instability in-
creases as well, eventually having an adverse effect on the agent’s ability to achieve optimal reward.
7
Published as a conference paper at ICLR 2018
TrUst-PCL	TRPO
Figure 1: The results of TrUst-PCL against a TRPO baseline. Each plot shows average greedy
reward with single standard deviation error intervals capped at the min and max across 4 best of 5
randomly seeded training runs after choosing best hyperparameters. The x-axis shows millions of
environment steps. We observe that Trust-PCL is consistently able to match and, in many cases,
beat TRPO,s performance both in terms of reward and sample efficiency.
Walker2d
Figure 2: The results of Trust-PCL across several values of e, defining the size of the trust re-
gion. Each plot shows average greedy reward across 4 best of 5 randomly seeded training runs after
choosing best hyperparameters. The x-axis shows millions of environment steps. We observe that
instability increases with e, thus concluding that the use of trust region is crucial.
Note that standard PCL (Nachum et al., 2017) corresponds to e → ∞ (that is, λ = 0). Therefore,
standard PCL would fail in these environments, and the use of trust region is crucial.
The main advantage of Trust-PCL over existing trust region methods for continuous control is its
ability to learn in an off-policy manner. The degree to which Trust-PCL is off-policy is determined
by a combination of the hyparparameters α, β, and P. To evaluate the importance of training
off-policy, we evaluate Trust-PCL with a hyperparameter setting that is more on-policy. We set
α = 0.95, β = 0.1, and P = 1, 000. In this setting, we also use large batches ofQ = 25 episodes of
length P (a total of 25, 000 environment steps per batch). Figure 3 shows the results of Trust-PCL
with our original parameters and this new setting. We note a dramatic advantage in sample efficiency
when using off-policy training. Although Trust-PCL (on-policy) can achieve state-of-the-art reward
performance, it requires an exorbitant amount of experience. On the other hand, Trust-PCL (off-
8
Published as a conference paper at ICLR 2018
Hopper
TrUst-PCL (on-policy)	TrUst-PCL
Figure 3: The results of Trust-PCL varying the degree of on/off-policy. We see that Trust-PCL
(on-policy) has a behavior similar to TRPO, achieving good final reward but requiring an exorbitant
number of experience collection. When collecting less experience per training step in Trust-PCL
(off-policy), we are able to improve sample efficiency while still achieving a competitive final re-
ward.
Domain	TRPO-GAE	TRPO(rllab)	TRPO (ours)	Trust-PCL	IPG
HalfCheetah	-4871.36-	2889	4343.6	-7057.1-	4767
Swimmer	137.25	一	288.1	297.0	—
Hopper	3765.78	一	3516.7	3804.9	—
Walker2d	6028.73	1487	2838.4	5027.2	3047
Ant	2918.25	1520	4347.5	6104.2	4415
Table 1: Results for best average reward in the first 10M steps of training for our implementations
(TRPO (ours) and Trust-PCL) and external implementations. TRPO-GAE are results of Schulman
(2017) available on the OpenAI Gym website. TRPO (rllab) and IPG are taken from Gu et al.
(2017b). These results are each on different setups with different hyperparameter searches and in
some cases different evaluation protocols (e.g.,TRPO (rllab) and IPG were run with a simple linear
value network instead of the two-hidden layer network we use). Thus, it is not possible to make any
definitive claims based on this data. However, we do conclude that our results are overall competitive
with state-of-the-art external implementations.
policy) can be competitive in terms of reward while providing a significant improvement in sample
efficiency.
One last hyperparameter is τ , determining the degree of exploration. Anecdotally, we found τ to
not be of high importance for the tasks we evaluated. Indeed many of our best results use τ =
0. Including τ > 0 had a marginal effect, at best. The reason for this is likely due to the tasks
themselves. Indeed, other works which focus on exploration in continuous control have found the
need to propose exploration-advanageous variants of these standard benchmarks (Haarnoja et al.,
2017; Houthooft et al., 2016).
6 Conclusion
We have presented Trust-PCL, an off-policy algorithm employing a relative-entropy penalty to im-
pose a trust region on a maximum reward objective. We found that Trust-PCL can perform well on a
set of standard control tasks, improving upon TRPO both in terms of average reward and sample effi-
ciency. Our best results on Trust-PCL are able to maintain the stability and solution quality of TRPO
while approaching the sample-efficiency of value-based methods (see e.g., Metz et al. (2017)). This
gives hope that the goal of achieving both stability and sample-efficiency without trading-off one for
the other is attainable in a single unifying RL algorithm.
9
Published as a conference paper at ICLR 2018
7 Acknowledgment
We thank Matthew Johnson, Luke Metz, Shane Gu, and the Google Brain team for insightful com-
ments and discussions.
References
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, MatthieU
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for large-
scale machine learning. arXiv:1605.08695, 2016.
Shun-Ichi Amari. Natural gradient works efficiently in learning. Neural Comput., 10, 1998.
Shun-Ichi Amari. Differential-geometrical methods in statistics, volume 28. Springer Science &
Business Media, 2012.
Mohammad Gheshlaghi Azar, Vicenc Gomez, and Hilbert J Kappen. Dynamic policy programming
with function approximation. AISTATS, 2011.
Mohammad Gheshlaghi Azar, Vicenc Gomez, and Hilbert J Kappen. Dynamic policy programming.
JMLR, 13, 2012.
J Andrew Bagnell and Jeff Schneider. Covariant policy search. 2003.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. OpenAI Gym. arXiv:1606.01540, 2016.
Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep
reinforcement learning for continuous control. 2016.
Roy Fox, Ari Pakman, and Naftali Tishby. G-learning: Taming the noise in reinforcement learning
via soft updates. Uncertainty in Artifical Intelligence, 2016. URL http://arxiv.org/abs/
1512.08562.
Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard E Turner, and Sergey Levine. Q-prop:
Sample-efficient policy gradient with an off-policy critic. ICLR, 2017a.
Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard E Turner, Bernhard Scholkopf, and
Sergey Levine. Interpolated policy gradient: Merging on-policy and off-policy gradient estimation
for deep reinforcement learning. arXiv preprint arXiv:1706.00387, 2017b.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with
deep energy-based policies. arXiv preprint arXiv:1702.08165, 2017.
Nicolas Heess, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, Yuval Tassa, Tom Erez,
Ziyu Wang, Ali Eslami, Martin Riedmiller, et al. Emergence of locomotion behaviours in rich
environments. arXiv preprint arXiv:1707.02286, 2017.
Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime:
Variational information maximizing exploration. In Advances in Neural Information Processing
Systems ,pp.1109-1117, 2016.
Sham M Kakade. A natural policy gradient. In NIPS, 2002.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. ICLR, 2015.
Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms, 2000.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate
curvature. In ICML, 2015.
10
Published as a conference paper at ICLR 2018
Luke Metz, Julian Ibarz, Navdeep Jaitly, and James Davidson. Discrete sequential prediction of
continuous actions for deep RL. CoRR, abs/1705.05035, 2017. URL http://arxiv.org/
abs/1705.05035.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. arXiv:1312.5602,
2013.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy P Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. ICML, 2016.
Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between
value and policy based reinforcement learning. CoRR, abs/1702.08892, 2017. URL http:
//arxiv.org/abs/1702.08892.
Mohammad Norouzi, Samy Bengio, Zhifeng Chen, Navdeep Jaitly, Mike Schuster, Yonghui Wu,
and Dale Schuurmans. Reward augmented maximum likelihood for neural structured prediction.
NIPS, 2016.
Neal Parikh, Stephen Boyd, et al. Proximal algorithms. Foundations and TrendsR in Optimization,
1(3):127-239, 2014.
Jan Peters and Stefan Schaal. Reinforcement learning of motor skills with policy gradients. Neural
networks, 21, 2008.
Jan Peters, Katharina Mulling, and Yasemin Altun. Relative entropy policy search. In AAAI, 2010.
Konrad Rawlik, Marc Toussaint, and Sethu Vijayakumar. On stochastic optimal control and rein-
forcement learning by approximate inference. In Twenty-Third International Joint Conference on
Artificial Intelligence, 2013.
John Schulman. Modular rl. http://github.com/joschu/modular_rl, 2017. Accessed:
2017-06-01.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In ICML, 2015.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-
dimensional continuous control using generalized advantage estimation. ICLR, 2016.
John Schulman, Pieter Abbeel, and Xi Chen. Equivalence between policy gradients and soft q-
learning. arXiv preprint arXiv:1704.06440, 2017a.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017b.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In Proceedings of the 31st International Conference on
Machine Learning (ICML-14), pp. 387-395, 2014.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026-
5033. IEEE, 2012.
Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. AAAI, 2016.
Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu,
and Nando de Freitas. Sample efficient actor-critic with experience replay. arXiv preprint
arXiv:1611.01224, 2016.
Christopher John Cornish Hellaby Watkins. Learning from delayed rewards. PhD thesis, University
of Cambridge England, 1989.
Ronald J Williams and Jing Peng. Function optimization using connectionist reinforcement learning
algorithms. Connection Science, 1991.
11
Published as a conference paper at ICLR 2018
A Implementation Benefits of Trust-PCL
We have already highlighted the ability of Trust-PCL to use off-policy data to stably train both a
parameterized policy and value estimate, which sets it apart from previous methods. We have also
noted the ease with which exploration can be incorporated through the entropy regularizer. We
elaborate on several additional benefits of Trust-PCL.
Compared to TRPO, Trust-PCL is much easier to implement. Standard TRPO implementations
perform second-order gradient calculations on the KL-divergence to construct a Fisher information
matrix (more specifically a vector product with the inverse Fisher information matrix). This yields a
vector direction for which a line search is subsequently employed to find the optimal step. Compare
this to Trust-PCL which employs simple gradient descent. This makes implementation much more
straightforward and easily realizable within standard deep learning frameworks.
Even if one replaces the constraint on the average KL-divergence of TRPO with a simple regu-
larization penalty (as in proximal policy gradient methods (Schulman et al., 2017b; Wang et al.,
2016)), optimizing the resulting objective requires computing the gradient of the KL-divergence.
In Trust-PCL, there is no such necessity. The per-state KL-divergence need not have an analyt-
ically computable gradient. In fact, the KL-divergence need not have a closed form at all. The
only requirement of Trust-PCL is that the log-density be analytically computable. This opens up
the possible policy parameterizations to a much wider class of functions. While continuous control
has traditionally used policies parameterized by unimodal Gaussians, with Trust-PCL the policy can
be replaced with something much more expressive—for example, mixtures of Gaussians or auto-
regressive policies as in Metz et al. (2017).
We have yet to fully explore these additional benefits in this work, but we hope that future investi-
gations can exploit the flexibility and ease of implementation of Trust-PCL to further the progress
of RL in continuous control environments.
B	Experimental Setup
We describe in detail the experimental setup regarding implementation and hyperparameter search.
B.1	Environments
In Acrobot, episodes were cut-off at step 500. For the remaining environments, episodes were cut-
off at step 1, 000.
Acrobot, HalfCheetah, and Swimmer are all non-terminating environments. Thus, for these envi-
ronments, each episode had equal length and each batch contained the same number of episodes.
Hopper, Walker2d, and Ant are environments that can terminate the agent. Thus, for these environ-
ments, the batch size throughout training remained constant in terms of steps but not in terms of
episodes.
There exists an additional common MuJoCo task called Humanoid. We found that neither our
implementation of TRPO nor Trust-PCL could make more than negligible headway on this task,
and so omit it from the results. We are aware that TRPO with the addition of GAE and enough fine-
tuning can be made to achieve good results on Humanoid (Schulman et al., 2016). We decided to
not pursue a GAE implementation to keep a fair comparison between variants. Trust-PCL can also
be made to incorporate an analogue to GAE (by maintaining consistencies at varying time scales),
but we leave this to future work.
B.2	Implementation Details
We use fully-connected feed-forward neural networks to represent both policy and value.
The policy πθ is represented by a neural network with two hidden layers of dimension 64 with tanh
activations. At time step t, the network is given the observation st. It produces a vector μt, which
is combined with a learnable (but t-agnostic) parameter ξ to parametrize a unimodal Gaussian with
mean μt and standard deviation exp(ξ). The next action at is sampled randomly from this Gaussian.
12
Published as a conference paper at ICLR 2018
The value network Vφ is represented by a neural network with two hidden layers of dimension 64
with tanh activations. At time step t the network is given the observation st and the component-wise
squared observation st st . It produces a single scalar value.
B.2.1	TRPO LEARNING
At each training iteration, both the policy and value parameters are updated. The policy is trained
by performing a trust region step according to the procedure described in Schulman et al. (2015).
The value parameters at each step are solved using an LBFGS optimizer. To avoid instability, the
value parameters are solved to fit a mixture of the empirical values and the expected values. That
is, We determine φ to minimize ∑s∈batch(Vφ(s) 一 κVφ(s) 一 (1 一 κ)Vφ(s))2, where again φ is the
previous value parameterization. We use κ = 0.9. This method for training φ is according to that
used in Schulman (2017).
B.2.2	Trust-PCL Learning
At each training iteration, both the policy and value parameters are updated. The specific updates
are slightly different between Trust-PCL (on-policy) and Trust-PCL (off-policy).
For Trust-PCL (on-policy), the policy is trained by taking a single gradient step using the Adam
optimizer (Kingma & Ba, 2015) with learning rate 0.001. The value network update is inspired by
that used in TRPO we perform 5 gradients steps with learning rate 0.001, calculated with regards
to a mix between the empirical values and the expected values according to the previous φ. We use
κ = 0.95.
For Trust-PCL (off-policy), both the policy and value parameters are updated in a single step using
the Adam optimizer with learning rate 0.0001. For this variant, we also utilize a target value network
(lagged at the same rate as the target policy network) to replace the value estimate at the final state
for each path. We do not mix between empirical and expected values.
B.3	Hyperparameter Search
We found the most crucial hyperparameters for effective learning in both TRPO and Trust-
PCL to be (the constraint defining the size of the trust region) and d (the rollout determin-
ing how to evaluate the empirical value of a state). For TRPO we performed a grid search
over ∈ {0.01, 0.02, 0.05, 0.1}, d ∈ {10, 50}. For Trust-PCL we performed a grid search over
∈ {0.001, 0.002, 0.005, 0.01}, d ∈ {10, 50}. For Trust-PCL we also experimented with the value
of τ, either keeping it at a constant 0 (thus, no exploration) or decaying it from 0.1 to 0.0 by a
smoothed exponential rate of 0.1 every 2,500 training iterations.
We fix the discount to γ = 0.995 for all environments.
C Pseudocode
A simplified pseudocode for Trust-PCL is presented in Algorithm 1.
13
Published as a conference paper at ICLR 2018
Algorithm 1 TrUst-PCL
Input: Environment ENV, trust region constraint e, learning rates η∏「仇,discount factor Y,
rolloUt d, batch size Q, collect steps per train step P, nUmber of training steps N, replay bUffer
RB with exponential lag β, lag on prior policy α.
function Gradients({st(:kt)+P }kB=1)
// C is the consistency error defined in Equation 22.
ComPUte δθ = PB=I PP=OIC(S(3+p+d, θ, φ)VθC(st+p:t+p+d , θ, φ).
ComPute ∆φ = PkB=1 PpP=-01 C(St(+k)p:t+p+d, θ, φ)VφC(
St+p:t+p+d , θ, φ).
Return ∆θ, ∆φ
end function
Initialize θ, φ, λ, set θ = θ.
Initialize emPty rePlay buffer RB(β).
for i = 0 to N - 1 do
// Collect
Sample P steps st：t+ P 〜 ∏θ on ENV.
Insert St:t+P to RB.
// Train
Sample batch {St(:kt)+P}kB=1 from RB to contain a total of Q transitions (B ≈ Q/P).
∆θ, ∆φ = Gradients({S(t:kt)+P}kB=1).
Update θ 一 θ 一 η∏ ∆θ.
Update φ J φ 一 % ∆φ.
// Update auxiliary variables
Update θ = αθ + (1 一 α)θ.
Update λ in terms of according to Section 4.3.
end for
14