Published as a conference paper at ICLR 2018
Learning Differentially Private Recurrent
Language Models
H. Brendan McMahan
mcmahan@google.com
Daniel Ramage
dramage@google.com
Kunal Talwar
kunal@google.com
Li Zhang
liqzhang@google.com
Ab stract
We demonstrate that it is possible to train large recurrent language models with
user-level differential privacy guarantees with only a negligible cost in predictive
accuracy. Our work builds on recent advances in the training of deep networks
on user-partitioned data and privacy accounting for stochastic gradient descent. In
particular, we add user-level privacy protection to the federated averaging algo-
rithm, which makes “large step” updates from user-level data. Our work demon-
strates that given a dataset with a sufficiently large number of users (a requirement
easily met by even small internet-scale datasets), achieving differential privacy
comes at the cost of increased computation, rather than in decreased utility as
in most prior work. We find that our private LSTM language models are quan-
titatively and qualitatively similar to un-noised models when trained on a large
dataset.
1	Introduction
Deep recurrent models like long short-term memory (LSTM) recurrent neural networks (RNNs)
have become a standard building block in modern approaches to language modeling, with applica-
tions in speech recognition, input decoding for mobile keyboards, and language translation. Because
language usage varies widely by problem domain and dataset, training a language model on data
from the right distribution is critical. For example, a model to aid typing on a mobile keyboard is
better served by training data typed in mobile apps rather than from scanned books or transcribed
utterances. However, language data can be uniquely privacy sensitive. In the case of text typed
on a mobile phone, this sensitive information might include passwords, text messages, and search
queries. In general, language data may identify a speaker—explicitly by name or implicitly, for
example via a rare or unique phrase—and link that speaker to secret or sensitive information.
Ideally, a language model’s parameters would encode patterns of language use common to many
users without memorizing any individual user’s unique input sequences. However, we know convo-
lutional NNs can memorize arbitrary labelings of the training data (Zhang et al., 2017) and recurrent
language models are also capable of memorizing unique patterns in the training data (Carlini et al.,
2018). Recent attacks on neural networks such as those of Shokri et al. (2017) underscore the im-
plicit risk. The main goal of our work is to provide a strong guarantee that the trained model protects
the privacy of individuals’ data without undue sacrifice in model quality.
We are motivated by the problem of training models for next-word prediction in a mobile keyboard,
and use this as a running example. This problem is well suited to the techniques we introduce, as
differential privacy may allow for training on data from the true distribution (actual mobile usage)
rather than on proxy data from some other source that would produce inferior models. However,
to facilitate reproducibility and comparison to non-private models, our experiments are conducted
on a public dataset as is standard in differential privacy research. The remainder of this paper is
structured around the following contributions:
1.	We apply differential privacy to model training using the notion of user-adjacent datasets, leading
to formal guarantees of user-level privacy, rather than privacy for single examples.
1
Published as a conference paper at ICLR 2018
2.	We introduce a noised version of the federated averaging algorithm (McMahan et al., 2016)
in §2, which satisfies user-adjacent differential privacy via use of the moments accountant (Abadi
et al., 2016a) first developed to analyze differentially private stochastic gradient descent (SGD) for
example-level privacy. The federated averaging approach groups multiple SGD updates together,
enabling large-step model updates.
3.	We demonstrate the first high quality LSTM language model trained with strong privacy guar-
antees in §3, showing no significant decrease in model accuracy given a large enough dataset. For
example, on a dataset of 763,430 users, baseline (non-private) training achieves an accuracy of
17.5% in 4120 rounds of training, where we use the data from 100 random users on each round. We
achieve this same level of accuracy with (4.6, 10-9)-differential privacy in 4980 rounds, processing
on average 5000 users per round—maintaining the same level of accuracy at a significant compu-
tational cost of roughly 60×.1 Running the same computation on a larger dataset with 108 users
would improve the privacy guarantee to (1.2, 10-9). We guarantee privacy and maintain utility de-
spite the complex internal structure of the LSTM—with per-word embeddings as well as dense state
transitions—by using the federated averaging algorithm. We demonstrate that the noised model’s
metrics and qualitative behavior (with respect to head words) does not differ significantly from the
non-private model. To our knowledge, our work represents the most sophisticated machine learning
model, judged by the size and the complexity of the model, ever trained with privacy guarantees,
and the first such model trained with user-level privacy.
4.	In extensive experiments in §3, we offer guidelines for parameter tuning when training complex
models with differential privacy guarantees. We show that a small number of experiments can
narrow the parameter space into a regime where we pay for privacy not in terms of a loss in utility
but in terms of an increased computational cost.
We now introduce a few preliminaries. Differential privacy (DP) (Dwork et al., 2006; Dwork, 2011;
Dwork and Roth, 2014) provides a well-tested formalization for the release of information derived
from private data. Applied to machine learning, a differentially private training mechanism allows
the public release of model parameters with a strong guarantee: adversaries are severely limited in
what they can learn about the original training data based on analyzing the parameters, even when
they have access to arbitrary side information. Formally, it says:
Definition 1. Differential Privacy: A randomized mechanism M : D → R with a domain D (e.g.,
possible training datasets) and range R (e.g., all possible trained models) satisfies (, δ)-differential
privacy if for any two adjacent datasets d, d0 ∈ D and for any subset of outputs S ⊆ R it holds that
Pr[M(d) ∈ S] ≤ e Pr[M(d0) ∈ S] +δ.
The definition above leaves open the definition of adjacent datasets which will depend on the
application. Most prior work on differentially private machine learning (e.g. Chaudhuri et al. (2011);
Bassily et al. (2014); Abadi et al. (2016a); Wu et al. (2017); Papernot et al. (2017)) deals with
example-level privacy: two datasets d and d0 are defined to be adjacent if d0 can be formed by adding
or removing a single training example from d. We remark that while the recent PATE approach of
(Papernot et al., 2017) can be adapted to give user-level privacy, it is not suited for a language model
where the number of classes (possible output words) is large.
For problems like language modeling, protecting individual examples is insufficient—each typed
word makes its own contribution to the RNN’s training objective, so one user may contribute many
thousands of examples to the training data. A sensitive word or phrase may be typed several times
by an individual user, but it should still be protected.2 * * In this work, we therefore apply the definition
of differential privacy to protect whole user histories in the training set. This user-level privacy is
ensured by using an appropriate adjacency relation:
Definition 2. User-adjacent datasets: Let d and d0 be two datasets of training examples, where each
example is associated with a user. Then, d and d0 are adjacent if d0 can be formed by adding or
removing all of the examples associated with a single user from d.
1The additional computational cost could be mitigated by initializing by training on a public dataset, rather
than starting from random initialization as we do in our experiments.
2Differential privacy satisfies a property known as group privacy that can allow translation from example-
level privacy to user-level privacy at the cost of an increased . In our setting, such a blackbox approach would
incur a prohibitive privacy cost. This forces us to directly address user-level privacy.
2
Published as a conference paper at ICLR 2018
Model training that satisfies differential privacy with respect to datasets that are user-adjacent satis-
fies the intuitive notion of privacy we aim to protect for language modeling: the presence or absence
of any specific user’s data in the training set has an imperceptible impact on the (distribution over)
the parameters of the learned model. It follows that an adversary looking at the trained model can-
not infer whether any specific user’s data was used in the training, irrespective of what auxiliary
information they may have. In particular, differential privacy rules out memorization of sensitive
information in a strong information theoretic sense.
2	Algorithms for user-level differentially private training
Our private algorithm relies heavily on two prior works: the FederatedAveraging (or
FedAvg) algorithm of McMahan et al. (2016), which trains deep networks on user-partitioned data,
and the moments accountant of Abadi et al. (2016a), which provides tight composition guarantees
for the repeated application of the Gaussian mechanism combined with amplification-via-sampling.
While we have attempted to make the current work as self-contained as possible, the above refer-
ences provide useful background.
FedAvg was introduced by McMahan et al. (2016) for federated learning, where the goal is to train
a shared model while leaving the training data on each user’s mobile device. Instead, devices down-
load the current model and compute an update by performing local computation on their dataset. It
is worthwhile to perform extra computation on each user’s data to minimize the number of commu-
nication rounds required to train a model, due to the significantly limited bandwidth when training
data remains decentralized on mobile devices. We observe, however, that FedAvg is of interest
even in the datacenter when DP is applied: larger updates are more resistant to noise, and fewer
rounds of training can imply less privacy cost. Most importantly, the algorithm naturally forms per-
user updates based on a single user’s data, and these updates are then averaged to compute the final
update applied to the shared model on each round. As we will see, this structure makes it possible
to extend the algorithm to provide a user-level differential privacy guarantee.
We also evaluate the FederatedSGD algorithm, essentially large-batch SGD where each mini-
batch is composed of “microbatches” that include data from a single distinct user. In some datacen-
ter applications FedSGD might be preferable to FedAvg, since fast networks make it more practical
to run more iterations. However, those additional iterations come at a privacy cost. Further, the pri-
vacy benefits of federated learning are nicely complementary to those of differential privacy, and
FedAvg can be applied in the datacenter as well, so we focus on this algorithm while showing that
our results also extend to FedSGD.
Both FedAvg and FedSGD are iterative procedures, and in both cases we make the following
modifications to the non-private versions in order to achieve differential privacy:
A)	We use random-sized batches where we select users independently with probability q,
rather than always selecting a fixed number of users.
B)	We enforce clipping of per-user updates so the total update has bounded L2 norm.
C)	We use different estimators for the average update (introduced next).
D)	We add Gaussian noise to the final average update.
The pseudocode for DP-FedAvg and DP-FedSGD is given as Algorithm 1. In the remainder of
this section, we introduce estimators for C) and then different clipping strategies for B). Adding the
sampling procedure from A) and noise added in D) allows us to apply the moments accountant to
bound the total privacy loss of the algorithm, given in Theorem 1. Finally, we consider the properties
of the moments accountant that make training on large datasets particular attractive.
Bounded-sensitivity estimators for weighted average queries Randomly sampling users (or
training examples) by selecting each independently with probability q is crucial for proving low
privacy loss through the use of the moments accountant (Abadi et al., 2016a). However, this proce-
dure produces variable-sized samples C, and when the quantity to be estimated f(C) is an average
rather than a sum (as in computing the weighted average update in FedAvg or the average loss on a
minibatch in SGD with example-level DP), this has ramifications for the sensitivity of the query f .
Specifically, we consider weighted databases d where each row k ∈ d is associated with a particular
user, and has an associated weight wk ∈ [0, 1]. This weight captures the desired influence of the
3
Published as a conference paper at ICLR 2018
Main training loop:
parameters
user selection probability q ∈ (0, 1]
Per-User example CaP W ∈ R+
noise scale z ∈ R+
estimator ff , or fc with Param Wmin
UserUPdate (for FedAvg or FedSGD)
CliPFn (FlatCliP or PerLayerCliP)
Initialize model θ0, moments aCCountant M
Wk = min( nWk, 1)for all users k
W = Pk∈d wk
for eaCh round t = 0, 1, 2, . . . do
Ct — (sample users With probability q)
for eaCh user k ∈ C t in parallel do
△k+1 - UserUPdate(k,θt, CliPFn)
Pk∈Ct Wk δ
qW
Pk∈Ct Wkdk
maχ(qWmin,Pfc∈Ct wk )
S — (bound on ∣∣∆k ∣∣ for ClipFn)
σ 一 n 箫 for ff or qWSnfor『J
θt+1 ― θt +∆t+1 + N (0,Iσ2)
M.accum_priv_spending(z)
print M. get _Privacy _Spento
∆t+1
~
for ff
〜
for fc
FlatClip(∆):
parameter S
return π(∆, S) // See Eq. (1).
PerLayerClip(∆):
parameters S1 , . . . Sm
S=qρjS2
for eaCh layer j ∈ {1, . . . , m} do
∆0(j) =π(∆(j),Sj)
return ∆0
UserUpdateFedAvg(k, θ0 , ClipFn):
parameters B, E, η
θ 一 θ0
for eaCh loCal epoCh i from 1 to E do
B 一 (k's data split into size B batches)
for batCh b ∈ B do
θ 一 θ - ηθ'(θ; b)
θ 一 θ0 + ClipFn(θ — θ0)
return update ∆k = θ - θ0 // Already clipped.
UserUpdateFedSGD(k, θ0 , ClipFn):
parameters B, η
select a batch b of size B from k’s examples
return update ∆k = CliPFn(-ηθ'(θ; b))
Algorithm 1: The main loop for DP-FedAvg and DP-FedSGD, the only difference being in the
user update function (UserUpdateFedAvg or UserUpdateFedSGD). The calls on the moments ac-
countant M refer to the API of Abadi et al. (2016b).
roW on the final outcome. For example, We might think of roW k containing nk different training
examples all generated by user k, With Weight wk proportional to nk. We are then interested in a
bounded-sensitivity estimate of f (C) = ^P∈c Wkδ for per-user vectors ∆k, for example to estimate
k∈C wk
the Weighted-average user update in FedAvg. Let W = k wk. We consider tWo such estimators:
ff (C )= Ek∈C^k X,	and	fc(C) = —⅞∈c WPZ	`.
qW	max(qWmin,	k∈Cwk)
Note ff is an unbiased estimator, since E[ k∈Cwk] = qW. On the other hand, fc matches f exactly
as long as We have sufficient Weight in the sample. For privacy protection, We need to control the
sensitivity of our query function f, defined as S(f) = maxC,k kf(C ∪ {k}) - f(C)k2, Where the
added user k can have arbitrary data. The loWer-bound qWmin on the denominator of fc is necessary
to control sensitivity. Assuming eachwk∆k has bounded norm, We have:
Lemma 1. If for all users k we have kwk ∆k k2 ≤ S, then the sensitivity of the two estimators is
bounded as S(ff) ≤ -S and S(fc) ≤ 2S .
f qW	c	qWmin
A proof is given in Appendix §A.
Clipping strategies for multi-layer models Unfortunately, When the user vectors ∆k are gradi-
ents (or sums of gradients) from a neural netWork, We Will generally have no a priori bound3 S such
that k∆k k ≤ S. Thus, We Will need to “clip” our updates to enforce such a bound before applying
ff or fc. For a single vector ∆, we can apply a simple L projection when necessary:
π(∆, S)
(1)
3To control sensitivity, Lemma 1 only requires that ∣Wk∆k ∣ is bounded. For simplicity, we only apply
clipping to the updates ∆k , using the fact Wk ≤ 1, leaving as future work the investigation of weight-aware
clipping schemes.
4
Published as a conference paper at ICLR 2018
Table 1: Privacy for different total numbers of users K (all with equal weight), expected number of
users sampled per round C, and the number of rounds of training. For each row, We set δ = κ1rτ
and report the value of for which (, δ)-differential privacy holds after 1 to 106 rounds. For large
datasets, additional rounds of training incur only a minimal additional privacy loss.
users K	sample 〜 C	noise z	Upper bound on privacy e after 1,10,					. . . 106 rounds	
			100	101	102	103	104	105	106
105	102	1.0	0.97	0.98	1.00	1.07	1.18	2.21	7.50
106	101	1.0	0.68	0.69	0.69	0.69	0.69	0.72	0.73
106	103	1.0	1.17	1.17	1.20	1.28	1.39	2.44	8.13
106	104	1.0	1.73	1.92	2.08	3.06	8.49	32.38	187.01
106	103	3.0	0.47	0.47	0.48	0.48	0.49	0.67	1.95
109	103	1.0	0.84	0.84	0.84	0.85	0.88	0.88	0.88
However, for deep networks it is more natural to treat the parameters of each layer as a separate
vector. The updates to each of these layers could have vastly different L2 norms, and so it can be
preferable to clip each layer separately.
Formally, suppose each update ∆k contains m vectors ∆k = (∆k(1), . . . , ∆k(m)). We consider
the following clipping strategies, both of which ensure the total update has norm at most S:
1.	Flat clipping Given an overall clipping parameter S, we clip the concatenation of all the
layers as ∆0k = π(∆k, S).
2.	Per-layer clipping Given a per-layer clipping parameter Sj for each layer, we set ∆0k (j) =
π(∆k(j), Sj). Let S =	Pjm=1 Sj2. The simplest model-independent choice is to take
Sj = √m for all j, which we use in experiments.
We remark here that clipping itself leads to additional bias, and ideally, we would choose the clipping
parameter to be large enough that nearly all updates are smaller than the clip value. On the other
hand, a larger S will require more noise in order to achieve privacy, potentially slowing training. We
treat S as a hyper-parameter and tune it.
A privacy guarantee Once the sensitivity of the chosen estimator is bounded, we may add Gaus-
sian noise scaled to this sensitivity to obtain a privacy guarantee. A simple approach is to use an
(, δ)-DP bound for this Gaussian mechanism, and apply the privacy amplification lemma and the
advanced composition theorem to get a bound on the total privacy cost. We instead use the Mo-
ments Accountant of Abadi et al. (2016a) to achieve much tighter privacy bounds. The moments
accountant for the sampled Gaussian mechanism upper bounds the total privacy cost of T steps of
the Gaussian mechanism with noise N(0, σ2) for σ = Z ∙ S, where Z is a parameter, S is the sensi-
tivity of the query, and each row is selected with probability q. Given a δ > 0, the accountant gives
an for which this mechanism satisfies (, δ)-DP. The following theorem is a slight generalization
of the results in Abadi et al. (2016a); see §A for a proof sketch.
Theorem 1. For the estimator (ff,fc), the moments accountant of the sampled Gaussian mechanism
correctly computes the privacy loss with the noise scale of Z = σ/S and steps T, where S = S/qW
~ . _.,____________ ~
for(ff) and 2S∕qWminfor(fc).
Differential privacy for large datasets We use the implementation of the moments accountant
from Abadi et al. (2016b). The moments accountant makes strong use of amplification via sampling,
which means increasing dataset size makes achieving high levels of privacy significantly easier. Ta-
ble 1 summarizes the privacy guarantees offered as we vary some of the key parameters. The take-
away from this table is that as long as we can afford the cost in utility of adding noise proportional
to Z times the sensitivity of the updates, we can get reasonable privacy guarantees over a large range
of parameters. The size of the dataset has a modest impact on the privacy cost of a single query
(1 round column), but a large effect on the number of queries that can be run without significantly
increasing the privacy cost (compare the 106 round column). For example, on a dataset with 109
5
Published as a conference paper at ICLR 2018
communication rounds
Figure 1: Noised training versus the non-private
baseline. The model with σ = 0.003 nearly
matches the baseline.
Table 2: Privacy (e at δ = 10-9) and accuracy af-
ter 5000 rounds of training for models with dif-
ferent σ and S from Figure 1. The e's are strict
upper bounds on the true privacy loss given the
dataset size K and C; ACCUraCyTop1 (ACCTI) is
estimated from a model trained with the same σ
as discussed in the text.
model		data		e	AccT1
σ	S	users K	C		
0.000^^	∞	763430	100	∞	17.62%
0.003	15	763430	5000	4.634	17.49%
0.006	10	763430	1667	2.314	17.04%
0.012	15	763430	1250	2.038	16.33%
0.003	15	108	5000	1.152	17.49%
0.006	10	108	1667	0.991	17.04%
0.012	15	108	1250	0.987	16.33%
users, the privacy upper bound is nearly constant between 1 and 106 calls to the mechanism (that is,
rounds of the optimization algorithm).
There is only a small cost in privacy for increasing the expected number of (equally weighted) users
C = qW = qK selected on each round as long as C remains a small fraction of the size of the
total dataset. Since the sensitivity of an average query decreases like 1/C (and hence the amount of
noise We need to add decreases proportionally), We can increase C until We arrive at a noise level
that does not adversely effect the optimization process. We show empirically that such a level exists
in the experiments.
3	Experimental Results
In this section, We evaluate DP-FedAvg While training an LSTM RNN tuned for language modeling
in a mobile keyboard. We vary noise, clipping, and the number of users per round to develop an
intuition of hoW privacy affects model quality in practice.
We defer our experimental results on FedSGD as Well as on models With larger dictionaries to
Appendix §D. To summarize, they shoW that FedAvg gives better privacy-utility trade-offs than
FedSGD, and that our empirical conclusions extend to larger dictionaries With relatively little need
for additional parameter tuning despite the significantly larger models. Some less important plots
are deferred to §C.
Model structure The goal of a language model is to predict the next Word in a sequence st from
the preceding Words s0...st-1. The neural language model architecture used here is a variant of
the LSTM recurrent neural netWork (Hochreiter and Schmidhuber, 1997) trained to predict the next
Word (from a fixed dictionary) given the current Word and a state vector passed from the previous
time step. LSTM language models are competitive With traditional n-gram models (Sundermeyer
et al., 2012) and are a standard baseline for a variety of ever more advanced neural language model
architectures (Grave et al., 2016; Merity et al., 2016; Gal and Ghahramani, 2016). Our model uses
a feW tricks to decrease the size for deployment on mobile devices (total size is 1.35M parameters),
but is otherWise standard. We evaluate using AccuracyTop1, the probability that the Word to
Which the model assigns highest probability is correct . Details on the model and evaluation metrics
are given in §B. All training began from a common random initialization, though for real-World
applications pre-training on public data is likely preferable (see §B for additional discussion).
Dataset We use a large public dataset of Reddit posts, as described by Al-Rfou et al. (2016).
Critically for our purposes, each post in the database is keyed by an author, so We can group the data
by these keys in order to provide user-level privacy. We preprocessed the dataset to K = 763, 430
users each with 1600 tokens. Thus, we take Wk = 1 for all users, so W = K. We write C =
6
Published as a conference paper at ICLR 2018
Figure 2: The effect of update clipping on the
convergence of FedAvg, after 100, 500, and
3000 rounds of training.
Figure 3: The effect of different levels of noise
σ for flat and per-layer clipping at S = 20. The
vertical dashed red line is σ = 0.2.
qK = qW for the expected number of users sampled per round. See §B for details on the dataset
and preprocessing. To allow for frequent evaluation, we use a relatively small test set of 75122
tokens formed from random held-out posts. We evaluate accuracy every 20 rounds and plot metrics
smoothed over 5 evaluations (100 rounds).
Building towards DP: sampling, estimators, clipping, and noise Recall achieving differential
privacy for FedAvg required a number of changes (§2, items A-D). In this section, We examine the
impact of each of these changes, both to understand the immediate effects and to enable the selection
of reasonable parameters for our final DP experiments. This sequence of experiments also provides
a general road-map for applying differentially private training to neW models and datasets. For these
experiments, We use the FedAvg algorithm With a fixed learning rate of 6.0, Which We verified Was
a reasonable choice in preliminary experiments.4 In all FedAvg experiments, We used a local batch
size of B = 8, an unroll size of 10 tokens, and made E = 1 passes over the local dataset; thus
FedAvg processes 80 tokens per batch, processing a user’s 1600 tokens in 20 batches per round.
First, We investigate the impact of changing the estimator used for the average per-round update,
as Well as replacing a fixed sample of C = 100 users per round to a variable-sized sample formed
by selecting each user with probability q = 100/763430 for an expectation of G= 100 users.
None of these changes significantly impacted the convergence rate of the algorithm (see Figure 5
in §C). In particular, the fixed denominator estimator ff works just as well as the higher-sensitivity
clipped-denominator estimator f Thus, in the remaining experiments we focus on estimator ff.
Next, we investigate the impact of flat and per-layer clipping on the convergence rate of FedAvg.
The model has 11 parameter vectors, and for per-layer clipping we simply chose to distribute the
clipping budget equally across layers with Sj = S/√Γ1. Figure 2 shows that choosing S ∈ [10,20]
has at most a small effect on convergence rate.
Finally, Figure 3 shows the impact of various levels of per-coordinate Gaussian noise N (0, σ2)
added to the average update. Early in training, we see almost no loss in convergence for a noise of
σ = 0.024; later in training noise has a larger effect, and we see a small decrease in convergence
past σ = 0.012. These experiments, where we sample only an expected 100 users per round, are not
sufficient to provide a meaningful privacy guarantee. We have S = 20.0 and G = qW = 100, so
the sensitivity of estimator ff is 20/100.0 = 0.2. Thus, to use the moments accountant with Z = 1,
we would need to add noise σ = 0.2 (dashed red vertical line), which destroys accuracy.
Estimating the accuracy of private models for large datasets Continuing the above example,
if instead we choose q so G = 1250, set the L2 norm bound S = 15.0, then we have sensitivity
4The proper choice of for the clipping parameters may depend on the learning rate, so if the learning rate is
changed, clipping parameter choices will also need to be re-evaluated.
7
Published as a conference paper at ICLR 2018
Figure 4: Different noise/clipping tradeoffs (all
of equal privacy cost), for initial training (red)
and adjusted after 4885 rounds (green and blue).
Solid lines use flat clipping, dashed are per-layer.
Table 3: Count histograms recording how many
of a model’s (row’s) top 10 predictions are found
in the n = 10, 50, or 100 most frequent words
in the corpus. Models that predict corpus top-n
more frequently have more mass to the right.
15/1250 = 0.012, and so we add noise σ = 0.012 and can apply the moments account with noise
scale z = 1. The computation is now significantly more computationally expensive, but will give a
guarantee of (1.97, 10-9)-differential privacy after 3000 rounds of training. Because running such
experiments is so computationally expensive, for experimental purposes it is useful to ask: does
using an expected 1250 users per round produce a model with different accuracy than a model
trained with only 100 expected users per round? If the answer is no, we can train a model with
C = 100 and a particular noise level σ, and use that model to estimate the utility of a model trained
with a much larger q (and hence a much better privacy guarantee). We can then run the moments
accountant (without actually training) to numerically upper bound the privacy loss. To test this, we
trained two models, both with S = 15 and σ = 0.012, one with C = 100 and one with C = 1250;
recall the first model achieves a vacuous privacy guarantee, while the second achieves (1.97, 10-9)-
differential privacy after 3000 rounds. Figure 7 in §C shows the two models produce almost identical
accuracy curves during training. Using this observation, we can use the accuracy of models trained
with C = 100 to estimate the utility of private models trained with much larger C . See also Figure 6
in §C, which also shows diminishing returns for larger C for the standard FedAvg algorithm.
Figure 1 compares the true-average fixed-sample baseline model (see Figure 5 in §C) with models
that use varying levels of clipping S and noise σ at G= 100. Using the above approach, We can
use these experiments to estimate the utility of LSTMs trained with differential privacy for different
sized datasets and different values of C . Table 2 shows representative values setting C so that
z = 1. For example, the model with σ = 0.003 and S = 15 is only worse than the baseline by an
additive -0.13% in AccuracyTop1 and achieves (4.6, 10-9)-differential privacy when trained with
C = 5000 expected users per round. As a point of comparison, we have observed that training on a
different corpus can cost an additive -2.50% in AccuracyTop1.5
Adjusting noise and clipping as training progresses Figure 1 shows that as training progresses,
each level of noise eventually becomes detrimental (the line drops somewhat below the baseline).
This suggests using a smaller σ and correspondingly smaller S (thus fixing z so the privacy cost
of each round is unchanged) as training progresses. Figure 4 (and Figure 8 in §C) shows this can
be effective. We indeed observe that early in training (red), S in the 10 - 12.6 range works well
(σ = 0.006 - 0.0076). However, if we adjust the clipping/noise tradeoff after 4885 rounds of
training and continue for another 6000, switching to S = 7.9 and σ = 0.0048 performs better.
Comparing DP and non-DP models While noised training with DP-FedAvg has only a small
effect on predictive accuracy, it could still have a large qualitative effect on predictions. We hy-
5This experiment was performed on different datasets, comparing training on a dataset of public social
media posts to training on a proprietary dataset which is more representative of mobile keyboard usage, and
evaluating on a held-out sample of the same representative dataset. Absolute AccuracyTop1 was similar to the
values we report here for the Reddit dataset.
8
Published as a conference paper at ICLR 2018
pothesized that noising updates might bias the model away from rarer words (whose embeddings
get less frequent actual updates and hence are potentially more influenced by noise) and toward the
common “head” words. To evaluate this hypothesis, we computed predictions on a sample of the
test set using a variety of models. At each st we intersect the top 10 predictions with the most
frequent 10, 50, 100 words in the dictionary. So for example, an intersection of size two in the top
50 means two of the model’s top 10 predictions are in the 50 most common words in the dictionary.
Table 3 gives histograms of these counts. We find that better models (higher AccuracyTop1) tend to
use fewer head words, but see little difference from changing C or the noise σ (until, that is, enough
noise has been added to compromise model quality, at which point the degraded model’s bias toward
the head matches models of similar quality with less noise).
4	Conclusions
In this work, we introduced an algorithm for user-level differentially private training of large neural
networks, in particular a complex sequence model for next-word prediction. We empirically evalu-
ated the algorithm on a realistic dataset and demonstrated that such training is possible at a negligible
loss in utility, instead paying a cost in additional computation. Such private training, combined with
federated learning (which leaves the sensitive training data on device rather than centralizing it),
shows the possibility of training models with significant privacy guarantees for important real world
applications. Much future work remains, for example designing private algorithms that automate
and make adaptive the tuning of the clipping/noise tradeoff, and the application to a wider range
of model families and architectures, for example GRUs and character-level models. Our work also
highlights the open direction of reducing the computational overhead of differentially private train-
ing of non-convex models.
References
Martin Abadi, Andy Chu, Ian Goodfellow, Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang.
Deep learning with differential privacy. In 23rd ACM Conference on Computer and Communications Secu-
rity (ACM CCS), 2016a.
Martin Abadi, Andy Chu, Ian Goodfellow, Brendan McMahan, Ilya Mironov, Kunal Talwar, Li Zhang, and Xin
Pan. Source code for “deep learning with differential privacy”. github, October 2016b. http://github.
com/tensorflow/models/tree/master/differential_privacy.
Rami Al-Rfou, Marc Pickett, Javier Snaider, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil. Conversational
contextual cues: The case of personalization and history for response ranking. CoRR, abs/1606.00372, 2016.
URL http://arxiv.org/abs/1606.00372.
R. Bassily, K. Nissim, U. Stemmer, and A. Thakurta. Practical locally private heavy hitters. ArXiv e-prints,
July 2017. URL https://arxiv.org/abs/1707.04982.
Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient algorithms
and tight error bounds. In Proceedings of the 2014 IEEE 55th Annual Symposium on Foundations of Com-
Puter Science, FOCS ,14, pages 464-473, Washington, DC, USA, 2014.IEEE Computer Society. ISBN 978-
1-4799-6517-5. doi: 10.1109/FOCS.2014.56. URL http://dx.doi.org/10.1109/FOCS.2014.
56.
N. Carlini, C. Liu, J. Kos, U. Erlingsson, and D. Song. The Secret Sharer: Measuring Unintended Neural
Network Memorization & Extracting Secrets. ArXiv e-prints, February 2018. URL https://arxiv.
org/abs/1802.08232.
T-H Hubert Chan, Mingfei Li, Elaine Shi, and Wenchang Xu. Differentially private continual monitoring of
heavy hitters from distributed streams. In International Symposium on Privacy Enhancing Technologies
Symposium, pages 140-159. Springer, 2012.
Kamalika Chaudhuri, Claire Monteleoni, and Anand D. Sarwate. Differentially private empirical risk mini-
mization. J. Mach. Learn. Res., 12, July 2011.
Cynthia Dwork. A firm foundation for private data analysis. Commun. ACM, 54(1):86-95, January 2011. ISSN
0001-0782. doi: 10.1145/1866739.1866758. URL http://doi.acm.org/10.1145/1866739.
1866758.
9
Published as a conference paper at ICLR 2018
Cynthia Dwork and Aaron Roth. The Algorithmic Foundations of Differential Privacy. Foundations and Trends
in Theoretical Computer Science. Now Publishers, 2014.
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private
data analysis. In TCC, pages 265-284. Springer, 2006.
Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent neural net-
works. In Advances in Neural Information Processing Systems, pages 1019-1027, 2016.
Edouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language models with a continuous
cache. arXiv preprint arXiv:1612.04426, 2016.
SePP HoChreiter and Jurgen SChmidhuber. Long short-term memory. Neural computation, 9(8):1735-1780,
1997.
H. Brendan MCMahan, Eider Moore, Daniel Ramage, and Blaise Aguera y ArCas. Federated learning of deep
networks using model averaging, 2016.
Stephen Merity, Caiming Xiong, James Bradbury, and RiChard SoCher. Pointer sentinel mixture models. arXiv
preprint arXiv:1609.07843, 2016.
Nicolas Papernot, Martin Abadi, UJlfar Erlingsson, Ian Goodfellow, and Kunal Talwar. Semi-supervised knowl-
edge transfer for deep learning from private training data. In Proceedings of the International Conference
on Learning Representations, 2017. JRL https://arxiv.org/abs/1610.05755.
Ofir Press and Lior Wolf. Jsing the output embedding to improve language models. In Proceedings of the 15th
Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short
Papers, pages 157-163. AssoCiation for Computational LinguistiCs, 2017.
Reddit Comments Dataset. Reddit Comments dataset. BigQuery, 2016. https://bigquery.cloud.
google.com/dataset/fh-bigquery.
Reza Shokri, MarCo Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inferenCe attaCks against
maChine learning models. In IEEE Symposium on Security and Privacy, Oakland, 2017. To Appear.
Martin Sundermeyer, Ralf Schluter, and Hermann Ney. LSTM neural networks for language modeling. In
Interspeech, pages 194-197, 2012.
Xi Wu, Fengan Li, Arun Kumar, Kamalika Chaudhuri, Somesh Jha, and Jeffrey F. Naughton. Bolt-on differen-
tial privacy for scalable stochastic gradient descent-based analytics. In Proceedings of SIGMOD, 2017.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Jnderstanding deep learning
requires rethinking generalization. 2017. JRL https://arxiv.org/abs/1611.03530.
10
Published as a conference paper at ICLR 2018
A Additional proofs
ProofofLemma 1. For the first bound, observe the numerator in the estimator f can change by
at most S between neighboring databases, by assumption. The denominator is a constant. For
the second bound, the estimator f can be thought of as the sum of the vectors wk∆k divided by
max(qWmin, Pk∈C ∆k). Writing Num(C) for the numerator Pk∈C wk∆k, and Den(C) for the
denominator max(qWmin, Pk∈C wk), the following are immediate for any C and C0 d=ef C ∪ {k}:
k Num(C0) - Num(C)k = kwk∆kk ≤ S.
k Den(C0) - Den(C)k ≤ 1.
k Den(C0)k ≥ qWmin.
It follows that
~ . ~
IIfc(C0)- fc(C)k
≤
Num(C0) Num(C)
----,_ . —―----, 一
Den(C0)	Den(C)
Num(C0) - Num(C)	1	1
Den(C0)	+ um( ) (Den(C0) - Den(C) )∖∖
Wk ∆k
Den(C0)
Num(C) (Den(C) — Den(C0)
Den(C) V	Den(C0)
≤
S1
qwmn+ kfc(C)k(行
2S
≤
qWmin
Here in the last step, we used the fact that Ifc(C)I ≤ S. The claim follows.
□
Proof of Theorem 1. It suffices to verify that 1. the moments (of the privacy loss) at each step are
correctly bounded; and, 2. the composability holds when accumulating the moments of multiple
steps.
At each step, users are selected randomly with probability q. If in addition the L2-norm of each
user’s update is upper-bounded by S, then the moments can be upper-bounded by that of the sampled
Gaussian mechanism with sensitivity 1, noise scale σ∕S, and sampling probability q.
Our algorithm, as described in Figure 1, uses a fixed noise variance and generates the i.i.d. noise
independent of the private data. Hence we can apply the composability as in Theorem 2.1 in Abadi
et al. (2016a).
We obtain the theorem by combining the above and the sensitivity bounds ff and fc.	□
B	Experiment details
Model The first step in training a word-level recurrent language model is selecting the vocabulary
of words to model, with remaining words mapped to a special “UNK” (unknown) token. Training
a fully differentially private language model from scratch requires a private mechanism to discover
which words are frequent across the corpus, for example using techniques like distributed heavy-
hitter estimation (Chan et al., 2012; Bassily et al., 2017). For this work, we simplified the problem
by pre-selecting a dictionary of the most frequent 10,000 words (after normalization) in a large
corpus of mixed material from the web and message boards (but not our training or test dataset).
Our recurrent language model works as follows: word st is mapped to an embedding vector et ∈ R96
by looking up the word in the model’s vocabulary. The et is composed with the state emitted by
the model in the previous time step st-1 ∈ R256 to emit a new state vector st and an “output
embedding” ot ∈ R96 . The details of how the LSTM composes et and st-1 can be found in
Hochreiter and Schmidhuber (1997). The output embedding is scored against the embedding of
each item in the vocabulary via inner product, before being normalized via softmax to compute a
probability distribution over the vocabulary. Like other standard language modeling applications,
11
Published as a conference paper at ICLR 2018
we treat every input sequence as beginning with an implicit “BOS” (beginning of sequence) token
and ending with an implicit “EOS” (end of sequence) token.
Unlike standard LSTM language models, our model uses the same learned embedding for the input
tokens and for determining the predicted distribution on output tokens from the softmax.6 This
reduces the size of the model by about 40% for a small decrease in model quality, an advantageous
tradeoff for mobile applications. Another change from many standard LSTM RNN approaches
is that we train these models to restrict the word embeddings to have a fixed L2 norm of 1.0, a
modification found in earlier experiments to improve convergence time. In total the model has
1.35M trainable parameters.
Initialization and personalization For many applications public proxy data is available, e.g., for
next-word prediction one could use public domain books, Wikipedia articles, or other web con-
tent. In this case, an initial model trained with standard (non-private) algorithms on the public data
(which is likely drawn from the wrong distribution) can then be further refined by continuing with
differentially-private training on the private data for the precise problem at hand. Such pre-training
is likely the best approach for practical applications. However, since training models purely on pri-
vate data (starting from random initialization) is a strictly harder problem, we focus on this scenario
for our experiments.
Our focus is also on training a single model which is shared by all users. However, we note that our
approach is fully compatible with further on-device personalization of these models to the particular
data of each user. It is also possible to give the central model some ability to personalize simply by
providing information about the user as a feature vector along with the raw text input. LSTMs are
well-suited to incorporating such additional context.
Accuracy metrics We evaluate using AccuracyTop1, the probability that the word to which the
model assigns highest probability is correct (after some minimal normalization). We always count it
as a mistake if the true next word is not in the dictionary, even if the model predicts UNK, in order
to allow fair comparisons of models using different dictionaries. In our experiments, we found that
our model architecture is competitive on AccuracyTop1 and related metrics (Top3, Top5, and
perplexity) across a variety of tasks and corpora.
Dataset The Reddit dataset can be accessed through Google BigQuery (Reddit Comments
Dataset). Since our goal is to limit the contribution of any one author to the final model, it is
not necessary to include all the data from users with a large number of posts. On the other hand,
processing users with too little data slows experiments (due to constant per-user overhead). Thus,
we use a training set where we have removed all users with fewer than 1600 tokens (words), and
truncated the remaining K = 763, 430 users to have exactly 1600 tokens.
We intentionally chose a public dataset for research purposes, but carefully chose one with a struc-
ture and contents similar to private datasets that arise in real-world language modeling task such as
predicting the next-word in a mobile keyboard. This allows for reproducibility, comparisons to non-
private models, and inspection of the data to understand the impact of differential privacy beyond
coarse aggregate statistics (as in Table 3).
6Press and Wolf (2017) independently introduced this technique and provide an empirical analysis compar-
ing models with and without weight tying.
12
Published as a conference paper at ICLR 2018
C S upplementary Plots
Figure 5: Comparison of sampling strategies and estimators. Fixed sample is exactly C = 100 users
per round, and variable sample selects uniformly with probability q for C = 100. The true average
corresponds to f, fixed denominator is ff, and clipped denominator is f
Figure 6: The effect of C for FedAvg using the
exact estimator and without noise or clipping.
Figure 8: The effect of different noise vs. clipping tradeoffs on convergence. Both plots use the same
legend, where we vary S and σ together to maintain the same z = 0.06 with 100 users (actually
used), or z = 1 with 1667 users. We take S = 20 and σ = 0.012 (black line) as a baseline; the left-
hand plot shows training from a randomly initialized model, and includes two different runs with
S = 20, showing only mild variability. For the right-hand plot, we took a snapshot of the S = 20
model after 4885 initial rounds of training, and resumed training with different tradeoffs.
Figure 7: Training with (expected) 100 vs 1250
users per round, both with flat-clipping at S =
15.0 and per-coordinate noise with σ = 0.012.
0.172
0.170
0.168
0.166
0.164
0.162
0.160
0.158
Continuing training with different tradeoffs
2000	4000	6000	8000	10000
communication rounds
13
Published as a conference paper at ICLR 2018
S, total bound on L? norm of updates
Figure 9: Effect of clipping on FedSGD with
C = 50 users per round and a learning rate of
η = 6. A much smaller clipping level S can be
used compared to FedAvg.
0.18
0.16
0.14
d
o
工 0.12
I
0 0.10
0.08
0.06
10-4	10-3	10-2	10-1
¾, noise Std dev
Figure 10: Effect of noised updates on FedSGD
with S = 20 (based on Figure 9, a smaller
value would actually be better when doing pri-
vate training). FedSGD is more sensitive to noise
than FedAvg, likely because the updates are
smaller in magnitude.
Effect of clipping updates (20k & 30k diet models)
Figure 11: Effect of clipping on models with
larger dictionaries (20000 and 30000 tokens).
Effect of noising updates (20k & 30k diet models)
Figure 12: Effect of noised updates on models
with larger dictionaries, when clipped at S = 20.
D Additional Experiments
Experiments with SGD We ran experiments using FedSGD taking B = 1600, that is, computing
the gradient on each user’s full local dataset. To allow more iterations, we used C = 50 rather than
100. Examining Figures 9 and 10, We see S = 2 and σ = 2 ∙ 10-3 are reasonable values, which
suggests for private training we would need in expectation qW = S/σ = 1500 users per round,
whereas for FedAvg we might choose S = 15 and σ = 10-2 for C = qW = 1000 users per round.
That is, the relative effect of the ratio of the clipping level to noise is similar between FedAvg and
FedSGD. However, FedSGD takes a significantly larger number of iterations to reach equivalent
accuracy. Fixing z = 1, C = 5000 (the value that produced the best accuracy for a private model in
Table 2) and total of 763,430 users gives (3.81, 10-9)-DP after 3000 rounds and (8.92, 10-9)-DP
after 20000 rounds, so there is indeed a significant cost in privacy to these additional iterations.
Models with larger dictionaries We repeated experiments on the impact of clipping and noise on
models with 20000 and 30000 token dictionaries, again using FedAvg training with η = 6, equally
weighted users with 1600 tokens, and G = 100 expected users per round. The larger dictionaries
give only a modest improvement in accuracy, and do not require changing the clipping and noise
parameters despite having significantly more parameters. Results are given in Figures 11 and 12.
Other experiments We experimented with adding an explicit L2 penalty on the model updates
(not the full model) on each user, hoping this would decrease the need for clipping by preferring
updates with a smaller L2 norm. However, we saw no positive effect from this.
14