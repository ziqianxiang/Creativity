Published as a conference paper at ICLR 2018
SGD Learns Over-parameterized Networks
that Provably Generalize on Linearly Separa-
ble Data
Alon Brutzkus & Amir Globerson
The Blavatnik School of Computer Science
Tel Aviv University, Israel
alonbrutzkus@mail.tau.ac.il,amir.globerson@gmail.com
Eran Malach & Shai Shalev-Shwartz
School of Computer Science
The Hebrew University, Israel
eran.malach@mail.huji.ac.il,shais@cs.huji.ac.il
Ab stract
Neural networks exhibit good generalization behavior in the over-parameterized
regime, where the number of network parameters exceeds the number of obser-
vations. Nonetheless, current generalization bounds for neural networks fail to
explain this phenomenon. In an attempt to bridge this gap, we study the problem
of learning a two-layer over-parameterized neural network, when the data is gen-
erated by a linearly separable function. In the case where the network has Leaky
ReLU activations and only the first layer is trained, we provide both optimization
and generalization guarantees for over-parameterized networks. Specifically, we
prove convergence rates of SGD to a global minimum, and provide generaliza-
tion guarantees for this global minimum that are independent of the network size.
Therefore, our result clearly shows that the use of SGD for optimization both finds
a global minimum, and avoids overfitting despite the high capacity of the model.
This is the first theoretical demonstration that SGD can avoid overfitting, when
learning over-specified neural network classifiers.
1	Introduction
Neural networks have achieved remarkable performance in many machine learning tasks. Al-
though recently there have been numerous theoretical contributions to understand their success,
it is still largely unexplained and remains a mystery. In particular, it is not known why in the over-
parameterized setting, in which there are far more parameters than training points, stochastic gradi-
ent descent (SGD) can learn networks that generalize well, as been observed in practice (Neyshabur
et al., 2014; Zhang et al., 2016).
In such over-parameterized settings, the loss function can contain multiple global minima that gen-
eralize poorly. Therefore, learning can in principle lead to models with low training error, but high
test error. However, as often observed in practice, SGD is in fact able to find models with low
training error and good generalization performance. This suggests that the optimization procedure,
which depends on the optimization method (SGD) and the training data, introduces some form of
inductive bias which directs it towards a low complexity solution. Thus, in order to explain the
success of neural networks, it is crucial to characterize this inductive bias and understand what are
the guarantees for generalization of over-parameterized neural networks.
In this work, we address these problems in a binary classification setting where SGD optimizes a
two-layer over-parameterized network with the goal of learning a linearly separable function. We
study a relatively simple case of SGD where the weights of the second layer are fixed throughout the
training process, and only the weights of the first layer are updated. Clearly, an over-parameterized
network is not necessary for classifying linearly separable data, since this is possible with linear
1
Published as a conference paper at ICLR 2018
classifiers (e.g., with the Perceptron algorithm) which also have good generalization guarantees
(Shalev-Shwartz & Ben-David, 2014). But, the key question which we address here is whether a
large network will overfit in such a case or not. As we shall see, it turns out that although the
networks we consider are rich enough to considerably overfit the data, this does not happen when
SGD is used for optimization. In other words, SGD introduces an inductive bias which allows it to
learn over-parameterized networks that can generalize well. Therefore, this setting serves as a good
test bed for studying the effect of over-paramaterization.
2	Problem Formulation
Define X = {x ∈ Rd : kxk ≤ 1}, Y = {±1}. We consider a distribution over linearly separable
points. Formally, let D be a distribution over XXY such that there exists w* ∈ Rd for which
P(χ,y)〜D(y hw*, Xi ≥ 1) = 1. 1 Let S = {(xι,yι),..., (xn,yn)} ⊆ X × Y be a training set
sampled i.i.d. from D. 2
Consider the following two-layer neural network, with 2k > 0 hidden units. 3 The network param-
eters are W ∈ R2k×d, v ∈ R2k, which we denote jointly by W = (W, v). The network output is
given by the function NW : Rd → R defined as: 4
NW (x) = v>σ(Wx)	(1)
where σ is a non-linear activation function applied element-wise.
We define the empirical loss over S to be the mean hinge-loss:
1n
LS (W)=力 £ max {1 — yiNw(Xi), 0}
n i=1
Note that for convenience of analysis, we will sometimes refer to LS as a function over a vector.
Namely, for a matrix W ∈ R2k×d, we will consider instead its vectorized version W~ ∈ R2kd (where
the rows of W are concatenated) and define, with abuse of notation, that LS (W~ ) = LS(W).
k	k
In our setting We fix the second layer tobe V = (V}|-7{, —v •…一 V) such that v > 0 and only learn
the weight matrix W . We will consider only positive homogeneous activations (Leaky ReLU and
ReLU) and thus the network we consider with 2k hidden neurons is as expressive as networks with
k hidden neurons and any vector v in the second layer. 5 Hence, we can fix the second layer without
limiting the expressive power of the two-layer network. Although it is relatively simpler than the
case where the second layer is not fixed, the effect of over-parameterization can be studied in this
setting as well.
Hence, the objective of the optimization problem is to find:
arg min LS (W)	(2)
W ∈R2k×d
where min LS (W) = 0 holds for the activations we will consider (Leaky ReLU and ReLU).
IThiSimPIieSthatkw* k ≥ L
2Without loss of generality, we will ignore the event that yi hw*, Xii < 1 for some i, since this is an event
of measure zero.
3We have an even number of hidden neurons for ease of exposition. See the definition of v below.
4Our results hold in the case where the first layer contains bias terms. This follows by the standard argument
of adding another dimension to the input and setting the value 1 in the extra dimension for each data point.
5For example, consider a network with k hidden neurons with positive homogeneous activations, where each
hidden neuron i has incoming weight vector wi and outgoing weight vi . Then, we can express this network
with the network defined in Eq. 1 as follows. For each i such that vi > 0, we define a neuron in the new
network with incoming weight vector viwi and outgoing weight 1. Similarly, if vi < 0, we define a neuron
in the new network with incoming weight vector -viwi and outgoing weight -1. For all other neurons in the
new network we define an incoming zero weight vector. Due to the positive homogeneity, it follows that this
network is equivalent to the network with k hidden neurons.
2
Published as a conference paper at ICLR 2018
We focus on the case where LS (W) is minimized using an SGD algorithm with batch of size 1,
and where only the weights of the first layer (namely W) are updated. At iteration t, SGD randomly
chooses a point (xt, yt) ∈ S and updates the weights with a constant learning rate η. Formally, let
Wt = (Wt, v) be the parameters at iteration t, then the update at iteration t is given by
∂
Wt = Wt-1 - η∂WL{(χt,yt)}(Wt-I)	⑶
We define a non-zero update at iteration t if it holds that ∂w L{(χt ,yt)}(Wt-ι) = 0. Finally, we will
need the following notation. For 1 ≤ i ≤ k, we denote by wt(i) ∈ Rd the incoming weight vector
of neuron i at iteration t. 6 Similarly, for 1 ≤ i ≤ k we define ut(i) ∈ Rd to be the incoming weight
vector of neuron k + i at iteration t.
3	Main Result
We now present our main results, for the case where σ is the Leaky ReLU function. Namely,
σ(z) = max{αz, z} where 0 < α < 1.
First, we show that SGD can find a global optimum of LS (W). Note that this is by no means
obvious, since LS (W) is a non-convex function (see Proposition 1). Specifically, we show that
SGD converges to such an optimum while making at most:
M =" + o m½
(4)
non-zero update steps (see Corollary 3). In particular, the bound is independent of the number of
neurons 2k. To the best of our knowledge, this is the first convergence guarantee of SGD for neural
networks with the hinge loss. Furthermore, we prove a lower bound of Ω(空？ + ∣∣w*k2) for the
number of non-zero updates (see Theorem 4).
Next, we address the question of generalization. As noted earlier, since the network is large, it can in
principle overfit. Indeed, there are parameter settings for which the network will have arbitrarily bad
test error (see Section 6.2). However, as we show here, this will not happen in our setting where SGD
is used for optimization. In Theorem 6 we use a compression bound to show that the model learned
by SGD will have a generalization error of O (Mngn) .7 ThiS implies that for any network size,
given a sufficiently large number of training samples that is independent of the network size, SGD
converges to a global minimum with good generalization behaviour. This is despite the fact that for
sufficiently large k there are multiple global minima which overfit the training set (see Section 6.2).
This implies that SGD is biased towards solutions that can be expressed by a small set of training
points and thus generalizes well.
To summarize, when the activation is the Leaky ReLU and the data is linearly separable, we provide
provable guarantees of optimization, generalization and expressive power for over-parameterized
networks. This allows us to provide a rigorous explanation of the performance of over-parameterized
networks in this setting. This is a first step in unraveling the mystery of the success of over-
parameterized networks in practice.
We further study the same over-parameterized setting where the non-linear activation is the ReLU
function (i.e., σ(z) = max{0, z}). Surprisingly, this case has different properties. Indeed, we
show that the loss contains spurious local minima and thus the previous convergence result of SGD
to a global minimum does not hold in this case. Furthermore, we show an example where over-
parameterization is favorable from an optimization point of view. Namely, for a sufficiently small
number of hidden neurons, SGD will converge to a local minimum with high probability, whereas
for a sufficiently large number of hidden neurons, SGD will converge to a global minimum with
high probability.
6These are the neurons with positive outgoing weight v > 0.
7See discussion in Remark 1 on the dependence of the generalizaion bound on η.
3
Published as a conference paper at ICLR 2018
The paper is organized as follows. We discuss related work in Section 4 . In Section 5 we prove the
convergence bounds, in Section 6 we give the generalization guarantees and in Section 7 the results
for the ReLU activation. We conclude our work in Section 8.
4	Related Work
The generalization performance of neural networks has been studied extensively. Earlier results
(Anthony & Bartlett, 2009) provided bounds that depend on the VC dimension of the network, and
the VC dimension was shown to scale linearly with the number of parameters. More recent works,
study alternative notions of complexity, such as Rademacher compexity (Bartlett & Mendelson,
2002; Neyshabur et al., 2015; Bartlett et al., 2017; Kawaguchi et al., 2017), Robustness (Xu & Man-
nor, 2012) and PAC-Bayes (Neyshabur et al., 2017b). However, all of these notions do not provide
provable guarantees for the generalization performance of over-parameterized networks trained with
gradient based methods (Neyshabur et al., 2017a). The main disadvantage of these approaches, is
that they do not depend on the optimization method (e.g., SGD), and thus do not capture its role in
the generalization performance. In a recent paper, Dziugaite & Roy (2017) numerically optimize
a PAC-Bayes bound of a stochastic over-parameterized network in a binary classification task and
obtain a nonvacuous generalization bound. However, their bound is effective only when optimiza-
tion succeeds, which their results do not guarantee. In our work, we give generalization guarantees
based on a compression bound that follows from convergence rate guarantees of SGD, and thus
take into account the effect of the optimization method on the generalization performance. This
analysis results in generalization bounds that are independent of the network size and thus hold for
over-parameterized networks.
Stability bounds for SGD in non-convex settings were given in Hardt et al. (2016); Kuzborskij &
Lampert (2017). However, their results hold for smooth loss functions, whereas the loss function we
consider is not smooth due to the non-smooth activation functions (Leaky ReLU, ReLU).
Other works have studied generalization of neural networks in a model recovery setting, where
assumptions are made on the underlying model and the input distribution (Brutzkus & Globerson,
2017; Zhong et al., 2017; Li & Yuan, 2017; Du et al., 2017; Tian, 2017). However, in their works
the neural networks are not over-parameterized as in our setting.
Soltanolkotabi et al. (2017) analyze the optimization landscape of over-parameterized networks and
give convergence guarantees for gradient descent to a global minimum when the data follows a
Gaussian distribution and the activation functions are differentiable. The main difference from our
work is that they do not provide generalization guarantees for the resulting model. Furthermore, we
do not make any assumptions on the distribution of the feature vectors.
In a recent work, Nguyen & Hein (2017) show that if training points are linearly separable then
under assumptions on the rank of the weight matrices of a fully-connected neural network, every
critical point of the loss function is a global minimum. Their work extends previous results in
Gori & Tesi (1992); Frasconi et al. (1997); Yu & Chen (1995). Our work differs from these in
several respects. First, we show global convergence guarantees of SGD, whereas they only analyze
the optimization landscape, without direct implications on performance of optimization methods.
Second, we provide generalization bounds and their focus is solely on optimization. Third, we
consider non-differentiable activation functions (Leaky ReLU, ReLU) while their results hold only
for continuously differentiable activation functions.
5	Convergence Analysis
In this section we consider the setting of Section 2 with a leaky ReLU activation function. In Section
5.1 we show SGD will converge to a globally optimal solution, and analyze the rate of convergence.
In Section 5.1 we also provide lower bounds on the rate of convergence. The results in this section
are interesting for two reasons. First, they show convergence of SGD for a non-convex objective.
Second, the rate of convergence results will be used to derive generalization bounds in Section 6.
4
Published as a conference paper at ICLR 2018
5.1	Upper B ound
Before proving convergence of SGD to a global minimum, we show that every critical point is a
global minimum and the loss function is non-convex. The proof is deferred to the appendix.
Proposition 1. LS (W) satisfies the following properties: 1) Every critical point is a global mini-
mum. 2) It is non-convex.
Let W~ t = (w(t1) . . . wt(k)ut(1) . . . ut(k)) ∈ R2kd be the vectorized version of Wt and Nt := NWt
where Wt = (Wt, v) (see Eq. 1). Since we will show an upper bound on the number of non-zero
updates, we will assume for simplicity that for all t we have a non-zero update at iteration t.
We assume that SGD is initialized such that the norms of all rows of W0 are upper bounded by some
constant R > 0. Namely for all 1 ≤ i ≤ k it holds that:
kw(0i)k, ku(0i)k ≤ R	(5)
TAq kw	l∣w*k2 I	l∣w*k2	I	√ R(8k2η2v2+8ηk)∣∣w* k1.5 .	2R∣∣w*k	TXr ∙	κ a
Define Mk :=	⅛L +	kk-2k-2	+	V——2k(ηvα)i.5--------- +	~ηV0k.	We give	an UPPer bound on
the number of non-zero updates SGD makes until convergence to a critical point (which is a global
minimum by ProPosition 1). The result is summarized in the following theorem.
Theorem 2. SGD converges to a global minimum after performing at most Mk non-zero updates.
We will briefly sketch the Proof of Theorem 2. The full Proof is deferred to the APPendix (see
Section 9.1.2). The analysis is reminiscent of the PercePtron convergence Proof (e.g. in Shalev-
Shwartz & Ben-David (2014)), but with key modifications due to the non-linear architecture. Con-
cretely, assume SGD Performed t non-zero uPdates. We consider the vector W~ t and the vec-
kk
Z 人 一 { Z	人 一 {
tor W* = (w* ...w*, —w* •••一 w*) ∈ R2kd which is a global minimum of LS. We define
F(Wt) = DW~ t, W~ *E and G(Wt) = kW~ tk. Then, we give an uPPer bound on G(Wt) in terms
of G(Wt-1) and by a recursive aPPlication of inequalities we show that G(Wt) is bounded from
above by a square root of a linear function of t. Similarly, by a recursive aPPlication of inequal-
ities, we show that F (Wt) is bounded from below by a linear function of t. Finally, we use the
Cauchy-SchWartz inequality, G(IF(Wt~!*∣∣ ≤ 1, to ShoW that t ≤ Mk.
To obtain a simPler bound than the one obtained in Theorem 2, we use the fact that we can set R, v
arbitrarily, and choose:8
(6)
Then by Theorem 2 We get the folloWing. The derivation is given in the APPendix (Section 9.1.3).
Corollary 3. Let R = V = √2k, then SGD converges to a global minimum after Perfoming at most
Mk = ⅛r + o(rnιk⅛⅛! non-zero updates.
Thus the bound consists of tWo terms, the first Which only dePends on the margin (via kw* k) and the
second Which scales inversely With η . More imPortantly, the bound is indePendent of the netWork
size.
5.2 Lower Bound
We use the same notations as in Section 5.1. The loWer bound is given in the folloWing theorem,
Which is Proved in the APPendix (Section 9.1.4).
Theorem 4. Assume SGD is initialized according to Eq. 6, then for any d there exists a sequence of
linearly separable points on which SGD will make at least ω(嵋 k + ∣∣w*k2) mistakes.
8This initialization resembles other initializations that are used in Practice (Bengio, 2012; Glorot & Bengio,
2010)
5
Published as a conference paper at ICLR 2018
Although this lower bound is not tight, it does show that the upper bound in Corollary 3 cannot be
much improved. Furthermore, the example presented in the proof of Theorem 4, demonstrates that
η → ∞ can be optimal in terms of optimization and generalization, i.e., SGD makes the minimum
number of updates (∣∣w* k2) and the learned model is equivalent to the true classifier w*. We will
use this observation in the discussion on the dependence of the generalization bound in Theorem 6
on η (see Remark 1).
5.3 Extensions - Updating both layers
The bounds we provide in this section rely on the assumption that the weights of the second layer
remain constant throughout the training process. Although this does not limit the expressive power
of the network, updating both layers effectively changes the dynamics of the problem, and it may
not be clear why the above bounds apply to this case as well. To answer this concern we show
the following. First, we run the same experiments as in Figure 1, but with both layers trained. We
show in Figure 2 that the training and generalization performance remain the same. Second, in the
complete proof of the upper bound given in Section 9.1.2, we relax the assumption that the weights of
the second layer are fixed, and only assume that they do not change signs during the training process,
and that their absolute values are bounded from below and from above. This results in a similar
bound, up to a constant factor. We corroborate our theoretical result with experiments and show in
Figure 3 that by choosing an appropriate constant learning rate, this in fact holds when updating
both layers - the weights of the last layer do not change their sign, and are correctly bounded.
Furthermore, the performance of SGD is not affected by the choice of the learning rate. A complete
theoretical analysis of training both layers is left for future work.
6 Generalization
In this section we give generalization guarantees for SGD learning of over-parameterized networks
with Leaky ReLU activations. These results are obtained by combining Theorem 2 with a com-
pression generalization bound (see Section 6.1). In Section 6.2 we show that over-parameterized
networks are sufficiently expressive to contain global minima that overfit the training set. Taken to-
gether, these results show that although there are models that overfit, SGD effectively avoids these,
and finds the models that generalize well.
6.1 Compression B ound
Given the bound in Theorem 2 we can invoke compression bounds for generalization guarantees with
respect to the 0-1 loss (Littlestone & Warmuth, 1986) . Denote by Nk a two-layer neural network
with 2k hidden neurons defined in Section 1 where σ is the Leaky ReLU. Let SGDk (S, W0) be
the output of running SGD for training this network on a set S and initialized with W0 that satisfies
Eq. 5. Define Hk to be the set of all possible hypotheses that SGDk(S, W0) can output for any S
and W0 which satisfies Eq. 5.
Now, fix an initialization W0 . Then the key observation is that by Theorem 2 we have
SGDk(S, W0) = BW0 (xi1 , ..., xic ) for ck ≤ Mk, some function BW0 : Xck → Hk and
(iι,..., ick) ∈ [n]ck .9 Equivalently, SGDk(∙, W0) and Bw° define a compression scheme of size Ck
for hypothesis class Hk (see Definition 30.4 in Shalev-Shwartz & Ben-David (2014)). Denote by
V = {xj : j ∈/ {i1, ..., ick}} the set of examples which were not selected to define SGDk(S, W0).
LetL0D-1(SGDk(S,W0)) andL0V-1(SGDk(S,W0)) be the true risk of SGDk(S, W0) and empir-
ical risk of SGDk(S, W0) on the set V , respectively. Then by Theorem 30.2 and Corollary 30.3
in Shalev-Shwartz & Ben-David (2014) we can easily derive the following theorem. The proof is
deferred to the Appendix (Section 9.2.1).
Theorem 5.	Let n ≥ 2ck, then with probability of at least 1 - δ over the choice of S and W0 we
have
LDI(SGDk(S, Wo)) ≤ LV-1(SGDk(S, Wo)) + {L0∕1(SGDk(S,Wo))4ck :g δ + 8ck ；g δ
9We use a subscript W0 because the function is determined by W0 .
6
Published as a conference paper at ICLR 2018
#hidden neurons = 10
#hidden neurons = 100
#hidden neurons = 1000
0.05
4 3 2 1
Oooo
0.0,0,0.
Jo」」①IS区
#hidden neurons = 10
#hidden neurons = 100
#hidden neurons = 1000
0,0%	20	40	60	80	100
epoch
(a)	(b)
Figure 1: Classifying MNIST images with over-parameterized networks. The setting of Section 5
is implemented (e.g., SGD with batch of size 1, only first layer is trained, Leaky ReLU activations)
and SGD is initialized according to the initialization defined in Eq. 6. The linearly separable data set
consists of 4000 MNIST images with digits 3 and 5, each of dimension 784. The size of the training
set is 3000 and the remaining 1000 points form the test set. Three experiments are performed which
differ only in the number of hidden neurons, 10, 100 and 1000. In the latter two, the networks are
over-parameterized. For each number of hidden neurons, 40 different runs of SGD are performed
and their results are averaged. (a) shows that in all experiments SGD converges to a global minimum.
(b) shows that the global minimum obtained by SGD generalizes well in all settings (including the
over-parameterized).
Since L0V-1(SGDk(S, W0)) = 0 holds at a global minimum of LS, then by Combining the results
of Corollary 3 and Theorem 5, we get the following theorem.
Theorem 6.	If n ≥ 2ck and assuming the initialization defined in Eq. 6, then with probability at
least 1 - δ over the choice of S and W0, SGD converges to a global minimum of LS with 0-1 test
error at most
8 kwɪ
n α2
+O
n
log δ
(7)
Thus for fixed ∣∣w* ∣∣ and η We obtain a sample complexity guarantee that is independent of the
network size (See Remark 1 for a discussion on the dependence of the bound on η). This is despite
the fact that for sufficiently large k, the netWork has global minima that have arbitrarily high test
errors, as We shoW in the next section. Thus, SGD and the linearly separable data introduce an
inductive bias Which directs SGD to the global minimum With loW test error While avoiding global
minima With high test error. In Figure 1 We demonstrate this empirically for a linearly separable
data set (from a subset of MNIST) learned using over-parameterized netWorks. The figure indeed
shoWs that SGD converges to a global minimum Which generalizes Well.
Remark 1. The generelization bound in Eq. 7 holds for η → ∞, which is unique for the setting
that we consider, and may seem surprising, given that a choice of large η often fails in practice.
Furthermore, the bound is optimal for η → ∞. To support this theoretical result, we show in
Theorem 4 an example where indeed η → ∞ is optimal in terms of the number of updates and
generalization. On the other hand, we note that in practice, it may not be optimal to use large
η in our setting, since this bound results from a worst-case analysis of a sequence of examples
encountered by SGD. Finally, the important thing to note is that the bound holds for any η, and is
thus applicable to realistic applications of SGD.
7
Published as a conference paper at ICLR 2018
6.2 Expressiveness
Let X ∈ Rd×n be the matrix with the points xi in its columns, y ∈ {-1, 1}n the corresponding
vector of labels and let NW(X) = v>σ(W X) be the network defined in Eq. 1 applied on the matrix
X . By Theorem 8 in (Soudry & Hoffer, 2017) we immediately get the following. For completeness,
the proof is given in the Appendix (Section 9.2.2).
Theorem 7. Assume that k ≥ 2 ∣- 2dn-2-∣ ∙ Thenfor any y ∈ { —1,1}n and for almost any X,10
kk
there exist W = (W, V) where W ∈ R2k×d and V = (^^V, —V ;∙二V) ∈ R2k, v > 0 such that
y = NW(X) ∙
Theorem 7 implies that for sufficiently large networks, the optimization problem (2) can have arbi-
trarely bad global minima with respect to a given test set, i.e., ones which do not generalize well on
a given test set.
7	ReLU- Success and Failure Cases
In this section we consider the same setting as in section 5, but with the ReLU activation function
σ(x) = max{0, x}. In Section 7.1 we show that the loss function contains arbitrarely bad local
minima. In Section 7.2 we give an example where for a sufficiently small network, with high prob-
ability SGD will converge to a local minimum. On the other hand, for a sufficiently large network,
with high probability SGD will converge to a global minimum.
7.1	Existence of Bad Local Minima
The result is summarized in the following theorem and the proof is deferred to the Appendix (Section
9.3.1). The main idea is to construct a network with weight paramater W such that for at least 煤
points (x, y) ∈ S it holds that hw, xi < 0 for each neuron with weight vector w. Furthermore, the
remaining points satisfy yNw(x) > 1 and thus the gradient is zero and LS(W) > 1.
kk
Theorem 8. Fix V = (1"ʌTr,-1 .;-V) ∈ R2k. Then,for every finite set ofexamples S ⊆ X ×Y
that is linearly separable, i.e.,forwhich there exists w* ∈ Rd such thatfor each (x,y) ∈ S we have
y hw*, Xi ≥ 1, there exists W ∈ R2k×d such that W is a local minimum point with LS (W) > 1.
7.2	Orthogonal Vectors - Simple Case Analysis
In this section we assume that S = {e1...ed} × {1} ⊆ X × Y where {e1, ..., ed} is the standard
basis of Rd . We assume all examples are labeled with the same label for simplicity, as the same
result holds for the general case.
Let NWt be the network obtained at iteration t, where Wt = (Wt, V). Assume we initialize with
kk
fixed v = (1"ʌTr,—1 •二 一 V), and Wo ∈ R2k×d is randomly initialized from a continuous sym-
metric distribution with bounded norm, i.e |[W0]i,j | ≤ C for some C > 0.
The main result of this section is given in the following theorem. The proof is given in the Appendix
(Section 9.3.2). The main observation is that the convergence to non-global minimum depends
solely on the initialization and occurs if and only if there exists a point x such that for all neurons,
the corresponding initialized weight vector w satisfies hw, xi ≤ 0.
Theorem 9. Fix δ > 0 and assume we run SGD with examples from S = {e1...ed} × {1}.
If k ≤ log2( — id(6)), then with probability of at least 1 — δ, SGD will converge to a non global
minimum point.
On the other hand, if k ≥ log2( 2d) ,then with probability of at least 1 — δ, SGD will ConVerge to a
global minimum point afterdmax{ dC, d }e iterations.
10That is, the set of entries of X which do not satisfy the statement is of Lebesgue measure 0.
8
Published as a conference paper at ICLR 2018
Note that in the first part of the theorem, we can make the basin of attraction of the non-global
minimum exponentially large by setting δ = e-αd for ɑ ≤ 1.
8	Conclusion
Understanding the performance of over-parameterized neural networks is essential for explaining
the success of deep learning models in practice. Despite a plethora of theoretical results for gener-
alization of neural networks, none of them give guarantees for over-parameterized networks. In this
work, we give the first provable guarantees for the generalization performance of over-parameterized
networks, in a setting where the data is linearly separable and the network has Leaky ReLU activa-
tions. We show that SGD compresses its output when learning over-parameterized networks, and
thus exhibits good generalization performance.
The analysis for networks with Leaky ReLU activations does not hold for networks with ReLU
activations, since in this case the loss contains spurious local minima. However, due to the success
of over-parameterized networks with ReLU activations in practice, it is likely that similar results
hold here as well. It would be very interesting to provide convergence guarantees and generalization
bounds for this case. Another direction for future work is to show that similar results hold under
different assumptions on the data.
Acknowledgments
This research is supported by the Blavatnik Computer Science Research Fund, ISF F.I.R.S.T.
(Bikura) grant and the European Research Council (TheoryDL project).
References
Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations. cambridge
university press, 2009.
Peter Bartlett, Dylan J Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural
networks. arXiv preprint arXiv:1706.08498, 2017.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.
Yoshua Bengio. Practical recommendations for gradient-based training of deep architectures. In
Neural networks: Tricks ofthe trade, pp. 437T78. Springer, 2012.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian
inputs. arXiv preprint arXiv:1702.07966, 2017.
Simon S Du, Jason D Lee, and Yuandong Tian. When is a convolutional filter easy to learn? arXiv
preprint arXiv:1709.06129, 2017.
Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. arXiv preprint
arXiv:1703.11008, 2017.
P Frasconi, M Gori, and A Tesi. Successes and failures of backpropagation: A theoretical. Progress
in Neural Networks: Architecture, 5:205, 1997.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence
and Statistics, pp. 249-256, 2θ10.
Marco Gori and Alberto Tesi. On the problem of local minima in backpropagation. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, 14(1):76-86, 1992.
Moritz Hardt, Tengyu Ma, and Benjamin Recht. Gradient descent learns linear dynamical systems.
arXiv preprint arXiv:1609.05191, 2016.
9
Published as a conference paper at ICLR 2018
Kenji Kawaguchi, Leslie Pack Kaelbling, and Yoshua Bengio. Generalization in deep learning.
arXiv preprint arXiv:1710.05468, 2017.
Ilja Kuzborskij and Christoph Lampert. Data-dependent stability of stochastic gradient descent.
arXiv preprint arXiv:1703.01678, 2017.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation.
arXiv preprint arXiv:1705.09886, 2017.
Nick Littlestone and Manfred Warmuth. Relating data compression and learnability. Technical
report, Technical report, University of California, Santa Cruz, 1986.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the
role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Conference on Learning Theory, pp. 1376-1401, 2015.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. Exploring gener-
alization in deep learning. arXiv preprint arXiv:1706.08947, 2017a.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. A pac-
bayesian approach to spectrally-normalized margin bounds for neural networks. arXiv preprint
arXiv:1707.09564, 2017b.
Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. arXiv
preprint arXiv:1704.08045, 2017.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-
rithms. Cambridge university press, 2014.
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimiza-
tion landscape of over-parameterized shallow neural networks. arXiv preprint arXiv:1707.04926,
2017.
Daniel Soudry and Elad Hoffer. Exponentially vanishing sub-optimal local minima in multilayer
neural networks. arXiv preprint arXiv:1702.05777, 2017.
Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its
applications in convergence and critical point analysis. arXiv preprint arXiv:1703.00560, 2017.
Huan Xu and Shie Mannor. Robustness and generalization. Machine learning, 86(3):391-423,
2012.
Xiao-Hu Yu and Guo-An Chen. On the local minima free condition of backpropagation learning.
IEEE Transactions on Neural Networks, 6(5):1300-1303, 1995.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees
for one-hidden-layer neural networks. arXiv preprint arXiv:1706.03175, 2017.
9	Appendix
9.1	Missing Proofs for Section 5
9.1.1	Proof of Proposition 1
1.	Denote by W = (w(1)... w(k)u⑴... u(k)) ∈ R2kd the vector of all parameters where
each w(i), u(i) ∈ Rd. Let (x, y) ∈ S, then if yNW(x) < 1, it holds that
(∂⅛L{(χ,y)}(W), W) = D-yσ0 (Dw(i),xE)χ, w*E ≤ -σ0 (Dw(i),XE)
<0
10
Published as a conference paper at ICLR 2018
and similarly,
(∂U(i) L{(x,y)}(w~ )
一 *
-W
yσ0
,x x, -w* ≤
w(i), x	< 0.
kk
Hence if We define W* = (w*... w*, —w* ..•— w*) ∈ R2kd, then
(幅L{(χ,y)}(W),W*) < 0
∂W
OtherWise, if yNW (x) ≥ 1, then the gradient vanishes and thus
(焉 L{(χ,y)}(W ),W*) = 0
∂W
It folloWs that if there exists (x, y) ∈ S, such that yNW (x) < 1, then We have
(∂~LS(W), w*) = n X ( d⅛LSi)}(W), W*)< 0
and thus -~^LS(W) = 0. Therefore, for any critical point it holds that yNw(x) ≥ 1 for
all (x, y) ∈ S, Which implies that it is a global minimum.
2.	For simplicity consider the function fx(w, u) = σ(hw, xi) — σ(hu, xi) for x 6= 0. Define
w1 = w2 = u1 = x and u2 = —x. Then
fx(w1, u1) = 0
fx(w2, u2) = (1 + α)kxk2
and
fx(w⅛w2,中) = kxk2
and thus fχ( w1 + w2, u1+ u2) > 1 fχ(wι, uι) + 11 fx(w2, u2) which implies that the func-
tion is not convex.
9.1.2	Proof of Theorem 2
We will start by analyzing a case with more relaxed assumptions - namely, we do not assume that
the weights of the second layer are fixed, but rather that they do not change signs, and are bounded
in absolute value. Formally, let vt(i) be the weight of the second layer neuron corresponding to the
weight vector w(i), and v(i) the weight corresponding to Uti). Then we assume there exist c,C > 0
such that:
v(i)	v(i)
C ≤ ∖ ≤ C, c ≤ ∖ ≤ C
(i)	(i)
v0	v0
(8)
And note that we take v(i) = —v(i) = v.
Assume SGD performed t non-zero updates. We will show that t ≤ Mk. We note that if there is no
(x, y ) ∈ S such that the corresponding update is non-zero, then SGD has reached a critical point of
kk
Z 人 -{ Z —	人 -{
LS (which is a global minimum by Proposition 1). Let W* = (w*... w*, —w* ∙•• — w*) ∈ R2kd
and note thatLS(W*) = 0, i.e., W* is a global minimum. Define the following two functions:
kk
F(Wt)=DW~t,W~ *E = X Dw(ti), w*E — X Dut(i), w*E
i=1	i=1
G(Wt) = kW~ tk
∖
kk
X kwt(i)k2 + X kut(i)k2
i=1	i=1
11
Published as a conference paper at ICLR 2018
Then, from Cauchy-Schwartz inequality we have
∣f mi = KWt,W*EI ≤ 1
G(Wt)∣W*k - kWtkkW*k -
(9)
Since the update at iteration t is non-zero, we have ytNt-1(xt) < 1 and the update rule is given by
Wti) = w(i)ι + ηv(i)pti)ytχt , Uti) = u(2ι - ηv(i)q(i)yXt	(10)
where pt(i) = 1 if wt(-i)1, xt ≥ 0 and p(ti) = α otherwise. Similarly qt(i) = 1 if u(ti-)1, xt ≥ 0
and qt(i) = α otherwise. It follows that:
kk
G(Wt)2 = X kwt(i)k2 + X kut(i)k2
i=1	i=1
kk	k	k
≤ X kwt-ιk2 + X kut-ιk2 + 2ηyt (X (w(-ι,xt) v(i)p(i) - X (ut-ι, χt) v(l)q(l) I +2kη2C2。2|如『
i=1	i=1	i=1	i=1
kk
< X kw(ti-)1k2 + X ku(ti-)1k2 + 2η + 2kη2v2 = G(Wt-1)2 + 2η + 2kη2 C2 v2
i=1	i=1
where the second inequality follows since yt (Pk=I (w(i)ι, xt〉v(i)p(i) - Pk=I <u(-ι, xt〉v(i)q(i)
ytNt-1(xt) < 1. Using the above recursively, we obtain:
G(Wt)2 ≤ G(W0)2 + t(2kη2C2v2 +2η)	(11)
On the other hand,
kk
F (Wt)=XDw(i), w*E - XDu(i), w*E
i=1	i=1
kk	k	k
=X (w3, w*〉- X (u3,w*〉+ ηX hytxt, w*i v(i)p(i) + ηX hytxt, w*i vt(iqt(i
i=1	i=1	i=1	i=1
kk
≥ X Dwt(-i)1, w* E - X Dut(-i)1, w* E + 2kηcvα = F(Wt-1) + 2kηcvα
i=1	i=1
where the inequality follows since hytxt, w*i ≥ 1. This implies that
F(Wt) ≥ F(W0) + 2kηcvαt	(12)
By combining equations Eq. 9, Eq. 11 and Eq. 12 we get,
-G(W0)kW~ *k + 2kηcvαt ≤ F(W0) + 2kηcvαt ≤ F(Wt) ≤ kW~ *kG(Wt)
≤ IIW*k√G(W0)2 + t(2kη2C2v2 + 2η)
Using √a + b ≤ √α + ʌ/b the above implies,
-G(Wo)kW*k + 2kηcvαt ≤ ∣∣W*∣∣G(W0) + IlW*k√t√2kη2C2v2 + 2η
Since ∣∣w0i)k, ∣u0i)k ≤ R we have G(Wo) ≤ √2kR. NotingthatkW*k = √2k∣w*k we get,
at ≤ bʌ/t + C
where a = 2kηcvα, b =，(4k2η2C2v2 + 4ηk)∣∣w*∣ and C = 4kR∣w*∣. By inspecting the roots
of the parabola P(x) = x2 - bX - C We conclude that
a
rc b	c	(4k2η2C(Iv(I + 4ηk)∣w*k2
αα*α	4k2η2c2v2α2	+
√(4k2η2C 2v2 +4ηk) ∣∣w*k ∕2R∣w*k	2R∣w*∣
V — + 一
2kηcvα
ηcvα
ηcvα
C 2kw*k2	kw*k2	√R(8k2η2C 2v2 + 8ηk)kw*k1∙5	2R∣∣w*∣∣
c2α2	kηc2v2α2	2k(ηcvα)1.5	ηcvα
(13)
12
Published as a conference paper at ICLR 2018
06
∩-
∙~∙ #h = 10, Ir = 0.001
H #h = 100, Ir = 0.001
X #h = 1000, Ir= 0.001

(a)	(b)
Figure 2: Classifying MNIST images with over-parameterized networks and training both layers.
The setting of Figure 1 is implemented, but now the second layer is trained as well. The second
layer is initialized as in Figure 1, i.e., all the weights are initialized to √= or — √=. The training
and generalization performance are similar to the performance in the case where only the first layer
is trained (see Figure 1).
b)
Figure 3: Classifying MNIST images with over-parameterized networks, training both layers and
choosing an appropriate learning rate. The setting of Figure 2 is implemented, but here a different
learning rate is chosen for each network size, in order to satisfy the conditions of the proof in Section
9.1.2. Figures (a) and (b) are train and test errors of MNIST classification for different network sizes
and the chosen learning rates. In this setting, SGD exhibits similar training and generalization
performance as in Figure 2. Figure (c) shows the minimal and maximal value of the second layer
weights divided by their initial value (denoted as c, C respectively in Section 9.1.2). It can be
seen that these values remain above zero, which implies that the weights do not flip signs during
the training process (namely they satisfy the sign condition in Section 9.1.2) and that they behave
similarly for different network sizes.
Notice that when assuming that the weights of the second layer are fixed, we get c = C = 1 and the
above is simply equal to Mk . Otherwise, if c, C are independent constants, we get a similar bound,
up to a constant factor.
9.1.3	Proof of Corollary 3
Since R = 1, we have by Theorem 2 and the inequality √a + b ≤ √∕α + Vb,
13
Published as a conference paper at ICLR 2018
Mk = kwk2 + O kw屿 + O X[ + O X) + O kwɪkʌ
α2	V η J √ J ∖ η J ∖ η J (14)
=kw12 + O( kw*k2 !
a2	y min{η, √η}."
9.1.4	Proof of Theorem 4
We will prove a more general theorem. Theorem 4 follows by setting R = V = √=.
Theorem 10. For any d there exists a sequence of linearly separable points on which SGD will
make at least
max { min {B1,B2}, ∣∣w*∣∣2}
updates, where
B1 = 3 +min !格
ηvα	2ηkv2
α∣∣w*∣∣, 0
—
and
B = Rkwɪk	[ IKT
2 = ηv +	2 2a2 ηkv2
kwk, 0
α
—
Proof. Define a sequence S of size d,
(e1, 1), (e2, 1),..., (ed, 1)
where {ei} is the standard basis of Rd and let w* = (1,1,…，1) ∈ Rd. Note that d = ∣∣w*∣2
and hw*, eii ≥ 1 for all 1 ≤ i ≤ d. We will consider the case where SGD runs on a sequence of
examples which consists of multiple copies of S one after the other.
Assume SGD is initialized with
(i)
w0
d
j=1
R
√d ej
for all 1 ≤ i ≤ k. Note that kw(0i)k, ku(0i)k ≤ R for all 1 ≤ i ≤ k.
Since w(0i) = w(0j) and u(0i) = u(0j) for all i 6= j, we have by induction that wt(i) = wt(j) and
ut(i) = u(tj) for all i 6= j and t > 0. Hence, we will denote wt = wt(i) and ut = ut(i) for all
1 ≤ i ≤ d and t > 0. Then for all 1 ≤ j ≤ d we have NWt (ej ) = kvσ0(hwt, ej i) hwt, eji -
kvσ0(hut, eji) hut,eji.
At the global minumum NWt,v (ej ) ≥ 1 for all 1 ≤ j ≤ d, thus it follows that a necessary
condition for convergence to a global minimum is that there exists an iteration t in which either
kvσ0(hwt, edi)hwt, ed ≥ 2 or —kvσ0(hut, edi) gt, ed ≥ 1. Equivalently, either 〈wt, ed ≥
7r7- orhUt, edi ≤ — f) ∖-.
2kv	t, d	2αkv .
Sincehw。, ed = - -R, then by the gradient updates (Eq. 10) it follows that after at least rjvR√2
Rd
nvay∕d
copies of S, or equivalently, after at least
iterations we will have 0 ≤ hwt , edi ≤ ηvα.
Then, after at least dmin{2kv-nva,0} iterations We have hwt, edi ≥ 21V. Thus, in total, after at least
RkwOk +min{kw⅛2 — α∣w*k2,0} iterations, we have hw ed ≥ 2i-.
By the same reasoning, we havehUt, edi ≤ —另kv after at least RIw k +min{ 2OXjkv2 一映ɪ, 0}
iterations. Finally, SGD must update on at least d points in order to converge to the global minimum.
The claim now follows.	□
14
Published as a conference paper at ICLR 2018
9.2	Missing Proofs for Section 6
9.2.1	Proof of Theorem 5
By Theorem 30.2 and Corollary 30.3 in Shalev-Shwartz & Ben-David (2014), for n ≥ 2ck we have
that with probability of at least 1 - δ over the choice of S
…,	4c	4 ^Ck o 4	…,	4 4ck log n	8Ck log n
LD(SGDk(S,Wo)) ≤ LV(SGDk(S,Wo)) + ∖/Lv(SGDk(S,Wo))	δ +	δ (15)
nn
The above result holds for a fixed initialization W0 . We will show that the same result holds with
high probability over S and W0 , where W0 is chosen independently of S and satisfies Eq. 5. Define
B to be the event that the inequality Eq. 15 does not hold. Then we know that PS (B|W0) ≤ δ for
any fixed initialization W0. 11 Hence, by the law of total expectation,
PS,W0(B)=EW0[PS(B|W0)] ≤δ
9.2.2	Proof of Theorem 7
We can easily extend Theorem 8 in (Soudry & Hoffer, 2017) to hold for labels in {-1, 1}. By the
theorem we can construct networks NW1 and NW2 such that for all i:
1.	NW1 (xi) = 1 if yi = 1 and NW1 (xi) = 0 otherwise.
2.	NW2 (xi) = 1 if yi = -1 and NW2 (xi) = 0 otherwise.
Then (Nwι - Nw?)(xi) = yi and Nw、- Nw? = NW for W = (W, V) where W ∈ R2k×d and
kk
zz-}|-{
v = (V ...V,-V ——V) ∈ R2k, V > 0.
9.3	Missing Proofs for Section 7
9.3.1	Proof of Theorem 8
We first need the following lemma.
Lemma 11. There exists W ∈ Rd that satisfies the following:
1.	There exists a > 0 such thatfor each (x, y) ∈ S we have |〈x, W)| > α.
2.	#{(x,y) ∈ S ： hW, Xi < 0} > 1 |S|.
Proof. Consider the set V = {v ∈ Rd : ∃(x,y)∈S hv, xi = 0}. Clearly, V is a finite union of hyper-
planes and therefore has measure zero, so there exists W ∈ Rd \ V. Let β = min(χ,y)∈s{|(W, x)|},
and since S is finite we clearly have α > 0. Finally, if
#{(x,y) ∈ S : (W,Xi < 0} > 2|S|
We can choose W and α = β and We are done. Otherwise, choosing -W and α = β satisfies all the
assumptions of the lemma.	□
We are now ready to prove the theorem. Choose W ∈ Rd that satisfies the assumptions in Lemma
11. Now, let c > kWα k, and let W = cW + w* and U = CW — w*. Define
kk
W = ∖ZW~}lW{, zj~}l~^u]> ∈ R2k×d
11This is where we use the independence assumption on S and W0 . In the proof of Theorem 30.2 in Shalev-
Shwartz & Ben-David (2014), the hypothesis hI needs to be independent of V . Our independence assumption
ensures that this holds.
15
Published as a conference paper at ICLR 2018
Let (x, y) ∈ S be an arbitrary example.
IfhW, Xi > α, then
hw, Xi = C (W, Xi + (w*, Xi ≥ Ca — ||w* k > 0
hu, Xi = c (W, Xi — (w*, xi ≥ Ca — ∣∣w* k > 0
It follows that
kk
NW (X) =	σ(hW, Xi) —	σ(hu, Xi)
kk
=X(C (W, Xi + hw*, Xi) — X(C (W, Xi — (w*, Xi)
11
=2k (w*, Xi
Therefore yNW (X) > 1, so we get zero loss for this example, and therefore the gradient of the loss
will also be zero.
If, on the other hand, (W, Xi < —a, then
(w, Xi = c (W, Xi + (w*, Xi ≤ — Ca + ∣∣w* k < 0
(u, Xi = c (W, Xi — (w*, Xi ≤ — Ca + ∣∣w* k < 0
and therefore
kk
NW (X) = X σ((w, Xi) — Xσ((u,Xi) = 0.
11
In this case the loss on the example would be max{1 — y NW (X), 0} = 1, but the gradient will also
be zero. Along with assumption 2, we would conclude that:
1∂
Ls (W) > 2, ∂WLS (W) = 0
Notice that since all the inequalities are strong, the following holds for all W0 ∈ R2k×d that satisfies
kW0 — W k < , for a small enough > 0. Therefore, W ∈ R2k×d is indeed a local minimum.
9.3.2 Proof of Theorem 9
Denote Wt =	[wt(1)	. . . wt(k)u(t1) . . . u(tk)]	and define Kt	=	{ej	:	∀i∈[k]	wt(i), ej	≤ 0}. We first
prove the following lemma.
Lemma 12. For every t we get Kt+1 = Kt.
Proof. Let ej be the example seen in time t. If NWt (ej) ≥ 1 then there is no update and we are
done. Otherwise, if ej ∈ Kt then for each i ∈ [k] We have	NWt (ej) = 0 and therefore the
∂wt	t
update does not change the value ofwt(i), and thus Kt+1 = Kt. If ej ∈/ Kt then there exists i ∈ [k]
such that w(ti) , ej > 0. In that case, we update w* - w(i) + ηej. Now, note that
Dwt(+i)1, ejE = Dw(ti), ej E +η(ej,eji > Dw(ti), ej E > 0
and therefore ej ∈/ Kt+1. Furthermore, for each e` where ` 6= j, by the orthogonality of the vectors
we know that for each i ∈ [k] it holds that
e`
e`
Thus e` ∈ Kt if and only if e` ∈ Kt+1 and this concludes the lemma.
□
16
Published as a conference paper at ICLR 2018
We can now prove the theorem. For each j ∈ [d], by the symmetry of the initialization, with
probability 2 over the initialization of w0i), We get that(w0i), ej ≤ 0. Since all Wi,s are initialized
independently, we get that:
P (ej ∈ KO)=P (∩i∈[k] Dw0i), ej E ≤ O) = γ P (Dwo∖ ej E ≤ 0)=21k
i∈[-]
Now, assuming k ≤ log2(-禧⑷),from the independence of the initialization of WPs coordinates
We get
P(∩j∈[d]ej ∈/ K0) = P(ej ∈/ K0)
j∈[d]
= (I- 1kj-)d ≤ e-2k ≤ δ
Therefore, with probability at least 1 - δ, there exists j ∈ [k] for which ej ∈ K0. By Lemma
12, this implies that for all t ∈ N we will get ej ∈ Kt, and therefore NWt (ej) ≤ 0. Since ej is
labeled 1, this implies that LS (W) > 0. By the separability of the data, and by the convergence
of the SGD algorithm, this implies that the algorithm converges to a stationary point that is not a
global minimum. Note that convergence to a saddle point is possible only if we define σ0 (0) = 0,
and for all i ∈ [k] we have at the time of convergence w(ti) , ej = 0. This can only happen if
w(0i) , ej = ηN for some N ∈ N, which has probability zero over the initialization of wt(i) .
Therefore, the convergence is almost surely to a non-global minimum point.
On the other hand, assuming k ≥ log2(d), using the union bound we get:
P(∪j∈[d] ej ∈ K0) ≤	P(ej ∈ K0)
j∈[d]
d
=2- ≤ δ
So with probability at least 1 - δ, we get K = 0 and by Lemma 12 this means Kt = 0 for all t ∈ N.
Now, if ej ∈/ Kt for all t ∈ N, then there exists i ∈ [k] such that w(ti), ej > 0 for all t ∈ N. If
after performing T update iterations we have updated N > max{ Cn, 1} times on ej∙, then clearly:
T
wt(i), ej	=	w(0i), ej	+ Xηhej,eji ≥	w(0i), ej	+Nη > 1
t=0
T
∀i∈[-]	s.t	u(0i),ej	>	0,	u(ti),ej	=	u(0i),ej	- Xηhej,eji ≤ C - Nη ≤ 0
t=0
and therefore NWt (ej) > 1, which implies that L{(ej,1)} (Wt) = 0. From this, we can conclude
that for each j ∈ [d], we perform at most dmax{ CC, 1}e update iterations on ej before reaching
zero loss, and therefore we can perform at most dmax{ dC, d }e update iterations until convergence.
Since we show that we never get stuck with zero gradient on an example with loss greater than zero,
this means we converge to a global optimum after at most dmax{ dC, d}e iterations.
17