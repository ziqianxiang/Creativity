Published as a conference paper at ICLR 2018
Sobolev GAN
Youssef MrOueht, Chun-Liang Li°,?, Tom Sercut,?, Anant Raj"? & Yu Chengt
f IBM Research AI
◦ Carnegie Mellon University
♦ Max Planck Institute for Intelligent Systems
? denotes Equal Contribution
{mroueh,chengyu}@us.ibm.com, chunlial@cs.cmu.edu,
tom.sercu1@ibm.com,anant.raj@tuebingen.mpg.de
Ab stract
We propose a new Integral Probability Metric (IPM) between distributions: the
Sobolev IPM. The Sobolev IPM compares the mean discrepancy of two distribu-
tions for functions (critic) restricted to a Sobolev ball defined with respect to a
dominant measure μ. We show that the SoboleV IPM compares two distributions
in high dimensions based on weighted conditional Cumulative Distribution Func-
tions (CDF) of each coordinate on a leaVe one out basis. The Dominant measure
μ plays a crucial role as it defines the support on which conditional CDFS are
compared. SoboleV IPM can be seen as an extension of the one dimensional Von-
Mises Cramer statistics to high dimensional distributions. We show how Sobolev
IPM can be used to train GeneratiVe AdVersarial Networks (GANs). We then ex-
ploit the intrinsic conditioning implied by Sobolev IPM in text generation. Finally
we show that a variant of Sobolev GAN achieves competitive results in semi-
supervised learning on CIFAR-10, thanks to the smoothness enforced on the critic
by Sobolev GAN which relates to Laplacian regularization. 1
1	Introduction
In order to learn Generative Adversarial Networks (Goodfellow et al., 2014), it is now well estab-
lished that the generator should mimic the distribution of real data, in the sense of a certain dis-
crepancy measure. Discrepancies between distributions that measure the goodness of the fit of the
neural generator to the real data distribution has been the subject of many recent studies (Arjovsky
& Bottou, 2017; Nowozin et al., 2016; Kaae S0nderby et al., 2017; Mao et al., 2017; Arjovsky et al.,
2017; Gulrajani et al., 2017; Mroueh et al., 2017; Mroueh & Sercu, 2017; Li et al., 2017), most of
which focus on training stability.
In terms of data modalities, most success was booked in plausible natural image generation after the
introduction of Deep Convolutional Generative Adversarial Networks (DCGAN) (Radford et al.,
2015). This success is not only due to advances in training generative adversarial networks in terms
of loss functions (Arjovsky et al., 2017) and stable algorithms, but also to the representation power
of convolutional neural networks in modeling images and in finding sufficient statistics that capture
the continuous density function of natural images. When moving to neural generators of discrete
sequences generative adversarial networks theory and practice are still not very well understood.
Maximum likelihood pre-training or augmentation, in conjunction with the use of reinforcement
learning techniques were proposed in many recent works for training GAN for discrete sequences
generation (Yu et al., 2016; Che et al., 2017; Hjelm et al., 2017; Rajeswar et al., 2017). Other
methods included using the Gumbel Softmax trick (Kusner & Hernandez-Lobato, 2016) and the
use of auto-encoders to generate adversarially discrete sequences from a continuous space (Zhao
et al., 2017). End to end training of GANs for discrete sequence generation is still an open prob-
lem (Press et al., 2017). Empirical successes of end to end training have been reported within the
framework of WGAN-GP (Gulrajani et al., 2017), using a proxy for the Wasserstein distance via a
1 Code for semi-supervised learning experiments is available on https://github.com/tomsercu/
SobolevGAN-SSL
1
Published as a conference paper at ICLR 2018
pointwise gradient penalty on the critic. Inspired by this success, we propose in this paper a new In-
tegral Probability Metric (IPM) between distributions that we coin Sobolev IPM. Intuitively an IPM
(Muller, 1997) between two probability distributions looks for a witness function f, called critic,
that maximally discriminates between samples coming from the two distributions:
sup Ex〜Pf(X) - Ex〜Qf (x).
f∈F
Traditionally, the function f is defined over a function class F that is independent to the distributions
at hand (Sriperumbudur et al., 2012). The Wasserstein-1 distance corresponds for instance to an IPM
where the witness functions are defined over the space of Lipschitz functions; The MMD distance
(Gretton et al., 2012) corresponds to witness functions defined over a ball in a Reproducing Kernel
Hilbert Space (RKHS).
We will revisit in this paper Fisher IPM defined in (Mroueh & Sercu, 2017), which extends the IPM
definition to function classes defined with norms that depend on the distributions. Fisher IPM can
be seen as restricting the critic to a LebSegUe ball defined with respect to a dominant measure μ.
The Lebsegue norm is defined as follows:
f 2(x)μ(x)dx.
X
where μ is a dominant measure of P and Q.
In this paper we extend the IPM framework to critics bounded in the Sobolev norm:
X
X
∣∣Vxf (x)k2 μ(x)dx,
In contrast to Fisher IPM, which compares joint probability density functions of all coordinates
between two distributions, we will show that Sobolev IPM compares weighted (coordinate-wise)
conditional Cumulative Distribution Functions for all coordinates on a leave on out basis. Matching
conditional dependencies between coordinates is crucial for sequence modeling.
Our analysis and empirical verification show that the modeling of the conditional dependencies can
be built in to the metric used to learn GANs as in Sobolev IPM. For instance, this gives an advantage
to Sobolev IPM in comparing sequences over Fisher IPM. Nevertheless, in sequence modeling when
we parametrize the critic and the generator with a neural network, we find an interesting tradeoff
between the metric used and the architectures used to parametrize the critic and the generator as
well as the conditioning used in the generator. The burden of modeling the conditional long term
dependencies can be handled by the IPM loss function as in Sobolev IPM (more accurately the
choice of the data dependent function class of the critic) or by a simpler metric such as Fisher IPM
together with a powerful architecture for the critic that models conditional long term dependencies
such as LSTM or GRUs in conjunction with a curriculum conditioning of the generator as done
in (Press et al., 2017). Highlighting those interesting tradeoffs between metrics, data dependent
functions classes for the critic (Fisher or Sobolev) and architectures is crucial to advance sequence
modeling and more broadly structured data generation using GANs.
On the other hand, Sobolev norms have been widely used in manifold regularization in the so called
Laplacian framework for semi-supervised learning (SSL) (Belkin et al., 2006). GANs have shown
success in semi-supervised learning (Salimans et al., 2016; Dumoulin et al., 2017; Dai et al., 2017;
Kumar et al., 2017). Nevertheless, many normalizations and additional tricks were needed. We show
in this paper that a variant of Sobolev GAN achieves strong results in semi-supervised learning on
CIFAR-10, without the need of any activation normalization in the critic.
The main contributions of this paper can be summarized as follows:
1.	We overview in Section 2 different metrics between distribution used in the GAN literature.
We then generalize Fisher IPM in Section 3 with a general dominant measure μ and show
how it compares distributions based on their PDFs.
2.	We introduce Sobolev IPM in Section 4 by restricting the critic ofan IPM to a Sobolev ball
defined with respect to a dominant measure μ. We then show that Sobolev IPM defines a
discrepancy between weighted (coordinate-wise) conditional CDFs of distributions.
2
Published as a conference paper at ICLR 2018
3.	The intrinsic conditioning and the CDF matching make Sobolev IPM suitable for discrete
sequence matching and explain the success of the gradient pernalty in WGAN-GP and
Sobolev GAN in discrete sequence generation.
4.	We give in Section 5 an ALM (Augmented Lagrangian Multiplier) algorithm for training
Sobolev GAN. Similar to Fisher GAN, this algorithm is stable and does not compromise
the capacity of the critic.
5.	We show in Appendix A that the critic of Sobolev IPM satisfies an elliptic Partial Differ-
ential Equation (PDE). We relate this diffusion to the Fokker-Planck equation and show
the behavior of the gradient of the optimal Sobolev critic as a transportation plan between
distributions.
6.	We empirically study Sobolev GAN in character level text generation (Section 6.1). We
validate that the conditioning implied by Sobolev GAN is crucial for the success and sta-
bility of GAN in text generation. As a take home message from this study, we see that text
generation succeeds either by implicit conditioning i.e using Sobolev GAN (or WGAN-
GP) together with convolutional critics and generators, or by explicit conditioning i.e using
Fisher IPM together with recurrent critic and generator and curriculum learning.
7.	We finally show in Section 6.2 that a variant of Sobolev GAN achieves competitive semi-
supervised learning results on CIFAR-10, thanks to the smoothness implied by the Sobolev
regularizer.
2	Overview of Metrics between Distributions
In this Section, we review different representations of probability distributions and metrics for com-
paring distributions that use those representations. Those metrics are at the core of training GAN.
In what follows, we consider probability measures with a positive weakly differentiable probabil-
ity density functions (PDF). Let P and Q be two probability measures with PDFs P(x) and Q(x)
defined on X ⊂ Rd . Let FP and FQ be the Cumulative Distribution Functions (CDF) of P and Q
respectively. For x = (x1 , . . . , xd), we have:
FP(x)
广…广
-∞ -∞
P(u1 , . . . ud)du1 . . . dud.
The score function of a density function is defined as: sp(x) = Nx log(P(χ)) ∈ Rd.
In this work, we are interested in metrics between distributions that have a variational form and can
be written as a suprema of mean discrepancies of functions defined on a specific function class. This
type of metrics include 夕-divergences as well as Integral Probability Metrics (SriPerUmbUdur et al.,
2009) and have the following form:
dF(P, Q) = Sup ∣∆(f; P, Q)|,
f∈F
where F is a function class defined on X and ∆ is a mean discrepancy, ∆ : F → R. The variational
form given above leads in certain cases to closed form expressions in terms of the PDFs P, Q or in
terms of the CDFs FP, FQ or the score functions sP, sQ.
In Table 1, we give a comparison of different discrepancies ∆ and function spaces F used in the
literature for GAN training together with our proposed Sobolev IPM. We see from Table 1 that
Sobolev IPM, compared to Wasserstein Distance, imposes a tractable smoothness constraint on the
critic on points sampled from a distribution μ, rather then imposing a Lipschitz constraint on all
points in the space X. We also see that Sobolev IPM is the natural generalization of the Cramer
Von-Mises Distance from one dimension to high dimensions. We note that the Energy Distance, a
form of Maximum Mean Discrepancy for a special kernel, was used in (Bellemare et al., 2017b)
as a generalization of the Cramer distance in GAN training but still needed a gradient penalty in
its algorithmic counterpart leading to a mis-specified distance between distributions. Finally it is
worth noting that when comparing Fisher IPM and Sobolev IPM we see that while Fisher IPM com-
pares joint PDF of the distributions, Sobolev IPM compares weighted (coordinate-wise) conditional
CDFs. As we will see later, this conditioning nature of the metric makes Sobolev IPM suitable for
comparing sequences. Note that the Stein metric (Liu et al., 2016; Liu, 2017) uses the score func-
tion to match distributions. We will show later how Sobolev IPM relates to the Stein discrepancy
(Appendix A).
3
Published as a conference paper at ICLR 2018
	∆(f; P, Q)	F Function class	dF (P, Q) Closed Form
4-DiVergenCe (Goodfellow et al.,2014) (Nowozin et al., 2016)	Eχ~pf(X - Ex~qg*(∕(x)) 4* Fenchel Conjugate	{f : X→ R,f ∈ domφ. O	Ex~Q ]夕(Qx)i
Wasserstein -1 (Arjovsky et al., 2017) (GUIrajanietaL,2017)	Ex~p∕(x) - Ex~q∕(x)	{f :X→R,kfkiip ≤ ι}	inf∏∈∏(p,Q) JX ∣∣x - y∣ι d∏(X, y) Sinkhorn Divergence (Genevay et al., 2017)
MMD (Li et al., 2017) (Li et al., 2015) (Dziugaite et al., 2015)	Ex~p∕(x) - Ex~q∕(x)	{f : X→ R,kfkHk ≤ l}	∣Ex~Pkx - Ex~Qkx∣∣Hk
Stein Discrepancy (Wang&Liu, 2016)	Ex~q [T(P)f (x)] T(P) = (Vx log(P(X))> + Vx.	{f : X → Rd f smooth with zero boundary condition ∖	NA in general has a closed form inRKHS
Cramer for d = 1 (Bellemare et al., 2017a)	Ex~p∕(x) — Ex~q∕(x)	nf:X → R, Ex~p( fXx) )2 ≤ 1, f smooth with zero boundary condition }	,Eχ~P (FP(XP-FQ(X) j X ∈ R
μ-Fisher IPM (Mroueh & Sercu, 2017)	Ex~p∕(x) — Ex~q∕(x)	[f : X→ R,f ∈ L2(X ,μ), Eχ~μf23 ≤ l}	Jj (…)2
μ-Sobolev IPM (This work)	Ex~p∕(x) - Ex~q∕(x)	[f : X→ R,f ∈ Wo1,2(X ,μ), Eχ~μ ∣∣Vχf(x)∣∣2 ≤ 1, with zero boundary condition )	14 ⅛ P(Φi(P)-Φi(Q)/ dy Eχ~μ 2i=ι I	μ(X)	) where φi (P)= PX-i(X-i)FPXi ∣X-i = χ-i] (Xi) X-i =(X1,…Xi-i,Xi+ι,…Xd)
Table 1: Comparison of different metrics between distributions used for GAN training. References
are for papers using those metrics for GAN training.
3	Generalizing Fisher IPM: PDF Comparison
Imposing data-independent constraints on the function class in the IPM framework, such as the
Lipschitz constraint in the Wasserstein distance is computationally challenging and intractable for
the general case. In this Section, we generalize the Fisher IPM introduced in (Mroueh & Sercu,
2017), where the function class is relaxed to a tractable data dependent constraint on the second
order moment of the critic, in other words the critic is constrained to be in a Lebsegue ball.
Fisher IPM. Let X ⊂ Rd and P(X) be the space of distributions defined on X. Let P, Q ∈
P (X), and μ be a dominant measure of P and Q, in the sense that
μ(x) = 0 =⇒ P(X) = 0 and Q(X) = 0.
We assume μ to be also a distribution in P(X), and assume μ(x) > 0, ∀x ∈ X. Let L2(X, μ) be
the space of μ-measurable functions. For f,g ∈ L2 (X,μ),we define the following dot product and
its corresponding norm:
hf, giL2(X,μ)
f(X)g(X)μ(X)dx, kfkL2(X ,μ)
f f2(X)μ(X)dX.
X
X
X
Note that L2 (X, μ), can be formally defined as follows:
L2(X,μ) = {f ： X→Rs.t kf∣∣L2(X,μ) < ∞}.
4
Published as a conference paper at ICLR 2018
We define the unit Lebesgue ball as follows:
B2(X,μ) = {f ∈ L(X,μ), kf J(x,“)≤ 1}.
Fisher IPM defined in (Mroueh & Sercu, 2017), searches for the critic function in the Lebesgue Ball
B2 (X, μ) that maximizes the mean discrepancy between P and Q. FiSher GAN (MroUeh & Sercu,
2017) was originally formulated specifically for μ = 1 (P + Q). We consider here a general μ as
long as it dominates P and Q. We define Generalized Fisher IPM as follows:
Fμ (P, Q)=	SUp	Ex〜Pf(X)- Ex〜Qf (x)	(1)
f∈B2(X,μ)
Note that:
P-Q
Ex〜Pf (X) - Ex〜Qf(X) =( f,	)	∙
μ L L2(X,μ)
Hence Fisher IPM can be written as follows:
Fμ(P, Q)=	sup	/f, P-Q]	⑵
f ∈B2(X ,μ) ∖	μ / L2 (X ,μ)
We have the following result:
Theorem 1 (Generalized Fisher IPM). The Fisher distance and the optimal critic are as follows:
1.	The Fisher distance is given by:
Fμ (P，Q)= P7Q	= ^Σ(p⅞W ∙
μ	L2(X ,μ)	V	∖	μ(X)	)
2.	The optimal fχ achieving the Fisher distance Fμ (P, Q) is:
1 P-Q
fχ =驴(P, Q) ʒ-, μ almost 4 surely ∙
Proof of Theorem 1. From Equation (2), the op-
timal fχ belong to the intersection of the hy-
perplane that has normal n = P-Q, and the
ball B2 (X, μ), hence fχ = n--------------. Hence
knkL2(X ,μ)
F (P, Q) = knkL2 (X,μ).	口
We see from Theorem 1 the role of the dominant measure μ: the optimal critic is defined with respect
to this measure and the overall Fisher distance can be seen as an average weighted distance between
probability density functions, where the average is taken on points sampled from μ. We give here
some choices of μ:
1.
2.
3.
For μ = 1 (P + Q), we obtain the symmetric chi-squared distance as defined in (Mroueh &
Sercu, 2017).
μGP, the implicit distribution defined by the interpolation lines between Pr and Qθ as in
(Gulrajani et al., 2017).
When μ does not dominate P, and Q, we obtain a non symmetric divergence. For example
for μ = P, Fp2 (P, Q) = ´X (P(X)P-Q(X)) dx. We see here that for this particular choice we
obtain the Pearson divergence.
4 Sobolev IPM
In this Section, we introduce the Sobolev IPM. In a nutshell, the Sobolev IPM constrains the critic
function to belong to a ball in the restricted Sobolev Space. In other words we constrain the norm
of the gradient of the critic Vxf (x). We will show that by moving from a Lebesgue constraint
as in Fisher IPM to a Sobolev constraint as in Sobolev IPM, the metric changes from a joint PDF
matching to weighted (ccordinate-wise) conditional CDFs matching. The intrinsic conditioning built
in to the Sobolev IPM and the comparison of cumulative distributions makes Sobolev IPM suitable
for comparing discrete sequences.
5
Published as a conference paper at ICLR 2018
4.1	Definition and Expression of Sobolev IPM in terms of Coordinate
Conditional CDFs
We will start by recalling some definitions on Sobolev Spaces. We assume in the following that X
is compact and consider functions in the SoboleV space W 1,2(X, μ):
W 1,2(X, μ) = {f : X → R, IX kVxf (x)k2 μ(x)dx < ∞},
We restrict ourselves to functions in W 1,2(X,μ) vanishing at the boundary, and note this space
Wo1,2(X, μ). Note that in this case:
kfkWl,2(X,μ)
k kVχf(x)k2 μ(x)dx
X
defines a semi-norm. We can similarly define a dot product in W1,2(X, μ), for f,g ∈ W1,2(X, μ):
hf,giW1,2(X ,μ) = / Wxf(X), Vχg(χ)iRd μ(X)dχ.
Hence we define the following Sobolev IPM, by restricting the critic of the mean discrepancy to the
Sobolev unit ball :
Sμ(P, Q) =	sup	{Ex〜Pf(X)- Ex〜Qf(X)}.
f ∈W0,2,kf kw1,2(X ,μ)≤1
(3)
When compared to the Wasserstein distance, the Sobolev IPM given in Equation (3) uses a data
dependent gradient constraint (depends on μ) rather than a data independent LiPchitz constraint. Let
FP and FQ be the cumulative distribution functions of P and Q respectively. We have:
and we define
D-i
P(x)
∂d
∂x1 . . . ∂xd
∂d-1
FP(x),
(4)
∂x1 . . . ∂xi-1∂xi+1 . . . ∂xd
for i = 1 . . . d.
D-i computes the (d - 1) high-order partial derivative excluding the variable i.
Our main result is presented in Theorem 2. Additional theoretical results are given in Appendix A.
All proofs are given in Appendix B.
Theorem 2 (Sobolev IPM). Assume that FP , and FQ and its d derivatives exist and are continuous:
FP and FQ ∈ Cd(X). Define the differential operator D- :
D- = (D-1,... D-d).
For X = (X1, . . . Xi-1, Xi, Xi+1, . . . Xd), let X-i = (X1, . . . Xi-1, Xi+1, . . . Xd).
The Sobolev IPM given in Equation (3) has the following equivalent forms:
1.	Sobolev IPM as comparison of high order partial derivatives of CDFs. The Sobolev IPM
has the following form:
S“(P, Q) = d UX
Pid=1(D-iFP(x) - D-iFQ(x))2
μ(x)
dx.
2.	Sobolev IPM as comparison of weighted (coordinate-wise) conditional CDFs. The Sobolev
IPM can be written in the following equivalent form:
1	d	PX-i (X-i)FP
sμ (p,q) = 1 Exi X (—	I-
(Xi) - QX-i(X-i)FQ[Xi|X-i=x-i] (Xi)
μ(x)
2
(5)
6
Published as a conference paper at ICLR 2018
3.	The optimal critic f * satisfies the following identity:
D r*/、	1	D-FQ(X) - D-FP(X)
Vxf (X) = ds;(pQ)-----------μ(X)---------，μ - almostsurely.	⑹
Sobolev IPM Approximation. Learning in the whole Sobolev space W01,2 is challenging hence we
need to restrict our function class to a hypothesis class H , such as neural networks. We assume
in the following that functions in H vanish on the boundary of X, and restrict the optimization
to the function space H . H can be a Reproducing Kernel Hilbert Space as in the MMD case or
parametrized by a neural network. Define the Sobolev IPM approximation in H :
SH,μ(P, Q)=	sup	(Ex〜Pf(X)- Ex〜Qf(χ)O	⑺
f∈H,kfkW01,2≤1
The following Lemma shows that the Sobolev IPM approximation in H is proportional to Sobolev
IPM. The tightness of the approximation of the Sobolev IPM is governed by the tightness of the
approximation of the optimal Sobolev Critic f * in H . This approximation is measured in the
Sobolev sense, using the Sobolev dot product.
Lemma 1 (Sobolev IPM Approximation in a Hypothesis Class). Let H be a function space with
functions vanishing at the boundary. For any f ∈ H and for f* the optimal critic in W01,2, we
have:
SH,μ(P, Q) = Sμ(P, Q)	SuP h {Vxf (x), Vxf *(x)>Rd μ(x)dx.
f ∈H ,kfkw1,2(X ,μ)≤1 JX
Note that this Lemma means that the Sobolev IPM is well approximated if the space H has an
enough representation power to express Vxf*(X). This is parallel to the Fisher IPM approximation
(Mroueh & Sercu, 2017) where it is shown that the Fisher IPM approximation error is proportional
to the critic approximation in the Lebesgue sense. Having in mind that the gradient of the critic is
the information that is passed on to the generator, we see that this convergence in the Sobolev sense
to the optimal critic is an important property for GAN training.
Relation to Fokker-Planck Diffusion. We show in Appendix A that the optimal Sobolev critic is
the solution of the following elliptic PDE (with zero boundary conditions):
P - Q
Sμ(P,Q)
-div(μ(x)Vxf(x)).
(8)
We further link the elliptic PDE given in Equation (8) and the Fokker-Planck diffusion. As we illus-
trate in Figure 2(b) the gradient of the critic defines a transportation plan for moving the distribution
mass from Q to P.
Discussion of Theorem 2. We make the following remarks on Theorem 2:
1.	From Theorem 2, we see that the Sobolev IPM compares d higher order partial derivatives of
the cumulative distributions FP and FQ, while Fisher IPM compares the probability density
functions.
2.	The dominant measure μ plays a similar role to Fisher:
sμ (P,Q)=d2 X Ej (H )：
the average distance is defined with respect to points sampled from μ.
7
Published as a conference paper at ICLR 2018
3.	Comparison of coordinate-wise Conditional CDFs.
(x1, . . . xi-1, xi+1, . . . xd). Note that we have:
We note in the following x-i
D-iFP(x)
∂d-1
∂x1 . . . ∂ xi-1 ∂ xi+1 . . . ∂xd
Z. ..	P(uι ... Ud)duι ... dud
∞	-∞
ZP(X1,..., Xi-ι, u, Xi+ι,..., xd)du
∞
xi
P[Xi|X-i=x-i](u|x1, . . . , xi-1, xi+1, . . .xd)du
∞
(Using Bayes rule)
= PX-i (x-i)FP[Xi|X-i=x-i] (xi),
Note that for each i, D-iFP(x) is the cumulative distribution of the variable Xi given the other
variables X-i = x-i, weighted by the density function of X-i at x-i. This leads us to the form
given in Equation 5.
We see that the Sobolev IPM compares for each dimension i the conditional cumulative distri-
bution of each variable given the other variables, weighted by their density function. We refer to
this as comparison of coordinate-wise CDFs on a leave one out basis. From this we see that we
are comparing CDFs, which are better behaved on discrete distributions. Moreover, the condi-
tioning built in to this metric will play a crucial role in comparing sequences as the conditioning
is important in this context (See section 6.1).
4.2 Illustrative Examples
Sobolev IPM / Cramer Distance and Wasserstein-I in one Dimension. In one dimension,
SoboleV IPM is the Cramer Distance (for μ uniform on X, We note this μ := 1). While SoboleV
IPM in one dimension measures the discrepancy between CDFs, the one dimensional Wasserstein-p
distance measures the discrepancy betWeen inVerse CDFs:
Sμ=ι(P,Q) = ^X(FP(X)- FQ(X))2dx versus Wp(P, Q) = ( ∣F-1(u) - F-1(u)∣pdu,
Recall also that the Fisher IPM for uniform μ is given by :
Fμ=ι(P, Q)= ( (P(x) - Q(x))2dx.
X
Consider for instance tWo point masses P = δa1 and Q = δa2 With a1, a2 ∈ R. The rationale behind
using Wasserstein distance for GAN training is that since it is a Weak metric, for far distributions
Wasserstein distance provides some signal (Arjovsky et al., 2017). In this case, it is easy to see that
WI(P, Q) = Sμ=ι = ∣αι - a2 |, while Fμ2.=ι(P, Q) = 2. As we see from this simple example,
CDF comparison is more suitable than PDF for comparing distributions on discrete spaces. See
Figure 1, for a further discussion of this effect in the GAN context.
Sobolev IPM between two 2D Gaussians. We consider P andQ tobe two dimensional Gaussians
with means μι and μ2 and covariances ∑ι and ∑2. Let (x, y) be the coordinates in 2D. We note FP
and Fq the CDFS of P and Q respectively. We consider in this example μ = p+2Q. We know from
Theorem 2 that the gradient of the Sobolev optimal critic is proportional to the following vector
field:
Vf*(χ,y)α-ɪ [dy((Fia)-FP(x,y))
μ(x,y) Ldx(FQ(X,y) — Fp(x,y)H
1.9 0.8
In Figure 2 we consider μι = [1,0], ∑ι = L g 1 3 μ2 = [1, -2], ∑2 =
(9)
1.9	-0.8
-0.8	1.3
In Figure 2(a) we plot the numerical solution of the PDE satisfied by the optimal Sobolev
critic given in Equation (8), using Matlab solver for elliptic PDEs (more accurately we solve
-div(μ(x)Vχf (x)) = P(X) — Q(x), hence we obtain the solution of Equation (8) UP to a nor-
malization constant (S (PQ))). We numerically solve the PDE on a rectangle with zero boundary
8
Published as a conference paper at ICLR 2018
(a) Smoothed discrete densities: PDF versus CDF of smoothed discrete densi-
ties with non overlapping supports.
(b) Smoothed Discrete and Continuous densities: PDF versus CDF of a
smoothed discrete density and a continuous density with non overlapping sup-
ports.
Figure 1: In the GAN context for example in text generation, we have to match a (smoothed) discrete
real distribution and a continuous generator. In this case, the CDF matching enabled by Sobolev
IPM gives non zero discrepancy between a (smoothed) discrete and a continuous density even if the
densities have disjoint supports. This ensures non vanishing gradients of the critic.
conditions. We see that the optimal Sobolev critic separates the two distributions well. In Figure
2(b) we then numerically compute the gradient of the optimal Sobolev critic on a 2D grid as given
in Equation 9 (using numerical evaluation of the CDF and finite difference for the evaluation of the
partial derivatives). We plot in Figure 2(b) the density functions ofP and Q as well as the vector field
of the gradient of the optimal Sobolev critic. As discussed in Section A.1, we see that the gradient
of the critic (wrt to the input), defines on the support of μ = P+2Q a transportation plan for moving
the distribution mass from Q to P.
5	Sobolev GAN
Now we turn to the problem of learning GANs with Sobolev IPM. Given the “real distribution”
Pr ∈ P (X), our goal is to learn a generator gθ : Z ⊂ Rnz → X, such that for Z 〜Pz, the
distribution of gθ(z) is close to the real data distribution Pr, where pz is a fixed distribution on Z
(for instance Z 〜N(0, Inz)). We note Qθ for the “fake distribution” of gθ(z), Z 〜Pz. Consider
{xi, i = 1... N}〜Pr, {zi, i = 1... N}〜N(0, Inz), and {xi, i = 1... N}〜μ. We consider
these choices for μ:
1.	μ = ■ i.eX 〜Pr or X = gθ(z),z 〜pz with equal probability 1.
2.	μGP is the implicit distribution defined by the interpolation lines between Pr and Qθ as
in (Gulrajani et al., 2017) i.e : X = UX +(1 — u)y,x 〜 Pr,y = gθ(z),z 〜 Pz and
U 〜Unif[0,1].
Sobolev GAN can be written as follows:	N	N
min	sup	E(fp,gθ) = ʊ Tfp(Xi)一守 ffp(gθ(Zi))
gθ fp, N pN=ιkVχfp(5i)k2 = l	i=1	i=1
ʌ
For any choice of the parametric function class Hp , note the constraint by Ωs(fp,gθ) =
N PN=IkVxfp(Xi)『.For example if μ = —, Ωs(fp,gθ)=备 PNLkVxfp(Xi)『十
9
Published as a conference paper at ICLR 2018
(a) Numerical solution of the PDE satisfied by the
optimal Sobolev critic.
(b) Optimal Sobolev Transport Vector Field
Vx f * (x) (arrows are the vector field Vx f * (x)
evaluated on the 2D grid. Magnitude of arrows was
rescaled for visualization.)
Figure 2: Numerical solution of the PDE satisfied by the optimal Sobolev critic and the transporta-
tion Plan induced by the gradient of Sobolev critic. The gradient of the critic (wrt to the input),
defines on the support of μ = P+2Q a transportation plan for moving the distribution mass from Q
to P. For a theoretical analysis of this transportation plan and its relation to Fokker-Planck diffusion
the reader is invited to check Appendix A.
2N PN=Ik Vxfp(gθ(Zi))k2. Note that, since the optimal theoretical critic is achieved on the sphere,
we impose a sphere constraint rather than a ball constraint. Similar to (Mroueh & Sercu, 2017) we
define the Augmented Lagrangian corresponding to Sobolev GAN objective and constraint
ρ
LS (P, θ, λ) = E(fp, gθ ) + λ(1 - ω S (fp, gθ )) - 2(Q S (fp, gθ ) - I)	(10)
where λ is the Lagrange multiplier and ρ > 0 is the quadratic penalty weight. We alternate between
optimizing the critic and the generator. We impose the constraint when training the critic only. Given
θ, we solve maxp minλ LS (p, θ, λ), for training the critic. Then given the critic parameters p we
ʌ
optimize the generator weights θ to minimize the objective minθ E(fp, gθ). See Algorithm 1.
Algorithm 1 Sobolev GAN
Input: P penalty weight, η Learning rate, nc number of iterations for training the critic, N batch
size
Initialize p, θ, λ = 0
repeat
for j = 1 to nc do
Sample a minibatch Xi, i = 1... N, xi 〜Pr
Sample a minibatch Zi, i = 1 …N, zi 〜Pz
(gp ,gλ) 一 (Vp LS, Vλ LS )(p,θ,λ)
P — P + η ADAM (p, gp)
λ 一 λ 一 ρgλ {SGD rule on λ with learning rate ρ}
end for
Sample Zi, i = 1 …N, zi 〜Pz
dθ — vθ E(fp, gθ ) = -vθN PN=Ifp (gθ (Zi))
θ 一 θ — η ADAM (θ,dθ)
until θ converges
Remark 1. Note that in Algorithm 1, we obtain a biased estimate since we are using same samples
for the cost function and the constraint, but the incurred bias can be shown to be small and vanishing
as the number of samples increases as shown and justified in (Shivaswamy & Jebara, 2010).
10
Published as a conference paper at ICLR 2018
Relation to WGAN-GP. WGAN-GP can be written as follows:
min sup
gθ f,kVχ fp(Xi)k = 1,Xi~μGP
1N	1N
E(fp,gθ) = N 工 fp(xi) - N '=2 fp(gθ(Zi))
The main difference between WGAN-GP and our setting, is that WGAN-GP enforces pointwise con-
StraintS on points drawn from μ = μGP Viaa point-wise quadratic penalty (E(fp, gθ) - λ PN=I(I -
∣∣Vχf (Xi)II )2) while We enforce that constraint on average as a SoboleV norm, allowing Us the Co-
ordinate weighted conditional CDF interpretation of the IPM.
6	Applications of Sobolev GAN
SoboleV IPM has two important properties; The first stems from the conditioning built in to the
metric through the weighted conditional CDF interpretation. The second stems from the diffuSion
properties that the critic of SoboleV IPM satisfies (Appendix A) that has theoretical and practical ties
to the Laplacian regularizer and diffusion on manifolds used in semi-superVised learning (Belkin
et al., 2006).
In this Section, we exploit those two important properties in two applications of SoboleV GAN: Text
generation and semi-superVised learning. First in text generation, which can be seen as a discrete
sequence generation, SoboleV GAN (and WGAN-GP) enable training GANs without need to do
explicit brute-force conditioning. We attribute this to the built-in conditioning in SoboleV IPM (for
the sequence aspect) and to the CDF matching (for the discrete aspect). Secondly using GANs in
semi-superVised learning is a promising aVenue for learning using unlabeled data. We show that a
Variant of SoboleV GAN can achieVe strong SSL results on the CIFAR-10 dataset, without the need
of any form of actiVation normalization in the networks or any extra ad hoc tricks.
6.1	Text Generation with Sobolev GAN
In this Section, we present an empirical study of SoboleV GAN in character leVel text generation.
Our empirical study on end to end training of character-leVel GAN for text generation is articulated
on four dimensions (loss, critic, generator, μ). (1) the loss used (GP: WGAN-GP (Gulrajani et al.,
2017), S: SoboleV orF: Fisher) (2) the architecture of the critic (Resnets or RNN) (3) the architecture
of the generator (Resnets or RNN or RNN with curriculum learning) (4) the sampling distribution μ
in the constraint.
Text Generation Experiments. We train a character-leVel GAN on Google Billion Word dataset and
follow the same experimental setup used in (Gulrajani et al., 2017). The generated sequence length
is 32 and the eValuation is based on Jensen-Shannon diVergence on empirical 4-gram probabilities
(JS-4) of Validation data and generated data. JS-4 may not be an ideal eValuation criteria, but it
is a reasonable metric for current character-leVel GAN results, which is still far from generating
meaningful sentences.
Annealed Smoothing of discrete Pr in the constraint μ. Since the generator distribution will
always be defined on a continuous space, we can replace the discrete “real” distribution Pr with a
smoothed Version (Gaussian kernel smoothing) Pr ? N (0, σ2Id). This corresponds to doing the
following sampling for Pr : X + ξ,x 〜Pr, and ξ 〜N(0, σ2Id). Note that we only inject noise to
the “real” distribution with the goal of smoothing the support of the discrete distribution, as opposed
to instance noise on both “real” and “fake” to stabilize the training, as introduced in (Kaae S0nderby
et al., 2017; ArjoVsky & Bottou, 2017). As it is common in optimization by continuation (Mobahi
& III, 2015), we also anneal the noise leVel σ as the training progresses on a linear schedule.
Sobolev GAN versus WGAN-GP with Resnets. In this setting, we compare (WGAN-
GP,G=Resnet,D=Resnet,μ = μGP) to (Sobolev,G=Resnet,D=Resnet,μ) where μ is one of: (1)
μGP, (2) the noise smoothed μs(σ) = Pr?N(0彳 Id)+Qθ or (3) noise smoothed with annealing
μa(σo) with σo the initial noise level. We use the same architectures of Resnet with 1D convolu-
tion for the critic and the generator as in (Gulrajani et al., 2017) (4 resnet blocks with hidden layer
size of 512). In order to implement the noise smoothing we transform the data into one-hot vec-
tors. Each one hot vector X is transformed to a probability vector p with 0.9 replacing the one and
0.1/(dictsize - 1) replacing the zero. We then sample from a Gaussian distribution N(0, σ2), and
11
Published as a conference paper at ICLR 2018
gβ iterations
(a) Comparing SoboleV With μGP and
WGAN-GP. The JS-4 are 0.3363 and
0.3302 respectiVely.
gθ iterations
(b) Comparing SoboleV with different μ
dominant measures and WGAN-GP. The JS-
4of μS(σo = 1.5) is 0.3268.
Figure 3: Result of Sobolev GAN for various dominating measure μ, for resnets as architectures of
the critic and the generator.
use softmax to normalize log p + . We use algorithm 1 for Sobolev GAN and fix the learning rate
to 10-4 and ρ to 10-5. The noise level σ was annealed following a linear schedule starting from an
initial noise level σ0 (at iteration i, σi = σ0(1 - Maxiter), Maxiter=30K). For WGAN-GP we used
the open source implementation with the penalty λ = 10 as in (Gulrajani et al., 2017). Results are
given in Figure 3(a) for the JS-4 evaluation of both WGAN-GP and Sobolev GAN for μ = μgp.
In Figure 3(b) we show the JS-4 evaluation of Sobolev GAN with the annealed noise smoothing
μa(σo), for various values of the initial noise level σ0. We see that the training succeeds in both
cases. Sobolev GAN achieves slightly better results than WGAN-GP for the annealing that starts
with high noise level σ0 = 1.5. We note that without smoothing and annealing i.e using μ = Pr +Qθ,
Sobolev GAN is behind. Annealed smoothing of Pr , helps the training as the real distribution is
slowly going from a continuous distribution to a discrete distribution. See Appendix C (Figure 6)
for a comparison between annealed and non annealed smoothing.
We give in Appendix C a comparison of WGAN-GP and Sobolev GAN for a Resnet generator
architecture and an RNN critic. The RNN has degraded performance due to optimization difficulties.
Fisher GAN Curriculum Conditioning versus Sobolev GAN: Explicit versus Implicit condi-
tioning. We analyze how Fisher GAN behaves under different architectures of generators and critics.
We first fix the generator to be ResNet. We study 3 different architectures of critics: ResNet, GRU
(we follow the experimental setup from (Press et al., 2017)), and hybrid ResNet+GRU (Reed et al.,
2016). We notice that RNN is unstable, we need to clip the gradient values of critics in [-0.5, 0.5],
and the gradient of the Lagrange multiplier λF to [-104, 104]. We fix ρF = 10-7 and we use
μ = μGp. We search the value for the learning rate in [10-5,10-4]. We see that for μ = μGP and
G = Resnet for various critic architectures, Fisher GAN fails at the task of text generation (Figure
4 a-c). Nevertheless, when using RNN critics (Fig 4 b, c) a marginal improvement happens over the
fully collapsed state when using a resnet critic (Fig 4 a). We hypothesize that RNN critics enable
some conditioning and factoring of the distribution, which is lacking in Fisher IPM.
Finally Figure 4 (d) shows the result of training with recurrent generator and critic. We follow (Press
et al., 2017) in terms of GRU architecture, but differ by using Fisher GAN rather than WGAN-GP.
We use μ = Pr+Qθ i.e. without annealed noise smoothing. We train (F, D=RNN,G=RNN,Pr+Qθ)
using curriculum conditioning of the generator for all lengths ` as done in (Press et al., 2017): the
generator is conditioned on 32 - ` characters and predicts the ` remaining characters. We increment
` = 1 to 32 on a regular schedule (every 15k updates). JS-4 is only computed when ` > 4. We see
in Figure 4 that under curriculum conditioning with recurrent critics and generators, the training of
Fisher GAN succeeds and reaches similar levels of Sobolev GAN (and WGAN-GP). Note that the
need of this explicit brute force conditioning for Fisher GAN, highlights the implicit conditioning
induced by Sobolev GAN via the gradient regularizer, without the need for curriculum conditioning.
12
Published as a conference paper at ICLR 2018
Figure 4: Fisher GAN with different architectures for critics: (a-c) We see that for μ = μGP and
G = Resnet for various critic architectures, Fisher GAN fails at the task of text generation. We
notice small improvements for RNN critics (b-c) due to the conditioning and factoring of the dis-
tribution. (d) Fisher GAN with recurrent generator and critic, trained on a curriculum conditioning
for increasing lengths `, increments indicated by gridlines. In this curriculum conditioning setup,
with recurrent critics and generators, the training of Fisher GAN succeeds and reaches similar levels
of Sobolev GAN (and WGAN-GP). It is important to note that by doing this explicit curriculum
conditioning for Fisher GAN, we highlight the implicit conditioning induced by Sobolev GAN, via
the gradient regularizer.
6.2	Semi-Supervised Learning with Sobolev GAN
A proper and promising framework for evaluating GANs consists in using it as a regularizer in the
semi-supervised learning setting (Salimans et al., 2016; Dumoulin et al., 2017; Kumar et al., 2017).
As mentioned before, the Sobolev norm as a regularizer for the Sobolev IPM draws connections with
the Laplacian regularization in manifold learning (Belkin et al., 2006). In the Laplacian framework
of semi-supervised learning, the classifier satisfies a smoothness constraint imposed by controlling
its SoboleV norm: ■ ∣Uf (x)k2 μ2 (χ)dχ (AlaoUi et al., 2016). In this Section, We present a variant
of Sobolev GAN that achieves competitive performance in semi-supervised learning on the CIFAR-
10 dataset Krizhevsky & Hinton (2009) without using any internal activation normalization in the
critic, such as batch normalization (BN) (Ioffe & Szegedy, 2015), layer normalization (LN) (Ba
et al., 2016), or weight normalization (Salimans & Kingma, 2016).
In this setting, a convolutional neural network Φω : X → Rm is shared between the cross entropy
(CE) training of a K-class classifier (S ∈ RK ×m) and the critic of GAN (See Figure 5). We have
the following training equations for the (critic + classifer) and the generator:
Critic + Classifier: max LD = LaGlAmN(f, gθ) - λCE	CE(p(y|x), y)	(11)
S,Φω,f
(x,y)∈lab
ʌ
Generator: max LG = E (f, gθ)	(12)
θ
where the main IPM objective with N samples: E(f, gθ) = N (Px∈unl f (x) — Pz〜pz f (gθ(z))).
Following (Mroueh & Sercu, 2017) we use the following “K + 1 parametrization” for the critic (See
Figure 5) :
K
f(x) = X p(y|x) hSy, Φω (x)i - hv, Φω(x)i
y=1	1	{z	}
1_	} f-:“fake” critic
f^^^^^^^^^^^^^{z^^^^^^^^^^^^}
f+ : “real” critic
Note that p(y|x) = Softmax(hS, Φω(x)i)y appears both in the critic formulation and in the Cross-
Entropy term in Equation (11). Intuitively this critic uses the K class directions of the classifier
Sy to define the “real” direction, which competes with another K+1th direction v that indicates fake
samples. This parametrization adapts the idea of (Salimans et al., 2016), which was formulated
specifically for the classic KL / JSD based GANs, to IPM-based GANs. We saw consistently better
results with the K + 1 formulation over the regular formulation where the classification layer S
13
Published as a conference paper at ICLR 2018
doesn’t interact with the critic direction v. We also note that when applying a gradient penalty based
constraint (either WGAN-GP or Sobolev) on the full critic f = f+ - f-, it is impossible for the
network to fit even the small labeled training set (underfitting), causing bad SSL performance. This
leads us to the formulation below, where we apply the Sobolev constraint only on f-. Throughout
this Section we fix μ = Pr+Qθ.
We propose the following two schemes for constraining the K+1 critic f(x) = f+(x) - f- (x):
1)	Fisher constraint on the critic: We restrict the critic to the following set:
f ∈ f =	f+	-f-,	ωF(f,gθ)	=	2N (X	f2(X)	+ X	f2(gθ(Z)))= ι}.
x∈unl	Z 〜Pz
This constraint translates to the following ALM objective in Equation (11):
LGmN(f, gθ)=E(f,gθ)+λF (I- ω f (f,gθ)) - 号(C f(f, gθ) -1)2,
where the Fisher constraint ensures the stability of the training through an implicit whitened mean
matching (Mroueh & Sercu, 2017).
2)	Fisher+Sobolev constraint: We impose 2 constraints on the critic: Fisher on f & Sobolev on f-
f ∈ {f = f+ - f-, ΩF(f,gθ) = 1 and ΩS(f-,gθ) = 1},
where ΩS(f-,gθ) =出(Pχ∈mιιkVχf-(χ)∣∣2 + Pz〜PzkVxf-(gθ(Z))『).
This constraint translates to the following ALM in Equation (11):
LGmN(f,gθ) = E(f,gθ) + λF(1 - ΩF(f,gθ)) + λs(I - ΩS(f-,gθ))
- PF(ωF(f,gθ) - 1)2 - PS(ωS(f-,gθ) - 1)2.
Note that the fisher constraint on f ensures the stability of the training, and the Sobolev constraints
on the “fake” critic f- enforces smoothness of the “fake” critic and thus the shared CNN Φω (x).
This is related to the classic Laplacian regularization in semi-supervised learning (Belkin et al.,
2006).
Table 2 shows results of SSL on CIFAR-10 comparing the two proposed formulations. Similar to
the standard procedure in other GAN papers, we do hyperparameter and model selection on the
validation set. We present baselines with a similar model architecture and leave out results with
significantly larger convnets. G and D architectures and hyperparameters are in Appendix D. Φω
is similar to (Salimans et al., 2016; Dumoulin et al., 2017; Mroueh & Sercu, 2017) in architecture,
but note that We do not use any batch, layer, or weight normalization yet obtain strong competitive
accuracies. We hypothesize that we don't need any normalization in the critic, because of the implicit
whitening of the feature maps introduced by the Fisher and Sobolev constraints as explained in
(Mroueh & Sercu, 2017).
_••••_ ffl
K ∖ 一
Φω
CNN
X
p(y∣χ)=
Softmax(hS, Φω (x)i)y
“real” critic
—Af+(X) = PK=I P(yIx) (Sy, φω(X)i
“fake” critic
—► f-(x) = hv, Φω(x)i
hv, Φω(x)i
→ f (x) = f+(x) - f-(x)
GAN critic
Figure 5: “K+1” parametrization of the critic for semi-supervised learning.
14
Published as a conference paper at ICLR 2018
Table 2: CIFAR-10 error rates for varying number of labeled samples in the training set. Mean and
standard deviation computed over 5 runs. We only use the K + 1 formulation of the critic. Note that
we achieve strong SSL performance without any additional tricks, and even though the critic does
not have any batch, layer or weight normalization. Baselines with * use either additional models
like PixelCNN, or do data augmentation (translations and flips), or use a much larger model, either
of which gives an advantage over our plain simple training method. f is the result We achieved in
our experimental setup under the same conditions but without “K+1” critic (see Appendix D), since
(Gulrajani et al., 2017) does not have SSL results.
Number of labeled examples Model	1000	2000 Misclassific	4000 ation rate	8000
CatGAN (Springenberg, 2015)			19.58	
FM (Salimans et al., 2016)	21.83 ± 2.01	19.61 ± 2.09	18.63 ± 2.32	17.72 ± 1.82
ALI (Dumoulin et al., 2017) Tangents Reg (Kumar et al., 2017) Π-model (Laine & Aila, 2016) * VAT (Miyato et al., 2017) Bad Gan (Dai et al., 2017) * VAT+EntMin+Large (Miyato et al., 2017) * Sajjadi (Sajjadi et al., 2016) *	19.98 ± 0.3 20.06 ± 0.5	19.09 ± 0.15	17.99 ± 0.54 16.78 ± 0.6 16.55 ± 0.29 14.87 14.41 ± 0.30 13.15 11.29	17.05 ± 0.50
WGAN-GP (Gulrajani et al., 2017) f	44.85 ± 0.28	37.62 ± 0.56	32.66 ± 0.48	30.38 ± 0.22
Fisher, layer norm (Mroueh & Sercu, 2017)	19.74 ± 0.21	17.87 ± 0.38	16.13 ± 0.53	14.81 ± 0.16
Fisher, no norm (Mroueh & Sercu, 2017)	21.49 ± 0.18	19.20 ± 0.46	17.30 ± 0.30	15.57 ± 0.33
Sobolev + Fisher, no norm (This Work)	20.14 ± 0.21	17.38 ± 0.10	15.77 ± 0.19	14.20 ± 0.08
7 Conclusion
We introduced the Sobolev IPM and showed that it amounts to a comparison between weighted
(coordinate-wise) CDFs. We presented an ALM algorithm for training Sobolev GAN. The intrinsic
conditioning implied by the Sobolev IPM explains the success of gradient regularization in Sobolev
GAN and WGAN-GP on discrete sequence data, and particularly in text generation. We highlighted
the important tradeoffs between the implicit conditioning introduced by the gradient regularizer in
Sobolev IPM, and the explicit conditioning of Fisher IPM via recurrent critics and generators in
conjunction with the curriculum conditioning. Both approaches succeed in text generation. We
showed that Sobolev GAN achieves competitive semi-supervised learning results without the need
of any normalization, thanks to the smoothness induced by the gradient regularizer. We think the
Sobolev IPM point of view will open the door for designing new regularizers that induce different
types of conditioning for general structured/discrete/graph data beyond sequences.
References
Ahmed El Alaoui, Xiang Cheng, Aaditya Ramdas, Martin J. Wainwright, and Michael I. Jordan.
Asymptotic behavior of p-based laplacian regularization in semi-supervised learning. CoRR,
abs/1603.00564, 2016.
Martin Arjovsky and Leon Bottou. Towards principled methods for training generative adversarial
networks. In ICLR, 2017.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein gan. ICML, 2017.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv:1607.06450,
2016.
Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A geometric frame-
work for learning from labeled and unlabeled examples. JMLR, 2006.
Marc G. Bellemare, Ivo Danihelka, Will Dabney, Shakir Mohamed, Balaji Lakshminarayanan,
Stephan Hoyer, and Remi Munos. The cramer distance as a solution to biased wasserstein gradi-
ents. CoRR, abs/1705.10743, 2017a.
15
Published as a conference paper at ICLR 2018
Marc G Bellemare, Ivo Danihelka, Will Dabney, Shakir Mohamed, Balaji Lakshminarayanan,
StePhan Hoyer, and Remi Munos. The Cramer distance as a solution to biased Wasserstein gradi-
ents. arXiv:1705.10743, 2017b.
Tong Che, Yanran Li, Ruixiang Zhang, Devon R Hjelm, Wenjie Li, Yangqiu Song, and
Yoshua Bengio. Maximum-likelihood augmented discrete generative adversarial netWorks.
arXiv:1702.07983, 2017.
KacPer ChWialkoWski, Heiko Strathmann, and Arthur Gretton. A kernel test of goodness of fit. In
ICML 2016, 2016.
Zihang Dai, Zhilin Yang, Fan Yang, William W Cohen, and Ruslan Salakhutdinov. Good semi-
suPervised learning that requires a bad gan. arXiv:1705.09783, 2017.
Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Alex Lamb, Martin Arjovsky, Olivier MastroPi-
etro, and Aaron Courville. Adversarially learned inference. ICLR, 2017.
Gintare Karolina Dziugaite, Daniel M. Roy, and Zoubin Ghahramani. Training generative neural
netWorks via maximum mean discrePancy oPtimization. In UAI, 2015.
I.	Ekeland and T. Turnbull. Infinite-dimensional Optimization and Convexity. The University of
Chicago Press, 1983.
A. Genevay, G. Peyre, and M. Cuturi. Learning generative models with sinkhorn divergences.
PrePrint 1706.00292, Arxiv, 2017. URL https://arxiv.org/abs/1706.00292.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.
Jackson Gorham and Lester W. Mackey. Measuring samPle quality with stein’s method. In NIPS,
pp. 226-234, 2015.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scholkopf, and Alexander Smola.
A kernel two-sample test. JMLR, 2012.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Im-
proved training of wasserstein gans. arXiv:1704.00028, 2017.
R. Devon Hjelm, Athul Paul Jacob, Tong Che, Kyunghyun Cho, and Yoshua Bengio. Boundary-
seeking generative adversarial networks. arXiv:1702.08431, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. Proc. ICML, 2015.
Casper Kaae S0nderby, Jose Caballero, Lucas Theis, Wenzhe Shi, and Ferenc Huszar. Amortised
map inference for image super-resolution. ICLR, 2017.
A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Master’s
thesis, 2009.
Abhishek Kumar, Prasanna Sattigeri, and P Thomas Fletcher. Improved semi-supervised learning
with gans using manifold invariances. NIPS, 2017.
Matt J. Kusner and Jose Miguel Hernandez-Lobato. Gans for sequences of discrete elements with
the gumbel-softmax distribution. arXiv:1611.04051, 2016.
Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning.
arXiv:1610.02242, 2016.
Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and BarnabaS P6czos. MMD GAN:
towards deeper understanding of moment matching network. NIPS, abs/1705.08584, 2017.
Yujia Li, Kevin Swersky, and Richard Zemel. Generative moment matching networks. In ICML,
2015.
16
Published as a conference paper at ICLR 2018
Qiang Liu. Stein variational descent as a gradient flow. NIPS, 2017.
Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference
algorithm. In Advances in Neural Information Processing Systems 29. 2016.
Qiang Liu, Jason D. Lee, and Michael I. Jordan. A kernelized stein discrepancy for goodness-of-fit
tests. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016,
New York City, NY, USA, June 19-24, 2016, 2016.
Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, and Zhen Wang. Least squares generative
adversarial networks. arXiv:1611.04076 ICCV, 2017.
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a
regularization method for supervised and semi-supervised learning. arXiv:1704.03976, 2017.
Hossein Mobahi and John W. Fisher III. A Theoretical Analysis of Optimization by Gaussian Con-
tinuation. In Proc. of 29th Conf. Artificial Intelligence (AAAI’15), 2015.
Youssef Mroueh and Tom Sercu. Fisher gan. arXiv:1705.09675 NIPS, 2017.
Youssef Mroueh, Tom Sercu, and Vaibhava Goel. Mcgan: Mean and covariance feature matching
gan. arXiv:1702.08398 ICML, 2017.
Alfred Muller. Integral probability metrics and their generating classes of functions. Advances in
Applied Probability, 1997.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers
using variational divergence minimization. In NIPS, 2016.
Chris J. Oates, Mark Girolami, and Nicolas Chopin. Control functionals for monte carlo integration.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 2017.
Ofir Press, Amir Bar, Ben Bogin, Jonathan Berant, and Lior Wolf. Language generation with recur-
rent generative adversarial networks without pre-training. arXiv:1706.01399, 2017.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv:1511.06434, 2015.
Sai Rajeswar, Sandeep Subramanian, Francis Dutil, Christopher Pal, and Aaron Courville. Adver-
sarial generation of natural language. arXiv:1705.10929, 2017.
Scott E Reed, Zeynep Akata, Santosh Mohan, Samuel Tenka, Bernt Schiele, and Honglak Lee.
Learning what and where to draw. In Advances In Neural Information Processing Systems, pp.
217-225, 2016.
Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transfor-
mations and perturbations for deep semi-supervised learning. In Advances in Neural Information
Processing Systems, pp. 1163-1171, 2016.
Tim Salimans and Diederik P Kingma. Weight normalization: A simple reparameterization to accel-
erate training of deep neural networks. In Advances in Neural Information Processing Systems,
pp. 901-901, 2016.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. NIPS, 2016.
Pannagadatta K. Shivaswamy and Tony Jebara. Maximum relative margin and data-dependent reg-
ularization. Journal of Machine Learning Research, 11:747-788, 2010. doi: 10.1145/1756006.
1756031. URL http://doi.acm.org/10.1145/1756006.1756031.
Jost Tobias Springenberg. Unsupervised and semi-supervised learning with categorical generative
adversarial networks. arXiv:1511.06390, 2015.
Bharath K. Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Scholkopf, and Gert R. G.
Lanckriet. On integral probability metrics, φ-divergences and binary classification. 2009.
17
Published as a conference paper at ICLR 2018
Bharath K. SriPerUmbUdur, Kenji FUkUmizu, Arthur Gretton, Bernhard SchOlkopf, and Gert R. G.
Lanckriet. On the empirical estimation of integral probability metrics. Electronic Journal of
Statistics, 2012.
Dilin Wang and Qiang LiU. Learning to draw samples: With application to amortized MLE for
generative adversarial learning. CoRR, abs/1611.01722, 2016.
Lantao YU, Weinan Zhang, JUn Wang, and Yong YU. Seqgan: SeqUence generative adversarial nets
with policy gradient. CoRR, abs/1609.05473, 2016.
JUnbo Jake Zhao, Yoon Kim, Kelly Zhang, Alexander M. RUsh, and Yann LeCUn. Adversarially
regUlarized aUtoencoders for generating discrete strUctUres. CoRR, 2017.
18
Published as a conference paper at ICLR 2018
A Theory: Approximation and Transport Interpretation
In this Section we present the theoretical properties of Sobolev IPM and how it relates to distributions
transport theory and other known metrics between distributions, notably the Stein distance.
A.1 Distribution Transport Perspective on Sobolev IPM
In this Section, we characterize the optimal critic of the Sobolev IPM as a solution of a non linear
PDE. The solution of the variational problem of the Sobolev IPM satisfies a non linear PDE that can
be derived using standard tools from calculus of variations (Ekeland & Turnbull, 1983; Alaoui et al.,
2016).
Theorem 3	(PDE satisfied by the SoboleV Critic). The optimal Critic of Sobolev IPM f satisfies
the following PDE:
∆f *(x) + (Vχ logμ(x), Vxf *(x)i +
P(X) — Q(X)
Sμ(P,Q)μ(x)
0.
(13)
Define the Stein Operator: T(μ)~(x) = 1 (〈Vx log(μ(x)), ~(x)i + div(~(x))). Hence We have the
following Transport Equation of P to Q:
Q(X) = P(X) + 2Sμ(P, Q)μ(x)T(μ)Vxf *(x).
Recall the definition of Stein Discrepancy :
S(Q,μ) = SuP |Ex〜Q [T(μ)~(χ)]∣,~: X → Rd.
~g
Theorem 4	(Sobolev and Stein Discrepanices). The following inequality holds true:
≤ 2	S(Q,μ)	Sμ(P,Q)	(14)
|{z}	|---{---}
Stein Good fitness of the model Q w.r.t to μ Sobolev Distance
Q(X) — P(X)
μ(x)
Consider for example μ = P, and sequence Qn. If the Sobolev distance goes Sp(P, Qn) → 0, the
Qn(x)
P(x)
ratio rn(X)
converges in expectation (W.r.t to Q) to 1. The speed of the convergence is
given by the Stein Discrepancy S(Qn, P).
Relation to Fokker-Planck Diffusion Equation and Particles dynamics. Note that PDE
satisifed by the Sobolev critic given in Equation (13) can be equivalently Written:
P—Q
S (P Q) = -div(μ(X)Vxf (X)),	(15)
Written in this form, We draW a connection With the Fokker-Planck Equation for the evolution of a
density function qt that is the density of particles Xt ∈ Rd evolving With a drift (a velocity field)
V (X, t) : X × [0, ∞[→ Rd:
dXt = V (Xt, t)dt, Where the density ofX0 is given by q0(X) = Q(X),
The Fokker-Planck Equation states that the evolution of the particles density qt satisfies:
ddt (x) = —div(qt(X)V (X,t))	(16)
Comparing Equation (15) and Equation (16), We identify then the gradient of Sobolev critic as a
drift. This suggests that one can define “Sobolev descent” as the evolution of particles along the
gradient floW:
dXt = Vxft (Xt)dt, where the density of Xo is given by ⅛o(x) = Q(x),
where ft is the Sobolev critic between qt and P. One can show that the limit distribution of the
particles is P. The analysis of “Sobolev descent” and its relation to Stein Descent (Liu & Wang,
2016; Liu, 2017) is beyond the scope of this paper and will be studied in a separate work. Hence
we see that the gradient of the Sobolev critic defines a transportation plan to move particles whose
distribution is Q to particles whose distribution is P (See Figure 2). This highlights the role of the
gradient of the critic in the context of GAN training in term of transporting the distribution of the
generator to the real distribution.
19
Published as a conference paper at ICLR 2018
B Proofs
Proof of Theorem 2. Let FP and FQ , be the cumulative distribution functions of P and Q respec-
tively. We have:
∂d
P(X)= ∂xι ...∂xdFP(X)，
(17)
We note D = ∂xιdd∂xd , and D-i = ∂xι...∂Xid1∂Xi+ι...∂xd , for i = 1 ...d.
D -i computes the d - 1 partial derivative excluding the variable i.
In the following we assume that FP , and FQ and its d derivatives exist and are continuous meaning
that FP and FQ ∈ Cd(X). The objective function in Equation (3) can be written as follows:
Ex〜Pf(X)- Ex〜Qf(x) = / f(x)D(FP(x) - FQ(X)) dx
∂
=I f(x)∂χiD (FP(X)- FQ(X))dx
(for any i, since FP and FQ ∈ Cd(X))
=-j D-(F '(Fp(x) - FQ(X))dX
X ∂Xi
(f vanishes at the boundary in W1,2(X, μ))
Let D- = (D-1, . . . , D-d) it follows that:
Ex〜Pf (X)- Ex〜Qf(X)=5 X [	^D-Il(FQ(X)- FP(X))dX
d i=1 X ∂Xi
=d JX (yxf (X), D-(FQ(X)- FP(X))〉Rd dX
(18)
Let Us define L2(X,μ)0d the space of measurable functions from X → Rd. For g,h ∈
L2(X, μ严 the dot product is defined as follows:
hg, hiL2(X,μ)0d = X hg(X), h(X)iRd 〃(X)dX
and the norm is given :
kgkL2(X μ)回d = I kgkRd μ(X)dX.
X
We can write the objective in Equation (18) in term of the dot product in L2(X,μ严：
Ex 〜Pf(X)
-Ex〜Qf(X)
1
d
D-(FQ - FP) \
μ	L L2(x,μ)名 d
(19)
On the other hand the constraint in Equation (3) can be written in terms of the norm in L2(X, μ产d:
kfkW01,2(χ,μ) = INxf kL2(χ,μ)馋 d
(20)
20
Published as a conference paper at ICLR 2018
Replacing the objective and constraint given in Equations (19) and (20) in Equation (3), we obtain:
S (P, Q) = 1 sup
f,kVxf kL2(χ ,μ)∙d ≤1
D-(FQ - FP)
L2(X ,μ誉 d
μ
1
s	Sup
d g∈L2(X,μ)0d,kgkL2(χ μ)Θd≤1
D-(FQ - FP)
g,---------
L2(X ,μ)㊁ d
μ
1	D-(FQ — FP)
d	μ	L2(X ,μ)^d
/
Bydefinitionofk.∣∣l2(x,*)加,g*
D-FQ(X) 一 D-FP(X)	1
μ(x)	Il D-(FQ-FP) 11
Il μ	l∣L2(X ,μ)的 d
1 / ∣∣D-Fq(x)- D-FP(X)『dχ
d∖∣ Jx	μ(X)
Hence We find also that the optimal critic f * satisfies:
Vxf *(x)
D-FQ(x) 一 D-FP(X)	1
μ(x)	Il D-(FQ-FP) 11
Il μ	l∣L2(X ,μ)回 d
□
Proof of Lemma 1.

d JX (Vχf (x), D-(FQ(X)- FP(X))〉Rd dX
S (PQ) / ∕v f( ) D-(FQ(X)- FP(X))
μ( , Q) 7χVxf ( ),	μ(X)dSμ(P, Q)
S”(P, Q) / hVχf (x), Vχf *(Xy) μ(X)dX
X
Sμ(P,Q) hf,f*iw01.2
μ(X)dX
Rd
Hence We have:
sup	Ex〜Pf(X)- Ex〜Qf(X) = Sμ(P, Q)	sup	hf, f*)w 1,2 ,
f∈H ,kfkW01,2≤1	f∈H ,kfkW01,2≤1	0
It folloWs therefore that:
SH (P, Q)= Sμ(P, Q)	Sup	hf,f*iw 1,2
f∈H,kfkW01,2≤1	0
□
We conclude that the Sobolev IPM can be approximated in arbitrary space as long as it has enough
capacity to approximate the optimal critic. Interestingly the approximation error is measured noW
With the Sobolev semi-norm, While in Fisher it Was measured With the Lebesgue norm. Approx-
imations With Sobolev Semi-norms are stronger then Lebesgue norms as given by the Poincare
inequality (||f ||L2 ≤ C kfkW1,2), meaning if the error goes to zero in Sobolev sense it also goes to
zero in the Lebesgue sense , but the converse is not true.
Proof of Theorem 3. The proof folloWs similar arguments in the proofs of the analysis of Laplacian
regularization in semi-supervised learning studied by (Alaoui et al., 2016).
Sμ (P,Q)	= SuPf ∈W1,2 {Ex〜P [f (x)] - Ex〜Q [f (x)]}
Ex〜μkVf(X)k2 ≤ 1,
s.t.
(21)
21
Published as a conference paper at ICLR 2018
Note that this problem is convex in f (Ekeland & Turnbull, 1983). Writing the lagrangian for
equation (21) we get :
L(f, λ) = Ex〜P [f (X)] - Ex〜Q [f (X)] +2 (1 — Ex〜μlNxf (X)k2)
=X f(X)(P(X)- Q(X)) dχ + 2 (1- (IMf (X)k2μ(X)dχ)
=/ f(x) μι(X) dX + 2 (1-/ l∣vxf(x)∣2 μ(X) ”x)
We denote (P(X) - Q(X)) as μι (x).To get the optimal f, We need to apply KKT conditions on the
above equation.
L(f, λ) = X f (X) μι(X) dX + 2 (1 -L kvxf (X)k2 μ(X) dX
From the calculus of variations:
L(f + eh, λ) = / (f + eh)(X) μι(X) dX + 2 (1 -/ ∣∣Vx(f + eh)(X)∣∣2 μ(X) Qx)
=/ (f (x) + eh(x)) μι(x) dx + λ (1 - /〈Vx(f + eh)(x), Vx(f + eh)(x)) μ(x)
=	(f (x) + eh(x)) μι(x) dx
X
+ 2 (1 - lχ [∣Vχf (x)∣2 + 2ehVxf (x), Vxh(X))+ O(e2)] μ(x)dx)
=L(f, λ) + e / h(x) μι(x) dx — λe / hVxf (x), Vxh(X)〉μ(x) dx + O(e2)
=L(f, λ) + e [ / h(x) μι(x) dx - λ / hVxf (x), Vxh(X)〉μ(x) dx] + O(e2)
NoW We apply integration by part and set h to be zero at boundary as in (Alaoui et al., 2016). We
get :
/ hVxf (x), Vxh(x)i μ(x) dx = /(Vxf (x) μ(x), Vxh(X)〉dx
=，h(x)μ(x)Vxf (x).n(x)dS(x) -/ div(μ(x)Vxf (x)) h(x) dx
∂X	X
=—	div(μ(x)Vxf (x)) h(x) dx
X
Hence,
L(f+eh,λ)=L(f,λ)
μi(x) h(x) dx + λ
div(μ(x)Vxf(x)) h(x)
dxi + O(e2)
X
X
L(f, λ) + e J (μι(x)+ λ div(μ(x)Vxf (x))
h(x) dx + O(e2)
The functional derivative of L(f, λ), at any test function h vanishing on the boundary:
/ ∂L≡(χ)h(χ)dχ = lim L(f + …f
X ∂f	→0	e
= / (μι(x) + λ div(μ(χ)Vxf(x))) h(χ) dχ
22
Published as a conference paper at ICLR 2018
Hence We have:
aLf ')(x) = μι(x) + λ div(μ(x)Vχf (x))
For the optimal f *,λ* first order optimality condition gives us:
μι(x) + λ* div(μ(x)Vχf*(x)) = 0
and
/ INxf* (x)『 μ(x)dx = 1
X
Note that (See for example (Alaoui et al., 2016)):
div(μ(x)%χf*(x)) = μ(x)∆2f *(x) + (Vxμ(x), Vxf *(x)i,
(22)
(23)
since div(Vχf *(x)) = ∆2f *(x). Hence from equation (22)
μι(x) + λ* div (μ(x)Vχf*(x)) = 0
⇒ μi(x) + λ* (μ(x)∆2f* (x) + hVχμ(x), Vχf *(x)i) = 0
⇒ μi(x) + λ* μ(x)∆2f*(x) + λ*<Vχμ(x), Vχf *(x)i = 0
一 八 f*	vχM(x) v7 ,*	μι(x)
⇒ Af (X) + ∖	, VXf (x)/ + λ*μρ=0
⇒ ∆2f *(x) + hVχ log4(x), Vχf *(x)i + P(X)J Q(X) = 0
λ*μ(x)
(24)
Hence f *, λ* satisfies:
∆2f * (x) + hVχ logμ(x), Vχf *(x)i + P(X)J Q(X) = 0	(25)
λ*μ(x)
and
k ∣Vχf *(x)『 μ(x)dx = 1.	(26)
X
Let us verify that the optimal critic as found in the geometric definition (Theorem 2) of Sobolev IPM
that satisfies:
▽	∂f*(X)	D-iFQ(x) - DTFP(X)
Vf(X) = Fr =-----------------λ⅞-7(x)--------- Vi ∈ [d],	(27)
satisfies indeed the PDE.
From equation (27), we want to compute df (X) for all i:
i
∂2f (x)	1 μ(x) [∂1"(D iFQ(X)- D iFP(X))] - [D iFQ(x) - D iFP(x)]viμ(X)
--:     -------------------------------------—:--------------------------------
∂x2	λ*d	μ2(x)
_ 1	μ(x)[Q(x) - P(x)] - [D-iFQ(x) - D-iFp(x)] Viμ(X)
=-------------------------------~~~：----------------------
λ*d	μ2 (x)
Q(X) - P(x)
λ*d μ(x)
Viμ(x)
μ(x)
Vif *(x)

Hence,
∂2f(x) + Viμ(x) v f( ) + (P(X) - Q(X))
∂x2	μ(x)	i	λ*d μ(x)
(28)
Adding equation (28) for all i ∈ [d], we get :
d
X
i=1
+ VgX2 vi f (X) +
λ* d μ(x)
0
23
Published as a conference paper at ICLR 2018
As a result, the solution f * of the partial differential equation given in equation (25) satisfies the
following :
∂f* (x) = D-iFQ(x) - DTFP(X)
∂xi	λ*d μ(x)
∀i ∈ [d]
Using the constraint in (26) we can get the value of λ* :
k ∣∣Vf*(x)k2 μ(x) dx = 1
⇒ / X(fx )2 "(x) dx =1
i=1
⇒λ*
1 X / (D-iFQ(X)- D-iFP(x))2
d t i=l∙∕	μ(X)
dx = Sμ(P,Q).
Proof of Theorem 4. Define the Stein operator (Oates et al., 2017):
T (μ)[Vχf (X)]
2 hVχf (x), Vxlogμ(x)i + 1 (Vχ, Vxf(X))
2 hVxf(x), Vxlogμ(x)i + l∆2f (x).
This operator was later used in defining the Stein discrepancy (Gorham & Mackey, 2015; Liu et al.,
2016; Chwialkowski et al., 2016; Liu, 2017).
Recall that Barbour generator theory provides us a way of constructing such operators that produce
mean zero function under μ. It is easy to verify that:
Ex 〜μT (μ)Vxf(x) = 0.
Recall that this operator arises from the overdamped Langevin diffusion, defined by the stochastic
differential equation:
dxt = 2 Vx log μ(xt) + dWt
where (Wt)t≥0 is a Wiener process. This is related to plug and play networks for generating
samples if the distribution is known, using the stochastic differential equation.
From Theorem 3, it is easy to see that the PDE the Sobolev Critic (f*, λ* = Sμ(P, Q)) can be
written in term of Stein Operator as follows:
T (μ)[Vxf *](x)
1 Q(X) — P(X)
2λ*	μ(∕)
□
Taking absolute values and the expectation with respect to Q:
|Ex~Q[T(〃)Vxf*(X)]1 = 2sμ⅛Qy Ex~Q
Recall that the definition of Stein Discrepancy :
S(Q,μ) =	sup	|Ex 〜Q [T (μ)~(X)]∣
~∈L2(X,μ)M
It follows that Sobolev IPM critic satisfies:
|Ex〜Q [T(μ)Vxf*(x)]∣≤ S(Q,μ),
Hence we have the following inequality:
24
Published as a conference paper at ICLR 2018
Q(X) — P(X)
μ(x)
≤ S(Q,μ)
This is equivalent to:
Ex [Q(x) — P(X)]	≤ 2	s(Q,μ)	Sμ(P,Q)
L	μ(X)	J	|一｛一｝	1-z一｝
Stein Good fitness of the model Q w.r.t to μ SoboleV Distance
Similarly we obtain:
Q(X) — P(X)
μ(x)
≤ 2 S(P,μ)
1"zz-｝
Stein Good fitness of μ w.r.t to P
Sμ(P,Q)
|---V----｝
SoboleV Distance
For instance consider μ = P, we have therefore:
1
2
^Q(x)^
_P(x)_
— 1 ≤ S(Q, P)SP(P, Q).
Note that the left hand side of the inequality is not the total variation distance.
Hence for a sequence Qn if the Sobolev distance goes SP(P, Qn) → 0, the ratio rn(X)
Qn(x)
P(x)
converges in expectation (w.r.t to Q) to 1. The speed of the convergence is given by the Stein
Discrepancy S(Qn, P).
One important observation here is that convergence of PDF ratio is weaker than the conditional
CDF as given by the Sobolev distance and of the good fitness of score function as given by Stein
discrepancy.
C Text experiments: Additional Plots
Comparison of annealed versus non annealed smoothing of Pr in Sobolev GAN.
0.65
0.60
0.55
平 0.50
uɔ
l^,0.45
0.40
0.35
0	5000	10000 15000 20000 25000 30000
[S, D=res
(S, D=res
(S, D=res
(S, D=res
(S, D=res
(S, D=res
G=res
G=res
G=res
G=res
G=res
G=res
4.9=0.1))
μj(σo=0.l))
μa(σ = 1.0))
μj(σo = 1-0))
μ,(σ=L5))
μl(σo = 1-5))
Figure 6: Comparison of annealed versus non annealed smoothing of Pr in Sobolev GAN. We see
that annealed smoothing outperforms the non annealed smoothing experiments.
Sobolev GAN versus WGAN-GP with RNN. We fix the generator architecture to Resnets. The
experiments of using RNN (GRU) as the critic architecture for WGAN-GP and Sobolev is shown
in Figure 7 where we used μ = μGP for both cases. We only apply gradient clipping to stabilize
the performance without other tricks. We can observe that using RNN degrades the performance.
We think that this is due to an optimization issue and a difficulty in training RNN under the GAN
objective without any pre-training or conditioning.
□
25
Published as a conference paper at ICLR 2018
(a) WGAN-GP
(b) Sobolev
Figure 7:	Result of WGAN-GP and Sobolev with RNNs.
That time out very came of their
But it Gaylen Was strosd of the
The Case had CaUrgr thing it las
GrOpate evong hould exficioul Pa
The See qust , so make starter S
Cauntsrs of Oprnnd accused there
Conpara Tizo is thene ano hastin
With Earaie Ipptaring Very woutd
When livht think not Braoph SPec
The phan teiled " Policy , tor e
Coydey GN11 ) -s pail is uniled
That ' s conpect d larce antin-iu
But it ' s familions a, IHican er
Nit was bad a year hitoloy hodat
And prenches gless fram Avers aa
If ' s might , comp-rime at overg
Jeads years lead of gonguied to
Asong he into get his ressson '
Nou projecti y bated with te de
CradfsCUel sad out the Gutoor
(WGAN-GP,D=res,G=res,μGP)
The Loraia arnup to Nou ands in
Nany tecalliexpeace in that veel
" It not has allown ourn Ehough
This bastly , suphoriation almo
"The pasts of a nuπmers said Nh
A loved the Cam feal switht with
Apenole ' s no. on walling any Cc
Furaymand chainting suppinally s
Larts Ginis , R-Ra tarkedment st
It what wowed night a chiols as
Overy shy really ——"Cyphil mad
She wore will be also a marged W
But Rere tained the sian hoy at
Hends to Won)).——u2- 2y this he
Felecter indoadoy is ne rtlayne
Pet isser juastivus also first i
But you had not of hiscered tnd
Thoir Taray taked an intervatter
She vagger conmisurespis herkied
His juestor foy not ar oreeoon t
Buile president up thepunsit an
In ealled osficers in a rould a
The mome of tot of not shodld ye
It ' snsacopprctionialso muss y ,
In reperted a "aMametan 's Gegtn
Sime Vmone onerge recighed a an
Rechardty ) " Gush 's Wolnes it l
Catious paice of an sepurying or
The vews dolerated badds to appe
Orgarda of to the cheek-ng nees
You all tnl torgave takely his e
"Ancrops than , Mumine of the s
"The Bontement is shouts will b
One if the rops of the Cutlent h
While paliless streaghal dustist
EVeryial with a Ecenbers are car
It has an atton<ent ligges of ha
Hςwnton , ∣- one in aroed that I
Anmithingly country cestents toa
The odtitians exptolises , began
The may last stoct was anso stad
But the antinf moted Chapiinabie
The saysuthat yearthand on the d
Even the giime was shopld on pist
(Sobolev GAN,D=res,G=res,μGρ)
If you ad someone bidding at a t
ITaS t ian at train , who is a m
"Be pls sahs this car and , you
I can reminere several wok sine
" I tihne the animal soun like a
No , I hsn hen am afra i tak th
Wel , maybe the a lost good tal
Binan and I han an met is to boo
Since tins the the and shime bro
I tihne thn aiimar thin you wasn
(SObOleV GAN,D=res,G=res,μa(σo = 1.5))
(Fisher GANQ=GRU,G=GRU+curr,ρr+Qθ)
Figure 8:	Text samples from various GANs considered in this paper.
D SSL: hyperparameters and architecture
For our SSL experiments on CIFAR-10, we use Adam with learning rate η = 2e-4, β1 = 0.5
and β2 = 0.999, both for critic f (without BN) and Generator (with BN). We selected λCE = 1.5
from [0.8, 1.5, 3.0, 5.0]. We train all models for 350 epochs. We used some L2 weight decay: 1e-6
on ω, S (i.e. all layers except last) and 1e-3 weight decay on the last layer v. For formulation 1
(Fisher only) we have ρF = 1e-7, modified critic learning rate ηD = 1e-4, critic iters nc = 2.
For formulation 2 (Sobolev + Fisher) we have ρF = 5e-8, ρS = 2e-8, critic iters nc = 1. For the
WGAN-GP (Gulrajani et al., 2017) baseline SSL experiment we followed the original paper with
critic iters nc = 5, ηG = ηD = 1e-4, Adam β2=0.9 and GP weight λGP = 10.0. Architectures
are as below. We determined λCE = 0.3 to be optimal from [0.03, 0.1, 0.3, 1.0, 3.0]. As mentioned
in Section 6.2, the K+1 critic formulation is not able to fit the training set with the GP constraint,
so we fall back to the plain critic formulation where the critic hv, Φω (x)i does not interact with the
classifier hS, Φω(x)i.
Architecture:
### CIFAR-10: 32x32. G is dcgan with G_extra_layers=2.
### D is in the flavor of OpenAI Improved GAN, ALI.
G(
(main): Sequential (
(0): ConvTranspose2d(100, 256, kernel_size=(4,
(1): BatchNorm2d(256, eps=1e-05, momentum=0.1,
(2): ReLU (inplace)
(3): ConvTranspose2d(256, 128, kernel_size=(4,
(4): BatchNorm2d(128, eps=1e-05, momentum=0.1,
(5): ReLU (inplace)
4), stride=(1, 1), bias=False)
affine=True)
4), stride=(2, 2), padding=(1, 1), bias=False)
affine=True)
26
Published as a conference paper at ICLR 2018
(6): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
(7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
(8): ReLU (inplace)
(9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
(10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
(11): ReLU (inplace)
(12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
(13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
(14): ReLU (inplace)
(15): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
(16): Tanh ()
)
)
D(
(main): Sequential (
(0):
(1):
(2):
(3):
(5):
(6):
(8):
(9):
(10)
(12)
(13)
(15)
(16)
(18)
(19)
(20)
(22)
(23)
(24)
(26)
(27)
(28)
(30)
(31)
)
Dropout (p = 0.2)
Conv2d(3, 96, kernel_size=(3, 3), stride=(1, 1),
LeakyReLU (0.2, inplace)
Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1)
LeakyReLU (0.2, inplace)
Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2)
LeakyReLU (0.2, inplace)
Dropout (p = 0.5)
Conv2d(96, 192, kernel_size=(3, 3), stride=(1,
LeakyReLU (0.2, inplace)
Conv2d(192, 192, kernel_size=(3,
LeakyReLU (0.2, inplace)
Conv2d(192, 192, kernel_size=(3,
LeakyReLU (0.2, inplace)
Dropout (p = 0.5)
Conv2d(192, 384, kernel_size=(3,
LeakyReLU (0.2, inplace)
Dropout (p = 0.5)
Conv2d(384, 384, kernel_size=(3,
LeakyReLU (0.2, inplace)
Dropout (p = 0.5)
Conv2d(384, 384, kernel_size=(1,
padding=(1, 1))
, padding=(1, 1), bias=False)
, padding=(1, 1), bias=False)
1), padding=(1, 1), bias=False)
1), padding=(1, 1), bias=False)
2), padding=(1, 1), bias=False)
1), bias=False)
1), bias=False)
1), bias=False)
LeakyReLU (0.2, inplace)
Dropout (p = 0.5)
3), stride=(1,
3), stride=(2,
3), stride=(1,
3), stride=(1,
1), stride=(1,
)
(V): Linear (6144 -> 1)
(S): Linear (6144 -> 10)
27