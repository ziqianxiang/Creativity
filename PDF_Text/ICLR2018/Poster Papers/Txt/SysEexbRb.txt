Critical Points of Linear Neural Networks:
Analytical Forms and Landscape Properties
Yi Zhou & Yingbin Liang
Department of Electrical and Computer Engineering
The Ohio State University
zhou.1172@osu.edu
Ab stract
Due to the success of deep learning to solving a variety of challenging machine
learning tasks, there is a rising interest in understanding loss functions for training
neural networks from a theoretical aspect. Particularly, the properties of critical
points and the landscape around them are of importance to determine the con-
vergence performance of optimization algorithms. In this paper, we provide a
necessary and sufficient characterization of the analytical forms for the critical
points (as well as global minimizers) of the square loss functions for linear neural
networks. We show that the analytical forms of the critical points characterize the
values of the corresponding loss functions as well as the necessary and sufficient
conditions to achieve global minimum. Furthermore, we exploit the analytical
forms of the critical points to characterize the landscape properties for the loss
functions of linear neural networks and shallow ReLU networks. One particu-
lar conclusion is that: While the loss function of linear networks has no spurious
local minimum, the loss function of one-hidden-layer nonlinear networks with
ReLU activation function does have local minimum that is not global minimum.
1 Introduction
In the past decade, deep neural networks Goodfellow et al. (2016) have become a popular tool that
has successfully solved many challenging tasks in a variety of areas such as machine learning, ar-
tificial intelligence, computer vision, and natural language processing, etc. As the understandings
of deep neural networks from different aspects are mostly based on empirical studies, there is a
rising need and interest to develop understandings of neural networks from theoretical aspects such
as generalization error, representation power, and landscape (also referred to as geometry) proper-
ties, etc. In particular, the landscape properties of loss functions (that are typically nonconex for
neural networks) play a central role to determine the iteration path and convergence performance of
optimization algorithms.
One major landscape property is the nature of critical points, which can possibly be global minima,
local minima, saddle points. There have been intensive efforts in the past into understanding such an
issue for various neural networks. For example, it has been shown that every local minimum of the
loss function is also a global minimum for shallow linear networks under the autoencoder setting and
invertibility assumptions Baldi & Hornik (1989) and for deep linear networks Kawaguchi (2016);
Lu & Kawaguchi (2017); Yun et al. (2017) respectively under different assumptions. The conditions
on the equivalence between local minimum or critical point and global minimum has also been
established for various nonlinear neural networks Yu & Chen (1995); Gori & Tesi (1992); Nguyen
& Hein (2017); Soudry & Carmon (2016); Feizi et al. (2017) under respective assumptions.
However, most previous studies did not provide characterization of analytical forms for critical
points of loss functions for neural networks with only very few exceptions. In Baldi & Hornik
(1989), the authors provided an analytical form for the critical points of the square loss function of
shallow linear networks under certain conditions. Such an analytical form further helps to establish
the landscape properties around the critical points. Further in Li et al. (2016b), the authors charac-
terized certain sufficient form of critical points for the square loss function of matrix factorization
problems and deep linear networks.
1
The focus of this paper is on characterizing the sufficient and necessary forms of critical points for
broader scenarios, i.e., shallow and deep linear networks with no assumptions on data matrices and
network dimensions, and shallow ReLU networks over certain parameter space. In particular, such
analytical forms of critical points capture the corresponding loss function values and the necessary
and sufficient conditions to achieve global minimum. This further enables us to establish new land-
scape properties around these critical points for the loss function of these networks under general
settings, and provides alternative (yet simpler and more intuitive) proofs for existing understanding
of the landscape properties.
Our Contribution
1)	For the square loss function of linear networks with one hidden layer, we provide a full (necessary
and sufficient) characterization of the analytical forms for its critical points and global minimizers.
These results generalize the characterization in Baldi & Hornik (1989) to arbitrary network pa-
rameter dimensions and any data matrices. Such a generalization further enables us to establish
the landscape property, i.e., every local minimum is also a global minimum and all other critical
points are saddle points, under no assumptions on parameter dimensions and data matrices. From
a technical standpoint, we exploit the analytical forms of critical points to provide a new proof for
characterizing the landscape around the critical points under full relaxation of assumptions, where
the corresponding approaches in Baldi & Hornik (1989) are not applicable. As a special case of
linear networks, the matrix factorization problem satisfies all these landscape properties.
2)	For the square loss function of deep linear networks, we establish a full (necessary and sufficient)
characterization of the analytical forms for its critical points and global minimizers. Such charac-
terizations are new and have not been established in the existing art. Furthermore, such analytical
form divides the set of non-global-minimum critical points into different categories. We identify the
directions along which the loss function value decreases for two categories of the critical points, for
which our result directly implies the equivalence between the local minimum and the global mini-
mum. For these cases, our proof generalizes the result in Kawaguchi (2016) under no assumptions
on the network parameter dimensions and data matrices.
3)	For the square loss function of one-hidden-layer nonlinear neural networks with ReLU activation
function, we provide a full characterization of both the existence and the analytical forms of the
critical points in certain types of regions in the parameter space. Particularly, in the case where there
is one hidden unit, our results fully characterize the existence and the analytical forms of the critical
points in the entire parameter space. Such characterization were not provided in previous work on
nonlinear neural networks. Moreover, we apply our results to a concrete example to demonstrate
that both local minimum that is not a global minimum and local maximum do exist in such a case.
Related Work
Analytical forms of critical points: Characterizing the analytical form of critical points for loss
functions of neural networks dates back to Baldi & Hornik (1989), where the authors provided an
analytical form of the critical points for the square loss function of linear networks with one hidden
layer. In Li et al. (2016b), the authors provided a sufficient condition of critical points of a generic
function, i.e., the fixed point of invariant groups. They then characterized certain sufficient forms of
critical points for the square loss function of matrix factorization problems and deep linear networks,
whereas our results provide sufficient and necessary forms of critical points for deep linear networks
via a different approach.
Properties of critical points: Baldi & Hornik (1989); Baldi (1989) studied the linear autoencoder
with one hidden layer and showed the equivalence between the local minimum and the global min-
imum. Moreover, Baldi & Lu (2012) generalized these results to the complex-valued autoencoder
setting. The deep linear networks were studied by some recent work Kawaguchi (2016); Lu &
Kawaguchi (2017); Yun et al. (2018), in which the equivalence between the local minimum and the
global minimum was established respectively under different assumptions. Particularly, Yun et al.
(2017) established a necessary and sufficient condition for a critical point of the deep linear network
to be a global minimum. A similar result was established in Freeman & Bruna (2017) for deep linear
networks under the setting that the widths of intermediate layers are larger than those of the input
2
and output layers. The effect of regularization on the critical points for a two-layer linear network
was studied in Taghvaei et al. (2017).
For nonlinear neural networks, Yu & Chen (1995) studied a nonlinear neural network with one hid-
den layer and sigmoid activation function, and showed that every local minimum is also a global
minimum provided that the number of input units equals the number of data samples. Gori & Tesi
(1992) considered a class of multi-layer nonlinear networks with a pyramidal structure, and showed
that all critical points of full column rank achieve the zero loss when the sample size is less than the
input dimension. These results were further generalized to a larger class of nonlinear networks in
Nguyen & Hein (2017), in which they also showed that critical points with non-degenerate Hessian
are global minimum. Choromanska et al. (2015a;b) connected the loss surface of deep nonlinear
networks with the Hamiltonian of the spin-glass model under certain assumptions and characterized
the distribution of the local minimum. Kawaguchi (2016) further eliminated some of the assump-
tions in Choromanska et al. (2015a), and established the equivalence between the local minimum
and the global minimum by reducing the loss function of the deep nonlinear network to that of the
deep linear network. Soudry & Carmon (2016) showed that a two-layer nonlinear network has no
bad differentiable local minimum. Feizi et al. (2017) studied a one-hidden-layer nonlinear neural
network with the parameters restricted in a set of directions of lines, and showed that most local
minima are global minima. Tian (2017) considered a two-layer ReLU network with Gaussian in-
put data, and showed that critical points in certain region are non-isolated and characterized the
critical-point-free regions.
Geometric curvature: Hardt & Ma (2017) established the gradient dominance condition of deep
linear residual networks, and Zhou & Liang (2017) further established the gradient dominance con-
dition and regularity condition around the global minimizers for deep linear, deep linear residual and
shallow nonlinear networks. Li et al. (2016a) studied the property of the Hessian matrix for deep lin-
ear residual networks. The local strong convexity property was established in Soltanolkotabi et al.
(2017) for overparameterized nonlinear networks with one hidden layer and quadratic activation
functions, and was established in Zhong et al. (2017) for a class of nonlinear networks with one
hidden layer and Gaussian input data. Zhong et al. (2017) further established the local linear con-
vergence of gradient descent method with tensor initialization. Soudry & Hoffer (2017) studied a
one-hidden-layer nonlinear network with a single output, and showed that the volume of sub-optimal
differentiable local minima is exponentially vanishing in comparison with the volume of global min-
ima. Dauphin et al. (2014) investigated the saddle points in deep neural networks using the results
from statistical physics and random matrix theory.
Notation: The pseudoinverse, column space and null space of a matrix M are denoted by
Mt, Col(M) and ker(M), respectively. For any index sets I, J ⊂ N, Mi,j denotes the SUb-
matrix of M formed by the entries with the row indices in I and the column indices in J. For
positive integers i ≤ j, we define i : j = {i, i + 1, . . . , j - 1, j}. The projection operator onto a
linear sUbspace V is denoted by PV .
2 Linear Neural Networks with One Hidden Layer
In this section, we stUdy linear neUral networks with one hidden layer. SUppose we have an inpUt
data matrix X ∈ Rd0×m and a corresponding oUtpUt data matrix Y ∈ Rd2×m, where there are in
total m data samples. We are interested in learning a model that maps from X to Y via a linear
network with one hidden layer. Specifically, we denote the weight parameters between the oUtpUt
layer and the hidden layer of the network as A2 ∈ Rd2 ×d1 , and denote the weight parameters
between the hidden layer and the inpUt layer of the network as A1 ∈ Rd1 ×d0 . We are interested in
the sqUare loss fUnction of this linear network, which is given by
L ：= 2 kA2 AiX — Y kF .
Note that in a special case where X = I, L redUces to a loss fUnction for the matrix factorization
problem, to which all oUr resUlts apply. The loss fUnction L has been stUdied in Baldi & Hornik
(1989) Under the assUmptions that d2 = d0 ≥ d1 and the matrices XX>, YX>(XX>)-1XY>are
invertible. In oUr stUdy, no assUmption is made on either the parameter dimensions or the invertibility
of the data matrices. SUch fUll generalization of the resUlts in Baldi & Hornik (1989) tUrns oUt to be
critical for oUr stUdy of nonlinear shallow neUral networks in Section 4.
3
We further define Σ := YX*XY> and denote its full singular value decomposition as UΛU>.
Suppose that Σ has r distinct positive singular values σι > •… >。丁 > 0 with multiplicities
mi,..., mr, respectively, and has m zero singular values. Recall that (Ai, A2) is defined to be a
critical point of L if Vai L = 0, Va2 L = 0. Our first result provides a full characterization of all
critical points of L.
Theorem 1 (Characterization of critical points). All critical points of L are necessarily and suf-
ficiently characterized by a matrix Li ∈ Rd1 ×d0, a block matrix V ∈ Rd2 ×d1 and an invertible
matrix C ∈ Rd1 ×d1 via
Ai = C-iV>U>YX* + Li - C-iV>V CLiXX*	(1)
A2 = UV C.	(2)
Specifically, V = [diag(V1,..., Vr, V), 0d2×(dl-rank(A2))], where both V ∈ Rmi×pi and V ∈
Rm ×p consist oforthonormal columns with the number of columns Pi ≤ mi, i = 1,...,r,p ≤ m
such that Er=I Pi + P = rank(A2), and Li, V, C satisfy
Pcol(UV)⊥Y X>L>iC>Pker(V) =0.	(3)
Theorem 1 characterizes the necessary and sufficient forms for all critical points of L. Intuitively,
the matrix C captures the invariance of the product A2Ai under an invertible transform, and Li
captures the degree of freedom of the solution set for linear systems.
In general, the set of critical points is uncountable and cannot be fully listed out. However, the
analytical forms in eqs. (1) and (2) do allow one to construct some critical points of L by specifying
choices of Li, V, C that fulfill the condition in eq. (3). For example, choosing Li = 0 guarantees
eq. (3), in which case eqs. (1) and (2) yield a critical point (C-iV>U>YX*, UVC) for any in-
vertible matrix C and any block matrix V that takes the form specified in Theorem 1. For nonzero
Li , one can fix a proper V and solve the linear equation on C in eq. (3). If a solution exists, we
then obtain the form of a corresponding critical point. We further note that the analytical structures
of the critical points are more important, which have direct implications on the global optimality
conditions and landscape properties as we show in the remaining part of the section.
Remark 1. We note that the block pattern parameters {pi}r=ι and P denote the number OfColUmnS
of {⅝}r=ι and V, respectively, and their sum equals the rank of A2, i.e., Er=I Pi + P = rank(A2).
The parameters Pi, i = 1,...,r,P of V contain all useful information of the critical points that
determine the function value of L as presented in the following proposition.
Proposition 1. AnyCritiCalPoint (Ai, A2) of L satisfies L(Ai, A2) = 11 (Tr(YY>) 一 Pr=iPiσi).
Proposition 1 evaluates the function value L at a critical point using the parameters {Pi}ir=i. To
explain further, recall that the data matrix Σ has each singular value σi with multiplicity mi . For
each i, the critical point captures Pi out of mi singular values σi . Hence, for a σi with larger value
(i.e., a smaller index i), it is desirable that a critical point captures a larger number Pi of them.
In this way, the critical point captures more important principle components of the data so that
the value of the loss function is further reduced as suggested by Proposition 1. In summary, the
parameters {Pi}ir=i characterize how well the learned model fits the data in terms of the value of the
loss function. Moreover, the parameters {Pi}ir=i also determine a full characterization of the global
minimizers as given below.
Proposition 2 (Characterization of global minimizers). A critical point (Ai, A2) ofL is a global
minimizer if and only if it falls into the following two cases.
1.	Case 1: min{d2, di} ≤	ir=i mi, A2 is full rank, and Pi = mi, . . . , Pk-i = mk-i,Pk =
rank(A2 ) 一 Pik=-ii mi ≤ mk for some k ≤ r;
2.	Case 2: min{d1, di} > Er=I mi, Pi = mi for i = 1,...,r, and P ≥ 0. In particular, A1 can
be non-full rank with rank(A2 ) =	ir=i mi.
The analytical form of any global minimizer can be obtained from Theorem 1 with further specifica-
tion to the above two cases.
Proposition 2 establishes the neccessary and sufficient conditions for any critical point to be a global
minimizer. If the data matrix Σ has a large number of nonzero singular values, i.e., the first case,
4
one needs to exhaust the representation budget (i.e., rank) of A2 and capture as many large singular
values as the rank allows to achieve the global minimum; Otherwise, A2 of a global minimizer can
be non-full rank and still captures all nonzero singular values. Note that A2 must be full rank in the
case 1, and so is A1 if we further adopt the assumptions on the network size and data matrices in
Baldi & Hornik (1989).
Furthermore, the parameters {pi }ir=1 naturally divide all non-global-minimum critical points
(A1, A2) of L into the following two categories.
•	(Non-optimal order): The matrix V specified in Theorem 1 satisfies that there exists 1 ≤ i < j ≤
r such that pi < mi and pj > 0.
•	(Optimal order): rank(A2) < min{d2, d1} and the matrix V specified in Theorem 1 satisfies
that p1 = m1, . . . , pk-1 = mk-1,pk = rank(A2) - Pik=-11 mi ≤ mk for some 1 ≤ k ≤ r.
To understand the above two categories, note that a critical point of L with non-optimal order cap-
tures a smaller singular value σj (since pj > 0) while skipping a larger singular value σi with a
lower index i < j (since pi < mi), and hence cannot be a global minimizer. On the other hand,
although a critical point of L with optimal order captures the singular values in the optimal (i.e.,
decreasing) order, it does not fully utilize the representation budget of A2 (because A2 is non-full
rank) to further capture nonzero singular values and reduce the function value, and hence cannot be
a global minimizer either. Next, we show that these two types of non-global-minimum critical points
have different landscape properties around them. Throughout, a matrix M is called the perturbation
of M if it lies in an arbitrarily small neighborhood of M .
Proposition 3 (Landscape around critical points). The critical points of L have the following land-
scape properties.
1.	A non-optimal-order critical point (A1, A2) has a perturbation (A1, A2) with rank(A2) =
rank(A2), which achieves a lower function value;
2.	An optimal-order critical point (A1, A2) has a perturbation (A1, A2) with rank(A2) =
rank(A2) + 1, which achieves a lower function value;
3.	Any point in X := {(A1, A2) : A2A1X 6= 0} has a perturbation (A1, A2), which achieves a
higher function value;
As a consequence, items 1 and 2 imply that any non-global-minimum critical point has a descent
direction, and hence cannot be a local minimizer. Thus, any local minimizer must be a global
minimizer. Item 3 implies that any point has an ascent direction whenever the output is nonzero.
Hence, there does not exist any local/global maximizer in X . Furthermore, item 3 together with
items 1 and 2 implies that any non-global-minimum critical point in X has both descent and ascent
directions, and hence must be a saddle point. We summarize these facts in the following theorem.
Theorem 2 (Landscape of L). The loss function L satisfies: 1) every local minimum is also a global
minimum; 2) every non-global-minimum critical point in X is a saddle point.
We note that the saddle points in Theorem 2 can be non-strict when the data matrices are singular. As
an illustrative example, consider the following loss function ofa shallow linear network L(a2, a1) =
1 (a2a1χ - y)2, where a1,a2,χ and y are all scalars. Consider the case y = 0. Then, the Hessian at
the saddle point a1 = 0, a2 = 1 is [x2 , 0; 0, 0], which does not have any negative eigenvalue.
From a technical point of view, the proof of item 1 of Proposition 3 applies that in Baldi (1989) and
generalizes it to the setting where Σ can have repeated singular values and may not be invertible.
To further understand the perturbation scheme from a high level perspective, note that non-optimal-
order critical points capture a smaller singular value σj instead of a larger one σi with i < j . Thus,
one naturally perturbs the singular vector corresponding to σj along the direction of the singular
vector corresponding to σi . Such a perturbation scheme preserves the rank of A2 and reduces the
value of the loss function.
More importantly, the proof of item 2 of Proposition 3 introduces a new technique. As a comparison,
Baldi & Hornik (1989) proves a similar result as item 2 using the strict convexity of the function,
which requires the parameter dimensions to satisfy d2 = d0 ≥ d1 and the data matrices to be invert-
ible. In contrast, our proof completely removes these restrictions by introducing a new perturbation
direction and exploiting the analytical forms of critical points in eqs. (1) and (2) and the condition in
5
eq. (3). The accomplishment of the proof further requires careful choices of perturbation parameters
as well as judicious manipulations of matrices. We refer the reader to the supplemental materials for
more details. As a high level understanding, since optimal-order critical points capture the singular
values in an optimal (i.e., decreasing) order, the previous perturbation scheme for non-optimal-order
critical points does not apply. Instead, we increase the rank of A2 by one in a way that the perturbed
matrix captures the next singular value beyond the ones that have already been captured so that the
value of the loss function can be further reduced.
3	Deep Linear Neural Networks
In this section, we study deep linear networks with ` ≥ 2 layers. We denote the weight parameters
between the layers as Ak ∈ Rdk ×dk-1 for k = 1, . . . , `, respectively. The input and output data are
denoted by X ∈ Rd0×m, Y ∈ Rd'×m, respectively. We are interested in the square loss function
of deep linear networks, which is given by
LD ：= 2 kA' …A2A1X - γkF.
Denote ∑k := Y(A(k,i)X)^ A(k,i)XY> for k = 0,...,' with the full singular value decom-
position UkΛkU>. Suppose that ∑k has r(k) distinct positive singular values σι(k) > •… >
σr(k)(k) > 0 with multiplicities mι(k),..., m『(k)(k), respectively, and m(k) zero singular Val-
ues. Our first result provides a full characterization of all critical points of LD, where we denote
A(i,j):= AiAi-I … Aj+ιAj for notational convenience1.
Theorem 3 (Characterization of critical points). All critical points of LD are necessarily and suf-
ficiently characterized by matrices Lk ∈ Rdk×dk-1, block matrices Vk ∈ Rdl ×dk+1 and invertible
matrices Ck ∈ Rdk+1×dk+1 for k = 0,...,' — 2 such that Ai,..., A' can be individually expressed
out recursively via the following two equations:
Ak+i = C-1V>U^>Y (A(k,i)X )t + Lk+i — C-1V>Vk Ck Lk+ιA(k,i)X (A(k,i)X )t,	(4)
A(',k+2) = Uk Vk Ck.	(5)
Specifically, Vk = [diag(V(k),..., V((Z), V(k)), 0dι×(dk+ι-rank(A(',k+2)))], where Wk) ∈
Rmi(k)×pi(k), V(k) ∈ Rm(k)×p(k) consist of orthonormal columns with pi(k) ≤ mi(k) for
i = 1,...,r(k), p(k) ≤ τn(k) such that Pr=k) Pi(k) + p(k) = rank(A',k+ι), and Lk, Vk, Ck
satisfy for k = 2, . . . , ` — 1
A(',k) = A(',k+i)Ak,	(I - Pcoi(A(',k+i)) )YX>A%ι,i) = 0.	⑹
Note that the forms of the individual parameters Ai,..., A' can be obtained as follows by recur-
sively applying eqs. (4) and (5). First, eq. (5) with k = 0 yields the form of A(',2). Then, eq. (4)
with k = 0 and the form of A(',2) yield the form of Ai. Next, eq. (5) with k = 1 yields the form
of A(',3), and then, eq. (4) with k = 1 and the forms of A(',3), Ai further yield the form of A2.
Inductively, one obtains the expressions of all individual parameter matrices. Furthermore, the first
condition in eq. (6) is a consistency condition that guarantees that the analytical form for the entire
product of parameter matrices factorizes into the forms of individual parameter matrices. Similarly
to shallow linear networks, while the set of critical points here is also uncountable, Theorem 3 sug-
gests ways to obtain some critical points. For example, if we set Lk = 0 for all k (i.e., eq. (6) is
satisfied), we can obtain the form of critical points for any invertible Ck and proper Vk with the
structure specified in Theorem 3. For nonzero Lk, eq. (6) needs to be verified for given Ck and Vk
to determine a critical point.
Similarly to shallow linear networks, the parameters {pi(0)}r(O),p(0) determine the value of the
loss function at the critical points and further specify the analytical form for the global minimizers,
as we present in the following two propositions.
Proposition 4. Any critical point (Ai, . . . , A') ofLD satisfies
LD (Ai,..., A ') = i [Tr(YYT)- Pr(O) Pi(0)σi(0)].
1Here, A(0,1) should be understood as identity matrix I.
6
Proposition 5 (Characterization of global minimizers). A critical point (Ai,..., a`) of LD is a
global minimizer if and only if it falls into the following two cases.
1.	Case 1: min{d',...,dι} ≤ P；=0) mi(0), A(',2) achieves the maximal rank, and pi(0) =
mι(0),...,Pk-i(0) = mk-i(0),Pk(0) = rank(A%2)) - Pk-1 mi(0) ≤ mk(0) for some
k ≤ r(0);
2.	Case 2: min{d', ...,dι} > P；=0) mi(0), Pi(0) = mi(0) for all i = 1,..., r(0) and /(0) ≥ 0.
In particular, A(',2) Can be non-full rank with rank(A(',2)) = Pr=O) mi (0).
The analytical form of any global minimizer can be obtained from Theorem 3 with further specifica-
tion to the above two cases.
In particular for case 1, if we further adopt the invertibility assumptions on data matrices as in Baldi
& Hornik (1989) and assume that all parameter matrices are square, then all global minima must
correspond to full rank parameter matrices.
We next exploit the analytical forms of the critical points to further understand the landscape of
the loss function LD. It has been shown in Kawaguchi (2016) that every local minimum of LD is
also a global minimum, under certain conditions on the parameter dimensions and the invertibility
of the data matrices. Here, our characterization of the analytical forms for the critical points allow
us to understand such a result from an alternative viewpoint. The proofs for certain cases (that we
discuss below) are simpler and more intuitive, and no assumption is made on the data matrices and
dimensions of the network.
Similarly to shallow linear networks, we want to understand the local landscape around the critical
points. However, due to the effect of depth, the critical points of LD are more complicated than
those of L. Among them, we identify the following subsets of the non-global-minimum critical
points (Ai,…，a`) of LD.
•	(Deep-non-optimal order): There exist 0 ≤ k ≤ ` - 2 such that the matrix Vk specified in
Theorem 3 satisfies that there exist 1 ≤ i < j ≤ r(k) such thatpi(k) < mi(k) andpj(k) > 0.
•	(Deep-optimal order): (a`, A'-ι) is not a global minimizer of LD with A('-2,1) being fixed,
rank(A') < min{d`, d`-1}, and the matrix V'-2 specified in Theorem 3 satisfies thatpi(l-2)=
mi (l - 2), . . . , pk-i (l - 2) = mk-i (l - 2), pk (l - 2) = rank(Al) - Pi=i mi (l - 2) ≤ mk (l - 2)
for some 1 ≤ k ≤ r(l - 2).
The following result summarizes the landscape of LD around the above two types of critical points.
Theorem 4 (Landscape of LD). The loss function LD has the following landscape properties.
1.	A deep-non-optimal-order critical point	(Ai,..., A')	has a perturbation
(Ai,..., Ak+i,..., a`) with rank(A') = rank(A'), which achieves a lower function
value.
2.	A deep-optimal-order critical point (Ai,..., A') has a perturbation (Ai,..., A'-ι, A') with
rank(A') = rank(A') + 1, which achieves a lower function value.
3.	Any point in XD := {(Ai, . . . , A') : A(',i)X 6= 0} has a perturbation (Ai, . . . , A') that
achieves a higher function value.
Consequently, 1) every local minimum of LD is also a global minimum for the above two types of
critical points; and 2) every critical point of these two types in XD is a saddle point.
Theorem 4 implies that the landscape of LD for deep linear networks is similar to that of L for
shallow linear networks, i.e., the pattern of the parameters {pi (k)}ir=(ki) implies different descent di-
rections of the function value around the critical points. Our approach does not handle the remaining
set of non-global minimizers, i.e., there exists q ≤ `- 1 such that (A', . . . , Aq) is a global minimum
point of LD with A(q-i,i) being fixed, and A(',q) is of optimal order. It is unclear how to perturb
the intermediate weight parameters using their analytical forms for deep networks , and we leave
this as an open problem for the future work.
7
4	Nonlinear Neural Networks with One Hidden Layer
In this section, we study nonlinear neural networks with one hidden layer. In particular, we consider
nonlinear networks with ReLU activation function σ : R → R that is defined as σ(x) := max{x, 0}.
Our study focuses on the set of differentiable critical points. The weight parameters between the
layers are denoted by A2 ∈ Rd2 ×d1 , A1 ∈ Rd1 ×d0, respectively, and the input and output data are
denoted by X ∈ Rd0 ×m , Y ∈ Rd2 ×m, respectively. We are interested in the square loss function
which is given by
LN := 1 kA2σ(A1 X) - Y kF ,	⑺
where σ acts on A1X entrywise. Existing studies on nonlinear networks characterized the sufficient
conditions for critical points being global minimum Gori & Tesi (1992); Nguyen & Hein (2017),
established the equivalence between local minimum and global minimum under the condition that
d0 = m Yu & Chen (1995), and provided understanding of geometric properties of the critical points
Tian (2017). In comparison, our results below provide a full characterization of the critical points of
LN with d1 = 1 and critical points of LN over certain parameter space with d1 > 1, and show that
local minimum of LN that is not global minimum can exist.
Since the activation function σ is piecewise linear, the entire parameter space can be partitioned
into disjoint cones. In particular, we consider the set of cones KI×J where I ⊂ {1, . . . , d1}, J ⊂
{1, . . . , m} that satisfy
KI×J := {(A2, A1) : (A1)I,:X:,J ≥ 0, other entries ofA1X < 0},	(8)
where "≥" and “<" represent entrywise comparisons. Within Ki×j, the term σ(A1X) activates
only the entries σ(A1X)i：j, and the corresponding loss function LN is equivalent to
∀(A2, Ai) ∈Ki×j,	LN := 2 k(A2)：,i(Ai)i,：X：,J - Y：„jkF + 1 kK,jCkF .	(9)
Hence, within KI×J, LN reduces to the loss of a shallow linear network with parameters
((A2):,I, (A1)I,:) and input & output data pair (X:,J, Y:,J). Note that our results on shallow
linear networks in Section 2 are applicable to all parameter dimensions and data matrices. Thus,
Theorem 1 fully characterizes the forms of critical points of LN in KI×J. Moreover, the exis-
tence of such critical points can be analytically examined by substituting their forms into eq. (8). In
summary, We obtain the following result, where We denote Σj := Y：jXJJX：,JYJ with the full
singular value decomposition UJΛJU>J, and suppose that ΣJ has r(J) distinct positive singular
values σι(J) > … > σrj)(J) with multiplicities mi,..., m『(j), respectively, and m(J) zero
singular values.
Proposition 6 (Characterization of critical points). All critical points of LN in KI ×J for any I ⊂
{1,..., di}, J ⊂ {1,..., m} are necessarily and sufficiently characterized by an Li ∈ RlIl×d0, a
block matrix V ∈ Rd2 ×lIl and an invertible matrix C ∈ RlIl×lIl such that
(Ai)I,: =C-iV>U>JY:,JX:J,J+Li -C-iV>VCLiX:,JX:J,J,	(10)
(A2):,I = UJVC.	(11)
Specifically, V = [diag(¼,..., V^(j), V), 0d2×(lI|-rank((A2):，I))], where V ∈ Rmi×pi, V ∈
Rm×p consist of orthonormal columns with Pi ≤ mi for i = 1,..., r(J), P ≤ m such that
Pr=JL) Pi + P = rank((A2)：,i), and Li, V, C satisfy
Pcol(UJ V)⊥Y:,JX>:,JL>iC>Pker(V) =0.	(12)
Moreover, a critical point in KI ×J exists if and only if there exists such C, V, Li that
(Ai)I,:X:,J=C-iV>U>JYX:J,JX:,J+C-i(I- Pker(V))CLiX:,J ≥0,	(13)
Other entries ofAiX < 0.	(14)
To further illustrate, we consider a special case where the nonlinear network has one unit in the
hidden layer, i.e., di = 1, in which case Ai and A2 are row and column vectors, respectively.
Then, the entire parameter space can be partitioned into disjoint cones taking the form ofKI×J, and
I = {1} is the only nontrivial choice. We obtain the following result from Proposition 6.
8
Proposition 7 (Characterization of critical points). Consider LN with d1 = 1 and any J ⊂
{1, . . . , m}. Then, any nonzero critical point of LN within K{1}×J can be necessarily and suffi-
Ciently characterized by an Z> ∈ R1×d0, a block unit vector V ∈ Rd2 ×1 and a scalar C ∈ R such
that
Ai = c-1v>U>Y:,JXJ + '> - '>X：,JXJ,	A2 = CUJv.	(15)
Specifically, v is a unit vector that is supported on the entries corresponding to the same singular
value of ΣJ. Moreover, a nonzero critical point in K{i}×J exists if and only if there exist such
c, v, 2> that satisfy
AiX：,j = c-1v>U>Y:,JXJX：,J ≥ 0,	(16)
AiX：,jC = c-1v>U>K,JX[JX：,jC + '>X：,JC- '>X：,JX：JX：,jC < 0.	(17)
We note that Proposition 7 characterizes both the existence and the forms of critical points of LN
over the entire parameter space for nonlinear networks with a single hidden unit. The condition in
eq. (12) is guaranteed because Pker(v) = 0 for v 6= 0.
To further understand Proposition 7, suppose that there exists a critical point in K{i}×J with v
being supported on the entries that correspond to the i-th singular value of ΣJ. Then, Proposition 1
implies that LN = 1 Tr(YYT) - 2 σ%(J). In particular, the critical point achieves the local minimum
2Tr(YYt)- 1 σι(J) in K{i}×J with i = 1. This is because in this case the critical point is full
rank with an optimal order, and hence corresponds to the global minimum of the linear network in
eq. (9). Since the singular values of ΣJ may vary with the choice of J, LN may achieve different
local minima in different cones. Thus, local minimum that is not global minimum can exist for LN .
The following proposition concludes this fact by considering a concrete example.
Proposition 8. For one-hidden-layer nonlinear neural networks with ReLU activation function,
there exists local minimum that is not global minimum, and there also exists local maximum.
The above proposition is demonstrated by the following example.
Example 1. Consider the loss function LN of the nonlinear network with d2 = d0 = 2, and d1 = 1.
The input and output data are set to be X = diag(1, 1), Y = diag(2, 1).
First, consider the cone KI×J with I = {1}, J = {1}. Calculation yields that ΣJ = diag(4, 0), and
the conditions for existence of critical points in eqs. (16) and (17) hold if 2c-1(v)i,： ≥ 0, (21)2,： <
0. Then choosing C = 1, v = (1, 0)T, `1 = (1, -1)T yields a local minimum in KI×J, because
the nonzero entry in v corresponds to the largest singular value of ΣJ. Then, calculation shows
that the local minimum achieves LN = 11. On the other hand, consider the cone Ki ×j with
I = {1}, J0 = {2}, in which ΣJ0 = diag(0, 1). The conditions for existence of critical points
in eqs. (16) and (17) hold if CT(v)i,： ≥ 0, (21)1,： < 0. Similarly to the previous case, choosing
C = 1, v = (1, 0)T, `1 = (-1, 0)T yields a local minimum that achieves the function value Ln = 2.
Hence, local minimum that is not global minimum does exist. Moreover, in the cone KI ×J00 with
I = {1},J00 = 0, the function LN remains to be the constant 2, and all points in this cone are local
minimum or local maximum. Thus, the landscape of the loss function of nonlinear networks is very
different from that of the loss function of linear networks.
Conclusion
In this paper, we provide full characterization of the analytical forms of the critical points for the
square loss function of three types of neural networks, namely, shallow linear networks, deep linear
networks, and shallow ReLU nonlinear networks. We show that such analytical forms of the critical
points have direct implications on the values of the corresponding loss functions, achievement of
global minimum, and various landscape properties around these critical points. As a consequence,
the loss function for linear networks has no spurious local minimum, while such point does exist for
nonlinear networks with ReLU activation. In the future, it is interesting to further explore nonlinear
neural networks. In particular, we wish to characterize the analytical form of critical points for
deep nonlinear networks and over the full parameter space. Such results will further facilitate the
understanding of the landscape properties around these critical points.
9
Acknowledgments
The work was supported in part by US National Science Foundation under the grants CCF-1761506,
ECCS-1818904, and was supported in part by DARPA FunLoL program.
References
P. Baldi. Linear learning: Landscapes and algorithms. In Proc. Advances in Neural Information
Processing Systems (NIPS),pp. 65-72.1989.
P. Baldi and K. Hornik. Neural networks and principal component analysis: Learning from examples
without local minima. Neural Networks, 2(1):53 - 58, 1989.
P. Baldi and Z. Lu. Complex-valued autoencoders. Neural Networks, 33:136-147, September 2012.
A. Choromanska, M. Henaff, M. Mathieu, G. Arous, and Y. LeCun. The loss surfaces of multilayer
networks. Journal of Machine Learning Research, 38:192-204, 2015a. ISSN 1532-4435.
A. Choromanska, Y. LeCun, and G. Arous. Open problem: The landscape of the loss surfaces of
multilayer networks. In Proc. Conference on Learning Theory (COLT), volume 40, pp. 1756-
1760, Jul 2015b.
Y. N. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli, and Y. Bengio. Identifying and attack-
ing the saddle point problem in high-dimensional non-convex optimization. In Proc. Advances in
Neural Information Processing Systems (NIPS). 2014.
S. Feizi, H. Javadi, J. Zhang, and D. Tse. Porcupine neural networks: (almost) all local optima are
global. ArXiv: 1710.02196, October 2017.
C. D. Freeman and J. Bruna. Topology and geometry of half-rectified network optimization. Proc.
International Conference on Learning Representations (ICLR), 2017.
I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT Press, 2016. http://www.
deeplearningbook.org.
M. Gori and A. Tesi. On the problem of local minima in backpropagation. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 14(1):76-86, Jan 1992.
M. Hardt and T. Ma. Identity matters in deep learning. Proc. International Conference on Learning
Representations (ICLR), 2017.
K. Kawaguchi. Deep learning without poor local minima. In Proc. Advances in Neural Information
Processing Systems (NIPS), pp. 586-594. 2016.
S. Li, J. Jiao, Y. Han, and T. Weissman. Demystifying resnet. Arxiv: 1611.01186, 2016a. URL
https://arxiv.org/pdf/1611.01186.
X. Li, Z. Wang, J. Lu, R. Arora, J. Haupt, H. Liu, and T. Zhao. Symmetry, saddle points, and global
geometry of nonconvex matrix factorization. ArXiv: 1612.09296v2, December 2016b.
H. T. Lu and K. Kawaguchi. Depth creates no bad local minima. ArXiv: 1702.08580, February
2017.
Q. Nguyen and M. Hein. The loss surface of deep and wide neural networks. In Proc. International
Conference on Machine Learning (ICML), volume 70, pp. 2603-2612, Aug 2017.
M. Soltanolkotabi, A. Javanmard, and J. D. Lee. Theoretical insights into the optimization landscape
of over-parameterized shallow neural networks. ArXiv:1707.04926, 2017.
D. Soudry and Y. Carmon. No bad local minima: Data independent training error guarantees for
multilayer neural networks. ArXiv: 1605.08361, May 2016.
D. Soudry and E. Hoffer. Exponentially vanishing sub-optimal local minima in multilayer neural
networks. ArXiv:1702.05777, February 2017.
10
A. Taghvaei, J. W. Kim, and P. Mehta. How regularization affects the critical points in linear net-
works. In Proc. International Conference on Neural Information Processing Systems (NIPS),
2017.
Y. Tian. An analytical formula of population gradient for two-layered ReLU network and its appli-
cations in convergence and critical point analysis. In Proc. International Conference on Machine
Learning (ICML), volume 70,pp. 3404-3413, 06-11 Aug 2017.
X. H. Yu and G. A. Chen. On the local minima free condition of backpropagation learning. IEEE
Transactions on Neural Networks, 6(5):1300-1303, Sep 1995.
C. Yun, S. Sra, and A. Jadbabaie. Global optimality conditions for deep neural networks. ArXiv:
1707.02444, 2017.
C. Yun, S. Sra, and A. Jadbabaie. Global optimality conditions for deep neural networks. In Proc.
International Conference on Learning Representations (ICLR), 2018.
K. Zhong, Z. Song, P. Jain, P. L. Bartlett, and I. S. Dhillon. Recovery guarantees for one-hidden-layer
neural networks. In Proc. International Conference on Machine Learning (ICML), volume 70,
pp. 4140-4149, Aug 2017.
Y. Zhou and Y. Liang. Characterization of gradient dominance and regularity conditions for neural
networks. ArXiv: 1710.06910v2, October 2017.
11
Supplementary Materials
Proof of Theorem 1
Notations: For any matrix M, denote vec(M) as the column vector formed by stacking its columns.
Denote the Kronecker product as “0”. Then, the following useful relationships hold for any dimen-
sion compatible matrices M, U, V , W :
vec(UMV) = (V>0U)vec(M) ,	(18)
(U 0 V )t = U10Vt,	(19)
(M0W)(U0V) = (MU)0(WV),	(20)
(M>M )tM> = M>(MM>)t = Mt,	(21)
MMtM = M, MtMMt = Mt.	(22)
Recall that a point (A1, A2) is a critical point of L ifit satisfies
Vai L = A>(A2A1X - Y)X> = O,	(23)
Va2L = (A2 AiX — Y )X>A> = O.	(24)
We first prove eqs. (1) and (2).
Lemma 1. Let (A2, A1) be a critical point of L. Then it must satisfy, for some L1 ∈ Rd1×d0, that
A1 = At2YXt + L1 - At2A2L1XXt,	(25)
Pcol(A2)ΣPcol(A2) = Σ Pcol(A2) = Pcol(A2)Σ.	(26)
Proof of Lemma 1. Since (A2, A1) is a critical point of L, eq. (23) implies that
A>2A2A1XX>= A>2Y X>.
Applying the vectorizing operator on both sides of the above equation and use the property in
eq. (18), we conclude that
(XX>0A>2A2)vec(A1) = (X0A>2)vec(Y) .
Since vec(A1) is a solution of the above linear equation, it must take the form of the solution of
linear systems, i.e., for some L1 ∈ Rd1 ×d0, we have
vec(A1) = (XX>0A>2A2)t(X0A>2)vec(Y) + [I - (XX>0A>2A2)t(XX>0A>2A2)]vec(L1)
= ((XX>)tX0(A>A2)tA>) VeC(Y) + [I — (XX>)tXX>0(A>A2)tA>A2]vec(L1)
=VeC((A>A2)t A>YX>(XX>)t + Li — (A>A2 )tA>A2L1XX>(XX>)t)
(=ii) veC
At2YXt + Li - At2A2LiXXt
where (i) uses eqs. (19) and (20) and (ii) uses eq. (21). Then, eq. (25) follows by reshaping the
vector into a matrix.
Next we prove eq. (26). Multiplying both sides of eq. (25) by A2 on the left and by X on the right
and then using eq. (22), we obtain
A2AiX = A2At2YXtX = Pcol(A2)YXtX.	(27)
Also, multiplying both sides of eq. (24) by A>2 on the right yields that A2AiXX>A>iA>2 =
Y X>A>iA>2. This equation, together with the above expression of A2AiX, further implies that
Pcol(A2)Σ Pcol(A2) = Σ Pcol(A2) .
Note that Pcol(A2)∑ Pcol(A2)is symmetric. Thus, We conclude that Σ Pcol(A2)= Pcol(A2)∑. □
12
Next, we derive the form of A2. Recall the full singular value decomposition Σ = U ΛU>, where Λ
is a diagonal matrix with distinct singular values σ1 > . . . > σr > 0 and multiplicities m1, . . . , mr,
respectively. We also assume that there are m number of zero singular values in Λ. Using the fact
that Pcol(A2) = U Pcol(U>A2)U>, the last equality in eq. (26) reduces to
Λ Pcol(U>A2) = Pcol(U>A2)Λ.
By the multiplicity pattern of the singular values in Λ, Pcol(U>A2 ) must be block diagonal. Specif-
ically, We can write Pcol(U^a?) = diag( Pi,..., Pr, P), where Pi ∈ Rmi×mi and P ∈ Rm×m.
Also, since Pcol(U>a2) is a projection, Pi,..., Pr, P must all be projections. Note that Pcol(U>a2)
has rank rank(A2), and suppose that Pi,..., Pr, P have ranks pι,...,Pr ,p, respectively. Then,
we must have Pi ≤ m for i = 1,...,r, P ≤ m and P；=i Pi + P = rank(A2). Also, note that each
projection can be expressed as Pi = ViViT with Vi ∈ Rmi×pi, V ∈ Rm×p consisting of orthonor-
√>√^)T
mal columns. Hence, we can write Pcol(U>A2) = VVTwhere V = diag(Vi, . . . , Vr, V). We then
conclude that Pcol(A2) = U Pcol(U>A2)UT = UVb VbTUT. Thus, A2 has the same column space
as UV, and there must exist an invertible matrix C ∈ Rd1×d1 such that A2 = U[V, 0]C, where
0 ∈ Rd2×(d1-rank(A2)) is a zero matrix. Denoting V = [Vb , 0], we conclude that A2 = UVC.
Then, plugging a2 = C-1V>U> into eq.(25) yields the desired form of Ai.
We now prove eq. (3). Note that the above proof is based on the equations Vai L = 0, (Va? L)AT =
0. Hence, the forms of Ai, A2 in eqs. (1) and (2) need to further satisfy Va?L = 0. By eq. (27)
and the form of A2, we obtain that A2AiX = Pcol(A2)YXtX = UV(UV)tYXtχ. This
expression, together with the form of Ai in eq. (1), implies that
A2AiXXTATi = UV (UV)TYXtXXTATi
(=i) UV (UV)TYXTATi
= UV(UV)TΣUV(CT)-i + UV(UV)TYXTLTi
- UV (UV)TYXTLTiCTVTV (CT)-i
= UVVTΛV(CT)-i + UV(UV)TYXTLTi(I - CTVTV(CT)-i)
(=ii) UΛV(CT)-i + UV(UV)TYXTLTi(I - CTVTV(CT)-i),
where (i) uses the fact that XtXXT = XT, (ii) uses the fact that the block pattern of V is compat-
ible with the multiplicity pattern of the singular values in Λ, and hence VVTΛV = ΛV. On the
other hand, we also obtain that
Y XTATi = ΣUV(CT)-i + Y XTLTi(I - CTVTV(CT)-i)
= UΛV(CT)-i + Y XTLTi(I - CTVTV(CT)-i).
Thus, to satisfy VA2L = 0 in eq. (24), we require that
(I - UV(UV)T)YXTLTi(I - CTVTV(CT)-i) = 0,
which is equivalent to
(I - UV(UV)T)YXTLTiCT(I - VTV) = 0.
Lastly, note that (I - UV (UV)T) = Pcol(UV )⊥, and (I - VTV) = Pker(V ), which concludes
the proof.
Proof of Proposition 1
By expansion we obtain that L = ɪTr(YYt) 一 Tr(A2AiXYt) + 2Tr(A2 AIXXTATAT). Con-
sider any (Ai, A2) that satisfies eq. (23), we have shown that such a point also satisfies eq. (27),
13
which further yields that
L = 2Tr(YY>) - Tr(A2A1XY>) + 2Tr(A2A1XX>A[A>)
=2Tr(YY>) - Tr( Pcol(A2)Σ) + 2Tr( Pcol(A2)Σ Pcol(A2))
=2 Tr(YY>) — 2 3 Pcol(A2)∑)
(=) 1 Tr(YYT)- 2Tτ( Pcol(U>A2)Λ)	(28)
where (i) follows from the fact that Tr( Pcol(A2)Σ Pcol(A2)) = Tr( Pcol(A2)Σ), and (ii) uses the
fact that Pcol(A2) = U Pcol(U>A2)UT. In particular, a critical point (A1, A2) satisfies eq. (28).
Moreover, using the form of the critical point A2 = U V C, eq. (28) further becomes
L = 2Tr(YYT)- 2Tr( Pcol(VC)Λ)
=) 1 Tr(YYT) - 2Tr(V>ΛV)
r
(=) 1 Tr(YYT)- 2 Xpiσi,
i=1
where (i) is due to Pcol(VC) = Pcol(V ) = VVT, and (ii) utilizes the block pattern of V and the
multiplicity pattern of Λ that are specified in Theorem 1.
Proof of Proposition 2
(1): Consider a critical point (A1, A2) with the forms given by Theorem 1. By choosing L1 = 0,
the condition in eq. (3) is guaranteed. Then, we can specify a critical point with any V that satisfies
the block pattern specified in Theorem 1, i.e., We can choose any Pi, i = 1,... ,r,p such that
Pi ≤ mi for i = 1,..., r, P ≤ m and P；=iPi + P = rank(A2). Suppose that (Ai, A2) is a
global minimizer, Proposition 1 gives that L(Ai, A2) = 1 Tr(YYT) - 11 P；=iPiσi. Under the
condition that min{d2, d1} ≤ ir=1 mi , the global minimum value is achieved by a full rank A2
With rank(A2) = min{d2, d1} and P1 = m1, . . . , Pk-1 = mk-1, Pk = rank(A2) - Pik=-11 mi ≤
mk for some k ≤ r. That is, the singular values are selected in a decreasing order to minimize the
function value.
(2): If (A2, A1) is a global minimizer and min{dy, d} > Pir=1 mi, the global minimum can be
achieved by choosing Pi = mi for all i = 1,..., r and P ≥ 0. In particular, we do not need a full
rank A2 to achieve the global minimum. For example, We can choose rank(A2) = Pir=1 mi <
min{dy, d} with Pi = rn^ for all i = 1,..., r andP = 0.
Proof of Proposition 3
We first prove item 1. Consider a non-optimal-order critical point (A1, A2). By Theorem 1, we
can write A2 = UVC where V = [diag(¼,..., Vr, V), 0] and Vi, i = 1,... ,r, V consist of
orthonormal columns. Define the orthonormal block diagonal matrix
sT:=diag(]Vm ,…,]Vrj ,图)，	(29)
where the matrices Oi,…，Or, O are such that each diagonal block forms an orthonormal sub-
matrix. By construction we have StV = [diag(I m1×p1,...,I mr ×pr, Im ×p), 0], where I mk ×pk
corresponds to the first Pk columns of the identity matrix Imk×mk . Then, A2 can be alternatively
written as A2 = USSTVC. Also, denote the columns of US as
U S = [us11 , . . . , us1p1 , . . . , usr1, . . . , urpr ,u 1,..., up].
Since (A1, A2) is a non-optimal-order critical point, there exists 1 ≤ i < j ≤ r such that Pi < mi
and Pj > 0. Then, consider the following perturbation of US for some > 0.
Mf = [u1ι,..., u1p1,..., uj1√1+pi+1),... U 1,..., U p],
14
>
with which we further define the perturbation matrix A2 = M S>V C. Also, let the pertur-
>
bation matrix Ai be generated by eq. (1) With U J M and V J S>V. Note that With
this construction, (A1, A2) satisfies eq. (25), Which further implies eq. (27) for (A1, A2), i.e.,
A2A1X = Pcol(A2)YX'X. Thus, eq. (28) holds for the point (Ai, A2), and we obtain that
L(A2, Ai) = 1 Tr(YYT)- 2Tr(Pcol(U段/)
=1 Tr(YY>) - 2Tr( Pcol(SS)STAS)
=1 Tr(YYT)- 2Tr( Pcol(S>u>Ms>v)S>AS)
=1Tr(YYT)- 2Tr(Pcol(S>u>Ms>v)A)，
Where the last equality uses the fact that STΛS = Λ, as can be observed from the block pattern of
T
S and the multiplicity pattern of Λ. Also, by the construction of M and the form of STV, a careful
calculation shoWs that only the i, j-th diagonal elements of Pcol(S>U>MfS>V) have changed, i.e.,
Pcol(S>U>MfS>V) k
ifk=i
ifk=j
As the index i, j correspond to the singular values σi , σj , respectively, and σi > σj , one obtain that
2
L(A2, AI) = L(A2, AI)- 1+e2 (σi - σj ) < L(A2, A1).
Thus, the construction of the point (A2, Ai) achieves a loWer function value for any > 0. Letting
→ 0 and noticing that M is a perturbation of US, the point (A2, Ai) can be in an arbitrary
neighborhood of (A2, Ai). Lastly, note that rank(A2) = rank(A2). This completes the proof of
item 1.
Next, We prove item 2. Consider an optimal-order critical point (Ai, A2). Then, A2 must be
non-full rank, since otherWise a full rank A2 With optimal order corresponds to a global minimizer
by Proposition 2. Since there exists some k ≤ r such that pi = mi, . . . , pk-i = mk-i,pk =
rank(A2) - Pik=-ii mi ≤ mk, the necessary form of A2 gives that A2 = UVC With V =
[diag(Vi, . . . , Vk), 0] := [Vdiag, 0]. Using this expression, eq. (1) yields that
Ai = C-i
(UKiag)TYXt
0
+ CLi -
(CLi)i:rank(A2),:XXt
0
We noW specify our perturbation scheme. Recalling the orthonormal matrix S defined in eq. (29).
Then, We consider the folloWing matrices for some i , 2 > 0
k-i
Ae2 = [U Vdiag , 2US:,q, 0]C, Where q =	mi +pk + 1,
i=i
(UVdiag)TYXt
0
+CLi-
(CLi)i:rank(A2),:XXt
i(US:,q)TYXt
0
Our goal is to shoW that L(Ai, A2) < L(Ai, A2) for i, 2 → 0. For this purpose, We need to
utilize the condition of critical points in eq. (3), Which can be equivalently expressed as
(I - UV(UV)T)YXTLTiCT(I - VTV) = 0
⇔(i)	(CLi)(rank(0A2)+i):d1,:X YT(I - UV(UV)T) = 0
⇔ (CLi)(rank(A2)+i):d1,:XYT(I - UV(UV)T) =0	(30)
(⇔ii) (CLi)(rank(A2)+i):d1,:XYT(I - US:,i:(q-i)(US:,i:(q-i))T) =0	(31)
15
where (i) follows by taking the transpose and then simplifying, and (ii) uses the fact that V =
SS>V = S:,1:(q-1) in the case of optimal-order critical point. Calculating the function value at
/ 、 ......
(A1, A2), we obtain that
L(Al, A2) = 2 k UVdiag(UVdiag)>YXtX
X------------------------{----------}
Q
+ 2US:,q(CL1)(rank(A2)+1),:X + 12US:,q(US:,q)>YXtX -Yk2F
---------------------------V------------------------}
P
=L(Aι, A2) + 1 [Tr(PP>) + 2Tr(PQ>) - 2Tr(PY>)].
We next simplify the above three trace terms using eq. (31). For the first trace term, observe that
Tr(P P>) = Tr(22US:,q(CL1)(rank(A2)+1),:XX>(CL1)>(rank(A2)+1),:(US:,q)>)
+ 2Tr(122US:,q(CL1)(rank(A2)+1),:XY>US:,q(US:,q)>)
+ Tr(e2e2US,q (US：,q )>∑US,q (US：,q )>)
(=i) Tr(22US:,q(CL1)(rank(A2)+1),:XX>(CL1)>(rank(A2)+1),:(US:,q)>)
+ Tr(e2e2US,q (US：,q )>∑US,q (US：,q )>)
=e2TT((CLl)(rank(A2)+ 1),：XX>(CLl)>rank(A2)+1),:) + 多2»区£九)
where (i) follows from eq. (31) as S:,q is orthogonal to the columns of S:,1:(q-1). For the second
trace term, we obtain that
2Tr(P Q>) = 2Tr(2US:,q(CL1)(rank(A2)+1),:XY>UVdiag(UVdiag)>)
+ 2Tr(χ2US,q (US：,q )>ΣUVdiag (UVdiag)>)
= 2Tr(2US:,q(CL1)(rank(A2)+1),:XY>UVdiag(UVdiag)>)
+ 2Tr(e1e2US"qS>qΛSS>Vdiag(UVdiag )>)
(=i) 2Tr(2US:,q(CL1)(rank(A2)+1),:XY>UVdiag(UVdiag)>)
+ 2Tr(e1e2σkUS,q e>S>Vdiag (UVdiag )>)
(=ii) 2Tr(2US:,q(CL1)(rank(A2)+1),:XY>UVdiag(UVdiag)>),
where (i) follows from S>gΛS = σk e>, and (ii) follows from e>S>Vdiag = 0. For the third trace
term, we obtain that
2Tr(PY >) = 2Tr(e2US,q (CL1)(rank(A2)+i),: XYT) + 2Tr(e1e2US,q (US：,q )>Σ)
=2Tr(e2US,q (CL1)(rank(A2)+i),: XY>) + 2Tr(eιe2S>qΛS,q).
Combining the expressions for the three trace terms above, we conclude that
2 [Tr(PP>) + 2Tr(PQ>) - 2Tr(PY>)]
=1 c2T⅛((CLl)(rank(A2) + 1),:XXT(CLl)>rank(A2) + 1),:)
+ (1 e2e2 -53Tr(STqʌs,g)
+22Tr(US:,q(CL1)(rank(A2)+1),:XYT[UVdiag(UVdiag)T-I])
-1 e2 Tr((CLI) (rank(A2)+l),:XXT(CLI)Trank(A2)+1),)
+ ( 1 e2e2 - ElE2)Tr(STqʌs,g)
=2E2Tr((cLl)(rank(A2) + 1),:XXT(CLI)Trank(A2) + 1),:) + (2e2e2 - e1e2)σk,
where (i) follows from eq. (30). Note that the first term in the last equation is nonnegative. Now,
letting 2 = 12 → 0, the overall perturbation of the function value becomes
2[Tt(PPt) + 2Tt(PQt) - 2Tr(PYt)] = O(∈4) - O(∈3) < 0.
16
Thus, the constructed perturbation (A1 , A2) achieves a lower function value, and it can be in an
arbitrary neighborhood of (A1, A2) as 2 = 12 → 0. Lastly, note that rank(A2) = rank(A2) + 1.
Next, we prove item 3. We first introduce a technical lemma. Consider row vectors a>, y> with
a> = 0. Define the scalar function f (α) = 2 ∣∣ɑa> - 丫>卜.Then, f (α) is strongly convex as
f00(α) = kak22 > 0. Thus, the following fact holds due to strong convexity.
Fact 1. For any α ∈ R, we can identify a perturbation αe such that f (αe) > f (α).
Now consider any point (A1, A2) ∈ X. Since A2A1X 6= 0, then there exists a certain row,
say, the i-th row (A2)i,:A1X, that is nonzero. We then apply the above fact with α = 1, a> =
(A2)i,:A1X, y> = Yi,:, and conclude that one can find a perturbation αe that achieves a higher
function value. Equivalently, one can treat the perturbation of α as the perturbation of (A2)i,:, i.e.,
define the perturbation (A2)i,: := αe(A2)i,:.
Proof of Theorem 3
We first derive the forms of the parameter matrices. Consider a critical point (Ai,..., a`) of LD.
By definition of the critical point, We have ^Ak LD = 0 for all k = 1,...,', which implies that
A>',k+ιA(',k+i) Ak A(k-ι,i)X (A(k-ι,i)X )> = A>',k+υYX>A>k-ι,υ.	(32)
Solving this linear system of Ak, we obtain that, for some Lk ∈ Rdk×dk-1
Ak = A；',k+i)Y(A(k-ι,i)X)t + Lk- A[',k+i)A(e,k+"kA(k-ι,i)X(A(k-ι,i)X)t	(33)
Multiplying eq. (33) on both sides by A%k+i) on the left and A(k-ι,i)X on the right and then
simplifying, we obtain that for all k = 1, . . . , ` - 1
A(',1)X = Pcol(A(',k+ι))Y (A(k-1,1)X )tA(k-i,i)X.	(34)
On the other hand, applying eq. (32) with k = ' and multiplying both sides by A> on the right, one
obtains that
A(',i)XX>A>',i) = YX>A>',i),	(35)
which, together with eq. (34), further implies that for all k = 1, . . . , ` - 1
Pcol(A(',k+i))£k-1 Pcol(A(',k+ι)) = ς1 Pcol(A(',k+1)).	(36)
Following the same argument as that in the proof of Theorem 1, we conclude that A(',k+i)=
Uk-1Vk-1Ck-1, where Uk-1, Vk-1, Ck-1 satisfy the conditions that are stated in the theorem.
Then, plugging the expression of A(',k+i) in eq. (33), one obtains the form of Ak in eq. (4).
We next prove the conditions in eq. (6). Note that the first condition is simply a consistency con-
dition on the matrix products. This is because eq. (36) only provides the forms of the matrices
A(',k+i), k = 1,..., l - 1, which must factorize into the product of individual matrices. For the
other condition in eq. (6), note that the proof of eq. (35) uses the weaker condition (Va' LD )A> = 0
than the original condition V a` LD = 0 of the critical point. Thus, the forms of the parameter ma-
trices must also satisfy va`LD = 0, i.e., A%i)X(A('-1,1)X)> = YX>A>e-ι i). Then, plugging
eq. (34) in the above condition and simplifying, one obtains eq. (6).
Proof of Proposition 4
Note that by expansion LD = ɪTr(YYT) - Tr(A(',i)XYT) + ɪTr(A(',i)XX>A>' 短.For any
Ai,..., a` that satisfy eq. (32), we have shown that they must satisfy eq. (34) with k = 1, with
which we further obtain that
L = 2Tr(YY>) - 1 Tr(Pcol(A”)∑o)
=2Tr(YY>) - 1 Tr(Pcol(u>a(',2))λo).
(37)
17
Consider a critical point (Ai,..., a`) so that eq. (37) holds. Using the form of critical points
A(',2) = UoVoCo, eq. (37) further becomes
L = 2 Tr(YY>) — 2 Tr( Pcol(V0C0)Λ0)
=1 Tr(YY>) - 1 Tr(V>ΛoV0)
i	r(o)
=1 Tr(YY>) — 1 ∑Pi(O)σi(0),
i=1
where (i) utilizes the block pattern of Vo and the multiplicity pattern of Λo that are specified in
Theorem 3.
Proof of Proposition 5
Observe that the product matrix A(',2)is equivalent to the class of matrices B? ∈ Rmin{d',...,d2}×dι.
Consider a critical point (B2, Ai) of the shallow linear network L := 2 ∣∣B2A1X 一 Y∣∣f. By
Proposition 2, we conclude that
•	If min{d',...,dι} ≤ P；=0) mi(0), then (Ai, B2) is a global minimizer if and only if B2
is full rank and pi = mi (0), . . . , pk-i = mk-i(0), pk = rank(B2) — Pik=-ii mi(0) ≤
mk(0) for some k ≤ r(0);
•	If min{d',...,dι} > Pr=Oi)mi (0), then (Ai, B2) is a global minimizer if and only if
Pi = mi(0) for all i = 1,..., r(0) and p(0) ≥ 0. In particular, B2 can be non-full rank
with rank(B2) = Pir=(oi) mi (0).
Note that LD achieves the same global minimum as L. Hence Proposition 1 and Proposition 4 must
match, which yields that Pir=(oi) piσi(0) = Pir=(oi) pi(0)σi(0). We then conclude that pi(0) = pi for
i = 1,..., r(0) as σι(0) > .… > °r(o)(0). ThiS proves the proposition.
Proof of Theorem 4
The proof is similar to that for shallow linear networks. Consider a deep-non-optimal-order critical
point (Ai,..., a`), and define the orthonormal block matrix Sk using the blocks of Vk ina similar
way as eq. (29). Then, A(l,k+2) takes the form A(l,k+2) = UkSkS>kVkCk. Since A(l,k+2) is of
non-optimal order, there exists i < j < r(k) such that pi(k) < mi(k) and pj (k) > 0. Thus,
us +us
We perturb the j-th column of UkSk to be ——√i+pi()+ ), and denote the resulting matrix as Mk.
>>
Then, we perturb a` to be a` = Mk (UkSk )>A' so that A'A('-i,k+2)= MkS>VkCk. Moreover,
>
we generate Ak+i by eq. (4) with Uk J Mk, Vk J SkVk. Note that such construction satisfies
eq. (32), and hence also satisfies eq. (34), which further yields that
A'A('-i,k+2)Ak + iA(k,i)X = Pcol(A'Adk+^Y (A(k,i)X )t A(k,i)X.
With the above equation, the function value at this perturbed point is evaluated as
L(A',..., Ak+i,...) = 2Tr(YY>) - iTr(Pcol(S>u>Mks>Vk)Ak).
Then, a careful calculation shows that only the i, j-th diagonal elements of Pcol(S>U>Mf S>V )Λ
2	i	kk k
have changed, and are ɪ+^, ɪ+^, respectively. We then conclude that
2
L(Ai,..∙, Ak+i, ..., Ag) = L(Ai,..∙, Ag)- i+e2 (σi(k) 一 σj(k)) < L(Ai,..., Ag).
Now consider a deep-optimal-order critical point (Ai,..., Ag). Note that with A(g-2,i) fixed to be
a constant, the deep linear network reduces to a shallow linear network with parameters (Ag, Ag-i).
Since (Ag, Ag-i) is not a non-global minimum critical point of this shallow linear network and Ag
18
is of optimal-order, we can apply the perturbation scheme in the proof of Proposition 3 to identify a
perturbation (a`, A'-i) With rank(A') = rank(A') + 1 that achieves a lower function value.
Consider any point in XD. Since A(',i)X = 0, we can scale the nonzero row, say, the i-th row
(A')i,：A('-1,1)X properly in the same way as that in the proof of Proposition 3 to increase the
function value. Lastly, item 1 and item 2 imply that every local minimum is a global minimum for
these two types of critical points. Moreover, combining items 1,2 and 3, we conclude that every
critical point of these two types in XD is a saddle point.
19