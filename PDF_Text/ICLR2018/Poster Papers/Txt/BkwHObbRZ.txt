Published as a conference paper at ICLR 2018
Learning One-hidden-layer Neural Networks
with Landscape Design
Rong Ge
Computer Science Department
Duke University
rongge@cs.duke.edu
Jason D. Lee
Data Sciences and Operations Department,
University of Southern California
jasonlee@marshall.usc.edu
Tengyu Ma
Facebook AI Research
tengyuma@cs.stanford.edu
Ab stract
We consider the problem of learning a one-hidden-layer neural network: we
assume the input x ∈ Rd is from Gaussian distribution and the label y =
a>σ(Bx) + ξ, where a is a nonnegative vector in Rm with m ≤ d, B ∈ Rm×d
is a full-rank weight matrix, and ξ is a noise vector. We first give an analytic for-
mula for the population risk of the standard squared loss and demonstrate that it
implicitly attempts to decompose a sequence of low-rank tensors simultaneously.
Inspired by the formula, We design a non-convex objective function G(∙) whose
landscape is guaranteed to have the following properties:
1.	All local minima of G are also global minima.
2.	All global minima of G correspond to the ground truth parameters.
3.	The value and gradient of G can be estimated using samples.
With these properties, stochastic gradient descent on G provably converges to
the global minimum and learn the ground-truth parameters. We also prove finite
sample complexity results and validate the results by simulations.
1	Introduction
Scalable optimization has played an important role in the success of deep learning, which has im-
mense applications in artificial intelligence. Remarkably, optimization issues are often addressed
through designing new models that make the resulting training objective functions easier to be
optimized. For example, over-parameterization (Livni et al., 2014), batch-normalization (Ioffe &
Szegedy, 2015), and residual networks (He et al., 2016a;b) are often considered as ways to improve
the optimization landscape of the resulting objective functions.
How do we design models and objective functions that allow efficient optimization with guarantees?
Towards understanding this question in a principled way, this paper studies learning neural networks
with one hidden layer. Roughly speaking, we will show that when the input is from Gaussian
distribution and under certain simplifying assumptions on the weights, we can design an objective
function G(∙), such that
[a]	all local minima of G(∙) are global minima
[b]	all the global minima are the desired solutions, namely, the ground-truth parameters (up to per-
mutation and some fixed transformation).
We note that designing such objective functions is challenging because 1) the natural `2 loss objec-
tive does have bad local minimum, and 2) due to the permutation invariance1, the objective function
inherently has to contain an exponential number of isolated local minima.
1Permuting the rows of B? and the coordinates of a? correspondingly preserves the functionality of the
network.
1
Published as a conference paper at ICLR 2018
1.1	Setup and known issues with proper learning
We aim to learn a neural network with a one-hidden-layer using a non-convex objective function.
We assume input x comes from Gaussian distribution and the label y comes from the model
y = a*>σ(B*x) + ξ	(1.1)
where a? ∈ Rm, B? 〜 Rm×d are the ground-truth parameters, σ(∙) is a element-wise non-linear
function, and ξ is a noise vector with zero mean. Here we can without loss of generality assume x
comes from spherical Gaussian distribution N(0, Idd×d). 2
For technical reasons, we will further assume m ≤ d and that a? has non-negative entries.
The most natural learning objective is perhaps the `2 loss function, given the additive noise. Con-
cretely, We can parameterize with training parameters a ∈ Rm, B 〜Rm×d of the same dimension
as a? and B? correspondingly,
y = a>σ(Bx),	(1.2)
and then use stochastic gradient descent to optimize the `2 loss function. In many parts of our paper,
we consider σ to be the ReLU function σ(x) = max{x, 0}. In such settings we assume rows of B?
have norm 1 because a? and rows of B? can be scaled simultaneously without changing the model
or the objective.
When we have enough training examples, we are effectively minimizing the following population
risk with stochastic updates,
f(a,B) = E [ky - yk2] .	(1.3)
However, empirically stochastic gradient descent cannot converge to the ground-truth parameters
in the synthetic setting above when σ(x) = ReLU(x) = max{x, 0}, even if we have access to an
infinite number of samples, and B? is a orthogonal matrix. We also show this phenomena gener-
alizes to the case when σ(x) is the sigmoid function and the learned network also have the same
architecture. Such empirical results have been reported in Livni et al. (2014) previously, and we also
provide our version in Figure 1 and Figure 2 of Section 4. This is consistent with observations and
theory that over-parameterization is important for training neural networks successfully (Livni et al.,
2014; Hardt et al., 2016; Soudry & Carmon, 2016).
These empirical findings suggest that the population risk f(a, B) has spurious local minima with
inferior error compared to that of the global minimum. This phenomenon occurs even if we assume
we know a? or a? = 1 is merely just the all one’s vector. Empirically, such landscape issues seem to
be alleviated by over-parameterization. By contrast, our method described in the next section does
not require over-parameterization and might be suitable for applications that demand the recovery
of the true parameters.
1.2	Our contributions
Towards learning with the same number of training parameters as the ground-truth model, we first
study the landscape of the population risk f (∙) and give an analytic formula for it ——as an explicit
function of the ground-truth parameters and training parameters with the randomness of the data
being marginalized out. The formula in equation (2.3) shows that f (∙) is implicitly attempting to
solve simultaneously an infinite number of low-rank tensor decomposition problems with commonly
shared components.
Inspired by the formula, we design a new training model whose associated loss function —— named
f0 and formally defined in equation (2.5) —— corresponds to the loss function for decomposing a
matrix (2-nd order tensor) and a 4-th order tensor (Theorem 2.2). Empirically, stochastic gradient
descent on f0 learns the network as shown Section 4.
Despite the empirical success of f0, we still lack a provable guarantee on the landscape of f0 . The
second contribution of the paper is to design a more sophisticated objective G(∙) whose landscape
2This is because if X 〜 N(0, Σ), then we can whiten the data by taking x0 = Σ-1∕2x and define B?0 =
BΣ1/2. We note that B？0x0 = Bx and therefore we maintain the functionality of the model.
2
Published as a conference paper at ICLR 2018
is Provably nice — all the local minima of G(∙) are proven to be global, and they correspond to the
permutation of the true parameters. See Theorem 2.3.
Moreover, the value and the gradient of G can be estimated using samples, and there are no con-
straints in the optimization. These allow us to use straightforward SGD (see guarantees in Ge et al.
(2015); Jin et al. (2017)) to optimize G(∙) and converge to a local minimum, which is also a global
minimum (Corollary 2.4).
Finally, we also prove a finite-sample complexity result. We will show that with a polynomial
number of samples, the empirical version of G share almost the same landscape properties as G
itself (Theorem 2.7). Therefore, we can also use an empirical version of G as a surrogate in the
optimization.
1.3	Related work
The work of Arora et al. (2014) is one of the early results on provable algorithms for learning
deep neural networks, where the authors give an algorithm for learning deep generative models with
sparse weights. Livni et al. (2014), Zhang et al. (2016; 2017b), and Daniely et al. (2016) study
the learnability of special cases of neural networks using ideas from kernel methods. Janzamin
et al. (2015) give a polynomial-time algorithm for learning one-hidden-layer neural networks with
twice-differentiable activation function and known input distributions. Their approach uses the idea
of score function to estimate the high order tensors related to the true components, and then apply
tensor decompositions to recover the true parameters. When applied to Gaussian input distribution,
the score function becomes Hermite polynomials.
A series of recent papers study the theoretical properties of non-convex optimization algorithms
for one-hidden-layer neural networks. Brutzkus & Globerson (2017) and Tian (2017) analyze
the landscape of the population risk for one-hidden-layer neural networks with Gaussian inputs
under the assumption that the weights vector associated to each hidden variable (that is, the filters)
have disjoint supports. Li & Yuan (2017) proves that stochastic gradient descent recovers the
ground-truth parameters when the parameters are known to be close to the identity matrix. Zhang
et al. (2017a) study the optimization landscape of learning one-hidden-layer neural networks with a
specific activation function, and they design a specific objective function that can recover a single
column of the weight matrix. Zhong et al. (2017) study the convergence of non-convex optimization
from a good initializer that is produced by tensor methods. Our algorithm works for a large family
of activation functions (including ReLU) and any full-rank weight matrix. To our best knowledge,
we give the first global convergence result for gradient-based methods for our general setting. 3
The optimization landscape properties have also been investigated on simplified neural networks
models. Kawaguchi (2016) shows that the landscape of deep neural nets does not have bad local
minima but has degenerate saddle points. Hardt & Ma (2017) show that re-parametrization using
identity connection as in residual networks He et al. (2016a) can remove the degenerate saddle points
in the optimization landscape of deep linear residual networks. Soudry & Carmon (2016) show that
an over-parameterized neural network does not have bad differentiable local minimum. Hardt et al.
(2016) analyze the power of over-parameterization in a linear recurrent network (which is equivalent
to a linear dynamical system.)
The optimization landscape has also been analyzed for other machine learning problems, including
SVD/PCA phase retrieval/synchronization, orthogonal tensor decomposition, dictionary learning,
matrix completion, matrix sensing Baldi & Hornik (1989); Srebro & Jaakkola (2013); Ge et al.
(2015); Sun et al. (2015); Bandeira et al. (2016); Ge et al. (2016); Bhojanapalli et al. (2016); Ge
et al. (2017). Our analysis techniques build upon that for tensor decomposition in Ge et al. (2015)
— we add two additional regularization terms to deal with spurious local minimum caused by the
weights a? and to remove the constraints.
Notations: We use ∣∣∙∣∣ to denote the Euclidean norm of a vector and spectral norm of a matrix. We
use k∙∣∣F to denote the FrobeniUS/Euclidean norm of a matrix or high-order tensor. For a vector x,
let kxk0 denotes its infinity norm and for a matrix A, let |A|0 be a shorthand for kvec(A)k0 where
vec(A) is the vectorization of A.
3The work of Janzamin et al. (2015); Zhong et al. (2017) are closely related, but they require tensor decom-
position as the algorithm/initialization.
3
Published as a conference paper at ICLR 2018
We use A 0 B to denote the Kronecker product of A and B, and A0k is a shorthand for A 0∙∙∙0 A
where A appears k times. For vectors a 0 b and a0k denote the tensor product. We denote the
identity matrix in dimension d × dby Idd×d, or Id when the dimension is clear from the context. We
will define other notations when we first use them.
2 Main Results
2.1	CONNECTING `2 POPULATION RISK WITH TENSOR DECOMPOSITION
We first show that a natural `2 loss for the one-hidden-layer neural network can be interpreted as
simultaneously decomposing tensors of different orders.
A straightforward approach of learning the model (1.1) is to parameterize the prediction by
y = a>σ(Bx),	(2.1)
where a ∈ Rd, B 〜 Rm×d are the training parameters. Naturally, We can use '2 as the empirical
loss, which means the population risk is
f(a,B) = E [ky - yk2] .	(2.2)
Throughout the paper, we use b?1>, . . . , b?m> to denote the row vectors of B? and similarly for B.
^b> 1	Γ b?> ^
That is, we have B =	.	and B? =	.	. Let a% and a?’S be the coordinates of a and a?
b>m	b?m>
respectively.
We give the following analytic formula for the population risk defined above.
Theorem 2.1.	Assume vectors bi , bi? ’s are unit vectors. Then, the population risk f defined in
equation (2.2) satisfies that
f (a,B) = Eσ2
k∈N
2
X a?b誉k- X aibfk	+ const.
i∈[m]	i∈[m]	F
(2.3)
where σk is the k-th Hermite coefficient of the function Q. See Section A.1 for a short introduction
of Hermite polynomial basis. 4
Connection to tensor decomposition: We see from equation (2.3) that the population risk of f is
essentially an average of infinite number of loss functions for tensor decomposition. For a fixed
k ∈ N, we have that the k-th summand in equation (2.3) is equal to (up to the scaling factor Qk)
fk , kTk- EaibfδkkF .	(2.4)
i∈[m]
where Tk = Pi∈[m] ai?bi?fk is a k-th order tensor in (Rd)fk. We note that the objective fk naturally
attempts to decompose the k-order rank-m tensor Tk into m rank-1 components a1bifk, . . . , ambfmk.
The proof of Theorem 2.1 follows from using techniques in Hermite Fourier analysis, which is
deferred to Section A.2.
Issues with optimizing f:. It turns out that optimizing the population risk using stochastic gradi-
ent descent is empirically difficult. Figure 1 shows that in a synthetic setting where the noise is zero,
the test error empirically doesn’t converge to zero for sufficiently long time with various learning
rate schemes, even if we are using fresh samples in iteration. This suggests that the landscape of the
population risk has some spurious local minimum that is not a global minimum. See Section 4 for
more details on the experiment setup.
4 When σ = ReLU, we have that σo = —=, σι = 1. For n ≥ 2 and even, σn = ((n—3)!!) . For n ≥ 2
2π	2	2πn!
and odd, σn = 0.
4
Published as a conference paper at ICLR 2018
An empirical fix:. Inspired by the connection to tensor decomposition objective described earlier
in the subsection, we can design a new objective function that takes exactly the same form as the
tensor decomposition objective function f2 + f4. Concretely, let's define y0 = a>γ(Bx) where
Y = σ2h2 + σ4h4 and h2(t) = √12(t2 - 1) and h4(t) = √4(t4 - 6t2 + 3) are the 2nd and 4th
normalized probabilists’ Hermite polynomials Wikipedia (2017a). We abuse the notation slightly
by using the same notation to denote the its element-wise application on a vector. Now for each
example We use ∣∣y0 - yk2 as loss function. The corresponding population risk is
f0(a,B)= E [ky0-yk2].	(2.5)
Now by an extension of Theorem 2.1, we have that the new population risk is equal to the σ2 f2 +
σ4f4.
Theorem 2.2.	Let f0 be defined as in equation (2.5) and f2 and f4 be defined in equation (2.4).
Assume bi , bi? ’s are unit vectors. Then, we have
f0 = σ2f2 + σlf4 + const	(2.6)
It turns out stochastic gradient descent on the objective f0(a, B) (with projection to the set of matri-
ces B with row norm 1) converges empirically to the ground truth (a?, B?) or one of its equivalent
permutations. (See Figure 3.) However, we don’t know of any existing work for analyzing the
landscape of the objective f0 (or fk for any k ≥ 3). We conjecture that the landscape of f0 doesn’t
have any spurious local minimum under certain mild assumptions on (a?, B?). Despite recent at-
tempts on other loss functions for tensor decomposition Ge & Ma (2017), we believe that analyzing
f0 is technically challenging and its resolution will be potentially enlightening for the understanding
landscape of loss function with permutation invariance. See Section 4 for more experimental results.
2.2	LANDSCAPE DESIGN FOR ORTHOGONAL B?
The population risk defined in equation (2.5) — though works empirically for randomly gener-
ated ground-truth (a?, B?) — doesn’t have any theoretical guarantees. It’s also possible that when
(a? , B? ) are chosen adversarially or from a different distribution, SGD no longer converges to the
true parameters.
To solve this problem, we design another objective function G(∙), such that the optimizer of G(∙)
still corresponds to the ground-truth, and G() has provably nice landscape — all local minima of
G() are global minima.
In this subsection, for simplicity, we work with the case when B? is an orthogonal matrix and
state our main result. The discussion of the general case is deferred to the end of this Section and
Section C.
We define our objective function G(B ) as
G(B)，sign*) E y ∙ E	φ(bj ,bk,x) - μ sign%) E y ∙ Edbj ,x)
j,k∈[d],j 6=k	j∈[d]
m
+ λ X(∣bi∣2 - 1)2	(2.7)
i=1
where 夕(∙，∙) is defined as
d(v,x) = 1 Ilvk4 - I(V>x)2kvk2 + U(V>x)4.	(2.8)
8	4	24
and φ(∙, ∙, ∙) is defined as
φ(V,w,X) =1 kvkliwk2 + hv,wi2 - 2Iiwk2(VTx)2 -1 kvk2(WTx)2
+ 2(v>x)(w>x)v>w + ∙∣(v>x)2(w>x)2 .
(2.9)
5
Published as a conference paper at ICLR 2018
The rationale behind of the choices of φ and φ will only be clearer and relevant in later sections.
For now, the only relevant property of them is that both are smooth functions whose derivatives are
easily computable.
We remark that We can sample G(∙) using the samples straightforwardly — it's defined as an average
of functions of examples and the parameters. We also note that only parameter B appears in the loss
function. We will infer the value of a? using straightforward linear regression after we get the
(approximately) accurate value of B? .
Due to technical reasons, our method only works for the case when ai? > 0 for every i. We will
assume this throughout the rest of the paper. The general case is left for future work. Let a?max =
max ai?, a?min = min ai?, and κ? = max ai? / min ai?. Our result will depend on the value of κ?.
Essentially we treat κ? as an absolute constant that doesn’t scale in dimension. The following
theorem characterizes the properties of the landscape of G(∙).
Theorem 2.3. Let c be a sufficiently small universal constant (e.g. c = 0.01 suffices) and sup-
pose the activation function σ satisfies σ4 = 0. Assume μ ≤ c∕κ*, λ ≥。-1。*&乂, and B? is an
orthogonal matrix. ThefUnction G(∙) defined as in equation (2.7) satisfies that
1.	A matrix B is a local minimum ofG if and only ifB can be written as B = DPB ? where
P is a permutation matrix and D is a diagonal matrix with Dii ∈ {±1 ± O(μ°maχ∕λ)}.5
Furthermore, this means that all local minima of G are also global.
2.	Any saddle point B has a strictly negative curvature in the sense that λmin(V2G(B)) ≥
一to where τo = Cmin{μamin∕(κ*d), λ}
3.	Suppose B is an approximate local minimum in the sense that B satisfies
kVG(B)k ≤ ε and λmm(V2G(B)) ≥ -τ°
Then B can be written as B = PDB ? + EB ? where P is a permutation matrix, D is a
diagonal matrix satisfying the same bound as in bullet 1, and ∣E∣∞ ≤ O(ε∕(σ4amin)).
As a direct consequence, B is Od(ε)-close to a global minimum in Euclidean distance,
where Od(∙) hides polynomial dependency on d and other parameters.
The theorem above implies that we can learn B ? (up to permutation of rows and sign-flip) if we
take λ to be sufficiently large and optimize G(∙) using stochastic gradient descent. In this case, the
diagonal matrix D in bullet 1 is sufficiently close to identity (up to sign flip) and therefore a local
minimum B is close to B ? up to permutation of rows and sign flip. The sign of each bi? can be
recovered easily after we recover a (see Lemma 2.5 below.)
SGD converges to a local minimum Ge et al. (2015) (under the additional property as established in
bullet 2 above), which is also a global minimum for the function G(∙). We will prove the theorem
in Section B as a direct corollary of Theorem B.1. The technical bullet 2 and 3 of the theorem is to
ensure that we can use SGD to converge to a local minimum as stated below.5 6
Corollary 2.4. In the setting of Theorem 2.3, we can use stochastic gradient descent to optimize
function G(∙) (withfresh samples at each iteration) and converge to an approximate global minimum
B that is ε-close to a global minimum in time poly(d, 1∕ε).
After approximately recovering the matrix B ?, we can also recover the coefficient a? easily. Note
that fixing B, we can fit a using simply linear regression. For the ease of analysis, we analyze a
slightly different algorithm. The lemma below is proved in Section D.
Lemma 2.5. Given a matrix B whose rows have unit norm, and are δ-close to B ? in Euclidean
distance up to permutation and sign flip with δ ≤ 1∕(2κ?). Then, we can give estimates a, B0 (using
e.g., Algorithm 1) such that there exists a permutation P where ∣∣a — Pa*k∞ ≤ 6。*&乂 and B0 is
row-wise δ-close to PB ?.
The key step towards analyzing objective G(B) is the following theorem that gives an analytic
formula for G(∙).
5MOrePreCisely, |Dii| = q≡==g=)
6In the most general setting, converging to a local minimum of a non-convex function is NP-hard.
6
Published as a conference paper at ICLR 2018
Theorem 2.6. Thefunction G(∙) satisfies
L	一	一	CC	ισ∣μ	一	，	ɪm	CC
G(B) =	2√6∣M∣∙	X a? X hb?,bj i2 hb?,bk i2-*	X a?hb?,bji4 +	λ £(|电『-1)2
i∈[d] j,k∈[d],j6=k	i,j∈[d]	i=1
(2.10)
Theorem 2.6	is proved in Section A. We will motivate our design choices with a brief overview in
Section 3 and formally analyze the landscape of G in Section B (see Theorem B.1).
Finite sample complexity bounds. Extending Theorem 2.3, we can characterize the landscape of
the empirical risk G, which implies that stochastic gradient on G also converges approximately to
the ground-truth parameters with polynomial number of samples.
Theorem 2.7.	In the setting of Theorem 2.3, suppose we use N empirical samples to approximate
G and obtain empirical risk G. There exists a fixed polynomial poly(d, 1∕ε) such that if N ≥
poly(d, 1∕ε), then with high probability the landscape ofG has the properties to that ofG in bullet
2 and 3 of Theorem 2.3.
All of the results above assume that B? is orthogonal. Since the local minimum are preserved by
linear transformation of the input space, these results can be extended to the general case when B?
is not orthogonal but full rank (with some additional technicality) or the case when the dimension is
larger than the number of neurons (m < d). See Section C.
3	Overview: Landscape Design and Analysis
In this section, We present a general overview of ideas behind the design of objective function G(∙).
Inspired by the formula (2.3), in Section 3.1, we envision a family of possible objective functions
for which we have unbiased estimators via samples. In Section 3.2, we pick a specific function that
feeds our needs: a) it has no spurious local minimum; b) the global minimum corresponds to the
ground-truth parameters.
3.1	Which objective can be estimated by samples ?
Recall that in equation (2.2) of Theorem 2.1 we give an analytic formula for the straightforward
population risk f. Although the population risk f doesn’t perform well empirically, the lesson that
we learn from it help us design better objective functions. One of the key fact that leads to the proof
of Theorem 2.1 is that for any continuous and bounded function γ, we have that
E [y ∙ γ(b>x)] = X Yk <^k (X aj?hbj?,biik).
k∈N	j∈[d]
Here σk and Yk are the k-th Hermite coefficient of the function σ and Y. That is, letting hk the k-th
normalized probabilists, Hermite polynomials Wikipedia (2017a) andh•,)be the standard inner
product between functions, we have σk ={hk, σ).
Note that Y can be chosen arbitrarily to extract different terms. For example, by choosing Y = hk,
we obtain that
E [y ∙ hk(b>x)] = σk X a?hb?,biik .	(3.1)
j∈[d]
That is, we can always access functions forms that involves weighted sum of the powers of hbi?, bii,
as in RHS of equation (3.1). Using a bit more technical tools in Fourier analysis (see details in
Section A), we claim that most of the symmetric polynomials over variables hbi?, bji can be estimated
by samples:
Claim 3.1 (informal). For any polynomial p() over a single variable, there exits a corresponding
function φp such that
E [y ∙ φp(B, x)] = X a? XP(Wj, bii)	(3.2)
7
Published as a conference paper at ICLR 2018
Moreover, for an any polynomial q(∙, ∙) over two variables, there exists corresponding φq such that
E [y ∙ φq(B,x)] = X a? X q(hbj,bii, hb?,bii)	(3.3)
We will not prove these two general claims. Instead, we only focus on the formulas in Theorem A.5
and Theorem A.6, which are two special cases of the claims above.
Motivated by Claim A.3, in the next subsection, we will pick an objective function which has no
spurious local minimum among those functional forms on the right-hand sides of equation (3.2)
and (3.3).
3.2	Which objective has no spurious local minima?
As discussed briefly in the introduction, one of the technical difficulties to design and analyze ob-
jective functions for neural networks comes from the permutation invariance — if a matrix B is a
good solution, then any permutation of the rows of B still gives an equally good solution (if we
also permute the coefficients in a accordingly). We only know of a very limited number of objective
functions that guarantee to enjoy permutation invariance and have no spurious local minima Ge et al.
(2015). We start by considering the objective function used in Ge et al. (2015),
min P(B) =XXhbi?,bji2hbi?,bki2
i j6=k
s.t. ∀i ∈ [d], kbik = 1	(3.4)
Note that here we overload the notation by using bi?’s to denote a set of fixed vectors that we wanted
to recover and using bi’s to denote the variables. Careful readers may notice that P(B) doesn’t fall
into the family of functions that we described in the previous section (that is, RHS equation of (3.2)
and (3.3)), because it lacks the weighting ai?’s. We will fix this issue later in the subsection. Before
that we first summarize the nice properties of the landscape of P(B).
For the simplicity of the discussion, let’s assume B? forms an orthonormal matrix in the rest of the
subsection. Then, any permutation and sign-flip of the rows of B? leads to a global minimum of
P(∙) — when B = SQB? with a permutation matrix Q and a sign matrix S (diagonal with ±1), we
have that P(B) = 0 because one of hbi?, bji2 and hbi?, bki2 has to be zero for all i, j, k7).
It turns out that these permutations/sign-flips of B? are also the only local minima8 of function
P(∙). To see this, notice that P(B) is a degree-4 polynomial of B. Thus if we pick an index S and
fix every row except for b§, then P(B) is a quadratic function over unit vector b§ 一 reduces to a
smallest eigenvector problem. Eigenvector problems are known to have no spurious local minimum.
Thus the corresponding function (w.r.t bs) has no spurious local minimum. It turns out the same
property still holds when we treat all the rows as variables and add the row-wise norm constraints.
However, there are two issues with using objective function P(B). The obvious one is that it doesn’t
involve the coefficients ai?’s and thus doesn’t fall into the forms of equation (3.3). Optimistically, we
would hope that for nonnegative ai?’s the weighted version ofP below would also enjoy the similar
landscape property
P0(B) =Xai?Xhbi?,bji2hbi?,bki2
i	j 6=k
When ai?’s are positive, indeed the global minimum of P0 are still just all the permutations of the
B?.9 However, when max ai? > 2 min ai?, we found that P0 starts to have spurious local minima
. It seems that spurious local minimum often occurs when a row of B is a linear combination of a
smaller number of rows of B?. See Section F for a concrete example.
7 Note that B? is orthogonal, and j 6= k
8We note that since there are constraints here, we consider the local minimum on the manifold defined by
the constraints.
9This is the main reason why we require a? ≥ 0.
8
Published as a conference paper at ICLR 2018
Figure 1: Data are generated by a network with ReLU activation without noise. The training model
uses the same architecture. Left: the estimated population risk doesn’t converge to zero. Right: the
parameter error using the surrogate in equation (4.1).
To remove such spurious local minima, we add a regularization term below that pushes each row of
B to be close to one of the rows of B?,
R(B) = -μX a? Xhb?,bji4	(3.5)
We see that for each fixed j, the part in R(B) that involves bj has the form -μ Pi a?〈b?, bji4 =
-μhPi a?b?04, bf4i This is commonly used objective function for decomposing tensor Pi a?b?04.
It’s known that for orthogonal bi?’s, the only local minima are ±b1?, . . . , ±b?d Ge et al. (2015). There-
fore, intuitively R(B) pushes each of the bis towards one of the b?’s. 10 Choosing μ to be small
enough, it turns out that P 0(B) + R(B) doesn’t have any spurious local minimum as we will show
in Section B.
Another issue with the choice ofP0(B) + R(B) is that we are still having a constraint minimization
problem. Such row-wise norm constraints only make sense when the ground-truth B? is orthogo-
nal and thus has unit row norm. A straightforward generalization of P (B) to non-orthogonal case
requires some special constraints that also depend on the covariance matrix B?B?>, which in turn
requires a specialized procedure to estimate. Instead, we move the constraints into the objective
function by considering adding another regularization term that approximately enforces the con-
straints.
It turns out the following regularizer suffices for the orthogonal case: S(B) = λ Pi(kbik2 - 1)2 .
Moreover, we can extend this easily to the non-orthogonal case (see Section C) without estimating
any statistics of B? in advance. We note that S(B) is not the Lagrangian multiplier and it does
change the global minima slightly. We will take λ to be large enough so that kbi k has to be close to
1. As a summary, we finally use the unconstrained objective
minG(B) , P0(B) + R(B) + S(B)
Since R(B) and S(B) are degree-4 polynomials of B, the analysis of G(B) is much more delicate,
and we cannot use much linear algebra as we could for P0(B). See Section B for details.
Finally We note that a feature of this objective G(∙) is that it only takes B as variables. We will
estimate the value of a? after we recover the value of B. (see Section D).
4 Simulation
In this section, we provide simple simulation results that verify that minimizing G(B) with SGD
recovers a permutation of B? ; however, minimizing Equation (2.2) with SGD results in finding
spurious local minima. Based on the formula for the population risk in Equation (2.3), we also
10However, note that R(B) by itself doesn’t work because it does not prevent the solutions where all the bi’s
are equal to the same bj? .
9
Published as a conference paper at ICLR 2018
Φ⊃-Λ> φ≥υθqo
Iterations	X104
Figure 2: Data are generated by a network with sigmoid activation without noise. The training model
uses the same architecture. Left: the estimated population risk doesn’t converge to zero. Right: the
parameter error using the surrogate in equation (4.1).
0.7
0.1
0	1
0.6
0.2
0 0.5
LU
⅞ 0.4
旨
0 0.3
Q,
2	3	4	5
Iterations	xιo4
Figure 3: The labels are generated from a network with ReLU activation. We learn with σ2 h2 + σ4 h4
activation. Left: the test loss subtracted by the theoretical global minimum value. Right: the error
in parameter space measured by equation (4.1)
0.8
J 0.6
2
IlJ
⅛
50.4
⅛'i
B
CL 0.2
3
×105
1	2
Iterations
0
0
verified empirically the conjecture that SGD would successfully recover B? using the activation
functions Y(Z) = σ2h2(z) + σ4h4(z),11 even if the data were generated via a model with ReLU
activation. (See Section 2.1 for the rationale behind such conjectures.)
For all of our experiments, we chose B? = Idd×d with dimension d = 50 and a? = 1 for simplicity,
and the data is generated from a one-hidden-layer network with ReLU or Sigmoid activation without
noise. We use stochastic gradient descent with fresh samples at each iteration, and we plot the
(expected) population error (that is, the error on a fresh batch of examples).
To test whether SGD converges to a matrix B which is equivalent to B? up to permutation of rows,
we use a surrogate error metric to evaluate whether B?-1B is close to a permutation matrix. Given
a matrix Q with row norm 1, let
e(Q) = min{1 - min max |Qij |, 1 - min max |Qij |}.	(4.1)
ij	ji
Then We have that if e (Q) ≤ ε for some ε < 1/3, then it implies that Q is √2ε -close to a permutation
matrix in infinity norm. On the other direction, we know that if e(Q) > ε, then Q is not ε-close to
any permutation matrix in infinity norm. The latter statement also holds when Q doesn’t have row
norm 1.
Figure 1 shows that without over-parameterization, using ReLU as an activation function, SGD
doesn’t converge to zero test error and the ground-truth parameters. We decreased step-size by a
11We also observed that using γ(z) = 1 |z| also works but due to the space limitation We don,t report the
experimental results here.
10
Published as a conference paper at ICLR 2018
-0.1
0
0.2
0.15
Φ
∙≡ 0.1
B
>
≥ 0.05
I
8 0
-0.05
2	4	6	8
Iterations	xi04
86 / 2
Oooo
J0山①EBJBd
2	4	6	8
Iterations	x104
Figure 4: Learning with objective function G(∙). Left: the test loss. Right: the error in parameter
space measured by equation (4.1)
factor of 4 every 5000 number of iterations after the error plateaus at 10000 iterations. For the final
5000 iterations, the step-size is less than 10-9, so we can be confident that the non-zero objective
value is not due to the variance of SGD. We see that none of the five runs of SGD converged to a
global minimum. Figure 2 shows the result for sigmoid activation which is quantitatively similar.
Figure 3 shows that using σ2h2 + σ4h4 as the activation function, SGD with projection to the set of
matrices B with row norm 1 converges to the ground-truth parameters. We also plot the loss function
which converges the value of a global minimum. (We subtracted the constant term in equation (2.6)
so that the global minimum has loss 0.)
Figure 4 shows that using our objective function G(B), the iterate converges to a permutation of
the ground truth matrix B? . The fact that the parameter error goes up and down is not surprising,
because the algorithm first gets close to a saddle point and then breaks ties and converges to a one
of the global minima.
Finally We note that using the loss function G(∙) seems to require significantly larger batch (and
sample complexity) to reduce the variance in the gradients estimation. We used batch size 262144
in the experiment for G(∙). However, in contrast, for the σ2h2 + σ'4h4 We used batch size 8192 and
for relu we used batch size 256.
5 Conclusion
In this paper we first give an analytic formula for the population risk of the standard `2 loss, which
empirically may converge to a spurious local minimum. We then design a novel population loss that
is guaranteed to have no spurious local minimum.
Designing objective functions with well-behaved landscape is an intriguing and potentially fruitful
direction. We hope that our techniques can be useful for characterizing and designing the optimiza-
tion landscape for other settings.
We conjecture that the objective αf2 + βf4 12 has no spurious local minimum when α, β are rea-
sonable constants and the ground-truth parameters are in general position. We provided empirical
evidence to support the conjecture.
Our results assume that the input distribution is Gaussian. Extending them to other input distribu-
tions is a very interesting open problem.
Acknowledgement
We thank Chi Jin for discussions at the beginning of this work. R.G. is funded by NSF CCF-
1704860.
12See equation (2.4) for the definition of fk and Theorem 2.2 for how to access αf2 + βf4 in the setting of
one-hidden-layer neural nets.
11
Published as a conference paper at ICLR 2018
References
Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma. Provable bounds for learning some deep represen-
tations. In International Conference on Machine Learning, pp. 584-592, 2014.
Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from examples
without local minima. Neural networks, 2(1):53-58, 1989.
Afonso S Bandeira, Nicolas Boumal, and Vladislav Voroninski. On the low-rank approach for semidefinite
programs arising in synchronization and community detection. arXiv preprint arXiv:1602.04426, 2016.
Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Global optimality of local search for low rank
matrix recovery. In Advances in Neural Information Processing Systems, pp. 3873-3881, 2016.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian inputs.
arXiv preprint arXiv:1702.07966, 2017.
Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks: The power of
initialization and a dual view on expressivity. In Advances In Neural Information Processing Systems, pp.
2253-2261, 2016.
R. Ge and T. Ma. On the Optimization Landscape of Tensor Decompositions. ArXiv e-prints, June 2017.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle pointsonline stochastic gradient for
tensor decomposition. In Conference on Learning Theory, pp. 797-842, 2015.
Rong Ge, Jason D. Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. Advances in
Neural Information Processing Systems (NIPS), 2016. URL http://arxiv.org/abs/1605.07272.
Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A unified
geometric analysis. arXiv preprint arXiv:1704.00708, 2017.
Moritz Hardt and Tengyu Ma. Identity matters in deep learning. In 5th International Conference on Learning
Representations (ICLR 2017), 2017.
Moritz Hardt, Tengyu Ma, and Benjamin Recht. Gradient descent learns linear dynamical systems. CoRR,
abs/1609.05191, 2016. URL http://arxiv.org/abs/1609.05191.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In
European Conference on Computer Vision, pp. 630-645. Springer, 2016b.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. In International Conference on Machine Learning, pp. 448-456, 2015.
Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of non-convexity: Guaranteed
training of neural networks using tensor methods. arXiv preprint arXiv:1506.08473, 2015.
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle points
efficiently. arXiv preprint arXiv:1703.00887, 2017.
Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information Processing
Systems, pp. 586-594, 2016.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. arXiv
preprint arXiv:1705.09886, 2017.
Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training neural networks.
In Advances in Neural Information Processing Systems, pp. 855-863, 2014.
Song Mei, Yu Bai, and Andrea Montanari. The landscape of empirical risk for non-convex losses. arXiv
preprint arXiv:1607.06534, 2016.
Ryan O’Donnell. Analysis of boolean functions. Cambridge University Press, 2014.
Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees for multi-
layer neural networks. arXiv preprint arXiv:1605.08361, 2016.
12
Published as a conference paper at ICLR 2018
Nathan Srebro and Tommi Jaakkola. Weighted low-rank approximations. In ICML, 2013.
G.W. Stewart and Ji guang Sun. Matrix Perturbation Theory. Computer science and scientific computing.
Academic Press, 1990. ISBN 9780126702309. URL https://books.google.com/books?id=
l78PAQAAMAAJ.
Ju Sun, Qing Qu, and John Wright. When are nonconvex problems not scary? arXiv preprint arXiv:1510.06096,
2015.
Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its applications
in convergence and critical point analysis. arXiv preprint arXiv:1703.00560, 2017.
Wikipedia. Hermite polynomials — wikipedia, the free encyclopedia, 2017a. URL https://en.
wikipedia.org/w/index.php?title=Hermite_polynomials&oldid=796842411. [On-
line; accessed 1-September-2017 ].
Wikipedia. Formal power series — wikipedia, the free encyclopedia, 2017b. URL https://en.
wikipedia.org/w/index.php?title=Formal_power_series&oldid=797671381. [On-
line; accessed 20-September-2017 ].
Qiuyi Zhang, Rina Panigrahy, and Sushant Sachdeva. Electron-proton dynamics in deep learning. CoRR,
abs/1702.00458, 2017a. URL http://arxiv.org/abs/1702.00458.
Yuchen Zhang, Jason D Lee, and Michael I Jordan. l1-regularized neural networks are improperly learnable in
polynomial time. In International Conference on Machine Learning, pp. 993-1001, 2016.
Yuchen Zhang, Jason Lee, Martin Wainwright, and Michael Jordan. On the learnability of fully-connected
neural networks. In Artificial Intelligence and Statistics, pp. 83-91, 2017b.
Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees for one-
hidden-layer neural networks. arXiv preprint arXiv:1706.03175, 2017.
A Analytic Formula for Population Risks
A. 1 Basics on Hermite Polynomials
In this section, we briefly review Hermite polynomials and Fourier analysis on Gaussian space. Let
Hj be the PrObabilists' Hermite polynomial WikiPedia (2017a), and let hj = 力Hj be the nor-
malized Hermite polynomials. The normalized Hermite polynomial forms a complete orthonormal
basis in the function space L2(R, e-x2/2) in the following sense13. For two functions f, g that map
R to R, define the inner product hf, gi with respect to the Gaussian measure as
hf, gi
E
X 〜N (0,1)
[f (x)g(x)] .
The polynomials h0, . . . , hm, . . . are orthogonal to each other under this inner product:
hhi , hj i = δij .
Here δij = 1 if i = j and otherwise δij = 0. Given a function σ ∈ L2 (R, e-x2/2) , let the k-th
Hermite coefficient of σ be defined as
σk = hσ, hki .
Since h0, . . . , hm , . . . , forms a complete orthonormal basis, we have the expansion that
σ(x) = J^σkhk (x).
k∈N
We will leverage several other nice properties of the Hermite polynomials in our proofs. The fol-
lowing claim connects the Hermite polynomial to the coefficients of Taylor expansion of a certain
exponential function. It can also serve as a definition of Hermite polynomials.
13We denote by L2 (R, e-x2 /2) the weighted L2 space, namely, L2 (R, e-x2/2)	,
nf : R-∞∞ f (x)2e-x2 /2dx < ∞o
13
Published as a conference paper at ICLR 2018
Claim A.1 ((O’Donnell, 2014, Equation 11.8)). We have that for t, z ∈ R,
exp(tz - 212) = X k1!Hk(z)tk
k=0
The following Claims shows that the expectation E [hn(x)hm(y)] can be computed easily when x, y
are (correlated) Gaussian random variables.
Claim A.2 ((O’Donnell, 2014, Section 11.2)). Let (x, y) be ρ-correlated standard normal variables
(that is, both x,y have marginal distribution N(0, 1) and E[xy] = ρ). Then,
E [hm (x)hn (y)] = ρ δmn .
As a direct corollary, We can compute Ex〜N(o,idd×d) [σ(u>x)γ(v>x)] by expanding in the Hermite
basis and applying the Claim above.
Claim A.3. Let σ, γ be tWo functions from R to R such that σ2, γ2 ∈ L2 (R, e-x2/2). Then, for any
unit vectors u, v ∈ Rd, We have that
E	[σ(u>x)γ(v>x)] = X σ 疽 ihu,V .
x 〜N (0,Idd×d)	i∈N
Proof of Claim A.3. Let s = u>x andt = v>x. Then s, t are tWo spherical standard normal random
variables that are hu, vi-correlated, and We have that
E	[σ(u>x)γ(v>x)] = E [σ(s)γ(t)] .
x~N (0,Idd×d)
We expand σ(s) and γ(t) in the Fourier basis and obtain that
E [σ(S)Y⑴]=E X σihi(S) X Yjhj⑴
i∈N	j∈N
=Xσiγj E [hi(s)hj⑴]
i,j
=^X σiYi hu, Vii	(by Claim A.2)
i
□
A.2 ANALYTIC FORMULA FOR POPULATION RISK f AND f0
In this section We prove Theorem 2.1 and Theorem 2.2, Which both folloW from the folloWing more
general Theorem.
Theorem A.4. Let γ, σ ∈ L2 (R, e-x2/2) ,and y = a>γ(Bx) with parameter a ∈ R' and B ∈
R'×d. Define the population risk fγ as
fγ(a,B) = E [ky - yk2].
Suppose B
and bi ’s and bi? ’s have unit `2 norm. Then,
f(a, B) =
k∈N
6kf a?泞-YkE ai 卑 k
i∈[m]	i∈[']
2
+ const,
F
where σk, Yk are the k-th Hermite coefficients ofthefUnction σ and Y respectively.
14
Published as a conference paper at ICLR 2018
We can see that Theorem 2.1 follows from choosing γ = σ and Theorem 2.2 follows from choosing
Y = σ2 h2 + σ4 h4. The key intuition here is that We can decompose σ into a weighted combination
of Hermite polynomials, and each Hermite polynomial influence the population risk more or less
independently (because they are orthogonal polynomials with respect to the Gaussian measure).
Proof of Theorem A.4. We have
fY = Ehky - yk2i = E (a*>σ(B?X)- a>γ(Bx)H
=E I Il x a?*，X)- x aiY(b>x)ll I
[M∈[m]	i∈[']	ll _|
=X^	E [a?a?G(b?>x)o(b?>x)] +	X^	E [aiajγ(b>x)γ(b>x)]
i∈[m],j∈[']	i∈[m],j∈[']
-2 X E ha?ajσ(bt>x)γ(b>x)i
i∈[m],j∈[']
=E a?a?E6khb?,b?ik+ E aiaw/hb,bjik
i,j∈[m]	k∈N	i,j∈[']	k∈N
-2 X	a?aj X6kYkhb?,bjik
i∈[m],j∈[']	k∈N
l	l2
=X σk X " - Yk X ai 中.
k∈N	i∈[m]	i∈ [']	F
(by Claim A.3)
□
A.3 ANALYTIC FORMULA FOR POPULATION RISK G
In this section we show that the population risk G(∙) (defined as in equation (2.7)) has the following
analytical formula:
G(B) = 2√6∣σ4∣∙ £a?	E	hb?,bji2hb?,bki2
i∈[d]	j,k∈[d],j 6=k
—
m
一√TΓ X aihbi,bj i + λ X(kbik - 1) .
i,j∈[d]
i=1
The formula will be crucial for the analysis of the landscape of G(∙) in Section B. The formula
follows straightforwardly from the following two theorems and the definition (2.7).
Theorem A.5. Let φ(∙, ∙, ∙) be defined as in equation (2.9), we have that
E y∙	∑	Φ(bj,bk,x) =2√6σ∙ Ea E	hb?,bji2hb?,bki2.
j,k∈[d],j6=k	i∈[d]	j,k∈[d],j 6=k
Theorem A.6. Let 夕(∙，∙) be defined as in equation (2.8), then we have that
E y ∙ X 夕(b,x) = √√6 X a?hb?,bji4.
j∈[d]	6 i,j∈[d]
In the rest of the section we prove Theorem A.5 and A.6.
We start with a simple but fundamental lemma. Essentially all the result in this section follows from
expanding the two sides of equation (A.1) below.
15
Published as a conference paper at ICLR 2018
Lemma A.7. Let u,v ∈ Rd be two fixed vectors and X 〜N (0, Idd×d). Then,for any s,t ∈ R,
exp(hu,v〉st) = E exp(u>xt — ^∣uk2t2)exp(v>xs — ；Ilvll2s2).
(A.1)
Proof. Using the fact that E [exp(v>x)] = exp(2 kv∣2), Wehave that,
E exp(u>xt — I ∣u∣2t2) exp(v>xs — ； Ilvll2s2)
=E [exp((tu + sv)>x)] exp( — 2∣∣uk2t2 — 2∣∣vk2s2)
=exp(1∣tu + sv∣2 — 11lukt2 — 11lvIl2s2)	(by the formula E [exp(v>χ)] = exp( 1 ∣∣vk2))
= exp(hu, vist) .
□
Next We extend some of the results in the previous section to the setting With different scaling (such
as When v in Claim A.3 is no longer a unit vector.)
Lemma A.8. Let u be a fixed unit vector and v be an arbitrary vector in Rd. Let 夕(v,x) =
8 Iivk4 - 1 (VTx)2Iivii2 + 214 (v>χ)4.
hu, vi4δ4,k = E [Hk((u>x)P(v, x)]	(A.2)
As a sanity check, we can verify that when v is a unit vector,夕(v, x) = √24h4 (v>x) and th Lemma
reduces to a special case of Claim A.2.
Proof. Let A, B be formal power series in variable s, t defined as A = exp(hu, vist) and B =
E [exp(u>xt - 2||uk2t2)exp(v>xs _ 1 ∣∣v∣∣2s2)]. We refer the readers to Wikipedia (2017b) for
more backgrounds of power series. For casual readers, one can just think of A as B as two power
series obtained by expanding the eχp(∙) via Taylor expansion. For a formal power series A in
variable x, let [xα]A to denote coefficient in front of the monomial xα . By Lemma A.7, we have
that A = B, and thus
[s4tk]A = [s4tk]B,
(A.3)
which implies that
[tk]卜xp(u>xt — gt2)) ∙ [s4] (exp(v>xs - I IvI2S2)
TTHk (u>x)P(v,x)
k!
(A.4)
where the last line is by the fact that 夕(v,χ) = [s4]exp(v>χs — 22 HvI2S2). This can be verified by
applying Claim A.1 with t = SHvH and Z = VkTx, and noting that H4(x) = x4 — 6x2 + 3.	□
Now we are ready prove Theorem A.6 using Lemma A.8.
Proofof Theorem A.6. Using the fact that σ(v>x) = P∞=° &$去(v>x), we have that
E y ∙ EiXbj,x)
j
X	a? E [σ(b?>x)i(bj,x)]
i,j∈[d]
∞
X	a? x EKkhk (b?>x)i(bj ,x)]
i,j∈[d]	k
16
Published as a conference paper at ICLR 2018
=X a? E hσ4h√b;>χmbj,x)i = √∣ X a?hb?,bji4
i,j∈[d]	i,j∈[d]
(by Lemma A.8 and hj = √j Hj)
□
Towards proving Theorem A.5, we start with the following Lemma. Inspired by the proofs above,
we design a function φ(v, w, x) such that we can estimate hu, vi2 hu, wi2 by taking expectation of
E σ(u> x)φ(v, w, x) .
Lemma A.9. Let a be a fixed unit vector in Rd and v,w two fixed vectors in Rd. Let 夕(∙，∙) be
defined as in Lemma A.8. Define φ(v, w, x) as
φ(v, w, x)=夕(V + w,x) + 夕(V — w,x) — 2夕(v, x) — 2夕(w, x)	(A.5)
=1 Ilvk2 Ilwk2 + hv,wi2 - 1 l∣wk2(v>χ)2 — 1 l∣vk2(w>x)2	(As)
—2(v>x)(w>x)v>w + gv>x)2(w>x)2 .
Then, we have that
E [σ(u>x)φ(v, w, x)] = 2∖f6σ4hu, v〉2〈u, Wy .
Proof. Using the fact that hu, V + wi2 + hu, V — wi4 — 2hu, Vi2 — 2hu, wi4 = 12hu, Vi2 hu, wi2 and
Lemma A.8, we have that
12(u, Vyku, wi2δ4,k = E [Hk(u>x)(夕(v + w,x) + 夕(v — w,x) — 2夕(v, x) — 2夕(w, x))]
= E [Hk (u> x)φ(V, w, x)] .	(A.7)
Using the fact that σ(u>x) = P∞=0 σkhk (u>x), we conclude that
∞
E [σ(u>x)φ(v, w, x)] = ɪ^k E [hk((u>x)φ(v,w,x)]
k=0
=√√4 E [H4(u>x)φ(v, w, x)] (by Lemma A.8 and hj = √jHj)
=2√6σ4hu,vi2hu,wi2	(by Lemma A.8 again)
□
Now we are ready to prove Theorem A.5 by using Lemma A.9 for every summand.
Proof of Theorem A.5. We have that
E y ∙ £。(%,bk,x)
j,k
=2√6σ4 £a? £(b?,bj〉2〈b?,bki2.	(by Lemma A.9)
□
B Landscape of Population Risk G(∙)
In this section we prove Theorem 2.3. Since the landscape property is invariant with respect to
rotations of parameters, without loss of generality we assume B? is the identity matrix id throughout
X a? X E K(b?> x)φ(bj ,bk, x)]
17
Published as a conference paper at ICLR 2018
this section. (See Section C for a precise statement for the invariance.) Recall that by Theorem 2.6,
the population risk G(∙) in the case of B? = Id is equal to
G(B) = 2√φ4∣X a? X(b>ei)2(b>ei)2
i	j6=k
dd	d
—斥 Xai X(bj ei) + λX(kbjIl - 1).	(B.1)
6 i=1 j=1	j=1
In the rest of section We work with the formula above for G(∙) instead of the original definition.
In fact, for future reference, we study a more general version of the function G. For nonnegative
vectors α, β and nonnegative number μ, let Gα,β,μ be defined as
d	dd	d
Gα,β,μ(B) = X αi X(b>ei)2(b>ei)2 -μXβi X(b")4 + λ£(|电∣2 - 1)2	(B.2)
i=1	j6=k	i=1	j=1	j=1
Here ei denotes the i-th natural basis vector. We see that G is sub-case of Gα,β,μ and we prove the
following extension of Theorem 2.3. Let αmax = maxi αi and αmin = mini αi .
Theorem B.1. Let Ka = αmaχ∕αmin and C be a sufficiently small universal constant (e.g. C = 10-2
suffices). Suppose μ ≤ Camin/βmaχ and λ ≥ 4max(μβmaχ, amaχ). Then, the function Gα,β,μ(B)
defined as in equation (B.2) satisfies that
1.
A matrix B isa local minimum of Ga,β,μ ifand only if B can be written as B = DP where
P is a permutation matrix and D is a diagonal matrix with Dii ∈
±
1
1-μβi∕λ
.
2.	Any saddle point B has strictly negative curvature in the sense that λmin(V2Gα,β,μ(B)) ≤
一to where τo = Cmin{μβmin∕(Kad), μβm"βmax, λ}
3.	Suppose B is an approximate local minimum in the sense that B satisfies
IVg(B)I ≤ ε and λmin(V2g(B)) ≥ -τ0
Then B can be written as B = DP + E where P is a permutation matrix, D is a diagonal
matrix with the entries satisfying
1	(	18dε2
1	μβi	∖	β2.
1----λ	∖	βmin
ε
≤ D2i ≤ 1一耍
—
2λ
and E is an error matrix satisfying
∣E∣∞ ≤ 3ε∕βmin.
As a direct consequence, B is Od(ε)-close to a global minimum in Euclidean distance,
where Od(∙) hides polynomial dependency on d and other parameters.
Here we recall that |E∣∞ denotes the largest entries in the matrix E. Theorem 2.3 follows straight-
forwardly from Theorem B.1 by setting α = 2√6∣σ4∣a? and β = ∣σ4∣α?∕√6. In the rest of the
section we prove Theorem B.1.
Note that our variable B is a matrix of dimension d × dand we use bi to denote the rows of B, that
-b>'
is, B =	. . Naturally, towards analyzing the properties of a local minimum B, the first step is
.
bd>
that we pick a row bs of B and treat only bs as variables and others rows as fixed. We will show
that local optimality of bs will imply that bs is equal to one of the basis vector ejup to some scaling
factor. This step is done in Section B.1. Then in Section B.2 we show that the local optimality of
all the variables in B implies that each of the rows of B corresponds to different basis vector, which
implies that B is a permutation matrix (up to scaling of the rows).
18
Published as a conference paper at ICLR 2018
B.1 Step 1: Analysis of Local Optimality of a Single Row
Suppose We fix bi, ∙…，bs-ι,bs+ι,…，b4, and optimize only over b§, We obtain the objective h of
the following form:
dd
hα,β,λ (x) = X αixi2 -X βix4 + λ(kxk2- 1)2	(B.3)
i=1	i=1
We can see that setting αi = ai? Pk6=s(bk>ei)2, βi = ai?, and x = bs gives us the original objective
G(B). In this subsection, we will work with h(∙) and analyze the properties of the local minima of
h(∙).
The following lemma shows that a local minimum X of the objective h(∙) must be a scaling of a
basis vector. For a vector x, let |x|2nd denotes the second largest absolute values of the entries for x.
We note that ∣∙∣2nd is not a norm. The lemma deals generally an approximate local minimum, though
we suggest casual readers simply think of ε, τ = 0 in the lemma.
Lemma B.2. Let h(∙) be defined in equation (B.3) with non-negative vectors a and β in Rd. Suppose
parameters ε,τ ≥ 0 satisfy that ε ≤ ,τ3∕βmin. If some point X satisfies ∣∣Vh(x)k ≤ ε and
λmin(V2h(x)) ≥ -T, then we have
Proof. Without loss of generality, we can take ε = PT3∕βmin which means T = ε1/30黑.The
gradient and Hessian of function h(∙) are
Vh(X) = 2 diag(α)X - 4 diag(β)X3 + γX
V2h(X) = 2 diag(α) - 12 diag(β X2) + γId + 8λXX>.	(B.4)
where γ , 4λ(∣X∣2 - 1).
Let S = {i : |Xi| ≥ δ} be the indices of the coordinates that are significantly away from zero, where
δ = (β^ε~) . Since ∣∣Vh(x)∣ ≤ ε, we have that ∣Vh(x)/ ≤ ε for every i ∈ [d], which implies
that
∀i ∈ [d], 2αiXi + γXi - 4βiXi3 ≤ ε	(B.5)
which further implies that
∀i ∈ S, ∣2αi + Y — 4βiX21 ≤ *	(B.6)
If |S| = 1, then we are done because |X|2nd ≤ δ. Next we prove that |S| ≥ 2. For the sake of
contradiction, we assume that |S | ≥ 2. Moreover, WLOG, we assume that |X|1 ≥ |X|2 are the two
largest entries of |X| in absolute values.
We take V ∈ Rd such that vi = -x2/，xi + x2, and v2 = xi/，x2 + χ2, and Vj = 0 for j ≥ 2.
Then we have that v>X = 0 and ∣v∣ = 1. We evaluate the quadratic form and have that
V>V2h(X)V = V> (2 diag(α) + γId)V - 12V> diag(β	X2)V	(since V>X = 0)
= (2αi + γ)Vi2 + (2α2 + γ)V22 - 12βiVi2X2i - 12β2V22X22
≤ -8βιv2χ2 - 8β2v2χ2 + .	(by equation (B.6) and ∣∣v∣ = 1)
22
≤ -8(βi+ β2) XΓ⅛⅛ + δ∙
≤ -8βminx2 + * .
Recall that δ = (jε-) / . Then we conclude that
v>V2h(x)v ≤ —6万乂：£2/3 = —6τ.
19
Published as a conference paper at ICLR 2018
This contradicts with the assumption that λmin(V2h(x)) ≥ -β%nε2/3 = τ and that kvk = 1.
Therefore we have |S | = 1 and
|x|2nd ≤ δ
ε Y/3<
βmU	≤
(using ε ≤ VZT3∕βmin)
□
For future reference, we can also show that for a sufficiently strong regularization term (sufficiently
large λ), the norm of a local minimum x should be bounded from below and above by 1∕2 and 2.
This are rather coarse bounds that suffice for our purpose in this subsection. In Section B.2 we will
show that all the rows of a local minimum B of G have norm close to 1.
Lemma B.3. In the setting of Lemma B.2,
1.	Suppose in addition that λ ≥ 4 max(βmax, τ) and ε≤ 0.1βmin d-3/2, then
kxk2≤2.
2.	Let i? = arg maxi |xi |. In addition to the previous conditions in bullet 1, assume that
λ ≥ 4αi?. Then,
kχk2 ≥ 2.
We remark that we have to state the conditions for the upperbounds and lowerbounds separately
since they will be used with these different conditions.
Proof. Let S = {i : |xi| ≥ δ} be the indices of the coordinates that are significantly away from
zero, where δ = (βε~)	. We first show that kχ∣∣2 ≤ 2. We divide into two cases:
1	S is empty. Since ε ≤ 0.1βmind-3/2, then δ ≤ √√d. We conclude that kx∣∣2 ≤ 2.
2	S is non-empty. For i ∈ S, recall equation (B.6) which implies that
4λ(Ilxk2 -I) ≤ δ + 4βix2
≤ δ + 4βmaxkxk2
kxk2 ≤ z⅛ + βmaxkxk2 + 1
4λδ	λ
Since λ ≥ 4βmaχ, so λ ≥ /乂：£2/3 ≥ 3ε, and thus from the display above We have that
kxk2 ≤ 2.
Next We show that kxk2 ≥ 2. Again We divide into two cases:
1.	S is empty. For the sake of contradiction, assume that kxk2 ≤ 2, then Y ≤ -2λ. We show
that there is sufficient negative curvature. Recall that
V2h(x) = 2 diag(α) - 12 diag(β x2) + γI + 8λxx>
2 diag(α) - 12 diag(β x2) - 2λI + 8λxx>
Choose index j? so that αj? = αmin, then
ej? V h(x)ej? = 2αmin - 12βj? xj? - 2λ + 8λxj?
≤ 2αmin + 8λδ2 - 2l
≤ 2αmin - λ(2 - 8δ2)
20
Published as a conference paper at ICLR 2018
≤ 2αmin - 3λ	(by δ2 ≤ 112 )
≤ — 5λ ≤ -3τ	(by λ ≥ 4max{amin,τ})
6
This contradicts with the fact that λmin(V2h(x)) ≥ —τ. Thus when S isempty, kx∣∣2 ≥ 2.
2.	S is non-empty. Recall that i? = arg maxi |xi |, and by definition i? ∈ S. Using Equation
(B.6)
ε
Y ≥ —2ai? — W
δ
which implies that
kxk2 ≥ 1 -笺
λ
ε
4λJ.
—
Since λ ≥ 4αi?, and λ ≥ /黑£2/3 ≥ ε, We conclude that kχ∣∣2 ≥ 1/2.
□
We have shown that a local minimum x ofh should be a scaling of the basis vector ei? . The following
lemma strengthens the result by demonstrating that not all basis vector can be a local minimum —
the corresponding coefficient αi? has to be reasonably small for ei? being a local minimum. The
key intuition here is that if αi? is very large compared to other entries of α, then if we move locally
the mass of ei? from entry i? to some other index j, the objective function will be likely to decrease
because αj xj2 is likely to be smaller than αi? xi2? . (Indeed, we will show that such movement will
cause a second-order decrease of the objective function in the proof.)
Lemma B.4. In the setting of Lemma B.2, let i? = arg maxi |xi|. If kVh(x)k ≤ ε, and
λmin(V2h(x)) > —τ for 0 ≤ T ≤ 0.1βmin∕d and ε ≤ y3f/3∕βmin,then
αi? ≤ αmin + 2ε + 2τ + 4βi? .
Proof. For the ease of notation, assume WLOG that i? = 1. Let δ = (τ∕βmin)1/2. By the assump-
tions, we have that δ ≤ √6d. By Lemma B.2, we have ∣∣χk2 ≥ 11, which implies that
χ2 ≥ llxk2 — (d -I) |x|2nd ≥ 2 — d |x|2nd ≥ 1 — dδ2 ≥ 3 ∙	(B.7)
Define V = — (Xk) eι + ek. Since xι is the largest entry of x, we can verify that 1 ≤ ∣∣v∣2 =
x2
1 + -2 ≤ 2. By the assumption, we have that
x1
v>V2 h(x)v ≥ —/ ∣v∣2 ≥ —4/ .	(B.8)
On the other hand, recall the form of Hessian (equation (B.4)), by straightforward algebraic manip-
ulation, we have that
v>V2h(x)v = v> (2diag(α) — 12diag(β Θ XG)2) + YId + 8λxx>.) V
=2αι(-^-)2 + 2ak — 12βιxk — 12βkXk + γ(--)2 + Y	(by v>x = 0)
x1	x1
≤ (2α1 + Y)( -)2 — 12(βι + βk)Xk + 2αk + (4βιx2 — 2αι + I-------P)
X1	1	|X1 |
(by equation (B.6))
≤ (4β1xι + jχ-∣ )(X )2 — 12(β1 + Bk)Xk + 2αk + (4βιχ2 — 2αι + ∣χ-1)
(by equation (B.6))
21
Published as a conference paper at ICLR 2018
= -8β1 xk - 12βk xk + 4β1 x1 + 2αk - 2α1 + 4ε
(by |xk| ≤ |x1| and |x1|2 ≥ 1/3)
≤ 2αk - 2α1 + 4ε + 8β1 .	(by kxk ≤ 2 using Lemma B.2)
Combining equation (B.8) and the equation above gives
α1 ≤ αk + 2ε + 2τ + 4β1 .
Since k is arbitrary We complete the proof.	□
The previous lemma implies that it’s very likely that the local minimum x can be written as x =
xi? ei? and the index i? is also likely to be the argmin of α. The folloWing technical lemma shoWs
that When this indeed happens, then We can strengthen Lemma B.2 in terms of the error bound’s
dependency on ε and τ. In Lemma B.2, We have that |x|2nd is bounded by a function of τ. Here We
strengthen the bound to be a function that only depends on ε. Thus as long as τ be small enough so
that We can apply Lemma B.2 and Lemma B.4 to meet the condition of the lemma beloW, then We
get an error bound that goes to zero as ε goes to zero. This translates to the error bound in bullet
3 of Theorem B.1 Where the bound on E only depends on ε. For casual readers We suggest to skip
this Lemma since its precise functionality Will only be clearer in the proof of Theorem B.1.
Lemma B.5. In the setting of Lemma B.2, in addition we assume that i = argmink ∣αk | and that X
can be written as x = xiei + x-i satisfying
∣∣x-ik∞ ≤ 0.1min{1∕√d, VZemin/(βmax)} .
Then, we can strengthen the bound to
3ε
kx-ik∞ ≤ 飞
βmin
Proof. WLOG, let i = 1. Let xj be the second largest entry of x in absolute value. Define v1 =
4βιχi 一 2αι 一 γ, and similarly Vj = 4βjXj 一 2αj - Y. Since ∣∣Vh(x)∣ ≤ ε, by equation (B.6), we
have that |vi| ≤ 鬲 and |vj| = 忘.Subtracting 4βιxj = 2αι + Y + vι and 4βjxj = 2αj + Y + Vj,
we obtain,
4β1 X1 = 4βj Xj 一 2(αj 一 α1 ) + V1 一 Vj
≤ 4βj Xjj + (V1 一 Vj)	(since αj 一 α1 ≥ 0)
Since ∣∣χk2 ≥ j, then xj ≥ j 一 dδ2 ≥ ɪ. Since ∣Xj| ≤ δ,
4βj Xjj ≤ 4βmaxδj
Combining the above two displays,
(V1 一 Vj) ≥ 4β1xj1 一 4βj δj
≥ 4βιxj - 4βmaχδ2
≥ 3Bl 一 3βmin	(Using χ1 ≥ 3 and δ ≤ pβmin/(6βmax))
≥ 2βmin	(B.9)
SinCe |v1| ≤ 向 and |v2| = ∣⅛,
2占 ≥ 2βmin,	(B.10)
|xj |	3
and re-arranging gives |xj | ≤ 3 ^ε-.	□
22
Published as a conference paper at ICLR 2018
B.2 Local Optimality of All the Variables
In this section we prove Theorem B.1. Results in Subsection B.1 have established that if B is a local
minimum, then each row bs of B has to be a scaling of a basis vector. In this section we show that
these basis vectors need to be distinct from each other. The following proposition summaries such a
claim (with a weak error analysis).
Proposition B.6. In the setting of Theorem B.1, suppose B satisfies
kVg(B)k ≤ ε and λmin(V2g(B)) ≥ -T
for parameters τ, ε satisfying 0 ≤ T ≤ C min{μβmin/(καd), λ} and ε ≤ C min{ɑmin, ,τ3/βmin}.
Then, the matrix B can be written as
B = DP +E,
where D is diagonal such that ∀i, |Dii| ∈ [1/4,2], and P is a permutation matrix, and |E∣∞ ≤ δ
1/2
with δ = ( ʃ )	.
∖μβmin )
As alluded before, in the proof we will first apply the results in Section B.1 to show that when B is
a local minimum, each row bs has a unique large entry. Then we will show that the largest entries of
each row sit on different columns. The key intuition behind the proof is that if two rows, say row s, t,
have their large entries on the same column, then it means that there exists a column— say column
k — that doesn’t contain largest entry of any row. Then either row s or t will violate Lemma B.4.
Or in other words, either row s or t can move their mass into the column k to decrease the function
value. This contradicts the assumption that B is a local minimum.
Proof. As pointed in the paragraph below equation (B.3), when we restrict our attention to a par-
ticular row of B and fix the rest of the rows the function Gα,β,μ reduces to the function h(∙) in
equation (B.3) so that we can apply lemmas in Section B.1.
Concretely, fix an index S ∈ [d] and let X = b§. For all i ∈ [d], let ai = a Pj=s(b>ei)2, and
βi = μβi. Then we have that
d
Gα,β,μ(B) = X αiχ2 - Xβix4 + λ(kxk2 - 1)2	(B.11)
i=1	i
We view the function above as h(x). Now we apply Lemma B.2 (by replacing α, β in Lemma B.2
by α,β). The assumption that λmin(V2gα,β,μ(B)) ≥ -T implies that λmin(V2h(x)) ≥ -T since
V2h(x) is a submatrix of V2g(B).Moreover, ∣∣Vh(x)k ≤ ∣∣VG(B)k ≤ ε ≤ PT3∕(μβmin)
Hence by Lemma B.2, we have that the second largest entry of |bs | satisfies
∀s,∣bs∣2nd≤ δ.	(B.12)
where δ, (μβm-)1/2 for the ease of notation. We can check that δ ≤ √1=l by the assumption.
Therefore, we have essentially shown that each row of B has only one single large entry, since the
second largest entry is at most δ.
Next we show that each row of B has largest entries on distinct columns. For each row j ∈ [d],
let ij = arg maxi |ei>bj | be the index of the largest entry of bj. We will show that i1, . . . , id are
distinct.
For the sake of contradiction, suppose they are not distinct, that is, there are two distinct rows s, t
that have the same largest entries on column l, that is, we assume that is = it = l. This implies
that {i1, . . . , id} 6= [d] and let k ∈ [d] be the index such that k ∈/ {i1, . . . , id}. We note that by the
1/2
assumption δ = ( β. )	≤ 4√-^ ≤ ɪ√^. We first bound from above ak
αk = αk X(b>ek)2 ≤ αkdδ2 ≤ if αmin .
j6=s
(by δ ≤ 4√καd)
23
Published as a conference paper at ICLR 2018
Assume in addition without loss of generality that |bs>el | ≤ |bt>el|. Let
zl , X(bj>el)2	(B.13)
j6=s
be the sum of squares of the entries on the column l without entry b> eι, and that α = αι zι . We
first prove that zl ≥ 1/3.
For the sake of contradiction, assume Zl < 1/3. Then We have that α? = a?z ≤ 1 α?. This implies
that λ ≥ 4 max{al, T}, and since l is the index of the largest column of b§ we can invoke LemmaB.3
and conclude that kbs k2 ≥ 1/2. This further implies that
(b>el)2 ≥ kbsk2 - d|bs∣2nd ≥ 1/2 - dδ2 ≥ 1/3	(by δ ≤ 1∕(4√d))
Since we have assumed that |bs>el | ≤ |bt>el|. Then we obtain that
zl ≥ |bt>el|2 ≥ |bs>el|2 ≥ 1/3,
which contradicts the assumption. Therefore, we conclude that zl ≥ 1/3. Then we are ready to
bound αl from below:
1
αl = αlzl ≥ 3 αl .
The display above and Equation (B.13) implies that
αl - αk ≥ 4 αmin.	(B.14)
Note that l is the largest entry in absolute value in the vector bs . We will apply Lemma B.4. We
fix every row of B except bs and consider the objective as a function of bs only. Again let αi =
a Pj=s(b>ei)2, and βi = μβi and we have the equation (B.11). (Note that now α depends on the
choice of s which we fixed.) Lemma B.4 gives us that
αl ≤ αk + 2ε + 2τ + 4β'.
11	1
Since ε ≤ 50am®, τ ≤ 而αmin and βl = μβl ≤ 50am®, we obtain that
al ≤ αk + 7 amin	(B.15)
5
which contradicts equation (B.14). Thus we have established that i1, . . . , id are distinct.
Finally, let Q be the matrix that only contain the largest entries (in absolute value) of each columns
of B . Since i1, . . . , id are distinct, we have that Q contains exactly one entry per row and per
column. Therefore Q can be written as DP where P is a permutation matrix and D is a diagonal
matrix. Moreover, we have that kbs k2∞ ≥ kbs k2 - d |bs |22nd ≥ 1/4 and kbs k2 ≤ 2. Therefore, the
largest entry of each row has absolute value between 1/4 and 2. Therefore |D|ii ∈ [1/4, 2]. Let
E = B 一 PD. Then we have that |E∣∞ ≤ maxs |b§ 屋 ≤ δ,which completes the proof.
□
Applying Lemma B.5, we can further strengthen Proposition B.6 with better error bounds and better
control of the largest entries of each column.
Proposition B.7 (Strengthen of Proposition B.6). In the setting of Proposition B.6. Suppose in
addition that T satisfies T ≤ cμβmmin ∕βmax∙ Then, the matrix B can be written as
B =DP+E,
where P is a permutation matrix, D is diagonal such that
1	(	18dε2	ε ʌ
F C-K 一 列
∀i ∈ [d],
≤ Diil2 ≤ —二(1 + -ε
-1 iil _ 1 - 竽卜 2λ
and
∣E∣∞
V 3ε
βmin
24
Published as a conference paper at ICLR 2018
1/2
Proof. By Proposition B.6, We know that ∣E∣∞ ≤ δ = T—	. Now We use Lemma B.5 to
J J ɪ	'	μ I…一	∖μβmin
strength the error bound.
As we have done in the proof of Proposition B.6, we again fix an arbitrary s ∈ [d] and all the rows
except bs and view Gα,β,μ as a function of b§. For all i ∈ [d], let α% = α Pj=s(b>ei)2, and
βi = μβi and view Gα,β,μ as a function of the form h(x) with α, β replaced by α, β, namely,
h(x) = X akXk - Xβkx4 + λ(kx∣∣2 - 1)2 + const
kk
We will verify the condition of Lemma B.5. Let i be the index of the largest entry in absolute value
of the vector bs . Since we have shown that the largest entry in each row sits on different columns,
and the second largest entry is always less than δ, we have that,
ai = αi X ：(bj ei) ≤ αidδ ≤ 16αmin∙	(by δ ≤ 4√K d)
j6=s
For any k 6= i, we know that the column k contains some entry (k, jk) which is the largest entry of
some row, and we also have that jk 6= s since the largest entry of row s is on column i. Therefore,
we have that
αk = αk X(b>efc)2 ≥ αι(b>kek)2 ≥ αι(kbk ∣∣2 - dδ2)
j6=s
≥ 3αι	(by δ ≤ 1∕(4√d))
Therefore, O^k ≥ 国 for any k = i and thus i = argmink∣αk |. By the fact that ∣E∣∞ ≤ δ, we have
that ∣x-i∣∞ ≤ δ ≤ 0.1 min{1∕√d, ∙∖∕βmin∕(βmaχ)}. Now we are ready to apply Lemma B.5 and
obtain that |b§ ∣2nd ≤ ^ε-. Applying the argument for every row S gives ∣E∣∞ ≤ ^ε-.
Finally, we give the bound for the entires in D. Let V be a short hand for Vh(bs) which is equal
to the s-th column of VG(B). Since B is an ε-approximate stationary point, then we have that
∣v∣ ≤ ε and by straightforward calculation of the gradient, we have
d
Vi = 2αiXi — 4μβiX3 + 4λ(^~^ xj2 — 1)xi .
j=1
Since xi 6= 0, dividing by xi gives,
0 = 2αi — 4μβiX2 + 4λ(^X xj — 1)——
j=1	xi
=(4λ — 4μβi)x2 + 4λ ^X x2 + 2αi — 4λ--
ij	X
Rearranging the equation above gives,
2 =	1
i 4λ — 4μβ-
4λ — 2α- - 4λ]Tx2
j6=i
r-⅛ (1 - α- - X X - 4λX-
To upper bound x2 , we note that |v-| < ε, α- > 0, and Pj=- xj > 0, so
xi2 ≤
(1+2λ )≤ 1+2μβF
(since λ ≥ 4μβ-)
25
Published as a conference paper at ICLR 2018
For the lower bound of χ2, We note that ∣E∣∞ ≤ δ = -^ε- implies Pj= xj ≤ dδ2. Moreover, We
have proved that each rows has largest entry at different columns. Also note that the largest entry
of row bs is on column i. Therefore, we have α% = αi Pj 6=s (bjTei)2 ≤ αmaxdδ2 . Using these two
estimates and δ = ^ε-, we have
xj ≥ —1^(1 - (αmaχ + i)dδ2 -3)
i - 1 -咪	' 2λ 丁 "	2λj
1	(	18dε2	ε ʌ
=i - μβi 1 - ~β2	2λ )
- λ '	Xmm	/
□
Finally we are ready to prove Theorem B.1 by applying Proposition B.6.
Proof of Theorem B.1. By setting ε= 0, τ = 0 in Proposition B.6, we have that any local minimum
B satisfies that B = DP where P is a permutation matrix and D is a diagonal and the precise
diagonal entries of D. It can be verified that all these points have the same function value, so that
they are all global minimizers.
Towards proving the second bullet, we note that a saddle point B satisfies that VG(B) = 0. We will
prove that λmin(V2G(B)) ≤ -τ0. For the sake of contradiction, suppose λmin(V2G(B)) ≥ 一τ0.
Then setting ε = 0 and τ = τ0 in Propostion B.7, we have that B = DP and Dii =
±
1
1-μBi∕λ
, which by bullet 1 implies that B is a local minimum.
This contradicts the as-
sumption that B is a saddle point.
The 3rd bullet is a just a rephrasing of Proposition B.7.
□
C Handling Non-Orthogonal Weights
In this section, we first show that when the weight vectors {bi?}0s are not orthonormal, the local
optimum of a slight variant of G(B) still allow us to recover B?. The main observation is that the
set of local minima are preserved (in a certain sense) by linear transformation of the variables. We
design an objective function F(B) that is equivalent to G(B) up to a linear transformation. This
allows us to use Theorem 2.3 as a black box to characterize all the local minima of F.
We use λmaχ(∙), λmin(∙) to denote the largest and smallest eigenvalues of a square matrix. Similarly,
σmax(∙) and σmin(∙) are used to denote the largest and smallest singular values.
C.1 Local Minimum after a Linear Transformation
Given a function f (y), we say function g(∙) is a linear transformation of f (∙) if there is a matrix W
such that g(x) = f(Wx). If W has full rank, the local minima of f are closely related to the local
minima of g.
We recall some standard notation in calculus first. We use Vf(t) to denote the gradient of f eval-
uated at t. For example, Vf (WX) is a shorthand for d∂(y) ∣y=wχ, and similarly V2f (WX) is
fy) |y=Wx.
The following theorem then connects the gradients and Hessians of f(Wx) and g(x). Essentially, it
shows that the set of local minima and saddle points have a 1-1 mapping between f and g, and the
corresponding norms/eigenvalues only differ multiplicatively by quantities related to the spectrum
ofW.
Theorem C.1. Let W ∈ Rd×m(d ≥ m) be a full rank matrix. Suppose g : Rm → R and
f : Rd → R are twice-differentiable functions such that g(x) = f(Wx) for any x ∈ Rm. Then, for
all x ∈ Rm, the following three properties hold:
1. σmin(W)kVf(Wx)k ≤ kVg(x)k ≤σmaχ(W)kVf(Wx)k.
26
Published as a conference paper at ICLR 2018
2.	If λmin(V2g(x)) < 0, then
σmaχ(W )2λmin(V2f (Wx)) ≤ λmin(V2g(x)) ≤ bmin(W )2 λmin(V2 f (Wx)).
3.	The point x satisfies the first and second order optimality condition for g iff y = Wx also
satisfy the first and second order optimality condition for f.
Proof. The proof follows from the relationship between the gradients of g and the gradients of f .
By basic calculus, we have
Vg(x) = d (Wx) = W > df(y)∣	= W >Vf (Wx)
∂x	∂y y=Wx
which immediately implies bullet 1. Similarly, we can compute the second order derivative:
V2g(x) = W> [V2f(Wx)]W.
To simplify notation, let A = V2f(Wx). Let x = arg minkxk=1 x> W> AWx, and y =
(W x)/kW xk. Therefore
λmin(A) ≤ y>Ay ≤ λmin(W>AW)∕kWxk2 ≤ λmin(W>AW)∕∣∣Wk2.
On the other hand, lety be the unit vector that minimizes y> Ay, we know y is in column span of W
because f is only defined on the row span, so there must exist a unit vector x such that Wx = λy
where λ ≥ σmin(W). For this x we have λmin (W>AW) ≤ x> W> AWx = λ2λmin(A) ≤
σm2 in(W)λmin(A). This finishes the proof for 2.
Finally, notice that W is full rank, so Vg(x) = W>Vf(Wx) = 0 iff Vf(Wx) = 0. Also,
V2g(x) = W>[V2f (Wx)]W 占 0 iff V2f (Wx)占 0.	□
C.2 Objective for Non-Orthogonal Weights
Now we will design a new objective function that can be linearly transformed to the orthonormal
case. The main idea is to view the rows of B? as the new basis that we work on (which is not
necessarily orthogonal). Note that this is already the case for the first two terms of the objective
function G(B), we change the objective function as follows: More concretely, we define
Fα,μ,λ(B) = 2√6σ ∙ X ai X hb?,bji2hb?,bki2
i∈[d]	j,k∈[d]
mm
—√μ x aihb?, bj i4+λ X((X a"% ,b?)2 -I)2-I)2.
6 i,j∈[d]	j=1 i=1
Note that the only change in the objective is the regularizer for the norm of bj . It is now replaced by
((Pim=1 αihbj, bi?i2 - 1)2 - 1)2, which tries to ensure the “norm” of bj in the basis defined by row
of B? to be 1. The objective function that we will optimize corresponds to choosing αi = ai?.
Similar as before, this function can be computed as expectations
Fa*,μ,λ(B) = E y ∙ E	φ(bj,bk,x) — μE y ∙ EiXbj,x)
j,k∈[d],j6=k	j∈[d]
+ λE(χ,y),(χ0,y0)[E y ∙ φ2(bi,x) ∙ y ∙ φ2(bi,x0)],	(CI)
i=1
where (x0, y0) is an independent sample, and φ2 (v, x) = (v>x)2 - kvk2.
Intuitively, ifwe can find a linear transformation that makes {bi?}’s orthonormal, that will reduce the
problem to the orthonormal case. This is in fact the whitening matrix:
27
Published as a conference paper at ICLR 2018
Let M = Pim=1 ai?bi?(bi?)> be the weighted covariance matrix of bi?’s. Suppose the SVD of M is
UDU> and let W = UDT/2. We apply the transformation W> to the vectors Jabis and obtain
that oi = W>，0?b?. We can verify that ois are orthogonal vectors because
oioi> = W>MW = Id	(C.2)
i∈[m]
For notational convenience, let’s extend the definition of the G(B) in equation by using the putting
the relevant information in the subscript
Gα,β,λ,o网=√6σ ∙ X a?	X	ho,, MY ho,, Wi- √μ X *,守.
i∈[d]	j,k∈[d],j 6=k	i,j∈[d]
m
+ λ X(kbik2-1)2
i=1
(That is, the index o denotes the ground-truth solution with respect to which G is defined.)
The next Theorem shows that we can rotate the objective function F properly so that it matches the
objective G with a ground-truth vector oi ’s.
Theorem C.2. Let W be defined as above, and let 1/a? be the vector whose i-th entry is 1/ai?.
Then, we have that
Gl/a? ,μ,λ,θi(B)= Fa*,μ,λ(BW >)).
Note this can be interpreted as a linear transformation as in vector format BW> is equal to B ∙
(W> 0 Idd×d)∙
Proof. The equality can be obtained by straightforward calculation. We note that since B
the rows of B ∙ (W> 0 Idd×d) are Wbι,..., Wbm.
Therefore, we have that
Fa*,μ,λ(B∙(W> 0 Idd×d))	(C.3)
=2√6σ ∙ X a? X hb?, Wbji2hb?, Wbki2
i∈[d]	j 6=k∈[d]
mm
— σ√μ X a?hb?, Wbji4 + λ X(X a?hWbj,b?)2- 1)2 .
6 i,j∈[d]	j=1 i=1
2√6σ ∙ X a; X hPa?W>b?, bji2hPa?W>b?, bki2
i∈[d] i j,k∈[d]
mm
—√6 X a?hpa?W b?,bji + λ X(Xhbj, Pa?W b?i -I).
6 i,j∈[d] ai	j=1 i=1
2√6σ ∙ X a? X hθi,bji2hθi,bki2
i∈[d] i j,k∈[d]
—
mm
选 i,Xd]” bji4+λ X(X S ,oii2 -1)2
(by the definition of oi ’s)
□
From Theorem 2.3 we can immediately get the following Corollary (note that the only difference is
that the coefficients now are 1/ai? instead of ai?). Recall a?max = maxi ai? and a?min = mini a?min ,
we have
28
Published as a conference paper at ICLR 2018
Corollary C.3. Let κa = a?max/a?min. Let c be a sufficiently small universal constant (e.g. c =
0.01 suffices). Assume μ ≤ c∕κo and λ ≥ (Camin)T. The function G\/&? ,μ,λg (∙) defined as in
Theorem C.2 satisfies that
1.	A matrix B is a local minimum of G if and only if B can be written as B = PDO where
O is a matrix whose rows are oi ’s, P is a permutation matrix and D is a diagonal matrix
with Dii ∈ {±1 ± OM6amin。.
2.	Any saddle point B has a Strictly negative curvature in the sense that λmin(V2G(B)) ≥
-to where τo = Cmin{μ∕(Kaamaχd), λ}
3.	Suppose B is an approximate local minimum in the sense that B satisfies
kVg(B)k ≤ ε and λmin(V2g(B)) ≥ -τ0
Then B can be written as B = PDO+E where P is a permutation matrix, D is a diagonal
matrix and ∣E∣∞ ≤ O(εamaχ∕σ4).
Finally, we can combine the theorem above and Theorem B.1 to give a guarantee for optimizing F .
Let Γ be a diagonal matrix with Γii = pf. Let M = B*>Γ2B? and K(M) = ∣∣Mk∕σmin(M).
Theorem C.4. Let C be a sufficiently small universal constant (e.g. C = 0.01 suffices). Let κa =
amiax/amin. Assume μ ≤ c∕κa and λ ≥ 1∕(c ∙ am®). Thefunction F(∙) defined as in Theorem C.2
satisfies that
1.	A matrix B is a local minimum of F if and only if B satisfy B-> = PDrB夫 where P is
a permutation matrix, Γ is a diagonal matrix with Γα = ʌ/ɑ?, and D is a diagonal matrix
with Dii ∈ {±1 ± O(μ∕λambin)}. Furthermore, this means that all local minima of F are
also global.
2.	Any saddle point B has a strictly negative curvature in the sense that λmin(V2F(B)) ≥
-to where to = Cmin{μ∕(Kadamax), λ}σmin(M).
3.	Suppose B is an approximate local minimum in the sense that B satisfies
∣VF(B)∣ ≤ ε and λmin(V2F(B)) ≥ -τo
Then B can be written as B-> = PDΓB? + E where Γ, D, P are as in 1, the error term
IlEIl ≤ O(εammaχ√md ∙ K(M)1/2∕σ4) (when εammaχ√md ∙ K(M)1/2∕σ4 < c).
Proof. Note that We can immediately apply Theorem 2.3 to Gι∕a*,μ,λ,0i(B) to characterize all its
local minima. See Corollary C.3.
Next we will transform the properties for local minima of G (stated in Corollary C.3) to F using
Theorem C.1. First we note that the transformation matrix W and M are closely related:
WW> =M,σmin(W)2 = 1∕IM I, IW I2 = 1∕σmin(M).	(C.4)
This is because according to the definition of W, the SVD ofM is M = UDU> andW = UD-1/2,
so WW> = UD-1U> = M-1. The claims of the singular values follow immediately from the
SVDofMandW.
As a result, all local minimum of F are of the form BW> where B is a local minimum of G. For
B = BW>, the gradient and Hessian of F(B) and G(B) are also related by Theorem C.1.
Let us first prove 1. By Corollary C.3, we know every local minimum of G is of the form B =
PDO. According to the definition of O in Theorem C.2, we know each row vector oi is equal to
W>(a?)1/2b?, therefore O = ΓB*W. As a result, all local minima of G are of the form B =
PDΓB? W. By Theorem C.1 and Theorem C.2, we know all local minima of F must be of the form
B = BW > = PDΓB^WW > = PDrB 大 M T.
Now we try to compute B->. To do that observe that [ΓB?]M-1[ΓB*]> = I. Therefore
[rB?M-1]-> = rB?, and for any local minimum B, we have
B-> = (PDrB?M-1)-> = P ->D->(rB?M -1)->
29
Published as a conference paper at ICLR 2018
= P ->D->ΓB?.
Note that P> is still a permutation matrix, and D-> is still a matrix whose diagonal entries are
{±1 ± O(μ∕λamin)}, so this is exactly the form We stated in 1. More concretely, the rows of B->
are permutations of ʌ/ofb?.
For bullet 2, it follows immediately from Property 2 in Theorem C.1. Note that by property 2,
λmin(V2F(BW>)) ≤ "mi*G(B)) = λmin(V2G(B))σmin(M).
kWk2
Finally we will prove 3. Let B = BW->, so that G(B) = F(B). We will prove properties of B
using the properties of B from Corollary C.3.
First we observe that by Theorem C.1,
λmin(V2G(B)) ≥ kWk2λmin(V2F(B)) ≥ -Cmin{μ/(Kadamaχ,λ}.
Therefore the second order condition for Claim 3 in Corollary C.3 is satisfied. Now when
∣∣VF(B)k ≤ ε, we have ∣∣VG(B)k ≤ ε∣∣W∣∣ = ε∕σmin(M)1/2. By Corollary C.3, we know B
can be expressed as PDO + E0 where D is the diagonal matrix, P is a permutation matrix and
|E0∣∞ ≤ O(εammaχ/(σ4σmin(M)1/2)). We will apply perturbation Theorem C.9 for matrix inver-
sion. Since σmin(P DO) ≥ 1/2, we know when ∣E0∣ ≤ 1/4,
Il(PDO + E0)-1 - (PDO)-1k ≤ 8√2∣∣E0k∙
Here ∣∣E∣ is bounded by IlEkF ≤ √md∣E0∣∞ ≤ O(εamaχ√md/(σ4σmin(M)1/2)), which is
smaller than 1/4 when ε is small enough.
The corresponding point in F is B = BW>, and in 1 we have already proved (PDOW>)-> is of
the form we want, therefore we can define E = B-> - (PDOW>)-> = (B - P DO)->W -1,
and
∣Ek = ∣W-1 kk(PDO + E0)-1 — (PDO)-1k = O(εamɑχ√md ∙ κ(M)1/2∕^4).
This finishes the proof.	□
C.3 Handle Undercomplete Case
The objective function F can handle the case when the weights bi?’s are not orthogonal, but still
requires the number of components m to be equal to the number of dimensions d. In this section
we show how to use similar ideas for the case when the number of components is smaller than the
dimension (m < d).
Note that all the terms in F(B) only depends on the inner-products hbj, bi?i. Let S be the span of
{bi?}’s and PS be the projection matrix to this subspace, it is easy to see that F(B) satisfies
F(B) = F(BPS).
That is, the previous objective function only depends on the projection of B in the space S . Using
similar argument as Theorem C.4, it is not hard to show the only local optimum in S satisfies the
same conditions, and allow us to recover B? . However, without modifying the objective, the local
optimum of F(B) can have arbitrary components in the orthogonal subspace S⊥.
In order to prevent the components from S⊥, we add an additional '2 regularizer: define Fa,μ,λ,δ as
follows:
Fα,μ,λ,δ (B) = Fα,μ,λ(B) + 2 IBkF	(C.5)
Intuitively, since the first term Fα,μ,λ(B) only cares about the projection BPS, minimizing ∣∣B∣F
will remove the components in the orthogonal subspace of S . We will choose δ carefully to make
30
Published as a conference paper at ICLR 2018
sure that the additional term does not change the local optima of Fα,μ,λ(B) by too much, while still
ensuring a small projection on S⊥.
In this case we will consider pseudo-inverse instead of inverse. In particular, for a m × d matrix B,
define its pseudo-inverse Bt to be the matrix such that BBt = Idm×m and BtB is the projection to
the row span of B .
Let M = Pm=I a?b?(b?)>, K(M) = kMk6m(M).
Theorem C.5. For any desired accuracy ε0, we can choose parameters ε, δ, τo, μ, λ, such that for
the objective function Fa?,μ,λ,δ (B), for any B SuCh that
∣∣VF(B)k≤ ε,	V2F(B) ≥-το∕2,
we have [Bt]> = B*DΓP + E where Γ is a diagonal matrix with entries ʌ/o?, D is a diagonal
matrix with entries close to 1, P is a permutation matrix and kEk ≤ ε0.
To choose the parameters, let c be a sufficiently small universal constant (e.g. c = 0.01 suffices).
Assume μ ≤ c∕κ? and λ ≥ 1∕(c ∙ 0m由).Let τo = Cmin{μ∕(KdamaX),λ}σmin(M)∙ Let δ ≤
min{ ? ^^m√σ4dK1∕2(M), τo∕2}, and ε = min{λσmin(M)1/2, cδ∕P∣∣Mk, cε0δσmin(M)}.
max
We first show that if the gradient is small, then the point cannot have a large component in S⊥.
Lemma C.6. If ∣∣VFa*,μ,λ,δ(B)Il ≤ ε, then ∣∣Ps⊥BkF ≤ ε∕δ.
Proof. Since Fa*,*,λ(B) only depends BPS, We know VFa*,*,λ(B)Ps⊥ = 0. Therefore ε ≥
∣∣VFa? ,μ,λ,δ (B)Ps⊥∣f = k(δB)Ps⊥ ∣f = δ∣Ps ⊥ BkF, and we have ∣∣BPs⊥ ∣f ≤ ε∕δ as desired.
Next we show that if the gradient of Fa*,μ,λ,δ(B) is small, and δ is also small, then the gradient of
Fa*,μ,λ(B) canbebounded.
Lemma C.7. In the setting of Theorem C.5, if ∣∣VFa*,μ,λ,δ (B)k ≤ ε ≤ λσmιin(M )1/2, then we
have	_____________
∣∣VFa*,μ,λ(B)k ≤ ε + δ√2m∕σmin(M).
Towards proving Lemma C.7, we first bound the norm of B by the following claim:
Claim C.8. If ∣∣VFa*,μ,λ,δ(B)k ≤ λσmin(M)1/2, then each row b must satisfy b>Mbi ≤ 2.
Proof. We prove by contradiction. Assume towards contradiction that there is a column bi such that
bi>Mbi ≥ 2. We consider the quantity,
hdF∂bμ,λ,δ (B),bii.
Note that Fa*,μ,λ,δ has 4 terms: (1) 2√6σ ∙ P^ αi Pjk即 (b?,bji2hb?,bfci2,⑵
- √ Pij∈[d] αihb?, bji4, (3) λPm=1((pi=1 αihbj,b?i2 - 1)2 - 1)2, (4)三∣B∣F.
Among these 4 terms, the first, third and forth terms all contribute positively to this inner-
product (because when bi is moved to (1 - ε)bi all those terms clearly decrease). Term 2
— Fj∈m] ai?hbi?, bj i4 contribute negatively. Therefore we can ignore terms 1 and 4:
h"∂j,λ,δ (B),bii ≥ h∂b^[-"√4μ Xa?hb?,bji4 + λ( X a?hb?,bji2 - 1)2],bii.
i∈[m]	i∈[m]
Let b>Mbi = C ≥ 2, we know Pi∈[m] a?hb?,bji4 ≤ amn Pi∈[m] (a?)2hb?,bji4 ≤ C^ammin.
Therefore,
√μ x a?hb?,bj i4],bii=- 4√μ x a?hb?,% i4 ≥ - 4√6μ ∙ aC-
6 i∈[m]	6 i∈[m]	6	amin
31
Published as a conference paper at ICLR 2018
On the other hand,
∂
hdb内£ a?hb?，bji2 — 1)2,bii =4λ(b>Mbi- 1)(b>Mbi) = 4λC(C - 1).
∂bi	i∈[m]
By the choice of λ, μ, We can see that the negative term is negligible, and We know
hd⅛μλδ(B),bii ≥ 2λC(C - 1)
∂bi
Since b>Mbi = C, we have ∣∣bik ≤ CC^mm(MA.). Therefore the norm of the gradient is at least
2λC(C 一 1)∕∣∣bi∣ ≥ 2√2λmmin(M), this contradicts with the assumption. The norm of the rows
must all be bounded.	□
ProofofLemma C.7. We have that b>Mbi ≤ 2 implies ∣∣bi∣2 ≤ 2∕mmin(M). The norm of the
whole matrix is bounded by∣B∣f ≤ '£黑Ikbik2 ≤ '2m∕mmin(M), so by triangle inequality
we have
∣NFoΛμ,λ(B)k ≤ ∣NFoΛμ,λ,δ(B)k+ δ∣∣B∣∣F ≤ ε + N?m/Qmin(M).
□
Finally we are ready to prove Theorem C.5.
Proof of Theorem C.5. We will separate B into two components BS = BPS and B⊥ = BPS⊥ .
We will first show that BS is close to the desirable solution. To do that we will use Theorem C.4
14. By the choice of ε, δ, we know from Lemma C.7 that ∣∣VFɑ*,μ,λ(Bs)∣ ≤ 2δ/2m∕mmin(M).
Also, V2Fa%μ,λ(Bs) ≥ V2Fa%μ,λ(B) — δ ≥ -τ0. Therefore we know BS must be of the form
[bS ]> = PDΓB? + E1,
where ∣E1 ∣ < ε0∕2. Also at the same time from the proof of Theorem C.4 we know BS = (PDO+
E0)W > where PDO + E0 has singular values close to 1. Therefore mmin(BS) ≥ mmin (W)∕2 =
1/2PW.
By Lemma C.6 we know ∣B⊥ ∣F ≤ ε∕δ. We apply inverse matrix perturbation (Theorem C.9)
again, using B = BS + B⊥, therefore we know
Bt = BS + E2,
where ∣E2 ∣ ≤ O(∣B⊥ ∣F ∕mm2 in (BS)) ≤ ε0∕2.
Combining these two perturbations we know
[Bt]> = P DΓB? + E1 + E2>,
and the error term Ei + E> has spectral norm at most ε0.	□
C.4 Toolbox: Matrix Perturbation
In the proof we used the following theorem for the perturbation of matrices.
Theorem C.9 (Stewart and Sun Stewart & guang Sun (1990)). Consider the perturbation ofa matrix
A: if B = A + E,then we have
kBt - Atk ≤√2kAtkkBtkkE∣.
As a corollary, if ∣E∣ ≤ mmin (A)∕2, then we have
kBt — Atk ≤ 2√2mmin(A)-2kEk.
14If we restrict all the vectors to the subspace S, we can still apply Theorem C.4 as long as we replace all
inverses with pseudo-inverses.
32
Published as a conference paper at ICLR 2018
D Recovering the Linear Layer
We will show in this section that if we have are given a δ-approximation of B? , then it is easy to
recover a? . The key observation here is that the correlation between the hbi? , xi and the output y is
exactly proportional to ai?. We also note that there could be multiple other ways to recover a?, e.g.,
using linear regression with the σ(Bx) as input and the y as output. We chose this algorithm mostly
because of the ease of analysis.
Algorithm 1 Recovering a?
Input: A matrix B with unit row norms that is row-wise δ-close to B? in Euclidean distance.
ɪʌ j	T . I	r-1τπ Γ / T ∖ T )	ŋ	.1	∙ ∙	1	c,	I I I	Λ 1
Return: Let ai = 2E[y(x, %)] where E means the empirical average. Set a% J |a] and b J
bisgn(a0i)
Lemma (Restatement of Lemma 2.5). Given a matrix B whose rows are δ-close to B? in Euclidean
distance up to permutation and sign flip with δ ≤ 1∕(2κ?). Then, we can give estimates a, B0 (using
e.g., Algorithm 1) such that there exists a permutation P where ∣∣a 一 Pa*k∞ ≤ 6。*&乂 and B0 is
row-wise δ-close to PB ?.
To see why this simple algorithm works for recovering a?, we need the following simple claim.
Claim D.1. For any vector v we have
1m
E[yhχ,vi] = 5 X a?hb?,vi.
2 i=1
The proof of this claim follows immediately from the property of Hermite polynomials. Now we
are ready to prove the corollary.
Proof. Without loss of generality we assume B is close to a sign flip of B ?. The unknown permu-
tation does not change the proof.
Since bi is δ close to Bi?, let u be the vector where uj = hbj?, bi 一 bi?i, we have
mm
ai = X a?hb?, bii = X。式网,b?i + W? bi - b?i) = a? + ha?, "i ∈ a? ±。^^.
i=1	i=1
Therefore a0i is always positive, ai is in the desirable range and ∣Bi0 一 Bi? ∣ ≤ δ.
Similarly, if -bi is δ close to B? We have ai ∈ -a? 土。m0方6, and the conclusion still holds.
□
For the settings considered in Section C, the vectors bi? are not necessarily orthogonal. In this case
we use the following algorithm:
Algorithm 2 Recovering a? for general case
Input: A matrix B with unit row norms, and B is δ-close to B? in spectral norm UP to permutation
and sign flip.
Let ui = 2E[yhx, bii] where E means the empirical average.
Let a0 = (BB>)-1u.
Return: Set ai J |a0i | and bi J bisgn(a0i)
Lemma D.2. Given a matrix B whose rows have unit norm, and ∣B - SPB? ∣ ≤ δ for some
permutation matrix P and diagonal matrix S with ±1 entries on diagonals.If ：&：?(EBm, we can give
estimates a, B0 (using e.g., Algorithm2)such that ∣a-Pa?| ≤ 2vz¾max√m∙δ and ∣B0-PB?| ≤ δ.
σmin(B)
33
Published as a conference paper at ICLR 2018
Proof. We again use Claim D.1: in this case we know the vector u satisfies u = B(B?)>a?. As a
result, for the vector a0, we have
a = (BB>)-1(B(B*)>)a* = (Bt)>(B *)>a* = (B?Bf)>a?.
By assumption we know B = SP B? + E where kE k ≤ δ. By the perturbation of matrix inverse
(Theorem C.9), We know if ∣∣E∣∣ ≤ δ ≤ σmin(B)/2, then Bt = (B*)*PTST + E0 where
∣∣E0k ≤ 2√2σmin(B)-2δ. Therefore
a0= (P-1S-1 + E0)>a? = S->P->a? + (E0)>a? = SPa + (E0)>a?.
(Here the last equality is because for both permutation matrix P and sign flip matrix S, P-> = P
and S-> = S.) Therefore, coordinates ofa0 are permutation and sign flips ofa?, up to an error term
(E0)>a?.
When δ ≤ 4⅛⅞, we know k (EO)>a*k ≤ kE'kamaxVm ≤ amin/2, therefore the signs are all
recovered correctly. After fixing the sign, we have ∣∣a 一 Pak ≤ ∣∣(E0)>a*k ≤ ”62-党χ√m, and
σmin(B)
∣∣B0 一 PB?k ≤ δ.	□
E S ample Complexity
In this section we will show that our algorithm only requires polynomially many samples to find the
desired solution. Note that we did not try to optimize the polynomial dependency.
Theorem E.1 (Theorem 2.7 Restated). In the setting of Theorem 2.3, suppose we use N empirical
samples to approximate G and obtain function Gb.
There exists a fixed polynomial such that if N ≥
poly(d, amax/amin, 1∕ε), with highProbabilityforanypoint B with λmin(V2Gb(B)) ≥ —τ0∕2 and
I I 7 zə / π ∖ I I - /C . 1	l ɪ	1	■ . .	l ɪ	7^Λ π . T-l I	I 1 "	.	. ■ 1-Λ ■
∣∣VG(B)∣ ≤ ε∕2, then B can be written as B = DP + E where P is a permutation matrix, D is a
diagonal matrix and |E∣∞ ≤ 0(£/(34。*由)).
In order to bound the sample complexity, we will prove a uniform convergence result: we show that
with polynomially many samples, the gradient and Hessian of G are point-wise close to the gradient
and Hessian ofG, therefore any approximate local minimum ofG must also be an approximate local
minimum of G.
However, there are two technical issues in showing the uniform convergence result. The first issue
is that when the norm of B is very large, both the gradient and Hessian of G and G are very large
and we cannot hope for good concentration. We deal with this issue by showing when B has a large
norm, the empirical gradient VG(B) must also have large norm, and therefore it can never be an
approximate local minimum (we do this later in Lemma E.5). The second issue is that our objective
function involves high-degree polynomials over Gaussian variables x, y, and is therefore not sub-
Gaussian or sub-exponential. We use a standard truncation argument to show that the function does
not change by too much if we restrict to the event that the Gaussian variables have bounded norm.
Lemma E.2. Suppose P0(B) + R(B) = E(x,y)[f (x, y, B)] where f is a polynomial of degree at
most 5 in x, y and at most 4 in B. Also assume that the sum of absolute values of coefficients is
bounded by Γ. For any ε ≤ Γ/2, let R = Cd log(amjaχΓ∕ε) for a large enough Constant C, let F
be the event that ∣x∣2 ≤ R, and let Gtrunc = E(x,y)[f (x, y, B)1F]. For any B such that ∣bi∣ ≤ 2
for all rows, we have
∣VG(B) 一 VGtrunc(B)∣ ≤ ε,
and
∣V2G(B) 一 V2Gtrunc(B)∣ ≤ ε,
Proof. By standard χ2 concentration bounds, for large enough C and any z > R, the probability
that ∣x∣2 ≥ Z is at most exp(-10z).
By simple calculation, it is easy to check that ∣∣Vbf(x,y,B)∣ ≤ drdLSamaxllxll5, and
IlVBf(x,y,B)∣∣ ≤ IZrd^maxllxlRWeknow IlVG(B)-VGtrunc(B)M = ∣∣E[Vbf(x,y,B)(1-
34
Published as a conference paper at ICLR 2018
1F)k. The expectation between kxk2 ∈ [2iR, 2i+1R], for i = 0, 1, 2, ..., is always bounded by
4Γd1.5amaχk2i+1Rk5 exp(-2iR)< ε∕2i+1. Therefore
∞
kVG(B) - VGtrunc(B)k ≤ Xε∕2i+1 ≤ ε.
i=0
The bound for the Hessian follows from the same argument.	□
Finally, we combine this truncation with a result of Mei et al. (2016) that proves universal con-
vergence of gradient and Hessian. For completeness here we state a version of their theorem with
bounded gradient/Hessian:
Theorem E.3 (Theorem 1 in Mei et al. (2016)). Let f(θ) be a function from Rp → R and f be its
empirical version. If the norm of the gradient and Hessian of a function is always bounded by τ,
for variables in a ball of radius r in p dimensions, there exists a universal constant C0 such that for
C = C0 max{logrτ∕δ, 1}, the following hold:
(a) The sample gradient converges to the population gradient. Namely ifN ≥ Cplogp we
have
- .. ,. ʌ ..
Pr[ sup kVf(θ) - Vfθ k ≤ T
kθk≤r
JCplog n 1 ≥ 1 -
n
δ.
(b) The sample Hessian converges to the empirical Hessian. Namely ifN ≥ Cplogp we have
Jc°log n I ≥ 1 -
n
- .. ,. ʌ ..
Pr[ sup kVf(θ) - Vfθ k ≤ τ
kθk≤r
δ.
As an immediate corollary of this theorem and Lemma E.2, we have
Corollary E.4. In the setting of Theorem 2.7, for every B whose rows have norm at most 2, we have
with high probability,
kVG(B)-VGb(B)k ≤ ε∕2,
and
kV2G(B) -V2Gb(B)k ≤ τ0∕2.
Proof. On the other hand, for all such matrices B, by Lemma E.2 we know the gradient and Hessian
of G is close to the gradient and Hessian of Gtrunc .
kVG(B) - VG
trunc(B)k ≤ ε∕4,
and
kV2G(B) - V2Gtrunc(B)k ≤ τ0∕4.
Now, the gradient and Hessian for individual samples for estimating Gtrunc are bounded by some
poly(d, 1∕ε), therefore by Theorem E.3 we know the gradient and Hessian of G are close to those
of Gtrunc. When N ≥ poly(d, 1∕ε) for a large enough polynomial, we have with high probability,
for all B with all rows kbi k ≤ 2,
kVGtrunc(B) - VGb(B)k ≤ ε∕4,
and
kV2Gtrunc(B) - V2Gb(B)k ≤ τ0∕4.
The corollary then follows from triangle inequality.	□
Finally we handle the case when B has a row with large norm. We will show that in this case VG(B)
must also be large, so B cannot be an approximate local minimum.
Lemma E.5. If bi is the row with largest norm and ∣∣bik ≥ 2, then when N ≥ poly(d,。£0方/0m加)
for some fixed polynomial, we have with high probability hVG(B), bii ≥ cλkbik4 for some universal
constant c > 0.
35
Published as a conference paper at ICLR 2018
Proof. The proof of this Lemma is very similar to Claim C.8. Note that by equa-
tion (2.7) there are three terms in Gb(B):(1) sign(σ4)E Iy ∙ Pj,k∈[d],j=k Φ(bj,bk,x)], (2)
-μ sign(σ4)E Iy ∙ Pj∈[d]
the samples.
夕(bj,x)], (3) λ Pm=I(IIbik2 - 1)2. Here E is the empirical average over
Note that the first two terms are homogeneous degree 4 polynomials over B, and the third term does
not depend on the sample. By argument similar to Corollary E.4, we know for any B where bi has
the largest row norm, with the number of samples we choose the gradient of the first two terms is
ca?minIbiI3 close to the gradient of their expectations, where c < 0.01 is a small constant.
By Theorem 2.6, We know the expectation of the first two terms are equal to A1(B) = 2√6∣σ4∣ ∙
Pi∈[d]a?Pj,k∈[d],j=khb?，bji2hb?，bki2 and A2(B)=-医 P"["h%bji4. Herethegra-
dient of the first term always have positive correlation with bi , so we can ignore it. For the second
term, we know the gradient
∂b^ [A2(B)I=-∣σ√6μ χ 垮伤也〉3 b?.
Taking the inner-product with bi , and use the fact that bi? form an orthonormal basis, we know
∂
h 而[A2(B)],bii≥-caminkbik4.
∂bi
On the other hand, when IbiI ≥ 2, we have for the third term
∂
h够[λ(kbik2 - 1)2],bii ≥ λ(kbik- 1)4 ≥ λkbik4∕16.
Since λ is larger than a?max, we know the negative contribution from A2 and the difference between
the empirical version and G are both negligible. Therefore we have WG(B) b%) ≥ cλ∣hi∣4 as
□
desired.
Now we are ready to prove Theorem 2.7:
Proof. By Lemma E.5, any point B with VG(B) ≤ ε must have ∣∣bi∣ ≤ 2 for all i. Now by
Corollary E.4, we know the point B we have must satisfy
∣VG(B)∣ ≤ ε; V2G(B)	-τ0Id.
By point 3 in Theorem 2.3, this implies the guarantee on B.
□
F SPURIOUS LOCAL MINIMUM FOR FUNCTION P0
In this section we give an example where the function P0 does have spurious local minimum.
In this example, d = 4, and the true vectors are the standard basis vectors bi? = ei . We will set
a1? = 1, and a?2 = a3? = a4? = 2 + δ (where δ > 0 is an arbitrary positive constant).
The spurious local minimum that we consider is bi = b2 = eι = b?, b3 = e2 = b?, b4 =
√e3 + √e4. That is,
∖
Ooo02
Ooo √22
0010
110 0
∕n^
=
B
/
The objective P0(B) = 1 and the only non-zero term is a1?hb?1, b1i2 hb1?, b2i2. In order to improve the
objective locally, we need to change either b1 or b2, otherwise the term a1?hb1?, b1i2hb1?, b2i2 is still 1,
and all other terms (ai?hbi?, bji2 hbi?, bki2) are non-negative.
36
Published as a conference paper at ICLR 2018
Assume We have a local perturbation B0, where b1 =，1 - ε2e1 + ε1u1, b2 =，1 - ε2e1 + ε2u2.
Here u1 , u2 are unit vectors that are orthogonal toe1 . Also, since this is a local perturbation, we
make sure ε1, ε2 ≤ ε, and b3(2) ≥ 1 - ε, [b4(3)]2, [b4(4)]2 ≥ 0.5 - ε. We will show that when ε is
small enough, the objective function P 0(B0) ≥ 1.
To see this, notice that the term a1?hb1?, b1i2hb1?, b2i2 is now equal to (1 - ε21)(1 - ε22). On the other
hand, for b1, we have
4	4	44
X ai? Xhbi?, b1i2hbi?, bki2 = ε12(2 + δ) X Xhbi?, u1i2hbi?, bki2
i=2	k=3	i=2 k=3
44
= ε12(2 + δ) Xhbi?, u1i2(Xhbi?, bki2)
i=2	k=3
4	44
≥ ε2(2 +δ) Ehb?, uιi2 •叫{£位 bk i2}
i=2	= k=3
≥ ε21(2 + δ)(0.5 -ε).
Similarly we have the same equation for b2 . Note that all the terms we analyzed are disjoint, there-
fore
P 0(B0) ≥ (1 - ε12)(1 -ε22) + ε21(2 + δ)(0.5 - ε) + ε22(2 + δ)(0.5 - ε).
By removing higher order terms of ε, it is easy to see that P 0(B0) ≥ 1 when ε is small enough.
Therefore B is a local minima of P 0 .
37