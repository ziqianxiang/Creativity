Published as a conference paper at ICLR 2018
The power of deeper networks
FOR EXPRESSING NATURAL FUNCTIONS
David Rolnick, Max Tegmark
Massachusetts Institute of Technology
{drolnick, tegmark}@mit.edu
Ab stract
It is well-known that neural networks are universal approximators, but that deeper
networks tend in practice to be more powerful than shallower ones. We shed light
on this by proving that the total number of neurons m required to approximate nat-
ural classes of multivariate polynomials of n variables grows only linearly with n
for deep neural networks, but grows exponentially when merely a single hidden
layer is allowed. We also provide evidence that when the number of hidden layers
is increased from 1 to k, the neuron requirement grows exponentially not with n
but with n1/k, suggesting that the minimum number of layers required for practi-
cal expressibility grows only logarithmically with n.
1	Introduction
Deep learning has lately been shown to be a very powerful tool for a wide range of problems, from
image segmentation to machine translation. Despite its success, many of the techniques developed
by practitioners of artificial neural networks (ANNs) are heuristics without theoretical guarantees.
Perhaps most notably, the power of feedforward networks with many layers (deep networks) has not
been fully explained. The goal of this paper is to shed more light on this question and to suggest
heuristics for how deep is deep enough.
Itis well-known (Cybenko, 1989; Funahashi, 1989; Hornik et al., 1989; Barron, 1994; Pinkus, 1999)
that neural networks with a single hidden layer can approximate any function under reasonable
assumptions, but it is possible that the networks required will be extremely large. Recent authors
have shown that some functions can be approximated by deeper networks much more efficiently
(i.e. with fewer neurons) than by shallower ones. Often, these results admit one or more of the
following limitations: “existence proofs” without explicit constructions of the functions in question;
explicit constructions, but relatively complicated functions; or applicability only to types of network
rarely used in practice.
It is important and timely to extend this work to make it more concrete and actionable, by deriving
resource requirements for approximating natural classes of functions using today’s most common
neural network architectures. Lin et al. (2017) recently proved that it is exponentially more efficient
to use a deep network than a shallow network when Taylor-approximating the product of input
variables. In the present paper, we move far beyond this result in the following ways: (i) we use
standard uniform approximation instead of Taylor approximation, (ii) we show that the exponential
advantage of depth extends to all general sparse multivariate polynomials, and (iii) we address the
question of how the number of neurons scales with the number of layers. Our results apply to
standard feedforward neural networks and are borne out by empirical tests.
Our primary contributions are as follows:
•	It is possible to achieve arbitrarily close approximations of simple multivariate and uni-
variate polynomials with neural networks having a bounded number of neurons (see §3).
•	Such polynomials are exponentially easier to approximate with deep networks than
with shallow networks (see §4).
1
Published as a conference paper at ICLR 2018
•	The power of networks improves rapidly with depth; for natural polynomials, the number
of layers required is at most logarithmic in the number of input variables, where the base
of the logarithm depends upon the layer width (see §5).
2	Related Work
Deeper networks have been shown to have greater representational power with respect to various
notions of complexity, including piecewise linear decision boundaries (Montufar et al., 2014) and
topological invariants (Bianchini & Scarselli, 2014). Recently, Poole et al. (2016) and Raghu et al.
(2016) showed that the trajectories of input variables attain exponentially greater length and curva-
ture with greater network depth.
Work including Daniely (2017); Eldan & Shamir (2016); Pinkus (1999); Poggio et al. (2017); Tel-
garsky (2016) shows that there exist functions that require exponential width to be approximated by
a shallow network. Barron (1994) provides bounds on the error in approximating general functions
by shallow networks. Mhaskar et al. (2016) and Poggio et al. (2017) show that for compositional
functions (those that can be expressed by recursive function composition), the number of neurons
required for approximation by a deep network is exponentially smaller than the best known upper
bounds for a shallow network. Mhaskar et al. (2016) ask whether functions with tight lower bounds
must be pathologically complicated, a question which we answer here in the negative.
Various authors have also considered the power of deeper networks of types other than the stan-
dard feedforward model. The problem has also been posed for sum-product networks (Delalleau
& Bengio, 2011) and restricted Boltzmann machines (Martens et al., 2013). Cohen et al. (2016)
showed, using tools from tensor decomposition, that shallow arithmetic circuits can express only a
measure-zero set of the functions expressible by deep circuits. A weak generalization of this result
to convolutional neural networks was shown in Cohen & Shashua (2016).
3	The power of approximation
In this paper, we will consider the standard model of feedforward neural networks (also called mul-
tilayer perceptrons). Formally, the network may be considered as a multivariate function N (x) =
Akσ(…σ(Aισ(Aox)) …)，where A0, Ai,..., Ak are constant matrices and σ denotes a scalar
nonlinear function applied element-wise to vectors. The constant k is referred to as the depth of the
network. The neurons of the network are the entries of the vectors σ(A' •…σ(Aι σ(A°x)) •…)，for
` = 1, . . . , k - 1. These vectors are referred to as the hidden layers of the network.
Two notions of approximation will be relevant in our results: -approximation， also known as uni-
form approximation， and Taylor approximation.
Definition 3.1. For constant > 0, we say that a network N(x) -approximates a multivariate
function f(x) (for x in a specified domain (-R, R)n) if supx |N (x) - f (x)| < .
Definition 3.2. We say that a network N (x) Taylor-approximates a multivariate polynomial p(x)
of degree d if p(x) is the dth order Taylor polynomial (about the origin) of N (x).
The following proposition shows that Taylor approximation implies -approximation for homoge-
neous polynomials. The reverse implication does not hold.
Proposition 3.3. Suppose that the network N(x) Taylor-approximates the homogeneous multivari-
ate polynomial p(x). Then, for every , there exists a network N(x) that -approximates p(x), such
that N(x) and N(x) have the same number of neurons in each layer. (This statement holds for
x ∈ (-R, R)n for any specified R.)
Proof. Suppose that N(x) = Akσ(∙∙∙ σ(Aισ(A°x)) •…)and thatP(X) has degree d. SinceP(X) is
a Taylor approximation of N(x)， we can write N(x) as p(x) +E(x)， where E(x) = Pi∞=d+1 Ei(x)
is a Taylor series with each Ei (X) homogeneous of degree i. Since N(X) is the function defined
by a neural network, it converges for every X ∈ Rn. Thus, E(x) converges, as does E(δx)∕δd =
Pi∞=d+1 δi-dEi (X). By picking δ sufficiently small， we can make each term δi-dEi(X) arbitrarily
small. Let δ be small enough that ∣E(δx)∕δd∣ < E holds for all X in (-R, R)n.
2
Published as a conference paper at ICLR 2018
Let A0 = δAo, Ak = Ak∕δd, and A' = a` for ' = 1,2,... ,k - 1. Then, for Ne(X)=
Akσ(…σ(A1σ(A0x))…),we observe that Ne(X) = N(δx)∕δd, and therefore:
∣Ne(x) - p(x)∣ = |N(δx)∕δd - p(x)∣
=∣p(δx)∕δd + E(δx)∕δd — p(x) |
=|E (δx)∕δd∣
< .
We conclude that Ne(X) is an ^-approximation of p(x), as desired.	□
For a fixed nonlinear function σ, we consider the total number of neurons (excluding input and
output neurons) needed for a network to approximate a given function. Remarkably, it is possible to
attain arbitrarily good approximations of a (not necessarily homogeneous) multivariate polynomial
by a feedforward neural network, even with a single hidden layer, without increasing the number of
neurons past a certain bound. (See also Corollary 1 in Poggio et al. (2017).)
Theorem 3.4. Suppose that p(x) is a degree-d multivariate polynomial and that the nonlinearity σ
has nonzero Taylor coefficients up to degree d. Let mek (p) be the minimum number of neurons in a
depth-k network that -approximates p. Then, the limit lime→0 mek(p) exists (and is finite). (Once
again, this statement holds for x ∈ (-R, R)n for any specified R.)
Proof. We show that lime→0 me1 (p) exists; it follows immediately that lime→0 mek (p) exists for
every k, since an -approximation to p with depth k can be constructed from one with depth 1.
Let p1(x),p2(x), . . . ,ps(x) be the monomials of p(x), so that p(x) = Pipi(x). We claim that
eachpi(x) can be Taylor-approximated by a network Ni(x) with one hidden layer. This follows, for
example, from the proof in Lin et al. (2017) that products can be Taylor-approximated by networks
with one hidden layer, since each monomial is the product of several inputs (with multiplicity); we
prove a far stronger result about Ni (x) later in this paper (see Theorem 4.1).
Suppose now that Ni (x) has mi hidden neurons. By Proposition 3.3, we conclude that since pi (x)
is homogeneous, it may be δ-approximated by a network Nδi(x) with mi hidden neurons, where
δ = ∕s. By combining the networks Nδi(x) for each i, we can define a network Ne(x) = Pi Nδi(x)
with Pi mi neurons. Then, we have:
INe(X)- P(X)I ≤ X M(X)- Pi(X)I
i
≤ X δ = sδ = .
i
Hence, Ne(x) is an -approximation of P(x), implying that me1(P) ≤ Pi mi for every . Thus,
lime→o m1(p) exists, as desired.	□
This theorem is perhaps surprising, since it is common for -approximations to functions to require
ever-greater complexity, approaching infinity as → 0. For example, the function exp(I - xI) may
be approximated on the domain (-π, π) by Fourier sums of the form Pkm=0 am cos(kx). However,
in order to achieve ^-approximation, We need to take m 〜 1∕√e terms. By contrast, We have
shown that a finite neural network architecture can achieve arbitrarily good approximations merely
by altering its Weights.
Note also that the assumption of nonzero Taylor coefficients cannot be dropped from Theorem 3.4.
For example, the theorem is false for rectified linear units (ReLUs), Which are pieceWise linear
and do not admit a Taylor series. This is because -approximating a non-linear polynomial With a
pieceWise linear function requires an ever-increasing number of pieces as → 0.
Theorem 3.4 alloWs us to make the folloWing definition:
Definition 3.5. Suppose that a nonlinear function σ is given. For P a multivariate polynomial,
let mukniform (P) be the minimum number of neurons in a depth-k network that -approximates P for
3
Published as a conference paper at ICLR 2018
all arbitrarily small. Set muniform(p) = mink mukniform (p). Likewise, let mTkaylor(p) be the min-
imum number of neurons in a depth-k network that Taylor-approximates p, and set mTaylor(p) =
Taylor
mink mk	(p).
In the next section, we will show that there is an exponential gap between mu1niform(p) and muniform(p)
and between m1Taylor(p) and mTaylor(p) for various classes of polynomials p.
4	The inefficiency of shallow networks
In this section, we compare the efficiency of shallow networks (those with a single hidden layer) and
deep networks at approximating multivariate polynomials. Proofs of our main results are included
in the Appendix.
4.1	Multivariate polynomials
Our first result shows that uniform approximation of monomials requires exponentially more neu-
rons in a shallow than a deep network.
Theorem 4.1.	Let p(x) denote the monomial x；1 x$2 •…Xnn, with d = En=I r. Suppose that the
nonlinearity σ has nonzero Taylor coefficients up to degree 2d. Then, we have:
(i)	mu1niform(p) = Qin=1(ri + 1),
(ii)	muniform(p) ≤ Pin=1(7dlog2(ri)e +4),
where dxe denotes the smallest integer that is at least x.
We can prove a comparable result for mTaylor under slightly weaker assumptions on σ. Note that
by setting r1 =r2 = . . . =rn = 1, we recover the result of Lin et al. (2017) that the product of
n numbers requires 2n neurons in a shallow network but can be Taylor-approximated with linearly
many neurons in a deep network.
Theorem 4.2.	Letp(x) denote the monomial x；1 x；2 ∙∙∙ Xnn, with d = En=I r. Suppose that σ has
nonzero Taylor coefficients up to degree d. Then, we have:
(i)	mT1aylor(p) = Qin=1(ri + 1),
(ii)	mTaylor(p) ≤ Pin=1(7dlog2(ri)e +4).
It is worth noting that neither of Theorems 4.1 and 4.2 implies the other. This is because it is
possible for a polynomial to admit a compact uniform approximation without admitting a compact
Taylor approximation.
It is natural now to consider the cost of approximating general polynomials. However, without
further constraint, this is relatively uninstructive because polynomials of degree d in n variables
live within a space of dimension n+dd , and therefore most require exponentially many neurons for
any depth of network. We therefore consider polynomials of sparsity c: that is, those that can be
represented as the sum of c monomials. This includes many natural functions.
The following theorem, when combined with Theorems 4.1 and 4.2, shows that general polynomials
p with subexponential sparsity have exponentially large mu1niform(p) and m1Taylor(p), but subexponen-
tial muniform(p) and mTaylor(p).
Theorem 4.3.	Let p(x) be a multivariate polynomial of degree d and sparsity c, having monomials
q1 (x), q2(x), . . . , qc(x). Suppose that the nonlinearity σ has nonzero Taylor coefficients up to degree
2d. Then, we have:
uniform	1	uniform
(i)	mi	(P) ≥ C maxj mJ	(qj).
(ii)	muniform(p) ≤ Pj muniform(qj ).
4
Published as a conference paper at ICLR 2018
These statements also hold if muniform is replaced with mTaylor.
As mentioned above with respect to ReLUs, some assumptions on the Taylor coefficients of the
activation function are necessary for the results we present. However, it is possible to loosen the
assumptions of Theorem 4.1 and 4.2 while still obtaining exponential lower bounds on mu1niform(p)
and m1Taylor(p):
Theorem 4.4.	Let p(x) denote the monomial x；1 x$2 •…Xnn, with d = En=I r∙ Suppose that the
nonlinearity σ has nonzero dth Taylor coefficient (other Taylor coefficients are allowed to be zero).
Then, mUnform(P) and mTyyor(P) are at least d Qn=1(ri + 1). (An even better lower bound is the
maximum coefficient in the polynomial i(1 + y + . . . + yri ).)
4.2	Univariate polynomials
As with multivariate polynomials, depth can offer an exponential savings when approximating uni-
variate polynomials. We show below (Proposition 4.5) that a shallow network can approximate any
degree-d univariate polynomial with a number of neurons at most linear in d. The monomial xd
requires d + 1 neurons in a shallow network (Proposition 4.6), but can be approximated with only
logarithmically many neurons in a deep network. Thus, depth allows us to reduce networks from
linear to logarithmic size, while for multivariate polynomials the gap was between exponential and
linear. The difference here arises because the dimensionality of the space of univariate degree-d
polynomials is linear in d, which the dimensionality of the space of multivariate degree-d polyno-
mials is exponential in d.
Proposition 4.5. Suppose that the nonlinearity σ has nonzero Taylor coefficients up to degree d.
Then, mT1aylor(P) ≤ d + 1 for every univariate polynomial P of degree d.
Proof. Pick a0, a1 , . . . , ad to be arbitrary, distinct real numbers. Consider the Vandermonde matrix
A with entries	Aij	=	aij .	It is well-known that det(A)	=	Qi<i0 (ai0	-	ai)	6=	0. Hence, A is
invertible, which means that multiplying its columns by nonzero values gives another invertible
matrix. Suppose that we multiply the jth column of A by σj to get A0, where σ(x) = Pj σjxj is
the Taylor expansion of σ(x).
Now, observe that the ith row of A0 is exactly the coefficients of σ(aix), up to the degree-d term.
Since A0 is invertible, the rows must be linearly independent, so the polynomials σ(aix), restricted
to terms of degree at most d, must themselves be linearly independent. Since the space of degree-d
univariate polynomials is (d + 1)-dimensional, these d + 1 linearly independent polynomials must
span the space. Hence, m1Taylor(P) ≤ d+ 1 for any univariate degree-d polynomial P. In fact, we can
fix the weights from the input neuron to the hidden layer (to be a0 , a1 , . . . , ad, respectively) and still
represent any polynomial P with d + 1 hidden neurons.	□
Proposition 4.6. Let P(x) = xd, and suppose that the nonlinearity σ(x) has nonzero Taylor coeffi-
cients up to degree 2d. Then, we have:
(i)	mu1niform(P) =d+1.
(ii)	muniform(P) ≤ 7dlog2(d)e.
These statements also hold if muniform is replaced with mTaylor.
Proof. Part (i) follows from part (i) of Theorems 4.1 and 4.2 by setting n = 1 and r1 = d.
For part (ii), observe that we can Taylor-approximate the square x2 of an input x with three neurons
in a single layer:
n /L、(σ(x) + σ(-x) - 2σ(0)) = x2 + O(x4 + X5 + ...).
2σ00(0)
We refer to this construction as a square gate, and the construction of Lin et al. (2017) as a product
gate. We also use identity gate to refer to a neuron that simply preserves the input of a neuron from
the preceding layer (this is equivalent to the skip connections in residual nets (He et al., 2016)).
5
Published as a conference paper at ICLR 2018
Consider a network in which each layer contains a square gate (3 neurons) and either a product
gate or an identity gate (4 or 1 neurons, respectively), according to the following construction: The
square gate squares the output of the preceding square gate, yielding inductively a result of the form
x2k , where k is the depth of the layer. Writing d in binary, we use a product gate if there is a 1 in
the 2k-1 -place; if so, the product gate multiplies the output of the preceding product gate by the
output of the preceding square gate. If there is a 0 in the 2k-1 -place, we use an identity gate instead
of a product gate. Thus, each layer computes x2k and multiplies x2k-1 to the computation if the
2k-1-place in d is 1. The process stops when the product gate outputs xd.
This network clearly uses at most 7dlog2(d)e neurons, with a worst case scenario where d + 1 is a
power of 2. Hence mTaylor(P) ≤ 7「log2(d)], with muniform(p) ≤ mTaylor(p) by Proposition 3.3 since
P is homogeneous.	□
5	How efficiency improves with depth
We now consider how mukniform(P) scales with k, interpolating between exponential in n (for k = 1)
and linear in n (for k = log n). In practice, networks with modest k > 1 are effective at represent-
ing natural functions. We explain this theoretically by showing that the cost of approximating the
product polynomial drops off rapidly as k increases.
By repeated application of the shallow network construction in Lin et al. (2017), we obtain the
following upper bound on mukniform(P), which we conjecture to be essentially tight. Our approach
leverages the compositionality of polynomials, as discussed e.g. in Mhaskar et al. (2016) and Poggio
et al. (2017), using a tree-like neural network architecture.
Theorem 5.1. Let p(x) equal the product x1x2 •…Xn, and suppose σ has nonzero Taylor Coefi-
cients up to degree n. Then, we have:
mUnform(P) = O (n(k-1)/k ∙ 2n1/k) .	(1)
Proof. We construct a network in which groups of the n inputs are recursively multiplied up to
Taylor approximation. Then inputs are first divided into groups of size b1, and each group is
multiplied in the first hidden layer using 2b1 neurons (as described in Lin et al. (2017)). Thus, the
first hidden layer includes a total of 2b1 n/b1 neurons. This gives us n/b1 values to multiply, which
are in turn divided into groups of size b2. Each group is multiplied in the second hidden layer using
2b2 neurons. Thus, the second hidden layer includes a total of 2b2n/(b1b2) neurons.
We continue in this fashion for b1,b2, ...,bk such that b® ∙…bk = n, giving US one neuron which
is the product of all of our inputs. By considering the total number of neurons used, we conclude
k	kk
mTaylor ≤ XQnT 2bi = X∣ Y bj I 2bi.	⑵
i=1	j=1 bj	i=1 j =i+1
By Proposition 3.3, mukniform(P) ≤ mTkaylor(P) since P is homogeneous. Setting bi = n1/k, for each i,
gives us the desired bound (1).	□
In fact, we can solve for the choice of bi such that the upper bound in (2) is minimized, under the
condition b1b2 •…bk = n. Using the technique of Lagrange multipliers, We know that the optimum
occurs at a minimum of the function
k	kk
n-Ybi λ+X Y bj 2bi.
i=1	i=1 j =i+1
6
Published as a conference paper at ICLR 2018
Figure 1: The optimal settings for {bi }ik=1 as n
varies are shown for k = 1, 2, 3. Observe that
the bi converge to n1/k for large n, as witnessed
by a linear fit in the log-log plot. The exact val-
ues are given by equations (4) and (5).
Figure 2: Performance of trained networks in
approximating the product of 20 input variables,
ranging from red (high error) to blue (low er-
ror). The error shown here is the expected ab-
solute difference between the predicted and ac-
tual product. The curve W = n(k-1)/k ∙ 2n1/k
for n = 20 is shown in black. In the region
above and to the right of the curve, it is possible
to effectively approximate the product function
(Theorem 5.1).
Differentiating L with respect to bi , we obtain the conditions
0= -λ Y bj + X (Qj=h+1bj ! 2bh + (log2) I YY bj I 2bi, for 1 ≤ i ≤ k ⑶
j6=i	h=1	bi	j=i+1
k
0 = n - Y bj.	(4)
j=1
Dividing (3) by Qjk=i+1 bj and rearranging gives us the recursion
bi = bi-1 +log2(bi-1 - 1/ log 2).	(5)
Thus, the optimal bi are not exactly equal but very slowly increasing with i (see Figure 1).
The following conjecture states that the bound given in Theorem 5.1 is (approximately) optimal.
Conjecture 5.2. Let p(x) equal to the product x1x2 •…Xn, and suppose that σ has all nonzero
Taylor coefficients. Then, we have:
mUniform (P) = 2©(n1/k),	(6)
i.e., the exponent grows as n1/k for n → ∞.
We empirically tested Conjecture 5.2 by training ANNs to predict the product of input values
x1 , . . . , xn with n = 20 (see Figure 2). The rapid interpolation from exponential to linear width
aligns with our predictions.
In our experiments, we used feedforward networks with dense connections between successive lay-
ers. In the figure, we show results for σ(x) = tanh(x) (note that this behavior is even better than
expected, since this function actually has numerous zero Taylor coefficients). Similar results were
also obtained for rectified linear units (ReLUs) as the nonlinearity, despite the fact that this function
7
Published as a conference paper at ICLR 2018
does not even admit a Taylor series. The number of layers was varied, as was the number of neurons
within a single layer. The networks were trained using the AdaDelta optimizer (Zeiler, 2012) to
minimize the absolute value of the difference between the predicted and actual values. Input vari-
ables xi were drawn uniformly at random from the interval [0, 2], so that the expected value of the
output would be of manageable size.
Eq. (6) provides a helpful rule of thumb for how deep is deep enough. Suppose, for instance, that
We wish to keep typical layers no wider than about a thousand (〜210) neurons. Eq. (6) then implies
n1/k < 10, i.e., that the number of layers should be at least
k > log 10 n.
It would be very interesting if one could show that general polynomials p in n variables require
a superpolynomial number of neurons to approximate for any constant number of hidden layers.
The analogous statement for Boolean circuits - whether the complexity classes T C0 and T C1 are
equal - remains unresolved and is assumed to be quite hard. Note that the formulations for Boolean
circuits and deep neural networks are independent statements (neither would imply the other) due
to the differences between computation on binary and real values. Indeed, gaps in expressivity
have already been proven to exist for real-valued neural networks of different depths, for which the
analogous results remain unknown in Boolean circuits (see e.g. Mhaskar (1993); Chui et al. (1994;
1996); Montufar et al. (2014); Cohen et al. (2016); Telgarsky (2016)).
6	Conclusion
We have shown how the power of deeper ANNs can be quantified even for simple polynomials. We
have proved that arbitrarily good approximations of polynomials are possible even with a fixed num-
ber of neurons and that there is an exponential gap between the width of shallow and deep networks
required for approximating a given sparse polynomial. For n variables, a shallow network requires
size exponential in n, while a deep network requires at most linearly many neurons. Networks with
a constant number k > 1 of hidden layers appear to interpolate between these extremes, following
a curve exponential in n1/k. This suggests a rough heuristic for the number of layers required for
approximating simple functions with neural networks. For example, if we want no layers to have
more than 210 neurons, say, then the minimum number of layers required grows only as log10 n. To
further improve efficiency using the O(n) constructions we have presented, it suffices to increase
the number of layers by a factor of log2 10 ≈ 3, to log2 n.
The key property we use in our constructions is compositionality, as detailed in Poggio et al. (2017).
It is worth noting that as a consequence our networks enjoy the property of locality mentioned in
Cohen et al. (2016), which is also a feature of convolutional neural nets. That is, each neuron in a
layer is assumed to be connected only to a small subset of neurons from the previous layer, rather
than the entirety (or some large fraction). In fact, we showed (e.g. Prop. 4.6) that there exist natural
functions computable with linearly many neurons, with each neuron is connected to at most two
neurons in the preceding layer, which nonetheless cannot be computed with fewer than exponentially
many neurons in a single layer, no matter how may connections are used. Our construction can also
be framed with reference to the other properties mentioned in Cohen et al. (2016): those of sharing
(in which weights are shared between neural connections) and pooling (in which layers are gradually
collapsed, as our construction essentially does with recursive combination of inputs).
This paper has focused exclusively on the resources (neurons and synapses) required to compute a
given function for fixed network depth. (Note also results ofLu et al. (2017); Hanin & Sellke (2017);
Hanin (2017) for networks of fixed width.) An important complementary challenge is to quantify
the resources (e.g. training steps) required to learn the computation, i.e., to converge to appropriate
weights using training data — possibly a fixed amount thereof, as suggested in Zhang et al. (2017).
There are simple functions that can be computed with polynomial resources but require exponential
resources to learn (Shalev-Shwartz et al., 2017). It is quite possible that architectures we have not
considered increase the feasibility of learning. For example, residual networks (ResNets) (He et al.,
2016) and unitary nets (see e.g. Arjovsky et al. (2016); Jing et al. (2017)) are no more powerful in
representational ability than conventional networks of the same size, but by being less susceptible
to the “vanishing/exploding gradient” problem, it is far easier to optimize them in practice. We look
forward to future work that will help us understand the power of neural networks to learn.
8
Published as a conference paper at ICLR 2018
7	Acknowledgments
This work was supported by the Foundational Questions Institute http://fqxi.org/, the Roth-
berg Family Fund for Cognitive Science and NSF grant 1122374. We would like to thank Tomaso
Poggio, Scott Aaronson, Surya Ganguli, David Budden, Henry Lin, and the anonymous referees for
helpful suggestions and discussions, and the Center for Brains, Minds, & Machines for an excellent
working environment.
References
Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In
International Conference on Machine Learning (ICML), pp. 1120-1128, 2016.
Andrew R Barron. Approximation and estimation bounds for artificial neural networks. Machine
Learning, 14(1):115-133, 1994.
Monica Bianchini and Franco Scarselli. On the complexity of neural network classifiers: A compar-
ison between shallow and deep architectures. IEEE transactions on neural networks and learning
systems, 25(8):1553-1565, 2014.
Charles K Chui, Xin Li, and Hrushikesh Narhar Mhaskar. Limitations of the approximation capa-
bilities of neural networks with one hidden layer. Advances in Computational Mathematics, 5(1):
233-243, 1996.
CK Chui, Xin Li, and HN Mhaskar. Neural networks for localized approximation. Mathematics of
Computation, 63(208):607-623, 1994.
Nadav Cohen and Amnon Shashua. Convolutional rectifier networks as generalized tensor decom-
positions. In International Conference on Machine Learning (ICML), 2016.
Nadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A tensor
analysis. Journal of Machine Learning Research (JMLR), 49, 2016.
George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Con-
trol, Signals, and Systems (MCSS), 2(4):303-314, 1989.
Amit Daniely. Depth separation for neural networks. In Conference On Learning Theory (COLT),
2017.
Olivier Delalleau and Yoshua Bengio. Shallow vs. deep sum-product networks. In Advances in
Neural Information Processing Systems (NIPS), pp. 666-674, 2011.
Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In Annual
Conference on Learning Theory (COLT), pp. 907-940, 2016.
Ken-Ichi Funahashi. On the approximate realization of continuous mappings by neural networks.
Neural networks, 2(3):183-192, 1989.
Boris Hanin. Universal function approximation by deep neural nets with bounded width and ReLU
activations. arXiv preprint arXiv:1708.02691, 2017.
Boris Hanin and Mark Sellke. Approximating continuous functions by ReLU nets of minimal width.
arXiv preprint arXiv:1710.11278, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778, 2016.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are uni-
versal approximators. Neural networks, 2(5):359-366, 1989.
Li Jing, Yichen Shen, Tena Dubcek, John Peurifoy, Scott Skirlo, Yann LeCun, Max Tegmark, and
Marin SoljaCic. Tunable efficient unitary neural networks (EUNN) and their application to RNNs.
In International Conference on Machine Learning (ICML), pp. 1733-1741, 2017.
9
Published as a conference paper at ICLR 2018
Henry W Lin, Max Tegmark, and David Rolnick. Why does deep and cheap learning work so well?
Journal OfStatistical Physics,168(6):1223-1247, 2017.
Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The expressive power of
neural networks: A view from the width. In Advances in Neural Information Processing Systems
(NIPS), pp. 6232-6240, 2017.
James Martens, Arkadev Chattopadhya, Toni Pitassi, and Richard Zemel. On the representational
efficiency of restricted Boltzmann machines. In Advances in Neural Information Processing Sys-
tems (NIPS), pp. 2877-2885, 2013.
Hrushikesh Mhaskar, Qianli Liao, and Tomaso Poggio. Learning functions: When is deep better
than shallow. arXiv:1603.00988v4, 2016.
Hrushikesh Narhar Mhaskar. Approximation properties of a multilayered feedforward artificial neu-
ral network. Advances in Computational Mathematics, 1(1):61-80, 1993.
Guido F Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear
regions of deep neural networks. In Advances in Neural Information Processing Systems (NIPS),
pp. 2924-2932, 2014.
Allan Pinkus. Approximation theory of the mlp model in neural networks. Acta Numerica, 8:
143-195, 1999.
Tomaso Poggio, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao. Why
and when can deep-but not shallow-networks avoid the curse of dimensionality: A review. Inter-
national Journal of Automation and Computing, 15(5):503-519, 2017.
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Expo-
nential expressivity in deep neural networks through transient chaos. In Advances In Neural
Information Processing Systems (NIPS), pp. 3360-3368, 2016.
Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. Survey of
expressivity in deep neural networks. arXiv:1611.08083, 2016.
Shai Shalev-Shwartz, Ohad Shamir, and Shaked Shammah. Failures of gradient-based deep learning.
In International Conference on Machine Learning (ICML), pp. 3067-3075, 2017.
Matus Telgarsky. Benefits of depth in neural networks. Journal of Machine Learning Research
(JMLR), 49, 2016.
Matthew D Zeiler. ADADELTA: an adaptive learning rate method. arXiv:1212.5701, 2012.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In International Conference on Learning Rep-
resentations (ICLR), 2017.
Appendix
7.1	Proof of Theorem 4.1.
Without loss of generality, suppose that ri > 0 for i = 1, . . . , n. Let X be the multiset in which xi
occurs with multiplicity ri .
We first show that Qin=1 (ri + 1) neurons are sufficient to approximate p(x). Appendix A in Lin
et al. (2017) demonstrates that for variables yι,..., yN, the product yι.yN can be Taylor-
approximated as a linear combination of the 2N functions σ(±yι 士…士 yd).
Consider setting y1, . . . , yd equal to the elements of multiset X. Then, we conclude that we can
approximate P(X) as a linear combination of the functions σ(±yι ± ∙∙∙ ± yd). However, these
functions are not all distinct: there are ri + 1 distinct ways to assign ± signs to ri copies of xi
(ignoring permutations of the signs). Therefore, there are Qin=1(ri + 1) distinct functions σ(±y1 ±
10
Published as a conference paper at ICLR 2018
∙∙∙± yN), proving that mTaylor(p) ≤ Qn=ι(ri + 1). Proposition 3.3 implies that for homogeneous
polynomials p, we have mu1niform (p) ≤ m1Taylor(p).
We now show that this number of neurons is also necessary for approximating p(x). Suppose that
N(x) is an -approximation to p(x) with depth 1, and let the Taylor series of N (x) bep(x)+E(x).
Let Ek (x) be the degree-k homogeneous component of E(x), for 0 ≤ k ≤ 2d. By the definition of
-approximation, supx E(x) goes to 0 as does, so by picking small enough, we can ensure that
the coefficients of each Ek (x) go to 0.
Let m = mu1niform(p) and suppose that σ(x) has the Taylor expansion Pk∞=0 σkxk. Then, by group-
ing terms of each order, we conclude that there exist constants aij and wj such that
σd
σk
Xm	Xn
wj	aij xi
j=1	i=1
Xm	Xn
wj	aij xi
j=1	i=1
d
= p(x) + Ed(x)
k
= Ek (x) for k 6= d.
For each S ⊆ X , let us take the derivative of this equation by every variable that occurs in S, where
we take multiple derivatives of variables that occur multiple times. This gives
σd ∙ d!
TSr
σk ∙ k!
ISr
mn
wj	ahj	aijxi
j=1	h∈S	i=1
mn
wj	ahj	aijxi
j=1 h∈S	i=1
d-|S|
k-|S|
∂∂
∂sp(x)+ ∂sEd(x),
∂
∂SEk (x).
(7)
(8)
Observe that there are r ≡ Qin=1(ri + 1) choices for S, since each variable xi can be included
anywhere from 0 to ri times. Define A to be the r × m matrix with entries AS,j = Qh∈S ahj .
We claim that A has full row rank. This would show that the number of columns m is at least the
number of rows r = Qin=1(ri + 1), proving the desired lower bound on m.
Suppose towards contradiction that the rows As',∙ admit a linear dependence:
r
Ec'AS',∙ = o,
'=1
where the coefficients e` are all nonzero and the s` denote distinct subsets of X. Let S* be such that
|c* | is maximized. Then, take the dot product of each side of the above equation by the vector with
entries (indexed by j) equal to Wj (Pn=I ajXi)d-|S*|:
rm	( n	、d-|S* |
0 = ∑C'∑Wj ∏ahj Eaijxi
'=1 j = 1	h∈S'	∖i=1	)
m	( n	∖ d-|S'|
= E c' £ Wj ∏ ahj IE aij χi I
'∣(∣S'∣ = ∣S*∣) j = 1 h∈S'	∖i=1	)
m
+ E CE Wjn ahj
'l(∣S'l=∣s*l)	j=ι	h∈S'
(d+ |s`| — |s*I)一|s` |
We can use (7) to simplify the first term and (8) (with k = d + ∣S'∣-∣S* |) to simplify the second
term, giving us:
0 = E	c' ∙
'l(IS'|=|S*I)
∣S'∣!
σd ∙ d!
p(x) +
(9)
∣ s' ∣!	d 口	(、
'I(∣sNs*J	σd+ls'l-ls*1	∙	(d +	∣s'∣-∣s*∣)！	ds'	d+ls'l-ls*1	x
11
Published as a conference paper at ICLR 2018
Consider the coefficient of the monomial 品P(X), which appears in the first summand with coeffi-
cient c* ∙ σS4. Since the S' are distinct, this monomial does not appear in any other term 瘾p(x),
but it could appear in some of the terms 悬Ek(x).
By definition, |c*| is the largest of the values |c'|, and by setting E small enough, all coefficients
of 篇Ek(X) can be made negligibly small for every k. This implies that the coefficient of the
monomial 会p(x) can be made arbitrarily close to c* ∙ σS⅛, which is nonzero since c* is nonzero.
However, the left-hand side of equation (9) tells us that this coefficient should be zero - a contradic-
tion. We conclude that A has full row rank, and therefore that mu1niform(p) = m ≥ Qin=1 (ri + 1).
This completes the proof of part (i).
We now consider part (ii) of the theorem. It follows from Proposition 4.6, part (ii) that, for each i,
we can Taylor-approximate xiri using 7dlog2(ri)e neurons arranged in a deep network. Therefore,
we can Taylor-approximate all of the xiri using a total of Pi 7dlog2 (ri)e neurons. From Lin et al.
(2017), we know that these n terms can be multiplied using 4n additional neurons, giving us a total
of Pi(7dlog2(ri)e +4). Proposition 3.3 implies again thatmu1niform(p) ≤ m1Taylor(p). This completes
the proof.
7.2	Proof of Theorem 4.2.
As above, suppose that ri > 0 for i = 1, . . . , n, and let X be the multiset in which xi occurs with
multiplicity ri .
It is shown in the proof of Theorem 4.1 that Qin=1 (ri + 1) neurons are sufficient to Taylor-
approximate p(x). We now show that this number of neurons is also necessary for approximating
p(X). Let m = m1Taylor(p) and suppose that σ(x) has the Taylor expansion Pk∞=0 σkxk. Then, by
grouping terms of each order, we conclude that there exist constants aij and wj such that
= p(X)
m
σd	wj
j=1
for 0 ≤ k ≤ N - 1.
(10)
(11)
For each S ⊆ X , let us take the derivative of equations (10) and (11) by every variable that occurs
in S, where we take multiple derivatives of variables that occur multiple times. This gives
m	n	d-|S|
审 X WjY ahj(X 5)	= ∂∂SP(X),
mn
σk ∙ k!	ʊ /
I S|! 7 wjj 11 ahj I	aijXi
j=1	h∈S	i=1
(12)
(13)
k-|S|
=0
for |S| ≤ k ≤ d - 1. Observe that there are r = Qin=1(ri + 1) choices for S, since each variable
xi can be included anywhere from 0 to ri times. Define A to be the r × m matrix with entries
AS,j = Qh∈S ahj . We claim that A has full row rank. This would show that the number of
columns m is at least the number of rows r = Qin=1(ri + 1), proving the desired lower bound on m.
Suppose towards contradiction that the rows As',∙ admit a linear dependence:
r
Ec'AS',∙ = o,
'=1
where the coefficients e` are nonzero and the S' denote distinct subsets of X. Set S = max' |S'|.
Then, take the dot product of each side of the above equation by the vector with entries (indexed by
12
Published as a conference paper at ICLR 2018
j) equal to wj (Pin=1 aijxi)d-s :
d-∖S' |
(d+lS'1-s)-lS'1
rm
0 = ΣSc'Σ Wj Π ahj
'=1 j=1	h∈S'
m
=E	c'E Wj ∏ ah
'∖(∖S'∖=s)	j = 1	h∈S'
m
+ E	c'∑Wjnahj
'∖(∖S' ∖<s)	j=1	h∈S'
We can use (12) to simplify the first term and (13) (with k = d + |S'| - S) to simplify the second
term, giving us:
0= E	c'∙
'∖(IS'∖=S)
|S'|!
σd ∙ d!
JdPg + X c'--------------------------lS^Iiq I——τy ∙ 0
dS'	'∖(∖⅛‰)	σd+∖S'∖-s ∙ (d + |S'|- S)!
E	c' ∙
'∖(IS'∖=S)
|S'|!
σd ∙ d!
焉' P(X).
Since the distinct monomials 品p(x) are linearly independent, this contradicts our assumption that
the c' are nonzero. We conclude that A has full row rank, and therefore that m1Taylor (P) = m ≥
Qin=1(ri + 1). This completes the proof of part (i).
Part (ii) of the theorem was demonstrated in the proof of Theorem 4.1. This completes the proof.
7.3	Proof of Theorem 4.3.
Our proof in Theorem 4.1 relied upon the fact that all nonzero partial derivatives of a monomial are
linearly independent. This fact is not true for general polynomials P; however, an exactly similar
argument shows that mu1niform(P) is at least the number of linearly independent partial derivatives of
P, taken with respect to multisets of the input variables.
Consider the monomial q of P such that mu1niform(q) is maximized, and suppose that q(x) =
x；1 x$2 .…Xnn. By TheOrem4.1, muniform(q) is equal to the number Q2ι(ri + 1) of distinct mono-
mials that can be obtained by taking partial derivatives of q. Let Q be the set of such monomials,
and let D be the set of (iterated) partial derivatives corresponding to them, so that for d ∈ D, we
have d(q) ∈ Q.
Consider the set of polynomials P = {d(P) | d ∈ D}. We claim that there exists a linearly
independent subset of P with size at least |D|/c. Suppose to the contrary that P0 is a maximal
linearly independent subset of P with |P0| < |D|/c.
Since P has c monomials, every element of P has at most c monomials. Therefore, the total number
of distinct monomials in elements of P0 is less than |D|. However, there are at least |D| distinct
monomials contained in elements ofP, since for d ∈ D, the polynomial d(P) contains the monomial
d(q), and by definition all d(q) are distinct as d varies. We conclude that there is some polynomial
P0 ∈ P\P0 containing a monomial that does not appear in any element ofP0. But then P0 is linearly
independent of P0, a contradiction since we assumed that P 0 was maximal.
We conclude that some linearly independent subset of P has size at least |D|/c, and therefore that
the space of partial derivatives ofP has rank at least |D|/c = mu1niform(q)/c. This proves part (i) of
the theorem. Part (ii) follows immediately from the definition of muniform(P).
Similar logic holds for mTaylor.
13
Published as a conference paper at ICLR 2018
7.4	Proof of Theorem 4.4.
We will prove the desired lower bounds for mu1niform(p); a very similar argument holds for m1Taylor(p).
As above, suppose that ri > 0 for i = 1, . . . , n. Let X be the multiset in which xi occurs with
multiplicity ri .
Suppose that N(x) is an -approximation to p(x) with depth 1, and let the degree-d Taylor poly-
nomial of N(x) be p(x) + E(x). Let Ed(x) be the degree-d homogeneous component of E(x).
Observe that the coefficients of the error polynomial Ed (x) can be made arbitrarily small by setting
sufficiently small.
Let m = mu1niform(p) and suppose that σ(x) has the Taylor expansion Pk∞=0 σkxk. Then, by group-
ing terms of each order, we conclude that there exist constants aij and wj such that
m
σd	wj
j=1
Xnd
aij xi	= p(x) + Ed(x)
For each S ⊆ X , let us take the derivative of this equation by every variable that occurs in S, where
we take multiple derivatives of variables that occur multiple times. This gives
m	n	d-|S|
σdSld! X Wj Y ahj (X 5)	= ∂∂Sp(x) + ∂∂SEd(x).
Consider this equation as S ⊆ X varies over all Cs multisets of fixed size s. The left-hand side rep-
resents a linear combination of the m terms (Pn=I ajxi)d-s. The polynomials ∂SP(X) + ∂SEd(x)
on the right-hand side must be linearly independent as S varies, since the distinct monomials ∂SP(X)
are linearly independent and the coefficients of ∂S Ed(X) Can be made arbitrarily small.
This means that the number m of linearly combined terms on the left-hand side must be at least the
number Cs of choices for S. Observe that Cs is the coefficient of the term ys in the polynomial
g(y) = Qi(1 + y + .∙∙ + yri). A simple (and not very good) lower bound for Cs is d Qnn=ι(ri + 1),
since there are Qin=1(ri + 1) distinct sub-multisets ofX, and their cardinalities range from 0 to d.
14