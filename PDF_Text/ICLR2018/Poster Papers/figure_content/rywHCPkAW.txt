Figure 1: Comparison of NoisyNet agent versus the baseline according to Eq. (19). The maximumscore is truncated at 250%.
Figure 2: CompariSon of the learning curveS of NoiSyNet agent verSuS the baSeline according to themedian human normaliSed Score.
Figure 3: Comparison of the learning curves of the average noise parameter Σ across five Atari gamesin NoisyNet-DQN. The results are averaged across 3 seeds and error bars (+/- standard deviation) areplotted.
Figure 4: Graphical representation of a noisy linear layer. The parameters μw, μb, σw and σb are thelearnables of the network whereas εw and εb are noise variables which can be chosen in factorisedor non-factorised fashion. The noisy layer functions similarly to the standard fully connected linearlayer. The main difference is that in the noisy layer both the weights vector and the bias is perturbedby some parametric zero-mean noise, that is, the noisy weights and the noisy bias can be expressed asW = μw + σw Θ εw and b = μb + σb Θ εb, respectively. The output of the noisy layer is then simplyobtained as y = wx + b.
Figure 5: Comparison of the learning curves of factorised and non-factorised NoisyNet-A3C versusthe baseline according to the median human normalised score.
Figure 6: Training curves for all Atari games comparing DQN and NoisyNet-DQN.
Figure 7: Training curves for all Atari games comparing Duelling and NoisyNet-Dueling.
Figure 8: Training curves for all Atari games comparing A3C and NoisyNet-A3C.
