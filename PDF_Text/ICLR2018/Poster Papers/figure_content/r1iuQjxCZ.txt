Figure 1: Memorizing networks are more sensitive to cumulative ablations. Networks weretrained on MNIST (2-hidden layer MLP, a), CIFAR-10 (11-layer convolutional network, b), andImageNet (50-layer ResNet, c). In a, all units in all layers were ablated, while in b and c, onlyfeature maps in the last three layers were ablated. Error bars represent standard deviation across 10random orderings of units to ablate.
Figure 2: Memorizing networks are more sensitive to random noise. Networks were trainedon MNIST (2-hidden layer MLP, a), and CIFAR-10 (11-layer convolutional network, b). Noisewas scaled by the empirical variance of each unit on the training set. Error bars represent standarddeviation across 10 runs. X-axis is on a log scale.
Figure 3: Networks which generalize poorly are more reliant on single directions. 200 net-works with identical topology were trained on unmodified CIFAR-10. a, Cumulative ablation curvesfor the best and worst 5 networks by generalization error. Error bars represent standard deviationacross 5 models and 10 random orderings of feature maps per model. b, Area under cumulativeablation curve (normalized) as a function of generalization error.
Figure 5: Impact of regularizers on networks’ reliance upon single directions. a, Cumulativeablation curves for MLPs trained on unmodified and fully corrupted MNIST with dropout fractions∈ {0.1, 0.2, 0.3}. Colored dashed lines indicate number of units ablated equivalent to the dropoutfraction used in training. Note that curves for networks trained on corrupted MNIST begin to dropsoon past the dropout fraction with which they were trained. b, Cumulative ablation curves fornetworks trained on CIFAR-10 with and without batch normalization. Error bars represent standarddeviation across 4 model instances and 10 random orderings of feature maps per model.
Figure 4: Single direction reliance as a signal for hyperparameter selection and early stopping.
Figure 6: Batch normalization decreases class selectivity and increases mutual information.
Figure 7: Selective and non-selective directions are similarly important. Impact of ablationas a function of class selectivity for MNIST MLP (a), CIFAR-10 convolutional network (b-c), andImageNet ResNet (d-e). c and e show regression lines for each layer separately.
Figure A1: Ablation to zero vs. ablation to the empirical feature map mean.
Figure A2: Class selectivity increases with depth. Class selectivity distributions as a function ofdepth for CIFAR-10 (a) and ImageNet (b).
Figure A3: Class selectivity is uncorrelated with L1-norm. Relationship between class selectiv-ity and the L1 -norm of the filter weights for CIFAR-10 (a) and ImageNet (b).
Figure A4: Mutual information is not a good predictor of unit importance. Impact of ablationas a function of mutual information for MNIST MLP (a), CIFAR-10 convolutional network (b-c),and ImageNet ResNet (d-e). c and e show regression lines for each layer separately.
