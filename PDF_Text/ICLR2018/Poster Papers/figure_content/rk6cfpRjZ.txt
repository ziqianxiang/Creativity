Figure 1: SPeedUPs of matrix multiplication using non-strUctured and structured sparsity. Speedsare measured in Intel MKL implementations in Intel Xeon CPU E5-2673 v3 @ 2.40GHz. Generalmatrix-matrix multiplication (GEMM) of W ∙ X is implemented by cblas_sgemm. The matrixsizes are selected to reflect commonly used GEMMs in LSTMs. For example, (a) represents GEMMin LSTMs with hidden size 1500, input size 1500 and batch size 10. To accelerate GEMM by spar-sity, W is sparsified. In non-structured sparsity approach, W is randomly sparsified and encoded asCompressed Sparse Row format for sparse computation (using mkl_scsrmm); in structured spar-sity approach, 2k columns and 4k rows in W are removed to match the same level of sparsity (i.e.,the percentage of removed parameters) for faster GEMM under smaller sizes.
Figure 2:	Intrinsic Sparse Structures (ISS) in LSTM units.
Figure 3:	Applying Intrinsic Sparse Structures in weight matrices.
Figure 4: Intrinsic Sparse Structures learned by group Lasso regularization (zoom in for betterview). Original weight matrices are plotted, where blue dots are nonzero weights and white onesrefer zeros. For better visualization, original matrices are evenly down-sampled by 10 × 10.
Figure 5: Intrinsic Sparse Structures unveiled by '1 regularization (zoom in for a better view). Thetop row shows the original weight matrices, where blue dots are nonzero weights and white onesrefer zeros; the bottom row are the weight matrices in the format of Fig. 3, where white strips are ISScomponents whose weights are all zeros. For better visualization, the original matrices are evenlydown-sampled by 10 × 10.
Figure 6: Histogram of vector lengths of “ISS weight groups” in BiDAF. The ISS-Iearned BiDAFis the one in the third row of Table 4 with EM 66.32 and F1 76.22. Using our approach, the lengthsare regularized closer to zeros with a peak at the zero, resulting in high ISS sparsity.
