Figure 1: Schedules for the learning rate (a) and batch size (b), as a function of training epochs.
Figure 2: Wide ResNet on CIFAR10. Training set cross-entropy, evaluated as a function of thenumber of training epochs (a), or the number of parameter updates (b). The three learning curvesare identical, but increasing the batch size reduces the number of parameter updates required.
Figure 3: Wide ResNet on CIFAR10. Test accuracy during training, for SGD with momentum (a),and Nesterov momentum (b). In both cases, all three schedules track each other extremely closely.
Figure 4: Wide ResNet on CIFAR10. The test set accuracy during training, for vanilla SGD (a) andAdam (b). Once again, all three schedules result in equivalent test set performance.
Figure 5: Wide ResNet on CIFAR10. Test accuracy as a function of the number of parameterupdates. “Increasing batch size” replaces learning rate decay by batch size increases. “Increasedinitial learning rate” additionally increases the initial learning rate from 0.1 to 0.5. Finally “Increasedmomentum coefficient” also increases the momentum coefficient from 0.9 to 0.98.
Figure 6: Inception-ResNet-V2 on ImageNet. Increasing the batch size during training achievessimilar results to decaying the learning rate, but it reduces the number of parameter updates fromjust over 14000 to below 6000. We run each experiment twice to illustrate the variance.
Figure 7: Inception-ResNet-V2 on ImageNet. Increasing the momentum parameter reduces thenumber of parameter updates required, but it also leads to a small drop in final test accuracy.
Figure 8: Wide ResNet on CIFAR10. We can only increase the initial learning rate to 〜0.4 beforethe final test accuracy starts to fall. Each point provided represents the median of five runs.
