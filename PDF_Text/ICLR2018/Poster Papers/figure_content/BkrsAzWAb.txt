Figure 1: Regular and hypergradient algorithms. Left-hand side: SGD with Nesterov (SGDN)(Algorithm 2) and Adam (Algorithm 3) are obtained by substituting the corresponding initialization(red) and update (blue) statements into regular SGD (Algorithm 1). Right-hand side: Hypergradientvariants of SGD with Nesterov (SGDN-HD) (Algorithm 5) and Adam (Adam-HD) (Algorithm 6) areobtained by substituting the corresponding statements into hypergradient SGD (SGD-HD) (Algo-rithm 4).
Figure 2: Online tuning of the learning rate for logistic regression and multi-layer neural network.
Figure 3: Behavior of hypergradient variants compared with their regular counterparts. Columns:left: logistic regression on MNIST; middle: multi-layer neural network on MNIST; right: VGG Neton CIFAR-10. Rows: top: evolution of the learning rate αt ; middle: training loss; bottom: validationloss. Main plots show epoch averages and inset plots highlight the behavior of the algorithms duringinitial iterations. For MNIST one epoch is one full pass through the entire training set of 60,000images (468.75 iterations with a minibatch size of 128) and for CIFAR-10 one epoch is one full passthrough the entire training set of 50,000 images (390.625 iterations with a minibatch size of 128).
Figure 4: Grid search for selecting α0 and β, looking at iterations to convergence to a training loss of0.29 for logistic regression. Everywhere to the left and below the shaded region marked by the redboundary, hypergradient variants (bottom) perform better than or equal to the baseline variants (top).
