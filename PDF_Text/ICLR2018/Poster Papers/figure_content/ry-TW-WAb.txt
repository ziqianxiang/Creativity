Figure 1: Distribution of weights (means θ and log-variance log σ2) before and after VNQ trainingof LeNet-5 on MNIST (validation accuracy before: 99.2% vs. after 195 epochs: 99.3%). Top row:scatter plot of weights (blue dots) per layer. Means were initialized from pre-trained deterministicnetwork, variances with log σ2 = -8. Bottom row: corresponding density1. Red shaded areasshow the funnel-shaped “basins of attraction” induced by the quantizing prior. Positive and negativetarget values for ternary quantization have been learned per layer. After training, weights with smallexpected absolute value or large variance (log αij ≥ log Tα = 2 corresponding to the funnel markedby the red dotted line) are pruned and remaining weights are quantized without loss in accuracy.
Figure 2: Approximation to the analytically intractable KL divergence DκL(qφ∣∣p), constructed byshifting and mixing known approximations to the KL divergence from a log uniform prior to theposterior. Top row: Shifted versions of the known approximation (Eq. (7)) in color and the groundtruth KL approximation (computed via Monte Carlo sampling) DMC(qφ∣∣p) in black. Middle row:weighting functions Ω(θ) that mix the shifted known approximation to form the final approxima-tion FKL shown in the bottom row (gold), compared against the ground-truth (MC sampled). Eachcolumn corresponds to a different value of σ. A comparison between ground-truth and our approx-imation over a large range of σ and θ values is shown in the Appendix in Fig. 4. Note that sincethe priors are improper, KL approximation and ground-truth can only be compared up to an additiveconstant C - the constant is irrelevant for network training but has been chosen in the plot such thatground-truth and approximation align for large values of θ.
Figure 3: Visualization of distribution over DenseNet weights after training on CIFAR-10 withVNQ. Each panel shows one (convolutional or dense) layer, starting in the top-left corner with theinput- and ending with the final layer in the bottom-right panel (going row-wise, that is first movingto the right as layers increase). The validation accuracy of the network shown is 91.68% beforepruning and quantization and 91.17% after pruning and quantization.
Figure 4: Quantitative analysis of the KL approxmiation quality. The top panel shows the “ground-truth” (computed via computationally expensive Monte Carlo approximation), the middle panelshows our approxiomation (Eq. (16)) and the bottom panel shows the difference between both. Themaximum absolute error between our approximation and the ground-truth is 1.07 nats.
Figure 5: Distribution of weights after training without local reparameterization but with functionalKL approximation (a) and after training with naive MC approximation (b). Top rows: scatter plot ofweights (blue dots) per layer. Bottom row: corresponding density.
Figure 6: Visualization of distribution over DenseNet weights after training on CIFAR-10 with naiveMC approximation for the KL divergence (and without local reparameterization). Each panel showsone layer, starting in the top-left corner with the input- and ending with the final layer in the bottom-right panel (going row-wise, that is first moving to the right as layers increase). Validation accuracybefore pruning and quantization is 79.25% but plunges to 22.29% after pruning and quantization.
