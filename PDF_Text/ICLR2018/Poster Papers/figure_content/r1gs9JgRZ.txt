Figure 1: Mixed precision training iteration for a layer.
Figure 2: Figure 2a shows the results of three experiemnts; baseline (FP32), pseudo FP16 withFP32 master copy, pseudo FP16 without FP32 master copy. Figure 2b shows the histogram for theexponents of weight gradients for Mandarin speech recognition training with FP32 weights. Thegradients are sampled every 4,000 iterations during training for all the layers in the model.
Figure 3: Histogram of activation gradient values during the training of Multibox SSD network.
Figure 4: English to French translation network training perplexity, 3x1024 LSTM model withattention. Ref1, ref2 and ref3 represent three different FP32 training runs.
Figure 5: bigLSTM training perplexityFigure 6: An uncurated set of face images generated by DCGAN. FP32 training (left) and mixed-precision training (right).
Figure 6: An uncurated set of face images generated by DCGAN. FP32 training (left) and mixed-precision training (right).
