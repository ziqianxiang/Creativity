Figure 1: Different crossover strategies for neural network policies. State-visitation distribution plotnext to each policy depicts the slice of state-space where that policy gives high returns. In a naiveapproach like parameter-space crossover (shown in bottom-right), edge weights are copied from theparent network to create the offspring. Our proposed state-space crossover operator, instead, aimsto achieve the behavior shown in bottom-left.
Figure 2: Schema for combining parentpolicies to produce an offspring policy.
Figure 3:	Comparison of Crossover operators.
Figure 4:	Performance of GPO and baselines on MuJoCo environments using PPO.
Figure 5: Ablation studies on two environments â€” Ant (left) and Walker-2D (right). Averagedover the two environments, the performance normalized to GPO in increasing order is Single (0.33),Base+C (0.47), Base+S (0.47), Base+C+S (0.5), Base+M (0.83), Base+M+S (0.86), Base+M+C(0.9), and GPO (1.0).
Figure 6: Final performance of the policy en-semble trained with GPO and Single onthe HalfCheetah environment.
Figure 7: Scaling by increasing the GPOpopulation size in Walker2d environment.
Figure 8:	Performance of GPO and baselines on MuJoCo environments using A2C.
Figure 9:	Performance of GPO and Joint baseline on MuJoCo environments from OpenAI Gym.
Figure 10: Effect of GPO population size when using same number of timesteps.
