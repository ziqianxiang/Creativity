Figure 1: The KL divergence from the inferred Weibull distribution to the target gamma one as (a)Gamma(0.05, 1), (b) Gamma(0.5, 1), and (c) Gamma(5, 1). Subplot (d) shows the KL divergenceas a function of the gamma shape parameter, where the gamma scale parameter is fixed at 1.
Figure 2: (a-b): Inference (or encoder/recognition) and generative (or decoder) models for (a) WHAIand (b) AVITM; (c) the generative model and a sketch of the upward-downward Gibbs sampler ofDLDA, where Zl are augmented latent counts that are upward sampled in each Gibbs samplingiteration. Circles are stochastic variables and squares are deterministic variables. The orange andblue arrows denote the upward and downward information propagation respectively, and the redones denote the data generation.
Figure 3: Plot of per-heldout-word perplexity as a function of time for (a) 20News, (b) RCV1, and(c) Wiki. Except for AVITM that has a single hidden layer with 128 topics, all the other algorithmshave the same network size of 128-64-32 for their deep generative models.
Figure 4: An example of hierarchical topics learned from Wiki by a three-hidden-layer WHAI ofsize 128-64-32.
Figure 5: Learned topics on MNIST digits with a three-hidden-layer WHAI of size 128-64-32. Shown in (a)-(c) are example topics for layers 1, 2 and 3, respectively, learned with adeterministic-upward-stochastic-downward encoder, and shown in (d)-(f) are the ones learned witha deterministic-upward encoder.
Figure 6: Latent space interpolations on the MNIST test set. Left and right columns correspond tothe images generated from z(13) and z(23), and the others are generated from the latent representationsinterpolated linearly from z(13) to z(23) .
Figure 7: An example of hierarchical topics learned from Wiki by a three-hidden-layer WHAI ofsize 128-64-32.
Figure 8: An example of hierarchical topics learned from Wiki by a four-hidden-layer WHAI of size256-128-64-32.
