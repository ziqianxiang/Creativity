Figure 1: Fitting Gaussian Mixture distributionimate a 1-D Gaussian mixture distribution withVI. The Gaussian mixture distribution has twoequally distributed unit-variance componentswhose means are -3 and 3. We compare KIVI with VI using a single Gaussian posterior (Fig. 1). Thevariational distribution used by KIVI generates samples by propagating a standard normal distributionthrough a two-layer MLP with 10 hidden units in each layer and one output unit. As shown, theGaussian posterior converges to a single mode. In contrast, KIVI can accurately approximate the two6Published as a conference paper at ICLR 2018Table 1: Average test set RMSE, predictive log-likelihood for the regression datasets.
Figure 2: Results for MNIST classification. The left table shows the test error rates. ? indicatesresults that are not directly comparable to ours: Wan et al. (2013) used an ensemble of 5 networks,and the second part of Blundell et al. (2015) changed the prior to a scale mixture. The plot on theright shows training lower bound in MNIST classification with prior-contrastive and KIVI.
Figure 3: Variational Autoencoders: (a) Gaussian posterior vs. implicit posterior, where fc denotesa fully-connected layer. They are used by the plain VAE and KIVI, respectively; (b) Training andevaluation curves of the lower bounds on statically binarized MNIST.
Figure 5: 2-D Bayesian logistic regressionWe also conduct experiments on a 2-D Bayesian logistic regression example, which has an intractableposterior. The model isW 〜N (0, I),	yi 〜Bernoulli。(w> Xi)),	i = 1,..., N,where w, xi ∈ R2; σ is the sigmoid function. N = 200 data points ({(xi, yi)}iN=1) are generatedfrom the true model as the training data (Fig. 5a). The unnormalized true posterior is plotted inFig. 5b. As a baseline, we first run VI with a factorized normal distribution. The result is shown inFig. 5c. It can be clearly seen that the factorized normal can capture the position and the scale of thetrue posterior but cannot fit well to the shape due to its independence across dimensions.
Figure 6: Variational distributions produced by only optimizing KL(q(w)kp(w)): (a) With thereverse ratio trick; (b) Without the reverse ratio trick.
Figure 7: Visualization of learned posteriors for regression on Boston housing.
Figure 8: True KL term vs. estimated KL term for posteriors with normalizing flows.
Figure 9: Epoch 1(b) KIVI(a) AVBFigure 10: Epoch 5(b) KIVIG Details of ExperimentsG.1 Toy Experiments1-D Gaussian Mixture The implicit posterior generates samples by propagating samples from astandard normal distribution through a two-layer MLP with 10 hidden units and one output unit. Weset the regularization coefficient λ to 0.003 and the density ratio clipping threshold to 10-8.
Figure 10: Epoch 5(b) KIVIG Details of ExperimentsG.1 Toy Experiments1-D Gaussian Mixture The implicit posterior generates samples by propagating samples from astandard normal distribution through a two-layer MLP with 10 hidden units and one output unit. Weset the regularization coefficient λ to 0.003 and the density ratio clipping threshold to 10-8.
Figure 13: Epoch 20implicit variational posterior used in KIVI. First, some 2-D random normal samples are propagatedthrough two fully-connected layers of size 20, producing h (we do not use activation functions forthe second layer), and then h is added with another random normal noise with trainable variances,producing z. Finally, we propagate z through a fully-connected layer of size 20 and then a linearoutput layer, getting the variational samples.
Figure 14: Epoch 25(b) KIVIG.2 Bayesian Neural NetworksG.2.1 RegressionAs the regression datasets have small feature dimensions (all less than 15, except 90 for Year), usingBNNs of one hidden layer (50 units) does not produce very high-dimensional weights. Therefore, westill use MLPs in the implicit variational posterior, of which the samples are generated by propagatingsamples from a standard normal distribution through an MLP. For all datasets, we use ReLU as theactivation function. The MLP has one hidden layer except that for Yacht it has two hidden layers.
