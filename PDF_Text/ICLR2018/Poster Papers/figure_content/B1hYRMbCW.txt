Figure 1:	A one (left) and a two (right) dimensional example showing that f * (O)-f * (X)=|O-X| onlyholds for coupled pairs (X,O)〜∏*.
Figure 2:	Non-differentiable optimal critic functions f* (shown in blue). Left: For two discrete dis-tributions: Circles and crosses belong to samples from the empirical distribution and the generativemodel, respectively. An approximating differentiable function is shown in green. Right: For twocontinuous distributions: The empirical distribution μ is shown in gray, the generative distribution Vis shown in green.
Figure 3: Level sets of the critic f of WGANs during training, after 10, 50, 100, 500, and 1000iterations. Yellow corresponds to high, purple to low values of f . Training samples are indicated inred, generated samples in blue. Top: GP-penalty with λ = 10. Middle: GP-penalty with λ = 1.
Figure 4: Evolution of the negative of WGAN critic’s loss (without the regularization term) forλ = 5. Median results over the 20 runs (blue area indicates quantiles, green dots outliers). Left: Forthe GP-penalty. Right: For the LP-penalty.
Figure 5: Evolution of the approximated Wasserstein-1 distance during training of WGANs (λ = 5,median results over 10 runs). Left: For the GP-penalty. Right: For the LP-penalty.
Figure 6: Comparison of the magnitude of the gradient penalty during training on CIFAR, showing< 1 and > 1 contributions (i.e. min(0, ||Vf (x)|| — 1)2 resp. max(0, ||Vf (x)|| - 1)2). Left: forregularization parameter λ = 5. Right: for regularization parameter λ = 100. The (one-sided)gradient penalty of WGAN-LP is depicted in blue (solid), the gradient penalty of WGAN-GP inred (dashed). All the values for every iteration (one mini-batch) are shown in light blue and red.
Figure 7: Values of the WGAN critic function for some generated data points can be higher than thecritic’s values for some real data points. Thus, fake and real points can not be distinguished basedon the critics values alone. Real data points are represented by O, generated by X.
Figure 8: Level sets of the critic (yellow corresponds to high, purple to low values) of WGANsduring training (after 10, 50, 100, 500, and 1000 iterations) on the 8Gaussian data set. Top: GP-penalty (λ = 10). Middle: GP-penalty (λ = 1). Bottom: LP-penalty (λ = 10).
Figure 9: Level sets of the critic (yellow corresponds to high, purple to low values) of WGANsduring training (after 10, 50, 100, 500, and 1000 iterations) on the 25Gaussian data set. Top: GP-penalty (λ = 10). Middle: GP-penalty (λ = 1). Bottom: LP-penalty (λ = 10).
Figure 10: Evolution of the WGAN-GP critics loss without the regularization term (λ = 1). Left:Median results over the 20 runs (blue area indicates quantiles, green dots outliers). Right: Singleruns.
Figure 11: Evolution of the approximated EM distance during training WGAN-GPs with λ = 1.
Figure 12: Evolution of the WGAN critic’s negative loss with local sampling (without the regular-ization term). Left: Median results over the 20 runs. Right: Single runs. Top: GP-penalty whengenerating samples by perturbing training samples only. Middle: For GP-penalty, perturbing train-ing and generated samples. Bottom: LP-penalty, perturbing training and generated samples (verysimilar to perturbing only training samples)Figure 13: Evolution of the approximated EM distance during training of WGANs with local pertur-bation (λ = 5). Left: Median results over the 10 runs. Right: Single runs. Top: For the GP-penalty.
Figure 13: Evolution of the approximated EM distance during training of WGANs with local pertur-bation (λ = 5). Left: Median results over the 10 runs. Right: Single runs. Top: For the GP-penalty.
Figure 14: Evolution of the WGAN critics loss (Left) and the approximated EM distance (Right) fora WGAN-LP trained to minimize the Wasserstein-2 distance (λ = 10). Shown are the medians over5 runs.
Figure 15: Evolution of Inception score on CIFAR for WGAN-LP in blue (solid) and WGAN-GP inred (dotted). Left: for regularization parameter λ = 5. Right: for regularization parameter λ = 100.
Figure 16: Evolution of validation loss on CIFAR. Black/purple curves indicate the total loss, bluecurves the loss without regularization term, and red the regularization term only. Light coloredcurves indicate the true values, dark solid lines the average over a window of 5 iterations. Left:WGAN-GP. Right: WGAN-LP. Top: with λ = 5. Bottom: with λ = 100.
Figure 17: Level sets of the critic f of WGANs during training, after 10, 50, 100, 500, and 1000iterations. Yellow corresponds to high, purple to low values of f . Training samples are indicated inred, generated samples in blue. Top: With the regularization term given in Equation (7) and λ = 10.
Figure 18: Inception scores for regularization Equation (7) for penalty weights 100 (red) and 5(blue), shown on the left, and Inception scores for training with the regularization Equation (9) forpenalty weights 100 (red) and 5 (blue), shown on the right.
