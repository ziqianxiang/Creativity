Figure 1: Ablation study: Train (left) and validation (right) perplexity on PTB word level modelingwith single layer LSTM (10M parameters). These curves study the learning dynamics of the baselinemodel, Π-model, Expectation-linear dropout (ELD), Expectation-linear dropout with modification(ELDM) and fraternal dropout (FD, our algorithm). We find that FD converges faster than theregularizers in comparison, and generalizes at par.
Figure 2: Ablation study: Validation perplexityon PTB word level modeling for Π-model andfraternal dropout. We find that FD convergesfaster and generalizes at par.
Figure 3: Ablation study: Average hidden stateactivation is reduced when any of the regular-izer described is used. The y-axis is the value of1 km ∙ htk2.
Figure 4: Ablation study: Train (left) and validation (right) perplexity on PTB word level model-ing with single layer LSTM (10M parameters). These curves study the learning dynamics of thebaseline model, temporal activity regularization (TAR), prediction regularization (PR), activity reg-ularization (AR) and fraternal dropout (FD, our algorithm). We find that FD both converges fasterand generalizes better than the regularizers in comparison.
