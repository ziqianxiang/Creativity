Figure 1: The forward and the backward networks predict the sequence s = {x1 , ..., x4} indepen-dently. The penalty matches the forward (or a parametric function of the forward) and the backwardhidden states. The forward network receives the gradient signal from the log-likelihood objectiveas well as Lt between states that predict the same token. The backward network is trained only bymaximizing the data log-likelihood. During the evaluation part of the network colored with orangeis discarded. The cost Lt is either a Euclidean distance or a learned metric ||g(htf) - htb||2 with anaffine transformation g. Best viewed in color.
Figure 2: Analysis for speech recognition experiments. (a): Training curves comparison for Twin-Nets and the baseline network. Dotted vertical lines denote stages of pre-training, training, and twostages of annealing. The L2 cost is plotted alongside. The TwinNet converges to a better solutionas well as provides better generalization. (b): Comparison of histograms of the cost for rare words(first 1500) versus frequent words (all other). The cost is averaged over characters of a word. Thedistribution of rare words is wider and tends to produce higher L2 cost. (c): L2 loss vs. averagecross-entropy loss.
Figure 3: Example of the L2 loss plotted along the time axis. Notice that spikes correspond to rarewords given the acoustic information where the entropy of the prediction is high. Dotted verticallines are plotted at word boundary positions.
