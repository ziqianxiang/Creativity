Figure 1: (Left) Taxonomy of adversarial attack methods. The Boundary Attack is applicable to real-world ML algorithms because it only needs access to the final decision of a model (e.g. class-labelor transcribed sentence) and does not rely on model information like the gradient or the confidencescores. (Right) Application to the Clarifai Brand Recognition Model.
Figure 2: (Left) In essence the Boundary Attack performs rejection sampling along the boundarybetween adversarial and non-adversarial images. (Center) In each step we draw a new randomdirection by (#1) drawing from an iid Gaussian and projecting on a sphere, and by (#2) makinga small move towards the target image. (Right) The two step-sizes (orthogonal and towards theoriginal input) are dynamically adjusted according to the local geometry of the boundary.
Figure 3: Adversarial examples generated by the Boundary Attack for an MNIST, CIFAR and Im-ageNet network. For MNIST, the difference shows positive (blue) and negative (red) changes. ForCIFAR and ImageNet, we take the norm across color channels. All differences have been scaled upfor improved visibility.
Figure 4: Example of an untargeted attack. Here the goal is to synthesize an image that is as closeas possible (in L2-metric) to the original image while being misclassified (the original image iscorrectly classified). For each image we report the total number of model calls (predictions) untilthat point (above the image) and the mean squared error between the adversarial and the original(below the image).
Figure 6: Distance between adversar-ial and original image over number ofmodel calls for 12 different images(until convergence). Very few stepsare already sufficient to get almost im-perceptible perturbations.
Figure 5: Adversarial perturbation (difference betweenthe adversarial and the original image) for ten repetitionsof the Boundary Attack on the same image. There arebasically two different minima with similar distance (firstrow and second row) to which the Boundary Attack con-verges.
Figure 7: Example of a targeted attack. Here the goal is to synthesize an image that is as close aspossible (in L2-metric) to a given image of a tiger cat (2nd row, right) but is classified as a dalmatiandog. For each image we report the total number of model calls (predictions) until that point.
Figure 8: Adversarial examples generated by the Boundary Attack for two black-box models byClarifai for brand-detection (left side) and celebrity detection (right side).
