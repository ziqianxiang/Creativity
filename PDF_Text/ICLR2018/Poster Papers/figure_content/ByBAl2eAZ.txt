Figure 1: Median DQN returns for several ALE environment plotted over training steps.
Figure 2: Median DDPG returns for continuous control environments plotted over epochs.
Figure 3: Median TRPO returns for continuous control environments plotted over epochs.
Figure 4: Median number of episodes before considered solved for DQN with different explorationstrategies. Green indicates that the problem was solved whereas blue indicates that no solution wasfound within 2 K episodes. Note that less number of episodes before solved is better.
Figure 5: Median DDPG returns for environments with sparse rewards plotted over epochs.
Figure 6: Simple and scalable environment to test for exploratory behavior (Osband et al., 2016a).
Figure 7: Median DQN returns for all ALE environment plotted over training steps.
Figure 8: Median DDPG returns for all evaluated environments with dense rewards plotted overepochs.
Figure 9: Median TRPO returns with three different environments with sparse rewards plotted overepochs.
