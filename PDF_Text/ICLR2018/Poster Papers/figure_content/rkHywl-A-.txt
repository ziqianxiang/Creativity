Figure 1: Ground truth (a) and learned rewards (b, c) onthe random MDP task. Dark blue corresponds to a rewardof 1, and white corresponds to 0. Note that AIRL with astate-only reward recovers the ground truth, whereas thestate-action reward is shaped.
Figure 2: Learning curve for thetransfer learning experiment on tabularMDPs. Value iteration steps are plot-ted on the x-axis, against returns for thepolicy on the y-axis.
Figure 3: Illustration of the shifting mazetask, where the agent (blue) must reach the goal(green). During training the agent must goaround the wall on the left side, but during testtime it must go around on the right.
Figure 4: Reward learned on the point massshifting maze task. The goal is located at thegreen star and the agent starts at the white circle.
Figure 5: Top row: An ant running forwards (right in the picture) in the training environment.
