Figure 1: Architecture for the MbPA model. Left: Training usage. The parametric network is useddirectly and experiences are stored in the memory. Right: Testing setting. The embedding is usedto query the episodic memory, the retrieved context is used to adapt the parameters of the outputnetwork.
Figure 2: Illustrative diagram of the local fitting on a regression task. Given a query (blue), weretrieve the context from memory showed in red.
Figure 3: (Left) Results on Permuted MNIST comparing baselines with MbPA using different mem-ory sizes. (Right) Results augmenting MbPA with EWC, showing the flexibility and complementar-ity of MbPA.
Figure 4: The figure compares the performance of MbPA (blue) against two baselines: the paramet-ric model (green) and the mixture of experts (red). (Left) Aggregated performance (Right) disentan-gled performance evaluated on new (dashed) and old (solid) classes.
Figure 5: (Left) MbPA outperformed both parametric and memory-based mixture baselines, in thepresence of unbalanced data on previously unseen classes (dashed lines). (Right) Example of MbPA.
Figure 6: Left: Performance of MbPA when varying the dictionary size. Right: Performance ofthe parametric, mixture and MbPA models varying the learning rate of the parametric model. Thecolour code is the same as in Figure 4 and the thickness of the lines indicate the learning rate used.
Figure 7: Performance of MbPA when varying the number of nearest neighours used for performingthe local adaptation.
Figure 8: Percent improvement when MbPA is included with the LSTM baseline and neural cache,split by training word frequency into five equally sized buckets. The bucket 1 contains the mostfrequent words, and bucket 5 contains the least frequent words. The average improvement Â±1standard deviation are shown. MbPA provides a directional improvement for less frequent words.
