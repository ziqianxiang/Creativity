Figure 2: Training procedure for metamodelskennen-o (top) and kennen-i (bottom).
Figure 3: Inputs designed to extract internal de-tails from MNIST digit classifiers. E.g. feedingthe middle image reveals the existence of a max-pooling layer With 94.8% chance.
Figure 4: kennen-o performance of against the size of meta-training set (left), number of queries(middle), and quality of queries (right). Unless stated otherwise, We use 100 probability outputs and5k models to train kennen-o. Each curve is linearly scaled such that random chance (0 trainingdata, 0 query, or top-0) performs 0%, and the perfect predictor performs 100%.
Figure 5: Performance of kennen-io with different number of queries (Left) and size of trainingset (Right). The curves are linearly scaled per attribute such that random chance performs 0%, andperfect predictor performs 100%.
Figure 6:	kennen-o/io performance at different number of queries. kennen-o is shown with100 independent query samples per level (black dots) - the dots are spread horizontally for visuali-sation purpose. Their mean (curve) and ±2 standard deviations (error bars) are also shown.
Figure 7:	AIP for an ImageNet classifier. The perturbations are generated at L2 = 1 × 10-4.
Figure 8:	Adversarial perturbations for the same input image (top) generated with diverse ImageNetclassifier families (S, V, B, R, D, SVBRD) at different norm constraints. The perturbation imagesare normalised at the maximal perturbation for visualisation. We observe diverse patterns acrossclassifier families within the same L2 ball.
Figure 9:	Probability query output embedded into 2-D plane via t-SNE. The same embeddingis shown with different colour-coding for each attribute. These are the inputs to the kennen-ometamodel.
Figure 10:	Probability query output embedded into 2-D plane via t-SNE. The same embeddingis shown with different colour-coding for each attribute. These are the inputs to the kennen-iometamodel.
Figure 11: Confusion matrices for kennen-o.
Figure 12: Confusion matrices for kennen-io.
