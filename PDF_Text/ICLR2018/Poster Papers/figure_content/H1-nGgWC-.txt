Figure 1: In this paper we consider fully connected feedforward networks with more than onehidden layer. We call the pre-nonlinearity an activation and post-nonlinearity an activity. As thenetwork becomes increasingly wide the distribution of the marginal distributions of the activationsat each layer and of the output will become close to a Gaussian process in a sense described in thetext.
Figure 2: A comparison of finite random neural networks to their corresponding Gaussian processanalogue using an (RBF) kernel estimator of the squared maximum mean discrepancy (MMD). Theresults are consistent with the emergence of Gaussian process behaviour as the networks becomewide. The red dashed line is for calibration and denotes the squared MMD between two Gaussianprocesses with isotropic RBF kernels and length scales l and 2l where l = √8 is the characteristiclength scale of the input space (see text).
Figure 3: A comparison between Bayesian posterior inference in a Bayesian deep neural networkand posterior inference in the analogous Gaussian process. The neural network has 3 hidden layersand 50 units per layer. The lines show the posterior mean and two σ credible intervals.
Figure 4: A comparison between posterior inference for a Gaussian process and a Bayesian deepnetwork for a real value embedding of the XOR function. Left and centre: The two posterior means.
Figure 5: A comparison of the predictive distributions of a Bayesian deep network and a Gaussianprocess on a randomly generated test case. Left: the per point log-densities of the two models.
