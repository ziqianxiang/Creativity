Figure 1: Program Aliasing is one difficulty of program synthesis: For the input-output specificationgiven in (1a), both programs are semantically correct. However, supervised training would penalizethe prediction of Program B, if A is the ground truth.
Figure 2: Architecture of our model. Each pair of Input-Output is embedded jointly by a CNN. Onedecoder LSTM is run for each example, getting fed in a concatenation of the previous token and theIO pair embedding (constant across timestep). Results of all the decoders are maxpooled and theprediction is modulated by the mask generated by the syntax model. The probability over the nexttoken is then obtained by a Softmax transformation.
Figure 3: Approximation using a beamsearch. All possibles next tokens are tried for each candidates,the S (here 3) most likely according to PÎ¸ are kept. When an End-Of-Sequence token (green) isreached, the candidate is held out. At the end, the most likely complete sequences are used toconstruct an approximate distribution, through rescaling.
Figure 3: Syntax Comparisongrams for large training datasets. Our second contribution incorporates syntax checking as anadditional conditioning mechanism for pruning the space of programs during decoding. We showthat incorporating syntax leads to significant improvements with limited training datasets.
Figure 4: The Domain-specific language for Karel programs.
