Figure 1: A block diagram of our S+U learning algorithm. The upper part of the diagram illus-trates our training algorithm. First, we generate labeled synthetic data using a simulator, and thentrain a predictor on this data. Once the predictor is trained, We predict the labels of the unlabeledreal data, which is available in the training phase. We call these predicted labels ‘pseudo-labels’.
Figure 2: An overview of our adaptive data generation process. For the ease of explanation, considerthe eye gaze estimation problem where the goal is to estimate a 2-dimensional gaze vector Z =(θ, φ) given an image of the eye region. In the beginning, the simulator arbitrarily initializes thelabel distribution, say PZ. It then draws random labels according to the label distribution, andgenerates corresponding images X according to some rendering rules. Using this synthetic dataset,it trains a predictor F, and then predicts the gaze vectors of each real image (initially unlabeled),i.e., annotates each image Y with a pseudo-label W = (Fθ(Y), Fφ(Y)). The last stage estimatesthe pseudo-label distribution PW, which is used as the initial distribution of the subsequent iteration.
Figure 3:	Atoy example where perfect cycle-consistency is achieved but labels are permuted.
Figure 4:	Comparison of our methodology with the original S+U learning approach (Shrivastavaet al., 2017; Bousmalis et al., 2017b).
Figure 5: Some examples of the translation results (real → synthetic → real). The first column is theoriginal real image, say Y . The second and third columns are GY→X (Y ) and GX→Y(GY→X(Y )),where G’s are trained with the cyclic consistency term. The last two columns are GY→X (Y ) andGX→Y(GY→X(Y )), where G’s are trained with both the cyclic consistency term and the featureconsistency term. Both approaches achieve nearly perfect cycle-consistency; However, only thelatter approach maintains the structure of the images when translated to the other domain.
Figure 6:	Some examples of the translation results (synthetic → real → synthetic). The first col-umn is the original synthetic image, say X. The second and third columns are GX→Y (X) andGY→X (GX →Y (X)), where G’s are trained with the cyclic consistency term. The last two columnsare GX→Y(X) and GY→X (GX →Y (X)), where G’s are trained with both the cyclic consistencyterm and the feature consistency term.
Figure 7:	Some examples of the translation results (real → synthetic → real). The first column is theoriginal real image, say Y . The second and third columns are GY→X (Y ) and GX→Y(GY→X(Y )),where G’s are trained with the cyclic consistency term. The last two columns are GY→X (Y ) andGX→Y(GY→X(Y )), where G’s are trained with both the cyclic consistency term and the featureconsistency term. Both approaches achieve nearly perfect cycle-consistency; However, only thelatter approach maintains the structure of the images when translated to the other domain.
Figure 8:	Some examples of the translation results (synthetic → real → synthetic). The first col-umn is the original synthetic image, say X. The second and third columns are GX→Y (X) andGY→X (GX →Y (X)), where G’s are trained with the cyclic consistency term. The last two columnsare GX→Y(X) and GY→X (GX →Y (X)), where G’s are trained with both the cyclic consistencyterm and the feature consistency term.
