Figure 1: Importance of stochasticity in video prediction. In each video, a random shape followsa random direction (first row). Given only the first frame, the deterministic model from Finn et al.
Figure 2: Probabilistic graphical model ofstochastic variational video prediction, assumingtime-invariant latent. The generative model pre-dicts the next frame conditioned on the previousframes and latent variables (solid lines), while thevariational inference model approximates the pos-terior given all the frames (dotted lines).
Figure 3: Architecture of SV2P. At training time, the inference network (top) estimates the posteriorqφ(z|x0：T) = N(μ(xo:T), σ(x0:T)). The latent value Z 〜 q@(z|xo：T) is passed to the generativenetwork along with the (optional) action. The generative network (from Finn et al. (2016)) predictsthe next frame given the previous frames, latent values, and actions. At test time, Z is sampled fromthe assumed prior N(0, I).
Figure 4: Three phases of training. In the first phase, the inference network is turned off and only thegenerative network is being trained, resulting in deterministic predictions. The inference networkis used in the second phase without a KL-loss. The last phase includes DKL (qφ(z∣xo:T)∣∣p(z)) toenable accurate sampling latent from p(z). (a) the KL-loss (b) the reconstruction loss (c) Train-ing stability. This graph compares reconstruction loss at the end of five training sessions on theBAIR robot pushing dataset, with and without following all the steps of the training procedure. Theproposed training is quite stable and results in lower error compared to naive training.
Figure 5: Stochasticity of SV2P predictions onthe action-free BAIR dataset. Each line presentsthe sample with highest PSNR compared toground truth, after multiple sampling. The num-ber on the right indicates the number of randomsamples. As can be seen, SV2P predicts highlystochastic videos and, on average, only threesamples is enough to predict outcomes withhigher quality compared to Finn et al. (2016).
Figure 6: Quantitative comparison of the prediction methods. The stochastic models have been sam-pled 100 times and the results with the best PSNR have been displayed. For SV2P, we demonstratethe results of both time-variant and time-invariant latent sampling. Repeat shows the results of thelower bound prediction by repeating the last seen frame as the prediction. In the last column, wecompare the results of video pixel networks (VPN). All the models, including Finn et al. (2016),have been trained up to the frame marked by vertical separator and the results beyond this line dis-play their generalization. The plots are the average SSIM and PSNR over the test set and shadow isthe 95% confidence interval. In all of these graphs, higher is better.
Figure 7: Quantitative comparison of the pre-dicted frames on Human3.6M dataset usingconfidence of object detection as quality metric.
Figure 8: Comparing the results of SV2P with Finn et al. (2016) (second row) on action-free BAIRrobot pushing dataset. Fourth and fifth rows are the predictions with minimum and maximum PSNRout of 100 random outputs with time-invariant latent sampling. The last two rows are random pre-dicted outcomes. The numbers on top indicate the predicted frame number. In lack of actions andtherefore high stochasticity, Finn et al. (2016) only blurs the robotic arm out while the proposedmethod predicts sharper frames on each sampling. SV2P also predicts the interaction dynamics be-tween random movements of the arm and the objects.
Figure 9: Similar comparison as Figure 8 this time action-conditioned with time-variant latent sam-pling. SV2P predicts sharper and slightly variant outcomes compared to Finn et al. (2016). This isI əftuws Z ΦIdu!BSUlOPUeHuIOPUeHmostly evident in zoomed in objects which have been pushed by the arm.
Figure 10: Prediction results on the action-free Human3.6M dataset. SV2P predicts a different out-come on each sampling given the latent. In the left example, the model predicts walking as well asstopping which result in different outputs in predicted future frames. Similarly, the right exampledemonstrates various outcomes including spinning.
Figure 11: Comparing the results of video pixel networks (VPN) (Kalchbrenner et al., 2017; Reedet al., 2017) with SV2P on the robotic pushing dataset. We use the same best PSNR out of 100random samples for both methods. Besides stochastic movements of the pushed objects, anothersource of stochasticity is the starting lag in movements of the robotic arm. SV2P generates sharperimages compared to Finn et al. (2016) (notice the pushed objects in zoomed images) with less noisecompared to Reed et al. (2017) (look at the accumulated noise in later frames).
