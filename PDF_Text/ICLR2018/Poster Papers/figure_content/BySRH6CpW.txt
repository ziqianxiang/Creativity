Figure 1: The top histogram in each subfigure shows the pre-activation of a random neuron, zil,which is calculated in a regular feed-forward setting when explicitly sampling the weights. Thebottom shows samples from the approximated pre-activation using Lyapunov CLT. (a) refers to thefirst hidden layer whereas (b) refers to the last. We can see the approximation is very close to theactual pre-activation when sampling weights and performing standard feed-forward. In addition,we see it even holds for the first hidden layer, where the number of elements is not large (in thisexample, 27 elements for a 3 × 3 × 3 convolution).
Figure 2: (a) is an example of a neuron from the first hidden layer at the end of the training, withouta probability decay term. Since weight distributions converged to deterministic values, randomnessis very small, causing the CLT approximation to no longer hold. (b) is an example of a neuron fromthe same layer at the end of the training, but from an experiment with probability decay. In thisexperiment our approximation holds much better.
Figure 3: Visualization of the first 25 kernels from the first layer of the network trained on MNIST,from the binary, ternary and full precision networks. For the binary and ternary networks, gray,black and white represent 0, -1 and +1, respectively.
Figure 4: Comparison of CIFAR-10 training with our method and Gumbel-softmax.
Figure 5: (a) Average entropy of the third convolutional layer, during the last part of the training. (b)Average entropy of the last convolutional layer. The network is trained on CIFAR-10 in a ternarysetting.
Figure 6: Histogram of p(wij = 0) for the last convolutional layer. (a) Initialized value, beginningof training. (b) Learned values, end of training. The network is trained on CIFAR-10 in a ternarysetting.
