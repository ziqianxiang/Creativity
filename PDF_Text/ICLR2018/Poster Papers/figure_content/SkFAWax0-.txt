Figure 1: An overview of the VoiceLoop architecture. The reader combines the encoding of thesentence’s phonemes using the attention weights to create the current context. A new representationis created by a shallow network that receives the context, the speaker ID, the previous output, andthe buffer. The new representation is inserted into the buffer and the earliest vector in the buffer isdiscarded. The output is obtained by another shallow network that receives the buffer and the speakeras inputs. Once trained, fitting a new voice is done by freezing the network, except for the speakerembedding.
Figure 2: Memory Location Significance. For each of the three networks Nu, Na and No, we averagethe absolute values of the weights to the hidden layer across all hidden neurons and across the d rowsof the buffer. The result is a measure of the relative importance of each column of the buffer. Bestviewed in color.
Figure 3: Top: The attention probabilities obtained when mimicking three different North Americanspeakers from VCTK using the same sentence: “but there is no eye contact”. The x-axis is thetime along the generated audio. The y-axis depicts the sequence of phonemes. Dots indicate themaximal response along time for each phoneme, illustrating learned phoneme duration differencesbetween identities (not given during training). Bottom: The 4-th Mel-cepstrum for the three generatedsentences (dashed) as well as the ground-truth (solid) of the leftmost speaker. Best viewed in zoom.
Figure 4: Fitting new speaker embeddings to an existing VoiceLoop model. The graph plots top-1 identification accuracy with respect to a sample set length (in minutes) per speaker. Scoreswere averaged over 5 splits each. The “Full training” horizontal line is the top-1 accuracy for thecorresponding speakers, when trained together with the model from scratch. The leftmost datapointis for two sentences (about 10sec) per speaker.
Figure 5: Same input, different intonations. A single in the wild speaker saying the sentence “primingis done like that ”, where each time S0 is initialized differently. (a) Without priming. (b) Primingwith the word “I". (c) Priming with the word “had”. (d) Priming with the word “must”. (e) Primingwith the word “bye”. The figure shows the raw waveform, spectrogram, and F0 estimation (includevoicedness) in the first, second and third rows respectively. From the spectrogram plots we canobserve different duration for some phonemes. The F0 estimation of (c) and (d) shows that thespeaker talks in higher tone while in (b) and (e) we can observe lower tone of the speaker. Thisdemonstrates how priming changes the intonations of the model outputs.
