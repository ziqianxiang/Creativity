Figure 1: Prediction accuracy and mean training set margin as a function of regularization coeffi-cient, for a logistic regression trained on random (a) and informative (b) labels of the same inputs.
Figure 2: The cross-entropy and log evidence ratio, evaluated on random (a) or informative (b)labels. The evidence, evaluated on the training set, is strongly correlated with the test cross-entropy.
Figure 3: The evolution during training of the test accuracy (a), and the test set cross-entropy (b).
Figure 4: The test accuracy for a range of batch sizes, during training (a) and after 10000 steps (b).
Figure 5: a) The test set accuracy as a function of batch size, for a range of learning rates . Theperformance peak shifts to the right as we increase , but the overall performance falls once & 3.
Figure 6: a) The test accuracy as a function of batch size, for a range of training set sizes. To reducenoise, we average each curve over five experiments. The performance peak shift to the right as weincrease the size of the training set. Unsurprisingly, the overall model performance also improves.
Figure 7: a) The test set accuracy as a function of batch size for a range of momentum coefficients.
Figure 8: The mean test accuracy (a) and the mean test cross-entropy (b) of a regularized modelduring training. While full batch training takes longer to converge, it achieves similar performanceat long times. The noise inherent in small batch training causes the performance to fluctuate.
Figure 9: The gradient distribution of a randomly selected parameter in the softmax layer, whenmeasured over a single training example (a), and when averaged over mini-batches of 30 images (b).
