Figure 1: IRLC takes as input a counting question and image. Detected objects are added to thereturned count through a sequential decision process. The above example illustrates actual modelbehavior after training.
Figure 2: Examples of question-answer pairs that are excluded from HowMany-QA. This selectionexemplifies the common types of “number” questions that do not require counting and thereforedistract from our objective: (from left to right) time, general number-based answers, ballparking, andreading numbers from images. Importantly, the standard VQA evaluation metrics do not distinguishthese from counting questions; instead, performance is reported for “number” questions as a whole.
Figure 3: (a) Each model includes three basic modules: vision (blue), language (green), and count-ing (red). Text in the shaded regions describes which aspects of these modules are shared acrossmodels. (b) (left) The language model embeds the question and compares it to each object using ascoring function, which is jointly trained with caption grounding; (right) IRLC counting module.
Figure 4: Grounded counts produced by IRLC. Counts are formed from selections of detectedobjects. Each image displays the objects that IRLC chose to count.
Figure 5: Model performance on the HowMany-QA development set, grouped according to thefrequency with which the counting subject appeared in the training data.
Figure 6: (Left) Average grounding quality for each of the COCO object categories, as measuredfor SoftCount and IRLC. Each point represents a COCO category and is colored according to howcommon the category was during training (as in Figure 5). (Right) Histogram showing the differencein grounding quality between IRLC and SoftCount.
Figure 7: Examples of failure cases with common and rare subjects (“people” and “ties,” respec-tively). Each example shows the output of IRLC, where boxes correspond to counted objects, andthe output of UpDown, where boxes are shaded according to their attention weights.
Figure 8: Example outputs produced by each model. For SoftCount, objects are shaded accordingto the fractional count of each (0=transparent; 1=opaque). For UpDown, we similarly shade theobjects but use the attention focus to determine opacity. For IRLC, we plot only the boxes fromobjects that were selected as part of the count.
Figure 9: Sequential counting of IRLC. At each timestep, we illustrate the unchosen boxes in pink,and shade each box according to κt (corresponding to the probability that the box would be selectedat that time step; see main text). We also show the already-selected boxes in blue. For each of thequestions, the counting sequence terminates at t = 3, meaning that the returned count C is 3. Foreach of these questions, that is the correct answer. The example on the far right is a ‘correct failure,’a case where the correct answer is returned but the counted objects are not related to the question.
Figure 10:	Results of a hyperparameter sweep over the penalty weights. The accuracy over thedevelopment set is reported for each weight setting.
Figure 11:	HoWMany-QA test set performance for models trained with the full HoWMany-QAtraining data (blue) and trained without the additional data from Visual Genome (green).
Figure 12: (Left) Average count probability (Eq. 15) from UpDown, grouped according to theestimated count. (Right) Cumulative distribution of the absolute difference between the top twopredicted counts, shown for when the most likely count was less than 5 (blue) and when it wasgreater than or equal to 5 (green). The probability distributions are much less smooth when theestimated count is large.
