Figure 1:	Deep Gradient Compression can reduce the communication time, improve the scalability,and speed up distributed training.
Figure 2:	Momentum Correctionrequires adding the layer normalization(Lei Ba et al., 2016). Gradient Dropping saves 99% of gradi-ent exchange while incurring 0.3% loss of BLEU score on a machine translation task. Concurrently,Chen et al. (2017) proposed to automatically tunes the compression rate depending on local gra-dient activity, and gained compression ratio around 200× for fully-connected layers and 40× forconvolutional layers with negligible degradation of top-1 accuracy on ImageNet dataset.
Figure 3: Learning curves of ResNet in image classification task (the gradient sparsity is 99.9%).
Figure 4:	Perplexity and training loss of LSTM language model on PTB dataset (the gradient sparsityis 99.9%).
Figure 5:	WER and training loss of 5-layer LSTM on AN4 (the gradient sparsity is 99.9%).
Figure 6: Deep Gradient Compression improves the speedup and scalability of distributed training.
Figure 7: Distributed Synchronous SGDAlgorithm 2 Distributed Synchronous SGD onnode k___________________________________Input: Dataset χInput: minibatch size b per nodeInput: the number of nodes NInput: Optimization Function SGDInput: Init parameters W = {w[0],…，w[M]}1:	for t = 0,1,…do2:	Gk - 03:	for i = 1,…，B do4:	Sample data x from χ5：	Gk J Gk + N1bOf(X; Wt)6:	end for7:	All-reduce Gtk : Gt J PkN=1 Gtk8:	Wt+1 J SGD (Wt, Gt)9:	end forB Gradient sparsification with Nestrov momentum correctionThe conventional update rule for Nesterov momentum SGD (Nesterov, 1983) follows,N
