Figure 1: Gradient masking in single-step adversarial training. We plot the loss of model v3advon points x* = X + e] ∙ g + e2 ∙ g⊥, where g is the signed gradient and g⊥ is an orthogonal adversarialdirection. Plot (b) is a zoom of (a) near x. The gradient poorly approximates the global loss.
Figure 2: The dimensionality of the adversarial cone. For 500 correctly classified points x, andfor ∈ {4, 10, 16}, we plot the probability that we find at least k orthogonal vectors ri such thatkri k∞ = and x + ri is misclassified. For ≥ 10, model v3adv shows a bimodal phenomenon:most points x either have 0 adversarial directions or more than 90.
Figure 3:	Adversarial Examples on MNIST. (top) clean examples. (middle) inputs are rotated by20° and 5 random pixels are flipped. (bottom) The I-FGSM with = 0.3 is applied.
Figure 4:	Additional illustrations of the local curvature artifacts introduced by adversarialtraining on ImageNet. We plot the loss of model v3adv on samples of the form x* = X + e-g + e2 ∙g⊥ , where g is the signed gradient of v3adv and g⊥ is an orthogonal adversarial direction, obtainedfrom an Inception v4 model. The right-side plots are zoomed in versions of the left-side plots.
Figure 5: Illustrations of the local curvature artifacts introduced by adversarial training onMNIST. We plot the loss of model Aadv on samples of the form x* = X + e] ∙ g + e2 ∙ g⊥, whereg is the signed gradient of model Aadv and g⊥ is an orthogonal adversarial direction, obtained frommodel B. The right-side plots are zoomed in versions of the left-side plots.
