Figure 1: Deep Voice 3 uses residual convolutional layers to encode text into per-timestep key andvalue vectors for an attention-based decoder. The decoder uses these to predict the mel-scale logmagnitude spectrograms that correspond to the output audio. (Light blue dotted arrows depict theautoregressive process during inference.) The hidden states of the decoder are then fed to a converternetwork to predict the vocoder parameters for waveform synthesis. See Appendix A for more details.
Figure 2: The convolution block consists ofa 1-D convolution with a gated linear unit and a residualconnection. Here C denotes the dimensionality of the input. The convolution output of size 2 ∙ C issplit into equal-sized portions: the gate vector and the input vector.
Figure 3: Positional encodings are added to both keys and query vectors, with rates of ωkey and ωqueryrespectively. Forced monotonocity can be applied at inference by adding a mask of large negativevalues to the logits. One of two possible attention schemes is used: softmax or monotonic attentionfrom Raffel et al. (2017). During training, attention weights are dropped out.
Figure 4: Attention distributions (a) before training, (b) after training, but without inference con-straints, (c) with inference constraints applied to the first and third layers. (We empirically observethat fixing the attention of one or two dominant layers is sufficient for high-quality output.)be fixed for the key to the ratio of output timesteps to input timesteps (computed across the entiredataset). For multi-speaker datasets, ωs is computed for both the key and query from the speakerembedding for each speaker (depicted in Fig. 3). As sine and cosine functions form an orthonormalbasis, this initialization yields an attention distribution in the form of a diagonal line (see Fig. 4 (a)).
Figure 5: Generated WORLD vocoder parameters with fully connected (FC) layers.
Figure 6: Deep Voice 3 uses a deep residual convolutional network to encode text and/or phonemesinto per-timestep key and value vectors for an attentional decoder. The decoder uses these to predictthe mel-band log magnitude spectrograms that correspond to the output audio. (Light blue dottedarrows depict the autoregressive synthesis process during inference.) The hidden state of the decoderthen gets fed to a converter network to output linear spectrograms for Griffin-Lim or parameters forWORLD, which can be used to synthesize the final waveform. Weight normalization (Salimans &Kingma, 2016) is applied to all convolution filters and fully-connected layer weight matrices in themodel.
Figure 7: The first two principal components of the learned embeddings for (a) VCTK dataset (108speakers) and (b) LibriSpeech dataset (2484 speakers).
