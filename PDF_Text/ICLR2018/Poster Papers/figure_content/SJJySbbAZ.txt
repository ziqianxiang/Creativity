Figure 1: Training GAN with GD converges to a limit cycle that oscilates around the equilibrium (we appliedweight-clipping at 10 for the discriminator). On the contrary training with OMD converges to equilibrium interms of last-iterate convergence.
Figure 2: Robustness of last-iterate convergence of OMD to stochastic gradients.
Figure 3: KL divergence of WGAN trained with different optimization strategies. Methods are ordered by themedian KL divergence. Methods in (a) are named by the category and version of the method. “ratio 1” denotestraining the discriminator once for every generator training. Otherwise, we performed 5 iterations. For (b)where we don’t combine the models trained with different learning rates, the learning rate is appended at theend of the method name. For momentum and Nesterov momentum, we used γ = 0.9. For Adagrad, we usedthe default = 1e-8.
Figure 4:	Comparison of Adam and Optimistic Adam on CIFAR10.
Figure 5:	Persistence of limit cycles in multiple variants of GD training.
Figure 6: Stability of OMD vs GD in the co-variance learning problem for a two-dimensional gaussian (d = 2).
Figure 7: Stability of OMD vs GD in the co-variance learning problem for a three-dimensional gaussian (d3). Weight clipping in [-1, 1] was applied in both dynamics.
Figure 8:	Stochastic GD dynamics for covariance learning of a two-dimensional gaussian (d = 2). Weightclipping in [-1, 1] was applied to the discriminator weights.
Figure 9:	Stability of OMD with stochastic gradients for covariance learning of a two-dimensional gaussian(d = 2). Weight clipping in [-1, 1] was applied to the discriminator weights.
Figure 10: Samples of images from Generator trained via Optimistic Adam on CIFAR10.
Figure 11: Sample of images from Generator of Epoch 19 trained via Optimistic Adam and 1:1 training ratio.
Figure 12: Sample of images from Generator of Epoch 19 trained via Adam and 1:1 training ratio.
Figure 13: Sample of images from Generator of Epoch 19 trained via Adam and 5:1 training ratio.
Figure 14: The inception scores across epochs for GANs trained with Optimistic Adam (ratio 1) and Adam (ra-tio 5) on CIFAR10 (the two top-performing optimizers found in Section 6, with 10%-90% confidence intervals.
