Figure 1: Memory footprint of activations (ACTs) and weights (W) during training and inference for mini-batch sizes 1 and 32.
Figure 2: Memory requirements of a feed forward convolutional deep neural network. Orange boxes denoteweights (W), blue boxes are activations (ACT) and green boxes are gradient-maps (Grad).
Figure 3: Efficiency improvements from low-precision operations on GPU, FPGA and ASIC.
