Figure 1: Aggressive learning rate (red) fol-lowed by a decay schedule (yellow) wins overconservative learning rate (blue) by makingmore progress along the low curvature direction(x direction).
Figure 3: Comparisons of the optimized learning rates and momenta trained by gradient descent (red), greedylearning rates and momenta (blue), and the optimized fixed learning rate and momentum (green) in both noisy(a) and deterministic (b) quadratic settings. In the deterministic case, our optimized schedule matched thegreedy one, just as the theory predicts.
Figure 4: Regular SGD in the form of a computation graph. The learning rate parameter Î± is part of thedifferentiable computations.
Figure 5: Meta-objective surfaces and SMD trajectories (red) optimizing initial effective learning rate anddecay exponent with horizons of {100, 1k, 5k, 20k} steps2. 2.5k random samples with Gaussian interpolationare used to illustrate the meta-objective surface.
Figure 6: Training curves with best learningrate schedules from meta-objective surfaces with{100, 1k, 5k, 20k} step horizons.
Figure 7: Training curves and learning rates from online SMD with lookahead of 5 steps (blue), and hand-tunedfixed learning rate (red). Each blue curve corresponds to a different initial learning rate.
Figure 8: Online SMD with deterministic lookahead of 5 steps (blue), compared with a manually tuned fixedlearning rate (red). Other settings are the same as Figure 7.
