Figure 1: Routing (forward) ExamplefHI}f32叵f2A routing network consists of two components: a router and a set of function blocks, each of whichcan be any neural network layer. The router is a function which selects from among the functionblocks given some input. Routing is the process of iteratively applying the router to select a se-quence of function blocks to be composed and applied to the input vector. This process is illustratedin Figure 1. The input to the routing network is an instance to be classified (v, t), v ∈ Rd is a repre-sentation vector of dimension d and t is an integer task identifier. The router is given v, t and a depth(=1), the depth of the recursion, and selects from among a set of function block choices availableat depth 1, {f13, f12, f11}, picking f13 which is indicated with a dashed line. f13 is applied to theinput (v, t) to produce an output activation. The router again chooses a function block from thoseavailable at depth 2 (if the function blocks are of different dimensions then the router is constrainedto select dimensionally matched blocks to apply) and so on. Finally the router chooses a functionblock from the last (classification) layer function block set and produces the classification y.
Figure 2:	Training (backward) ExampleWe train the selected function blocks using SGD/backprop. In the example of Figure 1 this meanscomputing gradients for f32, f21 and f13. We then use the computed trace to train the router usingan RL algorithm. The high-level procedure is summarized in Algorithm 2 and illustrated in Figure 2.
Figure 3:	Task-based routing. hvalue, taski is the input consisting of value, the partial evaluationof the previous function block (or input x) and the task label task. αi is a routing agent; αd is adispatching agent.
Figure 4: Influence of the RL algorithm onCIFAR-MTL. Detailed descriptions of the im-plementation each approach can be found in theAppendix in Section 7.3.
Figure 5: Comparison of Routing Architec-tures on CIFAR-MTL. Implementation detailsof each approach can be found in the Appendixin Section 7.3.
Figure 6: Results on domain CIFAR-MTLFigure 7: Results on domain MIN-MTL (miniImageNet)8Published as a conference paper at ICLR 2018The task-specific-1-fc baseline has a separate last fully connected layer for each task and shares therest of the layers for all tasks. The task specific-all-fc baseline has a separate set of all the fully con-nected layers for each task. These baseline architectures allow considerable sharing of parametersbut also grant the network private parameters for each task to avoid interference. However, unlikerouting networks, the choice of which parameters are shared for which tasks, and which parametersare task-private is made statically in the architecture, independent of task.
Figure 7: Results on domain MIN-MTL (miniImageNet)8Published as a conference paper at ICLR 2018The task-specific-1-fc baseline has a separate last fully connected layer for each task and shares therest of the layers for all tasks. The task specific-all-fc baseline has a separate set of all the fully con-nected layers for each task. These baseline architectures allow considerable sharing of parametersbut also grant the network private parameters for each task to avoid interference. However, unlikerouting networks, the choice of which parameters are shared for which tasks, and which parametersare task-private is made statically in the architecture, independent of task.
Figure 8: Results on domain MNIST-MTLThe routing network on the other hand learns apolicy which, unlike the baseline static models,partitions the network quite differently for eachtask, and also achieves considerable diversity inits choices as can be seen in Figure 11. This fig-ure shows the routing decisions made over thewhole MNIST MTL dataset. Each task is la-beled at the top and the decisions for each ofthe three routed layers are shown below. Webelieve that because the routing network hasseparate policies for each task, it is less sen-sitive to a bias for one or two function blocksand each agent learns more independently whatworks for its assigned task.
Figure 9: The Policies of all Agents for the firstfunction block layer for the first 100 samples ofeach task of MNIST-MTLFigure 10: The Probabilities of all Agents oftaking Block 7 for the first 100 samples of eachtask (totalling 1000 samples) of MNIST-MTL5	Qualitative ResultsTo better understand the agent interaction we have created several views of the policy dynamics.
Figure 10: The Probabilities of all Agents oftaking Block 7 for the first 100 samples of eachtask (totalling 1000 samples) of MNIST-MTL5	Qualitative ResultsTo better understand the agent interaction we have created several views of the policy dynamics.
Figure 11: An actual routing map forMNIST-MTL.
Figure 12: Influence of the “collaboration reward” ρ on CIFAR-MTL. The architecture is routing-all-fc with WPL routing agents.
Figure 13: Comparison of per-task training cost for cross-stitch and routing networks. We add afunction block per task and normalize the training time per epoch by dividing by the number oftasks to isolate the effect of adding function blocks on computation.
Figure 15: Results on the first n tasks of CIFAR-MTL(d) first 10 tasks15Published as a conference paper at ICLR 2018Name	Num Agents	Policy Representation	Part of State = (v, t, d) UsedMARL:WPL	Num Tasks	Tabular (num layers X num function blocks)	t, dREINFORCE	Num Tasks	Vector (num layers) of approx functions	v, t, dQ-Learning	Num Tasks	Vector (num layers) of approx functions	v, t, dQ-Learning	Num Tasks	Tabular (num layers X num function blocks)	t, dTable 3: Implementation details for Figure 4. All approx functions are 2 layer MLPs with a hiddendim of 64.
