Figure 1: Left: The attention mechanism a(W~hi , W~hj ) employed by our model, parametrizedby a weight vector ~a ∈ R2F0, applying a LeakyReLU activation. Right: An illustration of multi-head attention (with K = 3 heads) by node 1 on its neighborhood. Different arrow styles andcolors denote independent attention computations. The aggregated features from each head areconcatenated or averaged to obtain ~h01 .
Figure 2: A t-SNE plot of the computed feature representations of a pre-trained GAT model’sfirst hidden layer on the Cora dataset. Node colors denote classes. Edge thickness indicates ag-gregated normalized attention coefficients between nodes i and j , across all eight attention heads(PkK=1 αikj +αjki).
