Figure 1: Toy illustration of the method. (A) There are two distributions of word embeddings, English wordsin red denoted by X and Italian words in blue denoted by Y , which we want to align/translate. Each dotrepresents a word in that space. The size of the dot is proportional to the frequency of the words in the trainingcorpus of that language. (B) Using adversarial learning, we learn a rotation matrix W which roughly aligns thetwo distributions. The green stars are randomly selected words that are fed to the discriminator to determinewhether the two word embeddings come from the same distribution. (C) The mapping W is further refined viaProcrustes. This method uses frequent words aligned by the previous step as anchor points, and minimizes anenergy function that corresponds to a spring system between anchor points. The refined mapping is then usedto map all words in the dictionary. (D) Finally, we translate by using the mapping W and a distance metric,dubbed CSLS, that expands the space where there is high density of points (like the area around the word“cat”), so that “hubs” (like the word “cat”) become less close to other word vectors than they would otherwise(compare to the same region in panel (A)).
Figure 2: Unsupervised model selection.
Figure 3: English to English word alignment accuracy. Evolution of word translation retrieval accuracy withregard to word frequency, using either Wikipedia (Wiki) or the Gigaword corpus (Giga), and either skip-gram,continuous bag-of-words (CBOW) or fastText embeddings. The model can learn to perfectly align embeddingstrained on the same corpus but with different seeds (a), as well as embeddings learned using different models(overall, when employing CSLS which is more accurate on rare words) (b). However, the model has moretrouble aligning embeddings trained on different corpora (Wikipedia and Gigaword) (c). This can be explainedby the difference in co-occurrence statistics of the two corpora, particularly on the rarer words. Performancecan be further deteriorated by using both different models and different types of corpus (d).
