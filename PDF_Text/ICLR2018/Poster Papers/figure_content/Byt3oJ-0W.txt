Figure 1: Schematic of Sinkhorn Network for Jigsaw puzzles. Each piece of the scrambled digit X isprocessed with the same (convolutional) network g1 (arrows with solid circles). The outputs lying ona latent space (rectangles surrounding X) are then connected through g2 (arrows with empty circles)to conform the rows of the matrix g(X, θ); g(X, θ)i = g1 ◦ g2 (Xi). Rows may be interpreted asunnormalized assignment probabilities, indicating individual unnormalized likelihoods of pieces ofX to be at every position in the actual image. Applying S(∙) leads to a ‘soft-permutation, Pθ X thatresolves inconsistencies in g(X, θ). Pθ χ is then used to recover the actual X at training, althoughat test time one may use the actual M (g(X, θ)).
Figure 2: (a) Sinkhorn networks can be trained to solve Jigsaw Puzzles. Given a trained model, ‘soft’reconstructions are shown at different T using S(X∕τ). We also show hard reconstructions, made bycomputing M(X) with the Hungarian algorithm (Munkres, 1957). (b) Sinkhorn networks can alsobe used to learn to transform any MNIST digit into another. We show hard and soft reconstructions,with τ = 1.
Figure 3: Illustrating the Matching and Sinkhorn operators, and the Gumbel-Matching and Gumbel-Sinkhorn distributions. Each 5x5 grid represents a matrix, with the shading indicating cell values(a) Matching operator M(X) applied to a parameter matrix X. (b) Sinkhorn Operator S(X∕τ)approximating M(X) for different temperature τ and number of Sinkhorn iterations, L. (c). Firstrow: samples from the Matching Sinkhorn distribution. Second and third rows: samples fromthe Gumbel-Sinkhorn distribution at two temperatures. At low temperature, both distributions areindistinguishable.
Figure 4: First column: samples from dataset created by mixing all pieces of digits, and then re-assembling them into ‘digits’. Second column: random permutations of first column. Third column:hard reconstructions using M(X). Fourth column: soft reconstructions using S(X∕τ) and T = 1.
