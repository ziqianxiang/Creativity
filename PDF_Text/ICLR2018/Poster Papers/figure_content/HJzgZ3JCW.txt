Figure 1: Combining Winograd convolution with sparse weights and activations. (a) ConventionalWinograd-based convolution fills in the zeros in both the weights and activations. (b) Pruning the4 Ã— 4 transformed kernel restores sparsity to the weights. (c) Our proposed Winograd-ReLU CNN.
Figure 2: Test accuracy vs density for the three models in Figure 1 on VGG-nagadomi.
Figure 3: Test accuracy vs density for the three models in Figure 1 on ConvPool-CNN-C.
Figure 4: Top-1 and top-5 validation accuracy vs density for three models on a variation of ResNet-18.
Figure 5: Kernels of ResNet-18 Winograd-ReLU model res2a_2a layer with density of 100% (left,87.43% top-5 accuracy), 35% (middle, 87.36% top-5 accuracy) and 15% (right, 86.57% top-5accuracy). Positive, negative and pruned weights are in red, blue and black respectively.
