Figure 1: The NNGP often outperforms finite width networks, and neural network performancemore closely resembles NNGP performance with increasing width. Test accuracy and mean squarederror on MNIST and CIFAR-10 dataset are shown for the best performing NNGP and best perform-ing SGD trained neural networks for given width. ‘NN-best’ denotes the best performing (on thevalidation set) neural network across all widths and trials. Often this is the neural network with thelargest width.
Figure 2: Generalization gap for five hidden layer fully-connected networks with variable widths,using ReLU and Tanh nonlinearities on CIFAR-10. Random optimization and initialization hy-perparameters were used and results were filtered for networks with 100% classification trainingaccuracy, resulting in a total of 125 Tanh and 55 ReLU networks. The best generalizing networksare consistently the widest.
Figure 3: The Bayesian nature of NNGP allows it to assign a prediction uncertainty to each testpoint. This prediction uncertainty is highly correlated with the empirical error on test points. Thex-axis shows the predicted MSE for test points, while the y-axis shows the realized MSE. To allowcomparison of mean squared error, each plotted point is an average over 100 test points, binned bypredicted MSE. The hyperparameters for the NNGP are depth= 3, σw2 = 2.0, and σb2 = 0.2. SeeAppendix Figure 8 for dependence on training set size.
Figure 4:	The best performing NNGP hyperparameters agree with those predicted by deep signal22propagation. Test set accuracy heatmaps for NNGPs evaluated for a grid of σw and σb values. Theright plot in each subfigure (a), (b) is a theoretical phase diagram for that nonlinearity following themethodology of Schoenholz et al. (2017). We observe that the performance of the NNGP is bestalong the critical line (dotted lines). Additional depths are shown in the Appendix Figure 9.
Figure 5:	Samples from an NNGP prior for 1D functions. Different lines correspond to differentdraws (arbitrary colors).
Figure 6: The angular structure of the kernel and its evolution with depth. Also illustrated is thegood agreement between the kernel computed using the methods of Section 2.5 (blue, starred) andthe analytic form of the kernel (red). The depth l in Kl runs from l = 0, ..., 9 (flattened curves forincreasing l), and (σw2 , σb2) = (1.6, 0.1).
Figure 7: Graphical model for neural network’s computation.
Figure 8:	The prediction uncertainty for smaller number of training points. The details are the sameas Figure 3.
Figure 9:	Test set accuracy heatmaps for NNGPs evaluated for a grid of σw2 and σb2 values for varyingdepth. Rows correspond to Tanh and ReLU nonlinearities, and columns correspond to varying depth.
Figure 10:	Best performing NNGPs are distributed near the critical line. Weight and bias variancedistribution for the 25 best performing runs for NNGP with the given training set size is shown.
