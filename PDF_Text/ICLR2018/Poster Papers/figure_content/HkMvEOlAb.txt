Figure 1: Assume that we are given a dataset of hand-written digits such as MNIST where the overalltask is the complete categorization of each digit. Then, we simply augment the dataset by applyinga transformation to examples, e.g. rotating by 90o, and label each of them either as original or asrotated. This new augmented dataset is provided to the network as a two-class classification problem.
Figure 2: Neural network structure with the ACOL. Each softmax node corresponds to an individualsub-class of a parent, i.e. annotation. During feedforward operation of the network, pooling layercalculates final parent-class predictions through sub-class probabilities.
Figure 3: t-SNE visualization of the latent space F throughout the training for 2000 untransformedtest examples from MNIST. Color codes denote the ground-truths for the digits. Note the separationof clusters from epoch 1 to epoch 400 of the unsupervised (but pseudo supervised) training. Forreference, clustering accuracy for the entire test set is also provided. This figure is best viewed incolor.
Figure 4: The average value for each dimension of F, Z and softmax(Z) observed with respect tountransformed test set examples and the norm of the associated weights. Note that the representationon F is not distributed to the entire space but the weights associated to these unused dimensionsdo not decay. On the other hand, due to the pseudo supervision task, the output of the augmentedsoftmax layer i.e. softmax(Z), becomes a one-hot encoded representation of which 140 dimensionsare inactive for the untransformed examples; however, the representation at its input is distributed toall dimensions. The last plot shows how the dimension size of F affects the clustering performance.
Figure 5: Comparison of t-SNE visualizations of the latent spaces F, Z and softmax(Z) for 2000 testexamples from MNIST. Color codes denote the ground-truths for the digits and each label representsthe major digit of a cluster. Clusters are not well-separated on one-hot encoded softmax(Z); however,separations of the clusters are quite similar and clear on the representation spaces F and Z. Thisfigure is best viewed in color.
Figure 6: Visualizations of the graph GY and its spanning subgraph GM for randomly chosen 500test examples from MNIST (this selection is performed only for the simplicity of the visualization).
Figure 7: Illustration of a few examples of each cluster for two different k settings i.e. 7 and 20.
Figure 8: t-SNE visualizations of the representation spaces observed when different sets of trans-formations are adopted. The first row illustrates the clustering results when one of four differenttransformation types, i.e. scaling, shearing, translation and random permutation of the pixels, isapplied variably to generate 8 pseudo parent-classes. The second row presents the results obtainedwhen rotation-based transformations listed in (15) are adopted. To summarize, the type of the trans-formation generating the pseudo parent-classes is more important than their number and differenttransformations can reveal different clustering patterns. Therefore, finding the right transformationtype for the clustering task of concern is crucial for the proposed approach in this paper.
