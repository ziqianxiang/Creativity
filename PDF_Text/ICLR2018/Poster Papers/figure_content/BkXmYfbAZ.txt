Figure 1: Classes of existing deep multitask learning architectures. (a) Classical approachesadd a task-specific decoder to the output of the core single-task model for each task; (b) Column-based approaches include a network column for each task, and define a mechanism for sharingbetween columns; (c) Supervision at custom depths adds output decoders at depths based on a taskhierarchy; (d) Universal representations adapts each layer with a small number of task-specificscaling parameters. Underlying each of these approaches is the assumption of parallel ordering ofshared layers (Section 2.2): each one requires aligned sequences of feature extractors across tasks.
Figure 2: Fitting two random tasks. (a) The dotted lines show that permuted ordering fits n samplesas well as parallel fits n/2 for linear networks; (b) For ReLU networks, permuted ordering enjoys asimilar advantage. Thus, permuted ordering of shared layers eases integration of information acrossdisparate tasks.
Figure 3: Soft ordering of shared layers. Sample soft ordering network with three shared layers.
Figure 4: MNIST results. (a) Relative performance of permuted and soft ordering compared toparallel ordering improves as the number of tasks increases, showing how flexibility of order canhelp in scaling to more tasks. Note that cost savings of multitask over single task models in termsof number of trainable parameters scales linearly with the number of tasks. For a representativetwo-task soft order experiment (b) the layer-wise distance between scalings of the tasks increases byiteration, and (c) the scalings move towards a hard ordering. (d) The final learned relative scale ofeach shared layer at each depth for each task is indicated by shading, with the strongest path drawn,showing that a distinct soft order is learned for each task (â€¢ marks the shared model boundary).
Figure 5: UCI data sets and results. (a) The ten UCI tasks used in joint training; the varying typesof problems and dataset characteristics show the diversity of this set of tasks. (b) Mean test errorover all ten tasks by iteration. Permuted and parallel order show no improvement after the first 1000iterations, while soft order decisively outperforms the other methods.
Figure 6: Omniglot results. (a) Error by number of tasks trained jointly. Soft ordering significantlyoutperforms single task and both fixed ordering approaches for each number of tasks; (b) Distribu-tion of learned layer usage by depth across all 50 tasks for a soft order run. The usage of each layeris correlated (or inversely correlated) with depth. This coincides with the understanding that thereis some innate hierarchy in convolutional networks, which soft ordering is able to discover. Forinstance, the usage of Layer 3 decreases as the depth increases, suggesting that its primary purposeis low-level feature extraction, though it is still sees substantial use in deeper contexts; (c) Errorswith all 50 tasks for different training set sizes. The first five methods are previous deep MTL results(Yang and Hospedales, 2017), which use multitask tensor factorization methods in a shared parallelordering. Soft ordering significantly outperforms the other approaches, showing the approach scalesto real-world tasks requiring specialized components such as convolutional layers.
Figure 7: CelebA results. Layer usage by depth (a) without and (b) with inclusion of the identitylayer. In both cases, layers with lower usage at lower depths have higher usage at higher depths, andvice versa. The identity layer almost always sees increased usage; its application can increase con-sistency of representation across contexts. (c) Soft order models achieve a significant improvementover parallel ordering, and receive a boost from including the identity layer. The first two rows areprevious work with ResNet-50 that show their baseline improvement from single task to multitask.
