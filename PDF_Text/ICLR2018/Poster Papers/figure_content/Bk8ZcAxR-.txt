Figure 1: Successor representation, with respect to the uniform random policy, of state A (left). Thisexample is similar to Dayan’s (1993). The red color represents larger values while the blue colorrepresents smaller values (states that are temporally further away).
Figure 2: Neural network architecture used to learn the SR. The symbols N and 0 denote element-wise multiplication and the fact that gradients are not propagated further back, respectively.
Figure 3: Results in the rooms domain. The rightmost figure depicts the diffusion time as eigenop-tions are added to the agent’s action set (sorted by eigenvalues corresponding to the eigenpurposes).
Figure 4: Different environments (varying start and goal locations) used in our evaluation (a), aswell as the learning curves obtained in each one of these environments (b, C) for different number ofoptions obtained from the SR when estimated after 100 episodes. See text for more details.
Figure 5: Plots of density of state visitation of eigenoptions discovered in three Atari 2600 games.
Figure 6: Eigenoptions discovered in the game Freeway.
Figure 7: Evolution of the first eigenvector being estimated by the SR and baselines.
Figure 8: Evolution of the second eigenvector being estimated by the SR and baselines.
Figure 9: Evolution of the third eigenvector being estimated by the SR and baselines.
Figure 10: Evolution of the fourth eigenvector being estimated by the SR and baselines.
Figure 11:	Evolution of the first eigenoption being estimated by the SR and baselines.
Figure 12:	Evolution of the second eigenoption being estimated by the SR and baselines.
Figure 13:	Evolution of the third eigenoption being estimated by the SR and baselines.
Figure 14:	Evolution of the fourth eigenoption being estimated by the SR and baselines.
Figure 15: Different environments (varying start and goal locations) used when evaluating theagent,s ability to accumulate reward with and without eigenoptions.
Figure 16:	Plot depicting the agent,s performance when following options obtained through esti-mates of the SR (100, 500, and 1, 000 episodes), as well as through the true SR, in environment 1.
Figure 17:	Plot depicting the agent's performance when following options obtained through esti-mates of the SR (100, 500, and 1, 000 episodes), as Wen as through the true SR, in environment 2.
Figure 18:	Plot depicting the agent,s performance when following options obtained through esti-mates of the SR (100, 500, and 1, 000 episodes), as well as through the true SR, in environment 3.
Figure 19:	Plot depicting the agent,s performance when following options obtained through esti-mates of the SR (100, 500, and 1, 000 episodes), as well as through the true SR, in environment 4.
Figure 20:	Final 1-step predictions in the game Bank Heist. We use the task of predicting the nextgame screen as an auxiliary task when estimating the successor representation.
Figure 21:	Final 1-step predictions in the game Freeway. We use the task of predicting the nextgame screen as an auxiliary task when estimating the successor representation.
Figure 22: Final 1-step predictions in the game MONTEZUMA’S Revenge. We use the task ofpredicting the next game screen as an auxiliary task when estimating the successor representation.
Figure 23: Final 1-step predictions in the game Ms. PACMAN. We use the task of predicting thenext game screen as an auxiliary task when estimating the successor representation.
