Figure 1: Schematics of the attention mechanisms discussed in this paper. Each node representsthe possibility of the model attending to a given memory entry (horizontal axis) at a given outputtimestep (vertical axis). (a) In soft attention, the model assigns a probability (represented by theshade of gray of each node) to each memory entry at each output timestep. The context vector iscomputed as the weighted average of the memory, weighted by these probabilities. (b) At test time,monotonic attention inspects memory entries from left-to-right, choosing whether to move on to thenext memory entry (shown as nodes with ×) or stop and attend (shown as black nodes). The contextvector is hard-assigned to the memory entry that was attended to. At the next output timestep, it startsagain from where it left off. (c) MoChA utilizes a hard monotonic attention mechanism to choosethe endpoint (shown as nodes with bold borders) of the chunk over which it attends. The chunkboundaries (here, with a window size of 3) are shown as dotted lines. The model then performs softattention (with attention weighting shown as the shade of gray) over the chunk, and computes thecontext vector as the chunk’s weighted average.
Figure 2: Attention alignmentsplots and speech utterance fea-ture sequence for the speechrecognition task.
Figure 3: Speeds of different attention mechanisms on a synthetic benchmark.
Figure 4: Schematic of the test-time de-coding procedure of MAtChA. The se-mantics of the nodes and horizontal andvertical axes are as in figs. 1a to 1c.
