Figure 1: Noised training versus the non-privatebaseline. The model with σ = 0.003 nearlymatches the baseline.
Figure 2: The effect of update clipping on theconvergence of FedAvg, after 100, 500, and3000 rounds of training.
Figure 3: The effect of different levels of noiseσ for flat and per-layer clipping at S = 20. Thevertical dashed red line is σ = 0.2.
Figure 4: Different noise/clipping tradeoffs (allof equal privacy cost), for initial training (red)and adjusted after 4885 rounds (green and blue).
Figure 5: Comparison of sampling strategies and estimators. Fixed sample is exactly C = 100 usersper round, and variable sample selects uniformly with probability q for C = 100. The true averagecorresponds to f, fixed denominator is ff, and clipped denominator is fFigure 6: The effect of C for FedAvg using theexact estimator and without noise or clipping.
Figure 6: The effect of C for FedAvg using theexact estimator and without noise or clipping.
Figure 8: The effect of different noise vs. clipping tradeoffs on convergence. Both plots use the samelegend, where we vary S and σ together to maintain the same z = 0.06 with 100 users (actuallyused), or z = 1 with 1667 users. We take S = 20 and σ = 0.012 (black line) as a baseline; the left-hand plot shows training from a randomly initialized model, and includes two different runs withS = 20, showing only mild variability. For the right-hand plot, we took a snapshot of the S = 20model after 4885 initial rounds of training, and resumed training with different tradeoffs.
Figure 7: Training with (expected) 100 vs 1250users per round, both with flat-clipping at S =15.0 and per-coordinate noise with σ = 0.012.
Figure 9: Effect of clipping on FedSGD withC = 50 users per round and a learning rate ofη = 6. A much smaller clipping level S can beused compared to FedAvg.
Figure 10: Effect of noised updates on FedSGDwith S = 20 (based on Figure 9, a smallervalue would actually be better when doing pri-vate training). FedSGD is more sensitive to noisethan FedAvg, likely because the updates aresmaller in magnitude.
Figure 11: Effect of clipping on models withlarger dictionaries (20000 and 30000 tokens).
Figure 12: Effect of noised updates on modelswith larger dictionaries, when clipped at S = 20.
