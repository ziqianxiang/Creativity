Figure 1: Visualizations of the posterior approximations in a linear example.
Figure 2: The impact on regret of different approximated posteriors. We show (green) the actuallinear posterior, (orange) the diagonal posterior approximation and (blue) the precision approximationin 2a. In 2b and 2c we visualize the impact of the approximations on cumulative regret.
Figure 3: Wheel bandits for increasing values of δ ∈ (0, 1). Optimal action for blue, red, green,black, and yellow regions, are actions 1, 2, 3, 4, and 5, respectively.
Figure 4: Cumulative regret for Bayes By Backprop (Variational Inference, fixed noise σ = 0.75)applied to a linear model and an exact mean field solution, denoted PrecisionDiag, with a linear bandit(left) and with the Statlog bandit (right). The suffix of the BBB legend label indicates the number oftraining epochs in each training step. We emphasize that in this evaluation, all algorithms use thesame family of models (i.e., linear). While PrecisionDiag exactly solves the mean field problem, BBBrelies on partial optimization via SGD. As the number of training epochs increases, BBB improvesperformance, but is always outperformed by PrecisionDiag.
Figure 6: We qualitatively compare plots of the sample distribution from various methods, similarlyto Hernandez-Lobato et al. (2016). We plot the mean and standard deviation of 100 samples drawnfrom each method conditioned on a small set of observations with three outputs (two are from thesame underlying function and thus strongly correlated while the third (bottom) is independent). Thetrue underlying functions are plotted in red.
