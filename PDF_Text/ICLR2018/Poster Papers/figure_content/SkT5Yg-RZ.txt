Figure 1: Illustration of the self-play concept in a gridworld setting. Training consists of two typesof episode: self-play and target task. In the former, Alice and Bob take turns moving the agentwithin the environment. Alice sets tasks by altering the state via interaction with its objects (key,door, light) and then hands control over to Bob. He must return the environment to its original stateto receive an internal reward. This task is just one of many devised by Alice, who automaticallybuilds a curriculum of increasingly challenging tasks. In the target task, Bob’s policy is used tocontrol the agent, with him receiving an external reward if he visits the flag. He is able to learn todo this quickly as he is already familiar with the environment from self-play.
Figure 2: Left: The hallway task from section 4.1. The y axis is fraction of successes on the targettask, and the x axis is the total number of training examples seen. Standard policy gradient (red)learns slowly. Adding an explicit exploration bonus (Strehl & Littman, 2008) (green) helps signif-icantly. Our self-play approach (blue) gives similar performance however. Using a random policyfor Alice (magenta) drastically impairs performance, showing the importance of self-play betweenAlice and Bob. Right: Mazebase task, illustrated in Fig. 1, for p(Light off) = 0.5. Augmenting withthe repeat form of self-play enables significantly faster learning than training on the target task aloneand random Alice baselines.
Figure 3: Inspection of a Mazebase learning run, using the environment shown in Fig. 1. (a): rateat which Alice interacts with 1, 2 or 3 objects during an episode, illustrating the automaticallygenerated curriculum. Initially Alice touches no objects, but then starts to interact with one. But thisrate drops as Alice devises tasks that involve two and subsequently three objects. (b) by contrast, inthe random Alice baseline, she never utilizes more than a single object and even then at a much lowerrate. (c) plot of Alice and Bob’s reward, which strongly correlates with (a). (d) plot ofta as self-playprogresses. Alice takes an increasing amount of time before handing over to Bob, consistent withtasks of increasing difficulty being set.
Figure 4: Evaluation on MountainCar (left) and SwimmerGather (right) target tasks, comparing toVIME Houthooft et al. (2016) and SimHash Tang et al. (2016) (figures adapted from Tang et al.
Figure 5: Left: Different types of unit in the StarCraft environment. The arrows represent possibleactions (excluding movement actions) by the unit, and corresponding numbers shows (blue) amountof minerals and (red) time steps needed to complete. The units under agent’s control are outlined by agreen border. Right: Plot of reward on the StarCraft sub-task of training marine units vs #target-taskepisodes (self-play episodes are not included), with and without self-play. A count-based baselineis also shown. Self-play greatly speeds up learning, and also surpasses the count-based approach atconvergence.
Figure 6: Left: The performance of self-play when p(Light off) set to 0.3. Here the reverse formof self-play works well (more details in the text). Right: Reduction in target task episodes relativeto training purely on the target-task as the distance between self-play and the target task varies (forruns where the reward goes above -2 on the Mazebase task - unsuccessful runs are given a unityspeed-up factor). The y axis is the speedup, and x axis is p(Light off). For reverse self-play, thelow p(Light off) corresponds to having self-play and target tasks be similar to one another, whilethe opposite applies to repeat self-play. For both forms, significant speedups are achieved whenself-play is similar to the target tasks, but the effect diminishes when self-play is biased against thetarget task.
Figure 7: A single SwimmerGather training run.
Figure 8: Plot of Alice’s location at time of STOP action for the SwimmerGather training runshown in Fig. 7, for different stages of training. Note how Alice’s distribution changes as Bob learnsto solve her tasks.
Figure 9: Plot of reward on the StarCraft sub-task of training where episode length tMax is increasedto 300.
