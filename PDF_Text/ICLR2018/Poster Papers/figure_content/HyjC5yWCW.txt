Figure 1: A deep fully-connected neural network with N+2 layers and ReLU nonlinearities. With this genericfully connected network, we prove that, with a single step of gradient descent, the model can approximate anyfunction of the dataset and test input.
Figure 2: The effect of additional gradient steps at test time when attempting to solve new tasks. The MAMLmodel, trained with 5 inner gradient steps, can further improve with more steps. All methods are provided withthe same data - 5 examples - where each gradient step is computed using the same 5 datapoints.
Figure 3: Learning performance on out-of-distribution tasks as a function of the task variability. Recurrentmeta-learners such as SNAIL and MetaNet acquire learning strategies that are less generalizable than thoselearned with gradient-based meta-learning.
Figure 4: Comparison of finetuningfrom a MAML-initialized network anda network initialized randomly, trainedfrom scratch. Both methods achieveabout the same training accuracy. But,MAML also attains good test accu-racy, while the network trained fromscratch overfits catastrophically to the20 examples. Interestingly, the MAML-initialized model does not begin to over-fit, even though meta-training used 5steps while the graph shows up to 100.
Figure 5: Comparison of depth while keeping the number of parameters con-stant. Task-conditioned models do not need more than one hidden layer,whereas meta-learning with MAML clearly benefits from additional depth.
Figure 6: Left: Another comparison with out-of-distribution tasks, varying the phase of the sine curve. Thereis a clear trend that gradient descent enables better generalization on out-of-distribution tasks compared tothe learning strategies acquired using recurrent meta-learners such as SNAIL. Right: Here is another examplethat shows the resilience of a MAML-learned initialization to overfitting. In this case, the MAML model wastrained using one inner step of gradient descent on 5-way, 1-shot Omniglot classification. Both a MAML andrandom initialized network achieve perfect training accuracy. As expected, the model trained from scratchcatastrophically overfits to the 5 training examples. However, the MAML-initialized model does not begin tooverfit, even after 100 gradient steps.
