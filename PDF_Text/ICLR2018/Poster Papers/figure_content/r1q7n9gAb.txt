Figure 1:	Visualization of or main results on a synthetic dataset in which the L2 max margin vectorw^ is precisely known. (A) The dataset (positive and negatives samples (y = ±1) are respectivelydenoted by 0+0 and 0◦/), max margin separating hyperplane (black line), and the asymptotic solutionof GD (dashed blue). For both GD and GD with momentum (GDMO), we show: (B) The norm ofw (t), normalized so it would equal to 1 at the last iteration, to facilitate comparison. As expected(eq. 2.3), the norm increases logarithmically; (C) the training loss. As expected, it decreases as t-1(eq. 3.4); and (D&E) the angle and margin gap of W (t) from w^ (eqs. 3.2 and 3.3). As expected,these are logarithmically decreasing to zero. Implementation details: The dataset includes foursupport vectors: x1 = (0.5, 1.5) , x2 = (1.5, 0.5) with y1 = y2 = 1, and x3 = -x1, x4 = -x2 withy3 = y4 = -1 (the L? normalized max margin vector is then W = (1,1) /√2 with margin equalto √2), and 12 other random datapoints (6 from each class), that are not on the margin. We useda learning rate η = 1∕σmaχ (X), where °m&x (X) is the maximal singular value of X, momentumγ = 0.9 for GDMO, and initialized at the origin.
Figure 2:	Same as Fig. 1, except we multiplied all x2 values in the dastaset by 20, and also trainusing ADAM. The final weight vector produced after 2 ∙ 106 epochs of optimization using ADAM(red dashed line) does not converge to L2 max margin solution (black line), in contrast to GD (bluedashed line), or GDMO.
Figure 3: Training of a convolutional neural network on CIFAR10 using stochastic gradient descentwith constant learning rate and momentum, softmax output and a cross entropy loss, where weachieve 8.3% final validation error. We observe that, approximately: (1) The training loss decays as at-1, (2) the L2 norm of last weight layer increases logarithmically, (3) after a while, the validationloss starts to increase, and (4) in contrast, the validation (classification) error slowly improves.
Figure 4: Same as Fig. 1, except stochastic gradient decent is used (with mini-batch of size 4), insteadof GD.
