Figure 1: A schematic depiction of gradient methods. (a) Classical networks are trained by marching down theloss function until a minimizer is reached. Because classical loss functions are bounded from below, the solutionpath gets stopped when a minimizer is reached, and the gradient method remains stable. (b) Adversarial net lossfunctions may be unbounded from below, and training alternates between minimization and maximization steps.
Figure 2: A schematic depiction of the prediction method. When the minimization step is powerful andmoves the iterates a long distance, the prediction step (dotted black arrow) causes the maximization update tobe calculated further down the loss surface, resulting in a more dramatic maximization update. In this way,prediction methods prevent the maximization step from getting overpowered by the minimization update.
Figure 3:	Comparison of the classification accuracy (digit parity) and discriminator (noisy vs. no-noise)accuracy using SGD and Adam solver with and without prediction steps. θf and θd refers to variables in eq. (5).
Figure 4:	Comparison of GAN training algorithms for DCGAN architecture on Cifar-10 image datasets. Usingdefault parameters of DCGAN; lr = 0.0002, β1 = 0.5.
Figure 5:	Comparison of GAN training algorithms for DCGAN architecture on Cifar-10 image datasets withhigher learning rate, lr = 0.001, β1 = 0.5.
Figure 6: Model selection for learning a fair classifier. (a) Comparison of yt,delta (higher is better),and also ydisc (lower is better) and yacc on the test set using AFLR with and without prediCtivesteps. (b) Number of enCoder layers in the seleCted model. (C) Number of disCriminator layers (bothadversarial and task-speCifiC) in the seleCted model.
Figure 7:	Comparison of the classification accuracy of parity classification and noise discriminationusing the SGD and Adam solvers with and without prediction step.
Figure 8:	Comparison of GAN training algorithms on toy dataset. Results on, from top to bottom,GAN, GAN with G prediction, and unrolled GAN.
Figure 9: Comparison of GAN training algorithms on toy dataset of mixture of 100 Gaussians.
Figure 10: Comparison of GAN training algorithms for DCGAN architecture on Cifar-10 imagedatasets. Using higher momentum, lr = 0.0002, β1 = 0.9.
Figure 11:	Comparison of GAN training algorithms for DCGAN architecture on Cifar-10 imagedatasets. lr = 0.0004, β1 = 0.5.
Figure 12:	Comparison of GAN training algorithms for DCGAN architecture on Cifar-10 imagedatasets. lr = 0.0006, β1 = 0.5.
Figure 13:	Comparison of GAN training algorithms for DCGAN architecture on Cifar-10 imagedatasets. lr = 0.0008, β1 = 0.5.
Figure 14:	Comparison of Inception scores on high resolution Imagenet datasets measured at eachtraining epoch of ACGAN model with and without prediction.
