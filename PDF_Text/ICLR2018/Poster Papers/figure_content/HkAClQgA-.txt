Figure 1: Illustration of the encoder and decoder attention functions combined. The two contextvectors (marked “C”) are computed from attending over the encoder hidden states and decoderhidden states. Using these two contexts and the current decoder hidden state (“H”), a new word isgenerated and added to the output sequence.
Figure 2: Cumulated ROUGE-1 relative im-provement obtained by adding intra-attentionto the ML model on the CNN/Daily Maildataset.
