Figure 1: Density es-timate of Vφ ELBO fordifferent Keasiest to appreciate by noting that for K > 10, there is a roughly equal probability of the estimatebeing positive or negative, such that we are equally likely to increase or decrease the parameter valueat the next SGA iteration, inevitably leading to poor performance. On the other hand, when K = 1,it is far more likely that the gradient estimate is positive than negative, and so there is clear drift tothe gradient steps. We add to the empirical evidence for this behavior in Section 5. Note the criticaldifference for model learning is that Vθ ELBO does not, in general, decrease in magnitude as Kincreases. Note also that using a larger K should always give better performance at test time; it maythough be better to learn φ using a smaller K.
Figure 2: (Left) Log marginal likelihood analytically evaluated at every θ during optimization; theblack line indicates maxθ log pθ (yi：T) obtained by the expectation maximization (EM) algorithm.
Figure 3: (Left) Optimizing ELBOIS for the Gaussian unknown mean model with respect to φ resultsin worse φ as we increase number of particles K. (Middle, right) Optimizing ELBOSMC with respectto (θ, φ) for LGSSM and using the ALT algorithm for updating (θ, φ) with (Aθ, K) = (SMC, 1000)and (Aφ, Κφ) = (IS, 10). Right measures the quality of φ by showing JpT=ι(μkalman - μapprox)2where μkalman is the marginal mean obtained from the Kalman smoothing algorithm under the modelwith EM-optimized parameters and μapprox is an marginal mean obtained from the set of 10 SMCparticles with learned/bootstrap proposal.
Figure 4: (Left) Rolling mean over 5 epochs of max(ELBθsMc, ELBOIS) on the test set, lines in-dicate the average over 3 random seeds and shaded areas indicate standard deviation. The colorindicates the number of particles, the line style the used algorithm. (Right) The table shows the finalmax(ELBOSMC, ELBOIS)for each learned model.
Figure 5: T = 200 model described in Section 5.1. Kernel density estimation (KDE) of N§、ELBOSMCevaluated at θ1 = 0.1 with K = 16 using 100 samples.
Figure 6: (Left) particle set (xi：K, ai：K-iancestor indices.
Figure 7:	Visualisation of the learned model. Ground truth observations (top row in each sub figure)are only revealed to the algorithm up until t=19 inclusive. The second row shows the predictionaveraged over all particles, all following rows show the prediction made by a single particle. (Top)IWAE. (Bottom) AESMC.
Figure 8:	(Left) Optimizing ELBO with respect to φ for LGSSM. (Right) The lengths of the squares areproportional (with a constant factor) to y PT=ι(μkalman - μapprox)2 which is a proxy for inferencequality of φ described in the main text. The larger the square, the worse the inference.
