Figure 1: Dynamical systems view of ResNets. ResNets equally discretize the time interval [0, T]using time points T), Ti,..., Td, where T) = 0, Td = T and d is the total number of blocks.
Figure 2: A residual block.
Figure 3: The average L2-norm of the residual modules γ vs the number of residual blocksd. The curve resembles a reciprocal function, which is consistent with Eq. (5) and the dynamicalsystems view.
Figure 4: L2-norm of the input and output of the residual module G. ResNet-110 models aretrained on CIFAR-10 and CIFAR-100. The norms are evaluated at test time. It shows that withina residual block, the identity mapping contributes much more than the residual module. In otherwords, G(Yj) is relatively small for most residual blocks.
Figure 5: An illustration of the interpolation operation for one stage. We insert one residualblock right after each existing block in the stage. The model parameters, including convolutionalweights and batch normalization parameters, are copied from the adjacent old block to interpolatedblocks. After that, the explicit step size h is halved. For example, before interpolation, this stage hasthree residual blocks, numbered 1 to 3. After interpolation, block 1, 2 and 3 become block 1', 3'and 5' respectively. Three new blocks are inserted: block 2‘，4‘ and 6'，whose parameters are copiedfrom its previous block respectively.
Figure 6: Train and test curves using our multi-level method with ResNet-50-i and WResNet-50-i on CIFAR-10/100. The models are interpolated at epoch 60 and 110, dividing the training stepsto three cycles. Although both training and test accuracy temporarily drops at the start of each cycle,the performance eventually surpasses the previous cycles.
Figure 7: Comparison of kF(Yj)k among three models: (1) ResNet-32 with h = 1, (2) ResNet-62with h = 0.5, (3) ResNet-122 with h = 0.25. If the dynamical systems view is correct, the threecurves should approximately follow the same trend.
Figure 8: Train and test curves using our multi-level method with ResNet-50-i and WResNet-50-i on STL10. Although both training and testing accuracy temporarily drops at the start of eachcycle, the performance eventually surpasses the previous cycles.
Figure 9: Effect of ηmax and ηmin in cyclic learning rate. In most cases, as ηmax increases, thetesting accuracy increases first and then decreases.
Figure 10: Effect of resetting the learning rate. Resetting the learning rate at the beginning ofeach cycle gives better validation accuracy.
Figure 11: Shallow and deep ResNets. Training accuracy curves for a shallow ResNet (same depthas the starting model) and a deep ResNet (same depth as the final model).
