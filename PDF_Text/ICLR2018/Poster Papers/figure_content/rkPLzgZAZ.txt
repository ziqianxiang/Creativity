Figure 1: Modular continual learning in the TouchStream environment The TouchStream environment is atouchscreen-like GUI for continual learning agents, in which a spectrum of visual reasoning tasks can be posedin a large but unified action space. On each timestep, the environment (cyan box) emits a visual image (xt) and areward (rt). The agent recieves xt and rt as input and emits an action at . The action represents a “touch” atsome location on a two-dimensional screen e.g. at ∈ {0, . . . , H - 1} × {0, . . . , W - 1}, where H and W arethe screen height and width. The environment’s policy is a program computing xt and rt as a function of theagent’s action history. The agent’s goal is to learn how to choose optimal actions to maximize the amount ofreward it recieves over time. The agent consists of several component neural networks including a fixed visualbackbone (yellow inset), a set of learned neural modules (grey inset), and a meta-controller (red inset) whichmediates the deployment of these learned modules for task solving. The modules use the ReMaP algorithm§ 2 to learn how to estimate reward as a function of action (heatmap), conditional on the agent’s recent history.
Figure 2: Exemplar TouchStream tasks. Illustration of several task paradigms explored in this work using theTouchStream Environment. The top row depicts observation xt and the bottom shows the ground truth rewardmaps (with red indicating high reward and blue indicating low reward). a. Binary Stimulus-Response task. b.
Figure 3: Decision interfaces emerge naturally over the course of training. The ReMaP modules allow theagent to discover the implicit interfaces for each task. We observe that learning generally first captures theemergence of natural physical constructs before learning task-specific decision rules. Examples of this include:a. onscreen “buttons” appearing on the match screen of an MTS task before the specific semantic meaningof each button is learned (arrows indicate random motion), and b. the general discovery of objects and theirboundaries before the task-specific category rule is applied. This image is best viewed in color.
Figure 4: EMS modules as components of an efficient visual learning system. Validation reward obtainedover the course of training for modules on a. 4-way stimulus-response with a reward map split into fourquadrants, b. 2-way MTS with randomly moving match templates, c. 4-way MTS with two randomly movingclass templates shown at a time, and d. 4-way MTS with four randomly positioned images shown at a time.
Figure 5: Convolutional bottlenecks allow for fine resolution localization and detection in complex scenes.
Figure 6: The Dynamic Neural Voting Controller. Dynamic Neural Voting solves new tasks by computing acomposite execution graph through previously learned and newly allocated modules. Shown here is an agentwith two existing modules (yellow and green), as one newly allocated module is being learned (blue). For eachlayer, the controller takes as input the activations of all three modules and outputs a set of “voting results” —probabilistic weights to be used to scale activations within the corresponding components. Voting can be doneon either a per-layer basis or a per-unit basis, for clarity only the layer voting method is depicted. The weightedsum of these three scaled outputs is used as input to the next stage in the computation graph. If the new taskcan be solved through a combination of existing module components, these will be weighted highly, while thenew module will be effectively unused e.g. is assigned low weights. If however the task is quite different thanpreviously solved tasks, the new module will play a larger role in the execution graph as it learns to solve thetask.
Figure 7: Dyanmic Neural Voting quickly correctsfor “no-switch” switches. Although a new moduleis allocated for each task transition, if the new taskis identitcal to the original task, the controller quicklylearns to reuse the old module components. Top: post-switching learning curve for the EMS module on a binarystimulus-response task, after being trained on the sametask. For clarity, only the Layer Voting method is com-pared against a baseline module trained from scratch.
Figure S1: Exhaustive module performance study of the EMS module and 23 ablation control modules,measured as the Area Under the Curve for all SR, MTS, and LOC task variants. Shown is the AUC normalizedto the highest performing module in a task. Results in fig. 4 have further averaged this over the vertical task axis,and report only a salient subset of the ablations.
Figure S2:	Examples of the emergence of decision interfaces in MSCOCO MTS Reward map predictionsover the course of training for 5 different object classes.
Figure S3:	Additional Single-task performance ablation Learning curves. Seven learning curves shown fortask variants not seen in the main text body. Shown are the same ablations as the main text.
Figure S5: Action and reward map transformation switch examples. Three task switching experiments wereperformed to optimize the hyperparameters of the targeted transformations that augment the dynamic neuralvoting controller. These switches are also retested in the fully-integrated meta-controller and shown in theoriginal switching result figure. a. Binary stimulus-response class reversals, where the left class becomes theright class, and vice-versa. b. Rotations of the binary stimulus-response reward boundaries. c. A “squeezing” ofthe binary stimulus-response reward boundaries, where no reward is given on the new task on the bottom half ofthe screen, regardless of class shown.
Figure S6: Targeted controller transform ablation. Relative AUC Gain for the EMS module over the sameswitching snearios in the paper, but with the targeted transformations ablated.
Figure S7: Illustration of switching performance metrics. We quantify the switching performance of thedynamic neural controller and task-modules by two metrics: “relative gain in AUC” (ratio of green to purpleshaded regions), and “transfer” gain (difference of reward at T∆max). Relative AUC measures the overall gainrelative to scratch, and the transfer gain measures the speed of transfer. Curve shown is the EMS module withSingle-Unit voting method evaluated on a switch from a 4-way MTS task with two randomly moving classtemplates to a 4-way MTS task with four randomly moving templates.
