Figure 1: Visualization of experiment 1 training dynamics in two dimensions, demonstrated specifi-cally in the case where the model is initialized so that it it represents a linear manifold parallel tothe linear manifold of the training data. Here the GAN model (red points) converges upon the onedimensional synthetic data distribution (blue points). Specifically, this is an illustration of the parallelline thought experiment from (Arjovsky et al., 2017). When run in practice with a non-saturatingGAN, the GAN succeeds. In the same setting, minimization of Jensen-Shannon divergence would fail.
Figure 2: (Left) A recreation of Figure 2 of Arjovsky et al. (2017). This figure is used by Arjovskyet al. (2017) to show that a model they call the “traditional GAN” suffers from vanishing gradients inthe areas where D(x) is flat. This plot is correct if “traditional GAN” is used to refer to the minimaxGAN, but it does not apply to the non-saturating GAN. (Right) A plot of both generator losses fromthe original GAN paper, as a function of the generator output. Even when the model distributionis highly separated from the data distribution, non-saturating GANs are able to bring the modeldistribution closer to the data distribution because the loss function has strong gradient when thegenerator samples are far from the data samples, even when the discriminator itself has nearly zerogradient. While it is true that the 2 log(1 - D(X)) loss has a vanishing gradient on the right halfof the plot, the original GAN paper instead recommends implementing -1 log D(χ). This latter,recommended loss function has a vanishing gradient only on the left side of the plot. It makes sensefor the gradient to vanish on the left because generator samples in that area have already reached thearea where data samples lie.
Figure 3: Visualization of experiment 1 training dynamics in two dimensions. Here the GAN model(red points) converges upon the one dimensional synthetic data distribution (blue points). We notethat this is a visual illustration, and the results have not been averaged out over multiple seeds. Exactplots may vary on different runs. However, a single example of success is sufficient to refute claimsthat this this task is impossible for this model.
Figure 4: Visualization of experiment 2 training dynamics in two dimensions - where the GAN modelhas 3 latent variables. Here the rank one GAN model (red points) converges upon the one dimensionalsynthetic data distribution (blue points). We observe how for poor initialization the non-saturatingGAN suffers from mode collapse. However, adding a gradient penalty stabilizes training. We notethat this is a visual illustration, and the results have not been averaged out over multiple seeds. Exactplots may vary on different runs.
Figure 5: The square Frechet distance between the learned GaUSSian and the true GaUSSian distri-bution. For reference, we also plot the distance obtained by a randomly initialized generator with theSame architecture aS the trained generatorS. ReSultS are averaged over 1000 runS. Lower valueS arebetter.
Figure 6: ExampleS from the three dataSetS explored in thiS paper: Color MNIST (left), CIFAR-10(middle) and CelebA (right).
Figure 7: Negative Wasserstein distance estimated using an independent Wasserstein critic on the threedatasets we evaluate on. The metric captures overfitting to the training data and low quality samples.
Figure 8: Left plot shows sample diversity results on CelebA. It is important to look at this measurerelative to the measure on the test set: too much diversity can mean failure to capture the datadistribution, too little is indicative of mode collapse. To illustrate this, we report the diversity obtainedwhen adding normal noise with zero mean and 0.1 standard deviation to the test set: this results inmore diversity than the original data. The black dots report the results closest to the reference valuesobtained on the test set by each model. Middle plot: Inception Score results on CIFAR-10. Rightmost plot shows Inception Score computed using a VGG style network trained on CIFAR-10. Asa reference benchmark, we also compute these scores using samples from test data split; diversity:0.621, Inception Score: 11.25, Inception Score (VGG net trained on CIFAR-10): 9.18.
Figure 9: Synthetic Experiment 1. The square Frechet distance between the generated GaUssianparameters and true Gaussian parameters for different GAN variants, when varying the learning ratewhile keeping the input dimension fixed. Results averaged over 1000 runs. Lower values are better.
Figure 10: Synthetic Experiment 2. The square Frechet distance between the generated Gaussianparameters and true Gaussian parameters for different GAN variants, when varying the learning ratewhile keeping the input dimension fixed. Results averaged over 1000 runs. Lower values are better.
Figure 11: Synthetic Experiment 1. The square Frechet distance between the generated GaUssianparameters and true Gaussian parameters for different number of discriminator updates when trainingnon saturating GANs, with varying the learning rates. Results averaged over 1000 runs. Lower valuesare better.
Figure 12: Synthetic Experiment 2. The square Frechet distance between the generated Gaussianparameters and true Gaussian parameters for different number of discriminator updates when trainingnon saturating GANs, with varying the learning rates. Results averaged over 1000 runs. Lower valuesare better.
Figure 13: Synthetic Experiment 1. The square Frechet distance between the generated GaUssianparameters and true Gaussian parameters for different number of discriminator updates when trainingGAN-GP, with varying the learning rates. Results averaged over 1000 runs. Lower values are better.
Figure 14: Synthetic Experiment 2. The square Frechet distance between the generated Gaussianparameters and true Gaussian parameters for different number of discriminator updates when trainingGAN-GP, with varying the learning rates. Results averaged over 1000 runs. Lower values are better.
Figure 15: Comparison across models when doing one update for the discriminator in WassersteinGAN (WGAN-GP-1). The reduced performance in consistent with the observed decrease in samplequality when examining results. Inception Score results obtained on the test set: with Imagenettrained classifier: 11.25, With CIFAR-10 trained classifier: 9.18. Higher is better; the 10 black dotsrepresent the results obtained with the 10 best hyperparameter settings.
Figure 16: The metrics employed are able to capture mode collapse. Looking at the 5 worst values(the black dots) in a hyperparameter sweep according to sample diversity and negative Wassersteindistance as estimated by an Independent Wasserstein critic, we see that these metrics are able tocapture the two examples of model collapse that we have seen when training DRGAN-NS on CelebA,as shown in Figure 21. For sample diversity, the worst results are computed by the biggest absolutedifference to the reference point (test set diversity), while for negative Wasserstein distance the worstresults are computed by choosing the lowest value.
Figure 17:	Examples of mode collapse obtained for some hyperparameter settings with non-saturatingGAN.
Figure 18:	CIFAR-10 samples obtained from the GAN-GP, DRAGAN-NS, and WGAN-GP models.
Figure 19: CelebA samples obtained from the GAN-GP, DRAGAN-NS, and WGAN-GP models.
Figure 20: CMNIST samples obtained from the GAN-GP, DRAGAN-NS, and WGAN-GP models.
Figure 21: Mode collapse when adding gradient penalties to non-saturating GANs. GAN-GP onlyhad two instances of mode collapse, namely color mode collapse on Color-MNIST (left), whileDRAGAN-NS only had two instances of mode collapse, which ocurred when trained on CelebA(right and middle).
Figure 22: Examples of failure to capture the data distribution with WGAN-GP. The model puts toomuch mass around the data distribution when trained on the CelebA dataset.
