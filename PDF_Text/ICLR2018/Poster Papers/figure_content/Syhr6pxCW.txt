Figure 1: Our approach generates photorealistic output for various “incomplete" signals such asa low resolution image, a surface normal map, and edges/boundaries for human faces, cats, dogs,shoes, and handbags. Importantly, our approach can easily generate multiple outputs for a given in-put which was not possible in previous approaches (Isola et al., 2016) due to mode-collapse problem.
Figure 2: Mode collapse problem for GANs: We ran pix-to-pix pipeline of Isola et al. (2016) 72times. Despite the random noise set using dropout at test time, We observe similar output generatedeach time. Here We try to show 6 possible diverse examples of generation for a hand-picked best-looking output from Isola et al. (2016).
Figure 3: Overview of pipeline: Our approach is a two-stage pipeline. The first stage directlyregresses an image from an incomplete input (using a CNN trained with l2 loss). This image willtend to look like a “smoothed” average of all the potential images that could be generated. In thesecond stage, We look for matching pixels in similarly-smoothed training images. Importantly, Wematch pixels using multiscale descriptors that capture the appropriate levels of context (such thateye pixels tend to match only to eyes). To do so, We make use of off-the-shelf hypercolumn featuresextracted from a CNN trained for semantic pixel segmentation. By varying the size of the matchedset of pixels, we can generate multiple outputs (on the right).
Figure 4: Frequency Analysis: We show the image and its corresponding Fourier spectrum. Notehow the frequency spectrum improve as We move from left to right. The Fourier spectrum of ourfinal output closely matches that of original high resolution image.
Figure 5: Global vs. Compositional: Given the low-resolution input images on the left, we showhigh-frequency output obtained with a global nearest neighbor versus a compositional reconstruc-tion. We visualize the correspondences associated with the compositional reconstruction on theright. We surround the reconstruction with 8 neighboring training examples, and color code pixelsto denote correspondences. For example, when reconstructing the female face, forehead pixels arecopied from the top-left neighbor (orange), while right-eye pixels are copied from the bottom-leftneighbor (green).
Figure 6: Edges/NormalS to RGB:OUr approach used for faces, cats, and dogs to generate RGBmaps for a given edge/normal map as input. One output was picked from the multiple generations.
Figure 7: Low-Resolution to High-Resolution: We used our approach for hallucinating 96 × 96 im-ages from an input 12 × 12 low-resolution image. One output was picked from multiple generations.
Figure 8: Edges-to-Shoes: Our approach used to generate multiple outputs of shoes from the edges.
Figure 10: Multiple Outputs for Edges/Normals to RGB: Our approach used to generate multipleoutputs of faces, cats, and dogs from the edges/normals. AS an example, note how the subtle detailsSUCh as eyes, stripes, and whiskers of cat (left) that could not be inferred from the edge map aredifferent in multiple generations.
Figure 11: Comparison of our approach with Pix-to-Pix (Isola et al., 2016).
Figure 12: Controllable synthesis: We generate the output of cats given a user input from a edgemap. From the edge map, We do not know What type of cat it is. A user can suggest What kind ofthe output they would like, and our approach can copy-paste the information.
Figure 13: Failure Cases: We show some failure cases for different input types. Our approachmostly fails when it is not able to find suitable nearest neighbors.
