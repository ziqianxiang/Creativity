Figure 1: 2160 networks trained to 100% training accuracy on CIFAR10 (see §A.5.5 for experimen-tal details). Left: while increasing capacity of the model allows for overfitting (top), very few modelsdo, and a model with the maximum parameter count yields the best generalization (bottom right).
Figure 2: A 100%-accurate (on training data) MNIST network implements a function that is muchmore stable near training data than away from it. Left: depiction of a hypothetical circular trajec-tory in input space passing through three digits of different classes, highlighting the training pointlocations (π∕3, π, 5π∕3). Center: Jacobian norm as the input traverses an elliptical trajectory. Sen-sitivity drops significantly in the vicinity of training data while remaining uniform along randomellipses. Right: transition density behaves analogously. According to both metrics, as the inputmoves between points of different classes, the function becomes less stable than when it movesbetween points of the same class. This is consistent with the intuition that linear combinations ofdifferent digits lie further from the data manifold than those of same-class digits (which need nothold for more complex datasets). See §A.5.2 for experimental details.
Figure 3: Transition boundaries of the last (pre-logits) layer over a 2-dimensional slice throughthe input space defined by 3 training points (indicated by inset squares). Left: boundaries beforetraining. Right: after training, transition boundaries become highly non-isotropic, with trainingpoints lying in regions of lower transition density. See §A.5.3 for experimental details.
Figure 4: Improvement in generalization (left column) due to using correct labels, data augmenta-tion, ReLUs, mini-batch optimization (top to bottom) is consistently coupled with reduced sensi-tivity as measured by the Jacobian norm (center column). Transitions (right column) correlate withgeneralization in all considered scenarios except for comparing optimizers (bottom right). Eachpoint on the plot corresponds to two neural networks that share all hyper-parameters and the sameoptimization procedure, but differ in a certain property as indicated by axes titles. The coordinatesalong each axis reflect the values of the quantity in the title of the plot in the respective setting (i.e.
Figure 5:	Jacobian norm correlates with generalization gap on all considered datasets. Each pointcorresponds to a network trained to 100% training accuracy (or at least 99.9% in the case of CI-FAR100). See §A.5.4 and §A.5.5 for experimental details of bottom and top plots respectively.
Figure 6:	Jacobian norm plotted against individual test point loss. Each plot shows 5 random net-works that fit the respective training set with 100% accuracy, with each network having a uniquecolor. Top: Jacobian norm plotted against cross-entropy loss. These plots experimentally confirmthe relationship established in §A.3 and Figure App.11. Bottom: Jacobian norm plotted against'2-loss, for networks trained on '2-loss, exhibits a similar behavior. See §A.5.6 for experimentaldetails and Figure App.9 for similar observations on other datasets. 55 ConclusionWe have investigated sensitivity of trained neural networks through the input-output Jacobian normand linear regions counting in the context of image classification tasks. We have presented extensiveexperimental evidence indicating that the local geometry of the trained function as captured bythe input-output Jacobian can be predictive of generalization in many different contexts, and that itvaries drastically depending on how close to the training data manifold the function is evaluated. Wefurther established a connection between the cross-entropy loss and the Jacobian norm, indicatingthat it can remain informative of generalization even at the level of individual test points. Interestingdirections for future work include extending our investigation to more complex architectures andother machine learning tasks.
