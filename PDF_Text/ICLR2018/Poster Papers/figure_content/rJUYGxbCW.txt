Figure 1: An image sampled from the CIFAR-10 test dataset and various adversarial examplesgenerated from it. The text above shows the attacking method while the text below shows thepredicted label of the ResNet.
Figure 2: (a) Likelihoods of different perturbed images with attack = 8. (b) Test errors of a ResNeton different adversarial examples.
Figure 3: The distribution of p-values under the PixelCNN generative model. The inputs are moreoutside of the training distribution if their p-value distribution has a larger deviation from uniform.
Figure 4: An example of how purification works. The above row shows an image from CIFAR-10 test set and various attacking images generated from it. The bottom row shows correspondingpurified images. The text below each image is the predicted label given by our ResNet.
Figure 5: The bits-per-dimension distributions of purified images from FGSM adversarial eXamples.
Figure 6: ROC curves showing the efficacy of using p-values as scores to detect adversarial exam-ples. For computing the ROC, we assign negative labels to training images and positive labels toadversarial images (or clean test images). (a) Original adversarial examples. (b) Purified adversarialexamples after PixelDefend.
Figure 7: The distributions of p-values under the PixelCNN model after PixelDefend purification.
Figure 8: True and generated images from Fashion MNIST. The upper part shows true imagessampled from the dataset while the bottom shows generated images from PixelCNN.
Figure 9: True and generated images from CIFAR-10. The upper part shows true images sampledfrom the dataset while the bottom part shows generated images from PixelCNN.
Figure 10: The upper part shows adversarial images generated from FGSM attack while the bottompart shows corresponding purified images after PixelDefend. Here attack = 25 and defend = 32.
Figure 11: The upper part shows adversarial images generated from FGSM attack while the bottompart shows corresponding purified images by PixelDefend. Here attack = 8 and defend = 16.
