Figure 1: Our error-rate gaps between using full-precision and 1-bit-per-weight. All pointsexcept black crosses are data from some of our best results reported in this paper for each dataset.
Figure 2: Difference between our full-precision and 1-bit-per-weight networks. The “1 bit convand “scale” layers are equivalent to the operations shown in Eqn. (1).
Figure 3: Wide ResNet architecture. The design is mostly a standard pre-activation ResNet (Heet al., 2016; Zagoruyko & Komodakis, 2016). The first (stand-alone) convolutional layer (“conv”)and first 2 or 3 residual blocks have 64k (ImageNet) or 16k (other datasets) output channels. Thenext 2 or 3 blocks have 128k or 32k output channels and so on, where k is the widening parame-ter. The final (stand-alone) convolutional layer is a 1 × 1 convolutional layer that gives N outputchannels, where N is the number of classes. Importantly, this final convolutional layer is followedby batch-normalization (“BN”) prior to global-average-pooling (“GAP”) and softmax (“SM”). Theblocks where the number of channels double are downsampling blocks (details are depicted in Fig-ure 4) that reduce each spatial dimension in the feature map by a factor of two. The rectified-linear-unit (“RELU”) layer closest to the input is optional, but when included, it is best to learn the BNscale and offset in the subsequent layer.
Figure 4: Downsampling blocks in Wide ResNet architecture. As in a standard pre-activationResNet (He et al., 2016; Zagoruyko & Komodakis, 2016), downsampling (stride-2 convolution)is used in the convolutional layers where the number of output channels increases. The corre-sponding downsampling for skip connections is done in the same residual block. Unlike standardpre-activation ResNets we use an average pooling layer (“avg pool” ) in the residual path whendownsampling.
Figure 5:	Convergence through training. Left: Each marker shows the error rates on the test setand the training set at the end of each cycle of the warm-restart training method, for 20-4 ResNets(less than 5 million parameters). Right: each marker shows the test error rate for 20-10 ResNets,with and without cutout. C10 indicates CIFAR-10, and C100 indicates CIFAR-100.
Figure 6:	Influence of warm-restart and not learning BN gain and offset. Left: full-precisioncase. Right: 1-bit-per-weight case.
Figure 7: Residual networks compared with all-convolutional networks. The data in this figureis for networks with width 4×, i.e. with about 4.3 million learned parameters.
