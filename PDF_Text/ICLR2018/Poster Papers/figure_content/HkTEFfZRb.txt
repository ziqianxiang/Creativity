Figure 1: Blocks used in binary convolution architecture.
Figure 2: Accuracy of full-precision (A), binary (B), and scaled binary (C), models subject to tar-geted Carlini-Wagner L2 attacks of increasing strength on MNIST dataset. Models A/B256+ andA/C64+ were trained with 20 and 40 iterations of PGD, respectively.
Figure 3:	Decision surface for a three layer MLP with two hidden units in first two layers, andsigmoid output neuron [(a) and (c)]. Corresponding forward derivative with respect to input x2 [(b)and (d)]. Full-precision model [(a) and (b)] and model with a binarized hidden layer [(c) and (d)].
Figure 4:	(a) Decision surface for a 3 layer MLP with four hidden units in first two layers, one outputneuron, and quantized middle layer. (b) Corresponding forward derivative.
Figure 5: We reproduce the plot from (Goodfellow et al., 2015) by evaluating the logits of non-scaled binary [(a) and (b)] and full-precision [(c) and (d)] neural networks for an MNIST digit withvarying degrees of FGSM perturbation. Note that the true class of the digit is “1” in this instance.
