Figure 1: Memory footprint of activations(ACTs) and weights (W) during inference formini-batch sizes 1 and 8.
Figure 2: Schematic of the knowledge distillation setup. The teacher network is a high precision network andthe apprentice network is a low-precision network.
Figure 3: Difference in Top-1 error rate for low-precision variants of ResNet-18 with (blue bars) andwithout (red bars) distillation scheme. The differ-ence is calculated from the accuracy of ResNet-18with full-precision numerics. Higher % differencedenotes a better network configuration.
Figure 4: Difference in Top-1 error rate for low-precision variants of ResNet-34 and ResNet-50 with (bluebars) and without (red bars) distillation scheme. The difference is calculated from the accuracy of the baselinenetwork (ResNet-34 for (a) and ResNet-50 for (b)) operating at full-precision. Higher % difference denotes abetter network configuration.
Figure 5: Top-1 error rate versus epochs of four student networks using scheme-A and scheme-B.
Figure 6: Comparison of various configurations of ResNet on CIFAR-10 with and without Apprenticescheme.
Figure 7: Distillation followed by quantization.
