Figure 1: The mapping of work onto the GPU in a persistent approach; one row is processed by asingle warp.
Figure 2: Different algorithms processing a pruned recurrent layer with a variety of workloads withthe following baseline configuration: density of 10%, layer size of 1792, batch size of 4, and 256timesteps (emphasized in each subplot). Subplot a) varies the density, b) varies the layer size, c)varies batch size, and d) varies the number of timesteps. Annotated values show the speedup of ourtechnique over the next-best algorithm.
Figure 3:	Relative performance of algorithms processing a pruned RNN with a large layers. Commonparameters are: batch size=4, timesteps=256.
Figure 4: Various network sizes with a fixed number of nonzero parameters; i.e., larger layers aresparser. Note the log scale on the vertical axis.
Figure 5: Training with sparsity can yield a higher accuracy for a given number of nonzero parameters(â€œEffective Layer Size").
Figure 6: Our efficient algorithm can be used on pruned workloads; for a given performance target,pruned networks using a sparse persistent approach provide the best BLEU score. Likewise, for agiven network accuracy, a pruned network accelerated with our algorithm gives the highest throughput.
Figure 7: Various algorithms applied to different Deep Speech 2 models; for an aggressive per-formance target, pruned networks using a sparse persistent approach provide the lowest error rate.
