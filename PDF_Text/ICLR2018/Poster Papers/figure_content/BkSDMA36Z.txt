Figure 1: Architectures of region embedding using local context units in different perspectives3.1	Local context unitIn natural language processing, words are commonly converted to low dimensional vectors(wordembeddings) as the inputs to neural networks. More formally, the embedding ew of word w is3Published as a conference paper at ICLR 2018represented by a column in a matrix E ∈ Rh×v with a look up layer, where v is the size of thevocabulary, h is the embedding size.
Figure 2: Effect of the hyperparameters (region size and embedding size) on Yelp Review Fulldataset. (a) shows the comparison of single fixed region size 7 and multi sizes combination [3,5,7]and (b) shows the effect of different settings of embedding size among four kinds of models, unigramFastText, bigram FastText, CNN and ours. We use region size 7 for CNN and ours.
Figure 3: Heat maps of chosen words trained on Yelp Review Polarity, which is a binary sentimentanalysis dataset. Each row represent the context unit of the middle word. Region size is 7 andembedding size is 128.
