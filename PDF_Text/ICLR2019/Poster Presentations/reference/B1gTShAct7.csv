title,year,conference
 A theory for how sen-sorimotor skills are learned and retained in noisy and nonstationary neural circuits,2013, Proceedingsofthe NationalAcademyofSciences
 Expert gate: Lifelong learning with a networkofexperts,2017, In Proceedings CVPR 2017
 The option-critic architecture,2017, 2017
 Conditional computationin neural networks for faster models,2015, arXiv preprint arXiv:1511
 A massively parallel architecture for a self-organizingneural pattern recognition machine,1987, Computer vision
 Multitask learning,1997, Machine Learning
 Rieman-nian walk for incremental learning: Understanding forgetting and intransigence,2018, arXiv preprintarXiv:1801
 Pathnet: Evolution channels gradient descent in superneural networks,2017, arXiv preprint arXiv:1701
 Meta-learning and universality: Deep representations and gradientdescent can approximate any learning algorithm,2017, arXiv preprint arXiv:1710
 Model-agnostic meta-learning for fast adaptationof deep networks,2017, arXiv preprint arXiv:1703
 Catastrophic forgetting in connectionist networks,1999, Trends in cognitive sciences
 Neuron clustering for mitigating catastrophic forgetting in supervisedand reinforcement learning,2015, 2015
 Neural networks for machine learninglecture 6a overview of mini-batch gradient descent,2012, 2012
 Adaptive mixtures oflocal experts,1991, Neural computation
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Overcom-ing catastrophic forgetting in neural networks,2017, Proceedings of the National Academy of Sciences
 A memory frontier for complex synapses,2013, In Advances inneural information processing systems
 One shot learning ofsimple visual concepts,2011, In Proceedings of the Cognitive Science Society
 Lifelong learning with dynamicallyexpandable networks,2018, ICLR
 Overcomingcatastrophic forgetting by incremental moment matching,2017, In Advances in Neural InformationProcessing Systems
 Learning without forgetting,2016, In European Conference on ComputerVision
 Learn-ing robust options,2018, arXiv preprint arXiv:1802
 Why there are complementarylearning systems in the hippocampus and neocortex: insights from the successes and failures ofconnectionist models of learning and memory,1995, Psychological review
 Catastrophic interference in connectionist networks: Thesequential learning problem,1989, Psychology of learning and motivation
 Cross-stitch networks formulti-task learning,2016, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
 Human-levelcontrol through deep reinforcement learning,2015, Nature
 Learning and categorization in modular neural networks,1992, Lawrence ErlbaumAssociates
 Reptile: a scalable metalearning algorithm,2018, arXiv preprintarXiv:1803
 Optimization as a model for few-shot learning,2016, 2016
 icarl: Incremental clas-sifier and representation learning,2017, CVPR
 A deep learning and knowledge transferbased architecture for social media user characteristic determination,2015, In Proceedings of the thirdInternational Workshop on Natural Language Processingfor Social Media
 Representation stability as a regularizerfor improved text analytics transfer learning,2016, arXiv preprint arXiv:1704
 Generative knowl-edge distillation for general purpose function compression,2017, NIPS 2017 Workshop on TeachingMachines
 Scalable recollectionsfor continual lifelong learning,2017, arXiv preprint arXiv:1711
 Learning abstract options,2018, NIPS
 Continual learning in reinforcement environments,1994, PhD thesis
 Routing networks: Adaptive selection ofnon-linear functions for multi-task learning,2018, ICLR
 Optimal ordered problem solver,2004, Machine Learning
 Overcoming catastrophicforgetting with hard attention to the task,2018, arXiv preprint arXiv:1801
 Matching networks for oneshot learning,2016, In Advances in Neural Information Processing Systems
 Random sampling with a reservoir,1985, ACM Transactions on Mathematical Software(TOMS) 
 Deep multi-task representation learning: A tensor factori-sation approach,2017, ICLR
 A deeper look at experience replay,2017, arXiv preprintarXiv:1712
 A DQN with MERachieves approximately the asymptotic performance for the single task DQN by the end of trainingfor most tasks,2005, On the other hand
