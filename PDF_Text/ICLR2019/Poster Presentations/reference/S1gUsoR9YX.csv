title,year,conference
 Large scale distributed neural network training through online distillation,2018, arXiv preprintarXiv:1804
 Neural machine translation by jointlylearning to align and translate,2015, ICLR 2015
 Entropy-sgd: Biasing gradientdescent into wide valleys,2016, arXiv preprint arXiv:1611
 Ensemble distillation for neural ma-chine translation,2017, arXiv preprint arXiv:1702
 Convolutionalsequence to sequence learning,2017, In Proceedings of the 34th International Conference on MachineLearning
 Sentence-wise smooth regularization for sequenceto sequence learning,2018, In AAAI
 Non-autoregressive neuralmachine translation with enhanced decoder input,2018, In AAAI
 Toward multilingual neural machinetranslation with universal encoder and decoder,2016, CoRR
 Achieving human parityon automatic chinese to english news translation,2018, CoRR
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Googleâ€™s multilingual neural machine translation system: Enabling zero-shot transla-tion,2017, TACL
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Sequence-level knowledge distillation,2016, In Proceedings of the2016 Conference on Empirical Methods in Natural Language Processing
 Sequence-level knowledge distillation,2016, arXiv preprintarXiv:1606
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Learning fromnoisy labels with distillation,2017, In ICCV
 Aneural interlingua for multilingual machine translation,2018, CoRR
 Multi-tasksequence to sequence learning,2015, CoRR
 Bleu: a method for automaticevaluation of machine translation,2002, In Proceedings of the 40th Annual Meeting of the Associationfor Computational Linguistics
 Fitnets: Hints for thin deep nets,2014, arXiv preprint arXiv:1412
 Neural machine translation of rare words withsubword units,2016, In ACL 2016
 Dense information flow for neural machinetranslation,2018, In NAACL
 Double path networks forsequence to sequence learning,2018, In COLING
 Attention is all you need,2017, In NIPS 2017
 Beyond errorpropagation in neural machine translation: Characteristics of language also matter,2018, In EMNLP
 Knowledge distillation in generations:More tolerant teachers educate better students,2018, arXiv preprint arXiv:1805
 Deep mutual learning,2017, arXivpreprint arXiv:1706
