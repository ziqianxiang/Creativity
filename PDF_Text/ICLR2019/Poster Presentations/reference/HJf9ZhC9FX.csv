title,year,conference
 On the emergence of invariance and disentangling in deeprepresentations,2017, arXiv preprint arXiv:1706
 A characterization of stochastic mirror descent algorithms andtheir convergence properties,2019, In IEEE International Conference on Acoustics
 Mirror descent and nonlinear projected subgradient methods forconvex optimization,2003, Operations Research Letters
 The robustness of the p-norm algorithms,2003, Machine Learning
 General convergence results for lineardiscriminant updates,2001, Machine Learning
 Characterizing implicit bias interms of optimization geometry,2018, arXiv preprint arXiv:1802
 Implicit bias of gradient descenton linear convolutional networks,2018, arXiv preprint arXiv:1806
 Hoo optimal training algorithms and their relation to backprop-agation,1995, In Advances in Neural Information Processing Systems 7
 Hoo optimality criteria for LMS and backprop-agation,1994, In Advances in Neural Information Processing Systems 6
 Hoo optimality of the LMS algorithm,1996, IEEETransactions on Signal Processing
 Deep learning without poor local minima,2016, In Advances in Neural InformationProcessing Systems
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 The p-norm generalization of the LMS al-gorithm for adaptive filtering,2006, IEEE Transactions on Signal Processing
 Imagenet classification with deep convo-lutional neural networks,2012, In Advances in Neural Information Processing Systems
 Deep learning,2015, Nature
 Gradient descent onlyconverges to minimizers,2016, In Conference on Learning Theory
 Human-levelcontrol through deep reinforcement learning,2015, Nature
 Problem complexity andmethod efficiency in optimization,1983, 1983
 Geometry of opti-mization and implicit regularization in deep learning,2017, arXiv preprint arXiv:1705
 Online learning and online convex optimization,1935, Foundations and Trends inMachine Learning
 Opening the black box of deep neural networks via informa-tion,2017, arXiv preprint arXiv:1703
 Masteringthe game of go with deep neural networks and tree search,2016, Nature
 Theoretical insights into the optimiza-tion landscape of over-parameterized shallow neural networks,2017, arXiv preprint arXiv:1707
 The im-plicit bias of gradient descent on separable data,2017, arXiv preprint arXiv:1710
 The marginalvalue of adaptive gradient methods in machine learning,2017, In Advances in Neural InformationProcessing Systems
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
