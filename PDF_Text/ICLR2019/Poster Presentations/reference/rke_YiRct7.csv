title,year,conference
 A convergence theory for deep learning via over-parameterization,2018, arXiv preprint arXiv:1811
 Convexneural networks,2006, In Advances in neural information processing systems
 Globally optimal gradient descent for a ConvNet with gaussianinputs,2017, In International Conference on Machine Learning
 SGD learns over-parameterized networks that provably generalize on linearly separable data,2018, In InternationalConference on Learning Representations
 Theloss surfaces of multilayer networks,2015, In Artificial Intelligence and Statistics
 Gradient descent finds globalminima of deep neural networks,2018, arXiv preprint arXiv:1811
 Gradient descent provably optimizesover-parameterized neural networks,2018, arXiv preprint arXiv:1810
 Global optimality in neural network training,2017, In Proceedingsofthe IEEE Conference on Computer Vision and Pattern Recognition
 Deep learning without poor local minima,2016, In Advances in Neural InformationProcessing Systems
 Self-normalizingneural networks,2017, In Advances in Neural Information Processing Systems
 The multilinear structure of ReLU networks,2018, In InternationalConference on Machine Learning
 Deep linear networks with arbitrary loss: All local minima areglobal,2018, In International Conference on Machine Learning
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, In Advances in Neural Information Processing Systems
 Convergence analysis of two-layer neural networks with ReLU activa-tion,2017, In Advances in Neural Information Processing Systems
 Adding one neuron can eliminate all badlocal minima,2018, In Advances in Neural Information Processing Systems
 Depth creates no bad local minima,2017, arXiv preprintarXiv:1702
 Optimization landscape and expressivity of deep CNNs,2018, InInternational Conference on Machine Learning
 Spurious local minima are common in two-layer ReLU neural net-works,2018, In International Conference on Machine Learning
 Learning ReLUs via gradient descent,2017, In Advances in Neural InformationProcessing Systems
 No bad local minima: Data independent training error guaranteesfor multilayer neural networks,2016, arXiv preprint arXiv:1605
 Local minima in training ofneural networks,2016, arXiv preprint arXiv:1611
 An analytical formula of population gradient for two-layered ReLU network and itsapplications in convergence and critical point analysis,2017, In International Conference on MachineLearning
 Neural networks with finite intrinsic dimensionhave no spurious valleys,2018, arXiv preprint arXiv:1802
 No spurious local minima in a two hidden unit ReLUnetwork,2018, In International Conference on Learning Representations Workshop
 Diverse neural network learns true target functions,2016, arXivpreprint arXiv:1611
 Learning one-hidden-layer ReLUnetworks via gradient descent,2018, arXiv preprint arXiv:1806
 Critical points of neural networks: Analytical forms and landscapeproperties,2018, In International Conference on Learning Representations
 Stochastic gradient descent optimizesover-parameterized deep ReLU networks,2018, arXiv preprint arXiv:1811
