title,year,conference
 A convergence theory for deep learning via over-parameterization,2018, arXiv preprint arXiv:1811
 Training a 3-node neural network is NP-complete,1988, In Proceedingsof the 1st International Conference on Neural Information Processing Systems
 Globally optimal gradient descent for a ConvNet with gaussianinputs,2017, In International Conference on Machine Learning
 SGD learns over-parameterized networks that provably generalize on linearly separable data,2018, In InternationalConference on Learning Representations
 Accelerated methods for non-convexoptimization,2016, arXiv preprint arXiv:1611
 Gradient descent finds globalminima of deep neural networks,2018, arXiv preprint arXiv:1811
 Gradient descent provably optimizesover-parameterized neural networks,2018, arXiv preprint arXiv:1810
 A variational approach to copositive matrices,2010, SIAM review
 Deep learning without poor local minima,2016, In Advances in Neural InformationProcessing Systems
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 The multilinear structure of ReLU networks,2018, In InternationalConference on Machine Learning
 Gradient descent onlyconverges to minimizers,2016, In Conference on Learning Theory
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, In Advances in Neural Information Processing Systems
 Convergence analysis of two-layer neural networks with ReLU activa-tion,2017, In Advances in Neural Information Processing Systems
 Copositive matrices and definiteness of quadraticforms subject to homogeneous linear inequality constraints,1981, Linear Algebra and its Applications
 Some NP-complete problems in quadratic and nonlinearprogramming,1987, Mathematical programming
 Optimization landscape and expressivity of deep CNNs,2018, InInternational Conference on Machine Learning
 A generic approach for escaping saddle points,2017, arXiv preprintarXiv:1709
 Spurious local minima are common in two-layer ReLU neural net-works,2018, In International Conference on Machine Learning
 Eigenvalue analysis of equilibrium processes defined by linear complementarityconditions,1999, Linear Algebra and its Applications
 Learning ReLUs via gradient descent,2017, In Advances in Neural InformationProcessing Systems
 No bad local minima: Data independent training error guaranteesfor multilayer neural networks,2016, arXiv preprint arXiv:1605
 An analytical formula of population gradient for two-layered ReLU network and itsapplications in convergence and critical point analysis,2017, In International Conference on MachineLearning
 No spurious local minima in a two hidden unit ReLUnetwork,2018, In International Conference on Learning Representations Workshop
 Small nonlinearities in activation functions create badlocal minima in neural networks,2019, In International Conference on Learning Representations
 Learning one-hidden-layer ReLUnetworks via gradient descent,2018, arXiv preprint arXiv:1806
 Critical points of neural networks: Analytical forms and landscapeproperties,2018, In International Conference on Learning Representations
 Stochastic gradient descent optimizesover-parameterized deep ReLU networks,2018, arXiv preprint arXiv:1811
