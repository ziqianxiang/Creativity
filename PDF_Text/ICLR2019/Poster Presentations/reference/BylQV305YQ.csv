title,year,conference
 Revisiting distributed synchronousSGD,2016, ArXiv: 1604
 Exploitingbounded staleness to speed up big data analytics,2014, In 2014 USENIX Annual Technical Conference(USENIX ATC 14)
 Geeps: Scalabledeep learning on distributed gpus with a gpu-specialized parameter server,2016, In Proceedings of theEleventh European Conference on Computer Systems
 Petuum: A framework for iterative-convergent distributed ml,2013, arXiv preprintarXiv:1312
 Analysisof high-performance distributed ml at scale through parameter server consistency models,2015, InProceedings of the 29th AAAI Conference on Artificial Intelligence
 Large scale distributed deep networks,2012, In Advances inNeural Information Processing Systems
 Finding scientific topics,2004, PNAS
 Omnivore: An optimizerfor multi-device deep learning on cpus and gpus,2016, arXiv preprint arXiv:1606
 The movielens datasets: History and context,2016, ACMTransactions on Interactive Intelligent Systems (TiiS)
 Long short-term memory,1997, Neural computation
 Training neural networks using features replay,2018, arXivpreprint arXiv:1807
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, ArXiv:1609
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Learning multiple layers of features from tiny images,2009, 2009
 Fugue: Slow-worker-agnostic distributedlearning for big models on big data,2014, In Proceedings of the Seventeenth International ConferenceOnArtfiCial Intelligence and Statistics
 Slow learners are fast,2009, In Advances inNeural Information Processing Systems
 Scaling distributed machine learning with the parameterserver,2014, In OSDI
 Communication efficient distributed machinelearning with the parameter server,2014, In Proc
 Asynchronous parallel stochastic gradient fornonconvex optimization,2015, In Advances in Neural Information Processing Systems
 Visualizing deep network training trajectories with pca,2016, In The 33rd InternationalConference on Machine Learning JMLR volume
 Building a large annotatedcorpus of english: ThePenntreebank,1993, Computational linguistics
 Revisiting small batch training for deep neural networks,2018, ArXiv:1804
 Delay-tolerant algorithms for asynchronous distributedonline learning,2014, In Advances in Neural Information Processing Systems
 On the momentum term in gradient descent learning algorithms,1999, Neural Networks
 Hogwild: A lock-free approachto parallelizing stochastic gradient descent,2011, In NIPS
 Lightlda: Big topic models on modest computer clusters,2015, In Proceedings of the24th International Conference on World Wide Web
 Asynchronous distributed admm for consensus optimization,2014, InICML
 On convergence of model parallelproximal gradient algorithm for stale synchronous parallel system,2016, In The 19th InternationalConference on Artificial Intelligence and Statistics (AISTATS)
