title,year,conference
 Central limit theorems for interchangeableprocesses,1958, Canad
 Weight uncertainty inneural network,2015, In International Conference on Machine Learning
 Neural ordinary differentialequations,2018, arXiv preprint arXiv:1806
 Deep gaussian processes,2013, In Carlos M
 Dropout as a bayesian approximation: Representing modeluncertainty in deep learning,2015, arXiv preprint arXiv:1506
 Idealised bayesian neural networks cannot have adversarial examples:Theoretical and empirical study,2018, arXiv preprint arXiv:1806
 Identity mappings in deep residualnetworks,2016, In European conference on computer vision
 Robust multi-class gaussian process classification,2011, In J
 Imagenet classification with deep convo-lutional neural networks,2012, In Advances in neural information processing systems
 Deep gaussian processes withconvolutional kernels,2018, arXiv preprint arXiv:1806
 Simple and scalable predictiveuncertainty estimation using deep ensembles,2017, In Advances in Neural Information ProcessingSystems
 Handwritten digit recognition with a back-propagation net-work,1990, In Advances in neural information processing systems
 Deep neural networks as gaussian processes,2017, arXiv preprint arXiv:1711
 Deep informationpropagation,2016, arXiv preprint arXiv:1611
 Practical bayesian optimization of machinelearning algorithms,2012, In Advances in neural information processing systems
 Intriguing properties of neural networks,2013, arXiv preprint arXiv:1312
 Convolutional gaussian pro-cesses,2017, In Advances in Neural Information Processing Systems
 Deep kernellearning,2016, In Arthur Gretton and Christian C
 Mean field residual networks: On the edge of chaos,2017, In Advancesin neural information processing systems
 Wide residual networks,2016, arXiv preprintarXiv:1605
