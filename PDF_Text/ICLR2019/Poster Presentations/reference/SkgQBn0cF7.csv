title,year,conference
 Lipschitz continuity in model-based rein-forcement learning,2018, arXiv preprint arXiv:1804
 Robot learning from demonstration,1997, In ICML
 The option-critic architecture,2017, In AAAI
 Learning stochastic recurrent networks,2014, arXiv preprintarXiv:1411
 Generating sentences from a continuous space,2015, arXiv preprint arXiv:1511
 Sample-efficient reinforcement learning with stochastic ensemble value expansion,2018, arXiv preprintarXiv:1807
 Learning and queryingfast generative models for reinforcement learning,2018, arXiv preprint arXiv:1802
 Variational lossy autoencoder,2016, arXiv preprint arXiv:1611
 Recurrent environmentsimulators,2017, arXiv preprint arXiv:1704
 A recurrent latent variable model for sequential data,2015, In Advances in neural informationprocessing systems
 Self-consistent trajectory autoencoder: Hierarchical reinforcement learning with trajec-tory embeddings,2018, arXiv preprint arXiv:1806
 Sequential neural modelswith stochastic layers,2016, In Advances in neural information processing systems
 Z-forcing: Training stochastic recurrent networks,2017, In Advances in Neural InformationProcessing Systems
 Pixelvae: A latent variable model for natural images,2016, arXiv preprintarXiv:1611
 Generating sentences byediting PrototyPes,2018, Transactions of the Association of Computational Linguistics
 Recurrent world models facilitate policy evolution,2018, arXivpreprint arXiv:1809
 Long short-term memory,1997, Neural computation
 The effect of planning shape on dyna-style Planning in high-dimensional state sPaces,2018, arXiv preprint arXiv:1806
 Vime:Variational information maximizing exPloration,2016, In Advances in Neural Information ProcessingSystems
 Reinforcement learning with unsuPervised auxiliary tasks,2016, arXivpreprint arXiv:1611
 DeeP varia-tional bayes filters: UnsuPervised learning of state sPace models from raw data,2016, arXiv preprintarXiv:1605
 Adam: A method for stochastic oPtimization,2014, arXiv preprintarXiv:1412
 Auto-encoding variational bayes,2013, arXiv preprintarXiv:1312
 DeeP kalman filters,2015, arXiv preprintarXiv:1511
 Professor forcing: A new algorithm for training recurrent net-works,2016, In Advances In Neural Information Processing Systems
 Constrainedmodel Predictive control: Stability and oPtimality,2000, Automatica
 Recurrentneural network based language model,2010, In Eleventh Annual Conference of the International SpeechCommunication Association
 Prediction and control with temPoral segmentmodels,2017, arXiv preprint arXiv:1703
 Human-levelcontrol through deeP reinforcement learning,2015, Nature
 Asynchronous methods for deeP reinforcementlearning,2016, In International conference on machine learning
 Dimension reduction and its aPPlication to model-based exPlo-ration in continuous sPaces,2010, Machine Learning
 Value prediction network,2017, In Advances in NeuralInformation Processing Systems
 Curiosity-driven explorationby self-supervised prediction,2017, In International Conference on Machine Learning (ICML)
 Stochastic backpropagation andapproximate inference in deep generative models,2014, arXiv preprint arXiv:1401
 Trust regionpolicy optimization,2015, In International Conference on Machine Learning
 Proximal policyoptimization algorithms,2017, arXiv preprint arXiv:1707
 Loss is its own reward: Self-supervision for reinforcement learning,2016, arXiv preprint arXiv:1612
 Masteringthe game of go with deep neural networks and tree search,2016, nature
 Applications of the self-organising map to reinforcement learning,2002, Neuralnetworks
 Incentivizing exploration in reinforcementlearning with deep predictive models,2015, CoRR
 Reinforcement learning: An introduction,1998, MIT press
 Model regularization for stable sample rollouts,2014, In UAI
 Feudal networks for hierarchical reinforcement learning,2017, arXivpreprint arXiv:1703
 Imagination-augmented agents for deep reinforcement learning,2017, arXiv preprintarXiv:1707
