title,year,conference
 Convergence guarantees for rmsprop and adam in non-convexoptimization and their comparison to nesterov acceleration on autoencoders,2018, arXiv preprint arXiv:1807
 signsgd: compressed optimisation for non-convex problems,2018, arXiv preprint arXiv:1802
 Optimization methods for large-scale machine learning,2018, SIAM Review
 Convex optimization,2004, Cambridge university press
 Equilibrated adaptive learning rates for non-convex optimization,2015, InAdvances in neural information processing systems
 Incorporating nesterov momentum into adam,2016, 2016
 Global convergence of the heavy-ball method for convexoptimization,2015, In 2015 European Control Conference (ECC)
 Nostalgic adam: Weighing more of the past gradients when designing theadaptive learning rate,2018, arXiv preprint arXiv:1805
 Accelerated gradient descent escapes saddle points faster than gradientdescent,2017, arXiv preprint arXiv:1711
 Accelerating stochastic gradient descent using predictive variance reduction,2013, InAdvances in neural information processing systems
 Adam: A method for stochastic optimization,2014, arXiv preprint arXiv:1412
 Non-convex finite-sum optimization via scsg methods,2017, In Advances inNeural Information Processing Systems
 On the convergence of stochastic gradient descent with adaptive stepsizes,2018, arXiv preprintarXiv:1805
 Some methods of speeding up the convergence of iteration methods,1964, USSR ComputationalMathematics and Mathematical Physics
 On the convergence of adam and beyond,2018, In International Conference onLearning Representations
 Unified convergence analysis of stochastic momentum methods for convex andnon-convex optimization,2016, arXiv preprint arXiv:1604
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
 On the convergence of adaptive gradient methods for nonconvexoptimization,2018, arXiv preprint arXiv:1808
 On the convergence of adagrad with momentum for training deep neural networks,2018, arXivpreprint arXiv:1808
