title,year,conference
 Stronger generalization bounds fordeep nets via a compression approach,2018, In The 35th International Conference on Machine Learning
 Spectrally-normalized margin bounds forneural networks,2017, In Advances in Neural Information Processing Systems 30: Annual Conferenceon Neural Information Processing Systems 2017
 Size-independent sample complexity ofneural networks,2018, Computational Learning Theory
 A pac-bayesian margin bound for linear classifiers: Why svmswork,2000, In Advances in Neural Information Processing Systems 13
 Flat minima,1997, Neural Computation
 Learning overParameterized neural networks via stochastic gradientdescent on structured data,2018, In Advances in Neural Information Processing Systems 31: AnnualConference on Neural Information Processing Systems 2018
 Some Pac-bayesian theorems,1999, Machine Learning
 Generalization in deep networks: The role of distance frominitialization,2017, Deep Learning: Bridging Theory and Practice Workshop in Advances in NeuralInformation Processing Systems 30: Annual Conference on Neural Information Processing Systems2017
 In search of the real inductive bias:On the role of implicit regularization in deep learning,2015, International Conference on LearningRepresentations Workshop Track
 Exploring gen-eralization in deep learning,2017, Advances in Neural Information Processing Systems to appear
 A pac-bayesianapproach to spectrally-normalized margin bounds for neural networks,2018, International Conferenceon Learning Representations (ICLR)
 The implicit bias of gradient descent on separabledata,2018, International Conference on Learning Representations (ICLR)
 User-friendly tail bounds for sums of random matrices,2012, Foundations of ComputationalMathematics
 Understanding deeplearning requires rethinking generalization,2017, International Conference on Learning Representations(ICLR)
