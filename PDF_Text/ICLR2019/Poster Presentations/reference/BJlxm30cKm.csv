title,year,conference
 High-dimensional dynamics of generalization error inneural networks,2017, CoRR
 Scaling learning algorithms towards AI,2007, In Large Scale KernelMachines
 Curriculum learning,2009, InProceedings of the 26th annual international conference on machine learning
 Entropy-SGD: Biasing Gradi-ent Descent Into Wide Valleys,2016, ICLR â€™17
 Improved regularization of convolutional neural networkswith cutout,2017, arXiv preprint arXiv:1708
 Learning What Data to Learn,2017, 2017
 Model-agnostic meta-learning for fast adaptationof deep networks,2017, In Proc
 Flat minima,1997, Neural Computation
 Densely ConnectedConvolutional Networks,2017, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
 MentorNet: Learning data-driven curriculum for very deep neural networks on corrupted labels,2018, In Proceedings of the 35thInternational Conference on Machine Learning
 Not all samples are created equal: Deep learningwith importance sampling,2018, In Jennifer G
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Understanding black-box predictions via influence functions,2017, InDoina Precup and Yee Whye Teh (eds
 Learning multiple layers of features from tiny images,2009, 2009
 Self-Paced Learning for Latent VariableModels,2010, In Proc
 The mnist database of handwritten digits,1999, 1999
 Measuring the intrinsic dimensionof objective landscapes,2018, CoRR
 In search of the real inductive bias: Onthe role of implicit regularization in deep learning,2014, CoRR
 Deep learning generalizes be-cause the parameter-function map is biased towards simple functions,2018, CoRR
 Optimization as a model for few-shot learning,2017, In Proc
 Online Structured Laplace ApproximationsFor Overcoming Catastrophic Forgetting,2018, 2018
 Prioritized experience replay,2015, arXivpreprint arXiv:1511
 The Im-plicit Bias of Gradient Descent on Separable Data,2017, 2017
 Trainingconvolutional networks with noisy labels,2014, arXiv preprint arXiv:1406
 On the learning dynamics ofdeep neural networks,2018, 2018
 Identifying GeneralizationProperties in Neural Networks,2018, pp
 Convergence of sgd in learning relu models withseparable data,2018, CoRR
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
 Stochastic Optimization with Importance Sampling for RegularizedLoss Minimization,2015, In Proc
