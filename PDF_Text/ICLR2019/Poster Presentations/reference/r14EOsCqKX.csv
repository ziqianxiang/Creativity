title,year,conference
 Layer normalization,2016, arXiv preprintarXiv:1607
 Accel-erating asynchronous stochastic gradient descent for neural machine translation,2018, arXiv preprintarXiv:1808
 Sharp minima can generalizefor deep nets,2017, arXiv preprint arXiv:1703
 Essentially no barri-ers in neural network energy landscape,2018, arXiv preprint arXiv:1803
 Identity matters in deep learning,2016, arXiv preprint arXiv:1611
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 Three factors influencing minima in sgd,2017, arXiv preprintarXiv:1711
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Sgdr: stochastic gradient descent With restarts,2016, arXiv preprintarXiv:1608
 Theory ii: Landscape of the empirical risk in deep learning,2017, PhDthesis
 Svcca: Singular vectorcanonical correlation analysis for deep learning dynamics and interpretability,2017, In Advances inNeural Information Processing Systems
 Empirical analysis ofthe hessian of over-parametrized neural networks,2017, arXiv preprint arXiv:1706
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Cyclical learning rates for training neural networks,2017, In Applications of ComputerVision (WACV)
 The marginalvalue of adaptive gradient methods in machine learning,2017, In Advances in Neural InformationProcessing Systems
