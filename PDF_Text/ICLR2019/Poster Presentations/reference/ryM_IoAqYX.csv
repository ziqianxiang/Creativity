title,year,conference
 Sparse communication for distributed gradient descent,2017, In InternationalConference on Empirical Methods in Natural Language Processing
 QSGD: Communication-efficient SGDvia gradient quantization and encoding,2017, In Advances in Neural Information Processing Systems
 signSGD: Compressed optimisa-tion for non-convex problems,2018, In International Conference on Machine Learning
 Equilibrated adaptive learning rates for non-convex opti-mization,2015, In Advances in Neural Information Processing Systems
 High-accuracy low-precision training,2018, Preprint arXiv:1803
 Large scale distributed deep networks,2012, In Advances in NeuralInformation Processing Systems
 Network sketching: Exploiting binary structure in deepCNNs,2017, In International Conference on Computer Vision and Pattern Recognition
 Loss-aware binarization of deep networks,2017, In InternationalConference on Learning Representations
 Adam: A method for stochastic optimization,2015, In International Conference onLearning Representations
 Extremely low bit neural network: Squeeze the last bit out withadmm,2018, In AAAI Conference on Artificial Intelligence
 Scaling distributed machinelearning with the parameter server,2014, In USENIX Symposium on Operating Systems Design andImplementation
 Towards accurate binary convolutional neural network,2017, In Advancesin Neural Information Processing Systems
 Model compression via distillation and quantization,2018, InInternational Conference on Learning Representations
 Optimization of collective reduction operations,2004, In International Conference onComputational Science
 XNOR-Net: ImageNet classification usingbinary convolutional neural networks,2016, In European Conference on Computer Vision
 Stochastic variance reduction for nonconvexoptimization,2016, In International Conference on Machine Learning
 On the convergence of Adam and beyond,2018, In InternationalConference on Learning Representations
 1-bit stochastic gradient descent and application todata-parallel distributed training of speech DNNs,2014, In Interspeech
 Gradient sparsification for communication-efficient dis-tributed optimization,2017, Preprint arXiv:1710
 TernGrad: Ternary gradients to re-duce communication in distributed deep learning,2017, In Advances in Neural Information ProcessingSystems
 Training and inference with integers in deep neural networks,2018, InInternational Conference on Learning Representations
 DoReFa-Net: Training low bitwidth convolu-tional neural networks with low bitwidth gradients,2016, Preprint arXiv:1606
 Trained ternary quantization,2017, In International Conferenceon Learning Representations
