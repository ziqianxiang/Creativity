title,year,conference
 Accounting for the residual uncertainty of multi-layerperceptron based features,2014, In ICASSP
 Propagation of uncertainty through multi-layer perceptrons for robust automatic speech recognition,2011, In INTERSPEECH
 Robustness of classifiers:from adversarial to random noise,2016, In NIPS
 Chapter 19 gradient estimation,2006, In Shane G
 Bayesian convolutional neural networks with Bernoulli approx-imate variational inference,2015, arXiv:1506
 Assumed density filteringmethods for learning Bayesian neural networks,2016, In AAAI
 Monte Carlo methods in financial engineering,2004, Springer
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In AISTATS
 Probabilistic backpropagation for scalablelearning of Bayesian neural networks,2015, In ICML
 Uncertainty estimates and multi-hypotheses networks for optical flow,2018, In ECCV
 Batch renormalization: Towards reducing minibatch dependence in batch-normalizedmodels,2017, CoRR
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In ICML
 Variational dropout and the local reparame-terization trick,2015, In NIPS
 Gradient-based learning applied todocument recognition,2001, In Intelligent signal processing
 Multivariate logistic distributions,1973, The Annals of Statistics
 Expectation propagation for approximate Bayesian inference,2001, In Uncertainty inArtificial Intelligence
 Universaladversarial perturbations,2017, In CVPR
 Connectionist learning of belief networks,1992, Artif
 Variational inference with normalizing flows,2015, InICML
 Fine-grained recognition in the noisywild: Sensitivity analysis of convolutional neural networks approaches,2016, In BMVC
 Deep informationpropagation,2016, CoRR
 Normalization of neural networks using analytic variancepropagation,2018, In Computer Vision Winter Workshop
 Striving for simplicity: The allconvolutional net,2015, In ICLR (workshop track)
 Natural-parameter networks: A class of probabilisticneural networks,2016, In NIPS
 Fast dropout training,2013, In ICML
 Simple statistical gradient-following algorithms for connectionist reinforcementlearning,1992, Machine Learning
 Analytic approximations are onpar with Monte Carlo estimates and more accurate for small input noise,1000, (b
 Approximating the value of expected argmax indicator is shown in Fig,2006, B
