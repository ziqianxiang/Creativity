title,year,conference
 Fine-grained analysisof sentence embeddings using auxiliary prediction tasks,2016, arXiv preprint arXiv:1608
 A simple but tough-to-beat baseline for sentenceembeddings,2017, In ICLR
 Layer normalization,2016, arXiv preprintarXiv:1607
 A large anno-tated corpus for learning natural language inference,2015, arXiv preprint arXiv:1508
 Universal sentence encoder,2018, arXivpreprint arXiv:1803
 Whatyou can cram into a single vector: Probing sentence embeddings for linguistic properties,2018, InProceedings of ACL
 Geometrical and statistical properties of systems of linear inequalities with ap-plications in pattern recognition,1965, IEEE transactions on electronic ComPUters
 Scalablekernel methods via doubly stochastic gradients,2014, In Advances in Neural Information ProcessingSystems
 Understandingand improving morphological learning in the neural machine translation decoder,2017, In Proceedingsof the Eighth International Joint Conference on Natural Language Processing (Volume 1: LongPaPers)
 Model-free pomdp optimisation of tutoringsystems with echo-state networks,2013, In Proceedings of the SIGDIAL 2013 Conference
 Learn more by training less: systematicity in sentence processing by recurrentnetworks,2006, Connection Science
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE internationalconference on comPuter vision
 Learning distributed representations of sentencesfrom unlabelled data,2016, arXiv PrePrint arXiv:1602
 On-line processing of grammatical structure using reservoircomputing,2012, In International Conference on Artificial Neural Networks
 Extreme learning machine: theory andapplications,2006, NeurocomPuting
 The “echo state” approach to analysing and training recurrent neural networks-withan erratum note,2001, Technical report
 Learning visually groundedsentence representations,2017, arXiv preprint arXiv:1707
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Skip-thought vectors,2015, In Advances in neural information processingsystems
 Generalization without systematicity: On the compositional skillsof sequence-to-sequence recurrent networks,2018, In International Conference on Machine Learning(ICML)
 Distributed representations of sentences and documents,2014, arXivpreprint arXiv:1405
 Assessing the ability of lstms to learn syntax-sensitive dependencies,2016, arXiv preprint arXiv:1611
 Troubling trends in machine learning scholarship,2018, arXivpreprint arXiv:1807
 Reservoir computing approaches to recurrent neural net-work training,2009, Computer Science Review
 Random matrices,2004, Elsevier
 Unsupervised learning of sentence embed-dings using compositional n-gram features,2017, arXiv preprint arXiv:1703
 Learning and generalization characteristicsof the random vector functional-link net,1994, Neurocomputing
 Glove: Global vectors for wordrepresentation,2014, In Proceedings of the 2014 conference on empirical methods in natural languageprocessing (EMNLP)
 Random features for large-scale kernel machines,2008, In Advances inneural information processing systems
 Weighted sums of random kitchen sinks: Replacing minimizationwith randomization in learning,2009, In Advances in neural information processing systems
 How grammatical is character-level neural machine translation? assessing mt qualitywith contrastive translation pairs,2016, arXiv preprint arXiv:1612
 Baseline needs more love: On simple word-embedding-based models and associated pooling mechanisms,2018, In Proceedings of ACL
 Learning generalpurpose distributed sentence representations via large scale multi-task learning,2018, arXiv preprintarXiv:1804
 Learning gram-matical structure with echo state networks,2007, Neural networks
 Deep image prior,2017, arXiv preprintarXiv:1711
 The random projection method,2005, American Mathematical Society
 Glue:A multi-task benchmark and analysis platform for natural language understanding,2018, arXiv preprintarXiv:1804
 Pushing the limits of paraphrastic sentence embeddings withmillions of machine translations,2017, arXiv preprint arXiv:1711
 Towards universal paraphrasticsentence embeddings,2015, arXiv preprint arXiv:1511
4 points per task,2019, It seems to be especially helpful for classification
