title,year,conference
 Learning from noisy examples,1988, Machine Learning
 Classification from pairwise similarity and unlabeled data,2018, InICML
 The balanced accuracy and itsposterior distribution,2010, In ICPR
 Cluster kernels for Semi-SUPervisedlearning,2002, In NeurIPS
 Learning with bounded instance- and label-dependent label noise,2017, arXiv preprint arXiv:1709
 Clustering unclustered data: Unsupervised binarylabeling of two datasets having different class balances,2013, In TAAI
 Convex formulation for learning from positive andunlabeled data,2015, In ICML
 Learning classifiers from only positive and unlabeled data,2008, In KDD
 Training deep neural-networks using a noise adaptation layer,2017, InICLR
 Discriminative clustering by regularized information maxi-mization,2010, In NeurIPS
 Deep Learning,2016, MIT Press
 Semi-supervised learning by entropy minimization,2004, In NeurIPS
 Masking: A newperspective of noisy supervision,2018, In NeurIPS
 Co-teaching: Robusttraining deep neural networks with extremely noisy labels,2018, In NeurIPS
 Deep residual learning for image recognition,2016, In CVPR
 Learning discrete representations viainformation maximizing self augmented training,2017, In ICML
 Batch normalization: Accelerating deep network training by reducinginternal covariate shift,2015, In ICML
 MentorNet: Learning data-driven curriculum forvery deep neural networks on corrupted labels,2018, In ICML
 Semi-supervised learning via compact latent space clustering,2018, In ICML
 Adam: A method for stochastic optimization,2015, In ICLR
 Positive-unlabeled learning with non-negativerisk estimator,2017, In NeurIPS
 Learning multiple layers of features from tiny images,2009, Technical report
 Temporal ensembling for semi-supervised learning,2017, In ICLR
 Gradient-based learning applied to documentrecognition,1998, Proceedings ofthe IEEE
 Smooth neighbors on teacher graphs for semi-supervised learning,2018, In CVPR
 On the method of bounded differences,1989, In J
 Learning from corrupted binarylabels via class-probability estimation,2015, In ICML
 Learning from binary labels with instance-dependent corruption,2016, arXiv preprint arXiv:1605
 Distributional smoothing with virtualadversarial training,2016, In ICLR
 Foundations of Machine Learning,2012, MIT Press
 Learning with noisy labels,2013, In NeurIPS
 Reading digits in natural imageswith unsupervised feature learning,2011, In NIPS Workshop on Deep Learning and UnsupervisedFeature Learning
 Squared-loss mutual informationregularization: A novel information-theoretic approach to semi-supervised learning,2013, In ICML
 Theoretical comparisons of positive-unlabeled learning against positive-negative learning,2016, In NeurIPS
 Making deep neural networks robust tolabel noise: A loss correction approach,2017, In CVPR
 Dataset Shift in Ma-chine Learning,2009, MIT Press
 Training deep neuralnetworks on noisy labels with bootstrapping,2015, In ICLR workshop
 Learning with Kernels,2001, MIT Press
 Classification with asymmetric label noise: Consistency andmaximal denoising,2013, In COLT
 Understanding Machine Learning: From Theory to Algo-rithms,2014, Cambridge University Press
 Striving for simplicity: The allconvolutional net,2015, In ICLR
 Information-maximization cluster-ing based on squared-loss mutual information,2014, Neural Computation
 Training convolutional networkswith noisy labels,2015, In ICLR workshop
 Mean teachers are better role models: Weight-averaged consistencytargets improve semi-supervised deep learning results,2017, In NeurIPS
 Generalized maximum margin clustering and unsupervised kernel learn-ing,2006, In NeurIPS
 Fashion-MNIST: a novel image dataset for benchmarkingmachine learning algorithms,2017, arXiv preprint arXiv:1708
 Maximum margin clustering,2004, In NeurIPS
