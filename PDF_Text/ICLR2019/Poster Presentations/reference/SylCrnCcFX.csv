title,year,conference
 On the robustness of interpretability methods,2018, 2018ICML Workshop on Human Interpretability in Machine Learning (WHI 2018)
 Towards robust interpretability with self-explainingneural networks,2018, In Advances in Neural Information Processing Systems
 Unitary evolution recurrent neural networks,2016, InProceedings ofthe International Conference on Machine Learning
 The Cramer distance as a solution to biased WaSSerStein gradi-ents,2017, arXiv preprint arXiv:1705
 Semi-supervised support vector machines,1999, In Advances inNeural Information processing systems
 Maximum resilience of artificial neuralnetWorks,2017, In International Symposium on Automated Technology for Verification and Analysis
 Support-vector netWorks,1995, Machine learning
 Provable robustness of relu net-Works via maximization of linear regions,2019, Proceedings of the 22nd International Conference onArtificial Intelligence and Statistics
 Imagenet: A large-scalehierarchical image database,2009, In Proceedings of the IEEE international conference on computervision
 Largemargin deep netWorks for classification,2018, In Advances in Neural Information Processing Systems
 Deep neural netWorks as 0-1 mixed integer linear programs: Afeasibility study,2017, arXiv preprint arXiv:1712
 Explaining and harnessing adversarialexamples,2015, In International Conference on Learning Representations
 Caltech-256 object category dataset,2007, 2007
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE internationalconference on computer vision
 Orthogonal recurrent neural networks with scaledcayley transform,2018, Proceedings of the International Conference on Machine Learning
 Densely connectedconvolutional networks,2017, In Proceedings of the IEEE international conference on computer vision
 Reluplex: Anefficient smt solver for verifying deep neural networks,2017, In International Conference on ComputerAided Verification
 Adam: A method for stochastic optimization,2015, In InternationalConference on Learning Representations
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 An approach to reachability analysis for feed-forward reluneural networks,2017, arXiv preprint arXiv:1706
 Rectifier nonlinearities improve neural net-work acoustic models,2013, volume 30
 Differentiable abstract interpretation for prov-ably robust neural networks,2018, In International Conference on Machine Learning
 Notes on the number of linear regions of deep neural networks,2017, 2017
 Automatic differentiation inpytorch,2017, 2017
 On the convergence of adam and beyond,2018, InInternational Conference on Learning Representations
 A generative process for sam-pling contractive auto-encoders,2012, Proceedings of the 29th International Conference on MachineLearning
 Bounding and counting linearregions of deep neural networks,2018, In Proceedings of the 35th International Conference on MachineLearning
 Deep inside convolutional networks: Vi-sualising image classification models and saliency maps,2013, arXiv preprint arXiv:1312
 Certifying some distributional robustness withprincipled adversarial training,2018, In International Conference on Learning Representations
 Smoothgrad:removing noise by adding noise,2017, arXiv preprint arXiv:1706
 Striving forsimplicity: The all convolutional net,2014, arXiv preprint arXiv:1412
 Estimation of dependences based on empirical data,1995, 1982
 On structural risk minimization or overall risk in a problem ofpattern recognition,1977, Automation and Remote Control
 Learning to draw samples: With application to amortized mle forgenerative adversarial learning,2016, arXiv preprint arXiv:1611
 A genetic algorithm tutorial,1994, Statistics and computing
 Scaling provable adversarialdefenses,2018, In Advances in Neural Information Processing Systems
 Deep defense: Training dnns with improved adver-sarial robustness,2018, In Advances in Neural Information Processing Systems
 The input dimension D is 2 and the output dimension L is 1,2015, The loss function L(fÎ¸(x)
