title,year,conference
 Fathom: Referenceworkloads for modern deep learning methods,2016, In IEEE International Symposium on WorkloadCharacterization (IISWC)
 Neural optimizer search with reinforce-ment learning,2017, In Proceedings of the 34th International Conference on Machine Learning (ICML)
 Practical Gauss-Newton optimisation for deeplearning,2017, In Proceedings of the 34th International Conference on Machine Learning (ICML)
 Entropy-SGD: Biasing gradient descentinto wide valleys,2017, The International Conference on Learning Representations (ICLR)
 BDA-PCH: Block-diagonal approximationof positive-curvature Hessian for training neural networks,2018, arXiv preprint arXiv:1802
 DAWNBench: An end-to-end deep learningbenchmark and competition,2017, NIPS ML Systems Workshop
 Imagenet: A large-scalehierarchical image database,2009, In Computer Vision and Pattern Recognition (CVPR)
 Incorporating Nesterov Momentum into Adam,2016, ICLR workshop paper
 Deep Learning,2016, MIT Press
 Neural Turing machines,2014, arXiv preprintarXiv:1410
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition(CVPR)
 Long short-term memory,1997, Neural Comput
 Accessed 24,2018, SeP
 Adam: A method for stochastic oPtimization,2015, Proceedings of 3rdInternational Conference on Learning Representations (ICLR)
 Auto-encoding variational Bayes,2014, Proceedings of theInternational Conference on Learning Representations (ICLR)
 Gradient-based learning aPPlied todocument recognition,1998, Proceedings of the IEEE
 Novel approaches to the discrimination problem,1992, ZeitschriftfurOperations Research
 Deep learning via Hessian-free optimization,2010, In Proceedings of the 27th InternationalConference on International Conference on Machine Learning (ICML)
 Optimizing neural networks with Kronecker-factored approximatecurvature,2015, In Proceedings of the 32nd International Conference on International Conference onMachine Learning (ICML)
 Readingdigits in natural images with unsupervised feature learning,2011, In NIPS workshop on deep learningand unsupervised feature learning
 Some methods of speeding up the convergence of iteration methods,1964, USSRComputational Mathematics and Mathematical Physics
 On the convergence of Adam and beyond,2018, InInternational Conference on Learning Representations (ICLR)
 Unit tests for stochastic optimization,2013, arXivpreprint arXiv:1312
 No more pesky learning rates,2013, In Proceedings of the30th International Conference on Machine Learning (ICML)
 Practical Bayesian optimization of machinelearning algorithms,2012, In Advances in neural information processing systems
 Striving forsimplicity: The all convolutional net,2015, In ICLR (workshop track)
 Rethinkingthe inception architecture for computer vision,2016, In Proceedings of the IEEE conference on ComputerVision and Pattern Recognition (CVPR)
 Attention is all you need,2017, In Advances in Neural InformationProcessing Systems (NIPS)
 The marginalvalue of adaptive gradient methods in machine learning,2017, In Advances in Neural InformationProcessing Systems
 Fashion-MNIST: a novel image dataset for bench-marking machine learning algorithms,2017, arXiv preprint arXiv:1708
 Wide residual networks,2016, arXiv preprint arXiv:1605
 ADADELTA: An adaptive learning rate method,2012, arXiv preprint arXiv:1212
 Block-diagonal Hessian-freeoptimization for training neural networks,2017, arXiv preprint arXiv:1712
 TBD: Benchmarking and analyzing deep neural networktraining,2018, arXiv preprint arXiv:1803
