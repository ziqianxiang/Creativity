title,year,conference
 Threat of adversarial attacks on deep learning in computer vision:A survey,2018, arXiv preprint arXiv:1801
 Did you hear that? Adversarial examplesagainst automatic speech recognition,2018, arXiv preprint arXiv:1801
 Obfuscated gradients give a false sense ofsecurity: Circumventing defenses to adversarial examples,2018, arXiv preprint arXiv:1802
 Spectrally-normalized margin bounds forneural networks,2017, In Advances in Neural Information Processing Systems
 A theoretical analysis of feature pooling in visualrecognition,2010, In International Conference on Machine Learning
 Towards evaluating the robustness of neural networks,2017, InProceedings of the IEEE Symposium on Security and Privacy
 Adversarial examples are not easily detected: Bypassing tendetection methods,2017, In Proceedings of the 10th ACM Workshop on Artificial Intelligence andSecurity
 Audio adversarial examples: Targeted attacks on speech-to-text,2018, arXiv preprint arXiv:1801
 Provably minimally-distorted adver-sarial examples,2017, arXiv preprint arXiv:1709
 Show-and-fool: Craftingadversarial examples for neural image captioning,2017, arXiv preprint arXiv:1712
 Parsevalnetworks: Improving robustness to adversarial examples,2017, In International Conference on MachineLearning
 Improving generalization performance using double backpropa-gation,1992, IEEE Transactions on Neural Networks
 Hotflip: White-box adversarial examplesfor nlp,2017, arXiv preprint arXiv:1712
 Black-box generation of adversarial textsequences to evade deep learning classifiers,2018, arXiv preprint arXiv:1801
 Adversarial spheres,2018, arXiv preprint arXiv:1801
 Explaining and harnessing adversarialexamples,2014, arXiv preprint arXiv:1412
 Stable architectures for deep neural networks,2017, Inverse Problems
 Provable defenses against adversarial examples via the convex outeradversarial polytope,2017, arXiv preprint arXiv:1711
 Fooling end-to-end speaker verifica-tion by adversarial examples,2018, arXiv preprint arXiv:1801
 Magnet: a two-pronged defense against adversarial examples,2017, InProceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security
 On the difficulty of training recurrent neuralnetworks,2013, In International Conference on Machine Learning
 Certified defenses against adversarial exam-ples,2018, arXiv preprint arXiv:1801
 Improving the adversarial robustness and in-terpretability of deep neural networks by regularizing their input gradients,2017, arXiv preprintarXiv:1711
 Understanding and improvingconvolutional neural networks via concatenated rectified linear units,2016, In International Conferenceon Machine Learning
 Intriguing properties of neural networks,2014, In Proceedings of InternationalConference on Learning Representations
 Ensembleadversarial training: Attacks and defenses,2017, arXiv preprint arXiv:1705
 Scaling provable adversarialdefenses,2018, In Advances in Neural Information Processing Systems
 Robustness and generalization,2012, Machine learning
 Spectral norm regularization for improving the generalizabilityof deep learning,2017, arXiv preprint arXiv:1705
 Efficient defenses against adver-sarial attacks,2017, In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security
 Understandingdeep learning requires rethinking generalization,2017, In Proceedings of International Conference onLearning Representations
 Let x1 and x2 be two vectors with the size of a pooling window,2019, By triangle inequality
