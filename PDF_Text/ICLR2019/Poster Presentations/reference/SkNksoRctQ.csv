title,year,conference
 Stochastic modified equations for the asynchronous stochas-tic gradient descent,2018, arXiv preprint arXiv:1805
 Optimization methods for large-scale machinelearning,2018, SIAMReview
 Entropy production fluctuation theorem and the nonequilibrium work relation forfree energy differences,1999, Physical Review E
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Markoff random processes and the statistical mechanics of time-dependent phe-nomena,1954, II
 On the convergence of Adam and beyond,2018, InInternational Conference on Learning RePresentations
 Three factors influencing minima in SGD,2017, arXiv preprintarXiv:1711
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Batch size matters: A diffusion approxi-mation framework on nonconvex stochastic gradient descent,2017, arXiv preprint arXiv:1705
 Continuous-time limit of stochastic gra-dient descent revisited,2015, In 8th NIPS Workshop on Optimization for Machine Learning
 Natural Langevin dynamics for neural networks,2017, InInternational Conference on Geometric Science of Information
 In search of the real inductive bias: On therole of implicit regularization in deep learning,2014, arXiv preprint arXiv:1412
 Exploring general-ization in deep learning,2017, In Advances in Neural Information Processing Systems
 Reciprocal relations in irreversible processes,1931, I
 Fokker-Planck description of learning inbackpropagation networks,1990, In International Neural Network Conference
 The Fokker-Planck equation: methods of solution and applications,1984, Springer
 Absence of thermodynamic phase transition in a model glassformer,2000, Nature
 A Bayesian perspective on generalization and stochastic gradientdescent,2018, arXiv preprint arXiv:1710
 The implicit bias of gradientdescent on separable data,2018, In International Conference on Learning Representations
 Bayesian learning via stochastic gradient Langevin dynamics,2011, InProceedings of the 28th International Conference on Machine Learning
 The anisotropic noise in stochasticgradient descent: Its behavior of escaping from minima and regularization effects,2018, arXiv preprintarXiv:1803
