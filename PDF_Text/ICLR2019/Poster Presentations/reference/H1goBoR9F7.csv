title,year,conference
 Compression-aware training of deep networks,2017, In Advancesin Neural Information Processing Systems
 Sparsely-connected neural networks: towardsefficient vlsi implementation of deep neural networks,2016, arXiv preprint arXiv:1611
 Random projection in dimensionality reduction: applications toimage and text data,2001, In Proceedings of the seventh ACM SIGKDD international conference onKnowledge discovery and data mining
 Imagenet: A large-scalehierarchical image database,2009, In Computer Vision and Pattern Recognition
 Ultimate tensoriza-tion: compressing convolutional and fc layers alike,2016, arXiv preprint arXiv:1611
 Soft filter pruning for acceleratingdeep convolutional neural networks,2018, arXiv preprint arXiv:1808
 Channel pruning for accelerating very deep neural net-works,2017, In International Conference on Computer Vision (ICCV)
 Amc: Automl for modelcompression and acceleration on mobile devices,2018, In Proceedings of the European Conference onComputer Vision (ECCV)
 A novel channel pruningmethod for deep neural network compression,2018, arXiv preprint arXiv:1805
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 Learning multiple layers of features from tiny images,2009, 2009
 Imagenet classification with deep convo-lutional neural networks,2012, In Advances in neural information processing systems
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Deep learning,2015, nature
 Extremely low bit neural network: Squeeze thelast bit out with admm,2017, arXiv preprint arXiv:1707
 Pruning filters forefficient convnets,2016, arXiv preprint arXiv:1608
 Very sparse random projections,2006, In Proceedingsof the 12th ACM SIGKDD international conference on Knowledge discovery and data mining
 Crossbar-aware neural network pruning,2018, IEEE Access
 Predictivenet: An energy-efficient convolutional neural network via zero prediction,2017, In Circuits and Systems (ISCAS)
 Deep gradient compression: Re-ducing the communication bandwidth for distributed training,2017, arXiv preprint arXiv:1712
 Learn-ing efficient convolutional networks through network slimming,2017, In 2017 IEEE InternationalConference on Computer Vision (ICCV)
 Autopruner: An end-to-end trainable filter pruning method forefficient deep model inference,2018, arXiv preprint arXiv:1805
 Thinet: A filter level pruning method for deep neuralnetwork compression,2017, arXiv preprint arXiv:1707
 Discovering low-precision networks close to full-precision networks for efficient embedded inference,2018, arXiv preprint arXiv:1809
 All you need is a good init,2015, arXiv preprint arXiv:1511
 On the importance ofsingle directions for generalization,2018, arXiv preprint arXiv:1803
 Tensorizing neuralnetworks,2015, In Advances in Neural Information Processing Systems
 Compressing dma engine: Leveraging activation sparsity for training deep neural net-works,2018, In High Performance Computer Architecture (HPCA)
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Scalable and sustainable deep learning via randomizedhashing,2017, In Proceedings of the 23rd ACM SIGKDD International Conference on KnowledgeDiscovery and Data Mining
 meprop: Sparsified back propagationfor accelerated deep learning with reduced overfitting,2017, arXiv preprint arXiv:1706
 A casefor core-assisted bottleneck acceleration in gpus: enabling flexible data compression with assistwarps,2015, In ACM SIGARCH ComPuterArchitectureNews
 Random projection for high-dimensional optimization,2016, PhD thesis
 Learning structured sparsity indeep neural networks,2016, In Advances in Neural Information Processing Systems
 Terngrad:Ternary gradients to reduce communication in distributed deep learning,2017, In Advances in NeuralInformation Processing Systems
 Training and inference with integers in deepneural networks,2018, arXiv preprint arXiv:1802
 Group normalization,2018, arXiv preprint arXiv:1803
 Fashion-mnist: a novel image dataset for benchmark-ing machine learning algorithms,2017, arXiv preprint arXiv:1708
 Singular value decomposition basedlow-footprint speaker adaptation and personalization for deep neural network,2014, In Acoustics
 Tensor-train recurrent neural networks forvideo classification,2017, arXiv preprint arXiv:1707
 Rethinking the smaller-norm-less-informativeassumption in channel pruning of convolution layers,2018, arXiv preprint arXiv:1802
 Wide residual networks,2016, arXiv preprintarXiv:1605
 Frequent value locality and value-centric data cachedesign,2000, ACM SIGPLAN Notices
 Dorefa-net: Train-ing low bitwidth convolutional neural networks with low bitwidth gradients,2016, arXiv preprintarXiv:1606
