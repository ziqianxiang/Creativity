title,year,conference
 Tensorflow: A system for large-scale machine learning,2016, InOSDI
 On the optimization of deep networks: Implicit acceleration byoverparameterization,2018, In International Conference on Machine Learning
 Neural networks and principal component analysis: Learning from exampleswithout local minima,1989, Neural networks
 Gradient descent with identity initialization efficiently learnspositive definite linear transformations,2018, In International Conference on Machine Learning
 Convex optimization,2004, Cambridge university press
 Globally optimal gradient descent for a convnet with gaussian inputs,2017, InInternational Conference on Machine Learning
 The loss surfacesof multilayer networks,2015, In Artificial Intelligence and Statistics
 Algorithmic regularization in learning deep homogeneous models:Layers are automatically balanced,2018, arXiv preprint arXiv:1806
 Escaping from saddle pointsonline stochastic gradient fortensor decomposition,2015, In Conference on Learning Theory
 Matrix completion has no spurious local minimum,2016, In Advances inNeural Information Processing Systems
 Global optimality in neural network training,2017, In IEEE Conference onComputer Vision and Pattern Recognition
 Deep residual learning for image recognition,2016, InProceedings of the IEEE conference on computer vision and pattern recognition
 Matrix analysis,1990, Cambridge university press
 Deep learning without poor local minima,2016, In Advances in Neural Information ProcessingSystems
 Adaptive estimation of a quadratic functional by model selection,2000, TheAnnals of Statistics
 Deep linear networks with arbitrary loss: All local minima are global,2018, InInternational Conference on Machine Learning
 Gradient descent only converges tominimizers,2016, In Conference on Learning Theory
 Algorithmic regularization in over-parameterized matrix sens-ing and neural networks with quadratic activations,2018, In Proceedings of the 31st Conference On LearningTheory
 Almost global convergence to global minima for gradientdescent in deep linear networks,2018, 2018
 An elementary proof of anti-concentration of polynomials in Gaussian variables,2010, ElectronicColloquium on Computational Complexity
 Anti-concentration for Polynomials of Independent Random Vari-ables,2016, Theory of Computing
 The loss surface of deep and wide neural networks,2017, In InternationalConference on Machine Learning
 Gradient descent only converges to minimizers: Non-isolated criticalpoints and invariant regions,2017, In Innovations in Theoretical Computer Science
 On the cal-ibration of sensor arrays for pattern recognition using the minimal number of experiments,2014, Chemometricsand Intelligent Laboratory Systems
 Spurious local minima are common in two-layer relu neural networks,2018, InInternational Conference on Machine Learning
 Exact solutions to the nonlinear dynamics oflearning in deep linear neural networks,2014, International Conference on Learning Representations
 Exponential convergence time of gradient descent for one-dimensional deep linear neural net-works,2018, arXiv preprint arXiv:1809
 No bad local minima: Data independent training error guarantees for multi-layer neural networks,2016, arXiv preprint arXiv:1605
 On the importance of initialization andmomentum in deep learning,2013, In International Conference on Machine Learning
 An analytical formula of population gradient for two-layered relu network and its applicationsin convergence and critical point analysis,2017, arXiv preprint arXiv:1703
 Recovery guarantees for one-hidden-layer neural networks,2017, In International Conference on Machine Learning
 This establishesthe inductive step and completes the proof of Lemma 17,2019,	â–¡32Published as a conference paper at ICLR 2019By Lemma 17
