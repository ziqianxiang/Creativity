title,year,conference
 Tensorflow: Large-scale machine learning on heterogeneousdistributed systems,2016, arXiv preprint arXiv:1603
 Sparse communication for distributed gradient descent,2017, In Proceedingsof the 2017 Conference on Empirical Methods in Natural Language Processing
 Iterative parameter mixing for distributed large-margin training of structured predictors fornatural language processing,2015, PhD thesis
 Optimal distributed online prediction usingmini-batches,1532, J
 Communication quantization for data-parallel training ofdeep neural networks,2016, In 2016 2nd Workshop on Machine Learning in HPC Environments (MLHPC)
 On the rates of convergence of parallelized averaged stochasticgradient algorithms,2017, arXiv preprint arXiv:1710
 Onlarge-batch training for deep learning: Generalization gap and sharp minima,2016, arXiv preprint arXiv:1609
 Communication-efficient learning of deep networks from decentralized data,2017, In Aarti Singh and Jerry Zhu (eds
 On-chip training of recurrent neural networks with limitednumerical precision,2017, In 2017 International Joint Conference on Neural Networks (IJCNN)
 Automatic differentiation in pytorch,2017, 2017
 Advances in kernel methods,1999, chapter Fast Training of Support Vector Machines Using SequentialMinimal Optimization
 Making gradient descent optimal for strongly convexstochastic optimization,2012, In Proceedings of the 29th International Coference on International Conferenceon Machine Learning
 A Stochastic Approximation Method,1951, The Annals of Mathematical Statistics
 1-bit stochastic gradient descent and its applicationto data-parallel distributed training of speech DNNs,2014, In Haizhou Li
 Distributed stochastic optimization and learning,2014, In 2014 52nd Annual AllertonConference on Communication
 Stochastic gradient descent for non-smooth optimization: Convergence resultsand optimal averaging schemes,2013, In Sanjoy Dasgupta and David McAllester (eds
 Sparsified SGD with memory,2018, In S
 Scalable distributed DNN training using commodity GPU cloud computing,2015, In INTERSPEECH
 meProp: Sparsified back propagation for accelerateddeep learning with reduced overfitting,2017, In Doina Precup and Yee Whye Teh (eds
 Cooperative SGD: A unified framework for the design and analysis ofcommunication-efficient SGD algorithms,2018, CoRR
 Gradient sparsification for communication-efficientdistributed optimization,2017, CoRR
 Tern-grad: Ternary gradients to reduce communication in distributed deep learning,2017, In I
 Scaling SGD batch size to 32k for ImageNet training,2017, CoRR
 Parallel restarted SGD for non-convex optimization with fasterconvergence and less communication,2018, CoRR
 Deep learning with elastic averaging SGD,2015, In C
 Improving deep neural network acoustic models usinggeneralized maxout networks,2014, In 2014 IEEE International Conference on Acoustics
