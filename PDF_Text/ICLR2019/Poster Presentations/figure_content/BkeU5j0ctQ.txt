Figure 1: Architectures of the cem-rl (a) and erl (b) algorithmsfor 10,000 mini-batches, which are divided into 2000 mini-batches per learning actor. This is acommon practice in deep RL algorithms, where one mini-batch update is performed for each stepof the actor in the environment. We also choose this number of steps (10,000) to be the number ofgradient steps taken by half of the population at the next iteration. A pseudo-code of cem-rl isprovided in Algorithm 1.
Figure 2: Learning curves of td3, cem and cem-rl on the half-cheetah-v2, hopper-v2, andwalker2d-v2 benchmarks.
Figure 3: Learning curves of erl, cem-ddpg and cem-td3 on half-cheetah-v2, hopper-v2,ant-v2 and walker2d-v2. Both cem-rl methods are only trained 1 million steps.
Figure 4: Learning curves of cem-td3 and cem with and without importance mixing on the half-cheetah-v2, hopper-v2, walker2d-v2, swimmer-v2 and ant-v2 benchmarks.
Figure 5: Learning curves of cem-rl with and without action space noise on the half-cheetah-v2, hopper-v2, walker2d-v2, swimmer-v2 and ant-v2 benchmarks.
Figure 6: Evolution of the first two parameters of the actors when learning with (a) erl and (b)cem-td3. Dots are sampled parameters of the population and continuous lines represent parametersmoved through RL gradient steps.
Figure 7: Histogram of the average similarity in populations during learning with the erl algorithm.
Figure 8: Learning curves on the swimmer-v2 environment of (a): cem and td3, multi-actor td3and cem-rl; (b) erl, cem-ddpg and cem-td3.
Figure 9: Learning curves of CEM-RL with tanh and RELU as non-linearities in the actors, on the(a) half-cheetah-v2, (b) hopper-v2, (c) swimmer-v2, (d) ant-v2 and (e) walker2d-v2benchmarks. (f) shows the same of cem on the swimmer-v2 benchmark.
Figure 10: Learning curves of cem-rl, cem and td3 on the swimmer-v2 and ant-v2 bench-marks.
