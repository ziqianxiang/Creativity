Figure 1: Conceptual illustration of lemonade. (Left) lemonade maintains a population of trainednetworks that constitute a Pareto front in the multi-objective space. Parents are selected from thepopulation inversely proportional to their density. Children are generated by mutation operatorswith Lamarckian inheritance that are realized by network morphisms and approximate networkmorphisms. NM operators generate children with the same initial error as their parent. In contrast,children generated with ANM operators may incur a (small) increase in error compared to theirparent. However, their initial error is typically still very small. (Right) Only a subset of the generatedchildren is accepted for training. After training, the performance of the children is evaluated and thepopulation is updated to be the Pareto front.
Figure 2: Progress of the Pareto front of lemonade during architecture search. The Pareto front getsmore and more densely settled over the course of time. Very large models found (e.g., in generation25) are discarded in a later generation as smaller, better ones are discovered. Note: generation 1denotes the generation after one iteration of LEMONADE.
Figure 3: Comparison of LEMONADE with NASNet and MobileNet V2. LEMONADE optimized fiveobjectives: performance on CIFAR-10 (x-axis in all plots), performance on CIFAR-100 (top left),number of parameters (top right), number of multiply add operations (bottom left) and inference time(bottom right, measured in seconds on a Titan X GPU).
Figure 4: Transferring the cells discovered on CIFAR-10 to ImageNet64x64. A single Cell, namelyCell 2, outperforms all baselines. Utilizing 5 different cells (red line) further improves the results.
Figure 5: Performance on CIFAR-10 test data of models that have been trained under identicalconditions.
Figure 6: Ablation study on CIFAR-10. We deactivate different components of lemonade andinvestigate the impact. LEMONADE default: Performance of LEMONADE as proposed in this work.
Figure 7:	Comparison of LEMONADE with other NAS methods and hand-crafted architectures onCIFAR-10. This plot shows the same results as Figure 5 but zoomed into the range of errors less than0.06%.
Figure 8:	Transferring the cells discovered on CIFAR-10 to ImageNet64x64. Top-1 validation error.
Figure 9: Cell 0. Largest discovered cell.
Figure 10: Cell 220Published as a conference paper at ICLR 2019Figure 11: Cell 621Published as a conference paper at ICLR 2019Figure 12: Cell 922Published as a conference paper at ICLR 2019Figure 13: Cell 18Figure 14: Cell 21. The smallest possible cell in our search space is also discovered.
Figure 11: Cell 621Published as a conference paper at ICLR 2019Figure 12: Cell 922Published as a conference paper at ICLR 2019Figure 13: Cell 18Figure 14: Cell 21. The smallest possible cell in our search space is also discovered.
Figure 12: Cell 922Published as a conference paper at ICLR 2019Figure 13: Cell 18Figure 14: Cell 21. The smallest possible cell in our search space is also discovered.
Figure 13: Cell 18Figure 14: Cell 21. The smallest possible cell in our search space is also discovered.
Figure 14: Cell 21. The smallest possible cell in our search space is also discovered.
