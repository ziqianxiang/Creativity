Figure 1: At subfig. 1(a) we show the process of learning the prior distribution over kernels ofone convolutional layer. First, we train encoder r(z | w; φl) and decoder p(w | z; ψl) with VAEframework. Then, We use the decoder to construct the prior Pl (w). At subfig. 1(b) We show a batchof learned kernels of shape 7 × 7 form the first convolutional layer of a CNN trained on NotMNISTdataset, at subfig. 1(c) we show samples form the deep weight prior that is learned on these kernels.
Figure 2: For different sizes of training set of MNIST and CIFAR-10 datasets, we demonstrate theperformance of variational inference with a fully-factorized variational approximation with threedifferent prior distributions: deep weight prior (dwp), log-uniform, and standard normal. We foundthat variational inference with a deep weight prior distribution achieves better mean test accuracycomparing to learning with standard normal and log-uniform prior distributions.
Figure 3: We study the influence of initialization of convolutional filters on the performance ofrandom feature extraction. In the experiment, the weights of convolutional filters were initializedrandomly and fixed. The initializations were sampled from deep weight prior (dwp), learned filters(filters) and samples from Xavier distribution (xavier). We performed the experiment for differentsize of the model, namely, to obtain models of different sizes we scaled a number of filters in allconvolutional layers linearly by k. For every size of the model, we averaged results by 10 runs.
Figure 4: We found that initialization of weights of the models with deep weight priors or learnedfilters significantly increases the training speed, comparing to Xavier initialization. At subplot 4(a)we report a variational lower bound for variational auto-encoder, at subplots 4(b) and 4(c) we reportaccuracy for convolution networks on MINTS and CIFAR-10.
Figure 5: For different sizes of training set of MNIST dataset, we demonstrate the performance ofvariational inference with a fully-factorized variational approximation with three different prior dis-tributions: deep weight prior (dwp), log-uniform, standard normal and learned multivariate gaussian.
Figure 6: An illustration for Section 4.1. We visualize latent representations of convolutional filtersfor ConvNet on NotMNIST. Every point corresponds to mean of latent representation q(z | wi),where wi is a kernel of shape 7 × 7 from the first convolutional layer, and q(z | wi) is an inferencenetwork with a two-dimensional latent represenation.
Figure 7: An illustration for the Section 4.3 of samples from variational auto-encoder for threedifferent types of initialization of convolutional layers after 100, 200, 300, 400 and 500 steps of op-timization. The first row corresponds to deep weight prior initialization, the second to initializationwith learned kernels, and the third to Xavier initialization.
