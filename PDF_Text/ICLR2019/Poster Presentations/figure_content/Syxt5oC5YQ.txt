Figure 1: Minimizing a quadratic function. All optimizers use a fixed learning rate of 0.33. In the legend, λdenotes the corresponding eigenvalues.
Figure 2: Breaking oscillations with passive damping. The arrows show the direction and relative amplitudeof the velocities at various points in time. We discuss points (1) and (2) in Section 3.
Figure 3: Convergence on quadratics of varying condition number. AggMo interpolates between the conver-gence rates of CM at β = 0.9 and β = 0.99.
Figure 4: Convergence of Autoencoders Trainingloss during the first 350 epochs of training with eachoptimizer. The shaded region corresponds to one stan-dard deviation over 15 runs.
Figure 5: Damping Coefficient Investigation Opti-mizing autoencoders on MNIST with varying damp-ing coefficients and fixed learning rate. Nesterov isunstable with β = 0.999.
Figure 6: ResNet-32 Trained On CIFAR-100 The training loss and validation accuracy during training onCIFAR-100 for each optimizer.
Figure 7: Convergence of LSTM The training and validation perplexity during training. For each model weuse the hyperparameters that obtained the best validation loss. We found that there was very little differencewhen choosing hyperparameters based on training performance.
Figure 8: Equivalence of Nesterov and AggMo when β = 0.999. The optimization plots for f(x) = xTAx arevisibly identical (circles correspond to AggMo and squares to Nesterov - the markers are offset for readability).
Figure 9: Approximate equivalence of Nesterov and AggMo when β = 0.999. The optimization trajectories areinitially visibly identical but begin to differ slightly after more iterations.
Figure 10: Velocity during quadratic optimization with CM, Nesterov, and AggMo. (Best viewed in color)The shaded region shows the direction and relative magnitude of the velocities throughout optimization for eachoptimizer. AggMo has multiple shaded regions corresponding to the different velocities.
Figure 11: Comparison of classical momentum and aggregated momentum on toy problem (13) with a = 8, b10. In each case the optimizer is initialized at (x, y) = (-2, 0)Optimizer	Train Optimal	Validation Optimal		Train Loss	VaL Loss	Test LossCM β = 0.9	2:0T	4.95	4~98~Nesterov β = 0.9	1.94	4.63	4.62AggMo (Default)	1.60	3.14	3.04Table 4: MNIST Autoencoder with default settings We display the training MSE for the initial learning ratethat achieved the best training loss. The validation and test errors are displayed for the initial learning rate thatachieved the best validation MSE.
Figure 12: Beta-Averaged GD with a Beta prior on momentum (α = 100, β = 1).
