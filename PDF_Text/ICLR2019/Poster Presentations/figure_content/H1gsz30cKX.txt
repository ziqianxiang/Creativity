Figure 1: Left: ResNet basic block. Batch normalization (Ioffe & Szegedy, 2015) layers are markedin red. Middle: A simple network block that trains stably when stacked together. Right: Fixupfurther improves by adding bias parameters. (See Section 3 for details.)In the remaining of this paper, we first analyze the exploding gradient problem of residual networksat initialization in Section 2. To solve this problem, we develop Fixup in Section 3. In Section 4 wequantify the properties of Fixup and compare it against state-of-the-art normalization methods onreal world benchmarks. A comparison with related work is presented in Section 5.
Figure 2: Examples of p.h. sets in a ResNet without normalization: (1) the first convolution layerbefore max pooling; (2) the fully connected layer before softmax; (3) the union of a spatial down-sampling layer in the backbone and a convolution layer in its corresponding residual branch.
Figure 3: Depth of residual networks versus test accuracy at the first epoch for various methods onCIFAR-10 with the default BatchNorm learning rate. We observe that Fixup is able to train verydeep networks with the same learning rate as batch normalization. (Higher is better.)Figure 3 shows the test accuracy at the first epoch as depth increases. Observe that Fixup matchesthe performance of BatchNorm at the first epoch, even with 10,000 layers. LSUV and -∖∕1∕2-scalingare not able to train with the same learning rate as BatchNorm past 100 layers.
Figure 4: Minibatch training accuracy of ResNet-110 on CIFAR-10 dataset with different config-urations in the first 3 epochs. We use minibatch size of 128 and smooth the curves using 10-stepmoving average.
Figure 5: Training and test errors on ImageNet using ResNet-50 without additional regularization.
Figure 6: Test error of ResNet-50 on ImageNet with Mixup (Zhang et al., 2017). Fixup closelymatches the final results yielded by the use of GroupNorm, without any normalization.
