Figure 1: An example where corrections disambiguate an instruction. The agent is unable to fullydeduce the user’s intent from the instruction alone and iterative language corrections guide the agentto the correct position. Our method is concerned with meta-learning policies that can ground languagecorrections in their environment and use them to improve through iterative feedback.
Figure 2: The architecture of our model. The instruction module embeds the initial instruction L,while the correction modules embed the trajectory τi and correction ci from each previous trial. Thefeatures from these corrections are pooled and provided to the policy, together with the current state sand the embedded initial instruction.
Figure 3: Left: Overall training procedure. We collect data for each task [1, 2, . . . , N] using DAgger,storing it in the data buffer. This is used to train a GPL policy with supervised learning. The trainedpolicy is then used to collect data for individual tasks again, repeating the process until convergence.
Figure 4: The multi-room ob-ject manipulation environmentwith labeled components.
Figure 5: The robotic ob-ject relocation environment.
Figure 6: Sample complexity on test tasks. The mean completion rate is plotted against the numberof trajectories using during training per task. Our method (GPL) is shown in green.
Figure 7: Example task with corrections. Instruction: The agent receives the initial instruction.
Figure 8: Failure example. The orange arrow shows the task, the white arrows show the net trajectory.
Figure 9: Success example. The orange arrow shows the task.
Figure 10: Failure example. The orange arrow shows the task, the white arrows show the nettrajectory.
Figure 11: Success example. The orange arrow shows the task.
