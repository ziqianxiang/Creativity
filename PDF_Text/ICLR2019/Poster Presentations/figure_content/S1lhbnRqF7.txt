Figure 1: Snapshot of the knowledge graphs created byour model before and after reading the sentence in bold-face. Since the KG explicitly stores the current locationof light, CO2, and water as leaf, the model can inferthat mixture is formed in the leaf even though this isnot explicitly stated. The three participant entities alsoget destroyed in the process, which is captured in thegraph by pointing to a special Nowhere node.
Figure 2: Soft co-reference across time steps. The sentence at the current time step is highlighted. When theMRC model predicts a span (leaf ) present in the graph at the previous time step, KG-MRC does soft attentionand a gated update to preserve information across time steps (ยง 4.3). The thicker arrow shows higher attentionweight between the old and new node.
