Figure 1: Traditional pixel-based adversarial attacks yield unrealistic images under a larger perturba-tion (L∞-norm ≈ 0.82), however our parametric lighting and geometry perturbations output morerealistic images under the same norm (more results in Appendix A).
Figure 2: Parametrically-perturbed images remain natural, whereas pixel-perturbed ones do not.
Figure 4: By changing the lighting, we fool the classifier into seeing miniskirt and water tower,demonstrating the existence of adversarial lighting.
Figure 5: We construct a single lighting condition that can simultaneously fool the classifier viewingfrom different angles.
Figure 3: Our differentiable ren-derer based on analytical deriva-tives is faster and more scalablethan the previous method.
Figure 6: By specifying different target labels, we can create an optical illusion: a jaguar is classifiedas cat and dog from two different views after geometry perturbations.
Figure 8: Perturbing points on 3D shapes fools the classifier into seeing rifle/slug.
Figure 9: We construct a single adversarial geometry that fools the classifier seeing a mailbox fromdifferent angles.
Figure 10: A quantitative comparison using paramet-ric norm-balls shows the fact that adversarial light-ing/geometry perturbations have a higher successrate (%) in fooling classifiers comparing to randomperturbations in the parametric spaces.
Figure 11: Unlike much of the lit-erature on adversarial training, weevaluate against real photos (cap-tured by a camera), not computer-generated images. This figure il-lustrates a subset of our test data.
Figure 12: We compare our parametric perturbations (the first two columns) with pixel/color perturba-tions under the same L∞ pixel norm (small: 0.12, medium: 0.53, large: 0.82). As changing physicalparameters corresponds to real-world phenomena, our parametric perturbation are more realistic.
Figure 13: PBR models thephysics of light that emitted fromthe light source, interact with thescene, then arrive a camera.
Figure 14: Rasterization convertsa 3D scene into pixels.
Figure 15: We consider the Lam-bertian material (left) where lightsget reflected uniformly in everydirection.
Figure 16: This figure visualizes the images of oranges from CIFAR-100, random lighting, andadversarial lighting. In early training stage, small changes in lighting are sufficient to constructadversarial examples. In late training stage, we require more dramatic changes as the model isbecoming robust to differ lightings.
Figure 17: Prediction confidenceon rendered images, showingour rendering quality is faithfulenough to be confidently recog-nized by ImageNet models.
