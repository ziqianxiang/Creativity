Figure 1: Illustration of slimmable neural networks. The same model can run at different widths(number of active channels), permitting instant and adaptive accuracy-efficiency trade-offs.
Figure 2: Training and validation curves of slimmable networks. Left shows the training error ofthe largest switch. Right shows testing errors on validation set with different switches. For naiveapproach, the training is stable (left) but testing error is high (right, zoomed). Slimmable networkstrained with S-BN have stable and rank-preserved testing accuracy across all training iterations.
Figure 3: Top-activated images for same channel 3_9 in different switches in S-MobileNet v1. Dif-ferent rows represent results from different switches. Images with red outlines are mis-classified.
Figure 4: Values of BN parameters in different switches. We show BN values of both shallow (left,BN 1_1 to 1_8) and deep (right, BN 12」to 12_8) layers of S-MobileNet VL2	4 β ∙Values of Switchable Batch Normalization. Our proposed S-BN learns different BN transforma-tions for different switches. But how diverse are the learned BN parameters? We show the valuesof batch normalization weights in both shallow (BN 1_1 to 1_8) and deep (BN 12_1 to 12_8) layersof S-MobileNet v1 in Figure 4. The results show that for shallow layers, the mean, variance, scaleand bias are very close, while in deep layers they are diverse. The value discrepancy is increasedlayer by layer in our observation, which also indicates that the learned features ofa same channel indifferent switches have slight variations of semantics.
