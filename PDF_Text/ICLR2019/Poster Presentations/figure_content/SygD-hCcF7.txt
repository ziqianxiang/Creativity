Figure 1: DRPR learns low-dimensional representationsthat reflect the uncertainties of a pre-trained classifier (herefor categories blue and yellow represented by their centers)Prediction function: Let us assume thatwe are given the dataset matrix F = [fι, •一，fn]> ∈ Vn = Rn×d,the centers M = [μι, ∙∙∙,μ卜]> ∈Vk = Rk×d and the priors π = [∏ι,…，∏k]> ∈ (0,1)k. As in Eq. (1), we define our predictionfunction ψ(F, M, π) = Ψ ∈ Yn×k as the soft assignment matrix predicted by the BSCP given F,M and π. The elements of the matrix Ψ are then computed as follows:∀i, c, Ψic = kπ eXP(-d(fi，μc))Em=I πm exP(-d(fi, μm))(3)Optimization problem: DRPR learns the representation of F so that the predicted assignmentmatrix ψ(F, M, π) = Ψ is similar to Y. Given the optimal condition properties of the BSCP statedin Section 2.1, the optimal values of M and π also depend on Ψ and are therefore variables of ourdimensionality reduction problem that we formulate:min ∆n (ψ(F,M,π),Y)F,M,π(4)The function ∆n(Ψ, Y) is an empirical discrepancy loss between the predicted assignment matrix Ψand the target assignment matrix Y. Since the rows of Ψ and Y represent probability distributions,
Figure 2: (a) Original 3-dimensional dataset containing 6 clusters (one color per cluster); (b) 2Drepresentation obtained with t-SNE by exploiting the original 3D representation as input; (c) 2Drepresentation obtained with t-SNE by exploiting soft probability scores w.r.t. the 6 clusters; (d) 2Drepresentation obtained by our method by exploiting using the same supervision as (c). The relativeinter-cluster distances of the original dataset are preserved with our approach, unlike t-SNE.
Figure 4: MNIST (left half) and STL (right half) representations of DRPR (left) and t-SNE (right)illustration in Fig. 3). Examples that are between multiple clusters (usually in the middle of figures)are those that are harder to classify by the model. Detailed visualizations and analysis are provided inthe supplementary material.
Figure 3: Level sets representing responsibil-ities of the green cluster whose center is thegreen circle. Grey circles are the centers ofother clusters. The responsibility of the greencluster increases for the examples that are lo-cated in the spike at the right side of the figure.
Figure 5: CIFAR 100 representation learned by t-SNE when using softmax representations as inputFigure 6: Visualization of the representation learned on MNIST by our approach in the supervisedhard clustering setup. The left (resp. right) figure is the representation learned by our model when itsoutput dimensionality is d = 2 (resp. d = 3).
Figure 6: Visualization of the representation learned on MNIST by our approach in the supervisedhard clustering setup. The left (resp. right) figure is the representation learned by our model when itsoutput dimensionality is d = 2 (resp. d = 3).
Figure 7: (left) Visualization obtained with t-SNE on the toy dataset when replacing the `2 distancein the original space by the KL divergence to compare probability distribution representations. (right)Visualization obtained with t-SNE on the toy dataset when replacing the `2 distance in the originalspace by the Jensen Shannon divergence.
Figure 8: CIFAR 10 representation of DRPR and t-SNEC.4.2 Adapting t-SNE with other divergencesWe plot in Fig. 7 the visualization obtained by t-SNE when using the KL or JS divergences tocompare pairs of probability distribution representations. The representations obtained in this caseare still worse than using the original 3-dimensional representations as the cluster structures are notpreserved, nor the inter-cluster distances. This suggests that comparing pairs of examples, as doneby t-SNE, is less appropriate than our method that considers similarities between examples and thedifferent k = 6 clusters.
Figure 9: CIFAR 100 representation of DRPR and t-SNEFigures 10, 11, 12 and 13 illustrate the representations learned by our model for the STL, MNIST,CIFAR 100 and CIFAR 10 datasets, respectively. Instead of using colors that represent the categoriesof the embeddings as done in the submitted paper, we directly plot the images.
Figure 10: STL representation learned by DRPR.
Figure 11: MNIST representation learned by DRPR.
Figure 12: CIFAR 100 representation learned by DRPR.
Figure 13: CIFAR 10 representation learned by DRPR.
