Figure 1: The empirical histograms of (a) log σp in teacher WaveNet and (b) log σq in student IAFduring density distillation using reverse KLreg divergence.
Figure 2: (a) Text-to-wave model converts textual features into waveform. All components feed theirhidden representation to others directly. (b) Bridge-net maps frame-level hidden representation tosample-level through several convolution blocks and transposed convolution layers interleaved withSoftsign non-linearities. (c) Convolution block is based on gated linear unit.
Figure 3: The negative log-likelihoods (per dimension) of Gausssian WaveNet on hold-out audiosduring training. The learning rates in Adam optimizer are initially set to 0.001 and annealed by halffor every 200K steps.
Figure 4: The empirical histograms of predicted log σ (before clipping) in Gaussian WaveNet withdifferent clipping constants during training.
