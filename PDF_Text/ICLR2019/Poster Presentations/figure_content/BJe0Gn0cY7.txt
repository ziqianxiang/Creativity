Figure 1: Effect of δ in a toy model. Fitting an uncorrelated Gaussian for the posterior, qφ(z), to acorrelated Gaussian prior, pα(z), by minimizing DKL(qφ(z)kpα(z)) over φ. Left: committed rate(δ) as a function of the prior squared correlation α and the dimensionality n. Right: contours of theoptimal posterior and prior in 2d. As the correlation increases, the minimum rate grows.
Figure 2: Generative structures for the inference of sequential latent variables. The anti-causalstructure introduces an inductive bias to encode in each latent variable information about the future3	Related WorkThe main focus of our work is on representation learning and density modeling in latent variablemodels with powerful decoders. Earlier work has focused on this kind of architecture, but hasaddressed the problem of posterior collapse in different ways.
Figure 3: Random samples from our ImageNet 32 × 32 model. Each column in Fig. 3a showsmultiple samples from p(x∣z) for a fixed Z 〜Paux(z). Each image in Fig. 3b is decoded using adifferent sample from paux (z).
Figure 4: Comparison of CIFAR-10 test performance of δ-VAEs vs. models trained with free-bitsand β-VAE across many rates. δ-VAE is significantly more stable, achieves competitive densityestimation results across different rates, and its learned representations perform better in the down-stream linear classification task.
Figure 5: Architecture for imagesThe exact hyper-parameters of our network is detailed in Table 4. We used dropout only in ourdecoder and applied it the activations of the hidden units as well as the attention matrix. As in(Vaswani et al., 2017), we used rectified linear units and layer normalization (Ba et al., 2016) afterthe multi-head attention layers. We found layer normalization to be essential for stabilizing training.
Figure 6: Rate-Distortion for CIFAR-10 training set.
Figure 7: t-SNE plot of the posterior mean for 3000 CIFAR-10 images. Note the adjacent groupsand mixed regions of the plot: cats and dogs images are mostly interspersed as are automobiles andtrucks.The highest concentration of horses are on top of the region right above where deer examplesare.
Figure 8: Additional ImageNet samples. Top left: Each column is interpolation in the latent space.
Figure 9: Additional CIFAR-10 samples. Top left: Each column is interpolation in the latent space.
Figure 10: Random samples from the auxiliary (left) and AR(1) (right) priors of our high-rate (top)and low-rate(bottom) CIFAR-10 models. The high-rate (low-rate) model has -ELBO of 2.90 (2.83)and KL of 0.10 (0.01) bits/dim. Notice that in the high rate model that has a larger value of α,samples from the AR(1) prior can turn out too smooth compared to natural images. This is becauseof the gap between the prior and the marginal posterior, which is closed by the auxiliary prior.
Figure 11: Additional unconditional random samples from Imagenet 32x32. Each half-column ineach block contains 4 decodings of the same sample Z 〜Paux(Z)21Published as a conference paper at ICLR 2019==== Interpolating dimension 0 ====The company's stock price is also UP for a year-on-year rally, when theThe company,s shares are trading at a record high for the year, when they were trading atThe company,s shares were trading at $3.00, down from their 52-week lowThe company,s shares fell $1.14, or 5.7 percent, to $ UNKThe company, which is based in New York, said it would cut 1,000 jobs in theThe two-day meeting, held at the White House, was a rare opportunity for the United StatesThe company, which is based in New York, said it was looking to cut costs, but addedThe company is the only company to have a significant presence in China.
Figure 12:	One at a time interpolation of latent dimensions of a sample from the AR(1) prior. Thesentences of each segment are generated by sampling a 32 element sequence of 2D random vec-tors from the autoregressive prior, fixing one dimension interpolating the other dimension linearlybetween μ ± 3σ.
Figure 13:	One at a time interpolation of latent dimensions of a sample from the auxiliary prior. Thegeneration procedure is identical to Fig. 12 with the exception that the initial vector is sampled fromthe auxiliary prior.
Figure 14:	Text completion samples. For each sentence we prime the decoder with a fragment of arandom sample from the validation set (shown in bold), and condition the decoder on interpolationsbetween two samples from the latent space.
