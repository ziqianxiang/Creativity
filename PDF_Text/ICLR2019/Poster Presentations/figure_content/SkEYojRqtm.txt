Figure 1: 2D visualization. (a). Visualization of word embeddings trained from vanilla Trans-former (Vaswani et al., 2017) in Englishâ†’German translation task. (b). Visualization of wordembeddings trained from Word2Vec (Mikolov et al., 2013). (c). Visualization of hidden states andcategory embedding of a classification task, where different colors stand for different categories andthe blue triangles denote for category embeddings.
Figure 2: (a): Word embeddings trained from MLE-CosReg. (b): Singular values of embeddingmatrix. We normalize the singular values of each matrix so that the largest one is 1.
Figure 3: (a): WMT 2014 English-German Dataset preprocessed with BPE. (b): word-levelWikiText-2. In the two figures, the x-axis is the token ranked with respect to its frequency in de-scending order. The y-axis is the logarithmic value of the token frequency.
