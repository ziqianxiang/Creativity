Figure 1: An illustration of DeepCube. The training and solving process is split up into ADI andMCTS. First, we iteratively train a DNN by estimating the true value of the input states usingbreadth-first search. Then, using the DNN to guide exploration, we solve cubes using Monte CarloTree Search. See methods section for more details.
Figure 2: Visualizations of the Rubik’s Cube. (a) and (b) show the solved cube as it appears in theenvironment. (c) and (d) show the cube reduced in dimensionality for input into the DNN. Stickersthat are used by the DNN are white, whereas ignored stickers are dark.
Figure 3: Visualization of training set generation in ADI. (a) Generate a sequence of training inputsstarting from the solved state. (b) For each training input, generate its children and evaluate the valuenetwork on all of them. (c) Set the value and policy targets based on the Bellman Operator.
Figure 4: Distribution of solution lengths for DeepCube, Kociemba, and Korf. The left graph featuresnaive DeepCube to evaluate the effect of our shortest path search. The right graph features the Korfoptimal solver to evaluate how well DeepCube can find short solutions. The red lines represent the26 and 15 move upper bound on the left and right respectively.
Figure 5: An example of Deep-Cube’s strategy. On move 17of 30, DeepCube has createdthe 2x2x2 corner while group-ing adjacent edges and cornerstogether.
Figure 6: Comparison of the distribution of frequency of the two types of triplets. We split the tripletsinto conjugations (aba-1) and non-conjugations. We then calculate the frequency of each triplet andplot the two distributions. The two vertical lines are the means of their respective distributions.
Figure 7: Architecture for fθ .
