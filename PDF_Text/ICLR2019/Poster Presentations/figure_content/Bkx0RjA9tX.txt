Figure 1: Examples of questions that can be answered using only some question words (underlined).
Figure 2: Probabilities of generating question words given different answers for a standard andan adversarial SQuAD question, allowing us to interpret which questions words are explained bythe answer. In the standard setting, the model places greater probability on the question words thatappear near Isaac Newton, such as force compared to Galileo. In the adversarial setting, the questionword Earth distinguishes the true answer from the distractor.
Figure 3: Architecture of GQA decoder. Multiple inputs to a layer indicates concatenation. Blocksafter the first are connected with an additional residual connection, and LSTM cells also receivetheir state at time t - 1 as an input.
Figure 4: Final layer attention maps and word probabilities during question generation on a CLEVRvalidation question, when predicting the highlighted word. (1) The model considers all the rubberobjects for predicting the next word. (2) Objects to the left of a rubber object are considered. (3) Itdescribes the brown cylinder. (4, 5) Word distributions show the model understands the next words,but interestingly its attention focuses on the set of two objects meeting the constraints. In all cases,the word probability distributions are heavily skewed towards semantically valid choices.
