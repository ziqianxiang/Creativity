Figure 1: Left: Outline of the phenomena discussed in the paper. Curvature along thesharpest direction(s) initially grows (A to C). In most iterations, we find that SGD crossesthe minimum if restricted to the subspace of the sharpest direction(s) by taking a too largestep (B and C). Finally, curvature stabilizes or decays with a peak value determined bylearning rate and batch size (C, see also right). Right two: Representative example of theevolution of the top 30 (decreasing, red to blue) eigenvalues of the Hessian for a SimpleCNNmodel during training (with η = 0.005, note that η is close to λ^- = 160).
Figure 2: Top: Evolution of the top 10 eigenvalues of the Hessian for SimpleCNN andResnet-32 trained on the CIFAR-10 dataset with η = 0.1 and S = 128. Bottom: Zoom onthe evolution of the top 10 eigenvalues in the beginning of training. A sharp initial growthof the largest eigenvalues followed by an oscillatory-like evolution is visible. Training andtest accuracy of the corresponding models are provided for reference.
Figure 3: Full batch-size training of Resnet-32 for η = 0.01 and (left) η = 0.05 (right) onCIFAR-10. Evolution of the top 10 eigenvalues of the Hessian and accuracy are shown ineach case. The training is unstable; an initial growth of curvature scale is followed by asudden drop in accuracy. The CIFAR-10 dataset was subsampled to 2500 points.
Figure 4: Evolution of the two largest eigenvalues (solid and dashed line) of the Hessianfor Resnet-32, SimpleCNN, and LSTM (trained on the PTB dataset) models on a log-scalefor different learning rates (top) and batch-sizes (bottom). Blue/green/red correspond toincreasing η and decreasing S in each figure. Left side of the vertical blue bar in each plotcorresponds to the early phase of training. Larger learning rate or a smaller batch-sizecorrelates with a smaller and earlier peak of the spectral norm and the next largest eigenvalue.
Figure 5:	Average cosine between mini-batch gradient (y axis) and top sharpest directions(averaged over top 5) for different η (color) evaluated at different level of accuracies, duringtraining (x axis). For comparison, the horizontal purple line is alignment with a randomvector in the parameter space. Curves were smoothed for clarity.
Figure 6:	Early on in training, SGD finds a region such that in the sharpest direction, the SGDstep length is often too large compared to curvature. Experiments on SimpleCNN and Resnet-32, trained with η = 0.01, S = 128, learning curves are provided in the Appendix. Lefttwo: Average change in loss E[L(θ(t) — αηgι(t))] 一 L(θ(t)), for α = 0.5,1, 2 correspondingto red, green, and blue, respectively. On average, the SGD step length in the direction of thesharpest direction does not minimize the loss. The red points further show that increasingthe step size by a factor of two leads to increasing loss (on average). Right two: Qualitativevisualization of the surface along the top eigenvectors for SimpleCNN and Resnet-32 supportthat SGD step length is large compared to the curvature along the top eigenvector. Atiteration t We plot the loss L(θ(t) + keι∆θι(t)), around the current parameters θ(t), where∆θι(t) is the expected norm of the SGD step along the top eigenvector e> The x-axisrepresents the interpolation parameter k, the y-axis the epoch, and the z-axis the loss value,the color indicated spectral norm in the given epoch (increasing from blue to red).
Figure 7: Evolution of the loss and the top eigenvalue during training with variations ofSGD with parameter updates restricted to certain subspaces. All variants are initialized atthe last iteration in Fig. 1. An SGD variant (orange) that follows at each iteration only theprojection of the gradient on the top eigenvector of H effectively finds a region with lowerλmaxbut increases the loss, while a variant subtracting this pro jection from the gradient(red), finds a sharper region while achieving a similar loss level as vanilla SGD (blue).
Figure 8: Nudged-SGD for an increasing number of affected sharpest directions (K) optimizessignificantly faster, whilst generally finding increasingly sharp regions. Experiment is runusing Resnet-32 and the Cifar-10 dataset. Left and center: Validation accuracy and theλmax and Frobenius norm (y axis, solid and dashed) using increasing K (blue to red) comparedagainst SGD baseline using the same η (black), during training (x axis). Rightmost: Testaccuracy (red) and Frobenius norm (blue) achieved using NSGD with an increasing K (xaxis) compared to an SGD baseline using the same η (blue and red horizontal lines).
Figure 9: Full-batch training with Batch-Normalization is more stable. Evolution of the top10 eigenvalues of the Hessian, and accuracy, for Resnet-32 trained on the CIFAR-10 datasetwith η = 0.01 (left) and η = 0.05 (right).
Figure 10: Same as Fig. 4, but for the VGG-11 architecture.
Figure 11: Same as Fig. 4, but for Resnet-32 architecture using Batch-Normalization layers.
Figure 12: Depending on the length of the first stage of a learning rate schedule, spectralnorm of the Hessian and generalization performance evolve differently. Left: Spectral normof the Hessian (solid) and Frobenius norm (dashed), during training (x axis). Center:Accuracy and validation accuracy, during training (x axis). Right: Learning rate, duringtraining (x axis).
Figure 13: Momentum limits the maximum spectral norm of the Hessian throughout training.
Figure 14:	Training and validation accuracy for experiments in Fig 6. Left corresponds toSimpleCNN. Right corresponds to Resnet-32.
Figure 15:	Similar to Fig 6 but computed for the third eigenvector of the Hessian. We seethat the loss surface is much flatter in this direction. To facilitate a direct comparison, scaleof each axis is kept the same as in Fig. 6.
Figure 16:	Similar to Fig 6 but for the fourth eigenvector. We see that the loss surface ismuch flatter in this direction. To facilitate a direct comparison, scale of each axis is kept thesame as in Fig. 6.
Figure 17:	Similar to Fig 6 but for Resnet-32 (top) and SimpleCNN (bottom) with η = 0.05.
Figure 18: Similar to Fig 6 but for Resnet-32 (top) and SimpleCNN (bottom) run for 200epochs.
Figure 19: Evolution of the spectral norm and Frobenius norm of the Hessian (y axis, solidand dashed line, respectively) for different γ (color) for the experiment using a larger learning(left) and momentum (right), see text for details.
Figure 20: Same as Fig. 6, but for NSGD with γ = 0.01 and K = 5.
Figure 21: Same as Fig. 4, but for CNN model on the IMDB dataset.
Figure 22: NSGD experiments on IMDB dataset. Evolution of the validation accuracy (left)and spectral norm of the Hessian (left) for different γ.
