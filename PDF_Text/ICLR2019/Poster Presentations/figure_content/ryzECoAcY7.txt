Figure 1: An ant agent uses a 3-level hierarchy to traverse though rooms to reach its goal, representedby the yellow cube. ∏2 uses as input the current state (joint positions θ and velocities θ) and goalstate (yellow box) and outputs a subgoal state (green box) for Π1 to achieve. Π1 takes in the currentstate and its goal state (green box) and outputs a subgoal state (purple box) for Π0 to achieve. Π0takes in the current state and goal state (purple box) and outputs a vector of joint torques.
Figure 2: An example episode trajectory for a simple toy example. The tic marks along the trajectory showthe next states for the robot after each primitive action is executed. The pink circles show the original subgoalactions. The gray circles show the subgoal states reached in hindsight after at most H actions by the low-levelpolicy.
Figure 3: Episode sequences from the four rooms (top) and inverted pendulum tasks (bottom). In the fourrooms task, the k=2 level agent is the blue square; the goal is the yellow square; the learned subgoal is thepurple square. In the inverted pendulum task, the goal is the yellow sphere and the subgoal is the purple sphere.
Figure 4: Average success rates for 3-level (red), 2-level agent (blue), and flat (green) agents in eachtask. The error bars show 1 standard deviation.
Figure 5: Figure compares the performance of HAC (2 Levels) and HIRO. The charts show theaverage success rate and 1 standard deviation.
Figure 6: Results from the ablation studies examining our subgoal testing procedure. We compare our imple-mentation to two other options: (i) no subgoal testing and (ii) an implementation in which all missed subgoalsare penalized even when lower levels use noisy policies to try to achieve the subgoal state that is being tested.
