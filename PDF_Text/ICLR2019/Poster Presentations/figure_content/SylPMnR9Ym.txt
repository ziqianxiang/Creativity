Figure 1: Using latent composition to recover actions from passive data. a) Two sequences startingfrom different initial states but changing according to the same actions. Without requiring labels, ourmodel learns to represent the action in sequences like these identically. We train a representation Z tocapture the dynamics of the scene and its compositional structure: applying (z1 and z2) should havethe same effect as applying the composed representation g(z1, z2). These properties capture the factthat effector systems, such as a robot arm, use the same composable action space in many differentstates. b) The learned action space Z recovered by our method (PCA visualization). Points are coloredby the true action u: true actions can be easily decoded from z, validating that the structure of theaction space has been captured.
Figure 2: Components of the proposed architecture. Left: The stochastic video prediction model,shown for one timestep. During training, we estimate the latent variable zt using the approximateinference network (MLPinfer, CNNe) from the current and previous image. At test time, we produceZt using the prior distribution p(z)〜N(0, I). Future frames are estimated by passing Zt togetherwith images xt-1 through the generative network (LSTM, CNNd). Please refer to Appendices Aand B for architectural details. Right: Composability training. Latent samples Z are concatenatedpairwise and passed through the composition network MLPcomp that defines a distribution over ν inthe trajectory space. A sampled value of ν is decoded into an image through the same generativenetwork (LSTM and CNNd) and matched to the final image in the composed sequence.
Figure 3: Visualization of the learned action space, Z, on the reacher dataset. Each of the 1000 pointsdepicts a value of Z for a different frame pair from the dataset. We plot the projection of Z onto thefirst two principal components of the data. Each point is colored by the value of the ground truthrotation, in radians, depicted in the two images used to infer Z for that point. a) The latent spacelearned by the baseline model has no discernible correspondence to the ground truth actions. b) Ourmethod learns a latent space with a clear correspondence to the ground truth actions. In the Appendix,Fig. 15 further investigates why the baseline fails to produce a disentangled representation.
Figure 4: Illustration of how the learned representation canbe used for a) action-conditioned prediction by inferring thelatent variable, Zt , from the action, and b) visual servoing bysolving the control problem in latent space through iteratedrollouts and then mapping the latent variable to robot controlactions, ut .
Figure 5: Transplantation of action representations Z from one sequence to another. We infer actionrepresentations from the donor sequence and use them to create the recipient sequences from adifferent initial state. a) the reacher dataset. The previous frame is superimposed onto each frameto illustrate the movement. b) the BAIR dataset. The previous and the current position of the endeffector are annotated in each frame (red and blue dots, respectively) to illustrate the movement.
Figure 6: Visual servoing on the reacher task. Left: Planned and executed servoing trajectories. Eachof the first five rows shows the trajectory re-planned at the corresponding timestep. The first imageof each sequence is the current state of the system, and the images to the right of it show the modelprediction with the lowest associated cost. The target state (the reacher pointing to the upper left)is shown superimposed over each image. Right: Data efficiency measured as final distance to thegoal after servoing, shown depending on the number of videos used in training. Each point representsa model trained on a dataset with a restricted number of action-annotated training sequences. Fullresults are in Table 4 in the appendix.
Figure 7: Error histogram of the angle detection algorithm on the reacher training set. The output ofthis algorithm is used as a form of surrogate ground truth to evaluate model performance.
Figure 8: Trajectory transplantation with differing visual characteristics. The trajectory from thetop sequence is transplanted to a different environment and initial state in each of the two bottomsequences. Our model achieves almost perfect accuracy, which validates that it has indeed learneda representation of actions disentangled from the static content, such as the background, agent'sappearance, and the initial state. The previous frame is superimposed onto each frame to illustrate themovement. Top: dataset with varying backgrounds. Bottom: dataset with varying robots. Additionalgenerated videos are available at: https://daniilidis-group.github.io/learned_action_spaces/.
Figure 10: Learning from agents with varied visual appearance. Left: Sample agent configurationsfrom the training set. We cover a variety of visual appearances (i.e. arm lengths and widths) butnot the configuration used for testing. Right: Test time servoing example after pre-training onobservations of agents with varied visual appearances. The figure layout follows the layout of Fig. 6(left).
Figure 9: Servoing examples with randomly sampled static CIFAR-10 backgrounds. The figurelayout follows the layout of Fig. 6 (left).
Figure 11: Typical sequences sampled from the stochastic video prediction model. In the past, thesamples Z are generated from the approximate inference distribution and match the ground truthexactly. In the future, Z is sampled from the prior, and correspond to various possible futures. Thesethree sequences are different plausible continuations of the same past sequence. This shows thatthe model is capable of capturing the stochasticity of the data. Only five of ten predicted frames areshown for clarity. Additional generated videos are available at: https://daniilidis-group.
Figure 12: Typical action-conditioned prediction sequences on the reacher dataset. EaCh exampleshows top: the ground truth sequence, middle: our predictions, bottom: predictions of the baselinemodel (Denton & Fergus (2018)). To illustrate the motion, We overlay the previous position of thearm in each image (transparent arm). Our method produces sequences that are perfectly aligned withthe ground truth. The baseline never matches the ground truth motion and is only slightly better thanexecuting random actions. Best viewed on a computer, additional generated videos are available at:https://daniilidis-group.github.io/learned_action_spaces/.
Figure 13: Failure cases of the baseline model on trajectory transplantation. Each example showstop: the ground truth sequence, middle: our predictions, bottom: predictions of the baseline model(Denton & Fergus (2018)). The position of the end effector at the current (blue) and previous (red)timestep is annotated in each frame. The baseline often produces images with two different robot armsand other artifacts. Only six of ten predicted frames are shown for clarity. Best viewed on a computer,additional generated videos are available at: https://daniilidis-group.github.io/learned_action_spaces/.
Figure 14: Baseline failure cases on action-conditioned video prediction. Each example shows top:ground truth sequence, middle: our predictions, bottom: Denton & Fergus (2018) baseline. Theprevious and the current position of the end effector are annotated in each frame. The baselineoften produces images with two different robot arms and other artifacts. Only six of ten predictedframes are shown for clarity. Best viewed on a computer, additional generated videos are available at:https://daniilidis-group.github.io/learned_action_spaces/.
Figure 15: Visualization of the structure of the learned latent space of the baseline model withoutcomposability training on the reacher dataset. The visualization is done in the same manner as in Fig.
