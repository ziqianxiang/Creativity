Figure 1: Illustrating the neural persistence calculation of a network with two layers (l0 and l1 ).
Figure 2: Neural persistence values of trained perceptrons (green), diverging ones (yellow), randomGaussian matrices (red), and random uniform matrices (black). We performed 100 runs per cat-egory; dots indicate neural persistence while crosses indicate the predicted lower bound accordingto Theorem 2. The bounds according to Theorem 1 are shown as dashed lines.
Figure 3: Comparison of mean normalized neural persistence for trained networks without modific-ations (green), with batch normalization (yellow), and with 50% of the neurons dropped out duringtraining (red) for the ‘MNIST’ data set (50 runs per setting).
Figure 4: The visualizations depict the differences in accuracy and epoch for all comparison scen-arios of mean normalized neural persistence versus validation loss, while the table summarizes theresults on other data sets. Final test accuracies are shown irrespectively of early stopping to put theaccuracy differences into context.
