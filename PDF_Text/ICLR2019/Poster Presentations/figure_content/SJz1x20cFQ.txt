Figure 1: Environmental reward for our low-level policies, with and without phase conditioningFor Ant, phase-conditioning gives a respectable boost, but both achieve reasonable rewards andeasily move. For Humanoid, vanilla PPO (using a widely used implementation (Kostrikov, 2018))using the same default hyperparameters performs poorly, even though it performs well for all otherstandard Open AI Gym Mujoco tasks. With the phase conditioning, however, using PPO, we areable to achieve good results with Humanoid surviving and moving forward.
Figure 2: Traces of low-level policies after 100 timesteps of movement using different random seeds4.2	Mazes and navigationThe high-level policy network is trained according to Section 4.2, also using PPO. See Appendix Efor more training and network details.
Figure 3: Results on Fixed Ant Cross Maze and comparison to Haarnoja et al. (2018a). We showthe training curves for each method, showing the final distance of the agent to the goal.
Figure 4: Training curves on the Random Ant Cross and Skull Maze environments. We again showthe average final distance to the goalAnt Cross Maze Movements-5.0	-2.5	0.0	2.5	5.0	7.5	10.0	12.5	15.0Ant Skull Maze Movements-10.0	-7.5	-5.0	-2.5	0.0	2.5	5.0	7.5	10.0Figure 5: Traces of our method navigating in the ant Ant Cross and Skull MazesIn Figure 6, we show the results using Humanoid on a smaller version of the cross maze. Becausethe PPO without phase fails at the low-level policy, we omit all non-phase results. The low-levelHumanoid control problem is much harder, and moves less quickly (as we saw in Figure 2), so ourmethod does not do as well as it did on Ant. But it is still able to successfully reach the goal muchof the time, and again the baselines again fail to learn the maze. To our knowledge, ours is the onlywork that shows results on a Humanoid maze task.
Figure 5: Traces of our method navigating in the ant Ant Cross and Skull MazesIn Figure 6, we show the results using Humanoid on a smaller version of the cross maze. Becausethe PPO without phase fails at the low-level policy, we omit all non-phase results. The low-levelHumanoid control problem is much harder, and moves less quickly (as we saw in Figure 2), so ourmethod does not do as well as it did on Ant. But it is still able to successfully reach the goal muchof the time, and again the baselines again fail to learn the maze. To our knowledge, ours is the onlywork that shows results on a Humanoid maze task.
Figure 6: Training curves of our method and baseline on the Random Cross Maze Humanoid envi-ronment. This maze is smaller than the Ant version, so all curves start closer to the goal.
Figure 7: Additional experiments and comparisons on Ant5 DiscussionIn this work we present a simple, yet effective way of solving difficult sparse reward RL problems.
