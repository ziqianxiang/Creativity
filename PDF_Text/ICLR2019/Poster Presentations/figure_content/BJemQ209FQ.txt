Figure 1: Curriculum learning. Final state is used as the initial state for training the navigator agent. Thenavigator is assigned only with the easier sub-instruction {to: â€œLON"}. We detail this process in Section 4.3.
Figure 2: Action depen-dency graph for hierarchi-cal Q learning. D denotespicking a DOM element, Cdenotes a click or type ac-tion, and T denotes gener-ating a type sequence.
Figure 3: Network architectures a) QWeb and b) INET with attribute pointing. Boxes indicate fully connectedlayers (FC) with ReLU (for instruction encoding) or tanh (for shallow encoding) activation. (K, V) indicatesembeddings of key and value pairs on the instruction; <Elem> shows the leaf DOM element embeddings.
Figure 4: Performance of QWeb on a subset of Miniwob and Miniwob++ environments compared to previousstate-of-the-art approaches. AR denotes augmented reward.
Figure 5: Performance of variants of QWeb and MetaQWeb on the book-flight-form environment. SE, CI,CG, and AR denote shallow encoding, curriculum with warm-start, curriculum with simulated sub-goals, andaugmented reward, respectively.
