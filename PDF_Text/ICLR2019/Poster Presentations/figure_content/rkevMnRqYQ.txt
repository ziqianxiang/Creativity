Figure 1: An illustration of learning preferences from an initial state. Alice attempts to accomplish agoal in an environment with an easily breakable vase in the center. The robot observes the state ofthe environment, so, after Alice has acted for some time from an even earlier state s-τ. It considersmultiple possible human reward functions, and infers that states where vases are intact usually occurwhen Alice,s reward penalizes breaking vases. In contrast, it doesn,t matter much what the rewardfunction says about carpets, as we would observe the same final state either way. Note that while weconsider a specific s-T for clarity here, the robot could also reason using a distribution over s-T.
Figure 2: Evaluation of RLSP on our environments. Silhouettes indicate the initial position of anobject or agent, while filled in version indicate their positions after an agent has acted. The first rowdepicts the information given to RLSP. The second row shows the trajectory taken by the robot whenfollowing the policy πspec that is optimal for θspec. The third row shows the trajectory taken whenfollowing the policy πRLSP that is optimal for θfinal = θAlice + λθspec. (a) Side effects: Room with vase(b) Distinguishing environment effects: Toy train (c) Implicit reward: Apple collection (d) Desirableside effect: Batteries (e) “Unseen” side effect: Room with far away vase.
Figure 3: Reward achieved byπRLSP, as a fraction of the ex-pected reward of the optimalpolicy, for different values ofAlice’s planning horizon T .
Figure 4: Comparison of the Additive and Bayesian methods. We show how the percentage oftrue reward obtained by πRLSP varies as we change the tradeoff between θAlice and θspec . The zerotemperature case corresponds to traditional value iteration; this often leads to identical behavior andso the lines overlap. So, we also show the results when planning with soft value iteration, varying thesoftmax temperature, to introduce some noise into the policy. Overall, there is not much differencebetween the two methods. We did not include the Apples environment because θspec is uniformlyzero and the Additive and Bayesian methods do exactly the same thing.
