Figure 1: Signal-to-noise ratios (SNR), bias squared, and variance of gradient estimators with in-creasing K over 10 random trials with 1000 measurement samples per trial (mean in bold). Theobserved “bias” for IWAE-DReG is not statistically significant under a paired t-test (as expectedbecause IWAE-DReG is unbiased). IWAE-DReG is unbiased, its SNR increases with K, and it hasthe lowest variance of the estimators considered here.
Figure 2: MNIST generative modeling trained according to IWAE (left), RWS (middle), and JVI(right). The top row compares the variance of the original gradient estimator (dashed) with thevariance of the doubly reparameterized gradient estimator (solid). The bottom row compares testperformance. The left and middle plots show the IWAE (stochastic) lower bound on the test set.
Figure 3:	Log-likelihood lower bounds for generative modeling on MNIST. The left and middleplots compare performance with different number of samples K = 32, 256. For clarity the legendis shared between the plots. The bold lines are the average over three trials, and individual trialsare displayed as semi-transparent). The right plot compares performance as the convex combinationbetween IWAE-DReG and RWS-DReG is varied (Eq. 10). To highlight differences, we plot thedifference between the test IWAE bound and the test IWAE bound IWAE-DReG achieved at thatstep.
Figure 4:	Log-likelihood lower bounds for structured prediction on MNIST. The left plot uses K =64 samples and the right plot uses K = 256 samples. For clarity the legend is shared betweenthe plots. The bold lines are the average over three trials, and individual trials are displayed assemi-transparent). The right plot compares performance as the convex combination between IWAE-DReG and RWS-DReG is varied (Eq. 10). To highlight differences, we plot the difference betweenthe test IWAE bound and the test IWAE bound IWAE-DReG achieved at that step.
Figure 5: Omniglot generative modeling trained according to IWAE (left), RWS (middle), and JVI(right). The top row compares the variance of the original gradient estimator (dashed) with thevariance of the doubly reparameterized gradient estimator (solid). The bottom row compares testperformance. The left and middle plots show the IWAE (stochastic) lower bound on the test set.
Figure 6: Log-likelihood lower bounds for structured prediction on Omniglot. The left plot usesK = 64 samples and the right plot uses K = 256 samples. For clarity the legend is shared betweenthe plots. The bold lines are the average over three trials, and individual trials are displayed assemi-transparent). The right plot compares performance as the convex combination between IWAE-DReG and RWS-DReG is varied. To highlight differences, we plot the difference between the testIWAE bound and the test IWAE bound IWAE-DReG achieved at that step.
Figure 7:	Structured prediction on MNIST according to IWAE (left), RWS (middle), and JVI (right).
Figure 8:	Variance of the gradient estimators on the MNIST generative modeling task. We plotthe trace of the variance of the doubly reparameterized gradient estimator relative to the originalgradient estimator for IWAE (left), RWS (middle), and JVI (right) as the number of samples (K) isvaried.
