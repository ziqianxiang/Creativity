Figure 1: Plots of the first two principal components of (a) word embeddings (Pennington et al.,2014), (b) digit-sequence embeddings learned by an autoencoder (Section 2), and (c) sentences(InferSent: Conneau et al. 2017). All demonstrate systematicity in the learned vector spaces.
Figure 2: (a) A unidirectional sequence-to-sequence autoencoder. (b) The tensor product operation.
Figure 3: (a) The filler-role bindings assigned by the six role schemes to two sequences, 3116 and523197. Roles not shown are assigned the null filler. (b) The trees used to assign tree roles to thesesequences. (c) Substitution accuracy for three architectures at the autoencoding task with six roleschemes. Each bar represents an average across five random initializations.
Figure 4: Heatmap of substitution accuracies with various filler and role embedding dimensions.
