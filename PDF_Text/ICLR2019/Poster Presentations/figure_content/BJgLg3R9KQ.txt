Figure 1: The ClickMe interface for hu-man participants and their DCN partners.
Figure 2: (A) A representative selection of ILSVRC12 images and their ClickMe maps. Thetransparency channel of these images is set to the proportion of ClickMe bubbles for that locationacross participants, making feature visibility reflect importance. Animals are outlined in blue andnon-animals in red. (B) Features identified in “top-down” ClickMe maps are more diagnostic forobject recognition than those identified in “bottom-up” Salicon maps. A rapid visual categorizationexperiment compared human performance in detecting animals when features were revealed accordingto ClickMe maps (blue curve) or Salicon maps (red curve). ClickMe-masked and Salicon-maskedimage exemplars are shown for the condition in which 100% of important features are visible,demonstrating how “bottom-up” saliency is not necessarily relevant to the task. For clarity, we omitteddata between 1-10% of features visible from this plot where accuracy was chance for participants ofboth groups. Error bars are S.E.M. ***: p <0.001 (statistical testing with randomization tests detailedin the Appendix).
Figure 3: The global-and-local (GALA) module learns to combine local saliency with global contex-tual signals to guide attention towards image regions that are diagnostic for object recognition. Thediagram here depicts a GALA module applied to the feature activity from a convolutional layer in adeep network model, U. Information in the diagram flows from left to right, and U is processed withseparate local (Flocal) and global (Fglobal) operators to derive the local- and global-attention masks,which is integrated into the attention activity A. Attention is applied to the original activity, U, withelementwise multiplication (Fscale), to yield U0. A GALA module can optionally be supervised byClickMe maps (Mclickmaps in the yellow box) with an additional loss Lclickmaps that evaluates thedistance between these maps and A. Minimizing this loss encourages GALA to select visual featuresfavored by humans (see Section 4 for details).
Figure 4: Exemplars from the Microsoft COCO 2017 detection challenge validation set (zoom infor detail). The top panel depicts a random subset of these images depicting object categories alsopresent in ILSVRC12. In the middle panel, each of these images is shown with the transparencyset to the attention map it yielded in a GALA-ResNet-50 trained with ClickMe map supervision.
Figure 5: Distribution of participants per ClickMe map.
Figure 6: ClickMe map exemplars.
Figure 7: Training the GALA-ResNet-50 with ClickMe maps improves object classification per-formance and drives attention towards features selected by humans. We screened the influence ofClickMe maps on training by measuring model accuracy after training on a range of values of λ,which scales the contribution of ClickMe maps on the total model loss. A model optimized only forobject recognition uses features that explain 62.96% of variability in human ClickMe maps, which isconsistent with a ResNet-50 trained without attention (dashed red line). Incorporating ClickMe mapsin the loss yields a large improvement in predicting ClickMe maps (87.62%) as well as classificationaccuracy. The fraction of explained ClickMe map variability for each model is plotted as its ratio tothe human inter-rater reliability, and the dotted red line depicts the floor inter-rater reliability (shufflednull).
Figure 8: A GALA-ResNet-50 trained with ClickMe supervision uses visual features that are moresimilar to those used by human observers than a GALA-ResNet-50 trained without such supervisionon images held out from model training and validation. ClickMe maps highlight object parts thatare deemed important by human observers. The difference between normalized smoothed gradientimages (Smilkov et al., 2017) from each network shows relative feature preferences between networks(Gradient ∆). Image pixels preferred by GALA-ResNet-50 with ClickMe are red, and those preferredby a GALA-ResNet-50 without ClickMe are blue, depicting the preference for local features of theformer over the latter. The column-wise L2 norm of each network’s GALA modules reveals highlyinterpretable object and part-based attention for the ClickMe GALA-ResNet-50 (in red) vs. lessinterpretable and more diffuse attention for the vanilla GALA-ResNet-50 (in blue).
