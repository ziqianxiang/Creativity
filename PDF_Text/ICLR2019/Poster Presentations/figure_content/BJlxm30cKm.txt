Figure 1: Histograms of forgetting events on (from left to right) MNIST, permutedMNIST andCIFAR-10. Insets show the zoomed-in y-axis.
Figure 2: Pictures of unforgettable (Top) and forgettable examples (Bottom) of every CIFAR-10class. Forgettable examples seem to exhibit peculiar or uncommon features. Additional examplesare available in Supplemental Figure 15.
Figure 3: Distributions of forgetting events across training examples in CIFAR-10 when 20% oflabels are randomly changed. Left. Comparison of forgetting events between examples with noisyand original labels. The most forgotten examples are those with noisy labels. No noisy examplesare unforgettable. Right. Comparison of forgetting events between examples with noisy labels andthe same examples with original labels. Examples exhibit more forgetting when their labels arechanged.
Figure 4: Synthetic continual learning setup for CIFAR-10. Background color in each column indi-cates the training partition, curves track performance on both partitions during interleaved training.
Figure 5: Left Generalization performance on CIFAR-10 of ResNet18 where increasingly larger sub-sets of the training set are removed (mean +/- std error of 5 seeds). When the removed examples areselected at random, performance drops very fast. Selecting the examples according to our orderingcan reduce the training set significantly without affecting generalization. The vertical line indicatesthe point at which all unforgettable examples are removed from the training set. Right Differencein generalization performance when contiguous chunks of 5000 increasingly forgotten examples areremoved from the training set. Most important examples tend to be those that are forgotten the most.
Figure 6: Decrease in generalization performance when fractions of the training sets are removed.
Figure 7: Left. Ranking of examples by forgotten events stabilizes after 75 epochs in CIFAR-10.
Figure 8: From left to right, distributions of the first presentation at which each unforgettable andforgettable example was learned in MNIST, permutedMNIST and CIFAR-10 respectively. Rescaledview where the number of examples have been capped between 0 and 1500 for visualization pur-poses. Unforgettable examples are generally learnt early during training, thus may be considered as“easy” in the sense of Kumar et al. (2010), i.e. may have a low loss during most of the training.
Figure 9: Left 2D-histogram of the number of forgetting events and mean misclassification marginacross all examples of CIFAR-10. There is significant negative correlation (-0.74, Spearman rankcorrelation) between mean misclassification margin and the number of forgetting events.
Figure 10: Right: precision and recall of retrieving the unforgettable examples from a full run ofResNet18 (200 epochs), using the example ordering after 25, 50, and 75 epochs. The unforgettableexamples are retrieved with high precision and recall after 50 epochs. Left: same plot for the 17kexamples with the most forgetting events.
Figure 11: Distribution of forgetting events across all training examples in CIFAR-10 when alltraining images are augmented with increasing additive Gaussian noise. The presence of increas-ing amount of noise decreases the amount of unforgettable examples and increases the amount ofexamples in the second mode of the forgetting distribution.
Figure 12: Distribution of forgetting events across all training examples in CIFAR-10 when random20% of training examples undergo pixel noise (σnoise = 10) (Left) or label noise (Right) (same asFigure 3). We observe that the forgetting distribution under pixel noise resembles the one underlabel noise.
Figure 13: Histogram of forgetting events under true and random gradient steps. (Right) Zoomed-inversion where the number of forgetting events is capped at 3 for visualization.
Figure 14: 95% confidence interval on for-getting events averaged over 5 seeds.
Figure 15: Additional pictures of the most unforgettable (Left) and forgettable examples (Right) ofevery CIFAR-10 class, when examples are sorted by number of forgetting events (ties are brokenrandomly). Forgettable examples seem to exhibit peculiar or uncommon features.
Figure 16: Left: distribution of forgetting events in CIFAR-100. Right: distribution of forgettingevents in CIFAR-10 when 20% of the labels are changed at random. The distribution of forgetting inCIFAR-100 is much closer to that of forgetting in the noisy CIFAR-10 than it is to forgetting in theoriginal datasets presented in Figure 1.
Figure 17: The 36 most forgotten examples in CIFAR-100. Note that they are all images that ap-pear under multiple labels (not pictured: the ”girl” image also appears under the label ”baby”, the"mouse" image also appears under "shrew"，one of the 2 images of ‘oak_tree' appears under 'wil-low_tree' and the other under ,maple.tree,.
Figure 18: Generalization performance on CIFAR-100 of ResNet18 where increasingly larger sub-sets of the training set are removed (mean +/- std error of 5 seeds). When the removed examplesare selected at random, performance drops faster. Selecting the examples according to our orderingreduces the training set without affecting generalization.
