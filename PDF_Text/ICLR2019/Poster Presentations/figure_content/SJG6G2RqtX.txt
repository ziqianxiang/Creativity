Figure 1: Comparison between a random map of the VIN dataset, and a few randomconfiguration of our training environment. In our custom grid-worlds, the number of blocksincreases with size, but their percentage over the total available space is kept fixed. Agentand goal are shown as circles for better visualization, however they still occupy a single cell.
Figure 2: Average, min and max reward of all the models as they train on our curriculum.
Figure 3: Average, min, and max, test win rate obtained on our dymamic experiments. Eachagent was trained on the 8x8 instances of the scenario in a similar fashion to the static worldexperiments. Figure 3d shows an example of policy obtained after training on a avalanchetesting configuration. Agent and goal are shown as circles for better visualization, howeverthey still occupy a single cell.
Figure 4: StarCraft navigation results. Figure 4a shows a generated trajectory on a randomscenario at late stages of training. The red and blue overlays (not shown to the agent)indicate the distance required to interact with each enemy entity.
