Figure 1: The Discriminator-Actor-Critic imitation learning framework combined with a method toexplicitly learn rewards for the absorbing states.
Figure 2: We depict an episode of MDP with an absorbing state. The absorbing state transitions toitself with zero reward.
Figure 3: Comparisons of algorithms using 4 expert demonstrations. y-axis corresponds to normal-ized reward (0 corresponds to a random policy, while 1 corresponds to an expert policy).
Figure 4: Even without training, some reward functions can perform well on some tasks.
Figure 5:	Effect of absorbing state handling on Kuka environments. For these environments, we usehuman demonstrations as expert trajectories, and GAIL framework with a positive reward function.
Figure 6:	Effect of learning absorbing state rewards when using an AIRL discriminator within theDAC Framework in OpenAI Gym environments.
Figure 7: Comparisons of different algorithms given the same number of expert demonstrations.
Figure 8: Renderings of our Kuka-IIWA environment. Using a VR headset and 6DOF controller, ahuman participant can control the 6DOF end-effector pose in order to record expert demonstrations.
