Figure 1: Disentangling the role of network topology, equivariance, and invariance on testperformance, for SGD-trained and infinitely wide Bayesian networks (NN-GPs). Accuracy (%)on CIFAR10 of different models of the same depth, nonlinearity, and weight and bias variances. (a)Fully connected models - FCN (fully connected network) and FCN-GP (infinitely wide BayesianFCN) underperform compared to (b) LCNs (locally connected network, a CNN without weightsharing) and CNN-GP (infinitely wide Bayesian CNN), which have a hierarchical local topologybeneficial for image recognition. As derived in §5.1: (i) weight sharing has no effect in the Bayesiantreatment of an infinite width CNN (CNN-GP performs similarly to an LCN), and (ii) pooling has noeffect on generalization of an LCN model (LCN and LCN with pooling perform nearly identically).
Figure 2: A sample 2D CNN classifier annotated according to notation in §2.1, §3. The networktransforms n0 X d0 = 3 X 8 X 8-dimensional inputs y0(x) = X ∈ X into n3 = 10-dimensional logitsW2 (x). Model has two convolutional layers with k = (3,3)-ShaPed filters, nonlinearity φ, and a fullyconnected layer at the top (y2(χ) → N(x), §3.1). Hidden (Pre-)activations have n1 = n2 = 12filters. As min n1, n2 → ∞, the prior of this CNN will approach that of a GP indexed by inputsx and target class indices from 1 to nW3 = 10. The covariance of such GP can be computed as6×⅛ pa=。. he。/)2 (K 0 )]α,α+σ0 In, where the sum is over the {1,..., 4}2 hypercube(see §2.2, §3.1, §C). Presented is a CNN with stride 1 and no (valid) padding, i.e. the spatial shapeof the input shrinks as it propagates through it d0 = (8, 8) → d1 = (6, 6) → d2 = (4, 4) . Notethat for notational simplicity 1D CNN and circular padding with d0 = d1 = d2 = d is assumedin the text, yet our formalism easily extends to the model displayed (§C). Further, while displayed(pre-)activations have 3D shapes, in the text we treat them as 1D vectors (§2.1).
Figure 3: Visualization of sample finite neural networks and their infinitely Wide Bayesiancounterparts (NN-GPs). Presented are networks With nonlinearity φ, L = 2 hidden layers thatregress n3 = 10-dimensional outputs Z (x) for each of the 4 (1, 2, 3, 4) inputs X from the datasetX. A hierarchical computation performed by a NN corresponds to a hierarchical transformationof the respective covariance matrix (resulting in Kvec 0 I10). Top two rows: a fully connectednetwork (FCN, above) and the respective FCN-GP (below). Bottom two rows: a 1D CNN withno (Valid)Padding, andthe respective CNN-GP∙ The analogy between the two models (NN andGP) is qualitatively similar to the case of FCN, modulo additional spatial dimensions in the internalactivations and the respective additional entries in intermediary covariance matrices. Note that ingeneral d = 10 pixels would induce a 10 X 10-fold increase in the number of covariance entries (inK0, K∞∞, ... ) compared to the respective FCN, yet without pooling only a small fraction of them(displayed) nee ds be computed to obtain the top-layer GP covariance Kvec 0 I10 (see §3.1).
Figure 4: Different dimensionality collapsing strategies described in §3. Validation accuracy ofan MC-CNN-GP with pooling (§3.2.1) is consistently better than other models due to translationinvariance of the kernel. CNN-GP with zero padding (§3.1) outperforms an analogous CNN-GP without padding as depth increases. At depth 15 the spatial dimension of the output withoutpadding is reduced to 1 × 1, making the CNN-GP without padding equivalent to the center pixelselection strategy (§3.2.2) - which also performs worse than the CNN-GP (We conjecture, dueto overfitting to centrally-located features) but approaches the latter (right) in the limit of largedepth, as information becomes more uniformly spatially distributed (Xiao et al., 2018). CNN-GPsgenerally outperform FCN-GP, presumably due to the local connectivity prior, but can fail to capturenonlinear interactions between spatially-distant pixels at shallow depths (left). Values are reportedon a 2K/4K train/validation subset of CIFAR10. See §G.3 for experimental details.
Figure 5: Convergence of NN-GP Monte Carlo estimates. Validation accuracy (left) of an MC-CNN-GP increases with n × M (channel count × number of samples) and approaches that of theexact CNN-GP (not shown), while the distance (right) to the exact kernel decreases. The darkband in the left plot corresponds to ill-conditioning of KnL,+M1 when the number of outer productscontributing to KnL,+M1 approximately equals its rank. Values reported are for a 3-layer model appliedto a 2K/4K train/validation subset of CIFAR10 downsampled to 8 × 8. See Figure 10 for similarresults with other architectures and §G.2 for experimental details.
Figure 6:	(a): SGD-trained CNNs often perform better with increasing number of channels.
Figure 7:	Aspects of architecture and inference influencing test performance. Test accuracy(vertical axis, %) for the best model within each model family (horizontal axis), maximizing vali-dation accuracy over depth, width, and training and initialization hyperpameters ("No Undefitting”means only models that achieved 100% training accuracy were considered). CNN-GP outperformsSGD-trained models optimized with a small learning rate to 100% train accuracy. When SGDoptimization is allowed to underfit the training set, there is a significant improvement in general-ization. Further, when ReLU nonlinearities are paired with large learning rates, the performance ofSGD-trained models again improves relative to CNN-GPs, suggesting a beneficial interplay betweenReLUs and fast SGD training. These differences in performance between CNNs and CNN-GPs arenot observed between FCNs and FCN-GPs, or between LCNs and LCN-GPs (Figure 1), suggest-ing that equivariance is the underlying factor responsible for the improved performance of finiteSGD-trained CNNs relative to infinite Bayesian CNNs without pooling. Further comparison be-tween SGD-trained CNNs and CNN-GPs with pooling (omitted due to computational limitations),where both models do share the property of invariance, would be an interesting direction for futureresearch. See Table 1 for further results on other datasets, as well as comparison to GP performancein prior literature. See §G.5 for experimental details.
Figure 8: Validation loss convergence. Best validation loss (vertical axis) of trained neural net-works (dashed line) as the number of channels increases (horizontal axis) approaches that of arespective (MC-)CNN-GP (solid horizontal line). See Figure 6 (b) for validation accuracy, Figure9 for training loss and §G.1 for experimental details.
Figure 9:	No underfitting in small models. Training loss (vertical axis) of best (in terms of vali-dation loss) neural networks as the number of channels increases (horizontal axis). While perfect 0loss is not achieved (but 100% accuracy is), we observe no consistent improvement when increasingthe capacity of the network (left to right). This eliminates underfitting as a possible explanationfor why small models perform worse in Figure 6 (b). See Figure 8 for validation loss and §G.1 forexperimental details.
Figure 10:	Convergence of NN-GP Monte Carlo estimates. As in Figure 5, validation accuracy(left) of MC-GPs increases with n × M (i.e. width times number of samples), while the distance(right) to the the respective exact GP kernel (or the best available estimate in the case of CNN-GPwith pooling, top row) decreases. We remark that when using shared weights, convergence is sloweras smaller number of independent random parameters are being used. See §G.2 for experimentaldetails.
Figure 11:	Large depth performance of NN-GPs. Validation accuracy of CNN- and FCN-GPs asa function of weight (σω2 , horizontal axis) and bias (σb2, vertical axis) variances. As predicted in §B,the regions of good performance concentrate around the critical line (phase boundary, right) as thedepth increases (left to right). All plots share common axes ranges and employ the erf nonlinearity.
