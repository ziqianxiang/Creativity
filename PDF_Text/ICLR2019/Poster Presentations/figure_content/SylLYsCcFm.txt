Figure 1: A visual analogy problem. In this example, the model must (1) identify a relation(Progression ) on a particular domain ( shape quantity ) in the source sequence (top), and (2)apply it to a different domain (line color ) in order to find the candidate answer panel thatcorrectly completes target sequence (bottom). There are seven possible domains and four possiblerelations in the dataset.
Figure 2: In LABC, the multiple choice can-didates are all semantically plausible, in thatthey are all consistent completions of the targetdomain using some relation. Only the correctanswer uses the same relation as the sourcedomain, so, the only way to solve the prob-lem is to use analogical reasoning. In contrast,perceptually plausible incorrect candidates areconsistent with the target domain attributes butnot the relations and all other possible candi-dates are inconsistent with the target domainrelation and attributes.
Figure 3: (a), (b), (c) Three types of visual reasoning questions. Each question requires a differentdegree of analogy making, with the question on the right demanding the most fluid and abstractapplication of the underlying relation. (d) Learning analogies by contrasting. When learning bycontrasting, each answer choice is consistent with a relational structure in the target sequence. Onlythe correct answer choice is consistent with relations in both the source and target domains. Thisforces the network to consider the source sequence to infer the correct structure.
Figure 4: Results of the three experiments in the visual analogy domain for a network thatlearns from random candidate answers, by contrasting abstract structures or both types ofquestion interleaved. Bar heights depict the means across eight seeds in each condition; standarderrors were < 0.01 for each condition (not shown - see the appendix Table 4 for the values)I	I	Learning Analogies by	ContrastingI	I	Normal TrainingI	I	Mixed trainingci completes a decoy relation * = r with the target sequence. LABC ensures that, during training,models have no alternative but to first observe a relation r in the source domain and consider andcomplete the same relation in the target domain - i.e. to execute a full analogical reasoning step.2Note that in all experiments reported below, we generated 600,000 training questions, 10,000validation questions and test sets of 100,000 questions. These data will be published with the paper.
Figure 5: LABC supports emergent relational representations. Principle component analysis(PCA) (right) and t-SNE analysis (left) of RNN hidden states. Each dot represents a (64-dimensional)state coloured according to the relation type and domain of the corresponding question.
Figure 6: Structure alignment and mapping on symbolic, numeric data. In this task a particularstructure is implemented as a set-function, which in the depicted example is f = min. For example,the “answer” a1 could denote the minimum size from the sizes in the source symbolic vector set.
Figure 7: Examples of visual analogy problems. These visual analogy examples have been selectedfrom the interpolation test set. The correct multiple choice candidate for each problem is highlightedin green. In the top half, we have randomly chosen examples where our RNN model trained withLABC selects the correct answer. In the bottom half, we have randomly selected examples whereour RNN model trained with LABC chooses the incorrect candidates (highlighted in red). Theperformance of this model on the interpolation test set is 93%.
