Figure 1: StrokeNet architecture. The generator part of the model outputs 256 × 256 images. Theposition encoder encodes input coordinate into 64 × 64 spatial feature for each point. The agentdecodes different information about the stroke using three parallel FC-decoders.
Figure 2: Recurrent version of StrokeNet. Two separate CNNs are used as encoders for the agent.
Figure 3: Illustration of how a stroke is rendered.
Figure 4: Images from our three-body dataset.
Figure 5:	Agent trained on MNIST. MNIST sam-ple (left), generator output (middle), WebApp re-construction (right). 1.5 epochs.
Figure 6:	Agent trained on Omniglot datasetlearns to “sketch” the characters. Layout is thesame as Figure 5. 104 iterations.
Figure 7:	Agent trained on QuickDraw. Sam-ple (left), reconstruction (right). 6 recurrent steps.
Figure 8:	Agent trained on KanjiVG dataset.
Figure 10: Latent space arithmetics. (a) and (b)demonstrate different attributes of the digits.
Figure 11: Comparison of loss curves betweenStrokeNet (ours) and SPIRAL.
Figure 12: 16-step color image reconstruction.
Figure 13: Comparison of stroke orders betweenhuman and agent. We can see the stroke order iscompletely chaotic compared to natural order.
Figure 14: Training loss of generator and agent. The agent loss equals to the l2 distance betweenthe generator output and agent input plus the penalty term constraining the average point distancewithin a stroke. For (c) and (d) the learning rate is set to 10-4, batch size equals to 64.
Figure 15: A trained StrokeNet generates imagesthat resemble the output of painting software. Thefirst row depicts results generated by our model(left) and by the software (right) given the sameinput. The second row shows the model could pro-duce strokes with color and texture using simplearithmetic operations. The third and fourth rowshows the model’s ability to draw MNIST digits(left) on both its own generative model (middle)and real-world painting software (right).
Figure 16: (a) A trained position encoder mapstwo (xi , yi , pi) tuples to two feature maps. (b)Each pair of neighbouring features are added to-gether to eliminate sparsity and preserve sequen-tial information. (c) A trained brush encoder en-codes color and radius information into a spatialfeature which is later concatenated to the end ofposition features.
Figure 17: The generator tries to predict what thereal environment would ouput given the same in-put stroke data. Software output (left), generatorprediction (right).
Figure 18: Interpolation across four digits. In thecorners are the four MNIST samples.
