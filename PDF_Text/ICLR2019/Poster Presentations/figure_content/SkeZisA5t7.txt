Figure 1:	The graphs show the discrepancy of maximum activation values for a ReLU network. Onthe left is the maximum value across the whole network, on the right we also show the breakdownby layers. Here the last ReLU layer dominates the maximum values of the whole network. Thered area represents the source of inaccuracy brought by binning with non-adaptive range. Similarly,yellow area leads to inaccuracy when all layers are binned using one maximum value.
Figure 2:	Entropy-based binning allows making accurate mutual information estimates for any dis-tribution of hidden activity, without adjusting the number of bins to the magnitude of the hiddenactivity.
Figure 3:	One network initialization visualized of the information plane using two different binningestimators. Each line on the information plane represents a single network layer, with colour schemeindicating the epoch during training. Leftmost layer is the output layers, rightmost layer is the closestto the data.
Figure 4:	Information planes for the same network as in Figure 3, produced with non-adaptive KDEand the adaptive version. (a) Hidden layers show no compression, slight compression visible in theleftmost output layer. (b) Two-penultimate hidden layers undergo compression, as well as the outputlayer.
Figure 5:	Plots produced with EBAB for the same network configuration using ReLU. The variationof behaviour is caused solely by the stochasticity of the initialization. While the variation of be-haviour of the leftmost output layer is modest, the hidden layers have vastly different trajectories onthe information planes. ReLU compression happens only in Figure (a) which is marked by an initialleftward movement of two hidden layers. In Figure (b) some fitting is observed in hidden layers,which is signified by a rightward movement. In Figure (c) hidden layers do not compress or fit todata, the I(X, T ) values are stationary for hidden layers.
Figure 6:	The information plane averaged over 50 network initializations using ReLU activationfunction in the hidden layers. For comparison Figure (b) shows an information plane of an averagednetwork using saturating tanh activation function.
Figure 7:	Information planes of networks, using different non-saturating activation functions in thehidden layers. However all networks used softmax function in the output layer. Therefore, theshapes of the leftmost lines on all the information planes show less variation. For every activationfunction 50 random initializations were trained and averaged mutual information values were usedfor the information planes presented above.
Figure 8: Information plots of a network with ReLU using L2 regularization with different penalties.
Figure 8:	Plots (d)-(g) provide a detailed view of individual layer lines from plots (a)-(c), and areshown on a different scale. For reference, input layer has number 1, output layer is number 7. Colorsrepresent the trajectory transformation that corresponds to an increase in L2 regularization penalty.
Figure 9:	Scatter plots of compression scores compared with accuracies.
