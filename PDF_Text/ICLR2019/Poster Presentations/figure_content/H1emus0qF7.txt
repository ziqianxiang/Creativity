Figure 1: The hierarchical design we consider.
Figure 2: Learned representations (2D embeddings) of our method and a number of variants on aMuJoCo Ant Maze environment, with color gradient based on episode time-step (black for beginningof episode, yellow for end). The ant travels from beginning to end of a ⊃-shaped corridor along anx, y trajectory shown under XY. Without any supervision, our method is able to deduce this near-ideal representation, even when the raw observation is given as a top-down image. Other approachesare unable to properly recover a good representation.
Figure 3: Results of our method and a number of variants on a suite of tasks in 10M steps of training,plotted according to median over 10 trials with 30th and 70th percentiles. We find that outside ofsimple point environments, our method is the only one which can approach the performance oforacle x, y representations. These results show that our method can be successful, even when therepresentation is learned online concurrently while learning a hierarchical policy.
Figure 4: We investigate importance of various observation coordinates in learned representations ona difficult block-moving task. In this task, a simulated robotic ant must move a small red block frombeginning to end of a ⊃-shaped corridor. Observations include both ant and block x, y coordinates.
Figure 5: The tasks we consider in this paper. Each task is a form of navigation. The agent mustnavigate itself (or a small red block in 'Block' tasks) to the target location (green arrow). We alsoshow an example top-down view image (from an episode on the Ant Maze task). The image iscentered on the agent and shows walls and blocks (at times split across multiple pixels).
Figure 6: For our method, we utilize an objective based on Equation 8. The objective is similarto mutual information maximizing objectives (CPC; van den Oord et al. (2018)). We compare tovariants of our method that are implemented more in the style of CPC. Although we find that usinga dot product rather than distance function D is detrimental, a number of distance-based variants ofour approach may perform similarly.
Figure 7: We provide additional results comparing to variants of β-VAE (Higgins et al., 2016). Wefind that even with this additional hyperparameter, the VAE approach to representation learning doesnot perform well outside of the simple point mass environment. The drawback of the VAE is that it isencouraged to reconstruct the entire observation, despite the fact that much of it is unimportant andpossibly exhibits high variance (e.g. ant joint velocities). This means that outside of environmentswith high-information state observation features, a VAE approach to representation learning willsuffer.
Figure 8: We evaluate the ability of our learned representations to transfer from one task to another.
Figure 9: We replicate the results of Figure 2 but with representations learned according to datacollected by a random higher-level policy. In this setting, when there is even less of a connectionbetween the representation learning objective and the task objective, our method is able to recovernear-ideal representations.
Figure 10: Results of our method compared to the original formulation of HIRO (Nachum et al.,2018). The representation used in the original formulation of HIRO is a type of oracle - sub-goalsare defined as only the position-based (i.e., not velocity-based) components of the agent observation.
