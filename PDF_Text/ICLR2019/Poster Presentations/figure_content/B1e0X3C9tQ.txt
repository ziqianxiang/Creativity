Figure 1: (a) The red line shows the evolution of log γ, converging close to 0 during training asexpected. The two blue curves compare the associated pixel-wise reconstruction errors with Y fixedat 1 and with a learnable Y respectively. (b) The j-th eigenvalue of ∑z, denoted λj, should bevery close to either 0 or 1 as argued in Section 3. When λj is close to 0, injecting noise alongthe corresponding direction will cause a large variance in the reconstructed image, meaning thisdirection is an informative one needed for representing the manifold. In contrast, if λj is close to 1,the addition of noise does not make any appreciable difference in the reconstructed image, indicatingthat the corresponding dimension is a superfluous one that has been ignored/blocked by the decoder.
Figure 2: Singular value spectrums of latent sam-ple matrices drawn from qφ(z) (first stage) andqφ0 (u) (enhanced second stage).
