Figure 1: An example of our model for formula F = (p → q) ∨ (q → p). The initial vector w ispropagated through the tree recursive network part producing vectors p1, p2, q1, and q2, where p1corresponds to the first occurrence of the atom p in F. Vectors corresponding to the same atom areprocessed by RNN-Var and these results are then processed by RNN-All. The final output out isproduced by Final using the output of RNN-All.
Figure 2: Various choices of d used on a very simple variant of our network with a long short-termmemory (LSTM)—every Ci is a linear layer, RNN-Var and RNN-All are LSTMS with one recurrentlayer, and Final is a linear layer combined with log softmax.
Figure 3: RNN variants where 2 RNNs is TopDownNet (w is random and fixed). 1 RNN uses RNN-Var also as RNN-All and 1 RNN (padding) uses only RNN-Var where the sequences of distinctatoms are separated by padding.
Figure 4: The repeated applications of negation (c-) on v→ι (left) and v→2 (right), which are theresults of c→ (w), for a model where every ci is a linear layer with d = 8 and a learned w. Theindividual components of the vectors are shown.
