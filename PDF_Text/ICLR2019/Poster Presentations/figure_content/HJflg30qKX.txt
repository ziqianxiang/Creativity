Figure 1: Visualization of margin maximization and self-regularization of layers on synthetic datawith a 4-layer linear network compared to a 1-layer network (a linear predictor). Figure 1a shows theconvergence of 1-layer and 4-layer networks to the same margin-maximizing linear predictor on pos-itive (blue) and negative (red) separable data. Figure 1b shows the convergence of kWik2/kWikFto 1 on each layer, plotted against the risk.
Figure 2: A visualization of inter-layer alignment on data consisting of two well-separated circleswith a 3-layer linear network. Figure 2a depicts, as in Figure 1a, that optimizing 1- and 3-layerlinear networks finds the same maximum margin solution. The other three plots show the data as itis mapped through progressively more and more layers. Due to alignment, the product Wi …Wibecomes Uiu：, where Ui is the top left singular vector of Wi, which means that asymptotically themapped data will be well separated and lie along the span of ui , as depicted by the flattening inFigures 2b to 2d. Additionally, these three subfigures show that the top right singular vector vi+1 ofthe subsequent layer is aligned with this ui , which in these plots (with principal component axes)corresponds to following a horizontal line.
Figure 3: Risk and alignment of dense layers (the ratio kWik2/kWi kF) of (nonlinear!) AlexNet onCIFAR-10. Figure 3a uses default PyTorch initialization, while Figure 3b forces initial Frobeniusnorms to be equal amongst dense layers.
