Figure 1: APL model applied to the the classification of an Omniglot image. The encoded image iscompared to the entries of the memory and the most relevant ones are passed through a decoder thatoutputs a probability distribution over the labels. The dotted line indicates the parts of the graph thatare not updated via back-propagation.
Figure 2: Two of the three different decoder architectures of APL: relational feed-forward memory(left) and relational working memory (Santoro et al., 2018) (right). In the figure et corresponds tothe encoded target, e1...m to the encoded observations from the memory, l1...m = f (y1...m) are thelabels processed by an embedding layer and d1...m is the distance between et and e1...m.
Figure 3: APL training over several iterations. The encoder e embeds the query image that iscompared against the stored experiences in the memory M . Matches are fed alongside the encodedimage into a decoder d. Finally, the controller c decides whether the currently observed exampleshould be stored in memory.
Figure 4: a) Accuracy and size of memory for 5-way Omniglot. APL stops writing to memoryafter having 2 examples per class for 5-way classification. b), examples of evolution of posteriordistribution for 20-way classification for 3 images. The distribution starts as uniform for the very firststep, then starts to change as more items are added to memory. When the correct class is seen, thedistribution converges. c), the number of labels stored in memory per class is highly heterogeneous.
Figure 5: Evolution of top-1 accuracy and number of written examples to memory over a singleepisode for the Imagenet dataset. Curves are averages over 5 test episodes and smoothed withexponential moving average.
Figure 6: Left: Number analogy task. The colored symbols have unknown values that are consistentthroughout an episode. Right: Accuracy as a function of number of examples seen for the Analogytask where the number meanings are also shuffled each episode. Curves are averages over 10 testepisodes and smoothed with exponential moving average. On the left we fix the decoder (relationalself-attention feed-forward module) and vary k . As there are 100 possible combinations of symbols(10 numbers Ã— 10 symbols), the thick dashed line corresponds to the performance of a model capableof perfect 1-shot generalization. We can see that for k = 8 and k = 16 the decoder can infer thesymbol and number meanings to do better than direct 1-shot classifications. On the right we fixk = 16 and show that the relational self-attention feed-forward module can generalize better fromfew examples than other decoder architectures.
Figure 7: Multihead attention implementation.
Figure 8: The LSTM decoder for APL.
Figure 9:	Accuracy as a function of examples written to memory. We compared relational workingmemory, LSTM and the relational self-attention feed-forward module for omniglot on the 20-way,423-way and 1000-way tasks.
Figure 10:	Accuracy as a function of examples written to memory. Each plot corresponds to adifferent number k of retrieved nearest neighbors. We compared relational working memory, LSTMand the relational self-attention feed-forward module.
Figure 11:	Accuracy as a function of examples written to memory. Each plot corresponds to adifferent number k of retrieved nearest neighbors. We compared relational working memory, LSTMand the relational self-attention feed-forward module.
Figure 12: Accuracy of APL on a lifelong learning task where each task corresponds to learning 10new classes in Omniglot. The baseline is a progressive net where the convolutional encoder is pre-trained, and for every task a new logits layer is added and trained to classify the new classes. Resultsare the average accuracy over 5 runs. While not using any gradient information, APL performs aswell or better than progressive networks.
