Figure 1: The visualization of objects activation samples from a variance layer with two varianceneurons. The network was learned on a toy four-class classification problem. The two plots corre-spond to two different random initializations. We demonstrate that a variance layer can learn twofundamentally different kinds of representations (a) two neurons repeat each other, the informa-tion about each class is encoded in variance of each neuron (b) two neurons encode an orthogonalinformation, both neurons are needed to identify the class of the object.
Figure 2: The KL divergence between the Gaus-sian dropout posterior and different priors. TheKL-divergence between the Gaussian dropoutposterior and the Student’s t-prior is no longera function of just α; however, for small enoughν it is indistinguishable from the KL-divergencewith the log-uniform prior.
Figure 3: CIFAR-10 test set accuracy for aVGG-like neural network with layer-wise pa-rameterization with weights replaced by theirexpected values (deterministic), sampled fromthe variational distribution (sample), the test-time averaging (ensemble) and zero-mean ap-proximation accuracy. Note how the informa-tion is transfered from the means to the vari-ances between epochs 40 and 60.
Figure 5: Here we show that a variance layer canbe pruned up to 98% with almost no accuracydegradation. We use magnitude-based pruningfor σ ’s (replace all σij∙ that are below the thresh-old with zeros), and report test-time-averagingaccuracy.
Figure 4: Histograms of test accuracy of LeNet-5 networks on MNIST dataset. The blue his-togram shows an accuracy of individual weightsamples. The red histogram demonstrates thattest time averaging over 200 samples signifi-cantly improves accuracy.
Figure 6: Evaluation mean scores forCartPole-v0 and Acrobot-v1 environmentsobtained by policy gradient after eachepisode of training for deterministic, param-eter noise (Fortunato et al. (2017); Plappertet al. (2017)) and variance network policies(Section 3).
Figure 7: Results on iterative fast sign ad-versarial attacks for VGG-like architectureon CIFAR-10 dataset. For each iterationwe report the successful attack rate. Deepensemble of variance networks has a lowersuccessful attacks rate.
Figure 8: These are the learning curves for VGG-like architectures, trained on CIFAR-10 with layer-wise parameterization and with different prior distributions. These plots show that all three priorsare equivalent in practice: all three models converge to variance networks. The convergence forthe Student’s prior is slower, because in this case the KL-term is estimated using one-sample MCestimate. This makes the stochastic gradient w.r.t. log α very noisy when α is large.
Figure 9: Average distributions of activations of a variance layer for objects from different classesfor four random neurons. Each line corresponds to an average distribution, and the filled areascorrespond to the standard deviations of these p.d.f.s. Each neuron essentially has several “energylevels”, one for each class / a group of classes. On one “energy level” the samples have roughlythe same average magnitude, and samples with different magnitudes can easily be told apart withsuccessive layers of the neural network.
