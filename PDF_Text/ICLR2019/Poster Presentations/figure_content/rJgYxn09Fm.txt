Figure 1: Parameter sharing scheme. Left: A CNN (possibly a variant such as a residual network), with eachconvolutional layer i containing an individual parameter set W(i) . Middle: Parameter sharing among layers,where parameter templates T(1) , T(2) are shared among each layer i, which now only contains a 2-dimensionalparameter α(i). Weights W(i) (no longer parameters, illustrated with dotted boxes) used by layer i are generatedfrom α(i) and templates T(1), T(2). Right: If weights W(i) are outputs of a linear function (as in our method),learning parameter templates can be viewed as learning layer templates, offering a new (although equivalent)perspective for the middle diagram. Non-linearities are omitted for simplicity.
Figure 2: Connection between the LSM matrix S ( where Si,j = |表)：喘》| ) and the structure of the net-work. White and black entries correspond to maximum and minimum similarities (Si,j = 1 and Si,j = 0,respectively). Left: Empirically, CNNs present no similarity between parameters of different layers. Middle:Trained with our method, the layer similarity matrix (LSM) captures similarities between different layers, in-cluding pairs with close to maximum similarity. Such pairs (depicted by same-colored coefficients and weights,and by white entries in the LSM) perform similar operations on their inputs. Right: We can tie together pa-rameters of similar layers, creating a hard parameter sharing scheme. The network can then be folded, creatingself-loops and revealing an explicit recurrent computation structure.
Figure 3: Parameter efficiency for different models. On both CIFAR-10 and CIFAR-100, SWRNs are signifi-cantly more efficient than WRNs. DN and RNX denotes DenseNet and ResNeXt, respectively, and are plottedfor illustration: both models employ orthogonal efficiency techniques, such as bottleneck layers. Best viewedin color.
Figure 4: Extracting implicit recurrences from a SWRN 28-10-4. Left: Illustration of the stages of a SWRN-28-10-4 (residual connections omitted for clarity). The first two layers contain individual parameter sets, whilethe other six share four templates. All 3 stages of the network follow this structure. Middle: LSM for eachstage after training on CIFAR-10, with many elements close to 1. Hard sharing schemes can be created forpairs with large similarity by tying their coefficients (or, equivalently, their effective weights). Right: Foldingstages 2 and 3 leads to self-loops and a CNN With recurrent connections -LSM for stage 2 is a repetition of 2rows/columns, and folding decreases the number of parameters.
Figure 5: Shortest paths task. Best viewed in color.
Figure 6: SWRN 40-8-8 (8 parameter templates shared among groups of 40-4 — 2 = 10 layers) trained Withsoft parameter sharing on CIFAR-10. Each stage (originally with 12 layers - the first two do not participatein parameter sharing) can be folded to yield blocks With complex recurrences. For clarity, We use colors toindicate the computational flow: red takes precedence over green, which in turn has precedence over blue.
Figure 7: LSMs of a SWRN 40-8-8 (composed of 3 stages, each with 10 layers sharing 8 templates) trained onCIFAR-10 for 5 runs with different random seeds. Although the LSMs differ across different runs, hard param-eter sharing can be observed in all cases (off-diagonal elements close to 1, depicted by white), characterizingimplicit recurrences which would enable network folding. Moreover, the underlying structure is similar acrossruns, with hard sharing typically happening among layers i and i + 2, leading to a “chessboard” pattern.
Figure 8: LSMs of a SWRN 40-8-8 (composed of 3 stages, each with 10 layers sharing 8 templates) at differentepochs during training on CIFAR-10. The transition from an identity matrix to the final LSM happens mostlyin the beginning of training: at epoch 50, the LSM is almost indistinguishable from the final LSM at epoch 200,and most of the final patterns are observable already at epoch 25.
