Figure 1: (a) Example of a two-teacher knowledge flow. (b) DeeP net transformation of knowledgeflow. (c) Average normalized weights for teachers, and the student,s layers. At the beginning oftraining, the student heavily relies on teacher one. As training progresses, teacher one’s weightdecreases, and the student,s weight increases until the student is eventually independent.
Figure 2:	Comparison with progressive neural network and PathNet.
Figure 3:	Comparison with fine-tuning and baseline A3C on different combinations of environ-ment/teacher settings.
Figure 4: Comparison of top-1 accuracy of our approach, fine-tuning and baseline on the EMNISTLetters test dataset.
Figure 5: ComParison of toP-1 accuracy of our aPProach, fine-tuning and baseline on the STL-10 testdataset.
Figure 6:	ComParison with fine-tuning and baseline A3C on different combinations of environ-ment/teacher settings.
Figure 7:	Normalized weights for the teachers and the student in C10 experiments.
Figure 9: Ablation study regarding KL term. Seaquest and Riverraid experts are used as teachers forall experiments.
Figure 10:	Teachers’ architecture differs from the student’s architecture. Seaquest and Riverraidexperts are used as teachers for all experiments.
Figure 11:	Average network to compute θold . Riverraid expert is used as teacher for all experiments.
