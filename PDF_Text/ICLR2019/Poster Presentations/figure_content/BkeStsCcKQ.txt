Figure 2:	(Left) High-level perturbations do not induce a critical period. When the deficit onlyaffects high-level features (vertical flip of the image) or the last layer of the CNN (label permutation),the network does not exhibit critical periods (test accuracy remains largely flat). On the other hand,a sensory deprivation-like deficit (image is replaced by random noise) does cause a deficit, but theeffect is less severe than in the case of image blur. (Right) Dependence of the critical period profileon the network’s depth. Adding more convolutional layers increases the effect of the deficit duringits critical period (shown here is the decrease in test accuracy due to the deficit with respect to thetest accuracy reached without deficits).
Figure 3:	Critical periods in different DNN architectures and optimization schemes. (Left)Effect of an image blur deficit in a ResNet architecture trained on CIFAR-10 with learning rateannealing and (Center) in a deep fully-connected network trained on MNIST with a fixed learningrate. Different architectures, using different optimization methods and trained on different datasets,still exhibit qualitatively similar critical period behavior. (Right) Same experiment as in Figure 1,but using a fixed learning rate instead of an annealing scheme. Although the time scale of the criticalperiod is longer, the trends are similar, supporting the notion that critical periods cannot be explainedsolely in terms of the loss landscape of the optimization. (Bottom Left) Networks trained withoutweight decay have shorter and sharper critical periods. Gradually increasing the weight decay makesthe critical period longer, until the point where it stops training properly. (Bottom Right) Using adifferent optimization method (Adam) we observe a similar behavior to standard SGD.
Figure 4: Critical periods in DNNs are traced back to changes in the Fisher Information.
Figure 5: Normalized quantity of information contained in the weights of each layer as a functionof the training epoch. (Top Left) In the absence of deficits, the network relies mostly on the middlelayers (3-4-5) to solve the task. (Top Right) In the presence of an image blur deficit until epoch100, more resources are allocated to the higher layers (6-7) rather than to the middle layers. Theblur deficit destroys low- and mid-level features processed by those layers, leaving only the globalfeatures of the image, which are processed by the higher layers. Even if the deficit is removed, themiddle layers remain underdeveloped. (Top Center) When the deficit is removed at an earlier epoch,the layers can partially reconfigure (notice, e.g., the fast loss of information of layer 6), resulting inless severe long-term consequences. We refer to the redistribution of information and the relativechanges in effective connectivity as “Information Plasticity”. (Bottom row) Same plots, but using avertical flip deficit, which does not induce a critical period. As expected, the quantity of informationin the layers is not affected.
Figure 6: Log of the norm of the gradient means (solid line) and standard deviation (dashed line)during training when: (Left) No deficit is present, (Center) A blur deficit is present until epoch 70,and (Right) a deficit is present until the last epoch. Notice that the presence of a deficit does notdecrease the magnitude of the gradients propagated to the first layers during the last epochs, ratherit seems to increase it, suggesting that vanishing gradients are not the cause of the critical period forthe blurring deficit.
Figure 7:	Same plot as in Figure 5, but for a noise deficit. Unlike with blur, much more resourcesare allocated to the lower-layers rather than higher-layers. This may explain why it is easier for thenetwork to reconfigure to solve the task after the deficit is removed.
Figure 8:	Visualization of the filters of the first layer of the network used for the experiment inFigure 1. In absence of a deficit, the network learns high-frequency filters, as seen by the fact thatmany filters are not smooth (first picture). However, when a blurring deficit is present, the networklearns only smooth filters corresponding to low-frequencies of the input (third picture). If the deficitis removed after the end of the critical period, the network does not manage to learn high-frequencyfilters (second picture).
