Figure 1: Predictions on the toy funcction y = x3 . Here a × b represents a hidden layers of b units. Red dotsare 20 training points. The blue curve is the mean of final prediction, and the shaded areas represent standardderivations. We compare fBNNs and Bayes-by-Backprop (BBB). For BBB, which performs weight-spaceinference, varying the network size leads to drastically different predictions. For fBNNs, which perform function-space inference, we observe consistent predictions for the larger networks. Note that the 1 × 100 factorizedGaussian fBNNs network is not expressive enough to generate diverse predictions.
Figure 2: Extrapolating periodic structure. Red dots denote 20 training points. The green and blue linesrepresent ground truth and mean prediction, respectively. Shaded areas correspond to standard deviations. Weconsidered GP priors with two kernels: RBF (which does not model the periodic structure), and PER + RBF(which does). In each case, the fBNN makes similar predictions to the exact GP. In contrast, the standard BBB(BBB-1) cannot even fit the training data, while BBB with scaling down KL by 0.001 (BBB-0.001) manages tofit training data, but fails to provide sensible extrapolations.
Figure 4: Predictions on Mauna datasets. Red dots are training points. The blue line is the mean prediction andshaded areas correspond to standard deviations.
Figure 5: Bayesian Optimization. We plot the minimal value found along iterations. We compare fBNN,BBB and Random Feature methods for three kinds of functions corresponding to RBF, Order-1 ArcCosine andMatern12 GP kernels. We plot mean and 0.2 standard derivation over 10 independent runs.
