Figure 1: Left: An example illustrating the idea of a model patch. Right: An example of multi-task learning 2.
Figure 2: Function plots F (x; b(1), b(2)) for a 4-layer network given by equation 1 with k = 2 and all biasesexcept b(01) set to zero. From left to right: b(01) = 0, b(01) = -0.075, b(01) = -0.125 and b(01) = -0.425.
Figure 3: Performance of different fine-tuning approaches for different datasets for Mobilenet V2 and Incep-tion. The same color points correspond to runs with different initial learning rates, starting from 0.0045 to 0.45with factor 3. Best viewed in color.
Figure 4: The neural netWork is first trained to approximate class assignment shoWn in (a) (With the corre-sponding learned outputs in (c)), netWork parameters are then fine-tuned to match neW classes shoWn in (b). Ifall netWork parameters are trained, it is possible (d) to get a good approximation of the neW class assignment.
Figure 5: Plots similar to those shown in figure 4, but obtained for the embedding dimension of 4.
Figure 6: Plots similar to those shown in figure 4, but obtained for the embedding dimension of 8.
Figure 7: Original model trained to match class assignment shown in figure 4(a) results in theembedding shown in (a) that “folds” both circular regions together. After training the same modelon different classes (b), the new embedding (c) allows one to fine-tune the last layer alone to obtainoutputs shown in (d), (e) and (f).
Figure 8: Performance of different fine-tuning approaches for Mobilenet V2 and Inception V3 for Cifar100and Flowers. Best viewed in color.
Figure 9: Final accuracy as a function of learning rate. Note how full fine-tuning requires learning rate to besmall, while bias/scale tuning requires learning rate to be large enough.
