Figure 1: We present a novel approach to reinforcement learning that leverages an artificial networktrained on physiological signals correlated with autonomic nervous system responses.
Figure 2: An example of the blood volume pulse wave during driving in the simulated environment.
Figure 3: We used an eight-layer CNN (seven convolutional layers and a fully connected layer) toestimate the normalized pulse amplitude derived from the physiological response of the driver. Theinputs were the frames from the virtual environment, AirSim.
Figure 4: Frames from the environment ordered by the predicted pulse amplitude from our CNNintrinsic reward model. A lower value indicates a higher SNS/“fight or flight” response. This isassociated with more dangerous situations (e.g., driving close to walls and turning in tight spaces).
Figure 5: The graph plots average extrinsic reward per episode as the system evolves over timefor different values of λ. For all three tasks we observe that using appropriately balanced visceralrewards with the extrinsic reward leads to better learning rates when compared to either vanilla DQN(magenta triangle λ = 1) or DQN that only has the visceral component (red circle λ = 0). The errorbars in the plots correspond to standard error- non-overlapping bars indicate significant differences(p<0.05).
Figure 6: The graph plots average length per episode as the system evolves over time. For allthree tasks we observe that using visceral reward components leads to better longer episodes whencompared to vanilla DQN (λ = 1). This implies that the agent with the visceral reward componentbecomes more cautious about collisions sooner. The error bars in the plots correspond to standarderror- non-overlapping bars indicate significant differences (p<0.05).
Figure 7: Average velocity (left) and average length (right) per episode as the system evolves overtime. Blue) Performance with a time decaying contribution from the intrinsic reward (decaying at1/(No. of Episodes)). Red) Performance of the best λ at each episode (red lines) from the previousvelocity experiments (see Fig. 5 and 6). The episode length is superior with the time decayingintrinsic reward because we are directly optimizing for safety initially and the agent quickly learnsnot to crash.
Figure 8: Comparison ofthe CNN based intrinsic re-ward with a reward shap-ing mechanism. The plotsare (left) average extrinsic re-ward per episode and (right)length of episode as the sys-tem evolves and show the ad-vantages of the CNN basedapproach in both cases.
