Figure 1: RND exploration bonus over the course of the first episode where the agent picks up thetorch (19-21). To do so the agent passes 17 rooms and collects gems, keys, a sword, an amulet, andopens two doors. Many of the spikes in the exploration bonus correspond to meaningful events: losinga life (2,8,10,21), narrowly escaping an enemy (3,5,6,11,12,13,14,15), passing a difficult obstacle(7,9,18), or picking up an object (20,21). The large spike at the end corresponds to a novel experienceof interacting with the torch, while the smaller spikes correspond to relatively rare events that theagent has nevertheless experienced multiple times. See goo.gl/DGPC8E for videos.
Figure 2:	Novelty detection on MNIST: a predic-tor network mimics a randomly initialized targetnetwork. The training data consists of varying pro-portions of images from class “0” and a target class.
Figure 3:	Mean episodic return and numberof rooms found by pure exploration agents onMontezuma’s Revenge trained without accessto the extrinsic reward. The agents exploresmore in the non-episodic setting (see also Sec-tion 2.3). Curves are an average over 5 randomseeds.
Figure 4: Performance of different discount fac-tors for intrinsic and extrinsic reward streams. Ahigher discount factor for the extrinsic rewardsleads to better performance, while for intrinsic re-wards it hurts exploration. Curves are an averageover 5 random seeds.
Figure 5: Mean episodic return and number ofdiscovered rooms improve as the number of par-allel environments used for collecting the experi-ence increases. The runs have processed 0.5,2,4,and 16B frames. Curves are an average over 10random seeds.
Figure 6: Different ways of combining intrinsic and extrinsic rewards. Combining non-episodicstream of intrinsic rewards with the episodic stream of extrinsic rewards outperforms combiningepisodic versions of both steams in terms of number of explored rooms, but performs similarly interms of mean return. Single value estimate of the combined stream of episodic returns performs alittle better than the dual value estimate. The differences are more pronounced with RNN policies.
Figure 7: Mean episodic return of RND, dynamics-based exploration method, and PPO with extrinsicreward only on 6 hard exploration Atari games. RND achieves state of the art performance onGravitar, Montezuma’s Revenge, and Venture, significantly outperforming PPO on the latter two.
Figure 8: Comparison of recurrent and non-recurrent policies with the same number of pa-rameters with extrinsic reward discount factorsγE ∈ {0.99, 0.999}. Similar to the results inFigure 4, higher discount factors lead to betterperformance. Contrary to our expectations recur-rent policies perform worse than non-recurrentcounterparts. Curves are an average over 5 ran-dom seeds.
Figure 9: Comparison of RND with a CNN pol-icy with γI = 0.99 and γE = 0.999 with anexploration defined by the reconstruction errorof an autoencoder, holding all other choices con-stant (e.g. using dual value, treating intrinsicreturn as non-episodic etc). The performance ofthe autoencoder-based agent is worse than that ofRND, but exceeds that of baseline PPO. Curvesare an average over 5 random seeds.
