Figure 1: Over-parametrization phenomenon. Left panel: Training pre-activation ResNet18 architecture ofdifferent sizes on CIFAR-10 dataset. We observe that even when after network is large enough to completely fitthe training data(reference line), the test error continues to decrease for larger networks. Middle panel: Trainingfully connected feedforward network with single hidden layer on CIFAR-10. We observe the same phenomenaas the one observed in ResNet18 architecture. Right panel: Unit capacity captures the complexity of a hiddenunit and unit impact captures the impact of a hidden unit on the output of the network, and are important factorsin our capacity bound (Theorem 1). We observe empirically that both average unit capacity and average unitimpact shrink With a rate faster than 1 /vzh where h is the number of hidden units. Please See SupplementarySection A for experiments settings.
Figure 2:	Properties of two layer ReLU networks trained on CIFAR-10. We report different measures on thetrained network. From left to right: measures on the second (output) layer, measures on the first (hidden) layer,distribution of angles of the trained weights to the initial weights in the first layer, and the distribution of unitcapacities of the first layer. "Distance" in the first two plots is the distance from initialization in Frobenius norm.
Figure 3:	Behavior of terms presented in Table 1 with respect to the size of the network trained on CIFAR-10.
Figure 4: First panel: Training and test errors of fully connected networks trained on SVHN. Second panel:unit-wise properties measured on a two layer network trained on SVHN dataset. Third panel: number of epochsrequired to get 0.01 cross-entropy loss. Fourth panel: comparing the distribution of margin of data pointsnormalized on networks trained on true labels vs a network trained on random labels.
Figure 5: Left panel: Comparing network capacity bounds on CIFAR10 (unnormalized). Middle panel:Comparing capacity bounds on CIFAR10 (normalized). Right panel: Comparing capacity bounds on SVHN(normalized).
Figure 6: Different measures on fully connected netWorks With a single hidden layer trained on SVHN. Fromleft to right: measure on the output layer, measures in the first layer, distribution of angle to initial Weight in thefirst layer, and singular values of the first layer.
Figure 7: Different measures on fully connected networks with a single hidden layer trained on MNIST. Fromleft to right: measure on the output layer, measures in the first layer, distribution of angle to initial weight in thefirst layer, and singular values of the first layer.
Figure 8: Left panel: Training and test errors of fully connected networks trained on MNIST. Middle panel:Comparing capacity bounds on MNIST (normalized). Left panel: Comparing capacity bounds on MNIST(unnormalized).
