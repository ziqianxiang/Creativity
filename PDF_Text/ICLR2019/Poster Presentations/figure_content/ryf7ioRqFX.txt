Figure 1: The computational graph of a typical LSTM. Here we have omitted the inputs xi forconvenience. The top horizontal path through the cell state units ct s is the linear temporal pathwhich allows gradients to flow more freely over long durations. The dotted blue crosses along thecomputational paths denote the stochastic process of blocking the flow of gradients though the htstates (see Eq 2) during the back-propagation phase of LSTM. We call this approach h-detach.
Figure 2: Validation accuracy curves during training on copying task using vanilla LSTM (left) andLSTM with h-detach with probability 0.25 (middle) and 0.5 (right). Top row is delay T = 100 andbottom row is delay T = 300. Each plot contains multiple runs with different seeds. We see thatfor T = 100, even the baseline LSTM is able to reach ã€œ100% accuracy for most seeds and theonly difference we see between vanilla LSTM and LSTM with h-detach is in terms of convergence.
Figure 3: Validation accuracy curves of LSTM training on pixel by pixel MNIST. Each plot showsLSTM training with and without h-detach for different values of learning rate. We find that h-detach is both more robust to different learing rates and converges faster compared to vanilla LSTMtraining. Refer to the Fig. 6 in appendix for validation curves on multiple seeds.
Figure 4: The effect of removing gradient clipping from vanilla LSTM training vs. LSTM trainedwith h-detach on pixel by pixel MNIST dataset. Refer to Fig. 8 in appendix for experiments withmultiple seeds.
Figure 5: Validation accuracy curves for copying task T=100 (left) and pixel by pixel MNIST (right)using LSTM such that gradient is stochastically blocked through the cell state (the probability ofdetaching the cell state in this experiment is mentioned in sub-titles.). Blocking gradients fromflowing through the cell state path of LSTM (c-detach) leads to significantly worse performancecompared even to vanilla LSTM on tasks that requires long term dependencies. This suggests thatthe cell state path carry information about long term dependencies.
Figure 6: Validation accuracy curves on pixel by pixel MNIST dataset with vanilla LSTM trainingand LSTM training with h-detach with various values of learning rate and initialization seeds.
Figure 7: Validation accuracy curves on pMNIST dataset with vanilla LSTM training and LSTMtraining with h-detach.
Figure 8: The effect of removing gradient clipping during optimization. Validation accuracy curveson pixel by pixel MNIST dataset with vanilla LSTM training and LSTM training with h-detachwith various values of learning rate and initialization seeds. LSTM training using h-detach is bothsignificantly more stable and robust to initialization when removing gradient clipping compared withvanilla LSTM training.
