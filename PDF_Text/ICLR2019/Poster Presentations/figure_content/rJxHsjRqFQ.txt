Figure 1: An intuitive depiction of how images might be embedded in 2D. The location of theembeddings reflects the similarity between each image and that of a pug. Since the number of instanceswithin a given semantic distance from the central object grows exponentially, the Euclidean space isnot able to compactly represent such structure (left). In hyperbolic space (right) the volume growsexponentially, allowing for sufficient room to embed the images. For visualization, we have shrunk theimages in this Euclidean diagram, a trick also used by Escher.
Figure 2: The computational graph for the self-attention mechanism of the hyperbolic Transformer.
Figure 3: Left: Performance of the Recursive Transformer models on the Shortest Path LengthPrediction task on graphs of various sizes. The black dashed line indicates chance performance.
Figure 4: Left: Comparison of our models with low-capacity on the Sort-of-CLEVR dataset. The“EA”refers to the model that uses hyperbolic attention weights with Euclidean aggregation. Right:Performance of Relation Network extended by attention mechanism in either Euclidean or hyperbolicspace on the CLEVR dataset.
Figure 5: Relationships between different representations of points used in the paper. Left: Therelationship between pseudo-polar coordinates in Rn and the hyperboloid in Rn+1. Right: Projectionsrelating the hyperboloid, Klein and Poincare models ofhyperbolic space.
Figure 6: We show an example of a curriculum on the hyperbolic disk. In the first lesson, We take slicesfrom the graph only between angle 0 and π∕2. Inthe second lesson we will have to take the slice from 0to π.
Figure 7: An illustration of how trees can be represented in hyperbolic (left) and Euclidean geometry(right) in a cone. In hyperbolic space, as the tree grows the angles between the edges (θ) can bepreserved from one level to the next. In Euclidean space, since the number of nodes in the tree growsfaster than the rate that the volume grows, angles may not be preserved (θ to α). Lines in the leftdiagram are straight in hyperbolic space, but appear curved in this Euclidean diagram.
Figure 8: Hyperbolic embedding ofq (red) and k (blue) in a Poincare Ball on Cora dataset. Each pointcorresponds to a node in the graph. This visualization is obtained from a model trained with dropout.
Figure 9: Hyperbolic embeddings of q (red) and k (blue) in a Poincare Ball on Cora dataset. Eachpoint corresponds to a node in the graph. This visualization is obtained from a model trained withoutdropout. The figure on the left shows the embeddings going into the attention obtained from the firstlayer. The figure on the right shows the embedding of the second layer.
Figure 10: The comparisons between a hyperbolic recursive transformer with and without pseudo-polar(denoted as +spherical in the legend) coordinates on the travelling salesman problem.
