Figure 1: The difference between parameter sparsity (a) and representation sparsity (b) in a simple two taskscase. First layer indicates input patterns. Learning the first task utilizes parts indicated in red. Task 2 hasdifferent input patterns and uses parts shown in green. Orange indicates changed neurons activations as a resultof the second task. In (a), when an example from the first task is encountered again, the activations of the firstlayer will not be affected by the changes, however, the second and later layer activations are changed. Suchinterference is largely reduced when imposing sparsity on the representation (b).
Figure 2: Comparison of different regularization techniques on 5 permuted MNIST sequence. Representationbased regularizers are solid bars, bars with lines represent parameters regularizers, dotted bars represent activationfunctions. Average test accuracy over all tasks is given in the legend. Representation based regularizers achievehigher performance than other compared methods including parameters based regularizers. Our regularizer,SLNID, performs the best on the last two tasks indicating that more capacity is left to learn these tasks.
Figure 3: Comparison of different regularization techniques on a sequence of ten tasks from (a) Cifar split and(b) Tiny ImageNet split. The legend shows average test accuracy over all tasks. Simple L1-norm regularizer(L1-Rep) doesn,t help in such more complex tasks. Our regularizer SLNID achieves an improvement of 2%over Decov and 4 — 8% compared to No-Reg.
Figure 4: On the 5 permuted MNISTsequence, hidden layer=128, Top: per-centage of unused parameters in the 1stlayer using different λSLNiD ; Bottom:histogram of neural activations on thefirst task.
Figure 5: Comparison of different regularization techniques on 5 permuted MNIST sequence of tasks, hiddensize=64. Representation based regularizers are solid bars, bars with lines represent parameters regularizers,dotted bars represent activation functions. See Figure 2 for size 128.
Figure 6: (a) SLNID With EWC on 5 permuted MniSt sequence of tasks, hidden SiZe=128, (b) hidden size=64.
Figure 7: Comparison of different regularization techniques on a sequence of ten tasks from Cifar split. Hiddensize=128. See Figure 3(a) for size 256.
Figure 8: First layer neuron importance after learning the first task (blue). Left: SNID, Right: SLNID. Moreactive neurons are tolerated in SLNID.
Figure 9: First layer neuron importance after learning the second task (orange), superimposed on Figure 8. Left:SNID, Right: SLNID. SLNID allows new neurons, especially those that were close neighbours to previousimportant neurons, to become active and to be used for the new task. SNID penalizes all unimportant neuronsequally. As a result, previous neurons are adapted for the new tasks and less new neurons are getting activated.
Figure 10: First layer neuron importance after learning the third task (green), superimposed on Figure 9. Left:SNID, Right: SLNID. SLNID allows previous neurons to be re-used for the third task. It avoids changing theprevious important neurons by adding new neurons. For SNID, very few neurons are newly deployed. The newtask is learned mostly by adapting previous important neurons, causing more interference.
Figure 11: First layer neuron importance after learning the first task, sorted in descending order according to thefirst task neuron importance (blue). Left: SNID, Right: SLNID. More active neurons are tolerated in SLNID.
Figure 12: First layer neuron importance after learning the second task sorted in descending order accordingto the first task neuron importance (orange), superimposed on top of figure 11. Left: SNID, Right: SLNID.
Figure 13: First layer neuron importance after learning the third task sorted in descending order according tothe first task neuron importance (green), superimposed on top of figure 12. Left: SNID, Right: SLNID. SLNIDallows previous neurons to be re-used for the third task while activating new neurons to cope with the needsof the new task. For SNID, very few neurons are newly deployed while most previous important neurons forprevious tasks are re-adapted to learn the new task.
