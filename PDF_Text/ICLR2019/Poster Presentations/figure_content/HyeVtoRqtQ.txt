Figure 1: The interlayer transformation of TrellisNet, at an atomic level (time steps t and t + 1,layers i and i + 1) and on a longer sequence (time steps 1 to 8, layers i and i + 1).
Figure 2: Representing a truncated 2-layer RNN ρM as a trellis network τ . (a) Each unit of τ hasthree groups, which house the input, first-layer hidden vector, and second-layer hidden vector ofρM , respectively. (b) Each group in the hidden unit of τ in level i + 1 at time step t + 1 is computedby a linear combination of appropriate groups of hidden units in level i at time steps t and t + 1.
Figure 4: A TrellisNet with an LSTM nonlinearity, at an atomic level and on a longer sequence.
Figure 5: A 2-layer LSTM is expressed as a trellis network with mixed group convolutions on fourgroups of feature channels. (Partial view.)14Published as a conference paper at ICLR 2019B Optimizing and Regularizing TrellisNet with RNN and TCNMethods(a) History repackaging between truncated se-quences in recurrent networks.
Figure 6: Using the equivalence established by Theorem 1, We can transfer the notion of historyrepackaging in recurrent networks to trellis networks.
Figure 7: (a) RNN-inspired variational dropout. (b) ConvNet-inspired auxiliary losses.
