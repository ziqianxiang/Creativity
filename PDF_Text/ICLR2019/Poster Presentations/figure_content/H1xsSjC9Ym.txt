Figure 1: Different valid goal states forthe instruction “build an L-like shapefrom red blocks”.
Figure 2: Information flow during AGILE training. The policy acts conditioned on the instructionand is trained using the reward from the reward model (Figure 2a). The reward model is trained,as a discriminator, to distinguish between “A”, the hinstruction, goal-statei pairs from the dataset(Figure 2b), and “B”, the hinstruction, statei pairs from the agent’s experience.
Figure 3: Initial state and goal state for GridLU-Relations (top-left) and GridLU-Arrangementsepisodes (bottom-left), and the complete GridLU-Arrangements vocabulary (right), each with exam-ples of some possible goal-states.
Figure 4: Left: learning curves for A3C, A3C-RP (both using ground truth reward), and AGILE-A3Cwith different values of the anticipated negative rate ρ on the GridLU-Relations task. We reportsuccess rate (see Section 3). Middle: learning curves for policies trained with ground-truth RL, andwithin AGILE, with different model architectures. Right: the reward model’s accuracy for differentvalues of ρ.
Figure 5: Fine-tuning for an immovable red square.
Figure 6: The dynamics of the GridLU world illustrated by a 6-step trajectory. The order of thestates is indicated by arrows. The agent's actions are written above arrows.
Figure 7: Performance of AGILE for different sizes of the dataset of instructions and goal-states. Foreach dataset size of we report is the best average success rate over the course of training.
Figure 8: The discriminator’s errors in the course of training. Left: percentage of false positives.
Figure 9: Our policy and discriminator networks with a Neural Module Network (NMN) as the corecomponent. The NMN’s structure corresponds to an instruction WestFrom(Color(‘red’, Shape(‘rect’,SCENE)), Color(‘yellow’, Shape(‘triangle’, SCENE))). The modules are depicted as blue rectangles.
