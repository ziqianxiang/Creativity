Figure 1: High-level scene understanding. Given original image (a), We are able to imagine unseenobjects based on the structural relations among existing objects, resulting in extrapolated image (b).
Figure 2: Our model for visual program synthesis. (a) The input is an image consisting of multipleobjects with ordered arrangements. We also perform instance segmentation to get object masks. (b)We use two vision models to extract object attributes and predict object groups, respectively. (c)These representations are then sent to a sequence model to predict the program.
Figure 3: Qualitative results for visual program synthesis. (a) Results on the REGULAR test set,where all objects are placed on a grid. (b) Results on the generalization test set which contains morecomplex scenes. (c) Results on the RANDOM test set, where objects have different sizes and thegroups are placed with random continuous coordinates.
Figure 5: Inferring programs from partial observations. (Input Image) The input image containsobjects that are fully or mostly occluded. (Partial Observation) Output of Mask R-CNN where Wediscard mask proposals that are too small. The highlighted objects form the observation of our model.
Figure 4: Generating multiple possible programs.
Figure 6: Image Editing. Our model can be applied to edit images by inferring programs (a) and thenoperate on program space. Examples include image extrapolation (b, c) and attribute editing (d, e).
Figure 7: Generalizing to real images. (a) Input image which is described by a program generated byour model. (b) The object patches in the original image are extracted using Mask R-CNN, while newobjects are inferred by modifying the program iteration number and added as masks. (c) The editedimage is rendered by pix2pix.
Figure 8: Visual Analogy Making. Given example image pairs (a), (b), the input image (c) is encodedwith a representation, which is edited according to the example image pair. The edited representationis then decoded into a new image by our model (e) and an autoencoder (f), respectively. (d) showsanalogy making result made by human.
Figure 9: Data format of our scene programs. Each color represents a different type of argument,including red: program tokens, blue: iteration arguments, green: position arguments, purple: colorarguments.
