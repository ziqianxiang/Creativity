Figure 1: Adam performs better with decoupled weight decay (bottom row, AdamW) than with L2regularization (top row, Adam). We show the final test error of a 26 2x64d ResNet on CIFAR-10after 100 epochs of training with fixed learning rate (left column), step-drop learning rate (with dropsat epoch indexes 30, 60 and 80, middle column) and cosine annealing (right column). AdamW leadsto a more separable hyperparameter search space, especially when a learning rate schedule, such asstep-drop and cosine annealing is applied. Cosine annealing yields clearly superior results.
Figure 2: The Top-1 test error of a 26 2x64d ResNet on CIFAR-10 measured after 100 epochs. Theproposed SGDW and AdamW (right column) have a more separable hyperparameter space.
Figure 3: Learning curves (top row) and generalization results (bottom row) obtained by a 262x96d ResNet trained with Adam and AdamW on CIFAR-10. See text for details. SuppFigure 4 inthe Appendix shows the same qualitative results for ImageNet32x32.
Figure 4: Top-1 test error on CIFAR-10 (left) and Top-5 test error on ImageNet32x32 (right).
