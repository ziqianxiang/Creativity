Figure 1: Diagram of the learning framework. (1) Amino acid sequences are transformed intosequences of vector embeddings by the encoder model. (2) The similarity prediction module takespairs of proteins represented by their sequences of vector embeddings and predicts their sharedSCOP level. Sequences are first aligned based on L1 distance between their vector embeddingsusing SSA. From the alignment, a similarity score is calculated and related to shared SCOP levels byordinal regression. (3) The contact prediction module uses the sequence of vector embeddings topredict contacts between amino acid positions within each protein. The contact loss is calculated bycomparing these predictions with contacts observed in the 3D structure of the protein. Error signalfrom both tasks is used to fit the parameters of the encoder.
Figure 2:	Illustration of the embedding model. The amino acid sequence is first passed through thepretrained language model in both forward and reverse directions. The hidden states at each positionof both directions of the language model are concatenated together with a one hot representation ofthe amino acids and passed as input to the encoder. The final vector representations of each positionof the amino acid sequence are given by a linear transformation of the outputs of the final bidirectionalLSTM layer.
Figure 3:	Illustration of the SCOP hierarchy modified from Hubbard et al. [39].
