Figure 1:OUr goal is to use scene priors to improve navigation in unseen scenes and towards novelobjects. (a) There is no mug in the field of view of the agent, but the likely location for finding amug is the cabinet near the coffee machine. (b) The agent has not seen a mango before, but it infersthat the most likely location for finding a mango is the fridge since similar objects such as appleappear there as well. The most likely locations are shown with the orange box.
Figure 2: Overview of the architecture. Our model to incorporate semantic knowledge into se-mantic navigation. Specifically, we learn a policy network that decides an action based on the visualfeatures of the current state, the semantic target category feature and the features extracted from theknowledge graph. We extract features from the parts of the knowledge graph that are activated.
Figure 3: Scene priors. We extract relationships between objects from the Visual Genome (Krishnaet al., 2017) dataset. The relationships for two example object categories are illustrated.
Figure 4: Graph Convolutional Networks. Each node denotes an object category and is initializedbased on the the current state (image) and the word vector. We use three layers of GCN to performinformation propagation. The first two layers output 1024-d latent features, and the last layer gen-erates a single value for each node, which results in a |V | dimensional semantic knowledge vectorthat is passed to the policy model.
Figure 5: Learning curves. The top row shows success rate and the bottom row shows SPL.
Figure 6: Qualitative results. Examples of last eight frames and the corresponding actions atpredicted from our model on unseen scenes with novel target objects.
