Figure 1: Examples from the dataset.
Figure 2: The attentional LSTM and Transformer architectures are both consist of an encoder, thatparses the question, and a decoder, which maps the correct answer right-shifted by 1 to a distributionof the next character in the answer at every position (thus allowing auto-regressive prediction). (a)The Attentional LSTM encodes the question to a sequence of (key, value) positions, which are thenattended over by the decoder. (b) The Transformer has several stages of self- and input-attention; see(Vaswani et al., 2017) for details.
Figure 3: Model accuracy (probability of correct answer) averaged across modules. RMC is therelational recurrent neural network model.
Figure 4: Interpolation test performance on the different modules.
Figure 5: Extrapolation test performance on the different modules.
