Figure 1: Default policy-agent architecture.
Figure 2: Tasks visualization. (a): Go to one of K target tasks, with quadruped; (b): Move one boxto one of K targets task, with jumping ball (red); (c): Foraging in the maze task, with quadruped; (d):Walls task with humanoid, where the goal is avoid walls while running through a terrain.
Figure 3:	Results for the sparse-reward tasks with complex walkers. Left: go to moving targettask with humanoid. Center: foraging in the maze results with quadruped. Right: moving one boxto one of two targets and go to another target task with quadruped. The legends denote additional tothe proprioception, information passed to the default policy (except baseline, where we do not usedefault policy).
Figure 4:	Behavior analysis and transfer results. Left: the trajectory of the agent on go to movingtarget task with quadruped. Center: KL divergence from the agent policy to the proprioceptivedefault policy plotted over time for the same trajectory. Right: Performance of the transfer on moveone box to one of 3 targets task with quadruped. The legend whether the default policy is learned oris transferred. Furthermore, it specifies the task from which the default policy is transferred as well asadditional information other than the proprioceptive information that the default policy is conditionedon, if any.
Figure 5: Ablations. Left: Comparing various regularization schemes. Center: Benefits of defaultpolicy vanish when using (dense) shaping rewards. Right: Optimistic baselines comparing pretraineddefault policies.
Figure 6: DMLab30. Left, comparison between baseline (same as Espeholt et al. (2018)) that usesuniform distribution over actions as a default policy and three different possible default policies.
Figure 7: Single versus multiple actors comparison on go to moving target task. Left: 32 actors.
Figure 8: Walkers visualization.
Figure 9:	Results for the dense-reward tasks. Starting from left. First: walking quadruped task.
Figure 10:	Results for sparse-reward tasks with jumping ball walker.Left: go to moving target.
Figure 11:	Results for go to one of K targets tasks with quadruped. Left: go to 1 target. Center: goto one of 2 targets. Right: go to one of 3 targets. The legends denote additional to the proprioception,information passed to the default policy (except baseline, where we do not use default policy).
Figure 12:	Results for box pushing tasks with quadruped. Starting from left, first: move one boxto one of 2 targets with go to another. Second: move one box to 1 target. Third: move one boxto one of 2 targets. Forth: move one box to one of 3 targets. The legends denote additional to theproprioception, information passed to the default policy (except baseline, where we do not use defaultpolicy).
Figure 13: Performance of the transfer with quadruped walker. Left: Go to one of 3 targets.
Figure 14: Ablations for walls task with quadruped. Left: Comparing various regularization schemes.
Figure 15: KL direction results for go to moving target task with quadruped.
