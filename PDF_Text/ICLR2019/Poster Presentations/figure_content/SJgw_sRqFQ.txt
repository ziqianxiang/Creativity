Figure 1: MA versus EMA for various values of β.
Figure 2: Effect of averaging parameters for Mixture of Gaussians. From top to bottom: withoutaverage, EMA, MA, Optimistic Adam, Consensus Optimization, Zero-GP.
Figure 3: Generation for CIFAR-10 dataset.
Figure 4: CIFAR-10 FID and IS scores during training. Setting: Original GAN objective, conventionalarchitecture, ndis = 1Figure 5: Comparison of EMA, Optimistic Adam and Consensus Optimization on CIFAR-10.
Figure 5: Comparison of EMA, Optimistic Adam and Consensus Optimization on CIFAR-10.
Figure 6: Generation for CelebA dataset for various β values at 250k iterations. From top to bottom:(a) non-averaged generator, (b) β = 0.9, (c) β = 0.99, (d) β = 0.999, (e) β = 0.9999, (f) MA.
Figure 7: Generation for CelebA dataset for the same 2 noise samples from 50k to 200k with 10kintervals. (top 2 rows) without averaging, (bottom two rows) with EMA β = 0.99999Published as a conference paper at ICLR 20195.5	STL-10 & IMAGENETFigure 8 shows images generated with and w/o EMA for STL-10 and ImageNet. Even thoughquantitative scores are better for EMA, we do not see as clear visual improvements as in previousresults but rather small changes. Both models produce images that are unrecognizable to a largedegree. This observation strengthens our intuition that averaging brings the cycling generator closerto the local optimal point, but it does not necessarily find a good solution as the local optimum maynot be good enough.
Figure 8: Generation for STL-10 & ImageNet dataset after 500k iteration. (Top): STL-10, (Bot-tom):ImageNet, (Left): w/o averaging (Right): with EMA6 ConclusionWe have explored the effect of two different techniques for averaging parameters outside of the GANtraining loop, moving average (MA) and exponential moving average (EMA). We have shown thatboth techniques significantly improve the quality of generated images on various datasets, networkarchitectures and GAN objectives. In the case of the EMA technique, we have provided the firsttheoretical analysis of its implications, showing that even in simple bilinear settings it converges tostable limit cycles of small amplitude around the solution of the saddle problem. Averaging methodsare easy to implement and have minimal computation overhead. As a result, these techniques arereadily applicable in a wide range of settings. In the future, we plan to explore their effect on largerscales as well as on conditional GANs.
Figure 9: Generation for CIFAR-10 dataset after 300k iterations.
Figure 10: Generation for STL-10 dataset after 500k iterations.
Figure 11: Generation for ImageNet dataset after 500k iterations.
Figure 12: CIFAR-10 FID and IS scores during training. Setting: Original GAN objective/ ResNetArchitecture/ ndis = 5Figure 13: CIFAR-10 FID and IS scores during training. Setting: WGAN-GP objective/ ResNetArchitecture/ ndis = 115Published as a conference paper at ICLR 2019Figure 14: Generation for CelebA dataset for various β values at 100k iteration. From top to bottomrows: (a) non-averaged generator, (b) β = 0.9, (c) β = 0.99, (d) β = 0.999, (e) β = 0.9999, (f) MA16Published as a conference paper at ICLR 2019Figure 16: Generation for CelebA dataset for various β values at 200k iteration. From top to bottomrows: (a) non-averaged generator, (b) β = 0.9, (c) β = 0.99, (d) β = 0.999, (e) β = 0.9999, (f) MAFigure 17: Generation for CelebA dataset for various β values at 300k iteration. From top to bottomrows: (a) non-averaged generator, (b) β = 0.9, (c) β = 0.99, (d) β = 0.999, (e) β = 0.9999, (f) MA17Published as a conference paper at ICLR 2019Figure 18: Generation for CelebA dataset for various β values at 350k iteration. From top to bottomrows: (a) non-averaged generator, (b) β = 0.9, (c) β = 0.99, (d) β = 0.999, (e) β = 0.9999, (f) MAFigure 19: ImageNet FID and IS score during training. Setting: Original GAN objective/ ConventionalArchitecture/ ndis = 1
Figure 13: CIFAR-10 FID and IS scores during training. Setting: WGAN-GP objective/ ResNetArchitecture/ ndis = 115Published as a conference paper at ICLR 2019Figure 14: Generation for CelebA dataset for various β values at 100k iteration. From top to bottomrows: (a) non-averaged generator, (b) β = 0.9, (c) β = 0.99, (d) β = 0.999, (e) β = 0.9999, (f) MA16Published as a conference paper at ICLR 2019Figure 16: Generation for CelebA dataset for various β values at 200k iteration. From top to bottomrows: (a) non-averaged generator, (b) β = 0.9, (c) β = 0.99, (d) β = 0.999, (e) β = 0.9999, (f) MAFigure 17: Generation for CelebA dataset for various β values at 300k iteration. From top to bottomrows: (a) non-averaged generator, (b) β = 0.9, (c) β = 0.99, (d) β = 0.999, (e) β = 0.9999, (f) MA17Published as a conference paper at ICLR 2019Figure 18: Generation for CelebA dataset for various β values at 350k iteration. From top to bottomrows: (a) non-averaged generator, (b) β = 0.9, (c) β = 0.99, (d) β = 0.999, (e) β = 0.9999, (f) MAFigure 19: ImageNet FID and IS score during training. Setting: Original GAN objective/ ConventionalArchitecture/ ndis = 1Figure 20: CIFAR-10 FID and IS scores during training. Setting: Original GAN objective/ Conven-tional Architecture/ ndis = 1
Figure 14: Generation for CelebA dataset for various β values at 100k iteration. From top to bottomrows: (a) non-averaged generator, (b) β = 0.9, (c) β = 0.99, (d) β = 0.999, (e) β = 0.9999, (f) MA16Published as a conference paper at ICLR 2019Figure 16: Generation for CelebA dataset for various β values at 200k iteration. From top to bottomrows: (a) non-averaged generator, (b) β = 0.9, (c) β = 0.99, (d) β = 0.999, (e) β = 0.9999, (f) MAFigure 17: Generation for CelebA dataset for various β values at 300k iteration. From top to bottomrows: (a) non-averaged generator, (b) β = 0.9, (c) β = 0.99, (d) β = 0.999, (e) β = 0.9999, (f) MA17Published as a conference paper at ICLR 2019Figure 18: Generation for CelebA dataset for various β values at 350k iteration. From top to bottomrows: (a) non-averaged generator, (b) β = 0.9, (c) β = 0.99, (d) β = 0.999, (e) β = 0.9999, (f) MAFigure 19: ImageNet FID and IS score during training. Setting: Original GAN objective/ ConventionalArchitecture/ ndis = 1Figure 20: CIFAR-10 FID and IS scores during training. Setting: Original GAN objective/ Conven-tional Architecture/ ndis = 118Published as a conference paper at ICLR 2019Figure 21: CIFAR-10 FID and IS scores during training. Setting: Original GAN objective/ Conven-tional Architecture/ ndis = 5
Figure 16: Generation for CelebA dataset for various β values at 200k iteration. From top to bottomrows: (a) non-averaged generator, (b) β = 0.9, (c) β = 0.99, (d) β = 0.999, (e) β = 0.9999, (f) MAFigure 17: Generation for CelebA dataset for various β values at 300k iteration. From top to bottomrows: (a) non-averaged generator, (b) β = 0.9, (c) β = 0.99, (d) β = 0.999, (e) β = 0.9999, (f) MA17Published as a conference paper at ICLR 2019Figure 18: Generation for CelebA dataset for various β values at 350k iteration. From top to bottomrows: (a) non-averaged generator, (b) β = 0.9, (c) β = 0.99, (d) β = 0.999, (e) β = 0.9999, (f) MAFigure 19: ImageNet FID and IS score during training. Setting: Original GAN objective/ ConventionalArchitecture/ ndis = 1Figure 20: CIFAR-10 FID and IS scores during training. Setting: Original GAN objective/ Conven-tional Architecture/ ndis = 118Published as a conference paper at ICLR 2019Figure 21: CIFAR-10 FID and IS scores during training. Setting: Original GAN objective/ Conven-tional Architecture/ ndis = 5Figure 22: CIFAR-10 FID and IS scores during training. Setting: WGAN-GP objective/ ConventionalArchitecture/ ndis = 1Figure 23: CIFAR-10 FID and IS scores during training. Setting: WGAN-GP objective/ ConventionalArchitecture/ ndis = 5
Figure 17: Generation for CelebA dataset for various β values at 300k iteration. From top to bottomrows: (a) non-averaged generator, (b) β = 0.9, (c) β = 0.99, (d) β = 0.999, (e) β = 0.9999, (f) MA17Published as a conference paper at ICLR 2019Figure 18: Generation for CelebA dataset for various β values at 350k iteration. From top to bottomrows: (a) non-averaged generator, (b) β = 0.9, (c) β = 0.99, (d) β = 0.999, (e) β = 0.9999, (f) MAFigure 19: ImageNet FID and IS score during training. Setting: Original GAN objective/ ConventionalArchitecture/ ndis = 1Figure 20: CIFAR-10 FID and IS scores during training. Setting: Original GAN objective/ Conven-tional Architecture/ ndis = 118Published as a conference paper at ICLR 2019Figure 21: CIFAR-10 FID and IS scores during training. Setting: Original GAN objective/ Conven-tional Architecture/ ndis = 5Figure 22: CIFAR-10 FID and IS scores during training. Setting: WGAN-GP objective/ ConventionalArchitecture/ ndis = 1Figure 23: CIFAR-10 FID and IS scores during training. Setting: WGAN-GP objective/ ConventionalArchitecture/ ndis = 519Published as a conference paper at ICLR 2019
Figure 18: Generation for CelebA dataset for various β values at 350k iteration. From top to bottomrows: (a) non-averaged generator, (b) β = 0.9, (c) β = 0.99, (d) β = 0.999, (e) β = 0.9999, (f) MAFigure 19: ImageNet FID and IS score during training. Setting: Original GAN objective/ ConventionalArchitecture/ ndis = 1Figure 20: CIFAR-10 FID and IS scores during training. Setting: Original GAN objective/ Conven-tional Architecture/ ndis = 118Published as a conference paper at ICLR 2019Figure 21: CIFAR-10 FID and IS scores during training. Setting: Original GAN objective/ Conven-tional Architecture/ ndis = 5Figure 22: CIFAR-10 FID and IS scores during training. Setting: WGAN-GP objective/ ConventionalArchitecture/ ndis = 1Figure 23: CIFAR-10 FID and IS scores during training. Setting: WGAN-GP objective/ ConventionalArchitecture/ ndis = 519Published as a conference paper at ICLR 2019C	Network ArchitecturesWhen spectral normalization is used in ResNet, the feature number in each layer doubled by 2. Priordistribution for the generator is a 512-dimensional isotropic Gaussian distribution for conventionalarchitecture and 128 for ResNet. Samples from the distribution are normalized to make them lie on a
Figure 19: ImageNet FID and IS score during training. Setting: Original GAN objective/ ConventionalArchitecture/ ndis = 1Figure 20: CIFAR-10 FID and IS scores during training. Setting: Original GAN objective/ Conven-tional Architecture/ ndis = 118Published as a conference paper at ICLR 2019Figure 21: CIFAR-10 FID and IS scores during training. Setting: Original GAN objective/ Conven-tional Architecture/ ndis = 5Figure 22: CIFAR-10 FID and IS scores during training. Setting: WGAN-GP objective/ ConventionalArchitecture/ ndis = 1Figure 23: CIFAR-10 FID and IS scores during training. Setting: WGAN-GP objective/ ConventionalArchitecture/ ndis = 519Published as a conference paper at ICLR 2019C	Network ArchitecturesWhen spectral normalization is used in ResNet, the feature number in each layer doubled by 2. Priordistribution for the generator is a 512-dimensional isotropic Gaussian distribution for conventionalarchitecture and 128 for ResNet. Samples from the distribution are normalized to make them lie on aunit hypersphere before passing them into the generator.
Figure 20: CIFAR-10 FID and IS scores during training. Setting: Original GAN objective/ Conven-tional Architecture/ ndis = 118Published as a conference paper at ICLR 2019Figure 21: CIFAR-10 FID and IS scores during training. Setting: Original GAN objective/ Conven-tional Architecture/ ndis = 5Figure 22: CIFAR-10 FID and IS scores during training. Setting: WGAN-GP objective/ ConventionalArchitecture/ ndis = 1Figure 23: CIFAR-10 FID and IS scores during training. Setting: WGAN-GP objective/ ConventionalArchitecture/ ndis = 519Published as a conference paper at ICLR 2019C	Network ArchitecturesWhen spectral normalization is used in ResNet, the feature number in each layer doubled by 2. Priordistribution for the generator is a 512-dimensional isotropic Gaussian distribution for conventionalarchitecture and 128 for ResNet. Samples from the distribution are normalized to make them lie on aunit hypersphere before passing them into the generator.
Figure 21: CIFAR-10 FID and IS scores during training. Setting: Original GAN objective/ Conven-tional Architecture/ ndis = 5Figure 22: CIFAR-10 FID and IS scores during training. Setting: WGAN-GP objective/ ConventionalArchitecture/ ndis = 1Figure 23: CIFAR-10 FID and IS scores during training. Setting: WGAN-GP objective/ ConventionalArchitecture/ ndis = 519Published as a conference paper at ICLR 2019C	Network ArchitecturesWhen spectral normalization is used in ResNet, the feature number in each layer doubled by 2. Priordistribution for the generator is a 512-dimensional isotropic Gaussian distribution for conventionalarchitecture and 128 for ResNet. Samples from the distribution are normalized to make them lie on aunit hypersphere before passing them into the generator.
Figure 22: CIFAR-10 FID and IS scores during training. Setting: WGAN-GP objective/ ConventionalArchitecture/ ndis = 1Figure 23: CIFAR-10 FID and IS scores during training. Setting: WGAN-GP objective/ ConventionalArchitecture/ ndis = 519Published as a conference paper at ICLR 2019C	Network ArchitecturesWhen spectral normalization is used in ResNet, the feature number in each layer doubled by 2. Priordistribution for the generator is a 512-dimensional isotropic Gaussian distribution for conventionalarchitecture and 128 for ResNet. Samples from the distribution are normalized to make them lie on aunit hypersphere before passing them into the generator.
Figure 23: CIFAR-10 FID and IS scores during training. Setting: WGAN-GP objective/ ConventionalArchitecture/ ndis = 519Published as a conference paper at ICLR 2019C	Network ArchitecturesWhen spectral normalization is used in ResNet, the feature number in each layer doubled by 2. Priordistribution for the generator is a 512-dimensional isotropic Gaussian distribution for conventionalarchitecture and 128 for ResNet. Samples from the distribution are normalized to make them lie on aunit hypersphere before passing them into the generator.
