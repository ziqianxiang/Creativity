Figure 1: (left): A topic-word distribution due to global exposure, obtained from the matrix W as row-vector.
Figure 2: (left): DocNADE for the document v. (right): ctx-DocNADEe for the observable corresponding tovi âˆˆ v. Blue colored lines signify the connections that share parameters. The observations (double circle) foreach word vi are multinomial, where vi is the index in the vocabulary of the ith word of the document. hiDNand hiLM are hidden vectors from DocNADE and LSTM models, respectively for the target word vi. Connec-tions between each input Vi and hidden units hDN are shared. The symbol Vi represents the autoregressiveconditionals p(vi|v<i), computed using hi which is a weighted sum of hiDN and hiLM in ctx-DocNADEe.
Figure 3: Retrieval performance (IR-precision) on 6 datasets at different fractionsin IR-precision. In addition, the deep variant (d=3) with embeddings, i.e., ctx-DeepDNEe showscompetitive performance on TREC6 and Subjectivity datasets.
Figure 4: Evaluations at different fractions (20%, 40%, 60%, 80%, 100%) of the training set of TMNtitle(a) Text Retrievalthe ctx-DocNADEe extracts a more coherent topic due to embedding priors. To qualitatively in-spect the contribution of word embeddings and textTOvec representations in topic models, weanalyse the text retrieved for each query using the representations learned from DocNADE and ctx-DoocNADEe models. Table 9 illustrates the retrieval of the top 3 texts for an input query, selectedfrom TMNtitle dataset, where #match is YES if the query and retrievals have the same class label.
