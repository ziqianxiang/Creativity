Figure 1: Left: the graphical model representing the generative model pθ . Right: the architecture of theinference model. The inference network qφ uses a backward recurrent state bt (in red) to approximate thedependence of zt on future observations. it shares the forward recurrent state ht-1 with the generative modelto approximate the dependence of zt on past observations and latent variables. Boxes are deterministic hiddenstates. circles are random variables and filled circles represent variables observed during training.
Figure 2: Imitation Learning. We show comparison of our method with the baseline methods for Half-Cheetah, Reacher and Car Racing tasks. We find that our method is able to achieve higher reward fasterthan baseline methods and is more stable.
Figure 3: Model-Based RL. We show our comparison of our methods with baseline methods including SeCTArfor BabyAI PickUnlock task and Wheeled locomotion task with sparse rewards. We observe that our baselineachieves higher rewards than the corresponding baselines.
Figure 4: The first 18 plots show how the agent evolves in BabyAI environment for 18 steps. The last plotshows the the corresponding auxiliary cost in function of steps. The agent is in red. The gray regions in imagesare the agent’s observational space. The keys are doors can be an arbitary color, in this example, both the keyand the door are in blue. The auxillary cost generally descreases over time.
