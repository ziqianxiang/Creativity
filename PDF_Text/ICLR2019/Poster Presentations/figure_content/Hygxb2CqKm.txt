Figure 1: Stable and unstable variants of common recurrent architectures achieve similar perfor-mance across a range of different sequence tasks.
Figure 2: What is the intrinsic “price of stability”? For language modeling, we show the unstableLSTMs are actually stable in weaker, data-dependent sense. On the other hand, for polyphonicmusic modeling with short sequences, instability can improve model performance.
Figure 3: Unstable word and character-level language models exhibit vanishing gradients. We plotthe norm of the gradient with respect to inputs, kVχtpt+ik, as the distance between the input andthe loss grows, averaged over the entire training set. The gradient vanishes for moderate values of ifor both RNNs and LSTMs, though the decay is slower for LSTMs.
Figure 4: Effect of truncating unstable models. On both language and music modeling, RNNs andLSTMs exhibit diminishing returns for large values of the truncation parameter k . In LSTMs, largerk doesn’t affect performance, whereas for unstable RNNs, large k slightly decreases performanceProposition (4) holds for unstable models. In stable models, Proposition (4) in Section 3 ensuresthe distance between the weight matrices kwrecurr - wtrunc k grows slowly as training progresses,and this rate decreases as k becomes large. In Figures 5(a) and 5(b), we show a similar result holdsempirically for unstable word-level language models. All the models are initialized at the samepoint, and we track the distance between the hidden-to-hidden matrices W as training progresses.
Figure 5: Qualitative version of Proposition 4 for unstable, word-level language models. We assumek = 65 well-captures the full-recurrent model and plot kwtrunc - wrecurr k = kWk - W65 k astraining proceeds, where W denotes the recurrent weights. As Proposition 4 suggests, this quantitygrows slowly as training proceeds, and the rate of growth decreases as k increases.
Figure 6: Empirical validation Proposition 4 on random Gaussian instances. Without the 1/t rate,the gradient descent bound no longer appears qualitatively correct, suggesting the O(1/t) rate isnecessary.
