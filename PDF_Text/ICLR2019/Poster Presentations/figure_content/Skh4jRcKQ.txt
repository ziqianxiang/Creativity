Figure 1: The plots of the empirical loss moving by one step in the direction of negative coarsegradient v.s. the learning rate (step size) η for different sample sizes.
Figure 2: When initialized with weights (good minima) produced by the vanilla (orange) and clipped(blue) ReLUs on ResNet-20 with 4-bit activations, the coarse gradient descent using the identity STEends up being repelled from there. The learning rate is set to 10-5 until epoch 20.
Figure 3: The plots of 2-bit quantized ReLU σα (x) (with 22 = 4 quantization levels including 0)and the associated clipped ReLU σα (x). α is the resolution determined in advance of the networktraining.
Figure 4: When initialized with the weights produced by the clipped ReLU STE on ResNet-20 with2-bit activations (88.38% validation accuracy), the coarse gradient descent using the ReLU STE with10-5 learning rate is not stable there, and both classification and training errors begin to increase.
