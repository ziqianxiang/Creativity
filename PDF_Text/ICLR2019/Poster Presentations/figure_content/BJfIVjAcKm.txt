Figure 1: Plot and contour plot of the function F(x, y) = - tanh(1 + X ∙ y)3.3.2	Estimating ReLU Upper and Lower Bounds on ActivationsA key aspect of using RS Loss is determining the upper and lower bounds Uij, Iij for each ReLU (cf.
Figure 2: (a) Average number of unstable ReLUs by layer and (b) average verification solve timesof 6 networks trained with different weights on RS Loss for MNIST and = 0.1 . Averages aretaken over all 10000 MNIST test set inputs. Both metrics improve significantly with increasingRS Loss weight. An RS Loss weight of 0 corresponds to the control network, while an RS Lossweight of 0.00012 corresponds to the “+RS” network for MNIST and = 0.1 in Tables 2 and 3. (c)Improvement in the average time taken by a verifier to solve the verification problem after addingRS Loss to the training procedure, for different on MNIST. The weight on RS Loss was chosen sothat the “+RS” models have test set accuracies within 0.50% of the control models.
Figure 3: Removing some ReLUs does not hurt test set accuracy or accuracy against a PGD adver-sary12Published as a conference paper at ICLR 2019B	Adversarial Training and Weight SparsityIt is worth noticing that adversarial training against '∞ norm-bound adversaries alone already makesnetworks easier to verify by implicitly improving weight sparsity. Indeed, this can be shown clearlyin the case of linear networks. Recall that a linear network can be expressed as f(x) = Wx + b.
