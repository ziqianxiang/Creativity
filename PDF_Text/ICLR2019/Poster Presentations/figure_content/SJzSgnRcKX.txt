Figure 1: Probing model architecture (§ 3.1). All parameters inside the dashed line are fixed, whilewe train the span pooling and MLP classifiers to extract information from the contextual vectors.
Figure 2: Additional baselines for ELMo, evaluated on the test sets. CNNk adds a convolutionallayer that sees ±k tokens to each side of the center word. Lexical is the lexical baseline, equivalent tok = 0. Orthonormal is the full ELMo architecture with random orthonormal LSTM and projectionweights, but using the pretrained lexical layer. Full (pretrained) is the full ELMo model. Coloredbands are 95% confidence intervals (normal approximation).
Figure 3: Dependency labeling F1 score as a function of separating distance between the two spans.
