Figure 1: Schematic sketch of our HRL approach. By using the advantage-weighted importance, theproblem of finding the modes of the advantage-function can be reduced to that of finding the modesof the density of state action pairs.
Figure 2: Activation of the four options over time steps on the Walker2d task.
Figure 3: Performance of adInfoHRL. (a)-(d) show comparison with baseline methods. (e) and (f)show the output of the option network and the activation of options on Walker2d, respectively.
Figure 4: Distribution of options on the HalfCheetah-v1 task using adInfoHRL with two options.
Figure 5: Activation of options over time steps on the HalfCheetah-VI task using adInfoHRL withtwo options.
Figure 6: Distribution of options on Ant-rllab task using adInfoHRL With four options. The dimen-sionality is reduced by t-SNE for visualization.
Figure 7: Activation of the options over time steps on Ant-rllab task. Four options are learned.
Figure 8: Distribution of options on the Ant-v1 task using adInfoHRL with two options. The dimen-sionality is reduced by t-SNE for visualization.
Figure 9: Activation of options over time steps on the Ant-VI task using adInfoHRL with twooptions.
