Figure 1: Three steps to compute the lexical representations for the input sequence “a cute puppy”,using various lexical unit segmentations.
Figure 2: SDE computes the embedding for the word “puppy”. Both character n-grams embeddingsand latent semantic embeddings are shared among all languages.
Figure 3: Percentage of wordsby the edit distance from thematching words in the high-resource language.
Figure 4: Gain in word F-measure of SDE over sub-sep. Left: the target words are bucketed bythe number of subword pieces that their corresponding source words are segmented into. Right: thetarget words are bucketed by the edit distance between their source words and the correspondingwords in the high resource language.
Figure 5: Performance on three different vocabulary size (results of a single random seed).
Figure 6:	KL divergence of attention over latent embedding space between words from two relatedlanguages. Word pairs that match at the diagonal have similar meanings. Left: bel-rus. Right:glg-por.
Figure 7:	T-SNE visualizations of the embeddings of words in Table 9 encoded after the charactern-gram embedding stage (Left), or after the full process of SDE(Right). Words of the same colorhave similar meanings, and the language code of the word is placed in the parenthesis. It can be seenthat the character embedding stage is more sensitive to lexical similarity, while the full SDEmodelis more sensitive to similarity in meaning.
