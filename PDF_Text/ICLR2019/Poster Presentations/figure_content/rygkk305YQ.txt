Figure 1: Graphical model representation of the proposed models. Observed class often correspondsto the speaker label. The left illustrates equation 1, and the right illustrates the extension fromSection 2.3. The grey and white nodes correspond to observed and latent variables.
Figure 2: Training configuration of the GMVAE-Tacotron model. Dashed lines denotes sampling.
Figure 3: Assignment distributionover yl for each gender (upper) andfor each accent (lower).
Figure 4: Left: Euclidean distance between the means of each mixture component pair. Right: De-coding the same text conditioned on the mean of a noisy (center) and a clean component (right).
Figure 5: SNR as a function of the value in each latent dimen-sion, comparing clean (left) and noisy (right) components.
Figure 6: Mel-spectrograms of three samples with the same text, “Wemust burn the house down! said the Rabbit’s voice.” drawn from theproposed model, showing variation in speed, F0 , and pause duration.
Figure 7: (a) Mel-spectrograms of two unnatural GST samples when setting the weight for one token-0.1: first with tremolo at the end, and second with abnormally long duration for the first syllable. (b)F0 tracks and spectrograms from GMVAE-Tacotron using different values for the “speed” dimension.
Figure 8: Mel-spectrograms and F0 tracks of three random samples drawn from each of six se-lected mixture components. Each component represents certain gender and accent group. The inputtext is “The fake lawyer from New Orleans is caught again.” which emphasizes the differencebetween British and US accents. As mentioned in the paper, although samples from component3 and 5 both capture US female voices, each component captures specific speakers with differentF0 ranges. The former ranges from 100 to 250 Hz, and the latter ranges from 200 to 350 Hz. Au-dio samples can be found at https://google.github.io/tacotron/publications/gmvae_controllable_tts#multispk_en.sampleE.2 Quantitative analysis of the US female componentsTo quantify the difference between the three components that model US female speakers (3, 5, and 6),we draw 20 latent attribute encodings zl from each of the three components and decode the sameset of 25 text sequences for each one. Table 8 shows the average F0 computed over 500 synthesizedutterances for each component, demonstrating that each component models a different F0 range.
Figure 9: Mel-spectrograms and F0 tracks of the synthesized samples demonstratoing independentcontrol of several latent attributes. Each row traverses one dimension with three different values,keeping all other dimensions fixed.. All examples use the same input text: “The fake lawyer fromNew Orleans is caught again.” The plots for dimension 0 (top row) and dimension 2 (second row)mainly show variation along the time axis. The underlying F0 contour values do not change, howeverdimension 0 controls the duration of the initial pause before the speech begins, and dimension 2controls the overall speaking rate, with the F0 track stretching in time (i.e. slowing down) whenmoving from the left column to the right. Dimension nine (bottom row) mainly controls the degreeof F0 variation while maintaining the speed and starting offset. Finally, we note that differencesin accent controlled by dimension 3 (third row) are easier to recognize by listening to audio sam-ples, which can be found at https://google.github.io/tacotron/publications/gmvae_controllable_tts#multispk_en.control.
Figure 10: Mel-spectrograms of random samples drawn from a noisy (left) and a clean (right) mixturecomponent. Samples within each row are conditioned on the same speaker. Likewise, sampleswithin each column are conditioned on the same latent attribute representation zl . For all samples,the input text is “This model is trained on multi-speaker English data.” Samples drawn from theclean component are all clean, while samples drawn from the noisy component all contain obviousbackground noise. Finally, note that samples within each column contain similar types of noise sincethey are conditioned on the same zl. Audio samples can be found at https://google.github.
Figure 11: Mel-spectrograms of the synthesized samples demonstrating control of the backgroundnoise level by varying the value of dimension 13. Each row conditions on a seed zl drawn from amixture component, where all values except for dimension 13 are fixed. The embedding used in row1 and row 3 are drawn from a noisy component, and used in row 2 and row 4 are drawn from a cleancomponent. In addition, we condition the decoding on the same speaker for the first two rows, andthe same held-out speaker for the last two rows. The value of dimension 13 used in each column isshown at the bottom, and the input text is “Traversing the noise level dimension.” In all rows, sampleson the right are cleaner than those on the left, with the background noise gradually fading away as thevalue for dimension 13 increases. Audio samples can be found at https://google.github.io/tacotron/publications/gmvae_controllable_tts#noisy_multispk_en.controlF.3 Prior distribution of the noise level dimensionA4ωuQJp HmqeqzdFigure 12: Prior distributions of each component for dimension 13, which controls background noiselevel. The first four components (0-3) model noisy speech, and the other six (4-9) model cleanspeech. The two groups of mixture components are clearly separated in this dimension. Furthermore,the clean components have lower variances than the noisy components, indicated a narrower range ofnoise levels in clean components compared to noisy ones.
Figure 12: Prior distributions of each component for dimension 13, which controls background noiselevel. The first four components (0-3) model noisy speech, and the other six (4-9) model cleanspeech. The two groups of mixture components are clearly separated in this dimension. Furthermore,the clean components have lower variances than the noisy components, indicated a narrower range ofnoise levels in clean components compared to noisy ones.
Figure 13: Mel-spectrograms of reference and synthesized style transfer utterances. The fourreference utterances are shown on the top, and the four synthesized style transfer samples are shownbelow, where each row uses the same input text (shown above the spectrograms), and each columnis conditioned on the zl inferred from the reference in the top row. From left to right, the voicesof the three reference utterances can be described as (1) tremulous and high-pitched, (2) rough,low-pitched, and terrifying, and (3) deep and masculine. In all cases, the synthesized samplesresemble the prosody and the speaking style of the reference. For example, samples in the firstcolumn have the highest F0 (positively correlated to the spacing between horizontal stripes) and moretremulous (vertical fluctuations), and spectrograms in the middle column are more blurred, related toroughness of a voice. Audio samples can be found at https://google.github.io/tacotron/publications/gmvae_controllable_tts#singlespk_audiobook.transferFigure 13 demonstrates that the GMVAE-Tacotron can also be applied in a non-parallel style transferscenario to generate speech whose text content differs significantly from the reference.
Figure 14: Mel-spectrograms and F0 tracks of different input text with five random samples of zldrawn from the prior. The three input text sequences from left to right are: (1) “We must burn thehouse down! said the Rabbit’s voice.”, (2) “And she began fancying the sort of thing that wouldhappen: Miss Alice!”, and (3) “She tasted a bite, and she read a word or two, and she sipped theamber wine and wiggled her toes in the silk stockings.” The five samples of zl encode different styles:the first sample has the fastest speaking rate, the third sample has the slowest speaking rate, and thefourth sample has the highest F0. Audio samples can be found at https://google.github.io/tacotron/publications/gmvae_controllable_tts#singlespk_audiobook.sample25Published as a conference paper at ICLR 2019G.5 Control of style attributesDimension 8:pitch350300350300胃 rm∏jTLlrmaJrn0	100	200	300	400	0	100	200	300	400Frame	Frame
Figure 15: Synthesized mel-spectrograms demonstrating independent control of speaking style andprosody. The same input text is used for all samples: “He waited a little, in the vain hope thatshe would relent: she turned away from him.” In the top row, F0 is controlled by setting differentvalues for dimension eight. F0 tracks show that the F0 range increases from left to right, while otherattributes such as speed and rhythm do not change. In the second row, the duration of pause beforethe phrase “she turned away from him.” (red boxes) is varied. The three spectrograms are verysimilar, except for the width of the red boxes, indicating that only the pause duration changed. In thebottom row, the “roughness” of the voice is varied. The same region of spectrograms is zoomed-infor clarity, where the spectrograms became less blurry and the harmonics becomes better definedfrom left to right. Audio samples can be found at https://google.github.io/tacotron/publications/gmvae_controllable_tts#singlespk_audiobook.control.
Figure 16: Synthesized mel-spectrograms and F0 tracks demonstrating independent control of at-tributes related to style, recording channel, and noise-condition. The same text input was usedfor all the samples: ’”Are you Italian?” asked Uncle John, regarding the young man critically.’In each row we varied the value for a single dimension while holding other dimensions fixed. Inthe top row, we controlled the F0 by traversing dimension zero. Note that the speaker identitydid not change while traversing this dimension. In the second row, the F0 contours did changewhile traversing this dimension; however, it can be seen from the spectrograms that the leftmostone attenuated the energy in low-frequency bands, and the rightmost one attenuated energy inhigh-frequency bands. This dimension appears to control the shape of a linear filter applied tothe signal, perhaps corresponding to variation in microphone frequency response in the trainingdata. In the third row, the F0 contours did not change, either. However, the background noiselevel does vary while traversing this dimension, which can be heard on the demo page. In thebottom row, variation in the speaking rate can be seen while other attributes remain constant. Au-dio samples can be found at https://google.github.io/tacotron/publications/gmvae_controllable_tts#crowdsourced_audiobook.control.
