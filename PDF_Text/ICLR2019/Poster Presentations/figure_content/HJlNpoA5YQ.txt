Figure 1: Visualization of the shaped reward defined bythe L2 distance from the red cell on an (x, y) representation(left) and Laplacian representation (right).
Figure 2: FourRoom Env.
Figure 3: Evaluation of learned representations. The x-axis shows number of transitions usedfor training and y-axis shows the gap between the graph drawing objective of the learned repre-sentations and the optimal Laplacian-based representations (lower is better). We find our method(graph drawing) more accurately approximates the desired representations than previous methods.
Figure 4: Results of reward shaping with a learned Laplacian embedding in GridWorld environ-ments. The top row shows the L2 distance in the learned embedding space. The bottom row showsempirical performance. Our method (mix) can reach optimal performance faster than the baselines,especially in harder mazes. Policies are trained by DQN (Mnih et al., 2013).
Figure 5: Results of reward shaping with a learned Laplacian embedding in continuous control en-vironments. Our learned representations are used by the “mix” and “fullmix” variants (see text fordetails), whose performance dominates that of all other methods. Policies are trained by DDPG (Lil-licrap et al., 2015).
Figure 6: Evaluation of learned representations for d = 50, 100.
Figure 7: Ablation study for the value of β. We use n = 2000 and repr = position.
Figure 8: Results of reward shaping with pretrained-then-fixed v.s. online-learned representations.
