Figure 1: The relationship between the training loss and the learning rate for scale-invariant param-eters, with learning rate for scale-variant ones set to 0.1. Left: The average training loss of thelast 5 epochs (averaged across 10 experiments). In rare cases, the training loss becomes NaN in theexperiments for the yellow curve (SGD, separate) with learning rate larger than 10. Right: Theaverage training loss of each epoch (each curve stands for a single experiment).
Figure 2: The relationship between the training loss and the learning rate. For learning rate largerthan 10, the training loss of PSGD or SGD with BN removed is always either very large or NaN,and thus not invisible in the figure. Left: The average training loss of the last 5 epochs (averagedacross 10 experiments). In rare cases, the training loss becomes NaN in the experiments for thegreen curve (SGD, BN removed) with learning rate larger than 10-0.7. We removed such datawhen taking the average. Right: The average training loss of each epoch (each curve stands for asingle experiment).
Figure 3: The relationship between the test accuracy and the learning rate. Left: The average testaccuracy of the last 5 epochs (averaged across 10 experiments). Right: The test accuracy after eachepoch (each curve stands for a single experiment). Due to the implementation of Tensorflow, output-ing NaN leads to a test accuracy of 10%. Note that the magenta dotted curve (PSGD, lr=100),red dashed curve (SGD, BN removed, lr=1) and cyan dashed curve (SGD, BN removed,lr=10) are covered by the magenta dashed curve (SGD, BN removed, lr=100). They allhave 10% test accuracy.
