Figure 1: Comparison of a variety of gradient estimators in maximizing E (φ) = Ez〜BemoUUi(σ(φ)) [(z — Po)2] Viagradient ascent, where p0 ∈ {0.49, 0.499, 0.501, 0.51}; the optimal solution is σ(φ) = 1(p0 < 0.5). Shownin Rows 1 and 2 are the trace plots of the true/estimated gradients VφE (φ) and estimated Bernoulli probabilityparameters σ(φ), with φ updated via gradient ascent. Shown in Row 3 are the gradient variances for p0 = 0.49,estimated using K = 5000 Monte Carlo samples at each iteration; the theoretical gradient variances are alsoshown if they can be analytically calculated (see Appendices C and D and for related analytic expressions).
Figure 2: Training and validation negative ELBOs on MNIST-static with respect to the training iterations,shown in the top row, and with respect to the wall clock times on Tesla-K40 GPU, shown in the bottom row, forthree differently structured Bernoulli VAEs.
Figure 3: Trace plots of the log variance of the gradient estimators on the MNIST-static data for “Nonlinear”and “Linear” network architectures, whose corresponding trace plots of both the training and validation -ELBOare shown in Figures 2(a) and 2(b), respectively. The variance of the gradient of each element is estimated byperforming exponential smoothing, with the smoothing factor as 0.999, on its first two moments; the logarithmof the average over all elements’ gradient variances is shown for each stochastic gradient ascent step.
Figure 4: Comparison of a variety of gradient estimators for estimating the gradient of E(φ) =Ez〜BernoUlli(σ(φ))[(z -Po)2], Wherepo ∈ {0.49, 0.499, 0.501, 0.51} and the values of φ are updated via gradientascent with the true gradients. Shown in Rows 1, 2, and 3 are the trace plots of the estimated gradients usingK = 10, 100, and 5000 Monte Carlo samples, respectively.
Figure 5: Comparison of a variety of gradient estimators for estimating the gradient of E(φ) =Ez〜BernouUi(σ(φ))[(z - Po)2], Where po ∈ {0.49, 0.499, 0.501, 0.51} and the values of φ range from —2.5to 2.5. For each φ value, we compute for each estimator K = 1000 single-Monte-Carlo-sample gradientestimates, and use them to calculate their sample mean g, sample standard deviation Sg, and gradient signal-to-noise ratio SNRg = |g|/sg. In each estimator specific column, we plot g, Sg, and SNRg in Rows 1, 2, and 3,respectively. The theoretical gradient standard deviations and gradient signal-to-noise ratios are also shoWn ifthey can be analytically calculated (see Eq. 25 and Appendices C and D and for related analytic expressions).
Figure 6:	Training and validation negative ELBOs on MNIST-threshold with respeCt to the training iterations,shown in the top row, and with respeCt to the wall CloCk times on Tesla-K40 GPU, shown in the bottom row, forthree differently struCtured Bernoulli VAEs.
Figure 7:	Training and validation negative ELBOs on OMNIGLOT with respeCt to the training iterations, shownin the top row, and with respeCt to the wall CloCk times on Tesla-K40 GPU, shown in the bottom row, for threedifferently struCtured Bernoulli VAEs.
Figure 8: Randomly selected example results of predicting the lower half of a MNIST digit given itsupper half, using a binary stochastic network, which has two binary linear stochastic hidden layersand is trained by the ARM estimator based maximum likelihood estimation. Red squares highlightnotable variations between two random draws.
