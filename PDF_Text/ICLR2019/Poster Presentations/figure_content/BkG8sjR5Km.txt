Figure 1: Top-down view with individual camera views of 2v2 multi-agent soccer environment.
Figure 2: L1: selected set of agents in Nash support set with their respective support weights. L2: pair-wiseexpected goal difference among evaluator agents. L3: Elo ratings for all agents computed from tournamentmatches. L4: pair-wise expected goal difference among all agents.
Figure 3: Weighted expected goal difference shown in blue line. Agents’ expected goal difference againsteach evaluator agent in point plot. A dummy evaluator that takes random actions has been introduced to showlearning progress early in the training, with zero weight in the performance computation.
Figure 4: Evolution of hyper-parameters. Hyperparameters of individual agents within the population in gray.
Figure 5: Evolution of relative importance of dense shaping rewards over the course of training. Hyperparam-eters of individual agents within the population in gray.
Figure 6: Evolution of discount factor for each reward component. We show hyperparameters of individualagents within the population in gray.
Figure 7: Behavior statistics evolution.
Figure 8: L1: agent’s average velocity towards the ball. L2: percentage of time when players within a team arespread out. L3: KL divergence incurred by replacing a subset of state with counterfactual information.
Figure 9: L1: Comparison between two snapshots (5B vs 80B) of the same agent. L2: number of successfulpasses and interception occurred in the first 100 timesteps, aggregated over 100 episodes.
Figure 10: Win rate matrix for the Tournament between teams: from top to bottom, ordered by Elo, ascending:ff + evo; ff + evo + rwd_shp; lstm_q + evo + rwd_shp; lstm_q + evo + rwd_shp + channels; lstm + evo +rwd_shp + channels. ELo derived from the tournament is given in the table.
Figure 11: Hyperparameter evolution for three separate seeds, displayed over three separate rows.
Figure 12: On the left red agent 0 has passed to agent 1, who apparently ran into position to receive. On theright blue agent 1 has passed to agent 0.
