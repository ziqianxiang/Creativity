Figure 1: Left: Contingent region in FREEWAY; an object in a red box denotes what is under theagent’s control, whereas the rest is not. Right: A diagram for the proposed ADM architecture.
Figure 2: Learning curves on several Atari games: A2C+CoEX and A2C. The x-axis represents totalenvironment steps and the y-axis the mean episode reward averaged over 40 recent episodes. Themean curve is obtained by averaging over 3 random seeds, each shown in a light color.
Figure 3: Performance plot of ADM trained using on-policy samples from the A2C+CoEX agent.
Figure 6: Learning curves on several Atari games: A2C, A2C+CoEX, and A2C+CoEX+RAM.
Figure 7: Sample of clustering results for Venture, Hero, PrivateEye, and Montezuma’ sRevenge. Each column is one cluster, and we show 3 random samples assigned into this cluster.
Figure 8: Performance of ADM in terms of mean distance under different loss combinations in earlystages, trained using the same online trajectory data. Plots were obtained by averaging runs over 5random seeds.
Figure 9: Learning curves of A2C+CoEX with ADM trained under different training objectives. Thecurve in solid line shows the mean episode over 40 recent episodes, averaged over 3 random seeds.
Figure 10: Learning curves for the ablation study of state representation. The exploration algorithmwithout the contingent region information (purple) performs significantly worse, yielding almostno improvement on hard-exploration games such as Montezuma’ s Revenge, Venture, andFrostbite. The mean curve is obtained by averaging over 3 random seeds. See Table 6 for numbers.
