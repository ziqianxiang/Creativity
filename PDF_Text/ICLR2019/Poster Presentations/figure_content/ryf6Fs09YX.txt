Figure 1: Relating back-propagation (Rumelhart & Hinton, 1986) with the deep GO gradient in Theorem 2.
Figure 2: Gamma (a-c) and NB (d) toy experimental results. (a) The gradient variance of gamma shape α versusiterations, with posterior parameters α0 = 1, β0 = 0.5. (b)-(c) The gradient variance of α and ELBOs versusiterations respectively, when α0 = 0.01, β0 = 0.5. (d) The gradient variance of NB r versus iterations withr0 = 10, p0 = 0.2. In each iteration, gradient variances are estimated with 20 Monte Carlo samples (eachsample corresponds to one gradient estimate), among which the last one is used to update parameters.
Figure 3: Training curves for the discrete VAE experiment with 1-layer linear model (see Appendix I) on thestochastically binarized MNIST dataset(Salakhutdinov & Murray, 2008). All methods are run with the samelearning rate for 1, 000, 000 iterations. The black line represents the best training ELBO of REBAR and RELAX.
Figure 4: Generated samples from BGAN (top) andMNGAN-GO (bottom) trained on 4-bit quantizedMNIST.
Figure 5: ELBOs ofHVIfora 128-64 DEF on MNIST.
Figure 7: A strategy for discrete internal variables. Blue and red circles denote continuous and discrete variables,respectively. The centered dots represent the corresponding distribution parameters. (a) Practically, one uses aneural network (black arrow) to connect the left variable to the parameters of the center discrete one, and thenuses another neural network to propagate the sampled value to the next. (b) Instead, we suggest “extracting” thediscrete variable as a leaf one and propagate its parameters to the next.
Figure 8: Gamma (a-f) and NB (g-l) toy experimental results. Columns show the gradient variance for thefirst parameter (gamma α or NB r), that for the second parameter (gamma β or NB p), and the ELBO,respectively. The first two rows correspond to the gamma toys with posterior parameters α0 = 1, β0 = 0.5and α0 = 0.01, β0 = 0.5, respectively. The last two rows show NB toy results with r0 = 10, p0 = 0.2 andr0 = 0.5, p0 = 0.2, respectively. In each iteration, gradient variances are estimated with 20 Monte Carlosamples (each sample corresponds to one gradient estimate), among which the last one is used to updateparameters. 100 Monte Carlo samples are used to calculate the ELBO in the NB toys.
Figure 9: Training/Validation ELBOs for the discrete VAE experiments. Rows correspond to the experimentalresults on the MNIST/Omniglot dataset with the 1-layer-linear/nonlinear model, respectively. Shown in thefirst/second column is the ELBO curves as a function of iteration/running-time. All methods are run with thesame learning rate for 1, 000, 000 iterations. The black line represents the best training ELBO of REBARand RELAX. ELBOs are calculated using all training/validation data. Note GO does not suffer more fromover-fitting, as clarified in the text.
Figure 10:	Illustration of MNGAN-GO.
Figure 11:	Generated images from BGAN (top) and MNGAN-GO (bottom). Columns correspond to 1-bit(Bernoulli), 1-bit, 2-bit, 3-bit, 4-bit tasks, respectively.
Figure 12: (a) Test data samples and (b) their reconstruction via the learned 128-64-32 DLDA.
