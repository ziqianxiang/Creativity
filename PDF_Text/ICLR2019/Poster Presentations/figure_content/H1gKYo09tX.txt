Figure 1: Example of (a) code summarization of a Java code snippet, and (b) code captioning of aC# code snippet, along with the predictions produced by our models. The highlighted paths in eachexample are the top-attended paths in each decoding step. Because of space limitations we includedonly the top-attended path for each decoding step, but hundreds of paths are attended at each step.
Figure 2: An example of two Java methods that have exactly the same functionality. Although thesemethods have different sequential (token-based) representations, repeating paths, which might differin only a single node (a ForStmt node instead of a Do-while node), will be revealed if we considersyntactic patterns.
Figure 3: Our model encodes each AST path with its values as a vector, and uses the average of allof the k paths as the decoderâ€™s start state. The decoder generates an output sequence while attendingover the k encoded paths.
Figure 4: F1 score compared to the length of the input code. This experiment was performed forthe code summarization task on the Java-med test set. All examples having more than 30 lines werecounted as having 30 lines.
Figure 5:	Example of code captioning for a C# code snippet from our test set. The text boxes atthe bottom of each figure are the predictions produced by our model at each decoding step. Thehighlighted paths in each figure are the top-attended paths in each decoding step, and their widthsare proportional to their attention weight (becau1s6e of space limitations we included only the top-attended path for each decoding step, but hundreds of paths are attended at each step).
Figure 6:	C# examples from our test set for the code captioning task, along with the prediction ofour model and each of the baselines.
Figure 8: Java examples from our test set for the code summarization task, along with the predictionof our model and each of the baselines.
Figure 9:	Visualization of the BLEU score of our model compared to the baselines, for the codecaptioning task. The values are the same as in Table 2. Our model achieves significantly higherresults than the baselines.
Figure 10:	Visualization of the F1 score of our model compared to the baselines, for the codesummarization task, across datasets. The values are the F1 columns from Table 1. Our modelachieves significantly higher results than the baselines.
Figure 11: The relative decrease in precision and recall in each of the ablations, compared to the fullmodel.
