Figure 1: Policy and Training Architecture: Our scheme for learning exploration policies for navigation.
Figure 2: Coverage Performance. Policies are tested on 100 exploration runs (5 random start locations eachon 20 testing houses). Left figure plots average coverage as a function of number of time steps in the episode.
Figure 3: Ablation Study: We report average coverage as a function of time step in episode. As before weplot mean over 3 runs and minimum and maximum performance. Left figure shows that using the RGB imagehelps improve performance, center figure shows that using the map helps improve performance. We can alsosee that imitation learning improves coverage and reduces the variance in performance. Right figure shows thecomparison between different reward design. We can see that our intrinsic reward enables the agent to exploremore efficiently in the testing time.
Figure 4: Exploration for down-stream tasks. We evaluate utility ofexploration for the down-stream nav-igation tasks. Top plots shows ef-fectiveness of the exploration trajec-tory for localization of goal imagesin a new environment. Bottom tablecompares efficiency of reaching goalswithout and with maps built during ex-ploration.
