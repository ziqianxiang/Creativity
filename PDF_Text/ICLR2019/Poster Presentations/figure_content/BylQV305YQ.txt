Figure 1: (a)(c) The number of batches to reach 71% test accuracy on CIFAR10 for 4 variants of ResNet withvarying staleness, using 8 workers and SGD (learning rate 0.01) and Adam (learning rate 0.001). The mean andstandard deviation are calculated over 3 randomized runs. (b)(d) The same metrics as (a)(c), but each modelis normalized by the value under staleness 0 (s = 0), respectively. (e)(f) The number of batches to reach 92%accuracy for MLR and DNN with varying depths, normalized by the value under staleness 0. MLR with SGDdoes not converge within the experiment horizon (77824 batches) and thus is omitted in (f).
Figure 2: (a)(b)(c) The number of batches to reach 71% test accuracy on 1, 8, 16 workers with stalenesss = 0, ..., 16 using ResNet8. We consider 5 variants of SGD: SGD, Adam, Momentum, RMSProp, and Adagrad.
Figure 3: (a) The number of batches to reach training loss of 0.5 for Matrix Factorization (MF). (b) showsthe same metric in (a) but normalized by the values of staleness 0 of each worker setting, respectively (4 and 8workers). (c)(d) Convergence of LDA log likelihood using 10 and 100 topics under staleness levels s = 0, ..., 20,with 2 and 16 workers. The convergence is recorded against the number of documents processed by Gibbssampling. The shaded regions are 1 standard deviation around the means (solid lines) based on 5 randomizedruns. (e)(f) The number of batches to reach test loss 130 by Variational Autoencoders (VAEs) on 1 worker,under staleness s = 0, ..., 16. We consider VAEs with depth 1, 2, and 3 (the number of layers in the encoderand the decoder networks, separately). The numbers of batches are normalized by s = 0 for each VAE depth,respectively. Configurations that do not converge to the desired test loss are omitted in the graph, such as Adamoptimization for VAE with depth 3 and s = 16.
Figure 4: (a)(b) Cosine similarity between the gradient at the k-th iteration VF(xk), and the gradient m stepsprior VF(xk-m), over the course of convergence for ResNet32 on CIFAR10 optimized by SGD (a) and Adam(b) under staleness S = 4 on 8 workers with parameters in Table 1. Shaded region is 1 standard deviation over3 runs. For computational efficiency, we approximate the full gradient VF(xk) by gradients on a fixed set of1000 training samples Df ixed and use VDfixed F(xk). (c) The number of batches to reach 71% test accuracyon CIFAR10 for ResNet8-32 using 8 workers and SGD under geometric delay distribution (details in Appendix).
Figure 5: Gradient coherence for ResNet with varyingdepths optimized by SGD using 8 workers. The x-axism is defined in Fig. 4ically, minimizing the right hand side of Eq. (1) with regard to the maximum staleness s yields theOPtimaI ChOiCe s* = σμ,L(F(xol-iTx F高,to be highly coherent along the past iterates.
Figure 6: The number of batches to reach 95% test accuracy using 1 hidden layer and 1 worker,respectively normalized by s = 0.
Figure 7: The number of batches to reach 92% test accuracy using DNNs with varying numbers ofhidden layers under 1 worker. We consider several variants of SGD algorithms (a)-(e). Note that withdepth 0 the model reduces to MLR, which is convex. The numbers are averaged over 5 randomizedruns. We omit the result whenever convergence is not achieved within the experiment horizon (77824batches), such as SGD with momentum at depth 6 and s = 32.
Figure 8: The number of batches to reach 92% test accuracy with Adam and SGD on 1, 8, 16 workerswith varying staleness. Each model depth is normalized by the staleness 0’s values, respectively.
Figure 9: Convergence of LDA log likelihood using 10 topics with respect to the number of documentsprocessed by collapsed Gibbs sampling, with varying staleness levels and number of workers. Theshaded regions are 1 standard deviation around the means (solid lines) based on 5 randomized runs.
Figure 10: Convergence of LDA log likelihood using 100 topics with respect to the number ofdocuments processed by collapsed Gibbs sampling, with varying staleness levels and the numberof workers. The shaded regions are 1 standard deviation around the means (solid lines) based on 5randomized runs.
Figure 11: Convergence of Matrix Factorization (MF) using 4 and 8 workers, with staleness rangingfrom 0 to 50. The x-axis shows the number of batches processed across all workers. Shaded arearepresents 1 standard deviation around the means (solid curves) computed on 5 randomized runs.
Figure 12: The number of batches to reach test loss 130 by Variational Autoencoders (VAEs) on 1worker, under staleness 0 to 16. We consider VAEs with depth 1, 2, and 3 (the number of layers in theencoder and the decoder networks). The numbers of batches are normalized by s = 0 for each VAEdepth, respectively. Configurations that do not converge to the desired test loss are omitted, such asAdam optimization for VAE with depth 3 and s = 16.
Figure 13: The number of batches to converge (measured using perplexity) for RNNs on 8 workers,under staleness 0 to 16. We consider LSTMs with depth {1, 2, 3, 4}. See section A.8 for details.
Figure 14: The number of batches to converge ((measured using perplexity)) for RNNs on 8 workers,under staleness 0 to 16. We consider 4 variants of SGD: vanilla SGD, Adam, Momentum, RMSProp.
