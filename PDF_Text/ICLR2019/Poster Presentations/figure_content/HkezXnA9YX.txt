Figure 1: Different NMN layouts: NMN-Chain-Shortcut(left), NMN-Chain (center), NMN-Tree (right). See Section3.2 for details.
Figure 2: A positive (top) andnegative (bottom) example fromthe SQOOP dataset.
Figure 3: Top: Comparing the performance of generic models on datasets of varying diffi-culty (lower #rhs/lhs is more difficult). Note that NMN-Tree generalizes perfectly on the hardest#rhs/lhs=1 version of SQOOP, whereas MAC and FiLM fail to solve completely even the easi-est #rhs/lhs=18 version. Bottom: Comparing NMNs with different layouts and modules. We canclearly observe the superior generalization of NMN-Tree, poor generalization of NMN-Chain andmediocre generalization of NMN-Chain-Shortcut. Means and standard deviations after at least 5runs are reported.
Figure 4: Learning dynamics of layout in-duction on 1 rhs/lhs and 18 rhs/lhs datasetsusing the Residual module with p0 (tree) =0.5. All 5 runs do not learn to use the tree lay-out for 1 rhs/lhs, the very setting where thetree layout is necessary for generalization.
Figure 5: Attention quality κ vs accuracy forAttention N2NMN models trained on differ-ent #rhs/lhs splits. We can observe that gen-eralization is strongly associated with high κfor #rhs/lhs=1, while for splits with 2 and 18rhs/lhs blurry attention may be sufficient.
Figure 6: An example of how attention weights of modules 1 (left), 2 (middle), and 3 (right) evolveduring training of an Attention N2NMN model on the 18 rhs/lhs version of SQOOP. Modules 1and 2 learn to focus on different objects words, X and Y respectively in this example, but they alsoassign high weight to the relation word R. Module 3 learns to focus exclusively on R.
Figure 7: Model test accuracy vs κ for the MAC model on different versions of SQOOP. All exper-iments are run 10 times with different random seeds. We can observe a clear correlation between κand error rate for 1, 2 and 4 rhs/lhs. Also note that perfect generalization is always associated withκ close to 1.
