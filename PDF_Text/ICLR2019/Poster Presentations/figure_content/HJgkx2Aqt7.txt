Figure 1:	A high-level overview of our “learning to simulate” approach. A policy πω outputsparameters ψ which are used by a simulator to generate a training dataset. The main task model(MTM) is then trained on this dataset and evaluated on a validation set. The obtained accuracy servesas reward signal R for the policy on how good the synthesized dataset was. The policy thus learnshow to generate data to maximize the validation accuracy.
Figure 2:	Top row: The decision boundaries (shaded areas) of a non-linear SVM trained on datagenerated by q(x, y| ψi) for three different iterations i of our policy πω. The data points overlaid arethe test set. Bottom row: Decision boundary when trained on data sampled from p(x, y| ψreal) (left)and on the converged parameters ψ* (middle); Data sampled from q(x, y| ψ*) (right).
Figure 3: Example of rendered traffic scene with CARLA (Dosovitskiy et al., 2017) and the Unrealengine (Epic-Games, 2018).
Figure 4: (a) shows the reward on the validation set evolving over policy iterations. (b) shows thereward on the unseen test set. We observe that even when using the “adversarial” initialization ofparameters, our approach converges to the same reward R, but at a slower rate. (c) shows the rewardon the unseen test for different random parameter initializations.
Figure 5: (a) Reward R of the accumulated main task model on the car-counting task using differenttraining schemes. We observe that training an accumulated main task network using a learning policyat each step is superior to training it with a dataset generated either using the final parameters ofthe policy or random parameters. (b) Learning-to-simulate converges even with an “adversarial”initialization of parameters, albeit in more epochs. (c) Reward of the accumulated main task modelusing different number of training epochs ξ for hθ .
Figure 6: (a) Reward curves of our approach compared to a model trained on data generated withthe actual validation set parameters on the synthetic semantic segmentation task. (b) Reward curveson the real validation set of KITTI for semantic segmentation. We plot the learning-to-simulate,the maximum reward achieved using random search and the maximum and mean using randomparameters. All methods use 600 iterations.
Figure 7: (a) Shows the evolution of the non-normalized probabilities (logits) of spawning differentcar types in time while the parameters are learned using our method in the car counting task. (b)Shows the evolution of the non-normalized probabilities (logits) of rendering different weather typesin time.
Figure 8: Test and validation error from main task networks trained on 40,000 images during 5 epochsusing final learned parameters for different sizes of datasets generated per policy iteration.
Figure 9: Main task network reward and accumulated MTN reward converge using different randomseeds.
