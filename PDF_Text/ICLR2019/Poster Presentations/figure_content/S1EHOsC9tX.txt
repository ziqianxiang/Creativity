Figure 1: Overview over model architecture. In a nutshell: I) for each sample x we compute a lowerbound on the log-likelihood (ELBO) under each class using gradient descent in the latent space. II) Aclass-dependent scalar weighting of the class-conditional ELBOs forms the final class prediction.
Figure 2: Accuracy-distortion plots for each distance metric and all models. In (b) we see that athreshold at 0.3 favors Madry et al. while a threshold of 0.35 would have favored the Binary ABS.
Figure 3: Adversarial examples for the ABS models are perceptually meaningful: For each sample(randomly chosen from each class) we show the minimally perturbed L2 adversarial found by anyattack. Our ABS models have clearly visible and often semantically meaningful adversarials. Madryet al. requires perturbations that are clearly visible, but their semantics are less clear.
Figure 4: Images of ones classified with aprobability above 90%.
Figure 5:	L0 error quantiles: We always choose the minimally perturbed L0 adversarial found byany attack for each model. For an unbiased selection, we then randomly sample images within fourerror quantiles (0 - 25%, 25 - 50%, 50 - 75%, and 75 - 100%). Where 100% corresponds to themaximal (over samples) minimum (over attacks) perturbation found for each model.
Figure 6:	L? error quantiles: We always choose the minimally perturbed L? adversarial found by anyattack for each model. For an unbiased selection, We then randomly sample 4 images within fourerror quantiles (0 - 25%, 25 - 50%, 50 - 75%, and 75 - 100%).
Figure 7:	L∞ error quantiles: We always choose the minimally perturbed L∞ adversarial found byany attack for each model. For an unbiased selection, we then randomly sample images within fourerror quantiles (0 - 25%, 25 - 50%, 50 - 75%, and 75 - 100%).
Figure 8: Distribution of minimal adversarials for each model and distance metric. In (b) we see thata threshold at 0.3 favors Madry et al. while a threshold of 0.35 would have favored the Binary ABS.
