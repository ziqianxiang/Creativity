Figure 1: Our method is general and can be applied to a broad range of adversarial learning tasks.
Figure 2: Left: Overview of the variational discriminator bottleneck. The encoder first maps sam-ples x to a latent distribution E(z|x). The discriminator is then trained to classify samples z from thelatent distribution. An information bottleneck I(X, Z) ≤ Ic is applied to Z. Right: Visualizationof discriminators trained to differentiate two Gaussians with different KL bounds Ic .
Figure 3: Simulated humanoid performing various skills. VAIL is able to closely imitate a broadrange of skills from mocap data.
Figure 4: Learning curves comparing VAIL to other methods for motion imitation. Performance ismeasured using the average joint rotation error between the simulated character and the referencemotion. Each method is evaluated with 3 random seeds.
Figure 5: Left: Snapshots of the video demonstration and the simulated character trained withVAIL. The policy learns to run by directly imitating the video. Right: Saliency maps that visualizethe magnitude of the discriminator’s gradient with respect to all channels of the RGB input imagesfrom both the demonstration and the simulation. Pixel values are normalized between [0, 1].
Figure 6: Left: Learning curves comparing policies for the video imitation task trained using apixel-wise loss as the reward, GAIL, and VAIL. Only VAIL successfully learns to run from a videodemonstration. Middle: Effect of training with fixed values of β and adaptive β (Ic = 0.5). Right:.
Figure 7: Left: C-Maze and S-Maze. When trained on the training maze on the left, AIRL learnsa reward that overfits to the training task, and which cannot be transferred to the mirrored maze onthe right. In contrast, VAIRL learns a smoother reward function that enables more-reliable transfer.
Figure 8: Comparison of VGAN and other methods on CIFAR-10, with performance evaluatedusing the Frechet Inception Distance (FID).
Figure 9: VGAN samples on CIFAR-10, CelebA 128×128, and CelebAHQ 1024×1024.
Figure 10: Learning curves comparing VAIL to other methods for motion imitation. Performanceis measured using the average joint rotation error between the simulated character and the referencemotion. Each method is evaluated with 3 random seeds.
Figure 11: Learning curves comparing VAIL with a discriminator modeled by a phase-functionedneural network (PFNN), to modeling the discriminator with a fully-conneted network that receivesthe phase-variable φ as part of the input (no PFNN), and a discriminator modeled with a fully-connected network but does not receive φ as an input (no phase).
Figure 12: Left: Accuracy of the discriminator trained using different methods for imitating thedance skill. Middle:. Value of the dual variable β over the course of training. Right: KL lossover the course of training. The dual gradient descent update for β effectively enforces the VDBconstraint Ic .
Figure 13: Left: The C-maze used for training and its mirror version used for testing. Colourcontours show the ground truth reward function that we use to train the expert and evaluate transferquality, while the red and green dots show the initial and goal positions, respectively. Right: Theanalogous diagram for the S-maze.
Figure 14: Visualizations of recovered reward functions transferred to the mirrored C-maze. Alsoshown are trajectories executed by policies trained to maximize the corresponding reward in the newenvironment.
Figure 15: Visualizations of recovered reward functions transferred to the mirrored S-maze, likeFigure 14.
Figure 16: Random results on CIFAR-10 (Krizhevsky et al.): GAN (Goodfellow et al., 2014) FID:63.6, instance noise (S0nderby et al., 2016; Arjovsky & Bottou, 2017) FID: 30.7, spectral normal-ization (SN) (Miyato et al., 2018) FID: 23.9, gradient penalty (GP) (Mescheder et al., 2018) FID:22.6, WGAN-GP Gulrajani et al. (2017b) FID: 19.9, and the proposed VGAN-GP FID: 18.1. Thesamples produced by VGAN-GP (right) look the most realistic where objects like vehicles may bediscerned.
Figure 17: Random VGAN samples on CelebA 128 × 128 at 300k iterations.
Figure 18: VGAN samples on CelebA HQ (Karras et al., 2018) 1024 × 1024 resolution at 300k iter-ations. Models are trained from scratch at full resolution, without the progressive scheme proposedby Karras et al. (2017).
