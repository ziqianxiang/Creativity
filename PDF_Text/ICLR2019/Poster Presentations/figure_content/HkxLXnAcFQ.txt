Figure 1: Baseline and Baseline++ few-shot classification methods. Both the baseline andbaseline++ method train a feature extractor fθ and classifier C(.|Wb) with base class data in thetraining stage In the fine-tuning stage, we fix the network parameters θ in the feature extractor fθand train a new classifier C(.|Wn) with the given labeled examples in novel classes. Thebaseline++ method differs from the baseline model in the use of cosine distances between the inputfeature and the weight vector for each class that aims to reduce intra-class variations.
Figure 2: Meta-Iearning few-shot classification algorithms. The meta-learning classifier M(∙∣S)is conditioned on the support set S. (Top) In the meta-train stage, the support set Sb and the queryset Qb are first sampled from random N classes, and then train the parameters in M(.|Sb) tominimize the N-way prediction loss LN-way. In the meta-testing stage, the adapted classifierM(.|Sn ) can predict novel classes with the support set in the novel classes Sn . (Bottom) The designof M(∙∣S) in different meta-learning algorithms.
Figure 3: Few-shot classification accuracy vs. backbone depth. In the CUB dataset, gaps amongdifferent methods diminish as the backbone gets deeper. In mini-ImageNet 5-shot, somemeta-learning methods are even beaten by Baseline with a deeper backbone. (Please refer toFigure A3 and Table A5 for larger figure and detailed statistics.)However, note that our current setting only uses a 4-layer backbone, while a deeper backbone caninherently reduce intra-class variation. Thus, we conduct experiments to investigate the effects ofbackbone depth in the next section.
Figure 4: 5-shot accuracy in different scenarioswith a ResNet-18 backbone. The Baselinemodel performs relative well with larger domaindifferences.
Figure 5: Meta-learning methods with further adaptation steps. Further adaptation improvesMatchingNet and MAML, but has less improvement to RelationNet, and could instead harmProtoNet under the scenarios with little domain differences.All statistics are for 5-shot accuracywith ResNet-18 backbone. Note that different methods use different further adaptation strategies.
Figure A1: Validation accuracy trends of MAML and MAML with first order approximation.
Figure A2: Intra-class variation decreases as backbone gets deeper. Here we useDavies-Bouldin index to represent intra-class variation, which is a metric to evaluate the tightnessin a cluster (or class, in our case). The statistics are Davies-Bouldin index for all base and novelclass feature (extracted by feature extractor learned after training or meta-training stage) for CUBdataset under different backbone.
Figure A3: Few-shot classification accuracy vs. backbone depth. In the CUB dataset, gapsamong different methods diminish as the backbone gets deeper. In mini-ImageNet 5-shot, somemeta-learning methods are even beaten by Baseline with a deeper backbone.
