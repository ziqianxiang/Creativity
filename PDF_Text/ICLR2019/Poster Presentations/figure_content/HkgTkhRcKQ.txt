Figure 1: Experiments on stochastic counterexample.
Figure 2: Logistic Regression on MNIST.
Figure 3: Multilayer PercePtron on MNIST.
Figure 4:	ResNet on Cifar-10.
Figure 5:	DenSeNet on Cifar-10.
Figure 6: DenseNet on Tiny-ImageNet.
Figure 7: Generative and Recurrent model.
Figure 8: Both β1 and β2 influence the direction and speed of optimization in Adam. Critical valueof Ct , at which Adam gets into non-convergence, increases as β1 and β2 getting large. Leftmost twofor the sequential online optimization problem and rightmost two for stochastic online problem.
Figure 9: Learning rate sensitivity experiment with ResNet on CIFAR-10.
Figure 10: Learning rate sensitivity experiment with DenseNet on CIFAR-10.
Figure 11: βι and β? sensitivity experiment with ReSNet on CIFAR-10.
Figure 12: βι and β2 sensitivity experiment with DenseNet on CIFAR-10.
Figure 13: n sensitivity experiment with DenSeNet on Tiny-ImageNet.
Figure 14: m sensitivity experiment with DenseNet on Tiny-ImageNet.
Figure 15: n and m sensitivity experiment with Neural Machine Translation BLEU.
Figure 16: ReSNetonCIFAR-10.
Figure 17:	DenSeNet on OFAR-10.
Figure 18:	ResNet on CIFAR-10.
Figure 19: DenseNet on CIFAR-10.
Figure 20: DenSeNet on Tiny-ImageNet.
Figure 21: Ill-conditioned quadratic problem, with fixed learning rate.
Figure 22: Ill-conditioned quadratic problem, with linear learning rate decay.
Figure 23: Ill-conditioned quadratic problem, with exp learning rate decay.
Figure 24: Ill-conditioned quadratic problem, with fixed learning rate and insufficient iterations.
