Figure 1: The computational graph of generative (left) and inference (right) models of STCN. The approximateposterior q is conditioned on dt and is updated by the prior p which is conditioned on the TCN representationsof the previous time-step dt-1. The random latent variables at the upper layers have access to a long historywhile lower layers receive inputs from more recent time steps.
Figure 2: Graphical model view of generative models of STCN (left) and STCN-dense (middle), and theinference model (right), which is shared by both variants. Diamonds represent the outputs of deterministicdilated convolution blocks where the dependence of dt on the past inputs is not shown for clarity (see Eq. (2)).
Figure 3: (a) Handwriting samples from IAM-OnDB dataset. Generated samples from (b) VRNN, (c)SWaveNet and (d) our model STCN-dense. Each line corresponds to one sample.
Figure 4: Generative model of STCN-dense architecture. Building blocks are highlighted. Note that thedependence of dt,l = 1 â€¦L on past inputs is not visualized for clarity.
