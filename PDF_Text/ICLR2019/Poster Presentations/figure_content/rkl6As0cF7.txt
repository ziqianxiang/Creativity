Figure 1: Probabilistic recursive reasoning framework. PR2 decouples the connections betweenagents by Eq. 3. 1 : agent i takes the best response after considering all the potential consequences ofopponents’ actions given its own action ai. 2 : how agent i behaves in the environment serves as theprior for the opponents to learn how their actions would affect ai. 3 : similar to 1 , opponents takethe best response to agent i. 4 : similar to 2 , opponents’ actions are the prior knowledge to agent ion estimating how ai will affect the opponents. Looping from step 1 to 4 forms recursive reasoning.
Figure 2: Diagram of multi-agent PR2 learning algorithms. It conducts decentralized training withdecentralized execution. The light grey areas on two sides indicate decentralized execution for eachagent. White areas give the decentralized learning procedures. All agents share the interactionexperiences in the environment represented by dark area in the middle.
Figure 3: Learning paths on the iterated matrix game. a: IGA. b-d: PR2-Q.
Figure 4: Max of Two Quadratic Game.
Figure 5: The learning path of Agent 1 (x-axis) vs. Agent 2 (y-axis).
Figure 6: Performance of PR2-AC on the Particle World environment. Each bar shows the 0 - 1normalized score for agent in cooperative navigation task and the normalized advantage score (agentreward - adversary reward) in a set of competitive tasks. Higher score is better.
