Figure 1: PerformanCe Comparison of ZO-signSGD, ZO-M-signSGD, ZO-SGD, ZO-SCD, signSGD and SGDunder a synthetiC dataset. The solid line represents the loss/aCCuraCy averaged over 10 independent trials withrandom initialization, and the shaded region indiCates the standard deviation of results over random trials. (a)-(b):Training loss and test aCCuraCy versus iterations. (C)-(d): EffeCts of mini-batCh size q and number of randomdireCtion veCtors q on the ConvergenCe of studied algorithms. Here (C) presents the training loss versus iterations,and (d) is the heat map of the final loss for different values of b and q. (e)-(f): EffeCts of problem size d. Here (e)shows the final training loss versus d, and (f) presents the ConvergenCe trajeCtory when d ∈ {200, 400}.
Figure 2: Training loss (left) and testing accuracy (right) of ZO-signSGD, ZO-M-signSGD, ZO-D-signSGDand ZO-SGD versus iterations.
Figure 3: Black-box attacking loss versus iterations. The solid marker indicates the iteration number that findsthe first successful adversarial example, and its loss corresponds to the squared `2 distortion.
Figure A1: Comparison of different gradient-based and gradient sign-based first-order and ZO algorithmsin the example of sparse noise perturbation. The solid line represents the loss averaged over 10 independenttrials with random initialization, and the shaded region indicates the standard deviation of results over randomtrials. Left: Loss value against iterations for SGD, signSGD, ZO-SGD, ZO-signSGD and ZO-signSGD usingthe central difference based gradient estimator (10). Right: Local regions to highlight the effect of the gradientestimators (3) and (10) on the convergence of ZO-signSGD.
Figure A2: Statistics of gradient estimates during an entire training run of the binary classifier provided in thefirst experiment of Sec. 6. a) The `1 norm of the mean of gradient estimates versus iteration. b) Coordinate-wisegradient noise variance versus iteration. The solid line represents the variance averaged over all coordinates, andthe shaded region indicates the corresponding standard deviation with respect to all coordinates at each iteration.
Figure A3: The training loss at the last iteration versus the constant learning rate δ ∈ [10-3, 0.1]. Here thesolid line represents the loss averaged over 10 independent trials with random initialization, and the shadedregion indicates the standard deviation of results over random trials.
Figure A4: Additional plots of black-box attacking loss versus iteration on MNIST. The solid marker indicatesthe iteration number that finds the first successful adversarial example, and its loss corresponds to the squared `2distortion.
Figure A5: Additional plots of black-box attacking loss versus iteration on CIFAR-10. The solid markerindicates the iteration number that finds the first successful adversarial example, and its loss corresponds to thesquared `2 distortion.
