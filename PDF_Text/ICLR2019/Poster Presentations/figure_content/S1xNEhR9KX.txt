Figure 1: Variants of smoothed MNIST and saturated CIFAR10 datasets.
Figure 2: Accuracy, Robust Accuracy and Robustness w.r.t. Predictions on different data variantsexperiments. Unless otherwise specified, PGD attacks on MNIST variants run with = 0.3, stepsize of 0.01 and 40 iterations, and runs with = 8/255, step size of 2/255 and 10 iterations onCIFAR10 variants , same as in Madry et al. (2017). We use the PGD attack implementation fromthe AdverTorch toolbox (Ding et al., 2019).
Figure 3: Illustrations on Practical ImplicationsfMNIST: accuracy, standard training, 92.7%accuracy, PGD training, 81.2%robust accuracy, PGD training, 65.3%efMNIST: accuracy, standard training, 88.3%accuracy, PGD training, 87.2%robust accuracy, PGD training, 86.6%(b) Top: Examples of fashion-MNIST images andedge-fashion-MNIST; bottom: Robustness results onfMNIST and efMNISTadversarial robustness. efMNIST, on the other hand, can be viewed as a set of “more complex bi-nary symbols” compared to MNIST or binarized MNIST. It is harder to classify these more complexsymbols. However, it is easy to achieve high robustness due to the binary pixel value distribution.
Figure 4: Model capacity and training set size’s influences on accuracy and robust accuracy. In eachsubfigure, the top row contains accuracy and robust accuracy measured on training set, the bottomrow contains results measured on test set.
Figure 5: Inter-class distance’s influence on robust accuracy on different MNIST and CIFAR10variantsIntuitively, when the inter-class distance is large, i.e. the gap between two spheres are large, areasonable model should be able to achieve good standard accuracy. We have also observed suchphenomenon on original MNIST and saturated CIFAR10 (say level 16). As the inter-class distancegets smaller, although the model capacity could still be enough for the standard training, it mayno longer be enough for adversarial training, upon which we would observe that although the testaccuracies stay similar, accuracies under adversarial attack significantly would drop. We have alsoseen similar behavior on smooth MNIST data and smaller level of saturated CIFAR10 data. Finally,when the inter-class distance is so small such that even a high clean test accuracy may be difficult toachieve.
Figure 6: Illustration of the relationship between the inter-class distance and the required modelcapacity. Left: when distance is small, a small capacity polytope classifier could separate originaldata; middle: when distance is small, the small capacity polytope classifier is not able to separatedata points “robustly”, but a more complex nonlinear classifier could; right:when distance is large,the small capacity polytope classifier can separate data points “robustly”.
