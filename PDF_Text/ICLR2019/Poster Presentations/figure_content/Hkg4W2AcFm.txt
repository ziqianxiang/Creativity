Figure 1:	Unsupervised learning of disentangled representations on MNIST digits, using Jacobiansupervision. (a) Output of teacher model (k = 2, d = 0) when varying its two hidden units. (b)Output of the final student model (k = 2, d = 14), while varying the same hidden units. TheJacobian supervision makes the model maintain control of the factors of variation of the teacher,while obtaining significantly better reconstruction. (c) A student model (k = 2, d = 14) trainedwithout Jacobian supervision loses the control of the factor of variation discovered by the teacher.
Figure 2:	3rd to 6th principal factors of variation discovered by our unsupervised algorithm. Thefirst two factors of variation are learned by the first teacher model (Figure 1 (a)). Each time a hiddenunit is added to the autoencoder, a new factor of variation is discovered and learned. Each row showsthe variation of the newly discovered factor for three different validation samples, while fixing allthe other variables. The unsupervised discovered factors are related to stroke and handwriting style.
Figure 3: Diagram of the proposed training procedure for facial attributes disentangling. E and Dalways denote the same encoder and decoder module, respectively. Images x1 and x2 are randomlysampled and do not need to share any attribute or class. Their ground truth attribute labels are y 1 andy2 respectively. The latent code is split into a vector predicting the attributes y and an unspecifiedpart z . Shaded E indicates its weights are frozen, i.e., any loss over the indicated output does notaffect its weights.
Figure 4: Disentanglement versus reconstruction trade-off for the facial attribute manipulation ex-ample (top-left is better). The disentangling score measures the ability to flip facial attributes bymanipulating the corresponding latent variables.
Figure 5: Results of attribute manipulation with the student model with Jacobian supervision. Allthe images were produced with the same model and belong to the test set.
Figure 6: Comparison with single-attribute Fader Networks models (Lample et al., 2017). (a) Orig-inal image. (b) Reconstruction of Fader Networks with the provided ’eyeglasses’ model. (c) Ourteacher model achieves a sharper reconstruction using the same latent code dimension, and is ableto effectively manipulate up to 32 attributes, instead of only one. (d) Result of amplifying age withFader Networks with the provided aging model. (e) Our result for the same task.
Figure 7: Results of image reconstruction on test images. Left to right: original image, reconstruc-tion by the student model with Jacobian supervision (d = 8192), by the teacher (d = 2048), and bythe Fader Networks model trained for multiple attributes (d = 8192).
Figure 8: Results of smoothly controlling attributes for our model and Fader Networks. In each row,our result is shown on the top and Fader Networks’ on the bottom. From top to bottom, the attributesare: Arched eyebrows, Bags under eyes, Big nose, Pale skin, Age.
Figure 9:	The two principal factors of variation learned on SVHN appear related to shading of thedigit. Left to right: darker to lighter. Top to bottom: light color on the left to light color on the right.
Figure 10:	Third, fourth and fifth factors of variation automatically discovered on SVHN. Each rowcorresponds to one factor and each column corresponds to one sample. Each factor is varied whilemaintaining the rest of the latent units fixed.
Figure 11:	Factors of variation related to the center digit class appear to emerge on the 9th and10th discovered factor during the unsupervised progressive procedure described in Section 3. Herewe show how the student model with Jacobian supervision and d = 16 can be used to manipulatethe digit class while approximately maintaining the style of the digit, by varying the latent unitscorresponding to those factors. The bottom row shows the original images (reconstructed by theautoencoder). All images are from the test set and were not seen during training.
Figure 12: Result of the student with Jacobian supervision (d = 16) when varying the two factorslearned by the teacher (Fig. 9), for four different images (whose reconstruction is shown on thebottom row). The conditioning related to shading is maintained. (Left to right: darker to lighter.
