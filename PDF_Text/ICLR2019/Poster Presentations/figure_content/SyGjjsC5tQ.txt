Figure 1: Illustrationof the tandem game.
Figure 2: Results in the IPD. (A) Probability that agents cooperate, given memory state, at the endof 50 training runs. SOS and LOLA mostly play tit-for-tat, while others mostly defect. (B) Averageloss at each step, across 300 runs, with shaded deviations. SOS and LOLA outperform all others.
Figure 3: Results in the tandem game. (A) Average loss and (B) average p at each learning step,across 300 runs, with shaded deviations. SOS decays p to avoid arrogance and outperforms LOLA.
Figure 4: Generator distribution at sampled iterations. NL suffers from mode collapse and hop-ping, while CO and SOS learn the correct mixture of Gaussians. Below each plot: KL divergenceDKL(P || Q) from generator P to ground truth Q, estimated from 25600 samples. To the RHS ofeach row: learning rate α. Best result at each iteration shown in bold.
Figure 5: Ground truth in the Gaussian mixture experiment.
Figure 6: Generator distribution at sampled iterations for LA/LOLA/SGA. LA suffers in the earlystages from mode collapse and hopping, but incorporates more mixtures later on. LOLA and SGAlearn the correct mixture of Gaussians. Below each plot: KL divergence DKL(P || Q) from gener-ator P to ground truth Q, estimated from 25600 samples. Best result at each iteration in bold.
Figure 7: Semilog plot of kξ k at each iteration for SOS, LA and NL.
