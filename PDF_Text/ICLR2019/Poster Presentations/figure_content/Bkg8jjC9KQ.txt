Figure 1: Mirror descent (MD) in the non-monotone saddle-point problem f (x1, x2) = (x1 - 1/2)(x2 - 1/2) +1 exp(-(XI - 1/4)2 - (X2 - 3/4)2). Left: vanilla MD spirals outwards; right: optimistic MD converges.
Figure 2: Different algorithmic benchmarks (RMSprop and Adam): adding an extra-gradient step allows thetraining method to accurately learn the target data distribution and eliminates cycling and oscillatory instabilities.
Figure 3: Left: Inception score (left) and Fr6chet distance (right) on CIFAR-10 When training with Adam (withand without an extra-gradient step). Results are averaged over 8 sample runs with different random seeds.
Figure 4: Samples generated by Adam with an extra-gradient step on CelebA (left) and CIFAR-10 (right).
Figure 5: Trajectories of vanilla and optimistic mirror descent in a zero-sum game of Matching Pennies (leftand right respectively). Colors represent the contours of the objective, f(x1, x2) = (x1 - 1/2)(x2 - 1/2).
Figure 6: GAN training with and without an extra-gradient step in the CelebA and CIFAR-10 datasets.
