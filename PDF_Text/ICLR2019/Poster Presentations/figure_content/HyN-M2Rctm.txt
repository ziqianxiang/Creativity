Figure 1: In mode normalization, incoming samples {xn}nN=1 are weighted by a set of gatingfunctions {gk}3ι∙ Gated samples contribute to component-wise estimators μk and σk, under whichthe data is normalized. After a weighted sum, the batch is passed on to the next layer. After training,the shifts and scales {μk,σk}K=ι are taken from running averages rather than batch statistics.
Figure 2: Channel-wise histograms for conv3-64-1 in VGG13. Top row shows BN before andafter the normalization is applied to samples from CIFAR10. The bottom row shows how the layer’sdistribution is transformed when MN is inserted into the network.
Figure 3: Test samples from CIFAR10 that were clustered together by two experts in an early layer(top) and a deeper layer (bottom) of VGG13.
