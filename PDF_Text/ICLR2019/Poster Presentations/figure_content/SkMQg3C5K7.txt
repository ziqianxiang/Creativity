Figure 1: Experimental results. (a) Convergence of gradient descent training deep linear neural networks(depths 3 and 8) under customary initialization of layer-wise independent Gaussian perturbations with mean 0and standard deviation s. For each network, number of iterations required to reach = 10-5 from optimaltraining loss is plotted as a function of s (missing values indicate no convergence within 106 iterations). Datasetin this experiment is a numeric regression task from UCI Machine Learning Repository (details in text). Noticethat fast convergence is attained only in a narrow band of values for s, and that this phenomenon is more ex-treme with the deeper network. (b) Same setup as in (a), but with layer-wise independent initialization replacedby balanced initialization (Procedure 1) based on Gaussian perturbations with mean 0 and standard deviation s.
Figure 2: Figure for proof of Lemma 15. The dashed region denotes D. Not to scale.
