Table 1: The Statistics of DatasetsDataset	Nodes	Edges	Classes	Features	Label RateCora	2,708	5,429	7~	1,433	0.052Citeseer	3,327	4,732	6	3,703	0.036Pubmed	19,717	44,338	3	500	0.0035Published as a conference paper at ICLR 20194.2	BaselinesWe compare with several traditional semi-supervised learning methods, including label propagation(LP) (Zhu et al., 2003), semi-supervised embedding (SemiEmb) (Weston et al., 2012), manifoldregularization (ManiReg) (Belkin et al., 2006), graph embeddings (DeepWalk) (Perozzi et al., 2014),iterative classification algorithm (ICA) (Lu & Getoor, 2003) and Planetoid (Yang et al., 2016).
Table 2: Results of Detaching Feature Transformation from Convolution	Method	Cora	Citeseer	PubmedPrediction Accuracy	ChebyNet DetaChing-ChebyNet	81.2% 81.6%	69.8% 68.5%	74.4% 78.6%Number of Parameters	ChebyNet Detaching-ChebyNet	46,080 (K=2) 23,048 (K=4)	178,032 (K=3) 59,348 (K=2)	24,144 (K=3) 8,054 (K=3)As demonstrated in Table 2, with fewer parameters, we improve the accuracy on Pubmed by a largemargin. This is due to that the label rate of Pubmed is only 0.003. By detaching feature transfor-mation from convolution, the parameter complexity is significantly reduced, alleviating overfittingin semi-supervised learning and thus remarkably improving prediction accuracy. On Citeseer, thereis a little drop on the accuracy. One possible explanation is that reducing the number of parametersmay restrict the modeling capacity to some degree.
Table 3: Results of Node ClassificationMethod	Cora	Citeseer	PubmedMLP	55.1%	46.5%	71.4%ManiReg	59.5%	60.1%	70.7%SemiEmb	59.0%	59.6%	71.7%LP	68.0%	45.3%	63.0%DeepWalk	67.2%	43.2%	65.3%ICA	75.1%	69.1%	73.9%Planetoid	75.7%	64.7%	77.2%Spectral CNN	73.3%	58.9%	73.9%ChebyNet	81.2%	69.8%	74.4%GCN	81.5%	70.3%	79.0%MoNet	81.7±0.5%	—	78.8±0.3%GWNN	82.8%	一	71.7%	79.1%datasets. In particular, replacing Fourier transform with wavelet transform, the proposed GWNNis comfortably ahead of Spectral CNN, achieving 10% improvement on Cora and Citeseer, and 5%improvement on Pubmed. The large improvement could be explained from two perspectives: (1)Convolution in Spectral CNN is non-local in vertex domain, and thus the range of feature diffusionis not restricted to neighboring nodes; (2) The scaling parameter s of wavelet transform is flexible toadjust the diffusion range to suit different applications and different networks. GWNN consistently
Table 4: Statistics of wavelet transform and Fourier transform on Cora	Statistical Property	wavelet transform	Fourier transformTransform Matrix	Density Number of Non-zero Elements	2.8% 205,774	99.1% 7,274,383Projected Signal	Density Number of Non-zero Elements	10.9% 297	100% 2,7084.7	Analysis on Interpretab ilityCompare with graph convolution network using Fourier transform, GWNN provides good inter-pretability. Here, we show the interpretability with specific examples in Cora.
Table 5: Parameter complexity of Node ClassificationMethod	Cora	Citeseer	PubmedSpectral CNN	-62,392,320	197,437,488	158,682,416Spectral CNN (detaching)	28,456	65,379	47,482ChebyNet	46,080 (K=2)	178,032 (K=3)	24,144 (K=3)GCN	23,040	59,344	8,048GWNN	28,456	65,379	47,482In Cora and Citeseer, with smaller parameter complexity, GWNN achieves better performance thanChebyNet, reflecting that it is promising to implement convolution via graph wavelet transform. AsPubmed has a large number of nodes, the parameter complexity of GWNN is larger than ChebyNet.
Table 6: Statistics of spectral transform and Laplacian matrix on Cora	Density	Num of Non-zero Elementswavelet transform	^^8%~~	205,774Fourier transform	99.1%	7,274,383Laplacian matrix	0.15%	10,858The sparsity of Laplacian matrix is sparser than graph wavelets, and this property limits our method,i.e., the higher time complexity than some methods depending on Laplacian matrix and identity ma-trix, e.g., GCN. Specifically, both our method and GCN aim to improve Spectral CNN via designinglocalized graph convolution. GCN, as a simplified version of ChebyNet, leverages Laplacian ma-trix as weighted matrix and expresses the spectral graph convolution in spatial domain, acting asspatial-like method (Monti et al., 2017). However, our method resorts to using graph wavelets as anew set of bases, directly designing localized spectral graph convolution. GWNN offers a localizedgraph convolution via replacing graph Fourier transform with graph wavelet transform, finding goodspectral basis with localization property and good interpretability. This distinguishes GWNN fromChebyNet and GCN, which express the graph convolution defined via graph Fourier transform invertex domain.
