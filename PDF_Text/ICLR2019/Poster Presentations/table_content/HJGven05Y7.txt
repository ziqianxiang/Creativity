Table 1: MAML++ Omniglot 20-way Few-Shot Results: Our reproduction of MAML appears tobe replicating all the results except the 20-way 1-shot results. Other authors have come acrossthis problem as well Jamal et al. (2018). We report our own base-lines to provide better relativeintuition on how each method impacted the test accuracy of the model. We showcase how ourproposed improvements individually improve on the MAML performance. Our method improveson the existing state of the art.
Table 2: MAML++ Mini-Imagenet Results. MAML++ indicates MAML + all the proposed fixes.
Table 3: Mini-Imagenet Training Iteration Timing table. In this table, one can see per trainingiteration wall-clock timings for MAML vs MAML++. We provide timings for variants of the modelspanning 1 to 5 inner loop steps. It can be observed that MAML++ needs less time per trainingiteration, even though it needs more parameters and has more computations needed. We can see thatmore steps require more computation in return for better generalization performance as evidencedin Table 2.__________________________________________________________________________________Inner Loop Steps	1	2	3	4	5MAML ++ (ms/iter)	275.319	433.8172	579.314	786.3278	947.0376MAML (ms/iter)	294.373	475.1218	658.4436	859.1158	1028.165610Published as a conference paper at ICLR 2019Table 4: MAML++ Omniglot 5-way Results: MAML++ indicates MAML + all the proposed fixes.
Table 4: MAML++ Omniglot 5-way Results: MAML++ indicates MAML + all the proposed fixes.
