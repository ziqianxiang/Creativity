Table 1: Different choices of g(x), their corresponding proximal operators, and their relation tocommon activation functions. Squared hinge loss regularization of the activations yields a gener-alized Leaky ReLU. Log-barriers recover smooth activations, such as SoftPlus, Tanh, or Sigmoid.
Table 2: Comparison in accuracy between variants of the LeNet architecture on the CIFAR-10dataset. The variants differ in the location (D or ND) and number of BerDropoutp layers for both thebaseline networks and their stochastic solver counterpart Prox-SG. Accuracy consistently improveswhen Prox-SG is used. Accuracy is reported on the test set.
Table 3: Replacing the first convolutional layer of AlexNet by the deterministic Prox-GD solveryields consistent improvement in test accuracy on CIFAR-10 and CIFAR-100.
Table 4: Experiments with the VGG16 architecture on CIFAR-10 and CIFAR-100. Accuracy isreported on the test set.
Table 5: Comparison of the VGG16 architecture trained on CIFAR-100 with varying dropout ratesp in the last BerDropoutp layer. We compare the baseline to its stochastic solver counterpart withcorresponding sampling rate Ï„ = (1 - p)n1. Accuracy is reported on the test set.
