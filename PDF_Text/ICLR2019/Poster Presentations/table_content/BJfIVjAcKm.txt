Table 1: Improvement in provable adversarial accuracy and verification solve times when incremen-tally adding natural regularization methods for improving weight sparsity and ReLU stability intothe model training procedure, before verification occurs. Each row represents the addition of anothermethod - for example, RoW 3 uses adversarial training, '1-regularization, and small weight pruning.
Table 2: Provable Adversarial Accuracies for the control and “+RS” networks in each setting.
Table 3: Comparison of test set, PGD adversarial, and provable adversarial accuracy of networkstrained with and without RS Loss. We also provide the best available certifiable adversarial andPGD adversarial accuracy of any single models from Wong et al. (2018), Dvijotham et al. (2018),and Mirman et al. (2018) for comparison, and highlight the best provable accuracy for each .
Table 4: Comparison between the average number of unstable ReLUs as found by the exact verifierof Tjeng et al. (2017) and the estimated average number of unstable ReLUs found by improved IAand naive IA. We compare these estimation methods on the control and “+RS” networks for MNISTthat we described in Section 3.3.415Published as a conference paper at ICLR 2019C.4 On the Conservative Nature of IA BoundsThe upper and lower bounds we compute on each ReLU via either naive IA or improved IA areconservative. Thus, every unstable ReLU will always be correctly labeled as unstable, while stableReLUs can be labeled as either stable or unstable. Importantly, every unstable ReLU, as estimatedby IA bounds, is correctly labeled and penalized by RS Loss. The trade-off is that stable ReLUsmislabeled as unstable will also be penalized, which can be an unnecessary regularization of themodel.
Table 5: The addition ofRS Loss results in far fewer ReLUs labeled as unstable for both 3-layer and6-layer (Large) networks. The decrease in test set accuracy as a result of this regularization is small.
Table 6: Weights chosen using line search for `1 regularization and RS Loss in each settingFor each setting, we find a suitable weight on RS Loss via line search. The same weight is used foreach ReLU. The five weights we chose are displayed above in Table 6, along with weights chosenfor `1 -regularization.
Table 7: Full results on natural improvements from Table 1, control networks (which use all of thenatural improvements and ReLU pruning), and “+RS” networks from Tables 2 and 3. While we areunable to determine the true adversarial accuracy, we provide two upper bounds and a lower bound.
