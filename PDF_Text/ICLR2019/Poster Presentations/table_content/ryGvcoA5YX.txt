Table 1: Average accuracy over all tasks in a sequence after the tasks have all been learned.
Table 2: Average accuracy over 5 tasks in a sequence after the tasks have all been learned.
Table 3:	Average aCCuraCy with different % of parameters in solver’s last layer (one basiC unit) beingreplaCed by the parameters generated by DPG. EaCh sCore followed by a 95% ConfidenCe interval.
Table 4: Empirical evaluation of differ-ent componentsComponentsDPG + DG + ConstraintsDG + ConstraintsDG + label (like GR)Acc (%)96.07 ± 0.6290.96 ± 0.6988.21 ± 2.814 Related WorkMany approaches have been proposed to deal with catastrophic forgetting (CF), which is one of thechallenging problems of neural networks for lifelong learning (Chen & Liu, 2018). EWC (Kirkpatricket al., 2017) quantifies the importance of weights to previous tasks, and selectively alters the learningrates of weights. Following EWC, Zenke et al. (2017) measured the synapse consolidation strengthin an online fashion and used it as regularization. Learning without forgetting (LWF) (Li & Hoiem,2016) feeds the old network with new training data in new tasks and regards the output as ”pseudo-labels”. In Incremental Moment Matching (IMM) (Lee et al., 2017), each trained network on onetask is preserved and all networks are merged into one at the end of the sequence of tasks.
Table 5: Parameter settings for different datasetsMNIST: We set the basic unit (hidden layer) size of the 3-layer classifier to 600 and the droPout rate to 0.3. A 3-layer convolu-tional network (CNN), with 2 * 2 convolutions and 64, 128, 256 filters for each layer, is used as DG’s encoder. A deconvolutionalnetwork with the symmetric setting as the encoder is used as the decoder in DG. For our model, we use the Parameters generatedby DPG to rePlace 33% of the Parameters in the solver’s last hidden layer, and 10% of the first hidden layer.
Table 6: Empirical evaluation of different componentsTime	EWC	IMM	GR	Our PGMATraining Time/per epoch (s)	2.460	4.930	8.673	10.836Test time(s)	0.703	0.707	0.728	0.930ADAM gets the best accuracy on the last task as Adam optimizer learns the new task only and doesnot need to worry about the forgetting problem on the old tasks. For the last task, our PGMA systemobtained comparable accuracy on average with the baselines designed for overcoming forgetting.
Table 7: Accuracy of the last task after all tasks have been learned.
