Table 1: Mean and median of best scores computed across the Atari 2600 games from Table 3 and4 in the appendix, measured as human-normalized percentages (Nair et al., 2015). QR-DQN andIQN scores obtained from Table 1 in Dabney et al. (2018a), by removing the scores of Defender andSurround. DQN-IDS and C51-IDS averaged over 3 seeds.
Table 2: ALE hyperparametersHyperparameter	Value	Descriptionλ	0.1	Scale factor for computing regret surrogateρ2	1.0	Observation noise variance for DQN-IDS1 , 2	0.00001	Information-ratio constants; prevent division by 0mini-batch size	32	Size of mini-batch samples for gradient descent stepreplay buffer size	1M	The number of most recent observations stored in the replay bufferagent history length	4	The number of most recent frames concatenated as input to the networkaction repeat	4	Repeat each action selected by the agent this many timesγ	0.99	Discount factortraining frequency	4	The number of times an action is selected by the agent be- tween successive gradient descent stepsK	10	Number of bootstrap headsβ1	0.9	Adam optimizer parameterβ2	0.99	Adam optimizer parameterADAM	0.01/32	Adam optimizer parameterα	0.00005	learning ratelearning starts	50000	Agent step at which learning starts. Random policy before- handnumber of bins	51	Number of bins for Categorical DQN (C51)[VMIN , VMAX ]	[-10, 10]	C51 distribution rangenumber of quantiles target network	200	Number of quantiles for QR-DQN
Table 3: Raw evaluation scores. Episodes start with up to 30 no-op actions. Reference values fromWang et al. (2016) and Osband et al. (2016a). DQN-IDS averaged over 3 seeds. Bootstrap DQNscores for Berzerk, Phoenix, Pitfall!, Skiing, Solaris and Yars’ Revenge obtained from our customimplementation.
Table 4: Raw evaluation scores. Episodes start with up to 30 no-op actions. Reference values(available for a single seed) for C51, QR-DQN and IQN taken from Dabney et al. (2018b) andDabney et al. (2018a). C51-IDS averaged over 3 seeds.
