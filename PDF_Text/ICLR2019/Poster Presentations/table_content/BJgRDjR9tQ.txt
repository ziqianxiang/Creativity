Table 1: Comparison of various robust mean estimation methods. Net structure: One-hidden layer networkwith 20 hidden units when n = 50, 000 and 2 hidden units when n = 5, 000. The number in each cell is theaverage of`2 error kθ - θk with standard deviation in parenthesis estimated from 10 repeated experiments andthe smallest error among four methods is highlighted in bold.
Table 2: Experiment results for JS-GAN using networks with different structures in high dimension. Settings:e = 0.2, P ∈ {200,400} and n = 50,000.________________________________________________P	200-100-20-1	200-200-100-1	200-100-1	200-20-1200	0.0910 (0.0056)	0.0790 (0.0026)~~	0.3064 (0.0077)	0.1573 (0.08l5)-P	400-200-100-50-20-1	400-200-100-20-1	400-200-20-1	-400-200-1400	0.1477 (0.0053)	0.1732 (0.0397)	0.1393 (0.0090)	0.3604 (0.0990)-9Published as a conference paper at ICLR 20195.5	Adaptation to Unknown CovarianceThe robust mean estimator constructed through JS-GAN can be easily made adaptive to unknown covariancestructure, which is a special case of (16). We define(θ, Σ) = argmin maxη∈Rp,Γ∈Ep D∈D1nlo ɪ2 log D(Xi) + EN(η,Γ) IOg(I- D(Xi))n i=1+ log 4,The estimator θ, as a result, is rate-optimal even when the true covariance matrix is not necessarily identity andis unknown (see Theorem 4.1). Below, we demonstrate some numerical evidence of the optimality of θbas wellas the error of Σ in Table 3.
Table 3: Numerical experiments for robust mean estimation with unknown covariance trained with 50, 000samples. The covariance matrices Σ1 and Σ2 are generated by the same way described in Appendix B.2.
Table 4: Comparison of various methods of robust location estimation under Cauchy distributions. Samplesare drawn from (1 - e)Cauchy(0p, Ip) + eQ with e = 0.2, p = 50 and various choices of Q. Sample size:50,000. Discriminator net structure: 50-50-25-1. Generator gω(ξ) structure: 48-48-32-24-12-1 with absolutevalue activation function in the output layer.
Table 5: Choices of hyper-parameters. The parameter λ is the penalty factor for the regularization term (17)and other parameters are listed in Algorithm 1. We apply Xavier initialization (Glorot & Bengio, 2010) forboth JS-GAN and TV-GAN trainings.___________________________________________________Structure	Net	Yg	Yd	K	T	To	λ100-20-1	JS	0.02	0.2		150	~5Γ	0	TV	0.0001	0.3	2	150	1	0.1100-2-1	JS	0.01	0.2		150	^5"	0	TV	0.01	0.1	5	150	1	1200-20-1	JS	0.02	0.2		200	^5"	0	TV	0.0001	0.1	2	200	1	0.5200-200-100-1	JS	0.005	0.1	~2~	200	^5"	0400-200-20-T-	JS	0.02	0.05	~2~	250	^5"	0.5with deep network structures can significantly improve over shallow networks especially when thedimension is large (e.g. p ≥ 200). For a network with one hidden layer, the choice of width maydepend on the sample size. If we only have 5,000 samples of 100 dimensions, two hidden unitsperforms better than five hidden units, which performs better than twenty hidden units. If we have50,000 samples, networks with twenty hidden units perform the best.
Table 6: Experiment results for JS-GAN and TV-GAN with various network structures.
Table 7: Scenario I:，p/n < e. Setting: P = 100, n = 50,000, and e from 0.05 to 0.20. Network structure ofJS-GAN: one hidden layer with 5 hidden units. Network structure of TV-GAN: zero-hidden layer. The numberin each cell is the average of `2 error kθ - θk with standard deviation in parenthesis estimated from 10 repeatedexperiments. The bold character marks the worst case among our choices of Q at each e level. The results ofTV-GAN for Q = N(5 * 1p, Ip) are highlighted in slanted font. The failure of training in this case is due to thebad landscape when N(0p, Ip) and Q are linearly separable, as discussed in Section 3.1 (see Figure 1).
Table 8: Scenario II-a: /p/n > e. Setting: n = 1,000, e = 0.1, and P from 10 to 100. Other details are thesame as above.
Table 9: Scenario II-b:，p/n > e. Setting: P = 50, e = 0.1, and n from 50 to 1,000. Other details are thesame as above.
