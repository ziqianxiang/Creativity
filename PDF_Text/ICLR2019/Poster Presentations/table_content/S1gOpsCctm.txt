Table 1: Moore Machine extraction for MCEGame	Bh, Bf	Fine-Tuning Score		Before Minimization			After Minimization				Before (%)	After (%)	H	O	Acc. (%)	H	O	Acc. (%)Amnesia	~~^A 4,8	0.98 0.99	1 1	~^Γ~ 7	5~5~ 7	-1- 1	4^~ 4	4^~ 4	-1- 1	84 8,8	-1- 0.99	- 1	6~β~ 7	"ɪ 7	-1- 1	4~Γ~ 4	4~Γ~ 4	-1- 1Blind	4,8	-1- 1	- -	~2^1~ 12	6~6~ 8	-1- 1	^T0- 10	~^~ 1	-1- 1	84 8,8	-1- 0.78	- 1	13	6~6~ 8	-1- 1	^T0- 10	~^~ 1	-1- 1Tracker	~^A 4,8	0.98 0.99	0.98 1	^38^ 23	"ɪ 5	0.98 1	^30- 10	4~i~ 4	0.98 1	84 8,8	0.98 0.99	1 1	-9^ 85	"ɪ 5	-1- 1	^T0- 10	4~t~ 4	-1- 15.2	Tomita GrammarsThe Tomita Grammars are popular benchmarks for learning finite state machines (FSMs), includingwork on extracting FSMs from RNNs (e.g. (Watrous & Kuhn, 1992; Weiss et al., 2017)). Here weevaluate our approach over the 7 Tomita Grammars4, where each grammar defines the set of binarystrings that should be accepted or rejected. Since, our focus is on policy learning problems, we treatthe grammars as environments with two actions ‘accept’ and ‘reject’. Each episode corresponds toa random string that is either part of the particular grammar or not. The agent receives a reward of 1if the correct action accept/reject is chosen on the last symbol of a string.
Table 2: Moore Machine extraction for Tomita grammar#Grammar	RNN Acc. (%)	Bh	Fine-Tuning Acc.(%)		Before Minimization		After Minimization				Before	After	间	Acc. (%)	同	Acc. (%)		-8-	100	-	^T3-	100	2^Γ~	1001	100	16	100	-	28	100	2	100		-8-	100	-	^T3-	100	3^~	1002	100	16	100	-	14	100	3	100		-8-	100	-		100	5~5~	1003	100	16	100	-	39	100	5	100		-8-	100	-	~7∏~	100	4~Γ~	1004	100	16	100	-	18	100	4	100		-8-	-95-	96	T9^	96	TT^	965	100	16	100	-	316	100	4	100		-8-	-98-	98	T00^	98	~~DT	~~98-6	99	16	99	99	518	99	11	99		-8-	100	-	2525~	100	5~5~	1007	100	16	100	-	107	100	5	100solutions. In all cases, except for grammar 6 the minimized machines are identical to the minimalmachines that are known for these grammars (Tomita, 1982).
Table 3: Moore Machine extraction for trained Atari RNN policies. DQN (Mnih et al., 2015), A3C(Mnih et al., 2016) scores have been reported for performance comparison with trained policies.
