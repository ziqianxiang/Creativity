Table 1: Cumulative regret results on the wheel bandit problem with varying δ values. Results arenormalized with the performance of the uniform agent (as was done in Riquelme et al. (2018)) andresults shown are mean and standard error for cumulative regret calculated across 50 trialsδ. We can create a random training episode for pre-training by first sampling M different wheelproblems {δi}M=ι,δi 〜U(0,1), followed by sampling tuples of the form {(X, a, r)}" for contextX, action a, and observed reward r. As in Garnelo et al. (2018b), we use M = 64 and N = 562(where the support set has 512 items and the query set has 50 items). We then evaluate the trainedmeta-learning models on specific instances of the wheel bandit problem (determined by setting the δhyperparameter). Whereas the models in Riquelme et al. (2018) have no prior knowledge to start offwith when being evaluated on each problem, meta-learning methods, like our model and MAML,have a chance to develop some sort of prior that they can utilize to get a head start. MAML learns ainitialization of the neural network that it can then fine-tune to the given problem data, whereas ourmethod develops a prior over the model parameters that can be utilized to develop an approximateposterior given the new data. Thus, we can straightforwardly apply Thompson sampling in our modelusing the approximate posterior at each time step whereas for MAML we just take a greedy action ateach time step given the current model parameters.
Table 2: Few-shot classification accuracies with 95% confidence intervals on CIFAR-100 andminiImageNet.
Table 3: Hyperparameters for contextual bandit experiments. These hyperparameters were used forboth MAML and our model when comparing them. Hyperparameters tf and ts were used as definedin Riquelme et al. (2018) .
Table 4: Hyperparameters for our model for few-shot learning experiments.
