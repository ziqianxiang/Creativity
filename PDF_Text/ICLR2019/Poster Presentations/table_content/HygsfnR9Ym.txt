Table 1: On-Policy TRPO Hyper-params:- Length of the trajectory sampled from our backtrackingmodelE.2 Off-Policy. SACFor training the backtracking model, we used the high value states (under the current value function)from the buffer B . We sample a batch of 20K high value tuples from the experience replay buffer.
Table 2: Off Policy SAC Hyperparams:- Length of the trajectory sampled from our backtrackingmodelE.3 Prioritized Experience ReplayThe Experience Replay Buffer capacity was fixed at 100k. No entropy regularization was used.
Table 3: Hyperparameters for the PER Implementation.
