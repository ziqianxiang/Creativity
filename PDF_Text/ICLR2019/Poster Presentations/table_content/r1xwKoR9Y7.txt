Table 1: Training time speedups for the GRU model with state size of 128 on the position evaluationtask obtained compared to an un-optimized baseline. * indicates CPU and t indicates GPU.
Table 2: Test accuracies for position evaluation (Pos) and tactic prediction (Tac). f indicates kernel-level. t indicates mid-level without implicit arguments. For tactic argument prediction, We reportvalidation recall for models with a minimum precision of 10%Model	Post	POS^	TaCt	Tad	Tact argumentsConstant	53.66	53.66	44.75	44.75	-SVM	57.37	57.52	48.94	49.45	-GRU	65.30	65.74	58.23	57.70	25.98TreeLSTM	68.44	66.30	60.63	60.55	23.91Scaling to large proof trees In practice, ASTs can be on the order of thousands of nodes. We canapply two optimizations to scale to larger trees. The first optimization involves embedding sharing,where we memoize the embedding and the associated computation graph for any expression orsubtree when it appears another time in the same proof state. For instance, if the context has termsM1 M2 and M1 , the embedding for M1 is computed once. A single forward and backward pass isthus performed on this computation graph, with the gradients automatically accumulating from allplaces where the embedding was used. It thus helps save both memory and computation time ofour models. The second optimization involves dynamic batching (Looks et al., 2017; Polosukhin& Zavershynskyi, 2018), which enables us to efficiently batch the computation of ops that performthe same operation albeit on different inputs, and thus make better use of the parallel accelerationprovided by multi-core CPU’s and GPU’s. 5Table 1 shows the approximate speedups obtained from applying embedding sharing and dynamic
