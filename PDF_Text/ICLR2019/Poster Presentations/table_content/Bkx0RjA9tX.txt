Table 1: Exact Match (EM) and F1 on SQuAD, comparing to the best published single models atthe time of submission (September 2018).
Table 2: Development results on SQuAD for model ablations.
Table 3: Development results on SQuAD, varying the beam size during inference.
Table 4: Test results on CLEVR, demonstrating high accuracy at complex reasoning. GQA is thefirst approach to achieve high performance on both CLEVR and broad coverage QA tasks.
Table 5: Exact Match (EM) and F1 on biased subsets of SQuAD. All answers in each subset havethe indicated named-entity type; training documents have only one answer with this type, but fortesting there are multiple plausible answers. Discriminative models perform comparably to question-agnostic baselines, whereas our generative model learns to generalise.
Table 6: F1 scores on AdversarialSQuAD (from September 2018), which demonstrate thatour generative QA model is substantially more robust to this adversary than previous work, likelybecause the additional adversarial context sentence cannot explain all the question words.
Table 7: F1 scores on full document evaluation for SQuAD, which show our generative QA modelis capable of selecting the correct paragraph for question answering even when presented with othersimilar paragraphs. Baselines are from (Raison et al., 2018).
