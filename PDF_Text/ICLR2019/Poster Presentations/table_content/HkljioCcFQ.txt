Table 1: Comparison of the proposed MAAN with four baseline feature aggregators on the THU-MOS14 test set. All values are reported in percentage. The last column is the classification mAP.
Table 2: Comparison of our algorithm to the previous approaches on THUMOS14 test set. AP (%) isreported for different IoU thresholds. Both the fully-supervised and the weakly-supervised results arelisted. (“UN”： using UntrimmedNet features, “I3D”: using I3D features, “ours”: our implementation.)Supervision	Methods	0.1	0.2	0.3	AP 0.4	@IoU 0.5	0.6	0.7	0.8	0.9	Richard et al. (Richard & Gall, 2016)	39.7	35.7	30.0	23.2	15.2	-	-	-	-	Shou et al. (Shou et al., 2016)	47.7	43.5	36.3	28.7	19.0	10.3	5.3	-	-	Yeung et al. (Yeung et al., 2016)	48.9	44.0	36.0	26.4	17.1	-	-	-	-Fully	Yuan et al. (Yuan et al., 2016)	51.4	42.6	33.6	26.1	18.8	-	-	-	-Supervised	Shou et al. (Shou et al., 2017)	-	-	40.1	29.4	23.3	13.1	7.9	-	-	Yuan et al. (Yuan et al., 2017b)	51.0	45.2	36.5	27.8	17.8	-	-	-	-	Xu et al. (Xu et al., 2017)	54.5	51.5	44.8	35.6	28.9	-	-	-	-	Zhao et al. (Zhao et al., 2017)	66.0	59.4	51.9	41.0	29.8	-	-	-	-	Wang et al. (Wang et al., 2017)	44.4	37.7	28.2	21.1	13.7	-	-	-	-Weakly Supervised	Singh & Lee (Singh & Lee, 2017)	36.4	27.8	19.5	12.7	6.8	-	-	-	-	STPN (Nguyen et al., 2018) (UN)	45.3	38.8	31.1	23.5	16.2	9.8	5.1	2.0	0.3	STPN (Nguyen et al., 2018) (I3D)	52.0	44.7	35.5	25.8	16.9	9.9	4.3	1.2	0.1	STPN (Nguyen et al., 2018) (ours)	57.4	48.7	40.3	29.5	19.8	11.4	5.8	1.7	0.2	AutoLoc (Shou et al., 2018)	-	-	35.8	29.0	21.2	13.4	5.8	-	-	MAAN (ours)	59.8	50.8	41.1	30.6	20.3	12.0	6.9	2.6	0.2Table 3: Comparison of our algorithm to the state-of-the-art approaches on ActivityNet1.3 validation
Table 3: Comparison of our algorithm to the state-of-the-art approaches on ActivityNet1.3 validationset. AP (%) is reported for different IoU threshold α. (“ours” means our implementation.)Supervision	Methods	0.5	AP @ IoU 0.75	0.95	Singh & Cuzzolin (Singh & Cuzzolin, 2016)	34.5	-	-]h1111 V-QllΠPΓvi QpH	Wang & Tao (Wang & Tao, 2016)	45.1	4.1	0.0uy-supervse	Shou et al. (Shou et al., 2017)	45.3	26.0	0.2	Xiong et al. (Xiong et al., 2017)	39.1	23.5	5.5	STPN (Nguyen et al., 2018)	29.3	16.9	2.6Weakly-supervised	STPN (Nguyen et al., 2018) (ours)	29.8	17.7	4.1	MAAN (ours)	33.7	21.9	5.5et al., 2018). With the same threshold and experimental setting, our proposed MAAN modeloutperforms the STPN approach on the large-scale ActivityNet1.3. Similar to THUMOS14, ourmodel also achieves good results that are close to some of the fully-supervised approaches.
Table 4: Localization error on CUB-200-2011 test SetMethods	top1 err@IoU0.5	top5 err@IoU0.5GoogLeNet-GAP ((ZhoU et al., 2016b))	5900	-Weighted-CAM 4x4	5851	51.73weighted-CAM 7x7	58.11	50.21MAAN 4x4	55.90	47.60	MAAN 7x7		53.94	44.13Based on the model in (Zhou et al., 2016a) (denoted as CAM model), we replace the global averagepooling feature aggregator with other kinds of feature aggregator, such as the weighted sum poolingand the proposed MAA by extending the original 1D temporal version in temporal action localizationinto a 2D spatial version. We denote the model with weighted sum pooling as the weighted-CAMmodel. For the weighted-CAM model and the proposed MAAN model, we use an attention moduleto generate the attention weight λ in STPN or the latent discriminative probability p in MAAN. Theattention module consists of a 2D convolutional layer of kernel size 1 × 1, stride 1 with 256 units, aLeakyReLU layer, a 2D convolutional layer of kernel size 1 × 1, stride 1 with 1 unit, and a sigmoidnon-linear activation.
