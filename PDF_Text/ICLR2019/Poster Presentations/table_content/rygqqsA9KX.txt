Table 1: Results for multimodal speaker traits recognition on POM, multimodal sentiment analysis on CMU-MOSI, ICT-MMMO, YouTube, MOUD, and multimodal emotion recognition on IEMOCAP. SOTA1 and SOTA2refer to the previous best and second best state-of-the-art respectively, and ∆SOT A shows improvement overSOTA1. Symbols depict the baseline giving the result: # MFN, φ MARN, * TFN,* BC-LSTM, ◊ MV-LSTM, §EF-LSTM, b DF, Q SVM, • RF. For detailed tables with results for all models, please refer to the appendix.
Table 2: The effect of missing modalities on multimodal data reconstruction and sentiment prediction onCMU-MOSI. MFM with surrogate inference is able to better handle missing modalities during test time ascompared to the purely generative (Seq2Seq) or purely discriminative baselines.
Table 3: Results for personality trait recognition on the POM dataset. The best results are highlightedin bold and ∆SOT A shows the change in performance over previous state of the art. Improvementsare highlighted in green. MFM achieves state-of-the-art or competitive performance on all datasetsand metrics.
Table 4: Sentiment prediction results on CMU-MOSI, ICT-MMMO, YouTube and MOUD. The bestresults are highlighted in bold and ∆SOT A shows the change in performance over previous state ofthe art (SOTA). Improvements are highlighted in green. MFM achieves state-of-the-art or competitiveperformance on all datasets and metrics.
Table 5: Emotion recognition results on IEMOCAP test set. The best results are highlighted in boldand ∆SOT A shows the change in performance over previous SOTA. Improvements are highlighted ingreen. MFM achieves state-of-the-art or competitive performance on all datasets and metrics.
Table 6: Information-Based interpretation results showing ratios r = AJLy, JV, i ∈MI(Fa i ,Xi ){(')anguage, (v)isual, (a)coustic} for the POM dataset for personality traits prediction.
Table 7: Comparison with Hsu & Glass (2018) for sentiment analysis on CMU-MOSI, ICT-MMMO,YouTube, and MOUD. MFM outperforms the baselines across these datasets and metrics.
Table 8: Comparison with β-VAE for multimodal sentiment analysis on CMU-MOSI, ICT-MMMO,YouTube, and MOUD. MFM outperforms β-VAE across these datasets and metrics.
