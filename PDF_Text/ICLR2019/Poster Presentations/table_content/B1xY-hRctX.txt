Table 1: Comparison among MemNN, ∂ILP and the proposed NLM in family tree and graph reasoning, wherem is the size of the testing family trees or graphs. Both ∂ILP and NLM outperform the neural baseline andachieve perfect accuracy (100%) on test set. Note N/A mark means that ∂ILP cannot scale up in 2-OutDegree.
Table 2: Comparison between MemNN and the proposed NLM in the blocks world, sorting integers, and findingshortest paths, where m is the number of blocks in the blocks world environment or the size of the arrays/graphsin sorting/path environment. Both models are trained on instance size m ≤ 12 and tested on m = 10 or 50. Theperformance is evaluated by two metrics and separated by “/”: the probability of completing the task during thetest, and the average Moves used by the agents when they complete the task. There is no result for ∂ILP since itfails to scale up. MemNN fails to complete the blocks world within the maximum m × 4 Moves.
Table 3: Hyper-parameters for reinforcement learning tasks. The meaning of the hyper-parameterscould be found in Section A.1 and Section A.2. For the Path environment, the step limit is set to theactual distance between the starting point and the targeting point, to encourage the agents to find theshortest path.
Table 4: Hyper-parameters for Neural Logic Machines. The definition of depth and breadth areillustrated in Figure 2. “Res.” refers to the use of residual links. “Grad.” refers to the ratio ofsuccessful graduation in 10 runs with different random seeds, which partially indicates the difficultyof the task. “Num. Examples/Episodes” means the maximum number of examples/episodes used totrain the model in supervised learning and reinforcement learning cases.
