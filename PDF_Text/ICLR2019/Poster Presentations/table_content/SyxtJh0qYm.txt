Table 1: NRMSE (for continuous datasets) or PFC (for categorical ones) of imputations. Less is better.
Table 2: R2-score (for continuous targets) or accuracy (for categorical ones) of post-imputation regressionor classification. Higher is better.
Table 3: PSNR of inpaintings for different masks for Context Encoder (Pathak et al., 2016), model from“Semantic Image Inpainting with Deep Generative Models” (Yeh et al., 2017) and VAEAC. Higher is better.
Table 4: PSNR of inpaintings for different masks for Context Encoder (Pathak et al., 2016), model from“Generative Face Completion” (Li et al., 2017) and VAEAC. Higher is better.
Table 5: VAEAC and UM comparison on MNIST.
Table 6: Generative Face Completion (Li et al., 2017) masks. Image size is 128x128.
Table 7: NRMSE (for continuous datasets) or PFC (for categorical ones) of imputations for different GAINmodifications. Less is better. “Our modification” includes the reconstruction loss L0M (12), Bernoulli dis-tribution over b in the hint generation procedure, and fixed α = 10. Other columns refers original GAINwithout these modifications and with different values of α.
Table 8: Negative log-likelihood estimation of a hybrid model on the synthetic data. IS-S refers to Impor-tance Sampling log-likelihood estimation with S samples for each object (18). MC-S refers to Monte-Carlolog-likelihood estimation with S samples for each object (17).
Table 9: Average negative log-likelihood of inpaintings for 1000 objects. IS-S refers to Importance Samplinglog-likelihood estimation with S samples for each object (18). MC-S refers to Monte-Carlo log-likelihoodestimation with S samples for each object (17). Naive Bayes is a baseline method which assumes pixels andcolors independence.
Table 10: NRMSE (for continuous datasets) or PFC (for categorical ones) of imputations. Less is better.
Table 11: R2-score (for continuous targets) or accuracy (for categorical ones) of post-imputation regressionor classification. Higher is better.
