Table 1: Environment DetailsEnvironment	Observation	Action	RewardHopper	11	3	stvel - 0.001||at||1 22 + 1Walker2d	17	6	stvel - 0.001||at||22 + 1HalfCheetah	17	6	stvel -0.1||at||22 +1Quadruped	12	8	svel∆t - 0.008∆t∣at ∙ qt|& Liu, 2017), which implements the continuous control benchmarks in OpenAI Gym using PyDart(Ha, 2016). For all of our examples, we represent the policy as a feed-forward neural network withthree hidden layers, each consists of 64 hidden nodes.
Table 2:	Dynamic Randomization details for HopperDynamic Parmeter RangeFriction CoefficientRestitution CoefficientMassJoint DampingJoint Torque Scale[0.2, 1.0][0.0, 0.3][2.0, 15.0]kg[0.5, 3][50%, 150%]Table 3:	Dynamic Randomization details for Walker2dDynamic Parmeter RangeFriction Coefficient	[0.2, 1.0]Restitution Coefficient	[0.0, 0.8]Joint Damping	[0.1, 3.0]B.3 Simulated Reality GapsTo evaluate the ability of our method to overcome the modeling error, we designed six types ofmodeling errors. Each example shown in our experiments contains one or more modeling errors
Table 3:	Dynamic Randomization details for Walker2dDynamic Parmeter RangeFriction Coefficient	[0.2, 1.0]Restitution Coefficient	[0.0, 0.8]Joint Damping	[0.1, 3.0]B.3 Simulated Reality GapsTo evaluate the ability of our method to overcome the modeling error, we designed six types ofmodeling errors. Each example shown in our experiments contains one or more modeling errorslisted below.
Table 4:	Dynamic Randomization details for HalfCheetahDynamic Parmeter RangeFriction Coefficient	[0.2, 1.0]Restitution Coefficient	[0.0, 0.5]Mass	[1.0, 15.0]kgJoint Torque Scale [30%, 150%]is trained without any delay, it is usually very challenging to transfer it to problems withdelay added. The value of delay is usually below 50ms and we use 8ms and 50ms in ourexamples.
