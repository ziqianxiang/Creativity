Table 1: Hyperparameters for train-time	Iters	Epochs	# Tasks/itr	# TS/itr	K	outer LR	inner LR (η)Meta-learned approaches (3)	12	50	16	2000-3000	16	0.001	0.01Non-meta-learned approaches (2)	12	50	16	2000-3000	16	0.001	N/ATable 2: Hyperparameters for run-timeMOLe (ours)continued adaptation with meta-learningk-shot adaptation with meta-learningmodel-based RLmodel-based RL with online gradient updatesα (CRP)	LR (model update)	K (previous data)1	0.01	16N/A	0.01	16N/A	0.01	16N/A	N/A	N/AN/A	0.01	1614Published as a conference paper at ICLR 2019C ControllerAs mentioned in Section 6, we use the learned dynamics model in conjunction with a controller to
Table 2: Hyperparameters for run-timeMOLe (ours)continued adaptation with meta-learningk-shot adaptation with meta-learningmodel-based RLmodel-based RL with online gradient updatesα (CRP)	LR (model update)	K (previous data)1	0.01	16N/A	0.01	16N/A	0.01	16N/A	N/A	N/AN/A	0.01	1614Published as a conference paper at ICLR 2019C ControllerAs mentioned in Section 6, we use the learned dynamics model in conjunction with a controller toselect the next action to execute. The controller uses the learned model fθ together with a rewardfunction r(st , at) that encodes the desired task. Many methods could be used to perform this ac-tion selection, including cross entropy method (CEM) (Botev et al., 2013) or model predictive pathintegral control (MPPI) (Williams et al., 2015), but in our experiments, we use a random-sampling
