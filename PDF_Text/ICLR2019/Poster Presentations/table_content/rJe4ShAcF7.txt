Table 1: Comparing the overall relative memory complexity (intermediate relative embeddings (R orEr) + relative logits Srel), the maximal training lengths that can fit in a GPU with 16GB memoryassuming Dh = 64, and the memory usage per layer per head (in MB).
Table 2: Note-wise validation NLL (nats/token) on J.S. Bach Chorales where each token is a 16thnote. NLL is improved by using the Transformer with our memory-efficient relative global attentionformulation, also when including additional positional and relational information.
Table 3: Validation NLL (nats/token) for Maestro dataset, with event-based representation withlengths L = 2048. Training and/or evaluating on different lengths will result in different lossesbecause the amount of context available to the model would be different. The Transformer with ourmemory-efficient relative attention formulation achieves state-of-the-art results.
Table 4: Relative attention improves conditioned negative logliklihood (NLL) given groundtruthmelodies from the validation split of the Maestro dataset.
Table 5: A post-hoc comparison of each pair on their pairwise comparisons with each other, using theWilcoxon signed-rank test for matched samples. p value less than 0.01/6=0.0016 yields a statisticallysignificant difference and is marked by asterisk.
Table 6: Comparing each pair on their aggregates (comparisons with all models) in (wins, ties, losses),using the Mann-Whitney U test for independent samples.
