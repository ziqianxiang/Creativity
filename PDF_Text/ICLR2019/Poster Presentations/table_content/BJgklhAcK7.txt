Table 1: Test accuracies on miniImageNet and tieredImageNet. For each dataset, the first set ofresults use convolutional networks, while the second use much deeper residual networks, predomi-nantly in conjuction with pre-training.
Table 2: Ablation study and comparison to Meta-SGD. Unless otherwise specified, LEO stands forusing the stochastic generator for latent embedding optimization followed by fine-tuning.
Table 3: Architecture details for 5-way 1-shot miniImageNet and tieredImageNet. The shapes cor-respond to the meta-training phase. We used a meta-batch of 12 task instances in parallel.
Table 4: Learning rate annealing schedules used to train feature extractors for miniImageNet andtieredImageNet.
Table 5: Architecture details for 5-way 1-shot miniImageNet and tieredImageNet. The shapes cor-respond to the meta-training phase. We used a meta-batch of 12 task instances in parallel.
Table 6: Values of hyperparameters chosen to maximize meta-validation accuracy during randomsearch.
