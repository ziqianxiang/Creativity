Table 1: Testing character-level BPC values of quantized LSTM models and size of their weightmatrices in terms of KByte.
Table 2: Test character-level performance of our quantized models on the Text8 corpus.
Table 3: Test performance of the proposed LSTM with recurrent binary/ternary weights on the PennTreebank (PTB) corpus.______________________________________________________________________Word-PTB				Model	Precision	Test (Perplexity)	Size (KByte)	Operations (MOps)Small LSTM (baseline)	Full-precision	91.5	2880	1.4Small LSTM with binary weights (ours)	Binary	92.2	90	1.4Small LSTM with ternary weights (ours)	Ternary	90.7	180	1.4Small BinaryConnect LSTM (Courbariaux et al. (2015))	Binary	125.9	90	1.4Small Alternating LSTM (Xu et al. (2018))	2 bits	103.1	180	2.9Small Alternating LSTM (Xu et al. (2018))	3 bits	93.8	270	4.3Small Alternating LSTM (Xu et al. (2018))	4 bits	91.4	360	5.8Medium LSTM (baseline)	Full-precision	87.6	27040	13.5Medium LSTM with binary weights (ours)	Binary	87.2	422	13.5Medium LSTM with ternary weights (ours)	Ternary	86.1	845	13.5Medium BinaryConnect LSTM (Courbariaux et al. (2015))	Binary	108.4	422	13.5Large LSTM (baseline)	Full-precision	78.5	144000	72Large LSTM with binary weights (ours)	Binary	76.5	4500	72Large LSTM with ternary weights (ours)	Ternary	76.3	9000	72Large BinaryConnect LSTM (Courbariaux et al. (2015))	Binary	128.5	4500	726
Table 4: Test accuracy of the proposed LSTM with recurrent binary/ternary weights on the pixel bypixel MNIST classification task.
Table 5: Test accuracy of Attentive Reader with recurrent binary/ternary weights on CNN question-answering task.
Table 6: Testing character-level BPC values of quantized GRU models and size of their weightmatrices in terms of KByte.
Table 7: Implementation results of the proposed binary/ternary models Vs full-precision models.
