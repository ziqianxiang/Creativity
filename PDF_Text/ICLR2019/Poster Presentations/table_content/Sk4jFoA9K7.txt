Table 1: Optimization parameters for different architectures and datasets. Learning rate is decreasedat epochs 100, 175, and 250 with a step factor of 10-1.
Table 2: Comparison of PeerNet and classical CNN on the CIFAR-10 dataset using various gradient-based sample-specific attacks.
Table 3: Performance and fooling rates on the MNIST dataset for different levels œÅ of universaladversarial noise.
Table 4: Performance and fooling rates on the CIFAR-10 dataset. ResNet-32 v2 and PR-ResNet-32v2 have double the amount of feature maps after the last two convolutional blocks, meaning insteadof (16, 16, 32, 64), it has (16, 16, 64, 128).
Table 5: Performance and fooling rates on the CIFAR-100 dataset. PR-ResNet-110 v2 has double theamount of feature maps after the last two convolutional blocks, meaning instead of (16, 16, 32, 64), ithas (16, 16, 64, 128).
Table 6: Comparison of PeerNets to adversarially trained version of ResNet-32 baseline on CIFAR-10dataset.
Table 7: Evaluation of PeerNets on finite-difference based query black-box attacks using CIFAR-10dataset.
