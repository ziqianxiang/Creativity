Table 1: Comparison of the BALANCE algorithm and the learned algorithmDistribution	No. of advertisers	Budgets (common)	BALANCE	Learned	10	10	66.2 ± 0.1	61.6 ± 0.3Adversarial	10	20	128.4 ± 0.06	122.6 ± 1.3unpermuted	10	50	326.9 ± 0.06	300.1 ± 2.0	10	100	657.8 ± 0.06	608.2 ± 3.4	10	10	90.8 ± 0.1	77.8 ± 0.4Adversarial	10	20	181.1 ± 0.2	155.4 ± 1.8permuted	10	50	472.3 ± 0.3	427.7 ± 2.5	10	100	962.15 ± 0.4	770.8 ± 4.6	10	10	70.1 ± 0.1	67.5 ± 0.7Thick-z	10	20	135.2 ± 0.05	134.7 ± 1.5unpermuted	10	50	345.5 ± 0.06	331.5 ± 1.20	10	100	695.9 ± 0.1	663.6 ± 7.5	10	10	91.7 ± 0.2	82.8 ± 0.6Thickz	10	20	182.8 ± 0.3	166.5 ± 1.2permuted	10	50	475.6 ± 0.3	410.1 ± 2.8	10	100	867.5 ± 0.4	832.1 ± 7.614Published as a conference paper at ICLR 2019
Table 2: Comparison of the BALANCE algorithm and the learned algorithm with discretized statespace.
Table 3: This table compares the performance of the learned algorithm compared the BALANCEin the discretized state space. Here, the agent is trained on the adversarial graph with the ad slotsarriving in a permuted order. The agent was only trained on the input instance with 20 advertisersand a common budget of 20 but tested on instances with up to 106 ad slots.
Table 4: Comparison of the MSVV algorithm and the learned algorithm with discretized state space.
Table 5: Comparison of the Bang-per-Buck knapsack algorithm and the learned algorithm.
Table 6: Comparison of optimal algorithm and learned algorithm (scores are mean ± standard devi-ation)n	Optimal algorithm	Learned algorithm100	0.3733 ± 0.005	0.371 ± 0.0152000	0.372 ± 0.014	0.372 ± 0.01419Published as a conference paper at ICLR 2019P-OqSaJqlTimeFigure 10: The agent’s algorithm for the secretary problem with i.i.d. values for various values ofthe input length n.
Table 7: Comparison of optimal algorithm and learned algorithm (scores are mean ± standard devi-ation)n	Optimal algorithm	Learned algorithm10	0.393 ± 0.018	0.396 ± 0.01420	0.376 ± 0.017	0.386 ± 0.01150	0.371 ± 0.014	0.371 ± 0.015100	0.370 ± 0.022	0.358 ± 0.015Table 8: Comparison of optimal algorithm and learned algorithm. The scores obtained by the algo-rithm are taken by averaging 10 runs with each run traversing through 1000 iterations.
Table 8: Comparison of optimal algorithm and learned algorithm. The scores obtained by the algo-rithm are taken by averaging 10 runs with each run traversing through 1000 iterations.
Table 9: Comparison of optimal algorithm and learned algorithmn	Optimal algorithm	Learned algorithm10	0.393 ± 0.018	0.404 ± 0.01020	0.376 ± 0.017	0.366 ± 0.01650	0.371 ± 0.014	0.331 ± 0.021100	0.370 ± 0.022	0.267 ± 0.02121Published as a conference paper at ICLR 2019F Background on Algorithms and Input Length IndependenceIn this section, we visit a fundamental question that motivates our work at a very high level: what isan algorithm? We present a detailed and somewhat informal description of the various nuances thatmake the question of learning an algorithm challenging and subtle.
