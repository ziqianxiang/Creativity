Table 1: BLEU scores on VI-EN and EN-VI. Table 2: BLEU scores on DE-EN and EN-DE.
Table 3: Comparison of German-to-English translation examples. Matched key phrases are shown in thesame color. First example: “May” is not the date when the new prime minister visited Japan, but actuallyis the time he won the election. Second example: GNMT’s paraphrase choices are not as accurate as ours.
Table 4: ROUGE scores on Gigaword.	Table 5: ROUGE scores on DUC2004.
Table 6: ResUlts for image captioning on the COCO dataset.						Method	BLEU-1	BLEU-2	BLEU-3	BLEU-4	METEOR	CIDErSoft Attention (XU et al., 2015)	70.7	-492-	34.4	24.3	23.9	-Hard Attention (XU et al., 2015)	71.8	50.4	35.7	25.0	23.0	-Show & Tell (Vinyals et al., 2015)	-	-	-	27.7	23.7	85.5ATT-FCN (YoU et al., 2016)	70.9	53.7	40.2	30.4	24.3	-SCN-LSTM (Gan et al., 2017)	72.8	56.6	43.3	33.0	25.7	101.2Adaptive Attention (LU et al., 2017)	74.2	58.0	43.9	33.2	26.6	108.5Top-Down Attention (Anderson et al., 2018)	77.2	-	-	36.2	27.0	113.5No attention, Resnet-152						Show & Tell	70.3	53.7	39.9	29.5	23.6	87.1Show & Tell+Lseq (Ours)	70.9	54.2	40.4	30.1	23.9	90.0No attention, Tag						Show & Tell	72.1	55.2	41.3	30.1	24.5	93.4Show & Tell+Lseq (Ours)	72.3	55.4	41.5	31.0	24.6	94.7Soft attention, FastRCNN						Show, Attend & Tell	74.0	58.0	44.0	33.1	25.2	99.1Show, Attend & Tell+Lseq (Ours)	74.5	58.4	44.5	33.8	25.6	102.9split (Karpathy & Fei-Fei, 2015), 113,287 images are used for training and 5,000 images are usedfor validation and testing. We follow the implementation of the Show, Attend (Xu et al., 2015)4,
Table 7: BLEU scores on VI-EN and EN-VI. Table 8: BLEU scores on DE-EN and EN-DE.
Table 9: Examples on Text Summarization.
Table 11: BLEU scores on VI-EN and EN-VI us-ing different choices of model’s belief.
Table 12: Comparison experiment.
