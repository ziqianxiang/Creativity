Table A.l: Parameters and hyperparameters for the experiment on best practices and neural persistence. Dropout and batch normalization were applied after thefirst hidden layer. Throughout the networks, ReLU was the activation function of choice.
