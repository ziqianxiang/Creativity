Table 1: Datasets used in experiments (Section 5)Name	CIFAR-100	Flowers-102	Cars	Aircraft	Places-365	ImageNet#images	60,000	8,189	16,185	10,200	1.8 million	1.3 million#classes	100	102	196	102	365	1000We use TensorFlow (Abadi et al., 2015), and NVIDIA P100 and V100 GPUs for our experiments.
Table 2: Transfer-learning on Inception V3, against full-network fine-tuning.
Table 3: Learning Imagenet from SSD feature extractor (left) and random filters (right)Fine-tuned params. ∣ #Params ∣ COCO→Imagenet, Topl ∣ Random→ Imagenet,Top1Last layer	1.31M	29.2%	0%S/B + last layer	1.35M	47.8%	20%S/B only	34K	6.4%	2.3%All params	3.5M	71.8%	71.8%Next, we discuss the effect of learning rate. It is common practice to use a small learning rate whenfine-tuning the entire network. The intuition is that, when all parameters are trained, a large learningrate results in network essentially forgetting its initial starting point. Therefore, the choice of learn-ing rate is a crucial factor in the performance of transfer learning. In our experiments (Appendix B.2,Figure 9) we observed the opposite behavior when fine-tuning only small model patches: the accu-racy grows as learning rate increases. In practice, fine-tuning a patch that includes the last layer ismore stable w.r.t. the learning rate than full fine-tuning or fine-tuning only the scale-and-bias patch.
Table 4: Multi-task learning with MobilenetV2 on ImageNet and Places-365.
Table 5: Multi-task accuracies of 5 MobilenetV2 models acting at different resolutions on ImageNet.
Table 6: The effect of batch-norm statistics on logit-based fine-tuning for MobileNetV2Method	Flowers	Aircraft	Stanford Cars	Cifar100Last layer (logits)	80.2	43.3	51.4	45.0Same as above + batchnorm statistics	79.8	38.3	43.6	57.2Same as above + scales and biases	86.9	65.6	75.9	74.9B.2 Accuracy vs. learning rateIOO8060402010090807060504030MobiIenetV2∕Cifar-100
