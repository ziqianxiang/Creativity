Table 1: Performance of PPO and Self-Imitation (SI) on tasks with episodic rewards, noisy rewards withmasking probability pm, and dense rewards. All runs use 5M timesteps of interaction with the environment. ESperformance at 5M timesteps is taken from (Salimans et al., 2017). Missing entry denotes that we were unableto obtain the 5M timestep performance from the paper.
Table 2: Extension of Table 1 from Section 3. All runs use 5M timesteps of interaction with the environment.
Table 3: Performance of PPO+SIL (Oh et al., 2018) on tasks with episodic rewards, noisy rewards withmasking probability pm , and dense rewards. All runs use 5M timesteps of interaction with the environment.
Table 4: Performance of TD3 (Fujimoto et al., 2018) on tasks with episodic rewards, noisy rewards withmasking probability pm , and dense rewards. All runs use 5M timesteps of interaction with the environment.
Table 5: Performance of EX2 (Fu et al., 2017) and SI-interact-JS on the hard exploration MuJoCo tasks fromSection 3.2. SparseHalfCheetah, SparseHalfCheetah, SparseAnt use 1M, 1M and 2M timesteps of interactionwith the environment, respectively. Results are averaged over 3 separate runs.
