Table 1: Different robust accuracies on datasets with same inter-class distancesInter-class Distances	Smooth LEVEL OF Smoothed MNIST	Resilience OF Smoothed MNIST	Scale factor of Scaled Original MNIST	Resilience of Scaled Original MNIST	Scale factor of Scaled Binarized MNIST	Resilience of Scaled Binarized MNIST7.12	3	91.3 %	0.970	94.6 %	0.821	98.6 %7.01	4	90.3 %	0.955	95.5 %	0.809	98.6 %6.85	5	89.6 %	0.932	94.9 %	0.790	98.5 %could also increase. We use the “inter-class distance” as an approximation. Inter-class distance 9characterizes the distances between each class to rest of classes in each dataset. Intuitively, if thedistances between classes are larger, then it should be easier to achieve robustness. We also observed(in Appendix D.2.1 Figure 5) that inter-class distances are positively correlated with robust accuracy.
Table 2: Performance and Robustness of models trained on MNIST variants.
Table 3: Performance and Robustness of models trained on CIFAR10 variants.
Table 4: Performance and robustness of different sized LeNet5 models on MNIST variantsStandard Training, Accuracy														Training Set					Test Set							Widen factor	0.125	0.25	0.5	1	2	4		0.125	0.25	0.5	1	2	4BINARIZED	99.9%	100.0%	100.0%	99.6%	100.0%	100.0%		98.7%	99.0%	99.2%	98.5%	99.4%	99.2%ORIGINAL	100.0%	100.0%	100.0%	100.0%	100.0%	100.0%		98.8%	99.2%	99.2%	99.3%	99.4%	99.3%smooth 2	99.9%	100.0%	100.0%	100.0%	100.0%	100.0%		98.8%	99.0%	99.1%	99.3%	99.3%	99.4%smooth 3	99.9%	99.9%	100.0%	100.0%	100.0%	100.0%		98.8%	98.8%	99.2%	99.2%	99.1%	99.3%smooth 4	99.9%	100.0%	100.0%	100.0%	100.0%	100.0%		98.7%	99.0%	99.0%	99.1%	99.4%	99.4%smooth 5	99.8%	100.0%	100.0%	100.0%	100.0%	100.0%		98.5%	99.0%	99.2%	99.0%	99.3%	99.3%smooth 6	99.8%	100.0%	100.0%	100.0%	100.0%	100.0%		98.4%	98.9%	99.0%	99.1%	99.2%	99.3%smooth 7	99.8%	99.9%	100.0%	100.0%	100.0%	100.0%		98.5%	98.8%	99.0%	99.0%	99.3%	99.3%smooth 8	99.7%	100.0%	100.0%	100.0%	100.0%	100.0%		98.4%	98.9%	98.9%	99.0%	99.2%	99.0%					PGD Training, Accuracy											Training Set							Test Set			Widen factor	0.125	0.25	0.5	1	2	4	0.125		0.25	0.5	1	2	4BINARIZED	97.8%	99.6%	100.0%	100.0%	100.0%	100.0%	97.4%		98.3%	98.8%	98.9%	99.0%	99.2%ORIGINAL	97.0%	98.4%	99.8%	100.0%	100.0%	100.0%	97.0%		98.2%	98.9%	99.2%	99.1%	99.2%smooth 2	96.1%	98.1%	99.0%	99.9%	100.0%	100.0%	96.1%		97.8%	98.5%	98.9%	99.0%	99.0%smooth 3	96.3%	97.8%	98.9%	99.7%	99.9%	100.0%	96.5%		97.6%	98.6%	99.0%	99.1%	99.1%
Table 5: Performance and robustness of different sized Wide ResNet models on CIFAR10 variantsStandard Training, AccuracyTraining SetTest SetWiden factor	0.25	14		0.25	1		4saturate 1	85.5%	99.9%	100.0%	82.4%	91.1%	93.8%saturate 1.5	87.0%	99.9%	100.0%	84.2%	92.1%	94.7%saturate 1.75	87.4%	99.9%	100.0%	84.5%	93.0%	95.2%ORIGINAL	87.2%	99.9%	100.0%	84.4%	92.5%	95.0%saturate 2.25	87.3%	99.9%	100.0%	84.5%	92.5%	94.8%saturate 2.5	86.4%	99.9%	100.0%	83.7%	92.3%	94.8%saturate 3	86.2%	99.9%	100.0%	84.0%	92.2%	94.5%saturate 4	85.8%	99.9%	100.0%	83.1%	91.1%	93.8%saturate 8	84.6%	99.8%	100.0%	81.2%	90.1%	93.3%saturate 1 6	83.5%	99.7%	100.0%	81.0%	89.4%	92.9%saturate 64	80.5%	99.4%	100.0%	79.2%	86.9%	89.6%saturate 128	77.1%	98.7%	100.0%	74.6%	83.0%	85.3%saturate 256	73.7%	97.6%	100.0%	70.7%	76.5%	83.0%SATURATE INF	73.2%	97.3%	99.9%	70.6%	76.3%	80.3%		PGD Training, Accuracy				
Table 6: Perturbable volumes of different variants of MNIST and CIFAR10. Values shown in tableare the average log value (in bits) of volumes of test data. For MNIST, = 0.3, for CIFAR10= 8/255.
Table 7: PGD attack results with and without domain boundary constraints on MNIST and CIFAR10MNIST			CIFAR10		MNIST VARIANTS	Robust Accuracy w/ bound	Robust Accuracy w/o bound	CIFAR10 VARIANTS	Robust Accuracy w/ bound	Robust Accuracy w/o boundB INARIZED	98.1 %	96.1 %	SATURATE 1	33.0 %	32.7 %ORIGINAL	95.1 %	95.1 %	ORIGINAL	43.2 %	43.0 %smooth 2	93.0 %	92.9 %	SATURATE 4	64.0 %	64.0 %smooth 3	91.3 %	91.5 %	SATURATE 8	78.1 %	78.1 %smooth 4	90.3 %	90.6 %	SATURATE 16	79.4 %	79.4 %smooth 5	89.6 %	89.9 %	SATURATE INF	79.7 %	79.4 %is not able to use the significantly larger additional volumes even for binarized MNIST or highlysaturated CIFAR10, whose data points are on or very close to the corner. In some cases, allowingperturbation outside of domain boundary makes the attack slightly less effective. This might be dueto that data domain boundary constrained the perturbation to be in an “easier” region. This mightseem surprising considering the huge difference in perturbable volumes, these results conform withempirical results in previous research (Goodfellow et al., 2014; Warde-Farley and Goodfellow, 2016)that adversarial examples appears in certain directions instead of being distributed in small pocketsacross space. Therefore, the perturbable volume hypothesis is rejected.
