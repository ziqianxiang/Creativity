Table 1: Summary of experimental results(dx, dh, m)	# Runs	Sum M (Avg.)	Sum L (Avg.)	Sum K (Avg.)	P{L > 0}(10, 1, 1000)	40	290 (7.25)	0(0)	0(0)	0(10, 1, 10000)	40	371 (9.275)	1 (0.025)	0(0)	0.025(100, 1, 1000)	40	1,452(36.3)	0(0)	0(0)	0(100, 1, 10000)	40	2,976 (74.4)	2 (0.05)	0(0)	0.05(100, 10, 10000)	40	24,805 (620.125)	4(0.1)	0(0)	0.1(1000, 1, 10000)	40	14,194 (354.85)	0(0)	0(0)	0(1000, 10, 10000)	40	42,334(1,058.35)	37 (0.925)	1 (0.025)	0.6254	ExperimentsFor experiments, we used artificial datasets sampled iid from standard normal distribution, andtrained 1-hidden-layer ReLU networks with squared error loss. In practice, it is impossible to getto the exact nondifferentiable point, because they lie in a set of measure zero. To get close to thosepoints, we ran Adam (Kingma & Ba, 2014) using full-batch (exact) gradient for 200,000 iterationsand decaying step size (start with 10-3, 0.2× decay every 20,000 iterations). We observed thatdecaying step size had the effect of “descending deeper into the valley.”After running Adam, for each k ∈ [dh], we counted the number of approximate boundary datapoints satisfying |[W1xi + b1]k | < 10-5, which gives an estimate of Mk. Moreover, for thesepoints, we solved the QP (2) using L-BFGS-B (Byrd et al., 1995), to check if the terminated pointsare indeed (approximate) FOSPs. We could see that the optimal values of (2) are close to zero
