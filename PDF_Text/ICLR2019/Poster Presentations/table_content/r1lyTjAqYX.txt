Table 1: Comparison of Atari-57 and DMLab-30 results. R2D2 average final score over 3 seeds (1seed for feed-forward variant), IMPALA final score over 1 seed, Ape-X best training score with 1seed. Our re-run of IMPALA uses the same improved action set from (Hessel et al., 2018b) as R2D2,and is trained for a comparable number of environment frames (10B frames; the original IMPALAexperts in (Espeholt et al., 2018) were only trained for approximately 333M frames). R2D2+ refersto the adapted R2D2 variant matching deep IMPALA’s 15-layer ResNet architecture and asymmetricreward clipping, as well as using a shorter target update period of 400.
Table 2: Hyper-parameters values used in R2D2. All missing parameters follow the ones in Ape-X(Horgan et al., 2018).
Table 3: Performance of R2D2 and R2D2+, averaged over 3 seeds, compared to our own single-seed re-run of IMPALA (shallow/deep) with improved action-set and trained on the same amountof data (10B environment frames). Compared to standard R2D2, the R2D2+ variant uses a shortertarget network update frequency (400 compared to 2500), as well as the substantially larger 15-layerResNet and the custom ‘optimistic asymmetric reward clipping’ from (Espeholt et al., 2018).
Table 4: Performance of R2D2 (averaged over 3 seeds) compared to Reactor, IMPALA (shallow and deepexpert variants), and Ape-X, with 30 random no-op starts. Ape-X uses best score attained during training,whereas all other agents use final scores.
