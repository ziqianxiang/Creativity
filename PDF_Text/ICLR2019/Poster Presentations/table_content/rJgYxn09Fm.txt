Table 1: Test error (%) on CIFAR-10 and CIFAR-100. SWRN 28-10, the result of training aWRN 28-10 with our method and one template per layer, sig-nificantly outperforms the base model, suggesting thatour method aids optimization (both models have thesame capacity). SWRN 28-10-1, with a single tem-plate per sharing group, performs close to WRN 28-10while having significantly less parameters and capac-ity. * indicates models trained with dropout p = 0.3(Srivastava et al., 2014). Results are average of 5 runs.
Table 2: Performance of wider SWRNs. Parameter re-duction (k = 2) leads to lower errors for CIFAR-10,with models being competitive against newer modelfamilies that have bottleneck layers, group convolu-tions, or many layers. Best SWRN results are in bold,and best overall results are underlined.
Table 3: (below) ImageNet classification re-sults: training WRN 50-2 with soft parametersharing leads to better performance by itself,without any tuning on the number of templatesk. Top-1 and Top-5 errors (%) are computedusing a single crop.
Table 4: (right) Test error (%) on CIFAR-10of SWRNs and models found via neural archi-tecture search (NAS) (all trained with cutout).
