Published as a conference paper at ICLR 2019
Approximability of discriminators implies
diversity in GANs
Yu Bai	Tengyu Ma	Andrej Risteski
Stanford University	Stanford University	MIT
yub@stanford.edu	tengyuma@stanford.edu	risteski@mit.edu
Ab stract
While Generative Adversarial Networks (GANs) have empirically produced im-
pressive results on learning complex real-world distributions, recent works have
shown that they suffer from lack of diversity or mode collapse. The theoretical
work of Arora et al. (2017a) suggests a dilemma about GANs’ statistical proper-
ties: powerful discriminators cause overfitting, whereas weak discriminators can-
not detect mode collapse.
By contrast, we show in this paper that GANs can in principle learn distributions
in Wasserstein distance (or KL-divergence in many cases) with polynomial sample
complexity, if the discriminator class has strong distinguishing power against the
particular generator class (instead of against all possible generators). For various
generator classes such as mixture of Gaussians, exponential families, and invert-
ible and injective neural networks generators, we design corresponding discrimi-
nators (which are often neural nets of specific architectures) such that the Integral
Probability Metric (IPM) induced by the discriminators can provably approximate
the Wasserstein distance and/or KL-divergence. This implies that if the training is
successful, then the learned distribution is close to the true distribution in Wasser-
stein distance or KL divergence, and thus cannot drop modes. Our preliminary
experiments show that on synthetic datasets the test IPM is well correlated with
KL divergence or the Wasserstein distance, indicating that the lack of diversity in
GANs may be caused by the sub-optimality in optimization instead of statistical
inefficiency.
1	Introduction
In the past few years, we have witnessed great empirical success of Generative Adversarial Networks
(GANs) (Goodfellow et al., 2014) in generating high-quality samples in many domains. Various
ideas have been proposed to further improve the quality of the learned distributions and the stability
of the training. (See e.g., (Arjovsky et al., 2017; Odena et al., 2016; Huang et al., 2017; Radford
et al., 2016; Tolstikhin et al., 2017; Salimans et al., 2016; Jiwoong Im et al., 2016; Durugkar et al.,
2016; Xu et al., 2017) and the reference therein.)
However, understanding of GANs is still in its infancy. Do GANs actually learn the target dis-
tribution? Recent work (Arora et al., 2017a;b; Dumoulin et al., 2016) has both theoretically and
empirically brought the concern to light that distributions learned by GANs suffer from mode col-
lapse or lack of diversity — the learned distribution tends to miss a significant amount of modes of
the target distribution (elaborated in Section 1.1). The main message of this paper is that the mode
collapse can be in principle alleviated by designing proper discriminators with strong distinguishing
power against specific families of generators such as special subclasses of neural network generators
(see Section 1.2 and 1.3 for a detailed introduction.)
1.1	Background on mode collapse in GANs
We mostly focus on the Wasserstein GAN (WGAN) formulation (Arjovsky et al., 2017) in this paper.
Define the F-Integral Probability Metric (F-IPM)(Muller,1997) between distributions p, q as
WF(p,q):= sup IEX〜p[f (X)] - EX〜q[f (X)]∣ .	(1)
f∈F
1
Published as a conference paper at ICLR 2019
Given samples from distribution p, WGAN sets up a family of generators G, a family of discrimina-
tors F , and aims to learn the data distribution p by solving
min WF(pn,qm)	(2)
q∈G
where Pn denotes “the empirical version of the distribution p”, meaning the uniform distribution
over a set of n i.i.d samples from P (and similarly qm.)
When F = {all 1-Lipschitz functions}, IPM reduces to the Wasserstein-1 distance W1. In practice,
parametric families of functions F such as multi-layer neural networks are used for approximating
Lipschitz functions, so that we can empirically optimize this objective eq. (2) via gradient-based
algorithms as long as distributions in the family G have parameterized samplers. (See Section 2 for
more details.)
One of the main theoretical and empirical concerns with GANs is the issue of “mode-
collapse”(Arora et al., 2017a; Salimans et al., 2016) — the learned distribution q tends to generate
high-quality but low-diversity examples. Mathematically, the problem apparently arises from the
fact that IPM is weaker than W1 , and the mode-dropped distribution can fool the former (Arora
et al., 2017a): for a typical distribution P, there exists a distribution q such that simultaneously the
followings happen:
WF (P, q) . εand W1(P, q) & 1.	(3)
where ., & hide constant factors. In fact, setting q = PN with N = R(F)∕ε2, where R(F) is
a complexity measure of F (such as Rademacher complexity), q satisfies eq. (3) but is clearly a
mode-dropped version ofP when P has an exponential number of modes.
Reasoning that the problem is with the strength of the discriminator, a natural solution is to increase
it to larger families such as all 1-Lipschitz functions. However, Arora et al. (Arora et al., 2017a)
points out that Wasserstein-1 distance doesn’t have good generalization properties: the empirical
Wasserstein distance used in the optimization is very far from the population distance. Even for
a spherical Gaussian distribution P = N(0, dId×d) (or many other typical distributions), when the
distribution q is exactly equal to p, letting qm and Pn be two empirical versions of q and P with
m, n = poly(d), we have with high probability,
WI(Pn,qm) & 1 even though Wi(p, q) = 0.	(4)
Therefore even when learning succeeds (P = q), it cannot be gleaned from the empirical version of
W1.
The observations above pose a dilemma in establishing the theories of GANs: powerful discrimina-
tors cause overfitting, whereas weak discriminators result in diversity issues because IPM doesn’t ap-
proximate the Wasserstein distance The lack of diversity has also been observed empirically by (Sri-
vastava et al., 2017; Di & Yu, 2017; Borji, 2018; Arora et al., 2017b).
1.2	An approach to diversity: discriminator families with restricted
APPROXIMABILITY
This paper proposes a resolution to the conundrum by designing a discriminator class F that is
particularly strong against a specific generator class G . We say that a discriminator class F (and its
IPM WF) has restricted approximability w.r.t. a generator class G and the data distribution P, if F
can distinguish P and any q ∈ G approximately as well as all 1-Lipschitz functions can do:
WF has restricted approximability w.r.t. G and P
, ∀q ∈ G, γL(W1(P, q)) . WF (P, q) . γU (W1(P, q)),	(5)
where yl(∙) and YU(∙) are two monotone nonnegative functions with yl(0) = YU(0) = 0. The
paper mostly focuses on γL(t) = tα with 1 ≤ α ≤ 2 and γU (t) = t, although we use the term
“restricted approximability” more generally for this type of result (without tying it to a concrete
definition ofY). In other words, we are looking for discriminators F so that F -IPM can approximate
the Wasserstein distance W1 for the data distribution P and any q ∈ G.
Throughout the rest of this paper, we will focus on the realizable case, that is, we assume P ∈ G,
in which case we say WF has restricted approximability with respect to G if eq. (5) holds for all
2
Published as a conference paper at ICLR 2019
p, q ∈ G . We note, however, that such a framework allows the non-realizible case p ∈/ G in full
generality (for example, results can be established through designing F that satisfies the requirement
in Lemma 4.3).
A discriminator class F with restricted approximability resolves the dilemma in the following way.
First, F avoids mode collapse - if the IPM between P and q is small, then by the left hand side
of eq. (5), p and q are also close in Wasserstein distance and therefore significant mode-dropping
cannot happen. 1
Second, we can pass from population-level guarantees to empirical-level guarantees - as shown
in Arora et al. (2017a), classical capacity bounds such as the Rademacher complexity of F relate
Wf(p, q) to Wf(Prn, ^m). Therefore, as long as the capacity is bounded, We can expand on eq. (5)
to get a full picture of the statistical properties of Wasserstein GANs:
∀q ∈ G, YL(Wι(p, q)) . WF(p,q) ≈ WF(pr,qm) . YU(Wι(p,q)).
Here the first inequality addresses the diversity property of the distance WF, and the second ap-
proximation addresses the generalization of the distance, and the third inequality provides the re-
verse guarantee that if the training fails to find a solution with small IPM, then indeed P and q are far
away in Wasserstein distance.2 To the best of our knowledge, this is the first theoretical framework
that tackles the statistical theory of GANs with polynomial samples.
The main body of the paper will develop techniques for designing discriminator class F with re-
stricted approximability for several examples of generator classes including simple classes like mix-
tures of Gaussians, exponential families, and more complicated classes like distributions generated
by invertible neural networks. In the next subsection, we will show that properly chosen F provides
diversity guarantees such as inequalities eq. (5).
1.3	Design of discriminators with restricted approximability
We start with relatively simple families of distributions G such as Gaussian distributions and expo-
nential families, where we can directly design F to distinguish pairs of distribution in G. As we
show in Section 3, for Gaussians it suffices to use one-layer neural networks with ReLU activations
as discriminators, and for exponential families to use linear combinations of the sufficient statistics.
In Section 4, we study the family of distributions generated by invertible neural networks. We
show that a special type of neural network discriminators with one additional layer than the gener-
ator has restricted approximability3. We show this discriminator class guarantees that W1(P, q)2 .
WF (P, q) . W1(P, q) where here we hide polynomial dependencies on relevant parameters (Theo-
rem 4.2). We remark that such networks can also produce an exponentially large number of modes
due to the non-linearities, and our results imply that if WF (P, q) is small, then most of these expo-
nential modes will show up in the learned distribution q.
One limitation of the invertibility assumption is that it only produces distributions supported on
the entire space. The distribution of natural images is often believed to reside approximately on a
low-dimensional manifold. When the distribution P have a Lebesgue measure-zero support, the KL-
divergence (or the reverse KL-divergence) is infinity unless the support of the estimated distribution
coincides with the support of P.4 Therefore, while our proof makes crucial use of the KL-divergence
in the invertible case, the KL-divergence is fundamentally not the proper measurement of the statis-
tical distance for the cases where both P and q have low-dimensional supports.
The crux of the technical part of the paper is to establish the approximation of Waserstein distance by
IPMs for generators with low-dimensional supports. We will show that a variant of an IPM can still
be sandwiched by Wasserstein distance as in form of eq. (5) without relating to KL-divergence (The-
orem 4.5). This demonstrates the advantage of GANs over MLE approach on learning distributions
1Informally, if most of the modes of p are ε-far away from each other, then as long as W1 (p, q)	ε, q has
to contain most of the modes of p.
2We also note that the third inequality can hold for all p, q as long as F is a subset of Lipschitz functions.
3This is consistent with the empirical finding that generators and discriminators with similar depths are
often near-optimal choices of architectures.
4The formal mathematical statement is that Dkl (pkq) is infinity unless p is absolutely continuous with
respect to q.
3
Published as a conference paper at ICLR 2019
with low-dimensional supports. As the main proof technique, we develop tools for approximating
the log-density of a smoothed neural network generator.
We demonstrate in synthetic and controlled experiments that the IPM correlates with the Wasser-
stein distance for low-dimensional distributions with measure-zero support and correlates with KL-
divergence for the invertible generator family (where computation of KL is feasible) (Section 5
and Appendix G.) The theory suggests the possibility that when the KL-divergence or Wasserstein
distance is not measurable in more complicated settings, the test IPM could serve as a candidate
alternative for measuring the diversity and quality of the learned distribution. We also remark that
on real datasets, often the optimizer is tuned to carefully balance the learning of generators and
discriminators, and therefore the reported training loss is often not the test IPM (which requires
optimizing the discriminator until optimality.) Anecdotally, the distributions learned by GANs can
often be distinguished by a well-trained discriminator from the data distribution, which suggests
that the IPM is not well-optimized (See Lopez-Paz & Oquab (2016) for analysis of for the original
GANs formulation.) We conjecture that the lack of diversity in real experiments may be caused by
sub-optimality of the optimization, rather than statistical inefficiency.
1.4	Related work
Various empirical proxy tests for diversity, memorization, and generalization have been developed,
such as interpolation between images (Radford et al., 2016), semantic combination of images via
arithmetic in latent space (Bojanowski et al., 2017), classification tests (Santurkar et al., 2017), etc.
These results by and large indicate that while “memorization” is not an issue with most GANs, lack
of diversity frequently is.
As discussed thoroughly in the introduction, Arora et al. (2017a;b) formalized the potential theoret-
ical sources of mode collapse from a weak discriminator, and proposed a “birthday paradox” that
convincingly demonstrates this phenomenon is real. Many architectures and algorithms have been
proposed to remedy or ameliorate mode collapse (Dumoulin et al., 2016; Srivastava et al., 2017; Di
& Yu, 2017; Borji, 2018; Lin et al., 2017) with varying success. Feizi et al. (2017) showed prov-
able guarantees of training GANs with quadratic discriminators when the generators are Gaussians.
However, to the best of our knowledge, there are no provable solutions to this problem in more
substantial generality.
The inspiring work of Zhang et al. (Zhang et al., 2017) shows that the IPM is a proper metric (instead
of a pseudo-metric) under a mild regularity condition. Moreover, it provides a KL-divergence bound
with finite samples when the densities of the true and estimated distributions exist. Our Section 4.1
can be seen as an extension of (Zhang et al., 2017, Proposition 2.9 and Corollary 3.5). The strength
in our work is that we develop statistical guarantees in Wasserstein distance for distributions such
as injective neural network generators, where the data distribution resides on a low-dimensional
manifold and thus does not have proper density.
Liang (2017) considers GANs in a non-parametric setup, one of the messages being that the sample
complexity for learning GANs improves with the smoothness of the generator family. However, the
rate they derive is non-parametric - exponential in the dimension - unless the Fourier spectrum of
the target family decays extremely fast, which can potentially be unrealistic in practical instances.
The invertible generator structure was used in Flow-GAN (Grover et al., 2018), which observes that
GAN training blows up the KL on real dataset. Our theoretical result and experiments show that
successful GAN training (in terms of the IPM) does imply learning in KL-divergence when the data
distribution can be generated by an invertible neural net. This suggests, along with the message
in (Grover et al., 2018), that the real data cannot be generated by an invertible neural network.
In addition, our theory implies that if the data can be generated by an injective neural network
(Section 4.2), we can bound the closeness between the learned distribution and the true distribution
in Wasserstein distance (even though in this case, the KL divergence is no longer an informative
measure for closeness.)
2	Preliminaries and Notation
The notion of IPM (recall the definition in eq. (1)) includes a number of statistical distances such
as TV (total variation) and Wasserstein-1 distance by taking F to be 1-bounded and 1-Lipschitz
4
Published as a conference paper at ICLR 2019
functions respectively. When F is a class of neural networks, we refer to the F -IPM as the neural
net IPM.5
There are many distances of interest between distributions that are not IPMs, two of which we will
particularly focus on: the KL divergence Dkl(pkq) = Ep[logp(X) - log q(X)] (when the densities
exist), and the Wasserstein-2 distance, defined as W2 (p, q)2 = infπ∈Π E(X,Y)〜∏[kX — Y112] where
Π be the set of couplings of (p, q). We will only consider distributions with finite second moments,
so that W1 and W2 exist.
For any distribution p, We let Pn be the empirical distribution of n i.i.d. samples from
p. The Rademacher complexity of a function class F on a distribution p is Rn(F,p) =
E IsUpf∈F |n Pn=I Jf (Xi)I] where Xi 〜P i.i.d. and εi 〜 {±1} are independent. We define
Rn(F, G) = sUpp∈G Rn(F,p) to be the largest Rademacher complexity over p ∈ G. The training
IPM loss (over the entire dataset) for the Wasserstein GAN, assuming discriminator reaches opti-
mality, is Eqn [Wf(pn, qn)[6. Generalization of the IPM is governed by the quantity Rn(F, G), as
stated in the following result (see Appendix A.1 for the proof):
Theorem 2.1 (Generalization, c.f. (Arora et al., 2017a)). For anyP ∈ G, we have that
∀q ∈ G, EpnIWF(p,q) - Eqn WF(Pn,qn)] I ≤ 4Rn(F, G).
Miscellaneous notation. We let N (μ, Σ) denote a (multivariate) Gaussian distribution with mean μ
and covariance Σ. For quantities a, b > 0 a . b denotes that a ≤ Cb for a universal constant C > 0
unless otherwise stated explicitly.
3 Restricted Approximability for Basic Distributions
3.1	Gaussian distributions
As a warm-up, we design discriminators with restricted approximability for relatively simple param-
eterized distributions such Gaussian distributions, exponential families, and mixtures of Gaussians.
We first prove that one-layer neural networks with ReLU activation are strong enough to distinguish
Gaussian distributions with the restricted approximability guarantees.
We consider the set of Gaussian distributions with bounded mean and well-conditioned covariance
G = {pθ = N(μ, ∑) : kμk2 ≤ D, σ21inId W ∑ W σ21aχId}. Here D, σmin and σmaχ are considered
as given hyper-parameters. We will show that the IPM WF induced by the following discriminators
has restricted approximability w.r.t. G:
F := {x 7→ ReLU(v>x + b) : kvk2 ≤ 1, IbI ≤ D},	(6)
Theorem 3.1. The set of one-layer neural networks (F defined in eq. (6)) has restricted approx-
imability w.r.t. the Gaussian distributions in G in the sense that for any P, q ∈ G
K ∙ Wι(p, q) . WF(p, q) ≤ Wι(p, q).
with κ
-12 σmin. Moreover, F satisfies Rademacher complexity bound Rn(F, G) . D+σ√ax Jd
Apart from absolute constants, the lower and upper bounds differ by a factor of 1/√d.7 We point
out that the 1 / √d factor is not improvable unless using functions more sophisticated than Lipschitz
functions of one-dimensional projections of x. Indeed, WF (P, q) is upper bounded by the maxi-
mum Wasserstein distance between one-dimensional projections of p, q, which is on the order of
W1 (p, q)/√d when p, q have spherical covariances. The proof is deferred to Section B.1.
Extension to mixture of Gaussians. Discriminator family F with restricted approximability can
also be designed for mixture of Gaussians. We defer this result and the proof to Appendix C.
5This was defined as neural net distance in (Arora et al., 2017a).
6In the ideal case we can take the expectation over q, as the generator q is able to generate infinitely many
samples.
7As shown in (Feizi et al., 2017), the optimal discriminator for Gaussian distributions are quadratic func-
tions.
5
Published as a conference paper at ICLR 2019
3.2	Exponential families
Now we consider exponential families and show that the linear combinations of the sufficient statis-
tics are a family of discriminators with restricted approximability. Concretely, let G = {pθ : θ ∈
Θ ⊂ Rk} be an exponential family, where pθ(x) = z^ exp((θ,T(x)i), ∀χ ∈ X ⊂ Rd: here
T : Rd → Rk is the vector of sufficient statistics, and Z(θ) is the partition function. Let the discrim-
inator family be all linear functionals over the features T (x): F = {x → hv, T (x)i : kvk2 ≤ 1}.
Theorem 3.2. Let G be the exponential family and F be the discriminators defined above. Assume
that the log partition function log Z (θ) satisfies that YI W V2 log Z (θ) W βI. Thenwehaveforany
p, q ∈ G,
PDkl(Pkq) ≤ WF(p, q) ≤ Je= PDkl(Pkq)∙
√γ
Y
√β
(7)
If we further assume X has diameter D and T(x) is L-Lipschitz in X. Then,
DY
√β
WI(P, q) . WF (P, q) ≤ L ∙ WI(P, q)
(8)
Moreover, F has Rademacher complexity bound Rn(F, G) ≤

suPθ∈θ Epθ 口①(X)k2]
n
We note that the log partition function log Z(θ) is always convex, and therefore our assumptions only
require in addition that the curvature (i.e. the Fisher information matrix) has a strictly positive lower
bound and a global upper bound. For the bound eq. (8), some geometric assumptions on the sufficient
statistics are necessary because the Wasserstein distance intrinsically depends on the underlying
geometry of x, which are not specified in exponential families by default. The proof of eq. (7)
follows straightforwardly from the standard theory of exponential families. The proof of eq. (8)
requires machinery that we will develop in Section 4 and is therefore deferred to Section B.2.
4	Restricted Approximability for Neural Net Generators
In this section, we design discriminators with restricted approximability for neural net generators, a
family of distributions that are widely used in GANs to model real data.
In Section 4.1 we consider the invertible neural networks generators which have proper densities.
In Section 4.2, we extend the results to the more general and challenging setting of injective neu-
ral networks generators, where the latent variables are allowed to have lower dimension than the
observable dimensions (Theorem 4.5) and the distributions no longer have densities.
4.1	Invertible neural network generators
In this section, we consider the generators that are parameterized by invertible neural networks8.
Concretely, let G be a family of neural networks G = {Gθ : θ ∈ Θ}. Let Pθ be the distribution of
X = Gθ(Z), Z 〜N(0, diag(γ2)).	(9)
where Gθ is a neural network with parameters θ and Y ∈ Rd standard deviation of hidden factors.
By allowing the variances to be non-spherical, we allow each hidden dimension to have a different
impact on the output distribution. In particular, the case Y = [1k, δ1d-k] for some δ 1 has the
ability to model data around a “k-dimensional manifold” with some noise on the level of δ.
We are interested in the set of invertible neural networks Gθ. We let our family G consist of standard
'-layer feedforward nets X = Gθ (z) of the form
X = W'σ(W'-1σ(∙ ∙ ∙ σ(WιZ + b1) ∙ ∙ ∙) + b`-i) + b`,
where Wi ∈ Rd×d are invertible, bi ∈ Rd, and σ : R → R is the activation function, on which we
make the following assumption:
8Our techniques also applies to other parameterized invertible generators but for simplicity we only focus
on neural networks.
6
Published as a conference paper at ICLR 2019
Assumption 1 (Invertible generators). Let RW, Rb, κσ, βσ > 0, δ ∈ (0, 1] be parameters which are
considered as constants (that may depend on the dimension). We consider neural networks Gθ that
are parameterized by parameters θ = (Wi, bi)i∈['] belonging to the set
Θ = {(Wi,bi)i∈['] : max {kWikop , IlWiTlIopo ≤ RW, Mk? ≤ Rb, ∀i ∈ [']}.
The activation function σ is twice-differentiable with σ(0) = 0, σ0(t) ∈ [κσ-1, 1], and
∣(σ-1)00∕(σ-1)0∣ ≤ βσ. The standard deviation ofthe hidden factors satisfy Yi ∈ [δ, l].
Clearly, such a neural net is invertible, and its inverse is also a feedforward neural net with activation
σ-1. We note that a smoothed version of Leaky ReLU (Xu et al., 2015) satisfies all the conditions on
the activation functions. Further, it is necessary to impose some assumptions on the generator net-
works because arbitrary neural networks are likely tobe able to implement pseudo-random functions
which can’t be distinguished from random functions by even any polynomial time algorithms.
Lemma 4.1. For any θ ∈ Θ, the function log pθ can be computed by a neural network with at most
' + 1 layers, O('d2) parameters, and activation function among {σ-1, log σ-10, (∙)2} Oftheform
1	`
fφ(x) = 2〈hi, diag(γ-2)hι> + X (1d,logσ-1(hj))+ C,	(10)
where h` = W'(x 一 b`), hk = Wk (σ-1(hk+ι) 一 bk) for k ∈ {' 一 1,..., 1}, and the parameter φ =
((Wj,bj)j=i,C) satisfies φ ∈ Φ = {φ : ∣∣W∙∣∣op ≤ RW, IIbjk2 ≤ Rb, |C| ≤ (' 一 1)dlogRW}.
As a direct consequence, the following family F of neural networks with activation functions above
ofat most ` + 2 layers contains all the functions {log p 一 log q : p, q ∈ G} :
F = {fφι- fφ2 : Φ1,Φ2 ∈ Φ}.	(11)
We note that the exact form of the parameterized family F is likely not very important in practice,
since other family of neural nets also possibly contain good approximations of logp 一 log q (which
can be seen partly from experiments in Section G.)
The proof builds on the change-of-variable formula log pθ (x) = log φγ(Gθ-1(x)) +
log | det dG∂χ(x) | (where φγ is the density of Z 〜N(0, diag(γ2))) and the observation that G-1 is
a feedforward neural net with ` layers. Note that the log-det of the Jacobian involves computing the
determinant of the (inverse) weight matrices. A priori such computation is non-trivial for a given
Gθ. However, it’s just some constant that does not depend on the input, therefore it can be repre-
sentable by adding a bias on the final output layer. This frees us from further structural assumptions
on the weight matrices (in contrast to the architectures in flow-GANs (Gulrajani et al., 2017)). We
defer the proof of Lemma 4.1 to Section D.2.
Theorem 4.2. Suppose G = {pθ : θ ∈ Θ} is the set of invertible-generator distributions as defined
in eq. (9) satisfying Assumption 1. Then, the discriminator class F defined in Lemma 4.1 has
restricted approximability w.r.t. G in the sense that for any p, q ∈ G,
Wi(p,q)2 . Dkl(Pkq) + Dki(q∣p) ≤ WF(p,q) . ^2d (Wi(p, q) + dexp(-10d)),
When n & max {d, δ-8log1∕δ}, we have the generalization bound Rn(F, G) ≤ εgen :=
∕d4 log n
V δ4 n .
The proof of Theorem 4.2 uses the following lemma that relates the KL divergence to the IPM when
the log densities exist and belong to the family of discriminators.
Lemma 4.3 (Special case of (Zhang et al., 2017, Proposition 2.9)). Let ε > 0. Suppose F satisfies
that for every q ∈ G, there exists f ∈ F such that kf 一 (log p 一 log q)k∞ ≤ , and that all the
functions in F are L-Lipschitz. Then,
Dkl(Pkq) + Dkl(q∣p) - ε ≤ WF(p, q) ≤ L ∙ Wι(p, q)∙	(12)
We outline a proof sketch of Theorem 4.2 below and defer the full proof to Appendix D.3. As we
choose the discriminator class as in Lemma 4.1 which implements logP 一 log q for any P, q ∈ G,
7
Published as a conference paper at ICLR 2019
by Lemma 4.3, WF (p, q) is lower bounded by Dkl(pkq) + Dkl(qkp). It thus suffices to (1) lower
bound this quantity by the Wasserstein distance and (2) upper bound WF (p, q) by the Wasserstein
distance.
To establish (1), we will prove in Lemma D.3 that for any p, q ∈ G,
W1 (p, q)2 ≤ W2(p,q)2 . Dkl(pkq) + Dkl(qkp).
Such a result is the simple implication of transportation inequalities by Bobkov-Gotze and Gozlan
(Theorem D.1), which state that if X 〜P (or q) and f is I-LiPschitz implies that f (X) is SUb-
Gaussian, then the inequality above holds. In our invertible generator case, we have X = Gθ(Z)
where Z are independent Gaussians, so as long as Gθ is suitably Lipschitz, f(X) = f(Gθ (Z)) is a
sub-Gaussian random variable by the standard Gaussian concentration result (Vershynin, 2010).
The upper bound (2) would have been immediate if functions in F are Lipschitz globally in the
whole space. While this is not strictly true, We give two workarounds - by either doing a truncation
argument to get a W1 bound with some tail probability, or a W2 bound which only requires the
Lipschitz constant to grow at most linearly in kxk2. This is done in Theorem D.2 as a straightforward
extension of the result in (Polyanskiy & Wu, 2016).
Combining the restricted approximability and the generalization bound, we immediately obtain that
if the training succeeds with small expected IPM (over the randomness of the learned distributions),
then the estimated distribution q is close to the true distribution p in Wasserstein distance.
Corollary 4.4. In the setting of Theorem 4.2, with high probability over the choice of training data
Pn, we have that if the training process returns a distribution q ∈ G such that E^n [Wf(pn, qn)] ≤
εtrain, then with εgen := Jd[4n n, we have
Wl(p, q) ≤ W2(p, q) . (εtrain + εgen)1/2.	(13)
We note that the training error is measured by Eqm [Wf(pn, qm)], the expected IPM over the ran-
domness of the learned distributions, which is a measurable value because one can draw fresh sam-
ples from q to estimate the expectation. It’s an important open question to design efficient algorithms
to achieve a small training error according to this definition, and this is left for future work.
4.2	Injective neural network generators
In this section we consider injective neural network generators (defined below) which generate dis-
tributions residing on a low dimensional manifold. This is a more realistic setting than Section 4.1
for modeling real images, but technically more challenging because the KL divergence becomes
infinity, rendering Lemma 4.3 useless. Nevertheless, we design a novel divergence between two
distributions that is sandwiched by Wasserstein distance and can be optimized as IPM.
Concretely, we consider a family of neural net generators G = Gθ : Rk → Rd where k < d
and Gθ is injective function. 9 Therefore, Gθ is invertible only on the image of Gθ, which is a k-
dimensional manifold in Rd. Let G be the corresponding family of distributions generated by neural
nets in G.
Our key idea is to design a variant of the IPM, which provably approximates the Wasserstein dis-
tance. Let Pβ denote the convolution of the distribution P with a Gaussian distribution N(0, β2I).
We define a smoothed F -IPM between P, q as
dF(p,q) , inf (WF(Pe,qβ) + β log1∕β)1/2,
(14)
Clearly dF can be optimized as WF with an additional variable β introduced in the optimization.
We show that for certain discriminator class (see Section E for the details of the construction) such
that dF approximates the Wasserstein distance.
Theorem 4.5 (Informal version of Theorem E.1). Let G be defined as above. There exists a discrim-
inator class F such that for any pair of distributions P, q ∈ G, we have
Wι(p,q) . dF(p, q) . poly(d) ∙ WI(P,q)1/6 +exp(-Ω(d)).	(15)
9In other words, Gθ (x) 6= Gθ (y) if x 6= y.
8
Published as a conference paper at ICLR 2019
Furthermore, when n & poly(d), we have the generalization bound
Rn(F, G)
Here poly(d) hides polynomial dependencies on d and several other parameters that will be defined
in the formal version (Theorem E.1.)
The direct implication of the theorem Is that If d(pn, qn) Is small for n & poly(d), then W (p, q) Is
guaranteed to be also small and thus we don’t have mode collapse.
5	Simulation
Our theoretical results on neural network generators in Section 4 convey the message that mode
collapse will not happen as long as the discriminator family F has restricted approximability with
respect to the generator family G. In particular, the IPM WF (p, q) is upper and lower bounded by
the Wasserstein distance W1 (p, q) given the restricted approximability. We design certain specific
discriminator classes in our theory to guarantee this, but we suspect it holds more generally in GAN
training in practice.
We perform two sets of synthetic experiments to confirm that the practice is indeed consistent with
our theory. We design synthetic datasets, set up suitable generators, and train GANs with either our
theoretically proposed discriminator class with restricted approximability, or vanilla neural network
discriminators of reasonable capacity. In both cases, we show that IPM is well correlated with the
Wasserstein / KL divergence, suggesting that the restricted approximability may indeed hold in prac-
tice. This suggests that the difficulty of GAN training in practice may come from the optimization
difficulty rather than statistical inefficiency, as we observe evidence of good statistical behaviors on
“typcial” discriminator classes.
We briefly describe the experiments here and defer details of the second experiment to Appendix G.
(a)	We learn synthetic 2D datasets with neural net generators and discriminators and show that the
IPM is well-correlated with the Wasserstein distance (Section 5.1).
(b)	We learn invertible neural net generators with discriminators of restricted approximability and
vanilla architectures (Appendix G). We show that the IPM is well-correlated with the KL diver-
gence, both along training and when we consider two generators that are perturbations of each
other (the purpose of the latter being to eliminate any effects of the optimization).
5.1	Experiments on Synthetic 2d Datasets
In this section, we perform synthetic experiments with WGANs that learn various curves in two
dimensions. In particular, we will train GANs that learn the unit circle and a “swiss roll” curve (Gul-
rajani et al., 2017) - both distributions are supported on a one-dimensional manifold in R2, therefore
the KL divergence does not exist, but one can use the Wasserstein distance to measure the quality of
the learned generator.
We show that WGANs are able to learn both distributions pretty well, and the IPM WF is strongly
correlated with the Wasserstein distance W1. These ground truth distributions are not covered in
our Theorems 4.2 and 4.5, but our results show evidence that restricted approximability is still quite
likely to hold here.
Ground truth distributions We set the ground truth distribution to be a unit circle or a Swiss roll
curve, sampled from
Circle : (x, y)〜 Uniform({(x, y) : x2 + y2 = 1})
Swiss roll : (x, y) = (z cos(4πz),z sin(4πz)) : Z 〜Uniform([0.25,1]).
Generators and discriminators We use standard two-hidden-layer ReLU nets as both the generator
class and the discriminator class. The generator architecture is 2-50-50-2, and the discriminator
architecture is 2-50-50-1. We use the RMSProp optimizer (Tieleman & Hinton, 2012) as our update
9
Published as a conference paper at ICLR 2019
rule, the learning rates are 10-4 for both the generator and discriminator, and we perform 10 steps
on the discriminator in between each generator step.
Metric We compare two metrics between the ground truth distribution p and the learned distribution
q along training:
(1)	The neural net IPM WF (p, q), computed on fresh batches from p, q through optimizing a sepa-
rate discriminator from cold start.
(2)	The Wasserstein distance W1 (p, q), computed on fresh batches from p, q using the POT pack-
age10. As data are in two dimensions, the empirical Wasserstein distance Wι(p^, q) does not
suffer from the curse of dimensionality and is a good proxy of the true Wasserstein distance
W1(p, q) (Weed & Bach, 2017).
(a) Iteration 500.
(b) Iteration 10000.
(c) Comparing IPM and Wasserstein.
Figure 1: Experiments on the swiss roll dataset. The neural net IPM, the Wasserstein distance, and the sample
quality are correlated along training. (a)(b): Sample batches from the ground truth and the learned generator at
iteration 500 and 5000. (c): Comparing the F-IPM and the Wasserstein distance. RealG and fakeG denote the
ground truth generator and the learned generator, respectively.
Result See Figure 1 for the Swiss roll experiment and Figure 2 (in Appendix F) for the unit circle
experiment. On both datasets, the learned generator is very close to the ground truth distribution at
iteration 10000. Furthermore, the neural net IPM and the Wasserstein distance are well correlated.
At iteration 500, the generators have not quite learned the true distributions yet (by looking at the
sampled batches), and the IPM and Wasserstein distance are indeed large.
6	Conclusion
We present the first polynomial-in-dimension sample complexity bounds for learning various dis-
tributions (such as Gaussians, exponential families, invertible neural networks generators) using
GANs with convergence guarantees in Wasserstein distance (for distributions with low-dimensional
supports) or KL divergence. The analysis technique proceeds via designing discriminators with re-
stricted approximability - a class of discriminators tailored to the generator class in consideration
which have good generalization and mode collapse avoidance properties.
We hope our techniques can be in future extended to other families of distributions with tighter
sample complexity bounds. This would entail designing discriminators that have better restricted
approximability bounds, and generally exploring and generalizing approximation theory results in
the context of GANs. We hope such explorations will prove as rich and satisfying as they have been
in the vanilla functional approximation settings.
References
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein GAN. arXiv preprint
arXiv:1701.07875, 2017.
10https://pot.readthedocs.io/en/stable/index.html
10
Published as a conference paper at ICLR 2019
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium
in generative adversarial nets (GANs). In International Conference on Machine Learning, pp.
224-232, 2017a.
Sanjeev Arora, Andrej Risteski, and Yi Zhang. Do GANs actually learn the distribution? do gans
learn the distribution? some theory and empirics. ICLR, 2017b.
Piotr Bojanowski, Armand Joulin, David Lopez-Paz, and Arthur Szlam. Optimizing the latent space
of generative networks. arXiv preprint arXiv:1707.05776, 2017.
Ali Borji. Pros and cons of GAN evaluation measures. arXiv preprint arXiv:1802.03446, 2018.
James Demmel, Ioana Dumitriu, and Olga Holtz. Fast linear algebra is stable. Numerische Mathe-
matik, 108(1):59-91, 2007.
Xinhan Di and Pengqian Yu. Max-boost-GAN: Max operation to boost generative ability of gen-
erative adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 1156-1164, 2017.
Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro, Alex Lamb, Martin Ar-
jovsky, and Aaron Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704,
2016.
I. Durugkar, I. Gemp, and S. Mahadevan. Generative Multi-Adversarial Networks. ArXiv e-prints,
November 2016.
Soheil Feizi, Changho Suh, Fei Xia, and David Tse. Understanding GANs: the LQG setting. arXiv
preprint arXiv:1710.10793, 2017.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Aditya Grover, Manik Dhar, and Stefano Ermon. Flow-GAN: Combining maximum likelihood and
adversarial learning in generative models. In AAAI Conference on Artificial Intelligence, 2018.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved training of Wasserstein GANs. In Advances in Neural Information Processing Systems,
pp. 5769-5779, 2017.
Xun Huang, Yixuan Li, Omid Poursaeed, John Hopcroft, and Serge Belongie. Stacked generative
adversarial networks. In Computer Vision and Patter Recognition, 2017.
D. Jiwoong Im, H. Ma, C. Dongjoo Kim, and G. Taylor. Generative Adversarial Parallelization.
ArXiv e-prints, December 2016.
Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: isoperimetry and processes.
Springer Science & Business Media, 2013.
Tengyuan Liang. How well can generative adversarial networks (GAN) learn densities: A nonpara-
metric view. arXiv preprint arXiv:1712.08244, 2017.
Zinan Lin, Ashish Khetan, Giulia Fanti, and Sewoong Oh. PacGAN: The power of two samples in
generative adversarial networks. arXiv preprint arXiv:1712.04086, 2017.
David Lopez-Paz and Maxime Oquab. Revisiting classifier two-sample tests. arXiv preprint
arXiv:1610.06545, 2016.
Valentina Masarotto, Victor M Panaretos, and Yoav Zemel. Procrustes metrics on covariance oper-
ators and optimal transportation of gaussian processes. arXiv preprint arXiv:1801.01990, 2018.
Alfred Muller. Integral probability metrics and their generating classes of functions. Advances in
Applied Probability, 29(2):429-443, 1997.
11
Published as a conference paper at ICLR 2019
Hoi Nguyen, Terence Tao, and Van Vu. Random matrices: tail bounds for gaps between eigenvalues.
Probability Theory andRelatedFields,167(3-4):777-816, 2017.
Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with auxil-
iary classifier gans. arXiv preprint arXiv:1610.09585, 2016.
Yury Polyanskiy and Yihong Wu. Wasserstein continuity of entropy and outer bounds for interfer-
ence channels. IEEE Transactions on Information Theory, 62(7):3992-4002, 2016.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. In International Conference on Learning Repre-
sentations, 2016.
David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-
propagating errors. nature, 323(6088):533, 1986.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training GANs. In Advances in Neural Information Processing Systems,
2016.
Shibani Santurkar, Ludwig Schmidt, and Aleksander Madry. A classification-based perspective on
GAN distributions. arXiv preprint arXiv:1711.00970, 2017.
Bernhard A Schmitt. Perturbation bounds for matrix square roots and pythagorean sums. Linear
algebra and its applications, 174:215-227, 1992.
Akash Srivastava, Lazar Valkoz, Chris Russell, Michael U Gutmann, and Charles Sutton. VeeGAN:
Reducing mode collapse in gans using implicit variational learning. In Advances in Neural Infor-
mation Processing Systems, pp. 3310-3320, 2017.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-RMSProp: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26-
31, 2012.
Ilya Tolstikhin, Sylvain Gelly, Olivier Bousquet, Carl-Johann Simon-Gabriel, and Bernhard
Scholkopf. AdaGAN: Boosting generative models. arXiv preprint arXiv:1701.02386, 2017.
Ramon van Handel. Probability in high dimension. Technical report, PRINCETON UNIV NJ, 2014.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
arXiv:1011.3027, 2010.
Martin J. Wainwright. High-dimensional statistics: A non-asymptotic viewpoint.	To ap-
pear, 2018. URL https://www.stat.berkeley.edu/~wainwrig/nachdiplom/
Chap5_Sep10_2015.pdf.
Jonathan Weed and Francis Bach. Sharp asymptotic and finite-sample rates of convergence of em-
pirical measures in Wasserstein distance. arXiv preprint arXiv:1707.00087, 2017.
Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical evaluation of rectified activations in
convolutional network. arXiv preprint arXiv:1505.00853, 2015.
Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong
He. AttnGAN: Fine-grained text to image generation with attentional generative adversarial net-
works. arXiv preprint, 2017.
Pengchuan Zhang, Qiang Liu, Dengyong Zhou, Tao Xu, and Xiaodong He. On the discrimination-
generalization tradeoff in GANs. arXiv preprint arXiv:1711.02771, 2017.
12
Published as a conference paper at ICLR 2019
A Proofs for Section 2
A.1 Proof of Theorem 2.1
Fixing p^n, consider a random sample qn. It is easy to verify that the F-IPM satisfies the triangle
inequality, so we have
IWf(P,q) — Eqn[Wf(^n,qn)]∣ ≤ Eqn[|Wf(p,q) — WF(Pn,qn)]
≤ Eqn[Wf(p,Pn) + WF(q,qn)] = WF(p,Pn) + Eqn[Wf(q,qn)].
Taking expectation over Pn on the above bound yields
Epn [|Wf(p,q) — Eqn[Wf(Pn,qn)]∣] ≤ Epn[Wf(p,Pn)] + Eqn[Wf(q,qn)].
So it suffices to bound Epn [WF(p,Pn)] by 2Rn(F, G) and the same bound will hold for q. Let Xi
be the samples in pn. By symmetrization, We have
WF (p,pn)= E
1 n
SUP - ɪ2 f (Xi)- Ep[f (X)]
f∈F n i=1
1 n
≤ 2E sup -yZεif(Xi)
f∈F n i=1
2E[Rn(F,p)] ≤ 2Rn(F,G).
Adding up this bound and the same bound for q gives the desired result.
B Proofs for Section 3
B.1 Proof of Theorem 3.1
Recall that our discriminator family is
F = {x → σ(v>x + b) : ∣∣v∣∣2 ≤ 1, |b| ≤ D}.
Restricted approximability The upper bound WF(p1,p2) ≤ W1 (p1, p2) follows directly from
the fact that functions in F are 1-Lipschitz.
We now establish the lower bound. First, we recover the mean distance, in which we use the fol-
lowing simple fact: a linear discriminator is the sum of two ReLU discriminators, or mathematically
t = σ(t) 一 σ(-t). Taking V = ∣∣ μ1-μ2 , We have
—	'	)	k	l∣μι -μ21∣2
kμι — μ21∣2 = v>μι — V>μ2 = Epi [v>X] ― Ep2 [v>X]
=(Epi [σ(v>X)] — Ep2 [σ(v>X)]) + (—Epi [σ(i>X)]+ Ep2 [σ(i>X)])
≤ |Epi [σ(v>X)] — Ep2[σ(v>X)]∣ + |EpiQ(-v>X)] — Ep2[b(—v>X)]∣.
Therefore at least one of the above two terms is greater than ∣∣μι — μ2k2 /2, which shows that
wf(pι,P2) ≥ kμι — μ2∣∣2/2.
For the covariance distance, we need to actually compute Ep[σ(v>X + b)] for P = N(μ, Σ). Note
that X = ∑"Z + μ, where Z 〜N(0,I√). Further, we have v>X = ∣∣∑"v∣∣2 W + v>μ for
W 〜N(0,1), therefore
Ep[σ(v>X + b)] = E [σ (k1/2。|| W + v>μ + b)]
=产 M(W+⅛μ⅜ !I—R L).
(Defining R(a) = E[max {W + a, 0}] for W 〜N(0,1).) Therefore, the neuron distance between
the two Gaussians is
WF(p1,p2) =	SUP	∣∣∑1/2v∣∣ R (v μ1+∣b] — ^2/2v|| R (v μ2+∣b],
3∣2≤1,∣b∣≤D11	ll2	1∣∣∑y2v∣∣j	11 ιι2	1∣∣∑γ2v∣∣j
13
Published as a conference paper at ICLR 2019
As a → max {a + w, 0} is strictly increasing for all w, the function R is strictly increasing. It is
also a basic fact that R(O) = 1/√2∏.
Consider any fixed v. By flipping the sign of v, we can let v>μ1 ≥ v>μ2 without changing
(∑1∕2v1 . Now, letting b = -v>(μι - μ2)∕2 (note that |b| ≤ D is a valid choice), we have
v>μi + b = /(N-"2) ≥ 0, v>μ2 + b
As R is strictly increasing, for this choice of (v, b) we have
—
V>("1 - "2)≤ 0
2 一 .
∑y2Vll R	⅛±τb
T2	[『IL
v>μ2 + b
—
≥R(O)(卜 1ML-愕 RD=√12∏ (k TL-愕 RD.
Ranging over kvk2 ≤ 1 we then have
WF (P1,p2) ≥√1π kVuHM/RL-怛/MU∙
The quantity in the supremum can be further bounded as
∑1∕2dL
I∣ς1∕2 Il I _	B>(ς1- £2)v|	、	|v>(£l- £2)v|
B V|2 I =产mTΣ7^l ≥ λmax(∑1/2) + λmax(∑2/2)，
Choosing v = vmax(Σ1 - Σ2 ) gives
WF (P1，p2)≥√2∏ 潞 pUMFL
Now, using the perturbation bound
—
—
愕山≥∖
∑1/2
2op
≤ λmin(∑ι) + λmin(∑2)
•恪-立匾≤ 2σm-监-立毗,
—
1
(cf. (Schmitt, 1992, Lemma 2.2)), we get
WF(P1,P2) ≥ 2@1	∙ ^min kV -展||	≥ ^n G 卜- ^l1
2 2πσmax	op	2πσmax d	Fr
Combining the above bound with the bound in the mean difference, we get
WF(p1,p2) ≥ 1 ”μι-"2k
2 + Gin	H∑ι/2
2πdσmax
2Fr
≥	σmin
2 ^∖Z^2πdσ max
_	σmin
2 ^∖Z^2πdσ max
Jμι-μ2k2+U >u=inU >=/N2-U 或个
• W2(P1,P2) ≥ C /^门-WI(P1,P2)
2 2πdσmax
Fr
(16)
2
—
2
The last equality following directly from the closed-form expression of the W2 distance between
two Gaussians (Masarotto et al., 2018, Proposition 3). Thus the claimed lower bound holds with
C = 1∕(2√2∏).
KL Bound We use the W2 distance to bridge the KL and the F -distance, which uses the machinery
developed in Section D. Let p1,p2 be two Gaussians distributions with parameters θ% = (μi, ∑i) ∈
Θ. By the equality
Dkl(p1kp2)+Dkl(p2kp1) = (Ep1[logp1(X)]-Ep2[logp1(X)])+(Ep2[logp2(X)]-Ep1[logp2(X)]),
14
Published as a conference paper at ICLR 2019
it suffices to upper bound the term only involving logp1(X) (the other follows similarly), which by
Theorem D.2 requires bounding the growth of kV log pi (x) k 2. We have
l∣V log PI(X)I∣2 = ∣∣ς-I(X — μI)II2 ≤ σ-2n IIx — μik2 .
Further EpJ∣∣x — μι∣∣2] ≤ tr(∑i) + kμi — μι∣∣2 ≤ dσ2mχ + 4D2 for i = 1,2, therefore by (a trivial
variant of) Theorem D.2(c) we get
Epi [logPi(X)] — Ep2 [logPi(X)] ≤ σ-2n(√dσmaχ + 2D)W2(p1,p2).
The same bound holds for log p2 . Adding them up and substituting the bound appendix B.1 gives
that
Cf H ∖∣n∕ Ii 、/ Vdσmax + 2DT" /	、/ Vdσmax( Vdσmax + D)T" /	\
Dkl(PikP2) + Dkl(P2∣∣pi) . --------2-------W2(P1,P2) . -------------3------------WF(P1,P2).
σσ
min	min
Generalization We wish to bound for all θ = (μ, Σ) ∈ Θ
1 n >
Rn(F, Pθ) = Epθ	sup ,上Eεiσ(v>Xi + b)
J∣vk2≤1,∣b∣≤D n M
As σ : R → R is 1-Lipschitz, by the Rademacher contraction inequality (Ledoux & Talagrand,
2013), we have
1 n
Epθ	sup 一εiσ (v>Xi + b)
J∣v∣2≤1,∣b∣≤D n M
The right hand side can be bounded directly as
sup
kv∣2≤1,∣b∣≤D
1n
—Vεi(v>Xi + b)
— i=i
SUP	|b + v>μ∣E
kv∣2≤1,∣b∣≤D
≤ 2Epθ
pθ
Epθ
1n
n 工ε + epθ
1 n
sup 一	εi (v>Xi + b)
kv∣2≤1,∣b∣≤D n M
sup
kv∣2≤1,∣b∣≤D
1n
(b + v>μ)1∑
— i=i
SUp 1( v,1 Xεi(Xi — μ))
∣v∣2≤i	— i=i
≤
≤
i=i
1n
εi + — fεiV>(Xi — μ)
i=i
≤
n
B.2 Proof of Theorem 3.2
KL bounds Recall the basic property of exponential family that A(θ) = Epθ [T (X)]. Suppose
P = Pθ1 and q = Pθ2 . Then,
WF(p,q)= SUP Epθi [hv,T(X)i] — Epθ2 [hv,T(X)i]
∣v∣2≤i	1	2
=sup hv, VA(θι) -VA(θ2)i = ∣∣VA(θι)-VA(θ2)∣∣2.
∣v∣2≤i
By the assumption on V2A we have that
γ∣∣θi — Θ2k2 ≤ WF(Pθi,Pθ2) ≤ βkθi — Θ2k2	(17)
Moreover, the exponential family also satisfies that
Dkl(Pθi kPθ2) = A(θ2) — A(θi) — hVA(θ1),θ2 — θii = ∕1 ρ>V2A(θ2 + tρ)ρdt
0
where ρ = θi — θ2. Using the assumption we have that γIθi — θ2 I2 ≤ ρ>V2A(θ2 + tρ)ρ ≤
β∣∣θι — θ2k2 and therefore ɪγ∣θι — θ2k2 ≤ Dkl(pθJ∣Pθ2) ≤ 11 β∣∣θι — θ2k2. Combining this with
eq. (17) we complete the proof.
15
Published as a conference paper at ICLR 2019
Wasserstein bounds We show eq. (8). As diam(X ) = D, there exists x0 ∈ X such that
kx - x0k ≤ D for all x ∈ X. Hence for any 1-Lipschitz function f : Rd → R we have that
|f (X) - f (x0)| ≤ kx - x0 k2 ≤ D. By the Hoeffding Lemma, f (X) is D2 /4-sub-Gaussian.
Applying Theorem D.1(a), we get that for any p, q ∈ G,
，、	D2rZ~~n IDY ，、
Wι(p, q) ≤ ʌ/ɪDki(Pkq) . √β ∙ WF(p,q).
Generalization For any θ ∈ Θ we compute the Rademacher complexity
1 n
Rn(F,Pθ)= Epθ	sup -s)Zεi hv,T(Xi))
kvk2≤1 n i=1
1n
n X εT (Xi)
i=1
≤
1n
n X εT(Xi)
i=1
Epθ[kT (X )k2]
n
C Results on Mixture of Gaussians
We consider mixture of k identity-covariance Gaussians on Rd :
kk
Pθ = EwiN(μi,Id) Wi ≥ 0, Ewi = 1.
i=1	i=1
We assume that θ ∈ Θ = {∣∣μi∣∣2 ≤ D,wi ≥ exp(-Bw) i ∈ [k]}.
We will use a one-hidden-layer neural network that implements (a slight modification of) logpθ:
F = If 1	—	f2	： fi = logX wji) exp (μji)>x +	bji))	: exp(-Bw)	≤	wji)	≤ 1,	∣∣μji)∣∣2 ≤ D, 0 ≥	bji)	≥ -D2
Theorem C.1. The family F is suitable for learning mixture of k Gaussians. Namely, we have that
(1)	(Restricted approximability) For any θ1, θ2 ∈ Θ, we have
JI ∙ W2 (Pθ1,Pθ2 ) ≤ WF(Pθ1,Pθ2 ) ≤ 2D ∙ WI(Pθ1,Pθ2 ).
D2 + 1	1	2	1	2	1	2
(2)	(Generalization) We have for some absolute constant C > 0 that
D(P 、/厂 rk(logk + D2 + Bw)dlogn
sup Rn(F,Pθ) ≤ Cu---------------------.
θ∈Θ	n
C.1 The Gaussian concentration result
The Gaussian concentration result (Vershynin, 2010, Proposition 5.34) will be used here and in later
proofs, which we provide for convenience.
Lemma C.2 (Gaussian concentration). Suppose X 〜N (0, Id) and f : Rd → R is L-Lipschitz, then
f(X) is L2-sub-Gaussian.
C.2 Proof of Theorem C.1
Restricted approximability For the upper bound, it suffices to show that each
k
f(x) = log ɪ2 wj exp (μ>x + bj)	(18)
j=1
16
Published as a conference paper at ICLR 2019
is D-Lipschitz. Indeed, we have
I∣v log f (X)Il2
Pk=i Wj exp(μ>X + bj )μj∙
Pk=I Wj exp(μ>x + bj)
≤
2
This further shows that every discriminator f1 - f2 ∈
rem D.2(a) we get the upper bound.
Pjk=I wj eχp(μ>X + bj) kμj k2
.	,	'κ'''- -Z__/ .
∑j=ι wj eχp(μ> X + bj)	一
F is at most 2D-Lipschitz, so by Theo-
We now establish the lower bound. As F implements the KL divergence, for any two p1, p2 ∈ P,
we have
WF(p1,p2) ≥ Dkl(p1Ip2) + Dkl(p2Ip1).
We consider regularity properties of the distributions p1 , p2 in the Bobkov-Gotze sense (Theo-
rem D.1(a)). Suppose pi = P WjN(μj,Id). For any I-LiPschitz function f : Rd → R, we
have
j
f (X ) = Ef(N(μj ,Id)).
j=1
Letting Xj 〜 N(μj∙,Id) be the mixture components. By the Gaussian concentration (Lemma C.2),
each f(Xj) is 1-sub-Gaussian, so we have for any λ ∈ R
kk	k
E[eλf(x)] = X wjE[eλf(χj)] ≤ X eλE[f(χj )]+λ2/2 = eλ2/2 X
wje
j=1	j=1	j=1
'---
λE[f (Xj)]
}
"'l^^^^^^^{^^^^^^^"""
I
Now, term I is precisely the MGF of a discrete random variable on Y ∈ R which takes value
E[f (Xj)] with probability wj-. For Z 〜N(0,1) we have
∣E[f(Xj)] - E[f(Z)]| = ∣E[f(μj+ Z)] - E[f(Z)]∣ ≤ E[|f(〃j + Z)- f (Z)|] ≤ k〃j∣2 ≤ D.
Therefore the values {E[f (Xj)]}j∈[k] lie in an interval of length at most 2D. By the Hoeffding’s
Lemma, Y is D2-sub-Gaussian, so we have I ≤ exp(λE[Y] + D2λ2∕2), and so
E[eλf(χ)] ≤ exp (λ2 + λE[Y] + D2λ2∕2) = exp (λE[Y] + "(Dj + 1)).
Therefore f(X) is at most (D2 +1)-sub-Gaussian, and thus X satisfies the Bobkov-Gozlan condition
with σ2 = D2 + 1. Applying Theorem D.1(a) we get
WF(p1,p2) ≥ Dkl(pιkp2)+ Dkl(p2kpι) ≥ D⅛ ∙ W(p1,p2).
Generalization Reparametrize the one-hidden-layer neural net eq. (18) as
k
fθ (x) = log E exp(μ> X + bj + log Wj).
∣~	|---------}
j=1
{z
cj
It then suffices to bound the Rademacher complexity of fθ for θ ∈ Θ
{kμj∣∣2 ≤ D, cj ∈ [-(D2 + Bw), 0]}. Define the metric
ρ(θ,θ0) = m∈aχmaχ{∣∣μj -μj∣∣2, |cj - cj|}
and the Rademacher process
1n	1n	k
Yθ = n £ εifθ (Xi) = n £ εi log £ exp(μ> Xi + Cj),
i=1
i=1	j=1
we show that Yθ is suitably Lipschitz in θ (in the ρ metric) and use a one-step discretization bound.
Indeed, we have
MjYθ 口2
1 ∖、	exp(μj Xi + cj)
n i=ι ɛi Pk=I eχp(μ>Xi + cj)
1n
Xi	≤ n X IXi ∣2
n
2	i=1
17
Published as a conference paper at ICLR 2019
and
IVcj yθ |
1 X	exp(μ>Xi + Cj)
n i=ι ɛi Pk=I eχp(μ>χi + Cj)
≤ 1.
Therefore, for any ε > 0 we have
E sup ∣Yθ — Yθ I ≤ Ck (E [kX1k2] + 1) ε ≤ Ck(D + √d)ε	(19)
ρ(θ,θ0)≤ε
for some constant C > 0.
We now bound the expected supremum of the max over a covering set. Let N(Θ, ρ, ε) be a ε-
covering set of Θ under ρ, and N(Θ, ρ, ε) be the covering number. As P looks at each μi, Cj SePa-
rately, its covering number can be upper bounded by the product of each separate covering:
k
N (Θ,ρ,ε) ≤ Y N(B2(D),k∙k2 ,ε)∙N([-(D2+Bw), 3JI,ε) ≤ exp
j=1
(kdlog3D + k log 2(D2 +Bw)).
εε
Now, for each invididual Process Yθ is the i.i.d. average of random variables of the form
εi log Pk=ι exp(μ>X+cj). The log-sum-exp part is D-Lipschitz in X, so We can reuse the analysis
done precedingly (in the Bobkov-Gotze part) to get that log P：=i exp(μ>X + Cj) is D2 (D2 + 1)-
SUb-GaUSsian. Further, its expectation is bounded as (for X 〜P = P ViN(Vi, Id))
k	kk
EX 〜P log£exp(M>X + Cj) = EviEX 〜N(Vi ,Id) log£exp(〃>X + Cj)
j=1	i=1	j=1
kk	kk
≤ ΣSvi log∑ EX 〜N(Vi,Id)[exP(μ>X + Cj )] ≤ ɪ^vi log∑exP(μ>νi + llμj- k2 /2 + Cj )
≤ log k + (2D2 + Bw).
This shows that the term εi log Pj=I exp(μ>X + Cj) is (log k + D2 + Bw)2 + D2(D2 + 1)-sub-
Gaussian, and thus We have by sub-Gaussian maxima bounds that
肩	VJ ” /(logk + D2 + Bw)2 + D2(D2 + 1) ] WCʌ
E max	∣ Yθ ∣ ≤ C ∖ ---------------------- log N (Θ,ρ,ε)
θ∈N (Θ,ρ,ε)	n
/ C llog k + D2 + Bw 1.1 ^^D2 + Bw
≤ C ∖ -------------- kd log---.
nε
By the 1-step discretization bound and combining eq. (19) and appendix C.2, we get
E sup IYθ I ≤ E sup	IYθ - Yθ0 I
θ∈Θ	θ,θ0 ∈Θ,ρ(θ,θ0)≤ε
+ E max	IYθ I
θ∈N (Θ,ρ,ε)
≤
Ck(D + √d)ε + C∖∕log k + D2 +Bw ∙ kdlog D2 + Bw .
nε
Choosing ε = C/n for sufficiently small C (depending on D2, Bw) gives that

E sup IYθ I ≤ C
θ∈Θ
kd(log k + D2 + Bw) logn
n
D Proofs for Section 4
D. 1 Bounding KL by Wasserstein
The following theorem gives conditions on which the KL divergence can be lower bounded by the
Wasserstein 1/2 distance. For a reference see Section 4.1 and 4.4 in van Handel (2014).
18
Published as a conference paper at ICLR 2019
Theorem D.1 (Lower bound KL by Wasserstein). Let P be any distribution on Rd and Xi 吧 P be
the i.i.d. samples from p.
(a)	(Bobkov-Gotze) If f(X1) is σI 2 -sub-Gaussian for any 1-Lipschitz f : Rd → R, then
W1(P, q)2 ≤ 2σ2Dkl(Pkq) for all q.
(b)	(Gozlan) If f(X1, . . . , Xn) is σ2-sub-Gaussian for any 1-Lipschitz f : (Rd)n → R, then
W2(P, q)2 ≤ 2σ2Dkl(Pkq) for all q.
Theorem D.2 (Upper bounding f -contrast by Wasserstein). Let P, q be two distributions on Rd with
positive densities and denote their probability measures by P, Q. Let f : Rd → R be a function.
(a)	(Wi bound) Suppose f is L-Lipschitz, then Ep[f (X)] 一 Eq[f (X)] ≤ L ∙ Wι(p, q).
(b)	(Truncated W1 bound) Let D > 0 be any diameter of interest. Suppose for any Pe ∈ {P, q} we
have
(i)	f is L(D)-Lipschitz in the ball of radius D;
(ii)	We have Epe[f 2 (X)] ≤ M;
(iii)	We have Pe(kXk ≥ D) ≤ Ptail (D),
then we have
Ep[f (X)] 一 Eq[f (X)] ≤ L(D) ∙ Wι(p,q)+4,Mptaii(D).
(c)	(W2 bound) Suppose ∣∣Vf (x)k ≤ ci ∣∣xk + c? forall X ∈ Rd, then we have
Ep [f (X )] 一 Eq [f (X)] ≤ ( ci，Ep[kX『]+ c2 q 优q [∣Xk2] + c2) ∙ W2(P,q).
D.1.1 Proof of Theorem D.2
Proof. (a) This follows from the dual formulation of Wi .
(b)	We do a truncation argument. We have
Ep[f (X)] 一 Eq[f (X)] = Ep[f (X)1 {∣Xk ≤ D}] - Eq[f (X)1 {∣Xk ≤ D}]
'-----------------------------------------V--------------------}
I
+ Ep[f (X)1 {∣Xk > D}] - Eq[f (X)1 {∣Xk > D}].
'---------------------V---------------------}
II
Term II has the followng bound by Cauchy-Schwarz:
II ≤ ,Ep[f2(X)]∙ P(kXk >D) + qE[f2(X)]∙ Q(∣Xk >D) ≤ 2，〃入3(。).
We now deal with term I. By definition of the Wasserstein distance, there exists a coupling
(X, Y)〜∏ such that X 〜P, Y 〜Q, and E∏[∣X 一 Y∣∣] = Wi(p, q). On this coupling, We
have
I =	E∏[f(X)1 {∣Xk ≤ D} — f(Y)1 {∣Yk ≤ D}]
=	Eπ[(f(X) 一 f(Y))1 {∣X∣ ≤D,∣Y∣ ≤D}]
+Eπ[f(X)1{kXk ≤D,kYk > D}] 一 Eπ[f(Y)1 {kYk ≤D,kXk >D}]
(i)
≤	L(D)E∏[kX - Y k 1 {kX k ≤ D,kYk ≤ D}]+ E∏ [|f (X )|1 {kY k ≥ D}]
+E∏[|f(Y)|1 {kXk ≥ D}]
(ii)	1______________________
≤	L(D)E∏[kX — Yk] + PE∏[f2(X)]∙∏(kYk ≥ D)
+ PE∏[f2(Y)]∙∏(kXk ≥ D)
=L(D) ∙ Wi(p,q) + ,Ep[f2(X)]∙ Q(kXk ≥ D) + ,Eq[f2(X)]∙ P(kXk ≥ D)
≤	L(D) ∙ Wi(p, q) + 2PMptaii(D).
19
Published as a conference paper at ICLR 2019
Above, inequality (i) used the Lipschitzness of f in the D-ball, and (ii) used Cauchy-Schwarz.
Putting terms I and II together we get
Ep[f(X)] - Eq[f(X)] ≤ L(D) ∙ Wι(p,q)+4PMptaii(D).
(c)	This part is a straightforward extension of (Polyanskiy & Wu, 2016, Proposition 1). For com-
pleteness we present the proof here. For any x, y ∈ Rd we have
|f(x)- f(y)l = ∣/ hVf (tx + (1 -t)y),χ - yi dt∣ ≤ / ∣∣Vf (tx + (1 - t)y)k∣∣χ - yk dt
≤
Z1(c1t∣x∣
0
+c1(1 - t) ∣y∣ +c2) ∣x - y∣ dt
(c2 +c1 ∣x∣ /2+c1 ∣y∣ /2) ∣x-y∣ .
By definition of the W2 distance, there exists a coupling (X, Y)〜∏ such that X 〜P, Y 〜Q,
and E[kX - Yk2] = W22(p, q). On this coupling, taking expectation of the above bound, we
get
En[|f (X) - f (Y)|] ≤ c2E∏[kX - Yk] + I (En[kXkkX - Yk] + En[kYkkX - Yk])
≤ C2/En[kX - Yk2] + C1 (,En[kXk2]∙ En[kX - Yk2] + ,EnUYk2]∙ En[∣∣X - Yk2])
=卜 2+ci qEp[kχ k2]+1 qE [kχk2]) ∙ W2(p, q).
Finally, the triangle inequality gives
Ep [f (X )] - Eq [f (X )] = En [f (X ) - f (Y)] ≤ En [|f (X ) - f (Y )|],
so the left hand side is also bounded by the preceding quantity.
□
D.2 Proof of Lemma 4.1
It is straightforward to see that the inverse of x = Gθ(z) can be computed as
Z = W-1(σT(W-1σT(∙∙∙ σ-1(W-1 (x - b`)) ∙∙∙) - b2) - bi).	(21)
So G-1 is also a '-layer feedforward net with activation σ-i.
We now consider the problem of representing logpθ (x) by a neural network. Let φγ be the density
of Z 〜N(0, diag(γ2)). Recall that the log density has the formula
Pθ(x) = log φγ ((G-1(x)) + log det dGθ (X).
First consider the inverse network that implements Gθ-1. By eq. (21), this network has ` layers (`- 1
hidden layers), d2 + d parameters in each layer, and σ-1 as the activation function. Now, as log φγ
has the form log φγ(z) = a(γ) - Pi z2∕(2γ2), We can add one more layer on top of Z with the
square activation and the inner product with -γ-2∕2 to get this term.
Second, we show that by adding some branches upon this network, we can also compute the
log determinant of the Jacobian. Define h` = W-1(χ - b`) and backward recursively hk-i =
Wk--11(σ-1(hk) - bk-1) (so that Z = h1), we have
'G∂X(X) = W-Idiag(σ-i0(h2))W-1 …W-IIdiag(σ-10(h'))W-1.
Taking the log determinant gives
log det dG∂X(X) = C + X(1,logσ-10(hk)).
k=2
20
Published as a conference paper at ICLR 2019
As (h`, . . . , h2) are exactly the (pre-activation) hidden layers of the inverse network, we can add
one branch from each layer, pass it through the log σ-10 activation, and take the inner product with
1.
Finally, by adding up the output of the density branch and the log determinant branch, we get a
neural network that computes logpθ(x) with no more than ' + 1 layers and O('d2) parameters, and
choice of activations within {σ-1, log σ-10, (∙)2}.
D.3 Proof of Theorem 4.2
We state a similar restricted approximability bound here in terms of the W2 distance, which we also
prove.
W2(P,q)2 . Dkl(Pkq) + Dklgkp) ≤ WF(P,q).木∙ W2(P,q)∙
The theorem follows by combining the following three lemmas, which we show in sequel.
Lemma D.3 (Lower bound). There exists a constant c = c(RW , Rb, `) > 0 such that for any
θ1 , θ2 ∈ Θ, we have
WF (Pθι ,Pθ2) ≥c ∙ W2(Pθ1 ,Pθ2 )2 ≥ C ∙ W1(Pθ1,Pθ2 )2.
Lemma D.4 (Upper bound). There exists constants Ci = Ci(RW, Rb, `) > 0, i = 1, 2 such that for
any θ1 , θ2 ∈ Θ, we have
(1)	(Wi bound) WF(pθ1,Pθ2) ≤ Cδ√√d ∙ (W1(Pθ1,Pθ2) + √dexp(-10d)).
(2)	(W2 bound) WF (pθ` ,Pθ2) ≤ Cδ√√d ∙ W2 (pθι ,Pθ2)∙
Lemma D.5 (Generalization error). Consider n samples Xi iid pθ? for some θ? ∈ Θ. There exists a
constant C = C(RW, Rb, `) > 0 such that when n ≥ C max d, δ-8 logn , we have
R(T 、n C Cd4 log n
Rn(F,pθ? ) ≤ V 即 .
D.4 Proof of Lemma D.3
We show that p satisfies the Gozlan condition for any θ ∈ Θ and apply Theorem D.1. Let Xi 〜pθ
for i ∈ [n]. By definition, we can write
Xi = Gθ(Zi), Zi 吧 N(0, diag(γ2)).
Let Zi = (zi∕γi)d=ι and G§(e) = G§((Yizi)d=i), then We have Gθ(z) = Gθ(ee) and that Zi are
i.i.d. standard Gaussian. Further, suppose Gθ is L-Lipschitz, then for all z1, z2 ∈ Rd we have
∣Gθ(≡1)- Gθ(e2)I = ∣Gθ(zi) - Gθ(z2)| ≤ L kzi- z2k2 ≤ L k≡ι- ≡2k2,
the last inequality following from γi ≤ 1. Therefore Gθ is also L-Lipschitz.
Now, for any 1-Lipschitz f : (Rd)n → R, we have
∣∣f(Geθ(ze1),...,Geθ(zen))-f(Ge θ(ze10),...,Geθ(zen0))
Geθ(zei) -Ge θ(zei0)
1/2
≤
Therefore the mapping (ze1, . . . , zen) → f (Gθ (ze1), . . . , Gθ(zen)) is L-Lipschitz.	Hence by
Lemma C.2, the random variable
,~ , ~ , ~ , ~ ..
f(X1,...,Xn) =f(Ge θ(Ze1),...,Ge θ(Zen))
21
Published as a conference paper at ICLR 2019
is L2-sub-Gaussian, and thus the Gozlan condition is satisfied with σ2 = L2 . By definition of the
network Gθ we have
`
L ≤ γ kWk kop ∙ κσ-1 ≤ Rw'κσ-1=C(RW,`).
k=1
Now, for any θ1, θ2 ∈ Θ, we can apply Theorem D.1(b) and get
Dkl(PθikPθ2 ) ≥ 2CC2W2(Pθl ,Pθ2 )2,
and the same holds with pθ1 and pθ2 swapped. As log pθ1 - log pθ2 ∈ F, by Lemma 4.3, we obtain
WF (Pθl ,Pθ2 ) ≥ Dkl(PθιkPθ2 ) + Dkl(Pθ2 kPθl ) ≥ C12 W2(Pθl ,Pθ2 )2 ≥ C12 W1(Pθl ,Pθ2
The last bound following from the fact that W2 ≥ W1 .
D.5 Proof of Lemma D.4
We are going to upper bound WF by the Wasserstein distances through Theorem D.2. Fix θ1, θ2 ∈
Θ. By definition ofF, it suffices to upper bound the Lipschitzness of logPθ (x) for all θ ∈ Θ. Recall
that
1	`-1
logPθ(x) = 2 (h`,diag(γ-2)h" + X (1d,logσ-1'(hk)) +C(θ),
'--------{z---------} k=1
I	X----------{----------}
II
where h1, . . . , h`(= z) are the hidden-layers of the inverse network z = Gθ-1(x), and C(θ) is a
constant that does not depend on x.
We first show the W2 bound. Clearly log Pθ (x) is differentiable in x. As θ ∈ Θ has norm bounds,
each layer hk is C(RW, Rb, k)-Lipschitz in x, so term II is altogether √dβσ Pk=I C(RW, Rb, k) =
C(RW, Rb,')√d-Lipschitz in x. For term I, note that h` is C -Lipschitz in x, so we have
Ek2 ≤ min1^2 kh'(x)k2 kVχh'(x)k2 ≤ %(Ckxk2 + h`(θ)) ∙ C ≤ C(1 + kxk2).
Putting together the two terms gives
IlV log pθ (X)Il2 ≤ J2 (1 + kxk2) + J2 √d ≤ J2 (kxk2 + √d).	(22)
Further, under either Pθ1 or Pθ2 (for example Pθ1 ), we have
Epθi [kXk2] ≤ E[kGθι(Z)k2] ≤ C(RW,Rb,')E[(kZk2 + 1)2] ≤ Cd.
Therefore we can apply Theorem D.2(c) and get
Epθ1[logPθ(x)] — Epθ2 [logPθ(x)] ≤ C (2√Cd + √d) W2(pθ1 ,Pθ2) ≤ Cδ2dW2(pθ1 ,pθ2).
We now turn to the W1 bound. The bound eq. (22) already implies that for kX k2 ≤ D,
,一 ,... C	C
kVlogPθ(x)k2 ≤ δ2(D + √d).
Choosing D = K√d, for a sufficiently large constant K, by the bound ∣∣X∣∣2 ≤C(kZk2+1)we
have the tail bound
P(kXk2 ≥D) ≤ exp(-20d).
On the other hand by the bound | logpθ(x)| ≤ C((∣∣x∣∣2 + 1)2∕δ2 + √d(kχ∣∣2 + 1)) we get under
either Pθ1 or Pθ2 (for example Pθ1 ) we have
一	CrC 「/ C L	2 21	Cd2
Epθι [(logPθ(X))2] ≤ δ4E[(kXk2 + √d(kXk2 + 1)) ] ≤ ɪ.
22
Published as a conference paper at ICLR 2019
Thus We can substitute D = K√d, L(D) = C(1 + K)√d∕δ2, M = Cd2/δ4, and Ptail(D)=
exp(-2d) into Theorem D.2(b) and get
Epθ1 [logPθ(x)] — Epθ2 [logPθ(x)] ≤ C(1 +K)VZdW1(Pθ1,Pθ2) + 4 JCdL exp(-20d)
≤ ~2^Γ~ (WI(Pθι,Pθ2) + √deχp(-10d)).
D.6 Proof of Lemma D.5
For any log-density neural netWork Fθ(x) = logPθ(x), reparametrize so that (Wi, bi) represent the
Weights and the biases of the inverse netWork z = Gθ-1(x). By eq. (21), this has the form
(Wi, bi) V^- (Wj-i+l, -W'-i+ib'-i+l), ∀i ∈ ['].
Consequently the reparametrized θ = (Wi, bi)i∈['] belongs to the (overloading Θ)
Θ= nθ=	(Wi,bi)i'=1	:	maxnkWikop,Wi-1opo ≤RW,	kbik2 ≤RWRb,	∀i∈	[`]o.	(23)
As F = {Fθ1 - Fθ2 : θ1, θ2 ∈ Θ}, the Rademacher complexity ofF is at most tWo times the quan-
tity
Rn := Rn({Fθ : θ ∈ Θ}, Pθ? ) = sup
θ∈Θ
We do one additional re-parametrization. Note that the log-density netWork Fθ(x) = logPθ (x) has
the form
Fθ(x) = Φγ(G-1(x))+log det dG∂X(X) +K(θ) = |〈h', diag(γ-2)h"+log
(24)
The constant C(θ) is the sum of the normalizing constant for Gaussian density (Which is the same
across all θ, and as We are taking subtractions of tWo log Pθ, We can ignore this) and the sum of
Iogdet(Wi), which is upper bounded by d'Rw. We can additionally create a parameter K =
K(θ) ∈ [0, d'Rw] for this term and let θ J (θ, K).
For any (reparametrized) θ, θ0 ∈ Θ, define the metric
ρ(θ,θ0) = max {kWi - Wi,kop , kbi - bik2 , |K - K01 ： i ∈ [d]}∙
Then we have, letting Yθ = * Pnn==ι εiFθ(Xi) denote the Rademacher process, the one-step dis-
cretization bound (Wainwright, 2018, Section 5).
1n
-V2 εiFθ (Xi)
n
i=1
det -ɪ +k (θ).
Rn ≤ E	SUp 廊-Y0| + E SUp	∣Yθi | .	(25)
θ,θ0∈Θ,ρ(θ,θ0)≤ε	θi ∈N (Θ,ρ,ε)
We deal with the two terms separately in the following two lemmas.
Lemma D.6 (Discretization error). There exists a constant C = C(RW, Rb, `) such that, for all
θ, θ0 ∈ Θ such that ρ(θ, θ0) ≤ ε, we have
C 1 JfL	C、
廊-Yθo∣≤ δ2 — X (√d(1 + kXik2) + kXik2) ∙ ε.
δ n i=1
Lemma D.7 (Expected max over a finite set). There exists constants λ0, C (depending on RW, Rb, `
but not d, δ) such that for all λ ≤ λ0δ2n,
E max M |
θi ∈N (Θ,ρ,ε)
Cd2 log max{RW，Rb}	Cd2λ
≤	λ	+ δ4n .
23
Published as a conference paper at ICLR 2019
Substituting the above two Lemmas into the bound eq. (25), we get that for all ε ≤ min {RW, Rb}
and λ ≤ λ0δ2n,
z
I
Cd2 log maχ {RW ,Rb}
λ
{z
II
Cd2 λ
+ F.
|
As X = Gθ? (Z), we have ∣∣X∣∣2 ≤ CIlZIl2 + ∣∣Gθ*(0)∣∣2 ≤ C(IlZ∣∣2 + 1) for some constant C > 0.
As E[∣∣Z∣∣2] = k + δ2(d 一 k) ≤ d, We have E[√d(1 + ∣∣X∣∣2) + ∣∣X|图 ≤ Cd for some constant C,
giving that I ≤ CCd ∙ ε. Choosing ε = c∕(dn) guarantees that I ≤ 志.For this choice of ε, term II
has the form
II ≤
Cd2 log(ndmax {Rw,Rb})	Cd2λ
+ F
Cd2 logn	Cd2λ
≤	+k
λ
the last bound holding if n ≥ d. Choosing λ = ʌ/n log n∕δ4, which will be valid if n/ log n ≥
δ-8λ0-2, we get
II ≤ Cd2
/log n _ C d4 log n
V l4n = CV	δ4n .
This term dominates term I and is hence the order of the generalization error.
D.6.1 Proof of Lemma D.6
Fix θ, θ0 such that ρ(θ, θ0) ≤ ε. As Yθ is the empirical average over n samples and ∣ε∕ ≤ 1, it
suffices to show that for any x ∈ Rd,
lFθ(X)- Fθ0(x)| ≤ C (√d(1 + llxb) + 11x112) ∙ε.
For the inverse network Gθ-1(x), let hk(x) ∈ Rd denote the k-th hidden layer:
hι(x) = σ(W1x+b1),…，h'-i(x) = σ(We-1h'-2(x)+b'-1), h'(x) = Weh'-1 (x)+b' = G-1(x).
Let h0k(x) denote the layers of Gθ-01(x) accordingly. Using this notation, we have
1	`-1
Fθ(x) = 2 (h`, diag(γ-2)h" + X (1d,logσ-1 (h®))+ K.
Lipschitzness of hidden layers We claim that for all k, we have
k
∣hk∣2 ≤ (RWκσ)k ∣x∣2 + Rb X(RWκσ)j,	(26)
j=1
and consequently when ρ(θ, θ0) ≤ ε, we have
∣∣hk 一 hk∣2 ≤ C(RW, Rb, k)ε(1 + 限||2), C(RW, Rb, k) = O (X j(RWκ)j(1 + Rb) j .
j=0	(27)
We induct on k to show these two bounds. For eq. (26), note that h0 = ∣x∣2 and
∣hk∣2 = ∣σ(Wkhk-1 + bk)∣2 ≤ κσ(RW ∣hk-1 ∣2 + RWRb),
so an induction on k shows the bound. For eq. (27), note that
∣h1 一 h01∣2 ≤ ∣W1 一 W10∣op ∣x∣2 + ∣b1 一 b01∣2 ≤ ε(1 + ∣x∣2),
24
Published as a conference paper at ICLR 2019
so the base case holds. Now, suppose the claim holds for the (k - 1)-th layer, then for the k-th layer
we have
Ilhk - hk ∣∣2	=	∣∣σ(Wk hk-1 + bk ) - σ(Wkhk-I + bk ) H2 ≤ κσ (IlWkhk-I- Wk hk-i∣l2 + ||bk - bk 11 2 )
≤ κσ ε + kWkkop	hk-1 -	h0k-12 +	Wk	- Wk0	op	h0k-12
≤
κσ ε + RWC(RW , Rb, k - 1)ε(1 + kxk2) + ε (RW κσ)k-1 kxk2 + Rb X(RWκσ)j
j=1
!!
ε(1 + kxk2)
≤
k-1
κσRWC(RW, Rb, k - 1) + (1 + Rb) X(RWκσ)j
j=1
"{^^^^^^~
C(RW,Rb,k)
verifying the result for layer k.
Dealing with (∙)2 and logσ-10 For the logσ-10 term, note that |(logσ-10)0| = ∣σ-1"/σ-101 ≤
βσ by assumption. So we have the Lipschitzness
`-1
X 1d, log σ-1 (hk) - log σ-1 (h0k)
k=1
'-1
≤√dβσ X Mk-hk k2
k=1
≤
'-1
√dβσ X C(RW, Rb, k) ∙ε(1 + kxk2) = C√d(1 + kxk2) ∙ ε.
k=1
'-------{z-------}
C
For the quadratic term, let Aγ = diag(γ-2) for shorthand. Using the bound (1/2)| hu, Aui -
hv, Avi | ≤ kAkop (kvk2 ku - vk2 + ku - vk22 /2), we get
2 lhh',Aγh'i - hh',Aγh'il ≤ kAγkop (kh'k2 kh' - h'k2 + kh' - h'k2 /2
≤ δ2 (C ∙ C(RW, Rb, ')ε(1 + ∣∣xk2)2 + C(RW, Rb, ')2ε2(1 + ∣∣xk2)2/2) ≤ j2 (I+ Ilxk2)2 ∙ ε
Putting together Combining the preceding two bounds and that |K - K0 | ≤ ε, we get
∣Fθ(x) - Fθ0(x)l ≤ (C∙(1 + kx∣2)2 + C√d(1 + kxk2) + 1) ε ≤ C(√d(1 + |团b)+ ∣x∣2) ∙ ε.
D.6.2 Proof of Lemma D.7
Tail decay at a single θ Fixing any θ ∈ Θ, we show that the random variable
1 n	1 n	`-1
Yθ = - XεiFθ(Xi) = — Xε hh'(g),Aγh'(xi)i + X (1d,logσ-10(hk(4)))+ K .
i=1	i=1	k=1
is suitably sub-exponential. To do this, it suffices to look at a single x and then use rules for inde-
pendent sums.
First, each 1d, log σ-1(hk(x)) is sub-Gaussian, with mean and sub-Gaussianity parameter
O(Cd). Indeed, we have
(1d,logσ-1(hfc(x)))= (1d,logσ-1(hfc(Gθ*(z)))) = (1d,logσ-1(hfc(&©*(2)))),
where Z = [zi：k,z(k+i)：d/6] is standard Gaussian. Note that (1) LiPschitzness of G is bounded
by that of G, which is some C(RW,Rb,'), (2) all hidden-layers are C(RW, Rb,')-Lipschitz
(see eq. (27)), (3) V → Pd=Ilogσ-1(v7-) is √dβσ-Lipschitz. Hence the above term is a C√d-
Lipschitz function of a standard Gaussian, so is Cd-sub-Gaussian by Gaussian concentration C.2.
To bound the mean, use the bound
E∣Dld, logσ-1(hk(Gθ*⑶)))| ≤ E[√dβσC ke∣2] ≤ Cd.
25
Published as a conference paper at ICLR 2019
As we have ` - 1 terms of this form, their sum is still Cd-sub-Gaussian with a O(Cd) mean (ab-
sorbing ` into C).
Second, the term hh`, AYh`i is a quadratic function of a SUb-GaUssian random vector, hence is sub-
exponential. Its mean is bounded by E[∣∣AγIlop ∣∣h'∣∣2] ≤ Cd∕δ2. Its sub-exponential parameter is
1∕δ2 times the sub-Gaussian parameter of h`, hence also Cd∕δ2. In particular, there exists a constant
λ0 > 0 such that for all λ ≤ λ0δ2,
E[exp(λ hh`, Aγh'i)] ≤ exp (Cdλ + C^) ∙
(See for example (Vershynin, 2010) for such results.) Also, the parameter K is upper bounded by
d'Rw = Cd.
Putting together, multiplying by εi (which addes up the squared mean onto the sub-Gaussianity /
sub-exponentiality and multiplies it by at most a constant) and summing over n, we get that Yθ is
mean-zero sub-exponential with the MGF bound
n
E[exp(λYθ)]
E exp (" {h'(xi),Aγh`(Xi)) + χ(1d, logσ-10(hk(Xi))E)))
≤
exp
(Cd2 λ2 ∖
(δ4n )
∀λ ≤ λ0δ2n.
(28)
Bounding the expected maximum We use the standard covering argument to bound the expected
maximum. Recall that ρ(θ, θ0 )
max IWi - Wi0 Iop , Ibi - b0i I2 , |K - K0 | . Hence, the cover-
ing number of Θ is bounded by the product of independent covering numbers, which further by the
volume argument is
N(Θ,P,ε) ≤ YY (1 + 2RW[.口 (l + 2Rb)d ∙ (l + CdRW
≤ exp (C	IWW + 'dlogW).
Using Jensen’s inequality and applying the bound appendix D.6.2, we get that for any λ ≤ λ0δ2n,
≤
E max	Yθ ≤ ɪ ( log E
θi ∈N (Θ,ρ,ε)	λ
Cd2 log max {RW R	Cd2 λ
-------------ε-------1------.
λ	δ4n
N (Θ,ρ,ε)
exp(λYθi)
i=1
log N(Θ, ρ, ε) +
Cd2λ2 ʌ
δ4n J
E Proofs for Section 4.2
E.1 Formal theorem statement
Towards stating the theorem more quantitatively, we will need to specify a few quantities of the
generator class that will be relevant for us.
First, for notational simplicity, we override the definition of pθβ by a truncated version of the convo-
lution of p and a Gaussian distribution. Concretely, let Dz = {z : ∣∣zk ≤ √dlog2 d,z ∈ Rk} be
a truncated region in the latent space (which contains an overwhelming large part of the probability
mass), and the let Dx = {x : ∃z ∈ Dz, ∣∣G(z) 一 χ∣∣2 ≤ β√dlog2 d} be the image of Dz under Gq.
Recall that
Gθ(z) =σ(Wlσ(Wl-1...σ(W1z+b1)+...bl-1)+bl).
Then, letpθβ(X) be the distribution obtained by adding Gaussian noise with variance β2 to a sample
from Gθ , and truncates the distribution to a very high-probability region (both in the latent variable
26
Published as a conference paper at ICLR 2019
and observable domain.) Formally, let pθβ be a distribution over Rd, s.t.
e-kzk2 e-
kGθ (Z)-Xk2
β2	dz,∀x ∈ Dx
(29)
For notational convenience, denote by f : Rk → R the function f (Z) = -Ilzk2-∣∣Gθ (Z) - x∣∣2 /β2,
and denote by z* a maximum of f.
Furthermore, whenever clear from the context, we will drop θ from pθ and Gθ .
We introduce several regularity conditions for the family of generators G :
Assumption E.1. We assume the following bounds on the partial derivatives of f: we denote S :=
maXz∈Dz"∣z-z*k≤δ ∣∣V2(∣Gθ(z)-x∣∣2)k, andλmin := maXz∈Dz"∣z-z*∣∣≤δ λmin(V2∣Gθ(z)-x∣2).
Similarly, we denote t(z) := k3 max∣ι∣=3 dkGθ∂Z∣-xk (z). and T = maXz=z∈Dz |t(z)].
We will denote by R an upper bound on the quantity Lr QL=i σmin(W7-) andby LG an upper bound
on the quantity LG := 2L Qi=I 公;印外.
Finally, we assume the inverse activationfunction is Lipschitz, namely ∣σ-1(x) 一 σ-1(y)∣ ≤ L@ |x —
y|.
Note on asymptotic notation: For notational convenience, in this section, ., &, as well as the
Big-Oh notation will hide dependencies on R, LG, S, T (in the theorem statements we intentionally
emphasize the polynomial dependencies on d.) The main theorem states that for certain F , dF
approximates the Wasserstein distance.
Theorem E.1. Suppose the generator class G satisfies the assumption E.1 and let F be the family
of functions as defined in Theorem E.2. Then, we have that for every p, q ∈ G,
Wι(p, q) . d∙F(p, q) .poly(d) ∙ Wι(p, q)1/6 + exp(-d).	(30)
Furthermore, when n & poly(d) we have
on R, LG, S, and T.
Rn(F, G) . poly(d) y⅞n
. Here . hides dependencies
The main ingredient in the proof will be the theorem that shows that there exists a parameterized
family F that can approximate the log density of pβ for every p ∈ G .
Theorem E.2. Let G satisfy the assumptions in Assumption E.1. For β = O(poly(1/d)), there
exists a family of neural networks F of size poly (1, d) such that for every distribution P ∈ G, there
exists N ∈ F satisfying:
(1)	N approximates logP for typical X: given input X = G(z*) + r, for ∣∣r∣ ≤ 10β√dlog d, and
kz*k ≤ 10√d log d for β = O(poly (1/d)) it outputs N (x), s.t.
|N (x) — log Pβ (x)| = Opoly(d) (β log(1∕β)) + exp(-d)
(2)	N is globally an approximate lower bound ofP: on any input X, N outputs N(X) ≤ log Pβ (X) +
Opoly (d)(β Iog(I/e))+exP(-d).
(3)	N approximates the entropy in the sense that: the output N(x) satisfies ∣EpβN(x) — H(pβ)| =
Opoly(d)(β Iog(I/e))+exP(—d)
Moreover, everyfunction in F has Lipschitz constant O(β4poly(d)).
The approach will be as follows: we will approximate pβ (X) essentially by a variant of Laplace’s
method of integration, using the fact that
pβ(X) = C
/
z∈Dz
e-kzk2-
kG(z)-xk2
β2
dz
for a normalization constant C that can be calculated up to an exponentially small additive factor.
When X is typical (in case (1) of Theorem E.2), the integral will mostly be dominated by it’s maxi-
mum value, which we will approximately calculate using a greedy “inversion” procedure.
When X is a atypical, it turns out that the same procedure will give a lower bound as in (2).
We are ready to prove Theorem E.1, assuming the correctness of Theorem E.2:
27
Published as a conference paper at ICLR 2019
Proof of Theorem E.1. By Theorem E.2, we have that there exist neural networks N1, N2 ∈ F that
approximate log pβ and log qβ respectively in the sense of bullet (1)-(3) in Theorem E.2. Thus we
have that by bullet (2) for distribution qβ , and bullet (3) for distribution qβ , we have
E [Nι(x) - N2(x)] ≥ E[logp] - E[logq] - O(βlog1∕β)	(31)
pβ	pβ	pβ
Similarly, we have
E [N2(x) - N1(x)] ≥ E [log q] - E [log p] - O(β log 1∕β)	(32)
qβ	qβ	qβ
Combining the equations above, setting f = N1(x) - N2(x), we obtain that
WF(pβ,qβ) ≥Epβ [f] -Eqβ [f] ≥ Dkl(pβkqβ) + Dkl(qβkpβ) - O(β log 1∕β)	(33)
Therefore, by definition, and Bobkov-Gotze theorem (Dkl(Pe, qβ) & Wι(pβ, qβ))
Wi(P, q) ≤ Wι(pβ, qβ) + O(β) . (Dkl(Pekqβ) + Dkl(qβkpβ))1/2 + O(β)
≤ (WF(pβ, qβ) + O(βlog(1∕β))1/2 + O(β) ≤ O(dF(P, q))	(34)
Thus we prove the lower bound.
Proceeding to the upper bound, notice that WF (Pe, qe). 表 W1(pe, qe) since every function in
F is O(Poly(d)*)-Lipschitz by Theorem E.2. We relate W1(Pβ, qe) to W1(P, q), more precisely
we prove: W1(Pe, qe) ≤ (1 + e-d)W1(P, q). Having this, we’d be done: namely, we simply set
β = W1/6 to get the necessary bound.
Proceeding to the claim, consider the optimal coupling C of P, q, and consider the induced coupling
Cz on the latent variable z in P, q. Then,
W1(P, q)
∕∈Rd kG(Z)- G(Z0)k1dCz (ζ,z0)det (T) aS (d⅞R
Consider the coupling CZ on the latent variables of Pe, qe, specified as Cz(z, z0) = C(z, z0)(1 -
Pr[z ∈∕ Dz])2. The coupling C ofPe, qe specified by coupling z’s according to Cz and the (trun-
cated) Gaussian noise to be the same in Pe , qe, we have that
W1(Pe,qe) ≤
/
z∈D
kG(z) - G(z0)k1dCz(z, z0)det (dG^) det (dGZz))
≤
z∈D
(1 + e—d)kG(Z)-G(z0)k1dCz (z,z0)det( T)det
(∂Gθ (Zr))
≤ (1 + e-d)W1(P, q)
The generalization claim follows completely analogously to Lemma D.5, using the Lipschitzness
bound of the generators in Theorem E.2.
□
The rest of the section is dedicated to the proof of Theorem E.2, which will be finally in Section E.3.
E.2 Tools and helper lemmas
First, we prove several helper lemmas:
LemmaE.3 (Quantitativebijectivity). IlG(Z) - G(Z)Il ≥ Lr η匕 σmin(W∙)∣∣Z - z∣∣.
Proof. The proof proceeds by reverse induction on l. We will prove that
1L
Ilhi- hik ≥ ~~i- Hσmin(Wj )∣∣z -Zll
Lσ j=i
28
Published as a conference paper at ICLR 2019
The claim trivial holds for i = 0, so we proceed to the induction. Suppose the claim holds for i.
Then,
IlWihi + bi - (Wihi + bi)k
1
≥ σm而khi-hik
and
.. , ~ ...
∣∣σ(Wihi + bi) - σ(Wihi + bi)k
L
≥ σm⅛)…k
by LiPschitzness of σ-1. Since hi-ι = σ(Wihi + bi) and h- = σ(Wihi + bi),
Ilhi-I- hi-1k ≥
1L
Ll-(-) ΠJmin(Wj )kz-
zI
as we need.
□
Lemma E.4 (APProximate inversion). Let x ∈ Rd be s.t. ∃z, IGθ (z) - xI ≤ . Then, there is a
neural network N ofsize O(ld2), activation function σ-1 andLipschitz constant Llσ Q；=i ：max(Wi)
which recovers a z, s.t. ∣∣z 一 Zk ≤ e2lLlσ Qii=ι σ ∙ ；w.)
Proof. N will iteratively Produce estimates hi , s.t.
(1) h0 = x
(2) hi = σ-1 (argminhkWih + bi - σ-1(hi-i)∣2)
We will prove by induction that |hi -h | ≤ e2L Qj=ι σ ∙ (W∙). The claim trivial holds for i = 0,
so we Proceed to the induction. SuPPose the claim holds for i. Then,
♦ IlTTT- 7	7	1^ I I , IlTTT- 7	7	——1∕f∖ll
min ∣Wi+1 h + bi+1 - hi ∣ ≤ ∣Wi+1 hi+1 + bi+1 - σ (hi)∣
h
=∣σ-1(hi) - σ-1(hi)k
≤ Lσ Ilhi- hik
i1
≤ *Lσ∣Y σmm(Wj)
where the last inequality holds by the inductive hypothesis, and the next-to-last one due to Lips-
chitzness of σ-1.
Hence, denoting h = argminhIWi+1h + bi+1 - σ-1(hi)I22, we have
~	. . .	~	1 ʌ	1 ʌ .
kWi+1h - Wi + 1hi+1)k = kWi+1h + bi+1 - σ	(hi) + σ (hi) - Wi+1hi+1 - bi+1∣∣
~	lʌ...	lʌ.
≤ k Wi+1h + bi+1 - σ	(hi) k + ∣∣σ (hi) - Wi+1hi+1 - bi+11∣
~	1 ʌ ...	1 ʌ	1 . ...
=kWi+1h + bi+1 - σ	(hi)k + ∣∣σ (hi) - σ	(Ai)Il
i1
≤…?σm⅛
This implies that
i1
∣Wi+ι(h - hi+ι)k ≤ 2eL；+1 ∏ —
j=1 σmin (Wj )
which in turns means
i+1
kh - hi+ι∣≤，…j∏ EJ
which completes the claim.
29
Published as a conference paper at ICLR 2019
Turning to the size/Lipschitz constant of the neural network: all we need to notice is that hi =
σ 1(W- (hi-ι 一 bi)), which immediately implies the LiPschitzness/size bound by simple induction.
□
We also introduce a few tools to get a handle on functions that can be approximated efficiently by
neural networks of small size/Lipschitz constant.
Lemma E.5 (Composing Lipschitz functions). If f : Rd2 → Rd3 and g : Rd1 → Rd2 are L, K-
Lipschitz functions respectively, then f ◦ g : Rd1 → Rd3 is LK -Lipschitz.
Proof. The proof follows by definition essentially:
kf (g(X))- f (g(χ0))k ≤ LIlg(X)- g(χ0)k ≤ LKIIx - X0k
□
Lemma E.6 (Calculating singular value decomposition approximately, (Demmel et al., 2007)).
There is a neural network with size O(n3poly(log(1/))) that given a symmetric matrix A ∈ Rn×n
with minimum eigenvalue gap mι□i=j ∣λi — λj | ≥ δ and eigenvectors {ui} outputs {ui, λi} s.t.:
(1) ∣hUi,Uji∣ ≤ e,∀i = j and IIuik = I ± 匕
(2) Iui- ui| ≤ "δ, ∣λi - λi∣ ≤ g∀i.
(Note the eigenvalue/eigenvector pairs for A are unique since the minimum eigenvalue gap is non-
zero).
Lemma E.7 (Backpropagation, (Rumelhart et al., 1986)). Given a neural network f : Rm → R
of depth l and size N, there is a neural network of size O(N + l) which calculates the gradient
∂,i ∈ H.
E.3 Proof of Theorem E.2
We will proceed to prove the two parts one at a time.
First, we prove the following lemma, which can be seen as a quantitative version of Laplace’s method
for evaluating integrals:
Lemma E.8 (“Tail” bound for integral at z*). Let X = G(z*) + r, for ∣r∣ ≤ 10β√dlog d, and
∣∣z*k ≤ 10σ√dlogd. The, for β =O(Poly(1/d)), and
δ =100β log(1∕β)坐
R
it holds that
/
Jz:||z-z*
k>δ,z∈Dz
ef(z)dz ≤ β
z∈Dz
ef(z)dz
Proof. Let’s write
/
z∈D
ef(z)dz = /：kz-z*-(Z)dZ + ∕kz-z*k>δ,z∈jz)dz
(35)
To prove the claim of the Lemma, we will lower bound the first term, and upper bound the latter,
from which the conclusion will follow.
Consider the former term. Taylor expanding in a neighborhood around z* We have
f (z) = f (z*) + (z - z*)>V2f (z*)(z - z*) ± T- kz - z*k3
β2
where the first-order term vanishes since z* is a global optimum. Furthermore, V-f (z*) 0 for the
same reason. Hence, by Taylor’s theorem with remainder bounds, and using the fact that ex ≥ 1 +X,
we have
∕kz-z*k≤δ …≥ (1- β- δ3)f(z*) Lk
(z - z*)>V-f (z*)(z - z*)
z-z*k≤δ
30
Published as a conference paper at ICLR 2019
The integral on the right is nothing more than the (unnormalized) cdf of a Gaussian with covariance
matrix (-V2/(z*))-1.
Moreover, -V2f (z*) is positive definite with smallest eigenvalue bounded by R, since
-V2f(z*) = V2(∣∣z*k2) + -2-V2(∣∣G(z*) - xk2)
P2
=I +-12 V2(kG(z*)-xk2)
P2
and
V2(∣∣G(z*) - x∣∣2)占 E VGi(z*)V>Gi(z*) + £(Gi(z) - XNGi⑺
where Gi is the i-th coordinate of G. We claim Pi VGi(z*)V>Gi(z*)占 RI, and ∣∣ Pi(Gi(Z)-
Xi)V2Gi(z)∣2 . P√dlog d. The latter follows from the bound on r and Cauchy-Schwartz. For the
former, note that we have
v
T
(X VGi(z*) VTGi(Z*)
V = E(v, VGi(Z*)i2
i
X l⅛
i
Gi(z* + CV) — Gi(z*)
€
ByLemmaE.3, ∣G(z* + EV) - G(z*)∣ ≥ Re, so ∃i, s.t. ∣Gi(z* + EV) - Gi(z*)∣ ≥ √. Hence,
( lim
乙 ∖e→0
i
Gi(z* + ev) — Gi(z*)
€
R2
≥——
_ d
from which Pi VGi(z*)VTGi(Z*)占 R follows.
Using standard Gaussian tail bounds, since δ ≥ β⅛R≡ √d,wehave
/ (Z-
Jz: ||z _z* ∣∣≤δ
z*)tV2∕(z*)(z - z*) ≥ (2 - P)det(4π(-V2f(Z*)))1/2
(36)
We proceed to the latter term in eq. (35). We have
f(Z*)- f (Z)
kZ*k2 + kG(Z*)-xk
P
∣∣G(z*) — x
2	…2	kG(Z) - xk2
——∣z∣	P2
k2	∣G(z) - xk2
P2	P2
∣∣G(z*)- G(Z*)-r∣∣2
P2
-IkZll2-∣∣Z*k2∣
kG(Z) - G(Z*) + rk2
P2
-IkZk2-kZ*k2∣
≥
2
—
—
Φ(RkZF- W -与-IkZk2-kZ*k21
® (RkZ-Z*k-krk)2
_	P2
沙-kZ-Z*k2- 2∣z - Z
P2
*kkZ*k
—
where (T) follows from the definition of x, and G) by triangle inequality.
Note also that ∣RkZ - z* k ≥ ∣∣rk since δ ≥ 2R∣, which in turn implies
(RkZ-Z*k-krk)2
P2
krk2	„	3 R2
-* -kz - z*∣2 - 2∣z - z* kkz*k ≥ 1-6炉∣z - z*∣
31
Published as a conference paper at ICLR 2019
Finally, β1 2 3 ≤ R∣∣z*k, so that it follows
kz - z*k2 (13Rβ22 - l) — 2∣z - z*kkz*k ≤ 券kz - z*k2
Putting these estimates together, We get f (Z) < f (z*) 一 3R2∣∣z 一 z*∣2, which implies
∕kz-z*k>δ …≤ eg L
e-舞 kz-z*k2
z-z* k>δ
The integral on the right is again the unnormalized Cdf of a Gaussian with covariance matrix 3R21,
so by Gaussian tail bounds again, and using that the smallest eigenvalue of -V2f (z*) is lower-
R2
bounded by R we have
/
z:kz-z* k>δ
ef(z) ≤ βef(ZDdet(4π(-V2f(z*)))1/2
as we wanted.
Putting this together with eq. (36), we get the statement of the theorem.
□
With this in mind, we can prove part (1) of our Theorem E.2 restated below:
Theorem E.9. There is a neural network N ofsize poly(ɪ, R, Lg, T, S, d) with LiPschitz constant
O (poly (R, Lg, d, T, S), 1∕β4 5 6 7 8) which given as input X = G(z*) + r ,for ∣∣r∣ ≤ 10β√d log d,and
∣∣z*k ≤ 10√dlogdfor β = O(poly(1∕d)) outputs N(x), s.t.
|N(x) - logPie(x)| = OPOly⑷(βlog(1∕β)) +exp(-d)
Proof. It suffices to approximate
ef(z)dz
(37)
up to a multiplicative factor of 1 ± Opoly(d)(β log(1∕β)), since the normalizing factor satisfies
x∈Dx	z∈Dze-kzk2e
kGθ (Z)-Xk2
一β ― dz
(1 ± exp(-d))det(4πI )-1∕2det(4π∕β2I )-1/2

We will first present the algorithm, then prove that it:
(1)	Approximates the integral as needed.
(2)	Can be implemented by a small, Lipschitz network as needed.
The algorithm is as follows:
Algorithm 1 Discriminator family with restricted approximability for degenerate manifold
1: Parameters: Matrices E1 , E2 , . . . , Er ∈ R, matrices W1, W2 , . . . , Wl .
2: Let δ = 100 β Rog(1/e) and let S be the trivial β2-net of the matrices with spectral norm bounded
by O(1∕β2).
3: Let Z = NinV (x) be the output of the “invertor” circuit of Lemma E.4.
4: Calculate g = Vf (Z), H = V2f (Z) by the circuit implied in Lemma E.7.
5: Let M be the nearest matrix in S to H and Ei, i ∈ [r] be s.t. M + Ei has Ω(β)-separated
eigenvalues. (If there are multiple Ei that satisfy the separation condition, pick the smallest i.)
6: Let (ei , λi ) be approximate eigenvector/eigenvalue pairs of H + Ei calculated by the circuit
implied in Lemma E.6.
7: Approximate Ii = log(R|c ∣≤δ ecihei,gi+Pi c2λideə , i ∈ [r] by subdividing the interval (0, δ)
into intervals of size β2 and evaluating the resulting Riemannian sum instead of the integral.
8: Output Pi Ii .
32
Published as a conference paper at ICLR 2019
First, we will show (1), namely that the Algorithm 1 approximates the integral of interest. We’ll
use an approximate version of Lemma E.8 - with a slightly different division of where the “bulk”
of the integral is located. As in Algorithm 1, let Z = Ninv(X) be the output of the “invertor" circuit
of Lemma E.4. and let δ = 100d βRlog(I/e) and denote by B the set B = {z : |(z - Z, e%)| ≤
δ}. Furthermore, let’s define how the matrices Ei, i ∈ [r] are to be chosen. Let S be an β2-
net of the matrices with spectral norm bounded by O(1∕β2). We claim that there exist matrices
Ei, E2,..., Er, r = Ω(dlog(1∕β)), s.t. if M ∈ S, at least one of the matrices M + Ei, i ∈ [r]
has eigenvalues that are Ω(β)-separated and ∣∣Ei∣∣2 ≤ 斗.Indeed, let let E be a random Gaussian
matrix with entrywise variance 含.By Theorem 2.6 in (Nguyen et al., 2017), for any fixed matrix
A, with probability 3/4, mini ∣λi(A + E) - λi+ι(A + E)| = Ω(β). The number of matrices in S
is bounded by 2O(dlog(1/e)), so Ei, i ∈ [r] exist by the probabilistic method.
We can write the integral of interest as
/
z∈B
f(z)
dz +
/一
ZzBB
ef(z)dz
e

Note that
kz-z*k = kz- z + Z-z*k
≥kz- Zk-∣∣Z-z*k
、4LG Il ^
≥ ~Rkz-
zk
This means that {z : ∣∣z - z* k ≤ 4RGδ} ⊆ B, so by Lemma E.8, we have
/
J z∈B
ef(z) ≤ O(β)
z
ef(z)dz
which means that to prove the statement of the Theorem, it suffices for us to approximate
R	ef(z) dz.
z∈B
Consider the former term first. By Taylor’s theorem with remainder, expanding f in a δ-
neighborhood near Z, we get
T
f (z) = f (Z) + (z - Z)>Vf (Z) + (z - Z)>V2f (Z)(z - Z) ± β kz - Zk3
For notational convenience, same as in Algorithm 1, let,s denote by H := (Z - Z)> V2f (Z)(Z - Z),
and g := Vf (Z). Steps 5-8 effectively perform a change ofbasis in the eigenbasis of H and evaluate
the integral in this basis - however to ensure Lipschitzness (which we prove later on), we will need
to perturb H slightly.
Let M be the closest matrix to V2f (Z) in the β2-net S and let ei be approximate eigenvectors of
H = M + Ei in the sense of Lemma E.6, s.t. the eigenvalues of M + Ei are Ω(β)-separated.
Since ∣∣Eik ≤ √d, we have
|(z - Z)>H(z - Z) - (z - Z)>H(z - Z)| = O(β)
Hence,
T
f (z) = f (Z) + (z - Z)>Vf(Z) + (z - Z)>H(z - Z) ± -2kz - Zk3 ± O(β)
β2
Towards rewriting f in the approximate basis ei, let Z - Z = Ei Ciei for some scalars g. We have
T
f(Z) =f(z) + Ecihei,gi+Eci% ± 京(Ec2) /
β
ii	i
33
Published as a conference paper at ICLR 2019
By Taylor’s theorem with remainder,ex = 1+ X ± e X22 ,ifx < 1. Hence,
Z ef (z)dz = (1 ± ed3/2 Trδ)3∖ ef(Z) Y [	ecihe^g+P.。/
Jz∈B	β	β2 )	几 ∣≤δ
Calculating the integral by subdividing (0, δ) into intervals of size β 2, and approximating the integral
by the accompanying Riemannian sum, and taking into account 同 = O(β) and λi, kgk = O(含),
we get a multiplicative approximation of
/
∣ci∣≤δ
ecihei,gi+Pi ci2λi
of the order eβ = 1 + O(β ), which is what we want.
We turn to implementing the algorithm by a small neural network with good Lipschitz constant. Both
the Lipschitz constant and the size will be handled by the composition Lemma E.5 and analyzing
each step of Algorithm 1. Steps 3 and 4 are handled by our helper lemmas: the invertor circuit
by Lemma E.4 has LiPsChitz constant Lg； calculating the Hessian V2f (Z) can be performed by a
polynomially sized neural network by Lemma E.7 and since the third partial derivatives are bounded,
so the output of this network is O(差)-Lipschitz as well. We turn to the remaining steps.
Proceeding to the eigendecomposition, by Lemma E.6, we can perform an approximate eigende-
composition of H with a network of size O(POly (d)) -so we only need to handle the Lipschitzness.
We will show that the result of Steps 5-6, the vectors uj and scalars λj are Lipschitz functions of H.
Suppose that H and H0 are s.t. kH - H0 k ≤β 2 first. Then, H, H0 are mapped to the same
matrix M in S, {uj } and {u0j } are the eigenvectors of H + Ei and H0 + Ei for some i ∈ [r].
First, by Weyl’s theorem, since H + Ei = M + (H - M) + Ei, the eigenvalues of H + Ei are
Ω(β) — ∣∣H 一 Mk = Ω(β)-separated. Furthermore, since H0 + Ei = H + Ei + (H0 一 H), by
Wedin,s theorem, ∣∣u7-(H0 +	Ei)	-	Uj(H	+	Ei)∣	= O	(∣H	-	H0∣21).If, on the other hand,
∣H 一 H0∣ >β	2, and Ea, Eb are the perturbation matrices used for H and H0, since the eigenvalues
of H + Ea are Ω(β)-separated, by Wedin,s theorem,
∣Uj(H0 + Eb)-Uj(H + Ea)k ≤ O (β∙ (∣H - H0∣2 + ∣Eb - Eak2))=O Q∣H - H01。
so We get that the map to the vectors Uj is O(1∕β)-Lipschitz. A similar analysis shows that the map
to the eigenvalues λj is also Lipschitz.
Finally, we move to calculating the integral in Step 7: the trivial implementation of the integral by a
neural network has size O(poly(β)) and as a function of e%, g, λi is O(1∕β)-Lipschitz, which proves
the statement of the theorem.
□
Moving to Part (2) of Theorem E.2, we prove:
Theorem E.10. Let N be the neural network N used in Theorem E.9. The network additionally
satisfies N(x) ≤ logp(x) + θR,LG,d(β log(1∕β)) + exp(-d), ∀x ∈ D.
Proof. Recalling the proof of Theorem E.9, and reusing the notation there, we can express
q(x) =	exp(f(z)) =	ef(z)dz +	ef(z)dz
Zz	JzBB	J z∈B
Since the neural network N ignores the latter term, and we need only produce an upper bound, it
suffices to show that N approximates
z
ef(z)dz
kζ-Zk≤δ
34
Published as a conference paper at ICLR 2019
UP to a multiplicative factor of 1 - O(β log(1∕β)). However, if We consider the proof of The-
orem E.9, we notice that the approximation consider there indeed serves our purpose: Taylor-
expanding same as there, we have
e ef (z)dz =(1 ± ed3/2Teδ3) ef ⑶ Y [	…用什P "
Jz∈B	i '≤δ
This integral can be evaluated in the same manner as in Theorem E.9, as our bound on Tβ holds
universally on neighborhood of radius Dx.
□
Finally, part (3) follows easily from (1) and (2):
Proofof Part 3 of Theorem E.2. For points x, s.t. @z, ∣∣G(z) — Xk ≤ 10β√d log d, it holds that
p(x) = O(exp(-d)). On the other hand, by the Lipschitzness of N, we have kN(x)k = Oɪ (Ilxl∣).
β4
Since X ∈ Dx implies ∣∣χ∣ = O(poly(d)) the claim follows.	□
F Experiments on Synthetic 2d Datasets: Unit Circle
1.0
0.5
0.0
-0.5
-1.0
-1.0	-0.5	0.0	0.5	1.0
uωlsJcυSSEM
(a) Iteration 500.	(b) Iteration 10000.	(c) Comparing IPM and Wasserstein.
Figure 2: Experiments on the unit circle dataset. The neural net IPM, the Wasserstein distance, and the sample
quality are correlated along training. (a)(b): Sample batches from the ground truth and the learned generator at
iteration 500 and 10000. (c): Comparing the F-IPM and the Wasserstein distance. RealG and fakeG denote the
ground truth generator and the learned generator, respectively.
G Experiments on Invertible Neural Net Generators
We further perform synthetic WGAN experiments with invertible neural net generators (cf. Sec-
tion 4.1) and discriminators designed with restricted approximability (Lemma 4.1). In this case, the
invertibility guarantees that the KL divergence can be computed, and our goal is to demonstrate that
the empirical IPM WF (p, q) is well correlated with the KL-divergence between p and q on synthetic
data for various pairs of p and q (The true distribution p is generated randomly from a ground-truth
neural net, and the distribution q is learned using various algorithms or perturbed version of p.)
G.1 Setup
Data The data is generated from a ground-truth invertible neural net generator (cf. Section 4.1),
i.e. X = Gθ(Z), where Gθ : Rd → Rd is a '-layer layer-wise invertible feedforward net, and Z
is a spherical Gaussian. We use the Leaky ReLU with negative slope 0.5 as the activation function
σ, whose derivative and inverse can be very efficiently computed. The weight matrices of the layers
are set to be well-conditioned with singular values in between 0.5 to 2.
We choose the discriminator architecture according to the design with restricted approximability
guarantee (Lemma 4.1, eq. (10) eq. (11)). As log σ-10 is a piecewise constant function that is not
35
Published as a conference paper at ICLR 2019
differentiable, we instead model it as a trainable one-hidden-layer neural network that maps reals to
reals. We add constraints on all the parameters in accordance with Assumption 1.
Training To train the generator and discriminator networks, we generate stochastic batches (with
batch size 64) from both the ground-truth generator and the trained generator, and solve the min-max
problem in the Wasserstein GAN formulation. We perform 10 updates of the discriminator in be-
tween each generator step, with various regularization methods for discriminator training (specified
later). We use the RMSProp optimizer (Tieleman & Hinton, 2012) as our update rule.
Evaluation metric We evaluate the following metrics between the true and learned generator.
(1)	The KL divergence. As the density of our invertible neural net generator can be analytically
computed, we can compute their KL divergence from empirical averages of the difference of
the log densities:
, , _ __ , _______________________________________________ _ ___________
Dkl(P ,p) = EX〜Cn[logP (X) - logP(X)],
where p? and p are the densities of the true generator and the learned generator. We regard the
KL divergence as the “correct” and rather strong criterion for distributional closeness.
(2)	The training loss (IPM WF train). This is the (unregularized) GAN loss during training. Note:
as typically in the training of GANs, we balance carefully the number of steps for discriminator
and generators, the training IPM is potentially very far away from the true WF (which requires
sufficient training of the discriminators).
(3)	The neural net IPM (WF eval). We report once in a while a separately optimized WGAN loss
in which the learned generator is held fixed and the discriminator is trained from scratch to
optimality. Unlike the training loss, here the discriminator is trained in norm balls but with no
other regularization. By doing this, we are finding f ∈ F that maximizes the contrast and we
regard the f found by stochastic optimization an approximate maximizer, and the loss obtained
an approximation of WF .
Our theory shows that for our choice of G and F , WGAN is able to learn the true generator in
KL divergence, and the F -IPM (in evaluation instead of training) should be indicative of the KL
divergence. We test this hypothesis in the following experiments.
G.2 Convergence of generators in KL divergence
In our first experiment, G is a two-layer net in d = 10 dimensions. Though the generator is only a
shallow neural net, the presence of the nonlinearity makes the estimation problem non-trivial. We
train a discriminator with the architecture specified in Lemma 4.1), using either Vanilla WGAN
(clamping the weight into norm balls) or WGAN-GP (Gulrajani et al., 2017) (adding a gradient
penalty). We fix the same ground-truth generator and run each method from 6 different random
initializations. Results are plotted in Figure 3.
Our main findings are two-fold:
(1)	WGAN training with discriminator design of restricted approximability is able to learn the true
distribution in KL divergence. Indeed, the KL divergence starts at around 10 - 30 and the best
run gets to KL lower than 1. As KL is a rather strong metric between distributions, this is strong
evidence that GANs are finding the true distribution and mode collapse is not happening.
(2)	The WF (eval) and the KL divergence are highly correlated with each other, both along each
training run and across different runs. In particular, adding gradient penalty improves the opti-
mization significantly (which we see in the KL curve), and this improvement is also reflected
by the WF curve. Therefore the quantity WF can serve as a good metric for monitoring conver-
gence and is at least much better than the training loss curve.
To test the necessity of the specific form of the discriminator we designed, we re-do the same ex-
periment with vanilla fully-connected discriminator nets. Results (in Appendix G.4) show that IPM
with vanilla discriminators also correlate well with the KL-divergence. This is not surprising from
36
Published as a conference paper at ICLR 2019
Figure 3: Learning an invertible neural net generator on synthetic data. The x-axis in all the graphs indicates
the number of steps. The left-most figure shows the KL-divergence between the true distribution p and learned
distribution q at different steps of training, the middle the estimated IPM (evaluation) between p and q, and the
right one the training loss. We see that the estimated IPM in evaluation correlates well with the KL-divergence.
Moving average is applied to all curves.
a theoretical point of view because a standard fully-connected discriminator net (with some over-
parameterization) is likely to be able to approximate the log density of the generator distributions
(which is essentially the only requirement of Lemma 4.3.)
For this synthetic case, we can see that the inferior performance in KL of the WGAN-Vanilla algo-
rithm doesn’t come from the statistical properties of GANs, but rather the inferior training perfor-
mance in terms of the convergence of the IPM. We conjecture similar phenomenon occurs in training
GANs with real-life data as well.
G.3 Perturbed generators
Figure 4: Scatter plot of KL divergence and neural net IPM on perturbed generator pairs. Cor-
relation between log(Dkl(pkq) + Dkl(qkp)) and log WF is 0.7315. Dashed line is WF (p, q) =
100(Dkl(pkq) + Dkl(qkp)).
In this section, we remove the effect of the optimization and directly test the correlation between
p and its perturbations. We compare the KL divergence and neural net IPM on pairs of perturbed
generators. In each instance, we generate a pair of generators (G, G0) (with the same architecture
as above), where G0 is a perturbation of G by adding small Gaussian noise. We compute the KL
divergence and the neural net IPM between G and G0 . To denoise the unstable training process for
computing the neural net IPM, we optimize the discriminator from 5 random initializations and pick
the largest value as the output.
As is shown in Figure 4, there is a clear positive correlation between the (symmetric) KL divergence
and the neural net IPM. In particular, majority of the points fall around the line WF = 100Dkl,
which is consistent with our theory that the neural net distance scales linearly in the KL divergence.
37
Published as a conference paper at ICLR 2019
Note that there are a few outliers with large KL. This happens mostly due to the perturbation being
accidentally too large so that the weight matrices become poorly conditioned - in the context of our
theory, they fall out of the good constraint set as defined in Assumption 1.
G.4 Experiments with vanilla discriminator
G.4. 1 Convergence of generators in KL divergence
We re-do the experiments of Section G.2 with vanilla fully-connected discriminator nets. We use a
three-layer net with hidden dimensions 50-10, which has more parameters than the architecture with
restricted approximability. Results are plotted in Figure 5. We find that the generators also converge
well in the KL divergence, but the correlation is slightly weaker than the setting with restricted
approximability (correlation still presents along each training run but weaker across different runs).
This suggests that vanilla discriminator structures might be practically quite satisfying for getting a
good generator, though specific designs may help improve the quality of the distance WF .
Figure 5: Learning an invertible neural net generator on synthetic data with vanilla fully-connected discrim-
inator nets. The x-axis in all the graphs indicates the number of steps. The left-most figure shows the KL-
divergence between the true distribution p and learned distribution q at different steps of training, the middle
the estimated IPM (evaluation) between p and q, and the right one the training loss. We see that the estimated
IPM in evaluation correlates well with the KL-divergence. Moving average is applied to all curves.
G.4.2 Perturbed generators
Correlation between KL and neural net IPM is computed with vanilla fully-connected discriminators
and plotted in Figure 6. The correlation (0.7489) is roughly the same as for discriminators with
restricted approximability (0.7315).
Figure 6: Scatter plot of KL divergence and neural net IPM (with vanilla discriminators) on per-
turbed generator pairs. Correlation between log(Dkl(pkq) + Dkl(qkp)) and log WF is 0.7489.
Dashed line is WF(p, q) = 3pDki(p∣∣q) + Dki(q∣[p).
38