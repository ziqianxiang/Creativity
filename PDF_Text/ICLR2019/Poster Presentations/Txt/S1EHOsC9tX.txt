Published as a conference paper at ICLR 2019
Towards the first adversarially robust
neural network model on MNIST
Lukas Schott1-3*, JonasRauber1-3*, MatthiasBethge1,3,4* & WielandBrendel1,3*
1Centre for Integrative Neuroscience, University of Tubingen
2International Max Planck Research School for Intelligent Systems
3Bernstein Center for Computational Neuroscience Tubingen
4Max Planck Institute for Biological Cybernetics
*Joint first authors
*Joint senior authors
firstname.lastname@bethgelab.org
Ab stract
Despite much effort, deep neural networks remain highly susceptible to tiny input
perturbations and even for MNIST, one of the most common toy datasets in com-
puter vision, no neural network model exists for which adversarial perturbations
are large and make semantic sense to humans. We show that even the widely
recognized and by far most successful L∞ defense by Madry et al. (1) has lower
L0 robustness than undefended networks and is still highly susceptible to L2 per-
turbations, (2) classifies unrecognizable images with high certainty, (3) performs
not much better than simple input binarization and (4) features adversarial perturba-
tions that make little sense to humans. These results suggest that MNIST is far from
being solved in terms of adversarial robustness. We present a novel robust classifi-
cation model that performs analysis by synthesis using learned class-conditional
data distributions. We derive bounds on the robustness and go to great length to
empirically evaluate our model using maximally effective adversarial attacks by (a)
applying decision-based, score-based, gradient-based and transfer-based attacks
for several different Lp norms, (b) by designing a new attack that exploits the
structure of our defended model and (c) by devising a novel decision-based attack
that seeks to minimize the number of perturbed pixels (L0). The results suggest
that our approach yields state-of-the-art robustness on MNIST against L0, L2 and
L∞ perturbations and we demonstrate that most adversarial examples are strongly
perturbed towards the perceptual boundary between the original and the adversarial
class.
1	Introduction
Deep neural networks (DNNs) are strikingly susceptible to minimal adversarial perturbations
(Szegedy et al., 2013), perturbations that are (almost) imperceptible to humans but which can switch
the class prediction of DNNs to basically any desired target class.
One key problem in finding successful defenses is the difficulty of reliably evaluating model robust-
ness. It has been shown time and again (Athalye et al., 2018; Athalye & Carlini, 2018; Brendel &
Bethge, 2017) that basically all defenses previously proposed did not increase model robustness but
prevented existing attacks from finding minimal adversarial examples, the most common reason
being masking of the gradients on which most attacks rely. The few verifiable defenses can only
guarantee robustness within a small linear regime around the data points (Hein & Andriushchenko,
2017; Raghunathan et al., 2018).
The only defense currently considered effective (Athalye et al., 2018) is a particular type of adversarial
training (Madry et al., 2018). On MNIST, as of today this method is able to reach an accuracy of
88.79% for adversarial perturbations with an L∞ norm bounded by = 0.3 (Zheng et al., 2018). In
other words, if we allow an attacker to perturb the brightness of each pixel by up to 0.3 (range [0, 1]),
1
Published as a conference paper at ICLR 2019
then he can only trick the model on ≈ 10% of the samples. This is a great success, but does the
model really learn more causal features to classify MNIST? We here demonstrate that this is not the
case: For one, the defense by Madry et al. (SOTA on L∞) has lower L0 robustness than undefended
networks and is still highly susceptible in the L2 metric. Second, the robustness results by Madry
et al. can also be achieved with a simple input quantization because of the binary nature of single
pixels in MNIST (which are typically either completely black or white) (Schmidt et al., 2018). Third,
it is straight-forward to find unrecognizable images that are classified as a digit with high certainty.
Finally, the minimum adversarial examples we find for the defense by Madry et al. make little to no
sense to humans.
Taken together, even MNIST cannot be considered solved with respect to adversarial robustness. By
“solved” we mean a model that reaches at least 99% accuracy (see accuracy-vs-robustness trade-off
(Tsipras et al., 2018; Bubeck et al., 2018)) and whose adversarial examples carry semantic meaning
to humans (by which we mean that they start looking like samples that could belong to either class).
Hence, despite the fact that MNIST is considered “too easy” by many and a mere toy example, finding
adversarially robust models on MNIST is still an open problem.
A potential solution we explore in this paper is inspired by unrecognizable images (Nguyen et al.,
2015) or distal adversarials. Distal adversarials are images that do not resemble images from the
training set but which typically look like noise while still being classified by the model with high
confidence. It seems difficult to prevent such images in feedforward networks as we have little control
over how inputs are classified that are far outside of the training domain. In contrast, generative
models can learn the distribution of their inputs and are thus able to gauge their confidence accordingly.
By additionally learning the image distribution within each class we can check that the classification
makes sense in terms of the image features being present in the input (e.g. an image of a bus should
contain actual bus features). Following this line of thought from an information-theoretic perspective,
one arrives at the well-known concept of Bayesian classifiers. We here introduce a fine-tuned variant
based on variational autoencoders (Kingma & Welling, 2013) that combines robustness with high
accuracy.
In summary, the contributions of this paper are as follows:
•	We show that MNIST is unsolved from the point of adversarial robustness: the SOTA defense of
Madry et al. (2018) is still highly vulnerable to tiny perturbations that are meaningless to humans.
•	We introduce a new robust classification model and derive instance-specific robustness guarantees.
•	We develop a strong attack that leverages the generative structure of our classification model.
•	We introduce a novel decision-based attack that minimizes L0 .
•	We perform an extensive evaluation of our defense across many attacks to show that it surpasses
SOTA on L0, L2 and L∞ and features many adversarials that carry semantic meaning to humans.
We have evaluated the proposed defense to the best of our knowledge, but we are aware of the
(currently unavoidable) limitations of evaluating robustness. We will release the model architecture
and trained weights as a friendly invitation to fellow researchers to evaluate our model independently.
2	Related Work
The many defenses against adversarial attacks can roughly be subdivided into four categories:
•	Adversarial training: The training data is augmented with adversarial examples to make models
more robust (Madry et al., 2018; Szegedy et al., 2013; Tramer et al., 2017; Ilyas et al., 2017).
•	Manifold projections: An input sample is projected onto a learned data manifold (Samangouei
et al., 2018; Ilyas et al., 2017; Shen et al., 2017; Song et al., 2018).
•	Stochasticity: Certain inputs or hidden activations are shuffled or randomized (Prakash et al.,
2018; Dhillon et al., 2018; Xie et al., 2018).
•	Preprocessing: Inputs or hidden activations are quantized, projected into a different representation
or are otherwise preprocessed (Buckman et al., 2018; Guo et al., 2018; Kabilan et al., 2018).
There has been much work showing that basically all defenses suggested so far in the literature do not
substantially increase robustness over undefended neural networks (Athalye et al., 2018; Brendel &
2
Published as a conference paper at ICLR 2019
I. Optimize latent diStribution p(z|x) in eaCh digit model
to find likelihood of Sample x under eaCh model.
0 apo!ΛI
p(class | X )
LOGITS
II. DeCide baSed on moSt likely ClaSS
Forward
Gradient
0
1
2
3
4
5
6
7
8
9
6 apo!ΛI
一SCaIeO ―>>
mod. Softmax
Figure 1: Overview over model architecture. In a nutshell: I) for each sample x we compute a lower
bound on the log-likelihood (ELBO) under each class using gradient descent in the latent space. II) A
class-dependent scalar weighting of the class-conditional ELBOs forms the final class prediction.
Bethge, 2017). The only widely accepted exception according to Athalye et al. (2018) is the defense
by Madry et al. (2018) which is based on data augmentation with adversarials found by iterative
projected gradient descent with random starting points. However, as we see in the results section,
this defense is limited to the metric it is trained on (L∞) and it is straight-forward to generate small
adversarial perturbations that carry little semantic meaning for humans.
Some other defenses have been based on generative models. Typically these defenses use the gener-
ative model to project onto the (learned) manifold of “natural” inputs. This includes in particular
DefenseGAN (Samangouei et al., 2018), Adversarial Perturbation Elimination GAN (Shen et al.,
2017) and Robust Manifold Defense (Ilyas et al., 2017), all of which project an image onto the mani-
fold defined by a generator network G. The generated image is then classified by a discriminator in
the usual way. A similar idea is used by PixelDefend (Song et al., 2018) which uses an autoregressive
probabilistic method to learn the data manifold. Other ideas in similar directions include the use of
denoising autoencoders (Liao et al., 2017) as well as MagNets (Meng & Chen, 2017), which projects
or rejects inputs depending on their distance to the data manifold. All of these proposed defenses
except for the defense by Ilyas et al. (2017) have been tested by Athalye et al. (2018); Athalye &
Carlini (2018); Carlini & Wagner (2017) and others, and shown to be ineffective. It is straight-forward
to understand why: For one, many adversarials still look like normal data points to humans. Second,
the classifier on top of the projected image is as vulnerable to adversarial examples as before. Hence,
for any data set with a natural amount of variation there will almost always be a certain perturbation
against which the classifier is vulnerable and which can be induced by the right inputs.
We here follow a different approach by modeling the input distribution within each class (instead of
modeling a single distribution for the complete data), and by classifying a new sample according to
the class under which it has the highest likelihood. This approach, commonly referred to as a Bayesian
classifier, gets away without any additional and vulnerable classifier. A very different but related
approach is the work by George et al. (2017) which suggested a generative compositional model of
digits to solve cluttered digit scenes like Captchas (adversarial robustness was not evaluated).
3	Model description
Intuitively, We want to learn a causal model of the inputs (Scholkopf, 2017). Consider a cat: We
want a model to learn that cats have four legs and two pointed ears, and then use this model to check
Whether a given input can be generated With these features. This intuition can be formalized as
folloWs. Let (x, y) With x ∈ RN be an input-label datum. Instead of directly learning a posterior
p(y|x) from inputs to labels We noW learn generative distributions p(x|y) and classify neW inputs
using Bayes formula,
p(y∣χ) = P(Xyxp(y) (Xp(χ∣y)p(y).	(i)
The label distribution p(y) can be estimated from the training data. To learn the class-conditional
sample distributions p(x∣y) we use variational autoencoders (VAEs) (Kingma & Welling, 2013).
VAEs estimate the log-likelihood logp(x) by learning a probabilistic generative model pθ(x|z)
3
Published as a conference paper at ICLR 2019
with latent variables Z 〜p(z) and parameters θ (See Appendix A.3 for the full derivation). For
class-conditional VAEs we can derive a lower bound on the log-likelihood log p(x|y) as
logp(χ∣y) ≥ EZ〜qφ(z∣χ,y) [logPθ(χ∣z,u)] -DKL [qφ(z|x,y)||p(z)] =: 'y(x),	(2)
where p(z) = N(0,1) is a simple normal prior and qφ(z∣x, y) is the variational posterior with
parameters φ. The first term on the RHS is basically a reconstruction error while the second term
on the RHS is the mismatch between the variational and the true posterior. The term on the RHS is
the so-called evidence lower bound (ELBO) on the log-likelihood (Kingma & Welling, 2013). We
implement the conditional distributions pθ(x|z, y) and qφ(z∣x, y) as normal distributions for which
the means are parametrized as DNNs (all details and hyperparameters are reported in Appendix A.7).
Our Analysis by Synthesis model (ABS) is illustrated in Figure 1. It combines several elements to
simultaneously achieve high accuracy and robustness against adversarial perturbations:
•	Class-conditional distributions: For each class y we train a variational autoencoder VAEy on
the samples of class y to learn the class-conditional distribution p(x|y). This allows us to estimate
a lower bound 'y (x) on the log-likelihood of sample X under each class y.
•	Optimization-based inference: The variational inference qφ(z∣x, y) is itself a neural network
susceptible to adversarial perturbations. We therefore only use variational inference during
training and perform “exact” inference over pθ (X|z, y) during evaluation. This “exact” inference
is implemented using gradient descent in the latent space (with fixed posterior width) to find the
optimal zy which maximizes the lower bound on the log-likelihood for each class:
'y(x) = max logPθ(x∣z,y)-Dkl [N(z,σq 1)||N(0,1)].	(3)
Note that we replaced the expectation in equation 2 with a maximum likelihood sample to avoid
stochastic sampling and to simplify optimization. To avoid local minima we evaluate 8000 random
points in the latent space of each VAE, from which we pick the best as a starting point for a
gradient descent with 50 iterations using the Adam optimizer (Kingma & Ba, 2014).
•	Classification and confidence: Finally, to perform the actual classification, we scale all 'y (x)
with a factor α, exponentiate, add an offset η and divide by the total evidence (like in a softmax),
p(y∣χ) = (eɑ'Kx) + η) / XC (eα'c(X) + η) .	(4)
We introduced η for the following reason: even on points far outside the data domain, where
all likelihoods q(x, y) = eα'(x) + η are small, the standard softmax (η = 0) can lead to sharp
posteriors p(y|X) with high confidence scores for one class. This behavior is in stark contrast
to humans, who would report a uniform distribution over classes for unrecognizable images.
To model this behavior we set η > 0: in this case the posterior p(y|X) converges to a uniform
distribution whenever the maximum q(X, y) gets small relative to η . We chose η such that the
median confidence p(y|X) is 0.9 for the predicted class on clean test samples. Furthermore, for
a better comparison with cross-entropy trained networks, the scale α is trained to minimize the
cross-entropy loss. We also tested this graded softmax in standard feedforward CNNs but did not
find any improvement with respect to unrecognizable images.
•	Binarization (Binary ABS only): The pixel intensities of MNIST images are almost binary. We
exploit this by projecting the intensity b of each pixel to 0 if b < 0.5 or 1 if b ≥ 0.5 during testing.
•	Discriminative finetuning (Binary ABS only): To improve the accuracy of the Binary ABS
model we multiply 'y (x) with an additional class-dependent scalar Yy. The scalars are learned
discriminatively (see A.7) and reach values in the range γy ∈ [0.96, 1.06] for all classes y.
On important ingredient for the robustness of the ABS model is the Gaussian posterior in the
reconstruction term which ensures that small changes in the input (in terms of L2) can only entail
small changes to the posterior likelihood and thus to the model decision.
4 Tight Estimates of the Lower B ound for Adversarial Examples
The decision of the model depends on the likelihood in each class, which for clean samples is mostly
dominated by the posterior likelihood p(X|z). Because we chose this posterior to be Gaussian, the
4
Published as a conference paper at ICLR 2019
class-conditional likelihoods can only change gracefully with changes in x, a property which allows
us to derive lower bounds on the model robustness. To see this, note that equation 3 can be written as,
12
睢x)=max -DKL [N(zq 1)||N(0,1)] — —2 ∣∣G°(z) - x∣∣2 + C,	(5)
c	z	2σ2
where we absorbed the normalization constants of p(x|z) into C and Gc(z) is the mean of p(x|z, c).
Let y be the ground-truth class and let Zx be the optimal latent for the clean sample X for class y. We
can then estimate a lower bound on 'y (x + δ) for a perturbation δ with size E = ∣∣δk2 (see derivation
in Appendix A.4),
'y (x+δ) ≥ 'y(X) — σ2 E kGy (Zx) - xk2- 2σ2 e2+ C.	(6)
Likewise, We can derive an upper bound of 'y(x + δ) for all other classes C = y (see Appendix A.5),
'y(x + δ) ≤-Dkl [N(0,σq 1)∣∣N(0,1)] + C — 1212 (dc — E)2 Ifdc ≥ E.	⑺
0	else
for dc = minz kGc(z) — xk2. Now we can find E for a given image x by equating (7) = (6),
min max 0,
c6=y
dc + 'y(x) — DKL [N(0, σq 1)∣∣N(0,1)]
2(dc + kGy(Zx)- x∣2)
(8)
Note that one assumption we make is that we can find the global minimum of kGc(z) — xk22. In
practice we generally find a very tight estimate of the global minimum (and thus the lower bound)
because we optimize in a smooth and low-dimensional space and because we perform an additional
brute-force sampling step. We provide quantitative values for E in section 7.
5	Adversarial Attacks
Reliably evaluating model robustness is difficult because each attack only provides an upper bound
on the size of the adversarial perturbations (Uesato et al., 2018). To make this bound as tight
as possible we apply many different attacks and choose the best one for each sample and model
combination (using the implementations in Foolbox v1.3 (Rauber et al., 2017) which often perform
internal hyperparameter optimization). We also created a novel decision-based L0 attack as well as a
customized attack that specifically exploits the structure of our model. Nevertheless, we cannot rule
out that more effective attacks exist and we will release the trained model for future testing.
Latent Descent attack This novel attack exploits the structure of the ABS model. Let xt be the
perturbed sample X in iteration t. We perform variational inference p(z∣xt, y) = N(μy (xt), σqI) to
find the most likely class y that is different from the ground-truth class. We then make a step towards
the maximum likelihood posterior p(x∣z, y) of that class which We denote as xy,
xt → (1 — E)Xt + Exy.	(9)
We choose E = 10-2 and iterate until we find an adversarial. For a more precise estimate we perform
a subsequent binary search of 10 steps within the last E interval. Finally, we perform another binary
search between the adversarial and the original image to reduce the perturbation as much as possible.
Decision-based attacks We use several decision-based attacks because they do not rely on gradient
information and are thus insensitive to gradient masking or missing gradients. In particular, we
apply the Boundary Attack (Brendel et al., 2018), which is competitive with gradient-based attacks
in minimizing the L2 norm, and introduce the Pointwise Attack, a novel decision-based attack that
greedily minimizes the L0 norm. It first adds salt-and-pepper noise until the image is misclassified and
then repeatedly iterates over all perturbed pixels, resetting them to the clean image if the perturbed
image stays adversarial. The attack ends when no pixel can be reset anymore. We provide an
implementation of the attack in Foolbox (Rauber et al., 2017). Finally, we apply two simple noise
attacks, the Gaussian Noise attack and the Salt&Pepper Noise attack as baselines.
5
Published as a conference paper at ICLR 2019
(a) L2 distance
(b) L∞ distance
(c) L0 distance
Figure 2: Accuracy-distortion plots for each distance metric and all models. In (b) we see that a
threshold at 0.3 favors Madry et al. while a threshold of 0.35 would have favored the Binary ABS.
Transfer-based attacks Transfer attacks also don’t rely on gradients of the target model but instead
compute them on a substitute: given an input x we first compute adversarial perturbations δ on the
substitute using different gradient-based attacks (L2 and L∞ Basic Iterative Method (BIM), Fast
Gradient Sign Method (FGSM) and L2 Fast Gradient Method) and then perform a line search to find
the smallest for which x + δ (clipped to the range [0, 1]) is still an adversarial for the target model.
Gradient-based attacks We apply the Momentum Iterative Method (MIM) (Dong et al., 2017) that
won the NIPS 2017 adversarial attack challenge, the Basic Iterative Method (BIM) (Kurakin et al.,
2016) (also known as Projected Gradient Descent (PGD))—for both the L2 and the L∞ norm—as
well as the Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2014) and its L2 variant, the Fast
Gradient Method (FGM). For models with input binarization (Binary CNN, Binary ABS), we obtain
gradients using the straight-through estimator (Bengio et al., 2013).
Score-based attacks We additionally run all attacks listed under Gradient-based attacks using
numerically estimated gradients (possible for all models). We use a simple coordinate-wise finite
difference method (NES estimates (Ilyas et al., 2018) performed comparable or worse) and repeat the
attacks with different values for the step size of the gradient estimator.
Postprocessing (binary models only) For models with input binarization (sec. 6) we postprocess
all adversarials by setting pixel intensities either to the corresponding value of the clean image or the
binarization threshold (0.5). This reduces the perturbation size without changing model decisions.
6	Experiments
We compare our ABS model as well as two ablations—ABS with input binarization during test time
(Binary ABS) and a CNN with input binarization during train and test time (Binary CNN)—against
three other models: the SOTA L∞ defense (Madry et al., 2018)1, a Nearest Neighbour (NN) model
(as a somewhat robust but not accurate baseline) and a vanilla CNN (as an accurate but not robust
baseline), see Appendix A.7. We run all attacks (see sec. 5) against all applicable models.
For each model and Lp norm, we show how the accuracy of the models decreases with increasing
adversarial perturbation size (Figure 2) and report two metrics: the median adversarial distance
(Table 1, left values) and the model’s accuracy against bounded adversarial perturbations (Table 1,
right values). The median of the perturbation sizes (Table 1, left values) is robust to outliers and
summarizes most of the distributions quite well. It represents the perturbation size for which the
particular model achieves 50% accuracy and does not require the choice of a threshold. Clean samples
that are already misclassified are counted as adversarials with a perturbation size equal to 0, failed
attacks as ∞. The commonly reported model accuracy on bounded adversarial perturbations, on the
other hand, requires a metric-specific threshold that can bias the results. We still report it (Table 1,
right values) for completeness and set L2 = 1.5, L∞ = 0.3 and L0 = 12 as thresholds.
1We used the trained model provided by the authors: https://github.com/MadryLab/mnist_challenge
6
Published as a conference paper at ICLR 2019
	CNN	Binary CNN	Nearest Neighbor	Madry et al.	Binary ABS	ABS
Clean	99.1%	98.5%	96.9%	98.8%	99.0%	99.0%
L2-metric ( = 1.5)						
Transfer Attacks	1.1 / 14%	1.4 / 38%	5.4 / 90%	3.7 / 94%	2.5 / 86%	4.6 / 94%
Gaussian Noise	5.2 / 96%	3.4 / 92%	∞ / 91%	5.4 / 96%	5.6 / 89%	10.9 / 98%
Boundary Attack	1.2 / 21%	3.3 / 84%	2.9 / 73%	1.4 / 37%	6.0 / 91%	2.6 / 83%
Pointwise Attack	3.4 / 91%	1.9 / 71%	3.5 / 89%	1.9 / 71%	3.1 /86%	4.6 / 94%
FGM	1.4 / 48%	1.4 / 50%		∞ / 96%		
FGM w/ GE	1.4 / 42%	2.8 / 51%	3.7 / 79%	∞ / 88%	1.9 / 68%	3.5 / 89%
DeepFool	1.2 / 18%	1.0 / 11%		9.0 / 91%		
DeepFool w/ GE	1.3 / 30%	0.9/ 5%	1.6 / 55%	5.1 /90%	1.4 / 41%	2.4 / 83%
L2 BIM	1.1 / 13%	1.0 / 11%		4.8 /88%		
L2 BIM w/GE	1.1 /37%	∞ / 50%	1.7 / 62%	3.4 / 88%	1.6 / 63%	3.1 /87%
Latent Descent Attack					2.6 / 97%	2.7 / 85%
All L2 Attacks	1.1 / 8%	0.9/ 3%	1.5 / 53%	1.4 / 35%	1.3 / 39%	2.3 / 80%
L∞-metric ( = 0.3)						
Transfer Attacks	0.08 / 0%	0.44 / 85%	0.42 / 78%	0.39 /92%	0.49 /88%	0.34 / 73%
FGSM	0.10 / 4%	0.43 / 77%		0.45 /93%		
FGSM w/ GE	0.10 / 21%	0.42 / 71%	0.38 / 68%	0.47 / 89%	0.49 /85%	0.27 / 34%
L∞ DeepFool	0.08 / 0%	0.38 / 74%		0.42 /90%		
L∞ DeepFool w/ GE	0.09 / 0%	0.37 / 67%	0.21 /26%	0.53 / 90%	0.46 / 78%	0.27 / 39%
BIM	0.08 / 0%	0.36 / 70%		0.36 / 90%		
BIM w/ GE	0.08 / 37%	∞ / 70%	0.25 / 43%	0.46 /89%	0.49 /86%	0.25 / 13%
MIM	0.08 / 0%	0.37 / 71%		0.34 / 90%		
MIM w/ GE	0.09 / 36%	∞ / 69%	0.19 / 26%	0.36 / 89%	0.46 / 85%	0.26 / 17%
All L∞ Attacks	0.08 / 0%	0.34 / 64%	0.19 / 22%	0.34 / 88%	0.44 / 77%	0.23 / 8%
L0-metric ( = 12)						
Salt&Pepper Noise	44.0 / 91%	44.0 / 88% 161.0/ 88%		13.5 /56% 146.0/ 94% 165.0/ 94%		
Pointwise Attack 10x	9.0 / 19%	11.0 / 39%	10.0 / 34%	4.0 / 0%	22.0 / 77%	16.5 / 69%
All L0 Attacks	9.0 / 19%	11.0 / 38%	10.0 / 34%	4.0 / 0%	21.5 / 77%	16.5 / 69%
Table 1: Results for different models, adversarial attacks and distance metrics. Each entry shows the
median adversarial distance across all samples (left value, black) as well as the model’s accuracy
against adversarial perturbations bounded by the thresholds L2 = 1.5, L∞ = 0.3 and L0 = 12
(right value, gray). “w/ GE” indicates attacks that use numerical gradient estimation.
7	Results
Minimal Adversarials Our robustness evaluation results of all models are reported in Table 1 and
Figure 2. All models except the Nearest Neighbour classifier perform close to 99% accuracy on clean
test samples. We report results for three different norms: L2 , L∞ and L0.
•	For L2 our ABS model outperforms all other models by a large margin.
•	For L∞ , our Binary ABS model is state-of-the-art in terms of median perturbation size. In terms
of accuracy (perturbations < 0.3), Madry et al. seems more robust. However, as revealed by the
accuracy-distortion curves in Figure 2, this is an artifact of the specific threshold (Madry et al. is
optimized for 0.3). A slightly larger one (e.g. 0.35) would strongly favor the Binary ABS model.
•	For L0 , both ABS and Binary ABS are much more robust than all other models. Interestingly, the
model by Madry et al. is the least robust, even less than the baseline CNN.
In Figure 3 we show adversarial examples. For each sample we show the minimally perturbed L2
adversarial found by any attack. Adversarials for the baseline CNN and the Binary CNN are almost
7
Published as a conference paper at ICLR 2019
CNN
Binary Nearest Madry Binary
CNN Neighbor et al. ABS
ABS
6 2-7 7 9
→Jr → TkT → →
b Z ? f q
6 4 - 7 7 9
→/ →
3 f 2 3 q
8 4-8 2 9
∖z → → -‹→→ T
b。Z 21 月
6 7-8 7 9
→y → → r → →
O / Z 彳 X
6 7 3 7 7
∖z → TkT → →
O / 2 3 旦
I 6 4 Jo 9 7
b。Z 2 M 司
Binary Nearest Madry Binary
CNN Neighbor et al. ABS
CNN
8 0 3 9
T ; T : T T
J 〃7夕
ABS
Figure 3: Adversarial examples for the ABS models are perceptually meaningful: For each sample
(randomly chosen from each class) we show the minimally perturbed L2 adversarial found by any
attack. Our ABS models have clearly visible and often semantically meaningful adversarials. Madry
et al. requires perturbations that are clearly visible, but their semantics are less clear.
imperceptible. The Nearest Neighbour model, almost by design, exposes (some) adversarials that
interpolate between two numbers. The model by Madry et al. requires perturbations that are clearly
visible but make little semantic sense to humans. Finally, adversarials generated for the ABS models
are semantically meaningful for humans and are sitting close to the perceptual boundary between the
original and the adversarial class. For a more thorough comparison see appendix Figures 5, 6 and 7.
Lower bounds on Robustness For the ABS models and the L2 metric we estimate a lower bound
of the robustness. The lower bound for the mean perturbation2 for the MNIST test set is =
0.690 ± 0.005 for the ABS and = 0.601 ± 0.005 for the binary ABS. We estimated the error by
using different random seeds for our optimization procedure and standard error propagation over
10 runs. With adversarial training Hein & Andriushchenko (2017) achieve a mean L2 robustness
guarantee of = 0.48 while reaching 99% accuracy. In the Linf metric we find a median robustness
of 0.06.
Distal Adversarials We probe the behavior of CNN,
Madry et al. and our ABS model outside the data distri-
bution. We start from random noise images and perform
gradient ascent to maximize the output probability of
a fixed label until p(y∣x) ≥ 0.9 (as computed by the
modified softmax from equation (8)). The results are
visualized in Figure 4. Standard CNNs and Madry et al.
Figure 4: Images of ones classified with a
probability above 90%.
provide high confidence class probabilities for unrecognizable images. Our ABS model does not
provide high confidence predictions in out-of-distribution regions.
8	Discussion & Conclusion
In this paper we demonstrated that, despite years of work, we as a community failed to create neural
networks that can be considered robust on MNIST from the point of human perception. In particular,
we showed that even today’s best defense is susceptible to small adversarial perturbations that make
little to no semantic sense to humans. We presented a new approach based on analysis by synthesis
that seeks to explain its inference by means of the actual image features. We performed an extensive
analysis to show that minimal adversarial perturbations in this model are large across all tested Lp
norms and semantically meaningful to humans. Note that our architecture derives its robustness from
its design and does not require any additionally training with adversarial examples.
We acknowledge that it is not easy to reliably evaluate a model’s adversarial robustness and most
defenses proposed in the literature have later been shown to be ineffective. In particular, the structure
2The mean instead of the median is reported to allow for a comparison with (Hein & Andriushchenko, 2017).
8
Published as a conference paper at ICLR 2019
of the ABS model prevents the computation of gradients which might give the model an unfair
advantage. We put a lot of effort into an extensive evaluation of adversarial robustness using a large
collection of powerful attacks, including one specifically designed to be particularly effective against
the ABS model (the Latent Descent attack), and we will release the model architecture and trained
weights as a friendly invitation to fellow researchers to evaluate our model.
Looking at the results of individual attacks (Table 1) we find that there is no single attack that
works best on all models, thus highlighting the importance for a broad range of attacks. Without
the Boundary Attack, for example, Madry et al. would have looked more robust to L2 adversarials
than it is. For similar reasons Figure 6b of Madry et al. (2018) reports a median L2 perturbation size
larger than 5, compared to the 1.4 achieved by the Boundary Attack. Moreover,the combination of all
attacks of one metric (All L2 / L∞ / L0 Attacks) is often better than any individual attack, indicating
that different attacks are optimal on different samples.
Our conceptual implementation of the ABS model with one VAE per class neither scales efficiently
to more classes nor to more complex datasets (a preliminary experiment on CIFAR10 provided only
54% test accuracy). However, first experiments on two class CIFAR indicate that the proposed model
is also robust on CIFAR (we reach a median L2 robustness of 2.6 compared to 0.8 for a vanilla CNN,
see Appendix A.1) for details). To increase the accuracy, there are many ways in which the ABS
model can be improved, ranging from better and faster generative models (e.g. flow-based) to better
training procedures.
In a nutshell, we demonstrated that MNIST is still not solved from the point of adversarial robustness
and showed that our novel approach based on analysis by synthesis has great potential to reduce the
vulnerability against adversarial attacks and to align machine perception with human perception.
Acknowledgments
This work has been funded, in part, by the German Federal Ministry of Education and Research
(BMBF) through the Bernstein Computational Neuroscience Program Tubingen (FKZ: 01GQ1002) as
well as the German Research Foundation (DFG CRC 1233 on “Robust Vision”). The authors thank the
International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting L.S. and
J.R.; J.R. acknowledges support by the Bosch Forschungsstiftung (Stifterverband, T113/30057/17);
W.B. was supported by the Carl Zeiss Foundation (0563-2.8/558/3); M.B. acknowledges support by
the Centre for Integrative Neuroscience Tubingen (EXC 307); W.B. and M.B. were supported by
the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior / Interior
Business Center (DoI/IBC) contract number D16PC00003.
References
Anish Athalye and Nicholas Carlini. On the robustness of the cvpr 2018 white-box adversarial example defenses.
arXiv preprint arXiv:1804.03286, 2018.
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security:
Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018.
Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients through stochastic
neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.
W. Brendel and M. Bethge. Comment on “biologically inspired protection of deep networks from adversarial
attacks”. arXiv preprint arXiv:1704.01547, 2017.
Wieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-based adversarial attacks: Reliable attacks
against black-box machine learning models. In International Conference on Learning Representations, 2018.
URL https://openreview.net/forum?id=SyZI0GWCZ.
SCbastien Bubeck, Eric Price, and Ilya Razenshteyn. Adversarial examples from computational constraints.
arXiv preprint arXiv:1805.10204, 2018.
Jacob Buckman, Aurko Roy, Colin Raffel, and Ian Goodfellow. Thermometer encoding: One hot way to
resist adversarial examples. In International Conference on Learning Representations, 2018. URL https:
//openreview.net/forum?id=S18Su--CW.
9
Published as a conference paper at ICLR 2019
Nicholas Carlini and David Wagner. Magnet and" efficient defenses against adversarial attacks" are not robust to
adversarial examples. arXiv preprint arXiv:1711.08478, 2017.
Djork-Arn6 Clevert, Thomas Unterthiner, and SePP Hochreiter. Fast and accurate deep network learning by
exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.
Guneet S. Dhillon, Kamyar Azizzadenesheli, Jeremy D. Bernstein, Jean Kossaifi, Aran Khanna, Zachary C.
LiPton, and Animashree Anandkumar. Stochastic activation Pruning for robust adversarial defense. In Inter-
national Conference on Learning Representations, 2018. URL https://openreview.net/forum?
id=H1uR4GZRZ.
YinPeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Xiaolin Hu, Jianguo Li, and Jun Zhu. Boosting
adversarial attacks with momentum. arxiv PrePrint. arXiv preprint arXiv:1710.06081, 2017.
Dileep George, Wolfgang Lehrach, Ken Kansky, Miguel Ldzaro-Gredilla, Christopher Laan, Bhaskara Marthi,
Xinghua Lou, Zhaoshi Meng, Yi Liu, Huayan Wang, Alex Lavin, and D. Scott Phoenix. A generative vision
model that trains with high data efficiency and breaks text-based captchas. Science, 358(6368), 2017. ISSN
0036-8075. doi: 10.1126/science.aag2612. URL http://science.sciencemag.org/content/
358/6368/eaag2612.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples.
arXiv preprint arXiv:1412.6572, 2014.
Chuan Guo, Mayank Rana, Moustapha Cisse, and Laurens van der Maaten. Countering adversarial images
using input transformations. In International Conference on Learning Representations, 2018. URL https:
//openreview.net/forum?id=SyJ7ClWCb.
Matthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness of a classifier against
adversarial manipulation. In Advances in Neural Information Processing Systems 30, pp. 2266-2276. Curran
Associates, Inc., 2017.
Andrew Ilyas, Ajil Jalal, Eirini Asteri, Constantinos Daskalakis, and Alexandros G Dimakis. The robust manifold
defense: Adversarial training using generative models. arXiv preprint arXiv:1712.09196, 2017.
Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adversarial attacks with limited
queries and information. arXiv preprint arXiv:1804.08598, 2018.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Vishaal Munusamy Kabilan, Brandon Morris, and Anh Nguyen. Vectordefense: Vectorization as a defense to
adversarial examples. arXiv preprint arXiv:1804.08529, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,
2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world. arXiv preprint
arXiv:1607.02533, 2016.
Fangzhou Liao, Ming Liang, Yinpeng Dong, Tianyu Pang, Jun Zhu, and Xiaolin Hu. Defense against adversarial
attacks using high-level representation guided denoiser. arXiv preprint arXiv:1712.02976, 2017.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep
learning models resistant to adversarial attacks. In International Conference on Learning Representations,
2018. URL https://openreview.net/forum?id=rJzIBfZAb.
Dongyu Meng and Hao Chen. Magnet: a two-pronged defense against adversarial examples. In Proceedings of
the 2017 ACM SIGSAC Conference on Computer and Communications Security, pp. 135-147. ACM, 2017.
Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High confidence
predictions for unrecognizable images. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2015.
Aaditya Prakash, Nick Moran, Solomon Garber, Antonella DiLillo, and James Storer. Deflecting adversarial
attacks with pixel deflection. arXiv preprint arXiv:1801.08926, 2018.
10
Published as a conference paper at ICLR 2019
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial examples.
In International Conference on Learning Representations, 2018. URL https://openreview.net/
forum?id=Bys4ob-Rb.
Jonas Rauber, Wieland Brendel, and Matthias Bethge. Foolbox: A python toolbox to benchmark the robustness
of machine learning models. arXiv preprint arXiv:1707.04131, 2017. URL http://arxiv.org/abs/
1707.04131.
Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-GAN: Protecting classifiers against adversarial
attacks using generative models. In International Conference on Learning Representations, 2018. URL
https://openreview.net/forum?id=BkJ3ibb0-.
Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Adversarially
robust generalization requires more data. CoRR, abs/1804.11285, 2018. URL http://arxiv.org/abs/
1804.11285.
Bernhard Scholkopf. Causal learning, 2017. URL https://icml.cc/Conferences/2017/
Schedule?showEvent=931. Thirty-fourth International Conference on Machine Learning.
Shiwei Shen, Guoqing Jin, Ke Gao, and Yongdong Zhang. Ape-gan: Adversarial perturbation elimination with
gan. arXiv preprint arXiv:1707.05474, 2017.
Yang Song, Taesup Kim, Sebastian Nowozin, Stefano Ermon, and Nate Kushman. Pixeldefend: Leveraging
generative models to understand and defend against adversarial examples. In International Conference on
Learning Representations, 2018. URL https://openreview.net/forum?id=rJUYGxbCW.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob
Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Florian Tram3r, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. Ensemble
adversarial training: Attacks and defenses. arXiv preprint arXiv:1705.07204, 2017.
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. There is no
free lunch in adversarial robustness (but there are unexpected benefits). arXiv preprint arXiv:1805.12152,
2018.
Jonathan Uesato, Brendan O’Donoghue, Pushmeet Kohli, and Aaron van den Oord. Adversarial risk and the
dangers of evaluating against weak attacks. In Proceedings of the 35th International Conference on Machine
Learning, 2018. URL http://proceedings.mlr.press/v80/uesato18a.html.
Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan Yuille. Mitigating adversarial effects
through randomization. In International Conference on Learning Representations, 2018. URL https:
//openreview.net/forum?id=Sk9yuql0Z.
Tianhang Zheng, Changyou Chen, and Kui Ren. Distributionally adversarial attack. arXiv preprint
arXiv:1808.05537, 2018.
11
Published as a conference paper at ICLR 2019
A Appendix
A.1 TWO CLASS CIFAR
We estimate the robustness of our ABS model on two class CIFAR (airplane vs. automobile). Preliminary results
suggest that our robustness is not limited to MNIST.
In order to adapt to CIFAR, we modified the ABS slightly by modifying encoder and decoder to fit (32x32x3)
CIFAR images. We also increased the number of dimensions in the latent space form 8 to 20.
Model	CNN	ABS
Accuracy	97.1%	=	89.7%
Median L2 distance	0.8 (With BIM)	2.5 (with Latent Descent attack)
Table 2: Accuracy and estimated robustness on two class CIFAR.
A.2 Figures
CNN
--UBnσI
Binary
CNN
Madry
et al.
ABS
--UBnσz
--UBnσ-①I-UBnστ
Binary
ABS
Figure 5:	L0 error quantiles: We always choose the minimally perturbed L0 adversarial found by
any attack for each model. For an unbiased selection, we then randomly sample images within four
error quantiles (0 - 25%, 25 - 50%, 50 - 75%, and 75 - 100%). Where 100% corresponds to the
maximal (over samples) minimum (over attacks) perturbation found for each model.
12
Published as a conference paper at ICLR 2019
ABS
--UBnσ= ①I-UBnσz--UBnσm ①I-UBnστ
CNN
中二€
9 4
I → →
3q
- 卜
∙qo S-
8 S
s』£
Binary	Nearest	Madry
CNN	Neighbor	et al.
Binary
ABS
4 4 I C q
→ → ⅛Γ → 一
Cqaq
0 5 7 4
. → I— → Ir → →
43 0⅛Jr
4 8 __ 9 2
U 3g。/
7 5 ■ 8 S
→ r → : ∙ → < - 一
IKViS
z5s:o
5 7 I 4 2
1 → → ⅝,→1
8rv
6 7 8 7
→ → F → 一
CIYI
0 7 3 7
G 0 8Z
Z->
二 一
2 1
8 a
V S
3 3 7 E g C I 6 A
Sf TY OJGzy5
73 39 23 85
.1 → → → → → 「 τ→一
5,5 q 1 5 F 3
7©

◎ 2
Figure 6:	L? error quantiles: We always choose the minimally perturbed L? adversarial found by any
attack for each model. For an unbiased selection, We then randomly sample 4 images within four
error quantiles (0 - 25%, 25 - 50%, 50 - 75%, and 75 - 100%).
Nearest
Neighbor
h
4支
CNN
Binary
CNN
--UBnσ= ①I-UBnσz
,__________
--UBnσm--UBnστ
I》5
Eq4
0-6 2→1
∣4d
6-2 0-5
Madry	Binary
et al.	ABS
ABS
Figure 7:	L∞ error quantiles: We always choose the minimally perturbed L∞ adversarial found by
any attack for each model. For an unbiased selection, we then randomly sample images within four
error quantiles (0 - 25%, 25 - 50%, 50 - 75%, and 75 - 100%).
13
Published as a conference paper at ICLR 2019

0	12	3	4
(b) L∞ distance
(c) L0 distance
Figure 8: Distribution of minimal adversarials for each model and distance metric. In (b) we see that
a threshold at 0.3 favors Madry et al. while a threshold of 0.35 would have favored the Binary ABS.
(a) L2 distance
14
Published as a conference paper at ICLR 2019
A.3 Derivation I
Derivation of the ELBO in equation 2.
log pθ (x)
log
∕dz pθ(x∣z)p(z),
where p(z) = N(0, 1) is a simple normal prior. Based on the idea of importance sampling using a variational
posterior qφ(z∣x) With parameters φ and using Jensen's inequality We arrive at
log Jdz 需X^pθ(XIz)P(Z),
=log Ez〜qφ(z∣x)
^Pθ (x∣z)p(z)
- qφ(ZIX)
≥ Ez~qφ(ZIX)
=Ez~qφ(ZIX)
一 Pθ (XIz)P(Z)I
- g qφ(z∣χ)]
logPθ(XIz) + log
P(Z)]
qφ(zlX)」
Ez〜qφ(ζ∣χ) [logPθ(χ∣z)] - DKL [qφ(z∣χ)l∣p(z)] .
This loWer bound is commonly referred to as ELBO.
A.4 DERIVATION II: LOWER B OUND FOR L2 ROBUSTNESS ESTIMATION
Derivation of equation 6. Starting from equation 3 We find that for a perturbation δ With size = kδk2 of sample
X the lower bound 'y (x + δ) can itself be bounded by,
'y (x + δ) = maχ -DKL [N (z, σq1)||N (0, I)] - 2σ2 kGy (Z) - X - δk2 + C,
≥ -DKL [N(zX,σq 1)||N(0,1)] - 2σ2 kGy(ZX)- X - δk2 + C,
where ZX is the optimal latent vector for the clean sample X for class y,
=' (X) + ɪ δ>(Gy (ZX) - X) -+ e2 + C，
≥ 'y (x) - σ2 € kGy (ZX) - Xk2 - 2σ2 F + C.	(IO)
A.5 DERIVATION III: UPPER BOUND FOR L2 ROBUSTNESS ESTIMATION
Derivation of equation 7.
'c(x + δ) =	max - DKL [N(z, σq1)||N(0,1)] -	2°2	kGy(z) — X — δk2	+ C,
≤	-DKL [N(0, σq 1)∣∣N(0,1)] + C —	min	202 kGc(Z) — X —	δ∣∣2 ,
≤	-DKL [N(0, σq1)||N (0, 1)]+ C -	min	2σ2 kGc (Z)- X -	δk2 ,
-DKL [N (0, σq 1)IIN (0, 1)] +C-
(212 (dc - €)
if dc ≥ €
else
(11)
for dc = minz kGc (Z) - Xk2. The last equation comes from the solution of the constrained optimization
problem mind(d - €)2d s.t. d > dc. Note that a tighter bound might be achieved by assuming single δ for
upper and lower bound.
A.6 L∞ ROBUSTNESS ESTIMATION
We proceed in the same way as for L2. Starting again from
'c(x) = max — DKL [N(z, σq 1)||N(0,1)] — 2°2 kGc(Z) — xk2 + C,	(12)
15
Published as a conference paper at ICLR 2019
let y be the predicted class and let Zx be the optimal latent for the clean sample X for class y. We can then
estimate a lower bound on '* (X + δ) for a perturbation δ with size e = ∣∣δ∣∣∞,
'y (X + δ) = maχ -DKL [N (Z, σqI)IIN (0, I)] - --2 IlGy (Z) - X - δk2 + C,
z	2σ
≥ -DKL [N(zx,σq 1)∣∣N(0,1)] - 2σ2 kGy(Zx) - X - δ∣2 + C,
where zx is the optimal latent for the clean sample X for class y.
=⑹(x) + σ2 δ>(Gy (zx) - x) - 2σ2 kδ∣2+C,
≥ '；(X) + C +2σ2 min (2δ>(Gy(Zx) - χ) -kδk2),
=⑹(X) + C + 2σ2 X min (2δi[Gy (Zx) - χ]i - δ2),
i
_ '*( ) + C ,__1_ X ʃ [Gy (Zx)- χ]2	if |[Gy (Zx)- x]i| ≤ e	(13)
—y()+	+2σ2 - M |[Gy (Zx) - χ]i∣ else	^	( )
Similarly, We can estimate an upper bound on '* (χ + δ) on all other classes C = y,
'C(x + δ) ≤ -Dkl [N(0, σq1)∣∣N(0,1)] + C — min --2 ∣∣Gc(Z) — X — δ∣2 ,
z 2σ2
≤ -DKL [N(0, σq1)||N (0, 1)] + C - min ʒ-2 kGc (Z)- X - δk2 ,
z,δ 2σ
=-Dkl [N(0, σq1)∣∣N(0,1)] + C — min --2∙〉： min ([Gc(Z) — χ]i - δi)2 ,
z 2σ	δi
i
= -DKL [N (0, σq 1)IIN (0, 1)] +C
1	[0	if I[Gy(Zx) - X]i| ≤ e
-min 2-2	£	( ([Gy (Zx)-	χ]i	- e)2	if [Gy (Zx)- χ]i > e	.
Z	i	[([Gy (Zx)	-	X]i	+ e)2	if [Gy (Zx)- X]i <€
(14)
In this case there is no closed-form solution for the minimization problem on the RHS (in terms of the minimum
of kGc(Z) - Xk2) but we can still compute the solution for each given which allows us perform a line search
along to find the point where equation 13 = equation 14.
A.7 Model & training details
Hyperparameters and training details for the ABS model The binary ABS and ABS have the same
weights and architecture: The encoder has 4 layers with kernel sizes= [5, 4, 3, 5], strides= [1, 2, 2,1] and feature
map sizes= [32, 32, 64, 2*8]. The first3 layers have ELU activation functions (Clevertetal., 2015),the last layer
is linear. All except the last layer use Batch Normalization (Ioffe & Szegedy, 2015). The Decoder architecture
has also 4 layers with kernel sizes= [4, 5, 5, 3], strides= [1, 2, 2,1] and feature map sizes= [32, 16, 16, 1]. The
first 3 layers have ELU activation functions, the last layer has a sigmoid activation function, and all layers except
the last one use Batch Normalization.
We trained the VAEs with the Adam optimizer (Kingma & Ba, 2014). We tuned the dimension L of the latent
space of the class-conditional VAEs (ending up with L = 8) to achieve 99% test error; started with a high weight
for the KL-divergence term at the beginning of training (which was gradually decreased from a factor of 10 to 1
over 50 epochs); estimated the weighting γ = [1, 0.96, 1.001, 1.06, 0.98, 0.96, 1.03, 1, 1, 1] of the lower bound
via a line search on the training accuracy. The parameters maximizing the test cross entropy3 and providing a
median confidence of p(yIx) = 0.9 for our modified softmax (equation 8) are η = 0.000039 and α = 440. For
our latent prior, We chose σq = 1 and for the posterior width We choose σ = 1 / Vz2
Hyperparameters for the CNNs The CNN and Binary CNN share the same architecture but have dif-
ferent weights. The architecture has kernel sizes = [5, 4, 3, 5], strides = [1, 2, 2, 1], and feature map sizes
= [20, 70, 256, 10]. All layers use ELU activation functions and all layers except the last one apply Batch
Normalization. The CNNs are both trained on the cross entropy loss with the Adam optimizer (Kingma & Ba,
2014). The parameters maximizing the test cross entropy and providing a median confidence ofp(yIx) = 0.9 of
the CNN for our modified softmax (equation 8) are η = 143900 and α = 1.
3Note that this solely scales the probabilities and does not change the classification accuracy.
16
Published as a conference paper at ICLR 2019
Hyperparameters for Madry et al. We adapted the pre-trained model provided by Madry et al4. Basi-
cally the architecture contains two convolutional, two pooling and two fully connected layers. The network is
trained on clean and adversarial examples minimizing the cross cross-entropy loss. The parameters maximizing
the test cross entropy and providing a median confidence of p(y|x) = 0.9 for our modified softmax (equation 8)
are η = 60 and α = 1.
Hyperparameters for the Nearest Neighbour classifier For a comparison with neural networks, we
imitate logits by replacing them with the negative minimal distance between the input and all samples within each
class. The parameters maximizing the test cross entropy and providing a median confidence of p(y|x) = 0.9 for
our modified softmax (equation 8) are η = 0.000000000004 and α = 5.
4https://github.com/MadryLab/mnist_challenge
17