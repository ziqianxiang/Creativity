Published as a conference paper at ICLR 2019
Predicting the Generalization Gap in Deep
Networks with Margin Distributions
Yiding Jiang ∖ Dilip Krishnan, Hossein Mobahi, Samy Bengio
Google AI
{ydjiang, dilipkay, hmobahi, bengio}@google.com
Ab stract
Recent research has demonstrated that deep neural networks can perfectly fit ran-
domly labeled data, but with very poor accuracy on held out data. This phenomenon
indicates that loss functions such as cross-entropy are not a reliable indicator of
generalization. This leads to the crucial question of how generalization gap can be
predicted from training data and network parameters. In this paper, we propose
such a measure, and conduct extensive empirical studies on how well it can predict
the generalization gap. Our measure is based on the concept of margin distribution,
which are the distances of training points to the decision boundary. We find that
it is necessary to use margin distributions at multiple layers of a deep network.
On the CIFAR-10 and the CIFAR-100 datasets, our proposed measure correlates
very strongly with the generalization gap. In addition, we find the following other
factors to be of importance: normalizing margin values for scale independence,
using characterizations of margin distribution rather than just the margin (closest
distance to decision boundary), and working in log space instead of linear space
(effectively using a product of margins rather than a sum). Our measure can be
easily applied to feedforward deep networks with any architecture and may point
towards new training loss functions that could enable better generalization.
1	Introduction
Generalization, the ability of a classifier to perform well on unseen examples, is a desideratum for
progress towards real-world deployment of deep neural networks in domains such as autonomous
cars and healthcare. Until recently, it was commonly believed that deep networks generalize well
to unseen examples. This was based on empirical evidence about performance on held-out dataset.
However, new research has started to question this assumption. Adversarial examples cause networks
to misclassify even slightly perturbed images at very high rates (Goodfellow et al., 2014; Papernot
et al., 2016). In addition, deep networks can overfit to arbitrarily corrupted data (Zhang et al., 2016),
and they are sensitive to small geometric transformations (Azulay & Weiss, 2018; Engstrom et al.,
2017). These results have led to the important question about how the generalization gap (difference
between train and test accuracy) of a deep network can be predicted using the training data and
network parameters. Since in all of the above cases, the training loss is usually very small, it is clear
that existing losses such as cross-entropy cannot serve that purpose. It has also been shown (e.g. in
Zhang et al. (2016)) that regularizers such as weight decay cannot solve this problem either.
Consequently, a number of recent works (Neyshabur et al., 2017b; Kawaguchi et al., 2017; Bartlett
et al., 2017; Poggio et al., 2017; Arora et al., 2018) have started to address this question, proposing
generalization bounds based on analyses of network complexity or noise stability properties. However,
a thorough empirical assessment of these bounds in terms of how accurately they can predict the
generalization gap across various practical settings is not yet available.
In this work, we propose a new quantity for predicting generalization gap of a feedforward neural
network. Using the notion of margin in support vector machines (Vapnik, 1995) and extension
to deep networks (Elsayed et al., 2018), we develop a measure that shows a strong correlation
with generalization gap and significantly outperforms recently developed theoretical bounds on
*Work done as part of the Google AI Residency.
Data and relevant code are at https://github.com/google-research/google-research/tree/master/demogen
1
Published as a conference paper at ICLR 2019
Figure 1: (Best seen as PDF) Density plots (top) and box plots (bottom) of normalized margin of three
convolutional networks trained with cross-entropy loss on CIFAR-10 with varying test accuracy: left:
55.2%, middle: 70.6%, right: 85.1%. The left network was trained with 20% corrupted labels. Train
accuracy of all above networks are close to 100%, and training losses close to zero. The densities and
box plots are computed on the training set. Normalized margin distributions are strongly correlated
with test accuracy (moving to the right as accuracy increases). This motivates our use of normalized
margins at all layers. The (Tukey) box plots show the median and other order statistics (see section
3.2 for details), and motivates their use as features to summarize the distributions.
generalization2. This is empirically shown by studying a wide range of deep networks trained on
the CIFAR-10 and CIFAR-100 datasets. The measure presented in this paper may be useful for a
constructing new loss functions with better generalization. Besides improvement in the prediction of
the generalization gap, our work is distinct from recently developed bounds and margin definitions in
a number of ways:
1.	These recently developed bounds are typically functions of weight norms (such as the
spectral, Frobenius or various mixed norms). Consequently, they cannot capture variations in
network topology that are not reflected in the weight norms, e.g. adding residual connections
(He et al., 2016) without careful additional engineering based on the topology changes.
Furthermore, some of the bounds require specific treatment for nonlinear activations. Our
proposed measure can handle any feedforward deep network.
2.	Although some of these bounds involve margin, the margin is only defined and measured at
the output layer (Bartlett et al., 2017; Neyshabur et al., 2017b). For a deep network, however,
margin can be defined at any layer (Elsayed et al., 2018). We show that measuring margin at
a single layer does not suffice to capture generalization gap. We argue that it is crucial to use
margin information across layers and show that this significantly improves generalization
gap prediction.
3.	The common definition of margin, as used in the recent bounds e.g. Neyshabur et al. (2017b),
or as extended to deep networks, is based on the closest distance of the training points to
the decision boundary. However, this notion is brittle and sensitive to outliers. In contrast,
we adopt margin distribution (Garg et al., 2002; Langford & Shawe-Taylor, 2002; Zhang &
Zhou, 2017; 2018) by looking at the entire distribution of distances. This is shown to have
far better prediction power.
4.	We argue that the direct extension of margin definition to deep networks (Elsayed et al.,
2018), although allowing margin to be defined on all layers of the model, is unable to capture
2In fairness, the theoretical bounds we compare against were designed to be provable upper bounds rather than
estimates with low expected error. Nevertheless, since recent developments on characterizing the generalization
gap of deep networks are in form of upper bounds, they form a reasonable baseline.
2
Published as a conference paper at ICLR 2019
generalization gap without proper normalization. We propose a simple normalization scheme
that significantly boosts prediction accuracy.
2	Related Work
The recent seminal work of Zhang et al. (2016) has brought into focus the question of how general-
ization can be measured from training data. They showed that deep networks can easily learn to fit
randomly labeled data with extremely high accuracy, but with arbitrarily low generalization capability.
This overfitting is not countered by deploying commonly used regularizers. The work of Bartlett et al.
(2017) proposes a measure based on the ratio of two quantities: the margin distribution measured at
the output layer of the network; and a spectral complexity measure related to the network’s Lipschitz
constant. Their normalized margin distribution provides a strong indication of the complexity of
the learning task, e.g. the distribution is skewed towards the origin (lower normalized margin) for
training with random labels. Neyshabur et al. (2017b;a) also develop bounds based on the product
of norms of the weights across layers. Arora et al. (2018) develop bounds based on noise stability
properties of networks: more stability implies better generalization. Using these criteria, they are
able to derive stronger generalization bounds than previous works.
The margin distribution (specifically, boosting of margins across the training set) has been shown
to correspond to generalization properties in the literature on linear models (Schapire et al., 1998):
they used this connection to explain the effectiveness of boosting and bagging techniques. Reyzin &
Schapire (2006) showed that it was important to control the complexity of a classifier when measuring
margin, which calls for some type of normalization. In the linear case (SVM), margin is naturally
defined as a function of norm of the weights Vapnik (1995). In the case of deep networks, true
margin is intractable. Recent work (Elsayed et al., 2018) proposed a linearization to approximate the
margin, and defined the margin at any layer of the network. (Sokolic et al., 2016) provide another
approximation to the margin based on the norm of the Jacobian with respect to the input layer. They
show that maximizing their approximations to the margin leads to improved generalization. However,
their analysis was restricted to margin at the input layer. Poggio et al. (2017) and Liao et al. (2018)
propose a normalized cross-entropy measure that correlates well with test loss. Their proposed
normalized loss trades off confidence of predictions with stability, which leads to better correlation
with test accuracy, leading to a significant lowering of output margin.
3	Prediction of Generalization Gap
In this section, we introduce our margin-based measure. We first explain the construction scheme
for obtaining the margin distribution. We then squeeze the distributional information of the margin
to a small number of statistics. Finally, we regress these statistics to the value of the generalization
gap. We assess prediction quality by applying the learned regression coefficients to predict the
generalization gap of unseen models. We will start with providing a motivation for using the margins
at the hidden layers which is supported by our empirical findings. SVM owes a large part of its
success to the kernel that allows for inner product in a higher and richer feature space. At its crux, the
primal kernel SVM problem is separated into the feature extractor and the classifier on the extracted
features. We can separate any feed forward network at any given hidden layer and treat the hidden
representation as a feature map. From this view, the layers that precede this hidden layer can be
treated as a learned feature extractor and then the layers that come after are naturally the classifier.
If the margins at the input layers or the output layers play important roles in generalization of the
classifier, it is a natural conjecture that the margins at these hidden representations are also important
in generalization. In fact, if we ignore the optimization procedure and focus on a converged network,
generalization theories developed on the input such as Lv et al. (2019) can be easily extended to the
hidden layers or the extracted features.
3.1	Margin Approximation
First, we establish some notation. Consider a classification problem with n classes. We assume a
classifier f consists of non-linear functions fi : X → R, for i = 1, . . . , n that generate a prediction
score for classifying the input vector x ∈ X to class i. The predicted label is decided by the class
with maximal score, i.e. i* = arg maxi fi(x). Define the decision boundary for each class pair (i,j)
3
Published as a conference paper at ICLR 2019
as:
D(i,j) , {x | fi(x) = fj (x)}	(1)
Under this definition, the lp distance of a point x to the decision boundary D(i,j) can be expressed as
the smallest displacement of the point that results in a score tie:
df,x,(i,j) , minkδkp	s.t.	fi(x + δ) = fj(x + δ)
δ
(2)
Unlike an SVM, computing the “exact” distance of a point to the decision boundary (Eq. 2) for a deep
network is intractable3. In this work, we adopt the approximation scheme from Elsayed et al. (2018)
to capture the distance of a point to the decision boundary. This a first-order Taylor approximation to
the true distance Eq. 2. Formally, given an input x to a network, denote its representation at the lth
layer (the layer activation vector) by xl. For the input layer, let l = 0 and thus x0 = x. Then for
p = 2, the distance of the representation vector xl to the decision boundary for class pair (i, j) is
given by the following approximation:
df,(i,j)(xl)
fi(xl) - fj (xl)
l∣Vχifi(χl) - Vxlfj(χl)k2
(3)
Here fi(xl) represents the output (logit) of the network logit i given xl. Note that this distance can be
positive or negative, denoting whether the training sample is on the “correct” or “wrong” side of the
decision boundary respectively. This distance is well defined for all (i, j) pairs, but in this work we
assume that i always refers to the ground truth label and j refers to the second highest or highest class
(if the point is misclassified). The training data x induces a distribution of distances at each layer l
which, following earlier naming convention (Garg et al., 2002; Langford & Shawe-Taylor, 2002), we
refer to as margin distribution (at layer l). For margin distribution, we only consider distances with
positive sign (we ignore all misclassified training points). Such design choice facilitates our empirical
analysis when we transform our features (e.g. log transform); further, it has also been suggested that
it may be possible to obtain a better generalization bound by only considering the correct examples
when the classifier classifies a significant proportion of the training examples correctly, which is
usually the case for neural networks (Bartlett, 1998). For completeness, the results with negative
margins are included in appendix Sec. 7.
A problem with plain distances and their associated distribution is that they can be trivially boosted
without any significant change in the way classifier separates the classes. For example, consider
multiplying weights at a layer by a constant and dividing weights in the following layer by the
same constant. In a ReLU network, due to positive homogeneity property (Liao et al., 2018), this
operation does not affect how the network classifies a point, but it changes the distances to the
decision boundary4. To offset the scaling effect, we normalize the margin distribution. Consider
margin distribution at some layer l, and let xlk be the representation vector for training sample k. We
compute the variance of each coordinate of {xlk } separately, and then sum these individual variances.
This quantity is called total variation of xl . The square root of this quantity relates to the scale of
the distribution. That is, if xl is scaled by a factor, so is the square root of the total variation. Thus,
by dividing distances by the square root of total variation, we can construct a margin distribution
invariant to scaling. More concretely, the total variation is computed as:
nn
V(XI) = tr(n X(Xk - XI)(Xk - XI)T) ,	xl = n Xxlk,	⑷
k=1	k=1
i.e. the trace of the empirical covariance matrix of activations. Using the total variation, the normalized
margin is specified by:
df,(i,j)(Xk ) =dfpjX)	(5)
ν(Xl)
While the quantity is relatively primitive and easy to compute, Fig. 1 (top) shows that the normalized-
margin distributions based on Eq. 5 have the desirable effect of becoming heavier tailed and shifting
3This is because computing the distance of a point to a nonlinear surface is intractable. This is different from
SVM where the surface is linear and distance of a point to a hyperplane admits a closed form expression.
4For example, suppose the constant c is greater that one. Then, multiplying the weights of a layer by c
magnifies distances computed at the layer by a factor of c.
4
Published as a conference paper at ICLR 2019
to the right (increasing margin) as generalization gap decreases. We find that this effect holds across
a range of networks trained with different hyper-parameters.
3.2	Summarizing the Margin Distribution
Instead of working directly with the (normalized) margin distribution, it is easier to analyze a compact
signature of that. The moments of a distribution are a natural criterion for this purpose. Perhaps the
most standard way of doing this is computing the empirical moments from the samples and then
take the nth root of the nth moment. In our experiments, we used the first five moments. However,
it is a well-known phenomenon that the estimation of higher order moments based on samples can
be unreliable. Therefore, we also consider an alternate way to construct the distribution’s signature.
Given a set of distances D = {dm}nm=1, which constitute the margin distribution. We use the median
Q2, first quartile Q1 and third quartile Q3 of the normalized margin distribution, along with the two
fences that indicate variability outside the upper and lower quartiles. There are many variations for
fences, but in this work, with IQR = Q3 - Q1, we define the upper fence to be max({dm : dm ∈
D ∧ dm ≤ Q3 + 1.5I QR}) and the lower fence to be min({dm : dm ∈ D ∧ dm ≥ Q1 - 1.5I QR})
(McGill et al., 1978). These 5 statistics form the quartile description that summarizes the normalized
margin distribution at a specific layer, as shown in the box plots of Fig. 1. We will later see that
both signature representations are able to predict the generalization gap, with the second signature
working slightly better.
A number of prior works such as Bartlett et al. (2017), Neyshabur et al. (2017b), Liu et al. (2016), Sun
et al. (2015), Sokolic et al. (2016), and Liang et al. (2017) have focused on analyzing or maximizing
the margin at either the input or the output layer of a deep network. Since a deep network has many
hidden layers with evolving representations, it is not immediately clear which of the layer margins is
of importance for improving generalization. Our experiments reveal that margin distribution from all
of the layers of the network contribute to prediction of generalization gap. This is also clear from
Fig. 1 (top): comparing the input layer (layer 0) margin distributions between the left and right plots,
the input layer distribution shifts slightly left, but the other layer distributions shift the other way. For
example, if we use quartile signature, we have 5L components in this vector, where L is the total
number of layers in the network. We incorporate dependence on all layers simply by concatenating
margin signatures of all layers into a single combined vector θ that we refer to as total signature.
Empirically, we found constructing the total signature based on four evenly-spaced layers (input, and
3 hidden layers) sufficiently captures the variation in the distributions and generalization gap, and
also makes the signature agnostic to the depth of the network.
3.3	Evaluation Metrics
Our goal is to predict the generalization gap, i.e. the difference between training and test accuracy
at the end of training, based on total signature θ of a trained model. We use the simplest prediction
model, i.e. a linear form ^ = aτφ(θ) + b, where a ∈ Rdim⑻ and b ∈ R are parameters of the
predictor, and φ : R → R is a function applied element-wise to θ. Specifically, we will explore two
choices of φ: the identity φ(x) = x and entry-wise log transform φ(x) = log(x), which correspond
to additive and multiplicative combination of margin statistics respectively. We do not claim this
model is the true relation, but rather it is a simple model for prediction; and our results suggest that it
is a surprisingly good approximation.
In order to estimate predictor parameters a, b, we generate a pool of n pretrained models (covering
different datasets, architectures, regularization schemes, etc. as explained in Sec. 4) each of which
gives one instance of the pair θ, g (g being the generalization gap for that model). We then find a, b
by minimizing mean squared error: (a*,b*) = argmina,b Pi (aτφ(θi) + b - gi)2, where i indexes
the ith model in the pool. The next step is to assess the prediction quality. We consider two metrics
for this. The first metric examines quality of predictions on unseen models. For that, we consider a
held-out pool of m models, different from those used to estimate (a, b), and compute the value of
g on them via g = ατφ(θ) + b. In order to quantify the discrepancy between predicted gap g and
ground truth gap g we use the notion of coefficient of determination (R2) (Glantz et al., 1990):
R = 1 -	Pn=1(gj - gj 产	⑹
=	Pn=l(gj - 1 Pn=1 gj )2
5
Published as a conference paper at ICLR 2019
R2 measures what fraction of data variance can be explained by the linear model5 (it ranges from 0
to 1 on training points but can be outside that range on unseen points). To be precise, we use k-fold
validation to study how the predictor can perform on held out pool of trained deep networks. We use
90/10 split, fit the linear model with the training pool, and measure R2 on the held out pool. The
performance is averaged over the 10 splits. Since R2 is now not measured on the training pool, it does
not suffer from high data dimension and can be negative. In all of our experiments, we use k = 10.
We provide a subset of residual plots and corresponding univariate F-Test for the experiments in the
appendix (Sec. 8). The F-score also indicates how important each individual variable is. The second
metric examines how well the model fits based on the provided training pool; it does not require a
test pool. To characterize this, We use adjusted R2 (Glantz et al., 1990) defined as:
1 - (1 - R2) n - dim(θ) - 1 .
R2
(7)
The R2 can be negative when the data is non-linear. Note that R2 is always smaller than R2.
Intuitively, R2 penalizes the model if the number of features is high relative to the available data
points. The closer R2 is to 1, the better the model fits. Using R2 is a simple yet effective method
to test the fitness of linear model and is independent of the scale of the target, making it a more
illustrative metric than residuals.
4	Experiments
We tested our measure of generalization gap g, along with baseline measures, on a number of deep
networks and architectures: nine-layer convolutional networks on CIFAR-10 (10 with input layer),
and 32-layer residual networks on both CIFAR-10 and CIFAR-100 datasets. The trained models
and relevant Tensorflow Abadi et al. (2016) code to compute margin distributions are released at
https://github.com/google-research/google-research/tree/master/demogen
4.1	Convolutional Neural Networks on CIFAR-10
Using the CIFAR-10 dataset, we train 216 nine-layer convolutional networks with different settings
of hyperparameters and training techniques. We apply weight decay and dropout with different
strengths; we use networks with and without batch norm and data augmentation; we change the
number of hidden units in the hidden layers. Finally, we also include training with and without
corrupted labels, as introduced in Zhang et al. (2016); we use a fixed amount of 20% corruption of the
true labels. The accuracy on the test set ranges from 60% to 90.5% and the generalization gap ranges
from 1% to 35%. In standard settings, creating neural network models with small generalization gap
is difficult; in order to create sufficiently diverse generalization behaviors, we limit some models’
capacities by large weight regularization which decreases generalization gap by lowering the training
accuracy. All networks are trained by SGD with momentum. Further details are provided in the
supplementary material (Sec. 6).
For each trained network, we compute the depth-agnostic signature of the normalized margin
distribution (see Sec. 3). This results in a 20-dimensional signature vector. We estimate the parameters
of the linear predictor (a, b) with the log transform φ(x) = log(x) and using the 20-dimensional
signature vector θ. Fig. 2 (left) shows the resulting scatter plot of the predicted generalization gap g
and the true generalization gap g. As it can be seen, it is very close to being linear across the range of
generalization gaps, and this is also supported by the R2 of the model, which is 0.96 (max is 1).
As a first baseline method, we compare against the work of Bartlett et al. (2017) which provides one
of the best generalization bounds currently known for deep networks. This work also constructs a
margin distribution for the network, but in a different way. To make a fair comparison, we extract
the same signature θ from their margin distribution. Since their margin distribution can only be
defined for the output layer, their θ is 5-dimensional for any network. The resulting fit is shown in
Fig. 2(right). It is clearly a poorer fit than that of our signature, with a significantly lower R2 of 0.72.
For a fairer comparison, we also reduced our signature θ from 20 dimensions to the best performing
4 dimensions (even one dimension less than what we used for Bartlett’s) by dropping 16 components
in our θ. This is shown in Fig. 2 (middle) and has a R2 of 0.89, which is poorer than our complete
5 A simple manipulation shows that the prediction residual Pm=ι(gj — gj)2 (χ 1 — R2, so R2 can be
interpreted as a scale invariant alternative to the residual.
6
Published as a conference paper at ICLR 2019
	CNN+CIFAR10			ReSNet+CIFAR10			ReSNet+CIFAR100		
Experiment Settings	A R2	kf R2	mse	A R2	kf R2	mse	AR2	kf R2	mse
qrt+log	0.94	0.90	ι^55~	0.87	0.81	^04T	0.97	0.96	nɪ
qrt+log+unnorm	0.89	0.86	~0(Γ	0.82	0.74	^048^	0.95	0.94	T:9-
qrt+linear	0.88	0.84	2^2fΓ	0.77	0.66	^θɪ	0.91	0.87	^6-
sf+log	0.73	0.69		0.53	0.41	^080^	0.80	0.78	^0-
sl+log	0.86	0.84	2~2T	0.44	0.33	^087^	0.95	0.94	~L8~
moment+log	0.93	0.87		0.83	0.74	^θɪ	0.94	0.92	^0-
best4+log	0.89	0.88	~n~	0.54	0.43	~080~	0.93	0.92	^^4~
Spectral+log	0.73	0.70		-	-	-	-	-	-
Jacobian+log	0.42	N	^0-	0.20	N	~T0~	0.47	-N-	6~6XΓ
LM+linear	0.35	N	Z52Ξ	0.68	N	~066~	0.74	N	~T0-
Table 1: Ablation experiments considering different scenarios (see text for details). The last 3 rows
are baselines from: (Bartlett et al., 2017; Sokolic et al., 2016; Elsayed et al., 2018). A indicates
adjusted; kf indicates k-fold; mse indicates mean squared error in 10-3; N indicates negative.
θ but still significantly higher than that of Bartlett et al. (2017). In addition, we considered two
other baseline comparisons: Sokolic et al. (2016), where margin at input is defined as a function of
the Jacobian of output (logits) with respect to input; and Elsayed et al. (2018) where the linearized
approximation to margin is derived (for the same layers where we use our normalized margin
approximation).
Norm. Margin 20D	Norm. Margin 4D	Bartlett Margin 5D
predicted generalization gap	predicted generalization gap	predicted generalization gap
Figure 2: (Best seen as PDF) Regression models to predict generalization gap. Left: regression model
fit in log space for the full 20-dimensional feature space (R2 = 0.94); Middle: fit for a subset of only
4 features, 2 each from 2 of the hidden layers (RR = 0.89); Right: fit for features extracted from the
normalized margin distribution as used in Bartlett et al. (2017) (R2 = 0.72).
To quantify the effect of the normalization, different layers, feature transformation etc., we conduct a
number of ablation experiments with the following configuration: 1. linear/log: Use signature
transform of φ(x) = x or φ(x) = log(x); 2. sl: Use signature from the single best layer (θ ∈ R5);
3. sf: Use only the single best statistic from the total signature for all the layers (θ ∈ R4, individual
layer result can be found in Sec. 7); 4. moment: Use the first 5 moments of the normalized margin
distribution as signature instead of quartile statistics θ ∈ R20 (Sec. 3); 5. spectral: Use signature
of spectrally normalized margins from Bartlett et al. (2017) (θ ∈ R5); 6. qrt: Use all the quartile
statistics as total signature θ ∈ R20 (Sec. 3); 7. best4: Use the 4 best statistics from the total
signature (θ ∈ R4); 8. Jacobian: Use the Jacobian-based margin defined in Eq (39) of Sokolic
et al. (2016) (θ ∈ R5); 9. LM: Use the large margin loss from Elsayed et al. (2018) at the same
four layers where the statistics are measured (θ ∈ R4); 10. unnorm indicates no normalization. In
Table 1, We list the R2 from fitting models based on each of these scenarios. We see that, both quartile
and moment signatures perform similarly, lending support to our thesis that the margin distribution,
rather than the smallest or largest margin, is of importance in the context of generalization.
4.2	Residual Networks on CIFAR- 10
On the CIFAR-10 dataset, We train 216 convolutional netWorks With residual connections; these
netWorks are 32 layers deep With standard ResNet 32 topology (He et al., 2016). Since it is difficult
to train ResNet Without activation normalization, We created generalization gap variation With batch
7
Published as a conference paper at ICLR 2019
normalization (Ioffe & Szegedy, 2015) and group normalization (Wu & He, 2018). We further use
different initial learning rates. The range of accuracy on the test set ranges from 83% to 93.5% and
generalization gap from 6% to 13.5%. The residual networks were much deeper, and so we only
chose 4 layers for feature-length compatibility with the shallower convoluational networks. This
design choice also facilitates ease of analysis and circumvents the dependency on depth of the models.
Table 1 shows the R2. Note in the presence of residual connections that use convolution instead of
identity and identity blocks that span more than one convolutional layers, it is not immediately clear
how to properly apply the bounds of Bartlett et al. (2017) (third from last row) without morphing
the topology of the architecture and careful design of reference matrices. As such, we omit them for
ResNet. Fig. 3 (left) shows the fit for the resnet models, with R2 = 0.87. Fig. 3 (middle) and Fig. 3
(right) compare the log normalized density plots of a CIFAR-10 resnet and CIFAR-10 CNN. The
plots show that the Resnet achieves a better margin distribution, correlated with greater test accuracy,
even though it was trained without data augmentation.
Figure 3: (Best seen as PDF) Left: Regression model fit in log space for the full 20-dimensional
feature space for 216 residual networks (R = 0.87) on QFAR-10; Middle: Log density plot of
normalized margins of a particular residual network that achieves 91.7% test accuracy without data
augmentation; Right: Log density plot of normalized margins of a CNN that achieves 87.2% with
data augmentation. We see that the resnet achieves larger margins, especially at the hidden layers,
and this is reflected in the higher test accuracy.
4.3	RESNET ON CIFAR- 1 00
On the CIFAR-100 dataset, we trained 324 ResNet-32 with the same variation in hyperparameter
settings as for the networks for CIFAR-10 with one additional initial learning rate. The range of
accuracy on the test set ranges from 12% to 73% and the generalization gap ranges from 1% to 75%.
Table 1 shows R2 for a number of ablation experiments and the full feature set. Fig. 4 (left) shows
the fit of predicted and true generalization gaps over the networks (R2 = 0.97). Fig. 4 (middle)
and Fig. 4 (right) compare a CIFAR-100 residual network and a CIFAR-10 residual network with
the same architecture and hyperparameters. Under these settings, the CIFAR-100 network achieves
44% test accuracy, whereas CIFAR-10 achieves 61%. The resulting normalized margin density plots
clearly reflect the better generalization achieved by CIFAR-10: the densities at all layers are wider
and shifted to the right. Thus, the normalized margin distributions reflect the relative “difficulty” of a
particular dataset for a given architecture.
5	Discussion
We have presented a predictor for generalization gap based on margin distribution in deep networks
and conducted extensive experiments to assess it. Our results show that our scheme achieves a high
adjusted coefficient of determination (a linear regression predicts generalization gap accurately).
Specifically, the predictor uses normalized margin distribution across multiple layers of the network.
The best predictor uses quartiles of the distribution combined in multiplicative way (additive in
log transform). Compared to the strong baseline of spectral complexity normalized output margin
(Bartlett et al., 2017), our scheme exhibits much higher predictive power and can be applied to any
feedforward network (including ResNets, unlike generalization bounds such as (Bartlett et al., 2017;
Neyshabur et al., 2017b; Arora et al., 2018)). We also find that using hidden layers is crucial for the
predictive power. Our findings could be a stepping stone for studying new generalization theories and
8
Published as a conference paper at ICLR 2019
Norm. Margin 20D	Density (CIFAR-100)	Density (CIFAR-10)
Figure 4: (Best seen as PDF) Left: Regression model fit in log space for the full 20-dimensional
feature space for 300 residual networks (R2 = 0.97) on CIFAR-100; Middle: density plot of
normalized margins of a particular residual network trained on CIFAR-100 that achieves 44% test
accuracy; Right: Density plot of normalized margins of a residual network trained on CIFAR-10 that
achieves 61%.
new loss functions with better generalization properties. We include the results on cross architecture
and cross data comparison as well as some final thoughts in Appendix Sec. 9.
Acknowledgments
We are thankful to Gamaleldin Elsayed (Google), Tomer Koren (Google), Sergey Ioffe (Google),
Vighnesh Birodkar (Google), Shraman Ray Chaudhuri (Google), Kevin Regan (Google), Behnam
Neyshabur (NYU), and Dylan Foster (Cornell), for discussions and helpful feedbacks.
References
Mart´n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for large-scale
machine learning. In OSDI, volume 16, pp. 265—283, 2016.
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. arXiv preprint arXiv:1802.05296, 2018.
Aharon Azulay and Yair Weiss. Why do deep convolutional networks generalize so poorly to small
image transformations? arXiv preprint arXiv:1805.12177, 2018.
P. L. Bartlett. The sample complexity of pattern classification with neural networks: the size of the
weights is more important than the size of the network. IEEE Transactions on Information Theory,
44(2):525-536, March 1998. ISSN 0018-9448. doi: 10.1109/18.661502.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pp. 6240-6249, 2017.
Gamaleldin F Elsayed, Dilip Krishnan, Hossein Mobahi, Kevin Regan, and Samy Bengio. Large
margin deep networks for classification. arXiv preprint arXiv:1803.05598, 2018.
Logan Engstrom, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. A rotation and a
translation suffice: Fooling cnns with simple transformations. arXiv preprint arXiv:1712.02779,
2017.
Ashutosh Garg, Sariel Har-Peled, and Dan Roth. On generalization bounds, projection profile, and
margin distribution. In Machine Learning, Proceedings of the Nineteenth International Conference
(ICML 2002), University of New South Wales, Sydney, Australia, July 8-12, 2002, pp. 171-178,
2002.
Stanton A Glantz, Bryan K Slinker, and Torsten B Neilands. Primer of applied regression and
analysis of variance, volume 309. McGraw-Hill New York, 1990.
9
Published as a conference paper at ICLR 2019
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Kenji Kawaguchi, Leslie Pack Kaelbling, and Yoshua Bengio. Generalization in deep learning. arXiv
preprint arXiv:1710.05468, 2017.
John Langford and John Shawe-Taylor. Pac-bayes margins. In Proceedings of the 15th International
Conference on Neural Information Processing Systems, NIPS’02, pp. 439-446, Cambridge, MA,
USA, 2002. MIT Press.
Xuezhi Liang, Xiaobo Wang, Zhen Lei, Shengcai Liao, and Stan Z Li. Soft-margin softmax for
deep classification. In International Conference on Neural Information Processing, pp. 413-421.
Springer, 2017.
Qianli Liao, Brando Miranda, Andrzej Banburski, Jack Hidary, and Tomaso Poggio. A surprising
linear relationship predicts test performance in deep networks. arXiv preprint arXiv:1807.09659,
2018.
Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400,
2013.
Weiyang Liu, Yandong Wen, Zhiding Yu, and Meng Yang. Large-margin softmax loss for convolu-
tional neural networks. In ICML, pp. 507-516, 2016.
Shen-Huan Lv, Lu Wang, and Zhi-Hua Zhou. Optimal margin distribution network, 2019. URL
https://openreview.net/forum?id=HygcvsAcFX.
Robert McGill, John W Tukey, and Wayne A Larsen. Variations of box plots. The American
Statistician, 32(1):12-16, 1978.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. A pac-
bayesian approach to spectrally-normalized margin bounds for neural networks. arXiv preprint
arXiv:1707.09564, 2017a.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring general-
ization in deep learning. In Advances in Neural Information Processing Systems, pp. 5947-5956,
2017b.
Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram
Swami. Practical black-box attacks against deep learning systems using adversarial examples.
arXiv preprint arXiv:1602.02697, 2016.
Tomaso Poggio, Kenji Kawaguchi, Qianli Liao, Brando Miranda, Lorenzo Rosasco, Xavier Boix,
Jack Hidary, and Hrushikesh Mhaskar. Theory of deep learning iii: explaining the non-overfitting
puzzle. arXiv preprint arXiv:1801.00173, 2017.
Lev Reyzin and Robert E Schapire. How boosting the margin can also boost classifier complexity. In
Proceedings of the 23rd international conference on Machine learning, pp. 753-760. ACM, 2006.
Robert E Schapire, Yoav Freund, Peter Bartlett, Wee Sun Lee, et al. Boosting the margin: A new
explanation for the effectiveness of voting methods. The annals of statistics, 26(5):1651-1686,
1998.
Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel R. D. Rodrigues. Robust large margin
deep neural networks. CoRR, abs/1605.08254, 2016. URL http://arxiv.org/abs/1605.
08254.
10
Published as a conference paper at ICLR 2019
Shizhao Sun, Wei Chen, Liwei Wang, and Tie-Yan Liu. Large margin deep neural networks:
Theory and algorithms. CoRR, abs/1506.05232, 2015. URL http://arxiv.org/abs/
1506.05232.
Vladimir N. Vapnik. The Nature of Statistical Learning Theory. Springer-Verlag New York, Inc.,
New York, NY, USA, 1995. ISBN 0-387-94559-8.
Yuxin Wu and Kaiming He. Group normalization. arXiv preprint arXiv:1803.08494, 2018.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
Teng Zhang and Zhi-Hua Zhou. Multi-class optimal margin distribution machine. In Doina Precup
and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning,
volume 70 of Proceedings ofMachine Learning Research, pp. 4063-4071, International Convention
Centre, Sydney, Australia, 06-11 Aug 2017. PMLR. URL http://proceedings.mlr.
press/v70/zhang17h.html.
Teng Zhang and Zhi-Hua Zhou. Optimal margin distribution clustering, 2018. URL https:
//aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16895.
11
Published as a conference paper at ICLR 2019
6	Appendix: Experimental Details
6.1	CNN+ CIFAR- 1 0
We use an architecture very similar to Network in Network (Lin et al. (2013)), but we remove all
dropout and max pool from the network.
Layer Index	Layer Type	Output Shape
0	Input	32 × 32 × 3
1	3 X 3 convolution + stride 2	16 × 16 × 192
2	1 × 1 convolution + stride 1	16 × 16 × 192
3	1 × 1 convolution + stride 1	16 × 16 × 192
4	3 × 3 convolution + stride 2	8 × 8 × 192
5	1 × 1 convolution + stride 1	8 × 8 × 192
6	1 × 1 convolution + stride 1	8 × 8 × 192
7	3 × 3 convolution + stride 2	4 × 4 × 192
8	1 × 1 convolution + stride 1	4 × 4 × 192
9	1 × 1 convolution + stride 1	4 × 4 × 192
10	―	4 × 4 convolution + stride 1	1 × 1 × 10~~
Table 2: Architecture of base CNN model.
To create generalization gap in this model, we make the following modification to the base architec-
ture:
1.	Use channel size of 192, 288, and 384 to create different width
2.	Train with and without batch norm at all convolutional layers
3.	Apply dropout at layer 3 and 6 with p = 0.0, 0.2, 0.5
4.	Apply l2 regularization with λ = 0.0, 0.001, 0.005
5.	Train with and without data augmentation with random cropping, flipping and shifting
6.	Train each configuration twice
In total this gives us 3 × 2 × 3 × 3 × 2 × 2 = 216 different network architectures. The models are
trained with SGD with momentum (α = 0.9) at minibatch size of 128 and intial learning rate of 0.01.
All networks are trained for 380 epoch with 10× learning rate decay at interval of 100 epoch.
6.2	RESNET 32 + CIFAR- 1 0
For this experiments, we use the standard ResNet 32 architectures. We consider down sampling to the
marker of a stage, so there are in total 3 stages in the ResNet 32 architecture. To create generalization
gap in this model, we make the following modifications to the architecture:
1.	Use network width that are 1×, 2×, 4× wider in number of channels.
2.	Train with batch norm or group norm (Wu & He, 2018)
3.	Train with initial learning rate of 0.01, 0.001
4.	Apply l2 regularization with λ = 0.0, 0.02, 0.002
5.	Trian with and without data augmentation with random cropping, flipping and shifting
6.	Train each configuration 3 times
In total this gives us 3 × 2 × 2 × 3 × 2 × 3 = 216 different network architectures. The models are
trained with SGD with momentum (α = 0.9) at minibatch size of 128. All networks are trained for
380 epoch with 10× learning rate decay at interval of 100 epoch.
12
Published as a conference paper at ICLR 2019
6.3	RESNET 32 + CIFAR- 1 00
For this experiments, we use the standard ResNet 32 architectures. We consider down sampling to the
marker of a stage, so there are in total 3 stages in the ResNet 32 architecture. To create generalization
gap in this model, we make the following modifications to the architecture:
1.	Use network width that are 1×, 2×, 4× wider in number of channels.
2.	Train with batch norm or group norm (Wu & He, 2018)
3.	Train with initial learning rate of 0.1, 0.01, 0.001
4.	Apply l2 regularization with λ = 0.0, 0.02, 0.002
5.	Trian with and without data augmentation with random cropping, flipping and shifting
6.	Train each configuration 3 times
In total this gives us 3 × 2 × 3 × 3 × 2 × 3 = 324 different network architectures. The models are
trained with SGD with momentum (α = 0.9) at minibatch size of 128. All networks are trained for
380 epoch with 10× learning rate decay at interval of 100 epoch.
13
Published as a conference paper at ICLR 2019
7	Appendix: More Regression Results
7.1	Analysis with Negative Margins
The last two rows contain the results of including the negative margins and regress against both the
gap (generalization gap) and against acc (test accuracy). We see that including when negative
margin is included, it is in general easier to predict the accuracy of the models rather than the gap
itself. For convenience, we have reproduced Table 1.
	CNN+CIFAR10			ResNet+CIFAR10			ResNet+CIFAR100		
Experiment Settings	A R2	kf R2	mse	A R2	kf R2	mse	A R2	kf R2	mse
qrt+log	0.94	0.90	ι^55~	0.87	0.81	^04T	0.97	0.96	nɪ
qrt+log+unnorm	0.89	0.86	2^XΓ	0.82	0.74	^048^	0.95	0.94	~UΓ
qrt+linear	0.88	0.84	221~	0.77	0.66	^θɪ	0.91	0.87	2^66~
sf+log	0.73	0.69		0.53	0.41	^080^	0.80	0.78	^0^
sl+log	0.86	0.84	~^2Γ	0.44	0.33	^087^	0.95	0.94	~TJT
moment+log	0.93	0.87	~T6~	0.83	0.74	^θɪ	0.94	0.92	^0-
best4+log	0.89	0.88	2^1T~	0.54	0.43	~080~	0.93	0.92	Z2≡
Spectral+log	0.73	0.70		-	-	-	-	-	-
Jacobian+log	0.42	N	53XΓ	0.20	N	~T0~	0.47	N	-60-
LM+linear	0.35	N	~5Γ	0.68	N	^066^	0.74	N	^0-
qrt+linear+gap	0.90	0.86	"ɪð-	0.71	0.61	0~06~	0.93	0.89	^ɪ0^
qrt+linear+acc	0.91	0.88	~T6~	0.98	0.96	~0T~	0.92	0.89	^^0~
Table 3: Table 1 with two additional row. The second to last row shows prediction of generalization
gap when negative margins are used; the last row is for prediction of test accuracy rather than gap.
Note that negative margins can only be used with linear features.
7.2	Analysis for Individual Layer’ s Margin Distributions
This is a comparison of different individual layer’s predictive power by using only the margin
distribution at that layer. This results illustrates the importance of margins in the hidden layers.
	CNN+CIFAR10			ResNet+CIFAR10			ResNet+CIFAR100		
Experiment Settings	A R2	kf R2	mse	A R2	kf R2	mse	A R2	kf R2	mse
input	0.77	0.73		0.16	0.02	~T0~	0.82	0.81	~^9JΓ
hl	0.77	0.74	^1-	0.36	0.26	^094^	0.95	0.94	Tl-
h2	0.80	0.77	^0-	0.41	0.31	^090^	0.77	0.73	^4-
h3	0.86	0.84	~uτ	0.44	0.33	^087^	0.54	0.53	-63-
all layer	0.94	0.90	~1.5~	0.87	0.81	~04T	0.97	0.96	
Table 4: Single layer comparison, all with qrt+log
14
Published as a conference paper at ICLR 2019
8	Appendix: Further Analysis of Regression
8.1	CNN + CIFAR- 1 0 + All Quartile Signature
Figure 5: Residual plots for all explanatory variables, row: h0, h1, h2, h3, column: lower fence, Q1,
Q2, Q3, upper fence. lower fence is clipped because distance cannot be smaller than 0. The residual
is fairly evenly distributed around 0.
	lower fence	Qi	Q2	Q3	upper fence
"h0^	-30640-	114.41	39.56	12.54	507
"ɪr	-286.53-	9.42	5.16	17.29	-38.57
^EF	-259.68-	6.95	77.03	110.40	-152.20-
ɪ	188.59	10.29	49.76	83.40	143.23
	lower fence	Qi	Q2	Q3	upper fence
^E0^	3.59e-43	1.13e-21	1.76e-9	4.87e-4	-2.52e-2-
^hΓ	2.34e-41	2.41e-3	2.40e-2	4.64e-5	2.70e-09
^hF	8.76e-39	8.95e-3	5.38e-16	4.30e-21	9.12e-27
ɪ	3.40e-31	1.54e-3	2.37e-11	5.17e-17	1.31e-25
Table 5: F score (top) and p-values (bottom) for all 20 variables. Using p = 0.05, the null hypotheses
are rejected for every variable.
15
Published as a conference paper at ICLR 2019
8.2 ResNet 32 + CIFAR- 1 0 + All Quartile S ignature
Figure 6:	Residual plots for all explanatory variables, row: h0, h1, h2, h3, column: lower fence, Q1,
Q2, Q3, upper fence. lower fence is clipped because distance cannot be smaller than 0. The residual
is less evenly distributed as are in other two settings; this fact is well reflected in the cluster along the
X axis and in the RR2; We speculate that this is due to not having diverse enough generalization gap in
the models trained to cover the entire space of the “model” unlike in the other two settings.
	lower fence	Qi	Q2	Q3	upper fence
"h0^	-45.67-	16.67	6.97	1.71	0.68
~1Γ	-58.84-	88.14	44.15	15.59	9.36
~2Γ	-60.20-	78.57	35.76	12.89	7.52
T3~	59.75	0.27	1.192	7.37	44.22 一
	lower fence	Qi	Q2	Q3	upper fence
"h0^	1.30e-10	6.25e-5	8.88e-3	0.192	040
~1Γ	5.94e-13	9.33e-18	2.47e-10	1.06e-4	-2.49e-3-
~2Γ	3.45e-13	3.04e-16	9.21e-9	4.07e-4	-6.59e-3-
	4.14e-13	0.60	0.27	7.14e-3	2.4e-10
Table 6: F score (top) and p-values (bottom) for all 20 variables. Using p = 0.05, We see that the null
hypotheses are not rejected for 4 of the variables. We believe having a more diverse generalization
behavior in the study Will solve this problem.
16
Published as a conference paper at ICLR 2019
8.3 ResNet 32 + CIFAR- 1 00 + All Quartile Signature
Figure 7:	Residual plots for all explanatory variables, row: h0, h1, h2, h3, column: lower fence, Q1,
Q2, Q3, upper fence. lower fence is clipped because distance cannot be smaller than 0. The residual
is fairly evenly distributed around 0. There is one outlier in this experimental setting as shown in the
plots.
			lower fence		Qi	Q2	Q3	upper fence		
	^h0^		80.12		8.40	59.62	141.56	-24877-		
	^hT		65.24		109.86	343.57	700.91	-1124.43-		
			99.06		15.47	122.36	305.88	-512.69-		
	ZhE		244.07		128.45	65.58	28.10	2.34		
		lower fence		Qi		Q2^^	Q3		upper fence	
T0^		2.85e-17		4.00e-3		1.46e-13	2.65e-27		6.32e-42	
^hΓ		1.34e-14		2.60e-22		1.04e-52	8.12e-83		4.55e-107	
^hF		1.59e-20		1.03e-4		2.53e-24	1.29e-48		1.42e-68	
ɪ		2.40e-41 -		2.78e-25		1.16e-14	2.13e-7		0.127 一	
Table 7: F score (top) and p-values (bottom) for all 20 variables. Using p = 0.05, the null hypotheses
are rejected for every variable except for h3 upper fence.
17
Published as a conference paper at ICLR 2019
9 Appendix: Some Observations and Conjectures
Everythig here uses the full quartile description.
9.1	Cross Architecture Comparison
We perform regression analysis with both base CNN and ResNet32 on CIFAR-10. The resulting
R2 = 0.91 and the k-fold R2 = 0.88. This suggests that the same coefficient works generally well
across architectures provided they are trained on the same data. Somehow, the distribution at the 3
locations of the networks are comparable even though the depths are vastly different.
Figure 8: Scatter Plots
9.2	Cross Dataset Comparison
We perform regression analysis with ResNet32 on both CIFAR-10 and CIFAR-100. The resulting
R2 = 0.96 and the k-fold R2 = 0.95. This suggests that the same coefficient works generally well
across dataset of the same architecture.
Figure 9: Scatter Plots
18
Published as a conference paper at ICLR 2019
9.3	Cross Everything
We join all our experiment data and the resulting The resulting R2 = 0.93 and the k-fold R2 = 0.93.
It is perhaps surprising that a set of coefficient exists across both datasets and architectures.
Figure 10: Scatter Plots
9.4	Implications on Generalization Bounds
We believe that the method developed here can be used in complementary with existing generalization
bound; more sophisticated engineering of the predictor may be used to actually verify what kind
of function the generalization bound should look like up to constant factor or exponents; it may be
helpful for developing generalization bound tighter than the existing ones.
19