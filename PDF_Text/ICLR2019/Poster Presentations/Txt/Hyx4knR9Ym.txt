Published as a conference paper at ICLR 2019
Generalizable Adversarial Training via Spec-
tral Normalization
Farzan Farnia*, Jesse M. Zhang*, David N. Tse
Department of Electrical Engineering
Stanford University
{farnia,jessez,dntse}@stanford.edu
Ab stract
Deep neural networks (DNNs) have set benchmarks on a wide array of super-
vised learning tasks. Trained DNNs, however, often lack robustness to minor
adversarial perturbations to the input, which undermines their true practicality.
Recent works have increased the robustness of DNNs by fitting networks using
adversarially-perturbed training samples, but the improved performance can still be
far below the performance seen in non-adversarial settings. A significant portion
of this gap can be attributed to the decrease in generalization performance due to
adversarial training. In this work, we extend the notion of margin loss to adver-
sarial settings and bound the generalization error for DNNs trained under several
well-known gradient-based attack schemes, motivating an effective regularization
scheme based on spectral normalization of the DNN’s weight matrices. We also
provide a computationally-efficient method for normalizing the spectral norm of
convolutional layers with arbitrary stride and padding schemes in deep convolu-
tional networks. We evaluate the power of spectral normalization extensively on
combinations of datasets, network architectures, and adversarial training schemes.
1	Introduction
Despite their impressive performance on many supervised learning tasks, deep neural networks
(DNNs) are often highly susceptible to adversarial perturbations imperceptible to the human eye
(Szegedy et al., 2013; Goodfellow et al., 2014b). These “adversarial attacks" have received enormous
attention in the machine learning literature over recent years (Goodfellow et al., 2014b; Moosavi Dez-
fooli et al., 2016; Carlini & Wagner, 2016; Kurakin et al., 2016; Papernot et al., 2016; Carlini &
Wagner, 2017; PaPernot et al., 2017; Madry et al., 2018; Tramer et al., 2θ18). Adversarial attack Stud-
ies have mainly focused on developing effective attack and defense schemes. While attack schemes
attemPt to mislead a trained classifier via additive Perturbations to the inPut, defense mechanisms
aim to train classifiers robust to these Perturbations. Although existing defense methods result in
considerably better Performance comPared to standard training methods, the imProved Performance
can still be far below the Performance in non-adversarial settings (Athalye et al., 2018; Schmidt et al.,
2018).
A standard adversarial training scheme involves fitting a classifier using adversarially-Perturbed
samPles (Szegedy et al., 2013; Goodfellow et al., 2014b) with the intention of Producing a trained
classifier with better robustness to attacks on future (i.e. test) samPles. Madry et al. (2018) Provides a
robust oPtimization interPretation of the adversarial training aPProach, demonstrating that this strategy
finds the oPtimal classifier minimizing the average worst-case loss over an adversarial ball centered
at each training samPle. This minimax interPretation can also be extended to distributionally-robust
training methods (Sinha et al., 2018) where the offered robustness is over a Wasserstein-ball around
the emPirical distribution of training data.
Recently, Schmidt et al. (2018) have shown that standard adversarial training Produces networks that
generalize Poorly. The Performance of adversarially-trained DNNs over test samPles can be signifi-
cantly worse than their training Performance, and this gaP can be far greater than the generalization
* Equal Contributors
1
Published as a conference paper at ICLR 2019
Figure 1: Adversarial training performance with and without spectral normalization (SN) for AlexNet
fit on CIFAR10. The gain in the final test accuracies for FGM, PGM, and WRM after spectral
normalization are 0.09, 0.11, and 0.04, respectively (see Table 1 in the Appendix). For FGM and
PGM, perturbations have `2 magnitude 2.44.
gap achieved using standard empirical risk minimization (ERM). This discrepancy suggests that the
overall adversarial test performance can be improved by applying effective regularization schemes
during adversarial training.
In this work, we propose using spectral normalization (SN) (Miyato et al., 2018) as a computationally-
efficient and statistically-powerful regularization scheme for adversarial training of DNNs. SN
has been successfully implemented and applied for DNNs in the context of generative adversarial
networks (GANs) (Goodfellow et al., 2014a), resulting in state-of-the-art deep generative models
for several benchmark tasks (Miyato et al., 2018). Moreover, SN (Tsuzuku et al., 2018) and other
similar Lipschitz regularization techniques (Cisse et al., 2017) have been successfully applied in
non-adversarial training settings to improve the robustness of ERM-trained networks to adversarial
attacks. The theoretical results in (Bartlett et al., 2017; Neyshabur et al., 2017a) and empirical results
in (Yoshida & Miyato, 2017) also suggest that SN can close the generalization gap for DNNs in
non-adversarial ERM setting.
On the theoretical front, we extend the standard notion of margin loss to adversarial settings. We
leverage the PAC-Bayes generalization framework (McAllester, 1999) to prove generalization bounds
for spectrally-normalized DNNs in terms of our defined adversarial margin loss. We obtain adversarial
generalization error bounds for three well-known gradient-based attack schemes: fast gradient method
(FGM) (Goodfellow et al., 2014b), projected gradient method (PGM) (Kurakin et al., 2016), and
Wasserstein risk minimization (WRM) (Sinha et al., 2018). Our theoretical analysis shows that the
adversarial generalization error will vanish by applying SN to all layers.
On the empirical front, we show that SN can significantly improve the test performance of
adversarially-trained DNNs. We perform numerical experiments over various standard datasets
and DNN architectures. In almost all of our experiments, we obtain a better test performance after
applying SN. For example, Figure 1 shows the training and validation performance for AlexNet
fit on the CIFAR10 dataset using FGM, PGM, and WRM, resulting in adversarial test accuracy
improvements of 9, 11, and 4 percent, respectively. To perform our numerical experiments, we
develop a computationally-efficient approach for normalizing the spectral norm of convolution layers
with arbitrary stride and padding schemes. To summarize, the main contributions of this work are:
1.	Proposing SN as a regularization scheme for adversarial training of DNNs,
2.	Extending concepts of margin-based generalization analysis to adversarial settings and
proving margin-based generalization bounds for three gradient-based adversarial attack
schemes,
3.	Developing an efficient method for normalizing the spectral norm of convolutional layers in
deep convolution networks,
4.	Numerically demonstrating the improved test and generalization performance of DNNs
trained with SN.
2
Published as a conference paper at ICLR 2019
2	Preliminaries
In this section, we first review some standard concepts of margin-based generalization analysis in
learning theory. We then extend these notions to adversarial training settings.
2.1	Supervised learning, Deep neural networks, Generalization error
Consider samples {(x1, y1), . . . , (xn, yn)} drawn i.i.d from underlying distribution PX,Y . We
suppose X ∈ X and Y ∈ {1, 2, . . . , m} where m represents the number of different labels. Given
loss function ` and function class F = {fw, w ∈ W} parameterized by w, a supervised learner aims
to find the optimal function in F minimizing the expected loss (risk) averaged over the underlying
distribution P .
We consider Fnn as the class of d-layer neural networks with h hidden units per layer and activation
functions σ : R → R. Each fw : X → Rm in Fnn maps a data point x to an m-dimensional vector.
Specifically, We can express each fw ∈ Fnn as fw(x) = Wdσ(Wd-ι …σ(Wιx)…)).We use
kWi k2 to denote the spectral norm of matrix Wi , defined as the largest singular value of Wi , and
kWikF to denote Wi ’s Frobenius norm.
A classifier fw’s performance over the true distribution of data can be different from the training
performance over the empirical distribution of training samples P. The difference between the
empirical and true averaged losses, evaluated on respectively training and test samples, is called
the generalization error. Similar to Neyshabur et al. (2017a), we evaluate a DNN’s generalization
performance using its expected margin loss defined for margin parameter γ > 0 as
Lγ(fw) :=P	fw(X)[Y] ≤γ+maxfw(X)[j] ,
(1)
where fw(X)[j] denotes the jth entry of fw(X) ∈ Rm. For a given data point X, we predict the
label corresponding to the maximum entry of fw(X). Also, we use Lγ(fw) to denote the empirical
margin loss averaged over the training samples. The goal of margin-based generalization analysis is
to provide theoretical comparison between the true and empirical margin risks.
2.2	Adversarial attacks, Adversarial training
A supervised learner observes only the training samples and hence does not know the true distribution
of data. Then, a standard approach to train a classifier is to minimize the empirical expected loss `
over function class F = {fw : w ∈ W}, which is
1n
min, - £ ' (fw (xi), yi).	(2)
w∈W n
i=1
This approach is called empirical risk minimization (ERM). For better optimization performance,
the loss function ` is commonly chosen to be smooth. Hence, 0-1 and margin losses are replaced by
smooth surrogate loss functions such as the cross-entropy loss. However, we still use the margin loss
as defined in (1) for evaluating the test and generalization performance of DNN classifiers.
While ERM training usually achieves good performance over DNNs, several recent observations
reveal that adding some adversarially-chosen perturbation to each sample can significantly drop
the trained DNN's performance. Given norm function ∣∣ ∙ ∣∣ and adversarial noise power e > 0, the
adversarial additive noise for sample (x, y) and classifier fw is defined to be
δwdv(x) := argmax '(fw(x + δ),y).	(3)
kδk≤
To provide adversarial robustness against the above attack scheme, a standard technique, which is
called adversarial training, follows ERM training over the adversarially-perturbed samples by solving
1n	1n
w∈iw n X' (fw(χi+δwdv(Xi))，G= w∈iw n X ∣∣m隆 '(fw( Xi+/ y∙	(4)
i=1	i=1 k ik≤
However, (3) and (4) are intractable optimization problems. Therefore, several schemes have been
proposed in the literature to approximate the optimal solution of (3). In this work, we analyze the
3
Published as a conference paper at ICLR 2019
generalization performance of the following three gradient-based methods for approximating the
solution to (3). We note that several other attack schemes such as DeepFool (Moosavi Dezfooli et al.,
2016), CW attacks (Carlini & Wagner, 2017), target and least-likely attacks (Kurakin et al., 2016)
have been introduced and examined in the literature, which can lead to interesting future directions
for this work.
1.	Fast Gradient Method (FGM) (Goodfellow et al., 2014b): FGM approximates the solution
to (3) by considering a linearized DNN loss around a given data point. Hence, FGM perturbs (x, y)
by adding the following noise vector:
δWgm(x) :=argmax δτ NX '(fw (x),y)∙	(5)
kδk≤
For the special case of '∞-norm ∣∣ ∙ ∣∣∞, the above representation of FGM recovers the fast gradient
sign method (FGSM) where each data point (x, y) is perturbed by the -normalized sign vector of
the loss,s gradient. For '2-norm ∣∣ ∙ ∣∣2, We similarly normalize the loss,s gradient vector to have E
Euclidean norm.
2.	Projected Gradient Method (PGM) (Kurakin et al., 2016): PGM is the iterative version of
FGM and applies projected gradient descent to solve (3). PGM folloWs the folloWing update rules for
a given r number of steps:
∀1 ≤ i ≤ r : δpgm,i+1(x) := Y {δ皆m，i(x) + ανW) },	(6)
Be,k∙k(0)
Vw) := argmax δτNX '(fw(X + δWgm,i(x)), y).
kδk≤1
(i)
Here, We first find the direction νw along Which the loss at the ith perturbed point changes the most,
and then We move the perturbed point along this direction by stepsize α folloWed by projecting the
resulting perturbation onto the set {δ : ∣δ∣ ≤ E} With E-bounded norm.
3. Wasserstein Risk Minimization (WRM) (Sinha et al., 2018): WRM solves the folloWing
variant of (3) for data-point (X, y) Where the norm constraint in (3) is replaced by a norm-squared
Lagrangian penalty term:
δWWrm(x) := argmax '(fw(x + δ),y)一λ∣∣δ∣2.	(7)
δ2
As discussed earlier, the optimization problem (3) is generally intractable. HoWever, in the case of
Euclidean norm ∣∣ ∙ ∣∣2, if we assume Nχ'(fw(x), y)'s Lipschitz constant is upper-bounded by λ, then
WRM optimization (7) results in solving a convex optimization problem and can be efficiently solved
using gradient methods.
To obtain efficient adversarial defense schemes, we can substitute δwfgm, δwpgm , or δwwrm for δwadv in (4).
Instead of fitting the classifier over true adversarial examples, which are NP-hard to obtain, we can
instead train the DNN over FGM, PGM, or WRM-adversarially perturbed samples.
2.3 Adversarial generalization error
The goal of adversarial training is to improve the robustness against adversarial attacks on not only
the training samples but also on test samples; however, the adversarial training problem (4) focuses
only on the training samples. To evaluate the adversarial generalization performance, we extend the
notion of margin loss defined earlier in (1) to adversarial training settings by defining the adversarial
margin loss as
LYdv (fw )= PfX + δWdv(X))[Y ] ≤ Y + maχ fw (X + δf(X))j]).	(8)
Here, we measure the margin loss over adversarially-perturbed samples, and we use Lbaγdv(fw ) to
denote the empirical adversarial margin loss. We also use Lfγgm(fw ), Lγpgm(fw ), and Lγwrm(fw ) to
denote the adversarial margin losses with FGM (5), PGM (6), and WRM (7) attacks, respectively.
4
Published as a conference paper at ICLR 2019
3 Margin-based adversarial Generalization bounds
As previously discussed, generalization performance can be different between adversarial and non-
adversarial settings. In this section, we provide generalization bounds for DNN classifiers under
adversarial attacks in terms of the spectral norms of the trained DNN’s weight matrices. The bounds
motivate regularizing these spectral norms in order to limit the DNN’s capacity and improve its
generalization performance under adversarial attacks.
We use the PAC-Bayes framework (McAllester, 1999; 2003) to prove our main results. To derive
adversarial generalization error bounds for DNNs with smooth activation functions σ, we first extend
a recent result on the margin-based generalization bound for the ReLU activation function (Neyshabur
et al., 2017a) to general 1-Lipschitz activation functions.
Theorem 1. Consider Fnn = {fw : w ∈ W} the class of d hidden-layer neural networks with h
units per hidden-layer with 1-Lipschitz activation σ satisfying σ(0) = 0. Suppose that X, X’s support
set, is norm-bounded as kxk2 ≤ B, ∀x ∈ X. Also assume for constant M ≥ 1 any fw ∈ Fnn
satisfies
∀i : MM ≤kWik2 ≤ M,	βw := (Y kWi∣∣2)1∕d.
M	βw
i=1
Here βw denotes the geometric mean of fw ’s spectral norms across all layers. Then, for any η, γ > 0,
with probability at least 1 - η for any fw ∈ Fnn we have:
(SB2d2hlog(dh)Φerm(fw)+dlog ^ngM∖
L0fw)≤ LYfw) + Oiy	γ2n	),
where we define complexity score Φerm(fw):= (Qd=IIIWiI图 Pd=I ∣W⅛∙
Proof. We defer the proof to the Appendix.
□
We now generalize this result to adversarial settings where the DNN’s performance is evaluated under
adversarial attacks. We prove three separate adversarial generalization error bounds for FGM, PGM,
and WRM attacks.
For the following results, we consider Fnn, the class of neural nets defined in Theorem 1. Moreover,
We assume that the training loss '(y, y) and its first-order derivative are I-LiPschitz. Similar to Sinha
et al. (2018), we assume the activation σ is smooth and its derivative σ0 is 1-Lipschitz. This class of
activations include ELU (Clevert et al., 2015) and tanh functions but not the ReLU function. HoWever,
our numerical results in Table 1 from the APPendix suggest similar generalization Performance
betWeen ELU and ReLU activations.
Theorem 2. Consider Fnn , X in Theorem 1 and training loss function ` satisfying the assumptions
stated above. We consider an FGM attack with noise power E according to Euclidean norm ∣∣ ∙ ∣∣2.
For any fw ∈ Fnn assume K ≤ ∣∣Vχ'(∕w (x), y) ∣∣2 holds for constant κ > 0, any y ∈ Y, and any
x ∈ B,∣∣∙k2 (X) E-close to X ,s support set. Then, for any η, Y > 0 with probability 1 一 η thefollowing
bound holds for the FGM margin loss of any fw ∈ Fnn
Lf0gm(fw) ≤ Lbfγgm(fw) +O
(B + e)2d2hlog(dh) Φfgm(fw) + dlog TM
γ2n
where Φ矗 fw) := {Q3 ∣Wik2(l+(E/K)(Qd=IkWik2) Pd=IQj=IkWj g)}2 P= ⅛∣
Proof. We defer the Proof to the APPendix.
□
Note that the above theorem assumes that the change rate for the loss function around test samPles is
at least K, Which gives a baseline for measuring the attack PoWer E. In our numerical exPeriments, We
validate this assumPtion over standard image recognition tasks. Next, We generalize this result to
adversarial settings With PGM attack, i.e. the iterative version of FGM attack.
5
Published as a conference paper at ICLR 2019
Theorem 3. Consider Fnn , X and training loss function ` for which the assumptions in Theorem 2
hold. We consider a PGM attack with noise power E given Euclidean norm ∣∣ ∙ ∣∣2, r iterations for
attack, and stepsize α. Then, for any η, γ > 0 with probability 1 - η the following bound applies to
the PGM margin loss of any fw ∈ Fnn
L0pgm(fw) ≤ Lbγpgm(fw) +O
(B + e)2d2hlog(dh) ΦP%,α(fw) + dlog 也詈M
γ2 n
Here we define Φp,gκm,r,α(fw) as the following expression
iY=d1
∣Wi∣2 (l + (α∕κ)
1 一 (2α∕κ)rlip(V' ◦ fw)r
1 - (2α∕κ)lip(V' ◦ fw)
d	di	2
(YkWik2) X Y IWjk2)
i=1	i=1 j=1
d
X
i=1
∣WikF
kWik2 ,
where lip(V' ◦ fw):= (Qd=IkWik2) Pd=ι Qj=IkWj ∣∣2 provides an upper-bound on the Lips-
chitz constant of Vχ'(∕w (x), y).
Proof. We defer the proof to the Appendix.
□
In the above result, notice that if lip(V' ◦ fw)∕κ < 1∕(2a) then for any number of gradient steps the
PGM margin-based generalization bound will grow the FGM generalization error bound in Theorem
2 by factor 1∕(1 一 (2ɑ∕κ)lip(V' ◦ fw)). We next extend our adversarial generalization analysis to
WRM attacks.
Theorem 4. For neural net class Fnn and training loss ` satisfying Theorem 2’s assumptions, consider
a WRM attack with Lagrangian coefficient λ and Euclidean norm k ∙ ∣∣2. Given parameter 0 < τ < 1,
assume lip(V' ◦ fw) defined in Theorem 3 is upper-bounded by λ(1 — T) for any fw ∈ Fnn. For any
η > 0, the following WRM margin-based generalization bound holds with probability 1 - η for any
fw ∈ Fnn :
L0wrm(fw) ≤ Lbγwrm(fw) + O
(B + 1 Qi=ι kWik2)2d2hlog(dh)ΦWrm(fw) + dlog dnTηM
γ2n
where we define
d	d	di	d	2
φwrmfw ) := {∏ kWik2 (1 + λ 一而(V' ◦ fw ) (YkWik2) X Π kWj k“}2 X Mf
Proof. We defer the proof to the Appendix.
□
As discussed by Sinha et al. (2018), the condition lip(V' ◦ fw) < λ for the actual Lipschitz constant
of V' ◦ fw is in fact required to guarantee WRM,s convergence to the global solution. Notice that
the WRM generalization error bound in Theorem 4 is bounded by the product of 工_而(！`f)and
the FGM generalization bound in Theorem 2.
4	Spectral normalization of convolutional layers
To control the Lipschitz constant of our trained network, we need to ensure that the spectral norm
associated with each linear operation in the network does not exceed some pre-specified β. For
fully-connected layers (i.e. regular matrix multiplication), please see Appendix B. For a general class
of linear operations including convolution, Tsuzuku et al. (2018) propose to compute the operation’s
spectral norm through computing the gradient of the Euclidean norm of the operation’s output. Here,
we leverage the deconvolution operation to further simplify and accelerate computing the spectral
norm of the convolution operation. Additionally, Sedghi et al. (2018) develop a method for computing
all the singular values including the largest one, i.e. the spectral norm. While elegant, the method only
applies to convolution filters with stride 1 and zero-padding. However, in practice the normalization
factor depends on the stride size and padding scheme governing the convolution operation. Here
6
Published as a conference paper at ICLR 2019
we develop an efficient approach for computing the maximum singular value, i.e. spectral norm, of
convolutional layers with arbitary stride and padding schemes. Note that, as also discussed by Gouk
et al. (2018), the ith convolutional layer output feature map ψi is a linear operation of the input X :
M
ψi(X) = X Fi,j ? Xj,
j=1
where X has M feature maps, Fi,j is a filter, and ? denotes the convolution operation (which also
encapsulates stride size and padding scheme). For simplicity, we ignore the additive bias terms here.
By vectorizing X and letting Vi,j represent the overall linear operation associated with Fi,j, we see
that
ψi(X) = [V1,1 . . . V1,M] X,
and therefore the overall convolution operation can be described using
V1,1 ... V1,M
ψ(X) =	.	...	. X = WX.
VN,1 . . . VN,M
While explicitly reconstructing W is expensive, we can still compute σ(W), the spectral norm of W,
by leveraging the convolution transpose operation implemented by several modern-day deep learning
packages. This allows us to efficiently performs matrix multiplication with WT without explicitly
constructing W. Therefore we can approximate σ(W) using a modified version of power iteration
(Algorithm 1), wrapping the appropriate stride size and padding arguments into the convolution and
convolution transpose operations. After obtaining σ(W), we compute WSN in the same manner as for
the fully-connected layers. Like Miyato et al., we exploit the fact that SGD only makes small updates
to W from training step to training step, reusing the same U and running only one iteration per step.
Unlike Miyato et al., rather than enforcing σ(W) = β, we instead enforce the looser constraint
σ(W) ≤ β:
WSN = W/ max(1,σ(W )∕β),	(9)
which we observe to result in faster training for supervised learning tasks.
Algorithm 1 Convolutional power iteration
Initialize U with a random vector matching the shape of the convolution input
for t = 0, ..., T - 1 do
V — Conv(W, U)∕k Conv(W, U)k2
U J conv_transpose(W, V)/kconv_transpose(W, V)k2
end for
σ J V ∙ Conv(W, U)
5	Numerical Experiments
In this section we provide an array of empirical experiments to validate both the bounds we derived
in Section 3 and our implementation of spectral normalization described in section 4. We show that
spectral normalization improves both test accuracy and generalization for a variety of adversarial
training schemes, datasets, and network architectures.
All experiments are implemented in TensorFlow (Abadi et al., 2016). For each experiment, we cross
validate 4 to 6 values of β (see (9)) using a fixed validation set of 500 samples. For PGM, we used
r = 15 iterations and α = 2e∕r. Additionally, for FGM and PGM we used '2-type attacks (unless
specified) with magnitude E = 0.05EP [∣∣X∣∣2] (this value was approximately 2.44 for CIFAR10).
For WRM, we implemented gradient ascent as discussed by Sinha et al. (2018). Additionally, for
WRM training we used a Lagrangian coefficient of 0.002EP [kXk2] for CIFAR10 and SVHN and a
Lagrangian coefficient of 0.04EP [k X∣∣ 2] for MNIST in a similar manner to Sinha et al. (2018). The
code will be made readily available.
7
Published as a conference paper at ICLR 2019
5.1	Validation of spectral normalization implementation and bounds
We first demonstrate the effect of the proposed spectral normalization approach on the final DNN
weights by comparing the `2 norm of the input x to that of the output fw(x). As shown in Figure 2(a),
without spectral normalization (β = ∞ in (9)), the norm gain can be large. Additionally, because
we are using cross-entropy loss, the weights (and therefore the norm gain) can grow arbitrarily high
if we continue training as reported by Neyshabur et al. (2017b). As we decrease β, however, we
produce more constrained networks, resulting in a decrease in norm gain. At β = 1, the gain of
the network cannot be greater than 1, which is consistent with what we observe. Additionally, we
provide a comparison of our method to that of Miyato et al. (2018) in Appendix A.1, empirically
demonstrating that Miyato et al.’s method does not properly control the spectral norm of convolutional
layers, resulting in worse generalization performance.
Figure 2(b) shows that the `2 norms of the gradients with respect to the training samples are nicely
distributed after spectral normalization. Additionally, this figure suggests that the minimum gradient
'2-norm assumption (the K condition in Theorems 2 and 3) holds for spectrally-normalized networks.
The first column of Figure 3 shows that, as observed by Bartlett et al. (2017), AlexNet trained using
ERM generates similar margin distributions for both random and true labels on CIFAR10 unless we
normalize the margins appropriately. We see that even without further correction, ERM training with
SN allows AlexNet to have distinguishable performance between the two datasets. This observation
suggests that SN as a regularization scheme enforces the generalization error bounds shown for
spectrally-normalized DNNs by Bartlett et al. (2017) and Neyshabur et al. (2017a). Additionally, the
margin normalization factor (the capacity norm Φ in Theorems 1-4) is much smaller for networks
trained with SN. As demonstrated by the other columns in Figure 3, a smaller normalization factor
results in larger normalized margin values and much tighter margin-based generalization bounds (a
factor of 102 for ERM and a factor of 105 for FGM and PGM) (see Theorems 1-4).
(a) `2 norm gain due to the network f trained
using ERM.
Figure 2: Validation of SN implementation and distribution of the gradient norms using AlexNet
trained on CIFAR10.
Mtq(X))Il 2	∣Vχf(⅞v(χ))∣∣2
(b) Distributions of the `2 norms of the gradi-
ents with respect to training samples. Train-
ing regularized with SN.
5.2	Spectral normalization improves generalization and adversarial
ROBUSTNESS
The phenomenon of overfitting random labels described by Zhang et al. (2016) can be observed even
for adversarial training methods. Figure 4 shows how the FGM, PGM, or WRM adversarial training
schemes only slightly delay the rate at which AlexNet fits random labels on CIFAR10, and therefore
the generalization gap can be quite large without proper regularization. After introducing spectral
normalization, however, we see that the network has a much harder time fitting both the random and
true labels. With the proper amount of SN (chosen via cross validation), we can obtain networks that
struggle to fit random labels while still obtaining the same or better test performance on true labels.
We also observe that training schemes regularized with SN result in networks more robust to
adversarial attacks. Figure 5 shows that even without adversarial training, AlexNet with SN becomes
more robust to FGM, PGM, and WRM attacks. Adversarial training improves adversarial robustness
8
Published as a conference paper at ICLR 2019
Figure 3: Effect of SN on distributions of unnormalized (leftmost column) and normalized (other
three columns) margins for AlexNet fit on CIFAR10. The normalization factor is described by the
capacity norm Φ reported in Theorems 1-4.
more than SN by itself; however we see that we can further improve the robustness of the trained
networks significantly by combining SN with adversarial training.
Figure 4: Fitting random and true labels on CIFAR10 with AlexNet using adversarial training.
0.5
0.4
0.3
0.2
0	2	4
ERM training
WRM attacks
WRM training
WRM attacks
Figure 5: Robustness of AlexNet trained on CIFAR10 to various adversarial attacks.
5.3	Other datasets and architectures
We demonstrate the power of regularization via SN on several combinations of datasets, network
architectures, and adversarial training schemes. The datasets we evaluate are CIFAR10, MNIST, and
SVHN. We fit CIFAR10 using the AlexNet and Inception networks described by Zhang et al. (2016),
1-hidden-layer and 2-hidden-layer multi layer perceptrons (MLPs) with ELU activation and 512
hidden nodes in each layer, and the ResNet architecture (He et al. (2016)) provided in TensorFlow
9
Published as a conference paper at ICLR 2019
for fitting CIFAR10. We fit MNIST using the ELU network described by Sinha et al. (2018) and the
1-hidden-layer and 2-hidden-layer MLPs. Finally, we fit SVHN using the same AlexNet architecture
we used to fit CIFAR10. Our implementations do not use any additional regularization schemes
including weight decay, dropout (Srivastava et al., 2014), and batch normalization (Ioffe & Szegedy,
2015) as these approaches are not motivated by the theory developed in this work; however, we
provide numerical experiments comparing the proposed approach with weight decay, dropout, and
batch normalization in Appendix A.2.
Table 1 in the Appendix reports the pre and post-SN test accuracies for all 42 combinations evaluated.
Figure 1 in the Introduction and Figures 7-9 in the Appendix show examples of training and validation
curves on some of these combinations. We see that the validation curve generally improves after
regularization with SN, and the observed improvements in validation accuracy are confirmed by the
test accuracies reported in Table 1. Figure 6 visually summarizes Table 1, showing how SN can often
significantly improve the test accuracy (and therefore decrease the generalization gap) for several of
the combinations. We also provide Table 2 in the Appendix which shows the proportional increase in
training time after introducing SN with our TensorFlow implementation.
ERM training
Test acc
0.20
0.15
0.10
0.05
0.00
0.4	0.6	0.8	1.0
Test acc
FGM training
WRM training
0.20-
0.15-
o.ιo-
0.05-
o.oo-
0.4	0.6	0.8	1.0
Test acc
PGM training
0.20
0.15
0.10
0.05
0.00
0.4	0.6	0.8
Test acc
□°π vSo
Figure 6: Test accuracy improvement after SN for various datasets and network architectures.
6 Related Works
Providing theoretical guarantees for adversarial robustness of various classifiers has been studied
in multiple works. Wang et al. (2017) targets analyzing the adversarial robustness of the nearest
neighbor approach. Gilmer et al. (2018) studies the effect of the complexity of the data-generating
manifold on the final adversarial robustness for a specific trained model. Fawzi et al. (2018) proves
lower-bounds for the complexity of robust learning in adversarial settings, targeting the population
distribution of data. Xu et al. (2009) shows that the regularized support vector machine (SVM) can be
interpreted via robust optimization. Fawzi et al. (2016) analyzes the robustness of a fixed classifier to
random and adversarial perturbations of the input data. While all of these works seek to understand
the robustness properties of different classification function classes, unlike our work they do not focus
on the generalization aspects of learning over DNNs under adversarial attacks.
Concerning the generalization aspect of adversarial training, Sinha et al. (2018) provides optimization
and generalization guarantees for WRM under the assumptions discussed after Theorem 4. However,
their generalization guarantee only applies to the Wasserstein cost function, which is different from
the 0-1 or margin loss and does not explicitly suggest a regularization scheme. In a recent related
work, Schmidt et al. (2018) numerically shows the wide generalization gap in PGM adversarial
training and theoretically establishes lower-bounds on the sample complexity of linear classifiers in
Gaussian settings. While our work does not provide sample complexity lower-bounds, we study the
broader function class of DNNs where we provide upper-bounds on adversarial generalization error
and suggest an explicit regularization scheme for adversarial training over DNNs.
Generalization in deep learning has been a topic of great interest in machine learning (Zhang et al.,
2016). In addition to margin-based bounds (Bartlett et al., 2017; Neyshabur et al., 2017a), various
other tools including VC dimension (Anthony & Bartlett, 2009), norm-based capacity scores (Bartlett
& Mendelson, 2002; Neyshabur et al., 2015), and flatness of local minima (Keskar et al., 2016;
Neyshabur et al., 2017b) have been used to analyze generalization properties of DNNs. Recently,
Arora et al. (2018) introduced a compression approach to further improve the margin-based bounds
presented by Bartlett et al. (2017); Neyshabur et al. (2017a). The PAC-Bayes bound has also been
considered and computed by Dziugaite & Roy (2017), resulting in non-vacuous bounds for the
MNIST dataset.
10
Published as a conference paper at ICLR 2019
Acknowledgments
We are grateful for support under the National Science Foundation grant under CCF-1563098, and
the Center for Science of Information (CSoI), an NSF Science and Technology Center under grant
agreement CCF-0939370.
References
Martin Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado,
Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine learning on heterogeneous
distributed systems. arXiv preprint arXiv:1603.04467, 2016.
Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations. cambridge university
press, 2009.
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a
compression approach. arXiv preprint arXiv:1802.05296, 2018.
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security:
Circumventing defenses to adversarial examples. In Proceedings of the 35th International Conference on
Machine Learning, pp. 274-283, 2018.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural
results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural
networks. In Advances in Neural Information Processing Systems, pp. 6241-6250, 2017.
Nicholas Carlini and David Wagner. Defensive distillation is not robust to adversarial examples. arXiv preprint
arXiv:1607.04311, 2016.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017 IEEE
Symposium on Security and Privacy (SP), pp. 39-57. IEEE, 2017.
Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval networks:
Improving robustness to adversarial examples. arXiv preprint arXiv:1704.08847, 2017.
Djork-Arn6 Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by
exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.
Gintare Karolina Dziugaite and Daniel M Roy. Entropy-sgd optimizes the prior of a pac-bayes bound: Data-
dependent pac-bayes priors via differential privacy. arXiv preprint arXiv:1712.09376, 2017.
Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Robustness of classifiers: from
adversarial to random noise. In Advances in Neural Information Processing Systems, pp. 1632-1640, 2016.
Alhussein Fawzi, Hamza Fawzi, and Omar Fawzi. Adversarial vulnerability for any classifier. arXiv preprint
arXiv:1802.08686, 2018.
Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S Schoenholz, Maithra Raghu, Martin Wattenberg, and Ian
Goodfellow. Adversarial spheres. arXiv preprint arXiv:1801.02774, 2018.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing
systems, pp. 2672-2680, 2014a.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples.
arXiv preprint arXiv:1412.6572, 2014b.
Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael Cree. Regularisation of neural networks by enforcing
lipschitz continuity. arXiv preprint arXiv:1804.04368, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In
European conference on computer vision, pp. 630-645. Springer, 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
11
Published as a conference paper at ICLR 2019
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On
large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836,
2016.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv preprint
arXiv:1611.01236, 2016.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep
learning models resistant to adversarial attacks. In International Conference on Learning Representations,
2018.
David McAllester. Simplified pac-bayesian margin bounds. In Learning theory and Kernel machines, pp.
203-215. Springer, 2003.
David A McAllester. Pac-bayesian model averaging. In Proceedings of the twelfth annual conference on
Computational learning theory, pp. 164-170. ACM, 1999.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative
adversarial networks. arXiv preprint arXiv:1802.05957, 2018.
Seyed Mohsen Moosavi Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate
method to fool deep neural networks. In Proceedings of 2016 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), number EPFL-CONF-218057, 2016.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural networks. In
COLT, pp. 1376-1401, 2015.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. A pac-bayesian approach to
spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564, 2017a.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring generalization in deep
learning. In Advances in Neural Information Processing Systems, pp. 5949-5958, 2017b.
Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a defense to
adversarial perturbations against deep neural networks. In 2016 IEEE Symposium on Security and Privacy
(SP), pp. 582-597. IEEE, 2016.
Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami.
Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia Conference
on Computer and Communications Security, pp. 506-519. ACM, 2017.
LudWig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Adversarially
robust generalization requires more data. arXiv preprint arXiv:1804.11285, 2018.
Hanie Sedghi, Vineet Gupta, and Philip M Long. The singular values of convolutional layers. arXiv preprint
arXiv:1805.10408, 2018.
Aman Sinha, Hongseok Namkoong, and John Duchi. Certifiable distributional robustness with principled
adversarial training. In International Conference on Learning Representations, 2018.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A
simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):
1929-1958, 2014.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob
Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Florian Tram3r, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. Ensemble
adversarial training: Attacks and defenses. In International Conference on Learning Representations, 2018.
Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational mathematics,
12(4):389-434, 2012.
Yusuke Tsuzuku, Issei Sato, and Masashi Sugiyama. Lipschitz-margin training: Scalable certification of
perturbation invariance for deep neural networks. arXiv preprint arXiv:1802.04034, 2018.
Yizhen Wang, Somesh Jha, and Kamalika Chaudhuri. Analyzing the robustness of nearest neighbors to
adversarial examples. arXiv preprint arXiv:1706.03922, 2017.
12
Published as a conference paper at ICLR 2019
Huan Xu, Constantine Caramanis, and Shie Mannor. Robustness and regularization of support vector machines.
Journal of Machine Learning Research, 10(Jul):1485-1510, 2009.
Yuichi Yoshida and Takeru Miyato. Spectral norm regularization for improving the generalizability of deep
learning. arXiv preprint arXiv:1705.10941, 2017.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning
requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
13
Published as a conference paper at ICLR 2019
Appendices
A Further experimental results
Table 1: Train and test accuracies before and after spectral normalization for various datasets, network
architectures, and training schemes. The amount of spectral normalization was selected from 4-6
values of β via cross validation on 500 samples. For each row, the greater test accuracy is bolded
(both are bolded in the event of a tie). '∞ adversarial training was performed with magnitude 0.1.
Dataset	Architecture	Training	Train acc	Test acc	Train acc (SN)	Test acc (SN)
CIFAR10	AlexNet	ERM	1.00	0.79	1.00	0.79
CIFAR10	AlexNet	FGM `2	0.98	0.54	0.93	0.63
CIFAR10	AlexNet	FGM '∞	1.00	0.51	0.67	0.56
CIFAR10	AlexNet	PGM `2	0.99	0.50	0.92	0.62
CIFAR10	AlexNet	PGM '∞	0.99	0.44	0.86	0.54
CIFAR10	AlexNet	WRM	1.00	0.61	0.76	0.65
CIFAR10	ELU-AlexNet	ERM	1.00	0.79	1.00	0.79
CIFAR10	ELU-AlexNet	FGM `2	0.97	0.52	0.68	0.60
CIFAR10	ELU-AlexNet	PGM `2	0.98	0.53	0.88	0.61
CIFAR10	ELU-AlexNet	WRM	1.00	0.60	1.00	0.60
CIFAR10	Inception	ERM	1.00	0.85	1.00	0.86
CIFAR10	Inception	PGM `2	0.99	0.53	1.00	0.58
CIFAR10	Inception	PGM '∞	0.98	0.48	0.62	0.56
CIFAR10	Inception	WRM	1.00	0.66	1.00	0.67
CIFAR10	1-layer MLP	ERM	0.98	0.49	0.68	0.53
CIFAR10	1-layer MLP	FGM `2	0.60	0.36	0.60	0.46
CIFAR10	1-layer MLP	PGM `2	0.57	0.36	0.55	0.46
CIFAR10	1-layer MLP	WRM	0.60	0.41	0.62	0.50
CIFAR10	2-layer MLP	ERM	0.99	0.51	0.79	0.56
CIFAR10	2-layer MLP	FGM `2	0.57	0.36	0.66	0.49
CIFAR10	2-layer MLP	PGM `2	0.93	0.35	0.66	0.48
CIFAR10	2-layer MLP	WRM	0.87	0.35	0.73	0.52
CIFAR10	ResNet	ERM	1.00	0.80	1.00	0.83
CIFAR10	ResNet	PGM `2	0.99	0.49	1.00	0.55
CIFAR10	ResNet	PGM '∞	0.98	0.44	0.72	0.53
CIFAR10	ResNet	WRM	1.00	0.63	1.00	0.66
MNIST	ELU-Net	ERM	1.00	0.99	1.00	0.99*
MNIST	ELU-Net	FGM `2	0.98	0.97	1.00	0.97
MNIST	ELU-Net	PGM `2	0.99	0.97	1.00	0.97
MNIST	ELU-Net	WRM	0.95	0.92	0.95	0.93
MNIST	1-layer MLP	ERM	1.00	0.98	1.00	0.98*
MNIST	1-layer MLP	FGM `2	0.88	0.88	1.00	0.96
MNIST	1-layer MLP	PGM `2	1.00	0.96	1.00	0.96
MNIST	1-layer MLP	WRM	0.92	0.88	0.92	0.88
MNIST	2-layer MLP	ERM	1.00	0.98	1.00	0.98
MNIST	2-layer MLP	FGM `2	0.97	0.91	1.00	0.96
MNIST	2-layer MLP	PGM `2	1.00	0.96	1.00	0.97
MNIST	2-layer MLP	WRM	0.97	0.88	0.98	0.90
SVHN	AlexNet	ERM	1.00	0.93	1.00	0.93*
SVHN	AlexNet	FGM `2	0.97	0.76	0.95	0.83
SVHN	AlexNet	PGM `2	1.00	0.78	0.85	0.81
SVHN	AlexNet	WRM	1.00	0.83	0.87	0.84
* β = ∞ (i.e. no spectral normalization) achieved the highest validation accuracy.
14
Published as a conference paper at ICLR 2019
Table 2: Runtime increase after introducing spectral normalization for various datasets, network
architectures, and training schemes. These ratios were obtained by running the experiments on one
NVIDIA Titan Xp GPU for 40 epochs.
Dataset	Architecture	Training	no SN runtime	SN runtime	ratio
CIFAR10	AlexNet	ERM	229 s	283 s	1.24
CIFAR10	AlexNet	FGM `2	407 s	463 s	1.14
CIFAR10	AlexNet	FGM '∞	408 s	465 s	1.14
CIFAR10	AlexNet	PGM `2	2917 s	3077 s	1.05
CIFAR10	AlexNet	PGM '∞	2896 s	3048 s	1.05
CIFAR10	AlexNet	WRM	3076 s	3151 s	1.02
CIFAR10	ELU-AlexNet	ERM	231 s	283 s	1.23
CIFAR10	ELU-AlexNet	FGM `2	410s	466 s	1.14
CIFAR10	ELU-AlexNet	PGM `2	2939 s	3093 s	1.05
CIFAR10	ELU-AlexNet	WRM	3094 s	3150 s	1.02
CIFAR10	Inception	ERM	632 s	734s	1.16
CIFAR10	Inception	PGM `2	9994 s	6082 s	0.61
CIFAR10	Inception	PGM '∞	9948 s	6063 s	0.61
CIFAR10	Inception	WRM	10247 s	6356 s	0.62
CIFAR10	1-layer MLP	ERM	22 s	31 s	1.42
CIFAR10	1-layer MLP	FGM `2	25 s	35 s	1.43
CIFAR10	1-layer MLP	PGM `2	79 s	93 s	1.18
CIFAR10	1-layer MLP	WRM	73 s	86 s	1.18
CIFAR10	2-layer MLP	ERM	23 s	37 s	1.59
CIFAR10	2-layer MLP	FGM `2	27 s	41 s	1.51
CIFAR10	2-layer MLP	PGM `2	91 s	108s	1.19
CIFAR10	2-layer MLP	WRM	85 s	103s	1.21
CIFAR10	ResNet	ERM	315s	547 s	1.73
CIFAR10	ResNet	PGM `2	2994 s	3300 s	1.10
CIFAR10	ResNet	PGM '∞	2980 s	3300 s	1.11
CIFAR10	ResNet	WRM	3187 s	3457 s	1.08
MNIST	ELU-Net	ERM	55 s	97 s	1.76
MNIST	ELU-Net	FGM `2	91 s	136s	1.49
MNIST	ELU-Net	PGM `2	614s	676 s	1.10
MNIST	ELU-Net	WRM	635 s	670 s	1.06
MNIST	1-layer MLP	ERM	15s	24 s	1.60
MNIST	1-layer MLP	FGM `2	17s	27 s	1.57
MNIST	1-layer MLP	PGM `2	57 s	71 s	1.24
MNIST	1-layer MLP	WRM	51 s	63 s	1.24
MNIST	2-layer MLP	ERM	17s	31 s	1.84
MNIST	2-layer MLP	FGM `2	20 s	35 s	1.77
MNIST	2-layer MLP	PGM `2	67 s	89 s	1.32
MNIST	2-layer MLP	WRM	62 s	81 s	1.30
SVHN	AlexNet	ERM	334 s	412s	1.23
SVHN	AlexNet	FGM `2	596 s	676 s	1.13
SVHN	AlexNet	PGM `2	4270 s	4495 s	1.05
SVHN	AlexNet	WRM	4501 s	4572 s	1.02
15
Published as a conference paper at ICLR 2019
AIexNet FGM ∕00 (ε = 0.1) AIexNet PGM EB (ε = 0.1)
AuE.lnuuE
Figure 7: Adversarial training performance with and without spectral normalization for AlexNet fit
on CIFAR10.
AIexNet FGM ∕ββ (£ = 0.3)
AIexNet PGM lta (ε = 0.3)
0.0
0
Inception PGM
0.0
25000 50000 75000	0
training steps
Inception WRM
0.0
25000 50000 75000	0
training steps
ResNet PGM
0.0
25000 50000 75000	0
training steps
ResNet WRM
20000 40000
training steps
Figure 8:	Adversarial training performance with and without spectral normalization for Inception and
ResNet fit on CIFAR10.
ELU-AIexNet FGM
-B- train
valid
train (SN)
O= valid (SN)
ELU-AIexNet PGM
0.0 ɪn--------1-------1--------1-------Γj
0	20000 40000 60000 80000
training steps
0.0 ɪn--------1-------1--------1-------H
0	20000 40000 60000 80000
training steps
ELU-AIexNet WRM
0.0-.	.	.	.	.
0	20000 40000 60000 80000
training steps
Figure 9:	Adversarial training performance with and without spectral normalization for AlexNet with
ELU activation functions fit on CIFAR10.
A.1 Comparison of proposed method to Miyato et al. (2018)’s method
For the optimal β chosen when fitting AlexNet to CIFAR10 with PGM, we repeat the experiment
using the spectral normalization approach suggested by Miyato et al. (2018). This approach performs
spectral normalization on convolutional layers by scaling the convolution kernel by the spectral norm
of the kernel rather than the spectral norm of the overall convolution operation. Because it does not
account for how the kernel can amplify perturbations in a single pixel multiple times (see Section 4),
it does not properly control the spectral norm.
16
Published as a conference paper at ICLR 2019
In Figure 10, we see that for the optimal β reported in the main text, using Miyato et al. (2018)’s
SN method results in worse generalization performance. This is because although we specified
that β = 1.6, the actual β obtained using Miyato et al. (2018)’s method can be much greater
for convolutional layers, resulting in overfitting (hence the training curve quickly approaches 1.0
accuracy). The AlexNet architecture used has two convolutional layers. For the proposed method, the
final spectral norms of the convolutional layers were both 1.60; for Miyato et al. (2018)’s method,
the final spectral norms of the convolutional layers were 7.72 and 7.45 despite the corresponding
convolution kernels having spectral norms of 1.60.
Our proposed method is less computationally efficient in comparison to Miyato et al. (2018)’s
approach because each power iteration step requires a convolution operation rather than a division
operation. As shown in Table 3, the proposed approach is not significantly less efficient with our
TensorFlow implementation.
Figure 10: Adversarial training performance with proposed SN versus Miyato et al. (2018)’s SN
for AlexNet fit on CIFAR10 using PGM. The final train and validation accuracies for the proposed
method are 0.92 and 0.60. The final train and validation accuracies for Miyato et al. (2018)’s are 1.00
and 0.55.
Table 3: Runtime increase of the proposed spectral normalization approach compared to Miyato et al.
(2018)’s approach for CIFAR10 and various network architectures and training schemes. These ratios
were obtained by running the experiments on one NVIDIA Titan Xp GPU for 40 epochs.
Dataset	Architecture	Training	Proposed SN runtime Miyato SN runtime
CIFAR10	AlexNet	ERM	1.11
CIFAR10	AlexNet	FGM `2	1.06
CIFAR10	AlexNet	FGM '∞	1.11
CIFAR10	AlexNet	PGM `2	1.01
CIFAR10	AlexNet	PGM '∞	1.11
CIFAR10	AlexNet	WRM	1.02
CIFAR10	Inception	ERM	0.98
CIFAR10	Inception	PGM `2	1.04
CIFAR10	Inception	PGM '∞	1.06
CIFAR10	Inception	WRM	1.03
17
Published as a conference paper at ICLR 2019
A.2 Comparison of proposed method to weight decay, dropout, and batch
NORMALIZATION
Weight Decay	Dropout
Batch Norm
Figure 11: Adversarial training performance with proposed SN versus batch normalization, weight
decay, and dropout for AlexNet fit on CIFAR10 using PGM. The dropout rate was 0.8, and the
amount of weight decay was 5e-4 for all weights. The leftmost plot is from Figure 1 and compares
final performance of no regularization (train accuracy 1.00, validation accuracy 0.48) to that of SN
(train accuracy 0.92, validation accuracy 0.60). The final train and validation accuracies for batch
normalization are 1.00 and 0.54; the final train and validation accuracies for weight decay are 0.84
and 0.55; and the final train and validation accuracies for dropout are 0.99 and 0.52.
B S pectral normalization of fully-connected layers
For fully-connected layers, We approximate the spectral norm of a given matrix W using the approach
described by Miyato et al. (2018): the poWer iteration method. For each W, We randomly initialize a
vector U and approximate both the left and right singular vectors by iterating the update rules
V J W U ∕kW U∣∣2
U J WTV/kWTV∣∣2.
The final singular value can be approximated with σ(W) ≈ VTWU. Like Miyato et al., We exploit
the fact that SGD only makes small updates to W from training step to training step, reusing the same
U and running only one iteration per step. Unlike Miyato et al., rather than enforcing σ(W) = β, We
instead enforce the looser constraint σ(W) ≤ β as described by Gouk et al. (2018):
WSN = W/ max(1,σ(W )∕β),
which we observe to result in faster training in practice for supervised learning tasks.
C Proofs
C.1 Proof of Theorem 1
First let us quote the folloWing tWo lemmas from (Neyshabur et al., 2017a).
Lemma 1 (Neyshabur et al. (2017a)). Consider Fnn = {fw : w ∈ W} as the class of neural nets
parameterized by w where each fw maps input x ∈ X to Rm. Let Q be a distribution on parameter
vector chosen independently from the n training samples. Then, for each η > 0 with probability
at least 1 - η for any w and any random perturbation U satisfying Pru maxx∈X kfw+u(x) -
fw(x)k∞ ≤ 4) ≥ 2 we have
L0 (fw) ≤ Lbγ (fw) + 4
KL(Pw+u kQ) + log 6n
(10)
n-1
18
Published as a conference paper at ICLR 2019
Lemma 2 (Neyshabur et al. (2017a)). Consider a d-layer neural net fw with 1-Lipschitz activation
function σ where σ(0) = 0. Then for any norm-bounded input kxk2 ≤ B and weight perturbation
u : kUi k2 ≤ d IlWik2, we have the following perturbation bound:
kfw+u(x) - fw(x)k2 ≤ eB(Y kWiI。XX ∣≡⅛.
(11)
To prove Theorem 1, consider fw with weights W. Since (1 + d)d ≤ e and 1 ≤ (1 一 d)d-1, for any
weight vector W such that IkWik2 -IlWi∣∣2∣ ≤ dIWf i I2 for every i we have:
dd	d
(1∕e)dd1 ∏IWik2 ≤ ∏IWik2 ≤ e ∏kWik2.	(12)
i=1	i=1	i=1
We apply Lemma 1, choosing Q to be a zero-mean multivariate Gaussian distribution with diagonal
f
covariance matrix, where each entry of the ith layer Ui has standard deviation ξi = k .：k2 ξ with
ξ chosen later in the proof. Note that βw defined earlier in the theorem is the geometric average of
spectral norms across all layers. Then for the ith layer,s random perturbation vector Ui 〜 N(0,ξi2I),
we get the following bound from (Tropp, 2012) with h representing the width of the ith hidden layer:
Pr βwe
kUik2
..-~~^ ..
kWf ik2
t2
>t) ≤2h exp(-砺).
(13)
We now use a union bound over all layers for a maximum union probability of 1∕2, which implies the
normalized βw kkUikk2 for each layer is upper-bounded by ξ，2h log(4hd). Then for any W satisfying
kWi k2
IkWik2 -kW ik2∣ ≤ d kW i k2 for all i's
kma≤B kfw+u(x)- fw (x)k2 ≤ eB(∏ 凶0± ≡2
d
d
e2Bβwde-1 X βwe
i=1
kUik2
i=1
kWik2
kUik2
..ʃ-~~^ ..
kWf ik2
≤ e2dBβw-1ξ√2hlog(4hd).
(14)
Σ
-—■
Here (a) holds, since TWI^ Qd=IkWik2 ≤ ffζ-l Qd=IkWik2 is true for each j. Hence we choose
j	kWjk
ξ =-------d IY , for which the perturbation vector satisfies the assumptions of Lemma 2.
30dBβwde-1 h log(4hd)
Then, we bound the KL-divergence term in Lemma 1 as
KL(Pw+ukQ) ≤ XX kWWkF
302d2 B2βWdh log(4hd)
X IWikF
i=1 kWik2
(b 302e2d2B2 Qd=IkWik2hlog(4hd) XX ∣WikF
≤	2γ2	= ∣Wik2
O (d2B2hlog(hd) Qi=IkWik2
γ2
X kWikF
⅛ IWI
II
Note that (b) holds, because we assume IkWik2 - ∣∣Wi∣∣2∣ ≤ d∣∣Wik2 implying
f jk	Qi=IkWik2 ≤ (I-	d)	(d	1) kW jk Qi=IkWik2 ≤	kW jk	Qi=IkWik2 for each j∙ There-
fore, Lemma 1 implies with probability 1 - η we have the following bound hold for any W satisfying
19
Published as a conference paper at ICLR 2019
IkWik2-W ik2∣ ≤ d kW i k2 for all i's,
r " ∖zb "SB2d2h IOgmh)φermfw) + log 号 ∖
LOfW) ≤ LY f w)+OIy-------γ2n---------).
(15)
Then, we can give an upper-bound over all the functions in Fnn by finding the covering number of the
set of W's where for each feasible W We have the mentioned condition satisfied for at least one of W's.
We only need to form the bound for (ɪ )1/d ≤ βw ≤ ((YB )1/d which can be covered using a cover
of size dn1/2d as discussed in (Neyshabur et al., 2017a). Then, from the theorem's assumption we
know each IlWik2 will be in the interval [Mβw,Mβw] which we want to cover such that for any β
in the interval there exists a β satisfying ∣β 一 β∣ ≤ β∕d. For this purpose we can use a cover of Size
2 lOg1+1/d M ≤ 2(d+ 1) lOg M,1 which combined for all i's gives a cover with size O((d lOg M)d)
whose logarithm is growing as d lOg(d lOg M). This together with (15) completes the proof.
C.2 Proof of Theorem 2
We start by proving the following lemmas providing perturbation bound for FGM attacks.
Lemma 3. Consider a d-layer neural netfw with 1-Lipschitz and 1-smooth (1-Lipschitz derivative)
activation σ where σ(0) = 0. Let training loss ` : (Rm, Y) → R also be 1-Lipschitz and 1-smooth
for any fixed label y ∈ Y. Then, for any input x, label y, and perturbation vector u satisfying
∀i : ∣∣Uik2 ≤ dl∣Wik2 we have
∣∣Vχ'(fw+u(χ),y) - Vχ'(∕w(x),y)∣∣2	(16)
≤ W kWik2)X [ kkW⅛ + kxk2(∏kWjk2)t ≡2 ].
i=1	i=1	i 2	j=1	j=1	j 2
Proof. Since for a fixed y ` satisfies the same Lipschitzness and smoothness properties as σ, then
∣Vz'(z, y) k 2 ≤ 1 and applying the chain rule implies:
∣∣Vχ'fW+u(χ),y) - Vχ'fW(χ),y)∣∣2
=∣∣(Vχfw+u(χ))(V')fw+u(χ),y) -(VxfW(X))(V')fw(χ),y)∣∣2
≤ ∣∣(Vxfw+u(χ))(V')(fw+u(χ),y) -(VxfW(X))(V')(fw+u(χ),y)∣∣2
+ ∣∣(VxfW(X))(V')(fw+u(χ),y) -(Vxfw(X))(V')(fw(χ),y)∣∣2
d
≤ ∣∣Vxfw+u(χ) -Vxfw(χ)∣∣2 + (∏ kWik2)∣∣(V')(fw+u(χ),y) - (V')(fw(χ),y)∣∣2
i=1
d
≤ UVxfw+u(X)- VxfW(X)∣∣2 + (∏ kWik2)∣∣fw+u(χ) - fw (χ)∣∣2
i=1
≤ ∣∣Vxfw+u(χ) - Vxfw(X)∣∣2 + ekχk2(∏ kWik2)2 XX IWΦ.	(17)
i=1	i=1 kWi k2
The above result is a conclusion of Lemma 2 and the lemma's assumptions implying ∣Vxfw(χ)∣2 ≤
Qid=1 kWik2 for every χ. Now, we define ∆k = ∣∣Vxfw(k+)u(χ) - Vxfw(k)(χ)∣∣2 where fw(k) (χ) :=
Wkσ(Wk-ι …σ(W1 χ)) ∙∙∙) denotes the DNN's output at layer k. With (17) in mind, we complete
this lemma's proof by showing the following inequality via induction:
∆k ≤ e(1	+ d )k (Π	kWi k2) XX [ ≡2 +	kχk2(Π	kWjk2) X ≡2 ].	(18)
d i=1	i=1 kWik2	j=1	j=1 kWjk2
1Note that log 1 ≥ 1 — X implying log 1 11 ≥ ^1^- and hence (log(1 + 1/d)) T ≤ d + 1,
d+1
20
Published as a conference paper at ICLR 2019
The above equation will prove the lemma because for k ≤ d We have (1 + d)k ≤ (1 + d)d ≤ e. For
k = 0, ∆0 = 0 since fw(0)(x) = x and does not change with w. Given that (18) holds for k we have
∆k+1 = IIVxfw+1)(x) -Vxfwk+1)(x)∣∣2
=IIVx(Wk+1 + Uk+ι)σ(fW+ u(x)) - VxWk+ισ(M(x))∣∣2
= IIVxfw(k+)u(x)σ0(fw(k+)u(x))(Wk+1+Uk+1)T - Vxfw(k)(x)σ0(fw(k)(x))WkT+1II2
≤ IIVxfw(k+)u(x)σ0(fw(k+)u(x))(Wk+1+Uk+1)T - Vxfw(k+) u(x)σ0(fw(k)(x))(Wk+1 + Uk+1)T II2
+IIVxfw(k+)u(x)σ0(fw(k)(x))(Wk+1+Uk+1)T-Vxfw(k+)u(x)σ0(fw(k)(x))WkT+1II2
+ IIVxfw( +) u(x)σ0(fw(k) (x))WkT+1 -Vxfw(k) (x)σ0(fw(k) (x))WkT+1II2
≤ (1 + d)∣∣Wk+1∣∣2kvxfw+ u(x)k2∣∣fw+ u(x) - fW(x)∣∣2
+IIUk+1II2kVxfw(k+)u(x)k2kσ0(fw(k)(x))k2+IIWk+1II2kσ0(fw(k)(x))k2∆k
≤ (1+d)k+i(Y ∣Wik2) (ekxk2(Y ∣Wik2) XX kW⅜)
+ (1 + d)k (YI kWik2) p¾⅛ + ∣∣wk+ι∣Sk
k+i	k	k
≤ e(1 + 1)k+1(∏ kWik2)( kwk+1k2 + kxk2(∏ kWik2)∑ ⅛⅜) + ∣∣Wk+ι∣∣2∆k
d i=i	kWk+ik2	i=i	i=i kWik2	2
≤ e(1 + 1)k+1(∏1 ∣∣W∙k2)XX [ kuik2 + kxk2 (Π kW-k2) X kUjk2 ]
≤ e(1 + d) (U kWik2j ⅛ [kWik2 + kxk2 BkWj k2j j⅛ kWjk2 J
Therefore, combining (17) and (18) the lemma,s proof is complete	□
Before presenting the perturbation bound for FGM attacks, we first prove the following simple
lemma.
Lemma 4. Consider vectors zi, z and norm function ∣∣ ∙ ∣∣. If max{∣∣zιk, ∣∣z2k} ≥ K, then
∣∣ 扁zι -∣⅛ z2∣∣ ≤ K kzι - z2k.	(19)
Proof. Without loss of generality suppose kz2 k ≤ kz1k and therefore κ ≤ kz1 k. Then,
∣∣ 岛z1-扇z2∣∣ = '∣∣ 高(ZI- Zz-
kzik-∣Z2kL
∣Z1k	kzzk 2
≤ 扁kz1- zzk + 扁 IkzIk-kz2”
≤wkz1 - z2k
≤ 三 kz1 - zzk.
□
Lemma 5. Consider a d-layer neural network function fw with 1-Lipschitz, 1-smooth activation
σ where σ(0) = 0. Consider FGM attacks with noise power E according to Euclidean norm || ∙ ||z.
Suppose κ ≤ kVx'(fw(x), y)kz holds over the E-ball around the support set X. Then, for any
norm-bounded perturbation vector U such that ∣∣Uikz ≤ d k Wikz. ∀i, we have
kδwfg+mu(x)-δwfgm(x)k2 ≤
kUjkz ]
kWj kz『
21
Published as a conference paper at ICLR 2019
Proof. The FGM attack according to Euclidean norm is simply the DNN loss’s gradient normalized
to have E-EUclidean norm. The lemma is hence a direct result of combining Lemmas 3 and 4.	□
To prove Theorem 2, we apply Lemma 1 together with the result in Lemma 5. Similar to the proof for
Theorem 1, given weights we we consider a zero-mean multivariate Gaussian perturbation vector u
with diagonal covariance matrix where each element in the ith layer ui varies with the scaled standard
f
deviation ξi = k "∣2 ξ With ξ properly chosen later in the proof. Consider weights W for which
βwe
1
∀i : ∣kWik2-Wik2∣ ≤ dkWik2.	(20)
Since Ui 〜N(0, ξf I), (Tropp, 2012) shows the following bound holds
PMew ^kW⅛ >t ≤2h exp(- 2hξ2).	(21)
Then we apply a union bound over all layers for a maximum union probability of 1/2 implying the
normalized βw fik2 for each layer is upper-bounded by ξ p2h log(4hd). Now, if the assumptions
of Lemma 5 hold for perturbation vector u given the choice of ξ, for the FGM attack with noise
power E according to Euclidean norm ∣∣ ∙ k2 we have
kfw+u(x + δwmu(x)) - fw(X + δwm(x))k2	(22)
≤ kfw+u(x + δWmu(X)) - fw (x + δWmu(X)) ∣2 + kfw(x + δfgmu(X)) - fw (x + δwm(X)) ∣2
d
≤ ∣fw+u(x+δwmu(x)) - fw (x+δwmu(x))k2 + (YkWik2)kδwmu(x) - δwm(x)k2
i=1
≤ e(B + E) Y kWik2 XX 部 + 2e2K Y ∣Wik2 XX[辑 + B(Y IWj k2) XX	]
i=1	i=1 i 2	i=1	i=1 i 2	j=1	j=1 j 2
dd
≤ e2(B+E)Y kWfik2 X
i=1	i=1
kUi k2
kWfik2
+ 2e5 E Y kWik2 Xx [ %
κi=1	i=1 kWik2
ii
+ B(Y kW jk2)X
j=1	j=1
kUj k2
kWfjk2
d	d	di
≤ 2e5d(B + e)ξ√2h log(4hd){ Y kWik2 + "Y kWik2)(ι∕B + XY kWjk2)
Hence we choose
⅛ 一	>-----:------, .. -~~^ .. /	_, .. -~~^ .. . .	_,  ,	.. -~~^ ...、,
8e5d(B + e)√2h log(4hd) Qi=IkW ill2(1 + K Qi=IkW』2(I/B + Pi=ι Qj=IkW j II2))
(23)
for which the assumptions of Lemmas 1 and 5 hold. Assuming B ≥ 1, similar to Theorem 1’s proof
we can show for any W such that ∣∣kWi k2 - kWfi k2 ∣∣ ≤ d kW ik2 we have
KL(Pw+ukQ) ≤ χX ≡ξ2rF
O d2(B + E)2h log(hd)
Qd=I kWik2{ι + 蛾(Qd=I kWik2) Pd=IQj=1 kWj k2)}2
Y2
d
X
i=1
IWikF、
kW ik2∕
≤O(d2(B + >log(hd)Qd=I kWik2{1 + 康(Qd=I 吁⑸ Pd=I Qj=I kWjk2)}2 X
γ	i=1
kWikF )
kWik2 7
Then, applying Lemma 1 reveals that given any η > 0 with probability at least 1 - η for any W such
that∣kWik2 - kWik2∣ ≤ dkWik2 we have
(B + Eydihlog(dh) Φ±gm(fw) + log η
Lf0gm(fw) ≤ Lbfγgm(fw) + O
(24)
γ2n
22
Published as a conference paper at ICLR 2019
where ①道&)：={Q：=i IWik2(1 + ⅞{(Q：=i IWik2) Pd=IQj=IkWj∣∣2)}2 P= kW⅛.
Note that similar to our proof for Theorem 1 we can find a cover of size O((d log M)d dn1/2d)
for the spectral norms of the weights feasible set, where for any kWi k2 we have ai such that
kWik2-ai ≤ai /d. Applying this covering number bound to (24) completes the proof.
C.3 Proof of Theorem 3
We use the following two lemmas to extend the proof of Theorem 2 for FGM attacks to show Theorem
3 for PGM attacks.
Lemma 6. Consider a d-layer neural network function fw with 1-Lipschitz, 1-smooth activation σ
where σ(0) = 0 .We consider PGM attacks with noise power E according to Euclidean norm ∣∣∙∣∣2, r
iterations and StePSize a. Suppose K ≤ ∣∣Vχ'(fw(x), y) ∣∣2 holds over the E-ball around the support
set X. Thenfor any perturbation vector U such that IlUik2 ≤ d k Wi∣2 for every i we have
kδw+ur(X)- δpgm,r(x)∣2 ≤ e2(2α∕κ)
1 一 (2α∕κ)r lip(V' ◦ fw)r
1 - (2α∕κ)lip(V' ◦ fw)
(25)
dd
OWik2) X
i=1	i=1
IUik2
j∣Wik2
+ (kX12 + E)(D kWj k2)X Pt
j=1	j=1
Here lip(V' ◦ fw) denotes the actual LiPschitz constant of Vχ'(fw(x), y).
Proof. We use induction to show this lemma for different r values. The result for case r = 1 is a
direct consequence of Lemma 5. SuPPose that the result is true for r = k. Then, Lemmas 3 and 4
imPly
δwpg+mu,k+1(x) - δwpgm,k+1(x)2
≤ 2α∣∣Vχ'(fw+u(x + δW+Uk(x)))-Vχ'(fw (x + δWgm,k(x)))∣∣2
≤ 2α∣∣Vχ'(fw+u(x + δwmuk(x)))-Vχ'(fw (x + δwmuk(x)))∣∣2
κ
+ 2α∣∣ Vχ'(fw(x + δW+Uk (X)))- Vχ'(fw(x + δpgm,k (X)))I∣2
κ
≤ "(Y kWik2)X [ kkW⅛ + (kxk2 + E)(Π kWj k2)X ≡2
i=1	i=1	j=1	j=1
+ 2α iip(V' ◦ fw)∣∣VχδW+uk (x) - Vχδpgm,k (x)∣∣2
κ
≤ 2αe2(II kWik2)X [ kkW⅜ + (kxk2 + E)(Π kWj k2)X ≡2
i=1	i=1	j=1	j=1
+ — lip(V' ◦ fw)e2(2α∕κ)
κ
1 — (2α∕κ)k lip(V' ◦ fw)k
1 - (2α∕κ)lip(V' ◦ fw)
dd
(YkWik2) X
i=1	i=1
kkW⅜+(冈2+呼 kw^X 曙
e2 (2α∕κ)
1 - (2α∕κ)k+1lip(V' ◦ fw)k+1
1 - (2α∕κ)lip(V' ◦ fw)
× (Y kWik2) X [≡2 + (kxk2 + E)(YkWjk2) X 川
i=1	i=1	i 2	j=1	j=1 j 2
where the last line follows from the equality Pk=。Si = 1--+1. Therefore, by induction the lemma
holds for every value r ≥ 1.	口
×
23
Published as a conference paper at ICLR 2019
Lemma 7. Consider a d-layer neural networkfunction fw with 1 -Lipschitz, 1 -smooth activation σ
where σ(0) = 0. Also, assume that training loss ' is 1 -Lipschitz and 1 -smooth. Then,
d	d i
IiP(VX,(fw(x),y)) ≤ 而(V' o fw) := (Y IIWik2) X∏ IWj IR	(26)
i=1	i=1j=1
Proof. First of all note that according to the chain rule
lip 卜 χ'(fw(χ),y)) = lip 卜XfW(X)(V4)(fw (χ),y))
≤ lip(Vχfw(x)) + lip(fw)2
d
≤ lip (Vxfw (χ)) + ∏ IWi k2∙
Considering the above result, We complete the proof by inductively proving lip(VXfW(x)) ≤
(Qd=I ∣∣Wi∣2) Pd=ι Qj=I ∣∣Wj∣∣2. For d = 1, VXfW(x) is constant and hence the result holds.
Assume the statement holds for d = k. Due to the chain rule,
VxfWk+I)(X) = VχWk+ισ(M(x)) = VXfy)(X) σ'(M(x))wT+ι
and therefore for any X and V
kVXfWk+I)(X + V)-VXf*1)(x)∣∣2
≤ U VXfy)(X + v) σ'(fWk(x + V))WT+ι-VXM(X + v)σ'(M(x + V))WT十/？
≤ kWk+ι k2 IlVXf6)(x + v) σ0(fW(x + v)) - VX管)(x) σ0 f (x))∣∣2
≤k Wk+1 k 2 U VXff) (x + v) σ0 f (x + v)) - VX 需)(x) σ0 f (x + v)) ∣∣ 2
+ ∣Wk+1k2∣∣ Vxff)(x) σ0(僧)(x + v)) - VXf岁(x) σ'(ff)(x)) ∣∣?
≤ IWk+1k2{∣∣ VXf(k)(x + V)-VXa(X) ∣ ∣ 2 + ∣∣VXf(k)(x)∣∣2 ∣∣σ0(f(k)(x + v)) -σ0(f^(x))∣∣2}
≤ IWk+1k2kP(VXf^(X)) +lip(f^(X))卜v∣∣2
k+1	k+1i-1
≤ (∏ kWik2) XnkWj k2kvk2,
i=1	i=1j=1
which shows the statement holds for d = k + 1 and therefore completes the proof via induction. □
In order to prove Theorem 3, we note that for any norm-bounded ∣∣x∣∣2 ≤ B and perturbation vector
U such that ∀i, ∣∣Uik2 ≤ 1 ∣∣Wi∣∣2 we have
kfw+u(x + 鹿+f (X)) - fw(x + 然T,r(x))k2
≤ kfw+u(x+emτ (x)) - fw (x+c+ur (x))k2
+ kfw(X + 鹿+ur(x)) - fw(x + 然gm,r(x))k2
≤ e(B + M Mg 就
+ e2(2α∕κ)
1 - (2ɑ∕κ)r lip(V' o fw)r
1 - (2α∕κ)lip(V' o fw)
×
(∏ kWik2)X [ ≡ + (B + 叫 kWj k2)X 静]
i=1	i=1 l 11	11	j = 1	j=1 11 j 11 」
≤ " + 'MY kWik2)XX M
+ e2(2α∕κ)
1 - (2ɑ∕κ)rlip(V' o fw)r
1 - (2α∕κ)fip(V' o fw)
24
Published as a conference paper at ICLR 2019
X (口 kwik2)X [ ≡⅜ + (B+呼 kWj k2)jX 静]
The last inequality holds since as shown in Lemma 7 lip(Vχ'(∕w(x),y)) ≤ lip(V' ◦ fw):=
(Qd=I IIWik2) Pd=I Qj=I k Wjk2. Here the upper-bound lip(V' ◦ fw) for W changes by a factor
at most e2/r for W such that ∣∣Wi∣2 - ∣W i∣∣2∣ ≤ rd ∣W i∣ 2. Therefore, given W if similar to the
proof for Theorem 2 we choose a zero-mean multivariate Gaussian distribution Q for u with the ith
f
layer Ui S standard deviation to be ξi = k Hik2 ξ where
β^w
ξ=---------,	-------------J---------------------------—
8d(B + Mhlog(4hd)e4(α∕κ):：2**糕'解(Qd=I ∣Wik2)(l + Pd=IQj=1 kWjk2)
J. e ( (43∕κ )iɪp( V ◦f J WJ )	J
Then for any W satisfying ∣∣∣Wi∣∣2 -IlWi∣∣2∣ ≤ r⅛∣∣Wi∣2, applying union bound shows that the
assumption ofLemma 1 Pru(maxχ∈χ kfw+u(x + δpg+m,r(x)) — fw(x + δwgm,r(X))I∣∞ ≤ 4) ≥ 2
holds for Q, and further we have
KL(PW+ukQ) ≤ X kWWkF
= O d2(B + )2 h log(hd)×
q3 ∣Wik2 ι-e2⅞α∕κι⅞⅞⅞ {1+K (q3 kWi∣2) p3 Qj=ι kWig}2 X E)
Y2	i=1 kWik2J
≤ O d2(B + )2 h log(hd)×
Qd=ι kWik2⅛g⅛Βff {1 + K (Qd=ι kWik2) Pd=ι Qj=ι IWj k2}2 X1"kF ʌ
γ2	i⅛ kWik2 7
Applying the above bound to Lemma 1 shows that for any η > 0 the following holds with probability
1 — η for any W where IkWik2 — kfvik2∣ ≤ rdkWik2:
L0pgm(fw) ≤ Lbpγgm(fw) + O
(B + E)2d2h lθg(dh) ΦPgmr,α(fw) + log η
γ2n
where we consider Φp,gKm,r,α (fw) as the following expression
iY=d1
kWik2 1 + (α∕κ)
1 一(2α∕κ)rlip(V' ◦ fw)r
1 — (2α∕κ)lip(V' ◦ fw)
d	di	2
(∏kWik2)X∏ kWjk2)
i=1	i=1 j=1
d
X
i=1
kWikF
kWi k2.
Using a similar argument to our proof of Theorem 2, we can properly cover the spectral norms for
each Wi with 2rd log M points, such that for any feasible kWik2 value, satisfying the assumptions,
we have value a% in our cover where IkWik 2 - ai∣ ≤ rd。八 Therefore, we can cover all feasible
combinations of spectral norms with (2rd log M)d dn1/2d, which combined with the above discussion
completes the proof.
C.4 Proof of Theorem 4
We first show the following lemma providing a perturbation bound for WRM attacks.
Lemma 8. Consider a d-layer neural net fw satisfying the assumptions of Lemma 3. Then, for any
weight perturbation U such that ∣∣Ui∣∣2 ≤ d k Wik2 we have
kδw+u(x) — δWrm(x)k2 ≤
e2
λ — lip(V' ◦ fw)
25
Published as a conference paper at ICLR 2019
× (Y ∣Wik2) X [ w⅛ +(kxk2 + j^ W ∣Wjk2) X 部].
i=1	i=1	j=1	j=1
In the above inequality, lip(V' ◦ fw) denotes the Lipschitz constant of Vχ'(fw(x), y).
Proof. First of all note that for any X We have ∣∣δWrm(x)k2 ≤ (Qd=IIlWik2)∕λ, because We
assume lip(V' ◦ fw) < λ implying WRM's optimization is a convex optimization problem with the
global solution δWrm(x) satisfying δWrm(x) = 1 V' ◦ fw(x + δWrm(x)) which is norm-bounded by
lip('；/W) ≤ (Qd=I IIWik2)∕λ. Moreover, applying Lemma 3 we have
δwwr+mu(X)-δwwrm(X)2
=Il 1 Vχ'(fw+u(x + δW+U(x)) - 1 Vχ'(fw (x + δWrm(x))∣∣2
λλ
≤ Il 1 Vχ'(fw+u(x + δw+u(x)) - 1 Vχ'(fw (x + δw+u(x))∣∣2
+ Il1 Vχ'(fw(x + δw+u(x)) - 1 Vχ'(fw(x + δwrm(x))∣∣2
λλ
≤ e2 (Y ∣Wik2)X [ w⅜+(kx∣2+Q=J )(iɪ kwjk2) X *¾]
+ Iip(Wofw) ∣∣δw+u(x)- δwrm(x)∣∣2
which shows the following inequality and hence completes the proof:
(1 - IiP(V；° fw) ∣∣δW+U(x) - δwrm(x)∣∣2
≤ e2 (YY ∣Wik2)X [品 + (kx∣2 + j4wh )(∏ k Wjk2)X *⅛.
i=1	i=1	j=1	j=1
□
Combining the above lemma with Lemma 2, for any norm-bounded kxk2 ≤ B and perturbation
vector U where ∣∣Uik2 ≤ d1 ∣∣Wi∣2,
∣fw+u(x + δwwr+mu(x)) - fw(x + δwwrm(x))∣2
≤ ∣∣fw+u(x + δwwr+mu(x)) - fw(x + δwwr+mu(x))∣∣2 + ∣∣fw(x + δwwr+mu(x)) - fw(x + δwwrm(x))∣∣2
≤ e(B + "YWJk2 )(1YkWik2) X IW^ + (YIkWik2)
λ	i=1	i=1 ∣Wi∣2	i=1
e2	X [ kUik2
λ - lip(V'◦ fw) = [∣Wik2
×
+ (B + QHWk)(口 kWjk2)X ≡22]
≤ e(B + Q=1 kWjk2 )(YI kWik2) X 界% + CYIkWik2)
λ	i=1	i=1 kWik2	i=1
×
_e2	X [ kUik2
λ - lip(V'◦ fw) = [lWig
+ B+
Q=1 1Wjk2 )(YiIkWJk2)X
λ	j=1	j=1
kUj k2 -
kWj k2_
Similar to the proofs of Theorems 2,3, given we we choose a zero-mean multivariate Gaussian
distribution Q with diagonal covariance matrix for random perturbation U, with the ith layer Ui ’s
f
standard deviation parameter ξi = k .ik2 ξ where
ξ
______________________________Y
8e5d,2hlog(4hd)(B + Qd=IkWjk2∕λ)(Qd=1 iWik?)。+ 1一限葭fw)Pd=IQj=IkWJk2)
26
Published as a conference paper at ICLR 2019
Using a union bound suggests the assumption of Lemma 1 Pru maxx∈X kfw+u(x + δwwr+mu(x)) -
fw(X + δwrm(x))k∞ ≤ 4) ≥ 1 holds for Q. Then, for any W satisfying IkWik2 -IlWi∣∣2∣ ≤
4d∕τ kWfik2 We have lip(' ◦ fw) ≤ (eτ/4)2lip(' ◦ fw) ≤ (e^4)2λ(1-τ) ≤ f λ ≤ (1 - 2)λ
Which implies the guard-band τ for We applies to W after being modified by a factor 2. Hence,
KL(Pw+ukQ) ≤ XX ≡ξi2kF
d
≤O d2(B + ∏ kW jk2∕λ)2h log(hd)
j=1
JQ3 fik2)(1 + λ-而(除 fw)P3 Qj=ιf jk2)2 X s> `
×	γ2	⅛ kWik2)
d
≤ O d2(B + ∏ kWjk2∕λ))2hlog(hd)
j=1
(Qd=I kWik2)(ι + λ-∏p(3 'fw) Pd=IQj=1 kWjk2)2 X kWikFʌ
×	γ2	⅛ kWik2)
Using this bound in Lemma 1 implies that for any η > 0 the folloWing bound Will hold With
. ..	I_______	..r≥c .. 1 -l ..r≥c ..
probability 1 - η for any W where IkWik2 - IWik21 ≤ 4d/T∣∣Wi∣3
L0wrm(fw) ≤ Lbγwrm(fw) +O
(B + Qd=I kWik2∕λ)2d2hlog(dh) ΦWrm(fw) + dlog η
γ2 n
where we define Φλwrm(fw) to be
d	d	di	d	2
{γkWik2(1+^fy (UkWik2) X UkWjk2)}2 X Wl.
Using a similar argument to our proofs of Theorems 2 and 3, we can cover the possible spectral
norms for each Wi with O((8d∕τ) log M) points, such that for any feasible kWik2 value satisfying
the theorem,s assumptions, we have value ad in our cover where ∣ ∣∣Wdk2 - ad∣ ≤ 4d%ad. Therefore,
we can cover all feasible combinations of spectral norms with O(((8d∕τ) log M)ddn1/2d), which
combined with the above discussion finishes the proof.
27