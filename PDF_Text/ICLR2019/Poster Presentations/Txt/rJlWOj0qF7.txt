Published as a conference paper at ICLR 2019
Imposing Category Trees Onto Word-
Embeddings Using A Geometric Construction
Tiansi Dong1 & Christian Bauckhage1,2
1 B-IT, University of Bonn, Bonn, Germany
2Fraunhofer IAIS, Sankt Augustin, Germany
{dongt, bauckhag}@bit.uni-bonn.de
Hailong Jin & Juanzi Li
DCST, Tsinghua University, Beijing, China
jinhl15@mails.tsinghua.edu.cn
lijuanzi@tsinghua.edu.cn
Olaf H. Cremers, Daniel Speicher & Armin B. Cremers
B-IT, University of Bonn, Bonn, Germany
{cremerso, dsp, abc}@bit.uni-bonn.de
Jorg Zimmermann113
3Informatik II, University of Bonn
jg@iai.uni-bonn.de
Ab stract
We present a novel method to precisely impose tree-structured category infor-
mation onto word-embeddings, resulting in ball embeddings in higher dimen-
sional spaces (N -balls for short). Inclusion relations among N -balls implicitly
encode subordinate relations among categories. The similarity measurement in
terms of the cosine function is enriched by category information. Using a geo-
metric construction method instead of back-propagation, we create large N -ball
embeddings that satisfy two conditions: (1) category trees are precisely imposed
onto word embeddings at zero energy cost; (2) pre-trained word embeddings are
well preserved. A new benchmark data set is created for validating the cate-
gory of unknown words. Experiments show that N -ball embeddings, carrying
category information, significantly outperform word embeddings in the test of
nearest neighborhoods, and demonstrate surprisingly good performance in vali-
dating categories of unknown words. Source codes and data-sets are free for pub-
lic access https://github.com/GnodIsNait/nball4tree.git and
https://github.com/GnodIsNait/bp94nball.git.
1	Introduction
Words in similar contexts have similar semantic and syntactic information. Word embeddings are
vector representations of words that reflect this characteristic (Mikolov et al., 2013; Pennington
et al., 2014) and have been widely used in AI applications such as question-answering (Tellex et al.,
2003), text classification (Sebastiani, 2002), information retrieval (Manning et al., 2008), or even as
a building-block for a unified NLP system to process common NLP tasks (Collobert et al., 2011). To
enhance semantic reasoning, researchers proposed to represent words in terms of regions instead of
vectors. For example, Erk (2009) extended a word vector into a region by estimating the log-linear
probability of weighted feature distances and found that hyponym regions often do not fall inside
of their hypernym regions. By using external hyponym relations, she obtained 95.2% precision and
43.4% recall in hypernym prediction for a small scale data set. Her experiments suggest that regions
structured by hyponym relations may not be located within the same dimension as the space of word
embeddings. Yet, how to construct strict inclusion relations among regions is still an open problem
when representing hypernym relations.
In this paper, we restrict regions to be n dimensional balls (N -ball for short) and propose a novel
geometrical construction approach to impose tree-structured category information onto word embed-
dings. This is guided by two criteria: (1) Subordinate relations among categories shall be implicitly
and precisely represented by inclusion relations among corresponding N -balls. This way, the energy
costs of imposing structure will be zero; (2) Pre-trained word embeddings shall be well-preserved.
Our particular contributions are as follows: (1) The proposed novel geometric approach achieves
zero energy costs of imposing tree structures onto word-embeddings. (2) By considering category
information in terms of the boundary of an N -ball, we propose a new similarity measurement that is
1
Published as a conference paper at ICLR 2019
(b)
Figure 1: (a) The structure of N -ball; (b) →u is inside w→; (c) →u disconnects from w→
more precise and sensitive than conventional cosine similarity measurements. (3) We create a large
data set of N -ball embeddings using the pre-trained GloVe embeddings and a large category tree of
word senses extracted from Word-Net 3.0.
The remainder of our presentation is structured as follows: Section 2 presents the structure of N -ball
embeddings; Section 3 describes the geometric approach to construct N -ball embeddings; Section
4 presents experiment results; Section 5 briefly reviews related work; Section 6 concludes the pre-
sented work, and lists on-going research.
2	Region-Based Embeddings
Tree structures occur in many applications and are used to describe, say, file systems, syntactic
structures, taxonomies of plants or animals, or subordinate relations of governments. Here, we
use the tree structure of hyponym relations among word senses as to construct N -ball embed-
dings. Formally, an N -ball of word sense w with central point A~w and radius rw is written as
w→ = B(A~w, rw) and defined as the set of vectors whose Euclidean distance to A~w is less than rw:
B(A~w, rw) , {~p|kA~ w - p~k < rw}. N -balls are defined as open-regions, as illustrated in Figure 1(a),
so they are not RCC regions that can be either open or closed, or even a mixture, thus avoiding a
number of problems (Dong, 2008; Davis & Marcus, 2015).
2.1	RELATIONS BETWEEN N -BALLS
We distinguish two topological relations between N -balls: being inside and being disconnected
→→
from as illustrated in Figure 1(b, c). u = B(Au, ru) being inside w = B(Aw , rw) can be mea-
sured by the result of subtracting the sum of radius ru and the distance between their central vectors
from radius rw. Formally, we define Dinside(→u, w→) , rw - ru - kA~u - A~w k. So, →u is inside of
w→ (w is the hypernym of u), if and only if Dinside (→u , w→) ≥ 0. If v is the hypernym of w, w is
the hypernym of u, we have Dinside (→u , →v ) > Dinside (→u , w→) + Dinside (w→, →v ) ≥ Dinside (w→, →v ).
The direct hypernym of →v , written as dh(→v ), can be defined as the →x which produces the mini-
mal positive value of Dinside (→v, →x). Formally, dh(→v) , arg min→x,→x 6=→v Dinside (→v, →x) ≥ 0 =
min{→x |Dinside (→v, →x) ≥ 0 ∧ →x 6= →v}.
Similarly, →u disconnecting from w→ can be measured by the result of subtracting the distance between
their center vectors from the sum of their radii. Formally, we define Ddisc(→u, w→) , rw +ru - kA~u -
A~w k, That is, →u disconnects from w→, if and only if Ddisc(→u, w→) ≤ 0.
2.2	Similarity Measurement
Similarity is normally measured by the cosine value of two vectors, e.g., Mikolov et al. (2013). For
N -balls, the similarity between two balls can be approximated by the cosine value of their central
vectors. Formally, given two N -balls →u = B(A~u, ru) and w→ = B(A~w, rw), their cosine similarity
2
Published as a conference paper at ICLR 2019
(a)
[E E
pre-trained word embedding parent location code spatial extension code
in tree
(b)
Figure 2: (a) Hypernym relations of three word senses of flower in Word-Net 3.0; (b) Three compo-
nents of the center point of an N -ball.
can be defined as cos(A~u, A~w). One weakness of the method is that we do not know the boundary of
the lowest cos value below which two word senses are not similar. Using category information, we
can define that two word senses are not similar, if they have different direct hypernyms. Formally,
Sim0(→u,w→) , c-o1s(A~u,A~w)
dh(→u) = dh(w→)
otherwise
3	CONSTRUCTING N -BALL EMBEDDINGS
N -ball embeddings encode two types of information: (1) word embeddings and (2) tree structures
of hyponym relations among word senses. A word can have several word senses. We need to
create a unique vector to describe the location of a word sense in hypernym trees. We introduce a
virtual root (*root*) to be the parent of all tree roots, fix word senses in alphabetic order in each
layer, and number each word sense based on the fixed order. A fragment tree structure of word
senses is illustrated in Figure 2(a). The path of a word sense to *root* can be uniquely identified
by the numbers along the path. For example, the path from *root* to flower.n.03 is [entity.n.01,
abstraction.n.06, measure.n.02, flower.n.03]1, which can be uniquely named as [1,1,1,1]; We call
this vector location code of flower.n.03. The location code of the direct hypernym of flower.n.03 is
called Parent Location Code (PLC). PLC of flower.n.03 is [1,1,1], PLC of whole.n.02 is [1,2].
As a word and its hypernym may not co-occur in the same context, their co-occurrence relations
can be weak, and the cosine similarity of their word embeddings could even be less than zero.
For example, in GloVe embeddings, cos(ice_cream, dessert)= —0.1998, cos(tuberose, plant)=
-0.2191. It follows that the hypernym ball must contain the origin point of the n-dimensional space,
if this hypernym ball contains its hyponym, as illustrated in Figure 3 (a). When this happens to two
semantically unrelated hypernyms, N -balls of the two unrelated hypernyms shall partially overlap,
as they both contain the original point. For example, the ball of desert shall partially overlap with
the ball of plant. This violates our first criterion. To avoid such cases, we require that no N -ball
shall contain the origin O. This can be computationally achieved by adding dimensions and realized
by introducing a spatial extension code, a constant non-zero vector, as illustrated in Figure 2(b). To
intuitively understand this, imagine that you stand in the middle of two objects A and B, and cannot
see both of them. To see both of them without turning the head or the eyes, you would have to
walk several steps away so that the angle between A and B is less than some degree, as illustrated in
Figure 3(c).
1 We exclude *root* from the path
3
Published as a conference paper at ICLR 2019
(a)
(b)
(c)
(d)
Figure 3: (a) The angle between ball (A0, r 1) and ball (B0, r2) is greater than 90o, so, ball (A0, r 1)
will contain O, ifit contains ball (B~0, r2); (b) If ball (A~, r1) contains ball (B~, r2), not containing O,
the sum of α and β is less than 90o; (c) The angle ∠AOB = 180o. If we shift A and B one unit into
the new dimension, we have A0(0, 1, 1) and B 0(0, -1, 1), the angle ∠A0OB 0 will become 90o ; (d)
O1 , O2 , O3 are homothetic to O10 , O20 , O30 , inclusion relations among them are preserved.
3.1	The Structure of the Central Vector
Following Zeng et al. (2014) and Han et al. (2016), we structure the central vector of an N -ball by
concatenating three vectors: (1) the pre-trained word-embedding, (2) the PLC (if the code is shorter
than the max length, we append 0s till it reaches the fixed length), (3) the spatial extension code.
3.2	ZERO ENERGY LOSS FORN-BALL CONSTRUCTION USING GEOMETRIC APPROACH
Our first criterion is to precisely encode subordinate relations among categories into inclusion re-
lations among N -balls. This is a considerable challenge, as the widely adopted back-propagation
training process (Rumelhart et al., 1988) quickly reaches non-zero local minima and terminates2.
The problem is that when the location or the size of an N -ball is updated to improve its relation
with a second ball, its relation to a third ball will deteriorate very easily. We propose the classic
depth-first recursion process, listed in Algorithm 1, to traverse the category tree and update sizes
and locations of N -balls using three geometric transformations as follows.
Homothetic transformation (H-tran) which keeps the direction of the central vector and enlarges
lengths of A and r with the same
rate k. H(B(JA, r) ,k)，B(A0, r0), satisfying (1)图=号
=k>
0,and ⑵ ∣⅜ =看;
Shift transformation (S-tran) which keeps the length of the radius r and adds a new vector ~s to A~.
S (B(A~, r), ~s) , B(A~0, r), satisfying A~0 = A~ + ~s;
Rotation transformation (R-tran) which keeps the length of the radius r and rotates angle α of A~
inside the plane spanned by the i-th and the j-th dimensions of A~. R(B(A~, r), α, i, j) , B(A~0, r),
such that A~k = A~0k (k 6= i, j), A~0i = A~i cos α + A~j sin α and A~0j = A~j cos α - A~i sin α.
To satisfy our second criterion, we do not choose rotation dimensions among the dimensions of
pre-trained word embeddings. Rather, to prevent the deterioration of already improved relations, we
use the principle of family action: if a transformation is applied for one ball, the same transforma-
tion will be applied to all its child balls. Among the three transformations, only H-tran preserves
inclusion and disconnectedness relations among the family of N -balls as illustrated in Figure 3(d),
therefore H-tran has the priority to be used.
In the process of adjusting sibling N -balls to be disconnected from each other, we apply H-tran
obeying the principle of family action. When an N -ball is too close to the origin of the space, a
S-tran plus R-tran will be applied which may change the pre-trained word embeddings.
2This phenomenon appears even in very small data sets. We developed a visual simulation for illustration.
The source code is available at https://github.com/GnodIsNait/bp94nball.git
4
Published as a conference paper at ICLR 2019
Following the depth-first procedure, a parent ball is constructed after all its child balls. Given a child
N -ball B(B~, r2), a candidate parent ball B(A~, r1) is constructed as the minimal cover of B(B~, r2),
illustrated in Figure 3(b). The final parent ball B(P~, rp) is the minimal ball which covers these
already constructed candidate parent balls B(P~i, rpi ).
Algorithm 1: training .one family (root): Depth first algorithm to construct N-balls of all nodes
of a tree_______________________________________________________________________________
input : a tree pointed by root; each node stores a word sense and its word embedding
output: the N -ball embedding of each node
children4——get_all .children_of (root)
if number-of (children) > 0 then
foreach child ∈ children do
// depth first
training .one family (child)
end
if number_of (children) > 1 then
// adjusting siblings to be disconnected from each other
adjust do _be_disconnected (children)
end
// create parent ball for all children
root = CreateJparentJ)all _of (children)
else
I initializeJjall (root)
end
4 Experiments and Evaluations
4.1 EXPERIMENT 1: CONSTRUCT N -BALL EMBEDDINGS FOR LARGE-SCALE DATA SETS
We use the GloVe word embeddings of Pennington et al. (2014) as the pre-trained vector embedding
of words and extract trees of hyponym relations among word senses from Word-Net 3.0 of Miller
(1995). The data set has 54, 310 word senses and 291 trees, among which root entity.n.01 is the
largest tree with 43, 669 word senses. Source code and input data sets are publically available at
https://github.com/GnodIsNait/nball4tree.git. We proved that all subordinate
relations in the category tree are preserved in N -ball embeddings. That is, zero energy cost is
achieved by utilizing the proposed geometric approach. Therefore, the first criterion is satisfied.
4.2 Experiment 2: Test If Pre-Trained Word-Embeddings Are Well Preserved
We apply homothetic, shifting, and rotating transformations in the construction/adjusting process
of N -ball embeddings. Shifting and rotating transformations may change pre-trained word embed-
dings. The aim of this experiment is to examine the effect of the geometric process on pre-trained
word embeddings, and check whether the second criterion is satisfied.
Method 1 We examine the standard deviation (std) of the pre-trained word embedding in N -ball
embeddings of its word senses. The less their std, the better they are preserved.
The N -ball embeddings have 32,503 word stems. For each word stem, we extract the word em-
bedding parts from N -ball embeddings, normalize them, minus pre-train word embeddings, and
compute standard deviation.
The maximum std is 0.7666. There are 417 stds greater than 0.2, 6 stds in the range of (0.1, 0.2],
9 stds in the range of (10-12, 0.1], 9699 stds in the range of (0, 10-12], 22,372 stds equals 0. With
this statistics we conclude that only a tiny fraction (1.3%) of pre-trained word embeddings have a
small change (std ∈ (0.1, 0.7666]).
5
Published as a conference paper at ICLR 2019
Method 2 The quality of word embeddings can be evaluated by computing the consistency (Spear-
man’s co-relation) of similarities between human-judged word relations and vector-based word sim-
ilarity relations. The standard data sets in the literature are the WordSim353 data set (Finkelstein
et al., 2001) which consists of 353 pairs of words, each pair associated with a human-judged value
about the co-relation between two words, and Stanford’s Contextual Word Similarities (SCWS) data
set (Huang et al., 2012) which contains 2003 word pairs, each with 10 human judgments on the
similarity. Given a word w , we extract the word embedding part from its word senses’ N -ball
embeddings and use the average value as the word-embedding of w in the experiment.
Unfortunately, both data sets cannot be used directly within our experimental setting as some words
do not appear in the ball-embedding due to (1) words whose word senses have neither hypernym,
nor hyponym in Word-Net 3.0, e.g. holy; (2) words whose word senses have different word stems,
e.g. laboratory, midday, graveyard, percent, zoo, FBI, . . . ; (3) words which have no word senses,
e.g. Maradona; (4) words whose word senses use their basic form as word stems, e.g. clothes,
troops, earning, fighting, children. After removing all the missing words, we have 318 paired words
from WordSim353 and 1719 pairs from SCWS dataset for the evaluation.
We get exactly the same Spearman’s co-relation values in all 11 testing cases: the Spearman’s co-
relation on WordSim318 is 76.08%; each test on Spearman’s co-relations using SCWS1719 is also
the same. We conclude that N -ball embeddings are a “loyal” extension to word embeddings, there-
fore, the second criterion is satisfied.
4.3	EXPERIMENT 3: QUALITATIVE EVALUATION OF N -BALL EMBEDDINGS
Following Levy & Goldberg (2014), we do qualitative evaluations. We manually inspect nearest
neighbors and compare results with pre-trained GloVe embeddings. A sample is listed in Table 1 -
2 with interesting observations as follows.
Precise neighborhoods N -ball embeddings precisely separate word senses of a polysemy. For
example, the nearest neighbors of berlin.n.01 are all cities, the nearest neighbors of berlin.n.02 are
all names as listed in Table 1.
Typed cosine similarity function better than the normal cosine function Sim0 enriched by
category information produces much better neighborhood word senses than the normal cosine mea-
surement. For example, the top-5 nearest neighbors of beijing in GloVe using normal cosine mea-
SUrement are: China, taiwan, seoul, taipei, Chinese,, among which only Seoul and taipei are cities.
The top-10 nearest neighbors of berlin in GloVe using normal cosine measurement are: vienna,
warsaw, munich, Prague, germany, moscow, hamburg, bonn, Copenhagen, Cologne, among which
germany is a country. A worse problem is that neighbors of the word sense berlin.n.02 as the fam-
ily name do not appear. Without structural constraints, word embeddings are severely biased by a
training corpus.
Category information contributes to the sparse data problem Due to sparse data, some words
with similar meanings have negative cosine similarity value. For example, tiger as a fierce or au-
dacious person (tiger.n.01) and linguist as a specialist in linguistics (linguist.n.02) seldom appear in
the same context, leading to -0.1 cosine similarity value using GloVe word embeddings. However,
they are hyponyms of person.n.01, using this constraint our geometrical process transforms the N-
balls of tiger.n.01 and linguist.n.02 inside the N -ball of person.n.01, leading to high similarity value
measured by the typed cosine function.
Upper category identification Using N -ball embeddings, we can find upper-categories ofa word
sense. Given word sense ws, we collect all those cats satisfying Dinside (ws, cat) > 0. These cats
shall be upper-categories of ws. If we sort them in increasing order and mark the first cat with +1,
the second with +2 . . . , the cat marked with +1 is the direct upper-category of ws, the cat marked
with +2 is the direct upper-category of the cat with +1 . . . , as listed in Table 2.
6
Published as a conference paper at ICLR 2019
word sense 1	word sense 2 (Sim0, Cos)
beijing.n.01	london.n.01(>0.99, 0.47), atlanta.n.01(>0.99, 0.27) Washington.n.01(>0.99, -0.11), Paris.n.0 (>0.99, 0.46),potomac.n.02 (>0.99, 0.18), boston.n.01(>0,99, 028)
berlin.n.01	madrid.n.01(>0.99, 0.47),toronto.n.01(>0.99, 0.46), rome.n.01(>0.99, 0.68), COlumbia.n.03(>0.99, 0.39), Sydney.n.01(>0.99, 0.52), dallas.n.01(>0.99, 0.28)
berlin.n.02	simon.n.02 (>0.99, 0.34), Winiams.n.01(>0.99, 0.24), foster.n.01,(>0.99, 0.13), dylan.n.01 (>0.99, 0.10), mccartney.n.01 (>0.99, 0.23), lennon.n.01(>0.99, 0.25)
tiger.n.01	Survivor.n.02 (>0.99, 0.40), neighbor.n.01 (>0.99, 0.36), immune.n.01(>0.99, 0.10), linguist.n.02 (>0.99, -0.1), bilingual.n.01 (>0.99, -0.06), warrior.n.01 (>0.99, 0.68)
france.n.02	White.n.07(>0.99, 0.31), woollcott.n.01(>0,99, -0.12), uhland.n.01(>0,99,-0.32), london.n.02(>0.99, 0.52),journalist.n.01(>0.99, 0.33),poet.n.01(>0.99, 0.20)
cat.n.01	tiger.n.02(>0.99, 0.62), fox.n.01(>0.99, 0.44), wolf.n.01(>0.99, 0.67), WildCat.n.03(>0.99, 0.16),tigress.n.01(>0.99, 0.40), vixen.n.02(>0.99, 0.38)
y.n.02	q.n.01(>0.99, 0.45), delta.n.03(>0.99,0.33), n.n.05(>0.99, 0.60), p.n.02(>0.99, 0.44), f.n.04(>0.99, 0.55), g.n.09(>0.99, 0.52)	
Table 1: Top-6 nearest neighbors based on Sim0, the cos value of word stems are listed, e.g.
cos(beijing, london)= 0.47, tiger.n.01 refers to a fierce or audacious person.
Word sense 1	word sense 2
beijing.n.01	city.n.01+ι, municipality.n.01+2,region.n.03+3, location.n.01+4, ObjeCt.n.01+5, entity.n.01+6
berlin.n.02	SOngWriter.n.01+ι, composer.n.01+2, musician.n.02+3, artist.n.01+4, Creator.n.02+5
tiger.n.01	PerSOn.n.01+ι, OrganiSm.n.01+2, Whole.n.02+3, ObjeCt.n.01+4, entity.n.01+5
franCe.n.02	Writer.n.01+ι, COmmUniCator.n.01+2, PerSOn.n.01+3, OrganiSm.n.01+4
Cat.n.01	WiIdCat.n.03+ι, lynx.n.02+2, COUgar.n.01+3, bobCat.n.01+4, CaraCal.n.01+5, OCelot.n.01+6, feline.n.01+7,jaguarundi.n.01+8
y.n.02	letter.n.02+ι CharaCter.n.08+2 symbol.n.01+3 , SignaLn.01+4 COmmUniCation.n.02+5
Table 2: +k represents the kth minimal positive value of Dinside .
4.4	Experiment 4: Membership Validation
The fourth experiment is to validate the category of an unknown word, with the aim to demonstrate
the predictive power of the embedding approach (Baroni et al., 2014). We describe the task as
follows: Given pre-trained word embeddings EN with vocabulary size N, and a tree structure of
hyponym relations TK on vocabulary WK, (K < N). Given wx ∈/ WK and c ∈ WK, we need
to decide whether wx ∈ c. For example, when we read mwanza is also experiencing, we may
guess mwanza a person, if we continue to read mwanza is also experiencing major infrastructural
development, we would say mwanza is a city. If mwanza is not in the current taxonomy of hypernym
structure, we only have its word embedding from the text. Should mwanza be a city, or a person?
Dataset From 54,310 word senses, we randomly selected 1,000 word senses of nouns and verbs
as target categories, with the condition that each of them has at least 10 direct hyponyms; For each
target category, we randomly select p% (p ∈ [5, 10, 20, 30, 40, 50, 60, 70, 80, 90]) from its direct
hyponyms as training data. The test data sets are generated from three sources: (1) the rest 1 - p%
from the direct hyponyms as true values, (2) randomly choose 1,000 false values from WK ; (3)
1,000 words from WN which do not exist in WK. In total, we created 118,938 hyponymy relations
in the training set, and 17,975,042 hyponymy relations in the testing set.
Method We develop an N -ball solution to solve the membership validation task as follows:
Suppose c has a hypernym path [c, h1 , h2 , . . . , hm] and has several known members (direct hy-
ponyms) t1 , . . . , ts . For example, city.n.01 has a hypernym path [city.n.01, municipality.n.01,
Urbanqrea.n.01, ..., entity.n.01 ] and a number of known members oxford.n.01, banff.n.01,
chicago.n.01. We construct N -ball embeddings for this small tree with the stem [c, h1, h2, . . . , hm]
and leaves t1, . . . , ts, and record the history of the geometric transformations of each ball. Suppose
that wx be a member of c, we initialize the N -ball of wx using the same parameter as the N -ball of
t1, and apply the recorded history of the geometric transformations of t1’s N -ball for wx’s N -ball.
If the final N -ball of wx is located inside the N -ball of c, we will decide that wx is the member
of c, otherwise not. This method can be explained in terms of Support Vector Machines (Shawe-
7
Published as a conference paper at ICLR 2019
(a) Precision, recall, and F1
Figure 4: (a) Precision, recall, F1 score of hypernym prediction, when 5%, 10%, 20%, . . . , 90% of
the members are used for training; (b) Mode, mean, and population standard deviation of the recall,
when 5%, 10%, 20%, . . . , 90% of the members are used for training.
(b) Mode, mean, and pstdev of recall
Taylor & Cristianini, 2004) as follows: the boundary of c’s N -ball is supported by all N -balls of its
known members. If the unknown word were a member of c, its N -ball shall have the same PLC as
the member of c, and according to the principle of family action, it shall have the same geometric
transformation, and will contribute one candidate parent ball. We can introduce a ratio γ ≥ 1 to
zoom-out the boundary of c’s N -ball.
Evaluation and Analysis In the experiment, results show that the N -ball method is very precise
and robust as shown in Figure 4(a): the precision is always 100%, even only 5% from all members
is selected as training set. The method is quite robust: If we select 5% as training set, the recall
reaches 76.8%; if we select 50% as training set, the recall reaches 96.7%. Theoretically, the N-
ball method can not guarantee 100% recall as shown in Figure 4(b). If p < 70%, the population
standard deviation (pstdev) decreases with the increasing percentage of selected training data. When
p > 70%, there is a slight increase of pstdev. The reason is that in the experiment setting, if more
than 80% of the children are selected, it can happen that only one unknown member is left for
validating. If this single member is excluded outside of the category’s N -ball, the recall drops to
0, which increases pstdev. The experiment result can be downloaded at https://figshare.
com/articles/membership_validation_results/7571297.
In the literature of representational learning, margin-based score functions are the state-of-the-art
approach (Gutmann & Hyvarinen, 2012): The score of a positive sample shall be larger than the
score of a negative sample plus a margin. This can be understood as a simple use of categorization -
no chained subordinate relations, no clear membership relations of negative samples, no requirement
on zero energy loss. However, when category information is fully and strictly used, the precision
will increase significantly, and surprisingly reach 100% in this experiment.
5	Related Work
Lenci & Benotto (2012) explored the possibility of identifying hypernyms in distributional semantic
model; Santus et al. (2014) presented an entropy-based model to identify hypernyms in an unsuper-
vised manner; Kruszewski et al. (2015) induced mappings from words/sentences embeddings into
Boolean structure, with the aim to narrow the gap between co-occurrence based embeddings and
logic-based structures. There are some works on word embedding and knowledge graph embed-
ding using regions to represent words or entities. Athiwaratkun & Wilson (2017) used multi-modal
Gaussian distribution to represent words; He et al. (2015) embedded entities using Gaussian distri-
butions; Xiao et al. (2016) used manifolds to represent entities and relations; Nickel & Kiela (2017)
used Poincare balls to embed tree structures. Mirzazadeh et al. (2015) share certain common inter-
est with the presented work in embedding constraints. However, in none of these works, structural
imposition at zero-energy cost is targeted.
8
Published as a conference paper at ICLR 2019
6	Conclusion and On-going Work
We proposed a novel geometric method to precisely impose external tree-structured category in-
formation onto word embeddings, resulting in region-based (N -ball embeddings) word sense em-
beddings. They can be viewed as Venn diagrams (Venn, 1880) of the tree structure, if zero energy
cost is achieved. Our N -ball method has demonstrated great performance in validating the cate-
gory of unknown words, the reason for this being under further investigation. Our on-going work
also includes multi-lingual region-based knowledge graph embedding where multiple relations on
directed acyclic graphs need to be considered. N -balls carry both vector information from deep
learning and region information from symbolic structures. Therefore, N -balls establish a harmony
between Connectionism and Symbolicism, as discussed by Marcus (2003), and thus may serve as
a novel building block for the commonsense representation and reasoning in Artificial Intelligence.
N -balls, in particular, contribute a new topic to Qualitative Spatial Reasoning (QSR) that dates back
to Whitehead (1929).
Acknowledgments
Partial financial support from NSFC under Grant No. 61472177 and 61661146007, and from BMBF
(BUndesministeriUm fur BildUng Und ForschUng) of Germany under grant number 01/S17064 and
01/S18038C are greatly acknowledged. We are greatly indebted to Dagstuhl Seminar 15201 Cross-
LingualCross-Media Content Linking: Annotations and Joint Representations for frUitfUl and inter-
esting discUssions. We are also thankfUl for critical comments from three anonymoUs reviewers, and
for the OpenReview policy.
References
Ben AthiWaratkun and Andrew Wilson. Multimodal word distribUtions. In ACL'17, pp. 1645-1656,
2017.
Marco Baroni, Georgiana Dinu, and German Kruszewski. Don't count, predict! a systematic com-
parison of context-coUnting vs. context-predicting semantic vectors. In ACL’14, pp. 238-247.
Association for Computational Linguistics, 2014. doi: 10.3115/v1/P14-1023.
Ronan Collobert, Jason Weston, Leon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel
Kuksa. Natural language processing (almost) from scratch. J. Mach. Learn. Res., 12:2493-2537,
November 2011. ISSN 1532-4435.
Ernest Davis and Gary F. Marcus. Commonsense Reasoning and Commonsense Knowledge in
Artificial Intelligence. Communications of the ACM, 58(9):92-103, 9 2015. ISSN 0001-0782.
Tiansi Dong. A Comment on RCC: from RCC to RCC++. Journal of Philosophical Logic, 37(4):
319-352, 2008.
Katrin Erk. Supporting inferences in semantic space: Representing words as regions. In IWCS-8’09,
pp. 104-115, Stroudsburg, PA, USA, 2009. Association for Computational Linguistics.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and
Eytan Ruppin. Placing search in context: the concept revisited. In WWW, pp. 406-414, 2001.
Michael U. Gutmann and Aapo Hyvarinen. Noise-contrastive estimation of unnormalized statisti-
cal models, with applications to natural image statistics. J. Mach. Learn. Res., 13(1):307-361,
February 2012. ISSN 1532-4435.
Xu Han, Zhiyuan Liu, and Maosong Sun. Joint representation learning of text and knowledge for
knowledge graph completion. CoRR, abs/1611.04125, 2016.
Shizhu He, Kang Liu, Guoliang Ji, and Jun Zhao. Learning to represent knowledge graphs with
gaussian embedding. In CIKM’15, pp. 623-632, New York, USA, 2015. ACM.
Eric H. Huang, Richard Socher, Christopher D. Manning, and Andrew Y. Ng. Improving word
representations via global context and multiple word prototypes. In ACL’12: Long Papers -
Volume 1, pp. 873-882, Stroudsburg, PA, USA, 2012. Association for Computational Linguistics.
9
Published as a conference paper at ICLR 2019
German Kruszewski, Denis Paperno, and Marco Baroni. Deriving boolean structures from distribu-
tional vectors. Transactions ofthe Association of Computational Linguistics, 3:375-388, 2015.
Alessandro Lenci and Giulia Benotto. Identifying hypernyms in distributional semantic spaces. In
SemEval ’12, pp. 75-79, Stroudsburg, PA, USA, 2012. ACL.
Omer Levy and Yoav Goldberg. Dependency-based word embeddings. In ACL’14, Volume 2: Short
Papers, pp. 302-308, Baltimore, Maryland, June 2014. ACL.
Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schutze. Introduction to Informa-
tion Retrieval. Cambridge University Press, New York, NY, USA, 2008. ISBN 0521865719,
9780521865715.
Gary F. Marcus. The Algebraic Mind - Integrating Connectionism and Cognitive Science. The MIT
Press, 2003.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781, 2013.
George A. Miller. Word-Net: A Lexical Database for English. Commun. ACM, 38(11):39-41, 1995.
Farzaneh Mirzazadeh, Siamak Ravanbakhsh, Nan Ding, and Dale Schuurmans. Embedding Infer-
ence for Structured Multilabel Prediction. In NIPS’15, pp. 3555-3563. MIT Press, 2015.
Maximillian Nickel and Douwe Kiela. PoinCare embeddings for learning hierarchical representa-
tions. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 6338-6347. Cur-
ran Associates, Inc., 2017.
Jeffrey Pennington, Richard Socher, and Christopher D. Manning. GloVe: Global Vectors for Word
Representation. In EMNLP’14, pp. 1532-1543, 2014.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Neurocomputing: Foundations
of research. chapter Learning Representations by Back-propagating Errors, pp. 696-699. MIT
Press, Cambridge, MA, USA, 1988. ISBN 0-262-01097-6.
Enrico Santus, Alessandro Lenci, Qin Lu, and Sabine Schulte im Walde. Chasing hypernyms in
vector spaces with entropy. In EACL 2014, Gothenburg, Sweden, pp. 38-42, 2014.
Fabrizio Sebastiani. Machine learning in automated text categorization. ACM Comput. Surv., 34(1):
1-47, March 2002. ISSN 0360-0300. doi: 10.1145/505282.505283.
John Shawe-Taylor and Nello Cristianini. Kernel Methods for Pattern Analysis. Cambridge Univer-
sity Press, 2004. ISBN 0521813972.
Stefanie Tellex, Boris Katz, Jimmy Lin, Aaron Fernandes, and Gregory Marton. Quantitative eval-
uation of passage retrieval algorithms for question answering. In Proceedings of the 26th Annual
International ACM SIGIR Conference on Research and Development in Informaion Retrieval,
SIGIR ’03, pp. 41-47, New York, NY, USA, 2003. ACM. ISBN 1-58113-646-3.
John Venn. On the diagrammatic and mechanical representation of propositions and reasonings.
The London, Edinburgh and Dublin Philosophical Magazine and Journal of Science, 10(58):1-
18, 1880. doi: 10.1080/14786448008626877.
Alfred N. Whitehead. Process and Reality. Macmillan Publishing Co., Inc., 1929.
Han Xiao, Minlie Huang, and Xiaoyan Zhu. From one point to a manifold: Knowledge graph
embedding for precise link prediction. In IJCAI, pp. 1315-1321, 2016.
Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, and Jun Zhao. Relation classification via
convolutional deep neural network. In COLING, pp. 2335-2344, 2014.
10