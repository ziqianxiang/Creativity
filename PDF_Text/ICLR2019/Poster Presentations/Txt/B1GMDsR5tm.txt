Published as a conference paper at ICLR 2019
Initialized	Equilibrium	Propagation	for
Backprop-Free Training
Peter O’Connor, Efstratios Gavves, Max Welling
QUVA Lab
University of Amsterdam
Amsterdam, Netherlands
peter.ed.oconnor@gmail.com,egavves@uva.nl,m.welling@uva.nl
Ab stract
Deep neural networks are almost universally trained with reverse-mode automatic
differentiation (a.k.a. backpropagation). Biological networks, on the other hand,
appear to lack any mechanism for sending gradients back to their input neurons,
and thus cannot be learning in this way. In response to this, Scellier & Bengio
(2017) proposed Equilibrium Propagation - a method for gradient-based train-
ing of neural networks which uses only local learning rules and, crucially, does
not rely on neurons having a mechanism for back-propagating an error gradient.
Equilibrium propagation, however, has a major practical limitation: inference in-
volves doing an iterative optimization of neural activations to find a fixed-point,
and the number of steps required to closely approximate this fixed point scales
poorly with the depth of the network. In response to this problem, we propose Ini-
tialized Equilibrium Propagation, which trains a feedforward network to initialize
the iterative inference procedure for Equilibrium propagation. This feed-forward
network learns to approximate the state of the fixed-point using a local learning
rule. After training, we can simply use this initializing network for inference, re-
sulting in a learned feedforward network. Our experiments show that this network
appears to work as well or better than the original version of Equilibrium propaga-
tion while requiring fewer steps to converge. This shows how we might go about
training deep networks without using backpropagation.
1	Introduction
Deep neural networks are almost always trained with gradient descent, and gradients are almost
always computed with backpropagation. For those interested in understanding the working of the
brain in the context of machine learning, it is therefore distressing that biological neurons appear not
to send signals backwards.
Biological neurons communicate by sending a sequence of pulses to downstream neurons along a
one-way signaling pathway called an “axon”. If neurons were doing backpropagation, one would
expect a secondary signalling pathway wherein gradient signals travel backwards along axons. This
appears not to exist, so it seems that biological neurons cannot be doing backpropagation.
Moreover, backpropagation may not be the ideal learning algorithm for efficient implementation
in hardware, because it involves buffering activations for each layer until an error gradient returns.
This requirement becomes especially onerous when we wish to backpropagate through many steps
of time, or through many layers of depth. For these reasons, researchers are looking into other
means of neural credit assignment - mechanisms for generating useful learning signals without doing
backpropagation.
Recently, Scellier & Bengio (2017) proposed a novel algorithm called Equilibrium Propagation,
which enables the computation of parameter gradients in a deep neural network without backpropa-
gation. Equilibrium Propagation defines a neural network as a dynamical system, whose dynamics
0Code available at https://github.com/QUVA-Lab/init-eqprop
1
Published as a conference paper at ICLR 2019
follow the negative-gradient of an energy function. The “prediction” of this network is the fixed-
point of the dynamics - the point at which the system settles to a local minimum energy given the
input, and ceases to change. Because of this inference scheme, Equilibrium Propagation is imprac-
tically slow for large networks - the network has to iteratively converge to a fixed point at every
training iteration.
In this work, we take inspiration from Hinton et al. (2015) and distill knowledge from a slow, energy
based equilibrating network into a fast feedforward network by training the feedforward network
to predict the fixed-points of the equilibrating network with a local loss. At the end of training,
we can then discard the equilibrating network and simply use our feedforward network for test-
time inference. We thus have a way to train a feedforward network without backpropagation. The
resulting architecture loosely resembles a Conditional Generative Adversarial Network (Mirza &
Osindero, 2014), where the feedforward network produces a network state which is evaluated by the
energy-based equilibrating network.
To aid the reader, this paper contains a glossary of symbols in Appendix A.
2 Methods
2.1	Background: Equilibrium Propagation
Equilibrium Propagation (Scellier & Bengio, 2017) is a method for training an Energy-Based Model
Hopfield (1984) for classification. The network performs inference by iteratively converging to a
fixed-point, conditioned on the input data, and taking the state of the output neurons at the fixed
point to be the output of the network. The network’s dynamics are defined by an energy function
over neuron states s and parameters θ = (w, b):
Eθ (s,x)=2 Xsi2 - X biρ(si) -	X	wij ρ(si)ρ(sj) - X	xiwijρ(sj)	(1)
i∈S	i∈S	j ∈S,i∈αj ∩S	j∈S,i∈αj ∩I
Where I is the set of input neuron indices, S is the set of non-input neuron indices; s ∈ R|S| is the
vector of neuron states; where αj ⊂ {I ∪ S} is the set of neurons connected to neuron j; x denotes
the input vector; and ρ is some nonlinearity; w is a weight matrix with a symmetric constraint:
wij = wji1, and entries only defined for {wij : i ∈ αj} The state-dynamics for non-input neurons,
derived from Equation 1, are:
∂Eθ(s, x)
dsj
—
∂t
∂sj
bj+	wij ρ(si) + Σ	wij xi ∀j ∈ S
j ∈S,i∈αj ∩S	j ∈S,i∈αj ∩I
(2)
The network is trained using a two-phase procedure, with a negative and then a positive phase. In
the negative phase, the network is allowed to settle to an energy minimum s- := arg mins Eθ(s, x)
conditioned on a minibatch of input data x. In the positive phase, a target y is introduced, and
the energy function is augmented to “perturb” the fixed-point of the state towards the target with a
“clamping factor” β: Eθβ(s, x, y) = Eθ(s, x) + βC(sO, y), where β is a small scalar and C(sO, y)
is a loss defined between the output neurons in the network and the target y (we use C(sO, y) =
ksO - yk22). The network is allowed to settle to the perturbed state s+ := argmins(Eβ(s, x, y)).
Finally, the parameters of the network are learned based on a contrastive loss between the negative-
phase and positive-phase energy, which can be shown to be proportional to the gradient of the output
loss in the limit of β → 0:
1However as described in Scellier et al. (2018), the symmetry requirement can be relaxed without signifi-
cantly impacting performance. In this case there is no energy function, but one simply defines the network in
terms of the state dynamics. The network nevertheless seems to learn to settle to fixed-points rather than falling
into limit cycles or chaotic dynamics. The reason for this phenomenon is not well understood.
2
Published as a conference paper at ICLR 2019
η (∂Eθ(s+,x)	∂Eθ(s-,x) \ 〜∂C(s-,y)
- β V ∂θ	∂θ ) H	∂θ-
(3)
Where η is some learning rate; O ⊆ S is the subset of output neurons. This results in a local learning
rule, where parameter changes only depend on the activities of the pre- and post-synaptic neurons:
δWij = β (P(S+ )ρ(s+) - P(S-)P(S-))	⑷
δd = η (P(S+)-P(S-))	⑸
β
Intuitively, the algorithm works by adjusting θ to pull arg mins Eθ (S, x) closer to
arg mins Eθβ (S, x, y) so that the network will gradually learn to naturally minimize the output loss.
Because inference involves an iterative settling process, it is an undesirably slow process in Equi-
librium propagation. In their experiments, Scellier & Bengio (2017) indicate that the number of
settling steps required scales super-linearly with the number of layers. This points to an obvious
need for a fast inference scheme.
2.2	Adding an Initialization Network
We propose training a feedforward network fφ(x) → Sf ∈ R|S| to predict the fixed-point of the
equilibrating network. This allows the feedforward network to achieve two things: First, it initializes
the state of the equilibrating network, so that the settling process starts in the right regime. Second,
the feedforward network can be used to perform inference at test-time, since it learns to approximate
the minimal-energy state of the equilibrating network, which corresponds to the prediction. fφ(x)
is defined as follows:
fφ(x) := (Sjf : j ∈ S)
/ /
Sf := P	E ωiSf
i∈αjf ∩S
∈ R|S|
∈R
(6)
Where αjf = (i : (i ∈ αj ) ∧ (i < j)) is the set of feedforward connections to neuron j (which is
a subset of αj - the full set of connections to neuron j from the equilibrium network from Equation
1); φ = (ω, c) is the set of parameters of the feedforward network. This feedforward network
produces the initial state of the negative phase of equilibrium propagation network, given the input
data - i.e., instead of starting at a zero-state, the equilibrium-propagation network is initialized in a
state Sf := fφ(x). We train the parameters φ to approximate the minimal energy state S- of the
equilibrating network 2. In other words, we seek:
φ* := argmin L(Sf,s-)	(7)
φ
L(Sf,S-) := XLi(Sif, Si-) := X(Sif - Si-)2	(8)
i∈S	i∈S
The derivative of the forward parameters of the i’th neuron, φi = (ωαi,i, ci), can be expanded as:
2We could also minimize the distance with s+ , but found experimentally that this actually works slightly
worse than s- . We believe that this is because equilibrium propagation depends on s- being very close to
a true minimum of the energy function, and so initializing the negative phase to sf ≈ s- will lead to better
gradient computations than when we initialize the negative phase to sf ≈ x+
3
Published as a conference paper at ICLR 2019
local __________distant______
dL := X	dLj (Sj,	1 2 3 4 Sj	) = dLi dsf	+ X d Lj	dsj	dsf
dφi j∈S dφi	∂sj dφi j>i ∂sj	∂sj	dφi
(9)
∂sf
The distant term is problematic, because computing -f would require backpropagation, and the
∂si
entire purpose of this exercise is to train a neural network without backpropagation. However, we
find that only optimizing the local term 需 does not noticeably harm performance. In Section 2.4
∂φi
we go into more detail on why it appears to be sufficient to minimize local losses.
Over the course of training, parameters φ will learn until our feedforward network is a good predictor
of the minimal-energy state of the equilibrating network. This feedforward network can then be used
to do inference: we simply take the state of the output neurons to be our prediction of the target data.
The full training procedure is outlined in Algorithm 1. At the end of training, inference can be done
either by taking the output activations from the forward pass of the inference network fφ (Algorithm
2), or by initializing with a forward pass and then iteratively minimizing the energy (Algorithm
3). Experiments in Section 3 indicate that the forward pass performs just as well as the full energy
minimization.
Algorithm 1 Training
Algorithm 2 Feedforward Inference
1: Input: Dataset (x, y), Step Size ,
Learning Rate η , Network Architecture
α, Number of negative-phase steps T- ,
Number of positive-phase steps T +
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
φ J InitializeFeedfOrWardParameters(α)
θ J InitializeEquilibriumParameters (α)
while not converged do
xm, ym → SampleMinibatch(x, y)
s J sf J fφ (xm )
for t ∈ 1..T- do # Neg. Phase
S J S - e IdEdxm
s- J s
for t ∈ 1..T + do # Pos. Phase
∂Eθβ(s ,xm,ym)
S J S - C ∂
s Js
A	A η ∂ ∂Eθ (s+,x)	∂Eθ (s-,x)
θ J θ - β( ∂θ	∂θ
φi J φi- ndLi∂Φi,s-)∀i
1:	Input: Input Data x, Inference Parame-
ters φ
2:	s — fφ(x)
3:	return (si : i ∈ O) # Output unit states
Algorithm 3 Iterative Inference
1: Input: Input Data x, Initialization Pa-
rameters φ, Equilibriating Parameters θ,
Number of Negative Steps T-
2: s J fφ(x)
3: for t ∈ 1..T- do # Neg. Phase
4:	s J s - c d&(s，Xm)
∂s
5: return (si : i ∈ O) # Output unit states
15: Return: φ, θ # Parameters
2.3	Including the forward states in the energy function
The fixed point s- of the equilibrating network is a nonlinear function of x, whose value is com-
puted by iterative bottom-up and top-down inference using all of the parameters θ. The initial state
sf , by contrast, is generated in a single forward pass, meaning that the function relating sjf to its
direct inputs Saj ∈ R1 ɑj | is constrained to the form of Equation 6. Because of this, the computation
resulting in s- may be more flexible than that of the forward pass, so it is possible for the equili-
brating network to create targets that are not achievable by the neurons in the feedforward network.
This is similar to the notion of an “amortization gap” in variational inference, and we discuss this
connection more in Section 4.2.
Neurons in the feedforward network simply learn a linear mapping from the previous layer’s acti-
vations to the targets provided by the equilibrating network. In order to encourage the equilibrating
4
Published as a conference paper at ICLR 2019
network to stay in the regime that is reachable by the forward network, we can add a loss encourag-
ing the fixed points to stay in the regime of the forward pass.
Eθλ(s, x) = Eθ(s, x) + λ	(sjf - sj)2	(10)
j∈S
Where λ is a hyperparameter which brings the fixed-points of the equilibrating network closer to
the states of the forward pass, and encourages the network to optimize the energy landscape in the
region reachable by the forward network. Of course this may reduce the effective capacity of the
equilibrating network, but if our goal is only to train the feedforward network, this does not matter.
This trick has a secondary benefit: It allows faster convergence in the negative phase by pulling the
minimum of Eθλ(s, x) closer to the feedforward prediction, so we can learn with fewer convergence
steps. It can however, cause instabilities when set too high. We investigate the effect of different
values of λ with experiments in Appendix D.
2.4	Why the local loss is sufficient: Gradient Alignment
In Equation 9 we decompose the loss-gradient of parameters φ into a local and a global component.
Empirically (see Figures 1, 3), we find that using the local loss and simply ignoring the global loss
led to equally good convergence. To understand why this is the case, let use consider a problem
where We learn the mapping from an input X to a set of neuron-wise targets: s*. Assume these
targets are generated by some (unknown) set of ideal parameters φ*, so that s* = fφ* (x). To
illustrate, we consider a two layer network with φ = (w1,w2) and φ* = (w*,w2):
s1 =	ρ(xw1)	s1*	=	ρ(xw1*)	L1 = ks1 -	s1*k22
s2 =	ρ(s1w2)	s2*	=	ρ(s*1w2*)	L2 = ks2 -	s*2k22
(11)
It may come as a surprise that when φ is in the neighbourhood of the ideal parameters φ*, the cosine
similarity between the local and distant gradients: S (∂WL1, ∂WL2) is almost always positive, i.e.
the local and distant gradients tend to be aligned. This is a pleasant surprise because it means the
local loss will tend to guide us in the right direction. The reason becomes apparent when we define
∆w := w - w*, and write out the expression for the gradient in the limit of ∆w → 0 (see Appendix
B for derivation)
∂L1
∂w1
∂L2
∂w1
= xT (x∆w1 ρ0(xw1) ρ0(xw1))
∆w→0
________________________Gi
= xT x∆w1	ρ0(xw1)w2	ρ0(s1w2)2w2T	ρ0(xw1)
(12)
_________________G2
xT s1∆w2	ρ0(s1w2)2w2T	ρ0(xw1)
When the term w2 Θ ρ0(s1 ∙ w2)2 ∙ WT is proportional to an identity matrix, we can see that ∂L1
and G1 are perfectly aligned. This will be the case when w2 is orthogonal and layer 2 has a
linear activation. However, even for randomly sampled parameters and a nonlinear activation,
w2 Θ ρ0(s1 ∙ w2)2 ∙ WT tends to have a strong diagonal component and the terms thus tend to
be positively aligned. Figure 1 demonstrates that this gradient-alignment tends to increase as
then network trains to approximate a set of targets (i.e. as φ → φ*). Note that the align-
ment of the local loss-gradient with the global loss-gradient is at least as high as with the dis-
tant loss-gradient, because VφLglobal = VφLloCal + ^φLdistant and S(VφLdistant, VφLloCal) ≤
S(^φLdistant + φLlocal, VφLlocal ) ∀VφLlocal, VφLdistant.
This explains the empirical observation in Figures 1 and 3 that optimizing the local, as opposed to
the global, loss for the feedforward network does not appear to slow down convergence: Later layers
do not have to “wait” for earlier layers to converge before they themselves converge - earlier layers
5
Published as a conference paper at ICLR 2019
Iterations
S(V≠1Lι,VφlL2 : 7)
S(VfaL2,VφaL3^)
S(VfeL3,VfcL4:7)
S(VH4,%，5:7)
S(W5,5尸足6:7)
S(VH6，%47:7)
Iterations
Figure 1: We train a 6-layer network with parameters φ to predict layerwise targets generated by an-
other network with random parameters φ*. Left: We compare the convergence of the global loss of
two training runs starting from the same initial conditions and identical (untuned) hyperparamters: A
network with parameters φlocal trained using only local losses anda network with parameters φglobal
trained directly on the global loss. We note that the locally trained network converges significantly
faster, suggesting that optimization is easier in the absence of the “confusing” distant-gradient sig-
nals from the not-yet-converged higher layers. Right: We plot the cosine-similarity of local and
distant components of the gradient of φlocal as training progresses. We see that as we approach
convergence (as φiocai → φ*), the local and distant gradients tend to align.
optimize the loss of later layers right from the beginning of training. As shown in Figure 1, it may
in fact speed up convergence since each layer’s optimizer is solving a simpler problem (albeit with
changing input representations for layers > 1).
When local targets s- are provided by the equilibrating network, it is not in general true that there
exists some φ* such that S- = s*. In our experiments, We observed that this did not prevent the
forward network from learning to classify just as well as the equilibrating network. However, this
may not hold for more complex datasets. As mentioned in Section 2.3, this could be resolved in
future work with a scheme for annealing λ up to infinity while maintaining stable training.
3	Experiments
We base our experiments off of those of Scellier & Bengio (2017): We use the hard sigmoid ρ(x) =
max(0, min(1, x)) as our nonlinearity. We clip the state of si to the range (0, 1) because, since
ρ0 (x) = 0 : x < 0 ∨ x > 1, if the system in Equation 2 were run in continuous time, it should
never reach states outside this range. Borrowing a trick from Scellier & Bengio (2017) to avoid
instability problems arising from incomplete negative-phase convergence, we randomly sample β 〜
U ({-βbase , +βbase}), where βbase is a small positive number, for each minibatch and use this for
both the positive phase and for multiplying the learning rate in Equation 3 (for simplicity, this is not
shown in Algorithm 1). 3 . Unlike Scellier & Bengio (2017), we do not use the trick of caching
and reusing converged states for each data point between epochs. In order to avoid “dead gradient”
zones, we modify the activation function of our feedforward network (described in Equation 6) to
ρmod(x) = ρ(x) + 0.01x, where the 0.01 “leak” is added to prevent the feed-forward neurons from
getting stuck due to zero-gradients in the saturated regions. We use λ = 0.1 as the regularizing
parameter from Equation 10, having scanned for values in Appendix D.
3.1	MNIST
We verify that the our algorithm works on the MNIST dataset. The learning curves can be seen in
Figure 2. We find, somewhat surprisingly, that the forward pass of our network performs almost
indistinguishably from the performance of the negative-phase of Equilibrium Propagation. This
encouraging result shows that this approach for training a feedforward network without backprop
does indeed work. We also see from from the top-two panels of Figure 2 that our approach can
stabilize Equilibrium-Prop learning when we run the network for fewer steps than are needed for
3 When β is negative, the positive-state s+ is pushed away from the targets, but gradients still point in the
correct direction because the learning rate is scaled by — 1∕β. This trick avoids an instability when, due to
incomplete negative-phase convergence, the network continues approaching the true minimum of E(s, x) in
the positive phase, and thus on every iteration contues to push down the energy of this “true” negative minimum
6
Published as a conference paper at ICLR 2019
-	[784-500-10], 4-step	[784-500-500-500-10], 20-step
o 30 -
LU
⅛)
,20-
IlO-
"lΛ
S
to
O 0-
Figure 2: Learning Curves on MNIST comparing the performance of Equilibrium Propagation (Eq
Prop: s-), the Forward-Pass in Initialized Equilibrium Propagation (Fwd Eq Prop: sf) (Algorithm
2) and the Negative Phase in Initialized Equilibrium Propagation (Fwd Eq Prop: s- ) (Algorithm 3)
Numbers indicate error at the final test. Left Column: A shallow network with a single hidden layer
of 500 units. Right Column: A deeper network with 3 layers of [500, 500, 500] hidden units. Top
Row: Training with a small-number of negative-phase steps (4 for the shallow network, 20 for the
deeper) shows that feedfoward initialization makes training more stable by providing a good starting
point for the negative phase optimization. The Eq Prop s- lines on the upper plots are shortened
because we terminate training when the network fails to converge. Bottom Row: Training with
more negative-phase steps shows that when the baseline Equilibrium Propagation network is given
sufficient time to converge, it performs comparably with our feedforward network (Note that the
y-axis scale differs from the top).
Epochs
Figure 3: Test scores and gradient alignment on [784-500-500-500-10] network trained on MNIST
Left: We compare the performance of Initialized Equilibrium Propagation when the feedforward
network is trained using only local losses vs the global loss (i.e. using backpropagation). sf denotes
the forward pass and s- denotes the state at the end of the negative phase. Note that we observe
no disadvantage when we only use local losses. Right: We observe the same effect as for our toy
problem (see Figure 1). Early on in training, the local error gradients tend to align with gradients
coming from higher layers.
full convergence. By initializing the negative phase in a close-to-optimal regime, the network is able
to learn when the number of steps is so low that plain Equilibrium Propagation cannot converge.
Moreover we note that as the number of steps is enough for convergence, there is not much advantage
to using more negative-phase iterations - the longer negative phase does not improve our error.
In Figure 3 we demonstrate that using only local losses to update the feedforward network comes
with no apparent disadvantage. In line with our results from Section 2.4, we see that local loss
gradients become aligned with the loss gradients from higher layers, explaining why it appears to be
sufficient to only use the local gradients.
7
Published as a conference paper at ICLR 2019
4	Related Work
The most closely related work to ours is by Bengio et al. (2016). There, the authors examine the idea
of initializing an iterative settling process with a forward pass. They propose using the parameters
of the Equilibriating network to do a forward pass, and describe the conditions under which this
provides a good approximation of the energy-minimizing state. Their conclusion is that this criterion
is met when consecutive layers of the energy-based model form a good autoencoder. Their model
differs from ours in that the parameters of the forward model are tied to the parameters of the
energy-based model. The effects of this assumption are unclear, and the authors do not demonstrate
a training algorithm using this idea.
Our work was loosely inspired by Hinton et al. (2015), who proposed “distilling” the knowledge of
a large neural network or ensemble into a smaller network which is designed to run efficiently at
inference time. In this work, we distill knowledge from a slow, equilibrating network in to a fast
feedforward network.
4.1	Relation to Adversarial Learning
Several authors (Kim & Bengio, 2016), (Finn et al., 2016), (Zhai et al., 2016) have pointed out the
connection between Energy Based Models and Generative Adversarial Networks (GANs). In these
works, a feedforward generator network proposes synthetic samples to be evaluated by an energy-
based discriminator, which learns to push down the energy of real samples and push up the energy of
synthetic ones. In these models, both the generator/sample proposer and the discriminator/energy-
based-model are deep feedforward networks trained with backpropagation.
In our approach, we have a similar scenario. The inference network fφ can be thought of as a
conditional generator which produces a network state sf given a randomly sampled input datum
x: sf = fφ(x). Parameters φ are trained to approximate the minimal-energy states of the energy
function: minφ kfφ(x) - arg mins Eθ(s, x)k. However, in our model, the Energy-Based network
Eθ(s, x) does not directly evaluate the energy of the generated data sf, but of the minimizing state
s- = arg mins Eθ (s, x) which is produced by performing T - energy-minimization steps on sf
(see Algorithm 1). Like a discriminator, the energy-based model parameters θ learn based on a
contrastive loss which pushes up the energy of the “synthetic” network state s-
+
the energy of the real state s .
while pushing down
4.2	Relation to Amortized Variational Inference
In variational inference, we aim to estimate a posterior distribution p(z|x) over a latent variable z
given data x, using an approximate posterior q(z). Algorithms such as Expectation Maximization
(Dempster et al., 1977) iteratively update a set of posterior parameters μ per-data point, so that
Zn 〜q(z∣μn). In amortized inference, We instead learn a global set of parameters φ which can
map a sample Xn to a posterior estimate Zn 〜 qφ(z∣Xn). Dayan et al. (1995) proposed using
a “recognition’ network” as this amortized predictor, and Kingma & Welling (2013) showed that
you can train this recognition network efficiently using the reparameterization trick. However, this
comes at the expense of an “amortization gap” (Cremer et al., 2018) - where the posterior estimate
suffers due to the sharing of posterior estimation parameters across data samples. Several recent
works (Marino et al., 2018), (Li et al., 2017), (Kim et al., 2018), have proposed various versions of
a “teacher-student” framework, in which an amortized network qθ(z∣χ) provides an initial guess for
the posterior, which is then refined by a slow, non-amortized network which refines q(Z) in several
steps into a better posterior estimate. The “student” then learns to refine its posterior estimate using
the final result of the iterative inference. In the context of training Deep Boltzmann Machines,
Salakhutdinov & Larochelle (2010) trained a feedforward network with backpropagation to initialize
variational parameters which are then optimized to estimate the posterior over latent variables.
Initialized Equilibrium Propagation is a zero-temperature analog of amortized variational inference.
In the zero-temperature limit, the mean-field updates of variational inference reduce to coordinate
ascent on the variational parameters. The function of the amortized student network qΦ (z|x) is then
analogous to the function of our initializing network fφ(x), and the negative phase corresponds to
the iterative optimization of varational parameters from the starting point provided by fφ(x).
8
Published as a conference paper at ICLR 2019
4.3	Relation to other work in Local Credit-Assignment
Another interesting approach to shortening the inference phase in Equilibrium propagation was pro-
posed by Kohan et al. (2018). The authors propose a model that is almost a feedforward network,
except that the output layer projects back to the input layer. The negative phase consists of making
several feedforward passes through the network, reprojecting the output back to the input with each
pass. Although the resulting inference model is not a feedforward network, the authors claim that
this approach allows them to dramatically shorten convergence time of the negative phase.
There is also a notable similarity between Initialized Equilibrium Propagation and Method of Auxil-
iary Coordinates (Carreira-Perpinan & Wang, 2014). In that paper, the authors propose a scheme for
optimizing a layered feedforward network which consists of alternating optimization of the neural
activations (which can be parallelized across samples) and parameters (which can be parallelized
across layers). In order to ensure that the layer activations zk remain close to what a feedfor-
ward network can compute, the objective includes a layerwise cost 2∣∣zk - fk(zk-1)k2, where
Zk is layer k's activation, fk is layer k's function, and μ is a the strength of the layerwise cost (as
they anneal μ → ∞ this cost becomes a constraint). This is identical in form and function to our
λ Pj∈S (sf - s-)2 term in Equation 10. However, they differ from our method in that their neu-
rons backpropagate their gradients back to input neurons (albeit only across one layer). Taylor et al.
(2016) do something similar with using the Alternating Direction Method of Multipliers (ADMM),
where Lagrange multipliers enforce the “layer matching” constraints exactly. Both methods, unlike
Equilibrium Prop, are full-batch methods.
More broadly, other approaches to backprop-free credit assignment have been tried. Difference-
Target propagation (Lee et al., 2015) proposes a mechanism to send back targets to each layer,
such that locally optimizing targets also optimizes the true objective. Feedback-Alignment (Lill-
icrap et al., 2014) shows that, surprisingly, it is possible to train while using random weights for
the backwards pass in backpropagation, because the forward pass parameters tends to “align” to the
backwards-pass parameters so that the pseudogradients tend to be within 90° of the true gradients. A
similar phenomenon was observed in Equilibrium Propagation by Scellier et al. (2018), who showed
that when one removed the constraint of symmetric weight in Equilibrium propagation, the weights
would evolve towards symmetry through training. Finally, Jaderberg et al. (2016) used a very dif-
ferent approach - rather than create local targets, each layer predicts its own “pseudogradient”. The
gradient prediction parameters are then trained either by the true gradients (which no longer need
to arrive before a parameter update takes place) or by backpropagated versions of pseudogradients
from higher layers.
5	Discussion
In this paper we describe how to use a recurrent, energy-based model to provide layerwise targets
with which to train a feedforward network without backpropagation. This work helps us understand
how the brain might be training fast inference networks. In this view, neurons in the inference
network learn to predict local targets, which correspond to the minimal energy states, which are
found by the iterative settling of a separate, recurrently connected equilibrating network.
More immediately perhaps, this could lead towards efficient analog neural network designs in hard-
ware. As pointed out by Scellier & Bengio (2017), it is much easier to design an analog circuit to
minimize some (possibly unknown) energy function than it is to design a feedforward circuit and
a parallel backwards circuit which exactly computes its gradients. However it is very undesirable
for the function of a network to depend on peculiarities of a particular piece of analog hardware,
because then the network cannot be easily replicated. We could imagine using a hybrid circuit to
train a digital, copy-able feedforward network, which is updated by gradients computed in the ana-
log hardware. Without the constraint of having to backpropagate through the feedforward network,
designs could be simplified, for example to do away with the need for differentiable activation func-
tions or to use feedforward architectures which would otherwise suffer from vanishing/exploding
gradient effects.
9
Published as a conference paper at ICLR 2019
References
Yoshua Bengio, Benjamin Scellier, Olexa Bilaniuk, Joao Sacramento, and Walter Senn. Feedfor-
ward initialization for fast inference of deep generative networks is biologically plausible. arXiv
preprint arXiv:1606.01651, 2016.
Miguel Carreira-Perpinan and Weiran Wang. Distributed optimization of deeply nested systems. In
Artificial Intelligence and Statistics, pp.10-19, 2014.
Chris Cremer, Xuechen Li, and David Duvenaud. Inference suboptimality in variational autoen-
coders. arXiv preprint arXiv:1801.03558, 2018.
Peter Dayan, Geoffrey E Hinton, Radford M Neal, and Richard S Zemel. The helmholtz machine.
Neural computation, 7(5):889-904, 1995.
Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete data
via the em algorithm. Journal of the royal statistical society. Series B (methodological), pp. 1-38,
1977.
Chelsea Finn, Paul Christiano, Pieter Abbeel, and Sergey Levine. A connection between generative
adversarial networks, inverse reinforcement learning, and energy-based models. arXiv preprint
arXiv:1611.03852, 2016.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015.
John J Hopfield. Neurons with graded response have collective computational properties like those
of two-state neurons. Proceedings of the national academy of sciences, 81(10):3088-3092, 1984.
Max Jaderberg, Wojciech Marian Czarnecki, Simon Osindero, Oriol Vinyals, Alex Graves, David
Silver, and Koray Kavukcuoglu. Decoupled neural interfaces using synthetic gradients. arXiv
preprint arXiv:1608.05343, 2016.
Taesup Kim and Yoshua Bengio. Deep directed generative models with energy-based probability
estimation. arXiv preprint arXiv:1606.03439, 2016.
Yoon Kim, Sam Wiseman, Andrew C Miller, David Sontag, and Alexander M Rush. Semi-amortized
variational autoencoders. arXiv preprint arXiv:1802.02550, 2018.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Adam A Kohan, Edward A Rietman, and Hava T Siegelmann. Error forward-propagation: Reusing
feedforward connections to propagate errors in deep learning. arXiv preprint arXiv:1808.03357,
2018.
Dong-Hyun Lee, Saizheng Zhang, Asja Fischer, and Yoshua Bengio. Difference target propagation.
In Machine Learning and Knowledge Discovery in Databases, pp. 498-515. Springer, 2015.
Yingzhen Li, Richard E Turner, and Qiang Liu. Approximate inference with amortised mcmc. arXiv
preprint arXiv:1702.08343, 2017.
Timothy P Lillicrap, Daniel Cownden, Douglas B Tweed, and Colin J Akerman. Random feedback
weights support learning in deep neural networks. arXiv preprint arXiv:1411.0247, 2014.
Joseph Marino, Yisong Yue, and Stephan Mandt. Iterative amortized inference. arXiv preprint
arXiv:1807.09356, 2018.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint
arXiv:1411.1784, 2014.
Ruslan Salakhutdinov and Hugo Larochelle. Efficient learning of deep boltzmann machines. In
Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp.
693-700, 2010.
10
Published as a conference paper at ICLR 2019
Benjamin Scellier and Yoshua Bengio. Equilibrium propagation: Bridging the gap between energy-
based models and backpropagation. Frontiers in computational neuroscience, 11:24, 2017.
Benjamin Scellier, Anirudh Goyal, Jonathan Binas, Thomas Mesnard, and Yoshua Bengio. Ex-
tending the framework of equilibrium propagation to general dynamics, 2018. URL https:
//openreview.net/forum?id=SJTB5GZCb.
Gavin Taylor, Ryan Burmeister, Zheng Xu, Bharat Singh, Ankit Patel, and Tom Goldstein. Training
neural networks without gradients: A scalable admm approach. In International Conference on
Machine Learning,pp. 2722-2731, 2016.
Shuangfei Zhai, Yu Cheng, Rogerio Feris, and Zhongfei Zhang. Generative adversarial networks as
variational training of energy based models. arXiv preprint arXiv:1611.01799, 2016.
11
Published as a conference paper at ICLR 2019
A	Glossary
Here we we have a reference of symbols used in the paper, in (Greek, Latin) alphabetical order.
αi ⊂ {j : j ∈ S, j 6= i}: The set of neurons in the Equilibrating Network that connect to neuron i
αif = {j : j ∈ αi, j < i}: The set of neurons in the Feedforward Network connected to neuron i.
β ∈ R: The perturbation factor, which modulates how much the augmented energy Eθβ is affected
by the output loss.
η ∈ R+: The learning rate.
θ: The set of parameters (all wij ’s and bj ’s, in the Equilibrating network)
ρ: a neuron nonlinearity. In all experiments it is ρ(x) = max(0, min(1, x))
φ = (ω, c): The set of parameters (all ωj 's and Cj 's), in the feedforward network
φj: The set of parameters (all ωj's and Cj ')belonging to a neuron j
(ω, c): The (weights, biases) of the feedforward network. Collectively called φ
C(sO, y) ∈ R: The output loss function, defined on the states of the output units.
Eθ(s, x) ∈ R: The energy function of the Equilibrating network (Equation 1) produces a scalar
energy given a set of states s and input x
Ee(s,x,y) = Eθ(s,x) + β"0'∂O"y ∈ R: The augmented energy function of the Equilibrating
network, when it has been perturbed by a factor β by target data y
fφ(x) 7→ sf: The initialization function: A feedforward network which initializes the state of the
Equilibrating network.
I: The set of indices of input neurons.
L	: The total loss in the Feedforward network's prediction. Defined in Equation 8
Li	: The local loss on the i0th neuron in the feedforward network. Defined in Equation 8
O: The set of indices output neurons, a subset of S
S : The set of inidices non-input neurons.
s: The set of neuron states. s := {si : i ∈ S} ∈ Rdim(S)
s- := argminsE(s, x) ∈ Rdim(S): The minimizing state of the Energy function.
s+ := argminsEθβ(s, x, y) ∈ Rdim(S) The minimizing state of the augmented energy function.
sf := fφ(x)z ∈ Rdim(S): The state output by the feedforward network.
sO ∈ Rdim(O): the states of the output units
T-, T+: Hyperparameters for Equilibrium Prop defining the number of steps of convergence of the
negative/positive phase.
w, b: the parameters of the Equilibrating network (collectively called θ)
x ∈ RdimI: The input data
y ∈ RdimO: The target data
12
Published as a conference paper at ICLR 2019
B Gradient Alignment
Here to derive the result in Equation 12. First, We restate Equation 11 substituting w* = w + ∆w:
S1 =	p(xwι)	s* =	p(x(wι	—	∆wι))	L1	=	∣∣S1	—	S* ^∣
S2 =	P(SIW2)	S；	=	ρ(s* (w2	—	∆w2))	£2	=	∣∣S2	—	s2∣∣
Where:
X ∈ RN×D0; s1,s* ∈ RN×D1; S2,S* ∈ RN×D2; WL ∆wι ∈ RDO×D1; w2, ∆w2 ∈ RD1×D2
NoW We Will compute the gradient of each of the local losses With respect to ∆w1, in the limit Where
∆w1 is small.
∂L1	∂ £1 ∂S1
---=
∂wι ∂sι ∂wι
=(((si — s*) Θ PZ(XW))T ∙ x)
__________________g(∆wι)_______________
=Xt ((P(XW1) — P(X(WI — ∆wι)))ΘPZ(XWI))
hm=1-0 g(0) + ∆wι A (0) (1st order Taylor Expansion about ∆wι = 0)
=XT (p(x(wi — 0)) — P(XWI))PZ(XW1)) + XT (x∆wι Θ PZ(XWI)PZ(XW1))
=0 + xt (x∆W1 θ PZ(XW1)2)
∂L2	∂L2 ∂s2 ∂S1
-...=
∂W1	∂S2 ∂S1 ∂W1
=(((S2 — s2) Θ pz(s1 WI)WTPZ(XWI))Tx)
=xt ((s2 — s2) Θ PZ(SIWI)WTPZ(XWI))
_______________________________g(∆w1,∆w2)_______________________________
=Xt ((P(P(XW1)W2) — p(p(x(w1 — ∆W1))(W2 一 ∆w2 ))) Θ PZ(P(XWI)WI)WTPZ(XWI))
iim=1-0 g(0,0) + ∆w1(0,0) + ∆w2 fg (0,0) (1st order Taylor Expansion about ∆w1 = 0, ∆w2 = 0)
∂∆W1	∂∆W2
______________________G1________________________________________G2_________________
=0+xt (x∆w1 Θ p z(xw1)w2 Θ pZ(s1w2)2WT Θ pZ(XWI)) +xt (s1∆w2 Θ pZ(s1w2)2WT Θ pZ(XWI))
13
Published as a conference paper at ICLR 2019
C Gradient Alignment at Initialization
Why do we observe gradient alignment even at random initialization? Let us start with the same
2-layer network defined in Appendix B
∂L1	∂L1 ∂s1
——=
∂w1	∂s1 ∂w1
=(((si — si) Θ ρ0(xw))T ∙ x)
=XT ((P(Xwi) — p(xwl))Θp0(xWl)
GA
xτ ρ(xwι)Θρ0(xwι) — XT ρ(xwχ)Θρ0(xwι)
∂L2	∂L2 ∂s2 ∂s1
-...=
∂w1 ∂s2 ∂s1 ∂w1
=(((s2 — s2) Θ ρ0(sιw2)wTρ0(xwι))Tx)
= XT (s2 — si2) Θ ρ0(s1w2)w2Tρ0(Xw1)
=xT((P(P(Xw1)w2) — P(P(Xwl)w2)) Θ p0(p(χw1)w2)wTp0(χwι))
____________________Gb____________________
= XT P(P(Xw1)w2) Θ P0(P(Xw1)w2)w2TP0(Xw1) —
XT P(P(Xw1i)w2i) Θ P0(P(Xw1)w2)w2TP0(Xw1)
GA and GB tend to be aligned because the terms P(Xw1) and P(P(Xw1)w2) Θ P0(P(Xw1)w2)w2T
tend to be aligned. Suppose P is a piecewise saturating nonlinearity (as we have in this paper) with
P(X) = [a if (X < a); X if (X ∈ [a, b]); b otherwise]
Then we can define a weight matrix w20 by filtering rows of w2 to only include weights projecting
to non-saturated neurons: w20 = [w2(i)∀i : P0(P(Xw1)w2(i)) 6= 0] Where w2(i) denotes the i0th row of
w2.
Then our second term can be rewritten as: P(P(Xw1)w2) Θ P0(P(Xw1)w2)w2T = P(Xw1)w20 w20T .
Given a random matrix w20 , the matrix w20 w20T will tend to have a strong diagonal component, causing
this term to be aligned with P(Xw1)
14
Published as a conference paper at ICLR 2019
D EFFECT OF λ PARAMETER
In Equation 10 we introduce a new parameter λ which encourages the state of the equilibrating
network to state close to that of the forward pass. Here we perform a sweep of parameter λ to
evaluate its effect.
small: T- = 4	small: T~ = 10
---- Init Eq. Prop: sf
Init Eq. Prop: s-
7
8 6 4
Stt81
12	3	4
8 6 4 2 0
9J0。S as©1-
8 6 4
Stt81
large: T~ = 50
Figure 4: Here we scan the λ parameter and plot the final score at the end of training. Each point
in each plot corresponds to the final score of a network with parameter λ fixed at the given value
throughout training. The top row of plots is a for a small network with one hidden layer of 500
hidden units. The bottom is for a large network with 3 layers of [500, 500, 500] hidden units. Each
column is for a different number of steps of negative-phase convergence.
We see in Figure 4 that when the number of steps of negative-phase convergence is small, introducing
λ can allow for more stable training. This makes sense - if the minimizing state of the equilibating
network is “pulled” towards the state at the forward pass, it should take fewer steps of iteration to
reach this state when initialized at the state of the forward pass. However, we also see that training
fails when λ is too high. We believe this is because the simple iterative settling scheme (Euler
integration) used in this paper, as well as the original Equilibrium Prop by Scellier & Bengio (2017),
can become unstable when optimizing a loss surface with sharp, steep, minima (as are induced with
large λ). This could be addressed in later work by either using an adaptive λ term or an adaptive
Euler-integration step-size.
15