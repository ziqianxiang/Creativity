Published as a conference paper at ICLR 2019
Subgradient Descent Learns Orthogonal Dic-
TIONARIES
Yu Bai, Qijia Jiang & Ju Sun
Stanford University
{yub,qjiang2,sunju}@stanford.edu
Ab stract
This paper concerns dictionary learning, i.e., sparse coding, a fundamental repre-
sentation learning problem. We show that a subgradient descent algorithm, with
random initialization, can recover orthogonal dictionaries on a natural nonsmooth,
nonconvex `1 minimization formulation of the problem, under mild statistical
assumption on the data. This is in contrast to previous provable methods that re-
quire either expensive computation or delicate initialization schemes. Our analysis
develops several tools for characterizing landscapes of nonsmooth functions, which
might be of independent interest for provable training of deep networks with nons-
mooth activations (e.g., ReLU), among other applications. Preliminary synthetic
and real experiments corroborate our analysis and show that our algorithm works
well empirically in recovering orthogonal dictionaries.
1 Introduction
Dictionary learning (DL), i.e. , sparse coding, concerns the problem of learning compact represen-
tations, i.e., given data Y , one tries to find a representation basis A and coefficients X, so that
Y ≈ AX where X is most sparse. DL has numerous applications especially in image processing
and computer vision (Mairal et al., 2014). When posed in analytical form, DL seeks a transformation
Q such that QY is sparse; in this sense DL can be considered as an (extremely!) primitive “deep”
network (Ravishankar & Bresler, 2013).
Many heuristic algorithms have been proposed to solve DL since the seminal work of Olshausen &
Field (1996), most of them surprisingly effective in practice (Mairal et al., 2014; Sun et al., 2015).
However, understandings on when and how DL is solvable have only recently started to emerge. Under
appropriate generating models on A and X, Spielman et al. (2012) showed that complete (i.e., square,
invertible) A can be recovered from Y , provided that X is ultra-sparse. Subsequent works (Agarwal
et al., 2017; Arora et al., 2014; 2015; Chatterji & Bartlett, 2017; Awasthi & Vijayaraghavan, 2018)
provided similar guarantees for overcomplete (i.e. fat) A, again in the ultra-sparse regime. The
latter methods are invariably based on nonconvex optimization with model-dependent initialization,
rendering their practicality on real data questionable.
The ensuing developments have focused on breaking the sparsity barrier and addressing the prac-
ticality issue. Convex relaxations based on the sum-of-squares (SOS) SDP hierarchy can recover
overcomplete A when X has linear sparsity (Barak et al., 2015; Ma et al., 2016; Schramm &
Steurer, 2017), while incurring expensive computation (solving large-scale SDP’s or large-scale
tensor decomposition). By contrast, Sun et al. (2015) showed that complete A can be recovered
in the linear sparsity regime by solving a certain nonconvex problem with arbitrary initialization.
However, the second-order optimization method proposed there is still expensive. This problem is
partially addressed by (Gilboa et al., 2018) which proved that the first-order gradient descent with
random initialization enjoys a similar performance guarantee.
A standing barrier toward practicality is dealing with nonsmooth functions. To promote sparsity
in the coefficients, the `1 norm is the function of choice in practical DL, as is common in modern
signal processing and machine learning (Candes, 2014): despite its nonsmoothness, this choice often
admits highly scalable numerical methods, such as proximal gradient method and alternating direction
The reader is welcome to refer to our arXiv version for future updates.
1
Published as a conference paper at ICLR 2019
method (Mairal et al., 2014). The analyses in Sun et al. (2015); Gilboa et al. (2018), however, focused
on characterizing the algorithm-independent function landscape of a certain nonconvex formulation
of DL, which takes a smooth surrogate to `1 to get around the nonsmoothness. The tactic smoothing
there introduced substantial analysis difficulty, and broke the practical advantage of computing with
the simple `1 function.
In this paper, we show that working directly with a natural `1 norm formulation results in neat analysis
and a practical algorithm. We focus on the problem of learning orthogonal dictionaries: given data
{yi }i∈[m] generated as yi = Axi , where A ∈ Rn×n is a fixed unknown orthogonal matrix and each
xi ∈ Rn is an iid Bernoulli-Gaussian random vector with parameter θ ∈ (0, 1), recover A. This
statistical model is the same as in previous works (Spielman et al., 2012; Sun et al., 2015).
Write Y =. [y1, . . . , ym] and similarly X =. [x1, . . . , xm]. We propose to recover A by solving the
following nonconvex (due to the constraint), nonsmooth (due to the objective) optimization problem:
1	1m
minimizeq∈Rn Aq) = m ∣∣q Y∣∣1 = m £|q yi| subject to kqk2 = 1.	(1.1)
m	m i=1
Based on the statistical model, q>Y = q>AX has the highest sparsity when q is a column of A (up
to sign) so that q>A is 1-sparse. SPielman et al. (2012) formalized this intuition and optimized the
same objective as Eq. (1.1) with a ∣∣q∣∣∞ = 1 constraint, which only works when θ 〜O(1∕√n). Sun
et al. (2015) worked with the sphere constraint but replaced the `1 objective with a smooth surrogate,
introducing substantial analytical and computational deficiencies as alluded above.
In constrast, we show that with sufficiently many samples, the optimization landscape of formula-
tion (1.1) is benign with high probability (over the randomness of X), and a simple Riemannian
subgradient descent algorithm can provably recover A in polynomial time.
Theorem 1.1 (Main result, informal version of Theorem 3.1). Assume θ ∈ [1/n, 1/2]. For
m ≥ Ω(θ-2n4 log4 n), the following holds with high probability: there exists a poly(m,e-1)-
time algorithm, which runs Riemannian subgradient descent on formulation (1.1) from at most
O(n log n) independent, uniformly random initial points, and outputs a set of vectors {ab1, . . . , abn}
such that up to permutation and sign change, kabi - ai k2 ≤ e for all i ∈ [n].
In words, our algorithm works also in the linear sparsity regime, the same as established in Sun
et al. (2015); Gilboa et al. (2018), at a lower sample complexity O(n4) in contrast to the existing
O(n5.5) in Sun et al. (2015). 1 As for the landscape, we show that (Theorems 3.4 and 3.6) each of
the desired solutions {±ai}i∈[n] is a local minimizer of formulation (1.1) with a sufficiently large
basin of attraction so that a random initialization will land into one of the basins with at least constant
probability. To obtain the result, we integrate and develop elements from nonsmooth analysis (on
Riemannian manifolds), set-valued analysis, and random set theory, which might be valuable to
studying other nonconvex, nonsmooth optimization problems.
1.1	Related work
Dictionary learning Besides the many results sampled above, we highlight similarities of our result
to Gilboa et al. (2018). Both propose first-order optimization methods with random initialization,
and several quantities we work with in the proofs are the same. A defining difference is we work
with the nonsmooth `1 objective directly, while Gilboa et al. (2018) built on the smoothed objective
from Sun et al. (2015). We put considerable emphasis on practicality: the subgradient of the
nonsmooth objective is considerably cheaper to evaluate than that of the smooth objective in Sun
et al. (2015), and in the algorithm we use Euclidean projection rather than exponential mapping to
remain feasible—again, the former is much lighter for computation.
General nonsmooth analysis While nonsmooth analytic tools such as subdifferential for convex
functions are now well received in machine learning and relevant communities, that for general
functions are much less so. The Clarke subdifferential and relevant calculus developed for the family
of locally Lipschitz functions seem to be particularly relevant, and cover several families of functions
of interest, such as convex functions, differentiable functions, and many forms of composition
1The sample complexity in Gilboa et al. (2018) is not explicitly stated.
2
Published as a conference paper at ICLR 2019
(Clarke, 1990; Aubin, 1998; Bagirov et al., 2014). Remarkably, majority of the tools and results
can be generalized to locally Lipschitz functions on Riemannnian manifolds (Ledyaev & Zhu, 2007;
Hosseini & Pouryayevali, 2011). Our formulation (1.1) is exactly optimization of a locally Lipschitz
function (as it is convex) on a Riemannian manifold (the sphere). For simplicity, we try to avoid the
full manifold language, nonetheless.
Nonsmooth optimization on Riemannian manifolds or with constraints Equally remarkable is
many of the smooth optimization techniques and convergence results can be naturally adapted to opti-
mization of locally Lipschitz functions on Riemannian manifolds (Grohs & Hosseini, 2015; Hosseini,
2015; Hosseini & Uschmajew, 2017; Grohs & Hosseini, 2016). New optimization methods such
as gradient sampling and variants have been invented to solve general nonsmooth problems (Burke
et al., 2005; 2018; Bagirov et al., 2014; Curtis & Que, 2015; Curtis et al., 2017). Almost all available
convergence results pertain to only global convergence, which is too weak for our purpose. Our
specific convergence analysis gives us a local convergence result (Theorem 3.8).
Nonsmooth landscape characterization Nonsmoothness is not a big optimization barrier if the
problem is convex; here we review some recent work on analyzing nonconvex nonsmooth problems.
Loh & Wainwright (2015) study the regularized empirical risk minimization problem with nonsmooth
regularizers and show results of the type “all stationary points are within statistical error of ground
truth” under certain restricted strong convexity of the smooth risk. Duchi & Ruan (2017); Davis
et al. (2017) study the phase retrieval problem with `1 loss, characterizing its nonconvex nonsmooth
landscape and providing efficient algorithms.
There is a recent surge of work on analyzing one-hidden-layer ReLU networks, which are nonconvex
and nonsmooth. Algorithm-independent characterizations of the landscape are mostly local and
require strong initialization procedures (Zhong et al., 2017), whereas stronger global results can be
established via designing new loss functions (Ge et al., 2017), relating to PDEs (Mei et al., 2018), or
problem-dependent analysis of the SGD (Li & Yuan, 2017; Li & Liang, 2018). Our result provides
an algorithm-independent chacaterization of the landscape of non-smooth dictionary learning, and is
“almost global” in the sense that the initialization condition is satisifed by random initialization with
high probability.
Other nonsmooth problems in application Prevalence of nonsmooth problems in optimal control
and economics is evident from all monographs on nonsmooth analysis (Clarke, 1990; Aubin, 1998;
Bagirov et al., 2014). In modern machine learning and data analysis, nonsmooth functions are often
taken to encode structural information (e.g., sparsity, low-rankness, quantization), or whenever robust
estimation is desired. In deep learning, the optimization problem is nonsmooth when nonsmooth
activations are in use, e.g., the popular ReLU. The technical ideas around nonsmooth analysis,
set-valued analysis, and random set theory that we gather and develop here are particularly relevant
to these applications.
2	Preliminaries
Problem setup Given an unknown orthogonal dictionary A = [a1, . . . , an] ∈ Rn×n, we wish to
recover A through m observations of the form
yi = Axi,	(2.1)
or Y = AX in matrix form, where X = [x1, . . . , xm] and Y = [y1, . . . , ym].
The coefficient vectors xi are sampled from the Bernoulli-Gaussian distribution with parameter
θ ∈ (0, 1), denoted as BG(θ): each entry xij is independently drawn from a standard Gaussian with
probability θ and zero otherwise. The Bernoulli-Gaussian is a good prototype distribution for sparse
vectors, as Xi will be on average θ-sparse. For any Z 〜萩4 Ber(θ), We let Ω denote the set of non-zero
indices, which is a random set itself.
We assume that n ≥ 3 and θ ∈ [1/n, 1/2]. In particular, θ ≥ 1/n is to require that each xi has at
least one non-zero entry on average.
First-order geometry We will focus on the first-order geometry of the non-smooth objec-
tive Eq. (1.1): f (q) = ml Pm=∕q>yi∣. In the whole Euclidean space Rn, f is convex with
3
Published as a conference paper at ICLR 2019
sub-differential set
1m
∂f(q) = m Esign(q>yi)yi,
(2.2)
i=1
where sign(∙) is the set-valued sign function (i.e. sign(0) = [-1,1]). As we minimize f subject to
the constraint kqk2 = 1, our problem is no longer convex. The Riemannian sub-differential of f on
Sn-1 is defined as (Hosseini & Uschmajew, 2017):
∂Rf (q) =. (I - qq>)∂f(q).
(2.3)
A point q is stationary for problem Eq. (1.1) if 0 ∈ ∂Rf (q). We will not distinguish between local
maxima and saddle points—we call a stationary point q a saddle point if there is a descent direction
(i.e. direction along which the function is locally maximized at q).
Set-valued analysis As the subdifferential is a set-valued mapping, analyzing it requires some set-
valued analysis, which we briefly present here. The addition of two sets is defined as the Minkowski
summation: X + Y = {x + y : x ∈ X, y ∈ Y }. The expectation of random sets is a straightforward
extension of the Minkowski sum allowing any measurable “selection” procedure; for the concrete
definition see (Molchanov, 2013). The Hausdorff distance between two sets is defined as
dH (X1, X2) =. sup sup d (x1, X2) , sup d (x2, X1) .
x1∈X1
x2∈X2
(2.4)
Basic properties about the Hausdorff distance are provided in Appendix A.1.
Notations Bold small letters (e.g., x) are vectors and bold capitals are matrices (e.g., X). The dotted
equality = is for definition. For any positive integer k, [k] = {1,..., k}. By default, ∣∣∙k is the '2
norm if applied to a vector, and the operator norm if applied to a matrix. C and c or any indexed
versions are reserved for universal constants that may change from place to place.
3	Main Result
We now state our main result, the recovery guarantee for learning orthogonal dictionary by solving
formulation (1.1).
Theorem 3.1 (Recovering orthogonal dictionary via subgradient descent). Suppose we observe
m ≥ Cn1 2 3 4 *θ-2 log4 n	(3.1)
samples in the dictionary learning problem and we desire an accuracy ∈ (0, 1) for recovering the
dictionary. With probability at least 1 - exp -cmθ3n-3 log-3 m - exp (-c0R/n), an algorithm
which runs Riemannian subgradient descent R = C0n log n times with independent random initial-
izations on Sn-1 outputs a set of vectors {ab1, . . . , abn} such that up to permutation and sign change,
kabi - aik2 ≤ for all i ∈ [n]. The total number of subgradient descent iterations is bounded by
C〃R0T6/3e-8/3n4 log8/3 n.	(3.2)
Here C, C0, C00 , c, c0 > 0 are universal constants.
At a high level, the proof of Theorem 3.1 consists of the following steps, which we elaborate
throughout the rest of this section.
1. Partition the sphere into 2n symmetric “good sets” and show certain directional gradient is
strong on population objective E[f] inside the good sets (Section 3.1).
2. Show that the same geometric properties carry over to the empirical objective f with high
probability. This involves proving the uniform convergence of the subdifferential set ∂f to
E [∂f] (Section 3.2).
3. Under the benign geometry, establish the convergence of Riemannian subgradient descent
to one of {±ai : i ∈ [n]} when initialized in the corresponding “good set” (Section 3.3).
4. Calling the randomly initialized optimization procedure O(nlogn) times will recover all of
{a1, . . . , an} with high probability, by a coupon collector’s argument (Section 3.4).
4
Published as a conference paper at ICLR 2019
Scaling and rotating to identity Throughout the rest of this paper, we are going to assume WLOG
that the dictionary is the identity matrix, i.e. A = In, sothat Y = X, f(q) = q>X1, and the
goal is to find the standard basis vectors {±e1, . . . , ±en}. The case of a general orthogonal A can
be reduced to this special case via rotating by A> : q>Y = q>AX = (q0)>X where q0 = A>q
and applying the result on q0. We also scale the objective by，n/2 for convenience of later analysis.
3.1	Properties of the population objective
We begin by characterizing the geometry of the expected objective E [f]. Recall that we have rotated
A to be identity, so that we have
f⑷=/I∙；1||q>X|li = /I∙ɪX∣q>xi∣, df(q) = ∏∙ɪXsign(q>x) ◎. (3.3)
mm	m
i=1	i=1
Minimizers and saddles of the population objective We begin by computing the function value
and subdifferential set of the population objective and giving a complete characterization of its
stationary points, i.e. local minimizers and saddles.
Proposition 3.2 (Population objective value and gradient). We have
E [f](q) = ^n ∙ E [∣q>x∣] = Eokqok	(3.4)
d E [f ](q)=E [df ](q)=r ∙ E[sign (q>x) x]=eω 匿弟；k≤ i}, ::=0,35)
Proposition 3.3 (Stationary points). The stationary points of E [f] on the sphere are
S = {√kq ： q ∈ {-i,0, i}n, k:ko = k k ∈ [n]} .	(3.6)
The case k = i corresponds to the 2n global minimizers q = ±ei, and all other values of k
correspond to saddle points.
A consequence of Proposition 3.3 is that the population objective has no “spurious local minima”:
each stationary point is either a global minimizer or a saddle point, though the problem itself is
non-convex due to the constraint.
Identifying 2n “good” subsets We now define 2n subsets on the sphere, each containing one of the
global minimizers {±ei } and possessing benign geometry for both the population and empirical
objective, following (Gilboa et al., 2018). For any ζ ∈ [0, ∞) and i ∈ [n] define
S(i+) = Iq : qi > 0, q2 2 ≥ 1 + ZI S(i-) = Iq : % < 0, q2 2 ≥ 1 + Zl (3.7)
Z I	kq-ik∞ - ʃ ζ I	kq-ik∞ -
For points in Sζ(i+) ∪ Sζ(i-), the i-th index is larger than all other indices (in absolute value) by a
multiplicative factor of Z. In particular, for any point in these subsets, the largest index is unique, so
by Proposition 3.3 all population saddle points are excluded from these 2n subsets.
(i+)
Intuitively, this partition can serve as a “tiebreaker”: points in Sζ is closer to ei than all the other
2n - i signed basis vectors. Therefore, we hope that optimization algorithms initialized in this region
could favor ei over the other standard basis vectors, which we are going to show is indeed the case.
For simplicity, we are going to state our geometry results in Sζ(n+); by symmetry the results will
automatically carry over to all the other 2n - i subsets.
Theorem 3.4 (Lower bound on directional subgradients). Fix any Z0 ∈ (0, i). We have
(a)	For all q ∈ Sζ(n+) and all indices j 6= n such that qj 6= 0,
inf (e [dRf](q) , 一ej-----en) ≥ —θ (1 - θ) ɪ .	(3.8)
qj	qn	2n	i + Z0
5
Published as a conference paper at ICLR 2019
(b)	For all q ∈ Sζ(n+), we have that
inf hE [∂r∕] (q), qnq - eQ ≥ 1 θ (1 - θ) Zon-3/2 kq-nk .	(3.9)
8
These lower bounds verify our intuition: points inside Sζ(n+) have subgradients pointing towards en ,
both in a coordinate-wise sense and a combined sense: the direction en - qn q is exactly the tangent
direction of the sphere at q that points towards en .
3.2	Benign geometry of the empirical objective
We now show that the benign geometry in Theorem 3.4 is carried onto the empirical objective f given
sufficiently many samples, using a concentration argument. The key result behind is the concentration
of the empirical subdifferential set to the population subdifferential, where concentration is measured
in the Hausdorff distance between sets.
Proposition 3.5 (Uniform convergence of subdifferential). For any t ∈ (0, 1], when
m ≥ Ct-2n log2 (n/t),	(3.10)
with probability at least 1 一 exp (—cmθt2∕log m), we have
dH(∂f(q),E[∂f](q)) ≤t for all q ∈ Sn-1.	(3.11)
Here C, c ≥ 0 are universal constants.
The concentration result guarantees that the sub-differential set is close to its expectation given
sufficiently many samples with high probability. Choosing an appropriate concentration level t, the
lower bounds on the directional subgradients carry over to the empirical objective f, which we state
in the following theorem.
Theorem 3.6 (Directional subgradient lower bound, empirical objective). There exist universal con-
stants C, c ≥ 0 so that the following holds: for all Zo ∈ (0,1), when m ≥ Cn4θ-2Z-2 log2 (n/Zo),
with probability at least 1 - exp -cmθ3ζ02n-3 log-1 m , the following properties hold simultane-
ously for all the 2n subsets
[n] : (stated only for Sζ(n+))
(a)	For all q ∈ Sζ(n+) and allj ∈ [n] with qj 6= 0 and qn2 /qj2 ≤ 3,
MdRf ⑷,1 ej 一 :e) ≥ 4nθ (1 一 θ) 1⅛
(3.12)
(b)	For all q ∈ Sζ(n+),
infhdRf (q),qnq - eni ≥ 12θ(1 - θ)n-3Zo kq-nk ≥ 16θ(1 - θ)n-3Zo kq 一 enk.
(3.13)
The consequence of Theorem 3.6 is two-fold. First, it guarantees that the only possible stationary
point of f in Sζ(n+) is en: for every other point q 6= en, property (b) guarantees that 0 ∈/ ∂Rf (q),
therefore q is non-stationary. Second, the directional subgradient lower bounds allow us to estab-
lish convergence of the Riemannian subgradient descent algorithm, in a way similar to showing
convergence of unconstrained gradient descent on star strongly convex functions.
We now present an upper bound on the norm of the subdifferential sets, which is needed for the
convergence analysis.
Proposition 3.7. There exist universal constants C, c ≥ 0 such that
sup k∂f (q)k ≤2 ∀q∈Sn-1	(3.14)
with probability at least 1 — exp (—cmθ log-1 m), provided that m ≥ Cn log n. This particularly
implies that
supk∂Rf(q)k ≤2 ∀q∈Sn-1.	(3.15)
6
Published as a conference paper at ICLR 2019
3.3	Finding one basis via Riemannian subgradient descent
The benign geometry of the empirical objective allows a simple Riemannian subgradient descent
algorithm to find one basis vector a time. The Riemannian subgradient descent algorithm with
initialization q(0) and step size η(k) k≥0 is as follows. For an arbitrary v ∈ ∂Rf q(k) ,
q(k+1)
q(k) — η(k)v
∣∣q(k) — η(k)v∣∣，
for k = 0, 1, 2, ....
(3.16)
Each iteration moves in an arbitrary Riemannian subgradient direction followed by a projection back
onto the sphere. We show that the algorithm is guaranteed to find one basis as long as the initialization
is in the “right” region. To give a concrete result, we set ζ0 = 1/(5 log n).2
Theorem 3.8 (One run of subgradient descent recovers one basis). Let m ≥ Cθ-2n4 log4 n and
e ∈ (0, 2θ∕25]. With probability at least 1 — exp (—cmθ3n-3 log-3 m) the following happens. If
the initialization q(0) ∈ S1(n/(+5)logn), and we run the projected Riemannian subgradient descent with
step size η(k) = k-α/(100√n) with α ∈ (0,1/2), and keep trackof the best function value so far
until after iterate K is performed, producing qbest. Then, qbest obeys
16
f (qb t) — f (en) ≤ e, and	∣∣qb	一 ej∣ ≤ ., -。)e,	(3.17)
provided that
32000n5/2 logn (1 — α) 丫八1-。)(641⅛n3/2 logn) 1/a
θ (1 — θ) e )	, (5θ(1 — θ) e
K ≥ max
(3.18)
In particular, choosing α = 3/8 < 1/2, it suffices to let
K ≥ K3/8 = C00-8/3e-8/3n4 log8/3 n.	(3.19)
Here C, C0 , c ≥ 0 are universal constants.
The above optimization result (Theorem 3.8) shows that Riemannian subgradient descent is able to
find the basis vector en when initialized in the associated region S1(/n(+5)log n). We now show that a
simple uniformly random initialization on the sphere is guaranteed to be in one of these 2n regions
with at least probability 1/2.
Lemma 3.9 (Random initialization falls in “good set"). Let q(0)〜Uniform(SnT), then with
probability at least 1/2, q(0) belongs to one of the 2n sets S1(i/+(5)logn), S1(i/-(5) log n) : i ∈ [n] .
3.4	Recovering all bases from multiple runs
As long as the initialization belongs to S1(i/+(5)logn) or S1(i/-(5)logn), our finding-one-basis result in The-
orem 3.8 guarantees that Riemannian subgradient descent will converge to ei or —ei respectively.
Therefore if we run the algorithm with independent, uniformly random initializations on the sphere
multiple times, by a coupon collector’s argument, we will recover all the basis vectors. This is
formalized in the following theorem.
Theorem 3.10 (Recovering the identity dictionary from multiple random initializations). Let m ≥
Cn4θ-2 log4 n and e ∈ (0,1), with probability at least 1 — exp (—cmθ3n-3 log-3 m) thefoUowing
happens. Suppose we run the Riemannian subgradient descent algorithm independently for R
times, each with a uniformly random initialization on Sn-1, and choose the step size as η(k) =
k-3∕8∕(100√n). Then, provided that R ≥ C0n log n, all standard basis vectors will be recovered up
to e accuracy with probability at least 1 — exp (—cR∕n) in C0Rθ-16/3e-8/3n4 log8/3 n iterations.
Here C, C0 , c ≥ 0 are universal constants.
When the dictionary A is not the identity matrix, we can apply the rotation argument sketched in the
beginning of this section to get the same result, which leads to our main result in Theorem 3.1.
2It is possible to set ζ0 to other values, inducing different combinations of the final sample complexity,
iteration complexity, and repetition complexity in Theorem 3.10.
7
Published as a conference paper at ICLR 2019
4	Proof highlights
A key technical challenge is establishing the uniform convergence of subdifferential sets in Proposi-
tion 3.5, which we now elaborate. Recall that the population and empirical subdifferentials are
∂f (q) = yπ2 ∙ m1 X sign(q>xi)xi, E [∂f] (q) =点 E〜BG(θ) sign(q>x)x ,	(4.1)
i=1
and we wish to show that the difference between ∂f(q) and E [∂f] (q) is small uniformly over
q ∈ Q = Sn-1. Two challenges stand out in showing such a uniform convergence:
1.	The subdifferential is set-valued and random, and it is unclear a-priori how one could
formulate and analyze the concentration of random sets.
2.	The usual covering argument won’t work here, as the Lipschitz gradient property does not
hold: ∂f (q) and E [∂f] (q) are not Lipschitz in q. Therefore, no matter how fine we cover
the sphere in Euclidean distance, points not in this covering can have radically different
subdifferential sets.
4.1	Concentration of random sets
We state and analyze concentration of random sets in the Hausdorff distance (defined in Section 2).
We now illustrate how the Hausdorff distance is the “right” distance to consider for concentration of
subdifferentials—the reason is that the Hausdorff distance is closely related to the support function of
sets, which for any set S ∈ Rn is defined as
hS (u) =. sup hx, ui .	(4.2)
x∈S
For convex compact sets, the sup difference between their support functions is exactly the Hausdorff
distance.
Lemma 4.1 (Section 1.3.2, Molchanov (2013)). For convex compact sets X, Y ⊂ Rn, we have
dH (X,Y) = sup |hX (u) - hY (u)| .	(4.3)
u∈Sn-1
Lemma 4.1 is convenient for us in the following sense. Suppose we wish to upper bound the difference
of ∂f(q) and E [∂f] (q) along some direction u ∈ Sn-1 (as we need in proving the key empirical
geometry result Theorem 3.6). As both subdifferential sets are convex and compact, by Lemma 4.1
we immediately have
inf hg, ui - inf hg,ui = -h∂f(q) (-u) + hE[∂f](q) (-u) ≤ dH(∂f(q),E [∂f] (q)).
g∈∂f (q)	g∈E[∂f](q)
(4.4)
Therefore, as long as we are able to bound the Hausdorff distance, all directional differences between
the subdifferentials are simultaneously bounded, which is exactly what we want to show to carry the
benign geometry from the population to the empirical objective.
4.2	COVERING IN THE dE METRIC
We argue that the absence of gradient Lipschitzness is because the Euclidean distance is not the
“right” metric in this problem. Think of the toy example f(x) = |x|, whose subdifferential set
∂f(x) = sign(x) is not Lipschitz across x = 0. However, once we partition R into R>0, R<0 and
{0} (i.e. according to the sign pattern), the subdifferential set is Lipschitz on each subset.
The situation with the dictionary learning objective is quie similar: we resolve the gradient non-
Lipschitzness by proposing a stronger metric dE on the sphere which is sign-pattern aware and
averages all “subset angles” between two points. Formally, we define dE as
dE(P, q) = Px〜bg(θ) [sign(p>x) = sign(q>x)] = 1 E。/ (pω, q。),
(4.5)
8
Published as a conference paper at ICLR 2019
(the second equality shown in Lemma C.1.) Our plan is to perform the covering argument in dE,
which requires showing gradient Lipschitzness in dE and bounding the covering number.
Lipschitzness of ∂f and E [∂f] in dE For the population subdifferential E [∂f], note
that E [∂f](q) = Ex〜bg(6)[sign(q>x)x] (modulo rescaling). Therefore, to bound
dH(E [∂f] (p), E [∂f] (q)) by Lemma 4.1, we have the bound for all u ∈ Sn-1
∣hE[∂f](p)(u) - hE[∂f](q)(u)∣ = E [sup ∣sign(p›x) - sign(q>x)∣ ∙ ∣x>u∣]	(4.6)
≤ 2E [1 {sign(p>x) = sign(q>x)} ∣x>u∣] .	(4.7)
As long as dE(p, q) ≤ , the indicator is non-zero with probability at most , and thus the above
expectation should also be small - We bound it by O(e,log(1∕κ)) in Lemma F.5.
To show the same for the empirical subdifferential ∂f, one only needs to bound the observed
proportion of sign differences for all p, q such that dE(p, q) ≤ e, Which by a VC dimension argument
is uniformly bounded by 2e With high probability (Lemma C.5).
Bounding the covering number in dE Our first step is to reduce dE to the maximum length-2
angle (the d2 metric) over any consistent support pattern. This is achieved through the folloWing
vector angle inequality (Lemma C.2): for any p, q ∈ Rd (d ≥ 3), We have
/ (p, q) ≤ X L (pω, qΩ) provided / (p, q) ≤ π∕2.	(4.8)
Ω⊂[d],∣Ω∣=2
Therefore, as long as sign(p) = sign(q) (coordinate-wise) and max∣Ω∣=2 /(pω, Qω) ≤ e∕n2, we
would have for all ∣Ω∣≥ 3 that
L (Pω, qΩ) ≤ ∏∕2 and L (pω, Qω) ≤ X L (p。，, q。，) ≤ ( ； ) ∙ W ≤ e. (4.9)
Ω0⊂Ω,∣Ω0∣=2	、 n
By Eq. (4.5), the above implies that dE(p, q) ≤ e∕π, the desired result. Hence the task reduces to
constructing an η = e∕n2 covering in d2 over any consistent sign pattern.
Our second step is a tight bound on this covering number: the η-covering number in d2 is bounded
by exp(Cn log(n∕η)) (Lemma C.3). For bounding this, a first thought would be to take the covering
in all size-2 angles (there are n2 of them) and take the common refinement of all their partitions,
which gives covering number (C∕η)O(n2) = exp(Cn2 log(1∕η)). We improve upon this strategy by
sorting the coordinates in p and restricting attentions in the consecutive size-2 angles after the sorting
(there are n - 1 of them). We show that a proper covering in these consecutive size-2 angles by η∕n
will yield a covering for all size-2 angles by η . The corresponding covering number in this case is
thus (Cn∕η)O(n) = exp(Cn log(n∕η)), which modulo the log n factor is the tightest we can get.
5	Experiments
Setup We set the true dictionary A to be the identity and random orthogonal matrices, respectively.
For each choice, we sweep the combinations of (m, n) with n ∈ {30, 50, 70, 100} and m =
10n{0.5,1,1.5,2,2.5}, and fix the sparsity level at θ = 0.1, 0.3, 0.5, respectively. For each (m, n) pair,
we generate 10 problem instances, corresponding to re-sampling the coefficient matrix X for 10
times. Note that our theoretical guarantee applies for m = Ω(n4), and the sample complexity we
experiment with here is lower than what our theory requires. To recover the dictionary, we run
the Riemannian subgradient descent algorithm Eq. (3.16) with decaying step size η(k) = 1∕√k,
corresponding to the boundary case α = 1∕2 in Theorem 3.8 with a much better base size.
Metric As Theorem 3.1 guarantees recovering the entire dictionary with R ≥ Cnlogn independent
runs, we perform R = round (5n log n) runs on each instance. For each run, a true dictionary element
ai is considered to be found if kai - qbest k ≤ 10-3. For each instance, we regard it a successful
recovery if the R = round (5n log n) runs have found all the dictionary elements, and we report the
empirical success rate over the 10 instances.
9
Published as a conference paper at ICLR 2019
Result From our simulations, Riemannian subgradient descent succeeds in recovering the dictionary
as long as m ≥ Cn2 (Fig. 2), across different sparsity level θ. The dependency on n is consistent
with our theory and suggests that the actual sample complexity requirement for guaranteed recovery
4	32
might be even lower than O(n4) we established.3 The O(n2) rate we observe also matches the results
based on the SOS method (Barak et al., 2015; Ma et al., 2016; Schramm & Steurer, 2017). Moreover,
the problem seems to become harder when θ grows, evident from the observation that the success
transition threshold being pushed to the right.
0.5
0.1
___ ( ( ( __________________________________________________________
number of measurements (as power of n)
.1. .1. .1. "
-→-n=30
n=50
n=70
n=100
2.5
1	1.5	2
number of measurements (as power of n)
.1- .1- -1- -1- ■ -I-
0.2
0.1
1.5
number of measurements (as power of n)
2.5
5
1
2
Figure 1: Empirical success rates of recovery of the Riemannian subgradient descent with R = 5n log n runs,
averaged over 10 instances. Left to right: identitiy dictionaries with θ = 0.1, 0.3, 0.5. See Appendix G for the
results on orthogonal dictionaries, which have qualitatively the same behaviors.
Additional experiments A faster alternative algorithm for large-scale instances is tested in Ap-
pendix H. A complementary experiment on real images is included as Appendix I.
6	Conclusion and Future Directions
This paper presents the first theoretical guarantee for orthogonal dictionary learning using subgradient
descent on a natural `1 minimization formulation. Along the way, we develop tools for analyzing the
optimization landscape of nonconvex nonsmooth functions, which could be of broader interest.
For futute work, there is an O(n2) sample complexity gap between what we established in The-
orem 3.1, and what we observed in the simulations alongside previous results based on the SOS
method (Barak et al., 2015; Ma et al., 2016; Schramm & Steurer, 2017). As our main geometric
result Theorem 3.6 already achieved tight bounds on the directional derivatives, further sample
complexity improvement could potentially come out of utilizing second-order information such as
the strong negative curvature (Lemma B.2), or careful algorithm-dependent analysis.
While our result applies only to (complete) orthogonal dictionaries, a natural question is whether we
can generalize to overcomplete dictionaries. To date the only known provable algorithms for learning
overcomplete dictionaries in the linear sparsity regime are based on the SOS method (Barak et al.,
2015; Ma et al., 2016; Schramm & Steurer, 2017). We believe that our nonsmooth analysis has the
potential of handling over-complete dictionaries, as for reasonably well-conditioned overcomplete
dictionaries A, each ai (columns of A) makes a>A approximately 1-sparse and so ai>AX gives
noisy estimate of a certain row of X. So the same formulation as Eq. (1.1) intuitively still works. We
would like to leave that to future work.
Nonsmooth phase retrieval and deep networks with ReLU mentioned in Section 1.1 are examples
of many nonsmooth, nonconvex problems encountered in practice. Most existing theoretical results
on these problems tend to be technically vague about handling the nonsmooth points: they either
prescribe a rule for choosing a subgradient element, which effectively disconnects theory and practice
because numerical testing of nonsmooth points is often not reliable, or ignore the nonsmooth points
altogether, assuming that practically numerical methods would never touch these points—this sounds
intuitive but no formalism on this appears in the relevant literature yet. Besides our work, (Laurent &
von Brecht, 2017; Kakade & Lee, 2018) also warns about potential problems of ignoring nonsmooth
points when studying optimization of nonsmooth functions in machine learning.
3The O(∙) notation ignores the dependency on logarithmic terms and other factors.
10
Published as a conference paper at ICLR 2019
References
Alekh Agarwal, Animashree Anandkumar, and Praneeth Netrapalli. A clustering approach to learning
sparsely used overcomplete dictionaries. IEEE Trans. Information Theory, 63(1):575-592, 2017.
Sanjeev Arora, Rong Ge, and Ankur Moitra. New algorithms for learning incoherent and overcomplete
dictionaries. In Conference on Learning Theory, pp. 779-806, 2014.
Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra. Simple, efficient, and neural algorithms for
sparse coding. 2015.
Jean-Pierre Aubin. Optima and equilibria: an introduction to nonlinear analysis, volume 140.
Springer Science & Business Media, 1998.
Pranjal Awasthi and Aravindan Vijayaraghavan. Towards learning sparsely used dictionaries with
arbitrary supports. arXiv preprint arXiv:1804.08603, 2018.
Adil Bagirov, Napsu Karmitsa, and Marko M Makela. Introduction to Nonsmooth Optimization:
theory, practice and software. Springer, 2014.
Boaz Barak, Jonathan A Kelner, and David Steurer. Dictionary learning and tensor decomposition
via the sum-of-squares method. In Proceedings of the forty-seventh annual ACM symposium on
Theory of computing, pp. 143-151. ACM, 2015.
StePhane Boucheron, Olivier Bousquet, and Gabor Lugosi. Theory of classification: A survey of
some recent advances. ESAIM: probability and statistics, 9:323-375, 2005.
James V Burke, Adrian S Lewis, and Michael L Overton. A robust gradient sampling algorithm for
nonsmooth, nonconvex optimization. SIAM Journal on Optimization, 15(3):751-779, 2005. doi:
10.1137/030601296.
James V. Burke, Frank E. Curtis, Adrian S. Lewis, Michael L. Overton, and Lucas E. A. Simes.
Gradient sampling methods for nonsmooth optimization. arXiv:1804.11003, 2018.
Emmanuel Candes. Mathematics of sparsity (and a few other things). In Proceedings of the
International Congress of Mathematicians, volume 123, 2014.
Niladri Chatterji and Peter L Bartlett. Alternating minimization for dictionary learning with random
initialization. In Advances in Neural Information Processing Systems, pp. 1997-2006, 2017.
Frank H Clarke. Optimization and nonsmooth analysis. SIAM, 1990. doi: 10.1137/1.9781611971309.
Frank E Curtis and Xiaocun Que. A quasi-newton algorithm for nonconvex, nonsmooth optimization
with global convergence guarantees. Mathematical Programming Computation, 7(4):399-428,
2015. doi: 10.1007/s12532-015-0086-2.
Frank E Curtis, Tim Mitchell, and Michael L Overton. A bfgs-sqp method for nonsmooth, nonconvex,
constrained optimization and its evaluation using relative minimization profiles. Optimization
Methods and Software, 32(1):148-181, 2017.
Damek Davis, Dmitriy Drusvyatskiy, and Courtney Paquette. The nonsmooth landscape of phase
retrieval. arxiv:1711.03247, 2017. URL http://arxiv.org/abs/1711.03247.
John C. Duchi and Feng Ruan. Solving (most) ofa set of quadratic equalities: Composite optimization
for robust phase retrieval. arxiv:1705.02356, 2017. URL http://arxiv.org/abs/1705.
02356.
Rong Ge, Jason D. Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape
design. arxiv:1711.00501, 2017. URL http://arxiv.org/abs/1711.00501.
Dar Gilboa, Sam Buchanan, and John Wright. Efficient dictionary learning with gradient descent. In
ICML 2018 Workshop on Modern Trends in Nonconvex Optimization for Machine Learning, 2018.
URL https://arxiv.org/abs/1809.10313.
11
Published as a conference paper at ICLR 2019
P Grohs and S Hosseini. Nonsmooth trust region algorithms for locally Lipschitz functions on
Riemannian manifolds. IMA Journal of Numerical Analysis, 36(3):1167-1192, 2015. doi: 10.
1093/imanum/drv043.
Philipp Grohs and Seyedehsomayeh Hosseini. ε-subgradient algorithms for locally Lipschitz functions
on Riemannian manifolds. Advances in Computational Mathematics, 42(2):333-360, 2016. doi:
10.1007/s10444-015-9426-z.
Jean-Baptiste Hiriart-Urruty and Claude Lemarchal. Fundamentals of Convex Analysis. Springer-
Verlag Berlin Heidelberg, 2001. doi: 10.1007/978-3-642-56468-0.
S Hosseini and MR Pouryayevali. Generalized gradients and characterization of epi-lipschitz sets in
Riemannian manifolds. Nonlinear Analysis: Theory, Methods & Applications, 74(12):3884-3895,
2011. doi: 10.1016/j.na.2011.02.023.
Seyedehsomayeh Hosseini. Optimality conditions for global minima of nonconvex functions on
Riemannian manifolds. Pacific Journal of Optimization, 2015. URL http://uschmajew.
ins.uni-bonn.de/research/pub/hosseini/3.pdf.
Seyedehsomayeh Hosseini and Andre Uschmajew. A Riemannian gradient sampling algorithm for
nonsmooth optimization on manifolds. SIAM Journal on Optimization, 27(1):173-189, 2017. doi:
10.1137/16M1069298.
Sham Kakade and Jason D. Lee. Provably correct automatic subdifferentiation for qualified programs.
arXiv:1809.08530, 2018.
Thomas Laurent and James von Brecht. The multilinear structure of relu networks. arxiv:1712.10132,
2017. URL http://arxiv.org/abs/1712.10132.
Yu Ledyaev and Qiji Zhu. Nonsmooth analysis on smooth manifolds. Transactions of the American
Mathematical Society, 359(8):3687-3732, 2007. doi: 10.1090/S0002-9947-07-04075-5.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. arXiv:1808.01204, 2018.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation.
arxiv:1705.09886, 2017. URL http://arxiv.org/abs/1705.09886.
Po-Ling Loh and Martin J Wainwright. Regularized m-estimators with nonconvexity: Statistical and
algorithmic theory for local optima. Journal of Machine Learning Research, 16:559-616, 2015.
Tengyu Ma, Jonathan Shi, and David Steurer. Polynomial-time tensor decompositions with sum-of-
squares. 2016.
Julien Mairal, Francis Bach, and Jean Ponce. Sparse modeling for image and vision processing.
Foundations and TrendsR in Computer Graphics and Vision, 8(2-3):85-283, 2014.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of
two-layers neural networks. arXiv:1804.06561, 2018.
Ilya Molchanov. Foundations of stochastic geometry and theory of random sets. In Stochastic
Geometry, Spatial Statistics and Random Fields, pp. 1-20. Springer, 2013.
Bruno A Olshausen and David J Field. Emergence of simple-cell receptive field properties by learning
a sparse code for natural images. Nature, 381(6583):607, 1996.
Barrett O’neill. Semi-Riemannian geometry with applications to relativity, volume 103. Academic
press, 1983.
Saiprasad Ravishankar and Yoram Bresler. Learning sparsifying transforms. IEEE Transactions on
Signal Processing, 61(5):1072-1086, 2013.
Tselil Schramm and David Steurer. Fast and robust tensor decomposition with applications to
dictionary learning. arXiv:1706.08672, 2017.
12
Published as a conference paper at ICLR 2019
Daniel A Spielman, Huan Wang, and John Wright. Exact recovery of sparsely-used dictionaries. In
Conference on Learning Theory, pp. 37-1, 2012.
Shlomo Sternberg. Dynamical Systems. Dover Publications, Inc, 2013.
Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery over the sphere. arxiv:1504.06785,
2015. URL http://arxiv.org/abs/1504.06785.
Aad Van Der Vaart and Jon A Wellner. A note on bounds for VC dimensions. Institute of Mathematical
Statistics collections, 5:103, 2009.
Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science.
Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2018.
doi: 10.1017/9781108231596.
Kai Zhong, Zhao Song, Prateek Jain, Peter L. Bartlett, and Inderjit S. Dhillon. Recovery guarantees
for one-hidden-layer neural networks. arxiv:1706.03175, 2017. URL http://arxiv.org/
abs/1706.03175.
A	Technical tools
A. 1 Hausdorff distance
We need the Hausdorff metric to measure differences between nonempty sets. For any set X and a
point p in Rn, the point-to-set distance is defined as
d (q, X) =. inf kx - pk .	(A.1)
x∈X
For any two sets X1, X2 ∈ Rn, the Hausdorff distance is defined as
dH (X1,X2) =. sup	sup d (x1,X2) , sup d (x2, X1) .	(A.2)
x1∈X1	x2∈X2
When X1 is a singleton, say X1 = {p}. Then
dH ({p} ,X2) = sup kx2 - pk .	(A.3)
x2∈X2
Moreover, for any sets X1, X2, Y1 , Y2 ⊂ Rn,
dH (X1 +Y1,X2+Y2) ≤ dH(X1,X2)+dH(Y1,Y2).	(A.4)
On the sets of nonempty, compact subsets of Rn , the Hausdorff metric is a valid metric; particularly,
it obeys the triangular inequality: for nonempty, compact subsets X, Y, Z ⊂ Rn ,
dH (X, Z) ≤dH(X,Y)+dH(Y,Z).	(A.5)
See, e.g., Sec. 7.1 of Sternberg (2013) for a proof.
Lemma A.1 (Restatement of Lemma A.1). For convex compact sets X, Y ⊂ Rn, we have
dH (X,Y) = sup |hX (u) - hY (u)| ,	(A.6)
u∈Sn-1
where hS (u) =. supx∈S hx, ui is the support function associated with the set S.
A.2 Sub-Gaussian random matrices and processes
Proposition A.2 (Talagrand’s comparison inequality, Corollary 8.6.3 and Exercise 8.6.5 of Vershynin
(2018)). Let {Xx }x∈T be a zero-mean random process on a subset T ⊂ Rn. Assume that for all
x, y ∈ T we have
kXx-Xykψ2 ≤Kkx-yk.	(A.7)
Then, for any t > 0
sup ∣Xχ | ≤ CK [w(T) + t ∙ rad(T)]	(A.8)
x∈T
with probability at least 1 — 2exp (—12). Here w(T) = Eg~N(0,1)suPχ∈τ hx, g〉is the Gaussian
width ofT and rad(T) = supx∈T kxk is the radius ofT.
13
Published as a conference paper at ICLR 2019
Proposition A.3 (Deviation inequality for sub-Gaussian matrices, Theorem 9.1.1 and Exercise 9.1.8
of Vershynin (2018)). Let A be an n × m matrix whose rows Ai ’s are independent, isotropic, and
sub-Gaussian random vectors in Rm. Then for any subset T ⊂ Rm, we have
P SuPIkAxk -√n |罔|| > CK2 [w (T) +1 ∙ rad (T)] ≤ 2 exp (—12) .	(A.9)
x∈T
Here K = maxi Ai ψ .
B Proofs for Section 3.1
B.1 Proof of Proposition 3.2
We have
E[f](q)
J2 ∙ E [∣q>xΩ∣] = E[kqΩ k2] ,
(B.1)
where the last equality is obtained by conditioning on Ω and the fact that EZ〜N(o,σ2) [|Z|] = ,2∕∏σ.
The subdifferential expression comes from
qω
∂ kqΩk2 = {际ɪ,
[ {vΩ : kvΩk2 ≤ 1} ,
qΩ = 0;
qΩ = 0,
(B.2)
and the fact that ∂E [f](q) = ∂E [∣∣qΩ∣∣2] = E [∂ ∣∣qΩ∣∣2] as the sub-differential and expectation can
be exchanged for convex functions (Hiriart-Urruty & Lemarchal, 2001). By the same exchangability
result, we also have E [∂f(q)] = ∂E [f] (q).
B.2 Proof of Proposition 3.3
We first show that points in the claimed set are indeed stationary points by taking the choice vω = 0
in Eq. (3.5), giving the subgradient choice E [∂f](q) = E [qo/llqolb 1 {qΩ = 0}]. Let q ∈ S and
such that kqk0 = k. For all j ∈ supp(q), we have
eT e [df](q)=θqj ∙E:≡1{j ∈ °}
=θqj∙ XX θiT(1-θ)k-i ∙ r
i=1
k
j X θi(1-θ)k-i
i=1
1
√7
=. c(θ, k)qj
(B.3)
(B.4)
(B.5)
On the other hand, for all j / supp(q), we always have [qcj = 0, so e> E [∂f ] (q) = 0. Therefore,
we have that E [∂f] (q) = c(θ, k)q, and so
(I - qq>)E [∂f] (q) = c(θ, k)q - c(θ, k)q= 0.	(B.6)
Therefore q ∈ S is stationary. To see that {±ei : i ∈ [n]} are the global minima, note that for all
q ∈ Sn-1, we have
E[f](q)= E [kqΩk2] ≥ E [kqΩk2] = θ.	(B.7)
Equality holds if and only if ∣∣qΩ∣∣2 / {0, 1} almost surely, which is only satisfied at q /
{±ei : i ∈ [n]}.
To see that the other q’s are saddles, we only need to show that there exists a tangent direction along
which q is local max. Indeed, for any other q, there exists at least two non-zero entries (with equal
absolute value): WLOG assume that q1 = qn > 0. Using the reparametrization in Appendix B.3
and applying Lemma B.2, we get that E [f](q) is directionally differentiable along [-q-n； 1-qn],
with derivative zero (necessarily, because 0 ∈ E [∂Rf] (q)) and strictly negative second derivative.
14
Published as a conference paper at ICLR 2019
Therefore E [f] (q) is locally maximized at q along this tangent direction, which shows that q is a
saddle point.
The other direction (all other points are not stationary) is implied by Theorem 3.4, which guarantees
that 0 ∈/ E [∂Rf] (q) whenever q ∈/ S. Indeed, as long as q ∈/ S, q has a max absolute value
coordinate (say n) and another non-zero coordinate with strictly smaller absolute value (say j ). For
this pair of indices, the proof of Theorem 3.4(a) goes through for index j (even if q ∈ Sζ(n+) does not
necessarily hold because the max index might not be unique), which implies that 0 ∈/ E [∂Rf] (q).
B.3 Reparametrization
For analysis purposes, we introduce the reparametrization w = q1:(n-1) in the region S0(n+),
following (Sun et al., 2015) . With this reparametrization, the problem becomes
>
X
m皿m/weRn-1 g (w)=Jπ ∙ m
subject to kwk ≤
1
(B.8)
The constraint comes from the fact that qn ≥ 1∕√n and thus k w∣∣≤ Nn - 1)/n.
Lemma B.1. We have
Ex〜iidBG(θ)g (W) = (1 — θ) Eω ∣∣WΩk + ΘEω，1 - kwΩck2.	(B.9)
Proof. Direct calculation gives
Ex〜iidBG(θ)g (W)	(B.10)
=EΩ〜iidBer(θ),ω〜Ber(θ)Ez〜e(0,1),z〜N(0,1) w； J1 - kwk2	⑲;ω] ISI [z； ZD	(B∙11)
=(1-Θ)Eω~iid Ber(θ ) ^^Z~iidNz^(0,I)I^^。二 |
+ θEΩ〜iidBer(θ)Ez〜iidN(0,1),z〜N(0,1) w>z + J1 - kw∕z	(B」2)
=(1 - θ) EΩ〜iidBer(θ) ∣∣wΩ∣∣ + θEΩ〜iidBeMS)/1 - kwΩc ∣∣2,	(B.13)
as claimed.
Lemma B.2 (Negative-curvature region). For all unit vector v ∈ Sn-1 and all s ∈ (0, 1), let
hv (s) =. E [g] (sv)	(B.14)
it holds that
V2hv (S) ≤ —θ (1 — θ) .	(B.15)
In other words, for all W 6= 0, ±W /∣W∣ is a direction of negative curvature.
Proof. By Lemma B.1,
hv (S) = (1 — θ) sEω ∣vω k + ΘEω，1 -S .Ωc k2.
For s ∈ (0, 1), hv (s) is twice differentiable, and we have
V2 hv (S) = -ΘEω--_3/2
(1-s2∣VΩc『)/
≤-ΘEω∣vωc k2 = -θ (1 - θ),
(B.16)
(B.17)
(B.18)
completing the proof.
15
Published as a conference paper at ICLR 2019
Lemma B.3 (Inward gradient). For any w with kwk2+kwk2∞ ≤ 1,
DW/kwkE [g] (w) ≥ θ (1 - θ) 1/√1 + kwk∞/kwk2 -kwk .
(B.19)
Proof. For any unit vector v ∈ Rn-1, define hv (t) =. E [g] (tv) for t ∈ (0, 1).
from Lemma B.1
We have
Moreover,
hv (t) = (1 — θ) tEΩ kvΩk + ΘEω Ji- t2 ∣WΩ k2.
(B.20)
Vthv (t) = (1 - θ) Eω kvΩk — ΘEω
tkvΩk2
≠ -12 kvΩk2
(B.21)
=(1	θ) EΩ N - θEω	t = k2
k%k	,1-t2kvΩ k2
=(1 - θ) X Eω /	v21 {i ∈ 0}	2
i=1	ʌ/v21 {i / ω} + ∣∣vΩ∖i∣∣
(assuming 0 = 0)
(B.22)
—
n-1
θ X Eω
i=1
tv21 {i / Ω}
11 - t2v2l {i ∈ Ω} - t2 ∣∣VΩc∖i∣∣
(B.23)
2
n-1
θ (1 - θ) X Eω
i=1
vi2
---.	------,
Qv + ∣∣vΩ∖i∣∣2 J1 -t2v2 -t2∣∣vΩc∖i∣∣2
1
1
- Jt2v2 + t2 ∣∣vΩ∖i∣∣2 J1 - t2v2 - t2 ∣∣vΩc∖i∣∣2
1
1
一 Jt2v2 + t2 ∣∣vΩ∖i∣∣2 J1 - t2kvk2+t2 ∣∣vΩ∖i∣∣2 一
We are interested in the regime of t so that
1 -12 kvk2 ≥ t2 kvk∞ =⇒ t ≤ 1/√1 + kvk∞.
So Vthv (t) ≥ 0 holds always for t ≤ 1/“ + ∣∣v∣∣∞.
By Lemma B.2, V2hv (t) ≤ -θ (1 - θ) over t ∈ (0, 1), which implies
hVthv (t1) - Vthv (t2) , t1 - t2i ≤ -θ (1 - θ) (t1 - t2) .
Taking t1 = 1/ 1 + kvk2∞ and considering t2 ∈ [0, t1], we have
Vthv (t2) ≥ Vthv (tι) + θ (1 - θ)(tι - t2) ≥ θ (1 - θ) (1/∕1+-h∞ - t2).
(B.24)
(B.25)
. (B.26)
(B.27)
(B.28)
(B.29)
For any w, applying the above result to the unit vector w/kwk and recognizing that Vthw/kwk (t) =
Dw/kwk g (w) = Dwc /kwk g (w), we complete the proof.
B.4 Proof of Theorem 3.4
We first show Eq. (3.9) using the reparametrization in Appendix B.3. We have
h∂Rf (q) ,q — eni = h∂f (q) ,qnq — eni = qn h∂g (w) ,wi ,
(B.30)
16
Published as a conference paper at ICLR 2019
where the second equality follows by differentiating g via the chain rule. Now, by Lemma B.3,
qn hE [∂g](w) , Wi ≥ ∣∣wk θ (1 - θ) ∙ qn
∣w∣
VZkwk2 + kwk∞
- kwk	.
(B.31)
For each radial direction v =. w/kwk , consider points of the form tv with t ≤ 1/√1 + kvk∞.
Obviously, the function
h (t) = qn (tv)
∣tv ∣
VZktvk2 + ktvk∞
-ktvk I = qn (tv) (	/	1	0 - t
1 + kvk2∞
(B.32)
is monotonically decreasing wrt t. Thus, to derive a lower bound, it is enough to consider the largest
t allowed. In S[；+), the limit amounts to requiring qn/kwk∞ = 1 + Zo,
(B.33)
So for any fixed v and all allowed t for points in Sζ(n+), a uniform lower bound is
qn (t0V) I -/	= - t0
1 + kvk2∞
(B.34)
≥ Σ√ζ0 kvk∞ ≥ ξζ0n-3/2.
8n	8
(B.35)
So we conclude that for all q ∈ Sζ(n+),
hE [∂f] (q), qnq - eni ≥ 1 θ (1 - θ) Zon-3/2 kwk = 1 θ (1 - θ) ζon-3/2 kq-nk .
88
(B.36)
We now turn to showing Eq. (3.8). For ej with qj 6= 0,
命 l{qΩ = 0} + {vΩ : 皿 1}l{qΩ = 0}
一Eω
qj
M 120}
一θqj EΩ
qj
θE
Nq +忖。\｛力『_
≥ θ.
(B.37)
1
≥ :
1
1
J ∈ ω
1
1
So for all j with qj 6= 0, we have
(E [dRf] (q) , 一ej---en)
qj	qn
=( (I - qq>) E [df ] (q) , —ej----en
qj	qn
=(E [df ] (q) , —ej-----en)
qj	qn
1
1
(B.38)
(B.39)
ΘEω
Nq +帆\｛力『_
-ΘEω
_yqn + ∣∣qΩ∖{n}∣∣2,
(B.40)
Θ2Eω
1
qq2 + qn + ∣∣qΩ∖{j,n}∣∣2 .
+ θ (1 - θ) Eω
q2+ + ∣∣qΩ∖{j,n}∣∣2
1
17
Published as a conference paper at ICLR 2019
-θ (1-Θ)Eω	.	=
.Vqn + llqΩ∖{j,n}1.
1
q∕q2 + qn + llqΩ∖{j,n}∣∣2
1
2 (t + ∣lqΩ∖{j,n}∣∣2)
3/2 dt
qn2
=θ (1 - θ) Eω /
qj2
≥ θ (1—θ)2 (qn -
=≥ ɪ θ (1-θ)
2n
q2) ≥ θ( (1 - θ) (qn - kq-nk∞) ≥ θ( (1 - θ) ɪ0
2	2	1 + ζ0
Zo
1 + Zo
(B.41)
(B.42)
(B.43)
(B.44)
(B.45)
completing the proof.
C Proofs for Section 3.2
C.1 COVERING IN THE dE METRIC
For any θ ∈ (0, 1), define
dE,θ (p, q)= E [1 {sign (p>x) = Sign (q>x)}] WithX 〜iid BG(θ).	(C.1)
We stress that this notion alWays depend on θ, and We Will omit the subscript θ When no confusion is
expected. This indeed defines a metric on subsets of Sn-1.
Lemma C.1. Over any subset of Sn-1 with a consistent support pattern, dE is a valid metric.
Proof. Recall that / (x, y) = arccoshx, y〉defines a valid metric on Sn-1.4 In particular, the
triangular inequality holds. For dE and p, q ∈ Sn-1 With the same support pattern, We have
dE (p, q) = E [1 {sign (p>x) = Sign (q>x)}]	(C.2)
=EωEz〜N(0,I)1 {sign (p>z) = sign (q>z)}	(C.3)
=Eω (EzI {p>zqΩZ < 0} + EzI {p>z = 0 or q>z = 0, not both})	(C.4)
=Eω (EzI {p>zqΩZ < 0})	(C.5)
=1 Eω∕(pω, Qω) ,	(C.6)
π
where We have adopted the convention that L (0, V)= 0 for any v. It is easy to verify that dE (p, q)=
0 ^⇒ P = q, and dE (p, q) = dE (q, p). To show the triangular inequality, note that for any p, q
and r with the same support pattern, p。，q。，and r。are either identically zero, or all nonzero. For
the former case,
L (pω, Qω) ≤ L (pω, r。)+ L (q。，r。)
(C.7)
holds trivially. For the latter, since / (∙, ∙) obeys the triangular inequality uniformly over the sphere,
pω	qΩ )
西,MJ
≤ /
pω	rΩ ) J qΩ rΩ )
M,西 + ∖M,百J ,
(C.8)
which implies
/ (pω, q。)≤ / (pω, r。)+ / (qΩ, r。).	(C.9)
4This fact can be proved either directly, see, e.g., page 12 of this online notes: http://www.math.
mcgill.ca/drury/notes354.pdf, or by realizing that the angle equal to the geodesic length, which is
the Riemmannian distance over the sphere; see, e.g., Riemannian Distance of Chapter 5 of the book O’neill
(1983).
18
Published as a conference paper at ICLR 2019
So
Eω∕ (pω, Qω) ≤ Eω∕ (pω, tω) + Eω∕ (qΩ, tω),
(C.10)
completing the proof.
Lemma C.2 (Vector angle inequality). For n ≥ 2, consider u, V ∈ Rn so that L (u, V) ≤ π∕2. It
holds that
/(u, v) ≤ E /(uω, Vω).
Ω∈([2])
(C.11)
Proof. The inequality holds trivially when either of u, V is zero. Suppose they are both nonzero and
wlog assume both are normalized, i.e., kuk = kVk = 1. Then,
sin2 L (u, v) = 1 — cos2 L (u, v)		(C.12)
= kuk2 kVk2 - hu, Vi2		(C.13)
=	(uivj — ujvi)2 i,j:j>i	(Lagrange’s identity)	(C.14)
=X	kuΩk2 kvΩk2 - Ω∈([2])	huΩ, vq)2	(C.15)
= X kuck2kvck2 sin2 / (uω, vω) Ω∈([2])		(C.16)
≤ E sin2 Z (uω, vω) Ω∈([2])	.	(C.17)
If ∑ω∈(H) L (uω, vω) > ∏∕2, the claimed inequality holds trivially, as / (u, V) ≤ π∕2 by our
assumption. Suppose Pω∈([R) / (uω, vω) ≤ ∏∕2. Then,
E sin2∕(uΩ, vω) ≤ sin2 E / (uω, vω)
Ω∈([2])	Ω∈([2])
(C.18)
by recursive application of the following inequality: ∀ θ1, θ2 ∈ [0, π∕2] with θ1 + θ2 ≤ π∕2,
sin2 (θ1 +	θ2)	= sin2 θ1 + sin2 θ2 + 2sinθ1 sin θ2	cos (θ1	+ θ2)	≥ sin2	θ1	+ sin2	θ2.	(C.19)
So we have that when Pω∈([R) / (uω, vω) ≤ ∏∕2,
sin2∕(u, v) ≤ sin2 E / (uω, vω)=⇒ / (u, V) ≤ E / (uω, vω),
Ω∈([2])	Ω∈([2])
(C.20)
as claimed.
Lemma C.3 (Covering in maximum length-2 angles). For any η ∈ (0, 1∕3), there exists a subset
Q ⊂ Sn-1 of size at most (5n log(1∕η)∕η)2n-1 satisfying the following: for any p ∈ Sn-1, there
exists some q ∈ Q such that / (pω, Qω) ≤ ηforall Ω ⊂ [n] with ∣Ω∣≤ 2.
Proof. Define
d2(p, q) = max / (pω, Qω),
our goal is to give an η-covering of Sn-1 in the d2 metric.
(C.21)
Step 1 We partition Sn-1 according to the support, the sign pattern, and the ordering of the non-zero
elements. For each configuration, we are going to construct a covering with the same configuration
of support, sign pattern, and ordering. There are no more than 3n ∙ n! such configurations. Note that
we only need to construct one such covering for each support size, and for each support size we can
ignore the zero entries - the angle /(pω, Qω) is always zero when p, q have matching support and Ω
contains at least one zero index.
Therefore, the task reduces to bounding the covering number of
An = {p ∈ Sn-1 : P ≥ 0, 0 <Pι ≤ P2 ≤ ... ≤ Pn}	(C.22)
in d2 for all n.
19
Published as a conference paper at ICLR 2019
Step 2 We bound the covering number of An by induction. Suppose that
N(An) ≤ f5ɪ°g≡ ∙ n1
0
(Cηn0)n -1
(C.23)
η
holds for all n0 ≤ n -1. (The base case m = 2 clearly holds.) Let Cn0 ⊂ Sn0-1 be the correpsonding
covering sets.
We now construct a covering for An. Let R = 1∕η = rk for some r ≥ 1 and k to be determined.
Consider the set
Qr,k = q q ∈ Sn-1 : qi+1 ∈ {1,r,r2,...,rkT} for all 1 ≤ i ≤ n — 1 > .	(C.24)
We claim that Qr,k with properly chosen (r, k) gives a covering of
An,R = pP ∈ An : ʒ+-- ≤ R, ∀i} ⊂ An.
(C.25)
Indeed, we can decompose [1, R) into [1, r), [r, r2), . . . , [rk-1, R). Each consecutive ratio pi+1/pi
falls in one of these intervals, and we choose q so that qi+1/qi is the left endpoint of this interval.
Such a q satisfies q ∈ Qr,k and
pi+1/" ∈ [1,r) for all i ∈ [n 一 1].
qi+1/qi
By multiplying these bounds, we obtain that for all 1 ≤ i < j ≤ n,
(C.26)
ji ∈…
(C.27)
Take r = 1 + η∕2n, we have rn-1 = (1 + η∕2n)n-1 ≤ exp(η∕2) ≤ 1 + η. Therefore, for all i,j,
we have pqj/pi ∈ [1,1 + η), which further implies that / ((pi,pj), (qi, qj)) ≤ η by Lemma F.4. Thus
we have for all ∣Ω∣≤ 2 that L (pω, q。)≤ η. (The size-1 angles are all zero as we have sign match.)
For this choice of r, we have k = log R∕log r and thus
n
|Qr,k|=kn-1
log(1∕η)
4n log(1∕η)
(1 + η∕(2n))
(C.28)
r
η
and we have N(An,R) ≤ Nn.
Step 3 We now construct the covering of An \ An,R. For any p ∈ An \ An,R, there exists
some i such that pi+1∕pi ∈ [R, ∞), which means that the angle of the ray (pi,pi+1) is in between
[arctan(R), π∕2) = [π∕2 一 η, π∕2). As p is sorted, we have that
pi+j ≥ R for all j ≥ 1, l ≥ 0,
pi-l
(C.29)
So ifwe take q such that qi+1∕qi ∈ [R, ∞), q also has the above property, which gives that
/ ((Pi-1 ,Pi+j), (qi-ι, qi+j)) ≤ n/2 — (∏∕2 一 η) = η for all j ≥ 1, l ≥ 0.	(C.30)
Therefore to obtain the cover in d2, we only need to consider the angles for Ω ⊂ {1,...,i} and
Ω ⊂ {i + 1,..., n}, which can be done by taking the product of the covers in Ai and An-i.
By considering all i ∈ {1, . . . , n 一 1}, we obtain the bound
n-1
N(An\An,R) ≤	N(Ai)N(An-i).
(C.31)
i=1
20
Published as a conference paper at ICLR 2019
Step 4 Putting together Step 2 and Step 3 and using the inductive assumption, we get that
n-1
N (An) ≤ N(An,R) + N (An \ An,R) ≤ Nen + X N (Ai)N (An-i)	(C.32)
i=1
≤ (4nlog(1/n))n 1 + X(Cni)i-1(Cη(n - i))n-i-1	(C.33)
η	i=1
4 n-1
≤ (5)	(Cnn)n-1 + (n -1) ∙ Cn-2nn-2	(C.34)
≤ ((5)” 1 +	(Cnn)n-1 ≤ (Cnn)n-	(C.35)
This shows the case for m = n and completes the induction.
Step 5 Considering all configurations of {support, sign pattern, ordering}, we have
N(SnT) ≤ 3n ∙n!∙N(An) ≤ (3n)n (5log(1/n)n)、(5log(1/n)八了” ,	(C.36)
Lemma C.4 (Covering number in the dE metric). Assume n ≥ 3. There exists a numerical constant
C > 0 such that for any e ∈ (0,1), Sn-I admits an e-net of size exp(Cn log nn) w.r.t. dE defined
in Eq. (C.1): for any p ∈ Sn-1, there exists a q in the net with supp (q) = supp (p) and dE (p, q) ≤
e. We say such e nets are admissible for Sn-1 wrt dE.
Proof. Let η = e/n2 . By Lemma C.3, there exists a subset Q ⊂ Sn-1 of size at most
*≡)2n-1 = ( Sn"?//e) )2n-1 ≤ exp (Cn log n)
(C.37)
such that for any p ∈ Sn-1, there exists q ∈ Sn-I such that SuPP(P) = supp(q) and L (p。，q。) ≤ η
for all ∣Ω∣≤ 2. In particular, the ∣Ω∣= 1 case says that Sign(P) = sign(q), which implies that
/ (PΩ, q。) ≤ ∏∕2 ∀ Ω ∈ {0,1}n .	(C.38)
Thus, applying the vector angle inequality (Lemma C.2), for any p ∈ Sn-1 and the corresponding
q ∈ Q, we have
L (Pω, qΩ) ≤ X / (PΩ0, qΩ0) ≤ 2 (|?) η ≤ ∣Ω∣2η ∀ Ω With 3 ≤ ∣Ω∣≤ n. (C.39)
∣Ω0∣=2,Ω0⊂Ω	、	)
Summing up, we get
L (Pω, qΩ) ≤ max(2, ∣Ω∣2) η ≤ n2η = e ∀ Ω.	(C.40)
Therefore dE(P, q) ≤ e.
Below we establish the ”Lipschitz” property in terms ofdE distance.
Lemma C.5. Fix θ ∈ (0, 1). For any e ∈ (0, 1), let N be an admissible e-net for Sn-1 wrt dE. Let
xι,..., Xm beiid copies of X 〜忧壮 BG(θ) in Rn. When m ≥ Ce-2n, the inequality
1m
SUP	R (p, q) = — X 1 {sign (p>x, = sign (q>x,} ≤ 2e (C.41)
p∈Sn-1,q∈N	m i=1
supp(p)=supp(q), dE (p,q) ≤
holds with probability at least 1 - exp -ce2m . Here C and c are universal constants independent
ofe.
21
Published as a conference paper at ICLR 2019
Proof. We call any pair of p, q ∈ Sn-1 with q ∈ N, supp (p) = supp (q), and dE (p, q) ≤ an
admissible pair. Over any admissible pair (p, q), E [R] = dE (p, q). We next bound the deviation
R - E [R] uniformly over all admissible (p, q) pairs. Observe that the process R is the sample
average of m indicator functions. Define the hypothesis class
H = {x → 1 {sign (p>x) = sign (q>x)} : (p, q) is an admissible pair} .	(C.42)
and let dvc(H) be the VC-dimension of H. From concentration results for VC-classes (see, e.g., Eq
(3) and Theorem 3.4 of Boucheron et al. (2005)), we have
P
sup
(p,q) admissible
{R(p, q)- E [R](p, q)}≥ Co ^H
+t
≤ exp(-mt2 )
(C.43)
for any t > 0. It remains to bound the VC-dimension dvc(H). First, we have
dvc (H) ≤ dvc {x 1 1 {sign (p>x) = sign (q>x)} ： P, q ∈ Sn-1} .	(C.44)
Observe that each set in the latter hypothesis class can be written as
{x → 1 {sign (p>x) = sign (q>x)} ： P, q ∈ Sn-1
={x → 1 {p>x > 0, q>x ≤ 0} : p, q ∈ Sn-1} ∪ {x → 1 {p>x ≥ 0, q>x < 0} : p, q ∈ Sn-1}
(C.45)
∪ {x → 1 {p>x < 0, q>x ≥ 0} : p, q ∈ Sn-1} ∪ {x → 1 {p>x ≤ 0, q>x > 0} : p, q ∈ Sn-1}.
(C.46)
the union of intersections of two halfspaces. Thus, letting
Ho = {x → 1 {x>z ≥ 0} : Z ∈ Rn}	(C.47)
be the class of halfspaces, we have
H ⊂ (Ho uHo) t (Ho uHo) t (Ho uHo) t (Ho uHo).	(C.48)
Note that Ho has VC-dimension n + 1. Applying bounds on the VC-dimension of unions and
intersections (Theorem 1.1, Van Der Vaart & Wellner (2009)), we get that
dvc(H) ≤ C1dvc(HouHo) ≤ C2dvc(Ho) ≤ C3n.	(C.49)
Plugging this bound into Eq. (C.43), we can set t = /2 and make m large enough so that
Co√C3'n/m ≤ e/2, completing the proof.	■
C.2 Pointwise convergence of sub-differential
Proposition C.6 (Pointwise convergence). For any fixed q ∈ Sn-1,
PhdH (∂f(q), E[∂f](q)) > CaPnTm + Cbt/√mi ≤ 2exp (-t2) ∀ t > 0.	(C.50)
Here Ca , Cb ≥ 0 are universal constants.
Proof. Recall that
dH (∂f (q) ,E [∂f	(q)]) = sup	h∂f(q)	(u)	- hE∂f (q)	(u) = sup	h∂f(q)	(u) -	Eh∂f(q) (u)	.
u∈Sn-1	u∈Sn-1
(C.51)
Write Xu =. h∂f(q) (u) - Eh∂f(q) (u) and consider the zero-mean random process {Xu} defined
on Sn-1. For any u, v ∈ Sn-1, we have
kXu-Xvkψ2 = h∂f(q) (u) - Eh∂f(q) (u) - h∂f(q) (v) + Eh∂f(q) (v)ψ2	(C.52)
=Co ɪ X (hQi (u) - EhQi(U)- hQi (v) + EhQi(V))	(C.53)
i∈[m]	ψ2
22
Published as a conference paper at ICLR 2019
≤ CI m1 ( X k hQi (U)- EhQi(U) - hQi (V) + EhQi(V) kψ2 )	(C.54)
i∈[m]
≤ C2 m1 ( X khQi (u) - hQi (v)kψ2 I	(centering),	(C.55)
i∈[m]
where we write Qi =.	sign q>xi	xi	for all i ∈	[m]. Next we estimate	khQi	(U) -	hQi (V)kψ	. By
definition,
hQi (U) - hQi (V) = sup hz, Ui - sup hz0, Vi .	(C.56)
z∈Qi	z0∈Qi
If hQi (u) 一 hQi (v) ≥ 0 and let z* = arg maXz∈Qi(z, u)，We have
hQi (u) 一 hQi (v) ≤ (z*, Ui 一 (z*, Vi = (z*, U 一 Vi,	(C.57)
and
khQi (U) 一 hQi (V)kψ2 ≤ khz*,U 一 Vikψ2 ≤ xi> (U 一 V)ψ2 ≤ C3 kU 一 Vk ,	(C.58)
Where We have used Lemma F.1 to obtain the last upper bound. If hQi (U) 一 hQi (V) ≤ 0, hQi (V) 一
hQi (U) ≥ 0 and We can use similar argument to conclude that
khQi (U)- hQi (V)kψ2 ≤ C3 ku 一 vk .	(C.59)
So
kXu - Xv kψ2 ≤ √= kU 一 Vk .	(C.60)
Thus, {Xu} is a centered random process with SUb-GaUssian increments with a parameter C4∕√m.
We can apply Proposition A.2 to conclude that
P sup ∣h∂f(q) (u) 一 Eh∂f(q) (u)∣ > C5,n/m + Cot∕√m ≤ 2exp (-产)∀ t > 0,
u∈Sn-1
(C.61)
which implies the claimed resUlt.
C.3 Proof of Proposition 3.5 (Uniform convergence)
ThroUghoUt the proof, we let c, C denote Universal constants that coUld change from step to step.
Fix an ∈ (0, 1/2) to be decided later. Let N be an admissible net for Sn-1 wrt dE, with
|N| ≤ exp(Cn log(n/)) (Lemma C.4). By Proposition C.6 and the Union boUnd,
P [∃ q ∈ Ne, dH (∂f (q), E [∂f](q)) > t/3] ≤ exp (—cmt2 + Cnlog n)	(C.62)
provided that m ≥ Ct-2n.
For any p ∈ Sn-1, let q ∈ Ne satisfy supp (q) = supp (p) and dE (p, q) ≤ . Then we have
dH (∂f (p) ,E [∂f] (p)) ≤ dH (∂f (q) ,E [∂f] (q)) +dH (E [∂f] (p) ,E[∂f] (q)) +dH (∂f (p) ,∂f(q))
-----------------------------------{----------} '--------------{-----------} '---------{---------}
I	II	III
(C.63)
by the triangUlar ineqUality for the HaUsdorff metric. By the preceding Union boUnd, term I is boUnded
by t/3 as long as the bad event does not happen. For term II, we have
dH (E [∂f] (p) ,E [∂f] (q))
= sup ∣hE[∂f](p) (U) 一 hE[∂f](q) (U)∣	(C.64)
u∈Sn-1
23
Published as a conference paper at ICLR 2019
(C.65)
sup E sup sign p>xi xi>u - sup sign q>xi xi>u
u∈Sn-1
(C.66)
• sup ∣E [∣x>u∣l {sign (p>x, = Sign (q>x, }] ∣
u∈Sn-1
• 3e ʌ/ɪogɪ.
(C.67)
(C.68)
where the last line follows from Lemma F.5. As long as e ≤ ct//log(1∕t), the above term is upper
bounded by t/3. For term III, we have
dH (∂f (p) ,∂f (q))
sup ∣h∂f(p) (U) — h∂f(q) (U)∣
u∈Sn-1
(C.69)
π ∙ — sup
2 m u∈Sn-1
ɪ2	sup sign (p>Xi) x> U — sup sign (q>x, x> U
i∈[m]:Sign(p> Xi)=Sign(q>Xi)
(C.70)
π ∙ — sup
2 m u∈Sn-1
sixi>u
i∈[m]:Sign(p> Xi)=Sign(q>xi)
(si ∈ {+1, -1} dependent on p, q, xi and u)
≤
≤
—
(C.71)
π 2
2m
sixi
i∈ [m]:Sign(P>Xi)=sign(q>Xi)
(C.72)
By Lemma C.5, with probability at least 1 - exp(-ce2m), the number of different signs is upper
bounded by 2me for all p, q such that dE(p, q) ≤ e. On this good event, the above quantity can
be upper bounded as follows. Define a set T =. {s ∈ Rm : si ∈ {+1, -1, 0} , ksk0 ≤ 2me} and
consider the quantity sups∈T kXsk , where X = [x1, . . . , xm]. Then,
sixi
i∈[m]:Sign(P>Xi)=Sign(q>Xi)
uniformly (i.e., indepdent ofp, q and U). We have
w (T) = Esup s>g = E	sup
s∈T	K⊂[m],∣K∣≤2me
≤ sup kXsk
s∈T
(C.73)
|gi |
i∈K
(C.74)
≤ 2meE∣∣g∣∣∞ ≤ 4m√log me,	(here g ~N (0, Im))
rad (T) = √2me.
(C.75)
(C.76)
Noting that 1∕√θ • X has independent, isotropic, and sub-Gaussian rows with a parameter C/√θ,
we apply Proposition A.3 and obtain that
sup kXsk ≤ √θn√2me + √θ (4mPι°gme+10 √2me)
with probability at least 1 — 2exp —t20 . So we have over all admissible (p, q) pairs,
(C.77)
(C.78)
(C.79)
24
Published as a conference paper at ICLR 2019
Setting to = ct√m and e = ct,θ/log m, We have that
dH (∂f (p) ,∂f (q)) ≤ t,
(C.80)
provided that m ≥ Cet-2n = Ct-1ny∕θ∕log m, which is subsumed by the earlier requirement
m ≥ Ct-2n.
Putting together the three bounds Eq. (C.62), Eq. (C.67), Eq. (C.80), we can choose
e = ct↑ -——≤ | ≤ ≤ ct ∙ min < ʌ —, z ɪ = >
V log(m∕t)	[V logm √log(1∕t) ∫
and get that dH (∂ f (p), E [∂f] (p)) ≤ t with probability at least
1 — 2exp (—cmt2) — exp(-cme2) — exp (—cmt2 + Cnlog n)
≥ 1 - 2 exp(-cmt2 ) - exp
—
cmθt2	2	n log(m∕t)
ɪog(m/))-exp bcmt + Cn log
cmθt2
≥ exp ( log(m∕t)
(C.81)
(C.82)
(C.83)
(C.84)
provided that m ≥ Cnt-2 log n log(m/t). A sufficient condition is that m ≥ Cnt-2 log2(n∕t)
for sufficiently large C. When this is satisfied, the probability is further lower bounded by 1 —
exp(—cmθt2 ∕log m).
C.4 Proof of Theorem 3.6
Define
t = 3213/2。…ζ ≤ min (.θ(1 - θ)τ⅛, ⅛√√2θ(1 —吹0) .	(C.3
By Proposition 3.5, with probability at least 1 — exp (—cmθ3Z2n-3 log-1 m) we have
dH(E[∂f](q),∂f(q))≤t,	(C.86)
provided that m ≥ Cn4θ-2ζ0-2 log (n∕ζ0). We now show the properties Eq. (3.12) and Eq. (3.13)
on this good event, focusing on Sζ(n+) but obtaining the same results for all other 2n — 1 subsets by
the same arguments.
For Eq. (3.12), we have
(∂r∕ (q), ej∕qj — en∕q^^ = (∂f (q),(I — qq>) (ej∕qj — en∕qn)) = (∂f(q), ej∕qj — en∕qni .
(C.87)
Now
sup h∂f (q) ,en∕qn — ej∕qji
= h∂f (q) (en∕qn — ej∕qj)	(C.88)
= Eh∂f (q)	(en ∕qn	— ej ∕qj )	—	Eh∂f(q) (en ∕qn — ej ∕qj ) + h∂f (q) (en ∕qn — ej ∕qj )	(C.89)
≤ Eh∂f (q)	(en ∕qn	— ej ∕qj )	+	ken ∕qn — ej ∕qj k sup Eh∂f(q) (u) — h∂f(q) (u)	(C.90)
u∈Sn-1
=sup hE [∂f](q), en∕qn — ej∙∕qj + ∣∣en∕qn — ej∙∕qj∙∣∣ dH (E [∂f](q), ∂f (q)) .	(C.91)
By Theorem 3.4(a),
sup hE [∂f](q), en — q® ≤ — ɪθ (1 - θ) F .	(C.92)
2n	1 + ζ0
25
Published as a conference paper at ICLR 2019
Moreover, ken/qn - ej /q, ∣∣ =,\/磕2 + 1/j ≤ pi/q] + 3/qn ≤ 2√n. Meanwhile, we have
1ζ
dH (E [∂f](q) ,∂f (q)) ≤ t ≤ Bθ (1 - θ) iɪ.	(C.93)
We conclude that
inf h∂f (q) ,ej/qj - en/qni = - sup h∂f (q) , en/qn - ej/qji	(C.94)
≥ 5-θ (1 - θ) ι—2√n ∙ ；j-θ (1 - θ) ι ∙ √j= (C.95)
2n	1+ ζ0	4n	1+ ζ0 2 n
≥ ɪθ (1 - θ) ζ°r,	(C.96)
4n	1+ ζ0
as claimed.
For Eq. (3.13), we have by Theorem 3.4(b) that
sup h∂f (q) ,en - qnqi = h∂f(q) (en - qnq)	(C.97)
= Eh∂f (q) (en - qnq) - Eh∂f (q) (en - qnq) +	h∂f (q) (en - qnq)
(C.98)
≤ Eh∂f(q)	(en	- qnq) +	ken	-	qnqk	sup	Eh∂f (q)	(u)	- h∂f(q) (u)
u∈Sn-1
(C.99)
= sup hE [∂f] (q) ,en - qnqi+	kq-nk dH (E [∂f] (q) ,∂f(q)) .
(C.100)
As we are on the good event
,一一一一一 2 — √
dH (E [∂f](q) , ∂f (q)) ≤ t ≤ 16-3∕r ∙ θ (1 - θ) Z0,	(C.101)
we have
inf h∂f (q) ,qnq - eni = -sup h∂f (q) ,en - qnqi	(C.102)
≥ 8 θ (1 - θ) Zon-3/2 kw∣ -kq-nk 2--/2 ∙ θ (1 - θ) Zn-3/
(C.103)
≥ 空θ (1 - θ) Zon-332 kq-nk .	(C.104)
16
Noting that ∣q-nk ≥ √1/ ∣∣q 一 en∣ for all q with qn ≥ 0 completes the proof.
C.5 Proof of Proposition 3.7
For any q ∈ Sn-1,
supk∂f(q)k =dH({0},∂f(q)) ≤dH({0},E[∂f](q))+ dH(∂f(q),E[∂f](q)) (C.105)
by the metric property of the Hausdorff metric. On one hand, we have
sup ∣∣E [∂f] (q)k = sup Eω
尚 l{qΩ = 0} + {vΩ : kvΩk≤ 1}1…卜 1
(C.106)
On the other hand, by Proposition 3.5,
dH(∂f(q),E[∂f](q)) ≤ 1 ∀q∈Sn-1	(C.107)
with probability at least 1 - exp -c1mθ log-1 m , provided that m ≥ C2n2 logn (simplified using
θ ≥ 1/n). Combining the two results complete the proof.
26
Published as a conference paper at ICLR 2019
C.6 Additional geometries on the empirical objective
Proposition C.7. On the good event in Proposition 3.7, for all q ∈ Sζ(n+), we have
f(q)- f(en) ≤ 2√n kq - enk .	(C.108)
Proof. We use Lebourg’s mean value theorem for locally Lipschitz functions5, i.e., Theorem 2.3.7
of (Clarke, 1990). It is convenient to work in the w space here. By subdifferential chain rules, g (w)
is locally LiPschitz over {w : ∣∣w∣∣ < Jn— }. Thus, We have
f(q) - f (en) = g(w) -g(0) = hv, wi	(C.109)
for a certain t0 ∈ (0, 1) and a certain v ∈ ∂g (t0w). NoW for any q and the corresPonding w,
h∂g (w) , Wi = -1 <∂rf (q) ,qnq - ej .	(C.110)
qn
It folloWs
hv, Wi ≤ ɪsup h∂g (tow) ,towi = ɪ-1—； SuP <∂r∕ (q (tow)) ,qn (tow) q (tow) - eni
t0	t0 qn (t0w)
l∣qn (tow) q (tow) - enk / C kqn (t0w) q (tow) - enk
≤ SUP k∂Rf (q (tow))k------------------.-;-------- ≤ 2------------Z~ʌ----------, (C.111)
toqn (tow)	toqn (tow)
Where at the last inequality We have used ProPosition 3.7. Continuing the calculation, We further have
kqn (t0w) q (t0w) - enk	to kwk j /	「 II
-------7__=-ʌ	 =	≤ √n kwk ≤ √n kq - enk ,	(C.112)
toqn (tow)--------------------------------------------------------------------toqn (tow)
comPleting the Proof.
Proposition C.8. Assume θ ∈ [1/n, 1/2]. When m ≥ Cθ-2nlogn, with probability at least
1 — exp (-cmθ3 log-1 m), thefollowing holds: for all q ∈ S!；+) satisfying f (q) — f (en) ≤ 25θ,
f (q) - f (en) ≥ 总θ (1 - θ) kq-nk ≥ ɪθ (1 - θ) kq - enk .	(C.113)
16	16
Here C, c > 0 are universal constants.
Proof. We first establish uniform convergence of f (p) to E [f] (p). Consider the zero-centered
random Process Xp =. f (p) - E [f] (p) on Sn-1. Similar to Proof of ProPosition C.6, We can shoW
that for all p, q ∈ Sn-1
C
kXP - Xq kψ2 ≤ √m kp - qk .	(C.114)
APPlying ProPosition A.2 gives that
kf (p) - E[f](q)k≤ ɪθ ∀ q ∈ SnT	(C.115)
With Probability at least 1 - exP -cmθ2 , Provided that m ≥ Cθ-2n.
NoW We consider E [f] (q) - E [f] (en). For convenience, We first Work in the w sPace and note that
E [f] (q) - E [f] (en) = E [g] (w (q)) - E [g] (0). By Lemma B.3, E [g] is monotonically increasing
in every radial direction v until kwk2+ kwk2∞ ≤ 1, Which imPlies that
inf E [g] (w (q)) -E[g](0) = inf	E [g] (w (q)) -E[g](0).	(C.116)
kwk≥1∕2	kwk = 1∕2
5It is Possible to directly aPPly the manifold version of Lebourg’s mean value theorem, i.e., Theorem 3.3
of Hosseini & Pouryayevali (2011). We avoid this technicality by Working With the Euclidean version in w
sPace.
27
Published as a conference paper at ICLR 2019
For w with kwk = 1/2,
E[g](w) - E[g](0) = (1 - θ) Eω ∣wω∣ + ΘEω，1 - ∣wωc ∣2 - θ (Lemma B.1)	(C.117)
≥ (1 - θ) θ kw∣ + ΘEωVZi -kwk2 - θ
(C.118)
≥
≥
1 θ+1。-θ
4	2
ɪ θ.
10
(using θ ≤ 1/2 and kwk = 1/2)
(C.119)
(C.120)
So, back to the q space,
inf
q∈S(n + ): kq-nk≥1∕2
E[f](q)- E[f](0) ≥ 110θ.
(C.121)
Combining the results in Eq. (C.115) and Eq. (C.121), we conclude that with high probability
q∈s(n+)inf-nk≥ι∕2f (q) - f ⑼ ≥ 25 θ.
(C.122)
So when f (q) - f (0) ≤ 2/25 ∙ θ, kq-nk ≤ 1/2, which is equivalent to ∣∣wk ≤ 1/2 in the W space.
Under this constraint, by Lemma B.3,
D-w/kwk E [g] (w) ≤-θ (1 - θ)
1+ kwk2∞ /kwk2 - kwk
(C.123)
≤ -θ(1 -θ)
≤ - 5 θ (1 - θ).
(C.124)
So, emulating the proof of Eq. (3.9) in Theorem 3.4, we have that for q ∈ Sζ(n+) with ∣q-n∣ ≤ 1/2,
hE[∂κf](q) ,qnq - eni = qn (E[∂g](w), Wi ≥ qn ∣∣w∣ ∙ 1 θ (1 - θ) ≥ *θ (1 - θ) IlwIl ,
5	10
(C.125)
where at the last inequality We use qn = y 1 - Ilw『 ≥ √3∕2 when ∣∣w∣ ≤ 1/2. Moreover, We
emulate the proof of Eq. (3.13) in Theorem 3.6 to obtain that
inf hdRf (q), qnq - eni ≥√√2θ(1 - θ)kq-nk ≥ 116θ (I- θ) kq - enk
(C.126)
with probability at least 1 - exp -cmθ3 log-1 m , provided that m ≥ Cθ-2n log n.
The last step of our proof is invoking the mean value theorem, similar to the proof of Proposition C.7.
For any q, we have
f(q) -f(en) =g(w) -g(0) = hv, wi
for a certain t ∈ (0, 1) and a certain v ∈ ∂g (tw). We have
(C.127)
hv, wi≥ ɪ inf h∂g (tow), towi
t0
ɪ—1~~TinfhdRf (q (tow)) , qn (tow) q (tow) - eni
to qn (tow)
(C.128)
1 qn⅛)fθ (I-θ)ktowk
看θ (1 - θ) kwk
(C.129)
(C.130)
completing the proof.
≥ 16θ (1 - θ) kq - enk ,
(C.131)
≥
≥
28
Published as a conference paper at ICLR 2019
D Proofs for Section 3.3
D.1 STAYING IN THE REGION Sζ(n+)
Lemma D.1 (Progress in S[；+) \ S(n+)). Set η = to∕(100√n) for to ∈ (0,1). Forany Zo ∈ (0,1),
on the good events stated in Proposition 3.7 and Theorem 3.6, we have for all q ∈ Sζ(n+) \ S1(n+)
and q+ being the next step of Riemannian subgradient descent that
q+,n	≥	qn	A + 十 θ (I - θ) G A
kq+,-nk∞ ^ kq-nk∞ V	400n3∕2 (1 +Zo) J
(D.1)
In particular, we have q+ ∈ Sζ(n+)
Proof. We divide the index set [n - 1] into three sets
Io =. {j	∈	[n- 1]	: qj = 0},	(D.2)
Ii = {j	∈	[n — 1]	： q；属 > 1 +	2Zι	= 3,qj	= 0}	(D.3)
I = {j	∈	[n — 1]	： q；/q2 ≤ 1 +	2Q	= 3} .	(D.4)
We perform different arguments on different sets. We let g (q) ∈ ∂Rf (q) be the subgradient taken at
q and note by Proposition 3.7 that kgk ≤ 2, and so |gi | ≤ 2 for all i ∈ [n]. We have
q+,； = (q； - ng；)2 /Iq - ηgk2 = (q； - ng；)2
q+ ,j (qj — ηgj )2/kq — ηgk2 (qj — ηgj )2
For any j ∈ Io,
q+,； = (q； - ηg;)2 = 2(1 - ηg"q;)2 ≥ (1 - 2η√n)2
q+ ,j	η2gj	q；	η2gj	—	4nη2
Provided that η ≤ 1∕(4√n), 1 一 2η√n ≥ 1/2, and so
(1 — 2η√n)2 ≥	≥ 5
4nη2	— 16nη2 — 2,
where the last inequality holds when η ≤ 1/√40n.
(D.5)
(D.6)
(D.7)
For any j ∈ I1 ,
q+,；、q；(1 — Ingniqn)、q；(1 — Ingniqn) _ 3 (I- ηg；/q；)2、3(1 — 2η√n)2、5
≥	≥	=	≥	≥
q+j ~ q + η2gj	-	q；/3 + 4η2	1 + 12η2∕qn -	1 + 12nη2	- 2'
(D.8)
where the very last inequality holds when η ≤ 1/(26 √n).
Since q ∈ s!；+) \ S(；+), I2 is nonempty. For any j ∈ I2,
q+,n _ qn	gj ∕qj - gn∕qn "
q+j =飞 C + η 1 - ηgj/q3 )
(D.9)
Since gj/q§ ≤ 2√3n, 1 — ηgj/qj ≥ 1/2 when η ≤ 1/(4√3n). Conditioned on this and due to that
gj/qj — g；/q； ≥ 0, it follows
1+ηgj/qg；/q； A ≤ [1 + 2η (gj/qj —g；/q；)]2 ≤ ∣^1 + 2η (2√3n+2√n)] ≤ (1 + 11η√n)2
1 — ηgj /qj
(D.10)
29
Published as a conference paper at ICLR 2019
Ifqn2/qj2 ≤ 2, q+2 ,n/q+2 ,j ≤ 5/2 provided that
(1 + ∏η√n)2 ≤ 5/2 = 4
1
η ≤-----.
/ 一 100√n
(D.11)
As q ∈/ S1(n+), we have qn2 /kq-n k2∞ ≤ 2, so there must be a certain j ∈ I2 satisfying qn2 /qj2 ≤ 2.
We conclude that when
η ≤ min{ √40n, 26√n, ≡√n} = ≡√n
the index of largest entries of q+,-n remains in I2 .
(D.12)
On the other hand, when η ≤ 1∕(100√n), for all j ∈I,
(1 + ηjj )2 ≥ [1+ η 也 ∕qj - gn∕qn)]2 ≥ (1 + 4nθ …言Z01 。⑶
So when η = t∕(100√n) for any t ∈ (0,1),
q+,n	≥ 碌 (1 +1 θ(1-θ)Z0	∖
kq+,-nk∞ — kq-nk∞ V	400n3/2 (I + ζ0))
(D.14)
completing the proof.
Proposition D.2. For any ζ0 ∈ (0, 1), on the good events stated in Proposition 3.7 and Theorem 3.6,
if the step sizes satisfy
for all k,
(D.15)
the iteration sequence will stay in Sζ(n+) provided that our initialization q(0) ∈ Sζ(n+).
Proof. By Lemma D.1, if the current iterate q ∈ Sζ(n+) \ S1(n+), the next iterate q+ ∈ Sζ(n+),
provided that η ≤ 1∕(100√n). Now if the current q ∈ S(n+), i.e.,德∕qj2 ≥ 2 for all j ∈ [n - 1], we
can emulate the analysis of the set I1 in proof of Lemma D.1. Indeed, for any j ∈ [n - 1],
q+ ,n ≥ qn(1 - ηg"qn)2
q+j ― -q + η2g2-
≥ qn(1 - 2η√n)2 ≥ 2(1 - 2n√n)2 ≥ I
— qn ∕2 + 4η2	—	1 + 8nη2	—
(D.16)
where the last inequality holds provided that η ≤ (1 - Z0)，(9四).Combining the two cases finishes
the proof.
D.2 Proof of Theorem 3.8
As we have η(k) ≤ ιo0√n and q⑼ ∈ S!；+), the entire sequence {q(k)}k≥o will stay in S[；+)
by Proposition D.2.
For any q and any v ∈ ∂Rf (q), we have hv, qi = 0 and therefore
kq - ηvk2 = kqk2 +η2 kvk2 ≥ 1.	(D.17)
So q - ηv is not inside Bn . Since projection onto Bn is a contraction, we have
kq+ - en k2
q - ηv
l∣q - ηvk	en
2
≤ lq -ηv - enl2
≤ kq - e；『+ η2 kv∣2 -2η (v, q - e；i ≤ ∣∣q - e；『+4η2 -1 ηθ (1 - θ) n-3/Z ∣∣q - e；k ,
8
(D.18)
30
Published as a conference paper at ICLR 2019
where we have used the bounds in Proposition 3.7 and Theorem 3.6 to obtain the last inequality.
Further applying Proposition C.7, we have
kq+ - enk2 ≤ kq - enk2+ 4η2 - 116ηθ (1-θ) n-2ζ0 (f (q) - f(en)).
Summing up the inequalities until step K (assumed ≥ 5), we have
0 ≤ q(K) - en	+ 4
j=0
K
X η(j) f q(j) - f (en) ≤
j=0
K
1 - θ) n-2ζ0 X η(j) f q(j) -f(en)
j =0
16 IIq(K)- en∣∣2 + 64 PK=O (ηCj))2
16 q(K) -
θ(1-θ)n-2ζ0
en∣∣2 +64 PK=O (η⑶)2
Substituting the following estimates
°	K t-2α dt! ≤ 二--------1— (K 1-2α + 1),
104n 1 - 2α	,
j=0
K0
Xη(j)≥
j =0
and noting 16 q(K ) - en
α
1	K1-α
dt ≥ 102√n 1 - α,
II2 ≤ 32, we have
3200n5/2 (1 - α) + 16/25 ∙ n3/2 (1⅛K 1-2α + 1 - a
:	θ(1 - θ) ζ0κ1-α
Noting that
K≥
1
6400n5/2 (1 - α)∖ 1-α
θ(1-θ)ζ0
3200n5/2 (1 - α) e
θ (1 - θ) Z0K1-α ≤ 2,
(D.19)
(D.20)
(D.21)
(D.22)
(D.23)
(D.24)
(D.25)
(D.26)
and when K ≥ 1, K1-2α ≥ 1, yielding that
6 64n3S1⅛ ! 1	32n*21⅛K-α	e	16n3/2 Y1 - α)(A K 1-2α+ 1),e
≥ 125eθ (1 -	θ) Z0	=⇒	25θ(1 -	θ)	Z0	≤ 2	=⇒	25θ (1 - θ) Z0K1-α	≤ 2
(D.27)
So we conclude that when
f/6400n5/2 (1 - a)ʌ 1-α	64n^21⅛
≥ max I ( θ(1-θ) Z0e	)	, "θ(1-θ) Z0
f (qbest) - f (en) ≤ e. When this happens, by Proposition C.8,
(D.28)
Iqbest - en
16
≤ θ (1 - θ)e.
(D.29)
Plugging in the choice Z0 = 1/(5 log n) in Eq. (D.28) gives the desired bound on the number of
iterations.
E Proofs for Section 3.4
E.1 Proof of Lemma 3.9
Lemma E.1. For all n ≥ 3 and Z ≥ 0, it holds that
Vol K+)) > ɪ
Vol(SnT) ≥ 2n
9 log n ζ
8n .
—
(E.1)
31
Published as a conference paper at ICLR 2019
We note that a similar result appears in (Gilboa et al., 2018) but our definitions of the region Sζ are
slightly different. For completeness we provide a proof in Lemma F.3.
We now prove Lemma 3.9. Taking ζ = 1/(5 log n) in Lemma E.1, we obtain
vol (S1/(5)iog n))〉1	9 log n 1〉1
Vol(Sn-1)	— 2n	8 n 5 log n — 4n
(E.2)
By symmetry, all the 2n sets
at least 1∕(4n). As q(0)〜
nS (i+)
S1/(5 logn),
log n)
[n]
have the same volume which is
Uniform(Sn-1), it falls into their union with probability at least
2n ∙ 1∕(4n) = 1∕2,on which it belongs to a uniformly random one of these 2n sets.
:i ∈
E.2 Proof of Theorem 3.10
Assume that the good event in Proposition 3.7 happens and that in Theorem 3.6 happens to all the 2n
sets S1(i/+(5)logn),S1(i/-(5)logn) : i ∈ [n] , which by setting ζ0 = 1∕(5 log n) has probability at least
1 - exp(-cmθ3ζ02n-3 log-1 m) - exp(-cmθ log-1 m) = 1 - exp(-c0mθ3n-3 log m-3). (E.3)
By Lemma 3.9, random initialization will fall these 2n sets with probability at least 1∕2. When it
falls in one of these 2n sets, by Theorem 3.8, one run of the algorithm will find a signed standard
basis vector up to e accuracy. With R independent runs, at least S = 4 R of them are effective with
probability at least 1 - exp (-(R∕4)2∕(R∕4 ∙ 2)) = 1 - exp (-R∕8), due to Bernstein,s inequality.
After these effective runs, the probability any standard basis vector is missed (up to sign) is bounded
by
n(1 - n) ≤ exp (-S + log n) ≤ exp (-2S-) ,	(E.4)
where the second inequality holds whenever S ≥ 2n log n.
F Auxiliary calculations
Lemma F.1. For X ~ BG(θ), ∣∣x∣∣ψ2 ≤ Ca. For any vector U ∈ Rn and X ~αd BG(θ),
x>uψ2 ≤ Cb kuk. HereCa , Cb ≥ 0 are universal constants.
Proof. For any λ ∈ R,
exp (λx) = θ exp (λx) ≤ exp (λx) .	(F.1)
So kxkψ is bounded by a universal constant. Moreover,
u>xψ2 = Xuixi	≤ C1 Xui2 kxik2ψ2!	≤ C2 kuk ,	(F.2)
i	ψ2	i
as claimed.
Lemma F.2. Let aι,..., am, be iid copies of a ~αd BG(θ). Then,
> Ca√mn + Cb√mt ≤ 2 exp (-12) .	(F.3)
for any t ≥ 0. Here Ca , Cb ≥ 0 are universal constants.
Proof. Consider the zero-centered random process defined on Sn-1:	Xq	=.
Pi∈[m] (∣q>Xi∣ - E Iq>XI). Then, for any p, q ∈ Sn-1,
P sup	q>xi - E q>x
q∈Sn-1 ∣i∈[m]
kXp - Xqllψ2 = E (∣p>xi∣ - ∣q>xi∣ - E∣p>x∣ + E∣q>x∣)
i∈[m]	ψ2
(F.4)
32
Published as a conference paper at ICLR 2019
≤ C1 I X IllPTxiI-IqTxil- ElPTxl + ElqTxMψ2 )	(F.5)
i∈[m]
≤ C2 I X Il IPTxi ITqTxi l ∣∣ψ21	(Centering)	(Fs)
\ie[m]
1/2
(F.7)
=C3√m ∣∣p - qk,	(f.8)
≤ C2 (XJI(P .TL
where We use the estimate in Lemma F.1 to obtain the last inequality. Note that Xq is a mean-zero
random process, and We can invoke Proposition A.2 with W(SnT) = C4√n and rad (SnT) = 2 to
get the claimed result.	■
Lemma F.3. For all n ≥ 3 and ζ ≥ 0, it holds that
vol (S")一
vol(Sn-1) ≥ 2n
9 log n C
8 n .
(F.9)
—
Proof. We have
vol (S,+))
Vθl (Sn_1 ) = Pq〜UnifOrm(Sn-1) [成 ≥ (1 + C)kq-nk∞ ,qn ≥0
=Px〜N(0,In) Ixn ≥ 0, 一 ≥ (1 + Q x2 ∀ i = n]
∞	/n-1 rχn∕√1+ζ	∖
=(2π)n∕2	e-xn/2 Y	e-xj/2 dxj	dxn
0	j=1 ∕-xn/√1 + ζ
= (2n)1/2/ e-xn∕2ψnT ./pl + ζ) dxn
=√1+ C /∞ e-(1+Z)X2∕2ψn-1 (x) dx = h (ζ) > 0,
V 2π Jo
(F.10)
(F.11)
(F.12)
(F.13)
(F.14)
where we write ψ(t) = √2∏ /「exp (-s2∕2) ds. Now we derive a lower bound of the volume ratio
by considering a first-order Taylor expansion of the last equation around Z = 0 (as we are mostly
interested in small Z). By symmetry, h (0) = 1∕(2n). Moreover, we have
∂h(Z)l
∂ l ζ=o
(x) dx — √= / e-χ2∕2x2ψn-1 (x) dx
------Z= / e-χ2∕2x2ψn-1 (x) dx.
4n	2√2∏ Jo
Now we provide an upper bound for the second term of the last equation. Note that
1	f∞	一2 /0
√2∏ J	e	/2x2ψ 1 (x) dx = Ex〜N(0,In)IXn1 {xn ≥ kx-n∣∣∞} 1 {xn ≥ 0}]
2nEx〜N(0,In) llx∣∞ .
(F.15)
(F.16)
(F.17)
(F.18)
Now for any λ ∈ (0,1 /2),
n
exp (λE∣∣xk∞) ≤ Eexp (λ ∣∣x∣∣∞) ≤ XEexp (λx2) = nE,〜N(o,1) exp (λx2)
j=1
n
≤ ,	.
一 √1 - 2λ
(F.19)
33
Published as a conference paper at ICLR 2019
Taking logarithm on both sides, rearranging the terms, and setting λ = 1/4, we obtain
Ekxk∞ ≤ λ∈ini∕2)
log n + 1 log (1 - 2λ)-1
≤ 4 log n + 2 log 2.
(F.20)
λ
So
∂h (Z) I	1	1∕∣	1	、	9 log n
FrL ≥ 4n - 4n(	gn g2) ≥ -8丁
(F.21)
provided that n ≥ 3. Now We show that h (Z) ≥ h (0) + h0(0)Z by showing that h00(Z) ≥ 0. We have
∞
0
∂2h (Z) _ √ι+τ
4√2∏
x4
2x2
1 + ζ - (1 + Z)2
e- 1+ζx2ψn-1 (x) dx.
(F.22)
—
1
Using integration by part, we have
∞
0
—
4	3x2
X - 1÷?
e- 1+ζx2 ψn-1 (x) dx
(F.23)
1	- 1+ζ χ2 3	/ n-1 /、
1+^-	2	X3 ∙ ψ 1 (x)
∞	∞ 1	1+ζ D …	C	∕^2^ i2
+ J	ɪ-p^---X x3 (n - 1) ψn-2 (x) N---r dx
(F.24)
∞
0
1	_ ι+ζ χ2
——-2 X
1+Z
2	x2
x3 (n — 1) ψ 2 (x) d -- 2 dx ≥ 0,
(F.25)
and similarly
∞
0
x2
1
1+?
e- 1+ζx2 ψn-1 (x) dx
(F.26)
1
1+? e
-1+Z χ2
:2 X X
・ ψn-1(x)
∞	∞1
0+0	1+?-
- 1++ζx2 x (n - 1) ψn-2 (x) ,2-
χ2
ɪ
dx
(F.27)
∞	1	- 1+ζ χ2 / I、∕n-2∕、/2
=	TTV-	2 X (n - 1) ψ 2 (x) ∖ --
0	1+Z	π
Noting that
x2
2
dx ≥ 0.
(F.28)
x4 -
2x2
E - (1 + ζ)2
x4
3x2
1+Z
+1⅛ (x2-
1+Z
(F.29)
—

—
1
—

1
and combining the above integral results, we conclude that h00 (Z) ≥ 0 and complete the proof. ■
Lemma F.4. Let (x1, y1), (x2, y2) ∈ R2>0 be two points in the first quadrant satisfying y1 ≥ x1 and
y2 ≥ x2, and y2/X2 ∈ [1,1 + η] for some η ≤ 1, then we have / ((xι,yι), (x2,y2)) ≤ η∙
Proof. For i = 1, 2, let θi be the angle between the ray (xi, yi) and the x-axis. Our assumption
implies that θi ∈ [∏∕4,∏∕2) and θ2 ≥ θι, thus / ((χι,yι), (x2,y2)) = θ2 - θι, so we have
tanZ ((x1,y1), (x2,y2))
tan θ2 - tan θ1
1 + tan θ2 tan θ1
y2∕x2 - y1∕x1
1 + y2y1/(x2x1)
y2/x2 _ 1
=	IyIlX______
y2∕x2 + x1∕y1
≤ y2∕x2 - 1
一y1∕x1
≤ η.
(F.30)
Therefore L ((xι,yι), (x2,y2)) ≤ arctan(η) ≤ η.
34
Published as a conference paper at ICLR 2019
Lemma F.5. For any p, q ∈ Sn-I with the same support pattern such that dE(p, q) ≤ e ≤ 1, we
have for all u ∈ Sn-1 that
Ex〜BG(θ) [∣u›x∣l {sign(p>x) = sign(q>x)}] ≤ 3e Jog；.
(F.31)
Proof. Fix some threshold t > 0 to be determined. We have
E [∣u>x∣l {sign(p>x) = sign(q>x)}]
≤ E [∣u>x∣l {∣u>x∣> t}] + E [∣u>x∣l {∣u>x∣≤ t,Sign(P>x) = sign(q>x)}]
≤ (E [(u>x)2] ∙ P [∣u>x∣> t])" + tE [1 {sign(p>x) = sign(q>x)}]
≤ (θ ∙ 2exp(-12∕2))1/2 + et.
(F.32)
(F.33)
(F.34)
(F.35)
The second to last inequality uses Cauchy-Schwarz, and the last inequality uses the fact that u>x
u>xω is ∣∣uω∣∣2-sub-Gaussian conditioned on Ω and thus 1-sub-Gaussian marginally. Taking t
y 2 log 表,the above bound simplifies to
j2θ exp (— log
2log e1- = √2θe(1 + log!
where we have used θ ≤ 1/2 and ≤ 1/2.
2log≤ 3e jog；
(F.36)

G Results on orthogonal dictionaries
O- .1- O- .1- O-
O- .1- O- .1- -I-
0.5	1	1.5	2
-→-n=30
-∙-n=50
n=70
n=100
-→-n=30
n=50
n=70
—i—n=100
0.2
0.1
→-n=30
n=50
n=70
—n=100
number of measurements (as power of n)
2.5	0.5
1	1.5	2
number of measurements (as power of n)
1	1.5	2	2.5
number of measurements (as power of n)
( ( ( ( (
Figure 2: Empirical success rates of recovery of the Riemannian subgradient descent with R = 5n log n runs,
averaged over 10 instances. Left to right: orthogonal dictionaries with θ = 0.1, 0.3, 0.5.
H Faster alternative algorithm for large-scale instances
The Riemannian subgradient descent is cheap per iteration but slow in overall convergence, similar
to many other first-order methods. We also test a faster quasi-Newton type method, GRANSO,6
that employs BFGS for solving constrained nonsmooth problems based on sequential quadratic
optimization (Curtis et al., 2017). For a large dictionary of dimension n = 400 and sample complexity
m = 10n2 (i.e., 1.6 × 106), GRANSO successfully identifies a basis after 1500 iterations with CPU
time 4 hours on a two-socket Intel Xeon E5-2640v4 processor (10-core Broadwell, 2.40 GHz)—this
is approximately 10× faster than the Riemannian subgradient descent method, showing the potential
of quasi-Newton type methods for solving large-scale problems.
I Experiment with images
To experiment with images, we follow a typical setup for dictionary learning as used in image
processing (Mairal et al., 2014). We focus on testing if complete (i.e., square and invertible)
dictionaries are reasonable sparsification bases for real images, instead on any particular image
processing or vision tasks.
6Available online: http://www.timmitchell.com/software/GRANSO/.
35
Published as a conference paper at ICLR 2019
国琥立年期艰』势
mi3⅛- w≡⅛⅛
-,√ 7fiW√ SJ⅛⅞⅛
9QHaE9!⅞Ka
⅛κ-更SGir仁非:
由监空源物坦其U
八一一ll>==≡∙
lh>∙π■一己 r
=≡，/= _=B!±
Xb.'≡ 一■■"■孔
≡ _JjΛL1ll∙ -
h-==xmjτ
⅛>=mlLl∙≡
0.5
1.5
0
-40	-30	-20	-10	0	10	20	30	40
Figure 3: Results on two images. First row: the images; Second row: learned dictionaries; Third
row: histograms of the representation coefficients; Fourth row: zoomed-in versions of the histograms
around zero.
×104
36
Published as a conference paper at ICLR 2019
Setup Two natural images are picked for this experiment, as shown in the first row of Fig. 3, each
of resolution 512 × 512. Each image is divided into 8 × 8 non-overlapping blocks, resulting in
64 × 64 = 4096 blocks. The blocks are then vectorized, and stacked columnwise into a data matrix
Y ∈ R64×4096 . We precondition the data to obtain
Y = (YY>)-1∕2 Y,	(I.1)
so that nonvanishing singular values of Y are identically one. We then solve formulation (1.1)
round (5n log n) times with n = 64 using the BFGS solver based on GRANSO, obtaining
round (5n log n) vectors. Negative equivalent copies are pruned and vectors with large correla-
tions with other remaining vectors are sequentially removed until only 64 vectors are left. This forms
the final complete dictionary.
Results The learned complete dictionaries for the two test images are displayed in the second row
of Fig. 3. Visually, the dictionaries seem reasonably adaptive to the image contents: for the left
image with prevalent sharp edges, the learned dictionary consists of almost exclusively oriented sharp
corners and edges, while for the right image with blurred textures and occasional sharp features,
the learned dictionary does seem to be composed of the two kinds of elements. Let the learned
dictionary be A. We estimate the representation coefficients as ATY. The third row of Fig. 3
contains the histograms of the coefficients. For both images, the coefficients are sharply concentrated
around zero (see also the fourth row for zoomed versions of the portions around zero), and the
distribution resembles a typical zero-centered Laplace distribution—which is a good indication of
sparsity. Quantitatively, we calculate the mean sparsity level of the coefficient vectors (i.e., columns
of ATY) by the metric 2% / kJb: for a vector V ∈ Rn, kv% / ∣∣v∣∣2 ranges from 1 (when V is
one-sparse) to √n (when V is fully dense with elements of equal magnitudes), which serves as a
good measure of sparsity level for v. For our two images, the sparsity levels by the norm-ratio metric
are 5.9135 and 6.4339, respectively, while the fully dense extreme would have a value √64 = 8,
suggesting the complete dictionaries we learned are reasonable sparsification bases for the two natural
images, respectively.
37