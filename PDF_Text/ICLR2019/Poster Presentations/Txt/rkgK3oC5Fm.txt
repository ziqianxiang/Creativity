Published as a conference paper at ICLR 2019
Bayesian Prediction of Future Street Scenes
using Synthetic Likelihoods.
Apratim Bhattacharyya, Mario Fritz, Bernt Schiele
Max Planck Institute for Informatics, Saariand Informatics Campus, Saarbrucken, Germany
{abhattac, mfritz, schiele}@mpi-inf.mpg.de
Abstract
For autonomous agents to successfully operate in the real world, the ability to
anticipate future scene states is a key competence. In real-world scenarios, future
states become increasingly uncertain and multi-modal, particularly on long time
horizons. Dropout based Bayesian inference provides a computationally tractable,
theoretically well grounded approach to learn likely hypotheses/models to deal
with uncertain futures and make predictions that correspond well to observations 一
are well calibrated. However, it turns out that such approaches fall short to capture
complex real-world scenes, even falling behind in accuracy when compared to the
plain deterministic approaches. This is because the used log-likelihood estimate
discourages diversity. In this work, we propose a novel Bayesian formulation for
anticipating future scene states which leverages synthetic likelihoods that encour-
age the learning of diverse models to accurately capture the multi-modal nature of
future scene states. We show that our approach achieves accurate state-of-the-art
predictions and calibrated probabilities through extensive experiments for scene an-
ticipation on Cityscapes dataset. Moreover, we show that our approach generalizes
across diverse tasks such as digit generation and precipitation forecasting.
1 Introduction
The ability to anticipate future scene states which involves map-
ping one scene state to likely future states under uncertainty is
key for autonomous agents to successfully operate in the real
world e.g., to anticipate the movements of pedestrians and vehi-
cles for autonomous vehicles. The future states of street scenes
are inherently uncertain and the distribution of outcomes is often
multi-modal. This is especially true for important classes like
pedestrians. Recent works on anticipating street scenes (Luc et al.,
2017; Jin et al., 2017; Seyed et al., 2018) do not systematically
consider uncertainty.
Bayesian inference provides a theoretically well founded ap-
proach to capture both model and observation uncertainty but
with considerable computational overhead. A recently proposed
approach (Gal & Ghahramani, 2016b; Kendall & Gal, 2017) uses
dropout to represent the posterior distribution of models and cap-
ture model uncertainty. This approach has enabled Bayesian
inference with deep neural networks without additional computa-
tional overhead. Moreover, it allows the use of any existing deep
neural network architecture with minor changes.
Figure 1: Blue: Groundtruth distri-
bution. Black: Models sampled at
random from the model distribution.
However, when the underlying data distribution is multimodal and the model set under consideration
do not have explicit latent state/variables (as most popular deep deep neural network architectures),
the approach of Gal & Ghahramani (2016b); Kendall & Gal (2017) is unable to recover the true
model uncertainty (see Figure 1 and Osband (2016)). This is because this approach is known to
conflate risk and uncertainty (Osband, 2016). This limits the accuracy of the models over a plain
deterministic (non-Bayesian) approach. The main cause is the data log-likelihood maximization step
1
Published as a conference paper at ICLR 2019
during optimization - for every data point the average likelihood assigned by all models is maximized.
This forces every model to explain every data point well, pushing every model in the distribution
to the mean. We address this problem through an objective leveraging synthetic likelihoods (Wood,
2010; Rosca et al., 2017) which relaxes the constraint on every model to explain every data point,
thus encouraging diversity in the learned models to deal with multi-modality.
In this work: 1. We develop the first Bayesian approach to anticipate the multi-modal future of
street scenes and demonstrate state-of-the-art accuracy on the diverse Cityscapes dataset without
compromising on calibrated probabilities, 2. We propose a novel optimization scheme for dropout
based Bayesian inference using synthetic likelihoods to encourage diversity and accurately capture
model uncertainty, 3. Finally, we show that our approach is not limited to street scenes and generalizes
across diverse tasks such as digit generation and precipitation forecasting.
2	Related work
Bayesian deep learning. Most popular deep learning models do not model uncertainty, only a
mean model is learned. Bayesian methods (MacKay, 1992; Neal, 2012) on the other hand learn the
posterior distribution of likely models. However, inference of the model posterior is computationally
expensive. In (Gal & Ghahramani, 2016b) this problem is tackled using variational inference with
an approximate Bernoulli distribution on the weights and the equivalence to dropout training is
shown. This method is further extended to convolutional neural networks in (Gal & Ghahramani,
2016a). In (Kendall & Gal, 2017) this method is extended to tackle both model and observation
uncertainty through heteroscedastic regression. The proposed method achieves state of the art results
on segmentation estimation and depth regression tasks. This framework is used in Bhattacharyya
et al. (2018a) to estimate future pedestrian trajectories. In contrast, Saatci & Wilson (2017) propose
a (unconditional) Bayesian GAN framework for image generation using Hamiltonian Monte-Carlo
based optimization with limited success. Moreover, conditional variants of GANs (Mirza & Osindero,
2014) are known to be especially prone to mode collapse. Therefore, we choose a dropout based
Bayesian scheme and improve upon it through the use of synthetic likelihoods to tackle the issues
with model uncertainty mentioned in the introduction.
Structured output prediction. Stochastic feedforward neural networks (SFNN) and conditional
variational autoencoders (CVAE) have also shown success in modeling multimodal conditional
distributions. SFNNs are difficult to optimize on large datasets (Tang & Salakhutdinov, 2013) due
to the binary stochastic variables. Although there has been significant effort in improving training
efficiency (Rezende et al., 2014; Gu et al., 2016), success has been partial. In contrast, CVAEs (Sohn
et al., 2015) assume Gaussian stochastic variables, which are easier to optimize on large datasets
using the re-parameterization trick. CVAEs have been successfully applied on a large variety of
tasks, include conditional image generation (Bao et al., 2017), next frame synthesis (Xue et al., 2016),
video generation (Babaeizadeh et al., 2018; Denton & Fergus, 2018), trajectory prediction (Lee et al.,
2017) among others. The basic CVAE framework is improved upon in (Bhattacharyya et al., 2018b)
through the use of a multiple-sample objective. However, in comparison to Bayesian methods, careful
architecture selection is required and experimental evidence of uncertainty calibration is missing.
Calibrated uncertainties are important for autonomous/assisted driving, as users need to be able to
express trust in the predictions for effective decision making. Therefore, we also adopt a Bayesian
approach over SFNN or CVAE approaches.
Anticipation future scene scenes. In (Luc et al., 2017) the first method for predicting future scene
segmentations has been proposed. Their model is fully convolutional with prediction at multiple
scales and is trained auto-regressively. Jin et al. (2017) improves upon this through the joint prediction
of future scene segmentation and optical flow. Similar to Luc et al. (2017) a fully convolutional
model is proposed, but the proposed model is based on the Resnet-101 (He et al., 2016) and has a
single prediction scale. More recently, Luc et al. (2018) has extended the model of Luc et al. (2017)
to the related task of future instance segmentation prediction. These methods achieve promising
results and establish the competence of fully convolutional models. In (Seyed et al., 2018) a
Convolutional LSTM based model is proposed, further improving short-term results over Jin et al.
(2017). However, fully convolutional architectures have performed well at a variety of related tasks,
including segmentation estimation (Yu & Koltun, 2016; Zhao et al., 2017), RGB frame prediction
2
Published as a conference paper at ICLR 2019
(Mathieu et al., 2016; Babaeizadeh et al., 2018) among others. Therefore, we adopt a standard ResNet
based fully-convolutional architecture, while providing a full Bayesian treatment.
3	Bayesian models for prediction under uncertainty
We phrase our models in a Bayesian framework, to jointly capture model (epistemic) and observation
(aleatoric) uncertainty (Kendall & Gal, 2017). We begin with model uncertainty.
3.1	Model uncertainty
Let x ∈ X be the input (past) and y ∈ Y be the corresponding outcomes. Consider f : x 7→ y, we
capture model uncertainty by learning the distribution p(f |X, Y) of generative models f, likely to
have generated our data {X, Y}. The complete predictive distribution of outcomes y is obtained by
marginalizing over the posterior distribution,
p(y|x, X, Y) =	p(y|x, f)p(f |X, Y)df .
(1)
However, the integral in (1) is intractable. But, we can approximate it in two steps (Gal & Ghahramani,
2016b). First, we assume that our models can be described by a finite set of variables ω. Thus, we
constrain the set of possible models to ones that can be described with ω. Now, (1) is equivalently,
p(y|x, X, Y)
/p(y∣x,ω)p(ω∣X, Y)dω .
(2)
Second, we assume an approximating variational distribution q(ω) of models which allows for
efficient sampling. This results in the approximate distribution,
p(y|x, X, Y) ≈ p(y|x) =
p(y|x, ω)q(ω)dω .
(3)
For convolutional models, Gal & Ghahramani (2016a) proposed a Bernoulli variational distribution
defined over each convolutional patch. The number of possible models is exponential in the number
of patches. This number could be very large, making it difficult optimize over this very large set of
models. In contrast, in our approach (4), the number possible models is exponential in the number of
weight parameters, a much smaller number. In detail, we choose the set of convolutional kernels and
the biases {(W1, b1), . . . , (WL, bL)} ∈ W of our model as the set of variables ω. Then, we define
the following novel approximating Bernoulli variational distribution q(ω) independently over each
element wki,0j,k (correspondingly bk) of the kernels and the biases at spatial locations {i, j},
q(WK) = MK	ZK
zki,0j,k = Bernoulli(pK), k0 = 1, . . . , |K0|, k = 1, . . . , |K| .
(4)
Note, denotes the hadamard product, Mk are tuneable variational parameters, zki,0j,k ∈ ZK are
the independent Bernoulli variables, pK is a probability tensor equal to the size of the (bias) layer,
|K| (|K0|) is the number of kernels in the current (previous) layer. Here, pK is chosen manually.
Moreover, in contrast to Gal & Ghahramani (2016a), the same (sampled) kernel is applied at each
spatial location leading to the detection of the same features at varying spatial locations. Next, we
describe how we capture observation uncertainty.
3.2	Observation uncertainty
Observation uncertainty can be captured by assuming an appropriate distribution of observation noise
and predicting the sufficient statistics of the distribution (Kendall & Gal, 2017). Here, we assume a
Gaussian distribution with diagonal covariance matrix at each pixel and predict the mean vector μi,j
and co-variance matrix σi,j of the distribution. In detail, the predictive distribution of a generative
model draw from ω 〜q(ω) at a pixel position {i,j} is,
Pij (y∣x,ω) = N((μ 叼 x,ω), (σij∣x,ω)).	(5)
3
Published as a conference paper at ICLR 2019
We can sample from the predictive distribution p(y|x) (3) by first sampling the weight matrices ω
from (4) and then sampling from the Gaussian distribution in (5). We perform the last step by the
linear transformation of a zero mean unit diagonal variance Gaussian, ensuring differentiability,
yi,j 〜μi,j(x∣ω^) + Z X σi,j(x∣ω^), where p(z) is N(0,I) and ω 〜q(ω).	(6)
where, ^i,j is the sample drawn at a pixel position {i,j} through the liner transformation of Z (a vector)
with the predicted mean μi,j and variance σi,j. In case of street scenes, yi,j is a class-confidence
vector and sample of final class probabilities is obtained by pushing ^i,j through a softmax.
3.3	Training
For a good variational approximation (3), our approximating variational distribution of generative
models q(ω) should be close to the true posterior p(ω∣X, Y). Therefore, we minimize the KL
divergence between these two distributions. As shown in Gal & Ghahramani (2016b;a); Kendall &
Gal (2017) the KL divergence is given by (over i.i.d data points),
KL3ω) || p(ωlX, Y)) X KL(q(M
|| p(ω)) -
q(ω) log p(Y|X, ω)dω
KL(q(ω) || p(ω)) - / q(ω)( /logp(y∣x,ω)d(x, y))dω.
(7)
KL(q(ω) || P(ω)) - /(/
q(ω) log p(y|x, ω)dω d(x, y).
The log-likelihood term at the right of (7) considers every model for every data point. This imposes
the constraint that every data point must be explained well by every model. However, if the data
distribution (x, y) is multi-modal, this would push every model to the mean of the multi-modal
distribution (as in Figure 1 where only way for models to explain both modes is to converge to the
mean). This discourages diversity in the learned modes. In case of multi-modal data, we would not
be able to recover all likely models, thus hindering our ability to fully capture model uncertainty.
The models would be forced to explain the data variation as observation noise (Osband, 2016), thus
conflating model and observation uncertainty. We propose to mitigate this problem through the use
of an approximate objective using synthetic likelihoods (Wood, 2010; Rosca et al., 2017) — obtained
from a classifier. The classifier estimates the likelihood based on whether the models ω 〜q(ω)
explain (generate) data samples likely under the true data distribution p(y|x). This removes the
constraint on models to explain every data point - it only requires the explained (generated) data
points to be likely under the data distribution. Thus, this allows models ω 〜q(ω) to be diverse and
deal with multi-modality. Next, we reformulate the KL divergence estimate of (7) to a likelihood
ratio form which allows us to use a classifier to estimate (synthetic) likelihoods, (also see Appendix),
=KL(q(ω) || p(ω)) - / ( / q(ω) logp(y∣x, ω)dω)d(x, y).
=KL(q(ω) || p(ω)) - Z (Z q(ω)(log Ppyyi+ 1°gp(y∣x))dω)d(x, y).
(X KL(q(ω) || p(ω)) — Z Z q(ω) log py" dωd(x,y).
(8)
In the second step of (8), we divide and multiply the probability assigned to a data sample by a model
p(y|x, ω) by the true conditional probability p(y|x) to obtain a likelihood ratio. We can estimate
the KL divergence by equivalently estimating this ratio rather than the true likelihood. In order to
(synthetically) estimate this likelihood ratio, let us introduce the variable θ to denote, p(y|x, θ = 1)
the probability assigned by our model ω to a data sample (x, y) and p(y|x, θ = 0) the true probability
of the sample. Therefore, the ratio in the last term of (8) is,
KL(q(ω) || p(ω)) -
KL(q(ω) || p(ω)) -
KL(q(ω) || p(ω)) -
Il
ZZ
ZZ
q(ω) log
q(ω) log
p(y∣x,θ = 1)
p(y∣x,θ = 0)
p(θ = ι∣x,y)
p(θ = 0∣x,y)
dω d(x, y).
dω d(x, y). (Using Bayes theorem)	(9)
q(ω)logT⅞=⅛⅛ dωd(X, y).
4
Published as a conference paper at ICLR 2019
In the last step of (9) we use the fact that the events θ = 1 and θ = 0 are mutually exclusive. We can
approximate the ratio 1-索=?；)by jointly learning a discriminator D(x, y) that can distinguish
between samples of the true data distribution and samples (x, ^) generated by the model ω, which
provides a synthetic estimate of the likelihood, and equivalently integrating directly over (x, ^),
≈ KL(q(ω) || p(ω)) - f q q(ω)log (∣ DL、)dωd(x, y).	(10)
1 - D(x, y)
Note that the synthetic likelihood (IDD：；)) is independent of any specific pair (x, y) of the true
data distribution (unlike the log-likelihood term in (7)), its value depends only upon whether the
generated data point (x, ^) by the model ω is likely under the true data distributionp(y∣x). Therefore,
the models ω have to only generate samples (x, ^) likely under the true data distribution. The models
need not explain every data point equally well. Therefore, we do not push the models ω to the mean,
thus allowing them to be diverse and allowing us to better capture uncertainty.
Empirically, we observe that a hybrid log-likelihood term using both the log-likelihood terms of (10)
and (7) with regularization parameters α and β (with α ≥ β) stabilizes the training process,
α
Z Z q(ω)log (d¾⅛ )dωd(χ, w+β
q(ω) log p(y|x, ω)dω d(x, y).
(11)
Note that, although we do not explicitly require the posterior model distribution to explain all data
points, due to the exponential number of models afforded by dropout and the joint optimization
(min-max game) of the discriminator, empirically we see very diverse models explaining most data
points. Moreover, empirically we also see that predicted probabilities remain calibrated. Next, we
describe the architecture details of our generative models ω and the discriminator D(x, y).
3.4 Model architecture for street scene prediction
The architecture of our ResNet based generative models in our model distribution q(ω) is shown in
Figure 2. The generative model takes as input a sequence of past segmentation class-confidences
sp, the past and future vehicle odometry op, of (x = {sp, op, of}) and produces the class-confidences
at the next time-step as output. The additional conditioning on vehicle odometry is because the
sequences are recorded in frame of reference of a moving vehicle and therefore the future observed
sequence is dependent upon the vehicle trajectory. We use recursion to efficiently predict a sequence
of future scene segmentations y = {sf}. The discriminator takes as input sf and classifies whether it
was produced by our model or is from the true data distribution.
In detail, generative model architecture consists
of a fully convolutional encoder-decoder pair.
This architecture builds upon prior work of Luc
et al. (2017); Jin et al. (2017), however with key
differences. In Luc et al. (2017), each of the
two levels of the model architecture consists of
only five convolutional layers. In contrast, our
model consists of one level with five convolu-
taional blocks. The encoder contains three resid-
ual blocks with max-pooling in between and the
decoder consists of a residual and a convolua-
Figure 2: The architecture of our ResNet based gener-
ative models for street scene prediction in our model
distribution q(ω).
tional block with up-sampling in between. We double the size of the blocks following max-pooling in
order to preserve resolution. This leads to a much deeper model with fifteen convolutional layers, with
constant spatial convolutional kernel sizes. This deep model with pooling creates a wide receptive
field and helps better capture spatio-temporal dependencies. The residual connections help in the
optimization of such a deep model. Computational resources allowing, it is possible to add more
levels to our model. In Jin et al. (2017) a model is considered which uses a Res101-FCN as an encoder.
Although this model has significantly more layers, it also introduces a large amount of pooling. This
leads to loss of resolution and spatial information, hence degrading performance. Our discriminator
model consists of six convolutional layers with max-pooling layers in-between, followed by two fully
connected layers. Finally, in Appendix E we provide layer-wise details and discuss the reduction of
number of models in q(ω) through the use of Weight Dropout (4) for our architecture of generators.
5
Published as a conference paper at ICLR 2019
Method Top-10%
Groundtruth Bayes-S Samples
Bayes-SL Samples
I		I	五3不|					13	T	3	ɪ	
	7					N		同	V	7		ti
Mean
Bayes-S
CVAE
Bayes-SL
80.1±0.4
79.3±0.4
82.5±0.5
83.1±0.6
Figure 3: Left: MNIST generations: The models see the non grayed-out region of the digit. The samples are
generated from models drawn at random from ω 〜q(ω). Right: Top-10% accuracy on MNIST generation.
4	Experiments
Next, we evaluate our approach on MNIST digit generation and street scene anticipation on Cityscapes.
We further evaluate our model on 2D data (Figure 1) and precipitation forecasting in the Appendix.
4.1	MNIST digit generation
Here, we aim to generate the full MNIST digit given only the lower left quarter of the digit. This
task serves as an ideal starting point as in many cases there are multiple likely completions given
the lower left quarter digit, e.g. 5 and 3. Therefore, the learned model distribution q(ω) should
contain likely models corresponding to these completions. We use a fully connected generator with
6000-4000-2000 hidden units with 50% dropout probability. The discriminator has 1000-1000 hidden
units with leaky ReLU non-linearities. We set β = 10-4 for the first 4 epochs and then reduce it to 0,
to provide stability during the initial epochs. We compare our synthetic likelihood based approach
(Bayes-SL) with, 1. A non-Bayesian mean model, 2. A standard Bayesian approach (Bayes-S), 3. A
Conditional Variational Autoencoder (CVAE) (architecture as in Sohn et al. (2015)). As evaluation
metric we consider (oracle) Top-k% accuracy (Lee et al., 2017). We use a standard Alex-Net based
classifier to measure if the best prediction corresponds to the ground-truth class - identifies the
correct mode - in Table 3 (right) over 10 splits of the MNIST test-set. We sample 10 models from
our learned distribution and consider the best model. We see that our Bayes-SL performs best, even
outperforming the CVAE model. In the qualitative examples in Table 3 (left), we see that generations
from models ω 〜q(ω) sampled from our learned model distribution corresponds to clearly defined
digits (also in comparision to Figure 3 in Sohn et al. (2015)). In contrast, we see that the Bayes-S
model produces blurry digits. All sampled models have been pushed to the mean and shows little
advantage over a mean model.
4.2	Cityscapes street scene anticipation
Next, we evaluate our apporach on the Cityscapes dataset - anticipating scenes more than 0.5 seconds
into the future. The street scenes already display considerable multi-modality at this time-horizon.
Evaluation metrics and baselines. We use PSPNet Zhao et al. (2017) to segment the full training
sequences as only the 20th frame has groundtruth annotations. We always use the annotated 20th
frame of the validation sequences for evaluation using the standard mean Intersection-over-Union
(mIoU) and the per-pixel (negative) conditional log-likelihood (CLL) metrics. We consider the
following baselines for comparison to our Resnet based (architecture in Figure 2) Bayesian (Bayes-
WD-SL) model with weight dropout and trained using synthetic likelihoods: 1. Copying the last seen
input; 2. A non-Bayesian (ResG-Mean) version; 3. A Bayesian version with standard patch dropout
(Bayes-S); 4. A Bayesian version with our weight dropout (Bayes-WD). Note that, combination of
ResG-Mean with an adversarial loss did not lead to improved results (similar observations made in
Luc et al. (2017)). We use grid search to set the dropout rate (in (4)) to 0.15 for the Bayes-S and 0.20
for Bayes-WD(-SL) models. We set α, β = 1 for our Bayes-WD-SL model. We train all models
using Adam (Kingma & Ba, 2015) for 50 epochs with batch size 8. We use one sample to train the
Bayesian methods as in Gal & Ghahramani (2016a) and use 100 samples during evaluation.
Comparison to state of the art. We begin by comparing our Bayesian models to state-of-the-art
methods Luc et al. (2017); Seyed et al. (2018) in Table 1. We use the mIoU metric and for a fair
comparison consider the mean (of all samples) prediction of our Bayesian models. We alwyas
compare to the groundtruth segmentations of the validation set. However, as all three methods use a
slightly different semantic segmentation algorithm (Table 2) to generate training and input test data,
we include the mIoU achieved by the Last Input of all three methods (see Appendix C for results
6
Published as a conference paper at ICLR 2019
Table 1: Comparing mean predictions to the state-of-the-art.
		Timestep			
Method	+0.06sec	+0.18sec	+0.54sec	Table 2: Comparison	of seg-
					
Last Input (Luc et al. (2017))	x	49.4	36.9	mentation estimation methods on	
Luc et al. (2017) (ft)	X	59.4	47.8	Cityscapes validation set.	
Last Input (Seyed et al. (2018))	-62.6-	51.0	X		
Seyed et al. (2018)	71.3	60.0	X			
Last Input (Ours)	-671-	52.1	38.3	Method	mIoU
Bayes-S (mean)	71.2	64.8	45.7	DilationlO (Luc et al., 2017)	68.8
Bayes-WD (mean)	73.7	63.5	44.0	PSPNet (Seyed et al., 2018)	75.7
Bayes-WD-SL (mean)	74.1	64.8	45.9	PSPNet (Ours)	76.9
Bayes-WD-SL (ft, mean)	X	65.1	51.2		
Bayes-WD-SL (top 5%)	75.3	65.2	49.5		
Bayes-WD-SL (ft, top 5%)	X	66.7	52.5		
Table 3: Evaluation on capturing uncertainty (using mIoU top 5%).					Table 4: Ablation study and com- parison to a CVAE baseline.		
		Timestep					
	t+	5	t +10			Timestep	
Method	mIoU	CLL	mIoU	CLL		t + 5	t+10
Last Input	45.7	0.86	37.1	1.35	Method	mIoU	mIoU
ResG-Mean	59.1	0.49	46.6	0.89	CVAE (First)	58.7	45.5
Bayes-S	58.8	0.48	46.1	0.80	CVAE (Mid)	58.9	46.6
Bayes-WD	59.2	0.48	46.6	0.79	CVAE (Last)	59.2	46.8
Bayes-WD-SL	60.2	0.47	47.1	0.79	Bayes-WD-SL	60.2	47.1
using Dialation 10). Similar to Luc et al. (2017) we fine-tune (ft) to predict at 3 frame intervals
for better performance at +0.54sec. Our Bayes-WD-SL model outperforms baselines and improves
on prior work by 2.8 mIoU at +0.06sec and 4.8 mIoU/3.4 mIoU at +0.18sec/+0.54sec respectively.
Our Bayes-WD-SL model also obtains higher relative gains in comparison to Luc et al. (2017) with
respect the Last Input Baseline. These results validate our choice of model architecture and show
that our novel approach clearly outperforms the state-of-the-art. The performance advantage of
Bayes-WD-SL over Bayes-S shows that the ability to better model uncertainty does not come at the
cost of lower mean performance. However, at larger time-steps as the future becomes increasingly
uncertain, mean predictions (mean of all likely futures) drift further from the ground-truth. Therefore,
next we evaluate the models on their (more important) ability to capture the uncertainty of the future.
Evaluation of predicted uncertainty. Next, we evaluate whether our Bayesian models are able to
accurately capture uncertainity and deal with multi-modal futures, upto t + 10 frames (0.6 seconds) in
Table 3. We consider the mean of (oracle) best 5% of predictions (Lee et al. (2017)) of our Bayesian
models to evaluate whether the learned model distribution q(ω) contains likely models corresponding
to the groundtruth. We see that the best predictions considerably improve over the mean predictions -
showing that our Bayesian models learns to capture uncertainity and deal with multi-modal futures.
Quantitatively, we see that the Bayes-S model performs worst, demonstrating again that standard
dropout (Kendall & Gal, 2017) struggles to recover the true model uncertainity. The use of weight
dropout improves the performance to the level of the ResG-Mean model. Finally, we see that our
Bayes-WD-SL model performs best. In fact, it is the only Bayesian model whose (best) performance
exceeds that of the ResG-Mean model (also outperforming state-of-the-art), demonstrating the
effectiveness of synthetic likelihoods during training. In Figure 5 we show examples comparing
the best prediction of our Bayes-WD-SL model and ResG-Mean at t + 9. The last row highlights
the differences between the predictions - cyan shows areas where our Bayes-WD-SL is correct and
ResG-Mean is wrong, red shows the opposite. We see that our Bayes-WD-SL performs better at
classes like cars and pedestrians which are harder to predict (also in comparison to Table 5 in Luc
et al. (2017)). In Figure 6, we show samples from randomly sampled models ω^ 〜q(ω), which
shows correspondence to the range of possible movements of bicyclists/pedestrians. Next, we further
evaluate the models with the CLL metric in Table 3. We consider the mean predictive distributions (3)
up to t + 10 frames. We see that the Bayesian models outperform the ResG-Mean model significantly.
In particular, we see that our Bayes-WD-SL model performs the best, demonstrating that the learned
model and observation uncertainty corresponds to the variation in the data.
Comparison to a CVAE baseline. As there exists no CVAE (Sohn et al., 2015) based model for
future segmentation prediction, we construct a baseline as close as possible to our Bayesian models
7
Published as a conference paper at ICLR 2019
Groundtruth, t + 9	ResG-Mean, t + 9 Bayes-WD-SL, t + 9	Comparison
Figure 5: Bayes-WD-SL (top 1) vs ResG-Mean. Cyan: Bayes-WD-SL is correct and ResG-Mean is wrong.
Red: Bayes-WD-SL is wrong and ResG-Mean is correct, white: both right, black: both wrong/unlabeled.
Sample #1, t + 9	Sample #2, t + 9	Sample #3, t + 9	Sample #4, t + 9
Figure 6: Random samples from our Bayes-WD-SL model corresponds to the range of likely movements of
bicyclists/pedestrians.
based on existing CVAE based models for related tasks (Babaeizadeh et al., 2018; Xue et al., 2016).
Existing CVAE based models (Babaeizadeh et al., 2018; Xue et al., 2016) contain a few layers with
Gaussian input noise. Therefore, for a fair comparison we first conduct a study in Table 4 to find
the layers which are most effective at capturing data variation. We consider Gaussian input noise
applied in the first, middle or last convolutional blocks. The noise is input dependent during training,
sampled from a recognition network (see Appendix). We observe that noise in the last layers can
better capture data variation. This is because the last layers capture semantically higher level scene
features. Overall, our Bayesian approach (Bayes-WD-SL) performs the best. This shows that the
CVAE model is not able to effectively leverage Gaussian noise to match the data variation.
Uncertainty calibration. We further evaluate predicted
uncertainties by measuring their calibration - the Corre-
spondence between the predicted probability of a class and
the frequency of its occurrence in the data. As in Kendall
& Gal (2017), we discretize the output probabilities of
the mean predicted distribution into bins and measure the
frequency of correct predictions for each bin. We report
the results at t + 10 frames in Figure 4. We observe that
all Bayesian approaches outperform the ResG-Mean and
CVAE versions. This again demonstrates the effectiveness
of the Bayesian approaches in capturing uncertainty.
5	Conclusion
Figure 4: Uncertainty calibration at t + 10.
We propose a novel approach for predicting real-world semantic segmentations into the future
that casts a convolutional deep learning approach into a Bayesian formulation. One of the key
contributions is a novel optimization scheme that uses synthetic likelihoods to encourage diversity
and deal with multi-modal futures. Our proposed method shows state of the art performance in
challenging street scenes. More importantly, we show that the probabilistic output of our deep
learning architecture captures uncertainty and multi-modality inherent to this task. Furthermore, we
show that the developed methodology goes beyond just street scene anticipation and creates new
opportunities to enhance high performance deep learning architectures with principled formulations
of Bayesian inference.
8
Published as a conference paper at ICLR 2019
References
Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan, Roy H Campbell, and Sergey Levine.
Stochastic variational video prediction. In ICLR, 2018.
Jianmin Bao, Dong Chen, Fang Wen, Houqiang Li, and Gang Hua. Cvae-gan: fine-grained image
generation through asymmetric training. In ICCV, 2017.
Apratim Bhattacharyya, Mario Fritz, and Bernt Schiele. Long-term on-board prediction of people in
traffic scenes under uncertainty. In CVPR, 2018a.
Apratim Bhattacharyya, Mario Fritz, and Bernt Schiele. Accurate and diverse sampling of sequences
based on a “best of many” sample objective. In CVPR, 2018b.
Emily Denton and Rob Fergus. Stochastic video generation with a learned prior. arXiv preprint
arXiv:1802.07687, 2018.
Yarin Gal and Zoubin Ghahramani. Bayesian convolutional neural networks with Bernoulli approxi-
mate variational inference. In ICLR workshop track, 2016a.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. In ICML, 2016b.
Shixiang Gu, Sergey Levine, Ilya Sutskever, and Andriy Mnih. Muprop: Unbiased backpropagation
for stochastic neural networks. In ICLR, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In CVPR, 2016.
Xiaojie Jin, Huaxin Xiao, Xiaohui Shen, Jimei Yang, Zhe Lin, Yunpeng Chen, Zequn Jie, Jiashi Feng,
and Shuicheng Yan. Predicting scene parsing and motion dynamics in the future. In NIPS, 2017.
Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer
vision? In NIPS, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
Namhoon Lee, Wongun Choi, Paul Vernaza, Christopher B Choy, Philip HS Torr, and Manmohan
Chandraker. Desire: Distant future prediction in dynamic scenes with interacting agents. In CVPR,
2017.
Pauline Luc, Natalia Neverova, Camille Couprie, Jakob Verbeek, and Yann LeCun. Predicting deeper
into the future of semantic segmentation. In ICCV, 2017.
Pauline Luc, Camille Couprie, Yann Lecun, and Jakob Verbeek. Predicting future instance segmenta-
tions by forecasting convolutional features. arXiv preprint arXiv:1803.11496, 2018.
David JC MacKay. A practical bayesian framework for backpropagation networks. Neural computa-
tion, 4(3), 1992.
Michael Mathieu, Camille Couprie, and Yann LeCun. Deep multi-scale video prediction beyond
mean square error. In ICLR, 2016.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint
arXiv:1411.1784, 2014.
Radford M Neal. Bayesian learning for neural networks, volume 118. Springer Science & Business
Media, 2012.
Ian Osband. Risk versus uncertainty in deep learning: Bayes, bootstrap and the dangers of dropout.
NIPS Workshop on Bayesian Deep Learning, 2016.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. In ICML, 2014.
9
Published as a conference paper at ICLR 2019
Mihaela Rosca, Balaji Lakshminarayanan, David Warde-Farley, and Shakir Mohamed. Variational
approaches for auto-encoding generative adversarial networks. arXiv preprint arXiv:1706.04987,
2017.
Yunus Saatci and Andrew G Wilson. Bayesian gan. In NIPS, 2017.
Shahabeddin Nabavi Seyed, Mrigank Rochan, and Wang Yang. Future semantic segmentation with
convolutional lstm. In BMVC, 2018.
Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep
conditional generative models. In NIPS, 2015.
Yichuan Tang and Ruslan R Salakhutdinov. Learning stochastic feedforward neural networks. In
NIPS, 2013.
Simon N Wood. Statistical inference for noisy nonlinear ecological dynamic systems. Nature, 466
(7310):1102, 2010.
Shi Xingjian, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo.
Convolutional lstm network: A machine learning approach for precipitation nowcasting. NIPS,
2015.
Tianfan Xue, Jiajun Wu, Katherine Bouman, and Bill Freeman. Visual dynamics: Probabilistic future
frame synthesis via cross convolutional networks. In NIPS, pp. 91-99, 2016.
Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. In ICLR,
2016.
Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing
network. In CVPR, 2017.
10
Published as a conference paper at ICLR 2019
Appendix A. Detailed derivations.
KL divergence estimate. Here, we provide a detailed derivation of (8). Starting from (7), we have,
KL(q(ω∣X, Y) || p(ω∣X, Y))
Z KL(q(ω) || p(ω))-
KL(q(ω) || p(ω)) -
q(ω) log p(Y|X, ω)dω.
∕q(ω)( J logp(y∣x,ω )d(x, y))dω. (overi.i.d (x, y) ∈ (X, Y))
(S1)
KL(q(ω) || p(ω)) - J (∕q(ω)logp(y∣x,ω )dω)d(x,y).
Multiplying and dividing by p(y|x), the true probability of occurance,
=KL(q(ω) || p(ω)) - Z ( Z q(ω) (log PyX^) + logp(y∣x))dω)d(x, y).
p(y|x)
Using q(ω) dω = 1,
KL(q(ω) || p(ω)) - Z ( Z q(ω) log p”,? dω + logp(y∣x))d(x, y).
p(y|x)
KL(q(ω) || p(ω)) - f ∕q(ω)log少1：?dω d(x,y) - /logp(y∣x)d(x,y).
p(y|x)
(S2)
(S3)
As log p(y|x)d(x, y) is independent of ω, the variables we are optmizing over, we have,
z KL(q(ω) || p(ω)) - Z Z q(ω) log p，1：? dω d(x, y).
p(y|x)
(S4)
Appendix B. Results on simple multi-modal 2D data.
Figure 7: Blue: Data points. Black: Sampled models
ω ∈ q(ω) learned by the Bayes-S approach. All
models fit to the mean.
Figure 8: Blue: Data points. Black: Sampled models
ω ∈ q(ω) learned by the Bayes-SL approach. We
recover models covering both modes.
We show results on simple multi-modal 2d data as in the motivating example in the introduction. The
data consists of two parts: x ∈ [-10, 0] we have y = 0 and x ∈ [0, 10] we have y = (-0.3, 0.3).
The set of models under consideration is a two hidden layer neural network with 256-128 neurons
with 50% dropout. We show 10 randomly sampled models from ω 〜q(ω) learned by the Bayes-S
approach in Figure 7 and our Bayes-SL approach in Figure 8 (with α = 1, β = 0). We assume
constant observation uncertainty (=1). We clearly see that our Bayes-SL learns models which cover
both modes, while all the models learned by Bayes-S fit to the mean. Clearly showing that our
approach can better capture model uncertainty.
Appendix C. Additional details and evaluation on street s cenes.
First, we provide additional training details of our Bayes-WD-SL in Table 5.
11
Published as a conference paper at ICLR 2019
Property	Value
Generator learning rate	1 × 10-4
Discriminator learning rate	1 × 10-4
#	Generator updates per iteration	1
#	Discriminator updates per iteration 1
Table 5: Additional training details of our Bayes-WD-SL model.
Method	Timestep	
	+0.18sec	+0.54sec
Last Input (Dialation 10)	49.4	36.9
Luc et al. (2017) (ft)	59.4	47.8
Bayes-WD-SL (ft, mean)	60.1	48.7
Bayes-WD-SL (ft, Top 5%)	61.6	51.3
Table 6: Additional Comparison to Luc et al. (2017) using the same Dialation 10 approach to generate training
segmentations. Note: Fine Tuned (ft) means both approaches are trained to predict at intervals of three frames
(0.18 seconds).
Second, we provide additional evaluation on street scenes. In Section 4.2 (Table 1) we use a PSPNet
to generate training segmentations for our Bayes-WD-SL model to ensure fair comparison with
the state-of-the-art (Seyed et al., 2018). However, the method of Luc et al. (2017) uses a weaker
Dialation 10 approach to generate training segmentations. Note that our Bayes-WD-SL model already
obtains higher gains in comparison to Luc et al. (2017) with respect the Last Input Baseline, e.g. at
+0.54sec, 47.8 - 36.9 = 10.9 mIoU translating to 29.5% gain over the Last Input Baseline of Luc et al.
(2017) versus 51.2 - 38.3 = 12.9 mIoU translating to 33.6% gain over the Last Input Baseline of our
Bayes-WD-SL model in Table 1. But for fairness, here we additionally include results in Table 6
using the same Dialation 10 approach to generate training segmentations.
We observe that our Bayes-WD-SL model beats the model of Luc et al. (2017) in both short-term
(+0.18 sec) and long-term predictions (+0.54 sec). Furthermore, we see that the mean of the Top 5%
of the predictions of Bayes-WD-SL leads to much improved results over mean predictions. This again
confirms the ability of our Bayes-WD-SL model to capture uncertainty and deal with multi-modal
futures.
Appendix D.	Results on HKO precipitation forecasting data.
The HKO radar echo dataset consists of weather radar intensity images. We use the train/test split used
in Xingjian et al. (2015); Bhattacharyya et al. (2018b). Each sequence consists of 20 frames. We use
5 frames as input and 15 for prediction. Each frame is recorded at an interval of 6 minutes. Therefore,
they display considerable uncertainty. We use the same network architecture as used for street scene
segmentation Bayes-WD-SL (Figure 2 and with α = 5, β = 1), but with half the convolutional filters
at each level. We compare to the following baselines: 1. A deterministic model (ResG-Mean), 2. A
Bayesian model with weight dropout. We report the (oracle) Top-10% scores (best 1 of 10), over
the following metrics (Xingjian et al., 2015; Bhattacharyya et al., 2018b), 1. Rainfall-MSE: Rainfall
mean squared error, 2. CSI: Critical success index, 3. FAR: False alarm rate, 4. POD: Probability of
detection, and 5. Correlation, in Table 7,
Note, that Xingjian et al. (2015); Bhattacharyya et al. (2018b) reports only scores over mean of all
samples. Our ResG-Mean model outperforms these state of the art methods, showing the versatility
of our model architecture. Our Bayes-WD-SL can outperform the strong ResG-Mean baseline again
showing that it learns to capture uncertainty (see Figure 10). In comparison, the Bayes-WD baseline
struggles to outperform the ResG-Mean baseline.
We further compare the calibration our Bayes-SL model to the ResG-Mean model in Figure 9. We
plot the predicted intensity to the true mean observed intensity. The difference to ResG-Mean model
12
Published as a conference paper at ICLR 2019
Method	Rainfall-MSE	CSI	FAR	POD	Correlation
Xingjian et al. (2015) (mean)	1.420	0.577	0.195	0.660	0.908
Bhattacharyya et al. (2018b) (mean)	1.163	0.670	0.163	0.734	0.918
ResG-Mean	1.286	0.720	0.104	0.780	0.942
Bayes-WD (Top-10%)	1.067	0.718	0.113	0.771	0.944
Bayes-WD-SL (Top-10%)	1.033	0.721	0.102	0.780	0.945
Table 7: Evaluation on HKO radar image sequences.
Figure 9: Predicted radar intensity to the true mean observed intensity.
is stark in the high intensity region. The RegG-Mean model deviates strongly from the diagonal
in this region - it overestimates the radar intensity. In comparison, We see that our Bayes-WD-SL
approach stays closer to the diagonal. These results again show that our synthetic likelihood based
approach leads to more accurate predictions While not compromising on calibration.

Observation
Groundtruth
Figure 10: Example Top-1 predictions by our Bayes-WD-SL model.
Appendix E.	Additional architecture details.
Here, We provide layer-Wise details of our generative and discriminative models in Table 8 and
Table 9. We provide layer-Wise details of the recognition netWork of the CVAE baseline used in
Table 4 (in the main paper). Finally, in Table 11 We shoW the difference in the number of possible
13
Published as a conference paper at ICLR 2019
models using our weight based variational distribution 4 (weight dropout) versus the patch based
variational distribution (patch dropout) proposed in Gal & Ghahramani (2016a). The number of
patches is calculated using the formula,
Input Resolution × # Output Convolutional Filters,
because we use convolutional stride 1, padding to ensure same output resolution and each patch is
dropped out (in Gal & Ghahramani (2016a)) independently for each convolutional filter. The number
of weight parameters is given by the formula,
Filter size × # Input Convolutional Filters × # Output Convolutional Filters + # Bias.
Table 11 shows that our weight dropout scheme results in significantly lower number of parameters
compared to patch dropout Gal & Ghahramani (2016a).
Details of our generative model. We ShoW the layer Wise details in Table 8.________________
Layer	Type	Size	Activation	Input	Output
In1	Input			x	Conv1,1
Conv1,1	Conv2D	128	ReLU	In1	Conv1,2
Conv1,2	Conv2D	128	ReLU	Conv1,1	Conv1,3
Conv1,3	Conv2D	128	ReLU	Conv1,2	ResConc1
ResConc1	Residual Connection	128		{Conv1,1,Conv1,3}	MaxPool1
MaxPool1	Max Pooling	2×2		ResConc1	Conv2,1
Conv2,1	Conv2D	256	ReLU	MaxPool1	Conv2,2
Conv2,2	Conv2D	256	ReLU	Conv2,1	Conv2,3
Conv2,3	Conv2D	256	ReLU	Conv2,2	ResConc2
ResConc2	Residual Connection	128		{Conv1,3, Conv2,3}	MaxPool2
MaxPool2	Max Pooling	2×2		ResConc1	Conv3,1
Conv3,1	Conv2D	512	ReLU	MaxPool2	Conv3,2
Conv3,2	Conv2D	512	ReLU	Conv3,1	Conv3,3
Conv3,3	Conv2D	512	ReLU	Conv3,2	ResConc3
ResConc3	Residual Connection	128		{Conv2,3 , Conv3,3}	UpSamp1
UpSamp1	Up Sampling	2×2		ResConc3	Conv4,1
Conv4,1	Conv2D	256	ReLU	UpSamp1	Conv4,2
Conv4,2	Conv2D	256	ReLU	Conv4,1	Conv4,3
Conv4,3	Conv2D	256	ReLU	Conv4,2	ResConc4
ResConc4	Residual Connection	128		{Conv3,3 , Conv4,3}	UpSamp2
UpSamp2	Up Sampling	2×2		ResConc3	Conv5,1
Conv5,1	Conv2D	128	ReLU	UpSamp2	Conv5,2
Conv5,2	Conv2D	64	ReLU	Conv5,1	Conv5,3
Conv5,3	Conv2D	64	ReLU	Conv5,2	Conv5,3
Conv6	Conv2D	38		Conv5,3	GaussS
GaussS	Gaussian Sampling			Conv6	y
Table 8: Details our generative model. The final output of Conv6 is split into mean and variances for sampling
as in (6) of the main paper.
Details of our discriminator model. We shoW the layer Wise details in Table 9.
Details of the recognition model used in the CVAE baseline. We shoW the layer Wise details in
Table 10.
14
Published as a conference paper at ICLR 2019
Layer	Type	Size	Activation	Input	Output
In1	Input			x, y	Conv1,1
Conv1,1	Conv2D	128	ReLU	In1	Conv1,2
Conv1,2	Conv2D	128	ReLU	Conv1,1	MaxPool1
MaxPool1	Max Pooling	2×2		Conv1,2	Conv2,1
Conv2,1	Conv2D	256	ReLU	MaxPool1	Conv2,2
Conv2,2	Conv2D	256	ReLU	Conv2,1	MaxPool2
MaxPool2	Max Pooling	2×2		Conv2,2	Conv3,1
Conv3,1	Conv2D	512	ReLU	MaxPool2	MaxPool2
MaxPool3	Max Pooling	2×2		Conv3,1	Conv4,1
Conv4,1	Conv2D	512	ReLU	MaxPool3	MaxPool4
MaxPool4	Max Pooling	2×2		Conv4,1	Flatten
Flatten				MaxPool4	Dense1
Dense1	Fully Connected	1024	ReLU	Flatten	Dense2
Dense2	Fully Connected	1024	ReLU	Dense1	Out
Out	Fully Connected	-		Dense2	
Table 9: Details our discriminator model. The final output Out provides the synthetic likelihoods (IDD(Xy,y)).
Layer	Type	Size	Activation	Input	Output
In1	Input			x, y	Conv1,1
Conv1,1	Conv2D	128	ReLU	In1	Conv1,2
Conv1,2	Conv2D	128	ReLU	Conv1,1	MaxPool1
MaxPool1	Max Pooling	2×2		Conv1,2	Conv2,1
Conv2,1	Conv2D	128	ReLU	MaxPool1	Conv2,2
Conv2,2	Conv2D	128	ReLU	Conv2,1	MaxPool2
MaxPool2	Max Pooling	2×2		Conv2,2	Conv3,1
Conv3,1	Conv2D	128	ReLU	MaxPool2	Conv4,1
Conv4,1	Conv2D	128	ReLU	Conv3,1	UpSamp1
UpSamp1	Up Sampling	2×2		Conv4,1	Conv5,1
Conv5,1	Conv2D	128	ReLU	UpSamp1	UpSamp2
UpSamp2	Up Sampling	2×2		Conv3,2	Conv4,1
Conv6,1	Conv2D	32		UpSamp2	z1
Conv6,2	Conv2D	32		UpSamp2	z2
Conv6,3	Conv2D	32		UpSamp2	z3
Table 10: Details the recognition model used in the CVAE baseline. The final outputs are the Gaussian Noise
tensors z1 , z2 , z3 .
15
Published as a conference paper at ICLR 2019
Layer	Type	Filter Size	Kernel Size	Resolution	# Patches	# Weights
In1	Input					
Conv1,1	Conv2D	128	3×3	128×256	4,194,364	87,680
Conv1,2	Conv2D	128	3×3	128×256	4,194,364	147,584
Conv1,3	Conv2D	128	3×3	128×256	4,194,364	147,584
ResConc1	Residual Connection	128				
MaxPool1	Max Pooling	2×2				
Conv2,1	Conv2D	256	3×3	64×128	2,097,152	290,168
Conv2,2	Conv2D	256	3×3	64×128	2,097,152	590,080
Conv2,3	Conv2D	256	3×3	64×128	2,097,152	590,080
ResConc2	Residual Connection	128				
MaxPool2	Max Pooling	2×2				
Conv3,1	Conv2D	512	3×3	32×64	1,048,576	1,180,160
Conv3,2	Conv2D	512	3×3	32×64	1,048,576	2,359,808
Conv3,3	Conv2D	512	3×3	32×64	1,048,576	2,359,808
ResConc3	Residual Connection	128				
UpSamp1	Up Sampling	2×2				
Conv4,1	Conv2D	256	3×3	64×128	2,097,152	1,180,160
Conv4,2	Conv2D	256	3×3	64×128	2,097,152	590,080
Conv4,3	Conv2D	256	3×3	64×128	2,097,152	590,080
ResConc4	Residual Connection	128				
UpSamp2	Up Sampling	2×2				
Conv5,1	Conv2D	128	3×3	128×256	4,194,364	295,040
Conv5,2	Conv2D	64	3×3	128×256	2,097,152	73,792
Conv5,3	Conv2D	64	3×3	128×256	2,097,152	36,928
Conv6	Conv2D	38	3×3			
GaussS	Gaussian Sampling					
					Total possible models (2#)	
					Gal & Ghahramani (2016a)	Ours
					36,700,406	11,699,192
					36,700,406	11,699,192
Table 11: The difference in the number of possible models using our Weight dropout scheme versus patch
dropout Gal & Ghahramani (2016a) on the architecture for street scene prediction Figure 2
Model Architecture	Total possible models (2#)		
	Gal & Ghahramani (2016a)	Ours	Reduction by
Street Scene Prediction Figure 2	36,700,406	11,699,192	68.1%
Precipitation Forecasting (Appendix D)	18,350,203	5,849,596	68.1%
Table 12: Overview of the variational parameters using our Weight dropout scheme versus patch dropout Gal &
Ghahramani (2016a) of both architectures for street scene and precipitation forecasting
16