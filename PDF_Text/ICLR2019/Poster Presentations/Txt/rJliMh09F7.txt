Published as a conference paper at ICLR 2019
Diversity-sensitive Conditional
Generative Adversarial Networks
Dingdong Yang* t, Seunghoon Hong* t, Yunseok Jangt, Tianchen Zhaot, Honglak Lee*#
t University of Michigan, Ann Arbor, ML USA
^ Google Brain, Mountain View, CA, USA
Ab stract
We propose a simple yet highly effective method that addresses the mode-collapse
problem in the Conditional Generative Adversarial Network (cGAN). Although
conditional distributions are multi-modal (i.e., having many modes) in practice,
most cGAN approaches tend to learn an overly simplified distribution where an
input is always mapped to a single output regardless of variations in latent code.
To address such issue, we propose to explicitly regularize the generator to produce
diverse outputs depending on latent codes. The proposed regularization is sim-
ple, general, and can be easily integrated into most conditional GAN objectives.
Additionally, explicit regularization on generator allows our method to control a
balance between visual quality and diversity. We demonstrate the effectiveness
of our method on three conditional generation tasks: image-to-image translation,
image inpainting, and future video prediction. We show that simple addition of
our regularization to existing models leads to surprisingly diverse generations,
substantially outperforming the previous approaches for multi-modal conditional
generation specifically designed in each individual task.
1	Introduction
The objective of conditional generative models is learning a mapping function from input to out-
put distributions. Since many conditional distributions are inherently ambiguous (e.g. predicting
the future of a video from past observations), the ideal generative model should be able to learn a
multi-modal mapping from inputs to outputs. Recently, Conditional Generative Adversarial Net-
works (cGAN) have been successfully applied to a wide-range of conditional generation tasks, such
as image-to-image translation (Isola et al., 2017; Wang et al., 2018; Zhu et al., 2017a), image in-
painting (Pathak et al., 2016; Iizuka et al., 2017), text-to-image synthesis (Huang et al., 2017; Hong
et al., 2018), video generation (Villegas et al., 2017), etc.. In conditional GAN, the generator learns
a deterministic mapping from input to output distributions, where the multi-modal nature of the
mapping is handled by sampling random latent codes from a prior distribution.
However, it has been widely observed that conditional GANs are often suffered from the mode
collapse problem (Salimans et al., 2016; Arjovsky & Bottou, 2017), where only small subsets of
output distribution are represented by the generator. The problem is especially prevalent for high-
dimensional input and output, such as images and videos, since the model is likely to observe only
one example of input and output pair during training. To resolve such issue, there has been recent
attempts to learn multi-modal mapping in conditional generative models (Zhu et al., 2017b; Huang
et al., 2018). However, they are focused on specific conditional generation tasks (e.g. image-to-
image translation) and require specific network architectures and objective functions that sometimes
are not easy to incorporate into the existing conditional GANs.
In this work, we introduce a simple method to regularize the generator in conditional GAN to resolve
the mode-collapse problem. Our method is motivated from an observation that the mode-collapse
happens when the generator maps a large portion of latent codes to similar outputs. To avoid this, we
propose to encourage the generator to produce different outputs depending on the latent code, so as
to learn a one-to-one mapping from the latent codes to outputs instead of many-to-one. Despite the
simplicity, we show that the proposed method is widely applicable to various cGAN architectures
and tasks, and outperforms more complicated methods proposed to achieve multi-modal conditional
generation for specific tasks. Additionally, we show that we can control a balance between visual
* Equal contribution
1
Published as a conference paper at ICLR 2019
quality and diversity of generator outputs with the proposed formulation. We demonstrate the ef-
fectiveness of the proposed method in three representative conditional generation tasks, where most
existing cGAN approaches produces deterministic outputs: Image-to-image translation, image in-
painting and video prediction. We show that simple addition of the proposed regularization to the
existing cGAN models effectively induces stochasticity from the generator outputs.
2	Related Work
Resolving the mode-collapse problem in GAN is an important research problem, and has been ex-
tensively studied in the standard GAN settings (Metz et al., 2017; Arjovsky et al., 2017; Gulrajani
et al., 2017; Salimans et al., 2016; Miyato et al., 2018). These approaches include unrolling the gen-
erator gradient update steps (Metz et al., 2017), incorporating the minibatch statistics into the dis-
criminator (Salimans et al., 2016), employing the improved divergence measure to smooth the loss
landscape of the discriminator (Gulrajani et al., 2017; Arjovsky et al., 2017; Miyato et al., 2018),
etc.. Although these approaches have been successful in modeling unconditional data distribution
to some extent, recent studies have reported that it is still not sufficient to resolve a mode-collapse
problem in many conditional generative tasks, especially for high-dimensional input and output.
Recently, some approaches have been proposed to address the mode-collapse issue in conditional
GAN. Zhu et al. (2017b) proposed a hybrid model of conditional GAN and Variational Autoencoder
(VAE) for multi-modal image-to-image translation task. The main idea is designing the generator to
be invertible by employing an additional encoder network that predicts the latent code from the gen-
erated image. The similar idea has been applied to unsupervised image-to-image translation (Huang
et al., 2018) and stochastic video generation (Lee et al., 2018) but with non-trivial task-specific
modifications. However, these approaches are designed to achieve multi-modal generation in each
specific task, and there has been no unified solution that addresses the mode-collapse problem for
general conditional GANs. Recently, (Odena et al., 2018) proposed a method that regularizes the
generator by clamping the generator Jacobian within a certain range. Our method shares the similar
motivation with (Odena et al., 2018) but employs a different objective function that simply maxi-
mizes the norm of the generator gradient with an optional upper-bound, which we found that works
much more stable over a wide range of tasks with less number of hyper-parameters.
3	Method
Consider a problem of learning a conditional mapping function G : X → Y , which generates
an output y ∈ Y conditioned on the input x ∈ X . Our goal is to learn a multi-modal mapping
G : X × Z → Y, such that an input x can be mapped to multiple and diverse outputs in Y depending
on the latent factors encoded in z ∈ Z . To learn such multi-modal mapping G, we consider a
conditional Generative Adversarial Network (cGAN), which learns both conditional generator G
and discriminator D by optimizing the following adversarial objective:
min max LcGAN(G,D) = Ex,y [log D(x, y)] + Ex,z [log(1 - D(x, G(x, z)))].	(1)
Although conditional GAN has been proved to work well for many conditional generation tasks, it
has been also reported that optimization of Eq. (1) often suffers from the mode-collapse problem,
which in extreme cases leads the generator to learn a deterministic mapping from x to y and ig-
nore any stochasticity induced by z . To address such issue, previous approaches encouraged the
generator to learn an invertible mapping from latent code to output by E(G(x, z)) = z (Zhu et al.,
2017b; Huang et al., 2018). However, incorporating an extra encoding network E into the existing
conditional GANs requires non-trivial modification of network architecture and introduce the new
training challenges, which limits its applicability to various models and tasks.
We introduce a simple yet effective regularization on the generator that directly penalizes its mode-
collapsing behavior. Specifically, we add the following maximization objective to the generator:
尸…党 ∣^ ∙ (kG(x, zι) - G(x, N2)k	zɔʌ
max Lz(G) = Eziz min I ----------口句 - 勿口-------，丁 )1 ,	(2)
where ∣∣∙k indicates a norm and T is a bound for ensuring numerical stability. The intuition be-
hind the proposed regularization is very simple: when the generator collapses into a single mode
and produces deterministic outputs based only on the conditioning variable x, Eq. (2) approaches
its minimum since G(x, zι) ≈ G(x, z2) for all zι, z2 〜 N(0,1). By regularizing generator to
maximize Eq. (2), we force the generator to produce diverse outputs depending on latent code z .
2
Published as a conference paper at ICLR 2019
Our full objective function can be written as:
min max LcGAN (G, D) - λLz (G),	(3)
where λ controls an importance of the regularization, thus, the degree of stochasticity in G. If G has
bounded outputs through a non-linear output function (e.g. sigmoid), we remove the margin from
Eq. (2) in practice and control its importance only with λ. In this case, adding our regularization
introduces only one additional hyper-parameter.
The proposed regularization is simple, general, and can be easily integrated into most existing con-
ditional GAN objectives. In the experiment, we show that our method can be applied to various
models under different objective functions, network architectures, and tasks. In addition, our reg-
ularization allows an explicit control over a degree of diversity via hyper-parameter λ. We show
that different types of diversity emerge with different λ. Finally, the proposed regularization can be
extended to incorporate different distance metrics to measure the diversity of samples. We show this
extension using distance in feature space and for sequence data.
4	Analysis of the proposed regularization
Connection to Generator Gradient. We show in Appendix A that the proposed regularization in
Eq. (2) corresponds to a lower-bound of averaged gradient norm of G over [z1, z2] as:
Ez 2 [kG(x z2)- G(χ, z1)k ] ≤ E 2 [ Z 1kVzG(χ, Y(t))kdt]	(4)
kz2 - z1 k	0
where γ(t) = tz2 + (1 - t)z1 is a straight line connecting z1 and z2. It implies that optimizing our
regularization (LHS of Eq. (4)) will increase the gradient norm of the generator kVzGk.
It has been known that the GAN suffers from a gradient vanishing issue (Arjovsky & Bottou, 2017)
since the gradient of optimal discriminator vanishes almost everywhere VD ≈ 0 except near the
true data points. To avoid this issue, many previous works had been dedicated to smoothing out the
loss landscape of D so as to relax the vanishing gradient problem (Arjovsky et al., 2017; Miyato
et al., 2018; Gulrajani et al., 2017; Kurach et al., 2018). Instead of smoothing VD by regularizing
discriminator, we increase kVz Gk to encourage G(x, z) to be more spread over the output space
from the fixed Zj 〜p(z), so as to capture more meaningful gradient from D.
Optimization Perspective. We provide another perspective to understand how the proposed
method addresses the mode-collapse problem. For notational simplicity, here we omit the condi-
tioning variable from the generator and focus on a mapping of latent code to output Gθ : Z → Y .
Let a mode M denotes a set of data points in an output space Y, where all elements of the mode have
very small differences that are perceptually indistinguishable. We consider that the mode-collapse
happens if the generator maps a large portion of latent codes to the mode M.
Under this definition, we are interested in a situation where the generator output Gθ (z1) for a
certain latent code z1 moves closer to a mode M by a distance of via a single gradient up-
date. Then we show in Appendix B that such gradient update at z1 will also move the gen-
erator outputs of neighbors in a neighborhood Nr(z1) to the same mode M. In addition, the
size of neighborhood Nr(z1) can be arbitrarily large but is bounded by an open ball of a radius
r = e ∙ (4infZ {max (kGθt(Zz)-Gjt(Z)k, kGθt+1 (Zz)-Gjt+1(Z)k)}) , where θt and θt+ι denote
the generator parameters before and after the gradient update, respectively.
Without any constraints on kG(ZZ)-G(Z2)k, a single gradient update can cause the generator outputs
for a large amount of latent codes to be collapsed into a mode M. We propose to shrink the size of
such neighborhood by constraining kG(ZZ)-G(Z2)k above some threshold τ > 0, therefore prevent
the generator placing a large probability mass around a mode M.
Connection with BicycleGAN (Zhu et al., 2017b). We establish an interesting connection of our
regularization with Zhu et al. (2017b). Recall that the objective of BicycleGAN is encouraging an
invertibility ofa generator by minimizing kz -E(G(z))k1. By taking derivative with respect to z, it
implies that optimal E will satisfy I = VGE(G(z))VZG(z). Because an ideal encoder E should be
robust against spurious perturbations from inputs, we can naturally assume that the gradient norm of
3
Published as a conference paper at ICLR 2019
E should not be very large. Therefore, to maintain invertibility, we expect the gradient of G should
not be zero, i.e. ∣∣VzG(Z)k > T for some τ > 0, which prevents a gradient of the generator being
vanishing. It is related to our idea that penalizes a vanishing gradient of the generator. Contrary to
BicycleGAN, however, our method explicitly optimizes a generator gradient to have a reasonably
high norm. It also allows us to control a degree of diversity with a hyper-parameter λ.
5	Experiments
In this section, we demonstrate the effectiveness of the proposed regularization in three represen-
tative conditional generation tasks that most existing methods suffer from mode-collapse: image-
to-image translation, image inpainting and future frame prediction. In each task, we choose an
appropriate cGAN baseline from the previous literature, which produces realistic but deterministic
outputs, and apply our method by simply adding our regularization to their objective function. We
denote our method as DSGAN (Diversity-Sensitive GAN). Note that both cGAN and DSGAN use
the exactly the same networks. Throughout the experiments, we use the following objective:
min max LcGAN(G,D)+βLrec(G)-λLz(G),	(5)
where Lrec(G) is a regression (or reconstruction) loss to ensure similarity between a prediction y
and ground-truth y , which is chosen differently by each baseline method. Unless otherwise stated,
we use l1 distance for Lrec(G) = ∣G(x, z) - y ∣ and l1 norm for Lz(G)1. We provide additional
video results at anonymous website: https://sites.google.com/view/iclr19-dsgan/.
5.1	Image-to-Image Translation
In this section, we consider a task of image-to-image translation. Given a set of training data
(x, y ) ∈ (X, Y), the objective of the task is learning a mapping G that transforms an image in
domain X to another image in domain Y (e.g. sketch to photo image).
As a baseline cGAN model, we employ the generator and discriminator architectures from Bi-
cycleGAN (Zhu et al., 2017b) for a fair comparison. We evaluate the results on three datasets:
label→image (Radim Tylecek, 2013), edge→photo (Zhu et al., 2016; Yu & Grauman, 2014),
map→image (Isola et al., 2017). For evaluation, we measure both the quality and the diversity of
generation using two metrics from the previous literature. We employed Learned Perceptual Image
Path Similarity (LPIPS) (Zhang et al., 2018) to measure the diversity of samples, which computes
the distance between generated samples using features extracted from the pretrained CNN. Higher
LPIPS score indicates more perceptual differences in generated images. In addition, We use FreChet
Inception Distance (FID) (Heusel et al., 2017) to measure the distance between training and gener-
ated distributions using the features extracted by the inception network (Szegedy et al., 2015). The
lower FID indicates that the two distributions are more similar. To measure realism of the generated
images, we also present human evaluation results using Amazon Mechanical Turk (AMT). Detailed
evaluation protocols are described in Appendix D.1.
λ
Ground Truth
Input Label
(a) LPIPS and FID scores
(b) Qualitative generation results with randomly sampled images
Figure 1: Impact of our regularization on multi-modal conditional generation.
1We use l1 instead of l2 norm for consistency with reconstruction loss (Isola et al., 2017). We also tested
with l2 and observe minor differences.
4
Published as a conference paper at ICLR 2019
Impact of the Proposed Regularization. To analyze the impact of our regularization on learn-
ing a multi-modal mapping, we first conduct an ablation study by varying the weights (λ) for our
regularization. We choose label→image dataset for this experiment, and summarize the results in
Figure 1. From the figure, it is clearly observed that the baseline cGAN (λ = 0) experiences a
severe mode-collapse and produces deterministic outputs. By adding our regularization (λ > 0),
we observe that the diversity emerges from the generator outputs. Increasing the λ increases LPIPS
scores and lower the FID, which means that the generator learns a more diverse mapping from input
to output, and the generated distribution is getting closer to the actual distribution. If we impose
too strong constraints on diversity with high λ, the diversity keeps increasing, but generator out-
puts become less realistic and deviate from the actual distribution as shown in high FID (i.e. we got
FID= 191 and LPIPS=0.20 for λ = 20). It shows that there is a natural trade-off between realism
and diversity, and our method can control a balance between them by controlling λ.
Comparison with BicycleGAN (Zhu et al., 2017b). Next, we conduct comparison experiments
with BicycleGAN (Zhu et al., 2017b), which is proposed to achieve multi-modal conditional gen-
eration in image-to-image translation. In this experiment, we fix λ = 8 for our method across all
datasets and compare it against BicycleGAN with its optimal settings. Table 1 summarizes the re-
sults. Compared to the cGAN baseline, both our method and BicycleGAN are effective to learn
multi-modal output distributions as shwon in higher LPIPS scores. Compared to BicycleGAN, our
method still generates much diverse outputs and distributions that are generally more closer to actual
ones as shown in lower FID score. In human evaluation on perceptual realism, we found that there is
no clear winning method over others. It indicates that outputs from all three methods are in similar
visual quality. Note that applying BicycleGAN to baseline cGAN requires non-trivial modifications
in network architecture and obejctive function, while the proposed regularization can be simply in-
tegrated into the objective function without any modifications. Figure 2 illustrates generation results
by our method. See Appendix D.1.3 for qualitative comparisons to BicycleGAN and cGAN.
We also conducted an experiment by varying a length of latent code z . Table 2 summarizes the
results. As discussed in Zhu et al. (2017b), generation quality of BicycleGAN degrades with high-
dimensional z due to the difficulties in matching the encoder distribution E(x) with prior distribu-
tion p(z). Compared to BicycleGAN, our method is less suffered from such issue by sampling z
from the prior distribution, thus exhibits consistent performance over various latent code sizes.
Method	label → FID	image LPIPS	map → image		edge →photo	
			FID	LPIPS	FID	LPIPS
CGAN	85.07	0.01	90.08	0.02	31.80	0.02
BiCyCleGAN	62.95	0.15	55.53	0.11	20.27	0.11
DSGAN	57.20	0.18	49.92	0.13	23.06	0.12
Table 1: Comparisons of cGAN baseline, BicycleGAN and DSGAN (ours).
Figure 2: Diverse outputs generated by DSGAN. The first and second column shows ground-truth
and input images, while the rest columns are generated images with different latent codes.
5
Published as a conference paper at ICLR 2019
	|z| =8		|z| 二 FID	=32 LPIPS	|z| = 64		|z| =256	
	FID	LPIPS			FID	LPIPS	FID	LPIPS
BicycleGAN	62.95	0.15	79.31	0.16	94.47	^^0.17	111.45	0.17
DSGAN	57.20	0.18	58.34	0.18	59.81	0.18	60.79	0.18
Table 2: Comparisons of BicycleGAN and DSGAN (ours) using various lengths of latent code.
	FID	LPIPs	Segmentation acc (%)
cGAN (pix2pixHD)	48.85	0.00	0.93
BicycleGAN	89.42	0.16	0.72
DSGAN (pix2pixHD)	28.80	0.12	0.92
Table 3: Comparisons of high-resolution image synthesis results in Cityscape dataset.
Figure 3: Qualitative comparison for high-resolution image synthesis (1024 × 512 px.).
Extension to High-Resolution Image Synthesis. The proposed regularization is agnostic to the
choice of network architecture and loss, therefore can be easily applicable to various methods. To
demonstrate this idea, we apply our regularization to the network of pix2pixHD (Wang et al., 2018),
which synthesizes a photo-realistic image of 1024 × 512 resolution from a segmentation label. In
addition to the network architectures, Wang et al. (2018) incorporates a feature matching loss based
on the discriminator as a reconstruction loss in Eq. (5). Therefore, this experiment also demonstrates
that our regularization is compatible with other choices of Lrec .
Table 3 shows the comparison results on Cityscape dataset (Cordts et al., 2016). In addition to
FID and LPIPS scores, we compute the segmentation accuracy to measure the visual quality of the
generated images. We compare the pixel-wise accuracy between input segmentation label and the
predicted one from the generated image using DeepLab V3. (Chen et al., 2018). Since applying
BicycleGAN to this baseline requires non-trivial modifications, we compared against the original
BicycleGAN. As shown in the table, applying our method to the baseline effectively increases the
output diversity with a cost of slight degradation in quality. Compared to BicycleGAN, our method
generates much more visually plausible images. Figure 3 illustrates the qualitative comparison.
5.2	Image Inpainting
In this section, we demonstrate an application of our regularization to image inpainting task. The
objective of this task is learning a generator G : X → Y that takes an image with missing regions
x ∈ X and generates a complete image y ∈ Y by inferring the missing regions.
For this task, we employ generator and discriminator networks from Iizuka et al. (2017) as a base-
line cGAN model with minor modification (See Appendix for more details). To create a data for
inpainting, we take 256 × 256 images of centered faces from the celebA dataset (Liu et al., 2015)
and remove center pixels of size 128 × 128 which contains most parts of the face. Similar to the
image-to-image task, we employ FID and LPIPS to measure the generation performance. Please
refer Appendix D.2 for more details about the network architecture and implementation details.
In this experiment, we also test with an extension of our regularization using a different sample dis-
tance metric. Instead of computing sample distances directly from the generator output as in Eq. (2),
we use the encoder features that capture more semantically meaningful distance between samples.
Similar to feature matching loss (Wang et al., 2018), we use the features from a discriminator to
compute our regularization as follow:
Lz(G) = Ez1,z2
L PL=IlD (X Z1)- Dl (X Z2 )∣∣
kz1 - z2 k
(6)
6
Published as a conference paper at ICLR 2019
where Dl indicates a feature extracted from lth layer of the discriminator D. We denote our meth-
ods based on Eq. (2) and Eq. (6) as DSGANRGB and DSGANFM, respectively. Since there is no
prior work on stochastic image inpainting to our best knowledge, we present comparisons of cGAN
baseline along with our variants.
Analysis on Regularization. We conduct both quantitative and qualitative comparisons of our
methods and summarize the results in Table 4 and Figure 4, respectively. As we observed in the pre-
vious section, adding our regularization induces multi-modal outputs from the baseline cGAN. See
Figure F for qualitative impact of λ. Interestingly, we can see that sample variations in DSGANRGB
tend to be in a low-level (e.g. global skin-color). We believe that sample difference in color may
not be appropriate for faces, since human reacts more sensitively to the changes in semantic features
(e.g. facial landmarks) than just color. Employing perceptual distance metric in our regularization
leads to semantically more meaningful variations, such as expressions, identity, etc..
	FID	LPIPS
cGAN	13.99	0.00
DSGANRGB	13.95	0.01
DSGANFM	13.94	0.05
DSGAN-RGB
Input cGAN
DSGAN-FM
Table 4: Quantitative compar- Figure 4: Qualitative comparisons of our variants. We present one
isons of our variants.	example for baseline as it produces deterministic outputs.
Analysis on Latent Space. To further understand if our regularization encourages z to encode
meaningful features, we conduct qualitative analysis on z . We employ DSGANFM for this experi-
ments. We generate multiple samples across various input conditions while fixing the latent codes z .
Figure 5 illustrates the results. We observe that our method generates outputs which are realistic and
diverse depending on z . More interestingly, the generator outputs given the same z exhibit similar
attributes (e.g. gaze direction, smile) but also context-specific characteristics that match the input
condition (e.g. skin color, hairs). It shows that our method guides the generator to learn meaningful
latent factors in z, which are disentangled from the input context to some extent.
Figure 5: Stochastic image inpainting results. Given an input image with missing region (first row),
We generate multiple faces by sampling different Z (second-fifth rows). Each row is generated from
the same z , and exhibits similar face attributes.
5.3	Video Prediction
In this section, we apply our method to a conditional sequence generation task. Specifically, we
consider a task of anticipating T future frames {xK+1 , xK+2, ..., xK+T } ∈ Y conditioned on
K previous frames {x1 , x2, ..., xK} ∈ X . Since both the input and output of the generator are
sequences in this task, we simply modify our regularization by
Lz(G) = Ez1,z2
由 Pt=+T kG(Xi：t, zι)- G(xi：t, z2)k1
kz1 - z2 ki
(7)
7
Published as a conference paper at ICLR 2019
samples generated by each method. Compared to the baseline that produces deterministic outputs
and SAVP that has limited diversity in KTH, our method generates diverse futures in both datasets.
(Base: 1.0 × 10-3)
Method	Diversity	BAIR Sim max	DiStmin	Diversity	KTH Sim max	Distmin
cGAN	-2.48^^	861.92	22.46	0.04	802.79	5.00
SAVP	18.93	869.58	20.44	0.51	777.70	5.48
DSGAN	26.75	874.12	18.46	3.96	855.10	3.84
Table 5: Comparisons of cGAN, SAVP, and DSGAN (ours). Diversity: pixel-wise distance among
the predicted videos. Simmax : largest cosine similarity between the predicted video and the ground
truth. Distmin : closest pixel-wise distance between the predicted video and the ground truth.
where x1:t represents a set of frames from time step 1 to t.
We compare our method against SAVP (Lee et al., 2018), which also addresses the multi-modal
video prediction task. Similar to Zhu et al. (2017b), it employs a hybrid model of conditional GAN
and VAE, but using the recurrent generator designed specifically for future frame prediction. We
take only GAN component (generator and discriminator networks) from SAVP as a baseline cGAN
model and apply our regularization with λ = 50 to induce stochasticity. We use |z | = 8 for all
compared methods. See Appendix D.3.2 for more details about the network architecture.
We conduct experiments on two datasets from the previous literature: the BAIR action-free robot
pushing dataset (Ebert et al., 2017) and the KTH human actions dataset (Schuldt et al., 2004). To
measure both the diversity and the quality, we generate 100 random samples of 28 future frames for
each test video and compute the (a) Diversity (pixel-wise distance among the predicted videos) and
the (b) Distmin (minimum pixel-wise distance between the predicted videos and the ground truth).
Also, for a better understanding of quality, we additionally measured the (c) Simmax (largest cosine
similarity metric between the predicted video and the ground truth on VGGNet (Simonyan & Zisser-
man, 2015) feature space). An ideal stochastic video prediction model may have higher Diversity,
while having lower Distmin with higher Simmax so that a model still can predict similar to the ground
truth as a candidate. More details about evaluation metric are described in Appendix D.3.3.
We present both quantitative and qualitative comparison results in Table 5 and Figure 6, respectively.
As illustrated in the results, both our method and SAVP can predict diverse futures compared to the
baseline cGAN that produces deterministic outputs. As shown in Table 5, our method generates
more diverse and realistic outputs than SAVP with much less number of parameters and simpler
training procedures. Interestingly, as shown in KTH results, SAVP still suffers from a mode-collapse
problem when the training videos have limited diversity, whereas our method generally works well in
both cases. It shows that our method generalizes much better to various videos despite its simplicity.
6 Conclusion
In this paper, we investigate a way to resolve a mode-collapsing in conditional GAN by regularizing
generator. The proposed regularization is simple, general, and can be easily integrated into existing
8
Published as a conference paper at ICLR 2019
conditional GANs with broad classes of loss function, network architecture, and data modality.
We apply our regularization for three conditional generation tasks and show that simple addition
of our regularization to existing cGAN objective effectively induces the diversity. We believe that
achieving an appropriate balance between realism and diversity by learning λ and τ such that the
learned distribution matches an actual data distribution would be an interesting future work.
Acknowledgement This work was supported in part by ONR N00014-13-1-0762, NSF CAREER
IIS-1453651, DARPA Explainable AI (XAI) program #313498, and Sloan Research Fellowship.
References
Martin Arjovsky and Leon Bottou. Towards Principled Methods for Training Generative Adversarial
Networks. In ICLR, 2017.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein Generative Adversarial Net-
works. In ICML, 2017.
Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-
Decoder with Atrous Separable Convolution for Semantic Image Segmentation. In ECCV, 2018.
Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo
Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The Cityscapes Dataset for Semantic
Urban Scene Understanding. In CVPR, 2016.
Emily Denton and Rob Fergus. Stochastic Video Generation with a Learned Prior. In ICML, 2018.
Frederik Ebert, Chelsea Finn, Alex X Lee, and Sergey Levine. Self-Supervised Visual Planning with
Temporal Skip Connections. In CoRL, 2017.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved Training of Wasserstein GANs. In NIPS, 2017.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Gunter Klambauer, and
Sepp Hochreiter. GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash
Equilibrium. In NIPS, 2017.
Seunghoon Hong, Dingdong Yang, Jongwook Choi, and Honglak Lee. Inferring Semantic Layout
for Hierarchical Text-to-Image Synthesis. In CVPR, 2018.
Xun Huang, Yixuan Li, Omid Poursaeed, John Hopcroft, and Serge Belongie. Stacked generative
adversarial networks. In CVPR, 2017.
Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. Multimodal Unsupervised Image-to-
Image Translation. In ECCV, 2018.
Satoshi Iizuka, Edgar Simo-Serra, and Hiroshi Ishikawa. Globally and Locally Consistent Image
Completion. In SIGGRAPH, 2017.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-Image Translation with
Conditional Adversarial Networks. In CVPR, 2017.
Yunseok Jang, Gunhee Kim, and Yale Song. Video Prediction with Appearance and Motion Condi-
tions. In ICML, 2018.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet Classification with Deep Con-
volutional Neural Networks. In NIPS, 2012.
Karol Kurach, Mario Lucic, Xiaohua Zhai, Marcin Michalski, and Sylvain Gelly. The GAN Land-
scape: Losses, Architectures, Regularization, and Normalization. arXiv:1807.04720, 2018.
Alex X. Lee, Richard Zhang, Frederik Ebert, Pieter Abbeel, Chelsea Finn, and Sergey Levine.
Stochastic Adversarial Video Prediction. arXiv:1804.01523, 2018.
9
Published as a conference paper at ICLR 2019
Chuan Li and Michael Wand. Precomputed Real-Time Texture Synthesis with Markovian Genera-
tive Adversarial Networks. In ECCV, 2016.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep Learning Face Attributes in the Wild.
In ICCV, 2015.
Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled Generative Adversarial
Networks. In ICLR, 2017.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral Normalization
for Generative Adversarial Networks. In ICLR, 2018.
Augustus Odena, Jacob Buckman, Catherine Olsson, TomB Brown, Christopher Olah, Colin Raffel,
and Ian Goodfellow. Is generator conditioning causally related to gan performance? arXiv
preprint arXiv:1802.08768, 2018.
Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context
Encoders: Feature Learning by Inpainting. In CVPR, 2016.
Radim Sara Radim Tylecek. Spatial Pattern Templates for Recognition of Objects With Regular
Structure. In GCPR, 2013.
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional NetWorks for Biomed-
ical Image Segmentation. In MICCAI, 2015.
Tim Salimans, Ian J. GoodfelloW, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved Techniques for Training GANs. In NIPS, 2016.
Christian Schuldt, Ivan Laptev, and Barbara Caputo. Recognizing Human Actions: A Local SVM
Approach. In ICPR, 2004.
Karen Simonyan and AndreW Zisserman. Very Deep Convolutional NetWorks for Large-Scale Image
Recognition. In ICLR, 2015.
Akash Srivastava, Lazar Valkoz, Chris Russell, Michael U Gutmann, and Charles Sutton. VEEGAN:
Reducing Mode Collapse in GANs using Implicit Variational Learning. In NIPS, 2017.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and AndreW Rabinovich. Going Deeper With Convolutions. In
CVPR, 2015.
Ruben Villegas, Jimei Yang, Seunghoon Hong, Xunyu Lin, and Honglak Lee. Decomposing Motion
and Content for Natural Video Sequence Prediction. In ICLR, 2017.
Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, AndreW Tao, Jan Kautz, and Bryan Catanzaro. High-
Resolution Image Synthesis and Semantic Manipulation With Conditional GANs. In CVPR, 2018.
Tom White. Sampling Generative NetWorks. arXiv:1609.04468, 2016.
SHI Xingjian, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo.
Convolutional LSTM NetWork: A Machine Learning Approach for Precipitation NoWcasting. In
NIPS, 2015.
Aron Yu and Kristen Grauman. Fine-Grained Visual Comparisons With Local Learning. In CVPR,
2014.
Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The Unreasonable
Effectiveness of Deep Features as a Perceptual Metric. In CVPR, 2018.
Jun-Yan Zhu, Philipp Krahenbuhl, Eli Shechtman, and Alexei A Efros. Generative Visual Manipu-
lation on the Natural Image Manifold. In ECCV, 2016.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired Image-to-Image Translation
using Cycle-Consistent Adversarial NetWorks. In ICCV, 2017a.
Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A Efros, Oliver Wang, and Eli
Shechtman. ToWard Multimodal Image-to-Image Translation. In NIPS, 2017b.
10
Published as a conference paper at ICLR 2019
Appendix
A Derivation of lower-b ound of gradient norm
In this section, we provide a derivation of our regularization term from a true gradient norm of the
generator. Given arbitrary latent samples z1, z2, from gradient theorem we have
kG(x, Z2)- G(x, Zl)k _ kRγ[zι,Z2]VzG(X, Z) ∙ dzk
kz2 - z1k
kZ2- Z1k
=kRo1 VzG(x,γ(t)) ∙ Y0(t)dtk
-	kz2 — z1k
=k∕θ1 VzG(x,Y(t)) ∙ (z2 — ZI)dtk
kz2 - z1k
≤ R01kvz G(x, Y ⑴)kkz2 — ZIkdt
kz2 - z1 k
=Z 1kVz G(x, γ (t))kdt,
0
where γ is a straight line connecting Z1 and Z2, where γ(0) = Z1 and γ(1) = Z2.
(8)
Apply expectation on both sides of (8) with respect to Z1, Z2 from standard Gaussian distribution
gives Eqn. 4:
Ezι,z2 [kG(x,Z2)- G(X, z1)k ] ≤ Ezι,z2 [ Z 1kVzG(x, Y(t))kdtl .	(9)
kZ2 - Z1 k	0
B Collapsing to a Mode as a Group
For notational simplicity, we omit the conditioning variable from the generator and focus on a map-
ping of latent code to output Gθ : Z → Y where Y is the image space.
Definition B.1. A mode M is a subset of Y satisfying maXy∈Mky — y*k < α for some image y*
and α > 0. Let Z1 be a sample in latent space, we say Z1 is attracted to a mode M by from a
gradient step if ∣∣y* 一 Gθt+ι (zι)k + e < ∣∣y* 一 Gθt (zι)k, where y* ∈ M is an image in a mode,
θt and θt+1 are the generator parameters before and after the gradient updates respectively.
In other words, we define modes as sets consisting of images that are close to some real images, and
we consider a situation where the generator output Gθt (Z1) at certain Z1 is attracted to a mode M
by a single gradient update.
With Definition B.1, we are now ready to state and prove the following proposition.
Proposition B.1. Suppose Z1 is attracted to the mode M by , then there exists a neighborhood
Nr(Z1) of Z1 such that Z2 is attracted to M by /2, for all Z2 ∈ Nr(Z1). The size ofNr(Z1) can be
arbitrarily large but is bounded by an open ball of radius r where
r = e ∙
4 inf max
∣Gθt(zι)- Gθt(z)k
kz1 - Zk
∣Gθt+ι (ZI) — Gθt+ι (Z)k
kz1 - Zk
(10)
Proof. Consider the following expansion.
ky* - Gθt+1 (Z2)k≤ky* - Gθt+1 (Zl)k + kGθt+ι(Z1) - Gθt+1 (Z2)k
< ky* - Gθt(Zι)k + kGθt+ι (zi) - Gθt+1 ⑶川-e	(Definition B.1)
≤ ky* - Gθt (Z2)k + kGθt(Z2) - Gθt(Z1)k + kGθt+1(Z1) - Gθt+1(Z2)k -
= ky* - Gθt(Z2)k
+ "Gθt (zi)- Gθt (Z2)k
I kZ1 - Z2k
+ kGθt+ι (ZI) - Gθt+ι (Z2)k
kZ1 - Z2k
kZ1 - Z2 k - .
(11)
11
Published as a conference paper at ICLR 2019
Eq. (11) implies that
ky*- Gθt+ι (z2)k + ∣< ∣∣y*- Gθt (z2)k,
for all z2 that satisfies
“Gθt(zι) - Gθt(z2)k	kGθt+ι(zι)- Gθt+ι(z2)k∖ ,,	,l, ≡
(- +------------------------------------------) kz1 -z2k≤ 2.
(12)
(13)
Define NT(zι) = (z : max (kGθt(Z1)-Gθt(z)k, kGθt+1 (ZlG;-1 (Z)k) ≤ TO, then Eq. (13) holds
kz1 -zk	kz1 -zk
for any z? ∈ Uτ>0 {Be∕4τ(zι) ∩NT(zι)} = Nr(zι). We haveNr(zι) = 0 since zι ∈ Nr (zι).
The size of Nr (zι) Can be arbitrarily large if Be∕4τ(zι) ⊆ NT(zι) for arbitrarily small T. And
kGθ (z1)-Gθ (z)k kGθt+1 (z1)-Gθt+1 (z)k
for any τ > 0 such that T ≤ max (-t-Zkt , ——t+1 口乞］-2/+1 ) for all z, We have
Nr (zι) ⊆ Be∕4τ(zι). In particular, pick the largest T possible yields the boundNr (zι) ⊆ Br (zι),
r = e ∙
4 inf max
kGθt(zι)- Gθt(z)k
kzι - Zk
kGθt+ι (ZI) - Gθt+ι (z)k
kz1 - Zk
(14)
□
12
Published as a conference paper at ICLR 2019
C Ablation Study
C.1 Application to Unconditional GAN
Since the proposed regularization is not only limited to conditional GAN, we further analyze its im-
pact on unconditional GAN. To this end, we adopt synthetic datasets from (Srivastava et al., 2017;
Metz et al., 2017), a mixture of eight 2D Gaussian distributions arranged in a ring. For uncon-
ditional generator and discriminator, we adopt the vanilla GAN implementation from (Srivastava
et al., 2017), and train the model with and without our regularization (λ = 0.1). We follow the
same evaluation protocols used in (Srivastava et al., 2017). It generates 2,500 samples by the gen-
erator, and counts a sample as high-quality if it is within three standard deviations of the nearest
mode. Then the performance is reported by 1) counting the number of modes containing at least one
high-quality sample and 2) computing the portion of high-quality samples from all generated ones.
We summarize the qualitative and quantitative results (10-run average) in Figure A and Table A,
respectively.
As illustrated in Figure A, we observe that vanilla GAN experiences a severe mode collapse, which
puts a significant probability mass around a single output mode. Contrary to results reported in
(Srivastava et al., 2017), we observed that the mode captured by the generator is still not close
enough to the actual mode, resulted in 0 high-quality samples as shown in Table A. On the other
hand, applying our regularization effectively alleviates the mode-collapse problem by encouraging
the generator to efficiently explore the data space, enabling the generator to capture much more
modes compared to vanilla GAN setting.
Vanilla GAN
DSGAN
Method	2D Ring	
	Modes (MaX 8)	High Quality Samples (%)
Vanilla GAN-	0(1*)	0.0 (99.3*)
Unrolled GAN	7.6	35.6
VEEGAN	8	52.9
DSGAN	8	81.4
Table A: Quantitative evaluation results (* denotes the results reported in SrivastaVa et al. (2017)).
C.2 Impact on Unbalanced cGAN
In principle, our regularization can help the generator to CoPe with vanishing gradient from the dis-
criminator to some extent, as it spreads out the generator landscape thus increases the chance to
capture useful gradient signals around true data points. To verify its impact, we simulate the vanish-
ing gradient problem by training the baseline cGAN until it converges and retraining the model with
our regularization while initializing the discriminator with the pre-trained weights. Empirically we
observed that the pre-trained discriminator can distinguish the real data and generated samples from
the randomly initialized generator almost perfectly, and the generator experiences a severe vanishing
gradient problem at the beginning of the training. We use the image-to-image translation model on
label→image dataset for this experiment.
In the experiment, we found that the generator converges to the FID and LPIPS scores of 52.32
and 0.16, respectively, which are close to the ones we achieved with the balanced discriminator
(FID: 57.20, LPIPS: 0.18). We observed that our regularization loss goes down very quickly in the
13
Published as a conference paper at ICLR 2019
early stage of training, which helps the generator to efficiently explorer its output space when the
discriminator gradients are vanishing. Together with the reconstruction loss, we found that it helps
the generator to capture useful learning signals from the discriminator and learn both realistic and
diverse modes in the conditional distribution.
14
Published as a conference paper at ICLR 2019
D Additional experiment results
This section provides additional experiment details and results that could not be accommodated in
the main paper due to space restriction. We are going to release the code and datasets upon the
acceptance of the paper.
D. 1 Image-to-Image Translation
D.1.1	Baseline Models
BicycleGAN’s Generator and Discriminator. We use BicycleGAN’s generator and discrimina-
tor structures as a baseline cGAN. The baseline model has exactly the same hyperparameters as
BicycleGAN including the weight of pixel-wise L1 loss and GAN loss. The baseline model setting
then basically becomes a pix2pix image-to-image translation setting (Isola et al., 2017). The gen-
erator architecture is a U-Net style network (Ronneberger et al., 2015) and the discriminator is a
two-scale patchGAN-style (Li & Wand, 2016) network.
Pix2pixHD Baseline. In this setting, we adopt the generator and discriminator networks from
pix2pixHD (Wang et al., 2018) as a baseline cGAN. Compared to the original pix2pixHD network
that employs two nested generators, we use only one generator for simplicity. Since the original
generator does not contain stochastic component, we modified the generator network by injecting
the latent code after the downsampling layers by spatial tiling and depth-wise concatenation. The
discriminator is a two-scale patchGAN-style (Li & Wand, 2016) network. Following the original
setting, we employ feature matching loss based on discriminator (Wang et al., 2018) and perceptual
loss based on the pre-trained VGGNet (Simonyan & Zisserman, 2015) as a reconstruction loss in
Eq. (5).
D.1.2 Evaluation Metrics
Here we provide a detailed descriptions for evaluation metrics and evaluation protocols.
Learned Perceptual Image Patch Similarity, LPIPS (Zhang et al., 2018). LPIPS score mea-
sures the diversity of the generated samples using the L1 distance of features extracted from pre-
trained AlexNet (Krizhevsky et al., 2012). We generate 20 samples for each validation image, and
compute the average of pairwise distances between all samples generated from the same input. Then
we report the average of LPIPS scores over all validation images.
Frechet Inception Distance, FID (Heusel et al., 2017). For each method, We compute FID score
on the validation dataset. For each input from the validation dataset, we sample 20 randomly gen-
erated output. We take the generated images as a generated dataset and compute the FID score
betWeen the generated dataset and training dataset. If the size of an image is different betWeen the
training dataset and generated dataset, We resize training images to the size of generated images.
We use the features from the final average pooling layer of the InceptionV3 (Szegedy et al., 2015)
network to compute Frechet Distance.
Human Evaluation via Amazon Mechanical Turk (AMT). To compare the perceptual quality
of generations among different methods, we conduct human evaluation via AMT. We conduct side-
by-side comparisons between our method and a competitor (i.e. baseline cGAN and BicycleGAN).
Specifically, we present two sets of images generated by each compared method given the same
input condition, and ask turkers to choose the set that is visually more plausible and matches the
input condition. Each set has 6 randomly sampled images. We collect answers over 100 examples
for each dataset, where each question is answered by 5 unique turkers.
D.1.3 Qualitative Results
Qualitative Comparison. We present the qualitative comparisons of various methods presented in
Table 1 and Table 3 in the main paper. Figure B illustrates the qualitative comparison results of DS-
GAN (ours), baseline cGAN and BicycleGAN in Table 1. In the example of edges→photo dataset,
15
Published as a conference paper at ICLR 2019
the input edge images miss some of the features in the ground-truth images (e.g. shoelace). While
both DSGAN and baseline cGAN are able to capture such missing parts in an input, we observe that
some of BicycleGAN’s outputs are missing it. Also in the example of maps→images dataset, both
DSGAN and baseline cGAN can generate natural and variable vegetation in the corresponding area
while BicycleGAN tends to generate plain texture with global color variations in such area. These
two examples show how our regularization can help cGAN to learn more visually reasonable and
diverse results. We additionally present the qualitative comparison results of Table 3. We observe
that the generation results from BicycleGAN suffers from low-visual quality, while our method is
able to generate fine details of the objects and scene by exploiting the network for high-resolution
image synthesis.
Input and GT
Generated Images
cGAN BicycleGAN DSGAN
cGAN BicycleGAN DSGAN
Figure B: Qualitative comparisons of Table 1.
cGAN BicycleGAN DSGAN
16
Published as a conference paper at ICLR 2019
Figure C: Qualitative comparisons of Table 3.
Analysis on Latent Space. To better understand the latent space learned with the proposed reg-
ularization, we generate images in Cityscape dataset by interpolating two randomly sampled latent
vectors by spherical linear interpolation (White, 2016). Figure D illustrates the interpolation results.
As shown in the figure, the intermediate generation results are all reasonable and exhibit smooth
transitions, which implies that the learned latent space has a smooth manifold.
Figure D: Interpolation in latent space of DSGAN on Cityscapes dataset.
We also present the comparison of interpolation results between DSGAN and BicycleGAN on maps
→ images dataset. As shown in Figure E, DSGAN generates meaningful and diverse predictions
on ambiguous regions (e.g. forest on a map) and has a smooth transition from one latent code to
another. On contrary, the BicyceGAN does not show meaningful changes within the interpolations
and sometimes has a sudden changes on its output (e.g. last generated image). We also observe
similar patterns across many examples in this dataset. It shows an example that DSGAN learns
better latent space than BicycleGAN.
Figure E:	Comparison of latent space interpolation results between DSGAN and BicycleGAN.
17
Published as a conference paper at ICLR 2019
D.2 Image Inpainting
In this section, we provide details of image inpainting experiment.
Network Architecture. We employ the generator and discriminator networks from Iizuka et al.
(2017) as baseline conditional GAN. Our generator takes 256 × 256 image with the masked region
X as an input and produces 256 X 256 prediction of the missing region y as an output. Then We
combine the predicted image with the input by y = (1 — M) Θ X + M Θ y as an output of the
netWork, Where M is a binary mask indicating the missing region. Then the combined output y is
passed as an input to the discriminator. We apply two modifications to the baseline model to achieve
better generation quality. First, compared to the original model that employs the Mean Squared
Error (MSE) as a reconstruction loss Lrec (G) in Eq. (5), we apply the feature matching loss based
on the discriminator (Wang et al., 2018). Second, compared to the original model that employs
two discriminators applied independently to the inpainted region and entire image, we employ only
one discriminator on the inpainted region but using patchGAN-style discriminator (Li & Wand,
2016). Please note that these modifications are to achieve better image quality but irrelevant to our
regularization.
Analysis on Regularization. First, we conduct qualitative analysis on how the proposed regular-
ization controls a diversity of the generator outputs. To this end, we train the model (DSGANFM) by
varying the weights for our regularization, and present the results in Figure F. As already observed
in Section 5.1, imposing stronger constraints on the generator by our regularization indeed increases
the diversity in the generator outputs. With small weights (e.g. λ = 2), we observe limited visual
differences among samples, such as subtle changes in facial expressions or makeup. By increasing
λ (e.g. λ = 5), we can see that more meaningful diversity emerges such as hair-style, age, and even
identity while maintaining the visual quality and alignment to input condition. It shows more intu-
itively how our regularization can effectively help the model to discover more meaningful modes in
the output space.
λ = 3.5
Figure F:	Image inpainting results with different λ. We observe more diversity emerges from the
genrator outputs as we increase the weights for our regularization.
Analysis on Latent Space. We further conduct a qualitative analysis on the learned latent space.
To verify that the model learns a continuous conditional distribution with our regularization, we
conduct the interpolation experiment similar to the previous section. Specifically, we sample two
random latent codes from the prior distribution and generate images by linearly interpolating the
18
Published as a conference paper at ICLR 2019
latent code between two samples. Figure G illustrates the results. As it shows, the generator out-
puts exhibit a smooth transition between two samples, while most intermediate samples also look
realistic.
Figure G: Interpolation results on image inpainting task. For each row, we sample the two latent
codes (leftmost and rightmost images), and generate the images from the interpolated latent codes
from one latent code to another.
19
Published as a conference paper at ICLR 2019
D.3 Video Prediction
In this section, we provide more details on network architecture, datasets and evaluation metrics on
the video prediction task.
D.3.1 Dataset
We measure the effectiveness of our method based on two real-world datasets: the BAIR action-free
robot pushing dataset (Ebert et al., 2017) and the KTH human actions dataset (Schuldt et al., 2004).
For both of the dataset, we provide two frames as the condition and train the model to predict 10
future frames (k = 2, T = 10 in Eq. (7)). In testing time, we run each model to predict 28 frames
(k = 2, T = 28). Following Lee et al. (2018), we used 64 × 64 frames for both datasets. The details
of data pre-processing are described in below.
BAIR Action-Free (Ebert et al., 2017). This dataset contains randomly moving robot arms on a
table with a static background. This dataset contains the diverse movement of a robot arm with a
diverse set of objects. We downloaded the pre-processed data provided by the authors (Lee et al.,
2018) and used it directly for our experiment.
KTH (Schuldt et al., 2004). Each video in this dataset contains a single person in a static back-
ground performing one of six activities: walking, jogging, running, boxing, hand waving, and hand
clapping. We download the pre-processed videos from Villegas et al. (2017); Denton & Fergus
(2018), which contains the frames with reasonable motions. Following Jang et al. (2018), we added
a diversity to videos by randomly skipping frames in a range of [1,3].
D.3.2 Network Architecture
We compare our method against SAVP (Lee et al., 2018) which is proposed to achieve stochastic
video prediction. SAVP addresses a mode-collapse problem using the hybrid model of conditional
GAN and VAE. For a fair comparison, we construct our baseline cGAN by taking GAN component
from SAVP (the generator and discriminator networks). In below, we provide more details of the
generator and discriminator architectures used in our baseline cGAN model.
The generator is based on the encoder-decoder network with convolutional LSTM (Xingjian et al.,
2015). At each step, it takes a frame together with a latent code as inputs and produces the next frame
as an output. Contrary to the original SAVP that takes a latent code at each step to encode frame-
wise stochasticity, we modified the generator to take one latent code per sequence that encodes the
global dynamics of a video. Then the discriminator takes the entire video as an input and produces
a prediction on real or fake through 3D convolution operations.
D.3.3 Evaluation Metrics
We provide more details about the evaluation metrics used in our experiment. For each test video,
we generate 100 random samples with a length of 28 frames and evaluate the performance based on
the following metrics:
•	Diversity: To measure the degree of diversity of the generated samples, we computed the
frame-wise distance between each pair of the generated videos based on Mean Squared
Error (MSE). Then we reported the average distance over all pairs as a result.
•	Distmin : Following Lee et al. (2018), we evaluate the quality of generations by measur-
ing the distance of the closest sample among the all generated ones to the ground-truth.
Specifically, for each test video, we computed the minimum distance between the gener-
ated samples and the ground-truth based on MSE and reported the average of the distances
over the entire test videos.
•	Simmax : As another measure for the generation quality, we compute the similarity of the
closest sample to the ground-truth similar to Distmin but using the cosine similarity of fea-
tures extracted from VGGNet (Simonyan & Zisserman, 2015). We report the average of
the computed similarity for entire test videos.
20
Published as a conference paper at ICLR 2019
D.3.4 More Examples
We present more video prediction results on both BAIR and KTH datasets in Figure H, which corre-
sponds to Figure 6 in the main paper. As discussed in the main paper, the baseline cGAN produces
realistic but deterministic outputs, whereas both SAVP and our method generate far more diverse
future predictions. SAVP fails to generate the diverse outputs in KTH datasets, mainly because the
dataset contains many examples with small motions. On the contrary, our method generates diverse
outputs in both datasets, since our regularization directly penalizes the mode-collapsing behavior
and force the model to discover various modes. Interestingly, we found that our model sometimes
generates actions different from the input video when the motion in input frames are ambiguous
(e.g. hand-clapping to hand-waving in the highlighted example). It shows that our method can
generate diverse and meaningful futures.
Figure I presents more detailed qualitative comparison in BAIR robot arm dataset. Both baseline
cGAN and SAVP often suffer from the noise predictions in the background, since they fail to predict
the correct motion of foreground objects. On the other hand, our method can generate more clear
outputs and sometimes even an interaction between foreground and background objects by predict-
ing more meaningful dynamics of videos from latent code z. See captions of Figure I for more
detailed discussions.
21
Published as a conference paper at ICLR 2019
Input Frames	Predicted Frames
t = 1 t = 2	t = 3 t = 6 t = 9 t = 12 t = 15 t = 20 t = 25 t = 30
cGAN
selpmaS modna
Input Frames	Predicted Frames
t = 1 t = 2 t = 3 t = 6 t = 9 t = 12 t = 15 t = 20 t = 25 t = 30
lɪɪ
SAVP
Input Frames
t = 1 t = 2
Predicted Frames
t = 3 t = 6 t = 9 t = 12 t = 15 t = 20 t = 25 t = 30
Input Frames
t = 1 t = 2
Predicted Frames
t = 3 t = 6 t = 9 t = 12 t = 15 t = 20 t = 25 t = 30
LLLtHLilt.
selpmaS modnaR
snAL
dnuorG
selpmaS modna

1
Figure H: Stochastic video prediction results. In both datasets, our method presents diverse predic-
tion, whereas SAVP generate less diverse result especially in the KTH dataset. Interestingly, as you
can see from the dotted orange box, our model can explore not only the original condition (hand
clapping) but also other cases (hand waving) if the context is not too strong. Please check our web
page to see the videos: https://sites.google.com/view/iclr19-dsgan/
22
Published as a conference paper at ICLR 2019
Random Samples
Figure I: Qualitative comparison of various video prediction methods. Both baseline cGAN and
SAVP exhibit some noises in the predicted videos due to the failures in separating the moving fore-
ground object from the background clutters (red arrow). Compared to this, our method tends to
generate more clear predictions on both foreground and background. Interestingly, SAVP some-
times fail to predict interaction between objects (magenta arrows). For instance, the objects on a
table stay in the same position even after pushed by the robot arm. On the other hand, our method is
able to capture such interactions more precisely (blue arrows).
23