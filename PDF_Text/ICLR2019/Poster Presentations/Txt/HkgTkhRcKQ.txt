Published as a conference paper at ICLR 2019
AdaShift: Decorrelation and Convergence of
Adaptive Learning Rate Methods
Zhiming Zhou。Qingru Zhang* ^, Guansong Lu, HongWei Wang, Weinan Zhang, Yong Yu
Shanghai Jiao Tong University
%eyohai@apex.sjtu.edu.cn, ^neverquit@sjtu.edu.cn
Ab stract
Adam is shown not being able to converge to the optimal solution in certain
cases. Researchers recently propose several algorithms to avoid the issue of non-
convergence of Adam, but their efficiency turns out to be unsatisfactory in prac-
tice. In this paper, we provide a new insight into the non-convergence issue of
Adam as well as other adaptive learning rate methods. We argue that there exists
an inappropriate correlation between gradient gt and the second moment term vt
in Adam (t is the timestep), which results in that a large gradient is likely to have
small step size while a small gradient may have a large step size. We demonstrate
that such unbalanced step sizes are the fundamental cause of non-convergence of
Adam, and we further prove that decorrelating vt and gt will lead to unbiased
step size for each gradient, thus solving the non-convergence problem of Adam.
Finally, we propose AdaShift, a novel adaptive learning rate method that decorre-
lates vt and gt by temporal shifting, i.e., using temporally shifted gradient gt-n
to calculate vt . The experiment results demonstrate that AdaShift is able to ad-
dress the non-convergence issue of Adam, while still maintaining a competitive
performance with Adam in terms of both training speed and generalization.
1	Introduction
First-order optimization algorithms with adaptive learning rate play an important role in deep learn-
ing due to their efficiency in solving large-scale optimization problems. Denote gt ∈ Rn as the
gradient of loss function f with respect to its parameters θ ∈ Rn at timestep t, then the general
updating rule of these algorithms can be written as follows (Reddi et al., 2018):
θt+1=θt-√t mt.
(1)
In the above equation, mt , φ(g1, . . . , gt) ∈ Rn is a function of the historical gradients; vt ,
ψ(g1, . . . , gt) ∈ Rn+ is an n-dimension vector with non-negative elements, which adapts the learning
rate for the n elements in gt respectively; at is the base learning rate; and √t is the adaptive step
vt
size for mt.
One common choice ofφ(g1, . . . , gt) is the exponential moving average of the gradients used in Mo-
mentum (Qian, 1999) and Adam (Kingma & Ba, 2014), which helps alleviate gradient oscillations.
The commonly-used ψ(g1, . . . , gt) in deep learning community is the exponential moving average
of squared gradients, such as Adadelta (Zeiler, 2012), RMSProp (Tieleman & Hinton, 2012), Adam
(Kingma & Ba, 2014) and Nadam (Dozat, 2016).
Adam (Kingma & Ba, 2014) is a typical adaptive learning rate method, which assembles the idea
of using exponential moving average of first and second moments and bias correction. In general,
Adam is robust and efficient in both dense and sparse gradient cases, and is popular in deep learning
research. However, Adam is shown not being able to converge to optimal solution in certain cases.
Reddi et al. (2018) point out that the key issue in the convergence proof of Adam lies in the quantity
vt-1
αt-1
(2)
* Equally contributed.
1
Published as a conference paper at ICLR 2019
which is assumed to be positive, but unfortunately, such an assumption does not always hold in
Adam. They provide a set of counterexamples and demonstrate that the violation of positiveness of
Γt will lead to undesirable convergence behavior in Adam.
Reddi et al. (2018) then propose two variants, AMSGrad and AdamNC, to address the issue by
keeping Γt positive. Specifically, AMSGrad defines Vt as the historical maximum of vt, i.e., Vt =
max {vi}t=ι, and replaces Vt with Vt to keep Vt non-decreasing and therefore forces Γt to be
positive; while AdamNC forces vt to have “long-term memory” of past gradients and calculates vt
as their average to make it stable. Though these two algorithms solve the non-convergence problem
of Adam to a certain extent, they turn out to be inefficient in practice: they have to maintain a very
large Vt once a large gradient appears, and a large Vt decreases the adaptive learning rate √t and
slows down the training process.
In this paper, we provide a new insight into adaptive learning rate methods, which brings a new
perspective on solving the non-convergence issue of Adam. Specifically, in Section 3, we study the
counterexamples provided by Reddi et al. (2018) via analyzing the accumulated step size of each
gradient gt. We observe that in the common adaptive learning rate methods, a large gradient tends to
have a relatively small step size, while a small gradient is likely to have a relatively large step size.
We show that the unbalanced step sizes stem from the inappropriate positive correlation between Vt
and gt, and we argue that this is the fundamental cause of the non-convergence issue of Adam.
In Section 4, we further prove that decorrelating Vt and gt leads to equal and unbiased expected step
size for each gradient, thus solving the non-convergence issue of Adam. We subsequently propose
AdaShift, a decorrelated variant of adaptive learning rate methods, which achieves decorrelation
between Vt and gt by calculating Vt using temporally shifted gradients. Finally, in Section 5, we
study the performance of our proposed AdaShift, and demonstrate that it solves the non-convergence
issue of Adam, while still maintaining a decent performance compared with Adam in terms of both
training speed and generalization.
2	Preliminaries
Adam. In Adam, mt and Vt are defined as the exponential moving average of gt and gt2 :
mt = β1 mt-1 + (1 - β1)gt and Vt = β2Vt-1 + (1 - β2)gt2,	(3)
where β1 ∈ [0, 1) and β2 ∈ [0, 1) are the exponential decay rates for mt and Vt, respectively, with
m0 = 0 and V0 = 0. They can also be written as:
tt
mt = (1-β1)Xβ1t-igi and Vt = (1-β2)Xβ2t-igi2.	(4)
i=1	i=1
To avoid the bias in the estimation of the expected value at the initial timesteps, Kingma & Ba (2014)
propose to apply bias correction to mt and Vt . Using mt as instance, it works as follows:
m = (1-βι) Pt=ι βt-igi = Pt=ι βt-igi = (1-βι) Pi=ι βt-igi	(5)
t (1-βι) Pi=I βt-i	Pt=I βt-i	1-β1	.
Online optimization problem. An online optimization problem consists of a sequence of cost
functions f1(θ), . . . , ft(θ), . . . , fT (θ), where the optimizer predicts the parameter θt at each time-
step t and evaluate it on an unknown cost function ft(θ). The performance of the optimizer is usually
evaluated by regret R(T)，PT=ι[ft(θt) - ft (θ*)],WhiChiSthe sum of the difference between the
online prediction ft(θt) and the best fixed-point parameter prediction ft(θ*) for all the previous
steps, where θ* = arg mi□θ∈4 PT=I ft (θ) is the best fixed-point parameter from a feasible set H.
Counterexamples. Reddi et al. (2018) highlight that for any fixed β1 and β2, there exists an online
optimization problem where Adam has non-zero average regret, i.e., Adam does not converge to
optimal solution . The counterexamples in the sequential version are given as follows:
ft(θ) =
Cθ,
-θ,
if t mod d = 1;
otherwise,
(6)
2
Published as a conference paper at ICLR 2019
where C is a relatively large constant and d is the length of an epoch. In Equation 6, most gradients
of ft (θ) with respect to θ are -1, but the large positive gradient C at the beginning of each epoch
makes the overall gradient of each epoch positive, which means that one should decrease θt to
minimize the loss. However, according to (Reddi et al., 2018), the accumulated update of θ in Adam
under some circumstance is opposite (i.e., θt is increased), thus Adam cannot converge in such case.
Reddi et al. (2018) argue that the reason of the non-convergence of Adam lies in that the positive
assumption of Γt，(√Vt∕ɑt - √vt-ι∕ɑt-ι) does not always hold in Adam.
The counterexamples are also extended to stochastic cases in (Reddi et al., 2018), where a finite
set of cost functions appear in a stochastic order. Compared with sequential online optimization
counterexample, the stochastic version is more general and closer to the practical situation. For the
simplest one dimensional case, at each timestep t, the function ft(θ) is chosen as i.i.d.:
ft(θ)
Cθ,
-θ,
with probability P = e^;
with probability 1 - P = C+1 ,
(7)
where δ is a small positive constant that is smaller than C . The expected cost function of the above
problem is F(θ) = -11++1 Cθ — C-δθ = δθ, therefore, one should decrease θ to minimize the loss.
Reddi et al. (2018) prove that when C is large enough, the expectation of accumulated parameter
update in Adam is positive and results in increasing θ.
Basic Solutions Reddi et al. (2018) propose maintaining the strict positiveness of Γt as solution,
for example, keeping vt non-decreasing or using increasing β2 . In fact, keeping Γt positive is not
the only way to guarantee the convergence of Adam. Another important observation is that for
any fixed sequential online optimization problem with infinitely repeating epochs (e.g., Equation 6),
Adam will converge as long as β1 is large enough. Formally, we have the following theorem:
Theorem 1 (The influence of β1 ). For any fixed sequential online convex optimization problem
with infinitely repeating of finite length epochs (d is the length of an epoch), if ∃G ∈ R such that
∣∣Vft(θ)k∞ ≤ G and ∃T ∈ N, ∃e2 > eι > 0 such that eι < √0= G2 < €2 holds for all t > T, then,
for any fixed β2 ∈ [0, 1), there exists a β1 ∈ [0, 1) such that Adam has average regret ≤ 2;
The intuition behind Theorem 1 is that, if β1 → 1, then mt → Pid=1 gi ∕d, i.e., mt approaches
the average gradient of an epoch, according to Equation 5. Therefore, no matter what the adaptive
learning rate at∕√vt is, Adam will always converge along the correct direction.
3	The cause of non-convergence: unbalanced step size
In this section, we study the non-convergence issue by analyzing the counterexamples provided
by Reddi et al. (2018). We show that the fundamental problem of common adaptive learning rate
methods is that: vt is positively correlated to the scale of gradient gt, which results in a small step
size at∕√vt for a large gradient, and a large step size for a small gradient. We argue that such an
unbalanced step size is the cause of non-convergence.
We will first define net update factor for the analysis of the accumulated influence of each gradient
gt, then apply the net update factor to study the behaviors of Adam using Equation 6 as an example.
The argument will be extended to the stochastic online optimization problem and general cases.
3.1	Net update factor
When β1 6= 0, due to the exponential moving effect of mt, the influence of gt exists in all of its
following timesteps. For timestep i (i ≥ t), the weight ofgt is (1 - β1)β1i-t. We accordingly define
a new tool for our analysis: the net update net(gt) of each gradient gt, which is its accumulated
influence on the entire optimization process:
∞
net(gt), X √v=[(I- βι)βi-tgt]=k(gt) ∙ gt,
∞
where k(gt) = X √i= (1 - βι)βi-t
i=t	vi
(8)
and we call k(gt) the net update factor of gt, which is the equivalent accumulated step size for
gradient gt. Note that k(gt) depends on {vi}i∞=t, and in Adam, if β1 6= 0, then all elements in
{vi}i∞=t are related to gt. Therefore, k(gt) is a function ofgt.
3
Published as a conference paper at ICLR 2019
It is worth noticing that in Momentum method, vt is equivalently set as 1. Therefore, we have
k(gt) = αt and net(gt) = αtgt, which means that the accumulated influence of each gradient gt
in Momentum is the same as vanilla SGD (Stochastic Gradient Decent). Hence, the convergence
of Momentum is similar to vanilla SGD. However, in adaptive learning rate methods, vt is function
over the past gradients, which makes its convergence nontrivial.
3.2	Analysis on online optimization counterexamples
Note that vt exists in the definition of net update factor (Equation 8). Before further analyzing the
convergence of Adam using the net update factor, we first study the pattern of vt in the sequential
online optimization problem in Equation 6. Since Equation 6 is deterministic, we can derive the
formula of vt as follows:
Lemma 2. In the sequential online optimization problem in Equation 6, denote β1, β2 ∈ [0, 1) as
the decay rates, d ∈ N as the length of an epoch, n ∈ N as the index of epoch, and i ∈ {1, 2, ..., d}
as the index of timestep in one epoch. Then the limit of vnd+i when n → ∞ is:
lim Vnd+i = 1-βd (C2 - 1)βi-1 + 1.	(9)
n→∞	1-β2d
Given the formula of vt in Equation 9, we now study the net update factor of each gradient. We start
with a simple case where β1 = 0. In this case we have
lim k(gnd+i) = lim	Qt	.	(10)
n→∞	n→∞ √Vnd+i
Since the limit of vnd+i in each epoch monotonically decreases with the increase of index i accord-
ing to Equation 9, the limit of k(gnd+i) monotonically increases in each epoch. Specifically, the
first gradient gnd+1 = C in epoch n represents the correct updating direction, but its influence is the
smallest in this epoch. In contrast, the net update factor of the subsequent gradients -1 are relatively
larger, though they indicate a wrong updating direction.
We further consider the general case where β1 6= 0. The result is presented in the following lemma:
Lemma 3. In the sequential online optimization problem in Equation 6, when n → ∞, the limit of
net update factor k(gnd+i) of epoch n satisfies: ∃ 1 ≤ j ≤ d such that
lim k(C) = lim k(gnd+ι) < lim k(gnd+2) < …< lim k(gnd+j),	(11)
n→∞	n→∞	n→∞	n→∞
and
lim k(gnd+j) > lim k(gnd+j+ι) > …> lim k(gnd+d+ι) = lim k(C),	(12)
n→∞	n→∞	n→∞	n→∞
where k(C) denotes the net update factor for gradient gi = C.
Lemma 3 tells us that, in sequential online optimization problem in Equation 6, the net update
factors are unbalanced. Specifically, the net update factor for the large gradient C is the smallest in
the entire epoch, while all gradients -1 have larger net update factors. Such unbalanced net update
factors will possibly lead Adam to a wrong accumulated update direction.
Similar conclusion also holds in the stochastic online optimization problem in Equation 7. We derive
the expectation of the net update factor for each gradient in the following lemma:
Lemma 4. In the stochastic online optimization problem in Equation 7, assuming αt = 1, it holds
that k(C) < k(-1), where k(C) denote the expectation net update factor for gi = C and k(-1)
denote the expectation net update factor for gi = -1.
Though the formulas of net update factors in the stochastic case are more complicated than those in
deterministic case, the analysis is actually more easier: the gradients with the same scale share the
same expected net update factor, so we only need to analyze k(C) and k(-1). From Lemma 4, we
can see that in terms of the expectation net update factor, k(C) is smaller than k(-1), which means
the accumulated influence of gradient C is smaller than gradient -1.
4
Published as a conference paper at ICLR 2019
3.3	Analysis on non-convergence of Adam
As we have observed in the previous section, a common characteristic of these counterexamples
is that the net update factor for the gradient with large magnitude is smaller than these with small
magnitude. The above observation can also be interpreted as a direct consequence of inappropriate
correlation between vt and gt. Recall that vt = β2vt-1 + (1 -β2)gt2. Assuming vt-1 is independent
of gt, then: when a new gradient gt arrives, if gt is large, vt is likely to be larger; and if gt is small,
Vt is also likely to be smaller. If βι = 0, then k(gt) = αt∕√t. As a result, a large gradient is likely
to have a small net update factor, while a small gradient is likely to have a large net update factor in
Adam.
When it comes to the scenario where β1 > 0, the arguments are actually quite similar. Given vt =
β2vt-1 + (1 - β2)gt2. Assuming vt-1 and {gt+i}i∞=1 are independent from gt, then: not only does vt
positively correlate with the magnitude ofgt, but also the entire infinite sequence {vi}i∞=t positively
correlates with the magnitude ofgt. Since the net update factor k(gt) =P∞=t α,∕g∖- βι)βi-t
negatively correlates with each viin {vi}i∞=t, it is thus negatively correlated with the magnitude of
gt. That is, k(gt) for a large gradient is likely to be smaller, while k(gt) for a small gradient is likely
to be larger.
The unbalanced net update factors cause the non-convergence problem of Adam as well as all other
adaptive learning rate methods where vt correlates with gt. To construct a counterexample, the same
pattern is that: the large gradient is along the “correct” direction, while the small gradient is along
the opposite direction. Due to the fact that the accumulated influence of a large gradient is small
while the accumulated influence of a small gradient is large, Adam may update parameters along
the wrong direction.
Finally, we would like to emphasize that even if Adam updates parameters along the right direc-
tion in general, the unbalanced net update factors are still unfavorable since they slow down the
convergence.
4	The proposed method: decorrelation via temporal shifting
According to the previous discussion, we conclude that the main cause of the non-convergence of
Adam is the inappropriate correlation between vt and gt . Currently we have two possible solutions:
(1) making vt act like a constant, which declines the correlation, e.g., using a large β2 or keep vt non-
decreasing (Reddi et al., 2018); (2) using a large β1 (Theorem 1), where the aggressive momentum
term helps to mitigate the impact of unbalanced net update factors. However, neither of them solves
the problem fundamentally.
The dilemma caused by vt enforces us to rethink its role. In adaptive learning rate methods, vt
plays the role of estimating the second moments of gradients, which reflects the scale of gradient on
average. With the adaptive learning rate at∕√vt, the update step of gt is scaled down by √Vt and
achieves rescaling invariance with respect to the scale of gt, which is practically useful to make the
training process easy to control and the training system robust. However, the current scheme of vt,
i.e., vt = β2 vt-1 + (1 - β2)gt2, brings a positive correlation between vt and gt, which results in
reducing the effect of large gradients and increasing the effect of small gradients, and finally causes
the non-convergence problem. Therefore, the key is to let vt be a quantity that reflects the scale of
the gradients, while at the same time, be decorrelated with current gradient gt . Formally, we have
the following theorem:
Theorem 5 (Decorrelation leads to convergence). For any fixed online optimization problem with
infinitely repeating of a finite set of cost functions {f1 (θ), . . . , ft(θ), . . . fn(θ)}, assuming β1 = 0
and αt is fixed, we have, if vt follows a fixed distribution and is independent of the current gradient
gt, then the expected net update factor for each gradient is identical.
Let Pv denote the distribution of vt . In the infinitely repeating online optimization scheme, the
expectation of net update factor for each gradient gt is
∞
E[k(gt)] = X Evi 〜Pv [ -√= (1 — βι)βi-t].	(13)
i=t	i
5
Published as a conference paper at ICLR 2019
Given Pv is independent of gt, the expectation of the net update factor E[k(gt)] is independent of
gt and remains the same for different gradients. With the expected net update factor being a fixed
constant, the convergence of the adaptive learning rate method reduces to vanilla SGD.
Momentum (Qian, 1999) can be viewed as setting vt as a constant, which makes vt and gt indepen-
dent. Furthermore, in our view, using an increasing β2 (AdamNC) or keeping Vt as the largest Vt
(AMSGrad) is also to make vt almost fixed. However, fixing vt is not a desirable solution, because
it damages the adaptability of Adam with respect to the adapting of step size.
We next introduce the proposed solution to make Vt independent of gt, which is based on temporal
independent assumption among gradients. We first introduce the idea of temporal decorrelation,
then extend our solution to make use of the spatial information of gradients. Finally, we incorporate
first moment estimation. The pseudo code of the proposed algorithm is presented as follows.
Algorithm 1 AdaShift: Temporal Shifting with Block-wise Spatial Operation
Input： n, βι, β2, Φ, θo, {ft(θ)}T=1, {αt}T=1, {g-t}n-01,
1:	set V0 = 0
2:	for t = 1 to T do
3:	gt = Vft(%)
4:	mt = Pn-01 βi gt-i/Pn-o1 βi
5:	for i = 1 to M do
6:	vt[i] = β2vt-1[i] + (1 - β2)φ(g2-n[i])
7:	θt[i] = θt-i[i] 一 αt/Pvt[i] ∙ m/i]
8:	end for
9:	end for
10:	// We ignore the bias-correction, epsilon and other misc for the sake of clarity
4.1	Temporal decorrelation
In practical setting, ft(θ) usually involves different mini-batches xt, i.e., ft (θ) = f(θ; xt). Given
the randomness of mini-batch, we assume that the mini-batch xt is independent of each other and
further assume that f(θ; x) keeps unchanged over time, then the gradient gt = Vf (θ; xt) of each
mini-batch is independent of each other.
Therefore, we could change the update rule for vt to involve gt-n instead ofgt, which makes vt and
gt temporally shifted and hence decorrelated:
vt = β2vt-1 + (1 一 β2 )gt2-n.
(14)
Note that in the sequential online optimization problem, the assumption “gt is independent of each
other” does not hold. However, in the stochastic online optimization problem and practical neural
network settings, our assumption generally holds.
4.2	Making use of the spatial elements of previous timesteps
Most optimization schemes involve a great many parameters. The dimension ofθ is high, thus gt and
vt are also of high dimension. However, vt is element-wisely computed in Equation 14. Specifically,
we only use the i-th dimension of gt-n to calculate the i-th dimension of vt. In other words, it only
makes use of the independence between gt-n[i] and gt [i], where gt [i] denotes the i-th element of
gt. Actually, in the case of high-dimensional gt and vt, we can further assume that all elements of
gradient gt-n at previous timesteps are independent with the i-th dimension of gt. Therefore, all
elements in gt-n can be used to compute vt without introducing correlation. To this end, we propose
introducing a function φ over all elements of gt2-n, i.e.,
vt = β2vt-1 + (1 一 β2)φ(gt2-n).
(15)
For easy reference, we name the elements of gt-n other than gt-n[i] as the spatial elements of gt-n
and name φ the spatial function or spatial operation. There is no restriction on the choice of φ, and
we use φ(x) = maxi x[i] for most of our experiments, which is shown to be a good choice. The
maxi x[i] operation has a side effect that turns the adaptive learning rate vt into a shared scalar.
6
Published as a conference paper at ICLR 2019
An important thing here is that, we no longer interpret vt as the second moment of gt . It is merely a
random variable that is independent of gt, while at the same time, reflects the overall gradient scale.
We leave further investigations on φ as future work.
4.3	Block-wise adaptive learning rate SGD
In practical setting, e.g., deep neural network, θ usually consists of many parameter blocks, e.g., the
weight and bias for each layer. In deep neural network, the gradient scales (i.e., the variance) for dif-
ferent layers tend to be different (Glorot & Bengio, 2010; He et al., 2015). Different gradient scales
make it hard to find a learning rate that is suitable for all layers, when using SGD and Momentum
methods. In traditional adaptive learning rate methods, they apply element-wise rescaling for each
gradient dimension, which achieves rescaling-invariance and somehow solves the above problem.
However, Adam sometimes does not generalize better than SGD (Wilson et al., 2017; Keskar &
Socher, 2017), which might relate to the excessive learning rate adaptation in Adam.
In our temporal decorrelation with spatial operation scheme, we can solve the “different gradient
scales” issue more naturally, by applying φ block-wisely and outputs a shared adaptive learning rate
scalar vt[i] for each block:
vt[i] = β2vt-1[i] + (1 - β2)φ(gt2-n[i]).	(16)
It makes the algorithm work like an adaptive learning rate SGD, where each block has an adaptive
learning rate aj，v/i] while the relative gradient scale among in-block elements keep unchanged.
As illustrated in Algorithm 1, the parameters θt including the related gt and vt are divided into M
blocks. Every block contains the parameters of the same type or same layer in neural network.
4.4	Incorporating first moment estimation: moving averaging windows
First moment estimation, i.e., defining mt as a moving average of gt, is an important technique of
modern first order optimization algorithms, which alleviates mini-batch oscillations. In this section,
we extend our algorithm to incorporate first moment estimation.
We have argued that vt needs to be decorrelated with gt . Analogously, when introducing the first
moment estimation, we need to make vt and mt independent to make the expected net update factor
unbiased. Based on our assumption of temporal independence, we further keep out the latest n
gradients {gt-i}in=-01, and update vt and mt via
Vt = β2vt-1 + (1 - β2)φ(g2-n) and mt = P=；#gt-i.	(17)
i=0 β1i
In Equation 17, β1 ∈ [0, 1] plays the role of decay rate for temporal elements. It can be viewed as a
truncated version of exponential moving average that only applied to the latest few elements. Since
we use truncating, it is feasible to use large β1 without taking the risk of using too old gradients. In
the extreme case where β1 = 1, it becomes vanilla averaging.
The pseudo code of the algorithm that unifies all proposed techniques is presented in Algorithm 1
and a more detailed version can be found in the Appendix. It has the following parameters: spatial
operation φ, n ∈ N+, β1 ∈ [0, 1], β2 ∈ [0, 1) and αt.
Summary The key difference between Adam and the proposed method is that the latter temporally
shifts the gradient gt for n-step, i.e., using gt-n for calculating vt and using the kept-out n gradients
to evaluate mt (Equation 17), which makes vt and mt decorrelated and consequently solves the non-
convergence issue. In addition, based on our new perspective on adaptive learning rate methods, vt
is not necessarily the second moment and it is valid to further involve the calculation of vt with the
spatial elements of previous gradients. We thus proposed to introduce the spatial operation φ that
outputs a shared scalar for each block. The resulting algorithm turns out to be closely related to SGD,
where each block has an overall adaptive learning rate and the relative gradient scale in each block
is maintained. We name the proposed method that makes use of temporal-shifting to decorrelated vt
and mt AdaShift, which means “ADAptive learning rate method with temporal SHIFTing”.
7
Published as a conference paper at ICLR 2019
5 Experiments
In this section, we empirically study the proposed method and compare them with Adam, AMSGrad
and SGD, on various tasks in terms of training performance and generalization. Without additional
declaration, the reported result for each algorithm is the best we have found via parameter grid
search. The anonymous code is provided at http://bit.ly/2NDXX6x.
5.1	Online Optimization Counterexamples
Firstly, we verify our analysis on the stochastic online optimization problem in Equation 7, where
we set C = 101 and δ = 0.02. We compare Adam, AMSGrad and AdaShift in this experiment.
For fair comparison, we set α = 0.001, β1 = 0 and β2 = 0.999 for all these methods. The results
are shown in Figure 1a. We can see that Adam tends to increase θ, that is, the accumulate update
of θ in Adam is along the wrong direction, while AMSGrad and AdaShift update θ in the correct
direction. Furthermore, given the same learning rate, AdaShift decreases θ faster than AMSGrad,
which validates our argument that AMSGrad has a relatively higher vt that slows down the training.
In this experiment, we also verify Theorem 1. As shown in Figure 1b, Adam is also able to converge
to the correct direction with a sufficiently large β1 and β2. Note that (1) AdaShift still converges
with the fastest speed; (2) a small β1 (e.g., β1 = 0.9, the light-blue line in Figure 1b) does not make
Adam converge to the correct direction. We do not conduct the experiments on the sequential online
optimization problem in Equation 6, because it does not fit our temporal independence assumption.
To make it converge, one can use a large β1 or β2, or set vt as a constant.
Figure 1: Experiments on stochastic counterexample.
(b) Adam With large βι and β2.
5.2	Logistic Regression and Multilayer Perceptron on MNIST
We further compare the proposed method With Adam, AMSGrad and SGD by using Logistic Regres-
sion and Multilayer Perceptron on MNIST, Where the Multilayer Perceptron has tWo hidden layers
and each has 256 hidden units With no internal activation. The results are shoWn in Figure 2 and
Figure 3, respectively. We find that in Logistic Regression, these learning algorithms achieve very
similar final results in terms of both training speed and generalization. In Multilayer Perceptron,
We compare Adam, AMSGrad and AdaShift With reduce-max spatial operation (max-AdaShift) and
Without spatial operation (non-AdaShift). We observe that max-AdaShift achieves the loWest train-
ing loss, While non-AdaShift has mild training loss oscillation and at the same time achieves better
generalization. The Worse generalization of max-AdaShift may be due to overfitting in this task, and
the better generalization of non-AdaShift may stem from the regularization effect of its relatively
unstable step size.
5.3	DenseNet and ResNet on CIFAR- 1 0
ResNet (He et al., 2016) and DenseNet (Huang et al., 2017) are tWo typical modern neural netWorks,
Which are efficient and Widely-used. We test our algorithm With ResNet and DenseNet on CIFAR-
10 datasets. We use a 18-layer ResNet and 100-layer DenseNet in our experiments. We plot the
best results of Adam, AMSGrad and AdaShift in Figure 4 and Figure 5 for ResNet and DenseNet,
respectively. We can see that AMSGrad is relatively Worse in terms of both training speed and
generalization. Adam and AdaShift share competitive results, While AdaShift is generally slightly
better, especially the test accuracy of ResNet and the training loss of DenseNet.
8
Published as a conference paper at ICLR 2019
2.0
0.0 C
0
505
110
SSol CHraU÷
50000	100000 150000
iterations
l.8l.7l.6l.5
Θ Θ Θ Θ
SGD
Adam
AMSGrad
max-AdaShift
non-AdaShift
0.4 ft
0
2000 4000 6000
iterations
8000
Figure 2: Logistic Regression on MNIST.
5ΘΘπ------------------------ Θ.9
SSol 611111110.一*
Θ Θ Θ Θ Θ
O.
SGD
---Adam
AMSGrad
max-AdaShift
non-AdaShift
20000	40000 6θθθθ
iteration
Figure 3: Multilayer PercePtron on MNIST.
l.8l.6l.4l,2
Θ Θ Θ Θ
SSol 6111111.一
5000	10000 15000 2θθθθ
iterations
l.8l.7l.6
Θ Θ Θ
>urou =UUros
		SGD 	Adam AMSGrad max-AdaShift non-AdaShift	
0	20000	40000 6θθθθ
iteration
o-75o
Θ 5 Θ
9 8 8
Θ Θ Θ
=UUros
5000 10000 15000 2θθθθ
iterations
Figure 4:	ResNet on Cifar-10.

l.8l.6l.4l,2
Θ Θ Θ Θ
SSol 6111111.一
100
iterations
0-700
75
=UUros
50	100	150
iterations
Figure 5:	DenSeNet on Cifar-10.
5.4	DenseNet with Tiny-ImageNet
We further increase the comPlexity of dataset, switching from CIFAR-10 to Tiny-ImageNet, and
comPare the Performance of Adam, AMSGrad and AdaShift with DenseNet. The results are shown
in Figure 6, from which we can see that the training curves of Adam and AdaShift are basically
overlaPPed, but AdaShift achieves higher test accuracy than Adam. AMSGrad has relatively higher
training loss, and its test accuracy is relatively lower at the initial stage.
5.5	Generative model and Recurrent model
We also test our algorithm on the training of generative model and recurrent model. We choose
WGAN-GP (Gulrajani et al., 2017) that involves LiPschitz continuity condition (which is hard to
oPtimize), and Neural Machine Translation (NMT) (Luong et al., 2017) that involves tyPical re-
current unit LSTM, resPectively. In Figure 7a, we comPare the Performance of Adam, AMSGrad
9
Published as a conference paper at ICLR 2019
and AdaShift in the training of WGAN-GP discriminator, given a fixed generator. We notice that
AdaShift is significantly better than Adam, while the performance of AMSGrad is relatively unsat-
isfactory. The test performance in terms of BLEU of NMT is shown in Figure 7b, where AdaShift
achieves a higher BLEU than Adam and AMSGrad.
3.0
505
2 2 1
SSol 6111111.一
100	150	200	250
epoch
Figure 6: DenseNet on Tiny-ImageNet.
0.5
.4.3
Θ Θ
=UUros
O 50	100	150	200	25Θ
epoch
-3750
0
50000	100000
iteration
(a) Training WGAN Discriminator.
Figure 7: Generative and Recurrent model.
5
O 5000	10000	15000
iterations
(b) Neural Machine Translation BLEU.
6 Conclusion
In this paper, we study the non-convergence issue of adaptive learning rate methods from the per-
spective of the equivalent accumulated step size of each gradient, i.e., the net update factor defined in
this paper. We show that there exists an inappropriate correlation between vt and gt , which leads to
unbalanced net update factor for each gradient. We demonstrate that such unbalanced step sizes are
the fundamental cause of non-convergence of Adam, and we further prove that decorrelating vt and
gt will lead to unbiased expected step size for each gradient, thus solving the non-convergence prob-
lem of Adam. Finally, we propose AdaShift, a novel adaptive learning rate method that decorrelates
vt and gt via calculating vt using temporally shifted gradient gt-n.
In addition, based on our new perspective on adaptive learning rate methods, vt is no longer nec-
essarily the second moment of gt, but a random variable that is independent of gt and reflects the
overall gradient scale. Thus, it is valid to calculate vt with the spatial elements of previous gradi-
ents. We further found that when the spatial operation φ outputs a shared scalar for each block, the
resulting algorithm turns out to be closely related to SGD, where each block has an overall adaptive
learning rate and the relative gradient scale in each block is maintained. The experiment results
demonstrate that AdaShift is able to solve the non-convergence issue of Adam. In the meantime,
AdaShift achieves competitive and even better training and testing performance when compared
with Adam.
References
Timothy Dozat. Incorporating nesterov momentum into adam. International Conference on Learn-
ing Representations, Workshop track, 2016.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pp. 249-256, 2010.
10
Published as a conference paper at ICLR 2019
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved training of wasserstein gans. In Advances in Neural Information Processing Systems, pp.
5767-5777, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. 2017.
Nitish Shirish Keskar and Richard Socher. Improving generalization performance by switching from
adam to sgd. arXiv preprint arXiv:1712.07628, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Minh-Thang Luong, Eugene Brevdo, and Rui Zhao. Neural machine translation (seq2seq) tutorial.
https://github.com/tensorflow/nmt, 2017.
Ning Qian. On the momentum term in gradient descent learning algorithms. Neural networks, 12
(1):145-151, 1999.
Ali Rahimi and Ben Recht. test of time talk at nips 2017. URL http://www.argmin.net/
2017/12/11/alchemy-addendum/.
Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In
International Conference on Learning Representations, 2018. URL https://openreview.
net/forum?id=ryQu7f-RZ.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26-
31, 2012.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. In Advances in Neural Information
Processing Systems, pp. 4148-4158, 2017.
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,
2012.
11
Published as a conference paper at ICLR 2019
(a) Final result of θ for sequential problem
after 2000 updates, varied with β1 and β2.
(b) Critical value of C with varying β1 and
β2 under the sequential optimization setting.
(c) Final result of θ for stochastic problem
after 2000 updates, varied with β1 and β2 .
Figure 8: Both β1 and β2 influence the direction and speed of optimization in Adam. Critical value
of Ct , at which Adam gets into non-convergence, increases as β1 and β2 getting large. Leftmost two
for the sequential online optimization problem and rightmost two for stochastic online problem.
(d) Critical value of C with varying β1 and
β2 under the stochastic optimization setting.
A THE RELATION AMONG β1, β2 AND C
To provide an intuitive impression on the relation among C, d, β1, β2 and the convergence of Adam,
we let C = d = 6, initialize θ1 = 0, vary β1 and β2 among [0, 1) and let Adam go through
2000 timesteps (iterations). The final result of θ is shown in Figure 8a. It suggests that for a fixed
sequential online optimization problem, both of β1 and β2 determine the direction and speed of
Adam optimization process. Furthermore, we also study the threshold point of C and d, under
which Adam will change to the incorrect direction, for each fixed β1 and β2 that vary among [0, 1).
To simplify the experiments, we keep d = C such that the overall gradient of each epoch being +1.
The result is shown in Figure 8b, which suggests, at the condition of larger β1 or larger β2, it needs
a larger C to make Adam stride on the opposite direction. In other words, large β1 and β2 will make
the non-convergence rare to happen.
We also conduct the experiment in the stochastic problem to analyze the relation among C, β1, β2
and the convergence behavior of Adam. Results are shown in the Figure 8c and Figure 8d and the
observations are similar to the previous: larger C will cause non-convergence more easily and a
larger β1 or β2 somehow help to resolve non-convergence issue. In this experiment, we set δ = 1.
Lemma 6 (Critical condition). In the sequential online optimization problem Equation 6, let αt
being fixed, define S(β1, β2, C, d) to be the sum of the limits of step updates in a d-step epoch:
S(β1,β2,C) , XX lim md⅛ .	(18)
i=1 nd→∞ vnd+i
Let S(β1 , β2 , C) = 0, assuming β2 and C are large enough such that vt	1, we get the equation:
C+1
(1- βd)(√βd - βd)α-√⅞)
(I- βI)(Ve2 - βI)(I- p∕βd)
(19)
Equation 19, though being quite complex, tells that both β1 and β2 are closely related to the coun-
terexamples, and there exists a critical condition among these parameters.
12
Published as a conference paper at ICLR 2019
B Al len the In 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17	The AdaShift Pseudo Code gorithm 2 AdaShift: We use a first-in-first-out queue Q to denote the averaging window with the gth ofn. P ush(Q, gt) denotes pushing vector gt to the tail of Q, while P op(Q) pops and returns head vector of Q. And W is the weight vector calculated via β1. put: n, β1, β2, φ, , θ0, {ft(θ)}tT=1, {αt}tT=1 : set v0 = 0, p0 = 1 w = [βn-1,βn-2,…，β1,i]∕pn=01 βn : for t = 1 to T do gt = Vft(θt) : if t ≤ n then :	P ush(Q, gt) : else :	gt-n = P op(Q) :	P ush(Q, gt) mt = W ∙ Q :	pt = pt-1β2 :	for i = 1 to M do vt[i] = β2vt-1[i] + (I- β2)φ(g2-n[i]) θt[i] = θt-i[i] - αt∕(pvt[i]∕(f--pt) + e) ∙ m/i] :	end for : end if : end for
We provided the anonymous code where a Tensorflow implementation of this algorithm is available.
C CORRELATION BETWEEN gt AND vt
In order to verify the correlation between gt and vt in Adam and AdaShift, we conduct experiments
to calculate the correlation coefficient between gt and vt . We train the Multilayer Perceptron on
MNIST until converge and gather the gradient of the second hidden layer of each step. Based on
these data, we calculate vt and the correlation coefficient between gt [i] and gt-n [i], between gt [i]
andgt-n[j] and between gt [i] and vt[i] of the last 10 epochs using the Pearson correlation coefficient,
which is formulated as follows:
P =	Pn=I(Xi- X)(Yi - Y)
JP 乙(χi- x)2 JPNI(K - Y)2.
To verify the temporal correlation between gt [i] and gt-n [i], we range n from 1 to 10 and calculate
the average temporal correlation coefficient of all variables i. Results are shown in Table 1.
Table 1: Temporal correlation coefficient between gt[i] and gt-n [i].
n	1	2	3	4	5
P	-0.000368929	-0.000989286	-0.001540511	-0.00116966	-0.001613395
n	6	=	7	=	8	=	9	=	10	=
P	-0.00121172T	O000357474~	-0.00082293~	-0.00175523T-	-0.00126764Γ^
To verify the spatial correlation between gt[i] and gt-n [j], we again range n from 1 to 10 and
randomly sample some pairs of i and j and calculate the average spatial correlation coefficient of all
the selected pairs. Results are shown in Table 2.
To verify the correlation between gt [i] and vt [i] within Adam, we calculate vt and the average
correlation coefficient between gt2 and vt of all variables i. The result is 0.435885276.
To verify the correlation between gt-n [i] and vt [i] within non-AdaShift and between gt-n [i] and
vt within max-AdaShift, we range the keep number n from 1 to 10 to calculate vt and the average
correlation coefficient of all variables i. The result is shown in Table 3 and Table 4.
13
Published as a conference paper at ICLR 2019
Table 2: Spatial correlation coefficient between gt [i] and gt-n [j].
n	1	2	3	4	5
ρ	-0.000609471	-0.001948853	-0.001426661	0.000904615	0.000329359
n	6	=	7	=	8	=	9	=	10	=
ρ	0.000971337~	-0.000644563~	-0.0013780「	-0.001147973~	-0.000592037~
Table 3: Correlation coefficient between gt2-n [i] and vt [i] in non-AdaShift.
n	1	2	3	4	5
ρ	-0.010897023~	-0.010952548	-0.010890854	-0.010853069	-0.010810747
n	6	7	=	8	=	9	=	10	=
ρ	-0.010777789~	-0.01075946—	-0.010739279~	-0.010728553~	-0.010720019~
Table 4: Correlation coefficient between gt2-n [i] and vt in max-AdaShift.
n	1	2	3	4	5
ρ	-0.000706289	-0.000794959	-0.00076306	-0.000712474	-0.000668459
n	6	=	7	=	8	=	9	=	10	=
ρ	-0.000623162^	-0.000566573~	-0.000542046~	-0.000598015~	-0.000592707~
D Proof of Theorem 1
Proof.
With bias correction, the formulation of mt is written as follows
=(1-βι) Pi= βt-ig = Pt=I βt-igi
’2 (1 - βι) Pt= β1-	Pt= βT-
According to L’Hospitals rule, we can draw the following:
(20)
t
lim X β1t-i
β1→1	1
i=1
lim」
β1→1 1 -β1
t.
Thus,
Pti=1 gi
lim mt = -i=-.
β1→1	t
According to the definition of limitation, let g* = Ei=I g, We have, ∀e > 0, ∃βι ∈ (0,1), such that
∣∣mt - g*k∞ < e.
*
We set e to be | % |, then for each dimension of mt, i.e. mt [i],
4 ≤ mt[i]≤ 口
2t 2
So, mt shares the same sign with g* in every dimension.
Given it is a convex optimization problem, let the optimal parameter be θ*, and the maximum step
size is √α= G that holds e\/G < √^G < e2/G, we have,
lim ∣∣θt- θ*∣∣∞ <e2∕G.	(21)
t→∞
Given ∣∣Vft(θ)∣∞ ≤ G, we have ft(θ) - ft(θ*) < e?, which implies the average regret
T
R(T)∕T = X[ft(θt) - ft(θ*)]∕T < e2.	(22)
t=1
□
14
Published as a conference paper at ICLR 2019
E Proof of Lemma 2
Proof. Let β1,β2 ∈ [0,1) , d ∈ N , 1 ≤ i ≤ d and i ∈ N.
nd+i
mnd+i =(1 - βι) X βnd+i-jgj
j=ι
= (I- βI)
=(I- βI)
n	nd+i-1
(C +1) X β1d+i-1- X βj
j=0	j=0
(n+1)d	nd+i
1⅛⅜^ βi-1(c+d - ⅛⅞τ
1 - β1	1 - β1
1 - β(n+ι)d
1 - βd
(1 - βι)βi-1(c+1) - (1 - βnd+i)
For a fixed d, as n approach infinity, We get the limit of mnd+i as:
lim mnd+i
nd→∞
Γ≡⅛(C +1)βL- 1
Similarly, for Vnd+i:
vnd+i
n
=(1-β2) (C2 -1)£βjd+i-1
j=0
nd+i-1
+ X β2j
j = 0
=(1 - β2)
(n+1)d	nd+i
1- J	βi-1(C2 -1)+ 1 - β2
1 - βg	2	1	/	1 - β2
1	β (n+1)d
:β ad (1 - β2)βi-1(c+1) - (1 - βnd+i)
1 - β2
For a fixed d, as n approach infinity, we get the limit of vnd+i as:
lim vnd+i
nd→∞
1-β2 (c2 - 1)βi-1 +1.
1 - βd'	β2	+
□
F Proof of Lemma 3
Proof. First, we define Vi as:
1
Vei = lim
nd→∞ √vnd+i
-1)β2iT)
+1
where 1 ≤ i ≤ d and i ∈ N. And Vi has a period of d. Let t = t - nd, then we can draw:
15
Published as a conference paper at ICLR 2019
lim k(gnd+i)
nd→∞
∞
X
t=nd+i
(1- βι)βt-ndτ
%(C2 - 1)β2t-1)% d + 1
0
XX	(1- βι)βt-i
t0=i r电(C2- 1)β2t0T)% d + 1
∞ ld+i-1	00
X X (1 - βι)β1 - ∙ Vj00
l=1 j00 =(l-1)d+i
∞	i+d-1	0
Xβ1lτ)d X (1-βι)βj-i ∙ V
l=1	j0=i
XX β(lτ)dχ(1-βι)β1∙V于
l=1	j=0
∞
X β1(l-1)d
l=1
d-1
β1 ^X(I - βI)βj ∙ V+i+1 + (I- βI)(I- βd ) ∙ V
j=0
∞
=βι∙ lim k(gnd+i+ι) + X β(j)d(1 - βι)(1- βd) ∙ V
nd→∞
l=1
Thus, we can get the forward difference of k(gnd+i) as:
lim k(gnd+i+1) -
nd→
lim
nd→
∞
k(gnd+i) = X β1(l-1)d
l=1
∞∞
(I- βI)2 X βj ∙ Vej+i+1 + (I- βI)2 X βj •巧
j=0	j=0
=(1 -βι)2 X e(l-1)dX βj ∙ [V- - T
l=1	j=0
Vnd+1i monotonically increases within one period, when 1 ≤ i ≤ d and i ∈ N. And the weigh
β1j for every difference term Vej+i+1 - Vei is fixed when i varies. Thus, the weighted summa-
tion PdlI βj ∙ [vj+i+ι - Vei is monotonically decreasing from positive to negative. In other
words, the forward difference is monotonically decreasing, such that there exists j , 1 ≤ j ≤ d and
lim k(gnd+1) is the maximum among all net updates. Moreover, itis obvious that lim k(gnd+1)
nd→∞	nd→∞
is the minimum.
Hence, we can draw the conclusion: ∃1 ≤ j ≤ d, such that
lim k(C) = lim k(gnd+ι) < lim k(gnd+2) < …< lim k(gnd+j)
nd→∞	nd→∞	nd→∞	nd→∞
and
lim	k(gnd+j)	> lim	k(gnd+j+ι)	> •一> lim	k(gnd+d+ι)= lim k(C),
nd→∞	nd→∞	nd→∞	nd→∞
where K (C) is the net update factor for gradient gi = C.	□
G	Proof of Lemma 4
Lemma 7. 1 For a bounded random variable X and a differentiable function f(x), the expectation
of f (X) is as follows:
E[f(X)] = f(E[X]) + f (E[X])D(X) + R3	(23)
1See detial in: https://stats.stackexchange.com/questions/5782/variance-of-a-function-of-one-random-
variable
16
Published as a conference paper at ICLR 2019
where D(X) is variance of X, and R3 is as follows:
R3 = f^(-)E(X - EX])3+
/
|x-E[X]|>c
f(E[X])+f0(E[X])(x-E[X])2+f(X) dF(x)
(24)
(25)
+
F(x) is the distribution function of X. R3 is a small quantity under some condition. And c is large
enough, such that: for any > 0,
P(X ∈ [E[X] -c,E[X] +c])= P(|X -E[X]| ≤ c) ≤ 1 -
(26)
Proof. (Proof of Lemma 4 ) In the stochastic online optimization problem equation 7, the gradient
subjects the distribution as:
C-,1,
with probability p :
1+δ .
C+1 ；
with probability 1 - p :
C-δ
C+1 .
(27)
Then we can get the expectation of gi :
E[gi] = δ	(28)
E[g2] = C2 ∙ 71+δ + C-δ = C + δ(C +1)	(29)
C+ 1 C+ 1
D[gi] =C+δ(C+1)-δ2	(30)
E[gi4] = C(C2 -C+ 1)+δ(C- 1)(C2 + 1)	(31)
D[gi2] = C3 - 2C2 + C+δ(C3 - 3C2 - C - 1) -δ2(C+ 1)2	(32)
Meanwhile, under the assumption that gradients are i.i.d., the expectation and variance of vi are as
following when nd → ∞:
i
E[vi] = lim(1-β2)Xβ2i-jE[gj2] = lim (1 - β2i)E[gj2] =C+δ(C+1)	(33)
i→∞	i→∞
j=1
i
D[vi] = lim (1 - β2) X β2i-j D[gj2] = lim (1 - β2i)D[gj2] = D[gj2]	(34)
i→∞	i→∞
j=1
Then, for the gradient gi, the net update factor is as follows:
∞
k(gi) = X
t=0
__________________(1- βι)βt_________________
qβ2+1vi-Γ÷71-^2)β2"^∙"gF÷71-^2)Pj=Γβ2-jg2+j^
It should to be clarified that we define Ptj=1 β2t-j gi2+j equal to zero when t = 0. Then we define
Xt as:
t
Xt = βt+1 vi-ι +(1 - β2)β2 ∙ g2 +(1 - 尸2)^Xβ2 jg2+j
j=1
t
E[Xt] =βt+1E[vi-ι] + (1 - β2)βt ∙ g2 + (1- β2) Xβ2-jE[g2+j]	(35)
j=1
=βt+1E[g2] + (1 - β2)β2 ∙ g2 + (1- β2)E[g2]	(36)
=(i + β2+1 - β2 )E[g2] + (i - β2)βt ∙ g2	(37)
D[Xt] =β2(t+1)D[vi-ι]+ QTF)/ 凭t) D[g2+j]	(38)
1	- β2
β2(t+1) + (1 - β2)2(1 - β2t)] D[ 2]
β2	+	1 - β2 J D[g ]
17
Published as a conference paper at ICLR 2019
For the function f (x) = √1χ:
f (X) = W-/2
According to lemma 7, we can the expectation of f(Xt) as follows:
E[f (Xt)] = (E[Xt])-1/2 + 3(E[Xt])-5/2 ∙ D[Xt]
8
(40)
E[Xt] and D[Xt] are expressed by equation 35 and equation 38. Then we can obtain the expectation
expression of net update factor as follows:
k(gi)=Pt∞=0(1-β1)β1t
_______________1______________+ ________________3Dt_______________
√(i-β2)βt g2 + (i+βt+1-βk)E[g2]	8[(i-β2)βt 成+(i+βt+1-βt )E[g2]] 2
where Dt = D[Xk]. Then for gradient C and -1, the net update factor is as follows:
k(C) = Pt∞=0(1 - β1)β1t
and
________________1_________________I_______________3Dt________________
√(1-β2)βt C2 + (1+βt+1-βk)E[g2]	8[(l-β2)βtC2 + (1+β2+1-β2)E[g2]] 2
k(-1) = P∞ ∩(1 — βι)β1	/	1 '	+--------------3Dt------------5
1=0	[√(l-β2)βt + (1+βt+1-βk)E[g2]	8[(l-β2)βt +(1+βt+1-βt)E[g2]] 2
(41)
(42)
(43)
We can see that each term in the infinite series of k(C) is smaller than the corresponding one in
k(-1). Thus, k(C) < k(-1).
□
H Proof of Lemma 6
Proof. From Lemma 2, we can get:
lim mnd+i = 1-⅝ (C + 1)βi-1 - 1
nd→∞ √vnd+i	q 1-βd (C2 - 1)βi-1 + 1
We sum up all updates in an epoch, and define the summation as S(β1, β2, C).
d
S(β1,β2,C)=X lim
nd→∞
i=1
mnd+i
√Vnd+i
Assume β2 and C are large enough such that vt	1, we get the approximation of limit of vnd+i as:
lim	vnd+i
nd→∞
≈ 口(C 2 - 1)βi-1
Then we can draw the expression ofS(β1, β2, C) as:
d
S(β1,β2,C) =X
i=1
β2d
1i-1 - 1
，售(C 2- 1)βi-1
X 茂(C + 1)βi-1
M q 1-βd (C2 - 1)βi-τ
S 1 - βd
V(1-β2)βd-1
；(1- β2)βd-1(C - 1)
1 - β2d
d
1
. 1- β1 ∙ Pβ - βd -[
1 - βd √β2 - β V
(1 - β1)(βd - βd)√C+r
1- 82
(1-β2)βd-1
—
1 质-1
√C2-1 √82 -1
√β2d -1
(I-βd)(√β2 - 81)	√C + 1(√β2 - 1)
■)
18
Published as a conference paper at ICLR 2019
Let S(β1, β2, C) = 0, we get the equation about critical condition:
Cqi _ (i- βd)( √β2d - βd)(i -√β2)
C + 1 ——-----------；-------------Z
(I- β1)(√β2 - β1)(1 - Ped)
□
I Hyper-parameters Investigation
I.1	Hyper-parameters setting
Here, we list all hyper-parameter setting of all above experiments.
Table 5: Hyper-parameter setting of logistic regression in Figure 2.
Optimizer	learning rate	βι	β2	n
SGD	0.1	N/A	N/A	N/A
Adam	0.001	--0-	0.999	N/A
AMSGrad	0.001	~~Q~~	0.999	N/A
non-AdaShift	0.001	--0-	0.999	-1-
max-AdaShift	0.01	—	~~0~~	0.999	1
Table 6: Hyper-parameter setting of Multilayer Perceptron on MNIST in Figure 3.
Optimizer	learning rate	βι	β2	n
SGD	0.001	N/A	N/A	N/A
Adam	0.001	--0-	0.999	N/A
AMSGrad	0.001	~~(Γ~	0.999	N/A
non-AdaShift	-0.0005-	--0-	0.999	-1-
max-AdaShift	0.01	—	~~0~~	0.999	1
Table 7: Hyper-parameter setting of WGAN-GP in Figure 7a.
Optimizer	learning rate	βι	β2	n
Adam	1e-5	0	0.999	N/A
AMSGrad	1e-5	-0-	0.999	N/A
AdaShift	1.5e-4 一	-0-	0.999	1
Table 8: Hyper-parameter setting of Neural Machine Translation BLEU in Figure 7b.
Optimizer	learning rate	β1	β2	n
Adam	-0.0001-	0.9	0.999	N/A
AMSGrad	-0.0001-	^09^	0.999	N/A
AdaShift	0.01 一	~0.9~	0.999	30
Table 9: Hyper-parameter setting of ResNet on Cifar-10 in Figure 4, DenseNet on Cifar-10 in Figure
5 and DenseNet on Tiny-Imagenet in Figure 6.
Optimizer	learning rate	βι	β2	n
Adam	0.001	0.9	0.999	N/A
AMSGrad	0.001	^09^	0.999	N/A
AdaShift	0.01 一	~0.9~	0.999	10
19
Published as a conference paper at ICLR 2019
I.2	LEARNING RATE αt SENSITIVITY
In this section, we discuss the learning rate αt sensitivity of AdaShift. We set αt ∈
{0.1, 0.01, 0.001} and let n = 10, β1 = 0.9 and β2 = 0.999. The results are shown in Figure
9 and Figure 10. Empirically, we found that when using the max spatial operation, the best learning
rate for AdaShift is around ten times of Adam.
0-00
8 6 4 2
θ θ θ θ
SSol 6utut2
0.0^---———ɪ-~~------------
0	5000	10000 15000 20000
iterations
8 6 4 2
θ θ θ θ
SSol 6utut2
100
iterations
150
0.75,
0.70,
θ θ θ
Au2nuuπ+js ①+j
5000	10000 15000 20000
iterations
Figure 9: Learning rate sensitivity experiment with ResNet on CIFAR-10.
100
iterations
150
0
0
Figure 10: Learning rate sensitivity experiment with DenseNet on CIFAR-10.
I.3 β1 AND β2 SENSITIVITY
In this section, we discuss the β1 and β2 sensitivity of AdaShift. We set α = 0.01, n = 10 and
let β1 ∈ {0, 0.9} and β2 ∈ {0.9, 0.99, 0.999}. The results are shown in Figure 11 and Figure 12.
According to the results, AdaShift holds alow sensitivity to β1 and β2. In some tasks, using the first
moment estimation (with β1 = 0.9 and n = 10) or using a large β2, e.g., 0.999 can attain better
performance. The suggested parameters setting is n = 10, β1 = 0.9, β2 = 0.999.
0.0n
0
8 6 4 2
θ θ θ θ
SSol 6utut2
5000	10000 15000 20000
iterations
Figure 11: βι and β? sensitivity experiment with ReSNet on CIFAR-10.
0.75,
θ θ θ
Au2nuuπ+js ①+j
5000	10000 15000 20000
iterations
0
20
Published as a conference paper at ICLR 2019
0.0n
0
8 6 4 2
θ θ θ θ
SSol 6utut2
100
iterations
150
0.70j
θ θ θ θ
Au2nuuπ+js ①+j
100
iterations
150

0
Figure 12: βι and β2 sensitivity experiment with DenseNet on CIFAR-10.
I.4 n AND m SENSITIVITY
In this section, we discuss the n sensitivity of AdaShift. Here we also test a extended version of first
moment estimation where it only uses the latest m gradients (m ≤ n):
vt = β2vt-ι +(I—Qφ(gLn and mt = p=m-β1 gti.	(44)
i=0 β1
We set β1 = 0.9, β2 = 0.999. The results are shown in Figure 13, Figure 14 and Figure 15. In these
experiments, AdaShift is fairly stable when changing n and m. We have not find a clear pattern on
the performance change with respect to n and m.
3.0
1.0
0
5 θ 5
2 2 1
SSol 6utut2
50	100	150	200	250
epoch
0.5
θ θ
Au2nuuπ+js ①+j
0	50	100	150	200	250
epoch
Figure 13: n sensitivity experiment with DenSeNet on Tiny-ImageNet.
θ 5 θ 5
3 2 2 1
SSol 6utut2
0	50	100	150	200	250
epoch
θ θ θ
Au2nuuπ+js ①+j
0	50	100	150	200	250
epoch
Figure 14: m sensitivity experiment with DenseNet on Tiny-ImageNet.
21
Published as a conference paper at ICLR 2019
Figure 15: n and m sensitivity experiment with Neural Machine Translation BLEU.
J	Temporal-only and Spatial-only
In our proposed algorithm, we apply a spatial operation on the temporally shifted gradient gt-n
to update vt: vt[i] = β2vt-1 [i] + (1 - β2)φ(gt2-n[i]). It is based on the temporal independent
assumption, i.e., gt-n is independent of gt. And according to our argument in Section 4.2, one can
further assume every element in gt-n is independent of the i-th dimension of gt.
We purposely avoid involving the spatial elements of the current gradient gt, where the independence
might not holds: when a sample which is rare and has a large gradient appear in the mini-batch xt ,
the overall scale of gradient gt might increase. However, for the temporally already decorrelation
gt-i, further taking the advantage of the spatial irrelevance will not suffer from this problem.
We here provide extended experiments on two variants of AdaShift: (i) AdaShift (temporal-only),
which only uses the vanilla temporal independent assumption and evaluate vt with: vt = β2vt-1 +
(1 - β2)gt2-n; (ii) AdaShift (spatial-only), which directly uses the spatial elements without temporal
shifting.
θ-θ0
8 6 4 2
θ θ θ θ
SSol 6utut2
0.0 £....- --------ɪ------ɪ-—√..
0	5000	10000 15000 20000
iterations
Figure 16: ReSNetonCIFAR-10.
8 6 4 2
θ θ θ θ
SSol 6utut2
50	100
iterations
150
0.75,
θ θ θ
Au2nuuπ+js ①+j
5000	10000 15000 20000
iterations
0-700
θ θ θ θ
Au2nuuπ+js ①+j
50	100
iterations
150
0

Figure 17:	DenSeNet on OFAR-10.
22
Published as a conference paper at ICLR 2019
According to our experiments, AdaShift (temporal-only), i.e., without the spatial operation, is less
stable than AdaShift. In some tasks, AdaShift (temporal-only) works just fine; while in some other
cases, AdaShift (temporal-only) suffers from explosive gradient and requires a relatively small learn-
ing rate. The performance of AdaShift (spatial-only) is close to Adam. More experiments for
AdaShift (spatial-only) are included in the next section.
K Extended Experiments: Nadam and AdaShift(Space Only)
In this section, we extend the experiments and add the comparisons with Nadam and AdaShift
(spatial-only). The results are shown in Figure 18, Figure19 and Figure20. According to these
experiments, Nadam and AdaShift (spatial-only) share similar performence as Adam.
8 6 4 2
θ θ θ θ
SSol 6utut2
Adam
Nadam
——AMSGrad
——AdaShift
----AdaShift (spatial-only)
0.0 i----- ------ɪ-------ɪ-
0	5000 10000 15000 20000
0.75,
θ θ θ
Au2nuuπ+js ①+j
5000	10000 15000 20000
iterations
iterations
0
Figure 18:	ResNet on CIFAR-10.
0.0n
0
8 6 4 2
θ θ θ θ
SSol 6utut2
50	100
iterations
Figure 19: DenseNet on CIFAR-10.
θ 5 θ 5
3 2 2 1
SSol 6utut2
1.0^-----.------.----.-----.---
0	50	100	150	200	250
epoch
Figure 20: DenSeNet on Tiny-ImageNet.
θ θ θ
Au2nuuπ+js ①+j
0	50	100	150	200	250
epoch
23
Published as a conference paper at ICLR 2019
L Extension experiments: ill-conditioned quadratic problem
Rahimi & Recht raise the point, at test of time talk at NIPS 2017, that it is suspicious that gradient
descent (aka back-propagation) is ultimate solution for optimization. A ill-conditioned quadratic
problem with Two Layer Linear Net is showed to be challenging for gradient descent based methods,
while alternative solutions, e.g., Levenberg-Marquardt, may converge faster and better. The problem
is defined as follows:
L(Wι, W2; A) = Eχ∈N(0,i)kW1W2x - Axk2	(45)
where A is some known badly conditioned matrix (k = 1020 or 105), and W1 and W2 are the
trainable parameters.
We test SGD, Adam and AdaShift with this problem, the results are shown in Figure 21, Figure 24.
It turns out as long as the training goes enough long, SGD, Adam, AdaShift all basically converge
in this problem. Though SGD is significantly better than Adam and AdaShift.
We would tend to believe this is a general issue of adaptive learning rate method when comparing
with vanilla SGD. Because these adaptive learning rate methods generally are scale-invariance, i.e.,
the step-size in terms of gt/sqrt(vt) is basically around one, which makes it hard to converge very
well in such a ill-conditioning quadratic problem. SGD, in contrast, has a step-size gt ; as the training
converges SGD would have a decreasing step-size, makes it much easier to converge better. The
above analysis is confirmed with Figure 22 and Figure 23, with a decreasing learning rate, Adam
and AdaShfit both converge very good.
0.0	0.2
0.4	0.6
iterations
0.8
1.0
le7
Figure 21: Ill-conditioned quadratic problem, with fixed learning rate.
24
Published as a conference paper at ICLR 2019
tupn+jIU6BE+jutuIPBJ9
17
lθ^
3
2
-
7 15 1
@ @ 厂 T
1 11010-
sso—l
θ
θ
2
θ
0
6
θ
8
θ
03尸
110
1010
0.0
0.2
0.4
0.6
0.8
iterations
Figure 22: Ill-conditioned quadratic problem, with linear learning rate decay.
1.0
le7
4 12 5 8
0 0 Γ Γ Γ
1 I101010
pnτu6BluuTPBJ9
---- GD step=le-Θ5
GD step=le-Θ6
---- ADAM step=le-Θ3
----ADAM step=le-Θ4
Adashift SteP=Ie-Θ4
—— Adashift SteP=Ie-Θ5
0.0	0.2
0.4	0.6
iterations
0.8
1.0
le7
Figure 23: Ill-conditioned quadratic problem, with exp learning rate decay.
25
Published as a conference paper at ICLR 2019
Gradient magnitude	Loss
iterations
Figure 24: Ill-conditioned quadratic problem, with fixed learning rate and insufficient iterations.
26