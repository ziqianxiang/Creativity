Published as a conference paper at ICLR 2019
The role of over-parametrization
IN GENERALIZATION OF NEURAL NETWORKS
Behnam Neyshabur
School of Mathematics
Institute for Advanced Study
Princeton, NJ 08540
bneyshabur@gmail.com
Zhiyuan Li
Department of Computer Science
Princeton University
Princeton, NJ 08540
zhiyuanli@princeton.edu
Srinadh Bhojanapalli
Toyota Technological
Institute at Chicago
Chicago, IL 60637
srinadh@ttic.edu
Yann LeCun
Department of Computer Science
New York University New York, NY 10012
yann@cs.nyu.edu
Nathan Srebro
Toyota Technological
Institute at Chicago Chicago, IL 60637
nati@ttic.edu
Ab stract
Despite existing work on ensuring generalization of neural networks in terms
of scale sensitive complexity measures, such as norms, margin and sharpness,
these complexity measures do not offer an explanation of why neural networks
generalize better with over-parametrization. In this work we suggest a novel com-
plexity measure based on unit-wise capacities resulting in a tighter generalization
bound for two layer ReLU networks. Our capacity bound correlates with the
behavior of test error with increasing network sizes (within the range reported
in the experiments), and could partly explain the improvement in generalization
with over-parametrization. We further present a matching lower bound for the
Rademacher complexity that improves over previous capacity lower bounds for
neural networks.
1 Introduction
Deep neural networks have enjoyed great success in learning across a wide variety of tasks. They
played a crucial role in the seminal work of Krizhevsky et al. (2012), starting an arms race of training
larger networks with more hidden units, in pursuit of better test performance (He et al., 2016). In
fact the networks used in practice are over-parametrized to the extent that they can easily fit random
labels to the data (Zhang et al., 2017). Even though they have such a high capacity, when trained with
real labels they achieve smaller generalization error.
Traditional wisdom in learning suggests that using models with increasing capacity will result in
overfitting to the training data. Hence capacity of the models is generally controlled either by limiting
the size of the model (number of parameters) or by adding an explicit regularization, to prevent
from overfitting to the training data. Surprisingly, in the case of neural networks we notice that
increasing the model size only helps in improving the generalization error, even when the networks
are trained without any explicit regularization - weight decay or early stopping (Lawrence et al., 1998;
Srivastava et al., 2014; Neyshabur et al., 2015c). In particular, Neyshabur et al. (2015c) observed that
training on models with increasing number of hidden units lead to decrease in the test error for image
classification on MNIST and CIFAR-10. Similar empirical observations have been made over a wide
range of architectural and hyper-parameter choices (Liang et al., 2017; Novak et al., 2018; Lee et al.,
2018). What explains this improvement in generalization with over-parametrization? What is the
right measure of complexity of neural networks that captures this generalization phenomenon?
Complexity measures that depend on the total number of parameters of the network, such as VC
bounds, do not capture this behavior as they increase with the size of the network. Existing works
suggested different norm, margin and sharpness based measures, to measure the capacity of neural
networks, in an attempt to explain the generalization behavior observed in practice (Neyshabur et al.,
2015b; Keskar et al., 2017; Dziugaite & Roy, 2017; Neyshabur et al., 2017; Bartlett et al., 2017;
1
Published as a conference paper at ICLR 2019
two layer ReLU net
#hidden units
Figure 1: Over-parametrization phenomenon. Left panel: Training pre-activation ResNet18 architecture of
different sizes on CIFAR-10 dataset. We observe that even when after network is large enough to completely fit
the training data(reference line), the test error continues to decrease for larger networks. Middle panel: Training
fully connected feedforward network with single hidden layer on CIFAR-10. We observe the same phenomena
as the one observed in ResNet18 architecture. Right panel: Unit capacity captures the complexity of a hidden
unit and unit impact captures the impact of a hidden unit on the output of the network, and are important factors
in our capacity bound (Theorem 1). We observe empirically that both average unit capacity and average unit
impact shrink With a rate faster than 1 /vzh where h is the number of hidden units. Please See Supplementary
Section A for experiments settings.
Neyshabur et al., 2018; Golowich et al., 2018; Arora et al., 2018). In particular, Bartlett et al. (2017)
showed a margin based generalization bound that depends on the spectral norm and `1,2 norm of the
layers of a network. However, as shown in Neyshabur et al. (2017) and in Figure 5, these complexity
measures fail to explain why over-parametrization helps, and in fact increase with the size of the
network. Dziugaite & Roy (2017) numerically evaluated a generalization bound based on PAC-Bayes.
Their reported numerical generalization bounds also increase with the increasing network size. These
existing complexity measures increase with the size of the network, even for two layer networks, as
they depend on the number of hidden units either explicitly, or the norms in their measures implicitly
depend on the number of hidden units for the networks used in practice (Neyshabur et al., 2017) (see
Figures 3 and 5).
To study and analyze this phenomenon more carefully, we need to simplify the architecture making
sure that the property of interest is preserved after the simplification. We therefore chose two layer
ReLU networks since as shown in the left and middle panel of Figure 1, it exhibits the same behavior
with over-parametrization as the more complex pre-activation ResNet18 architecture. In this paper we
prove a tighter generalization bound (Theorem 2) for two layer ReLU networks. Our capacity bound,
unlike existing bounds, correlates with the test error and decreases with the increasing number of
hidden units, in the experimental range considered. Our key insight is to characterize complexity at a
unit level, and as we see in the right panel in Figure 1 these unit level measures shrink at a rate faster
than 1 /√h for each hidden unit, decreasing the overall measure as the network size increases. When
measured in terms of layer norms, our generalization bound depends on the Frobenius norm of the
top layer and the Frobenius norm of the difference of the hidden layer weights with the initialization,
which decreases with increasing network size (see Figure 2).
The closeness of learned weights to initialization in the over-parametrized setting can be understood
by considering the limiting case as the number of hidden units go to infinity, as considered in Bengio
et al. (2006) and Bach (2017). In this extreme setting, just training the top layer of the network, which
is a convex optimization problem for convex losses, will result in minimizing the training error, as
the randomly initialized hidden layer has all possible features. Intuitively, the large number of hidden
units here represent all possible features and hence the optimization problem involves just picking
the right features that will minimize the training loss. This suggests that as we over-parametrize the
networks, the optimization algorithms need to do less work in tuning the weights of the hidden units
to find the right solution. Dziugaite & Roy (2017) indeed have numerically evaluated a PAC-Bayes
measure from the initialization used by the algorithms and state that the Euclidean distance to the
initialization is smaller than the Frobenius norm of the parameters. Nagarajan & Kolter (2017) also
make a similar empirical observation on the significant role of initialization, and in fact prove an
initialization dependent generalization bound for linear networks. However they do not prove a
similar generalization bound for neural networks. Alternatively, Liang et al. (2017) suggested a
2
Published as a conference paper at ICLR 2019
Fisher-Rao metric based complexity measure that correlates with generalization behavior in larger
networks, but they also prove the capacity bound only for linear networks.
Contributions: Our contributions in this paper are as follows.
•	We empirically investigate the role of over-parametrization in generalization of neural
networks on 3 different datasets (MNIST, CIFAR10 and SVHN), and show that the existing
complexity measures increase with the number of hidden units - hence do not explain the
generalization behavior with over-parametrization.
•	We prove tighter generalization bounds (Theorem 2) for two layer ReLU networks, improv-
ing over previous results. Our proposed complexity measure for neural networks decreases
with the increasing number of hidden units, in the experimental range considered (see
Section 2), and can potentially explain the effect of over-parametrization on generalization
of neural networks.
•	We provide a matching lower bound for the Rademacher complexity of two layer ReLU
networks with a scalar output. Our lower bound considerably improves over the best known
bound given in Bartlett et al. (2017), and to our knowledge is the first such lower bound that
is bigger than the Lipschitz constant of the network class.
1.1 Preliminaries
We consider two layer fully connected ReLU networks with input dimension d, output dimension
c, and the number of hidden units h. Output of a network is fV,U(X) = V[UX]+1 where X ∈ Rd,
U ∈ Rh×d and V ∈ Rc×h. We denote the incoming weights to the hidden unit i by ui and the
outgoing weights from hidden unit i by vi. Therefore ui corresponds to row i of matrix U and vi
corresponds to the column i of matrix V.
We consider the c-class classification task where the label with maximum output score will be selected
as the prediction. Following Bartlett et al. (2017), We define the margin operator μ : Rc X [c] → R
as a function that given the scores f(X) ∈ Rc for each label and the correct label y ∈ [c], it returns
the difference between the score of the correct label and the maximum score among other labels, i.e.
μ(f (x), y) = f (x)[y] 一 maxi=y f (x)[i]. We now define the ramp loss as follows:
0
μ(f (χ),y) > Y
'γ(f(Xby)=	μf (X),y"γ μ(f (χ),y) ∈ [0,γ]
(1)
1
μ(f(χ),y) < 0.
For any distribution D and margin γ > 0, we define the expected margin loss of a predictor f(.) as
LY (f) = P(x,y)〜D ['γ (f(x), y)]. The loss LY (.) defined this way is bounded between 0 and 1. We
use LY(f) to denote the empirical estimate of the above expected margin loss. As setting γ = 0
reduces the above to classification loss, we will use L0 (f) and L0 (f) to refer to the expected risk
and the training error respectively.
2 Generalization of Two Layer ReLU Networks
For any function class F, let 'γ ◦ F denote the function class corresponding to the composition of
the loss function and functions from class F. With probability 1 一 δ over the choice of the training
set of size m, the following generalization bound holds for any function f ∈ F (Mohri et al., 2012,
Theorem 3.1):	_____
Lo(f) ≤ Lγ(f) + 2Rs('γ ◦ F) + 3Jinm).	⑵
where RS (H) is the Rademacher complexity of a class H of functions with respect to the training set
S which is defined as:
1m
RS(H) = — E sup X ξif(χi) .	(3)
m ξ〜{±1}m f ∈H M
1Since the number of bias parameters is negligible compare to the size of the network, we drop the bias
parameters to simplify the analysis. Moreover, one can model the bias parameters in the first layer by adding an
extra dimension with value 1.
3
Published as a conference paper at ICLR 2019
≠hidden units
angle to initial weight
distance to initial weight
Figure 2:	Properties of two layer ReLU networks trained on CIFAR-10. We report different measures on the
trained network. From left to right: measures on the second (output) layer, measures on the first (hidden) layer,
distribution of angles of the trained weights to the initial weights in the first layer, and the distribution of unit
capacities of the first layer. "Distance" in the first two plots is the distance from initialization in Frobenius norm.
Rademacher complexity is a capacity measure that captures the ability of functions in a function class
to fit random labels which increases with the complexity of the class.
2.1	An Empirical Investigation
We will bound the Rademacher complexity of neural networks to get a bound on the generalization
error. Since the Rademacher complexity depends on the function class considered, we need to choose
the right function class that only captures the real trained networks, which is potentially much smaller
than networks with all possible weights, to get a complexity measure that explains the decrease in
generalization error with increasing width. Choosing a bigger function class can result in weaker
capacity bounds that do not capture this phenomenon. Towards that we first investigate the behavior
of different measures of network layers with increasing number of hidden units. The experiments
discussed below are done on the CIFAR-10 dataset. Please see Section A for similar observations on
SVHN and MNIST datasets.
First layer: As we see in the second panel in Figure 2 even though the spectral and Frobenius norms
of the learned layer decrease initially, they eventually increase with h, with Frobenius norm increasing
at a faster rate. However the distance Frobenius norm, measured w.r.t. initialization (kU - U0 kF),
decreases. This suggests that the increase in the Frobenius norm of the weights in larger networks is
due to the increase in the Frobenius norm of the random initialization. To understand this behavior in
more detail we also plot the distance to initialization per unit and the distribution of angles between
learned weights and initial weights in the last two panels of Figure 2. We indeed observe that per
unit distance to initialization decreases with increasing h, and a significant shift in the distribution of
angles to initial points, from being almost orthogonal in small networks to almost aligned in large
networks. This per unit distance to initialization is a key quantity that appears in our capacity bounds
and we refer to it as unit capacity in the remainder of the paper.
Unit capacity. We define βi = ui - ui0 2 as the unit capacity of the hidden unit i.
Second layer: Similar to first layer, we look at the behavior of different measures of the second layer
of the trained networks with increasing h in the first panel of Figure 2. Here, unlike the first layer, we
notice that Frobenius norm and distance to initialization both decrease and are quite close suggesting
a limited role of initialization for this layer. Moreover, as the size grows, since the Frobenius norm
kVkF of the second layer slightly decreases, we can argue that the norm of outgoing weights vi from
a hidden unit i decreases with a rate faster than 1 / √h. If We think of each hidden unit as a linear
separator and the top layer as an ensemble over classifiers, this means the impact of each classifier
on the final decision is shrinking with a rate faster than 1 / √h. This per unit measure again plays an
important role and we define it as unit impact for the remainder of this paper.
Unit impact. We define αi = kvi k2 as the unit impact, which is the magnitude of the outgoing
weights from the unit i.
Motivated by our empirical observations we consider the following class of two layer neural networks
that depend on the capacity and impact of the hidden units of a network. Let W be the following
restricted set of parameters:
W = {(V, U) | V ∈ Rc×h, U ∈ Rh×d, kvik ≤ αi ,∣Iui- u0∣∣2 ≤ βi} ,	(4)
4
Published as a conference paper at ICLR 2019
We now consider the hypothesis class of neural networks represented using parameters in the set W :
FW = {f (x)= V [Ux]+ | (V, U) ∈W} .	(5)
Our empirical observations indicate that networks we learn from real data have bounded unit capacity
and unit impact and therefore studying the generalization behavior of the above function class can
potentially provide us with a better understanding of these networks. Given the above function class,
we will now study its generalization properties.
2.2	Generalization B ound
In this section we prove a generalization bound for two layer ReLU networks. We first bound the
Rademacher complexity of the class FW in terms of the sum over hidden units of the product of unit
capacity and unit impact. Combining this with the equation (2) will give us the generalization bound.
Theorem 1. Given a training set S = {xi}im=1 andγ > 0, Rademacher complexity of the composition
of loss function 'γ over the class FW defined in equations (4) and (5) is bounded as follows:
RS ('γ ◦FW) ≤ 2√mc X aj (ej kXkF+:冈 2)	.
≤ 2√ck kak2 (kβk2 t m^ X kxik2+t m X kU0χik2
(6)
(7)
The proof is given in the supplementary Section C. The main idea behind the proof is a new technique
to decompose the complexity of the network into complexity of the hidden units. To our knowledge,
all previous works decompose the complexity to that of layers and use Lipschitz property of the
network to bound the generalization error. However, Lipschitzness of the layer is a rather weak
property that ignores the linear structure of each individual layer. Instead, by decomposing the
complexity across the hidden units, we get the above tighter bound on the Rademacher complexity of
the two layer neural networks.
The generalization bound in Theorem 1 is for any function in the function class defined by a specific
choice of α and β fixed before the training procedure. To get a generalization bound that holds for
all networks, it suffices to cover the space of possible values for α and β and take a union bound
over it. The following theorem states the generalization bound for any two layer ReLU network 2.
Theorem 2. For any h ≥ 2, γ > 0, δ ∈ (0, 1) and U0 ∈ Rh×d, with probability 1 - δ over the
choice of the training setS = {xi}im=1 ⊂ Rd, for any function f(x) = V[Ux]+ such that V ∈ Rc×h
and U ∈ Rh×d, the generalization error is bounded as follows:
, . ^ ~
Lo(f) ≤ LY (f) + O
^ ~
≤ LY (f) + O
√ kVkF (IIU - u0∣∣f IXIf + ∣∣u0 XllF)
γm
√ kVkF (l∣u - U0IIf + I∣U0II2) J* P乙 kXik2
γ√m
The above generalization bound empirically improves over the existing bounds, and decreases with
increasing width for networks learned in practice (see Section 2.3). We also show an explicit
lower bound for the Rademacher complexity (Theorem 3), matching the first term in the above
generalization bound, thereby showing its tightness. The additive factor O( ,h/m) in the above
bound is the result of taking the union bound over the cover of α and β. As we see in Figure 5, in the
regimes of interest this additive term is small and does not dominate the first term, resulting in an
overall decrease in capacity with over-parametrization. In Appendix Section B, we further extend the
generalization bound in Theorem 2 to `p norms, presenting a finer tradeoff between the two terms.
2.3	Comparison with Existing Results
In table 1 we compare our result with the existing generalization bounds, presented
for the simpler setting of two layer networks. In comparison with the bound
2For the statement with exact constants see Lemma 14 in Supplementary Section C.
5
Published as a conference paper at ICLR 2019
#	Reference	Measure
互	HarVeyeta ( )17 )	Θ(dh)
⑵	Bartlett & Mendelson ( 0))	θ (kuk∞,1kvk∞,1)
互	Neyshabur et al. (2015 ), Golowich et al. (201 )	θ (kU∣∣F kV∣∣F)
(4)	Bartlett et a ( 01 ), Golowich et al. ( 01 )	Θ (kU∣∣2 kV - V0k1,2 + kU - U0k1,2 kVk2)
IE	Neyshabur et al. (2018)	Θ j∣Uk2 kV - VokF + √ kU - UokF kVk；)
(6)	Theorem 2	Θ (kUok2 kV∣∣F + IlU - U0∣∣F kV∣∣F + √h
Table 1: Comparison with the existing generalization measures presented for the case of two layer ReLU
networks with constant number of outputs and constant margin.
#hidden units
Figure 3:	Behavior of terms presented in Table 1 with respect to the size of the network trained on CIFAR-10.
Θ (kU∣2 l∣V - V0k1,2 + kU - U0k1,2 l∣Vk2J ( artlettet al., 2017; GoloWichetal., 2018 ): The
first term in their bound kUk2 kV - V0 k1,2 is of smaller magnitude and behaves roughly similar
to the first term in our bound lU0 l2 lVlF (see Figure 3 last tWo panels). The key complexity
term in their bound is lU - U0l1,2 lVl2, and in our bound is U - U0F lVlF, for the range
of h considered. lVl2 and lVlF differ by number of classes, a small constant, and hence behave
similarly. HoWeVer, ∣U — Uo k 12 can be as big as √h ∙ ∣∣U 一 U011f when most hidden units have
similar capacity. Infact their bound increases With h mainly because of this term lU - U0l1,2 . As
We see in the first and second panels of Figure 3, `1 norm terms appearing in Bartlett & Mendelson
(2002); Bartlett et al. (2017); GoloWich et al. (2018) over hidden units increase With the number of
units as the hidden layers learned in practice are usually dense. Neyshabur et al. (2015b); GoloWich
et al. (2018) shoWed a bound depending on the product of Frobenius norms of layers, Which increases
With h, shoWing the important role of initialization in our bounds. In fact the proof technique of
Neyshabur et al. (2015b) does not alloW for getting a bound With norms measured from initialization,
and our neW decomposition approach is the key for the tighter bound.
Experimental comparison. We train tWo layer ReLU netWorks of size h on CIFAR-10 and SVHN
datasets With values of h ranging from 26 to 215. The training and test error for CIFAR-10 are shoWn
in the first panel of Figure 1, and for SVHN in the left panel of Figure 4. We observe for both datasets
that even though a netWork of size 128 is enough to get to zero training error, netWorks With sizes
Well beyond 128 can still get better generalization, even When trained Without any regularization.
We further measure the unit-Wise properties introduce in the paper, namely unit capacity and unit
impact. These quantities decrease With increasing h, and are reported in the right panel of Figure 1
and second panel of Figure 4. Also notice that the number of epochs required for each netWork size
to get 0.01 cross-entropy loss decreases for larger netWorks as shoWn in the third panel of Figure 4.
For the same experimental setup, Figure 5 compares the behavior of different capacity bounds
over networks of increasing sizes. Generalization bounds typically scale as，C/m where C is
the effective capacity of the function class. The left panel reports the effective capacity C based
on different measures calculated with all the terms and constants. We can see that our bound is
the only that decreases with h and is consistently lower that other norm-based data-independent
bounds. Our bound even improves over VC-dimension for networks with size larger than 1024.
While the actual numerical values are very loose, we believe they are useful tools to understand the
relative generalization behavior with respect to different complexity measures, and in many cases
applying a set of data-dependent techniques, one can improve the numerical values of these bounds
significantly (Dziugaite & Roy, 2017; Arora et al., 2018). In the middle and right panel we presented
6
Published as a conference paper at ICLR 2019
4 2。
Ooo
.lo
4 2 0
alnseE
SlPOd8#
1000-
500-
normalized margin
一It:-1《I-K-?
2017m11
Oooo
Illl
A4urodrau
Figure 4: First panel: Training and test errors of fully connected networks trained on SVHN. Second panel:
unit-wise properties measured on a two layer network trained on SVHN dataset. Third panel: number of epochs
required to get 0.01 cross-entropy loss. Fourth panel: comparing the distribution of margin of data points
normalized on networks trained on true labels vs a network trained on random labels.
Figure 5: Left panel: Comparing network capacity bounds on CIFAR10 (unnormalized). Middle panel:
Comparing capacity bounds on CIFAR10 (normalized). Right panel: Comparing capacity bounds on SVHN
(normalized).
each capacity bound normalized by its maximum in the range of the study for networks trained
on CIFAR-10 and SVHN respectively. For both datasets, our capacity bound is the only one that
decreases with the size even for networks with about 100 million parameters. All other existing
norm-based bounds initially decrease for smaller networks but then increase significantly for larger
networks. Our capacity bound therefore could potentially point to the right properties that allow the
over-parametrized networks to generalize.
Finally we check the behavior of our complexity measure under a different setting where we compare
this measure between networks trained on real and random labels (Neyshabur et al., 2017; Bartlett
et al., 2017). We plot the distribution of margin normalized by our measure, computed on networks
trained with true and random labels in the last panel of Figure 4 - and as expected they correlate well
with the generalization behavior.
3 Lower B ound
In this section we will prove a lower bound for the Rademacher complexity of neural networks, that
matches the dominant term in the upper bound of Theorem 1. We will show our lower bound on a
smaller function class than FW, with an additional constraint on spectral norm of the hidden layer.
This allows for comparison with the existing results, and also extends the lower bound to the bigger
class FW .
Theorem 3. Define the parameter set
W0 = {(V, U) | V ∈ R1×h, U ∈ Rh×d,kvjk ≤ αj, ∣∣Uj - u0∣∣2 ≤ βj,kU - U0k2 ≤ maxβj},
and let FW0 be the function class defined on W0 by equation (5). Then, for any d = h ≤ m,
{αj , βj }jh=1 ⊂ R+ and U0 = 0, there exists S = {xi }im=1 ⊂ Rd, such that
Ph 1 αjβjkXkF
RS (FW) ≥ RS(Fw0) = Ω "1 j j -.
m
Clearly, W0 ⊆ W, since it has an extra constraint. The complete proof is given in the supplementary
Section C.3.
7
Published as a conference paper at ICLR 2019
aku0xk2)= ω (Ph= αku0xk2
mm
The above complexity lower bound matches the first term, Ei=I mYikXkF, in the upper bound of
Theorem 1, upto Y, which comes from the 1 -Lipschitz constant of the ramp loss l7.
To match the second term in the upper bound for Theorem 1, consider the setting with c = 1 and
β = 0, resulting in,
h
rs (FW) = RU0θS]+(FV) = X ω
j=1
where FV = {f (x) = Vx | V ∈ R1×h, kvj k ≤ αj}. In other words, when β = 0, the
function class FW0 on S = {xi}im=1 is equivalent to the linear function class FV on [U0 ◦ S]+ =
{[U0 xi ]+ }im=1, and therefore we have the above lower bound, showing that the upper bound provided
in Theorem 1 is tight. It also indicates that even if we have more information, such as bounded
spectral norm with respect to the reference matrix is small (which effectively bounds the Lipschitz of
the network), we still cannot improve our upper bound.
To our knowledge, all the previous capacity lower bounds for spectral norm bounded classes of neural
networks with a scalar output and element-wise activation functions correspond to the Lipschitz
constant of the network. Our lower bound strictly improves over this, and shows a gap between the
Lipschitz constant of the network (which can be achieved by even linear models), and the capacity of
neural networks. This lower bound is non-trivial, in the sense that the smaller function class excludes
the neural networks with all rank-1 matrices as weights, and thus shows a Θ(√h)-capacity gap
between the neural networks with ReLU activations and linear networks. The lower bound therefore
does not hold for linear networks. Finally, one can extend the construction in this bound to more
layers by setting all the weight matrices in the intermediate layers to be the Identity matrix.
Comparison with existing results. Bartlett et al. (2017) have proved a Rademacher complexity
lower bound of Ω (s1 s2mχkF) for the function class defined by the parameter set:
Wspec = {(V, U) | V ∈ R1×h, U ∈ Rh×d,kVk2 ≤ S1,kU∣2 ≤ s2} .	(8)
Note that s1s2 is the Lipschitz bound of the function class FWspec. Given Wspec with bounds s1 and
s2, choosing α and β such that kαk2 = s1 and maxi∈[h] βi = s2 results in W0 ⊂ Wspec. Hence we
get the following result from Theorem 3, showing a stronger lower bound for this function class as
well.
Corollary 4. ∀h = d ≤ m, s1, s2 ≥ 0, ∃S ∈ Rd×m such that RS(FWspec) = Ω ($-¢11XkF )
Hence our result improves the lower bound in Bartlett et al. (2017) by a factor of √h. Theorem 7
in Golowich et al. (2018) also gives a Ω(s1s2√c) lower bound, C is the number of outputs of the
network, for the composition of 1-Lipschitz loss function and neural networks with bounded spectral
norm, or ∞-Schatten norm. Our above result even improves on this lower bound.
4 Discussion
In this paper we present a new capacity bound for neural networks that empirically decreases with the
increasing number of hidden units, and could potentially explain the better generalization performance
of larger networks. In particular, we focused on understanding the role of width in the generalization
behavior of two layer networks. More generally, understanding the role of depth and the interplay
between depth and width in controlling capacity of networks, remain interesting directions for future
study. We also provided a matching lower bound for the capacity improving on the current lower
bounds for neural networks. While these bounds are useful for relative comparison between networks
of different size, their absolute values still remain larger than the number of training samples, and it
is of interest to get bounds with numerically smaller values.
In this paper we do not address the question of whether optimization algorithms converge to low
complexity networks in the function class considered in this paper, or in general how does different
hyper parameter choices affect the complexity of the recovered solutions. Itis interesting to understand
the implicit regularization effects of the optimization algorithms (Neyshabur et al., 2015a; Gunasekar
et al., 2017; Soudry et al., 2018) for neural networks, which we leave for future work.
8
Published as a conference paper at ICLR 2019
Acknowledgements
The authors thank Sanjeev Arora for many fruitful discussions on generalization of neural networks
and David McAllester for discussion on the distance to random initialization. The authors also thank
Wei Zhan for discussion on the lower bound. This research was supported in part by NSF IIS-RI
award 1302662 and Schmidt Foundation.
References
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. In Proceedings of the 35th International Conference on
Machine Learning, pp. 254-263, 2018.
Francis Bach. Breaking the curse of dimensionality with convex neural networks. Journal of Machine
Learning Research, 18(19):1-53, 2017.
Peter L Bartlett and Shahar Mendelson. Rademacher and Gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pp. 6241-6250, 2017.
Yoshua Bengio, Nicolas L Roux, Pascal Vincent, Olivier Delalleau, and Patrice Marcotte. Convex
neural networks. In Advances in neural information processing systems, pp. 123-130, 2006.
Gintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. In Proceedings
of the Thirty-Third Conference on Uncertainty in Artificial Intelligence, 2017.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neural networks. In Proceedings of the 31st Conference On Learning Theory, pp. 297-299, 2018.
Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro.
Implicit regularization in matrix factorization. In Advances in Neural Information Processing
Systems, 2017.
Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight VC-dimension bounds for
piecewise linear neural networks. In Proceedings of the 30th Conference On Learning Theory, pp.
1064-1068, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter
Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In
Proceeding of the International Conference on Learning Representations, 2017.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet classification with deep convolu-
tional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Steve Lawrence, C Lee Giles, and Ah Chung Tsoi. What size neural network gives optimal gen-
eralization? Convergence properties of backpropagation. Technical report, U. of Maryland,
1998.
Michel Ledoux and Michel Talagrand. Probability in banach spaces: isoperimetry and processes.
1991.
Jaehoon Lee, Jascha Sohl-dickstein, Jeffrey Pennington, Roman Novak, Sam Schoenholz, and
Yasaman Bahri. Deep neural networks as gaussian processes. In International Conference on
Learning Representations, 2018.
9
Published as a conference paper at ICLR 2019
Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, and James Stokes. Fisher-Rao metric, geometry,
and complexity of neural networks. arXiv preprint arXiv:1711.01530, 2017.
Andreas Maurer. A vector-contraction inequality for Rademacher complexities. In International
Conference OnAlgorithmic Learning Theory, pp. 3-17, 2016.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT
press, 2012.
Vaishnavh Nagarajan and J.Zico Kolter. Generalization in deep networks: The role of distance from
initialization. Advances in neural information processing systems workshop on Deep Learning:
Bridging Theory and Practice, 2017.
Behnam Neyshabur, Ruslan R Salakhutdinov, and Nati Srebro. Path-SGD: Path-normalized opti-
mization in deep neural networks. In Advances in Neural Information Processing Systems, pp.
2422-2430, 2015a.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Proceedings of the 28th Conference On Learning Theory, pp. 1376-1401, 2015b.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias:
On the role of implicit regularization in deep learning. International Conference on Learning
Representations workshop track, 2015c.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. Exploring general-
ization in deep learning. In Advances in Neural Information Processing Systems, 2017.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-Bayesian approach to
spectrally-normalized margin bounds for neural networks. In International Conference on Learning
Representations, 2018.
Roman Novak, Yasaman Bahri, Daniel A. Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein.
Sensitivity and generalization in neural networks: an empirical study. In International Conference
on Learning Representations, 2018.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, and Nathan Srebro. The implicit bias of gradient
descent on separable data. In International Conference on Learning Representations, 2018.
Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. Journal of machine learning
research, 15(1):1929-1958, 2014.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understand-
ing deep learning requires rethinking generalization. In International Conference on Learning
Representations, 2017.
10
Published as a conference paper at ICLR 2019
A	Experiments
A.1 Experiments Settings
Below we describe the setting for each reported experiment.
ResNet18 In this experiment, we trained a pre-activation ResNet18 architecture on CIFAR-10
dataset. The architecture consists of a convolution layer followed by 8 residual blocks (each of
which consist of two convolution) and a linear layer on the top. Let k be the number of channels
in the first convolution layer. The number of output channels and strides in residual blocks is then
[k, k, 2k, 2k, 4k, 4k, 8k, 8k] and [1, 1, 1, 2, 1, 2, 1, 2] respectively. Finally, we use the kernel sizes 3
in all convolutional layers. We train 11 architectures where for architecture i we set k = d22+i/2e. In
each experiment we train using SGD with mini-batch size 64, momentum 0.9 and initial learning
rate 0.1 where we reduce the learning rate to 0.01 when the cross-entropy loss reaches 0.01 and stop
when the loss reaches 0.001 or if the number of epochs reaches 1000. We use the reference line in
the plots to differentiate the architectures that achieved 0.001 loss. We do not use weight decay or
dropout but perform data augmentation by random horizontal flip of the image and random crop of
size 28 × 28 followed by zero padding.
Two Layer ReLU Networks We trained fully connected feedforward networks on CIFAR-10,
SVHN and MNIST datasets. For each data set, we trained 13 architectures with sizes from 23 to 215
each time increasing the number of hidden units by factor 2. For each experiment, we trained the
network using SGD with mini-batch size 64, momentum 0.9 and fixed step size 0.01 for MNIST
and 0.001 for CIFAR-10 and SVHN. We did not use weight decay, dropout or batch normalization.
For experiment, we stopped the training when the cross-entropy reached 0.01 or when the number
of epochs reached 1000. We use the reference line in the plots to differentiate the architectures that
achieved 0.01 loss.
Evaluations For each generalization bound, we have calculated the exact bound including the log-
terms and constants. We set the margin to 5th percentile of the margin of data points. Since bounds
in Bartlett & Mendelson (2002) and NeyshabUr et al. (2015c) are given for binary classification, We
multiplied Bartlett & Mendelson (2002) by factor C and Neyshabur et al. (2015c) by factor √c to
make sUre that the boUnd increases linearly With the nUmber of classes (assUming that all oUtpUt Units
have the same norm). Furthermore, since the reference matrices can be used in the bounds given
in Bartlett et al. (2017) and Neyshabur et al. (2018), We used random initialization as the reference
matrix. When plotting distributions, We estimate the distribution using standard Gaussian kernel
density estimation.
A.2 Supplementary Figures
Figures 6 and 7 shoW the behavior of several measures on netWorks With different sizes trained on
SVHN and MNIST datasets respectively. The left panel of Figure 8 shoWs the over-parametrization
phenomenon in MNSIT dataset and the middle and right panels compare our generalization bound to
others.
#hidden units
first layer
angle to initial weight
1.0-
0.5-
o.o-
0	5	10
singular value
Figure 6: Different measures on fully connected netWorks With a single hidden layer trained on SVHN. From
left to right: measure on the output layer, measures in the first layer, distribution of angle to initial Weight in the
first layer, and singular values of the first layer.
11
Published as a conference paper at ICLR 2019
80
9 4。
S20
①
E 10
5
Figure 7: Different measures on fully connected networks with a single hidden layer trained on MNIST. From
left to right: measure on the output layer, measures in the first layer, distribution of angle to initial weight in the
first layer, and singular values of the first layer.
6 3 0
Ill
Ooo
Ill
Ad8
#hidden units
Figure 8: Left panel: Training and test errors of fully connected networks trained on MNIST. Middle panel:
Comparing capacity bounds on MNIST (normalized). Left panel: Comparing capacity bounds on MNIST
(unnormalized).
B EXTENDING THE GENERALIZATION BOUND TO `p NORM
In this section we generalize the Theorem 2 to `p norm. The main new ingredient in the proof is the
Lemma 11, in which we construct a cover for the `p ball with entry-wise dominance.
Theorem 5. For any h,p ≥ 2, γ > 0, δ ∈ (0, 1) and U0 ∈ Rh×d, with probability 1 - δ over the
choice of the training setS = {xi}im=1 ⊂ Rd, for any function f(x) = V[Ux]+ such that V ∈ Rc×h
and U ∈ Rh×d, the generalization error is bounded as follows:
.. ^ ~
Lo(f) ≤ LY(f) + O
√h 2-P WTL 2 G2-P 付-u%p,2 kχL+nu0χL
γm
+
where k.kp,2 is the `p norm of the row `2 norms.
For p of order ln h, √ehF ≈ constant improves on the √h additive term in Theorem 2 and
h 1-11lVTIlp2 ≈ h 1 -∏⅛ ∣∣Vτ∣∣inh 2 which is a tight upper bound for IlVkF and is of the Same
order if all rows of V have the same norm - hence giving a tighter bound that decreases with h for
larger values. In particular for p = ln h we get the following bound.
Corollary 6. Under the settings of Theorem 5, with probability 1 - δ over the choice of the training
set S = {xi}im=1, for any function f(x) = V[Ux]+, the generalization error is bounded as follows:
.. ^ ~
Lo(f) ≤ LY(f) + O
^ ~
≤ LY (f) + O
√c2-InLh WTh h,2 ①1-InLh IIu - UOh h,2 kxkF + kU0XkF )
γm
(√ch1-lnh IIvTIIln h,2 (h 1-lnh IIu - U0IIln h,2 + 12m) J / Pm=IkXik2 ]
γ√m
/

12
Published as a conference paper at ICLR 2019
C Proofs
C.1 Proof of Theorem 1
We start by stating a simple lemma which is a vector-contraction inequality for Rademacher complex-
ities and relates the norm of a vector to the expected magnitude of its inner product with a vector
of Rademacher random variables. We use the following technical result from Maurer (2016) in our
proof.
Lemma 7 (Propostion 6 of Maurer (2016)). Let ξi be the Rademacher random variables. For any
vector v ∈ Rd, the following holds:
kv∣∣2 ≤ √2	E
ξi^{±1},i∈[d]
[∣hξ, vi∣].
The above lemma can be useful to get Rademacher complexities in multi-class settings. The below
lemma bounds the Rademacher-like complexity term for linear operators with multiple output centered
around a reference matrix. The proof is very simple and similar to that of linear separators. See
Bartlett & Mendelson (2002) for similar arguments.
Lemma 8. For any positive integer c, positive scaler r > 0, reference matrix V0 ∈ Rc×d and set
{xi}im=1 ⊂ Rd, the following inequality holds:
m
E	sup X hξi, VXii ≤ r√c ∣∣X∣∣F .
ξi∈{±1}c,i∈[m] kV-V0kF≤r i=1
Proof.
m
E	sup	X hξi, VXii
ξi∈{±1}c,i∈[m] kV-V0kF≤r i=1
E
ξi∈{±1}c,i∈[m]
kV-sVu0pkF≤r	V,Xi=m1ξiXi>
E
ξi∈{±1}c,i∈[m]
sup
kV-V0kF ≤r
V-V0+V0
Xi=m1 ξiXi>
E
ξi∈{±1}c,i∈[m]
E
ξi∈{±1}c,i∈[m]
E
ξi∈{±1}c,i∈[m]
≤r E
ξi∈{±1}c,i∈[m]
sup
kV-V0kF ≤r
sup
kV-V0kF ≤r
sup
kV-V0kF ≤r
V-V0,Xi=m1ξiXi>
*v-V0,X ξix>+:
*v-V0,X ξix>+#
+E
ξi∈{±1}c,i∈[m]
+E
ξi∈{±1}c,i∈[m]
kV-sVu0pkF≤r V0, Xi=m1 ξiXi>
"*V0, X Q#
(i)
≤r E
ξi∈{±1}c,i∈[m]
m
XξiXi>
i=1
F
2	1/2
rX E
j=1 ξ∈{±1}m
m
XξiXi>
i=1
2
F
1/2
F
=r√c kXkF .
(i) follows from the Jensen’s inequality.
□
13
Published as a conference paper at ICLR 2019
We next show that the Rademacher complexity of the class of networks defined in (5) and (4) can be
decomposed to that of hidden units.
Lemma 9 (Rademacher Decomposition). Given a training set S = {xi}im=1 andγ > 0, Rademacher
complexity of the class FW defined in equations (5) and (4) is bounded as follows:
rs ('γ ◦ FW) ≤ —— X E
γm j=1 ξi∈{±1}c,i∈[m]
m
sup	hξi,vji [huj,xii]+
kuj-uj0 k2≤βj,kvjk2≤αj i=1
Proof. We prove the inequality in the lemma statement using induction on t in the following inequal-
ity:
mRs ('γ ◦ Fw) ≤ E
ξi∈{±1}c,i∈[m]
E
ξi∈{±1}c,i∈[m]
t-1 h	m
SUp	-XX hξi, VjiKUj, χii]+ + X ξil'γ (V[Uxi ]+,yi)
(V,U)∈W γ i=1 j=1	i=t
sup ξtι'γ (V[Uxt ]+,yt) + ΦV,U
(V,U)∈W
where for simplicity of the notation, we let φtV,U = W Pt-1 Ph=1 hξi, Vj ihuj , Xii +
pm=t+ι ξii'γ(V[Uxi]+,yi). The above statement holds trivially for the base case of t = 1 by
the definition of the Rademacher complexity (3). We now assume that it is true for any t0 ≤ t and
prove it is true for t0 = t + 1.
mRs ('γ ◦ Fw) ≤	E ξi∈{±1}c,i∈[m]		sup ξtι'γ(V[Uxt] + ,yt) + ΦV U (V,U)∈w		
1 二5	E 2 ξi∈{±1}c,i∈[m]	(V,u),(supuo)∈w'γ(V[UXt] +，yt) - 'γ(VOUXt]+,yt) + φV,U + φV0U			
≤ 1	E 2 ξi∈{±1}c,i∈[m]	(V,U),(sUpU0)∈W FkVUXt]+ - v0U0χt]+k2 + φV,U + φV0,U[			.	(9)
The last inequality follows from the m LiPSchitzneSS of the ramp loss. The ramp loss is 1∕γ
Lipschitz with respect to each dimension but since the loss at each point only depends on score of the
correct labels and the maximum score among other labels, it is 辛-Lipschitz. By Lemma 7, the right
14
Published as a conference paper at ICLR 2019
hand side of the above inequality can be bounded as follows:
mRs ('γ OFW) ≤
E
SUP ξtι'γ(V[Uxt] + , yt) + φV
ξi∈{±1}c,i∈[m]	(V,u)∈W
,U
≤ 1 E
2 ξi∈{±1}c,i∈[m]
≤ 1 E
2 ξi∈{±1}c,i∈[m]
+ φtV,U + φtV0 ,U
sup
√2
(V,U),(V0,U0)∈W γ
sup
IIV[Uxt]+ - V0[U0xt]+ k2 + φV,U + φV0,U0
(V,U),(V0 ,U0 )∈W γ ξt0 ∈{±1}c
2
[，E、[∣hξ0, V[Uxt]+ - V0[U0Xt]+i∣]
2
ξt,ξi∈{±1}c,i∈[m] MU),：：%，)∈W Y lhξt, VUxt]+ - V U xt] + il + φV,U + φV0
,U0
1
2 ξi∈{±Ec,i∈[m]
2
(v,u),(sVpuo)∈W Ylhξt, V[Uxt]+ - V [Uxt]+il + φV,u + Mb
1
2 ξi∈{±Ec,i∈[m]
2
(v,u),(sVpuo)∈w Y hξt, V[Uxt]+-V [U xt]+i + φV,U + MR
1
2 ξi∈{±Ec,i∈[m]
2
(VUp∈w Y hξt, V[Uxt]+i + φV,U
1
2 ξi∈{±Ec,i∈[m]
ξi∈{±1}c,i∈[m]	(V,U)∈W γ
2h
suP — Ehξt, Vj i[huj, xti]+ + φV,u
(V,u)∈W Yj =1
2t h	m
SUP -∑∑hξi,Vj i [huj ,xii]+ + E ξii'γ(V[Uxi] + ,y)
i=1 j=1
i=t+1
1
≤ —
一2
E
0
E
This completes the induction proof.
□
Lemma 10 (Ledoux-Talagrand contraction, Ledoux & Talagrand (1991)). Let f : R+ → R+ be
convex and increasing. Let φi : R → R satisfy φi(0) = 0 and be L-Lipschitz. Let ξi be independent
Rademacher random variables. For any T ⊆ Rn,
n
ξ∈{Ei}m f 1tUP
Xn ξiφi (ti)	≤ E
i=1	ξ∈{±1}m
f L sup	ξiti
t∈T
i=1
The above lemma will be used in the following proof of Theorem 1.
Proof of Theorem 1. By Lemma 9, we have:
15
Published as a conference paper at ICLR 2019
m
E	sup	hξi, vji [huj , xii]+
ξi∈{±1}c,i∈[m] kuj-uj0k2≤βj,kvjk2≤αj i=1
αj E
ξi∈{±1}c,i∈[m]
m
sup	[huj , xii]+ξi
kuj -uj0 k2 ≤βj i=1	2
≤ αj E
ξi∈{±1}c,i∈[m]
sup
kuj -uj0 k2≤βj ∈{±1}c
≤ √2αj	E
∈{±1}c ξi∈{±1}c,i∈[m]
sup
kuj-uj0k2≤βj
, xii]+ξi,
, xii]+ξi,
+#
√2αj∙	E
ξi∈{±1}c,i∈[m]
sup
kuj-uj0k2≤βj
E
E
√2αj∙	E
ξi∈{±1}c,i∈[m]
sup
kuj-uj0k2≤βj
mc
[huj,xii]+ξik
i=1 k=1
Now we can apply Lemma 10 with n = m × c, f (x) = x, T = {(tik)im,k,c=1,1 | tik
huj , xii}, φik (x) = [x]+ , ∀i ∈ [m], k ∈ [c], and we get
√2aj	E
ξi∈{±1}c,i∈[m]
≤2√2αj	E
ξi∈{±1}c,i∈[m]
=2√2aj	E
ξi∈{±1}c,i∈[m]
sup
kuj-uj0k2≤βj
sup
kuj-uj0k2≤βj
mc
XX[huj,xii]+ξik
i=1 k=1
m c
huj , xi i ξik
mc
sup	ΣΣ huj, xii ξik
kuj -uj0 k2 ≤βj i=1 k=1
(10)
≤2√2αj (βj ∣∣X0kF + ∣∣u0X0∣∣2), where X0 = [X X ... X] ∈ Rd×cm
≤2√2Cαj (βj∣X∣F +∣∣u0X∣∣2)
The proof is completed by taking sum of above inequality over j from 1 to h.
□
C.2 Proof of Theorems 2 and 5
We start by the following covering lemma which allows us to prove the generalization bound in
Theorem 5 without assuming the knowledge of the norms of the network parameters. The following
lemma shows how to cover an `p ball with a set that dominates the elements entry-wise, and bounds
the size of a one such cover.
Lemma 11 (`p covering lemma). Given any , D, β > 0, p ≥ 2, consider the set SpD,β = {x ∈ RD |
∣x∣p ≤ β}. Then there exist N sets {Ti}iN=1 of the form Ti = {x ∈ RD | |xj | ≤ αij, ∀j ∈ [D]}
such that SDe ⊆ SN=1 Ti and ∣∣αi∣∣2 ≤ D1/p-1/2e(1 + e), ∀i ∈ [N] where N = (K+——1) and
K
D
(1 + E)P — 1
Proof. We prove the lemma by construction.
{α ∈ RD | ∀iαP ∈ {jβp∕K}^=IJakp = βp(1 + DIK)}.
Consider the set Q =
For any x ∈ SpD,β, consider α0
16
Published as a conference paper at ICLR 2019
such that for any i ∈ [D],
1/p
. It is clear that
D
kα0kpp = X
i=1
|xi | ≤ α0i . Moreover, we have:
›P∣κ
βp
βp
K
D
≤X
i=1
∣χP∣κ
βp
+1
βp
K
=kχkp+Dβp
≤β P (1+K)
It is not possible to conclude α0 ∈ Q from the above inequality since kαkpp is not necessarily equal to
βP(1 + D). However, it is possible to increase the magnitude of the last dimension of α to grow its
p	|xp|K	1/p
norm to the desired value. Let ai0 = ai for i ∈ [D - 1] and aD = (K K + D - Mep 1	.
Therefore, we have α00 ∈ Q and for any i ∈ [D], |xi| ≤ α0i ≤ α0i0. Furthermore for any α ∈ Q, we
have:
kαk2 ≤ D1/2-1/P kαkP
=βpD1∕2τ∕p(1 + D/K)
≤ βD1∕2τ∕p (1 + (1 + e)p - 1)1/p = βD1∕2τ∕p(1 + e)
where the first inequality is based on relationship between '2 and 'p and the rest follows from
definition of Q and K. Therefore, to complete the proof, we only need to bound the size of the set Q.
The size of the set Q is equal to the number of unique solutions for the problem PiD=1 zi = K + D
for non-zero integer variables z% which is (K+D-1).	□
Lemma 12. For any h,p ≥ 2, d, c,γ, μ > 0,δ ∈ (0,1) and U0 ∈ Rh×d, with probability 1 - δ
over the choice of the training set S = {xi}im=1 ⊂ Rd, for any function f(x) = V[Ux]+, such that
V ∈ Rc×h,U ∈ Rh×d,kV> kP,2 ≤ C1, kU - U0kP,2 ≤ C2, the generalization error is bounded as
follows:
,. ^ ,.
Lo(f) ≤ LY(f) +
2√2C(μ + 1)Ph1 -1 Ci (h2-1C2 ∣∣XkF + ∣∣U0X∣∣F)
γm
+3
∕2ln Np,h +ln(2∕δ)
V 2m
where Np,h = Ohhand ||.|卜? is the 'p norm of the column '2 norms.
Proof. The proof of this lemma follows from using the result of Theorem 1 and taking a union bound
to cover all the possible values of {V | kVkP,2 ≤ C1} andU = {U | U - U0P,2 ≤ C2}.
Note that ∀x ∈ Rh and P ≥ 2, we have ∣∣xk2 ≤ h2- 1 ∣∣xkp. Recall the result of Theorem 1, given
any fixed α, β, we have
RS('γ ◦Fw) ≤ 2γm2c kα∣2 (kβ∣2 ∣X∣F + ∣∣U0X∣∣F)
≤ 2√2ch 2 - P kakp (h 1 -1 kβkp kχ∣F + ∣∣UOXilF
(11)
17
Published as a conference paper at ICLR 2019
By Lemma 11, picking E = ((1 + μ)1∕p — 1), We can find a set of vectors, {ai}NP,h, where
K = lhm ,Np,h = (K+-"-2) such that ∀x,kx∣p ≤ Cl, ∃1 ≤ i ≤ Np,h, Xj ≤ αj, ∀j ∈ [h].
Similarly, picking E = ((1 + μ)1∕p 一 1), we can find a set of vectors, {βi}NP,h, where K =
lμm , Np,h	=	(K+h-2) such that ∀x, ∣x∣p	≤	C2, ∃1 ≤ i ≤	Np,h,	Xj	≤	βj,	∀j	∈	[h].	□
Lemma 13. For any h,p ≥ 2, c, d, γ, μ > 0, δ ∈ (0,1) and U0 ∈ Rh×d, with probability 1 — δ
over the choice of the training set S = {xi}im=1 ⊂ Rd, for any function f(x) = V[Ux]+ such that
V ∈ Rc×h and U ∈ Rh×d, the generalization error is bounded as follows:
τ -f …、l 4√2C(μ + 1)2 (h1 -P kV>kp,2 + 1)(h2-P kU - U0kp,2 ∣X∣f + ∣∣u0x∣∣f + 1)
L0(f ) ≤ LY (f) +
γm
+3
ln Np,h +ln(γ√m∕δ)
m
(12)
where Np,h = ("he+"-2) and ∣∣.kp,2 is the `p norm of the column `2 norms.
Proof. This lemma can be proved by directly applying union bound on Lemma 12 with for every C1 ∈
{"1/2-1/p 1 i ∈ [[γ√√m]] } and every C2 ∈ {
h1/2-1/PkXkF
I i ∈ [[γ√4ml] }. For ∣V>kp,2 ≤
"1/2-i/p, we can use the bound where Ci = 1, and the additional constant 1 in Eq. 12 will cover
that. The same is true for the case of IlUkp 2 ≤ hi∕2-1ip∣X∣ . When any of h1/2-1/pkV>kp 2 and
F
h1/2-1/p ∣∣x∣f ∣∣U∣p,2 is larger than ∣"γ√m], the second term in Eq. 12 is larger than 1 thus holds
trivially. For the rest of the case, there exists (Ci, C2) such that h1/2-1/pCi ≤ h1/2-1/pkV>kp,2 + 1
and h1∕2τ∕pC2 ≤ h1/2-1/p ∣X∣f ∣∣X∣∣f ∣∣U∣∣p,2 + 1. Finally, we have γ√m ≥ 1 otherwise the
second term in Eq. 12 is larger than 1. Therefore, 1 γ√√m] ≤ γ√m + 1 ≤ γ√√m.
i
□
We next use the general results in Lemma 13 to give specific results for the case p = 2.
Lemma 14. For any h ≥ 2, c, d, γ > 0, δ ∈ (0, 1) and U0 ∈ Rh×d, with probability 1 - δ over the
choice of the training setS = {xi}im=1 ⊂ Rd, for any function f(x) = V[Ux]+ such that V ∈ Rc×h
and U ∈ Rh×d, the generalization error is bounded as follows:
, . ^ , .
Lo(f) ≤ Lγ (f) +
6√C(kv>kF + i)(ku - U0kF IXIf + I∣U0X∣∣F +1)
+3
γm
5h + ln(γλ∕m∕δ)
,
m
(13)
Proof. To prove the lemma, we directly upper bound the generalization bound given in Lemma 13
for P = 2 and μ = 3√√2 — 1. For this choice of μ andp, we have 4(μ + 1)2/p ≤ 3√2 and ln Np,h is
bounded as follows:
,V , "h∕μ] + h — 2、 ，d d dh∕μ∖ + h — 2] h-∖ ,, 、，d	「h/〃] 一 1、
ln Np," = ln C h - 1	)≤ l(d h - 1	∖ )=(h- 1)ln Se 彳-T-)
≤ (h — 1) ln (e + e~j^~~^ ≤ h ln(e + 2e∕μ) ≤ 5h
□
ProofofTheorem 2. The proof directly follows from Lemma 14 and using O notation to hide the
constants and logarithmic factors.	□
Next lemma states a generalization bound for any p ≥ 2, which is looser than 14 for p = 2 due to
extra constants and logarithmic factors.
18
Published as a conference paper at ICLR 2019
Lemma 15. For any h,p ≥ 2, c, d, γ > 0, δ ∈ (0, 1) and U0 ∈ Rh×d, with probability 1 - δ
over the choice of the training set S = {xi}im=1 ⊂ Rd, for any function f(x) = V[Ux]+ such that
V ∈ Rc×h and U ∈ Rh×d, the generalization error is bounded as follows:
ʌ	4e2√2C(h1 - 1 kV>kp,2 + 1) (h2-1 kU - U0kp,2 kX∣∣F + ∣∣U0X∣∣F +1)
L0(f) ≤ LY(f) +------------------------ʒm-----------------------------------Z
+3

de1-ph — 1] ln (eh) + ln(γ√m∕δ)
m
(14)
k.kp,2 is the `p norm of the column `2 norms.
Proof. To prove the lemma, we directly upper bound the generalization bound given in Lemma 13 for
μ = ep — 1. For this choice of μ and p, We have (μ + 1)2/p = e2. Furthermore, if μ ≥ h, Np,h = 0,
otherwise ln Np,h is bounded as follows:
ln Np,h = ln (dh4 + h - 2) =ln "μe + h - 2) ≤ ln ( ∖e d~ +h-2 广M-1]
∖ h -1	/	1 dh∕μe-1 J	dh∕μe-1 」	)
= (dh∕(ep- 1)e— 1)ln (e + e dhj — 1)e- 1 ) ≤ (∖e^-ph] — 1)ln(eh)
Since the right hand side of the above inequality is greater than zero for μ ≥ h, it is true for every
μ > 0.	□
>Λ	C CEl	L rɪ-il	Γ∙ 1 ∙	,1 Cll	Γ∙	T	<L	1 ♦	7、	.	. 1 ∙ 1 .Λ
Proof of Theorem 5. The proof directly folloWs from Lemma 15 and using O notation to hide the
constants and logarithmic factors.	□
C.3 Proof of the Lower Bound
Proof of Theorem 3. We Will start With the case h = d = 2k, m = n2k for some k, n ∈ N.
We Will pick V = α>
[α1 . . . α2k] for every ξ, and S = {xi}im=1, Where xi
ed ni ].
That is, the
Whole dataset are divides into 2k groups, While each group has n copies of a different element in
standard orthonormal basis.
jn
We further define j(ξ)	= P	ξi, ∀j	∈ [2k] and F = (f1,f2, . . . , f2k) ∈
(j-1)n+1
{—2-k/2,2-k√2}2k×2k be the Hadamard matrix which satisfies(力，fj = δj. Note that for
s ∈ Rd,sj = αjβj,∀j ∈ [d], it holds that
2k
∀i ∈	[d],	max{hs,	[fi]+i,	hs, [—fi]+i} ≥ 2	(hs, fi]+i +	hs,	[—fi] + i) =	Ej== 2-2-1α>β.
Thus without loss of generality, we can assume ∀i ∈ [2k],{s, [f/+)≥ 2-2-1α>β by flipping the
signs of fi .
For any ξ ∈ {—1, 1}n, let Diag(β) be the square diagonal matrix with its diagonal equal to β and
F(ξ) be the following:
Fe(ξ) := [ef1,ef2, . . . ,ef2k] such that if i(ξ) ≥ 0, efi = fi, and if i(ξ) < 0, efi = 0,
and we will choose U(ξ) as Diag(β) × F(ξ).
〜	♦	.1	11 .1 1 Γ' ∙ . ∙	「备 /4	1	Il π / J=∖ Il / T	1.1	C	1
Since F is orthonormal, by the definition of F(ξ), we have kF(ξ)k2 ≤ 1 and the 2-norm of each row
of Fe is upper bounded by 1. Therefore, we have kU(ξ)k2 ≤ kDiag(β)k2 kFe(ξ)k2 ≤ maxi βi, and
19
Published as a conference paper at ICLR 2019
∣∣Ui - u0k2 = I∣uik2 ≤ βike>F(ξ)k2 ≤ βi. Inotherwords, f(X) = V[U(ξ)x]+ ∈ FW. WeWill
omit the index ξ when it’s clear.
Now we have
n	2k	jn	2k
X ξiV[Uxi]+ = X	X	ξiV[Uxi]+ = X j(ξ)V[Uej]+.
i=1	j=1 i=(j-1)n+1	j=1
Note that
□ VUej]+ = ej(DiagGe)α, fj]+)= £j hs, [fj]+i Lj>0 ≥ 2-2-1α>βkj]+.
The last inequality uses the previous assumption, that ∀j ∈ [2k],hs, fj]+i ≥ 2-2-1α>β.
Thus,
m
mRS(FW0) ≥ E X ξiV[U(ξ)xi]+
ξ~{±i}m	|_i=i
≥a>β2-2 T	E
ξ 〜{±1}m
2k
X[ej (ξ)]+
j=1
α>β22-I 〜{Ei}n[klQ + ]
‘C2 I'〜{Ei}n[1e1Q]
≥a>β2 k-1 -2√n
α>β√2d m
8	∖∣~d
α>β√2m
8
where the last inequality is by Lemma 7.
For arbitrary d = h ≤ m, d,h,m ∈ Z+, let k = [log? d[, d0 = h0 = 2k, m0 = [2kC * 2k. Then we
have h0 ≥ h, m0 ≥ 22. Thus there exists S ⊆ [h], such that Pi∈s 0iβi ≥ P3 α%βi. Therefore we
can pick h0 hidden units out of h hidden units, d0 input dimensions out of d dimensions, m0 input
samples out of m to construct a lower bound of "i∈Sαyi v^m ≥ α>β√m.	□
20