Published as a conference paper at ICLR 2019
G-SGD: OPTIMIZING RELU NEURAL NETWORKS IN
its Positively Scale-Invariant Space
Qi Meng1*, Shuxin Zheng27, Huishuai Zhang1, Wei Chen1, QiWei Ye1, Zhi-Ming Ma4,
Nenghai Yu3, Tie-Yan Liu1
1Microsoft Research Asia, 2,3University of Science and Technology of China
4University of Chinese Academy of Sciences
1	{meq, huishuai.zhang, wche, qiwye,tie-yan.liu}@microsoft.com
2	zhengsx@mail.ustc.edu.cn, 3ynh@ustc.edu.cn, 4mazm@amt.ac.cn
Ab stract
It is well known that neural networks with rectified linear units (ReLU) activation
functions are positively scale-invariant. Conventional algorithms like stochastic
gradient descent optimize the neural networks in the vector space of weights, which
is, however, not positively scale-invariant. This mismatch may lead to problems
during the optimization process. Then, a natural question is: can we construct a
new vector space that is positively scale-invariant and sufficient to represent ReLU
neural networks so as to better facilitate the optimization process ? In this paper,
we provide our positive answer to this question. First, we conduct a formal study
on the positive scaling operators which forms a transformation group, denoted as
G. We show that the value of a path (i.e. the product of the weights along the
path) in the neural network is invariant to positive scaling and prove that the value
vector of all the paths is sufficient to represent the neural networks under mild
conditions. Second, we show that one can identify some basis paths out of all the
paths and prove that the linear span of their value vectors (denoted as G-space) is an
invariant space with lower dimension under the positive scaling group. Finally, we
design stochastic gradient descent algorithm in G-space (abbreviated as G-SGD) to
optimize the value vector of the basis paths of neural networks with little extra cost
by leveraging back-propagation. Our experiments show that G-SGD significantly
outperforms the conventional SGD algorithm in optimizing ReLU networks on
benchmark datasets.
1	Introduction
Over the past ten years, neural networks with rectified linear hidden units (ReLU) (Hahnloser et al.,
2000) as activation functions have demonstrated the power in many important applications, such as
information system (Cheng et al., 2016; Wang et al., 2017), image classification (He et al., 2016a;
Huang et al., 2017), text understanding (Vaswani et al., 2017), etc. These networks are usually trained
with Stochastic Gradient Descent (SGD), where the gradient of loss function with respect to the
weights can be efficiently computed via back propagation method (Rumelhart et al., 1986).
Recent studies (Neyshabur et al., 2015; LeCun et al., 2015) show that ReLU networks have positively
scale-invariant property, i.e., if the incoming weights of a hidden node with ReLU activation are
multiplied by a positive constant c and the outgoing weights are divided by c, the neural network
with the new weights will generate exactly the same output as the old one for an arbitrary input.
Conventional SGD optimizes ReLU neural networks in weight space. However, it is clear that weight
vector is not positively scale-invariant. This mismatch may lead to problems during the optimization
process (Neyshabur et al., 2015).
Then, a natural question is: can we construct a new vector space that is positively scale-invariant and
sufficient to represent ReLU neural networks so as to better facilitate the optimization process ? In
this paper, we provide positive answer to this question.
*The notation * denotes equal contribution.
^This work was done when the author was visiting Microsoft Research Asia.
1
Published as a conference paper at ICLR 2019
We investigate the positively scale-invariant space to sufficiently represent ReLU neural networks
by the following four steps. Firstly, we define the positive scaling operators and show that they
form a transformation group (denoted as G). The transformation group G will induce an equivalence
relationship called positive scaling equivalence. Then, We found that the values of the paths are
invariant to positive scaling operators. Furthermore, we prove that two weight vectors are positively
scale-equivalent if and only if the values of the paths in one neural network equal to those in the
other neural network, given the signs of some weights unchanged. That is to say, the values of all the
paths can sufficiently represent a ReLU neural network. After that, we show that the path vectors are
linearly dependent.1 We define the maximal group of paths which are linearly independent as basis
path, which corresponds to the basis of the structure matrix constituted by the path vectors. Thus, the
values of the basis paths are also positively scale-invariant and can sufficiently to represent the ReLU
neural networks. We denote the vector whose coordinations are composed by values of basis paths as
basis path value vector and call the vector space composed by basis path value vector as G-space.
In addition, we prove that the dimension of G-space is "H" smaller comparing to the weight space,
where H is the total number of hidden units in a multi-layer perceptron (MLP) or feature maps in a
convolutional networks (CNN).
To sum up, we find G-space constituted by the values of the basis paths, which is positively scale-
invariant, can sufficiently represent the ReLU neural networks, and has a smaller dimension than the
vector space of weights.
Therefore, we propose to optimize the ReLU neural networks in its positively scale-invariant space,
i.e., G-space. We design a novel stochastic gradient descent algorithm in G-space (abbreviated as
G-SGD) to optimize the ReLU neural networks utilizing the gradient with respect to the values of the
basis paths. First, we design skeleton method to construct one group of the basis paths. Then, we
develop inverse-chain rule and weight allocation to efficiently compute the gradient of the values of
the basis paths by leveraging the back-propagation method. Please note that by using these techniques,
there is very little additional computation overhead for G-SGD in comparison with the conventional
SGD.
We conduct experiments to show the effectiveness of G-SGD. First, we evaluate G-SGD of training
deep convolutional networks on benchmark datasets and demonstrate that G-SGD achieves clearly
better performance than baseline optimization algorithms. Second, we empirically test the perfor-
mance of G-SGD with different degrees of positive scale-invariance. The experimental results show
that the higher the positive scale-invariance is, the larger the performance improvement of G-SGD
over SGD. This is consistent with that, the positive scale-invariance in weight space will negatively
influence the optimization and our proposed G-SGD algorithm can effectively solve this problem.
2	Backgrounds
2.1	Related Works
There have been some prior works that study the positively scale-invariant property of ReLU networks
and design algorithms that are positively scale-invariant. For example, Badrinarayanan et al. (2015)
notice the positive scale-invariance in ReLU netowrks, and inspired by this, they design algorithms to
normalize gradients by layer-wise weight norm. Du et al. (2018) study the gradient flow in MLP or
CNN models with linear, ReLU or Leaky ReLU activation, and prove the squared norms of gradient
across different layers are automatically balanced and remained invariant in gradient descent with
infinitesimal step size. In our work, we do not care whether the models are balanced or not. Besides,
many other optimization algorithms also have positively scale-invariant property such as Newton’s
method and natural gradient descent. The most related work is Path-SGD (Neyshabur et al., 2015),
which also considers the geometry inspired by path norm. This work is different from ours: 1) they
regularize the gradient in weight space by path norm while we optimize the loss function directly
in a positively scale-invariant space; 2) they do not consider the dependency between paths and it’s
hard for them to compute the exactly path-regularized gradients. Different from the previous works,
we propose to directly optimize the ReLU networks in its positively scale-invariant space, instead of
1A path vector is represented by one element in {0, 1}m, where m is the number of weights. Please check
the details in Section 2.2.
2
Published as a conference paper at ICLR 2019
optimizing in the weight space which is not positive scale-invariant. To the best of our knowledge, at
the first time, we solve this mismatch by theoretical analysis and an effective and efficient algorithm.
2.2	ReLU Neural Networks
Let Nw (x) : X → Y denote a L-layer multi-layer perceptron (MLP) with weight w ∈ W ⊂ Rm,
the input space X ⊂ Rd and the output space Y ⊂ RK. In the l-th layer (l = 0, ∙∙∙ ,L), there are
hl nodes. It is clear that, h0 = d, hL = K. We denote the il-th node and its value as Oill and olil,
respectively. We use wl to denote the weight matrix between layer l- 1 and layer l, and use wl (il-1, il)
to denote the weight connecting nodes Oil-1 and Oill . The values of the nodes are propagated as
ol = σ((wl)Tol-1), where σ(∙) = max(∙, 0) is the ReLU activation function. We use (io, ∙∙∙ ,iL)to
denote the path starting from input feature node Oi0 to output node OiL passing though hidden nodes
O11 ,…，OL-1.
Wecan also regard the network structure as a directed graph (O, E), where O = {Oι,…,OH+d+K }
is the set of nodes where H denotes the number of hidden nodes and E = {eij } denote the set of
edges in a network where eij denotes the edge pointing to Oj from nodes Oi. We use we , e ∈ E
to denote the weight on edge e. If |E| = m, the weights compose a vector W = (wι,…，Wm)T.
We define a path as a vector P = (pi,…，PmyT and if the edge e is contained in path p, Pe = 1;
otherwise pe = 0. Because a path crosses L edges for an L-layer MLP, there are L elements
with value 1 and others elements with value 0. Using these notations, the value of path P can be
calculated as vp(W) = Qim=1 Wipi and the activation status of path P can be calculated as ap(x; W) =
Qj:	=1 I(oj (x; W) > 0). We denote the set composed by all paths as P and the set composed by
j :peij =1
paths which contain edge connecting the i0-th input node and the k-th output node as Pi0,k. Thus,
the output can be computed as follows:
d
Nwk(x)=	Vp(w) ∙ ap(x; W) ∙ Xio .	(1)
i0=1 p∈Pi0 ,k
3	Positively Scale-invariant Space of ReLU Networks
In this section, we first define positive scaling transformation group and the equivalence class induced
by this group. Then we study the invariant space under positive scaling transformation group of
ReLU networks and study its dimension.
3.1	Positive Scaling Transformation Group
We formally define the positive scaling operator. We first define a node positive scaling operator
gc,O(w) : W → W with constant c > 0 and one hidden node O as
W
gc,Oil (w),
where Wl(iι-ι,iι) = C ∙ Wl(iι-ι,iι) for i1 = 1,…，hι-ι; u)l+1(iι,iι+ι) = ɪ ∙ wl+1(iι,iι+ι) for
iι+ι = 1, •…，hl+ι; and values of other elements of W are the same with w.
Definition 3.1 (positive scaling operator) Suppose that {Oι,…，Oh } is the Set of all the hidden
nodes in the network where H denotes the number of hidden nodes. A positive scaling operator
g(ci,…，ch)(.) : W → W with ci,…，ch ∈ R+ is defined as
g(cι,…，ch ) (∙) := gcι ,Oι ◦ gc2 ,O2 ◦ ∙∙∙ ◦ gCH ,Oh S ,
where ◦ denotes function composition.
We then collect all the g©,…，^)(•) together to form a set G := {g©, . ,ch)(•) ： ci,…，ch ∈ R+}. It
is easy to check that G together with the operation "◦" is a group which is called positive scaling
transformation group, and we call the group action of G on W as G-action. (Please refer to Section 8
in Appendix.) Clearly, if there exists an operator g ∈ G to make W = g(W0), ReLU networks Nw and
Nw0 will generate the same output for any fixed input x. We define the positive scaling equivalence
induced by G-action.
3
Published as a conference paper at ICLR 2019
Definition 3.2 Consider two ReLU networks with weights w, w0 ∈ W and the positive scaling
transformation group G. We say w and w0 are positively scale-equivalent if ∃g ∈ G such that
W = g(w0), denote as W 〜G w0.
Given G-action on W, the equivalence relation "〜G" partitions W into G-equivalent classes. The
following theorem shows that the sufficient and necessary condition for ReLU networks in the same
equivalent class is that they have the same values and activation status of paths.
Theorem 3.3 Consider two ReLU neural networks with weights w, w0 ∈ W. We have that W 〜G w0
iff for ∀ path p ∈ P and any fixed input x ∈ X, we have vp (w) = vp(w0) and ap(x; w) = ap(x; w).
Invariant variables for a group action are important and widely studied in group theory and geometry.
We say a function f : W → R is invariant variable of G-action if f (W) = f (g(W)), ∀g ∈ G. Based
on Theorem 3.3, a direct corollary is that values and activation status of paths are invariant variables
under G-action. Considering that 1) values of paths are G-invariant variables while the weights aren’t;
2) values of paths together with the activation status determines an positive scale-equivalent class,
and are sufficient to determine the loss, we propose to optimize the values of paths instead of weights.
3.2	Positively Scale-Invariant Space and Its Dimension
Although Theorem 3.3 shows the values of paths are
invariant variables under G-action, we find that the
paths have inner-dependency and therefore their val-
ues and activation statuses are not independent. Let
us consider the example in Figure 1. The values of
paths have the relationship vp4 (w)
Figure 1: This is a simple ReLU network with
one hidden node. Suppose path values are
vp1 (w) = w1w3, vp2 (w) = w1w4, vp3 (w) =
w2w3 , vp4 (w) = w2w4, we can see the inner-
dependency between them, i.e., vp4 (w) =
v一2 (w)∙V-3 (w)
p V I(W)——.A is the structure matrix of this
example.
vp2 (W)∙vp3 (W)
VpI(W)
Using the vector representation of paths that is de-
scribed in section 2.2, we find that their path vectors
follow the relationship p4 = p2 + p3 - p1 , which
means that they are not linearly independent.
For a feedforward ReLU neural network, we suppose
that P ⊂ {0, 1}m is the set composed by all path
vectors. We denote the matrix composed by all paths
as A and call it structure matrix of ReLU networks.
The size of A is m × n where n is the number of
paths. We observe that the paths in matrix A are not linearly independent. Then we study the rank of
matrix A and find a maximal linearly independent group of paths.
Theorem 3.4 If A is the structure matrix for a ReLU network, then we have rank(A) = m - H,
where m is the dimension of weight vector W and H is the total number of hidden nodes for MLP (or
feature maps for CNN models) with ReLU respectively.
Definition 3.5 (basis path) A set of paths Po = {p1,…，pm-H } which is a subset of P is called a
set of basis paths if p1, ∙∙∙ , Pm-H compose a maximal linearly independent group ofcolumn vectors
in structure matrix A.
We design an algorithm called skeleton method to identify basis paths efficiently, which will be
introduced in Section 4. For given values of basis paths and structure matrix, the values of W can not
be determined unless the values of free variables are fixed (Lay (1997)). Assume Ws1 , ∙ ∙ ∙ , WsH are
selected to be the free variables which are called free skeleton weights, we prove that the activation
status can be uniquely determined by the values of basis paths if signs of free skeleton weights are
fixed. Thus, we have the following theorem which is a modification of Theorem 3.3.
Theorem 3.6 Consider two ReLU neural networks with weights W, W0 ∈ W with the same signs of
skeleton weights. We have that W 〜G w0 iff for ∀p ∈ Po, we have Vp (W) = Vp(WO).
The detailed proof of Theorem 3.4 and Theorem 3.6 are both depends on Lemma 9.1 in Appendix. In
the following context, we always suppose that ∀W ∈ W have the same signs of free skeleton weights.
4
Published as a conference paper at ICLR 2019
According to Theorem 3.6 and the linear dependency between values of paths, the loss function can
be calculated using values of basis paths if signs of free skeleton weights are fixed. We denote the
the loss at training instance (x, y) as l(v; x, y) and propose to optimize the values of basis paths.
Considering that values of basis paths are obtained through structure matrix A, the dimension of the
space composed by values of basis paths should be equal to rank(A). Then we define the following
space.
Definition 3.7 (G-space) The G-space is defined as V := {v = (vpι,…,Vpm-H) : V ∈
(R/{0})m-H}.
We call the space composed by the values of basis paths G-space, which is invariant under transfor-
mation group G, i.e., it is composed by invariant variables under G-action. Immediately, we can get
the following corollary according to Theorem 3.6.
Corollary 3.8 The dimension of G-space is m - H, where m is the number of weights and H is the
total number of hidden nodes for MLP or the total number of feature maps for CNN.
We measure the reduction of the dimension for positively scale-invariant space using the invariant
ratio H/m, thus we can empirically test how severe this equivalence will influence the optimization
in weight space.
4 ALGORITHM: G-SGD
In this section, we will introduce the G-SGD that optimizes ReLU neural
network models in the G-space. This novel algorithm makes use of three
methods, named Skeleton Method, Inverse-Chain-Rule (ICR) and Weight-
Allocation (WA), respectively, to calculate the gradients w.r.t. basis path
vector and project the updates back to weights efficiently (with little extra
computation in comparison with standard SGD).
Figure 2: The weights
4.1 S keleton Method	with red color are skele-
ton weights.
Before the calculation of gradients in G-space, we first design an algorithm
called skeleton method to construct skeleton weights and basis paths for
MLP whose depth is L and width is h.
1. Construct Skeleton weights: for weight matrix w2, ∙ ∙ ∙ , WL-1, We select diagonal elements to be
the skeleton weights. For weight matrix w1, we select the element w1(i1 mod d, i1) for column
i1 with i1 = 1,…，h1 to be the skeleton weights. For weight matrix WL, we select the element
wL(iL-1 ,iL-1 mod K) for row iL-1 with iL-1 = 1, ∙ ∙ ∙ , hL-1 to be the skeleton weights. We
call the rest weights non-skeleton weights. Figure 2 gives an illustration for skeleton weights in a
MLP network.
2. Construct basis paths: A path which contains at most one non-skeleton weights is a basis path.
The proof of this statement could be found in Appendix. For example, in Figure 2, the paths in red
color and the paths with only one black weight are basis paths. Beyond that, the paths are non-basis
paths.
Once we have basis paths, we can calculate the gradients w.r.t. basis path vector Vpi, and iteratively
update the model by
v
t+1
pj	=
vptj - ηt
∂l(v; Sit)
∂Vpj
Jj = 1, ∙∙∙ ,m - H,
v=vt
(2)
where St is the mini-batch training data in iteration t. For the calculation of the gradients w.r.t. basis
path vector, we introduce inverse-chain-rule method in next section.
5
Published as a conference paper at ICLR 2019
∂l(w; x, y)
∂l(w; x, y)
∂w1
∂wm
)=(
∂l(v; x, y)
∂l(v; x, y)
∂vp1
∂ vpm-H
(
)∙
4.2 Inverse-Chain-Rule (ICR) Method
The basic idea of the Inverse-Chain-Rule method is to connect the gradients w.r.t. weight vector and
those w.r.t. basis path vector by exploring the chain rules in both directions. That is, we have,
∂vp1	∂vp1
∂w1	∂wm
…	(3)
∂v
pm-H	∂vp
m-H
------- ... 	
∂w1----∂wm
We first compute the gradients w.r.t. weights, i.e., d(∂WX,y for i = 1, ∙ ∙ ∙ ,m using standard back
propagation. Then We solve Eqn.(3) to obtain the gradients w.r.t. basis paths, i.e.,	for
j = 1, ∙ ∙ ∙ ,m 一 H. We denote matrix at the right side ofEqn.(3) as G. Given the following facts:
(1) ∂vp = Wp if the edge e is contained in path p, otherwise 0;(2) according to the skeleton method,
each non-skeleton weight will be contained in only one basis path, which means there is only one
non-zero element in each column corresponding to non-skeleton weights in G, G is sparse and thus
the solution of Eqn.(3) is easy to obtain.
4.3 Weight-Allocation (WA) Method
After the values of basis paths are updated by SGD, in a new iteration, we employ ICR again by
leveraging BP with the new weight. Thus, we need to project the updates on basis paths back to the
updates of weights.
We define the path-ratio of pj at iteration t as Rt(pj) := vpt j /vpt-j 1 and the weight-ratio of wi at
iteration t as rt(wi) := wit/wit-1. Assume that we have already obtained the path-ratio for all the
basis paths Rt+1(pj) according to ICR method and the SGD update rule. Then we want to project the
path-ratios onto the weight-ratios. We use the notation to denote the operation w pj = Qim=1 wipi
Because we have vpj (w) = w pj, the weight-ratios obtained after the projection should satisfy the
following relationship: Rt(pj) = rt (w) pj. Generalize the operator from vectors to matrices 2,
we have the following relationship:
(Rt(p1),…，Rt(Pm-H)) = (rt(wι),…，rt(wm)) Θ A0,	(4)
where the matrix A0 = (p1, •…，Pm-H). According to this relationship, we design Weight-Allocation
Method to project the path-ratio to weight-ratio as described below. Suppose that wι,…，WH
are the free skeleton weights. We first add H elements with value 1 at the beginning in vector
(Rt(p1),…,Rt(Pm-H)) to get a new m-dimensional vector. Then we append H columns in matrix
A0 to get a new matrix A as A = [B, A0] with B = [I, 0]T where I is an H × H identity matrix with
diagonal elements 1 and 0 is an H × m zero matrix with all elements 0. Then it is easy to prove that
rank (A) = m and we have the following relationship:
(1, ∙∙∙ , 1,Rt(p1), ∙ ∙ ∙ ,Rt(pm-H)) = (rt(wι),…，rt(wm)) Θ A.	(5)
We can get the weight-ratio by Θ A-1 on both sides of Eq.(5). Because we have
(rt(wι),…，rt(Wm)) Θ A Θ AT = (rt(wι), ∙ ∙ ∙ ,rt(wm')~) (refer to Section 8 in Appendix), we have
(rt (wι), ∙ ∙ ∙ ,rt(wm)) = (1, ∙∙∙ ,1,Rt (p1), ∙ ∙ ∙ ,Rt(pm-H)) Θ A-1.	(6)
After the projection, we can see that weight-ratios of free skeleton weights equal 1 which means that
free skeleton weights will not be changed during the training process. According to the skeleton
method again, A is a sparse matrix and it is easy to calculate its inverse.
Actually, the projection method is not unique. Although we choose one special projection which fixed
values of free skeleton weights in the Weight-Allocation method, we prove that different projection
methods will results in the same updates in G-space if they don’t change the signs of free skeleton
weights.
Theorem 4.1 Suppose that there are two different projections T1 and T2 that project the path-ratio
to weight-ratio. If the projection will not change the signs of free skeleton weights, the values of basis
paths will keep the same at every iteration for two G -SGD processes that are initialized with the same
values of basis paths and use different projections T1 and T2 in WA method respectively.
6
Published as a conference paper at ICLR 2019
Algorithm 1 G-SGD
Require: initialization w0 , learning rate ηt , training data set D.
1.	Construct skeleton weights using skeleton method and construct AT according to skeleton
method and WA method.
for t = 1,…，T do
2.	Implement feed forward process and back propagation process to get dl(∂WStIw=W=.
3.	Calculate 讥片田)for j = 1,…，m - H according to Eqn.(3).
4.	Using SGD to update the values of basis paths: Vj = Vpj - ηt∙ d∂Vxy ∙
5.	Calculate path-ratios: Rt(pj) = vpt+j1 /vptj.
6.	Calculate weight-ratio rt(wi) using A-1 according to Eqn.(6).
7.	Update the weights as w；+1 = Wi ∙ rt+1(wi).
end for
Ensure: wT .
Please note by combining the ICR and WA methods, we can obtain the explicit update rule for
G-SGD, which is concluded in Algorithm 1. In this way, we obtain the correct gradients. The
extra computational complexity of the ICR and WA methods are far lower than that of forward and
backward propagation, and can therefore be neglected in practice.
5 Experiments
In this section, we first evaluate the performance of G-SGD on training deep convolutional networks
and verify that if our proposed algorithm outperforms other baseline methods. Then we investigate
the influence of positive scaling invariance on the optimization in weight space, and examine whether
optimization in G space brings performance gain. At last, we compare G-SGD with Path-SGD
(Neyshabur et al., 2015) and show the necessity of considering the dependency between paths. All
experiments are averaged over 5 independent trials if without explicit note.
5.1	Deep Convolutional Network
In this section, we apply our G-SGD to image classification tasks and conduct experiments on CIFAR-
10 and CIFAR-100 (Krizhevsky & Hinton, 2009). In our experiments, we employ the original ResNet
architecture described in (He et al., 2016a). Specifically, there is no positive scaling invariance across
residual blocks since the residual connections break down the structure matrix described in Section
3.2, we target the invariance in each residual block. For better comparison, we also conduct our
studies on a stacked deep CNN described in He et al. (2016a) (refer to PlainNet), and target the
positive scaling invariance across all layers. We train 34 layers ResNet and PlainNet models on the
datasets following the training strategies in the original paper, and compare the performance between
G-SGD2 3 and vanilla SGD algorithm. The detailed training strategies could be found in Appendix. In
this section, we focus on the performance of different optimization algorithms, and will discuss the
combination of G-SGD and regularization in Appendix.
Table 1: Classification error rate (%) on image classification task.
		C10	C100
Plain-34	SGD G-SGD	7.76 (±0.17) 7.00 (±0.10)	36.41(±0.54) 30.74 (±0.29)
ResNet-34	SGD G-SGD	7.13 (±0.22) 6.66 (±0.13)	28.60(±0.51) 27.74 (±0.24)
As shown in Figure 3 and Table 1, our G-SGD clearly outperforms SGD on each network and
each dataset. To be specific, 1) both the lowest training loss and best test accuracy are achieved by
2For strict description of operation "", please refer to Section 8 in Appendix.
3Batch normalization is widely used in modern CNN models. Please refer to Appendix for the combination
of G-SGD and batch normalization.
7
Published as a conference paper at ICLR 2019
0'≈d°
OOTH3D
Figure 3: Training loss and test accuracy w.r.t. the number of effective passes on PlainNet and
ResNet.

ResNet-34 with G-SGD on both datasets, which indicates that G-SGD indeed helps the optimization
of ResNet model; 2) Since G-SGD can eliminate the influence of positive scaling invariance across all
layers of PlainNet, we observe the performance gain on PlainNet is larger than that on ResNet. For
PlainNet model, G-SGD surprisingly improves the accuracy numbers by 0.8 and 5.7 for CIFAR-10
and CIFAR-100, respectively, which verifies both the improper influence of positive scaling invariance
for optimization in weight space and the benefit of optimization in G space. Moreover, Plain-34
trained by G-SGD achieves even better accuracy than ResNet-34 trained by SGD on CIFAR-10,
which shows the influence of invariance on optimization in weight space as well.
5.2 The Influence of Invariance
In this section, we study the influence of invariance on the optimization for ReLU Networks. As
proved in Section 3, the dimension of weight space is larger than G-space by H , where H is the total
number of the hidden nodes in a MLP or the feature maps in a CNN. We define the invariant ratio
as H/m. We train several 2-hidden-layer MLP models on Fasion-MNIST (Xiao et al., 2017) with
different number of hidden nodes in each layer, and analyze the performance gap ∆ between the
models optimized by G-SGD and SGD. The detailed training strategies and network structures could
be found in Appendix.
GIOI-Ko- CTC-C-EH
∙10JJB-jSBJ.
Figure 4: Training loss and test error on MLPs. The invariant ratio decreases as H increases.
From Figure 4, we can see that, 1) for each number of H, G-SGD clearly outperforms SGD on both
training loss and test error, which verifies our claim that optimization loss function in G space is a
better choice; 2) as H increases, the invariant ratio decreases (because m also increases for MLP) and
∆ gradually decreases as well, which provides the evidence for that the positive scaling invariance in
weight space indeed improperly influences the optimization.
5.3 Comparison with Path-SGD
In this section, we compare the performance of Path-SGD and that of G-SGD. As described in Section
2.1, Path-SGD also consider the positive scaling invariance, but 1) instead of optimizing the loss
function in G-space, Path-SGD regularizes optimization by path norm; 2) Path-SGD ignores the
dependency among the paths. We extend the experiments in Neyshabur et al. (2015) to G-SGD
8
Published as a conference paper at ICLR 2019
without unbalance initialization, and conduct our studies on MNIST and CIFAR-10 datasets. The
detailed training strategies and description of network structure can be found in Appendix.
ClFAR-IO
MNlST
MNlST
a ι∙75
o
6
；| 1.50
,S
H
1.25
.IOJattc,J.
CIFAR-IO
100 200 300 400 500	0	100 200 300 400 500	0	100 200 300 400 500
Epoch	Epoch	Epoch
(b)	(C)	(d)
Figure 5: Performance of MLP models with Path-SGD and G-SGD.
As shown in Figure 5, while Path-SGD achieves better or equally good test accuracy and training loss
than SGD for both MNIST and CIFAR10 datasets, G-SGD achieves even better performance than
Path-SGD, which is consistent with our theoretical analysis that considering the dependency between
the paths and optimizing in G-space bring benefit.
6	Conclusion
In this paper, we study the G-space for ReLU neural networks and propose a novel optimization
algorithm called G-SGD. We study the positive scaling operators which forms a transformation group
G and prove that the value vector of all the paths is sufficient to represent the neural networks. Then
we show that one can identify basis paths and prove that the linear span of their value vectors (denoted
as G-space) is an invariant space with lower dimension under the positive scaling group. We design
G-SGD algorithm in G-space by leveraging back-propagation. We conduct extensive experiments
to verify the empirical effectiveness of our proposed approach. In the future, we will examine the
performance of G-SGD on more large-scale tasks.
Acknowledgments
This work is partially supported by National Center for Mathematics and Interdisciplinary Sciences
(NCMIS).
References
Vijay Badrinarayanan, Bamdev Mishra, and Roberto Cipolla. Symmetry-invariant optimization in deep networks.
arXiv preprint arXiv:1511.01754, 2015.
Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson,
Greg Corrado, Wei Chai, Mustafa Ispir, et al. Wide & deep learning for recommender systems. In Proceedings
of the 1st Workshop on Deep Learning for Recommender Systems, pp. 7-10. ACM, 2016.
Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous models:
Layers are automatically balanced. Advances in Neural Information Processing Systems, 2018.
Richard HR Hahnloser, Rahul Sarpeshkar, Misha A Mahowald, Rodney J Douglas, and H Sebastian Seung.
Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit. Nature, 405(6789):
947, 2000.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level
performance on imagenet classification. In Proceedings of the IEEE international conference on computer
vision, pp. 1026-1034, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In
European conference on computer vision, pp. 630-645. Springer, 2016b.
9
Published as a conference paper at ICLR 2019
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional
networks. In CVPR, volume 1, pp. 3, 2017.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. CIFAR, 2009.
David C Lay. Linear Algebra and its applications, 1997. Addison Wesley Longman, Inc. ISBN 0-201-76717-1,
1997.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436-444, 2015.
Behnam Neyshabur, Ruslan R Salakhutdinov, and Nati Srebro. Path-sgd: Path-normalized optimization in deep
neural networks. In Advances in Neural Information Processing Systems, pp. 2422-2430, 2015.
David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-propagating
errors. nature, 323(6088):533, 1986.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pp.
5998-6008, 2017.
Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. Deep & cross network for ad click predictions. In
Proceedings of the ADKDD’17, pp. 12. ACM, 2017.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine
learning algorithms. 2017.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.
Shuxin Zheng, Qi Meng, Huishuai Zhang, Wei Chen, Nenghai Yu, and Tie-Yan Liu. Capacity control of relu
neural networks by basis-path norm. arXiv preprint arXiv:1809.07122, 2018.
10
Published as a conference paper at ICLR 2019
APPENDIX: G-SGD: OPTIMIZING RELU NEURAL NETWORKS IN ITS
Positively Scale-Invariant space
The Appendix document is composed of examples of skeleton weights and basis paths for different
MLP structures, proofs of propositions, lemmas and theorems and the additional information about
the experiments in the paper Optimization of ReLU Neural Networks using G-Stochastic Gradient
Descent .
7	Notations
Table 2: Notations
Notations	Object
m	dimension of weight space
H	total number of hidden nodes or feature maps
n	total number of paths
m-H	total number of basis paths and dimension of G-space
W ⊂ Rm	weight vector space
W = (wi, ∙∙∙ ,Wm)	weight vector with m = PL=I hι-ιhi for MLP
wl	weight matrix at layer l with size hι-ι × hi for MLP
wl(il-1 , il)	weight element in matrix wl at position (i1-1, ii)
Oill	the ii-th hidden node at layer l
E= {eij}	the set of edges in neural network model
Table 3: Index
Index	Range	Object
l	{1,…，L}	index of layer
ii	{1,…，hi}	index of hidden nodes at l-layer
(iL, iL-1, ∙∙∙ ,io)	ii ∈ [hi], l ∈ [L]	explicit index of path
p	P	path
pi	{p1, ∙∙∙ ,pm-H } = Po	basis path
sj	{sι,…，SH} ⊂ {1,…，m}	free skeleton weight
Table 4: Mathematical Notations
Notation	Meaning
#	the number of
/	division
◦ function composition
8	Some Concepts in Abstract Algebra
Definition 8.1 (Transformation group) Suppose that G is a set of transformations, and ◦ is an
operation defined between the elements of G. If G satisfies the following conditions: 1) (operational
closure) for any two elements g1 , g2 ∈ G, it has g1 ◦ g2 ∈ G; 2) (associativity) for any three elements
g1, g2, g3 ∈ G, it has (g1 ◦ g2) ◦ g3 = g1 ◦ (g2 ◦ g3); 3) (unit element) there exists unit element e ∈ G,
so that for any element g ∈ G, there is g ◦ e = g; 4) (inverse element) for any element g ∈ G, there
exists an inverse element g-1 ∈ G of g such that g ◦ g-1 = e. Then, G together with the operation
"◦" is called a transformation group.
Definition 8.2 (Group action) If G is a group and W is a set, then a (left) group action φG,W of G
on W is a function φG,W : G × W → W that satisfies the following two axioms (where we denote
φ(g, W) as g ∙ W): 1) (identity) e ∙ W = W; 2) (compatibility) (g ◦ h) ∙ W = g ◦ (h ∙ W) for all g,h ∈ G
and all w ∈ W.
11
Published as a conference paper at ICLR 2019
Definition 8.3 (Operation	) We define as a right group action for matrix Wm×s =
[wij]i=ι,…，mj=ι…，s with Wij ∈ (R∕{0}) as:
∖ Qs=1 sgn(WIj) ∙ lw1j |aj1	…	Qs=1 sgn(WIj ) ∙ lw1j |ajn 1
W Θ A =	…	…	•…	,
_ Qs=I	sgn (Wmj )	∙ ∣Wmj ∣aj1	…Qs=I Sgn (Wmj )	∙ ∣Wmj ∣ajn
where As×n = [ajk]i=ι,…，s；k=i,…，n is a matrix with aj ∈ R.
According to the definition, we can prove that W Θ A Θ A-1 = W if A is a square matrix. We use
A-1 = [Gkz]k=ι,…，s；z=i,…，s. Then the element at the b-th row and c-th column of W Θ A Θ A-1
is calculated as Qj=ι sgn(Wbj) ∙ Qk=1(QS=1 ∙∣wbj∣ajk)akc = Q；=i sgn(Wbj) ∙ Qj=ι ∙∣wbj∣Pk=1 Mkc
When j = c, Pk=ι ajkGkc = 1； otherwise, Pk=ι ajkAkc = 0. Thus we have Q；=i sgn(Wbj) ∙
Qk=I(Qj=ι ∙∣WbjIajk)akc = wbc). Thus we have prove the claim: W Θ A Θ AT = W. Thus Eq.(6)
in the main paper is established.
9 Proofs of Theoretical Results
In this section, we will provide proofs of the lemma and theorems in Section 3 of the main paper.
9.1	Proof of Theorem 3.3
Theorem 3.3: Consider two ReLU neural networks with weights w, w0 ∈ W. We have that W 〜w0
iff for ∀ path p and∀ input x ∈ X, we have vp (W) = vp(W0) and ap(W, x) = ap(W0, x).
Proof: For the necessity, if W 〜w0, then there exist a positive scaling operator g(∙) to make
g(W0) = W. We use il to denote the node index of nodes in layer-l and il ∈ [hl]. Then we have
Wι(iι-ι, iι) = -111- ∙ Cj ∙ W0(iι-ι,iι) for l = 2,…，L 一 1, because each weight may be modified by
cil-1
the operators of its connected two nodes	gcl-1	,Ol-1	and gcl	,Ol . Thus	vp(W)	=	vp(W0)	is satisfied
because
L	L-1 1	1
Vp(W0) = TT W0(ii-ι,iι) = c1ιWi(io,iι) ∙ TT ~7-r ∙ &W0(ii-ι,iι)∙ ∙ l— WL(iL-i,iL) (7)
l=1	l=2 cil-1	ciL-1
L
=	Wl(il-1, il) = vp(W).	(8)
l=1
Next we need to prove that ap(W, x) = ap (W0, x) is also satisfied. Because the value of the activation
is determined by the sign of ol,l = {1,…，L 一 1}, wejust need to prove that
[oW,ι(x),…，oW,hι (x)] = [ci ∙ oW0,1(x),…，Chι ∙ Ow0,hι (x)],
where cli , il ∈ [hl], l ∈ [L 一 1] are positive numbers. We prove it by induction.
(1)	For o1 of a L-layer MLP (L > 2): Suppose that σ(∙) is a ReLU activation function. For the iι-th
hidden node, we have
h0	h0	h0
oW,iι = σ ( £ W1(i0,i1)xi0 I = σ I ɪ2 c1ι ∙ Wi(io,iι)xio I = ciι ∙ σ I ɪ2 Wi(io,iι)xio I = c1ι ∙ oWo,i「
i0=i	i0=i	i0=i
(9)
(2)	For ol of the L-layer MLP (l > 2): Suppose that
[oW,i(χ),…，oW,hj (χ)] = [ci ∙ oWo,i(x),…，chj ∙ oWo,hj (x)],j = 口,…，l-1}.
Then we have
Chl-I	∖	( hl-1	l
X sWfiI)Ow-iι-ι(X) =σ X ~τ-τ ∙ ciι ∙ Wla-I,iI) ∙ ciι-iι ∙ MiI-I (X)
il-1=i	il-1=i cil-1
=ciι ∙ ow0,iι.
12
Published as a conference paper at ICLR 2019
Thus we finish the proof of necessity.
For the sufficiency, we need to prove that if vp(w) = vp(w0) and ap(w, x) = ap(w0, x) for ∀ path p and
∀ input x ∈ X, there exists g ∈ G to make w0 = g(w). First, for hidden node Oi1 in layer-1, we claim
that their incoming weights satisfy the following relationship:
w1 (i0,i1) = C11 ∙ W1(i0,i1),∀i0 ∈ [ho].
Because there exist path pz1,…，pzho ∈ P to make
VpZi (w) : VpZ2 (w):…:VpZho (W) = Wι(1, iι) : w1(2, iι):…:w1(h0, iι),
and vp(w) = vp(w0) for any p, we have
wι(1, iι) : wι(2, iι):…：W1(h0, iι) = w；(1, iι) : w0ι(2,iι):…：w；(ho, i1).
Then the claim is established. Then we prove each ci1 is positive. If there exist a constant ci1 is
negative, then oi； (w, x) 6= oi； (w0, x). If oli (w, x) > 0, we have oli (w0, x) = 0. Here we assume
that x ∈ X where X is a compact set to make that ap(w, x) = ap (w0, x), ∀p, ∀x is equivalent
to sgn(oIil (w, x)) = sgn(oi (w0, x)), ∀x, ∀l = 1,…，L. Thus all the c； for i； = 1,…，h； are
positive. Then we use the operator gci,oi ◦•••◦ g* ,oɪ (W) to make the two networks with the
same weights at layer 1. Then based on the networks with the same weights at layer 1, we gradually
deal with the weights in other layers from layer 2 to the last layer using similar techniques as layer 1.
We can finally construct g ∈ G. Thus we finish the proof of the sufficiency.
9.2 Proof of Theorem 3.4 and Theorem 3.6
In order to prove Theorem 3.4 and Theorem 3.6, we need to prove that there exist a group of paths
which are independent and can represent other paths, and the activation status can be calculated using
values of basis paths and signs of free skeleton weights. In order to simplify the proof, we leverage
the basis paths constructed by skeleton method. We only show the proof of the following lemma,
from which we can easily get Theorem 3.4 and Theorem 3.6.
Lemma 9.1 The paths selected by skeleton method are basis paths and ap(w, x) can be calculated
using signs of free skeleton weights and the values of basis paths in a recursive way.
Proof sketch: Let us consider the matrix A0 = (p1,…,Pm-H) composed by basis paths constructed
by skeleton method:
I
B；
B02
(10)
There is an identity matrix with size z × z where z is the number of skip skeleton paths. This identity
matrix means that wi is a non-skeleton weight and is contained in pi , i ≤ z . Thus, through the
row transformation of the matrix, B； can be transformed to zero matrix. According to skeleton
method, column vectors in B2 are independent because skeleton weight will only appear in one
all-basis path. Thus the independent property has been proved. Furthermore, by leveraging the
structure of matrix A0, it is easily to check that for a non-skeleton path p, it can be calculated as
p = PZ=ι αipi - Pm=ZHI ajPj where ai = 0 or 1 and αj = 0,1, 2 …，L - 1. More specifically,
ifp contains wi, i ≤ z, then αi = 1; otherwise, αi = 0.
For the second statement, because the activation status is determined by all the oli (x), we just need
to prove the sign of oli (x) is determined by the value of basis path vector V. For each hidden node
Oil , there exist only one basis path which passes it and only contain skeleton weights (We call the
basis path which contains only skeleton weights all-basis path). We denote the value of all-basis
path which passes Olil as Vpa (OlJ = WI(OiJ ∙ QL=2 Wj (OiJ where Wj (OiJ denotes the skeleton
weight ofPa(Oil ) at the j-th layer which is also an skeleton outgoing weight for one hidden node.
We will prove oli (x) can be calculated as
oil (X)=环~~1	Sl、∙ Fil (v； X)
jL=l+1 wj(Oill)
(11)
13
Published as a conference paper at ICLR 2019
where Fil (v; x) is a function which is determined by v and the input x. If Eqn(11) is satisfied, the
sign of oli (x) can be determined as following:
sgn (oIil (x)) = sgn (wι+ι(Ollι)) ∙ ∙ ∙ sgn (WL(Oil)) ∙ sgn (Fil (v; x)).	(12)
Next we prove Eqn(11) by induction.
(	1) For l = 1,
h0	h0	( (i i ))	1	h0
oiι(X) = X wι(i0,iι)) ∙ xio = X W-------------^―― ∙ χio = ψψL----------- X ps(wι(i0,iI))) ∙ χio,
i0=1	i0=1 jL=2wj(Oi11)	jL=2wj(Oi11) i0=1
(13)
where vps (w1(i0, i1))) is the value of basis path which contains w1(i0, i1) and wj (Oi1 ) is the
outgoing skeleton weight (free skeleton weight) of Oi1 . It means that Eqn(11) is satisfied with
FiI(S X) = Ph00=1 Vps (w1(i0,i1))) ∙ χio.
(	2) For l > 1, suppose that oli-1 (x)
• Fi- (v; x),
hl-1
olil (x) =	wι(ii-ι,iι) ∙ oi--1ι
il-1=1
hX VPs (wι(ii-ι,iι)	i-ι
il-=ι Q=ι+ι Wj (Oil) . 0il-1
X _________________VPs (wι(ii-ι,iι)________________
il⅛ι QL=ι+ι Wj (Oil) ∙ W1 (Oi-11) ∙ Qj=1 Wj (Oi-11)
1	X	Vps (wι(iι-1,iι))
QL=ι+1 Wj (Oιl) il⅛1 E-TjwF)
—f——1—∙ Fiι-1 (v; x)
QL=ι Wj (OI-11) il-1
-T--ɪ——ΓF- ∙ Fi-I (v; x)
QL=ι Wj(Oi-1ι)
_______1________ X VPs (Wι(iι-1,iI)) Fι-1 (Vx)
QL=ι+1 Wj (O!l) il⅛1	VPa (OtII)	∙ i j (；)
(14)
(15)
(16)
(17)
(18)
(19)
Thus we have finished the proof the second statement.
9	.3 Proof of Corollary 3.8
Corollary 3.8 The dimension of G-space is m - H, where m is the number of weights and H is the
total number of hidden nodes for MLP or the total number of feature maps for CNN.
Proof: The dimension of a mathematical space (or object) means that the minimum number of
coordinates needed to specify any point within it. When the signs of free variables are fixed, the
number of the variables which are used to represent the output of ReLU neural networks is m - H
according to Theorem 3.4. Thus the dimension of G-space is m - H .
9	.4 Proof of Theorem 4.1
Theorem 4.1: Suppose that there are two different projections T1 and T2 that project the path-ratio
to weight-ratio. If the projection will not change the signs of free skeleton weights, the values of basis
paths will keep the same at every iteration for two G -SGD processes that are initialized with the same
values of basis paths and use different projections T1 and T2 in WA method respectively.
Proof: In fact, the gradients of basis paths only depends on the values of basis paths and are
independent with how the weights distribute.
14
Published as a conference paper at ICLR 2019
Specifically, suppose that the two training processes start from the same initial point, i.e., v(01) = v(02) .
Because the values of paths and the activation status can be calculated using values of basis paths
when the signs of free skeleton weights are fixed, the loss functions can be represented using the
values of basis paths. Then the neural network with equal values of basis paths will produce the same
gradient of basis paths.
After one step of SGD, we have v(11) = v(12) . Then all the path ratios are also the same, i.e.,
R(01) (pi) = R(02) (pi) for every basis path pi. Then we use T1 and T2 to project the path ratios to
weight ratios. Although T1 (R(01) (pi)) 6= T2 (R(02) (pi)), the two networks are still have equal values
of basis paths after the projection, which means they are still in the same equivalent class. Again, the
neural network with equal values of basis paths will produce the same gradient of basis paths. So the
values of basis paths will always keep the same during the G-SGD process.
10 APPENDIX INFORMATION OF THE G-SGD ALGORITHM
10.1	UPDATE RULE AND TIME COMPLEXITY OF G-SGD
Suppose thatPi with i = 1, ∙∙∙ ,z is the basis path containing one non-skeleton weight (denoted as
wi), and pj with j = Z + 1,…，m - H is the basis path containing skeleton weights only, Wj is its
skeleton weights at layer 1, and wk is the free skeleton weights that are not updated.
First, according to the ICR Method, we can get the update rule of value of skeleton paths as below,
t + 1 t	δwi ∙ wi
vi	= vi - ηt	Vt
t+1	t
vj	= vj - ηt
wj ∙ (δw j - Ppi：wj δVi ∙ W)
vt
(20)
(21)
Combined with the weight allocation method, we can get the update rule of G-SGD as follows:
t+1 t
wj	= wj - ηt ∙
δW j ∙ (Wt )2 - wj ∙ Ppi ：wj δwt ∙ wi
(Vj)2
wit+1
wi — ηt∙
δWi∙(wt)2
(Vi )2
rt (wj : pi)
wkt+1 = wkt , wk 6= wi , wj
(22)
(23)
(24)
where rt (wj : pi) is ratio of the skeleton weight wj at layer 1 which is contained in basis path pi.
If the free skeleton weights wk are initialized as 1, then vjt = wjt. Thus we can first calculate the
path-ratio for all-basis paths:
t
Rtj	vj+1	1	wj∙(δwj - PpijVi ∙ W)	1
Rt(Pj) = j- = 1 - 用--------------j------------ = 1 - 小
δWj - Ppi ：wj δVi ∙ Wit
wjt
(25)
According to the WA method, rt (wj : pi) = Rt(pj). Then the update rule for different kinds of
weights can be classified into the following three types
wi+1 = wj∙ Rt(Pjj
t	δWi∙(wi)2
t + 1	wi - ηt ∙	(Vi)2
wi	= ------77-------ɪ----
i	rt (wj : Pi)
t+1 t
wk = wk , wk 6= wi , wj
(26)
(27)
(28)
where rt(wj : pi) is ratio of the skeleton weight wj at layer 1 which is contained in basis path pi, wj
denotes the skeleton weight at layer 1, wi denotes the non-skeleton weight and wk denotes the free
skeleton weights (the skeleton weights that not in layer 1).
The forward and backward propagation take the dominating time in both mini-batch SGD and G-SGD.
To be specific, if the mini-batch size is B, the forward and backward propagation would both take
15
Published as a conference paper at ICLR 2019
BT time. Comparing with SGD, the extra computation is the calculation of Rt(pj), which the time
complexity is independent with the mini-batch and is equal to the update step in SGD. Thus the time
complexity of G-SGD is upper bound by O((B + 1)T). Please see the training throughput of our
experiments on GPU server in Section 11.1.
10.2	Skeleton Method for ResNet and ICR Method for Batch Normalization
For ResNet, we implement the skeleton method to construct skeleton weights and basis paths in each
residual block. Because of the skip-connection, there is an identity weight which doesn’t change
during the optimization. Thus, if the skip-connected weight connects node O, there isn’t a positive
scaling operator go,c ∈ G to make w 〜go,c(w). So We can't construct basis paths for the whole
network. The invariance of ResNet only exists in each residual block. In this sense, the equivalence
of invariance is less severe than other neural network structure.
Because of the existence of the batch-normalization layers, the output of neural network with BN
l
is oij
方,Where (Oij
means the j -th output in layer-l Which is calculated using the i-th
sample, μj = 1 Pb=1 OIij is the expectation of oj,i = 1,…，b and σj = 1 Pb=I(Oij - μj)2 is
the variance. Assume that olij = wjl oli-1 and the inputs oli-1 has expectation 0 and variance 1 (It can
be roughly satisfied for neural netWorks With BN.), We have σj2 ≈ kwjl k2. If We define the value of a
Path as Vw (P) = QL=I Wlkw-Ikil)
Where wil denotes the incoming Weight vector of node Oil . Thus
the loss function of the NN With BN layers can be approximately represented as l(v; x, y).
Thus inverse-chain-rule for NN With BN layer can be approximated by the folloWing equations. If w
is an incoming Weight of node O, We have
∂l(w; x,y) 〜X ∂l(v; x,y) ,	∂vi________
∂wl(ii-ι,iι)	∂vi	∂wl(ii-ι,iι) IlwllI∣ ,
Which results in
Kw;X叫∙kwil k ≈ X ^vxy ∙ A JVir.	(30)
∂wl (il-1 , il)	l	i=1	∂vi	∂wl (il-1 , il)
Then We use the above equation in the ICR methods.
11 Appendix Information of the Experiments
11.1	TRAINING THROUGHPUT OF G-SGD
Training throughput on GPU server with PCI
switch
■ SGD Plain34 B^-SGD Plaiπ34 ■ SGO ResNet34 ■ g-SGD ResNet34
Figure 6: Training throughput on GPU server With 4 NVIDIA GTX Titan Xp GPUs and PCI sWitch.
Mini-batch size per GPU is alWays 128.
We implement our G-SGD using the Pytorch frameWork With v0.31 stable release, and conduct our
experiments comparing With Pytorch built-in SGD optimizer. Our experiments are conducted on a
16
Published as a conference paper at ICLR 2019
GPU server with 4 NVIDIA GTX Titan Xp GPUs and PCI switch. We show the training throughput
(processing images per second) on CIFAR-10 dataset with SGD and G-SGD optimizer on different
network architectures. The multi-GPU training is done by Pytorch built-in multiple GPU module
torch.nn.parallel.DataParallel based on NCCL. As shown in Figure 6, when the mini-batch size is
set to 128, the training throughput of G-SGD is slightly lower than vanilla SGD by about 7%, which
indicates that our implementation of G-SGD is indeed efficient.
11.2	Initialization Method of Skeleton Weights
According to our analysis in the main paper, only the signs of skeleton weights matter the optimization.
Thus we need to determine the signs of skeleton weights before training process. For the absolute
value of skeleton weights, we can see from section 10.1 that different absolute value of skeleton
weights well determine different scale of learning rate. Although our theoretical results show that the
absolute value of skeleton weights can be randomly set, we choose them to be 1 for easier learning
rate tuning and robustness.
In order to verify how signs of skeleton weights influence the performance. We test the performance
for various combination of signs for them on image classification task (see section 5.2). Results
shows that there are no differences for them. A intuitive explanation is that the selected network
model is over-parameterized and the approximation ability will not be influenced by signs of skeleton
weights. For simplicity, we initialize the value of skeleton weights as 1.
11.3	OPTIMIZING 110-LAYER RESNET WITH G-SGD
In this section, we study the optimization performance of G-SGD on deeper ResNet model with 110
layers. We employ the same training strategies on both model structures, which are detailed described
in next subsection. As we can see in the Figure 7 and Table 5, our G-SGD clearly outperforms
SGD on both shallow and deep ResNet model. The best test accuracy on ResNet-110 are achieved
with G-SGD on both dataset, which meets the same conclusions in Section 5.1.Please note that
deeper ResNet doesn’t significantly outperform shallow ResNet on CIFAR-100 dataset, which also
be observed in the original ResNet paper He et al. (2016a) and the authors propose a new ResNet
architecture to solve this problem in He et al. (2016b).
Training loss	Test accuracy
→≈ Res Net-34 SGD
T- Res Net-34 e-SGD
→≈ ResNet-IlOSGD
-→- ResNet-IlOASGD
0	40	80	120	160
Epoch
0	40	80	120	160
Epoch
Figure 7: Training loss and test accuracy on 110-layer ResNet network w.r.t. the number of effective
passes on CIFAR-10 and CIFAR-100 dataset.
11.4	Detailed Training Strategies in Section 5.1 and 11.2
In previous experiment section, we extend our G-SGD to deep convolutional networks. CIFAR-
10 and CIFAR-100 have been used in the experiment. We apply random crop to the input image
by size of 32 with padding 4, and normalize each pixel value to [0,1]. We then apply random
horizontal flipping to the image. The mini-batch size of 128 is used in this experiment. The training
is conducted for 64k iterations. We follow the learning rate schedule strategy in the original paper
17
Published as a conference paper at ICLR 2019
Table 5: Classification error rate (%) on image classification task.
		C10	C100
ResNet-34	SGD G-SGD	7.13 (±0.22) 6.66 (±0.13)	28.60(±0.51) 27.74 (±0.24)
ResNet-110	SGD G-SGD	6.83 (±0.25) 6.49 (±0.06)	29.44(±0.66) 27.74 (±0.36)
(He et al., 2016a), specifically, the initial learning rates of vanilla SGD and G-SGD are set to 1.0
and then divided by 10 after 32k and 48k iterations. The ResNet implementation can be found in
https://github.com/pytorch/vision/ and the models are initialized by the default methods in PyTorch.
11.5	THE COMBINATION OF G-SGD AND REGULARIZATION
The optimization algorithms achieve better generalization performance on test dataset by combin-
ing with proper regularization methods. In the previous experiments, we focus on the difference
performance of optimization algorithms. In this section, we conduct the experiments to investigate
the combination of G-SGD and regularization. In weight space, weight norm is widely used as
regularization for ReLU networks (He et al., 2016a; Huang et al., 2017). Recently, (Zheng et al.,
2018) propose the basis path norm in G-space. In this section, we reproduce the experiments in
(He et al., 2016a; Zheng et al., 2018) on SGD regularized by weight norm (SGD+WD) and G-SGD
regularized by basis path norm (G-SGD+BPR), and extend them on CIFAR-100 dataset. The learning
rate of 1.0 is widely used to train ResNet model and its variants on CIFAR dataset, hence we employ
it in our experiment as well. We do a wide range grid search for the hyper-parameter λ for weight
decay and basis path regularization from {0.1, 0.2, 0.5} × 10-α, where α ∈ {3, 4, 5, 6}, and report
the best performance based on the CIFAR-10 validation set. We use the same hyper-parameter on
CIFAR-100 dataset.
11.6	Detailed Training Strategies in Section 5.2
In this section, our aim is to verify the influence of invariance to optimization in weight space.
We train several 2-hidden-layer MLP models with different invariant ratio (i.e. H/m) on Fasion
dataset. The original size of input image is 28 × 28. We normliazed the input data, and to reduce
the dimensions of input feature, we downsample the image to 7 × 7 by using average pooling. The
network structue is followed by [49:h:h:10] where h is the number of hidden nodes in each layer. The
detailed model properties are shown in table 6. All models are initialized by (He et al., 2015) except
the skeleton weights which is mentioned in Section 4 without explicit note. We use the learning rate
of 0.01 and mini-batch size of 64 for vanilla SGD and G-SGD, and train each model for 100 epochs.
Table 6: Network information in Section 5.2.
#hidden nodes	8	16	32	64	128	256	512	1024
#weights	536	1200	2912	-~7872^^	23936	80640	292352	1108992
H	16	32	64	128	256	512	1024	2048
invariant ratio	1.49×10-2	1.33×10-2	1.10×10-2	8.13×10-3	5.35×10-3	3.17×10-3	1.75×10-3	9.23×10-4
As shown in Figure 8 and 9, the G-SGD achieves clearly better performance than SGD with combining
regularization method on all models and all datasets. To be specific, the test accuracy of 94.29% is
gained by ResNet-34 trained by SGD and weight decay on CIFAR-10 dataset (the number reported in
(He et al., 2016a) is 91.25%), while G-SGD with basis path regularization improves the test accuracy
to 94.67%. On CIFAR-100 dataset, the test accuracy of ResNet-34 trained by SGD and weight decay
is 74.39% (the ResNet-34 result on CIFAR-100 hasn’t been reported in He et al. (2016a), a result
of ResNet-110 with similar training strategy on this dataset is reported in Zagoruyko & Komodakis
(2016) which is 74.84%), while G-SGD with basis path regularization improves the test accuracy to
75.20%. The experimental results verify our analysis again that optimization in G-space is a better
choice.
18
Published as a conference paper at ICLR 2019
0∙TT
loo
1 1
寸lNU-d
Figure 8: Training loss and test accuracy w.r.t. the number of effective passes on CIFAR-10 dataset.
Training Loss
寸"lc,NU--d 寸"lc,NSlu≈
80	100	120	140	16Q
Epoch
Figure 9: Training loss and test accuracy w.r.t. the number of effective passes on CIFAR-100 dataset.
11.7 Detailed Training Strategies in Section 5.3
Path-SGD (Neyshabur et al., 2015) also notice the positive scale invariance in neural networks with
linear or ReLU activation. Instead of optimizing the loss function in G-space, they use path norm as
regularizer to the gradient in weight space. Meanwhile, the dependency among all paths hasn’t been
noted, which leads to the computation overhead of the gradient of path norm is very high. In section
5.3, we extend the experiments in (Neyshabur et al., 2015) to G-SGD on MNIST and CIFAR-10
dataset. A 5-hidden-layer MLP model is used in this experiment with 64 units in each layer. We do
grid search for the learning rate of each algorithm from 1.0 × 10-α, where α is an integer between 0
to 10. We report the best result for each algorithm. The mini-batch size of 64 is used, and the input
images of gray scale are normalized to [0, 1].
19