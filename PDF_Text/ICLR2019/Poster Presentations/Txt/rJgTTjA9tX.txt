Published as a conference paper at ICLR 2019
The Comparative Power of ReLU Networks
and Polynomial Kernels in the Presence of
Sparse Latent Structure
Frederic Koehler*
Department of Mathematics
Massachusetts Institute of Technology
Andrej Risteski *
Department of Mathematics and IDSS
Massachusetts Institute of Technology
Ab stract
There has been a large amount of interest, both in the past and particularly recently,
into the relative advantage of different families of universal function approximators,
for instance neural networks, polynomials, rational functions, etc. However, current
research has focused almost exclusively on understanding this problem in a worst-
case setting: e.g. characterizing the best L1 or L∞ approximation in a box (or
sometimes, even under an adversarially constructed data distribution.) In this
setting many classical tools from approximation theory can be effectively used.
However, in typical applications we expect data to be high dimensional, but struc-
tured - so, it would only be important to approximate the desired function well
on the relevant part of its domain, e.g. a small manifold on which real input data
actually lies. Moreover, even within this domain the desired quality of approxima-
tion may not be uniform; for instance in classification problems, the approximation
needs to be more accurate near the decision boundary. These issues, to the best of
our knowledge, have remain unexplored until now.
With this in mind, we analyze the performance of neural networks and polyno-
mial kernels in a natural regression setting where the data enjoys sparse latent
structure, and the labels depend in a simple way on the latent variables. We give
an almost-tight theoretical analysis of the performance of both neural networks
and polynomials for this problem, as well as verify our theory with simulations.
Our results both involve new (complex-analytic) techniques, which may be of
independent interest, and show substantial qualitative differences with what is
known in the worst-case setting.
1	Introduction
The concept of representational power has been always of great interest in machine learning. In part
the reason for this is that classes of “universal approximators” abound - e.g. polynomials, radial
bases, rational functions, etc. Some of these were known to mathematicians as early as Bernstein and
Lebesgue* 1 - yet it is apparent that not all such classes perform well empirically.
In recent years, the class of choice is neural networks in tasks as simple as supervised classification,
and as complicated as reinforcement learning - inspiring an immense amount of theoretical study.
Research has focus on several angles of this question, e.g. comparative power to other classes of
functions (Yarotsky, 2017; Safran and Shamir, 2017; Telgarsky, 2017), the role of depth and the
importance of architecture (Telgarsky, 2016; Safran and Shamir, 2017; Eldan and Shamir, 2016), and
many other topics such as their generalization properties and choice of optimization procedure (Hardt
et al., 2016; Zhang et al., 2017; Bartlett et al., 2017).
* Email: fkoehler@mit.edu. Research is partially supported by NSF Large CCF-1565235 and Ankur Moitra's
David and Lucile Packard Fellowship.
,Email: risteski@mit.edu.
1Lebesgue made use of the universality of absolute value and hence ReLu - see the introduction of (Newman
et al., 1964).
1
Published as a conference paper at ICLR 2019
Our results fall in the first category: comparing the relative power of polynomial kernels and ReLU
networks - with a significant twist, that makes our results more relevant to real-life settings. The
flavor of existing results in this subject is roughly the following: every function in a class C1
can be approximately represented as a function in a different class C2, with some blowup in the
size/complexity of the function (e.g. degree, number of nodes, depth). The unsatisfying aspect of such
results is the “worst-case” way in which the approximation is measured: typically, one picks a domain
coarsely relevant for the approximation (e.g. an interval or a box), and considers the L∞, L2, L1 , . . .
norm of the difference between the two functions on this domain. In some of the constructions (e.g.
(Eldan and Shamir, 2016; Safran and Shamir, 2017)), the evaluation is even more adversarial: it’s the
mean-square error over a specially-designed measure.
Instead, in practically relevant settings, it’s reasonable to expect that approximating a predictor
function well only on some “relevant domain” would suffice, e.g. near the prediction boundary or
near a lower-dimensional manifold on which the data lives, as would be the case in settings like
images, videos, financial data, etc. A good image classifier need not care about “typical” data points
from the '∞-ball, which mostly look like white noise.
The difficulty with the above question is that it’s not immediate how to formalize what the “relevant
domain” is or how to model the data distribution. We tackle here a particularly simple (but natural)
incarnation of this question: namely, when the data distribution has sparse latent structure, and all
we ask is to predict a linear function of the latent variables based upon (noisy) observations. The
assumption of sparsity is very natural in the context of realistic, high-dimensional data: sparsity under
the correct choice of basis is essentially the reason that methods such as lossy image compression
work well, and it is also the engine behind the entire field of compressed sensing (Donoho, 2006).
2	Overview of results
We will be considering a regression task where the data has a sparse latent structure. More precisely,
we wish to fit pairs of (observables, labels) (X, Y ) generated by a (latent-variable) process:
•	Sample a latent vector Z ∈ Rm from H, where H is a distribution over sparse vectors.
•	To produce X ∈ Rn, set X = AZ + ξ, where the noise ξ 〜subG(σ2) is a subgaussian
random vector with variance proxy σ2 (e.g. N(0, σ2I)).
•	To produce Y ∈ R, we set Y = hw, Zi.
We hope the reader is reminded of classical setups like sparse linear regression, compressive sensing
and sparse coding: indeed, this distribution on the data distribution X is standard in these setups. In
our setting, we additionally attach a regression task to this data distribution, wherein the labels Y are
linearly generated2 by a predictor w from the latent vector Z .
Note our interest is slightly different than usual: in the traditional setup, we are interested in the
statistical/algorithmic problem of inferring Z, given X as input (the former studying the optimal
rates of “reconstruction” for Z, the latter efficient algorithms for doing so). In particular, we do not
typically care about the particular form of the predictor as long as it is efficiently computable.
By contrast, we want to understand how well different subsets of universal approximator families can
fit the data points (X, Y ). Namely, regardless of the specifics of the training procedure, the end will
be an element of some function class like a linear function of a kernel embedding of X , or a neural
network. Therefore, we ask if these classes are rich enough to reconstruct Y given X accurately
(i.e. compared to the Bayes-optimal estimator E[Y |X]): if the answer is negative, then we know
our predictor will perform poorly, no matter the training method. We measure the performance of
these estimators in the natural3 distributional sense: expected reconstruction error, E[(Y - Y)2].
Informally, what we will show is the following.
Theorem (Informal). For the problem of predicting Y given X in the generative model for data
described above, it holds that:
2One could also imagine producing discrete labels by applying a softmax operation. We stick to the regression
setting for reasons of technical simplicity and leave generalizing our results to future work.
3This is natural because it is the (only) loss which the conditional expectation E[Y |X] minimizes. In
statistical language, we are asking about the minimum excess risk achievable by estimators in our function class.
2
Published as a conference paper at ICLR 2019
(	1) Small two-layer ReLU networks achieve close to the statistically optimal rate.
(	2) Polynomial predictors of degree lower than log m achieve a statistical rate which is substantially
worse. (In fact, in a certain sense, close to “trivial”.) Conversely, polynomial predictors of degree
O((log n)2) achieve close to the statistically optimal rate.
The lower bound in (2) is relevant since fitting a polynomial to data points of the form (xi, yi)
requires4 searching through the space of multivariate polynomials of degree Ω(log m) which has
dimension mω(Iog(m)), and thus even writing down all of the variables in this optimization problem
takes super-polynomial time. Practical aspects of using polynomial kernels even with much lower
degree than this have been an important concern and topic of empirical research; see for example
(Chang et al., 2010) and references within. On the other hand, the upper bound in (2) shows that
our analysis is essentially tight: greater than polylog(m) degree is not required to achieve good
statistical performance, which is qualitatively different from the situation in worst-case analyses (see
Section 4.2.2 for more details). Our mathematical analysis closely matches the observed behavior in
experiments: see Section 6.
For formal statements of the theorems, see Section 4.
3	Prior Work
There has been a large body of work studying the ability of neural networks to approximate polyno-
mials and various classes of well-behaved functions, such as recent work (Yarotsky, 2017; Safran
and Shamir, 2017; Telgarsky, 2017; Poggio et al., 2017). These results exclusively focus on the
worst-case setting where the goal is to find a network close to some function in some norm (e.g. L∞
or L1 -norm, often under an adversarially chosen measure).
In contrast there is little work on the problem of approximating ReLU networks by polynomials,
mostly because it is well-known by classical results of approximation theory (Newman et al., 1964;
DeVore and Lorentz, 1993) that polynomials of degree Ω(1∕e) are required to approximate even
a single ReLU function within error in L∞-norm on [-1, 1]. On the other hand, we will show
that if we do not seek to achieve -error everywhere for the ReLU (in particular not near the non-
smooth point at 0) we can build good approximations to ReLU using polynomials of degree only
O(log2(1/)) (see discussion in Section 4.2.2 and Theorem 5.2).
Because of the trivial Ω(1∕e) lower bound for worst-case approximation of ReLU networks by
polynomials, (Telgarsky, 2017) studied the related problem of approximating a neural network by
rational functions. (A classical result of approximation theory (Newman et al., 1964) shows that
rational functions of degree O(log2(1/)) can get within -error of the absolute value function.) In
particular, (Telgarsky, 2017) shows that rational functions of degree polylog(1/) can get within
distance in L∞-norm of bounded depth ReLU neural networks.
Somewhat related is also the work of (Livni et al., 2014) who considered neural networks with
quadratic activations and related their expressivity to that of sigmoidal networks in the depth-2
case building on results of (Shalev-Shwartz et al., 2011) for approximating sigmoids. The result
in (Shalev-Shwartz et al., 2011) is also proved using complex-analytic tools, though the details are
substantially different.
The work of (Zhang et al., 2016) studied the power of kernel regression methods to simulate a certain
class of neural networks. More precisely, they bounded the `2 norm of kernel regression models
approximating neural networks with bounded depth, “nice” activation functions (not including ReLU),
and small input and edge weights. By standard generalization theory, this gives a corresponding
sample complexity result for improper learning via kernels. In our setting, their result does not apply:
first, the network of interest has ReLU activations; even ignoring this issue, their bounds would be
4We note that for kernel ridge regression, one can use the kernel trick to reduce to train in a space with
dimension equal to the size of the training dataset. However, since training data sets are often very large, this is
not necessarily helpful. More usefully, in the classification context, the optimization may become feasible due to
the existence of a smaller number of support vectors. In this case, the ability of algorithms to efficiently find a
good function depends on more complex interactions between the particular choice of kernel and the margin
properties of the data; we leave analyzing these kinds of quantities for future work.
3
Published as a conference paper at ICLR 2019
roughly exponential in n because the '2 norm of the network's input vector is large, of order Θ(σ√n).
.
There is a vast literature on high dimensional regression and compressed sensing which we do not
attempt to survey, since the main goal of our paper is not to develop new techniques for sparse
regression but rather to analyze the representation power of kernel methods and neural networks.
Some relevant references for sparse recovery can be found in (Vershynin, 2018; Rigollet, 2017). We
only emphasize that the upper bound via soft thresholding we show (Theorem 4.1) is implicit in the
literature on high-dimensional statistics; we include the proofs here solely for completeness.
4	Main Results
In this section we will give formal statements of the results and give some insight into the techniques
used.
First, we state the assumptions on the parameters of our generative model:
•	Z is sparse: more precisely, |supp(Z)| ≤ k and kZ k1≤M with high probability.5 6
•	A is a μ-incoherent n X m matrix, which means that k A>A 一 11∣∞ ≤ μ for some μ ≥ 0.
•	kwk∞ = 1 (w.l.o.g., since changing the magnitude of w rescales Y)
The assumption on A is standard in the literature on sparse recovery (see reference texts (Rigollet,
2017; Moitra, 2018)). In general one needs an assumption like this (or a stronger one, such as the
RIP property) in order to guarantee that standard algorithms such as LASSO actually work for sparse
recovery. For the reader not familiar with this literature, this property is a proxy for the matrix being
“random-like” - e.g. a matrix with i.i.d. entries of the form ±l∕√n has μ = O(1∕√n), even when
m >> n. We also note that for notational convenience, we will denote kAk∞ = maxi,j |Ai,j |.
Before proceeding to the results, we note that the first-time reader may freely assume that μ = 0
and n = m; the results are still interesting in this setting and no important technical idea is needed
for the more general case. For the upper bounds, we have included results for the more general
setting (with μ ≥ 0) to show that our results are relevant even to very high-dimensional settings
where m >> n. We have only proven the lower bound in the case μ = 0: this is the easiest setting
for algorithms, so this makes the lower bounds the strongest.
4.1 Regression Using ReLU Networks
We prove the following theorem, which shows that small 2-layer ReLU networks can achieve
an almost optimal statistical rate. Let us denote the soft threshold function with threshold τ as
Pτ(x) := sgn(x) min(0, |x| — T) = ReLU(X — T) 一 ReLU(-x + T). Let,s introduce the notation
椁m to denote the map given by applying ρτ coordinate-wise to a vector in Rm. Consider the
following estimator (for y), corresponding to a 2-layer neural network:
Znn ：= ρfm(AτX)
YNN := hw, ZNNi
We can prove the following result for the estimator (see Appendix A of the supplement):
Theorem 4.1 (2-layer ReLU). With high probability, the estimator YNN satisfies
(YNN — Y)2 = O((1 + μ)σ2k2 log(m) + μ2k2M2)
Notice that the size of the ReLU net is comparable to the input: one of the layers has the same
dimension as A, the other the same dimension as w. Furthermore, to interpret this result, recall that
we think of μ as quite small - in particular μ《1. Thus the error of the estimator is essentially
O(σ2k2 log(m)), i.e. essentially ∣σ∣ error “per-nonzero-coordinate”. It can be shown that this upper
5See Proposition 1 of Zhang et al. (2016)
6The assumed 1-norm bound M plays a minor role in our bounds and is only used when the incoherence
μ > 0.
4
Published as a conference paper at ICLR 2019
bound is nearly information-theoretically optimal (see Remark B.1), except that there is an additional
factor of k . This additional factor is artificial and can be removed with added technical effort; we
show how to do this in the μ = 0 case in Theorem A.1.
We emphasize that the analysis of this kind of soft thresholding estimator is implicit in much of the
literature on sparse linear regression. For completeness, we include a complete and self-contained
proof of Theorem 4.1 in Section A.
4.2 Regression Using Polynomials
4.2	. 1 Lower B ounds for Low-Degree Polynomials
We first show that polynomials of degree smaller than O(log m) essentially cannot achieve a “non-
trivial” statistical rate. This holds even in the easiest case for the dictionary A: when it’s the identity
matrix.
More precisely, We consider the situation in which A is an orthogonal matrix (i.e. μ = 0,m = n),
w ∈ {±1}m, the noise distribution is Gaussian N(0, σ2I), and the entries of Z are independently 0
with probability 1 - k/m and N(0, γ2) with probability k/m. Then we show
Theorem 4.2. Suppose k < m/2 and f is a multivariate degree d polynomial. Then
γ2k
E[(f(X) - Y)2] ≥ (1/4)
1 + pk/m(d + 1)3d+2 (1 + (γ∕σ)d)
To parse the result, observe that the numerator is of order γ2k which is the error of the trivial
estimator7 and the denominator is close to 1 unless d is sufficiently large with respect to m. More
precisely, assuming the signal-to-noise ratio γ∕σ does not grow too quickly with respect to m, we see
thatthe denominator is close to 1 unless dd = Ω(√m), i.e. unless d is of size Ω((log m)/log log m).
On a technical note we observe that this statement is given with respect to expectation but a similar
one can be made with high probability, see Remark B.2.
4.2	.2 Nearly Matching Upper Bounds
The lower bound of the previous section leaves open the possibility that polynomials of degree
O(polylog(m)) still do not suffice to perform sparse regression and solve our inference problem;
Indeed, it is a well-known fact (see e.g. (Telgarsky, 2017)) that to approximate a single ReLU to
-closeness in infinity norm in [-1, 1] requires polynomials of degree poly(1/); this follows from
standard facts in approximation theory (DeVore and Lorentz, 1993) since ReLU is not a smooth
function.
Proceeding with this “worst-case” way of thinking: our upper bound follows by designing a polyno-
mial approximation to ReLU into our neural network construction; since estimates for Y typically
accumulate error from estimating each of the m coordinates of Z, to guarantee accurate reconstruction
we would need me to be small. Plugging in the the best approximation to ReLU in infinity norm,
we would need a Ω(√m)-degree polynomial for this to yield a multivariate polynomial with similar
statistical performance to the 2-layer ReLU network which computes YNN. Thus, naively, we might
suspect that the degree of the kernel needs to be as high as √m to get a reasonable approximation.
Surprisingly, we show this intuition is incorrect! In fact, we show how using only a polylog(m)
degree polynomial, our converted ReLU network has similar statistical performance. Formally this is
summarized by the following theorem, where Yd,M is the corresponding version of YNN formed by
replacing each ReLU by our polynomial approximation.
Theorem 4.3. Suppose T = Θ(σ,(1 + μ) log m+μM) andd ≥ do = Ω((2+ M) log2(Mm∕τ2)).
With high probability, the estimator Yd,M satisfies8
(Yd,M - Y)2 = O(k2((1 + μ)σ2 log(m) + μ2M2))
7I.e. the estimator which always returns 0, without looking at the data.
8As in Theorem 4.1, there is a spurious factor of k in this bound which can be removed with additional
technical effort. In particular in the μ = 0 case we can remove it using the same argument as Theorem A.1;
details are omitted.
5
Published as a conference paper at ICLR 2019
The idea behind our construction is described in Section 5.3. Our methods are novel and may be of
independent interest; we are not aware of a way to get this result using only generic techniques such
as FT-Mollification (Diakonikolas et al., 2010).
5	Overview of proofs
In this section, we will sketch the ideas behind the proofs of our results. The full proofs are relegated
to the appropriate appendices. We proceed with each of our results in turn.
5.1	Upper bound for ReLU Networks
As previously mentioned, this kind of result is well-known in the literature on sparse regression and
we include a proof primarily for completeness. The intuition is simple: the estimator ZNN can make
use of the non-linearity in the soft threshold to zero out the coordinates in the estimate A>X which
are small and thus “reliably” not in the support of the true z . Thus, the estimator only makes mistakes
on the non-zero coordinates. The full proofs are in Section A.
5.2	Lower bound: proof sketch of Theorem 4.2
The proof of Theorem 4.2 has two main ideas, which we detail below:
(1)	A structural lemma, which shows that the optimal predictor has a “decoupled” structure along the
coordinates of the latent variable.
(2)	An analysis of this decoupled estimator using a bias-variance calculation in an appropriately
chosen basis.
The full proofs of this Section are in Appendix B.
5.2.1	Structure of the Optimal Estimator
As explained above, our structural lemma shows that the optimal low-degree polynomial estimator
decouples along the coordinates of the latent variable. In order to understand why this should be true,
first observe that the optimal estimator for Y = hw, Zi given X has a particularly simple structure.
Concretely, the optimal estimator is the conditional expectation E[hw, Zi|X] = Pi wiE[Zi|X], so
the optimal estimator for Y simply reconstructs Z as well as possible coordinate-wise, then takes an
inner product with w.
With this in mind, note the coordinates of Z are independent in our setting, so optimal estimation
of Zi should not depend in any way on reconstructing Zj for j 6= i. This allows us to show that the
optimal polynomial of degree d to estimate Y has no “mixed monomials” in an appropriate basis.
This is the content of the next lemma, whose proof is in Appendix B.
Lemma 5.1. Suppose X = AZ + ξ where A is an orthogonal m × m matrix, Z has independent
entries and ξ 〜N(0, σ2Id). Then there exists a unique minimizer fd over all degree dpolynomials
fd of the square-loss,
E[(fd(A>X) - hw, Zi)2]
and furthermore fd has no mixed monomials. In other words, we can write fd(A>X) =
Pi fd,i((A>X)i) where each Ofthe fd,i are univariate degree d polynomials.
5.2.2	Fourier Analysis and Bias-Variance Trade-Off
Once we have reduced to considering estimators with decoupled structure, it becomes feasible to
analyze the performance of all possible low degree polynomials using a bias-variance calculation in a
carefully chosen basis. This is the second (and more involved) step in the proof. In order to perform
the calculation, we need to apply Fourier analytic methods, so we need to switch to an orthonormal
basis. Since the noise we chose for the lower bound instance is Gaussian9, a natural choice is the
Hermite polynomials.
9The proof does not rely heavily on this choice; as long as we choose the correct orthogonal basis of
polynomials, the proof would go through with minor modifications.
6
Published as a conference paper at ICLR 2019
We review the definition of the Hermite polynomials in Appendix B, but for the purposes of this
proof overview, the Hermite polynomials are polynomials Hn(x) indexed by multi-indices n ∈ N0m
with the important property that they are orthogonal with respect to the standard m-variate Gaussian
distribution, namely
0 if n 6= n0
EX~N(0,σ2I)Hn(X㈤Hn0(X㈤={]: .^^
From this, we can derive Plancherel’s Theorem in this basis:
Theorem 5.1 (Plancherel S Theorem in Hermite Basis). Let f (x) = En f (n)H (x∕σ) ,then
EX〜N(0,σ21 )[∣f(X)|2] = X lf(n)∣2
n
We use this theorem, along with the structural Lemma 5.1 to perform a bias-variance tradeoff analysis
of any predictor: namely, we show
(1)	If the Fourier coefficients |f (n)| are large, then the estimator will be very sensitive to noise (i.e.
has too high of a variance).
(2)	On the other hand, if |f (n)| is small and f is low-degree, then the estimator cannot match the
correct mean well regardless of noise (i.e. has too high of a bias).
Efficient application of Plancherel’s theorem is key to proving both results: in the first case, we
apply it over the randomness in the noise ξ, and in the second case, we apply it over the randomness
in the latent vector Z, which has Gaussian entries conditioned on its support. Note that when f is
sufficiently high-degree, it can effectively take advantage of the difference in scales between the noise
and the signal to achieve both low bias and low variance simultaneously: see the following upper
bound section for details.
5.3 Upper Bound: Proof Sketch of Theorem 4.3
As previously mentioned, it’s a result from classical approximation theory that no low-degree
polynomial is close to the ReLU function on all of [-1, 1]. The crux of these results is that it’s hard
to approximate ReLU well at 0, its point of non-smoothness.
However, in our setting precisely approximating ReLU everywhere is not important for getting a good
regression rate: instead, the approximation needs to be very close to 0 when the input is negative, and
only very coarsely accurate otherwise. The reason for this is the intuition we described for 2-layer
ReLU networks: the property OfReLU that is useful in this setting is it's “denoising" ability - the
fact that it zeroes out negative inputs.
Consequently, we design a polynomial approximation to ReLU of degree O(log2 n) which sacrifices
accuracy near the point of non-smoothness in favor of closeness to 0 in the negative region.
More precisely, we prove the following theorem, in which the parameter τ in our theorem controls
the trade-off between the polynomial pd being close to 0 for x < 0 and being close to x for x > 0.
Theorem 5.2. Suppose R > 0, 0 < τ < 1∕2 and d ≥ 7. Then there exists a polynomial pd = pd,τ,R
of degree d such that for x ∈ [-R, 0]
|pd(x) - ReLU(x)| ≤ 14R
—Jπτd∕4
and for x ∈ [0, R],
|pd(x) - ReLU(x)| ≤ 2Rτ + 2R
r4T +12R∖ Pde-k4.
πd	τπ
The proof of this theorem proceeds in two steps:
(1)	First, one takes a “soft-max” mollification of ReLU of the form gβ (x) := 1 log(1 + eβx) with an
appropriately tuned β, so that gβ is sufficiently close to ReLU.
(2)	Second, if β is not too large, we prove that the poles (in the complex plane) of the function gβ are
7
Published as a conference paper at ICLR 2019
Dimension (n)	32	45	64	91	128
2-Layer ReLU NetWOrk	6.500282	6.969625	7.479449	8.324109	8.969790
Degree 17 Polynomial	7.258900	7.723297	8.727798	9.993010	10.256913
Table 1: Test errors of baseline ReLU network (Section 4.1) and Degree 17 polynomial kernel; error
is unnormalized. Experiments were run for n up to 4096 and the error between the two methods
continued to be similar - results are omitted for concision.
not too close to the origin. This, it turns out, governs the polynomial approximability of gβ due to a
powerful theorem of Bernstein in complex analysis. (See Theorem C.1, and it’s quantitative analogue
we prove as Theorem C.2.) Once we have this approximation to ReLU, we directly plug it into our
2-Layer ReLU network estimator from Section 4.1 to prove Theorem 4.3.
The full proofs are in Appendix C.
6	Simulations
Finally, we provide synthetic experiments to verify the predictions from Theorem 4.2 and Theorem 4.3.
The setup is as follows: We generate a large synthetic data set (with n = m and μ = 0) in the following
fashion:
•	A is a random orthogonal matrix and w is sampled from a n-dimensional standard Gaussian.
•	Z ∈ Rn is sampled by including each coordinate with probability k/n, and sampling a
standard Gaussian for each included coordinate.
•	X and Y are sampled according to the generative model in Section 2, using Gaussian noise
with standard deviation σ.
For each fixed degree, we fit a polynomial using least-squares regression, and evaluate the performance
on a corresponding test set10 generated in the same fashion (reusing the same A and w). Solving the
regression problem for large degrees is intractable using standard training methods; to overcome this
issue, we used structural observation in Lemma 5.1 to reduce the regression problem for estimating
Y from X to that of estimating Zi given Xi, which is a much lower dimensional problem. 11
The results of the experiment are in Figure 1, graphed on a log scale. All experiments were run
with k = 5 and σ = 0.06. We see that for low degrees, i.e. before our prediction error is close
to the information-theoretic limit, the log-error decays roughly linearly with respect to polynomial
degree. This matches the prediction of the lower bound in Theorem 4.2 after taking a log of the
right-hand-side.
For completeness, we also evaluate the baseline 2-Layer ReLU network described in Section 4.1 in
the same experimental setup. Table 1 shows the test error of the baseline 2-Layer ReLU network and,
for comparison, the best polynomial of degree 17 in the same experiment. Despite the high degree,
the ReLU network is still slightly better.
7	Conclusions
In this paper, we considered the problem of providing representation lower and upper bounds for
different classes of universal approximators in a natural statistical setup that exhibits sparse latent
structure. We hope this will inspire researchers to move beyond the worst-case setup when considering
the representational power of different function classes.
10Small technical remark: our upper bound (Theorem 4.3) is with-high-probability and not in expectation,
so we drop the outlier 1 percent of test results which had largest error. The reason is that we may draw a
rare tail event where the input is unusually large/non-sparse and then the polynomial predictor may make an
exponentially large error in the degree.
11Another technical remark: we only run the experiment for odd degrees, since the optimal estimator is an
odd function.
8
Published as a conference paper at ICLR 2019
Polynomial Degree
Figure 1: Degree vs Log L2 Error on test set for different values of n, the dimensionality of the
problem. This plot was generated using a training set of 8000 examples from the generative model
and a test set of 1000 additional examples; error is unnormalized.
log(n)
8
7
6
5
4
The techniques we develop are interesting in their own right: unlike standard approximation theory
setups, we need to design polynomials which may only need to be accurate in certain regions.
Conceivably, in classification setups, similar wisdom may be helpful: the approximator needs to only
be accurate near the decision boundary.
Finally, we conclude with a tantalizing open problem: In general it is possible to obtain non-trivial
sparse recovery guarantees for LASSO even when the sparsity k is nearly of the same order as n under
assumptions such as RIP. Since LASSO can be computed quickly using iterated soft thresholding
(ISTA and FISTA, see Beck and Teboulle (2009)), we see that sufficiently deep neural networks can
compute a near-optimal solution in this setting as well. It would be interesting to determine whether
shallower networks and polynomials of degree polylog(n) can achieve a similar guarantees.
References
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pages 6241-6250, 2017.
Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse
problems. SIAM journal on imaging sciences, 2(1):183-202, 2009.
Yin-Wen Chang, Cho-Jui Hsieh, Kai-Wei Chang, Michael Ringgaard, and Chih-Jen Lin. Training
and testing low-degree polynomial data mappings via linear svm. Journal of Machine Learning
Research, 11(Apr):1471-1490, 2010.
Ronald A DeVore and George G Lorentz. Constructive approximation, volume 303. Springer Science
& Business Media, 1993.
Ilias Diakonikolas, Daniel M Kane, and Jelani Nelson. Bounded independence fools degree-2
threshold functions. In Foundations of Computer Science (FOCS), 2010 51st Annual IEEE
Symposium on, pages 11-20. IEEE, 2010.
David L Donoho. Compressed sensing. IEEE Transactions on information theory, 52(4):1289-1306,
2006.
Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In Conference
on Learning Theory, pages 907-940, 2016.
Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic
gradient descent. In International Conference on Machine Learning, pages 1225-1234, 2016.
Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training neural
networks. In Advances in Neural Information Processing Systems, pages 855-863, 2014.
9
Published as a conference paper at ICLR 2019
Ankur Moitra. Algorithmic aspects of machine learning. Preprint. Cambridge University Press (to
appear), 2018.
Donald J Newman et al. Rational approximation to |x|. The Michigan Mathematical Journal, 11(1):
11-14,1964.
Tomaso Poggio, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao. Why and
when can deep-but not shallow-networks avoid the curse of dimensionality: A review. International
Journal of Automation and Computing, 14(5):503-519, 2017.
Phillippe Rigollet. High-dimensional statistics. Lecture notes (MIT), 2017.
Itay Safran and Ohad Shamir. Depth-width tradeoffs in approximating natural functions with neural
networks. In International Conference on Machine Learning, pages 2979-2987, 2017.
Shai Shalev-Shwartz, Ohad Shamir, and Karthik Sridharan. Learning kernel-based halfspaces with
the 0-1 loss. SIAM Journal on Computing, 40(6):1623-1646, 2011.
Matus Telgarsky. Benefits of depth in neural networks. In Conference on Learning Theory, pages
1517-1539, 2016.
Matus Telgarsky. Neural networks and rational functions. In International Conference on Machine
Learning, pages 3387-3393, 2017.
Roman Vershynin. High-Dimensional Probability. Cambridge University Press (to appear), 2018.
Dmitry Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks, 94:
103-114, 2017.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. ICLR, 2017.
Yuchen Zhang, Jason D Lee, and Michael I Jordan. l1-regularized neural networks are improperly
learnable in polynomial time. In International Conference on Machine Learning, pages 993-1001,
2016.
A	Upper b ound for 2-Layer ReLu Networks
We will first prove a bound on the error of the soft-thresholding estimator ZNN (Lemma A.2),
which corresponds to the hidden layer of the neural network: this is essentially a standard fact in
high-dimensional statistics (see reference text (Rigollet, 2017)). The idea is that the soft thresholding
will correctly zero-out most of the coordinates in the support while adding only a small additional
error to the coordinates outside the support.
From the recovery guarantee for ZNN, we will then deduce Theorem 4.1.
Towards proving the above result, we first need an estimate on the bias of A>x, i.e. the error without
noise:
LemmaA.1. Suppose A is μ-incoherenti.e. kA>A 一 Id∣∣∞ ≤ μ. Thenfor any Z, ∣∣A>Az 一 z∣∣∞ ≤
μkzkι.
Proof. We have
(A>Az)i = hAi,XzjAji = zihAi,Aii + XzjhAi,Aji
so applying the incoherence assumption we have ∣(A>Az)i - Zi| ≤ μ∣∣z∣∣ ι.	□
Using this we can analyze the error in thresholding.
10
Published as a conference paper at ICLR 2019
Lemma A.2. Suppose A is μ-incoherent i.e. ∣∣A>A 一 11∣∞ ≤ μ. Let Z be an arbitrary fixed vector
such that ∣∣zkι ≤ M and |Supp(z)| ≤ k. Suppose X = Az + ξ where ξ 〜N(0,σ2In×n). Then
for some T = Θ(σ,(1+ μ)log m + μM) and Z = PMn(A>x), with high probability we have
Ilz 一 z∣∣∞ ≤ 2τ and supp(Z) ⊂ supp(Z).
Proof. Observe that
A>x = z + (A>A - I)z + A>ξ.
Note that entry i of A>ξ is (Ai, ξ where ∣ Ai∣2 ≤ (1 + μ) so (Atξ)i is SUbgaUssian with variance
proxy at most σ2 (1 + μ).
By concentration and Union boUnd, with high probability all coordinates not in the trUe sUpport are
thresholded to 0. Similarly we see that for each of the coordinates in the sUpport, an error of at most
2τ is made.	□
From the above lemma, we can easily prove the main theorem of this section:
Proof of Theorem 4.1. When the high probability above event happens, we have the following Upper
boUnd by Holder’s ineqUality:
IYNN-Y|2 = hw|supp(h), (ZNN-Z)ISUPP(h)i2 ≤ k2kZNN-Zk∞ = θ(k2((1+μ)σ2 log(m)+μ2M2))
□
For the lower bounds we will be interested mostly in the case when μ = 0, i.e. A is orthogonal
and so m = n, the coordinates of Z are independent and each is nonzero with probability at most
k/n, and the noise is GaUssian. Then the error estimate we had in the previoUs theorem specializes
to O(σ2k2 log(n)), bUt Under these assUmptions we know that the information-theoretic optimal is
actUally σ2k log(n). While not very important to the flow of the paper, for completeness we can
improve the analysis to eliminate the extra factor of k, withoUt changing the algorithm:
Theorem A.1. Suppose A is orthogonal (hence m = n), the coordinates of Z are independent, and
ξ 〜N(0, σ2I). Then
E∣YNN — YI2 = O(kσ2 log(m))
Proof. In this case, We have A>X = Z + ξ0 where ξ0 〜N(0, σ2I). Therefore the coordinates of Z
are independent of each other, and so we see
EIYNN - YI2 = X W2E[(ZNN 一 Z)2] ≤ X E[(Znn - Z)2].
Let Ei denote the event that Iξ0 Ii > τ . Then
X E[(ZNN - Z)2] = X E[(IEi + IEC)(ZNN - Z)2]
ii
≤ 4kτ2 + ^X E[IEC (ZNN 一 Z)2]
i
=4kτ2 + X Pr(IEC )E[(Znn 一 Z)21IEC = 1]
ii
i
≤ 4kτ2 + XPr(IEC)E[(τ + IX0 - ZiI)2I⅛C = 1]
ii
i
≤ 4kτ2 + XPr(Iec)(2τ2 + 2E[IξiI2IIec = 1])
i
≤ 4kτ2 + X C (2τ2 + 2C0τ2)
m
i
where the first ineqUality follows as in Lemma A.2, the second ineqUality Uses that Iρτ (x) 一 xI ≤ τ,
the third Uses that (a+b)2 = a2 +2ab+b2 ≤ 2a2 +2b2 by YoUng’s ineqUality, and the last ineqUality
follows from standard tail boUnds on GaUssians. We see the last expression is O(kσ2 log(m)) so we
have proved the result.	□
11
Published as a conference paper at ICLR 2019
B Lower Bounds for Polynomial Kernels
In this section, prove the lower bounds for polynomial kernels.
We recall the lower bound instance: the noise distribution is N(0, σ2Id) and the distribution for Z is
s.t. every coordinate is first chosen to be non-zero with probability k/n, and if it is non-zero, it’s set
as an independent sample from N(0, γ2). This construction makes Z approximately k-sparse with
high probability while making its coordinates independent. We choose A as an arbitrary orthogonal
matrix, so m = n. We choose w to be an arbitrary ±1 sign vector, so wi2 = 1 for every i.
As a warmup, we first show that linear predictors, and subsequently fixed low degree polynomials
cannot achieve the information-theoretic rate 12 of O(σ2k logn) - in fact, We will show that they
achieve a “trivial” rate. Furthermore, we will show that even if the degree of our polynomials is
growing with n, if d = o(log n/ log log n) the state of affairs is similar.
B.1 Warmup: Linear Predictors
As a warmup, and to illustrate the main ideas of the proof techniques, we first consider the case of
linear predictors. (i.e. kernels of degree 1.)
The main idea is to use a bias-variance trade-off: namely, we show that the linear predictor we use,
say f (x) = (W, Xi either has to have too high of a variance (when ∣∣Wk is large), or otherwise has too
high of a bias. (Recall, the bias captures how well the predictor captures the expectation.)
We prove:
Theorem B.1. For any W ∈ Rn,
2
E[((W,Xi- Y)2] ≥ γ2k	2
γ2 (k/n) + σ2
Before giving the proof, let us see how the theorem should be interpreted.
The trivial estimator which always returns 0 makes error of order γ2K and a good estimator (such
as thresholding) should instead make error of order σ2Klogn when γ >> σ√log n. The next
theorem shows that as long as the signal to noise ratio is not too high, more specifically as long as
Y2(k∕n) = o(σ2), any linear estimator must make square loss of Ω(γ2k), i.e. not significantly better
than the trivial 0 estimate.
Note that the most interesting (and difficult) regime is when the signal is not too much larger than the
noise, e.g. γ2 = σ2polylog(n) in which case it is definitely true that γ2 (k/n) << σ2.
Proof. Note that
(W, Xi- y = (W, Az + ξi — hw, Zi = (A>W — w, z) + (W, ξ
which gives the following bias-variance decomposition for the square loss:
E[((W, x) — y)2] = E[((A>W — w,z) + (W, ξ))2]
=E[(A>W — W, z)2 + (W, ξ)2]
=kY2kA>W - Wk2 + σ2kt^k2
n
k
=—Y2kW — AWk2+ σ2kWk2
n
where in the second-to-last step we used that the covariance matrix of Z is γ2 (k/n)I, and in the last
step we used that A is orthogonal. Now observe that if we fix R = kW∣∣2, then by the Pythagorean
theorem the minimizer of the square loss is given by the projection of AW onto the R-dilated unit
sphere, so W=，R2/m(AW)since IlAWk2 = l∣W∣∣2 = √m. In this case the square loss is then of
the form
k C ------ C C CkC	_ C CC
—γ2 k pR2/m(AW)— AWk2 + σ2kW∣2 = —γ2(R — √m)2 + σ2R2
n	2	2n
12See Remark B.1 for why this is the optimal information-theoretic rate.
12
Published as a conference paper at ICLR 2019
and the risk is minimized when
k n , _	,—.	C —
0 = 2—γ2(R — √m) + 2σ2R
n
i.e. when
R = 2 γ2(k∕n) 2 √m
γ2 (k/n) + σ2
so the minimum square loss is
__	C	CC	2	2k k
(√m — R)σ2R + σ2R2 = σ2	Y ：	2
γ2 (k/n) + σ2
since m = n.
□
B.2	Structure of the Optimal Estimator: Proof of Lemma 5.1
ProofofLemma 5.1. Let X0 = A>X, so by orthogonality X0 = Z + ξ where ξ 〜N(0, σ2Id).
Observe that if we look at the optimum over all functions f, we see that
minE[(f(X0)—hw,Zi)2]=E[(E[hw,Zi|X0]—hw,Zi)2]
=E[(XwiE[Zi|X0] — hw, Zi)2]
i
=E[(XwiE[Zi|Xi0] — hw, Zi)2].
i
where where in the first step we used that the conditional expectation minimizes the squared loss, in
the second step we used linearity of conditional expectation, and in the last step we used that Zi is
independent of X6=0 i .
By the Pythagorean theorem, the optimal degree d polynomial fd is just the projection of
Pi wiE[Zi|Xi0] onto the space of degree d polynomials. On the other hand observe that
E[(XwiE[Zi|Xi0] —hw,Zi)2] =Xwi2E[(E[Zi|Xi0] —Zi)2]
ii
so the optimal projection fd is just Pi Wifd(X0) where f3 is just the projection of each of the
E[Zi |X]. Therefore fd has no mixed monomials.	□
Remark B.1. The previous calculation shows additionally that the problem of minimizing the squared
loss for predicting Y is equivalent to that of minimizing the squared loss for the sparse regression
problem of recovering Z. It is a well-known fact that the information theoretic rate for sparse
regression (with our normalization convention) is Θ(σ2k) (see for example (Rigollet, 2017)), and so
the information-theoretic rate for predicting Y is the same, and is matched by Theorem A.1.
B.3	Bias-Variance, Fourier Analysis and Proof of Theorem 4.2
We recall that the lower bound for polynomials combines the observation of Lemma 5.1 with a
bias-variance tradeoff calculation using Fourier analysis on orthogonal polynomials. Concretely,
since the noise we chose for the lower bound instance is Gaussian, the most convenient basis will be
the Hermite polynomals.
We recall the probabilist’s Hermite polynomial Hen(x), defined by the recurrence relation
Hen+1 (x) = xHen(x) — nHen-1(x).	(1)
where He0 (x) = 1, He1 (x) = x. In terms of this, the normalized Hermite polynomial Hn(x) is
Hn(X) = H——i-Hen (X).
n!
13
Published as a conference paper at ICLR 2019
Let Hn(x) for a vector of indices n ∈ N0m denote the multivariate polynomial Πim=1Hni (xi). It’s
easy to see the polynomials Hn(x) form an orthogonal basis with respect to the standard m-variate
Gaussian distribution. As a consequence, we get
EX〜N(0,σ2I)Hn(X∕σ)Hnθ(X∕σ) = [0, ifn = n0
(,	[1, otherwise
which gives us Plancherel’s theorem:
Theorem B.2 (Plancherel in Hermite basis). Let f(x) =	n f (n)Hn(x∕σ), then
EX〜N(0,σ21 )[∣f(X)|2] = X lf(n)∣2
n
We can use Plancherel’s theorem to get lower bounds on the noise sensitivity of degree d polynomials.
This will be an analogue of the variance.
Lemma B.1. [Variance analogue in Hermite basis] Let f(x) =	n f(n)Hn(x∕σ) and let f6=0 :=
f - fb(0). Then
E[(f(A>X)-Y)2] ≥ (1-k∕n)kfb6=0k22
Proof. First we suppose Z (and thus Y ) is fixed and consider the randomness of the noise. Let S
denote the support of Z. Recall that A>x = Z + ξ where ξ 〜N(0, σ2In×n). Define fz(ξ):=
f(Z + ξ) - Y , then by Plancherel
Eξ[(f(A>x)-Y)2] =Eξ0[fZ(ξ0)2] =X|fcZ(n)|2
n
Furthermore
X|fcZ(n)|2 ≥ X	|fb(n)|2
n	msupp(n)⊂S
because (ξ0 + Z )∣sc = ξ0∣sc so by expanding out fz in terms of the fourier expansion of f, we see
fz (n) = f(n) for n such that supp(n) 6⊂ S. Finally the probability n ⊂ S for n 6= 0 is upper
bounded by the probability a single element of its support is in S, which is k∕n. Therefore
Ez,ξ[(f (A>x) - Y)2] ≥ X ∣f(n)∣2E[l‘^心]≥ X(1 - k∕n)∣f(n)∣2
which proves the result.	□
Next we give a lower bound for the bias, showing that if kfb6=0k22 is small for a low-degree polynomial,
it cannot accurately predict y. Here we will assume f is of the form given by Lemma 5.1.
Lemma B.2 (Low variance implies high bias). Suppose f is a multivariate polynomial of degree d
with no mixed monomials, i.e. f(x) = i fi(xi) where fi is a univariate polynomial of degree d.
Expand f in terms of Hermite polynomials as f(x) =	n f(n)Hn(x∕σ). Then
n
E[(f(A>X) - Y)2] ≥ (k∕n) Xwi2 max(0, γ -
i=1
n
X lf(kei)∣2(d +1)3d+2(1 + (γ∕σ)d))2
i=1
∖
Before proving the lemma, let us see how it proves the main theorem:
Proof of Theorem 4.2. By Lemma 5.1, Lemma B.1, and Lemma B.2 we have that for the f which
minimizes the square loss among degree d polynomials, we have a variance-type lower bound
nd
E[(f(A>X) - Y )2] ≥ (1 - k∕n) XX ∣f(kei)∣2
i=1 k=1
14
Published as a conference paper at ICLR 2019
and (using that wi2 = 1 to simplify) a bias-type lower bound
n
E[(f (A>X) - Y)2] ≥ (k/n) X max(0, γ -
i=1
n
X lf(kei)∣2(d +1)3d+2(1 + (γ∕σ)d))2.
i=1
∖
Let
gives
kfik2 ：= √∑n=ι lf(kei)∣2
. Then averaging these lower bounds and simplifying using k < n∕2
n
E[(f (A>X) - Y)2] ≥ (1/4) Xmax(kfik2, p∕n(γ -kb∣∣2(d +1)3d+2(1 + (γ∕σ)d)))2
i=1
n
≥ (1/4) X
i=1
γ2 (k/n)
(1 + pk∕n(d +1)3d+2(1 + (γ∕σ)d))2
≥ (1/4)
γ2k
(1 + √k∕n(d +1)3d+2(1 + (γ∕σ)d))2
□
Returning to the proof of Lemma B.2, we have:
Proof of Lemma B.2. Since f has no mixed monomials, we get for the Hermite expansion that
f (n) = 0 unless ∣supp(n)∣ ≤ 1. Let X0 := A>X = Z + ξ where ξ 〜N(0, σ2I). Next observe
by independence that
E[(f (X0) - Y)2] = X w2E[(fi(X0) - Zi)2] ≥ (k∕n) X w2E[(fi(X0) - Zi)2Z = 0]
ii
where the last inequality follows since there is a k∕n chance that Zi 〜 N(0, σ2I), equivalently that
Zi 6= 0. By the conditional Jensen’s inequality we have
(k∕n) Xw2E[(fi(X0) - Zi)2∣Zi = 0] ≥ (k∕n) X w?E[(E[fi(X0)∣Zi] - Zi)2∣Zi = 0].
ii
Observe that fi (Xi0) = Pkd=0 fb(kei)Hk(Zi∕σ + ξ∕σ) andletgi(Zi) := E[fi(Xi0)|Zi] - Zi, so then
gi is a polynomial of degree d in Zi . Write the Hermite polynomial expansion of gi in terms of
Hk (Zi∕γ) as
d
gi(x) = Xgbi(k)Hk(Zi∕γ),
k=0
then by Plancherel’s formula
d
(k∕n) X wi2E[(E[g(Zi) - Zi)2|Zi 6= 0] = (k∕n) X wi2 X |gbi(k)|2 ≥ (k∕n) X wi2|gbi(1)|2
i	i	k=0	i
and it remains to lower bound |gbi(1)|. By orthogonality and direct computation,
gi(1) = EZi〜N(0,γ)[(E[fiX)∣Zi] - Zi)Hι(Zi∕γ)] = -Y + EZi〜N(o,γ)[E[fiX)∣Zi](Zi∕γ)]∙
Now we upper bound the last term
d
EZi 〜N (0,γ)[E[fi(X0)∣Zi](Zi∕γ)] = f(0)E[Zi∕γ ] + Ef(kei)Ezi 〜N (o,γ)[E[Hk (Zi∕σ + ξ0∕σ)∣Zi](Zi∕γ)]
k=1
d
X
f(kei)EZi〜N(0,γ)[Hk(Zi∕σ + ξ ㈤出血)]
k=1
≤ Xd |fb(kei)|2
k=1
1/2
EZi〜N(0,γ) [Hk IZiQ + ξ%)(Z"γ)]2)
15
Published as a conference paper at ICLR 2019
where the second equality is by the law of total expectation and the last inequality is Cauchy-Schwarz.
Using the recurrence relation (1), We can bound the sum of the absolute value of the coefficients of
Hk (x) by kk/√k! ≤ kk. We can also bound the moments of the absolute value of a Gaussian by
Eξ〜N(o,i)[∣ξ∣k] ≤ kk. Therefore by Holder,s inequality
EZi 〜N (0,γ)[Hk(Zi∕σ + ξ%)(Zi∕γ)] ≤ kk Sup ∣Ez,〜N(0,γ)[(Zi∕σ + ξ0∕σ)'(Zi∕γ)]∣
'=1
≤ kk SupEZi〜N(0,γ)[(∣Zi∣∕σ + ∣ξ0∣∕σ)'(∣Zi∣∕γ)]
'=1
≤ 2kkk Sup(EZi〜N(0,γ)[∣Zi∣'+1∕σ'γ] + EZi〜N(0,γ)[∣ξ0l'∣Zi∣∕σ'γ)])
'=1
≤ 2kkk[max(1, (γ∕σ)k)(k + 1)k+1 + kk]
≤ (k + 1)3k+1(1 + (γ∕σ)k).
Therefore by reverse triangle inequality
|gi(1)|2 ≥ max(0, γ -
∖
n
X l∕(kei)∣2(d +1)3d+2(1 + (γ∕σ)d))2.
i=1
□
Remark B.2. Remarks on results:
We make a feW remarks regarding the results in this section. Recall that γ2k is the square loss
of the trivial zero-estimator. Suppose as before that γ = Θ(σ2polylog(n)), then We see that
if d = o(log n∕ log log n) then the denominator of the loWer bound tends to 1, hence any such
polynomial estimator has a rate no better than that of the trivial zero-estimate.
It is possible to derive a similar statement to Theorem 4.2 that holds With high probability instead
of in expectation for polynomials of degree o(log n∕ log log n). All that is needed is to bound the
contribution to the expectation from very rare tail events When the realization of the noise ξ is
atypically large. Since the polynomials We consider are very loW degree o(log n∕ log log n), they can
only groW at a rate of xd = xo(log(n)/ log log n); thus standard groWth rate estimates (e.g. the Remez
inequality) combined With the Gaussian tails of the noise can be used to shoW that a polynomial
Which behaves reasonably in the high-probability region (e.g. Which has small W.h.p. error) cannot
contribute a large amount to the expectation in the tail region.
C Upper Bounds for Polynomial Kernels
In this section, We construct polynomials achieving close to the information-theoretic optimal rate of
degree only O(log2 m). Recall this is nearly optimal due to our previous lower bound of Ω(log n).
As previously mentioned, the key technical result here Will be Theorem 5.2, giving the construction
of a new polynomial approximation to ReLU. Before proceeding to the proof of that theorem, we
show how it implies the final result, Theorem 4.3.
Towards that, we substitute our polynomial construction for ρτ into our ReLU neural network and
derive the analogous version of Lemma A.2. First, define Mτ = M + 2τ and let
Pd,τ,M = Pd,τ∕Mτ ,Mτ (x - T) + Pd,τ∕Mτ,Mτ(-X + T)
where p is the polynomial constructed in Theorem 5.2. We then have:
Lemma C.1. Suppose , T > 0 and M ≥ L Then for all d ≥ do = Ω(M log2(MT)), for
|x| ∈ (T, Mτ ) we have
∣pd,τ,M(x) - x| ≤ 3τ + C
and for |x| ≤ T we have
∣Pd,τ,M (x)| ≤ C
16
Published as a conference paper at ICLR 2019
Proof. By the guarantee of Theorem 5.2, we see that for for |x| ≤ τ that
∣Pd,τ,M(x)| ≤ 28Mτ rdMe-√πτd∕4Mτ.
Thus We see that taking d = Ω(M log2(MT)) suffices to make the latter expression at most e.
Similarly for |x| > τ we know that
ιpd,τ,M (x)| ≤ 2τ+2Mτ r Mnd+26Mτ r ^T e-√πτd/4MT
and taking d = Ω(M log2(MT)) with sufficiently large constant guarantees the middle term is at
most T and the last term is at most e.	□
Using this, we can show that if we use a polynomial of degree Ω((M∕σ√log n) log2 m) we can
achieve similar statistical performance to the ReLu network:
Lemma C.2. Suppose A is μ-incoherent i.e. ∣∣A>A 一 Id∣∣∞ ≤ μ. Let Z be an arbitrary fixed vector
such that ∣∣z∣ι ≤ M and |supp(z)| ≤ k. Suppose X = Az + ξ where ξ 〜 N(0, σ2Idn×n). Then
for some T = Θ(σ y∕(1+ μ) log m + μM), for any d ≥ do = Ω( M log2(Mτ m∕τ2)), if we take
Z := Pf)n m (A>x), then with high probability we have ∣∣Z — z∣ι ≤ 6kτ.
Proof. Apply Lemma C.1 with = T∕m. Then we see for |x| ∈ (T, Mτ) we havn
∣Pd,τ,M(x) — x| ≤ (3 + 1∕m)τ ≤ 4τ
and for |x| ≤ T we have
∣Pd,τ,M(x)| ≤ τ∕m
Observe that
A> x = Z + (A> A — Id)Z + A> ξ.
Note that entry i of A>ξ ishA%, ξ where IlAik2 ≤ (1 + μ) so (Atξ)i is Gaussian with variance at
most σ2(1 + μ).
By choosing τ with sufficiently large constant, then applying the sub-Gaussian tail bound and union
bound, with high probability all coordinates not in the true support are thresholded to at most τ∕m.
Similarly we see that for each of the coordinates in the support, an error of at most 5τ is made.
Therefore ∣∣Z — z∣ι ≤ 5kτ + m(τ∕m) ≤ 6kτ.	□
Now we have all the ingredients to prove Theorem 4.3:
ProofofTheorem 4.3. Define an estimate for Y by taking ZdM := PfnM(A>X) where T is
defined as in the Lemma, and then taking Yd,M := hw, Zd,M i. Applying the previous Lemma, we
get analogous versions of Theorem 4.1 by the same argument as in that theorem.	□
Finally, we return to the proof of the key Theorem 5.2:
Proof of Theorem 5.2. We start with the case where R = 1∕2. We build the approximation in two
steps. First we approximate ReLu by the following “annealed” version of ReLu, for parameters
β > π, τ > 0 to be optimized later:
gβ(X) = 1 log(1 + eβx)
β
fβ,τ (X) = gβ(X — τ).
Observe that when we look at negative inputs, g§(—x) = ɪ log(1 + e-βx) ≤ ɪe-βx. Therefore
when X < 0, fβ (X) ≤ 1e-βτ.
For the second step,, we need to show fβ can be well-approximated by low-degree polynomials.
In fact, because fβ is analytic in a neighborhood of the origin, it turns out that its optimal rate of
17
Published as a conference paper at ICLR 2019
approximation is determined exactly by its complex-analytic properties. More precisely, define Dρ to
be the region bounded by the ellipse in C = R2 centered at the origin with equation
22
xy
+	= 1
a2 十 b2	1
With semi-axes a = 1 (P + PT) and b = 2 ∣ρ - ρ-1∣; the focii of the ellipse are ±1. For an arbitrary
function f : [-1, 1] → R, let Ed(f) denote the error of the best polynomial approximation of degree
d in infinity norm on the interval [-1, 1] of f. Then the folloWing theorem of Bernstein exactly
characterizes the groWth rate of Ed(f):
Theorem C.1 (Theorem 7.8.1, (DeVore and Lorentz, 1993)). Let f be a function defined on [-1, 1].
Let P0 be the supremum of all P such that f has an analytic extension on Dρ. Then
limsup PEd(Ty = ɪ
d→∞	P0
For our application We need only the upper bound and We need a quantitative estimate for finite n.
FolloWing the proof of the upper bound in (DeVore and Lorentz, 1993), We get the folloWing result:
Theorem C.2. Suppose f is analytic on the interior of Dρ1
Then
and |f(z)| ≤ M on the closure of Dρ1.
Ed(f) ≤
2M -n
F P1
The proof is fairly simple: by Writing f in terms of cos(x) one gets an expansion into Chebyshev
polynomials and it suffices to bound the coefficients of the corresponding Fourier series: to do this,
We Write them as integrals over the unit circle, and use the analyticity assumption on Dρ1 to contour
shift the integral to a different circle, Which immediately gives us the desired exponential decay. For
details see (DeVore and Lorentz, 1993).
We Will noW apply this theorem to gβ . First, We claim that gβ is analytic on Dρ1 Where P1 is the
solution to this equation for the semi-axis of the ellipse:
1	-1	π
2(P - ρ ) = 2β
which is	________
P4β2 十 ∏2 十 ∏	IC
Pi =-----而-------> 1 + π∕2β.
2β
To see this, first extend log to the complex plane by taking a branch cut at (-∞, 0]. To prove gβ
is analytic on Dρ1, we just need to prove that 1 十 eβz avoids (-∞, 0] for z ∈ Dρ1. This follows
because by the definition of ρι,for every Z ∈ Dp、, =(Z) < 言 hence <(1+ eβz) ≥ 1. We also see
that for z ∈ Dρ1 ,
∣gβ(z)| = 11 log(1 十 eβz)| ≤ 1 SUp | log(1 十 ew)| ≤ 1(log(1 + eβ) + ∏) < 6.
β	β w∈Dβρ1	β
Therefore by Theorem C.2 we have
Ed(gβ) ≤ — (1+ ∏∕2β)-n ≤ —e-nn/4e
ππ
where in the last step we used that 1 + X ≥ exp(χ∕2) for x < 1/2 and that β > ∏. Let gβ,d denote
the best polynomial approximation to gβ of degree d and let fβ,τ,d = gβ,d(x - T)
Thus for x ∈ [-1 十 τ, 0],
IReLu(X) - fβ,τ,d(χ)∖ ≤ lfβ,τ (x)| 十 lgβ,d(x - T) - gβ,τ(x - T )| ≤ 万 e-βτ +-Pe-π"4β
βπ
Take β = ,πd∕4τ and require d > 7 so that β > 1, then for X ∈ [-1 + τ, 0],
∖ReLu(x) - fβ,τ,d(x)∖ ≤ 7∖He-√πτd/4
Tπ
18
Published as a conference paper at ICLR 2019
For x ∈ (0, 1 - τ] we have by the 1-Lipschitz property of gβ and calculus that
|x - fβ,τ(x)∣ ≤ T + |x - gβ(x)| ≤
log2
τ+ɪ
so
IReLu(X) - fβ,τ,d(x)1 ≤ |x - fβ,τ (X)I + lgβ,d(x - T) - gβ,τ(x - T )| ≤ T +--*-----1-Π-e-πd/4β.
Plugging in our value of βand using log 2 ≤ 1 gives
IReLu(X)- fe，T，d(X)I ≤τ+vid + 6v Tπe-√πτd/4
Now the result for general R follows by taking Pd (x) = 2Rfβ τ d(x∕2R), since 2R ∙ ReLu(X/2R)=
ReLu(X) and [-l∕2,1/2] ⊂ [-1 + τ, 1 - T].	一	□
19