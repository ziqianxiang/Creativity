Published as a conference paper at ICLR 2019
Data-Dependent Coresets for Compressing
Neural Networks with Applications to Gener-
alization Bounds
Cenk Bayka仲，Lucas LiebenWeint* *,Igor GilitsChenski*, Dan Feldman[ Daniela Rust
Ab stract
We present an efficient coresets-based neural network compression algorithm that
sparsifies the parameters of a trained fully-connected neural network in a manner
that provably approximates the network’s output. Our approach is based on an im-
portance sampling scheme that judiciously defines a sampling distribution over the
neural network parameters, and as a result, retains parameters of high importance
while discarding redundant ones. We leverage a novel, empirical notion of sensi-
tivity and extend traditional coreset constructions to the application of compressing
parameters. Our theoretical analysis establishes guarantees on the size and accu-
racy of the resulting compressed network and gives rise to generalization bounds
that may provide new insights into the generalization properties of neural networks.
We demonstrate the practical effectiveness of our algorithm on a variety of neural
network configurations and real-world data sets.
1	Introduction
Within the past decade, large-scale neural networks have demonstrated unprecedented empirical suc-
cess in high-impact applications such as object classification, speech recognition, computer vision,
and natural language processing. However, with the ever-increasing size of state-of-the-art neural net-
works, the resulting storage requirements and performance of these models are becoming increasingly
prohibitive in terms of both time and space. Recently proposed architectures for neural networks, such
as those in Krizhevsky et al. (2012); Long et al. (2015); Badrinarayanan et al. (2015), contain millions
of parameters, rendering them prohibitive to deploy on platforms that are resource-constrained, e.g.,
embedded devices, mobile phones, or small scale robotic platforms.
In this work, we consider the problem of sparsifying the parameters of a trained fully-connected neural
network in a principled way so that the output of the compressed neural network is approximately
preserved. We introduce a neural network compression approach based on identifying and removing
weighted edges with low relative importance via coresets, small weighted subsets of the original set
that approximate the pertinent cost function. Our compression algorithm hinges on extensions of
the traditional sensitivity-based coresets framework (Langberg & Schulman, 2010; Braverman et al.,
2016), and to the best of our knowledge, is the first to apply coresets to parameter downsizing. In
this regard, our work aims to simultaneously introduce a practical algorithm for compressing neural
network parameters with provable guarantees and close the research gap in prior coresets work, which
has predominantly focused on compressing input data points.
In particular, this paper contributes the following:
1.	A coreset approach to compressing problem-specific parameters based on a novel, empirical
notion of sensitivity that extends state-of-the-art coreset constructions.
2.	An efficient neural network compression algorithm, CoreNet, based on our extended coreset
approach that sparsifies the parameters via importance sampling of weighted edges.
3.	Extensions of the CoreNet method, CoreNet+ and CoreNet++, that improve upon the edge
sampling approach by additionally performing neuron pruning and amplification.
,Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology,
emails: {baykal, lucasl, igilitschenski, rus}@mit.edu
^Robotics and Big Data Laboratory, University of Haifa, email: dannyf.post@gmail.com
*These authors contributed equally to this work
1
Published as a conference paper at ICLR 2019
4.	Analytical results establishing guarantees on the approximation accuracy, size, and general-
ization of the compressed neural network.
5.	Evaluations on real-world data sets that demonstrate the practical effectiveness of our algo-
rithm in compressing neural network parameters and validate our theoretical results.
2	Related Work
Our work builds upon the following prior work in coresets and compression approaches.
Coresets Coreset constructions were originally introduced in the context of computational geome-
try (Agarwal et al., 2005) and subsequently generalized for applications to other problems via an
importance sampling-based, sensitivity framework (Langberg & Schulman, 2010; Braverman et al.,
2016). Coresets have been used successfully to accelerate various machine learning algorithms such
as k-means clustering (Feldman & Langberg, 2011; Braverman et al., 2016), graphical model train-
ing (Molina et al., 2018), and logistic regression (Huggins et al., 2016) (see the surveys of Bachem
et al. (2017) and Munteanu & Schwiegelshohn (2018) for a complete list). In contrast to prior work,
We generate coresets for reducing the number of parameters - rather than data points - via a novel
construction scheme based on an efficiently-computable notion of sensitivity.
Low-rank Approximations and Weight-sharing Denil et al. (2013) Were among the first to em-
pirically demonstrate the existence of significant parameter redundancy in deep neural netWorks. A
predominant class of compression approaches consists of using loW-rank matrix decompositions, such
as Singular Value Decomposition (SVD) (Denton et al., 2014), to approximate the Weight matrices
With their loW-rank counterparts. Similar Works entail the use of loW-rank tensor decomposition ap-
proaches applicable both during and after training (Jaderberg et al., 2014; Kim et al., 2015; Tai et al.,
2015; Ioannou et al., 2015; Alvarez & Salzmann, 2017; Yu et al., 2017). Another class of approaches
uses feature hashing and Weight sharing (Weinberger et al., 2009; Shi et al., 2009; Chen et al., 2015b;a;
Ullrich et al., 2017). Building upon the idea of Weight-sharing, quantization (Gong et al., 2014; Wu
et al., 2016; Zhou et al., 2017) or regular structure of Weight matrices Was used to reduce the effective
number of parameters (Zhao et al., 2017; SindhWani et al., 2015; Cheng et al., 2015; Choromanska
et al., 2016; Wen et al., 2016). Despite their practical effectiveness in compressing neural netWorks,
these Works generally lack performance guarantees on the quality of their approximations and/or the
size of the resulting compressed netWork.
Weight Pruning Similar to our proposed method, Weight pruning (LeCun et al., 1990) hinges on the
idea that only a feW dominant Weights Within a layer are required to approximately preserve the output.
Approaches of this flavor have been investigated by Lebedev & Lempitsky (2016); Dong et al. (2017),
e.g., by embedding sparsity as a constraint (Iandola et al., 2016; Aghasi et al., 2017; Lin et al., 2017).
Another related approach is that of Han et al. (2015), Which considers a combination of Weight pruning
and Weight sharing methods. Nevertheless, prior Work in Weight pruning lacks rigorous theoretical
analysis of the effect that the discarded Weights can have on the compressed netWork. To the best of
our knoWledge, our Work is the first to introduce a practical, sampling-based Weight pruning algorithm
With provable guarantees.
Generalization The generalization properties of neural netWorks have been extensively investigated
in various contexts (Dziugaite & Roy, 2017; Neyshabur et al., 2017a; Bartlett et al., 2017). HoWever,
as Was pointed out by Neyshabur et al. (2017b), current approaches to obtaining non-vacuous gen-
eralization bounds do not fully or accurately capture the empirical success of state-of-the-art neural
netWork architectures. Recently, Arora et al. (2018) and Zhou et al. (2018) highlighted the close con-
nection betWeen compressibility and generalization of neural netWorks. Arora et al. (2018) presented
a compression method based on the Johnson-Lindenstrauss (JL) Lemma (Johnson & Lindenstrauss,
1984) and proved generalization bounds based on succinct reparameterizations of the original neural
netWork. Building upon the Work of Arora et al. (2018), We extend our theoretical compression re-
sults to establish novel generalization bounds for fully-connected neural netWorks. Unlike the method
of Arora et al. (2018), Which exhibits guarantees of the compressed netWork’s performance only on
the set of training points, our method’s guarantees hold (probabilistically) for any random point draWn
from the distribution. In addition, We establish that our method can ε-approximate the neural netWork
output neuron-Wise, Which is stronger than the norm-based guarantee of Arora et al. (2018).
In contrast to prior Work, this paper addresses the problem of compressing a fully-connected neu-
ral netWork While provably preserving the netWork’s output. Unlike previous theoretically-grounded
compression approaches - Which provide guarantees in terms of the normed difference -, our method
provides the stronger entry-Wise approximation guarantee, even for points outside of the available data
2
Published as a conference paper at ICLR 2019
set. As our empirical results show, ensuring that the output of the compressed network entry-wise
approximates that of the original network is critical to retaining high classification accuracy. Over-
all, our compression approach remedies the shortcomings of prior approaches in that it (i) exhibits
favorable theoretical properties, (ii) is computationally efficient, e.g., does not require retraining of the
neural network, (iii) is easy to implement, and (iv) can be used in conjunction with other compression
approaches - such as quantization or Huffman coding - to obtain further improved compression rates.
3	Problem Definition
3.1	Fully-Connected Neural Networks
A feedforward fully-connected neural network with L ∈ N+ layers and parameters θ defines a mapping
fθ : X → Y fora given input X ∈ X ⊆ Rd to an output y ∈ Y ⊆ Rk as follows. Let η' ∈ N+ denote
the number of neurons in layer ` ∈ [L], where [L] = {1, . . . , L} denotes the index set, and where
η1 = d and ηL = k. Further, let η = PL=2 η` and η* = maxg∈{2,…,l} η'. For layers ' ∈ {2,...,L},
let W' ∈ Rη'×η' 1 be the weight matrix for layer ' with entries denoted by Wj, rows denoted by
w' ∈ R1×η' 1, and θ = (W2,..., Wl). For notational simplicity, We assume that the bias is
embedded in the weight matrix. Then for an input vector X ∈ Rd, let a1 = X and z' = W'a'-1 ∈ Rn ,
∀' ∈ {2,...,L}, where a`-1 = φ(z'-1) ∈ Rn denotes the activation. We consider the activation
function to be the Rectified Linear Unit (ReLU) function, i.e., φ(∙) = max{∙, 0} (entry-wise, if the
input is a vector). The output of the network for an input X is fθ (X) = zL, and in particular, for
classification tasks the prediction is argmaxi∈[k] fθ (X)i = argmaxi∈[k] ziL.
3.2	Neural Network Coreset Problem
Consider the setting where a neural network fθ(∙) has been trained on a training set of independent
and identically distributed (i.i.d.) samples from a joint distribution on X × Y, yielding parameters
θ = (W2, . . . , WL). We further denote the input points ofa validation data set as P = {Xi}in=1 ⊆X
and the marginal distribution over the input space X as D. We define the size of the parameter tuple θ,
nnz(θ), to be the sum of the number of non-zero entries in the weight matrices W2 , . . . , WL.
For any given ε, δ ∈ (0, 1), our overarching goal is to generate a reparameterization θ, yielding the
neural network fθ(∙), using a randomized algorithm, such that nnz(θ)《 nnz(θ), and the neural
network output fθ(x), X ~ D can be approximated up to 1 ± ε multiplicative error with probability
greater than 1 - δ. We define the 1 ± ε multiplicative error between two k-dimensional vectors
a, b ∈ Rk as the following entry-wise bound: a ∈ (1 ± ε)b ⇔ ai ∈ (1 ± ε)bi ∀i ∈ [k], and formalize
the definition of an (ε, δ)-coreset as follows.
Definition 1 ((ε, δ)-coreset). Given user-specified ε, δ ∈ (0, 1), a set of parameters θ =
(W 2,..., W L) is an (ε, δ)-coreset for the network parameterized by θ if for X ~ D, it holds that
P (fθ(X) ∈ (1 土 ε)fθ(x)) ≥ 1 - δ,
θ,x
7 IΓħ 1 ,	7 7 ∙T .	∙ . 1	. .	1 1 ,	∙ ,	1,1	. . A
where P© X denotes a probability measure With respect to a random data point X and the output θ
generated by a randomized compression scheme.
4 Method
In this section, we introduce our neural network compression algorithm as depicted in Alg. 1. Our
method is based on an important sampling-scheme that extends traditional sensitivity-based coreset
constructions to the application of compressing parameters.
4.1 CoreNet
i.i.d.
Our method (Alg. 1) hinges on the insight that a validation set of data points P ~ Dn can be used
to approximate the relative importance, i.e., sensitivity, of each weighted edge with respect to the input
3
Published as a conference paper at ICLR 2019
data distribution D. For this purpose, we first pick a subsample of the data points S ⊆ P of appropriate
size (see Sec. 5 for details) and cache each neuron’s activation and compute a neuron-specific constant
to be used to determine the required edge sampling complexity (Lines 2-6).
Algorithm 1 C ORENET
Input: ε, δ ∈ (0, 1): error and failure probability, respectively; P ⊆ X: a set of n points from the input
space X SuCh that P i'id. Dn; θ = (W2,..., WL): parameters of the original uncompressed neural network.
Output: θ = (W2,..., WL): SParSified parameter set such that f,(∙) ∈ (1 土 ε)fθ(∙) (see Sec. 5 for details).
1:	ε0 — 2(L-i); η . maχ'∈{2,…,L-i}η'; η — PL=2η'; λ — log(ηη*)/2;
2:	S — Uniform sample (without replacement) of「log (8 η η*∕δ) log(η η*)] points from P;
3:	a* 1(x) — X ∀x ∈ S;
4:	for x ∈ S do
5:	for ` ∈ {2, . . . , L} do
6:	a'(x) — Φ(W'a'-1(x)); ∆'(x) — pk∈[η'-1] |w'k £1(：;
∣∑k∈[η'-i] wik ak (X)I
7:	for ` ∈ {2, . . . , L} do
8:	∆' ― (ɪ maxi∈[η'] px∈s ∆'(x)) + κ, where K = √2λ?(1 + √2λ∑log (8ηη*∕δ));
9:	W' — (~,...,~) ∈ Rη'×η'-1; ∆ '→ — QL=' ∆k; ε' — ∆→;
10:	for all i ∈ [η'] do
11:	W+ — {j ∈ [η'-1]: w'j > 0}; W- — {j ∈ [η'-1] ： wj < 0};
12:	w'+ — SPARSIFY(W+,w',ε' ,δ, S ,a'-1); W'- — SPARSIFy(W-,-w',ε',δ, S ,a'-1);
13:	Wi — W'+ — W'-;	Wi∙ — W';	. Consolidate the weights into the ith row of W';
14:	return θ= (W2,...,WL);
Algorithm 2 SPARSIFY(W, w,ε,δ, S,a(∙))
Input: W ⊆ [η'-1]: index set; w ∈ R1×η' 1: row vector corresponding to the weights incoming to node
i ∈ [η'] in layer ` ∈ {2, . . . , L}; ε, δ ∈ (0, 1): error and failure probability, respectively; S ⊆ P : subsample
of the original point set; a(∙): cached activations of previous layer for all X ∈ S.
Output: W: sparse weight vector.
1:
2:
for j ∈ W do
sj — maxx∈S
wj aj (x)
Pk∈W Wk ak (X);
. Compute the sensitivity of each edge
3:	S—	j∈Wsj;
4:	for j ∈ W do	. Generate the importance sampling distribution over the incoming edges
5:	qj — Sj;
6:	m — l8 S log]??10?，8 η∕δ) m ;	. Compute the number of required samples
7:	C — a multiset of m samples from W where each j ∈ W is sampled with probability qj ;
8:	W — (0,..., 0) ∈ R1×η ;	. Initialize the compressed weight vector
9:	for j ∈ C do	. Update the entries of the sparsified weight matrix according to the samples C
10:	Wj — Wj + mWq-;	. Entries are reweighted by m1q- to ensure unbiasedness of our estimator
11:	return W;
Subsequently, we apply our core sampling scheme to sparsify the set of incoming weighted edges to
each neuron in all layers (Lines 7-13). For technical reasons (see Sec. 5), we perform the sparsification
on the positive and negative weighted edges separately and then consolidate the results (Lines 11-
13). By repeating this procedure for all neurons in every layer, we obtain a set θ = (W2,..., WL)
of sparse weight matrices such that the output of each layer and the entire network is approximately
preserved, i.e., W'a'-1(x) ≈ W'a'-1(x) and f@(x) ≈ fθ(x), respectively1.
1a'-1(x) denotes the approximation from previous layers for an input X 〜D; see Sec. 5 for details.
4
Published as a conference paper at ICLR 2019
4.2	Sparsifying Weights
The crux of our compression scheme lies in Alg. 2 (invoked twice on Line 12, Alg. 1) and in particular,
in the importance sampling scheme used to select a small subset of edges of high importance. The
cached activations are used to compute the sensitivity, i.e., relative importance, of each considered
incoming edge j ∈ W to neuron i ∈ [η'], ' ∈ {2,...,L} (Alg. 2, Lines 1-2). The relative importance
of each edge j is computed as the maximum (over x ∈ S) ratio of the edge’s contribution to the sum of
contributions of all edges. In other words, the sensitivity sj of an edge j captures the highest (relative)
impact j had on the output of neuron i ∈ [η'] in layer ' across all X ∈ S.
The sensitivities are then used to compute an importance sampling distribution over the incoming
weighted edges (Lines 4-5). The intuition behind the importance sampling distribution is that if sj is
high, then edge j is more likely to have a high impact on the output of neuron i, therefore we should
keep edge j with a higher probability. m edges are then sampled with replacement (Lines 6-7) and the
sampled weights are then reweighed to ensure unbiasedness of our estimator (Lines 9-10).
4.3	Extensions: Neuron Pruning and Amplification
In this subsection we outline two improvements to our algorithm that that do not violate any of our
theoretical properties and may improve compression rates in practical settings.
Neuron pruning (CoreNet+) Similar to removing redundant edges, we can use the empirical acti-
vations to gauge the importance of each neuron. In particular, if the maximum activation (over all
evaluations X ∈ S) of a neuron is equal to 0, then the neuron - along with all of the incoming and
outgoing edges - can be pruned without significantly affecting the output with reasonable probability.
This intuition can be made rigorous under the assumptions outlined in Sec. 5.
Amplification (CoreNet++) Coresets that provide stronger approximation guarantees can be con-
structed via amplification - the procedure of constructing multiple approximations (coresets)
(W')ι,..., (Wi)T over T trials, and picking the best one. To evaluate the quality of each approxi-
mation, a different subset T ⊆ P \ S can be used to infer performance. In practice, amplification
would entail constructing multiple approximations by executing Line 12 of Alg. 1 and picking the one
that achieves the lowest relative error on T.
5	Analysis
In this section, we establish the theoretical guarantees of our neural network compression algorithm
(Alg. 1). The full proofs of all the claims presented in this section can be found in the Appendix.
5.1	Preliminaries
Let x 〜D be a randomly drawn input point. We explicitly refer to the pre-activation and activation
values at layer ' ∈ {2,...,'} with respect to the input X ∈ SuPP(D) as zi(x) and ai(x), respectively.
The values of zi(x) and Oe(X) at each layer ' will depend on whether or not We compressed the
previous layers ' ∈ {2,...,'}. To formalize this interdependency, we let zi(x) and ai(x) denote the
respective quantities of layer ' when we replace the weight matrices W2,..., Wi in layers 2,...,'
by W2,..., Wi, respectively.
For the remainder of this section (Sec. 5) we let ' ∈ {2,...,L} be an arbitrary layer and let i ∈ [ηi]
be an arbitrary neuron in layer `. For purposes of clarity and readability, we will omit the the variable
denoting the layer ' ∈ {2,...,L}, the neuron i ∈ [ηi], and the incoming edge index j ∈ [η'-1],
whenever they are clear from the context. For example, when referring to the intermediate value of a
neuron i ∈ [ηi] in layer ' ∈ {2,...,L}, z'(x) = (w', αi-1(x)) ∈ R with respect to a point x, we will
simply write Z(X) = hw, a(x)i ∈ R, where W := w' ∈ R1×η and a(x) := ai-1(x) ∈ Rn ×1.
Under this notation, the weight of an incoming edge j is denoted by wj ∈ R.
5.2	Importance Sampling Bounds for Positive Weights
In this subsection, we establish approximation guarantees under the assumption that the weights are
positive. Moreover, we will also assume that the input, i.e., the activation from the previous layer, is
5
Published as a conference paper at ICLR 2019
non-negative (entry-wise). The subsequent subsection will then relax these assumptions to conclude
that a neuron’s value can be approximated well even when the weights and activations are not all
positive and non-negative, respectively. Let W = {j ∈ [η'-1] : Wj > 0} ⊆ [η`-1] be the set of
indices of incoming edges with strictly positive weights. To sample the incoming edges to a neuron,
we quantify the relative importance of each edge as follows.
Definition 2 (Relative Importance). The importance of an incoming edge j ∈ W with respect to an
input X ∈ SuPP(D) is given by the function gj (x), where gj (x) = P W W (X[(x) Vj ∈ W ∙
Note that gj (x) is a function of the random variable X 〜D. We now present our first assumption that
pertains to the Cumulative Distribution Function (CDF) of the relative importance random variable.
Assumption 1. For all j ∈ W ,the CDF of the random variable gj (x), denoted by Fj (∙), satisfies
Fj (M/K) ≤ exP(-1/K), where M = min{X ∈ [0, 1] : Fj (X) = 1},
and K ∈ [2, log(ηη*)] is a universal constant.2 * *
Assumption 1 is a technical assumption on the ratio of the weighted activations that will enable
us to rule out pathological problem instances where the relative importance of each edge can-
not be well-approximated using a small number of data points S ⊆ P . Henceforth, we con-
sider a uniformly drawn (without replacement) subsample S ⊆ P as in Line 2 of Alg. 1, where
|S| = dlog (8ηη*∕δ) log(ηη*)], and define the sensitivity of an edge as follows.
iid
Definition 3 (Empirical Sensitivity). Let S ⊆ P be a subset OfdiStmctPomtSfrom P 〜 Dn.Then,
the sensitivity over positive edges j ∈ W directed to a neuron is defined as sj = maxx∈S gj (X).
Our first lemma establishes a core result that relates the weighted sum with respect to the sparse row
vector W, Pk∈w Wk ^k (x), to the value of the of the weighted sum with respect to the ground-truth
row vector w, J2k∈w Wk ^k(x). We remark that there is randomness with respect to the randomly
generated row vector Wi, a randomly drawn input X 〜 D, and the function a(∙) = ^i-1(∙) defined by
the randomly generated matrices W2,..., Wi-1 in the previous layers. Unless otherwise stated, we
will henceforth use the shorthand notation P(∙) to denote P1^', χ,^'-ι (∙). Moreover, for ease of presen-
tation, we will first condition on the event E1/2 that a(x) ∈ (1 士 1∕2)a(x) holds. This conditioning will
simplify the preliminary analysis and will be removed in our subsequent results.
Lemma 1 (Positive-Weights Sparsification). Let ε,δ ∈	(0,1),	and X 〜	D.
SPARSIFY(W, W, ε, δ, S, a(∙)) generates a row vector W such that
P (X Wk ^k(x) ∈ (1 ± ε) X Wk ^k(x)
k∈W	k∈W
| E1/2
3δ
≤——
8η
where nnz(W) ≤ ∣8 S 10晨笑?10晨8”商],and S = Pj∈w Sj.
5.3	Importance Sampling Bounds
We now relax the requirement that the weights are strictly positive and instead consider the following
index sets that partition the weighted edges: W+ = {j ∈ [η'-1] : Wj > 0} and W- = {j ∈ [η'-1]:
Wj < 0}. We still assume that the incoming activations from the previous layers are positive (this
assumption can be relaxed as discussed in Appendix A.2.4). We define ∆'(x) for a point X 〜D
and neuron i ∈ [ηi] as δ'(X)
Pk∈[η'-i]∣w'k αk-1(x)∣
∣Pk∈[η'-1] w'k αk I(X)I
The following assumption serves a similar
purpose as does Assumption 1 in that it enables us to approximate the random variable ∆'(x) via an
empirical estimate over a small-sized sample of data points S ⊆ P .
Assumption 2 (Subexponentiality of ∆'(x)). For any layer ' ∈ {2,...,L} and neuron i ∈ [ηi],
the centered random variable Δ = ∆'(x) — E X〜D[∆'(x)] is subexponential (Vershynin, 2016) with
parameter λ ≤ log(ηη*)∕2, i.e., E[exp(sΔ)] ≤ exp(s2λ2) ∀∣s∣ ≤ 1.2
2The upper bound of log(ηη*) for K and λ can be considered somewhat arbitrary in the sense that, more
generally, we only require that K, λ ∈ O(POlylog(ηη*∣P∣). Defining the upper bound in this way simplifies
the presentation of the core ideas without having to deal with the constants involved in the asymptotic notation.
6
Published as a conference paper at ICLR 2019
For ε ∈ (0, 1) and ` ∈ {2, . . . , L}, we let ε0
2(L⅛ and define ε' = ∆→ = 2(L-1)Ql='∆k ,
where ∆^' =(a max^n'] Pχo∈s ∆'(x0)) + κ. To formalize the interlayer dependencies, for each
i ∈ [η'] We let Ei denote the (desirable) event that ^'(x) ∈ (1 ± 2(' 一 1) ε'+ι) z'(x) holds, and let
E' = ∩i∈[η'] Ei be the intersection over the events corresponding to each neuron in layer '.
Lemma 2 (Conditional Neuron Value Approximation). Let ε,δ ∈ (0,1), ' ∈ {2,...,L}, i ∈ [η'],
and X 〜D. CORENET generates a row vector Wi = W'+ — W'- ∈ R1×n	such that
P (Ei I E'-1) = P (z'(x) ∈ (1 ± 2(' - 1) ε'+ι) z'(x) | E'-1) ≥ 1 — δ∕n,	(1)
where ε = -∆ε→ and nnz(W') ≤ [8 S logs,,log(8"∕δ) ] + 1, where S = Pj∈w++ Sj + Pj∈w- Sj.
The following core result establishes unconditional layer-wise approximation guarantees and culmi-
nates in our main compression theorem.
Lemma 3 (Layer-wise Approximation). Let ε,δ ∈ (0,1), ' ∈ {2,...,L}, and X 〜D. CoreNet
generates a sparse weight matrix W' ∈ Rn ×n such that, for z'(x) = W'a'(x),
P	(Ei) = P
(W2,…,W' ),x	(W2,…,W')
(z'(x) ∈ (1 ± 2(' - 1) ε'+ι) z'(x)) ≥ 1 - δ P'0=2」
xη
Theorem 4 (Network Compression). For ε, δ ∈ (0, 1), Algorithm 1 generates a set of parameters
θ= (W 2,...,W l) of size
nnz(θ) ≤
XL Xn' & *
i=2	i=1
32(L - 1)2 (∆'→)2 s' iog(ηη*) iog(8η∕δ)
ε2
+1
in O (η η* log (ηη*∕δ)) time such that Pθ,χ〜D (fθ(x) ∈ (1 ± ε)fθ(x)) ≥ 1 - δ.
We note that we can obtain a guarantee for a set of n randomly drawn points by invoking Theorem 4
with δ0 = δ∕n and union-bounding over the failure probabilities, while only increasing the sampling
complexity logarithmically, as formalized in Corollary 12, Appendix A.2.
5.4	Generalization B ounds
As a corollary to our main results, we obtain novel generalization bounds for neural networks in terms
of empirical sensitivity. Following the terminology of Arora et al. (2018), the expected margin loss ofa
classifier fθ : Rd → Rk parameterized by θ with respect to a desired margin γ > 0 and distribution D
is defined by LY (fθ) = P(χ,y)〜DX Y (fθ (x)y ≤ Y + maxi=y fθ (x)i). We let LY denote the empirical
estimate of the margin loss. The ,following corollary follows directly from the argument presented
in Arora et al. (2018) and Theorem 4.
Corollary 5 (Generalization Bounds). For any δ ∈ (0, 1) and margin γ > 0, Alg. 1 generates weights
θ such that with probability at least 1 一 δ, the expected error Lo(f^~) With respect to the points in
P ⊆ X, |P| = n, is bounded by
Lo(fθ) ≤ Lγ(fθ)
6 Results
In this section, we evaluate the practical effectiveness of our compression algorithm on popular
benchmark data sets (MNIST (LeCun et al., 1998), FashionMNIST (Xiao et al., 2017), and CIFAR-
10 (Krizhevsky & Hinton, 2009)) and varying fully-connected trained neural network configurations:
2 to 5 hidden layers, 100 to 1000 hidden units, either fixed hidden sizes or decreasing hidden size
denoted by pyramid in the figures. We further compare the effectiveness of our sampling scheme in
7
Published as a conference paper at ICLR 2019
reducing the number of non-zero parameters of a netWork, i.e., in sparsifying the Weight matrices, to
that of uniform sampling, Singular Value Decomposition (SVD), and current state-of-the-art sampling
schemes for matrix sparsification (Drineas & Zouzias, 2011; Achlioptas et al., 2013; Kundu & Drineas,
2014), which are based on matrix norms - 'ι and '2 (Frobenius). The details of the experimental setup
and results of additional evaluations may be found in Appendix B.
Experiment Setup We compare against three variations of our compression algorithm: (i) sole edge
sampling (CoreNet), (ii) edge sampling with neuron pruning (CoreNet+), and (iii) edge sampling with
neuron pruning and amplification (CoreNet++). For comparison, we evaluated the average relative
error in output (`1 -norm) and average drop in classification accuracy relative to the accuracy of the
uncompressed network. Both metrics were evaluated on a previously unseen test set.
Results Results for varying architectures and datasets are depicted in Figures 1 and 2 for the average
drop in classification accuracy and relative error (`1 -norm), respectively. As apparent from Figure 1, we
are able to compress networks to about 15% of their original size without significant loss of accuracy
for networks trained on MNIST and FashionMNIST, and to about 50% of their original size for CIFAR.
Percentage of Non-zero Parameters Retained
4 3 2 1
【求】U- dRα ωCT2ω><
Figure 1: Evaluation of drop in classification accuracy after compression against the MNIST, CIFAR, and
FashionMNIST datasets With varying number of hidden layers (L) and number of neurons per hidden layer
(η*). Shaded region corresponds to values within one standard deviation of the mean.
3 2 10 12
2 2 2 2 - -
2 2
」Rli]a>qID-aα ωmrob><
Uniform
MNIST： L= 3, η = 1000, Pyramid
」OJ」山 3>le-3l±ωCT2ω><
CIFAR: L= 3,η* = 1000, Pyramid
3 2 10
2 2 2 2
-IOJ」山 a>qIDω□::
FashionMNIST: L= 3, η = 1000, Pyramid
10	20	30	40	10	20	30	40	10	20	30	40
Percentage of Non-zero Parameters Retained	Percentage of N on-zero Parameters Retained	Percentage of N on-zero Parameters Retained
Figure 2: Evaluation of relative error after compression against the MNIST, CIFAR, and FashionMNIST datasets
With varying number of hidden layers (L) and number of neurons per hidden layer (η*).
Discussion The simulation results presented in this section validate our theoretical results established
in Sec. 5. In particular, our empirical results indicate that We are able to outperform netWorks com-
pressed via competing methods in matrix sparsification across all considered experiments and trials.
The results presented in this section further suggest that empirical sensitivity can effectively capture
the relative importance of neural netWork parameters, leading to a more informed importance sampling
scheme. Moreover, the relative performance of our algorithm tends to increase as We consider deeper
architectures. These findings suggest that our algorithm may also be effective in compressing modern
convolutional architectures, Which tend to be very deep.
7	Conclusion
We presented a coresets-based neural netWork compression algorithm for compressing the parameters
of a trained fully-connected neural netWork in a manner that approximately preserves the netWork’s
output. Our method and analysis extend traditional coreset constructions to the application of com-
pressing parameters, Which may be of independent interest. Our Work distinguishes itself from prior
approaches in that it establishes theoretical guarantees on the approximation accuracy and size of the
generated compressed netWork. As a corollary to our analysis, We obtain generalization bounds for
8
Published as a conference paper at ICLR 2019
neural networks, which may provide novel insights on the generalization properties of neural net-
works. We empirically demonstrated the practical effectiveness of our compression algorithm on a
variety of neural network configurations and real-world data sets. In future work, we plan to extend
our algorithm and analysis to compress Convolutional Neural Networks (CNNs) and other network ar-
chitectures. We conjecture that our compression algorithm can be used to reduce storage requirements
of neural network models and enable fast inference in practical settings.
Acknowledgments
This research was supported in part by the National Science Foundation award IIS-1723943. We thank
Brandon Araki and Kiran Vodrahalli for valuable discussions and helpful suggestions. We would also
like to thank Kasper Green Larsen, Alexander Mathiasen, and Allan Gronlund for pointing out an error
in an earlier formulation of Lemma 6.
References
Dimitris Achlioptas, Zohar Karnin, and Edo Liberty. Matrix entry-wise sampling: Simple is best.
SubmittedtoKDD, 2013(1.1):1-4, 2013. 8, 27
Pankaj K Agarwal, Sariel Har-Peled, and Kasturi R Varadarajan. Geometric approximation via core-
sets. Combinatorial and computational geometry, 52:1-30, 2005. 2
Alireza Aghasi, Afshin Abdi, Nam Nguyen, and Justin Romberg. Net-trim: Convex pruning of deep
neural networks with performance guarantee. In Advances in Neural Information Processing Sys-
tems, pp. 3180-3189, 2017. 2
Jose M Alvarez and Mathieu Salzmann. Compression-aware training of deep networks. In Advances
in Neural Information Processing Systems, pp. 856-867, 2017. 2
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep
nets via a compression approach. arXiv preprint arXiv:1802.05296, 2018. 2, 7
Olivier Bachem, Mario Lucic, and Andreas Krause. Practical coreset constructions for machine learn-
ing. arXiv preprint arXiv:1703.06476, 2017. 2
Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoder-
decoder architecture for image segmentation. CoRR, abs/1511.00561, 2015. URL http://
arxiv.org/abs/1511.00561. 1
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pp. 6241-6250, 2017. 2
Vladimir Braverman, Dan Feldman, and Harry Lang. New frameworks for offline and streaming
coreset constructions. arXiv preprint arXiv:1612.00889, 2016. 1, 2
Wenlin Chen, James T. Wilson, Stephen Tyree, Kilian Q. Weinberger, and Yixin Chen. Compress-
ing convolutional neural networks. CoRR, abs/1506.04449, 2015a. URL http://arxiv.org/
abs/1506.04449. 2
Wenlin Chen, James T. Wilson, Stephen Tyree, Kilian Q. Weinberger, and Yixin Chen. Compressing
neural networks with the hashing trick. CoRR, abs/1504.04788, 2015b. URL http://arxiv.
org/abs/1504.04788. 2
Yu Cheng, Felix X Yu, Rogerio S Feris, Sanjiv Kumar, Alok Choudhary, and Shi-Fu Chang. An
exploration of parameter redundancy in deep networks with circulant projections. In Proceedings of
the IEEE International Conference on Computer Vision, pp. 2857-2865, 2015. 2
Anna Choromanska, Krzysztof Choromanski, Mariusz Bojarski, Tony Jebara, Sanjiv Kumar, and Yann
LeCun. Binary embeddings with structured hashed projections. In International Conference on
Machine Learning, pp. 344-353, 2016. 2
Misha Denil, Babak Shakibi, Laurent Dinh, Marc Aurelio Ranzato, and Nando de Freitas. Pre-
dicting parameters in deep learning. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahra-
mani, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 26,
pp. 2148-2156. Curran Associates, Inc., 2013. URL http://papers.nips.cc/paper/
5025-predicting-parameters-in-deep-learning.pdf. 2
9
Published as a conference paper at ICLR 2019
Emily Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. Exploiting linear struc-
ture within convolutional networks for efficient evaluation. CoRR, abs/1404.0736, 2014. URL
http://arxiv.org/abs/1404.0736. 2
Benjamin Doerr. Probabilistic tools for the analysis of randomized optimization heuristics. arXiv
preprint arXiv:1801.06733, 2018. 23
Xin Dong, Shangyu Chen, and Sinno Pan. Learning to prune deep neural networks via layer-wise
optimal brain surgeon. In Advances in Neural Information Processing Systems, pp. 4860-4874,
2017. 2
Petros Drineas and Anastasios Zouzias. A note on element-wise matrix sparsification via a matrix-
valued bernstein inequality. Information Processing Letters, 111(8):385-389, 2011. 8, 28
Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. arXiv preprint
arXiv:1703.11008, 2017. 2
Dan Feldman and Michael Langberg. A unified framework for approximating and clustering data. In
Proceedings of the forty-third annual ACM symposium on Theory of computing, pp. 569-578. ACM,
2011. 2
Yunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev. Compressing Deep Convolutional Net-
works using Vector Quantization. arXiv preprint arXiv:1412.6115, 2014. 2
Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural network
with pruning, trained quantization and huffman coding. CoRR, abs/1510.00149, 2015. URL http:
//arxiv.org/abs/1510.00149. 2, 29
Jonathan H Huggins, Trevor Campbell, and Tamara Broderick. Coresets for scalable bayesian logistic
regression. arXiv preprint arXiv:1605.06423, 2016. 2
Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt
Keutzer. Squeezenet: AleXnet-level accuracy with 50x fewer parameters andj 0.5 mb model size.
arXiv preprint arXiv:1602.07360, 2016. 2
Yani Ioannou, Duncan Robertson, Jamie Shotton, Roberto Cipolla, and Antonio Criminisi. Training
cnns with low-rank filters for efficient image classification. arXiv preprint arXiv:1511.06744, 2015.
2
Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks
with low rank expansions. arXiv preprint arXiv:1405.3866, 2014. 2
William B Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings into a hilbert space.
Contemporary mathematics, 26(189-206):1, 1984. 2
Yong-Deok Kim, Eunhyeok Park, Sungjoo Yoo, Taelim Choi, Lu Yang, and Dongjun Shin. Com-
pression of deep convolutional neural networks for fast and low power mobile applications. arXiv
preprint arXiv:1511.06530, 2015. 2
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
7, 27
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep
convolutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q.
Weinberger (eds.), Advances in Neural Information Processing Systems 25, pp. 1097-
1105. Curran Associates, Inc., 2012. URL http://papers.nips.cc/paper/
4824-imagenet-classification-with-deep-convolutional-neural-networks.
pdf. 1
Abhisek Kundu and Petros Drineas. A note on randomized element-wise matrix sparsification. arXiv
preprint arXiv:1404.0320, 2014. 8, 28
Michael Langberg and Leonard J Schulman. Universal ε-approximators for integrals. In Proceedings of
the twenty-first annual ACM-SIAM symposium on Discrete Algorithms, pp. 598-607. SIAM, 2010.
1,2
10
Published as a conference paper at ICLR 2019
Vadim Lebedev and Victor Lempitsky. Fast convnets using group-wise brain damage. In Computer
Vision and Pattern Recognition (CVPR), 2016 IEEE Conference on, pp. 2554-2564. IEEE, 2016. 2
Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in neural infor-
mation processing systems, pp. 598-605, 1990. 2
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998. 7, 27
Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural pruning. In Advances in Neural
Information Processing Systems, pp. 2178-2188, 2017. 2
Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic seg-
mentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2015. 1
Alejandro Molina, Alexander Munteanu, and Kristian Kersting. Core dependency networks. In Pro-
ceedings ofthe 32nd AAAI Conference on Artificial Intelligence (AAAI). AAAI Press Google Scholar,
2018. 2
Alexander Munteanu and Chris Schwiegelshohn. Coresets-methods and history: A theoreticians design
pattern for approximation and streaming algorithms. KI-KUnstliche Intelligenz, 32(1):37-53, 2018.
2
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. A pac-
bayesian approach to spectrally-normalized margin bounds for neural networks. arXiv preprint
arXiv:1707.09564, 2017a. 2
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring general-
ization in deep learning. In Advances in Neural Information Processing Systems, pp. 5949-5958,
2017b. 2
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming
Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. In
NIPS-W, 2017. 27
Qinfeng Shi, James Petterson, Gideon Dror, John Langford, Alex Smola, and SVN Vishwanathan.
Hash kernels for structured data. Journal of Machine Learning Research, 10(Nov):2615-2637,
2009. 2
Vikas Sindhwani, Tara Sainath, and Sanjiv Kumar. Structured transforms for small-footprint deep
learning. In Advances in Neural Information Processing Systems, pp. 3088-3096, 2015. 2
Cheng Tai, Tong Xiao, Yi Zhang, Xiaogang Wang, et al. Convolutional neural networks with low-rank
regularization. arXiv preprint arXiv:1511.06067, 2015. 2
Karen Ullrich, Edward Meeds, and Max Welling. Soft weight-sharing for neural network compression.
arXiv preprint arXiv:1702.04008, 2017. 2
Roman Vershynin. High-dimensional probability. An Introduction with Applications, 2016. 6, 18
Kilian Q. Weinberger, Anirban Dasgupta, Josh Attenberg, John Langford, and Alexander J. Smola.
Feature hashing for large scale multitask learning. CoRR, abs/0902.2206, 2009. URL http:
//arxiv.org/abs/0902.2206. 2
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep
neural networks. In Advances in Neural Information Processing Systems, pp. 2074-2082, 2016. 2
Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, and Jian Cheng. Quantized Convolutional
Neural Networks for Mobile Devices. In Proceedings of the Inernational Conference on Computer
Vision and Pattern Recognition (CVPR), 2016. 2
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms, 2017. 7, 27
Xiyu Yu, Tongliang Liu, Xinchao Wang, and Dacheng Tao. On compressing deep models by low rank
and sparse decomposition. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 7370-7379, 2017. 2
11
Published as a conference paper at ICLR 2019
Liang Zhao, Siyu Liao, Yanzhi Wang, Jian Tang, and Bo Yuan. Theoretical properties for neural
networks with weight matrices of low displacement rank. CoRR, abs/1703.00144, 2017. URL
http://arxiv.org/abs/1703.00144. 2
Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen. Incremental Network Quantiza-
tion: Towards Lossless CNNs with Low-Precision Weights. In Proceedings of the International
Conference on Learning Representations (ICLR) 2017, feb 2017. 2
Wenda Zhou, Victor Veitch, Morgane Austern, Ryan P Adams, and Peter Orbanz. Compressibility and
generalization in large-scale deep learning. arXiv preprint arXiv:1804.05862, 2018. 2
12
Published as a conference paper at ICLR 2019
A Proofs of the Analytical Results in Section 5
This section includes the full proofs of the technical results given in Sec. 5.
A. 1 Analytical Results for Section 5.2 (Importance Sampling Bounds for
Positive Weights)
A.1.1 Order Statistic Sampling
We now establish a couple of technical results that will quantify the accuracy of our approximations of
edge importance (i.e., sensitivity).
Lemma 6. Let K > 0 be a universal constant and let D be a distribution with CDF F(∙) satisfying
F(M/K) ≤ exp(-1/K), where M = min{x ∈ [0, 1] : F(x) = 1}. Let P = {X1, . . . , Xn} be a
set of n = |P| i.i.d. samples each drawn from the distribution D. Let Xn+ι ~ D be an i.i.d. sample.
Then,
P K max X < Xn+1	≤ exp(-n/K)
Proof. Let Xmax = maxX∈P ; then,
M
P(K Xmax < Xn+1) =	P(Xmax < x/K|Xn+1 = x) d P(x)
0
M
= P (X < x/K)n d P(x)
0
≤ Z MF(x/K)ndP(x)
0
M
≤ F(M/K)n	d P(x)
0
= F(M/K)n
≤ exp(-n/K)
and this completes the proof.
since X1, . . . , Xn are i.i.d.
where F (∙) is the CDF of X ~ D
by monotonicity of F
CDF Assumption,
□
We now proceed to establish that the notion of empirical sensitivity is a good approximation for the
relative importance. For this purpose, let the relative importance gj (x) of an edge j after the previous
layers have already been compressed be
=Wj a(X)
gj( )= Pk∈w Wk &k(X).
Lemma 7 (Empirical Sensitivity Approximation). Let ε ∈ (0, 1/2), δ ∈ (0, 1), ` ∈ {2, . . . , L},
Consider a set S = {xι,...,Xn} ⊆ P of size |S| ≥ dlog(8 ηη*∕δ)log(ηη*)]. Then, conditioned
on the event E∖" occurring, i.e., α(x) ∈ (1 土 1∕2)a(x),
XPD (∃j ∈W : Csj <gj (x) | E1/2) ≤ [
where C = 3 log(ηη*) and W ⊆ [η'-1].
Proof. Consider an arbitrary j ∈ W and x0 ∈ S corresponding to gj (x0) with CDF Fj (∙) and recall
that M = min{x ∈ [0, 1] : Fj (x) = 1} as in Assumption 1. Note that by Assumption 1, we have
F(M/K) ≤ exp(-1/K),
and so the random variables gj(x0) for x0 ∈ S satisfy the CDF condition required by Lemma 6. Now
let E be the event that K sj < gj (x) holds. Applying Lemma 6, we obtain
P(E) =P(Ksj < gj (x)) =P K m0axgj(x0) < gj (x) ≤ exp(-|S|/K).
13
Published as a conference paper at ICLR 2019
Now let E denote the event that the inequality CSj < gj(x) = PkWjW(Xl(X) holds and note that
the right side of the inequality is defined with respect to gj (x) and not gj (x). Observe that since We
conditioned on the event £1/2, we have that a(x) ∈ (1 士 1∕2)ɑ(x).
Now assume that event E holds and note that by the implication above, we have
CS < ʌ (X)=	wj aj(X)	≤	(1 + 1/2)wj a(X)
S <gj ( ) = Pk∈w wk ^k (X) ≤ (1 - 1/2) Pk∈w wk ak(x)
≤ 3 ∙	Wj aj(X)
一 Pk∈W wk ak(X)
3 gj (x).
where the second inequality follows from the fact that 1+1/2/1-1/2 ≤ 3. Moreover, since we know
that C ≥ 3K, we conclude that if event E occurs, we obtain the inequality
3 K Sj ≤ 3 gj (X) ⇔ K Sj ≤ gj (X),
which is precisely the definition of event £. Thus, we have shown the conditional implication (E |
E1/2 ⇒ E , which implies that
,^ . , .. , ，-、
P(E | E1/2) = P(CSj < gj(x) | E1/2) ≤ P(E)
≤ exp(-|S|/K).
Since our choice of j ∈ W was arbitrary, the bound applies for any j ∈ W. Thus, we have by the
union bound
P(∃j ∈ W : C Sj < gj (x) | E1/2) ≤ E P(CSj
j∈W
<gj(x) | E1/2) ≤ |W| exp(-∣S∣∕K)
□
In practice, the set S referenced above is chosen to be a subset of the original data points, i.e., S ⊆ P
(see Alg. 1, Line 2). Thus, we henceforth assume that the size of the input points |P | is large enough
(or the specified parameter δ ∈ (0, 1) is sufficiently large) so that |P| ≥ |S|.
A.1.2 Proof of Lemma 1
We now state the proof of Lemma 1. In this subsection, we establish approximation guarantees under
the assumption that the weights are strictly positive. The next subsection will then relax this assumption
to conclude that a neuron’s value can be approximated well even when the weights are not all positive.
Lemma 1 (Positive-Weights Sparsification). Let ε,δ ∈	(0,1), and X 〜	D,
SPARSIFY(W, w, ε, δ, S, a(∙)) generates a row vector w such that
P (X wk ^k(X) / (1 ± ε) X wk ^k(x) | E1/2 ) ≤ 3δ
k∈W	k∈W	8η
where nnz(w) ≤ [8 S log(nZ?log(8η∕δ) ], and S = Pj∈w Sj.
Proof. Let ε, δ ∈ (0, 1) be arbitrary. Moreover, let C be the coreset with respect to the weight indices
W ⊆ η'-1] used to construct w. Note that as in SPARSIFY, C is a multiset sampled from W of size
m = 8S log®]?log(8n/))m, where S = Pj∈w Sj and C is sampled according to the probability
distribution q defined by
qj = S	Vj / W.
S
14
Published as a conference paper at ICLR 2019
Let a(∙) be an arbitrary realization of the random variable ^'-1(∙), let X be a realization of X 〜D,
and let
Z= E Wk ak(x)
k∈W
be the approximate intermediate value corresponding to the sparsified matrix W and let
Z= E Wk ^k (x).
k∈W
Now define E to be the (favorable) event that Z ε-approximates Z, i.e., Z ∈ (1 土 ε)Z, We will now show
that the complement of this event, Ec, occurs with sufficiently small probability. Let Z ⊆ supp(D)
be the set of well-behaved points (defined implicitly with respect to neuron i ∈ [η'] and realization ^)
and defined as follows:
Z = {x0 ∈ SuPP(D) : gj(x0) ≤ Csj Vj ∈ W},
where C = 3 log(η η*). Let EZ denote the event that X ∈ Z where X is a realization of X 〜D.
Conditioned on EZ, event EC occurs with probability ≤ 品: Let X be a realization of X 〜D such
that X ∈ Z and let C = {c1 , . . . , cm } be m samples from W with respect to distribution q as before.
Define m random variables Tc1 , . . . , Tcm such that for all j ∈ C
Wj aj (x) = S Wj aj (x)
m qj	m sj
(2)
For any j ∈ C, we have for the conditional expectation of Tj :
E [Tj | a(∙), x, EZ, Ei/2] = X Wk 氤(X) ∙ qk
k∈W	mqk
Σ
k∈W
Z
,
m
Wk ^k (x)
m
where we use the expectation notation E [∙] with the understanding that it denotes the conditional
expectation E C ∣ ^1-1(.), X [∙]. Moreover, we also note that conditioning on the event EZ (i.e., the event
that X ∈ Z) does not affect the expectation of Tj . Let T = j∈C Tj = Z denote our approximation
and note that by linearity of expectation,
E [T | a(∙), X, EZ, E1/2] = X E[Tj | ^(∙), x, EZ, E1/2] = Z
j∈C
Thus, Z = Tis an unbiased estimator of Z for any realization a(∙) and x; thus, we will henceforth refer
to E [T | a(∙), x] as simply Z for brevity.
For the remainder of the proof we will assume that Z > 0, since otherwise, Z = 0 if and only if Tj = 0
for all j ∈ C almost surely, which follows by the fact that Tj ≥ 0 for all j ∈ C by definition of W and
the non-negativity of the ReLU activation. Therefore, in the case that Z = 0,it follows that
P(|Z — Z| >εZ | a(∙), x) = P(Z > 0 | ^(∙), x) = P(0 > 0)=0,
which trivially yields the statement of the lemma, where in the above expression, P(∙) is short-hand for
the conditional probability Pw ∣ a」一1(,),χ (∙).
We now proceed with the case where Z > 0 and leverage the fact that X ∈ Z3 to establish that for all
j∈W:
Csj ≥ gj (χ)
Wj aj (x)
∑k∈W Wk ak(χ)
Wj aj (x)
Z
3Since we conditioned on the event EZ .
15
Published as a conference paper at ICLR 2019
⇔	Wja(X) ≤ cz.
(3)
sj
Utilizing the inequality established above, we bound the conditional variance of each Tj , j ∈ C as
follows
Var(T∙ | ^(∙), x, EZ, Ei/2) ≤ E [(T-)2 ∣ a(∙), x, EZ, E1/2]
Σ (Wk ^k(X))2
(mqk)2	qk
k∈W	qk
—X
m2
(Wk ^k(x))2
k∈W
sk
S
≤ m^
E Wk ^k (x) I Cz
k∈W
_ SCZ2
m2 ,
where Var(∙) is short-hand for Var°∣ ai-i(∙),χ (∙). Since T is a Sum of (conditionally) independent
random variables, we obtain
Var(T | a(∙), x, EZ, Ei/2) = m Var(TjI ^(∙), x, EZ, E1∕2)
SCZq-
≤ ------.
m
(4)
Now, for each j ∈ C let
二 一 一一 .,. - - 一
Tj= Tj- E [Tj 1 aS，x, EZ, E1/2] = Tj- z,
and let T =	j ∈C Tj . Note that by the fact that we conditioned on the realization x of x such that
x ∈ Z (event EZ), we obtain by definition ofTj in (2) and the inequality (3):
S Wj aj(x)	SCz
Tj = 一jjx ≤ ——.	(5)
msj	m
We also have that S ≥ 1 by definition. More specifically, using the fact that the maximum over a set is
greater than the average and rearranging sums, we obtain
S =	sj =	m0ax gj (x0)
x0 ∈S
j∈W j∈W
≥1⅛ XX gj (x0)
j∈W x0∈S
=⅛ XS 1=1.
x ∈S
⅛ XX gj (x0)
x0∈Sj∈W
Thus, the inequality established in (5) with the fact that S ≥ 1 we obtain an upper bound on the
absolute value of the centered random variables:
(5) and the fact that -m ≥ 0, it
(6)
16
Published as a conference paper at ICLR 2019
if Tj < m: Then, using the fact that Tj ≥ 0 and S ≥ 1, we obtain
SCz
〜
〜
m
Applying Bernstein’s inequality to both T and -T we have by symmetry and the union bound,
P(Ec∣ a(∙), x, EZ, E1∕2) = P (|T- z| ≥ εz | a(∙), x, EZ, Eι∕2)
ε2z2	∖
≤ 2 exp -
≤ 2 exp -
2 exp -
2 Var(T | a(∙), x) 十 等M
ε2z2	!
2SC22 I 2SC22 )
m '	3m
3 ε2 m ∖
8SC )
δ
≤ 4η,
where the second inequality follows by our upper bounds on Var(T | ^(∙), x) and ∣Tej-∣ and the fact
that ε ∈ (0, 1), and the last inequality follows by our choice of m =
8 S iog(ηη* )iog(8η∕δ)
m. This
establishes that for any realization ^(∙) of ^l-1(∙) and a realization X of X satisfying X ∈ Z, the event
EC occurs with probability at most 卷.
Removing the conditioning on EZ : We have by law of total probability
P(E	|	^(∙),	E1∕2) ≥ [	P(E	| a(∙), x,	EZ, Eι∕2)	PJx	= x | ^(∙),	Eι∕2) dx
Jχ∈Z	X〜D
≥ 1------
8η
where the second-to-last inequality follows from the fact that P(EC | a(∙), x, EZ, Eι∕2) ≤ 篇 as was
established above and the last inequality follows by Lemma 7.
Putting it all together Finally, we marginalize out the random variable a`-1 (∙) to establish
P(E	| E1∕2)	= [	P(E	|	a(∙),	E1∕2) P(a(∙)	|	E1∕2)	da(∙)
a(∙)
≥(1- 83δ )yαω P(a(∙)ιE1∕2) 风)
=1 - 3δ.
8η
Consequently,
P(Ec∣ E1∕2) ≤ 1 -(1-力=3δ,
8η	8η
and this concludes the proof.
□
17
Published as a conference paper at ICLR 2019
A.2 Analytical Results for Section 5.3 (Importance Sampling Bounds)
We begin by establishing an auxiliary result that we will need for the subsequent lemmas.
A.2.1 EMPIRICAL ∆' APPROXIMATION
Lemma 8 (Empirical ∆' Approximation). Let δ ∈ (0,1), λ* = log(ηη*)∕2, and define
∆'
W mx Xs "0))+ K，
where K = √2λ7(1 + √2λ7 log (8 ηη* /δ)) and S ⊆ P is as in Alg. 1. Then,
P ( max ∆'(x) ≤ ∆' I ≥ 1 ——^-.
x~D ∖i∈[η']	J	4η
Proof. Define the random variables Yχo = E [∆'(x0)] 一 ∆'(x0) for each x0 ∈ S and consider the Sum
Y = X Yχ0 = X (E [∆'(x)] - ∆'(x0))∙
x0∈S	x0∈S
We know that each random variable Yx0 satisfies E [Yx0] = 0 and by Assumption 2, is subexponential
with parameter λ ≤ λ*. Thus, Y is a sum of |S| independent, zero-mean λ*-subexponential ran-
dom variables, which implies that E [Y] = 0 and that we can readily apply Bernstein’s inequality for
subexponential random variables (Vershynin, 2016) to obtain for t ≥ 0
P
AY ≥t
≤ exp —|S|
min
Since S = dlog(8ηη*∕δ)log(ηη*)] ≥ log (8ηη*∕δ) 2λ*,we have for t = √2λ*,
P (E[∆'(x)] -A X ∆'(x0) ≥ t) = P (|S|Y ≥ t
≤ exp
≤ exp(-log(8ηη*∕δ))
_ δ
8 ηη*.
Moreover, for a single Yx, we have by the equivalent definition of a subexponential random vari-
able (Vershynin, 2016) that for u ≥ 0
P(∆'(x) — E [∆'(x)] ≥ U) ≤ exp
- min
ʃ U2 U ]∖
t-4λf, 2λZ∫ )∙
Thus, for u = 2λ* log (8ηη*∕δ) We obtain
P(∆'(x) - E [∆'(x)] ≥ U) ≤ exp (-log (8ηη*∕δ)) = 8^
Therefore, by the union bound, we have with probability at least 1 — 4^:
∆'(x) ≤ E [∆'(x)]+ U
≤ (|S| Xδ'3)+ t! + u
=ɪ X ∆'(X0)+ (p2λ: + 2λ* log(8 ηη*∕δ))
|S| x0∈S
18
Published as a conference paper at ICLR 2019
=∣Ss∣ X δ'(XO) + K
|S | x0∈S
≤ ∆',
where the last inequality follows by definition of ∆'.
Thus, by the union bound, we have
XPD (max δ'(x) > A') = p (∃i ∈ [η']: δ'(X) >
≤ X P (∆'(χ) > ∆')
i∈[η']
≤ η' (4⅛ )
≤ ɪ,
4η
where the last line follows by definition of η* ≥ η'.
□
A.2.2 Notation for the Subsequent Analysis
Let W'+ and W'- denote the sparsified row vectors generated when SPARSIFY is invoked with first
two arguments corresponding to (W+, wi`) and (W-, -wi`), respectively (Alg. 1, Line 12). We will
at times omit including the variables for the neuron i and layer ` in the proofs for clarity of exposition,
and for example, refer to W'+ and Wi- as simply W+ and W-, respectively.
Let x 〜D and define
Z+(x) = ^X W+ ^k(x) ≥ 0 and	Z-(X)= ^X (—W-) ^k(x) ≥ 0
k∈W+	k∈W-
be the approximate intermediate values corresponding to the sparsified matrices W+ and W-; let
Z+(x) = ^X Wk ak(x) ≥ 0 and	Z-(x) = ^X (—Wk) ^k(x) ≥ 0
k∈W+	k∈W-
be the corresponding intermediate values with respect to the the original row vector W; and finally, let
z+(X) =	Wk ak (X) ≥ 0 and	z-(X) =	(—Wk) ak(X) ≥ 0
k∈W+	k∈W-
be the true intermediate values corresponding to the positive and negative valued weights.
Note that in this context, we have by definition
Zi(X) = (W, ^(x)i = Z+(x) — Z-(x),
Zi(X) = (w, ^(x)i = Z+(x) — Z-(x), and
Zii(X) = hW, a(X)i = Z+(X) — Z- (X),
where we used the fact that W = W+ — W- ∈ R1×η' 1.
A.2.3 Proof of Lemma 2
Lemma 2 (Conditional Neuron Value Approximation). Let ε, δ ∈ (0, 1), ` ∈ {2, . . . , L}, i ∈ [ηi],
and X 〜D. CORENET generates a row vector Wi = W'+ — W'- ∈ R1×η such that
P (Ef | Ei-1) = P (Zi(X) ∈ (1 ± 2(' - 1) ε'+ι) Zi(X) | Ei-1) ≥ 1 — δ∕η,	(1)
where Ei = ∆→ andnnz(Wf) ≤ [8S 1。虱ηr,j^0虱8"∕δ)] + ι, where S = Pj∈w++ Sj + Pj∈w- Sj.
19
Published as a conference paper at ICLR 2019
Proof. Let ε, δ ∈ (0,1) be arbitrary and let W+ = {j ∈ [η'-1] : Wj > 0} and W- = {j ∈
[η`-1] : Wj < 0} as in Alg. 1. Let 号' be defined as before,号' =^'→, where A'→ = QL= Ak and
A' = (∣S maxi∈[η'] Px∈s △'(#))十 κ
Observe that Wj > 0 ∀j ∈ W+ and similarly, for all (—Wj) > 0 ∀j ∈ W-. That is, each of index
sets W+ and W- corresponds to strictly positive entries in the arguments Wi and — w', respectively
passed into SPARSIFY. Observe that since We conditioned on the event Ei-1, We have
2(2 — 2) εg ≤ 2(' — 2)
ε
2(L — 1) ∏L=i △k
≤
≤
≤
ε
∏L=i △k
ε
2L-'+1
ε
2,
Since △k ≥ 2 ∀k ∈ {',...,L}
where the inequality △k ≥ 2 follows from the fact that
△k =	17 max X △'(x0) + κ
I ⑸ i∈wιχ⅛
≥ 1 + K	Since △'(x0) ≥ 1 ∀x0 ∈ supp(D) by definition
≥ 2.
we obtain that a(x) ∈ (1 土 ε∕2)a(x), where, as before, a and a are shorthand notations for
^'-1 ∈ Rη' 1×1 and a'-1 ∈ Rη' 1×1, respectively. This implies that E'-1 ⇒ E1/2 and since
m = ∣"8 S log(吗? log(8 η∕δ) ^j in Alg. 2 we can invoke Lemma 1 with ε
invocations to conclude that
号' on each of the Sparsify
P (z+(x) ∈ (1 ± ε')z+(x) | E'-1) ≤ P (z+(x) ∈ (1 ± εe)Z+ (x) | EiQ ≤ 册,
and
P (Z-(X) ∈ (1 ± ε')Z-(x) | E'-1) ≤ ∣δ.
Therefore, by the union bound, we have
P (Z+(x) ∈ (1 土 ε')Z+(x) or Z-(x) ∈ (1 土 ε')Z-(x) | E'-1)
< 3δ	3δ _ 3δ
- 8η	8η	4η
Moreover, by Lemma 8, we have with probability at most 卷 that
△'(x) > △'.
Thus, by the union bound	over	the failure events, we have that with probability at least 1 —
(3δ∕4η + δ∕4η) = 1 — δ∕η that	both	of the following events occur
1.	Z+ (x) ∈ (1 土 ε')Z+(x)	and	Z-(x) ∈ (1 土 ε')Z-(x)	(7)
2.	△f (x) ≤ △'	(8)
Recall that ε0 = ^ (L-I), ε' = ~ε→, and that event Ef denotes the (desirable) event that
*(X)(I ± 2(' - 1) ε'+1) z'(X)
holds, and similarly, E' = ∩f∈[η'] Ef denotes the vector-wise analogue where
Z∕x)(1 土 2(' — 1) ε'+1) z'(x).
20
Published as a conference paper at ICLR 2019
Let k = 2(2 — 1) and note that by conditioning on the event E'_1, i.e., We have by definition
^'-1(x) ∈ (1 士 2(2 — 2)ε')a'-1(x) = (1 ± kεg)&'-1(x),
which follows by definition of the ReLU function. Recall that our overarching goal is to establish that
Z'(x) ∈ (1 ± 2(2- 1)εe+ι) z'(x),
which would immediately imply by definition of the ReLU function that
^i(x) ∈ (1 ± 2(2 - 1)ε'+ι) a，(x).
Having clarified the conditioning and our objective, we will once again drop the index i from the
expressions moving forward.
Proceeding from above, we have with probability at least 1 — δ∕η
z(x) = Z+(x) — Z-(X)
≤ (1 + ε') Z+(x) - (1 - ε') Z-(X)
≤ (1 + εg)(1 + kε?) z+(x) — (1 — εg)(1 — kε?) z (x)
=(1 + εg(k + 1) + kε2) z+(x) + (—1 + (k + 1)εg — kε2) Z (x)
=(1 + kε2) z(x) + (k + 1) εg (z+(x) + Z-(X))
=(1 + kε2) Z(X) + QL+ 1" (z+(x) + Z-(X))
≤ (1 + kε2)Z(X) + δJ:QlIAk (z+(X) + Z-(X))
=(1+kε2)z(x)+QL+1)∆k IZ(X)I
=(1 + kε2) z(x) + (k + 1) ε'+ι ∣z(x) ∣.
By Event (7) above
Conditioning on event E'-1
By Event (8) above
By ∆'(x) = z+(X) + :(X)
∣z(x)∣
To upper bound the last expression above, we begin by observing that kε2 ≤ εg, which follows from
the fact that εg ≤ ? (L-I) ≤ k by definition. Moreover, we also note that εg ≤ ε'+ι by definition of
∆' ≥ 1.
Now, we consider two cases.
Case of z(x) ≥ 0: In this case, we have
Z(x) ≤ (1 + kε2) z(x) + (k + 1) ε^+ι ∣z(x)∣
≤ (1 + ε)z(x) + (k + 1)ε'+ιz(x)
≤ (1 + ε'+I)Z(X) + (k + 1)ε'+1Z(X)
=(1 + (k + 2) ε'+ι) z(x)
=(1 + 2 (2 - 1)ε'+1) z(x),
where the last line follows by definition of k = 2 (2 — 2), which implies that k + 2 = 2(2 — 1). Thus,
this establishes the desired upper bound in the case that z(x) ≥ 0.
Case of z(x) < 0: Since z(x) is negative, we have that (1 + kε2) z(x) ≤ z(x) and ∣z(x)∣ = —z(x)
and thus
Z(x) ≤ (1 + kε2) z(x) + (k + 1) ε^+ι ∣z(x)∣
≤ z(x) — (k + 1)ε'+ιz(x)
≤ (1 — (k + 1)ε'+ι) z(x)
≤ (1 — (k + 2)ε'+ι) z(x)
=(1 — 2(2 — 1)ε'+ι) z(x),
and this establishes the upper bound for the case of z(x) being negative.
21
Published as a conference paper at ICLR 2019
Putting the results of the case by case analysis together, We have the upper bound of Z(x) ≤ z(x) +
2(' — 1)ε'+ι∣z(x)∣. The proof for establishing the lower bound for z(x) is analogous to that given
above, and yields Z(x) ≥ z(x) —2 ('—1)ε'+ι∣z(x)∣. Putting both the upper and lower bound together,
we have that with probability at least 1 — δ:
Z(x) ∈ (1 ± 2 (' - 1)ε'+ι) z(x),
and this completes the proof.
□
A.2.4 Remarks on Negative Activations
We note that up to now we assumed that the input a(x), i.e., the activations from the previous layer,
are strictly nonnegative. For layers ` ∈ {3, . . . , L}, this is indeed true due to the nonnegativity of the
ReLU activation function. For layer 2, the input is a(x) = x, which can be decomposed into a(x) =
apos(x) — aneg(x), where apos(x) ≥ 0 ∈ Rn and aneg(x) ≥ 0 ∈ Rn . Furthermore, we can
define the sensitivity over the set of points {apos (x), aneg(x) | x ∈ S} (instead of {a(x) | x ∈ S}),
and thus maintain the required nonnegativity of the sensitivities. Then, in the terminology of Lemma 2,
we let
zp+os (x) =	wk apos,k (x) ≥ 0
k∈W+
be the corresponding positive parts, and
zneg (x) =	wk aneg,k(x) ≥ 0
k∈W+
and	zn-eg (x) =	(—wk) aneg,k (x) ≥ 0
k∈W-
and	zp-os(x) =	(—wk) apos,k (x) ≥ 0
k∈W-
be the corresponding negative parts of the preactivation of the considered layer, such that
z+ (x) =	zp+os(x)	+zn-eg(x)	and	z-(x)	=	zn+eg(x)	+	zp-os(x).
Wealsolet	∆'(x)=m(X)
|z(x)|
be as before, with z+ (x) and z- (x) defined as above. Equipped with above definitions, we can
rederive Lemma 2 analogously in the more general setting, i.e., with potentially negative activations.
We also note that we require a slightly larger sample size now since we have to take a union bound
over the failure probabilities of all four approximations (i.e. zp+os(x), z-5g(x), z+eg(x), and z-os(x))
to obtain the desired overall failure probability of δ∕n.
A.2.5 Proof of Theorem 4
The following corollary immediately follows from Lemma 2 and establishes a layer-wise approxima-
tion guarantee.
Corollary 9 (Conditional Layer-wise Approximation). Let ε,δ ∈ (0,1), ' ∈ {2,...,L}, and X 〜D.
CORENET generates a sparse weight matrix W' = (t^1 ,...,wn `) > ∈ Rn'×n'-1 such that
P(E' | E'-1) = P (Ze(X) ∈ (1 ± 2(' - 1)£5) z'(x) | E'-1) ≥ 1 — ?,	(9)
where s` = -∆→, z'(x) = W'a'(x), and z'(x) = WCae(X).
Proof. Since (1) established by Lemma 2 holds for any neuron i ∈ [ηe] in layer ' and since (Ee)c =
∪i∈[n'] (E∕)c, it follows by the union bound over the failure events (EiyC for all i ∈ [ηe] that with
` δ
probability at least 1 — nn-
Ze(x) = W'a'-1(x) ∈ (1 土 2(' - 1) E.) Weae-1(x) = (1 ± 2(' - 1) e-) ze(x).
□
22
Published as a conference paper at ICLR 2019
The following lemma removes the conditioning on E'-1 and explicitly considers the (compounding)
error incurred by generating coresets W2,..., W' for multiple layers.
Lemma 3 (Layer-wise Approximation). Let ε,δ ∈ (0,1), ' ∈ {2,...,L}, and X 〜D. CORENET
generates a sparse weight matrix W' ∈ Rn ×η such that, for Z'(x) = W'a'(x),
(W2
pW"(z'(x) ∈(1 ± 2(' - 1) ε'+ι) z'(x)) ≥ 1 - ILP=L
Proof. Invoking Corollary 9, we know that for any layer `0 ∈ {2, . . . , L},
`0
P	(E' | E' T) ≥ 1 - ɪ.	(10)
W '0,x,^'0τ(∙)	η
We also have by the law of total probability that
P(E') = P(E' | E'0-1) P(E'0-1) + P(E' | (E'，T)C) P((E'，T)C)
≥ P(E' | E'0-1) P(E'0-1)	(11)
Repeated applications of (10) and (11) in conjunction with the observation that P(E1) = 14 yield
P(E') ≥ P(E' | E'0-1) P(E'0-1)
`
≥ Y P(E' | E'0-1)
'0 = 2
≥1-
`
δ	`0
η
η七
Repeated applications of (11)
By (10)
By the Weierstrass Product Inequality,
where the last inequality follows by the Weierstrass Product Inequality5 and this establishes the lemma.
□
Appropriately invoking Lemma 3, we can now establish the approximation guarantee for the entire
neural network. This is stated in Theorem 4 and the proof can be found below.
Theorem 4 (Network Compression). For ε, δ ∈ (0, 1), Algorithm 1 generates a set of parameters
θ = (W2,..., W l) of size
nnz(θ) ≤
XX X (&
'=2 i=1
32(L - 1)2 (A'→)2 Si log(ηη*) log(8 η∕δ)
ε2
+1
in O (η η* log (ηη* ∕δ)) time such that P^ X〜D (fθ(x) ∈ (1 土 ε)fθ (x)) ≥ 1 — δ.
4Since we do not compress the input layer.
5The Weierstrass Product Inequality (Doerr, 2018) states that for p1, . . . ,pn ∈ [0, 1],
nn
Y(1-pi) ≥ 1-Xpi.
i=1	i=1
23
Published as a conference paper at ICLR 2019
Proof. Invoking Lemma 3 with ' = L,we have that for θ = (W2,..., WL),
JP (fθ(X) ∈ 2 (L -I) εL+1fθ (X)) = F(ZL(X) ∈ 2 (L -I) εL+1zL(X))
θ,x	θ, x
= P(EL )
≥ 1 - δ pL=2 η
η	η
= 1 - δ,
where the last equality follows by definition of η = PL=2 η`. Note that by definition,
ε
εL+1 = 2(L - 1)QL=l+i ∆k
ε
=2(L-1),
where the last equality follows by the fact that the empty product QL=L+ι ∆k is equal to 1.
Thus, we have
2 (L - 1)εL+1 = ε,
and so we conclude
P (fθ(x) ∈ (1 ± ε)fθ(x)) ≥ 1 - δ,
θ, X
which, along with the sampling complexity of Alg. 2 (Line 6), establishes the approximation guarantee
provided by the theorem.
For the computational time complexity, we observe that the most time consuming operation per itera-
tion of the loop on Lines 7-13 is the weight sparsification procedure. The asymptotic time complexity
of each SPARSIFY invocation for each neuron i ∈ [η'] in layers ' ∈ {2,...,L} (Alg. 1, Line 12) is
dominated by the relative importance computation for incoming edges (Alg. 2, Lines 1-2). This can be
done by evaluating wi`k a`k-1 (X) for all k ∈ W and X ∈ S, for a total computation time that is bounded
above by O (|S| η'-1) since |W| ≤ η'-1 for each i ∈ [η']. Thus, Sparsify takes O (|S| η'-1) time.
Summing the computation time over all layers and neurons in each layer, we obtain an asymptotic time
complexity of O( |S| PL=2 η'-1η') ⊆ O (|S| η* η). Since |S| ∈ O(log(η η*∕δ)), we conclude that
the computational complexity our neural network compression algorithm is
O (η η* log (ηη*∕δ)) .	(12)
□
A.2.6 Proof of Theorem 11
In order to ensure that the established sampling bounds are non-vacuous in terms of the sensitivity, i.e.,
not linear in the number of incoming edges, we show that the sum of sensitivities per neuron S is small.
The following lemma establishes that the sum of sensitivities can be bounded instance-independent by
a term that is logarithmic in roughly the total number of edges (η ∙ η*).
Lemma 10 (Sensitivity Bound). For any ' ∈ {2,...,L} and i ∈ [η'], the sum of sensitivities S =
S+ + S- is bounded by
S ≤ 2 |S| = 2 dlog(8ηη7δ)log(ηη*)].
Proof. Consider S+ for an arbitrary ' ∈ {2,...,L} and i ∈ [η']. For all j ∈ W we have the following
bound on the sensitivity of a single j ∈ W,
max
x∈S
gj (X) ≤	gj (X) =
x∈S	x∈S
Wj aj(X)
pk∈W wk ak(X)
24
Published as a conference paper at ICLR 2019
where the inequality follows from the fact that we can upper bound the max by a summation over
x ∈ S since gj (x) ≥ 0, ∀j ∈ W. Thus,
S+ = sj ≤	gj(x)
j∈W	j∈W x∈S
=Pj∈W wj aj (X) = |S|
X∈S Pk∈W Wk ak(X)	,
where we used the fact that the sum of sensitivities is finite to swap the order of summation.
Using the same argument as above, we obtain S- =	j∈W sj ≤ |S|, which establishes the lemma.
_ □
Note that the sampling complexities established above have a linear dependence on the sum of sen-
Sitivities, PL=2 pη=ι S' which is instance-dependent, i.e., depends on the sampled S ⊆ P and the
actual weights of the trained neural network. By applying Lemma 10, we obtain a bound on the size
of the compressed network that is independent of the sensitivity.
Theorem 11 (Sensitivity-Independent Network Compression). For any given ε, δ ∈ (0, 1) our sam-
pling scheme (Alg. 1) generates a set of parameters θ of size
nnz(θ) ∈ o (卜驹/——工小营5心ηL2 X(△修产),
in O (η 炉 log (ηη*/δ)) time, SUCh that P^ X〜D (fj(x) ∈ (1 土 ε)fθ(x)) ≥ 1 — δ.
Proof. Combining Lemma 10 and Theorem 4 establishes the theorem.
□
A.2.7 Generalized Network Compression
Theorem 4 gives US an approximation guarantee with respect to one randomly drawn point X 〜D. The
following corollary extends this approximation guarantee to any set of n randomly drawn points using
a union bound argument, which enables approximation guarantees for, e.g., a test data set composed
of n i.i.d. points drawn from the distribution. We note that the sampling complexity only increases by
roughly a logarithmic term in n.
Corollary 12 (Generalized Network Compression). For any ε, δ ∈ (0, 1) and a set of i.i.d. input
points P0 ofcardinality |P0| ∈ N+, i.e., P0 ''* DIP |, consider the reparameterized version ofAlg. 1
with
1.	S ⊆ P of size |S| ≥ dlog(16 |P 01 ηη*/δ)log(ηη*)[,
2.	∆` = (看 maxi∈[η'] Pχθ∈s △' (x0)) + K as before, but K is instead defined as
K = p2λ*(1 + p2λ* log (16 |P01 ηη*∕δ)) , and
3.	m ≥ l8 S 'og(nn )εog(16lP i η∕δ) m in the sample complexity in SPARSIFYWEIGHTS.
Then, Alg. 1 generates a set of neural network parameters θ of size at most
32(L — 1)2 (△ '→)2 S' log(ηη*) log(16∣P0| η∕δ)
ε2
+1
∈ O (log(ηη*) log(ηIP0|/s)L2
L	n'
X(∆ r χ s`
'=2	i=1
25
Published as a conference paper at ICLR 2019
in O (η η* log (ηη* |P0∣∕δ)) time such that
F (Vx ∈ P0 : fθ(x) ∈ (1 ± ε)fθ(x)) ≥ 1 - δ.
θ,x	2
Proof. The reparameterization enables US to invoke Theorem 4 with δ0 = δ∕2 |P0∣; applying the union
bound over all |P0∣ i.i.d. samples in P0 establishes the corollary.	口
26
Published as a conference paper at ICLR 2019
B Additional Results
In this section, we give more details on the evaluation of our compression algorithm on popular bench-
mark data sets and varying fully-connected neural network configurations. In the experiments, we
compare the effectiveness of our sampling scheme in reducing the number of non-zero parameters of a
network to that of uniform sampling and the singular value decomposition (SVD). All algorithms were
implemented in Python using the PyTorch library (Paszke et al., 2017) and simulations were conducted
on a computer with a 2.60 GHz Intel i9-7980XE processor (18 cores total) and 128 GB RAM.
For training and evaluating the algorithms considered in this section, we used the following off-the-
shelf data sets:
•	MNIST (LeCun et al., 1998) — 70, 000 images of handwritten digits between 0 and 9 in the
form of 28 × 28 pixels per image.
•	CIFAR-10 (Krizhevsky & Hinton, 2009) — 60, 000 32 × 32 color images, a subset of the
larger CIFAR-100 dataset, each depicting an object from one of 10 classes, e.g., airplanes.
•	FashionMNIST (Xiao et al., 2017) — A recently proposed drop-in replacement for the
MNIST data set that, like MNIST, contains 60, 000, 28 × 28 grayscale images, each as-
sociated with a label from 10 different categories.
We considered a diverse set of network configurations for each of the data sets. We varied the number
of hidden layers between 2 and 5 and used either a constant width across all hidden layers between
200 and 1000 or a linearly decreasing width (denoted by ”Pyramid” in the figures). Training was
performed for 30 epochs on the normalized data sets using an Adam optimizer with a learning rate of
0.001 and a batch size of 300. The test accuracies were roughly 98% (MNIST), 45% (CIFAR10), and
96% (FashionMNIST), depending on the network architecture. To account for the randomness in the
training procedure, for each data set and neural network configuration, we averaged our results across
4 trained neural networks.
B.1	Details on the Compression Algorithms
We evaluated and compared the performance of the following algorithms on the aforementioned data
sets.
1. Uniform (Edge) Sampling — A uniform distribution is used, rather than our sensitivity-based
importance sampling distribution, to sample the incoming edges to each neuron in the net-
work. Note that like our sampling scheme, uniform sampling edges generates an unbiased
estimator of the neuron value. However, unlike our approach which explicitly seeks to mini-
mize estimator variance using the bounds provided by empirical sensitivity, uniform sampling
is prone to exhibiting large estimator variance.
2. Singular Value Decomposition (SVD) — The (truncated) SVD decomposition is used to gen-
erate a low-rank (rank-r) approximation for each of the weight matrices (W2,..., WL) to
obtain the corresponding parameters θ = (IW72,..., IW1L) for various values of r ∈ N+.
Unlike the compared sampling-based methods, SVD does not sparsify the weight matrices.
Thus, to achieve fair comparisons of compression rates, we compute the size of the rank-r
matrices constituting θ as,
Lr
nnz(θ) = XX (nnZ(U') + nnz(vi` ) ,
'=2 i=1
where W' = U'∑'(V')> for each ' ∈ {2,..., L}, with σι ≥ σ2 ... ≥ ση'-ι and u' and
Vi denote the ith columns of U' and V' respectively.
3. `1 Sampling (Achlioptas et al., 2013) — An entry-wise sampling distribution based on the
ratio between the absolute value ofa single entry and the (entry-wise) `1 - norm of the weight
matrix is computed, and the weight matrix is subsequently sparsified by sampling accordingly.
In particular, entry wij of some weight matrix W is sampled with probability
pij
|wj|
kW k'ι,
27
Published as a conference paper at ICLR 2019
4.
and reweighted to ensure the unbiasedness of the resulting estimator.
`2 Sampling (Drineas & Zouzias, 2011) — The entries (i, j) of each weight matrix W are
sampled with distribution
5.
2
wi2j
IpL kWkF,
where |卜|后 is the Frobenius norm of W, and reweighted accordingly.
'1 + '2 Sampling (Kundu & Drineas, 2014) 一 The entries (i,j) of each weight matrix W are
sampled with distribution
6.
7.
8.
p.. = 1 W + WI
ij	2	i∣wkF kwk'1 ,
where ∣∣∙∣∣ f is the Frobenius norm of W, and reweighted accordingly. We note that Kundu &
Drineas (2014) constitutes the current state-of-the-art in data-oblivious matrix sparsification
algorithms.
CoreNet (Edge Sampling) — Our core algorithm for edge sampling shown in Alg. 2, but
without the neuron pruning procedure.
CoreNet+ (CoreNet & Neuron Pruning) — Our algorithm shown in Alg. 1 that includes the
neuron pruning step.
CoreNet++ (CoreNet+ & Amplification) — In addition to the features of Corenet+, multi-
ple coresets C1 , . . . , Cτ are constructed over τ ∈ N+ trials, and the best one is picked by
evaluating the empirical error on a subset T ⊆ P \ S (see Sec. 4 for details).
B.2	Preserving the Output of a Neural Network
We evaluated the accuracy of our approximation by comparing the output of the compressed network
with that of the original one and compute the `1 -norm of the relative error vector. We computed the
error metric for both the uniform sampling scheme as well as our compression algorithm (Alg. 1). Our
results were averaged over 50 trials, where for each trial, the relative approximation error was averaged
over the entire test set. In particular, for a test set Ptest ⊆ Rd consisting of d dimensional points, the
average relative error of with respect to the f© generated by each compression algorithm was computed
errθrPtest (fθ) = lP1j X llfθ(x) - fθ(x)llι.
|Ptest| x∈Ptest
Figures 3, 4, and 5 depict the average performance of the compared algorithms for various network
architectures trained on MNIST, CIFAR-10, and FashionMNIST, respectively. Our algorithm is able to
compress networks trained on MNIST and FashionMNIST to about 10% of their original size without
significant loss of accuracy. On CIFAR-10, a compression rate of 50% yields classification results com-
parable to that of uncompressed networks. The shaded region corresponding to each curve represents
the values within one standard deviation of the mean.
B.3	Preserving the Classification Performance
We also evaluated the accuracy of our approximation by computing the loss of prediction accuracy on
a test data set, Ptest. In particular, let accPtest (fθ) be the average accuracy of the neural network fθ,
i.e,.
accPtest fθ) = P5-T X a a argmax fθ (X) = y(X)),
|Ptest | x∈Ptest	i∈[ηL]
where y(X) denotes the (true) label associated with X. Then the drop in accuracy is computed as
aCCPtest fθ ) - accPtest (fθ).
Figures 6, 7, and 8 depict the average performance of the compared algorithms for various network
architectures trained on MNIST, CIFAR-10, and FashionMNIST respectively. The shaded region cor-
responding to each curve represents the values within one standard deviation of the mean.
28
Published as a conference paper at ICLR 2019
B.4	Preliminary Results with Retraining
We compared the performance of our approach with that of the popular weight thresholding heuristic -
henceforth denoted by WT - of Han et al. (2015) when retraining was allowed after the compression,
i.e., pruning, procedure. Our comparisons with retraining for the networks and data sets mentioned
in Sec. 6 are as follows. For MNIST, WT required 5.8% of the number of parameters to obtain the
classification accuracy of the original model (i.e., 0% drop in accuracy), whereas for the same per-
centage (5.8%) of the parameters retained, CoreNet++ incurred a classification accuracy drop of 1%.
For CIFAR, the approach of Han et al. (2015) matched the original models accuracy using 3% of the
parameters, whereas CoreNet++ reported an accuracy drop of 9.5% for 3% of the parameters retained.
Finally, for FashionMNIST, the corresponding numbers were 4.1% of the parameters to achieve 0%
loss for WT, and a loss of 4.7% in accuracy for CoreNet++ with the same percentage of parameters
retained.
B.5	Discussion
As indicated in Sec. 6, the simulation results presented here validate our theoretical results and suggest
that empirical sensitivity can lead to effective, more informed sampling compared to other methods.
Moreover, we are able to outperform networks that are compressed via state-of-the-art matrix sparsifi-
cation algorithms. We also note that there is a notable difference in the performance of our algorithm
between different datasets. In particular, the difference in performance of our algorithm compared to
the other method for networks trained on FashionMNIST and MNIST is much more significant than for
networks trained on CIFAR. We conjecture that this is partially due to considering only fully-connected
networks as these network perform fairly poorly on CIFAR (around 45% classification accuracy) and
thus edges have more uniformly distributed sensitivity as the information content in the network is
limited. We envision that extending our guarantees to convolutional neural networks may enable us to
further reason about the performance on data sets such as CIFAR.
29
Published as a conference paper at ICLR 2019
Percentage of N on-zero Parameters Retained
Percentage of Non-zero Parameters Retained
10	20	30	40
Percentage of Non-zero Parameters Retained
MNIST： L = 3, q* = 500, Pyramid
a CoreNet
CoreNet+
CoreNet+ +
MNIST=L= 3, "* = 1000, Pyramid
10	20	30	40
Percentage of Non-zero Parameters Retained
MNIST： £= 5, η* = 200, Pyramid
10	20	30	40
Percentage of Non-zero Parameters Retained
Percentage of Non-zero Parameters Retained
10	20	30	40
Percentage of Non-zero Parameters Retained
Percentage of Non-zero Parameters Retained
10	20	30	40
Percentage of Non-zero Parameters Retained
Percentage of Non-zero Parameters Retained
10	20	30	40
Percentage of Non-zero Parameters Retained
Figure 3:	Evaluations against the MNIST dataset with varying number of hidden layers (L) and number of
neurons per hidden layer (η*). Shaded region corresponds to values within one standard deviation of the mean.
The figures show that our algorithm’s relative performance increases as the number of layers (and hence the
number of redundant parameters) increases.
30
Published as a conference paper at ICLR 2019
CIFAR: L = 2, ŋ* = 200, Pyramid
CIFAR=L= 2, η = 500, Pyramid
6 4 2 0
2 2 2 2
.ioɪi 山 3>qπ>ωα ωCTEω><
---Uniform Sampling
——SVD
CoreNet
,∙^<≡≡^CoreNet+
5 3 11
2 2 2-
2
,IOjJ 山ω> 一⅛ωH WCTSω><
---Uniform Sampling
---SVD
CoreNet
'==≡---CereNet+
≤=- CoreIMet+≠———
4 2 0 2
2 2 2-
2
」OJJ 山 3>qnωH ωmsω><
ClFAR=L= 2, η = 1000, Pyramid

---SVD
CoreNet
---CoreNet++
10	20	30	40	50
Percentage of Non-zero Parameters Retained
CIFAR: L = 3, ŋ* = 200, Pyramid
7 5 3 1
2 2 2 2
」0」」山 3>qπ>ωH ωCT2ω><
---Uniform Sampling
——SVD
CoreNet
-----CoreNet+
'2fc-c≡iCore Net++
10	20	30	40	50
Percentage of Non-zero Parameters Retained
10	20	30	40	50
Percentage of Non-zero Parameters Retained
CIFAR: L= 3f ∏* = 500, Pyramid
6 4 2 0
2 2 2 2
」0」」3 9>⅛αjα ωCT2ω><
Uniform Sampling
SVD
CoreNet
CoreNet+
'CDreNet±+
10	20	30	40
Percentage of Non-zero Parameters Retained
CIFAR： L= 3, q* = 1000, Pyramid
4 2 0
2 2 2
」0」」3 9>⅛αjα ωCT2ω><
10	20	30	40
Percentage of Non-zero Parameters Retained
10	20	30	40	50
Percentage of Non-zero Parameters Retained
u29
lij27
⅞25
CIFAR: L= 5, ŋ* = 200, Pyramid
---Uniform Sampling
——SVD
CoreNet
CoreNet+
∙c≡ς- CoreNet++
521
210
28
26
24
( 22
2。
□FAR: L= 5, ŋ* = 500, Pyramid
---Uniform Sampling
——SVD
CoreNet
CoreNet+
CoreNet++
-IO-UW 一⅛-ωɑ
10	20	30	40
Percentage of Non-zero Parameters Retained
CIFAR: L= 3, ηκ = 700
10	20	30	40	50
Percentage of Non-zero Parameters Retained
CIFAR： £= 3, n* = 200
10	20	30	40	50
Percentage of Non-zero Parameters Retained
7 5 3 1
2 2 2 2
.ioɪi 山 3>qπ>-3H ωCTEω><
---Uniform Sampling
——SVD
CoreNet
----CoreNet+
et+ +
CIFAR: £= 3,∏* = 500
7 5 3 1 1
2 2 2 2-
2
,IOjJ 山 3>≈eωH ωmsω><
10	20	30	40	50
Percentage of N on-zero Parameters Retained
Uniform Sampling
SVD
CoreNet
CoreNet+
7 5 3 1 1
2 2 2 2-
2
,IOjJ 山 3>≈EωH ωmsω><
10	20	30	40	50
Percentage of Non-zero Parameters Retained
10	20	30	40	50
Percentage of N on-zero Parameters Retained
CIFAR: L = 5, η* = 200
9 7 5 3 1
2 2 2 2 2
3>qπ>05H ωCT2ω><
aFAR: L = 5,η* = 500
---Uniform Sampling
——SVD
CoreNet
---CoreNet+
——CoreNet++
0 8 6 4 2 0
212 2 2 2 2
-Io-U 山 9>.l-lE ⑥& ωCTEω><
Uniform Sampling
SVD
CoreNet
CoreNet+
CoreNet++
-8 6 4 2 0
U 2 2 2 2 2
」oxl 山ω>≈E ⑥&
10	20	30	40	50
Percentage of Non-zero Parameters Retained
10	20	30	40	50
Percentage of Non-zero Parameters Retained
10	20	30	40
Percentage of Non-zero Parameters Retained
Figure 4:	Evaluations against the CIFAR-10 dataset with varying number of hidden layers (L) and number of
neurons per hidden layer (η*). The trend of our algorithm's improved relative performance as the number of
parameters increases (previously depicted in Fig. 3) also holds for the CIFAR-10 data set.
31
Published as a conference paper at ICLR 2019
FashionMNIST: L= 2, η = 200, Pyramid
3 113
2 2 - -
2 2
,IoJ」w<υ>qnωH<υ6n.l<υ><
---Uniform SamβT
---SVD
CoreNet
≡ CoreNet+
CoreNet++
2 0 2 4
2 2 - -
2 2
,IoJ」w 3>≈EωH 36E.I<υ><
10	20	30	40
Percentage of Non-zero Parameters Retained
FashionMNlST: L = 3, ηa, = 200, Pyramid
-KW山 3>-⅛ωH ωmsω><
2-ι
10	20	30	40
Percentage of Non-zero Parameters Retained
FashionMNlST: L = 5f n* = 200, Pyramid
7 5 3 1 1
2 2 2 2-
2
,IojJ 山 3>qnωH ωmsω><
10	20	30	40
Percentage of Non-zero Parameters Retained
,IojJ 山<υ>qnωH ωmrab><
5 3 I I 3
2 2 2 - -
2 2
,IojJ 山<υ>qnωH ωCTrab><
10	20	30	40
Percentage of Non-zero Parameters Retained
FaShiOnMNlST: L = 5, "* = 200
5 3 113
2 2 2 - -
2 2
JaULu3>-⅛ωH ωmrab><
Uniform Sampling
SVD
CoreNet
CoreNet+
FashionMNlST: L= 2, η = 500, Pyramid
ɪ Uniform Sam∣
SVD
CoreNet+
---- CoreNet+ +
4 2 0 2 4
2 2 2 - -
2 2
,IoJ」w 3>≈EωH 36E.I<υ><
FashionMNlST: £= 2, η = 1000, Pyramid
CoreNet
CoreNet+
---CoreNet++
10	20	30	40
Percentage of Non-zero Parameters Retained
10	20	30	40
Percentage of Non-zero Parameters Retained
5 3 113
2 2 2 - -
2 2
JaULu<υ>-⅛ωH ωmsω><
FashionMNIST: L= 3, η = 1000, Pyramid
23
2 10 12
2 2 2 - -
2 2
-KW山 3>-⅛ωH ωmsω><
10	20	30	40
Percentage of Non-zero Parameters Retained
FashionMNIST: L = 5, η* = 500, Pyramid
5 3 113
2 2 2 - -
2 2
,IojJ 山 3>qEωH ωmsω><
10	20	30	40
Percentage of Non-zero Parameters Retained
FashionMNIST:2.= 3.η = 500
---SVD
CoreNet
CoreNet+
---CoreNet++
10	20	30	40
Percentage of Non-zero Parameters Retained
FashionMNIST: L= 5, η^= 500
5 3 113
2 2 2 - -
2 2
」0J 由 E⅛ωH ωmrab><
10	20	30	40
Percentage of Non-zero Parameters Retained
FashionMNIST: £= 5, ηκ = 1000, Pyramid
.I。-U山 3>qE-3H ωσsω><
2-3
10	20	30	40
Percentage of Non-zero Parameters Retained
4 2 0 2 4
2 2 2 - -
2 2
,IOjJ 山 3>≈EωH ωmrab><
10	20	30	40
Percentage of Non-zero Parameters Retained
」OJ」山 3>一⅛ωH ωmrab><
FashionMNIST: L= 5, η"^ = 700
---Uniform Sampling
——SVD
CoreNet
CoreNet+
10	20	30	40	10	20	30	40	10	20	30	40
Percentage of Non-zero Parameters Retained Percentage of Non-zero Parameters Retained Percentage of Non-zero Parameters Retained
Figure 5:	Evaluations against the FashionMNIST dataset with varying number of hidden layers (L) and number
of neurons per hidden layer (η*).
32
Published as a conference paper at ICLR 2019
MN∣SΓ: L= 2, η = 200, Pyramid
10	20	30	40
Percentage of Non-zero Parameters Retained
Ooooo
8 6 4 2
【％】 &2T-UU< U- dRα
Ooooo
8 6 4 2
【％】 &2T-UU< U- dRα
Percentage of N on-zero Parameters Retained
Percentage of N on-zero Parameters Retained
Percentage of Non-zero Parameters Retained
OOQO
8 6 4 2
【％】 &2T-UU< U- dRα ωmEω><
MNIST：L= 5. n = 200, Pyramid
10	20	30	40
Percentage of Non-zero Parameters Retained
10	20	30	40
Percentage of Non-zero Parameters Retained
Percentage of Non-zero Parameters Retained
Percentage of Non-zero Parameters Retained
MNIST: /.= 3, n* = 700
10	20	30	40
Percentage of Non-zero Parameters Retained
≡100
A
≡ 80
3
υ
< 60
c
d
e 40
Q
<u
S, 20
αj
< 0
10	20	30	40
Percentage of Non-zero Parameters Retained
Percentage of Non-zero Parameters Retained
Percentage of Non-zero Parameters Retained
Percentage of Non-zero Parameters Retained
Figure 6:	Evaluations against the MNIST dataset with varying number of hidden layers (L) and number of
neurons per hidden layer (η*). Shaded region corresponds to values within one standard deviation of the mean.
The figures show that our algorithm’s relative performance increases as the number of layers (and hence the
number of redundant parameters) increases.
33
Published as a conference paper at ICLR 2019
CIFAR: L = 2, η* = 200. Pyramid
10	20	30	40	50
Percentage of Non-zero Parameters Retained
10	20	30	40	50
Percentage of Non-zero Parameters Retained
Percentage of Non-zero Parameters Retained
Ooo
3 2 1
【％】 >UE⊃UU< U- dRα ωrareb><
Percentage of N on-zero Parameters Retained
Percentage of Non-zero Parameters Retained
CIFAR: L = 5, n*= 1000, Pyramid
Ooo
3 2 1
【％】 >UE⊃UU< U- dRα ωrareb><
Percentage of Non-zero Parameters Retained
CIFAR： L= 5f ŋ*= 500, Pyramid
区40
A
U
3 30
LJ
<
⅛20
e
□
&io
lŋ
10	20	30	40	50
Percentage of Non-zero Parameters Retained
10	20	30	40
Percentage of Non-zero Parameters Retained
10	20	30	40	50
Percentage of Non-zero Parameters Retained
Percentage of Non-zero Parameters Retained
Percentage of Non-zero Parameters Retained
CIFAR: L= 5, η* = 700
10	20	30	40
Percentage of Non-zero Parameters Retained
Figure 7:	Evaluations against the CIFAR-10 dataset with varying number of hidden layers (L) and number of
neurons per hidden layer (η*). The trend of our algorithm's improved relative performance as the number of
parameters increases (previously depicted in Fig. 6) also holds for the CIFAR-10 data set.
34
Published as a conference paper at ICLR 2019
FashionMNlST: £= 2, n- = 200. Pyramid
^100
J	--- Uniform Samolina
10	20	30	40
Percentage of N on-zero Parameters Retained
Percentage of Non-zero Parameters Retained
FashionMNIST: L= 2, η* = 1000, Pyramid
Percentage of Non-zero Parameters Retained
FashionMNlST: £= 5, q* = 200, Pyramid
Percentage of Non-zero Parameters Retained
FashionMNIST: L= 3, η = 1000. Pyramid
10	20	30	40
Percentage of N on-zero Parameters Retained
≥100
E 80
- FashionMNlST: L
≥100
£ 40
tu
3 20
:60
---Uniform Sampling
——SVD
oreNet
£
----CoreNet+∙
20
30
40
□
0
10
Percentage of Non-zero Parameters Retained
80
60
40
20
0
5, η" = 500, Pyramid
---Uniform Sampling
——SVD
CoreNet
CoreNet+
---CoreNet+ +
£
⅛
30
40
Percentage of Non-zero Parameters Retained
FashionMNlST: L = 3, rj* = 500
80
60
40
20
0
FashionMNlST: £= 5, q* = 1000, Pyramid
---- Uniform Sampling
——SVD
gιoo
CoreNet
CoreNet+
---CoreNet++
30
40
Percentage of Non-zero Parameters Retained
FashionMNlST: L= 3, ∏* = 700
glOO
ε so
FashionMNlST: L= 3, ∏* = 200
^100
^100
< 60
£ 40
<u
g* 20
Uniform Sampling
SVD
CoreNet
CoreNet+
CoreNet++
20
30
40
0
10
80
60
40
20
---Uniform Sampling
——SVD
CoreNet
CoreNet+
---CoreNet++
⅛
20
30
40
0
10
80
60
40
20
0
---Uniform Sampling
——SVD
CoreNet
CoreNet+
---CoreNet++
30
40
Percentage of Non-zero Parameters Retained
FashionMNlST: £= 5, q* = 700
Percentage of Non-zero Parameters Retained
Percentage of Non-zero Parameters Retained
承Oo
Uniform Sampling
80
60
£
40
20
FashionMNlST: L= 5, η"^ = 200
FashionMNlST: L = 5, ŋ* = 500
≥100
£
20
30
40
20
30
40
Percentage of Non-zero Parameters Retained
Percentage of Non-zero Parameters Retained
0
10
gιoo
A
0
10
80
60
---CoreNet++
40
20
■ Uniform Sampling
-SVD
CoreNet
CoreNet+
20
30
40
Percentage of Non-zero Parameters Retained
0
10


e
e

e

e

Figure 8:	Evaluations against the FashionMNIST dataset with varying number of hidden layers (L) and number
of neurons per hidden layer (η*).
35