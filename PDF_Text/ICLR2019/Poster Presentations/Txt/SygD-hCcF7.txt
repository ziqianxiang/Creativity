Published as a conference paper at ICLR 2019
Dimensionality Reduction for Representing
the Knowledge of Probabilistic Models
Marc T. Law & Jake Snell
University of Toronto, Canada
Vector Institute, Canada
Amir-massoud Farahmand
Vector Institute, Canada
Raquel Urtasun
University of Toronto, Canada
Vector Institute, Canada
Uber ATG, Canada
Richard S. Zemel
University of Toronto, Canada
Vector Institute, Canada
CIFAR Senior Fellow
Ab stract
Most deep learning models rely on expressive high-dimensional representations
to achieve good performance on tasks such as classification. However, the high
dimensionality of these representations makes them difficult to interpret and prone
to over-fitting. We propose a simple, intuitive and scalable dimension reduction
framework that takes into account the soft probabilistic interpretation of standard
deep models for classification. When applying our framework to visualization, our
representations more accurately reflect inter-class distances than standard visual-
ization techniques such as t-SNE. We experimentally show that our framework
improves generalization performance to unseen categories in zero-shot learning.
We also provide a finite sample error upper bound guarantee for the method.
1 Introduction
Dimensionality reduction is an important problem in machine learning tasks to increase classification
performance of learned models, improve computational efficiency, or perform visualization. In the
context of visualization, high-dimensional representations are typically converted to two or three-
dimensional representations so that the underlying relations between data points can be observed
and interpreted from a scatterplot. Currently, a major source of high-dimensional representations
that machine learning practitioners have trouble understanding are those generated by deep neural
networks. Techniques such as PCA or t-SNE (Van der Maaten, 2014) are typically used to visualize
them, e.g., in Law et al. (2017); Snell et al. (2017). Moreover, Schulz et al. (2015) proposed a
visualization technique that represents examples based on their predicted category only. However,
none of these techniques exploit the fact that deep models have soft probabilistic interpretations.
For instance, the output of deep classifiers typically employs softmax regression, which optimizes
classification scores across categories by minimizing cross entropy. This results in soft probabilistic
representations that reflect the confidence of the model in assigning examples to the different
categories. Many other deep learning tasks such as semantic segmentation (Long et al., 2015) or
boundary/skeleton detection (Xie & Tu, 2015) also optimize for probability distributions. In this
paper, we experimentally demonstrate that the soft probability representations learned by a neural
network reveal key structure about the learned model. To this end, we propose a dimensionality
reduction framework that transforms probability representations into a low-dimensional space for
easy visualization.
Furthermore, our approach improves generalization. In the context of zero-shot learning where
novel categories are added at test time, deep learning approaches often learn high-dimensional
representations that over-fit to training categories. By learning low-dimensional representations that
match the classification scores of a high-dimensional pre-trained model, our approach takes into
account inter-class similarities and generalizes better to unseen categories than standard approaches.
Proposed approach: We propose to exploit as input representations the probability scores generated
by a high-dimensional pre-trained model, called the teacher model or target, in order to train a
1
Published as a conference paper at ICLR 2019
lower-dimensional representation, called the student. In detail, our approach learns low-dimensional
student representations of examples such that, when applying a specific soft clustering algorithm on
the student representations, the predicted clustering scores are similar to the target probability scores.
Contributions: This paper makes the following contributions: (1) We propose the first dimen-
sionality reduction approach optimized to consider some soft target probabilistic representations
as input. (2) By exploiting the probability representations generated by a pre-trained model, our
approach reflects the learned semantic structure better than standard visualization approaches. (3) We
experimentally show that our approach improves generalization performance in zero-shot learning.
(4) We theoretically analyze the statistical properties of the approach and provide a finite sample error
upper bound guarantee for it.
2 Dimensionality Reduction for Probabilistic Representations
(DRPR)
Our method, called Dimensionality Reduction of Probabilistic Representations (DRPR, pronounced
“Derper”), is given probability distribution representations generated from high-dimensional data
as target. Its goal is to learn a low-dimensional representation such that the soft clustering scores
predicted by a soft clustering algorithm are similar to the target. If the targets are probability
distributions generated by a pre-trained classifier, we want the low-dimensional space to reflect
the relationships between categories interpreted by the classifier. The position of each example in
the low-dimensional space should then reflect the ambiguity of the classifier for the example (see
Fig. 1). We summarize in Section 2.1 the soft clustering algorithm that is used by DRPR in the
low-dimensional space, the algorithm is detailed in Banerjee et al. (2005). The general learning
algorithm of DRPR is introduced in Section 2.2.
2.1	Family of efficient soft clustering algorithms
Probability density: We consider that We are given a set of n vectors fι, •…，fn ∈ V concatenated
into a single matrix F = [fι, •…，fn]> ∈ Vn. Inthe following, we consider V = Rd, and Vn = Rn×d.
The goal is to partition n examples into k soft clusters. Each cluster Cc with C ∈ {1, ∙∙∙ ,k} has a
center μc ∈ V and its corresponding probability density is pc(fj) = exp(-d(" μc)) b(fi), where
d is a regular Bregman divergence (Banerjee et al., 2005; Bregman, 1967) and b : V → R+ is a
uniquely determined function that depends on d and ensures that the integral of the density over V
is 1 (e.g., b(fi) = ,1∕(2π)d if d is the squared Euclidean distance). For simplicity, we consider
that d is the squared Euclidean distance. The density pc(fi) decreases as the divergence between the
example f and the center μc increases.
Bregman Soft Clustering problem (BSCP): The BSCP (Banerjee et al., 2005) is defined as that
of learning the maximum likelihood parameters Γ = {μc, Πc}C= of a mixture model p(fi∣Γ)=
Pk=ι ∏c exp(-d(fi, μc)) b(fi) where ∏c is the prior probability that f is generated by 'Cc. To
partition the n examples into k clusters, Banerjee et al. (2005) apply the EM algorithm to maximize
the likelihood parameter estimation problem for mixture models formulated as: maxΓ ∏n=1 P(fi∣Γ)
Assignment matrix: Partitioning the n observations in F into k soft clusters is equivalent to
determining some soft assignment matrix Y ∈ (0, i)n×k in the set Yn×k of matrices whose rows
are positive and sum to 1. Formally, Yn×k is written Yn×k := {Y ∈ (0,1)n×k : Y 1k = 1n}
where 1k ∈ {1}k is the k-dimensional vector containing only 1. For a given value of Γ, the element
Yic = p(Cc|fi) ∈ (0, 1) is the posterior probability, or responsibility of Cc for fi. The higher the
value of Yic, the more likely fi belongs to cluster Cc .
maximum of
Local maximum condition: Once the BSCP has converged to a local
maxr Qn=ι p(fi∣Γ), the following equations are all satisfied:
(E-step) ∀i,c, P(Cyfi)= YiC
(M-step) ∀c, μc = TP=1 Ycfi
i=1 Yic
∏C exp(-d(fi, μC))
Pm=I πm exp(-d(fi, μm))
1n
and	∀c, πc = — Y Yic
n
i=1
(1)
(2)
2
Published as a conference paper at ICLR 2019
I—' /1 ∖	1 , ,1 1-1	C ,F LTl ιr 1	∙ ,1	,1 ,	,	“c Ic ∖	^x 'r	F	,1
Eq. (1) corresponds to the E-step of the EM algorithm that computes p(Cc|fi) = Yic when the
parameters F and Γ are given. Eq. (2) corresponds to the M-step, which has a simple form since
the likelihood is a regular exponential family function (Banerjee et al., 2005). The M-step may be
computationally expensive for other types of exponential family distributions (Banerjee et al., 2005,
Section 5.2). It is worth noting that these optimality conditions do not depend on the function b used
to define pc(fi), so b can be ignored.
2.2	Learning the mapping to the low-dimensional space
Section 2.1 explains how to perform soft clustering on some fixed representation F.
We now describe how to learn F so that
the soft clustering scores predicted by
the BSCP match those of the target. We
assume that we are given the probability
representation y ∈ [0,1]k as target for
each training example " These represen-
tations are concatenated into a single ma-
trix Y = [yι,…,yn]> ∈ Yn×k which
is the target of our method for F. DRPR
learns the representation of F ∈ Vn such
that the soft assignment matrix obtained
from applying the BSCP on F is close to
Y. We first give the formulation of our
prediction function in Eq. (3). Our di-
mensionality reduction problem is given
in Eq. (4).
Figure 1: DRPR learns low-dimensional representations
that reflect the uncertainties of a pre-trained classifier (here
for categories blue and yellow represented by their centers)
Prediction function: Let us assume that
we are given the dataset matrix F = [fι, •一，fn]> ∈ Vn = Rn×d,the centers M = [μι, ∙∙∙,μ卜]> ∈
Vk = Rk×d and the priors π = [∏ι,…，∏k]> ∈ (0,1)k. As in Eq. (1), we define our prediction
function ψ(F, M, π) = Ψ ∈ Yn×k as the soft assignment matrix predicted by the BSCP given F,
M and π. The elements of the matrix Ψ are then computed as follows:
∀i, c, Ψic = kπ eXP(-d(fi，μc))
Em=I πm exP(-d(fi, μm))
(3)
Optimization problem: DRPR learns the representation of F so that the predicted assignment
matrix ψ(F, M, π) = Ψ is similar to Y. Given the optimal condition properties of the BSCP stated
in Section 2.1, the optimal values of M and π also depend on Ψ and are therefore variables of our
dimensionality reduction problem that we formulate:
min ∆n (ψ(F,M,π),Y)
F,M,π
(4)
The function ∆n(Ψ, Y) is an empirical discrepancy loss between the predicted assignment matrix Ψ
and the target assignment matrix Y. Since the rows of Ψ and Y represent probability distributions,
we formulate ∆n as the average KL-divergence between the rows of Y and the rows of Ψ. Let ψi>
and y> be the i-th rows of Ψ and Y, respectively, we formulate ∆n(Ψ, Y) = ɪ P；=、DκL(yikΨi).
Note that the choice of the discrepancy loss ∆n is independent of the chosen Bregman divergence d.
Moreover, the number of classes k has an impact on the number of clusters in the low-dimensional
space but is not related to the dimensionality d of the model. DRPR considers that each class
C ∈ {1, ∙∙∙ ,k} is represented by one cluster prototype 从丁 ∈ Rd.
Terminology: In our experiments, the target (or teacher) is the assignment matrix Y ∈ Yn×k that
contains the probability scores generated by a pre-trained neural network. It corresponds to the output
of a classifier trained with softmax regression in the visualization experiments, and to the matrices Y1
and Y2 described in Section 4.2. The goal is then to learn F, M and π so that (1) F, M and π reach
the BSCP optimality conditions given in Section 2.1 and (2) Ψ = ψ(F, M, π) is similar to Y.
Visualization: DRPR can be used for visualization since many models (e.g., usual neural networks)
have probabilistic interpretations w.r.t. the k training categories. In our visualization task, the matrices
M and π are not provided, whereas the target matrix Y is given. By using the optimality conditions
3
Published as a conference paper at ICLR 2019
Algorithm 1 Dimensionality Reduction of Probabilistic Representations (DRPR)
input : Set of training examples (e.g., images) in X and their target probability scores (e.g., classification scores
w.r.t. k training categories), nonlinear mapping gθ parameterized by parameters θ, number of iterations t
1:	for iteration 1 to t do
2:	Randomly sample n training examples xι, •…,Xn ∈ X and create target assignment matrix Y ∈ Yn ×k
containing the target probability scores yι, ∙∙∙ , Nn (i.e., Y = [yι, •…,yn]> ∈ Yn×k)
3:	Create matrix F — [fι,…，fn]> ∈ Vn such that ∀i, f = gθ(xi)
4:	Create matrix of centers M — diag(Y>1n)-1Y>F and prior vector π — nY>1n
5:	Update the parameters θ by performing a gradient descent iteration of ∆n (ψ(F, M, π), Y) (i.e., Eq. (4))
6:	end for
output : nonlinear mapping gθ
in Eq. (2), we can write the desired values of M and π as a function of F and Y : at each iteration,
for some current value of F, the optimal values M = diag(Y>1n)-1Y>F and π = 1Y> 1n are
computed and F is updated via gradient descent. DRPR is illustrated in Algorithm 1 in the case
where F is the output of a model gθ parameterized by θ, e.g., gθ can be a neural network. However,
we represent F as non-parametric embeddings in our visualization experiments to have an equitable
comparison to non-parametric baselines. The learning algorithm then modifies the matrix F at each
iteration. If the priors ∏c are all equal, then the priors are updated in step 4 as follows: π J ɪ 1k.
Zero-shot learning: DRPR can be used to improve zero-shot learning generalization since high-
dimensional models may overfit to training categories and the goal of zero-shot learning is to
generalize to novel categories. In the considered zero-shot learning task, the variable F concatenates
image representations (outputs of a neural network) in the same way as step 3 of Algorithm 1, and
the variable M concatenates category representations extracted from text (outputs of another neural
network). Both F and M are of different nature and are therefore computed as concatenating the
outputs of two distinct neural networks taking different sources as input. To optimize Eq. (4), both
neural networks are trained jointly by fixing the other neural network during backpropagation.
Choice of divergence: In our experiments, we consider the squared Euclidean distance d(fi, μC)=
∣∣fi 一 μck2. However, DRPR can be used with any regular Bregman divergence (Banerjee et al.,
2005). The algorithm is then identical, with the exception of the chosen divergence d to compute the
prediction in Eq. (3).
Convergence and scalability: Although our problem has 3 variables (F, M and π), we use the
optimal properties of the BSCP to write them as a function of each other (see step 4 of Algo 1).
Eq. (4) is then an optimization problem w.r.t. only one variable F. Since the problem is differentiable
wrt F and unconstrained, it is easy to optimize by gradient descent (e.g., back-propagation when
training neural networks). Moreover, our loss is nonnegative, hence lower-bounded. It is worth noting
that we do not apply multiple iterations of the EM algorithm at each gradient descent iteration, as we
first use the optimal properties of BSCP to obtain closed-form formulations of M and π, and then
compute ψ(F, M, π) = Ψ to minimize our problem in Eq. (4). Unlike t-SNE and many iterative DR
problems, the complexity of DRPR is linear in n (instead of quadratic) and linear in k, which makes
it efficient and scalable. Our visualization experiments take less than 5 minutes to do 105 iterations
while t-SNE takes 1 hour to do 1000 iterations. PCA, which has an efficient closed-form solution,
is still much faster. Our approach is thus simple to optimize, hence scalable, and it generalizes to a
large family of Bregman divergences.
Gradient interpretation: We now interpret the gradient of our optimization problem w.r.t. examples.
To simplify its formulation, we consider that all the priors ∏ι,…，∏k are equal and the matrix M
does not depend on F, which is the case in our zero-shot learning task.
When d is the squared Euclidean distance, all the priors are equal and M does not depend on F, the
gradient of Eq. (4) w.r.t. fi is:
kk	k
n X Yicψic[ X exp(Ifi- μck2 -kfi 一 μmk2)][-μc + X ψimμm]	(5)
c=1	m=1	m=1
4
Published as a conference paper at ICLR 2019
One can observe that its magnitude depends on both the target scores Yic ∈ (0, 1) and the predicted
responsibilities Ψic. The gradient tries to make the vector f closer to each centroid 模© while
separating it from all the centroids μmj depending on their predicted scores Ψim ∈ (0,1).
Statistical guarantee: We analyze the statistical property of the algorithm in Appendix A. Theorem 1
provides a finite sample upper bound guarantee on the quality of the minimizer of the empirical
discrepancy of Eq. (4). We show that it is upper bounded by the minimum of the true expected
discrepancy, and an estimation error term of O(n-1/2) (under certain conditions and with high
probability). We defer the detail to the appendix.
3 Related Work
This paper introduces a dimensionality reduction method that represents the relations in a dataset that
has probabilistic interpretation. It can be seen as a metric learning approach.
Metric Learning: The most related approaches try to solve the “supervised” hard clustering problem
(Law et al., 2016) . During training, they are given the target hard assignment matrix Y ∈ {0, 1}n×k
where Yic = 1 if the training example fi has to belong to Cc, and 0 otherwise. The goal is to
learn a representation such that applying a hard clustering algorithm (e.g., kmeans) on the training
dataset will return the desired assignment matrix Y . These approaches can be decomposed in 2
groups: (1) the methods that optimize some regression problem (Lajugie et al., 2014; Law et al.,
2016; 2017) and exploit orthogonal projection properties that hold only in the hard clustering context,
and (2) the methods that use exponential families (Mensink et al., 2012; Snell et al., 2017) to
describe some probability score. In the latter case, the learning problem is written as a multi-
class logistic regression problem where the probability of a category Cc given an example fi is
P(CcIfi) = Pkexpexp(Wδfi,μcμ))and δw is the learned dissimilarity function. DRPR generalizes
those approaches and can also be used for hard clustering learning.1 For instance, Snell et al. (2017)
is a special case of DRPR where π = 11k, Y ∈ {0,1}n×k is a hard assignment matrix and ∆n is
the same ∆n as ours (i.e., ∆n(Ψ, Y) = 1 PZi DκL(yikΨi) by using the convention 0log0 = 0).
Moreover, the optimal value of M is not implicitly written as a function of F and Y in Snell et al.
(2017). The approach in Mensink et al. (2012) is similar to Snell et al. (2017) but only considers
linear models. In summary, both Snell et al. (2017) and Mensink et al. (2012) do not exploit the
BSCP formulation to its full potential as they consider some restricted hard clustering case. DRPR
generalizes these approaches to the soft clustering context at no additional algorithmic complexity.
Dimensionality reduction: Learning models by exploiting soft probability scores predicted by
another pre-trained model as supervision was proposed in Ba & Caruana (2014) for classification. It
was experimentally observed in Ba & Caruana (2014) that using the output of a large pre-trained neural
network as supervision, instead of ground truth labels, improves classification performance of small
neural networks. However, in Ba & Caruana (2014), each dimension of the student representation
describes the confidence score of the model for one training category, which is problematic in contexts
such as zero-shot learning where categories can be added or removed at test time. Our approach
can learn a representation with dimensionality different from the number of training categories. It
can therefore be used in zero-shot learning. Dimensionality reduction with neural networks has
been proposed in unsupervised contexts (e.g., to maximize variance in the latent space (Hinton &
Salakhutdinov, 2006)) and in supervised contexts (e.g., using ground truth labels as supervision
(Salakhutdinov & Hinton, 2007)). Instead, our approach exploits probability scores generated by a
teacher pre-trained model. These scores may be very different from ground truth labels if the pre-
trained model does not generalize well on the dataset. Our approach can then help understand what
was learned by the teacher by visualizing groups of examples the teacher has trouble distinguishing.
1We experimentally show in the supplementary material that DRPR can be learned for hard supervised
clustering when the centers are implicit (see Section C.3).
5
Published as a conference paper at ICLR 2019
Original 3D dataset t-SNE using original dataset as input
(a)
t-SNE using soft clustering scores as input
(c)
OUr 2D representation
-2	-1	0	1	2	3
(d)
Figure 2: (a) Original 3-dimensional dataset containing 6 clusters (one color per cluster); (b) 2D
representation obtained with t-SNE by exploiting the original 3D representation as input; (c) 2D
representation obtained with t-SNE by exploiting soft probability scores w.r.t. the 6 clusters; (d) 2D
representation obtained by our method by exploiting using the same supervision as (c). The relative
inter-cluster distances of the original dataset are preserved with our approach, unlike t-SNE.
4	Experiments
We evaluate the relevance of our method in two types of experiments. The first learns low-dimensional
representations for visualization to better interpret pre-trained deep models. The second experiment
exploits the probability scores generated by a pre-trained classifier in the zero-shot learning context;
these probability scores are used as supervision to improve performance on novel categories.
4.1	Visualization
Interpreting deep models is a difficult task, and one of the most common tools to solve that task is
visualization. Representations are most often visualized with t-SNE which does not account for the
probabilistic interpretation of the learned models. We propose to exploit probability classification
scores as input of our dimensionality reduction framework. In the visualization experiments of this
section, DRPR learns non-parametric low-dimensional embeddings (i.e. our representations are not
outputs of neural networks but vectors) as done by non-parametric baselines. Nonetheless, DRPR
can also be learned with neural networks (e.g., as done in Section 4.2).
Toy dataset: As an illustrative toy experiment, we compare the behavior of t-SNE and DRPR
when applied to reduce a simple artificial 3-dimensional dataset as 2D representations. The 3D
dataset illustrated in Fig. 2 (a) contains k = 6 clusters, each generated by a Gaussian model and
containing 1,000 artificial points. To generate target soft clustering probability scores in the 3D space,
we compute the relative distances of the examples to the different cluster centers and normalize
them to obtain (probability) responsibilities as done in Eq. (3). In detail, let us note the original
3-dimensional dataset X = [xι, •一，Xn]> ∈ Rn×3 (where n is the number of examples) plotted in
Fig. 2 (a). The target soft assignment matrix Y ∈ Yn×k where k = 6 is constructed by computing
Yic = Pkexp(-kxi-ccJ )∣∣2∖ where Cc ∈ R3 is the center of the c-th Gaussian model (defined by its
e=1 exp(-kxi -ce k )
color) in the original space, and priors are equal. We plot in Fig. 2 the 2D representation of our model
when using Y as input/target, and the t-SNE representations obtained when using the original dataset
X as input in Fig. 2 (b) and Y as input in Fig. 2 (c). Two observations can be made from Fig. 2:
(1) the global structure of the original dataset is better preserved with DRPR than with t-SNE; (2)
DRPR satisfies the relative distances between the different clusters better than t-SNE since DRPR
tries to preserve the relative responsibilities of the different clusters. We quantitatively evaluate these
two observations in the following. It is also worth noting that it is known that distances between
clusters obtained with t-SNE may not be meaningful (Wattenberg et al., 2016) as t-SNE preserves
local neighborhood instead of global similarities.
In the following (i.e. in Fig. 4 and tables of results), we only consider the case where t-SNE takes as
input the logits (i.e., classification scores before the softmax operation) instead of probability scores
since the latter case returns bad artifacts such as the one in Fig. 2 (c). Other examples of bad artifacts
obtained with t-SNE exploiting probability scores are provided in the supplementary material. We
also provide in the supplementary material the visualizations obtained by t-SNE when replacing the
'2-norm in the input space by the KL-divergence and the Jensen-Shannon divergence to compare
probabilistic representations that DRPR uses as input. These lead to worse visualizations than Fig 2
(b) in terms of preserving original clusters and their inter-cluster distances.
6
Published as a conference paper at ICLR 2019
TSINM LTS
UO^aonpa! uIyP 一 UO^aonpa1日汨
PCA	0	0.1	1.3	2.4	8.5	14.1
LLE	0	0.1	0.2	1.8	4.1	8.6
ISOMAP	0.5	1.0	2.0	3.3	10.4	16.7
t-SNE	5.2	7.4	9.8	12.1	20.4	27.7
DRPR (Ours)	7.0	9.4	13.4	16.3	24.5	30.3
OlNVdIU
Table 1: NPR performance (in %) for different values of κ
κ	1	2	5	10	50	100	κ	1	2	5	10	50	100
logits	6.3	8.7	12.4	16.0	28.2	36.5	logits	1.7	2.8	4.8	6.9	17.0	24.7
PCA	0.1	0.2	0.3	0.8	3.2	5.9
LLE	1.3	1.6	3.4	4.2	9.8	15.6
ISOMAP	0.2	0.3	0.6	1.0	4.1	7.5
t-SNE	1.9	2.7	4.1	5.6	12.8	19.5
DRPR	8.4	10.2	13.2	15.2	21.1	26.6
logits	5.9	8.4	11.8	14.8	25.5	32.4
PCA
LLE
ISOMAP
t-SNE
DRPR (Ours)
1419
0.0.0.5.
367.1
1.2.1.12
760.6
0.1.1.10
4.3.0.7
8.111026
logits 2.5	4.1	6.3	8.5	15.9	19.3
PCA	0	0.1	0.2	0.4	1.6	2.8
LLE	0	0	0	0.1	0.2	1.4
ISOMAP	0	0.1	0.4	0.6	2.3	3.9
t-SNE	2.4	3.7	5.0	6.5	13.4	16.1
DRPR	9.1	12.6	17.9	24.0	52.7	58.6
Quantitative evaluation metrics: We quantify the
relevance of our approach with the two following
evaluation metrics: (1) an adaptation of the Neigh-
borhood Preservation Ratio (NPR) (Van der Maaten
& Hinton, 2012): for each image i, it counts the
ratio of κ nearest neighbors of i (i.e. that have the
closest probability scores w.r.t. the KL divergence)
that are also in the set of κ nearest neighbors in the
learned low-dimensional space (w.r.t. the Euclidean
Table 2: CDPR performance (in %)
	model	MNIST	STL	CIFAR10	CIFAR100
	logits	81.5	80.6	79.9	67.9
dim reduction	PCA	69.1	66.9	67.4	56.4
	LLE	53.9	55.3	56.5	53.8
	ISOMAP	67.9	70.1	69.5	58.7
	t-SNE	52.8	65.9	66.5	60.4
	DRPR (ours)	76.7	70.5	70.0	69.0
distance). It is averaged over all images i. This metric evaluates how much images that have similar
probability scores are close to each other with the student representation. (2) Clustering Distance
Preservation Ratio (CDPR): we randomly sample 105 triplets of images (i, i+, i-) such that the
3 images all belong to different categories and i has closer probability score to i+ than to i- w.r.t.
the KL divergence. The metric counts the percentage of times that the learned representation of i
is closer to i+ than to i- w.r.t. the Euclidean distance in the low-dimensional representation. This
evaluates how well inter-cluster distances are preserved.
We evaluate our approach on the test sets of the MNIST (LeCun et al., 1998), STL (Coates et al.,
2011), CIFAR 10 and CIFAR 100 (Krizhevsky & Hinton, 2009) datasets with pre-trained models that
are publicly available and optimized for cross entropy.2 The dimensionality of the high-dimensional
representations is equal to the number of categories in the respective datasets (i.e. 10 except for
CIFAR 100 that contains 100 categories). Our goal is to visualize the teacher representations
with 2-dimensional representations by using their probability scores as target, not the ground truth
labels. Quantitative comparisons with standard visualization techniques such as t-SNE, ISOMAP
(Tenenbaum et al., 2000), Locally Linear Embedding (LLE) (Roweis & Saul, 2000) and PCA using
the 2 leading dimensions are provided in Tables 1 and 2. We also report the scores obtained with the
logit representations which are not dimensionality reduction representations but provide an estimate
of the behavior of the original dataset. DRPR outperforms the dimensionality reduction baselines
w.r.t. both evaluation metrics and is competitive with the logit representation. Examples that have
similar probability-based representations are closer with our approach than with other dimensionality
reduction baselines. DRPR also better preserves inter-cluster distances. It is worth noting that DRPR
exploits as much supervision as the “unsupervised” visualization baselines. Indeed, all the compared
methods use as input the same source of supervision which is included in the (classifier output)
representations given as input.
Qualitative results: Visualizations of pre-trained models obtained with DRPR and t-SNE are
illustrated in Fig. 4 for MNIST and STL. The visualizations for CIFAR 10 and 100 are in the
supplementary material. DRPR representations contain spiked groups at the corners to better reflect
examples that have high confidence scores for one category. Indeed, an example in a spike at the
corner of a figure has a soft assignment score w.r.t. its closest center close to 1. This means that the
pre-trained model has very high confidence to assign the example to the corresponding category (see
2We use the pre-trained models available at https://github.com/aaron- xichen/
pytorch-playground.
7
Published as a conference paper at ICLR 2019
Figure 4: MNIST (left half) and STL (right half) representations of DRPR (left) and t-SNE (right)
illustration in Fig. 3). Examples that are between multiple clusters (usually in the middle of figures)
are those that are harder to classify by the model. Detailed visualizations and analysis are provided in
the supplementary material.
One can observe that representations obtained with
DRPR reflect the semantic structure between cate-
gories. On MNIST, categories that contain a curve
at the bottom of the digit (i.e., 0, 3, 5, 6, 8 and 9) are
in the bottom of Fig. 4 (left); some pairs of digits
that are often hard to differentiate by classifiers (i.e.,
4 and 9, 1 and 7, 3 and 8) are also adjacent. On STL
and CIFAR 10, animal categories are illustrated on
the right whereas machines are on the left. Seman-
tically close categories such as airplane and bird,
or car and truck are also adjacent in the figures.
Figure 3: Level sets representing responsibil-
ities of the green cluster whose center is the
green circle. Grey circles are the centers of
other clusters. The responsibility of the green
cluster increases for the examples that are lo-
cated in the spike at the right side of the figure.
One main difference between the DRPR and t-SNE
representations for STL is the distance between the
clusters ship and airplane. These categories are ac-
tually hard for the model to differentiate since they contain blue backgrounds and relatively similar
objects. In particular, the STL airplane category contains many images of seaplanes lying on the sea
and can then be mistaken for ships. This ambiguity between both categories is not observed on the
t-SNE representation.
Due to lack of space, a detailed analysis for the CIFAR 100 and STL datasets is available in the
supplementary material. A summary of the results is that categories that belong to the same superclass
(e.g., categories hamster, mouse, rabbit, shrew, squirrel are part of the superclass small mammals) are
grouped together with DRPR. The DRPR visualization also reflects some semantical structure: plants
and insects are on the top left; animals are on the bottom left and categories on the right are outdoor
categories. Medium mammals are also represented between small mammals and large carnivores.
In conclusion, the quantitative results show that the representations of DRPR are meaningful since
they better preserve the cluster structure and allow observation of ambiguities between categories.
4.2	Zero-shot learning
We consider the same zero-shot learning scenario as Reed et al. (2016) and Snell et al. (2017). In
particular, we test our approach on the same datasets and splits as them. The main goal is to learn two
mappings, one for image representations and one for category representations, in a common space V .
The latter mapping takes some category descriptions as input (e.g., from text description or visual
attributes). Image representations are then learned so that they are closer to the representative of their
category than to the representative of any other category. At test time, categories that were unseen
during training are considered, and their representative is obtained by using the second mapping. An
image is assigned to the category with closest representative.
Training datasets: We use the medium-scaled Caltech-UCSD Birds (CUB) dataset (Welinder et al.,
2010) and Oxford Flowers-102 (Flowers) dataset (Nilsback & Zisserman, 2008). CUB contains
11,788 bird images from 200 different species categories split into disjoint sets: 100 categories
for training, 50 for validation and 50 for test. Flowers contains 8,189 flower images from 102
different species categories: 62 categories are used for training, 20 for validation and 20 for test. To
represent images, Reed et al. (2016) train a GoogLeNet (Szegedy et al., 2015) model whose output
dimensionality is 1,024. For each category, Reed et al. (2016) extract some text annotations from
8
Published as a conference paper at ICLR 2019
Table 3: Test accuracy on CUB		Table 4: Test accuracy on Flowers dataset	
Method	Accuracy	Method	Accuracy
TMV-HLP Oquab et al. (2014)	47.9 %	DS-SJE Reed et al. (2016) (Char CNN)	47.3 %
SJE Akata et al. (2015)	50.1%	DS-SJE Reed et al. (2016) (Bag-of-words)	57.7 %
DS-SJE Reed et al. (2016) (Bag-of-words)	44.1 %	DS-SJE Reed et al. (2016) (Word CNN)	60.7 %
DS-SJE Reed et al. (2016) (Char CNN-RNN)	54.0 %	DS-SJE Reed et al. (2016) (Char CNN-RNN) [reported]	63.7 %
Ziming & Saligrama Zhang & Saligrama (2016)	55.3 %	DS-SJE Reed et al. (2016) (Char CNN-RNN) [publicly available]	59.6 %
DS-SJE Reed et al. (2016) (Word CNN-RNN)	56.8 %	DS-SJE Reed et al. (2016) (Word CNN-RNN)	65.6 %
Prototypical Networks Snell et al. (2017)	58.3 %	Prototypical Networks Snell et al. (2017)	63.9 %
Ours - using DS-SJE (Char CNN-RNN) as supervision	57.7 %	Ours - using DS-SJE (Char CNN-RNN) [publicly available]	62.4 %
Ours - using Prototypical Networks as supervision	60.3 %	Ours - using Prototypical Networks as supervision	68.2 %
which they learn a representative vector (e.g., based on Char CNN-RNN (Zhang et al., 2015)). The
image representations of examples and the text representations of categories are learned jointly so
that each image is more similar to the representative vector of its own category than to any other.
Supervision: We now describe how we generate the supervision/target of our model from the models
pre-trained by (Reed et al., 2016; Snell et al., 2017) and provided by their respective authors.
Once its training is over, Prototypical Network (Snell et al., 2017) represents each image i by some
vector f and each category C by some vector μ口 By concatenating the different vectors into matrices
F = [fι,…，fn]> and M = [μ「…，μ卜]> and formulating Tn = ɪ 1k, DRPR considers the soft
assignment matrix Y1 = ψ(F, Mf, ∏) ∈ Yn×k as target (i.e. supervision).
In the case of Reed et al. (2016), we consider the same preprocessing as Snell et al. (2017). Each
image i is represented by some vector £, each category C is represented by some vector μc (that
is `2 -normalized in Snell et al. 2017). The target soft assignment matrix of DRPR is then Y2 =
ψ(F, M, k 1k) in this case.
Trained models: In the model that Snell et al. (2017) provide and that obtains 58.3% accuracy
on CUB, ProtoNet trains two models that take as arguments the representations learned by Reed
et al. (2016). They train one model g% for images such that ∀i, f = f^ɪ (fi), and one model g%
for text representative vectors such that ∀c, μ C = g^ (μj. Following Snell et al. (2017), We train
two (neural network) models: gθ1 for images, and gθ2 for categories. Both of them take as input the
image and category representations used to create the target soft assignment matrix (i.e., we take the
representations learned by Reed et al. (2016) when its probability scores Y2 are used as supervision,
and the representations learned by Snell et al. (2017) otherwise). In this context, we alternately
optimize gθ1 by fixing M (which depends on gθ2) and optimize gθ2 by fixing F (which depends on
gθ1 ).
Implementation details: We consider that the learned models gθ1 and gθ2 have the same architecture
and are multilayer perceptrons (MLP) with tanh activation functions. The number of hidden layers
λ ∈ {0, 1, 2} and output dimensionality d are hyperparameters cross-validated from the accuracy on
the validation set. More details on their architecture can be found in the supplementary material.
Results: We report the performance of our approach on the test categories of the CUB and Flowers
datasets in Tables 3 and 4, respectively. The performance is measured as the average classification
accuracy across all unseen classes. We use DS-SJE (Char CNN-RNN) and Prototypical Networks
as supervision for our model because they are the only approaches whose pre-trained models are
publicly available. Our approach obtains state-of-the-art results on both CUB and Flowers datasets
by significantly improving the classification performance of the different classifiers. For instance, it
improves the scores of 63.9% obtained by ProtoNet of Flowers up to 68.2%. In general, it improves
zero-shot learning performance of the different classifiers by 2% to 4.3%.
Impact of dimensionality: We report in the supplementary material the performance of our model
on both the validation and test sets using different numbers of hidden layers, and ranging the output
dimensionality d from 16 to the dimensionality e of the input representations. Except for linear
models (i.e. λ = 0), reducing the dimensionality improves generalization. This shows that the zero-
shot learning performance of a given model can be significantly improved by taking its prediction
scores as supervision of our model. To study the impact of the dimensionality reduction generated
DRPR, we also ran the codes of Reed et al. (2016); Snell et al. (2017) by learning representations
9
Published as a conference paper at ICLR 2019
with dimensionality smaller than those provided (using the same ranges as those in the tables of the
supp. material). This decreased their generalization performance. Therefore, directly learning a
low-dimensional representation is not a sufficient condition to generalize well. Our framework that
learns representations so that examples with similar ambiguities (i.e. similar teacher predictions) are
close to each other acts as a semantic regularizer. This is suggested by the fact that test accuracy is
improved with DRPR even when e = d (as long as the MLPs contain hidden layers).
It is worth mentioning that one test category of the CUB dataset (Indigo bunting) belongs to the
ImageNet dataset (Deng et al., 2009) that was used to pretrain GoogLeNet. By using the train/val/test
category splits proposed by Xian et al., we did not observe a change of performance of the different
models on CUB.
5	Conclusion
We have proposed a dimensionality reduction approach such that the soft clustering scores obtained
in the low-dimensional space are similar to those given as input. We experimentally show that our
approach improves generalization performance in zero-shot learning on challenging datasets. It can
also be used to complement t-SNE, as a visualization tool to better understand learned models. In
particular, we show that we can give a soft clustering interpretation to models that have probabilistic
interpretations. Real-world applications that can be used with DRPR include distillation. For
instance, when the teacher model is too large to store on a device with small memory (e.g., mobile
phone), the student model which has a smaller memory footprint is used instead. Low-dimensional
representations can also speed up retrieval tasks. Future work includes applying our approach to the
task of distillation in the standard classification task where training categories are also test categories.
6	Acknowledgments
We thank Fartash Faghri and the anonymous reviewers for their helpful comments on early versions
of this manuscript. This work was supported by Samsung and the Intelligence Advanced Research
Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC) contract
number D16PC00003. The U.S. Government is authorized to reproduce and distribute reprints for
Governmental purposes notwithstanding any copyright annotation thereon. AMF acknowledges
funding from the Canada CIFAR AI Chairs Program.
Disclaimer: The views and conclusions contained herein are those of the authors and should not
be interpreted as necessarily representing the official policies or endorsements, either expressed or
implied, of IARPA, DoI/IBC, or the U.S. Government.
References
Zeynep Akata, Scott Reed, Daniel Walter, Honglak Lee, and Bernt Schiele. Evaluation of output
embeddings for fine-grained image classification. In Computer Vision and Pattern Recognition
(CVPR), 2015 IEEE Conference on, pp. 2927-2936. IEEE, 2015.
Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In Advances in Neural Information
Processing Systems (NeurIPS - 27), pp. 2654-2662. 2014.
Arindam Banerjee, Srujana Merugu, Inderjit S Dhillon, and Joydeep Ghosh. Clustering with Bregman
divergences. Journal of Machine Learning Research (JMLR), 6(Oct):1705-1749, 2005.
Peter L. Bartlett and Shahar Mendelson. Rademacher and Gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research (JMLR), 3:463-482, 2002.
Peter L. Bartlett, Olivier Bousquet, and Shahar Mendelson. Local Rademacher complexities. The
Annals of Statistics, 33(4):1497-1537, 2005.
Lev M Bregman. The relaxation method of finding the common point of convex sets and its
application to the solution of problems in convex programming. USSR computational mathematics
and mathematical physics, 7(3):200-217, 1967.
10
Published as a conference paper at ICLR 2019
Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised
feature learning. In Proceedings of the fourteenth international conference on artificial intelligence
and statistics,pp. 215-223, 2011.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009.
IEEE Conference on, pp. 248-255. IEEE, 2009.
Paul Doukhan. Mixing: Properties and Examples, volume 85 of Lecture Notes in Statistics. Springer-
Verlag, Berlin, 1994.
Amir-massoud Farahmand and Csaba SzePesv疝i. Regularized least-squares regression: Learning
from a β -mixing sequence. Journal of Statistical Planning and Inference, 142(2):493 - 505, 2012.
Evarist Gine and Richard Nickl. Mathematical Foundations OfInfinite-Dimensional Statistical Models.
Cambridge University Press, 2015.
Ldszl6 Gyorfi, Michael Kohler, Adam Krzyzak, and Harro Walk. A Distribution-Free Theory of
Nonparametric Regression. Springer Verlag, New York, 2002.
Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural
networks. science, 313(5786):504-507, 2006.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.
Technical report, University of Toronto, 2009.
Remi Lajugie, Sylvain Arlot, and Francis Bach. Large-margin metric learning for constrained
partitioning problems. In ICML, pp. 297-305, 2014.
Marc T. Law, Yaoliang Yu, Matthieu Cord, and Eric P. Xing. Closed-form training of Mahalanobis
distance for supervised clustering. In CVPR. IEEE, 2016.
Marc T. Law, Raquel Urtasun, and Richard S. Zemel. Deep spectral clustering learning. In Interna-
tional Conference on Machine Learning, pp. 1985-1994, 2017.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic
segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 3431-3440, 2015.
Ron Meir. Nonparametric time series prediction through adaptive model selection. Machine Learning,
39(1):5-34, 2000.
Thomas Mensink, Jakob Verbeek, Florent Perronnin, and Gabriela Csurka. Metric learning for large
scale image classification: Generalizing to new classes at near-zero cost. Computer ViSion-ECCv
2012, pp. 488-501, 2012.
Mehryar Mohri and Afshin Rostamizadeh. Rademacher complexity bounds for non-i.i.d. processes.
In Advances in Neural Information Processing Systems 21, pp. 1097-1104. Curran Associates,
Inc., 2009.
Mehryar Mohri and Afshin Rostamizadeh. Stability bounds for stationary φ-mixing and β-mixing
processes. Journal of Machine Learning Research (JMLR), 11:789-814, 2010. ISSN 1532-4435.
Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number
of classes. In Proceedings of the Indian Conference on Computer Vision, Graphics and Image
Processing, Dec 2008.
Maxime Oquab, Leon Bottou, Ivan Laptev, and Josef Sivic. Learning and transferring mid-level
image representations using convolutional neural networks. In Computer Vision and Pattern
Recognition (CVPR), 2014 IEEE Conference on, pp. 1717-1724. IEEE, 2014.
11
Published as a conference paper at ICLR 2019
Scott Reed, Zeynep Akata, Honglak Lee, and Bernt Schiele. Learning deep representations of
fine-grained visual descriptions. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 49-58, 2016.
Sam T Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear embedding.
Science, 290(5500):2323-2326, 2000.
Ruslan Salakhutdinov and Geoff Hinton. Learning a nonlinear embedding by preserving class
neighbourhood structure. In Artificial Intelligence and Statistics, pp. 412-419, 2007.
Alexander Schulz, Andrej Gisbrecht, and Barbara Hammer. Using discriminative dimensionality
reduction to visualize classifiers. Neural Processing Letters, 42(1):27-54, 2015.
Jake Snell, Kevin Swersky, and Richard S Zemel. Prototypical networks for few-shot learning. In
Advances in Neural Information Processing Systems (NeurIPS - 30). 2017.
Ingo Steinwart and Andreas Christmann. Support Vector Machines. Springer, 2008.
Ingo Steinwart and Andreas Christmann. Fast learning from non-i.i.d. observations. In Advances in
Neural Information Processing Systems (NeurIPS - 22), pp. 1768-1776. Curran Associates, Inc.,
2009.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1-9, 2015.
Joshua B Tenenbaum, Vin De Silva, and John C Langford. A global geometric framework for
nonlinear dimensionality reduction. Science, 290(5500):2319-2323, 2000.
Sara A. van de Geer. Empirical Processes in M-Estimation. Cambridge University Press, 2000.
Laurens Van der Maaten. Accelerating t-SNE using tree-based algorithms. Journal of Machine
Learning Research (JMLR), 15(1):3221-3245, 2014.
Laurens Van der Maaten and Geoffrey Hinton. Visualizing non-metric similarities in multiple maps.
Machine learning, 87(1):33-55, 2012.
Martin Wattenberg, Fernanda Viegas, and Ian Johnson. HoW to use t-SNE effectively. Distill, 2016.
doi: 10.23915/distill.00002. URL http://distill.pub/2016/misread-tsne.
P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona. Caltech-UCSD
Birds 200. Technical Report CNS-TR-2010-001, California Institute of Technology, 2010.
Yongqin Xian, Bernt Schiele, and Zeynep Akata.
Saining Xie and ZhuoWen Tu. Holistically-nested edge detection. In Proceedings of the IEEE
international conference on computer vision, pp. 1395-1403, 2015.
Bin Yu. Rates of convergence for empirical processes of stationary mixing sequences. The Annals of
Probability, 22(1):94-116, January 1994.
Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional netWorks for text
classification. In Advances in neural information processing systems, pp. 649-657, 2015.
Ziming Zhang and Venkatesh Saligrama. Zero-shot recognition via structured prediction. In European
conference on computer vision, pp. 533-548. Springer, 2016.
12
Published as a conference paper at ICLR 2019
A Theoretical Analysis
This section provides a statistical guarantee for the algorithm presented in Section 2.2. We show
that the minimizer of the empirical discrepancy (4), which uses a finite number of data points
xι,…，Xn ∈ X, is close to the minimizer of the true expected discrepancy measure, to be defined
shortly. Let us define the setup here.
We are given data points Dn = {X1, . . . , Xn}.3 We suppose that each Xi ∈ X is independent and
identically distributed (i.i.d.) with the distribution ν ∈ M(X), where M(X) is the space of all
probability distributions defined over X. The teacher is a fixed function φ = [φ(∙; 1),..., φ(∙; k)]
that maps points in X to a k-dimensional simplex, and provides the target probability distributions.
That is, the target yi for Xi is computed as yi = φ(Xi).
Consider a function space G whose domain is X and its range is a subset of Rd . This function
space might be represented by a DNN, but we do not make such an assumption in our statistical
analysis. Given a function g ∈ G (called gθ in the main article) and the number of clusters k, we
define ψg (x) = [ψg (x; 1), . . . , ψg (x; k)] as
Πcexp (- kg(x) - μc(g)k2)
ψg(x； C) = —k-------------7------------------K,
Pb=I πb exP -- kg(x) - 〃b(g)k2)
with the cluster centres
E[Φc(X )g(X)] = ∕φc(X)g(X)dV(X)
μc(g) =	E[Φc(X)]	= R φc(χ)dν(x)
and the priors
πc
φc(X)dν(X),
for c ∈ {1, . . . , k} (cf. Section 2.1). Note that ψg(X) defines a k-dimensional probability distribution,
and μc(g) is a mapping of the function g to a point in Rd.
Similarly, given Dn, we define the empirical cluster centres
μc(g)
1 Pn=I Φc(Xi)g(χj
n Pn=1 Φc(Xi)
Λ .1	-∣∙	7	/	∖	l^7∕r∖	7	/	7 ∖T
and their corresponding ψg (X) = [ψg (X; 1), . . . , ψg (X; k)]
ʌ	πc exP (-kg(X) - μc(g)k2)
ψg (x； C) = —k---------7----------------2V .
Pb=I πbexP (-kg(X) - μb(g)k2)
Note that here for simplicity of analysis, we assume that the priors πc are exact, and not estimated
from data.
The student’s goal is to find a g such that ψg is close to φ. The closeness of the student to the teacher
is defined based on their KL divergence. Specifically, Algorithm 1 minimizes the distorted empirical
discrepancy (4), which can be written as4
1n
∆n(Ψ,Φ) = n ∑ KL(φ(Xi)∣∣ψ(Xi)),
where Xis are from dataset Dn. Notice that the distorted empirical discrepancy ∆n(ψ, φ) is defined
based on ψ, which uses the empirical centres ^c, instead of the expected centres μ° We also define an
empirical discrepancy w.r.t. the true μc as ∆n(ψ, φ). This quantity is not accessible to the algorithm.
3Here we use Xi instead of xi in order to emphasize their randomness.
4Algorithm 1 does not explicitly specify what function generates y; we need to specify it here, so we use
φ(x) for that purpose.
13
Published as a conference paper at ICLR 2019
We evaluate the quality of g, and its corresponding ψg, based on how well, in average, it performs on
new points x ∈ X. We consider the expected KL divergence between ψ and φ w.r.t. distribution ν as
the measure of performance. Therefore, the discrepancy is
∆(ψ,φ) = Ex” [KL(φ(X)∣∣ψ(X))]
/ KL(O3||砂(X))dv(x).
The output of Algorithm 1 is the minimizer5 of ∆n (ψ, φ), which We denote by g
g — argmin ∆n (ψg ,φ).	(6)
g∈G
We also define the minimizer of the discrepancy by g*:
g* — argmin ∆(Ψg ,Φ).
g∈G
We would like to compare the performance of g when evaluated according to discrepancy, that is
∆(ψg,φ), and compare it with ∆(ψg*, φ).
Before stating our results, we enlist our assumptions. We shall remark on them as we introduce.
Assumption A1 (Samples) The dataset Dn = {Xi}in=1 consists of i.i.d. samples drawn from ν(X).
The i.i.d. assumption simplifies the analysis. With extra effort, one can provide similar results for
some classes of dependent processes too. For example, if the dependent process comes from a time
series and it gradually “forgets” its past, one may still obtain similar statistical guarantees. Forgetting
can be formalized through the notion of “mixing” of the underlying stochastic process (Doukhan,
1994). One can then provide statistical guarantees for learning algorithms under various mixing
conditions (Yu, 1994; Meir, 2000; Steinwart & Christmann, 2009; Mohri & Rostamizadeh, 2009;
2010; Farahmand & Szepesvdri, 2012).
Assumption A2 (Bounded Function Space) Functions in G are L-bounded for some L > 0, i.e.,
g(x) ∈ [-L, L]d for any g ∈ G and x ∈ X.
This is a mild and realistic assumption on the function space G, and is mainly here to simplify some
steps of the analysis.
Assumption A3 (Teacher) Part I) The output of the teacher φ is a probability distribution, i.e., for
any x ∈ X and c = {1, . . . , k}, we have φ(x; c) ≥ 0 and Pck=1 φ(x; c) = 1.
Part II) We assume that πc = E [φc(X)] is bounded away from zero for all c. We set πmin = minc πc.
This first part of the assumption explicitly expresses the fact that the algorithm expects to receive a
probability distribution from the teacher. If it does not, for example if φ(x; c) is negative for some x
and c but we treat it as a probability in the calculation of the KL divergence, the algorithm would not
be well-defined.
The second part of this assumption requires that prior probability for each cluster is bounded away
from zero, and has the probability at least πmin. This is a technical assumption used by the proof
technique; it might be possible that one can relax this assumption.
We need to make some assumptions about the function space G and its complexity, i.e., capacity. We
use covering number (and its logarithm, i.e., metric entropy) as the characterizer of the complexity.
The covering number at resolution ε is the minimum number of balls with radius ε required to cover
the space M according to a particular metric. We use N(ε, G, |卜||) to denote the covering number
of G w.r.t. the norm ∣∣∙k, which we shall explicitly specify. As ε decreases, the covering number
increases (or more accurately, the covering number is non-decreasing). For example, the covering
number for a p-dimensional linear function approximator with constraint on the magnitude of its
5Since our focus is on the statistical analysis, we ignore the issue of the quality of the optimization procedure,
and the fact that one may not find a global minimizer of ∆n(ψg, φ) within G for a function space described by a
DNN.
14
Published as a conference paper at ICLR 2019
functions behaves like O(εp). A similar result holds when the subgraphs of a function space has
a VC-dimension p. Function spaces whose covering number grows faster are more complex, and
estimating a function within them is more difficult. This leads to larger estimation error. On the other
hand, those function spaces are often (but not always) show better function approximation properties
too. For more detail and many examples, refer to Gyorfi et al. (2002); van de Geer (2000); Steinwart
& Christmann (2008); Gine & Nickl (2015).
Let us define a norm for the function space G :
kgk∞,2 = xsup kg(x)k2
This is a mixed-norm where We compute the '2-norm for each g(x) ∈ Rd, and then take the supremum
norm over the resulting '2-norm. We use this norm to characterize the covering number of G.
Assumption A4 (Metric Entropy) There exists constants B > 0 and 0 < α < 1 such that for any
ε, the following covering number (i.e., metric entropy) condition is satisfied:
log N (ε, G,k∙k∞,2) ≤ (B )2α.
The logarithm of the covering number of G is O(*).It grows much faster than the metric entropy
of linear models, which is O(Plog(ɪ)). This behaviour is suitable to capture the complexity of large
function spaces such as the Sobolev space Wk (Rd) and many reproducing kernel Hilbert spaces
(RKHS).6 Note that we use a mixed-norm to define the covering number. The use of supremum norm
in the definition might be considered conservative. Using a more relaxed norm, for example based on
the empirical Lp (PX1:n)-norm for some 1 ≤ p < ∞, is an open technical question for future work.
Finally let us define the pointwise loss function
I(X; g) = KL(O(X)M(X))
k
φ(X; c) log
c=1
φ(x; C)
ψg(x； C)
(7)
Notice that ∆n(ψg, φ) = ɪ £乙 l(Xi; g) and ∆(ψg, φ) = EV [l(X; g)]. We define the following
function space:
L = { X 7→ l(X, g) : g ∈ G } .	(8)
We also define the entropy integral (Dudley integral)
J (ε) = ( Jlog2N (u, G,k∙k∞j du.	(9)
We are now ready to state the main theoretical result of this section.
Theorem 1. Suppose that Assumptions A1, A2, andA3 hold. Consider g obtained by solving (6).
There exists a finite C1 > 0 such that for any δ > 0, with probability at least 1 - δ, we have
64√2L√d J (16dL2+2√0g2 k )	∕loσ(1 /δ)
∆(ψg, φ) ≤ min ∆(ψg,φ) +--------------√=--------------+ ci (dL2 + log(k)) J--------+
g∈G	n	n
C2L√d
πmin
dJ (2L)
√n
+ Ljd ln(k∕δ)
n
Furthermore, if the metric entropy satisfies Assumption A4, there exist constants C2 , C4 > 0 and a
function C3(α) > 0, which depends only on α, such that for any δ > 0, with probability at least 1 - δ,
we have
∆(ψg,φ) ≤ min∆(ψg,φ) + C2 1 + c3(α)
g∈G
BL√d !”
dL2 + log k )
IdL2 + log(k)) Jlogn画+
c4
π min
-L2-αB αd3/2
1-α
+ ^2d∖∖∕lθg(k∕δ).
n
6 For Wk(Rd) = Wk,2(Rd), the Sobolev space defined w.r.t. the L2-norm of the weak derivatives, we can
set α = 2dk, see e.g., Lemma 20.6 of Gyorfi et al. (2002).
15
Published as a conference paper at ICLR 2019
This theorem provides a finite sample error upper bound on the true discrepancy of g, and relates it to
the expressiveness and complexity of the function space G, the number of samples n, and some other
properties. The term ming∈G ∆(ψg, φ) = ∆(ψg*, φ) is the function approximation error, and reflects
the expressiveness of G . This is the minimum achievable error given the function space G . The other
terms in the upper bound correspond to the estimation error caused by having a finite number of data
points.
Let us focus on the second part of the theorem, which is under the particular choice of covering
number according to Assumption A4. In that case, the estimation error shows n-1/2 dependence
on the number of samples, and hence decreases as we have more training data. We observe that the
upper bound increases as the range L of the function space G, the dimension d of the low-dimensional
space, and the number of clusters k increases.
The effect of using the distorted empirical discrepancy ∆n(ψg, φ) instead of ∆n(ψg, φ) shows itself
in the last term, i.e., the term with the constant multiplier of c2∏L^ in the first part of the theorem and
∏C4- in the second part. Note the appearance of ∏mi∩ in the denominator of the last term. This causes
a large error if one of the clusters is improbable. Also observe that if We have k clusters, ∏min ≤ 1
(equality under the uniform distribution over classes), so we have at least a linear dependence on k in
the upper bound. This dependence might be due to our proof technique; it remains to be seen whether
this can be improved.
Proof. To simplify the notation, we denote ∆n(g) = ∆n(ψg , φ), ∆n (g) = ∆n(ψg, φ), and ∆(g) =
∆(ψg, Φ)∙ We want to relate ∆(g) to ∆(g*), the supremum of the empirical process ∆(g) - ∆n(g)
and the supremum of the distortion of the empirical loss ∆n(g) - ∆n(g). We have the following
relations:
△⑼=∆n(g) + (∆(g)- ∆n(g))
=△ n(g) + (∆n(g) - △ n(^)) + (∆(g) — ∆n(g))
≤ △ n(g*) + (∆n(g) — △ n (g)) + (∆(g) — ∆n(g))
=^n(g*) + (△ n(g*) — ^n(g*)) + (^n(g) — △ n(g) + (△(g) — △n(g))
=△(g*) + (^n(g*) — △(g*)) + (△ n(g*) — ^n(g*)) + (^n(g) — △ n(g)) + (△(g) — △n(g))
≤ △(g*)+2sup |^(g) — ^n(g)∣ +2sup l^n(g) — △ n(g)l .	(10)
g∈G	g∈G
The first inequality is because of the the optimizer property of g, i.e., ^n(g) ≤ '(g) for any g ∈ G,
including g* .
We need to upper bound the supremum of the empirical process, that is supg∈° |^(g) — ^n(g)∣,
and the supremum of the distortion caused by using ψ in minimizing △n instead of ψ, that is
SUPg∈G l^n(g) — △ n(g)∣, cf (6).
Upper Bounding supg∈° |^(g) — ^n(g)∣. We use Lemma 7 in Appendix B.3 in order to upper
bound the supremum of the empirical process, supg∈° |^(g) — ^n(g)∣, which is equivalent to
supι∈L 11 pn=1 l(Xi) — E [l(X)]∣. That lemma, which is originally Theorem 2.1 of Bartlett et al.
(2005), relates the supremum of the empirical process to the Rademacher complexity of L, defined in
the same appendix.
To apply the lemma, we first provide upper bound on l(x) and Var [l(X)]. By assumption, g(x) is
within [—L, L]d. It is not difficult to see that μ0(g) is also within [—L, L]d; for example, see the
proof leading to (19). So
kg(X)- μ°(g)k2 ≤ 4dL2.
We evoke Proposition 4 with the choice of f (x; C) = kg(χ) — μck2 and L0 = 4dL2 to obtain that
l(x; g) ≤ 8dL2 + log2 k.	(11)
16
Published as a conference paper at ICLR 2019
Since l(x; g) is bounded, we have
Var[l(X; g)] ≤ E [l(X; g)2] ≤(8dL2 + log2 k)2 .
By the choice of β = 1, B = (8dL2 + log2 k), and r = (8dL2 + log2 k)2 in Lemma 7, We get that
for any δ1 > 0,
sup
l∈L
1n
-El(Xi)-
n i=1
E [l(X)]∣∣∣ ≤4E[Rn(L)] + (8dL2 + log2 k)
2log(2∕δι) 1
2(8dL2 +log2 k)5lθgMI)
6
(12)
n
n
with probability at least 1 - δ1 .
It remains to provide an upper bound on E [Rn(L)]. This can be done by using Dudley’s integral to
relate the Rademacher complexity of L to the covering number of L. Afterwards, we use Lemma 3
in Appendix A.1 to relate the covering number of L to the covering number of G. We have
diam(L)
√log 2N (u, L, L(Pxi：n)) du
E[Rn(L)]
16dL2+2 log2 k I------------ '
JlOg2N (u, L, k∙k∞) du
16dLi k S (最,G"G du
4√2(8L√d) J (1*6+√°g2 k)
(13)
In the second inequality, We benefit from tWo observations: first, We use l(x) ≤ 8dL2 + log2 k for
any l ∈ L (11) to upper bound diam(L); second, the covering number W.r.t. L2 (PX1:n) can be upper
bounded by the covering number W.r.t. the supremum norm.7
Upper Bounding supg∈g ∣∆n(g) - ∆n(g) ∣. We use Proposition 5 in Appendix A.2, which states
that for any δ2 > 0,
SUp ∣∆n(g) - ∆n(g)∣ ≤
g∈G
cιL√d
πmin
C2L√d
πmin
dJ (2L)
dJ (2L)
+L
+L
dln(k∕δ2)十 ln(dk∕δ2)
dln(k∕δ2)
))
(14)
≤
n
n
n
with probability at least 1 - δ2 .
Plugging (12) and (14) in (10) and using the entropy integral upper bound (13) lead to the desired
result of the first part.
To prove the second part of the theorem, We use logN (ε, G, ∣∣∙∣∣∞ 2) ≤ (B)2α to calculate J(ε),
which results in
Bαε1-α
J(ε) ≤ j-----
1-α
By plugging in ε = 16dL2 + 2 log2 k, we get that
E [Rn(L)] ≤
32√2(BL√d)α (8dL2 + log2 k)1-α
(1 - α)√n
7This version of Dudley's integral is from Theorem 2.3.7 of GinC & Nickl (2015). We use it similar to
the argument in their proof of Theorem 3.5.1 and the comments thereafter. One could also use Theorem A.7
by Bartlett et al. (2005) (originally from Dudley) and note that the upper bound of the integral does not need to
go up to infinity when the function space is bounded in the norm.
17
Published as a conference paper at ICLR 2019
We upper bound J (2L) in (14) to obtain:
1-α
J (2L) ≤
BαL
1-α
1-α
2
After some simplifications these lead to the desired result of the second part.
□
A.1 Some Technical Tools
We develop some technical tools required in the proof of Theorem 1. Proposition 2 provides Lipschitz
constant for some functions that are used in the proof of Lemma 3 to relate the covering number of
the function space L, defined in (8), to that of G. This was a key step of the proof of the theorem.
Proposition 4 provides an upper bound on the magnitude of l(x; f), shortly defined in (16).
We introduce a few more notations to reduce the clutter. Let dg(x; C) = Ilg(X) - μ∕g)k2, so We can
write
ψg (x; C)=	:π exp(-dg E C))	.	C ∈{1,...,k}
Pk=I ∏b exp(-dg (x; b))
For a function f : X × {1, . . . , k} → R, we define
pf(x;C) =	XPf-X)).	C∈{1,...,k}
(15)
We overload the pointwise loss function l(x; g), defined in (7), and define a similar definition for
l(x; f) as follows
l(x; f) = X φ(x; c) log φ(x,C) .
c=1	pf (x; C)
It is clear that with the choice of f = dg , the probability distribution pf is the same as ψg .
The following proposition specifies the Lipschitz properties of dg and l(x; f).
Proposition 2. Suppose that Assumption A3 hold.
(16)
Part I)	Consider g1, g2∈ G and let Assumption A2 hold. For any x ∈ X and C ∈ {1, . . . , k}, we have
∣dgι (x; c) - dg2(x; c)| ≤ 4L√d ∣∣gι(x) - g2(x)k2 + SuP kgι(x0) - g2(x0)k2
x0∈X
Part II)	Consider two functions f1, f2 : X × {1, . . . , k} → R. For any x ∈ X we have
|l(x; fι)- l(x; f2)∣≤ 2 ∣fι(x; ∙) - f2(x; ∙)∣∞ .
Proof. Part I) First notice that for any two vectors u and v, by the Cauchy-Schwarz inequality we
have
∣u∣22 - ∣v∣22 = X(ui2 - vi2) = X(ui - vi)(ui +vi)
ii
jχ (Ui+ vi)2
∣u - v∣2 ∣u + v∣2 .
Consider g1 , g2 ∈ G , and their corresponding dg1 and dg2 . We have
|dgi(x;C)- dg2(x;C)I = NgI(X)-μc(gι)k2 kg2(X)- μc(g2)∣2∣
≤ k(g1 (X) - 〃c(gI)) - (g2(X) - μc(g2))∣2 ×
k(g1 (X) - μc(gI)) + (g2(X) - μc(g2))∣2
≤ [kgl (X)- g2(x)∣2 + k〃c(gI)- μc(g2)∣2] ×
k(gl(x) + g2(X))- (μc(gl) + μc(g2))∣2 .	(17)
18
Published as a conference paper at ICLR 2019
Note that
I∣μc(gι) - μc(g2)k2
IlR φc(XxgI(X) - g2 (X))dν (X)II2 ≤
Φ φc(x)dν(x)	一
J φc(x) kgι(x) — g2(x)k2 dν(x)
φ φc(x)dν(x)
Supx kgι(x) — g2(x)k2 J φc(x)dν(x)
φc(X)dν(X)
sup Ig1 (X) — g2 (X)I2 ,
x
(18)
where We used Jensen's inequality and the fact that φc(χ) ≥ 0. As μc(0) = 0, We also obtain that
kμc(g)k2 = kμ°(g) - μ0(0)k2 ≤ SUp I∣g(x)k2.	(19)
x
As g(x) ∈ [-L,L]d,itholdsthat ∣g(x)∣2 ≤ L√d. Thereforeby (17), (18), and (19),
ldgι(x； C)- dg2(x； c)| ≤ 4L√d [kgι(X)- g2(x)k2 + kμ°(gl) - μc(g2)k2]
≤ 4L√d kgι(X) - g2(X)k2 + SUp ∣∣gι(x0) - g2(X0)k2 .
x0
Part II) For functions f1, f2, using the definition of l(X; f) (16), We get that
l(X; f1- IEf2)=xx °(x; C)Iog Pf2≡.
(20)
By substituting the definition (15) and some simplifications, We get
log
Pf2 (x； C)
PfI (X； C)
log
∏c exp(-f2(x;C))	'
Pk=I ∏b exp(-f2(x；b))
∏c exp(-fι(x;C))
Pk=I πb exP(-fι (Xib)).
[fi(X； c) - f2(x； c)] + [ρ(-f2(X; ∙)) - p(-fi(X； •))],
(21)
With
ρ(u)
log
X πb exp(ub)
Where u ∈ Rk .
We study the Lipschitz property of ρ(u) as a function of u in order to upper bound the second term
on the right-hand side (RHS).
We take the derivative of ρ(u) W.r.t. the C-th component ofu to obtain that
∂ρ(u) =	∏c exp(uc),()
∂uc	Pb ∏b exp(ub)	qc U .
Notice that qC(u) is a probability distribution. We denote (q1(u), . . . , qk(C)) by q(u).
By Taylor’s theorem, for u, u0 ∈ Rk, We have
ρ(u0) = P(U) + hu0 - U, Vuρ(u) i
=ρ(u) + hU0 - U, q(u) i ,
for some U = (1 - λ)u + λu0 with 0 ≤ λ ≤ 1. By Holder inequality, for any Holder conjugate
1 + S = 1 (r,s ∈ [1, ∞]), we get
IP(UO) -P(U)I ≤ ku0 - Ukr max 0 kq(U)Ils,
u≤u≤u0
where max over U ≤ U ≤ u0 should be understood as the maximum over the line segment between U
and U0 .
In particular,
IP(UO)-P(U)I ≤ kU0 -u∣∣∞ max Okq(U)kι = kU0 -u∣∣∞ .
u≤u≤u0
19
Published as a conference paper at ICLR 2019
Here we used the fact that qc(u) is a probability distribution and its sum is equal to 1, for any choice
of u.
We substitute (21) in (20) and use the upper bound ∣ρ(-f2(χ; ∙)) - ρ(-fι(χ; ∙))∣	≤
kfι(x; ∙) - f2(x; ∙)k∞, Which is just shown, to get that
∣l(x; fι) - l(x; f2)l ≤ 2 kfι(x; ∙) - f2(x; ∙)k∞,
as desired.	□
The following lemma relates the covering number of the function space L to the covering number of
the function space G .
Lemma 3. Consider the function space G and its induced function space L (8). Let Assumptions A2
and A3 hold. For any ε > 0, we have
N (ε, L,k∙k∞) ≤N (16Lε√d, G, k∙k∞,2).
Proof. By choosing f (x; C) = dg (x; C) = ∣∣g(x) - μc(g) k2 and using both parts of Proposition 2,
we get that
Il(X； gI)- I(X； g2)| ≤ 2 kdgι (XL)- dg2 (x, ∙)k∞
≤ 8L√d ∣∣gι(x) - g2(x)k2 + sup ∣∣g1(x0) - g2(x0)k2 .
x0∈X
If we have an ε-cover of G w.r.t. ∣g∣∞ 2 = suPχ∈χ Ilg(X) k2, it induces a 16L√dε-cover on L w.r.t.
the supremum norm.	□
The following proposition upper bounds the magnitude of l(X; f).
Proposition 4. Suppose that Assumption A3 hold and |f (X; C)| ≤ L0 for any X ∈ X and C ∈
{1, . . . , k}. Consider pf defined in (15). It holds that
X φ(x; C) log φ(x; c) ≤ 2L0 + log2 k.
c=1	pf (X; C)	2
Proof. For simplicity, we ignore the dependence on X. We use the definition of pf (15) to get
k
X
c=1
φ”焉
c φ(C) log φ(C) - c φ(C) log πc + c φ(C)f (C) +
φ(C) log
c
Xb πb exp(-f (b))
Let us consider each term on the RHS.
•	As log φ(C) ≤ 0, we have c φ(C) log φ(C) ≤ 0.
•	The priors (π1, . . . , πk) indeed defines a probability distribution, as each πc is non-
negative and Pc πc = Pc R φc(X)dν(X) = RPc φc(X)dν(X) = R 1 × dν(X) = 1. So
- Pc φ(C) log πc is the entropy of a probability distribution over an alphabet with size k,
which is at most log2 k.
•	The summation Pc φ(C)f (C) is upper bounded by L0 because of the boundedness of f(C) ≤
L0 and the fact that φ is a probability distribution and sums to one.
•	Consider the term Pc φ(C) log (Pb πb exp(-f (b))). By the boundedness of f (b), we have
Pb πb exp(-f (b)) ≤ Pb πb exp(+L0) ≤ exp(L0). Therefore, the term is upper bounded
by Pc φ(C) log(exp(L0)) ≤ L0.
Collecting all these terms leads to the upper bound of 2L0 + log? k.	□
20
Published as a conference paper at ICLR 2019
A.2 Controlling the Distorted Loss
This section provides an upper bound on the distortion of the empirical discrepancy, i.e.,
suPg∈G 公/。)- δ∕9"
Proposition 5. Suppose that Assumptions A1, A2, and A3 hold. For any δ > 0, there exists a constant
c1 > 0 such that with probability at least 1 - δ, we have
sup∣∆n(g)-∆n(g)∣ ≤cιL√d "dJ(2L)+ Lr√din(≡+ …!#.
g∈G	πmin	n	n	n
2
Proof. Recall that dg(x； C) = Ilg(X) - μc(g)∣∣2, ∆n(g) = ∆n(ψg,Φ), and ∆n(g) = ∆n(ψg,Φ).
2
Let Us define dg(x; C) = ∣∣g(x) - μc(g)∣2. Also from the definition of l(x, f) (16), We can write
∆n(ψg,φ) = n Pn=I l(Xi； dg). Similarly we have ∆n(g) = ∆n(ψg, φ) = 1 Pn=I l(Xi； dg).
Therefore, for any g ∈ G and by the application of Proposition 2 (Part II), we have
∣	∣	∣1 n	∣
[△nS) - δ n(g)| = n X l(Xi, dg ) - l(Xi, dg )
∣ i=1	∣
n
≤ n X ∣∣dg (Xig- dg (Xi； ∙)∣∣oo,	(22)
where the sUpremUm norm is taken over the centres C = {1, . . . ,k}. For any x ∈ X and any
C = {1, . . . ,k}, we have
∣dg(x；C)- dg(x；C)ITkg(X)- μc(g)∣2 - kg(X)-μc(g)k2∣
=kμc(g) - μc(g)k2 l∣2g(x) - (μc(g) + μc(g))k2 .	(23)
It was shown in (19) that ∣μc(x)∣2 is upper bounded by suPχ∈χ ∣∣g(x)∣∣2∙ One may similarly show
the same for ∣∣∕^c(x)k2:
kμc(g)k2
1 pn=ι Φc(Xi)g(Xi)
1 Pn=1 Φc(Xi)
≤
2
1 pn=ι Φc(Xi)kg(Xi)∣2
1 Pn=1 Φc(Xi)
SUPx∈χ ∣g(x)∣21 Pnn=ι φc(Xi)
1 Pn=1 Φc(Xi)
xs∈uXp ∣g(X)∣2.
As g(χ) ∈ [-L, +L]d, we have that kg(χ)∣2, kμc(χ)∣2,and ∣∣μc(χ)∣2 are all L√d-bounded, so
∣dg(x； c) - dg(x； c)∣ ≤ 4L√d ∣∣μc(g) - μc(g)∣2 .
This together with (22) and (23) show that
∣∆n(g) - △ n(g)∣ ≤ 8L√dmax ∣∣μc(g) - μc(g)k2 .	(24)
Proposition 6, which we prove soon, upper bounds supg∈g ∣∣μc(g) - μc(g)∣2. By a union bound
argument over C ∈ {1, . . . ,k}, we get that for any fixed δ > 0, there exists a constant C1 > 0 such
that
sup∣∆n(g)-△ n(g)∣ ≤C1(8L√d) "dJ(2L)+ Lfrdln≡ + ln(≡)#,
g∈G	πmin	n	n	n
with probability at least 1 - δ.	□
This proposition upper bounds the supremum of the `2 distance between cluster centres.
21
Published as a conference paper at ICLR 2019
Proposition 6. Suppose that Assumptions A1, A2, and A3 hold. Consider a fixed c ∈ {1, . . . , k}.
For any δ > 0, there exists a constant c1 > 0 such that with probability at least 1 - δ, we have
sup kμc(g) — μc(g)k2 ≤ 旦[dj√≡ + L (jd^n^ + ln≡))
g∈G	2	πmin	n	n	n
Proof. To shorten the formulae, We Use the notation En [f (Xi)] = 1 PN1 f (Xi) to denote the
empirical expectation. We Can decompose μc(g) - μc(g) as follows
μc(g) - μc(g)
E [φc(X)g(X)]	En [φc(Xi)g(Xi)]
----------------------------------
E [φc(X)]
En [φc(Xi)]
E [φc(X)g(X)] - En [φc(Xi)g(Xi)] + En [φc(Xi)g(Xi)]	En [φc(Xi)g(Xi)]
-----------------------:---:.---------------------------------:--:------
E[Φc(X)]	En [Φc(Xi)]
En [φc(Xi)g(Xi)]
E [φc(X)]
'----------------------7------
(I)
1	] E[Φc(X)g(X)] - En [φc(Xi)g(Xi)]
En [Φc(Xi)]J +	E[Φc(X)]
---------------} ×------------------{------------------}
(I)
In the rest of the proof, we provide an upper bound for Term (I) and Term (II).
Term (I): Fix δ1 > 0. We denote 1d as the d-dimensional vector with all components equal to 1. As
g(x) ∈ [-L, +L]d and φc(x) ≥ 0, we have
1d	1d
|En [φc(Xi)g(Xi)]∣ ≤ - X ∣φc(Xi)IIg(Xi)I ≤ L1d- X ∣φc(Xi)| = L1dEn [φc(Xi)],
n i=1	n i=1
where the comparison is dimension-wise.
Therefore,
En [φc(Xi)g(Xi)](En [φc(Xi)] - E [φc(X)]) < L1dEn [φc(Xi)] |En [φc(Xi)] - E [φc(X)]]
En [Φc(Xi)] E[φc(X)]	≤	En [φc(Xi)] E[φc(X)]
=H⅛Γ IEn [Φc(Xi)] - E [Φc(X)]∣. (25)
E[φc(X)]
We provide a probabilistic upper bound on IEn [φc(Xi)] - E [φc(X)]I. Since φc is a fixed --bounded
function, we use Hoeffding’s inequality to upper bound it. After some manipulations, we obtain that
IEn [Φc(Xi)] - E [φc(X)]| ≤ \吗也,
2n
with probability at least - - δ1. We use this along (25) and the assumption that E [φc(X)] ≥ πmin to
get
≤ L√d / ln(∣Zδ1)	(26)
2	πmin	2n
with probability at least - δ1.
Term (II): We want to upper bound the supremum of the norm of second term (II). Fix δ2 >
0. To simplify the notation, let us first define function f(x) = φc(x)g(x) corresponding to a
function g. Functions f are mapping from X to Rd . Furthermore, we define the function space
F = { x 7→ φc(x)g(x) : g ∈ G }. For each dimension s ∈ {-, . . . , d}, we also define Fs =
{ x 7→ φc(x)gs(x) : g ∈ G }, where gs is the s-th dimension ofg. With this notation, we can write
sup
g∈G
E [φc(X)g(X)]- En [φc(Xi)g(Xi)]
E[φc(X)]
2 = E[φ⅛ 泮 kEn [f(Xi)]- Ef(X )]k2 .
Let us focus on the supremum over F term. We have
d
supkEn[f(Xi)]-E[f(X)]k22 = supXIEn[fs(Xi)] -E[fs(X)]I2
f∈F	f∈Fs=1
d
≤X sup IEn[fs(Xi)] -E[fs(X)]I2.
s=1 fs∈Fs
22
Published as a conference paper at ICLR 2019
We take the square root of both sides and use the fact that /a2 + ... + ad ≤ aι + ... + ad (for
non-negative ai ) to get
d
supkEn[f(Xi)]-E[f(X)]k2 ≤X sup |En[fs(Xi)]-E[fs(X)]|.
f∈F	s=1 fs ∈Fs
Notice that as φc (x) is bounded by 1 and g is L-bounded dimension-wise, each fs (x) is also L-
bounded. We use Lemma 7 (Appendix B.3) on each term to obtain a high probability upper bound.
We choose the parameters of that lemma as β = 1, B = L, r = L2, and δ = δ2/d. This leads to
4E[Rn(Fs)]+Lr2叵δ“8Lln≡2# , (27)
n	3n
with probability at least 1 - δ2 .
d
supkEn[f(Xi)]-E[f(X)]k2 ≤X
f∈F	s=1
In order to control E [Rn(Fs)], we relate it to the covering number of Fs. We see that the covering
number of Fs can be upper bounded by the covering number of F, which in turn can be upper
bounded by the covering number of G. First, we use Dudley’s integral to get
4√2	「∕*diam(Fs)	一
E [Rn(Fs)] ≤ F E (	√log2N (u, Fs,L2 (Pxi：n)) du
(28)
The covering number argument is as follows. Choose any functions fs, fs0 ∈ Fs. For any sequence
xi：n, the squared of the empirical '2-norm is
n	nd	n
n X Ifs(Xi) - fs (Xi)∣2 ≤ - XX Ifr (Xi) - fr (Xi)∣2 = - X kf (Xi) - f 0(Xi)k2
i=1
i=1 r=1	i=1
1n
n E kΦc(χi)g(χi) - Φc(χi)g0(χi)k2 ≤
i=1
where in the last step we used the fact that φc(x) ≤ 1.
1n
-J2kg(xi)- g0(χi)k2,
ni=1
An ε-cover of G w.r.t. ∣∣∙k∞ ? induces an ε-cover of G w.r.t. L2(Pχ1], which in turn induces an
ε-cover on Fs w.r.t. L2(Px1:n), as the inequality above shows. Notice that this holds for any choice
of X1:n, including X1:n appearing in the Dudley’s integral (28). Therefore, by (28) and setting
diam(Fs ) = 2L, we have
E [Rn(Fs )] ≤
du
4√2J (2L)
√n
(29)
where we used the definition of the entropy integral of G (9).
After plugging this upper bound in (27) and use the upper bound (26), which was obtained for Term
(I), we see that
SUp kμc(g) - μc(g)k2 ≤ k(I)∣2 +	SUp kEn [f (Xi)] - Ef(X)]k2
g∈G	E [φc(X)] f∈F
ln(2∕δι) I
2n
d	" 16√2J(2L)+ L /2ln(2d∕δ2)+ 8L ln(2d∕δ2)-
∏min	√n	n n	3 n ,
with probability at least - - (δ1 + δ2). We set δ1 = δ2 = δ∕2 and simplify the upper bound to obtain
the desired result.	□
B	Auxiliary Results
For the convenience of the reader, we collect some auxiliary definitions and results that are used in
the our proofs.
23
Published as a conference paper at ICLR 2019
B.1	Function Space and Norms
Here we briefly define some of the notations that we use throughout the paper. Consider the domain
X and a function space F : X → R. We do not deal with measure theoretic considerations, so we
just assume that all functions involved are measurable w.r.t. an appropriate σ-algebra for that space.
We use M(X ) to refer to the set of all probability distributions defined over that space. We use
symbols such as ν ∈ M(X) to refer to probability distributions defined over that space.
We use kf kp,ν to denote the Lp(ν)-norm (1 ≤ p < ∞) of a measurable function f : X → R, i.e.,
kfkp,ν, Zx If(X)Ipdv(x).
The supremum norm is defined as
kfk∞ = sup If (x)I
x∈X
Let x1, . . . , xn be a sequence of points in X. We use x1:n to refer to this sequence. The empirical
measure Pn is the probability measure that puts a mass of 1 at each Xi, i.e.,
i=1
where δx is the Dirac’s delta function. For Dn = z1:x, the empirical L2(Pn)-norm of function
f : X → R is
1n
kfkDn = kfkPn = n X If (Xi )I2 .
i=1
We can also define other Lp(Pn)-norms similarly.8 When there is no chance of confusion about Dn,
we may denote the empirical norm simply by kf kn .
B.2	The Metric Entropy and the Covering Number
We quote the definition of the covering number from Gyorfi et al. (2002).
Definition 1 (Definition 9.3 of Gyorfi et al. 2002). Let ε > 0, F be a set of real-valued functions
defined on X, and νX be a probability measure on X. Every finite collection of Nε = {f1, . . . , fNε}
defined on X with the property that for every f ∈ F, there is a function f0 ∈ Nε such that
kf — f 0kp,νχ < ε is called an ε-cover of F w.r.t. k∙kp,νχ.
Let N (ε, F, ∣∣∙∣∣p VX) be the size of the smallest ε -cover of F w.r.t. ∣∣∙kp Vx. If no finite ε-cover
exists, take N(ε, F, ∣∙∣p Vx) = ∞. Then N(ε, F, ∣∙∣p Vx) is called an ε-covering number of F and
logN(ε, F, ∣∙∣p Vx) is called the metric entropy of F w.r.t. the same norm.
Given a X1:n = (X1, . . . , Xn) ⊂ X and its corresponding empirical measure Pn = Px1:n, we can
define the empirical covering number of F w.r.t. the empirical norm ∣∣∙∣pχι and is denoted by
Np(ε, F,xi：n)= N(ε, F,Lp(xi：n)).
B.3	Rademacher Complexity
We define Rademacher complexity and quote a result from Bartlett & Mendelson (2002). For more
information about Rademacher complexity, we refer the reader to Bartlett & Mendelson (2002);
Bartlett et al. (2005).
Let σ1, . . . , σn be independent random variables with P {σi = 1} = P {σi = -1} = 1/2. For
a function space F : X → R, define RnF = SuPf∈f nn Pn=1 σif (Xi) with Xi 〜V. The
Rademacher complexity (or average) of F is E [RnG], in which the expectation is w.r.t. both σ and
Xi . Rademacher complexity appears in the analysis of the supremum of an empirical process right
after the application of the symmetrization technique. As such, its behaviour is closely related to the
8Or maybe more clearly, Pn (A) = 1 Pn=I I{xi ∈ A} for any measurable subset A ⊂ X.
24
Published as a conference paper at ICLR 2019
behaviour of the empirical process. One may interpret the Rademacher complexity as a complexity
measure that quantifies the extent that a function from F can fit a noise sequence of length n (Bartlett
& Mendelson, 2002).
The following is a simplified (and slightly reworded) version of Theorem 2.1 of Bartlett et al. (2005).
Lemma 7. Let F : X → R be a measurable function space with B-bounded functions. Let
X1 , . . . , Xn ∈ X be independent random variables. Assume that for some r > 0, Var [f (Xi)] ≤ r
for every f ∈ F. Then for every δ > 0, with probability at least 1 - δ,
sup
f∈F
1n
E[f(X)]- n ∑ f (Xi)
≤
inf [2(1+ β)E[Rn(F)] + 4 2r ln(2/6 +2B (；+；)㈣2© J .
β>0	n	n	3 β	n
C Experimental results
We give implementation details about the experiments of our submitted paper in Section C.1. We
report the detailed performance of our model as a function of the output dimensionality and the
number of hidden layers in the zero-shot learning context in Section C.2. We show in Section C.3
that our method can be generalized to hard clustering by using implicit centers. We give additional
visualization results in Section C.4.
C.1 Implementation details
We coded our method in PyTorch and ran all our experiments on a single Nvidia GeForce GTX 1060
which has 6GB of RAM.
PyTorch automatically calculates the gradient w.r.t. the mini-batch representation F. Nonetheless, it is
worth mentioning that both the first and second arguments of our prediction function ψ(F, M, π) de-
pend on F in the case where the centers are implicit (i.e., when we write M = diag(Y >1n)-1Y >F).
In this case, the gradient of our loss function w.r.t. F depends on both the first and second arguments
of ψ.
C.1.1 Zero-shot learning experiments
We now give details specific to the zero-shot experiments.
In the zero-shot learning experiment where F and M are computed from different sources (i.e.,
images and text) and are the output of two different networks, the optimization is performed by
alternately optimizing one variable while fixing the other.
Mini-batch size: The training datasets of CUB and Flowers contain 5894 and 5878 images, re-
spectively. In order to fit into memory, we set our mini-batch sizes as 421 (= 5894/14) and 735
(≈ 5878/8) for CUB and Flowers, respectively.
Reed et al. (2016) and Snell et al. (2017) use 10 different views per image during training (middle,
upper left, upper right, lower left and lower right crops for the original and horizontally-flipped
image), and 1 view for test which is the middle crop of the original image. On the other hand, we use
only 1 view per image (i.e., the middle crop of the original image) during training and test.
Optimizer: We use the Adam optimizer with a learning rate of 10-5 to train both models 中§、and
gθ2.
Target soft assignment matrix: When we use the representations provided by Snell et al. (2017),
.	.	i'.	♦	.	I / 7-1 Tl~Γ ~ ∖	,ʃ,	_ r TT, 'V ^k∙	1	.1	.	∙	7~1	1 7l~Γ
our target soft assignment matrix is Y1 = ψ(F, M, π) = Ψ ∈ Yn×k where the matrices F and M
• 1	1	1	~	1 -t ml	1	,	,ʃ,	♦ , ,	,ʃ,
are provided and π = 11k. The elements of Ψ are written Ψic
we formulate:
~	/ I ∕7∙ ~ ∖ ∖
πc exp(-d(fi,MC))
Pk=I πe expJdfi,μ e)) .
In this case,
25
Published as a conference paper at ICLR 2019
Using a temperature of 10 made the optimization more stable as it avoided gradients with high values.
We use a temperature of 2 when using the representations provided by Reed et al. (2016).
Initial temperature of our model: To make our optimization framework stable, we start with a
temperature of 50. We then formulate our Bregman divergence as:
d(fi, μC)=而 kfi - μck2
c 50 c
where f and μc are the representations learned by our model. We decrease our temperature by 10%
(i.e., tempt+1 = 0.9tempt) every 3000 epochs until the algorithm stops training.
We stop training at 10k epochs on CUB and 1k epochs on Flowers.
C.1.2 Visualization experiments
We now give details specific to the visualization experiments.
Dataset size: To be comparable to t-SNE, we directly learn two-dimensional embeddings instead
of neural networks. Our mini-batch size is the size of the test set (i.e., the number of examples is
n = 104 for most datasets except STL that contains n = 8000 test examples).
Optimizer: We use the RMSprop optimizer with a learning rate of 10-3, α = 0.99, = 10-6, the
weight decay and momentum are both 0, and the data is not centered. We also we formulate the empiri-
cal discrepancy loss ∆n(Ψ, Y) = Pn=IDKL(yikΨi) instead of ∆n(Ψ, Y) = 1 Pn=IDKL(yikΨi).
Target soft assignment matrix: Let US note Zi = [zi,1,…，zi,k]> ∈ Rk the vector containing the
logits of the learned representation of the i-th test example. We formulate yi = [yi,1,…,yi,k]> ∈
Rk our target assignment vector for the i-th test example as follows:
=exp(攀)
yi,c = PL exp(等)
(30)
where τ = 5 for all the dataset except CIFAR-100 for which τ = 4.
We report the quantitative scores for τ = 1.
Initial temperature of our model: We learned our representation by using a fixed temperature of 1
(i.e., using the standard squared Euclidean distance).
We stop the algorithm after 8000 iterations.
Tuning t-SNE: we tested different ranges of scaling (1/1, 1/10, 1/100) and perplexity (i.e., 1, 10, 30
(default) and 100) and reported the representations that obtained the best quantitative results.
C.2 Impact of dimensionality in zero-shot learning
Let e ∈ N be the dimensionality of the representations taken as input and d the output dimensionality
of the models gθ1 and gθ2 , the architecture of the models is e-d and e-e-d in the 1 and 2 hidden
layer cases, respectively. The hyperparameter d ∈ {16,32,64,…，e} is also a hyperparameter
cross-validated on the validation set.
We give the detailed accuracy performance of our model in Tables 5 to 9.
•	Table 5 reports the test performance of our model on CUB when using the features provided
by Reed et al. (2016) as supervision for different numbers of hidden layers and values of output
dimensionality of our model.
•	Table 6 (resp. Table 7) reports the validation (resp. test) performance of our model on CUB when
using the features provided by Snell et al. (2017) as supervision.
•	Table 8 (resp. Table 9) reports the validation (resp. test) performance of our model on Flowers
when using the features provided by Snell et al. (2017) as supervision.
Dimensionality reduction improves performance, though the optimal dimensionality is dataset specific.
In general, increasing the number of hidden layers also helps.
26
Published as a conference paper at ICLR 2019
Dimensionality d	16	32	64	128	256	512	1024
Linear model	50.1	54.2	54.2	54.2	54.3	54.2	54.2
1 hidden layer	51.4	54.4	54.6	54.3	54.5	54.7	53.8
2 hidden layers	51.5	54.4	57.1	57.4	57.7	57.7	56.5
Table 5: Test accuracy (in %) as a function of d when using DS-SJE (Reed et al., 2016) (Char
CNN-RNN) as supervision on CUB
Dimensionality d	16	32	64	128	256	512
Linear model	75.1	82.2	82.4	82.6	82.6	82.4
1 hidden layer	77.0	81.9	82.1	82.4	83.3	82.9
2 hidden layers	72.4	72.9	75.6	77.5	79.7	79.7
Table 6: Validation accuracy (in %) as a function of the output dimensionality d when using ProtoNet
(Snell et al., 2017) as supervision on CUB
Dimensionality d	16	32	64	128	256	512
Linear model	56.0	58.3	58.6	58.6	58.6	58.4
1 hidden layer	57.7	59.8	60.2	60.3	60.3	60.0
2 hidden layers	57.4	58.5	59.4	59.6	59.5	59.3
Table 7: Test accuracy (in %) as a function of the output dimensionality d when using ProtoNet
(Snell et al., 2017) as supervision on CUB
Dimensionality d	16	32	64	128	256	512	1024
Linear model	49.6	62.5	76.3	82.6	86.5	87.7	87.6
1 hidden layer	86.7	87.1	87.1	87.3	87.7	87.9	87.8
2 hidden layers	86.3	86.3	87.3	87.5	87.6	88.1	87.9
Table 8: Validation accuracy (in %) as a function of the output dimensionality when using ProtoNet
(Snell et al., 2017) as supervision on Flowers
Dimensionality d	16	32	64	128	256	512	1024
Linear model	54.6	58.0	61.5	64.2	64.2	64.7	64.4
1 hidden layer	66.8	66.5	65.9	65.9	65.9	66.7	65.3
2 hidden layers	67.2	67.3	67.9	67.7	67.7	68.2	66.0
Table 9: Test accuracy (in %) as a function of the output dimensionality when using ProtoNet (Snell
et al., 2017) as supervision on Flowers
C.3 Generalization to hard clustering
We validate that DRPR can be used to perform hard clustering as in Snell et al. (2017) but with
implicit centers. To this end, we train a neural network with 2 convolutional layers on MNIST (LeCun
et al., 1998) followed by a fully connected layer. Its output dimensionality is d = 2 or d = 3, the
mini-batch size is n = 1000, the number of categories is k = 10 and the target hard assignment
matrix Y ∈ {0, 1}n×k contains category membership information (i.e., Yic is 1 if the example fi
belongs to category c, 0 otherwise). We train the model on the training set of MNIST and plot in
Fig. 6 the representations of the test set. By assigning each test example to the category with closest
centroid (obtained from the training set), the model obtains 98% (resp. 99%) accuracy when d = 2
(resp. d = 3). DRPR can then be learned for hard clustering when the centers are implicitly written
as a function of the mini-batch matrix representation F and the target hard assignment matrix Y .
27
Published as a conference paper at ICLR 2019
Figure 5: CIFAR 100 representation learned by t-SNE when using softmax representations as input
Figure 6: Visualization of the representation learned on MNIST by our approach in the supervised
hard clustering setup. The left (resp. right) figure is the representation learned by our model when its
output dimensionality is d = 2 (resp. d = 3).
C.4 Visualization results
We now present visualization results.
C.4.1 Artifacts with t-SNE
Fig. 5 illustrates the CIFAR 100 representation learned by t-SNE when its input data is the target
probability distribution that we give as supervision/input of our algorithm.
Following the recommendations mentioned in https://lvdmaaten.github.io/tsne/
when the representation form a strange ball with uniformly distributed points and obtaining very low
error, we decreased the perplexity from 30 (which is the value by default) to 10 and 1 and divided our
data by 10, 100 and 1000. Nonetheless, we still obtained the same type of representation as in Fig. 5.
This kind of artifact is the reason why we only report results obtained with logits.
28
Published as a conference paper at ICLR 2019
Figure 7: (left) Visualization obtained with t-SNE on the toy dataset when replacing the `2 distance
in the original space by the KL divergence to compare probability distribution representations. (right)
Visualization obtained with t-SNE on the toy dataset when replacing the `2 distance in the original
space by the Jensen Shannon divergence.
Figure 8: CIFAR 10 representation of DRPR and t-SNE
C.4.2 Adapting t-SNE with other divergences
We plot in Fig. 7 the visualization obtained by t-SNE when using the KL or JS divergences to
compare pairs of probability distribution representations. The representations obtained in this case
are still worse than using the original 3-dimensional representations as the cluster structures are not
preserved, nor the inter-cluster distances. This suggests that comparing pairs of examples, as done
by t-SNE, is less appropriate than our method that considers similarities between examples and the
different k = 6 clusters.
C.4.3 Additional results
Fig. 8	illustrates the DRPR and t-SNE representations of CIFAR 10. Animal categories are illustrated
on the right whereas machines are on the left.
Fig. 9	illustrates the DRPR and t-SNE representations of CIFAR 100. We learned the representations
by exploiting 100 clusters but plot only 20 colors (one for each superclass of CIFAR 100), which
is why multiple spikes have the same color. Groups with same colors are better defined with our
approach than with t-SNE, this means that different categories from the same superclass (e.g., hamster,
mouse, rabbit, shrew, squirrel which are small mammals) are grouped together with DRPR. One can
observe a semantic structure in the 2D representation of DRPR: plants and insects are on the top left;
animals are on the bottom left and categories on the right are outdoor categories. Medium mammals
are also represented between small mammals and large carnivores.
29
Published as a conference paper at ICLR 2019
aquatic mammals
fish
flowers
food containers
fruit and vegetables
household electrical devices
household furniture
insects
large carnivores
large man-made o□td∞r things
large natural outdoor scenes
large omnivores and herbivores
medium mammals
non-insect invertebrates
people
reptiles
small mammals
trees
vehicles 1
vehicles 2
Figure 9: CIFAR 100 representation of DRPR and t-SNE
Figures 10, 11, 12 and 13 illustrate the representations learned by our model for the STL, MNIST,
CIFAR 100 and CIFAR 10 datasets, respectively. Instead of using colors that represent the categories
of the embeddings as done in the submitted paper, we directly plot the images.
In general, we observe that images towards the end of spikes consist of a clearly visible object in
a standard viewpoint on a simple background. Those closer to the center often have objects with a
non-standard viewpoint or have a complex textured background. At a high-level, the classes appear
to be organized by their backgrounds.
Taking the STL-10 visualization as an example, deer and horses are close together since they both
tend to be found in the presence of green vegetation. These classes are far from boats and planes,
which often have solid blue backgrounds. Looking more closely, the ordering of classes is sensible.
Planes are neighbors to both boats (similar background) and birds (similar silhouette). And trucks
neighbor both cars (similar background) and horses, which appear visually similar, particularly for
images in which the horse is pulling a cart.
Taking the MNIST visualization as another example, one can observe that written characters in spikes
are easy to recognize as they correspond to examples for which the learned model has high confidence
in its scores. On the other hand, ambiguous examples are between multiple spikes (e.g., the characters
0 and 6 between spikes are more ambiguous than their neighbors in spikes).
30
Published as a conference paper at ICLR 2019
Figure 10: STL representation learned by DRPR.
31
Published as a conference paper at ICLR 2019
F
3 m
%
5

书8 9 ?
VIS
5 5
55
5555
Figure 11: MNIST representation learned by DRPR.
? q q ?
)q q q
3
O JO o
^>oo
DoQ。
Io OC
32
Published as a conference paper at ICLR 2019
Figure 12: CIFAR 100 representation learned by DRPR.
33
Published as a conference paper at ICLR 2019
Figure 13: CIFAR 10 representation learned by DRPR.
34