Published as a conference paper at ICLR 2019
Trellis Networks for Sequence Modeling
Shaojie Bai
Carnegie Mellon University
J. Zico Kolter
Carnegie Mellon University and
Bosch Center for AI
Vladlen Koltun
Intel Labs
Ab stract
We present trellis networks, a new architecture for sequence modeling. On the
one hand, a trellis network is a temporal convolutional network with special struc-
ture, characterized by weight tying across depth and direct injection of the input
into deep layers. On the other hand, we show that truncated recurrent networks
are equivalent to trellis networks with special sparsity structure in their weight
matrices. Thus trellis networks with general weight matrices generalize truncated
recurrent networks. We leverage these connections to design high-performing trel-
lis networks that absorb structural and algorithmic elements from both recurrent
and convolutional models. Experiments demonstrate that trellis networks outper-
form the current state of the art methods on a variety of challenging benchmarks,
including word-level language modeling and character-level language modeling
tasks, and stress tests designed to evaluate long-term memory retention. The code
is available here1.
1	Introduction
What is the best architecture for sequence modeling? Recent research has produced significant
progress on multiple fronts. Recurrent networks, such as LSTMs, continue to be optimized and
extended (Merity et al., 2018b; Melis et al., 2018; Yang et al., 2018; Trinh et al., 2018). Temporal
convolutional networks have demonstrated impressive performance, particularly in modeling long-
range context (van den Oord et al., 2016; Dauphin et al., 2017; Bai et al., 2018). And architectures
based on self-attention are gaining ground (Vaswani et al., 2017; Santoro et al., 2018).
In this paper, we introduce a new architecture for sequence modeling, the Trellis Network. We aim
to both improve empirical performance on sequence modeling benchmarks and shed light on the
relationship between two existing model families: recurrent and convolutional networks.
On the one hand, a trellis network is a special temporal convolutional network, distinguished by two
unusual characteristics. First, the weights are tied across layers. That is, weights are shared not only
by all time steps but also by all network layers, tying them into a regular trellis pattern. Second, the
input is injected into all network layers. That is, the input at a given time-step is provided not only
to the first layer, but directly to all layers in the network. So far, this may seem merely as a peculiar
convolutional network for processing sequences, and not one that would be expected to perform
particularly well.
Yet on the other hand, we show that trellis networks generalize truncated recurrent networks (recur-
rent networks with bounded memory horizon). The precise derivation of this connection is one of
the key contributions of our work. It allows trellis networks to serve as bridge between recurrent and
convolutional architectures, benefitting from algorithmic and architectural techniques developed in
either context. We leverage these relationships to design high-performing trellis networks that ab-
sorb ideas from both architectural families. Beyond immediate empirical gains, these connections
may serve as a step towards unification in sequence modeling.
We evaluate trellis networks on challenging benchmarks, including word-level language model-
ing on the standard Penn Treebank (PTB) and the much larger WikiText-103 (WT103) datasets;
character-level language modeling on Penn Treebank; and standard stress tests (e.g. sequential
MNIST, permuted MNIST, etc.) designed to evaluate long-term memory retention. On word-level
1https://github.com/locuslab/trellisnet
1
Published as a conference paper at ICLR 2019
Penn Treebank, a trellis network outperforms by more than a unit of perplexity the recent architec-
ture search work of Pham et al. (2018), as well as the recent results of Melis et al. (2018), which
leveraged the Google Vizier service for exhaustive hyperparameter search. On character-level Penn
Treebank, a trellis network outperforms the thorough optimization work of Merity et al. (2018a).
On word-level WikiText-103, a trellis network outperforms by 7.6% in perplexity the contempora-
neous self-attention-based Relational Memory Core (Santoro et al., 2018), and by 11.5% the work
of Merity et al. (2018a). (Concurrently with our work, Dai et al. (2019) employ a transformer and
achieve even better results on WikiText-103.) On stress tests, trellis networks outperform recent
results achieved by recurrent networks and self-attention (Trinh et al., 2018). It is notable that the
prior state of the art across these benchmarks was held by models with sometimes dramatic mutual
differences.
2	Background
Recurrent networks (Elman, 1990; Werbos, 1990; Graves, 2012), particularly with gated cells such
as LSTMs (Hochreiter & Schmidhuber, 1997) and GRUs (Cho et al., 2014), are perhaps the most
popular architecture for modeling temporal sequences. Recurrent architectures have been used to
achieve breakthrough results in natural language processing and other domains (Sutskever et al.,
2011; Graves, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Vinyals et al., 2015; Karpathy
& Li, 2015). Convolutional networks have also been widely used for sequence processing (Waibel
et al., 1989; Collobert et al., 2011). Recent work indicates that convolutional networks are effec-
tive on a variety of sequence modeling tasks, particularly ones that demand long-range information
propagation (van den Oord et al., 2016; Kalchbrenner et al., 2016; Dauphin et al., 2017; Gehring
et al., 2017; Bai et al., 2018). A third notable approach to sequence processing that has recently
gained ground is based on self-attention (Vaswani et al., 2017; Santoro et al., 2018; Chen et al.,
2018). Our work is most closely related to the first two approaches. In particular, we establish a
strong connection between recurrent and convolutional networks and introduce a model that serves
as a bridge between the two. A related recent theoretical investigation showed that under a certain
stability condition, recurrent networks can be well-approximated by feed-forward models (Miller &
Hardt, 2018).
There have been many combinations of convolutional and recurrent networks (Sainath et al., 2015).
For example, convolutional LSTMs combine convolutional and recurrent units (Donahue et al.,
2015; Venugopalan et al., 2015; Shi et al., 2015). Quasi-recurrent neural networks interleave con-
volutional and recurrent layers (Bradbury et al., 2017). Techniques introduced for convolutional
networks, such as dilation, have been applied to RNNs (Chang et al., 2017). Our work establishes a
deeper connection, deriving a direct mapping across the two architectural families and providing a
structural bridge that can incorporate techniques from both sides.
3	Sequence Modeling and Trellis Networks
Sequence modeling. Given an input x1:T = x1, . . . , xT with sequence length T, a sequence model
is any function G : XT → YT such that
y1:T = y1, . . . ,yT = G(x1, . . . ,xT),	(1)
where yt should only depend on x1:t and not on xt+1:T (i.e. no leakage of information from the
future). This causality constraint is essential for autoregressive modeling.
In this section, we describe a new architecture for sequence modeling, referred to as a trellis net-
work or TrellisNet. In particular, we provide an atomic view of TrellisNet, present its fundamental
features, and highlight the relationship to convolutional networks. Section 4 will then elaborate on
the relationship of trellis networks to convolutional and recurrent models.
Notation. We use x1:T = (x1, . . . , xT) to denote a length-T input sequence, where vector xt ∈ Rp
is the input at time step t. Thus x1:T ∈ RT ×p. We use zt(i) ∈ Rq to represent the hidden unit at
time t in layer i of the network. We use Conv1D(x; W) to denote a 1D convolution with a kernel
W applied to input x = x1:T .
A basic trellis network. At the most basic level, a feature vector zt(+i+11) at time step t + 1 and level
i + 1 of TrellisNet is computed via three steps, illustrated in Figure 1a:
2
Published as a conference paper at ICLR 2019
(a) TrelliSNet at an atomic level
Figure 1: The interlayer transformation of TrellisNet, at an atomic level (time steps t and t + 1,
layers i and i + 1) and on a longer sequence (time steps 1 to 8, layers i and i + 1).
1.	The hidden input comprises the hidden outputs zt(i), zt(+i)1 ∈ Rq from the previous layer i, as well
as an injection of the input vectors xt, xt+1. At level 0, we initialize to zt(0) = 0.
2.	A pre-activation output Z(V ∈ Rr is produced by a feed-forward linear transformation:
z(iʌI)=WJ Itj+W2⅛)1],	⑵
zt	zt+1
where W1,W2 ∈ Rr×(p+q) are weights, and r is the size of the pre-activation output 2(2口.
(Here and throughout the paper, all linear transformations can include additive biases. We omit
these for clarity.)
3.	The output zt(+i+11) is produced by a nonlinear activation function f : Rr × Rq → Rq applied
to the pre-activation output z(∖1) and the output z(i) from the previous layer. More formally,
(i+1)	f (i+1) (i)
zt+1 = f zt+1 , zt	.
A full trellis network can be built by tiling this elementary procedure across time and depth. Given
an input sequence x1:T, we apply the same production procedure across all time steps and all layers,
using the same weights. The transformation is the same for all elements in the temporal dimen-
sion and in the depth dimension. This is illustrated in Figure 1b. Note that since we inject the
same input sequence at every layer of the TrellisNet, we can precompute the linear transformation
Xt+ι = Wxxt + Wlxt+1 for all layers i. This identical linear combination of the input can then be
added in each layer i to the appropriate linear combination of the hidden units, W1zzt(i) + W2zzt(+i)1 ,
where Wjx ∈ Rr×p, Wjz ∈ Rr×q .
Now observe that in each level of the network, we are in effect performing a 1D convolution over the
hidden units z1(i:T) . The output of this convolution is then passed through the activation function f .
Formally, with W ∈ Rr×q as the kernel weight matrix, the computation in layer i can be summarized
as follows (Figure 1b):
z(i+1) = Conv1D (z(iT; W)+ XLT,	z(21) = f 化” z(iT_J .	(3)
The resulting network operates in feed-forward fashion, with deeper elements having progressively
larger receptive fields. There are, however, important differences from typical (temporal) convo-
lutional networks. Notably, the filter matrix is shared across all layers. That is, the weights are
tied not only across time but also across depth. (Vogel & Pock (2017) have previously tied weights
across depth in image processing.) Another difference is that the transformed input sequence Xi：T
is directly injected into each hidden layer. These differences and their importance will be analyzed
further in Section 4.
The activation function f in Equation (3) can be any nonlinearity that processes the pre-activation
output ^(i+1)and the output from the previous layer z(：T-1. We will later describe an activation
function based on the LSTM cell. The rationale for its use will become clearer in light of the
analysis presented in the next section.
3
Published as a conference paper at ICLR 2019
4	TrellisNet, TCN, and RNN
In this section, we analyze the relationships between trellis networks, convolutional networks, and
recurrent networks. In particular, we show that trellis networks can serve as a bridge between con-
volutional and recurrent networks. On the one hand, TrellisNet is a special form of temporal con-
volutional networks (TCN); this has already been clear in Section 3 and will be discussed further in
Section 4.1. On the other hand, any truncated RNN can be represented as a TrellisNet with special
structure in the interlayer transformations; this will be the subject of Section 4.2. These connections
allow TrellisNet to harness architectural elements and regularization techniques from both TCNs
and RNNs; this will be summarized in Section 4.3.
4.1	TrellisNet and TCN
We briefly introduce TCNs here, and refer the readers to Bai et al. (2018) for a more thorough dis-
cussion. Briefly, a temporal convolutional network (TCN) is a ConvNet that uses one-dimensional
convolutions over the sequence. The convolutions are causal, meaning that, at each layer, the trans-
formation at time t can only depend on previous layer units at times t or earlier, not from later points
in time. Such approaches were used going back to the late 1980s, under the name of “time-delay
neural networks” (Waibel et al., 1989), and have received significant interest in recent years due to
their application in architectures such as WaveNet (van den Oord et al., 2016).
In essence, TrellisNet is a special kind of temporal convolutional network. TCNs have two distinc-
tive characteristics: 1) causal convolution in each layer to satisfy the causality constraint and 2) deep
stacking of layers to increase the effective history length (i.e. receptive field). Trellis networks have
both of these characteristics. The basic model presented in Section 3 can easily be elaborated with
larger kernel sizes, dilated convolutions, and other architectural elements used in TCNs; some of
these are reviewed further in Section 4.3.
However, TrellisNet is not a general TCN. As mentioned in Section 3, two important differences
are: 1) the weights are tied across layers and 2) the linearly transformed input Xi：T is injected
into each layer. Weight tying can be viewed as a form of regularization that can stabilize training,
support generalization, and significantly reduce the size of the model. Input injection mixes deep
features with the original sequence. These structural characteristics will be further illuminated by
the connection between trellis networks and recurrent networks, presented next.
4.2	TrellisNet and RNN
Recurrent networks appear fundamentally different from convolutional networks. Instead of operat-
ing on all elements of a sequence in parallel in each layer, an RNN processes one input element at a
time and unrolls in the time dimension. Given a non-linearity g (which could be a sigmoid or a more
elaborate cell), we can summarize the transformations in an L-layer RNN at time-step t as follows:
ht(i) = g Wh(ix)h(ti-1) + Wh(ih)h(ti-)1	for1≤i≤L,	ht(0) =xt.	(4)
Despite the apparent differences, we will now show that any RNN unrolled to a finite length is
equivalent to a TrellisNet with special sparsity structure in the kernel matrix W. We begin by
formally defining the notion of a truncated (i.e. finite-horizon) RNN.
Definition 1. Given an RNN ρ, a corresponding M-truncated RNN ρM, applied to the sequence
x1:T, produces at time step t the output yt by applying ρ to the sequence xt-M +1:t (here x<0 = 0).
Theorem 1. Let ρM be an M -truncated RNN with L layers and hidden unit dimensionality d. Then
there exists an equivalent TrellisNet τ with depth (M + L - 1) and layer width (i.e. number of
channels in each hidden layer) Ld. Specifically, for any xi：T, PM(xi：T) = 丁工陋-^+上工e此上T) (i.e.
the TrellisNet outputs contain the RNN outputs).
Theorem 1 states that any M -truncated RNN can be represented as a TrellisNet. How se-
vere of a restriction is M -truncation? Note that M -truncation is intimately related to truncated
backpropagation-through-time (BPTT), used pervasively in training recurrent networks on long se-
quences. While RNNs can in principle retain unlimited history, there is both empirical and theoreti-
cal evidence that the memory horizon of RNNs is bounded (Bai et al., 2018; Khandelwal et al., 2018;
4
Published as a conference paper at ICLR 2019
Miller & Hardt, 2018). Furthermore, if desired, TrellisNets can recover exactly a common method
of applying RNNs to long sequences - hidden state repackaging, i.e. copying the hidden state across
subsequences. This is accomplished using an analogous form of hidden state repackaging, detailed
in Appendix B.
Proof of Theorem 1. Let ht(,it)0 ∈ Rd be the hidden state at time t and layer i of the truncated RNN
ρt-t0+1 (i.e., the RNN begun at time t0 and run until time t). Note that without truncation, history
starts at time t0 = 1, so the hidden state h(ti) ofρ can be equivalently expressed as h(t,i1). When t0 > t,
we define ht,t0 = 0 (i.e. no history information if the clock starts in the future).
By assumption, ρM is an RNN defined by the following parameters: {Wh(ix), Wh(ih), g, M}, where
Wh(ih) ∈ Rw×d for all i, Wh(1x) ∈ Rw×p, and Wh(ix) ∈ Rw×d for all i = 2, . . . , L are the weight
matrices at each layer (w is the dimension of pre-activation output). We now construct a TrellisNet τ
according to the exact definition in Section 3, with parameters {W1, W2, f}, where
00
00
..
..
..
Wh(xL)	0
(5)
such that W1, W2 ∈ RLw×(p+Ld). We define a nonlinearity f by f(α, β) = g(α) (i.e. applying g
only on the first entry).
Let t ∈ [T] , j ≥ 0 be arbitrary and fixed. We now claim that the hidden unit at time t and layer j of
TrellisNet τ can be expressed in terms of hidden units at time t in truncated forms of ρ:
zt(j) = hht(1t)-j+1 ht(2t)-j+2 ... ht(Lt-)j+Li> ∈ RLd,	(6)
t,t-j +	t,t-j +	t,t-j +L
where zt(j) is the time-t hidden state at layer j of τ and h(t,it)0 is the time-t hidden state at layer i of
ρt-t0+1.
We prove Eq. (6) by induction on j . As a base case, consider j = 0; i.e. the input layer of τ .
Since ht,t0 = 0 when t0 > t, we have that zj(0) = [0 0 . . . 0]> . (Recall that in the input layer
of TrellisNet we initialize zt(0) = 0.) For the inductive step, suppose Eq. (6) holds for layer j, and
consider layer j + 1. By the feed-forward transformation of TrellisNet defined in Eq. (2) and the
nonlinearity f we defined above, we have:
(j+1)	xt-1	xt
Zt	= W1 7(j)	+ W2 jj-)
zt-1	zt
-0 Whh)	0	...	0
=0	0	Whh)…0
.	.	....
..	.	..
0	0	0	... Wh(hL)
00
00
..
..
..
Wh(Lx )	0
Wh(1h)ht(-1)1,t-j +Wh(1x)xt
.
=.
.
W(L)h(L)	+ W(L)h(L-1)
Whh ht-1,t-j+L-1 + Whx ht,t-j+L-1
z(j+1) = f (^”1),z(-)ι) = g(zj+1))= [h(,1t-j	h(2t)-j+ι
ht(,Lt-) j+L-1i>
(7)
xt
h(1)
ht,t-j + 1
.
.
.
h(t,Lt-) j+L
(8)
(9)
(10)
where in Eq. (10) we apply the RNN non-linearity g following Eq. (4). Therefore, by induction, we
have shown that Eq. (6) holds for all j ≥ 0.
If TrellisNet τ has M+L- 1 layers, then at the final layer we have zt(M+L-1) = [........... h(t,Lt)+1-M]>.
Since ρM is an L-layer M -truncated RNN, this (taking the last d channels of zt(M+L-1)) is exactly
the output of ρM at time t.
5
Published as a conference paper at ICLR 2019
y4 Output
(a) Representing RNN units as channel groups
Whh^W^
χ3 碟	h323
χ3 h3l)	0
X4
X4
x4
χ4
χ4
Group 1 Group 2 Group 3
Channels at time t
I
0
0
0
0
0
4
(b) Mixed group convolution
Figure 2: Representing a truncated 2-layer RNN ρM as a trellis network τ . (a) Each unit of τ has
three groups, which house the input, first-layer hidden vector, and second-layer hidden vector of
ρM , respectively. (b) Each group in the hidden unit of τ in level i + 1 at time step t + 1 is computed
by a linear combination of appropriate groups of hidden units in level i at time steps t and t + 1.
The linear transformations form a mixed group convolution that reproduces computation in ρM .
(Nonlinearities not shown for clarity.)
In other words, we have shown that ρM is equivalent to a TrellisNet with sparse kernel matrices
W1,W2. This completes the proof.	□
Note that the convolutions in the TrellisNet τ constructed in Theorem 1 are sparse, as shown in
Eq. (5). They are related to group convolutions (Krizhevsky et al., 2012), but have an unusual
form because group k at time t is convolved with group k - 1 at time t + 1. We refer to these
as mixed group convolutions. Moreover, while Theorem 1 assumes that all layers of ρM have the
same dimensionality d for clarity, the proof easily generalizes to cases where each layer has different
widths.
For didactic purposes, we recap and illustrate the construction in the case of a 2-layer RNN. The
key challenge is that a naive unrolling of the RNN into a feed-forward network does not produce
a convolutional network, since the linear transformation weights are not constant across a layer.
The solution, illustrated in Figure 2a, is to organize each hidden unit into groups of channels, such
that each TrellisNet unit represents 3 RNN units simultaneously (for xt, h(t1) , ht(2)). Each TrellisNet
unit thus has (p + 2d) channels. The interlayer transformation can then be expressed as a mixed
group convolution, illustrated in Figure 2b. This can be represented as a sparse convolution with the
structure given in Eq. (5) (with L = 2). Applying the nonlinearity g on the pre-activation output,
this exactly reproduces the transformations in the original 2-layer RNN.
The TrellisNet that emerges from this construction has special sparsity structure in the weight matrix.
It stands to reason that a general TrellisNet with an unconstrained (dense) weight matrix W may
have greater expressive power: it can model a broader class of transformations than the original
RNN ρM . Note that while the hidden channels of the TrellisNet τ constructed in the proof of
Theorem 1 are naturally arranged into groups that represent different layers of the RNN ρM (Eq.
(6)), an unconstrained dense weight matrix W no longer admits such an interpretation. A model
defined by a dense weight matrix is fundamentally distinct from the RNN ρM that served as our
point of departure. We take advantage of this expressivity and use general weight matrices W, as
presented in Section 3, in our experiments. Our ablation analysis will show that such generalized
dense transformations are beneficial, even when model capacity is controlled for.
The proof of Theorem 1 did not delve into the inner structure of the nonlinear transformation g in
RNN (or f in the constructed TrellisNet). For a vanilla RNN, for instance, f is usually an elemen-
twise sigmoid or tanh function. But the construction in Theorem 1 applies just as well to RNNs
with structured cells, such as LSTMs and GRUs. We adopt LSTM cells for the TrellisNets in our
experiments and provide a detailed treatment of this nonlinearity in Section 5.1 and Appendix A.
4.3	TrellisNet as a Bridge Between Recurrent and Convolutional Models
In Section 4.1 we concluded that TrellisNet is a special kind of TCN, characterized by weight tying
and input injection. In Section 4.2 we established that TrellisNet is a generalization of truncated
6
Published as a conference paper at ICLR 2019
RNNs. These connections along with the construction in our proof of Theorem 1 allow TrellisNets
to benefit significantly from techniques developed originally for RNNs, while also incorporating ar-
chitectural and algorithmic motifs developed for convolutional networks. We summarize a number
of techniques here. From recurrent networks, we can integrate 1) structured nonlinear activations
(e.g. LSTM and GRU gates); 2) variational RNN dropout (Gal & Ghahramani, 2016); 3) recurrent
DropConnect (Merity et al., 2018b); and 4) history compression and repackaging. From convolu-
tional networks, we can adapt 1) larger kernels and dilated convolutions (Yu & Koltun, 2016); 2)
auxiliary losses at intermediate layers (Lee et al., 2015; Xie & Tu, 2015); 3) weight normaliza-
tion (Salimans & Kingma, 2016); and 4) parallel convolutional processing. Being able to directly
incorporate techniques from both streams of research is one of the benefits of trellis networks. We
leverage this in our experiments and provide a more comprehensive treatment of these adaptations
in Appendix B.
5	Experiments
5.1	A TRELLISNETWITH GATED ACTIVATION
based on the LSTM cell.
In our description of generic trellis networks in Section 3, the
activation function f can be any nonlinearity that computes
z(i+1) based on z(i+1) and z(iT_「In experiments, We use a
gated activation based on the lStm cell. Gated activations have
been used before in convolutional networks for sequence model-
ing (van den Oord et al., 2016; Dauphin et al., 2017). Our choice
is inspired directly by Theorem 1, which suggests incorporating
an existing RNN cell into TrellisNet. We use the LSTM cell due
to its effectiveness in recurrent networks (Jozefowicz et al., 2015;
Greff et al., 2017; Melis et al., 2018). We summarize the construc-
tion here; a more detailed treatment can be found in Appendix A.
In an LSTM cell, three information-controlling gates are com-
puted at time t. Moreover, there is a cell state that does not par-
ticipate in the hidden-to-hidden transformations but is updated in
every step using the result from the gated activations. We integrate the LSTM cell into the TrellisNet
as follows (Figure 3):
z(+II)= W1
xt
(it)	+	W2
zt,2
[2t+1,1 zt+1,2
xt+1
z(++L1i = σ(zt+1,1) ◦ z(il + σ(zt+1,2) ◦ tanh(5t+1,3)
m=σ(zt+ι,4) ◦tanh(Z(+11))
zt+1,3 zt+1,4]>	(II)
(12; Gated activation f)
Thus the linear transformation in each layer of the TrellisNet produces a pre-activation feature
^t+ι with r = 4q feature channels, which are then processed by elementwise transformations and
Hadamard products to yield the final output zt(+i+11) = zt(+i+11,1),zt(+i+11,2) of the layer.
5.2	Results
We evaluate trellis networks on word-level and character-level language modeling on the standard
Penn Treebank (PTB) dataset (Marcus et al., 1993; Mikolov et al., 2010), large-scale word-level
modeling on WikiText-103 (WT103) (Merity et al., 2017), and standard stress tests used to study
long-range information propagation in sequence models: sequential MNIST, permuted MNIST
(PMNIST), and sequential CIFAR-10 (Chang et al., 2017; Bai et al., 2018; Trinh et al., 2018). Note
that these tasks are on very different scales, with unique properties that challenge sequence models
in different ways. For example, word-level PTB is a small dataset that a typical model easily over-
fits, so judicious regularization is essential. WT103 is a hundred times larger, with less danger of
overfitting, but with a vocabulary size of 268K that makes training more challenging (and precludes
the application of techniques such as mixture of softmaxes (Yang et al., 2018)). A more complete
description of these tasks and their characteristics can be found in Appendix C.
7
Published as a conference paper at ICLR 2019
Table 1: Test perplexities (ppl) on word-level language modeling with the PTB corpus. ` means
lower is better.
Word-level Penn Treebank (PTB)		
Model	Size	Test perplexity'
Generic TCN (Bai et al., 2018)	13M	88.68
Variational LSTM (Gal & Ghahramani, 2016)	66M	73.4
NAS Cell (Zoph & Le, 2017)	54M	62.4
AWD-LSTM (Merity et al., 2018b)	24M	58.8
(Black-box tuned) NAS (Melis et al., 2018)	24M	59.7
(Black-box tuned) LSTM + skip conn. (Melis et al., 2018)	24M	58.3
AWD-LSTM-MoC (Yang et al., 2018)	22M	57.55
DARTS (Liu et al., 2018)	23M	56.10
AWD-LSTM-MoS (Yang et al., 2018)	24M	55.97
ENAS (Pham et al., 2018)	24M	55.80
Ours - TrellisNet	24M	56.97
Ours - TrellisNet (1.4x larger)	33M	56.80
Ours - TrellisNet-MoS	25M	54.67
Ours - TrellisNet-MoS (1.4x larger)	34M	54.19
Table 2: Test perplexities (ppl) on word-level language modeling with the WT103 corpus.
	WOrd-IeVelWikiTeXt-103(WT103)		
Model	Size	Test perplexity'
LSTM (Grave et al., 2017b)	-	48.7
LSTM+continuous cache (Grave et al., 2017b)	-	40.8
Generic TCN (Bai et al., 2018)	150M	45.2
Gated Linear ConvNet (Dauphin et al., 2017)	230M	37.2
AWD-QRNN (Merity et al., 2018a)	159M	33.0
Relational Memory Core (Santoro et al., 2018)	195M	31.6
Ours - TrellisNet	180M	29.19
The prior state of the art on these tasks was set by completely different models, such as AWD-LSTM
on character-level PTB (Merity et al., 2018a), neural architecture search on word-level PTB (Pham
et al., 2018), and the self-attention-based Relational Memory Core on WikiText-103 (Santoro et al.,
2018). We use trellis networks on all tasks and outperform the respective state-of-the-art models
on each. For example, on word-level Penn Treebank, TrellisNet outperforms by a good margin the
recent results of Melis et al. (2018), which used the Google Vizier service for exhaustive hyper-
parameter tuning, as well as the recent neural architecture search work of Pham et al. (2018). On
WikiText-103, a trellis network outperforms by 7.6% the Relational Memory Core (Santoro et al.,
2018) and by 11.5% the thorough optimization work of Merity et al. (2018a).
Many hyperparameters we use are adapted directly from prior work on recurrent networks. (As
highlighted in Section 4.3, many techniques can be carried over directly from RNNs.) For others,
we perform a basic grid search. We decay the learning rate by a fixed factor once validation error
plateaus. All hyperparameters are reported in Appendix D, along with an ablation study.
Word-level language modeling. For word-level language modeling, we use PTB and WT103. The
results on PTB are listed in Table 1. TrellisNet sets a new state of the art on PTB, both with and
without mixture of softmaxes (Yang et al., 2018), outperforming all previously published results by
more than one unit of perplexity.
WT103 is 110 times larger than PTB, with vocabulary size 268K. We follow prior work and use
the adaptive softmax (Grave et al., 2017a), which improves memory efficiency by assigning higher
capacity to more frequent words. The results are listed in Table 2. TrellisNet sets a new state of
the art on this dataset as well, with perplexity 29.19: about 7.6% better than the contemporaneous
8
Published as a conference paper at ICLR 2019
Table 3: Test bits-per-character (bpc) on character-level language modeling with the PTB corpus.
Char-level PTB		
Model	Size	Test bpc'
Generic TCN (Bai et al., 2018)	3.0M	1.31
Independently RNN (Li et al., 2018)	12.0M	1.23
Hyper LSTM (Ha et al., 2017)	14.4M	1.219
NAS Cell (Zoph & Le, 2017)	16.3M	1.214
Fast-Slow-LSTM-2 (Mujika et al., 2017)	7.2M	1.19
Quasi-RNN (Merity et al., 2018a)	13.8M	1.187
AWD-LSTM (Merity et al., 2018a)	13.8M	1.175
Ours - TrellisNet	13.4M	1.158
Table 4: Test accuracies on long-range modeling benchmarks. h means higher is better.
Model	Seq. MNIST Test acc.h	Permuted MNIST Test acc.h	Seq. CIFAR-10 Test acc.h
Dilated GRU (Chang et al., 2017)	99.0	94.6	-
IndRNN (Li et al., 2018)	99.0	96.0	-
Generic TCN (Bai et al., 2018)	99.0	97.2	-
r-LSTM w/ Aux. Loss (Trinh et al., 2018)	98.4	95.2	72.2
Transformer (self-attention) (Trinh et al., 2018)	98.9	97.9	62.2
Ours - TrellisNet	99.20	98.13	73.42
self-attention-based Relational Memory Core (RMC) (Santoro et al., 2018). TrellisNet achieves this
better accuracy with much faster convergence: 25 epochs, versus 90 for RMC.
Character-level language modeling. When used for character-level modeling, PTB is a medium-
scale dataset with stronger long-term dependencies between characters. We thus use a deeper net-
work as well as techniques such as weight normalization (Salimans & Kingma, 2016) and deep
supervision (Lee et al., 2015; Xie & Tu, 2015). The results are listed in Table 3. TrellisNet sets a
new state of the art with 1.158 bpc, outperforming the recent results of Merity et al. (2018a) by a
comfortable margin.
Long-range modeling with Sequential MNIST, PMNIST, and CIFAR-10. We also evaluate the
TrellisNet for ability to model long-term dependencies. In the Sequential MNIST, PMNIST, and
CIFAR-10 tasks, images are processed as long sequences, one pixel at a time (Chang et al., 2017;
Bai et al., 2018; Trinh et al., 2018). Our model has 8M parameters, in alignment with prior work.
To cover the larger context, we use dilated convolutions in intermediate layers, adopting a common
architectural element from TCNs (Yu & Koltun, 2016; van den Oord et al., 2016; Bai et al., 2018).
The results are listed in Table 4. Note that the performance of prior models is inconsistent. The
Transformer works well on MNIST but fairs poorly on CIFAR-10, while r-LSTM with unsuper-
vised auxiliary losses achieves good results on CIFAR-10 but underperforms on Permuted MNIST.
TrellisNet outperforms all these models on all three tasks.
6	Discussion
We presented trellis networks, a new architecture for sequence modeling. Trellis networks form a
structural bridge between convolutional and recurrent models. This enables direct assimilation of
many techniques designed for either of these two architectural families. We leverage these connec-
tions to train high-performing trellis networks that set a new state of the art on highly competitive
language modeling benchmarks. Beyond the empirical gains, we hope that trellis networks will
serve as a step towards deeper and more unified understanding of sequence modeling.
There are many exciting opportunities for future work. First, we have not conducted thorough
performance optimizations on trellis networks. For example, architecture search on the structure of
the gated activation f may yield a higher-performing activation function than the classic LSTM cell
9
Published as a conference paper at ICLR 2019
we used (Zoph & Le, 2017; Pham et al., 2018). Likewise, principled hyperparameter tuning will
likely improve modeling accuracy beyond the levels we have observed (Melis et al., 2018). Future
work can also explore acceleration schemes that speed up training and inference.
Another significant opportunity is to establish connections between trellis networks and self-
attention-based architectures (Transformers) (Vaswani et al., 2017; Santoro et al., 2018; Chen et al.,
2018), thus unifying all three major contemporary approaches to sequence modeling. Finally, we
look forward to seeing applications of trellis networks to industrial-scale challenges such as machine
translation.
References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In International Conference on Learning Representations (ICLR),
2015.
Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional
and recurrent networks for sequence modeling. arXiv:1803.01271, 2018.
James Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher. Quasi-recurrent neural net-
works. In International Conference on Learning Representations (ICLR), 2017.
Shiyu Chang, Yang Zhang, Wei Han, Mo Yu, Xiaoxiao Guo, Wei Tan, Xiaodong Cui, Michael
Witbrock, Mark Hasegawa-Johnson, and Thomas Huang. Dilated recurrent neural networks. In
Neural Information Processing Systems (NIPS), 2017.
Mia Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey, George Foster,
Llion Jones, Niki Parmar, Noam Shazeer, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Mike
Schuster, Zhifeng Chen, Yonghui Wu, and Macduff Hughes. The best of both worlds: Com-
bining recent advances in neural machine translation. In Annual Meeting of the Association for
Computational Linguistics (ACL), 2018.
KyUnghyUn Cho, Bart Van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties
of neural machine translation: Encoder-decoder approaches. arXiv:1409.1259, 2014.
Ronan Collobert, Jason Weston, Leon Bottou, Michael Karlen, Koray KavUkcUoglu, and Pavel P.
Kuksa. Natural language processing (almost) from scratch. Journal of Machine Learning Re-
search (JMLR), 12, 2011.
Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le, and
Ruslan Salakhutdinov. Transformer-XL: Language modeling with longer-term dependency.
arXiv:1901.02860, 2019.
Yann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated
convolutional networks. In International Conference on Machine Learning (ICML), 2017.
Jeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venu-
gopalan, Trevor Darrell, and Kate Saenko. Long-term recurrent convolutional networks for visual
recognition and description. In Computer Vision and Pattern Recognition (CVPR), 2015.
Jeffrey L Elman. Finding structure in time. Cognitive Science, 14(2), 1990.
Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent
neural networks. In Neural Information Processing Systems (NIPS), 2016.
Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional
sequence to sequence learning. In International Conference on Machine Learning (ICML), 2017.
Edouard Grave, Armand Joulin, Moustapha Cisse, David Grangier, and Herve Jegou. Efficient
softmax approximation for GPUs. In International Conference on Machine Learning (ICML),
2017a.
Edouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language models with a
continuous cache. In International Conference on Learning Representations (ICLR), 2017b.
10
Published as a conference paper at ICLR 2019
Alex Graves. Supervised Sequence Labelling with Recurrent Neural Networks. Springer, 2012.
Alex Graves. Generating sequences with recurrent neural networks. arXiv:1308.0850, 2013.
Klaus Greff, RUPesh Kumar Srivastava, Jan Koutnlk, Bas R. SteUnebrink, and Jurgen Schmidhuber.
LSTM: A search space odyssey. IEEE Transactions on Neural Networks and Learning Systems,
28(10), 2017.
David Ha, Andrew Dai, and Quoc V Le. HyPerNetworks. In International Conference on Learning
Representations (ICLR), 2017.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural COmputatiOn, 9(8),
1997.
Rafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever. An empirical exploration of recurrent
network architectures. In InternatiOnal COnference On Machine Learning (ICML), 2015.
Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray
Kavukcuoglu. Neural machine translation in linear time. arXiv:1610.10099, 2016.
Andrej Karpathy and Fei-Fei Li. Deep visual-semantic alignments for generating image descrip-
tions. In COmputer VisiOn and Pattern RecOgnitiOn (CVPR), 2015.
Urvashi Khandelwal, He He, Peng Qi, and Dan Jurafsky. Sharp nearby, fuzzy far away: How neural
language models use context. In Annual Meeting Of the AssOciatiOn fOr COmputatiOnal Linguistics
(ACL), 2018.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-
nical report, University of Toronto, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. ImageNet classification with deep convo-
lutional neural networks. In Neural InfOrmatiOn PrOcessing Systems (NIPS), 2012.
Yann LeCun, Bernhard Boser, John S. Denker, Donnie Henderson, Richard E. Howard, Wayne
Hubbard, and Lawrence D. Jackel. Backpropagation applied to handwritten zip code recognition.
Neural COmputatiOn, 1(4), 1989.
Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu. Deeply-
supervised nets. In AISTATS, 2015.
Shuai Li, Wanqing Li, Chris Cook, Ce Zhu, and Yanbo Gao. Independently recurrent neural network
(IndRNN): Building a longer and deeper RNN. In COmputer VisiOn and Pattern RecOgnitiOn
(CVPR), 2018.
Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: Differentiable architecture search.
arXiv:1806.09055, 2018.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated
corpus of English: The Penn treebank. COmputatiOnal Linguistics, 19(2), 1993.
Gabor Melis, Chris Dyer, and Phil Blunsom. On the state of the art of evaluation in neural language
models. In InternatiOnal COnference On Learning RepresentatiOns (ICLR), 2018.
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture
models. In InternatiOnal COnference On Learning RepresentatiOns (ICLR), 2017.
Stephen Merity, Nitish Shirish Keskar, and Richard Socher. An analysis of neural language modeling
at multiple scales. arXiv:1803.08240, 2018a.
Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing LSTM
language models. In InternatiOnal COnference On Learning RepresentatiOns (ICLR), 2018b.
Tomas Mikolov, Martin Karafiat, LUkaS Burget, Jan Cernocky, and Sanjeev Khudanpur. Recurrent
neural network based language model. In Interspeech, 2010.
11
Published as a conference paper at ICLR 2019
John Miller and Moritz Hardt. When recurrent models don’t need to be recurrent. arXiv:1805.10369,
2018.
Yasumasa Miyamoto and Kyunghyun Cho. Gated word-character recurrent language model.
arXiv:1606.01700, 2016.
Asier Mujika, Florian Meier, and Angelika Steger. Fast-slow recurrent neural networks. In Neural
Information Processing Systems (NIPS), 2017.
Hieu Pham, Melody Y Guan, Barret Zoph, Quoc V Le, and Jeff Dean. Efficient neural architecture
search via parameters sharing. In International Conference on Machine Learning (ICML), 2018.
Tara N. Sainath, Oriol Vinyals, Andrew W. Senior, and Hasim Sak. Convolutional, long short-term
memory, fully connected deep neural networks. In International Conference on Acoustics, Speech
and Signal Processing (ICASSP), 2015.
Tim Salimans and Diederik P Kingma. Weight normalization: A simple reparameterization to ac-
celerate training of deep neural networks. In Neural Information Processing Systems (NIPS),
2016.
Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Theophane Weber,
Daan Wierstra, Oriol Vinyals, Razvan Pascanu, and Timothy Lillicrap. Relational recurrent neural
networks. In Neural Information Processing Systems (NIPS), 2018.
Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo.
Convolutional LSTM network: A machine learning approach for precipitation nowcasting. In
Neural Information Processing Systems (NIPS), 2015.
Ilya Sutskever, James Martens, and Geoffrey E. Hinton. Generating text with recurrent neural net-
works. In International Conference on Machine Learning (ICML), 2011.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
In Neural Information Processing Systems (NIPS), 2014.
Trieu H Trinh, Andrew M Dai, Thang Luong, and Quoc V Le. Learning longer-term dependencies
in RNNs with auxiliary losses. In International Conference on Machine Learning (ICML), 2018.
Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,
Nal Kalchbrenner, Andrew W. Senior, and Koray Kavukcuoglu. WaveNet: A generative model
for raw audio. arXiv:1609.03499, 2016.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Neural Information Processing
Systems (NIPS), 2017.
Subhashini Venugopalan, Marcus Rohrbach, Jeffrey Donahue, Raymond J. Mooney, Trevor Darrell,
and Kate Saenko. Sequence to sequence - video to text. In International Conference on Computer
Vision (ICCV), 2015.
Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image
caption generator. In Computer Vision and Pattern Recognition (CVPR), 2015.
Christoph Vogel and Thomas Pock. A primal dual network for low-level vision problems. In German
Conference on Pattern Recognition, 2017.
Alex Waibel, Toshiyuki Hanazawa, Geoffrey Hinton, Kiyohiro Shikano, and Kevin J Lang. Phoneme
recognition using time-delay neural networks. IEEE Transactions on Acoustics, Speech, and
Signal Processing, 37(3), 1989.
Paul J Werbos. Backpropagation through time: What it does and how to do it. Proceedings of the
IEEE, 78(10), 1990.
Saining Xie and Zhuowen Tu. Holistically-nested edge detection. In International Conference on
Computer Vision (ICCV), 2015.
12
Published as a conference paper at ICLR 2019
Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W. Cohen. Breaking the softmax
bottleneck: A high-rank RNN language model. International Conference on Learning Represen-
tations (ICLR), 2018.
Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. In Interna-
tional Conference on Learning Representations (ICLR), 2016.
Julian Georg Zilly, RUPesh Kumar Srivastava, Jan Koutnlk, and Jurgen Schmidhuber. Recurrent
highway networks. In International Conference on Machine Learning (ICML), 2017.
Barret ZoPh and Quoc V Le. Neural architecture search with reinforcement learning. In Interna-
tional Conference on Learning Representations (ICLR), 2017.
13
Published as a conference paper at ICLR 2019
A Expressing an LSTM AS A TrellisNet
(a) An atomic view
(b) A sequence view
Figure 4: A TrellisNet with an LSTM nonlinearity, at an atomic level and on a longer sequence.
Here we trace in more detail the transformation ofan LSTM into a TrellisNet. This is an application
of Theorem 1. The nonlinear activation has been examined in Section 5.1. We will walk through the
construction again here.
In each time step, an LSTM cell computes the following:
ft' = σ(Wf h('T) + Uf h(-ι)增=σ(Wih尸)+ UM-I) g(') = tanh(Wg h尸 + Ug M-I)
Oy) = σ(Woh尸) + UoM-I)炉=ft()◦ c(-i + i(' ◦ M ht' = o(' ◦ tanh(ct'))	''
where h(t0) = xt , and ft , it , ot are typically called the forget, input, and output gates. By a similar
construction to how we defined τ in Theorem 1, to recover an LSTM the mixed group convolution
needs to produce 3q more channels for these gated outputs, which have the form ft,t0 , it,t0 and gt,t0
(see Figure 5 for an example). In addition, at each layer of the mixed group convolution, the network
also needs to maintain a group of channels for cell states ct,t0. Note that in an LSTM network, ct is
updated “synchronously” with ht , so we can similarly write
ct(,1t)0 = ft(,1t)0 ◦ ct(-1)1,t0 + i(t,1t)0 ◦ gt(,1t)0 ht(,1t)0 = ot(,1t)0 ◦ tanh(c(t,1t)0)	(14)
Based on these changes, we show in Figure 4 an atomic and a sequence view of TrellisNet with the
LSTM activation. The hidden units z1:T consist of two parts: z1:T,1, which gets updated directly
via the gated activations (akin to LSTM cell states), and z1:T,2, which is processed by parameterized
convolutions (akin to LSTM hidden states). Formally, in layer i:
Z(：TI) = Conv1D(z(:T,2； W) + X1:T = [z1:T,1	z1:T,2	z1:T,3	z1:T,4「
z(：T? = σ(Ztτ,ι) ◦ z(:T-1,1 + a(zi:T,2) ◦ tanh(Zib,3)
z1:T,2 = σ(Zi:T,4)◦ tanh(z1:T,1))
Figure 5: A 2-layer LSTM is expressed as a trellis network with mixed group convolutions on four
groups of feature channels. (Partial view.)
14
Published as a conference paper at ICLR 2019
B Optimizing and Regularizing TrellisNet with RNN and TCN
Methods
(a) History repackaging between truncated se-
quences in recurrent networks.
Figure 6: Using the equivalence established by Theorem 1, We can transfer the notion of history
repackaging in recurrent networks to trellis networks.
(b) History repackaging in mixed group convolutions, where
We write out Zt explicitly by Eq. (6).
in section 4, we formally described the relationship between TrellisNets, RNNs, and temporal con-
volutional networks (TCN). On the one hand, TrellisNet is a special TCN (with weight-tying and
input injection), while on the other hand it can also express any structured RNN via a sparse con-
volutional kernel. These relationships open clear paths for applying techniques developed for either
recurrent or convolutional networks. We summarize below some of the techniques that can be ap-
plied in this way to TrellisNet, categorizing them as either inspired by RNNs or TCNs.
B.1	From Recurrent Networks
History repackaging. One theoretical advantage of RNNs is their ability to represent a history of
infinite length. However, in many applications, sequence lengths are too long for infinite backprop-
agation during training. A typical solution is to partition the sequence into smaller subsequences
and perform truncated backpropagation through time (BPTT) on each. At sequence boundaries, the
hidden state ht is “repackaged” and passed onto the next RNN sequence. Thus gradient flow stops
at sequence boundaries (see Figure 6a). such repackaging is also sometimes used at test time.
We can now map this repackaging procedure to trellis networks. As shown in Figure 6, the notion of
passing the compressed history vector ht in an RNN corresponds to specific non-zero padding in the
mixed group convolution of the corresponding TrellisNet. The padding is simply the channels from
the last step of the final layer applied on the previous sequence (see Figure 6b, where without the
repackaging padding, at layer 2 we will have h(T1+) 1,T +1 instead of h(T1+) 1,1). We illustrate this in Figure
6b, where we have written out zt(i) in TrellisNet explicitly in the form of ht,t0 according to Eq. (6).
This suggests that instead of storing all effective history in memory, we can compress history in a
feed-forward network to extend its history as well. For a general TrellisNet that employs a dense
kernel, similarly, we can pass the hidden channels of the last step of the final layer in the previous
sequence as the “history” padding for the next TrellisNet sequence (this works in both training and
testing).
Gated activations. in general, the structured gates in RNN cells can be translated to gated acti-
vations in temporal convolutions, as we did in Appendix A in the case of an LsTM. While in the
experiments we adopted the LsTM gating, other activations (e.g. GRUs (Cho et al., 2014) or acti-
vations found via architecture search (zoph & Le, 2017)) can also be applied in trellis networks via
the equivalence established in Theorem 1.
RNN variational dropout. Variational dropout (VD) for RNNs (Gal & Ghahramani, 2016) is a
useful regularization scheme that applies the same mask at every time step within a layer (see Fig-
ure 7a). A direct translation of this technique from RNN to the group temporal convolution implies
that we need to create a different mask for each diagonal of the network (i.e. each history starting
point), as well as for each group of the mixed group convolution. We propose an alternative (and
extremely simple) dropout scheme for TrellisNet, which is inspired by VD in RNNs as well as Theo-
15
Published as a conference paper at ICLR 2019
Original Loss Lorig
---► Mask 1
---► Mask 2
—► Mask 3
——► Mask 4
——► Mask 5
(b) Auxiliary loss on intermediate layers
in a TrellisNet.
Figure 7: (a) RNN-inspired variational dropout. (b) ConvNet-inspired auxiliary losses.
(a) Left: variational dropout (VD) in an RNN. Right: VD in a
TrellisNet. Each color indicates a different dropout mask.
rem 1. In each iteration, we apply the same mask on the post-activation outputs, at every time step in
both the temporal dimension and depth dimension. That is, based on Eq. (6) in Theorem 1, we adapt
VD to the TrellisNet setting by assuming ht,t0±δ ≈ ht,t0; see Figure 7a. Empirically, we found this
dropout to work significantly better than other dropout schemes (e.g. drop certain channels entirely).
Recurrent weight dropout/DropConnect. We apply DropConnect on the TrellisNet kernel. Merity
et al. (2018b) showed that regularizing hidden-to-hidden weights Whh can be useful in optimizing
LSTM language models, and we carry this scheme over to trellis networks.
B.2	From Convolutional Networks
Dense convolutional kernel. Generalizing the convolution from a mixed group (sparse) convolution
to a general (dense) one means the connections are no longer recurrent and we are computing directly
on the hidden units with a large kernel, just like any temporal ConvNet.
Deep supervision. Recall that for sparse TrellisNet to recover truncated RNN, at each level the
hidden units are of the form ht,t0, representing the state at time t if we assume that history started at
time t0 (Eq. (6)). We propose to inject the loss function at intermediate layers of the convolutional
network (e.g. after every ` layers of transformations, where we call ` the auxiliary loss frequency).
For example, during training, to predict an output at time t with a L-layer TrellisNet, besides zt(L) in
the last layer, We can also apply the loss function on z(L-'), z(L-2'), etc. 一 where hidden units
will predict with a shorter history because they are at lower levels of the network. This had been
introduced for convolutional models in computer vision (Lee et al., 2015; Xie & Tu, 2015). The
eventual loss of the network will be
Ltotal
Lorig + λ ∙ Laux,
(15)
where λ is a fixed scaling factor that controls the weight of the auxiliary loss.
Note that this technique is not directly transferable (or applicable) to RNNs.
Larger kernel and dilations (Yu & Koltun, 2016). These techniques have been used in convolu-
tional networks to more quickly increase the receptive field. They can be immediately applied to
trellis networks. Note that the activation function f of TrellisNet may need to change if we change
the kernel size or dilation settings (e.g. with dilation d and kernel size 2, the activation will be
f (z1：T, z(iT-d)).
Weight normalization (Salimans & Kingma, 2016). Weight normalization (WN) is a technique
that learns the direction and the magnitude of the weight matrix independently. Applying WN on the
convolutional kernel was used in some prior works on temporal convolutional architectures (Dauphin
et al., 2017; Bai et al., 2018), and have been found useful in regularizing the convolutional filters
and boosting convergence.
Parallelism. Because TrellisNet is convolutional in nature, it can easily leverage the parallel pro-
cessing in the convolution operation (which slides the kernel across the input features). We note
that when the input sequence is relatively long, the predictions of the first few time steps will have
insufficient history context compared to the predictions later in the sequence. This can be addressed
by either history padding (mentioned in Appendix B.1) or chopping off the loss incurred by the first
few time steps.
16
Published as a conference paper at ICLR 2019
C Benchmark Tasks
Word-level language modeling on Penn Treebank (PTB). The original Penn Treebank (PTB)
dataset selected 2,499 stories from a collection of almost 100K stories published in Wall Street Jour-
nal (WSJ) (Marcus et al., 1993). After Mikolov et al. (2010) processed the corpus, the PTB dataset
contains 888K words for training, 70K for validation and 79K for testing, where each sentence is
marked with an <eos> tag at its end. All of the numbers (e.g. in financial news) were replaced
with a ? symbol with many punctuations removed. Though small, PTB has been a highly studied
dataset in the domain of language modeling (Miyamoto & Cho, 2016; Zilly et al., 2017; Merity
et al., 2018b; Melis et al., 2018; Yang et al., 2018). Due to its relatively small size, many compu-
tational models can easily overfit on word-level PTB. Therefore, good regularization methods and
optimization techniques designed for sequence models are especially important on this benchmark
task (Merity et al., 2018b).
Word-level language modeling on WikiText-103. WikiText-103 (WT103) is 110 times larger than
PTB, containing a training corpus from 28K lightly processed Wikipedia articles (Merity et al.,
2017). In total, WT103 features a vocabulary size of about 268K2, with 103M words for training,
218K words for validation, and 246K words for testing/evaluation. The WT103 corpus also retains
the original case, punctuation and numbers in the raw data, all of which were removed from the
PTB corpus. Moreover, since WT103 is composed of full articles (whereas PTB is sentence-based),
it is better suited for testing long-term context retention. For these reasons, WT103 is typically
considered much more representative and realistic than PTB (Merity et al., 2018a).
Character-level language modeling on Penn Treebank (PTB). When used for character-level
language modeling, PTB is a medium size dataset that contains 5M chracters for training, 396K for
validation, and 446K for testing, with an alphabet size of 50 (note: the <eos> tag that marks the
end of a sentence in word-level tasks is now considered one character). While the alphabet size of
char-level PTB is much smaller compared to the word-level vocabulary size (10K), there is much
longer sequential token dependency because a sentence contains many more characters than words.
Sequential and permuted MNIST classification. The MNIST handwritten digits dataset (LeCun
et al., 1989) contains 60K normalized training images and 10K testing images, all of size 28 × 28.
In the sequential MNIST task, MNIST images are presented to the sequence model as a flattened
784 × 1 sequence for digit classification. Accurate predictions therefore require good long-term
memory of the flattened pixels - longer than in most language modeling tasks. In the setting of
permuted MNIST (PMNIST), the order of the sequence is permuted at random, so the network can
no longer rely on local pixel features for classification.
Sequential CIFAR-10 classification. The CIFAR-10 dataset (Krizhevsky & Hinton, 2009) contains
50K images for training and 10K for testing, all of size 32 × 32. In the sequential CIFAR-10 task,
these images are passed into the model one at each time step, flattended as in the MNIST tasks.
Compared to sequential MNIST, this task is more challenging. For instance, CIFAR-10 contains
more complex image structures and intra-class variations, and there are 3 channels to the input.
Moreover, as the images are larger, a sequence model needs to have even longer memory than in
sequential MNIST or PMNIST (Trinh et al., 2018).
D Hyperparameters and Ablation Study
Table 5 specifies the trellis networks used for the various tasks. There are a few things to note while
reading the table. First, in training, we decay the learning rate once the validation error plateaus
for a while (or according to some fixed schedule, such as after 100 epochs). Second, for auxiliary
loss (see Appendix B for more details), we insert the loss function after every fixed number of
layers in the network. This “frequency” is included below under the “Auxiliary Frequency” entry.
Finally, the hidden dropout in the Table refers to the variational dropout we translated from RNNs
(see Appendix B), which is applied at all hidden layers of the TrellisNet. Due to the insight from
Theorem 1, many techniques in TrellisNet were translated directly from RNNs or TCNs. Thus,
most of the hyperparameters were based on the numbers reported in prior works (e.g. embedding
size, embedding dropout, hidden dropout, output dropout, optimizer, weight-decay, etc.) with minor
2As a reference, Oxford English Dictionary only contains less than 220K unique English words.
17
Published as a conference paper at ICLR 2019
adjustments (Merity et al., 2018b; Yang et al., 2018; Bradbury et al., 2017; Merity et al., 2018a;
Trinh et al., 2018; Bai et al., 2018; Santoro et al., 2018). For factors such as auxiliary loss weight
and frequency, we perform a basic grid search.
Table 5: Models and hyperparameters used in experiments. “-" means not applicable/used.
	Word-PTB (w/o MoS)	Word-PTB (w/ MoS)	Word-WT103	Char-PTB	(P)MNIST/CIFAR-10
Optimizer	SGD	SGD	Adam	Adam	Adam
Initial Learning Rate	20	20	1e-3	2e-3	2e-3
Hidden Size (i.e. ht)	1000	1000	2000	1000	100
Output Size (only for MoS)	-	480	-	-	-
# of Experts (only for MoS)	-	15	-	-	-
Embedding Size	400	280	512	200	-
Embedding Dropout	0.1	0.05	0.0	0.0	-
Hidden (VD-based) Dropout	0.28	0.28	0.1	0.3	0.2
Output Dropout	0.45	0.4	0.1	0.1	0.2
Weight Dropout	0.5	0.45	0.1	0.25	0.1
# of Layers	55	55	70	125	16
Auxiliary Loss λ	0.05	0.05	0.08	0.3	-
Auxiliary Frequency	16	16	25	70	-
Weight Normalization	-	-	!	!	!
Gradient Clip	0.225	0.2	0.1	0.2	0.5
Weight Decay	1e-6	1e-6	0.0	1e-6	1e-6
Model Size	24M	25M	180M	13.4M	8M
We have also performed an ablation study on TrellisNet to study the influence of various ingredients
and techniques on performance. The results are reported in Table 6. We conduct the study on word-
level PTB using a TrellisNet with 24M parameters. When we study one factor (e.g. removing hidden
dropout), all hyperparameters and settings remain the same as in column 1 of Table 5 (except for
“Dense Kernel”, where we adjust the number of hidden units so that the model size remains the
same).
Table 6: Ablation study on word-level PTB (w/o MoS)
	Model Size	Test ppl	∆ SOTA
TrellisNet	24.1M	56.97	-
- Hidden (VD-based) Dropout	24.1M	64.69	J 7.72
- Weight Dropout	24.1M	63.82	J 6.85
- Auxiliary Losses	24.1M	57.99	J 1.02
- Long Seq. Parallelism	24.1M	57.35	J 0.38
- Dense Kernel (i.e. mixed group conv)	24.1M	59.18	J 2.21
- Injected Input (every 2 layers instead)	24.1M	57.44	J 0.47
- Injected Input (every 5 layers instead)	24.1M	59.75	J 2.78
- Injected Input (every 10 layers instead)	24.1M	60.70	J 3.73
- Injected Input (every 20 layers instead)	24.1M	74.91	J 17.94
18