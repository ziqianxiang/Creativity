Published as a conference paper at ICLR 2019
A2BCD: Asynchronous Acceleration with Optimal
Complexity
Robert Hannah； Fei Feng, Wotao Yins
Department of Mathematics
University of California, Los Angeles
520 Portola Plaza, Los Angeles, CA 90095, USA
Abstract
In this paper, we propose the Asynchronous Accelerated Nonuniform Randomized
Block Coordinate Descent algorithm (A2BCD). We prove A2BCD converges linearly
to a solution of the convex minimization problem at the same rate as NU_ACDM,
so long as the maximum delay is not too large. This is the first asynchronous
Nesterov-accelerated algorithm that attains any provable speedup. Moreover, we
then prove that these algorithms both have optimal complexity. Asynchronous
algorithms complete much faster iterations, and A2BCD has optimal complexity.
Hence we observe in experiments that A2BCD is the top-performing coordinate
descent algorithm, converging up to 4 - 5× faster than NU_ACDM on some data
sets in terms of wall-clock time. To motivate our theory and proof techniques, we
also derive and analyze a continuous-time analogue of our algorithm and prove it
converges at the same rate.
1 Introduction
In this paper, we propose and prove the convergence of the Asynchronous Accelerated Nonuni-
form Randomized Block Coordinate Descent algorithm (A2BCD), the first asynchronous Nesterov-
accelerated algorithm that achieves optimal complexity. No previous attempts have been able to
prove a speedup for asynchronous Nesterov acceleration. We aim to find the minimizer x； of the
unconstrained minimization problem:
mind f (x) = f x(1)
,...,x (n))
x∈Rd
(1.1)
where f is σ-strongly convex for σ > 0 with L -Lipschitz gradient V f = (Vι f,..., V nf). x ∈ Rd is
composed of coordinate blocks x ⑴,...,x ⑺).The coordinate blocks of the gradient V if are assumed
Li -Lipschitz with respect to the ith block. That is, ∀x, h ∈ Rd:
IlV if (x + Pih) -V if (x )∣∣ ≤ LiIl h Il	(1.2)
where Pi is the projection onto the ith block of Rd. Let L，1 '∑n=ι Li be the average block
Lipschitz constant. These conditions on f are assumed throughout this whole paper. Our algorithm
can also be applied to non-strongly convex objectives (σ = 0) or non-smooth objectives using the
black box reduction techniques proposed in Allen-Zhu & Hazan (2016). Hence we consider only
* Corresponding author: RobertHannah89@gmail.com
^ fei.feng@math.ucla.edu
"wotaoyin@math.ucla.edu
1
Published as a conference paper at ICLR 2019
the coordinate smooth, strongly-convex case. Our algorithm can also be applied to the convex
regularized ERM problem via the standard dual transformation (see for instance Lin et al. (2014)):
1n	λ
f (x) = n^2fi Hai ,x〉)+ 2 IiX Ii
n i=1
(1.3)
Hence A2BCD can be used as an asynchronous Nesterov-accelerated finite-sum algorithm.
Coordinate descent methods, in which a chosen coordinate block ik is updated at every iteration,
are a popular way to solve equation 1.1. Randomized block coordinate descent (RBCD, Nesterov
(2012)) updates a uniformly randomly chosen coordinate block ik with a gradient-descent-like step:
xk+ι = xk 一 (1 /Lik )Vik f (xk). The complexity K(e) of an algorithm is defined as the number of
iterations required to decrease the error E(f (xk) 一 f (x*)) to less than e(f (xo) 一 f (x*)). Randomized
coordinate descent has a complexity of K(e) = O(n(L/σ)ln(1 /e)).
Using a series of averaging and extrapolation steps, accelerated RBCD Nesterov (2012) improves
RBCD's iteration complexity K(e) to
L/σ ln(1 /e)), which leads to much faster convergence
when L is large. This rate is optimal when all Li are equal Lan & Zhou (2015). Finally, using
a special probability distribution for the random block index ik , the non-uniform accelerated
coordinate descent method Allen-Zhu et al. (2015) (NU_ACDM) can further decrease the complexity
to O(E n=r Y Ljσ ln(1 /e)), which can be up to √n times faster than accelerated RBCD, since some
Li can be significantly smaller than L. NU_ACDM is the current state-of-the-art coordinate descent
algorithm for solving equation 1.1.
Our A2BCD algorithm generalizes NU_ACDM to the asynchronous-parallel case. We solve equation 1.1
with a collection of p computing nodes that continually read a shared-access solution vector y into
local memory then compute a block gradient V if, which is used to update shared solution vectors
(x, y, v ). Proving convergence in the asynchronous case requires extensive new technical machinery.
A traditional synchronous-parallel implementation is organized into rounds of computation: Every
computing node must complete an update in order for the next iteration to begin. However, this
synchronization process can be extremely costly, since the lateness of a single node can halt the entire
system. This becomes increasingly problematic with scale, as differences in node computing speeds,
load balancing, random network delays, and bandwidth constraints mean that a synchronous-parallel
solver may spend more time waiting than computing a solution.
Computing nodes in an asynchronous solver do not wait for others to complete and share their
updates before starting the next iteration. They simply continue to update the solution vectors with
the most recent information available, without any central coordination. This eliminates costly idle
time, meaning that asynchronous algorithms can be much faster than traditional ones, since they
have much faster iterations. For instance, random network delays cause asynchronous algorithms
to complete iterations Ω(ln(P)) time faster than synchronous algorithms at scale. This and other
factors that influence the speed of iterations are discussed in Hannah & Yin (2017a). However, since
many iterations may occur between the time that a node reads the solution vector, and the time
that its computed update is applied, effectively the solution vector is being updated with outdated
information. At iteration k, the block gradient Vik f is computed at a delayed iterate ^k defined as1:
yk = (( yk - j ( k,I))⑴，•••)( yk - j ( k,n))( n))
(1.4)
1 Every coordinate can be outdated by a different amount without significantly changing the proofs.
2
Published as a conference paper at ICLR 2019
1.1 Summary of Contributions
for delay parameters j(k, 1), . . . , j(k, n) ∈ N. Here j(k, i) denotes how many iterations out of date
coordinate block i is at iteration k. Different blocks may be out of date by different amounts, which
is known as an inconsistent read. We assume2 that j (k, i) ≤ τ for some constant τ < ∞.
Asynchronous algorithms were proposed in Chazan & Miranker (1969) to solve linear systems.
General convergence results and theory were developed later in Bertsekas (1983); Bertsekas &
Tsitsiklis (1997); Tseng et al. (1990); Luo & Tseng (1992; 1993); Tseng (1991) for partially and
totally asynchronous systems, with essentially-cyclic block sequence ik . More recently, there has
been renewed interest in asynchronous algorithms with random block coordinate updates. Linear
and sublinear convergence results were proven for asynchronous RBCD Liu & Wright (2015); Liu
et al. (2014); Avron et al. (2014), and similar was proven for asynchronous SGD Recht et al. (2011),
and variance reduction algorithms Reddi et al. (2015); Leblond et al. (2017); Mania et al. (2015);
Huo & Huang (2016), and primal-dual algorithms Combettes & Eckstein (2018).
There is also a rich body of work on asynchronous SGD. In the distributed setting, Zhou et al. (2018)
showed global convergence for stochastic variationally coherent problems even when the delays grow
at a polynomial rate. In Lian et al. (2018), an asynchronous decentralized SGD was proposed with
the same optimal sublinear convergence rate as SGD and linear speedup with respect to the number
of workers. In Liu et al. (2018), authors obtained an asymptotic rate of convergence for asynchronous
momentum SGD on streaming PCA, which provides insight into the tradeoff between asynchrony
and momentum. In Dutta et al. (2018), authors prove convergence results for asynchronous SGD
that highlight the tradeoff between faster iterations and iteration complexity. Further related work
is discussed in Section 4.
1.1 Summary of Contributions
In this paper, we prove that A2BCD attains NU_ACDM’s state-of-the-art iteration complexity to highest
order for solving equation 1.1, so long as delays are not too large (see Section 2). The proof is very
different from that of Allen-Zhu et al. (2015), and involves significant technical innovations and
complexity related to the analysis of asynchronicity.
We also prove that A2BCD (and hence NU_ACDM) has optimal complexity to within a constant factor
over a fairly general class of randomized block coordinate descent algorithms (see Section 2.1). This
extends results in Lan & Zhou (2015) to asynchronous algorithms with Li not all equal. Since
asynchronous algorithms complete faster iterations, and A2BCD has optimal complexity, we expect
A2BCD to be faster than all existing coordinate descent algorithms. We confirm with numerical
experiments that A2BCD is the current fastest coordinate descent algorithm (see Section 5).
We are only aware of one previous and one contemporaneous attempt at proving convergence results
for asynchronous Nesterov-accelerated algorithms. However, the first is not accelerated and relies
on extreme assumptions, and the second obtains no speedup. Therefore, we claim that our results
are the first-ever analysis of asynchronous Nesterov-accelerated algorithms that attains a speedup.
Moreover, our speedup is optimal for delays not too large3 .
The work of Meng et al. claims to obtain square-root speedup for an asynchronous accelerated SVRG.
In the case where all component functions have the same Lipschitz constant L, the complexity they
obtain reduces to (n + K)ln(1 /° for K = O(Tn2) (Corollary 4.4). Hence authors do not even obtain
accelerated rates. Their convergence condition is τ < 4∆bj8 for SParSity parameter ∆. Since the
dimension d satisfies d ≥ 夫,they require d ≥ 216T8. So T = 20 requires dimension d > 1015.
2This condition can be relaxed however by techniques in Hannah & Yin (2017b); Sun et al. (2017); Peng
et al. (2016c); Hannah & Yin (2017a)
3 Speedup is defined precisely in Section 2
3
Published as a conference paper at ICLR 2019
In a contemporaneous preprint, authors in Fang et al. (2018) skillfully devised accelerated schemes for
asynchronous coordinate descent and SVRG using momentum compensation techniques. Although
their complexity results have the improved ʌ/K dependence on the condition number, they do
not prove any speedup. Their complexity is τ times larger than the serial complexity. Since τ is
necessarily greater than p, their results imply that adding more computing nodes will increase running
time. The authors claim that they can extend their results to linear speedup for asynchronous,
accelerated SVRG under sparsity assumptions. And while we think this is quite likely, they have
not yet provided proof.
We also derive a second-order ordinary differential equation (ODE), which is the continuous-time
limit of A2BCD (see Section 3). This extends the ODE found in Su et al. (2014) to an asynchronous
accelerated algorithm minimizing a strongly convex function. We prove this ODE linearly converges
to a solution with the same rate as A2BCD’s, without needing to resort to the restarting techniques.
The ODE analysis motivates and clarifies the our proof strategy of the main result.
2 Main results
We should consider functions f where it is efficient to calculate blocks of the gradient, so that
coordinate-wise parallelization is efficient. That is, the function should be “coordinate friendly”
Peng et al. (2016b). This is a very wide class that includes regularized linear regression, logistic
regression, etc. The L2-regularized empirical risk minimization problem is not coordinate friendly in
general, however the equivalent dual problem is, and hence can be solved efficiently by A2BCD (see
Lin et al. (2014), and Section 5).
To calculate the k + 1’th iteration of the algorithm from iteration k, we use only one block of the
gradient V%七 f. We assume that the delays j (k,i) are independent of the block sequence ik, but
otherwise arbitrary (This is a standard assumption found in the vast majority of papers, but can be
relaxed Sun et al. (2017); Leblond et al. (2017); Cannelli et al. (2017)).
Definition 1. Asynchronous Accelerated Randomized Block Coordinate Descent
(A2BCD). Let f be σ-strongly convex, and let its gradient Vf be L-Lipschitz with block coor-
dinate Lipschitz parameters Li as in equation 1.2. We define the condition number K = L∕σ, and
let L = mini Li. Using these parameters, We sample ik in an independent and identically distributed
(IID) fashion according to
P[ik = j] = Lj/2/S, j ∈ {1 ,...,n},	for S =工i=1 L1 /2.	⑵I)
Let τ be the maximum asynchronous delay. We define the dimensionless asynchronicity parame-
ter ψ, Which is proportional to τ , and quantifies hoW strongly asynchronicity Will affect convergence:
ψ = 9(ST12L-1 /2L334K114) × T	(2.2)
We use the above system parameters and ψ to define the coefficients α, β, and γ via eqs. (2.3)
to (2.5). Hence A2BCD algorithm is defined via the iterations: eqs. (2.6) to (2.8).
α,	(1 + (1 + ψ) σ T / 2 S )-1	(2.3)	yk	αvk + (1 - α)xk,	(2.6)
β,	1 - (1 - ψ) σ1 / 2 S T	(2.4)	xk+1	yy- - hL—ik f (yk),	(2.7)
h,	1 - 1 σ1 / 2L-1 / 2 ψ.	(2.5)	vk+1	β βvk + (1 - β)yk - σT1 / L-//▽ ik f (yk).	(2.8)
See Section A for a discussion of why it is practical and natural to have the gradient Vιk f (yk) to be
outdated, While the actual variables xk, yk, vk can be efficiently kept up to date. Essentially it is
4
Published as a conference paper at ICLR 2019
because most of the computation lies in computing Vik f (^k). After this is computed, xk, yk, Vk can
be updated more-or-less atomically with minimal overhead, meaning that they will always be up to
date. However our main results still hold for more general asynchronicity.
A natural quantity to consider in asynchronous convergence analysis is the asynchronicity error,
a powerful tool for analyzing asynchronous algorithms used in several recent works Peng et al.
(2016a); Hannah & Yin (2017b); Sun et al. (2017); Hannah & Yin (2017a). We adapt it and use a
weighted sum of the history of the algorithm with decreasing weight as you go further back in time.
Definition 2. Asynchronicity error. Using the above parameters, we define:
τ	6	τ	i-j-1
Ak =工CjIlyk +1-j - yk-jll	(2.9) for Ci = SL1 /2K3/2T 工(1 - σ1 /2ST)	ψ-1. (2.10)
j=1	j=i
Here we define yk = y0 for all k < 0. The determination of the coefficients Ci is in general a very
involved process of trial and error, intuition, and balancing competing requirements. The algorithm
doesn’t depend on the coefficients, however; they are only an analytical tool.
We define Ek[X] as the expectation of X conditioned on (x0, . . . , xk), (y0, . . . , yk), (V0, . . . , Vk), and
(i o,..., ik-ι). To simplify notation4, We assume that the minimizer x * = 0, and that f (x *) = 0 with
no loss in generality. We define the Lyapunov function:
Pk = Il Vk ∣∣2 + Ak + cf( Xk)	(2.11)	for C = 2 σ-1 /2 S-1( βα-1(1 - α) + 1).	(2.12)
We now present this paper’s first main contribution.
Theorem 1. Let f be σ-strongly convex with a gradient Vf that is L-Lipschitz with block Lipschitz
constants {Li}乙.Let ψ defined in equation 2.2 satisfy ψ ≤ 7 (i.e. τ ≤ 吉S1 /2L1 /2L-3/4K-1 /4).
Then for A2BCD we have:
Ek [Pk +1] ≤(1 -(1 — ψ)σ1 /2 ST) ρk.
To obtain E[Pk] ≤ tρ0, it takes K⅛2bcd(e) iterations for:
KA2BCD(e) = (σT//S + O(1)) 1n-ψ),	(2.13)
where O(∙) is asymptotic with respect to σ-1 /2S → ∞, and uniformly bounded.
This result is proven in Section B. A stronger result for Li ≡ L can be proven, but this adds to the
complexity of the proof; see Section E for a discussion. In practice, asynchronous algorithms are far
more resilient to delays than the theory predicts. τ can be much larger without negatively affecting
the convergence rate and complexity. This is perhaps because we are limited to a worst-case analysis,
which is not representative of the average-case performance.
Allen-Zhu et al. (2015) (Theorem 5.1) shows a linear convergence rate of 1 - 2/(1 + 2σ-1 /2S)
for NU_ACDM, which leads to the corresponding iteration complexity of Knu_acdm(e) =
(σ-1 /2S + O(1)) ln(1 /e). Hence, we have:
KA2BCD(e) = 1----ψ (1 + o(I))KNU_ACDM(e)
4We can assume X* = 0 with no loss in generality since We may translate the coordinate system so that X* is
at the origin. We can assume f (X*) = 0 with no loss in generality, since we can replace f (X) with f (X) — f (X*).
Without this assumption, the Lyapunov function simply becomes: ∣∣Vk — X* ∣2 + Ak + C(f (Xk) — f (X*)).
5
Published as a conference paper at ICLR 2019
2.1 Optimality
When 0 ≤ ψ《1, or equivalently, when T《S1 /2L1 /2L-3/4KT/4, the complexity of A2BCD
asymptotically matches that of NU_ACDM. Hence A2BCD combines state-of-the-art complexity with
the faster iterations and superior scaling that asynchronous iterations allow. We now present some
special cases of the conditions on the maximum delay τ required for good complexity.
Corollary 3. Let the conditions of Theorem 1 hold. If all coordinate-wise Lipschitz constants Li are
equal (i.e. Li = Lι, ∀i), then We have Ka2bcd(e)〜Knu_acdm(e) When τ《n1 /2K-1 /4(Lι/L)3/4. IfWe
further assume all coordinate-wise LipSChitZ constants Li equal L. Then K⅛2bcd(e)〜Knu_acdm(C)=
KACDM (e), when T《n1 /2 K-1 /4.
Remark 1. Reduction to synchronous case. Notice that when τ = 0, we have ψ = 0, ci ≡ 0
and hence Ak ≡ 0. Thus A2BCD becomes equivalent to NU_ACDM, the Lyapunov function5 ρk becomes
equivalent to one found in Allen-Zhu et al. (2015)(pg. 9), and Theorem 1 yields the same complexity.
The maximum delay T will be a function T(p) of p, number of computing nodes. Clearly T ≥ p, and
experimentally it has been observed that T = O(p) Leblond et al. (2017). Let gradient complexity
K(e, τ) be the number of gradients required for an asynchronous algorithm with maximum delay T
to attain suboptimality e. T(1) = 0, since with only 1 computing node there can be no delay. This
corresponds to the serial complexity. We say that an asynchronous algorithm attains a complexity
speedup if RK[；'；(P) is increasing in P. We say it attains linear complexity speedup if PK(^T(P) = Ω(P).
In Theorem 1, we obtain a linear complexity speedup (for p not too large), whereas no other prior
attempt can attain even a complexity speedup with Nesterov acceleration.
In the ideal scenario where the rate at which gradients are calculated increases linearly with P,
algorithms that have linear complexity speedup will have a linear decrease in wall-clock time. However
in practice, when the number of computing nodes is sufficiently large, the rate at which gradients are
calculated will no longer be linear. This is due to many parallel overhead factors including too many
nodes sharing the same memory read/write bandwidth, and network bandwidth. However we note
that even with these issues, we obtain much faster convergence than the synchronous counterpart
experimentally.
2.1	Optimality
NU_ACDM and hence A2BCD are in fact optimal in some sense. That is, among a fairly wide class of
coordinate descent algorithms A, they have the best-possible worst-case complexity to highest order.
We extend the work in Lan & Zhou (2015) to encompass algorithms are asynchronous and have
unequal Li . For a subset S ∈ Rd, we let IC(S) (inconsistent read) denote the set of vectors v whose
components are a combination of components of vectors in the set S. That is, v = (v1,1, v2,2, . . . , vd,d)
for some vectors v1, v2, . . . , vd ∈ S. Here vi,j denotes the jth component of vector vi.
Definition 4. Asynchronous Randomized Incremental Algorithms. Consider the uncon-
strained minimization problem equation 1.1 for function f satisfying the conditions stated in Section
1. We define the class A as algorithms G on this problem such that:
1.	For each parameter set (σ, L1, . . . , Ln, n), G has an associated IID random variable ik with some
fixed distribution P[ik] = Pi for E>] Pi = 1.
2.	The iterates of A satisfy: Xk+ι ∈ span{IC(Xk)N,°f(IC(X0))Ni】f(IC(X1)),…Nikf(IC(Xk))}
This is a rather general class: xk+1 can be constructed from any inconsistent reading of past iterates
IC(Xk), and any past gradient of an inconsistent read Vj f (IC(Xj)).
5Their Lyapunov function is in fact a generalization of the one found in Nesterov (2012).
6
Published as a conference paper at ICLR 2019
Theorem 2.	For any algorithm G ∈ A that solves eq. (1.1), and parameter set (σ, L1, . . . , Ln, n),
there is a dimension d, a corresponding function f on Rd, and a starting point x0 , such that
EllXk- x*『/Ilxo - x*『≥ |(1 - 4/(工,√Li/σ + 2n))k
2	j=1
Hence A has a complexity lower bound: K(e) ≥ 4(1 + o(1))( E；= ʌ/L%∕σ + 2n) ln(1 /2e)
Our proof in Section D follows very similar lines to Lan & Zhou (2015); Nesterov (2013).
3 ODE Analysis
In this section we present and analyze an ODE which is the continuous-time limit of A2BCD. This
ODE is a strongly convex, and asynchronous version of the ODE found in Su et al. (2014). For
simplicity, assume Li = L, ∀i. We rescale (I.e. We replace f (x) with σ f.) f So that σ = 1, and
hence K = L∕σ = L. Taking the discrete limit of synchronous A2BCD (i.e. accelerated RBCD), We can
derive the following ODE6 (see Section equation C.1):
Y + 2 n T K T12 Y + 2 n-2 K-1V f (Y) = 0
(3.1)
We define the parameter η，nκ112, and the energy: E(t) = en IK -/t(f (Y) + ɪ ∣∣Y + nY『). This
is very similar to the Lyapunov function discussed in equation 2.11, with 1∣∣Y(t) + ηY(t)∣∣2 fulfilling
the role of Ilvk∣∣2, and Ak = 0 (since there is no delay yet). Much like the traditional analysis in the
proof of Theorem 1, we can derive a linear convergence result with a similar rate. See Section C.2.
Lemma 5. If Y satisfies equation 3.1, the energy satisfies E'(t) ≤ 0, E(t) ≤ E(0), and hence:
f(Y(t)) + 1∣∣Y(t) + nκ112Y(t)∣∣2 ≤(f(Y(0)) + 4∣∣Y(0) + ηY(0)∣∣2
e-n-1K-1/2t
We may also analyze an asynchronous version of equation 3.1 to motivate the proof of our main
theorem. Here Y(t) is a delayed version of Y(t) with the delay bounded by T.
Y + 2nT κT12 Y + 2n-2 K-1Vf (Y) = 0,	(3.2)
Unfortunately, this energy satisfies (see Section equation C.4, equation C.7):
e-η-tE，(t) ≤
12
-8η∣∣y∣∣ + 3κ2η 1 τD(t), for D(t)
,/	∣∣y(S)∣∣2ds.
Hence this energy E(t) may not be decreasing in general. But, we may add a continuous-time
asynchronicity error (see Sun et al. (2017)), much like in Definition 2, to create a decreasing
energy. Let c0 ≥ 0 and r > 0 be arbitrary constants that will be set later. Define:
A(t) = /	c(t — s)∣∣Y(s) ∣∣2ds, for c(t)，c0 (e-rt +
(eTr- I))
Lemma 6. When rτ ≤ ɪ, the asynchronicity error A(t) satisfies:
e Tt d (ertA (t)) ≤ C ο∣∣Y( t )∣∣2 -5 T C。D (t).
6
6For compactness, We have omitted the (t) from time-varying functions Y(t), Y(t), ▽ Y(t), etc.
7
Published as a conference paper at ICLR 2019
See Section C.3 for the proof. Adding this error to the Lyapunov function serves a similar purpose
in the continuous-time case as in the proof of Theorem 1 (see Lemma 11). It allows us to negate
1T-1Co units of D(t) for the cost of creating C0 units of ∣∣Y(t)∣∣ . This restores monotonicity.
Theorem 3.	Let co = 6K2η-1 τ2, and r = η-1. If T ≤ √∣nκ-1 /2 then We have:
e-η-1t £ (E(t) + eη-1t A(t)) ≤ 0.	(3.3)
Hence f (Y(t)) convergence linearly to f (x*) with rate O(exp(—t/(nκ1 /2)))
Notice how this convergence condition is similar to Corollary 3, but a little looser. The convergence
condition in Theorem 1 can actually be improved to approximately match this (see Section E).
Proof.
e-η-1td (E(t) + eη-1 tA(t)) ≤ 卜o- 1 η) ∣∣Y『+ 卜κ2ηTT- 1 TTCo)d(t)
=6η-1 κ2 卜2 — ɪn2κ-1) ||Y『≤ 0	口
The preceding should hopefully elucidate the logic and general strategy of the proof of Theorem 1.
4 Related work
We now discuss related work that was not addressed in Section 1. Nesterov acceleration is a
method for improving an algorithm’s iteration complexity’s dependence the condition number κ.
Nesterov-accelerated methods have been proposed and discovered in many settings Nesterov (1983);
Tseng (2008); Nesterov (2012); Lin et al. (2014); Lu & Xiao (2014); Shalev-Shwartz & Zhang (2016);
Allen-Zhu (2017), including for coordinate descent algorithms (algorithms that use 1 gradient block
V if or minimize with respect to 1 coordinate block per iteration), and incremental algorithms
(algorithms for finite sum problems n En=I fi(x) that use 1 function gradient Vf(x) per iteration).
Such algorithms can often be augmented to solve composite minimization problems (minimization
for objective of the form f(x) + g(x), especially for nonsomooth g), or include constraints.
In Peng et al. (2016a), authors proposed and analyzed an asynchronous fixed-point algorithm called
ARock, that takes proximal algorithms, forward-backward, ADMM, etc. as special cases. Work has
also been done on asynchronous algorithms for finite sums in the operator setting Davis (2016);
Johnstone & Eckstein (2018). In Hannah & Yin (2017b); Sun et al. (2017); Peng et al. (2016c);
Cannelli et al. (2017) showed that many of the assumptions used in prior work (such as bounded
delay T < ∞) were unrealistic and unnecessary in general. In Hannah & Yin (2017a) the authors
showed that asynchronous iterations will complete far more iterations per second, and that a wide
class of asynchronous algorithms, including asynchronous RBCD, have the same iteration complexity
as their synchronous counterparts. Hence certain asynchronous algorithms can be expected to
significantly outperform traditional ones.
In Xiao et al. (2017) authors propose a novel asynchronous catalyst-accelerated Lin et al. (2015)
primal-dual algorithmic framework to solve regularized ERM problems. They structure the parallel
updates so that the data that an update depends on is up to date (though the rest of the data may
not be). However catalyst acceleration incurs a log(κ) penalty over Nesterov acceleration in general.
In Allen-Zhu (2017), the author argues that the inner iterations of catalyst acceleration are hard to
tune, making it less practical than Nesterov acceleration.
8
Published as a conference paper at ICLR 2019
5	Numerical experiments
To investigate the performance of A2BCD, we solve the ridge regression problem. Consider the
following primal and corresponding dual objective (see for instance Lin et al. (2014)):
min P(W) =	ɪllATW	- l∖∖2 + λIlWIl2, min D(a)	= —2-IlAa∣∣2 +	ɪ∣∣ɑ + l『	(5.1)
w∈Rd	2n	2 α∈Rn	2d2λ	2d
where A ∈ Rd×n is a matrix of n samples and d features, and l is a label vector. We let A =
[A1 , . . . , Am] where Ai are the column blocks of A. We compare A2BCD (which is asynchronous
accelerated), synchronous NU_ACDM (which is synchronous accelerated), and asynchronous RBCD
(which is asynchronous non-accelerated). Nodes randomly select a coordinate block according to
equation 2.1, calculate the corresponding block gradient, and use it to apply an update to the
shared solution vectors. synchronous NU_ACDM is implemented in a batch fashion, with batch size p
(1 block per computing node). Nodes in synchronous NU_ACDM implementation must wait until all
nodes apply their computed gradients before they can start the next iteration, but the asynchronous
algorithms simply compute with the most up-to-date information available.
We use the datasets w1a (47272 samples, 300 features), wxa which combines the data from from
w1a to w8a (293201 samples, 300 features), and aloi (108000 samples, 128 features) from LIBSVM
Chang & Lin (2011). The algorithm is implemented in a multi-threaded fashion using C++11 and
GNU Scientific Library with a shared memory architecture. We use 40 threads on two 2.5GHz
10-core Intel Xeon E5-2670v2 processors. See Section A.1 for a discussion of parameter tuning and
estimation. The parameters for each algorithm are tuned to give the fastest performance, so that a
fair comparison is possible.
A critical ingredient in the efficient implementation of A2BCD and NU_ACDM for this problem is the
efficient update scheme discussed in Lee & Sidford (2013b;a). In linear regression applications such
as this, it is essential to be able to efficiently maintain or recover Ay . This is because calculating
block gradients requires the vector AiT Ay, and without an efficient way to recover Ay , block gradient
evaluations are essentially 50% as expensive as full-gradient calculations. Unfortunately, every
accelerated iteration results in dense updates to yk because of the averaging step in equation 2.6.
Hence Ay must be recalculated from scratch.
However Lee & Sidford (2013a) introduces a linear transformation that allows for an equivalent
iteration that results in sparse updates to new iteration variables p and q. The original purpose of
this transformation was to ensure that the averaging steps (e.g. equation 2.6) do not dominate the
computational cost for sparse problems. However we find a more important secondary use which
applies to both sparse and dense problems. Since the updates to p and q are sparse coordinate-block
updates, the vectors Ap, and Aq can be efficiently maintained, and therefore block gradients can be
efficiently calculated. The specifics of this efficient implementation are discussed in Section A.2.
In Table 5, we plot the sub-optimality vs. time for decreasing values of λ, which corresponds to
increasingly large condition numbers κ. When κ is small, acceleration doesn’t result in a significantly
better convergence rate, and hence A2BCD and async-RBCD both outperform sync-NU_ACDM since they
complete faster iterations at similar complexity. Acceleration for low κ has unnecessary overhead,
which means async-RBCD can be quite competitive. When κ becomes large, async-RBCD is no
longer competitive, since it has a poor convergence rate. We observe that A2BCD and sync-NU_ACDM
have essentially the same convergence rate, but A2BCD is up to 4 - 5× faster than sync-NU_ACDM
because it completes much faster iterations. We observe this advantage despite the fact that we
are in an ideal environment for synchronous computation: A small, homogeneous, high-bandwidth,
low-latency cluster. In large-scale heterogeneous systems with greater synchronization overhead,
bandwidth constraints, and latency, we expect A2BCD’s advantage to be much larger.
9
Published as a conference paper at ICLR 2019
Table 1: Sub-optimality f (yk) - f (X*) (y-axis) Vs time in seconds (x-axis) for A2BCD, synchronous
NU_ACDM, and asynchronous RBCD for data sets w1a, wxa and aloi for various values of λ.
6	Acknowledgement
The authors would like to thank the reviewers for their helpful comments. The research presented
in this paper was supported in part by AFOSR MURI FA9550-18-10502, NSF DMS-1720237, and
ONR N0001417121.
References
Zeyuan Allen-Zhu. Katyusha: The First Direct Acceleration of Stochastic Gradient Methods. In
Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2017,
pp. 1200-1205, New York, NY, USA, 2017. ACM.
Zeyuan Allen-Zhu and Elad Hazan. Optimal Black-Box Reductions Between Optimization Objectives.
arXiv:1603.05642, March 2016.
Zeyuan Allen-Zhu, Zheng Qu, Peter Richtarik, and Yang Yuan. Even Faster Accelerated Coordinate
Descent Using Non-Uniform Sampling. arXiv:1512.09103, December 2015.
Yossi Arjevani. Limitations on Variance-Reduction and Acceleration Schemes for Finite Sums
Optimization. In Advances in Neural Information Processing Systems 30, pp. 3540-3549. Curran
Associates, Inc., 2017.
H. Avron, A. Druinsky, and A. Gupta. Revisiting asynchronous linear solvers: Provable convergence
rate through randomization. In Paral lel and Distributed Processing Symposium, 2014 IEEE 28th
International, pp. 198-207, May 2014.
Dimitri P. Bertsekas. Distributed asynchronous computation of fixed points. Mathematical Pro-
gramming, 27(1):107-120, 1983.
10
Published as a conference paper at ICLR 2019
REFERENCES
Dimitri P. Bertsekas and John N. Tsitsiklis. Paral lel and Distributed Computation: Numerical
Methods. Athena Scientific, 1997.
Loris Cannelli, Francisco Facchinei, Vyacheslav Kungurtsev, and Gesualdo Scutari. Asynchronous
Parallel Algorithms for Nonconvex Big-Data Optimization. Part II: Complexity and Numerical
Results. arXiv:1701.04900, January 2017.
Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A Library for Support Vector Machines. ACM
Trans. Intell. Syst. TechnoI, 2⑶:27:1-27:27, May 2011.
D. Chazan and W. Miranker. Chaotic relaxation. Linear Algebra and its Applications, 2(2):199-222,
April 1969.
Patrick L. Combettes and Jonathan Eckstein. Asynchronous block-iterative primal-dual decompo-
sition methods for monotone inclusions. Mathematical Programming, 168(1-2):645-672, March
2018.
Damek Davis. SMART: The stochastic monotone aggregated root-finding algorithm. arX-
iv:1601.00698, January 2016.
Sanghamitra Dutta, Gauri Joshi, Soumyadip Ghosh, Parijat Dube, and Priya Nagpurkar. Slow
and Stale Gradients Can Win the Race: Error-Runtime Trade-offs in Distributed SGD. arX-
iv:1803.01113 [cs, stat], March 2018.
Cong Fang, Yameng Huang, and Zhouchen Lin. Accelerating Asynchronous Algorithms for Convex
Optimization by Momentum Compensation. arXiv:1802.09747 [cs, math], February 2018.
Robert Hannah and Wotao Yin. More Iterations per Second, Same Quality - Why Asynchronous
Algorithms may Drastically Outperform Traditional Ones. arXiv:1708.05136, August 2017a.
Robert Hannah and Wotao Yin. On Unbounded Delays in Asynchronous Parallel Fixed-Point
Algorithms. Journal of Scientific Computing, pp. 1-28, December 2017b.
Zhouyuan Huo and Heng Huang. Asynchronous Stochastic Gradient Descent with Variance Reduction
for Non-Convex Optimization. arXiv:1604.03584, April 2016.
Patrick R. Johnstone and Jonathan Eckstein. Projective Splitting with Forward Steps: Asynchronous
and Block-Iterative Operator Splitting. arXiv:1803.07043 [cs, math], March 2018.
Guanghui Lan and Yi Zhou. An optimal randomized incremental gradient method. arXiv:1507.02000,
July 2015.
Remi Leblond, Fabian Pedregosa, and Simon Lacoste-Julien. ASAGA: Asynchronous Parallel SAGA.
In Proceedings of the 20th International Conference on Artificial Intel ligence and Statistics, pp.
46-54, April 2017.
Y. T. Lee and A. Sidford. Efficient Accelerated Coordinate Descent Methods and Faster Algorithms
for Solving Linear Systems. In 2013 IEEE 54th Annual Symposium on Foundations of Computer
Science, pp. 147-156, October 2013a.
Yin Tat Lee and Aaron Sidford. Efficient Accelerated Coordinate Descent Methods and Faster
Algorithms for Solving Linear Systems. arXiv:1305.1922, May 2013b.
Xiangru Lian, Wei Zhang, Ce Zhang, and Ji Liu. Asynchronous Decentralized Parallel Stochastic
Gradient Descent. In International Conference on Machine Learning, pp. 3043-3052, July 2018.
11
Published as a conference paper at ICLR 2019
REFERENCES
Hongzhou Lin, Julien Mairal, and Zaid Harchaoui. A Universal Catalyst for First-Order Optimization.
arXiv:1506.02186, June 2015.
Qihang Lin, Zhaosong Lu, and Lin Xiao. An Accelerated Proximal Coordinate Gradient Method
and its Application to Regularized Empirical Risk Minimization. arXiv:1407.1296, July 2014.
J. Liu and S. Wright. Asynchronous stochastic coordinate descent: Parallelism and convergence
properties. SIAM Journal on Optimization, 25(1):351-376, January 2015.
Ji Liu, Steve Wright, Christopher Re, Victor Bittorf, and Srikrishna Sridhar. An Asynchronous
Parallel Stochastic Coordinate Descent Algorithm. In International Conference on Machine
Learning, pp. 469-477, January 2014.
Tianyi Liu, Shiyang Li, Jianping Shi, Enlu Zhou, and Tuo Zhao. Towards Understanding Acceleration
Tradeoff between Momentum and Asynchrony in Nonconvex Stochastic Optimization. In Advances
in Neural Information Processing Systems 32. Curran Associates, Inc., 2018.
Zhaosong Lu and Lin Xiao. On the complexity analysis of randomized block-coordinate descent
methods. Mathematical Programming, 152(1-2):615-642, August 2014.
Z. Q. Luo and P. Tseng. On the convergence of the coordinate descent method for convex differentiable
minimization. Journal of Optimization Theory and Applications, 72(1):7-35, January 1992.
Zhi-Quan Luo and Paul Tseng. On the convergence rate of dual ascent methods for linearly
constrained convex minimization. Mathematics of Operations Research, 18(4):846-867, November
1993.
Horia Mania, Xinghao Pan, Dimitris Papailiopoulos, Benjamin Recht, Kannan Ramchandran,
and Michael I. Jordan. Perturbed Iterate Analysis for Asynchronous Stochastic Optimization.
arXiv:1507.06970, July 2015.
Qi Meng, Wei Chen, Jingcheng Yu, Taifeng Wang, Zhi-Ming Ma, and Tie-Yan Liu. Asynchronous
Accelerated Stochastic Gradient Descent.
Y. Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems. SIAM
Journal on Optimization, 22(2):341-362, January 2012.
Yurii Nesterov. A method of solving a convex programming problem with convergence rate O (1/k2).
In Soviet Mathematics Doklady, volume 27, pp. 372-376, 1983.
Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Springer Science &
Business Media, December 2013.
Z. Peng, Y. Xu, M. Yan, and W. Yin. ARock: An Algorithmic Framework for Asynchronous Parallel
Coordinate Updates. SIAM Journal on Scientific Computing, 38(5):A2851-A2879, January 2016a.
Zhimin Peng, Tianyu Wu, Yangyang Xu, Ming Yan, and Wotao Yin. Coordinate friendly structures,
algorithms and applications. Annals of Mathematical Sciences and Applications, 1(1):57-119,
2016b.
Zhimin Peng, Yangyang Xu, Ming Yan, and Wotao Yin. On the Convergence of Asynchronous
Parallel Iteration with Unbounded Delays. arXiv:1612.04425 [cs, math, stat], December 2016c.
Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild!: A lock-free approach to
parallelizing stochastic gradient descent. In Advances in Neural Information Processing Systems
24, pp. 693-701, 2011.
12
Published as a conference paper at ICLR 2019
REFERENCES
Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and AleX Smola. On Variance
Reduction in Stochastic Gradient Descent and its Asynchronous Variants. arXiv:1506.06840, June
2015.
Nicolas L. RouX, Mark Schmidt, and Francis R. Bach. A Stochastic Gradient Method with an
EXponential Convergence _Rate for Finite Training Sets. In Advances in Neural Information
Processing Systems 25, pp. 2663—2671. Curran Associates, Inc., 2012.
Shai Shalev-Shwartz and Tong Zhang. Accelerated proXimal stochastic dual coordinate ascent for
regularized loss minimization. Mathematical Programming, 155(1-2):105—145, January 2016.
Weijie Su, Stephen Boyd, and Emmanuel Candes. A Differential Equation for Modeling Nesterov’s
Accelerated Gradient Method: Theory and Insights. In Advances in Neural Information Processing
Systems 27, pp. 2510-2518. 2014.
Tao Sun, Robert Hannah, and Wotao Yin. Asynchronous Coordinate Descent under More Realistic
Assumptions. In Advances in Neural Information Processing Systems 30, pp. 6183-6191. 2017.
P. Tseng. On the rate of convergence of a partially asynchronous gradient projection algorithm.
SIAM Journal on Optimization, 1(4):603-619, November 1991.
P. Tseng, D. Bertsekas, and J. Tsitsiklis. Partially asynchronous, parallel algorithms for network flow
and other problems. SIAM Journal on Control and Optimization, 28(3):678-710, March 1990.
Paul Tseng. On accelerated proXimal gradient methods for conveX-concave optimization. Department
of Mathematics, University of Washington, Tech. Rep., 2008.
Lin Xiao, Adams Wei Yu, Qihang Lin, and Weizhu Chen. DSCOVR: Randomized Primal-Dual
Block Coordinate Algorithms for Asynchronous Distributed Optimization. arXiv:1710.05080,
October 2017.
Zhengyuan Zhou, Panayotis Mertikopoulos, Nicholas Bambos, Peter Glynn, Yinyu Ye, Li-Jia Li,
and Li Fei-Fei. Distributed Asynchronous Optimization with Unbounded Delays: How Slow Can
You Go? In International Conference on Machine Learning, pp. 5970-5979, July 2018.
13
Published as a conference paper at ICLR 2019
A Efficient Implementation
An efficient implementation will have coordinate blocks of size greater than 1. This to ensure the
efficiency of linear algebra subroutines. Especially because of this, the bulk of the computation for
each iteration is computing Vik f (^k), and not the averaging steps. Hence the computing nodes only
need a local copy of yk in order to do the bulk of an iteration’s computation. Given this gradient
Vikf (^k), updating yk and Vk is extremely fast (Xk can simply be eliminated). Hence it is natural
to simply store yk and Vk centrally, and update them when the delayed gradients Vik f (yk). Given
the above, a write mutex over (y, V) has minuscule overhead (which we confirm with experiments),
and makes the labeling of iterates unambiguous. This also ensures that Vk and yk are always up to
date when (y, V) are being updated. Whereas the gradient Vik f (yk) may at the same time be out of
date, since it has been calculated with an outdated version of yk . However a write mutex is not
necessary in practice, and does not appear to affect convergence rates or computation time. Also it
is possible to prove convergence under more general asynchronicity.
A.1 Parameter selection and tuning
When defining the coefficients, σ may be underestimated, and L, L1, . . . , Ln may be overestimated
if exact values are unavailable. Notice that xk can be eliminated from the above iteration, and the
block gradient Vik f (^k) only needs to be calculated once per iteration. A larger (or overestimated)
maximum delay τ will cause a larger asynchronicity parameter ψ , which leads to more conservative
step sizes to compensate.
To estimate ψ, one can first performed a dry run with all coefficient set to 0 to estimate τ . All
function parameters can be calculated exactly for this problem in terms of the data matrix and λ.
We can then use these parameters and this tau to calculate ψ . ψ and τ merely change the parameters,
and do not change execution patterns of the processors. Hence their parameter specification doesn’t
affect the observed delay. Through simple tuning though, we found that ψ = 0.25 resulted in good
performance.
In tuning for general problems, there are theoretical reasons why it is difficult to attain acceleration
without some prior knowledge of σ, the strong convexity modulus Arjevani (2017). Ideally σ is
pre-specified for instance in a regularization term. If the Lipschitz constants Li cannot be calculated
directly (which is rarely the case for the classic dual problem of empirical risk minimization objectives),
the line-search method discussed in Roux et al. (2012) Section 4 can be used.
A.2 Sparse update formulation
As mentioned in Section 5, authors in Lee & Sidford (2013a) proposed a linear transformation of
an accelerated RBCD scheme that results in sparse coordinate updates. Our proposed algorithm
can be given a similar efficient implementation. We may eliminate xk from A2BCD, and derive the
equivalent iteration below:
(yk+1 ʌ = ( 1 - αβ,	αβ ∖(	yk	\_	f	aσ--1 /2 L-1 / 2 + h (1 - α) Li) V fyk)
∖ vk+1 )	∖ 1 - e，	β J∖vk	J	[	卜T/2L- /2) Vik f (yk)
, C	yVkk	-Qk
14
Published as a conference paper at ICLR 2019
A.2 Sparse update formulation
where C and Qk are defined in the obvious way. Hence we define auxiliary variables pk , qk defined
via:
(yJ= Ck ( Ppk )	AI)
These clearly follow the iteration:
(pk++)(PPk) - C-(k+1) Qk	(A⑵
Since the vector Qk is sparse, we can evolve variables Pk , and qk in a sparse manner, and recover
the original iteration variables at the end of the algorithm via A.1.
The gradient of the dual function is given by:
▽ D( y ) = 1 (1 AT Ay + λ (y +1))
λd d
As mentioned before, it is necessary to maintain or recover Ayk to calculate block gradients. Since
Ayk can be recovered via the linear relation in equation A.1, and the gradient is an affine function,
we maintain the auxiliary vectors Apk and Aqk instead.
Hence we propose the following efficient implementation in Algorithm 1. We used this to generate
the results in Table 5. We also note also that it can improve performance to periodically recover vk
and yk, reset the values of pk, qk, and C to vk, yk , and I respectively, and restarting the scheme
(which can be done cheaply in time O(d)).
We let B ∈ R2×2 represent Ck, and b represent B-1. 0 is the Kronecker product. Each computing
node has local outdated versions of p, q, Ap, Aq which We denote p, q, Ap, Aq respectively. We also
find it convenient to define:
=
- -
k1k2
DD
■ _
a。-112 J 1 /2 + h (I- a) L-
σ—1 / 2L— / 2
ik
(A.3)
15
Published as a conference paper at ICLR 2019
Algorithm 1 Shared-memory implementation of A2BCD
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
Inputs: Function parameters A, λ, L, {Li}in=1, n, d. Delay τ (obtained in dry run). Starting
vectors y, v .
Shared data: Solution vectors p, q; auxiliary vectors Ap, Aq; sparsifying matrix B
Node local data: Solution vectors p, q, auxiliary vectors Ap, Aq, SParsifying matrix B.
Calculate parameters ψ, α, β, h via 1. Set k = 0.
Initializations: p J y, q J v , Ap J Ay, Aq J Av, B J I .
while not converged, each computing node asynchronous do
Randomly select block i via equation 2.1.
Read shared data into local memory: P J P, q J q, Ap J Ap, Aq J Aq, B J B.
Compute block gradient: V if (y) = =(i
Compute quantity gi = AT V if (y)
Shared memory updates:
Λ ʌ	∖	. / ʌ . ʌ . \ \
,1 JAp + Bi, 2 A q) + λ^I3ι, 1P + Bi, 2 q))
Update B J
1 - αβ αβ
1-β β
D1k
D2k
× B, calculate inverse b — B-1.
qp	-=b
Increase iteration count: k J k + 1
③ Vif(y),
Ap
Aq
1 I
k1k2
DD
- -
end while
Recover original iteration variables: vy J B qp . Output y.
—
一:
b
⑤gi
B Proof of the main result
We first recall a couple of inequalities for convex functions.
Lemma 7. Let f be σ-strongly convex with L-Lipschitz gradient. Then we have:
f (y) ≤	f (x)	+〈y	— x, Vf (x)〉+ 1 Llly — x∣∣2,	∀x, y	(B.1)
f (y) ≥	f (x)	+〈y	— x, Vf (x)〉+ 1 σIly — x∣∣2,	∀x, y	(B.2)
We also find it convenient to define the norm:
n
Is∣* = ∖ 工 L” ∣SiI2	(B.3)
i=1
16
Published as a conference paper at ICLR 2019
B.1 Starting point
B.l Starting point
First notice that using the definition equation 2.8 of Vk+ι We have:
Il Vk+1∣∣2 = Il βvk + (1 - β) yk『-2 L1 /2 L【y1 βvk + (1 - β) yk, ▽ ikf( yk ))+ σ-1 LmN 次〃 yk )『
EkIVk+1∣∣2 = Iβvk + (1- β)yk『-2σ-1 /2ST〈βvk + (1- β)yk, Vf (^k)〉	(B.4)
×--------V--------Z	X------------V-----------Z
A	B
n
+ S-%τ 工 L-1/2IlVif(yk)『
i=1
--------------------V---------Z
C
We have the folloWing general identity:
Ilβχ + (1 - β)yIl2 = βIχIl2 + (1 - β)IyIl2 - β(1 - β)IX - yIl2, ∀χ,y	(B.5)
It can also easily be verified from equation 2.6 that we have:
Vk = yk + Q-1(1 - Q)(yk - Xk)	(B.6)
Using equation B.5 on term A, equation B.6 on term B, and recalling the definition equation B.3 on
term C, we have from equation B.4:
EkIVk+1I2 = βIVkI2 + (1 - β)Iyk『一β(1 - β)IVk - yk『+ S-1 σ-1 /2∣∣Vf(^k)∣[	(B.7)
-2σ-1 /2STβQ-1(1 - Q)(yk - Xk, Vf (^k)〉- 2σ-1 /2S-1(yk, Vf (^k))
This inequality is our starting point. We analyze the terms on the second line in the next section.
B.2 The Cross Term
To analyze these terms, we need a small lemma. This lemma is fundamental in allowing us to deal
with asynchronicity.
Lemma 8. Let χ,A> 0. Let the delay be bounded by T. Then:
2 X T a 2 + 2 XT
Allyk- yk Il ≤
T
∑I yk +1-j- yk - j I2
j =1
Proof. See Hannah & Yin (2017a).
Lemma 9. We have:
□
1	OI	二	o
一(Vf (yk),yk〉≤ -f (yk) - 2σ(1 - ψ)∣∣yki + 万Lκψ 1T	yk+1-j - yk-j∏
(Vf (^k), Xk- yk〉≤ f (Xk) - f (yk)
(B.8)
(B.9)
T
+ ^Lα (1 — α)
-11 K-1 ψβHVk — yk∣∣2 + κψ-1 β-1 τ £||yk+ι-j — yk-j∣∣2
j=1
The terms in bold in equation B.8 and equation B.9 are a result of the asynchronicity, and are
identically 0 in its absence.
17
Published as a conference paper at ICLR 2019
B.3 Function-value term
Proof. Our strategy is	to separately analyze terms that appear in the traditional	analysis	of Nesterov
(2012), and the terms	that result from asynchronicity. We first prove equation	B.8:
-Bf (^k),yk〉= -Bf (yk),yk〉-〈▽ f (^k) - ▽ f(yk), yk〉
≤ -f( yk ) - 2 σ Il yk l∣2 + L Ilyk- yk Illl yk Il	(BIO)
equation B.10 follows	from strong convexity (equation B.2 with x = yk and y	= x*), and	the fact
that Vf is L-Lipschitz. The term due to asynchronicity becomes:
τ
∑I yk+1-j-yk-j I2
j=1
LIly)k- ykl∣lykIl ≤ 1 LK-1 ψIlykIl2 + 2Lκψ-1T
using Lemma 8 with X = κψ-1, A = ∣∣yk∣∣. Combining this with equation B.10 completes the proof
of equation B.8.
We now prove equation B.9:
〈Vf (yk), Xk — yk)=〈Vf (yk), Xk — yk〉+〈V f (^k) - Vf (yk), Xk — yk〉
≤ f (Xk) - f (yk) + LIl^k - ykIIIIXk - yk∣∣
≤ f(Xk) - f(yk)
+ 2L ( K-1 ψβα-1(1 - α)IXk - ykIl2 + κψ-1 β-1 α(1 - α)-1 T EIlyk+1-j - yk-j『
Here the last line follows from Lemma 8 with X = κψ-1β-1α(1 - α)-1, A = nXk - yk. We can
complete the proof using the following identity that can be easily obtained from equation 2.6:
yk - Xk = α(1 - α)-1(Vk - yk)	口
B.3 Function-value term
Much like Nesterov (2012), we need a f(Xk) term in the Lyapunov function (see the middle of page
357). However we additionally need to consider asynchronicity when analyzing the growth of this
term. Again terms due to asynchronicity are emboldened.
Lemma 10. We have:
E k f (Xk +1) ≤ f (yk) - 2 h(2 - h (1 + 1 σ1 / 2L-1 / 2 ψ)卜 TllVf( yk )∣∣2
τ
+ STLσ1 /2κψTT Enyk+1-j - yk-j H2
Proof. From the definition equation 2.7 of Xk+1 , we can see that Xk+1 - yk is supported on block
ik . Since each gradient block Vif is Li Lipschitz with respect to changes to block i, we can use
18
Published as a conference paper at ICLR 2019
B.4 Asynchronicity error
equation B.1 to obtain:
1	2
以Xk +ι) ≤ f (yk) +〈▽f (yk),χk+ι — yk〉+ 2LikIlXk+1 — yk∣
(from equation 2.7) = f(yk)-"/〈▽ ikf (yk)Zik f(^k))+ |h2L-1 ∣Vikf (^k)『
=f (yk )-hL-ι(V ik f (yk) - V ik f (^k), V ik f (^k ))- 2 h (2 - h) LmIV ikf( ^k )『
1
E k f (Xk +1) ≤ f (yk) - hS T £lJ/2(V if (yk) - V if (^k), V if (^k )〉一 - h (2 - h) S TllVf( ^k )「
i =1
(B.11)
Here the last line followed from the definition equation B.3 of the norm ∣∣∙∣∣*ι/2. We now analyze
the middle term:
n
-工 L-1 /2(V if (yk) - V if (^k), V if (^k ))
i=1
—
n
L > 4(V if (yk)-V if (^k)), £ L-1 / 4V if (^k))
i=1
Il W n
工 LT4 (V if (yk) - V if (^k ))11 区 LT 4V if (^k)
(Cauchy Schwarz) ≤
i=1	i=1
⅛∖ 1 /2 / n	∖ 1 /2
L-1/2IlV i f (yk)-V i f (^k )『)(工 L-I/2|IV if (^k )『)
CL ≤ Li, ∀ i and equation B.3) ≤ LT14∣V f (yk) - V f (^k )∣∣∣∣Vf( ^k )∣∣*
(Vf is L-Lipschitz) ≤ L-114Lllyk - ^kMlVf(yk)∣∣*
We then apply Lemma 8 to this with X = 2h-1 σ112L114κψ-1 ,A = ∣∣Vf (yk)∣∣* to yield:
n	τ
-£ L-1 /2(V if (yk) - V if (yk), V if (^k )〉≤ h-1 Lσ112 κψ-1T £| yk +― - yj『 (B.12)
i=1	j=1
+ -hσ112L-112 ψ∣Vf( ^k )「
Finally to complete the proof, we combine equation B.11, with equation B.12.	口
B.4 Asynchronicity error
The previous inequalities produced difference terms of the form ∣∣ yk +1-j — yk-j『.The following
lemma shows how these errors can be incorporated into a Lyapunov function.
Lemma 11. Let 0 < r < 1 and consider the asynchronicity error and corresponding coefficients:
∞
Ak =工CjIlyk +1-j - yk-j∣∣2
j=1
∞
Ci = £ rr-jTSj
j=i
19
Published as a conference paper at ICLR 2019
B.4 Asynchronicity error
This sum satisfies:
∞
E k [ Ak +1 - rAk ] = CIE k Il yk +1 - yk『-^^Sj ∣∣ yk +1- j - yk - j『
j =1
Remark 2. Interpretation. This result means that an asynchronicity error term Ak can negate
a series of difference terms — E∞Lr Sj ∣∣yk +1-j — yk-j ∣∣2 at the cost of producing an additional error
C1EkIyk+1 — yk∣∣2, while maintaining a convergence rate of r. This essentially converts difference
terms, which are hard to deal with, into a ∣∣yk+1 — yk∣∣2 term which can be negated by other terms
in the Lyapunov function. The proof is straightforward.
Proof.
∞	∞
Ek[Ak +1 — r Ak] = Ek): cj + 1 Il yk +1- j — yk- j ∣∣ — rEk)：Cj ∣∣ yk +1- j — yk- j ∣∣
j=0	j=1
∞
=c 1Ek Il yk +1 — yk Il + Ek):(cj + 1 — rcj )∣∣ yk +1- j — yk- j ∣∣
j=1
Noting the following completes the proof:
∞	∞
Ci+1 — rc =工 ri+1-j-1Sj— r 工 ri-j-1Sj = — si	□
j=i +1	j=i
Given that Ak allows US to negate difference terms, We now analyze the cost C1Ek∣∣yk+1 — yk∣∣2 of
this negation.
Lemma 12. We have:
Ekllyk+1 — yk∣2 ≤ 2α2β2∣Vk — yk∣2 + 2S-1L-1∣Vf(^k)∣2
Proof.
yk +1 — yk = (αvk +1 +(1 — α) Xk +1) — yk
=α (βvk + (1 — β)yk — L1 /2L-1 /2Vik f (yk)) +(1 — α)(yk — hL^Vik f (^k)) — yk
(B.13)
=αβvk + α (1 — β) yk — aσ-1 /2 L-"2V ik f (yk) — ayk — (1 — α)hL-IV ik f (yk)
=αβ (Vk — yk) — (ασ-1 /2 L-1 /2 + h (1 — α) Likl)V if ^k)
∣yk+1 — yk∣2 ≤ 2α2β2∣Vk — yk∣2 + 2(ασ-1 /2L-/2 + h(1 — α)L- 1)2∣Vif ^k)∣2	(B.14)
20
Published as a conference paper at ICLR 2019
B.5 Master inequality
Here equation B.13 following from equation 2.8, the definition of vk+1. equation B.14 follows from
the inequality ∣∣x + y∣∣2 ≤ 2∣∣x∣∣2 + 2∣∣y∣∣2. The rest is simple algebraic manipulation.
Ilyk+1 - yk∣2 ≤ 2α2β2∣Vk- yk∣2 + 2L- (MT/2 + h(1 - α)L-/2)2∣Vitl f (^k)『
(L ≤ Li, ∀ i) ≤ 2 α 2 β 2∣ Vk- yk ∣2 + 2 L- (MT / 2 + h (1 - α )L-1 / 2)2IlV ik f (yk )∣2
=2α2β2∣Vk- yk∣2 + 2L-^1 (L/2L1 /2α + h(1 - α))2IlVitJ(^k)∣2
Ellyk+1 - ykI2 ≤ 2α2β2IVk- yk∣2 + 2STLT(L1 /2σ-1 /2α + h(1 - α))2∣Vf (^k)∣2
Finally, to complete the proof, We prove L1 /2 σ-1 /2 α + h (1 一 α) ≤ 1.
L1 / 2 σ T / 2 a + h (1 - α)
h + α(L1 /2 σ T /2 - h)
1
(definitions of h and α: equation 2.3, and equation 2.5)
≤1
-1 σ1 /2L-1 /2ψ + σ1 /2ST (L1 /2σ-1 /2)
-σ1 /2L-1 /2Qψ - σ-1 /2STLI)
(B.15)
Rearranging the definition of ψ, We have:
S-1 = ' ψ 2L1L-3 / 2 K-1 / 2 τ-2
(T ≥1 and ψ ≤ 1) ≤ 击 L1L-3/2 K-1 /2
Using this on equation B.15, We have:
L1 /2ασ-1 /2 + h(1 - α) ≤ 1 - σ1 /2L-1 /2Qψ -七L1L/2κ-1 /2σ-1 /2L1)
=1 - σ 1 /2L-1 "QΨ - 1⅛(L/L)2)
(ψ ≤ 1) = 1 - σ 1 /2L-1 /2( 24 - 182 ) ≤ 1 ∙
This completes the proof.	□
B.5 Master inequality
We are finally in a position to bring together all the all the previous results together into a master
inequality for the Lyapunov function ρk (defined in equation 2.11). After this lemma is proven, We
Will prove that the right hand size is negative, Which Will imply that ρk linearly converges to 0 With
rate β.
21
Published as a conference paper at ICLR 2019
B.5 Master inequality
Lemma 13. Master inequality.	We have:
Ek [Pk +1 - βPk] ≤ + H yk H2 + HVk - yk『	× (1 - β - σ-172S-1 σ(1 - ψ))	(B.16) × β(2a2βc 1 + S-1 βL172 K-172ψ - (1 - β))
+ f(yk)	× (c — 2σ-172 S-1 (βɑ-1(1 — ɑ) + 1))
+ f (Xk) T + ɪ2Hyk +1-j - yk-j H2 j=1 + HV f (^k )H2	× β(2 σ-17 2 S -1α-1(1 — a) — c) × S-1 Lκψ-1 τσ172 (2σ-1 + c) — S × S-1 (σ-1 + 2L-1 c 1 - 1 ch(2 - h (1 + ∣σ172 L-172ψ)))
Proof.
EkIlVk+ι『-βHVk『
(B.7)	= (1 - β)∣∣yk『一β(1 - β)∣Vk - yk『+ S-%-1∣Nf(^k)H2
-2厂1 /2St〈yk, Vf (^k))
-2σ-1 72STβα-1(1 - α)(yk - Xk, Vf (^k))
≤ (1 - β)Hyk『一β(1 - β)HVk - ykH2 + STσ-1∣∣Vf(^k)∣2	(B.17)
(B.8)	+ 2σ-172S-1 ( -f (yk) - 2σ(I- ψ)∣∣yk12 + 2LκψTτ£||yk+ι-j - yk-j12 J
	22 j=1
(B.9)	- 2σ-1 /2S-1 βα-1(1 - α)(f (Xk) - f (yk))
+ σ-1 /2S-1 βL 卜-1 ψβHVk - ykH2 + κψ-1 β-1 τ g∣yk +i-j - yk-j∣2
We now collect and organize the similar terms of this inequality.
≤ + H yk H2 + HVk - yk ∣∣2 -f ( yk ) + f (Xk) T	× (1 — β — σ-172 S-1 σ(1 — ψ)) × β (σ-17 2 S-1 βLκ-1 ψ — (1 — β)) ×2σ-172S-1(βɑ-1(1 - α) + 1) ×2 σ-17 2 S-1 βα-1(1 — a)
+ £||yk +1-j - yk-j ∣∣2	×2σ-172S-1 Lκψ-1T
j=1 + HV f (^k )H：	× σ-1S-1
22
Published as a conference paper at ICLR 2019
B.6 Proof of main theorem
Now finally, we add the function-value and asynchronicity terms to our analysis. We use Lemma 11
is with r = 1 - σ1 /2 S-1, and
[s = 6 S-1L1 /2 K3/2 ψ-1 τ,	1 ≤ i ≤ T
0,	i > τ
(B.18)
Notice that this choice of si will recover the coefficient formula given in equation 2.9. Hence we
have:
Ek [cf(xk+1) + Ak+1 - β(cf(xk) + Ak)]
(Lemma 10) ≤ cf(yk) -
2Ch (2 - h (l + 1 σ1 /2L-1 /2ψ))sTll▽/(^k)∣∣2 - βcf(Xk)
(B.19)
τ
+ STLσ 1 /2κψTT £llyk+1-j - yk-jIl2
j=1
(Lemmas 11 and 12) + C1(2α2β2∣vk - yk∣∣2 + 2STLTll▽ f (^k)『)	(B.20)
∞
-Sj Il yk +1-j - yk-j l2 + Ak (r - β)
j=1
Notice Ak(r 一 β) ≤ 0. Finally, combining equation B.17 and equation B.19 completes the proof. □
In the next section, we will prove that every coefficient on the right hand side of equation B.16 is 0
or less, which will complete the proof of Theorem 1.
B.6 Proof of main theorem
Lemma 14. The coefficients of ∣∣ yk ∣∣2, f (yk), and Eτ=J yk+1-j — yk - j ∣∣2 in Lemma 13 are
non-positive.
Proof. The coefficient 1 — (1 — ψ)σ1 /2S-1 — β of ∣∣yk∣∣2 is identically 0 via the definition equation 2.4
of β. The coefficient C - 2σ-1 /2 S-1(βɑ-1(1 - a) + 1) of f (yk) is identically 0 via the definition
equation 2.12 of C.
First notice from the definition equation 2.12 of C:
c = 2σ-1 /2 S-1(βɑ-1(1 - Q) + 1)
(definitions of α, β) = 2σ-1 /2S-1 ((1 - σ1 /2S-1 (1 - ψ)) (1 + ψ)σ-1 /2S + 1)
=2 σ-1 / 2 S-1 ((1 + ψ) σ-1 / 2 S + ψ 2)
=2 σ-1((1 + ψ) + ψ2 σ1 /2 S-1)	(B.21)
C≤ 4σ-1	(B.22)
23
Published as a conference paper at ICLR 2019
B.6 Proof of main theorem
Here the last line followed since ψ ≤ 11 and σ1 /2 S-1 ≤ 1. We now analyze the coefficient of
Στ =1H yk +1-j — yk - j ∣∣2∙
S-1 Lκψ-1 τσ1 /2(2σ-1 + C) — S
(B,22) ≤ 6L1 /2K3/2ψ-1T — s
(definition equation B.18 of S) ≤ 0	□
Lemma 15. The coefficient β(2σ-1 /2 S-1α-1(1 — α) — C) of f (Xk) in Lemma 13 is non-positive.
Proof.
2 σ-1 / 2 S Ta-1(1 — α) — C
(B.21) = 2 σ-1 / 2 S-1(1 + ψ) σ-1 / 2 S — 2 σ-1 ((1 + ψ) + ψ 2 σ1 / 2 S T)
=2 σ-1 ((1 + ψ) — ((1+ Ψ) + ψ2 σ1 /2 S T))
=—2ψ2σ-1 /2S-1 ≤ 0	□
Lemma 16. The coefficient S-1 (σ-1 + 2L-1 c 1 — 1 Ch(2 — h (1 + 1 σ1 /2L-1 /2ψ))) of ∣∣Vf (^k)∣∣2
in Lemma 13 is non-positive.
Proof. We first need to bound 5.
(equation B.18 and equation 2.9) C1
equation B.18
τ	_ .
=S ∑(1 - σ1 /2 S-1)-j
j=1
τ	-j
≤ 6S-1L1 /2K3/2ψ-1 τ 工(1 — σ1 /2 ST)-
j=1
≤ 6S-1L1 /2K3/2ψ-1 τ2 (1 — σ1 /2ST)-T
It can be easily verified that if x ≤ 2 and y ≥ 0, then (1 — x)-y ≤ exp(2xy). Using this fact with
X=σ1 / 2 S-1 and y = τ, we have:
(since ψ ≤ 3/7 and hence τσ1 /2 S-1
C1
6S-1L1 /2κ3/2ψ-1T2 exp τrσ1 /2ST)
S-1L1 /2K3/2ψ-1T2 × 6exp(；)
7 S-1L1 / 2 K 3 / 2 ψ-1 τ 2
(B.23)
≤ 7)
≤
≤
≤
24
Published as a conference paper at ICLR 2019
B.6 Proof of main theorem
We now analyze the coefficient of ∣∣Vf (^)，
+ 2L-1cι - 1 Ch(2 - h (1 +
LT12 ψ))
(B.23 and 2.5)
≤ σ-1 + 14STLTL112κ312ψ-1 τ2 - ∣Ch (1 + +ILTψ2)
≤ σ-1 + 14STLT L112 K332 ψT τ2 - 1 Ch
(definition 2.2 of ψ)
σ-1 + —σ-1 ψ - -ch
81	"	2
(B.21, definition 2.5 of h)
(σ112L-112 ≤ 0 and σ112S-1 ≤ 1)
σ-1
(1 + 84ψ - ((1+ ψ) + ψ2σ112ST) (1 - 1 σ112L-112ψ))
≤ σT(I + 14ψ - (1 + ψ) (1 - 1 ψ))
σ
1
1
—σ
2
1 / 2
-1	14	1	1
σ ψ∖81 + 2ψ - 2)
(ψ ≤ 2) ≤ 0
□
Lemma 17. The coefficient β(2α2βc 1 + S-1 βL112K-112ψ — (1 — β)) of IIvk — yk∣∣2 in 13 is non-
positive.
Proof.
2a2 βc 1 + σ112 S-1 βψ - (1 - ψ) σ112 S-1
(B.23) ≤ 14α2βS-1L112K312ψ-1 τ2 + σ112S-1 βψ - (1 - ψ)σ112S-1
≤ 14σS-3L112 κ312ψ-1 τ2 + σ112 S-1 ψ - (1 - ψ)σ112 S-1
=σ112 S-1(14S-2Lκτ2ψ-1 +2ψ - 1)
Here the last inequality follows since β ≤ 1 and α ≤ σ112 S-1. We now rearrange the definition of ψ
to yield the identity:
S-2 K =A L2 L-3 τ-4 ψ 4
Using this, we have:
14 S-2 Lκτ 2 ψ-1 + 2 ψ — 1
=94 L2 L-2 ψ3 τ-2 + 2 ψ -1
14 /3、3 _2	6
≤ 94 (7) 1	+7 - 1 ≤ 0
Here the last line followed since L ≤ L, ψ ≤ 7, and T ≥ 1. Hence the proof is complete.	口
Proof of Theorem 1. Using the master inequality 13 in combination with the previous Lemmas 14,
15, 16, and 17, we have:
Ek [Pk +1] ≤ βpk = (1 - (1 - ψ)σ112S-1) Pk
25
Published as a conference paper at ICLR 2019
When we have:
(1 -(1 - ψ)σ1 /2Sτ)k ≤ e
then the Lyapunov function Pk has decreased below eρo in expectation. Hence the complexity K(E)
satisfies:
K(E)ln(l - (1 - ψ)σ1 /2ST) = ln(E)
-1
K(E ) = ln(1 - (1 - ψ)σ1 /2ST) In(I /)
Now it can be shown that for 0 < x ≤ ɪ, we have:
1
-—1
x
-1
ln(1 - x)
-1	1	1
≤ ln(1 - x) ≤ x - 2
= 1 + O(1)
x
SinCe n ≥ 2, we have σ 1/2ST ≤ 1. Henee:
K(E) = 1-ψ 卜T/2 S + O(1)) ln(1 /)
An expression for Knu_acdm(E), the complexity of NU_ACDM follows by similar reasoning.
KNU_ACDM(E)=卜 T /2 S + O(1))ln(1 IE)
Finally we have:
S _	1	(σ-1 /2S + O(1) ∖ K
K(E) = 口 (σ-1 /2S + O(1)) KMU-ACDM(E)
=------7(1 + o (I)) KNU_ACDM(E)
1-ψ
which completes the proof.
(B.24)
□
C Ordinary Differential Equation Analysis
C.1 Derivation of ODE for synchronous A2BCD
If we take expectations with respect to Ek, then synchronous (no delay) A2BCD becomes:
yk = αvk + (1 - α)xk
EkXk +1 = yk - nT K-1Vf (yk)
EkVk +1 = βvk + (1 - β)yk - n-1 κ-1 /2Vf (yk)
We find it convenient to define η = nκ1 /2. Inspired by this, we consider the following iteration:
yk = αvk + (1 - α)xk	(C.1)
Xk +1 = yk - S1 /2K-1 /2η-1Vf (yk)	(C.2)
Vk +1 = βvk + (1- β)yk - S1 /2η-1Vf (yk)	(C.3)
26
Published as a conference paper at ICLR 2019
C.1 Derivation of ODE for synchronous A2BCD
for coefficients:
α = (1 + S-112 η)
β = 1 — s112 η-1
s is a discretization scale parameter that will be sent to 0 to obtain an ODE analogue of synchronous
A2BCD. We first use equation B.6 to eliminate Vk from from equation C.3.
0 = — Vk +1 + βvk + (1 — β) yk — s112 η-1V f (yk)
0 = — α 1 yk +1 + ɑ 1(1 — α) Xk +1
+ β (α-1 yk — α-1(1 — α) Xk) + (1 — β) yk — S112 η-1V f (yk)
(times by α) 0 = — yk+1 + (1 — α)Xk +1
+ β (yk — (1 — α) Xk) + α (1 — β) yk — aS112 η-1V f (yk)
=—yk +1 + yk (β + α (1 — β))
+ (1 — α) Xk+1 — Xk β (1 — a) — aS112 η-1V f (yk)
We now eliminate Xk using equation C.1:
0 = — yk +1 + yk (β + a (1 — β))
+ (1 — a) (yk — S112η-1K-112Vf (yk)) — (yk-1 — S112η-1K-112Vf(yk-1)) β(1 — a)
—aS112 η TVf (yk)
=—yk +1 + yk(β + a (1 — β ) + (1 — a)) — β (1 — a) yk-1
+ S112 η-1V f (yk-1)( β — 1)(1 — a)
—aS112 η TVf (yk)
=(yk — yk+1) + β (1 — a)(yk — yk-I)
+ S112 η-1(V f (yk-1)( β — 1)(1 — a) — a V f (yk))
Now to derive an ODE, we let yk = Y(kS112). Then Vf (yk-1) = Vf (yk) + O(S112). Hence the
above becomes:
0 = (yk — yk +1) + β (1 — α)(yk — yk-1)
+ S112 η-1(( β — 1)(1 — α) — α Rf (yk) +。［3 /2)
0 = (—S112Y — 1 ^Y) + β(1 — α) (S112Y — 1 ^Y)
+ s112 η-1(( β — 1)(1 — α) — α )V f (yk) +。［312)
(C.4)
27
Published as a conference paper at ICLR 2019
C.2 Convergence proof for synchronous ODE
We now look at some of the terms in this equation to find the highest-order dependence on S.
β (1 — α )=(
(
S
S-112 η + 1
1 — s112 η-1
1 + S112 η-1
1 — 2 S112 η-1 + O( s )
We also have:
(β — 1)(1 — α) — α = β (1 — α) — 1
=—2 S1 / 2 η-1 + O( S)
Hence using these facts on equation C.4, We have:
0 = (—S112Y — 1 sYλ) + (1 — 2S112η-1 + O(S)) (S112Y — ∣SY)
+ S112η-1 (—2S112η-1 + O(S)) Vf (yk) + O (s312)
0 = — S112 Y — 1 ^Y + ( s 112 Y — 1SY — 2 S1 η-1Y + O
(—2 S1 η-2 + θ( s 312))V f (yk)+ θ( s 312)
0 = — ^Y — 2 sη-1Y — 2 sη-2V f (yk) + θ( s 312)
0 = — Y — 2 η T Y — 2 η-2V f (yk) + θ( s 112)
Taking the limit as S → 0, we obtain the ODE:
Y( t) + 2 η-1Y + 2 η-2 V f (Y) = 0
C.2 Convergence proof for synchronous ODE
e - L tE'(t) =〈▽〃 K (t)) ,Y-(t )〉+ η T f (Y (t))
+ J〈γ (t)+ ηγ(t) ,γr(t) + ηγ(t )〉+ η TIllγ (t) + ηγ(t )||
(strong convexity equation B.2) ≤〈▽ f (γ), γ)+ η-1(Vf (γ), γ)— ∙∣ηTlIY『
+ 1〈 Y + ηγ,-Y — 2 η-1V f (γ )〉+ η T ||| γ (t) + η1^( t )||
=—4ηTHY『-1 ηW∕ ≤ 0
2
2
28
Published as a conference paper at ICLR 2019
C.3 Asynchronicity error lemma
Hence We have E'(t) ≤ 0. Therefore E(t) ≤ E(0). That is:
E (t) = e T R-"I f (Y) + 4∣Iy + ηYf^ ≤ E (0) = f( γ(0))+ 1∣p(0)+ η'Y(0)∖^2
Which implies:
f(Y(t)) + 4∖∖Y(t)+ ηY(t)∖∖2 ≤ e-n-1H7"(f(Y(0)) + 4∖∖Y(0)+ ηY(0)∖∖2)
(C.5)
(C.6)
C.3 Asynchronicity error lemma
This result is the continuous-time analogue of Lemma 11. First notice that c(0) = co and c(T) = 0.
We also have:
C'(t) ∣C 0
-re - rt - re - rt
e - rτ
1 - e- rτ
c'(t)
-r
卜-rt
-r 卜-rt+(e - rt -1) I
e - rτ )
+ 1 - e-rτ )
-rc(t) -
e-rτ
rc0 1 - e-rτ
+e - rt i≡M
Hence using c(T) = 0:
A( t )= C o∖∖Y( t )∖∖2 +
c '(t — S) ∖∖ Y (S) ∖∖2 ds
t — T
-rτ
=C0∖∖Y(t)∖∖ -rA(t) - rc0ɪ-77D(t)
Now when 2 ≤ 1, we have T⅛ ≥ 12-1. HenCe when rτ ≤ 1, we have:
21
A(t) ≤ C0∖∖Y(t)∖∖ - rA(t) - -τ-1C0D(t)
and the result easily follows.
C.4 Convergence analysis for the asynchronous ODE
We consider the same energy as in the synchronous case (that is, the ODE in equation 3.1). Similar
to before, we have:
e-LEt) ≤〈▽〃Y), YY + η-1(Vf (Y), Y〉- 2ηTlIY『
+ 2( Y + ηY,-Y - 2 η-1V f (Y ))+ η-1 -∖∖ Y (t) + ηY( t )∖∖2
=(Vf(Y), YY + η-1(Vf(Y),Y〉- -ηTHY『
+ -〈Y + ηY,-Y - 2η-1Vf (Y)Y + η-11∖∖Y(t) + ηY(t)∖∖2
-η-1(Y + ηY, Vf (Y) - Vf (Y ))
=-4ηTHY『-4η∖Y∕ - ηT(Y + ηY, Vf (Y) - Vf (Y))
29
Published as a conference paper at ICLR 2019
where the final equality follows from the proof in Section C.2. Hence
e-η-1 tE,(t) ≤-1 ηTml2 - 1 n|p『+ LηTmly -叶 + LIPIIy -叶 (C.7)
Now we present an inequality that is similar to equation 8.
Lemma 18. Let A,χ > 0. Then:
∣∣m(t) - P(t)∣∣A ≤ 2χτD(t) + 1 XTA2
Tt f n ∙ ʌ / J ∖ ∙ 1 1 1	∙	C -∖7^ / ι∖	1	ʌ / J ∖ ^∖ T / / ι∖∖ C	C J ∙ ∙ / J ∖ 、 C
Proof. Since Y(t) is a delayed version of Y(t), we have: Y(t) = Y(t — j (t)) for some function j(t) ≥ 0
(though this can be easily generalized to an inconsistent read). Recall that for χ > 0, we have
ab ≤ 2 X(a2 + χ-1 b2). Hence
t
X (t) - X( t )= I
s-
X X( s) ds
=t - j (t)
X(t) - X(t)∣∣A = ∣∣∕t	X/(S)ds A
(Holder,s inequality) ≤
2 χ∣∕ ；-j( t)x,(S)ds	+1X-1 a 2
2 XUSI,( t JX X's )"2 ds Xit-,( t)1 ds)
+ 2 X-1A2
≤1 χτ US [j t J X(s )|2 d^+2X-1A 2
We use this lemma twice on IlYIl ∣∣ Y — Y∣∣ and ∣∣ Y∣∣ ∣∣ Y — Y∣∣ in equation C.7 with X = 2L, A = IIYIl
and X = 4Lη-1, A = ∣∣ Y∣∣ respectively, to yield:
e - η-1 tE,(t) ≤- 4 η TllY『-4 η∣∣Y∣∣2
+ Lη-1( LτD (t ) + 4 LTIIY『)
=-1 η∣∣Y∕ +3 L 2 η-1 τD (t)
+ L 0Lη-1 τD(t) + 8L-1η∣∣Y∣∣2)
The proof of convergence is completed in Section 3.
D Optimality proof
For parameter set σ, L 1,...,Ln,n, we construct a block-separable function f on the space Rbn
(separated into n blocks of size b), which will imply this lower bound. Define Ki = Li/σ. We define
≤
□
30
Published as a conference paper at ICLR 2019
the matrix Ai ∈ Rb×b via:
2	-1	0
-1	2	... ...
Ai,	o ...	... -1
.
.. -1	2
0	-1
Hence we define fi on Rb via:
fi = Li 4 σ (1〈x,AixX
∖
f+3
0	，for θi =	1 /2 I ]
κi	+ 1
-1
θi
-〈eι,x〉) + σIlXIl2
which is clearly σ-strongly convex and Li-Lipschitz on Rb. From Lemma 8 of Lan & Zhou (2015),
we know that this function has unique minimizer
x *, (i) , (qi, qi ,...,qi ) , for q = 1 /2	.
κi + 1
Finally, we define f via:
n
f (x),工 fi (x(i)).
i=1
Now let e(i, j) be the jth unit vector of the ith block of size b in Rbn . For I1 , . . . , In ∈ N, we define
the subspaces
Vi(I) = span{e(i, 1), . . . , e(i, I)},
V (I1,...,In ) = V1 (11) ㊉…㊉ Vn (In ) ∙
V(I1 , . . . , In ) is the subspace with the first I1 components of block 1 nonzero, the first I2 components
of block 2 nonzero, etc. First notice that IC(V(I1, . . . , In)) = V(I1, . . . , In). Also, clearly, we have:
▽ if (V (11 ,...,In)) ⊂ V (0,..., 0, min{Ii + 1 ,b}, 0,…,0).	(D.1)
▽ if is supported on the ith block, hence Why all the other indices are 0. The patten of nonzeros in
A means that the gradient will have at most 1 more nonzero on the ith block (see Nesterov (2013)).
Let the initial point xο belong to V ∣I,,..., In^. Let Iκ,i be the number of times we have had ik = i
for k = 0, . . . , K - 1. By induction on condition 2 of Definition 4 using equation D.1, We have:
Xk ∈ V (min {-Tl + Ik, 1, b},..., min{ In + Ik,m, b})
Hence if x0,(i) ∈ Vi (0) and k ≤ b, then
b
Ilxk,(i)-χ*,(i)||2 ≥ min Ilχ-χ*,(i)||2 =工 qj = (q2lk,i+2 -q2b+2)/(1 - qi2)
x∈Vi(Ik,i)
j=Ik,i+1
Therefore for all i, we have:
Ell Xk-X *∣2 ≥ Ell Xk, (i) - X *, (i )||2 ≥ ERq Ik,i+2 - qib+2) / (1 - q2)]
31
Published as a conference paper at ICLR 2019
To evaluate this expectation, We note:
j
=(1 - Pi)k (1 + 成 Pi (1 - Pi 厂1)
=(1 - (1 -成)Pi)k
Hence
Ell xk - x *∣∣2 ≥ ((1 - (1 - q2) pi) - qi b) qi / (1 - Qi) .
For any i, we may select the starting iterate xo by defining its block j = 1,… ,n via:
X0,(j) = (1 - δij)x*,(j)
For such a choice of xo , we have
Iiχ o- χ *∣∣2 = Iiχ *,( i)||2 = Q +... ÷ q2 b = Q1 - ?
Hence for this choice of x0 :
EIlxk -	x*∣∣ 2/Iχo - χ*∣∣ 2 ≥	((1	-	(1 -	q2)pi)	-	q2b)/ (I-Q2b)
Now notice:
(1-(1-q2)Pi)k = (Q- - (Q-- 1)Pi)kq2k ≥ q2k
Hence
EIlχk - χ*l|2/llχo - χ*∣∣2 ≥ (1 - (1 - q2)Pi)fc(1 - q2b-2k)/ (1 - q2b)
Now if we let b = 2k, then we have:
EIlχk - χ*l∣2/Ilχo- χ*∣∣2 ≥ (1 - (1 -Q2)Pi)fc(1 - q2k)/(1 - qik)
=(1 -(I-Q2) p )k/(1 + Qi k)
EIlXk - X*∣∣2/IlXo - X*∣∣2 ≥ 1max(1 - (1 - Q2)Pi)k
Now let us take the minimum of the right-hand side over the parameters Pi, subject to ɪɪi Pi = 1.
The solution to this minimization is clearly:
32
Published as a conference paper at ICLR 2019
Hence
Ellxk - x*『/Ilx0 - x*『
/
1 -
∖-1∖k
i2)-1
n
£(i-*)T
j=1
∖ / ∕
n
4 EG/2 +2 + KI/2)
j=1
EllXk- x*『/1x0 - x*『
≥ 4 (E
≥ 2 (1-
1
≥ —
-2
κ1 / 2 + 2 n
4
∑ n= κ1/ 2
Tk
+ 2 n J
Hence the complexity I(e) satisfies:
e ≥ 2 (1 -
)I ( e)
I(e) ≥ -
-4/2——))ln(1 / 2 O
1 K12+2nJ J
4
κ1 / 2 + 2 n
I(I + o(I))In + E κi/2j In(I/2e)
E Extensions
As mentioned, a stronger result than Theorem 1 is possible. In the case when Li = L for all i, We
can consider a slight modification of the coefficients:
α，(1 + (1 + Ψ)L1 /2S)-1
β , 1 - (1 + ψ)-1 σ112S-1
h，(1 + 1 σ112 L-112 ψ) 1.
for the asynchronicity parameter:
This leads to complexity:
(E.1)	yk =	avk + (1 — α) Xk,	(E.4)
(E.2)	Xk+1 =	yk- hL-1 ▽ / f (yk),	(E.5)
(E.3)	Vk +1 =	βvk + (1 - β)yk - σ-112L-112Vik f (yk).	(E.6)
ψ	=6 K1 / 2 n	-1 × τ	(E.7)
K (e) =	(1 + ψ)nκ1 /2 ln(1 /e)		(E.8)
Here there is no restriction on ψ as in Theorem 1, and hence there is no restriction on T. Assuming
ψ ≤ 1 gives optimal complexity to within a constant factor. Notice then that the resulting condition
of T
T ≤1 nκ-1 /2
(E.9)
33
Published as a conference paper at ICLR 2019
now essentially matches the one in Theorem 3 in Section 3. While this result is stronger, it increases
the complexity of the proof substantially. So in the interests of space and simplicity, we do not
prove this stronger result.
34