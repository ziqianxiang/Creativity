Published as a conference paper at ICLR 2019
Supervised Community Detection with Line
Graph Neural Networks
Zhengdao Chen
Courant Institute of Mathematical Sciences
New York University, New York, NY
Lisha Li
Amplify Partners
San Francisco, CA
Joan Bruna
Courant Institute of Mathematical Sciences
New York University, New York, NY
Ab stract
Community detection in graphs can be solved via spectral methods or posterior
inference under certain probabilistic graphical models. Focusing on random graph
families such as the stochastic block model, recent research has unified both
approaches and identified both statistical and computational detection thresholds
in terms of the signal-to-noise ratio. By recasting community detection as a
node-wise classification problem on graphs, we can also study it from a learning
perspective. We present a novel family of Graph Neural Networks (GNNs) for
solving community detection problems in a supervised learning setting. We show
that, in a data-driven manner and without access to the underlying generative
models, they can match or even surpass the performance of the belief propagation
algorithm on binary and multiclass stochastic block models, which is believed
to reach the computational threshold in these cases. In particular, we propose
to augment GNNs with the non-backtracking operator defined on the line graph
of edge adjacencies. The GNNs are achieved good performance on real-world
datasets. In addition, we perform the first analysis of the optimization landscape of
using (linear) GNNs to solve community detection problems, demonstrating that
under certain simplifications and assumptions, the loss value at any local minimum
is close to the loss value at the global minimum/minima.
1	Introduction
Graph inference problems encompass a large class of tasks and domains, from posterior inference
in probabilistic graphical models to community detection and ranking in generic networks, image
segmentation or compressed sensing on non-Euclidean domains. They are motivated both by practical
applications, such as in the case of PageRank, and also by fundamental questions on the algorithmic
hardness of solving such tasks.
From a data-driven perspective, these problems can be formulated in unsupervised, semi-supervised
or supervised learning settings. In the supervised case, one assumes a dataset of graphs with labels on
their nodes, edges or the entire graphs, and attempts to perform node-wise, edge-wise and graph-wise
classification by optimizing a loss over a certain parametric class, e.g. neural networks. Graph Neural
Networks (GNNs) are natural extensions of Convolutional Neural Networks to graph-structured data,
and have emerged as a powerful class of algorithms to perform complex graph inference leveraging
labeled data (Gori et al., 2005; Bronstein et al., 2017) (and references therein). In essence, these
neural networks learn cascaded linear combinations of intrinsic graph operators interleaved with
node-wise (or edge-wise) activation functions. Since they utilize intrinsic graph operators, they can
be applied to varying input graphs, and they offer the same parameter sharing advantages as their
CNN counterparts.
In this work, we focus on community detection problems, a wide class of node classification tasks
that attempt to discover a clustered, segmented structure within a graph. The algorithmic approaches
to this problem include a rich class of spectral methods, which take advantage of the spectrum of
1
Published as a conference paper at ICLR 2019
certain operators defined on the graph, as well as approximate message-passing methods such as
belief propagation (BP), which performs approximate posterior inference under predefined graphical
models (Decelle et al., 2011). Focusing on the supervised setting, we study the ability of GNNs to
approximate, generalize or even improve upon these class of algorithms. Our motivation is two-fold.
On the one hand, this problem exhibits algorithmic hardness on some settings, opening up the
possibility to discover more efficient algorithms than the current ones. On the other hand, many
practical scenarios fall beyond pre-specified probabilistic models, requiring data-driven solutions.
We propose modifications to the GNN architecture, which allow it to exploit edge adjacency informa-
tion, by incorporating the non-backtracking operator of the graph. This operator is defined over the
edges of the graph and allows a directed flow of information even when the original graph is undi-
rected. It was introduced to community detection problems by Krzakala et al. (2013), who propose a
spectral method based on the non-backtracking operator. We refer to the resulting GNN model as
a Line Graph Neural Network (LGNN). Focusing on important random graph families exhibiting
community structure, such as the stochastic block model (SBM) and the geometric block model
(GBM), we demonstrate improvements in the performance by our GNN and LGNN models compared
to other methods, including BP, even in regimes within the so-called computational-to-statistical
gap. A perhaps surprising aspect is that these gains can be obtained even with linear LGNNs, which
become parametric versions of power iteration algorithms.
We want to mention that besides community detection tasks, GNN and LGNN can be applied to other
node-wise classification problems too. The reason we are focusing on community detection problems
is that this is a relatively well-studied setup, for which different algorithms have been proposed and
where computational and statistical thresholds have been studied in several scenarios. Moreover,
synthetic datasets can be easily generated for community detection tasks. Therefore, we think it is a
nice setup for comparing different algorithms, besides its practical values.
The good performances of GNN and LGNN motivate our second main contribution: the analysis of
the optimization landscape of simplified and linear GNN models when trained with planted solutions
of a given graph distribution. Under reparametrization, we provide an upper bound on the energy
gap controlling the energy difference between local and global minima (or minimum). With some
assumptions on the spectral concentration of certain random matrices, this energy gap will shrink as
the size of the input graphs increases, which would mean that the optimization landscape is benign
on large enough graphs.
Summary of Main Contributions:
•	We propose an extension of GNNs that operate on the line graph using the non-backtracking
operator, which yields improvements on hard community detection regimes.
•	We show that on the stochastic block model we reach detection thresholds in a purely
data-driven fashion, in the sense that our results improve upon belief propagation in hard
SBM detection regimes, as well as in the geometric block model.
•	We perform the first analysis of the learning landscape of GNN models, showing that under
certain simplifications and assumptions, they exhibit a form of “energy gap”, where local
mimima are confined in low-energy configurations.
•	We show that our model can perform well on community detection problems with real-world
datasets.
2	Problem Setup
We are interested in a specific class of node-classification tasks in which given an input graph
G = (V, E), a labeling y : V → {1, . . . , C} that encodes a partition of V into C communities is to
be predicted at each node. We assume that a training set {(Gt, yt)}t≤T is given, with which we train
a model that predicts y = Φ(G, θ) by minimizing
L(θ) = TT X '(Φ(Gt,θ),yt)
t≤T
2
Published as a conference paper at ICLR 2019
Since y encodes a partition of C groups, the specific label of each node is only important up to a
global permutation of {1, . . . , C}. Section 4.3 describes how to construct loss functions ` with such
a property. A permutation of the observed nodes translates into the same permutation applied to
the labels, which justifies models Φ that are equivariant to permutations. Also, we are interested in
inferring properties of community detection algorithms that do not depend on the specific size of the
graphs1. We therefore require that the model Φ accepts graphs of variable size for the same set of
parameters, similar to sequential RNN or spatial CNN models.
3	Related Work
GNN was first proposed in Gori et al. (2005); Scarselli et al. (2009). Bruna et al. (2013) generalize
convolutional neural networks on general undirected graphs by using the graph Laplacian’s eigenbasis.
This was the first time the Laplacian operator was used in a neural network architecture to perform
classification on graph inputs. Defferrard et al. (2016) consider a symmetric Laplacian generator to
define a multiscale GNN architecture, demonstrated on classification tasks. Similarly, Kipf & Welling
(2016) use a similar generator as effective embedding mechanisms for graph signals and applies it to
semi-supervised tasks. This is the closest application of GNNs to our current contribution. However,
we highlight that semi-supervised learning requires bootstrapping the estimation with a subset of
labeled nodes, and is mainly interested in generalization within a single, fixed graph. In comparison,
our setup considers community detection across a distribution of input graphs and assumes no initial
labeling on the graphs in the test dataset except for the adjacency information.
There have been several extensions of GNNs by modifying their non-linear activation functions,
parameter sharing strategies, and choice of graph operators (Li et al., 2015; Sukhbaatar et al., 2016;
Duvenaud et al., 2015; Niepert et al., 2016). In particular, Gilmer et al. (2017) interpret the GNN
architecture as learning an approximate message-passing algorithm, which extends the learning of
hidden representations to graph edges in addition to graph nodes. Recently, Velickovic et al. (2017)
relate adjacency learning with attention mechanisms, and Vaswani et al. (2017) propose a similar
architecture in the context of machine translation. Another recent and related piece of work is by
Kondor et al. (2018), who propose a generalization of GNN that captures high-order node interactions
through covariant tensor algebra. Our approach to extend the expressive power of GNN using the
line graph may be seen as an alternative to capture such high-order interactions.
Our energy landscape analysis is related to the recent paper by Shamir (2018), which establishes an
energy bound on the local minima arising in the optimization of ResNets. In our case, we exploit
the properties of the community detection problem to produce an energy bound that depends on the
concentration of certain random matrices, which one may hope for as the size of the input graphs
increases. Finally, Zhang (2016)’s work on data regularization for clustering and rank estimation is
also motivated by the success of using Bethe-Hessian-like perturbations to improve spectral methods
on sparse networks. It finds good perturbations via matrix perturbations and also has successes on
the stochastic block model. Yang & Leskovec (2012a) curate benchmark datasets for community
detection and quantify the quality of these datasets, while Yang & Leskovec (2012b) develop new
algorithms for community detection by fitting to the networks the Affliation Graph Model (AGM), a
generative model for graphs with overlapping communities.
4	Line Graph Neural Networks
This section introduces our GNN architectures that include the power graph adjacency (Section 4.1)
and its extension to line graphs using the non-backtracking operator (Section 4.2), as well as the
design of losses invariant to global label permutations (Section 4.3).
4.1	Graph Neural Networks using a family of graph operators
The Graph Neural Network (GNN), introduced in Scarselli et al. (2009) and later simplified in Li et al.
(2015); Duvenaud et al. (2015); Sukhbaatar et al. (2016), is a flexible neural network architecture
based on local operators on a graph G = (V,E). Given a state vector X ∈ R1 Vl×b on the vertices of
1In this work, however, we assume that C is fixed.
3
Published as a conference paper at ICLR 2019
Figure 1. Overview of the architecture of LGNN (Section 4.2). Given a graph G, we construct its line graph
L(G) with the non-backtracking operator (Figure 2). In every layer, the states of all nodes in G and L(G) are
updated according to (2). The final states of nodes in G are used to predict node-wise labels, and the trainining is
performed end-to-end using standard backpropagation with a label permutation invariant loss (Section 4.3).
G, we consider intrinsic linear operators of the graph that act locally on x, which can be represented
as |V |-by-|V | matrices. For example, the adjacency matrix A is defined entry-wise by Ai1i2 = 1 if
(i1, i2) ∈ E and Ai1i2 = 0 if (i1, i2) ∈/ E, for every pair (i1, i2) ∈ V × V . The degree matrix D is
then defined as diag(A1), i.e., D is a diagonal matrix with Dii being the number of edges that the
ith node has. We can also define power graph adjacency matrices as A(j) = min(1, A2j ), which
encodes 2j -hop neighborhoods into a binary graph. Finally, there is also the identity matrix, I. Given
such a family of operators for each graph, FAJ = {I, D, A, A(2), ..., A(J)}, we define a GNN layer
that maps x(k) ∈ RlVl×bk to x(k+1) ∈ R|V l×bk+1 as follows. First, We compute
ZM)= P	E OiX(k)θi ,z(k+1) = E OiX(k)θi	(1)
bk+1
where θj ∈ Rbk × ~^2~ are trainable parameters and ρ(∙) is a point-wise nonlinear activation function,
chosen in this work to be the ReLU function, i.e. ρ(z) = max(0, z) for z ∈ R. Then we define
x(k+1) = [z(k+1),z(k+1)] ∈ RlVl×bk+1 as the concatenation of z(k+1) and z(k+1). The layer thus
includes linear “residual connections" (He et al., 2016) via Z(k), both to ease with the optimization
when using large number of layers and to increase the expressivity of the model by enabling it
to perform power iterations. Since the spectral radius of the learned linear operators in (1) can
grow as the optimization progresses, the cascade of GNN layers can become unstable to training.
In order to mitigate this effect, we consider spatial batch normalization (Ioffe & Szegedy, 2015)
at each layer.2 In our experiments, the initial states are set to be the degrees of the nodes, i.e.,
x(0) = deg(A) := A1. In addition, the model Φ(G, x(0)) = x(K) satisfies the permutation
equivariance property required for community detection: Given a permutation π among the nodes in
the graph, Φ(Gπ, Πx(0)) = ΠΦ(G, x(0)), where Π is the permutation matrix associated with π.
Analogy between GNN and power iterations In our setup, spatial batch normalization not only
prevents gradient blowup, but also performs the orthogonalisation relative to the constant vector,
which reinforces the analogy with the spectral methods for community detection, some background
of which is described in B.1. In essence, in certain regimes, the eigenvector of A corresponding to its
second largest eigenvalue and the eigenvector of the Laplacian matrix, L = D - A, corresponding
to its second smallest eigenvalue (i.e. the Fiedler vector), are both correlated with the community
structure of the graph. Thus, spectral methods for community detection performs power iterations on
2The term “spatial batch normalization” may be slightly misleading, since each batch only contain one graph
in our experiments. However, we used spatial batch normalization in our implementation for convenience, as it
performs orthogonalization and normalization, as further explained in the next paragraph.
4
Published as a conference paper at ICLR 2019
these matrices to obtain the eigenvectors of interest and predicts the community structure based on
them. For example, to extract the Fiedler vector of a matrix M , whose eigenvector corresponding to
the smallest eigenvalue is known to be v, one performing power iterations on M = kM kI - M by
y(n+1) = Mx(n) , x(n+1) = y(n+1) -VTvy(n+1) . If V is a constant vector, which is the case for L,
ky (n+1) -vTvy (n+1) k
then the normalization above is precisely performed within the spatial batch normalization step.
By incorporating a family of operators into the neural network framework, the GNN can not only
approximate but also go beyond power iterations. As explained in Section B.1, the Krylov subspace
generated by the graph Laplacian (Defferrard et al., 2016) is not sufficient to operate well in the sparse
regime, as opposed to the space generated by {I, D, A}. The expressive power of each layer is further
increased by adding multiscale versions of A, although this benefit comes at the cost of computational
efficiency, especially in the sparse regime. The network depth is chosen to be of the order of the graph
diameter, so that all nodes obtain information from the entire graph. In sparse graphs with small
diameter, this architecture offers excellent scalability and computational complexity. Indeed, in many
social networks diameters are constant (due to hubs), or log(|V |), as in the stochastic block model
in the constant average degree regime (Riordan & Wormald, 2010). This results in a model with
computational complexity on the order of |V | log(|V |), making it amenable to large-scale graphs.
4.2	LGNN: GNN on line graphs with the non-backtracking operator
For graphs with few cycles, posterior inference can be remarkably approximated by loopy belief
propagation (Yedidia et al., 2003). As described in Section B.2, the message-passing rules are defined
over the edge adjacency graph (see equation 57). Although its second-order approximation around
the critical point can be efficiently approximated with a power method over the original graph, a data-
driven version of BP requires accounting for the non-backtracking structure of the message-passing.
In this section we describe an upgraded GNN model that exploits the non-backtracking structure.
The line graph L(G) = (VL, EL) is a graph representing
the edge adjacency structure of G. If G = (V, E) is an
undirected graph, then the vertices VL of L(G) are the
ordered edges in E, i.e., VL = {(i → j) : (i, j) ∈ E} ,
and so |VL| = 2|E |. The non-backtracking operator on
the line graph is represented by a matrix B ∈ R21El×2lE|
defined as
B	1 if j = i0 and j 0 6= i ,
(i→j),(i0→j0)	0	otherwise.
This operator enables the directed propagation of infor-
mation through on the line graph and was first proposed
in the context of community detection on sparse graphs
in Krzakala et al. (2013). The message-passing rules of
BP can be expressed as a diffusion in the line graph L(G)
using this non-backtracking operator, with specific choices
of activation function that turn product of beliefs into sums.
A natural extension of the GNN architecture presented in Section 4.1 is thus to consider a second
GNN defined on L(G), where B and DB = diag(B1) play the role of the adjacency and the degree
matrices, respectively. This effectively defines edge features that are updated according to the edge
adjacency of G. Edge and node features communicate at each layer using the edge indicator matrices
Pm,Pd ∈ {0,1}lVl×2lEl,definedas Pm%(i→j) = 1, Pd…)=1, Pdi,(f)= 1, Pd/)=—1
and 0 otherwise. With the skip linear connections defined similarly, the resulting model becomes
Figure 2. Construction of the line graph
L(G) using the non-backtracking Operator.
The nodes of L(G) correspond to oriented
edges of G.
z(k+1) = ρ	Oix(k)θi +	Oj0y(k)θi0
Oi ∈FA	Oj0 ∈FAB
j	(2)
w(k+1) = ρ X	Ol00y(k)θi00 + X	(Oj0)Tx(k+1)θj000
Ol00∈FB	Oj0 ∈FAB
5
Published as a conference paper at ICLR 2019
where FA = {I,D,A,A(2),...,A(J)}, FB = {IB,DB,B,B(2),...,B(J)}, FAB = {Pm,Pd},
and the trainable parameters are θi, θi0, θi00 ∈ Rbk×bk+1 and θi000 ∈ Rbk+1 ×bk+1. We call such a model
a Line Graph Neural Network (LGNN).
In our experiments, we set x(0) = deg(A) and y(0) = deg(B). For graph families with constant
average degree d (as |V| grows), the line graph has size 2|E| 〜O(d∣V|), and is therefore feasible
from the computational point of view. Furthermore, the construction of line graphs can be iterated
to generate L(L(G)), L(L(L(G))), etc. to yield a graph hierarchy, which could capture high-order
interactions among nodes of G. Such an hierarchical construction is related to other recent efforts to
generalize GNNs (Kondor et al., 2018).
Relationship between LGNN and edge feature learning approaches Several authors have pro-
posed combining node and edge feature learning. Battaglia et al. (2016) introduce edge features
over directed and typed graphs, but does not discuss the undirected case. Kearnes et al. (2016);
Gilmer et al. (2017) learn edge features on undirected graphs using fe = g(x(i), x(j)) for an edge
e = (i, j), where g is commutative on its arguments. Finally, Velickovic et al. (2017) learns directed
edge features on undirected graphs using stochastic matrices as adjacencies (which are either row or
column-normalized). However, we are not aware of works that consider the edge adjacency structure
provided by the non-backtracking matrix on the line graph. With non-backtracking matrix, our LGNN
can be interpreted as learning directed edge features from an undirected graph. Indeed, if each node
i contains two distinct sets of features xs(i) and xr(i), the non-backtracking operator constructs
edge features from node features while preserving orientation: For an edge e = (i, j), our model is
equivalent to constructing oriented edge features fi→j = g(xs (i), xr(j)) and fj→i = g(xr(i), xs(j))
(where g is trainable and not necessarily commutative on its arguments) that are subsequently prop-
agated through the graph. Constructing such local oriented structure is shown to be important for
improving performance in the next section.
For comparison, we also define a linear LGNN (LGNN-L) as the the LGNN that drops the nonlinear
activation functions ρ in (2), and a symmetric LGNN (LGNN-S) as the LGNN whose line graph is
defined on the undirected edges of the original graph: In LGNN-S, two edges of G are connected
in the line graph if and only if they share one common node; also, F = {P}, with P ∈ R|V l×lEl
defined as Pi,(j→k) = 1 if i = j or k and 0 otherwise.
4.3	A Loss Function Invariant Uner Label permutation
Let C = {1, . . . , C } denote the set of all community labels, and consider first the case where
communities do not overlap. By applying the softmax function at the end, we interpret the cth
dimension of the output of the models at node i as the conditional probability that the node belongs
to community c, o%,。= p(yi = C ∣θ,G). Let G = (V, E) be the input graph and let yi be the ground
truth community label of node i. Since the community structure is defined up to global permutations
of the labels, we can define a loss function with respect to a given graph instance as
`(θ) = inf - X log oi,π(yi) ,	(3)
π∈SC i∈V
where SC denotes the permutation group of C elements. This is essentially taking the the cross entropy
loss minimized over all possible permutations of C . In our experiments, we considered examples with
small numbers of communities such as 2 and 5. In general scenarios where C is much larger, the
evaluation of the loss function (3) can be impractical due to the minimization over SC. A possible
solution is to randomly partition C labels into C groups, and then to marginalize the model outputs
Oi,c, C ≤ C into oi,c = Ec∈δ Oi,c, C ∈ C, and and finally use '(θ) = inf∏∈sδ - Ei∈v log oi∏(yi) as
an approximate loss value, which only involves a permutation group of size (C!).
Finally, if communities may overlap, we can enlarge C to include subsets of communities and define
the permutation group accordingly. For example, if there are two overlapping communities, we let
C = {{1}, {2}, {1, 2}}, and only allow the permutation between 1 and 2 when computing the loss
function as well as the overlap to be introduced in Section 6.
6
Published as a conference paper at ICLR 2019
5	Energy Landscape of Linear GNN optimization
As described in the numerical experiments, we found that the GNN models without nonlinear
activations already provide substantial gains relative to baseline (non-trainable) algorithms. This
section studies the optimization landscape of linear GNNs. Despite defining a non-convex objective,
we prove that the landscape is “benign” under certain further simplifications, in the sense that the
local minima are confined in sublevel sets of low energy.
For simplicity, we consider only the binary c = 2 case where we replace the node-wise binary
cross-entropy loss by the squared cosine distance3, assume a single feature map (bk = 1 for all k),
and focus on the GNN described in Section 4.1 (although our analysis carries equally to describe
the line graph version; see remarks below). We also make the simplifying assumption to replace the
layer-wise spatial batch normalization by a simpler projection onto the unit `2 ball (thus we do not
remove the mean). Without loss of generality, assume that the input graph G has size n, and denote
by F = {A1, . . . , AQ} the family of graph operators appearing in (1). Each layer thus applies an
arbitrary polynomial PqQ=1 θq(k) Aq to the incoming node feature vector x(k). Given an input node
vector w ∈ Rn , the network output can thus be written as
Y=总，With e = Y X θ(k)Aq w.	(4)
kek	k=1 q≤Q
We highlight that this linear GNN setup is fundamentally different from the linear fully-connected
neural netWorks (that is, neural netWorks With linear activation function), Whose landscape has been
analyzed in KaWaguchi (2016). First, the output of the GNN is on the unit sphere, Which has a different
geometry. Next, since the operators in F depend on the input graph, they introduce fluctuations
in the landscape. In general, the operators in F are not commutative, but by considering the
generalized Krylov subspace generated by poWers of F, FK = {O1 = A1K , O2 = A1A2K -1, O3 =
A1A2A1K -2, . . . OQK = AQK }, one can reparametrize (4) as e = PjQ=K1 βjOjw With β ∈ RM, With
M = Qk. Given the target y ∈ Rn, the loss incurred by each pair (G, y) becomes 1 一 |〈洋| , and
therefore the population loss, When expressed in terms of β, equals
Ln(β) = 1- EX”上 片3，With	(5)
Γ (O1 w)「
Yn = znzn> ∈RM×M, (zn)j = hOjw, yi and Xn = UnUn> ∈RM×M,Un =	...	.
(OM w)>
The landscape is thus specified by a pair of random matrices Yn , Xn ∈ RM ×M .
Assuming that EXn 0, We Write the Cholesky decomposition of EXn as EXn = RnRnT , and
define An = R-IYn(R-I)T, An = EAn = R-IEYn(R-I)T, Bn = RnIXn(RnbT, and ∆Bn =
Bn 一 In. Given a symmetric matrix K ∈ RM×M, We let λ1(K), λ2(K), ..., λM (K) denote the
eigenvalues of K in nondecreasing order. Then, the folloWing theorem establishes that under
appropriate assumptions, the concentration of relevant random matrices around their mean controls
the energy gaps betWeen local and global minima of L.
Theorem 5.1. For a given	n,	let	ηnι	=	(λι(An)	—	λ2(An))-1, μn = E[∣λι(An)∣6],	Vn	=
E[∣λι(Bn)∣-6], δn = E[∣∣∆Bn∣∣6], and assume that all four quantities are finite. Then if
βl ∈ SM-1 is a local minimum of Ln, and βg ∈ SM-1 is a global minimum of Ln, we have
LnCel) ≤ (I + eηn,μn,νn,δj ∙ LnCeg), Where ∈ηn,μn,νn,δn = θ3^ forgiven Hn, Rn, νn as δn → 0
and its formula is given in the appendix.
Corollary 5.2. If (ηn)n∈N*, (μn)n∈N*, (νn)n∈N* are all bounded sequences, and limn→∞ δn = 0,
then ∀e > 0, ∃ne such that ∀n > ne, ∣Ln(βl) — Ln(βg)| ≤ e ∙ Ln(βg).
The main strategy of the proof is to consider the actual loss function Ln as a perturbation of
Ln(β) = 1 — Eχn,γn βTEXββ = 1 — βTEXnβ, which has a landscape that is easier to analyze and
3to account for the invariance up to global flip of label
7
Published as a conference paper at ICLR 2019
does not have poor local minima, since it is equivalent to a quadratic form defined over the sphere
SM-1. Applying this theorem requires estimating spectral fluctuations of the pair Xn, Yn, which
in turn involve the spectrum of the C * algebras generated by the non-commutative family F. For
example, for stochastic block models, it is an open problem how the bound behaves as a function
of the parameters p and q . Another interesting question is to understand how the asymptotics of
our landscape analysis relate to the hardness of estimation as a function of the signal-to-noise ratio.
Finally, another open question is to what extent our result could be extended to the non-linear residual
GNN case, perhaps leveraging ideas from Shamir (2018).
6	Experiments
We present experiments on community detection in synthetic datasets (Sections 6.1, 6.2 and Appendix
C.1) as well as real-world datasets (Section 6.3). In the synthetic experiments, the performance is
measured by the overlap between predicted (y) and true labels (y), which quantifies how much better
than random guessing a predicted labeling is, given by (1 Pu 5豆@)府@)- C)/(1 - -C), where δ
is the Kronecker delta function, and this quantity is maximized over global permutations within a
graph of the set of labels. In the real-world datasets, as the communies are overlapping and not
balanced, the prediction accuracy is measured by 1 Pu 6y(u),y(u), and the set of permutations to be
maximized over is described in Section 4.3. We used Adamax (Kingma & Ba, 2014) with learning
rate 0.004 across all experiments. All the neural network models have 30 layers and 8 features in the
middle layers (i.e., bk = 8) for experiments in Sections 6.1 and 6.2, and 20 layers and 6 features for
Section 6.3. GNNs and LGNNs have J = 2 across the experiments except the ablation experiments
in Section C.3. 4
6.1	Binary Stochastic Block Model
The stochastic block model is a random graph model with planted community structure. In its
simplest form, the graph consists of |V | = n nodes, which are partitioned into C communities, that
is, each node is assigned a label y ∈ {1, ..., C}. An edge connecting any two vertices u, v is drawn
independently at random with probability p if y(v) = y(u), and with probability q otherwise. In
the binary case (i.e. C = 2), the sparse regime, where p, q ' 1/n, is well understood and provides
an initial platform to compare the GNN and LGNN with provably optimal recovery algorithms
(Appendix B). We consider two learning scenarios. In the first scenario, we choose different pairs
of p and q, and train the models for each pair separately. In particular, for each pair of (pi, qi), we
sample 6000 graphs under G 〜SBM(n = 1000,pi, qi,C = 2) and then train the models for each i.
In the second scenario, reported in Appendix C.2, we train a single set of parameters θ from a set
of 6000 graphs sampled from a mixture of SBM with different pairs of (pi, qi), and average degree.
Importantly, his setup shows that our models are not simply approximating known algorithms such as
BP for particular SBM parameters, since the parameters vary in this dataset.
For the first scenario, we chose five different pairs of (pi, qi) while fixing pi + qi, thereby correspond-
ing to different signal-to-noise ratios (SNRs). Figure 3 reports the performance of our models on the
binary SBM model for the different SNRs, compared with baseline methods including BP, spectral
methods using the normalized Laplacian and the Bethe Hessian as well as Graph Attention Networks
(GAT)5 from Velickovic et al. (2017). We observe that both GNN and LGNN reach the performance
of BP. In addition, even the linear LGNN achieves a performance that is quite close to that of BP, in
accordance to the spectral approximations of BP given by the Bethe Hessian (see supplementary),
and significantly outperforms performing 30 power iterations on the Bethe Hessian or the normalized
Laplacian, as was done in the spectral methods. We also notice that our models outperform GAT in
this task. We ran experiments in the dissociative case (q > p), as well as with C = 3 communities
and obtained similar results, which are not reported here.
4Code is available at https://github.com/zhengdao-chen/GNN4CD
5Our implementation of GAT is based on https://github.com/Diego999/pyGAT. We modified
the code so that the number of layers in the network is flexible, and also added spatial batch normalization at the
end of each layer, similar to in our GNN and LGNN, as our experiments showed that including spatial batch
normalization improved the performance. The results in sections 6.1 and 6.2 are from a trained GAT with 30
layers and 8 features.
8
Published as a conference paper at ICLR 2019
Figure 3. Binary associative SBM detection (C = 2, p > q). X-axis corresponds to SNR, and Y-axis to overlap
between the prediction and the ground truth.
Avg.
Std. Dev.
GNN
0.18
0.04
LGNN
0.21
0.05
LGNN-L
■0.18
0.04
LGNN-S
0.18
0.04
GAT
0.16
0.04
BP
0.14
0.02
Table 1: Performance of different models on 5-community dissociative SBM graphs with n = 400, C = 5,
p = 0, q = 18/n, corresponding to an average degree of 14.5. The first row gives the average overlap across
test graphs, and the second row gives the graph-wise standard deviation of the overlap.
6.2	Computational-to-Statistical Thresholds in the SBM
In SBM with fewer than 4 communities, itis known that BP provably reaches the information-theoretic
threshold (Abbe, 2017; MassoUlia 2014; Coja-Oghlan et al., 2016). The situation is different for
k > 4, where it is conjectured that a gap emerges between the theoretical performance of MLE
estimators and the performance of any polynomial-time estimation procedure (Decelle et al., 2011).
In this context, one can use the GNN models to search the space of the generalizations of BP, and
attempt to improve upon the detection performance of BP for scenarios where the SNR falls within
the computational-to-statistical gap. Table 1 presents results for the 5-community dissociative SBM,
with n = 400, p = 0 and q = 18/n. The SNR in this setup is above the information-theoretic
threshold but below the asymptotic threshold above which BP is able to detect (Decelle et al., 2011).
Note that since p = 0, this also amounts to a graph coloring problem.
We see that the GNN and LGNN models outperform BP in this experiment, indeed opening up the
possibility to reduce the computation-information gap. That said, our model may taking advantage
of finite-size effects, which will vanish as n → ∞. The asymptotic study of these gains is left for
future work. In terms of average test accuracy, LGNN has the best performance. In particular, it
outperforms the symmetric version of LGNN, emphasizing the importance of the non-backtracking
matrix used in LGNN. Although equipped with the attention mechanism, GAT does not explicitly
incorporate in itself the degree matrix, the power graph adjacency matrices or the line graph structure,
and has inferior performance compared with the GNN and LGNN models. Further ablation studies
on GNN and LGNN are described in Section C.3.
6.3	Real Datasets from SNAP
We now compare the models on the SNAP datasets, whose domains range from social networks to
hierarchical co-purchasing networks. We obtain the training set as follows. For each SNAP dataset,
we start by focusing only on the 5000 top quality communities provided by the dataset. We then
identify edges (i, j) that cross at least two different communities. For each of such edges, we consider
pairs of communities C1, C2 such that i ∈/ C2 and j ∈/ C1, i ∈ C1, j ∈ C2, and extract the subset of
nodes determined by C1 ∪ C2 together with the edges among them. The resulting graph is connected
since each community is connected. Finally, we divide the dataset into training and testing sets by
enforcing that no community belongs to both the training and the testing set. In our experiment,
due to computational limitations, we restrict our attention to the three smallest datasets in the SNAP
9
Published as a conference paper at ICLR 2019
collection (Youtube, DBLP and Amazon), and we restrict the largest community size to 200 nodes,
which is a conservative bound.
We compare the performance of GNN and LGNN models with GAT as well as the Community-
Affiliation Graph Model (AGM), which is a generative model proposed in Yang & Leskovec (2012b)
that captures the overlapping structure of real-world networks. Community detection can be achieved
by fitting AGM to a given network, which was shown to outperform some state-of-the-art algorithms.
Table 2 compares the performance, measured with a 3-class (C = {{1}, {2}, {1, 2}}) classification
accuracy UP to global permutation 1 什 2. GNN, LGNN, LGNN-S and GAT yield similar results
and outperform AGMfit. It further illustrates the benefits of data-driven models that strike the right
balance between expressivity and structural design.
	train/test	Avg IVI	Avg IEI		GNN	LGNN	LGNN-S	GAT	AGMfit
Amazon	805/142	"^60	161	Avg.	0.97	^O6-	-0.97	0.95	-0.90
				Std. Dev.	0.12	-0:13-	ɪn	0.13	
DBLP	4163/675-	"^6	77∏	Avg.	0.90	-0.90-	-0.89	0.88	-0.79
				Std. Dev.	0.13	^013-	-0.13	0.13	^0T8
Youtube	20000/1242	^^93	"^01	Avg.	0.91	^O2-	-0.91	0.90	-0.59
				Std. Dev.	0.11	ɪn	ɪn	0.13	^0T6
Table 2: Comparison of the node classification accuracy by different models on the three SNAP datasets. Note
that the average accuracy was computed graph-wise with each graph weighted by its size, while the standard
deviation was computed graph-wise with equal weights among the graphs.
7	Conclusion
In this work, we have studied data-driven approaches to supervised community detection with graph
neural networks. Our models achieve comparable performance to BP in binary SBM for various
SNRs, and outperform BP in the sparse regime of 5-class SBM that falls between the computational-
to-statistical gap. This is made possible by considering a family of graph operators including the
power graph adjacency matrices, and importantly by introducing the line graph equipped with the
non-backtracking matrix. We also provided a theoretical analysis of the optimization landscapes of
simplified linear GNN for community detection and showed the gap between the loss value at local
and global minima are bounded by quantities related to the concentration of certain random matricies.
One word of caution is that our empirical results are inherently non-asymptotic. Whereas models
trained for given graph sizes can be used for inference on arbitrarily sized graphs (owing to the
parameter sharing of GNNs), further work is needed in order to understand the generalization
properties as |V | increases. Nevertheless, we believe our work opens up interesting questions, namely
better understanding how our results on the energy landscape depend upon specific signal-to-noise
ratios, or whether the network parameters can be interpreted mathematically. This could be useful
in the study of computational-to-statistical gaps, where our model can be used to inquire about the
form of computationally tractable approximations. Another current limitation of our model is that it
presumes a fixed number of communities to be detected. Other directions of future research include
the extension to the case where the number of communities is unknown and varied, or even increasing
with |V |, as well as applications to ranking and edge-cut problems.
References
Emmanuel Abbe. Community detection and stochastic block models: recent developments. arXiv
preprint arXiv:1703.10146, 2017.
Emmanuel Abbe, Afonso S. Bandeira, and Georgina Hall. Exact recovery in the stochastic block
model. arXiv:1405.3267v4, 2014.
Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction networks
for learning about objects, relations and physics. In Advances in Neural Information Processing
Systems,pp. 4502-4510, 2016.
Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geometric
deep learning: going beyond euclidean data. IEEE Signal Processing Magazine, 2017.
10
Published as a conference paper at ICLR 2019
Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally
connected networks on graphs. arXiv:1312.6203., 2013.
Amin Coja-Oghlan, Florent Krzakala, Will Perkins, and Lenka Zdeborova. Information-theoretic
thresholds from the cavity method. arXiv preprint arXiv:1611.00814, 2016.
AUrelien Decelle, Florent Krzakala, Cristopher Moore, and Lenka Zdeborov叁 Asymptotic analysis
of the stochastic block model for modular networks and its algorithmic applications. Physical
Review E, 84(6):066106, 2011.
Michael Defferrard, Xavier Bresson, and Pierre Vndergheynst. Convolutional neural networks on
graphs with fast localized spectral filtering. In Advances in Neural Information Processing Systems,
pp. 3837-3845, 2016.
David Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, Rafael Gdmez-Bombarelli, Timo-
thy Hirzel, Alan Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning
molecular fingerprints. In Neural Information Processing Systems, 2015.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.
M. Gori, G. Monfardini, and F. Scarselli. A new model for learning in graph domains. In Proc.
IJCNN, 2005.
Karol Gregor and Yann LeCun. Learning fast approximations of sparse coding. ICML, 2010.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 770-778, 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information
Processing Systems, pp. 586-594, 2016.
Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and Patrick Riley. Molecular graph
convolutions: moving beyond fingerprints. Journal of computer-aided molecular design, 30(8):
595-608, 2016.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.
arXiv preprint arXiv:1609.02907, 2016.
Risi Kondor, Hy Truong Son, Horace Pan, Brandon Anderson, and Shubhendu Trivedi. Covariant
compositional networks for learning graphs. arXiv preprint arXiv:1801.02144, 2018.
Florent Krzakala, Cristopher Moore, Elchanan Mossel, Joe Neeman, Allan Sly, Lenka Zdeborova,
and Pan Zhang. Spectral redemption in clustering sparse networks. Proceedings of the National
Academy of Sciences, 110(52):20935-20940, 2013.
Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural
networks. arXiv preprint arXiv:1511.05493, 2015.
Laurent MassouliC. Community detection thresholds and the weak ramanujan property. In Pro-
ceedings of the forty-sixth annual ACM symposium on Theory of computing, pp. 694-703. ACM,
2014.
Elchanan Mossel, Joe Neeman, and Allan Sly. A proof of the block model threshold conjecture.
arXiv:1311.4115, 2014.
Mark EJ Newman. Modularity and community structure in networks. Proceedings of the national
academy of sciences, 103(23):8577-8582, 2006.
11
Published as a conference paper at ICLR 2019
Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural networks
for graphs. In International conference on machine learning, pp. 2014-2023, 2016.
Oliver Riordan and Nicholas Wormald. The diameter of sparse random graphs. Combinatorics,
Probability and Computing, 19(5-6):835-926, 2010.
Alaa Saade, Florent Krzakala, and Lenka Zdeborovd. Spectral clustering of graphs with the bethe
hessian. In Advances in Neural Information Processing Systems, pp. 406-414, 2014.
Abishek Sankararaman and FrangOiS Baccelli. Community detection on euclidean random graphs.
In Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, pp.
2181-2200. SIAM, 2018.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The
graph neural network model. IEEE Trans. Neural Networks, 20(1):61-80, 2009.
Ohad Shamir. Are resnets provably better than linear predictors? arXiv preprint arXiv:1804.06739,
2018.
Dan Spielman. Spectral graph theory, am 561, cs 662, 2015.
Sainbayar Sukhbaatar, Rob Fergus, et al. Learning multiagent communication with backpropagation.
In Advances in Neural Information Processing Systems, pp. 2244-2252, 2016.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,匕UkaSZ
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information
Processing Systems, pp. 5998-6008, 2017.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.
Jaewon Yang and Jure Leskovec. Defining and evaluating network communities based on ground-truth.
ICDM., 7(2):43-55, 2012a.
Jaewon Yang and Jure Leskovec. Community-affiliation graph model for overlapping network
community detection. Proceeding ICDM ’12 Proceedings of the 2012 IEEE 12th International
Conference on Data Mining, 390(.):1170-1175, 2012b.
Jonathan S Yedidia, William T Freeman, and Yair Weiss. Understanding belief propagation and its
generalizations. Exploring artificial intelligence in the new millennium, 8:236-239, 2003.
Pan Zhang. Robust spectral detection of global structures in the data by learning a regularization. In
Arxiv preprint, pp. 541-549, 2016.
A	Proof of Theorem 5.1
For simplicity and with an abuse of notation, in the remaining part we redefine L and L in the following
way, to be the negative of their original definition in the main section: Ln(β) = Eχn,γn β>J^β,
Ln(β) = Eχn,γn IITEXee. Thus, minimizing the loss function (5) is equivalent to maximizing the
function Ln (β) redefined here.
We write the Cholesky decomposition of EXn as EXn = RnRTn , and define An = Rn-1Yn(Rn-1)T,
An = EAn = R-IEYn(R-I)T, Bn = RnIXn(R-I)T, and ∆Bn = Bn - In. Given a symmetric
matrix K ∈ RM×M, we let λ1(K), λ2(K), ..., λM (K) denote the eigenvalues of K in nondecreasing
order.
First, we have
∖Ln(Bl) - Ln(βg )| ≤ ∖Ln(Bl ) - L n(!βl )| + IL n(βl) - L n (Bg )| + IL n(Bg ) - Ln(βg )|	(6)
Let us denote by βg a global minimum of the mean-field loss Ln . Taking a step further, we can
extend this bound to the following one (the difference is in the second term on the right hand side):
12
Published as a conference paper at ICLR 2019
Lemma A.1.
ILn (eI)- Ln(βg )| ≤ ∖Ln(Bl ) - L n(8l )| + IL n(βl) - L n (∕βg )| + IL n(8g ) - Ln(βg )|	(7)
Proof of Lemma A.1. We consider two separate cases: The first case is when Ln (βl) ≥ Ln (βg ).
Then Ln(Bl) - L n(βg ) ≥ Ln(βl) - L n(Bg ) ≥ 0, and so ∖Ln(Bl ) - Ln(βg )| ≤ ∖Ln(Bl ) - Lη(βl)1 +
.~ , _ , ~ , ≈ . .	. ~ , _ ,	_ , ..
|L n(βl ) - L n(Bg ) | +	IL n(Bg )	-	Ln(βg )|.
The other case is when Ln(βl) < Ln(βg). Note that Ln(βl) ≥ Ln(βg). Then ILn(βl) - Ln (βg)I ≤
._ , _ , ~ , . .	. ~ , _ ,	_ , . .	.	_	,	_	,	~	,	. .	. ~	,	_ ,	~	, ≈	. .	. ~	,	_	.
∖Ln(8l ) - L n(βl)1 +	∖Ln(βg )	-	Ln(8g )|	≤	ILn(同) -	L n(βl )| + IL n(βl) -	Ln(!βg )| + ∖Ln(βg	) -
Ln(βg)I.
□
Hence, to bound the "energy gap" ILn(βl) - Ln(βg)I, if suffices to bound the three terms on the right
hand side of Lemma A.1 separately. First, we consider the second term, ILn(βl) - Ln(βg)I.
Let γl	=	RnBl,γg	=	RRnβg	and	Yg	=	Rnβg.	Define	Sn(Y)	=	Ln(RnTY)	and	Sn(Y)	=
Ln(Rn-TY), for any Y ∈ RM. Thus, we apply a change-of-variable and try to bound ISn(Yl) -
~ ..
Sn(Yg )I.
Since βl is a local maximum of Ln λι(V2Ln(βl)) ≤ 0. Since V2Sn(YI) = R-1V2Ln(βl)R-T,
where Rn is invertible, We know that λι(V2Sn(Yl)) ≤ 0, thanks to the following lemma:
Lemma A.2. If R, Q ∈ RM×M, R is invertible, Q is symmetric and λq > 0 is an eigenvalue of Q,
then λι(RQRT) ≥ λ" (RRT)
Proof of Lemma A.2. Say Qw = λw for some vector w ∈ RM.
vT (RQRT)v = wTQw = λkwk2. Note that kwk2 = vTRRTv
λι(RQRτ) ≥ v(RQRT)v
≥ kwk2∕kM∣(RRT)≥λ ∙ λM(RRT)
Let v = R-T w. Then
≥ kvk2λM (RRT). Hence
□
Since	V2Sn(Yl)	=	V2Sn(Yl)	+	(V2Sngl)	-	V2Sn(Yl)),由ere is 0 ≥	λι(V2Sn(Yl))	≥
λι(V2Sn(Yl)) - kV2Sn(Yl) - V2Sn(Yl)k∙ Hence,
λl(V2Sn(Yl )) ≤ kV2Sn(Yl ) — V2Sn(Yl)k	(8)
Next, we relate the left hand side of the inequality above to cos(yi ,Yg), thereby obtaining an upper
bound on [1 一 cos2(yi,Yg)], which will then be used to bound ISn(Yl) — Sn(Tg)I∙
Lemma A.3. ∀Y ∈ Rd,
λl(V2Sn(Y)) ≥ ɪ {[1 - COS2(Y,Yg )] ∙ [λl(An) - λ2(An)] - 21∣ Y ∣∏∣VSn (Y) k}
Proof of Lemma A.3.
V2STn(Y) =2E
<YTY)An - (YTAnY)I +
(τTτP	+
=2E
Γ (YT Y)An - (YT AnY )I + 4[(yt Y)An - (YT AnY)I]YYT '
[	(YTY)2	+	(YTY)3	.
(9)
=2
Γ (YTY)An - (YTAnY)I
[	(YTYP
Thus, if we define Ql = (ytY)[(γTY)An — (ytAnY)I], Q2 = 4[(ytY)An — (ytAnY)I]yyt, we
have
2
V Sn(Y) = η~6q (QI- Q2)	(IO)
kY k6
To bound λ1(V2STn(Y)), we bound λ1(Q1) and kQ2 k as follows:
13
Published as a conference paper at ICLR 2019
Since An is symmetric, let ^ι,... YM be the orthonormal eigenvectors of An corresponding to
nonincreasing eigenvalues 1ι,...1M. Note that the global minimum satisfies Yg = ±γι. Write
Y = PM=I αiYi, and let ai
α
√pm=i α
.Then | cos(γ,7g)1 = I cos(γ,Yι)∣ = ∣αι∣.
Then,
M
M
λ1(Q1)=(YTY) 11	αi2-	1iαi2
i=1	i=1
≥(γTγ)
αi2 ) - α21	(11 - 12 )
(11)
=(YT γ)2[(1- α2 )(11-12)]
To bound kQ2 k:
M
M
M
[(YTY)An-(YTAnγ)I]γ = E Ik £ α2 - E liα2 αkγk
(12)
k=1
i=1
i=1
Note that given vectors v, w ∈ RM,
∣∣v ∙ WTk = IvTw|
Therefore,
Thus,
kQ2k =4
[lk(	αi2) -( liα2)]αkYk
MM
k=1 i=1
=4 J —2— YTVS(Y)
≤2(y T Y)2kY k∣VS(Y )k
i=1
(13)
λ1(Q1 - Q2) ≥λ1 (Q1) - ∣Q2∣
≥(yty)2([(1 - ɑ2)(1ι - 12)] - 2∣YkkVγS(Y)Il)
This yields the desired lemma.
(14)
Combining inequality 8 and Lemma A.3, we get
1 - cos2(Yι,Yg) ≤
2∣Ylk∙kVSn(Yl)k + " ∣V2Sn(Yl ) - V2Sn(Yl)I
λι(An)- λ2(An)
(15)
Thus, to bound the angle between Yl and Y, We can aim to bound IlVSn(Yι)∣ and ∣∣V2Sn(Y1)-
V2Sn(Yi) )∣ as functions of the quantities μn Vn and δn
Lemma A.4.
........ ≈.... , . _ _ 、
∣∣Ylk ∙ ∣VSn(Yι )∣ ≤ 2μnVnδn(1 + 3/ + δνn
(16)
Proof of Lemma A.4.
VSn(Y) = 2E-AP- - 2E (YTAnY)BnY
YTBnY	(YTBnY)2
VSn(Y) = 2EATy - 2E (YTAnY)Y
YTY	(YTY)2
(17)
M
□
(18)
14
Published as a conference paper at ICLR 2019
Combining equations 17 and 18, we get
, , ~ , .
VSn(Y) -VSn(Y)= E
(γT Bnγ)(γT γ)
Since VSn(Yl ) = 0, we have
2(YTAnYH(YTY)2BnY - (YTBnY)2Y] ^
(YT BnY)2(YTY )2
(19)
~ ...
kVSn(Yl)k
=E Γ 2(YT Yl - YT BnYl)AnYl _ 2(YT AnYl)[(YT YlyBnYl — (YT BnYl ¥Yl ] 1 Il
[(YT Bn Yl)(YT Yl)	(YT BnYl )2(YT Yl)2	J Il
≤ ɪ F J ∣λl(An)∣k∆Bnk + 3 ∣λι(An)∣k∆Bnk + & (An) ||| ∆Bn k 2 -
≤kYlk	[	∣λM (Bn)I	+	λM (Bn)	+	λM (Bn)	一
(20)
Then, by the generalized Holder,s inequality,
1
kVSn(Yl)k ≤k¾ h (Elλ1(An)13FkδBnk3F
+ 3 (Elλ1(An)l3FkδBnk3FʌpY	(21)
+ (F1》1(An)l3FkδBnk6Fʌp Y h
Hence, written in terms of the quantities μn, Vn and δn, we have
IlYl k ∙k VSn(Yl)Il ≤2(μnνnδn + 3μnνnδn + μnδnVn)	(22)
=2μnνnδn(1 + 3νn + δνn)
□
LemmaA.5. With δn = (Fk∆Bnk6)1, F∣λι(Bn)∣6 ≤ 64 + 63δn
Proof of Lemma A.5.
F∣λι(Bn)∣6 =FkBnk6
=FkI + ∆Bnk6
≤F(kIk + k∆Bnk)6
=F(1 + k∆Bnk)6
Note that
(23)
gma	F(1 +X)6 =FX6+6FX5+15FX4+20FX3+15FX2 +6FX+1	(24)
and for k ∈ {1, 2, 3, 4, 5}, ifX is a nonnegative random variable,
FXk =1X>1FXk + 1X≤1FXk
≤1 +1X≤1FX6	(25)
≤1 + FX6
Therefore, F∣λι(Bn)∣6 ≤ 64 + 63Fk∆Bnk6.	□
From now on, for simplicity, We introduce δn = (64 + 63δn)1, as a function of δn.
Lemma A.6. ∀Y ∈ RM,
kYlk2 ∙ ∣∣V2Sn(Y) - V2Sn(Y)k ≤μnVnδn(10 + 14Vn + 2δn〜 + 16Vn + 16®nVn
+ 8δn0 Vn2 + 8δn0 Vn + 8δn δn0 V)
(26)
15
Published as a conference paper at ICLR 2019
Proof of Lemma A.6.
where
V2Sn(Y) - V2Sn(Y) =2E[Hι] - 2E[H)] + 8E[H)] 一 8E[H4]	(27)
H = (YTY)An -(YTBnY)An 1 = ~(YT BnY)(YTY)-	(28)
H = (YTAnY)[(YTY)2Bn - (YTBnY)2]I)	(29)
2 =	(Y T By)2 (YTY)2 H = (YTAnY)[(γTγ)3BnYYTBT - (YTBnγ)3γγT]	
	(30)
3 =	(YT BnY )3(YTY)3	
H = (YTY)2AnYYTBn - (YTBnY)2AYYT	(31)
4 =	(YT BnY )2(YTY)2	
Thus, IN2Sn(Y) - V2Sn(Y)Il ≤ 2E∣∣Hι∣∣ +2E∣∣H2∣∣ +8E∣∣H3∣∣ +8E∣∣H4∣∣,andwetrytobound
each term on the right hand side separately.
For the first term, there is
ɪ ∣∆Bnk∣λl(An)∣
k lk - kY k2	∣λM (Bn )|
Applying generalized Holder,s inequality, we obtain
1
kYk2 ∙ EkHIk ≤ (E∣∖ 1『3丫 (E∣λι(An)∣3)1(Ek∆Bnk3)1
∖ ∖λM (Bn)∖3 )
≤μnνnδn .
(32)
(33)
For the second term, there is
(YTAnγ)[(γTY)2∆Bn 一 2(γTY)(γT∆Bnγ)I -(YT∆Bnγ)2I]
(YT BnY Y(TY)
Hence,
kH)k ≤ EP λ^(BJ ∣λl(An )∣(3k∆Bn k + k∆Bnk2)	(35)
Applying generalized Holder’s inequality, we obtain
1
kYk2 ∙ EkH)k ≤ (E,ʌ	ɜ .∣6丫 (E∣λι(An)∣3)3(Ek∆Bnk3)1
∖λM (Bn)∖6
+ (EH ：3、|6) 3 (E∣λι(An)∣3)3(Ek∆Bnk6)1
∖λM (Bn)∖6
≤μnνnδn(3νn + δnνn)
(36)
For H3 , note that
(YTY)3BnYYTBnT - (YT BnY)3YYT =(YTY)3(Bn - I)YYTBn + (YTY)3YYT(Bn -I)
+ [(YTY)3 - (YTBnY)3]YYT
=(YTY)3∆BnYYTBn + (YTY)3YYT∆Bn
+ [(YTBnY)2(-YT∆BnY)YYT + (YTBnY)(-YT∆BnY)YYT
+ (-YT∆BnY)YYT]
(37)
16
Published as a conference paper at ICLR 2019
Hence,
Thus,
H3 =(γTAnγ)h
(γTBnγ)3(γTγ)3
(-γTδBnY)YYT	(-YTδBnY)YYT i
(YT BnY)2(YTY) + (YT BnY )(Y TY )2 1
(38)
kH3k ≤ ɪ k* (in%(Bn)I + mBnk) +	^" + ^k k
(39)
Applying generalized Holder,s inequality, We obtain
1
kYk2 ∙ EkH3k ≤ (EI、1『6丫 (E∣λ1(An)I6) 1(Ek∆Bnk6)6(E∣λ1(Bn)∣6)6
∖ lλM (Bn)F/
+ 2 (E	1 Y (E∣λ1(An)∣3)3(Ek∆Bnk6)1
∣λM (Bn)∣6
1
+ (EI、1『6丫(E∣λι(An)∣3)3(Ek∆Bnk3)1
∣λM (Bn)∣6
+ (E n 1、|3 Y (E∣λ1(An )∣3) 3 (Ek∆Bnk3) 1
∣λM(Bn)∣3
≤μnνnδn (δn Vn + 2ν2 + Vn + I)
(40)
For the last term,
H4
[-2(YTY)(YT∆BnY)I - (YT∆BnY)2I]AnYYTBn + (YTBnY)2 An YYT ∆Bn
(YTBnY)2(YTY)2
(41)
Thus,
kH4k ≤ 赤 λ^1BJ ⑵ CBnk + k∆Bnk2)∣λ1(An)∣∣λ1(Bn)∣ + ^^ ∣ 后(Bn ) ∣∣ λι (An ) ∣∣∣ ∆Bn ∣∣
M n	M n	(42)
Applying generalized Holder’s inequality, We obtain
1
kYk2 ∙ EkH4k ≤2 (E∣λT(Bjpy (E∣λ1(An)∣3)1 (Ek∆Bnk6)6(E∣λ1(Bn)∣6)6
+ (E∣∖ 1『6Y (E∣λ1(An)∣6)1 (Ek∆Bnk6)3(E∣λ1(Bn)∣6)1
∣λM (Bn)∣6
1
+ (E∣λ4⅛)3 (E∣λ1(An)∣6)1 (Ek∆Bnk6)6(E∣λ1(Bn)∣6)1
≤μnνnδn (2νnδn + δnδnνn + δn νn )
(43)
Therefore, summing up the bounds above, We obtain
∣∣Ylk2 ∙ IV2Sn(Y) - V2Sn(Y)k ≤μnVnδn(10+ 14% + /nV + 16Vn + 16®nVn
+ 8δn0 Vn2 + 8δn0 Vn + 8δn δn0 V)
(44)
Hence, combining inequality 15, Lemma A.4 and Lemma A.6, We get
1 - Cos (Yl, Yg ) ≤ηn [4μn νnδn (1 + 3νnδn μn) + 2 μn νnδn (10 + 14 νn + 2δn νn + 16 νn
+16δn0 Vn + 8δn0 Vn2 + 8δn0 Vn + 8δnδn0 V)]
=Mnνnδnηn(9 + 19 νn + 5δnνn + (8^2 + 8δn νn + 4δn IVn + 4δn Vn + 4δnδn νn)
(45)
+
17
Published as a conference paper at ICLR 2019
For simplicity, wedefineC(δn, νn) = 9+19νn+5δnνn+8νn2+8δn0 νn+4δn0 νn2+4δn0 νn+4δnδn0 νn.
Thus,
1 - Cos2(γi,Yg ) ≤ μnVnδnηnC(δn,Vn)	(46)
□
Following the notations in the proof of Lemma A.3, We write Yl = PM=I α%Yi. Note that Yg = ±^ι,
and I Cos(Y,^i) I 二	二 Iα∕. Thus, L n(Bl ) =Sn(Yl) _p3aII_x 2l	(47) =~^M	W =	αi li i=1 αi	i=1
Since Yrb is positive Semidefinite, EYn is also positive Semidefinite, and hence An = RnEYn(R-I)T
is positive SemIdefinite as well. ThiS means that l ≥ 0,∀i ∈ {1,…，M}. Since Ln(βg) = Sn(Yg)=
Sn(YI) = lι, there is
IL n(βg ) - Ln(Bl)∖ ≤ (1 - α2)l1 ≤ (1 - cos2(Yl, Yg ))λ1(An)	(48)
Next, we bound the first and the third term on the right hand side of the inequality in Lemma A.1.
Lemma A.7. ∀β,
ILn(B) - Ln(B)I ≤ (Ek∆Bnk3) 3 ∙ (E∣λl(An )|3) 3 ∙ (e∣	幽
λM (Bn)
Proof of Lemma A.7. Let Y = TnTB.
~ , . . , ~
ILn(B) - Ln(B)I =Sn(Y)- Sn(Y)
=F (YTδBnY)(YTAnY)
(y T BnY )(yt y)	(50)
≤e k∆BnkIλl(An)I
一	IλM (Bn )I
Thus, we get the desired lemma by the generalized Holder,s inequality.
□
Combining inequality 46, inequality 48 and Lemma A.7, we get
1
3
+ (1 - COS2(Yl,7g ))λι(An )
≤2μnνnδn + μnνnδnηnC(δn, Vn) ∙ λ1(An)
(51)
Meanwhile,
._ , _ , ~ , ≈ . . ,. _ , _ , ~ ,-、..一 , ≈ , ~
ILn(Bg ) - Ln(Bg )I ≤ max{ILn(Bg ) - L n(Bg	Ln(Bg ) - L n(Bg ) I}
≤(Ek∆Bnk3)1 ∙ (EIλι(An)I3)1 ∙ (EIʒ-ɪʒI3)3	(52)
λM (Bn)
≤ μn νn δn
Hence,
Ln(Bg) ≥Ln(Bg) - μnνnδn
≥λ1 (An) - μnνnδn	(53)
≥η-1 - μnVnδn

ILn(Bl ) - Ln(Bg )I ≤2(Ek∆Bnk3) 1 ∙ (E1 λ] (An ) I3 ) 3 ∙ E
18
Published as a conference paper at ICLR 2019
, or
λ1 (An) ≤ Ln(βg ) + μnνnδn	(54)
Therefore,
| Ln (βl ) - Ln (Bg ) | ≤2μnνnδn + (I - cos (Yl, Yg ))[Ln (Bg ) + μnνnδn]
≤μn Vnδn [2 + ηnμn VnδnC(δn, Vn)] + ηnμn VnδnC(δn, Vn)Ln (Bg )
/丁 μ μn v f μn Vn δn [2 + nnMnVn8nC(8n, Vn)]	X E X ∖ ɪ /eeʌ
≤Ln ( βg ) <	—I	Z	+ ηnμn Vn δn C ( δn, Vn ) f (55)
I	η- - μnVnδn	J
_ 2ηnμn Vn*n [2 + C]δn, Vn)] 不，凡、
=	1 - ηnμnVnδn	IAe)
Hence, We have proved the theorem, with eηn,μn,νn,δn =
2ηnμnνnδn [2 + C(6n , Vn)]	∣ ∣
1 - ηnμn Vn δn
B Background
B.1 Graph Min-Cuts and Spectral Clustering
We consider graphs G = (V, E), modeling a system of N = |V | elements presumed to exhibit some
form of community structure. The adjacency matrix A associated with G is the N × N binary matrix
such that Ai,j = 1 when (i, j) ∈ E and 0 otherwise. We assume for simplicity that the graphs are
undirected, therefore having symmetric adjacency matrices. The community structure is encoded in a
discrete label vector s : V → {1, . . . , C} that assigns a community label to each node, and the goal
is to estimate s from observing the adjacency matrix.
In the binary case, we can set s(i) = ±1 without loss of generality. Furthermore, we assume that the
communities are associative, which means two nodes from the same community are more likely to be
connected than two nodes from the opposite communities. The quantity
(1 - s(i)s(j))Ai,j
i,j
measures the cost associated with cutting the graph between the two communities encoded by s, and
we wish to minimize it under appropriate constraints (Newman, 2006). Note that Pi,j Ai,j = sT Ds,
with D = diag(A1) (called the degree matrix), and so the cut cost can be expressed as a positive
semidefinite quadratic form
min sT(D - A)s = sT∆s
s(i)=±1
that we wish to minimize. This shows a fundamental connection between the community structure
and the spectrum of the graph Laplacian ∆ = D -A, which provides a powerful and stable relaxation
of the discrete combinatorial optimization problem of estimating the community labels for each
node. The eigenvector of ∆ associated with the smallest eigenvalue is, trivially, 1, but its Fiedler
vector (the eigenvector associated with the second smallest eigenvalue) reveals important community
information of the graph under appropriate conditions (Newman, 2006), and is associated with the
graph conductance under certain normalization schemes (Spielman, 2015).
Given linear operator L(A) extracted from the graph (that we assume symmetric), we are thus
interested in extracting eigenvectors at the edge of its spectrum. A particularly simple algorithm is
the power iteration method. Indeed, the Fiedler vector of L(A) can be obtained by first extracting the
leading eigenvector v of A = kL(A)kI - L(A), and then iteratively compute
y(n) = Aw(n-1)
w(n)
y(n) — hy(n), Viv
∣∣y(n) - hy(n), vivk
Unrolling power iterations and recasting the resulting model as a trainable neural network is akin to
the LISTA sparse coding model, which unrolled iterative proximal splitting algorithms (Gregor &
LeCun, 2010).
Despite the appeal of graph Laplacian spectral approaches, it is known that these methods fail in
sparsely connected graphs (Krzakala et al., 2013) . Indeed, in such scenarios, the eigenvectors of
19
Published as a conference paper at ICLR 2019
the graph Laplacian concentrate on nodes with dominant degrees, losing their correlation with the
community structure. In order to overcome this important limitation, people have resorted to ideas
inspired from statistical physics, as explained next.
B.2	Probabilistic Graphical Models and Belief-Propagation
Graphs with labels on nodes and edges can be cast as a graphical model where the aim of clustering
is to optimize label agreement. This can be seen as a posterior inference task. If we simply assume
the graphical model is a Markov Random Field (MRF) with trivial compatibility functions for cliques
greater than 2, the probability of a label configuration σ is given by
P(σ) = τ1 Y φi(σi) Y ψij(σi,σj).
i∈V	ij∈E
(56)
Generally, computing marginals of multivariate discrete distributions is exponentially hard. For
instance, in the case of P(σi) we are summing over |X |n-1 terms (where X is the state space of
discrete variables). But if the graph is a tree, we can factorize the MRF more efficiently to compute
the marginals in linear time via a dynamic programming method called the sum-product algorithm,
also known as belief propagation (BP). An iteration of BP is given by
bi→j (σi)
φ	φi(σi)
Zi→j
ψik(σi, σk)bk→i(σk).
k∈δi∖j σk∈X
(57)
The beliefs (bi→j(σi)) are interpreted as the marginal distributions of σi. Fixed points of BP can be
used to recover marginals of the MRF above. In the case of the tree, the correspondence is exact:
Pi(σi) = bi(σi). Certain sparse graphs, like SBM with constant degree, are locally similar to trees for
such an approximation to be successful (Mossel et al., 2014). However, convergence is not guaranteed
in graphs that are not trees. Furthermore, in order to apply BP, we need a generative model and
the correct parameters of the model. If unknown, the parameters can be derived using expectation
maximization, further adding complexity and instability to the method since it is possible to learn
parameters for which BP does not converge.
B.3	Non-backtracking operator and Bethe Hessian
The BP equations have a trivial fixed-point where every node takes equal probability in each group.
Linearizing the BP equation around this point is equivalent to spectral clustering using the non-
backtracking matrix (NB), a matrix defined on the directed edges of the graph that indicates whether
two edges are adjacent and do not coincide. Spectral clustering using NB gives significant improve-
ments over spectral clustering with different versions of the Laplacian matrix L and the adjacency
matrix A. High degree fluctuations drown out the signal of the informative eigenvalues in the case
of A and L, whereas the eigenvalues of NB are confined to a disk in the complex plane except for
the eigenvalues that correspond to the eigenvectors that are correlated with the community structure,
which are therefore distinguishable from the rest.
NB matrices are still not optimal in that they are matrices on the edge set and also asymmetric,
therefore unable to enjoy tools of numerical linear algebra for symmetric matrices. Saade et al. (2014)
showed that a spectral method can do as well as BP in the sparse SBM using the Bethe Hessian matrix
defined by BH(r) := (r2 - 1)I - rA + D, where r is a scalar parameter. This is due to a one-to-one
correspondence between the fixed points of BP and the stationary points of the Bethe free energy
(corresponding Gibbs energy of the Bethe approximation) (Saade et al., 2014). The Bethe Hessian is
a scaling of the Hessian of the Bethe free energy at an extrema corresponding to the trivial fixed point
of BP. Negative eigenvalues of BH(r) correspond to phase transitions in the Ising model where new
clusters become identifiable. The success of the spectral method using the Bethe Hessian gives a
theoretical motivation for having a family of matrices including I, D and A in our GNN defined in
Section 4, because in this way the GNN is capable of expressing the algorithm of performing power
iteration on the Bethe Hessian. While belief propagation requires a generative model, and the spectral
method using the Bethe Hessian requires the selection of the parameter r, whose optimal value also
depends on the underlying generative model, the GNN does not need a generative model and is able
to learn and then make predictions in a data-driven fashion.
20
Published as a conference paper at ICLR 2019
B.4	Stochastic Block Model
We briefly review the main properties needed in our analysis, and refer the interested reader to
Abbe (2017) for an excellent recent review. The stochastic block model (SBM) is a random graph
model denoted by SBM(n, p, q, C). Implicitly there is an F : V → {1, . . . , C} associated with
each SBM graph, which assigns community labels to each vertex. One obtains a graph from this
generative model by starting with n vertices and connecting any two vertices u, v independently at
random with probability p if F (v) = F (u), and with probability q if F (v) 6= F (u). We say the
SBM is balanced if the communities are the same size. Let Fn : V → {1, C} be our predicted
community labels for SBM (n, p, q, C). We say that the Fn’s give exact recovery on a sequence
{SBM(n1p, q)}n if P(Fn = Fn) →n 1, and give weak recovery or detection if ∃e > 0 such that
P(∣Fn - Fn| ≥ l/k + E) →n 1 (i.e Fn's do better than random guessing).
It is harder to tell communities apart if P is close to q (if P = q We just get an Erdos Renyi random
graph, which has no communities). In the two community case, It was shown that exact recovery
is possible on SBM(n,p = a ln n, q = b ln n) if and only if a+b ≥ 1 + √ab (Mossel et al., 2014;
Abbe et al., 2014). For exact recovery to be possible, p, q must grow at least O(log n) or else the
sequence of graphs will not be connected, and thus the vertex labels will be underdetermined. There
is no information-computation gap in this regime, and so there exist polynomial time algorithms when
recovery is possible (Abbe, 2017; Mossel et al., 2014)). In the sparser regime of constant degree,
SBM(n,p = a, q = n), detection is the best we could hope for. The constant degree regime is also
of most interest to us for real world applications, as most large datasets have bounded degree and
are extremely sparse. It is also a very challenging regime; spectral approaches using the Laplacian
in its various (un)normalized forms or the adjacency matrix, as well as semidefinite programming
(SDP) methods do not work well in this regime due to large fluctuations in the degree distribution
that prevent eigenvectors from concentrating on the clusters (Abbe, 2017). Decelle et al. (2011)
first proposed the BP algorithm on the SBM, which was proven to yield Bayesian optimal values in
Coja-Oghlan et al. (2016).
In the constant degree regime with k balanced communities, the signal-to-noise ratio is defined as
SNR = (a - b)2/(k(a + (k + 1)b)), and the Kesten-Stigum (KS) threshold is given by SNR = 1
(Abbe, 2017). When SNR > 1, detection can be solved in polynomial time by BP (Abbe, 2017;
Decelle et al., 2011). For k = 2, it has been shown that when SNR < 1, detection is not solvable,
and therefore SNR = 1 is both the computational and the information theoretic threshold (Abbe,
2017). For k > 4, it has been shown that for some SNR < 1, there exists non-polynomial time
algorithms that are able to solve the detection problem (Abbe, 2017). Furthermore, it is conjectured
that no polynomial time algorithm can solve detection when SNR < 1, in which case a gap would
exist between the information theoretic threshold and the KS threshold (Abbe, 2017).
C Further experiments
C.1 Geometric Block Model
The success of belief propagation on
Table 3: Overlap performance (in percentage) of GNN and LGNN
on graphs generated by the Geometric Block Model compared with
two spectral methods
Model	S = 1	S = 2	S = 4
Norm. Laplacian	1 ± 0.5	1 ± 0.6	-1±1-
Bethe Hessian	18 ± 1	38 ± 1	38 ± 2
GNN	20 ± 0.4	39 ± 0.5	39 ± 0.5
LGNN	22 ± 0.4	50 ± 0.5	76 ± 0.5
the SBM relies on its locally hyper-
bolic properties, which make it tree-
like with high probability. This be-
havior is completely different if one
considers random graphs with locally
Euclidean geometry. The Geometric
Block Model (Sankararaman & Bac-
celli, 2018) is a random graph gener-
ated as follows. We start by sampling
n points x1, . . . , xn i.i.d. from a Gaus-
sian mixture model given by means μι,... μk ∈ Rd at distances S apart and identity covariances.
The label of each sampled point corresponds to which Gaussian it belongs to. We then draw an edge
between two nodes i,j if kxi - Xjk ≤ T∕√n. Due to the triangle inequality, the model contains
a large number of short cycles, which affects the performance of loopy belief propagation. This
21
Published as a conference paper at ICLR 2019
Figure 4. GNN mixture (Graph Neural Network trained on a mixture of SBM with average degree 3), GNN
full mixture (GNN trained over different SNR regimes), BH(√Γ) and BH(-√d). left: k = 2. We verify that
BH(r) models cannot perform detection at both ends of the spectrum simultaneously.
motivates other estimation algorithms based on motif-counting that require knowledge of the model
likelihood function (Sankararaman & Baccelli, 2018).
Table 3 shows the performance of GNN and LGNN on the binary GBM model, obtained with d = 2,
n = 500, T = 5√2 and varying S, as well as the performances of two spectral methods, using
respectively the normalized Laplacian and the Bethe Hessian, which approximates BP around its
stationary solution. We note that LGNN model, thanks to its added flexibility and the multiscale
nature of its generators, is able to significantly outperform both spectral methods as well as the
baseline GNN.
C.2 Mixture of binary SBM
We report here our experiments on the SBM mixture, generated with
G 〜SBM(n = 1000,p = kd — q,q 〜Unif(0, d — Pd), C = 2),
where the average degree dis either fixed constant or also randomized with d 〜 Unif(I,t). Figure 4
shows the overlap obtained by our model compared with several baselines. Our GNN model is either
competitive with BH or outperforms BH, which achieves the state of the art along with BP Saade et al.
(2014), despite not having any access to the underlying generative model (especially in cases where
GNN was trained on a mixture of SBM and thus must be able to generalize the r parameter in BH).
They all outperform by a wide margin spectral clustering methods using the symmetric Laplacian
and power method applied to kBH kI — BH using the same number of layers as our model. Thus
GNN’s ability to predict labels goes beyond approximating spectral decomposition via learning the
optimal r for BH(r). The model architecture could allow it to learn a higher dimensional function
of the optimal perturbation of the multiscale adjacency basis, as well as nonlinear power iterations,
that amplify the informative signals in the spectrum.
C.3 Ablation studies of GNN and LGNN
Compared to f, each of h, i and k has one fewer operator in F, and j has two fewer. We see that with
the absence of A(2), k has much worse performance than the other four, indicating the importance
of the power graph adjacency matrices. Interestingly, with the absence of I, i actually has better
average accuracy than f. One possibly explanation is that in SBM, each node has the same expected
degree, and hence I may be not very far from D, which might make having both I and D in the
family redundant to some extent.
Comparing GNN models a, b and c, we see it is not the case that having larger J will always lead to
better performance. Compared to f, GNN models c, d and e have similar numbers of parameters but
22
Published as a conference paper at ICLR 2019
		#layers	#features	J	FA	#parameters	Avg.	Std. Dev.
(a)	GNN	30	8	2	I, D, A, A2	8621	0.1792	0.0385
(b)	GNN	30	8	4	I, D, A,..., A(4)	12557	0.1855	0.0438
(c)	GNN	30	8	11	I, D, A,..., A(11)	26333	0.1794	0.0359
(d)	GNN	30	15	2	I, D, A, A(2)	28760	0.1894	0.0388
(e)	GNN	30	12	4	I, D, A,..., A(4)	27273	0.1765	0.0371
(f)	LGNN	30	8	2	I, D, A, A(2)	25482	0.2073	0.0481
(g)	LGNN-L	30	8	2	I, D, A, A(2)	25482	0.1822	0.0395
(h)	LGNN	30	8	2	I, A, A(2)	21502	0.1981	0.0529
(i)	LGNN	30	8	2	D, A, A(2)	21502	0.2212	0.0581
(j)	LGNN	30	8	2	A, A(2)	17622	0.1954	0.0441
(k)	LGNN	30	8	1	I, D, A	21502	0.1673	0.0437
(l)	LGNN-S	30	8	2	I, D, A, A(2)	21530	0.1776	0.0398
Table 4: The effects of different architectures and choices of the operator family for GNN and LGNN, as
demonstrated by their performance on the 5-class dissociative SBM experiments with the exact setup as in
Section 6.2. For LGNN, FB is the same as FA except for changing A to B.
all achieve worse average test accuracy, indicating that the line graph structure is essential for the
good performance of LGNN in this experiment. In addition, l also performs worse than f, indicating
the significance of the non-backtracking line graph compared to the symmetric line graph.
23