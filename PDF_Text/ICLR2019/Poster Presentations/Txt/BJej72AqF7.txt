Published as a conference paper at ICLR 2019
A Max-Affine Spline Perspective of
Recurrent Neural Networks
Zichao Wang, Randall Balestriero & Richard G. Baraniuk
Department of Electrical and Computer Engineering
Rice University
Houston, TX 77005, USA
{zw16,rb42,richb}@rice.edu
Ab stract
We develop a framework for understanding and improving recurrent neural net-
works (RNNs) using max-affine spline operators (MASOs). We prove that RNNs
using piecewise affine and convex nonlinearities can be written as a simple piece-
wise affine spline operator. The resulting representation provides several new per-
spectives for analyzing RNNs, three of which we study in this paper. First, we
show that an RNN internally partitions the input space during training and that
it builds up the partition through time. Second, we show that the affine slope
parameter of an RNN corresponds to an input-specific template, from which we
can interpret an RNN as performing a simple template matching (matched filter-
ing) given the input. Third, by carefully examining the MASO RNN affine map-
ping, we prove that using a random initial hidden state corresponds to an explicit
`2 regularization of the affine parameters, which can mollify exploding gradients
and improve generalization. Extensive experiments on several datasets of various
modalities demonstrate and validate each of the above conclusions. In particular,
using a random initial hidden states elevates simple RNNs to near state-of-the-art
performers on these datasets.
1	Introduction
Recurrent neural networks (RNNs) are a powerful class of models for processing sequential inputs
and a basic building block for more advanced models that have found success in challenging prob-
lems involving sequential data, including sequence classification (e.g., sentiment analysis (Socher
et al., 2013) , sequence generation (e.g., machine translation (Bahdanau et al., 2014)), speech recog-
nition (Graves et al., 2013), and image captioning (Mao et al., 2015). Despite their success, however,
our understanding of how RNNs work remains limited. For instance, an attractive theoretical result
is the universal approximation property that states that an RNN can approximate an arbitrary func-
tion (Schafer & Zimmermann, 2006; Siegelmann & Sontag, 1995; Hammer, 2000). These classical
theoretical results have been obtained primarily from the dynamical system (Siegelmann & Sontag,
1995; Schafer & Zimmermann, 2006)and measure theory (Hammer, 2000) perspectives. These
theories provide approximation error bounds but unfortunately limited guidance on applying RNNs
and understanding their performance and behavior in practice.
In this paper, we provide a new angle for understanding RNNs using max-affine spline operators
(MASOs) (Magnani & Boyd, 2009; Hannah & Dunson, 2013) from approximation theory. The
piecewise affine approximations made by compositions of MASOs provide a new and useful frame-
work to study neural networks. For example, Balestriero & Baraniuk (2018); Balestriero & Baraniuk
(2018a) have provided a detailed analysis in the context of feedforward networks. Here, we go one
step further and find new insights and interpretations from the MASO perspective for RNNs. We
will see that the input space partitioning and matched filtering links developed in Balestriero &
Baraniuk (2018); Balestriero & Baraniuk (2018a) extend to RNNs and yield interesting insights into
their inner workings. Moreover, the MASO formulation of RNNs enables us to theoretically justify
the use ofa random initial hidden state to improve RNN performance.
For concreteness, we focus our analysis on a specific class of simple RNNs (Elman, 1990) with
piecewise affine and convex nonlinearities such as the ReLU (Glorot et al., 2011). RNNs with
1
Published as a conference paper at ICLR 2019
Figure 1: Visualization of an RNN that highlights a cell (purple), a layer (red) and the initial hidden state of
each layer (green). (Best viewed in color.)
such nonlinearities have recently gained considerable attention due to their ability to combat the
exploding gradient problem; with proper initialization (Le et al., 2015; Talathi & Vartak, 2016) and
clever parametrization of the recurrent weight (Arjovsky et al., 2016; Wisdom et al., 2016; Jing
et al., 2017; Hyland & Ratsch, 2017; Mhammedi et al., 2017; HeIfrich et al., 2018), these RNNs
achieve performance on par with more complex ones such as LSTMs. Below is a summary of our
key contributions.
Contribution 1. We prove that an RNN with piecewise affine and convex nonlinearities can be
rewritten as a composition of MASOs, making it a piecewise affine spline operator with an elegant
analytical form (Section 3).
Contribution 2. We leverage the partitioning of piecewise affine spline operators to analyze the
input space partitioning that an RNN implicitly performs. We show that an RNN calculates a new,
high-dimensional representation (the partition code) of the input sequence that captures informative
underlying characteristics of the input. We also provide a new perspective on RNN dynamics by
visualizing the evolution of the RNN input space partitioning through time (Section 4).
Contribution 3. We show the piecewise affine mapping in an RNN associated with a given input
sequence corresponds to an input-dependent template, from which we can interpret the RNN as
performing greedy template matching (matched filtering) at every RNN cell (Section 5).
Contribution 4. We rigorously prove that using a random (rather than zero) initial hidden state in
an RNN corresponds to an explicit regularizer that can mollify exploding gradients. We show em-
pirically that such a regularization improves RNN performance (to state-of-the-art) on four datasets
of different modalities (Section 6).
2	Background
Recurrent Neural Networks (RNNs). A simple RNN unit (Elman, 1990) per layer ` and time step
t, referred to as a “cell,” performs the following recursive computation
h(',t) = σ(W(')h('-1,t) + w(')h(',t-1) + b⑶),	⑴
where h(`,t) is the hidden state at layer ` and time step t, h(0,t) := x(t) which is the input sequence,
σ is an activation function and W('), WT('), and b(') are time-invariant parameters at layer '. h(',0)
is the initial hidden state at layer ` which needs to be set to some value beforehand to start the
RNN recursive computation. Unrolling the RNN through time gives an intuitive view of the RNN
dynamics, which we visualize in Figure 1. The output of the overall RNN is typically an affine
transformation of the hidden state of the last layer L at time step t
z(t) = W h(L,t) + b.	(2)
In the special case where the RNN has only one output at the end of processing the entire input
sequence, the RNN output is an affine transformation of the hidden state at the last time step, i.e.,
z(T) = Wh(L,T) + b.
Max-Affine Spline Operators (MASOs). A max-affine spline operator (MASO) is piecewise
affine and convex with respect to each output dimension k = 1, . . . , K. It is defined as a parametric
function S : RD → RK with parameters A ∈ RK ×R×D and B ∈ RK ×R . A MASO leverages K
2
Published as a conference paper at ICLR 2019
independent max-affine splines (Magnani & Boyd, 2009), each with R partition regions. Its output
for output dimension k is produced via
[y]k = [S(x)]k = max {h[A]k,r,∙ , Xi + [B]k,r},	⑶
r=1,...,R
where x ∈ RD and y ∈ RK are dummy variables that respectively denote the input and output of
the MASO S and(,, •)denotes inner product. The three subscripts of the “slope" parameter [A]k,r,d
correspond to output k, partition region r, and input signal index d. The two subscripts of the “bias”
parameter [B]k,r correspond to output k and partition region r.
We highlight two important and interrelated MASO properties relevant to the discussions throughout
the paper. First, a MASO performs implicit input space partitioning, which is made explicit by
rewriting (3) as
R
[y]k= X[Q]k,r (h[A]k,r,∙ , Xi + [B]k,r) ,	(4)
r=1
where Q ∈ RK ×R is a partition selection matrix1 calculated as
[Q]k,r =	l(r =	[r*]k),	where	[r*]k	= argmax([A]k,r,∙	,	x〉+	[B]k,r .	(5)
r = 1,…，R
Namely, Q contains K stacked one-hot row vectors, each of which selects the [r*]kh partition of the
input space that maximizes (4) for output dimension k . As a consequence, knowing Q is equivalent
to knowing the partition of an input X that the MASO implicitly computes. We will use this property
in Section 4 to provide new insights into RNN dynamics.
Second, given the partition r* that an input belongs to, as determined by (5), the output of the MASO
of dimension k from (3) reduces to a simple affine transformation of the input
[y]k = [A]k,∙x + [B]k , where [A]k,∙ = [A]k,[r*]k and [B]k,∙ = [B]k,[r*]k .	(6)
Here, the selected affine parameters A ∈ RK ×D and B ∈ RK are specific to the input’s partition
region [r*]k , which are simply the [r*]/ slice and [r*]/ column of A and B, respectively, for
output dimension k. We emphasize that A and B are input-dependent; different inputs X induce
different A and B .1 2 We will use this property in Section 5 to link RNNs to matched filterbanks.
3 RNNs as Piecewise Affine Spline Operators
We now leverage the MASO framework to rewrite, interpret, and analyze RNNs. We focus on RNNs
with piecewise affine and convex nonlinearities in order to derive rigorous analytical results. The
analysis of RNNs with other nonlinearities is left for future work.
We first derive the MASO formula for an RNN cell (1) and then extend to one layer of a time-
unrolled RNN and finally to a multi-layer, time-unrolled RNN. Let z(',t) = [h('-1,t)>, h(',t-1)> ]>
be the input to an RNN cell that is the concatenation of the current input h(`-1,t) and the previous
hidden state h(',t-1). Then We have the following result, which is a straightforward extension of
Proposition 4 in Balestriero & Baraniuk (2018).
Proposition 1. An RNN cell of the form (1) is a MASO with
h(',t) =A(',t)z(',t) + B(',t),	(7)
where A(',t) = A夕t) [W('), Wj(')] and B(t) = A*)b(')are the affine parameters and A夕t) is the
affine parameter corresponding to the piecewise affine and convex nonlinearity σ(∙)that depends on
the cell input z(`,t).
1Prior work denotes the partition selection matrix as T . But in the context of RNNs, T usually denotes the
length of the input sequence. Thus we denote this matrix as Q in this work to avoid notation conflicts.
2The notation for the affine spline parameters A and B in (Balestriero & Baraniuk, 2018; Balestriero &
Baraniuk, 2018b) are A[x] and B [x], respectively, in order to highlight their input dependency. In this paper,
we drop the input dependency when writing these affine parameters to simplify the notation, and we use brackets
to exclusively denote indexing or concatenation.
3
Published as a conference paper at ICLR 2019
We now derive an explicit affine formula for a time-unrolled RNN at layer '. Let h(`-1) =
[h('-1,I),…,h('-1,T) ] be the entire input sequence to the RNN at layer ', and let h(') =
[h(',1)τ,…,h(',T)T]> be all the hidden states that are output at layer '. After some algebra and
simplification, we arrive at the following result.
Theorem 1.	The `th layer of an RNN is a piecewise affine spline operator defined as
∕h(',T)
.
.
.
∖h(',1)
(心T ... a1'T∖ ∕A(',t 1W (') ...	0
∖ 0	... A^1J ∖	0	... A(',1)W (')
X-------------V--------------
upper triangular
^™^{^^^™
diagonal
∕h('-1,τ)
.
.
.
∖h('τ,1)
}
/ PP A('TB(',t) +A0'Th(',叶
+ t=T	= A(') h('-1) + B(')	(8)
+	∙	= yiRNNh	+ BRNN， (8)
.
.
∖ Al'lB(',t)+A0'1 h('，0)
where AaT，= (Qs=T，A?'S,WF)) fort < T0 and identity otherwise,3 h(',0) is the initial hidden
state of the RNN at layer ', and ARNN and BRNN are affine parameters that depend on the layer
input h('-1) and the initial hidden state h(',0).
We present the proof for Theorem 1 in Appendix G. The key point here is that, by leveraging
MASOs, we can represent the time-unrolled RNN as a simple affine transformation of the entire
input sequence (8). Note that this affine transformation changes depending on the partition region
in which the input belongs (recall (4) and (5)). Note also that the initial hidden state affects the
layer output by influencing the affine parameters and contributing a bias term A0'th(',o) to the bias
parameter BRNn∙ We study the impact of the initial hidden state in more detail in Section 6.
We are now ready to generalize the above result to multi-layer RNNs. Let X = [x(1)T,…,X(T)T ] >
be the input sequence to a multi-layer RNN, and let Z = [z(I)T, ∙∙∙ , Z(T)T]> be the output se-
quence. We state the following result for the overall mapping ofa multi-layer RNN.
Theorem 2.	The output ofan L-layer RNN is a piecewise affine spline operator defined as
Z=Wh(L)+b=W(ARNNX+BRNN)+b,	(9)
where ARNN = QI=L ARNN and BRNN = P3 (QL=' ARNN) BRNN are the affine parameters
of the L-layer RNN. W and b are parameters of the fully connected output layer, where W =
[W, W, . . . , W] when the RNN outputs at every time step and W = [W, 0, . . . , 0] when the RNN
outputs only at the last time step.
Theorem 2 shows that, using MASOs, we have a simple, elegant, and closed-form formula showing
that the output of an RNN is computed locally via very simple functions. This result is proved by
recursively applying the proof for Theorem 1.
The affine mapping formula (9) opens many doors for RNN analyses, because we can shed light on
RNNs by applying established matrix results. In the next sections, we provide three analyses that
follow this programme. First, we show that RNNs partition the input space and that they develop the
partitions through time. Second, we analyze the forms of the affine slope parameter and link RNNs
to matched filterbanks. Third, we study the impact of the initial hidden state to rigorously justify
the use of randomness in initial hidden state. From this point, for simplicity, we will assume a zero
initial hidden state unless otherwise stated.
4	Internal Input Space Partitioning in RNNs
The MASO viewpoint enables us to see how an RNN implicitly partitions its input sequence through
time, which provides a new perspective of its dynamics. To see this, first recall that, for an RNN
3In our context, Qn=m ai := am ∙ am-ι ∙ ∙ ∙ an+ι ∙ an for m > n as opposed to the empty product.
4
Published as a conference paper at ICLR 2019
300 time steps
500 time steps
All (784) time steps
Figure 2: t-SNE (van der Maaten & Hinton, 2008) visualization of the evolution of the RNN partition codes
of input sequences from the MNIST test set. Each color represents one of the ten classes. We see clearly that
the RNN gradually develops and refines the partition codes through time to separate the classes.
cell, the piecewise affine and convex activation nonlinearity partitions each dimension of the cell
input z (`,t) into R regions (for ReLU, R = 2). Knowing the state of the nonlinearity (which
region r is activated) is thus equivalent to knowing the partition of the cell input. For a multi-layer
RNN composed of many RNN cells (recall Figure 1), the input sequence partition can be retrieved
by accessing the collection of the states of all of the nonlinearities; each input sequence can be
represented by a partition “code” that determines the partition to which it belongs.
Since an RNN processes an input sequence one step at a time, the input space partition is gradually
built up and refined through time. As a consequence, when seen through the MASO lens, the
forward pass of an RNN is simply developing and refining the partition code of the input sequence.
Visualizing the evolution of the partition codes can be potentially beneficial for diagnosing RNNs
and understanding their dynamics.
As an example, we demonstrate the evolution of the partition codes of a one-layer ReLU RNN
trained on the MNIST dataset, with each image flattened into a 1-dimensional sequence so that
input at each time step is a single pixel. Details of the model and experiments are in Appendix C.
Since the ReLU activation partitions its input space into only 2 regions , we can retrieve the RNN
partition codes of the input images simply by binarizing and concatenating all of the hidden states.
Figure 2 visualizes how the partition codes of MNIST images evolve through time using t-SNE,
a distance-preserving dimensionality reduction technique (van der Maaten & Hinton, 2008). The
figure clearly shows the evolution of the partition codes from hardly any separation between classes
of digits to forming more and better separated clusters through time. We can also be assured that
the model is well-behaved, since the final partition shows that the images are well clustered based
on their labels. Additional visualizations are available in Section D.
5	RNNs as Matched Filterbanks
The MASO viewpoint enables us to connect RNNs to classical signal processing tools like the
matched filter. Indeed, we can directly interpret an RNN as a matched filterbank, where the clas-
sification decision is informed via the simple inner product between a “template” and the input
sequence. To see this, we follow an argument similar to that in Section 4. First, note that the
slope parameter A(',t) for each RNN cell is a “locally optimal template” because it maximizes each
of its output dimensions over the R regions that the nonlinearity induces (recall (3) and (7)). For
a multi-layer RNN composed of many RNN cells, the overall “template” ARNN corresponds to
the composition of the optimal templates from each RNN cell, which can be computed simply via
dz/dx (recall (9)).
Thus, we can view an RNN as a matched filterbank whose output is the maximum inner product
between the input and the rows of the overall template ARNN (van Trees, 1992; 2013). The overall
template is also known in the machine learning community as a salience map; see Li et al. (2016)
for an example of using saliency maps to visualize RNNs. Our new insight here is that a good tem-
plate produces a larger inner product with the input regardless of the visual quality of the template,
thus complementing prior work. The template matching view of RNNs thus provides a principled
methodology to visualize and diagnose RNNs by examining the inner products between the inputs
and the templates.
To illustrate the matched filter interpretation, we train a one-layer ReLU RNN on the polarized Stan-
ford Sentiment Treebank dataset (SST-2) (Socher et al., 2013), which poses a binary classification
5
Published as a conference paper at ICLR 2019
Figure 3: Templates corresponding to the correct (left) and incorrect class (right) of a negative sentiment input
from the SST-2 dataset. Each column contains the gradient corresponding to an input word. Quantitatively, we
can see that the inner product between input and the correct class template (left) produces a larger value than
that between input and the incorrect class template (right).
problem, and display in Figure 3 the templates corresponding to the correct and incorrect classes
of an input where the correct class is a negative sentiment. We see that the input has a much larger
inner product with the template corresponding to the correct class (left plot) than that corresponding
to the incorrect class (right plot), which informs us that the model correctly classifies this input.
Additional experimental results are given in Appendix E.
6	Improving RNNs Via Random Initial Hidden State
In this section, we provide a theoretical motivation for the use of a random initial hidden state in
RNNs. The initial hidden state needs to be set to some prior value to start the recursion (recall
Section 2). Little is understood regarding the best choice of initial hidden state other than Zim-
mermann et al. (2012)’s dynamical system argument. Consequently, it is typically simply set to
zero. Leveraging the MASO view of RNNs, we now demonstrate that one can improve significantly
over a zero initial hidden state by using a random initial hidden state. This choice regularizes the
affine slope parameter associated with the initial hidden state and mollifies the so-called exploding
gradient problem (Pascanu et al., 2013).
Random Initial Hidden State as an Explicit Regularization. We first state our theoretical re-
sult that using random initial hidden state corresponds to an explicit regularization and then dis-
cuss its impact on exploding gradients. Without loss of generality, we focus on one-layer ReLU
RNNs. Let N be the number of data points and C the number of classes. Define Ah := A1:T =
QI=T Aσs)Wr(') (recall (8)).
〜
Theorem 3. Let L be an RNN loss function, and let L represent the modified loss function when
the RNN initial hidden state is Set to a Gaussian random vector e 〜N(0, σ2I) with small Standard
deviation σ. Then we have that E
Le
R
diag
i=j
Ah
L + R. For the cross-entropy loss L with softmax output,
2
, where ybni is the ith dimension of the softmax output of
the nth data point and i, j ∈ {1, . . . , C} are the class indices. For the mean-squared error loss L,
R = 2n PL kAhk2.
We prove this result for the cross-entropy loss in Appendix G.2. The standard deviation σ controls
the importance of the regularization term and recovers the case of standard zero initial hidden state
when σ = 0.
Connection to the Exploding Gradient Problem. Backpropagation through time (BPTT) is the
default RNN training algorithm. Updating the recurrent weight Wr with its gradient using BPTT
involves calculating the gradient of the RNN output with respect to the hidden state at each time step
t = 0, . . . , T
YY 黑!=dzW(γY Aσs)Wr!.	(10)
s=T	s=T
When kA(σs)Wr k2 > 1, the product term Qts+=1T A(σs)Wr in (10) blows up, which leads to unstable
training. This is known as the exploding gradient problem (Pascanu et al., 2013).
dL _ dL dz
dh⑶=dZ dh(T)
6
Published as a conference paper at ICLR 2019
Figure 4: Visualization of the regularization effect of a random initial hidden state on the adding task (T =
100). (Top) Norm of Ah every 100 iterations; (Middle) norm of the gradient of the recurrent weight every 100
iterations; (Bottom) validation loss at every epoch. Each epoch contains 1000 iterations.
Our key realization is that the gradient of the RNN output with respect to the initial hidden state h(0)
features the term Ah from Theorem 3
黑=dLW (Yl Aσs)Wr! = dLWAh .	(11)
dh(0)	dz	σ	dz
Of all the terms in (10), this one involves the most matrix products and hence is the most erratic.
Fortunately, Theorem 3 instructs us that introducing randomness into the initial hidden state effects
a regularization on Ah and hence tamps down the gradient before it can explode. An interesting
direction for future work is extending this analysis to every term in (10).
Experiments. We now report on the results of a number of experiments that indicate the signifi-
cant performance gains that can be obtained using a random initial hidden state of properly chosen
standard deviation σ . Unless otherwise mentioned, in all experiments we use ReLU RNNs with
128-dimensional hidden states and with the recurrent weight matrix Wi(') initialized as an identity
matrix (Le et al., 2015; Talathi & Vartak, 2016). We summarize the experimental results; experi-
mental details and additional results are available in Appendices C and F.
Visualizing the Regularizing Effect of a Random Initial Hidden State. We first consider a simulated
task of adding 2 sequences of length 100. This is a ternary classification problem with input X ∈
R2×T and target y ∈ {0,1,2}, y = Pi lχ2i=1Xii. The first row of X contains randomly chosen
0’s and 1’s; the second row of X contains 1’s at 2 randomly chosen indices and 0’s everywhere
else. Prior work treats this task as a regression task (Arjovsky et al., 2016); our regression results
are provided in Appendix F.1.
In Figure 4, We visualize the norm of Ah the norm of the recurrent weight gradient k ddL ∣∣, and the
validation loss against training epochs for various random initial state standard deviations. The top
two plots clearly demonstrate the effect of the random initial hidden state in regularizing both Ah
and the norm of the recurrent weight gradient, since larger σ reduces the magnitudes of both Ah
and k ddWL ∣∣. Notably, the reduced magnitude of the gradient term ∣∣ dWL ∣∣ empirically demonstrates
the mollification of the exploding gradient problem. The bottom plot shows that setting σ too large
can negatively impact learning. This can be explained as having too much regularization effect. This
suggests the question of choosing the best value of σ in practice, which we now investigate.
Choosing the Standard Deviation of the Random Initial Hidden State. We examine the effect on
performance of different random initial state standard deviations σ in RNNs using RMSprop and
SGD with varying learning rates. We perform experiments on the MNIST dataset with each image
flattened to a length 784 sequence (recall Section 4). The full experimental results are included in
Appendix F.2; here, we report two interesting findings. First, for both optimizers, using a random
initial hidden state permits the use of higher learning rates that would lead to an exploding gradient
when training without a random initial hidden state. Second, RMSprop is less sensitive to the choice
of σ than SGD and achieves favorable accuracy even when σ is very large (e.g., σ = 5). This
might be due to the gradient smoothing that RMSprop performs during optimization. We therefore
recommend the use of RMSprop with a random initial hidden state to improve model performance.
7
Published as a conference paper at ICLR 2019
Table 1: Classification accuracies on the (permuted) MNIST and SST-2 test sets for various models. A random
initial hidden state elevates simple RNNs from also-rans to strong competitors of complex, state-of-the-art
models.
Model	Dataset		
	MNIST	permuted MNIST	SST-2
RNN, 1 layer, zero initial hidden state	0.970	0.891	0.871
RNN, 1 layer, random initial hidden state	0.981	0.922	0.873
	(σ = 0.1)	(σ = 0.01)	(σ=0.1)
RNN, 2 layers, zero initial hidden state	0.969	0.873	0.884
RNN, 2 layers, random initial hidden state	0.987	0.927	0.888
	(σ = 0.5)	(σ = 0.005)	(σ=0.005)
GRU	0.986	0.888	0.881
LSTM	0.978	0.913	0.849
uRNN (Arjovsky et al., 2016)	0.951	0.914	一
scoRNN (Helfrich et al., 2018)	0.985	0.966	一
C-LSTM (Zhou et al., 2015)	一	—	0.878
Tree-LSTM (Tai et al., 2015)	一	—	0.88
Bi-LSTM+SWN-Lex (Teng et al., 2016)	一	—	0.892
We used RMSprop to train ReLU RNNs of one and two layers with and without random initial
hidden state on the MNIST, permuted MNIST4 and SST-2 datasets. Table 1 shows the classification
accuracies of these models as well as a few state-of-the-art results using complicated models. It is
surprising that a random initial hidden state elevates the performance of a simple ReLU RNN to near
state-of-the-art performance.
Random Initial Hidden State in Complex RNN Models. Inspired by the results of the previous exper-
iment, we integrated a random initial hidden state into some more complex RNN models. We first
evaluate a one-layer gated recurrent unit (GRU) on the MNIST and permuted MNIST datasets, with
a random and zero initial hidden state. Although the performance gains are not quite as impressive
as those for ReLU RNNs, our results for GRUs still show worthwhile accuract improvements, from
0.986 to 0.987 for MNIST and from 0.888 to 0.904 for permuted MNIST.
We continue our experiments with a more complex, convolutional-recurrent model composed of 4
convolution layers followed by 2 GRU layers (Cakir et al., 2017) and the Bird Audio Detection Chal-
lenge dataset.5 This binary classification problem aims to detect whether or not an audio recording
contains bird songs; see Appendix C for the details. We use the area under the ROC curve (AUC)
as the evaluation metric, since the dataset is highly imbalanced. Simply switching from a zero to a
random initial hidden state provides a significant boost in the AUC: from 90.5% to 93.4%. These
encouraging preliminary results suggest that, while more theoretical and empirical investigations are
needed, a random initial hidden state can also boost the performance of complicated RNN models
that are not piecewise affine and convex.
7 Conclusions and Future Work
We have developed and explored a novel perspective of RNNs in terms of max-affine spline oper-
ators (MASOs). RNNs with piecewise affine and convex nonlinearities are piecewise affine spline
operators with a simple, elegant analytical form. The connections to input space partitioning (vec-
tor quantization) and matched filtering followed immediately. The spline viewpoint also suggested
that the typical zero initial hidden state be replaced with a random one that mollifies the exploding
gradient problem and improves generalization performance.
There remain abundant promising research directions. First, we can extend the MASO RNN frame-
work following (Balestriero & Baraniuk, 2018b) to cover more general networks like gated RNNs
(e.g, GRUs, LSTMs) that employ the sigmoid nonlinearity, which is neither piecewise affine nor
convex. Second, we can apply recent random matrix theory results (Martin & Mahoney, 2018) to
the affine parameter ARNN (e.g., the change of the distribution of its singular values during training)
to understand RNN training dynamics.
4We apply a fixed permutation to all MNIST images.
5The leaderboard of benchmarks can be found at https://goo.gl/TyaFrd.
8
Published as a conference paper at ICLR 2019
Acknowledgements
We thank the anonymous reviewers for their constructive feedback. This work was partially sup-
ported by NSF grants IIS-17-30574 and IIS-18-38177, AFOSR grant FA9550-18-1-0478, ARO
grant W911NF-15-1-0316, ONR grants N00014-17-1-2551 and N00014-18-12571, DARPA grant
G001534-7500, and DOD Vannevar Bush Faculty Fellowship (NSSEFF) grant N00014-18-1-2047.
References
M. Arjovsky, A. Shah, and Y. Bengio. Unitary evolution recurrent neural networks. In Proc. Int.
Conf. Mach. Learn. (ICML), volume 48,pp.1120-1128, Jun. 2016.
D.	Bahdanau, K. Cho, and Y. Bengio. Neural Machine Translation by Jointly Learning to Align and
Translate. ArXiv e-prints, 1409.0473, September 2014.
R. Balestriero and R. G. Baraniuk. Mad max: Affine spline insights into deep learning. ArXiv
e-prints, 1805.06576, May 2018a.
R. Balestriero and R. G. Baraniuk. From Hard to Soft: Understanding Deep Network Nonlinearities
via Vector Quantization and Statistical Inference. ArXiv e-prints, 1810.09274, Oct 2018b.
R. Balestriero and R. G. Baraniuk. A spline theory of deep networks. In Proc. Int. Conf. Mach.
Learn. (ICML), volume 80, pp. 374-383, Jul 2018.
E.	Cakir, S. Adavanne, G. Parascandolo, K. Drossos, and T. Virtanen. Convolutional recurrent neural
networks for bird audio detection. In Eur. Signal Process. Conf. (EUSIPCO), pp. 1744-1748, Aug
2017.
T. Cooijmans, N. Ballas, C. Laurent, C. Gulcehre, and A. C. Courville. Recurrent batch normaliza-
tion. In Proc. Int. Conf. Learn. Representations (ICLR), Apr. 2017.
A. Dieng, R. Ranganath, J. Altosaar, and D. Blei. Noisin: Unbiased regularization for recurrent
neural networks. In Proc. Int. Conf. Mach. Learn. (ICML), volume 80, pp. 1252-1261, Jul. 2018.
J. L. Elman. Finding structure in time. Cogn. Sci., 14:179-211, 1990.
X Glorot, Bordes A., and Y. Bengio. Deep sparse rectifier neural networks. In Proc. Int. Conf.
Artificial Intell. and Statist. (AISTATS), volume 15, pp. 315-323, Apr. 2011.
A.	Graves, A. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural networks.
In Proc. IEEE Int. Conf. Acoust., Speech and Signal Process. (ICASSP), pp. 6645-6649, May
2013.
B.	Hammer. On the approximation capability of recurrent neural networks. Neurocomputing, 31(1):
107-123, Mar. 2000.
L.	A. Hannah and D. B. Dunson. Multivariate convex regression with adaptive partitioning. J. Mach.
Learn. Res., 14:3261-3294, 2013.
K. Helfrich, D. Willmott, and Q. Ye. Orthogonal recurrent neural networks with scaled Cayley
transform. In Proc. Int. Conf. Mach. Learn. (ICML), volume 80, pp. 1969-1978, Jul. 2018.
M.	Henaff, A. Szlam, and Y. LeCun. Recurrent orthogonal networks and long-memory tasks. In
Proc. Int. Conf. Mach. Learn. (ICML), volume 48, pp. 2034-2042, Jun. 2016.
S. L. Hyland and G. Ratsch. Learning unitary operators with help from U (n). In Proc. AAAI conf.
Artificial Intell., pp. 2050-2058, Feb. 2017.
L. Jing, Y. Shen, T. Dubcek, J. Peurifoy, S. Skirlo, Y. LeCun, M. Tegmark, andM. SoljaciC. Tunable
efficient unitary neural networks (EUNN) and their application to RNNs. In Proc. Int. Conf.
Mach. Learn. (ICML), volume 70, pp. 1733-1741, Aug. 2017.
C. Jose, M. Cisse, and F. Fleuret. Kronecker recurrent units. In Proc. Int. Conf. Learn. Representa-
tions (ICLR), Apr. 2018.
9
Published as a conference paper at ICLR 2019
D Krueger, T. Maharaj, J. Kramar, M. Pezeshki, N. Ballas, N. R. Ke, A. Goyal, Y. Bengio,
H. Larochelle, A. C. Courville, and C. Pal. Zoneout: Regularizing rnns by randomly preserv-
ing hidden activations. In Proc. Int. Conf. Learn. Representations (ICLR), Apr. 2017.
Q. V. Le, N. Jaitly, and G. E. Hinton. A Simple Way to Initialize Recurrent Networks of Rectified
Linear Units. ArXiv e-prints, 1504.00941, Apr. 2015.
J. Li, X. Chen, E. Hovy, and D. Jurafsky. Visualizing and understanding neural models in NLP.
In Proc. Conf. North Amer. Chapter Assoc. Comput. Linguistics: Human Language Technol.
(NAACL HLT),pp. 681-691, Jun. 2016.
A. Magnani and S. P. Boyd. Convex piecewise-linear fitting. Optimization Eng., 10(1):1-17, Mar.
2009.
J. Mao, W. Xu, Y. Yang, J. Wang, Z. Huang, and A. Yuille. Deep captioning with multimodal
recurrent neural networks (m-rnn). In Proc. Int. Conf. Learn. Representations (ICLR), May 2015.
C. H. Martin and M. W. Mahoney. Implicit Self-Regularization in Deep Neural Networks: Evidence
from Random Matrix Theory and Implications for Learning. ArXiv e-prints, 1810.01075, Oct
2018.
Z. Mhammedi, A. Hellicar, A. Rahman, and J. Bailey. Efficient orthogonal parametrisation of re-
current neural networks using householder reflections. In Proc. Int. Conf. Mach. Learn. (ICML),
volume 70, pp. 2401-2409, Aug. 2017.
R. Pascanu, T. Mikolov, and Y. Bengio. On the difficulty of training recurrent neural networks. In
Proc. Int. Conf. Mach. Learn. (ICML), pp. 1310-1318, Jun. 2013.
J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word representation. In
Proc. Conf. Empirical Methods Natural Language Process. (EMNLP), pp. 1532-1543, Oct. 2014.
V. Pham, T. Bluche, C. Kermorvant, and J. Louradour. Dropout improves recurrent neural networks
for handwriting recognition. In Proc. Int. Conf. Frontiers Handwriting Recognition (ICFHR), pp.
285-290, Sept. 2014.
A. M. Schafer and H. G. Zimmermann. Recurrent neural networks are universal approximators. In
Proc. Int. Conf. Artificial Neural Netw. (ICANN), pp. 632-640, Sept. 2006.
H. T. Siegelmann and E. D. Sontag. On the computational power of neural nets. J. Comput. Syst.
Sci., 50(1):132-150, Feb. 1995.
R.	Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng, and C. Potts. Recursive deep mod-
els for semantic compositionality over a sentiment treebank. In Proc. Conf. Empirical Methods
Natural Language Process. (EMNLP), pp. 1631-1642, Oct. 2013.
D. Stowell and M. D. Plumbley. An open dataset for research on audio field recording archives:
Freefield1010. In Proc. Audio Eng. Soc. 53rd Conf. Semantic Audio (AES53), pp. 1-7, 2014.
Kai Sheng Tai, Richard Socher, and Christopher D. Manning. Improved semantic representations
from tree-structured long short-term memory networks. In Proc. Annu. Meeting Assoc. Comput.
Linguistics (ACL), pp. 1556-1566, Jul. 2015.
S.	S. Talathi and A. Vartak. Improving performance of recurrent network network with relu nonlin-
earity. In Proc. Int. Conf. Learn. Representations (ICLR), Apr. 2016.
Z. Teng, D. T. Vo, and Y. Zhang. Context-sensitive lexicon features for neural sentiment analysis.
In Proc. Conf. Empirical Methods Natural Language Process. (EMNLP), pp. 1629-1638, Nov.
2016.
L. van der Maaten and G. Hinton. Visualizing data using t-SNE. J. Mach. Learn. Res., 9(Nov):
2579-2605, 2008.
H. L. van Trees. Detection, Estimation, and Modulation Theory: Radar-Sonar Signal Processing
and Gaussian Signals in Noise. Krieger Publishing Co., Inc., 1992.
10
Published as a conference paper at ICLR 2019
H. L. van Trees. Detection, estimation, and modulation theory, part I. John Wiley & Sons, Inc.,
2013.
S. Wager, S. Wang, and P. Liang. Dropout training as adaptive regularization. In Proc. Advances
Neural Inform. Process. Syst (NIPS), volume 1,pp. 351-359, Dec. 2013.
S. Wisdom, T. Powers, J. R. Hershey, J. Le Roux, and L. Atlas. Full-capacity unitary recurrent
neural networks. In Proc. Advances Neural Inform. Process. Syst. (NIPS), Dec. 2016.
W. Zaremba, I. Sutskever, and O. Vinyals. Recurrent Neural Network Regularization. ArXiv e-prints,
1409.2329, September 2014.
C. Zhou, C. Sun, Z. Liu, and F. C. M. Lau. A C-LSTM Neural Network for Text Classification.
ArXiv e-prints, 1511.08630, November 2015.
H. Zimmermann, C. Tietz, and R. Grothmann. Forecasting with recurrent neural networks: 12 tricks.
Neural Netw.: Tricks of the Trade, pp. 687-707, 2012.
11
Published as a conference paper at ICLR 2019
A Notation
L, '	Total number of layers: L ≥ 0; Index of the layer in an RNN: ' ∈ {0, ∙∙∙ ,L}
T, t	Total number of time steps: T ≥ 0; Index of time steps of an RNN: t ∈ {0,…，T}
C, C	Total number of output classes: C > 0; Index of the output class: C ∈ {1,…，C}
N, n	total number of examples: N ≥ 1; Index of example in a dataset: n ∈ {1,…，N}
R⑶	Index of the partition region induced by the piecewise nonlinearity at layer l
D⑶	Dimension of input to the RNN at layer '
K, k	Total number of output dimensions of a MASO, K ≥ 0; Index OfMASO output dimension, k ∈ {1,…，K}
Q	The partition region selection matrix
X㈤	tth time step of a discrete time-serie, x(t) ∈ RD(O)
X	Concatenation of the whole length T time-serie: X = [x(1)>,…，X(T)> ] , X ∈ RD(O)T
X	A dataset of N time-series: X = {Xn}N
b(χ)	Output/prediction associated with input X
yn	True label (target variable) associated with the nth time-serie example Xn. For classification ynj ∈{1,...,C}, C > 1; For regression ynj ∈ RC, C ≥ 1
h(e,t)	Output of an RNN cell at layer ' and time step t; Alternatively, input to an rNn cell at layer ' + 1 and time step t - 1
h(')	Concatenation of hidden state	h(',t) of all time steps at layer ':	h('	=	[h%1)>,…，x(',t)>]	,	h('	∈	RD(')T
z(',t)	Concatenated input to an RNN cell at layer ' and time step t: ZSt) = [h('τ,t)>, h(',t-1)>i , ZSt) ∈ R2D(')
WF)	'th layer RNN weight associated with the input h(',t-1) from the previous time step: W(') ∈ RD(')×D(')
W (')	'th layer RNN weight associated with the input h('-1,t) from the previous layer: W(' ∈ RD(')×D(' 1)
W	Weight of the last fully connected layer: W ∈ RC × D(L)
b(')	'th layer RNN bias: b(') ∈ RD(')
b	Bias of the last fully connected layer: b ∈ RC
σ(∙)	Pointwise nonlinearity in an RNN (assumed to be piecewise affine and convex in this paper)
σe	Standard deviation of noise injected into the initial hidden state h(',0) ∀'
Art)	MASO formula of the RNN activation σ(∙) atlayer ' and time step t: Aσ ∈ RD' × D(')
A(',t), BWt)	MASO parameters of an RNN at layer ' and time step t: A(',t) ∈ RD(')×R(')×D('-1) , B(',t) ∈ RD(')×R(')
B	Datasets and Preprocessing Steps
Below we describe the datasets and explain the preprocessing steps for each dataset.
MNIST. The dataset6 consists of 60k images in the training set and 10k images in the test set. We
randomly select 10k images from the training set as validation set. We flatten each image to a 1-
6http://yann.lecun.com/exdb/mnist/
12
Published as a conference paper at ICLR 2019
dimensional vector of size 784. Each image is also centered and normalized with mean of 0.1307
and standard deviation of 0.3081 (PyTorch default values).
permuted MNIST. We apply a fixed permutation to all images in the MNIST dataset to obtain the
permuted MNIST dataset.
SST-2. The dataset7 consists of 6920, 872, 1821 sentences in the training, validation and test set,
respectively. Total number of vocabulary is 17539, and average sentence length is 19.67. Each
sentence is minimally processed into sequences of words and use a fixed-dimensional and trainable
vector to represent each word. We initialize these vectors either randomly or using GloVe (Penning-
ton et al., 2014). Due to the small size of the dataset, the phrases in each sentence that have semantic
labels are also used as part of the training set in addition to the whole sentence during training.
Dropout of 0.5 is applied to all experiments. Phrases are not used during validation and testing, i.e.,
we always use entire sentences during validation and testing.
Bird Audio Dataset. The dataset8 consists of 7, 000 field recording signals of 10 seconds sampled
at 44 kHz from the Freesound Stowell & Plumbley (2014) audio archive representing slightly less
than 20 hours of audio signals. The audio waveforms are extracted from diverse scenes such as
city, nature, train, voice, water, etc., some of which include bird sounds. The labels regarding the
bird detection task can be found in the file freefield1010. Performance is measured by Area
Under Curve (AUC) due to the unbalanced distribution of the classes. We preprocess every audio
clip by first using short-time Fourier transform (STFT) with 40ms and 50% overlapping Hamming
window to obtain audio spectrum and then by extracting 40 log mel-band energy features. After
preprocessing, each input is of dimension D = 96 and T = 999.
C Experimental Setup
Experiment setup for various datasets is summarized in Table 2. Some of the experiments do not
appear in the main text but in the appendix; we include setup for those experiments as well. A
setting common to all experiments is that we use learning rate scheduler so that when validation loss
plateaus for 5 consecutive epochs, we reduce the current learning rate by a factor of 0.7.
Setup of the experiments on influence of various standard deviations in random initial hidden
state under different settings. We use σ chosen in {0.001, 0.01, 0.1, 1, 5} and learning rates in
{1×10-5, 1×10-4, 1.5×10-4,2×10-4}forRMSpropand{1×10-7, 1×10-6, 1.25×10-6, 1.5×
10-6} plain SGD.
Setup of input space partitioning experiments. For the results in the main text, we use t-SNE
visualization (van der Maaten & Hinton, 2008) with 2 dimensions and the default settings from the
python sklearn package. Visualization is performed on the whole 10k test set images. For finding
the nearest neighbors of examples in the SST-2 dataset, since the examples are of varying lengths, we
constrain the distance comparison to within +/-10 words of the target sentence. When the sentence
lengths are not the same, we simply pad the shorter ones to the longest one, then process it with
RNN and finally calculate the distance as the `2 distance of the partition codes (i.e., concatenation
of all hidden states) that RNN computes. We justify the comparison between examples of different
lengths using padding by noting that batching examples and padding the examples to the longest
example within a batch has been a common practice in modern natural language processing tasks.
Setup of exploratory experiments. We experimented with one-layer GRU with 128 hidden unites
for MNIST and permuted MNIST datasets. We use RMSprop optimizer with an initial learning rate
of 0.001. We experimented with various standard deviations in random initial hidden state including
{0.01, 0.05, 0.1, 0.5, 1, 5}. The optimal standard deviations that produce the results in the main text
are σ = {0.01, 0.05, 0.01}, for MNIST, permuted MNIST and bird detection datasets, respectively.
D Additional Input Space Partition Visualizations
We provide ample additional visualizations to demonstrate the partition codes that an RNN computes
on its input sequences. Here, the results are focused more on the properties of the final partition
7
https://nlp.stanford.edu/sentiment/index.html
8 http://machine- listening.eecs.qmul.ac.uk/bird- audio- detection- challenge/
13
Published as a conference paper at ICLR 2019
Table 2: Various experiment setup. Curly brackets indicate that we attempted more than one value
for this experiment.
Settings	Dataset				
	Add task	MNIST	Permuted MNIST	SST-2	Bird detection
Type of RNN	ReLU RNN	ReLU RNN	ReLU RNN	ReLU RNN	GRU
#Layers	{1, 2}	{1, 2}	{1, 2}	{1, 2}	2
Input size	2	1	1	128	96
Hidden size	128	128	128	300	256
Output size	3	3	10	2	2
Initial	1e-4	1e-4	1e-4	1e-4	1e-4
Learning Rate Optimizer	RMSprop	{RMSprop, SGD}	RMSprop	Adam	Adam
Batch size	50	64	64	64	64
Epochs	100	200	200	100	50
bnG.jɔ
Jn-G，.3 6
2「J> ,b 2
πr75 二：
3 r- G 11
O	0	OOU	0	七厂7		7	弓	5 S^	ς	ς	ς
I	i	/ / /	(	二”	y	G	4	Q 6	(3		g
3	2	3 3 2	q	Irr ɔ		tI	r	1 7	7	ι	1
3	2›	3 ʒ ʒ	3	.r r	a	-	8	B 8	3	g	g
7	÷	H ⅛t V	H	:M	1	ς	β∖	斗4	et	(I	吁
Figure 5: Visualization of partition codes for pixel-by-pixel (i.e., flattened to a 1-dimensional, length
784 vector) MNIST dataset using a trained ReLU RNN (one layer, 128-dimensional hidden state).
Here, we visualize the nearest 5 and farthest 5 images of one selected image from each class. The
distance is computed using the partition codes of the images. Leftmost column is the original image;
the middle 5 images are the 5 nearest neighbors; the rightmost 5 images are the farthest neighbors.
codes computed after the RNN processes the entire input sequence rather than part of the input
sequence. Several additional sets of experimental results are shown; the first three on MNIST and
the last one on SST-2.
First, we visualize the nearest and farthest neighbors of several MNIST digits in Figure 5. Distance
is calculated using the partition codes of the images. The left column is the original image; The
next five columns are the five nearest neighbors to the original image; The last five columns are five
farthest neighbors. This figure shows that partition codes the images are well clustered.
Second, we show the two dimensional projection using t-SNE of the raw pixel and VQ represen-
tations of each data points in the MNIST dataset and visualize them in Figure 6. We clearly see a
more distinct clustering using VQ representation of the data than using the raw pixel representation.
This comparison demonstrate the ability of the RNN to extract useful information from the raw
representation of the data in the form of VQ.
Third, we perform a KNN classification with k ∈ {1, 2, 5, 10} using 1) the RNN computed partition
codes of the inputs and 2) raw pixel data representation the MNIST test set to illustrate that the data
reparametrized by the RNN has better clustering property than the original data representations. We
use 80% of the test set to train the classifier and the rest for testing. The results are reported in
Table 3. We see that the classification accuracies when using RNN computed partition codes of
the inputs are significantly higher than those when using raw pixel representations. This result again
shows the superior quality of the input space partitioning that RNN produces, and may suggest a new
way to improve classification accuracy by just using the reparametrized data with a KNN classifier.
Finally, we visualize the 5 nearest and 5 farthest neighbors of a selected sentence from the SST-
2 dataset to demonstrate that the partitioning effect on dataset of another modality. Again, the
distances are computed using the partition codes of the inputs. The results are shown in Figure 7.
14
Published as a conference paper at ICLR 2019
Figure 6: t-SNE visualization of MNIST test set images using raw pixel representation (left) and
RNN VQ representation (right). We see more distinct clusters in the t-SNE plot using RNN VQ
representation of images than the raw pixel representation, implying the useful information that
RNN extracts in the form of VQ.
Table 3: K-nearest neighbor classification accuracies using data reparametrized by RNN compared
to those using raw pixel data. We can see that classification accuracies using RNN reparametrized
data are much higher than those using raw pixel data for all k’s.
k	MNIST, raw pixels	MNIST, VQ
1	0.950	0.977
2	0.936	0.974
5	0.951	0.977
10	0.939	0.975
We can see that all sentences that are nearest neighbors are of similar sentiment to the target sentence,
whereas all sentences that are farthest neighbors are of the opposite sentiment.
E	Additional Template Visualizations
We provide here more templates on images and texts in Figures 8 and 9. Notice here that, although
visually the templates may look similar or meaningless, they nevertheless have meaningful inner
product with the input. The class index of the template that produces the largest inner product with
the input is typically the correct class, as can be seen in the two figures.
F	Additional Experimental Results for Random Initial Hidden
State
F.1 Regularization Effect for Regression Problem
We present the regularization effect on adding task formulated as a regression problem, following
setup in Arjovsky et al. (2016). Result is shown in Figure 10. We see regularization effect similar
to that presented in Figure 4, which demonstrates that the regularization effect does indeed happens
for both classification and regression problems, as Thm. 3 suggests.
F.2 Choosing Standard Deviation in Random Initial Hidden State
Table 4 shows the classification accuracies under various settings. The discussion of the results is in
Section 6.
15
Published as a conference paper at ICLR 2019
Original text
It is a film that will have people walking out halfway through , will encourage others to stand up and applaud
, and will , undoubtedly , leave both camps engaged in a ferocious debate for years to come . (+)
Nearest 5 neighbors	Farthest 5 neighbors
Marries the amateurishness of The Blair Witch
Well-written , nicely acted and beautifully shot and
scored , the film works on several levels , openly
questioning social mores while ensnaring the audi-
ence with its emotional pull . (+, 22.00)
A stunning piece of visual poetry that will , hope-
fully , be remembered as one of the most important
stories to be told in Australia ’s film history . (+,
22.23)
Cute , funny , heartwarming digitally animated fea-
ture film with plenty of slapstick humor for the kids
, lots of in-jokes for the adults and heart enough for
everyone . (+, 22.61)
Though it is by no means his best work , Laissez-
Passer is a distinguished and distinctive effort by a
bona-fide master , a fascinating film replete with re-
wards to be had by all willing to make the effort to
reap them . (+, 22.78)
An absorbing trip into the minds and motivations of
people under stress as well as a keen , unsentimental
look at variations on the theme of motherhood . (+,
22.89)
Project with the illogic of Series 7 : The Contenders
to create a completely crass and forgettable movie .
(-, 37.60)
K-19 may not hold a lot of water as a submarine epic
, but it holds even less when it turns into an elegia-
cally soggy Saving Private Ryanovich . (-, 37.42)
This is a great subject for a movie , but Hollywood
has squandered the opportunity , using it as a prop
for warmed-over melodrama and the kind of chore-
ographed mayhem that director John Woo has built
his career on . (-, 37.24)
Flotsam in the sea of moviemaking , not big enough
for us to worry about it causing significant harm and
not smelly enough to bother despising . (-, 37.15)
If you ’re not a prepubescent girl , you ’ll be laugh-
ing at Britney Spears ’ movie-starring debut when-
ever it does n’t have you impatiently squinting at
your watch . (-, 36.98)
Input image
Figure 7: Nearest and furthest neighbors of a postive movie review. The sentiment (+ or -) and the
euclidean distance between the input and the neighbor vector quantizations are shown in parenthesis
after each neighbor.
0, -31.9	1, 2.9	2, -21.9	3, -26.1	4, -24.9	5,, -23.1	6, -24.3	7, -18.5	8, -24.7	9, -26.1
Input image
0,-6.2	1,-3.9	2,5.0	3,-5.8	4,-13.5	5,,-10.6	6,-9.0	7,-9.3	8,-14.0	9,-14.9
Input image
0, -5.5	1, -9.0	2, -2.4	3, -3.7	4, -8.8	5,, -6.6	6, -5.7	7, -8.0	8, 1.8	9, -5.9
Figure 8: Templates of three selected MNIST images. The leftmost column are the original input
image. The next ten images of each row are the ten templates of a particular input image corre-
sponding to each class. For each template image, we show the class and the inner product of this
template with the input. Text under the template of the true class of each input image is bolded.
G Proofs
G.1 Proof of Thm. 1
To simplify notation, similar to the main text, in the proof here we drop the affine parameters
dependencies on the input, but keep in mind the input-dependency of these parameters.
16
Published as a conference paper at ICLR 2019
Figure 9: Additional template visualizations of an example from the SST-2 dataset. Each word in
the sentence is marked as a tick label in the x axis. The values of inner products are marked below
each template. The template that has the bigger inner product is the true class of the sentence. We
see that the template corresponding to the correct class produces a significantly bigger inner product
with the input than other templates.
O	20	40	SO	90	IOO
Epochs
Figure 10: Various plots during training of add problem (T=100, regression). Top: norm of Ah at
every 100 iterations; Middle: norm of gradient of recurrent weight at every 100 iterations; Bottom:
validation loss at every epoch. Each epoch contains 1000 iterations.
We first derive the expression for a hidden state h(`,t) at a given time step t and layer `. Using
Prop. 1, we start with unrolling the RNN cell of layer ` at time step t for two time steps to t - 2 by
recursively applying (1) as follows:
17
Published as a conference paper at ICLR 2019
Table 4: Classification accuracy for MNIST dataset under 2 different optimizers, various learning
rates and different standard deviation σ in the random initial hidden state. Results suggest RMSprop
tolerates various choices of σ while SGD works for smaller σ .
σ	RMSprop				SGD			
	1e-5	1e-4	1.5e-4	2e-4	1e-7	1e-6	1.25e-6	1.5e-6
0	0.960	0.973	0.114	0.114	0.837	0.879	0.870	0.098
0.001	0.963	0.974	0.978	0.970	0.835	0.895	0.913	0.875
0.01	0.962	0.980	0.978	0.976	0.834	0.898	0.922	0.918
0.1	0.955	0.976	0.981	0.976	0.803	0.833	0.913	0.908
1	0.956	0.977	0.980	0.976	0.520	0.640	0.901	0.098
5	0.952	0.981	0.973	0.981	0.471	0.098	0.098	0.098
h(',t) = σ (W(')h('-1,t) + b(') + wr(')h(',t-1)J	(12)
=A('，t)W (')h('-1,t) + A(rt)b(') + Aσt)Wr(')h(',t-1)	(13)
=AWt)W (')h('-1,t) + A(t)b(')
+ A(',t)Wr(')T) W⑶h('-1，t-1) + Ay,t-1)b⑶+ A/tT)Wy)h('，t-2))	(14)
=(A('，t)W (')h('-1,t) + A(',t) W (') A(',t-1)W (E)h('-1，t-I))
+ 0卢)+A(',t)Wr(')A(',t-1)) b⑶+A(',t)Wr(')A('，t-i)Wr(')h('，t-2).	(15)
From (12) to (13), we use the result of Prop. 1. From (13) to (14), we expand the term that involves
the hidden state h(',t-1) at the previous time step recursively using (13). From (14) to (15), We
group terms and write them compactly.
Now define Asfts) := Qk=t Aak) W(') for s < t and AFtt) ：= I where I is the identity matrix.
Using this definition, We reWrite (15) and proceed With the unrolling to the initial time step as
follows:
h(`,t)
(`))Aσf,t)W⑶(h(f-1,t) v χ a(`)b(`,s)
AtT 八 Aσ',tτ)W (')	h('-1,t-1))+ ZAs：tB
(16)
/Aσf,t)W⑶、∕h('-1,t)
∖a1,I)W(f)) ∖h('-1,I)
t-1
+ X As')B(',s) + A0'th(',0),	(17)
s=t
where B(',s) = /'，§)"')as defined in Prop. 1.
18
Published as a conference paper at ICLR 2019
Repeat the above derivation for t ∈ {1, ∙∙∙ ,T} and stack h(',t) in decreasing time steps from top
to bottom, we have:
∕h(e,τ)\	(AT)T ... A1T ∖ ∕A∕τ) W (') ...	0	∖ ∕h('-ι,T)
.)=」.).....I .
∖h(',1)√	∖ 0 ... A^J ∖	0	... A(',1)W ⑶)IhdI)
(P A('TB('")+A0'Th(',0)∖
+ t=r	.	= ARNNh('-1)+BRNN,	(18)
.
.
∖ Al') B(',t) +A0') h(',0)	/
where
A
(`)
RNN
(ATr
0
(P A('TB(',t+ A0'Th(',0)∖	∕h('τ,r)
B(') = t=r	h('-1) =	.
BRNN =	.	，h =	.
.
∖ A1') B(',t+ A0'1h(',o) )	'h('T1)
which concludes the proof.
Thm. 2 follows from the recursive application of the above arguments for each layer ' ∈{1,…，L}.
G.2 Proof of Thm. 3
We prove for the case of multi-class classification problem with softmax output. The proof for the
case of regression problems easily follows.
Let ai be the ith row of the input-dependent affine parameter Ah where Ah := A1:T =
Q1=r AF,s)W(') (recall Section 6), Xn = [x'1) ,…，Xnr) ]> be the concatenation of the nth
input sequence of length T and c be the index of the correct class. We assume the amplitude of
random initial hidden state is small so that the input-dependent affine parameter Ah, which also
depends on h(0), does not change when using random h(0). Also, let zn = fRNN(Xn, h(0)) be the
overall RNN computation that represents (9).
We first rewrite the cross entropy loss with random initial hidden state LCE
LCE softmax fRNN Xn , h(0) + as follows:
1N
LCE = N X -log 卜oftmaX f rnn (Xn1:T), h(0) + €)))
1N
N X -log
n=1
1N
N X
n=1
exp(znc + a〈0
PjC=1 exp(znj + aj)
-znc - ac + log
exp(znj + aj )
(19)
Taking expectation of the L with respect to the distribution of the random Gaussian vector that the
initial hidden state is set to, we have
E[LCE] = LCE + R,
(20)
where
log
exp(znj + aj)
- log
exp(znj)
(21)
19
Published as a conference paper at ICLR 2019
We note that similar forms of (21) have been previously derived by Wager et al. (2013).
We now simplify (21) using second order Taylor expansion on h(0) of the summation inside the log
function. Define function u(x(n1:T), h(0)) := log(Pj exp(znj)) = log(Pj exp(f (x(n1:T), h(0)))).
Then, we can approximate (21) as follows:
R ≈ ： XXX (e "u(xn, h(0)) + d"Xh(h⑼)e + 1 e>d%^ J - "'" h⑼))
-L V	] I	Vj.f l>v /	Zz	Vj. \ /	I
(22)
where du⅛h(0) ) is the Hessian matrix:
du('n, h(0))2	du('n, h(O))2
_	d2h ⑼	L= dh(0)dh(0)
_ d	exp (Zni)
dh(0) PC=1 eχp (Znj) Q'
、-------V--------}
ybni
dbni dZnl
dznl h(0) ai
bni(li=ι - bni) hai, aιi .
Then, we can write the trace term in (22) as follows:
2
As a result, using the above approximations, we can rewrite the loss with random initial state in (19)
as:
2
LCE
σ2 N
LCE + 2N X Ildiag
n=1
(23)
We see that this regularizer term does not dependent on the correct class index c of each data points.
H	Prior Work on the Exploding Gradient in RNNs
The problem of exploding gradients has been widely studied from different perspectives. First ap-
proaches have attempted to directly control the amplitude of the gradient through gradient clip-
ping (Pascanu et al., 2013). A more model driven approach has leveraged the analytical formula
of the gradient when using specific nonlinearities and topologies in order to develop parametriza-
tion of the recurrent weights. This has led to various unitary reparametrizations of the recurrent
weight (Arjovsky et al., 2016; Wisdom et al., 2016; Helfrich et al., 2018; Henaff et al., 2016; Jing
et al., 2017; Mhammedi et al., 2017; Hyland & Ratsch, 2017; Jose et al., 2018). A soft version of
such parametrization lies in regularization of the DNNs. This includes dropout applied to either the
output layer (Pham et al., 2014) or hidden state (Zaremba et al., 2014), noisin (Dieng et al., 2018),
zoneout (Krueger et al., 2017) and recurrent batch normalization (Cooijmans et al., 2017). Lastly,
identity initialization of ReLU RNNs has been studied in Le et al. (2015) and Talathi & Vartak
(2016). Our results complements prior works in that simply using random initial hidden state in-
stead of zero initial hidden state and without changing the RNN structure also relieves the exploding
gradient problem by regularization the potentially largest term in the recurrent weight gradient.
20