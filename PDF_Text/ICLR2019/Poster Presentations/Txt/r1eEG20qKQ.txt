Published as a conference paper at ICLR 2019
Self-Tuning Networks:
Bilevel Optimization of Hyperparameters us-
ing Structured Best-Response Functions
Matthew MacKay∖ Paul Vicol*, Jon Lorraine, David Duvenaud, Roger Grosse
{mmackay,pvicol,lorraine,duvenaud,rgrosse}@cs.toronto.edu
University of Toronto
Vector Institute
Ab stract
Hyperparameter optimization can be formulated as a bilevel optimization prob-
lem, where the optimal parameters on the training set depend on the hyperpa-
rameters. We aim to adapt regularization hyperparameters for neural networks
by fitting compact approximations to the best-response function, which maps hy-
perparameters to optimal weights and biases. We show how to construct scalable
best-response approximations for neural networks by modeling the best-response
as a single network whose hidden units are gated conditionally on the regular-
izer. We justify this approximation by showing the exact best-response for a shal-
low linear network with L2-regularized Jacobian can be represented by a similar
gating mechanism. We fit this model using a gradient-based hyperparameter op-
timization algorithm which alternates between approximating the best-response
around the current hyperparameters and optimizing the hyperparameters using the
approximate best-response function. Unlike other gradient-based approaches, we
do not require differentiating the training loss with respect to the hyperparameters,
allowing us to tune discrete hyperparameters, data augmentation hyperparameters,
and dropout probabilities. Because the hyperparameters are adapted online, our
approach discovers hyperparameter schedules that can outperform fixed hyperpa-
rameter values. Empirically, our approach outperforms competing hyperparam-
eter optimization methods on large-scale deep learning problems. We call our
networks, which update their own hyperparameters online during training, Self-
Tuning Networks (STNs).
1 Introduction
Regularization hyperparameters such as weight decay, data augmentation, and dropout (Srivastava
et al., 2014) are crucial to the generalization of neural networks, but are difficult to tune. Pop-
ular approaches to hyperparameter optimization include grid search, random search (Bergstra &
Bengio, 2012), and Bayesian optimization (Snoek et al., 2012). These approaches work well with
low-dimensional hyperparameter spaces and ample computational resources; however, they pose
hyperparameter optimization as a black-box optimization problem, ignoring structure which can be
exploited for faster convergence, and require many training runs.
We can formulate hyperparameter optimization as a bilevel optimization problem. Let w denote
parameters (e.g. weights and biases) and λ denote hyperparameters (e.g. dropout probability). Let
LT and LV be functions mapping parameters and hyperparameters to training and validation losses,
respectively. We aim to solve* 1:
λ* = arg min LV (λ, w*) subject to w* = arg min LT (λ, w)	(1)
λw
* Equal contribution.
1The uniqueness of the arg min is assumed.
1
Published as a conference paper at ICLR 2019
Substituting the best-responSefunction w* (λ) = arg mi□w LT(λ, W) gives a single-level problem:
λ* = arg min LV (λ, w*(λ))	(2)
λ
If the best-response w* is known, the validation loss can be minimized directly by gradient descent
using Equation 2, offering dramatic speed-ups over black-box methods. However, as the solution to
a high-dimensional optimization problem, it is difficult to compute w* even approximately.
Following Lorraine & Duvenaud (2018), we propose to approximate the best-response w* directly
with a parametric function Wφ. We jointly optimize φ and λ, first updating φ so that Wφ ≈ w* in a
neighborhood around the current hyperparameters, then updating λ by using Wφ as a proxy for w*
in Eq. 2:
λ* ≈ arg min LV (λ, Wφ(λ))	(3)
λ
Finding a scalable approximation Wφ when W represents the weights of a neural network is a sig-
nificant challenge, as even simple implementations entail significant memory overhead. We show
how to construct a compact approximation by modelling the best-response of each row in a layer’s
weight matrix/bias as a rank-one affine transformation of the hyperparameters. We show that this
can be interpreted as computing the activations of a base network in the usual fashion, plus a cor-
rection term dependent on the hyperparameters. We justify this approximation by showing the exact
best-response for a shallow linear network with L2-regularized Jacobian follows a similar structure.
We call our proposed networks Self-Tuning Networks (STNs) since they update their own hyperpa-
rameters online during training.
STNs enjoy many advantages over other hyperparameter optimization methods. First, they are easy
to implement by replacing existing modules in deep learning libraries with “hyper” counterparts
which accept an additional vector of hyperparameters as input2. Second, because the hyperparam-
eters are adapted online, we ensure that computational effort expended to fit φ around previous
hyperparameters is not wasted. In addition, this online adaption yields hyperparameter schedules
which we find empirically to outperform fixed hyperparameter settings. Finally, the STN train-
ing algorithm does not require differentiating the training loss with respect to the hyperparameters,
unlike other gradient-based approaches (Maclaurin et al., 2015; Larsen et al., 1996), allowing us
to tune discrete hyperparameters, such as the number of holes to cut out of an image (DeVries &
Taylor, 2017), data-augmentation hyperparameters, and discrete-noise dropout parameters. Empir-
ically, we evaluate the performance of STNs on large-scale deep-learning problems with the Penn
Treebank (Marcus et al., 1993) and CIFAR-10 datasets (Krizhevsky & Hinton, 2009), and find that
they substantially outperform baseline methods.
2	B ilevel Optimization
A bilevel optimization problem consists of two sub-problems called the upper-level and lower-level
problems, where the upper-level problem must be solved subject to optimality of the lower-level
problem. Minimax problems are an example of bilevel programs where the upper-level objective
equals the negative lower-level objective. Bilevel programs were first studied in economics to model
leader/follower firm dynamics (Von Stackelberg, 2010) and have since found uses in various fields
(see Colson et al. (2007) for an overview). In machine learning, many problems can be formulated as
bilevel programs, including hyperparameter optimization, GAN training (Goodfellow et al., 2014),
meta-learning, and neural architecture search (Zoph & Le, 2016).
Even if all objectives and constraints are linear, bilevel problems are strongly NP-hard (Hansen
et al., 1992; Vicente et al., 1994). Due to the difficulty of obtaining exact solutions, most work
has focused on restricted settings, considering linear, quadratic, and convex functions. In contrast,
we focus on obtaining local solutions in the nonconvex, differentiable, and unconstrained setting.
Let F, f : Rn × Rm → R denote the upper- and lower-level objectives (e.g., LV and LT) and
λ ∈ Rn , W ∈ Rm denote the upper- and lower-level parameters. We aim to solve:
min F (λ, W)	(4a)
λ∈Rn
subject to W ∈ arg min f (λ, W)	(4b)
W∈Rm
2We illustrate how this is done for the PyTorch library (Paszke et al., 2017) in Appendix G.
2
Published as a conference paper at ICLR 2019
It is desirable to design a gradient-based algorithm for solving Problem 4, since using gradient
information provides drastic speed-ups over black-box optimization methods (Nesterov, 2013). The
simplest method is simultaneous gradient descent, which updates λ using dF∕∂λ and W using df∕∂w.
However, simultaneous gradient descent often gives incorrect solutions as it fails to account for the
dependence of W on λ. Consider the relatively common situation where F doesn’t depend directly
on λ , so that dF∕∂λ ≡ 0 and hence λ is never updated.
2.1	Gradient Descent via the Best-Response Function
A more principled approach to solving Problem 4 is to use the best-response function (Gibbons,
1992). Assume the lower-level Problem 4b has a unique optimum w*(λ) for each λ. Substituting
the best-response function w* converts Problem 4 into a single-level problem:
min F*(λ) := F(λ, w*(λ))	(5)
If w* is differentiable, we can minimize Eq. 5 using gradient descent on F* with respect to λ. This
method requires a unique optimum w* (λ) for Problem 4b for each λ and differentiability of w* . In
general, these conditions are difficult to verify. We give sufficient conditions for them to hold in a
neighborhood of a point (λ0 , w0 ) where w0 solves Problem 4b given λ0.
Lemma 1. (Fiacco & Ishizuka, 1990) Let w0 solve Problem 4b for λ0. Suppose f is C2 in a neigh-
borhood of (λo, wo) and the Hessian d2f/∂W2(λo, w0) is positive definite. Thenfor some neighbor-
hood U of λ0, there exists a continuously differentiable function w* : U → Rm such that w* (λ) is
the unique solution to Problem 4b for each λ ∈ U and w* (λ0) = w0.
Proof. See Appendix B.1.
□
The gradient of F * decomposes into two terms, which we term the direct gradient and the response
gradient. The direct gradient captures the direct reliance of the upper-level objective on λ, while
the response gradient captures how the lower-level parameter responds to changes in the upper-level
parameter:
dF- (λo) = ∂F (λ0, w*(λo)) + ∂F (λ0, w*(λo)) ∂W* (λo)
∂ λ	∂ λ	∂ w	∂ λ
(6)
X---------{---------} '---------------{---------------}
Direct gradient	Response gradient
Even if dF∕∂λ ≡ 0 and simultaneous gradient descent is possible, including the response gradient
can stabilize optimization by converting the bilevel problem into a single-level one, as noted by Metz
et al. (2016) for GAN optimization. Conversion to a single-level problem ensures that the gradient
vector field is conservative, avoiding pathological issues described by Mescheder et al. (2017).
2.2	Approximating the Best-Response Function
In general, the solution to Problem 4b is a set, but assuming uniqueness of a solution and differ-
entiability of w- can yield fruitful algorithms in practice. In fact, gradient-based hyperparameter
optimization methods can often be interpreted as approximating either the best-response w- or its
Jacobian dw*∕∂λ, as detailed in Section 5. However, these approaches can be computationally expen-
sive and often struggle with discrete hyperparameters and stochastic hyperparameters like dropout
probabilities, since they require differentiating the training loss with respect to the hyperparameters.
Promising approaches to approximate w- directly were proposed by Lorraine & Duvenaud (2018),
and are detailed below.
1.	Global Approximation. The first algorithm proposed by Lorraine & Duvenaud (2018) approx-
imates w- as a differentiable function Wφ with parameters φ. If W represents neural net weights,
then the mapping Wφ is a hypernetwork (Schmidhuber, 1992; Ha et al., 2016). If the distribution
p(λ) is fixed, then gradient descent with respect to φ minimizes:
Eλ〜p(λ) [f(λ, Wφ(λ))]	⑺
If support(p) is broad and Wφ is sufficiently flexible, then Wφ can be used as a proxy for w- in
Problem 5, resulting in the following objective:
miɪn F (λ, W φ(A))	⑻
3
Published as a conference paper at ICLR 2019
2.	Local Approximation. In practice,俞φ is usually insufficiently flexible to model w* on
SUPPort(p). The second algorithm of Lorraine & DUvenaUd (2018) locally approximates w* in a
neighborhood around the current upper-level parameter λ. They setp(e∣σ) to a factorized Gaussian
noise distribution with a fixed scale parameter σ ∈ Rn+, and found φ by minimizing the objective:
嗅〜p(e∣σ) [f (λ + W, Wφ(λ + C))]	⑼
Intuitively, the upper-level parameter λ is perturbed by a small amount, so the lower-level parameter
learns how to respond. An alternating gradient descent scheme is used, where φ is updated to
minimize equation 9 and λ is updated to minimize equation 8. This approach worked for problems
using L2 regularization on MNIST (LeCun et al., 1998). However, itis unclear if the approach works
with different regularizers or scales to larger problems. It requires Wφ, which is a priori unwieldy
for high dimensional w. It is also unclear how to set σ, which defines the size of the neighborhood
on which φ is trained, or if the approach can be adapted to discrete and stochastic hyperparameters.
3	Self-Tuning Networks
In this section, We first construct a best-response approximation Wφ that is memory efficient and
scales to large neural networks. We justify this approximation through analysis of simpler situations.
Then, we describe a method to automatically adjust the scale of the neighborhood φ is trained on.
Finally, we formally describe our algorithm and discuss how it easily handles discrete and stochastic
hyperparameters. We call the resulting networks, which update their own hyperparameters online
during training, Self-Tuning Networks (STNs).
3.1	An Efficient Best-Response Approximation for Neural Networks
We propose to approximate the best-response for a given layer’s weight matrix W ∈ RDout ×Din
and bias b ∈ RDout as an affine transformation of the hyperparameters λ3:
Wφ(λ) =	Welem	+	(V λ)	row	Whyper,	bφ(λ) = belem +	(Cλ)	bhyper	(10)
Here, indicates elementwise multiplication and row indicates row-wise rescaling. This archi-
tecture computes the usual elementary weight/bias, plus an additional weight/bias which has been
scaled by a linear transformation of the hyperparameters. Alternatively, it can be interpreted as di-
rectly operating on the pre-activations of the layer, adding a correction to the usual pre-activation to
account for the hyperparameters:
Wφ(λ)x + bφ(λ) = [Welemx + belem] + [(V λ)	(Whyperx) + (Cλ) bhyper]	(11)
This best-response architecture is tractable to compute and memory-efficient: it requires
Dout (2Din + n) parameters to represent Wφ and Dout (2 + n) parameters to represent bφ, where
n is the number of hyperparameters. Furthermore, it enables parallelism: since the predictions can
be computed by transforming the pre-activations (Equation 11), the hyperparameters for different
examples in a batch can be perturbed independently, improving sample efficiency. In practice, the
approximation can be implemented by simply replacing existing modules in deep learning libraries
with “hyper” counterparts which accept an additional vector of hyperparameters as input4.
3.2	Exact Best-Response for Two-Layer Linear Networks
Given that the best-response function is a mapping from Rn to the high-dimensional weight space
Rm , why should we expect to be able to represent it compactly? And why in particular would
equation 10 be a reasonable approximation? In this section, we exhibit a model whose best-response
function can be represented exactly using a minor variant of equation 10: a linear network with
Jacobian norm regularization. In particular, the best-response takes the form of a network whose
hidden units are modulated conditionally on the hyperparameters.
Consider using a 2-layer linear network with weights W = (Q, s) ∈ RD×D × RD to predict targets
t ∈ R from inputs x ∈ RD:
3We describe modifications for convolutional filters in Appendix C.
4We illustrate how this is done for the PyTorch library (Paszke et al., 2017) in Appendix G.
4
Published as a conference paper at ICLR 2019

Change to principal
component basis
MatmUl Q()I—
Mask the hidden state
SB
∙∙Q
Θ
-o∙∙-a
Project to 1-D Space
Matmul So ---
O
y
MatmUl V - Add C 一 (J
Construct Λ-dependent mask
Figure 1: Best-response architecture for an L2-Jacobian regularized two-layer linear network.
a(x; w) = Qx,	y(x; w) = s>a(x; w)
(12)
Suppose We use a squared-error loss regularized with an L2 penalty on the Jacobian dy∕∂x, where
the penalty weight λ lies in R and is mapped using exp to lie R+ :
2
LT (λ, w) =	(y(x; w) - t)2
(x,t)∈D
∂y
+ |D| exp(λ) ∂y(x;W)
∂x
(13)
Theorem 2. Let w0 = (Q0, s0), where Q0 is the change-of-basis matrix to the principal compo-
nents of the data matrix and s0 solves the unregularized version of Problem 13 given Q0. Then there
exist v, C ∈ RD such that the best-response function5 w * (λ) = (Q*(λ), s*(λ)) is:
Q* (λ) = σ(λv + C) Θrow Qo,	s* (λ) = S0,
where σ is the sigmoid function.
Proof. See Appendix B.2.	□
Observe that y(x; w* (λ)) can be implemented as a regular network with weights w0 = (Q0, s0)
with an additional sigmoidal gating of its hidden units a(x; w* (λ)):
a(x; w* (λ)) = Q* (λ)x = σ(λv + C) Θrow (Q0x) = σ(λv + C) Θrow a(x; w0)	(14)
This architecture is shown in Figure 1. Inspired by this example, we use a similar gating of the
hidden units to approximate the best-response for deep, nonlinear networks.
3.3	Linear Best-Response Approximations
The sigmoidal gating architecture of the preceding section can be further simplified if one only
needs to approximate the best-response function for a small range of hyperparameter values. In
particular, for a narrow enough hyperparameter distribution, a smooth best-response function can be
approximated by an affine function (i.e. its first-order Taylor approximation). Hence, we replace the
sigmoidal gating with linear gating, in order that the weights be affine in the hyperparameters. The
following theorem shows that, for quadratic lower-level objectives, using an affine approximation
to the best-response function and minimizing Ee〜P(Wσ) [f (λ + e, Wφ(λ + e))] yields the correct
best-response Jacobian, thus ensuring gradient descent on the approximate objective F(λ, Wφ(λ))
converges to a local optimum:
Theorem 3. Suppose f is quadratic with d2f∕∂W2 * 0, p(e∣σ) is Gaussian with mean 0 and variance
σ2I, and Wφ is affine. Fix λo ∈ Rn and let φ* = arg mine ESp(Wσ) f (λ + e, Wφ(λ + e))]. Then
we have wΦ∕∂λ(λ°) = dw*∕∂λ(λ0).
Proof. See Appendix B.3.
□
5
Published as a conference paper at ICLR 2019
Figure 2: The effect of the sampled neighborhood. Left: If the sampled neighborhood is too small (e.g., a
point mass) the approximation learned will only match the exact best-response at the current hyperparameter,
with no guarantee that its gradient matches that of the best-response. Middle: If the sampled neighborhood is
not too small or too wide, the gradient of the approximation will match that of the best-response. Right: If the
sampled neighborhood is too wide, the approximation will be insufficiently flexible to model the best-response,
and again the gradients will not match.
3.4	Adapting the Hyperparameter Distribution
The entries of σ control the scale of the hyperparameter distribution on which φ is trained. If
the entries are too large, then 俞φ will not be flexible enough to capture the best-response over the
samples. However, the entries must remain large enough to force Wφ to capture the shape locally
around the current hyperparameter values. We illustrate this in Figure 2. As the smoothness of the
loss landscape changes during training, it may be beneficial to vary σ.
To address these issues, we propose adjusting σ during training based on the sensitivity of the upper-
level objective to the sampled hyperparameters. We include an entropy term weighted by τ ∈ R+
which acts to enlarge the entries of σ. The resulting objective is:
Ee 〜p(e∣σ) [F(λ + €, Wφ(λ + €))] - TH[p(e∣σ)]	(15)
This is similar to a variational inference objective, where the first term is analogous to the negative
log-likelihood, but τ 6= 1. As τ ranges from 0 to 1, our objective interpolates between variational
optimization (Staines & Barber, 2012) and variational inference, as noted by Khan et al. (2018).
Similar objectives have been used in the variational inference literature for better training (Blundell
et al., 2015) and representation learning (Higgins et al., 2017).
Minimizing the first term on its own eventually moves all probability mass towards an optimum λ*,
resulting in σ = 0 if λ* is an isolated local minimum. This compels σ to balance between shrinking
to decrease the first term while remaining sufficiently large to avoid a heavy entropy penalty. When
benchmarking our algorithm,s performance, We evaluate F(λ, Wφ(λ)) at the deterministic current
hyperparameter λ0 . (This is a common practice when using stochastic operations during training,
such as batch normalization or dropout.)
3.5	Training Algorithm
We now describe the complete STN training algorithm and discuss how it can tune hyperparameters
that other gradient-based algorithms cannot, such as discrete or stochastic hyperparameters. We use
an unconstrained parametrization λ ∈ Rn of the hyperparameters. Let r denote the element-wise
function which maps λ to the appropriate constrained space, which will involve a non-differentiable
discretization for discrete hyperparameters.
Let LT and LV denote training and validation losses which are (possibly stochastic, e.g., if using
dropout) functions of the hyperparameters and parameters. Define functions f, F by f (λ, W) =
LT (r(λ), W) and F(λ, W) = LV (r(λ), W). STNs are trained by a gradient descent scheme which
alternates between updating φ for Ttrain steps to minimize ESp(Wσ) f (λ + €, Wφ(λ + €))] (Eq. 9)
and updating λ and σ for TValid steps to minimize Esp(e∣σ) [F(λ + €, Wφ(λ + €))] - TH[p(e∣σ)]
(Eq. 15). We give our complete algorithm as Algorithm 1 and show how it can be implemented in
code in Appendix G. The possible non-differentiability of r due to discrete hyperparameters poses
no problem. To estimate the derivative of ESp(Wσ) f (λ + €, Wφ(λ + €))] with respect to φ, we
can use the reparametrization trick and compute df∕∂W and dWφ∕∂φ, neither of whose computation
paths involve the discretization r. To differentiate Ee^p(e∣σ)[F(λ + €, Wφ(λ + €))] - TH[p(e∣σ)]
with respect to a discrete hyperparameter λi, there are two cases we must consider:
5This is an abuse of notation since there is not a unique solution to Problem 13 for each λ in general.
6
Published as a conference paper at ICLR 2019
Algorithm 1 STN Training Algorithm
Initialize: Best-response approximation parameters φ, hy-
perparameters λ, learning rates {αi}i3 4 *=1
while not converged do
for t = 1, . . . , Ttrain do
e 〜p(e∣σ)
φ J φ - αι ∂φ f(λ + e, WΦ(λ + W))
for t = 1, . . . , Tvalid do
w 〜p(e∣σ)
λ J λ-αc2 ∂λ (F(λ + w, wφ(λ + W)) — TH[p(w∣σ)])
σ J σ-α3∂σ(F(λ + w, Wφ(λ + W)) — TH[p(w∣σ)D
Case 1:	For most regularization
schemes, LV and hence F does not
depend on λi directly and thus the
only gradient is through wφ. Thus,
the reparametrization gradient can
be used.
Case 2:	If LV relies explic-
itly on λi, then we can use the
REINFORCE gradient estimator
(Williams, 1992) to estimate the
derivative of the expectation with
respect to λi . The number of hid-
den units in a layer is an example of
a hyperparameter that requires this
approach since it directly affects the validation loss. We do not show this in Algorithm 1, since we
do not tune any hyperparameters which fall into this case.
4 Experiments
We applied our method to convolutional networks and LSTMs (Hochreiter & Schmidhuber, 1997),
yielding self-tuning CNNs (ST-CNNs) and self-tuning LSTMs (ST-LSTMs). We first investigated
the behavior of STNs in a simple setting where we tuned a single hyperparameter, and found that
STNs discovered hyperparameter schedules that outperformed fixed hyperparameter values. Next,
we compared the performance of STNs to commonly-used hyperparameter optimization methods
on the CIFAR-10 (Krizhevsky & Hinton, 2009) and PTB (Marcus et al., 1993) datasets.
4.1 Hyperparameter Schedules
Due to the joint optimization of the hypernetwork weights and hyperparameters, STNs do not use a
single, fixed hyperparameter during training. Instead, STNs discover schedules for adapting the hy-
perparameters online, which can outperform any fixed hyperparameter. We examined this behavior
in detail on the PTB corpus (Marcus et al., 1993) using an ST-LSTM to tune the output dropout rate
applied to the hidden units.
The schedule discovered by an ST-LSTM for output dropout, shown in Figure 3, outperforms the
best, fixed output dropout rate (0.68) found by a fine-grained grid search, achieving 82.58 vs 85.83
validation perplexity. We claim that this is a consequence of the schedule, and not of regularizing
effects from sampling hyperparameters or the limited capacity of Wφ.
To rule out the possibility that the improved performance is due to stochasticity introduced by
sampling hyperparameters during STN training, we trained a standard LSTM while perturbing its
dropout rate around the best value found by grid search. We used (1) random Gaussian perturba-
tions, and (2) sinusoid perturbations for a cyclic regularization schedule. STNs outperformed both
perturbation methods (Table 1), showing that the improvement is not merely due to hyperparameter
stochasticity. Details and plots of each perturbation method are provided in Appendix F.
Method	Val	Test
p = 0.68, Fixed	85.83	83.19
p = 0.68 w/ Gaussian Noise	85.87	82.29
p = 0.68 w/ Sinusoid Noise	85.29	82.15
p = 0.78 (Final STN Value)	89.65	86.90
STN	82.58	79.02
LSTM w/ STN Schedule	82.87	79.93
Table 1: Comparing an LSTM trained with fixed and per-
turbed output dropouts, an STN, and LSTM trained with the Figure 3: Dropout schedules found by the ST-
STN schedule.	LSTM for different initial dropout rates.
Iteration
7
Published as a conference paper at ICLR 2019
PTB	CIFAR-10
Method	Val Perplexity	Test Perplexity	Val Loss	Test Loss
Grid Search	97.32	94.58	0.794	0.809
Random Search	84.81	81.46	0.921	0.752
Bayesian Optimization	72.13	69.29	0.636	0.651
STN	70.30	67.68	0.575	0.576
Table 2: Final validation and test performance of each method on the PTB word-level language modeling task,
and the CIFAR-10 image-classification task.
To determine whether the limited capacity of Wφ acts as a regularizer, We trained a standard LSTM
from scratch using the schedule for output dropout discovered by the ST-LSTM. Using this schedule,
the standard LSTM performed nearly as well as the STN, providing evidence that the schedule
itself (rather than some other aspect of the STN) was responsible for the improvement over a fixed
dropout rate. To further demonstrate the importance of the hyperparameter schedule, we also trained
a standard LSTM from scratch using the final dropout value found by the STN (0.78), and found that
it did not perform as well as when following the schedule. The final validation and test perplexities
of each variant are shown in Table 1.
Next, we show in Figure 3 that the STN discovers the same schedule regardless of the initial hyper-
parameter values. Because hyperparameters adapt over a shorter timescale than the weights, we find
that at any given point in training, the hyperparameter adaptation has already equilibrated. As shown
empirically in Appendix F, low regularization is best early in training, while higher regularization
is better later on. We found that the STN schedule implements a curriculum by using a low dropout
rate early in training, aiding optimization, and then gradually increasing the dropout rate, leading to
better generalization.
4.2 Language modeling
We evaluated an ST-LSTM on the PTB corpus (Marcus et al., 1993), which is widely used as a
benchmark for RNN regularization due to its small size (Gal & Ghahramani, 2016; Merity et al.,
2018; Wen et al., 2018). We used a 2-layer LSTM with 650 hidden units per layer and 650-
dimensional word embeddings. We tuned 7 hyperparameters: variational dropout rates for the input,
hidden state, and output; embedding dropout (that sets rows of the embedding matrix to 0); Drop-
Connect (Wan et al., 2013) on the hidden-to-hidden weight matrix; and coefficients α and β that
control the strength of activation regularization and temporal activation regularization, respectively.
For LSTM tuning, we obtained the best results when using a fixed perturbation scale of 1 for the
hyperparameters. Additional details about the experimental setup and the role of these hyperparam-
eters can be found in Appendix D.
(a) Time comparison
(b) STN schedule for dropouts
(c) STN schedule for α and β
Figure 4: (a) A comparison of the best validation perplexity achieved on PTB over time, by grid search,
random search, Bayesian optimization, and STNs. STNs achieve better (lower) validation perplexity in less
time than the other methods. (b) The hyperparameter schedule found by the STN for each type of dropout. (c)
The hyperparameter schedule found by the STN for the coefficients of activation regularization and temporal
activation regularization.
We compared STNs to grid search, random search, and Bayesian optimization.6 Figure 4a shows
the best validation perplexity achieved by each method over time. STNs outperform other meth-
6For grid search and random search we used the Ray Tune libraries (https://github.com/
ray-project/ray/tree/master/python/ray/tune). For Bayesian optimization, we used
Spearmint (https://github.com/HIPS/Spearmint).
8
Published as a conference paper at ICLR 2019
Figure 6: The hyperparameter schedule prescribed by the STN while training for image classification. The
dropouts are indexed by the convolutional layer they are applied to. FC dropout is for the fully-connected
layers.
ods, achieving lower validation perplexity more quickly. The final validation and test perplexities
achieved by each method are shown in Table 2. We show the schedules the STN finds for each hyper-
parameter in Figures 4b and 4c; we observe that they are nontrivial, with some forms of dropout used
to a greater extent at the start of training (including input and hidden dropout), some used throughout
training (output dropout), and some that are increased over the course of training (embedding and
weight dropout).
4.3 Image Classification
We evaluated ST-CNNs on the CIFAR-10 (Krizhevsky
& Hinton, 2009) dataset, where it is easy to overfit
with high-capacity networks. We used the AlexNet
architecture (Krizhevsky et al., 2012), and tuned: (1)
continuous hyperparameters controlling per-layer ac-
tivation dropout, input dropout, and scaling noise ap-
plied to the input, (2) discrete data augmentation hy-
perparameters controlling the length and number of
cut-out holes (DeVries & Taylor, 2017), and (3) con-
tinuous data augmentation hyperparameters control-
ling the amount of noise to apply to the hue, satura-
tion, brightness, and contrast of an image. In total, we
considered 15 hyperparameters.
Figure 5: A comparison of the best validation
loss achieved on CIFAR-10 over time, by grid
search, random search, Bayesian optimization,
and STNs. STNs outperform other methods for
many computational budgets.
We compared STNs to grid search, random search, and
Bayesian optimization. Figure 5 shows the lowest val-
idation loss achieved by each method over time, and
Table 2 shows the final validation and test losses for each method. Details of the experimental set-
up are provided in Appendix E. Again, STNs find better hyperparameter configurations in less time
than other methods. The hyperparameter schedules found by the STN are shown in Figure 6.
5	Related Work
Bilevel Optimization. Colson et al. (2007) provide an overview of bilevel problems, and a compre-
hensive textbook was written by Bard (2013). When the objectives/constraints are restricted to be
linear, quadratic, or convex, a common approach replaces the lower-level problem with its KKT con-
ditions added as constraints for the upper-level problem (Hansen et al., 1992; Vicente et al., 1994).
In the unrestricted setting, our work loosely resembles trust-region methods (Colson et al., 2005),
which repeatedly approximate the problem locally using a simpler bilevel program. In closely re-
lated work, Sinha et al. (2013) used evolutionary techniques to estimate the best-response function
iteratively.
Hypernetworks. First considered by Schmidhuber (1993; 1992), hypernetworks are functions map-
ping to the weights of a neural net. Predicting weights in CNNs has been developed in various forms
(Denil et al., 2013; Yang et al., 2015). Ha et al. (2016) used hypernetworks to generate weights for
modern CNNs and RNNs. Brock et al. (2017) used hypernetworks to globally approximate a best-
response for architecture search. Because the architecture is not optimized during training, they
require a large hypernetwork, unlike ours which locally approximates the best-response.
9
Published as a conference paper at ICLR 2019
Gradient-Based Hyperparameter Optimization. There are two main approaches. The first ap-
Proach approximates w* (λ0) using WT(λo, w0), the value of W after T steps of gradient descent on
f with respect to w starting at (λ0, w0). The descent steps are differentiated through to approxi-
mate dw*∕∂λ(λo) ≈ dwτ∕∂λ(λo, w0). This approach was proposed by Domke (2012) and used by
Maclaurin et al. (2015), Luketina et al. (2016) and Franceschi et al. (2018). The second approach
uses the Implicit Function Theorem to derive dw*∕∂λ(λo) under certain conditions. This was first
developed for hyperparameter optimization in neural networks (Larsen et al., 1996) and developed
further by Pedregosa (2016). Similar approaches have been used for hyperparameter optimization
in log-linear models (Foo et al., 2008), kernel selection (Chapelle et al., 2002; Seeger, 2007), and
image reconstruction (Kunisch & Pock, 2013; Calatroni et al., 2015). Both approaches struggle
with certain hyperparameters, since they differentiate gradient descent or the training loss with re-
spect to the hyperparameters. In addition, differentiating gradient descent becomes prohibitively
expensive as the number of descent steps increases, while implicitly deriving dw*∕∂λ requires using
Hessian-vector products with conjugate gradient solvers to avoid directly computing the Hessian.
Model-Based Hyperparameter Optimization. A common model-based approach is Bayesian op-
timization, which models p(r∣λ, D), the conditional probability of the performance on some metric
r given hyperparameters λ and a dataset D = {(λi,r.}. We can model p(r∣λ, D) with various
methods (Hutter et al., 2011; Bergstra et al., 2011; Snoek et al., 2012; 2015). D is constructed itera-
tively, where the next λ to train on is chosen by maximizing an acquisition function C(λ; p(r∣λ, D))
which balances exploration and exploitation. Training each model to completion can be avoided if
assumptions are made on learning curve behavior (Swersky et al., 2014; Klein et al., 2017). These
approaches require building inductive biases into p(r∣λ, D) which may not hold in practice, do not
take advantage of the network structure when used for hyperparameter optimization, and do not scale
well with the number of hyperparameters. However, these approaches have consistency guarantees
in the limit, unlike ours.
Model-Free Hyperparameter Optimization. Model-free approaches include grid search and ran-
dom search. Bergstra & Bengio (2012) advocated using random search over grid search. Successive
Halving (Jamieson & Talwalkar, 2016) and Hyperband (Li et al., 2017) extend random search by
adaptively allocating resources to promising configurations using multi-armed bandit techniques.
These methods ignore structure in the problem, unlike ours which uses rich gradient information.
However, it is trivial to parallelize model-free methods over computing resources and they tend to
perform well in practice.
Hyperparameter Scheduling. Population Based Training (PBT) (Jaderberg et al., 2017) considers
schedules for hyperparameters. In PBT, a population of networks is trained in parallel. The per-
formance of each network is evaluated periodically, and the weights of under-performing networks
are replaced by the weights of better-performing ones; the hyperparameters of the better network
are also copied and randomly perturbed for training the new network clone. In this way, a single
model can experience different hyperparameter settings over the course of training, implementing
a schedule. STNs replace the population of networks by a single best-response approximation and
use gradients to tune hyperparameters during a single training run.
6	Conclusion
We introduced Self-Tuning Networks (STNs), which efficiently approximate the best-response of
parameters to hyperparameters by scaling and shifting their hidden units. This allowed us to use
gradient-based optimization to tune various regularization hyperparameters, including discrete hy-
perparameters. We showed that STNs discover hyperparameter schedules that can outperform fixed
hyperparameters. We validated the approach on large-scale problems and showed that STNs achieve
better generalization performance than competing approaches, in less time. We believe STNs offer
a compelling path towards large-scale, automated hyperparameter tuning for neural networks.
Acknowledgments
We thank Matt Johnson for helpful discussions and advice. MM is supported by an NSERC CGS-
M award, and PV is supported by an NSERC PGS-D award. RG acknowledges support from the
CIFAR Canadian AI Chairs program.
10
Published as a conference paper at ICLR 2019
References
Eugene L Allgower and Kurt Georg. Numerical Continuation Methods: An Introduction, volume 13.
Springer Science & Business Media, 2012.
Jonathan F Bard. Practical Bilevel Optimization: Algorithms and Applications, volume 30. Springer
Science & Business Media, 2013.
Yoshua Bengio, Jerome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In
International Conference on Machine Learning, pp. 41-48. ACM, 2009.
James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of
Machine Learning Research, 13:281-305, 2012.
James S. Bergstra, Remi Bardenet, Yoshua Bengio, and Balazs KegL Algorithms for hyper-
parameter optimization. In Advances in Neural Information Processing Systems, pp. 2546-2554.
2011.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in
neural networks. arXiv preprint arXiv:1505.05424, 2015.
Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. SMASH: One-shot model
architecture search through hypernetworks. arXiv preprint arXiv:1708.05344, 2017.
Luca Calatroni, Cao Chung, Juan Carlos De Los Reyes, Carola-Bibiane Schonlieb, and Tuomo
Valkonen. Bilevel approaches for learning of variational imaging models. arXiv preprint
arXiv:1505.02120, 2015.
Olivier Chapelle, Vladimir Vapnik, Olivier Bousquet, and Sayan Mukherjee. Choosing multiple
parameters for Support Vector Machines. Machine Learning, 46(1-3):131-159, 2002.
Beno^t Colson, Patrice Marcotte, and Gilles Savard. A trust-region method for nonlinear bilevel
programming: Algorithm and computational experience. Computational Optimization and Ap-
plications, 30(3):211-227, 2005.
Beno^t Colson, Patrice Marcotte, and Gilles Savard. An overview of bilevel optimization. Annals of
Operations Research, 153(1):235-256, 2007.
Misha Denil, Babak Shakibi, Laurent Dinh, Nando De Freitas, et al. Predicting parameters in deep
learning. In Advances in Neural Information Processing Systems, pp. 2148-2156, 2013.
Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks
with cutout. arXiv preprint arXiv:1708.04552, 2017.
Justin Domke. Generic methods for optimization-based modeling. In Proceedings of Machine
Learning Research, pp. 318-326, 2012.
John Duchi. Properties of the trace and matrix derivatives, 2007. URL https://web.
stanford.edu/-jduchi/Projects/matrix_prop.pdf.
Anthony V Fiacco and Yo Ishizuka. Sensitivity and stability analysis for nonlinear programming.
Annals of Operations Research, 27(1):215-235, 1990.
Chuan-sheng Foo, Chuong B Do, and Andrew Y Ng. Efficient multiple hyperparameter learning for
log-linear models. In Advances in Neural Information Processing Systems, pp. 377-384, 2008.
Luca Franceschi, Paolo Frasconi, Saverio Salzo, and Massimilano Pontil. Bilevel programming for
hyperparameter optimization and meta-learning. arXiv preprint arXiv:1806.04910, 2018.
Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent
neural networks. In Advances in Neural Information Processing Systems, pp. 1027-1035, 2016.
Robert Gibbons. A Primer in Game Theory. Harvester Wheatsheaf, 1992.
11
Published as a conference paper at ICLR 2019
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor-
mation Processing Systems,pp. 2672-2680, 2014.
David Ha, Andrew Dai, and Quoc V Le. Hypernetworks. arXiv preprint arXiv:1609.09106, 2016.
Pierre Hansen, Brigitte Jaumard, and Gilles Savard. New branch-and-bound rules for linear bilevel
programming. SIAM Journal on Scientific and Statistical Computing, 13(5):1194-1217, 1992.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning.
Springer Series in Statistics. Springer, New York, NY, USA, 2001.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. Beta-VAE: Learning basic visual concepts with a
constrained variational framework. In International Conference on Learning Representations,
2017.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):
1735-1780, 1997.
Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Sequential model-based optimization
for general algorithm configuration. In International Conference on Learning and Intelligent
Optimization, pp. 507-523. Springer, 2011.
Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M Czarnecki, Jeff Donahue, Ali
Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, et al. Population-based train-
ing of neural networks. arXiv preprint arXiv:1711.09846, 2017.
Kevin Jamieson and Ameet Talwalkar. Non-stochastic best arm identification and hyperparameter
optimization. In International Conference on Artificial Intelligence and Statistics, 2016.
Mohammad Emtiyaz Khan, Didrik Nielsen, Voot Tangkaratt, Wu Lin, Yarin Gal, and Akash Srivas-
tava. Fast and scalable Bayesian deep learning by weight-perturbation in Adam. arXiv preprint
arXiv:1806.04854, 2018.
Aaron Klein, Stefan Falkner, Jost Tobias Springenberg, and Frank Hutter. Learning curve prediction
with Bayesian neural networks. In International Conference on Learning Representations, 2017.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. In
Technical Report, University of Toronto, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet classification with deep convo-
lutional neural networks. In Advances in Neural Information Processing Systems, pp. 1097-1105,
2012.
Karl Kunisch and Thomas Pock. A bilevel optimization approach for parameter learning in varia-
tional models. SIAM Journal on Imaging Sciences, 6(2):938-983, 2013.
Jan Larsen, Lars Kai Hansen, Claus Svarer, and M Ohlsson. Design and regularization of neural
networks: The optimal use of a validation set. In IEEE Workshop on Neural Networks for Signal
Processing, pp. 62-71. IEEE, 1996.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyper-
band: Bandit-based configuration evaluation for hyperparameter optimization. Journal of Ma-
chine Learning Research, 18(1):6765-6816, 2017.
Jonathan Lorraine and David Duvenaud. Stochastic hyperparameter optimization through hypernet-
works. arXiv preprint arXiv:1802.09419, 2018.
Jelena Luketina, Mathias Berglund, Klaus Greff, and Tapani Raiko. Scalable gradient-based tuning
of continuous regularization hyperparameters. In International Conference on Machine Learning,
pp. 2952-2960, 2016.
12
Published as a conference paper at ICLR 2019
Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimiza-
tion through reversible learning. In International Conference on Machine Learning, pp. 2113-
2122, 2015.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated
corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313-330, 1993.
Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing LSTM
language models. International Conference on Learning Representations, 2018.
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of GANs. In Advances in
Neural Information Processing Systems, pp. 1825-1835, 2017.
Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial
networks. arXiv preprint arXiv:1611.02163, 2016.
Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course, volume 87.
Springer Science & Business Media, 2013.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
PyTorch. In Advances in Neural Information Processing Workshop, 2017.
Fabian Pedregosa. Hyperparameter optimization with approximate gradient. In International Con-
ference on Machine Learning, pp. 737-746, 2016.
Jurgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent
networks. Neural Computation, 4(1):131-139, 1992.
JUrgen Schmidhuber. A ‘self-referential, weight matrix. In International Conference on Artificial
Neural Networks, pp. 446-450. Springer, 1993.
Matthias Seeger. Cross-validation optimization for large scale hierarchical classification kernel
methods. In Advances in Neural Information Processing Systems, pp. 1233-1240, 2007.
Ankur Sinha, Pekka Malo, and Kalyanmoy Deb. Efficient evolutionary algorithm for single-
objective bilevel optimization. arXiv preprint arXiv:1303.3901, 2013.
Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical Bayesian optimization of machine
learning algorithms. In Advances in Neural Information Processing Systems, pp. 2951-2959,
2012.
Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram,
Mostofa Patwary, M Prabhat, and Ryan Adams. Scalable Bayesian optimization using deep neural
networks. In International Conference on Machine Learning, pp. 2171-2180, 2015.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine
Learning Research, 15(1):1929-1958, 2014.
Joe Staines and David Barber. Variational optimization. arXiv preprint arXiv:1212.4507, 2012.
Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Freeze-thaw Bayesian optimization. arXiv
preprint arXiv:1406.3896, 2014.
Luis Vicente, Gilles Savard, and JoaqUim Judice. Descent approaches for quadratic bilevel program-
ming. Journal of Optimization Theory and Applications, 81(2):379-399, 1994.
Heinrich Von Stackelberg. Market Structure and Equilibrium. Springer Science & Business Media,
2010.
Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of neural
networks using Dropconnect. In International Conference on Machine Learning, pp. 1058-1066,
2013.
13
Published as a conference paper at ICLR 2019
Yeming Wen, Paul Vicol, Jimmy Ba, Dustin Tran, and Roger Grosse. Flipout: Efficient pseudo-
independent weight perturbations on mini-batches. International Conference on Learning Repre-
sentations, 2018.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine Learning, 8(3-4):229-256,1992.
Zichao Yang, Marcin Moczulski, Misha Denil, Nando de Freitas, Alex Smola, Le Song, and Ziyu
Wang. Deep fried convnets. In International Conference on Computer Vision, pp. 1476-1483,
2015.
Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization.
arXiv preprint arXiv:1409.2329, 2014.
Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint
arXiv:1611.01578, 2016.
14
Published as a conference paper at ICLR 2019
A Table of Notation
Table 3: Table of Notation
λ, W
λo, W0
n, m
f(λ, w),F (λ, w)
r
Lτ(λ, w), LV(λ, w)
Hyperparameters and parameters
Current, fixed hyperparameters and parameters
Hyperparameter and elementary parameter dimension
Lower-level & upper-level objective
Function mapping unconstrained hyperparameters to the appropriate restricted space
Training loss & validation loss - (LT (r(λ), w), LV (r(λ), w)) = (f (λ, w), F(λ, w))
w*(λ)
F *(λ)
λ*
W φ(λ)
φ
σ
σ
p(e∣σ),p(λ∣σ)
α
Ttrain , Tvalid
x, t
D
D
y(x, w)
row
Q,s
Q0, s0
Q* (λ), s*(λ)
a(x; w)
W,b
Dout, Din
∂Lt (λ,w)
∂λ
∂w*(λ)
∂λ
∂Lt(λ,w*(λ)) ∂w*(λ)
∂w* (λ)	∂λ
dLτ (λ,w)
dλ
Best-response of the parameters to the hyperparameters
Single-level objective from best-response, equals F(λ, w*(λ))
Optimal hyperparameters
Parametric approximation to the best-response function
Approximate best-response parameters
Scale of the hyperparameter noise distribution
The sigmoid function
Sampled perturbation noise, to be added to hyperparameters
The noise distribution and induced hyperparameter distribution
A learning rate
Number of training steps on the training and validation data
An input datapoint and its associated target
A data set consisting of tuples of inputs and targets
The dimensionality of input data
Prediction function for input data x and elementary parameters w
Row-wise rescaling - not elementwise multiplication
First and second layer weights of the linear network in Problem 13
The basis change matrix and solution to the unregularized Problem 13
The best response weights of the linear network in Problem 13
Activations of hidden units in the linear network of Problem 13
A layer's weight matrix and bias
A layer,s input dimensionality and output dimensionality
The (validation loss) direct (hyperparameter) gradient
The (elementary parameter) response gradient
The (validation loss) response gradient
The hyperparameter gradient: a sum of the validation losses direct and response gradients
15
Published as a conference paper at ICLR 2019
B Proofs
B.1	Lemma 1
Because w0 solves Problem 4b given λ0 , by the first-order optimality condition we must have:
df (λ0, wo)=0	(16)
∂w
The Jacobian of df∕∂w decomposes as a block matrix with sub-blocks given by:
h 忌 I d22 i	(17)
∂λ∂w	∂w2
We know that f is C2 in some neighborhood of (λo, w0), so df∕∂w is continuously differentiable
in this neighborhood. By assumption, the Hessian d2f∕∂w2 is positive definite and hence invertible
at (λ0, w0). By the Implicit Function Theorem, there exists a neighborhood V of λ0 and a unique
continuously differentiable function w* : V → Rm such that df∕∂w(λ, w*(λ)) = 0 for λ ∈ V and
w*(λo) = W0.
Furthermore, by continuity we know that there is a neighborhood W1 × W2 of (λ0 , w0 ) such that
∂2f∕∂w2 is positive definite on this neighborhood. Setting U = V ∩ W1 ∩ (w*)-1 (W2), we can
conclude that ∂2f∕∂w2(λ, w*(λ)) 0 for all λ ∈ U. Combining this with ∂f∕∂w(λ, w*(λ)) = 0 and
using second-order sufficient optimality conditions, we conclude that w* (λ) is the unique solution
to Problem 4b for all λ ∈ U .
B.2	Lemma 2
This discussion mostly follows from Hastie et al. (2001). We let X ∈ RN ×D denote the data matrix
where N is the number of training examples and D is the dimensionality of the data. We let t ∈ RN
denote the associated targets. We can write the SVD decomposition of X as:
X =UDV>
(18)
where U and V are N × D and D × D orthogonal matrices and D is a diagonal matrix with
entries di ≥ d2 ≥ ∙ ∙ ∙ ≥ d0 > 0. We next simplify the function y(x; W) by setting U = s>Q, so
that y(x; w) = s>Qx = u>x. We see that the Jacobian ∂y∕∂x ≡ u is constant, and Problem 13
simplifies to standard L2 -regularized least-squares linear regression with the following loss function:
X (U>χ — t)2 + 焉 exp(λ) kuk2
(x,t)∈D
(19)
It is well-known (see Hastie et al. (2001), Chapter 3) that the optimal solution U* (λ) minimizing
Equation 19 is given by:
U*(λ) = (X>X + exp(λ)I)-1X>t = V(D2 + exp(λ)I)-1DU>t	(20)
Furthermore, the optimal solution U* to the unregularized version of Problem 19 is given by:
U* = V D-1U>t	(21)
Recall that we defined Q0 = V >, i.e., the change-of-basis matrix from the standard basis to the
principal components of the data matrix, and we defined s0 to solve the unregularized regression
problem given Q0. Thus, we require that Q0>s0 = U* which implies s0 = D-1U >t.
There are not unique solutions to Problem 13, so we take any functions Q(λ), s(λ) which sat-
isfy Q(λ)>s(λ) = v* (λ) as “best-response functions”. We will show that our chosen functions
Q* (λ) = σ(λv + c) row Q0 and s*(λ) = s0, where v = -1 and ci = 2 log(di) for i = 1, . . . , D,
meet this criteria. We start by noticing that for any d ∈ R+, we have:
σ(-λ + 2 log(d))
1	1	d2
1 + exp(λ — 2 log(d))	1 + d-2 exp(λ)	d2 + exp(λ)
(22)
16
Published as a conference paper at ICLR 2019
It follows that:
Q*(λ)>s*(λ) = [σ(λv + c) Θrow Qo]>S0
L	/	∖	r T
σ σ(-λ + 2log(d1 ))∖
=diag	.	Qo so
一	∖σ(-λ + 2log(dp)))	.
■	/ _d_ ∖]
d2+exp(λ) ∖
diag . I I so
■凡 I I
.	∖d'D +eχp(λ"」
V
■	/ _d_ M
d2+exp(λ) ∖
diag . I I DTUTt
先11
.	∖dD+eχp(λ"」
V
' fdl_∖]
d2+exp(λ) ∖
diag . I I Uτt
.
V [(D2 + exp(λ)I)-1D] Uτt
v*(λ)
(23)
(24)
(25)
(26)
(27)
(28)
(29)
B.3 THEOREM 3
By assumption f is quadratic, so there exist A ∈ Rn×n, B ∈ Rn×m, C ∈ Rm×m and d ∈ Rn, e ∈
Rm such that:
fg w)=2 (λ]	WT) (BT C)C)+d>λ+e>w
One can easily compute that:
f (λ, W) = Bt λ + C W + e
∂ w
∂2f,	、
TT^2 (λ, W) = C
∂ W2
(30)
(31)
(32)
Hence, We find:
We let w φ(λ)
Since we assume d2f∕∂w2 * 0, we must have C * 0. Setting the derivative equal to 0 and using
second-order sufficient conditions, we have:
W*(λ) = -C-1(e + Bτλ)	(33)
dW*m	「trt 次 (λ) = -C B	(34)
Uλ + b, and define f to be the function given by:	
O , 			 .	一	・.，_		，-	.	_ ., f(λ, U,	b, σ)	= Ecp(Wb)	[f (λ	+ e, U (λ	+ e)	+ b)]	(35)
Substituting and simplifying:

ʌ , _    、 _	1 , _	、-T.,_	、 ，-	、_T    	、 -、
f(λo, U, b,σ) = ESp(Wb) 2(λo + e) A(λo + e) + (λo + e) B(U(λo + e) + b)
+ 2(U (λo + e) + b)τC(U(λo + e) + b)
+ dτ (λo + e) + eT (U (λo + e) + b)] (36)
17
Published as a conference paper at ICLR 2019
Expanding, We find that equation 36 is equal to:
Ecp(Wb)[① + @ + ③ + ④]	(37)
where we have:	①=1 (λ> Aλo + 2βτAλo + βτAβ)	(38)
②=λ> BUλ0 + λ> BUe + λ>Bb + eτBUλ0 + eτBUe + eτBb	(39)
③=1(λ> U τCUλ0 + λ0U TCUe + λ0 U τCb + eτU τCUλ0
+ eτ U TCUe + eτU τCb + bτCUλ0 + bτ CUe + bτCb) (40)
④=dτλ0 + dτe + eτUλ0 + eτUe + eτb	(41)
We can simplify these expressions considerably by using linearity of expectation and that e 〜p(e∣σ)
has mean 0:
Ee 〜p(e∣σ)[①]=1 λτAλ0	(42)
Ecp(e∣b)[②]=λτBUλ0 + λτBb + Ecp(e∣b) [eτBUe]	(43)
Ee〜p(e∣σ)[③]=2(λτUτCUλ0 + λ0UTCb+
Ecp(e∣b) [eτUτCUe] + bτCUλ0 + bτCb) (44)
Ee〜p(e∣σ)[④]=dτλ0 + eτUλ0 + eτb	(45)
We can use the cyclic property of the Trace operator, Ee〜p(e∣ σ) [eeτ] = σ2I, and commutability of
expectation and a linear operator to simplify the expectations of (2) and (ɪ):
Ee〜p(e∣ σ) [(2)] = λTBUλ0 + λTBb + Tτ [σ2BU]	(46)
Ee〜p(e∣σ)[③]=2(λTUτCUλ0 + λ0UTCb+
Tr [σ2UτCU] + bτCUλ0 + bτCb) (47)
We can then differentiate f by making use of various matrix-derivative equalities (Duchi, 2007) to
find:
^
∂b(λ0, U, b, σ) = 1CτUλ0 + 1 CUλ0 + Bτλ0 + e + Cb	(48)
^
f (λ0, U, b,σ) = Bτλ0λT + σ2Bτ + CbλT + eλT + CUλολT + σ2CU (49)
Setting the derivative df∕∂b(λ0, U, b, σ) equal to 0, we have:
b = -C-1(C τUλ0 + Bτλ0 + e)	(50)
Setting the derivative for df∕∂u(λ0, U, b, σ) equal to 0, we have:
CU(λ0λT + σ2C) = -Bτλ0λT - σ2Bτ - CbλT - eλT	(51)
Substituting the expression for b given by equation 50 into equation 51 and simplifying gives:
CU (λ0 λT + σ2I) = -σ2Bτ + CUλ0λT	(52)
=⇒ σ2CU = -σ2Bτ	(53)
=⇒ U = -CTBτ	(54)
This is exactly the best-response Jacobian dw*∕∂λ(λ) as given by Equation 34. Substituting U =
C-1B into the equation 50 gives:
b = C-1Bτλ0 - C-1Bτλ0 - CTe	(55)
This is w*(λ0) - dw*∕∂λ(λ0), thus the approximate best-response is exactly the first-order Taylor
series of w* about λ0.
18
Published as a conference paper at ICLR 2019
B.4 Best-Response Gradient Lemma
Lemma 4. Under the same conditions as Lemma 1 and using the same notation, for all λ ∈ U, we
have that:
∂ w *	Γ∂2f
lλ(λ) =-而(λ, w (λ))
-1 ∂2 f
Ww (λ,w *(λ))
(56)
Proof. Define ι* : U → Rn × Rm by ι*(λ) = (λ, w*(λ)). By first-order optimality conditions, we
know that:
(f ◦ ∣*) (λ) =0 ∀λ ∈ U	(57)
Hence, for all λ ∈ U:
0=∂λ (f o ι)㈤	(58)
∂2 f	∂w*	∂2 f
=熹(IW)示 (λ) + ∂λ∂ffw (IW)	(59)
∂2 f	∂w*	∂2 f
=U (λ, w*(λ)) F ㈤ + ∂f (λ w*(λ))	(60)
Rearranging gives Equation 56.	□
C Best-Response Approximations for Convolutional Filters
We let L denote the number of layers, Cl the number of channels in layer l’s feature map, and Kl
the size of the kernel in layer l. We let Wl,c ∈ RCl-1 ×Kl ×Kl and bl,c ∈ R denote the weight and
bias respectively of the cth convolution kernel in layer l (so c ∈ {1, . . . , Cl}). For ul,c, al,c ∈ Rn,
We define best-response approximations Wzφ,c and bφc by:
Wφ,c(λ) = (λ>ul,c) Θ Wyper + Wellem	(61)
琮(λ) = (λ>al,c) Θ bhyper + belCm	(62)
Thus, the best-response parameters used for modeling Wl,c ,	bl are
{ul,c, al,c, Whly,cper, Well,ecm, blh,ycper, ble,lecm}. We can compute the number of parameters used as
2n + 2(|Wl,c| + |bl,c|). Summing over channels c, We find the total number of parameters is
2nCl + 2p, Where p is the total number of parameters in the normal CNN layer. Hence, We use
tWice the number of parameters in a normal CNN, plus an overhead that depends on the number of
hyperparameters.
For an implementation in code, see Appendix G.
D Language Modeling Experiment Details
Here We present additional details on the setup of our LSTM language modeling experiments on
PTB, and on the role of each hyperparameter We tune.
We trained a 2-layer LSTM With 650 hidden units per layer and 650-dimensional Word embeddings
(similar to (Zaremba et al., 2014; Gal & Ghahramani, 2016)) on sequences of length 70 in mini-
batches of size 40. To optimize the baseline LSTM, We used SGD With initial learning rate 30,
Which Was decayed by a factor of 4 based on the non-monotonic criterion introduced by Merity et al.
(2018) (i.e., Whenever the validation perplexity fails to improve for 5 epochs). FolloWing Merity
et al. (2018), We used gradient clipping 0.25.
To optimize the ST-LSTM, We used the same optimization setup as for the baseline LSTM. For the
hyperparameters, We used Adam With learning rate 0.01. We used an alternating training schedule
in Which We updated the model parameters for 2 steps on the training set and then updated the
hyperparameters for 1 step on the validation set. We used one epoch of Warm-up, in Which We
19
Published as a conference paper at ICLR 2019
updated the model parameters, but did not update hyperparameters. We terminated training when
the learning rate dropped below 0.0003.
We tuned variational dropout (re-using the same dropout mask for each step in a sequence) on
the input to the LSTM, the hidden state between the LSTM layers, and the output of the LSTM.
We also tuned embedding dropout, which sets entire rows of the word embedding matrix to 0,
effectively removing certain words from all sequences. We regularized the hidden-to-hidden weight
matrix using DropConnect (zeroing out weights rather than activations) (Wan et al., 2013). Because
DropConnect operates directly on the weights and not individually on the mini-batch elements, we
cannot use independent perturbations per example; instead, we sample a single DropConnect rate per
mini-batch. Finally, we used activation regularization (AR) and temporal activation regularization
(TAR). AR penalizes large activations, and is defined as:
α∖∖m Θ ht∣∣2	(63)
where m is a dropout mask and ht is the output of the LSTM at time t. TAR is a slowness regularizer,
defined as:
β∖∖ht-ht+1∖∖2	(64)
For AR and TAR, we tuned the scaling coefficients α and β. For the baselines, the hyperparameter
ranges were: [0, 0.95] for the dropout rates, and [0, 4] for α and β. For the ST-LSTM, all the dropout
rates and the coefficients α and β were initialized to 0.05 (except in Figure 3, where we varied the
output dropout rate).
E	Image Classification Experiment Details
Here, we present additional details on the CNN experiments. For all results, we held out 20% of the
training data for validation.
We trained the baseline CNN using SGD with initial learning rate 0.01 and momentum 0.9, on
mini-batches of size 128. We decay the learning rate by 10 each time the validation loss fails to
decrease for 60 epochs, and end training if the learning rate falls below 10-5 or validation loss
has not decreased for 75 epochs. For the baselines—grid search, random search, and Bayesian
optimization—the search spaces for the hyperparameters were as follows: dropout rates were in the
range [0, 0.75]; contrast, saturation, and brightness each had range [0, 1]; hue had range [0, 0.5]; the
number of cutout holes had range [0, 4], and the length of each cutout hole had range [0, 24].
We trained the ST-CNN’s elementary parameters using SGD with initial learning rate 0.01 and
momentum of 0.9, on mini-batches of size 128 (identical to the baselines). We use the same decay
schedule as the baseline model. The hyperparameters are optimized using Adam with learning rate
0.003. We alternate between training the best-response approximation and hyperparameters with
the same schedule as the ST-LSTM, i.e. Ttrain = 2 steps on the training step and Tvalid = 1 steps
on the validation set. Similarly to the LSTM experiments, we used five epochs of warm-up for
the model parameters, during which the hyperparameters are fixed. We used an entropy weight of
τ = 0.001 in the entropy regularized objective (Eq. 15). The cutout length was restricted to lie in
{0, . . . , 24} while the number of cutout holes was restricted to lie in {0, . . . , 4}. All dropout rates, as
well as the continuous data augmentation noise parameters, are initialized to 0.05. The cutout length
is initialized to 4, and the number of cutout holes is initialized to 1. Overall, we found the ST-CNN
to be relatively robust to the initialization of hyperparameters, but starting with low regularization
aided optimization in the first few epochs.
F	Additional Details on Hyperparameter Schedules
Here, we draw connections between hyperparameter schedules and curriculum learning. Curriculum
learning (Bengio et al., 2009) is an instance of a family of continuation methods (Allgower & Georg,
2012), which optimize non-convex functions by solving a sequence of functions that are ordered by
increasing difficulty. In a continuation method, one considers a family of training criteria Cλ (w)
with a parameter λ, where C1 (w) is the final objective we wish to minimize, and C0(w) represents
the training criterion for a simpler version of the problem. One starts by optimizing C0(w) and then
gradually increases λ from 0 to 1, while keeping w at a local minimum of Cλ (w) (Bengio et al.,
2009). This has been hypothesized to both aid optimization and improve generalization. In this
20
Published as a conference paper at ICLR 2019
section, we explore how hyperparameter schedules implement a form of curriculum learning; for
example, a schedule that increases dropout over time increases stochasticity, making the learning
problem more difficult. We use the results of grid searches to understand the effects of different
hyperparameter settings throughout training, and show that greedy hyperparameter schedules can
outperform fixed hyperparameter values.
First, we performed a grid search over 20 values each of input and output dropout, and measured the
validation perplexity in each epoch. Figure 7 shows the validation perplexity achieved by different
combinations of input and output dropout, at various epochs during training. We see that at the start
of training, the best validation loss is achieved with small values of both input and output dropout.
As we train for more epochs, the best validation performance is achieved with larger dropout rates.
0.9
0.8
+j°∙7
I0-6
朝
⅛ 0.4
d
⅛0.3
O
0.2
0.1
°⅞
.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Input Dropout
(a)
0.9
0.8
0.7
0.6
0.5
0.4
o.ι
Input Dropout
(b)
Epoch 25
∣.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Input Dropout
(c)
Figure 7:	Validation performance of a baseline LSTM given different settings of input and output
dropout, at various epochs during training. (a), (b), and (c) show the validation performance on PTB given
different hyperparameter settings, at epochs 1, 10, and 25, respectively. Darker colors represent lower (better)
validation perplexity.
Next, we present a simple example to show the potential benefits of greedy hyperparameter sched-
ules. For a single hyperparameter—output dropout—we performed a fine-grained grid search and
constructed a dropout schedule by using the hyperparameter values that achieve the best validation
perplexity at each epoch in training. As shown in Figure 8, the schedule formed by taking the best
output dropout value in each epoch yields better generalization than any of the fixed hyperparameter
values from the initial grid search. In particular, by using small dropout values at the start of train-
ing, the schedule achieves a fast decrease in validation perplexity, and by using larger dropout later
in training, it achieves better overall validation perplexity.
(a) Greedy schedule for output dropout, derived
by taking the best hyperparameters in each epoch
from a grid search.
Epoch
(b) Comparison of fixed output dropout values and
the dropout schedule derived from grid searches
Figure 8:	Grid search-derived schedule for output dropout.
Figure 9 shows the perturbed values for output dropout we used to investigate whether the improved
performance yielded by STNs is due to the regularization effect, and not the schedule, in Section 4.1.
21
Published as a conference paper at ICLR 2019
Iteration
(a) P 〜N(0.68, 0.05)
(b) p = 0.68 with sin noise
(c) ST-LSTM
Figure 9:	Comparison of output dropout schedules. (a) Gaussian-perturbed output dropout rates around the
best value found by grid search, 0.68; (b) sinusoid-perturbed output dropout rates with amplitude 0.1 and a
period of 1200 mini-batches; (c) the output dropout schedule found by the ST-LSTM.
G Code Listings
In this section, we provide PyTorch code listings for the approximate best-response layers used to
construct ST-LSTMs and ST-CNNs: the HyperLinear and HyperConv2D classes. We also
provide a simplified version of the optimization steps used on the training set and validation set.
Listing 1: HyperLinear, used as a drop-in replacement for Linear modules
class HyperLinear(nn.Module):
def __init___(self, input_dim, Output_dim, n_hparams):
Super(HyperLinear, self).___init__()
self.input_dim = input_dim
self.output_dim = Output_dim
self.n_hparams = n_hparams
self.n_ScalarS = Output_dim
self.elem_w = nn.Parameter(torch.Tensor(output_dim, input_dim))
self.elem_b = nn.Parameter(torch.Tensor(output_dim))
self.hnet_w = nn.Parameter(torch.Tensor(output_dim, input_dim))
self.hnet_b = nn.Parameter(torch.Tensor(output_dim))
self.htensor_to_scalars = nn.Linear(self.n_hparams,
self.n_Scalars*2, bias=FalSe)
self.init_params()
def forward(self, input, hnet_tensor):
output = F.linear(input, self.elem_w, self.elem_b)
if hnet_tensor is not None:
hnet_scalars = self.htensor_to_scalars(hnet_tensor)
hnet_wscalars = hnet_scalars[:, :self.n_scalars]
hnet_bscalars = hnet_scalars[:, self.n_Scalars:]
hnet_out = hnet_wscalars * F.linear(input, self.hnet_w)
hnet_out += hnet_bscalars * self.hnet_b
output += hnet_out
return output
Listing 2: HyperConv2d, used as a drop-in replacement for Conv2d modules
class HyperConv2d(nn.Module):
22
Published as a conference paper at ICLR 2019
def __init__(self, in_channels, out_channels, kernel_size, padding,
num_hparams,
Stride=1, bias=True):
Super(HyperConv2d, self).___init__()
self.in_Channels = in_channels
self.out_ChannelS = out_channels
self.kernel_size = kernel_size
self.padding = padding
self.num_hparams = num_hparams
self.Stride = stride
self.elem_Weight = nn.Parameter(torch.Tensor(
out_channels, in_channels, kernel_size, kernel_Size))
self.hnet_Weight = nn.Parameter(torch.Tensor(
out_channels, in_channels, kernel_size, kernel_Size))
if bias:
self.elem_bias = nn.Parameter(torch.Tensor(out_ChanneIS))
self.hnet_bias = nn.Parameter(torch.Tensor(out_ChanneIS))
else:
self.register_parameter(' elem_bias', None)
self.register_parameter(' hnet_bias', None)
self.htensor_to_scalars = nn.Linear(
self.num_hparams, self.out_Channels*2, bias=False)
self.elem_SCaIar = nn.Parameter(torch.ones(1))
self.init_params()
def forward(self, input, htensor):
"""
Arguments :
input (tensor): size should be (B, C, H, W)
htensor (tensor): size should be (B, D)
"""
output = F.conv2d(input, self.elem_Weight, self.elem_bias,
padding=self.padding,
stride=self.stride)
output *= self.elem_SCaIar
if htensor is not None:
hnet_scalars = self.htensor_to_scalars(htensor)
hnet_WSCaIarS = hnet_scalars[:,
:self.out_channels].unsqueeze(2).unsqueeze(2)
hnet_bSCaIarS = hnet_scalars[:, self.out_Channels:]
hnet_out = F.conv2d(input, self.hnet_Weight,
padding=self.padding,
stride=self.stride)
hnet_out *= hnet_wscalars
if self.hnet_bias is not None:
hnet_out += (hnet_bscalars *
self.hnet_bias).unsqueeze(2).unsqueeze(2)
output += hnet_out
return output
def init_params(self):
n = self.in_channels * self.kernel_size * self.kernel_size
stdv = 1. / math.sqrt(n)
self.elem_Weight.data.uniform_(-stdv, stdv)
self.hnet_Weight.data.uniform_(-stdv, stdv)
if self.elem_bias is not None:
self.elem_bias.data.uniform_(-stdv, stdv)
self.hnet_bias.data.uniform_(-stdv, stdv)
23
Published as a conference paper at ICLR 2019
self.htensor_to_scalars.weight.data.normal_(std=0.01)
Listing 3: Stylized optimization step on the training set for updating elementary parameters
#	Perturb hyperparameters around current value in unconstrained
# parametrization .
batch_htensor = PertUrb(htensor, hscale)
#	Apply necessary reparametrization of hyperparameters.
hparam_tensor = hparam_transform(batch_htensor)
#	Sets data augmentation hyperparameters in the data loader.
dataset.set_hparams(hparam_tensor)
#	Get next batch of examples and apply any input transformation
#	(e.g. input dropout) as dictated by the hyperparameters.
images, labels = next_batch(dataset)
images = apply_input_transform(images, hparam_tensor)
#	Run everything through the model and do gradient descent.
pred = hyper_cnn(images, batch_htensor, hparam_tensor)
XentrOPy_loss = F.cross_entropy(pred, labels)
XentrOPy_loss.backward()
cnn_optimizer.step()
Listing 4: Stylized optimization step on the validation set for updating hyperparameters/noise scale
#	Perturb hyperparameters around current value in unconstrained
#	parametrization, so We can assess sensitivity of validation
# loss to the scale of the noise.
batch_htensor = perturb(htensor, hscale)
#	Apply necessary reparametrization of hyperparameters.
hparam_tensor = hparam_transform(batch_htensor)
#	Get next batch of examples and run through the model.
images, labels = next_batCh(VaIid_dataset)
pred = hyper_cnn(images, batch_htensor, hparam_tensor)
xentropy_loss = F.cross_entropy(pred, labels)
#	Add extra entropy weight term to loss.
entropy = compute_entropy(hscale)
loss = xentropy_loss - args.entropy_Weight * entropy
loss.backward()
#	Tune the hyperparameters.
hyper_optimizer.step()
#	Tune the scale of the noise applied to hyperparameters.
scale_optimizer.step()
H	Sensitivity Studies
In this section, we present experiments that show how sensitive our STN models are to different
meta-parameters.
In particular, we investigate the effect of using alternative schedules (Figure 10) for the number of
optimization steps performed on the training and validation sets.
Additionally, we investigate the effect of using different initial perturbation scales for the hyperpa-
rameters, which are either fixed or tuned (Figure 11).
24
Published as a conference paper at ICLR 2019
140 τ ›130 §120 A-11 a			ɪɪ I BayesOpt		
	Z		1 Train / 1		Val
	Γ		-1 Train / 2 	7 Train / 1		Val Val
ɪ- ɪɪu CU 工1 nn	u		-10 Train / 10 Val		
	 ±UU > nn.				10 Train /		15 Val
Best, J ∞ U 5 O C			15 I ram / ±u vaι 	15 Train / 15 Val		
		一一		—		-、一	
/U					
O 20k 4Ok 60k	80k IOOk
Time (s)
∙0s∙8∙7∙65
Iooooo
SSo-I-B>
(b) CNN Train & Valid Step Schedules
(a) LSTM Train & Valid Step Schedules
Figure 10:	The effect of using a different number of train/val steps. For the CNN, we include Bayesian
Optimization and the reported STN parameters for comparison. During these experiments we found schedules
which achieve better final loss with CNNs.
Time (s)
∙0s∙8∙7∙65
Iooooo
sso,而>
(b) CNN Scales
(a) LSTM Scales
Figure 11:	The effect of using different perturbation scales. For the CNN, we include Bayesian Optimization
and the reported STN parameters for comparison. For (a), the perturbation scales are fixed.
25