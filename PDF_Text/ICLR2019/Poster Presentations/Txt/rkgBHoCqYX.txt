Published as a conference paper at ICLR 2019
A Kernel Random Matrix-Based Approach
for Sparse PCA
Mohamed El Amine Seddik1,2, Mohamed Tamaazousti1 & Romain Couillet2,3,*
1CEA List,2 CentraleSupeiec, 3GIPSA-Lab University of GrenobleAlPes
{mohamedelamine.seddik,mohamed.tamaazousti}@cea.fr
romain.couillet@gipsa-lab.grenoble-inp.fr
Ab stract
In this paper, we present a random matrix approach to recover sparse principal
components from n p-dimensional vectors. Specifically, considering the large
dimensional setting where n, p → ∞ with p/n → c ∈ (0, ∞) and under Gaussian
ʌ
vector observations, we study kernel random matrices of the type f (C), where f is
a three-times continuously differentiable function applied entry-wise to the sample
ʌ
covariance matrix C of the data. Then, assuming that the principal components
are sparse, we show that taking f in such a way that f0 (0) = f00 (0) = 0 allows
for powerful recovery of the principal components, thereby generalizing previous
ideas involving more specific f functions such as the soft-thresholding function.
1 Introduction
Principal component analysis (PCA) is extensively used in data analysis and machine learning
applications. It is a dimension reduction technique that aims to project a given dataset onto principal
subspaces spanned by the leading eigenvectors of the sample covariance matrix (Wold et al., 1987),
which represent the principal modes of variance. Basically, the statistical interpretation of PCA lies in
the fact that most of the variance in the data is captured by these modes. Consequently, PCA reduces
the dimension of the feature space while keeping most of the information in the data. It is well-known
(Anderson, 1963) that PCA performs efficiently in the traditional data setting where the number of
features is small and the number of samples is large.
Consider a data matrix Y ∈ Rp×n consisting of n centered samples, each sample having p features.
The standard PCA method requires the computation of the sample covariance matrix C = YY|/n
and estimates the first principal components u1, u2, . . . (i.e., the successive dominant eigenvectors of
C = E [YY |/n]) by the ordered eigenvectors Ui, U?,... of C. (Johnstone & Lu, 2009) demonstrated
that, in the high dimensional regime where n, p → ∞ with p/n → c > 0, the principal component
Ui estimated by standard PCA is inconsistent. Essentially, if p/n 9 0 then ∣∣Uι - uik2 9 0 in
the high-dimensional asymptotic regime. This phenomenon is well investigated within the field
of random matrix theory for covariance models of the form C = n Σp∕2XXlΣp/2, where Σp is a
positive semi-definite matrix and X is ap × n matrix with random i.i.d. entries. One of the main
results from random matrix theory concerns the so-called spiked models, where Σp is a low-rank
perturbation of the identity matrix, namely Σp = Ip + Pk=I 3%曲吗 with k fixed with respect to p, n.
(Baik et al., 2005) and (Paul, 2007) notably exhibited a phase transition phenomenon: as p/n → c, if
ωi < √c the estimated principal component Ui using standard PCA is (almost surely) asymptotically
orthogonal to the true principal component Ui (i.e., U|ui → 0); on the other hand, if ωi > √c,
lim infn |U|〃i | > 0. This phase transition phenomenon has attracted recently much attention within
the random matrix community (Benaych-Georges & Nadakuditi, 2011; Capitaine et al., 2009; Feral
& Peche, 2007; Knowles & Yin, 2013).
The inconsistency of standard PCA in high dimensions motivated the idea to look for more structural
information on the principal components. In particular, considering that the principal components
*Couillet,s work is supported by the GSTATS UGA IDEX Datascience chair and the ANR RMT4GRAPH
(ANR-14-CE28-0006).
1
Published as a conference paper at ICLR 2019
are sparse in an appropriate basis (e.g., in the wavelet domain), a large body of works have emerged
and proposed improved PCA approaches that account for sparsity. One of the most consistent sparse
PCA methods in the literature is the covariance thresholding (CT) algorithm (Krauthgamer et al.,
ʌ
2015). Based on the intuition that the small entries of the empirical covariance matrix C induce noise
in its principal components, this method consists in applying the popular soft-thresholding function
(with threshold T > 0); soft(∙; T) : t → sign(t) ∙ (|t| 一 T)+, entry-wise to the empirical covariance
ʌ
matrix C and performing PCA on the resulting matrix. (Deshpande & Montanari, 2014; 2016)
have theoretically demonstrated that the covariance thresholding algorithm recovers the sought-for
principal components with high probability under controlled growth rates between p, n and the
sparsity level. In this paper, we show that the soft-thresholding method in fact falls within a broader
class of kernel-based1 PCA algorithms that are particularly suited to sparse PCA recovery. This
ʌ ʌ
method consists in considering the matrix f(C) instead of C where f is a function applied entry-wise.
By imposing some constraints on f, most importantly that f0(0) = f00(0) = 0, sparse PCA can be
performed with provably high accuracy for sufficiently large n, p.
The rest of the paper is organized as follows. In Section 2, we present the related work on sparse
PCA. We recall some necessary concentration of measure tools and notions about sparse matrices
in Section 3. Our main theoretical results are then provided in Section 4. Section 5 discusses the
practical aspects and provides experimental results. Section 6 concludes the article.
Notation: In following, the notation [n] denotes the set {1, . . . , n}, bac denotes the integer part
of a. Given a vector X ∈ Rn, the '2-norm of X is denoted as ∣∣xk2 = PZι x2∙ Given an n X n
matrix M, Mij or [M]j denote the entry of the matrix M at line i and column j. [M]∙,j denotes
the j-th column vector of M and [M]i,∙ its i-th line vector. The Frobenius norm ∣∣M∣∣f of the
matrix M is defined as ∣M ∣2F = Pin,j=1 Mi2j, and the operator norm ∣M ∣op of M is defined as
∣M ∣op = maxkxk=1 ∣M X∣. Finally, denotes the Hadamard product, with [M N]ij = Mij Nij.
2	Related work
The problem of sparse PCA has been tackled with a large range of techniques. Mainly, three
classes of approaches emerge in the literature. Most popular techniques are optimization-based
algorithms (d’Aspremont et al., 2005; Moghaddam et al., 2006; Zass & Shashua, 2007; Zou et al.,
2006; Wright et al., 2009), where the idea is to see the problem of sparse PCA through an optimization
perspective, and to propose methods to solve the latter by either considering a different formulation
一 e.g. semi-definite programming (SDP) or convex relaxations - or adding penalties to the original
optimization problem such as a LASSO regularization. The second class of approaches covers
matrix decomposition-based techniques (Asteris et al., 2014; Papailiopoulos et al., 2013; Shen &
Huang, 2008), where sparse principal components are extracted through solving a low rank matrix
approximation problem based on Singular Value Decomposition. Finally, most consistent sparse PCA
methods adopt thresholding-based approaches: initial heuristics used factor rotation techniques and
thresholding of eigenvectors to obtain sparsity (Cadima & Jolliffe, 1995). Based on the well-known
power method, (Yuan & Zhang, 2013) introduced an efficient sparse PCA approximation to obtain
the exact level of required sparsity, by truncating to zero the principal components iteratively except
for their largest entries. A step further, under a spiked covariance model (see Section 4.1), (Ma et al.,
2013) proposed a very efficient iterative thresholding approach for estimating principal subspaces
in the sparse setting. Similarly, assuming a single-spike model, (Krauthgamer et al., 2015) proved
that, when the sparsity level S ≥ Ω(√n), a standard SDP approach cannot recover consistently the
sparse spike; in particular, the authors presented empirical results suggesting that for S = O(√n),
recovery is possible by a simple covariance thresholding algorithm. More recently, (Deshpande &
Montanari, 2016) analyzed and theoretically proved, under a spiked model, that indeed the covariance
thresholding algorithm (Krauthgamer et al., 2015) succeeds with high probability under controlled
growth rates between p, n and S.
In this work, while restricting ourselves to a setting where p and n grow at a controlled joint rate,
we provide an elementary argument, based on a matrix-wise Taylor expansion controlled through a
concentration of measure approach, that generalizes the CT method to a large family of kernel-based
1We use the kernel-based terminology to highlight that our work falls within the framework of kernel random
matrices and should not be confused with the standard kernel PCA.
2
Published as a conference paper at ICLR 2019
methods, by means of a kernel random matrix approach (El Karoui, 2010b;a). Concretely, we study
kernel random matrices of the form f (YY|/n) where Y = Σp∕2X and X is a random matrix with
N(0, 1) i.i.d. entries. (El Karoui, 2010b) studied kernel matrices of the form f(Y|Y/n) (i.e., the so-
called inner-product kernel matrices), which is equivalent to the case Σp = Ip when considering the
form f(YY|/n). In particular, we elaborate from El Karoui’s study by Taylor expanding f(YY|/n)
in the vicinity of Σp entry-wise and controlling the resulting matrices via concentration arguments.
3	Preliminaries
Before introducing our model setting we recall some definitions and notions of the concentration of
measure theory (Ledoux, 2005) that are at the heart of our main results. Furthermore, we recall a
definition, introduced by (El Karoui, 2008), of sparse matrices in the large-dimensional context that
will also be exploited in this paper.
3.1	Concentration of measure results
We start by a definition of the notion of concentration for a real random variable.
Definition 1 (Concentration of a Random Variable). Given a function δ : R+ → R+, a random
variable Z is said to be δ-concentrated (around its mean) and we write Z ∈ δ, if for all t > 0,
P {|Z - EZ| ≥ t} ≤ δ(t). In particular, Z is said to be normally (resp., exponentially) concentrated
when δ(t) = Ce-Ctt (resp., δ(t) = Ce-Ct) and we write Z ∈ CN(C ∙) (resp., Z ∈ CE(c ∙)), where
C, c > 0 are some absolute constants.
In particular, δ-concentration remains stable by application of Lipschitz functions:
Proposition 1 (Concentration of Lipschitz Functions). Given a λ-Lipschitz function f : R → R and
a concentrated random variable Z ∈ δ, we have f (Z) ∈ δ (∙∕λ).
As a consequence, linear combinations of δ-concentrated random variables remain concentrated.
However, products of δ-concentrated random variables are more technical to handle, but we still have
the following proposition in the case of normally concentrated random variables and which will be
essential in this article.
Proposition 2 (Square of Normally Concentrated Random Variables). GiVen Z ∈ CN(c∙), the
random variable Z2 is exp-normally concentrated, precisely
Z 2 ∈ KC E (C ) + KCN (16E[Z]2 ),	⑴
where KC > 0 is a constant depending only on C.
The extension of the notion of concentration to random vectors Z ∈ Rp demands that Rp → R
Lipschitz functions are concentrated random variables.
Definition 2 (Concentration of a Random Vector). Given a function δ : R+ → R+ and a normal
space (E, k.k), a random vector Z ∈ E is said to be δ-concentrated if for any 1-Lipschitz function
f : E → R, the random variable f(Z) is δ-concentrated. We note again Z ∈ δ.
In particular, we have the concentration of Gaussian random vectors in the sense of Definition 2 in
the following proposition (Tao, 2012, Theorem 2.1.12).
Proposition 3 (Normal Concentration of Gaussian Random Vectors). A Gaussian vector Z ∈ Rp,
with independent and identically distributed N (0, 1) entries, is normally concentrated independently
on the dimensionP. Furthermore, Z ∈ 2N (∙∕2).
Remark 1. According to Definition 2, given a Lipschitz application F : Rp → Rq for q ∈ N*,
Proposition 3 provides the normal concentration of all the random vectors F(Z). In particular, note
that our results are extensible to this family of vectors and random vectors with independent entries.
3
Published as a conference paper at ICLR 2019
3.2	ε-SPARSE matrices
When considering a large-dimensional random matrix setting, the notion of sparsity for such matrices
is particularly attached to the choice of the matrix norm.2 (El Karoui, 2008) introduced a definition
(ε-sparsity) for sparsity of matrices that is compatible with spectral analysis, and specifically adapted
to the operator norm. The ε-sparsity definition requires some notions from graph theory that we
present in the following: to each p × p symmetric matrix M , we define its corresponding adjacency
matrix as A(M) = {IMij=0}Pj=1, which corresponds to a graph Gp with P vertices. A walk is said
to be closed on this graph if it starts and finishes at the same vertex and the number of edges traversed
by a walk defines the length of this walk. Denote Cp(k) the set of closed walks of length k on Gp.
Definition 3 (ε-sparse matrices (El Karoui, 2008, Definition 1)). A sequence of covariance matrices
{Σp}p∞=1 is said to be ε-sparse if the sequence of their associated graphs {Gp}p∞=1 satisfies, for all
k ∈ 2N, |Cp(k)| ≤ Ck pε(k-1)+1 where ε ∈ [0, 1], Ck > 0 independent of p and |S| denotes the
cardinality of the set S.
The ε-sparsity is both useful and convenient to out study for the following reasons: 1) it is adapted to
the analysis of the operator norm of large sparse matrices (as we give concentration results on the
operator norm); 2) it is also more general than other sparsity notions such as in (Bickel & Levina,
2008). In the latter, the authors developed a natural permutation-invariant notion of sparsity which is
more specific than Definition 3 as pointed out in the introduction of their article. Furthermore, note
that both sparsity notions (Definition 3 and the one in (Bickel & Levina, 2008)) provide equivalent
bounds for ε < 2 and when considering the large dimensional P 〜n setting (see subsection
2.4 in (Bickel & Levina, 2008)); this is precisely the setting considered in Corollary 2 introduced
subsequently (cf. μ > 0).
Remark 2. As Definition 3 is based on a graph defined by its corresponding adjacency matrix, we
have the following property: given an ε-sparse matrix M and a function f such that f (0) = 0
and f(x) 6=x6=0 0, the matrix f (M), resulting from the application of f entry-wise to M, remains
ε-sparse; this is simply a consequence ofA(M) = A(f (M)).
4	Main Results
In this section, we first present the setting of the article. Then, we provide an asymptotic equivalent
to the matrix f (C). Finally, we treat as a special case the application of our result to the context of
sparse PCA.
4.1	General Setting and Main Results
Consider a data matrix Y ∈ Rp×n defined as
γ ≡ ∑p∕2χ = (Ip + P )1/2 χ,	(2)
where X ∈ Rp×n is a random matrix with i.i.d. N(0,1) entries, P = Pk=1 ωi%u∣ and
U = [u1, . . . , uk] ∈ Rp×k is isometric. Here, k refers to the number of principal components
(or eigenvectors) u1, . . . , uk ∈ Rp to be evaluated, with ω1 > . . . > ωk > 0 the corresponding
eigenvalues respectively. We define the quantity3 βp ≡ maxi ∣∣ [Σp∕2].,ik.
Assumptions: There exists B > 0 independent of p, n such that maxi7-1 [Σp ]j | < B. Besides, there
exists e > 0 such that βp ≤ B0 n 1 -e for all p, n and for some absolute constant B0 > 0.
Under these assumptions, our main technical result is as follows:
2Considering the identity matrix (which is a sparse matrix), kIpkop = 1 while IlIpkF = √p → ∞.
3The role of βp is to ensure the concentration of the quadratic form in equation 4 introduced subsequently.
When Σp is a sparse matrix, βp plays the same role as the maximum spike strength in the bounds given in
(Deshpande & Montanari, 2016) for the CT method.
4
Published as a conference paper at ICLR 2019
Theorem 1 (Asymptotic Equivalent). For f a three-times continuously differentiable function, define
the matrices F and F respectively by4
F -Iff[1 γγτ] !) F — f(∑、* X f(k) ②P) G [∑ι∕2 ( 1XX। I ʌ ∑ι∕21θk
F≡ yUn	]iJ)τ,F≡f(∑p) + ʌ~^τ~[ςp Uxx -Ip) P ∖ .
Then for η > 0 and for an absolute constant C > 0, we have with probability at least 1 - η
β6 p
l|F - F∣∣op ≤ C j3^.	(3)
n3/2 η
For a general smooth function f, the kernel random matrix f (C) is particularly difficult to analyze
through the usual tools of random matrix theory, such as the moment or Stieltjes transform-based
methods (Tao, 2012). Rather than directly analyzing such a kernel random matrix, Theorem 1
gives an asymptotic equivalent to it, in operator norm, that has mainly two properties. First, the
approximation matrix F contains “simple” objects that have already been analyzed in random matrix
∙-v
theory - in particular, the term (XXl∕n 一 Ip) in the expression of F. Second, the approximation
in operator norm implies (by Weyl’s inequality (Eisenstat & Ipsen, 1998, Theorem 4.1)) that, when
∙-v	∙-v
kF - F kop → 0, F and F have the same eigenvalues and same “isolated” eigenvectors asymptotically
(see Corollary 2 subsequently).
Sketch of Proof of Theorem 1. The main idea of the proof relies on the following intuition: for large
n, the entries of XXl/n - Ip and its successive Hadamard products tend to zero at controllable rate.
The concentration of measure framework then allows for the control of non-linear functions of the
entries of XXl/n - Ip. Of utmost importance to this end is the following lemma.
Lemma 1 (A Concentration Result). For all i, j ∈	[p], the bilinear form gij (X) ≡
[∑p∕2]i,∙ (1XXl 一 Ip) [Σp∕2]∙,j Satisfies
% (X) ∈ KE (r)+KN (E)	(4)
for some absolute constants c1, c2 , K > 0.
1∕2
Proof. Denoting by vi the i-th column vector of the matrix Σp , we have by the polarization identity,
for all M Hermitian, v：Mvj = 4 [(vi + Vj)lM(Vi + Vj) 一 (Vi 一 Vj)lM(Vi 一 Vj)]. It thus suffices
to prove the result for the quadratic form g(X) = vi (1XXl 一 Ip) V where V ∈ Rp. Noticing that
VlXXiv = ∣∣viX∣∣2 and E [1 viXXiv] = viv, we need to prove the concentration of the random
variable ∣∣viX∣∣2. In fact, since viX is a Gaussian vector, by Proposition 3, ∣viX∣∣ ∈ 2N (乖P)
by Remark 1 and by Definition 2 since M 7→ VlM and u 7→ ∣u∣ are respectively ∣V ∣-Lipschitz and
1-Lipschitz functions. We get the final result by Proposition 2.	□
A Taylor expansion of F around f(Σp) then leads to controlling the operator norm of f(3)(ξn)
[Σp∕2(XXl∕n — Ip)Σp∕2]θ3 for ξn a matrix with entries in the set [[YYl∕n]ij, [∑p]ij] (or
[[Σp]ij , [YYi /n]ij ]). This follows precisely from exploiting Lemma 1 twice, to control the fluctu-
ations of the entries of both ξn (by the conditions on maxj ∣[Σp]j | and βp) and [Σp∕2(XXl/n 一
Ip)Σp∕2]θ3, with the bound provided in the theorem statement, thereby completing the proof. □
A detailed proof of Theorem 1 is provided in Section A.2 of the Appendix. From now on, to simplify
our arguments, we make the following assumptions:
Assumptions: As n → ∞,
A1 p/n → c ∈ (0, ∞),	A2 lim supn maxi ωi < ∞; specifically lim supn βp < ∞.
Under this setting, we have the following important corollary to Theorem 1.
4f and f(k) are applied entry-wise and k stands for the element-wise k-th power.
5
Published as a conference paper at ICLR 2019
∙-v
Corollary 1. Define the matrices F and F as in Theorem 1 and Assumptions A1 and A2 hold. Then,
for η > 0
1
F = F + On (n-2),	(5)
where the notation X = Oηm (n-α) stands for the fact that P
some absolute constant C > 0 and non-negative integer m.
{kXlIop ≥ Cn-α η-2m} ≤ η for
As a consequence of Corollary 1, we have, by the sin(Θ) theorem of (Davis & Kahan, 1970), the
∙-v
corollary below concerning the eigenvectors of the matrices F and F.
Corollary 2. Let vι,..., Vk and Vι,..., Vk denote respectively the k principal eigenvectors of F
∙-v
and F. Denote by ∆i = ωi - ωi+1 for i ∈ [k - 1]. Then for η > 0, we have
max min	∆2 ∣∣Vi - SViIl2 =On(n-1).	(6)
i∈[k] s∈{+1,-1}
4.2 Special Case: Sparse PCA
To get an insight on our coming results, consider the scenario where U contains finitely many
non-zero entries. In this case, the perturbation matrix P in Eq. equation 11 contains finitely many
non-zero entries (say s) on each line and a simple enumeration shows that |Cp(k)| ≤ p sk-1, thus P
is 0-sparse in the sense of Definition 3. Similarly, Ip is 0-sparse and by the additive stability5 of the
ε-sparsity notion, Σp remains 0-sparse. More generally, if We assume that it exists ε ∈ [0, 2) such
that the population covariance matrix Σp is ε-sparse, we have the following set of consequences. By
Corollary 1, choosing f in such a Way that f0(0) = f00(0) = 0 ensures that the terms f0(Σp)	. . .
∙-v
and f00(Σp)	. . . vanish in the expression of F. Indeed, for k ∈ {1, 2}
(i)	Only finitely entries of f(k) (Σp) do not vanish, precisely by Remark 2, since A(f (k) (Σp)) =
A(Σp),6 the matrix f(k) (Σp) is also (almost) ε-sparse.
(ii)	The matrix F(k) = [∑p/2 (ɪXX| — Ip) Σp/2] θ has entries of order O(n-k/2). As a
result,7 We have for η > 0 and for all m > 0, max∙i,j |Fjk)| =Om (n-2+ *).
Since in addition the operator norm of Σp∕2(XX|/n — Ip)Σp/2 is typically of order O(1) (see
e.g., (Bai & Silverstein, 1998)), it is then easily seen that, for each k ≥ 1, the operator norm of the
Hadamard product f(k)(Σp) F(k) vanishes (see Lemma 5 in Appendix A). In particular, note that
the non-zero entries of Σp are controlled through the maximum entry of F(k) Which is vanishing
asymptotically, as mentioned in item (ii) above. On the opposite f(Σp) does not vanish since it has
entries bounded aWay from zero (as long of course as f 6= 0). We precisely have the folloWing result.
Theorem 2. Let μ > 0 and suppose Σp is a 2++μ -sparse matrix. For f a three-times continuously
differentiable function andfor η > 0, we havefor all E ∈ (0, 2(3,2*))
F = f(∑p) + on1/" (n2(-+μ) +e(2-2+1μ)) s.t. f0(0) = f00(0) = 0.	⑺
Proof. See Section A.3 in Appendix A. (See Corollary 1 for the notation Om(.)).	□
Remark 3. Theorem 2 gives a general result concerning the estimation of ε-sparse covariance
matrices (more precisely, element-wise functionals of sparse covariance matrices). In particular, the
SPiked model in Eq. equation 11 with U sparse corresponds to the particular case when μ → ∞; in
this case, for η > 0 andforall E ∈ (0,1/4), F = f (Σp) + O；1/'」(n-1 +2e).
5See Fact .1 in (El Karoui, 2008).
6Given M ∈ Rp×p, its corresponding adjacency matrix is defined as A(M) = {Mij=≠}p-j==1∙
7See proof of Lemma 5 in Appendix A for a proof of this result.
6
Published as a conference paper at ICLR 2019
ytisneD ytisneD
01234
Eigenvalues
1
0.5
3
57
9
0
1
ʌ ʌ
Figure 1: (Left) Spectrum of C (Up) and f (C) (bottom) for P = 2048 and n = 7500. Limiting
Marcenko-Pastur density (Marcenko & Pastur, 1967) in blue versus spectrum of Σp in black, with
ω1 = 2; estimated largest eigenvalue in red. (Right) Alignment between estimated PC and GT (the
“Three Peak" example of (Johnstone & Lu, 2009) in the “Symmlet 8" wavelet basis), in terms of pp/n.
We considered ωι = 5 and thus the phase transition for standard PCA occurs at pp/n = 5, thereby
suggesting another phase transition for f -PCA. Curves obtained from 500 realizations of X .
----t
一f (t)
---- f'(t)
i∙∙∙f 〃⑴
One may then perform a PCA on F for some function f with f0(0) = f00(0) = 0 (we denote by
f1,2(0) = 0 these two conditions in the following). But, while Σp = Ip+P is alow rank perturbation
of the identity (therefore having only k eigenvalues strictly greater than 1), f(Σp) is likely more
complex and not a mere low rank deformation of the identity. Now, if Σp has all its non-zero entries
greater than a certain threshold τ , an appropriate choice for f that avoids the deformation of Ip + P
is such that f1,2(0) = 0 and f(t) = t for all |t| > τ.
Such a convenient choice is	2
f(t)=t(1-e-at2),	(8)
for some a > 0. This function notably satisfies 0
f0(t)=1+e-at2(2at2-1)⇒f0(0)=0,	-2
f 00(t) = -2αte-αt2 (2at2 - 3)⇒ f0θ(0) = 0.
The figure above depicts the function f along with its derivatives for a = 1. Note that a compromise
in the choice of a must be made that both maintains a close approximation of the identity by f on
a large range and rather small values of f00 in the vicinity of zero. Interestingly, it can be verified
that the extrema of f are independent of a but are found at ±qζ2a and thus smaller values of a create
sharper f0 in the vicinity of zero. Similarly, the extrema of f00 are found at 士q 3±√√6 H 1/√a, and
precisely given by the four values 2,3a(3 ± √6)e-2(3±√6) H √a. Thus smaller a induce larger maxima
for f00 but no sharper slope.
5	Experiments
In this section, we provide some experiments in the context of sparse PCA, where we consider the
spiked model presented in Section 4.1. Precise setting given in caption of Figure 1. The spectrum
of the sample covariance matrix (in gray) is quite different from that of Σp . One instead observes
a “bulk" of eigenvalues spread in the vicinity of 1. Furthermore, one observes a gap between the
true spike and the estimated spike (in red) through the sample covariance matrix. This phenomenon
is well-understood in random matrix theory. In particular, the extreme eigenvalue in our setting
converges almost surely to the quantity(1 + ωι) (1 + ^^), where we recall that C = limnp/n.
However, thanks to sparsity, the spectrum of F = f(C) closely matches that of Σp, as suggested by
Theorem 2. In particular, the extreme eigenvalue, which corresponds to the principal component,
is consistently estimated. Figure 1 (right) depicts the alignment between the estimated principal
7
Published as a conference paper at ICLR 2019
00
0
,
1
00
0
,
2
00
0
,
1
00
0
,
2
00
0
,
1
00
0
,
2
)sruo(ACP-f
00
0
,
1
00
0
,
2
0
0
0
0
0
Figure 2: Principal component recovery (in orange) by standard PCA (up) and our method (down)
for the “Three Peak” example of (Johnstone & Lu, 2009). The signal is sparse in the “Symmlet 8”
wavelet basis. We use p = 2048, ω1 = 5 for the strength of the spike and different values of n.
component and ground truth, by standard PCA (in blue) and our method (in black), in terms of ,p/n.
Our method retrieves the principal component even when the spike is not visible in the spectrum
of C; namely beyond the phase transition √p∕n ≥ ωι. In fact, the standard PCA result is too noisy
ʌ
compared with the one when considering f (C), as depicted in Figure 2. Further detailed examples
are provided in Section A.4 of Appendix A, that confirm the consistency of the proposed method.
In terms of complexity, as our method consists in computing the sparse eigenvectors of a p × p
matrix which can be done by power method, the complexity of estimating the principal component
is about O(ps) where s is the sparsity level. And regarding the performance w.r.t. state-of-the-
art methods, Figure 3 depicts the performances of standard PCA, different state-of-the-art sparse
PCA methods and our method, in terms of total projections score (left) and total projections error
(right), for different values of the amplitudes ω∕s. We refer, in this figure, to standard PCA as PCA,
TpowPCA for the method in (Yuan & Zhang, 2013), ITSPCA for the method in (Ma et al., 2013),
CT refers to the method in (Deshpande & Montanari, 2016) and finally we refer to our method as
f-PCA. The total projections score S and error E are given respectively by S = ɪ P：=i(u|Ui)2
and E = ∣∣UU1 - U U TkF, where U = [uι,...,uk ] are the ground truth principal components and
U = [Uι,...,Uk] are the estimated ones.
As suggested theoretically and verified experimentally, our proposed method strongly attenuates the
“noise component” of the sample covariance matrix and thus consistently estimates the principal
components. In particular, in term of total projections score, PCA is the most inconsistent. In general,
ITSPCA, CT and our method give equivalent results. The same holds when considering the total
projections error as a metric, except that TpowPCA performs inconsistently, compared to PCA, for
small values of amplitudes due to the initialization step from the PCA eigenvectors.
Figure 3: Performances of standard PCA, different state-of-the-art sparse PCA methods and our
method in term of total projections score (left) and total projections error (right) for different values
of the amplitudes ωi . The PCs ui , for i ∈ [4] are the “Three Peak”, “Piece Poly”, “Step New” and
“Sing” signals of (Johnstone & Lu, 2009). We use p = 2048 and n = 1024. The soft-parameters a
and τ (respectively for our method and CT) are selected by cross-validation using a validation set of
size n. The selected parameters are a = 20 and τ = 0.1.
8
Published as a conference paper at ICLR 2019
The mostly used concurrent methods to PCA in a sparse context are iterative truncated power methods
(such as the TPower (Yuan & Zhang, 2013) algorithm or the ITSPCA (Ma et al., 2013) approach).
These algorithms, despite great observed performances, as compared to standard PCA, suffer from
two limitations. First, they are usually initialized from the PCA eigenvectors themselves and may
not converge to good estimates. For weak signals, PCA is so impacted by noise that the mentioned
initialization limitation may lead to non convergent or dramatically erroneous outcomes of the
method. The proposed approach deals precisely with this limitation by strongly attenuating the “noise
component” of the sample covariance matrix. In particular, our approach gives equivalent results to
the CT method while generalizing it to the class of smooth functions f such that f0 (0) = f00(0) = 0,
in the considered regime. The second limitation concerns the choice of the hyper-parameters; in fact,
TPower and ITSPCA need to set up an arbitrary deterministic threshold value that maintains at each
iteration step only most powerful components. The proposed method as well as CT need also to set
up a “soft” parameter (a and τ respectively). But, on the basis of (Cheng & Singer, 2013; Kammoun
& Couillet, 2017), we believe that our present investigation can be extended to the asymptotically
non-trivial setting where ω% = O(1/√p) (in which case the dominant eigenmodes scale at a similar
rate with residual noise); this setting may likely allow to exhibit and estimate optimal hyper-parameter
choices. Notably, this setting has already been used in (Tiomoko Ali et al., 2018) in a different
context, for hyper-parameters estimation.
6	Conclusion
In this paper, we tackled the problem of sparse PCA through a random matrix perspective thereby
generalizing recent ideas to a broader kernel-based method. Our analysis of this problem has yielded
insights into how the principal components can be consistently estimated. Namely, given a spiked
ʌ
covariance model C and a smooth function f , we gave in this paper sufficient conditions on f to
ʌ
consistently estimate the principal components through the matrix f (C). Our methodology can be
generalized to other sparse covariance matrix-based contexts, in the same vein as the works in (Bickel
& Levina, 2008; El Karoui, 2008).
References
Theodore Wilbur Anderson. Asymptotic theory for principal component analysis. The Annals of
Mathematical Statistics, 34(1):122-148,1963.
Megasthenis Asteris, Dimitris Papailiopoulos, and Alexandros Dimakis. Nonnegative sparse pca with
provable guarantees. In International Conference on Machine Learning, pp. 1728-1736, 2014.
Zhi-Dong Bai and Jack W Silverstein. No eigenvalues outside the support of the limiting spectral
distribution of large-dimensional sample covariance matrices. Annals of probability, pp. 316-345,
1998.
Jinho Baik, Gerard Ben Arous, Sandrine Pecha et al. Phase transition of the largest eigenvalue for
nonnull complex sample covariance matrices. The Annals of Probability, 33(5):1643-1697, 2005.
Florent Benaych-Georges and Raj Rao Nadakuditi. The eigenvalues and eigenvectors of finite, low
rank perturbations of large random matrices. Advances in Mathematics, 227(1):494-521, 2011.
Peter J Bickel and Elizaveta Levina. Covariance regularization by thresholding. The Annals of
Statistics, pp. 2577-2604, 2008.
Jorge Cadima and Ian T Jolliffe. Loading and correlations in the interpretation of principle compenents.
Journal of Applied Statistics, 22(2):203-214, 1995.
Mireille Capitaine, Catherine Donati-Martin, and Delphine Feral. The largest eigenvalues of finite
rank deformation of large wigner matrices: convergence and nonuniversality of the fluctuations.
The Annals of Probability, pp. 1-47, 2009.
Xiuyuan Cheng and Amit Singer. The spectrum of random inner-product kernel matrices. Random
Matrices: Theory and Applications, 2(04):1350010, 2013.
9
Published as a conference paper at ICLR 2019
Alexandre d’Aspremont, Laurent E Ghaoui, Michael I Jordan, and Gert R Lanckriet. A direct
formulation for sparse pca using semidefinite programming. In Advances in neural information
processing Systems, pp. 41-48, 2005.
Chandler Davis and William Morton Kahan. The rotation of eigenvectors by a perturbation. iii. SIAM
Journal on Numerical Analysis, 7(1):1-46, 1970.
Yash Deshpande and Andrea Montanari. Sparse pca via covariance thresholding. In Advances in
Neural Information Processing Systems, pp. 334-342, 2014.
Yash Deshpande and Andrea Montanari. Sparse pca via covariance thresholding. J. Mach. Learn.
Res., 17(1):4913-4953, January 2016. ISSN 1532-4435.
Stanley C Eisenstat and Ilse CF Ipsen. Three absolute perturbation bounds for matrix eigenvalues
imply relative bounds. SIAM Journal on Matrix Analysis and Applications, 20(1):149-158, 1998.
Noureddine El Karoui. Operator norm consistent estimation of large-dimensional sparse covariance
matrices. The Annals of Statistics, pp. 2717-2756, 2008.
Noureddine El Karoui. On information plus noise kernel random matrices. The Annals of Statistics,
38(5):3191-3216, 2010a.
Noureddine El Karoui. The spectrum of kernel random matrices. The Annals of Statistics, 38(1):
1-50, 2010b.
DelPhine F6ral and Sandrine P6ch6. The largest eigenvalue of rank one deformation of large Wigner
matrices. Communications in mathematical physics, 272(1):185-228, 2007.
Iain M Johnstone and Arthur Yu Lu. On consistency and sparsity for principal components analysis
in high dimensions. Journal of the American Statistical Association, 104(486):682-693, 2009.
Abla J Kammoun and Romain Couillet. Subspace kernel clustering of large dimensional data.
(submitted to) Annals of Applied Probability, 2017.
Antti KnoWles and Jun Yin. The isotropic semicircle laW and deformation of Wigner matrices.
Communications on Pure and Applied Mathematics, 66(11):1663-1749, 2013.
Robert Krauthgamer, Boaz Nadler, and Dan Vilenchik. Do semidefinite relaxations solve sparse pca
up to the information limit? Ann. Statist., 43(3):1300-1322, 06 2015.
Michel Ledoux. The concentration of measure phenomenon. Number 89. American Mathematical
Soc., 2005.
Zongming Ma et al. Sparse principal component analysis and iterative thresholding. The Annals of
Statistics, 41(2):772-801, 2013.
Vladimir A Marcenko and Leonid Andreevich Pastur. Distribution of eigenvalues for some sets of
random matrices. Mathematics of the USSR-Sbornik, 1(4):457, 1967.
Baback Moghaddam, Yair Weiss, and Shai Avidan. Spectral bounds for sparse pca: Exact and greedy
algorithms. In Advances in neural information processing systems, pp. 915-922, 2006.
Dimitris Papailiopoulos, Alexandros Dimakis, and Stavros Korokythakis. Sparse pca through loW-
rank approximations. In International Conference on Machine Learning, pp. 747-755, 2013.
Debashis Paul. Asymptotics of sample eigenstructure for a large dimensional spiked covariance
model. Statistica Sinica, 17:1617-1642, 2007.
Haipeng Shen and Jianhua Z Huang. Sparse principal component analysis via regularized loW rank
matrix approximation. Journal of multivariate analysis, 99(6):1015-1034, 2008.
Terence Tao. Topics in random matrix theory, volume 132. American Mathematical Society
Providence, RI, 2012.
10
Published as a conference paper at ICLR 2019
Hafis Tiomoko Ali, Abla Kammoun, and Romain Couillet. Random matrix asymptotics of inner
product spectral clustering. IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), 2018.
Svante Wold, Kim Esbensen, and Paul Geladi. Principal component analysis. Chemometrics and
intelligent laboratory systems, 2(1-3):37-52, 1987.
John Wright, Arvind Ganesh, Shankar Rao, Yigang Peng, and Yi Ma. Robust principal component
analysis: Exact recovery of corrupted low-rank matrices via convex optimization. In Advances in
neural information processing systems, pp. 2080-2088, 2009.
Xiao-Tong Yuan and Tong Zhang. Truncated power method for sparse eigenvalue problems. Journal
of Machine Learning Research, 14(Apr):899-925, 2013.
Ron Zass and Amnon Shashua. Nonnegative sparse pca. In Advances in Neural Information
Processing Systems, pp. 1561-1568, 2007.
Hui Zou, Trevor Hastie, and Robert Tibshirani. Sparse principal component analysis. Journal of
computational and graphical statistics, 15(2):265-286, 2006.
A	Proofs and further experiments
In this appendix we provide the proofs of the different results presented in the paper and some
additional experiments that validate our findings. It includes the following items: (i) the proof of
Theorem 1 (Section A.2); (ii) The proof of Theorem 2 concerning the analysis of the sparse case
(Section A.3); and finally (iii) further experiments which confirm the consistency of our method and
the necessity of the conditions f0 (0) = f00(0) = 0 on the kernel function f, using the signals of
Johnstone et al. (Johnstone & Lu, 2009) (Section A.4).
For convenience, we make the present appendix self-contained by recalling the preliminaries and the
results presented in the main paper.
A.1 Preliminaries
Proposition 2 (Square of Normally Concentrated Random Variables). Given Z ∈ CN(c∙), the
random variable Z2 is exp-normally concentrated, precisely
Z2 ∈ KC E (C ) + KCN (16E[Z]2 ) ,	(9)
where KC > 0 is a constant depending only on C.
Definition 2 (Concentration of a Random Vector). Given a function δ : R+ → R+ and a normal
space (E, k.k), a random vector Z ∈ E is said to be δ-concentrated if for any 1-Lipschitz function
f : E → R, the random variable f(Z) is δ-concentrated. We note again Z ∈ δ.
Proposition 3 (Normal Concentration of Gaussian Random Vectors (Tao, 2012, Theorem 2.1.12)).
A Gaussian vector Z ∈ Rp, with independent and identically distributed N(0, 1) entries, is normally
concentrated independently on the dimension P. Furthermore, Z ∈ 2N (∙∕2).
Remark 1. According to Definition 2, given a Lipschitz application F : Rp → Rq for q ∈ N*,
Theorem 3 provides the normal concentration of all the random vectors F(Z).
Definition 3 (ε-sparse matrices (El Karoui, 2008, Definition 1)). A sequence of covariance matrices
{Σp}p∞=1 is said to be ε-sparse if the sequence of their associated graphs8 {Gp}p∞=1 satisfies, for all
k ∈ 2N
|Cp(k)| ≤ Ckpε(k-1)+1,	(10)
where ε ∈ [0, 1], Ck > 0 independent ofp and |S| denotes the cardinality of the set S.
Remark 2. As Definition 3 is based on a graph defined by its corresponding adjacency matrix, we
have the following property: given an ε-sparse matrix M and a function f such that f (0) = 0
and f(x) 6=x6=0 0, the matrix f(M), resulting from the application off entry-wise to M, remains
ε-sparse; this is simply a consequence of A(M) = A(f (M)).1
8Defined through the corresponding adjacency matrix to Σp ; given an p × p real symmetric matrix M, its
corresponding adjacency matrix is defined as A(M) = {lMij=o}p j=ι.
11
Published as a conference paper at ICLR 2019
A.2 Proof of Theorem 1
Setting: Consider a data matrix Y ∈ Rp×n defined as
Y ≡ Σp1∕2X = (Ip +P)1∕2X,
(11)
where X ∈ Rp×n with i.i.d. N(0,1) entries, P = Pk=1 ωiUiu|, U = [uι,...,uk] ∈ Rp×k
is isometric, k refers to the number of principal components and ω1 > . . . > ωk > 0. Let
βp ≡ maxi k[∑p∕2]∙,ik∙
Assumptions: There exists B > 0 independent of p, n such that maxi7-1 [Σp ]j | < B. Besides, there
exists e > 0 such that βp ≤ B0 n 1 -e for all p, n and for some absolute constant B0 > 0.
Under these assumptions, we have
∙-v
Theorem 1.	For f a three-times continuously differentiable function, define the matrices F and F
respectively by
p
F ≡ f(1YY|
n
f
2
F ≡ f(∑p) + X
k=1
f(k)(Σp)
k!
ij!)
i,j=1
® 卜 p/2 (1XXI- Ip)∑∕2
1YY |
n
Then for η > 0 and for an absolute constant C > 0, we have with probability at least 1 - η
β6 p
l∣F - F∣∣op ≤ C -pjf^.
n3∕2 η
(12)
Proof. Before starting the proof, we need to introduce the following key lemmas:
Lemma 1 (A Concentration Result). For all i, j ∈ [p], the bilinear form gij (X) ≡
[∑p∕2]i,∙ (1XX| - IP) [Σp∕2]∙,j satisfies
gj(X) ∈ KE (r)+KN (E
for some absolute constants c1, c2 , K > 0.
(13)
Proof. Denoting by vi the i-th column vector of the matrix Σ1p∕2, we have by the polarization identity,
for all M Hermitian, v：Mvj = 1 [(vi + Vj)lM(Vi + Vj) — (Vi — Vj)lM(Vi — Vj)]. It thus suffices
to prove the result for the quadratic form g(X) = vi (1XXl - IP) V where V ∈ Rp. Noticing that
VlXXiv = ∣∣viX∣∣2 and E [1 viXXiv] = viv, we need to prove the concentration of the random
variable ∣∣viX∣∣2. In fact, since viX is a Gaussian vector, by Proposition 3, ∣viX∣∣ ∈ 2N (列中)
by Remark 1 and by Definition 2 since u 7→ ∣u∣ and M 7→ ViM are respectively 1-Lipschitz and
□
kvk-Lipschitz functions. We get the final result by Proposition 2.
Lemma 2 (A Moment Result). For gj(X) ≡ [Σp∕2]i,. (1XX1 - Ip) [Σp/2]∙,j, we have, for all
k ∈ N and for some absolute constant Ck > 0,
β4k
E∣gij(X)∣2k ≤ Ck A.
(14)
Proof. Given a random variable Z, we have
∀m> 0, E|Z∣m = ∞ mtm-1 P{∣Z∣ ≥ t}dt,
0
whenever the right hand side is finite. Applying this identity to the random variable gij (X) with
m = 2k and exploiting the concentration property in Lemma 1 yields the result.
□
12
Published as a conference paper at ICLR 2019
The proof starts by a Taylor expansion of Fij in the vicinity of [Σp]ij, i.e.,
2
Fij = X
k=0
f(k)(σij)
k!
j+f36ξn) Fj3)
where σij = [Σp]ij,ξinj ∈ b[Y Y |/n]ij, σijc,9 10 and F(k) is the matrix with entries
Fjk) ≡ [∑y2(n-1xx| - ip)∑p/2]j = gj(X)k.
We have by Lemma 1 that [Y Y |/n]ij concentrates around σij, so that ξinj is bounded by σij + ε,
for all ε > 0, with high probabilityɪɔ (note that the condition maxj ∣σj∣ < B ensures that σj is
bounded and the condition on βp ensures the quasi-exponential concentration of [YY |/n]ij around
σij ; see considered Assumptions above), formally
2
-β2 min(CIε, 2	)
βp	βp
P {∣ξjl ≥ σj + ε} ≤ P{∣gj(X)| ≥ ε} ≤ Ke
≤ Ke-K n2+2« min(CIε,K0c2ε2n-2+2e) ≡ pn → 0
where K0 > 0. And since f(3) is continuous, we deduce that f(3) (ξinj) is in particular bounded by
A ≡ max	|f (3) (x)|,
x∈[σij -ε,σij +ε]
with probability 1 - pn . Knowing that the operator norm is bounded by the Frobenius norm, we look
for a control of the Frobenius norm of the tailing term. We have
kf(3)(ξn)F(3)k2F ≤A2kF(3)k2F.	(15)
By Lemma 2, for all k ∈ N
EkF (k)k2F
p	p2 β4k
X E[∣gj(X产]≤ Ck力,
i,j=1
for some absolute constant Ck > 0. Thus, by Markov’s inequality, we have for all η > 0
P kF (k)kF
Recalling Eq. equation 15, we have with probability at least 1 - η
kf ⑶(ξn) © F ⑶ kF ≤ CJpfiμ
n 2 ʌ/n
A.3 Proof of Theorem 2
□
Assumptions: As n → ∞,
A1 p/n → c ∈ (0, ∞).
A2 lim supn maxi ωi < ∞; specifically lim supn βp < ∞.
With these assumptions, we have the following corollary to Theorem 1.
Corollary 1. Define the matrices F and F as in Theorem 1 and let Assumptions A1 and A2 hold.
Then, for η > 0
1
F = F + On (n-2 )	(16)
where the notation X = Oηm (n-α) stands for the fact that P
some absolute constant C > 0 and non-negative integer m.
{kXlIop ≥ Cn-α η-2m} ≤ η for
9The notation ba, bc stands for the interval [a, b] if a < b or [b, a] otherwise.
10For a given asymptotic variable n, we say that an event En occurs with high probability when it exist a
function ψ(n) quasi-exponentially decreasing in n such that P{En} ≥ 1 - ψ(n).
13
Published as a conference paper at ICLR 2019
Theorem 2.	Let μ > 0 and suppose Σp is a 2++μ -sparse matrix. For f a three-times continuously
differentiable function andfor η > 0, we havefor all E ∈ (0, 2(3,2*))
F = f (Σp) + Oη1∕ec (n2(-μμ) +e(2-2+μ)) s.t. f0(0) = f00(0) = 0.	(17)
Proof. The proof needs the introduction of the following two lemmas, that can be found in (El Karoui,
2008, Lemma A.1 and A.2) and which are a consequence of the ε-sparsity notion11
Lemma 3. Given an ε-sparse p×p real symmetric matrix M and calling m = maxij |Mij |, we have,
forallk∈ 2N
kM kop ≤ trace(Mk)1∕k = O(mpε(1-1∕k)+1∕k).	(18)
Lemma 4. Given two real symmetric matrices M and N with |Mij | ≤ Nij . Then, we have
kMkop≤ kNkop.
First, we show that when Σp is ε-sparse, the Hadamard product f(k)(Σp)	F(k) is of vanishing
operator norm for k ≥ 1, precisely
Lemma 5. Let μ > 0, suppose Σp is a 2+μ -sparse matrix. For f a real and differentiablefunction,
k ∈ {1, 2} such that f (k)(0) = 0 andfor η > 0, we haveforall E ∈ (0, k(j++)-)2)
kf(k)(∑p) Θ F(k)kop = Ob1∕ec (n2-k++μ' +e(2-2+μ)).
Proof. We start by proving that the matrix F(k) has entries of order O(n-k∕2). In fact, we have by
Lemma 2, for all m ∈ N*
E|Fi(jk)|2m =E|gij(X)|2km = O(n-km),
thus applying Markov’s inequality to the random variable |Fi(jk) |2m yields to the following tail control.
E|F (k)|2m
P{∣Fijk)l ≥ t}≤	tijm	≤ Cnim t-2m,
where C is an absolute constant. Recalling Assumption A1 and by the union bound, we have
p
P{miax|Fi(jk)| ≥t} ≤	P{|Fi(jk)| ≥t} ≤ p2 P{|Fi(jk)| ≥t} ≤ C n2-km t-2m,
j	i,j=1
which implies for η > 0 and for all m > 0
miaj x ∣Fijk)∣ =Om (n-2 + m )	(19)
Besides, let M be the matrix defined as M ≡ maxij∙ ∣Fijk) | ∙ f(k) (Σp), We have
|[f (k)(∑p) Θ F(k)]ij | ≤ Mij,
thus, one has by Lemma 4
kf (k)(∑p) Θ F(k)kop ≤ kMkop = max |Fjk)| ∙ kf (k)(∑p)kop.
ij
In particular, since f (k)(∑p) is 2+μ-sparse (by Remark 2), we have by Lemma 3 and by equation 19,
for some η > 0
kf(k)(∑p) Θ F(k)∣∣op =Onm (n2+μ(I-2mm)+2mm-2 + 2mm),
choosing E =焉 < kj；3+)-)2 yields the final result.	□
When considering f such that f0(0) = f00 (0) = 0, the result holds by Corollary 1 and Lemma 5. In
fact, the dominant order corresponds to k = 1 in Lemma 5. Which completes the proof. □
11Through the identity trace(Mk) ≤ maxj |Mj|k ∙ ∣Cp(k)∣.
14
Published as a conference paper at ICLR 2019
A.4 Further Experiments
A.4. 1 Higher Rank Case
In this section, we provide further experiments by considering a rank three case and by using the
“Three Peak”, “Piece Poly” and “Step New” signals of Johnstone et al. (Johnstone & Lu, 2009), in
the “Symmlet 8” wavelet basis, as principal components. We compare the estimated PCs by our
method with the kernel function in equation 8 to the estimated ones through standard PCA and the
CT method (Deshpande & Montanari, 2016). As shown in Figure 4, the proposed method retrieves
consistently the principal components compared to a standard PCA. In particular, we obtain results
that are similar to the ones obtained by the CT method while generalizing it to the class of smooth
functions with f0(0) = f00(0) = 0.
(N
U
d
f-PCA (ours)
0	1,000	2,000
Figure 4: Multiple principal components (k = 3) recovery (in orange) with standard PCA (left), the
CT method (middle) and our method (right) where the PCs are considered to be the “Three Peak”,
“Piece Poly” and “Step New” signals of (Johnstone & Lu, 2009), in the “Symmlet 8” wavelet basis.
We use p = 2048, n = 1024 and the spikes strengths are set respectively as ω1 = 100, ω2 = 75 and
ω3 = 50. In particular, we note the similarity between the results obtained by our method and CT.
15
Published as a conference paper at ICLR 2019
A.4.2 Other choices of the kernel function f
In this section, we consider functions of the form f (t) = αt3 + βt2 + γt where α, β, γ ∈ R are some
parameters to fix in order to allow or not the conditions f0(0) = f00(0) = 0. In particular, we set
different parameters choices for α, β and γ in order to validate these conditions. Figure 5 depicts
different PC recovery using the f -PCA method with the considered class of functions. As we can
observe from this figure, the “cleanest” signal recovery is obtained when α 6= 0, β = 0, γ = 0 (i.e.,
when f0(0) = f00(0) = 0) thereby validating our theoretical conditions on the kernel function f for a
consistent sparse PCA recovery. Note that these conditions are necessary but not sufficient in the
sense that f has to be linear for large values of t (In particular, this is the case for the function f given
by equation 8). In fact, the outcome provided by f -PCA for f(t) = αt3 with α 6= 0 is not optimal as
the obtained signal is deformed (due to the unverified linearity condition), compared to the GT one.
α = 0, β = 0,γ = 0
0	1,000	2,000
α = 0, β = 0, γ = 0
0	1,000	2,000
α = 0, β = 0, γ = 0
0	1,000	2,000
Figure 5: PC recovery (in orange) by f -PCA with the function f (t) = αt3 + βt2 + γt for different
values of the parameters (α, β, γ) ∈ R3 . We consider the “Three Peak” example of (Johnstone & Lu,
2009) which is sparse in the “Symmlet 8” wavelet basis. We use p = 2048, n = 256 and ω1 = 5. In
particular, we notice that the “cleanest” signal is obtained when α 6= 0, β = 0, γ = 0 which validate
our theoretical conditions f0(0) = f00(0) = 0.
16