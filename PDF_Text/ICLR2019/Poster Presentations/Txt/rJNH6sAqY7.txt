Published as a conference paper at ICLR 2019
On Computation and Generalization of Gener-
ative Adversarial Networks under Spectrum
Control
Haoming Jiang, Zhehui Chen, Minshuo Chen & Tuo Zhao *
School of Industrial and Systems Engineering
Georgia Institute of Technology
Atlanta, GA 30318, USA
{jianghm, zhchen, mchen393, tourzhao}@gatech.edu
Feng Liu & Dingding Wang
Department of Computer & Electrical Engineering and Computer Science
Florida Atlantic University
Boca Raton, FL 33431, USA
{fliu2016, wangd}@fau.edu
Ab stract
Generative Adversarial Networks (GANs), though powerful, is hard to train. Sev-
eral recent works (Brock et al., 2016; Miyato et al., 2018) suggest that controlling
the spectra of weight matrices in the discriminator can significantly improve the
training of GANs. Motivated by their discovery, we propose a new framework
for training GANs, which allows more flexible spectrum control (e.g., making the
weight matrices of the discriminator have slow singular value decays). Specifically,
we propose a new reparameterization approach for the weight matrices of the
discriminator in GANs, which allows us to directly manipulate the spectra of the
weight matrices through various regularizers and constraints, without intensively
computing singular value decompositions. Theoretically, we further show that the
spectrum control improves the generalization ability of GANs. Our experiments
on CIFAR-10, STL-10, and ImgaeNet datasets confirm that compared to other
methods, our proposed method is capable of generating images with competitive
quality by utilizing spectral normalization and encouraging the slow singular value
decay.
1	Introduction
Many efforts have been recently devoted to studying Generative Adversarial Networks (GANs, Good-
fellow et al. (2014)). GANs provide a general unsupervised framework to learn a generative model
from unlabeled real data. Successful applications of GANs include many unsupervised learning
tasks, such as image generation, dialogue generation, and image inpainting (Abadi & Andersen,
2016; Goodfellow, 2016; Ho & Ermon, 2016; Li et al., 2017; Yu et al., 2018). Different from other
unsupervised learning methods, which directly maximize the likelihood of deep generative models
(e.g., Variational Auto-encoder, Nonlinear ICA, and Restricted Boltzmann Machine), GANs introduce
a competition between two neural networks. Specifically, one neural network serves as the generator
that yields artificial samples, and the other serves as the discriminator that distinguishes the artificial
samples from the real data.
Mathematically, GANs can be formulated as the following min-max optimization problem:
1n
min∏wx f(θ, W) ：= ~ fφ (A(DW (Xi))) + Ex 〜Dgθ [φ(1 - A(Dw (x)))],	(1)
n i=1
where {xi}in=1 are n real data points, Gθ denotes the generative deep neural network parameterized by
θ, DW denotes the discriminative neural network parameterized by W, DGθ denotes the distribution
*Tuo Zhao is the corresponding author.
1
Published as a conference paper at ICLR 2019
generated by Gθ, φ(∙) : [0,1] → R is a properly chosen monotone function, and A(∙) denotes a
monotone function related to the function φ(∙). There have been many options for φ(∙) and A(∙) in
existing literature. For example, the original GAN proposed in Goodfellow et al. (2014) chooses
φ(x) = log(χ), A = ι+eχp(-χ); Arjovsky et al. (2017) use φ(χ) = x, A(X) = x, and (1) becomes
the Wasserstein GAN. Min-max problem (1) has a natural interpretation: The minimization problem
aims to find a discriminator DW, which can distinguish between the real data and the artificial
samples generated by Gθ, while the maximization problem aims to find a generator Gθ, which can
fool the discriminator DW . From the perspective of game theory, the generator and discriminator are
essentially two players competing with each other and eventually achieving some equilibrium.
From an optimization perspective, problem (1) is a nonconvex-nonconcave min-max problem, that is,
f(θ, W) is nonconvex in θ given a fixed W and nonconcave in W given a fixed θ. Unlike convex-
concave min-max problems, which have been well studied in existing optimization literature, there
is very limited understanding of general nonconvex-nonconcave min-max problems. Thus, most of
existing algorithms for training GANs are heuristics. Although some theoretical guarantees have been
established for a few algorithms, they all require very strong assumptions, which are not satisfied in
practice (Heusel et al., 2017).
Despite of the lack of theoretical justifications, significant progress has been made in empirical studies
of training GANs. Numerous empirical evidence has suggested several approaches for stabilizing the
training of the discriminator, which can eventually improve the training of the generator. For example,
Goodfellow et al. (2014) adopt a simple algorithmic trick that updates W for multiple iterations
after updating θ for one iteration, i.e., training the discriminator more frequently than the generator.
Besides, Xiang & Li (2017) suggest that the weight normalization approach proposed in Salimans
& Kingma (2016) can also stabilize the training of the discriminator. More recently, Miyato et al.
(2018) propose a spectral normalization approach to control the spectral norm of the weight matrix in
each layer. Specifically, in each forward step, they normalize the weight matrix by the approximation
of its spectral norm, which is obtained by the one-step power method. They further show that spectral
normalization essentially controls the Lipschitz constant of the discriminator with respect to the input.
Compared to other methods for controlling the Lipschitz constant of the discriminator, e.g., gradient
penalty (Gulrajani et al., 2017; Gao et al., 2017), the experiments in Miyato et al. (2018) show
that the spectral normalization approach achieves better performance with fairly low computational
cost. Moreover, Miyato et al. (2018) show that spectral normalization suffers less from the mode
collapse, that is, the generator outputs only over a fairly small support. Such a phenomenon, though
not well understood, suggests that the spectral normalization will balance the discrimination and
representation well.
Besides the aforementioned algorithmic tricks and normalization approaches, regularization can also
stabilize the training of the discriminator (Brock et al., 2016; Roth et al., 2017; Nagarajan & Kolter,
2017; Liu et al., 2018). For instance, orthogonal regularization, proposed by Brock et al. (2016),
forces the columns of weight matrices in the discriminator to be orthonormal by augmenting the
objective function f(θ, W) with λ PiL=1 kWi>Wi - IkF, where λ > 0 is the regularization parameter,
Wi denotes the weight matrix of the i-th layer in the discriminator, I denotes the identity matrix, and
L is the depth of the discriminator. The experimental results in Brock et al. (2016) show that the
orthogonal regularization improves the performance and generalization ability of GANs. However,
the empirical evidence in Miyato et al. (2018) shows that the orthogonal regularization is still less
competitive than the spectral normalization approach. One possible explanation is that the orthogonal
normalization, forcing all non-zero singular values to be 1, is more restrictive than the spectral
normalization, which only forces the largest singular value of each weight matrix to be 1.
Motivated by the spectral normalization, we propose a novel training framework, which provides
more flexible and precise control over the spectra of weight matrices in the discriminator. Specifically,
we reparameterize each weight matrix Wi ∈ Rdi×di+1 as Wi = UiEiVi>, where Ui and Vi are
required to have orthonormal columns, Ei = diag(ei1, ..., eir ) denotes a diagonal matrix with ri =
min(di, di+ι), and eɪ ≥ ∙∙∙ ≥ e∏ ≥ 0 are singular values of Wi. With such a reparameterization,
an L-layer discriminator becomes
D(x; U, E, V) = ULELVL σL-i(∙∙∙ σι(UιEιV^r x) ∙∙∙),
where σi(∙) is the entry-wise activation operator of the i-th layer, U := {Uι,…，Ul}, E :=
{E1 , ..., EL }, and V := {V1 , ..., VL } denote the parameters of the discriminator D, and x denotes the
input vector. This reparameterization allows us to control the spectra of the original weight matrix Wi
2
Published as a conference paper at ICLR 2019
by manipulating Ei. For example, we can rescale Ei by its largest diagonal element, which essentially
is the spectral normalization. Besides, we can also manipulate the diagonal entries of Ei to control
the decays in singular values (e.g., fast or slow decays). Recall that our reparameterization requires
Ui and Vi to have orthonormal columns. This requirement can be achieved by several methods in
the existing literature, such as the stiefel manifold gradient method. However, Huang et al. (2017)
show that the stochastic stiefel manifold gradient method is unstable. Moreover, other methods, such
as cayley transformation and householder transformation, suffer from several disadvantages: (I).
High computational cost1; (II). Sophisticated implementation (Shepard et al., 2015). Different from
the methods mentioned above, our framework applies the orthogonal regularization to all Ui ’s and
Vi’s. Such a regularization suffices to guarantee the approximate orthogonality of Ui’s and Vi’s in
practice, which is supported by our experiments. Moreover, our experimental results on CIFAR-10,
STL-10 and ImageNet datasets show that our proposed method achieves competitive performance on
CIFAR-10 and better performance than the spectral normalization and other competing approaches
on STL-10 and ImageNet. Besides the empirical studies, we provide theoretical analysis, which
characterizes how the spectrum control benefits the generalization ability of GANs. Specifically,
denote μ as the underlying data distribution and Vn as the distribution given by the well trained
generator. We establish a generalization bound under spectrum control as follows (informal):
J /	\,，一，7	/	∖,K
dF,φ(μ, Vn) ≤ inf dF,φ(μ, V) + O
ν∈DG
where d = max{dι,..., dL}, d，F,φ(∙, ∙) is the F-distance, and DG denotes the class of distributions
generated by generators. Compared to the results in Zhang et al. (2017), our result improves the
generalization bound up to an exponential factor of the depth of the discriminator. More details will
be discussed in Section 3.
The rest of the paper is organized as follows: Section 2 introduces our proposed training framework
in detail; Section 3 presents the generalization bound for GANs under spectrum control; Section 4
presents numerical experiments on CIFAR-10, STL-10, and ImageNet datasets.
Notations: Given an integer d > 0, we denote [d] = {1, 2, ..., d}. Given a vector v ∈ Rd, we denote
kvk22 = Pid=1 |vi|2 as its Euclidean norm. Given a matrix M ∈ Rm×n , we denote the spectral norm
by ∣∣M∣∣2 as the largest singular value of M. We adopt the standard O(∙) notation, which is defined
as f(x) = O (g(x)) as x → ∞, if and only if there exists M > 0 and x0, such that |f (x)| ≤ Mg(x)
for X ≥ x0. We use O(∙) to denote O(∙) with hidden logarithmic factors.
2	Methodology
We present a new framework for flexibly controlling the spectra of weight matrices. We first consider
an L-layer discriminator D as follows:
D(x; W) = Wl0l-i(Wl-i ∙∙∙ σ1(W1x) ∙∙∙),	(2)
where σi (∙) denotes the entry-wise activation operator of the i-th layer, Wi ∈ Rdi+1×di denotes the
weight matrix of the i-th layer, x ∈ Rd1 denotes the input feature, W := {W1, ..., WL} denotes the
parameters of the discriminator D, and dL+1 = 1.
2.1 SVD reparameterization
Our framework directly applies an SVD reparameterization to each weight matrix Wi in the discrim-
inator D, i.e., Wi = UiEiVi>, where ri = min(di, di+1), Ui ∈ Rdi+1 ×ri and Vi ∈ Rdi×ri denote
two matrices with orthonormal columns, Ei = diag(eɪ,…，e^) denotes a diagonal matrix, and
el ≥ ∙∙∙ ≥ £^ ≥ 0 are the singular values of Wi. The discriminator can be rewritten as follows:
D(x;U, E, V) = ULELVLσL-i(UL-iEL-iVL-i …σ1(U1E1V1>x)…)),	⑶
where U := {U1, ..., UL}, E := {E1, ..., EL}, and V := {V1, ..., VL}2 denote the parameters of the
discriminator D. Throughout the rest of the paper, if not clear specified, we denote D(x; U, E, V)
1Without a sparse matrix implementation, these methods are highly unscalable and inefficient (not supported
by the existing deep learning libraries such as TensorFlow and PyTorch in GPU).
2WL essentially is a vector. To be consistent, we still use UL EL VL> to reparametrize WL. Actually, it is not
necessary. We can directly control the norm of WL in practice.
3
Published as a conference paper at ICLR 2019
by D(x) for notational simplicity. The motivation behind this reparameterization is to control the
singular values of each weight matrix Wi by explicitly manipulating Ei . We then consider a new
min-max problem as follows:
1n
min Emaxv {n Xφ (A(D(Xi)))+Ex 〜dgθ [。(I-A(D(X)))] -YR(E)},
, ,	i=1
X----------------------V---------------------}
f(θ,E,U,V)
subject to E ∈ Ω, U>Ui = Ii, and Vi>% = Ii ∀ i ∈ [L],	(4)
where Ii denotes the identity matrix of size ri, R(E) is the regularizer with a regularization parameter
γ > 0, and Ω denotes a feasible set. By choosing different Ω and R(E), (4) can control the spectrum
of the weight matrix Wi flexibly. For example, if We take the feasible set Ω = {E : ej = 1 ∀ej ∈
Ei, Ei ∈ E} and R(E) = 0, then our method essentially is the orthogonal regularization. We will
discuss some options of Ω and R(E) later in detail.
As mentioned earlier, the orthogonal constraints in (4) suffer from the high computational cost
and sophisticated implementation. To address these drawbacks, we directly apply the orthogonal
regularization to all Ui’s and Vi’s. Therefore, problem (4) becomes
L
min max f (θ, E,U, V)- λX(kU>Ui- IikF + kViτ¼ - IikF)- YR(E), s.t. E ∈ Ω,	(5)
θ E,U,V
i=1
where λ > 0 is a regularization parameter. A relative large λ (e.g., λ = 1), ensures the orthogonality
of Ui and Vi. See more details in Section 4.1. Moreover, (5) can be efficiently solved by stochastic
gradient algorithms. Projection may be needed to handle the constraint Ω. See more details later.
2.2 Spectrum Control
We provide a few options of Ω and R(E) for controlling the spectra of weight matrices in the
discriminator, which is motivated by Miyato et al. (2018). Miyato et al. (2018) have shown that for
an L-layer discriminator D, we have:
D(X)- D(y)| ≤ kWLk2 ( γ ∣∣Wik2	∙	Pi)	kχ - yk2 =	eL	(Y	e1	∙ Pi)	kχ -	yk2,⑹
where Pi is the Lipschitz constant of σi(∙). The last equation holds for our proposed reparameteriza-
tion. For commonly used activation operators, such as the sigmoid, ReLU, and leak-ReLU functions,
Pi ≤ 1. Therefore, QiL=1 ei1 is essentially an upper bound for the Lipschitz constant, which can be
controlled by our proposed Ω and R(E). Note that WL is a vector with only one singular value. For
simplicity, we set e1L = 1 in the following analysis.
2.2.1	Flexible Spectral Control
Comparing to the orthogonal regularization, Miyato et al. (2018) suggest that we should allow more
flexibility by using spectral normalization, which only bounds the largest singular value. They
implement spectral normalization by one-step power iteration.
•	Spectrum Normalization: We can also easily implement spectral normalization under our SVD
reparameterization framework. Specifically, the spectral normalization rescales the weight matrix Ei
by its spectral norm ei1 , which is equivalent to solving the following problem:
where Q :
L
min max f(θ, Q(E), U, V) - λ X(kUiτUi - IikF2 + kViτVi - IikF2),
θ Q(E),U,V
i=1
f Ei	ELI
{可,...,eL }.
•	Spectrum Constraint: Note that the spectral normalization essentially reparameterize the Lipschitz
constraint Ω:
Ω = {E :0 ≤ el ≤ 1 ∀i ∈ [L]} .	(7)
This essentially controls QiL=1 ei1 by forcing each ei1 ≤ 1. Instead of spectral normalization, we
consider directly solving the problem with the Lipschitz constraint. To maintain the feasibility ofE,
4
Published as a conference paper at ICLR 2019
we only need a simple projection for each Ei in the back propagation, which can be implemented by
a simple entry-wise clipping operator defined as
g(t) := 0 ∙ 1{t≤0} + t ∙ 1{0<t<1} + 1 ∙ 1{t≥1},	(8)
where 1t∈A = 1 if t ∈ A, and 0 otherwise.
These two methods are essentially solving the same problem, but in different formulations. Therefore,
different algorithms are adopted. Due to the nonconvex-nonconcave structure of (5), different
solutions are obtained.
• Lipschitz Regularizer: We can also directly penalize QiL=1 ei1 to control the Lipschitz constant of
the discriminator D. Specifically, we define the Lipschitz regularizer as:
R(E ) := max log Y ei1 , 0 = max X log ei1 , 0 .
i=1	i=1
Compared to the spectral constraint, which enforces all ei1 ≤ 1, the Lipschitz regularizer is more
flexible since it allows ei1 > 1 for some i ∈ [L].
2.2.2 Slow singular value decay
Miyato et al. (2018) owe their empirical success of training SN-GAN to controlling the spectral
norm while allowing flexibility. This perspective, however, is not very concrete. As we know,
orthogonal regularization and spectral normalization with SVD can both control the spectral norm.
Their empirical performance is actually worse than SN-GAN. For example, on the STL-10 dataset,
SN-GAN achieves an inception score of 8.83, while singular value truncation only achieves 8.69 and
orthogonal regularization achieves 8.77.
The reason behind is that SN-GAN implements the spectral normalization via one-step power iteration.
This procedure consistently underestimates spectral norms of weight matrices. Consequently, in
addition to controlling the spectral norms, the spectral normalization in SN-GAN affects the whole
spectrum of the weight matrix (encourages slow singular value decay as in Figure 1), which we refer
to as “flexibility”. Encouraging slow decay is essentially encouraging the network to capture as
many features as possible while allowing correlation between neurons. Built upon these empirical
observations, we conjecture that controlling the whole spectrum better improves the performance of
GANs, which is further corroborated by our numerical experiments (Section 4).
0.2 ■
1.0 -
0.8 ■
0.6 -
0.4-
0.0	0.2	0.4	0.6	0.8	1.0
Orthogonal Reg. SN w/ Power Iteration SN w/ SVD	D-optimal Reg.
No Decay	Slow Decay	Fast Decay	Slower Decay
IS: 8.77 ± .07 IS: 8.83 ± .07 IS: 8.69 ± .08	IS: 9.25 ± .08
Figure 1: An illustration of smooth singular value decays with different methods. The vertical axis
denotes the value and the horizontal axis denotes the normalized rank. The inception scores on
STL-10 are also reported.
• D-Optimal Regularizer: We propose the D-Optimal Regularizer as follows:
L-1	L-1	ri
R(E) = 2 X log(∣(E>Ei)-1∣) = - X log (Y ek)，	⑼
i=1	i=1	k=1
which is motivated by D-optimal design. D-optimal design (Wu & Hamada, 2011) is a popular
principle in experimental design, where people aim to estimate parameters of statistical models with
a minimum number of experiments. Specifically, D-optimal design maximizes the determinant of
Fisher information matrix while allowing correlation between features in experiments. Existing
literature has shown the superiority of D-optimal design to the orthogonal (uncorrelated) design
on nonlinear model estimation (Yang et al., 2013; Li & Majumdar, 2009; Mentre et al., 1997).
5
Published as a conference paper at ICLR 2019
Analogously, our proposed D-Optimal Regularizer essentially maximizes the log Gram determinant
of the weight matrix,
L-1	L-1
2 X log(l(W>Wi)-11) ≈ 2 X log(l(E>Ei)-11).
i=1	i=1
The approximation holds due to the SVD reparameterization Wi = UiEi Vi , with Ui , Vi approximately
orthogonal. Moreover, note that the derivative of log(t) is t, a monotone decreasing function. Then
log(t) has a significant impact when t is small. Thus, D-optimal regularizer R(E) encourages a slow
singular value decay.
• Divergence Regularizer: We propose a divergence regular-
ization to precisely control the slow decay as shown in Fig-
ure 1. To mimic such a decay, we consider a reference distri-
bution, y = 1 — min(∣z∣, 1), where Z 〜 N (0, a2). Figure 2
shows the decays of 10K order statistics sampled from y. We
then denote the density function of y as p and the probability
mass function of a uniform discrete distribution over {eij }jri=-11
as Qi(ej) = r--ɪ∀j ∈ 屋—1]. Note that the K-L divergence
between a discrete distribution and a continuous distribution is
∞. To address this issue, we discretize p. Specifically, given
{eij }jri=1, we construct a discrete distribution over {eij }jri=-11 with
a probability mass function Pii*, defined as follows:
Figure 2: The plot of normalized
ranks versus values of 10K order
statistics sampled from reference
distributions. The vertical axis
denotes the value; the horizontal
axis denotes the normalized rank.
P(ej)(ej+ι - j
Pk=II Pek )(ek+ι- ek)
∀j ∈ [ri — 1].
Ignoring the normalization term in the denominator, we then define the regularizer as follows:
L-1	ri -1
R(E )=X j X log
(ri — 1)-1
(eik+1 — eik)P(eik) .
Note that the divergence regularizer requires the singular values in the interval [0, 1] and D-optimal
regularizer cannot control the Lipschitz constant of the discriminator D. Therefore, we incorporate
the divergence regularizer with the spectrum constraint and combine the D-optimal regularizer with
the spectral normalization to bound the Lipschitz constant. Our experimental results show that both
combinations improve the training of GANs on CIFAR10 and STL-10 datasets.
3 Theory
We show how the spectrum control benefits the generalization of GANs. Before proceed, we define
F -distance as follows.
Definition 1 (F -distance). Let F be a class of functions from Rd to [0, 1] such that if f ∈ F,
1 — f ∈ F. Let φ be a concave function. Then given two distributions μ and V supported on Rd, the
F-distance d，F,φ(μ, V) with respect to φ is defined as
d，F,φ(μ, v) = sup Ex〜μ[φ(f(x))]+ Ex”[φ(1 — f(x))] — 2φ(1∕2).
f∈F
Note that F -distance unifies Jensen-Shannon distance, Wasserstein distance and neural dis-
tance as proposed in Arora et al. (2017). For example, when taking φ(x) = x and F =
{all 1-Lipschitz functions from Rd to [0, 1]}, the F -distance is the Wasserstein distance. Recall
that by (1), the training of GANs is essentially minimizing the F -distance with F being the collection
of composite functions A(D(∙)), where D(J is the L-layer discriminator network defined by (2). To
establish the generalization bound, we impose the following assumption.
Assumption 1. The activation operator σi is 1-Lipschitz with σi(0) = 0 for any i ∈ [L — 1]. A is
1 -Lipschitz such that if A(D(∙)) ∈ F, 1 — A(D(∙)) ∈ F. φ is ρφ-Lipschitz. The spectral norms of
weight matrices are bounded respectively, i.e., kWik2 ≤ BWi for any i ∈ [L].
6
Published as a conference paper at ICLR 2019
Note that commonly used functions A, such as the sigmoid function, satisfy the assumption. We
denote by μ the underlying data distribution, and by bn the empirical data distribution. We further
denote νn as the distribution given by the generator that minimizes the loss (1) up to accuracy , i.e.,
dF,φ(μn, Vn) ≤ inf dF ,φ(μn, V) + g
ν∈DG
where DG is the class of distributions generated by generators. Then we give the generalization
bound based on the PAC-learning framework as follows.
Theorem 2. Under Assumption 1, assume that the input data xi ∈ Rd1 is bounded, i.e., kxik2 ≤ Bx
for i ∈ [n]. Then given activation operators σ1, . . . , σL-1, A, and φ, with probability at least 1 - δ
over the joint distribution of x1, . . . , xn, we have
dF,φ(μ,νn) ≤ ν∈nfGdF,φ(μ,ν) + O
∖
where β = Bx QiL=1 BWi and d = max(d1, . . . , dL).
'pφβ{d2 L log (√dnLβ)
+ ,
The detailed proof is provided in Appendix A.1. By constraining each BWi = 1, the generalization
bound is reduced to of the order O ('d2L/n), which is polynomial in d and L.On the contrary,
without such spectrum constraints, the bound can be exponentially dependent on L. For example,
if BWi ≥ 1 + r with some constant r > 0 for any i = 1, . . . , L, we have β ≥ Bx(1 + r)L, which
implies that GANs cannot generalize with polynomial number of samples.
Remark 3. Empirical Rademacher complexity (ERC) is adopted to derive our generalization bound,
which is of the order O (βpd2L∕n). Directly applying the ERC based generalization bound in
Bartlett et al. (2017) yields a bound of the order O (β pd2L3∕n).Our bound is tighter, and is
derived by exploiting the Lipschitz continuity of the discriminator with respect to its model parameters
(weight matrices). Similar idea is used in Zhang et al. (2017), however, we derive sharper Lipschitz
constants 3 by the key step of decoupling the spectral norms of weight matrices and the number of
parameters, i.e., separating β and d2L.
Remark 4. Theorem 2 shows the advantage of spectrum control in generalization by constraining
the class of discriminators. However, as suggested in Arora et al. (2017), the class of discriminators
needs to be large enough to detect lack of diversity. Despite of a lack of theoretical justifications,
empirical results in Miyato et al. (2018) show that discriminators with spectral normalization are
powerful in distinguishing Vn from μ, and suffer less from the mode collapse. We conjecture that the
observed singular value decay (as illustrated in Figure 1) contributes to preventing mode collapse.
We leave this for future theoretical investigation.
4	Experiment
To demonstrate our proposed new methods, we conduct experiments on CIFAR-10 (Krizhevsky &
Hinton, 2009), STL-10 (Coates et al., 2011), and ImageNet (Russakovsky et al., 2015). We illustrate
the importance of spectrum control in GANs training by revealing a close relation between the
performance and the singular value decays.
All implementations are done in Chainer as the official implementation of the SN-GAN (Miyato et al.,
2018). Note that SN-GAN is using power iteration. If not specified, all orther Spectral Normalization
(SN) methods are under SVD framework. For quantitative assessment of generated examples, we use
inception score (Salimans et al., 2016) and Frechet inception distance (FID, Heusel et al. (2017)).
All reported results correspond to 10 runs of the GAN training with different random initializations.
The discussion of this paper is based on fully connected layer. When dealing with convolutional
layer, we only need to reshape the 4D weight tensor to a 2D matrix. Denote the weight tensor of
a convolutional layer as WC ∈ Rco,ci,kh,kw, where co, ci, (kh, kw) denotes the output channel, the
input channel and the kernel size. We reshape WC as W ∈ Rco,ci×kh×kw (Huang et al., 2017), i.e.,
merging the last three dimensions while preserving the first dimension. See more implementation
details in Appendix C.1.
3The Lipschitz constant in Zhang et al. (2017) can be of the order dL.
7
Published as a conference paper at ICLR 2019
4.1	DC-GAN
We test our methods on DC-GANs with two datasets, CIFAR-10 and STL-10. Specifically, we adopt
a 5-layer CNN as the generator and a 7-layer CNN as the discriminator. Recall that our proposed
training framework tries to solve the equilibrium for equation (5). We set φ(∙) = log(∙) and A being
the sigmoid function. Denote fD (E, U, V) = f (θ, E, U , V) - λLorth - γR(E) for a fixed θ and
fG(θ) = -Ex〜Dgθ [A(D(x))] for fixed U, V, and E, where Lorth(U, V) = P3(kU>Ui-6||2 +
kVi>Vi - Ii kF2). We maximize fD (E, U, V) for ndis iterations (ndis ≥ 1) followed by minimizing
fG(θ) for one iteration. Note that we use a log D trick (Goodfellow et al., 2014) to ease the
computation of minimizing fG (θ). Detailed implementations are provided in Appendices B and C.2.
We choose tuning parameters λ = 10 and γ = 1 in all the experiments except for the Divergence
regularizer, where we pick λ = 10 and γ = 0.054. γ is chosen according to the output range of
different regularizers. We set a smaller gamma for Divergence Regularizer, since its output is much
larger than other regularizers. We take 100K iterations in all the experiments on CIFAR-10 and 200K
iterations on STL-10 as suggested in Miyato et al. (2018).
To solve (5), we adopt the setting in Radford et al. (2015), which has been shown to be robust for
different GANs by Miyato et al. (2018). Specifically, we use the Adam optimizer (Kingma & Ba,
2014) with the following hyperparameters: (1) ndis = 1; (2) α = 0.0002, the initial learning rate; (3)
β1 = 0.5, β2 = 0.999, the first and second order momentum parameters of Adam respectively.
Before we present our results, we show the effectiveness of our proposed reparameterization,
which aims to approximate the singular values of weight matrices while avoiding direct SVDs.
As can be seen, in Table 1, Ui and Vi have nearly orthonormal columns respectively, i.e.,
kUi> Ui - Ii kF2, kVi> Vi - Ii kF2 ≤ 10-4. Although the reparameterization introduces more model
parameters, it maintains comparable computational efficiency. See more details in Appendix D.2.
Table 1: The sub-orthogonality of Ui’s and Vi’s in the discriminator with the divergence regularizer
on CIFAR-10 after 100K iterations. For other settings, we also observe that all Ui ’s and Vi ’s have
nearly orthonormal columns.
		Layer 0		Layer 1	Layer 2	Layer 3	Layer 4	Layer 5	Layer 6
kU>	U	- IkF2	2.3e-5	1.2e-5	1.5e-5	1.6e-5	2.7e-5	2.5e-5	2.1e-5
kV>	V	- IkF2	7.9e-5	1.0e-5	1.7e-5	2.5e-5	4.1e-5	7.1e-5	3.9e-5
Figure 4 shows that the singular value decays of weight
matrices with two different methods: SN-GAN and D-
optimal regularizer with spectral normalization. As can
be seen, our method achieves a slower decay in singular
values than that of SN-GAN. See more results of other
methods in Appendix D.3. Such a slower decay improves
the performance of GANs. Specifically, Table 2 presents
the inception scores and FIDs of our proposed methods
as well as other methods on CIFAR-10 and STL-10. As
can be seen, under CNN architecture, our methods achieve
significant improvements on STL-10. Compared with
STL-10, CIFAR-10 is easy to learn, and thus GAN train-
ing can only limitedly benefits from encouraging the slow
singular value decay. As a result, on CIFAR-10, our meth-
ods slightly improve the result of SN-GAN. Moreover, as
Figure 3: Inception scores on ImageNet.
We can see that our method outperforms
SN-GAN.
shown in Figure 4, at the early stage (5k iteration), our method achieves slow decay while SN-GAN
still decays fast. Thus, it converge faster than SN-GAN as shown in Figure 5.
4In fact, the performance is not sensitive to these hyperparameters, since we only observe negligible difference
by fine tuning these parameters. Specifically, when λ ∈ [1, 100] and γ ∈ [0.2, 5] (γ ∈ [0.01, 0.1] for Divergence
regularizer), the algorithm yields similar results.
8
Published as a conference paper at ICLR 2019
5K
50K
100K	200K
Figure 4: Illustrations of singular value decay in 7 layers at 5K-th, 50K-th, 100K-th, and 200K-th
iteration. The above figures are for the SN-GANs; the below are for D-optimal regularizer with SN.
CIFAR: FID
CIFAR: Inception Score	STL: FID	STL: Inception Score
Figure 5: The inception scores and FID’s with error bar over 10 runs. Due to the space limit we only
present the comparisaon between SN-GAN and D-optimial regularizer with SN, which is the best
among our proposed methods. The full comparison with all proposed methods is in Appendix D.4.
4.2	RESNET-GAN
We also test our proposed method on ResNet, a more advanced structure, on both discriminator and
generator (Appendix C.2). For these experiments, we adopt the hinge loss for adversarial training on
discriminators:
1n
fD(E, U, V) = Ex〜Dgθ [min(0, -1 - D(x))] + —工 min (0, -1 + D(Xi)).
n i=1
We also adopt the commonly used hyperparameter settings for the Adam optimizer on ResNet:
ndis = 5, α = 0.0002, β1 = 0, and β2 = 0.9 (Gulrajani et al., 2017). Due to our computational
resource limit, we only test the method of spectral normalization (our version) with D-optimal
regularizer, which achieves the best performance on CNN experiments. We also test on the official
subsampled 64 × 64 ImageNet data using the conditional GAN with a projected discriminator Miyato
& Koyama (2018).
The results of our experiments on CIFAR-10 and STL-10 are listed in Table 2, and results on
ImageNet are shown in Figure 3. We see that our method is much better than the other methods
on STL-10 and ImageNet. As for CIFAR-10, our method is better than orthogonal regularizer but
slightly worse than SN-GAN. We believe the reason behind is that CIFAR-10 is relatively easy. As
can be seen, for CIFAR-10, the inception scores of all methods are around 8, while the inception
score of real data is around 11. In contrast, for STL-10, the inception score of real data is around 26,
while inception scores of all methods are less than 10. As a result, when the dataset is complicated
and network needs high capacity, our method performs better than SN-GAN.
9
Published as a conference paper at ICLR 2019
Table 2: The inception scores and FIDs on CIFAR-10 and STL-10. For consistency, we reimplement
baselines under our Chainer environment.
Method	Inception Score		FID	
	CIFAR-10	STL-10	CIFAR-10	STL-10
Real Data	11.24 ±.12	26.08 ± .26-	7.8	7.9
CNN Baseline WGAN-GP	6.72 ± .11	8.42 ± .09	39.0 ±.29	54.1 ± .35
Orthogonal Reg.	7.31 ± .09	8.77 ±.07	25.7 ±.33	44.5 ± .30
SN-GAN (Power Iter.)	7.39 ± .05	8.83 ± .07	24.7 ±.25	45.5 ± .34
Ours CNN (Under SVD) Spectral Norm.	7.35 ± .05		8.69 ±.08	25.2 ±.22	44.8 ± .39
Spectral Constraint	7.43 ± .08	8.97 ±.05	24.8 ±.30	44.0 ± .42
Lipschitz Reg.	7.43 ± .08	8.99 ±.06	24.1 ±.28	45.3 ± .38
SC + Divergence Reg.	7.44 ±.05	9.21 ±.09	24.3 ±.21	41.9 ± .37
SN + D-Optimal Reg.	7.48 ± .06	9.25 ± .08	23.0 ±.27	40.5 ± .41
ResNet Structure Orthogonal Reg.	7.90 ±.05	8.83 ±.05	22.3 ±.26	44.9 ± .35
SN-GAN (Power Iter.)	8.21 ± .05	9.15 ±.06	19.5 ±.22	43.0 ± .44
SN + D-Optimal Reg.	8.06 ± .06	9.65 ± .06	20.5 ±.18	39.9 ±.33
5	Conclusion
In this paper, we propose a new SVD-type reparameterization for weight matrices of the discriminator
in GANs, allowing us to efficiently manipulate the spectra of weight matrices. We than establish a
new generalization bound of GAN to justify the importance of spectrum control on weight matrices.
Moreover, we propose new regularizers to encourage the slow singular value decay. Our experi-
ments on CIFAR-10, STL-10, and ImageNet datasets support our proposed methods, theory, and
discoveries.
References
Mardn Abadi and David G Andersen. Learning to protect communications with adversarial neural
cryptography. arXiv preprint arXiv:1610.06918, 2016.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein gan. arXiv preprint
arXiv:1701.07875, 2017.
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium
in generative adversarial nets (gans). arXiv preprint arXiv:1703.00573, 2017.
Shane Barratt and Rishi Sharma. A note on the inception score. arXiv preprint arXiv:1801.01973,
2018.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pp. 6241-6250, 2017.
Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. Neural photo editing with
introspective adversarial networks. arXiv preprint arXiv:1609.07093, 2016.
Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised
feature learning. In Proceedings of the fourteenth international conference on artificial intelligence
and statistics, pp. 215-223, 2011.
DC Dowson and BV Landau. The frechet distance between multivariate normal distributions. Journal
of multivariate analysis, 12(3):450-455, 1982.
Rui Gao, Xi Chen, and Anton J. Kleywegt. Wasserstein distributional robustness and regularization
in statistical learning. CoRR, abs/1712.06050, 2017. URL http://arxiv.org/abs/1712.
06050.
10
Published as a conference paper at ICLR 2019
Ian Goodfellow. Nips 2016 tutorial: Generative adversarial networks. arXiv preprint
arXiv:1701.00160, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural informa-
tion processing systems,pp. 2672-2680, 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville.
Improved training of wasserstein gans. In Advances in Neural Information Processing Systems, pp.
5769-5779, 2017.
Martin HeUseL HUbert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Gunter Klambauer, and
Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a nash equilibrium.
arXiv preprint arXiv:1706.08500, 2017.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural
Information Processing Systems, pp. 4565-4573, 2016.
Lei Huang, Xianglong Liu, Bo Lang, Adams Wei Yu, and Bo Li. Orthogonal weight normalization:
Solution to optimization over multiple dependent stiefel manifolds in deep neural networks. arXiv
preprint arXiv:1709.06079, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.
Gang Li and Dibyen Majumdar. Some results on d-optimal designs for nonlinear models with
applications. Biometrika, 96(2):487-493, 2009.
Jiwei Li, Will Monroe, Tianlin Shi, SebaStien Jean, Alan Ritter, and Dan Jurafsky. Adversarial
learning for neural dialogue generation. arXiv preprint arXiv:1701.06547, 2017.
Weiyang Liu, Rongmei Lin, Zhen Liu, Lixin Liu, Zhiding Yu, Bo Dai, and Le Song. Learning
towards minimum hyperspherical energy. arXiv preprint arXiv:1805.09298, 2018.
France Mentre, Alain Mallet, and Doha Baccar. Optimal design in random-effects regression models.
Biometrika, 84(2):429-442, 1997.
Takeru Miyato and Masanori Koyama. cgans with projection discriminator. arXiv preprint
arXiv:1802.05637, 2018.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for
generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.
Vaishnavh Nagarajan and J Zico Kolter. Gradient descent gan optimization is locally stable. In
Advances in Neural Information Processing Systems, pp. 5591-5600, 2017.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Kevin Roth, Aurelien Lucchi, Sebastian Nowozin, and Thomas Hofmann. Stabilizing training
of generative adversarial networks through regularization. In Advances in Neural Information
Processing Systems, pp. 2015-2025, 2017.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition
challenge. International Journal of Computer Vision, 115(3):211-252, 2015.
Tim Salimans and Diederik P Kingma. Weight normalization: A simple reparameterization to
accelerate training of deep neural networks. In Advances in Neural Information Processing
Systems, pp. 901-909, 2016.
11
Published as a conference paper at ICLR 2019
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. In Advances in Neural Information Processing Systems, pp.
2234-2242, 2016.
Ron Shepard, Scott R Brozell, and Gergely Gidofalvi. The representation and parametrization of
orthogonal matrices. The Journal of Physical Chemistry A, 119(28):7924-7939, 2015.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru
Erhan, Vincent Vanhoucke, Andrew Rabinovich, et al. Going deeper with convolutions. Cvpr,
2015.
CF Jeff Wu and Michael S Hamada. Experiments: planning, analysis, and optimization, volume 552.
John Wiley & Sons, 2011.
Sitao Xiang and Hao Li. On the effects of batch and weight normalization in generative adversarial
networks. stat, 1050:22, 2017.
Min Yang, Stefanie Biedermann, and Elina Tang. On optimal designs for nonlinear models: a general
and efficient algorithm. Journal of the American Statistical Association, 108(504):1411-1420,
2013.
Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S Huang. Generative image
inpainting with contextual attention. arXiv preprint arXiv:1801.07892, 2018.
Pengchuan Zhang, Qiang Liu, Dengyong Zhou, Tao Xu, and Xiaodong He. On the discrimination-
generalization tradeoff in gans. arXiv preprint arXiv:1711.02771, 2017.
12
Published as a conference paper at ICLR 2019
A Proof in Section 3
A.1 Proof of Theorem 2
Proof. We bound the output of D(∙) as follows,
L
∣D(x)∣ ≤ kWLk2kσL-i(…σ1(W1x)…)k2 ≤ ∙∙∙ ≤ Bx Y BWi ∙
i=1
Consider dF,φ(μ, Vn) - infv∈Dg &f,φ(μ, ν). We have
dF,φ(μ, Vn) - inf dF,φ(μ, V)
ν∈DG
=dF ,φ(μ, Vn) - dF ,φ(μn, Vn) + dF ,φ(μn, Vn) - inf dF ,φ(μn, V)
ν∈DG
+ inf dF,φ(bn,V) - inf dF,φ(μ, v)
ν∈DG	ν∈DG
≤2 I sup Ex〜μ[φ(A(D(x)))] - Ex〜μn[φ(A(D(x)))][ + e.
∖ad(∙)∈f	)
Note that given x1, . . . , xi, . . . , xn and x1, . . . , x0i, . . . , xn, we have
sup Ex 〜μ[φ(A(D(χ)))] + Ex 〜μn[φ(A(D(χ)))]
AD(-)∈F
(10)
-sup Ex 〜μ[φ(A(D(χ)))]+ Ex“n [Φ(A(D(x)))]
AD(∙)∈F
≤ ∣Φ(A(D(xi))) - Φ(A(D(xi)))∣
≤ρφ
2L
≤- PφBxl IBWi.
n i=1
Then McDiarmid,s inequality gives us, with probability at least 1 - δ∕2,
sup Ex〜μ[φ(A(D(χ)))] - Ex〜μn[φ(A(D(χ)))]
AD(∙)∈F
≤E sup Ex〜μ[Φ(A(D(χ)))] - Ex〜μn[φ(A(D(χ)))]
AD(∙)∈F
By the argument of symmetrization, we have
+ 2pΦBx Y BWi y _2nδ
i=1
(11)
sup Ex〜μ[φ(A(D(χ)))] - Ex〜μn[φ(A(D(χ)))]
AD(∙)∈F
≤2Exi 〜μ,e
1
— SUP
n AD(∙)∈F
iφ(A(D(xi))) ,
i=1
(12)
where i’s are i.i.d. Rademacher random variables, i.e., P(i = 1) = P(i = -1) = 1∕2. McDi-
armid’s inequality again gives us, with probability at least 1 - δ∕2, we have
Exi 〜μ,e
1n
-sup V"∈iφ(A(D(xi)))
n ad(∙)∈fM
≤E
1
— SUP
n AD(∙)∈F
iφ(A(D(xi))) +
i=1
2ρΦBx Y BWiJ -2nδ
i=1
(13)
Note that Ee [l SUPAD(∙)∈fPn=ι ∕Φ(A(D(x)))] is essentially the empirical Rademacher com-
plexity of φ(A(D(∙))). Since φ and A are both Lipschitz, by Talagrand,s lemma, We have
E
n
n
n
1n
n aDup∈f X
1n
6iφ(A(D(xi))) ≤ PφEe —supTEiD(xi).
nD
i=1
13
Published as a conference paper at ICLR 2019
We then use the standard Dudley's entropy integral to bound Ee [1 SuPD Pn=1 GD(xi)]. We exploit
the parametric form of discriminators to find a tight covering number. We have to investigate
the Lipschitz continuity of D(∙) with respect to the weight matrices Wι,..., Wl. We based our
argument on telescoping. Given two sets of weight matrices W1, . . . , WL and W10 , . . . , WL0 and fix
the activation operators and A, we have
kD(x) - D0(x)k∞
≤ k Wlgl-i(…σ1(W1x)…)-WLσL-i(…σι(W1 x)…)∣∣2
=∣∣WLσL-1(∙ ∙ ∙ σ1(W1x) ∙ ∙ ∙ ) - WL σL-i(…σι (WIx)…)k2
+ ∣WL σL-i(…σ1(W1x) ∙∙∙ )WL σL-i(∙∙∙ σι(Wj0 x) ∙∙∙ )∣∣2
≤∣Wl - WL∣2∣σL-i(…σ1(W1x)…)∣∣2
+ IlWL∣∣2∣∣σL-i(…σ1(W1x)…)-σL-i(…σι(W1 x)…)∣2
L-1
≤ IlWL - WLIl 2Bx Y BWi + IlWL ∣∣2kσL-1(…σ1 (WIx)…) - σL-1(∙ ∙ ∙ σ1(W0 X)…) ∣2
i=1
≤.....
≤ X BxjBWj …H
B QL BW
For notational simplicity, We denote LWi =——B=1——i. When the activation operators and A
are given, function D has a one to one correspondence to weight matrices W1, . . . , WL . Thus, to
construct a covering of F, it is enough to construct matrix coverings of W1, . . . , WL, and their
Cartesian product gives us a covering of F. The standard argument of volume ratio gives us an upper
bound of the covering number of matrices with bounded spectral norms. Suppose M = {M ∈
Rd×h : kM ∣∣2 ≤ λ}, the covering number N (M,3∣∣∙k2) at scale E with respect to spectral norm is
bounded by
N(M,e, k∙k2) ≤ (l + mm(√d, √h)λ
Therefore, the covering number N (F, e, ∣∙∣∞) is bounded by
L
N (F f∙k∞) ≤ Y N (Wi, LL
i=1
L
≤Y
i=1
Take d = max{d1, . . . , dL}. We get
—,k∙k2)
W-i
1 + LLWi min(√⅞, √di+ι)Bwi、+
E
N (F ,e,k∙k∞) ≤ fl+ √dLBx Q=I BW
d2L
Then Dudley’s entropy integral gives us
1n
Ee — SuPy^ EiD(Xi)
nD
i=1
4α	12 产工 QL=ι BWi √n ，----=---------
≤√n + G L	PlogN(F,e,k∙k∞))d
E
It is enough to pick α
≤√n+12 Bx Y BWi
i=1
d2Llog(l+ √dLBx Q=1 BWi
表,which yields
α
1n
Ee -supY^ GD(Xi)
nD
i=1
≤ 4 +
n
12Bχ ∏L=1 BWi yd2Llog(2√dnLBχ∏L=1BWiJ
14
Published as a conference paper at ICLR 2019
Thus, we immediately have,
1n
一sup 废 elφ(A(D(xi)))
n AD(∙)∈F M
≤ 4Pφ +
n
12ρφBx ∏L=ι BWi Jd2Llog^2√dnLBXHL=1BWiJ
(14)
Now, combining equations (10), (11), (12), (13), and (14) together, we get
dF ,φ(μ, Vn ) ≤ inf dF ,φ(μ, Vn) +--上 +
ν∈DG	n
48pφβ ^d2L log ^2√dnLβ
+ 12ρφβV ”
where β = Bx QiL=1 BWi. On the other hand, naively applying the argument from Bartlett et al.
(2017) yields the generalization bound
dF,φ(μ, Vn) ≤ inf dF,φ(μ, Vn) + O
ν∈DG
P pφβydL3log^√dnLβJ
∖
Combining the two generalization bound together, we get
dF,φ(μ, Vn)
+ ρφβ
≤ ν∈nfGdF ,φ(μ,νn)+O
Ppφβ{dLmin(d, L2) log (√dnLβ)
+ ρφβ
(15)
∖
□
15
Published as a conference paper at ICLR 2019
B Algorithm
Recall that we are maximizing the following objective function fD (E , U, V) for discriminator D in
Section 4.1:
fD(E,U,V) =f(θ,E,U,V)-λLorth(U,V)-γR(E).
The detailed training algorithm is described in Algorithm 1:
Algorithm 1 Adversarial training with Spectrum Control of Discriminator, D
Initialization
1:	for l = 1, .., L do
2:	Determine the rank of l-layer: ri = min{di, di+1}.
3:	Initialize Ui ∈ Rdi×ri and Vi ∈ Rdi+1 ×ri with orthonormal columns.
4:	Initialize Ei = Iri .
5:	end for___________________________________________________________________________________
Forward pass
Input: mini-batch input Hi ∈ Rm×di from previous layer
Output: mini-batch output Si+1 ∈ Rm×di+1
Parameters: Ui, Vi, and Ei
1:	Perform Singluar value update on Ei
2:	Calculate weight matrix: Wi = UiEiVi> ∈ Rdi ×di+1.
3:	Calculate output: Si+1 = HiWi .
Backward pass
Input: activation derivative Vsi+ι f
Output: vHif, NUifD, VVi fD, V Eii, fD
1:	Calculate: VHi f = VSi+1 fWi> as standard linear module.
2:	Calculate: VWif = Hi>VSi+1f as standard linear module.
3:	Calculate: VUif, VVif, VEif based on VWif.
4:	Calculate: VUifD = VUif - λVUiLorth, VVifD = VVif - λVViLorth.
5:	Calculate: VEi fD = VEif - γVEiR(E).
6:	Update Ui, Vi, and Ei with the Adam Optimizer.
Singluar value update
Input: Ei
Output: Ei with ei1 ∈ [0, 1], i.e. the largest singular value is bounded by 1
1:	If use spectrum constraint: Ei = g(Ei), where g(∙) is the clipping operator defined in (8).
2:	If use spectrum normalization: Ei = Ei/ei1.
3:	If use Lipschitz regularizer: do nothing, since we do not need to layer-wisely control the Lipschitz
constant in this case.
Note that we omit the bias term for simplicity.
16
Published as a conference paper at ICLR 2019
C Experiment Setting
C.1 Performance Measure
Inception score is introduced by Salimans et al. (2016):
I({xi}in=1) := exp(E[DKL[p(y|x)||p(y)]]),
where p(y) is estimated by § PZi p(y∣xi) andp(y|x) is estimated by a pretrained Inception Net,
fincept Szegedy et al. (2015). Following the procedure in Salimans et al. (2016), we calculated the
score for randomly generated 5000 examples from generator for 10 times. The average and the
standard deviation of the inception scores are reported.
FreChet inception distance (FID) is introduced by Heusel et al. (2017). FID uses 2nd order information
of the final layer of the inception model applied to the examples. To begin with, Frechet distance (FD,
Dowson & Landau (1982)) is 2-Wasserstein distance between two Gaussian distribution pi and p2 :
F(P1,P2) = kμι - μ2k2 + tr[∑ι + ∑2 - 2(∑ι∑2)1/2],
where {μι, ∑ι} and {μ2, ∑2} are the mean and covariance of pi and p respectively. FID between
two image distribution pi and p2 is the FD between fincept(pi) and fincept(p2), i.e., the distribution
after the inception net transformation. The emperical FID is calculated by sampling 10000 true
images and 5000 images from generator, DGθ . Different from inception score, multiple repetition of
the experiments did not exhibit any notable variations on this score.
Acknowledging that different realizations of Inception Net results in different inception scores (Barratt
& Sharma, 2018), we test inception scores with the standard tensorflow inception net for consistency.
17
Published as a conference paper at ICLR 2019
C.2 Network Architecture
Table 3: The standard CNN architecture for CIFAR-10 and STL-10. For CIFAR-10, M = 32, Mg =
4. While for STL-10, M = 48, Mg = 6. The slopes coefficient is 0.1 for all LeakyReLU activations.
(a) Generator	(b) Discriminator
Input:	Z ∈ R128 〜N(0, I)	Input:	Image x ∈ RM ×M×3
Linear:	128 → Mg X Mg X 512	Conv:	[3 X 3, 64, stride = 1] LeakyReLU
Deconv: [4 X 4, 256, stride = 2] BN, ReLU		Conv:	[4 X 4, 64, stride = 2] LeakyReLU
Deconv: [4 X 4,128, Stride = 2] BN, ReLU		Conv:	[3 X 3, 128, stride = 1] LeakyReLU
Deconv: [4 X 4, 64, Stride = 2] BN, ReLU		Conv:	[4 X 4,128, Stride = 2] LeakyReLU
Conv:	[3 X 3, 3, stride = 1] Tanh	Conv:	[3 X 3, 256, Stride = 1] LeakyReLU
Conv: [4 X 4, 256, stride = 2] LeakyReLU
Conv: [3 × 3, 512, stride = 1] LeakyReLU
Linear: Mg × Mg × 512 → 1
Table 4: The ResNet architectures for CIFAR-10 and STL-10 datasets.
(a) CIFAR-10 Generator	(b) CIFAR-10 Discriminator
Input:	^^Z ∈ R128 〜N(0, I)	Input:	Image x ∈ R32×32×3
Linear:	128 7 4 X 4x 256	ResBlocks: [128,Down-Sampling] ×2
ResBlocks: [256, Up-sampling] x3		ResBlocks: [128] ×2	—
	BN,ReLU		ReLU, Global sum pooling
Conv:	[3 x 3, 3, stride = 1], Tanh	Linear:	4 X 4× 128 → 1
	(c) STL-10 Generator	(d) STL-10 Discriminator
Input:	Z ∈ R128 〜N(0, I)	=	Input:	Image x ∈ R48×48×3
Linear:	128 7 6 X 6x 512	ResBlock: [64, Down-Sampling]
ResBlock: [256, Up-sampling]		ResBlock: [128, DOWn-SamPling]	一
ResBlock: [128,Up-sampling]		ResBlock: [256, Down-Sampling]
ResBlock: [64, Up-sampling]		ResBlock: [512, Down-Sampling]
	BN,ReLU		ResBlock: [1024]	一
Conv:	[3 x 3, 3, stride = 1], Tanh	ReLU, Global sum pooling
Linear:	3 X 3× 128 → 1
18
Published as a conference paper at ICLR 2019
Table 5: The ResNet architectures for ImageNet dataset. Recall that we adopts conditional GAN
framework with projection discriminator. The ResBlock is implemented with the conditional batch
normalization for the generator. hEmbed(y), hi is the inner product of label embedding, Embed(y),
and the hidden state, h, after the global sum pooling. (Miyato & Koyama, 2018). We use the same
Residual Block as Gulrajani et al. (2017) describes.
(a) Generator	(b) Discriminator
Input:	Z ∈ R128 〜N(0, I)	Input:	Image x ∈ R64×64×3 Label y ∈ {1,2,3,...,1000}
Linear:	128 → 4 X 4× 1024		
ResBlock: [1024,Up-sampling]	ResBlock:	[64, Down-Sampling]
ResBlock: [512, Up-sampling]	ResBlock:	[128, Down-Sampling]
ResBlock: [128,Up-sampling]	ResBlock:	[256, Down-Sampling]
ResBlock: [64, Up-sampling]	ResBlock:	[512, Down-Sampling]
BN,ReLU	-	ResBlock:	[1024, Down-Sampling]
Conv:	[3 × 3, 3, stride = 1], Tanh		ReLU, Global sum pooling
	Projection+ Linear	:	hEmbed(y),hi + [1024 → 1]
19
Published as a conference paper at ICLR 2019
D Supplementary Results
D. 1 Accuracy of Approximating Singular Value Decomposition
In this section we evaluate how accurate can Ei = {eij }jri=1 approximate the true singular value
r
Ei = {eeij }jri=1 of recovered weight matrix Wi = Ui Ei Vi> . In Table 6, we compare the maximum,
.1	∙	. 1	1 , 1	∙	i' TΓ~↑	1 τ-t
the minimum, the mean and the variance of Ei and Ei .
Table 6: The accuracy of singular value estimation on CIFAR-10 experiment with divergence
regularizer after 100K iterations. For other layers and other setting, we can also observe highly
accurate singular value approximation.
	Max		Min 一		Mean		Var		-IlU >U - IIlF	IlV >V - IllF
	Ei	Ei	Ei	Eei	Ei	Ei	Ei	Ei		
0-th Layer	1.0000	1.0000	0.8344	0.8344	0.9448	0.9448	0.0037	0.0037	23e-5	79e-5
1-th Layer	1.0000	0.9999	0.8128	0.8128	0.9441	0.9441	0.0036	0.0036	T2e3	T0e3
2-th Layer	1.0000	0.9999	0.7972	0.7971	0.9438	0.9438	0.0036	0.0036	Γ3e3	T7e3
3-th Layer	1.0000	1.0000	0.7969	0.7969	0.9438	0.9438	0.0036	0.0036	1?6e-5	25e-5
4-th Layer	1.0000	1.0000	0.7816	0.7815	0.9432	0.9432	0.0036	0.0036	27e-5	-4Γe3
5-th Layer	1.0000	1.0002	0.7817	0.7816	0.9434	0.9434	0.0037	0.0037	25e-5	7?Te-5
6-th Layer	1.0000	1.0001	0.7765	0.7765	0.9591	0.9591	0.0033	0.0033	2.1e-5 一	3.9e-5	—
D.2 Timing Comparison
We compare the computational time on CIFAR-10 for different methods in Figure 6. We can see
Spectral Constraint with Divergence Regularizer is slightly slower than other methods. Note that the
training time will be impacted by the hardware condition, i.e. how many tasks are simultaneously
running on the same server. In order to leverage such fluctuation, we test timing for every 1000
iterations and then take their average.
s∙u4epdn .!04eJ∙uu∙u6 OOOI」。J SPU0*s
Figure 6:	Time cost per 1000 iterations.
D.3 Singular Value Decay Pattern
Here, we present an illustration of the singular value decays of weight matrices in 7 layers after 5K,
50K, 100K, and 200K iterations on STL-10 Data for different methods in Table 7. On CIFAR-10, we
also observe a very similar pattern, and thus we only present the STL-10 results.
We summarize our observation as follows:
(1)	Spectral normalization in SN-GAN yields a slow decay pattern because the power iteration
consistently underestimate the spectral norm. So it rescales lower-ranking eigenvalues to br 1. (2)
Under SVD reparametrization, the spectral norm is accurate. And thus the eigenvalues decay faster
20
Published as a conference paper at ICLR 2019
than the one with power iteration. (3) Because of D-Optimal regularization, the eigenvalues are
pushed towards 1 and thus they decay slower. (4) The divergence regularizer pushes the eigenvalues
towards a predefined slow decay curve and thus yields such a pattern. (5) The spectral constraint only
clip the singluar values which are out of the range [0, 1]. It will not rescale the other singular values
as the spectral normalization. The singular values tend to grow through the training process. Thus it
yields a spectrum with almost no decay. (6) Similar to the spectral constraint, Lipschitz regularizer
only penalize the largest singular value of each layer. Thus it yields a spectrum with almost no decay.
But it does not enforces the largest singular value of each layer to be 1.
Table 7:	Singular value decays.
5K
50K
100K	200K
o∙β-
a.«-
α.4∙
Spectral Normalization (Power Iteration, SN-GAN)
IM
0.95
o.v>
035
oxa
«.75
Cg
。斑
aβa
。3 1«
Spectral Normalization (SVD Parameterization)
SN + D-Optimal Regularization
IM
0.95
14n
1 35
1 35
0.99
α.90
O .M
0.97
0.9«
0Λ	LΛ
0Λ	1.0
0Λ	LΛ
0Λ	1.V
SC + Divergence Regularization
1Λ-
0.9-
03-
0-7-
0Λ-
OS-
OA-
03-
02-
Spectral Constraint
1Λ-
13-
---04ħlaye
l∙*ħlayv
---24ħlaye
—34ħlaye
---44h Iaye
---54ħlayv
6-th Iaye
----0-tħlayv
----l-ttιlayv
----2-ttιlayv
----34hlayv
----44ħ Iayvr
----5∙ttι32
C-Uilayvr
0.0	02 0Λ a-β 03	1.0
04	0.2 0Λ QS 03 1Λ
ιa∙
1.1 ■
1Λ∙
Lipschitz Regularization
0.0	02	04 0M 03	1.0
Iaye
Iaye
Iaye
Iaye
⅛ye
Iaye
g
——<Hħ
l-ttι
--Uh
——Xh
——44ħ
—Mh


21
Published as a conference paper at ICLR 2019
D.4 Image Generation
For CIFAR-10, the growth of inception score and FID over iterations is shown in Figure 7. While the
image generation results are shown in Figure 9.
For STL-10, the growth of inception score and FID over iterations is shown in Figure 8. While the
image generation results are shown in Figure 10. For ImageNet, the results are shown in Figure 11
7 6 5 4 3
°」OUS uo-*dωu-
m 200 -
+j
石
§ 150-
口
Q.
Φ
⊂ 100-
φ
150-
LL
0
20000	40000	60000	80000	100000
Iterations
Figure 7: The growth of inception score and FID over iterations on CIFAR-10 dataset.
22
Published as a conference paper at ICLR 2019
-------- - - - - -
98765432 O O O O O
L 5 0 5 0 5
出 OUS Uo-du2 2 11
uuP Uo-dulp
O 25000	50000	75000 IOOOOO 125000	150000	175000	200000
Iterations
Figure 8: The inception scores and FID’s of different methods on STL-10. The top part is for
the inception score, and the bottom is for the FID. The inner graph zooms in the results after 55K
iterations. The x-axis denotes the number of iterations.
23
Published as a conference paper at ICLR 2019
(g) SN+D-Optimal Regularizer
Figure 9: Image generation on CIFAR-10 dataset.
24
Published as a conference paper at ICLR 2019
(a) SN-GAN
(b) Spectrum Normalization
(c) Spectrum Constraint	(d) Lipschitz Regularization
Figure 10: Image generation on STL-10 dataset.
25
Published as a conference paper at ICLR 2019
(a) SN-GAN
(b) SN+D-Optimal Regularizer
Figure 11: Image generation on ImageNet.
(g) Cardoon
(i) Jack-o’-lantern
(h) Altar
Figure 12: Conditional Image generation on ImageNet (SN+D-Optimal Regularizer)
26