Published as a conference paper at ICLR 2019
Overcoming Catastrophic Forgetting for
Continual Learning via Model Adaptation
WenPeng Hu1,2,*, ZhoU Lin1# , Bing Liu3#, Chongyang Tao2, ZhengWei Tao2, Dongyan Zhao2,
JinWen Ma1, and RUi Yan2, t
1DePartment of Information Science, School of Mathematical Sciences, Peking University
2ICST, Peking University, Beijing, China
3DePartment of ComPuter Science, University of Illinois at Chicago
{wenpeng.hu,jokerlin,chongyangtao,tttzw,zhaody,ruiyan}@pku.edu.cn liub@uic.edu
jwma@math.pku.edu.cn
Ab stract
Learning multiPle tasks sequentially is imPortant for the develoPment of AI and
lifelong learning systems. HoWever, standard neural netWork architectures suffer
from catastroPhic forgetting Which makes it difficult for them to learn a sequence
of tasks. Several continual learning methods have been ProPosed to address the
Problem. In this PaPer, We ProPose a very different aPProach, called Parameter
Generation and Model Adaptation (PGMA), to dealing With the Problem. The
ProPosed aPProach learns to build a model, called the solver, With tWo sets of
Parameters. The first set is shared by all tasks learned so far and the second set
is dynamically generated to adaPt the solver to suit each test examPle in order
to classify it. Extensive exPeriments have been carried out to demonstrate the
effectiveness of the ProPosed aPProach.
1	Introduction
It is Well-knoWn that neural netWorks (NNs) suffer from catastrophic forgetting (CF) (McCloskey &
Cohen, 1989), Which refers to the Phenomenon that When learning a sequence of tasks, the learning of
each neW task may cause the NN to forget the models learned for the Previous tasks. Without solving
this Problem, an NN is hard to adaPt to lifelong or continual learning, Which is imPortant for AI.
Problem Statement: Given a sequence of supervised learning tasks T = (T1, T2, . . . , TN), we want
to learn them one by one in the given sequence such that the learning of each new task will not forget
the models learned for the previous tasks.
In recent years, many aPProaches (often called continual learning) have been ProPosed to lessen
the effect of CF (Chen & Liu, 2018; Parisi et al., 2018), e.g., dynamically exPandable netWork
(DEN) (Yoon et al., 2018), learning Without forgetting (LWF) (Li & Hoiem, 2016)), elastic Weight
consolidation (EWC) (KirkPatrick et al., 2017), incremental moment matching (IMM) (Lee et al.,
2017), gradient ePisodic memory (GEM) (LoPez-Paz et al., 2017), generative rePlay (GR) (Shin et al.,
2017), etc. These existing studies excePt DEN and LWF focused on learning a model Parameterized
by a single joint set of Parameters θ* Which is assumed to Work Well for all tasks. We call them
joint parameterization (JP) methods. DEN and LWF require constantly increasing the number of
Parameters and thus can result in a huge and comPlex model.
JP methods, hoWever, suffer from accuracy deterioration. Assume We have tWo tasks A and B that
need to be learned sequentially. Let θA* and θB* be the oPtimal Parameters for Task A and Task B
resPectively When each of them is learned individually. Let θ* be the joint Parameters learned by the
existing aPProaches to Perform Tasks A and B in sequence. Inevitably, θ* is different from θA* and/or
θB* and is highly likely to result in more errors for the tWo tasks than θA* and θB* individually.
* Equal contribution
,Corresponding author
1
Published as a conference paper at ICLR 2019
In this paper, we propose a different approach, called Parameter Generation and Model Adaptation
(PGMA), to dealing with CF and to significantly reducing accuracy deterioration. PGMA does not
learn ajoint parameter set θ*. Instead, it learns a parameter generator f (∙) and a shared parameter set
θ0. The key idea of PGMA is as follows: The overall classification network, called the solver S, has
two disjoint subsets/parts of parameters. The first subset is θ0 , which is shared by all tasks learned so
far. The second subset is just a place holder H that will be filled by parameters generated by f (∙) for
each test instance. That is, given a test instance, a set of parameters will be generated by the learned
parameter generator f (∙) to replace H. Solver S then combines θo and the generated parameters for
the test instance to classify it. Clearly, unlike existing JP approaches, we do not have a network with
a set of fixed parameters for S. The idea is that θ0 contains the common features of all tasks, and the
generated parameters for each test instance adapt S for each test instance in order to classify it.
Since f (∙) and the shared parameters θo will change during training for each new task, forgetting
can occur for previous tasks. To deal with it, in training f (∙) and S for each task Ti, in addition to
the training data of Ti , a small number of replayed samples will be generated by a data generation
network for the previous tasks to ensure that the knowledge learned for previous tasks remain
stable/unforgotten. Compared with the existing generative replay methods, our method does not
need labels for the replayed data because we use the replayed data only to constrain the training of S
and f (∙) rather than to treat them as surrogates of labeled training data from previous tasks. Further,
since in the existing replay methods, the labels are produced by the current learned network itself,
the labels can be noisy and biased, which may result in errors being accumulated and propagated to
subsequent tasks. Our method does not have this problem.
Apart from reducing the effect of accuracy deterioration of the existing approaches, the proposed
approach also has some other advantages. First, no parameter increase or network expansion is
needed to learn new tasks. Second, no previous data needs to be stored to enable the system to
remember the previously learned models or knowledge.
Experiments conducted using two image datasets (MNIST and CIFAR-10) and two text datasets
(DBPedia ontology (Lehmann et al., 2015) and THUCNews (Li et al., 2006)) show that the proposed
approach PGMA works well for different scenarios and different types of datasets, and outperforms
the existing strong baselines markedly.
2	Proposed PGMA Framework
Let the sequence of supervised learning tasks be T = (T1, T2, . . . , TN). Each task Ti is represented
by Ti = {xij, yij |j ∈ (1, . . . , Ni)}, where xij is the j-th example/sample of Ti, and yij is its label.
We use (xt, yt) to denote a test instance/example. To simplify the notation, we will just use (xi, yi)
to denote a training example from task Ti , omitting the second subscript j . Note that in the paper, we
use the terms example, sample, and instance interchangeably.
The proposed parameter generation and model adaptation (PGMA) architecture has three main
components:
• Solver S: It is the main classification model. As mentioned in Section 1, the parameter set of
S consists of two subsets, θ0 that is shared by all tasks (and instances) and H, a parameter place
holder, which will be replaced by the generated parameters set pi (or pt) for each training (or testing)
example xi (or xt). pi (or pt) basically serves to adapt the solver S to classify the example in
training or testing. This is the key idea of our approach. We adopt this parameter split as several
studies (Yoon et al., 2018; Li & Hoiem, 2016; Kirkpatrick et al., 2017) have shown that only part of
parameters of a neural network needs to be adjusted when learning a new task. See Section 2.2
• Dynamic Parameter Generator (DPG) f (∙): It takes the embedding Zi (or Zt) of each input
training (or testing) example xi (or xt) to generate the parameters pi (or pt) for solver S. Note that
we use Zi (or Zt) rather than the raw data xi (or xt) because the raw data’s dimension can be very
high. The embedding Zi (or Zt) as a low dimensional dense representation reduces the mapping space
for DPG and thus reduces the difficulty in its parameter generation. See Section 2.2.
• Data Generator (DG): It has two functions. The main function is to generate a set of replayed data
or samples {x0m}mM=1 using its decoder DGD for previous tasks to deal with catastrophic forgetting.
The other function is to generate the embedding Zi (or Zt) of each input training (or testing) example
xi (or xt) using its encoder DGE. See Section 2.3.
2
Published as a conference paper at ICLR 2019
2.1	Overall Approach of PGMA
We work backward by describing testing first before training. Given a test instance xt , DGE first
generates its embedding zt, which is fed to f (∙) to generate a set of parameters pt. Solver S then
takes xt as input and uses the trained/learned shared parameters θ0 and pt to classify xt . θ0 contains
the common features of all tasks learned so far. pt simply adapts S for xt in order to classify xt .
For training, the pipeline of the proposed PGMA framework is shown in Figure 1. Given a new task
Ti with its data (xi, yi), solver S and DPG f (∙) are jointly trained to learn Ti and also not to forget
the previously learned tasks. In each iteration, a set of parameters pi is generated by the current DPG
f (zi, μ) for each training instance Xi (Zi being its embedding), where μ is the set of parameters of
the DPG network, which is trained.
In training the solver S and DPG
for the new task Ti, both f (∙)
and the shared parameters θ0
will change, which can cause
forgetting in DPG for previous
tasks. To keep DPG remember-
ing the acquired knowledge for
previous tasks, we minimize the
variation of certain layers’ out-
put caused by the changes of
θo and f (∙) using the set of re-
played samples {x0m}mM=1 gen-
erated by DG.
Figure 1: Sequential training in the PGMA framework.
Inputs
PGMA has two training components: (1) DPG and solver S (DPG&S for short), i.e., DPG is
optimized together with solver S. (2) DG. The two components have their respective objective
functions (see Sections 2.2 and 2.3), and are trained alternately. This is because DPG takes input
zi (which is produced by DG) to generate parameters, and alternating training ensures consistent
convergence rates of DG and DPG.
2.2	DYNAMIC PARAMETER GENERATOR (DPG) AND S OLVER S IMPLEMENTATION
Several neural networks can be used to implement DPG for parameter generation, e.g., convolutional
neural network (CNN), recurrent neural network (RNN) and multilayer perceptron (MLP). The
objective of this paper is not to explore all possible implementations. We found that a straightforward
implementation using MLP can already achieve good results. Formally, DPG can be written as:
Pi = f(Zi, μ) = σ(wDZi + b)	(1)
where σ is the activation function, and WD and b are the parameters of DPG and denoted by μ.
For the implementation of solver S, several deep learning networks can be used as well. Again, a
MLP is employed in this work. Each layer of the MLP is a perceptron and can be formalized by
output = σ(wxinput), where xinput denotes the input of the particular perceptron. We also call a
perceptron a basic unit 1. In general, for each basic unit k of the solver S, we can have a shared
portion of the parameters θ0,k and a generated portion of the parameters pi,k, i.e.,
θi = combine(θ0, Pi) = {[wi,k]}K=ι = {[θ0,k ； Pi,k]}K=I	⑵
where combine(.) is concatenation and K is the number of basic units that the solver has. In our case,
K is the number of hidden layers (basic units) of the solver MLP.
It is important to note that the above is the most general case. In practice, there is no need to adapt all
the basic units in the solver, but only a subset of them. In our experiments (Section 3), we will see
that adapting only the final layer of the solver MLP can already achieve good results.
1This is because most of the existing networks can be regarded as the composition of multiple basic units,
even for those with complex structures, e.g., seq2seq (Sutskever et al., 2014), R-Net (Wang et al., 2017), Resnet
(He et al., 2016) and Densenet (Huang et al., 2017). Further, complex structures give us more freedom to decide
which part of the parameters should be generated.
3
Published as a conference paper at ICLR 2019
However, one may still ask whether the proposed combination method has sufficient capacity to
adjust the solver S in general because only part of the parameters are generated. Actually, pik can
affect all dimensions of the output of its corresponding basic unit k:
wi,kxinput = [θ0,k; pi,k]xinput = θ0,kxinput + pi,kxinput	(3)
where xi1nput and xi2nput are the block vectors of xinput. pi,kxi2nput can be regarded as the bias and
can adapt the output vector to any point in the vector space.
To train DPG&S, we use cross entropy loss (Lce). The objective function of the solver S including
DPG f (zi, μ) and θo for learning each new classification task Ti is defined as:
minimize Lce(S(xi,θ>yi)
μ,θo
M
s.t. X ∣∣R(Xm,θ*) -R(xm,θ*-1 )||<金	(4)
m=1
where Lce is the cross entropy loss, R(∙) denotes the output of all basic units (which can be any layer
or a set of layers2) in solver S, which we will introduce later, and θi is the whole set of parameters of
S (θ* = combine(pi, θ0)) representing the adaptation of S. We can see that the generated replayed
samples x0m are used as constraints to alleviate DPG&S’s forgetting. Specially, we extend the
knowledge distillation loss (Hinton et al., 2015) to the general situation. That is, to keep the past
learned knowledge, the output of the basic units in the solver should not change much when learning
a new task with the help of the generated data. If we do not consider the activation function, the
constraints in Eq. 4 can also be written as:
MK
min E £||w,kxm,k - wi-1,k xm,k ||	(5)
m=1 k=1
where K again denotes the number of basic units and M denotes the number of replayed samples.
The basic unit with a smaller k is at the relative lower layer of the solver network.3 x0m,k is the input of
the kth basic unit and is calculated through forward propagation, except x0m,1 = DGD (zsmample, θd0 )
which is the initial replayed sample x0m generated by DG (before optimizing the current task Ti).
w*,k is the combined parameter introduced in Section 2.2.
Two questions may be asked about Eqs. 4 and 5. (1) Since the replayed data x0 (denoting all x0m) is
used in Eqs. 4 and 5, does it play the same role as the original data x from previous tasks? (2) What
is the impact of the constraints on learning the new task? We answer the two questions now.
Question 1: The answer is positive. Since the purpose of the constraints is to maintain the learned
effect of the old data, clearly, using the original old data x in the constraints is better. However, we
proved the positive answer to question 1, which is given in Appendix.
Question 2: In calculating Eq. 5, if we stack all {x0m,k}mM=1 (column vectors) into a matrix X0k, Eq.
5 can be regarded as constraining Wik Xk to remain unchanged. If Xk is a row low rank matrix,
w* k can be trained to fit the new tasks. Otherwise, especially if Xk is a row full rank matrix, Wi k
cannot be trained and therefore cannot learn new tasks. However, benefiting from dynamic parameter
generation, our approach will not suffer from this problem. That is because wii,k is specially generated
through DPG to perform well with input X0k . And the new parameters will be generated to fit new
tasks while wii,k is able to remain unchanged.
2.3	Data Generator (DG)
As indicated earlier, DG has two functions. First, it compresses the original input data xi to zi using
its encoder to reduce the number of dimensions of xi and consequently reduces the mapping space of
DPG to make the generation of the parameters pi easier. The compression is formulated by:
zi = DGE(xi, θe)	(6)
2Note that in this case, the parameters in R is a subset of parameters (θ*) for solver S. To simplify the
expression and without causing confusion, we don’t introduce new symbols and let the symbols unchanged.
3 Note that constraining only the units in the last layer can already achieve good results.
4
Published as a conference paper at ICLR 2019
where DGE is the encoder of DG with parameters θe .
Second, it generates the replayed data for previous tasks to deal with forgetting in solver S and DPG.
Note that DG differs from data generators in the existing generative replay (GR) methods (Shin et al.,
2017) as GR needs to generate both the replay data x0m and their labels ym0 (using the solver learned
so far), which can be noisy. DG only generates the data x0m but not the labels (which are not needed
by our approach), and then the labeling errors won’t affect our model PGMA, but will hurt GR.
Each replayed sample x0m is generated by the decoder of DG, called DGD :
x0m = DGD(z4 smample, θd0)
(7)
where zsmample is the mth sample sampled from the multivariate normal distribution.4 θd0 is the set of
parameters of DGD before optimizing the current task Ti .
DG can be implemented with an auto-encoder, e.g., VAE-like (Variational Auto-Encoder (Kingma &
Welling, 2013)) and WAE-like (Wasserstein Auto-Encoder) auto-encoders. We use WAE in DG since
it can let different examples get a chance to stay far away from each other, which promotes better
reconstruction (Tolstikhin et al., 2017).5 Note that DG also suffers from forgetting, which is dealt
with like DPG and also using specially designed losses (see below).
To train DG, we use mean square error as the reconstruction loss to enable its replay ability of the
past data, and add to it the penalized form of the Wasserstein distance between the distribution of zi
and multivariate normal distribution to help generate data (Tolstikhin et al., 2017) (together denoted
as Lwae).
Note also since we only have
one data generator DG, it
has the forgetting problem
caused by incremental train-
ing of new tasks too. To
avoid forgetting in DG, we
investigated using loss func-
tions to constrain DG to over-
come its forgetting as shown
in Figure 2. We describe this
approach here:
DGQ;, θ)
reconstruction
(used in r,,l.1)
■— Encoder
Decoder-----
dggm)
z.
J
Randomly
Eq. 8 (constraint)
matching multivariate
normal distribution
(used in C,.,„ )
Decoder
DGH 协)I
Encoder
DGE(XO
1.9
(constraint)
Figure 2: Data Generator training pipeline demonstration.
M
min X ∣∣zmample - DGE(xm,θe)∣∣	⑻
m=1
M
min X ||DGD(zsmample, θd) - x0m ||	(9)
m=1
where x0m is the replayed data (see Eq. 7), θe and θd are the encoder’s and decoder’s parameters
of DG in the process of learning the new task Ti, respectively, and zsmample and M have the same
meanings as they are in DPG&S. Eq. 8 constrains the consistency of DG’s decoder and encoder over
the randomly sampled zsmample . Eq. 9 ensures that DG’s decoder can still remember the old data.
Using Eqs. 8 and 9, we can maintain DG’s ability to reflect the old data. Overall, the final objective
function for DG is composed by Lwae, Eq. 8 and 9.
2.4	The complete Training Procedure
Finally, we summarize the whole training procedure of PGMA in Algorithm 1, which is self-
explanatory. “//” is followed by comments. The test procedure is straightforward, see Section 2.1.
4In training, we force zi to satisfy the multivariate normal distribution. We can then sample many zsmample
from the multivariate normal distribution.
5Our approach is not limited to using WAE. Other auto-encoders may also be applied.
5
Published as a conference paper at ICLR 2019
Algorithm 1 PGMA (Parameter Generation and Model Adaptation) training
Input: T = {Ti}iN=1, where Ti = {xij,yij}
Initial: Randomly initialize f (∙), DG, S;
// Learning the first task
for all n = 0, . . . , until convergence do
5:	Sample a mini-batch from T1;
// Training DG
Minimize Lwae and then update DG;
// Training DPG and S. pn and Sn below
// denote a batch of generated parameters
10:	// and adapted solvers, respectively
Compute pn using Eqs. 1 & 6;	// Section 2.2
Construct Sn using pn and θ0 ;	// Section 2.2
Minimize Lce then update f (∙) and S;
end for
15: // Learning the subsequent tasks
for all i = 2; i ≤ N ; i + + do
Generate replayed samples x0m by DG;
for all n = 0, . . . , until convergence do
Sample a mini-batch from Ti ;
20:	Minimize Lwae, Eqs. 8 &9 and then update
DG;	// Section 2.3
Compute pn using Eqs. 1 & 6;
Construct Sn using pn and θ0 ;
Minimize Lce, Eq. 5, and then update f (∙)
and S; // Section 2.2
end for
25: end for
3	Experiments
We now evaluate the proposed approach PGMA6 and compare it with state-of-the-art baselines using
two image datasets and two text datasets.
Datasets
•	Two image datasets: (1) MNIST: this dataset consists of 70,000 images of handwritten digits from
0 to 9. We use 60,000/3000/7000 images for training/validation/testing respectively. (2) CIFAR-10:
this dataset consists of 60,000 32x32 color images of 10 classes, with 6000 images per class. There
are 50,000/3000/7000 images for training/validation/testing respectively.
•	Two text datasets: (1) DBPedia ontology: this is a crowd-sourced dataset (Lehmann et al.,
2015) with 560,000 training samples and 70,000 test samples. Out of the 70,000 test samples,
we use 10,000 for validation and 60,000 for test. (2) THUCNews: this dataset consists of 65,000
sentences of 10 classes (Li et al., 2006). We randomly select 50,000/5000/10,000 sentences for
training/validation/testing respectively.
Experiment Settings
Data Preparation: To simulate sequential learning, we adopt the same two data processing methods
as in (Lee et al., 2017), named disjoint and shuffled.
•	Disjoint: This method divides each dataset into several subsets of classes. Each subset is a task.
For example, we divide the MNIST dataset into two tasks (or subsets of classes). The first task
consists of digits (classes) {0, 1, 2, 3, 4} and the second task consists of the remaining digits (classes)
{5, 6, 7, 8, 9}. The systems learn the two subsets as two tasks in a sequential fashion and regard
them together as 10-class classification. In order to consider more tasks in testing, for MNIST,
CIFAR-10, and THUCNews, which all have 10 classes, we created two experiment settings of 2 tasks
(5 classes per task) and 5 tasks (2 classes per task). For DBPedia, which has 14 classes, we created
three experiment settings, 2 tasks (7 classes per task), 3 tasks (5, 5, and 4 classes for the three tasks
respectively), and 5 tasks (3, 3, 3, 3, and 2 classes for the 5 tasks respectively).
•	Shuffled: This method shuffles the input pixels of an image with a fixed random permutation. Two
experiment settings were created: 3 tasks and 5 tasks. In both cases, the dataset for the first task is
the original dataset. The datasets for the rest of the tasks are constructed through shuffling. Since
shuffling of words in a sentence will change the sentence meaning and results in confusion, thus this
experiment is not done on text datasets.
Baselines: We use three state-of-the-art baselines that are representative of the current approaches:
1) EWC (elastic weight consolidation) (Kirkpatrick et al., 2017); 2) IMM (incremental moment
matching) (Lee et al., 2017); 3) GR (generative replay) (Shin et al., 2017). We use the open source
code released by the authors or the third party for comparison. We use Adam algorithm to update the
parameters and also use Adam as a baseline to show how serious the forgetting problem is.
6https:〃github.com/morning-dews/PGMA_tensorflow
6
Published as a conference paper at ICLR 2019
Training Details: For fair comparison, our proposed approach uses the same solver (or classifier) as
the baselines. That is, a multilayer perceptron is adopted as the solver/classifier (as the baselines all
use this method), which is a 3-layer network (i.e., two basic units with each hidden layer as a unit)
followed by a softmax layer. For our approach, the total number of parameters in the solver includes
both the generated parameters p and the shared parameters θ0 . Due to the differences among different
datasets, we adopt different settings for them, see Table 5 in Appendix for details. All baselines and
our approach use the same setting for the same dataset. We use a 3-layer perceptron (with 2 hidden
layers) network (we also call it T-net) for DPG and set the size of each hidden layer to 1000. Each
T-net can generate 100 parameters at a time. We can parallel several T-nets in the DPG to generate
more parameters when needed. The network parameters are updated using the Adam algorithm with
a learning rate of 0.001.
Results and Analysis
Figure 3 shows the test accuracy plots per 40 training steps of each method for each task as the tasks
are sequentially learned. IMM is not shown here because IMM works by combining well trained
models of individual tasks to form one joint model for all tasks. Thus, we cannot draw an accuracy
curve with increased training steps like others. However, its final results and those of the other
systems are given in Tables 1 and 2. Shuffled CIFAR-10 is also not included in the figure because our
experiments show that it cannot be learned by CNN. It is also not used in any existing paper. Note
that due to space limitations, Figure 3 only plots the results of those settings with only 2 and 3 tasks.
From Figure 3, we can make the following observations:
(1)	. The proposed PGMA method consistently outperforms the baselines in overcoming forgetting,
by a big margin in most cases.
(2)	. EWC does not perform well for the disjoint setting. GR is better but still poorer than our method.
Adam’s results show that forgetting is very serious for all datasets and settings.
Time efficiency: Due to DPG, our method uses slightly more time than baselines, under 25% for
training and under 28% for testing compared to GR, which is efficient. This is a small price to pay
for the major gain in accuracy in dealing with catastrophic forgetting.
Memory usage: There is no need for our method to save data, which results in a large memory
saving. For more details about time efficiency and memory usage, please see Appendix (Section C).
Table 1: Average accuracy over all tasks in a sequence after the tasks have all been learned.
Model	shuffled MNIST (3 tasks)	disjoint MNIST (2 tasks)	disjoint CIFAR-10 (2 tasks)	THUCNews (2 tasks)	DBPedia (2 tasks)	DBPedia (3 tasks)
Adam	91.46	48.64	41.99	46.78	47.66	~^41.10
EWC	96.70	48.96	37.75	45.02	53.95	35.89
GR	97.57	89.96	65.11	81.55	88.41	82.14
IMM	97.92	94.12	62.98	80.32	92.65	85.31
Our PGMA	98.14	96.53	69.51	85.12	94.70	88.06
From Table 1, we can see that our PGMA method is consistently superior to EWC, GR, and IMM on
different datasets with 2 or 3 tasks (5 tasks results are given in Table 2). We believe that is because
our PGMA model can reduce the effect of the accuracy deterioration problem discussed in Section 1.
Adam’s results show that forgetting is very serious as it does not deal with the forgetting problem.
EWC’s performance is poor for the disjoint case (a more realistic setting in practice), which is also
reported by other researchers (Lee et al., 2017). Appendix has the results for only the last task.
Table 2: Average accuracy over 5 tasks in a sequence after the tasks have all been learned.
Model	shuffled MNIST	disjoint MNIST	DBPedia	CIFAR-10	THUCNews
GR	94.54	75.47	^^63.71 ^^	31.09	47.35
IMM	96.09	67.25	64.04	32.36	46.61
Our PGMA	96.77	81.70	69.68	40.47	52.93
Table 2	shows the results for 5 tasks. EWC and Adam are not included as they performed poorly.
Again, we observe that our PGMA method is markedly better than the baselines.
7
Published as a conference paper at ICLR 2019
0 5 0 5 0 5 0 5 0 5 0 5
0 9 9 8 0 9 9 8 0 9 9 8
■ ■■■ ■■■■ ■■■■
Iooo Iooo Iooo
I*seJ. Nxse.L*seJ.
Task I	Task 2	Task 3
Training time
(a) Shuffled MNIST (3 tasks)
1.0
0.8
0.6
0.4
0.2
0.0
1.0
,jf-
Taskl
Task 2
Taskl	Task 2
Esel
NXSel 工 SelNYSel
Our MA
GR
EWC
—Adam
Training time
(b) Disjoint MNIST (2 tasks)
Training time
(C) Disjoint CIFAR-10 (2 tasks)
Task 1	Task 2
ι.o- J	----1_ . 一
工seɪ
0.0
1.0
0.8
5 0.6
S
U 0.4
0.2
Training time
(e)	DBPedia (2 tasks)
(d) THUCNews (2 tasks)
ι.o-
ιθ∙8-
.V 0.6-
S
® 0.4-
I- 0.2-
o.o-
ι.o-
0.8-
0.6-
0.4-
0.2-
o.o-
1.0-
0.8-
0.6-
0.4-
0.2-
Task 1	Task 2	Task 3
Training time
(f)	DBPedia (3 tasks)
Nxsel
NXSel 2sel



Figure 3: ACCuraCy Curves - we test the system’s aCCuraCy per 40 training steps on the test set of eaCh
method. Y-axis shows the aCCuraCy of different tasks as the training time (steps) inCreases. Note that
Shuffled CIFAR-10 is not inCluded as CNN Cannot learn it direCtly.
Ablation Study
Here we study how the system behaves with less and less parameters in θ0 or more and more
parameters replaCed by the parameters generated by DPG. We seleCted the disjoint MNIST 2 tasks
setting to ConduCt the experiment as it is more useful than the shuffled setting and also more diffiCult.
Table 3:	Average aCCuraCy with different % of parameters in solver’s last layer (one basiC unit) being
replaCed by the parameters generated by DPG. EaCh sCore followed by a 95% ConfidenCe interval.
Model	GR	Our PGMA
		0%	20%	40%	60%	80%	100%
Accuracy	88.71 (±2.64)	90.96 (±0.69)	92.44(±1.31)	93.96(±0.63)	94.51(±0.87)	96.07(±0.62)	96.05(±0.64)
Table 3 shows the aCCuraCy results (averaged in the same way as in Table 1) when a portion of the
parameters in only the last layer of the solver is replaCed by the parameters generated by DPG.
We observe that the aCCuraCy improves with inCreased perCentages of parameters being replaCed.
8
Published as a conference paper at ICLR 2019
The best accuracy is obtained when 80% of the parameters in the last layer are replaced through
DPG, and the accuracy won’t further improve with more replaced parameters. This observation
indicates that replacing a part of parameters in solver to adapt new input tasks is sufficient. The same
conclusion can also be made by replacing the parameters in the first hidden layer of the solver (which
has 2 hidden layers). We fix the replacing percentage of the last layer to 20%, and then increase the
replacing percentages of the first layer. The best accuracy reaches 94.31%(±0.84%) when replacing
40% parameters of the first layer, which gains only 1.87% in accuracy compared with no replacement.
This result indicates that it suffices to replace the parameters in the last layer.
Table 4 shows the contribution of different components.
We can see a significant drop if DPG is removed. The
second row gives the performance of our model when only
DG and constraints are used. The third row shows the
result without using constraints (DG+label(like GR)) but
replacing them with the replay method. We can see that
constraints work better than predicting labels.
Table 4: Empirical evaluation of differ-
ent components
Components
DPG + DG + Constraints
DG + Constraints
DG + label (like GR)
Acc (%)
96.07 ± 0.62
90.96 ± 0.69
88.21 ± 2.81
4 Related Work
Many approaches have been proposed to deal with catastrophic forgetting (CF), which is one of the
challenging problems of neural networks for lifelong learning (Chen & Liu, 2018). EWC (Kirkpatrick
et al., 2017) quantifies the importance of weights to previous tasks, and selectively alters the learning
rates of weights. Following EWC, Zenke et al. (2017) measured the synapse consolidation strength
in an online fashion and used it as regularization. Learning without forgetting (LWF) (Li & Hoiem,
2016) feeds the old network with new training data in new tasks and regards the output as ”pseudo-
labels”. In Incremental Moment Matching (IMM) (Lee et al., 2017), each trained network on one
task is preserved and all networks are merged into one at the end of the sequence of tasks.
Above approaches focus on adjusting the network weights. Another main approach is to add some
data of past tasks to the new task training to prevent forgetting the past. Gradient Episodic Memory
(GEM) (Lopez-Paz et al., 2017) stores a subset of training data for every finished task, and limits the
loss function on these so-called ”memories”. Instead of keeping some real data from previous tasks,
Generative Replay (GR) (Shin et al., 2017) keeps data generators for previous tasks and learns using
a mix of real data of the new tasks and replayed data of previous tasks. Seff et al. (2017) proposed to
solve continual generative modeling by combining the ideas of data generation and EWC.
Other existing approaches include iCaRL (Rebuffi et al., 2017), Pathnet (Fernando et al., 2017),
memory aware synapses (Aljundi et al., 2017), phantom sampling (Venkatesan et al., 2017), active
long term memory networks (Furlanello et al., 2016), conceptor-aided backprop (He & Jaeger, 2018),
gating networks (Masse et al., 2018; Serra et al., 2018), dynamically expandable networks (DEN)
(Yoon et al., 2018), progress & compress (Schwarz et al., 2018) (active column is distilled into the
knowledge base, taking care to protect any previously acquired skills), and incremental regularized
least squares (Camoriano et al., 2017). Most of those works suffer from accuracy deterioration as
discussed in Section 1 while some dynamically expanding the network size (e.g., DEN and LWF),
which results in a huge and complex model. Our method is different from these existing approaches.
Several existing works generate parameters, but they are for different purposes. Denil et al. (2013)
trained several different architectures by learning only a small number of weights and predicting the
rest. Ha et al. (2016) used a small network to generate the weights for a larger network. Brock et al.
(2017) learned an auxiliary HyperNet to generate weights for the main model. Our method uses the
generated parameters for model adaptation for continual learning.
5 Conclusion
This paper proposed a novel approach PGMA to dealing with catastrophic forgetting. The approach
learns to build a model with two sets of parameters. The first set is shared by all tasks learned so far
and the second set is dynamically generated to adapt the model (solver) to suit each individual test
example. As we discussed in related work, this is different from all existing methods. Experimental
results showed that the proposed approach outperformed the existing baseline methods markedly.
9
Published as a conference paper at ICLR 2019
Acknowledgments
We thank Zhangming Chan and Shen Gao for helping check the code. This work was supported by
the National Key Research and Development Program of China (No. 2017YFC0804001). Bing Liu’s
work was partially supported by National Science Foundation (NSF) under grant no. IIS 1838770, by
a research contract with Huawei Technologies Co. Ltd., and by a research gift from Tencent Holdings
Limited. Rui Yan’s work was supported by the National Science Foundation of China (NSFC No.
61672058; NSFC No. 61876196), CCF-Tencent Open Research Fund and Microsoft Research Asia
(MSRA) Collaborative Research Program.
References
Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars.
Memory aware synapses: Learning what (not) to forget. arXiv preprint arXiv:1711.09601, 2017.
Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. Smash: one-shot model
architecture search through hypernetworks. arXiv preprint arXiv:1708.05344, 2017.
Raffaello Camoriano, Giulia Pasquale, Carlo Ciliberto, Lorenzo Natale, Lorenzo Rosasco, and
Giorgio Metta. Incremental robot learning of new objects with fixed update time. In Robotics and
Automation (ICRA), 2017 IEEE International Conference on, pp. 3207-3214. IEEE, 2017.
Zhiyuan Chen and Bing Liu. Lifelong machine learning. Morgan & Claypool Publishers, 2018.
Misha Denil, Babak Shakibi, Laurent Dinh, Nando De Freitas, et al. Predicting parameters in deep
learning. In Advances in neural information processing systems, pp. 2148-2156, 2013.
Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A Rusu,
Alexander Pritzel, and Daan Wierstra. Pathnet: Evolution channels gradient descent in super neural
networks. arXiv preprint arXiv:1701.08734, 2017.
Tommaso Furlanello, Jiaping Zhao, Andrew M Saxe, Laurent Itti, and Bosco S Tjan. Active long
term memory networks. arXiv preprint arXiv:1606.02355, 2016.
David Ha, Andrew Dai, and Quoc V Le. Hypernetworks. arXiv preprint arXiv:1609.09106, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2016.
Xu He and Herbert Jaeger. Overcoming catastrophic interference using conceptor-aided backpropa-
gation. 2018.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In CVPR, volume 1, pp. 3, 2017.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming
catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, pp.
201611835, 2017.
Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha, and Byoung-Tak Zhang. Overcoming
catastrophic forgetting by incremental moment matching. In Advances in Neural Information
Processing Systems, pp. 4652-4662, 2017.
10
Published as a conference paper at ICLR 2019
Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N Mendes,
Sebastian Hellmann, Mohamed Morsey, Patrick Van Kleef, Soren Auer, et al. DbPedia-a large-
scale, multilingual knowledge base extracted from wikipedia. Semantic Web, 6(2):167-195,
2015.
Jingyang Li, Maosong Sun, and Xian Zhang. A comparison and semi-quantitative analysis of words
and character-bigrams as features in chinese text categorization. In ACL, 2006.
Shen Li, Zhe Zhao, Renfen Hu, Wensi Li, Tao Liu, and Xiaoyong Du. Analogical reasoning on
chinese morphological and semantic relations. In Proceedings of the 56th Annual Meeting of the
Association for Computational Linguistics (Volume 2: Short Papers), pp. 138-143. Association for
Computational Linguistics, 2018. URL http://aclweb.org/anthology/P18-2023.
Zhizhong Li and Derek Hoiem. Learning Without Forgetting. ECCV, 9908(1):614-629, 2016.
David Lopez-Paz et al. Gradient episodic memory for continual learning. In Advances in Neural
Information Processing Systems, pp. 6467-6476, 2017.
Nicolas Y Masse, Gregory D Grant, and David J Freedman. Alleviating catastrophic forgetting using
context-dependent gating and synaptic stabilization. arXiv preprint arXiv:1802.01569, 2018.
Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The
sequential learning problem. In Psychology of learning and motivation, volume 24, pp. 109-165.
Elsevier, 1989.
German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual
lifelong learning with neural networks: A review. arXiv preprint arXiv:1802.07569, 2018.
Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word
representation. In Empirical Methods in Natural Language Processing (EMNLP), pp. 1532-1543,
2014. URL http://www.aclweb.org/anthology/D14-1162.
Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl:
Incremental classifier and representation learning. In Proc. CVPR, 2017.
Jonathan Schwarz, Jelena Luketina, Wojciech M Czarnecki, Agnieszka Grabska-Barwinska,
Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress & compress: A scalable frame-
work for continual learning. arXiv preprint arXiv:1805.06370, 2018.
Ari Seff, Alex Beatson, Daniel Suo, and Han Liu. Continual learning in generative adversarial nets.
arXiv preprint arXiv:1705.08395, 2017.
Joan Serra, Dldac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic
forgetting with hard attention to the task. arXiv preprint arXiv:1801.01423, 2018.
Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative
replay. In Advances in Neural Information Processing Systems, pp. 2990-2999, 2017.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
In Advances in neural information processing systems, pp. 3104-3112, 2014.
Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein auto-encoders.
arXiv preprint arXiv:1711.01558, 2017.
Ragav Venkatesan, Hemanth Venkateswara, Sethuraman Panchanathan, and Baoxin Li. A strategy
for an uncompromising incremental learner. arXiv preprint arXiv:1705.00744, 2017.
Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang, and Ming Zhou. Gated self-matching networks
for reading comprehension and question answering. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pp. 189-198,
2017.
Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong learning with dynamically
expandable networks. 2018.
Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence.
arXiv preprint arXiv:1703.04200, 2017.
11
Published as a conference paper at ICLR 2019
APPENDIX
A Proof for Question 1
Proof: Our objective is to minimize ||R(x, θ*) - R(χ, θ*-ι)||. Since R(∙) is differentiable and its
derivative function is bounded (as it uses I-LiPsChitZ activation function, e.g., RELU or tanh), R(∙)
thus satisfies the Lipschitz condition. After some manipulation, we obtain:
≤
≤
∣∣R(x,θ*)-R(χ,θt-ι)∣∣
∣[R(x,θf) -R(x0,θ*) + R(x0,θ*) -R(χ0,θ*-ι) + R(x0,θi-1)-R(x,θ二i)||
∣∣R(χ,θ*) -R(χ0,θ*)∣∣ + ∣∣R(χ0,θD-R(xo,θ二ι)ll + ∣∣R(χ0,θ二ι)-R(χ,θ^ι)∣∣
Lι∣l(χ - X0)ll + I∣R(x0,θ;) - R(χ0,θl-ι)ll + L2∖∖(χ - χ0)ll	(IO)
||R(X0,θS)- R(x0 ,θi-1)∖∖ +(LI + L2 )∖∖(x - x0)∖∖
V-----------{z-----------}
minimize rePlayed samPles
where the last inference is based on LiPschitz continuity, and L1 and L2 are LiPschitz constants. We
can see that if the solver has a small LiPschitz constant or the DG’s reconstruction error is small,
minimizing Eq. 4 is consistent with constraining using the original data.
B Parameter Settings
Table 5: Parameter settings for different datasets
MNIST: We set the basic unit (hidden layer) size of the 3-layer classifier to 600 and the droPout rate to 0.3. A 3-layer convolu-
tional network (CNN), with 2 * 2 convolutions and 64, 128, 256 filters for each layer, is used as DG’s encoder. A deconvolutional
network with the symmetric setting as the encoder is used as the decoder in DG. For our model, we use the Parameters generated
by DPG to rePlace 33% of the Parameters in the solver’s last hidden layer, and 10% of the first hidden layer.
CIFAR-10: We set the basic unit (hidden layer) size of the 3-layer classifier to 1000 and the droPout rate to 0.5. For our model,
we use the Parameters generated by DPG to rePlace 25% Parameters of the solver’s last hidden layer, and 10% of the first hidden
layer. Since each image in this dataset contains a comPlex background and is thus more difficult to classify, we add a 3-layer
CNN below the 3-layer MLP classifier to imProve the Performance. 3-layer CNN has the same setting as the encoder of DG.
Due to the comPlexity of the images in CIFAR-10, the rePlayed images are usually noisy. Text datasets usually cannot be exactly
rePlayed. To alleviate this Problem, we ProPose to rePlay the features (e.g. using an additional CNN to extract features) of inPut
data which Performs well in the exPeriment. In that case, the 3-layer CNN added to the classifier Plays an imPortant role in
feature extraction and it needs to be fixed; if not, the system won’t get a stable inPut and thus will cause forgetting. Note that the
feature extractor can be Pre-trained using a large dataset (e.g. ImageNet for images and wikiPedia for text.) and thus can Provide
sufficient features.
Text Datasets: We set the basic unit (hidden layer) size of the 2-layer classifier to 1000 and droPout rate to 0.5. For our model,
we use the Parameters generated by DPG to rePlace 25% Parameters of the solver’s last hidden layer, and 10% of the first hidden
layer. To get better results on text, we use Pre-trained embeddings (Pennington et al., 2014; Li et al., 2018) for all exPeriments.
C More Experimental Results
Memory
•	The memory used by our model is around 151.49MB, by IMM is around 38.96MB * task_num
(task_num will be 3 if there are three tasks), and by GR is around 70.13MB. The memory required by
all these systems are very small as comPared to the total memory available in a modern comPuter.
•	Our data generator DG has around 1,460,361 Parameters (around 44.57MB, calculated by 32 Byte
Per Parameter), which is fixed. Saving data would need much more memory. Taking the MNIST
dataset as an examPle. It has 60,000 training samPles and the size of each samPle is 28*28. The
memory needed to store the data is 28*28*60,000 = 47,040,000 (around 358.89MB, calculated by 8
Byte Per unit). And this number multiPlies when the number of tasks increases.
Training and Test Time
Taking the CIFAR10 dataset as an examPle (others are similar), the total time used by our model and
baselines are shown in Table 6. We can see that our method needs a bit more time than baselines but
not too much (under 25% for training and under 28% for testing comPared with GR). This is a small
Price to Pay for the major gain in accuracy and in dealing with the catastroPhic forgetting Problem.
Accuracy on the Last Task
12
Published as a conference paper at ICLR 2019
Table 6: Empirical evaluation of different components
Time	EWC	IMM	GR	Our PGMA
Training Time/per epoch (s)	2.460	4.930	8.673	10.836
Test time(s)	0.703	0.707	0.728	0.930
ADAM gets the best accuracy on the last task as Adam optimizer learns the new task only and does
not need to worry about the forgetting problem on the old tasks. For the last task, our PGMA system
obtained comparable accuracy on average with the baselines designed for overcoming forgetting.
However, we are much better at preventing forgetting of old tasks. The results for the last task are
given in Table 7. Note that although EWC is quite good at the last task but it suffers seriously from
forgetting as shown in Table 1 or Figure 3, which is consistent with results reported in other papers.
Table 7: Accuracy of the last task after all tasks have been learned.
Model	shuffled MNIST	shuffled MNIST	disjoint MNIST	disjoint MNIST	disjoint CIFAR-10	THUCNews	DBPedia	DBPedia	DBPedia
	3 tasks	5 tasks	2 tasks	5 tasks	2 tasks	2 tasks	2 tasks	3 tasks	5 tasks
Adam	-98.30^^	98.27	97.28	95.78	83.98	93.56	95.30	97.52	98.45
EWC	98.12	97.99	96.97	95.61	75.38	86.72	96.11	96.55	93.32
GR	98.10	97.18	96.34	92.31	77.82	87.20	93.18	89.40	98.76
IMM	97.06	95.36	95.07	86.94	48.74	65.08	93.84	85.72	70.19
Our PGMA	98.15	97.26	96.22	93.47	73.75	86.36	96.01	91.14	82.99
13