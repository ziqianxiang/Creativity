Published as a conference paper at ICLR 2019
Understanding Straight-Through Estimator
in Training Activation Quantized Neural Nets
Penghang Yin,* Jiancheng LyuJ Shuai ZhangJ Stanley Osher,* Yingyong QiJ Jack Xint
"Department of Mathematics, University of California, Los Angeles
yph@ucla.edu, sjo@math.ucla.edu
tDepartment of Mathematics, University of California, Irvine
jianchel@uci.edu, jxin@math.uci.edu
^ Qualcomm AI Research, San Diego
{shuazhan,yingyong}@qti.qualcomm.com
Ab stract
Training activation quantized neural networks involves minimizing a piecewise
constant function whose gradient vanishes almost everywhere, which is undesir-
able for the standard back-propagation or chain rule. An empirical way around
this issue is to use a straight-through estimator (STE) (Bengio et al., 2013) in
the backward pass only, so that the “gradient” through the modified chain rule
becomes non-trivial. Since this unusual “gradient” is certainly not the gradient of
loss function, the following question arises: why searching in its negative direction
minimizes the training loss? In this paper, we provide the theoretical justification
of the concept of STE by answering this question. We consider the problem of
learning a two-linear-layer network with binarized ReLU activation and Gaussian
input data. We shall refer to the unusual “gradient” given by the STE-modifed
chain rule as coarse gradient. The choice of STE is not unique. We prove that
if the STE is properly chosen, the expected coarse gradient correlates positively
with the population gradient (not available for the training), and its negation is a
descent direction for minimizing the population loss. We further show the asso-
ciated coarse gradient descent algorithm converges to a critical point of the popu-
lation loss minimization problem. Moreover, we show that a poor choice of STE
leads to instability of the training algorithm near certain local minima, which is
verified with CIFAR-10 experiments.
1	Introduction
Deep neural networks (DNN) have achieved the remarkable success in many machine learning ap-
plications such as computer vision (Krizhevsky et al., 2012; Ren et al., 2015), natural language
processing (Collobert & Weston, 2008) and reinforcement learning (Mnih et al., 2015; Silver et al.,
2016). However, the deployment of DNN typically require hundreds of megabytes of memory
storage for the trainable full-precision floating-point parameters, and billions of floating-point op-
erations to make a single inference. To achieve substantial memory savings and energy efficiency
at inference time, many recent efforts have been made to the training of coarsely quantized DNN,
meanwhile maintaining the performance of their float counterparts (Courbariaux et al., 2015; Raste-
gari et al., 2016; Cai et al., 2017; Hubara et al., 2018; Yin et al., 2018b).
Training fully quantized DNN amounts to solving a very challenging optimization problem. It calls
for minimizing a piecewise constant and highly nonconvex empirical risk function f (w) subject
to a discrete set-constraint w ∈ Q that characterizes the quantized weights. In particular, weight
quantization of DNN have been extensively studied in the literature; see for examples (Li et al., 2016;
Zhu et al., 2016; Li et al., 2017; Yin et al., 2016; 2018a; Hou & Kwok, 2018; He et al., 2018; Li&
Hao, 2018). On the other hand, the gradient Vf (W) in training activation quantized DNN is almost
everywhere (a.e.) zero, which makes the standard back-propagation inapplicable. The arguably
most effective way around this issue is nothing but to construct a non-trivial search direction by
1
Published as a conference paper at ICLR 2019
properly modifying the chain rule. Specifically, one can replace the a.e. zero derivative of quantized
activation function composited in the chain rule with a related surrogate. This proxy derivative used
in the backward pass only is referred as the straight-through estimator (STE) (Bengio et al., 2013). In
the same paper, Bengio et al. (2013) proposed an alternative approach based on stochastic neurons.
In addition, Friesen & Domingos (2017) proposed the feasible target propagation algorithm for
learning hard-threshold (or binary activated) networks (Lee et al., 2015) via convex combinatorial
optimization.
1.1	Related Works
The idea of STE originates to the celebrated perceptron algorithm (Rosenblatt, 1957; 1962) in 1950s
for learning single-layer perceptrons. The perceptron algorithm essentially does not calculate the
“gradient” through the standard chain rule, but instead through a modified chain rule in which the
derivative of identity function serves as the proxy of the original derivative of binary output function
1{x>0} . Its convergence has been extensive discussed in the literature; see for examples, (Widrow
& Lehr, 1990; Freund & Schapire, 1999) and the references therein. Hinton (2012) extended this
idea to train multi-layer networks with binary activations (a.k.a. binary neuron), namely, to back-
propagate as if the activation had been the identity function. Bengio et al. (2013) proposed a STE
variant which uses the derivative of the sigmoid function instead. In the training of DNN with
weights and activations constrained to ±1, (Hubara et al., 2016) substituted the derivative of the
signum activation function with 1{∣χ∣≤i} in the backward pass, known as the saturated STE. Later the
idea of STE was readily employed to the training of DNN with general quantized ReLU activations
(Hubara et al., 2018; Zhou et al., 2016; Cai et al., 2017; Choi et al., 2018; Yin et al., 2018b), where
some other proxies took place including the derivatives of vanilla ReLU and clipped ReLU. Despite
all the empirical success of STE, there is very limited theoretical understanding ofit in training DNN
with stair-case activations.
Goel et al. (2018) considers leaky ReLU activation of a one-hidden-layer network. They showed
the convergence of the so-called Convertron algorithm, which uses the identity STE in the backward
pass through the leaky ReLU layer. Other similar scenarios, where certain layers are not desirable for
back-propagation, have been brought up recently by (Wang et al., 2018) and (Athalye et al., 2018).
The former proposed an implicit weighted nonlocal Laplacian layer as the classifier to improve
the generalization accuracy of DNN. In the backward pass, the derivative of a pre-trained fully-
connected layer was used as a surrogate. To circumvent adversarial defense (Szegedy et al., 2013),
(Athalye et al., 2018) introduced the backward pass differentiable approximation, which shares the
same spirit as STE, and successfully broke defenses at ICLR 2018 that rely on obfuscated gradients.
1.2	Main Contributions
Throughout this paper, we shall refer to the “gradient” of loss function w.r.t. the weight variables
through the STE-modified chain rule as coarse gradient. Since the backward and forward passes do
not match, the coarse gradient is certainly not the gradient of loss function, and it is generally not the
gradient of any function. Why searching in its negative direction minimizes the training loss, as this
is not the standard gradient descent algorithm? Apparently, the choice of STE is non-unique, then
what makes a good STE? From the optimization perspective, we take a step towards understanding
STE in training quantized ReLU nets by attempting these questions.
On the theoretical side, we consider three representative STEs for learning a two-linear-layer net-
work with binary activation and Gaussian data: the derivatives of the identity function (Rosenblatt,
1957; Hinton, 2012; Goel et al., 2018), vanilla ReLU and the clipped ReLUs (Cai et al., 2017;
Hubara et al., 2016). We adopt the model of population loss minimization (Brutzkus & Globerson,
2017; Tian, 2017; Li & Yuan, 2017; Du et al., 2018). For the first time, we prove that proper choices
of STE give rise to training algorithms that are descent. Specifically, the negative expected coarse
gradients based on STEs of the vanilla and clipped ReLUs are provably descent directions for the
minimizing the population loss, which yield monotonically decreasing energy in the training. In
contrast, this is not true for the identity STE. We further prove that the corresponding training algo-
rithm can be unstable near certain local minima, because the coarse gradient may simply not vanish
there.
2
Published as a conference paper at ICLR 2019
Complementary to the analysis, we examine the empirical performances of the three STEs on
MNIST and CIFAR-10 classifications with general quantized ReLU. While both vanilla and clipped
ReLUs work very well on the relatively shallow LeNet-5, clipped ReLU STE is arguably the best
for the deeper VGG-11 and ResNet-20. In our CIFAR experiments in section 4.2, we observe that
the training using identity or ReLU STE can be unstable at good minima and repelled to an inferior
one with substantially higher training loss and decreased generalization accuracy. This is an impli-
cation that poor STEs generate coarse gradients incompatible with the energy landscape, which is
consistent with our theoretical finding about the identity STE.
To our knowledge, convergence guarantees of perceptron algorithm (Rosenblatt, 1957; 1962) and
Convertron algorithm (Goel et al., 2018) were proved for the identity STE. It is worth noting that
Convertron (Goel et al., 2018) makes weaker assumptions than in this paper. These results, however,
do not generalize to the network with two trainable layers studied here. As aforementioned, the
identity STE is actually a poor choice in our case. Moreover, it is not clear if their analyses can
be extended to other STEs. Similar to Convertron with leaky ReLU, the monotonicity of quantized
activation function plays a role in coarse gradient descent. Indeed, all three STEs considered here
exploit this property. But this is not the whole story. A great STE like the clipped ReLU matches
quantized ReLU at the extrema, otherwise the instability/incompatibility issue may arise.
Organization. In section 2, we study the energy landscape of a two-linear-layer network with binary
activation and Gaussian data. We present the main results and sketch the mathematical analysis for
STE in section 3. In section 4, we compare the empirical performances of different STEs in 2-
bit and 4-bit activation quantization, and report the instability phenomena of the training algorithms
associated with poor STEs observed in CIFAR experiments. Due to space limitation, all the technical
proofs as well as some figures are deferred to the appendix.
Notations. k ∙ k denotes the Euclidean norm of a vector or the spectral norm of a matrix. 0n ∈ Rn
represents the vector of all zeros, whereas 1n ∈ Rn the vector of all ones. In is the identity matrix
of order n. For any w, z ∈ Rn, w>z = hw, zi = Pi wizi is their inner product. w z denotes
the Hadamard product whose ith entry is given by (w z)i = wizi.
2 Learning Two-Linear-Layer CNN with B inary Activation
We consider a model similar to (Du et al., 2018) that outputs the prediction
m
y(Z, v, w) :=	viσ(Zi>w) = v>σ(Zw)
i=1
for some input Z ∈ Rm×n . Here w ∈ Rn and v ∈ Rm are the trainable weights in the first and
second linear layer, respectively; Zi> denotes the ith row vector of Z; the activation function σ acts
component-wise on the vector Zw, i.e., σ(Zw)i = σ((Zw)i) = σ(Zi>w). The first layer serves
as a convolutional layer, where each row Zi> can be viewed as a patch sampled from Z and the
weight filter w is shared among all patches, and the second linear layer is the classifier. The label
is generated according to y*(Z) = (v*)>σ(Zw*) for some true (non-zero) parameters v* and w*.
Moreover, we use the following squared sample loss
'(v, w; Z) := 2 (y(Z, v, w) - y*(Z))2 = 1 (v>σ(Zw) - y*(Z))2 .	(1)
Unlike in (Du et al., 2018), the activation function σ here is not ReLU, but the binary function
σ(x) = 1{x>0} .
We assume that the entries of Z ∈ Rm×n are i.i.d. sampled from the Gaussian distribution N (0, 1)
(Zhong et al., 2017; Brutzkus & Globerson, 2017). Since `(v, w; Z) = `(v, w/c; Z) for any scalar
c > 0, without loss of generality, we take kw* k = 1 and cast the learning task as the following
population loss minimization problem:
min
v∈Rm,w∈Rn
f(v, w) := EZ [`(v, w; Z)] ,
(2)
where the sample loss `(v, w; Z) is given by (1).
3
Published as a conference paper at ICLR 2019
2.1 Back-propagation and Coarse Gradient Descent
With the Gaussian assumption on Z, as will be shown in section 2.2, it is possible to find the analytic
expressions of f (v, w) and its gradient
Vf(v, W)
-∂V (v, w)'
f (V, w) 一
The gradient of objective function, however, is not available for the network training. In fact, we
can only access the expected sample gradient, namely,
_ 「d''	—
EZ ∂V (v, w; Z)
and EZ
,∂', i
∂W(v，w; Z)
We remark that EZ [∂∂W(v, w; Z)] is not the same as f (v, w) = dEz['dWw；Z)]. By the standard
back-propagation or chain rule, we readily check that
∂'
∂V(v, w; Z)= σ(Zw)(v>σ(Zw) - y (Z))	(3)
and
∂'
∂w(v, w; Z) = Z> (σ (Zw) Θ V)(^v>σ(Zw) - y (Z)).	(4)
Note that σ0 is zero a.e., which makes (4) inapplicable to the training. The idea of STE is to simply
replace the a.e. zero component σ0 in (4) with a related non-trivial function μ0 (Hinton, 2012; Bengio
et al., 2013; Hubara et al., 2016; Cai et al., 2017), which is the derivative of some (sub)differentiable
function μ. More precisely, back-propagation using the STE μ0 gives the following non-trivial sur-
rogate of ∂∂W (v, w; Z), to which We refer as the coarse (partial) gradient
gμ(v, w; Z) = Z>(μ0(Zw) Θ V乂v>σ(Zw) - y*(Z)).	(5)
Using the STE μ0 to train the two-linear-layer convolutional neural network (CNN) with binary
activation gives rise to the (full-batch) coarse gradient descent described in Algorithm 1.
Algorithm 1 Coarse gradient descent for learning two-linear-layer CNN with STE μ0.
Input: initialization V0 ∈ Rm, w0 ∈ Rn, learning rate η.
for t = 0, 1, . . . do
Vt+1 = Vt - η EZ [∂v (vt, wt; Z)]
wt+1 = wt - η EZ [gμ(vt, wt; Z)]
end for
2.2 Preliminaries
Let us present some preliminaries about the landscape of the population loss function f(V, w).
To this end, we define the angle between w and w* as θ(w, w*) := arccos (总>片；)for any
w = 0n. Recall that the label is given by y*(Z) = (v*)>Zw* from (1), we elaborate on the
analytic expressions of f(V, w) and Vf(V, w).
Lemma 1. If w 6= 0n, the population loss f(V, w) is given by
8 v> (Im + 1m1m)v - 2v> ( (1 - ∏θ(w, w*)) Im + 1m1m) v* + (V*)> (Im + 1m1m)v*
In addition, f (v, w) = 8(v* )>(Im + 1m1m)V* for w = 0n.
Lemma 2. Ifw 6= 0n and θ(w, w*) ∈ (0, π), the partial gradients off(V, w) w.r.t. V and w are
∂V (v, w) = 1 (Im + 1m】m)V - 4 ((l - ∏ θ(w, w*)) Im +	V*	⑹
4
Published as a conference paper at ICLR 2019
and
∂f
∂w
(v, w)
v>
ww>
2πk
ww>
(7)
—
v
*
—
w
2
w
*
—
w
2
w
*
respectively.
For any v ∈ Rm, (v, 0m) is impossible to be a local minimizer. The only possible (local) minimizers
of the model (2) are located at
1.	Stationary points where the gradients given by (6) and (7) vanish simultaneously (which
may not be possible), i.e.,
v>v* = 0 and V = (Im + 1m Im)T((I - 2 θ(w, w*)) Im + 1mim) v*.	(8)
2.	Non-differentiable points where θ(w, w*) = 0 and v = v*, or θ(w, w*) = π and v
(Im + 1m1m)	(ImIm - Im)v*.
Among them, {(v, w) : v = v*, θ(w, w*) = 0} are obviously the global minimizers of (2). We
show that the stationary points, if exist, can only be saddle points, and {(v, w) : θ(w, w*) =
π, v = (Im + 1m1m) 1(1m1mb - Im)v*} are the only potential spurious local minimizers.
Proposition 1. Ifthe true parameter v* satisfies (1mv*)2 < m+1 ∣∣v*∣∣2 ,then
(v , w) : v = (Im + 1m 1m )
(m+ι)kv*k2 -(Imv*产
Im + 1m1>m v*,
θ(w, w*)=2um^‰
(9)
give the saddle points obeying (8), and {(v, w) : θ(w, w*) = π, V = (Im + 1m,1m,) i(1m1mb
—
Im)v*} are the spurious local minimizers. Otherwise, the model (2) has no saddle points or spurious
local minimizers.
We further prove that the population gradient Vf (v, W) given by (6) and (7), is LiPschitz continuous
when restricted to bounded domains.
Lemma 3. For any differentiable points (v, W) and (V, W) with min{kwk,∣∣W∣∣} = Cw > 0 and
max{∣∣vk, kV∣∣} = Cv, there exists a Lipschitz constant L > 0 depending on Cv and Cw, such that
l∣Vf(v, w) -Vf(V,W)Il ≤ L∣∣(v, w) - (V,W)∣∣.
3 Main Results
We are most interested in the complex case where both the saddle points and spurious local mini-
mizers are present. Our main results are concerned with the behaviors of the coarse gradient descent
summarized in Algorithm 1 when the derivatives of the vanilla and clipped ReLUs as well as the
identity function serve as the STE, respectively. We shall prove that Algorithm 1 using the derivative
of vanilla or clipped ReLU converges to a critical point, whereas that with the identity STE does not.
Theorem 1 (Convergence). Let {(Vt, wt)} be the sequence generated by Algorithm 1 with ReLU
μ(x) = max{x, 0} or clipped ReLU μ(x) = min {max{x, 0}, 1}. Suppose ∣∣wt∣ ≥ Cw for all
t with some Cw > 0. Then if the learning rate η > 0 is sufficiently small, for any initialization
(V0, w0), the objective sequence {f(Vt, wt)} is monotonically decreasing, and {(Vt, wt)} con-
verges to a saddle point or a (local) minimizer of the population loss minimization (2). In addi-
tion, if 1>mV* 6= 0 and m > 1, the descent and convergence properties do not hold for Algorithm
1 with the identity
V = (Im + 1m 1m )
function%(x) = X near the local minimizers satisfying θ(w, w*)
-1(1m1>m - Im)V*.
π and
Remark 1. The convergence guarantee for the coarse gradient descent is established under the
assumption that there are infinite training samples. When there are only a few data, in a coarse scale,
the empirical loss roughly descends along the direction of negative coarse gradient, as illustrated
by Figure 1. As the sample size increases, the empirical loss gains monotonicity and smoothness.
This explains why (proper) STE works so well with massive amounts of data as in deep learning.
5
Published as a conference paper at ICLR 2019
Remark 2. The same results hold, if the Gaussian assumption on the input data is weakened to that
their rows i.i.d. follow some rotation-invariant distribution. The proof will be substantially similar.
In the rest of this section, we sketch the mathematical analysis for the main results.
sample size = 10
sample size = 50
sample size = 1000
Figure 1: The plots of the empirical loss moving by one step in the direction of negative coarse
gradient v.s. the learning rate (step size) η for different sample sizes.
3.1 Derivative of the Vanilla ReLU as STE
If we choose the derivative of ReLU μ(x) = max{x, 0} as the STE in (5), it is easy to see
μ0(x) = σ(x), and we have the following expressions of EZ [焉(v, w; Z)] and EZ [grelu(v, w; Z)]
for Algorithm 1.
Lemma 4. The expected partial gradient of `(v, w; Z) w.r.t. v is
「d'' J ∂f,	、
EZ 济(v,w；Z) = ∂v(v,w).
∂f
∂v
(10)
Let μ(x) = max{x, 0} in (5). The expected coarse gradient w.r.t. W is
EZ grelU(v, w; Z) =
h(v, v*) w
2V2∏	Ilwk
- cos
θ(w, w*)	v>v*
√2∏
尚 + w*	1
尚+w*∣∣,
(11)
2
where h(v,v*) = kvk2 + (1>mv)2 - (1>mv)(1>mv*) + v>v*.
As stated in Lemma 5 below, the key observation is that the coarse partial
gradient
EZ [grelu(v, w; Z)] has non-negative correlation with the population partial gradient ∣f (v, w),
and -Ez IgrelU(v, w; Z)] together with -Ez [器(v, w; Z)] form a descent direction for minimiz-
ing the population loss.
Lemma 5. If w 6= 0n and θ(w, w*) ∈ (0, π), then the inner product between the expected coarse
and population gradients w.r.t. w is
(EZ IgrelU (v, w; Z)],f (v, w )) = sin(θ(w, w F (v > V * )2 ≥ 0.
∂w	2( 2π)3kwk
Moreover, if further kvk ≤ Cv and kwk ≥ cw, there exists a constant ArelU > 0 depending on Cv
and cw, such that
2
EZ hgrelU(v,w; Z)i2
≤ Arelu	Il ∂V (V, w)	+
(Ez[grelu (v, w； Z)] , f (V, w))). (12)
Clearly, When(Ez IgrelU(V, w; Z)], If (V, w)>> 0, EZ IgrelU(V, w; Z)] is roughly in the same
direction as f (v, w). Moreover, since by Lemma 4, EZ [案(v, w; Z)] = f(v, w), We expect 1
1We redefine the second term as 0n in the case θ(w, w*) = π, or equivalently, k^ + W
* = 0n
6
Published as a conference paper at ICLR 2019
that the coarse gradient descent behaves like the gradient descent directly on f (v, w). Here we
would like to highlight the significance of the estimate (12) in guaranteeing the descent property of
Algorithm 1. By the LiPschitz continuity of Vf specified in Lemma 3, it holds that
f(vt+1
wt+1
-η
t+1 - Vt) + (∂f(vt, wt), wt+1 - Wt)
tk2 + kw t+1 -w tk2)
∂f (vt, Wt)	+ Ln- l∣Ez IgrelU(vt, Wt ； Z)H
(vt, Wt), EZ grelU(vt, Wt;
2 -卜 - (1+ Arelu) Ln" ) । df (Vt Wt)|
— (n - 're1；"" ) ( ∂W (Vt，Wt)，EZhgrelu(vt, Wt； Z)]),
(13)
where a) is due to (12). Therefore, if η is small enough, we have monotonically decreasing energy
until convergence.
Lemma 6. WhenAlgorithmI converges, EZ [段(v, w; Z)] and EZ IgrelU(v, w; Z)] Vanishsimul-
taneously, which only occurs at the
1.	Saddle points where (8) is satisfied according to Proposition 1.
2.	Minimizers of(2) where V = v*, θ(w, w*) = 0, or V = (Im + 1m1>b)-i (1m1>b 一 Im)v*,
θ(w, w*) = π.
Lemma 6 states that when Algorithm 1 using ReLU STE converges, it can only converge to a critical
point of the population loss function.
3.2 Derivative of the Clipped ReLU as STE
For the STE using clipped ReLU, μ(x) = min {max{x, 0}, 1} and μ0(x) = 1{o<χ<i}(x). We
have results similar to Lemmas 5 and 6. That is, the coarse partial gradient using clipped ReLU
STE EZ gcrelU (V， W; Z) generally has positive correlation with the true partial gradient of the
population loss f (v, w) (Lemma 7)). Moreover, the coarse gradient vanishes and only vanishes at
the critical points (Lemma 8).
Lemma 7. If w 6= 0nand θ(w, w*) ∈ (0, π) ,then
EZ [gcrelu(V, W； Z)] = P(O，M" V ' ^W - (VTV* ) csc(θ∕2) ∙ q(θ, w) “ kwk +—
2	kwk	∣∣ 谓K+w*
-(vtv*) (p(θ, w) 一 cot(θ∕2) ∙ q(θ, w)) ^^,
where h(V, V*) := kVk2 + (1Tm V)2 - (1TmV)(1TmV*) + VTV* same as in Lemma 5, and
p(θ, w) := 1- Z 2	cos(φ)ξ (sec(φ)) dφ, q(θ, w) := ɪ Z 2 sin(φ)ξ (sec(φ)) dφ
2π J-π+θ	kw l∣wk )	2π J-π+θ	kw l∣wk )
with ξ(x) := R0X r2 exp(-彳)dr. The inner product between the expected coarse and true gradients
w.r.t. w
(EZhgcrelu(V, W； Z)] , f (v, W)) = q7, WI) (VT V*)2 ≥ 0.
∂w	2πkwk
7
Published as a conference paper at ICLR 2019
Moreover, if further kV k ≤ Cv and kwk ≥ cw, there exists a constant Acrelu > 0 depending on Cv
and cw, such that
gcrelu (V, w; Z)i∣∣	≤ Acrelu
, w)
+ (Ez ∣gcrelu(V, w； Z)]，f M W)
Lemma 8. WhenAlgorithm 1 converges, EZ [岩(v, w; Z)] andEZ [gcreiu(V, w； Z)] vanish Simul-
taneously, which only occurs at the
1.	Saddle points where (8) is satisfied according to Proposition 1.
2.	Minimizers of(2) where V = v*, θ(w, w*) = 0, or V = (Im + 1m1mb)-i (1m1mb 一 Im)v*
θ(w, w*) = π.
3.	3 Derivative of the Identity Function as STE
Now we consider the derivative of identity function. Similar results to Lemmas 5 and 6 are not valid
anymore. It happens that the coarse gradient derived from the identity STE does not vanish at local
minima, and Algorithm 1 may never converge there.
Lemma 9. Let μ(x) = X in (5). Then the expected coarse partial gradient w.r.t. w is
EZ gid(V, w; Z)
kvk2 高一(v>v*)w*
(14)
If θ(w, w*) = ∏ and V = (Im + 1m1ml)^1 (1m1ml - Im)V*,
∣∣EZhgid(V, w； Z)]∣∣=√2∏mmji)2(ImV*)2 ≥ 0,
i.e., EZ gid(V, w; Z) does not vanish at the local minimizers if1>mV* 6= 0 and m > 1.
Lemma 10. Ifw 6= 0n and θ(w, w*) ∈ (0, π), then the inner product between the expected coarse
and true gradients w.r.t. w is
EZ hgid(V，w； Z)i ,f (v，w))=Sinswkw^(V>V*)2 ≥ 0.
(15)
When θ(w, w*) → π, V → (Im + 1m1工),(1m1工 一 Im)v*, if 1>bv^ = 0 and m > 1, we have
∣∣Ez Igid(v, w； Z)i ∣∣
Ildv(V, w)∣∣ + DEZhgid(V, w； Z)i, ∂f(V, W)E
→ +∞.
(16)
Lemma 9 suggests that if 1>mV* 6= 0, the coarse gradient descent will never converge near the
spurious minimizers with θ(w, w*) = π and V = (Im + 1m1m) 1(1m1m 一 Im)V*, because
EZ gid(V, w； Z) does not vanish there. By the positive correlation implied by (15) of Lemma
10, for some proper (V0, w0), the iterates {(Vt, wt)} may move towards a local minimizer in
the beginning. But when {(Vt, wt)} approaches it, the descent property (13) does not hold for
EZ [gid(V, w； Z)] because of (16), hence the training loss begins to increase and instability arises.
4 Experiments
While our theory implies that both vanilla and clipped ReLUs learn a two-linear-layer CNN, their
empirical performances on deeper nets are different. In this section, we compare the performances
of the identity, ReLU and clipped ReLU STEs on MNIST (LeCun et al., 1998) and CIFAR-10
(Krizhevsky, 2009) benchmarks for 2-bit or 4-bit quantized activations. As an illustration, we plot
8
Published as a conference paper at ICLR 2019
the 2-bit quantized ReLU and its associated clipped ReLU in Figure 3 in the appendix. Intuitively,
the clipped ReLU should be the best performer, as it best approximates the original quantized ReLU.
We also report the instability issue of the training algorithm when using an improper STE in section
4.2. In all experiments, the weights are kept float.
The resolution α for the quantized ReLU needs to be carefully chosen to maintain the full-precision
level accuracy. To this end, we follow (Cai et al., 2017) and resort to a modified batch normalization
layer (Ioffe & Szegedy, 2015) without the scale and shift, whose output components approximately
follow a unit Gaussian distribution. Then the α that fits the input of activation layer the best can
be pre-computed by a variant of Lloyd’s algorithm (Lloyd, 1982; Yin et al., 2018a) applied to a set
of simulated 1-D half-Gaussian data. After determining the α, it will be fixed during the whole
training process. Since the original LeNet-5 does not have batch normalization, we add one prior
to each activation layer. We emphasize that we are not claiming the superiority of the quantization
approach used here, as it is nothing but the HWGQ (Cai et al., 2017), except we consider the uniform
quantization.
The optimizer we use is the stochastic (coarse) gradient descent with momentum = 0.9 for all ex-
periments. We train 50 epochs for LeNet-5 (LeCun et al., 1998) on MNIST, and 200 epochs for
VGG-11 (Simonyan & Zisserman, 2014) and ResNet-20 (He et al., 2016) on CIFAR-10. The pa-
rameters/weights are initialized with those from their pre-trained full-precision counterparts. The
schedule of the learning rate is specified in Table 2 in the appendix.
4.1	Comparison Results
The experimental results are summarized in Table 1, where we record both the training losses and
validation accuracies. Among the three STEs, the derivative of clipped ReLU gives the best overall
performance, followed by vanilla ReLU and then by the identity function. For deeper networks,
clipped ReLU is the best performer. But on the relatively shallow LeNet-5 network, vanilla ReLU
exhibits comparable performance to the clipped ReLU, which is somewhat in line with our theoret-
ical finding that ReLU is a great STE for learning the two-linear-layer (shallow) CNN.
	Network	BitWidth	Straight-through estimator		
			identity	vanilla ReLU	clipped ReLU
MNIST	LeNet5	2	2.6 X 10-2/98.49	5.1 X 10—3/99.24	5.4 X 10—3/99.23
		4	6.0 X 10—3/98.98	9.0 X 10—4/99.32	8.8 X 10—4/99.24
	VGG11	2	0.19/86.58	0.10/88.69	0.02/90.92
CIFAR10		4	3.1 X 10—2/90.19	1.5 X 10—3/92.01	1.3 X 10—3/92.08
	ResNet20	2	1.56/46.52	1.50/48.05	0.24/88.39
		4	1.38/54.16	0.25/86.59	0.04/91.24
Table 1: Training loss/validation accuracy (%) on MNIST and CIFAR-10 with quantized activations
and float weights, for STEs using derivatives of the identity function, vanilla ReLU and clipped
ReLU at bit-widths 2 and 4.
4.2	Instability
We report the phenomenon of being repelled from a good minimum on ResNet-20 with 4-bit acti-
vations when using the identity STE, to demonstrate the instability issue as predicted in Theorem 1.
By Table 1, the coarse gradient descent algorithms using the vanilla and clipped ReLUs converge to
the neighborhoods of the minima with validation accuracies (training losses) of 86.59% (0.25) and
91.24% (0.04), respectively, whereas that using the identity STE gives 54.16% (1.38). Note that the
landscape of the empirical loss function does not depend on which STE is used in the training. Then
we initialize training with the two improved minima and use the identity STE. To see if the algorithm
is stable there, we start the training with a tiny learning rate of 10-5 . For both initializations, the
training loss and validation error significantly increase within the first 20 epochs; see Figure 4.2. To
speedup training, at epoch 20, we switch to the normal schedule of learning rate specified in Table
2 and run 200 additional epochs. The training using the identity STE ends up with a much worse
minimum. This is because the coarse gradient with identity STE does not vanish at the good minima
in this case (Lemma 9). Similarly, the poor performance of ReLU STE on 2-bit activated ResNet-20
9
Published as a conference paper at ICLR 2019
is also due to the instability of the corresponding training algorithm at good minima, as illustrated
by Figure 4 in Appendix C, although it diverges much slower.
CReLU 1.53
ReLU
50	100	150	200
epoch
Figure 2: When initialized with weights (good minima) produced by the vanilla (orange) and clipped
(blue) ReLUs on ResNet-20 with 4-bit activations, the coarse gradient descent using the identity STE
ends up being repelled from there. The learning rate is set to 10-5 until epoch 20.
5	Concluding Remarks
We provided the first theoretical justification for the concept of STE that it gives rise to descent
training algorithm. We considered three STEs: the derivatives of the identity function, vanilla ReLU
and clipped ReLU, for learning a two-linear-layer CNN with binary activation. We derived the
explicit formulas of the expected coarse gradients corresponding to the STEs, and showed that the
negative expected coarse gradients based on vanilla and clipped ReLUs are descent directions for
minimizing the population loss, whereas the identity STE is not since it generates a coarse gradient
incompatible with the energy landscape. The instability/incompatibility issue was confirmed in
CIFAR experiments for improper choices of STE. In the future work, we aim further understanding
of coarse gradient descent for large-scale optimization problems with intractable gradients.
Acknowledgments
This work was partially supported by NSF grants DMS-1522383, IIS-1632935, ONR grant N00014-
18-1-2527, AFOSR grant FA9550-18-0167, DOE grant DE-SC0013839 and STROBE STC NSF
grant DMR-1548924.
References
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420,
2018.
YOshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients
through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian
inputs. arXiv preprint arXiv:1702.07966, 2017.
Zhaowei Cai, Xiaodong He, Jian Sun, and Nuno Vasconcelos. Deep learning with low precision by
half-wave gaussian quantization. In IEEE Conference on Computer Vision and Pattern Recogni-
tion, 2017.
Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi Srini-
vasan, and Kailash Gopalakrishnan. Pact: Parameterized clipping activation for quantized neural
networks. arXiv preprint arXiv:1805.06085, 2018.
Ronan Collobert and Jason Weston. A unified architecture for natural language processing: Deep
neural networks with multitask learning. In International Conference on Machine Learning, pp.
160-167. ACM, 2008.
10
Published as a conference paper at ICLR 2019
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural
networks with binary weights during propagations. In Advances in Neural Information Processing
Systems,pp. 3123-3131, 2015.
Simon S. Du, Jason D. Lee, Yuandong Tian, Barnabas Poczos, and Aarti Singh. Gradient de-
scent learns one-hidden-layer CNN: Don’t be afraid of spurious local minimum. arXiv preprint
arXiv:1712.00779, 2018.
Yoav Freund and Robert E Schapire. Large margin classification using the perceptron algorithm.
Machine learning, 37(3):277-296, 1999.
Abram L Friesen and Pedro Domingos. Deep learning as a mixed convex-combinatorial optimiza-
tion problem. arXiv preprint arXiv:1710.11573, 2017.
Surbhi Goel, Adam Klivans, and Raghu Meka. Learning one convolutional layer with overlapping
patches. arXiv preprint arXiv:1802.02547, 2018.
Juncai He, Lin Li, Jinchao Xu, and Chunyue Zheng. ReLU deep neural networks and linear finite
elements. arXiv preprint arXiv:1807.03973, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.
Geoffrey Hinton. Neural networks for machine learning, coursera. Coursera, video lectures, 2012.
Lu Hou and James T Kwok. Loss-aware weight quantization of deep networks. arXiv preprint
arXiv:1802.08635, 2018.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized
neural networks: Training neural networks with weights and activations constrained to +1 or -1.
arXiv preprint arXiv:1602.02830, 2016.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized
neural networks: Training neural networks with low precision weights and activations. Journal
of Machine Learning Research, 18:1-30, 2018.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Tech Report, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in Neural Information Processing Systems, pp. 1097-1105,
2012.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Dong-Hyun Lee, Saizheng Zhang, Asja Fischer, and Yoshua Bengio. Difference target propagation.
In Joint european conference on machine learning and knowledge discovery in databases, pp.
498-515. Springer, 2015.
Fengfu Li, Bo Zhang, and Bin Liu. Ternary weight networks. arXiv preprint arXiv:1605.04711,
2016.
Hao Li, Soham De, Zheng Xu, Christoph Studer, Hanan Samet, and Tom Goldstein. Training
quantized nets: A deeper understanding. In Advances in Neural Information Processing Systems,
pp. 5811-5821, 2017.
Qianxiao Li and Shuji Hao. An optimal control approach to deep learning and applications to
discrete-weight neural networks. In International Conference on Machine Learning, 2018.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation.
In Advances in Neural Information Processing Systems, pp. 597-607, 2017.
11
Published as a conference paper at ICLR 2019
Stuart P. Lloyd. Least squares quantization in PCM. IEEE Trans. Info. Theory, 28:129-137,1982.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529, 2015.
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet
classification using binary convolutional neural networks. In European Conference on Computer
Vision, pp. 525-542. Springer, 2016.
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-time object
detection with region proposal networks. In Advances in Neural Information Processing systems,
pp. 91-99, 2015.
Frank Rosenblatt. The perceptron, a perceiving and recognizing automaton Project Para. Cornell
Aeronautical Laboratory, 1957.
Frank Rosenblatt. Principles of neurodynamics. Spartan Book, 1962.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. Nature, 529(7587):484, 2016.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its
applications in convergence and critical point analysis. arXiv preprint arXiv:1703.00560, 2017.
Bao Wang, Xiyang Luo, Zhen Li, Wei Zhu, Zuoqiang Shi, and Stanley J Osher. Deep neural nets
with interpolating function as output activation. In Advances in Neural Information Processing
Systems, 2018.
Bernard Widrow and Michael A Lehr. 30 years of adaptive neural networks: perceptron, madaline,
and backpropagation. Proceedings of the IEEE, 78(9):1415-1442, 1990.
Penghang Yin, Shuai Zhang, Yingyong Qi, and Jack Xin. Quantization and training of low bit-width
convolutional neural networks for object detection. arXiv preprint arXiv:1612.06052, 2016.
Penghang Yin, Shuai Zhang, Jiancheng Lyu, Stanley Osher, Yingyong Qi, and Jack Xin. Bina-
ryrelax: A relaxation approach for training deep neural networks with quantized weights. arXiv
preprint arXiv:1801.06313, 2018a.
Penghang Yin, Shuai Zhang, Jiancheng Lyu, Stanley Osher, Yingyong Qi, and Jack Xin.
Blended coarse gradient descent for full quantization of deep neural networks. arXiv preprint
arXiv:1808.05240, 2018b.
Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees
for one-hidden-layer neural networks. arXiv preprint arXiv:1706.03175, 2017.
Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Train-
ing low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint
arXiv:1606.06160, 2016.
Chenzhuo Zhu, Song Han, Huizi Mao, and William J Dally. Trained ternary quantization. arXiv
preprint arXiv:1612.01064, 2016.
12
Published as a conference paper at ICLR 2019
Appendix
A. The Plots of Quantized and Clipped ReLUs
quantized ReLU
Figure 3: The plots of 2-bit quantized ReLU σα (x) (with 22 = 4 quantization levels including 0)
and the associated clipped ReLU σα (x). α is the resolution determined in advance of the network
training.
clipped ReLU
B.	The Schedule of Learning Rate
Network	# epochs	Batch size	Learning rate		
			initial	decay rate	milestone
LeNet5	50	64	0.1	0.1	[20,40]
VGG11	200	128	0.01	0.1	[80,140]
ResNet20	200	128	0.01	0.1	[80,140]
Table 2: The schedule of the learning rate.
C. Instability of ReLU STE on ResNet-20 with 2-bit Activations
Figure 4: When initialized with the weights produced by the clipped ReLU STE on ResNet-20 with
2-bit activations (88.38% validation accuracy), the coarse gradient descent using the ReLU STE with
10-5 learning rate is not stable there, and both classification and training errors begin to increase.
D. Additional Supporting Lemmas
Lemma 11. Let z ∈ Rn be a Gaussian random vector with entries i.i.d. sampled from N(0, 1).
Given nonzero vectors W, W ∈ Rn with the angle θ, we have
E [1{z> w>0}] = 2 , E [1{z> w>0, z> W >0}]
∏ - θ
2π
13
Published as a conference paper at ICLR 2019
and
1W
E [z1{z>w>0}] = √2∏ kWk , E [z1{z>w>0,z>W>0}]
cos(θ∕2) kWk
√2π	∣∣ W
∣∣ kwk
+ k¾	2
+ kWk∣∣
Proof of Lemma 11. The third identity was proved in Lemma A.1 of (Du et al., 2018). To
show the first one, without loss of generality we assume W = [w1, 0n>-1]> with w1 > 0, then
E [1{z>w>0}] = P(ZI > O) = 21.
We further assume W= [W1,W2, 0>-2]>. It is easy to see that
π - θ
E [l{z>w>0, z>W>0}] = P(zτW > 0, z>W > 0) = -∏^-
To prove the last identity, we use polar representation of two-dimensional Gaussian random vari-
ables, where r is the radius and φ is the angle with dPr = r exp(-r2∕2)dr and dPφ = 2∏dφ. Then
E [zi1{z>w>0, z>w>o}] = 0 for i ≥ 3. Moreover,
1∞
E [z11{z>w>0, z>w>0}] = 2∏ J Mexp
π
八 /2	/ A1+.	1 + cos(θ)
dr	cos(φ)dφ =	C k
J-2+θ	2√2π
and
m「 r	I 1	f∞ 2	(八、∕,2	.一、一 sin(θ)
E [z21{z>w>0,z>w>0}]	= 2∏	J r exp (-^2^J	dr J	sin(φ)dφ	= 2√2∏.
Therefore,
E [z1{z>w>0,z>W>0}]
cos(θ/2) [cos(θ/2), sin(θ/2), 0>-2]τ
2π
cos(θ∕2)稿
√2π ∣∣ W
∣∣ IWi
+ k¾
+ kwk
where the last equality holds because ^k and j∣-kWk
kwk k H
θ∕2.
+ W
+ kW k
+k¾k
are two unit-normed vectors with angle
□
Lemma 12. Let z ∈ Rn be a Gaussian random vector with entries i.i.d. sampled from N(0, 1).
Given nonzero vectors w, W ∈ Rn with the angle θ, we have
w
E [z1{0<z>w<1}] = P(O, W) kW,
W
E [z1{0<z>w<1,z>W>0}] = ((P(θ, W)) - cot(θ/2) ∙ q(θ, W)) kW
W i W
+ csc(θ/2) ∙ q(θ, W) ∣kwk~~kwk-∣.
∣∣ kwk + kWk ∣∣
Here
π
1	/2
p(θ, W) = 2∏ J 7r jos(φ)ξ
dφ,q(θ, W)=2n /'sin(φ)ξ (需)dφ
with ξ(x) = Rx r2 exp(-r2)dr satisfying ξ(+∞) = Pn. In addition,
P(π, W) = q(O, W) = q(π, W) = O.
2Same as in Lemma 4, We redefine E [z1{z>w>o, z>w>0}] = 0n in the case θ(w, W) = π.
14
Published as a conference paper at ICLR 2019
Proof of Lemma 12. Following the proof of Lemma 11, we let w = [w1, 0n>-1]> with w1 > 0,
W = [W1,W2,0>-2]>, and use the polar representation of two-dimensional Gaussian distribution.
Then
π	Sec⑻	/ C、
1 /2	/ kwk	r r2∖
E [zι1{0<z>w<i}] = 2∏ J 7r cos(φ) J0	r2 exp (一 "2JdrdΦ = P(0，W)
and
π	sec(φ)
m「 .	1	1 P . , ,. ∕Ε 2	r2∖、、 ,c 、 C
E ∣Z2l{0<z>w<i}J = 2∏ J . sin(φ) J r exp (--J drdφ = q(0, w) = 0,
since the integrand above is an odd function in φ. Moreover, E zi1{0<z>w<1} = 0 for i ≥ 3. So
the first identity holds. For the second one, we have
E [z11{0<z>w<1,z>W>0}]
2∏ 1
π
2
sec(φ)	/	2、
cos(φ)	r2 exp I--------I drdφ = p(θ, w),
+θ	0	2
π
2
and similarly, E [z2l{0<z>w<1,z>w>0}] = q(θ, w). Therefore,
E[z1{0<z> w<1,z>W>0}] = [p(θ, W), q(θ, W), 0n-2] .
W I W
Since kwk = [1, 0>-ι]> and ∣∣盟+嗯∣∣ = [cos(θ∕2), sin(θ∕2), 0>-2]>, it is easy to check the
11 11	k E+ E k
second identity is also true.	□
Lemma 13. Letp(θ, w) and q(θ, w) be defined in Lemma 12. Thenfor θ ∈ [∏2, π], we have
1.	p(θ, W) ≤ q(θ, W).
2.	(1 - θ) P(0, w) ≤ q(θ, w).
Proof of Lemma 13. 1. Let θ ∈ [2 ,π], since ξ ≥ 0,
p(θ, w) =	ɪ	/	cos(φ)ξ (sec(φ)) dφ = ɪ / Sin	(π - φ) ξ (sec(φ)) dφ
2π	θθ-2	∖ kwk J 2π θθ-2	12	/	∖ kwk J
≤	1-	Z 2	sin(θ - φ)ξ (sec(φ)) dφ ≤ 1- Z 2	sin(φ)ξ (sec(φ)) dφ = q(θ, w),
2π θθ-2	∖ kwk J 2π θθ-2	k kwk J
where the last inequality is due to the rearrangement inequality since both sin(φ) and ξ (Skwφ)
are increasing in φ on [0, 2].
2.	Since cos(φ)ξ (Sewk)) is even, We have
1	I'2 cos(φ)ξ (竹)dφ =2/2 cos(φ)ξ (竹)dφ
KJ- 2	∖ kwk J π Jo	∖ kwk J
≤ 2 Z 2 sin(φ)ξ (sec(φ)) dφ ≤ -ɪr Z 2 sin(φ)ξ (sec(φ)) dφ.
π Jo	kw kwk J	π - θ θθ-2	∖ kwk J
The first inequality is due to part 1 which gives p(π∕2, w) ≤ q(π∕2, w), whereas the second one
holds because sin(φ)ξ (sewj)) is increasing on [0, 2].	□
Lemma 14. For any nonzero vectors W and W with ∣∣w∣∣ ≥ ∣∣wk = c > 0, we have
15
Published as a conference paper at ICLR 2019
1. ∣θ(w, w*) — θ(W,w*)| ≤ 2∏-∣∣w — WIl.
2.
—
1
≤ * IIw ― WIl.
Proof of Lemma 14. 1. Since by Cauchy-Schwarz inequality,
〜 CW
w,W-丽
WTW — CkWil ≤ 0,
where We used the fact Sin(X) ≥ 2∏x for x ∈ [0, ∏] and the estimate in (17).
cw
画
(17)
2
2. Since
—
π∣2 ) w* is the projection of w* onto the complement space of w, and likewise for
ɪwr) w*, the angle between
E w* and
ɪwr )w* is equal to the angle
between W and W. Therefore,
w*
—
落)w*
—
—

—
—
and thus
The second equality above holds because
2
W
I∣w∣2
W
PF
1
l∣wk
2(w, W)
∣∣w — W ||2
IH 臼 w∣2 - IH 臼 w∣2 ∙
—
ι 1
ɪ+而
—
□
E.	MAIN Proofs
Lemma 1. If W = 0n ,the population loss f (v, w) is given by
1
8
VT(Im + 1m1m)v — 2vT ((1 — 2θ(w, W*)) Im + 1m1m)v* + (v*)T(Im + 1m1m)v*
In addition, f (v, w) = 8(v*)t (Im + 1m1>n)v* for W = 0n.
16
Published as a conference paper at ICLR 2019
Proof of Lemma 1. We notice that
f (v, W) = ɪ (vτEz [σ(Zw)σ(Zw)τ]v — 2vτEz [σ(Zw)σ(Zw*)τ]v*
+ (v* )τEz[σ(Zw*)σ(Zw*)τ]v*).
Let Zτ be the ith row vector of Z. Since w = 0n, using Lemma 11, we have
Ez ∣σ(Zw)σ(ZW)T]ii = E [σ(ZTW)σ(Z[w)] = E [l{z>w>0}] = 2,
and for i = j,
Ez ∣σ(Zw)σ(ZW)T]ij = E [σ(ZTW)bWM] = E [1{z>w>0}] E [1{z>w>0}] = 4.
Therefore, EZ [σ(Zw)σ(Zw)τ] = EZ [σ(Zw*)σ(Zw*)τ] = 1 (Im + 1m1工).Furthermore,
Ez [σ(ZW)σ(ZW*)τ]ii = E [1{z>w>0,z>w*>0}] = π^T W )，
and Ez [σ(Zw)σ(Zw*)τ] .j = ɪ. So,
Ez [σ(Zw)σ(Zw*)τ] =4 ((1 — 2叱 W*)) Im + 1硒1*).
Then it is easy to validate the first claim. Moreover, if W = 0n, then
f(v, w) = 1(v*)τEz[σ(Zw*)σ(Zw*)τ]v* = %v*)T(Im + 〃匕)”
2	8
□
Lemma 2. If W = 0n and θ(w, w*) ∈ (0, π) ,the partial gradients of f (v, w) w.r.t. v and W are
∂ff (V, w) = 1 (Im + 1m1m)v — 4 ((1 — f θ(w, W*)) Im + 1m1∑∖ v*
and
∂W(v，w)
vτv*
告)W*
—
respectively.
Proof of Lemma 2. The first claim is trivial, and We only show the second one. Since θ(w, w*)
arccos (Wiwwɪ) is differentiable w.r.t. W at θ(w, w*) ∈ (0, π), we have
∂f
∂w
(v, w)
vτ
∂θ
vτ
2 一
2W
— (wτ
w*
2π ∂w
2π
∣∣w∣∣3 W
(w>w
(w, w*)
—
—
2∏k
vτ
*
w
*
w
v
*
v
*
k
w
k
*
i
w
)
*
i
2
w
12
□
17
Published as a conference paper at ICLR 2019
Proposition 1. Ifthe true parameter v* satisfies (ɪ^v*)2 < m+1 ∣W*∣∣2 ,then
(V) W) : V = (Im + ɪmɪm)
(m+1)∣∣v*ιι2 — (ɪm v*)2
Im + ɪmɪm ) V*,
川 W*) = 2U)w‰
give the saddle points obeying (8), and {(v, w) : θ(w, w*) = π, V = (Im + ɪmɪm) ɪ(ɪmɪm
—
Im)v*} are the spurious local minimizers. Otherwise, the model (2) has no saddle points or spurious
local minimizers.
Proof of Proposition 1. Suppose v>v* = 0 and ∂f (v, w) = 0, then by Lemma 1,
0 = v>v* = (v*)>(Im + ɪm/)T ((1 — f^M w*)) Im + ɪm/) v*.	(18)
From (18) it follows that
2
-θ(w, w* )(v*)τ(Im + ɪmɪm)lv* = (v*)T(Im + ɪm ^)-1(^ + ɪmɪm) V* = ∣∣V*∣∣2 . (19)
On the other hand, from (18) it also follows that
(2 θ(w, w*) — 1) (v*)τ(Im + ɪmɪm)lv* = (v*)T(Im + ɪm ^) T ɪm ( ^ V* ) = ^^2 ,
∖π	J	m + 1
where we used (Im + ɪmɪm)ɪm = (m + 1)ɪm. Taking the difference of the two equalities above
gives
(v*)>(Im + ɪmɪm)-1 v* = ∣∣v*∣∣2 —	.
m + 1
By (19), We have θ(w, w*) = 2 (m+1(m+I)TE*2, which requires
π	(m + 1)kv*l∣2	,	■ , 2, ∕1T *、2 ,
2(m + 1)∣∣v*∣∣2-αmv*)2 <π, OreqUivalently, (ɪmV ) <
Furthermore, since ∂f (v, w) = 0, we have
V = (Im + ɪmɪm) 1 ( (1 — ∏θ(W) w*)) Im + ɪmɪm)'
m+1 kv*k2.
*
v*
(20)
(Im + ɪmɪm)T
(m+ι)kv*k2 - αmv*)2
Im + ɪm ɪm	v* .
Next, we check the local optimality of the stationary points. By ignoring the scaling and constant
terms, we rewrite the objective function as
f(v,θ):= VT(Im + ɪmɪm)v — 2VT ( 11 - -θ) Im + ɪmɪm) V*, for θ ∈ [0,π].
It is easy to check that its Hessian matrix
v2f(v,θ)= ]2(IQy”	4v*
is indefinite. Therefore, the stationary points are saddle points.
Moreover, if (^v*)2 < m+1 ∣∣v*∣∣2, at the point (v,θ) = ((Im + ɪmɪm)T(ɪmɪm — Im)v*,π),
we have
VTV* = (v*)T(Im + ɪmɪm)T(ɪmɪm — Im)v*
=kv*k2 — 2(v*)T(Im + ɪmɪm)rv* = 2(ImV*)2 — kv*k2 < 0,
(21)
m+1
18
Published as a conference paper at ICLR 2019
where we used (20) in the last identity. We consider an arbitrary point (V + ∆v, π + ∆θ) in the
neighborhood of (v, π) with ∆θ ≤ 0. The perturbed objective value is
f(v + ∆v,π + ∆θ) = (v + ∆v)τ(Tm + ɪmɪm)(v +∆v) - 2(v + ∆v)> (lmɪɪ - Im) V*
+ 2AΘ(V + ∆v)>v*.
π
On the right hand side, since V = (Im + 1m1工)-1(1m1工-Im)v* is the unique minimizer to the
quadratic function f (v,π),we have if ∆v = 0m,
(v + ∆v)τ(Im + 1m1m)(v +∆v) - 2(v +∆v)> (1馆1北一Im) v* > f(v,π).
Moreover, for sufficiently small ∣∣∆v∣∣, it holds that ∆θ ∙ (v + ∆v)τv* > 0 for ∆θ < 0 because
of (21). Therefore, f (v + ∆v, π + ∆θ) > f (v, π) whenever (∆v, ∆θ) is small and non-zero, and
((Im + 1m1m)-1(1m1m - Im)v*,π) isalocal minimizer of f .
To prove the second claim, suppose (1mv*)2 ≥ m+ ∣∣v*∣∣2, then either f (v, W) does not exist,
or ∂f (v, w) and If (v, w) do not vanish simultaneously, and thus there is no stationary point.
At the point (v,θ) = ((Im + 1m1mt)-1(1m1mt, - Im)v*,π), we have
vτv* = 2(ImV*)2
m + 1
-kv*k2 ≥ 0.
If vτv* > 0, since Vf (v, θ) = ɪ [0工，2vτv*]τ, a small perturbation [0工,∆θ]τ with ∆θ < 0
will give a strictly decreased objective value, so (v, θ) = ((Im + 1m1工)-1(1m1工 -Im)v*,π) is
not a local minimizer. If v 1 v* = 0, then Vf (v, θ) = 0n+1, the same conclusion can be reached by
examining the second order necessary condition.	□
Lemma 3. For any differentiable points (v, w) and (V, W) with min{kwk,∣∣W∣∣} = Cw > 0 and
max{∣∣v∣∣, ∣∣V∣∣} = Cv, there exists a Lipschitz constant L > 0 depending on Cv and Cw, such that
l∣Vf(v, w) - Vf(V, W)k ≤ L∣(v, w) - (V, w)∣.
Proof of Lemma 3. It is easy to check that ∣Im + 1m1m∣ = m + 1. Then
∂V (v, w) - ∂∂V (V , w)|| = 1 I I (Im + 1m1m)(v - V) + 2(θ(w, w*) - θ(w, w*))v*
≤ m : 1 l∣V - Vk + ∣v-∣ ∣θ(w, w*) - θ(w , w*)|
4	2π
m + 1	∣v*∣
≤	l∣v - Vk +	l∣w - Wk
≤ 1 (m + 1 + lv-k) k(v, w) - (V, W)k,
4	Cw)
where the last inequality is due to Lemma 14.1.
19
Published as a conference paper at ICLR 2019
We further have
—
陪■) w*
V>v*
f (V, W)―乎(V, W)
∂w	∂w
vτv*
vτv*
vτv*
vτv*
~ I *
V i V*
∣v>v*l h
2πcW
W — W Il +
∣v*∣∣
2πCw
l∣v ― Vk
(Cv + Cw )∣∣v*∣∣
2πcW
ll(v, W) —(V, w)k,
≤
≤
≤
—
—
告)w*
—
—
—
+
*
W⅛ W*
E w*
陪■ w*
—
—
—
等)W*
萼w*
—
—
—
—
新W*
W¢) W*
等w*
—
等)W*
—
萼w*
where the second last inequality is to due to Lemma 14.2. Combining the two inequalities above
validates the claim.
□
Lemma 4. The expected partial gradient of '(v, w; Z) w.r.t. v is
_ Γ∂' , 一
EZ 防(v,w; Z)
∂ft 、
沆(v,w)
Let μ(x) = max{x, 0} in (5). The expected coarse gradient w.r.t. W is
EZ Igrelu(v, W； Z)]
h(v, v*) w
2√2∏ IlWIl
一cos
θ(w, W*)、vτv*	∣W∣ + w*	3
√2π
⅛+w*
2
where h(v, v*) = ∣∣v∣2 + (ImV)2 — (ImV)(ImV*) + vtv*.
Proof of Lemma 4. The first claim is true because 累(v, w; Z) is linear in v. By (5),
gμ(v, w; Z) = ZT (μ0(Zw) Θ v) (v>σ(Zw) — (v*)>σ(Zw*)).
Using the fact that μ0 = σ = 1{χ>0},we have
EZ [grelu(v, w； Z)] = EZ
i=1
m
vi1{Z>w>0}— Σv*l{Z>w*>0}
Viσ(Z>w)— J>:σ(Z>w*)
Ziviσ(Ziτw)
1{Z> W>0}viZi)]
Invoking Lemma 11, we have
i=1
E [Zil{Z>w>0,Z>w>0}
W
Wk
W
kWk
ifi= j,
ifi=j,
and
E [Zi1{Z>W>0,Z>W*>0}]
cos(Θ(w,w*)∕2)	⅜J +w*
Il w+w*ll
if i = j,
2√2∏ IIWk
if i = j.
m

1 W
3We redefine the second term as 0n in the case θ(w, w*) = π, or equivalently, ɪwɪ + w* = 0n
20
Published as a conference paper at ICLR 2019
Therefore,
m	mm
EZ [grelu (v, w; Z)] = Xvi2E Zi1{Zi>w>0} + XXvivjE Zi1{Zi>w>0,Zj>w>0}
i=1	i=1 j=1
j6=i
m
-X viviE [Zi1{Z> w>0,Z>w*>0}]
i=1
mm
-XX vivjE IZi1{Z>w>0,Z>w*>0}]
i=1 j=1
j 6=i
=修(kvk2+(Imv)2)高-cos (θ(ww)) v>f rk^+w
2√2π	kwk	2	2	∏ √2π Il w + w*
kwk
1w
-2√2π ((Imv)(Imv)- v v)M,
and the result follows.
□
Lemma 5. Ifw6= 0n and θ(w, w*) ∈ (0, π), then the inner product between the expected coarse
and true gradients w.r.t. wis
u(v, w; Z)i，∂f (v, w)
Sin (θ(w, w*))
2(√2∏)3kwk
(v>v*)2 ≥ 0.
Moreover, if further kv k ≤ Cv and kwk ≥ cw, there exists a constant Arelu > 0 depending on Cv
and cw, such that
快Zhgrelu(v, w； Z)i I1 ≤ Arelu ( Il df (v, W)
+ (EZ Igrelu(v, w； Z)i，|f (v，w)
Proof of Lemma 5. By Lemmas 2 and 4, we have
f (V w)
v>v*
—
and
EZ grelu(v, w； Z)
h(v, v*) W	(θ(w, w*)) v>v* kwwk + w*
F 两-cos√ F Il 尚+w*
21
Published as a conference paper at ICLR 2019
Notice that (In - Ww2) W = 0n and IIW* ∣∣ = 1, if θ(w, w*) = 0, π, then We have
u(v,w； Z)],∂W- (v, W)
f θ(w, W*) ∖ (vτv*)2 / 1	(In - w⅛)w*
COS	J 懑币 ∖M KIn-告)w*||
w*
局+ w*
θ(w,w*)) (VTv*)2	∣∣w∣2 — (wτw*)2
2	) (√2π)3 IIkWk2w* - W(WTW*)∣ ∣w + ∣∣w∣w*∣
θ θ(w, W*)
cos \	2-
(vτv*)2_____________∣w∣2 - (WTW*)2_____________
(√2π)3 p∣wk4 一 ∣w∣2(WTW*)2p2(∣w∣2 + ∣w∣(wTW*))
θ(w, w*))	(vTv*)2	∣∣w∣2 — (WTW*)2
2	/ 4(P∏kw∣)3 PkWk2 - (WTW*)2PkWk + (WTW*)
(θ(w,w*)∖ (vTv*)2≠τW
cosI 2 )	4(√∏)3kwk
(θ(w,w*)) (vTv*)2pl — cos(θ(w,W*))
COS(^^)	4(√∏)3kwk
sin(θ(w,w*)) (VTv*)2
2(√2∏)3kwk (	) .
To show the second claim, without loss of generality, we assume ∣∣w∣ = 1. Denote θ := θ(w, w*).
By Lemma 1, We have

4 (Im + ιmιm)v -1 ((1 - 2θ
Im +
By Lemma 4,
EZ Igrelu(v, W； Z)]
h(v, v*) w
2√2∏	∣∣wk
θθ∖ vTv* 尚 + w*
—COS -	-7∣-^--------
V2/ √2π II ∞ + w*
Ilkwk
(22)
where
h(v, v*) = ∣v∣2+(ImV)2 -(ImV)(Imv*)+vTv*
VT(Im + ɪmɪm) V - VT(ImIm - Im)v*
VT(Im + ɪmɪm) V - VT
ImIm + (1 - ^∏) Im) v* + 2 (1 - ∏) VTv
4vT
,w) + 2 (1 - ∏) vTv*,
(23)
and by the first claim,
u(v，W； Z)],⅛f (v，W)) = 2(√⅛θ) Wk(VTv*)2
22
Published as a conference paper at ICLR 2019
Hence, for some Arelu depending only on Cv and cw, we have
EZ hgrelu(v, w; Z)i
2v> ∂f (V, W
)W	θ θ∖ V>V* / W
"M +cos ⑴ √π IM
k¾ + w*
kWk+ w*
√2∏
+
< 6CV
≤ ---
π
w
kwk
+
< 6CV
≤ ---
π
< 6C2
≤ ---
π
2
+
+ 11 — — — Cos
π
>v 2 + 3sn(θ)
2π
≤ Arelu	f (V，W)
u(V，w; Z)i，f (V，W)
k¾ + w*
kWk+ w*
2π
2
3(v>v*)2
where the equality is due to (22) and (23), the first inequality is due to Cauchy-Schwarz in-
”,.	π +w*	A
equality, the second inequality holds because the angle between kWk and -τ∣-kWWk------------------------------ɪ is 2 and
11 11 k	+w k
W +w*
W_____∣∣w∣∣ +w
阿― k尚+w*k
and
≤ 2, whereas the third inequality is due to sin(χ) ≥ 2∏x, Cos(X) ≥ 1 一 2∏x,
1 — 2x - cos
π
2x	2x
Cos(X) — 1 +---I ICos(X) + 1------
≤ sin(x)(2 Cos(x)) = sin(2x)
for all x ∈ [0, 2].
Lemma 6. WhenAlgorithmI converges, EZ [段(v, w; Z)] and EZ IgrelU(v, w; Z)] Vanishsimul-
taneously, which only occurs at the
1.	Saddle points where (8) is satisfied according to Proposition 1.
2.	Minimizers of(2) where V = v*, θ(w, w*) = 0, or V = (Im + 1m1>b)-i (1m1>b — Im)v*,
θ(w, w*) = π.
Proof of Lemma 6. By Lemma 4, suppose we have
EZh ∂v (V，W; Z)i = 4 (Im + 1m1m)V — 4((一 ∏ θ(w, W*)) Im + 1m	V* = 0m (24)
and
EZ grelu(V, W; Z)
h(≠)鸟—Cos ( θM S )『h kwk + W*H = 0n,
2√2π kWk V 2	√ √2π Iikwk + w*∣∣
(25)
where h(V, V*) = kVk2 + (1>mV)2 — (1>mV)(1>mV*) + V>V*. By (25), we must have θ(W, W*) = 0
or θ(W, W*) = π or V>V* = 0.
—
—
2
□
23
Published as a conference paper at ICLR 2019
If θ(w, w*) = 0, then by (24), V = v*, and (25) is satisfied.
If θ(w, w*) = π, then by (24), V = (Im + 1m1工)-1(1m1工一Im)v*, and (25) is satisfied.
If v>v* = 0, then by (24), We have the expressions for V and θ(w, w*) from Proposition 1, and
(25) is satisfied.	□
Lemma 7. If W = 0n and θ(w, w*) ∈ (0, π) ,then
EZ [gcrelu(v, W； Z)] = p(0, W)" V ' -w- 一 (v> V* ) csc(θ∕2) ∙ q(θ, W) UkWk +—
2 kwk	U 谓T + W
一(v>v*) (p(θ, W)- cot(θ∕2) ∙ q(θ, W)) Ilwr,	(26)
kWk
where h(V, V*) := kVk2 + (1>mV)2 一 (1>mV)(1>mV*) + V>V* same as in Lemma 5, and
PG W) :=	1- Z 2 COS(O)ξ (sec(φ))	dφ, q(θ, W)	:= 1-	Z 2 Sin(O)ξ	(sec(φ))	dφ
2π J-π +θ	∖ kWk )	2π	J-π+θ	∖ kWk )
with ξ(x) := RX r2 exp(- r2 )dr. The inner product between the expected coarse and true gradients
w.r.t. W
(Ez[gcrelu(V, W； Z)] , f (v, W)) =	^, WI) (v> V*)2 ≥ 0∙
∂W	2πkW k
Moreover, if further kV k ≤ Cv and kWk ≥ cw, there exists a constant Acrelu > 0 depending on Cv
and cw, such that
UUUEZ hgcrelu(V, W； Z)iUUU
≤ Acrelu
+ (eZ Igcrelu(V, W； Z)] √f (v, W)
Proof of Lemma 7. Denote θ := θ(W, W* ). We first compute EZ gcrelu (V, W； Z) . By (5),
gμ(V, w； Z) = Z> (μ0(Zw) Θ V) (v>g(Zw) — (V*)>σ(Zw*)).
Since μ0 = 1{0<x<i} and σ = 1{χ>0}, we have
EZ [gcrelu (V, W； Z)] = EZ
EZ
m
Viμ0(Z>W) - X v*μ0(Z>w*)
i=1
m
Vi 1{0<Z>w<1} — Ev*1{0<Z>w*<1}
i=1
Ziviσ(Zi>W)
1{Zi>w>0}viZi
p(0, W)h(V, V*) W
kWk
丁	昌 + W*
一(v>v*)csc(Θ∕2) ∙ q(θ, w) U M---
U π + w*
U kWk
一(v>v*)(p(θ, kWk)- cot(θ∕2) ∙ q(θ, w))高.
In the last equality above, We called Lemma 12.
Notice that
WW2)w = 0n and ∣∣w*k = 1. If θ(w, w*) = 0,π, then the inner product
betWeen EZ
Igcrelu(V, W； Z)] and f (v, w) is given by
(Ez ∣gcrelu(V, W； Z)] , f (v, w))
(θ) q(θ W)	>	2 / J__(In
⑴	(V V ) ∖M向
ww>
ww>
W*
W*
WIU M + w*
UUU
—
2
—
—
* (V>V*)2≥ 0.
2π∣∣w∣l J -
24
Published as a conference paper at ICLR 2019
In the last line, q(θ, w) ≥ 0 because sin(φ)ξ (SeWφ)) is odd in φ and positive for φ ∈ (0, ∏2].
Next, we bound IIIEZ hgcrelu (V, W； Z)iIII . Since (23) gives
h(v, v*) = 4v> f (v, w) + 2 11 — — )v>v*,
∂V	π
where according to Lemma 1,
∂f (V, W) = 4 (Im + 1m1m)v - 4
We rewrite the coarse partial gradient w.r.t. W in (26) as
EZ [gcrelu(v, W； Z)] = P(0, W) (2v> ∂f- (v, W) + (l
Im +
v*.
—	(v>v*) csc(θ∕2) ∙ q(θ, W)
-	Bv>v*) kw
局+w*
局+w*∣∣
-	(v>v*) (p(θ, W) - cot(θ∕2) ∙ q(θ, W)) kWk.
> ∂f W
2p(0, w)v> √l(v, w)t,-Γ
∂V	kW k
+ (v>v* ) ((l - ∏) p(0, w) - p(θ, w)
+(v>v* …/2 ∙ q(θ, W) (A
W
k kwk
Iw + W*
kwk
Iw + W*
kwk
-(V>v*)(Csc⑼2)-cote/2))qG W)高
To prove the last claim, we notice that in the above equality,
卜IfmW)舟I ≤ Cvil∂
,w)2 ,
(27)
w
(csc(θ∕2) - cot(θ∕2))q(θ, W)^^
2
= (csc(θ∕2) - cot(θ∕2))2q(θ, W)2 ≤ q(θ, W)2,
(28)
Tw + W*
kwk
Iw + W*
kwk
≤ (s⅛) q(θ,W)2 ≤ z4τq(θ,W)2.
(29)
W
…•虱仇W) ( M
—
2
—
Now, What is left is to bound ((1 一 θ) p(0, w) 一 p(θ, w))2, using a multiple of q(θ, w). Recall
that
p(θ, w) =	ɪ	Z 2 cos(φ)ξ	(sec(φ))	dφ, q(θ, w)	= ɪ	Z 2 sin(φ)ξ	(sec(φ)) dφ.
2π	J-π+θ	kw IHI )	2π	J-π+θ	kw IHI )
We first show that both ((1 一 θ) p(0, w) 一 p(θ, w))2 and q(θ, w) are symmetric with respect to
θ = ∏2 on [0, π]. This is because
q(π -θ, w)=2∏ Zn-θ sin(φ)ξ (sewr)dφ
=q(θ, w)+2∏ Zn -θ+θ sin(φ)ξ (sewφ))dφ=q(θ, W).
25
Published as a conference paper at ICLR 2019
and
1 — π——-)p(0, W) — p(π — θ, W)
π
π
θ 1 广
c--s(φ c	CosS)ξ
π 2π J - ∏
π
2
cos(φ)ξ
π
一2
1
dφ - 2∏ Jn
π
2
cos(φ)ξ
e(φ)
Ilwk
-θ
1
dφ + 2∏
p(0, w) - p(θ, w) .
SeC(O)"不
Iwr 尸 φ
Z-2- cos(φ)ξ (sew))dφ
Therefore, it suffices to consider θ ∈ [∏2,π] only. Then calling Lemma 13 for θ ∈ [∏,π], We have
p(θ, w) ≤ q(θ, w) and(1 — -) p(0, w) ≤ q(θ, w). Therefore,
p(0, w)
≤ q(θ, w)2.
Combining the above estimate together With (27), (28) and (29), and using Cauchy-SchWarz in-
equality, We have
kEZ[gcrelu(v, w; Z)]k2 ≤ 4
2+ π42) q(θ, w)2(v>v*)2),
Where p(0, w) and q(θ, w) are uniformly bounded. This completes the proof.
□
Lemma 8. WhenAlgorithm 1 converges, EZ [段(v, w; Z)] andEZ [gcreiu(v, w; Z)] vanish Simul-
taneously, which only occurs at the
1.	Saddle points where (8) is satisfied according to Proposition 1.
2.	Minimizers of(2) where V = v*, θ(w, w*) = 0, or V = (Im + 1m1>b)-i (1m1>b — Im)v*
θ(w, w*) = π.
Proof of Lemma 8. The proof of Lemma 8 is similar to that of Lemma 6, and We omit it here. The
core part is that q(θ, w) defined in Lemma 12 is non-negative and equals 0 only at θ = 0, π, as Well
as p(0, w) ≥ p(θ, w) ≥ p(π, w) = 0.	□
Lemma 9. Let μ(x) = X in (5). Then the expected coarse partial gradient w.r.t. w is
EZhgid(V, w； Z)i = √1= (kvk2 iiwi∣ — (V>v*)w* ).
2π	kwk
If θ(w, w*) = π and V = (Im + 1悄1工)-1(，1"—Im)v*,
胆Zhgid(V, w； Z)H= √2m -J) am*2 ≥ 0,
2π(m + 1)2
i.e., EZ gid(V, w； Z) does not vanish at the local minimizers if1>mV* 6= 0 and m > 1,.
Proof of Lemma 9. By (5),
gμ(V, w; Z) = Z> (μ0(Zw) Θ v) ^V>σ(Zw) — (V*)>σ(Zw*)).
26
Published as a conference paper at ICLR 2019
Using the facts that μ0 = 1 and σ = 1{χ>0}, we have
EZ [gid(v, w; Z)] = EZ
m
vi1{Zτw>0} - Ev*1{Z>w*>0}
i=1
mm
ViZi)]
XXViVje [zi1{zτw>0)] - XXv*Vje IZi 1{z}w*>0}]
i=1j=1
i=1j=1
同2扁-(V>V*)W*)
In the last equality above, we called the third identity in Lemma 11. If θ(w, w*) = π and v
(Im + 1m 1m)	(1m1K - Im)v*, then
IIEz [gid(v, w; Z)]k = √2= IVT(V + v*)∣
=√2=	|(v*)T(ImIm -	Im)(Im + 1m1M)	^((Im	+	1m1M)	lm1*	-	Im)	+	Im) v* |
=-L kv*)>(1m1m - Im) (Im + ^^)-1(〃 + 1m ^)-1 1m (1；^* ) |
2=
-2=(m + 1)2
2(m - 1)
-2=(m + 1)2
∣(v*)>(1mim - Im)1m(1>nvt)∖
Kv*)2.
In the third equality, We used the identity (Im + 1m1工)1m=(m + 1)1m twice.
Lemma 10. If W = 0n and θ(w, w*) ∈ (0, =) ,then the inner product between the expected coarse
and true gradients w.r.t. W is
(EZ[gid(v，w;Z)],f (v，W)) = Sin⅞wiWI))(VTV*)2 ≥ 0.
When θ(w, w*) → π, V → (Im + 1m】m)1(1m1m — Im)v*, if 1mv* = 0 and m > 1, we have
JlEZ[gid(v，w； Z)]『_____________________________________> +∞.
Il ∂v(v, W)Il+ DEZ[s⅛(v, w；Z)], ∂f (v, W)E
Proof of Lemma 10. By Lemmas 2 and 4, we have
f (V W)
and
vTv*	(In
2=kwk 向
ww>
ww>
W2
W*
W*
EZ Igid(v, w; Z)
kvk2高-(V>v*)W*
m m
2
—
—
—
Since
—
□
WWrr) W = 0n and ∣∣w*∣ = 1, if θ(w, w*) = 0, =, then we have
EZ [gid(v, w; Z)], ∂W(v, w)
(vT v*)2	(w* )丁 (In
—
稿2) w*
Z T *、2	1	(WTw*)2
(VTV*)2	1- kwk2
(—=)3IlWll J1 - (WTw*)2
V	Ilwk2
(VTV*)2	sin(θ(w, w*)).
/	(—=)3∣∣wk	KIn -告)w*
(VTV*)2	S^	(WTW*)2
(√2n)3∣∣w∣∣ V1 - kw∣∣2
(√2n)3∣w∣∣
27
Published as a conference paper at ICLR 2019
When θ(w,w*) → ∏, V → (Im + 1m1工)-1(1m1工-Im)v*,
both
and(Ez Igid(v, w； Z)], ∂f (v, w))
converge to 0.
But if 1mv* =
Ildf (v, W)Il
0 and m > 1,
Ez gid (v, w； Z)	→
√2∏m-+))2 (Imv*产 > 0, WhiCh completes the proof.
□
Theorem 1. Let {(vt, wt)} be the Sequence generated by Algorithm 1 with ReLU μ(x) =
max{x,0} or clipped ReLU μ(x) = min{max{x,0}, 1}. Suppose ∣∣wtk ≥ Cw for all t with
some cw > 0. Then if the learning rate η > 0 is sufficiently small, for any initialization
(v0, w0), the objective sequence {f(vt, wt)} is monotonically decreasing, and {(vt, wt)} con-
verges to a saddle point or a (local) minimizer of the population loss minimization (2). In addi-
tion, if 1>mv* 6= 0 and m > 1, the descent and convergence properties do not hold for Algorithm
1 with the identity function μ(x) = X near the local minimizers satisfying θ(w, w*) = π and
v = (Im+1m1>m)-1(1m1>m - Im)v*.
Proof of Theorem 1 . We first prove the upper boundedness of {vt}. Due to the coerciveness of
f (v, w) W.r.t v, there exists Cv > 0, such that ∣v∣ ≤ Cv for any v ∈ {v ∈ Rm : f(v, w) ≤
f(v0, w0) for some w}. In particular, ∣v0∣ ≤ Cv. Using induction, suppose We already have
f(vt, wt) ≤ f (v0, w0) and ∣vt∣ ≤ Cv. If θ(wt, w*) = 0 or π, then θ(wT, w*) = 0 or π for all
T ≥ t, and the original problem reduces to a quadratic program in terms ofv. So {vt} Will converge
to v* or (Im + 1m1>m)-1(1m1>m - Im)v* by choosing a suitable step size η. In either case, We have
∣∣Ez h券(vt, wt； Z)[ Il and ∣∣Ez Igrelu(Vt, wt; Z)] ∣∣ both converge to 0. Else if θ(wt, w*) ∈ (0,π),
We define for any a ∈ [0, 1] that
vt(a) := vt - a(vt+1 - vt) = vt - aηEz
,∂'/ t t f
∂V (vt, Wt ；Z)
and
wt (a) := wt - a(wt+1 - wt) = wt - aηEz grelu (v t, wt； Z) ,
Which satisfy
vt(0) = vt, vt(1) = vt+1, wt(0) = wt, wt(1) = wt+1.
Let us fix 0 < c ≤ CW and C ≥ Cv. By the expressions of EZ [岩(vt, wt； Z)] and
EZ [grelu(vt, wt; Z)] given in Lemma 4, and since ∣∣wt∣ = 1, for sufficiently small η depending
on Co and cw, with η ≤ η, it holds that ∣∣vt(a)k ≤ C and ∣∣wt(a)k ≥ C for all a ∈ [0,1]. Possibly
at some point ao where θ(wt(ao), w*) = 0 or π, the partial gradient ∂df (vt(a0), wt(a0)) does not
exist. Otherwise, ∣∣ ∂f (vt(a), wt(a))∣∣ is uniformly bounded for all a ∈ [0,1]/{a0}, which makes
it integrable over the interval [0, 1]. Then for some constants L and Arelu depending on C and C, we
28
Published as a conference paper at ICLR 2019
have
f (vt+1, wt+1) =f(vt+(vt+1 - vt), wt + (wt+1 - wt))
f (vt, wt) +
vt+1
+
(vt (a), wt(a)), wt+1 -
(vt, wt), EZ grelu (vt, w
-η
AreluLη
η -
≤ f(vt, wt) - η - (1 + Arelu)
(vt , wt), EZ grelu (vt, w
(30)
—
The third equality is due to the fundamental theorem of calculus. In the first inequality, we called
Lemma 3 for (vt , wt) and (vt(a), wt(a)) with a ∈ [0, 1]/{a0}. In the last inequality, we used
Lemma 5. So when η < ηo := min I (I+4)L,η1, we have f(vt+1, wt+1) ≤ f(vt, Wt) ≤
f (v0, w0), and thus kvt+1 k ≤ Cv.
Summing up the inequality (30) over t from 0 to ∞ and using f ≥ 0, we have
X(1 - (1 + Arelu)L2η) I ∂V(vt, Wt) 2 +(1 - Are2Lη)(∂W(vt, wt), EZ [9^^, wt； Z)D
≤ f(v0, w0)∕η < ∞.
Hence,
lim
t→
0
and
lim
t→
(vt, wt), EZ grelu (vt, wt;
0.
Invoking Lemma 5 again, we further have
lim IIIEZ hgrelu (vt, wt; Z)iIII = 0.
Invoking Lemma 6, We have that coarse gradient descent with ReLU μ(χ) (SUbseqUentiany) Con-
verges to a saddle point or a minimizer.
Using Lemmas 7, 8 and similar argUments, we can prove the convergence of coarse gradient descent
with clipped ReLU STE.
The second claim follows from Lemmas 9 and 10.
□
F. Convergence to Global Minimizers
We prove that if the initialization weights (v0, w0) satisfy (v0)>v* > 0, θ(w0, w*) ‹ ∏2 and
(imv*)(imv0) ≤ (imʃv*)2, then we have convergence guarantee to global optima by using the
vanilla or clipped ReLU STE.
29
Published as a conference paper at ICLR 2019
Theorem 2. Under the assumptions of Theorem 1, if further the initialization (v0 , w0) satisfies
(v0)>v* > 0, θ(w0, w*) < ∏2 and (1/幻")(1工v0) ≤ (1^v*)2, then by using the vanila or clipped
ReLU STEfor sufficiently learning rate η > 0, we have (Vt)>v* > 0 and θ(wt, w*) ‹ ∏2 for all
t > 0, and {(vt, wt)} converges to a global minimizer.
Proof of Theorem 2. Proof by induction. Suppose (vt)>v* > 0, θ(wt, w*) < ∏2 and
(Imv*)(1^vt) ≤ (1^v*)2. Then for small enough η, We have
(Vt+1)>V*= (Vt- 4 ((Im + Imim )vt- ((l - Z θ(wt, W*)) Im + "^*)[ V*
=(i — 4)(vt)>v*+4 ((imv*)2 - (imv*)(imv>))
+ 4 (l - 2θ(wt, w*))kv*k2 > 0.
and
(ImVt+1)(imv*) = (ι — (m+Sh) (imvt)Kv*)+4 (m+ι - 2。Μ W*))(imv*)2
≤ (1 - 2Zθ(w,W*))amV*)2 ≤ (imv*)2.
Moreover, by Lemmas 4, 5 and 7, both EZ [grelu (V, w; Z)] and EZ[gcrelu(V, w; Z)] can be Written in
the form of ɑι (Im 一 Wwww>■) w* + α2w, where ɑι ≤ 0 and α2 is bounded by a constant depending
on Cv and cw . Therefore,
(wt+1)>w* = (1 - ηα2)(wt)>w* - αι(w*)> (Im - ww2) W* ≥ (1 - ηα2)(wt)>w* > 0,
kwk2
and thus θ(wt+1, w*) < 2. Finally, since {(Vt, wt)} converges, it can only converge to a global
minimizer.	□
30