Published as a conference paper at ICLR 2019
Boosting Robustness Certification
of Neural Networks
Gagandeep Singh, Timon Gehr, Markus Puschel, Martin Vechev
Department of Computer Science
ETH Zurich, Switzerland
{gsingh,timon.gehr,pueschel,martin.vechev}@inf.ethz.ch
Ab stract
We present a novel approach for the certification of neural networks against ad-
versarial perturbations which combines scalable overapproximation methods with
precise (mixed integer) linear programming. This results in significantly better
precision than state-of-the-art verifiers on challenging feedforward and convolu-
tional neural networks with piecewise linear activation functions.
1 Introduction
Neural networks are increasingly applied in critical domains such as autonomous driving (Bojarski
et al., 2016), medical diagnosis (Amato et al., 2013), and speech recognition (Hinton et al., 2012).
However, it has been shown by Goodfellow et al. (2014) that neural networks can be vulnerable
against adversarial attacks, i.e., imperceptible input perturbations cause neural networks to misclas-
sify. To address this challenge and prove that a network is free of adversarial examples (usually, in a
region around a given input), recent work has started investigating the use of certification techniques.
Current verifiers can be broadly classified as either complete or incomplete.
Complete verifiers are exact, i.e., if the verifier fails to certify a network then the network is non-
robust (and vice-versa). Existing complete verifiers are based on Mixed Integer Linear Programming
(MILP) (Lomuscio & Maganti, 2017; Fischetti & Jo, 2018; Dutta et al., 2018; Cheng et al., 2017)
or SMT solvers (Katz et al., 2017; Ehlers, 2017). Although precise, these can only handle net-
works with a small number of layers and neurons. To scale, incomplete verifiers usually employ
overapproximation methods and hence they are sound but may fail to prove robustness even if it
holds. Incomplete verifiers use methods such as duality (Dvijotham et al., 2018), abstract interpre-
tation (Gehr et al., 2018; Singh et al., 2018; 2019), linear approximations (Weng et al., 2018; Wong
& Kolter, 2018; Zhang et al., 2018), semidefinite relaxations (Raghunathan et al., 2018), combi-
nation of linear and non-linear approximation (Xiang et al., 2017), or search space discretization
(Huang et al., 2017). Incomplete verifiers are more scalable than complete ones, but can suffer from
precision loss for deeper networks. In principle, incomplete verifiers can be made asymptotically
complete by iteratively refining the input space (Wang et al., 2018a) or the neurons (Wang et al.,
2018b); however, in the worst case, this may eliminate any scalability gains and thus defeat the
purpose of using overapproximation in the first place.
This work: boosting complete and incomplete verifiers. A key challenge then is to design a
verifier which improves the precision of incomplete methods and the scalability of complete ones.
In this work, we make a step towards addressing this challenge based on two key ideas: (i) a combi-
nation of state-of-the-art overapproximation techniques used by incomplete methods, including LP
relaxations, together with MILP solvers, often employed in complete verifiers; (ii) a novel heuristic,
which points to neurons whose approximated bounds should be refined. We implemented these ideas
in a system called RefineZono, and showed that is is faster than state-of-the-art complete verifiers on
small networks while improving precision of existing incomplete verifiers on larger networks.
The recent works of (Wang et al., 2018b) and Tjeng et al. (2019) have also explored the combination
of linear programming with overapproximation. However, both use simpler and coarser overap-
proximations than ours. Our evaluation shows that RefineZono is faster than both for complete
verification. For example, RefineZono is faster than the work of Tjeng et al. (2019) for the complete
1
Published as a conference paper at ICLR 2019
x1l = 4.875
xi3 = 4.875
+ 0.625 ∙ ηι
+ 1.875 ∙ η2
-0.625 ∙ η3
+ 0.25 ∙ η4
l13 = 3.25
x^7 = 1.75
+ 0.25 ∙ ηι
+0.75 ∙ η2
-0.25 ∙ η3
l7 = 1
x^9 = 1.75
+0.25 ∙ ηι
+0.75 ∙ η2
-0.25 ∙ η3
l9 = 1
+ 0.625 ∙ ηι
+ 1.875 ∙ η2
-0.625 ∙ η3
+ 0.25 ∙ η4
l11 = 3.25
x^1 = 0.5
+0.5 ∙ ηι
l1 = 0
x5 = 1
+0.5 ∙ ηι
+0.5 ∙ η2
l5 = 0
X^3 = 1
+0.5 ∙ηι
+0.5 ∙ η2
l3 = 0
l2 = 0
u4 = 1
u2 = 1
+0.75 ∙ η2
-0.25 ∙ η3
l8 = -1
+ 0.375 ∙ η2
-0.125 ∙ η3
-0.25 ∙ η4
l12 = 2
l4 = -1
+0.375 ∙ η2
-0.125 ∙ η3
-0.25 ∙ η4
l14 = 2
-0.25 ∙ η2
+0.25 ∙ η3
l6 = -0.5
+0.375 ∙ η2
-0.125 ∙ η3
+0.25 ∙ η4
l10 = -0.5
u6 = 1
u8 = 1
u10 = 1
u12 = 3
u14 = 3
Figure 1: Robustness analysis of a toy example neural network using our method. Here, approxi-
mation results computed with DeepZ (blue box) are refined using MILP whereas those in green are
refined using LP.
verification of a 3 × 50 network, while for the larger 9 × 200 network their method does not finish
within multiple days on images which RefineZono verifies in ≈ 14 minutes.
Main contributions. Our main contributions are:
•	A refinement-based approach for certifying neural network robustness that combines the
strengths of fast overapproximation methods with MILP solvers and LP relaxations.
•	A novel heuristic for selecting neurons whose bounds should be further refined.
•	A complete end-to-end implementation of our approach in a system called RefineZono,
publicly available at https://github.com/eth-sri/eran.
•	An evaluation, showing that RefineZono is more precise than existing state-of-the-art in-
complete verifiers on larger networks and faster (while being complete) than complete ver-
ifiers on smaller networks.
2	Overview
We now demonstrate how our method improves the precision ofa state-of-the-art incomplete verifier.
The main objective here is to provide an intuitive understanding of our approach; full formal details
are provided in the next section.
Consider the simple fully connected feedforward neural network with ReLU activations shown in
Fig. 1. There are two inputs to the network, both in the range [0, 1]. The network consists of an
input layer, two hidden layers, and one output layer. Each layer consist of two neurons each. For
our explanation, we separate each neuron into two parts: one represents the output of the affine
transformation while the other captures the output of the ReLU activation. The weights for the
affine transformation are represented by weights on the edges. The bias for each node is shown
above or below it. Our goal is to verify that for any input in [0, 1] × [0, 1], the output at neuron x13
is greater than the output at x14.
We now demonstrate how our verifier operates on this network. We assume that the analysis results
after the second affine transformation are refined using a MILP formulation of the network whereas
the results after the third affine transformation are refined by an LP formulation of the network. In
2
Published as a conference paper at ICLR 2019
ReLU(X)
Figure 2: ReLU transformers, computing an affine form. Here, lx , ux are the original bounds,
whereas lx0 , u0x are the refined bounds. The slope of the two non-vertical parallel blue lines is
λ = ux/(ux - lx) and the slope of the two non-vertical parallel green lines is λ0 = u0x/(u0x - l0x).
The blue parallelogram is used to compute an affine form in DeepZ, whereas the green parallelogram
is used to compute the output of the refined ReLU transformer considered in this work.
the next section, we will explain our heuristic for selecting MILP or LP formulations of different
neurons in the network. Our analysis leverages the Zonotope domain (Ghorbal et al., 2009) together
with the abstract Zonotope transformers specialized to neural network activations as used in DeepZ
(Singh et al., 2018), a state of the art verifier for neural network robustness. The Zonotope domain
associates an affine form X with each neuron X in the network:
p
^ ：= co + Eci ∙ η
i=1
Here, c0, ci ∈ R are real coefficients and ηi ∈ [si, ti] ⊆ [-1, 1] are the noise symbols, which are
shared between the affine forms for different neurons. This sharing makes the domain relational and
thus more precise than non-relational domains such as Interval (Box). An abstract element in our
analysis is an intersection between a Zonotope (given as a list of affine forms) and a bounding box.
Thus, for each neuron x, we keep the affine form X and an interval [lχ, Uχ].
First layer. Our analysis starts by setting
Xi = 0.5 + 0.5 ∙ η1,l1 = 0,uι = 1
and
X2 = 0.5 + 0.5 ∙ η2,l2 = 0,u2 = 1,
representing the input [0, 1] at X1 and [0, 1] at X2 in our domain, respectively. Next, an affine trans-
formation is applied on the inputs resulting in the output
X3 = Xi + X2 = 1+0.5 ∙ ηι + 0.5 ∙ η2,l3 = 0,u3 = 2
and
X4 = Xi — X2 = 0.5 ∙ ηι — 0.5 ∙ η2,l4 = —1, u4 = 1.
Note that the Zonotope affine transformer is exact for this transformation. Next, the Zonotope ReLU
transformer is applied. We note that as l3 ≥ 0, the neuron X3 provably takes only non-negative
values. Thus, the ReLU Zonotope transformer outputs X5 = X3 and We set l5 = l3,u5 = l3 which is
the exact result. For X4 , l4 < 0 and u4 > 0 and thus neuron X4 can take both positive and negative
values. The corresponding output does not have a closed affine form and hence the approximation
in blue shown in Fig. 2 is used to compute the result. This approximation minimizes the area of the
result in the input-output plane and introduces a new noise symbol η3 ∈ [—1, 1]. The result is
X6 = 0.25 + 0.25 ∙ ηι — 0.25 ∙ η2 + 0.25 ∙ η3,l6 = —0.5, U6 = 1.
Note that the Zonotope approximation for X6 from Fig. 2 permits negative values whereas X6 can
only take non-negative values in the concrete. This overapproximation typically accumulates as the
analysis progresses deeper into the network, resulting in overall imprecision and failure to prove
properties that actually hold.
3
Published as a conference paper at ICLR 2019
MILP-based refinement at second layer. Next, the analysis handles the second affine transfor-
mation and computes
X7 = X5 - X6 + 1 = 1.75 + 0.25 •5 +0.75 •小一0.25 ∙ η3,l7 = 0.5,u7 = 3
and
X8 = X5 - X6 - 1 = -0.25 + 0.25 ∙ m + 0.75 ∙ η - 0.25 ・隼,18 = -1.5, u = 1.
Here, x7 is provably positive, whereas x8 can take both positive and negative values. Due to the ap-
proximation for x6, the bounds for x7 and x8 are imprecise. Note that the DeepZ ReLU transformer
for x8 applied next will introduce more imprecision and although the ReLU transformer for prov-
ably positive inputs such as x7 does not lose precision with respect to the input, it still propagates
the imprecision in the computation of the abstract values for x7 .
Thus, to reduce precision loss, in our method we refine the bounds for both x7 and x8 by formulating
the network up to (and including) the second affine transformation as a MILP instance based on a
formulation from Tjeng et al. (2019) and compute bounds for x7 and x8 , respectively. The MILP
solver improves the lower bounds for x7 and x8 to 1 and -1, respectively, which then updates the
corresponding lower bounds in our abstraction, i.e., l7 = 1 and l8 = -1.
Next, the ReLU transformer is applied. Since χ7 is Provably positive, We get X9 = X7,l9 = l7, and
u9 = u7 . We note that x8 can take both positive and negative values and is therefore approximated.
HoWever, the ReLU transformer noW uses the refined bounds instead of the original bounds and thus
the approximation shoWn in green from Fig. 2 is used. This approximation has smaller area in the
input-output plane compared to the blue one and thus reduces the approximation error. The result is
X10 = 0.125 + 0.125 ∙ m +0.375 ∙ η - 0.125 ∙小 + 0.25	= -0.5,u8 = 1.
LP-based refinement at final layer. Continuing With the analysis, We noW process the final affine
transformation, Which yields
X11 = 4.875 + 0.625 ∙ ηι + 1.875 ∙ η - 0.625 ∙ η3 + 0.25 ∙ η4,ln = 1.75, uπ = 8.25
and
X12 = 2.625 + 0.125 ∙m +0.375 •小-0.125 •小-0.25 •%加=1.75,ui2 = 3.5.
Due to the approximations from previous layers, the computed values can be imprecise. We note
that, as the analysis proceeds deeper into the netWork, refining bounds With MILP becomes expen-
sive. Thus, We refine the bounds by encoding the netWork up to (and including) the third affine
transformation using the faster LP relaxation of the netWork based on Ehlers (2017) and compute
the bounds for x11 and x12, respectively. This leads to better results for l11 = 3.25, l12 = 2, and
u12 = 3. As both x11 and x12 are provably positive, the subsequent ReLU transformations set
X13 = X11,l13 = l11,u13 = u11 and X14 = X12 ,l14 = l12,u14 = u12.
Proving robustness. Since the loWer bound l13 for x13 is greater than the upper bound u14 for
x14, our analysis can prove that the given neural netWork provides the same label for all inputs in
[0, 1] × [0, 1] and is thus robust. In contrast, DeepZ Without our refinement Would compute
X13 = 4.95 + 0.6 ∙ ηι + 1.8 ∙ η2 — 0.6 •m + 0.3 ∙ η4,l13 = 1.65, u13 = 8.25
and
X14 = 2.55 + 0.15 ∙ ηι + 0.45 •小一0.15 •小一0.3 ∙ η4,l14 = 1.5, u14 = 3.6.
As a result, DeepZ fails to prove that x13 is greater than x14, and thus fails to prove robustness.
Generalization to other abstractions. We note that our refinement-based approach is not re-
stricted to the Zonotope domain. It can be extended for refining the results computed by other
abstractions such as Polyhedra (Singh et al., 2017) or the abstraction used in DeepPoly (Singh et al.,
2019). For example, the ReLU transformer in Singh et al. (2019) also depends on the bounds of
input neurons and thus it Will benefit from the precise bounds computed using our refinement. Since
DeepPoly often produces more precise results than DeepZ, We believe a combination of this Work
With DeepPoly Will further improve verification results.
4
Published as a conference paper at ICLR 2019
3 Our Approach
We now describe our approach in more formal terms. As in the previous section, we will consider
affine transformations and ReLU activations as separate layers. As illustrated earlier, the key idea
will be to combine abstract interpretation (Cousot & Cousot, 1977) with exact and inexact MILP
formulations of the network, which are then solved, in order to compute more precise results for
neuron bounds. We begin by describing the core ingredients of abstract interpretation.
Our approach requires an abstract domain An over n variables (i.e., some set whose elements can
be encoded symbolically) such as Interval, Zonotope, the abstraction in DeepPoly, or Polyhedra. An
abstract domain has a bottom element ⊥ ∈ An as well as the following components:
•	A (potentially non-computable) concretization function γn : An → P (Rn) that associates
with each abstract element a ∈ An the set of concrete points from Rn that it abstracts. We
have γn(⊥) = 0.
•	An abstraction function αn : Bn → An, where X ⊆ γn(αn(X)) for all X ∈ Bn. We assume
that αn(Qi[li, ui]) is a computable function of l, u ∈ Rn. Here, Bn = Sl,u∈Rn Qi[li, ui]
and Qi[li, ui] = {x ∈ Rn | li ≤ xi ≤ ui}. (For many abstract domains, αn can be defined
on a larger domain Bn , but in this work, we only consider Interval input regions.)
•	A bounding box function ιn : An → Rn × Rn, where γn(a) ⊆ Qi[li, ui] for (l, u) = ιn(a)
for all a ∈ An .
•	A meet operation a u L for each a ∈ An and linear constraints L over n real variables,
where {x ∈ γn(a) | L(x)} ⊆ γn(a u L).
•	An affine abstract transformer T→Aχ+b: Am → An for each transformation of the form
(x 7→ Ax + b) : Rm → Rn , where
{Ax + b | X ∈ Yn(O)} ⊆ Yn(Tx→Ax+b(O))
for all a ∈ Am .
• A ReLU abstract transformer TR#eLU|	: An → An, where
e	i[li,ui]
{ReLU(x) | x ∈ Yn(O) ∩ Y[li, ui]} ⊆ TReLU|Q [l ,u ] (O)
i
for all abstract elements O ∈ An and for all lower and upper bounds l, u ∈ Rn on input
activations of the ReLU operation.
Verification via Abstract interpretation. As first shown by Gehr et al. (2018), any such abstract
domain induces a method for robustness certification of neural networks with ReLU activations.
For example, assume that we want to certify that a given neural network f : Rm → Rn considers
class i more likely than class j for all inputs X with ||X — x∣∣∞ ≤ E for a given X and e. We can
first use the abstraction function αm to compute a symbolic overapproximation of the set of possible
inputs X, namely
ain = αm({X ∈ Rm |||X - x∣∣∞ ≤ e}).
Given that the neural network can be written as a composition of affine functions and ReLU layers,
we can then propagate the abstract element Oin through the corresponding abstract transformers to
obtain a symbolic overapproximation Oout of the concrete outputs of the neural network.
For example, if the neural network f (X) = A0 ∙ ReLU(AX + b) + b0 has a single hidden layer with
h hidden neurons, We first compute a0 = TX→Ax+b(ai∏), which is a symbolic overapproximation
of the input to the ReLU activation function. We then compute (l, u) = ιh(O0) to obtain opposite
corners of a bounding box of all possible ReLU input activations, such that we can apply the ReLU
abstract transformer:
O00 = T #	(O0).
O	ReLU|Qi[li,ui] O .
Finally, We apply the affine abstract transformer again to obtain a°ut = Tx→a，χ+bo (a00). Using our
assumptions, we can conclude that the set Yn(Oout) contains all output activations that f can possibly
produce when given any of the inputs X. Therefore, if a°ut U (Xi ≤ Xj) = ⊥, we have proved the
property: for all X, the neural network considers class i more likely than class j.
5
Published as a conference paper at ICLR 2019
Incompleteness. While this approach is sound (i.e., whenever we prove the property, it actually
holds), it is incomplete (i.e., we might not prove the property, even if it holds), because the ab-
stract transformers produce a superset of the set of concrete outputs that the corresponding concrete
executions produce. This can be quite imprecise for deep neural networks, because the overapprox-
imations introduced in each layer accumulate.
Refining the bounds. To combat spurious overapproximation, we use mixed integer linear pro-
gramming (MILP) to compute refined lower and upper bounds l0 , u0 after applying each affine ab-
stract transformer (except for the first layer). We then refine the abstract element using the meet
operator of the underlying abstract domain and the linear constraints li0 ≤ xi ≤ u0i for all input
activations i, i.e., we replace the current abstract element a by a0 = a u (Vi li0 ≤ xi ≤ u0i), and
continue analysis with the refined abstract element.
Importantly, we obtain a more refined abstract transformer for ReLU than the one used in DeepZ by
leveraging the new lower and upper bounds. That is, using the tighter bounds l0x , u0x for x, we define
the ReLU transformer for y := max(0, x) as follows:
p,	if iχ > 0,
y=(θ,	if Ux ≤ 0,
I λ ∙ x + μ + μ ∙ dew, otherwise.
Here λ = UUxP-, μ = - 2.(Ux —“), and Enew ∈ [-1,1] is anew noise symbol.
The refined ReLU transformer benefits from the improved bounds. For example, when lx < 0 and
ux > 0 holds for the original bounds then after refinement:
•	If lx0 > 0, then the output is the same as the input and no overapproximation is added.
•	Else if u0x ≤ 0, then the output is exact.
•	Otherwise, as shown in Fig. 2, the approximation with the tighter lx0 and u0x has smaller area
in the input-output plane than the original transformer that uses the imprecise lx and ux .
Obtaining constraints for refinement. To enable refinement with MILP, we need to obtain con-
straints which fully capture the behavior of the neural network up to the last layer whose abstract
transformer has been executed. In our encoding, we have one variable for each neuron and we write
(k)
xi to denote the variable corresponding to the activation of the i-th neuron in the k-th layer, where
the input layer has k = 0. Similarly, we write li(k) and ui(k) to denote the best derived lower and
upper bounds for this neuron.
From the input layer, we obtain constraints of the form li0 ≤ xi(0) ≤ ui0, from affine layers, we
obtain constraints of the form xi(k) = Pj ai(jk-1)x(jk-1) + bi(k-1) and from ReLU layers we obtain
constraints of the form xi(k) = max(0, xi(k-1)).
MILP. Let 夕(k) denote the conjunction of all constraints UP to and including those from layer k.
To obtain the best possible lower and upper bounds for layer k with p neurons, we need to solve the
following 2 ∙ P optimization problems:
li0(k) =	min	xi(k), for i = 1, . . . , p,
i	x(0)	x(k)	i
x1 ,...,xp
s.t. W(k)(x10),…,xpk))
u0i(k) =	max	xi(k), for i = 1, . . . , p.
i	x(0)	x(k)	i
x1 ,...,xp
s.t.产(xi0),…,xpk))
As was shown by Tjeng et al. (2019), such optimization problems can be encoded exactly as MILP
instances using the bounds computed by abstract interpretation and the instances can then be solved
using off-the-shelf MILP solvers to compute li0(k) and u0i(k).
6
Published as a conference paper at ICLR 2019
LP relaxation. While not introducing any approximation, unfortunately, current MILP solvers
do not scale to larger neural networks. It becomes increasingly more expensive to refine bounds
with the MILP-based formulation as the analysis proceeds deeper into the network. However, for
soundness it is not crucial that the produced bounds are the best possible: for example, plain abstract
interpretation uses sound bounds produced by the bounding box function ι instead. Therefore, for
deeper layers in the network, we explore the trade-off between precision and scalability by also
considering an intermediate method, which is faster than exact MILP, but also more precise than
abstract interpretation. We relax the constraints in 夕(k) using the bounds computed by abstract
interpretation in the same way as Ehlers (2017) to obtain a set of weaker linear constraints 夕LP). We
then use the solver to solve the relaxed optimization problems that are constrained by 夕LP) instead
of 夕(k), producing possibly looser bounds l0(k) and u0(k). Note that the encoding of subsequent
layers depends on the bounds computed in previous layers, where tighter bounds reduce the amount
of newly introduced approximation.
Anytime MILP relaxation. MILP solvers usually provide the option to provide an explicit time-
out after which the solver must terminate. In return, the solver may not be able to solve the instance
exactly, but it will instead provide lower and upper bounds on the objective function in a best-effort
fashion. This provides another way to compute sound but inexact bounds l0(k) and u0(k).
In practice, we choose a fraction θ ∈ (0, 1] of neurons in a given layer k and compute bounds for
them using MILP with a timeout T in a first step. In the second step, for a fraction δ ∈ [0, 1 - θ]
of neurons in the layer, We set the timeout to β ∙ T, where T is the average time taken by the MILP
solver to solve one of the instances from the first step and β ∈ [0, 1] is a parameter.
Neuron selection heuristic. To select the θ-fraction of neurons for the first step of the anytime
MILP relaxation for the k-th layer, we rank the neurons. If the next layer is a ReLU layer, we first
ignore all neurons whose activations can be proven to be non-positive using abstract interpretation
(i.e., using the bounds produced by ι), because in this case it is already known that ReLU will map
the activation to 0. The remaining neurons are ordered in up to two different ways, once by width
(k)	(k)
(i.e. neuron i has key ui - li ), and possibly once by the sum of absolute output weights. i.e.,
if the next layer is a fully connected layer x 7→ Ax + b, the key of neuron i is Pj |Ai,j |. If the
next layer is a ReLU layer, we skip the ReLU layer and use the weights from the fully connected
layer that follows it (if any). The two ranks of a neuron in both orders are added, and the θ-fraction
with smallest rank sum is selected and their bounds arerefined with a timeout of T whereas the next
δ-fraction of neurons are refined with a timeout of β ∙ T.
RefineZono: end-to-end approach. To certify robustness of deep neural networks, we combine
MILP, LP relaxation, and abstract interpretation. We first pick numbers of layers kMILP , kLP , kAI
that sum to the total number of layers of the neural network. For the analysis of the first kMILP
layers, we refine bounds using anytime MILP relaxation with the neuron selection heuristic. As
an optimization, we do not perform refinement after the abstract transformer for the first layer in
case it is an affine transformation, as the abstract domain computes the tightest possible bounding
box for an affine transformation of a box (this is always the case in our experiments). For the next
kLP layers, we refine bounds using LP relaxation (i.e., the network up to the layer to be refined is
encoded using linear constraints) combined with the neuron selection heuristic. For the remaining
kAI layers, we use abstract interpretation without additional refinement (however, this also benefits
from refinement that was performed in previous layers), and compute the bounds using ι.
Final property certification. Let k be the index of the last layer and p be the number of output
classes. We can encode the final certification problem using the output abstract element aout obtained
after applying the abstract transformer for the last layer in the network. Ifwe want to prove that class
i is assigned a higher probability than class j, it suffices to show that aout u (xi(k) ≤ x(jk)) = ⊥. If
this fails, one can resort to complete verification using MILP: the property is satisfied if and only if
the set of constraints 夕(k)(x10),..., Xpk)) ∧ (x(k) ≤ xjk)) is unsatisfiable.
7
Published as a conference paper at ICLR 2019
Table 1: Neural network architectures used in our experiments.
Dataset	Model	TyPe	#Neurons	#layers	Defense
MNIST	3 × 50	fully connected	160	3	None
	5 × 100	fully connected	510	5	DiffAI
	6 × 100	fully connected	610	6	None
	9 × 100	fully connected	910	9	None
	6 × 200	fully connected	1 210	6	None
	9 × 200	fully connected	1810	9	None
	ConvSmall	convolutional	3 604	3	None
	ConvBig	convolutional	34 688	6	DiffAI
	ConvSuper	convolutional	88 500	6	DiffAI
CIFAR10	6 × 100	fully connected	610	6	None
	ConvSmall	convolutional	4852	3	DiffAI
ACAS Xu	6 × 50	fully connected	305	6	None
4	Evaluation
We evaluate the effectiveness of our approach for the robustness verification of ReLU-based feedfor-
ward and convolutional neural networks. The results show that our approach enables faster complete
verification than the state-of-the-art complete verifiers: Wang et al. (2018b) and Tjeng et al. (2019),
and produces more precise results than state-of-the-art incomplete verifiers: DeepZ (Singh et al.,
2018) and DeepPoly (Singh et al., 2019), when complete certification becomes infeasible.
We implemented our approach in a system called RefineZono. RefineZono uses Gurobi (Gurobi Op-
timization, LLC, 2018) for solving MILP and LP instances and is built on top of the ELINA library
(eli, 2018; Singh et al., 2017) for numerical abstract domains. All of our code, neural networks, and
images used in our experiments are publicly available at https://github.com/eth-sri/eran.
Evaluation datasets. We used the popular MNIST (Lecun et al., 1998), CIFAR10 (Krizhevsky,
2009), and ACAS Xu (Julian et al., 2018) datasets in our experiments. MNIST contains grayscale
images of size 28 × 28 pixels whereas CIFAR10 contains RGB images of size 32 × 32. ACAS Xu
contains 5 inputs representing aircraft sensor data.
Neural networks. Table 1 shows 12 different MNIST, CIFAR10, and ACAS Xu feedforward
(FNNs) and convolutional networks (CNNs) with ReLU activations used in our experiments. Out
of these 4 were trained to be robust against adversarial attacks using DiffAI (Mirman et al., 2018)
whereas the remaining 8 had no adversarial training. The largest network in our experiments con-
tains > 88K neurons whereas the deepest network contains 9 layers.
Robustness properties. For MNIST and CIFAR10, we consider the L∞-norm (Carlini & Wagner,
2017) based adversarial region parameterized by ∈ R. Our goal here is to certify that the network
produces the correct label on all points in the adversarial region. For ACAS Xu, our goal is to verify
that the property φ9 (Katz et al., 2017) holds for the 6 × 50 network (known to be hard).
Experimental setup. All experiments for the 3 × 50 MNIST FNN and all CNNs were carried out
on a 2.6 GHz 14 core Intel Xeon CPU E5-2690 with 512 GB of main memory; the remaining FNNs
were evaluated on a 3.3 GHz 10 Core Intel i9-7900X Skylake CPU with a main memory of 64 GB.
Benchmarks. For each MNIST and CIFAR10 network, we selected the first 100 images from
the respective test set and filtered out those images that were not classified correctly. We consider
complete certification with RefineZono on the ACAS Xu network and the 3 × 50 MNIST network.
For the 3 × 50 network, we choose an for which the incomplete verifier DeepZ certified < 40% of
all candidate images. We consider incomplete certification for the remaining networks and choose
an for which complete certification with RefineZono becomes infeasible.
8
Published as a conference paper at ICLR 2019
Table 2: Precision and runtime of RefineZono vs. DeepZ and DeepPoly.
Dataset	Model		DeepZ		DeepPoly		RefineZono	
			precision(%)	time(s)	precision(%) time(s)		precision(%) time(s)	
MNIST	5 × 100	0.07	38	0.6	53	0.3	53	381
	6 × 100	0.02	31	0.6	47	0.2	67	194
	9 × 100	0.02	28	1.0	44	0.3	59	246
	6 × 200	0.015	13	1.8	32	0.5	39	567
	9 × 200	0.015	12	3.7	30	0.9	38	826
	ConvSmall	0.12	7	1.4	13	6.0	21	748
	ConvBig	0.2	79	7	78	61	80	193
	ConvSuper	0.1	97	133	97	400	97	665
CIFAR10	6 × 100	0.0012	31	4.0	46	0.6	46	765
	ConvSmall	0.03	17	5.8	21	20	21	550
4.1	Complete certification
RefineZono first runs DeepZ analysis on the whole network collecting the bounds for all neurons in
the network. If DeepZ fails to certify the network, then the collected bounds are used to encode the
robustness certification as a MILP instance (discussed in section 3).
ACAS Xu 6 × 50 network. As this network has only 5 inputs, we uniformly split the pre-condition
defined by φ9 to produce 6 300 smaller input regions. We certify that the post-condition defined by
φ9 holds for each region with RefineZono. RefineZono certifies that φ9 holds for the network in 227
seconds which is > 4x faster than the fastest verifier for ACAS Xu from Wang et al. (2018b).
MNIST 3 × 50 network. We use = 0.03 for the L∞-norm attack. We compare RefineZono
against the state-of-the-art complete verifier for MNIST from Tjeng et al. (2019). This approach
is also MILP-based like ours, but it uses Interval analysis and LP to determine neuron bounds. We
implemented the Interval analysis and LP-based analysis to determine the initial bounds. We call the
MILP solver only if LP analysis (or Interval analysis) fails to certify. All complete verifiers certify
the neural network to be robust against L∞-norm perturbations on 85% of the images. The average
runtime of RefineZono, MILP with bounds from the Interval analysis, and MILP with bounds from
the LP analysis are 28, 123, and 35 seconds respectively. Based on our result, we believe that the
Zonotope analysis offers a good middle ground between the speed of the Interval analysis and the
precision of LP for bound computation, as it produces precise bounds faster than LP.
4.2	Incomplete certification
We next compare RefineZono against DeepZ and DeepPoly for the incomplete robustness certifica-
tion of the remaining networks. We note that DeepZ has the same precision as Fast-Lin (Weng et al.,
2018) and DeepPoly has the same precision as CROWN (Zhang et al., 2018). The values used
for the L∞-norm attack are shown in Table 2. The values for networks trained to be robust are
larger than for networks that are not. For each verifier, we report the average runtime per image in
seconds and the precision measured by the % of images for which the verifier certified the network
to be robust. We note that running the Interval analysis to obtain initial bounds is too imprecise for
these large networks with the values considered in our experiments. As a result, the approach from
Tjeng et al. (2019) has to rely on applying LP per neuron to obtain precise bounds for the MILP
solver which does not scale. For example, on the 9 × 200 network, determining bounds with LP
already takes > 20 minutes (without calling the MILP solver which is more expensive than LP)
whereas RefineZono has an average running time of ≈ 14 minutes.
Parameter values. We experimented with different values of the analysis parameters kMILP , kLP ,
kAI , θ, δ, β, T and chose values that offered the best tradeoff between performance and precision for
the certification of each neural network. We refine the neuron bounds after all affine transformations
9
Published as a conference paper at ICLR 2019
that are followed by a ReLU except the first one. In a given layer, we consider all neurons that can
take positive values after the affine transformation as refinement candidates.
For the MNIST FNNs, we refine the bounds of the candidate neurons in layers 2-4 with MILP and
those in the remaining layers using LP. For MILP based refinement, We use θ = 5kω2p where ω is
the number of candidates and p is the total number of neurons in layer k. For LP based refinement,
we use θ = 2k-5.p. We use timeout T = 1 second, β = 0.5, and δ = P - θ for both MILP and LP
based refinements. For the CIFAR10 FNN, we use the same values except that we use θ = ^ωrp
for MILP refinement and set T = 6 seconds for both MILP and LP based refinement as it is more
expensive to refine neuron bounds in CIFAR10 networks due to these having more input neurons.
For the CNNs, the convolutional layers have large number of candidates so we do not refine these.
Instead, we refine all candidates in the fully connected layers with a larger timeout so to compen-
sate for the more difficult problem instances for the solver. For the MNIST ConvSmall, ConvBig
and CIFAR10 ConvSmall networks, we refine all the candidate neurons using MILP with T = 10
seconds. For the MNIST ConvSuper network, we refine similarly but use LP with T = 15 seconds.
Results for incomplete certification. Table 2 shows the precision and the average runtime of all
three verifiers. RefineZono either improves or achieves the precision of the state-of-the-art verifiers
on all neural networks. It certifies more images than DeepZ on all networks except the MNIST
ConvSuper network. This is because DeepZ is already very precise for the considered. We could
not try larger for this network, as the DeepZ analysis becomes too expensive. RefineZono certifies
the network to be more robust on more images than DeepPoly on 6 out of 10 networks.
It can be seen that the number of neurons in the network is not the determining factor for the average
runtime of RefineZono. We observe that RefineZono runs faster on the networks trained to be robust
and the top three networks with the largest runtime for RefineZono are all networks not trained to
be robust. This is because robust networks are relatively easier to certify and produce only a small
number of candidate neurons for refinement, which are easier to refine by the solver. For example,
even though the same parameter values are used for refining the results on the MNIST ConvSmall
and ConvBig networks, the average runtime of RefineZono on the robustly trained ConvBig network
with ≈ 35K neurons, 6 layers and a perturbation region defined using = 0.2 is almost 4 times less
than on the non-robust ConvSmall network with only 3 604 neurons, 3 layers and a smaller = 0.12.
4.3	Effect of neuron selection heuristic
We use the neuron selection heuristic from section 3 to determine neurons which need to be refined
more than others for FNNs, as refining all neurons in a layer with MILP can significantly slow down
the analysis. To check whether our heuristic can identify important neurons, we ran the analysis on
the MNIST 9 × 200 FNN by keeping all analysis parameters the same, except instead of selecting
the neurons with the smallest rank sum first we selected the neurons with the largest rank sum first
(thus refining neurons more if our heuristic deems them unimportant). With this change, the average
runtime does not change significantly. However, the modified analysis loses precision and fails to
certify two images that the analysis refining with our neuron selection heuristic succeeds on.
5	Conclusion
We presented a novel refinement-based approach for effectively combining overapproximation tech-
niques used by incomplete verifiers with linear-programming-based methods used in complete ver-
ifiers. We implemented our method in a system called RefineZono and showed its effectiveness on
verification tasks involving feedforward and convolutional neural networks with ReLU activations.
Our evaluation demonstrates that RefineZono can certify robustness properties beyond the reach of
existing state-of-the-art complete verifiers (these can fail due to scalability issues) while simultane-
ously improving on the precision of existing incomplete verifiers (which can fail due to using too
coarse ofan overapproximation).
Overall, we believe combining the strengths of overapproximation methods with those of mixed
integer linear programming as done in this work is a promising direction for further advancing the
state-of-the-art in neural network verification.
10
Published as a conference paper at ICLR 2019
References
ELINA: ETH Library for Numerical Analysis, 2018. URL http://elina.ethz.ch.
FiliPPo Amato, Alberto Lopez, Eladia Maria Pena-Mendez, Petr Vanhara, Ales HamPL and Josef
Havel. Artificial neural networks in medical diagnosis. Journal of Applied Biomedicine, 11(2):47
-58,2013.
Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat FlePP, Prasoon
Goyal, Lawrence D. Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, Xin Zhang, Jake Zhao,
and Karol Zieba. End to end learning for self-driving cars. CoRR, abs/1604.07316, 2016.
Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks. In
Proc. IEEE Symposium on Security and Privacy (SP), PP. 39-57, 2017.
Chih-Hong Cheng, Georg Nuhrenberg, and Harald Ruess. Maximum resilience of artificial neural
networks. In Automated Technology for Verification and Analysis (ATVA), 2017.
Patrick Cousot and Radhia Cousot. Abstract interPretation: a unified lattice model for static anal-
ysis of Programs by construction or aPProximation of fixPoints. In Proceedings of the 4th ACM
symposium on Principles of programming languages (POPL), PP. 238-252, 1977.
SouradeeP Dutta, Susmit Jha, Sriram Sankaranarayanan, and Ashish Tiwari. OutPut range analysis
for deeP feedforward neural networks. In NASA Formal Methods (NFM), 2018.
Krishnamurthy Dvijotham, Robert Stanforth, Sven Gowal, Timothy A. Mann, and Pushmeet Kohli.
A dual aPProach to scalable verification of deeP networks. CoRR, abs/1803.06567, 2018.
Rudiger Ehlers. Formal verification of piece-wise linear feed-forward neural networks. In Auto-
mated Technology for Verification and Analysis (ATVA), 2017.
Matteo Fischetti and Jason Jo. DeeP neural networks and mixed integer linear oPtimization. Con-
straints, 23(3):296-309, 2018.
T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov, S. Chaudhuri, and M. Vechev. AI2: Safety
and robustness certification of neural networks with abstract interPretation. In Proc. IEEE Sym-
posium on Security and Privacy (SP), volume 00, PP. 948-963, 2018.
Khalil Ghorbal, Eric Goubault, and Sylvie Putot. The zonotoPe abstract domain taylor1+. In Proc.
Computer Aided Verification (CAV), PP. 627-633, 2009.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. ExPlaining and harnessing adversarial
examPles. arXiv preprint arXiv:1412.6572, abs/1706.07351, 2014.
Gurobi OPtimization, LLC. Gurobi oPtimizer reference manual, 2018. URL http://www.
gurobi.com.
G. Hinton, L. Deng, D. Yu, G. E. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen,
T. N. Sainath, and B. Kingsbury. DeeP neural networks for acoustic modeling in sPeech recogni-
tion: The shared views of four research grouPs. IEEE Signal Processing Magazine, 29(6):82-97,
2012.
Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. Safety verification of deeP neural
networks. In Computer Aided Verification (CAV), PP. 3-29, 2017.
Kyle D. Julian, Mykel J. Kochenderfer, and Michael P. Owen. DeeP neural network comPression
for aircraft collision avoidance systems. CoRR, abs/1810.04240, 2018.
Guy Katz, Clark Barrett, David L. Dill, Kyle Julian, and Mykel J. Kochenderfer. ReluPlex: An
efficient SMT solver for verifying deeP neural networks. In Computer Aided Verification (CAV),
PP. 97-117, 2017.
Alex Krizhevsky. Learning multiPle layers of features from tiny images. Technical rePort, 2009.
11
Published as a conference paper at ICLR 2019
Yann Lecun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. In Proc. ofthe IEEE,pp. 2278-2324, 1998.
Alessio Lomuscio and Lalit Maganti. An approach to reachability analysis for feed-forward ReLU
neural networks. CoRR, 2017.
Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for prov-
ably robust neural networks. In Proc. International Conference on Machine Learning (ICML),
pp. 3575-3583, 2018.
Aditi Raghunathan, Jacob Steinhardt, and Percy S Liang. Semidefinite relaxations for certifying
robustness to adversarial examples. In Proc. Advances in Neural Information Processing Systems
(NeurIPS), pp. 10900-10910. 2018.
Gagandeep Singh, Markus PUsCheL and Martin Vechev. Fast polyhedra abstract domain. In Proc.
Principles of Programming Languages (POPL), pp. 46-59, 2017.
Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus Puschel, and Martin Vechev. Fast and
effective robustness certification. In Proc. Advances in Neural Information Processing Systems
(NeurIPS), pp. 10825-10836. 2018.
Gagandeep Singh, Timon Gehr, Markus Puschel, and Martin Vechev. An abstract domain for certify-
ing neural networks. Proc. ACM Program. Lang., 3(POPL):41:1-41:30, 2019. ISSN 2475-1421.
Vincent Tjeng, Kai Y. Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed
integer programming. In Proc. International Conference on Learning Representations (ICLR),
2019.
Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. Formal security analysis
of neural networks using symbolic intervals. In USENIX Security Symposium (USENIX Security
18), pp. 1599-1614, 2018a.
Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. Efficient formal
safety analysis of neural networks. In Proc. Advances in Neural Information Processing Systems
(NeurIPS), pp. 6369-6379. 2018b.
Lily Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane Boning,
and Inderjit Dhillon. Towards fast computation of certified robustness for ReLU networks. In
Proc. International Conference on Machine Learning (ICML), volume 80, pp. 5276-5285, 2018.
Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer ad-
versarial polytope. In Proc. International Conference on Machine Learning (ICML), volume 80,
pp. 5286-5295, 2018.
Weiming Xiang, Hoang-Dung Tran, and Taylor T. Johnson. Output reachable set estimation and
verification for multi-layer neural networks. CoRR, abs/1708.03322, 2017.
Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Efficient neural
network robustness certification with general activation functions. In Proc. Advances in Neural
Information Processing Systems (NeurIPS), pp. 4944-4953. 2018.
12