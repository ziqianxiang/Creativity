Published as a conference paper at ICLR 2019
A Variational Inequality Perspective on
Generative Adversarial Networks
Gauthier Gidel* 1,*	Hugo Berard1,3,*	Gaetan VignoudI	Pascal Vincent1,2,3
1Mila & DIRO, University of Montreal 2Canada CIFAR AI Chair
Simon Lacoste-Julien1,2	3Facebook Artificial Intelligence Research
Ab stract
Generative adversarial networks (GANs) form a generative modeling approach
known for producing appealing samples, but they are notably difficult to train.
One common way to tackle this issue has been to propose new formulations of
the GAN objective. Yet, surprisingly few studies have looked at optimization
methods designed for this adversarial training. In this work, we cast GAN optimiza-
tion problems in the general variational inequality framework. Tapping into the
mathematical programming literature, we counter some common misconceptions
about the difficulties of saddle point optimization and propose to extend techniques
designed for variational inequalities to the training of GANs. We apply averaging,
extrapolation and a computationally cheaper variant that we call extrapolation from
the past to the stochastic gradient method (SGD) and Adam.
1	Introduction
Generative adversarial networks (GANs) (Goodfellow et al., 2014) form a generative modeling
approach known for producing realistic natural images (Karras et al., 2018) as well as high quality
super-resolution (Ledig et al., 2017) and style transfer (Zhu et al., 2017). Nevertheless, GANs are
also known to be difficult to train, often displaying an unstable behavior (Goodfellow, 2016). Much
recent work has tried to tackle these training difficulties, usually by proposing new formulations of
the GAN objective (Nowozin et al., 2016; Arjovsky et al., 2017). Each of these formulations can
be understood as a two-player game, in the sense of game theory (Von Neumann and Morgenstern,
1944), and can be addressed as a variational inequality problem (VIP) (Harker and Pang, 1990), a
framework that encompasses traditional saddle point optimization algorithms (Korpelevich, 1976).
Solving such GAN games is traditionally approached by running variants of stochastic gradient
descent (SGD) initially developed for optimizing supervised neural network objectives. Yet it is
known that for some games (Goodfellow, 2016, §8.2) SGD exhibits oscillatory behavior and fails to
converge. This oscillatory behavior, which does not arise from stochasticity, highlights a fundamental
problem: while a direct application of basic gradient descent is an appropriate method for regular
minimization problems, it is not a sound optimization algorithm for the kind of two-player games
of GANs. This constitutes a fundamental issue for GAN training, and calls for the use of more
principled methods with more reassuring convergence guarantees.
Contributions. We point out that multi-player games can be cast as variational inequality problems
(VIPs) and consequently the same applies to any GAN formulation posed as a minimax or non-zero-
sum game. We present two techniques from this literature, namely averaging and extrapolation,
widely used to solve VIPs but which have not been explored in the context of GANs before.1
We extend standard GAN training methods such as SGD or Adam into variants that incorporate
these techniques (Alg. 4 is new). We also explain that the oscillations of basic SGD for GAN
training previously noticed (Goodfellow, 2016) can be explained by standard variational inequality
optimization results and we illustrate how averaging and extrapolation can fix this issue.
* Equal contribution, correspondence to firstname.lastname@Umontreal.ca.
1The preprints for (Mertikopoulos et al., 2019) and (YazICI et al., 2019), which respectively explored
extrapolation and averaging for GANs, appeared after our initial preprint. See also the related work section §6.
1
Published as a conference paper at ICLR 2019
We introduce a technique, called extrapolation from the past, that only requires one gradient computa-
tion per update compared to extrapolation which requires to compute the gradient twice, rediscovering,
with a VIP perspective, a particular case of optimistic mirror descent (Rakhlin and Sridharan, 2013).
We prove its convergence for strongly monotone operators and in the stochastic VIP setting.
Finally, we test these techniques in the context of GAN training. We observe a 4-6% improvement
over Miyato et al. (2018) on the inception score and the Frechet inception distance on the CIFAR-10
dataset using a WGAN-GP (Gulrajani et al., 2017) and a ResNet generator.2
Outline. §2 presents the background on GAN and optimization, and shows how to cast this
optimization as a VIP. §3 presents standard techniques and extrapolation from the past to optimize
variational inequalities in a batch setting. §4 considers these methods in the stochastic setting,
yielding three corresponding variants of SGD, and provides their respective convergence rates. §5
develops how to combine these techniques with already existing algorithms. §6 discusses the related
work and §7 presents experimental results.
2	GAN optimization as a variational inequality problem
2.1	GAN formulations
The purpose of generative modeling is to generate samples from a distribution qθ that matches best the
true distribution p of the data. The generative adversarial network training strategy can be understood
as a game between two players called generator and discriminator. The former produces a sample
that the latter has to classify between real or fake data. The final goal is to build a generator able to
produce sufficiently realistic samples to fool the discriminator.
In the original GAN paper (Goodfellow et al., 2014), the GAN objective is formulated as a zero-sum
game where the cost function of the discriminator DP is given by the negative log-likelihood of the
binary classification task between real or fake data generated from qθ by the generator,
min max L(θ, φ) where L(θ, φ) def - E [log Dp(x)] - E [log(1 - Dp(x0))].	(1)
θ p	X〜P	x0 〜qθ
However Goodfellow et al. (2014) recommends to use in practice a second formulation, called
non-saturating GAN. This formulation is a non-zero-sum game where the aim is to jointly minimize:
Lg(Θ, φ) def - E log Dp(x0) and LD(θ, φ) def - E log DP(X) - E log(1 - Dp(x0)). (2)
x0 〜qθ	X 〜P	x0 〜qθ
The dynamics of this formulation has the same stationary points as the zero-sum one (1) but is
claimed to provide “much stronger gradients early in learning” (Goodfellow et al., 2014) .
2.2	Equilibrium
The minimax formulation (1) is theoretically convenient because a large literature on games studies
this problem and provides guarantees on the existence of equilibria. Nevertheless, practical consider-
ations lead the GAN literature to consider a different objective for each player as formulated in (2).
In that case, the two-player game problem (Von Neumann and Morgenstern, 1944) consists in finding
the following Nash equilibrium:
θ* ∈ arg min LG (θ, ') and，∈ arg min LD (θ*, φ).	(3)
θ∈Θ	P∈Φ
Only when LG = -LD is the game called a zero-sum game and (3) can be formulated as a minimax
problem. One important point to notice is that the two optimization problems in (3) are coupled and
have to be considered jointly from an optimization point of view.
Standard GAN objectives are non-convex (i.e. each cost function is non-convex), and thus such
(pure) equilibria may not exist. As far as we know, not much is known about the existence of these
equilibria for non-convex losses (see Heusel et al. (2017) and references therein for some results). In
2Code available at https://gauthiergidel.github.io/projects/vip-gan.html.
2
Published as a conference paper at ICLR 2019
our theoretical analysis in §4, our assumptions (monotonicity (24) of the operator and convexity of
the constraint set) imply the existence of an equilibrium.
In this paper, we focus on ways to optimize these games, assuming that an equilibrium exists. As is
often standard in non-convex optimization, we also focus on finding points satisfying the necessary
stationary conditions. As we mentioned previously, one difficulty that emerges in the optimization of
such games is that the two different cost functions of (3) have to be minimized jointly in θ and φ.
Fortunately, the optimization literature has for a long time studied so-called variational inequality
problems, which generalize the stationary conditions for two-player game problems.
2.3 Variational inequality problem formulation
We first consider the local necessary conditions that characterize the solution of the smooth two-player
game (3), defining stationary points, which will motivate the definition of a variational inequality. In
the unconstrained setting, a stationary point is a couple (θ*, φ*) with zero gradient:
kVθ LG(θ*,φ*)∣∣ = ∣∣%Ld (θ*,φ*)∣∣ =0.	(4)
When constraints are present,3 a stationary point (θ*, φ*) is such that the directional derivative of
each cost function is non-negative in any feasible direction (i.e. there is no feasible descent direction):
VθLG(θ*,φ*)>(θ - θ*) ≥ 0 and VPLD(θ*,φ*)>(φ - φ*) ≥ 0 , ∀ (θ, φ) ∈ Θ X Φ. (5)
Defining ω =f (θ, φ), ω* =f (θ*, φ*), Ω =f Θ × Φ, Eq. (5) can be compactly formulated as:
F(ω*)>(ω 一 ω*) ≥ 0, ∀ω ∈ Ω where F(ω) =f [VθLg(Θ, φ) VrLD(θ, φ)]> .	(6)
These stationary conditions can be generalized to any continuous vector field: let Ω ⊂ Rd and
F : Ω → Rd be a continuous mapping. The Variationai inequality problem (Harker and Pang, 1990)
(depending on F and Ω) is:
find ω* ∈ Ω such that F(ω*)>(ω — ω*) ≥ 0, ∀ω ∈ Ω .	(VIP)
We call optimal set the set Ω* of ω ∈ Ω verifying (VIP). The intuition behind it is that any ω* ∈ Ω*
is a fixed point of the constrained dynamic of F (constrained to Ω).
We have thus showed that both saddle point optimization and non-zero sum game optimization, which
encompass the large majority of GAN variants proposed in the literature, can be cast as VIPs. In the
next section, we turn to suitable optimization techniques for such problems.
3	Optimization of Variational Inequalities (batch setting)
Let us begin by looking at techniques that were developed in the optimization literature to solve VIPs.
We present the intuitions behind them as well as their performance on a simple bilinear problem (see
Fig. 1). Our goal is to provide mathematical insights on averaging (§3.1) and extrapolation (§3.2)
and propose a novel variant of the extrapolation technique that we called extrapolation from the past
(§3.3). We consider the batch setting, i.e., the operator F(ω) defined in Eq. 6 yields an exact full
gradient. We present extensions of these techniques to the stochastic setting later in §4.
The two standard methods studied in the VIP literature are the gradient method (Bruck, 1977) and
the extragradient method (Korpelevich, 1976). The iterates of the basic gradient method are given
by ωt+ι = Po[ωt — nF(ωt)] where PΩ[∙] is the projection onto the constraint Set (if constraints
are present) associated to (VIP). These iterates are known to converge linearly under an additional
assumption on the operator4 (Chen and Rockafellar, 1997), but oscillate for a bilinear operator as
shown in Fig. 1. On the other hand, the uniform average of these iterates converge for any bounded
monotone operator with a O(1∕√t) rate (NediC and Ozdaglar, 2009), motivating the presentation of
averaging in §3.1. By contrast, the extragradient method (extrapolated gradient) does not require
any averaging to converge for monotone operators (in the batch setting), and can even converge at
the faster O(1/t) rate (Nesterov, 2007). The idea of this method is to compute a lookahead step (see
intuition on extrapolation in §3.2) in order to compute a more stable direction to follow.
3An example of constraint for GANs is to clip the parameters of the discriminator (Arjovsky et al., 2017).
4Strong monotonicity, a generalization of strong convexity. See §A.
3
Published as a conference paper at ICLR 2019
3.1	Averaging
More generally, we consider a weighted averaging scheme with weights ρt ≥ 0. This weighted
averaging scheme have been proposed for the first time for (batch) VIP by Bruck (1977),
T-1
ωτ =f E= ρtωt
T -1
def
ST =	ρt .
t=0
(7)
Averaging schemes can be efficiently implemented in an online fashion noticing that,
ωτ = (1 — PT )ωτ-1 + PTωτ Where 0 ≤ PT ≤ 1.	(8)
For instance, setting PT = T yields uniform averaging (Pt = 1) and Pt = 1 — β < 1 yields geometric
averaging, also knoWn as exponential moving averaging (Pt = βT-t , 1 ≤ t ≤ T). Averaging is
experimentally compared With the other techniques presented in this section in Fig. 1.
In order to illustrate hoW averaging tackles the oscillatory behavior in game optimization, We consider
a toy example where the discriminator and the generator are linear: DP(X) = φτX and Ge(Z) = θz
(implicitly defining qθ). By substituting these expressions in the WGAN objective,5 We get the
following bilinear objective:
min max	φτE[x] — φτθE[z] .	(9)
θ∈Θ p∈Φ,∣∣p∣∣≤1
A similar task was presented by Nagarajan and Kolter (2017) where they consider a quadratic
discriminator instead of a linear one, and show that gradient descent is not necessarily asymptotically
stable. The bilinear objective has been extensively used (Goodfellow, 2016; Mescheder et al., 2018;
Yadav et al., 2018) to highlight the difficulties of gradient descent for saddle point optimization.
Yet, ways to cope with this issue have been proposed decades ago in the context of mathematical
programming. For illustrating the properties of the methods of interest, we will study their behavior
in the rest of §3 on a simple unconstrained unidimensional version of Eq. 9 (this behavior can be
generalized to general multidimensional bilinear examples, see §B.3):
min max θ ∙ φ and	(θ*,φ*) = (0,0).	(10)
θ∈R φ∈R
The operator associated with this minimax game is F(θ, φ) = (φ, —θ). There are several ways to
compute the discrete updates of this dynamics. The two most common ones are the simultaneous and
the alternating gradient update rules,
Simultaneous update:
θt+1 = θt — ηφt
φt+1 = φt + ηθt
θt+1 = θt — ηφt
Alternating update:	. (11)
φt+1 = φt + ηθt+1
Interestingly, these two choices give rise to completely different behaviors. The norm of the si-
multaneous updates diverges geometrically, whereas the alternating iterates are bounded but do not
converge to the equilibrium. As a consequence, their respective uniform average have a different
behavior, as highlighted in the following proposition (proof in §B.1 and generalization in §B.3):
Proposition 1. The simultaneous iterates diverge geometrically and the alternating iterates defined
in (11) are bounded but do not converge to 0 as
Simultaneous:	θt2+1	+ φt2+1	= (1 +	η2)(θt2	+ φt2)	, Alternating:	θt2	+ φt2	= Θ(θ02	+ φ02)	(12)
where ut = Θ(vt) ⇔ ∃α, β, t0 > 0 such that ∀t ≥ t0, αvt ≤ ut ≤ βvt.
The uniform average (&, φt) d=f 1 PSl0(°s,Φs) ofthe simultaneous updates (resp. the alternating
updates) diverges (resp. converges to 0) as,
Simultaneous: Θ2+φ2 = Θ (诙[70 (1 + η2)t), Alternating: Θ2 + φ2 = Θ (^0+φ0) . (13)
This sublinear convergence result, proved in §B, underlines the benefits of averaging when the
sequence of iterates is bounded (i.e. for alternating update rule). When the sequence of iterates is not
bounded (i.e. for simultaneous updates) averaging fails to ensure convergence. This theorem also
shows how alternating updates may have better convergence properties than simultaneous updates.
5Wasserstein GAN (WGAN) proposed by Arjovsky et al. (2017) boils down to the following minimax
formulation: minθ∈θ maXp∈φ,∣∣D川∣l≤i Ex〜p[Dp(x)] - Ex，〜qa [Dp(x0)].
4
Published as a conference paper at ICLR 2019
3.2	Extrapolation
Another technique used in the variational inequality literature to prevent oscillations is extrapolation.
This concept is anterior to the extragradient method since Korpelevich (1976) mentions that the idea
of extrapolated “prices” to give “stability” had been already formulated by Polyak (1963, Chap. II).
The idea behind this technique is to compute the gradient at an (extrapolated) point different from the
current point from which the update is performed, stabilizing the dynamics:
Compute extrapolated point: ωt+1∕2 = Po[ωt - nF(ωt)],	(14)
Perform update step: ωt+ι = PΩ[ωt - nF(ωt+1∕2)].	(15)
Note that, even in the unconstrained case, this method is intrinsically different from Nesterov’s
momentum6 (Nesterov, 1983, Eq. 2.2.9) because of this lookahead step for the gradient computation:
Nesterov's method: ωt+1∕2 = ωt - nF(ωt),	ωt+ι = ωt+1∕2 + β(ωt+1∕2 - ωt).	(16)
Nesterov’s method does not converge when trying to optimize (10). One intuition of why extrapolation
has better convergence properties than the standard gradient method comes from Euler’s integration
framework. Indeed, to first order, we have ωt+1∕2 ≈ ωt+ι + o(η) and consequently, the update
step (15) can be interpreted as a first order approximation to an implicit method step:
Implicit step: ωt+1 = ωt - nF(ωt+1) .	(17)
Implicit methods are known to be more stable and to benefit from better convergence properties (Atkin-
son, 2003) than explicit methods, e.g., in §B.2 we show that (17) on (10) converges for any n. Though,
they are usually not practical since they require to solve a potentially non-linear system at each step.
Going back to the simplified WGAN toy example (10) from §3.1, we get the following update rules:
θt+1 = θt - nφt+1	θt+1 = θt - n(φt + nθt)
Implicit:	, Extrapolation:	.	(18)
φt+1 = φt + nθt+1	φt+1 = φt + n(θt - nφt)
In the following proposition, we see that for n < 1, the respective convergence rates of the implicit
method and extrapolation are highly similar. Keeping in mind that the latter has the major advantage
of being more practical, this proposition clearly underlines the benefits of extrapolation. Note that
Prop. 1 and 2 generalize to general unconstrained bilinear game (more details and proof in §B.3),
Proposition 2. The squared norm of the iterates Nt2 d=ef θt2 + φt2, where the update rule of θt and φt
are defined in (18), decreases geometrically for any n < 1 as,
Implicit: N∖ι =(1 - η2 + n4 + O(η6))N： , Extrapolation: N2+ι = (1 - η2 + n4)N2 . (19)
3.3	Extrapolation from the past
One issue with extrapolation is that the algorithm “wastes” a gradient (14). Indeed we need to
compute the gradient at two different positions for every single update of the parameters. We thus
propose a technique that we call extrapolation from the past that only requires a single gradient
computation per update. The idea is to store and re-use the extrapolated gradient for the extrapolation:
Extrapolation from the past: ωt+1∕2 = Po[ωt - ηF(ωt-1∕2)]	(20)
Perform update step: ωt+ι = Pς1∖ωt - ηF(ωt+1∕2)] and store: F(ωt+1∕2)	(21)
The same update scheme was proposed by Chiang et al. (2012, Alg. 1) in the context of online convex
optimization and generalized by Rakhlin and Sridharan (2013) for general online learning. Without
projection, (20) and (21) reduce to the optimistic mirror descent described by Daskalakis et al. (2018):
Optimistic mirror descent (OMD): ωt+1∕2 = 3『\/2 - 2ηF3t-∖∕2 + ηF(ωt-3∕2)	(22)
We rediscovered this technique from a different perspective: it was motivated by VIP and inspired
from the extragradient method. Using the VIP point of view, we are able to prove a linear convergence
rate for extrapolation from the past (see details and proof of Theorem 1 in §B.4). We also provide
results for a stochastic version in §4. In comparison to the results from Daskalakis et al. (2018) that
6Sutskever (2013, §7.2) showed the equivalence between “standard momentum” and Nesterov’s formulation.
5
Published as a conference paper at ICLR 2019
―■— Adam with Y = 0.01	—H- Extrapolation from the past Y = 0.5
-►— Gradient method Y = 0.1	—♦— Extrapolation Y = 0.6
—♦— Averaging Y = 2.0
Figure 1: Comparison of the basic gradient method (as well as
Adam) with the techniques presented in §3 on the optimization
of (9). Only the algorithms advocated in this paper (Averag-
ing, Extrapolation and Extrapolation from the past) converge
quickly to the solution. Each marker represents 20 iterations.
We compare these algorithms on a non-convex objective in §G.1.
Algorithm 1 AvgSGD	Algorithm 2 AvgExtraSGD	Algorithm 3 AvgPastExtraSGD
Let ωo ∈ Ω for t = 0 . . . T - 1 do ξt 〜P	(mini-batch) dt J F3,ξt) ωt+ι — Po[ωt — ηtdt] end for Return ωT J P=T-ηtωt t=0 ηt	for t = 0 ...T — 1 do ξt,ξ0 〜P (mini-batches) dt J F(ωt,ξt) ωt J PΩ[ωt— ηtdt] d0t J F(ωt0,ξt0) ωt+ι J PΩ[ωt— ηtdt] end for PT-1	0 Return ωT J *0-ηtωt t=0 ηt	Let ωo ∈ Ω for t = 0 . . . T — 1 do ξt 〜P	(mini-batch) ωt J PΩ[ωt — ηtdt-ι] dt J F(ωt0, ξt) ωt+ι J PΩ[ωt — ηtdt] end for PT-1	0 Return ωT J ^fiT-Tω t=0 ηt
Figure 2: Three variants of SGD computing T updates, using the techniques introduced in §3.
hold only for a bilinear objective, we provide a faster convergence rate (linear vs sublinear) on the
last iterate for a general (strongly monotone) operator F and any projection on a convex Ω. One
thing to notice is that the operator of a bilinear objective is not strongly monotone, but in that case
one can use the standard extrapolation method (14) which converges linearly for a (constrained or
not) bilinear game (Tseng, 1995, Cor. 3.3).
Theorem 1 (Linear convergence of extrapolation from the past). If F is μ-Strongly monotone (see
§A for the definition of strong monotonicity) and L-Lipschitz, then the updates (20) and (21) with
η = 4L provide linearly converging iterates,
kωt-ω*k2 ≤ (1 - 4L) l∣ωo- ω*k2, Vt ≥0.
(23)
4 Optimization of VIP with stochastic gradients
In this section, we consider extensions of the techniques presented in §3 to the context of a stochastic
operator, i.e., we no longer have access to the exact gradient F(ω) but to an unbiased stochastic
estimate of it, F(ω, ξ), where ξ 〜P and F(ω) := Eξ〜P[F(ω, ξ)]. It is motivated by GAN training
where we only have access to a finite sample estimate of the expected gradient, computed on a
mini-batch. For GANs, ξ is a mini-batch of points coming from the true data distribution p and the
generator distribution qθ .
For our analysis, we require at least one of the two following assumptions on the stochastic operator:
Assumption 1. Bounded variance byσ2.∙Eξ[∣F(ω) - F(ω,ξ)∣2] ≤σ 2 , ∀ω ∈ Ω .
Assumption 2. Bounded expected squared norm by M2: Eξ[∣F(ω, ξ)∣2] ≤ M2, ∀ω ∈ Ω.
Assump. 1 is standard in stochastic variational analysis, while Assump. 2 is a stronger assumption
sometimes made in stochastic convex optimization. To illustrate how strong Assump. 2 is, note that it
does not hold for an unconstrained bilinear objective like in our example (10) in §3. It is thus mainly
reasonable for bounded constraint sets. Note that in practice we have σ M.
We now present and analyze three algorithms that are variants of SGD that are appropriate to
solve (VIP). The first one Alg. 1 (AvgSGD) is the stochastic extension of the gradient method
for solving (VIP); Alg. 2 (AvgExtraSGD) uses extrapolation and Alg. 3 (AvgPastExtraSGD) uses
extrapolation from the past. A fourth variant that re-use the mini-batch for the extrapolation step
(ReExtraSGD, Alg. 5) is described in §D. These four algorithms return an average of the iterates
(typical in stochastic setting). The proofs of the theorems presented in this section are in §F.
6
Published as a conference paper at ICLR 2019
To handle constraints such as parameter clipping (Arjovsky et al., 2017), we gave a projected version
of these algorithms, where PΩ[ω0] denotes the projection of ω0 onto Ω (See §A). Note that When
Ω = Rd, the projection is the identity mapping (unconstrained setting). In order to prove the
convergence of these four algorithms, we will assume that F is monotone:
(F(ω) — F(ω0))>(ω — ω0) ≥ 0 ∀ω, ω0 ∈ Ω .	(24)
If F can be written as (6), it implies that the cost functions are convex.7 Note however that general
GANs parametrized with neural networks lead to non-monotone VIPs.
Assumption 3. F is monotone and Ω is a compact convex set, SUCh that maxω,ω0∈ω ∣∣ω—ω0k2 ≤ R.
In that setting the quantity g(ω*) := maXω∈Ω F(ω)>(ω* — ω) is well defined and is equal to
0 if and only if ω* is a solution of (VIP). Moreover, if we are optimizing a zero-sum game, we
have ω = (θ, φ), Ω = Θ X Φ and F(θ, φ) = [VθL(θ, φ) — V^L(θ, ^)]>. Hence, the quantity
h(θ*,，) ：= maxp∈φ L(θ*, φ) — mine∈θ L(θ,，)is well defined and equal to 0 if and only if
(θ*, φ^) is a Nash equilibrium of the game. The two functions g and h are called merit functions
(more details on the concept of merit functions in §C). In the following, we call,
max 」(θ,，)— L(θ0") if F(θ, φ) = [VθLeS —小£(♦,0]>
Err(ω) def 20,p0)∈ω
I	max F (ω0)>(ω — ω0) otherwise.
(25)
Averaging. Alg. 1 (AvgSGD) presents the stochastic gradient method with averaging, which
reduces to the standard (simultaneous) SGD updates for the two-player games used in the GAN
literature, but returning an average of the iterates.
Theorem 2.	Under Assump. 1, 2 and 3, SGD with averaging (Alg. 1) with a constant step-size gives,
R2	M2 + σ2	df 1 T-1
E[Err(ωT)] ≤ — + η—where ωT d= - V ωt, VT ≥ 1.
2ηT	2	T t=0
(26)
Thm. 2 uses a similar proof as (Nemirovski et al., 2009). The constant term η(M2 + σ2)∕2 in (26) is
called the variance term. This type of bound is standard in stochastic optimization. We also provide
in §F a similar O(1/√t) rate with an extra log factor when η = √.We show that this variance term
is smaller than the one of SGD with prediction method (Yadav et al., 2018) in §E.
Extrapolations. Alg. 2 (AvgExtraSGD) adds an extrapolation step compared to Alg. 1 in order
to reduce the oscillations due to the game between the two players. A theoretical consequence is
that it has a smaller variance term than (26). As discussed previously, Assump. 2 made in Thm. 2
for the convergence of Alg. 1 is very strong in the unbounded setting. One advantage of SGD with
extrapolation is that Thm. 3 does not require this assumption.
Theorem 3.	(Juditsky et al., 2011, Thm. 1) Under Assump. 1 and 3, if Eξ [F] is L-Lipschitz, then
SGD with extrapolation and averaging (Alg. 2) using a constant step-size η ≤ √3l gives,
R2	7	1 T -1
E[Err(ωT)] ≤	-- +——ησ2	where ωT	=f — £ ω0,	VT	≥ 1.
T	ηT 2	T T t=0 t
(27)
Since in practice σ M, the variance term in (27) is significantly smaller than the one in (26). To
summarize, SGD with extrapolation provides better convergence guarantees but requires two gradient
computations and samples per iteration. This motivates our new method, Alg. 3 (AvgPastExtraSGD)
which uses extrapolation from the past and achieves the best of both worlds (in theory).
Theorem 4.	Under Assump. 1 and 3, if Eξ [F] is L-Lipschitz then SGD with extrapolation from the
past using a constant step-size η ≤ 2√L, gives that the averaged iterates converge as,
R2	13	def 1 T-1
E[Err(ωT)] ≤	+	ησ2 where ωT = — ω0 VT ≥ 1.
ηT 2	T t=0
(28)
The bound is similar to the one provided in Thm. 3 but each iteration of Alg. 3 is computationally
half the cost of an iteration of Alg. 2.
7The convexity of the cost functions in (3) is a necessary condition (not sufficient) for the operator to be
monotone. In the context of a zero-sum game, the convexity of the cost functions is a sufficient condition.
7
Published as a conference paper at ICLR 2019
5 Combining the techniques with established algorithms
In the previous sections, we presented several techniques that converge for stochastic monotone
operators. These techniques can be combined in practice with existing algorithms. We propose
to combine them to two standard algorithms used for training deep neural networks: the Adam
optimizer (Kingma and Ba, 2015) and the SGD optimizer (Robbins and Monro, 1951). For the
Adam optimizer, there are several possible choices on how to update the moments. This choice can
lead to different algorithms in practice: for example, even in the unconstrained case, our proposed
Adam with extrapolation from the past (Alg. 4) is different from Optimistic Adam (Daskalakis et al.,
2018) (the moments are updated differently). Note that in the case of a two-player game (3), the
previous convergence results can be generalized to gradient updates with a different step-size for
each player by simply rescaling the objectives LG and LD by a different scaling factor. A detailed
pseudo-code for Adam with extrapolation step (Extra-Adam) is given in Algorithm 4. Note that our
interest regarding this algorithm is practical and that we do not provide any convergence proof.
Algorithm 4 Extra-Adam: proposed Adam with extrapolation step.
input: step-size η, decay rates for moment estimates β1,β2, access to the stochastic gradients V't(∙)
and to the projection PΩ [∙] onto the constraint set Ω, initial parameter ω0, averaging scheme (pt)t≥ι
for t = 0 . . . T - 1 do
Option 1: Standard extrapolation.
Sample new mini-batch and compute stochastic gradient: gt — V't(ωJ
Option 2: Extrapolation from the past
Load previously saved stochastic gradient: gt = V't-1∕2(ωt-1∕2)
Update estimate of first moment for extrapolation: mt-1∕2 — β1mt-1 + (1 - βι)gt
Update estimate of second moment for extrapolation: Ivt-1/2 — β2vt-1 + (1 - βz)g2
Correct the bias for the moments: mt-1∕2 — mt-1∕2∕(l — β2t-1), Vt-1/2 J vt-1∕2∕(l — β2t-1)
Perform extrapolation step from iterate at time t: ωt-ι∕2 - PΩ[ωt — η rm^~'/-]
v	VZvt_1∕2+e
Sample new mini-batch and compute stochastic gradient: gt+1/2 — N't+1∕2(ωt+1∕2)
Update estimate of first moment: mt J β1mt-1∕2 + (1 — β1)gt+1∕2
Update estimate of second moment: vt J β2vt-1∕2 + (1 — β2)gt2+1∕2
Compute bias corrected for first and second moment: mt J mt∕(1 — β2t), ^t — vt∕(1 — β2t)
Perform update step from the iterate at time t: ωt+ι J PΩ[ωt — η√m[6]
end for
Output： ωτ-1∕2, ωτ or ωT = PT-I pt+1ωt+1∕2/PT-1 pt+ι (See (8) for online averaging)
6 Related Work
The extragradient method is a standard algorithm to optimize variational inequalities. This algorithm
has been originally introduced by Korpelevich (1976) and extended by Nesterov (2007) and Ne-
mirovski (2004). Stochastic versions of the extragradient have been recently analyzed (Juditsky et al.,
2011; Yousefian et al., 2014; Iusem et al., 2017) for stochastic variational inequalities with bounded
constraints. A linearly convergent variance reduced version of the stochastic gradient method has
been proposed by Palaniappan and Bach (2016) for strongly monotone variational inequalities. Ex-
trapolation can also be related to optimistic methods (Chiang et al., 2012; Rakhlin and Sridharan,
2013) proposed in the online learning literature (see more details in §3.3). Interesting non-convex
results were proved, for a new notion of regret minimization, by Hazan et al. (2017) and in the context
of online learning for GANs by Grnarova et al. (2018).
Several methods to stabilize GANs consist in transforming a zero-sum formulation into a more general
game that can no longer be cast as a saddle point problem. This is the case of the non-saturating
formulation of GANs (Goodfellow et al., 2014; Fedus et al., 2018), the DCGANs (Radford et al.,
2016), the gradient penalty8 for WGANs (Gulrajani et al., 2017). Yadav et al. (2018) propose an
optimization method for GANs based on AltSGD using an additional momentum-based step on the
generator. Daskalakis et al. (2018) proposed a method inspired from game theory. Li et al. (2017)
8The gradient penalty is only added to the discriminator cost function. Since this gradient penalty depends
also on the generator, WGAN-GP cannot be cast as a SP problem and is actually a non-zero sum game.
8
Published as a conference paper at ICLR 2019
suggest to dualize the GAN objective to reformulate it as a maximization problem and Mescheder
et al. (2017) propose to add the norm of the gradient in the objective to get a better signal. Gidel
et al. (2019) analyzed a generalization of the bilinear example (9) with a focus put on the effect
of momentum on this problem. They do not consider extrapolation (see §B.3 for more details).
Unrolling steps (Metz et al., 2017) can be confused with extrapolation but is fundamentally different:
the perspective is to try to approximate the “true generator objective function" unrolling for K steps
the updates of the discriminator and then updating the generator.
Regarding the averaging technique, some recent work appear to have already successfully used
geometric averaging (7) for GANs in practice, but only briefly mention it (Karras et al., 2018;
Mescheder et al., 2018). By contrast, the present work formally motivates and justifies the use
of averaging for GANs by relating them to the VIP perspective, and sheds light on its underlying
intuitions in §3.1. Subsequent to our first preprint, Yazici et al. (2019) explored averaging empirically
in more depth, while Mertikopoulos et al. (2019) also investigated extrapolation, providing asymptotic
convergence results (i.e. without any rate of convergence) in the context of coherent saddle point.
The coherence assumption is slightly weaker than monotonicity.
7	Experiments
Our goal in this experimental section is not to provide new state-of-the art results with architectural
improvements or a new GAN formulation, but to show that using the techniques (with theoretical
guarantees in the monotone case) that we introduced earlier allows us to optimize standard GANs
in a better way. These techniques, which are orthogonal to the design of new formulations of GAN
optimization objectives, and to architectural choices, can potentially be used for the training of any
type of GAN. We will compare the following optimization algorithms: baselines are SGD and Adam
using either simultaneous updates on the generator and on the discriminator (denoted SimAdam
and SimSGD) or k updates on the discriminator alternating with 1 update on the generator (denoted
AltSGD{k} and AltAdam{k}).9 Variants that use extrapolation are denoted ExtraSGD (Alg. 2)
and ExtraAdam (Alg. 4). Variants using extrapolation from the past are PastExtraSGD (Alg. 3)
and PastExtraAdam (Alg. 4). We also present results using as output the averaged iterates, adding
Avg as a prefix of the algorithm name when we use (uniform) averaging.
7.1	Bilinear saddle point (stochastic)
J M, = -a
ɪ MTθ* = -b
We first test the various stochastic algorithms on a simple
(n = 103 , d = 103) finite sum bilinear objective (a
monotone operator) constrained to [-1, 1]d:
n
-X (θ>M(i)φ + θ>a(i) + 9>b⑺)	(29)
n i=1
solvedby (θ*, q*) s.t.
where a ：= 1 P>ι a⑴，b := 1 Pn=I b(i) and
M = 1 Pn=ι M⑴.The matrices Mkj), af), bf) ；
- ≤ i ≤ n, - ≤ j, k ≤ d were randomly generated,
but ensuring that (θ*, φ^) belongs to [-1, l]d. Results
are shown in Fig. 3. We can see that AvgAltSGD1 and
AvgPastExtraSGD perform the best on this task.
Number of gradient Computation/n
Figure 3: Performance of the considered
stochastic optimization algorithms on the bilin-
ear problem (29). Each method uses its respec-
tive optimal step-size found by grid-search.
7.2	WGAN AND WGAN-GP ON CIFAR10
We evaluate the proposed techniques in the context of GAN training, which is a challenging stochastic
optimization problem where the objectives of both players are non-convex. We propose to evaluate
the Adam variants of the different optimization algorithms (see Alg. 4 for Adam with extrapolation)
by training two different architectures on the CIFAR10 dataset (Krizhevsky and Hinton, 2009). First,
we consider a constrained zero-sum game by training the DCGAN architecture (Radford et al.,
9In the original WGAN paper (Arjovsky et al., 2017), the authors use k = 5.
9
Published as a conference paper at ICLR 2019
Model	WGAN (DCGAN)	WGAN-GP (ResNet)
Method	no avg	uniform avg	EMA	no avg	uniform avg	EMA
SimAdam	6.05 ±.12	5.85 ± .16	6.08 ± .10	7.51 ±.17	7.68 ±.43	7.60 ± .17
AltAdam5	5.45±.08	5.72 ± .06	5.49 ±.05	7.57 ±.02	8.01 ± .05	7.66 ± .03
ExtraAdam	6.38 ± .09	6.38 ±.20	6.37 ± .08	7.90 ± .11	8.47 ± .10	8.13 ± .07
PastExtraAdam	5.98 ±.15	6.07 ±.19	6.01 ± .11	7.84 ±.06	8.01 ± .09	7.99 ± .03
OptimAdam	5.74±.10	5.80 ±.08	5.78 ±.05	7.98±.08	8.18 ±.09	8.10 ± .06
Table 1: Best inception scores (averaged over 5 runs) achieved on CIFAR10 for every considered Adam variant.
OptimAdam is the related Optimistic Adam (Daskalakis et al., 2018) algorithm. EMA denotes exponential
moving average (with β = 0.9999, see Eq. 8). We see that the techniques of extrapolation and averaging
consistently enable improvements over the baselines (in italic).
B-Sr-3
⅛H一⅛二*，岂苫◎繇
11 bv Sr - _rial C
消■也ΞM≡EI电 WK.
B 印与 uq≡EIFEE≡
S3，：』、■£ 7
轿电能普:色k'E□藩
Σ⅛同盘履.B□wlMm
 
 
“同之,敬向支S
AvgSimAdam
----AvgAltAdamI
----AvgAltAdam5
----AvgExtraAdam
----AvgPaSlEXtraAdam
1	2	3	4	5
Number of generator updates	×105
T噂 ISFMr.XE 蠢一
Figure 4: Left: Mean and standard deviation of the inception score computed over 5 runs for each method
on WGAN trained on CIFAR10. To keep the graph readable We show only SimAdam but AltAdam performs
similarly. Middle: Samples from a ResNet generator trained with the WGAN-GP objective using AvgExtraAdam.
Right: WGAN-GP trained on CIFAR10: mean and standard deviation of the inception score computed over 5
runs for each method using the best performing learning rates; all experiments were run on a NVIDIA Quadro
GP100 GPU. We see that ExtraAdam converges faster than the Adam baselines.
2016) with the WGAN objective and weight clipping as proposed by Arjovsky et al. (2017). Then,
we compare the different methods on a state-of-the-art architecture by training a ResNet with the
WGAN-GP objective similar to Gulrajani et al. (2017). Models are evaluated using the inception
score (IS) (Salimans et al., 2016) computed on 50,000 samples. We also provide the FID (Heusel
et al., 2017) and the details on the ResNet architecture in §G.3.
For each algorithm, we did an extensive search over the hyperparameters of Adam. We fixed β1 = 0.5
and β2 = 0.9 for all methods as they seemed to perform well. We note that as proposed by Heusel
et al. (2017), it is quite important to set different learning rates for the generator and discriminator.
Experiments were run with 5 random seeds for 500,000 updates of the generator.
Tab. 1 reports the best IS achieved on these problems by each considered method. We see that the
techniques of extrapolation and averaging consistently enable improvements over the baselines (see
§G.5 for more experiments on averaging). Fig. 4 shows training curves for each method (for their
best performing learning rate), as well as samples from a ResNet generator trained with ExtraAdam
on a WGAN-GP objective. For both tasks, using an extrapolation step and averaging with Adam
(ExtraAdam) outperformed all other methods. Combining ExtraAdam with averaging yields results
that improve significantly over the previous state-of-the-art IS (8.2) and FID (21.7) on CIFAR10
as reported by Miyato et al. (2018) (see Tab. 5 for FID). We also observed that methods based on
extrapolation are less sensitive to learning rate tuning and can be used with higher learning rates with
less degradation; see §G.4 for more details.
8 Conclusion
We newly addressed GAN objectives in the framework of variational inequality. We tapped into
the optimization literature to provide more principled techniques to optimize such games. We
leveraged these techniques to develop practical optimization algorithms suitable for a wide range
of GAN training objectives (including non-zero sum games and projections onto constraints). We
experimentally verified that this could yield better trained models, improving the previous state of the
art. The presented techniques address a fundamental problem in GAN training in a principled way,
and are orthogonal to the design of new GAN architectures and objectives. They are thus likely to be
widely applicable, and benefit future development of GANs.
10
Published as a conference paper at ICLR 2019
Acknowledgments.
This research was partially supported by the Canada CIFAR AI Chair Program, the Canada Excellence
Research Chair in “Data Science for Realtime Decision-making”, by the NSERC Discovery Grant
RGPIN-2017-06936 and by a Google Focused Research award. Gauthier Gidel would like to
acknowledge Benoit Joly and Florestan Martin-Baillon for bringing a fresh point of VieW on the proof
of Proposition 1.
References
M. ArjoVsky, S. Chintala, and L. Bottou. Wasserstein generatiVe adVersarial networks. In ICML,
2017.
K. E. Atkinson. An introduction to numerical analysis. John Wiley & Sons, 2003.
S. Boyd and L. Vandenberghe. Convex optimization. Cambridge uniVersity press, 2004.
R. E. Bruck. On the weak conVergence of an ergodic iteration for the solution of Variational
inequalities for monotone operators in Hilbert space. Journal of Mathematical Analysis and
Applications, 1977.
G. H. Chen and R. T. Rockafellar. Convergence rates in forward-backward splitting. SIAM Journal
on Optimization, 1997.
C.-K. Chiang, T. Yang, C.-J. Lee, M. Mahdavi, C.-J. Lu, R. Jin, and S. Zhu. Online optimization with
gradual variations. In COLT, 2012.
G. P. Crespi, A. Guerraggio, and M. Rocca. Minty variational inequality and optimization: scalar and
vector case. In Generalized Convexity, Generalized Monotonicity and Applications, 2005.
C. Daskalakis, A. Ilyas, V. Syrgkanis, and H. Zeng. Training GANs with optimism. In ICLR, 2018.
W. Fedus, M. Rosca, B. Lakshminarayanan, A. M. Dai, S. Mohamed, and I. Goodfellow. Many paths
to equilibrium: GANs do not need to decrease a divergence at every step. In ICLR, 2018.
G. Gidel, T. Jebara, and S. Lacoste-Julien. Frank-Wolfe algorithms for saddle point problems. In
AISTATS, 2017.
G. Gidel, R. Askari Hemmat, P Mohammad, H. Gabriel, L. R6mi, L.-J. Simon, and M. Ioannis.
Negative momentum for improved game dynamics. In AISTATS, 2019.
I. Goodfellow. NIPS 2016 tutorial: Generative adversarial networks. arXiv:1701.00160, 2016.
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and
Y. Bengio. Generative adversarial nets. In NIPS, 2014.
P. Grnarova, K. Y. Levy, A. Lucchi, T. Hofmann, and A. Krause. An online learning approach to
generative adversarial networks. In ICLR, 2018.
I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville. Improved training of
wasserstein GANs. In NIPS, 2017.
P. T. Harker and J.-S. Pang. Finite-dimensional variational inequality and nonlinear complementarity
problems: a survey of theory, algorithms and applications. Mathematical programming, 1990.
E. Hazan, K. Singh, and C. Zhang. Efficient regret minimization in non-convex games. In ICML,
2017.
M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. GANs trained by a two
time-scale update rule converge to a local Nash equilibrium. In NIPS, 2017.
A. Iusem, A. Jofr6, R. I. Oliveira, and P. Thompson. Extragradient method with variance reduction
for stochastic variational inequalities. SIAM Journal on Optimization, 2017.
11
Published as a conference paper at ICLR 2019
A. Juditsky, A. Nemirovski, and C. Tauvel. Solving variational inequalities with stochastic mirror-prox
algorithm. Stochastic Systems, 2011.
T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive growing of GANs for improved quality,
stability, and variation. In ICLR, 2018.
D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
G. Korpelevich. The extragradient method for finding saddle points and other problems. Matecon,
12, 1976.
A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Master’s thesis,
University of Toronto, Canada, 2009.
T. Larsson and M. Patriksson. A class of gap functions for variational inequalities. Math. Program.,
1994.
C. Ledig, L. Theis, F. Huszdr, J. Caballero, A. Cunningham, A. Acosta, A. P Aitken, A. Tejani,
J. Totz, Z. Wang, et al. Photo-realistic single image super-resolution using a generative adversarial
network. In CVPR, 2017.
Y. Li, A. Schwing, K.-C. Wang, and R. Zemel. Dualing GANs. In NIPS, 2017.
P. Mertikopoulos, H. Zenati, B. Lecouat, C.-S. Foo, V. Chandrasekhar, and G. Piliouras. Mirror
descent in saddle-point problems: going the extra (gradient) mile. In ICLR, 2019. To appear.
L. Mescheder, S. Nowozin, and A. Geiger. The numerics of GANs. In NIPS, 2017.
L. Mescheder, A. Geiger, and S. Nowozin. Which training methods for GANs do actually converge?
In ICML, 2018.
L. Metz, B. Poole, D. Pfau, and J. Sohl-Dickstein. Unrolled generative adversarial networks. In ICLR,
2017.
T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. Spectral normalization for generative adversarial
networks. In ICLR, 2018.
V. Nagarajan and J. Z. Kolter. Gradient descent GAN optimization is locally stable. In NIPS, 2017.
A. Nedic and A. Ozdaglar. Subgradient methods for saddle-point problems. J Optim Theory APPl,
2009.
A.	Nemirovski. Prox-method with rate of convergence O(1/t) for variational inequalities with
lipschitz continuous monotone operators and smooth convex-concave saddle point problems. SIAM
J. OPtim., 2004.
A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to
stochastic programming. SIAM Journal on oPtimization, 2009.
Y. Nesterov. Introductory Lectures On Convex OPtimization. Springer, 1983.
Y. Nesterov. Dual extrapolation and its applications to solving variational inequalities and related
problems. Math. Program., 2007.
S. Nowozin, B. Cseke, and R. Tomioka. f-GAN: Training generative neural samplers using variational
divergence minimization. In NIPS, 2016.
B.	Palaniappan and F. Bach. Stochastic variance reduction methods for saddle-point problems. In
NIPS, 2016.
B. T. Polyak. Gradient methods for minimizing functionals. Zhurnal Vychislitel’noi Matematiki i
Matematicheskoi Fiziki, 1963.
A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional
generative adversarial networks. In ICLR, 2016.
12
Published as a conference paper at ICLR 2019
A. Rakhlin and K. Sridharan. Online learning with predictable sequences. In COLT, 2013.
H.	Robbins and S. Monro. A stochastic approximation method. The Annals of Mathematical Statistics,
1951.
T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved techniques
for training GANs. In NIPS, 2016.
I.	Sutskever. Training recurrent neural networks. PhD thesis, 2013.
P. Tseng. On linear convergence of iterative methods for the variational inequality problem. Journal
of Computational and Applied Mathematics, 1995.
J.	Von Neumann and O. Morgenstern. Theory of games and economic behavior. Princeton University
Press, 1944.
A. Yadav, S. Shah, Z. Xu, D. Jacobs, and T. Goldstein. Stabilizing adversarial nets with prediction
methods. In ICLR, 2018.
Y. Yazici, C.-S. Foo, S. Winkler, K.-H. Yap, G. Piliouras, and V. Chandrasekhar. The unusual
effectiveness of averaging in GAN training. In ICLR, 2019. To appear.
F. Yousefian, A. Nedic, and U. V. Shanbhag. Optimal robust smoothing extragradient algorithms for
stochastic variational inequality problems. In CDC. IEEE, 2014.
J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks. In ICCV, 2017.
13
Published as a conference paper at ICLR 2019
A	Definitions
In this section, we recall usual definitions and lemmas from convex analysis. We start with the
definitions and lemmas regarding the projection mapping.
A.1 Projection mapping
Definition 1. The projection P。onto Ω is defined as,
Pω3) ∈ arg min ∣∣ω - ω0∣∣2 .	(30)
ω0∈Ω
When Ω is a convex set, this projection is unique. This is a consequence of the following lemma that
we will use in the following sections: the non-expansiveness of the projection onto a convex set.
Lemma 1. Let Ω a convex set, the projection mapping Pω : Rd → Ω is nonexpansive, i.e.,
kPo(ω) - PΩ(ω0)∣2 ≤ ∣∣ω - ω0∣2 , ∀ω, ω0 ∈ Ω .	(31)
This is standard convex analysis result which can be found for instance in (Boyd and Vandenberghe,
2004). The following lemma is also standard in convex analysis and its proof uses similar arguments
as the proof of Lemma 1.
Lemma 2. Let ω ∈ Ω and ω+ =f Po(ω + U), thenfor all ω0 ∈ Ω we have,
∣ω+ - ω0 ∣22 ≤ ∣ω - ω0 ∣22 + 2u> (ω+ - ω0) - ∣ω+ - ω∣22 .	(32)
Proof of Lemma 2. We start by simply developing,
∣ω+ - ω0∣22 = ∣(ω+ - ω) + (ω - ω0)∣22 = ∣ω - ω0∣22 + 2(ω+ - ω)>(ω - ω0) + ∣ω+ - ω∣22
= ∣ω - ω0 ∣22 + 2(ω+ - ω)>(ω+ - ω0) - ∣ω+ - ω∣22 .
Then since ω+ is the projection onto the convex set Ω of ω + u, We have that
(ω+ - (ω + u))>(ω+ -ω0) ≤ 0, ∀ ω0 ∈ Ω, leading to the result of the Lemma.	□
A.2 Smoothness and Monotonicity of the operator
Another important property used is the Lipschitzness of an operator.
Definition 2. A mapping F : Rp → Rd is said to be L-Lipschitz if,
∣∣F(ω) - F(ω0)∣∣2 ≤ L∣∣ω - ω0∣∣2 ,	∀ω, ω0 ∈ Ω .	(33)
In this paper, we also use the notion of strong monotonicity, which is a generalization for operators of
the notion of strong convexity. Let us first recall the definition of the latter,
Definition 3. A differentiable function f : Ω → R is said to be μ-strongly convex if
f (ω) ≥ f (J) + ▽/(J)>(3 - ωO) + 2 kω - ω0k2 ∀ω, ω ∈ ω .	(34)
Definition 4. A function (θ, φ) → L(θ, φ) is said convex-concave if L(∙, φ) is convex for all
φ ∈ Φ and L(θ, ∙) is concave for all θ ∈ Θ. An L is said to be μ-StrOngIy convex-concave if
(θ,φ)→L(θ,φ)- μ∣θ∣2
+ μ kφ∣2 is convex-concave.
If a function f (resp. L) is strongly convex (resp. strongly convex-concave), its gradient Vf (resp.
[VθL -VpL]>) is strongly monotone, i.e.,
Definition 5. For μ > 0, an operator F : Ω → Rd is said to be μ-strongly monotone if
(F(M- F(J))>(ω - J) ≥ μkω - ω0k2 .	(35)
14
Published as a conference paper at ICLR 2019
B Gradient methods on unconstrained bilinear games
In this section, we will prove the results provided in §3, namely Proposition 1, Proposition 2 and
Theorem 1. For Proposition 1 and 2, let us recall the context. We wanted to derive properties of some
gradient methods on the following simple illustrative example
min max θ ∙ φ
θ∈R φ∈R
(36)
B.1 Proof of Proposition 1
Let us first recall the proposition:
Proposition’ 1. The simultaneous iterates diverge geometrically and the alternating iterates defined
in (11) are bounded but do not converge to 0 as
Simultaneous: Θt2+1	+	φt2+1	= (1 + η2)(Θt2 + φt2) ,	Alternating:	Θt2	+ φt2	= Θ(Θ02	+ φ02)	(37)
where ut = Θ(vt) ⇔ ∃α, β, t0 > 0 such that ∀t ≥ t0, αvt ≤ ut ≤ βvt.
The uniform average (&, φt) d=f 1 PS=0(Θs,Φs) Ofthe simultaneous updates (resp. the alternating
updates) diverges (resp. converges to 0) as,
Simultaneous: Θ2+φ2 = Θ (诙 +2φ (1 + η2)t), Alternating: Θ2 + φ2 = Θ (诙 *φ0 ) . (38)
Proof. Let us start with the simultaneous update rule:
θt+1 = θt - ηφt
φt+1 = φt + ηθt .
Then we have,
θt2+1 + φt2+1 = (θt - ηφt)2 + (φt + ηθt)2
= (1 + η2)(θt2 + φt2) .
(39)
(40)
(41)
The update rule (39) also gives us,
ηφt = θt - θt+1
ηθt = φt+1 - φt .
Summing (42) for 0 ≤ t ≤ T - 1 to get telescoping sums, we get
(η2T2)(Φτ + θT) = (。0 - θT)2 + (φ0 - φT)2
= ((1 + η2 )T + 1)(θ02 + φ20 ) - 2θ0θT - 2φ0φT
=Θ(1+η2)T((θ02+φ20) .
(42)
(43)
(44)
(45)
Let us continue with the alternating update rule
θt+1 = θt - ηφt
φt+1 = φt + ηθt+1 = φt + η(θt - ηφt)
(46)
Then we have,
θt+1
φt+1
1	-η
η 1 - η2
def
By simple linear algebra, for η < 2, the matrix M =
eigenvalues which are
λ± = 1 - η
θt
φt .
1
η1
-η
2
- η2
(47)
has two complex conjugate
η ± i 4 - η2
(48)
2
15
Published as a conference paper at ICLR 2019
and their squared magnitude is equal to det(M) = 1 - η2 + η2 = 1. We can diagonalize M meaning
that there exists P an invertible matrix such that M = P-1 diag(λ+, λ-)P. Then, we have
θt
φt
Mt
θ0
φ0
P-1 diag(λt+, λt-)P
θ0
φ0
(49)
and consequently,
2
θt2 + φt2
θt
φt
P -1 diag(λt+, λt-)P
φθ00	≤ ∣P-1∣∣P∣(θ02+φ20)	(50)
C
where ∣∣ ∙ ∣∣c is the norm in C2 and ∣∣P∣∣ := maxu∈c2 喘. is the induced matrix norm. The same
way we have,
2
θ02 +φ20
M-t
θt
φt
P-1 diag(λ+-t, λ--t)P
θt
φt
≤ kP-1kkPk(θt2+φt2)	(51)
C
C
Hence, if θ02 + φ20 > 0, the sequence (θt , φt) is bounded but do not converge to 0. Moreover the
update rule gives us,
ηφt = θt - θt+1
ηθt = φt - φt-1
T-1
T X φt
t=0
T-1
T X θt
t=0
θo — θτ
-T-
φτ
⇒
φτ-1 - Φo + ηθo	I θτ
θo — θτ
ηT
φT-1 - φ0 + ηθ0
(52)
ηT
Consequently, since θt2 + φt2 = Θ(θ02 + φ02),
qθ2+φ2=θ (ρθ0η+^2!
(53)
B.2 Implicit and extrapolation method
In this section, we will prove a slightly more precise proposition than Proposition 2,
Proposition’ 2. The squared norm of the iterates Nt2 d=ef θt2 +
is defined in (18), decrease geometrically for any 0 < η < 1 a
where the update rule of θt and φt
Implicit: Nt2+1
N2
-----a , Extrapolation: N2+ι = (1 - η2 + η4)N2 ,
1 + η2	t+	t
∀t ≥ 0	(54)
Proof. Let us recall the update rule for the implicit method
( θt+1 = θt - ηφt+1 ⇒ ( (1 + η2)θt+1 = θt - ηφt
φt+1 = φt + ηθt+1	(1 + η2)φt+1 = φt + ηθt
(55)
Then,
(1 +η2)2(θt2+1 +φt2+1) = (θt - ηφt)2 + (φt + ηθt)2
= θt2 + φt2 + +η2(θt2 + φt2),
(56)
(57)
implying that
θt2 + φt2
1 + η2，
(58)
10Note that the relationship (54) holds actually for any η for the implicit method, and thus the decrease is
geometric for any non-zero step size.
⇒
T
□
16
Published as a conference paper at ICLR 2019
which is valid for any η .
For the extrapolation method, we have the update rule
( θt+1 = θt - η(φt + ηθt)	(59)
φt+1 = φt + η(θt - ηφt)
Implying that,
θt2+1 + φt2+1	=	(θt - η(φt + ηθt))2 + (φt + η(θt	- ηφt))2	(60)
=θt2+φt2 - 2η2(θ2 + φ2) + η2((θt	- ηφt)2 + (φt	+ ηθt)2)	(61)
= (1 -η2 +η4)(θt2 +φt2)	(62)
□
B.3 Generalization to general unconstrained bilinear objective
In this section, we will show how to simply extend the study of the algorithm of interest provided in
§3 on the general unconstrained bilinear example,
min max θ> A∞ 一 b>θ 一 c>∞	(63)
θ∈Rd p∈Rp
where, A ∈ Rd×p , b ∈ Rd and c ∈ Rp . The only assumption we will make is that this problem is
feasible which is equivalent to say that there exists a solution (θ*,，)to the system
A A，= b
μ> θ*=C.	(64)
In this case, we can re-write (63) as
min max (θ — θ*)>A(∞ — ∞*) + C	(65)
θ∈Rd p∈RP
where C := —θ*>A^* is a constant that does not depend on θ and φ.
First, let us show that we can reduce the study of simultaneous, alternating, extrapolation and implicit
updates rules for (63) to the study of the respective unidimensional updates (11) and (18).
This reduction has already been proposed by Gidel et al. (2019). For completeness, we reproduce
here similar arguments. The following lemma is a bit more general than the result provided by Gidel
et al. (2019). It states that the study of a wide class of unconstrained first order method on (63) can
be reduced to the study of the method on (36), with potentially rescaled step-sizes.
Before explicitly stating the lemma, we need to introduce a bit of notation to encompass easily our
several methods in a unified way. First, we let ωt := (θt, ψt), where the index t here is a more
general index which can vary more often than the one in §3. For example, for the extrapolation
method, we could consider ω1 = ω00 +1/2 and ω2 = ω10 , where ω0 was the sequence defined for the
extragradient. For the alternated updates, we can consider ωι = (θɪ, &) and ω2 = (θ1, φ1) (this
also defines θ2 = θ1), where θ0 and φ0 were the sequences originally defined for alternated updates.
We are thus ready to state the lemma.
Lemma 3. Let us consider the following very general class of first order methods on (63), i.e.,
θt ∈ θ0 + span(Fθ(ω0), . . . , Fθ(ωt)) , ∀t ∈ N,
φt ∈ φo + Span(F中(ωo),..., Fφ(ωt)), Vt ∈ N,
(66)
where ωt := (θt, φt) and Fe(ωt) := Aqt — b, Fφ(ωt) = A>θt — c. Then, we have
θt = U>(θt — θ*) and Ψt = V>(9t —，),	(67)
ι	A	τ τ ɪʌτ r-Γ ∕oτ zzʌ ι	∖ ι.ι	ι / Γzʌ 1 Γ ~T ∖	r ιι .ι	ι	ι
where A = UDV 1 (SVD decomposition) and the couples ([θt]i, [<q]i)ι≤i≤r follow the update rule
of the same method on a unidimensional problem (36). In particular, for our methods of interest, the
couples ([θt]i, [q]i)ι≤i≤r follow the same update rule with a respective step-size σiη, where σi are
the singular values on the diagonal of D.
17
Published as a conference paper at ICLR 2019
Proof. Our general class of first order methods can be written with the following update rules:
t+1
Θt+1 = θ0 + EλstA(8s - 8*)
s=0
t+1
Ψt+1 = ψ0 + μrμ>Aγ-s - θ*),
s=0
where λit, μit ∈ R , 0 ≤ i ≤ t + 1. We allow the dependence on t for the algorithm coefficients
λ and μ (for example, the alternating rule would zero out some of the coefficients depending on
whether We are updating θ or φ at the current iteration). Notice also that if both λ(t+i)t and μ(t+i)t
are non-zero, we have an implicit scheme.
Thus, using the SVD of A = UDV> , we get
t+1
U>(θt+ι - θ*) = U>(θ0 — θ*) + XλstDVτ(φs -，)
s=0
Vτ(ψt+ι - 8*) = Vτ(80 - 8*) + XμstDτU>(θs - θ*),
s=0
which is equivalent to
t+1
Θt+1 = θo + X λstD0s
s=0
t+1
0t+1 = 00 + ]TμstDτθs ,
s=0
(68)
where D is a rectangular matrix with zeros except on a diagonal block of size r. Thus, each coordinate
of θt+ι and 4t+ι are updated independently, reducing the initial problem to r unidimensional
problems,
,〜	t+1
[θt+1 ]i = [θθ]i + ɪ2 λstσi[0s]i
s=0
1 ≤i≤r,
t+1	--,
[0t+l]i = [00]i +)： μstσi [θs]i
、	s=0
where σ1 ≥ . . . ≥ σr > 0 are the positive diagonal coefficients of D.
(69)
Finally, for the coordinate i where the diagonal coefficient of D is equal to 0, we can notice that
the sequence ([θt]i, [i0t∖i) is constant. Moreover, we have the freedom to chose any [θ*]i ∈ R and
[,]i ∈ R as a coordinate of the solution of (63). We thus set them respectively equal to [θo]i and
[00]i . The update rule (69) corresponds to the update rule of the general first order method considered
on this proof on the unidimensional problem (36).
Note that the only additional restriction is that the coefficients (λst) and (σst) (that are the same for
1 ≤ i ≤ r) are rescaled by the singular values of A. In practice, for our methods of interest with
a step-size η, it corresponds to the study of r unidimensional problem with a respective step-size
σiη , 1 ≤ i ≤ r.	□
From this lemma, an extension of Proposition 1 and 2 directly follows to the general unconstrained
bilinear objective (63). We note
N ：= dist(θt, Θ*)2 +dist(0t, Φ*)2,	(70)
where (Θ*, Φ*) is the set of solutions of (63). The following corollary is divided in two points, the
first point is a result from Gidel et al. (2019) (note that the result on the average is a straightforward
extension of the one provided in Proposition 1 and was not provided by Gidel et al. (2019)), the
second result is new. Very similar asymptotic upper bounds regarding extrapolation and implicit
methods can be derived by Tseng (1995) computing the exact values of the constant τ1 and τ2 (and
18
Published as a conference paper at ICLR 2019
noticing that τ3 = ∞) introduced in (Tseng, 1995, Eq. 3 & 4) for the unconstrained bilinear case.
However, since Tseng (1995) works in a very general setting, the bound are not as tight as ours and
his proof technique is a bit more technical. Our reduction above provides here a simple proof for our
simple setting.
Corollary 1.	• Gidel et al. (2019): The simultaneous iterates diverge geometrically and the
alternating iterates are bounded but do not converge to 0 as,
Simultaneous: Nt2+1 = (1 + (σmin(A)η)2)Nt2 , Alternating: Nt2 = Θ(N02) ,	(71)
where ut = Θ(vt) ⇔ ∃α, β, t0 > 0 such that ∀t ≥ t0 , αvt ≤ ut ≤ βvt. The uniform
average (θt,φt) = 1 P：=0(θs,φs) of the simultaneous updates (resp. the alternating
updates) diverges (resp. converges to 0) as,
Simultaneous: NN2 ≤ Θ ( —2∙(1 + (σmin(A)η)2)t , Alternating: NN2 = Θ (	.
• Extrapolation and Implicit method: The iterates respectively generated by the update
rules (14) and (17) on a bilinear unconstrained problem (63) do converge linearly for any
0 < η <-------1 at a rate,11
σmax (A)
N2
ImPlicit： Nt+1 ≤ 1 + (σmin(A)η)2 , ∀t ≥ 0
Extrapolation: Nt2+1 ≤ (1 - (σmin (A)η)2 + (σmin(A)η)4)Nt2 ,	∀t ≥ 0 .
Particularly, for η = ?α 1 (A) we getfor the extrapolation method,
ExtraPolation: Nt2+1 ≤ (1 - 8K)tN2 ,	Vt ≥ 0 .
where K := ；max(A)2 is the condition number of A>A.
(72)
(73)
(74)
B.4 Extrapolation from the past for strongly convex objectives
Let us recall what we call projected extrapolation form the past, where we used the notation
ω0 = ωt+1∕2 for compactness,
Extrapolation from the past: ω0 = PΩ[ωt - nF(ω0-ι)]	(75)
Perform update step: ωt+ι = PΩ∖ωt — nF(ω0)] and store: F(ω0)	(76)
where PΩ[∙] is the projection onto the constraint set Ω. An operator F : Ω → Rd is said to be
μ-strongly monotone if
(F(M- F(J))>(a - J) ≥ μkω -ω1l2.	(77)
If F is strongly monotone, we can prove the following theorem:
Theorem' 1. If F is μ-strongly monotone (see §A for the definition of strong monotonicity) and
L-Lipschitz, then the updates (20) and (21) with n = ± provide linearly converging iterates,
kωt-ω*k2 ≤ (ι - 4L) kω0 -ω*k2, Vt ≥ 0.	(78)
Proof. In order to prove this theorem, we will prove a slightly more general result,
kωt+ι-ω*k2 + l∣ω0-ι-ω0k2 ≤ (1 - 4L) (kωt- ω*k2 + l∣ω0-ι- ω0-2k2).	(79)
with the convention that ω00 = ω-0 1 = ω-0 2 . It implies that
kωt+ι- ω*k2 ≤ kωt+ι- ω*k2 + l∣ω0-ι- ω0k2 ≤ (1 - 4L) kωo- ω*k2.	(80)
Let us first proof three technical lemmas.
11As before, the inequality (73) for the implicit scheme is actually valid for any step-size.
19
Published as a conference paper at ICLR 2019
Lemma 4. If F is μ-strongly monotone, we have
μ (l∣ωt — ω*Ili - 2kωt- ωtki) ≤2F(ω0)>(ω0 — ω*), Vω* ∈ ω* .	(8I)
Proof. By strong monotonicity and optimality of ω*,
2μkωt - ω*ki ≤2F(ω*)τ(ω0 - ω*) ÷ 2μkω0 - ω"∣∣i ≤2F(ω0)τ(ω0 - ω*)	(82)
and then We use the inequality 2∣ω0 - ω*∣∣i ≥ ∣ω - ω*∣i - 2∣ω0 - ωtIli to get the result
claimed.
Lemma 5. If F is L-Lipschitz, we havefor any ω ∈ Ω,
2nF(“r(a0 - ω ≤ ∣∣ωt -ωki -∣∣ωt+ι - ω∣ιi -IM - ωtl∣i ÷ηiLillω0-ι - ω0∣∣i. (83)
Proof. Applying Lemma 2 for	(ω, u, ω+, ωt)	=	(ω,, -ηtF(ω0), ωt+1, ω)	and
(ω, u, ω+, ωt) = (ωt, -ηtF(ω0-1), ω0, ωt+ι), we get,
I“t+1 - “ki ≤ l∣ωt - ω∣i - 2ηtF(ω't)τ(ωt+ι - ω) -。+1 - atIli	(84)
and
llω0 - ωt+ι∣∣i ≤ ∣∣ωt - ωt+ι∣∣i - 2ηtf(ω0-ι)T(ω0 - ωt+ι) -∣∣ω0 - ωt∣∣i.	(85)
Summing (84) and (85) we get,
l∣ωt+ι - ω∣ιi ≤ ι∣ωt - ω∣ιi - 2ηtF(ω0)τ(ωt+ι - “)	(86)
-	2ηtF (ω0-ι)τ(ω0 - ωt+ι) - lid - atiii - IM - ωt+ι∣∣i	(87)
=ι∣ωt - ω∣ιi - 2ηtF(ω0 )τ(ω0 - 3)-ι∣ωt - ωt∣ιi -ι∣ω0 - ωt+"∣i
-	2ηt(F(ω0-ι) - F(ω0))τ(ω0 - ωt+ι).	(88)
Then, we can use the Young,s inequality 2aτb ≤ l∣α!i ÷ l∣bli to get,
l∣ωt+ι - ω∣∣i ≤ ι∣ωt - ω∣ιi - 2ηtF(ω0)τ(ω0 - ω)÷ 褚||F (“-J - F (ω0)∣∣i
÷ ∣∣ω0 - ωt+ι∣ιi -IM - ωt∣∣i -ι∣ω0 - ωt+ι∣ιi	(89)
=∣∣ωt - ω∣∣i - 2ηtF(ω0)τ(ω0 - ω)÷ ηi∣F(ω0-ι) - F(ω0)∣∣i -∣∣ω0 - ωtι∣i
≤ l∣ωt - ω∣∣i - 2ηtF(ω0)τ(ω0 - ω)÷ ηiLikω0-1 - aRi -∣∣ω0 - atlli .(90)
□
Lemma 6. FOr all t ≥ 0, ifwe set ω-i = ω-1 = ω0 we have
llωt-ι - ωt111 ≤ 4∣∣ωt - ω0∣∣i ÷ 4ηi-ιLikω0-ι - ω0-ill -∣∣ω0-1 - ω0∣∣i.	(91)
Proof. We start with ∣∣α ÷ b∣∣ ≤ 2∣a∣i ÷2∣b∣i.
l∣ωt-ι - ωtIli ≤ 2∣∣ωt - ω0lli ÷ 2∣∣ωt - ω0-ιl∣i.	(92)
Moreover, since the projection is contractive we have that
llωt - ωt-i1i ≤ Ilωt-1 - ηt-iF(ω0-i) - ωt-i - ηt-iF(ω0-i)∣∣i	(93)
=ηi-ι∣F(ω0-ι)- F(ω0-i)∣∣i	(94)
≤ ηi-ιLikωt-ι - ω0-ili .	(95)
Combining (92) and (95) we get,
Ilωt-1 - ωtl∣i = 2IM-I- ω0111 -kω0-ι - ωtl∣i	(96)
≤ 4∣∣ωt - ω0ι∣i ÷ 4∣∣ωt - aLilli -∣∣ω0-ι - ω0∣∣i	(97)
≤ 4∣∣ωt - ω0∣∣i ÷4ηi-iLikω0-i - ω0-ik∣ -I∣ω0-1 - ω0∣∣i .	(98)
□
20
Published as a conference paper at ICLR 2019
Proof of Theorem 1. Let ω* ∈ Ω* bean optimal point of (VIP). Combining Lemma 4 and Lemma 5
we get,
ηtμ (kωt- ω*k2- 2kω0- ωtk2) ≤ l∣ωt-ω*k2-kωt+ι-ω*k2+η2L2kω0-ι-ω0Il2-kω0-ωtk2
leading to,
l∣ωt+ι - ω*k2 ≤ (I- ntμ) l∣ωt - ω*k2 + η2L2kω0-ι - ωtk2 -(I- 2ηtμ)kω0 - ωt∣l2 (99)
Now using Lemma 6 we get,
l∣ωt+ι - ω*k2 ≤ (I- ntμ) ∣∣ωt - ω*k2 + 样刀2(4后-"2||"-1 - ω0-2∣∣2 -∣∣ω0-1 - ωt∣∣2)
-(I- 2ηtμ - 4η2L2)kω0 - ωt ι∣2	(100)
Now with ηt = 4L ≤ 4μ We get,
l∣ωt+ι - ω*k2 ≤ (1 - 4l) kω - ω*k2 + 16 (4l∣ω0-ι - ω0-2k2 - kω0-ι - ωtk2)
Hence, using the fact that 告 ≤ 4 we get,
l∣ωt+ι- ω*l2 + ⅛l∣ω0-ι- ω01∣2 ≤ (1 - 4L) (lωt- ω*k2 +16l∣ω0-ι- ω0-2∣∣2) . (101)
□
C More on merit functions
In this section, we will present how to handle an unbounded constraint set Ω with a more refined
merit function than (25) used in the main paper. Let F be the continuous operator and Ω be the
constraint set associated with the VIP,
find ω* ∈ Ω such that F(ω*)>(ω — ω*) ≥ 0, ∀ω ∈ Ω .	(VIP)
When the operator F is monotone, we have that F(ω*)›(ω - ω*) ≤ F(ω)>(ω - ω*), ∀ω, ω*.
Hence, in this case (VIP) implies a stronger formulation sometimes called Minty variational inequal-
ity (Crespi et al., 2005):
find ω* ∈ Ω such that F(ω)>(ω — ω*) ≥ 0 , ∀ω ∈ Ω .	(MVI)
This formulation is stronger in the sense that if (MVI) holds for some ω* ∈ Ω, then (VIP) holds
too. A merit function useful for our analysis can be derived from this formulation. Roughly, a merit
function is a convergence measure. More formally, a function g : Ω → R is called a merit function if
g is non-negative such that g(ω) = 0 ⇔ ω ∈ Ω* (Larsson and Patriksson, 1994). A way to derive a
merit function from (MVI) would be to use g(ω*) = suPω∈Ω F(ω)>(ω* - ω) which is zero if and
only if (MVI) holds for ω*. To deal with unbounded constraint sets (leading to a potentially infinite
valued function outside of the optimal set), we use the restricted merit function (Nesterov, 2007):
ErrR(ωt) d=ef	max	F (ω)> (ωt - ω) .	(102)
ω∈Ω, ∣∣ω-ωo ∣∣≤R
This function acts as merit function for (VIP) on the interior of the open ball of radius R around ω0,
as shown in Lemma 1 of Nesterov (2007). That is, let Ωr = Ω ∩{ω : ∣∣ω - ω0k < R}. Then for
any point ω ∈ Ωr, we have:
Errκ(ω) =0 ⇔ & ∈ Ω* ∩ Ωr .	(103)
The reference point ω0 is arbitrary, but in practice it is usually the initialization point of the algorithm.
R has to be big enough to ensure that Ωr contains a solution. ErrR measures how much (MVI) is
violated on the restriction Ωr. Such merit function is standard in the variational inequality literature.
A similar one is used in (Nemirovski, 2004; Juditsky et al., 2011). When F is derived from the
gradients (5) of a zero-sum game, we can define a more interpretable merit function. One has to be
careful though when extending properties from the minimization setting to the saddle point setting
(e.g. the merit function used by Yadav et al. (2018) is vacuous for a bilinear game as explained in
App C.2).
In the appendix, we adopt a set of assumptions a little more general than the one in the main paper:
21
Published as a conference paper at ICLR 2019
Assumption 4.	• F is monotone and Ω is convex and closed.
• R is set big enough such that R > ∣∣ωo 一 ω* k and F is a monotone operator.
Contrary to Assumption 3, in Assumption 4 the constraint set in no longer assumed to be bounded.
Assumption 4 is implied by Assumption 3 by setting R to the diameter of Ω, and is thus more general.
C.1 More general merit functions
In this appendix, we will note Err(RVI) the restricted merit function defined in (102). Let us recall its
definition,
Err(RVI)(ωt)
d=ef	max	F(ω)>(ωt 一 ω) .
ω∈Ω, ∣∣ω—ω0 ∣∣≤R
(104)
When the objective is a saddle point problem i.e.,
F(θ, φ) = [VθL(θ, φ) -%L(θ”)]>	(105)
and L is convex-concave (see Definition 4 in §A), We can use another merit function than (104) on Ωr
that is more interpretable and more directly related to the cost function of the minimax formulation:
ErrRiP) e“t)
def
= max
p∈Φ,θ∈Θ
L(θt,0-L(θ,Vt).
(106)
k(θ,p)-(θ0,p0)k≤R
In particular, if the equilibrium (θ*, φ^) ∈ Ω* ∩ Ωr and we have that L(∙, φ^) and -L(θ*, ∙) are
μ-strongly convex (see §A), then the merit function for saddle points upper bounds the distance for
(θ, φ) ∈ Ωr to the equilibrium as:
ErrRSP)(θ,一 ≥ 2(∣θ - θ*∣∣2 + ∣3 一 -*k2).	(107)
In the appendix, we provide our convergence results with the merit functions (104) and (106),
depending on the setup:
def ∫ ErrRiP) (ω) if F isa SP OPerator (5)
ErrR(ω) =	R(VI)	(108)
IErrR 7(ω) otherwise.
C.2 On the importance of the merit function
In this section, we illustrate the fact that one has to be careful when extending results and properties
from the minimization setting to the minimax setting (and consequently to the variational inequality
setting). Another candidate as a merit function for saddle point optimization would be to naturally
extend the suboptimality f (ω) — f (ω*) used in standard minimization (i.e. find ω* the minimizer
of f) to the gap P(θ, φ) = L(θ, q*) — L(θ*, 4). In a previous analysis of a modification of the
stochastic gradient descent (SGD) method for GANs, Yadav et al. (2018) gave their convergence rate
on P that they called the “primal-dual“ gap. Unfortunately, if we do not assume that the function L is
strongly convex-concave (a stronger assumption defined in §A and which fails for bilinear objective
e.g.), P may not be a merit function. It can be 0 for anon optimal point, see for instance the discussion
on the differences between (106) and P in (Gidel et al., 2017, Section 3). In particular, for the simple
2D bilinear example L(θ, φ) = θ ∙ φ, we have that θ* = φ^ = 0 and thus P(θ, 一)= 0 ∀θ, φ.
C.3 Variational inequalities for non-convex cost functions
When the cost functions defined in (3) are non-convex, the operator F is no longer monotone.
Nevertheless, (VIP) and (MVI) can still be defined, though a solution to (MVI) is less likely to exist.
We note that (VIP) is a local condition for F (as only evaluating F at the points ω*). On the other
hand, an appealing property of (MVI) is that it is a global condition. In the context of minimization
of a function f for example (where F = Vf), if ω* solves (MVI) then ω* is a global minimum
of f (and not just a stationary point for the solution of (MVI); see Proposition 2.2 from Crespi et al.
(2005)).
22
Published as a conference paper at ICLR 2019
A less restrictive way to consider variational inequalities in the non-monotone setting is to use a local
version of (MVI). if the cost functions are locally convex around the optimal couple (θ*, ,) and
if our iterates eventually fall and stay into that neighborhood, then we can consider our restricted
merit function ErrR(∙) with a well suited constant R and apply our convergence results for monotone
operators.
D Another way of implementing extrapolation to SGD
We now introduce another way to combine extrapolation and SGD. This extension is very similar to
AvgExtraSGD Alg. 2, the only difference is that it re-uses the mini-batch sample of the extrapolation
step for the update of the current point. The intuition is that it correlates the estimator of the gradient
of the extrapolation step and the one of the update step leading to a better correction of the oscillations
which are also due to the stochasticity. One emerging issue (for the analysis) of this method is that
since ωt0 depend on ξt, the quantity F(ωt0, ξt) is a biased estimator of F (ωt0).
Algorithm 5 Re-used mini-batches for stochastic extrapolation (ReExtraSGD)
1:	Let ωo ∈ Ω
2:	for t = 0 . . . T - 1 do
3:	Sample ξt 〜P
def
4:	ω0 = PΩ[ωt — ηtF(ωt,ξt)]	. Extrapolation step
5:	ωt+ι =f PΩ[ωt — ηtF(ω0, ξj]	. Update step with the same sample
6:	end for
7:	RetUm ωτ = PT=,1 ηt3t/ PT=11 η
Theorem 5. Assume that kωt0 — ω0k ≤ R, ∀t ≥ 0 where (ωt0)t≥0 are the iterates of Alg. 5. Under
Assumption 1 and 4, for any T ≥ 1, Alg. 5 with constant step-size η ≤ √^ has the following
convergence properties:
R2	σ2 + 4L2(4R2 + σ2)	def 1 -	0
E[Errκ(ωT)] ≤ ηT + η------------2--------- where ωT = T	ω0.
Particularly, ηT = √T gives E[Errκ(ωT)] ≤ O√T).
The assumption that the sequence of the iterates provided by the algorithm is bounded is strong, but
has also been made for instance in (Yadav et al., 2018). The proof of this result is provided in §F.
E	Variance comparison between AvgSGD and SGD with prediction
METHOD
To compare the variance term of AvgSGD in (26) with the one of the SGD with prediction method (Ya-
dav et al., 2018), we need to have the same convergence certificate. Fortunately, their proof can
be adapted to our convergence criterion (using Lemma 7 in §F), revealing an extra σ2/2 in the
variance term from their paper. The resulting variance can be summarized with our notation as
(M2 (1 + L) + σ2 )/2 where the L is the Lipschitz constant of the operator F. Since M σ, their
variance term is then 1 + L time larger than the one provided by the AvgSGD method.
F	Proof of Theorems
This section is dedicated on the proof of the theorems provided in this paper in a slightly more general
form working with the merit function defined in (108). First we prove an additional lemma necessary
to the proof of our theorems.
Lemma 7. Let F be a monotone operator and let (ωt), (ωt0), (zt), (∆t), (ξt) and (ζt) be six random
sequences such that, for all t ≥ 0
2ηt F (ωt0)> (ωt0 — ω) ≤ Nt — Nt+1 + ηt2(M1 (ωt, ξt) + M2 (ωt0, ζt)) + 2ηt∆t>(zt — ω) ,
23
Published as a conference paper at ICLR 2019
where Nt = N(ωt, ωt0-1, ωt0-2) ≥ 0 and we extend (ωt0) with ω-0 2 = ω-0 1 = ω00 . Let also assume
that with N ≤ R, E[∣∣∆tk2] ≤ σ2, E[∆t∣zt, ∆o,..., △1] = 0, E[Mι(ωt,ξt)] ≤ Mi and
E[M2 (ωt0 , ζt)] ≤ M2 , then,
E[ErrR(ωT)] ≤ R2 + MI + M2 +	X n	(109)
ST	2ST	t=0
def T -1	0	def T -1
where ωT = 工t=0 ηtω0 / St and ST =工t=0 ηt.
Proof of Lemma 7. We sum (7) for 0 ≤ t ≤ T - 1 to get,
T-1
2 X ηtF (ωt0)> (ωt0 - ω) ≤
t=0
T-1
X [(Nt - Nt+i) + ηt ((M1(ωt,ξt) + M2(ωt ,Zt))+2nA>(zt - ω)] .	(IIO)
t=0
We will then upper bound each sum in the right-hand side,
△t> (zt - ω) = △t> (zt - ut) + △t> (ut - ω)
where ut+i def Pω(u - ηt∆t) and uo def ω0. Then,
kut+ι- ωk2 ≤ kut- ωk2- 2ηt^> (Ut- ω)+硝&||2
leading to
2ηt^> (Zt- ω) ≤ 2ηt^> (Zt- Ut) + kut- ωk2- kut+ι- ωk2 + 褚||乙||2	(III)
Then noticing that z0 d=ef ω0, back to (110) we get a telescoping sum,
T-i	T-i
2 X ηtF(ω0)>(ω0-ω) ≤ 2N0+X [η2((Mi(ωt,ξt)+M2(ω0,0)+11^112)+224;(Zt-ut)].
t=0	t=0
(112)
If F is the operator of a convex-concave saddle point (5), We get, with ω0 = (θt, ψt)
F(ω0)>(ω0 - ω) ≥ ReL(θt, ψt)τ(θt - θ) - ▽「L(θt, Ψt)τ(ψt -⑺
≥ L(θt") - L(θt"t) + L(θt, ψt) - L(θ"t)
(by convexity and concavity)
=L(θt")-L(θ"t)
then by convexity of L(∙, φ) and concavity of L(θ, ∙), we have that,
T-i
2ST X Sr-F(ω0)> (ω0 - ω) ≥ 2ST
t=0 ST
T-i
X S(L(θt,6- L(θ, ψt)) ≥ 2St(L(θt, φ) - L(θ, φt))
t=0 ST
(113)
Otherwise if the operator F is just monotone since F (ωt0)> (ωt0 - ω) ≥ F (ω0)>(ωt0 - ω) we have
that
T-i	T-i
2STXηtF(ωt0)>(ωt0 -ω) ≥2STXηtF(ω0)>(ωt0 -ω) =2STF(ω0)>(ω2t -ω)	(114)
t=0	t=0
In both cases, we can now maximize the left hand side respect to ω (since the RHS does not depend
on ω) to get,
T-i
2St ErrR(ωt) ≤ 2R2 + X [η2((Mι(ωt,ξt) + M2(ω'0,Zt)) + ∣∣∆tk2) + 2ηt∆>(zt-Ut)] . (115)
t=0
Then taking the expectation, since E[∆∕zt, Ut]	=	E[∆t∣zt, ∆o,..., ∆t-ι]
Eζt [k∆tk2] ≤ σ2, Eξt [Mι(ωt,ξt)] ≤ Mi and E^ “(a0, Zt)] ≤ M2 , we get that,
R2
E[Errκ(ωT)] ≤ — +
ST
Mi + M2 + σ
2sT
2 T-i
Xηt2
t=0
0,
(116)
□
24
Published as a conference paper at ICLR 2019
F.1 Proof of Thm. 2
First let us state Theorem 2 in its general form,
Theorem’ 2. Under Assumption 1, 2 and 4, Alg. 1 with constant step-size η has the following
convergence rate for all T ≥ 1,
R M + σ	def 1
E[ErrR(ωT)] ≤ —— + η——-—— where ωT = — ω .
2ηT	2	T
t=0
(117)
Particularly, η = √T(M2+* gives E[ErrR(Oτ)] ≤ RVMT+σ2
Proof of Theorem 2. Let any ω ∈ Ω such that ∣∣ωo - ω∣∣2 ≤ R,
Ilωt+1 - ωk2 = kPΩ(ωt - ηtF(ωt, St))- ωk2
≤ kωt - ηtF(ωt,ξt)) - ωk22
(projections are non-contractive, Lemma 1)
= Iωt - ωI22 - 2ηtF(ωt, ξt)>(ωt - ω) + IηtF(ωt, ξt)I22
Then we can make appear the quantity F(ωt)>(ωt - ω) on the left-hand side,
2ηtF (ωt)>(ωt-ω) ≤ Iωt-ωI22-Iωt+1-ωI22+ηt2IF (ωt, ξt)I22+2ηt(F(ωt)-F(ωt, ξt))>(ωt-ω)
(118)
we can sum (118) for 0 ≤ t ≤ T - 1 to get,
T-1
2 X ηtF (ωt)>(ωt - ω) ≤
t=0
T-1
X (Iωt - ωI2 - Iωt+1 - ωI2) + ηt2IF(ωt, ξt)I22 + 2ηt∆t>(ωt - ω)	(119)
t=0
def
where we noted ∆t = F(ωt) - F(ωt, ξt).
By monotonicity, F(ωt)>(ωt - ω) ≥ F(ω)>(ωt - ω) we get,
T-1
2SτF(ω)>(ζωτ-ω) ≤ X [(Ilωt - ωk2 - kωt+ι - ωk2) + η2kF(ωt,ξt)k2 + 2ηt∆>(ωt - ωR ,
t=0
(120)
def T -1	def 1	T -1
Where Sτ = Et=O ηt and ωτ = ST ∑t=o ηtωt.
We will then upper bound each sum in the right hand side,
∆t> (ωt - ω) = ∆t> (ωt - ut) + ∆t> (ut - ω)
def
where ut+ι = Pω(u — ηt∆t) anduo = o。. Then,
Iut+1 - ωI22 ≤ Iut - ωI22 - 2ηt∆t> (ut - ω) + ηt2 I∆t I22
leading to
2ηt∆t>(ωt -	ω)	≤	2ηt∆t>(ωt	- ut) + Iut	-	ωI22	- Iut+1	- ωI22	+ ηt2 I∆tI22	(121)
Then noticing that u0 d=ef ω0, back to (120) we get a telescoping sum,
τ-1	τ-1
2SτF(O)T(Oτ - ω) ≤ 2kω0- ωk2 + X η2(IlF(ωt,ξt)k2 + IAk2) + 2 X ^δ>(Ot- Ut)
t=0	t=0
τ-1	τ-1
≤ 2R + X ηt2(IF (Ot, ξt)I22 + I∆tI22) + 2 X ηt∆t>(Ot - ut)
t=0	t=0
25
Published as a conference paper at ICLR 2019
Then the right hand side does not depends on ω, we can maximize over ω to get,
T-1	T-1
2StErrR(ωT) ≤ 2R + X 褚(||「(ωt,ξt)k∣ + |出||2) + 2 X %∆3t - Ut)	(122)
t=0	t=0
Noticing that E[∆t∣ωt, ut] = 0 (the estimates of F are unbiased), by Assumption 2
E[(kF(ωt,ξt)k22] ≤ M2 and by Assumption 1 E[k∆tk22] ≤ σ2 we get,
2	2 T-1
E[ErrR(ωT)] ≤ SR + M2+	X 褚	(123)
ST	2ST	t=0
particularly for η = η and η = √=^ We respectively get,
t+1
2R	η
E[ErrR(ωT)] ≤ 讨 + 2(M2 + σ2)	(124)
and
4R	M2 + σ2
E[ErrR(ωT)] ≤ _===	+2ηln(T +1)√F=L	(125)
η T+1 -1	T+1 -1
□
F.2 Proof of Thm. 3
Theorem’ 3. Under Assumption 1 and 4, if Eξ [F] is L-Lipschitz, then Alg. 2 with a constant step-size
η ≤ √3l has the following Convergence ratefor any T ≥ 1,
E[Errκ(ωT)] ≤ RT + 7ησ2 where ωT =f ： X ω0.	(126)
Particularly, η = √√R gives E[ErrR (ωT)] ≤ √√Rσ.
ProofofThm. 3. Let any ω ∈ Ω such that ∣∣ωo - ω∣∣2 ≤ R. Then, the update rules become
ωt+ι = PΩ(ωt - ηtF(ω0,Zt)) and ω0 = Pω3 - ηF(ωt,ξt)), We start by applying Lemma 2 for
(ω, u, ω0, ω+) = (ωt, -ηF (ωt0, ζt), ω, ωt+1) and (ω,u,ω0,ω+) = (ωt, -ηtF(ωt, ξt), ωt+1, ωt0),
∣ωt+1 - ω∣22 ≤ ∣ωt - ω∣22 - 2ηtF (ωt0, ζt)>(ωt+1 - ω) - ∣ωt+1 - ωt∣22
∣ωt0 - ωt+1∣22 ≤ ∣ωt - ωt+1∣22 - 2ηtF(ωt, ξt)> (ωt0 - ωt+1) - ∣ωt0 - ωt ∣22
Then, summing them We get
∣ωt+1 - ω∣22 ≤ ∣ωt -ω∣22 - 2ηtF(ωt0,ζt)>(ωt+1- ω)
- 2ηtF(ωt, ξt)>(ωt0 - ωt+1) - ∣ωt - ωt0 ∣22 - ∣ωt+1 - ωt0∣22 (127)
leading to
∣ωt+1 - ω ∣22 ≤ ∣ωt - ω ∣22 - 2ηtF(ωt0, ζt)> (ωt0 - ω)
+ 2ηt(F(ωt0, ζt) - F(ωt, ξt))>(ωt0 - ωt+1) - ∣ωt - ωt0 ∣22 - ∣ωt+1 - ωt0∣22
Then With 2a>b ≤ ∣a∣22 + ∣b∣22 We get
∣ωt+1 - ω∣22 ≤ ∣ωt - ω∣22 - 2ηtF (ωt0 , ζt)> (ωt0 - ω)
- ∣ωt - ωt0∣22 + ηt2 ∣F (ωt0, ζt) - F(ωt, ξt)∣22
Using the inequality ∣a + b + c∣22 ≤ 3(∣a∣22 + ∣b∣22 + ∣c∣22) We get,
∣ωt+1 - ω∣22 ≤ ∣ωt - ω∣22 - 2ηtF (ωt0, ζt)> (ωt0 - ω) - ∣ωt - ωt0 ∣22
+3ηt2(∣F(ωt)-F(ωt,ξt)∣22+∣F(ωt0)-F(ωt0,ζt)∣22+∣F(ωt0)-F(ωt)∣22)
26
Published as a conference paper at ICLR 2019
Then we can use the L-Lipschitzness of F to get,
kωt+1 - ω k22 ≤ kωt - ωk22 - 2ηtF (ωt0 , ζt)> (ωt0 - ω) - kωt - ωt0 k22
+3ηt2(kF(ωt)-F(ωt,ξt)k22+kF(ωt0)-F(ωt0,ζt)k22+L2kωt-ωt0k22)
As We restricted the step-size to η ≤ √^ We get,
2ηt F (ωt0)> (ωt0 - ω) ≤ kωt - ωk22 - kωt+1 - ωk22 + 2ηt (F (ωt0) - F(ωt0, ζt))>(ωt0 - ω)
+ 3ηt2kF(ωt) - F(ωt, ξt)k22 + 3ηt2kF(ωt0) - F(ωt0, ζt)k22
We get a particular case of (7) so We can use Lemma 7 Where Nt = kωt - ωk22 ,
M1 (ωt, ξt) = 3kF (ωt)-F (ωt, ξt)k22 , M2 (ωt0, ζt) = 3kF (ωt0)-F (ωt0, ζt)k22, ∆t = F (ωt0)-F (ωt0, ζt)
and zt = ωt0.	By Assumption 1, M1	= M2 = 3σ2 and by the fact that
E[F(ω0) - F (ω0 ,Zt) ∣ω0, ∆o,..., △T] = E[E[F (ω0) - F(ω0 ,Zt) ∣ω0]∣∆°,..., △T] = 0
the hypothesis of Lemma 7 hold and We get,
R2	7σ2 T-1
E[ErrR(ωT)] ≤	+ E ηt	(128)
ST	2ST t=0
□
F.3 Proof of Thm. 4
Theorem’ 4. Under Assumption 1, if Eξ [F] is L-Lipschitz, then AvgPastExtraSGD (Alg. 3) with a
constant step-size η ≤ 2√3L has thefollowing convergence rate for any T ≥ 1,
R2	13	2	def 1 -	0
E[ErrR (ωT)] ≤ - + ɪησ where ωT = T ω0.
Particularly, η = σ√⅛ g^ves E[Errκ(ωT)] ≤ √√Rσ.
First let us recall the update rule
fe1 = Po[ωt - ηtF(ω[,ξt)]
1 = PΩ[ωt+ι - ηt+ιF(ω0, ξt)].
(129)
(130)
Lemma 8. We have for any ω ∈ Ω,
2ηF(ωt0, ξt)>(ωt0 - ω) ≤ kωt - ωk22 - kωt+1 - ωk22 - kωt0 - ωtk22 + 3ηt2L2 kωt0-1 - ωt0k22
+ 3ηt2kF (ωt0-1, ξt-1) - F(ωt0-1)k22 + kF(ωt0) - F(ωt0,ξt)k22 .
(131)
Proof. Applying Lemma 2 for (ω, u, ω+, ω0)	=	(ωt, -ηtF(ωt0, ξt), ωt+1, ω) and
(ω, u, ω+, ω0) = (ωt, -ηtF (ωt0-1, ξt-1), ωt0, ωt+1), We get,
kωt+1 - ωk22 ≤ kωt - ωk22 -2ηtF(ωt0, ξt)>(ωt+1 - ω) - kωt+1 - ωt k22	(132)
and
kωt0 -	ωt+1 k22	≤	kωt - ωt+1 k22	-2ηtF(ωt0-1, ξt-1)>(ωt0	- ωt+1) -	kωt0	-	ωt k22 .	(133)
Summing (132) and (133) We get,
kωt+1 - ωk22 ≤ kωt - ωk22 -2ηtF(ωt0, ξt)>(ωt+1 - ω)	(134)
-	2ηtF (ωt0-1, ξt-1)>(ωt0 - ωt+1) - kωt0 - ωtk22 - kωt0 - ωt+1k22	(135)
= kωt - ωk22 -2ηtF(ωt0, ξt)>(ωt0 - ω) - kωt0 - ωtk22 - kωt0 - ωt+1 k22	(136)
-	2ηt(F (ωt0-1, ξt-1) - F(ωt0, ξt))>(ωt0 - ωt+1) .	(137)
27
Published as a conference paper at ICLR 2019
Then, We can use the inequality of arithmetic and geometric means 2α>b ≤ ∣∣α∣∣∣ + ∣∣b∣∣∣ to get,
l∣ωt+ι - ωk∣ ≤ l∣ωt- ωk∣ - 2ηtF(ωt, ξt)>(ω0- ω) + η∣kF(ω0-ι,ξtτ)- F(ωt,ξt)ll∣
+ kωt- ωt+ιk∣- kωt- ωtk∣- kωt- ωt+ιk∣	(138)
=kωt- ω∣l∣ - 2ηtF(ω0, ξt)τ(ω0 - ω)	(139)
+ η∣kF(ωt-ι,ξt-ι)- F(ωt, ξt)k∣- kωt- ωtk∣.	(14O)
Using the inequality ∣∣α + b + ck∣ ≤ 3(kɑk∣ + ∣∣bk∣ + ∣∣ck∣) we get,
kF(ω0-1,ξt-1) - F(ω0&)∣∣∣ ≤ 3(∣IF(ω0-1 ,ξt-1) - F(ω0-ι)l∣ + kF3-ι) - F(")k∣
+ IlF(ω0)- F(ω0,ξt)k∣)	(141)
≤ 3(kF(ω0-ι,ξt-ι) - F(ω0-ι)k∣ + L∣kω0-1 - ω0k∣
+ IlF(ω0)- F(ω0,ξt)k∣),	(142)
where we used the L-Lipschitzness of F for the last inequality.
Combining (140) with (142) we get,
kωt+ι- ωk∣ ≤ kωt- ωk∣- 2ηtF(ω0, ξt)>(ω0 - ω) -kω0- ωtk∣ + 3η∣L∣kωt-ι- ωtk∣
+ 3褚[kF(ω0-i,ξt-i) - F(ω0-i)k∣ + kF(ω0) - F(ω0,ξt)k∣] .	(143)
□
Lemma 9. For all t ≥ 0, ifwe set ω-1 = ω-1 = ω0 we have
kω0-ι-ωtk∣ ≤ 4kωt- ω0k∣ + i2η∣-ι(kF(ω0-ι,ξt-I)- f(ω0-ι)k∣+ LIkω0-ι-ω0-∣k∣
+ kF (ω0-∣) - F (ωt-∣ ,ξt-I)IlI)) -kω0-i - ωt k∣ .	(144)
Proof. We start with ka + bk∣ ≤ 2kak∣ + 2kbk∣.
kωt-ι- ωtki ≤ 2kωt- ωtk∣+ 2kωt- ω0-ιk∣.	(145)
Moreover, since the projection is contractive we have that
kωt - ω0-1k∣ ≤ kωt-i - ηt-iF(ω0-1,ξt-i) - ωt-i - ηt-iF(ω0-∣,ξt-∣)k∣	(146)
=η∣-ikF(ω0-i,ξt-i)- F(ω0-∣,ξt-∣)k∣	(147)
≤ 3ηiLι(kF(ωt-i, ξt-1)- F(ω0-i)k∣ + LIkω0-i - ω0-∣k∣
+ kF(d-1)- F (ωt-∣, ξt-I)IlI)).	(148)
where in the last line we used the same inequality as in (142). Combining (144) and (148) we get,
kωt-ι- ωt k∣ = 2kωt-ι- ωt ki -kd-ι- ωt k∣	(149)
≤ 4kωt- ωt k∣+ 4kωt- d-iki -k"-ι- ωt k∣	(150)
≤ 4kωt- 3； k∣ + 12褚-1(||F(d-ι,ξt-ι)- F(d-i)kI+ LIk“-1- ωt-∣k∣
+ kF - I)- F (3t-I ,ξt-∣)k∣)) -k3t-i - 3t k∣ .	(151)
□
Proof of Theorem 4. Combining Lemma 9 and Lemma 8 we get,
2ηtF(3t, ξt)>(3t - 3) ≤ k3t - 3k2 -k3t+1 - 3k∣
+ 36瑞瑞-1LI (IIF(ωt-1,ξt-1) - F(3t-1)k∣ + LIkωt-1 - 3t-∣k∣
+ kF(ωt-∣)- F(ωt-∣,ξt-∣)k∣)
-3η∣ l I k3t-1- 3t k∣+ (12η∣LI-I) IM - 3tk∣
+ 3η∣[kF(ωt-1,ξt-1)- F(ωt-1)k∣ + kF(0) - F3t,ξt)k∣].
(152)
28
Published as a conference paper at ICLR 2019
Then for ηt ≤ 2√1^ We have 36η2 η2-1L4 ≤ 3η2-1L2,
2ηtF(ωt0)>(ωt0 - ω) ≤ kωt - ωk22 - kωt+1 - ωk22
+ 3L2(ηt2-1 kωt0-1 - ωt0-2 k22 - ηt2 kωt0-1 - ωt0 k22)
+ 2ηt (F (ωt0) - F (ωt0 , ξt))> (ωt0 - ω)
+ 3η2 [|F(ω0-2, ξt-2) - F(ωt-2)k2 + 2kF(ω0-1,ξt-1) - F(ω0-i)k2
+kF(ωt0)-F(ωt0,ξt)k22].	(153)
We can then use Lemma 7 Where
Nt = kωt - ωk22 + 3L3ηt-1 kωt0-1 - ωt0-2 k22,
M1 (ωt , ξt) = 0
M2(ωt0, ξt) = 3kF (ωt0) - F (ωt0, ξt)k22 + 6kF (ωt0-1) - F (ωt0-1, ξt-1)k22
+ 3kF (ωt0-2) - F(ωt0-2, ξt-2)k22
∆t = F(ωt0) - F(ωt0,ξt)
zt = ωt0 .
By Assumption 1, M2	=	12σ2 and by the fact that
E[F(ω0) - F (ω0 ,ξt) ∣ω0, ∆o,..., △T] = E[E[F (ω0) - F(ω0 & ∣ω0]∣∆°,..., △T] = 0
the hypothesis of Lemma 7 hold and We get,
R2	13σ2 T-1
E[ErrR(ωT)] ≤ ▽ + X ηt	(154)
ST	2ST t=0
□
F.4 Proof of Theorem 5
Theorem 5 has been introduced in §D. This theorem is about Algorithm 5 Which consists in another
Way to implement extrapolation to SGD. Let us first restate this theorem,
Theorem’ 5. Assume that kωt0 - ω0k ≤ R, ∀t ≥ 0 where (ωt0)t≥0 are the iterates of Alg. 5. Under
Assumption 1 and 4, for any T ≥ 1, Alg. 5 with Constant step-size η ≤ √^ has the following
convergence properties:
R2	σ2 + 4L2 (4R2 + σ2)	def 1 -	0
E[Errκ(ωT)] ≤ 次 + η-------------2--------- where ωT = T	ω0.
Particularly, ηt = √T gives E[Errκ(ωT)] ≤ O√T.
ProofofThm. 5. Let any ω ∈ Ω such that ∣∣ωo - ω∣∣2 ≤ R. Then, the update rules become
ωt+ι = Pω3 - ηtF(ω0,ξt)) and ω0 = Pω3 - ηF(ωt,ξt)). We start the same way as
the proof of Thm. 3 by applying Lemma 2 for (ω, u, ω0, ω*) = (ωt, -ηF(ω0, ξt), ω, ωt+ι) and
(ω, u, ω0, ω+) = (ωt, -ηtF(ωt,ξt),ωt+1,ωt0),
∣ωt+1 - ω∣22 ≤ ∣ωt - ω∣22 - 2ηtF(ωt0, ξt)> (ωt+1 - ω) - ∣ωt+1 - ωt ∣22
∣ωt0 - ωt+1 ∣22 ≤ ∣ωt - ωt+1 ∣22 - 2ηtF(ωt, ξt)> (ωt0 - ωt+1 ) - ∣ωt0 - ωt ∣22
Then, summing them we get
∣ωt+1 - ω∣22 ≤ ∣ωt - ω∣22 - 2ηtF(ωt0,ξt)>(ωt+1 - ω)
-	2ηtF(ωt, ξt)>(ωt0 - ωt+1) - ∣ωt0 - ωt∣22 - ∣ωt+1 - ωt0∣22 (155)
leading to
∣ωt+1 - ω∣22 ≤ ∣ωt - ω∣22 - 2ηtF(ωt0, ξt)>(ωt0 - ω)
+ 2ηt(F(ωt0, ξt) - F(ωt, ξt))> (ωt0 - ωt+) - ∣ωt0 - ωt ∣22 - ∣ωt+ - ωt0 ∣22
29
Published as a conference paper at ICLR 2019
Then with 2a>b ≤ kak22 + kbk22 we get
kωt+1 - ωk22 ≤ kωt - ωk22 - 2ηtF (ωt0 , ξt)> (ωt0 - ω)
+ηt2 kF (ωt0, ξt) - F(ωt, ξt)k22 - kωt0 - ωtk22
Using the Lipschitz assumption we get
kωt+1 - ωk22 ≤ kωt - ωk22 - 2ηtF(ωt0, ξt)>(ωt0 - ω) + (ηt2L2 - 1)kωt - ωt0 k22
Then we add 2ηt F (ωt0)> (ωt0 - ω) in both sides to get,
2ηtF(ωt0)>(ωt0 - ω) ≤ kωt - ωk22 - kωt+1 - ωk22
-	2ηt (F (ωt0, ξt) - F (ωt0))> (ωt0 - ω) + (ηt2L2 - 1)kωt - ωt0 k22 (156)
Here, unfortunately we cannot use Lemma 7 because F(ωt0, ξt) is biased. We will then deal with the
quantity A = (F (ωt0 , ξt) - F (ω0))> (ωt0 - ω ) . We have that,
A = (F (ωt0, ξt) - F(ωt, ξt))>(ω - ωt0) + (F(ωt) - F (ωt0))> (ω - ωt0)
+ (F (ωt, ξt) - F (ωt))> (ωt - ωt0) + (F (ωt, ξt) - F (ωt))> (ω - ωt)
≤ 2Lkωt0 - ωtk2 kωt0 - ωk2 + kF(ωt, ξt) - F(ωt)kkωt0 - ωtk2
+ (F (ωt , ξt) - F (ωt))> (ω - ωt)
(Using Cauchy-Schwarz and the L-Lip of F )
Then using 2|同||回| ≤ 6|同，+ δ∣∣bk2,for δ = 4,
-2ηt(F(ωt, ξt) - F(ωt))>(ω0- ω) ≤ 2l∣ω0 - ωtk2 + 8η2L2kω0 - ωk2
+4ηt2∣F(ωt,ξt)-F(ωt)∣22
+ 4kω0 - ωtk2 + 2ηt(F(ωt,ξt) - F(ωt))>(ω - ωt)
leading to,
2ηt F (ωtt)> (ωtt - ω) ≤ ∣ωt - ω∣22 - ∣ωt+1 - ω∣22 + 2ηt(F(ωt, ξt) - F(ωt))> (ω - ωt)
+ (η2L2 - 1 )kωt - ω01∣2
+4ηt2(2L2∣ωtt-ω∣22+∣F(ωt,ξt)-F(ωt)∣22)
If one assumes finally that ∣ω0 - ω0 ∣∣2 ≤ R (assumption of the theorem) and that η ≤ 芸 We get,
2ηt F (ωtt)> (ωtt - ω) ≤ ∣ωt - ω∣22 - ∣ωt+1 - ω∣22 + 2ηt(F(ωt, ξt) - F(ωt))> (ω - ωt)
+ 4ηt2 (4L2 R2 + ∣F(ωt, ξt) - F(ωt)∣22)
Where We used that ∣ωtt - ω∣2 ≤ ∣ωtt - ω0 ∣2 + ∣ω0 - ω∣2 ≤ 2R. Once
again this equation is a particular case of Lemma 7 Where Nt = ∣ωt - ω∣22,
M1(ωt, ξt) = 4(4L2 R2 + ∣F(ωt, ξt) - F(ωt)∣22), M2 (ωtt, ζt)	=	0, zt	= ωt	and
∆t = F(ωt, ξt) - F(ωt). By Assumption 1 E[M1(ωt, ξt)]	≤	16L2 R2	+	4σ2	and
E[∆t∣ωt, ∆o,..., ∆t-ι] = E[E[∆t∣ωt]∣∆o,..., ∆t-ι] = 0 so we can use Lemma 7 and get,
R2	σ2 + 16L2R2 + 4σ2 T-1 2
E[ErrR(aτ)] ≤ — +-----------------------E η2.	(157)
ST	2ST	t=0
□
30
Published as a conference paper at ICLR 2019
G Additional experimental results
G.1 Toy non-convex GAN (2D and deterministic)
We now consider a task similar to (Mescheder et al., 2018) where the discriminator is linear
Dp(ω) = φτω, the generator is a Dirac distribution at θ, q® = δθ and the distribution We try
to match is also a Dirac at ω*, P = δω*. The minimax formulation from Goodfellow et al. (2014)
gives:
min max 一 log(1 + e-?Tω*) 一 log(1 + e^θ)	(158)
θ P
Note that as observed by Nagarajan and Kolter (2017), this objective is concave-concave, making
it hard to optimize. We compare the methods on this objective where we take ω* = -2, thus the
position of the equilibrium is shifted towards the position (θ, φ) = (-2,0). The convergence and
the gradient vector field are shown in Figure 5. We observe that depending on the initialization, some
methods can fail to converge but extrapolation (18) seems to perform better than the other methods.
Gradient Vector Field and Trajectory
Training Curves
0 5 0 5 0 5 0
3 2 2 1 1
IUnIUlκ0 ə-s00ud2ss
0	200	400	600	800	1000
Number of Iterations
Figure 5: Comparison of five algorithms (described in Section 3) on the non-convex GAN objective
(158), using the optimal step-size for each method. Left: The gradient vector field and the dynamics
of the different methods. Right:The distance to the optimum as a function of the number of iterations.
G.2 DCGAN with WGAN-GP objective
In addition to the results presented in section §7.2, we also trained the DCGAN architecture with the
WGAN-GP objective. The results are shown in Table 3. The best results are achieved with uniform
averaging of AltAdam5. However, its iterations require to update the discriminator 5 times for every
generator update. With a small drop in best final score, ExtraAdam can train WGAN-GP significantly
faster (see Fig. 6 right) as the discriminator and generator are updated only twice.
G.3 FID scores for ResNet architecture with WGAN-GP objective
In addition to the inception scores, we also computed the FID scores (Heusel et al., 2017) using
50,000 samples for the ResNet architecture with the WGAN-GP objective; the results are presented
in Table 5. We see that the results and conclusions are similar to the one obtained from the inception
scores, adding an extrapolation step as well as using Exponential Moving Average (EMA) consistently
improves the FID scores. However, contrary to the results from the inception score, we observe that
uniform averaging does not necessarily improve the performance of the methods. This could be
due to the fact that the samples produced using uniform averaging are more blurry and FID is more
sensitive to blurriness; see §G.3 for more details about the effects of uniform averaging.
31
Published as a conference paper at ICLR 2019
Generator
Input: Z ∈ R128 〜N(0, I)
Linear 128 → 512 × 4 × 4
Batch Normalization
ReLU
transposed conv. (kernel: 4×4, 512 → 256, stride: 2, pad: 1)
Batch Normalization
ReLU
transposed conv. (kernel: 4×4, 256 → 128, stride: 2, pad: 1)
Batch Normalization
ReLU
transposed conv. (kernel: 4×4, 128 → 3, stride: 2, pad: 1)
Tanh(∙)
Discriminator
Input: X ∈ R3 × 32 × 32
conv. (kernel: 4×4, 1 → 64; stride: 2; pad:1)
LeakyReLU (negative slope: 0.2)
conv. (kernel: 4×4, 64 → 128; stride: 2; pad:1)
Batch Normalization
LeakyReLU (negative slope: 0.2)
conv. (kernel: 4×4, 128 → 256; stride: 2; pad:1)
Batch Normalization
LeakyReLU (negative slope: 0.2)
Linear 128 × 4 × 4 × 4 → 1
Table 2: DCGAN architecture used for our CIFAR-10 experiments. When using the gradient penalty
(WGAN-GP), we remove the Batch Normalization layers in the discriminator.
Model Method	WGAN-GP (DCGAN)	
	no averaging	uniform avg
SimAdam	6.00 ± .07	6.01 ± .08
AltAdam5	6.25 ±.05	6.51 ±.05
ExtraAdam	6.22 ±.04	6.35 ±.05
PastExtraAdam	6.27 ± 0.06	6.23 ± 0.13
Table 3: Best inception scores (averaged over 5 runs) achieved on CIFAR10 for every consid-
ered Adam variant. We see that the techniques of extrapolation and averaging consistently enable
improvements over the baselines (in italic).
32
Published as a conference paper at ICLR 2019
⅛18s Uo33d8ul
3.0
2.5
2.0
AltAdam5 Y = 1 ∙ 10-4
---- AvgExtraAdam Y = 5 ∙ 10-4
---- AvgPaStExtraAdam Y =1 • 10-4
3 O
2. 5 -
5 0 5 0 5 0 5
. . . . . . .
6 6 5 5 4 4 3
⅛18s Uo3d8ul
0.2	0.4	0.6	0.8	1.0
Wall-Clock time in seconds	×104
1	2	3	4	5
Number of generator updates	×105
Figure 6: DCGAN architecture with WGAN-GP trained on CIFAR10: mean and standard deviation
of the inception score computed over 5 runs for each method using the best performing learning rate
plotted over number of generator updates (Left) and wall-clock time (Right); all experiments were
run on a NVIDIA Quadro GP100 GPU. We see that ExtraAdam converges faster than the Adam
baselines.
Generator
Input: Z ∈ R128 〜N(0,I)
Linear 128 → 128 × 4 × 4
ResBlock 128 → 128
ResBlock 128 → 128
ResBlock 128 → 128
Batch Normalization
ReLU
transposed conv. (kernel: 3×3, 128 → 3, stride: 1, pad: 1)
Tanh(∙)
Discriminator
Input: X ∈ R3 × 32 × 32
ResBlock 3 → 128
ResBlock 128 → 128
ResBlock 128 → 128
ResBlock 128 → 128
Linear 128 → 1
Table 4: ResNet architecture used for our CIFAR-10 experiments. When using the gradient penalty
(WGAN-GP), we remove the Batch Normalization layers in the discriminator.
Model Method	WGAN-GP (ResNet)		
	no averaging	uniform avg	EMA
SimAdam	23.74 ± 2.79	26.29 ± 5.56	21.89 ± 2.51
AltAdam5	21 .65 ± 0.66	19.91 ± 0.43	20.69 ± 0.37
ExtraAdam	19.42 ± 0.15	18.13 ± 0.51	16.78 ± 0.21
PastExtraAdam	19.95 ± 0.38	22.45 ± 0.93	17.85 ± 0.40
OptimAdam	18.88 ± 0.55	21.23 ± 1.19	16.91 ± 0.32
Table 5: Best FID scores (averaged over 5 runs) achieved on CIFAR10 for every considered Adam
variant. OptimAdam is the related Optimistic Adam (Daskalakis et al., 2018) algorithm. We see that
the techniques of extrapolation and EMA consistently enable improvements over the baselines (in
italic).
33
Published as a conference paper at ICLR 2019
1
000
-4 3
20。S Uo3d8ul
0.25	0.50	0.75	100	1.25 L50	1.75	2 00
Number of generator updates	×105
(a) learning rate of 10-3
-4 3
20。S Uo3d8ul
1 H----1-------1------1------1------1------1-----1----
000	0.25	0.50	0.75	100	1.25	L50	1.75	2 00
Number of generator updates ×105
(b) learning rate of 10-4
Figure 7: Inception score on CIFAR10 for WGAN-GP (DCGAN) over number of generator updates
for different learning rates. We can see that AvgExtraAdam is less sensitive to the choice of learning
rate.
G.4 Comparison of the methods with the same learning rate
In this section, we compare how the methods presented in §7 perform with the same step-size. We
follow the same protocol as in the experimental section §7, we consider the DCGAN architecture
with WGAN-GP experiment described in App §G.2. In Figure 7 we plot the inception score provided
by each training method as a function of the number of generator updates. Note that these plots
advantage AltAdam5 a bit because each iteration of this algorithm is a bit more costly (since it
perform 5 discriminator updates for each generator update). Nevertheless, the goal of this experiment
is not to show that AltAdam5 is faster but to show that ExtraAdam is less sensitive to the choice of
learning rate and can be used with higher learning rates with less degradation.
In Figure 8, we compare the sample quality on the DCGAN architecture with the WGAN-GP objective
of AltAdam5 and AvgExtraAdam for different step-sizes. We notice that for AvgExtraAdam, the
sample quality does not significantly change whereas the sample quality of AltAdam5 seems to be
really sensitive to step-size tunning.
We think that robustness to step-size tuning is a key property for an optimization algorithm in order
to save as much time as possible to tune other hyperparameters of the learning procedure such as
regularization.
34
Published as a conference paper at ICLR 2019
(a) AvgExtraAdam with η = 10-3
(c) AltAdam with η = 10-3
Figure 8: Comparison of the samples quality on the WGAN-GP (DCGAN) experiment for different
methods and learning rate η.
(b) AvgExtraAdam with η = 10-4
(d) AltAdam with η = 10-4
35
Published as a conference paper at ICLR 2019
G.5 Comparison of the methods with and without uniform averaging
In this section, we compare how uniform averaging affect the performance of the methods presented
in §7. We follow the same protocol as in the experimental section §7, we consider the DCGAN
architecture with the WGAN and weight clipping objective as well as the WGAN-GP objective. In
Figure 9 and 10, we plot the inception score provided by each training method as a function of the
number of generator updates with and without uniform averaging.
We notice that uniform averaging seems to improve the inception score, nevertheless it looks like
the sample are a bit more blurry (see Figure 11). This is confirmed by our result (Figure 12) on the
Frechet Inception Distance (FID) which is more sensitive to blurriness. A similar observation about
FID was made in §G.3.
0 5 0 5 0 5 0
. . . . . . ∖
6 5 5 4 4 3 3
8J0UOd80
2.0 "I---1-------1------1------1------1------1------1------
0.00	0.25	0.50	0.75	1.00	1.25	1.50	1.75	2.00
Number of generator updates	×105
(a) with averaging
0 5 0 5 0 5 0
. . . . . . ∖
6 5 5 4 4 3 3
8J0∞UOd80-
0.25	0.50	0.75	100 L 25	150	175	2.00
Number of generator updates	×105
(b) without averaging
Figure 9: Inception Score on CIFAR10 for WGAN over number of generator updates with and
without averaging. We can see that averaging improve the inception score.
AltAdam5 Y = 1 ∙ 10-4
---- SimAdam Y = 1 ∙ 10-4
---- PaStEXtraAdam Y = 1 ∙ 10-4
---- ExtraAdam Y = 5 • 10-4
2	3	4
Number of generator updates
×:
(a) with averaging
(b) without averaging
Figure 10: Inception score on CIFAR10 for WGAN-GP (DCGAN) over number of generator updates
36
Published as a conference paper at ICLR 2019
(a) PastExtraAdam without averaging
(b) PastExtraAdam with averaging
(c) AltAdam5 without averaging
Figure 11: Comparison of the samples of a WGAN trained with the different methods with and
without averaging. Although averaging improves the inception score, the samples seem more blurry
(d) AltAdam5 with averaging
Figure 12: The Frechet Inception Distance (FID) from HeUsel et al. (2017) computed using 50,000
samples, on the WGAN experiments. ReExtraAdam refers to Alg. 5 introduced in §D. We can see
that averaging performs worse than when comparing with the Inception Score. We observed that the
samples generated by using averaging are a little more blurry and that the FID is more sensitive to
blurriness, thus providing an explanation for this observation.
37
Published as a conference paper at ICLR 2019
H Hyperparameters
(DCGAN) WGAN Hyperparameters
Batch size Number of generator update Adam β1 Adam β2 Weight clipping for the discriminator Learning rate for generator	= 64 = 500, 000 = 0.5 = 0.9 = 0.01 = 2 × 10-5 (for Adam1, Adam5, PastExtraAdam, OptimisticAdam) = 5 × 10-5 (for ExtraAdam)
Learning rate for discriminator	= 2 × 10-4 (for Adam1, Adam5, PastExtraAdam, OptimisticAdam) = 5 × 10-4 (for ExtraAdam)
β for EMA	= 0.999
(DCGAN) WGAN-GP Hyperparameters
Batch size Number of generator update Adam β1 Adam β2 Gradient penalty Learning rate for generator	= 64 = 500, 000 = 0.5 = 0.9 = 10 = 1 × 10-4 (for Adam1, Adam5, PastExtraAdam, OptimisticAdam) = 5 × 10-4 (for ExtraAdam)
Learning rate for discriminator	= 1 × 10-4 (for Adam1, Adam5, PastExtraAdam, OptimisticAdam) = 5 × 10-4 (for ExtraAdam)
β for EMA	= 0.999
(ResNet) WGAN-GP Hyperparameters
Batch size Number of generator update Adam β1 Adam β2 Gradient penalty Learning rate for generator	= 64 = 500, 000 = 0.5 = 0.9 = 10 = 2 × 10-5 (for Adam1, Adam5, PastExtraAdam, OptimisticAdam) = 5 × 10-5 (for ExtraAdam)
Learning rate for discriminator	= 2 × 10-4 (for Adam1, Adam5, PastExtraAdam, OptimisticAdam) = 5 × 10-4 (for ExtraAdam)
β for EMA	= 0.9999
38