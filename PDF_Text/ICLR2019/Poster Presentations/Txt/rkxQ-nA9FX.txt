Published as a conference paper at ICLR 2019
Theoretical Analysis of Auto Rate-tuning
by Batch Normalization
Sanjeev Arora	Zhiyuan Li
Princeton University and Institute for Advanced Study Princeton University
arora@cs.princeton.edu	zhiyuanli@cs.princeton.edu
Kaifeng Lyu *
Tsinghus University
lkf15@mails.tsinghua.edu.cn
Ab stract
Batch Normalization (BN) has become a cornerstone of deep learning across
diverse architectures, appearing to help optimization as well as generalization.
While the idea makes intuitive sense, theoretical analysis of its effectiveness has
been lacking. Here theoretical support is provided for one of its conjectured prop-
erties, namely, the ability to allow gradient descent to succeed with less tuning of
learning rates. It is shown that even if we fix the learning rate of scale-invariant
parameters (e.g., weights of each layer with BN) to a constant (say, 0.3), gradient
descent still approaches a stationary point (i.e., a solution where gradient is zero)
in the rate of T -1/2 in T iterations, asymptotically matching the best bound for
gradient descent with well-tuned learning rates. A similar result with convergence
rate T -1/4 is also shown for stochastic gradient descent.
1	Introduction
Batch Normalization (abbreviated as BatchNorm or BN) (Ioffe & Szegedy, 2015) is one of the most
important innovation in deep learning, widely used in modern neural network architectures such as
ResNet (He et al., 2016), Inception (Szegedy et al., 2017), and DenseNet (Huang et al., 2017). It
also inspired a series of other normalization methods (Ulyanov et al., 2016; Ba et al., 2016; Ioffe,
2017; Wu & He, 2018).
BatchNorm consists of standardizing the output of each layer to have zero mean and unit variance.
For a single neuron, if x1 , . . . , xB is the original outputs in a mini-batch, then it adds a BatchNorm
layer which modifies the outputs to
BN(Xi)= Yxi∙^μ + β,	(1)
σ
where μ = -^ PB=I Xi and σ2 = B PB==ι(xi - μ)2 are the mean and variance within the mini-
batch, and γ, β are two learnable parameters. BN appears to stabilize and speed up training, and
improve generalization. The inventors suggested (Ioffe & Szegedy, 2015) that these benefits derive
from the following:
1.	By stabilizing layer outputs it reduces a phenomenon called Internal Covariate Shift,
whereby the training of a higher layer is continuously undermined or undone by changes
in the distribution of its inputs due to parameter changes in previous layers.,
2.	Making the weights invariant to scaling, appears to reduce the dependence of training on
the scale of parameters and enables us to use a higher learning rate;
3.	By implictly regularizing the model it improves generalization.
*Work done while visiting PrinCeton University.
1
Published as a conference paper at ICLR 2019
But these three benefits are not fully understood in theory. Understanding generalization for deep
models remains an open problem (with or without BN). Furthermore, in demonstration that intuition
can sometimes mislead, recent experimental results suggest that BN does not reduce internal covari-
ate shift either (Santurkar et al., 2018), and the authors of that study suggest that the true explanation
for BN’s effectiveness may lie in a smoothening effect (i.e., lowering of the Hessian norm) on the
objective. Another recent paper (Kohler et al., 2018) tries to quantify the benefits of BN for simple
machine learning problems such as regression but does not analyze deep models.
Provable quantification of Effect 2 (learning rates). Our study consists of quantifying the effect
of BN on learning rates. Ioffe & Szegedy (2015) observed that without BatchNorm, a large learning
rate leads to a rapid growth of the parameter scale. Introducing BatchNorm usually stabilizes the
growth of weights and appears to implicitly tune the learning rate so that the effective learning rate
adapts during the course of the algorithm. They explained this intuitively as follows. After BN the
output of a neuron z = BN(w>x) is unaffected when the weight w is scaled, i.e., for any scalar
c > 0,
BN(w>x) = BN((cw)>x).
Taking derivatives one finds that the gradient at cw equals to the gradient at w multiplied by a factor
1/c. Thus, even though the scale of weight parameters of a linear layer proceeding a BatchNorm no
longer means anything to the function represented by the neural network, their growth has an effect
of reducing the learning rate.
Our paper considers the following question: Can we rigorously capture the above intuitive behavior?
Theoretical analyses of speed of gradient descent algorithms in nonconvex settings study the number
of iterations required for convergence to a stationary point (i.e., where gradient vanishes). But they
need to assume that the learning rate has been set (magically) to a small enough number determined
by the smoothness constant of the loss function — which in practice are of course unknown. With
this tuned learning rate, the norm of the gradient reduces asymptotically as T -1/2 in T iterations.
In case of stochastic gradient descent, the reduction is like T -1/4. Thus a potential way to quantify
the rate-tuning behavior of BN would be to show that even when the learning rate is fixed to a
suitable constant, say 0.1, from the start, after introducing BN the convergence to stationary point
is asymptotically just as fast (essentially) as it would be with a hand-tuned learning rate required
by earlier analyses. The current paper rigorously establishes such auto-tuning behavior of BN (See
below for an important clarification about scale-invariance).
We note that a recent paper (Wu et al., 2018) introduced a new algorithm WNgrad that is motivated
by BN and provably has the above auto-tuning behavior as well. That paper did not establish such
behavior for BN itself, but it was a clear inspiration for our analysis of BN.
Scale-invariant and scale-variant parameters. The intuition of Ioffe & Szegedy (2015) applies
for all scale-invariant parameters, but the actual algorithm also involves other parameters such as γ
and β whose scale does matter. Our analysis partitions the parameters in the neural networks into
two groups W (scale-invariant) and g (scale-variant). The first group, W = {w(1) , . . . , w(m) },
consists of all the parameters whose scales does not affect the loss, i.e., scaling w(i) to cw(i) for
any c > 0 does not change the loss (see Definition 2.1 for a formal definition); the second group,
g, consists of all other parameters that are not scale-invariant. In a feedforward neural network with
BN added at each layer, the layer weights are all scale-invariant. This is also true for BN with `p
normalization strategies (Santurkar et al., 2018; Hoffer et al., 2018) and other normalization layers,
such as Weight Normalization (Salimans & Kingma, 2016), Layer Normalization (Ba et al., 2016),
Group Normalization (Wu & He, 2018) (see Table 1 in Ba et al. (2016) for a summary).
1.1 Our contributions
In this paper, we show that the scale-invariant parameters do not require rate tuning for lowering
the training loss. To illustrate this, we consider the case in which we set learning rates separately
for scale-invariant parameters W and scale-variant parameters g. Under some assumptions on the
smoothness of the loss and the boundedness of the noise, we show that
1. In full-batch gradient descent, if the learning rate for g is set optimally, then no matter how
the learning rates for W is set, (W; g) converges to a first-order stationary point in the rate
2
Published as a conference paper at ICLR 2019
O(T -1/2), which asymptotically matches with the convergence rate of gradient descent
with optimal choice of learning rates for all parameters (Theorem 3.1);
2. In stochastic gradient descent, if the learning rate for g is set optimally, then no matter how
the learning rate for W is set, (W ; g) converges to a first-order stationary point in the rate
O(T -1/4 polylog(T)), which asymptotically matches with the convergence rate of gradi-
ent descent with optimal choice of learning rates for all parameters (up to a polylog(T)
factor) (Theorem 4.2).
In the usual case where we set a unified learning rate for all parameters, our results imply that we
only need to set a learning rate that is suitable for g. This means introducing scale-invariance into
neural networks potentially reduces the efforts to tune learning rates, since there are less number of
parameters we need to concern in order to guarantee an asymptotically fastest convergence.
In our study, the loss function is assumed to be smooth. However, BN introduces non-smoothness
in extreme cases due to division by zero when the input variance is zero (see equation 1). Note
that the suggested implementation of BN by Ioffe & Szegedy (2015) uses a smoothening constant
in the whitening step, but it does not preserve scale-invariance. In order to avoid this issue, we
describe a simple modification of the smoothening that maintains scale-invariance. Also, our result
cannot be applied to neural networks with ReLU, but it is applicable for its smooth approximation
softplus (Dugas et al., 2001).
We include some experiments in Appendix D, showing that it is indeed the auto-tuning behavior
we analysed in this paper empowers BN to have such convergence with arbitrary learning rate for
scale-invariant parameters. In the generalization aspect, a tuned learning rate is still needed for the
best test accuracy, and we showed in the experiments that the auto-tuning behavior of BN also leads
to a wider range of suitable learning rate for good generalization.
1.2 Related works
Previous work for understanding Batch Normalization. Only a few recent works tried to theo-
retically understand BatchNorm. Santurkar et al. (2018) was described earlier. Kohler et al. (2018)
aims to find theoretical setting such that training neural networks with BatchNorm is faster than
without BatchNorm. In particular, the authors analyzed three types of shallow neural networks,
but rather than consider gradient descent, the authors designed task-specific training methods when
discussing neural networks with BatchNorm. Bjorck et al. (2018) observes that the higher learning
rates enabled by BatchNorm improves generalization.
Convergence of adaptive algorithms. Our analysis is inspired by the proof for WNGrad (Wu
et al., 2018), where the author analyzed an adaptive algorithm, WNGrad, motivated by Weight
Normalization (Salimans & Kingma, 2016). Other works analyzing the convergence of adaptive
methods are (Ward et al., 2018; Li & Orabona, 2018; Zou & Shen, 2018; Zhou et al., 2018).
Invariance by Batch Normalization. Cho & Lee (2017) proposed to run riemmanian gradient
descent on Grassmann manifold G(1, n) since the weight matrix is scaling invariant to the loss
function. Hoffer et al. (2018) observed that the effective stepsize is proportional to jww2.
2 General framework
In this section, we introduce our general framework in order to study the benefits of scale-invariance.
2.1	Motivating examples of neural networks
Scale-invariance is common in neural networks with BatchNorm. We formally state the definition
of scale-invariance below:
Definition 2.1. (Scale-invariance) Let F (w, θ0) be a loss function. We say that w is a scale-invariant
parameter of F if for all c > 0, F(w, θ0) = F (cw, θ0); if w is not scale-invariant, then we say w
is a scale-variant parameter of F .
3
Published as a conference paper at ICLR 2019
We consider the following L-layer “fully-batch-normalized” feedforward network Φ for illustration:
1B
L(θ) = EZ〜DB B Efyb(BN(W(L)σ(BN(W(LT)…σ(BN(W⑴Xb)))))).	⑵
b=1
Z = {(x1, y1), . . . , (xB, yB)} is a mini-batch of B pairs of input data and ground-truth label from a
data set D. fy is an objective function depending on the label, e.g., fy could be a cross-entropy loss
in classification tasks. W(1), . . . , W(L) are weight matrices of each layer. σ : R → R is a nonlinear
activation function which processes its input elementwise (such as ReLU, sigmoid). Given a batch
of inputs zι,..., ZB ∈ Rm, BN(Zb) outputs a vector Zb defined as
zb,k
zb,k - μk I q
Yk---------+ βk,
σk
(3)
where μk = Eb∈[B][zb,k] and σ2 = Eb∈[B][(zb,k - μk)2] are the mean and variance of zb Yk
and βk are two learnable parameters which rescale and offset the normalized outputs to retain the
representation power. The neural network Φ is thus parameterized by weight matrices W(i) in each
layer and learnable parameters Yk , βk in each BN.
BN has the property that the output is unchanged when the batch inputs z1,k, . . . , zB,k are scaled
or shifted simultaneously. For zb,k = w>Xb being the output of a linear layer, it is easy to see that
wk is scale-invariant, and thus each row vector of weight matrices W(1), . . . , W(L) in Φ are scale-
invariant parameters of L(θ). In convolutional neural networks with BatchNorm, a similar argument
can be done. In particular, each filter of convolutional layer normalized by BN is scale-invariant.
With a general nonlinear activation, other parameters in Φ, the scale and shift parameters Yk and
βk in each BN, are scale-variant. When ReLU or Leaky ReLU (Maas et al., 2013) are used as the
activation σ, the vector (Y1, . . . , Ym, β1, . . . , βm) of each BN at layer 1 ≤ i < L (except the last
one) is indeed scale-invariant. This can be deduced by using the the (positive) homogeneity of these
two types of activations and noticing that the output of internal activations is processed by a BN
in the next layer. Nevertheless, we are not able to analyse either ReLU or Leaky ReLU activations
because we need the loss to be smooth in our analysis. We can instead analyse smooth activations,
such as sigmoid, tanh, softplus (Dugas et al., 2001), etc.
2.2	Framework
Now we introduce our general framework. Let Φ be a neural network parameterized by θ . Let
D be a dataset, where each data point Z 〜D is associated with a loss function Fz (θ) (D can
be the set of all possible mini-batches). We partition the parameters θ into (W; g), where W =
{w(1), . . . , w(m)} consisting of parameters that are scale-invariant to all Fz, and g contains the
remaining parameters. The goal of training the neural network is to minimize the expected loss over
the dataset: L(W; g) := Ez〜D[Fz(W; g)]. In order to illustrate the optimization benefits of scale-
invariance, we consider the process of training this neural network by stochastic gradient descent
with separate learning rates for W and g:
w(+)ι - w(i) - ηw,⅛vw(i)Fzt(θt),	gt+ι - gt - ηg,tVgtFzt(θt).	⑷
2.3	The intrinsic optimization problem
Thanks to the scale-invariant properties, the scale of each weight w(i) does not affect loss values.
However, the scale does affect the gradients. Let V = {v(1), . . . , v(m)} be the set of normalized
weights, where v(i) = w(i)/kw(i) k2. The following simple lemma can be easily shown:
Lemma 2.2 (Implied by Ioffe & Szegedy (2015)). For any W and g,
Vw(i) Fz (W; g) = r-1^ Vv(i) Fz (V; g),	Vg Fz (W; g) = Vg Fz (V; g).	⑸
kw(i)k2
To make kVw(i) Fz (W; g)k2 to be small, one can just scale the weights by a large factor. Thus there
are ways to reduce the norm of the gradient that do not reduce the loss.
4
Published as a conference paper at ICLR 2019
For this reason, we define the intrinsic optimization problem for training the neural network. Instead
of optimizing W and g over all possible solutions, we focus on parameters θ in which kw(i) k2 = 1
for all w(i) ∈ W. This does not change our objective, since the scale of W does not affect the loss.
Definition 2.3 (Intrinsic optimization problem). LetU = {θ | kw(i) k2 = 1 for all i} be the intrinsic
domain. The intrinsic optimization problem is defined as optimizing the original problem in U :
min L(W; g).	(6)
(W M∈U
For {θt} being a sequence of points for optimizing the original optimization problem, we can define
{θt }, where θt = (Vt ; gt ), as a sequence of points optimizing the intrinsic optimization problem.
In this paper, we aim to show that training neural network for the original optimization problem by
gradient descent can be seen as training by adaptive methods for the intrinsic optimization problem,
and it converges to a first-order stationary point in the intrinsic optimization problem with no need
for tuning learning rates for W .
2.4	Assumptions on the loss
We assume Fz (W ; g) is defined and twice continuously differentiable at any θ satisfying none of
w(i) is 0. Also, we assume that the expected loss L(θ) is lower-bounded by Lmin.
Furthermore, for V = {v(1), . . . , v(m)}, where v(i) = w(i)/kw(i) k2, we assume that the following
bounds on the smoothness:
∂∂
∂VWVj)Fz(Hg) 2 ≤ LL *v,	∂vwgFz(Hg) 2 ≤ Lvg,	UVgFz(H皿2 ≤ Lgg.
In addition, we assume that the noise on the gradient of g in SGD is upper bounded by Gg :
E [kVgFz(V;g) - Ez〜D [VgFz(V;g)]k2i ≤ G2.
Smoothed version of motivating neural networks. Note that the neural network Φ illustrated in
Section 2.1 does not meet the conditions of the smooothness at all since the loss function could be
non-smooth. We can make some mild modifications to the motivating example to smoothen it 1:
(1)	. The activation could be non-smooth. A possible solution is to use smooth nonlinearities, e.g.,
sigmoid, tanh, softplus (Dugas et al., 2001), etc. Note that softplus can be seen as a smooth approx-
imation of the most commonly used activation ReLU.
(2)	. The formula of BN shown in equation 3 may suffer from the problem of division by zero. To
avoid this, the inventors of BN, Ioffe & Szegedy (2015), add a small smoothening parameter > 0
to the denominator, i.e.,
Zb,k =Yk ZbFk + βk,
√σ2+^
(7)
However, when zb,k = w>Xb, adding a constant E directly breaks the scale-invariance of Wk.
We can preserve the scale-invariance by making the smoothening term propositional to kwk k2, i.e.,
replacing E with d∣Wk k2. By simple linear algebra and letting U := Eb∈[B][Xb], S := Varb∈[B](Xb),
this smoothed version ofBN can also be written as
zb,k
w> (Xb - U)
kwk l∣S+eI
+ βk .
(8)
Since the variance of inputs is usually large in practice, for small E, the effect of the smoothening
term is negligible except in extreme cases.
Using the above two modifications, the loss function is already smooth. However, the scale of
scale-variant parameters may be unbounded during training, which could cause the smoothness
unbounded. To avoid this issue, we can either project scale-variant parameters to a bounded set, or
use weight decay for those parameters (see Appendix C for a proof for the latter solution).
1Our results to this network are rather conceptual, since the smoothness upper bound can be as large as
MO(L), where L is the number of layers and M is the maximum width of each layer.
5
Published as a conference paper at ICLR 2019
2.5	Key observation: the growth of weights
The following lemma is our key observation. It establishes a connection between the scale-invariant
property and the growth of weight scale, which further implies an automatic decay of learning rates:
Lemma 2.4. For any scale-invariant weight w(i) in the network Ψ, we have:
⑴.w(i) and V (i) Fz (θt) are always perpendicular;
wt	t
式i) l∣2 一 II*⑴ l∣2 _l g2 Il V7 ,、T ，/)'l∣2 一 IJ 式 i)l|2_L ηW,t Il V7 ,、T ，石、||2
(2). kwt+1k2 = kwt k2 + nw,tkVw⑴ Fzt(θt)k2 = kwt k2 + 7~(i⅛2 kV v⑴ Fzt (θt )k 2 ♦
t	kwt k2 t
Proof. Let θt0 be all the parameters in θt other than wt(i) . Taking derivatives with respect to c for
the both sides of Fzt (w(i), θ0) = Fzt(cw(i), θ0), We have 0 = IcFzt (cw(i), θ0). The right hand
side equals V (i) Fz (cwt(i), θt0)>wt(i), so the first proposition follows by taking c = 1. Applying
cwtt'
Pythagorean theorem and Lemma 2.2, the second proposition directly follows.	□
Using Lemma 2.4, We can shoW that performing gradient descent for the original problem is equiv-
alent to performing an adaptive gradient method for the intrinsic optimization problem:
Theorem 2.5. Let G(ti) = kwt(i) k22. Then for all t ≥ 0,
v(+ι=Π (v(i)-得Vv(i)Fzt(θt)) ,	G(+ι = Gti) + 宗kVv(i)Fzt(θt)k2,	(9)
Gt	t	Gt	t
where Π is a projection operator which maps any vector w to w/kwk2.
Remark 2.6. Wu et al. (2018) noticed that Theorem 2.5 is true for Weight Normalization by direct
calculation of gradients. Inspiring by this, they proposed a new adaptive method called WNGrad.
Our theorem is more general since it holds for any normalization methods as long as it induces
scale-invariant properties to the network. The adaptive update rule derived in our theorem can be
seen as WNGrad with projection to unit sphere after each step.
Proof for Theorem 2.5. Using Lemma 2.2, we have
(i)	(i)	ηw,t	(i)	(i)	ηw,t
wt+1 =	wt	-	^^^(iΓ- Vv(i) Fzt (θt)	= kwt	k2	I	vt	—	-(i) Vv(i) Fzt (θt)
kwt k2 t	Gt t
which implies the first equation. The second equation is by Lemma 2.4.	□
While popular adaptive gradient methods such as AdaGrad (Duchi et al., 2011), RMSprop (Tieleman
& Hinton, 2012), Adam (Kingma & Ba, 2014) adjust learning rates for each single coordinate, the
adaptive gradient method described in Theorem 2.5 sets learning rates ηw,t/GYi for each scale-
invariant parameter respectively. In this paper, we call ηw,t∕Gtii the effective learning rate of v(i)
or WF, because it is ηw,t∕∣∣w(ii∣∣2 instead of ηw,t alone that really determines the trajectory of
gradient descent given the normalized scale-invariant parameter vt(i). In other words, the magnitude
of the initialization of parameters before BN is as important as their learning rates: multiplying the
initialization of scale-invariant parameter by constant C is equivalent to dividing its learning rate
by C2 . Thus we suggest researchers to report initialization as well as learning rates in the future
experiments.
3	Training by full-batch gradient descent
In this section, we rigorously analyze the effect related to the scale-invariant properties in training
neural network by full-batch gradient descent. We use the framework introduced in Section 2.2 and
assumptions from Section 2.4. We focus on the full-batch training, i.e., zt is always equal to the
whole training set and Fzt (θ) = L(θ).
6
Published as a conference paper at ICLR 2019
3.1	Settings and main theorem
Assumptions on learning rates. We consider the case that we use fixed learning rates for both W
and g, i.e.,	ηw,0	∙ ∙ ∙	ηw,τ —ι	ηw	and ηg,0	∙ ∙ ∙	ηg,τ —ι	ηg.	^We	assume that ηg	is
tuned carefully to ηg = (1 - cg)/Lgg for some constant cg ∈ (0, 1). For ηw, we do not make any
assumption, i.e., ηw can be set to any positive value.
Theorem 3.1. Consider the process of training Φ by gradient descent with ηg = 2(1 - cg)/Lgg and
arbitrary ηw > 0. Then Φ converges to a stationary point in the rate of
min
0≤t<T
... ~
kVL(Vt;gt)∣∣2 ≤ O
ι + ηw “
√ηg ))
(10)
where Vt = {v(1),..., v(m)} with v(i) = w(i)∕kw(i)k2, O suppresses polynomial factors in Lv,
Lv, Lgg, ∣∣w(i)∣∣2, I∣w0i)k-1, L(θo) - Lmin Jbrall i,j, and we see Lgg = Ω(1).
This matches the asymptotic convergence rate of GD by Carmon et al. (2018).
3.2	Proof sketch
The high level idea is to use the decrement of loss function to upper bound the sum of the squared
norm of the gradients. Note that ∣∣VL(Vt; gt)k2 = Pm=IkVv(i)L(Vt; gt)k2 + ∣∣VgtL(Vt; gt)k2.
ForthefirstPart Pm=IkVv(i)L(Vt; gt)∣2, we have
m T-1	m T-1
XXkVv(i)L(Vt;gt)k22=XXkwt(i)k22kVw(i)L(Wt;gt)k22
i=1 t=0	i=1 t=0
m
≤ X kwT⅜2 ∙
i=1
kwT(i)k22 - kw0(i)k22
(11)
ηw2
Thus the core of the Proof is to show that the monotone increasing kwT(i) k2 has an uPPer bound for
all T. It is shown that for every w(i), the whole training Process can be divided into at most two
phases. In the first phase, the effective learning rate ηw∕∣∣w(i)k2 is larger than some threshold ə-
(defined in Lemma 3.2) and in the second Phase it is smaller.
Lemma 3.2 (Taylor Expansion). Let Ci := 11 Pm=I LV + Vvlm/(CgLg) = O(1). Then
L(θt+1) -L(θt) = - XηwkVw(i)L(θt)k2 ( 1 -	) - 1 Cgng kVgtL(θt)k2.	(12)
i=1	t	kwt k22	2
If kwt(i) k2 is large enough and that the process enters the second phase, then by Lemma 3.2 in
each step the loss function L will decrease by ηw||Vw(i)L(θt)k2 = kwt+1k2-kwt k2 (Recall that
kwt(+i)1 k22 - kwt(i) k22 = ηw2 tkV (i) L(θt)k22 by Lemma 2.4). Since L is lower-bounded, we can
t+1	t	w,t	wt
conclude kwt(i) k22 is also bounded .
For the second part, we can also show that by Lemma 3.2
T-1
X kVgtL(Vt;gt)k22
t=0
L(θ0) - L(θT) +
m
X
i=1
CikwT)k2
kw(i)k2
Thus We can conclude O(√T) convergence rate OfkVL(θt)k2 as follows.
T-1	m	m
0m<TkVL(Vt;gt)kl ≤ T X X kVv(i)L(Vt;gt)k2 + X kVgtL(Vt;gt)k2 ≤ O(-)
≤ <	t=0 i=1	i=1
The full proof is postponed to Appendix A.
7
Published as a conference paper at ICLR 2019
4	Training by stochastic gradient descent
In this section, we analyze the effect related to the scale-invariant properties when training a neu-
ral network by stochastic gradient descent. We use the framework introduced in Section 2.2 and
assumptions from Section 2.4.
4.1	Settings and main theorem
Assumptions on learning rates. As usual, we assume that the learning rate for g is chosen carefully
and the learning rate for W is chosen rather arbitrarily. More specifically, we consider the case that
the learning rates are chosen as
ηw,t = ηw ∙ (t + 1)-α,	ηg,t = η Yt + I)T/2.
We assume that the initial learning rate ηg of g is tuned carefully to ηg = (1 - cg)/Lgg for some
constant cg ∈ (0, 1). Note that this learning rate schedule matches the best known convergence rate
O(T -1/4) of SGD in the case of smooth non-convex loss functions (Ghadimi & Lan, 2013).
For the learning rates of W, we only assume that 0 ≤ α ≤ 1/2, i.e., the learning rate decays equally
as or slower than the optimal SGD learning rate schedule. ηw can be set to any positive value. Note
that this includes the case that We set a fixed learning rate ηw,o = …=ηw,τ-ι = ηw for W by
taking α = 0.
Remark 4.1. Note that the auto-tuning behavior induced by scale-invariances always decreases the
learning rates. Thus, if we set α > 1/2, there is no hope to adjust the learning rate to the optimal
strategy Θ(t-1/2). Indeed, in this case, the learning rate 1/Gt in the intrinsic optimization process
decays exactly in the rate ofΘ(t-α), which is the best possible learning rate can be achieved without
increasing the original learning rate.
Theorem 4.2. Consider the process of training Φ by gradient descent with ηw,t = ηw ∙ (t + 1)-α
and ηg,t = ηg ∙ (t + 1)-1/2, where ηg = 2(1 — Cg)∕Lgg and ηw > 0 is arbitrary. Then Φ converges
to a stationary point in the rate of
0mi<TE UVL(Vt；gt)k2] ≤
0 ≤ α < 1/2;
α = 1/2.
(13)
where Vt = {v(1),..., v(m)} with v(i) = w(i)∕kw(i)k2, O suppresses polynomial factors in
ηw ,ηw1,η-1, LVj, Lv, Lgg, kw0i)k2, kw0i)k-1, L(θo) -Lmin forall i,j, and we see Lgg = Ω(1).
Note that this matches the asymptotic convergence rate of SGD, Within a polylog(T ) factor.
4.2	Proof sketch
We delay the full proof into Appendix B and give a proof sketch in a simplified setting Where there
is no g and α ∈ [0, 2). We also assume there,s only one Wi, that is, m = 1 and omit the index i.
By Taylor expansion, We have
2 Lvv
E [L(θt+1)] ≤ L(θt) — Γw⅛kVwtL(θt)k2 + E ηw⅛kVwtFzt (θt)k2	(14)
kvt k	kwt k2
We can lower bound the effective learning rate 常WTρ^ and upper bound the second order term re-
spectively in the folloWing Way:
(1)	. For all 0 ≤ α < 2, the effective learning rate 八黑口 = Ω(TT/2);
(2)	PT η22 ,tLvv kv F (θ ) k2 — O /lc O. ( kwTk AA — O(1 C T T)
(2)	. 2^t=0 kwtk2 kvwt Fzt (θt)k2 = o (lθg	) J = O(IOg T ).
Taking expectation over equation 14 and summing it up, we have
T-1	T-1 2 Lvv
E E rw⅛kVvtL(θt)k2 ≤ L(θo) — E[L(θτ)]+ E ∑ηw⅛kVwtFzt(θt)k2
t=0 kwt k	t=0 kwtk2
8
Published as a conference paper at ICLR 2019
Plug the above bounds into the above inequality, we complete the proof.
T-1
Ω(T-1/2) ∙ E X kVvtL(θt)k2 ≤L(θo)- E[L(θτ)]+ O(logT).
t=0
5 Conclusions and future works
In this paper, we studied how scale-invariance in neural networks with BN helps optimization, and
showed that (stochastic) gradient descent can achieve the asymptotic best convergence rate without
tuning learning rates for scale-invariant parameters. Our analysis suggests that scale-invariance in
nerual networks introduced by BN reduces the efforts for tuning learning rate to fit the training data.
However, our analysis only applies to smooth loss functions. In modern neural networks, ReLU or
Leaky ReLU are often used, which makes the loss non-smooth. It would have more implications by
showing similar results in non-smooth settings. Also, we only considered gradient descent in this
paper. It can be shown that if we perform (stochastic) gradient descent with momentum, the norm
of scale-invariant parameters will also be monotone increasing. It would be interesting to use it to
show similar convergence results for more gradient methods.
Acknowledgments
Thanks Yuanzhi Li, Wei Hu and Noah Golowich for helpful discussions. This research was done
with support from NSF, ONR, Simons Foundation, Mozilla Research, Schmidt Foundation, DARPA,
Amazon, and SRC. We thank Amazon Web Services for providing compute time for the experiments
in this paper.
References
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
Nils Bjorck, Carla P Gomes, Bart Selman, and Kilian Q Weinberger. Understanding batch normal-
ization. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett
(eds.), Advances in Neural Information Processing Systems 31, pp. 7705-7716. Curran Asso-
ciates, Inc., 2018.
Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding stationary
points of non-convex, smooth high-dimensional functions. 2018.
Minhyung Cho and Jaehyung Lee. Riemannian approach to batch normalization. In I. Guyon, U. V.
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in
Neural Information Processing Systems 30, pp. 5225-5235. Curran Associates, Inc., 2017.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
Charles Dugas, Yoshua Bengio, Francois Belisle, Claude Nadeau, and Rene Garcia. Incorporating
second-order functional knowledge for better option pricing. In T. K. Leen, T. G. Dietterich, and
V. Tresp (eds.), Advances in Neural Information Processing Systems 13, pp. 472-478. MIT Press,
2001.
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochas-
tic programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Yee Whye Teh and Mike Titterington (eds.), Proceedings of the Thirteenth Interna-
tional Conference on Artificial Intelligence and Statistics, volume 9 of Proceedings of Machine
Learning Research, pp. 249-256, Chia Laguna Resort, Sardinia, Italy, 13-15 May 2010. PMLR.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
9
Published as a conference paper at ICLR 2019
Elad Hoffer, Ron Banner, Itay Golan, and Daniel Soudry. Norm matters: efficient and accurate
normalization schemes in deep networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31,
pp. 2164-2174. Curran Associates, Inc., 2018.
Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 2261-2269. IEEE, 2017.
Sergey Ioffe. Batch renormalization: Towards reducing minibatch dependence in batch-normalized
models. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 1945-1953. Curran
Associates, Inc., 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: accelerating deep network training by
reducing internal covariate shift. In Proceedings of the 32nd International Conference on Inter-
national Conference on Machine Learning-Volume 37, pp. 448-456. JMLR. org, 2015.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Jonas Kohler, Hadi Daneshmand, Aurelien Lucchi, Ming Zhou, Klaus Neymeyr, and Thomas
Hofmann. Towards a theoretical understanding of batch normalization. arXiv preprint
arXiv:1805.10694, 2018.
Philipp Krahenbuhl, Carl Doersch, JeffDonahue, and Trevor Darrell. Data-dependent initializations
of convolutional neural networks. arXiv preprint arXiv:1511.06856, 2015.
Xiaoyu Li and Francesco Orabona. On the convergence of stochastic gradient descent with adaptive
stepsizes. arXiv preprint arXiv:1805.08114, 2018.
Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectifier nonlinearities improve neural net-
work acoustic models. In in ICML Workshop on Deep Learning for Audio, Speech and Language
Processing. Citeseer, 2013.
Dmytro Mishkin and Jiri Matas. All you need is a good init. arXiv preprint arXiv:1511.06422,
2015.
Tim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate
training of deep neural networks. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp. 901-909. Curran
Associates, Inc., 2016.
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch nor-
malization help optimization? In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 2488-
2498. Curran Associates, Inc., 2018.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander Alemi. Inception-v4, inception-
resnet and the impact of residual connections on learning. In AAAI Conference on Artificial
Intelligence, 2017.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26-
31, 2012.
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing in-
gredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016.
Rachel Ward, Xiaoxia Wu, and Leon Bottou. Adagrad stepsizes: Sharp convergence over nonconvex
landscapes, from any initialization. arXiv preprint arXiv:1806.01811, 2018.
10
Published as a conference paper at ICLR 2019
Xiaoxia Wu, Rachel Ward, and Leon Bottou. WNGrad: Learn the Learning Rate in Gradient De-
scent. arXiv preprint arXiv:1803.02865, 2018.
Yuxin Wu and Kaiming He. Group normalization. In The European Conference on Computer Vision
(ECCV), September 2018.
Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu. On the convergence of
adaptive gradient methods for nonconvex optimization. arXiv preprint arXiv:1808.05671, 2018.
Fangyu Zou and Li Shen. On the convergence of adagrad with momentum for training deep neural
networks. arXiv preprint arXiv:1808.03408, 2018.
A Proof for Full-Batch Gradient Descent
By the scale-invariant property of w(i), we know that L(W; g) = L(V ; g). Also, the following
identities about derivatives can be easily obtained:
∂w(i)∂ Wj) "W' g ∣∣w(i)∣∣2∣∣w(j)∣∣2 ∂ V⑴∂Vj) ""； g
∂w⑶∂g"" g ∣∣W(i)∣∣2∣∣W(j)∣∣2 ∂ V⑴∂g""' g
▽g L(W' g) = Vg L(V; g).
Thus, the assumptions on the smoothness imply
∂
∂w(i)∂w(j) L(W' g) 2
∂
∂WW L(W' g)2
V2gL(W;g)2
≤ L
~ kw(i)k2kw(j)k2
≤	Lvg
_ kw⑴k2
≤ Lgg .
(15)
(16)
(17)
Proof for Lemma 3.2. Using Taylor expansion, we have ∃γ ∈ (0, 1), such that for wt(i0) = (1 -
γ)wt(i) +γwt(+i)1,
m>
L(θt+1) - L(θt) ≤ X ∆wt(i) Vw(i)L(θt) + ∆gt>VgtL(θt)
i=1	t
mm
+2 XXbw(Cl"j"
i=1 j=1	2
Lj
2mi1∣2M叫 2
m	vg
+X卜叫2闷此F
m
+ 2 X k∆gtk2 Lgg.
i=1
Note that wt(+i)1 - wt(i) = ηwVw(i) L(θt) is perpendicular to wt(i), we have
kwt(i0)k2 ≥ kγ(wt(+i)1 - wt(i)) +wt(i)k2 ≥ kwt(i)k2.
Thus,
11
Published as a conference paper at ICLR 2019
m>
L(θt+ι) -L(θt) ≤ X ∆w(i) Vw(i)L(θt) + ∆gt>VgtL(θt)
i=1
mm
+2 xx"i) Ww(j"
i=1 j=1
m
+X ∆wt(i)2 k∆gtk
i=1
m
+ 2 X k∆gtk2 Lgg.
i=1
岑
2 Hl M叫 2
Lvg
2 FI
By the inequality of arithmetic and geometric means, we have
△w("2"(IL 同；
≤ 1卜则2
(i)
wt
+ 1bw(j"∣2 IjF
2	2" t%叫2
∆wt(i)	k∆gtk2
wt(i)2
≤ ∆wt(i)22
Lvg2m3gg)+4 CgiM k2 Lm.
(i)
wt
2
2
Taking ∆wt(i)
-ηwVw(i)L(θt), ∆gt = -ηgVgtL(θt), we have
m
L(θt+1) - L(θt) ≤ - X ηwkVw(i)L(θt)k22 - ηgkVgtL(θt)k22
i=1
mm	2
+ o X X % + 2LvgmKCg Lgg) r⅛ kvw(i) L(θt)k2
2 i=1 j=1	wt(i)2 t
+ 2(1 + Cgan kVgtL(θt)k2 Lgg.
We can complete the proof by replacing ɪ Pm=I Lvv + Lvg2m/(CgLgg) With Ci.	□
Using the assumption on the smoothness, we can show that the gradient with respect to w(i) is
essentially bounded:
Lemma A.1. For any W and g, we have
IlVW⑶ L(W; g)k2 ≤ |如扁2
(18)
Proof. A.1 Fix all the parameters except w(i). Then L(W; g) can be Written as a function f (w(i))
on the variable w(i). Let S = {w(i) | Iw(i) I2 = 1}. Since f is continuous and S is compact, there
must exist vm(i)in ∈ S such that f(vm(i)in) ≤ f (w(i)) for all w(i) ∈ S. Note that w(i) is scale-invariant,
so wm(i)in is also a minimum in the entire domain and Vf (wm(i)in) = 0.
For an arbitrary w(i), let v(i) = w(i)/Iw(i) I2. Let h : [0, 1] → S be a curve such that h(0) = vm(i)in,
h(1) = v(i), and h goes along the geodesic from vm(i)in to v(i) on the unit sphere S With constant
speed. Let H(τ) = Vf (h(τ)). By Taylor expansion, We have
Vf(v(i)) = H(1) = H(0) + H0(ξ) = V2f(h(ξ))h0(ξ).
Thus, kVf(v(i))k2 ≤ πLvv and |Vw(i)L(W;g)∣2 ≤ k^ɪ.
□
12
Published as a conference paper at ICLR 2019
The following lemma gives an upper bound to the weight scales.
Lemma A.2. For all T ≥ 0, we have
1.	for i = 1, . . . , m, let ti be the maximum time 0 ≤ τ < T satisfying kwτ(i) k22 ≤ 2Ciηw
(ti = -1 if no such τ exists), then kwt(i)+1 k2 can be bounded by
kw(i+ιk2 ≤ kw0i)k2 + ηw(2Ci + (∏LVv)2	；	(19)
kw0 k22
2.	the following inequality on weight scales at time T holds:
m kw(i)k2	kw(i)k2	1	T-1	m
X k T k22 k 0 k2 + 2Cgηg X kVgtL(θt)k2 ≤ L(θo) - Lmin + XKi, (20)
i=1	2ηw	2	t=0	i=1
where Ki = (kW0⅞ +1) " + S产 k⅛) = O(ηw + 1)∙
Proof. A.2 For every i = 1,...,m, let S(i) := -ηw Pt-I ||Vw(i)L(θτ )k2 ( 1 -「笥? ). Also
wτ	kwτ k2
let Gt := -(1∕2)cgηg PT=O IlVgtL(θt)∣2. By Lemma 3.2 We know that L(θτ) ≤ L(θo) +
Pim=1 ST(i) + GT .
Upper Bound for Weight Scales at ti + 1.
we can bound |wt(i)+1 |22 by Lemma A.1:
Ifti = -1, then |wt(ii)+1|22 = |w0(i)|22. For ti ≥ 0,
|wt(ii)+1|22 = |wt(ii)|22 +ηw2
Vwt(ii)L(θti)	≤ 2Ciηw + ηw2
2
∏LVv	ʌ
Ilw0i)k2√
2
2
ηw I + (nLVV)2 ⅛
Ineither case, Wehave kw(i+ι∣∣2 ≤ kw0i)k2 + ηw ( 2Ci + (∏Lvv)2产琳
kw0 k2
Upper Bound for Weight Scales at T. Since Iwτ(i) I22 is non-decreasing with τ and Iwt(i)+1 I22 >
2Ciηw,	i
T-1
ST(i) - St(ii+) 1 = -ηw X	IVwτ(i) L(θτ )I22
τ=t0i+1
T-1
≤-ηw	X	1VwTi)L(θτ)k2
τ=t0i+1
≤ --— kw(i)k2 + ɪ I kw(i)k2 + ηw 2 2Ci + (∏Lvv)2 ηw ))
≤ 2ηwk t k2 + 2ηw C 0 k2 + 由 ( i + ( i ) ∣w0i)12〃
≤
—
kwT)k2 , kw0i)k2 l 1 Λr LTrvv)2	ηw
k + k + 2(2Ci + (πLi )M
13
Published as a conference paper at ICLR 2019
WhereWeUsethefactthatkw(i)k2 = ∣∣w0i)∣∣2 + nWPT=0 IVw(i)L(θτ)k2 atthethirdline. Wecan
bound S(ii+) 1 by
Sa ≤ nw (X k" L(θτ )k2) ⅛
≤ (kw(i+ιk2- kw0i)k2) ∙	Ci) 2
kw0 k22
≤ nw (2Ci + (∏Lvv)2 √nfc! ∙ ^C⅛.
kw0 k22	kw0 k22
Combining them together, We have
kwT)k2- kw0i)k2
2ηw
2Ci+(πLvv )2 kW⅛
Ciηw
T kw
-ST + Ki .
+ 1	2Ci + (πLVv )2 -^W-
2	kw0(i)k22
Taking sUm over all i = 1, . . . , m and also sUbtracting GT on the both sides, We have
m	(i) 2	(i) 2	T-1	m	m
X kwT k22- kw0 k2 + (1∕2)cgng X kVgt L(θt)k2 ≤-X SE)- Gt + X Ki
i=1	2nw	t=0	i=1	i=1
m
≤ L(θ0) - Lmin + X Ki.
i=1
where Lmin ≤ L(θτ) ≤ L(θo) + Pm=I ST + GT is used at the second line.	□
Combining the lemmas above together, We can obtain oUr resUlts.
Proof for Theorem 3.1. By Lemma A.2, We have
m T-1
XXVvt(i)L(Vt ;gt )
i=1 =0
i=1 =0
Vwt(i)L(Wt ;gt )22
≤
≤
≤
—
—
+
≤
1
+ 2
华k2
X kwT)k2 ∙kwT)k2 -kw0i)k2
i=1	nw
)m
• X
i=1
kwT(i)k22 - kw0(i)k22
nw
≤
nw
wT(i)k22
max
1≤i≤m
m
i=1
+	Ki +
m
min+	Ki
i=1
≤ o (1 + 1nw
ng
kw0(i)k22
i=1
nw
m
L(θ0) -Lmin+XKi
i=1
m
14
Published as a conference paper at ICLR 2019
Combining them together we have
T ∙ min
0≤t<T
▽v(i)L(Vt; gt)∣∣2 + kgtk2}
m T-1	2 T-1
≤ XX	∣R(i) L(Vt;	gt)∣∣2	+ X	kVgt L(Vt;	gt)k2
i=1 t=0	t=0
≤ O (-1 + nW +
ηw	w
Thus mino≤t<τ ∣∣VL(Vt, gt) ∣∣2 converges in the rate of
min
0≤t<T
∣VL(Vt,gt)∣2
〜
O
1 + nw ”
√ng ))
□
B Proof for Stochastic Gradient Descent
Let Ft = σ{zo,..., zt-ι} be the filtration, where σ{∙} denotes the sigma field.
We use Lt := Lzt(θt), Ft ：= Fzt (θt) for simplicity. As usual, We define v(i) = w(i)∕kw(i)∣2.
We use the notations V (i) Lt := V (i) Lz (Vt, gt) and V (i) Ft := V (i) Fz (Vt, gt) for short. Let
vt	vt	t	vt	vt	t
∆wt(i) = -nw,tVw(i)Ft, ∆gt = -ng,tVgt Ft.
Lemma B.1. For any a0 , . . . , aT ∈ [0, B] with a0 > 0,
T	T-1
X pt-t— ≤ log2 I X at/ao ) + 1 + 2B∕ao
t=1	τ=0 aτ	t=0
Proof. Let ti be the minimum 1 ≤ t ≤ T such that Ptτ-=10 aτ ≥ ao ∙ 2i. Let k be the maximum i
such that ti exists. Let tk+1 = T + 1. Then we know that
ti+1-1
X
t=ti
at	≤ ao ∙ 2i + B ≤
PT=O aτ -	ao ∙ 2i	-
1 + B ∙ 2-i
a0
Thus, PT=I PtaI aτ ≤ Pk=O (1+ IO ∙ 2-i) = k +1 + 20 ≤ log2 (PT=OIat/ao) + 1 + 2B/ao.
τ = 0 T	□
Lemma B.2. Fix T > 0. Let
m
Ci :=2 X Lv + LVg2m∕(CgLgg)
2 j=1
U := 1 + ；g/2 LggGg
T-1	T-1 n2
Si :=	— X nW,t ∣∣V (i)Ltkg	+ Ci	X ”,t	∣∣V (i)Ftkg
i	(i)	g vt t g i	(i)	4 vt t g
t=o ∣wt ∣g	t=o ∣wt ∣g
T-1	T-1
R := - 2 Cg X ng,tkVgtLtkg + U X ng,tGg
t=o	t=o
Then E[LT] - Lo ≤ Pim=1 E[Si] + E[R].
15
Published as a conference paper at ICLR 2019
Proof. Conditioned on Ft , by Taylor expansion, we have
m
E[Lt+ι I Ft] ≤Lt-ηw,t X kVw(i)Ltk2 -ηg,tkVg∕tk2+ E[Qt | Ft]	(21)
i=1
where Qt is
m m	vv
Qt=2 XX FjC "g"2
m	Lvg	1
+ ∑ r⅛ k∆w(i)∣∣2k∆gtk2 + - Lggk∆gtk2
i=1 wt(i)	2
By the inequality √ab ≤ 2a + 11 b,we have
vv
k∆w(i)k2k∆w(j)k2	(i) ij (j
kwt(i)k2kwt(j)k2
k∆w(i)k2k∆gtk2 -^L-
kwt(i)k2
-k∆w(i)k2 -Lv- + - k∆w(j) k2 -j
2	kw(i)k2	2	kw(j)k2
k∆wt(i)k22
LFmeg) + - Cg k∆gtk2 上.
4m
kw
F) k2
≤
≤
NotethatE[k∆gtk22 | Ft] ≤ ηg2,t kVgtFtk22 + Gg2 . Thus,
m - m	η2
E[Qt I Ft] ≤ X 2 XLj + 2LVg2m/(CgLgg)	-jw⅛kvw(i)Ftk2
i=1 2j=1	kwt k22	t
+ V√2Lgg ∙ ηlt (kVgtFtk2 + Gg)
m η2
≤ ECi√⅛kVw(i)Ftk2 + (1 - Cg/2)ηg,tkVgtFtk2 + Unit
i=1	kwt k22	t
Taking this into equation 21 and summing up for all t, we have
m T-1
E[LT]-L0 ≤ -XXE
m	T-1
+ X Ci X E
i=1	t=0
-1
7jw⅛kVv(i)Ltk2 - 2Cg X E [ng,tkVgtLtk2]
kwt k22	t	2	t=0
η2	T-1
⅛ kVw(i)Ftk2	+ U X n2,tGg,
kwt k2	t=0
and the right hand side can be expressed as Pim=1 E[Si] + E[R] by definitions.
□
Lemma B.3. For any T ≥ 0,	≤ i ≤ m, we have
nw,τ ≥ [Ω(T-1/2)	if 0 ≤ α < 1/2;
kw⅛ ≥ [Ω((TlogT)-1/2)	if α = 1/2.
16
Published as a conference paper at ICLR 2019
Proof. By Lemma 2.4, we have
kwt(+i)1k24
(kw(i)k2 + rηw⅛ kVv(i)Ftk2)
kwt	k22	t
kw(i)k4 + 2ηW ,tkQi)Ftk2 + -η⅛- kVv(i) Ftk2
t	kwt	k42	t
≤
≤
kw(i)k2 + 2ηW(t + i)-2α (∏LViv)2 + nW(t + i)-4α (苔Vr)
kw0(i)k2
kw(i)k2 + (t + i)-2α 0nW(∏LVv)2 + nW (XnLvj!)
≤
kw0i)k4 + (X；!卜(πLiiv)2 + ηw (⅛!4
For 0 ≤ α < 1/2, PT-OL ɪa =O(T 1-2α), so ^w(i⅞ = Ω(T-1/2); for α = 1/2,
kwT k2
PT-OL F+⅛α =O(logT), so 4 = Ω((TlogT)-1/2).	□
kwT k2
Lemma B.4. For T > 0,
•	Si can be bounded by Si ≤ -γw,T2P PTOIkvr⑶ Ltk2 + O(log T);
kwT k2	vt
•	R can be bounded by R ≤ —Ω(T-1/2) PT-ɔ1 ∣∣Vgt Ltk2 + O(log T).
Proof. Fix i ∈ {1, . . . , m}. First we bound Si. Recall that
Si :
T-L
-X
t=O
nw,t
kw(i)k2
T-L
kVv(i) Ltk22 + Ci X
vt
t=O
nw,t
kv(i)k2
kVv(i) Ft k22 .
vt
Note that
ηw,t
kw(i)k2
is non-increasing, so
T-L
-X	nW,,t	kV (i)Ltk2 ≤ -
t=0 kw(i)k2	vt
nw,τ
T-L
kw
∕iη^ E kvv(i) Ltk 2.
T k2 t=O
(22)
Also note that kwt(i)k22 ≥ kwO(i)k22 + Ptτ-=LO nW2,τ kVw(i)Ftk22. By Lemma B.1,
T-L n2	T-L
CiX ɪ"Ftk2 ≤ CiX
t=O kwt k2	t=O
nW2 ,tkVw(i)Ftk22
kw0i)k2 + PT=O nW ,τkVw(i)Ftk2
≤a" R)+1 +
kwO k22
2nW2 (πLiviv)2
kw0i)k2
~ , .
O(log T).
(23)
We can get the bound for Si by combining equation 22 and equation 23.
Now we bound R. Recall that
T-L	T-L
R := - 2 Cg X ng,tkVgtLtk2 + U X ng,tGg.
t=O	t=O
The first term can be bounded by -Ω(T-1/2) PT-31 kVgtLtk2 by noticing that %,t ≥ ng,τ =
Ω(T-1∕2). The second term can be bounded by O(logT) since PT-O ng t = PT-ɔ1 O(1∕t)=
-τ	_
O(log T).	□
17
Published as a conference paper at ICLR 2019
Proof for Theorem 4.2. Combining Lemma B.2 and Lemma B.4, for 0 ≤ α < 1/2, we have
T-1
Lmin -Lo ≤ E[Lt] -Lo ≤ -Ω(T-1/2) X E [∣∣VL(Vt;矶)|同 + O(logT).
t=0
Thus,
0m<TE [∣∣VL(%;gt)k2i ≤ TX E [∣∣VL(%;gt)k2i ≤ O (号).
Similarly, for α = 1/2, we have
T-1
Lmin -Lo ≤ E[Lt] -Lo ≤ -Ω((TlogT)-1/2) X E [∣∣VL(Vt;gt)k2] + O(logT).
t=0
Thus,
0mt<TE hkVL(¼; gt)k2i ≤ T £E hkVL(½; gt)k2i ≤ O ((lθg√T3/2
□
C Proof for the smoothness of the motivating neural network
In this section we prove that the modified version of the motivating neural network does meet the
assumptions in Section 2.4. More specifically, we assume:
•	We use the network structure Φ in Section 2.1 with the smoothed variant of BN as described
in Section 2.4;
•	The objective fy (∙) is twice continuously differentiable, lower bounded by 九由 and Lips-
chitz(∣fy(y)∣ ≤ αf);
•	The activation σ(∙) is twice continuously differentiable and Lipschitz (|fy(y)| ≤ ασ);
•	We add an extra weight decay (L2 regularization) term 2 ∣∣gk2 to the loss in equation 2 for
some λ > 0.
First, we show that gt (containing all scale and shift parameters in BN) is bounded during the training
process. Then the smoothness follows compactness using Extreme Value Theorem.
We use the following lemma to calculate back propagation:
Lemma C.1. Let x1, . . . , xB be a set of vectors. Let u := Eb∈[B] [xb] and S := Varb∈[B] (xb). Let
zb :
w> (xb - u)
Y ∣wkS+eI
+ β.
Let y := f(z1, . . . , zB) = f(z) for some function f. If ∣Vf (z)∣2 ≤ G, then
dy ≤ G√B	dy ≤ G√B	∣Vχby∣2 ≤ 3γG√B.
∂γ	∂β	b
Proof. Let X ∈ RB be the vector where Xb := (w>(xb 一u))∕∣w∣s+∈ι. Itis easy to see ∣Xk2 ≤ B.
Then
∂y
∂γ
∂y
∂β
IVf(z)>X∣ ≤ Gkxk2 ≤ G√Β.
≤ kVf(z)k1 ≤ G√B.
18
Published as a conference paper at ICLR 2019
For xb , we have
(一 B	B B B	2 2
B X(x>W)2 - (B X x>W)
b0=1	b0=1
Iww>(Xb- u).
B
Then
(1b=b0 — 1/B)kwks+eiW — WT(Xb - U) ∙ kwkS ∙ BBww>(Xb — U)
VxbzbO=Y-----------------------‰----------------------------
Since (w> (xb - u))2 ≤ Bkwk
l∣Vχbzb0k2 ≤ Y
2S+I,
1
(Ii- 1/B)kWk2+∏b ∙ 2kw0 ≤ 3γ
Thus,
kVxby∣2 ≤
lwlS+I
∖ 1/2
l∣Vχbzb0k2 )	≤ -^-G√B3.
Lemma C.2. If lg0 l2is bounded by a constant, there exists some constant K such that lgtl2 ≤ K.
Proof. Fix a time t in the training process. Consider the process of back propagation. Define
B	mi
Ri=Xb=1kX=1
∂Xg Fz(θ)!
□
where x(bi,)k is the output of the k-th neuron in the i-th layer in the b-th data sample in the batch. By
the Lipschitzness of the objective, RL can be bounded by a constant. If Ri can be bounded by a
constant, then by the Lipschitzness of σ and Lemma C.1, the gradient of Y and β in layer i can also
be bounded by a constant. Note that
gt+1,k
∂
gt,k - ηg,t Hg=卜 Fz (θt) - λ∏g,tgt,k
Thus Y and β in layer i can be bounded by a constant since
1	∂
∣gt+1,k∣ ≤ (1 - ηg,tλXgt,k + ηg,tλ ∙ λ I ∂gt k FZ (θt) .
Also Lemma C.1 and the Lipschitzness of σ imply that Ri-1 can be bounded if Ri and Y in the
layer i can be bounded by a constant. Using a simple induction, we can prove the existence of K for
bounding the norm of ∣gt∣2 for all time t.	□
Theorem C.3. If lg0l2 is bounded by a constant, then Φ satisfies the assumptions in Section 2.4.
Proof. Let C be the set of parameters θ satisfying lgl ≤ K and lW(i) l2 = 1 for all 1 ≤ i ≤ m.
By Lemma C.2, C contains the set of θ associated with the points lying between each pair ofθt and
θt+1 (including the endpoints).
It is easy to show that Fz (θ) is twice continously differentiable. Since C is compact, by the Ex-
treme Value Theorem, there must exist such constants Livjv , Livg , Lgg , Gg for upper bounding the
smoothness and the difference of gradients.
19
Published as a conference paper at ICLR 2019
D Experiments
In this section, we provide experimental evidence showing that the auto rate-tuning behavior does
empower BN in the optimization aspect.
We trained a modified version of VGGNet (Simonyan & Zisserman, 2014) on Tensorflow. This
network has 2 × conv64, pooling, 3 × conv128, pooling, 3 × conv256, pooling, 3 × conv512,
pooling, 3 × conv512, pooling, fc512, fc10 layers in order. Each convolutional layer has kernel
size 3 × 3 and stride 1. ReLU is used as the activation function after each convolutional or fully-
connected layer. We add a BN layer right before each ReLU. We set = 0 in each BN, since we
observed that the network works equally well for being 0 or an small number (such as 10-3, the
default value in Tensorflow). We initialize the parameters according to the default configuration in
Tensorflow: all the weights are initialized by Glorot uniform initializer (Glorot & Bengio, 2010); β
and γ in BN are initialized by 0 and 1, respectively.
In this network, every kernel is scale-invariant, and for every BN layer except the last one, the
concatenation of all β and γ parameters in this BN is also scale-invariant. Only β and γ parameters
in the last BN are scale-variant (See Section 2.1). We consider the training in following two settings:
1.	Train the network using the standard SGD (No momentum, learning rate decay, weight
decay and dropout);
2.	Train the network using Projected SGD (PSGD): at each iteration, one first takes a step
proportional to the negative of the gradient calculated in a random batch, and then projects
each scale-invariant parameter to the sphere with radius equal to its 2-norm before this iter-
ation, i.e., rescales each scale-invariant parameter so that each maintains its length during
training.
Note that the projection in Setting 2 removes the adaptivity of the learning rates in the corresponding
intrinsic optimization problem, i.e., Gt(i) in equation 9 remains constant during the training. Thus,
by comparing Setting 1 and Setting 2, we can know whether or not the auto-tuning behavior of BN
shown in theory is effective in practice.
SSo- 6uc'≡hJ
epochs
----SGDj separate,	Ir=O.Ol ........ PSGD,	separate,	Ir=O.01
----SGDj separate,	Ir=O. ɪ ...... PSGD, separate, Ir=O. 1
----SGD, separate,	Ir=I ........... PSGD,	separate,	Ir=I
----SGD, separate,	Ir=IO .......... PSGD,	separate,	Ir=IO
----SGD, separate,	Ir=IOO ......... PSGD,	separate,	Ir=IOO
Figure 1: The relationship between the training loss and the learning rate for scale-invariant param-
eters, with learning rate for scale-variant ones set to 0.1. Left: The average training loss of the
last 5 epochs (averaged across 10 experiments). In rare cases, the training loss becomes NaN in the
experiments for the yellow curve (SGD, separate) with learning rate larger than 10. Right: The
average training loss of each epoch (each curve stands for a single experiment).
20
Published as a conference paper at ICLR 2019
P5GDj ∣Γ=0.01
PSGDj Ir=O.1
PSGDj Ir=I
PSGDj Ir=IO
PSGDj Ir=IOO
SGD1 BN removed, lr-0.01
SGD, BN removed, Ir=O.1
SGD, BN removed, Ir=I
SGD, BN removed, Ir=IO
5GD, BN removed, Ir=IOO
SGDj ∣Γ=0.01
----SGDj Ir=O.1
----SG□j Ir=I
——SG□j Ir=IO
----SG□j Ir=IOO
Figure 2: The relationship between the training loss and the learning rate. For learning rate larger
than 10, the training loss of PSGD or SGD with BN removed is always either very large or NaN,
and thus not invisible in the figure. Left: The average training loss of the last 5 epochs (averaged
across 10 experiments). In rare cases, the training loss becomes NaN in the experiments for the
green curve (SGD, BN removed) with learning rate larger than 10-0.7. We removed such data
when taking the average. Right: The average training loss of each epoch (each curve stands for a
single experiment).
■■■ PSGDj ∣Γ=0.01
P5GDj Ir=O. 1
..PSGDJr=I
PSGD, Ir=IO
—PSGD, Ir= 1∞
Figure 3: The relationship between the test accuracy and the learning rate. Left: The average test
accuracy of the last 5 epochs (averaged across 10 experiments). Right: The test accuracy after each
epoch (each curve stands for a single experiment). Due to the implementation of Tensorflow, output-
ing NaN leads to a test accuracy of 10%. Note that the magenta dotted curve (PSGD, lr=100),
red dashed curve (SGD, BN removed, lr=1) and cyan dashed curve (SGD, BN removed,
lr=10) are covered by the magenta dashed curve (SGD, BN removed, lr=100). They all
have 10% test accuracy.
—SGD1 separate —PSGD, separate
-∙- SGD	— PSGD
40	60
epochs
∞	100
----SGDj Iγ=0.01
----SGDj Iγ=0.1
----SGDj Ir=I
----SGD. Ir=IO
----SGDj Ir=I(H)
---SGD, BN removed, lr=0.01
----SGD1 BN removed, lr≡0.1
---SGD, BN removed, lr≡l
---SGD, BN removed, Ir=IO
---SGD, BN removed, Ir= 1∞
21
Published as a conference paper at ICLR 2019
D.1 Separate Learning Rates
As in our theoretical analysis, we consider what will happen if we set two learning rates separately
for scale-invariant and scale-variant parameters. We train the network in either setting with different
learning rates ranging from 10-2 to 102 for 100 epochs.
First, we fix the learning rate for scale-variant ones to 0.1, and try different learning rates for scale-
invariant ones. As shown in Figure 1, for small learning rates (such as 0.1), the training processes
of networks in Setting 1 and 2 are very similar. But for larger learning rates, networks in Setting 1
can still converge to 0 for all the learning rates we tried, while networks in Setting 2 got stuck with
relatively large training loss. This suggests that the auto-tuning behavior of BN does takes effect
when the learning rate is large, and it matches with the claimed effect of BN in (Ioffe & Szegedy,
2015) that BN enables us to use a higher learning rate. Though our theoretical analysis cannot
be directly applied to the network we trained due to the non-smoothness of the loss function, the
experiment results match with what we expect in our analysis.
D.2 Unified Learning Rate
Next, we consider the case in which we train the network with a unified learning rate for both scale-
invariant and scale-variant parameters. We also compare Setting 1 and 2 with the setting in which
we train the network with all the BN layers removed using SGD (we call it Setting 3).
As shown in Figure 2, the training loss of networks in Setting 1 converges to 0. On the contrast, the
training loss of networks in Setting 2 and 3 fails to converge to 0 when a large learning rate is used,
and in some cases the loss diverges to infinity or NaN. This suggests that the auto-tuning behavior
of BN has an effective role in the case that a unified learning rate is set for all parameters.
For a fair comparison, we also trained neural networks in Setting 3 with initialization essentially
equivalent to the ones with BN. This is done in the same way as (Krahenbuhl et al., 2015; Mishkin
& Matas, 2015) and Section 3 of (Salimans & Kingma, 2016): we first randomly initialize the
parameters, then feed the first batch into the network and adjust the scaling and bias of each neuron
to make its outputs have zero mean and unit variance. In this way, the loss of the networks converges
to 0 when the learning rate is smaller than 10-2.0, but for a slightly larger learning rate such as
10-1.8 , the loss fails to converge to 0, and sometimes even diverges to infinity or NaN. Compared
with experimental results in Setting 1, this suggests that the robustness of training brought by BN is
independent of the fact that BN changes the effective initialization of parameters.
D.3 Generalization
Despite in Setting 1 the convergence of training loss for different learning rates, the convergence
points can be different, which lead to different performances on test data.
In Figure 3, we plot the test accuracy of networks trained in Setting 1 and 2 using different unified
learning rates, or separate learning rates with the learning rate for scale-variant parameters fixed to
0.1. As shown in the Figure 3, the test accuracy of networks in Setting 2 decreases as the learning
rate increases over 0.1, while the test accuracy of networks in Setting 1 remains higher than 75%.
The main reason that the network in Setting 2 doesn’t perform well is underfitting, i.e. the network
in Setting 2 fails to fit the training data well when learning rate is large. This suggests that the auto-
tuning behavior of BN also benefits generalization since such behavior allows the algorithm to pick
learning rates from a wider range while still converging to small test error.
22