Published as a conference paper at ICLR 2019
Stable Recurrent Models
John Miller & Moritz Hardt
University of California, Berkeley
{miller _john,hardt}@berkeley.edu
Ab stract
Stability is a fundamental property of dynamical systems, yet to this date it has had
little bearing on the practice of recurrent neural networks. In this work, we con-
duct a thorough investigation of stable recurrent models. Theoretically, we prove
stable recurrent neural networks are well approximated by feed-forward networks
for the purpose of both inference and training by gradient descent. Empirically,
we demonstrate stable recurrent models often perform as well as their unstable
counterparts on benchmark sequence tasks. Taken together, these findings shed
light on the effective power of recurrent networks and suggest much of sequence
learning happens, or can be made to happen, in the stable regime. Moreover,
our results help to explain why in many cases practitioners succeed in replacing
recurrent models by feed-forward models.
1	Introduction
Recurrent neural networks are a popular modeling choice for solving sequence learning problems
arising in domains such as speech recognition and natural language processing. At the outset, recur-
rent neural networks are non-linear dynamical systems commonly trained to fit sequence data via
some variant of gradient descent.
Stability is of fundamental importance in the study of dynamical system. Surprisingly, however,
stability has had little impact on the practice of recurrent neural networks. Recurrent models trained
in practice do not satisfy stability in an obvious manner, suggesting that perhaps training happens in
a chaotic regime. The difficulty of training recurrent models has compelled practitioners to success-
fully replace recurrent models with non-recurrent, feed-forward architectures.
This state of affairs raises important unresolved questions. Is sequence modeling in practice inher-
ently unstable? When and why are recurrent models really needed?
In this work, we shed light on both of these questions through a theoretical and empirical investiga-
tion of stability in recurrent models.
We first prove stable recurrent models can be approximated by feed-forward networks. In particular,
not only are the models equivalent for inference, they are also equivalent for training via gradient
descent. While it is easy to contrive non-linear recurrent models that on some input sequence cannot
be approximated by feed-forward models, our result implies such models are inevitably unstable.
This means in particular they must have exploding gradients, which is in general an impediment to
learnibility via gradient descent.
Second, across a variety of different sequence tasks, we show how recurrent models can often be
made stable without loss in performance. We also show models that are nominally unstable often op-
erate in the stable regime on the data distribution. Combined with our first result, these observation
helps to explain why an increasingly large body of empirical research succeeds in replacing recurrent
models with feed-forward models in important applications, including translation (Vaswani et al.,
2017; Gehring et al., 2017), speech synthesis (Van Den Oord et al., 2016), and language model-
ing (Dauphin et al., 2017). While stability does not always hold in practice to begin with, it is often
possible to generate a high-performing stable model by imposing stability during training.
Our results also shed light on the effective representational properties of recurrent networks trained
in practice. In particular, stable models cannot have long-term memory. Therefore, when stable and
1
Published as a conference paper at ICLR 2019
unstable models achieve similar results, either the task does not require long-term memory, or the
unstable model does not have it.
1.1	Contributions
In this work, we make the following contributions.
1.	We present a generic definition of stable recurrent models in terms of non-linear dynamical
systems and show how to ensure stability of several commonly used models. Previous
work establishes stability for vanilla recurrent neural networks. We give new sufficient
conditions for stability of long short-term memory (LSTM) networks. These sufficient
conditions come with an efficient projection operator that can be used at training time to
enforce stability.
2.	We prove, under the stability assumption, feed-forward networks can approximate recurrent
networks for purposes of both inference and training by gradient descent. While simple in
the case of inference, the training result relies on non-trivial stability properties of gradient
descent.
3.	We conduct extensive experimentation on a variety of sequence benchmarks, show stable
models often have comparable performance with their unstable counterparts, and discuss
when, if ever, there is an intrinsic performance price to using stable models.
2	Stable Recurrent Models
In this section, we define stable recurrent models and illustrate the concept for various popular
model classes. From a pragmatic perspective, stability roughly corresponds to the criterion that
the gradients of the training objective do not explode over time. Common recurrent models can
operate in both the stable and unstable regimes, depending on their parameters. To study stable
variants of common architectures, we give sufficient conditions to ensure stability and describe how
to efficiently enforce these conditions during training.
2.1	Defining Stable Recurrent Models
A recurrent model is a non-linear dynamical system given by a differentiable state-transition map
φw : Rn × Rd → Rn , parameterized by w ∈ Rm. The hidden state ht ∈ Rn evolves in discrete time
steps according to the update rule
ht = φw (ht-1, xt) ,	(1)
where the vector xt ∈ Rd is an arbitrary input provided to the system at time t. This general for-
mulation allows us to unify many examples of interest. For instance, for a recurrent neural network,
given weight matrices W and U, the state evolves according to
ht = φW,U(ht-1,xt) = tanh (W ht-1 + Uxt) .
Recurrent models are typically trained using some variant of gradient descent. One natural—even if
not strictly necessary—requirement for gradient descent to work is that the gradients of the training
objective do not explode over time. Stable recurrent models are precisely the class of models where
the gradients cannot explode. They thus constitute a natural class of models where gradient descent
can be expected to work. In general, we define a stable recurrent model as follows.
Definition 1. A recurrent model φw is stable if there exists some λ < 1 such that, for any weights
w ∈ Rm, states h, h0 ∈ Rn, and input x ∈ Rd,
kφw(h,x) - φw(h0,x)k ≤ λ kh- h0k .	(2)
Equivalently, a recurrent model is stable if the map φw is λ-contractive in h. If φw is λ-stable, then
∣∣Vhφw(h,χ)k < λ, and for Lipschitz loss p, kVw Pk is always bounded (Pascanu et al., 2013).
Stable models are particularly well-behaved and well-justified from a theoretical perspective. For
instance, at present, only stable linear dynamical systems are known to be learnable via gradient
2
Published as a conference paper at ICLR 2019
descent (Hardt et al., 2018). In unstable models, the gradients of the objective can explode, and it is
a delicate matter to even show that gradient descent converges to a stationary point. The following
proposition offers one such example. The proof is provided in the appendix.
Proposition 1. There exists an unstable system φw where gradient descent does not converge to a
stationary point, and ∣∣VwPk → ∞ as the number ofiterations N → ∞.
2.2	Examples of Stable Recurrent Models
In this section, we provide sufficient conditions to ensure stability for several common recurrent
models. These conditions offer a way to require learning happens in the stable regime- after each
iteration of gradient descent, one imposes the corresponding stability condition via projection.
Linear dynamical systems and recurrent neural networks. Given a Lipschitz, point-wise non-
linearity ρ and matrices W ∈ Rn×n and U ∈ Rn×d, the state-transition map for a recurrent neural
network (RNN) is
ht = ρ(W ht-1 + Uxt).
If ρ is the identity, then the system is a linear dynamical system. Jin et al. (1994) show if ρ is
LP-Lipschitz, then the model is stable provided ∣∣ W∣∣ < L-. Indeed, for any states h, h0, and any x,
kρ(Wh+Ux) -ρ(Wh0+Ux)k ≤ Lρ kWh+Ux - Wh0 - Uxk ≤ Lρ kWk kh- h0k .
In the case of a linear dynamical system, the model is stable provided ∣W∣ < 1. Similarly, for
the 1-Lipschitz tanh-nonlinearity, stability obtains provided ∣W∣ < 1. In the appendix, we verify
the assumptions required by the theorems given in the next section for this example. Imposing this
condition during training corresponds to projecting onto the spectral norm ball.
Long short-term memory networks. Long Short-Term Memory (LSTM) networks are another
commonly used class of sequence models (Hochreiter & Schmidhuber, 1997). The state is a pair
of vectors s = (c, h) ∈ R2d, and the model is parameterized by eight matrices, W ∈ Rd×d and
U ∈ Rd×n, for ∈ {i, f, o, z}. The state-transition map φLSTM is given by
ft = σ(Wfht-1+ Ufxt)
it = σ(Wiht-1 + Uixt)
ot = σ(Woht-1 + Uoxt)
zt = tanh(Wz ht-1 + Uzxt )
ct = it ◦ zt + ft ◦ ct-1
ht = ot ∙ tanh(Ct),
where ◦ denotes elementwise multiplication, and σ is the logistic function.
We provide conditions under which the iterated system ΦLstm = Φlstm ◦ ∙∙∙ ◦ Φlstm is stable.
Let ∣f ∣∞ = supt ∣ft ∣∞. If the weights Wf , Uf and inputs xt are bounded, then ∣f ∣∞ < 1
since ∣σ∣ < 1 for any finite input. This means the next state Ct must “forget” a non-trivial portion
of ct-1. We leverage this phenomenon to give sufficient conditions for φLSTM to be contractive
in the '∞ norm, which in turn implies the iterated system ΦLstm is contractive in the '2 norm for
r = O(log(d)). LetkW∣∣∞ denote the induced '∞ matrix norm, which corresponds to the maximum
absolute row sum maxi Pj |Wij |.
Proposition 2. If ∣Wi∣∞ , ∣Wo∣∞	< (1 - ∣f∣∞),	∣Wz ∣∞ ≤	(1/4)(1 - ∣f∣∞), ∣Wf ∣∞ <
(1 - ∣f∣∞)2, and r = O(log(d)), then the iterated system φrLSTM is stable.
The proof is given in the appendix. The conditions given in Proposition 2 are fairly restrictive.
Somewhat surprisingly we show in the experiments models satisfying these stability conditions still
achieve good performance on a number of tasks. We leave it as an open problem to find different pa-
rameter regimes where the system is stable, as well as resolve whether the original system φLSTM is
stable. Imposing these conditions during training and corresponds to simple row-wise normalization
of the weight matrices and inputs. More details are provided in Section 4 and the appendix.
3
Published as a conference paper at ICLR 2019
3	Stable Recurrent Models Have Feed-forward Approximations
In this section, we prove stable recurrent models can be well-approximated by feed-forward net-
works for the purposes of both inference and training by gradient descent. From a memory perspec-
tive, stable recurrent models are equivalent to feed-forward networks—both models use the same
amount of context to make predictions. This equivalence has important consequences for sequence
modeling in practice. When a stable recurrent model achieves satisfactory performance on some
task, a feed-forward network can achieve similar performance. Consequently, if sequence learn-
ing in practice is inherently stable, then recurrent models may not be necessary. Conversely, if
feed-forward models cannot match the performance of recurrent models, then sequence learning in
practice is in the unstable regime.
3.1	Truncated recurrent models
For our purposes, the salient distinction between a recurrent and feed-forward model is the latter has
finite-context. Therefore, we say a model is feed-forward if the prediction made by the model at step
t is a function only of the inputs xt-k, . . . , xt for some finite k.
While there are many choices for a feed-forward approximation, we consider the simplest one—
truncation of the system to some finite context k. In other words, the feed-forward approximation
moves over the input sequence with a sliding window of length k producing an output every time the
sliding window advances by one step. Formally, for context length k chosen in advance, we define
the truncated model via the update rule
htk =φw(htk-1,xt),	htk-k =0.	(3)
Note that htk is a function only of the previous k inputs xt-k , . . . , xt . While this definition is
perhaps an abuse of the term “feed-forward”, the truncated model can be implemented as a standard
autoregressive, depth-k feed-forward network, albeit with significant weight sharing.
Let f denote a prediction function that maps a state ht to outputs f (ht) = yt. Let ytk denote the
predictions from the truncated model. To simplify the presentation, the prediction function f is not
parameterized. This is without loss of generality because it is always possible to fold the parameters
into the system φw itself. In the sequel, we study yt - ytk both during and after training.
3.2	Approximation during inference
Suppose we train a full recurrent model φw and obtain a prediction yt . For an appropriate choice of
context k, the truncated model makes essentially the same prediction ytk as the full recurrent model.
To show this result, we first control the difference between the hidden states of both models.
Lemma 1. Assume φw is λ-contractive in h and Lx-Lipschitz in x. Assume the input sequence
IlxtIl ≤ Bx for all t. Ifthe truncation length k ≥ logι∕λ ((L-Bxε)，then the difference in hidden
states ht - htk ≤ ε.
Lemma 1 effectively Says stable models do not have long-term memory- distant inputs do not change
the states of the system. A proof is given in the appendix. If the prediction function is Lipschitz,
Lemma 1 immediately implies the recurrent and truncated model make nearly identical predictions.
Proposition 3. If φw is a Lx-Lipschitz and λ-contractive map, and f is Lf Lipschitz, and the
truncation length k ≥ logι∕λ (Lf-XBx) ,then Ilyt 一 yk∣∣ ≤ ε.
3.3	Approximation during training via gradient descent
Equipped with our inference result, we turn towards optimization. We show gradient descent for
stable recurrent models finds essentially the same solutions as gradient descent for truncated models.
Consequently, both the recurrent and truncated models found by gradient descent make essentially
the same predictions.
Our proof technique is to initialize both the recurrent and truncated models at the same point and
track the divergence in weights throughout the course of gradient descent. Roughly, we show if
4
Published as a conference paper at ICLR 2019
k ≈ O(log(N∕ε)), then after N steps of gradient descent, the difference in the weights between
the recurrent and truncated models is at most ε. Even if the gradients are similar for both models
at the same point, it is a priori possible that slight differences in the gradients accumulate over time
and lead to divergent weights where no meaningful comparison is possible. Building on similar
techniques as Hardt et al. (2016), we show that gradient descent itself is stable, and this type of
divergence cannot occur.
Our gradient descent result requires two essential lemmas. The first bounds the difference in gradient
between the full and the truncated model. The second establishes the gradient map of both the full
and truncated models is Lipschitz. We defer proofs of both lemmas to the appendix.
Let pT denote the loss function evaluated on recurrent model after T time steps, and define pkT
similarly for the truncated model. Assume there some compact, convex domain Θ ⊂ Rm so that the
map φw is stable for all choices of parameters w ∈ Θ.
Lemma 2. Assume p (and therefore pk) is Lipschitz and smooth. Assume φw is smooth, λ-
contractive, and Lipschitz in x and w. Assume the inputs satisfy kxt k ≤ Bx, then
∣∣VwPτ -NwPTlI= γkλk,
where γ = O Bx (1 - λ)-2 , suppressing dependence on the Lipschitz and smoothness parame-
ters.
Lemma 3. For any w, w0 ∈ Θ, suppose φw is smooth, λ-contractive, and Lipschitz in w. If p is
Lipschitz and smooth, then
kNwpT (w) - NwpT (w0)k ≤ β kw - w0k ,
where β = O (1 - λ)-3 , suppressing dependence on the Lipschitz and smoothness parameters.
Let wriecurr be the weights of the recurrent model on step i and define wtirunc similarly for the trun-
cated model. At initialization, wr0ecurr = wt0runc . For k sufficiently large, Lemma 2 guarantees the
difference between the gradient of the recurrent and truncated models is negligible. Therefore, after
a gradient update, lwr1ecurr - wt1runc l is small. Lemma 3 then guarantees that this small difference
in weights does not lead to large differences in the gradient on the subsequent time step. For an
appropriate choice of learning rate, formalizing this argument leads to the following proposition.
Proposition 4. Under the assumptions of Lemmas 2 and 3, for compact, convex Θ, after N steps of
projected gradient descent with step size at = α∕t, ∣∣wNcurr — WNunc ∣∣ ≤ αγkλkNαβ+1.
The decaying step size in our theorem is consistent with the regime in which gradient descent is
known to be stable for non-convex training objectives (Hardt et al., 2016). While the decay is faster
than many learning rates encountered in practice, classical results nonetheless show that with this
learning rate gradient descent still converges to a stationary point; see p. 119 in Bertsekas (1999)
and references there. In the appendix, We give empirical evidence the O(1∕t) rate is necessary for
our theorem and show examples of stable systems trained with constant or O(1∕√t) rates that do
not satisfy our bound.
Critically, the bound in Proposition 4 goes to 0 as k → ∞. In particular, if we take α = 1 and
k ≥ Ω(log(γNβ/ε)), then after N steps of projected gradient descent, ∣∣wNcurr — WNunc ∣∣ ≤ ε.
For this choice of k, we obtain the main theorem. The proof is left to the appendix.
Theorem 1. Let p be Lipschitz and smooth. Assume φw is smooth, λ-contractive, Lipschitz in
x and W. Assume the inputs are bounded, and the prediction function f is Lf -Lipschitz. If
k ≥ Ω(log(γNβ/ε)), then after N steps of projected gradient descent with SteP size a = '/t,
llyT — yTk ll ≤ ε.
4	Experiments
In the experiments, we show stable recurrent models can achieve solid performance on several
benchmark sequence tasks. Namely, we show unstable recurrent models can often be made stable
without a loss in performance. In some cases, there is a small gap between the performance between
unstable and stable models. We analyze whether this gap is indicative of a “price of stability” and
show the unstable models involved are stable in a data-dependent sense.
5
Published as a conference paper at ICLR 2019
4.1	TASKS
We consider four benchmark sequence Problems-Word-level language modeling, character-level
language modeling, polyphonic music modeling, and slot-filling.
Language modeling. In language modeling, given a sequence of Words or characters, the model
must Predict the next Word or character. For character-level language modeling, We train and evalu-
ate models on Penn Treebank (Marcus et al., 1993). To increase the coverage of our exPeriments, We
train and evaluate the Word-level language models on the Wikitext-2 dataset, Which is tWice as large
as Penn Treebank and features a larger vocabulary (Merity et al., 2017). Performance is rePorted
using bits-Per-character for character-level models and PerPlexity for Word-level models.
Polyphonic music modeling. In PolyPhonic music modeling, a Piece is rePresented as a sequence
of 88-bit binary codes corresPonding to the 88 keys on a Piano, With a 1 indicating a key that is
Pressed at a given time. Given a sequence of codes, the task is to Predict the next code. We evaluate
our models on JSB Chorales, a PolyPhonic music dataset consisting of 382 harmonized chorales by
J.S. Bach (Allan & Williams, 2005). Performance is measured using negative log-likelihood.
Slot-filling. In slot filling, the model takes as inPut a query like “I Want to Boston on Monday”
and outputs a class label for each word in the input, e.g. Boston maps to Departure-City
and Monday maps to Departure-Time. We use the Airline Travel Information Systems (ATIS)
benchmark and report the F1 score for each model (Price, 1990).
4.2	Comparing Stable and Unstable Models
For each task, we first train an unconstrained RNN and an unconstrained LSTM. All the hyperpa-
rameters are chosen via grid-search to maximize the performance of the unconstrained model. For
consistency with our theoretical results in Section 3 and stability conditions in Section 2.2, both
models have a single recurrent layer and are trained using plain SGD. In each case, the resulting
model is unstable. However, we then retrain the best models using projected gradient descent to
enforce stability without retuning the hyperparameters. In the RNN case, we constrain kW k < 1.
After each gradient update, we project the W onto the spectral norm ball by computing the SVD
and thresholding the singular values to lie in [0, 1). In the LSTM case, after each gradient update,
we normalize each row of the weight matrices to satisfy the sufficient conditions for stability given
in Section 2.2. Further details are given in the appendix.
Stable and unstable models achieve similar performance. Table 1 gives a comparison of the
performance between stable and unstable RNNs and LSTMs on each of the different tasks. Each of
the reported metrics is computed on the held-out test set. We also show a representative comparison
of learning curves for word-level language modeling and polyphonic music modeling in Figures 1(a)
and 1(b).
(a) Word-level language modeling
Figure 1: Stable and unstable variants of common recurrent architectures achieve similar perfor-
mance across a range of different sequence tasks.
(b) Polyphonic music modeling
6
Published as a conference paper at ICLR 2019
Table 1: Comparison of stable and unstable models on a variety of sequence modeling tasks. For
all the tasks, stable and unstable RNNs achieve the same performance. For polyphonic music and
slot-filling, stable and unstable LSTMs achieve the same results. On language modeling, there is
a small gap between stable and unstable LSTMs. We discuss this in Section 4.3. Performance is
evaluated on the held-out test set. For negative log-likelihood (nll), bits per character (bpc), and
perplexity, lower is better. For F1 score, higher is better.
Model
RNN	LSTM
Sequence Task	Dataset (measure)	Unstable	Stable	Unstable	Stable
Polyphonic Music	JSB Chorales (nll)	8.9	8.9	8.5	8.5
Slot-Filling	Atis (F1 score)	94.7	94.7	95.1	94.6
Word-level LM	Wikitext-2 (perplexity)	146.7	143.5	95.7	113.2
Character-level LM	Penn Treebank (bpc)	1.8	1.9	1.4	1.9
Across all the tasks we considered, stable and unstable RNNs have roughly the same performance.
Stable RNNs and LSTMs achieve results comparable to published baselines on slot-filling (Mesnil
et al., 2015) and polyphonic music modeling (Bai et al., 2018). On word and character level language
modeling, both stable and unstable RNNs achieve comparable results to (Bai et al., 2018).
On the language modeling tasks, however, there is a gap between stable and unstable LSTM models.
Given the restrictive conditions we place on the LSTM to ensure stability, it is surprising they work
as well as they do. Weaker conditions ensuring stability of the LSTM could reduce this gap. It is
also possible imposing stability comes at a cost in representational capacity required for some tasks.
4.3	What is the “price of stability” in sequence modeling?
The gap between stable and unstable LSTMs on language modeling raises the question of whether
there is an intrinsic performance cost for using stable models on some tasks. If we measure stability
in a data-dependent fashion, then the unstable LSTM language models are stable, indicating this gap
is illusory. However, in some cases with short sequences, instability can offer modeling benefits.
LSTM language models are stable in a “data-dependent” way. Our notion of stability is conser-
vative and requires stability to hold for every input and pair of hidden states. If we instead consider
a weaker, data-dependent notion of stability, the word and character-level LSTM models are stable
(in the iterated sense of Proposition 2). In particular, we compute the stability parameter only using
input sequences from the data. Furthermore, we only evaluate stability on hidden states reachable
via gradient descent. More precisely, to estimate λ, we run gradient ascent to find worst-case hidden
states h, h0 to maximize kφw(h：h_”("，X)k. More details are provided in the appendix.
The data-dependent definition given above is a useful diagnostic— when the sufficient stability
conditions fail to hold, the data-dependent condition addresses whether the model is still operating
in the stable regime. Moreover, when the input representation is fixed during training, our theoretical
results go through without modification when using the data-dependent definition.
Using the data-dependent measure, in Figure 2(a), we show the iterated character-level LSTM,
φrLSTM, is stable for r ≈ 80 iterations. A similar result holds for the word-level language model for
r ≈ 100. These findings are consistent with experiments in Laurent & von Brecht (2017) which find
LSTM trajectories converge after approximately 70 steps only when evaluated on sequences from the
data. For language models, the “price of stability” is therefore much smaller than the gap in Table 1
suggests- even the “unstable" models are operating in the stable regime on the data distribution.
Unstable systems can offer performance improvements for short-time horizons. When se-
quences are short, training unstable models is less difficult because exploding gradients are less of
an issue. In these case, unstable models can offer performance gains. To demonstrate this, we train
truncated unstable models on the polyphonic music task for various values of the truncation parame-
ter k. In Figure 2(b), we simultaneously plot the performance of the unstable model and the stability
7
Published as a conference paper at ICLR 2019
parameter λ for the converged model for each k. For short-sequences, the final model is more unsta-
ble (λ ≈ 3.5) and achieves a better test-likelihood. For longer sequence lengths, λ decreases closer
to the stable regime (λ ≈ 1.5), and this improved test-likelihood performance disappears.
Aa三qe∞au①PU①d①α,se
(a) Data-dependent stability of character-level lan-
guage models. The iterated-LSTM refers to the it-
eration system ΦLstm = Φlstm ◦•••◦ Φlstm.
3.5
Stability Constant vs. Truncation Length
(Polyphonic Music)
20	40	60	80
Truncation Length
EPqEE-I
100 0.0
(b) Unstable models can boost performance for
short sequences.
Figure 2: What is the intrinsic “price of stability”? For language modeling, we show the unstable
LSTMs are actually stable in weaker, data-dependent sense. On the other hand, for polyphonic
music modeling with short sequences, instability can improve model performance.
4.4 Unstable Models Operate in the Stable Regime
In the previous section, we showed nominally unstable models often satisfy a data-dependent notion
of stability. In this section, we offer further evidence unstable models are operating in the stable
regime. These results further help explain why stable and unstable models perform comparably in
experiments.
Vanishing gradients. Stable models necessarily have vanishing gradients, and indeed this ingre-
dient is a key ingredient in the proof of our training-time approximation result. For both word and
character-level language models, we find both unstable RNNs and LSTMs also exhibit vanishing
gradients. In Figures 3(a) and 3(b), we plot the average gradient of the loss at time t+i with respect
to the input at time t, kVxtpt+i k as t ranges over the training set. For either language modeling
task, the LSTM and the RNN suffer from limited sensitivity to distant inputs at initialization and
throughout training. The gradients of the LSTM vanish more slowly than those of the RNN, but
both models exhibit the same qualitative behavior.
Vanishing Gradients on Wikitext-2
2 10
Lu-ON IUSPe.ID
Ind①me」①>4
Distance i from input Xt to loss Pt+i
(a) Word-Level language modeling
E-ON IU--Pe.ID
Ind-①me」①>4
Vanishing Gradients on Penn TreeBank
----- Unstable LSTM, Epoch 0
----- Unstable RNN, Epoch 0
----- Unstable LSTM, Epoch 100
-----Unstable RNN, Epoch 100
0	20	40	60
Distance i from input Xt to loss Pt+i
(b) Character-level language modeling
Figure 3: Unstable word and character-level language models exhibit vanishing gradients. We plot
the norm of the gradient with respect to inputs, kVχtpt+ik, as the distance between the input and
the loss grows, averaged over the entire training set. The gradient vanishes for moderate values of i
for both RNNs and LSTMs, though the decay is slower for LSTMs.
8
Published as a conference paper at ICLR 2019
Truncating Unstable Models. The results in Section 3 show stable models can be truncated with-
out loss of performance. In practice, unstable models can also be truncated without performance
loss. In Figures 4(a) and 4(b), we show the performance of both LSTMs and RNNs for various
values of the truncation parameter k on word-level language modeling and polyphonic music mod-
eling. Initially, increasing k increases performance because the model can use more context to make
predictions. However, in both cases, there is diminishing returns to larger values of the truncation
parameter k. LSTMs are unaffected by longer truncation lengths, whereas the performance of RNNs
slightly degrades as k becomes very large, possibly due to training instability. In either case, dimin-
ishing returns to performance for large values of k means truncation and therefore feed-forward
approximation is possible even for these unstable models.
(a) Word-level language modeling
(b) Polyphonic music modeling
Figure 4: Effect of truncating unstable models. On both language and music modeling, RNNs and
LSTMs exhibit diminishing returns for large values of the truncation parameter k . In LSTMs, larger
k doesn’t affect performance, whereas for unstable RNNs, large k slightly decreases performance
Proposition (4) holds for unstable models. In stable models, Proposition (4) in Section 3 ensures
the distance between the weight matrices kwrecurr - wtrunc k grows slowly as training progresses,
and this rate decreases as k becomes large. In Figures 5(a) and 5(b), we show a similar result holds
empirically for unstable word-level language models. All the models are initialized at the same
point, and we track the distance between the hidden-to-hidden matrices W as training progresses.
Training the full recurrent model is impractical, and we assume k = 65 well captures the full-
recurrent model. In Figures 5(a) and 5(b), we plot kWk - W65 k for k ∈ {5, 10, 15, 25, 35, 50, 64}
throughout training. As suggested by Proposition (4), after an initial rapid increase in distance,
kWk - W65 k grows slowly, as suggested by Proposition 4. Moreover, there is a diminishing return
to choosing larger values of the truncation parameter k in terms of the accuracy of the approximation.
Difference in Recurrent Weight Matrices
k=5
k=10
k=15
k=25
k=35
k=50
k=64
(a) Unstable RNN language model
Difference in Recurrent Weight Matrices
k=5
k=10
k=15
k=25
k=35
k=50
k=64
(b) Unstable LSTM language model
Figure 5: Qualitative version of Proposition 4 for unstable, word-level language models. We assume
k = 65 well-captures the full-recurrent model and plot kwtrunc - wrecurr k = kWk - W65 k as
training proceeds, where W denotes the recurrent weights. As Proposition 4 suggests, this quantity
grows slowly as training proceeds, and the rate of growth decreases as k increases.
9
Published as a conference paper at ICLR 2019
5 Are recurrent models truly neces sary
Our experiments show recurrent models trained in practice operate in the stable regime, and our
theoretical results show stable recurrent models are approximable by feed-forward networks, As
a consequence, we conjecture recurrent networks trained in practice are always approximable by
feed-forward networks. Even with this conjecture, we cannot yet conclude recurrent models as com-
monly conceived are unnecessary. First, our present proof techniques rely on truncated versions of
recurrent models, and truncated recurrent architectures like LSTMs may provide useful inductive
bias on some problems. Moreover, implementing the truncated approximation as a feed-forward
network increases the number of weights by a factor of k over the original recurrent model. Declar-
ing recurrent models truly superfluous would require both finding more parsimonious feed-forward
approximations and proving natural feed-forward models, e.g. fully connected networks or CNNs,
can approximate stable recurrent models during training. This remains an important question for
future work.
6 Related Work
Learning dynamical systems with gradient descent has been a recent topic of interest in the machine
learning community. Hardt et al. (2018) show gradient descent can efficiently learn a class of stable,
linear dynamical systems, Oymak (2018) shows gradient descent learns a class of stable, non-linear
dynamical systems. Work by Sedghi & Anandkumar (2016) gives a moment-based approach for
learning some classes of stable non-linear recurrent neural networks. Our work explores the theo-
retical and empirical consequences of the stability assumption made in these works. In particular,
our empirical results show models trained in practice can be made closer to those currently being
analyzed theoretically without large performance penalties.
For linear dynamical systems, Tu et al. (2017) exploit the connection between stability and trunca-
tion to learn a truncated approximation to the full stable system. Their approximation result is the
same as our inference result for linear dynamical systems, and we extend this result to the non-linear
setting. We also analyze the impact of truncation on training with gradient descent. Our training time
analysis builds on the stability analysis of gradient descent in Hardt et al. (2016), but interestingly
uses it for an entirely different purpose. Results of this kind are completely new to our knowledge.
For RNNs, the link between vanishing and exploding gradients and kW k was identified in Pascanu
et al. (2013). For 1-layer RNNs, Jin et al. (1994) give sufficient conditions for stability in terms
of the norm kW k and the Lipschitz constant of the non-linearity. Our work additionally considers
LSTMs and provides new sufficient conditions for stability. Moreover, we study the consequences
of stability in terms of feed-forward approximation.
A number of recent works have sought to avoid vanishing and exploding gradients by ensuring
the system is an isometry, i.e. λ = 1. In the RNN case, this amounts to constraining kW k = 1
(Arjovsky et al., 2016; Wisdom et al., 2016; Jing et al., 2017; Mhammedi et al., 2017; Jose et al.,
2018). Vorontsov et al. (2017) observes strictly requiring kW k = 1 reduces performance on several
tasks, and instead proposes maintaining kW k ∈ [1 - ε, 1 + ε]. Zhang et al. (2018) maintains this
“soft-isometry” constraint using a parameterization based on the SVD that obviates the need for the
projection step used in our stable-RNN experiments. Kusupati et al. (2018) sidestep these issues and
stabilizes training using a residual parameterization of the model. At present, these unitary models
have not yet seen widespread use, and our work shows much of the sequence learning in practice,
even with nominally unstable models, actually occurs in the stable regime.
From an empirical perspective, Laurent & von Brecht (2017) introduce a non-chaotic recurrent ar-
chitecture and demonstrate it can perform as well more complex models like LSTMs. Bai et al.
(2018) conduct a detailed evaluation of recurrent and convolutional, feed-forward models on a va-
riety of sequence modeling tasks. In diverse settings, they find feed-forward models outperform
their recurrent counterparts. Their experiments are complimentary to ours; we find recurrent models
can often be replaced with stable recurrent models, which we show are equivalent to feed-forward
networks.
10
Published as a conference paper at ICLR 2019
Acknowledgements
This material is based upon work supported by the National Science Foundation Graduate Research
Fellowship Program under Grant No. DGE 1752814 and a generous grant from the AWS Cloud
Credits for Research program.
References
Moray Allan and Christopher Williams. Harmonising chorales by probabilistic inference. In Ad-
Vances in Neural Information Processing Systems (NeurIPS),pp. 25-32, 2005.
Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In
International Conference on Machine Learning (ICML), pp. 1120-1128, 2016.
Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional
and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018.
Dimitri P. Bertsekas. Nonlinear Programming. Athena Scientific, 1999.
Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated
convolutional networks. In International Conference on Machine Learning (ICML), pp. 933-941,
2017.
Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional
sequence to sequence learning. In International Conference on Machine Learning (ICML), pp.
1243-1252, 2017.
Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of
stochastic gradient descent. In International Conference on Machine Learning (ICML), pp. 1225-
1234, 2016.
Moritz Hardt, Tengyu Ma, and Benjamin Recht. Gradient descent learns linear dynamical systems.
The Journal of Machine Learning Research, 19(1):1025-1068, 2018.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Liang Jin, Peter N Nikiforuk, and Madan M Gupta. Absolute stability conditions for discrete-time
recurrent neural networks. IEEE Transactions on Neural Networks, 5(6):954-964, 1994.
Li Jing, Yichen Shen, Tena Dubcek, John Peurifoy, Scott Skirlo, Yann LeCun, Max Tegmark, and
Marin Soljacic. Tunable efficient unitary neural networks (EUNN) and their application to rnns.
In International Conference on Machine Learning (ICML), pp. 1733-1741, 2017.
Cijo Jose, Moustpaha Cisse, and Francois Fleuret. Kronecker recurrent units. In International
Conference on Machine Learning (ICML), pp. 2380-2389, 2018.
Aditya Kusupati, Manish Singh, Kush Bhatia, Ashish Kumar, Prateek Jain, and Manik Varma. Fast-
grnn: A fast, accurate, stable and tiny kilobyte sized gated recurrent neural network. In Advances
in Neural Information Processing Systems (NeurIPS), pp. 9031-9042, 2018.
Thomas Laurent and James von Brecht. A recurrent neural network without chaos. In International
Conference on Learning Representations (ICLR), 2017.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated
corpus of english: The penn treebank. Computational linguistics, 19(2):313-330, 1993.
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture
models. In International Conference on Learning Representations (ICLR), 2017.
Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and Optimizing LSTM
Language Models. In International Conference on Learning Representations (ICLR), 2018.
11
Published as a conference paper at ICLR 2019
Gregoire MesniL Yann Dauphin, Kaisheng Yao, Yoshua Bengio, Li Deng, Dilek Hakkani-Tur, Xi-
aodong He, Larry Heck, Gokhan Tur, Dong Yu, et al. Using recurrent neural networks for slot
filling in spoken language understanding. IEEE/ACM Transactions on Audio, Speech, and Lan-
guage Processing, 23(3):530-539, 2015.
Zakaria Mhammedi, Andrew Hellicar, Ashfaqur Rahman, and James Bailey. Efficient orthogonal
parametrisation of recurrent neural networks using householder reflections. In International Con-
ference on Machine Learning (ICML), pp. 2401-2409, 2017.
Samet Oymak. Stochastic gradient descent learns state equations with nonlinear activations. arXiv
preprint arXiv:1809.03019, 2018.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural
networks. In International Conference on Machine Learning (ICML), pp. 1310-1318, 2013.
Patti J Price. Evaluation of spoken language systems: The atis domain. In Speech and Natural
Language: Proceedings of a Workshop Held at Hidden Valley, Pennsylvania, June 24-27, 1990,
1990.
Hanie Sedghi and Anima Anandkumar. Training input-output recurrent neural networks through
spectral methods. CoRR, abs/1603.00954, 2016.
Stephen Tu, Ross Boczar, Andrew Packard, and Benjamin Recht. Non-asymptotic analysis of robust
control from coarse-grained identification. arXiv preprint arXiv:1707.04791, 2017.
Aaron Van Den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,
Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for
raw audio. arXiv preprint arXiv:1609.03499, 2016.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-
mation Processing Systems (NeurIPS), pp. 6000-6010, 2017.
Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. On orthogonality and learn-
ing recurrent networks with long term dependencies. In International Conference on Machine
Learning (ICML), pp. 3570-3578, 2017.
Scott Wisdom, Thomas Powers, John Hershey, Jonathan Le Roux, and Les Atlas. Full-capacity uni-
tary recurrent neural networks. In Advances in Neural Information Processing Systems (NeurIPS),
pp. 4880-4888, 2016.
Jiong Zhang, Qi Lei, and Inderjit Dhillon. Stabilizing gradients for deep neural networks via efficient
SVD parameterization. In International Conference on Machine Learning (ICML), pp. 5806-
5814, 2018.
12
Published as a conference paper at ICLR 2019
A Proofs from Section 2
A.1 Gradient descent on unstable systems need not converge
Proof of Proposition 1. Consider a scalar linear dynamical system
ht = aht-1 + bxt
yt = ht,
(4)
(5)
where h0 = 0, a, b ∈ R are parameters, and xt , yt ∈ R are elements the input-output sequence
{(xt, yt)}T=ι, where L is the sequence length, and yt is the prediction at time t. Stability of the
above system corresponds to |a| < 1.
Suppose (xt, yt) = (1, 1) for t = 1, . . . , L. Then the desired system (4) simply computes the
identity mapping. Suppose we use the squared-loss '(yt,yjt) = (1∕2)(yt - yt)2, and suppose further
b = 1, so the problem reduces to learning a = 0. We first compute the gradient. Compactly write
t-1
ht = Xatb
i=0
Let δt = (yt - yt). The gradient for step T is then
d	d	T-1
-ς-'(yτ, yτ) = δτ- = δτ X aT-Iht
da	da
t=0
T-1
δT XaT-1-t
t=0
δT
占XIat-高:
(1 - aT)	TaT-1
t [(1 - a)2 - (I-Σ)
Plugging in yt = 1, this becomes
d	(1 - aT )
da'(yτ，yT )= (⅛-W -1
(1 - aτ)	Taτ-1
__
(1 — a)2	(1 — a)
(6)
For large T, if |a| > 1, then aL grows exponentially with T and the gradient is approximately
da'(yτ, yτ) ≈ (aTT- 1) Taτ - ≈ Ta2T-3
Therefore, if a0 is initialized outside of [-1, 1], the iterates ai from gradient descent with step size
αi = (1/i) diverge, i.e. ai → ∞, and from equation (6), it is clear that such ai are not stationary
points.	□
A.2 Proofs from section 2.2
A.2.1 Recurrent neural networks
Assume kW k ≤ λ < 1 and kUk ≤ BU. Notice tanh0(x) = 1 - tanh(x)2, so since tanh(x) ∈
[-1, 1], tanh(x) is 1-Lipschitz and 2-smooth. We previously showed the system is stable since, for
any states h, h0 ,
ktanh(W h + Ux) - tanh(W h0 + Ux)k
≤ kWh+Ux-Wh0-Uxk
≤kWkkh-h0k.
13
Published as a conference paper at ICLR 2019
Using Lemma 1 with k = 0, ∣∣htk ≤ BUBx for all t. Therefore, for any W, W0, U, U0,
ktanh(W ht + Ux) - tanh(W 0 ht + U0x)k
≤ kWht+Ux-W0ht - U0xk
≤supkhtkkW-W0k+BxkU-U0k.
t
≤
BU Bx
(I- λ)
kW-W0k+BxkU-U0k,
so the model is Lipschitz in U, W . We can similarly argue the model is BU Lipschitz in x. For
smoothness, the partial derivative with respect to h is
Ww,, x = diag(tanh0(Wh + Ux))W,
so for any h, h0, bounding the '∞ norm with the '2 norm,
；, x) — Aφw(^ , x) = Ildiag(tanh0(Wh + Ux))W — diag(tanh0(Wh0 + Ux))W∣∣
≤ kWk diag(tanh0(W h + Ux) - tanh0 (Wh0 + U x))
≤ 2 ∣∣Wk kWh + Ux — Wh0 — Uxk∞
≤ 2λ2 kh—h0k.
For any W, W0, U, U0 satisfying our assumptions,
'φw∖'' x) — aφw(h' x) = ∣∣diag(tanh0(Wh + Ux))W — diag(tanh0(W0h + U0x))W0∣∣
∣ ∂h	∂h ∣
≤ ∣∣diag(tanh0 (W h + Ux) — tanh0(W0h + U 0 x))∣∣ kWk
+ ∣∣diag(tanh0(W 0h + U0x))∣∣ kW — W0k
≤ 2λ k(W — W0)h + (U — U0)xk∞ + kW — W0k
≤ 2λ k(W — W0)kkhk +2λ kU — U0kkxk + kW — W0k
≤ 2λBUBx +\(： - λ) kW — W0k + 2λBx kU — U0k .
(1 — λ)
Similar manipulations establish dφ∂h,x) is LiPschitz in h and w.
A.2.2 LSTMS
Similar to the previous sections, we assume s0 = 0.
The state-transition map is not Lipschitz in s, much less stable, unless kck is bounded. However,
assuming the weights are bounded, we first prove this is always the case.
Lemma 4. Let kfk∞ = supt kftk∞. If kWf k∞ < ∞, kUf k∞ < ∞, and kxtk∞ ≤ Bx, then
kfk∞ < 1 and kctk∞ ≤ (1-kf 葭)for allt.
ProofofLemma4. Note ∣tanh(x)∣, ∣σ(x)∣ ≤ 1 for all x. Therefore, for any t, ∣∣htk∞ =
kot ◦ tanh(ct)k∞ ≤ 1. Since σ(x) < 1 for x < ∞ and σ is monotonically increasing
kftk∞ ≤ σ (kW/h1 + Ufxtk∞)
≤ σ (kW/k∞ kht-ik∞ + kU/k∞ kxtk∞)
≤σ(BW+BuBx)
< 1.
Using the trivial bound, kitk∞ ≤ 1 and kzt k∞ ≤ 1, so
kct+1k∞ = kit ◦ Zt + ft ◦ ctk∞ ≤ 1 + kftk∞ kctk∞ .
14
Published as a conference paper at ICLR 2019
Unrolling this recursion, we obtain a geometric series
t
kct+1k∞ ≤Xkftki∞≤
i=0
1
(1 TfU.
□
Proofof Proposition 2. We show Φlstm is λ-contractive in the '∞-norm for some λ < 1. For
r ≥ logι∕λ(√d), this in turn implies the iterated system ΦLstm is contractive is the '2-norm.
Consider the pair of reachable hidden states s = (c, h), s0 = (c0, h0). By Lemma 4, c, c0 are bounded.
Analogous to the recurrent network case above, since σ is (1/4)-Lipschitz and tanh is 1-Lipschitz,
ki-i0k≤ 4 kWik∞ kh-h0k∞
kf - f0k≤ 4 kWfk∞ kh-h0k∞
ko-o0k≤ 4 kWok∞ kh-h0k∞
kz-z0k ≤ kWzk∞kh-h0k∞.
Both kzk∞ , kik∞ ≤ 1 since they’re the output ofa sigmoid. Letting c+ and c0+ denote the state on
the next time step, applying the triangle inequality,
∣∣c+ - c+∣∣∞ ≤ Ili◦ z-i0◦ z0k∞ +11/◦ c-f0◦ c0k∞
≤k(i - i0) ◦ zk∞ + ki0 ◦ (z - z0)k∞ + kf ◦ (c - c0)k∞ + kc。(/ - / 0)k∞
≤	Ii-i0I∞IzI∞+Iz-z0I∞Ii0I∞+Ic-c0I∞IfI∞+If-f0I∞IcI∞
≤	(kWik∞ + kCk∞ kWf k∞ + kWzk∞) kh - h0k∞ + k/k∞ kc - c0k∞ .
A similar argument shows
∣	∣h+ -h+∣∣∞ ≤ ko-o0k∞ + ∣∣c+ -c+∣∣∞ ≤ kWok∞ kh-h0k∞ + ∣∣c+ -c+∣∣∞.
By assumption,
(kWik∞ + kck∞ 厂k∞ + kwok∞ + kWzk∞) < 1 -k∕k∞ ,
and so
∣∣h+ - h+∣∣∞ < (1 -k∕k∞) kh - h0k∞ + k∕k∞ kc- c0k∞ ≤ks - s0k∞,
as well as
∣∣c+ - c+∣∣∞ < (1 -k∕k∞) kh - h0k∞ + k∕k∞ kc - c0k∞ ≤ks - s0k∞ ,
which together imply
∣∣s+ - s0+∣∣∞ < ks - s0k∞ ,
establishing Φlstm is contractive in the '∞ norm.	□
B Proofs from section 3
Throughout this section, we assume the initial state h0 = 0. Without loss of generality, we also
assume φw(0, 0) = 0 for all w. Otherwise, we can reparameterize φw(h, x) 7→ φw(h, x) - φw(0, 0)
without affecting expressivity ofφw. For stable models, we also assume there some compact, convex
domain Θ ⊂ Rm so that the map φw is stable for all choices of parameters w ∈ Θ.
15
Published as a conference paper at ICLR 2019
Proof of Lemma 1. For any t ≥ 1, by triangle inequality,
khtk = kφw(ht-1,xt) - φw(0,0)k ≤ kφw(ht-1,xt) - φw(0,xt)k + kφw (0, xt) - φw(0,0)k .
Applying the stability and Lipschitz assumptions and then summing a geometric series,
khtk ≤ λ kht-ιk + Lx kxtk ≤ XXλiLχBχ ≤ -LxB-.
i=0	(1 - λ)
Now, consider the difference between hidden states at time step t. Unrolling the iterates k steps and
then using the previous display yields
Ilht- hk∣∣ = ∣∣Φw (ht-ι,χt) - Φw (hk-ι,χt)∣∣ ≤ λ ∣∣ht-ι - hk-ι∣∣ ≤ λk 也一 k ≤ λLxB,
(1 - λ)
and solving for k gives the result.	□
B.1 Proofs from section 3.3
Before proceeding, we introduce notation for our smoothness assumption. We assume the map φw
satisfies four smoothness conditions: for any reachable states h, h0, and any weights w, w0 ∈ Θ,
there are some scalars βww , βwh, βhw , βhh such that
1.1∣dφw∂Wχ) - dφ∂警)∣∣≤βww kw-w0k.
2.	∣∣dφ∂hx) -*"∣∣ ≤ βwh kh - h0k.
3.	∣∣dφw∂hx) - d⅛hx)∣∣ ≤ βhw kw - w0k.
4.	∣∣dφw∂hχ) - dφw∂h,x)∣∣ ≤βhhkh-h0k.
B.1.1	Gradient difference due to truncation is negligible
In the section, we argue the difference in gradient with respect to the weights between the recurrent
and truncated models is O(kλk ). For sufficiently large k (independent of the sequence length),
the impact of truncation is therefore negligible. The proof leverages the “vanishing-gradient”
phenomenon- the long-term components of the gradient of the full recurrent model quickly van-
ish. The remaining challenge is to show the short-term components of the gradient are similar for
the full and recurrent models.
Proof of Lemma 2. The Jacobian of the loss with respect to the weights is
∂hτ ∂ht
∂ht ∂w
where 煞 is the partial derivative of h with respect to w, assuming ht-ι is constant with respect
to w. Expanding the expression for the gradient, we wish to bound
卜WPT(W) -VwPT(W)Il = XX (IhtIt) vhτPT -
T
X
t=T -k+1
∂hk Y
∂w J
VhkT PkT
≤
T-k
X
t=1
∂hτ Iht
∂ht ∂W
>
VhT PT
T
+X
t=T -k+1
(If dht)	VhTPT -
(∂hT ∂hk∖>
(∂hk ∂w )
VhkT PT
The first term consists of the “long-term components” of the gradient for the recurrent model. The
second term is the difference in the “short-term components” of the gradients between the recurrent
and truncated models. We bound each of these terms separately.
16
Published as a conference paper at ICLR 2019
For the first term, by the LiPschitz assumptions, kVhτPT∣∣ ≤ Lp and ∣∣Vwhtk ≤ Lw. Since φw is
λ-contractive, so
∂ht
∂ht-ι
≤ λ. Using submultiplicavity of the spectral norm,
T
∂pτ T-k ∂pτ ∂ht
∂hτ	∂ht ∂w
≤∣VhTpT∣
T-k
t=0
i=t
∂hi
∂hi-i
∣Vw ht k ≤ LpLw X λτT ≤ λk ~LpLw.
t=0	(1 - λ)
Focusing on the second term, by triangle inequality and smoothness,
T
XT
T k+1
t=T -k+1
∂hr ∂ht
∂ht ∂w
>
VhTPT -
∂htk >
M VhTPT
≤	XT	VhTPT-VhkTPkT
t=T -k+1
T
∂hT ∂htk
∂hk ∂w
≤ X	βp IIhT - hkT II λT-tLw + Lp
t=T-k+11
-{z
(a)
Using Lemma 1 to upper bound (a),
T
X βp hT - hkT λT-t Lw ≤
t=T -k
+kVhTPTk
∂hT ∂ht	∂hkT ∂htk
---------------------
_----:   ; : 
∂ht ∂w	∂hk ∂w
∂hT ∂ht ∂hkT ∂htk
---------------------
∂ht ∂w
^^™{z
(b)
∂ht ∂w
T
X λT-t
t=T -k
λkβpLw LxBx ≤ λkβpLwLxBx
(1 - λ)
(1 -λ)2
✓
|
✓
Using the triangle inequality, Lipschitz and smoothness, (b) is bounded by
T
X	Lp
t=T -k+1
T
∂hT ∂ht ∂hkT ∂htk
____ ___ _ ____ ___
∂h t ∂w ∂hk ∂w
Lp
t=T -k+1
T
∂hk
∂w
+ Lp
X	Lp λT -tβwh ht - htk + LpLw
t=T -k+1
T
kλk Lpew-LX)Bx + LpLw X
( 一 )	t=T -k+1
X-------------
∂hτ
加
∂hk∣∣∣∣ ∂hτ
∂w Il Il ∂ht
∂hT
∂hk
∂hτ
而
∂hT
∂hk
∂hT
∂hk
(c)
where the last line used llht — hk ll ≤ λt-(τTk) L-B) for t ≥ T 一 k. It remains to bound (c), the
difference of the hidden-to-hidden Jacobians. Peeling off one term at a time and applying triangle
inequality, for any t ≥ T 一 k + 1,
∂hτ
∂hT
∂hT -i
∂hT	11 Il ∂hT -1
dhT-i Il Il dht
βhh khT-1 -hT-1kλT-t-1+λ
∂hT
dhT-1
∂hT -ι
∂ht
dhT -1
∂hk
∂hT-1
∂hk
≤
≤
≤
—
dhk 1 ≤
≤
∂hτ Ml ∂ht
而 Il Il ∂w
	
	
	
—
+
	
}
IH
—
—
T-1
≤ X βhhλT-t-1 IIIhi - hik III
i=t
≤
≤
T-1
βhhLxBx	i
Ar i=tλ
βhhLχBx
(1 - λ)2 ,
so (c) is bounded by kλ
kLpLw βhhLχ Bx
the entire sum is O (1--12)2
Ti-λ)2
. Ignoring Lipschitz and smoothness constants, we’ve shown
□
17
Published as a conference paper at ICLR 2019
B.1.2	Stable recurrent models are smooth
In this section, We prove that the gradient map NwPT is Lipschitz. First, We show on the forward
pass, the difference between hidden states ht (w) and h0t(w0) obtained by running the model with
weights w and w0, respectively, is bounded in terms of kw - w0 k. Using smoothness of φ, the
difference in gradients can be written in terms of kht(w) - h0t(w0)k, which in turn can be bounded
in terms of kw - w0 k. We repeatedly leverage this fact to conclude the total difference in gradients
must be similarly bounded.
We first show small differences in weights don’t significantly change the trajectory of the recurrent
model.
Lemma 5. For some w, w0, suppose φw , φw0 are λ-contractive and Lw Lipschitz in w. Let
ht (w), ht (w0) be the hidden state at time t obtain from running the model with weights w, w0 on
common inputs {xt}. If h0 (w) = h0 (w0), then
Lw kw - w k
llht(w) — ht(w )k ≤ ―(1 - λ)—.
Proof. By triangle inequality, followed by the Lipschitz and contractivity assumptions,
lht(w) - ht(w0)l
= lφw(ht-1(w), xt) - φw0 (ht-1(w0), xt)l
≤ lφw(ht-1(w), xt) - φw0 (ht-1(w), xt)l + lφw0 (ht-1(w), xt) - φw0 (ht-1(w0), xt)l
≤ Lw lw - w0l + λ lht-1(w) - ht-1 (w0)l .
Iterating this argument and then using h0 (w) = h0 (w0), we obtain a geometric series in λ.
lht(w) - ht(w0)l ≤ Lw lw -w0l + λ lht-1(w) - ht-1(w0)l
t
≤ X Lw lw - w0 l λi
i=0
≤ Lw kw - w0k	π
≤	(1- λ).
The proof of Lemma 3 is similar in structure to Lemma 2, and follows from repeatedly using smooth-
ness of φ and Lemma 5.
Proof of Lemma 3. Let h0t = ht (w0). Expanding the gradients and using lht(w) - ht(w0)l ≤
Lw(IWIw k from Lemma 5.
lNw pT (w) - NwpT (w0)l
NhT pT -
T
≤	VhT pT - Vh0T pT
第t 察［VhTpT
∂hT ∂ht
∂ht ∂w
+ lVhT pT l
∂hT ∂ht ∂h0T ∂h0t
_____________________
∂h t ∂w ∂ht ∂w
T
≤ X βp lhT - h0T l λT-tLw + Lp
t=1
∂ hT ∂ht ∂h0T ∂h0t
_________ _ _________
∂h t ∂w ∂ht ∂w
< βPLW kw - w0k
≤	(1 - λ)2
T
+ Lp X
t=1
∂hT ∂ht ∂h0T ∂h0t
____ ____ _ ____ ____
∂ht ∂w ∂ht ∂w
{z^
(a)
18
Published as a conference paper at ICLR 2019
Focusing on term (a),
T
LpX
t=1
∂hT ∂ht ∂h0T ∂h0t
____ ____ _ _____ ___
∂ht ∂w ∂h[ ∂w
XX ∂hτ	∂hT
≤ p=	∂ht	∂h't
∂ht
∂w
+ Lp
T
≤ LpLw
t=1
T
≤ LpLw
t=1
∂hτ ∂hT
西-西
∂hτ ∂h0r
而—西
T
+ LpX	λT-t (βwh kht - h0t k + βww kw - w0 k)
t=1
LpeWhLW k w - W Il
+	D2
LpeWW k w — W k
+ 一1λ一
-- /
{z^^^^^^^^^^^^^
(b)
|
where the penultimate line used,
∂ht ∂ht
∂W ∂W
∂φW (ht-1, xt) ∂φW (h0t-1, xt)
≤	-----Z----------------Z------
∂W	∂W
≤ eWh kh — h0 k + eWW kW — W0 k .
∂φw (ht-i,xt) _ ∂φw0 (ht-i,xt)
∂W	∂W
To bound (b), we peel off terms one by one using the triangle inequality,
T
LpLW
t=1
∂hT ∂h0T
____________
∂ht ∂ht
T
≤ LpLW
t=1
∂hτ
∂hτ-1
∂hT HH ∂hτ-1
∂hT-1 Illl ^ahT
+ Il g∣∣∣ ∂hτ-ι — ∂hτ-ι II
十 |1 ∂hτ-J1 ∂ht	∂ht U
≤ LpLW	X	(βhh∣∣hτ-1	-	IhT-i∣∣ + βhw	∣∣w - w0k) λ-t-1	+ λ	Idhh-I	—	^dh-I
T
≤ LpLW
t=1
T-t
ehW(T — t)λT -t-1 kW — W0k + ehhX	llhT -i — h0T -i llλT-t-1
i=1
T
≤ LpLW X βhW(T — t)λT-t-1 kw — w0k + βhh W kW- W k (T - t)λT-T
t=ι L	( - )
LpLWβhW kw 一 w0k	LpLWβhh kw 一 w0k
一 (1 - λ)2	+	(1 — λ)3
Supressing Lipschitz and smoothness constants, we’ve shown the entire sum is O(1/(1 — λ)3), as
required.	□
B.1.3	Gradient descent analysis
Equipped with the smoothness and truncation lemmas (Lemmas 2 and 3), we turn towards proving
the main gradient descent result.
Proof of Proposition 4. Let ΠΘ denote the Euclidean projection onto Θ, and let δi =
lwriecurr — wtirunc l. Initially δ0 = 0, and on step i + 1, we have the following recurrence rela-
19
Published as a conference paper at ICLR 2019
tion for δi+1 ,
δi+1 = wre+curr - wtr+unc
= ΠΘ (wri
ecurr -αiNpT(Wy) - πθ (WirUnc - αiVpT (Wt
rUnc))
≤ WrecUrr - αiVpT (Wi)) - WtirUnc - αiVpkT(Wti
rUnc )
≤ WrecUrr - Wtir
Unc + αi VpT (Wri
ecUrr) - VpkT (Wti
rUnc )
≤ δi + αi VpT (WrecUrr ) - VpT (Wti
rUnc) +αi VpT (Wti
rUnc ) - VpkT (Wti
rUnc )
≤ δi + a (βδt + γkλk)
≤ exp (αiβ) δi + αiγkλk,
the penultimate line applied lemmas 2 and 3, and the last line used 1 + x ≤ ex for all x. Unwinding
the recurrence relation at step N ,
NN
δN ≤XY
exp(αj β) αiγkλk
i=1 j=i+1
≤x {n Jxp(αβ)
i=1 j=i+1
N f ( N 1
=X 卜Xp (αβ X j
i=1 I ∖ j=i+1j
αγkλk
i
αγkλk
i
Bounding the inner summation via an integral, PN=%+、j ≤ log(Ν∕i) and simplifying the resulting
expression,
N	αγkλk
δN ≤ Xexp(αβlog(Ν∕i))αγ-
i=1	i
N1
=αγkλkNαβV --τ
iαβ+1
i=1
≤ αγkλkNαβ+1 .
□
B.1.4	Proof of theorem 1
Proof of Theorem 1. Using f is Lf -Lipschitz and the triangle inequality,
yT - yTk ≤ Lf hT (WrNecUrr) - hkT (WtNrUnc)
≤ Lf hT(WrNecUrr) - hT (WtNrUnc) + Lf hT(WtNrUnc) - hkT(WtNrUnc) .
By Lemma 5, the first term is bounded by Lwkwreju-r-wtrunck, and by Lemma 1, the second term is
bounded by λk Lχ-Bλχ). Using Proposition 4, after N steps of gradient descent, We have
k	LfLw WrecUrr - WtrUnc	k LfLxBx
“yτ TT n ≤-----1λ----------+λ TT-q
k aLf LwNαβ+1	. Lf LxBx
≤kλ (i - λ)	+ λ æ,
and solving for k such that both terms are less than ε∕2 gives the result.
□
20
Published as a conference paper at ICLR 2019
Difference in Recurrent Weight Matrices
^ = l≥l!g≥-
E-JON©ds ≡ əɔuə'əjj一
Figure 6: Empirical validation Proposition 4 on random Gaussian instances. Without the 1/t rate,
the gradient descent bound no longer appears qualitatively correct, suggesting the O(1/t) rate is
necessary.
C Experiments
The O(1/t) rate may be necessary. The key result underlying Theorem 1 is the bound on the
parameter difference kwtrunc - wrecurr k while running gradient descent obtained in Proposition 4.
We show this bound has the correct qualitative scaling using random instances and training randomly
initialized, stable linear dynamical systems and tanh-RNNs. In Figure 6, we plot the parameter error
kwttrunc - wrtecurrk as training progresses for both models (averaged over 10 runs). The error scales
comparably with the bound given in Proposition 4. We also find for larger step-sizes like ɑ/ʌ/t or
constant α, the bound fails to hold, suggesting the O(1/t) condition is necessary.
Concretely, we generate random problem instance by fixing a sequence length T = 200, sampling
input data Xt i±. N(0,4 ∙ I32), and sampling yτ 〜Unif[-2,2]. Next, we set λ = 0.75 and
randomly initialize a stable linear dynamical system or RNN with tanh non-linearity by sampling
",Wij i好. N(0,0.5) and thresholding the singular values of W so ∣∣W∣∣ ≤ λ. We use the
squared loss and prediction function f (ht, Xt) = Cht + Dxt, where C, D i削. N (0, I32). We fix
the truncation length to k = 35, set the learning rate to at = α∕t for α = 0.01, and take N = 200
gradient steps. These parameters are chosen so that the γkλkNαβ+1 bound from Proposition 4 does
not become vacuous - by triangle inequality, we always have IlwtrUnc — WrecUrrk ≤ 2λ.
Stable vs. unstable models. The word and character level language modeling experiments are
based on publically available code from Merity et al. (2018). The polyphonic music modeling code
is based on the code in Bai et al. (2018), and the slot-filling model is a reimplementation of Mesnil
et al. (2015) 1
Since the sufficient conditions for stability derived in Section 2.2 only apply for networks with a
single layer, we use a single layer RNN or LSTM for all experiments. Further, our theoretical
results are only applicable for vanilla SGD, and not adaptive gradient methods, so all models are
trained with SGD. Table 2 contains a summary of all the hyperparameters for each experiment.
All hyperparameters are shared between the stable and unstable variants of both models. In the RNN
case, enforcing stability is conceptually simple, though computationally expensive. Since tanh is 1-
Lipschitz, the RNN is stable as long as ∣W∣ < 1. Therefore, after each gradient update, we project
W onto the spectral norm ball by taking the SVD and thresholding the singular values to lie in [0, 1).
In the LSTM case, enforcing stability is conceptually more difficult, but computationally simple. To
ensure the LSTM is stable, we appeal to Proposition 2. We enforce the following inequalities after
each gradient update
1The word-level language modeling code is based on https://github.com/pytorch/examples/
tree/master/word_language_model, the character-level code is based on https://github.
com/salesforce/awd-lstm-lm, and the polyphonic music modeling code is based on https://
github.com/locuslab/TCN.
21
Published as a conference paper at ICLR 2019
Table 2: Hyperparameters for all experiments
Model
RNN LSTM
Word LM	Number layers	1	1
	Hidden units	256	1024
	Embedding size	1024	512
	Dropout	0.25	0.65
	Batch size	20	20
	Learning rate	2.0	20.
	BPTT	35	35
	Gradient clipping	0.25	1.0
	Epochs	40	40
Char LM	Number layers	1	1
	Hidden units	768	1024
	Embedding size	400	400
	Dropout	0.1	0.1
	Weight decay	1e-6	1e-6
	Batch size	80	80
	Learning rate	2.0	20.0
	BPTT	150	150
	Gradient clipping	1.0	1.0
	Epochs	300	300
Polyphonic Music	Number layers	1	1
	Hidden units	1024	1024
	Dropout	0.1	0.1
	Batch size	1	1
	Learning rate	0.05	2.0
	Gradient clipping	5.0	5.0
	Epochs	100	100
Slot-Filling	Number layers	1	1
	Hidden units	128	128
	Embedding size	64	64
	Dropout	0.5	0.5
	Weight decay	1e-4	1e-4
	Batch size	128	128
	Learning rate	10.0	10.0
	Gradient clipping	1.0	1.0
	Epochs	100	100
22
Published as a conference paper at ICLR 2019
1.	The hidden-to-hidden forget gate matrix should satisfy kWf k∞ < 0.128, which is enforced
by normalizing the `1- norm of each row to have value at most 0.128.
2.	The input vectors xt must satisfy kxt k∞ ≤ Bx = 0.75, which is achieved by thresholding
all values to lie in [-0.75, 0.75].
3.	The bias of the forget gate bf, must satsify kbf k∞ ≤ 0.25, which is again achieved by
thresholding all values to lie in [-0.25, 0.25].
4.	The input-hidden forget gate matrix Uf should satisfy kUf k∞ ≤ 0.25. This is enforced by
normalizing the `1- norm of each row to have value at most 0.25.
5.	Given 1-4, the forget gate can take value at most f∞ < 0.64. Consequently, we enforce
kWik∞,kWok∞ ≤ 0.36, IIWzk ≤ 0.091, and kWf k∞ < min {0.128, (1 - 0.64)2}=
0.128.
After 1-5 are enforced, by Proposition 2, the resulting (iterated)-LSTM is stable. Although the above
description is somewhat complicated, the implementation boils down to normalizing the rows of the
LSTM weight matrices, which can be done very efficiently in a few lines of PyTorch.
Data-dependent stability. Unlike the RNN, in an LSTM, it is not clear how to analytically com-
pute the stability parameter λ. Instead, we rely on a heuristic method to estimate λ. Recall a model
is stable if for all x, h, h0, we have
S(h, h0, X)= kφw(h,X) - φw(h0,x)k ≤ λ < 1.	⑺
Ih - h0I
To estimate suph,h0,x S(h, h0, x), we do the following. First, we take x to be point in the training set.
In the language modeling case, x is one of the learned word-vectors. We randomly sample and fix x,
and then we perform gradient ascent on S(h, h0, x) to find worst-case h, h0. In our experiments, we
initialize h,h0 ~ N (0,0.1 ∙ I) and run gradient ascent with learning rate 0.9 for 1000 steps. This
procedure is repeated 20 times, and we estimate λ as the maximum value of S(h, h0, x) encounted
during any iteration from any of the 20 random starting points.
23