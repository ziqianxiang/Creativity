Published as a conference paper at ICLR 2019
Unsupervised Speech Recognition via Segmen-
tal Empirical Output Distribution Matching
Chih-Kuan Yeh*
Machine Learning Department
Carnegie Mellon University
Pittsburgh, PA 15213, USA
cjyeh@cs.cmu.edu
Jianshu Chen, Chengzhu Yu & Dong Yu
Tencent AI Lab
Bellevue, WA 98004, USA
{jianshuchen,czyu,dyu}@tencent.com
Ab stract
We consider the problem of training speech recognition systems without using
any labeled data, under the assumption that the learner can only access to the in-
put utterances and a phoneme language model estimated from a non-overlapping
corpus. We propose a fully unsupervised learning algorithm that alternates be-
tween solving two sub-problems: (i) learn a phoneme classifier for a given set
of phoneme segmentation boundaries, and (ii) refining the phoneme boundaries
based on a given classifier. To solve the first sub-problem, we introduce a novel un-
supervised cost function named Segmental Empirical Output Distribution Match-
ing, which generalizes the work in (Liu et al., 2017) to segmental structures. For
the second sub-problem, we develop an approximate MAP approach to refining
the boundaries obtained from Wang et al. (2017). Experimental results on TIMIT
dataset demonstrate the success of this fully unsupervised phoneme recognition
system, which achieves a phone error rate (PER) of 41.6%. Although it is still
far away from the state-of-the-art supervised systems, we show that with oracle
boundaries and matching language model, the PER could be improved to 32.5%.
This performance approaches the supervised system of the same model architec-
ture, demonstrating the great potential of the proposed method.
1 Introduction
Over the past years, the performance of automatic speech recognition (ASR) has been improved
greatly and the recognition accuracy in certain scenarios could be on par with human performance
(Xiong et al., 2016). Most of the state-of-the-art ASR systems are constructed by training deep
neural networks on large-scale labeled data using supervised learning (Hinton et al., 2012; Dahl
et al., 2012; Xiong et al., 2016; Graves et al., 2013; 2006; Graves, 2012); they rely on a large
number of human labeled data to train the recognition model. In this paper, we are working towards
the grand mission of training speech recognition models without any human annotated data. Such
an approach could potentially save a huge amount of human labeling costs for developing ASR
systems by leveraging massive unlabeled speech data. It is especially valuable when developing
ASR systems for low-resource languages, where labeled data are more expensive to obtain.
Specifically, we consider the phoneme recognition problem, for which we learn a sequential classi-
fier that maps speech waveform into a sequence of phonemes. In our unsupervised learning setting,
the learning algorithm can only access (i) the input speech acoustic features, and (ii) a pretrained
phoneme language model (LM). There is no human supervision presented to the algorithm at any
level; that is, we do not provide any (frame-level) label for input samples, nor do we provide any
(sentence-level) transcription for input utterances. The language model could be trained from a
separate (text) corpus in an unsupervised manner with the help of a pre-defined lexicon* 1.
There have been some recent successes in developing fully unsupervised method for neural ma-
chine translation (Artetxe et al., 2018; Lample et al., 2018) and sequence classifications (Liu et al.,
*The work was done during an internship at TenCent AI Lab, Bellevue, WA.
1A lexicon in ASR is a pre-defined dictionary that maps word sequences into phoneme sequences.
1
Published as a conference paper at ICLR 2019
2017). However, different from these problems, speech recognition problem has segmental struc-
tures that impose unique challenges for developing unsupervised learning algorithms. First, each
phoneme generally consists of a segment of consecutive input samples (frames) that are associated
to the same phoneme label. Second, the lengths and the boundaries of these segments are usually
unknown a priori. For this reason, we could not directly apply the previous techniques to develop
unsupervised ASR algorithms. To address the first challenge, we develop a novel unsupervised
learning cost function for ASR systems by extending the Empirical Output Distribution Matching
(Empirical-ODM) cost in (Liu et al., 2017) to segmental structures. The key ideas of our Segmental
Empirical-ODM are: (i) the distribution of the predicted outputs across consecutive segments shall
match the phoneme language model and (ii) the predicted outputs within each segment should be
equal to each other as they belong to the same phoneme. This cost function allows us to learn the
classifier without labeled data for a given set of phoneme segmentation boundaries. To address the
second challenge, we develop a novel unsupervised approach to estimate (and refine) the segmen-
tation boundaries using the current classification model. Our algorithm alternates between these
two steps of learning classifier and estimating the boundaries to successively improve the perfor-
mance of each other. Therefore, unlike previous works in (Liu et al., 2018), which relies on an
oracle or forced alignment methods to obtain the phoneme segmentation boundaries, our method is
fully unsupervised in both segmentation and classification. Furthermore, we also adapt the semi-
supervised HMM learning technique (Zavaliagkos et al., 1998; Kemp and Waibel, 1999; Nallasamy
et al., 2012) to our unsupervised setting to further improve the performance. In our experiments on
TIMIT phoneme recognition task, our unsupervised learning method achieves a promising phone
error rate (PER) of 41.6%. To our best knowledge, this is the first empirical success of a fully unsu-
pervised speech recognition that does not use any oracle segmentation or labels. Furthermore, when
the oracle phoneme segmentation boundaries are given (similar to the setting in Liu et al. (2018)),
our method achieves a PER of 32.5% with matching language model, which approaches supervised
learning with the same model architecture, demonstrating a great potential of our method.
2	Fully Unsupervised S peech Recognition
2.1	Problem formulation
We consider the unsupervised phoneme recognition problem. Specifically, for a given sequence
of input feature vectors x = (x1, . . . , xT), we want to map it into a sequence of phonemes q =
(q1, . . . , qU), where xt ∈ Rm is an m-dimensional input acoustic feature vector (e.g., mel-frequency
cepstral coefficients (MFCC)), qi ∈ Y is a categorical variable representing the phoneme class, Y
denotes the set of phonemes, T is the length of the input sequence, and U is the length of the
output sequence. Note that the length of the input sequence is usually much larger than that of the
output sequence.2 This is because speech data have a special segmental structure where a segment of
consecutive input frames are associated with one phoneme class, as shown in Figure 1. Furthermore,
the length and boundaries of each phoneme segment are generally varying and unknown a priori.
We introduce a binary variable bt ∈ {0, 1} to characterize the segment boundaries: bt = 1 denotes
the start of a new phoneme segment (see Figure 1). Let yt ∈ Y be the frame-wise phoneme label
indicating the phoneme class that the t-th input frame xt belongs to. In this work, we focus on
learning a framewise phoneme classifier that maps the input sequence x1 , . . . , xT into its frame-
wise label sequence y = (y1 , . . . , yT ). Once this is done, we could use a standard speech decoder
to obtain the desired phoneme sequence q1, . . . , qU from y1, . . . , yT. We model the framewise
phoneme classifier pθ (yt |xt) (i.e., the posterior probability of the frame label yt given the input xt)
by a context dependent DNN (Dahl et al., 2012), where θ denotes the model parameter and the input
feature vector xt is a concatenation of the acoustic feature vectors within a context window around
time t. We may also use other model architectures such as recurrent neural network (RNN), which
are left as the future work. The objective of our unsupervised learning algorithm is to learn the model
parameter θ from: (i) a training set of input sequences Dx, and (ii) a pretrained phoneme language
model pLM (q). Note that the language model could be trained from a separate (text) corpus in an
unsupervised manner so that there is no supervision at any level.
There are two main challenges for unsupervised speech recognition: (i) how to learn the classifier
Pθ(yt|xt) from Dx andPLM(q) for a given set of segmentation boundaries, and (ii) how to estimate
2We will use notation t to index input frames and use notation i to index segments (or phonemes).
2
Published as a conference paper at ICLR 2019
Segment
Phoneme
Frame label
Boundary
Input vector
5ι------------<S2→{^------------ S3
aw	n	ix
OOOOOOO OOOjOOO∣OOOO OOOOOO
1000000000∣100∣1000000000
Figure 1: Segmental structure of speech data. Circles of the same color denote the same frame label
in each segment. The shaded input vectors represent the sampled vectors to compute Jodm(Θ).
the segmentation boundaries in an unsupervised manner. Unlike text where content can be broken
into word units relatively easily, speech inputs are continuous and thus it is difficult to obtain the
phoneme boundaries. This challenge is unique to unsupervised speech recognition and does not
appear in e.g. unsupervised machine translation. In the following sections, We address the above
challenges by developing a new unsupervised learning cost function by extending the Empirical-
ODM cost in (Liu et al., 2017) to segmental structures. Furthermore, we develop a maximum a
posteriori (MAP) estimator to refine the segmentation boundaries based on the current pθ(yt∖χt).
Our algorithm alternates between these two steps, and after the iteration completes, we employ an
unsupervised HMM training technique to further boost our unsupervised results.
2.2	Unsupervised frame classification with given segmentation boundaries
In this section, we develop an unsupervised algorithm to learn the classification model pθ (yt |xt) with
a given set of segmentation boundaries {bt}. To this end, we define anew unsupervised learning cost
function that exploits the segmental structure of the problem. Specifically, our new cost function is
based on the following two observations: (i) the distribution of the predicted outputs across con-
secutive segments shall match the phoneme language model pLM (q), and (ii) the predicted outputs
within each segment should be equal to each other as they belong to the same phoneme. Accord-
ingly, our unsupervised cost function consists of two parts, characterizing the above inter-segment
and intra-segment distributions, respectively.
We first define the cost function associated with the inter-segment distribution. Before that, we
introduce the following terms and notations, which are also illustrated in Figure 1. To simplify
notation, we assume that all the utterances in Dx are concatenated into one long sequence. Let
there be a total of K segments in the entire training set Dx and let Si be a set that includes all the
time indexes in the i-th segment. We use τ = (t1, . . . , tK) to denote a sequence of time indexes
sampled from S1, . . . , SK, one per segment, i.e., ti ∈ Si. Without loss of generality, we consider
N -gram phoneme language models throughout this work and define pLM (z) , pLM (qi-N+1 =
z1, . . . , qi = zN), where z = (z1, . . . , zN) ∈ YN denotes a particular N -gram. Furthermore,
let τi = (ti-N+1, . . . , ti) be a length-N contiguous subsequence of τ that ends at ti. We use the
compact notation xτi and yτi to represent (xti-N+1, . . . , xti) and (yti-N+1, . . . , yti ), respectively.
Then, the cost function that characterizes the inter-segment output distribution match is defined as:
JODM (θ) = -XX
PLM(Z)In pθ (Z)	(1)
T∈Sι×…×Sk Z∈YN
where PT(Z) = K PK=1 pθ(y∏ = z∖χ∏) is defined as the inter-segment output distribution with
Pθ(yτi = z∖xτi) , Qj=i-N+1 Pθ(Iytj = Zj∖xj). The cost function (1) generalizes the Empirical-
ODM cost in Liu et al. (2017) to the segmental structures, and it degenerates into the original
Empirical-ODM cost when there is only one frame in each segment. The probability PT(Z) charac-
terizes the empirical N -gram frequency of the predicted output across N consecutive segments, and
the cost function measures the cross-entropy between the pretrained N-gram LMPLM(Z) andPT(z).
This form of cost function enjoys several properties that are suitable for unsupervised learning of
sequence classifiers, and the readers are referred to Liu et al. (2017) for more detailed discussions.
3
Published as a conference paper at ICLR 2019
Next, we define the cost function that characterizes the intra-segment distribution matching as:
K2
JFS (θ) =X X X pθ(yt=y|xt) - pθ(yt+1 = y|xt+1)	(2)
i=1 t,t+1∈Si y∈Y
where the subscript “FS” stands for frame-wise smoothness. The cost (2) encourages the predictions
for adjacent frames within the same segment to be similar. It captures the strong intra-segment
temporal structure in speech signals that complements the cost (1). Our final unsupervised cost
function combines the inter-segment and intra-segment distribution matching via:
min { J(θ)，Jodm(Θ) + λJps(θ)}	(3)
θ
where λ is a parameter controlling the trade-off between the two parts. We call the cost function
J(θ) Segmental Empirical-ODM as it captures the segmental structure through the inter-segment
and intra-segment terms. To optimize this cost function, we sample a sequence τ at the beginning
of each epoch and applies stochastic gradient descent (SGD) with momentum to update θ. Note
that in Jodm(θ), there is an empirical average over all K segments in PT(z), which is inside the
logarithmic function. This makes stochastic gradient descent intrinsically biased if we also sample
this empirical average by a mini-batch average. To alleviate this effect, we use a large mini-batch
size to estimate the stochastic gradients.
Note that our method directly optimizes the classifier p§ (yt|xt) that takes the raw acoustic feature
vector xt (e.g., MFCC features) and maps it into output space. This is different from the previous
work (Liu et al., 2018), which first performs clustering in the speech space and then maps the clusters
into output space using adversarial training. This makes its performance upper-bounded by the
purity of the initial clusters since input frames of different phonemes may be mapped into the same
cluster. In contrast, our algorithm is end-to-end trained without using a separate clustering algorithm.
This enables us to outperform the cluster purity upper bound, as shown in our experiment section.
2.3	Segmentation b oundary refinement using the classification model
In this section, we develop an approach to refining the estimated segmentation boundaries {bt} using
a learned framewise phoneme classifier pθ (yt |xt). More formally, for each input utterance sequence
x = (x1, . . . , xT), we would like to infer the corresponding boundary sequence b = (b1, . . . , bT).
We propose a simple yet effective MAP estimation strategy by recognizing the fact that bt = I(yt 6=
yt-1). Therefore, we can perform an MAP estimate for y = (y1 , . . . , yT ) and then predict the
boundaries by bt = I(yt 6= yt-1). The MAP estimator ofy can be expressed as (see Appendix A):
T
arg max p(y∣x) = argmax Y p(yt∣yι, ...,yt-ι) pθ yt ：t	(4)
y	y t=1	p(yt)
Note that p(yt|y1, . . . , yt-1) is the transition probability of the frame labels. Assuming that yt
belongs to the i-th segment, we can express p(yt|y1, . . . , yt-1) as:
p(yt|y1, . . . ,yt-1) = I(yt=yt-1)p(yt=yt-1)+I(yt 6= yt-1)p(yt 6=yt-1)pLM(qi=yt|q1:i-1)
= I(yt = yt-1)p(bt = 0) +I(yt 6=yt-1)p(bt=1)pLM(qi=yt|q1:i-1)	(5)
where q1:i-1 denotes the previous i - 1 phonemes that the sequence y1, . . . , yt-1 has traversed.
The first term in (5) characterizes the probability that yt stays in the same phoneme segment as
yt-1 and the second term defines the probability that yt belongs to a new phoneme segment. Note
that pLM(qi = yt|q1:i-1) in (5) could be obtained from the phoneme language model. It remains
to estimate p(bt = 0) and p(bt = 1), which we approximate by p(bt = 0|x) and p(bt = 1|x),
respectively. To obtain p(bt = 1|x), we leverage the work of Wang et al. (2017), which shows that
the temporal structure of the gate signals in a gated RNN (GRNN) auto-encoder is highly correlated
with phoneme boundaries. Therefore, we apply a sigmoid function to the Difference GAS value
(defined as Wang et al. (2017)) to obtain p(bt = 1|x). With all these information, we substitute (5)
into (4) and perform a beam search to solve (4) for an approximate MAP estimate of y. It follows
that bt = I(yt = yt-ι) and We have refined the boundaries using pθ(yt |xt).
4
Published as a conference paper at ICLR 2019
Figure 2: Self-validation metric. (a) The learning curves of the self-validation loss and the validation
FER. (b) The self-validation loss and the validation FER for different values of λ in (3).
2.4	Alternating Training Algorithm
Our unsupervised learning algorithm for pθ (yt |xt) alternates between the above two steps of estimat-
ing θ for a given b and refining b for a given θ . The overall algorithm is summarized in Algorithm 1.
We initialize the algorithm by thresholding the temporal update gate activation in Wang et al. (2017)
to obtain an initial rough estimate for b. After the training converges,3 we could apply the unsu-
pervised HMM training technique discussed in Section 3 to further boost the performance. Note
that although the training process requires boundary estimation, at testing stage, it is not necessary
because the learned pθ(yt ∣xt) could be used in standard speech decoders just as supervised models.
Algorithm 1: Training Algorithm
Input: Phoneme language model pLM (z), Training data Dx , initial boundary binit obtained by using
techniques proposed in Wang et al. (2017).
Output: Model parameter θ
1	Initialization for parameters θ .
2	while not converged do
3	Given a set of boundaries b, obtain a new θ by optimizing (3).
4	Given the model parameter θ, obtain a new estimate for the boundaries b by optimizing (4).
5	end
2.5 Unsupervised Model Selection
Since there are no labeled data during the training process, we need to develop an unsupervised
self-validation metric to perform model selection. We propose to use the value of the loss func-
tion (1) on a heldout validation set (including only input features) to perform model selection. This
self-validation loss gives us an estimate of which model configuration is better, and is used to a)
determine when to stop training, and b) to select the best hyper-parameters. To validate the effec-
tiveness for our self-validation loss, we show the learning curves of this self-validation loss and the
validation frame error rate in Figure 2(a). We observe that the self-validation loss aligns well with
the true validation error. Furthermore, in Figure 2(b) we plot the self-validation loss and the vali-
dation FER for different values of λ, which shows that the two metrics are highly correlated. The
results demonstrate that the self-validation loss can be effectively used to select a good model.
3	Unsupervised HMM Training
To further improve the performance of proposed unsupervised speech recognition system, we ex-
plore the semi-supervised hidden Markov model (HMM) training strategy (Zavaliagkos et al., 1998;
Kemp and Waibel, 1999) that has commonly been used in speech recognition. The semi-supervised
HMM training is an effective technique where a seed model trained on a relatively small amount
3We observe in our experiments that two iterations are sufficient to converge.
5
Published as a conference paper at ICLR 2019
of labeled speech data is used for providing labels for larger amount of non-transcribed speech data
for iterative model refinement. A major difference of the HMM training strategy used in this work
compared to the ones used in semi-supervised learning is that we use the transcripts generated from
proposed unsupervised speech recognition system (i.e., predicted labels for 3969 TIMIT training
utterances) for bootstrapping the training of HMM-based models. Therefore, the training of HMM
models in this work does not require any manually provided supervised information. The training
of HMM based speech recognition models follows the standard recipes in Kaldi speech recognition
toolkit (Povey et al., 2011). We experimented with monophone and triphone models with MFCC
feature as input as well as more advanced speaker adaptive training (SAT) (Matsoukas et al., 1997)
approach with feature space maximum likelihood linear regression (fMLLR) (Gales et al.) as input.
4	Experiments
4.1	Experiment setting
We perform experiments on the TIMIT dataset where 6300 prompted English speech sentences
are recorded. The preparation of training and test sets follow the standard protocol of the TIMIT
dataset. The phoneme transcription of these utterances are manually segmented and labelled with a
lexicon of 61 distinct phoneme classes. These phoneme labels are mapped to 39 phoneme classes
for scoring phone error rate (Lee and Hon, 1989). We use 39 dimensional feature vectors including
13 mel-frequency cepstral coefficients (MFCC) plus its acceleration features that are extracted with
25 ms Hamming window at 10 ms interval. The classifier pθ (yt |xt) is modeled by a fully connected
neural network with one hidden layer of 512 ReLU units. The input to the neural network is a
concatenation of frames within a context window of size 11. We follow the default hyper-parameters
in Wang et al. (2017) to estimate the phoneme boundaries, which are used to initialize our algorithm.
The optimization of (3) is performed with momentum SGD with a fixed schedule of increasing batch
size from 5000 to 20000. λ in (3) is chosen to be 10-5. We use both frame error rate (FER) and
phone error rate (PER) as our evaluation metrics. Details of the experiment setting and other hyper-
parameters can be found in Appendix B.
4.2	Baseline Methods
Adversarial Mapping The first baseline we consider is the work by Liu et al. (2018), which
learns an unsupervised embedding by a sequence-to-sequence autoencoder followed by k-means
clustering. Each cluster is then mapped to a phoneme by adversarial training between the cluster
sequences and the phoneme sequence. The phoneme boundaries are given by a supervised oracle.
Cluster Purity The accuracy of Adversarial Mapping (Liu et al., 2018) is upper-bounded by the
cluster purity, which is the frame accuracy when assigning all the frames in each cluster to its most
frequent phonemes. It is a supervised baseline since it relies on the phoneme labels. We show the
cluster purity for 1000 clusters, which is the largest number of clusters used by Liu et al. (2018).
Supervised Neural Network We train a supervised neural network with the same architecture as
our unsupervised model with standard cross-entropy loss.
Supervised RNN Transducer It is one of the state-of-the-art methods, which learns a BiLSTM-
RNN Transducer with supervised learning (Graves et al., 2013).
4.3	Experiment Results
Unsupervised speech recognition with oracle boundary In our proposed unsupervised learning
algorithm, we use the cost function (3) to train the classifier, which is different from the cross entropy
cost in supervised learning. To examine the effect of replacing cross entropy with this new unsuper-
vised cost function, we first conduct experiments under the oracle phoneme boundaries. This setting
also allows us to compare our method to the one in Liu et al. (2018), which assumes oracle phoneme
boundaries. Specifically, we consider two settings. In the first setting, we follow the standard TIMIT
partition to divide the training data into a training and validation sets of 3696 and 400 utterances,
respectively. We use the phoneme transcription of the 3696 utterances to train our language model
pLM (z) and call this setting “matching language model”. Then we use this learned pLM (z) together
with the 3696 input utterances to train our model by minimizing (3). In the second setting, we divide
6
Published as a conference paper at ICLR 2019
Language Model		Matcthing		I Non-MatCthing	
Evaluation Metric	FER*	FER	PER	FER	PER
Supervised Methods					
RNN Transducer (Graves et al., 2013)	一	—	17.7	—	一
Supervised Neural Network	35.5	31.0	30.2	31.7	31.1
Cluster Purity (1000) (Liu et al., 2018)	41.0	—	—	—	—
Unsupervised Methods					
Adversarial Mapping (Liu et al., 2018)	47.5	—	—	—	—
Our Model	38.2	33.3	32.5	40.0	40.1
Table 1: Phoneme classification results when phoneme boundaries are given by a supervised oracle.
Language Model	Matcthing		I Non-Matcthing	
Evaluation MetriC	FER	PER	FER	PER
Supervised Methods				
RNN Transducer (Graves et al., 2013)	一	17.7	—	—
Supervised Neural Network	31.0	30.2	31.7	31.1
Unsupervised Methods				
Our Model: 1st iteration	47.4	47.0	63.5	61.7
Our Model: 2nd iteration	45.4	42.6	51.6	49.1
Our Model: 2nd iteration + HMM (mono)	一	41.5	—	44.7
Our Model: 2nd iteration + HMM (tri)	一	39.4	—	44.9
Our Model: 2nd iteration + HMM (tri + SAT)	一	36.5	—	41.6
Table 2: Results for fully unsupervised phoneme classification.
the data into a training and a validation sets of 3000 and 1096 utterances, respectively. We train our
language model pLM (z) on the phoneme transcription of the 1096 utterances, and use the learned
pLM (z) with the other 3000 input utterances to train our model by minimizing (3). In this setting,
the training corpus for pLM (z) does not overlap with the 3000 input training utterances, and we call
it “non-matching language model”. Note that both settings are unsupervised since we do not use
any phoneme label in our training process. The only difference is the source of the language model.
The results of our algorithm and other baselines are summarized in Table 1. Although we are still
far away from the state-of-the-art, in the matching LM setting, the performance of our algorithm
(32.5% PER) is approaching that of the supervised system with the same model architecture (30.2%
PER). This is an encouraging result, showing that replacing the supervised cross entropy loss with
our unsupervised learning cost does not degrade the performance much. On the other hand, when
we use non-matching LM, the gap becomes larger (40.1% vs 31.1% in PER). We think the reason
is due to the discrepancy of the output distributions between the two sets and the reduced training
corpus for pLM (z). We believe such a discrepancy could be alleviated by using a large-scale dataset.
Other than the standard FER and PER, we additionally show the evaluation result for FER* where
the starting and ending silences are removed following the setting in Liu et al. (2018). We observe
that our approach significantly outperforms the unsupervised Adversarial Mapping method in Liu
et al. (2018), and even outperforms the Cluster Purity (supervised upper bound in Liu et al. (2018)).
This result is not surprising since the clustering does not exploit the output distribution, and may
group inputs of different phonemes into the same cluster.
Fully unsupervised speech recognition We now consider the fully unsupervised setting where
only input speech features and a language model is given. The phoneme boundaries are not given
and has to be estimated in an unsupervised manner using our Algorithm 1. We show the quality of
the learned model after each iteration of the learning process in Table 2. And we observe that our
iteration process improves the results by a great margin especially in the non-matching LM case,
significantly lowering the FER and PER by over 10%. This demonstrates that our boundary refining
process has resulted in a better set of boundaries, which greatly improves the output distribution
7
Published as a conference paper at ICLR 2019
Evaluation Metric	Recall	Precision	F-score	R-value
Dusan and Rabiner (2006)	75.2	66.8	70.8	—
Qiao et al. (2008)	77.5	76.3	76.9	—
Lee and Glass (2012)	76.2	76.4	76.3	—
Rasanen (2014)	74.0	70.0	73.0	76.0
Hoang and Wang (2015)	—	—	78.2	81.1
Michel et al. (2016)	74.8	81.9	78.2	80.1
Wang et al. (2017)	78.2	82.2	80.1	82.6
Ours refined boundaries	80.9	84.3	82.6	84.8
Table 3: Results for unsupervised phoneme boundary segmentation.
matching. Moreover, we also report the results of using unsupervised HMM training where the PER
can be further improved. In the matching LM setting, HMM training with monophone, triphone,
and speaker adaptation training (SAT) improves the PER by a similar amount. In the non-matching
LM setting, HMM training significantly improves the PER, and SAT additionally improves 3% in
PER. Overall, our hybrid system with matching and non-matching LM achieved 36.5% and 41.6%
PER, respectively, which is only 10% below the supervised system of the same architecture.
Unsupervised Phoneme Segmentation To understand how much our proposed boundary refine-
ment method in Section 2.3 improves the segmentation quality, we follow the setting in previous
works and report in Table 3 the recall, precision, F-score, and R-value with a 20-ms tolerance win-
dow on TIMIT’s training set (Scharenborg et al., 2010; Versteegh et al., 2016; Rasanen, 2014). We
compare our results (obtained with matching LM) with several unsupervised phoneme segmenta-
tion methods (Dusan and Rabiner, 2006; Qiao et al., 2008; Lee and Glass, 2012; Rasanen, 2014;
Hoang and Wang, 2015; Michel et al., 2016; Wang et al., 2017). Note that our refined segmentation
significantly improves over the initial boundaries generated by Wang et al. (2017) and also outper-
forms other baselines. This result also confirms its contribution to the much improved phoneme
recognition performance in the 2nd iteration (see “Our Model: 2nd iteration” in Table 2) to the 1st
iteration. However, we emphasize that our method is designed towards unsupervised speech recog-
nition rather than unsupervised phoneme segmentation. Estimating the segmentation boundary only
serves as an auxiliary task to enable the unsupervised learning of the recognition model. And in the
testing stage, there is no need to estimate the segmentation boundaries. Instead, our trained model
could be directly used with a speech decoder just as any supervised recognition model would do.
Further analysis We include some further experiments and analysis in Appendix C, where we
show the importance of the frame smoothness term in (3). We also compare the performance of our
unsupervised algorithm to supervised learning with different amounts of labeled training data.
5	Related Work
Unsupervised sequence-to-sequence learning Recently, unsupervised sequence-to-sequence
learning has achieved great success in several problems. Liu et al. (2017) showed that it is possible
to learn a sequence classifier without any labeled data by exploiting the output sequential structure
using an unsupervised cost function named Empirical-ODM. Artetxe et al. (2018) and Lample et al.
(2018) showed that unsupervised neural machine translation (uNMT) systems can be achieved by
utilizing cross-lingual alignments and an adversarial structure without any form of parallel informa-
tion. The success in the unsupervised sequence-to-sequence learning in various applications shed
light on building our fully unsupervised speech recognition system. In particular, our work extends
the Empirical-ODM in Liu et al. (2017) to problem with segmental structures.
Unsupervised speech segmentation One line of unsupervised segmentation methods designs ro-
bust acoustic features that are likely to remain stable within a phoneme, and capture the change of
8
Published as a conference paper at ICLR 2019
features for phoneme boundaries (Esposito and Aversano, 2005; Hoang and Wang, 2015; Khanagha
et al., 2014; Rasanen et al., 2011; Michel et al., 2016; Wang et al., 2017). Another line of research
uses a simpler segmentation method as an initialization, and jointly trains the segmenting and acous-
tic models for phonemes or words (Kamper et al., 2015; Glass, 2003; Siu et al., 2014; Lee and Glass,
2012). Qiao et al. (2008) use dynamic programming methods in order to the derive optimal segmen-
tation, but requires the number of segments and is not fully unsupervised. In Wang et al. (2017), the
authors use the update gate of a GRNN autoencoder to discover the phoneme boundaries.
Unsupervised spoken term discovery Recently, the discovery of acoustic tokens including sub-
word and word units has become a popular research topic (Dunbar et al. (2017); Versteegh et al.
(2016); Burget et al.). The term “Spoken term discovery” includes lexicon discovery, word seg-
mentation, and subword matching (Dunbar et al. (2017)). The standard approaches segment audio
signals that are acoustically similar, and cluster the obtained segmented signals (Lee and Glass,
2012; Glass, 2012; Park and Glass, 2008; Driesen et al., 2012). Walter et al. (2013) uses the discov-
ered unit index sequence as the transcription for the acoustic model training, similar to the HMM
training in section 3. Kamper et al. (2017) iterates between the clustering and segmentation steps.
Ondel et al. (2016) improves upon previous methods by replacing Gibbs sampling by variational in-
ference, and Ondel et al. (2017) further improves the result by including a bigram language model.
The effectiveness of these approaches has been demonstrated on query-by-example spoken term de-
tection or by calculating the normalized mutual information between the self-discovered units and
the actual labels. Overall, these methods differ from our method in that they segment and cluster the
raw speech signals to self-discovered units, but does not recognize them into phoneme or word la-
bels directly. More recently, Chung et al. (2018) show that unsupervised spoken word classification
is possible by using adversarial cross-modal alignments similar to that in uNMT systems.
Unsupervised speech recognition with oracle segmentation There have been several attempts
(Liu et al., 2018; Chen et al., 2018) on building an unsupervised speech recognition model inspired
by the success of the uNMT. These methods first learn an embedding from the acoustic data, and then
map the clustered embeddings to the output space by either adversarial training or iterative mapping.
In contrast, our approach learns a neural network model that directly maps the raw acoustic features
into the output space by optimizing the Segmental Empirical-ODM cost, and outperforms the upper
bound of the above cluster-based approaches. Furthermore, all methods in Liu et al. (2018); Chen
et al. (2018) assume that the phoneme boundaries are given by a supervised oracle. In contrast, our
method iteratively estimates the boundaries without any labeled data, making it fully unsupervised.
6	Conclusion
We have developed a fully unsupervised learning algorithm for phoneme recognition. The algorithm
alternates between two steps: (i) learn a phoneme classifier for a given set of phoneme segmenta-
tion boundaries, and (ii) refining the phoneme boundaries based on a given classifier. For the first
step, we developed a novel unsupervised cost function named Segmental Empirical-ODM by gen-
eralizing the work (Liu et al., 2017) to segmental structures. For the second step, we developed
an approximate MAP approach to refining the boundaries obtained from Wang et al. (2017). Our
experimental results on TIMIT phoneme recognition task demonstrate the success of a fully unsu-
pervised phoneme recognition system. Although the fully unsupervised system is still far away from
the state-of-the-art supervised methods (e.g., supervised RNN transducer), we show that with oracle
boundaries the performance of our algorithm could approach that of the supervised system with the
same model architecture. This demonstrates the potential of our method if, in future work, we can
further improve the accuracy of boundary estimation. We want to further point out that the tech-
niques we proposed in this paper, although was evaluated in speech recognition, can be exploited
to attack other similar sequence recognition problems where the source and destination sequences
have different lengths and labels are not available or hard to get.
References
Yu Liu, Jianshu Chen, and Li Deng. Unsupervised sequence classification using sequential output
statistics. In Advances in Neural Information Processing Systems, pages 3550-3559, 2017.
9
Published as a conference paper at ICLR 2019
Yu-Hsuan Wang, Cheng-Tao Chung, and Hung-yi Lee. Gate activation signal analysis for
gated recurrent neural networks and its correlation with phoneme boundaries. arXiv preprint
arXiv:1703.07588, 2017.
Wayne Xiong, Jasha Droppo, Xuedong Huang, Frank Seide, Mike Seltzer, Andreas Stolcke, Dong
Yu, and Geoffrey Zweig. Achieving human parity in conversational speech recognition. arXiv
preprint arXiv:1610.05256, 2016.
Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly,
Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep neural networks
for acoustic modeling in speech recognition: The shared views of four research groups. IEEE
Signal processing magazine, 29(6):82-97, 2012.
George E Dahl, Dong Yu, Li Deng, and Alex Acero. Context-dependent pre-trained deep neural
networks for large-vocabulary speech recognition. IEEE Transactions on audio, speech, and
language processing, 20(1):30-42, 2012.
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recur-
rent neural networks. In Acoustics, speech and signal processing (icassp), 2013 ieee international
conference on, pages 6645-6649. IEEE, 2013.
Alex Graves, Santiago Fernandez, FaUstino Gomez, and Jurgen Schmidhuber. Connectionist tem-
poral classification: labelling unsegmented sequence data with recurrent neural networks. In
Proceedings of the 23rd international conference on Machine learning, pages 369-376. ACM,
2006.
Alex Graves. Sequence transduction with recurrent neural networks. arXiv preprint
arXiv:1211.3711, 2012.
Mikel Artetxe, Gorka Labaka, Eneko Agirre, and Kyunghyun Cho. Unsupervised neural machine
translation. In Proc. ICLR, 2018.
Guillaume Lample, Ludovic Denoyer, and Marc’Aurelio Ranzato. Unsupervised machine transla-
tion using monolingual corpora only. In Proc. ICLR, 2018.
Da-Rong Liu, Kuan-Yu Chen, Hung-yi Lee, and Lin-Shan Lee. Completely unsupervised phoneme
recognition by adversarially learning mapping relationships from audio embeddings. In Inter-
speech 2018, 19th Annual Conference of the International Speech Communication Association,
Hyderabad, India, 2-6 September 2018., pages 3748-3752, 2018.
George Zavaliagkos, Man-Hung Siu, Thomas Colthurst, and Jayadev Billa. Using untranscribed
training data to improve performance. In Fifth International Conference on Spoken Language
Processing, 1998.
Thomas Kemp and Alex Waibel. Unsupervised training ofa speech recognizer: Recent experiments.
In Sixth European Conference on Speech Communication and Technology, 1999.
Udhyakumar Nallasamy, Florian Metze, and Tanja Schultz. Active learning for accent adaptation
in automatic speech recognition. In Spoken Language Technology Workshop (SLT), 2012 IEEE,
pages 360-365. IEEE, 2012.
Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra Goel,
Mirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz, et al. The kaldi speech recognition
toolkit. In IEEE 2011 workshop on automatic speech recognition and understanding, number
EPFL-CONF-192584. IEEE Signal Processing Society, 2011.
Spyros Matsoukas, Rich Schwartz, Hubert Jin, and Long Nguyen. Practical implementations of
speaker-adaptive training. In DARPA Speech Recognition Workshop. Citeseer, 1997.
Mark JF Gales et al. Maximum likelihood linear transformations for hmm-based speech recognition.
K-F Lee and H-W Hon. Speaker-independent phone recognition using hidden markov models. IEEE
Transactions on Acoustics, Speech, and Signal Processing, 37(11):1641-1648, 1989.
10
Published as a conference paper at ICLR 2019
Sorin Dusan and Lawrence Rabiner. On the relation between maximum spectral transition positions
and phone boundaries. In Ninth International Conference on Spoken Language Processing, 2006.
Yu Qiao, Naoya Shimomura, and Nobuaki Minematsu. Unsupervised optimal phoneme segmenta-
tion: Objectives, algorithm and comparisons. In Acoustics, Speech and Signal Processing, 2008.
ICASSP 2008. IEEE International Conference on, pages 3989-3992. IEEE, 2008.
Chia-ying Lee and James Glass. A nonparametric bayesian approach to acoustic model discovery. In
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long
Papers-Volume 1, pages 40-49. Association for Computational Linguistics, 2012.
Okko Rasanen. Basic cuts revisited: Temporal segmentation of speech into phone-like units with
statistical learning at a pre-linguistic level. In Proceedings of the Annual Meeting of the Cognitive
Science Society, volume 36, 2014.
Dac-Thang Hoang and Hsiao-Chuan Wang. Blind phone segmentation based on spectral change
detection using legendre polynomial approximation. The Journal of the Acoustical Society of
America, 137(2):797-805, 2015.
Paul Michel, Okko Rasanen, Roland Thiolliere, and Emmanuel Dupoux. Blind phoneme Segmenta-
tion with temporal prediction errors. arXiv preprint arXiv:1608.00508, 2016.
Odette Scharenborg, Vincent Wan, and Mirjam Ernestus. Unsupervised speech segmentation: An
analysis of the hypothesized phone boundaries. The Journal of the Acoustical Society of America,
127(2):1084-1095, 2010.
Maarten Versteegh, Xavier Anguera, Aren Jansen, and Emmanuel Dupoux. The zero resource
speech challenge 2015: Proposed approaches and results. Procedia Computer Science, 81:67-
72, 2016.
Anna Esposito and Guido Aversano. Text independent methods for speech segmentation. In Non-
linear Speech Modeling and Applications, pages 261-290. Springer, 2005.
Vahid Khanagha, Khalid Daoudi, Oriol Pont, and Hussein Yahia. Phonetic segmentation of speech
signal using local singularity analysis. Digital Signal Processing, 35:86-94, 2014.
Okko Rasanen, Unto Laine, and Toomas Altosaar. Blind segmentation of speech using non-linear
filtering methods. In Speech Technologies. InTech, 2011.
Herman Kamper, Aren Jansen, and Sharon Goldwater. Fully unsupervised small-vocabulary speech
recognition using a segmental bayesian model. In Sixteenth Annual Conference of the Interna-
tional Speech Communication Association, 2015.
James R Glass. A probabilistic framework for segment-based speech recognition. Computer Speech
& Language, 17(2-3):137-152, 2003.
Man-hung Siu, Herbert Gish, Arthur Chan, William Belfield, and Steve Lowe. Unsupervised train-
ing of an hmm-based self-organizing unit recognizer with applications to topic classification and
keyword discovery. Computer Speech & Language, 28(1):210-223, 2014.
Ewan Dunbar, Xuan Nga Cao, Juan Benjumea, Julien Karadayi, Mathieu Bernard, Laurent Besacier,
Xavier Anguera, and Emmanuel Dupoux. The zero resource speech challenge 2017. In Automatic
Speech Recognition and Understanding Workshop (ASRU), 2017 IEEE, pages 323-330. IEEE,
2017.
Lukas Burget, Sanjeev Khudanpur, Najim Dehak, Jan Trmal, Reinhold Haeb-Umbach, Graham
Neubig, Shinji Watanabe, Daichi Mochihashi, Takahiro Shinozaki, Ming Sun, et al. Building
speech recognition system from untranscribed data report from jhu workshop 2016.
James Glass. Towards unsupervised speech processing. In Information Science, Signal Processing
and their Applications (ISSPA), 2012 11th International Conference on, pages 1-4. IEEE, 2012.
Alex S Park and James R Glass. Unsupervised pattern discovery in speech. IEEE Transactions on
Audio, Speech, and Language Processing, 16(1):186-197, 2008.
11
Published as a conference paper at ICLR 2019
Joris Driesen et al. Fast word acquisition in an nmf-based learning framework. In Acoustics, Speech
and Signal Processing (ICASSP), 2012 IEEE International Conference on, pages 5137-5140.
IEEE, 2012.
Oliver Walter, Timo Korthals, Reinhold Haeb-Umbach, and Bhiksha Raj. A hierarchical system
for word discovery exploiting dtw-based initialization. In Automatic Speech Recognition and
Understanding (ASRU), 2013 IEEE Workshop on, pages 386-391. IEEE, 2013.
Herman Kamper, Karen Livescu, and Sharon Goldwater. An embedded segmental k-means model
for unsupervised segmentation and clustering of speech. In Automatic Speech Recognition and
Understanding Workshop (ASRU), 2017 IEEE, pages 719-726. IEEE, 2017.
Lucas OndeL Lukas BUrgeL and Jan Cernocky. Variational inference for acoustic unit discovery.
Procedia Computer Science, 81:80-86, 2016.
Lucas OndeL Lukas Burget, Jan Cernocky, and Santosh Kesiraju. BayeSian phonotactic language
model for acoustic unit discovery. In Acoustics, Speech and Signal Processing (ICASSP), 2017
IEEE International Conference on, pages 5750-5754. IEEE, 2017.
Yu-An Chung, Wei-Hung Weng, Schrasing Tong, and James Glass. Unsupervised cross-modal
alignment of speech and text embedding spaces. arXiv preprint arXiv:1805.07467, 2018.
Yi-Chen Chen, Chia-Hao Shen, Sung-Feng Huang, and Hung-yi Lee. Towards unsupervised
automatic speech recognition trained by unaligned speech and text only. arXiv preprint
arXiv:1803.10952, 2018.
12
Published as a conference paper at ICLR 2019
Supplementary Material
A Derivation of the MAP estimate for the segmentation
B OUNDARIES
In this appendix, we derive the MAP estimate for y = (y1, . . . , yT) given an input utterance se-
quence x = (y1, . . . , yT). Specifically, we have
arg max p(y|x) = arg max p(y, x)
yy
= arg max p(y)p(x|y)
y
T
=) arg max TT p(yt∣yι,..., yt-i )pθ (xt∣yt)
y
t=1
T
TT / I	pθθ(ytlxt)p(χt)
=argmax∣ [p(yt∣yι,...,yt-ι)---Py~)-----
T
(b)	… TΓ / I	PP((yt|Xt)	心
=argmax∣[p(yt∣yι,...,yt-ι) 口@)	(6)
where in step (a) we approximate the p(x|y) by its factored form and in step (b) we dropped the
constant term that is independent of y .
B	Detailed Experiment setting
We perform experiments on the TIMIT dataset where 6300 prompted English speech utterances are
recorded. The phoneme transcription of these utterances are manually segmented and labelled with
a lexicon of 61 distinct phoneme classes, where we compact the 61 phoneme classes into 48 phone
classes and train the language model with the validation dataset, which is later used to train our main
algorithm. These 48 phoneme classes are mapped to 39 phoneme classes for scoring phone error
rate (Lee and Hon, 1989).
The 39 dimensional feature vectors including 13 mel-frequency cepstral coefficients (MFCC) plus
its acceleration features that are extracted with 25 ms Hamming window at 10 ms interval. The
classifier Pθ is modeled by a fully connected neural network with one hidden layer of 512 ReLU
units. The input to the neural network is a concatenation of frames within a context window of size
11, and we repeat the starting or ending frames if the window has reached the start or end of the
sentence.
The optimization of (3) is performed with momentum SGD with momentum 0.9 and learning rate
of 10-3 with learning rate decay with a fixed schedule of increasing batchsize from 5000 to 20000
and decreasing temperature for softmax. The scheduler parameter is listed below: first 200 epochs
with batchsize 500 and temperature 1.0, next 300 epochs with batchsize 5000 and temperature 0.9,
followed by 300 epochs with batchsize 10000 and temperature 0.8, finally 300 epochs with batchsize
20000 and temperature 0.7. Whenever the batchsize is increased, we set the learning rate to the inital
learning rate value. The scheduling procedure is determined by self-validation, and is not extensively
tuned during the experiments.
In our experiments, we chose N = 5 for the N-gram in (1), and for computational issues we only
consider the most frequent 10000 5-grams. We do not observe any noticeable performance drop by
considering only the 10000 5-grams. Among the 5-gram language model PLM, 69553 5-grams (out
of 485 possible 5-grams) are non-zero, and the top 10000 5-grams account for almost half of the
probability. To sample τ , we use a standard truncated normal distribution for sampling the frame in
every segment, with some necessary scaling and rounding. The truncated distribution is to ensure
that our sampling will give us bounded frames that lie in the correct segment. This distribution can
also be replaced by the uniform distribution. To obtain p(bt = 1|x), we apply a sigmoid function to
the normalized Difference GAS value (defined as Wang et al. (2017)) that are local maxima. For the
Difference GAS value that are not local maxima, we set it to -0.1 to get a low probability.
13
Published as a conference paper at ICLR 2019
We randomly sample 10000 continuous frames to optimize (2), which is sampled every batch. λ
in (3) is chosen to be 10-5. We use both frame error rate (FER) and phone error rate (PER) as our
evaluation metrics. All phone error rate (PER) results reported has been obtained by a Kaldi decoder
by considering the per-frame softmax value and the language model, and the weight between the two
set to 1, which is fixed in all unsupervised setting.
C Additional experiments and analysis
First, we examine the importance of the frame smoothness term in (3) in the fully unsupervised
setting. In Figure 3(a), we show the FER of our model after the first iteration of Algorithm 1 for
different values of λ. Note that when λ is close to the order of 10-5, the result does not differ a
lot from the best result. However, when λ is set to zero, the performance degrades significantly.
This confirms the importance of incorporating the temporal structure of speech data into the cost
function, as discussed in Section 2.2. Second, we would like to study another important question
regarding our unsupervised learning method: how much labeled data is it equivalent to? In Figure
3(b), we show the supervised neural network with different sizes of training data, where x-axis is
the percentage of the original labeled set being used to train the model. We observe that with oracle
boundary and matching LM, our algorithm is equivalent to supervised learning with 30% labeled
data. With unsupervised boundary estimation, we still see a big performance loss. Therefore, it is
critical to improve the boundary estimation performance in our future work.
Λ FER
5 × 10-6 49.2
1 × IO-5 47.4
2 × 10~5 48.1
0	70.2
Supervised learning
Our model (given boundary, matching)
Our model (given boundary, non-matching)
Our model (fully unsupervised, 2nd iter, matching)
Our model (fully unsupervised, 2nd iter, non-matching)
(b)
(a)
Figure 3: Further analysis of our algorithm. (a) Influence of λ after the 1st iteration of fully unsu-
pervised learning with matching language model. (b) Equivalent amount of labeled data.
14