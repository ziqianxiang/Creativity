Published as a conference paper at ICLR 2019
Near-Optimal Representation Learning
for Hierarchical Reinforcement Learning
Ofir Nachum, Shixiang Gu, Honglak Lee & Sergey Levine*
Google Brain
{ofirnachum,shanegu,honglak,slevine}@google.com
Ab stract
We study the problem of representation learning in goal-conditioned hierarchi-
cal reinforcement learning. In such hierarchical structures, a higher-level con-
troller solves tasks by iteratively communicating goals which a lower-level policy
is trained to reach. Accordingly, the choice of representation - the mapping of
observation space to goal space - is crucial. To study this problem, We develop a
notion of sub-optimality of a representation, defined in terms of expected reward
of the optimal hierarchical policy using this representation. We derive expressions
Which bound the sub-optimality and shoW hoW these expressions can be translated
to representation learning objectives Which may be optimized in practice. Results
on a number of difficult continuous-control tasks shoW that our approach to rep-
resentation learning yields qualitatively better representations as Well as quantita-
tively better hierarchical policies, compared to existing methods.12
1 Introduction
Hierarchical reinforcement learning has long held the promise of extending the successes of existing
reinforcement learning (RL) methods (Gu et al., 2017; Schulman et al., 2015; Lillicrap et al., 2015)
to more complex, difficult, and temporally extended tasks (Parr & Russell, 1998; Sutton et al., 1999;
Barto & Mahadevan, 2003). Recently, goal-conditioned hierarchical designs, in Which higher-level
policies communicate goals to loWer-levels and loWer-level policies are reWarded for reaching states
(i.e. observations) Which are close to these desired goals, have emerged as an effective paradigm
for hierarchical RL (Nachum et al. (2018); Levy et al. (2017); Vezhnevets et al. (2017), inspired
by earlier Work Dayan & Hinton (1993); Schmidhuber & Wahnsiedler (1993)). In this hierarchical
design, representation learning is crucial; that is, a representation function must be chosen mapping
state observations to an abstract space. Goals (desired states) are then specified by the choice of a
point in this abstract space.
Previous Works have largely studied tWo Ways to choose the representation: learning the representa-
tion end-to-end together With the higher- and loWer-level policies (Vezhnevets et al., 2017), or using
the state space as-is for the goal space (i.e., the goal space is a subspace of the state space) (Nachum
et al., 2018; Levy et al., 2017). The former approach is appealing, but in practice often produces
poor results (see Nachum et al. (2018) and our oWn experiments), since the resulting representation
is under-defined; i.e., not all possible sub-tasks are expressible as goals in the space. On the other
hand, fixing the representation to be the full state means that no information is lost, but this choice
is difficult to scale to higher dimensions. For example, if the state observations are entire images,
the higher-level must output target images for the loWer-level, Which can be very difficult.
We instead study hoW unsupervised objectives can be used to train a representation that is more
concise than the full state, but also not as under-determined as in the end-to-end approach. In order
to do so in a principled manner, We propose a measure of sub-optimality of a given representation.
This measure aims to ansWer the question: HoW much does using the learned representation in place
of the full representation cause us to lose, in terms of expected reWard, against the optimal policy?
This question is important, because a useful representation Will compress the state, hopefully making
* Also at UC Berkeley.
1See videos at https://sites.google.com/view/representation- hrl
2Find open-source code at https://github.com/tensorflow/models/tree/master/
research/efficient-hrl
1
Published as a conference paper at ICLR 2019
the learning problem easier. At the same time, the compression might cause the representation to lose
information, making the optimal policy impossible to express. It is therefore critical to understand
how lossy a learned representation is, not in terms of reconstruction, but in terms of the ability to
represent near-optimal policies on top of this representation.
Our main theoretical result shows that, for a particular choice of representation learning objective,
we can learn representations for which the return of the hierarchical policy approaches the return of
the optimal policy within a bounded error. This suggests that, if the representation is learned with
a principled objective, the ‘lossy-ness’ in the resulting representation should not cause a decrease
in overall task performance. We then formulate a representation learning approach that optimizes
this bound. We further extend our result to the case of temporal abstraction, where the higher-level
controller only chooses new goals at fixed time intervals. To our knowledge, this is the first result
showing that hierarchical goal-setting policies with learned representations and temporal abstraction
can achieve bounded sub-optimality against the optimal policy. We further observe that the represen-
tation learning objective suggested by our theoretical result closely resembles several other recently
proposed objectives based on mutual information (van den Oord et al., 2018; Ishmael Belghazi et al.,
2018; Hjelm et al., 2018), suggesting an intriguing connection between mutual information and goal
representations for hierarchical RL. Results on a number of difficult continuous-control navigation
tasks show that our principled representation learning objective yields good qualitative and quanti-
tative performance compared to existing methods.
2	Framework
Following previous work (Nachum et al., 2018), we consider a two-level hierarchical policy on
an MDP M = (S, A, R, T ), in which the higher-level policy modulates the behavior of a lower-
level policy by choosing a desired goal state and rewarding the lower-level policy for reaching this
state. While prior work has used a sub-space of the state space as goals (Nachum et al., 2018),
in more general settings, some type of state representation is necessary. That is, consider a state
representation function f : S → Rd . A two-level hierarchical policy on M is composed of a
higher-level policy ∏hi(g∣s), where g ∈ G = Rd is the goal space, that samples a high-level action
(or goal) gt 〜∏hi(g∣st) every C steps, for fixed c. A non-stationary, goal-conditioned, lower-level
policy ∏io(a|st, gt, st+k, k) then translates these high-level actions into low-level actions at+k ∈ A
for k ∈ [0,c - 1]. The process is then repeated, beginning with the higher-level policy selecting
another goal according to st+c. The policy ∏io is trained using a goal-conditioned reward; e.g. the
reward of a transition g, s, s0 is -D(f (s0), g), where D is a distance function.
In this work we adopt a slightly differ-
ent interpretation of the lower-level pol-
icy and its relation to ∏hi. Every C steps,
the higher-level policy chooses a goal gt
based on a state st. We interpret this
state-goal pair as being mapped to a non-
stationary policy π(a∣st+k,k),∏ ∈ Π,
where Π denotes the set of all possible
c-step policies acting on M. We use Ψ
to denote this mapping from S X G to
Π. In other words, on every cth step, we
encounter some state St ∈ S. We use
the higher-level policy to sample a goal
Figure 1: The hierarchical design we consider.
gt ~ ∏hi(g∣st) and translate this to a policy ∏t = Ψ(st,gt). We then use ∏t to sample actions
at+k ~ ∏t(a∣st+k,k) for k ∈ [0, c - 1]. The process is then repeated from st+c.
Although the difference in this interpretation is subtle, the introduction of Ψ is crucial for our sub-
sequent analysis. The communication of gt is no longer as a goal which πhi desires to reach, but
rather more precisely, as an identifier to a low-level behavior which πhi desires to induce or activate.
The mapping Ψ is usually expressed as the result of an RL optimization over Π; e.g.,
c
ψ(St,g) = argmaχE YkTEPn (st+k∣st)[-Df(St+k ),g)],
π∈Π k=1
(1)
2
Published as a conference paper at ICLR 2019
where We use Pn (st+k |st) to denote the probability of being in state st+k after following ∏ for k
steps starting from st. We will consider variations on this low-level objective in later sections. From
Equation 1 it is clear how the choice of representation f affects Ψ (albeit indirectly).
We will restrict the environment reward function R to be defined only on states. We use Rmax to
denote the maximal absolute reward: Rmax = supS |R(s)|.
3	Hierarchical Policy Sub-Optimality
In the previous section, we introduced two-level policies where a higher-level policy πhi chooses
goals g, which are translated to lower-level behaviors via Ψ. The introduction of this hierarchy leads
to a natural question: How much do we lose by learning πhi which is only able to act on M via
Ψ? The choice of Ψ restricts the type and number of lower-level behaviors that the higher-level
policy can induce. Thus, the optimal policy on M is potentially not expressible by πhi . Despite the
potential lossy-ness of Ψ, can one still learn a hierarchical policy which is near-optimal?
To approach this question, we introduce a notion of sub-optimality with respect to the form of Ψ:
Let ∏hi(g∣s, Ψ) be the optimal higher-level policy acting on G and using Ψ as the mapping from G to
low-level behaviors. Let ∏芸e「be the corresponding full hierarchical policy on M. We will compare
∏hier to an optimal hierarchical policy ∏* agnostic to Ψ. To define ∏* we begin by introducing
an optimal higher-level policy ∏{J*(∏∣s) agnostic to Ψ; i.e. every C steps, ∏^* samples a low-level
behavior ∏ ∈ Π which is applied to M for the following C steps. In this way, ∏{^* may express all
possible low-level behaviors. We then denote ∏* as the full hierarchical policy resulting from ∏h*.
We would like to compare ∏hier to ∏*. A natural and common way to do so is in terms of state
values. Let V π (s) be the future value achieved by a policy π starting at state s. We define the
sub-optimality of Ψ as
SubOPt(Ψ) = sup Vπ* (S) — Vπhier (s).	(2)
s∈S
*，、
The state values V πhier (s) are determined by the form ofΨ, which is in turn determined by the choice
of representation f. However, none of these relationships are direct. It is unclear how a change in f
will result in a change to the sub-optimality. In the following section, we derive a series of bounds
which establish a more direct relationship between SubOpt(Ψ) and f. Our main result will show
that if one defines Ψ as a slight modification of the traditional objective given in Equation 1, then
one may translate sub-optimality of Ψ to a practical representation learning objective for f.
4	Good Representations Lead to Bounded Sub -Optimality
In this section, we provide proxy expressions that bound the sub-optimality induced by a specific
choice of Ψ. Our main result is Claim 4, which connects the sub-optimality of Ψ to both goal-
conditioned policy objectives (i.e., the objective in 1) and representation learning (i.e., an objective
for the function f).
4.1	SINGLE-STEPS (C = 1) AND DETERMINISTIC POLICIES
For ease of presentation, we begin by presenting our results in the restricted case of C = 1 and
deterministic lower-level policies. In this setting, the class of low-level policies Π may be taken
to be simply A, where a ∈ Π corresponds to a policy which always chooses action a. There is no
temporal abstraction: The higher-level policy chooses a high-level action g ∈ G at every step, which
is translated via Ψ to a low-level action a ∈ A. Our claims are based on quantifying how many of
the possible low-level behaviors (i.e., all possible state to state transitions) can be produced by Ψ
for different choices of g. To quantify this, we make use of an auxiliary inverse goal model 夕(s, a),
which aims to predict which goal g will cause Ψ to yield an action α = Ψ(s, g) that induces a next
state distribution P(s0∣s, a) similar to P(s0∣s, a).3 We have the following theorem, which bounds
the sub-optimality in terms of total variation divergences between P (s0 |s, a) and P(s0|s, αa):
Theorem 1. Ifthere exists 夕：S X A → G such that,
SuP DTV(P(s0∣s, a)||P(s0∣s,Ψ(s3(s, a)))) ≤ e,	(3)
s∈S,a∈A
then SubOPt(Ψ) ≤ CG where C =(/二产 Rmax.
3In a deterministic, C = 1 setting,g may be seen as a state-conditioned action abstraction mapping A → G.
3
Published as a conference paper at ICLR 2019
Proof. See Appendices A and B for all proofs.
Theorem 1 allows us to bound the sub-optimality of Ψ in terms of how recoverable the effect of
any action in A is, in terms of transition to the next state. One way to ensure that effects of actions
in A are recoverable is to have an invertible Ψ. That is, if there exists 夕：S X A → G such that
Ψ(s,夕(s, a)) = a for all s, a, then the sub-optimality of Ψ is 0.
However, in many cases it may not be desirable or feasible to have an invertible Ψ. Looking back at
Theorem 1, we emphasize that its statement requires only the effect of any action to be recoverable.
That is, for any s, ∈ S, a ∈ A, We require only that there exist some g ∈ G (given by 夕(s, a)) which
yields a similar next-state distribution. To this end, we have the following claim, which connects the
sub-optimality of Ψ to both representation learning and the form of the low-level objective.
Claim 2. Let ρ(s) be a prior and f,夕 be so that, for K(s0∣s, a) H ρ(s0) exp(-D(f (s0), φ(s, a))),4
sup DKL(P(s0|s, a)||K(s0|s, a)) ≤ 2/8.	(4)
s∈S,a∈A
If the low-level objective is defined as
Ψ(s, g) = arg max EP (s0|s,a)[-D(f (s0), g) + logρ(s0) - logP(s0|s, a)],	(5)
a∈A
then the sub-optimality of Ψ is bounded by C.
We provide an intuitive explanation of the statement of Claim 2. First, consider that the distribution
K(s0|s, a) appearing in Equation 4 may be interpreted as a dynamics model determined by f and
夕.By bounding the difference between the true dynamics P(s0∣s, a) and the dynamics K(s0∣s, a)
implied by f and 夕，Equation 4 states that the representation f should be chosen in such a way that
dynamics in representation space are roughly given by 夕(s, a). This is essentially a representation
learning objective for choosing f, and in Section 5 we describe how to optimize it in practice.
Moving on to Equation 5, we note that the form of Ψ here is only slightly different than the one-
step form of the standard goal-conditioned objective in Equation 1.4 5 * * Therefore, all together Claim 2
establishes a deep connection between representation learning (Equation 4), goal-conditioned policy
learning (Equation 5), and sub-optimality.
4.2 TEMPORAL ABSTRACTION (c ≥ 1) AND GENERAL POLICIES
We now move on to presenting the same results in the fully general, temporally abstracted setting, in
which the higher-level policy chooses a high-level action g ∈ G every c steps, which is transformed
via Ψ to a c-step lower-level behavior policy π ∈ Π. In this setting, the auxiliary inverse goal model
夕(s, ∏) is a mapping from S × Π to G and aims to predict which goal g will cause Ψ to yield a
policy π = Ψ(s,g) that induces future state distributions Pn(st+k |st) similar to Pn(st+k |st), for
k ∈ [1, c]. We weight the divergences between the distributions by weights wk = 1 for k < c and
Wk = (1 一 Y)T for k = c. We denote W = Pk=ι YkTwk. The analogue to Theorem 1 is as
follows:
Theorem 3. Consider a mapping 夕：S × Π → G and define Ek : S × Π → R for k ∈ [1, c] as,
Ck (St,π)= DTV (Pn (St+k lst)llPΨ(st,w(st,n)) (St+k lst))∙	⑹
If
1c
sup	= y"γk-1wkCk(st,∏) ≤ C,	(7)
st∈S,n∈∏ w M
then SubOPt(Ψ) ≤ Ce, where C = ɪ-YcRmaxw.
For the analogue to Claim 2, we simply replace the single-step KL divergences and low-level rewards
with a discounted weighted sum thereof:
4K may be interpreted as the conditional P (state = s0∣repr =夕(s, a)) of thejoint distribution P (state =
s0)P(repr = z|state = s0) = ρ(s0) exp(-D(f (s0), z))/Z for normalization constant Z.
5As evident in the proof of this Claim (see Appendix), the form of Equation 5 is specifically designed so
that it corresponds to a KL between P(s0∣s, a) and a distribution U(s0∣s,g) (X ρ(s0) exp(-D(f (s0), g)).
4
Published as a conference paper at ICLR 2019
Claim 4. Let ρ(s) be a prior over S. Let f,夕 be such that,
1c
sup	— ∖2γk-1WkDκL(P∏(st+k∣st)∣∣K(st+k∣st,∏)) ≤ €2/8,	(8)
st∈S,π∈Π W M
where K(st+k |st, ∏) H ρ(st+k)exp(-D(f (st+k),夕(st, ∏))).
If the low-level objective is defined as
c
Ψ(st,g) = arg max V YkTwkEPn ⑶十％ ∣st)[-D(f (st+k), g)+log ρ(st+k )-log Pn (st+k |«t)],⑼
π ∈Π k=1
then the sub-optimality of Ψ is bounded by C€.
Claim 4 is the main theoretical contribution of our work. As in the previous claim, we have a strong
statement, saying that if the low-level objective is defined as in Equation 9, then minimizing the
sub-optimality may be done by optimizing a representation learning objective based on Equation 8.
We emphasize that Claim 4 applies to any class of low-level policies Π, including either closed-loop
or open-loop policies.
5	Learning
We now have the mathematical foundations necessary to learn representations that are provably good
for use in hierarchical RL. We begin by elaborating on how we translate Equation 8 into a practical
training objective for f and auxiliary 夕(as well as a practical parameterization of policies ∏ as input
to 夕). We then continue to describe how one may train a lower-level policy to match the objective
presented in Equation 9. In this way, we may learn f and lower-level policy to directly optimize a
bound on the sub-optimality of Ψ. A pseudocode of the full algorithm is presented in the Appendix
(see Algorithm 1).
5.1	Learning Good Representations
Consider a representation function fθ : S → Rd and an auxiliary function 夕θ : S X Π → Rd,
parameterized by vector θ. In practice, these are separate neural networks: fθ`,中§?, θ = [θ1, θ2].
While the form of Equation 8 suggests to optimize a supremum over all st and π, in practice we
only have access to a replay buffer which stores experience s0 , a0 ,s1 , a1 , . . . sampled from our
hierarchical behavior policy. Therefore, we propose to choose st sampled uniformly from the replay
buffer and use the subsequent c actions at:t+c-1 as a representation of the policy π, where we
use at:t+c-1 to denote the sequence at, . . . , at+c-1 . Note that this is equivalent to setting the set of
candidate policies Π to Ac (i.e., Π is the set of c-step, deterministic, open-loop policies). This choice
additionally simplifies the possible structure of the function approximator used for 夕θ (a standard
neural net which takes inst and at:t+c-1 ). Our proposed representation learning objective is thus,
J(θ)
st,at:t+c-1〜replay[J(θ, St, at:t+c-1)],	(10)
where J(θ,st, at:t+c-1 ) will correspond to the inner part of the supremum in Equation 8.
We now define the inner objective J(θ,st, at:t+c-1 ). To simplify notation, we use Eθ(s0,s, π) =
exp(-D(fθ(s0),夕θ(s, π))) and use K(s0∣s, π) as the distribution over S such that K(s0∣s, π) H
ρ(0)Eθ(s0,s, π). Equation 8 suggests the following learning objective on eachst, π ≡ at:t+c-1 :
c
J(θ, st,∏) = X YkTwkDkl(P∏(st+k∣st)∣∣Kθ(st+k∣st,∏))	(11)
k=1
c
=B + X-Y k-1wk EPn(St+k∣st) [log Kθ (st+k lst,π)]	(12)
k=1
c
=B + E-YkTwkEPn(st+k∣st) [log Eθ(st+k,st,∏)] + YkTWk logES〜P [Eθ(S, st,∏)], (13)
k=1
where B is a constant. The gradient with respect to θ is then,
X -YkTwkEPn(st+k∣st) [Vθ logEθ(st+k, st, ∏)]+ YkTwkE蓝。IEE ^st;7]	(14)
E_[	Eg 〜。 [Eθ (s, st, π)]
5
Published as a conference paper at ICLR 2019
The first term of Equation 14 is straightforward to estimate using experienced st+1:t+k. We set ρ
to be the replay buffer distribution, so that the numerator of the second term is also straightforward.
We approximate the denominator of the second term using a mini-batch S of states independently
sampled from the replay buffer:
-	--，	」	,~, -I _ _，	、
Eg~ρ [Eθ(s,st,∏)] ≈∣s∣ 1 Pg∈e Eθ(s,st,∏).	(15)
This completes the description of our representation learning algorithm.
Connection to Mutual Information Estimators. The form of the objective we optimize (i.e.
Equation 13) is very similar to mutual information estimators, mostly CPC (van den Oord et al.,
2018). Indeed, one may interpret our objective as maximizing a mutual information MI(st+k; st, π)
via an energy function given by Eθ (st+k, st, π). The main differences between our approach and
these previous proposals are as follows: (1) Previous approaches maximize a mutual information
M I (st+k ; st) agnostic to actions or policy. (2) Previous approaches suggest to define the energy
function as exp(f (st+k)T Mk f (st)) for some matrix Mk, whereas our energy function is based on
the distance D used for low-level reward. (3) Our approach is provably good for use in hierarchical
RL, and hence our theoretical results may justify some of the good performance observed by others
using mutual information estimators for representation learning. Different approaches to translating
our theoretical findings to practical implementations may yield objectives more or less similar to
CPC, some of which perform better than others (see Appendix D).
5.2	Learning a Lower-Level Policy
Equation 9 suggests to optimize a policy ∏st,g(a∣st+k,k) for every st, g. This is equivalent to the
parameterization ∏io(a∣st,g,st+k,k), which is standard in goal-conditioned hierarchical designs.
Standard RL algorithms may be employed to maximize the low-level reward implied by Equation 9:
-D(f(st+k),g) + log ρ(st+k) - log Pn(St+k |st),	(16)
weighted by wk and where π corresponds to πlo when the state st and goal g are fixed. While the first
term ofEquation 16 is straightforward to compute, the log probabilities log ρ(st+k), log P∏ (st+k | st)
are in general unknown. To approach this issue, we take advantage of the representation learning
objective for f,夕.When f,夕 are optimized as dictated by Equation 8, We have
logPn(st+k∣St) ≈ logρ(st+k) - D(f(st+k),φ(st,∏)) - logEg~ρ[E(E, St,∏)].	(17)
We may therefore approximate the low-level reward as
-D(f (st+k),g) + D(f(st+k),φ(st,∏)) + logEs~ρ[E(E, St,∏)].	(18)
As in Section 5.1, we use the sampled actions at：t+c-1 to represent ∏ as input to 夕.We approximate
the third term of Equation 18 analogously to Equation 15. Note that this is a slight difference from
standard low-level rewards, which use only the first term of Equation 18 and are unweighted.
6	Related Work
Representation learning for RL has a rich and diverse existing literature, often interpreted as an
abstraction of the original MDP. Previous works have interpreted the hierarchy introduced in hi-
erarchical RL as an MDP abstraction of state, action, and temporal spaces (Sutton et al., 1999;
Dietterich, 2000; Thomas & Barto, 2012; Bacon et al., 2017). In goal-conditioned hierarchical de-
signs, although the representation is learned on states, it is in fact a form of action abstraction (since
goals g are high-level actions). While previous successful applications of goal-conditioned hier-
archical designs have either learned representations naively end-to-end (Vezhnevets et al., 2017),
or not learned them at all (Levy et al., 2017; Nachum et al., 2018), we take a principled approach
to representation learning in hierarchical RL, translating a bound on sub-optimality to a practical
learning objective.
Bounding sub-optimality in abstracted MDPs has a long history, from early work in theoretical
analysis on approximations to dynamic programming models (Whitt, 1978; Bertsekas & Castanon,
1989). Extensive theoretical work on state abstraction, also known as state aggregation or model
minimization, has been done in both operational research (Rogers et al., 1991; Van Roy, 2006) and
RL (Dean & Givan, 1997; Ravindran & Barto, 2002; Abel et al., 2017). Notably, Li et al. (2006)
6
Published as a conference paper at ICLR 2019
introduce a formalism for categorizing classic work on state abstractions such as bisimulation (Dean
& Givan, 1997) and homomorphism (Ravindran & Barto, 2002) based on what information is pre-
served, which is similar in spirit to our approach. Exact state abstractions (Li et al., 2006) incur no
performance loss (Dean & Givan, 1997; Ravindran & Barto, 2002), while their approximate variants
generally have bounded sub-optimality (Bertsekas & Castanon, 1989; Dean & Givan, 1997; Sorg &
Singh, 2009; Abel et al., 2017). While some of the prior work also focuses on learning state abstrac-
tions (Li et al., 2006; Sorg & Singh, 2009; Abel et al., 2017), they often exclusively apply to simple
MDP domains as they rely on techniques such as state partitioning or Q-value based aggregation,
which are difficult to scale to our experimented domains. Thus, the key differentiation of our work
from these prior works is that we derive bounds which may be translated to practical representa-
tion learning objectives. Our impressive results on difficult continuous-control, high-dimensional
domains is a testament to the potential impact of our theoretical findings.
Lastly, we note the similarity of our representation learning algorithm to recently introduced scal-
able mutual information maximization objectives such as CPC (van den Oord et al., 2018) and
MINE (Ishmael Belghazi et al., 2018). This is not a surprise, since maximizing mutual information
relates closely with maximum likelihood learning of energy-based models, and our bounds effec-
tively correspond to bounds based on model-based predictive errors, a basic family of bounds in
representation learning in MDPs (Sorg & Singh, 2009; Brunskill & Li, 2014; Abel et al., 2017). Al-
though similar information theoretic measures have been used previously for exploration in RL (Still
& Precup, 2012), to our knowledge, no prior work has connected these mutual information esti-
mators to representation learning in hierarchical RL, and ours is the first to formulate theoretical
guarantees on sub-optimality of the resulting representations in such a framework.
7	Experiments
We evaluate our proposed representation learning objective compared to a number of baselines:
•	XY: The oracle baseline which uses the x, y position of the agent as the representation.
•	VAE: A variational autoencoder (Kingma & Welling, 2013) on raw observations.
•	E2C: Embed to control (Watter et al., 2015). A method which uses variational objectives
to train a representation of states and actions which have locally linear dynamics.
•	E2E: End-to-end learning of the representation. The representation is fed as input to the
higher-level policy and learned using gradients from the RL objective.
•	Whole obs: The raw observation is used as the representation. No representation learning.
This is distinct from Nachum et al. (2018), in which a subset of the observation space was
pre-determined for use as the goal space.
Figure 2: Learned representations (2D embeddings) of our method and a number of variants on a
MuJoCo Ant Maze environment, with color gradient based on episode time-step (black for beginning
of episode, yellow for end). The ant travels from beginning to end of a ⊃-shaped corridor along an
x, y trajectory shown under XY. Without any supervision, our method is able to deduce this near-
ideal representation, even when the raw observation is given as a top-down image. Other approaches
are unable to properly recover a good representation.
7
Published as a conference paper at ICLR 2019
Ant Fall
Ant Block
Whole obs
XY (oracle)
E2E
Figure 3: Results of our method and a number of variants on a suite of tasks in 10M steps of training,
plotted according to median over 10 trials with 30th and 70th percentiles. We find that outside of
simple point environments, our method is the only one which can approach the performance of
oracle x, y representations. These results show that our method can be successful, even when the
representation is learned online concurrently while learning a hierarchical policy.
We evaluate on the following continuous-control MuJoCo (Todorov et al., 2012) tasks (see Ap-
pendix C for details):
•	Ant (or Point) Maze: An ant (or point mass) must navigate a ⊃-shaped corridor.
•	Ant Push: An ant must push a large block to the side to reach a point behind it.
•	Ant Fall: An ant must push a large block into a chasm so that it may walk over it to the
other side without falling.
•	Ant Block: An ant must push a small block to various locations in a square room.
•	Ant Block Maze: An ant must push a small block through a ⊃-shaped corridor.
In these tasks, the raw observation is the agent's x, y coordinates and orientation as well as local
coordinates and orientations of its limbs. In the Ant Block and Ant Block Maze environments We
also include the x, y coordinates and orientation of the block. We also experiment with more difficult
raw representations by replacing the x,y coordinates of the agent with a low-resolution 5 X 5 X 3
top-down image of the agent and its surroundings. These experiments are labeled 'Images’.
For the baseline representation learning methods which are agnostic to the RL training (VAE and
E2C), we provide comparative qualitative results in Figure 2. These representations are the result
Ant and block	Ant pushing small block through corridor	Representations
Figure 4: We investigate importance of various observation coordinates in learned representations on
a difficult block-moving task. In this task, a simulated robotic ant must move a small red block from
beginning to end of a ⊃-shaped corridor. Observations include both ant and block x, y coordinates.
We show the trajectory of the learned representations on the right (cyan). At four time steps, we also
plot the resulting representations after perturbing the observation’s ant coordinates (green) or the
observation’s block coordinates (magenta). The learned representations put a greater emphasis (i.e.,
higher sensitivity) on the block coordinates, which makes sense for this task as the external reward
is primarily determined by the position of the block.
8
Published as a conference paper at ICLR 2019
of taking a trained policy, fixing it, and using its sampled experience to learn 2D representations of
the raw observations. We find that our method can successfully deduce the underlying near-optimal
x, y representation, even when the raw observation is given as an image.
We provide quantitative results in Figure 3. In these experiments, the representation is learned
concurrently while learning a full hierarchical policy (according to the procedure in Nachum et al.
(2018)). Therefore, this setting is especially difficult since the representation learning must learn
good representations even when the behavior policy is very far from optimal. Accordingly, we find
that most baseline methods completely fail to make any progress. Only our proposed method is able
to approach the performance of the XY oracle.
For the ‘Block’ environments, we were curious what our representation learning objective would
learn, since the x, y coordinate of the agent is not the only near-optimal representation. For example,
another suitable representation is the x, y coordinates of the small block. To investigate this, we
plotted (Figure 4) the trajectory of the learned representations of a successful policy (cyan), along
with the representations of the same observations with agent x, y perturbed (green) or with block
x, y perturbed (magenta). We find that the learned representations greatly emphasize the block x, y
coordinates over the agent x, y coordinates, although in the beginning of the episode, there is a
healthy mix of the two.
8 Conclusion
We have presented a principled approach to representation learning in hierarchical RL. Our approach
is motivated by the desire to achieve maximum possible return, hence our notion of sub-optimality
is in terms of optimal state values. Although this notion of sub-optimality is intractable to optimize
directly, we are able to derive a mathematical relationship between it and a specific form of represen-
tation learning. Our resulting representation learning objective is practical and achieves impressive
results on a suite of high-dimensional, continuous-control tasks.
Acknowledgments
We thank Bo Dai, Luke Metz, and others on the Google Brain team for insightful comments and
discussions.
References
David Abel, D Ellis Hershkowitz, and Michael L Littman. Near optimal behavior via approximate
state abstraction. arXiv preprint arXiv:1701.04113, 2017.
Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. arXiv
preprint arXiv:1705.10528, 2017.
Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In AAAI,, pp. 1726-
1734, 2017.
Andrew G Barto and Sridhar Mahadevan. Recent advances in hierarchical reinforcement learning.
Discrete Event Dynamic Systems, 13(4):341-379, 2003.
Dimitri P Bertsekas and David Alfred Castanon. Adaptive aggregation methods for infinite horizon
dynamic programming. IEEE transactions on Automatic Control, 34(6):589-598, 1989.
Emma Brunskill and Lihong Li. Pac-inspired option discovery in lifelong reinforcement learning.
In International Conference on Machine Learning, pp. 316-324, 2014.
Peter Dayan and Geoffrey E Hinton. Feudal reinforcement learning. In Advances in neural infor-
mation processing systems, pp. 271-278, 1993.
Thomas Dean and Robert Givan. Model minimization in markov decision processes. In AAAI/IAAI,
pp. 106-111, 1997.
Thomas G Dietterich. Hierarchical reinforcement learning with the maxq value function decompo-
sition. Journal of Artificial Intelligence Research, 13:227-303, 2000.
9
Published as a conference paper at ICLR 2019
Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. Deep reinforcement learning for
robotic manipulation with asynchronous off-policy updates. In Robotics and Automation (ICRA),
2017 IEEEInternational Conference on,pp. 3389-3396. IEEE, 2017.
Irina Higgins, Loic Matthey, Xavier Glorot, Arka Pal, Benigno Uria, Charles Blundell, Shakir Mo-
hamed, and Alexander Lerchner. Early visual concept learning with unsupervised deep learning.
arXiv preprint arXiv:1606.05579, 2016.
R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Adam Trischler, and
Yoshua Bengio. Learning deep representations by mutual information estimation and maximiza-
tion. arXiv preprint arXiv:1808.06670, 2018.
Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron
Courville, and R Devon Hjelm. Mine: Mutual information neural estimation. arXiv preprint
arXiv:1801.04062, 2018.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Andrew Levy, Robert Platt, and Kate Saenko. Hierarchical actor-critic. arXiv preprint
arXiv:1712.00948, 2017.
Lihong Li, Thomas J Walsh, and Michael L Littman. Towards a unified theory of state abstraction
for mdps. In ISAIM, 2006.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
Ofir Nachum, Shane Gu, Honglak Lee, and Sergey Levine. Data-efficient hierarchical reinforcement
learning. NIPS, 2018.
Ronald Parr and Stuart J Russell. Reinforcement learning with hierarchies of machines. In Advances
in neural information processing systems, pp. 1043-1049, 1998.
Balaraman Ravindran and Andrew G Barto. Model minimization in hierarchical reinforcement
learning. In International Symposium on Abstraction, Reformulation, and Approximation, pp.
196-211. Springer, 2002.
David F Rogers, Robert D Plante, Richard T Wong, and James R Evans. Aggregation and dis-
aggregation techniques and methodology in optimization. Operations Research, 39(4):553-582,
1991.
Jurgen Schmidhuber and Reiner Wahnsiedler. Planning simple trajectories using neural subgoal
generators. In From Animals to Animats 2: Proceedings of the Second International Conference
on Simulation of Adaptive Behavior, volume 2, pp. 196. MIT Press, 1993.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning, pp. 1889-1897, 2015.
Jonathan Sorg and Satinder Singh. Transfer via soft homomorphisms. In Proceedings of The 8th
International Conference on Autonomous Agents and Multiagent Systems-Volume 2, pp. 741-748.
International Foundation for Autonomous Agents and Multiagent Systems, 2009.
Susanne Still and Doina Precup. An information-theoretic approach to curiosity-driven reinforce-
ment learning. Theory in Biosciences, 131(3):139-148, 2012.
Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A frame-
work for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181-
211, 1999.
Philip S Thomas and Andrew G Barto. Motor primitive discovery. In Development and Learning
and Epigenetic Robotics (ICDL), 2012 IEEE International Conference on, pp. 1-8. IEEE, 2012.
10
Published as a conference paper at ICLR 2019
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In InteUigentRobots and Systems (IROS), 2012IEEE/RSJ International Conference on,pp. 5026-
5033. IEEE, 2012.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. arXiv preprint arXiv:1807.03748, 2018.
Benjamin Van Roy. Performance loss bounds for approximate value iteration with state aggregation.
Mathematics of Operations Research, 31(2):234-244, 2006.
Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg, David
Silver, and Koray Kavukcuoglu. Feudal networks for hierarchical reinforcement learning. arXiv
preprint arXiv:1703.01161, 2017.
Manuel Watter, Jost Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to control:
A locally linear latent dynamics model for control from raw images. In Advances in neural
information processing systems, pp. 2746-2754, 2015.
Ward Whitt. Approximations of dynamic programs, i. Mathematics of Operations Research, 3(3):
231-243, 1978.
11
Published as a conference paper at ICLR 2019
A Proof of Theorem 3 (Generalization of Theorem 1)
Consider the sub-optimality with respect to a specific state s0, V π (s0) - V πhier (s0). Recall that
∏* is the hierarchical result of a policy ∏h* : S → ∆(Π), and note that ∏h* may be assumed to be
deterministic due to the Markovian nature of M. We may use the mapping 夕 to transform ∏{^* to a
high-level policy πhi on G and using the mapping Ψ:
g 〜∏hi(-∣s) ≡ ∏ 〜∏苛(一∣s),g ：= φ(s,∏).	(19)
Let ∏hier be the corresponding hierarchical policy. We will bound the quantity Vπ* (so) - V πhier (so),
which will bound Vπ* (so) - VπMr (so). We follow logic similar to Achiam et al. (2017) and begin
by bounding the total variation divergence between the γ-discounted state visitation frequencies of
the two policies.
Denote the k-step state transition distributions using either ∏ or ∏hier as,
Pk (SlS)= P∏^^-(s)(st+k = SlSt = S),	QO)
Phkier(s0|s) = PΨ(s,πhi(s))(st+k = s0|st = s),	(21)
for k ∈ [1,c]. Considering P* ,Phier as linear operators, We may express the state visitation frequen-
cies d*,dhier of ∏*,∏hier, respectively, as
d* = (1 - γ)A*(I - γcPC)-1μ,	(22)
dhier = (1 - Y)Ahier(I - YcPhier)-1μ,	(23)
where μ is a Dirac δ distribution centered at so and
c-1
A* = I+XYkP*k,	(24)
k=1
c-1
Ahier = I+XYkPhkier.	(25)
k=1
We will use dc*, dchier to denote the every-c-steps Y-discounted state frequencies of π*, πhier; i.e.,
dc = (1-Yc)(I- YcPC)-1μ,	(26)
dhier = (1 -Yc)(I-YcPhier)-1μ.	(27)
By the triangle inequality, we have the following bound on the total variation divergence |dhier -d* |:
Idhier- d* | ≤ (1 - Y)IAhier(I - YcPhier) 1M - Ahier(I - YcP；) 7l
+ (1- Y)1Ahier(I - YcPc)-1μ - A*(I - YcP；)-1μ∣.	(28)
We begin by attacking the first term of Equation 28. We note that
c-1
IAhierI ≤ III+XYkIPhkierI=
k=1
1 - Y c
1 - Y
(29)
Thus the first term of Equation 28 is bounded by
(1 - Yc)I(I - YcPhier)-1 μ -(I-YcPc)-1μ∣
=(1 - Yc)I(I - YcPhier)-1((I - YcP；) - (I-YcPlcier))(I-YcP；) - 1〃|
=YcI(I-YcPhcier)-1(Phcier -P*c)dc*I.	(30)
By expressing (I - YcPhcier)-1 as a geometric series and employing the triangle inequality, we have
I(I - YcPhcier)-1 I ≤ (1 - Yc)-1, and we thus bound the whole quantity (30) by
Yc(1-Yc)-1I(Phcier-P*c)dc*I.	(31)
We now move to attack the second term of Equation 28. We may express this term as
(1-Y)(1-Yc)-1I(Ahier-A*)dc*I.
(32)
12
Published as a conference paper at ICLR 2019
Furthermore, by the triangle inequality we have
c-1
| (Ahier - A*)dCI ≤ X Yk | (Phier - Pk )倒	(33)
k=1
Therefore, recalling wk = 1 for k < c and wk = (1 - γ)-1 for k = c, we may bound the total
variation of the state visitation frequencies as
Idhier - d* | ≤ Y(I - Y)(I - Yc)-1 X Yk-1Wk |(PIkier- Pk)d：|	(34)
k=1
=2y(1 - Y)(1 - Y c)-1 X Yk-1WkEs 〜若[Dτv(Pk (SlS)||Plkier(S0∣s))]	(35)
k=1
c
= 2y(1-y)(1-Y c)-1Esγ X Y k-1 Wk DTV(Pk (SlS)IIPIkier(S0∣s)) .	(36)
k=1
By condition 7 of Theorem 3 we have,
Idhier- d*| ≤ 2y(1 - y)(1 - Yc)-1We	(37)
We now move to considering the difference in values. We have
Vπ*(S0) = (1 — Y)-1 [ Q*(s)R(s) dS,
S
Vπhier(S0) = (1 - Y)-1	dhier(S)R(S) dS.
S
Therefore, we have
IVn(S0)- Vπhier (S0) | ≤ (I- Y) 1RmaxIdhier- d* |
V	2Y P -
≤ 1---Yc Rmaxwe,
(38)
(39)
(40)
(41)
as desired.
B Proof of Claim 4 (Generalization of Claim 2)
Consider a specific St,π. Let K(S[St,π) H p(s0) exp(-D(f (s0),夕(St, ∏))). Note that the defini-
tion of Ψ(St,夕(St, ∏)) may be expressed in terms of a KL:
1c
Ψ(St"(St,∏)) = argmin = TYk-1wkDKL(Pn，($t+k M)IIK($t+k M,∏))∙	(42)
∏0∈π w 七
k=1
Therefore we have,
1c
WEY	IwkDKL(PΨ(st,w(st ,n))(St+k〔St)IIK (St+k ISt,n))
k=1
≤ 二 X Yk 1wkDκL(P∏(St+kISt)IIK(St+kISt,∏)). (43)
w
k=1
By condition 8 we have,
1c
二 EY k-1wk Dkl(P∏ (St+k ISt)IIK (St+k ISt, ∏)) ≤ e2∕8.
w
k=1
Jensen’s inequality on the sqrt function then implies
1 c	______________________________
=y^Yk-1wkVz2DKL(Pn(St+k〔St)IIK(St+kISt,∏)) ≤ e∕2.
w z—z
k=1
(44)
(45)
13
Published as a conference paper at ICLR 2019
Pinsker’s inequality now yields,
1c
W^γk 1 WkDTV(Pn(st+k|st)||K⑶+k|st,∏)) ≤ e/2.	(46)
k=1
Similarly Jensen’s and Pinsker’s inequality on the LHS of Equation 43 yields
1 c k1
加工7 k IWkDTV (PΨ(st ,晨 St ,n)) (St+k Ist)IIK (St+k |st,π)) ≤ c∕2∙	(47)
k=1
The triangle inequality and Equations 46 and 47 then give us,
1c
W〉： γ	wkDTV (Pn(St+k |st川 PΨ(s,∕(s,π))⑶+k |st))
k=1
≤ W X Y k-1 wk (DTV (Pn (St+k |st )||K (St+k |st,π )) + DTV (PΨ(st 产(St ,n)) (St+k |st )||K (St+k |st,π )))
k=1
≤ , (48)
as desired.
C Experimental Details
C.1 Environments
The environments for Ant Maze, Ant Push, and Ant Fall are as described in Nachum et al. (2018).
During training, target (x, y) locations are selected randomly from all possible points in the environ-
ment (in Ant Fall, the target includes a z coordinate as well). Final results are evaluated on a single
difficult target point, equal to that used in Nachum et al. (2018).
The Point Maze is equivalent to the Ant Maze, with size scaled down by a factor of 2 and the agent
replaced With a point mass, which is controlled by actions of dimension two - one action determines
a rotation on the pivot of the point mass and the other action determines a push or pull on the point
mass in the direction of the pivot.
For the 'Images' versions of these environments, we zero-out the x, y coordinates in the observation
and append a low-resolution 5 X 5 X 3 top-down view of the environment. The view is centered on
the agent and each pixel covers the size of a large block (size equal to width of the corridor in Ant
Maze). The 3 channels correspond to (1) immovable blocks (walls, gray in the videos), (2) movable
blocks (shown in red in videos), and (3) chasms where the agent may fall.
Ant (or Point) Maze
Ant Push
Ant Fall
Ant Block
Ant Block Maze
Top-Down View
Figure 5: The tasks we consider in this paper. Each task is a form of navigation. The agent must
navigate itself (or a small red block in 'Block' tasks) to the target location (green arrow). We also
show an example top-down view image (from an episode on the Ant Maze task). The image is
centered on the agent and shows walls and blocks (at times split across multiple pixels).
14
Published as a conference paper at ICLR 2019
The Ant Block environment puts the ant in a 16 × 16 square room next to a 0.8 × 0.8 × 0.4 small
movable block. The agent is rewarded based on negative L2 distance of the block to a desired target
location. During training, these target locations are sampled randomly from all possible locations.
Evaluation is on a target location diagonally opposite the ant.
The Ant Block Maze environment consists of the same ant and small movable block in a ⊃-shaped
corridor. During training, these target locations are sampled randomly from all possible locations.
Evaluation is on a target location at the end of the corridor.
C.2 Training Details
We follow the basic training details used in Nachum et al. (2018). Some differences are listed below:
•	We input the whole observation to the lower-level policy (Nachum et al. (2018) zero-out
the x, y coordinates for the lower-level policy).
•	We use a Huber function for D, the distance function used to compute the low-level reward.
•	We use a goal dimension of size 2. We train the higher-level policy to output actions in
[-10, 10]2. These actions correspond to desired deltas in state representation.
•	We use a Gaussian with standard deviation 5 for high-level exploration.
•	Additional differences in low-level training (e.g. reward weights and discounting) are im-
plemented according to Section 5.
We parameterize fθ with a feed-forward neural network with two hidden layers of dimension 100
using relu activations. The network structure for 夕θ is identical, except using hidden layer dimen-
sions 400 and 300. We also parameterize 夕(s, ∏) := fθ(S) + 夕θ(s, ∏). These networks are trained
with the Adam optimizer using learning rate 0.0001.
D Objective Function Evaluation
XY (oracle)	Ours (CPC-Style with dot product)
Ours	Ours (CPC-Style with distance)
Figure 6: For our method, we utilize an objective based on Equation 8. The objective is similar
to mutual information maximizing objectives (CPC; van den Oord et al. (2018)). We compare to
variants of our method that are implemented more in the style of CPC. Although we find that using
a dot product rather than distance function D is detrimental, a number of distance-based variants of
our approach may perform similarly.
15
Published as a conference paper at ICLR 2019
E β-VAE
Point Maze	Ant Maze
■ Ours
β-VAE (β = 0)
β-VAE (β = 0.1)
β-VAE (β = 0.25)
VAE
βV β-VAE (β = 4)
βV β-VAE (β = 10)
βV β-VAE (β = 25)
Figure 7: We provide additional results comparing to variants of β-VAE (Higgins et al., 2016). We
find that even with this additional hyperparameter, the VAE approach to representation learning does
not perform well outside of the simple point mass environment. The drawback of the VAE is that it is
encouraged to reconstruct the entire observation, despite the fact that much of it is unimportant and
possibly exhibits high variance (e.g. ant joint velocities). This means that outside of environments
with high-information state observation features, a VAE approach to representation learning will
suffer.
F Generalization Capability
Figure 8: We evaluate the ability of our learned representations to transfer from one task to another.
For these experiments, we took a representation function f learned on Ant Maze, fixed it, and then
used it to learn a hierarchical policy on a completely different task. We evaluated the ability of the
representation to transfer to “Reflected Ant Maze” (same as Ant Maze but the maze shape is changed
from '⊃' to '∩') and “Ant Push”. We find that the representations are robust these changes to the
environment and can generalize successfully. We are able to learn well-performing policies in these
distinct environments even though the representation used was learned with respect to a different
task.
G Additional Qualitative Results
Ant Maze Env	XY	Ours	Ours (Images)
口D。)
Figure 9: We replicate the results of Figure 2 but with representations learned according to data
collected by a random higher-level policy. In this setting, when there is even less of a connection
between the representation learning objective and the task objective, our method is able to recover
near-ideal representations.
16
Published as a conference paper at ICLR 2019
H Comparison to HIRO
Ant Maze	Ant Push	Ant Fall
1.0
0.8
0.6
0.4
0.2
0.0
1.0
0.8
0.6
0.4
0.2
0.0
Figure 10: Results of our method compared to the original formulation of HIRO (Nachum et al.,
2018). The representation used in the original formulation of HIRO is a type of oracle - sub-goals
are defined as only the position-based (i.e., not velocity-based) components of the agent observation.
In our own experiments, we found this method to perform similarly to the XY oracle in non-image
tasks. However, when the state observation is more complex (images) performance is much worse.
17
Published as a conference paper at ICLR 2019
Algorithm 1 Representation learning for hierarchical RL.
Input: Replay buffer D, number of training steps N, batch size B, Parameterizations
fθ ,ψθ ,πhi,πφ
function EstLogP art(S, st, at:t+c-1)
## Equation 15
Return log 看 Ps∈S exp(-D(fθ(S),ψθ(st, at：t+c-i))).
end function
function CompLowReward(g, st, s0, at:t+c-1, L)
## Equation 18
Return -D(fθ(s0), g) + D(fθ(s0)3θ(st, at：t+c-i)) + L.
end function
function CompReprLoSs(st, s0, at：t+c—i, s, L)
## Equation 14
Compute attractive term Jatt = D(fθ(s0),夕θ(st, at：t+c—i)).
Compute repulsive term JreP = exp(-D(fθ(s),夕θ(st, at：t+c—i)) - StoPgrad(L)).
Return Jatt + Jrep .
end function
for T = 1 to N do
Sample experience g, s, a, r, s0 and add to replay buffer D (Nachum et al., 2018).
SamPle batch of C-SteP transitions {(g(i), s(i * * *)+c, a(i)+c-1,/+103 〜D.
Sample indices into transition: {k(i)}B=ι 〜[1, c]B.
Sample batch of states S = {s(i)}B=ι 〜D.
Estimate log-partitions {L(i)}iB=1 = {EstLogP art(Se, st(i), at(:it)+c-1)}iB=1.
// Reinforcement learning
Compute low-level rewards {r(i)}B=ι = {CompLowReward(g ⑴,s(i), s(i+k(i), a(i)+c-ι, L(i))}B=ι
Update πlφo (Nachum et al., 2018) with experience {(g(i), s(ti+)k(i)-1, at(i+)k(i)-1,r(i),s * *(+k(i))}iB=1 .
Update πhφi (Nachum et al., 2018) with experience {(g(i), s(t:it)+c, at(:it)+c-1, rt(:it)+c-1)}iB=1.
// Representation learning
Compute loss J = -B PB=I Wk(i)Yk⑺-ICompReprLoSS(k ⑴,sti),s(+k(i) ,/+-,S⑴,L⑴).
Update θ based on Vθ J.
end for
18