Published as a conference paper at ICLR 2019
Adaptivity of deep ReLU network for learn-
ing in Besov and mixed smooth Besov spaces:
OPTIMAL RATE AND CURSE OF DIMENSIONALITY
Taiji Suzuki
The University of Tokyo, Tokyo, Japan
Center for Advanced Intelligence Project, RIKEN, Tokyo, Japan
Japan Digital Design, Tokyo, Japan
taiji@mist.i.u-tokyo.ac.jp
Ab stract
Deep learning has shown high performances in various types of tasks from visual
recognition to natural language processing, which indicates superior flexibility
and adaptivity of deep learning. To understand this phenomenon theoretically, we
develop a new approximation and estimation error analysis of deep learning with
the ReLU activation for functions in a Besov space and its variant with mixed
smoothness. The Besov space is a considerably general function space including
the Holder space and Sobolev space, and especially can capture spatial inhomo-
geneity of smoothness. Throughout the analysis in the Besov space, it is shown
that deep learning can achieve the minimax optimal rate and outperform any non-
adaptive (linear) estimator such as kernel ridge regression, which shows that deep
learning has higher adaptivity to the spatial inhomogeneity of the target function
than other estimators such as linear ones. In addition to this, it is shown that deep
learning can avoid the curse of dimensionality if the target function is in a mixed
smooth Besov space. We also show that the dependency of the convergence rate
on the dimensionality is tight due to its minimax optimality. These results support
high adaptivity of deep learning and its superior ability as a feature extractor.
1	Introduction
Deep learning has shown great success in several applications such as computer vision and natural
language processing. As its application range is getting wider, theoretical analysis to reveal the rea-
son why deep learning works so well is also gathering much attention. To understand deep learning
theoretically, several studies have been developed from several aspects such as approximation theory
and statistical learning theory. A remarkable property of neural network is that it has universal ap-
proximation capability even if there is only one hidden layer (Cybenko, 1989; Hornik, 1991; Sonoda
& Murata, 2017). Thanks to this property, deep and shallow neural networks can approximate any
function with any precision (of course, the meaning of the terminology “any” must be rigorously
defined like “any function in L1(R)”). A natural question coming next is its expressive power. It is
shown that the expressive power of deep neural network grows exponentially against the number of
layers (Montufar et al., 2014; Bianchini & Scarselli, 2014; Cohen et al., 2016; Cohen & Shashua,
2016; Poole et al., 2016) where the “expressive power” is defined by several ways.
The expressive power of neural network can be analyzed more precisely by specifying the target
function’s property such as smoothness. Barron (1993; 1994) developed an approximation theory
for functions having limited “capacity” that is measured by integrability of their Fourier transform.
An interesting point of the analysis is that the approximation error is not affected by the dimen-
sionality of the input. This observation matches the experimental observations that deep learning is
quite effective also in high dimensional situations. Another typical approach is to analyze function
spaces with smoothness conditions such as the Holder space. In particular, deep neural network
with the ReLU activation (Nair & Hinton, 2010; Glorot et al., 2011) has been extensively stud-
ied recently from the view point of its expressive power and its generalization error. For example,
Yarotsky (2017) derived the approximation error of the deep network with the ReLU activation for
1
Published as a conference paper at ICLR 2019
Table 1: Comparison between the performances of deep learning and linear methods. N is the
number of parameters to approximate a function in a Besov space (Bps,q([0, 1]d)), and n is the sample
size. The approximation error is measured by Lr -norm. The O symbol hides the poly-log order.
Model		Deep learning	Linear method
Approximation error rate	~ , ≈. O(N - S)	O (N- d+(1- r)+)
Estimation error rate	2s O(n-2s+d)	/	2s-(2∕(p∧1)-1) ~~ Ω (n 2s + 1-(2∕(p∧1)-1))
functions in the Holder space. Schmidt-Hieber (2018) evaluated the estimation error of regularized
least squares estimator performed by deep ReLU network based on this approximation error analysis
in a nonparametric regression setting. Petersen & Voigtlaender (2017) generalized the analysis of
Yarotsky (2017) to the class of piece-wise smooth functions. Imaizumi & Fukumizu (2018) utilized
this analysis to derive the estimation error to estimate the piece-wise smooth function and concluded
that deep leaning can outperform linear estimators in that setting. Although these error analyses are
standard from a nonparametric statistics view point and the derived rates are known to be (near)
minimax optimal, the analysis is rather limited because they are given mainly based on the Holder
space. However, there are several other function spaces such as the Sobolev space and the space of
finite total variations. A comprehensive analysis to deal with such function classes from a unified
view point is required.
In this paper, we give generalization error bounds of deep ReLU networks for a Besov space and
its variant with mixed smoothness, which includes the Holder space, the Sobolev space, and the
function class with total variation as special cases. By doing so, (i) we show that deep learning1
achieves the minimax optimal rate on the Besov space and notably it outperforms any linear esti-
mator such as the kernel ridge regression, and (ii) we show that deep learning can avoid the curse of
dimensionality on the mixed smooth Besov space and achieves the minimax optimal rate. As related
work, Mhaskar & Micchelli (1992); Mhaskar (1993); Chui et al. (1994); Mhaskar (1996); Pinkus
(1999) also developed an approximation error analysis which essentially leads to analyses for Besov
spaces. However, the ReLU activation is basically excluded and comprehensive analyses for the
Besov space have not been given. As a summary, the contribution of this paper is listed as follows:
(i)	To investigate adaptivity of deep learning, we give an explicit form of approximation and
estimation error bounds for deep learning with the ReLU activation where the target func-
tions are in the Besov spaces (Bps,q) for s > 0 and 0 < p, q ≤ ∞ with s > d(1/p - 1/r)+
where Lr -norm is used for error evaluation. In particular, deep learning outperforms any
linear estimator such as kernel ridge regression if the target function has highly spatial
inhomogeneity of its smoothness. See Table 1 for the overview.
(ii)	To investigate the effect of dimensionality, we analyze approximation and estimation prob-
lems in so-called the mixed smooth Besov space by ReLU neural network. It is shown that
deep learning with the ReLU activation can ease the curse of dimensionality and achieve
the near minimax optimal rate. The theory is developed on the basis of the sparse grid
technique (Smolyak, 1963). See Table 2 for the overview.
2	Set up of function spaces
In this section, we define the function classes for which we develop error bounds. In particular, we
define the Besov space and its variant with mixed smoothness. The typical settings in statistical
learning theory is to estimate a function with a smoothness condition. There are several ways to
characterize “smoothness.” Here, we summarize the definitions of representative functional spaces
that are appropriate to define the smoothness assumption.
Let Ω ⊂ Rd be a domain of the functions. Throughout this paper, We employ Ω = [0,1]d. For
a function f : Ω → R, let ∣∣f kp := ||/必(ω) ：= (Rω |f|pdx)1/p for 0 < p < ∞. For P =
1Inthis paper, we mainly consider a regularized empirical risk minimization given in Eq. (7) as an estimation
procedure, and say “deep learning” to indicate it.
2
Published as a conference paper at ICLR 2019
Table 2: Summary of relation between related existing work and our work for a mixed smooth Besov
space. N is the number of parameters in the deep neural network, n is the sample size. β represents
the smoothness parameter, and d represents the dimensionality of the input. The approximation
accuracy is measured by L2-norm and estimation accuracy is measured by the square of L2 -norm.
See Theorem 3 for the definition of u.
Function class	Holder	Barron class	m-Sobolev (0 <β ≤ 2)	m-Besov (0 <β)	
Approximation				
Author	Yarotsky (2017), Liang & Srikant (2016)	Barron(1993)	Montanelli & DU (2017)	This work
Approx. error	O(N - d)	O(N T/2)	O(N T)	O(N T)	
Estimation				
Author	Schmidt-Hieber (2018)	Barron (1993)	—-	This work
Estimation er- ror	-二	2β O(n-2β+d)	~77	Γ^ O(n- 2 )	—-	-二	2β O(n-2β+ι	× 2(d-I)(U+e) log(n)	1+2β	)
∞, we define ∣∣f k∞ := kf∣∣L∞(Ω) ：= suPx∈ω lf(x)∣. For α ∈ Rd, let |a| = Pd=I ∣αj∙|. Let
C0(Ω) be the set of continuous functions equipped with L∞-norm: C0(Ω) := {f : Ω → R |
f is continuous and ∣∣f ∣∞ < ∞} 2. For α ∈ Z；, We denote by Daf(X) = ∂αιXd (x) 3.
Definition 1 (Holder space (Cβ(Ω))). Let β > 0 with β 6∈ N be the smoothness parameter. For an
m times differentiable function f : Rd → R ,let the norm of the Holder space Cβ (Ω) be IlfkCβ :=
max∣a∣≤m ∣∣Daf k∞ + max∖α∖=m suPx,y∈Ω IDaf(-)yβDif(y)|
where m = bβc (the largest integer
less than β). Then, (β-)Holderspace Cβ(Ω) is defined as Cβ(Ω) = {f | kf kcβ < ∞}.
The parameter β > 0 controls the “smoothness” of the function. Along with the Holder space, the
Sobolev space is also important.
Definition 2 (Sobolev space (Wpk (Ω))). Sobolev space (Wpk (Ω)) with a regularity parameter k ∈
N and a parameter 1 ≤ p ≤ ∞ is a set of functions such that the Sobolev norm ∣f∣Wk :=
(P∖ɑ∖≤k l∣Daf kp) 1 is finite (where the derivative D is taken in the weak sense).
There are some ways to define a Sobolev space with fractional order, one of which will be defined
by using the notion of interpolation space (DeVore, 1998; Adams & Fournier, 2003), but we don’t
pursue this direction here. Finally, we introduce Besov space which further generalizes the definition
of the Sobolev space. To define the Besov space, we introduce the modulus of smoothness.
Definition 3. For a function f ∈ Lp(Ω) for some P ∈ (0, ∞], the r-th modulus of smoothness of f
is defined by
wr,p (f, t) = sup ∣∆rh(f)∣p,
h∈RdRhk2≤t
r rrlf∖< ∖	JPj=o Q(T)r-jf (X + jh)	(X ∈ ω, * * x + rh ∈ ω),
where∆rh(f)(x)= 0 j=0 j	(otherwise).
Based on the modulus of smoothness, the Besov space is defined as in the following definition.
Definition 4 (Besov space (Bp,,q(Ω))). For 0 < p,q ≤ ∞, α > 0, r := [α] + 1, let the seminorm
| ∙ ∣Ba be
p,q
1
(R(T(t-αwr,p(f,t))衅)q (q < ∞),
supt>0 t-αwr,p (f, t)	(q = ∞).
The norm ofthe Besov space B% (Ω) can be defined by ∣∣f kBfq := IlfkP + |f ∣b1 q, and B≈q (Ω)=
{f ∈ Lp(Ω) | kf kB?q < ∞}.
p,q
2Since Ω = [0,1]d in our setting, the boundedness automatically follows from the continuity.
3 We let N := {1, 2, 3, . . . }, Z+ := {0, 1, 2, 3, . . . }, Zd+ := {(z1 , . . . , zd) | zi ∈ Z+}, R+ := {x ≥ 0 |
x ∈ R}, and R++ := {x > 0 | x ∈ R}.
1力Bα,q := {
3
Published as a conference paper at ICLR 2019
Note that p, q < 1 is also allowed. In that setting, the Besov space is no longer a Banach space but a
quasi-Banach space. The Besov space plays an important role in several fields such as nonparametric
statistical inference (Kerkyacharian & Picard, 1992; Donoho et al., 1996; Donoho & Johnstone,
1998; Gine & NickL 2015) and approximation theory (Temlyakov, 1993a). These spaces are closely
related to each other as follows (Triebel, 1983):
•	For m ∈ N, Bmι(Ω) → Wm(Ω) → Bm∞(Ω), and Bm2(Ω) = Wm(Ω) 4.
•	For 0 < s < ∞ and s ∈ N, C4 s(Ω) = B∞,∞(Ω).
•	For 0 < s,p,q,r ≤ ∞ With s > δ := d(1∕p - 1/r)+, it holds that Bp,q(Ω) → Br-δ(Ω).
In particular, under the same condition, from the definition of ∣∣∙∣∣bs , it holds that
p,q
BP,q(Ω) → Lr(Ω).	(1)
•	For 0 < s, p, q ≤ ∞, if s > d/p, then
Bp,q(Ω) →C0(Ω).	⑵
Hence, if the smoothness parameter satisfies s > d/p, then it is continuously embedded in the set of
the continuous functions. HoWever, if s < d/p, then the elements in the space are no longer contin-
uous. Moreover, it is knoWn that B11,1([0, 1]) is included in the space of bounded total variation and
B11,∞([0, 1]) includes it (Peetre & Dept, 1976). Hence, the Besov space also alloWs spatially inho-
mogeneous smoothness With spikes and jumps; Which makes difference betWeen linear estimators
and deep learning (see Sec. 4.1).
It is knoWn that the minimax rate to estimate fo is loWer bounded by n-2s/(2s+d) , (Kerkyacharian
& Picard, 1992; Donoho et al., 1996; Donoho & Johnstone, 1998; Gine & Nickl, 2015). We see
that the curse of dimensionality is unavoidable as long as We consider the Besov space. This is an
undesirable property because We easily encounter high dimensional data in several machine learning
problems. Hence, We need another condition to derive approximation and estimation error bounds
that are not heavily affected by the dimensionality. To do so, We introduce the notion of mixed
smoothness. The Besov space With mixed smoothness is defined as folloWs (Schmeisser, 1987;
Sickel & Ullrich, 2009). To define the space, We define the coordinate difference operator as
∆h,i(f)(x) = ∆h(f(xι,...,Xi-i, ∙,Xi+ι, . . .,Xd))(Xi)
for f : Rd → R, h ∈ R+, i ∈ [d], and r ≥ 1. By applying this difference operator to each coordinate
in a subset e ⊂ {1, . . . , d} of coordinates, the mixed differential operator for a step length h ∈ Rd
is defined as
△hRf )=(Qi∈e ∆hi)(f )，∆h,°(f )= f.
Then, the mixed modulus of smoothness is defined as
wr,p(f,t) := sup∣hi∣≤ti,i∈e k^h，e(f)kp
for t ∈ Rd+ and 0 < p ≤ ∞. This quantity measures hoW the function f is “rough” in a coordinate-
Wise manner. In contrast to the Besov space (Def. 3), the differentiation is taken over all coordinates
in e, Which imposes a coordinate-Wise smoothness. Letting 0 < p, q ≤ ∞, α ∈ Rd++ and ri :=
[α∕ + 1, the semi-norm | ∙ Imb/，/ based on the mixed smoothness is defined by
{∕Ω[(Qi∈e t-αi )wr,p(f,t)]q ∏i⅛ 01/q (O < 9 < ∞),
suPt∈Ω(Qi∈e t-αi )we,p(f,t)	(9 = ∞).
By summing up the semi-norm over the choice of e, the (quasi-)norm of the mixed smooth Besov
space (abbreviated to m-Besov space) is defined by
IlfIIMBpa,q := IlfllP + X	|f lMBα,,(e ,
e⊂{1,...,d}
4The notation ,→ means a continuous embedding; that is, X ,→ Y for tWo normed spaces X, Y means X
is continuously embedded in Y .
4
Published as a conference paper at ICLR 2019
and thus MBp,q(Ω) := {f ∈ Lp(Ω) | kf kMB<^q < ∞} where 0 < p,q ≤ ∞ and α ∈ R；+. In
this paper, We assume that αι =…=ad. With a slight abuse of notation, We also use the notation
MBα for α > 0 to indicate MBp(αq,...,α) .
p,q	,
It is knoWn that, When p = q, the m-Besov space is characterized as a tensor product space of
Bps,q ([0, 1]) (Sickel & Ullrich, 2009). The m-Besov space includes several important models con-
sidered in the literature of statistical learning, e.g., the additive model (Meier et al., 2009) and the
tensor model (Signoretto et al., 2010). It is knoWn that an appropriate estimator in these models
can avoid curse of dimensionality (Meier et al., 2009; Raskutti et al., 2012; KanagaWa et al., 2016;
Suzuki et al., 2016). What We Will shoW in this paper supports that this fact is also applied to deep
learning from a unifying vieWpoint.
The difference betWeen the (normal) Besov space and the m-Besov space can be informally ex-
plained as folloWs. For regularity condition αi ≤ 2 (i = 1, 2), the m-Besov space consists of
functions for Which the folloWing derivatives are “bounded”:
∂f ∂f ∂2f ∂2f	∂2f	∂3f	∂3f	∂4f
∂xι，∂x2，∂x2，∂x2，∂x1∂x2，∂xι∂x2，∂x2∂x2，∂x2∂x2 "
That is, the “max” of the orders of derivatives over coordinates needs to be bounded by 2. On the
other hand, the Besov space only ensures the boundedness of the folloWing derivatives:
∂f ∂f ∂f ∂f	∂2f
∂xι , ∂x2 , ∂x2 , ∂x2 , ∂xι∂x2 ,
Where the “sum” of the orders needs to be bounded by 2. This difference directly affects the rate of
convergence of approximation accuracy. Further details about this space and related topics can be
found in a comprehensive survey (Dung et al., 2016).
Relation to Barron class. Barron (1991; 1993; 1994) shoWed that, if the Fourier transform of
a function f satisfies some integrability condition, then We may avoid curse of dimensionality for
estimating neural netWorks With sigmoidal activation functions. The integrability condition is given
by JCd ∣∣ωk∣f (ω)∣dω < ∞, where f is the Fourier transform of a function f. We call the class of
functions satisfying this condition Barron class. A similar function class is analyzed by KlusoWski
& Barron (2016) too. We cannot compare directly the m-Besov space with the Barron class, but they
are closely related. Indeed, if P = q = 2 and S = αι = •…=ɑd, then m-Besov space MBs,2(Ω)
is equivalent to the tensor product of Sobolev space (Sickel & Ullrich, 2011) which consists of
functions f : Ω → R satisfying RCd Qd=1(1 + ∣ωi∣2)s∣∕(ω)∣2dω < ∞. Therefore, our analysis
gives a (similar but) different characterization of conditions to avoid curse of dimensionality.
3 Approximation error analysis
In this section, we evaluate how well the functions in the Besov and m-Besov spaces can be ap-
proximated by neural networks with the ReLU activation. Let us denote the ReLU activation by
η(x) = max{x, 0} (x ∈ R), and for a vector x, η(x) is operated in an element-wise manner. Define
the neural network with height L, width W, sparsity constraint S and norm constraint B as
Φ(L, W, S, B) := {(W(L)η(∙) + b(L)) ◦…◦ (W(I)X + b(1)) | W(L) ∈ R1×w, b(L)∈ R,
W⑴ ∈ RW×d, b(1) ∈ Rw, W(') ∈ Rw×w, b(') ∈ RW(I < ' < L),
XL=1(kW(')ko + kb(')ko) ≤ S,max ∣W(')k∞ ∨ M)k∞ ≤ B},
where ∣∣ ∙ ∣o is the 'o-norm of the matrix (the number of non-zero elements of the matrix) and ∣∣ ∙ ∣∞
is the '∞-norm of the matrix (maximum of the absolute values of the elements). We want to evaluate
how large L, W, S, B should be to approximate fo ∈ MBp,q(Ω) by an element f ∈ Φ(L, W, S, B)
with precision > 0 measured by Lr-norm: minf ∈Φ ∣f - fo ∣r ≤ .
3.1 Approximation error analysis for Besov spaces
Here, we show how the neural network can approximate a function in the Besov space which is
useful to derive the generalization error of deep learning. Although its derivation is rather standard
5
Published as a conference paper at ICLR 2019
as considered in ChUi et al. (1994); Bolcskei et al. (2017), it should be worth noting that the bound
derived here cannot be attained by any non-adaptive method and the generalization error based on
the analysis is also unattainable by any linear estimators including the kernel ridge regression. That
explains the high adaptivity of deep neural network and how it outperforms usual linear methods
such as kernel methods.
To show the approximation accuracy, a key step is to show that the ReLU neural network can ap-
proximate the cardinal B-spline with high accuracy. Let N (x) = 1 (x ∈ [0, 1]), 0 (otherwise), then
the cardinal B-spline of order m is defined by taking m + 1-times convolution ofN:
Nm(X) =(n*n* …*N)(x),
ι^^^^^^^^{^^^^^^^^^}}
m + 1 times
where f * g(x) := f f (X - t)g(t)dt. It is known that Nm is a piece-wise polynomial of order m.
Fork = (k1, . . . ,kd) ∈ Zd+ andj = (j1, . . . ,jd) ∈ Zd+, let Mkd,j (x) = Qid=1Nm(2kixi-ji). Even
for k ∈ Z+, we also use the same notation to express Mkd,j (X) = Qid=1 Nm(2kXi - ji). Here, k
controls the spatial “resolution” and j specifies the location on which the basis is put. Basically, we
approximate a function f in a Besov space by a super-position of Mkm,j (X), which is closely related
to wavelet analysis (Mallat, 1999).
Mhaskar & Micchelli (1992); Chui et al. (1994) have shown the approximation ability of neural
network for a function with bounded modulus of smoothness. However, the class of the activation
functions in their analysis does not include ReLU but they dealt with activation functions satisfying
the following conditions,
lim η(x)∕xk → 1,	lim η(x)∕xk = 0, ∃K > 1 s.t. ∣η(x)∣ ≤ K(1 + |x|)k (X ∈ R),	(3a)
x→∞	x→-∞
for k = 2 which excludes ReLU. Mhaskar (1993) analyzed deep neural network under the same
setting but it restricts the smoothness parameter to s = k + 1. Mhaskar (1996) considered the
Sobolev space Wpm with an infinitely many differentiable “bump” function which also excludes
ReLU. However, approximating the cardinal B-spline by ReLU can be attained by appropriately
using the technique developed by Yarotsky (2017) as in the following lemma.
Lemma 1 (Approximation of cardinal B-spline basis by the ReLU activation). There exists a con-
Stant c(d,m)depending only on d and m such that, for all e > 0, there exists a neural network M ∈
Φ(Lo, Wo, S0,B0) with Lo := 3+2 卜0g2 (j：：))+ 51dlog2(d ∨ m)], Wo := 6dm(m+2)+2d,
S0 := L0W02 and B0 := 2(m + 1)m that satisfies
kMd,0 - MMkL∞(Rd) ≤ e,
and M(X) = 0 for all x ∈ [0, m + 1]d.
The proof is in Appendix A. Based on this lemma, we can translate several B-spline approximation
results into those of deep neural network approximation. In particular, combining this lemma and
the B-spline interpolant representations of functions in Besov spaces (DeVore & Popov, 1988; De-
Vore et al., 1993; Dung, 2011b), We obtain the optimal approximation error bound for deep neural
networks. Here, let U(H) be the unit ball of a quasi-Banach space H, and for a set F of functions,
define the worst case approximation error as
Rr(F,H) := sup inf kfo - fkLr([o,1]d).
fo∈U(H) f∈F
Proposition 1 (Approximation ability for Besov space). Suppose that 0 < p, q, r ≤ ∞ and 0 <
s < ∞ satisfy the following condition:
s > d(1/p - 1/r)+.	(4)
Assume that m ∈ N satisfies 0 < s < min(m, m - 1 + 1/p). Let δ = d(1/p - 1/r)+ and
V =(S — δ)∕(2δ). For sufficiently large N ∈ N and C = N-s∕d-(V 1+d 1)(d/p-s)+ log(N)-', let
d∨m
L = 3 + 2dlog2 I ------- ) + 5edlog2(d∨ m)],	W = NWo,
Cc(d,m)
S = [(L - 1)W02 + 1]N,	B = O(N(νT+dT)(1∨(d∕p-s)+)),
then it holds that
Rr(Φ(L, W, S, B), Bp,q([0,1]d)) . N-s∕d.
6
Published as a conference paper at ICLR 2019
Remark 1. By Eq. (1), the condition (4) indicates that fo ∈ Bp,q satisfies fo ∈ Lr (Ω). Ifwe set
P = q = ∞ and r = ∞, then Bp,q(Ω) = Cs(Ω) which yields the result by Yarotsky (2017) as a
special case.
The proof is in Appendix B. According to the theorem, the approximation error N -s/d can be
achieved by the settings L = O(log(N)), W = O(N) and S = O(N log(N)) for N ∈ N. The
convergence rate N-s/d is controlled by the smoothness s and the dimensionality d; as the smooth-
ness s goes up, we have better approximation error, and as the dimensionality d goes up, we have
worse error. An interesting point is that the statement is valid even for p 6= r. In particular, the theo-
rem also supports non-continuous regime (s < d/p) in which L∞-convergence does no longer hold
but instead Lr-convergence is guaranteed under the condition s > d(1/p - 1/r)+. In that sense,
the convergence of the approximation error is guaranteed in considerably general settings. Pinkus
(1999) gave an explicit form of convergence when 1 ≤ p = r for the activation functions satisfying
Eq. (3) which does not cover ReLU and an important setting p 6= r. Petrushev (1998) considered
p = r = 2 and activation function with Eq. (3) where s is an integer such that s ≤ k+ 1 + (d- 1)/2.
ChUi et al. (1994) and BoIcskei et al. (2017) dealt with the smooth sigmoidal activation satisfying
the condition (3) with k ≥ 2 or a “smoothed version” of the ReLU activation which excludes ReLU;
but Bolcskei et al. (2017) presented a general strategy for neural-net approximation by using the
notion of best M -term approximation. Mhaskar & Micchelli (1992) gives an approximation boUnd
using the modulus of smoothness, but the smoothness s and the order of sigmoidal function k in
(3) is tightly connected and fo is assumed to be continuous which excludes the situation s < d/p.
On the other hand, the above proposition does not require such a tight connection and it explicitly
gives the approximation bound for Besov spaces. Williamson & Bartlett (1992) derived a spline ap-
proximation error bound for an element in a Besov space when d = 1, but the derived bound is only
O(N -s+(1/p-1/r)+) which is the rate of non-adaptive methods described below, and approximation
by a ReLU activation network is not discussed. We may also use the analysis of Cohen et al. (2001)
which is based on compactly supported wavelet bases, but the cardinal B-spline is easy to handle
through quasi-interpolant representation as performed in the proof of Proposition 1.
It should be noted that the presented approximation accuracy bound is not trivial because it can not
be achieved by a non-adaptive method. Actually, the best N -term approximation error (Kolmorogov
width) of the Besov space is lower bounded as
(N-s/d+(1/p-1/r)+ (ι <p<r ≤ 2, s> d(1∕p - 1/r)),
Oinf SUp jnf kf - f∣∣Lr(Ω) & N N-s/d+1/p-1/2	(1 <p< 2 <r ≤ ∞, s > d/p),
SN ⊂Bp,q f ∈U (Bs,q) f∈SN	[n-s/d	(2 ≤ p<r ≤∞, s>d∕2),
(5)
if 1 < p < r ≤ ∞, 1 ≤ q < ∞ and 1 < s, where SN is any N -dimensional subspace of Bps,q
(Romanyuk, 2009; Myronyuk, 2016; Vybaral, 2008). That is, any linear/non-linear approximator
with fixed N -bases does not achieve the approximation error N -s/d in some parameter settings
such as 1 < p < 2 < r . On the other hand, adaptive methods including deep learning can improve
the error rate up to N-s/d which is rate optimal (Dung, 2011b). The difference is significant when
p < r. This implies that deep neural network possesses high adaptivity to find which part of the
function should be intensively approximated. In other words, deep neural network can properly
extracts the feature of the input (which corresponds to construct an appropriate set of bases) to
approximate the target function in the most efficient way.
3.2 Approximation error analysis for m-Besov space
Here, we deal with m-Besov spaces instead of the ordinary Besov space. The next theorem gives the
approximation error bound to approximate functions in the m-Besov spaces by deep neural network
k	d-1
models. Define Dk,d :=(1 + d-1) (1 + d-ι)	. Then, We have the following theorem.
Theorem 1 (Approximation ability for m-Besov space). Suppose that 0 < p, q, r ≤ ∞ and s < ∞
satisfies s > (1/p - 1/r)+. Assume that m ∈ N satisfies 0 < s < min(m, m - 1 + 1/p). Let
δ = (1/p - 1/r)+ and V = (S — δ)∕(2δ). For any K ≥ 1, let K * =dK (1 + α-δδ” Then, for
N =(2+(1 - 2-ν )-1 )2k Dκ*,d, ifwe set
L = 3 + 2 1log2 (C3d∨mm)) +5 + (s + (P - s)+ + 1)K* +log([e(m + 1)]d(1 + K*))] dlog2(d∨ m)],
7
Published as a conference paper at ICLR 2019
W = WoN, S = [(L - 1)W02 + 1]N, B = O(N(VT + I)(IV(I/P-S)+4,
then it holds that
(i) For p ≥ r,
(ii) For p < r,
Rr(Φ(L, W, S, B), MBp4 s,q([0, 1]d)). 2-KsDK(1,/dmin(r,1)-1/q)+,
2-KsD(1/r-1/q)+	(r < ∞)
Rr(Φ(L,W,S,B),MBps,q([0,1]d)). 22-KsDDK(1,-d1/q)+	((rr =< ∞∞))
(6a)
(6b)
The proof is given in Appendix C.3. It holds that N ' 2KK(d-1), which implies 2-K '
N-1 logd-1(N) if N d (see also the discussion right after Theorem 5 in Appendix C.1 for
more details of calculation). Therefore, when r q, the approximation error can be evaluated as
O(N-slogs(d-1)(N))for L = O(log(N)), W = O(N)and S = O(Nlog(N))for N ∈ N in
which the effect of dimensionality d is much milder than that of Proposition 1. This means that the
curse of dimensionality is much eased in the mixed smooth space.
The obtained bound is far from obvious. Actually, it is better than any linear approximation meth-
ods as follows. Let the linear M -width introduced by Tikhomirov (1960) be λN (MBps,q, Lr) :=
inf LN supf∈U(MBs ) kf - LN(f)kr, where the infimum is taken over all linear oprators LN with
rank N from MBps,q to Lr. The linear N -width of the m-Besov space has been extensively studied
as in the following proposition (See Lemma 5.1 of Dung (2011a), and RomanyUk (2001)).
Proposition 2. Let 1 ≤ p, r ≤ ∞, 0 < q ≤ ∞ and s > (1/p - 1/r)+. Then we have the following
asymptotic order of the linear width for the asymptotics N	d:
(a)	For p ≥ r,
(NT logd-1(N))s
λN(MBps,q,Lr)'
(NT logd-1(N))s(logd-1(N))1∕rτ∕q
、(NT logd-1(N))s (logd-1(N))(1∕2τ∕q)+
)(q ≤ 2 ≤ r ≤ P < ∞),
(q≤ 1, p=r=∞),
(1 <p=r ≤ 2, q ≤ r),
(1 < p = r ≤ 2, q > r),
(2 ≤ q, 1 < r < 2 ≤ p < ∞),
(b)	For 1 < p < r < ∞,
λN(MBps,q,Lr)'
(N-1 logd-1(N))s+1/r-1/p
(N-1 logd-1(N))s+1/r-1/p(logd-1(N))(1/r-1/q)+
(2 ≤ p, 2 ≤ q ≤ r),
(r ≤ 2).
Therefore, the approximation error given in Theorem 1 achieves the optimal linear width
((N -1 logd-1(N))s) for several parameter settings ofp, q, s. In particular, when p < r, the bound
in Theorem 1 is better than that of Proposition 2. This is because to prove Theorem 1, we used an
adaptive recovery technique instead of a linear recovery method. This implies that, by constructing
a deep neural network accurately, we achieve the same approximation accuracy as the adaptive one
which is better than that of linear approximation.
4	Estimation error analysis
In this section, we connect the approximation theory to generalization error analysis (estimation er-
ror analysis). For the statistical analysis, we assume the following nonparametric regression model:
yi = f (xi ) + ξi (i = 1, . . . , n),
where Xi 〜Pχ with density 0 ≤ p(x) < R on [0,1]d, and ξi 〜N(0,σ2). The data Dn =
(xi, yi)in=1 is independently identically distributed. We want to estimate fo from the data. Here, we
consider a regularized learning procedure:
n
f = argmin	X(Iyi- fg)	(7)
f:f ∈Φ(L,W,S,B) i=ι
where f is the clipping of f defined by f = min{max{f, -F}, F} for F > 0 which is realized by
ReLU units. Since the sparsity level is controlled by S and the parameter is bounded by B, this esti-
mator can be regarded as a regularized estimator. In practice, it is hard to exactly compute f. Thus,
we approximately solve the problem by applying sparse regularization such as L1 -regularization and
optimal parameter search through Bayesian optimization. The generalization error that we present
here is an “ideal” bound which is valid if the optimal solution f is computable.
8
Published as a conference paper at ICLR 2019
4.1	Estimation error in Besov spaces
In this subsection, we provide the estimation error rate of deep learning to estimate functions in
Besov spaces.
Theorem 2. Suppose that 0 < p,q ≤ ∞ and S > d(1∕p — 1/2)+. If fo ∈ Bp,q(Ω) ∩ L∞(Ω) and.
∣f o ∣Bps q ≤ 1 and ∣fo∣∞ ≤ F for F ≥ 1, then letting (W, L, S, B) be as in Proposition 1 with
d
N N n 2s+d, we obtain
2s
EDn[kfo — f kL2(Pχ)] . n-k log(n)3,
where EDn [∙] indicates the expectation w.r.t. the training data Dn.
The proof is given in Appendix D. The condition kfo k∞ ≤ F is required to connect the empirical
L2-norm 1 pn=1 (f(χi) — f o(χi))2 to the population L2-norm ∣∣f - f0∣∣L2(pχ). It is known that
_	2s
the convergence rate n 2s+d is mini-max optimal (Kerkyacharian & Picard, 1992; Donoho et al.,
1996; Donoho & Johnstone, 1998; Gine & Nickl, 2015). Thus, it cannot be improved by any estima-
tor. Therefore, deep learning can achieve the minimax optimal rate up to log(n)3 -order. The term
log(n)3 could be improved to log(n)2 by using the construction of Petersen & Voigtlaender (2017).
However, we don’t pursue this direction for simplicity.
Here an important remark is that this minimax optimal rate cannot be achieved by any linear es-
timator. We call an estimator linear when the estimator depends on (yi)in=1 linearly (it can be
non-linearly dependent on (xi)in=1). Several classical methods such as the kernel ridge regression,
the Nadaraya-Watson estimator and the sieve estimator are included in the class of linear estimators
(e.g., kernel ridge regression is given as f(x) = kx,X(kXX + λI)-1Y). The following proposition
given by Donoho & Johnstone (1998); Zhang et al. (2002) states that the minimax rate of linear
estimators is lower bounded by n-{2s-2(1/p-1/2)+}/{2s+1-2(1/p-1/2)+} for d = 1 which is larger
-^2s^
than the minimax rate n 2s+1 if p < 2.
Proposition 3 (Donoho & Johnstone (1998); Zhang et al. (2002)). Suppose that d = 1 and the
input distribution PX is the uniform distribution on [0, 1]. Assume that s > 1/p, 1 ≤ p, q ≤ ∞ or
s = p = q = 1. Then,
2s — v
2s + 1-v
f:imnL TBp'q ) EDn [kf θ - fkL2(PX )]& n
.^

^
where v = 2/(p ∧ 2) - 1 and f runs over all linear estimators, that is, f depends on (yi)in=1 linearly.
When p < 2, the smoothness of the Besov space is somewhat inhomogeneous, that is, a function in
the Besov space contains spiky/jump parts and smooth parts (remember that when s = p = q = 1
for d = 1, the Besov space is included in the set of functions with bounded total variation). Here, the
setting p < 2 is the regime where there appears difference between non-adaptive methods and deep
learning in terms of approximation accuracy (see Eq. (5)). On the other hand, the linear estimator
captures only global properties of the function and cannot capture variability of local shapes of the
function. Hence, the linear estimator cannot achieve the minimax optimal rate if the function has
spatially inhomogeneous smoothness. However, deep learning possesses adaptivity to the spatial
inhomogeneity.
Imaizumi & Fukumizu (2018) has pointed out that such a discrepancy appears when the target
function is non-smooth. Interestingly, the parameter setting s > 1/p assumed in Proposition 3
ensures smoothness (see Eq. (2)). This means that non-smoothness is not necessarily required
to characterize the superiority of deep learning, but non-convexity of the set of target functions is
essentially important. In fact, the gap is coming from the property that the quadratic hull of the
model U (Bps,q) is strictly larger than the original set (Donoho & Johnstone, 1998).
4.2	Estimation error in mixed smooth Besov spaces
Here, we provide the estimation error rate to estimate functions in mixed smooth Besov spaces.
Theorem 3. Suppose that 0 < p, q ≤ ∞ and s > (1∕p — 1∕2)+. Let u = (1 — 1∕q)+ for p ≥ 2 and
U = (1/2 — 1∕q)+ forP < 2. If fo ∈ MBp,q(Ω) ∩ L∞(Ω) and IIfokMBp,q ≤ 1 and IIfoi∣∞ ≤ F
9
Published as a conference paper at ICLR 2019
for F ≥ 1, then letting (W, L, S, B) be as in Theorem 1, we obtain
2s	2(d-1)(u+s)
EDn [kf o - fllL2(Pχ )] . n-2s+1 IOgS)	1+2sIOgs)3.
Under the same assumption, if s > u log2(e) is additionally satisfied, we also have
^ C	2s-2u log2 (e)	C
EDn[lfo-fbl2L2(PX)] . n-2s+ι+(i-2u)ιog2(e) log(n)3.
The proof is given in Appendix D. The risk bound (Theorem 3) indicates that the curse of dimen-
sionality can be eased by assuming the mixed smoothness compared with the ordinary Besov space
2s
(n 2s+d). We show that this is almost minimax optimal in Theorem 4 below. In the first bound, the
dimensionality d comes in the exponent of pOly lOg(n) term. If u = 0, then the effect of d can be
further eased. Actually, in this situation (u = 0), the second bound can be rewritten as
2s
n~ 2s+1+log2(e) log(n)3,
where the effect of the dimensionality d completely disappears from the exponent. This explains
partially why deep learning performs well for high dimensional data. Here, we again remark the
adaptivity of deep learning. Remind that this rate cannot be achieved by the linear estimator for
p < 2 when d = 1 by Proposition 3. Montanelli & Du (2017) has analyzed the mixed smooth
Holder space with s < 2. However, our analysis is applicable to the m-Besov space which is more
general than the mixed smooth Holder space and the covered range of s,p,q is much larger.
Minimax optimal rate for estimating a function in the m-Besov space Here, we show the min-
imax optimality of the obtained bound as follows.
Theorem 4. Assume that 0 < p, q ≤ ∞ and s > (1/p - 1/2)+ and PX is the uniform distribution
over [0, 1]d. Regarding d as a constant, the minimax learning rate in the asymptotics of n → ∞ is
lower bounded as follows: There exists a constant Cb1 such that
2s	2(d-i)(s+i/2-i/q)+
inf sup	EDn[kf - f0kL2(Pχ)] ≥ CIn-E log(n)	2s+1	(8)
fb fo∈U(MBps,q)
where “inf” is taken over all measurable functions of the observations (xi, yi)in=1 and the expecta-
tion is taken for the sample distribution.
The proof is given in Appendix E. Because of this theorem, our bound given in Theorem 3 achieves
the minimax optimal rate in the regime of p < 2 and 1/2 - 1/q > 0 up to lOg(n)3 order. Even
outside of this parameter setting, the discrepancy between our upper bound and the minimax lower
bound is just a poly-lOg oder. See also Neumann (2000) for some other related spaces and specific
examples such as p = q = 2.
5 Conclusion
This paper investigated the learning ability of deep ReLU neural network when the target function
is in a Besov space or a mixed smooth Besov space. Based on the analysis for the Besov space, it
was shown that deep learning using the ReLU activation can achieve the minimax optimal rate and
outperform the linear method when p < 2 which indicates the spatial inhomogeneity of the shape
of the target function. The analysis for the mixed smooth Besov space showed that deep learning
can adaptively avoid the curse of dimensionality. The bound was derived by sparse grid technique.
All analyses in the paper adopted the cardinal B-spline expansion and the adaptive non-linear ap-
proximation technique, which allowed us to show the minimax optimal rate. The consequences of
the analyses partly support the superiority of deep leaning in terms of adaptivity and ability to avoid
curse of dimensionality. From more high level view point, these favorable property is reduced to its
high feature extraction ability.
Acknowledgment The author is grateful to Satoshi Hayakawa for showing the correct statement
of Proposition 4 by modifying the proof of Lemma 10 of Schmidt-Hieber (2018). TS was partially
supported by MEXT Kakenhi (25730013, 25120012, 26280009, 15H05707 and 18H03201), Japan
Digital Design and JST-CREST.
10
Published as a conference paper at ICLR 2019
References
R. Adams and J. Fournier. Sobolev Spaces. Pure and Applied Mathematics. Elsevier Science, 2003.
A. R. Barron. Approximation and estimation bounds for artificial neural networks. In Proceedings
ofthe 4th Annual Workshop on Computational Learning Theory,pp. 243-249, 1991.
A. R. Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE
Transactions on Information Theory, 39(3):930-945, 1993.
A. R. Barron. Approximation and estimation bounds for artificial neural networks. Machine Learn-
ing, 14(1):115-133, 1994.
M.	Bianchini and F. Scarselli. On the complexity of neural network classifiers: A comparison
between shallow and deep architectures. IEEE Transactions on Neural Networks and Learning
Systems, 25(8):1553-1565, 2014.
H. Bolcskei, P. Grohs, G. KUtyniok, and P. Petersen. Optimal approximation with sparsely connected
deep neural networks. arXiv preprint arXiv:1705.01714, 2017.
C. ChUi, X. Li, and H. Mhaskar. NeUral networks for localized approximation. Mathematics of
Computation, 63(208):607-623, 1994.
A. Cohen, W. Dahmen, I. DaUbechies, and R. A. DeVore. Tree approximation and optimal encoding.
Applied and Computational Harmonic Analysis, 11(2):192-226, 2001.
N.	Cohen and A. ShashUa. ConvolUtional rectifier networks as generalized tensor decompositions.
In Proceedings of the 33rd International Conference on Machine Learning, volUme 48 of Pro-
ceedings of Machine Learning Research, pp. 955-963, 2016.
N.	Cohen, O. Sharir, and A. ShashUa. On the expressive power of deep learning: A tensor analysis.
In Proceedings of the 29th Annual Conference on Learning Theory, volUme 49 of Proceedings of
Machine Learning Research, pp. 698-728, 2016.
G. Cybenko. Approximation by sUperpositions of a sigmoidal fUnction. Mathematics of Control,
Signals, and Systems (MCSS), 2(4):303-314, 1989.
R. A. DeVore. Nonlinear approximation. Acta Numerica, 7:51-150, 1998.
R. A. DeVore and V. A. Popov. Interpolation of Besov spaces. Transactions of the American
Mathematical Society, 305(1):397-414, 1988.
R. A. DeVore, G. Kyriazis, D. Leviatan, and V. M. Tikhomirov. Wavelet compression and
nonlinearn-widths. Advances in Computational Mathematics, 1(2):197-214, 1993.
D. L. Donoho and I. M. Johnstone. Minimax estimation via wavelet shrinkage. The Annals of
Statistics, 26(3):879-921, 1998.
D. L. Donoho, I. M. Johnstone, G. Kerkyacharian, and D. Picard. Density estimation by wavelet
thresholding. The Annals of Statistics, 24(2):508-539, 1996.
D. Dung. On recovery and one-sided approximation of periodic functions of several variables.
Doklady Akademii Nauk SSSR, 313:787-790, 1990.
D. Dung. On optimal recovery of multivariate periodic functions. In ICM-90 Satellite Conference
Proceedings, pp. 96-105, 1991.
D. Dung. Optimal recovery of functions of a certain mixed smoothness. Vietnam Journal OfMathe-
matics, 20(2):18-32, 1992.
D. Dung. B-spline quasi-interpolant representations and sampling recovery of functions with mixed
smoothness. Journal of Complexity, 27(6):541-567, 2011a.
D.	Dung. Optimal adaptive sampling recovery. Advances in Computational Mathematics, 34(1):
1-41, 2011b.
11
Published as a conference paper at ICLR 2019
D. Dung, V. N. Temlyakov, and T. Ullrich. Hyperbolic cross approximation. arXiv preprint
arXiv:1601.03978, 2016.
E.	M. Galeev. Linear widths of Holder-Nikol Skii classes of periodic functions of several variables.
Matematicheskie Zametki, 59(2):189-199, 1996.
E.	Gine and R. Nickl. Mathematical Foundations of Infinite-Dimensional Statistical Models. Cam-
bridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2015.
X. Glorot, A. Bordes, and Y. Bengio. Deep sparse rectifier neural networks. In Proceedings of the
14th International Conference on Artificial Intelligence and Statistics, volume 15 of Proceedings
of Machine Learning Research, pp. 315-323, 2011.
K. Hornik. Approximation capabilities of multilayer feedforward networks. Neural Networks, 4(2):
251-257, 1991.
M. Imaizumi and K. Fukumizu. Deep neural networks learn non-smooth functions effectively. arXiv
preprint arXiv:1802.04474, 2018.
H. Kanagawa, T. Suzuki, H. Kobayashi, N. Shimizu, and Y. Tagami. Gaussian process nonpara-
metric tensor estimator and its minimax optimality. In Proceedings of the 33rd International
Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pp.
1632-1641, 2016.
G. Kerkyacharian and D. Picard. Density estimation in Besov spaces. Statistics & Probability
Letters, 13:15-24, 1992.
J. M. Klusowski and A. R. Barron. Risk bounds for high-dimensional ridge function combinations
including neural networks. arXiv preprint arXiv:1607.01434, 2016.
S. Liang and R. Srikant. Why deep neural networks for function approximation? arXiv preprint
arXiv:1610.04161, 2016. ICLR2017.
S. Mallat. A Wavelet Tour of Signal Processing. Academic Press, 1999.
L. Meier, S. van de Geer, and P. Buhlmann. High-dimensional additive modeling. The Annals of
Statistics, 37(6B):3779-3821, 2009.
H. N. Mhaskar. Neural networks for optimal approximation of smooth and analytic functions. Neural
Computation, 8(1):164-177, 1996.
H. N. Mhaskar and C. A. Micchelli. Approximation by superposition of sigmoidal and radial basis
functions. Advances in Applied mathematics, 13(3):350-373, 1992.
H. N. Mhaskar. Approximation properties of a multilayered feedforward artificial neural network.
Advances in Computational Mathematics, 1(1):61-80, 1993.
H. Montanelli and Q. Du. Deep relu networks lessen the curse of dimensionality. arXiv preprint
arXiv:1712.08688, 2017.
G. F. Montufar, R. Pascanu, K. Cho, and Y. Bengio. On the number of linear regions of deep neural
networks. In Advances in Neural Information Processing Systems, pp. 2924-2932. 2014.
V. Myronyuk. Kolmogorov widths of the anisotropic Besov classes of periodic functions of many
variables. Ukrainian Mathematical Journal, 68(5), 2016.
V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In Proceed-
ings of the 27th International Conference on Machine Learning, pp. 807-814, 2010.
M. H. Neumann. Multivariate wavelet thresholding in anisotropic function spaces. Statistica Sinica,
10(2):399-431, 2000.
J. Peetre and D. U. M. Dept. New Thoughts on Besov Spaces. Duke University mathematics series.
Mathematics Dept., Duke University, 1976.
12
Published as a conference paper at ICLR 2019
P. Petersen and F. Voigtlaender. Optimal approximation of piecewise smooth functions using deep
ReLU neural networks. arXiv preprint arXiv:1709.05289, 2017.
P. P. Petrushev. Approximation by ridge functions and neural networks. SIAM Journal on Mathe-
matiCalAnalysis, 30(1):155-189,1998.
A.	Pinkus. Approximation theory of the mlp model in neural networks. Acta Numerica, 8:143-195,
1999.
B.	Poole, S. Lahiri, M. Raghu, J. Sohl-Dickstein, and S. Ganguli. Exponential expressivity in deep
neural networks through transient chaos. In Advances in Neural Information Processing Systems,
pp. 3360-3368. 2016.
G. Raskutti, M. J. Wainwright, and B. Yu. Minimax-optimal rates for sparse additive models over
kernel classes via convex programming. The Journal of Machine Learning Research, 13(1):389-
427, 2012.
A. S. Romanyuk. Linear widths of the Besov classes of periodic functions of many variables. II.
Ukrainian Mathematical Journal, 53(6):965-977, 2001.
A. S. Romanyuk. Bilinear approximations and Kolmogorov widths of periodic Besov classes. The-
ory of Operators, Differential Equations, and the Theory of Functions, 6(1):222-236, 2009.
H.-J. Schmeisser. An unconditional basis in periodic spaces with dominating mixed smoothness
properties. Analysis Mathematica, 13(2):153-168, 1987.
J. Schmidt-Hieber. Nonparametric regression using deep neural networks with ReLU activation
function. ArXiv preprint arXiv:1708.06633(v3), 2018.
W. Sickel and T. Ullrich. Tensor products of Sobolev-Besov spaces and applications to approxima-
tion from the hyperbolic cross. Journal of Approximation Theory, 161(2):748-786, 2009.
W. Sickel and T. Ullrich. Spline interpolation on sparse grids. Applicable Analysis, 90(3-4):337-
383, 2011.
M. Signoretto, L. D. Lathauwer, and J. Suykens. Nuclear norms for tensors and their use for convex
multilinear estimation. Technical Report 10-186, ESAT-SISTA, K.U.Leuven, 2010.
S.	Smolyak. Quadrature and interpolation formulas for tensor products of certain classes of func-
tions. Doklady Akademii Nauk SSSR, 148:1042-1045, 1963.
S.	Sonoda and N. Murata. Neural network with unbounded activation functions is universal approx-
imator. Applied and Computational Harmonic Analysis, 43(2):233-268, 2017.
T.	Suzuki, H. Kanagawa, H. Kobayashi, N. Shimizu, and Y. Tagami. Minimax optimal alternat-
ing minimization for kernel nonparametric tensor learning. In Advances In Neural Information
Processing Systems, pp. 3783-3791, 2016.
V.	Temlyakov. Approximation of periodic functions of several variables with bounded mixed differ-
ence. Mathematics of the USSR-Sbornik, 41(1):53-66, 1982.
V.	Temlyakov. Approximation of Periodic Functions. Nova Science Publishers, 1993a.
V.	Temlyakov. On approximate recovery of functions with bounded mixed derivative. Journal of
Complexity, 9:41-59, 1993b.
V.	M. Tikhomirov. Diameters of sets in function spaces and the theory of best approximations.
Uspekhi Matematicheskikh Nauk, 15(3):81-120, 1960.
H. TriebeL Theory ofFunction Spaces. Monographs in Mathematics. Birkhauser Verlag, 1983.
A. W. van der Vaart and J. A. Wellner. Weak Convergence and Empirical Processes: With Applica-
tions to Statistics. Springer, New York, 1996.
J. Vybaral. Widths of embeddings in function spaces. Journal of Complexity, 24:545-570, 2008.
13
Published as a conference paper at ICLR 2019
R. C. Williamson and P. L. Bartlett. Splines, rational functions and neural networks. In Advances in
Neural Information Processing Systems,pp. 1040-1047,1992.
Y. Yang and A. Barron. Information-theoretic determination of minimax rates of convergence. The
Annals of Statistics, 27(5):1564-1599, 1999.
D. Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks, 94:
103-114, 2017.
S. Zhang, M.-Y. Wong, and Z. Zheng. Wavelet threshold estimation of a regression function with
random design. Journal of Multivariate Analysis, 80(2):256-284, 2002.
A Proof of Lemma 1
ProofofLemma 1. First note that Nm(X)= + Pm=01(-1)j (m++1)(x - j* (see Eq. (4.28) of
Mhaskar & Micchelli (1992) for example). Thus, if we can make an approximation of η(x)m, then
by taking a summation of those basis, we obtain an approximate ofNm(x). It is shown by Yarotsky
(2017); Schmidt-Hieber (2018) that, for D ∈ N and any > 0, there exists a neural network
φmult ∈ Φ(L, W, S, B) with L = dlog2
such that
+ 5edlog2(D)e, W = 6d, S = LW2 and B = 1
D
) -	xi ≤ ,
i=1
sup	φmult (x1 , . . . , xD
x∈[0,1]D
and φmult(0, . . . , 0) = 0 for y ∈ RD such that QjD=1 yj = 0. Moreover, for any M > 0, we can
realize the function min{M, max{X, 0}} by a single-layer neural network φ(0,M) (X) := η(X) -
η(X - M)(= min{M, max{X, 0}}). Thus, for X ∈ R, it holds that
sup ∣Φmuit(Φ(0,i)(x∕M),..., Φ(0,i)(x∕M)) - (Φ(0,i)(x∕M))m∣ ≤ 匚
x∈[0,M]
Now, Nm(X) = 0 for X 6∈ [0, m + 1] gives that
Nm(x)
m1 X (Ty (m： )φ(0,m+1-j)(x - j)m
j =0
1 m+1	m + 1
m X(-I)‘ ( ：	((m+1ymφ(0,1-j∕(m+i))((X- j)/(m+1))m.
Therefore, letting
m+1
1
f(X) = mm!∑(-1)j (m+1)m
j=0
m +1
j φmult
---V----
m-times
x X - j
1-m+ι) Im + 1
we have that f(X) = 0 for all X ≤ 0 and
SUp |Nm(x) - f (x)| ≤	ɪ	X	(m +	1)	(m + 1)me	≤ Jm + 1)m— 2m+1e
0≤x<m+ι 1 m() f ( )| ≤	m!	j=0	V j	J +)	≤	√2∏mm+1∕2e-m
(2e)m	0
≤ e v ._ € =: e ,
m	√m
where We used Pjm=01 (m++1) = 2m+1 and Stirling,s approximation m! ≥ √2∏mm+1∕2e-m in the
second inequality. Hence, we also have that, for all X > m + 1,
f(x) = m Xi(-1)j (m+1)(m +1)m
14
Published as a conference paper at ICLR 2019
× φmult
m+1-j
1-m+ι) I m + 1
m m + 1 — j
φ(0,1- j) I m + 1
: δ0.
It holds that ∣δ0∣ ≤ 0. Since it is possible that δ0 6= 0, we modify f(x) so that f(x) = 0 outside the
interval [0, m + 1]. Because of this and noting 0 ≤ Nm(x) ≤ 1, we see that g(x) := φ(0,1) (f (x) —
mδ+rφ(0,m+1)(X)) yields
sup |Nm(x) — g(x)| ≤ 20,
x∈R
sup |g(x)| ≤ 1, g(x) = 0 (∀x 6∈ [0, m + 1]).
x∈R
Hence, by applying φmult again, we finally obtain that
sup |M0d,0(x) — φmult (g(x1), . . . , g(xd))|
x∈[0,1]d
d d
≤ sup	M0d,0(x) —	g(xj) + sup	g(xj) — φmult (g(x1), . . . , g(xd))
x∈[0,1]d	,	j=1	x∈[0,1]d j=1
≤2d0 + .
We again applying φ(0,1), we obtain that h(x)= φ(0,1) ◦ φmult(g(x1), . . . , g(xd)) satisfies kM0d,0 —
hkL∞(Rd) ≤ 2dE0+E, h(x) = 0 for all x 6∈ [0, m+1]d, and khk∞ ≤ 1. Finally, by carefully checking
the network construction, it is shown that h ∈ Φ(L, W, S, B) with L = 3 + 2「log (3d∨m) +
5edlog2(d ∨ m)e, W = 6dm(m + 2) + 2d, S = LW2 and B = 2(m + 1)m. Hence, resetting
E J 2de0 + E =(1 + 2de (√L )e, h becomes the desired MM.
□
B Proof of Proposition 1
For the order m ∈ N of the cardinal B-spline bases, let J(k) = {—m, —m + 1, . . . , 2k — 1, 2k}d
and the quasi-norm of the coefficient (αk,j)k,j for k ∈ Z+ andj ∈ J(k) be
k(αk,j)k,jkbp,q.= [χ 2k(S-d∕p)( X ∣αk,j∣p)1∕p	∖	.
[k∈Z+ |_	j∈J (k)	J
Lemma 2. Under one of the conditions (4) in Proposition 1 and the condition 0 < s < min(m, m—
1 + 1 /p) where m ∈ N is the order ofthe cardinal B-SPline bases, for any f ∈ Bp,q (Ω) ,there exists
fN that satisfies
kf - fN kLr(Ω) . N7dkfkBs,	⑼
p,q
for N 1, and has the following form:
K	K*	nk
fN(x)=X X αk,jMkd,j(x)+ X Xαk,jiMkd,ji(x),	(10)
k=0 j∈J(k)	k=K+1 i=1
where (ji)n=ι ⊂ J (k), K = 」Ci log(N ”d], K * = dlog(λN )ν Te + K + 1, n =
∖λN2-ν(k-K)e (k = K +1,..., K*) for δ = d(1∕p — 1/r)+ and V = (s — δ)∕(2δ) where the real
number constants C1 > 0 and λ > 0 are chosen to satisfy PkK=1(2k + m)d + PkK=K+1 nk ≤ N
indePendently to N. Moreover, we can choose the coefficients (αk,j ) to satisfy
k(αk,j)k,jkbsp,q . kfkBps,q.
Proof of Lemma 2. DeVore & Popov (1988) constructed a linear bounded operator Pk having the
following form:
Pk(f)(x) = X ak,jMkd,j (x)	(11)
j∈J(k)
15
Published as a conference paper at ICLR 2019
where αk,j is constructed in a certain way, where for every f ∈ Lp ([0, 1]d) with 0 < p ≤ ∞, it
holds
kf-Pk(f)kLp ≤ Cwr,p(f, 2-k).	(12)
Let
pk(f) :=Pk(f) - Pk-1(f), P-1(f) =0.
Then, it is shown that for 0 < p, q ≤ ∞ and 0 < s < min(m, m - 1 + 1/p), f belongs to Bps,q if
and only if f can be decomposed into
∞
f = X pk (f),
k=0
with the convergence condition k(pk(f))k∞=0kbsp(Lp) := (Pk∈Z+ (2sk kpk kLp)q)1/q < ∞; in
particular, kf kBps,q ' k(pk(f))k∞=0kbsp(Lp). Here, each pk can be expressed as pk(x) =
Pj∈J(k) αk,j Mkd,j (x) for a coefficient (αk,j )k,j which could be different from (ak,j)k,j appear-
ing in Eq. (11). Hence, f ∈ Bps,q can be decomposed into
∞
f = X X αk,jMkd,j (x)	(13)
k=0 j∈J(k)
with convergence in the sence of Lp .	Moreover, it is shown that kpk kLp	'
(2-kd Pj∈J(k) lαk,j|p)1/p and thus
kf kBps,q ' k(αk,j)k,j kbsp,q.	(14)
Based on this decomposition, Dung (2011b) proposed an optimal adaptive recovery method such that
the approximator has the form (10) under the conditions for K, K*,n given in the statement and
satisfies the approximation accuracy (9). This can be proven by applying the proof of Theorem 3.1 in
Dung (2011b) to the decomposition (13) instead of Eq. (3.8) of that paper. See also Theorem 5.4 of
Dung (2011b). Moreover, the equivalence (14) gives the norm bound of the coefficient (αk,j). □
Proof of Proposition 1. Basically, we combine Lemma 1 and Lemma 2. We substitute the ap-
proximated cardinal B-spline basis MM into the decomposition of fn (10). Let the set of indexes
(k, j) ∈ Z × Z that consists fN given in Eq. (10) be EN; i.e., fN = P(k,j)∈E αk,j Mkd,j. Accord-
ingly, We set f := P(k 加石^ ak,jMd,j. For each X ∈ Rd, it holds that
|fN(X)- f(x)1 ≤ E	|ak,jιιMd,j(X)- Mkj(X)I
(k,j)∈EN
≤ e X	Iαk,j ∣1{Mkj(x)=0}
(k,j)∈EN
≤ e(m +1)d(1 + K*)2K*(d/p-S)+ ||/|展,,
.log(N)N(VT+d-"d∕p-S)+ e∣∣fkBs
p,q
where we used the definition of K in the last inequality. Therefore, for each f ∈ U(BpS,q([0, 1]d)),
it holds that
kf -川 Lr . kf - fN kLr + kfN -川 Lr A lθg(N )N …LI ⑷Pf kf kBs J + N-s/d.
p,q
By taking E to satisfy log(N)N(V 1+d I)(d/p-S)+ E	≤ N-s/d (i.e., E ≤
N-s∕d-(V 1+d 1)(d/p-s)+ log(N)-1), then we obtain the approximation error bound.
Next, we bound the magnitude of the coefficients. Each coefficient aj,k satisfies ∣aj,k ∣ .
2k(d∕p-S)+ kfkms	≤ 2k(d/p-s)+ . N(V 1+d 1)(d/p-s)+ for k ≤ K*. Finally, the magni-
p,q
tudes of the coefficients hidden in Mdj are evaluated. Remembering that Mmj(x) = M(2kxι -
jι,..., 2k Xd - jd), we see that we just need to bound the quantity 2k (k ≤ K *). However, this is
bounded by 2k ≤ NV 1+d 1 for k ≤ K*. Hence, we obtain the assertion.
□
16
Published as a conference paper at ICLR 2019
C Proof of Theorem 1
C.1 Preparation: sparse grid
Here, we give technical details behind the approximation bound. The analysis utilizes the so called
sparse grid technique Smolyak (1963) which has been developed in the function approximation
theory field.
As we have seen in the above, in a typical B-spline approximation scheme, we put the basis functions
Mkm,j (x) on a “regular grid” for k = 1, . . . , K and (j1, . . . , jd) ∈ J(k), and take its superposition as
f(x) ≈ Pk=1,...,K Pj∈J(k) αk,j Mkm,j (x), which consists of O(2Kd) terms (see Eq. (10)). Hence,
the number of parameters O(2Kd) is affected by the dimensionality d in an exponential order. How-
ever, to approximate functions with mixed smoothness, we do not need to put the basis on the whole
range of the regular grid. Instead, we just need to put them on a sparse grid which is a subset of the
regular grid and has much smaller cardinality than the whole set. The approximation algorithm uti-
lizing sparse grid is based on Smolyak’s construction (Smolyak, 1963) and its applications to mixed
smooth spaces (Dung, 1990; 1991; 1992; Temlyakov, 1982; 1993a;b). Dung (2011a) studied an
optimal non-adaptive linear sampling recovery method for the mixed smooth Besov space based on
the cardinal B-spline bases. We adopt this method, and combining this with the adaptive technique
developed in Dung (2011b), We give the following approximation bound using a non-linear adaptive
method to obtain better convergence for the setting p < r.
Before we state the theorem, we define an quasi-norm of a set of coefficients αk,j ∈ R for k ∈ Zd+
and j ∈ Jm(k) := {—m, —m +1,..., 2k1 - 1, 2k1 }×∙∙∙× {-m, —m +1,..., 2kd - 1, 2kd} as
k(αk,j)k,jk叫,q=(χ bSTmkkkI ( X ∣αk,j ∣p)1"].
k∈Zd+	j ∈Jmd (k)
Theorem 5. Suppose that 0 < p, q, r ≤ ∞ and s > (1/p — 1/r)+. Assume that the order m ∈ N
of the cardinal B-spline satisfies 0 < s < min(m, m — 1 + 1/p). Let δ = (1/p — 1/r)+. Then, for
any f ∈ MBp,q (Ω) and K > 0, there exists RK (f) such that RK (f) can be represented as
nk
RK(f)(x) =	αk,jMkd,j(x) +	αk,j(k)Mkd,j(k) (x),
k∈Zd+: j∈Jmd (k)	k∈Zd+:	i=1	i
kkkι≤K	K<kk∣∣ι≤K*
where K * =「K(1 + s¾ )1，(j(k))n=kι ⊂ Jm (k), and n = d2K-s-δ (kkk1-K)], and satisfies the
following properties:
(i)	For p ≥ r,
kf—RK(f)kr.2-KsDK(1,/dmin(r,1)-1/q)+kfkMBps,q.
(ii)	For p < r,
kf—R (f)k . (2-KsDK(1,/dr-1/q)+kfkMBps,q	(r<∞),
K r	2-KsDK(1,-1/q)+kfkMBps,q	(r=∞).
Moreover, the coefficients (αk,j)k,j can be taken to hold k(αk,j)k,j kmbs . kf kMBs .
2K+1 Kd+—d1—1
The proof is given in Appendix C.2. The total number of cardinal B-spline bases consisting of
RK(f) can be evaluated as
+	nk
k：K<kkk1≤K*
.2kDκ,d + 2kDκ*,d . 2kDκ,d	(■： Eq. (17)).
17
Published as a conference paper at ICLR 2019
Here, DK,d can be evaluated as
DK,d . Kd-1 or DK,d . dK.
Therefore, the total number of bases can be evaluated as
2K min{Kd-1, dK}
which is much smaller than 2Kd which is required to approximate functions in the ordinal Besov
space (see Lemma 2). In this proposition, K controls the resolution and as M goes to infinity, the
approximation error goes to 0 exponentially fast. A remarkable point in the proposition is in the
construction of RK(f) in which the superposition is taken over kkk1 ≤ M instead of kkk∞ ≤
K * = O(K). Hence, the number of terms appearing in the summation is at most O(2k K d-1)
while the full grid takes O(2Kd) terms. This represents how the mixed smoothness is important to
ease the curse of dimensionality.
Several aspects of the m-Besov space such as the optimal N -term approximation error and Kol-
mogorov widths have been extensively studied in the literature (see a comprehensive survey (Dung
et al., 2016)). An analogous result is already given by Dung (2011a) in which S > 1/p is
assumed and a linear interpolation method is investigated. However, our result only requires
s > (1/p - 1/q)+. This difference comes from a point that our analysis allows nonlinear adap-
tive interpolation instead of (linear) non-adaptive sampling considered in Dung (2011a). Because of
this, our bound is better than the optimal rate of linear methods (Galeev, 1996; Romanyuk, 2001)
and non-adaptive methods (Dung, 1990; 1991; 1992; Temlyakov, 1982; 1993a;b) especially in the
regime of p < r (Dung (1992) also deals with adaptive methods but does not cover p < r for
adaptive method). See Proposition 2 for comparison.
C.2 Proof of Theorem 5
Proof of Theorem 5. For k = (k1, . . . , kd) ∈ Zd+, let Pk(i) f (x) be the function operating Pki defined
in (11) to f as a function of xi with other components xj (j 6= i) fixed, and let
d
pk := Y(Pk(ii) -Pk(ii)-1)f.	(15)
i=1
Then, pk can be expressed as pk(x) = Pj∈Jd (k) αk,jMkd,j(x).
Let Tk(i) = I - Pk(i) and kf kp,i be the Lp-norm of f as a function of xi with other components
xj (j 6= i) fixed (i.e., ifp < ∞, kf kpp,i = R |f(x)|pdxi), then Eq. (12) gives
kTk(ii)fkp,i .	sup	k∆rh,ii(f)kp,i.
∣hi∣≤2-ki
Thus, by applying the same argument again, it also holds
kkTk(ii)Tk(jj)fkp,ikp,j . k sup	k∆rh,ii(Tkjf)kp,ikp,j
∣hi∣≤2-ki
= sup	IIkTkj ∆h,i(f )kp,j kp,i (√ the definition of ∆, and Fubini,s theorem)
∣hi∣≤2-ki	"	2
. sup sup	kk∆rh,ii(∆rh,jj(f))kp,jkp,i,
∣hi∣≤2-ki |hj ∣≤2-kj
for i 6= j. Thus, applying the same argument recursively, for u ⊂ [d], it holds that
YTk(ii)f	.
wru,p(f,2-k),
i∈u
p
for k ∈	Zd+(u).	Therefore, since pk	=	Qid=1(Tk(ii)-1 - Tk(ii) )f
Pu⊂[d] (-1)|u| Qi∈u Tk(ii) Qi6∈u Tk(ii-) 1 f, by letting e = {i | ki > 0}, we have that
.X wU,p(f, 2-(ku)^) . X wu,p(f, 2-ku)
u⊂[d]	e⊂u
kpk kp. XMY Tki) Y Tki-J f
u⊂[d]	i∈u	i6∈u
18
Published as a conference paper at ICLR 2019
where kU := k (i ∈	U) and kiu := k -	1(i ∈ u),	U = {i | kU	≥ 0}, and (ku)u is a vector	such that
(ku)u,i = kU for i ∈	U and (ku)u,i = 0	for i ∈ U.	Now let
k(Pk)kkbα(LP)= ( X (2°kkk1kPkkLp)q!
k∈Zd+
for Pk ∈ Lp(Ω) (k ∈ Z+). Hence, if We set ak = Pe⊂u Wr,p(f, 2-ku) for k ∈ Z； and e = {i |
ki > 0}, we have that
k(pk)kkbqα(Lp). k(ak)kbqα(Lp)'kfkMBps,q.
On the other hand, following the same line of Theorem 2.1 (ii) of Dung (2011a), we also obtain the
opposite inequality kf kMBs ' k(ak)kbα(Lp) . k(pk)kkbα(Lp) (note that the analogous inequality
p,q	q	q
to Lemma 2.3 of Dung (2011a) also holds in our setting by replacing q§ with Ps and ωf(f, 2-k)p by
re,p).
Therefore, f ∈ MBpα,q if and only if (Pk)k∈Zd given by Eq. (15) satisfies k(Pk)kkbα(Lp) < ∞ and
f can be decomposed into f = Pk∈Zd Pk where convergence is in MBpα,q . Moreover, it holds that
kf kMBaq ' k(Pk)k kbα(Lp). This canbe shown by Theorem 2.1 of Dung (2011a). Moreover, by the
quasi-norm equivalence ∣∣pk ∣∣p ` 2-kkkl∕p(Pj∈jd ⑻ ∣αk,j |p)1/p,we also have ∣∣(αk,j )k,j ∣∣mb∣,q `
kf kMBα .
p,q
If p ≥ r, the assertion can be shown in the same manner as Theorem 3.1 of Dung (2011a).
For the setting of P < r, we need to use an adaptive approximation method. In the following, we
assume p <r. For a given K, by choosing K appropriately later, we set
RK (f)(x) =	Pk +	Gk (Pk),
k∈Z*"∣kkι≤K	k∈Z+=K<kk∣h≤K *
where Gk(Pk) is given as
Gk (Pk) = X αk,ji Mkd,ji (x)
1≤i≤nk
where (ak,ji )iJm(k)1 is the sorted coefficients in decreasing order of their absolute value: ∣αk,j∙11 ≥
|ak,j2| ≥ ：，≥ lak,jJd(k)||. Theiitholdsthat	'
∣Pk - Gk(Pk)∣r ≤ ∣Pk∣p2δkkk1nk-δ,
where δ := (1/p - 1/r) (see the proof of Theorem 3.1 of Dung (2011b) and Lemma 5.3 of Dung
(2011a)). Moreover, we also have
∣Pk∣r ≤ ∣Pk∣p2δkkk1
for k ∈ Z； with ∣∣k∣ι > K*.
Here, we define N as
N = dlog2(K)e.
Let E = (s - δ)∕(2δ), and
K * = dK (1 + 1/e)],
andnk = d2K-e(kkki-K)e for k ∈ Z； with K + 1 ≤ ∣∣k∣ι ≤ K*.
Then, by Lemma 5.3 of Dung (2011a), we have that
∣f-RK(f)∣rLr.	X	∣Pk - Gk(Pk)∣rLr +	X	∣Pk∣rLr
K<kkk1≤K*	K*<kkk1
.	X	[∣Pk∣p2δkkk1nk-δ]r+	X	[2δkkk1∣Pk∣Lp]r.	(16)
K<kkk1≤K*	K*<kkk1
19
Published as a conference paper at ICLR 2019
In the following, We require an upper bound of (k+--1). Hence, We evaluate this quantity before-
hand. This can be upper bounded by the Stering,s formula as
(k + d:1) ≤?C + T)Z + 八Γ≤%.
∖ d — 1 J	2π∖ k J ∖ d — 1J
、--------V----------}
= Dk,d
Let ξ > 0 be a positive real number satisfying 1 + ξ ≥ K*/K. We can see that ξ can be chosen as
ξ = 1/e + o(1). Then, we have that
D _D	(1 + d-1 )k* (1 + d-T )d-1 ≤ D (1 + 琮T )k*
K*,d	Kd (1+ dKl )K (1 + dκτ)d-l - Kd (1+ dKl )K
d-1
1 + dKι + (d — 1)(1 + d-T)
1
K *
d—1
≤ DK,d (1 + -K
K*-K
d — 1 + K *、d-1	d d — 1、ξK,	、d-1
1--T+k )	= DK (1 + ɪ) (1 + ξ)
≤ DK,de(d-1)ξ(1 + ξ)d-1' Dy.
(17)
(a)	Suppose that q ≤ r and r < ∞. Then
『q
kf — RK (f )1% = kf — Rk (f )kLr
<	∖ X	[2δkkk1 n-δ MkLp ]r+ X	[2δkkk1 弧山]r ∖	(∙.∙ Eq. (16))
[κ<∣∣k∣∣ι≤K*	K*<kkkι
<	X	[2δ"fc"1 n-δMkLp]q + X [2δkkkl 弧kLp]q
κ<∣∣k∣∣ι≤κ*	K*<∣∣k∣∣ι
≤ N-δq2-(s-δ)Kq	X	[2-(s-δ-f)(kkk1-K)2skkk1 kpkIlLp]q + 2-q(s-δ)κ*	X [2skkk1 kpkIlLp]q
κ<∣∣k∣∣ι≤κ *	≤1	K*<∣∣k∣∣ι
<	(N-δ2-(s-δ)κ + 2-(s-δ)κ*)qkfkMBs
p,q
≤ (N-s)q fkMBa .
p,q
(b)	Suppose that q > r and r < ∞. Then, letting V = q/r (> 1) and V = 1/(1 — 1/v) = q/(q — r)
(note that T + 十=1), we have
kf — Rk(f)kLr < E	[2δkkk1 n-δkPkkLp]r +	E [2δkkk1kPkkLp]r	(「Eq. (16))
K<∣∣k∣∣ι≤K*	K*<∣∣k∣∣1
≤ N-δr2-(s-δ)Kr	E	[2-(S-δ-*)(kkk1-K)2skkk1kPk∣∣Lp]r + E [2skkk1 kPk∣∣Lp]r(2-(S-δ)kkk1 )r
K<∣∣k∣∣1≤K*	K*<∣∣k∣∣1
≤ (N-δ2-(s-δ)κ + 2-(s-δ)κ*)r{ X	[2-(s-δ-*)(kkk1-K)2skkk1 kpk∣∣Lp]r
K<∣∣k∣∣1≤K*
+ X [2skkk1 kpk kLp]r2-(s-"(kkk1-κ*)r }
K*<∣∣k∣∣1
1/v
≤ (N-δ2-(s-δ)κ + 2-(S-δ)κ*)r ( E	[2skkk1 kPkkLp]rν + E [2skkk1 kPkkLp]rν
[κ<∣∣k∣∣1≤K*	K*<∣∣k∣∣1
(	∖ 1/J
× ) E [2-(s-δ-δe)(∣∣k∣∣1-K)]rJ + E	[2-(s-δ)( ∣∣ k ∣∣ 1-K* )]rν0
[κ<∣∣k∣∣1≤K*	K*<∣∣k∣∣1
< (N-δ2-(s-δ)κ + 2-(s-δ)κ*)rkfkMBs DK/-1/q)
p,q ,
< (N-SDK,L/q)rkfkMBs J
,	p,q
(∙.∙ Eq. (17))
20
Published as a conference paper at ICLR 2019
(c) Suppose that r = ∞. Then, similarly to the analysis in (b), we can evaluate
kf - RK (f)kLr
. N -δ 2-(s-δ)K
Σ
[2-(s-δ-δ)(kkk1-K)2skkk1 kpkkLp] +
K<kkkι≤K*
K*<kkkι
[2skkk1kpkkLp](2-(s-δ)kkk1)
.(N-δ2-(S-S)K + 2-(s-δ)K*)DK7/q)+kfkMBp,
. N-sDK(1,-d1/q)+kfkMBps,q.
□
C.3 Proof of Theorem 1
Let Zd+ (e) := {k ∈ Zd+ | ki = 0, i 6∈ e} and for k ∈ Zd+ (e), we define
2-k := (2-ki1 , . . . , 2-ki|e| ) ∈ R|+e| where {i1, . . . ,i|e|} = e. By defining k(gk)kkbsq,e :=
(pk∈z+(e)(2skkkl ∣gk Dq) for a sequence (gk)k∈z+(e),theηit holds that
|f |MBps,,qe ' X	k(wre,p(f,2-k))kkbsq,e.
e⊂{1,...,d}
Then, we can prove Theorem 1 based on Theorem 5 as follows.
Proof of Theorem 1. The result is immediately follows from Theorem 5. Let the set of indexes of
(k, j) consisting of RK be EK: RK(f) = P(k,j)∈E αk,jMkd,j(x). As in the proof of Proposition
1, we approximate RK(f) by a neural network given as
f(x) =	E	αk,jMMd,j(X).
(k,j)∈EK
Each coefficient αj,k satisfies ∣αj,k∣ . 2kkk1(1/p-s)+ kf kMBps,q . 2K*(1/p-s)+ . The difference
between RK(f) and f can be evaluated as
∣Rk(f) - f(χ)∣≤ E	∣αk,j∣∣Mdj(X)- Mk,j(X)∣
(k,j)∈EK
≤ X ∣αk,j∣1{Mkd,j(X) 6=0}
(k,j)∈EK
.e(m +1)d(1 + K*)DK*,d2K*⑴…kf ∣∣MBp,,
Therefore, by taking E so that e(m + 1)d(1 + K*)DK*,d2K*(1/p-S)+ ≤ 2-Ks is satisfied, it holds
that
∣Rk(f) - /(x)∣ . 2-Ks.
By the inequality Dκ*,d ≤ eκ*+d-1, it suffices to let E ≤ e[eKm++*ι+K+)1). The cardinality of
E(K) is bounded as
X 2κ κ+d-d-11
κ=0,...,K
+	nk
k.K<kkkι≤K*
≤2K+1 (K	+ d -	1) + X	2K-ɪ (K-K)	(K + d -	1
≤2	d-1	+	K<κ≤K*	2	d-1
≤2K+1Dκ,d + 2k(1 - 2-s-δ)-1Dκ*,d ≤ (2+(1 - 2-s2-δ)-1)2kDK*,d = N.
Since each unit Mkj requires width Wο, the whole width becomes W = NW°. The number of
nonzero parameters to construct Mdj is bounded by S =(L - 1)W02N + N.
21
Published as a conference paper at ICLR 2019
Finally, the magnitudes of the coefficients hidden in Mkj are evaluated. Remembering that
MMk j (x) = M(2k1 xι 一 jι,..., 2kdXd 一 jd), here maximum of 2kj is bounded by 2K* . N(I+1∕ν).
Hence, we obtain the assertion. Similarly, it holds that ∣aj,k | . N(I+1∕νK1∨(I/p-S)+}.	□
D Proofs of Theorems 2 and 3
ProofofTheorem 2. We use Proposition 4. We just need to evaluate the covering number of F 二
{f | f ∈ Ψ(L, W, S, B)} for (L, W, S, B) given in Theorem 1 where f is the clipped function for a
given f. Note that the covering number of F is not larger than that of Ψ(L, W, S, B). Hence, it is
sufficient to evaluate that of Ψ(L, W, S, B). From Lemma 3, the covering number is obtained as
log N(δ, F, ∣H∣∞) . Nlog(NHlog(N)2 +log(δ-1)].
From Proposition 1, it holds that
kfo - RK(fo)k2 . N-s/d.
Note that
kf - f0kL2(Pχ) . kf 一 f°k2∙
for any f : [0, 1]d → R because p(x) ≤ R. Therefore, by applying Proposition 4 with δ = 1/n, we
have that
EDnIkf - f0kL2(Pχ)] . N-2s/d + Nlog(N)(T +lOg(n))+ n.	(18)
1	1	d d	∙ d d	d
Here, the right hand side is minimized by setting N N n 2s+d up to log(n)3-order, and then have an
upper bound of the RHS as
n-2s+d log(n)3.
This gives the assertion.	□
Proof of Theorem 3. The proof follows the almost same line as the proof of Theorem 2. By noting
S = O(2K DK,d), L = O(K) and W = O(2K DK,d), Lemma 3 gives an upper bound of the
covering number as
logN(δ, F, k ∙ k∞) . 2kDκ,d[K2 log(2κDκ,d) + log(δ-1)] . 2kDκ,d(K3 +log(1∕δ)).
Letting r = 2, we have that
kfo - RK(fo)k2 . 2-SKDK,d
where u = (1 一 1∕q)+ forp ≥ 2 and u = (1∕2 一 1∕q)+ forp < 2.
Then, by noting that
kf - f0kL2(Pχ) . kf 一 fok2,
for any f : [0, 1]d → R, and by applying Proposition 4 with δ = 1∕n, we have that
EDn[kf - f0kL2(Pχ)] . 2-2SKDKUd + 2KDKdK3n+log(δτ)) + n.	(19)
Here, We use the following evaluations for DK,d (a) D&d . Kd-1, and (b) DkG . [e(1 + K)]k.
(a) For the evaluation, DK,d . Kd-1, we have an upper bound of the right hand side of Eq. (19) as
2-2sK K 2u(d-1) + 2K K d-1(K 3 + log(n))
n
which is minimized by setting K = dɪ+^ log2(n) + (2乜-?2,-1) log? log(n)] up to log log(n)-
order. In this situation, we have the generalization error bound as
--2s- 1	2 2 2(d-1)(u+s)	, 、3
n 2s+1 log(n)	1+2s	log(n)3.
22
Published as a conference paper at ICLR 2019
(b) For the evaluation, Dκ,d . [e(1 + K)]K ≤ eκed, Eq. (19) gives an upper bound of
2-2sK e2uK + 2K eK (K 2 +logS))
n
Then, the right hand side is minimized by K = d i+2s+(i-2u)iog2(e)log2(n)]. Then, We have that
n
2s-2u log2 (e)
1+2s+(I-2U)Iog2(IS) log(n)2.
This gives the assertion.
□
E	Minimax optimality
Proofof Theorem 4. First note that since Pχ is the uniform distribution, it holds that k ∙ ∣∣L2(Pχ)=
k ∙ ∣∣L2([o,i]d). The e-covering number N(e, G, L2(PX)) with respect to L2(PX) for a function class
G is the minimal number of balls With radius measured by L2 (Pχ)-norm needed to cover the set
G (van der Vaart & Wellner, 1996). The δ-packing number M(δ, G, L2 (PX)) of a function class
G with respect to L2(PX) norm is the largest number of functions {f1, . . . , fM} ⊆ G such that
∣fi - fj∣L2(PX) ≥ δ for all i 6= j. It is easily checked that
N(δ∕2, G,L2(Pχ)) ≤M(δ, G,L2(Pχ)) ≤N(δ, G,L2(Pχ)).	(20)
For a given δn > 0 and εn > 0, let Q be the δn packing number M(δn, U (MBps,q), L2 (Pχ)) of
U (MBps,q) and N be the εn covering number of that. Raskutti et al. (2012) utilized the techniques
developed by Yang & Barron (1999) to show the following inequality in their proof of Theorem
2(b):
δ2
inf	SUp	EDn[kf - f*∣∣L2(pχ)] ≥ inf	SUp	nPP[kf - f*llL2(Pχ)≥ δn∕2]
f f*∈U(MBp,q)	f f*∈U(MBp,q) 2
δP (	IOg(N) + 2P2εP + log(2) λ
≥"	ιog(Q	广
Thus by taking δn and εn to satisfy
5^^2 εn ≤ IOg(N),	(21)
2σ2 n
8 lOg(N) ≤ lOg(Q),	(22)
4 lOg(2) ≤ lOg(Q),	(23)
the minimax rate is lower bounded by δn. This can be achieved by properly setting εP ` δP. Now,
for given N with respect to δn > 0, M = lOg(N) satisfies
δn & M-Slog(M)(d-1)(s + 1/2-1/q) +
(Theorem 6.24 of Dung et al. (2016)). Hence, it suffices to take
1	2(d-1)(s + 1∕2-1∕q) +
M ` n2s+ι log(n) 口 ,	(24)
2s	2(d-1)(s + 1∕2-1∕q) +
εP ` δn ` n- 2s+1 log(n)	2s+1	,	(25)
which gives the assertion.
□
F	Auxiliary lemmas
Let the e-covering number with respect to L2 (Pχ ) for a function class G be N(e, G, L2 (Pχ )) as
defined in the proof of Theorem 4.
23
Published as a conference paper at ICLR 2019
Proposition 4 (Schmidt-Hieber (2018)). Let F be a set of functions. Let f be the least squares
estimator in F :
n
fb = argmin	(yi - f (xi))2 .
Assume that kfok∞ ≤ F and all f ∈ F satisfies kfk∞ ≤ F for some F ≥ 1. If δ > 0 satisfies
N(δ, F, k ∙ ∣∣∞) ≥ 3, then it holds that
EDn [kfb-fo kL2(Pχ)] ≤ C [ inf kf - f 0kL2(Pχ) + (F2 + σ2)log N (δ, F, k∙k∞) + δ(F + σ),
f∈F	n
where C is a universal constant.
Proof of Proposition 4. This is almost direct consequence of Lemma 10 of Schmidt-Hieber (2018)5.
The only difference is the assumption of ∣f ∣∞ ≤ F for f ∈ F and f = fo while Lemma 10 of
Schmidt-Hieber (2018) assumed 0 ≤ f (x) ≤ F0 for F0 > 1. However, this can be easily fixed by
shifting the function value by +F then the range of f is modified to [0, 2F]. Then, our situation is
reduced to that of Lemma 10 of Schmidt-Hieber (2018) by substituting F0 - 2F.	□
Lemma 3 (Covering number evaluation). The covering number of Φ(L, W, S, B) can be bounded
by
log N (δ, Φ(L, W,S, B), k ∙ ∣∞) ≤ S log(δ-1L(B ∨ 1)LT(W + 1)2L)
≤ 2SL log((B ∨ 1)(W + 1)) + S log(δ-1L).
Proof of Lemma 3. Given a network f ∈ Φ(L, W, S, B) expressed as
f (x) = (W(L)η(∙) + b(L)) ◦…◦ (W⑴X + b(1)),
let
Ak (f )(χ) = η ◦ (W (k-1)η(∙) + b(kT))。…。(W (I)X + b(1)),
and
Bk (f )(χ) = (W(L)η(∙) + b(L))。…。(W (k)η(x) + b(k)),
for k = 2, . . . , L. Corresponding to the last and first layer, we define BL+1 (f)(X) = X and
Aι(f )(x) = x. Then, it is easy to see that f (x) = Bk+ι(f)。(W(k) ∙ +b(k)) ◦ Ak(f )(x). Now,
suppose that a pair of different two networks f, g ∈ Φ(L, W, S, B) given by
f(x) = (W(L)η(∙)+b(L))。…。(W(I)X+b(1)), g(x) = (W(L)0η(∙)+b(L)O)。…。(W(I)OX+b(1)0),
has a parameters with distance δ: ∣∣W(')一 W(')0∣∞ ≤ δ and ∣∣b(') 一 b(')0∣∞ ≤ δ. Now, not that
∣Ak(f)k∞ ≤ maxjkW(kτ)kιkAι(f)∣∞ + ∣b(k-1)k∞ ≤ WB∣Ak-ι(f)k∞ + B ≤ (B ∨
1)(W + 1)∣Ak-1(f)∣∞ ≤ (B ∨ 1)k-1(W + 1)k-1, and similarly the Lipshitz continuity of Bk (f)
with respect to ∣∣ ∙ ∣∣∞-norm is bounded as (BW)L-k+1. Then, it holds that
|f (X) 一 g(X)|
L
=XBk+ι(g)。(W(k) ∙ +b(k))。Ak(f)(χ) - Bk+ι(g)。(W(k)0 ∙ +b(k)0)。Ak(f)(χ)
k=1
L
≤ X(BW)L-k∣(W(k) ∙ +b(k)) oAk(f)(χ) - (W(k)0 ∙ +b(k)0) oAk(f)(χ)∣∞
k=1
L
≤X(BW)L-kδ[W(B∨1)k-1(W+1)k-1+1]
k=1
5We noticed that there exit some technical flaws in the proof of the lemma, e.g., an incorrect application of
the uniform bound to derive the risk of an estimator. However, these flaws can be fixed and the statement itself
holds with a slight modification.
24
Published as a conference paper at ICLR 2019
L
≤X(BW)L-kδ(B∨1)k-1(W +1)k ≤ δL(B ∨ 1)L-1(W + 1)L.
k=1
Thus, for a fixed sparsity pattern (the locations of non-zero parameters), the covering number is
bounded by (δ∕[L(B ∨ 1)LT(W + 1)l]) S. There are the number of configurations of the spar-
sity pattern is bounded by ((W+1) ) ≤ (W + I)LS. Thus, the covering number of the whole space
Φ is bounded as
(W + 1)LS {δ∕[L(B ∨ 1)LT(W + 1)L]}-S = [δ-1L(B ∨ 1)l-1(W + 1)2L]S,
which gives the assertion.
□
25