Published as a conference paper at ICLR 2019
Soft Q-Learning with
Mutual-Information Regularization
Jordi Grau-Moya, Felix Leibfried and Peter Vrancx
PROWLER.io
Cambridge, United Kingdom
{jordi}@prowler.io
Ab stract
We propose a reinforcement learning (RL) algorithm that uses mutual-information
regularization to optimize a prior action distribution for better performance and
exploration. Entropy-based regularization has previously been shown to improve
both exploration and robustness in challenging sequential decision-making tasks.
It does so by encouraging policies to put probability mass on all actions. How-
ever, entropy regularization might be undesirable when actions have significantly
different importance. In this paper, we propose a theoretically motivated frame-
work that dynamically weights the importance of actions by using the mutual-
information. In particular, we express the RL problem as an inference problem
where the prior probability distribution over actions is subject to optimization.
We show that the prior optimization introduces a mutual-information regularizer
in the RL objective. This regularizer encourages the policy to be close to a non-
uniform distribution that assigns higher probability mass to more important ac-
tions. We empirically demonstrate that our method significantly improves over
entropy regularization methods and unregularized methods.
1	Introduction
Reinforcement Learning (RL) (Sutton & Barto, 1998) is a framework for solving sequential
decision-making problems under uncertainty. Contemporary state-of-the-art RL methods often use
an objective that includes an entropy regularization term (Haarnoja et al., 2018c; 2017; Teh et al.,
2017). Entropy regularized RL has been shown to capture multi-modal behaviour, as well as exhibit-
ing superior exploration (Haarnoja et al., 2017). Additionally, the learned policies are more robust,
as the entropy bonus accounts for future action stochasticity (Grau-Moya et al., 2016) and reduces
value overestimation (Fox et al., 2016).
While encouraging high-entropy policies can provide several benefits, it is possible to devise exam-
ples where entropy regularization actually impedes exploration. Since high-entropy policies tend to
spread the probability mass across all actions equally, they can perform poorly when the RL problem
contains actions that are rarely useful. In this paper, we propose to overcome the previous limitation
by designing a reinforcement learning algorithm that dynamically adjusts the importance of actions
while learning. We motivate our algorithm by phrasing RL as an inference problem with an adap-
tive prior action distribution. Where previous work assumes a uniform prior distribution over the
actions (Rawlik et al., 2012; Levine, 2018), we generalize the formulation by optimizing the prior.
We show that this optimization process leads to an RL objective function with a regularizer based
on the mutual information between states and actions.
Additionally, we develop a novel algorithm that uses such mutual-information regularization to ob-
tain an optimal action-prior for better performance and exploration in high-dimensional state spaces.
This novel regularizer for RL encourages policies to be close to the marginal distribution over ac-
tions. This results in assigning higher probability to actions frequently used by the optimal policy,
while actions that are used infrequently have lower probability under the prior. We demonstrate sig-
nificant improvements on 19 Atari games over a deep Q-network (Mnih et al., 2015) (DQN) baseline
without any regularization and over soft Q-learning (Schulman et al., 2017; Leibfried et al., 2018)
(SQL) that employs standard entropy regularization without prior adaptation.
1
Published as a conference paper at ICLR 2019
2	Background
2.1	Reinforcement Learning
We consider the standard Markov decision process (MDP) setting. Formally, an MDP is defined
as the tuple hS , A, P, R, γi where S is the state space, A the action space, and P : S × A ×
S → [0, 1] denotes the state transition function. Upon taking action at ∈ A in state st ∈ S, the
agent transitions to st+1 with probability P (st+1 |st, at). The reward function R : S × A → R
quantifies the agent’s performance. The goal is to find a policy that maximizes the value function,
i.e.
π* (a|s)
arg maxπ Vπ(s), where Vπ(s)
E PtT=0 γtr(st, at)|s0 = s .
Here, γ is a
discount factor (0 < γ < 1) that allows to account for the future in different ways.
The policy-dependent state transition probabilities are defined as Pn(Sls) := Ea P(s0∣a, s)∏(a∣s)
which can be written in matrix notation as Pπ ∈ R|S | × R|S| where the rows are indexed by s and the
columns by s0. This allows us to conveniently define the agent’s stationary distribution over states
μ∏ (s) and actions ρ∏ (a) as follows:
Definition 1 (Stationary distribution over states). The stationary distribution over states (assumed
to exist and to be unique) is defined in vector form as μ> := limt→∞ ν>P∏ with ν0 being an
arbitrary vector of probabilities over states at time t = 0. The stationary distribution satisfies
μ∏ (s0) = Es Pn(s0∣s)μ∏(s) and therefore is a fixed point under the state transition probabilities
μπ = μπ Pn ∙
Definition 2 (Stationary distribution over actions). Let μ∏(s) be the stationary distribution over
states induced by the policy π. Then the stationary distribution over actions under the policy π is
defined as Pn (a) := Es∈s μ∏ (s)π(a∣s).
r(st, at) - 1 log∏(at∣st)) Iso = s
(1)
2.2	Maximum Entropy Reinforcement Learning
Maximum entropy reinforcement learning augments the standard RL reward objective with an addi-
tional policy entropy term. The optimal value function under entropy regularization (Haarnoja et al.,
2017) is defined as:
∞
V*(s) = maxE	Yt
n
t=0
where 1 trades off between reward and entropy maximization, and the expectation operation is
over state-action trajectories. The optimal policy that solves (1) can be written in closed form as:
π*(a∣s) = P eβQ βQa(s,a), where Q*(s, a) := r(s, a) + Pso∈s P(s0∣s,a)V*(s0). Note that the
a∈A e	,
above represents a generalization of standard RL settings, where β → ∞ corresponds to a standard
RL valuation (lime→∞ V*(s) = max∏ Vn(s)), while for β → 0 We recover the valuation under a
random uniform policy. For intermediate values ofβ, we can trade off between reward maximization
and entropy maximization.
Interestingly, one can formulate the maximum entropy RL objective as an inference prob-
lem (Levine, 2018) by specifying a prior distribution over trajectories that assumes a fixed uniform
distribution over actions. Precisely this assumption is what encourages the policies to maximize
entropy. As outlined in the introduction, encouraging policies to be close to a uniform distribution
might be undesirable when some actions are simply non-useful or not frequently used.
In Section 3, we show that when relaxing the previous assumption, i.e. allowing for prior opti-
mization, we obtain a novel variational inference formulation of the RL problem that constrains the
policy’s mutual-information between states and actions. We show that such policies must be close to
the marginal distribution over actions which automatically assigns high probability mass to overall
useful actions and low probability to infrequently used actions.
Before proceeding, however, it is insightful to show how prior optimization bridges the gap between
entropy regularization and mutual-information regularization in a non-sequential decision-making
scenario that considers one time step only.
2
Published as a conference paper at ICLR 2019
2.3	Mutual-Information Regularization for One-Step Decision-Making
In a one-step decision-making scenario, entropy regularization assumes the following form
max	P(s)π(a∣s) (r(s,a) 一 ɪ logπ(a∣s) j ,
π sa	β
s,a
where P(S) is some arbitrary distribution over states and the optimal policy balances expected reward
maximization versus expected entropy maximization.
Entropy regularization discourages deviations from a uniform prior policy. In a more general setting,
when discouraging deviations from an arbitrary prior ρ(a), a similar objective can be written as
max XP(s)π(a∣s) (r(s,a) - 1 log ；[；))=
max X p(s)n(a|s)r(s,a) - 1 X P(S)KL(π3s)iip(∙)),
(2)
where KL refers to the Kullback-Leiber (KL) divergence. This framework has been proposed before
in the literature under the name information-theory for decision-making (Ortega & Braun, 2013).
Going one step further, one can also optimize for ρ in addition to π, which essentially means that
policies are discouraged to deviate from an optimal prior distribution, leading to the following opti-
mization problem
max XP(S)n(a1s)r(s，a)- IminXP(S)KL(n(・1s)|1p(»
s,a	s
(3)
where we utilize the fact that only the expected KL penalty depends on ρ.
The minimum expected KL relates to the mutual information as follows:
Proposition 1 (Mutual Information). Let If be a functional, in particular: If(pX,pY |X, qY ) :=
XPxpχ(X)KL(pγ∣χ(∙∣x)∣∣qγ(∙)), where pχ(x) is the distribution of the input, pγ∣χ(y|x) is the
conditional distribution of the output conditioned on the input, and qY (y) a variational distribution
of the output. Then, the mutual information 1 is recovered with
I [X,Y] = mmin If(PX ,Py ∣χ ,qγ),
where the optimal variational distribution is qY(y) = Expχ(x)pγ∣χ(y|x), i.e. the true marginal
distribution. See e.g. (Cover & Thomas, 2006, Lemma 10.8.1) for details.
This allows us to rewrite the problem from Equation (3) as
max	p(s)π(a∣s)r(s, a) - —S[S; A],
πβ
s,a
(4)
yielding a penalty on the mutual information between states and actions.
Notice that this problem is mathematically equivalent to rate-distortion theory from information-
theory (Shannon, 1959) which formulates how to efficiently send information over an information-
theoretic channel with limited transmission rate. This framework has also been used to describe
decision-making problems with limited information budgets (Sims, 2011; Genewein et al., 2015;
Leibfried & Braun, 2015; 2016; Peng et al., 2017; Hihn et al., 2018). In a decision-making context,
the agent is considered as information-theoretic channel ∏(a∣s) where S is the channel input and a the
channel output. The agent aims at maximizing expected reward under the constraint that the infor-
mation transmission rate is limited, where the transmission rate is given by the mutual-information
between states and actions (Cover & Thomas, 2006). Intuitively, this means that the agent has to
discard reward-irrelevant information in s to not exceed the limits in information transmission.
In the following section, we generalize the rate-distortion formulation for decision-making to be
applicable to a sequential decision-making scenario, i.e. the RL setting. We propose an inference-
based formulation where the mutual information arises as a consequence of allowing for optimizing
the action-prior distribution.
1i(x;Y) ：= Px,yP(χ,y)iogPYPyxpX)(X) = PxPX(X)KL(py∣x(∙∣x)IIpy(∙))
3
Published as a conference paper at ICLR 2019
3	Mutual-Information Regularization in RL
In this section, we first derive mutual-information regularization for the RL setting from a variational
inference perspective. Subsequently, we derive expressions for the optimal policy and the optimal
prior that are useful for constructing a practical algorithm.
3.1	Variational Inference RL Formulation with Optimal Action-Priors
The RL problem can be expressed as an inference problem by introducing a binary random variable
R that denotes whether the trajectory τ := (s0, a0, . . . sT, aT) is optimal (R = 1) or not (R = 0).
The likelihood of an optimal trajectory can then be expressed as p(R = 1 ∣τ) 8 exp(PT=o r(st,at))
(Levine, 2018). We additionally introduce a scaling factor β > 0 into the exponential, i.e. p(R =
1∣τ) 8 exp(β PT=O r(st, at)). This Will allow Us to trade off reward and entropy maximization 2.
Next, we can define the posterior trajectory probability assuming optimality, i.e. p(τ |R = 1).
Here we treat τ as a latent variable with prior probability p(τ), and we specify the log-evidence
as logp(R = 1) = log /p(R = 1∣τ)p(τ)dτ. We now introduce a variational distribution q(τ) to
approximate the posterior p(τ |R = 1). This leads to an Evidence Lower BOUnd (ELBO) of the
previous expression (scaled by 1 )3:
11
βlog PpR =I) = β logJ P(R
1∣τ )p(τ )dτ
≥ β ET〜q(T)
^ln OpR=JlTMTI
.g	q(τ)
(5)
The generative model is written as p(τ) = P(So) QT=OI ρ(at)P(st+ι∣st, at) and the variational
distribution as q(τ) = p(so) Qt=OI ∏(at∣st)P(st+ι∣st, at). The RL problem can now be stated as a
maximization of the ELBO w.r.t π. The maximum entropy RL objective is recovered when assuming
a fixed uniform prior distribution over actions, i.e. ρ(at)= 旨 for all t.
We obtain a novel variational RL formulation by introducing an adaptive prior over actions ρ. Con-
trary to maximum entropy RL, where the prior of the generative model is fixed and uniform, here the
prior over actions is subject to optimization. Starting from Equation (5) and substituting p(τ) and
q(τ) we obtain the following ELBO: max∏,ρ Eq IPT=O (r(st, at) — 1 log 蓝：1：?
i . Since we are
interested in infinite horizon problems, we introduce a discount factor and take the limit limT →∞
(Haarnoja et al., 2017). This leads to the optimization objective that we use in our experiments:
max Eq
π,ρ
∞1
EYt (r(st,at) 一 β log
t=O	β
∏(at∣st)
ρ(at)
(6)
where 0 < γ < 1 is the discount factor. In the following, we show that the the solution for the prior
and the policy can be expressed in a concise form giving rise to a novel RL regularization scheme.
3.2	Recursion, Optimal Policies and Optimal Priors
Crucial for the construction of a practical algorithm are concise expressions for the optimal pol-
icy and the prior. More concretely, the optimal policy takes the form of a Boltzmann distribution
weighted by the prior ρ. When fixing the policy, the optimal prior is the marginal distribution over
actions under the discounted stationary distribution over states. This finding is important to devise a
method for efficiently learning an optimal prior in practice.
Optimal policy for a fixed prior ρ: We start by defining the value function with the information
cost as V∏,ρ(s) := E [P∞=0 γt 卜(st, at) — 1 log 端苧)|so = s] where one can show that V∏,ρ
2Other authors absorb this scaling factor into the reward, but we keep it as an explicit hyperparameter.
3This is obtained by multiplying and dividing by q(τ) inside the integral and applying Jensen’s inequality
f (Ex〜q[x]) ≥ Ex〜q[f (x)] (for any concave function f).
4
Published as a conference paper at ICLR 2019
satisfies a recursion similar to the Bellman equation:
Vπ,ρ(S)= En r(s,a) -万 log (J、) + YEsO [V∏,P(SO)] .	(7)
β	ρ(a)
When considering a fixed ρ, the problem of maximizing Equation (7) over the policy can be solved
analytically by standard variational calculus (Rubin et al., 2012; Genewein et al., 2015). The optimal
policy is then given by
∏*(a∣s)：=万ρ(a)exp(βQ∏*,ρ(s,a))	(8)
Z
with Z = Pa ρ(a) exp(βQπ*,ρ(S, a)), and the the soft Q-function is defined as
Qπ,ρ(S, a) := r(S, a) + γEs0 [Vπ,ρ(S0)].	(9)
Being able to write the optimal policy in this way as a function of Q-values is needed in order to
estimate the optimal prior as we show next.
Optimal prior for a fixed policy: In order to solve for the optimal prior, we rewrite the problem
in Equation (6) as
Imax XX γ tνt(4 s) Xn(a|s) (r(s, G- 1 log πa))),
where we have defined the marginal distribution over states at time t as
Vt(S)=	E	p(sο) ɪɪ ∏(atο∣Stο)P (St，+1 ∣Stο ,at，) ∏(at-i ∣St-i)P (s∣st-i, at-i).
s0,a0,...,st-1,at-1	t0=0
(10)
For fixed π, we eliminate the max operator for π and all components that do not depend on ρ:
∞
max-Tj XX YtVt(S)KL(π(∙ls)“ρ(∙))∙
ρ β t=0 s
Swapping the sums and letting p(S) := Pt∞=0 YtVt (S) be the unnormalized discounted marginal
distribution over states, We obtain maxρ -ɪ PsP(S)KL(π(∙∣s)∣∣ρ(∙)). The solution to the latter,
ρ?(a) = PS p(s)∏(als) , can easily be obtained by adding the constraint that the action-prior is
s,a P(s)π(a∣s)'
a valid distribution (i.e., Pa ρ(a) = 1 and ρ(a) > 0 ∀a), and using the method of Lagrange
multipliers and standard variational calculus. The connection to the mutual information becomes
clear when plugging ρ? back into the objective yielding -k ∙ ɪ I (S, A) scaled by a positive constant
k (because p(S) is not normalized) that can be absorbed into β. Additionally, We also formalize the
connection to the stationary mutual-information for the limit case of Y → 1 in the Appendix.
With the form of the optimal prior for a fixed policy at hand, one can easily devise a stochastic
approximation method (e.g. ρi+ι(a) = (1 - αρ)ρi(a) + αρ∏(a∣S) with ɑ` ∈ [0,1] and S 〜 p(∙)
to estimate the optimal ρ using the current estimate of the optimal policy from Equation (8). We
note that here we sample from the undiscounted distribution over states instead, rather than the true
discounted state-distribution in the equation above. This is a common practice used in actor-critic
RL that results in a biased estimator of the objective (Thomas, 2014).
4 MIRL: A Practical Mutual Information RL Algorithm
In this section, we present the MIRL agent. We focus on the tabular setting first and then port our
algorithm to high-dimensional state spaces that require parametric function approximators.
4.1 MIRL for Tabular Q- Learning
Our tabular MIRL agent is a modification of an ordinary Q-learning agent with a different update
scheme for Q-values. In parallel to updating Q-values, the MIRL agent needs to update the prior ρ
as well as the parameter β . The behavioral policy π is also different and utilizes soft Q-values. This
is outline in more detail below.
5
Published as a conference paper at ICLR 2019
Prior Updates: We approximate the optimal prior by employing the following update equation,
Pi+ι(a) = (1 - αρ)ρi(a) + αρ∏(a∣Si)	(11)
where Si 〜％(•) and αρ is a learning rate. Assuming a fixed policy, it is easy to show that this
iterative update converges to ρ∏(a) = Ps μ∏ (s)π(a∣s), thus estimating correctly the optimal prior.
Q-function Updates: Concurrently to learning the prior, MIRL updates the tabular Q-function as
Q(S, a) - Q(S, a) + αQ ((TPiftQθ)(s, a, s0) - Q(S, a))	(12)
where αQ is a learning rate and TsρoftQ is the empirical soft-operator defined as (TsρoftQ)(s, a, s0) :=
r(s, a)+γ 1 log Pao ρ(a0) exp (βQ(s0, a0)). Importantly, this operator differs from other soft opera-
tors arising when employing entropy regularization. Entropy regularization assumes a fixed uniform
prior, whereas in our case, the optimal prior is estimated in the course of learning.
Behavioural policy: Since the Q-function can be learned off-policy, the experience samples can
conveniently be drawn from a behavioural policy πb different from the current estimate of the op-
timal policy. As such, the behavioural policy used in our experiments is similar in spirit to an
-greedy policy but it better exploits the existence of the estimated optimal prior when both explor-
ing and exploiting. When exploring, MIRL’s behavioural policy samples from the current estimate
of the optimal prior ρi which has adjusted probabilities, in contrast to vanilla -greedy that samples
all actions with equal frequency. Additionally, when exploiting, MIRL selects the maximum prob-
ability action that depends not only on the Q-values but also on the current estimate of the optimal
action-prior, instead of selecting the action with highest Q-value as in traditional -greedy. More
formally, given a random sample U 〜 UnifOrm[0,1] and epsilon G the action a is obtained by
a arg maxa ∏i(a∣Si), if u > e
OJtL [a 〜pi(∙)	if u ≤ €,
where ∏i(a∣s) = -Zρi(a)exp(βiQi(s, a)),see Equation (8).
Parameter β Updates: The parameter β can be seen as a Lagrange multiplier that quantifies the
magnitude of penalization for deviating from the prior. As such, a small fixed value of β would
restrict the class of available policies and evidently constrain the asymptotic performance of MIRL.
In order to remedy this problem and obtain better asymptotic performance, we use the same adaptive
β-scheduling over rounds i from (Fox et al., 2016) in which βi is updated linearly according to
βi+ι = C ∙ i with some positive constant c. This update favours small values of β at the beginning
of training and large values towards the end of training when the error over Q-values is small.
Therefore, towards the end of training when β is large, MIRL recovers ordinary Q-learning without
a constraint. This ensures that the asymptotic performance of MIRL is not hindered.
4.2 MIRL with Parametric Function Approximators
For parametric function approximators, the scheme for updating the prior and the behavioural policy
is the same as in the tabular setting but Q-function updates and β-scheduling need to be adjusted for
high-dimensional state spaces. The pseudocode of our proposed algorithm is outlined in Algorithm 1
and follows standard literature for parametric value learning, see e.g. Mnih et al. (2015).
Q-function Updates: Q-function parameters are obtained by minimizing the following loss
L(θ, ρ)
:= Es,a,r,s0 〜M ]((TotQ°)(s,a,s0)- Qθ(s,α))21	(13)
where M isa replay memory (Mnih et al., 2015), Qa isa target network that is updated after a certain
number of training iterations and TsρoftQ is the empirical soft-operator from the tabular setting, here
repeated for convenience (ToftQ)(s, a, s0) := r(s, a) + Yɪ log P&o ρ(a0) exp (βQ(s0, a0)).
Parameter β Updates: We use the same adaptive β-scheduling from Leibfried et al. (2018) in
which βi is updated according to the inverse of the empirical loss of the Q-function, i.e. βi+1 =
(1 - αβ)βi + αβ( l(θ p +])).This provides more flexibility than the linear scheduling scheme from
the tabular setting, more suitable for high-dimensional state spaces where it is impossible to visit all
state-action pairs.
6
Published as a conference paper at ICLR 2019
Algorithm 1 MIRL
1:	Input: the learning rates αρ, αQ and α0, a Q-network Qθ(s, a), a target network Qθ(s, a), a
behavioural policy πb , an initial prior ρ0 and parameters θ0 at t = 0.
2:	for i = 1 to N iterations do
3:	Get environment state Si and apply action ai 〜∏b(∙∣si)
4:	Get ri, si+1 and store (si, ai, ri, si+1) in replay memory M
5:	Update prior pi+ι(∙) = Pi(∙)(1 - αρ) + αρ∏z(∙∖s^^
6:	if i mod update frequency == 0 then
7:	Update Q-function θi+1 = θi — αQ VθL(θ, ρi+1)∖θi according to Equation (13)
8:	Update parameter βi+ι = (1 — α0)βi + α0 (L(θi,pi+1))
9:	end if
10:	end for
Grid World 3x20
—QL
SQL
SQ 5
MlRL
p∙l"Mα,α
Grid World 8x8
200	400	600	800	1000
Environment interactions (×102)
200	400	600	800	1000
Environment interactions (×102)
Correct Infrequent Action
Uo-40410auo0u- -0-eφ*l) Co-S luauoɔ ：i
2500 5000 7500 10000 12500 15000
Interactions in state (3,6)
Figure 1: Grid world experiments. Left column: The top shows a corridor grid world with 3 × 20
cells where the goal is on the right. The bottom shows an 8 × 8 grid world where an important
action (left) has to be made exactly once to arrive at the goal. Middle column: Evaluation of Q-
learning (QL), SQL with standard uniform exploration and with marginal exploration (SQL_m), and
MIRL. We clearly see that MIRL outperforms the baselines on the corridor, and is comparable to the
baselines on the 8 × 8 world. Right column: We see that MIRL is able to identify the correct action
(go right) faster than the baselines in the corrider (top). The bottom reports how having infrequent
but important actions does not affect the performance of MIRL.
5 Experiments
We evaluate our MIRL agent both in the tabular setting using a grid world domain, and in the
parametric function approximator setting using the Atari domain.
5.1 Grid World
As an intuitive example, we evaluate our method in a grid world domain where the agent has to
reach a goal. Reaching the goal gives a reward of 9 but each step yields a reward of -1. After the
end of an episode (when reaching the goal), the agent’s location is randomly re-sampled uniformly
over the state space. We compare against two baselines, Q-learning without any regularization,
and SQL (Fox et al., 2016) which employs entropy regularization with the dynamic β-scheduling
scheme outlined earlier. We train the agents for 2.5 ∙ 105 environment steps following the procedure
outlined in Fox et al. (2016). Both SQL and MIRL update the Lagrange multiplier β over time by
using a linear scheduling scheme with a constant c = 10-3. MIRL additionally updates the estimate
of the optimal prior by using a learning rate αρ = 2 ∙ 10-3. In all experiments, We use an adaptive
7
Published as a conference paper at ICLR 2019
Figure 2: Left panel: Median normalized score across 19 Atari games. Comparison between our
method mutual information RL (MIRL), SQL and DQN, demonstrating MIRL’s superior perfor-
mance. Right panels: Top figures show the raw score for 2 example games reporting MIRL’s
superior performance on RoadRunner and Seaquest. The bottom plots show the evolution of the es-
timated prior over actions. For RoadRunner the prior converges to stable values during training. In
Seaquest, the algorithm seems not to have converged yet after 50 million environment steps which
is why the prior probabilities have not converged yet either (however, the formation of separate
trajectory clusters towards the end of training indicates the ongoing process of convergence). See
Appendix for details and plots for all environments. The curves are smoothed with an exponential
moving average with effective window size of 106 environment steps.
Road RunnerNoFramesIcI p-v4
0.0
∂ i 2	3	4	5
Enviivnment Steps le7
0.125
.^0.100
0.075
0050
0.025
SeaquestNoFra mesld p-v4
0.000
learning rate for Q-values αQ = n(s, a)-ω that depends on the state-action-visitation frequencies
n(s, a) (Fox et al., 2016). See Appendix for further details.
During the training of each algorithm, snapshots of the Q-tables (and the estimate of the prior in the
case of MIRL) are stored every 100 environment steps for evaluation. The evaluation for a single
snapshot is conducted by running the policy for 30 episodes lasting at most 100 environment steps.
The epsilon value when in evaluation mode is set to = 0.05 (same as in training). Every individual
experiment is repeated with 10 different initial random seeds and results are averaged across seeds.
Figure 1 summarizes our grid world experiments on two instances: a corridor where the goal is
to the right, and a square world where one important action has to be executed exactly once. In
the corridor, MIRL clearly outperforms competing approaches (Q-learning and SQL), whereas in
the square world, MIRL attains comparable performance as the baselines. Note that these results
remain valid when equipping SQL with the same marginal exploration scheme as MIRL.
5.2 Atari
We conduct experiments on 19 Atari games (Brockman et al., 2016) with Algorithm 1 (MIRL),
and compare against DQN (Mnih et al., 2015) and SQL (Haarnoja et al., 2018c) with a dynamic
β-scheduling scheme based on loss evolution that leads to improved performance over vanilla SQL
with fixed β (Leibfried et al., 2018). The three algorithms use a neural network for the estimation of
Q-values as in (Mnih et al., 2015). The network receives as an input the state s which is composed
of the last four frames of the game with some extra pre-processing (see Appendix), and it outputs a
vector of Q-ValUes, i.e. one value for each valid action. We train the network for 5 ∙ 107 environment
steps, where a training iteration is performed every four steps. The target network Qg is updated
every 104 training iterations. Both SQL and MIRL update the Lagrange multiplier β over time by
using an exponential moving average of the inverse loss4 (Leibfried et al., 2018) with ɑg = 3 ∙ 10-5.
In addition, MIRL updates the estimate of the optimal prior by using a learning rate aρ = 5 ∙ 10-5.
Additional details can be found in the Appendix.
For evaluation, we create snapshots of the network agents every 105 environment steps. Evaluating
a single snapshot offline is done by running the policy for 30 episodes that last at most 4.5 ∙ 103
environment steps but terminate earlier in case of a terminal event. When evaluating the agents, the
4Pracitcally, we replace the squared loss with the Huber loss Mnih et al. (2015).
8
Published as a conference paper at ICLR 2019
epsilon value is = 0.05, whereas in training is linearly annealed over the first 106 steps (Mnih
et al., 2015) from 1.0 to 0.1.
To summarize the results across all games,
we normalize the episodic rewards ob-
tained in the evaluation. The normal-
ized episodic rewards are computed as fol-
lows ZnOrmaIized = Z^z~-zdom_ ∙ 100%,
zhuman -zrandom
where z stands for the score obtained
from our agent at test time, zrandom stands
for the score that a random agent ob-
tains and zhuman for the score a human ob-
tains. Random and human scores are taken
from Mnih et al. (2015) and Van Hasselt
et al. (2016). As seen in Figure 2, our algo-
rithm significantly outperforms the base-
lines in terms of the median normalized
score. In particular, after 50 million inter-
actions we obtain about 30% higher me-
dian normalized score compared to SQL
and 50% higher score compared to DQN.
MIRL attains the final performance of
SQL in about half the amount of inter-
actions with the environment and, simi-
larly, it attains DQN’s final performance in
about five times less interactions.
In Table 1 we show the comparison be-
tween best-performing agents for all the
Game	DQN (%)	SQL (%)	MIRL (%)
Alien	101.58	51.02	40.23
Assault	250.61	283.62	357.40
Asterix	166.32	242.73	330.19
Asteroids	9.74	8.57	7.80
BankHeist	97.12	94.62	166.26
BeamRider	99.16	113.64	117.21
Boxing	2178.57	2283.33	2338.89
ChopperCommand	72.71	26.37	65.03
DemonAttack	350.95	451.78	469.30
Gopher	474.18	538.87	429.44
Kangaroo	351.48	393.16	405.9
Krull	843.16	886.68	1036.04
KungFuMaster	122.14	142.04	121.41
Riverraid	77.21	109.37	76.02
RoadRunner	548.90	613.62	695.88
Seaquest	21.95	36.00	64.86
SpaceInvaders	166.62	200.38	164.79
StarGunner	653.44	681.12	574.89
UpNDown	183.19	230.82	394.21
Mean	356.26	388.83	413.46
Table 1: Mean Normalized score in 19 Atari games for
DQN, SQL and our approach MIRL.
environments, where the best-performing agent is the agent that achieves the best score in evalu-
ation mode considering all snapshots. Although this measure is not very robust, we include it since
it is a commonly reported measure of performance in the field. MIRL outperforms the other base-
lines in 11 out of 19 games compared to SQL and DQN that are best on 5 and 3 games respectively.
In Figure 3, we conduct additional experiments on a subset of eight Atari games comparing MIRL
and DQN with two different ablations of SQL: one ablation using uniform exploration and another
ablation using the same marginal exploration scheme as MIRL (denoted SQL_m). These experi-
ments confirm the importance of the difference in values between MIRL and SQL rather than the
difference in the exploration protocol.
6 Related Work
The connection between reinforcement learning and inference is well established (Dayan & Hin-
ton, 1997; Levine, 2018). Several authors have proposed RL algorithms based on optimizing the
ELBO in Equation (5). In policy search, a popular approach is to optimize the lower bound using an
iterative procedure similar to expectation maximization (Deisenroth et al., 2013). Different approx-
imations can then be used for the trajectory distributions, resulting in different algorithms (Kober
& Peters, 2009; Peters et al., 2010; Hachiya et al., 2011). The recent maximum a posteriori policy
optimisation (MPO) (Abdolmaleki et al., 2018) framework uses a similar approach and combines
an expectation maximization style update with off-policy estimation of a regularized Q-function. A
key difference between MPO and our method is that MPO treats the approximate distribution q(τ)
as an auxiliary distribution used to optimize the policy that generates p(τ). In our method, as well
as in the maximum entropy based methods discussed below, we optimize the policy used to generate
q(τ), while using the distribution p(τ) generated by the prior as an auxiliary distribution. While
both approaches can be related to optimizing the ELBO, they can be shown to optimize different
versions of the KL constraint on the target policy (Levine, 2018).
Maximum entropy reinforcement learning represents another family of inference-based RL meth-
ods. This formulation can be derived from the same evidence lower bound in Equation (5), by fixing
the generative policy for p(τ ) to be a uniform prior. Maximum entropy RL has been derived under
9
Published as a conference paper at ICLR 2019
different conditions in many different settings. Ziebart et al. (2008) proposed maximum entropy
learning for inverse reinforcement learning problems. Several authors have applied the same prin-
ciples for trajectory optimization (Kappen, 2005; Todorov, 2008; Levine & Koltun, 2013). These
maximum entropy methods typically assume the availability of some sort of transition model. More
recently, maximum entropy learning has also been studied in model-free settings by introducing al-
ternative soft-operators (Asadi & Littman, 2017) or soft Q-learning approaches (Rawlik et al., 2012;
Fox et al., 2016; Haarnoja et al., 2017). Soft Q-learning learns a softened value function by replac-
ing the hard maximum operator in the Q-learning update with a softmax operator. Several authors
have discussed the benefits of this approach and provided generalizations under linear programming
formulations (Neu et al., 2017). In particular, Fox et al. (2016) and Haarnoja et al. (2017) show
that maximum entropy learning improves exploration and robustness. Furthermore, Haarnoja et al.
(2018b) show that the resulting policies are composable and can be used to directly build solutions to
unseen problems. Additionally, entropy-regularization has shown to be crucial to prove convergence
guarantees on value learning with non-linear function approximators (Dai et al., 2018). The soft Q-
learning framework has also been used in actor-critic settings (Haarnoja et al., 2018c) and to show a
connection between value-based and policy gradient methods (Schulman et al., 2017; Nachum et al.,
2017). The method has also been extended to hierarchical settings (Florensa et al., 2017; Haarnoja
et al., 2018a). In the multi-task setting, the Distral framework (Teh et al., 2017) combines entropy
regularization with an additional KL regularization used to transfer knowledge between tasks.
Environment Steps	le7
Figure 3: Normalized score for eight games
comparing MIRL against standard SQL and
a modified version of SQL that explores
with the marginal distribution over actions
(SQL_m). The exploration method slightly
improves SQL but not sufficiently enough to
achieve MIRL’s performance. See individual
plots and games in the Appendix.
The mutual information, central to our approach,
is a basic quantity in information theory to mea-
sure the statistical dependence between two ran-
dom variables. Machine learning applications that
use the mutual information are numerous including
the information-bottleneck method (Tishby et al.,
1999), rate-distortion theory (Cover & Thomas,
2006; Tishby & Polani, 2011), clustering (Still &
Bialek, 2004) and curiosity driven exploration (Still
& Precup, 2012).
7 Discussion and Conclusion
Using a variational inference perspective, we derived
a novel RL objective that allows optimization of the
prior over actions. This generalizes previous meth-
ods in the literature that assume fixed uniform priors.
We show that our formulation is equivalent to apply-
ing a mutual-information regularization and derive
a novel algorithm (MIRL) that learns the prior over
actions. We demonstrate that MIRL significantly im-
proves performance over SQL and DQN.
We recognize that our approach might fail under cer-
tain conditions. For example, in the case when there
is an action that is useful only once (similar to our
8 × 8 grid world example) but is most of the times penalized with a negative reward. When the neg-
ative reward is too strong, MIRL might assign very low probability to that action and never explore
it. However, this problem might be alleviated by a weighted mixing of our exploration policy with a
uniform distribution.
An interesting direction for future work is to investigate the convergence properties of the alternating
optimization problem presented here. We believe that, at least in the tabular case, the framework
of stochastic approximation for two timescales (Borkar, 2009) is sufficient to prove convergence.
On the experimental side, one could also investigate how our approach can be combined with the
Rainbow framework (Hessel et al., 2017) which is the current state of the art in performance.
10
Published as a conference paper at ICLR 2019
References
Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Mar-
tin Riedmiller. Maximum a posteriori policy optimisation. arXiv preprint arXiv:1806.06920,
2018.
Kavosh Asadi and Michael L Littman. An alternative softmax operator for reinforcement learning.
In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 243-
252. JMLR. org, 2017.
Dimitri P Bertsekas. Dynamic Programming and Optimal Control: Volume 2. Athena Scientific,
1995.
Vivek S Borkar. Stochastic approximation: a dynamical systems viewpoint, volume 48. Springer,
2009.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, second
edition, 2006.
Bo Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, and Le Song. Sbeed:
Convergent reinforcement learning with nonlinear function approximation. In International Con-
ference on Machine Learning, pp. 1133-1142, 2018.
Peter Dayan and Geoffrey E Hinton. Using expectation-maximization for reinforcement learning.
Neural Computation, 9(2):271-278, 1997.
Marc Peter Deisenroth, Gerhard Neumann, Jan Peters, et al. A survey on policy search for robotics.
Foundations and TrendsR in Robotics, 2(1-2):1-142, 2013.
Carlos Florensa, Yan Duan, and Pieter Abbeel. Stochastic neural networks for hierarchical rein-
forcement learning. arXiv preprint arXiv:1704.03012, 2017.
Roy Fox, Ari Pakman, and Naftali Tishby. Taming the noise in reinforcement learning via soft
updates. In Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence,
pp. 202-211. AUAI Press, 2016.
Tim Genewein, Felix Leibfried, Jordi Grau-Moya, and Daniel Alexander Braun. Bounded rational-
ity, abstraction, and hierarchical decision-making: An information-theoretic optimality principle.
Frontiers in Robotics and AI, 2:27, 2015.
Jordi Grau-Moya, Felix Leibfried, Tim Genewein, and Daniel A Braun. Planning with information-
processing constraints and model uncertainty in markov decision processes. In Joint Euro-
pean Conference on Machine Learning and Knowledge Discovery in Databases, pp. 475-491.
Springer, 2016.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with
deep energy-based policies. In International Conference on Machine Learning, pp. 1352-1361,
2017.
Tuomas Haarnoja, Kristian Hartikainen, Pieter Abbeel, and Sergey Levine. Latent space policies for
hierarchical reinforcement learning. In International Conference on Machine Learning, 2018a.
Tuomas Haarnoja, Vitchyr Pong, Aurick Zhou, Murtaza Dalal, Pieter Abbeel, and Sergey
Levine. Composable deep reinforcement learning for robotic manipulation. arXiv preprint
arXiv:1803.06773, 2018b.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-
policy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint
arXiv:1801.01290, 2018c.
11
Published as a conference paper at ICLR 2019
Hirotaka Hachiya, Jan Peters, and Masashi Sugiyama. Reward-weighted regression with sample
reuse for direct policy search in reinforcement learning. Neural COmputatiOn, 23(11):2798-2832,
2011.
Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan
Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in
deep reinforcement learning. arXiv preprint arXiv:1710.02298, 2017.
Heinke Hihn, Sebastian Gottwald, and Daniel Alexander Braun. Bounded rational decision-making
with adaptive neural network priors. In IAPR WOrkshOp On Artificial Neural NetwOrks in Pattern
RecOgnitiOn, 2018.
Hilbert J Kappen. Path integrals and symmetry breaking for optimal control theory. JOurnal Of
statistical mechanics: theOry and experiment, 2005(11):P11011, 2005.
Jens Kober and Jan R Peters. Policy search for motor primitives in robotics. In Advances in neural
infOrmatiOn prOcessing systems, pp. 849-856, 2009.
Felix Leibfried and Daniel Alexander Braun. A reward-maximizing spiking neuron as a bounded
rational decision maker. Neural COmputatiOn, 27:1682-1720, 2015.
Felix Leibfried and Daniel Alexander Braun. Bounded rational decision-making in feedforward
neural networks. In COnference On Uncertainty in Artificial Intelligence, 2016.
Felix Leibfried, Jordi Grau-Moya, and Haitham B Ammar. An information-theoretic optimality
principle for deep reinforcement learning. arXiv preprint arXiv:1708.01867, 2018.
Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review.
arXiv preprint arXiv:1805.00909, 2018.
Sergey Levine and Vladlen Koltun. Variational policy search via trajectory optimization. In Ad-
vances in Neural InfOrmatiOn PrOcessing Systems, pp. 207-215, 2013.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529, 2015.
Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between
value and policy based reinforcement learning. In Advances in Neural InfOrmatiOn PrOcessing
Systems, pp. 2775-2785, 2017.
Gergely Neu, Anders Jonsson, and ViCenC Gomez. A unified view of entropy-regularized markov
decision processes. arXiv preprint arXiv:1705.07798, 2017.
Pedro Ortega and Daniel Alexander Braun. Thermodynamics as a theory of decision-making with
information-processing costs. PrOceedings Of the ROyal SOciety A, 469(2153):20120683, 2013.
Zhen Peng, Tim Genewein, Felix Leibfried, and Daniel Alexander Braun. An information-theoretic
on-line update principle for perception-action coupling. In IEEE/RSJ InternatiOnal COnference
On Intelligent RObOts and Systems, 2017.
Jan Peters, Katharina Mulling, and Yasemin Altun. Relative entropy policy search. In AAAI., pp.
1607-1612. Atlanta, 2010.
Martin Puterman. Markov decision processes: Discrete stochastic dynamic programming. 1994.
Konrad Rawlik, Marc Toussaint, and Sethu Vijayakumar. On stochastic optimal control and rein-
forcement learning by approximate inference. In RObOtics: science and systems, volume 13, pp.
3052-3056, 2012.
Jonathan Rubin, Ohad Shamir, and Naftali Tishby. Trading value and information in mdps. In
DecisiOn Making with Imperfect DecisiOn Makers, pp. 57-74. Springer, 2012.
John Schulman, Xi Chen, and Pieter Abbeel. Equivalence between policy gradients and soft q-
learning. arXiv preprint arXiv:1704.06440, 2017.
12
Published as a conference paper at ICLR 2019
Claude Shannon. Coding theorems for a discrete source with a fidelity criterion. Institute of Radio
Engineers, International Convention Record, 7:142-163,1959.
Christopher Sims. Rational inattention and monetary economics. Handbook of Monetary Eco-
nomics, 3, 2011.
Susanne Still and William Bialek. How many clusters? an information-theoretic perspective. Neural
computation, 16(12):2483-2506, 2004.
Susanne Still and Doina Precup. An information-theoretic approach to curiosity-driven reinforce-
ment learning. Theory in Biosciences, 131(3):139-148, 2012.
R Sutton and A Barto. Reinforcement learning. MIT Press, Cambridge, 1998.
Yee Teh, Victor Bapst, Wojciech M Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nicolas
Heess, and Razvan Pascanu. Distral: Robust multitask reinforcement learning. In Advances in
Neural Information Processing Systems, pp. 4496-4506, 2017.
Philip Thomas. Bias in natural actor-critic algorithms. In International Conference on Machine
Learning, pp. 441-448, 2014.
Naftali Tishby and Daniel Polani. Information theory of decisions and actions. In Perception-Action
Cycle. Springer, 2011.
Naftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. The
37th Annual Allerton Conference on Communication, Control, and Computing., 1999.
Emanuel Todorov. General duality between optimal control and estimation. In Decision and Control,
2008. CDC 2008. 47th IEEE Conference on, pp. 4286-4292. IEEE, 2008.
Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. 2016.
Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse
reinforcement learning. In AAAI, volume 8, pp. 1433-1438. Chicago, IL, USA, 2008.
A Appendix
A. 1 CONNECTION TO MUTUAL INFORMATION FOR γ → 1.
The goal of this section is to show that when γ → 1, Equation (6) can be expressed as the following
average-reward formulation (Puterman, 1994) with a constraint on the stationary mutual information
maxEs〜μ∏ V∏(a∣s)r(s,a)	s.t. If(μ∏,π,ρ∏) ≤ C,	(14)
π
a
where μ∏ and ρ∏ are the stationary distributions induced by the policy ∏ over states and actions,
respectively, and thus If (μ∏, ∏, ρ∏) is defined as the stationary mutual-information. Note that for
a fixed stationary distribution over states, this problem coincides exactly with the well-known rate-
distortion problem (Cover & Thomas, 2006).
We start by expressing (6) as a constrained problem
∞
max Eq	γtr(st, at)
π	t=0
∞
s.t. min	γtIf (νt, π, ρ) ≤ K(γ),
ρ t=0
(15)
where K(Y) := ιCγ (although We have set K(∙) as a function of γ, it is without loss of gener-
ality since we can always obtain a desired K(∙) by choosing an appropriate C for a given γ) and
the marginal probability of state st at time t following the Markovian dynamics is written as in
Equation (10).
13
Published as a conference paper at ICLR 2019
A standard result in the MDP literature (Bertsekas, 1995) is that
arg max lim (1 - γ)Eq
π γ→1
∞
γtr(st, at)
t=0
^⇒ arg max Es〜μπ ) π(a∣s)r(s, a)
π
a
which basically says that the optimal policy for the limit γ → 1 of an infinite horizon problem
is equivalent to the average reward formulation. Now it only remains to make explicit a similar
equivalence on the constraint.
We rewrite the constraint by multiplying on both sides by (1 - γ) assuming γ ∈ (0, 1)
∞∞
min£ YtIf(Vt,π,ρ) ≤ K(Y) ^⇒ min(1 - Y) EYtIf (Vt ,π,ρ) ≤ C.
ρ t=0	ρ	t=0
Taking the limit γ → 1 in the last inequality and interchanging the limit and the min operators 5, we
obtain the constraint minρ limγ→1 (1 - γ) Pt∞=0 γtIf(νt, π, ρ) ≤ C. Then, we see the connection
between the last inequality and the constraint on Equation (14) using the following Proposition 2.
Proposition 2. Let μ∏(S) and ρ∏ (a) be the stationary distribution over states and actions under
policy π according to Definitions (1) and (2). Then the stationary mutual information defined as
If (μ∏, π, ρ∏) := min。If (μ∏π, ρ) can also be written as
∞
If (μ∏ ,π,P∏) =min li→ι(1 - Y) EYtIf (Vt ,π,P)
ρ γ→	t=0
(16)
where Vt(s) is defined as in (10).
Proof. Following similar steps as in (Bertsekas, 1995, p.186) we have
If (μ∏ ,π,ρ∏) = min lim
ρ N→∞
1 N-1
N EIf(Vt,π,ρ)
t=0
min lim lim
ρ	N→∞ γ1
PtN=-01 YtIf (Vt, π, ρ)
PN=0 Yt
min lim lim
ρ γ→1 N →∞
∞
min lim (1 - Y)	YtIf(Vt,π,ρ),
ρ γ→1
t=0
where we used Proposition 3 (shown next) in the first equality and where the limits in the third
equality can be interchanged due to the monotone convergence theorem.	□
Proposition 3. Let μ∏(s) and ρ∏ (a) be the stationary distribution over states and actions under
policy π according to Definitions (1) and (2). Then the stationary mutual information defined as
If (μ∏ ,π,ρ∏) can also be written as
1 N-1
If(μ∏,π,ρ∏) =minJim k V y2νt(st)KL(π(∙lst)llρ(∙)),	(17)
ρ N→∞N
where Vt(s) is defined as in (10).
5We can interchange the operators because the l.h.s. of the inequality is finite for any 0 < γ < 1.
14
Published as a conference paper at ICLR 2019
Proof. Let ZP(s) := KL(π(∙∣s)∣∣ρ(∙)) and Iρπ(s) := KL(π(∙∣s)∣∣ρ∏(∙)). Note that both previous
quantities are bounded for all t. Then
	1 N-1 minJim 而 EfVt(S)IP(S) = ρ N→∞ N =minNιim∞N (XXVt(S)IP(S)+ x xVt(S)IP(S)) t=0 s	t=K+1 s 1 N-1 = minjim — E EVt(S)IP(S) P N →∞ N t=K+1 s 1 N-1 = min lim — E fμ不(S)IP(S) P N →∞ N t=K+1 s 1 N-1 = min£ μ∏(S)IP(S)Iim — E 1 P	N →∞ N s	t=K +1 =X μ∏(s)Iμ (S)N→∞ NNK s =£〃n(S)I“ (s) s =If(μπ, π,ρπ ),
where We assumed that Vt(S)= μ∏(S) for all t > K and finite but large enough K.	□
Since in practice we use a discount factor γ / 1, our original problem formulation in (6) can be
seen as an approximation to the problem with stationary mutual-information constraints in (14).
The conclusion of this section is that we have established a clear link between the ELBO with opti-
mizable priors and the average reward formulation with stationary mutual-information constraints.
A.2 Hyperparameters
Here we describe the hyperparameters used for both the Grid World experiments (see Table 2) and
the Atari experiments (see Table 3).
Parameter
γ
αρ
ω
Value
0.99
2∙10-3
0.8
Table 2: Hyperparameters for tabular experiments.
15
Published as a conference paper at ICLR 2019
Parameter	Value
Frame size	-[84, 84]-
Frame skip	4
History frames (in s)	4
Reward clipping	{-1, 0, +1}
Max environment steps	27000
Target update frequency (train. steps)	10000
Training update frequency (env. steps)	4
Batch size	32
Memory capacity	106
γ	0.99
αρ	2∙10-6
αQ	2∙10-5
αβ	3.3 ∙ 10-6
β0	0.01
Table 3: Hyperparameters for Atari experiments.
A.3 Specific plots for individual games
Al IenNoF ramesklρ-v4
1301
Assa UltNoF ramesklρ-v4
AsterixNoFramesklρ-v4
Ooooo
BB,eφud、*上
AsteroIdsNoF ramesklρ-v4
12	3	4	5
BankHeIstNoF ramesklρ-v4
□	12	3	4	5
ChOPPerCoEEandNoFeEeSklP-V4
□	12	3	4	5
1β7
BeamRIde rNoFramesklρ-v4
□	12	3	4	5
1β7
BOXIngNOFraEeSidρ∙v4
crtl,l*∙rtrtrt≡rt* e*ta*β
DemonAHackNoFra mesklρ-v4
GopherNoF r⅝E⅜sklρ∙⅜4
□	12	3	4	5
1β7
KangarooNo Fra mesldρ-v4
□	12	3	4	5
1β7
KrUlINoFeEeSkIpM
□	12	3	4	5
1β7
Kung Fu MasterNoFra mesldρ-v4
□	12	3	4	5
1β7
Rlverra IdNoF ramesklρ-v4
□	12	3	4	5
'一，
0.125
⅝□.100
*0075
I
士0050
k0∞5
0∞0
6
12	3	4	5
ErWmnment Steps le7
□	12	3	4	5
'一，
SPaCemvaderSNoFeEeSkIρ∙v4
_ ___
*-,3OM
_ ____
MfImq3£、*上
Sta TGunnerNoFra mesldρ-v4
_ _ __
*-,3OM
□	12	3	4	5
ErWmnmentStepS le7
Ol 2345	Ol 2345
EWlronmentSteW le7	ErWmnmentSaW 1"
□	12	3	4	5
Road Run nerNoFramesklρ-v4
□	12	3	4	5
ErWmnmerC Steps le7
□
□

_ ___
BB,eφud、*上
_ ___ ____
SeUI=。Zf Wd

Figure 4:	Prior Evolution for all games. We can see that MIRL’s prior has fully converged for some
games whereas for other games it is still about to converge.
16
Published as a conference paper at ICLR 2019
AIenNoFramesM p-v4
40∞
AssautNoRε(nes∣4H∕4
----DQN
SQL
----MIRL
1	2	3	4 S
EnvtoniTieit SKos	le7
BariCHeIStNoI=TameSlφ∙v4
--DQN
—SQL
——MIRL
EnvkonnemSteDS
Uq|；|
1QOOQ
1	2	3	4	5
Envt-OniTientSieps	le7
DeTlCrnAitackNoFre(Tledφ∙8
DQN
—SQL
--MIRL
S
ie7
Beam RtderNof= ram esk⅞H∕4
Envkonnem Steps
---DQN
SQL
---MIRL
S
ie7
GoftietNo I=Tanedφ"M
-DQN
SQL
——MIRL
2S0<B
20000
15000
100∞
5000
O
O
1	2	3	4	5
EnvtoniTiefitSteps le7
Kung Rj MasterNoRan esk⅞H∕4
EnAonnemt Steps
DQN
SQL
MIRL
Rivenald NoFrameSnH/4
S
ie7
ɪɔj'j'j
IQQOO
---DQN
SQL
——MIRL
1	2	3	4	5
EnvtoniTiefitSteps le7
DQN
SQL
---MIRL
Envt-CHiinert Steps
S
ie7
Spacelnvad eιsNol=ranes∣4H∕4
---DQN
SQL
——MIRL
12	3	4
EnvfroiinertSteps
60000
500∞
40000
300∞
20000
IOOOO
O
O
StadJinerNof= ram esk⅞H∕4
DQN
SQL
---MIRL
1	2
ErWt`onTIert
4
S
ie7
DQN
SQL
——MIRL
1	2	3	4	5
Envton mert Steps	le7
pJ9M9H PJ9M9tt
<λK'J'J
40000
30000
20000
IOOOO
0
2S0∞
20000
15000
IOOOO
SOOO
。
0	1	2	3	4	5
Envtonmem Steps	le7
0	1	2	3	4	5
Envtonmem Steps	le7
Ctl /(>€TCanmandNOI=Tane dφ"M



1
1
1
2
2
2
3
4

3
4
3
4
Figure 5:	Scores for all games on the evaluation snapshots.
A.4 Ablation study
In the ablation study summarized in Figure 3, we show how marginal exploration affects SQL. In
Figure 6, we show the same plot for individual games. We clearly see that the marginal exploration
improves performance, but is not the defining factor for all the improvements obtained by MIRL.
BtnkHelstNoFrenesHt-W-
Ctι<φp<rCc∙nmandNol=rε(ne9φ-v4
AssaunNof= rameslφ-v4
20000
150∞
10∞
800
50∞
40∞
Uqi;i
1QOOQ
——SQLm
—SQLJU
--MIRL
12	3	4
EnvfrcninertSteps
RIVenaldNoI=TameSM>∙v4
pJ9M9H PJBM9tt
IOOOO
5000
0
SQL_m
J	——SQL.U
--- MIRL
0	1	2	3	4	5
Envtonnent Steps le7
-SQL,m
—SQL.U
--MIRL
Envronnemi
bezSQLju
-MIRL
4 S
ie7
一 SQLm
——SQLju
---MIRL
1	2	3	4	5
Envtoninefit Steps	le7


Figure 6:	Comparison between standard SQL (SQL_u), SQL with marginal exploration (SQL_m),
and MIRL.
A.5	Evolution of the Lagrange multiplier
In Figure 7, we show the evolution of β over time (environment steps) for the MIRL agent. As we
can see, the β-values usually start at a high value (not shown for visual reasons) and typically go
down and stabilize at some value. At first sight, this might be seen as a negative side effect since
17
Published as a conference paper at ICLR 2019
AssauItNoFra mes H p-v4
,0.00 0.25 0.50 0.75 1.00 1.25
TfKnn HeraHnne 1α7
AsterixNoFra mes Id p-v4	ASterOidSNoFrameSldP-V4
Figure 7: Beta evolution over time.
lower β values imply a stronger constraint. However, we note that the constraint is highly dependent
on the scale of the reward (or its sparsity), and therefore, the β value is not meaningful without a
proper specification of this reward scaling.
Consequently, given that β-values are not meaningful here, we propose to instead show the mul-
tiplication of β times the current maximum Q-value estimates denoted as β × max Q. Note that
βQ(s, a) appears on the exponential term of the policy, i.e. π(a∣s) = Zρ(a) exp(βQ(s, a)), and
therefore, is the term that shapes the deviation from the action-prior distribution. Additionally, the
Q-values serve us as proper scaling for each game and account also for the learning of the agent. In
particular, while the agent is learning and increasing its reward acquisition, the Q-values are going
to be higher, thus, effectively needing a smaller β to shape the policy probabilities.
On Figure 8, we show the evolution of the term β × max Q for all the games. As we can see for the
majority of games, β × max Q increases over time or has high value. This is important since a high
value denotes that the policy is highly affected by this term.
18
Published as a conference paper at ICLR 2019
Al IenNoFrames lα p-v4
0	1	2	3	4	5
FnuimnmAnl- Ctan	1 a7
HanIcHeistNoFra meslcip-v4
0	1	2	3	4	5
FnuimnmAnl- Ctan	1 a7
DerTtonAttaclcNoFramesIcip-VA
— ,
a∙euJ κ°
BeamRider⅜to FemeSldρ~⅜⅛
QngarooNoFramesId ρ-v4
0	1	2	3	4	5
FnuimnmAnl- Ctan	1 a7
KungFuMasterNoFramesIap-VA
0	1	2	3	4	5
FnuimnmAnl- Ctan	1 a7
SpacelnvadersNoFramesIcip-VA
0	1	2	3	4	5
。加 C	1 a7
GOPherNoFrameSki>v4
0	1	2	3	4	5
Fnuimnmanl- C¼an	1ZT
StarGu nrɪerNoFrameSki>v4
0	1	2	3	4	5
FnuimnmAni- C¼an	1ZT
ROadRUnnerNoFrameSki[>∙v4
ChoPPerCOmmarKiNoFmmeSki[>∙v4

Figure 8: Evolution of β × maxQ over time while training. Specifically, for an environment step i,
we compute the βi maxa Qθi (si, a), where βi is the current β-value, Qθi the current approximation
of Q and si is the state at the step i.
19