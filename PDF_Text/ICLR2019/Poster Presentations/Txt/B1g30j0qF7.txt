Published as a conference paper at ICLR 2019
Bayesian Deep Convolutional Networks with
Many Channels are Gaussian Processes
Roman NoVak *, LeChao Xiao * *, Jaehoon Lee ^ * Yasaman Bahri ^ Greg Yang。，
Jiri Hron, Daniel A. Abolafia, Jeffrey Pennington, Jascha Sohl-Dickstein
Google Brain, ◦Microsoft Research AI, Department of Engineering, University of Cambridge
{romann, xlc, jaehlee, yasamanb, ◦gregyang@microsoft.com,
jh2084@cam.ac.uk, danabo, jpennin, jaschasd}@google.com
Ab stract
There is a previously identified equivalence between wide fully connected neural
networks (FCNs) and Gaussian processes (GPs). This equivalence enables, for
instance, test set predictions that would have resulted from a fully Bayesian, in-
finitely wide trained FCN to be computed without ever instantiating the FCN, but
by instead evaluating the corresponding GP. In this work, we derive an analogous
equivalence for multi-layer convolutional neural networks (CNNs) both with and
without pooling layers, and achieve state of the art results on CIFAR10 for GPs
without trainable kernels. We also introduce a Monte Carlo method to estimate
the GP corresponding to a given neural network architecture, even in cases where
the analytic form has too many terms to be computationally feasible.
Surprisingly, in the absence of pooling layers, the GPs corresponding to CNNs
with and without weight sharing are identical. As a consequence, translation
equivariance, beneficial in finite channel CNNs trained with stochastic gradient
descent (SGD), is guaranteed to play no role in the Bayesian treatment of the in-
finite channel limit - a qualitative difference between the two regimes that is not
present in the FCN case. We confirm experimentally, that while in some scenarios
the performance of SGD-trained finite CNNs approaches that of the correspond-
ing GPs as the channel count increases, with careful tuning SGD-trained CNNs
can significantly outperform their corresponding GPs, suggesting advantages from
SGD training compared to fully Bayesian parameter estimation.
1	Introduction
Neural networks (NNs) demonstrate remarkable performance (He et al., 2016; Oord et al., 2016;
Silver et al., 2017; Vaswani et al., 2017), but are still only poorly understood from a theoretical
perspective (Goodfellow et al., 2015; Choromanska et al., 2015; Pascanu et al., 2014; Zhang et al.,
2017). NN performance is often motivated in terms of model architectures, initializations, and train-
ing procedures together specifying biases, constraints, or implicit priors over the class of functions
learned by a network. This induced structure in learned functions is believed to be well matched to
structure inherent in many practical machine learning tasks, and in many real-world datasets. For
instance, properties of NNs which are believed to make them well suited to modeling the world
include: hierarchy and compositionality (Lin et al., 2017; Poggio et al., 2017), Markovian dynamics
(Tino et al., 2004; 2007), and equivariances in time and space for RNNs (Werbos,1988) and CNNs
(Fukushima & Miyake, 1982; Rumelhart et al., 1985) respectively.
The recent discovery of an equivalence between deep neural networks and GPs (Lee et al., 2018;
Matthews et al., 2018b) allow us to express an analytic form for the prior over functions encoded by
deep NN architectures and initializations. This transforms an implicit prior over functions into an
explicit prior, which can be analytically interrogated and reasoned about.* 1
Previous work studying these Neural Network-equivalent Gaussian Processes (NN-GPs) has estab-
lished the correspondence only for fully connected networks (FCNs). Additionally, previous work
has not used analysis of NN-GPs to gain specific insights into the equivalent NNs.
*Google AI Residents (g.co/airesidency). * J Equal contribution.
1While there is broad literature on empirical interpretation of finite CNNs (Zeiler & Fergus, 2014; Simonyan
et al., 2014; Long et al., 2014; Olah et al., 2017), it is commonly only applicable to fully trained networks.
1
Published as a conference paper at ICLR 2019
Figure 1: Disentangling the role of network topology, equivariance, and invariance on test
performance, for SGD-trained and infinitely wide Bayesian networks (NN-GPs). Accuracy (%)
on CIFAR10 of different models of the same depth, nonlinearity, and weight and bias variances. (a)
Fully connected models - FCN (fully connected network) and FCN-GP (infinitely wide Bayesian
FCN) underperform compared to (b) LCNs (locally connected network, a CNN without weight
sharing) and CNN-GP (infinitely wide Bayesian CNN), which have a hierarchical local topology
beneficial for image recognition. As derived in §5.1: (i) weight sharing has no effect in the Bayesian
treatment of an infinite width CNN (CNN-GP performs similarly to an LCN), and (ii) pooling has no
effect on generalization of an LCN model (LCN and LCN with pooling perform nearly identically).
(C) Local connectivity combined with equivariance (CNN) is enabled by weight sharing in an
SGD-trained finite model, allowing for a significant improvement. ](d) Finally, invariance enabled
by weight sharing and pooling allows for the best performance. Due to computational limitations,
the performance of CNN-GPs with pooling - which also possess the property of invariance - remains
an open question. Values are reported for 8-layer ReLU models. See §G.6 for experimental details,
Figure 7, and Table 1 for more model comparisons.
In the present work, we extend the equivalence between NNs and NN-GPs to deep Convolutional
Neural Networks (CNNs), both with and without pooling. CNNs are a particularly interesting ar-
chitecture for study, since they are frequently held forth as a success of motivating NN design based
on invariances and equivariances of the physical world (Cohen & Welling, 2016) - specifically, de-
signing a NN to respect translation equivariance (Fukushima & Miyake, 1982; Rumelhart et al.,
1985). As we will see in this work, absent pooling, this quality of equivariance has no impact in the
Bayesian treatment of the infinite channel (number of convolutional filters) limit (Figure 1).
The specific novel contributions of the present work are:
1.	We show analytically that CNNs with many channels, trained in a fully Bayesian fashion,
correspond to an NN-GP (§2, §3). We show this for CNNs both with and without pooling,
with arbitrary convolutional striding, and with both same and valid padding. We prove
convergence as the number of channels in hidden layers approach infinity simultaneously
(i.e. min n1, . . . , nL → ∞, see §E.4 for details), extending the result of Matthews et al.
(2018a) under mild conditions on the nonlinearity derivative. Our results also provide a
rigorous proof of the assumption made in Xiao et al. (2018) that pre-activations in hidden
layers are i.i.d. Gaussian.
2.	We show that in the absence of pooling, the NN-GP for a CNN and a Locally Connected
Network (LCN) are identical (Figure 1, §5.1). An LCN has the same local connectivity
pattern as a CNN, but without weight sharing or translation equivariance.
3.	We experimentally compare trained CNNs and LCNs and find that under certain conditions
both perform similarly to the respective NN-GP (Figure 6, b, c). Moreover, both archi-
tectures tend to perform better with increased channel count, suggesting that similarly to
FCNs (Neyshabur et al., 2015; Novak et al., 2018) CNNs benefit from overparameterization
(Figure 6, a, b), corroborating a similar trend observed in Canziani et al. (2016, Figure 2).
However, we also show that careful tuning of hyperparameters allows finite CNNs trained
with SGD to outperform their corresponding NN-GPs by a significant margin. We ex-
2
Published as a conference paper at ICLR 2019
perimentally disentangle and quantify the contributions stemming from local connectivity,
equivariance, and invariance in a convolutional model in one such setting (Figure 1).
4.	We introduce a Monte Carlo method to compute NN-GP kernels for situations (such as
CNNs with pooling) where evaluating the NN-GP is otherwise computationally infeasible
(§4).
We stress that we do not evaluate finite width Bayesian networks nor do we make any claims about
their performance relative to the infinite width GP studied here or finite width SGD-trained networks.
While this is an interesting subject to pursue (see Matthews et al. (2018a); Neal (1994)), it is outside
of the scope of this paper.
1.1 Related work
In early work on neural network priors, Neal (1994) demonstrated that, in a fully connected network
with a single hidden layer, certain natural priors over network parameters give rise to a Gaussian
process prior over functions when the number of hidden units is taken to be infinite. Follow-up
work extended the conditions under which this correspondence applied (Williams, 1997; Le Roux
& Bengio, 2007; Hazan & Jaakkola, 2015). An exactly analogous correspondence for infinite width,
finite depth deep fully connected networks was developed recently in Lee et al. (2018); Matthews
et al. (2018b), with Matthews et al. (2018a) extending the convergence guarantees from ReLU to
any linearly bounded nonlinearities and monotonic width growth rates. In this work we further relax
the conditions to absolutely continuous nonlinearities with exponentially bounded derivative and
any width growth rates.
The line of work examining signal propagation in random deep networks (Poole et al., 2016; Schoen-
holz et al., 2017; Yang & Schoenholz, 2017; 2018; Hanin & Rolnick, 2018; Chen et al., 2018; Yang
et al., 2018) is related to the construction of the GPs we consider. They apply a mean field ap-
proximation in which the pre-activation signal is replaced with a Gaussian, and the derivation of the
covariance function with depth is the same as for the kernel function of a corresponding GP. Re-
cently, Xiao et al. (2017b; 2018) extended this to convolutional architectures without pooling. Xiao
et al. (2018) also analyzed properties of the convolutional kernel at large depths to construct a phase
diagram which will be relevant to NN-GP performance, as discussed in §B.
Compositional kernels coming from wide convolutional and fully connected layers also appeared
outside of the GP context. Cho & Saul (2009) derived closed-form compositional kernels for recti-
fied polynomial activations (including ReLU). Daniely et al. (2016) proved approximation guaran-
tees between a network and its corresponding kernel, and show that empirical kernels will converge
as the number of channels increases.
There is a line of work considering stacking of GPs, such as deep GPs (Lawrence & Moore, 2007;
Damianou & Lawrence, 2013). These no longer correspond to GPs, though they can describe a
rich class of probabilistic models beyond GPs. Alternatively, deep kernel learning (Wilson et al.,
2016b;a; Bradshaw et al., 2017) utilizes GPs with base kernels which take in features produced by a
deep neural network (often a CNN), and train the resulting model end-to-end. Finally, van der Wilk
et al. (2017) incorporates convolutional structure into GP kernels, with follow-up work stacking
multiple such GPs (Kumar et al., 2018; Blomqvist et al., 2018) to produce a deep convolutional GP
(which is no longer a GP). Our work differs from all of these in that our GP corresponds exactly
to a fully Bayesian CNN in the infinite channel limit, when all layers are taken to be of infinite
size. We remark that while alternative models, such as deep GPs, do include infinite-sized layers
in their construction, they do not treat all layers in this way - for instance, through insertion of
bottleneck layers which are kept finite. While it remains to be seen exactly which limit is applicable
for understanding realistic CNN architectures in practice, the limit we consider is natural for a large
class of CNNs, namely those for which all layers sizes are large and rather comparable in size.
Deep GPs, on the other hand, correspond to a potentially richer class of models, but are difficult to
analytically characterize and suffer from higher inference cost.
Borovykh (2018) analyzes the convergence of CNN outputs at different spatial locations (or different
timepoints for a temporal CNN) to a GP for a single input example. Thus, while they also consider a
GP limit (and perform an experiment comparing posterior GP predictions to an SGD-trained CNN),
3
Published as a conference paper at ICLR 2019
they do not address the dependence of network outputs on multiple input examples, and thus their
model is unable to generate predictions on a test set consisting of new input examples.
In concurrent work, Garriga-Alonso et al. (2018) derive an NN-GP kernel equivalent to one of the
kernels considered in our work. In addition to explicitly specifying kernels corresponding to pooling
and vectorizing, we also compare the NN-GP performance to finite width SGD-trained CNNs and
analyze the differences between the two models.
2 Many-channel Bayesian CNNs are Gaussian processes
2.1 Preliminaries
General setup. For simplicity of presentation we consider 1D convolutional networks with
circularly-padded activations (identically to Xiao et al. (2018)). Unless specified otherwise, no
pooling anywhere in the network is used. If a model (NN or GP) is mentioned explicitly as “with
pooling”, it always corresponds to a single global average pooling layer at the top. Our analysis
is straightforward to extend to higher dimensions, using zero (same) or no (valid) padding, strided
convolutions, and pooling in intermediary layers (§C). We consider a series of L + 1 convolutional
layers, l = 0, . . . , L.
Random weights and biases. The parameters of the network are the convolutional filters and biases,
ωilj,β and bli, respectively, with outgoing (incoming) channel index i (j) and filter relative spatial
location β ∈ [±k] ≡ {-k, . . . , 0, . . . , k}.2 Assume a Gaussian prior on both the filter weights and
biases,
2
ωj,β 〜N (0,vβ 常
bi 〜N (0,σ2).
(1)
The weight and bias variances are σω2 , σb2, respectively. nl is the number of channels (filters) in layer
l, 2k + 1 is the filter size, and vβ is the fraction of the receptive field variance at location β (with
Pβ vβ = 1). In experiments we utilize uniform vβ = 1/(2k + 1), but nonuniform vβ 6= 1/(2k + 1)
should enable kernel properties that are better suited for ultra-deep networks, as in Xiao et al. (2018).
Inputs, pre-activations, and activations. Let X denote a set of input images (training set, valida-
tion set, or both). The network has activations yl (X) and pre-activations zl (X) for each input image
X ∈ X ⊂ Rn0d, with input channel count n0 ∈ N, number of pixels d ∈ N, where
l	Xi,α	l = 0	l	n k l l	l
yi,α(x) ≡	φ (zl-1(χ))	l> 0 ,	zi,α(x) ≡	ΣL ωj,β yja+β(X)+ bi-	(2)
i,α	j =1 β=-k
We emphasize the dependence of yil,α (X) and zil,α (X) on the input X. φ : R → R is a nonlinearity
(with elementwise application to higher-dimensional inputs). Similarly to Xiao et al. (2018), yl
is assumed to be circularly-padded and the spatial size d hence remains constant throughout the
network in the main text (a condition relaxed in §C and Remark E.3). See Figures 2 and 3 for a
visual depiction of our notation.
Activation covariance. A recurring quantity in this work will be the empirical uncentered covari-
ance matrix Kl of the activations yl , defined as
l	1 nl	l l
[K ] α,α0 (X,x ) ≡ n	yi,α (X)yi,α0 (X ).
(3)
i=1
Kl is a random variable indexed by two inputs X, X0 and two spatial locations α, α0 (the dependence
on layer widths n1 , . . . , nl , as well as weights and biases, is implied and by default not stated
explicitly). K0, the empirical uncentered covariance of inputs, is deterministic.
Shapes and indexing. Whenever an index is omitted, the variable is assumed to contain all possible
entries along the respective dimension. For example, y0 is a vector of size |X | n0d, Kl αα0 is a
matrix of shape |X | × |X |, and zjl is a vector of size |X | d.	,
2We will use Roman letters to index channels and Greek letters for spatial location. We use letters i, j, i0 , j0,
etc to denote channel indices, α, α0, etc to denote spatial indices and β, β0, etc for filter indices.
4
Published as a conference paper at ICLR 2019
y 0(χ) = X
Figure 2: A sample 2D CNN classifier annotated according to notation in §2.1, §3. The network
transforms n0 X d0 = 3 X 8 X 8-dimensional inputs y0(x) = X ∈ X into n3 = 10-dimensional logits
W2 (x). Model has two convolutional layers with k = (3,3)-ShaPed filters, nonlinearity φ, and a fully
connected layer at the top (y2(χ) → N(x), §3.1). Hidden (Pre-)activations have n1 = n2 = 12
filters. As min n1, n2 → ∞, the prior of this CNN will approach that of a GP indexed by inputs
x and target class indices from 1 to nW3 = 10. The covariance of such GP can be computed as
6×⅛ pa=。. he。/)2 (K 0 )]α,α+σ
0 In, where the sum is over the {1,..., 4}2 hypercube
(see §2.2, §3.1, §C). Presented is a CNN with stride 1 and no (valid) padding, i.e. the spatial shape
of the input shrinks as it propagates through it d0 = (8, 8) → d1 = (6, 6) → d2 = (4, 4) . Note
that for notational simplicity 1D CNN and circular padding with d0 = d1 = d2 = d is assumed
in the text, yet our formalism easily extends to the model displayed (§C). Further, while displayed
(pre-)activations have 3D shapes, in the text we treat them as 1D vectors (§2.1).
Our work concerns proving that the top-layer pre-activations zL converge in distribution to
an |X| nL+1d-variate normal random vector with a particular covariance matrix of shape
|X| nL+1d X |X| nL+1d as min n1, . . . , nL → ∞. We emphasize that only the channels in
hidden layers are taken to infinity, and nL+1
, the number of channels in the top-layer pre-activations
zL, remains fixed. For convergence proofs, we always consider zl, yl, as well as any of their indexed
subsets like zjl, yil,α to be 1D vector random variables, while Kl, as well as any of its indexed subsets
(when applicable, e.g. Kl α,α0, Kl (x, x0)) to be 2D matrix random variables.
2.2 Correspondence between Gaussian processes and Bayesian deep CNNs with
INFINITELY MANY CHANNELS
We next consider the prior over outputs zL computed by a CNN in the limit of infinitely many
channels in the hidden (excluding input and output) layers, min n1, . . . , nL → ∞, and derive
its equivalence to a GP with a compositional kernel. This section outlines an argument showing
that zL is normally distributed conditioned on previous layer activation covariance KL, which itself
becomes deterministic in the infinite limit. This allows to conclude convergence in distribution of
the outputs to a Gaussian with the respective deterministic covariance limit. This section omits many
technical details elaborated in §E.4.
2.2. 1 A single convolutional layer is a GP conditioned on the uncentered
covariance matrix of the previous layer’s activations
As can be seen in Equation 2, the pre-activations zl are a linear transformation of the multivariate
Gaussian ωl, bl , specified by the previous layer’s activations yl. A linear transformation ofa mul-
tivariate Gaussian is itself a Gaussian with a covariance matrix that can be derived straightforwardly.
Specifically,
(Zl ∣yl)〜N (0, A (Kl) 0 Inl+1),	(4)
where Inl+1 is an nl+1 X nl+1 identity matrix, and A Kl is the covariance of the pre-activations zil
and is derived in Xiao et al. (2018). Precisely, A : PSD|X |d → PSD|X |d is an affine transformation
(a cross-correlation operator followed by a shifting operator) on the space of positive semi-definite
|X | d X |X | d matrices defined as follows:
[A (K)]α,α0 (x, x0) ≡ σb2 +σω2 Xvβ [K]α+β,α0+β (x, x0) .	(5)
β
5
Published as a conference paper at ICLR 2019
A preserves positive semi-definiteness due to Equation 4. Notice that the covariance matrix in
nl+1
Equation 4 is block diagonal due to the fact that separate channels zil i=1 are i.i.d. conditioned on
yl, due to i.i.d. weights and biases {ω4,苗}η=1.
We further remark that per Equation 4 the normal distribution of zl |yl only depends on
the random variable zl|Kl has the same distribution by the law of total expectation:
(zl|Kl)〜N (0, A (Kl)③ I"+】).
Kl, hence
(6)
2.2.2 Activation covariance matrix becomes deterministic with increasing
CHANNEL COUNT
It follows from Equation 6 that the summands in Equation 3 are i.i.d. conditioned on fixed Kl-1.
Subject to weak restrictions on the nonlinearity φ, we can apply the weak law of large numbers and
conclude that the covariance matrix Kl becomes deterministic in the infinite channel limit in layer l
(note that pre-activations zl remain stochastic). Precisely,
3
∀Kl-1 ∈ PSD∣χ∣d	(Kl|Kl-1) -P-→ (COA)(Kl-1)	(inprobability),	(7)
nl→∞
where C is defined for any |X | d × |X | d PSD matrix K as
[C (K)]α,α0 (x, x0) ≡ Wu〜N(0,K) [φ (Ua(X)) Φ "，(X0))].	⑻
The decoupling of the kernel “propagation” into C and A is highly convenient since A is a simple
affine transformation of the kernel (see Equation 5), and C is a well-studied map in literature (see
§G.4), and for nonlinearities such as ReLU (Nair & Hinton, 2010) and the error function (erf) C can
be computed in closed form as derived in Cho & Saul (2009) and Williams (1997) respectively. We
refer the reader to Xiao et al. (2018, Lemma A.1) for complete derivation of the limiting value in
Equation 7.
A less obvious result is that, under slightly stronger assumptions on φ, the top-layer activation co-
variance KL becomes unconditionally (dependence on observed deterministic inputs y0 is implied)
deterministic as channels in all hidden layers grow to infinity simultaneously:
KL -「-—-→ K∞ ≡ (Co A)L(K0),	(9)
min{n1,...,nL}→∞
i.e. K∞L is (C ◦ A) applied L times to K0, the deterministic input covariance. We prove this in §E.4
(Theorem E.5). See Figure 3 for a depiction of the correspondence between neural networks and
their infinite width limit covariances K∞l .
2.2.3 A conditionally normal random variable becomes normal if its
COVARIANCE BECOMES DETERMINISTIC
§2.2.1 established that zL|KL is Gaussian, and §2.2.2 established that its covariance matrix
A (KL)③ InL+ι converges in probability to a deterministic A (K∞) 0 I"+1in the infinite channel
limit (since KL → K∞, and A(∙) 0 InL+ι : RlXld×lXld → RnL+1|X|dxnL+1|X|d is continuous).
As we establish in §E.4 (Theorem E.6), this is sufficient to conclude with the following result.
Result. If φ : R → R is absolutely continuous and has an exponentially bounded derivative, i.e.
∃ a, b ∈ R : ∣φ0 (x)| ≤ a exp(bx) a.e. (almost everywhere), then the following convergence in
distribution holds:
(zL∣y0)	- 1 D L—-→N(0, A(K∞) 0InL+1 ) .	(10)
min{n1 ,...,nL}→∞
For more intuition behind Equation 10 and an informal proof please consult §E.3.
3The weak law of large numbers allows convergence in probability of individual entries of Kl . However,
due to the finite dimensionality of Kl, joint convergence in probability follows.
6
Published as a conference paper at ICLR 2019
ZON- dqN0u-
10 classes
per input
Λ Λ ∙	■	■
4x4 input covariance
z0
y
y2
SUo4E>4OE ①。UEμE>oo
(ɑɛZzo dqNN。
X = y0
convolution
∠sU1a J⅛1	Z
-CT ∣∕fl
W'』
matrix-multiply
φ u∣^f
10 classes
per input
4 inputs, 3 features,
10 pixels
SUo4E>4OE ①。UEμE>oo
4
Φ
Figure 3: Visualization of sample finite neural networks and their infinitely Wide Bayesian
counterparts (NN-GPs). Presented are networks With nonlinearity φ, L = 2 hidden layers that
regress n3 = 10-dimensional outputs Z (x) for each of the 4 (1, 2, 3, 4) inputs X from the dataset
X. A hierarchical computation performed by a NN corresponds to a hierarchical transformation
of the respective covariance matrix (resulting in Kvec 0 I10). Top two rows: a fully connected
network (FCN, above) and the respective FCN-GP (below). Bottom two rows: a 1D CNN with
no (Valid)Padding, andthe respective CNN-GP∙ The analogy between the two models (NN and
GP) is qualitatively similar to the case of FCN, modulo additional spatial dimensions in the internal
activations and the respective additional entries in intermediary covariance matrices. Note that in
general d = 10 pixels would induce a 10 X 10-fold increase in the number of covariance entries (in
K0, K∞∞, ... ) compared to the respective FCN, yet without pooling only a small fraction of them
(displayed) nee ds be computed to obtain the top-layer GP covariance Kvec 0 I10 (see §3.1).
3 Transforming a GP over spatial locations into a GP over
CLASSES
In §2.2 we showed that in the infinite channel limit a deep CNN is a GP indexed by input samples,
output spatial locations, and output channel indices. Further, its covariance matrix KvL can be
computed in closed form. Here we show that transformations to obtain class predictions that are
common in CNN classifiers can be represented as either vectorization or projection (as long as we
treat classification as regression (see §G), identically to Lee et al. (2018)). Both of these operations
preserve the GP equivalence and allow the computation of the covariance matrix of the respective
GP (now indexed by input samples and target classes) as a simple transformation of KL.
In the following we will denote nL+2 as the number of target classes, zL+1 (x) ∈ Rn +2 as the
outputs of the CNN-based classifier, and ω L+1, bL+1 as the last (number L + 1) layer weights and
biases. For each class i we will denote the empirical uncentered covariance of the outputs as
T
Ki ≡ (ZL+1) (ZL+1),	(II)
a random variable of shape |X| × |X|. We will show that in the scenarios considered below in
L+1 l 0；
§ ).1 and § .2, the outputs (z，+ |y0) will converge in distribution to a multivariate Gaussian (i.i.d.
7
Published as a conference paper at ICLR 2019
for every class i) as min{n1,..., nL+1} → ∞ with covariance denoted as K∞ ∈ RlXl×lXl, or,
jointly, (zL+1 |y0) → N (0, K∞ 乳 InL+2). As in §2.2, we postpone the formal derivation until §E.4
(Theorem E.6).
3.1 Vectorization
One common readout strategy is to vectorize (flatten) the output of the last convolutional layer
z L (x) ∈ RnL+1d into a vector4 vec zL (x) ∈ RnL+1d and stack a fully connected layer on top:
nL+1d
zL+1 (X) ≡ X ωj+1φ (Vec 卜L (X)])j + bL+1,	(12)
j=1
where the weights ωL+1 ∈ RnL+2xnL+1d and biases bL+1 ∈ RnL+2 are i.i.d. Gaussian, ωj+1 〜
N (0,σω/(nL+Id)), bL+1 〜N (0,σ2).
It is easy to verify that (WL+1 |KL+1) is a multivariate Gaussian with covariance:
2
E [κvec∣KL+1] =E [a+1)a+1) ∣KL+1] = σ X [KL+1]α,α + σ.	(13)
α
The argument of §2.2 can then be extended to conclude that in the limit of infinite width zWiL+1 |y0
converges in distribution to a multivariate Gaussian (i.i.d. for each class i) with covariance
2
K∞c = σ X[K∞+1]α,α + σ2∙	(14)
α
A sample 2D CNN using this readout strategy is depicted in Figure 2, and a sample correspondence
between a FCN, FCN-GP, CNN, and CNN-GP is depicted in Figure 3.
Note that as observed in Xiao et al. (2018), to compute the summands K∞L+1 αα (X, X0) in Equa-
tion 13, one needs only the corresponding terms K∞L α,α (X, X0). Consequently, we only need to
compute [K∞l α,α (X, X0) : X, X0 ∈ X, α ∈ {1 . . . d} l=0 L and the memory cost is O |X |2 d
(or O (d) per covariance entry in an iterative or distributed setting). Note that this approach ignores
pixel-pixel covariances and produces a GP corresponding to a locally connected network (see §5.1).
3.2 Projection
Another readout approach is a projection to collapse the spatial dimensions. Let h ∈ Rd be a
deterministic projection vector, ωj+1 〜N(0, σ22/nL+v), and bL+1 be the same as in §3.1.
Define the output to be
nL+1	d
zL+1 (x) ≡ X ωj+1 ( X φ (ZL (X)) j,α hɑ I + bL+1, leading to (analogously to §3.1) (15)
j =1	α=1
Kh∞ ≡ σω2 X hαhα0 [K∞L+1α,α0 +σb2.	(16)
α,α0
Examples of this approach include
1.	Global average pooling: take h = d 1d. Then
(17)
4 Note that since per our notation described in §2.1 zL (x) is already considered a 1D vector, here this
operation simply amounts to re-indexing zL (x) with one channel index j instead of two (channel i and pixel
8
Published as a conference paper at ICLR 2019
Xɔmnɔɔv
MC-CNN-GP with pooling
CNN-GP
CNN-GP (center pixel)
CNN-GP (without padding)
FCN-GP
Figure 4: Different dimensionality collapsing strategies described in §3. Validation accuracy of
an MC-CNN-GP with pooling (§3.2.1) is consistently better than other models due to translation
invariance of the kernel. CNN-GP with zero padding (§3.1) outperforms an analogous CNN-
GP without padding as depth increases. At depth 15 the spatial dimension of the output without
padding is reduced to 1 × 1, making the CNN-GP without padding equivalent to the center pixel
selection strategy (§3.2.2) - which also performs worse than the CNN-GP (We conjecture, due
to overfitting to centrally-located features) but approaches the latter (right) in the limit of large
depth, as information becomes more uniformly spatially distributed (Xiao et al., 2018). CNN-GPs
generally outperform FCN-GP, presumably due to the local connectivity prior, but can fail to capture
nonlinear interactions between spatially-distant pixels at shallow depths (left). Values are reported
on a 2K/4K train/validation subset of CIFAR10. See §G.3 for experimental details.
This approach corresponds to applying global average pooling right after the last convolu-
tional layer.5 This approach takes all pixel-pixel covariances into consideration and makes
the kernel translation invariant. However, it requires O |X |2 d2 memory to compute the
sample-sample covariance of the GP (or O d2 per covariance entry in an iterative or dis-
tributed setting). It is impractical to use this method to analytically evaluate the GP, and we
propose to use a Monte Carlo approach (see §4).
2.	Subsampling one particular pixel: take h = eα,
K∞eα ≡ σω2 K∞L+1α,α +σb2.	(18)
This approach makes use of only one pixel-pixel covariance, and requires the same amount
of memory as vectorization (§3.1) to compute.
We compare the performance of presented strategies in Figure 4. Note that all described strategies
admit stacking additional FC layers on top while retaining the GP equivalence, using a derivation
analogous to §2.2 (Lee et al., 2018; Matthews et al., 2018b).
4 Monte Carlo evaluation of intractable GP kernels
We introduce a Monte Carlo estimation method for NN-GP kernels which are computationally im-
practical to compute analytically, or for which we do not know the analytic form. Similar in spirit
to traditional random feature methods (Rahimi & Recht, 2007), the core idea is to instantiate many
random finite width networks and use the empirical uncentered covariances of activations to estimate
the Monte Carlo-GP (MC-GP) kernel,
1 Mn
[Kn,M] α,αo (X, XO) ≡ Mn XX yCα (x; θm) ylcθ0' (X0； θm)	(19)
m=1 c=1
5 Spatially local average pooling in intermediary layers can be constructed in a similar fashion (§C). We
focus on global average pooling in this work to more effectively isolate the effects of pooling from other
aspects of the model like local connectivity or equivariance.
9
Published as a conference paper at ICLR 2019
W (-Biqo*)--
PG-NNC-CM
∙4KUUrnUUV
W (-①uuoiqo*)--
5∙(①uu0itjSIa)OlMOI-6∙
5
0	log2(#SamPles)	10	0 log2(#SamPles) 10
Figure 5: Convergence of NN-GP Monte Carlo estimates. Validation accuracy (left) of an MC-
CNN-GP increases with n × M (channel count × number of samples) and approaches that of the
exact CNN-GP (not shown), while the distance (right) to the exact kernel decreases. The dark
band in the left plot corresponds to ill-conditioning of KnL,+M1 when the number of outer products
contributing to KnL,+M1 approximately equals its rank. Values reported are for a 3-layer model applied
to a 2K/4K train/validation subset of CIFAR10 downsampled to 8 × 8. See Figure 10 for similar
results with other architectures and §G.2 for experimental details.
where θ consists of M draws of the weights and biases from their prior distribution, θm 〜P (θ), and
n is the width or number of channels in hidden layers. The MC-GP kernel converges to the analytic
kernel with increasing width, limn→∞ Knl ,M = K∞l in probability.
For finite width networks, the uncertainty in Knl ,M is Var Knl ,M = Varθ Knl (θ) /M. From
Daniely et al. (2016, Theorems 2, 3), we know t,hat for well-be,haved nonlinearities (that include
ReLU and erf considered in our experiments) Varj Kll1n (θ)] 8 1, which leads to Varj [K± M] 8
M1n. For finite n, Kn M is also a biased estimate of K∞, where the bias depends solely on net-
work width. We do not currently have an analytic form for this bias, but we can see in Figures
5 and 10 that for the hyperparameters we probe it is small relative to the variance. In particular,
Knl ,M (θ) - K∞L F2 is nearly constant for constant Mn. We thus treat Mn as the effective sample
size for the Monte Carlo kernel estimate. Increasing M and reducing n can reduce memory cost,
though potentially at the expense of increased compute time and bias.
In a non-distributed setting, the MC-GP reduces the memory requirements to compute GPpool from
O |X |2 d2 to O |X |2 + n2 + nd , making the evaluation of CNN-GPs with pooling practical.
5	Discussion
5.1	Bayesian CNNs with many channels are identical to locally connected
networks, in the absence of pooling
Locally Connected Networks (LCNs) (Fukushima, 1975; Lecun, 1989) are CNNs without weight
sharing between spatial locations. LCNs preserve the connectivity pattern, and thus topology, of a
CNN. However, they do not possess the equivariance property of a CNN - if an input is translated,
the latent representation in an LCN will be completely different, rather than also being translated.
The CNN-GP predictions without spatial pooling in §3.1 and §3.2.2 depend only on sample-sample
covariances, and do not depend on pixel-pixel covariances. LCNs destroy pixel-pixel covariances:
K∞L LαC,αN0 (x, x0) = 0, for α 6= α0 and all x, x0 ∈ X and L > 0. However, LCNs preserve the co-
variances between input examples at every pixel: K∞L LαC,αN (x, x0) = K∞L CαN,αN (x, x0). As a result,
in the absence of pooling, LCN-GPs and CNN-GPs are identical. Moreover, LCN-GPs with pooling
are identical to CNN-GPs with vectorization of the top layer (under suitable scaling of yL+1). We
confirm these findings experimentally in trained networks in the limit of large width in Figures 1
and 6 (b), as well as by demonstrating convergence of MC-GPs of the respective architectures to the
same CNN-GP (modulo scaling of yL+1) in Figures 5 and 10.
10
Published as a conference paper at ICLR 2019
5.2	Pooling leverages equivariance to provide invariance
The only kernel leveraging pixel-pixel covariances is that of the CNN-GP with pooling. This enables
the predictions of this GP and the corresponding CNN to be invariant to translations (modulo edge
effects) - a beneficial quality for an image classifier. We observe strong experimental evidence
supporting the benefits of invariance throughout this work (Figures 1, 4, 5, 6 (b); Table 1), in both
CNNs and CNN-GPs.
5.3	Finite channel SGD-trained CNNs can outperform infinite channel
Bayesian CNNs, in the absence of pooling
In the absence of pooling, the benefits of equivariance and weight sharing are more challenging to
explain in terms of Bayesian priors on class predictions (since without pooling equivariance is not
a property of the outputs, but only of intermediary representations). Indeed, in this work we find
that the performance of finite width SGD-trained CNNs often approaches that of their CNN-GP
counterpart (Figure 6, b, c),6 suggesting that in those cases equivariance does not play a beneficial
role in SGD-trained networks.
However, as can be seen in Figures 1, 6 (c), 7, and Table 1, the best CNN overall outperforms the
best CNN-GP by a significant margin - an observation specific to CNNs and not FCNs or LCNs.7
We observe this gap in performance especially in the case of ReLU networks trained with a large
learning rate. In Figure 1 we demonstrate this large gap in performance by evaluating different
models with equivalent architecure and hyperparameter settings, chosen for good SGD-trained CNN
performance.
We conjecture that equivariance, a property lacking in LCNs and the Bayesian treatment of the in-
finite channel CNN limit, contributes to the performance of SGD-trained finite channel CNNs with
the correct settings of hyperparameters. Nonetheless, more work is needed to disentangle and quan-
tify the separate contributions of stochastic optimization and finite width effects,8 and differences in
performance between CNNs with weight sharing and their corresponding CNN-GPs.
6	Conclusion
In this work we have derived a Gaussian process that corresponds to fully Bayesian multi-layer
CNNs with infinitely many channels. The covariance of this GP can be efficiently computed either
in closed form or by using Monte Carlo sampling, depending on the architecture.
The CNN-GP achieves state of the art results for GPs without trainable kernels on CIFAR10. It can
perform competitively with CNNs (that fit the training set) of equivalent architecture and weight
priors, which makes it an appealing choice for small datasets, as it eliminates all training-related
hyperparameters. However, we found that the best overall performance, at least in the absence of
pooling, is achieved by finite SGD-trained CNNs and not by their infinite Bayesian counterparts.
We hope our work stimulates future research towards understanding the distribution over functions
induced by model architecture and training approach, and what aspects of this distribution are im-
portant for model performance.
Another natural extension of our work is the study of other deep learning architectures in the in-
finitely wide limit. After the publication of this paper, Yang (2019) devised a unifying framework
proving the GP convergence for even more models (such as ones using batch normalization, (self-
)attention, LSTM) with slightly different assumptions on the nonlinearity.
6This observation is conditioned on the respective NN fitting the training set to 100%. Underfitting breaks
the correspondance to an NN-GP, since train set predictions of such a network no longer correspond to the true
training labels. Properly tuned underfitting often also leads to better generalization (Table 1).
7Performing an analogous large-dataset comparison between CNNs and CNN-GPs with pooling was not
computationally feasible. Their relative performance remains an interesting open question for future research.
8We remark that concerns about GPs not being able to learn hierarchical representations have been raised
in the literature (Matthews et al., 2018a, Section 7), (Neal, 1995, Chapter 5), (MacKay, 2003, Section 45.7)
However, practical impact of these assertions have not been extensively investigated empirically or theoretically,
and we hope that our work stimulates research in this direction.
11
Published as a conference paper at ICLR 2019
CNN w/ pooling
0.85
0.8
0.75
0.7
0.65
2
6 7 8 9
100
3	456789
1000
#ChanneIs
(C)
Accuracy
Global Average Pooling
z□j
Ooo
5 3 5 2 5
(b)	No Pooling
Ooo
Xow-mɔɔv
5 3 5 2 5
0.35
0.3
0.25
0.2
0.15
-„	GP
NN
10 2	5 100 2	5 1000 2
Ooo
Xow-mɔɔv
5 3 5 2 5
O
—loo
-GP
+ NN
10 2	5 100 2	5 1000 2	5
XOEnOOV


#Channels
Figure 6:	(a): SGD-trained CNNs often perform better with increasing number of channels.
Each line corresponds to a particular choice of architecture and initialization hyperparameters, with
best learning rate and weight decay selected independently for each number of channels (x-axis).
(b): SGD-trained CNNs often approach the performance of their corresponding CNN-GP
with increasing number of channels. All models have the same architecture except for pooling
and weight sharing, as well as training-related hyperparameters such as learning rate, weight decay
and batch size, which are selected for each number of channels (x-axis) to maximize validation
performance (y-axis) of a neural network. As the number of channels grows, best validation accu-
racy increases and approaches accuracy of the respective GP (solid horizontal line). (c): However,
the best-performing SGD-trained CNNs can outperform their corresponding CNN-GPs. Each
point corresponds to the test accuracy of: (y-axis) a specific CNN-GP; (x-axis) the best (on valida-
tion) CNN with the same architectural hyper-parameters selected among the 100%-accurate models
on the full training CIFAR10 dataset with different learning rates, weight decay and number of
channels. While CNN-GP appears competitive against 100%-accurate CNNs (above the diagonal),
the best CNNs overall outperform CNN-GPs by a significant margin (below the diagonal, right).
For further analysis of factors leading to similar or diverging behavior between SGD-trained finite
CNNs and infinite Bayesian CNNs see Figures 1, 7, and Table 1. Experimental details: all net-
works have reached 100% training accuracy on CIFAR10. Values in (b) are reported on an 0.5K/4K
train/validation subset downsampled to 8 × 8 for computational reasons. See §G.5 and §G.1 for full
experimental details of (a, c) and (b) plots respectively.
12
Published as a conference paper at ICLR 2019
氐"ɔpjnɔɔv⅛①I
FCN
CNN w/ ERF
CNN w/ small LR (learning rate) CNN w/ ReLU & large LR
Figure 7:	Aspects of architecture and inference influencing test performance. Test accuracy
(vertical axis, %) for the best model within each model family (horizontal axis), maximizing vali-
dation accuracy over depth, width, and training and initialization hyperpameters ("No Undefitting”
means only models that achieved 100% training accuracy were considered). CNN-GP outperforms
SGD-trained models optimized with a small learning rate to 100% train accuracy. When SGD
optimization is allowed to underfit the training set, there is a significant improvement in general-
ization. Further, when ReLU nonlinearities are paired with large learning rates, the performance of
SGD-trained models again improves relative to CNN-GPs, suggesting a beneficial interplay between
ReLUs and fast SGD training. These differences in performance between CNNs and CNN-GPs are
not observed between FCNs and FCN-GPs, or between LCNs and LCN-GPs (Figure 1), suggest-
ing that equivariance is the underlying factor responsible for the improved performance of finite
SGD-trained CNNs relative to infinite Bayesian CNNs without pooling. Further comparison be-
tween SGD-trained CNNs and CNN-GPs with pooling (omitted due to computational limitations),
where both models do share the property of invariance, would be an interesting direction for future
research. See Table 1 for further results on other datasets, as well as comparison to GP performance
in prior literature. See §G.5 for experimental details.
Model	CIFAR10	MNIST	Fashion-MNIST
CNN with pooling	14.85 (15.65)		
CNN with ReLU and large learning rate	24.76 (17.64)		
CNN-GP	32.86	0.88	7.40
CNN with small learning rate	33.31(22.89)		
CNN with erf (any learning rate)	33.31(22.17)		
Convolutional GP (van der Wilk et al., 2017)	35.40	1.17	
ResNet GP (Garriga-Alonso et al., 2018)		0.84	
Residual CNN-GP (Garriga-Alonso et al., 2018)		0.96	
CNN-GP (Garriga-Alonso et al., 2018)		1.03	
FCN-GP	41.06	1.22	8.22
FCN-GP (Lee et al., 2018)	44.34	1.21	
FCN	45.52 (44.73)		
Table 1: Best achieved test error for different model classes (vertical axis) and datasets (hor-
izontal axis). Test error (%) for the best model within each model family, maximizing validation
accuracy over depth, width, and training and initialization hyperpameters. Except where indicated
by parentheses, all models achieve 100% training accuracy. For SGD-trained CNNs, numbers in
parentheses correspond to the same model family, but without restriction on training accuracy. CNN-
GP achieves state of the art results on CIFAR10 for GPs without trainable kernels and outperforms
SGD-trained models optimized with a small learning rate to 100% train accuracy. See Figure 7 for
visualization and interpretation of these results. See §G.5 for experimental details.
13
Published as a conference paper at ICLR 2019
7 Acknowledgements
We thank Sam Schoenholz, Vinay Rao, Daniel Freeman, Qiang Zeng, and Phil Long for frequent
discussion and feedback on preliminary results.
References
Martin Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S
Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine
learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.
Patrick Billingsley. Probability and Measure. John Wiley & Sons, 1995.
Patrick Billingsley. Convergence of probability measures. John Wiley & Sons, 1999.
Kenneth Blomqvist, Samuel Kaski, and Markus Heinonen. Deep convolutional gaussian processes.
arXiv preprint arXiv:1810.03052, 2018.
Anastasia Borovykh. A gaussian process perspective on convolutional neural networks. arXiv
preprint arXiv:1810.10798, 2018.
John Bradshaw, Alexander G de G Matthews, and Zoubin Ghahramani. Adversarial examples,
uncertainty, and transfer testing robustness in gaussian process hybrid deep networks. arXiv
preprint arXiv:1707.02476, 2017.
Alfredo Canziani, Adam Paszke, and Eugenio Culurciello. An analysis of deep neural network
models for practical applications. arXiv preprint arXiv:1605.07678, 2016.
Minmin Chen, Jeffrey Pennington, and Samuel Schoenholz. Dynamical isometry and a mean field
theory of RNNs: Gating enables signal propagation in recurrent neural networks. In Proceedings
of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
Learning Research,pp. 873-882, StockhoImsmssan, Stockholm Sweden, 10-15 Jul 2018. PMLR.
URL http://proceedings.mlr.press/v80/chen18i.html.
Youngmin Cho and Lawrence K Saul. Kernel methods for deep learning. In Advances in neural
information processing systems, pp. 342-350, 2009.
Anna Choromanska, Mikael Henaff, Michael Mathieu, Gerard Ben Arous, and Yann LeCun. The
loss surfaces of multilayer networks. In Artificial Intelligence and Statistics, pp. 192-204, 2015.
Taco Cohen and Max Welling. Group equivariant convolutional networks. In International confer-
ence on machine learning, pp. 2990-2999, 2016.
Andreas Damianou and Neil Lawrence. Deep gaussian processes. In Artificial Intelligence and
Statistics, pp. 207-215, 2013.
Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks:
The power of initialization and a dual view on expressivity. In Advances In Neural Information
Processing Systems, pp. 2253-2261, 2016.
Kunihiko Fukushima. Cognitron: A self-organizing multilayered neural network. Biological cyber-
netics, 20(3-4):121-136, 1975.
Kunihiko Fukushima and Sei Miyake. Neocognitron: A self-organizing neural network model for
a mechanism of visual pattern recognition. In Competition and cooperation in neural nets, pp.
267-285. Springer, 1982.
Adria Garriga-Alonso, Laurence Aitchison, and Carl Edward Rasmussen. Deep convolutional
networks as shallow Gaussian processes. arXiv preprint arXiv:1808.05587, aug 2018. URL
https://arxiv.org/abs/1808.05587.
14
Published as a conference paper at ICLR 2019
Daniel Golovin, Benjamin Solnik, Subhodeep Moitra, Greg Kochanski, John Karro, and D Scul-
ley. Google vizier: A service for black-box optimization. In Proceedings of the 23rd ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1487-1495.
ACM, 2017.
Ian J Goodfellow, Oriol Vinyals, and Andrew M Saxe. Qualitatively characterizing neural network
optimization problems. International Conference on Learning Representations, 2015.
Boris Hanin and David Rolnick. How to start training: The effect of initialization and architecture.
arXiv preprint arXiv:1803.01719, 2018.
Tamir Hazan and Tommi Jaakkola. Steps toward deep kernel methods from infinite neural networks.
arXiv preprint arXiv:1508.05133, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. 3rd International
Conference for Learning Representations, 2015.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, Citeseer,
2009.
Vinayak Kumar, Vaibhav Singh, PK Srijith, and Andreas Damianou. Deep gaussian processes with
convolutional kernels. arXiv preprint arXiv:1806.01655, 2018.
Neil D Lawrence and Andrew J Moore. Hierarchical gaussian process latent variable models. In
Proceedings of the 24th international conference on Machine learning, pp. 481-488. ACM, 2007.
Nicolas Le Roux and Yoshua Bengio. Continuous neural networks. In Artificial Intelligence and
Statistics, pp. 404-411, 2007.
Yann Lecun. Generalization and network design strategies. In Connectionism in perspective. Else-
vier, 1989.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Jaehoon Lee, Yasaman Bahri, Roman Novak, Sam Schoenholz, Jeffrey Pennington, and Jascha Sohl-
dickstein. Deep neural networks as gaussian processes. In International Conference on Learning
Representations, 2018. URL https://openreview.net/forum?id=B1EA-M-0Z.
Henry W Lin, Max Tegmark, and David Rolnick. Why does deep and cheap learning work so well?
Journal of Statistical Physics, 168(6):1223-1247, 2017.
Jonathan L Long, Ning Zhang, and Trevor Darrell. Do convnets learn correspondence? In Advances
in Neural Information Processing Systems, pp. 1601-1609, 2014.
Philip M Long and Hanie Sedghi. On the effect of the activation function on the distribution of
hidden nodes in a deep network. arXiv preprint arXiv:1901.02104, 2019.
David JC MacKay. Information theory, inference and learning algorithms. Cambridge university
press, 2003.
Alexander G de G Matthews, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin Ghahramani.
Gaussian process behaviour in wide deep neural networks. arXiv preprint arXiv:1804.11271, 9
2018a.
Alexander G. de G. Matthews, Jiri Hron, Mark Rowland, Richard E. Turner, and Zoubin Ghahra-
mani. Gaussian process behaviour in wide deep neural networks. In International Conference
on Learning Representations, 4 2018b. URL https://openreview.net/forum?id=
H1-nGgWC-.
15
Published as a conference paper at ICLR 2019
Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In
Proceedings of the 27th international conference on machine learning (ICML-10), pp. 807-814,
2010.
Radford M. Neal. Priors for infinite networks (tech. rep. no. crg-tr-94-1). University of Toronto,
1994.
Radford M Neal. BAYESIAN LEARNING FOR NEURAL NETWORKS. PhD thesis, University of
Toronto, 1995.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On
the role of implicit regularization in deep learning. Proceeding of the international Conference
on Learning Representations workshop track, abs/1412.6614, 2015.
Roman Novak, Yasaman Bahri, Daniel A. Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein.
Sensitivity and generalization in neural networks: an empirical study. In International Confer-
ence on Learning Representations, 2018. URL https://openreview.net/forum?id=
HJC2SzZCW.
Chris Olah, Alexander Mordvintsev, and Ludwig Schubert. Feature visualization. Distill, 2(11):e7,
2017.
Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,
Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for
raw audio. arXiv preprint arXiv:1609.03499, 2016.
Razvan Pascanu, Yann N Dauphin, Surya Ganguli, and Yoshua Bengio. On the saddle point problem
for non-convex optimization. arXiv preprint arXiv:1405.4604, 2014.
Tomaso Poggio, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao.
Why and when can deep-but not shallow-networks avoid the curse of dimensionality: A re-
view. International Journal of Automation and Computing, 14(5):503-519, Oct 2017. ISSN
1751-8520. doi: 10.1007/s11633-017-1054-2. URL https://doi.org/10.1007/
s11633-017-1054-2.
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Expo-
nential expressivity in deep neural networks through transient chaos. In Advances In Neural
Information Processing Systems, pp. 3360-3368, 2016.
JoaqUin QUinonero-Candela and Carl Edward Rasmussen. A unifying view of sparse approximate
gaussian process regression. Journal of Machine Learning Research, 6(Dec):1939-1959, 2005.
Ali Rahimi and Ben Recht. Random features for large-scale kernel machines. In In Neural Infom-
ration Processing Systems, 2007.
Carl Edward Rasmussen and Christopher KI Williams. Gaussian processes for machine learning,
volume 1. MIT press Cambridge, 2006.
David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal representations
by error propagation. Technical report, California Univ San Diego La Jolla Inst for Cognitive
Science, 1985.
Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information
propagation. ICLR, 2017.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. Nature, 550(7676):354, 2017.
Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks:
Visualising image classification models and saliency maps. ICLR Workshop, 2014.
Peter Tino, Michal Cernansky, and Lubica BenUSkova. Markovian architectural bias of recurrent
neural networks. IEEE Transactions on Neural Networks, 15(1):6-15, 2004.
16
Published as a conference paper at ICLR 2019
Peter Tino, Barbara Hammer, and Mikael Boden. Markovian bias of neural-based architectures
with feedback connections. In Perspectives ofneural-Symbolic integration, pp. 95-133. Springer,
2007.
Michalis Titsias. Variational learning of inducing variables in sparse gaussian processes. In Artificial
Intelligence and Statistics, pp. 567-574, 2009.
Mark van der Wilk, Carl Edward Rasmussen, and James Hensman. Convolutional gaussian pro-
cesses. In Advances in Neural Information Processing Systems 30, pp. 2849-2858, 2017.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
LUkaSz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infar-
mation Processing Systems, pp. 5998-6008, 2017.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
arXiv:1011.3027, 2010.
Paul J Werbos. Generalization of backpropagation with application to a recurrent gas market model.
Neural networks, 1(4):339-356, 1988.
Christopher KI Williams. Computing with infinite networks. In Advances in neural information
processing systems, pp. 295-301, 1997.
Andrew G Wilson, Zhiting Hu, Ruslan R Salakhutdinov, and Eric P Xing. Stochastic variational
deep kernel learning. In Advances in Neural Information Processing Systems, pp. 2586-2594,
2016a.
Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel learning.
In Artificial Intelligence and Statistics, pp. 370-378, 2016b.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms, 2017a.
Lechao Xiao, Yasaman Bahri, Sam Schoenholz, and Jeffrey Pennington. Training ultra-deep cnns
with critical initialization. In NIPS Workshop, 2017b.
Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel Schoenholz, and Jeffrey Pennington.
Dynamical isometry and a mean field theory of CNNs: How to train 10,000-layer vanilla con-
volutional neural networks. In Proceedings of the 35th International Conference on Machine
Learning, volume 80 of Proceedings of Machine Learning Research, pp. 5393-5402, Stock-
holmsmssan, Stockholm Sweden, 10-15 Jul 2018. PMLR. URL http://proceedings.
mlr.press/v80/xiao18a.html.
Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior,
gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760,
2019.
Greg Yang and Sam S. Schoenholz. Deep Mean Field Theory: Layerwise Variance and Width
Variation as Methods to Control Gradient Explosion. ICLR Workshop, February 2018. URL
https://openreview.net/forum?id=rJGY8GbR-.
Greg Yang and Samuel Schoenholz. Mean field residual networks: On the edge of chaos. In
Advances in neural information processing systems, pp. 7103-7114, 2017.
Greg Yang, Jeffrey Pennington, Vinay Rao, Jascha Sohl-Dickstein, and Sam S. Schoenholz. A mean
field theory of batch normalization. ICLR, February 2018. URL https://openreview.
net/forum?id=rJGY8GbR-.
Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In
European conference on computer vision, pp. 818-833. Springer, 2014.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In International Conference on Learning Rep-
resentations, 2017.
17
Published as a conference paper at ICLR 2019
Appendices
A Additional figures
No Pooling
Global Average Pooling
SSoa uo-wp-υʌ
zu)
ZZu
3 2 11^
SSoa Uolγep-w>
#Channels
Figure 8: Validation loss convergence. Best validation loss (vertical axis) of trained neural net-
works (dashed line) as the number of channels increases (horizontal axis) approaches that of a
respective (MC-)CNN-GP (solid horizontal line). See Figure 6 (b) for validation accuracy, Figure
9 for training loss and §G.1 for experimental details.
0.1
0.01
100μ
10μ
1μ
No Pooling
0.001
5
10	2	5	100 2	5 1000
0.1
0.01
O -
匕 0.001
U
100μ
S
自10μ
1μ 5
Global Average Pooling
∙1z! 〃 NNN
O0.00000
O 1
NNC
00
1X
O
0
1X
∙1z! 〃 NNN
oo∙omo°
S 1
SSOaUIUIWa
10	2	5	100	2	5 1000 2	5
00
1x
O
0
1x
2
5
#Channels
Figure 9:	No underfitting in small models. Training loss (vertical axis) of best (in terms of vali-
dation loss) neural networks as the number of channels increases (horizontal axis). While perfect 0
loss is not achieved (but 100% accuracy is), we observe no consistent improvement when increasing
the capacity of the network (left to right). This eliminates underfitting as a possible explanation
for why small models perform worse in Figure 6 (b). See Figure 8 for validation loss and §G.1 for
experimental details.
18
Published as a conference paper at ICLR 2019
10
0
0.42
0.1
0	log2(#SamPles)	10
10
0
0.5
-6.5
0 log2(#SamPles) 10
10
0
0.42
0.1
0	log2(#SamPles)	10
10
0
0.5
-6.5
0 log2(#SamPles) 10
10
0
0	log2(#SamPles)	10
0.42
10
0.5
0.1
-6.5
10
0
0	log2(#SamPles)	10
0.42
0.1
0 log2(#SamPles) 10
10
0
0 log2(#SamPles) 10
0.5
-6.5
δ
0
Figure 10:	Convergence of NN-GP Monte Carlo estimates. As in Figure 5, validation accuracy
(left) of MC-GPs increases with n × M (i.e. width times number of samples), while the distance
(right) to the the respective exact GP kernel (or the best available estimate in the case of CNN-GP
with pooling, top row) decreases. We remark that when using shared weights, convergence is slower
as smaller number of independent random parameters are being used. See §G.2 for experimental
details.
19
Published as a conference paper at ICLR 2019
Depth →	1	10	100	1000	Phase boundary
CKkZZU CKkZUH
2 ①ɔu--jBA s~mO
8
0 Koumgv 0
0.1 Weight variance 5
Figure 11:	Large depth performance of NN-GPs. Validation accuracy of CNN- and FCN-GPs as
a function of weight (σω2 , horizontal axis) and bias (σb2, vertical axis) variances. As predicted in §B,
the regions of good performance concentrate around the critical line (phase boundary, right) as the
depth increases (left to right). All plots share common axes ranges and employ the erf nonlinearity.
See §G.2 for experimental details.
B	Relationship to Deep Signal Propagation
The recurrence relation linking the GP kernel at layer l+1 to that of layer l following from Equation 9
(i.e. K∞l+1 = (C ◦ A) K∞l ) is precisely the covariance map examined in a series of related papers
on signal propagation (Xiao et al., 2018; Poole et al., 2016; Schoenholz et al., 2017; Lee et al.,
2018) (modulo notational differences; denoted as F, C or e.g. A ? C in Xiao et al. (2018)). In
those works, the action of this map on hidden-state covariance matrices was interpreted as defining
a dynamical system whose large-depth behavior informs aspects of trainability. In particular, as
l → ∞, K∞+1 = (C◦A) (K∞∞) ≈ K∞ ≡ K∞, i.e. the covariance approaches a fixed point K∞.
The convergence to a fixed point is problematic for learning because the hidden states no longer
contain information that can distinguish different pairs of inputs. It is similarly problematic for
GPs, as the kernel becomes pathological as it approaches a fixed point. Precisely, in both chaotic
and ordered regimes, outputs of the GP become asymptically identically correlated. Either of these
scenarios captures no information about the training data in the kernel and makes learning infeasible.
This problem can be ameliorated by judicious hyperparameter selection, which can reduce the rate
of exponential convergence to the fixed point. For hyperpameters chosen on a critical line separating
two untrainable phases, the convergence rates slow to polynomial, and very deep networks can be
trained, and inference with deep NN-GP kernels can be performed - see Figure 11.
C S trided convolutions, average pooling in intermediate layers,
HIGHER DIMENSIONS
Our analysis in the main text can easily be extended to cover average pooling and strided convolu-
tions (applied before the pointwise nonlinearity). Recall that conditioned on Kl the pre-activation
zjl (x) ∈ Rd1 is a zero-mean multivariate Gaussian. Let B ∈ Rd2×d1 denote a linear operator. Then
Bzjl(x) ∈ Rd2 is a zero-mean Gaussian, and the covariance is
E{ωl,bl} [(Bzj (X))(Bzj (XO))TlK l] = BE{ωl,bl} hzj (X) ZIj (XO)TlK l i BT.	QO)
One can easily see that BzjlKl j are i.i.d. multivariate Gaussian as well.
Strided convolution. Strided convolution is equivalent to a non-strided convolution composed with
subsampling. Let s ∈ N denote size of the stride. Then the strided convolution is equivalent to
choosing B as follows: Bij = δ(is - j) for i ∈ {0, 1, . . . (d2 - 1)}.
Average pooling. Average pooling with stride s and window size ws is equivalent to choosing
Bij = 1/ws for i = 0, 1, . . . (d2 - 1) and j = is, . . . , (is + ws - 1).
20
Published as a conference paper at ICLR 2019
ND convolutions. Note that our analysis in the main text (1D) easily extends to higher-dimensional
convolutions by replacing integer pixel indices and sizes d, α, β with tuples (see also Figure 2).
In Equation 2 β values would have to span the hypercube [±k]N = {-k, . . . , k}N in the pre-
activation definition. Similarly, in §3 the normalizing factor d (d2) should be the product (squared)
of its entries, and summations over α, β should span the [d0] ×∙∙∙× Mn] hypercube as well. The
definition of the kernel propagation operator A in Equation 5 will remain exactly the same, so
long as β is summed over the hypercube, and the variance weights remain respectively normalized
Pβ vβ = 1.
D Review of exact Bayesian regression with GPs
Our discussion in the paper has focused on model priors. A crucial benefit we derive by mapping
to a GP is that Bayesian inference is straightforward to implement and can be done exactly for
regression (Rasmussen & Williams, 2006, chapter 2), requiring only simple linear algebra. Let
X denote training inputs x1, ..., x|X|, tT = t1, ..., t|X| training targets, and collectively D for
the training set. The integral over the posterior can be evaluated analytically to give a posterior
predictive distribution on a test point y* which is Gaussian, (z*∣D,y*)〜N (μ*,σ2), with
μ* = K (x*, X)T (K (X, X) + σ∣I∣χ∣)-11,	(21)
σ2 = K (x*,x*) -K (x*, X )T (K (X, X) + σ∣IlXl)-1 K (x*, X).	(22)
We use the shorthand K (X, X) to denote the |X| × |X| matrix formed by evaluating the GP co-
variance on the training inputs, and likewise K (x*, X) is a ∣X∣-length vector formed from the co-
variance between the test input and training inputs. Computationally, the costly step in GP posterior
predictions comes from the matrix inversion, which in all experiments were carried out exactly,
and typically scales as O |X|3 (though algorithms scaling as O |X |2.4 exist for sufficiently
large matrices). Nonetheless, there is a broad literature on approximate Bayesian inference with
GPs which can be utilized for efficient implementation (Rasmussen & Williams, 2006, chapter 8);
(Quifionero-Candela & Rasmussen, 2005; Titsias, 2009).
E Equivalence between randomly initialized NNs and GPs
In this section, we present two different approaches, the sequential limit (§E.3) and simultaneous
limit (§E.4), to illustrate the relationship between many-channels Bayesian CNNs and GPs.
Sequential limit (§E.3) involves taking the infinite channel limit in hidden layers in a sequence,
starting from bottom (closest to inputs) layers and going upwards (to the outputs), i.e. n1 →
∞, . . . , nL → ∞. Note that this approach in fact only gives intuition into construction a GP using a
NN architecture to define its covariance, and does not provide guarantees on actual convergence of
large but finite Bayesian CNNs to GPs (which is of most practical interest), nor does it guarantee the
existence of the specified GP on a given probability space. However, it has the following benefits:
1.	Weak assumptions on the NN activation function φ and on the distribution of the NN pa-
rameters.
2.	The arguments can be easily extended to more complicated network architectures, e.g.
architectures with max pooling, dropout, etc.
3.	A straightforward and intuitive way to compute the covariance of the Gaussian process
without diving into mathematical details.
Simultaneous limit (§E.4) considers growing the number of channels in hidden layers uniformly, i.e.
min n1, . . . , nL → ∞. This approach establishes convergence of finite channel Bayesian CNNs
to GPs and is thus a more practically relevant result. However, it makes stronger assumptions, and
the proof is more involved.
We highlight that the GPs obtained by the two approaches are identical.
In both sections, we only provide the arguments for CNNs. Itis straightforward (and in fact simpler)
to extend them to LCNs and FCNs. Indeed, an FCN is a particular case of a CNN where the
21
Published as a conference paper at ICLR 2019
inputs and filters have singular spatial dimensions (d = 1, k = 0). For LCNs, the proof goes
through in an identical fashion if we replace A with ALCN defined as ALCN (K) αα0 (x, x0) ≡
δα,α0 [A(K)]α,α0 (x, x0).
E.1	Setup
Probability space. Let P be a collection of countably many mutually independent random vari-
ables (R.V.s) defined on a probability space (Ω, F, P), where F is a product Borel σ-algebra and P
is the probability measure. Here P ≡ W ∪ B ∪ H is the collection of parameters used to define
neural networks:
(i)	Weights. W = Sl∈N W l and W l = nωilj,β : i, j ∈ N, β ∈ [±k]o, where [±k] ≡ [-k, k] ∩ Z.
We assume ωilj,β are i.i.d. R.V.s with mean zero and finite variance 0 < σω2 < ∞ (note the
lack of scaling by input dimensionality, compensated for later in Equations 29 and 42; see also
similar notation used in Matthews et al. (2018a)). When l = 0, we further assume they are
Gaussian distributed.
(ii)	Biases. B = Sl∈N Bl and Bl = blj : j ∈ N . We assume blj are i.i.d. Gaussian with mean
zero and variance 0 ≤ σb2 < ∞.
(iii)	Place-holder. H is a place-holder to store extra (if needed) R.V.s , e.g. parameters coming
from the final dense layer.
Inputs. We will consider a fixed X ⊆ Rn0 ×d to denote the inputs, with input channel count n0,
number of pixels d. Assume x 6= 0 for all x ∈ X and |X |, the cardinality of the inputs, is finite.
However, our results can be straightforwardly extended to a countably-infinite input indexing spaces
X for certain topologies via an argument presented in Matthews et al. (2018a, section 2.2), allowing
to infer weak convergence on X from convergence on any finite subset (which is the case we consider
in this text; see also Billingsley (1999, page 19) for details). For this reason, as in Matthews et al.
(2018a), weak convergence of countably-infinite stochastic processes will be considered with respect
to the topology generated by the following metric:
ν (s, s0) ≡ X 2-k min (1, |sk - s0k |) , ∀s, s0 ∈ RN.
k=1
Notation, shapes, and indexing. We adopt the notation, shape, and indexing convention similar to
§2.1, which the reader is encouraged to review. We emphasize that whenever an index is omitted,
the variable is assumed to contain all possible entries along the respective dimension (e.g. whole X
if x is omitted, or all nl channels if the channel i is omitted).
E.2 Preliminary
We will use the following well-known theorem.
Theorem E.1. Let X, {Xn}n∈N be R.V.s in Rm. The following are equivalent:
(i)	Xn →D X (converges in distribution / converges weakly),
(ii)	(Portmanteau Theorem) For all bounded continuous function f : Rm → R,
lim E[f(Xn)] =E[f(X)],	(23)
(iii)
(Levy,s Continuity Theorem) The characteristic functions of Xn, i.e. E [e,tTX1
those of X pointwise, i.e. for all t ∈ Rm,
→m E [etTXni = E [etTXi ,
n , converge to
(24)
where d denotes the imaginary unit.
22
Published as a conference paper at ICLR 2019
Using the equivalence between (i) and (iii), it is straightforward to show that
Theorem E.2 (Cramer-Wold, (Billingsley (1995), Theorem 29.4)).
Xn → X	o	aτXn → aτX for all a ∈ Rm.	(25)
In particular,
Xn → N (0, Σ)	0	aτXn → N(0,aτΣa) for all a ∈ Rm	(26)
E.3 Sequential limit
In this section, we give intuition into constructing a Gaussian process using an infinite channel
CNN with the limits taken sequentially. Informally, our argument amounts to showing that if the
inputs zl-1 to the layer l are a multivariate Gaussian with covariance A (Kl) 0 InI, then it's out-
puts Zl converge in distribution to a multivariate Gaussian with covariance A (Kl+1) 0 In1+1 as
nl → ∞. This “allows” us to sequentially replace z1 , z2 , . . . , zL with their respective limiting
Gaussian R.V.s as we take n1 → ∞, n2 → ∞, . . . , nL → ∞. However, since each convergence
only holds in distribution and does not guarantee the existence of the necessary GPs on a given
probability space, what follows merely gives intuition into understanding the relationship between
wide Bayesian neural networks and GPs (contrary to §E.4, which presents a rigorous convergence
proof for min n1 , . . . , nL → ∞).
Let Ψ1 denote the space of functions with a uniformly bounded second moment, i.e. having
C2 (R,φ) ≡ sup	Ex〜N(o,r) ∣Φ(x)∣2 < ∞ for every R ≥ 1.	(27)
ι∕R≤r≤R	'
Let N* = N∖{0}. We construct Gaussian processes with the following iterative in l (from 0 to L)
procedure:
(i)
If l > 0, define random variables
(a GP) zil,∞ i∈N
in (Ω, F, P) as i.i.d. Gaussian with
mean zero and covariance A K∞l , so that they are also independent from any future events,
i.e. independent from the σ-algebra generated by all R.V.s with layer index greater than l. Here
We implicitly assume the probability space (Ω, F, P) has enough capacity to fit in the R.V.s
generated by the above procedure, if not We will extend the sample space Ω using product
measures.
(ii)	For n ∈ N* , define
yil,α(x) ≡
xi,α	l = 0
φzil,-α1,∞(x)	l>0 ,
,	.√n0 Pj∈[n0] Pβ∈[±k] √vβωij,βyj,α+β (X) + bli	l = 0
zi,α(χ) ≡
l	√n Σj∈[n] ∑β∈[±k] √vβωij,βyj,α+β(X) + bi	l >0
(28)
(29)
where
[n] ≡ {1, . . . , n} and [±k] ≡ {-k, . . . , 0, . . . k} .	(30)
(iii	) Prove that for any finite m ≥ 1, zil,n	⊆ R|X |d converges in distribution to a multivariate
i∈[m]
normal with mean zero and covariance A K∞l	0 Im as n → ∞, where K∞l is defined
identically to Equation 9. As a consequence, per our remark in §E.1, zil,n	converges
i	i∈N*
weakly to the GP zil,∞
i∈N*
In what follows, we use the central limit theorem to prove (iii).
23
Published as a conference paper at ICLR 2019
Theorem E.3. If φ ∈ Ψ1, then for every l ≥ 0 and every m ≥ 1, zil,n	⊆ R|X |d converges in
i∈[m]
distribution to a multivariate normal with mean zero and covariance A (K∞) 0 Im.
Proof. We proceed by induction. This is obvious in the base case l = 0, since the weights and biases
are assumed to be independent Gaussian. Now we assume the theorem holds for l - 1. This implies
{yi}i∈N* = {φ (zi-1,∞) } i are i.i.d. random variables.
Choose a vector a ≡ [ai (x)]iT∈[m], x∈X ∈ Rm|X|. Then
X ai(X)Zi,n(X)= χ ai(x)√1n I χ χ √vβωij,βyj,α+β(X)+bi I	(31)
i∈[m]	i∈[m]	j∈[n] β∈[±k]
x∈X	x∈X
/	∖
√n	E E ai(X) Σ √vβωij,βyj,α+β(X) + E ai(X)bi	附
j∈[n] i∈[m]	β∈[±k]	i∈[m]
x∈X	x∈X
≡ √n X uj + q.	(33)
j∈[n]
It is not difficult to see that uj are i.i.d. and q is Gaussian. Then we can use the central limit theorem
to conclude that the above converges in distribution to a Gaussian, once we verify the second moment
of uj is finite. Using the fact that
over these R.V.s first, we get
ωilj,β i,β
is a collection of independent R.V.s, and integrating
2
Eu2 =E	X	ai(X) X √vβωij,βyj,α+β(X)	(34)
i∈[m] , x∈X	β∈[±k]
2
=σω2 X X vβ E Xai(X)yjl,α+β(X)	(35)
i∈[m] β∈[±k]	x∈X
∑aT A (K∞ )ai
i∈[m]
(36)
where by A we denote the linear transformation part of A from Equation 5, i.e. A without the
translation term σb2 :
[A(K)] αR (X, X0) ≡ σ22 X vβ [K]α+β,α0 + β (X, X0) .	(37)
β
To prove finiteness of the second moment in Equation 36, it is sufficient to show that all the diagonal
terms of K∞l are finite. This easily follows from the assumption φ ∈ Ψ1 and the definition of
K∞ = (C ◦ A)l (K0). Together with the distribution of v (whose covariance is straightforward
to compute), the joint distribution of zil,n	converges weakly to a mean zero Gaussian with
i∈[m]
covariance matrix A (K∞) 0 Im by Theorem E.1 (Equation 26).	□
Remark E.1. The results of Theorem E.3 can be strengthened / extended in many directions.
(1) The same result for a countably-infinite input index set X follows immediately according to the
respective remark in §E.1.
(2) The same analysis carries over (and the covariance matrix can be computed without much extra
effort) if we stack a channel-wise deterministic affine transform after the convolution operator.
Note that average pooling (not max pooling, which is not affine), global average pooling and the
convolutional striding are particular examples of such affine tranforms. Moreover, valid padding
(i.e. no padding) convolution can be regarded as a subsampling operator (a linear projection)
composed with the regular circular padding.
24
Published as a conference paper at ICLR 2019
(3) The same analysis applies to max pooling, but computing the covariance may require non-trivial
effort. Let m denote the max pooling operator and assume it is applied right after the activation
function. The assumption yil i∈[n]
are i.i.d. implies m yil	i∈[n] are
also i.i.d. Then we can proceed exactly as above except for verifying the finiteness of second
moment of m(yil) with the following trivial estimate:
Emax {yig}；+ ≤ E X ∣yi,α∣2
α∈[s]
(38)
where s is the window size of the max pooling.
In general, one can stack a channel-wise deterministic operator op on yil so long as the second
moment of op yil is finite. One can also stack a stochastic operator (e.g. dropout), so long
as the outputs are still channel-wisely i.i.d. and have finite second moments.
E.4 Simultaneous limit
In this section, we present a sufficient condition on the activation function φ so that the neural net-
works converge to a Gaussian process as all the widths approach infinity simultaneously. Precisely,
let t ∈ N and for each l ≥ 0, let nl : N → N be the width function at layer l (by convention
n0 (t) = n0 is constant). We are interested in the simultaneous limit nl = nl (t) → ∞ as t → ∞,
i.e., for any fixed L ≥ 1
min {n1 (t), . . . , nL (t)} ---→ ∞.	(39)
t→∞
Define a sequence of finite channel CNNs as follows:
xi,α,	l = 0
l-1t	,	(40)
φ zi,α , (x) , l > 0
(41)
..	.√n0 Pj∈[n0] Pβ∈[±k] √vβωij,βyjα+β (X) + bi , l = 0
zlα(x) ≡ ∖	lt	.	(42)
[Pj∈[nl(t)] Pβ∈[±k] √vβ ωij,β yj,,α+β(X) + bi， l > 0
This network induces a sequence of covariance matrices Ktl (which are R.V.s): for l ≥ 0 and t ≥ 0,
for X, X0 ∈ X
1	nl(t)
[Kt]α,α0(X,x0) ≡ nψy E yi,α(X)yi%(XO).	(43)
We make an extra assumption on the parameters.
Assumption: all R.V.s in W are Gaussian distributed.
Notation. Let PSDm denote the set of m × m positive semi-definite matrices and for R ≥ 1, define
PSDm(R) ≡ {Σ ∈ PSDm : 1/R ≤ Σα,α ≤ R for 1 ≤ α ≤ m} .	(44)
Further let T∞ : PSD2 → R be a function given by
T∞(∑) ≡ E(χ,y)〜N(0,∑) [Φ(X)Φ(y)],	(45)
and Ck (φ, R) (may equal ∞) denotes the uniform upper bound for the k-th moment
Ck (φ, R) ≡ SUp	Ex〜N(0,r) ∣Φ(X)∣k .	(46)
1∕R≤r≤R
Let Ψ denotes the space of measurable functions φ with the following properties:
25
Published as a conference paper at ICLR 2019
1.	Uniformly bounded second moment: for every R ≥ 1, C2 (φ, R) < ∞.
2.	Lipschitz continuity: for every R ≥ 1, there exists β = β (φ, R) > 0 such that for all
Σ, Σ0 ∈ PSD2(R),
∣T∞(∑) -T∞(∑0)l ≤ βk∑ - ∑0k∞ ；	(47)
3.	Uniform convergence in probability: for every R ≥ 1 and every ε > 0 there exists
a positive sequence ρn(φ, ε, R) with ρn(φ, ε, R) → 0 as n → ∞ such that for every
Σ ∈ PSD2(R) and any {(χi,yi)}rn=ι i.i.d.〜N(0, ∑)
P (In〉： φ (Xi) φ (yi) - T∞ (与 > ε) ≤ Pr (φ, ε, R) .	(48)
We will also use Ψ1, Ψ2 and Ψ3 to denote the spaces of measurable functions φ satisfying properties
1, 2, and 3, respectively. It is not difficult to see that for every i, Ψi is a vector space, and so is
Ψ = ∩iΨi.
Finally, we say that a function f : R → R is exponentially bounded if there exist a, b > 0 such that
|f (x)| ≤ aeb|x| a.e. (almost everywhere)	(49)
We now prove our main result presented in §2.2.3 through the following three theorems.
Theorem E.4. If φ is absolutely continuous and φ0 is exponentially bounded then φ ∈ Ψ.
Theorem E.5. If φ ∈ Ψ, then for l ≥ 0, Ktl →P K∞l .
Theorem E.6. If for l ≥ 0, Ktl →P K∞l and m ≥ 1, the joint distribution of zjl,t	converges
j∈[m]
in distribution to a multivariate normal distribution with mean zero and covariance A (K∞) 0 ".
The proofs of Theorems E.4, E.5, and E.6 can be found in §E.7, §E.6, and §E.5 respectively. The
proof of Theorem E.5 is slightly more technical and we will borrow some ideas from Daniely et al.
(2016).
E.5 Proof of Theorem E.6
Per Equation 26, it suffices to prove that for any vector [ai(x)]i∈[m], x∈X ∈ Rm|X|,
X ai(x)zi,t(x) -→ N I 0, X	ai(x)TA (K∞) ai(x)
i∈[m]	i∈[m], x∈X
x∈X
Indeed, the characteristic function
E exp I i X	ai(x)zi,t(x) I
i∈[m], x∈X
=E eχp i X ai(χ) I pn== X X √vβ3lij,βyjα+β(X) + bi I
i∈[m]	j∈[rl (t)] β∈[±k]
x∈X
(50)
(51)
(52)
Note that conditioned on
yjl,t j∈[rl(t)]
, the exponent in the above expression is just a linear com-
bination of independent Gaussian R.V.s
3ilj,β
, bli
, which is also a Gaussian. Integrating out these
26
Published as a conference paper at ICLR 2019
R.V.s using the formula of the characteristic function of a Gaussian distribution yields
E exp i X	ai(x)zil,t(x)	(53)
i∈[m], x∈X
=E eχp ( -1 X aT (A(Kt)) ai )	(54)
i∈[m]
—→exp I — 2 ^X aT (A (K∞)) ai I as t τ ∞,	(55)
i∈[m]
where we have used Ktl -→ K∞l and Lipschitz continuity of the respective function in the vicinity of
K∞l in the last step. Therefore, Equation 50 is true by Theorem E.1 (iii). As in §E.3, the same result
for a countably-infinite input index set X follows immediately according to the respective remark in
§E.1.
□
Remark E.2. We briefly comment how to handle the cases when stacking an average pooling, a
subsampling or a dense layer after flattening the activations in the last layer.
(i)	Global Average pooling / subsampling. Let B ∈ R1×d be any deterministic linear functional
defined on Rd . The fact that
Ktl -P→ K∞l
(56)
implies that the empirical covariance of
Byjl,t
n1t) X B01X|y；，t(B©X|y丁)T -→ B^XK∞ (BqXI)T
j∈nl(t)
(57)
where B0|X| ∈ R1×dlXl, |X| copies of B. Invoking the same “characteristic function” argu-
ments as above, it is not difficult to show that stacking a dense layer (assuming the weights and
biases are drawn from i.i.d. Gaussian with mean zero and variances σω2 and σb2, and are prop-
erly normalized) on top of Byjl,t the outputs are i.i.d. Gaussian with mean zero and covari-
ance σ2B0lXlK∞ (B01X1)T + σb2. Taking B = (ɪ,..., ɪ) ∈ R1×d (or B = e. ∈ R1×d)
implies the result of global average pooling (§3.2.1, Equation 17), or subsampling (§3.2.2,
Equation 18).
(ii)	Vectorization and a dense layer. Let {ωj,a}i∈[m] ,日"⑶]α∈[d] be the weights of the dense
layer, ωj,a represents the weight connecting the α-th pixel of the j-channel to the i-th output.
Note that the range of α is [d] not [±k] because there is no weight sharing. Define the outputs
to be
fi(X) = Pnb aW」…bi
(58)
Now let [ai(x)]i∈[m],x∈X ∈ Rm|X | and compute the characteristic function of
ai(x)fi(x).
i，x
(59)
27
Published as a conference paper at ICLR 2019
Using the fact Eωij,αωi0j0 ,α0 = 0 unless (ij, α) = (i0j0 , α0) and integrating out the R.V.s of
the dense layer, the characteristic function is equal to
/
E exp
- 1 XX ai(χ)ai(χ0)
i∈[m] x,x0∈X
∖
(
1
nl (t)d
∖
σω2yjl,,tα(x)yjl,,tα(x0) +σb2
j∈[nl(t)]
α∈[d]
-2 XX ai(X)ai(xO)丘(σωκιt(χ,χ,)+σ2
i∈[m] x,x0∈X
-^→ exp I -2 X X ai(x)ai(x0)δ∙ (σ22K∞(x,x0) + σ2
i∈[m] x,x0∈X
E exp
(60)
(61)
(62)
where tr denotes the mean trace operator acting on the pixel by pixel matrix, i.e. the functional
computing the mean of the diagonal te∙ms of the pixel by pixel mat∙ix. The∙efo∙e [fi]i∈[m]
converges weakly to a mean zero Gaussian with covariance [σ22tr (K∞ (x, x0)) + σ2] X ,0∈χ ③
Im in the case of vectorization (§3.1).
E.6 Proof of Theorem E.5
Proof. We recall KL and K∞ to be random matrices in RdlXl×dlXl, and We will
prove convergence KL	P →	K∞ with respect to ∣∣ ∙ ∣∣∞, the pointwise '∞-norm
t→∞
(i.e. ∣∣K∣∣∞ = maxχ,χo,α,α0 ∣Kα,α0 (x,x0)∣)∙ Note that due to finite dimensionality of KL, con-
vergence w.r.t. all other norms follows.
We first note that the affine transform A is σω2 -Lipschitz and property 2 of Ψ implies that the C
operator is β-Lipschitz (both w.r.t. w.r.t. ∣∙ ∣∣∞). Indeed, if we consider
Σ≡
α,α (x, x)
α0,α (x , x)
[K]α,α0 (x, x )
[K]α0,α0 (x0, x0)
(63)
then [C (K)]α,α0 (x, x0) = T∞(Σ). Thus C ◦ A is σω2 β-Lipschitz.
We now prove the theorem by induction. Assume Ktι →P K∞ι as t → ∞ (obvious for l = 0).
We first remark that K∞ι ∈ PSD|X|d, since PSD|X |d 3 E Ktι → K∞ι and PSD|X |d is closed.
Moreover, due to Equation 4, A necessarily preserves positive semi-definiteness, and therefore
A K∞ι ∈ PSD|X|daswell.
Now let ε > 0 be sufficiently small so that the 言-neighborhood of A(K∞) is contained in
PSD|X|d(R), where we take R to be large enough for K∞ι to be an interior point of PSD|X|d(R).9
Since
Uk∞+1-Kt+1L ≤ Uk∞+1-coA(Kt)L + UC。A(Kt) -Kt+1L	(64)
=IIC ◦A(K∞) -C。A(Kt) L + UC。A(Kt) - Kt+1L,	(65)
to prove Kl+1 → K∞+1, it suffices to show that for every δ > 0, there is a t* such that for all t > t*,
P (IIC ◦A (k∞ ) -CoA (Kt)L > ε) + P (IIC ◦A (Kt) - KtTL > ε )<δ (66)
By our induction assumption, there is a tι such that for all t > tι
P (iIk∞ - k'iIi∞ > 2σωβ) < 3.	(67)
9 Such R always exists, because the diagonal terms of K∞l are always non-zero. See (Long & Sedghi, 2019,
Lemma 4) for proof.
28
Published as a conference paper at ICLR 2019
Since C ◦ A is σω2 β-Lipschitz, then
P QIC。/(K∞)-CoA(Kt)L > ε)< ：.	(68)
To bound the second term in Equation 66, let U (t) denote the event
U (t) ≡ {A (Kt) ∈ ΡSD∣x∣d (R)}	(69)
and U (t)c its complement. For all t > tl its probability is
P (U (t)c) <P (M(K∞) -A(Kt )∣∣∞ > 2β)	[assumption on small ε]	(70)
< P - ∣∣K∞—KtIL > 2β)	[A is σω -LiPsh陷	(71)
=P (∣IK∞ - KtIL > 2στβ) < 3.	[Equation 67]	(72)
Finally, denote
[V( t )]α,α0 (x, x0) ≡ { I [C OA (Kt)] α,α0 (x, 2 - [K"] a,a，Ε 2 | > 三}， 的
i.e. the event that the inequality inside the braces holds. The fact
{ ∣∣C OA(Kt) -Kt + 1IL > ε O ⊆ U (t)c [ I U	[V (t)]α,α0 (x,X0) \ U (t)l
∖x,x0,α,α0	)
implies
P ({IIC oA(Kt) - Kι+1(t)ii∞ > ε}) ≤ 3+∣χ∣2 d2 XmaxaOP (ρ冏。,“ (χ,χo) ∩ U ⑴)，
, ,,	(74)
where the maximum is taken over all (x, x0, α, α0) ∈ X2 × [d].
Consider a fixed κ ∈ PSD|X |d, and define
ς 0 T — — ([A (K)]a,a (x, X)	[A (K)]a,a0 (x,χ0)	75.
-a,.,X,/ ) ≡ [/(K)]”,. (X0,X) [A(κ)]a0,aθ(x0,X0)，	心
a deterministic matrix in PSD2 . Then
[COA (κ)]a,a0 (x, X0)= T∞ (∑ (κ, a, a0, x, x0)),	(76)
and, conditioned on Ktl ,
1	nl+1(t)
[Kt + 1 IKt =	K] a,ao	(X, x0)	=	nl + 1(t)	E φ	(Zi,W	(x))	φ (Zi- (XO))	,	(77)
where {(，；,；(x), Zll，t，(x0)) ∣Kl = κ} ,「+]()] are i.i.d.〜N(0, ∑ (κ, a, a0, x, x0)).
Then if Σ (K, a, a0, X, X0) ∈ PSD2 (R) we can apply property 3 of Ψ to conclude that:
P (([V (t)]a,a0 (x, X0) ∩ U (t)) ∣Kt = K) < Ρnl + I(t) (φ, R, ') .	(78)
However, if Σ (K, a, a0, X, X0) 6∈ PSD2 (R), then necessarily A (K) 6∈ PSD|X |d (R) (since
Σ (K, a, a0, X, X0) ∈ PSD2), ensuring that P U (t) |Ktl = K = 0. Therefore Equation 78 holds
for any K ∈ PSD|X|d, and for any (X, X0, a, a0) ∈ X2 × [d]2 .
29
Published as a conference paper at ICLR 2019
We further remark that Pni+i(t)(φ, R, 2)is deterministic and does not depend on (κ, x, x0, α, α0).
Marginalizing out Ktl and maximizing over (x, x0, α, α0) in Equation 78 we conclude that
X ∏ι⅛ιxα, P ([V (t)]αH (x, x0) ∩ U (t)) < Pni+i(t) (φ, R, ε) .	(79)
Since Pn (φ,R, ε) → 0 as n → ∞, there exists n such that for any nl+1(t) ≥ n,
max	P ([V ⑴]α.	(X,xO) ∩ U ⑴)<	Pnl + I(t)	(φ, R,ε)	≤ :	”：2	T2 ,	(80)
x,x0,α,α0	α,α	2	3 |X |2 d2
and, substituting this bound in Equation 74,
P ({M。A(Kt) - Kt+1∖∖∞ > εO ∩ U(t)) < 2δ.	(81)
Therefore we just need to choose tl+1 > tl so that nl+1(t) ≥ n for all t>tl+1.	□
Remark E.3. We list some directions to strengthen / extend the results of Theorem E.5 (and thus
Theorem E.6) using the above framework.
1.	Consider stacking a deterministic channel-wise linear operator right after the convolutional
layer. Again, strided convolution, convolution with no (valid) padding and (non-global)
average pooling are particular examples of this category. Let B ∈ Rd0 ×d denote a linear
operator. Then the recurrent formula between two consecutive layers is
K∞+1 = C^B^A(K∞)	(82)
where B is the linear operator on the covariance matrix induced by B . Conditioned on
Ktl, since the outputs after applying the linear operator B are still i.i.d. Gaussian (and the
property 3 is applicable), the analysis in the above proof can carry over with A replaced by
BoA.
2.	More generally, one may consider inserting an operator op (e.g. max-pooling, dropout and
more interestingly, normalization) in some hidden layer.
3.	Gaussian prior on weights and biases might be relaxed to sub-Gaussian.
E.7 Proof of Theorem E.4
Note that absolutely continuous exponentially bounded functions contain all polynomials, and are
closed under multiplication and integration in the sense that for any constant C the function
x φ(t)dt + C
0
(83)
is also exponentially bounded. Theorem E.4 is a consequence of the following lemma.
Lemma E.7. The following is true:
1.	for k ≥ 1, Ck (φ, R) < ∞ ifφ is exponentially bounded.
2.	φ ∈ Ψ2 ifφ0 exists a.e. and is exponentially bounded.
3.	φ ∈ Ψ3ifC4(φ,R) < ∞.
Indeed, ifφ is absolutely continuous and φ0 is exponentially bounded, then φ is also exponentially
bounded. By the above lemma, φ ∈ Ψ.
ProofofLemma E.7. 1. We prove the first statement. Assume ∣φ(x)∣ ≤ aeb|x|.
Ex〜N(0,r) ∣φ (x)∣k= Ex〜N(0,1) ∖φ (√rx)∣k ≤ Ex〜N(0,i)∣ae√rblXl∣k ≤ 2akek2b2r/2.	(84)
30
Published as a conference paper at ICLR 2019
Thus
Ck (φ, R)=	SUp Ex〜N(0,r) ∣φ (x)|k ≤ 2akek2b2R/2.
1∕R≤r≤R
(85)
2.	To prove the second statement, let Σ, Σ0 ∈ PSD2 (R) and define A (similarly for A0):
A≡
∑22∑11-∑12
ςii
(86)
Then AAT = Σ (and A0A0T = Σ0). Let
A(t) ≡ (1 -t)A+tA0,	t ∈ [0, 1]	(87)
and
f(w) ≡ φ(x)φ(y) where w ≡ (x, y)T .	(88)
Since φ0 is exponentially bounded, φ is also exponentially bounded due to being absolutely contin-
uous. In addition, P (Ilwk2) ∣∣Vf (w)∣b is exponentially bounded for any polynomialP (Ilwk2).
Applying the Mean Value Theorem (we use the notation . to hide the dependence on R and other
absolute constants)
∣T∞(Σ) -T∞(Σ0)|
2∏ / (f (Aw) — f (A0w))exp (— kwk2 /2)dw
(89)
= 21∏ // ] (Vf (A(t)w)) ((A0 - A) w)exp (—kwk2 /2)dtdw
. [0,1]	k(A0 - A) wk2 kVf (A(t)w)k2 exp- kwk22 /2dwdt
≤ [0,1]	kA0 - Akop kwk2 kVf (A(t)w)k2 exp - kwk22 /2 dwdt.
(90)
(91)
(92)
Note that the operator norm is bounded by the infinity norm (up to a multiplicity constant) and
kwk2 kVf (A(t)w)k2 is exponentially bounded. There is a constant a (hidden in .) and b such that
the above is bounded by
[0,1]	kA0 - Ak∞ exp (b kA(t)k∞ kwk2) exp - kwk22 /2 dwdt
.kA0 - Ak∞ [0 1J exP (b√R kwk2 - kwk2 ∕2) dwdt
.kA0-Ak∞
.kΣ0-Σk∞.
Here we have applied the facts
kA0 — Ak∞ . k∑ — ∑0k∞ and kA(t)k∞ ≤ √R∙
(93)
(94)
(95)
(96)
(97)
3.	Chebyshev’s inequality implies
P (In X φ(xi) φ (yi)-T∞ (∑) >ε)	(98)
≤ SVar (φ (Xi) φ (yi)) ≤ —2 Elφ (Xi) φ (yi)|2	(99)
n2 n2
≤-ɪC4 (φ, R) → 0 as n → ∞.	(100)
□
31
Published as a conference paper at ICLR 2019
Remark E.4. In practice the 1/n decay bound obtained by Chebyshev’s inequality in Equation 100
is often too weak to be useful. However, if φ is linearly bounded, then one can obtain an exponential
decay bound via the following concentration inequality:
Lemma E.8. If ∣φ(x)∣ ≤ a + b|x| a.e., then there is an absolute constant c > 0 and a constant
κ = κ(a, b, R) > 0 such that property 3 (Equation 48) holds with
ρn(φ, , R)
2 exp
-c min
(101)
Proof. We postpone the proof of the following claim.
ClaimE.9. Assume ∣φ(x)∣ ≤ a+b|x|. Then there is a K = κ(a, b, R) such that for all Σ ∈ PSD2(R)
and all p ≥ 1,
(E(x,y)〜N(o,∑)lφ(X)0(y)|p)1/p ≤ κP∙	(102)
Claim E.9 and the triangle inequality imply
(E(x,y)〜N(0,∑)lφ(X)0(y) - Eφ(χ)φ(y)|p)1/p ≤ 2κp∙	(103)
We can apply Bernstein-type inequality (Vershynin, 2010, Lemma 5.16) to conclude that there is a
c > 0 such that for every Σ ∈ PSD2(R) and any {(xi, yi)}：=、i.i.d.〜N (0, Σ)
P ŋn Xφ(Xi)φ(yi) - T∞(∑)
>)≤ 2exp (-cmin {金，S'}) ∙
(104)
It remains to prove Claim E.9. For p ≥ 1,
(E(x,y)〜N(0,∑)IΦ(χ)Φ(y)lp)1∕p ≤ (Ex〜N(o,∑ιι)IΦ(χ产)1/2p (Ey〜N(0,∑22)IΦ(y)l2p)1∕2p W)
≤ (a + b (E∣x∣2p) 1∕2p) (a + b (E∣y∣2p) 1∕2p)	(106)
≤ (a + b√R (Eu〜N(0,i)|u|2p)1/2p)2	(107)
≤ (a + b√R (c02ppp) 1/2p) 2	(108)
≤ (a + bc02 √r) P	(109)
≡κp.	(110)
We applied Cauchy-Schwarz’ inequality in the first inequality, the triangle inequality in the second
one, the fact Σ11, Σ22 ≤ R in the third one, absolute moments estimate of standard Gaussian in the
fourth one, where c0 is a constant such that
(Eu〜N(0,1)|u|p严 ≤ c0√p.	(111)
□
F Glossary
We use the following shorthands in this work:
1.	NN - neural network;
2.	CNN - convolutional neural network;
3.	LCN - locally connected network, a.k.a. convolutional network without weight sharing;
4.	FCN - fully connected network, a.k.a. multilayer perceptron (MLP);
5.	GP - Gaussian process;
6.	X-GP - a GP equivalent to a Bayesian infinitely wide neural network of architecture X (§2).
32
Published as a conference paper at ICLR 2019
7.	MC-(X-)-GP - a Monte Carlo estimate (§4) of the X-GP.
8.	Width, (number of) filters, (number of) channels represent the same property for CNNs and
LCNs.
9.	Pooling - referring to architectures as “with” or “without pooling” means having a single
global average pooling layer (collapsing the spatial dimensions of the activations yL+1)
before the final linear FC layer giving the regression outputs zL+1 .
10.	Invariance and equivariance are always discussed w.r.t. translations in the spatial dimen-
sions of the inputs.
G	Experimental Setup
Throughout this work we only consider 3 × 3 (possibly unshared) convolutional filters with stride 1
and no dilation.
All inputs are normalized to have zero mean and unit variance, i.e. lie on the d-dimensional sphere
of radius √d, where d is the total dimensionality of the input.
All labels are treated as regression targets with zero mean. i.e. for a single-class classification
problem with C classes targets are C-dimensional vectors with -1/C and (C - 1)/C entries in
incorrect and correct class indices respectively.
If a subset of a full dataset is considered for computational reasons, it is randomly selected in a
balanced fashion, i.e. with each class having an equal number of samples. No data augmentation is
used.
All experiments were implemented in Tensorflow (Abadi et al., 2016) and executed with the help of
Vizier (Golovin et al., 2017).
All neural networks are trained using Adam (Kingma & Ba, 2015) minimizing the mean squared
error loss.
G. 1 Many-channel CNNs and LCNs
Relevant Figures: 6 (b), 8, 9.
We use a training and validation subsets of CIFAR10 of sizes 500 and 4000 respectively. All images
are bilinearly downsampled to 8 × 8 pixels.
All models have 3 hidden layers with an erf nonlinearity. No (valid) padding is used.
Weight and bias variances are set to σω2 ≈ 1.7562 and σb2 ≈ 0.1841, corresponding to the pre-
activation variance fixed point q* = 1 (Poole et al., 2016) for the erf nonlinearity.
NN training proceeds for 219 gradient updates, but aborts ifno progress on training loss is observed
for the last 100 epochs. If the training loss does not reduce by at least 10-4 for 20 epochs, the
learning rate is divided by 10.
All computations are done with 32-bit precision.
The following NN parameters are considered:10
1.	Architecture: CNN or LCN.
2.	Pooling: no pooling or a single global average pooling (averaging over spatial dimensions)
before the final FC layer.
3.	Number of channels: 2k for k from 0 to 12.
4.	Initial learning rate: 10-k for k from 0 to 15.
5.	Weight decay: 0 and 10-k for k from 0 to 8.
6.	Batch size: 10, 25, 50, 100, 200.
10Due to time and memory limitations certain large configurations could not be evaluated. We believe this
did not impact the results of this work in a qualitative way.
33
Published as a conference paper at ICLR 2019
For NNs, all models are filtered to only 100%-accurate ones on the training set and then for each
configuration of {architecture, pooling, number of channels} the model with the lowest validation
loss is selected among the configurations of {learning rate, weight decay, batch size}.
For GPs, the same CNN-GP is plotted against CNN and LCN networks without pooling. For
LCN with pooling, inference was done with an appropriately rescaled CNN-GP kernel, i.e.
Kv∞ec - σb2 /d + σb2 , where d is the spatial size of the penultimate layer. For CNNs with pool-
ing, a Monte Carlo estimate was computed (see §4) with n = 212 filters and M = 26 samples.
For GP inference, the initial diagonal regularization term applied to the training convariance matrix
is 10-10; if the cholesky decompisition fails, the regularization term is increased by a factor of 10
until it either succeeeds or reaches the value of 105, at which point the trial is considered to have
failed.
G.2 Monte Carlo Evaluation of Intractable GP Kernels
Relevant Figures: 5, 10.
We use the same setup as in §G.1, but training and validation sets of sizes 2000 and 4000 respectively.
For MC-GPs we consider the number of channels n (width in FCN setting) and number of NN
instantiations M to accept values of 2k for k from 0 to 10.
Kernel distance is computed as:
kK∞ - Kn,M kF
kK∞kF
(112)
where K∞ is substituted with K210,210 for the CNN-GP pooling case (due to impracticality of com-
puting the exact K∞pool). GPs are regularized in the same fashion as in §G.1, but the regularization
factor starts at 10-4 and ends at 1010 and is multiplied by the mean of the training covariance diag-
onal.
G.3 Transforming a GP over spatial locations into a GP over classes
Relevant Figure: 4.
We use the same setup as in §G.2, but rescale the input images to size of 31 × 31, so that at depth
15 the spatial dimension collapses to a 1 × 1 patch if no padding is used (hence the curve of the
CNN-GP without padding halting at that depth).
For MC-CNN-GP with pooling, we use samples of networks with n = 16 filters. Due to computa-
tional complexity we only consider depths up to 31 for this architecture. The number of samples M
was selected independently for each depth among 2k for k from 0 to 15 to maximize the valida-
tion accuracy on a separate 500-points validation set. This allowed us to avoid the poor conditioning
of the kernel. GPs are regularized in the same fashion as in §G.1, but for MLP-GP the multiplicative
factor starts at 10-4 and ends at 1010.
G.4 Relationship to Deep Signal Propagation
Relevant Figure: 11.
We use a training and validation subsets of CIFAR10 of sizes 500 and 1000 respectively.
We use the erf nonlinearity. For CNN-GP, images are zero-padded (same padding) to maintain the
spatial shape of the activations as they are propagated through the network.
Weight and bias variances (horizontal axis σω2 and vertical axis σb2 respectively) are sampled from a
uniform grid of size 50 × 50 on the range [0.1, 5] × [0, 2] including the endpoints.
All computations are done with 64-bit precision. GPs are regularized in the same fashion as in §G.1,
but the regularization factor is multiplied by the mean of the training covariance diagonal. If the
experiment fails due to numerical reasons, 0.1 (random chance) validation accuracy is reported.
34
Published as a conference paper at ICLR 2019
G.5 CNN-GP on full datasets
Relevant Figures 6 (a, c), 7 and Table: 1.
We use full training, validation, and test sets of sizes 50000, 10000, and 10000 respectively for
MNIST (LeCun et al., 1998) and Fashion-MNIST (Xiao et al., 2017a), 45000, 5000, and 10000 for
CIFAR10 (Krizhevsky, 2009). We use validation accuracy to select the best configuration for each
model (we do not retrain on validation sets).
GPs are computed with 64-bit precision, and NNs are trained with 32-bit precision. GPs are regu-
larized in the same fashion as in §G.4.
Zero-padding (same) is used.
The following parameters are considered:
1.	Architecture: CNN or FCN.
2.	Nonlinearity: erf or ReLU.
3.	Depth: 2k for k from 0 to 4 (and up to 25 for MNIST and Fashion-MNIST datasets).
4.	Weight and bias variances. For erf: q* from {0.1,1,2,..., 8}. For ReLU: a fixed weight
variance σω2 = 2 + 4e-16 and bias variance σb2 from {0.1, 1, 2, . . . , 8}.
On CIFAR10, we additionally train NNs for 218 gradient updates with a batch size of 128 with
corresponding parameters in addition to11
1.	Pooling: no pooling or a single global average pooling (averaging over spatial dimensions)
before the final FC layer (only for CNNs).
2.	Number of channels or width: 2k for k from 1 to 9 (and up to 210 for CNNs with pooling
in Figure 6, a).
3.	Learning rate: 10-k × 216/ (width × q*) for k from 5 to 9, where width is substituted with
the number of channels for CNNs and q* is substituted with σb2 for ReLU networks. “Small
learning rate” in Table 1 refers to k ∈ {8, 9}.
4.	Weight decay: 0 and 10-k for k from 0 to 5.
For NNs, all models are filtered to only 100%-accurate ones on the training set (expect for values
in parentheses in Table 1). The reported values are then reported for models that achieve the best
validation accuracy.
G.6 Model comparison on CIFAR 1 0
Relevant Figure: 1.
We use the complete CIFAR10 dataset as described in §G.5 and consider 8-layer ReLU models with
weight and bias variances of σω2 = 2 and σb2 = 0.01. The number of channels / width is set to 25,
210 and 212 for LCN, CNN, and FCN respectively.
GPs are computed with 64-bit precision, and NNs are trained with 32-bit precision.
No padding (valid) is used.
NN training proceeds for 218 gradient updates with batch size 64, but aborts if no progress on
training loss is observed for the last 10 epochs. If the training loss does not reduce by at least 10-4
for 2 epochs, the learning rate is divided by 10.
Values for NNs are reported for the best validation accuracy over different learning rates (10-k
for k from 2 to 12) and weight decay values (0 and 10-k for k from 2 to 7). For GPs, validation
accuracy is maximized over initial diagonal regularization terms applied to the training convariance
matrix: 10-k × [mean of the diagonal] for k among 2, 4 and 9 (if the cholesky decompisition fails,
the regularization term is increased by a factor of 10 until it succeeds or k reaches the value of 10).
11Due to time and compute limitations certain large configurations could not be evaluated. We believe this
did not impact the results of this work in a qualitative way.
35