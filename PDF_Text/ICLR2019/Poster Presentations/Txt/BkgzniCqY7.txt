Published as a conference paper at ICLR 2019
Structured Adversarial Attack:
Towards General Implementation and Better
Interpretability
KaidiXuI * SijiaLiu2* PuZhaoI Pin-Yu Chen2 HuanZhang3 QuanfuFan2
Deniz Erdogmus1 Yanzhi Wang1 Xue Lin1
1 Northeastern University, USA
2MIT-IBM Watson AI Lab, IBM Research, USA
3University of California, Los Angeles, USA
Abstract
When generating adversarial examples to attack deep neural networks (DNNs),
`p norm of the added perturbation is usually used to measure the similarity be-
tween original image and adversarial example. However, such adversarial at-
tacks perturbing the raw input spaces may fail to capture structural informa-
tion hidden in the input. This work develops a more general attack model, i.e.,
the structured attack (StrAttack), which explores group sparsity in adversarial
perturbations by sliding a mask through images aiming for extracting key spa-
tial structures. An ADMM (alternating direction method of multipliers)-based
framework is proposed that can split the original problem into a sequence of an-
alytically solvable subproblems and can be generalized to implement other at-
tacking methods. Strong group sparsity is achieved in adversarial perturbations
even with the same level of `p-norm distortion (p ∈ {1, 2, ∞}) as the state-
of-the-art attacks. We demonstrate the effectiveness of StrAttack by extensive
experimental results on MNIST, CIFAR-10 and ImageNet. We also show that
StrAttack provides better interpretability (i.e., better correspondence with dis-
criminative image regions) through adversarial saliency map (Papernot et al.,
2016b) and class activation map (Zhou et al., 2016). Our code is available at
https://github.com/KaidiXu/StrAttack.
1 Introduction
Deep learning achieves exceptional suc-
cesses in domains such as image recog-
nition (He et al., 2016; Geifman & El-
Yaniv, 2017), natural language processing
(Hinton et al., 2012; Harwath et al., 2016),
medical diagnostics (Chen et al., 2016; Shi
et al., 2018) and advanced control (Sil-
ver et al., 2016; Fu et al., 2017). Re-
cent studies (Szegedy et al., 2013; Good-
fellow et al., 2014; Nguyen et al., 2015;
Kurakin et al., 2016; Carlini & Wagner,
2017) show that DNNs are vulnerable to
adversarial attacks implemented by gen-
erating adversarial examples, i.e., adding
well-designed perturbations to original le-
gal inputs. Delicately crafted adversarial
examples can mislead a DNN to recog-
nize them as any target image label, while
the perturbations appears unnoticeable to
human eyes. Adversarial attacks against
* Equal contribution
Figure 1: Group sparsity demonstrated in adversarial pertur-
bations obtained by C&W attack and our StrAttack, where
‘ostrich’ is the original label, and ‘unicycle’ is the mis-
classified label. Here each group is a region of 13×13×3 pix-
els and the strength of adversarial perturbations (through their
`2 norm) at each group is represented by heatmap. C&W at-
tack perturbs almost all groups, while StrAttack yields strong
group sparsity, with more semantic structure: the perturbed
image region matches the feature of the target object, namely,
the frame of the unicycle.
1
Published as a conference paper at ICLR 2019
DNNs not only exist in theoretical models
but also pose potential security threats to the real world (Kurakin et al., 2016; Evtimov et al., 2017;
Papernot et al., 2017). Several explanations are proposed to illustrate why there exist adversarial ex-
amples to DNNs based on hypotheses such as model linearity and data manifold (Goodfellow et al.,
2014; Gilmer et al., 2018). However, little is known to their origins, and convincing explanations
remain to be explored.
Besides achieving the goal of (targeted) mis-classification, an adversarial example should be as
“similar” to the original legal input as possible to be stealthy. Currently, the similarity is measured
by the `p norm (p = 0, 1, 2, ∞) of the added perturbation (Szegedy et al., 2013; Carlini & Wagner,
2017; Chen et al., 2017b;a), i.e., `p norm is being minimized when generating adversarial example.
However, measuring the similarity between the original image and its adversarial example by `p
norm is neither necessary nor sufficient (Sharif et al., 2018). Besides, no single measure can be
perfect for human perceptual similarity (Carlini & Wagner, 2017) and such adversarial attacks may
fail to capture key information hidden in the input such as spatial structure or distribution. Spurred
by that, this work implements a new attack model i.e., structured attack (StrAttack) that imposes
group sparsity on adversarial perturbations by extracting structures from the inputs. As shown in
Fig. 1, we find that StrAttack identifies minimally sufficient regions that make attacks successful,
but without incurring extra pixel-level perturbation power. The major contributions are summarized
as below.
•	(Structure-driven attack) This work is the first attempt towards exploring group-wise
sparse structures when implementing adversarial attacks, but without losing `p distortion
performance when compared to state-of-the-art attacking methods.
•	(Generality) We show that the proposed attack model covers many norm-ball based attacks
such as C&W (Carlini & Wagner, 2017) and EAD (Chen et al., 2017a).
•	(Efficient implementation) We develop an efficient algorithm to generate structured adver-
sarial perturbations by leveraging the alternating direction method of multipliers (ADMM).
We show that ADMM splits the original complex problem into subproblems, each of which
can be solved analytically. Besides, we show that ADMM can further be used to refine an
arbitrary adversarial attack under the fixed sparse structure.
•	(Interpretability) The generated adversarial perturbations demonstrate clear correlations
and interpretations between original and target images. With the aid of adversarial saliency
map (Papernot et al., 2016b) and class activation map (Zhou et al., 2016), we show that the
obtained group-sparse adversarial patterns better shed light on the mechanisms of adver-
sarial perturbations to fool DNNs.
Related work Many works studied norm-ball constrained adversarial attacks. For example, FGM
(Goodfellow et al., 2014) and IFGSM (Kurakin et al., 2017) attack methods were proposed to max-
imize the classification error subject to '∞-norm based distortion constraints. Moreover, L-BFGS
(Szegedy et al., 2013) and C&W (Carlini & Wagner, 2017) attacks found an adversarial example
by minimizing its '2-norm distortion. By contrast, JSMA (Papernot et al., 2016b) and one-pixel
(Su et al., 2017) attacks attempted to generate adversarial examples by perturbing the minimum
number of pixels, namely, minimizing the `0 norm of adversarial perturbations. Different from the
above norm-ball constrained attacks, some works (Karmon et al., 2018; Brown et al., 2017) crafted
adversarial examples by adding noise patches. However, the resulting adversarial perturbations are
no longer imperceptible to humans. Here we argue that imperceptibility could be important since it
helps us to understand how/why DNNs are vulnerable to adversarial attacks while perturbing natural
examples just by indistinguished adversarial noise.
In the aforementioned norm-ball constrained adversarial attacks, two extremely opposite principles
have been applied: C&W attack (or '∞ attacks) seeks the minimum image-level distortion but al-
lows to modify all pixels; one-pixel attack only perturbs a few pixels but suffers a high pixel-level
distortion. Both attacking principles might lead to a high noise visibility due to perturbing too many
pixels or perturbing a few pixels too much. In this work, we wonder if there exists a more effective
attack that can be as successful as existing attacks but achieves a tradeoff between the perturbation
power and the number of perturbed pixels. We will show that the proposed StrAttack is able to iden-
tify sparse perturbed regions that make attacks successful, but without incurring extra pixel-level
2
Published as a conference paper at ICLR 2019
perturbations. It is also worth mentioning that one-pixel attack has much lower attack success rate
on ImageNet than C&W attack and StrAttack.
In addition to adversarial attacks, many defense works have been proposed. Examples include de-
fensive distillation (Papernot et al., 2016c) that distills the original DNN and introduces temperature
into the softmax layer, random mask (Anonymous, 2019) that modifies the DNN structures by ran-
domly removing certain neurons before training, adversarial training through enlarging the training
dataset with adversarial examples, and robust adversarial training (Madry et al., 2017; Sinha et al.,
2018) through the min-max optimization. It is commonly known that the robust adversarial train-
ing method ensures the strongest defense performance against adversarial attacks on MNIST and
CIFAR-10. In this work, we will evaluate the effectiveness of StrAttack to three defense methods, a)
defensive distillation (PaPemot et al., 2016c), b) adversarial training via data augmentation (Tramer
et al., 2018) and c) robust adversarial training (Madry et al., 2017).
Although the adversarial attack and defense have attracted an increasing amount of attention, the
visual exPlanation on adversarial Perturbations is less exPlored since the distortion Power is mini-
mized and the resulting adversarial effects become imPercePtible to humans. The work (Dong et al.,
2017) attemPted to understand how the internal rePresentations of DNNs are affected by adversar-
ial examPles. However, only an ensemble-based attack was considered, which fails to distinguish
the effectiveness of different norm-ball constrained adversarial attacks. Unlike (Dong et al., 2017),
we emPloy the interPretability tools, adversarial saliency maP (ASM) (PaPernot et al., 2016b) and
class activation maP (CAM) (Zhou et al., 2016) to measure the effectiveness of different attacks in
terms of their interPretability. Here ASM Provides sensitivity analysis for Pixel-level Perturbation’s
imPact on label classification, and CAM localizes class-sPecific image discriminative regions (Xiao
et al., 2018). We will show that the sParse adversarial Pattern obtained by StrAttack offers a great
interPretability through ASM and CAM comPared with other norm-ball constrained attacks.
2	Structured Attack: Explore group structures from images
In the section, we introduce the concePt of StrAttack, motivated by the question: ‘what Possible
structures could adversarial Perturbations have to fool DNNs?’ Our idea is to divide an image into
sub-grouPs of Pixels and then Penalize the corresPonding grouP-wise sParsity. The resulting sParse
grouPs encode minimally sufficient adversarial effects on local structures of natural images.
Let ∆ ∈ RW×H×C be an adversarial Perturbation added to an original image X0, where W × H
gives the sPatial region, and C is the dePth, e.g., C = 3 for RGB images. To characterize the local
structures of ∆, we introduce a sliding mask M with stride S and size r × r × C. When S = 1,
the mask moves one Pixel at a time; When S = 2, the mask jumPs 2 Pixels at a time while sliding.
By adjusting the stride S and the mask size r, different grouP sPlitting schemes can be obtained.
If S < r, the resulting grouPs will contain overlapping Pixels. By contrast, grouPs will become
non-overlapped when S = r.
A sliding mask M finally divides ∆ into a set of grouPs {∆Gp,q } for p ∈ [P] and q ∈ [Q], where
P = (W - r)/S + 1, Q = (H - r)/S + 1, and [n] denotes the integer set {1, 2, . . . , n}. Given
the grouPs {∆Gp,q}, the grouP sParsity can be characterized through the following sParsity-inducing
function (Yuan & Lin, 2006; Bach et al., 2012; Liu et al., 2015), motivated by the Problem of grouP
Lasso (Yuan & Lin, 2006):
g(∆) = PpP=1 PqQ=1 k∆Gp,qk2,	(1)
where ∆°p,q denotes the set of pixels of ∆ indexed by Gp,q, and k ∙ k2 is the '2 norm. We refer
readers to Fig. A1 for an illustrative examPle of our concePts on grouPs and grouP sParsity.
3	Structured Adversarial Attack with ADMM
In this section, we start by proposing a general framework to generate prediction-evasive adversarial
examples, where the adversary relies only on gradients of the loss function with respect to inputs
of DNNs. Our model takes into account both commonly-used adversarial distortion metrics and
the proposed group-sparsity regularization that encodes spatial structures in attacks. We show that
the process of generating structured adversarial examples leads to an optimization problem that is
3
Published as a conference paper at ICLR 2019
difficult to solve using the existing optimizers Adam (for C&W attack) and FISTA (for EAD attack)
(Carlini & Wagner, 2017; Chen et al., 2017a). To circumvent this challenge, we develop an efficient
optimization method via alternating direction method of multipliers (ADMM).
Given an original image x0 ∈ Rn , we aim to design the optimal adversarial perturbation δ ∈ Rn so
that the adversarial example (x0 + δ) misleads DNNs trained on natural images. Throughout this
paper, we use vector representations of the adversarial perturbation ∆ and the original image X0
without loss of generality. A well designed perturbation δ can be obtained by solving optimization
problems of the following form,
minimize	f(x0 + δ, t) + γD(δ) + τg(δ)
δ
subject to	(x0 + δ) ∈ [0, 1]n, kδk∞ ≤ ,
(2)
where f(x, t) denotes the loss function for crafting adversarial example given a target class t, D(δ)
is a distortion function that controls the perceptual similarity between a natural image and a per-
turbed image, g(δ) = PP=I PQ=I |四0,, ∣g is given by (1), and ∣∣ ∙ kp signifies the 'p norm. In
problem (2), the ‘hard’ constraints ensure the validness of created adversarial examples with -
tolerant perturbed pixel values. And the non-negative regularization parameters γ and τ place our
emphasis on the distortion of an adversarial example (to an original image) and group sparsity of
adversarial perturbation. Tuning the regularization parameters will be discussed in Appendix F.
Problem (2) gives a quite general formulation for design of adversarial examples. If we remove the
group-sparsity regularizer g(δ) and the '∞ constraint, problem (2) becomes the same as the C&W
attack (Carlini & Wagner, 2017). More specifically, if we further set the distortion function D(δ)
to the form of 'o, '2 or '∞ norm, then We obtain C&W '0, '2 or '∞ attack. If D(δ) is specified
by the elastic-net regularizer, then problem (2) becomes the formulation of EAD attack (Chen et al.,
2017a).
In this paper, We specify the loss function of problem (2) as beloW, Which yields the best knoWn
performance of adversaries (Carlini & Wagner, 2017),
f (xo + δ, t) = C ∙ max{max Z (x0 + δj — Z (x0 + δ)t, 一κ},
j6=t
(3)
Where Z(x)j is the jth element of logits Z(x), representing the output before the last softmax layer
in DNNs, and κ is a confidence parameter that is usually set to zero if the attack transferability is not
much cared. We choose D(δ) = ∣δ∣22 for a fair comparison With the C&W '2 adversarial attack.
In this section, we assume that {Gp,q} are non-overlapping groups, i.e., Gp,q ∩ Gp，© = 0 for q = q0
or p 6= p0 . The overlapping case Will be studied in the next section.
The presence of multiple non-smooth regularizers and ‘hard’ constraints make the existing optimiz-
ers Adam and FISTA (Carlini & Wagner, 2017; Chen et al., 2017a; Kingma & Ba, 2015; Beck &
Teboulle, 2009) inefficient for solving problem (2). First, the subgradient of the objective function
of problem (2) is difficult to obtain especially when {Gp,q} are overlapping groups. Second, it is
impossible to compute the proximal operations required for FISTA with respect to all non-smooth
regularizers and ‘hard’ constraints. Different from the existing work, we show that ADMM, a first-
order operator splitting method, helps us to split the original complex problem (2) into a sequence
of subproblems, each of which can be solved analytically.
We reformulate problem (2) in a way that lends itself to the application of ADMM,
minimize	f(z + x0) + γD(δ) + τ PiP=Q1 kyDi k2 + h(w)
δ,z,w,y
subject to z = δ, z = y, z = w,
(4)
where z, y and w are newly introduced variables, for ease of notation let D(q-1)P +p = Gp,q, and
h(w) is an indicator function with respect to the constraints of problem (2),
h(w) =	0	if (x0 +w) ∈ [0,1]n, kwk∞ ≤ ,	(5)
∞ otherwise.
ADMM is performed by minimizing the augmented Lagrangian of problem (4),
L(z, δ, y, w, u,v,s) = f(z + x0) + γD(δ) + τ PiP=Q1 kyDi k2 + h(w) + uT(δ - z)	(6)
+vτ(y - z) + ST(W - z) + 2l∣δ - zk2 + PIly - zk2 + PI∣w - zk2,	‘ J
4
Published as a conference paper at ICLR 2019
where u, v and s are Lagrangian multipliers, and ρ > 0 is a given penalty parameter. ADMM splits
all of optimization variables into two blocks and adopts the following iterative scheme,
{δk+1, wk+1, yk+1} = arg min L(δ, zk, w, y, uk, vk, sk),	(7)
δ,w,y
zk+1 = arg min L(δk+1, z, wk+1, yk+1, uk, vk, sk),	(8)
z
uk+1 = uk + ρ(δk+1 - zk+1),
vk+1 = vk + ρ(yk+1 - zk+1),	(9)
sk+1 = sk + ρ(wk+1 - zk+1),
where k is the iteration index, steps (7)-(8) are used for updating primal variables, and the last
step (9) is known as the dual update step. We emphasize that the crucial property of the proposed
ADMM approach is that, as we demonstrate in Proposition 1, the solution to problem (7) can be
found in parallel and exactly.
Proposition 1 When D(δ) = kδk22, the solution to problem (7) is given by
δk+1
P a
ρ+2γa,
(min{1 - [x0]i, } bi > min{1 - [x0]i, }
max{-[x0]i, -} bi < max{-[x0]i, -}
bi	otherwise,
[yk+1]Di = (1- Pki⅛ )+*i ,i ∈ [PQ],
for i ∈ [n],
(10)
(11)
(12)
where a := Zk — Uk/ρ, b := Zk — Sk/ρ, C := Zk — vk/ρ, (x)+ = X if X ≥ 0 and 0 otherwise, [x]i
denotes the ith element ofx, and [x]Di denotes the sub-vector ofx indexed by Di.
Proof: See Appendix B.

It is clear from Proposition 1 that introducing auxiliary variables does not increase the computational
complexity of ADMM since (10)-(12) can be solved in parallel. Moreover, if another distortion
metric (different from D(δ) = kδk22) is used, then ADMM only changes at the δ-step (10).
We next focus on the Z-minimization step (8), which can be equivalently transformed into
minimize f(xo + Z) + P∣∣z - a0∣∣2 + Pkz - b0∣∣2 + P∣∣z - c0∣∣2,	(13)
z	222
where a0 := δk+1 + uk /ρ, b0 := wk+1 + sk /ρ, and c0 := yk+1 + vk /ρ. We recall that attacks
studied in this paper belongs to ‘first-order’ adversaries (Madry et al., 2017), which only have access
to gradients of the loss function f. Spurred by that, we solve problem (13) via a linearization
technique that is commonly used in stochastic/online ADMM (Ouyang et al., 2013; Suzuki, 2013;
Liu et al., 2018) or linearized ADMM (Boyd et al., 2011; Liu et al., 2017). Specifically, we replace
the function f with its first-order Taylor expansion at the point Zk by adding a Bregman divergence
term (ηk/2)∣∣z — Zk ∣∣2. As a result, problem (13) becomes
minimize	(Vf (zk + XO))T (z — zk) + ηk ∣z — zk∣∣2 + P ∣∣z — a0∣∣2
z	22
+2 kz - b0k2 + 2 Ilz- c0k2,
(14)
where 1/nk > 0 is a given decaying parameter, e.g., ηk = α√k for some α > 0, and the Bregman
divergence term stabilizes the convergence of Z-minimization step. It is clear that problem (14)
yields a quadratic program with the closed-form solution
zk+1 = (1/ (ηk + 3P)) ηkzk + Pa +Pb +Pc — Vf (zk + x0) .	(15)
In summary, the proposed ADMM algorithm alternatively updates (7)-(9), which yield closed-form
solutions given by (10)-(12) and (15). The convergence of linearized ADMM for nonconvex op-
timization was recently proved by (Liu et al., 2017), and thus provides theoretical validity of our
approach. Compared to the existing solver for generation of adversarial examples (Carlini & Wag-
ner, 2017; Papernot et al., 2016b), our algorithm offers two main benefits, efficiency and generality.
That is, the computations for every update step are efficiently carried out, and our approach can be
applicable to a wide class of attack formulations.
5
Published as a conference paper at ICLR 2019
4	Overlapping Group and Refined StrAttack
In this section, we generalize our proposed ADMM solution framework to the case of generating
adversarial perturbations with overlapping group structures. We then turn to an attack refining model
under fixed sparse structures. We will show that both extensions can be unified under the ADMM
framework. In particular, the refined approach will allow us to gain deeper insights on the structural
effects on adversarial perturbations.
4.1	Overlapping group structure
We recall that groups {Di} (also denoted by {Gp,q}) studied in Sec. 3 could be overlapped with each
other; see an example in Fig. A1. Therefore, {Di} is in general a cover rather than a partition of [n].
To address the challenge in coupled group variables, we introduce multiple copies of the variable y
in problem (4), and achieve the following modification
minimize f(z+x0) + γD(δ) + τ PiP=Q1 kyi,Di k2 + h(w)
δ,z,w,{yi}	(16)
subject to z = δ, z = w, z = yi,	i ∈ [P Q],
where compared to problem (4), there exist PQ variables yi ∈ Rn for i ∈ [PQ], and yi,Di denotes
the subvector of yi with indices given by Di. It is clear from (16) that groups {Di} become non-
overlapped since each of them lies in a different copy yi. The ADMM algorithm for solving problem
(16) maintains a similar procedure as (7)-(9) except y-step (12) and z-step (15); see Proposition 2.
Proposition 2 Given the same condition of Proposition 1, the ADMM solution to problem (16) in-
volves the δ-step same as (10), the w-step same as (11), and two modified y- and z-steps,
I 卜:+]Di = (1 - ρk[ciTDi k2 + + [ci]Di , for i ∈ [PQ],
"，Di = [c「
zk+1 = (1/ (ηk +2ρ + PQρ)) (ηkZk + ρa0 + ρb0 + P PPQI ci — Pf(Zk + xo)),
(17)
(18)
where Ci := Zk — VklP, Vi is the Lagrangian multiplier associated with equality constraint Ni = Z,
similar to (9) we obtain vik+1 = vik + ρ(yk+1 - zk+1), [n]/Di denotes the difference of sets [n] and
Di, a0 and b0 have been defined in (13), and c0i = yik+1 + vik /ρ.
Proof: See Appendix C.

We note that updating PQ variables {yi} is decomposed as shown in (17). However, the side effect
is the need of PQ times more storage space than the y-step (12) when groups are non-overlapped.
4.2	Refined StrAttack under fixed sparse pattern
The approaches proposed in Sec. 3 and Sec. 4.1 help us to identify structured sparse patterns in
adversarial perturbations. This section presents a method to refine structured attacks under fixed
group sparse patterns. Let δ* denote the solution to problem (2) solved by the proposed ADMM
method. We define a σ-sparse perturbation δ via δ*,
δi = 0 if δ* ≤ σ, for any i ∈ [n],	(19)
where a hard thresholding operator is applied to δ* with tolerance σ. Our refined model imposes the
fixed σ-sparse structure (19) into problem (2). This leads to
minimize f(x0 + δ) + γD(δ)
δ
subject to	(x0 + δ) ∈ [0, 1]n , kδk∞ ≤
δi = 0, ifi ∈ Sσ,
(20)
where Sσ is defined by (19), i.e., Sσ := {j | δj ≤ σ, j ∈ [n]}. Compared to problem (2), the group-
sparse penalty function is eliminated as it has been known as a priori. With the priori knowledge of
group sparsity, problem (20) is formulated to optimize and refine the non-zero groups, thus achieving
better performance on highlighting and exploring the perturbation structure. Problem (20) can be
solved using ADMM, and its solution is presented in Proposition 3.
6
Published as a conference paper at ICLR 2019
Proposition 3 The ADMM solution to problem (20) is given by
i0	i ∈ Sσ
min{1 - [xo]i, e}	2γρ+ρai > min{1 - [χo]i ,e}, i ∈ Sσ
max{- [xo]i, -e}	2γρ+ρai < max{- [xo]i,-e},i / Sσ
2γρ+ρ ai	otherwise,
k+1	0	i ∈	Sσ
团」i —	[ 1∕(ηk + P)	[ηk[zk]i + ρ[a0]i -	[▽/(Zk +	xo)]i]	i /	Sσ,
(21)
(22)
for i ∈ [n], where Z = δ is the introduced auxiliary variable similar to (4), a := δk+1 一 uk∕ρ,
a0 := δk+1 + uk /ρ, uk+1 = uk + ρ(δk+1 - zk+1), andρ and ηk have been defined in (6) and (14).
The ADMM iterations can be initialized by δ*, the known solution to problem (2).
Proof: See Appendix D.
5 Empirical Performance of StrAttack
We evaluate the performance of the proposed StrAttack on three image classification datasets,
MNIST (Lecun et al., 1998), CIFAR-10 (Krizhevsky & Hinton, 2009) and ImageNet (Deng et al.,
2009). To make fair comparison with the C&W `2 attack (Carlini & Wagner, 2017), we use `2 norm
as the distortion function D(δ) = kδk22. And we also compare with FGM (Goodfellow et al., 2014)
and IFGSM `2 attacks (Kurakin et al., 2017) as a reference. We evaluate attack success rate (ASR)1
as well as `p distortion metrics for p ∈ {0, 1, 2, ∞}. The detailed experiment setup is presented in
Appendix F. Our code is available at https://github.com/KaidiXu/StrAttack.
For each attack method on MNIST or CIFAR-10, we choose 1000 original images from the test
dataset as source and each image has 9 target labels. So a total of 9000 adversarial examples are
generated for each attack method. On ImageNet, each attack method tries to craft 900 adverdarial
examples with 100 random images from the test dataset and 9 random target labels for each image.
Fig. 2 compares adversarial examples generated by StrAttack and C&W attack on each dataset.
We observe that the perturbation of the C&W attack has poor group sparsity, i.e., many non-zeros
groups with small magnitudes. However, the ASR of the C&W attack is quite sensitive to these
small perturbations. As applying a threshold to have the same `0 norm as our attack, we find that
only 6.7% of adversarial examples generated from C&W attack remain valid. By contrast, StrAttack
is able to highlight the most important group structures (local regions) of adversarial perturbations
without attacking other pixels. For example, StrAttack misclassifies a natural image (4 in MNIST) as
an incorrect label 3. That is because the pixels that appears in the structure of3 are more significantly
perturbed by our attack; see the top right plots of Fig. 2. Furthermore, the ‘goose-sorrel’ example
shows that misclassification occurs when we just perturb a small number of non-sparse group regions
on goose’s head, which is more consistent with human perception. We refer readers to Appendix G
for more results.
By quatitatively analysis, we report `p norms and ASR in Table 1 for p ∈ {0, 1, 2, ∞}. We show
that StrAttack perturbs much fewer pixels (smaller `0 norm), but it is comparable to or even better
than other attacks in terms of '1, '2, and '∞ norms. Specifically, the FGM attack yields the worst
performance in both ASR and `p distortion. On MNIST and CIFAR-10, StrAttack outperforms other
attacks in '0, '1 and '∞ distortion. On ImageNet, StrAttack outperforms C&W attack in '0 and '1
distortion. Since the C&W attacking loss directly penalizes the '2 norm, it often causes smaller '2
distortion than StrAttack. We also observe that the overlapping case leads to the adversarial per-
turbation of less sparsity (in terms of '0 norm) compared to the non-overlapping case. This is not
surprising, since the sparsity of the overlapping region is controlled by at least two groups. However,
compared to C&W attack, the use of overlapping groups in StrAttack still yields sparser perturba-
tions. Unless specified otherwise, we focus on the case of non-overlapping groups to generate the
most sparse adversarial perturbations. We highlight that although a so-called one-pixel attack (Su
et al., 2017) also yields very small '0 norm, it is at the cost of very large '∞ distortion. Unlike
one-pixel attack, StrAttack achieves the sparsity without losing the performance of '∞ , '1 and '2
distortion.
Furthermore, we compare the performance of StrAttack with the C&W '∞ attack and IFGSM while
attacking the robust model (Madry et al., 2017) on MNIST. We remark that all the considered attack
1The percentage of adversarial examples that successfully fool DNNs.
7
Published as a conference paper at ICLR 2019
Target: 3
Target: airplane
Target: sorrel
Original Image: 4
Original Image: horse
Original Image: goose
Target: 3
Target: airplane
Target: sorrel
Figure 2: C&W attack vs StrAttack. Here each grid cell represents a 2 × 2, 2 × 2, and 13 × 13 small region in
MNIST, CIFAR-10 and ImageNet, respectively. The group sparsity of perturbation is represented by heatmap.
The colors on heatmap represent average absolute value of distortion scale to [0, 255]. The left two columns
correspond to results of using C&W attack. The right two columns show results of StrAttack.
Table 1: Adversarial attack success rate (ASR) and `p distortion values for various attacks.
Data Set	I Attack I I Method I	I	BeSt Case*	∣					I	AVerage Case*	∣					I	WarSt Case*					
		PASR	`0	`1	`2	g∞ I	PASR	'o	'ι	'2		g∞ I	PASR	`0	`1	'2	'∞	
	FGM	99.3	456.5	28.2	2.32	0.57	35.8	466	39.4	3.17	0.717	0	N.A.**	N.A.	N.A.	N.A.
MNIST	IFGSM	100	549.5	18.3	1.57	0.4	100	588	30.9	2.41	0.566	99.8	640.4	50.98	3.742	0.784
	C&W	100	479.8	13.3	1.35	0.397	100	493.4	21.3	1.9	0.528	99.7	524.3	29.9	2.45	0.664
	StrAttack	100	73.2	10.9	1.51	0.384	100	119.4	18.05	2.16	0.47	100	182.0	26.9	2.81	0.5
	+overlap	100	84.4	9.2	1.32	0.401	100	157.4	16.2	1.95	0.508	100	260.9	22.9	2.501	0.653
	FGM	98.5	3049	12.9	0.389	0.046	44.1	3048	34.2	0.989	0.113	0.2	3071	61.3	1.76	0.194
CIFAR-10	IFGSM	100	3051	6.22	0.182	0.02	100	3051	13.7	0.391	0.0433	100	3060	22.9	0.655	0.075
	C&W	100	2954	6.03	0.178	0.019	100	2956	12.1	0.347	0.0364	99.9	3070	16.8	0.481	0.0536
	StrAttack	100	264	3.33	0.204	0.031	100	487	7.13	0.353	0.050	100	772	12.5	0.563	0.075
	+overlap	100	295	3.35	0.169	0.029	100	562	7.05	0.328	0.047	100	920	12.9	0.502	0.063
	FGM	12	264917	152	0.477	0.0157	2	263585	51.3	0.18	0.00614	0	N.A.	N.A.	N.A.	N.A.
ImageNet	IFGSM	100	267079	299.32	0.9086	0.02964	100	267293	723	2.2	0.0792	98	267581	1378	4.22	0.158
	C&W	100	267916	127	0.471	0.016	100	263140	198	0.679	0.03	100	265212	268	0.852	0.041
	StrAttack	100	14462	55.2	0.719	0.058	100	52328	152	1.06	0.075	100	80722	197	1.35	0.122
* Please refer to Appendix F for the definition of best case, best case and worst case.
** N.A. means not available in the case of zero ASR, +overlap means structured attack with overlapping groups.
methods are performed under the same '∞ -norm based distortion constraint with an upper bound
∈ {0.1, 0.2, 0.3, 0.4}. Here we obtain a (refined) StrAttack subject to kδk∞ ≤ by solving
problem (20) at γ = 0. In Table 2, we demonstrate the ASR and the number of perturbed pixels
for various attacks over 5000 (untargeted) adversarial examples. The ASR define as the proportion
of the final perturbation results less than given ∈ {0.1, 0.2, 0.3, 0.4} bound over number of test
images. Here an successful attack is defined by an attack that can fool DNNS and meets the '∞
distortion constraint. As we can see, StrAttack can achieve the similar ASR compared to other attack
methods, however, it perturbs a much less number of pixels. Next, we evaluate the performance
of StrAttack against two defense mechanisms: defensive distillation (Papernot et al., 2016c) and
adversarial training (Tramer et al., 2018). We observe that StrAttack is able to break the two defense
methods with 100% ASR. More details are provided in Appendix H.
Lastly, we evaluate the transferability of StrAttack from Inception V3 (Szegedy et al., 2016) to other
network models including Inception V2, Inception V4 (Szegedy et al., 2017), ResNet 50, ResNet
152 (He et al., 2016), DenseNet 121 and DenseNet 161 (Huang et al., 2017). For comparison,
we also present the transferbility of IFGSM and C&W. This experiment is performed under 1000
(target) adversarial examples on ImageNet2. It can be seen from in Table 3 that StrAttack yields the
largest attack success rate while transferring to almost every network model.
2We follow the experiment setting in (Su et al., 2018), where the transferability is evaluated by the target
class top-5 success rate at each transferred model.
8
Published as a conference paper at ICLR 2019
Table 2: Attack success rate (ASR) and `0 norm of adversarial perturbations for various attacks against robust
adversarial training based defense on MNIST.
	ASR at e = 0.1	ASR at e = 0.2	ASR at = 0.3	ASRat = 0.4	'0
IFGSM	001	0.02	0.09	0.94	^654^
C&W '∞ attack	0.01	0.02	0.10	0.96	723
StrAttack	0.01	0.02	0.10	0.99	279
Table 3: Comparison of transferability of different attacks over 6 ImageNet models.
	Incept V2	Incept V4	ResNet50	ResNet152	DenseNet121	DenseNet161
IFGSM	0.27	0.22	~^027~~	0.19	0.16	0.19
C&W	0.25	0.24	0.23	0.23	0.15	0.15
StrAttack	0.28	0.27	0.25	0.25	0.26	0.25
6	StrAttack Offers Better Interpretability
In this section, we evaluate the effects of structured adversarial perturbations on image classification
through adversarial saliency map (ASM) (Papernot et al., 2016b) and class activation map (CAM)
(Zhou et al., 2016). Here we recall that ASM measures the impact of pixel-level perturbations
on label classification, and CAM localizes class-specific image discriminative regions that we use
to visually explain adversarial perturbations (Xiao et al., 2018). We will show that compared to
C&W attack, StrAttack meets better interpretability in terms of (a) a higher ASM score and (b) a
tighter connection with CAM, where the metric (a) implies interpretability at a micro-level, namely,
perturbing pixels with largest impact on image classification, and the metric (b) demonstrates inter-
pretability at a macro-level, namely, perturbations can be mapped to the most discriminative image
regions localized by CAM.
Given an input image x0 and a target class t, let ASM(x0, t) ∈ Rd denote ASM scores for every
pixel of x0 corresponding to t. We elaborate on the mathematical definition of ASM in Appendix E.
Generally speaking, the ith element of ASM(x0, t), denoted by ASM(x0, t)[i], measures how much
the classification score with respect to the target label t will increase and that with respect to the
original label t0 will decrease if a perturbation is added to the pixel i. With the aid of ASM, we then
define a Boolean map BASM ∈ Rd to encode the regions ofx0 most sensitive to targeted adversarial
attacks, where BASM (i) = 1 if ASM(x0, t) > ν, and 0 otherwise. Here ν is a given threshold to
highlight the most sensitive pixels. we then define the interpretability score (IS) via ASM,
IS(δ) = kBASM ◦ δ∣∣2∕∣∣δ∣∣2,	(23)
where ◦ is the element-wise product. The rationale behind (23) is that IS(δ) → 1 if the sensitive
region identified by ASM perfectly predicts the locations of adversarial perturbations. By contrast,
ifIS(δ) → 0, then adversarial perturbations cannot be interpreted by ASM. In Fig. 3(a), we compare
IS of our proposed attack with C&W attack versus the threshold ν, valued by different percentiles of
ASM scores. We obsreve that our attack outperforms C&W attack in terms of IS, since the former
is able to extract important local structures of images by penalizing the group sparsity of adversarial
perturbations. It seems that our improvement is not significant. However, StrAttack just perturbs
very few pixels to obtain this benefit, leading to perturbations with more semantic structure; see
Fig. 3(b) for an illustrative example.
Besides ASM, we show that the effect of adversarial perturbations can be visually explained through
the class-specific discriminative image regions localized by CAM (Zhou et al., 2016). In Fig. 3(c),
we illustrate CAM and demonstrate the differences between our attack and C&W in terms of their
connections to the most discriminative regions of x0 with label t0 . We observe that the mechanism
of StrAttack can be better interpreted from CAM: only a few adversarial perturbations are needed
to suppress the feature of the original image with the true label. By replacing ASM with CAM, we
can similarly compute IS in (23) averaged over 500 examples on ImageNet, yielding 0.65 for C&W
attack and 0.77 for our attack. More examples of ASM and CAM can be viewed in Appendix E.
To better interpret the mechanism of adversarial examples, we study adversarial attacks on some
complex images, where the objects of the original and target labels exist simultaneously as shown in
Fig. 4. It can be visualized from CAM that both C&W attack and StrAttack yields similar adversarial
effects on natural images: Adversarial perturbations are used to suppress the most discriminative
9
Published as a conference paper at ICLR 2019
region with respect to the true label, and simultaneously promotes the discriminative region of the
target label. The former principle is implied by the location of perturbed regions and C(x0 , t0) in
Fig. 4, and the latter can be seen from C(xCW, t) or C (xStr, t) against C(x0, t). However, compared
to C&W attack, StrAttack perturbs much less but ‘right’ pixels which have better correspondence
with class-specific discriminative image regions localized by CAM.
0.3	-----------------------------
30 40 50 60 70 80 90
ν
(a)
(b)
(c)
Figure 3: Interpretabilicy comparison of StrAttack and C&W attack. (a) ASM-based IS vs ν, given from the
30th percentile to the 90th percentile of ASM scores. (b) Overlay ASM and BASM ◦ δ on top of image with
the true label ‘Tibetan Mastiff’ and the target label ‘streetcar’. From left to right: original image, ASM (darker
color represents larger value of ASM score), BASM ◦ δ under StrAttack, and BASM ◦ δ under C&W attack.
Here ν in BASM is set by the 90th percentile of ASM scores. (c) From left to right: original image with true
label ‘stove’, CAM of ‘stove’, and perturbations with target label ‘water ouzel’ under StrAttack and C&W.
7	Conclusion
This work explores group-wise sparse structures when implementing adversarial attacks. Different
from previous works that use `p norm to measure the similarity between an original image and an ad-
versarial example, this work incorporates group-sparsity regularization into the problem formulation
of generating adversarial examples and achieves strong group sparsity in the obtained adversarial
perturbations. Leveraging ADMM, we develop an efficient implementation to generate structured
adversarial perturbations, which can be further used to refine an arbitrary adversarial attack under
fixed group sparse structures. The proposed ADMM framewrok is general enough for implement-
ing many state-of-the-art attacks. We perform extensive experiments using MNIST, CIFAR-10 and
ImageNet datasets, showing that our structured adversarial attack (StrAttack) is much stronger than
the existing attacks and its better interpretability from group sparse structures aids in uncovering the
origins of adversarial examples.
Acknowledgement
This work is supported by Air Force Research Laboratory FA8750-18-2-0058, and U.S. Office of
Naval Research. Sijia Liu, Pin-Yu Chen, Huan Zhang and Quanfu Fan were supported by the MIT-
IBM Watson Ai Lab, IBM Research.
References
Anonymous. Random mask: Towards robust convolutional neural networks. In Submitted to In-
ternational Conference on Learning Representations, 2019. URL https://openreview.
net/forum?id=SkgkJn05YX. under review.
F. Bach, R. Jenatton, J. Mairal, and G. Obozinski. Optimization with sparsity-inducing penalties.
Foundations and Trends® in Machine Learning, 4(1):1-106, 2012.
A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse prob-
lems. SIAM journal on imaging sciences, 2(1):183-202, 2009.
S. Boyd, N. Parikh, E. Chu, B. Peleato, J. Eckstein, et al. Distributed optimization and statistical
learning via the alternating direction method of multipliers. Foundations and TrendsR in Machine
Learning, 3(1):1-122, 2011.
10
Published as a conference paper at ICLR 2019
to： holster, t: revolver
to： pug, t: street sign
Figure 4: CAMs of adversarial examples generated by the C&W attack and StrAttack under the original label
(t0) and target label (t). Here xCW and xStr denote adversarial examples crafted by different attacks. At each
row, the subplots from left to right represent the original image x0 , perturbations generated by C&W attack,
perturbations generated by StrAttack, and CAMs with respect to natural or adversarial example under t0 or t.
Here the class c specified CAM with respect to image x is denoted by C(x, c).
Tom B Brown, Dandelion Man6, AUrko Roy, Martin Abadi, and Justin Gilmer. Adversarial patch.
arXiv preprint arXiv:1712.09665, 2017.
N. Carlini and D. Wagner. Towards evaluating the robustness of neural networks. In Security and
Privacy (SP), 2017 IEEE SymPosium on,pp. 39-57. IEEE, 2017.
J. Chen, L. Yang, Y. Zhang, M. Alber, and D. Chen. Combining fully convolutional and recur-
rent neural networks for 3d biomedical image segmentation. In Advances in Neural Information
Processing Systems, pp. 3036-3044, 2016.
P.-Y. Chen, Y. Sharma, H. Zhang, J. Yi, and C.-J. Hsieh. Ead: elastic-net attacks to deep neural
networks via adversarial examples. arXiv PrePrint arXiv:1709.04114, 2017a.
P.-Y. Chen, H. Zhang, Y. Sharma, J. Yi, and C.-J. Hsieh. Zoo: Zeroth order optimization based
black-box attacks to deep neural networks without training substitute models. In Proceedings of
the 10th ACM WorkshoP on Artificial Intelligence and Security, pp. 15-26. ACM, 2017b.
J. Deng, W. Dong, R. Socher, L. Li, K. Li, and F.-F. Li. Imagenet: A large-scale hierarchical image
database. In ComPuter Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on,
pp. 248-255. IEEE, 2009.
Y. Dong, H. Su, J. Zhu, and F. Bao. Towards interpretable deep neural networks by leveraging
adversarial examples. arXiv PrePrint arXiv:1708.05493, 2017.
I.	Evtimov, K. Eykholt, E. Fernandes, T. Kohno, B. Li, A. Prakash, A. Rahmati, and D. Song. Robust
physical-world attacks on machine learning models. arXiv PrePrint arXiv:1707.08945, 2017.
J.	Fu, J. Co-Reyes, and S. Levine. Ex2: Exploration with exemplar models for deep reinforcement
learning. In Advances in Neural Information Processing Systems, pp. 2574-2584, 2017.
Y. Geifman and R. El-Yaniv. Selective classification for deep neural networks. In Advances in neural
information Processing systems, pp. 4885-4894, 2017.
11
Published as a conference paper at ICLR 2019
J. Gilmer, L. Metz, F. Faghri, S. Schoenholz, M. Raghu, M. Wattenberg, and I. Goodfellow. Adver-
sarial spheres. arXiv preprint arXiv:1801.02774, 2018.
I. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. arXiv
preprint arXiv:1412.6572, 2014.
D. Harwath, A. Torralba, and J. Glass. Unsupervised learning of spoken language with visual con-
text. In Advances in Neural Information Processing Systems, pp. 1858-1866, 2016.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings
of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.
G. Hinton, L. Deng, D. Yu, G. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen,
T. Sainath, et al. Deep neural networks for acoustic modeling in speech recognition: The shared
views of four research groups. IEEE Signal Processing Magazine, 29(6):82-97, 2012.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In CVPR, volume 1, pp. 3, 2017.
D. Karmon, D. Zoran, and Y. Goldberg. Lavan: Localized and visible adversarial noise. arXiv
preprint arXiv:1801.02608, 2018.
D. Kingma and J. Ba. Adam: A method for stochastic optimization. 2015 ICLR, arXiv preprint
arXiv:1412.6980, 2015. URL http://arxiv.org/abs/1412.6980.
A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Master’s
thesis, Department of Computer Science, University of Toronto, 2009.
A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial examples in the physical world. arXiv
preprint arXiv:1607.02533, 2016.
A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial machine learning at scale. 2017 ICLR, arXiv
preprint arXiv:1611.01236, 2017. URL http://arxiv.org/abs/1611.01236.
Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278-2324, Nov 1998. ISSN 0018-9219. doi:
10.1109/5.726791.
Q. Liu, X. Shen, and Y. Gu. Linearized admm for non-convex non-smooth optimization with con-
vergence analysis. arXiv preprint arXiv:1705.02502, 2017.
S. Liu, S. Kar, M. Fardad, and P. K. Varshney. Sparsity-aware sensor collaboration for linear coherent
estimation. IEEE Transactions on Signal Processing, 63(10):2582-2596, 2015.
S. Liu, J. Chen, P.-Y. Chen, and A. O. Hero. Zeroth-order online admm: Convergence analysis and
applications. In Proceedings of the Twenty-First International Conference on Artificial Intelli-
gence and Statistics, volume 84, pp. 288-297, April 2018.
A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models
resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.
A. Nguyen, J. Yosinski, and J. Clune. Deep neural networks are easily fooled: High confidence
predictions for unrecognizable images. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 427-436, 2015.
H. Ouyang, N. He, L. Tran, and A. Gray. Stochastic alternating direction method of multipliers. In
International Conference on Machine Learning, pp. 80-88, 2013.
N. Papernot, I. Goodfellow, R. Sheatsley, R. Feinman, and P. McDaniel. cleverhans v1.0.0: an
adversarial machine learning library. arXiv preprint arXiv:1610.00768, 2016a.
N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, B. Celik, and A. Swami. The limitations of
deep learning in adversarial settings. In Security and Privacy (EuroS&P), 2016 IEEE European
Symposium on, pp. 372-387. IEEE, 2016b.
12
Published as a conference paper at ICLR 2019
N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami. Distillation as a defense to adversarial
perturbations against deep neural networks. In Security and Privacy (SP), 2016 IEEE Symposium
on,pp. 582-597. IEEE, 2016c.
N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. Celik, and A. Swami. Practical black-box attacks
against machine learning. In Proceedings of the 2017 ACM on Asia Conference on Computer and
Communications Security, pp. 506-519. ACM, 2017.
N. Parikh, S. Boyd, et al. Proximal algorithms. Foundations and TrendsR in Optimization, 1(3):
127-239, 2014.
M. Sharif, L. Bauer, and M. K. Reiter. On the suitability of lp-norms for creating and preventing
adversarial examples. CoRR, abs/1802.09653, 2018.
X. Shi, M. Sapkota, F. Xing, F. Liu, L. Cui, and L. Yang. Pairwise based deep ranking hash-
ing for histopathology image classification and retrieval. Pattern Recognition, 81:14 - 22,
2018. ISSN 0031-3203. doi: https://doi.org/10.1016/j.patcog.2018.03.015. URL http:
//www.sciencedirect.com/science/article/pii/S0031320318301055.
D. Silver, A. Huang, C. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser,
I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural
networks and tree search. nature, 529(7587):484-489, 2016.
A. Sinha, H. Namkoong, and J. Duchi. Certifying some distributional robustness with principled
adversarial training. 2018.
Dong Su, Huan Zhang, Hongge Chen, Jinfeng Yi, Pin-Yu Chen, and Yupeng Gao. Is robustness
the cost of accuracy?-a comprehensive study on the robustness of 18 deep image classification
models. arXiv preprint arXiv:1808.01688, 2018.
J. Su, D. Vargas, and S. Kouichi. One pixel attack for fooling deep neural networks. arXiv preprint
arXiv:1710.08864, 2017.
T. Suzuki. Dual averaging and proximal gradient descent for online alternating direction multiplier
method. In International Conference on Machine Learning, pp. 392-400, 2013.
C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing
properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architec-
ture for computer vision. 2016 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 2818-2826, 2016.
Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi. Inception-v4,
inception-resnet and the impact of residual connections on learning. In AAAI, volume 4, pp.
12, 2017.
F. Tramer, A. Kurakin, N. Papernot, I. Goodfellow, D. Boneh, and P. McDaniel. Ensemble adver-
sarial training: Attacks and defenses. 2018 ICLR, arXiv preprint arXiv:1705.07204, 2018.
C. Xiao, J. Zhu, B. Li, W. He, M. Liu, and D. Song. Spatially transformed adversarial examples.
CoRR, abs/1801.02612, 2018. URL http://arxiv.org/abs/1801.02612.
M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 68(1):49-67, 2006.
B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Learning deep features for discrim-
inative localization. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 2921-2929, 2016.
13
Published as a conference paper at ICLR 2019
Appendix
A Illustrative Example of Group Sparsity
non-overlapping groups
(1,1)	(1,2)	(1,3)	(1,4)
(2,1)	(2,2)	I(^	
(3,1)	(3,2)	(3,3)	(3,4)
(4,1)	(4,2)	(4,3)	(4,4)
H δ G1,J2 = JZ A2
Wi, j 阉J
overlapping groups
ASgJ∣2 = Z	∆ 2	V
(i∣ (i，j)⅛,2
(1^(1,2)		(1,3)	(1,4)
幽	(2,2)	(2,3)	(2,4)
(3,1)	(3,2)	(3,3)	(3,4)
(4,1)	(4,2)	(4,3)	(4,4)
Figure A1: An example of 4 × 4 perturbation matrix under sliding masks with different strides. The values of
matrix elements are represented by color’s intensity (white stands for 0). Left: Non-overlapping groups with
r = 2 and S = 2. Right: Overlapping groups with r = 2 and S = 1. In both cases, two groups G1,1 and G1,2
are highlighted, where G1,1 is non-sparse, and G1,2 is sparse.
B Proof of Proposition 1
We recall that the augmented Lagrangian function L(δ, z, w, y, u, v, s) is given by
L(z,δ,y,w,u,v,s) =f(z +x0) + γD(δ) +τ PiP=Q1 kyDi k2 + h(w) + uT(δ - z)
+ VT (y -Z) + ST (W —Z) + 2kδ — zk2 + 2ky - zk2 + p kw — zk2.	(24)
Problem (7), to minimize L(δ, zk , w, y, uk , vk , sk), can be decomposed into three sub-problems:
minimize γD(δ) + Pkδ — ak2,	(25)
δ2
minimize h(w) + Pkw — bk2,	(26)
w2
PQ
minimize T £['0/2 + Pky — ck2,	(27)
i=1
where a := Zk — uk/P, b := Zk — sk/P, and c := Zk — Vk/P.
δ-step Suppose D(δ) = kδk22, then the solution to problem (25) is easily acquired as below
δ k+1 = -~ρ- a
P+2γ
w-step Based on the definition of h(w), problem (26) becomes
minimize kw — bk22
w
subject to (x0 + w) ∈ [0, 1]n, kwk∞ ≤ .
Problem (29) is equivalent to
minimize (wi — ai)22
wi
subject to —[x0]i ≤ wi ≤ 1 — [x0]i, |wi| ≤
for i ∈ [n], where xi or [x]i represents the ith element of x, and 1 — [x0]i > 0 since [x0]i ∈ [0, 1].
Problem (30) then yields the solution
min{1 — [x0]i, } ai > min{1 — [x0]i, }
[wk+1]i =	max{—[x0]i, —} ai < max{—[x0]i, —}	(31)
ai	otherwise.
(28)
(29)
(30)
14
Published as a conference paper at ICLR 2019
y-step Problem (27) becomes
PQ
minimize X IIyDik2 + 2TI∣y - ck2,	(32)
i=1
The solution is given by the proximal operator associated with the '2 norm with parameter τ∕ρ
(Parikh et al., 2014)
[yk+1]Di =(1 - ,∣∕ II ) [c]Di, i ∈ [PQ],	(33)
ρI[c]Di I2 +
where recall that ∪i∈[pQ]Di = [n], and Di ∩ Dj = 0 if i = j.	□
C	Proof of Proposition 2
The augmented Lagrangian of problem (16) is given by
PQ
L(z, δ, w, {yi}, u, vi, s) =f(z+x0) + γD(δ) + h(w) +τXIyi,DiI2 +uT(δ -z) +sT(w -z)
i=1
PQ	PQ
+ X VT(yi - Z) + 2kδ - zk2 + 2 kw - zk2 + 2 X kyi - zk2,
i=1	i=1
(34)
where u, vi and s are the Lagrangian multipliers.
ADMM decomposes the optimization variables into two blocks and adopts the following iterative
scheme,
{δk+1, wk+1, yik+1} = arg min L(zk, δ, w, yi, uk, vik, sk),	(35)
δ,w,{yi}
zk+1 = arg min L(z, δk+1, wk+1, yik+1, uk, vik, sk),	(36)
z
(uk+1 = uk + ρ(δk+1 - zk + 1),
vik+1 = vik + ρ(yik+1 - zk+1), fori∈ [P Q],	(37)
[sk+1 = sk + ρ(wk+1 - zk+1),
where k is the iteration index. Problem (35) can be split into three subproblems as shown below,
minδηize γD(δ) + P∣∣δ — a∣2,	(38)
minimize h(w) + P∣∣w 一 b∣2,	(39)
w2
minimize T∣M,Dik2 + P∣∣yi - ci∣2, for i ∈ [PQ].	(40)
yi	2
where a = ζk — uk∕ρ, b = ζk — sk/ρ and Ci = zk — Vk/ρ. Each problem has a closed form
solution. Note that the solutions to problem (38) and problem (39) are given (28) and (31).
yi -step Problem (40) can be rewritten as
minymize τ∣yi,Dik2 + ρ∣yi,Di - NDik2 + ρ∣yi,[n]/Di - [Ci][η]∕Di k2, for i ∈ [PQ],	(41)
which can be decomposed into
minimize T∣∣yi,Di∣2 + 2∣∣yi,Di - [Ci]Dik2, for i ∈ [PQ],	(42)
yi,Di
15
Published as a conference paper at ICLR 2019
and
minimize kyi,[n]/Di - [ci][n]/Dik22, fori ∈ [P Q].	(43)
yi,[n]/Di
The solution to problem (42) can be obtained through the block soft thresholding operator (Parikh
et al., 2014),
[yk+1]Di = (1- ρk[c⅛i72)+ NDi，fori ∈ [PQ]，	(44)
The solution to problem (43) is given by,
yik+1[n]/Di = [ci][n]/Di, fori ∈ [PQ].	(45)
z-step Problem (36) can be simplified to
PQ
minimize f(XO + Z) + PIlz - a0k2 + PIlz - b0k2 + P EkZ - c'ik2,	(46)
z	222
i=1
where a0 := δk+1 + uk /P, b0 := wk+1 + sk/P, and c0i := yik+1 + vik/P. We solve problem
(46) using the linearization technique (Suzuki, 2013; Liu et al., 2018; Boyd et al., 2011). More
specifically, the function f is replaced with its first-order Taylor expansion at the point zk by adding
a Bregman divergence term (ηk∕2)∣ζ — zk^. As a result, problem (46) becomes
minimize
z
(Vf (zk + XO))T(z - zk) + η2k∣z - zk∣2 + P∣z - a0k2
PQ
+2kz - b0k2+2 χ Iiz - cik2,
i=1
(47)
whose solution is given by
k+ι = η zk + Pa + Pb + P P=Q Ci — Vf(Zk + XO)	(48)
=	ηk + (2 + PQ)P	.	(8)
D Proof of Proposition 3
We start by converting problem (20) into the ADMM form
minimize f(XO + z) + g(z) + γD(δ) + h(δ) + g(δ)
δ,z	(49)
subject to δ = z,
where z and δ are optimization variables, g(δ) is an indicator function with respect to the constraint
{δi = 0, if i ∈ Sσ}, and h(δ) is the other indicator function with respect to the other constraints
(XO+δ) ∈ [0, 1]n, IδI∞ ≤.
The augmented Lagrangian of problem (20) is given by
L(δ, z, u) =f (z + xο) + g(z) + γD(δ) + h(δ) + g(δ) + UT(δ - z) + P ∣∣δ - z∣2,	(50)
where u is the Lagrangian multiplier.
ADMM yields the following alternating steps
δk+1 = arg min L(δ, zk, Uk)	(51)
δ
zk+1 = arg min L(δk+1, z, Uk)	(52)
z
Uk+1 = Uk + P(δk+1 - zk+1).	(53)
16
Published as a conference paper at ICLR 2019
δ-step Suppose D(δ) = kδk22, problem (51) becomes
minimize γkδk2 + 2kδ - ak2
δ
subject to (x0 + δ) ∈ [0, 1]n, kδk∞ ≤
δi =0, ifi ∈ Sσ,
where a := Zk - Uk/ρ. Problem (54) can be decomposed elementwise
2
minimize	CYPPδi - 2aiδi + a2 = γpρ (δi - 2γ+ρai)
subject to ([xo]i + δi) ∈ [0,1], ∣δi∣≤ E
δi = 0, ifi ∈ Sσ.
(54)
(55)
The solution to problem (55) is then given by
[δk+1]i
0
min{1 - [X0]i ,E}
max{- [X0]i , -E}
2Cp+P ai
i ∈Sσ
2γp+Pai > min{1 - [x0]i , e}, i ∈ Sσ
2γp+Pai < max{- [x0]i , -e}, i ∈ Sσ
otherwise.
(56)
z-step Problem (52) yields
minimize
z
subject to
f (XO + Z) + Pkz - a0k2
zi
0, if i ∈ Sσ ,
(57)
where a0 = δk+1 + Uk/ρ. We solve problem (57) using the linearization technique (Suzuki, 2013;
Liu et al., 2018; Boyd et al., 2011),
minimize	(Vf(XO+zk))T(Z- zk)+ηkkz-zkk2+ρkz-a0k2
subject to zi = 0, if i ∈ Sσ ,
(58)
where ηk is a decaying parameter associated with the Bregman divergence term kZ - Zkk22. In
problems (57) and (58), only variables {zi} satisfying i ∈/ Sσ are unknown. The solution to problem
(58) is then given by
k+1	0	i ∈ Sσ
[z ]i = § ηk [zk]i+p[a0]i-[Vf(zk+xo)]i i ∈ s	(59)
I	ηk+p	σ σ.
E Adversarial Saliency Map (ASM) and Class Activation
Mapping (CAM)
ASM(X, t) ∈ Rd is defined by the forward derivative of a neural network given the input sample X
and the target label t (Papernot et al., 2016b)
ASM(X, t)[i]
otherwise,
if * < 0 or Pj=t¾j > 0
(60)
where Z(X)j is the jth element of logits Z(X), representing the output before the last softmax layer
in DNNs. If there exist many classes in a dataset (e.g., 1000 classes in ImageNet), then computing
Pj=t d⅛j is intensive. To circumvent the scalability issue of ASM, We focus on the logit change
with respect to the true label tO and the target label t only. More specifically, we consider three
quantities, dZXX)t, - dζ∂χx)0, and (dZ∂Xx)t) ∣P j=t dZXxj], which correspond to a) promotion of the
score of the target label t, b) suppression of the classification score of the true label tO, and c) a dual
role on suppression and promotion. As a result, we modify (60) as
ASM(X, t)[i]
dZ(X)to
∂Xi
if dZ(Xt < 0 or dζ∂X)t0 > 0
Xi	Xi
otherwise.
(61)
17
Published as a conference paper at ICLR 2019
CAM allows us to visualize the perturbation of adversaries on predicted class scores given any pair
of image and object label, and highlights the discriminative object regions detected by CNNs (Zhou
et al., 2016). In Fig. A2, we show ASM and the discriminative regions identified by CAM on several
ImageNet samples.
Original label: Komdo dragon
Target label: tailed frog
Original label: Africa elephant
Target label: dining table
Original label: titi
Target label: gondola
Original label: jeep
Target label: ballon
Original label: stove
Target label: ouzel
(a)
Original label: photocopier
Target label: accordion
(b)
Figure A2: (a) Overlay ASM and BASM ◦ δ on top of image with the true and the target label. From left to
right: original image, ASM (darker color represents larger value of ASM score), BASM ◦ δ under our attack,
and BASM ◦ δ under C&W attack. Here V in BASM is set by the 90th percentile of ASM scores. (b) From left to
right: original image, CAM of original label, and perturbations with target label generated from the StrAttack
and C&W attack, respectively.
F	Experiment Setup and Parameter Setting
In this work, we consider targeted adversarial attacks since they are believed stronger than untargeted
attacks. For targeted attacks, we have different methods to choose the target labels. The average case
selects the target label randomly among all the labels that are not the correct label. The best case
performs attacks using all incorrect labels, and report the target label that is the least difficult to
attack. The worst case performs attacks using all incorrect labels, and report the target label which
is the most difficult to attack.
In our experiments, two networks are trained for MNIST and CIFAR-10, respectively, and a pre-
trained network is utilized for ImageNet. The model architectures for MNIST and CIFAR-10 are the
same, both with four convolutional layers, two max pooling layers, two fully connected layers and a
softmax layer. It can achieve 99.5% and 80% accuracy on MNIST and CIFAR-10, respectively. For
ImageNet, a pre-trained Inception v3 network (Szegedy et al., 2016) is applied which can achieve
96% top-5 accuracy. All experiments are conducted on machines with NVIDIA GTX 1080 TI GPUs.
The implementations of FGM and IFGM are based on the CleverHans package (Papernot et al.,
2016a). The key distortion parameter is determined by a fine-grained grid search. For IFGM, we
perform 10 FGM iterations and the distortion parameter 0 is set to /10 for effectiveness as shown
in Tramer et al. (2018). The implementation of the C&W attack is based on the opensource code
provided by Carlini & Wagner (2017). The maximum iteration number is set to 1000 and it has 9
binary search steps.
In the StrAttack, the group size for MNIST and CIFAR-10 is 2 × 2 and its stride is set to 2 if the
non-overlapping mask is used, otherwise the group size is 3 × 3 and stride is 2. The group size
for ImageNet is 13 × 13 and its stride is set to 13. In ADMM, the parameter ρ achieves a trade-off
between the convergence rate and the convergence value. A larger ρ could make ADMM converging
faster but usually leads to perturbations with larger `p distortion values. A proper configuration of
the parameters is suggested as follows: We set the penalty parameter ρ = 1, decaying parameter
in (14) η1 = 5, τ = 2 and γ = 1. Moreover, we set c defined in (3) to 0.5 for MNIST, 0.25 for
CIFAR-10, and 2.5 for ImageNet. Refined attack technique proposed in Sec. 4.2 is applied for all
18
Published as a conference paper at ICLR 2019
experiments, We set σ is equal to 3% quantile value of non-zero perturbation in δ*. We observe that
73% of δ* can be retrained to a σ-sparse perturbation successfully which proof the effective of our
refined attack step.
G Supplementary Experimental Results
C&W attack
structured attack
Target: 2
Target: 3
Original Image: 1
Target: 3
Target: 0
Target: 2
Original Image: 4
Target: 0
Target: 9
Original Image: 7
Target: 2
Target: 9
Figure A3: C&W attack vs StrAttack on MNIST with grid size 2 × 2.
δ
7
Some random choice samples from MNIST (Fig. A3), CIFAR-10 (Fig. A4) and ImageNet (Fig. A5)
compare StrAttack with C&W attack. For better sparse visual effect, we only show non-overlapping
mask function results here. From these samples, we can discover a consistent phenomenon that our
StrAttack is more interested in some particular regions, they usually appear on the objects or their
edges in original images, distinctly seen in MNIST (Fig. A3) and ImageNet (Fig. A5).
H	StrAttack Against Defensive Distillation and Adversarial
Training
In this section, we present the performance of the StrAttack against defensive distillation (Papernot
et al., 2016C) and adversarial training (Tramer et al., 2018). In defensive distillation, we evaluate the
19
Published as a conference paper at ICLR 2019
structured attack
Target:
automobile
airplane
bird
deer
C&W attack
Original Image:
airplane
ship
truck
cat
Target:
automobile
airplane
bird
deer
Figure A4: C&W attack vs StrAttack on CIFAR-10 with grid size 2 × 2.
StrAttack for different temperature parameters on MNIST and CIFAR-10. We generate 9000 ad-
versarial examples with 1000 randomly selected images from MNIST and CIFAR-10, respectively.
The attack success rates of the StrAttack for different temperatures T are all 100%. The reason is
that distillation at temperature T makes the logits approximately T times larger but does not change
the relative values of logits. The StrAttack which works on the relative values of logits does not fail.
We further use the StrAttack to break DNNs training on adversarial examples (Tramer et al., 2018)
with their correct labels on MNIST. The StrAttack is performed on three neural networks: the first
network is unprotected, the second is obtained by retraining with 9000 C&W adversarial examples,
and the third network is retained with 9000 adversarial examples crafted by the StrAttack. The
success rate and distortions on the three networks are shown in Table A1. The StrAttack can break
all three networks with 100% success rate. However, adversarial training shows certain defense
effects as an increase on the `1 or `2 distortion on the latter two networks over the unprotected
network is observed.
Table A1: StrAttack against adversarial training on MNIST
Best case
Adversarial
training
None
C&W
structured
ASR	'1	'2
100	10.9	1.51
100	16.1	1.87
100	15.6	1.86
Average case
ASR	'	'2Γ
Taa~18.05~2TΓ6
100	25.1	2.58
1aa	25.1	2.61
Worst case
ASR	'1	'2
1aa	26.9	2.81
1aa	34.2	3.26
1aa	34.6	3.31
2a
Published as a conference paper at ICLR 2019
C&W attack
structured attack
Target:
television
Original Image:
black swan
boathouse
wombat
miniature schnauzer
EntleBucher
jellyfish
flatworm
leaf beetle
brambling
Target:
television
boathouse
miniature schnauzer
EntleBucher
leaf beetle
Figure A5: C&W attack vs StrAttack on ImageNet with grid size 13 × 13.
21