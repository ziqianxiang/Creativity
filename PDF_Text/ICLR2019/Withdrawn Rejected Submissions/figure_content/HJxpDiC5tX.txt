Figure 1: The full visual speech recognition system introduced by this work consists of a dataprocessing pipeline that generates lip and phoneme clips from YouTube videos (see Section 3), and ascalable deep neural network for phoneme recognition combined with a production-grade word-leveldecoding module used for inference (see Section 4).
Figure 2: Left: A comparison of sentence-level (word sequence) visual speech recognition datasets.
Figure 3: Example homo-phone issue when modellingcharacters with CTC.
Figure 4: This heatmap shows which insertion and deletion errors were most common on the test set.
Figure 5: Saliency map for “kind of" and the top-3 predictions of each frame. The CTC blankcharacter is represented by'； The unaligned ground truth phoneme sequence is /k aIndV v/.
Figure 6: V2P could be helpful for performing silent speech recognition for those with aphonia2.
Figure 7: Phoneme confusion matrix for V2P, estimated by computing the edit distance alignmentbetween each predicted sequence of phonemes and the corresponding ground-truth, and counting thecorrect phonemes and the substitutions. The diagonal values are scaled downwards to de-emphasizethe correct phonemes. Blue indicates more substitutions occurred.
Figure 8: Heatmap showing the performance of V2P on different head rotations. Tilt and pan axesare in degrees. As shown, it performs similarly at all pan and tilt angles in [-30°, 30°], the range atwhich it was trained.
Figure 9: Saliency maps, the top-3 predictions of each frame and the ground truth phonemes.
Figure 10: Random sample of test-set lip images from LSVSR. This illustrates the substantial diversityin our dataset.
