Figure 1: Coupled autoencoder architecture (CAE)3 Bootstrapping W14 to W28, W28 to W14. The appendix describes an architecture in which webootstrap, starting with a W14 learned in experiment 1 or 2 and learn a W28.
Figure 2: Classification network6Under review as a conference paper at ICLR 2019rotation. The data set for which these errors are reported is MNIST-rot. We choose[10, 5, 5, 5, 5, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1] as our multiplicities for the ai’s (hy-perparameters) and [8, 4, 4, 4, 4, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1] for the bi’s (hyperparame-ters). Figure 3 and Figure 4 shows graphs of the error functions for W28 constructed inAE, CAE architectures as a function of the number of samples given to learn the W28 . Evenwhen the number of samples is as small as 50, we discover CW-bases which are good at rotationand reconstruction in both architectures. In Figure 7, in the Appendix we visualize a few basiselements learned.
Figure 3: MSE - Reconstruction (MNIST-rot) Figure 4: MSE - Rotation of images (MNIST-rot)No.ofSamples2.	Results on Classification, MNIST In Figure 5 we plot the accuracy of these various W28 ob-tained above when deployed for classification. We plot the accuracy of the classifier as a functionof the number of samples used to construct the CW-bases. When the number of samples is aslow as 50 a CAE-W28 performs better than an AE-W28. Beyond 100 samples the difference isinsignificant. Plots using a bootstrapping CW-basis are similar to that of the AE and omitted.
Figure 5: Classification accuracy (MNIST-rot)Coupling interchangeability To test how coupled the CAE-W28 and CAE-W14 are, we did thefollowing experiment - the classifier trained in row 4 with CAE-W28 was presented with downsized 14x14 images for classification. No additional training was done. Instead we use the tophalf of the coupling network from Figure 1 - given a test image y We compute y = WT4y usingthe CAE-W14. Then ψ((y 0 y)㊉ y) is fed to the trained classifier of row 4. These results arereported in row 5 as [28/14 Tensor].
Figure 6: Autoencoder architecture (AE).
Figure 7: Basis for 28 x 28 images learned from MNIST-rot using AEEl	1 , ∙ 1 ∙ ∙ , ∙	∙ -χ`τ^	rrʌ ι rʌl 1 ,1 ,	/'i'τ^∖	1	Γ<^> -1 -ι T ɪ τ	7	CT	λThe multiplicities in Y are a = [2, 1, 2] and that of φ(Y ) are b = [0, 1, 1]. Hence da = 8, db = 4and φ with arbitrary chosen values is given below.
