Figure 1: (A) GAIA network architecture. (B) An example AE latent distribution (purple) whichis non-convex. Black circles represent two projections from X to Z . The solid line between themrepresents an interpolation, where the red region contains points in the interpolation that do notcorrespond to the true data distribution. The dashed line is an interpolation which passes throughonly the true data distribution. One hypothesis is that training will warp the distribution on the leftto produce linearly interpolations within the distribution. (C) An alternative hypothesis is that bytraining the network upon interpolations, samples in the interpolated regions (lighter purple) will betrained to generate data similar to the x, without manipulating the distribution of z. (D) Pairwise in-terpolations between samples of x in GAIA will not necessarily make the latent distribution convex,because two point interpolations in Z do not reach all of the points in between the interpolated datazint. .
Figure 2: GAIA vs. AE and VAE on reconstruction on the S dataset. GAIA is shown both withthe local-structure (ll=1) loss and without (ll=0). Results for the other four 2D datasets are shown inFigure 6. (Top) Interpolations in latent space Gd(zint.) in red, plotted over true distribution plottedas blue heat-map. (Second row) A mesh-grid showing translation from a uniform sampling of theconvex hull of z reconstructed as Gd(Z). (Third row) A mesh-grid showing translation from theconvex hull of x reconstructed as G(X). (Bottom) The probability distribution of representationsfor each network, showing that the Ldist term promotes the preservation of structure from X toG(X).
Figure 3: Interpolations between autoencoded images in our network. The farthest left and rightcolumns correspond to the input images, and the middle columns correspond to a linear interpolationin latent-space.
Figure 4: Attribute vectors added to Z representation of different images from the CELEBA-HQdataset.
Figure 5: Data reconstructions from a subset of bidirectional GAN network architectures. Input im-ages (x), and network reconstruction images (G(x)) are shown side by side, with inputs on the left.
Figure 6: The same plots as in Figure 2, with the remaining four datasets.
Figure 7: Attribute vectors passed through the decode of the network. Attributes on the top arefound by subtracting means. Attributes on the bottom are found using ordinary least-squares regres-sion. The attribute vectors on the bottom are more variable (for example, see Male vs Goatee vsMoustache in both panels). Zoom-in to see labels.
