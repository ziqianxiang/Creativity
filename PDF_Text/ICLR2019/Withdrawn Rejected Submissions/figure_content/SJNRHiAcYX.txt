Figure 1: Analyzing normalizing flows vs. Gaussian: (a)(b) Consider a 2D Gaussian distribution with zeromean and factorized variance σ2 = 0.12. Samples from the GaUssian form an empirical distribution ∏o (reddots in (b)) and define the KL ball B(Π°, e) = {π : KL[∏o ∣∣π] ≤ e} centered at ∏o. Find a typical normalizingflows distribution and a Gaussian distribution at the boundary of B(π, 0.01) such that the constraint is tight.
Figure 2: Expressiveness of normalizing flows policy: (a) Bandit problem with reward r(a) = -aTΣ-1a. Themaximum entropy optimal policy is a Gaussian distribution with Σ as its covariance (red contours). normalizingflows policy (blue) can capture such covariance while Gaussian cannot (green). (b) Bandit problem withmultimodal reward (red contours the reward landscape). normalizing flows policy can capture the multimodality(blue) while Gaussian cannot (green). (c) Trajectories of Ant robot. The trajectories of Gaussian policy centerat the initial position (the origin [0.0, 0.0]), while trajectories of normalizing flows policy are much morewidespread.
Figure 3: MuJoCo Benchmark: learning curves on MuJoCo locomotion tasks. Tasks with (L) are from rllab.
Figure 4: Roboschool Humanoid Benchmark : learning curves on Roboschool Humanoid locomotion tasks.
Figure 5: MuJoCo and Roboschool Benchmarks : learning curves on locomotion tasks for ACKTR. Each curveis averaged over 3 random seeds and shows mean ± std performance. Each curve corresponds to a differentpolicy representation (Red: Normalizing flows (labelled as implicit), Blue: Gaussian). Vertical axis is thecumulative rewards and horizontal axis is the number of time steps. Tasks with (R) are from Roboschool.
Figure 6: Sensitivity to Hyper-parameters: quantile plots of policies’ performance on MuJoCo benchmark tasksunder various hyper-parameter settings. For each plot, we randomly generate 30 hyper-parameters for the policyand train for a fixed number of time steps. Reacher for 106 steps, Hopper and HalfCheetah for 2 ∙ 106 steps andSimpleHumanoid for ≈ 5 ∙ 106 steps. normalizing flows policy is in general more robust to Gaussian policy.
Figure 7: Sensitivity to normalizing flows Hyper-parameters: training curves of normalizing flows policy underdifferent hyper-parameter settings (number of hidden units l1 and number of transformation K , on Reacher andHopper task. Each training curve is averaged over 5 random seeds and we show the mean ± std performance.
Figure 8: Illustration of benchmark tasks in OpenAI MuJoCo (Brockman et al., 2016; Todorov, 2008), rllab(top line) (Duan et al., 2016) and Roboschool (bottom line) (Schulman et al., 2017).
