Figure 1: Training Procedure: Network fψ for opponent modeling takes state s as input and outputsthe prediction value Vθ(s), where θ is the private type for current opponent. Network gφ for policylearning takes state S and 论(S) as input and out put the policy. Both networks are train via MAML.
Figure 2: The results for chasing games. (a) is the initialization state for chasing games. Player 1 isthe red grid and player 2 is the green one. Each player 2 has a private goal which is unseen on themap. (b) shows the probability that player chooses its goal. (c) shows the average rewards player 1gets during meta-testing process. MO, MA and MOA initialize their parameters with results frommeta-training process. New opponents are used for testing.
Figure 3: The results for blocking games. (a) is the initialization state. Player 1 is the red one andplayer 2 is the green one. Each player 2 choose one path to get to the top space and player 1 triesto block it. (b) shows the average rewards player 1 gets during the meta-training process. (c) showsthe average rewards player 1 gets during meta-testing process. MO, MA and MOA initialize theirparameters with results from meta-training process. New opponents are used for testing.
Figure 4: The results for recommending games. (a) is the initialization state. Player 1 is the redgrid and player 2 is the green one. Blue grids are goals for player 2 and purple grids are objects forplayer 1. Each player 2 choose one of goals and player 1 tries to recommend a corresponding object.
