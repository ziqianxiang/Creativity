Figure 1: The architecture illustration of the proposed model. In the upper part (Bag-of-WordsGenerator), the Gbow takes discrete one-hot topic code c and continuous noise vector z as input,and generates bag-of-words. The Dbow discriminates its input bag-of-words is from Gbow or fromhuman-written text. The topic model Q predicts the latent topic of input bag-of-words. The noisepredictor predicts the noise of input bag-of-words. In the lower part (Word Sequence Generator), thesequence generator Gseq takes bag-of-words from Gbow as input and generates text. After trainingGbow and Gseq separately, we train upper part and lower part jointly. During joint training, weadd an extra sequential text discriminator Dseq which takes text and bag-of-words as input anddiscriminates the input bag-of-words and text pair is real or fake.
