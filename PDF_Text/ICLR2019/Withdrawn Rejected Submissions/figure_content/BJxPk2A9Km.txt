Figure 1:^↑Θ↑Λ一⅞↑N↑A,93x自 一 t∕3τθ↑æ^↑- - ↑A(b) Spatial LEMNSl	e2	e3	e4	e5(c) Spatio-Temporal LEMNɛl		e3	e4	e5Illustration of memory-retention architectures3.1	Memory-Retention MechanismsIn this section, we describe three different memory-retention mechanisms in detail. We first encodeinput xt and each memory cell mt,i to a vector representation as follows:ct = ψ(xt)et,i = φ(mt,i)where Ct, et,i ∈ Rd, ψ(∙) can be an input embedding similar to RNN controller in (Gulcehre et al.,2016) or position encoding in (Sukhbaatar et al., 2015) and 夕(∙)can be a memory encoding layer ofthe base external memory neural network. For example, MemN2N (Sukhbaatar et al., 2015) uses abag of words and embedding matrices to convert to memory representation.
Figure 3: Visualization of MQN retention agent’s status: (a-b) I-Maze and (c-d) Single Goal with Indicator.
Figure 4: Example of (a) Original task, (b) Noisy task and (c) Large task. Sentences in green are noisesentences and ones in blue are supporting facts of each question.
Figure 5: Examples of memory state of IM-LEMN and ST-LEMN on the Large task. Idx denotes index ofsentence in one episode. Sentences in blue are supporting facts, and ones in green are irrelevant ones.
Figure 6: (a) An example of TriviaQA dataset, (b) A visualization of how our model operates in order to solvethe problem. LEMN sequentially reads the sentence one by one while scheduling the memory to retain onlythe important sentences. During the process in (b), it retained the word ‘Italy’(Thick blue) in order to answera given question, by generating high retention value(Thick blue) on the memory cell containing ‘Italy’, anddecides to erase uninformative memory cells with low retention value (Thick red / * Mark).
