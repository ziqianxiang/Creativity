Figure 1: A toy example of invariant structures. The periodic and modulated structures are pickedout by exploiting the low rank structure in the reshaped matrix.
Figure 2: Diagram of a ∈R, v ∈ RI, M ∈ RI×J andT∈ RI×J×K.
Figure 3: Tensor operation illustration. Examples of tensor operations in which M ∈ RJ0×J1 ,X ∈ RI0 ×I1 ×I2 and Y ∈ RJ0×J1×J2 are input matrix/tensors, and T1 ∈ RI1 ×I2 ×J0×J2, T2 ∈RJ0×I1×I2, T3 ∈ RI’×I1×I2×J0×J2 and T4 ∈ RIo ×I1×I2×J0×J2 are output tensors of correspond-ing operations. Similar definitions apply to general mode-(i, j) tensor operations.
Figure 4: Diagrams of kernel decompositions. Figure (b) (c) and (d) are three types of plaintensor decomposition for a traditional convolutional kernel K in (a). Figure (f) (g) and (h) are threetypes of reshaped tensor decomposition for our tensorized kernel K'in (e) where the reshaping orderm ∈ Z is chosen to be 3 for illustrative simplicity.
Figure 5: Convolutional layer diagram. Input U is passed through the layer kernel K. The forwardpropogation operation of an uncompressed layer, a plain tensor decomposition compressed layer andour reshaped tensor decomposition compressed layer are illustrated in (a), (b) and (c) respectively.
Figure 6: Convergence rate for Seq vs.
