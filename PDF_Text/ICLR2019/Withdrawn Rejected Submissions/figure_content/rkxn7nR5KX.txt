Figure 1: Our proposed attention attractor network for incremental few-shot learning. A: In stage 1,we learn Wa and the feature extractor CNN backbone through supervised pretraining. B: In stage2 we learn Wb on a few-shot episode through an iterative solver, to minimize cross entropy plus anadditional energy term predicted by attending to the base class representation Wa . C: The attentionattractor network is learned end-to-end to minimize the expected query loss.
