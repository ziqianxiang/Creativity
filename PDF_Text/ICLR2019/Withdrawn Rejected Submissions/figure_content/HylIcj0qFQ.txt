Figure 1: Memorization performances according to the (a) width and (b) depth of the FCDNN.
Figure 2:	(a) Mutual information according to the number of inputs N.(b) The relationship betweenthe number of parameters and the capacity of networks in FCDNNs and CNNs.
Figure 3:	Quantization effect when networks use the (a) full capacity and (b) half capacity.
Figure 4:	Quantization performance according to the number of data to train. (a) FCDNN, (b) CNN,and (c) RNN.
Figure 5: Loss surface and accuracy of floating-point and fixed-point CNNs. Î± denotes the scale ofthe parameter noise.
Figure 6:	Performance degradation according to the quantization precision on (a) CNN trained withCIFAR-10 dataset and (b) RNN LM trained with PTB dataset.
Figure 7:	(a) Accuracy of fixed-point FCDNN on MNIST dataset. (b) Top-1 test accuracy offixed-point ResNets trained on ImageNet dataset. (c) Perplexity of fixed-point WLMs on PTB testset.
Figure 8: Loss surface of fixed-point large model trained with CIFAR-10 dataset. The top andbottom plots are the results for the training and test dataset, respectively.
Figure 9: Loss surface of fixed-point small model trained with CIFAR-10 dataset.
Figure 10: LossPTB dataset.
Figure 11: Loss surface of fixed-point RNN LM with two 16-dimensional LSTM layers trained usingPTB dataset.
