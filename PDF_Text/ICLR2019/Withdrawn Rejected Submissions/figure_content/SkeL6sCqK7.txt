Figure 1: MNIST data-set(b) The transition point of the SNR (Y -axis) ver-sus the beginning of the information compression(X-axis), for different mini-batch sizesInverting this relation, the time to compress the representation Tk by ∆I(X; Tk) = ∆Ik scalesas: T(∆Ik) Z QI(X；T))。，where R = 2 Pλ.∈λNI (4⅛i} Note that R depends solely on theproblem, f (x) or p(y, x), and not on the architecture. The idea behind this argument is as follows- one can expand the function in any orthogonal basis (e.g. Fourier transform). The expansioncoefficients determine both the dimensionality of the relevant/informative dimensions and the totaltrace of the irrelevant directions. Since these traces are invariant to the specific function basis, thesetraces remain the same when expanding the function in the network functions using the weights.
Figure 2: Change in the SNR of the gradients and the Gaussian bound on the MI during the trainingof the network for one layer on ResNet-32, in log-log scale.
Figure 3: The computational benefit of the layers - The converged iteration as function of the numberof layers in the networkWe provide a new Gaussian bound on the representation compression and then relate the diffusionexponent to the compression time. One key outcome of this analysis is a novel proof of the compu-tational benefit of the hidden layers, where we show that they boost the overall convergence time ofthe network by at least a factor of K2, where K is the number of non-degenerate hidden layers. Thisboost can be exponential in the number of hidden layers if the diffusion is ”ultra slow”, as recentlyreported.
