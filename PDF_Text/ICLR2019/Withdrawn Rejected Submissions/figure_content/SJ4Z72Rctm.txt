Figure 1: Policy composition in the tabular case. All tasks are in an infinite-horizon tabular 8x8world. The action space is the 4 diagonal movements (actions at the boundary transition back tothe same state) (a-c) shows 3 reward functions (color indicates reward, dark blue r = +1, lightblue r = 0.75). The arrows indicate the action likelihoods for the max-ent optimal policy for eachtask. (d-f) The log regret for the max-ent returns for 3 qualitatively distinct compositional tasksrb = bri+(1-b)rj, using different approaches to transfer from the base policies. The compositionaltasks we consider are left-right (LR), left-up (LU) and the “tricky“ tasks (T).
Figure 2: Tricky point mass. The continuous “tricky” task with a simple 2-D velocity controlledpointed mass. (a) Environment and example trajectories. The rewards are (r1 = 1, r2 = 0), (0, 1)and (0.75, 0.75) for the green, red and yellow squares. Lines show sampled trajectories (starting inthe center) for the compositional task r1/2 with CO (red), GPI (blue) and DC (black). Only DC, DCheuristics and CondQ (not shown) find the optimal transfer policy of navigating to yellow rewardarea for the joint task which is the optimal solution for the compositional task. (b) The returns foreach transfer method. DC and CondQ methods recover significantly better performance than GPI,and the CO policy performs poorly. (c) The Renyi divergence of the two base policies as a functionof position: the two policies are compatible except near the bottom left corner where the rewardsare non-overlapping. (d) QOpt at the center position for the combined task. As both policies prefermoving left and down, most of the energy is on these actions. (e) However, the future divergenceC∞2 under these actions is high, which results in the (f) DC differing significantly from CO.
Figure 3: “Tricky” task with planar manipulator. The “tricky” tasks with a 5D torque-controlledplanar manipulator. The training tasks consists of (mutually exclusive) rewards of (1, 0), (0, 1) whenthe finger is at the green and red targets respectively and reward (0.75, 0.75) at the blue target. (b)Finger position at the end of the trajectories starting from randomly sampled start states) for thetransfer task with circles indicating the rewards. DC and CondQ trajectories reach towards theblue target (the optimal solution) while CO and GPI trajectories primarily reach towards one of thesuboptimal partial solutions. (c) The returns on the transfer tasks (shaded bars show SEM, 5 seeds).
Figure 4: “Tricky” task with mobile bodies. “Tricky” task with two bodies: a 3 DOF jumpingball (supplementary figure 6) and (a) 8 DOF ant (both torque controlled). The task has rewards(1, 0), (0, 1) in the green and red boxes respectively and (0.75, 0.75) in the blue square. (b-c) Re-turns for both walkers when started in the center position. CO approach does not recover the optimalpolicy for the compositional task while the other approaches largely do, although CondQ does notlearn a good policy on the ant (shaded bars show SEM, 3 seeds for jumping ball, 5 seeds for ant). (e)Sampled trajectories of the ant on the transfer task starting from a neutral position for b = 1. GPIand DC consistently go to the blue square (optimal), CondQ and CO do not.
Figure 5: (a) Trajectories of the ant during transfer on non-composable subtasks. In this experimentthe two base tasks consists of rewards at the red and green square respectively. As expected, inthis task, where the two base tasks have no compositional solution, CO (red) performs poorly withtrajectories that end up between the two solutions. GPI (blue) performs well, as does DC (black).
Figure 6: Jumping ball tricky taskThe point mass was velocity controlled, all other tasks were torque controlled. The planar manipu-lator task was based off the planar manipulator in the DM control suite. The reward in all tasks wassparse as described in the main text.
