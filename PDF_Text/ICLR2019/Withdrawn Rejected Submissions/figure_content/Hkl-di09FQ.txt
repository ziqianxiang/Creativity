Figure 1: State Representation Learning (SRL) vs End-to-End Reinforcement Learning. In End-to-Endlearning, the feature extraction is implicit.
Figure 2: SRL Splits model: combines a reconstruction of an image I, a reward (r) prediction and an inversedynamic models losses, using two splits of the state representation s. Arrows represent model learning andinference, dashed frames represent losses computation, rectangles are state representations, circles are realobserved data, and squares are model predictions.
Figure 3: Environments for state representation learning from S-RL toolbox (Raffin et al., 2018)1D/2D random target mobile navigation: This environment consists of a navigation task using a mobilerobot, similar to the task in (Jonschkowski & Brock, 2015), with either a cylinder (2D target) or a horizontalband (1D target) on the ground as a goal, randomly initialized at the beginning of each episode. The mobilerobot can move in four directions (forward, backward, left, right) and will get a +1 reward when reachingthe target, -1 when hitting walls, and 0 otherwise. Episodes have a maximum length of 250 steps (hence, anupper bound max. reward of 250).
Figure 4: Ablation study of SRL Splits (mean and standard error for 10 runs) for PPO algorithm in Navi-gation 2D random target environment. Models details are explained in Table 11, e.g., SRL_3_splits modelallocates separate parts of the state representation to each loss (reconstruction/reward/inverse).
Figure 5: Performance (mean and standard error for 10 runs) for PPO algorithm for different state repre-sentations learned in Navigation 1D target environment.
Figure 6: Performance (mean and standard error for 10 runs) for PPO algorithm for different state repre-sentations learned in Navigation 2D random target environment.
Figure 7: Performance (mean and standard error for 10 runs) for A2C algorithm for different state repre-sentations learned in Navigation 2D random target environment.
Figure 8:	Performance (mean and standard error for 10 runs) for PPO algorithm for different state repre-sentations learned in robotic arm with random target environment.
Figure 9:	Learning curve (mean and standard error for 10 runs) for PPO algorithm for different state repre-sentations learned in robotic arm with random moving target environment.
Figure 10: Influence of random seed (mean and standard error for 10 runs) for PPO algorithm for SRLSplits in Navigation 2D random target environmentFigure 10 shows that the SRL Split method is stable and its performance does not depend on the randomseed used.
Figure 11:	Influence of the state dimension (mean and standard error for 10 runs) for PPO algorithm forSRL Splits in Navigation 2D random target environment. Each label correspond to the state dimension ofthe model.
Figure 12:	Influence of the training set size (mean and standard error for 10 runs) for PPO algorithm forSRL Splits in Navigation 2D random target environment. Each label corresponds to the number of samplesused to train the SRL model.
