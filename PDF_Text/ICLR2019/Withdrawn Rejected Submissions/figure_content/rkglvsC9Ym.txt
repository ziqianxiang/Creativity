Figure 2: Random generated samples of VAEs trained on MNIST dataset.
Figure 3: Reconstructed samples of VAEs trained on MNIST dataset. For comparision, some corre-sponding samples are marked by boxes with the same color.
Figure 4: Random generated samples of VAEs trained on CelebA dataset. Samples that are verydark and blurry or suffer from serious artifacts are highlighted.
Figure 5: Reconstructed samples of VAEs trained on CelebA dataset. For comparision, some corre-sponding samples are marked by boxes with the same color.
Figure 6: (a) Reconstruction error during training. For comparison, we use squared L2 to measurethe error of both models. We show two corresponding reconstructed sample here: While the upperone is clearly better, it has a larger L? error. (b) Latent space error measured by DκL(qφ(z∣χ) kpθ(z)). (c) Generation quality measured by FID (smaller is better) of models trained with log-coshand λ-scaled squared L2 loss. We show reconstructed digit 3 of models trained with L2 on the topand log-cosh on the bottom. The curvature at 0 is a for log-cosh with parameter a and 2λ for λ-scaled squared L2. Since the log-cosh loss behaves like a 0.5a-scaled squared L2 loss only when theerror is small and is not sensitive to large outliers, it does not harm optimization of latent space muchand increasing a improves reconstruction and generation simultaneously. In contrast, without ourloss, simply scaling the L2 does improve reconstruction but severely harm generation. Moreover,the reconstructed samples are still worse than our method.
Figure 7: Samples of VAEs trained with L1 reconstruction loss on MNIST dataset.
Figure 8: Samples of VAEs trained with L1 reconstruction loss on CelebA dataset.
Figure 9: Interpolation results of VAEs trained on MNIST.
Figure 10: SSIM index of images restored with different loss. Each point corresponds to a restoredimage. Larger SSIM index is better. The green line indicates equal SSIM index. The log-cosh lossfails to outperform the L1 loss in only 1 out of the 100 tested images as highlighted by red pointD	Applying log-cosh to image restoration tasksIt is promising to apply the log-cosh loss to various image restoration applications such as denoising,deblurring, demosaicking, and super-resolution. Here we provide an example of text removal, andshow that the log-cosh loss consistently outperforms the L2 and L1 loss. We use a recently proposeddenoising framework called noise2noise Lehtinen et al. (2018), which learns to restore good imagesby only going through bad ones. The network architecture is based on the idea of convolutionalauto-encoder, consisting of convolutional and deconvolutional layers. The training dataset consistsof 291 images widely used (e.g. in Schulter et al. (2015); Kim et al. (2016)). Among these, 200images of the training set come from the Berkeley Segmentation Dataset Martin et al. (2001) andthe rest 91 images come from Yang et al. (2010). Following Lehtinen et al. (2018), we add texts tothe images and let the model learn to remove these texts. After training, we test the trained modelon the Urban 100 dataset Huang et al. (2015) which consists of 100 high resolution images. Weuse the SSIM index Wang et al. (2004) which is often used as a measure to evaluate the similaritybetween the restored images and the target images in the denoising tasks. We compare the squaredL2 loss and the L1 loss to the log-cosh loss, and the results are shown in Fig. 10. We can see thatthe log-cosh loss consistently outperforms both the squared L2 loss and the L1 loss. Actually, thelog-cosh loss fails to outperform the L1 loss in only 1 out of the 100 tested images, illustrated by the
Figure 11: Text removal results of models trained with the squared L2, L1 and log-cosh loss. We list1-SSIM here to show the difference between the restored images and the target images. The resultsof the L2 loss suffers from severe color shift. The results of the L1 loss suffer from severe artifacts.
