Figure 1: Forward and backward pass through a node computing dilationWhere x0 = [x, 0] and x0k denotes the kth component of vector x0. The 0 is appended to the inputx to take care of the ‘bias’. Here we try to learn the structuring element (s). Note that erosionoperation can also be written in the following form.
Figure 2: Single Layer DenMo-Net with n dilation and m erosion neuron and c output neurons3.4 Function ApproximationHere we show that with the linear combination of dilation and erosion, any function can be ap-proximated, and the approximation error decreases with increase in the number of neurons in thedilation-erosion layer. Before that we need to describe some concepts.
Figure 3: Decision boundaries of different networks(c) DenMo-NetTable 1: Training accuracy achieved on the circle dataset by different networksMethods	Hidden nodes	Parameters	Training accuracyNN-ReLU	2	12	68.87NN-tanh	2	12	69.10Maxout Network (h=2)	2	18	87.17DenMo-Net	2	12	91.64.1	Visualization with a toy datasetFor visualizing the decision boundaries learned by the classifiers, we have generated data on twoconcentric circles belonging to two different classes with center at the origin. We compare theresults when only two neurons are taken in the hidden layer in all the networks. It is observed thatclassical neural network fails to classify this data with two hidden neurons as it learns one hyperplaneper one hidden neuron. The boundaries learned by the network with ReLU activation function (NN-ReLU) is shown in figure 3a. The result of maxout network is better (87.17% training accuracy)as it introduces extra parameters with max function to achieve non-linearity. In the maxout layerwe have taken maximum among h = 2 features. As we see in the figure 3b the network learns(2 * h =)4 straight lines when trying to classify these data. For the same data and two neurons indilation-erosion layer, our DenMo-Net has learned 6 lines to form the decision boundary (figure 3c).
Figure 4: Test accuracy achieved over epochs in the MNIST datasetTable 2: Accuracy on MNIST dataset with different architecturesNeurons in dilation-erosion layer (l)Test Accuracy10	50	100	20076.35	93.38	95.51	96.854.3	Fashion-MNIST DatasetThe Fashion-MNIST dataset (Xiao et al., 2017) has been proposed with the aim of replacing thepopular MNIST dataset. Similar to the MNIST dataset this also contains 28 × 28 images of 10classes and 60,000 training and 10,000 testing samples. While MNIST is still a popular choicefor benchmarking classifiers, the authors’ claim that MNIST is too easy and does not represent themodern computer vision tasks. This dataset aims to provide the accessibility of the MNIST datasetwhile posing a more challenging classification task.
Figure 5: Test accuracy over epochs on CIFAR-10 datasetTable 4: Test accuracy achieved on CIFAR-10 dataset by different networksArchitecture		l=200				l=400				l=600			parameters	accuracy	parameters accuracy		parameters a	ccuracyNN-tanh	616,610	^^48.88	1,233,210	49.39	1,849,810	51.24NN-ReLU	616,610	49.28	1,233,210	50.43	1,849,810	52.25Maxout-Network	1,231,210	49.51	2,462,410	50.10	3,693,610	51.51DenMo-Net	616,610	51.84	1,233,210	53.41	1,849,810	54.49happens because our network is able to learn more hyperplanes with number of parameters similarto normal artificial neural networks. When using only a single type of neurons in our network, wesee a different result for this dataset (Figure 5b). The network takes time to learn with only erosionneurons. The situation improves a little when using only dilation neurons. When using both type ofmorphological neurons, the network is able to perform better by leveraging the power of both theoperations.
Figure 6: Test accuracy over epochs on CIFAR-100 datasetTable 5: Comparison with Baseline CIFAR100Architecture	l=200		l=400			l=600			parameters accuracy		parameters accuracy		parameters a	ccuracyNN-tanh	634,700	19.50	1,269,300	19.62	1,903,900	20.46NN-ReLU	634,700	17.83	1,269,300	19.63	1,903,900	20.77Maxout-Network	1,249,300	21.58	2,498,500	21.49	3,747,700	21.69DenMo-Net	634,700	23.65	1,269,300	25.89	1,903,900	26.93Therefore, only a single element of the structuring element is updated. So, the learning can be slow.
Figure 7: Train and Test accuracy achieved over epochs in the CIFAR-10 datasetsufficiently smooth function without requiring any non-linear activation function. These networksare able to learn a large number of hyperplanes with very few neurons in the dilation-erosion layerthereby providing superior results compared to other networks with three layer architecture. The im-proved results could also be the result of ‘feature selection’ by the max/min operator in the dilationerosion layer. In this work we have only worked with fully connected layers, i.e. a node in a layeris connected to all the nodes in the previous layer. This type of connectivity is not very efficient forimage data where architectures with convolution layers perform better. So, extending this work tothe case where a structuring element operates by sliding over the whole image, should be the nextlogical step.
