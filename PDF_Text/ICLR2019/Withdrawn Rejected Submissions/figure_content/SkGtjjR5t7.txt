Figure 1: Driving model inputs (a-g) and output (h).
Figure 2: Training the driving model. (a) The core model with a FeatureNet and an AgentRNN,(b) Co-trained road mask prediction net and PerceptionRNN, and (c) Training losses are shown inblue, and the green labels depict the ground-truth data. The dashed arrows represent the recurrentfeedback of predictions from one iteration to the next.
Figure 3: (a) Schematic of the AgentRNN. (b) Memory updates over multiple iterations.
Figure 4: Model ablation test results on three scenario types.
Figure 5: Prediction Error for mod-els M0 and M4 on unperturbedevaluation data.
Figure 6: Software architecture for the end-to-end driving pipeline.
Figure 7: Trajectory Perturbation. (a) An original logged training example where the agent is drivingalong the center of the lane. (b) The perturbed example created by perturbing the current agentlocation (red point) in the original example away from the lane center and then fitting a new smoothtrajectory that brings the agent back to the original target location along the lane center.
Figure 8: Visualization of predictions and loss functions on an example input. The top row is at theinput resolution, while the bottom row shows a zoomed-in view around the current agent location.
Figure 9: Prediction Error for models M0 and M1 on perturbed evaluation data.
Figure 10: Comparison of ground-truth trajectory in (a) with the predicted trajectories from modelsM0 and M1 in (b) and (c) respectively on two perturbed examples. The red point is the referencepose (u0, v0), white points are the past poses and green points are the future poses.
Figure 11: Sampling speed profiles. The probability distribution P1(x, y) predicted by the model attimestep k = 1 allows us to sample different speed profiles conditioned on which the later distribu-tion P5 (x, y) gets more constrained.
