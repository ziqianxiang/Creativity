Figure 1:	The figure shows the qualitative behavior of the accuracy curves. The dashed accuracyover randomness curves(a) depicts models which overfit, as the model has enough capacity to fitnoise. The dotted accuracy curves(a) on the other hand show model which underfit. The shape ofthe accuracy curves(a) is controlled by a regularization parameter 位1. Each curve in (b) depictsqualitatively a accuracy over regularization curves. The mark(b) depicts the point there the accuracybegins to descent.
Figure 2:	The figures show the accuracy curves for three different data sets. Each curve representsa different l1 -regularization. The curves start with no regularization, depicted in blue, to strongregularization depicted in red. As the regularization is increase the curve tend to pushed below thestraight line, confirming our intuition.
Figure 3: The plots show the accuracy curves of the network trained on cifar10 over different de-grees of randomness with increasing degree of l1-regularization, after 19999 iterations. We select 位such that the blue accuracy curve stays below the optimal green line and is convex. Following ourconvexity criterion We choose 位; = 0.00011 as regularization factor.
Figure 4: For (a) an Alexnet-type network was trained on mnist with varying degrees of randomness.
Figure 5: The plots shows the accuracy of the network trained on cifar10 over different degrees ofrandomness with increasing degree of l1-regularization. The network trained for 199999 iterations.
Figure 6: In our steep descent criterion we propose to detect the first change point at which theaccuracy is not constant anymore but falls linearly. In the figure this is depicted by the green curves:around 10-4 the accuracy begins to fall. As a measure of how much the net learned about the data,we also provide the accuracy curves for random data. We conclude from the gap between the red andthe blue curve that the net learned something meaningful about the data, instead of just memorizingthe data.
Figure 7: The plots(a)-(d) show the margin histograms as trained in cifar101.0 for an increasing regu-larization factor. According to our third criterion we choose the 位 = 0.00011, as here(d) the margindistribution developed a second mode for the first time.
Figure 8:	The figure shows a sketch of the networks used for most experiments on cifar10.
Figure 9:	The plots show different l1-regularizations for cifar10(a)-(d) and cifar10-random(e)-(h). Be-ginning in (a) (and (f)) the regularizations are e-5, 0.0001, 0.048, 1.0. We note that for (a) and (e)the model overfits, for (b) and (f) it is just right, and for (c) and (g) it underfits.
Figure 10:	The plots shows the accuracy of the network trained on cifar10 over different degrees ofrandomness with increasing degree of l1-regularization. The network trained for 019999 iterations.
Figure 11:	The plots shows the accuracy of the network trained on cifar10 over different degrees ofrandomness with increasing degree of l1-regularization. The network trained for 039999 iterations.
Figure 12:	The plots shows the accuracy of the network trained on cifar10 over different degrees ofrandomness with increasing degree of l1-regularization. The network trained for 059999 iterations.
Figure 13:	The plots shows the accuracy of the network trained on cifar10 over different degrees ofrandomness with increasing degree of l1-regularization. The network trained for 079999 iterations.
Figure 14:	The plots shows the accuracy of the network trained on cifar10 over different degrees ofrandomness with increasing degree of l1-regularization. The network trained for 099999 iterations.
Figure 15:	The plots shows the accuracy of the network trained on cifar10 over different degrees ofrandomness with increasing degree of l1-regularization. The network trained for 119999 iterations.
Figure 16:	The plots shows the accuracy of the network trained on cifar10 over different degrees ofrandomness with increasing degree of l1-regularization. The network trained for 139999 iterations.
Figure 17:	The plots shows the accuracy of the network trained on cifar10 over different degrees ofrandomness with increasing degree of l1-regularization. The network trained for 159999 iterations.
Figure 18:	The plots shows the accuracy of the network trained on cifar10 over different degrees ofrandomness with increasing degree of l1-regularization. The network trained for 179999 iterations.
Figure 19:	The plots shows the accuracy of the network trained on cifar10 over different degrees ofrandomness with increasing degree of l1-regularization. The network trained for 199999 iterations.
