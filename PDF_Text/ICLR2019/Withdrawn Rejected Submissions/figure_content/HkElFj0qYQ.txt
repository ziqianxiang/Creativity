Figure 1: Permutation Phase Defense (PPD): image in the pixel domain is first passed through arandom permutation block with a fixed seed. The seed is fixed for all images and hidden fromthe adversary. The permuted image is then converted to the phase of its Fourier transform and fedto the neural network. The random permutation block conceals what pixels are neighbors, and thepixel2phase block determines values of the pixels in the phase domain based on frequency of changein neighboring pixels. This pipeline is used in both training and inference phases.
Figure 2: Phase component of 2d-DFT contains key information of the image. Thus, neural net-works can be trained in the phase domain rather than the original pixel domain. (Left) originalimage. (Middle) image reconstructed from magnitude only by setting phase to zero. It almost hasno information of the original image. (Right) image reconstructed from phase only by setting mag-nitude to unity. Edges are preserved and main features of the original image are restored.
Figure 3: (Top) training and testing accuracy of a PPD model on clean images. Pixels of eachimage are permuted according to a fixed permutation, then phase component of the 2d-DFT of thepermuted image is fed to the neural network. A 3-layer dense neural network can achieve 96% outof sample accuracy on MNIST and 45% on CIFAR 10. (Bottom) accuracy of ensemble of modelson clean test images. Each model in the ensemble is trained for a different permutation. Ensembleof 10 PPD models can reach above 97.75% accuracy on clean MNIST test images and around 48%accuracy on clean CIFAR-10 test images.
Figure 4: Performance of ensemble of 50 PPD models against different '∞ attacks. Left columnshows results on MNIST, right column on CIFAR-10. Adversary uses a PPD model with wrong per-mutation seed to generate adversarial examples (solid blue line). Adversarial examples are then fedto the ensemble of other 50 models (dashed red line). Performance of ensemble of 50 PPD modelson images distorted by uniform noise is shown for a benchmark comparison (dashed black line). OnMNIST, it can be seen that except for MIM with large '∞ perturbation, adversarial perturbationscannot fool PPD more than random noise. On CIFAR-10, for '∞ perturbations less than 0.1, adver-sarial attacks are not more effective than random noise distortion. See Figure 6 in the Appendix toconfirm that perturbations larger than 0.1 on CIFAR-10 can make it difficult even for human eye toclassify correctly.
Figure 5: Performance of ensemble of 50 PPD models against different `2 attacks. Left columnshows results on MNIST, right column on CIFAR-10. Adversary uses a PPD model with wrongpermutation seed to generate adversarial examples (solid blue line). Adversarial examples are thenfed to the ensemble of other 50 models (dashed red line). Performance of ensemble of 50 PPDmodels on images distorted by uniform noise is shown for a benchmark comparison (dashed blackline). It can be seen that for `2 perturbations less than 4, adversarial attacks on PPD are not moreeffective than random noise distortion. See Figure 6 in the Appendix to visualize perturbed images.
Figure 6: MNIST image of digit 2 and CIFAR-10 image of a ship distorted by adversarial attacksand random noise. Left group of columns is for '∞ perturbation, right group denotes '2 perturbation.
