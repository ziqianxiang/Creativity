Figure 1:	Example descriptions with corresponding ground truth image. Shown are natural (NL) and synthetic(SYN) language descriptions. Annotation linguistic errors have been preserved.
Figure 2:	Diagram of model used for experiments. A representation network parses multiple descriptions of ascene from different viewpoints by taking in camera coordinates and a textual caption. The representations foreach viewpoint are aggregated into a scene representation vector r which is then used by a generation networkto reconstruct an image of the scene as seen from a new camera coordinate.
Figure 3: Samples generated from the synthetic (top) and natural language (bottom) model. Correspondingcaptions are: ”There is a pink cone to the left of a red torus. There is a pink cone close to a purple cone. Thecone is to the left of the cone. There is a red torus to the right of a purple cone.”; ”There are two objects in theimage. In the back left corner is a light green cone, about half the height of the wall. On the right side of theimage is a bright red capsule. It is about the same height as the cone, but it is more forward in the plane of theimage.”4.1	A note on evaluationWhile pixel loss compared with gold images is an obvious metric to optimise against when generat-ing images, human judgements are more suitable for evaluating this task. More precisely, pixel lossis too strict a metric for evaluating the generative model due to the differing degrees of specificitybetween language and visual data points. Consider the images and descriptions in Figure 1. Adjec-tives such as big or relative positions such as to the far right are sufficient to give us a high-levelidea of a scene, but lack the precision required to generate an exact copy of a given image. The samedescription can be satisfied by an infinite number of visual renderings—implying that pixel loss witha gold image is not a precise measure of whether a given image is semantically consistent with thematching scene description. Human judgements on the other hand allow us to evaluate this task onthe desired property, preferring spatial consistency over pixel-accuracy in the generated images.
Figure 4: On the left, ELBO numbers for the model variants under training for train/validation/test splits. Onthe right, human ranking of consistency of visual scene samples with matching synthetic caption. For SLIMt(NL+SYN) numbers shown were calculated only with natural language inputs.
Figure 5: Top, transformations following Gershman &Tenenbaum (2015). Bottom, averaged cosine similarity be-tween the representation obtained by encoding a single cap-tion with a fixed camera angle and the transformed represen-tation. The meaning preserving representation is the mostsimilar; followed by noun, adjective and preposition changes.
Figure 6: Left, samples from the model when fed input contexts transformed according to Gershman & Tenen-baum (2015) transformations. Right, average cosine distance between base representation and aggregatedrepresentation induced by applying one of the four transformations to each of the context inputs. Black barsrepresent 95% CI, average over 32 scenes.
Figure 7: a) t-SNE of single description encodings, coloured by camera angle. b) Distance between singledescription representations of the same scene as a function of the angle between their viewpoints. c) Distancebetween aggregated representations drawn from opposing arcs as a function of the size of those arcs. Bluecompares same scene representations, red different scene representations. d) Output samples for a constantscene and coordinate under varying input conditions. Top: a single description (from black arrow), bottom:aggregated descriptions from an increasingly sized arc.
