Figure 1: Unsupervised Visual Abstraction Model: There are three mutual information losses be-ing computed in this process: I(Gt, Gt+∆), I((Gt, Gt+1), At) and I(Vt, Ct). I(Gt, Gt+∆) forcesthe VQ to distinguish frames in the same unroll or large temporal segment with frames outside ofthis window. I((Gt, Gt+1), At) encourages the VQ to encode action controllable elements in theframe. I(Vt, Ct) forces the VQ to represent color information.
Figure 2: Agent Architecture: (a) The input st is used to compute the visual entities vt . Theirone-hot encoding is a set of E {0, 1} masks. These are used to compute geometric measurementssuch as: area and centroid positions (Sec. 3). Temporal differences in these constitute intrinsicrewards rint for an option bank. (b) The input is separately passed through different a CNN andLSTM network, whose output is then fed to: Qtask, Qmeta and options bank with E × M Q functions.
Figure 3: Unsupervised Visual Abstractions: Inputs and inferred visual abstractions using ourmodel. Each row represents temporally close frames from different environments including naviga-tion and Atari. The colors in the abstraction images correspond to visual instances with temporallyconsistent labels.
Figure 4:	Navigation domain. (a) Top-down view of a 3D Maze. The agent observes pixels fromthe first person view. The task is described in section 4.1. (b) The green curve denotes an optimalagent with access to ground truth visual abstractions. Our learnt model achieves close to optimalperformance while the baseline fails to solve the task. (c) Plot showing how the meta control policyswitches between options bank and task control policies. Time increases top to bottom. The rightmost column is the task policy and all other columns denote option policies. Initially the agent usesthe options policies to explore and then gradually shift over to the task policy as training progresses.
Figure 5:	Quantitative results on the Atari domain: Average returns per episode on 3 Atarigames where exploration is considered very challenging. On these domains we achieve better orcomparable results to a strong baseline.
Figure 6: Quantitative results on the DMLab domain: Average returns per episode on 3 DMLablevels.
