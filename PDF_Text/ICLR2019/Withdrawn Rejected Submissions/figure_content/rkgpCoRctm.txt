Figure 1: An illustration of the complete proposed model. At each BN layer, we extract the input,normalize it using the running statistics, and compute the first and second order features. The outputsare fed to a linear decision function to predict if the input sample is out-of-distribution or not.
Figure 2: Visualization of the normalized BN latent space distribution for one ID sample (a), andone OOD sample (b) at BN layers l = 1, 5, 10, 15, 20, 25 from a WideResNet-28-10. We also showthe normal distribution for comparison. In the mid layers, the (arbitrary) ID CIFAR-10 sample (a)is more similar to the batch stastitics than the (arbitrary) OOD TinyImageNet(r) sample (b). Bars:normalized feature histograms. Blue curve: KDE. Orange curve: N (0, 1) (always the same insideeach column but scaled for visualization). More in Appendix A.
Figure 3: Manifold visualization of the proposed features (mean and Std) from a WRN-28-10 model.
Figure 4: Averaged TNR @95% TPR over the OOD datasets using only a few samples to fit thelogistic regressor (for WRN-28-10). The logistic regressor was fitted using TinyImageNet (c) andGaussian validation sets using only S(Il as features. This experiment shows, in accordance to ourresults, that CIFAR-100 is more difficult to differentiate from other OOD datasets than CIFAR-10.
Figure 5: More visualization results following the same procedure as in Figure 2 for a WRN-28-10model.
Figure 6: More visualization results following the same procedure as in Figure 2 but for a DenseNet-BC-100-12model.
Figure 7: TNR @95% TPR obtained When aggregating information from multiple layers. Theleftmost bin corresponds to the deepest (last) layer, and the rightmost bin to the first BN layer inthe network using a WRN-28-10 trained on CIFAR-10/CIFAR-100, the logistic regressor was fittedon TinyImageNet (c) + Gaussian validation sets and the results are an average over all OOD testdatasets.
