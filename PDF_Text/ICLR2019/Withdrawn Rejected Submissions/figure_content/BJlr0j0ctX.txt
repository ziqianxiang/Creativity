Figure 1: The effect of adversarial training on the numerator and denominator for L on MNIST.
Figure 2: Adversarial training squishes the logits. Training on stronger adversaries squishes more.
Figure 3: Label smoothing by itself worsens robustness. With the addition of Gaussian augmenta-tion, robustness improves. This enhanced robustness results from both gradients getting squashedand gradients becoming more aligned - two effects that shrink the denominator in Equation 4. Plotsuse MNIST test samples with σ = 0.5 and α = 0.75. The y-axis of the logit gap is chopped at 30k.
Figure 4: Label smoothing plus Gaussian augmentation works by decreasing the gradient gap. Thisis done by both shrinking the gradients and aligning them. The effects get larger with the increasein the smoothing parameter. Logit squeezing plus Gaussian noise works by increasing the logit gapand decreasing the gradient gap. Increasing the squeezing parameter, strengthens these effects.
Figure 5: The cross-entropy landscape of the first eight images of the validation set for the modeltrained for 160k iterations with hyper-parameters β = 10 and σ = 30. To plot the loss landscape wetake two random directions r1 and r2 (i.e. r1, r2 〜RademaCher(0.5)). We plot the cross-entropy(i.e. Xent) loss at 祖fferent points X = Xi + e1 ∙ r1 + e2 ∙ r2. Where Xi is the clean image and-10 ≤ 1,2 ≤ 10.
Figure 6: The effect of adversarial training on the numerator and denominator of the expressionfor L on CIFAR-10. (left-top) The numerator of Equation 4 (i.e, the logit gap). (left-bottom)The denominator of Equation 4. (right-top) A histogram of values of L calculated by applyingEquation 4 to test data. (right-bottom) Cosine between the gradient vectors of the logits (i.e., gradientcoherence).
Figure 7: Label smoothing decreases the difference between the adversarial gradients (the denomi-nator of Equation 4) by shrinking the gradient magnitudes. The gradients magnitudes are depictedhere using a histogram.
Figure 8: Random Gaussian augmentation helps logit squeezing. Graphs are for MNIST test exam-ples with σ = 0.5 and β = 0.5.
Figure 9: The cross-entropy landscape of the first eight images of the validation set for the modeltrained for 160k iteration and hyper-parameters β = 10 and σ = 30. To plot the loss landscapeWe take walks in one random direction r1 〜RademaCher(0.5) and the adversarial direction a2 =SignExXent) where xent is the cross-entropy loss. We plot the cross-entropy (i.e. Xent) loss atdifferent points X = Xi + e1 ∙ r1 + e2 ∙ a2. Where Xi is the clean image and -10 ≤ e1 ,e2 ≤ 10. Asit can be seen moving along the adversarial direction changes the loss value a lot and moving alongthe random direction does not make any significant major changes.
Figure 10: For MNIST, we plot the cumulative magnitude of activations for all neurons in featurelayer of a network produced by natural training, adversarial training, natural training with randomnoise, label smoothing (α = 0.2) with random noise, and logit squeezing (β = 0.5) with randomnoise. In all cases, the noise is Gaussian with σ = 0.5. Interestingly, the combination of Gaussiannoise and label smoothing, similar to the combination of Gaussian noise and logit squeezing, deac-tivates roughly 400 neurons. This is similar to adversarial training. In some sense it seems that allthree methods are causing the “effective” dimensionality of the deep feature representation layer toshrink.
Figure 11: For MNIST, we plot the cumulative sum of activation magnitudes for all neurons in logitlayer of a network produced by natural training, adversarial training, natural training with randomnoise, label smoothing (LS = 0.2) with random noise, and logit squeezing (β = 0.5) with randomnoise. In all cases, the noise is Gaussian with σ = 0.5.
