Figure 1: Learning curve on 2Dpoint mass. See text for details.
Figure 2: Visualization of GASIL policy on 2D point mass. The first two rows show the agentâ€™s trajectories andtop-k trajectories at different training steps from left to right. The third row visualizes the learned discriminatorat the corresponding training steps. Each arrow shows the best action at each position of the agent for whichthe discriminator gives the highest reward. The transparency of each arrow represents the magnitude of thediscriminator reward (higher transparency represents lower reward).
Figure 3: Visualization of PPO policy on 2D point mass. Compared to GASIL (Figure 2), PPO tends toprematurely learn a worse policy.
Figure 4:	Learning curves on OpenAI Gym MuJoCo tasks averaged over 10 independent runs. x-axis and y-axiscorrespond to the number of steps and average reward.
Figure 5:	Learning curves on stochastic Walker2d-v2 averaged over 10 independent runs. The leftmost plotshows the learning curves on the original task without any noise in the environment. The other plots showlearning curves on stochastic Walker2d-v2 task where Gaussian noise with standard deviation of {0.05, 0.1, 0.5}(from left to right) is added to the observation for each step independently.
Figure 6:	Learning curves on delayed-reward versions of OpenAI Gym MuJoCo tasks averaged over 10independent runs. x-axis and y-axis correspond to the number of steps and average reward.
Figure 7: Effect of GASIL hyperparameters.
Figure 8: Learning curves on hard exploration Atari games averaged over 3 independent runs. x-axis and y-axiscorrespond to the number of steps and average reward.
