Figure 1:	Comparison of crowd ensemble and standard Q-learning approaches on the low-dimensioncart-pole task.
Figure 2:	Randomly selected agents for Ne = 1, the single Q-learner, and Ne = 50.
Figure 3:	Comparison of DQN results against the crowd ensemble DQN approach on the high-dimension cart-pole task.
Figure 4: Comparison of results on the bipedal walker task.
Figure 5: Solution efficiency histograms for the non-ensemble approach and the first five Ne values.
Figure 6: Two-dimension views of the Q-function and decision space volatility for the low-dimension cart-pole task for a non-ensemble Q-function.
Figure 7: Two-dimension views of the Q-function and decision space volatility for the low-dimension cart-pole task for a Q-function ensemble of size 20.
Figure 8: Frames of a movie showing the agent’s movement through the state space for an examplerun of a single Q-learner (Ne = 1). Each Frame is a single time step. The first 120 time steps areshown. State space locations are colored according to the number of times the preferred action atthat location changed. The red dot in the agent’s current location in state space. Color bar is scaledto match colors in Figure 913Under review as a conference paper at ICLR 2019■2.5 2.5∂Figure 9: Frames of a movie showing the agent’s movement through the state space for an examplerun ofan ensemble Q-learner (Ne = 20). Each Frame is a single time step. The first 120 time stepsare shown. State space locations are colored according to the number of times the preferred action atthat location changed. The red dot in the agent’s current location in state space. Color bar is scaledto match colors in Figure 814Under review as a conference paper at ICLR 20198.3	Crowd ensemble DQN additional informationFOjlow*A1
Figure 9: Frames of a movie showing the agent’s movement through the state space for an examplerun ofan ensemble Q-learner (Ne = 20). Each Frame is a single time step. The first 120 time stepsare shown. State space locations are colored according to the number of times the preferred action atthat location changed. The red dot in the agent’s current location in state space. Color bar is scaledto match colors in Figure 814Under review as a conference paper at ICLR 20198.3	Crowd ensemble DQN additional informationFOjlow*A1+ A2+ A3Conv layer 2Figure 10: Example deep Q-learning network with two convolutional layers and two fully-connectedlayers and three action outputs. Shown are two example input frames and two example features ateach convolutional layer. The input frames and subsequent features were captured from our DQNagent during training.
Figure 10: Example deep Q-learning network with two convolutional layers and two fully-connectedlayers and three action outputs. Shown are two example input frames and two example features ateach convolutional layer. The input frames and subsequent features were captured from our DQNagent during training.
