Figure 1: On the performance of the proposed GDM. Given four consecutive frames of Atari games,and a sequence of eight actions, GDM generates sequences of the future frames almost identical tothe real frames. First row: A sequence of real frames. Second row: a corresponding sequence ofgenerated frames2016a). Furthermore, more general settings like partial monitoring games are theoretically tackled(Bart6k et al., 2014) and minmax regret guarantees are provided.
Figure 2: Goldfish looking for a Gold bucket. The Q function is initialized such that the yellow arrowsrepresent the greedy action of each state. The red arrows are the actions suggested by followingMCTS on the learned model. (a) Following GATS with depth two locally prevents the goldfish fromhitting the sharks but slows down learning that action down is sub-optimal due to the delay in negativesignal. (b) Even if the goldfish uses the prediction of the future event for further learning, it mightjust slightly mitigate the slow-down in the learning process, but the fundamental issue is still present,and the agent suffers from a slow-down in learning. (c) For a grid world of 10 × 10, GATS with depthof 10 (GATS-10) results in the highest return. Moreover, GATS with nonzero depth locally saves theagent from hitting the sharks, but in the long run it degrades the performance.
Figure 3: The sequence of four consecutive decision states, and corresponding learned Q-function byDQN at t, t + 1, t + 2, t + 3 from left to right, where the agent loses the point. At time step t, theoptimal action is up but the Q-value of going up is lower than other actions. More significantly, eventhough the agent chooses action down and goes down, the Q value of action down at time step t isconsiderably far from the maximum Q value of the next state at time step t + 1.
Figure 4: States at t - 1 → t and the corresponding Q function learned through DQN at time t.
Figure 5: left:GATS learns a better policy faster than plain DQN (2 times faster). GATS k denotesGATS of depth k . right: Accuracy of RP. The Y axis shows the number of mistakes per episode andeach episode has average length of 2k, so the acc is almost always around 99.8%. This accuracy isconsistent among runs and different lookahead lengths.
Figure 6: left:The optimism approach for GATS improves the sample complexity and learns a betterpolicy faster. right: Sampling the replay buffer uniformly at random to train GDM, makes GDM slowto adapt to novel parts of state space.
Figure 7: The GATS algorithm on Asterix and Breakout with 1 and 4 step look-ahead, compared tothe DDQN baseline.
Figure 8: Roll-out of depth two starting from state xt . Here xb’s are the generated states byGDM. Q(x, a(x)) denotes the predicted value of state x choosing the greedy action aQ (x) :=arg maxa0∈A Q(x, a0).
Figure 9: Training GAN and Qθ0 using the longer trajectory of experiencespart. We concatenate the bottleneck and next 5 deconvolution layers with a random Gaussian noiseof dimension 100, the action sequence, and also the corresponding layer in the encoder. The lastlayer of decoder is not concatenated. Fig. 10. For the discriminator, instead of convolution, we useSN-convolution (Miyato et al., 2018) which ensures the Lipschitz constant of the discriminator isbelow 1. The discriminator consists of four SN-convolution layers followed by Batch Normalizationlayers and a leaky RELU activation with negative slope of -0.2. The number of channels increaseas 64, 128, 256, 16 with kernel size of 8, 4, 4, 3, which is followed by two fully connected layers ofsize 400 and 18 where their inputs are concatenated with the action sequence. The output is a singlenumber without any non-linearity. The action sequence uses one hot encoding representation.
Figure 10: The GDM generator is an encoder-decoder architecture with skip-connections betweenmirrored layers, with action and Gaussian noise concatenated in the bottleneck and decoder layers.
Figure 11: Training and evaluating domain transfer for GDM on new game dynamics for Pong (Mode1, Difficulty 1). GDM domain transfer from Pong (Mode 0, Difficulty 0) on left and GDM fromre-initialized parameters on right. L1 and L2 loss curves displayed top. Ground truth next framesdisplayed middle with predicted next frames displayed bottom.
Figure 12: Eight 5-step roll-outs of the GDM on the Pong domain. Generated by sampling an initialstate with 8 different 5-action length sequences.
Figure 13: Eight 5-step roll-outs of the GDM on the Asterix domain. Generated by sampling an initialstate with 8 different 5-action length sequences.
