Figure 1: The left figure reviews, for a single channel at a particular layer within a single batch,notable quantities computed during the forward pass and during back-propagation of BN. LetNXxi	2an and σi=1 N(Xi - μ)=Let L be a function de-pendent on the normalized activations zi defined for each j by zj(Xj- M)O--------This, along Withσpartial derivatives, are shoWn in the left figure. Our Work establishes a novel identity on the quanti-ties shoWn in hexagons. The right figure illustrates our main result in a scatter plot, in Which eachpair ( zi, - ) is shown as a data point in the regression.
Figure 2: We review the normalization partitions of BN, LN, GN, and IN. Each normalizationpartition contains a separate set of data points on which the gradient regression is performed. Onepartition for each method is illustrated in blue. This figure also shows the correspondence betweena single activation and a gradient regression data point for BN.
Figure 3: This figure illustrates the original (He et al., 2016a) and improved (He et al., 2016b)residual mappings in ResNets. Arrows point in the direction of the forward pass. Dotted linesindicate that gradients are zero-centered and decorrelated with respect to downstream activations inthe residual mapping. The improved ordering has BN coming first, and thus constrains that gradientsof the residual map must be decorrelated with respect to some normalized activations inside theresidual mapping.
