Figure 1:	(a) Approach Overview: We study the problem of layout estimation in 3D by reasoning about rela-tionships between objects. Given an image and object detection boxes, we first predict the 3D pose (translation,rotation, scale) of each object and the relative pose between each pair of objects. We combine these predictionsand ensure consistent relationships between objects to predict the final 3D pose of each object. (b) Output: Anexample result of our method that takes as input the 2D image and generates the 3D layout.
Figure 2:	Approach Details: We use the instance encoder to create an embedding for each object (instance)in the scene. The instance decoder uses this embedding to predict a pose for each object independently. Therelative encoder takes each pair of instances and their embeddings to output an embedding for the pair. Therelative decoder predicts a relationship (relative pose) between the pairs. We combine these relative and per-object predictions to predict final pose estimates for each object in an end-to-end differentiable framework.
Figure 3: We visualize sample prediction results in the setting with known ground-truth boxes. Our methodproduces better estimates than the baseline, e.g., bottom right where the distance between the chairs and tableis predicted correctly, and the pose of the yellow chair is corrected. Best viewed in color.
Figure 4: We visualize sample prediction results on the NYUv2 dataset in the setting with known ground-truthboxes. Our method produces better estimates than the baseline, e.g., for the top left image we see that the chairsare better aligned by our method as compared to the baseline. We use the ground-truth meshes for visualizationdue to lack of variability in shape annotations for learning.
Figure 5: We visualize sample prediction results in the detection setting. Our method produces better estimatesthan the baseline, e.g., in the first image the television set and the table are better placed. The colors onlyindicate separate instances, and do not correspond between the ground-truth and the predicted representations.
Figure 6:	We plot the precision-recall (PR) curves for the detection setting for SUNCG and also displaythe mean Average Precision (AP) values in the legend. In each of these curves, we vary the criteria usedto determine a true positive. This helps us analyze the relative contribution of each component (translation,rotation, scale) to the final performance.
Figure 7:	We visualize predictions for randomly sampled images in the setting with known ground-truth boxesfor the SUNCG dataset.
