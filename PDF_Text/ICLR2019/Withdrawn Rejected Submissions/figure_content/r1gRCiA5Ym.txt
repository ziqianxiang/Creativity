Figure 1: Portion of activate neurons on differ-ent layers throughout the training process. Thenetwork is “CIFAR10(s)” (see Sec. 4).
Figure 2: Comparison of the original dropout,dropout with our rescaling and jumpout, on theirperformance (after 150 training epochs) whenused with or without bactch normalization (BN)in “CIFAR10(s)” network (see Sec. 4). Jumpoutwill be formally introduced in Sec. 3.4.
Figure 3: Comparison of mean/variance drift when using (1 - p)-1, (1 - p)-0.5 and (1 - p)-0.75 as thedropout rescaling factor applied to y, when p = 0.2. The network is “CIFAR10(s)” (see Sec. 4). The left plotshows the empirical mean of y with dropout divided by the case without dropout (averaged over all layers), andthe second plot shows the similar ratio for the variance. Ideally, both ratios should be close to 1. As shown in theplots, (1 - p)-0.75 gives nice trade-offs between the mean and variance rescaling.
Figure 4: Top Left: WideReSNet-28-10+Dropout and WideReSNet-28-10+Jumpout on CIFAR10; Top Right:WideResNet-28-10+Dropout and WideReSNet-28-10+Jumpout on CIFAR100; Bottom Left: WideResNet-16-8+Dropout and WideReSNet-16-8+Jumpout on SVHN; Bottom Right: WideResNet-16-8+Dropout andWideReSNet-16-8+Jumpout on STL10.
Figure 5: Comparison of mean/variance drift when using (1 - p)-1, (1 - p)-0.5 and (1 - p)-0.75 as thedropout rescaling factor applied to y, when p = 0.1. The network is “CIFAR10(s)” (see Sec. 4). The left plotshows the empirical mean of y with dropout divided by the case without dropout (averaged over all layers), andthe second plot shows the similar ratio for the variance. Ideally, both ratios should be close to 1. As shown in theplots, (1 - p)-0.75 gives nice trade-offs between the mean and variance rescaling.
