Figure 1: (a) Diagram of asynchronous ER-based RL algorithms. (b) Neural network architectureemployed by RACER. Blue arrows connect each output with the elements of the actor-critic frameworkfor which it is used. Red arrows represent the flow of the error signals.
Figure 2: Cumulative rewards on OpenAI MuJoCo tasks for DDPG (green line), DDPG with rank-based PER (gray line), u-DDPG with regular ER (blue), and ReF-ER with C = 8 (purple), C = 4(red), C = 2 (orange). Implementation details in App. D.
Figure 3: Average Kullback-Leibler divergence between the policy πw = mw+N (0, σ2I) and thebehaviors μt in the RM during training for DDPG (green line), U-DDPG with regular ER (blue), andReF-ER with C = 8 (purple), C = 4 (red), C = 2 (orange).
Figure 4: Cumulative rewards on the OpenAI MuJoCo tasks for NAF (blue line), NAF with rank-basedPER (gray line), NAF with ReF-ER and C = 8 (purple), C = 4 (red), C = 2 (orange).
Figure 5: Average cumulative rewards on MuJoCo OpenAI Gym tasks obtained with PPO (dashedblack lines), ACER (dash-dot purple lines) and with RACER by independently varying the two mainhyper-parameters of ReF-ER: the RM size N and C (colored lines).
Figure 6: Mean cumulative rewards with RACER on OpenAI Gym tasks exploring the effect of thechoice of advantage parameterization, ReF-ER tolerance factor D, mini-batch size B, number oftime steps per gradient step F, and learning rate η. Dashed-black lines refer to the baseline parameters.
Figure 8: 20th and 80th percentiles of cumulative rewards for episodes of OpenAI Gym tasks endedwithin intervals of 2 ∙ 105 time steps across 5 independent training runs. The figures includes resultsfor PPO (black contours), RACER baseline (green contours, C = 4, N = 1019), RACER withconservative ReF-ER constraints and large RM size (blue contours, C = 2, N = 1020), and RACERwith relaxed ReF-ER constraints and smaller RM size (red contours, C = 8, N = 1018).
Figure 9: 20th and 80th percentiles of cumulative rewards for episodes of OpenAI Gym tasks endedwithin intervals of 2 ∙ 105 time steps across 5 independent training runs. The figures includes results forDDPG (green contours), DDPG with PER (red contours), and u-DDPG with ReF-ER (blue contours,C = 4, and all other hyper-parameters set according to App. D).
Figure 10: 20th and 80th percentiles of cumulative rewards for trajectories of DeepMind Control Suitetasks ended within intervals of 2 ∙ 105 time steps across 5 independent training runs using the baselineRACER hyper-parameters (Table 1).
Figure 11: 20th and 80th percentiles of cumulative rewards for trajectories of DeepMind Control Suitetasks ended within intervals of 2 ∙ 105 time steps across 5 independent training runs for U-DDPG withReF-ER, C = 4, and all other hyper-parameters set according to App. D.
