Figure 1: LeNet-300-100 on MNIST. (a) Test accuracy plotted against number of trainable parameters fordifferent methods. Dashed lines are used for full dense model and for compression methods, whereas allreparameterization methods maintaining a fixed sparsity level throughout training are represented by solid lines.
Figure 2: WRN-28-2 on CIFAR10. (a) Test accuracy plotted against number of trainable parameters fordifferent methods. Conventions same as in Figure 1a. (b) Breakdown of final sparsities emerged from ourdynamic sparse parameterization algorithm (Algorithm 1) at different levels of global sparsity. (c) A furtherbreakdown of final sparsities of individual residual blocks in the WRN groups at three scales, at the overallsparsity of 0.6. (d) Same as (c) at overall sparsity of 0.8. All results in (b, c &d) are mean and standard deviationfrom 5 runs.
Figure 3: Block-wise breakdown of final sparsities of Resnet-500.8. Similar to Figure 2c. (b) Same as (a) at overall sparsity of 0.92	3Residual group(b)1.0Overall sparsity: 0.98 6 4 2 00000A+ISJTSds .!0AT21	2	3	4Residual grouptrained on Imagenet. (a) At overall sparsitywas on par with compressed sparse, it lagged behind dynamic sparse at high sparsity levels. At lowsparsity levels SET largely closed the gap to compressed sparse.
Figure 4: Comparison to HashedNet. (a) Test accuracy for LeNet-300-100-10 trained on MNIST. (b) Testaccuracy for WRN-28-2 trained on CIFAR10. Conventions same as in Figure 1a.
Figure 5: Test accuracy for WRN-28-2 trained on CIFAR10 for two variants of dynamic sparse, i.e. kernel-levelgranularity of sparsity and non-structured (same as dynamic sparse in the main text), as well as the thin densebaseline. Conventions same as in Figure 1a.
Figure 6: Test accuracy for LeNet-300-100-10 on MNIST of dynamic sparse (i.e. Algorithm 1) comparedagainst constrained dynamic sparse for which parameter reallocation occurred only within, but not across, layers.
