Figure 1: Examples in character recognition datasets.
Figure 2: The LBPNet architecture. The LBP opera-tion generates feature maps with comparison and bit-allocation, while random projection fuses the interme-diate channels.
Figure 3: (a)A traditional local binary pattern. (b)-(d)Our learnable local binary patterns. The red arrows de-note pushing forces during training.
Figure 4: An example of an LBP operation onmultiple input channels. LBP operations for chan-nel (a) ch.a and (b) ch.b. Each pattern has foursampling points restricted in a 3-by-3 area.
Figure 5: An example of LBP channel fusing. Thetwo 4-bit responses from Fig. 3 are fused and as-signed to pixel s13 on the output feature map.
Figure 6: Basic LBPNet blocks. (a) the well-known building blockof residual networks. (b) The transition-type building block uses a1-by-1 convolutional layer for the channel fusion of a precedingLBP layer. (c) The multiplication and accumulation (MAC) freebuilding block for LBPNet.
Figure 7: The classification error trade-off(DET) curves of a 3-layer LBPNet and a 3-layer CNN on the INRIA pedestrian dataset(Dalal & Triggs, 2005). We plot the resultson Fig.8(a) of Dollar et al. (2Î¸09) for Com-parison with the other five approaches.
Figure 8: Error curves on benchmark datasets. (a) test errors on MNIST; (b) test errors on SVHN.
Figure 9: Sensitivity analysis of k w.r.t. training error on MNIST. (a) Training error; (b) training loss; (c) testerror.
