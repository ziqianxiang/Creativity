Figure 2: Experiment 1. Agents do associative and cause-effect reasoning from observational data. a) Aver-age reward earned by the agents tested in this experiment. See main text for details. b) Performance split bythe presence or absence of at least one parent (Parent and Orphan respectively) on the externally intervenednode. c) Quiz phase for a test DAG. Green (red) edges indicate a weight of +1 (-1). Black represents theintervened node, green (red) nodes indicate a positive (negative) value at that node, white indicates a zerovalue. The blue circles indicate the agent’s choice. Left panel: G and the nodes taking the mean valuesprescribed by p(X1:N\j|Xj = -5), including backward inference to the intervened node’s parent. TheOptimal Associative Baseline’s choice is consistent with maximizing these (incorrect) node values. Rightpanel: G→Xj=-5 and the nodes taking the mean values prescribed by p→Xj=-5 (Xi：N\j |Xj = -5). Wesee that the Passive-Conditional Agent’s choice is consistent with maximizing these (correct) node values.
Figure 1: Active and PassiveConditional AgentsFor (ii), we see the crucial result that the Passive-Conditional Agent’sperformance is significantly above the Optimal Associative Baseline,i.e. it performs better than what is possible using only correlations.
Figure 4: Experiment 2. Agents do cause-effect reasoning from interventional data. a) Average rewardearned by the agents tested in this experiment. See main text for details. b) Performance split by thepresence or absence of unobserved confounders (abbreviated as Conf. and Unconf. respectively) on theexternally intervened node. c) Quiz phase for a test DAG. See Fig. 2 for a legend. Here, the left panelshows the full G and the nodes taking the mean values prescribed by p(X1:N\j |Xj = -5). We see that thePassive-Cond Agent’s choice is consistent with choosing based on these (incorrect) node values. The rightpanel shows G→Xj=-5 and the nodes taking the mean values prescribed by p→Xj=-5 (Xi：N\j |Xj = -5).
Figure 3: Active and Passive In-terventional AgentsWe focus on three key questions in this experiment: (i) Can our agentslearn to do cause-effect reasoning from interventional data?, (ii) Howdoes the cause-effect reasoning in our agents which have access tointerventional data differ from the cause-effect reasoning measured inExperiment 1 (in agents that have access only to observational data)?(iii) In addition to making causal inferences, can our agent also choosegood actions in the information phase to generate the data it observes?For (i) we see in Fig. 4a that the Passive-Interventional Agent’s performance is comparable to the OptimalCause-Effect Baseline, indicating that it is able to do close to perfect cause-effect reasoning in this domain.
Figure 5: Experiment 3. Agents do counterfactual reasoning. a) Average reward earned by the agentstested in this experiment. See main text for details. b) Performance split by if the maximum node value inthe quiz phase is degenerate (Deg.) or distinct (Dist.). c) Quiz phase for an example test-DAG. See Fig. 2for a legend. Here, the left panel shows G→Xj =-5 and the nodes taking the mean values prescribed byp→Xj=-5 (Xi：N\j |Xj = -5). We see that the Passive-Int. Agent's choice is consistent with maximizingon these node values, where it makes a random choice between two nodes with the same value. Theright panel panel shows G→Xj=-5 and the nodes taking the exact values prescribed by the means ofp→Xj=-5 (Xi：N\j |Xj = -5), combined with the specific randomness inferred from the previous time step.
Figure 6: Active and PassiveCounterfactual Agents8Under review as a conference paper at ICLR 20195 Summary of resultsWe introduced and tested a framework for learning causal reasoning in various data settings—observational,interventional, and counterfactual—using deep meta-RL. Crucially, our approach did not require explicitencoding of formal principles of causal inference. Rather, by optimizing an agent to perform a task thatdepended on causal structure, the agent learned implicit strategies to use the available data for causalreasoning, including drawing inferences from passive observation, actively intervening, and makingcounterfactual predictions. Below, we summarize the keys results from each of the three experiments.
Figure 7: Reward distributionWe can also compare the performance of these agentsto two standard model-free RL baselines. The Q-totalagent learns a Q-value for each action across all steps forall the episodes. The Q-episode agent learns a Q-valuefor each action conditioned on the input at each timestep [ot,at-1,rt-1], but with no LSTM memory to storeprevious actions and observations. Since the relationshipbetween action and reward is random between episodes,Q-total was equivalent to selecting actions randomly, re-sulting in a considerably negative reward. The Q-episodeagent essentially makes sure to not choose the arm thatis indicated by mt to be the external intervention (whichis assured to be equal to -5), and essentially choosesrandomly otherwise, giving an average reward of 0.
Figure 9: (a) Comparing agent performances with different data. graph in the training set, which could(b) Comparing information phase intervention policies.	result in a type of overfitting. We there-fore ran a new set of experiments wherethe entire equivalence class of each test graph was held out from the training set11. Performance on the testset therefore indicates generalization of the inference procedures learned to previously unseen equivalenceclasses of causal DAGs. For these experiments, we used graphs with N = 6 nodes, because 5-node graphshave too few equivalence classes to partition in this way. All other details were the same as in the mainpaper.
Figure 10: (a): Directed acyclic graph. The node X3 is acollider on the path Xi → X3 J X2 and a non-collider onthe path X2 → X3 → X4. (b): Cyclic graph obtained from(a) by adding a link from X4 to X1.
