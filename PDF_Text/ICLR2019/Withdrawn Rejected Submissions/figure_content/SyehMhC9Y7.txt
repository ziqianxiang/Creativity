Figure 1: We apply our approach to navigation in CARLA (Dosovitskiy et al., 2017). Columns1,2: Images depicting the current scene. The overhead image depicts a 50 m2 area. Column 3:LIDAR input and goals are provided to our deep imitative trajectory model, and plans to the goalsare computed under the model’s likelihood objective, and colored according to their ranking underthe objective, with red indicating the best plan. The red square indicates the chosen high-level goal,and the yellow cross indicates a point along our plan used as a setpoint for a PID controller. TheLIDAR map is 100 m2, and each goal is ≥20 m away from the vehicle. Column 4: Our model canincorporate arbitrary test-time costs, and use them to adjust its planning objective and plan ranking.
Figure 2: A brief taxonomy of learning-basedcontrol methods. In our scenario, we avoid on-line data collection, specifically from the policywe seek to imitate. We structure our imitationlearner with a model to make it flexible to newtasks at test time. We compare against other of-fline approaches (front face).
Figure 3: Planning to a sequence of goals (here, 10) allows for more control over the inferred paths.
Figure 4: Imitative planning to goals subject to a cost at test time. The cost bumps corresponds tosimulated “potholes,” which the imitative planner is tasked with avoiding. The imitative planner gen-erates and prefers routes that curve around the potholes, stay on the road, and respect intersections.
Figure 5: Architecture of mt and σt ,modified from (Rhinehart et al., 2018)with permission.
Figure 6: Illustration of our method applied to autonomous driving. Our method trains an ImitativeModel from a dataset of expert examples. After training, the model is repurposed as an ImitativePlanner. At test time, a route planner provides waypoints to the Imitative Planner, which computesexpert-like paths to each goal. The best plan chosen according to the planning objective, and pro-vided to a low-level PID-controller in order to produce steering and throttle actions.
Figure 7: Test-time pothole planning. The preferred plans steer left around the simulated potholes.
Figure 8: Tolerating bad waypoints. The planner prefers waypoints in the distribution of expertbehavior: on the road at a reasonable distance. Columns 1,2: Planning with 1/2 decoy waypoints.
