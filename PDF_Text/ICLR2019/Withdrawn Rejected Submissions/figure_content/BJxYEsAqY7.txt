Figure 1: The sequential FEED. The first FT stage is the same as the original work in (Kim et al.,2018), except that the teacher network and the student network have the same architecture. The dottedline means that the trained student network is used as a teacher network for the next stage. Thistraining procedure is repeatedly performed over several stages to deliver ensemble knowledge to thesuccessive student.
Figure 2: Illustration of our second proposed algorithm and KD to compare the differences. (a) Fullview of our second proposed training procedure, the parallel FEED. (b) Full view of the trainingprocedure of KDlearn the knowledge of the teacher network. This motivation provides us chances to propose severalmodified versions of existing methods. In this section, we explain the two feature-level ensembletraining algorithms that we use for boosting the performance of a student network without introducingany additional calculations at test time. The proposed methods are collectively called as FEED whichis an abbreviation for the Feature-level Ensemble Effect for knowledge Distillation.
Figure 3: Paraphraser reconstruction lossLrec (training) for Resnet-56 with sFEED.
Figure 4: Paraphraser reconstruction lossLrec(training) for WRN28-10 with pFEED.
