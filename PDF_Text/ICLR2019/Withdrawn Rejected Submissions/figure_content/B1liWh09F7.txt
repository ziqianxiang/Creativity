Figure 1: SALSA-TEXT generator and discriminator architecture designed using Transformerencoder structureGAN-based image generation, especially proved in SAGAN (Zhang et al., 2018), we apply it to theweights of the discriminator and the generator in our network. We did not find layer normalization(used in original Transformer) to be useful, when applied along with spectral normalization in thegenerator and discriminator architectures. Hence, only use spectral normalization in our GANstructures.
Figure 2: SALSA-AAE for text generation. As explained in the figure, Transformer-based encoder,decoder and discriminator (with self attention) architectures are used. Decoder uses fixed-size fullcodes.
Figure 3: SALSA-ARAE for text generation. Similarly to SALSA-AAE, all blocks are Transformer-based and decoder uses fixed-size full codes. Generator and discriminator comprise of self-attentionlayers.
