Figure 1: Proposed Positive Unlabeled system: Divergent-GAN Learning model. xF N representsthe fake generated samples which are similar to real negative samples xN . G is the generativemodel. D is the discriminator. C is the classifier used to perform the binary Positive-Negative (PN)classification.
Figure 2: This is a minibatch of fake negative images xFN generated, after 20 epochs iteration, withthe D-GAN based on the Loss-Sensitive GAN (Qi, 2017) and by replacing BN with LN. The PUtraining dataset uses the celebA dataset images with settings ρ = 0.5, πP = 0.3 and ”Male” thepositive class chosen arbitrarily. The images size is cropped to 64*64*3. All generated samples arequalitatively relevant fake counter-examples.
Figure 3: D-GAN functioning on clusters of 2D points such that D and G have a multilayer-perceptron structure. (a), (b) and (c) are respectively sets of labeled positive, unlabeled, and gener-ated samples. The generated samples only follow the distribution of the counter-examples.
Figure 4: Normalization effect on the histogram distribution of output values predicted by D on anunlabeled samples minibatch. (a) is with BN, (b) without any normalization, and (c) with LN. Theaxe in depth represents the training iterations. The lighter histogram corresponds to the last trainingiteration. Settings are with positive class 8 and negative class 3 of MNIST dataset, with ρ = 0.5and πP = 0.3. We observe that BN in the figure 4(a) does not enable to distinguish positive fromnegative samples when the discriminator is trained on a PU dataset. On the contrary, LN (figure4(c)) enables it for the reasons specified in the subsection 3.2. Thus with LN in D, G can convergeexclusively towards the unlabeled negative samples distribution pN , as expected.
Figure 5: Classifier test Accuracy evolution in function of the GAN epochs (first stage). 8 vs. RestMNIST task, with ρ = 0.5 and πp = 0.5. (a) D-GAN and PGAN are trained without normalizationlayers. (b) D-GAN and PGAN are respectively trained with LN and BN inside the discriminator.
Figure 6:	Link between the PN loss function suggested (equation 6) and the distribution of thediscriminator output predictions for an unlabeled minibatch. For this experiement, l is the meansquared error (MSE) and D is a multi-layer perceptron. D has been trained to distinguish a 2Dgaussian distribution to another one by using the risk RP U on a PU dataset. (a) Shows a set of2D points considered as positive samples. (b) Shows a set of 2D points considered as unlabeledsamples. Both curves in (c) and (d) have been normalized to get a better visualization. For (c),pY (D(xU)) (in blue) represents the probability distribution of D predicted outputs for a minibatchof unlabeled samples, with πP = 0.5. RPN (D, δ) (in red) represents the PN risk computed infunction of δ with the equation 6 on a minibatch of positive and negative labeled samples, once D istrained with RPU risk. (d) shows the same curves as in (c) but by giving in input a concatenation ofan unlabeled minibatch with a positive labeled minibatch. Unlabeled and labeled positive samplesprovide a unified prediction output distribution.
Figure 7:	D-GAN Noisy-labeled version functioning on sets of 2D points such that (a), (b), (c)and (d) are respectively sets of real noisy labeled positive (20 percents of noise), real noisy labelednegative (20 percents of noise), fake generated positive and fake generated negative samples. D , Gand GP have a multilayer-perceptron structure. The generated samples only follow the ditributionof the well-labeled samples of the input sets.
Figure 8:	Details of figure 4 Batch Normalization effect on discriminator output predictions ondistinct Positive and Unlabeled minibatches. Settings are with positive class 8 and negative class3 of MNIST dataset, with ρ = 0.5 and πP = 0.3. The results on the first row are done withBN, without on the second row, and with LN on the third row. The first column (a) is the meanD loss value estimated in function of training iterations. The second column (b) is the histogramdistributions of output values predicted by D on the positive and unlabeled minibatches. The thirdcolumn (c) is the histogram distribution of output values predicted by D on the positive minibatch.
Figure 9: Minibatch of counter-examples (women faces) generated after 20 epochs with the pro-posed D-GAN framework adapted to the LSGAN-GP variant. The D-GAN is applied on a PUcelebA dataset (celebrity RGB images of size 64*64), with 30% of positive samples (men faces)among the unlabeled ones. We can visually observe that 100% of the generated samples fol-low the counter-examples (women faces) distribution. The LSGAN-GP code is available at:https : //github.com/maple - research - lab/lsgan - gp - alt.
