Figure 1: Word vector space for corpus C. Words in different partitions are represented by differentsubscripts and separated by hyperplanes. Bold fonts represent words’ presence in the document3 Motivation: Averaging vs Partition AveragingLet’s consider a corpus (C) with N documents with the corresponding most frequent words vocab-ulary (V ). Figure 1, represents the word-vectors space of V , where similar meaning words occurcloser to each other. We can apply sparse coding to partition the word vectors space to a five (totaltopics K = 5) topic vector space, as shown in figure 1. These five topic vector spaces represent thefive topics present in corpus (C). Few words are polysemic and belong to multiple topics with someproportion (see Table 1). In figure 1 we represent the topic number of the word in subscript. Let’sconsider a document dn: ‘Data journalists deliver data science news to general public. They oftentake part in interpreting the data models. In addition, they create graphical designs and interviewthe directors and CEO’s.‘One can directly average the word vectors to represent the document (~vdn), as in (SIF), shown inequation equation 1. Here, + represents element-wise vector addition.
Figure 2: Relative performance improvement pf P-SIF over SIF in accuracy (P-SSF-SIF % ) w.r.taverage numbers of words in documents for 27 sts (sentence textual similarity) datasets.
Figure 3: Word vector space for corpus C. Words in different topic is represented by different sub-script and separated by hyperplanes. Bold represent words from example documents.
