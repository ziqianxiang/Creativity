Figure 1: #Steps vs. Model Size Across Different Application Domains:X-axis represents modelsize in terms of the number of parameters in log-scale. Y-axis is the number of steps to minimumvalidation loss. Different lines represent different dataset sizes (percentage of full dataset). Forcharacter LM, we vary the dataset size from 0.01% to 0.4% of the 1B dataset. For word LM, wevary dataset size from 0.1% to 10% of 1B dataset. For speech, we vary the dataset size from 1% to10% of an internal 20000 hour dataset.
Figure 2: Learning Rate Impact on #Steps.
Figure 3: Depth/Dataset Size Impact on #StepsDifferent lines presents different dataset sizes. We can make the following observations from thesefigures:•	The number of steps to convergence drops and the drop-down pattern is well approximatedwith power law relationship (a * Model_Size-k + b).
Figure 4: When growing model size (parameter count), the number of training epochs to convergedeclines until reaching a minimum number of passes through the training data.
Figure 5: #Steps to Convergence and Accuracy at Convergence vs. Model Size (DistributionAcross 10 Runs.)This behavior is expected and can be explained theoretically (see Section B). One interpretation ofthis is that in higher dimensional space the number of locals minimas that are as good as globalminimas exponentially increases ( Sagun et al. (2014)) and if they are not symmetrically distibutedwrt. origin where the initial point lives around, there exists a potential for the direct distance to getshorter.
Figure 6: Distribution of Direct Distance from Initial to Final Weights In the Weight Space(Across 10 Runs.)6×1015×101Model Size (million Parameters)(d) All Layers3.	Weights within hidden layers take larger steps than weights within softmax layer and em-bedding layer. This can be partially contributed to larger number of weights in hiddendimension.
Figure 7: Step size across multiple layers(Distribution Across 10 Runs.)(d) All Layersfaster despite the increasing computational requirements of each training step. We study the effectof network structure on halting time and show that larger models—wider models in particular—takefewer training steps to converge. Results show that halting time improves when growing model’swidth for three different applications, and the improvement comes from each factor: The distancefrom initialized weights to converged weights shrinks with a power-law-like relationship, the averagestep size grows with a power-law-like relationship, and gradient vectors become more aligned witheach other during traversal.
Figure 8:	Average Angle Between Gradient Vector and the path connecting the Current Weightto Initialization Point (Distribution Across 10 Runs.)9Under review as a conference paper at ICLR 2019ReferencesDario Amodei, Rishita Anubhai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Jing-Dong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, et al. Deep Speech 2: End-to-EndSpeech Recognition in English and Mandarin. In Proceedings of The International Conferenceon Machine Learning (ICML), pp. 173-182, 2016.
Figure 9:	Width=1A B enchmarksWe evaluate our finding on three established deep learning models: character-level LM, word-levelLM and speech recognition. Language models (LMs) aim to predict probability distributions for thenext character, word, or other textual grams conditioned on previous sequence of input text. LMsare very important model features for domains such as speech recognition and machine translation,helping to identify most probable sequences of grams. Relative to other machine learning domains,LMs have low-dimensional input and output spaces, and can be trained with very large labeled sets.
Figure 10: Width=2B Theoretical ResultsWe begin with a simple example to show the effect of width on D0 - the average distance frominitial weights to final weights within the weight space. For a simple linear neural network with L2loss, we show that the increase in width results in reduction in D0 .
