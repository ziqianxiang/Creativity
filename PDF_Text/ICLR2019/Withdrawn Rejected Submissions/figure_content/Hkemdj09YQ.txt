Figure 1: Comparison of attribution methods. See Section 5 for details on the visualization.
Figure 2: Feature map visualization for an image with a noisy saliency map.
Figure 3: Impact of background feature activation occlusion on the final decision.
Figure 4: Saliency maps produced from a CNN trained on occluded images. The upper left cornerof all the images in the training dataset is replaced with a 10 × 10 random patch, as shown above.
Figure 5: Effect of threshold τ (columns) on RectGrad for 3 images of the cabbage butterfly classin ImageNet (rows). The second column shows attribution maps with τ = 0, which is equivalent toGuided Backpropagation * Input. For the following columns, τ is set to qth percentile of importancescores. The padding trick was used for all attribution maps above.
Figure 6: Evaluation of coherence across different classes without and with final thresholding.
Figure 7: Comparison of attribution maps for images (left column) and their adversarial examples(right column) without and with final thresholding. This figure shows examples where attributionmaps produced by RectGrad changed significantly.
Figure 8: Comparison of amount of attribution on occluded patch. The left and right charts comparethe amount of attribution inside occluded patch without and with final thresholding respectively. Thenumbers in parentheses show the custom threshold levels.
Figure 9: Larger-scale study of the impact of background feature activation occlusion on the finaldecision.
Figure 10: Saliency map and RectGrad attributions at Inception v4 intermediate layers as they arepropagated toward the input layer. We show channel-wise average attributions for hidden layerinputs with respect to the output layer. For each subfigure, first row shows the input image andSaliency map and RectGrad attribution maps. Second and third rows show Saliency map and Rect-Grad attributions at intermediate layers, respectively. An attribution map is closer to the output layerif it is closer to the right.
Figure 11: Evaluation of coherence within the same class (rows) without and with final thresholding.
Figure 12: Comparison of attribution maps for images (left column) and their adversarial examples(right column) without and with final thresholding. This figure shows examples where attributionmaps produced by RectGrad did not change significantly.
Figure 13: Comparison of amount of attribution on the background. The left and right charts com-pare the amount of attribution outside mask (on background) without and with final thresholdingrespectively.
Figure 14: Comparison of average total variation. The left and right charts compare average totalvariation without and with final thresholding respectively.
Figure 15: Comparison of Sensitivity. The left plot compares RectGrad with local attribution meth-ods and the right plot with with global attribution methods. We also include the random baseline(patches are randomly removed) for reference. Lower AUC indicates a better attribution method.
Figure 16: Comparison of Sensitivity after final thresholding. The left plot compares RectGrad withlocal attribution methods and the right plot with with global attribution methods. We also includethe random baseline (patches are randomly removed) for reference. Lower AUC indicates a betterattribution method. We took the average over 500 randomly chosen test set images.
Figure 17: Comparison of ROAR. The left plot compares RectGrad with local attribution methodsand the right plot with with global attribution methods. We also include the random baseline (pixelsare randomly removed) for reference. Lower AUC indicates a better attribution method.
Figure 18: Comparison of KAR. The left plot compares RectGrad with local attribution methodsand the right plot with with global attribution methods. We also include the random baseline (pixelsare randomly removed) for reference. Higher AUC indicates a better attribution method.
Figure 19: Comparison of attribution methods in sensitivity. The first row shows the image as topN 2 × 2 patches are occluded according to RectGrad. The second and third rows show the positiveparts (indicated by +) of RectGrad and Gradient * Input attribution maps as top N 2 × 2 patches areoccluded respectively. We did not cap outlying values in this visualization.
Figure 20: RectGrad attribution maps produced from a CNN trained on images occluded accordingto RectGrad. We show images whose original and occluded versions were both classified correctly.
