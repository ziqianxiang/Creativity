Figure 1: An annotated PTB example sentence.
Figure 2: POS and CCG tagging accuracies for different amounts of LSTM encoder and classifiertraining data. We show results for the best performing layer of each model. Note, BiLMs are dis-played with the attention models and forward LMs are displayed with the models without attention.
Figure 3:	POS and CCG tagging accuracies for different amounts of classifier training data in termsof percentage points over the word-conditional most frequent class (WC-MFC) baseline. We showresults for the best performing layer and model for each task.
Figure 4:	POS and CCG tagging accuracies in terms of percentage points over the word-conditionalmost frequent class baseline. We display results for the best performing models for each task.
Figure 5: Performance of classifiers trained to predict the identity of the word a fixed number oftimesteps away. Note, the forward LM has asymmetrical access to this information in its input.
Figure 6: Here we display results for the word identity prediction task with randomly initializedLSTM encoders with up to 4 layers. Lower layers have a more peaked shape and upper layers amore flat shape, meaning that the lower layers encode relatively more nearby neighboring wordinformation, while upper layers encode relatively more distant neighboring word information.
