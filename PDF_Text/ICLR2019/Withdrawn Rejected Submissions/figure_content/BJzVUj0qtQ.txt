Figure 1: Demonstration of the attention shift phenomenon of the defense models compared withnormally trained models. We adopt class activation mapping (Zhou et al., 2016) to visualize theattentive regions of three normally trained modelsâ€”Inception v3 (Szegedy et al., 2016), InceptionResNet v2 (Szegedy et al., 2017), ResNet 152 (He et al., 2016a) and four defense models (Trameret al., 2018; Liao et al., 2018; Xie et al., 2018a; Guo et al., 2018). These defense models focus theirattention on slightly different regions compared with normally trained models, which may affect thetransferability of adversarial examples.
Figure 2: The adversarial examples generated for Inc-v3 using FGSM and A-FGSM.
Figure 3: The success rates (%) of the adversarial examples generated for Inc-v3 agasinst IncRes-v2ens, HGD, R&P, TVM and NIPS-r3, with the kernel length ranging from 1 to 21.
Figure 4: The adversarial examples generated for Inc-v3 by A-FGSM with different kernel sizes.
