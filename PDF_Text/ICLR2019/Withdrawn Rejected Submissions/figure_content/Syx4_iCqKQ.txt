Figure 1: Illustrative overview of polar prototype networks for classification and regressionwith a 3-dimensional output space. For classification, we position class polar prototypes (six inFig. 1a) in the output space with maximal separation prior to learning. The output space is deter-mined by maximal polar similarity to the prototypes. For regression, we perform a polar interpo-lation of two opposing prototypes, denoting the lower and upper regression bounds (pl and pu inFig. 1b). Note that the output space is visualized as a sphere for ease of visualization. We do notneed to project output vectors explicitly onto the hypersphere during training or inference.
Figure 2: Illustration of polar proto-type networks for regression with 3Doutputs. For a training example, wecompute the cosine similarity betweenthe output prediction (zi) and polar up-per bound (pu). This value is comparedto the expected similarity (ri) and dif-ference between the values determinesThe loss function of Eq. 5 computes a squared loss be- the gradient direction and magnitude.
Figure 3: Minimal amount of output dimensions in polar prototype networks. For CIFAR-10 andCIFAR-100, we can largely reduce the output space with minimal loss in performance. We also com-pare to softmax cross-entropy and find that the performance is similar. This experiment highlightsthe ability of our approach to provide effective results with the simplest of output structures.
Figure 4: Joint regression and classification in the same outputspace on a rotated MNIST subset. The regression and classificationstructure is shown separately for ease of visualization, we explicitlynote that the space is identical. Rotation is along the z-axis (colordenotes regression value), while classification is on the (x, y)-plane(color denotes class). With polar prototype networks, the outputspace can disentangle different tasks in a structured manner.
