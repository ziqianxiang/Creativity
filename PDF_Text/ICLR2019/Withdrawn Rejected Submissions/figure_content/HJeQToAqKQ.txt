Figure 1: Graphical models.
Figure 2: Modified graphical model for world Q, instead of Figure 3b, the world we desire whichsatisfies the joint density in Equation 41. Notice that this graphical model encodes all of the sameconditional independencies as the original.
Figure 3: Graphical models for standard Bayesian inference.
Figure 4: Graphical models for the traditional discriminative case.
Figure 5: Hand-crafted optimal model. Toy Model illustrating the difference between selected pointson the three dimensional optimal surface defined by γ, δ, and σ. See Section 3 for more description of theobjectives, and Appendix H for details on the experiment setup. Top (i): Three distributions in data space:the true data distribution, p(x), the model’s generative distribution, g(x) = z q(z)q(x|z), and the empiricaldata reconstruction distribution, d(x) = Px0 Pz p(x0)q(z|x0)q(x|z). Middle (ii): Four distributions in latentspace: the learned (or computed) marginal q(z), the empirical induced marginal e(z) = Px p(x)q(z|x), theempirical distribution over z values for data vectors in the set X0 = {xn : zn = 0}, which we denote bye(z0) in purple, and the empirical distribution over z values for data vectors in the set X1 = {xn : zn = 1},which we denote by e(z1) in yellow. Bottom: Three K × K distributions: (iii) q(z|x), (iv) q(x|z) and (v)q(x0 |x) = Pz q(z|x)q(x0|z).
Figure 6: Supervised Learning approaches.
Figure 7: VIB style objectives.
Figure 8: Autoencoder objectives.
Figure 9: VAE style objectives.
Figure 10: Full Objective. σ = 0.5, γ = 1000, δ = 1, ρ = 0.9. Simple demonstration of the behaviorwith all terms present in the objective.
