Figure 1: SENSE-S (add) architectureSoftmax-SP-JoM 6u'=0qs"usds0 sepou 6u'=0qs"usds0__________S ,JOsθ> p」。M UJ-P，M A ∙JO1OΘ> xθrθ>∙≡'uFigure 2: SENSE-S (concat) architectureSQPOU 6u'=0qs"usds0SP-JoM 6u'=0qs"usds01graph. On the other hand, Le & Mikolov (2014) extends skip-gram models to learn embeddings ofvarious chunks of text, e.g., sentences, paragraphs or entire documents, via sampling the neighboringwords over a sliding window within the text. Motivated by the effectiveness of these models, webuild two SENSE-S models, SENSE-S (add) and SENSE-S (concat), as detailed below.
Figure 2: SENSE-S (concat) architectureSQPOU 6u'=0qs"usds0SP-JoM 6u'=0qs"usds01graph. On the other hand, Le & Mikolov (2014) extends skip-gram models to learn embeddings ofvarious chunks of text, e.g., sentences, paragraphs or entire documents, via sampling the neighboringwords over a sliding window within the text. Motivated by the effectiveness of these models, webuild two SENSE-S models, SENSE-S (add) and SENSE-S (concat), as detailed below.
Figure 3: Multi-label classification on Wikispeedia dataset for varying Train:Validation:Test splitsof data. Document length = first 500 characters (n2v: node2vec, p2v (add)/(cat): paragraph2vec(add)/(concat), n2v w/ p2v: node2vec with paragraph2vec (add) initialization, p2v w/ n2v: para-graph2vec (add) with node2vec initialization, IteraVec: Iterative Vectorization, cat(p2v,n2v): con-catenation of p2v and n2v vectors).
Figure 4: Link prediction inCitation Network.
Figure 5: Accuracy of decoding a node sequence vector (Dim: dimension of the vectors).
