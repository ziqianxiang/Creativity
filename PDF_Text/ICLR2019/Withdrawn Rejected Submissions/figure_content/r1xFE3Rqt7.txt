Figure 1: A toy classification problem with a rank-1 factorization of the weight matrices. (b) and (c)are distributions of 2D data in the 1D projected space. The linear projection to lower dimension leadsto significant information loss (results in 83% classification accuracy), while our proposed approachlearns to adaptively avoid this (achieving 97% classification accuracy). (c) is the distribution ofprojection through a random matrix followed by tanh.
Figure 2: Illustration of different types linear projection weights V colored by responses to aparticular input h: (a) A data-independent non-adaptive weight matrix, (b) fully adaptive weightmatrix which can be very expensive, (c) the proposed adaptive mixture approach.
Figure 3: (a) regular factorization and (b) adaptive mixture of low-rank factorizations. First computeZk = ∏k(h)((V(k))Th) and then h0 = Pk U⑻Zk, where Z can be treated as the middle layer.
Figure 4: Visualization of low-rank projected 2D space. Non-adaptive versus adaptive low-rank, inboth original 2-d space and TSNE enhanced 2D space. With adaptive mixtures, we observe betterseparation among data points of different classes, closely positioning of the data of the same class.
Figure 5: FLOPs vs. perplexity. The horizontal line is the full LSTM’s baseline accuracy. Wealso compare variants of the proposed approaches with regular low-rank factorization, indicated bydifferent colors and markers. Lower perplexity is better.
Figure 6: Perplexity vs. number of mixing components. Different curves denote for different rank-d,as a ratio to the averaged input and output dims, i.e. m+dn.
Figure 7: Visualization of the mixtures from the last bottleneck layer in our MobileNet-CIFAR (1/4size). Each row is averaged from one of the 10 classes. The mixtures show a clear class-discriminativepattern. (Best viewed in color.)Visualization of Mixture. To see whether the mixtures π generated for each sample providesclass-discriminative information, we visualize the values of mixtures in CIFAR experiments. Werecord all the mixing weights on CIFAR-10 validation set and average them for each classes. Theresults are shown in Fig 7, we find the distribution of the mixtures are different for each classes. Itshows that the adaptive mixture is able to capture class-discriminative features.
Figure 9: Illustration of low-rank decomposition in depth-wise separable convolutions. No non-linearity is added in the bottleneck.
Figure 8: Number of parameters Vs perplexity. The horizontal line is for the full matrix baseline, anddifferent color/ShaPe indicating different methods: including the non-adaptive and adaptive low-rankfactorizations, with three ways of generating the adaptive maxing weights. The smaller perplexity ismore desirable.
Figure 10: The connection between MobileNet V1 and V2. MObileNet-V2 can be regarded as thelow-rank factorized version ofV1, where residual connection is added in the bottleneck.
