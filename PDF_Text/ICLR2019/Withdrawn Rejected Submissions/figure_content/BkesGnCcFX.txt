Figure 1: Plappert et al. (2018) introduce challenging tasks on the Fetch robot and the ShadowDextrous hand. We use these tasks for our experiments. Images are taken from the technical report.
Figure 2: For the Fetch tasks, we compare our method (red) against HER (blue) (Andrychowiczet al., 2016) and FWRL (green) (Kaelbling, 1993) on the distance-from-goal and success rate met-rics. Both metrics are plotted against two progress measures: the number of training epochs and thenumber of reward computations. Except for the Fetch Slide task, we achieve comparable or betterperformance across the metrics and progress measures.
Figure 3:	For the hand tasks, we compare our method (red) against HER (blue) (Andrychowicz et al.,2016) and FWRL (green) (Kaelbling, 1993) for the distance-from-goal and success rate metrics.
Figure 4:	(a) Effects of removing the step-loss from our methods. Results show that it is a criticalcomponent to learning in the absence of goal-rewards. (b) Adding goal-rewards to our algorithmthat does have an effect further displaying how they are avoidable.
Figure 5:	We measure the sensitive of HER and our method to the dsitance-threshold () with respectto the success-rate and distance-from-goal metrics. Both algorithms success-rate is sensitive thethreshold while only HERâ€™s distance-from-goal is affected by it.
Figure 7: Even when the Goal rewards areremoved from HER Andrychowicz et al.
Figure 6: Ablation on loss functions for FetchPush task. The Floyd-Warshall inspired lossfunctions Llo and Lup do not help much. Lstephelps a little but only in conjunction withHER Andrychowicz et al. (2016).
