Figure 1: The overall architecture of existing models and our model.
Figure 2: VampPrior VAE. h is a representation of input text from the encoder. u1 , . . . , uK arepseudo-inputs. This model is our modified version of VampPrior VAE. The red and green arrowsrepresent neural networks with shared weights.
Figure 3: A self-attention encoder. This model encodes variable length input into a fixed lengthrepresentation using an attention mechanism. The fixed length representation is acquired by summingup the hidden states of the bi-directional LSTM based on attention weights. Attention weightsas1, . . . , asn are calculated by (as1, . . . , asn) = softmax(ws2 tanh(W1HT)).
Figure 4: Visualized attention weight of the self-attention encoder. We show the maximum attentionweight for each word. Darker red represents a larger attention weight. Interrogatives and prepositionsare assigned larger weights compared to nouns.
Figure 5: The mean of each component of the multimodal prior distribution, visualized with t-SNE.
Figure 6: This Gaussian mixture distribution (blue line) has two equally weighted components withmeans 2.0 and -2.0, and variance 1.0. The distribution (green line) to the left is N (2.0, 0.3) and to theright is N(0, 5). The Kullback-Leibler divergence KL(qIP) between Gaussian mixture distribution Pand normal distribution q of the left image is 1.4, and the right image is 12.4.
