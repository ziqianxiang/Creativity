Figure 1: Difference between ReLU model studied in Livni et al. (2014); DasGupta et al. (1994) andtypical fully connected counterpartAnother interesting line of research in understanding the hardness of training ReLU neural networksassumes that data is coming from some distribution. More recent work in this direction includeShamir (2016) which shows a smooth family of functions for which gradient of squared error func-tion is not informative while training neural network over Gaussian input distribution. Song et al.
Figure 2: (2,1)-ReLU Neural Network. Also called 2-ReLU NN after dropping ‘1’. Here ReLUfunction is presented in each node to specify the type of activation function at the output of eachnode.
Figure 3: Gadget: Blue points represent set T1 and red points represent set T05Under review as a conference paper at ICLR 2019So this is a polynomial time reduction.
Figure 4: X-axis in figures above is output of the first layer of 2-ReLU NN i.e. w1 l1 (π) + +w2 l2 (π) +. Y-axis is the output of second hidden layer node. Since output of first hidden layergoes to input of second hidden layer, we are essentially trying to fit ReLU node of second hiddenlayer. In particular, red and blue dots represent output of first hidden layer on data points with label1 and 0 respectively. In fig (a) we see that hard-sorted input can be classified as 0/1 by a ReLUfunction. In fig (b) and (c) we see that input which is not hard-sorted can not be classified exactly as0/1 by a ReLU function.
