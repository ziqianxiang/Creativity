Figure 1: Starting from a dataset of demonstration videos, without expert actions, MetaMimic learns a one-shothigh-fidelity imitation policy π(ot,gt) with off-policy RL. This policy, represented with a massive deep neuralnetwork, enables the robot arm to mimic any demonstration in one-shot. In addition to producing an imitationpolicy that generalizes well, MetaMimic populates its replay memory with all its rich experiences, including notonly the demonstration videos, but also its past observations, actions and rewards. By harnessing these augmentedexperiences, a task policy π(ot) can be trained to solve difficult sparse-reward control tasks.
Figure 2: Imitation actor algorithm (left) and illustration of the environment and stochastic task (right).
Figure 3: Network architecture. Since high-fidelity imitation is a fine-grained perception task, we found itnecessary to use large convolutional neural networks. Our best network consists of a residual network (He et al.,2016) with twenty convolutional layers, instance normalization (Ulyanov et al.) between convolutional layers,layer normalization (Ba et al., 2016) between fully connected layers, and exponential linear units (Clevert et al.,2015). We use a similar network architecture for the imitation policy and task policy, however the task policydoes not receive a goal gt .
Figure 4: One-shot high-fidelity imitation. Given novel, diverse, test-set demonstration videos (three examplesare shown above), the imitation policy is able to closely mimic the human demonstrator. In particular, itsuccessfully maps image observations to arm velocities while managing the complex interaction forces amongthe arm, blocks and ground.
Figure 5: More demonstrations improve generalization when using the imitation policy. The goal ofone-shot high-fidelity imitation is generalization to novel demonstrations. We analyze how generalizationperformance increases as we increase the number of demonstrations in the training set: 10 (—), 50 (—), 100(—), 500 (—). With 10 demonstrations, the policy is able to closely mimic the training set, but generalizespoorly. With 500 demonstrations, the policy has more difficultly closely mimicking all trajectories, but has similarimitation reward on train and validation sets. The figure also shows that higher imitation reward when followingthe imitation policy (B ottom Row) results in higher task reward (Top Row). Here we normalize the task rewardby dividing itby the average of demonstration cumulative reward.
Figure 6: Larger networks and normalization improve rewards in high-fidelity imitation. We comparethe ResNet model used by the IMPALA agent (15 conv layers, 32 channels) (—) with the much larger networksused by MetaMimic inspired by ResNet34 (20 conv layers, 512 channels) with (—) and without (—) instancenormalization. We use three metrics for this comparison: task reward (Left), imitation reward when trackingpixels (Middle) and imitation reward when tracking arm joint positions and velocities (Right). We findlarge neural networks, and normalization significantly improve performance for high-fidelity imitation. To ourknowledge, MetaMimic uses the largest neural network trained end-to-end using reinforcement learning.
Figure 7: Efficient task policy. The task policy (which does not condition on a demonstration) is able to outper-form the demonstration videos. We test this by initializing the environment to the same initial state as a demonstra-tion from the training-set. The task policy is able to stack within 50 frames, while the demonstration stacks in 200frames. The task policy has no incentive to behave like the demonstration, so it often lays its arm down after stacking.
Figure 8: Comparing task policies. We compareMetaMimic (—) to demonstrations (—), D4PG (—),and two methods that use demonstrations with accessto additional information: D4PGfD (—), D4PG with ademonstration curriculum (- -), and MetaMimic with acurriculum (- -). D4PG is not able to reach the perfor-mance of the demonstrations. The methods with accessto additional information are able to quickly outperformthe demonstrators. MetaMimic matches their perfor-mance even without access to expert rewards or actions.
