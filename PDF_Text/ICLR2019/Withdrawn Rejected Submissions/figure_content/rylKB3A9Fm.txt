Figure 1: Schematic of the three versions of an environment.
Figure 2: MountainCar: heatmap of the rewards achieved by A2C with the FF architecture on DRand DE. The axes are the two environment parameters varied in R and E.
Figure 3: Pendulum: heatmap of the rewards achieved by A2C with the FF architecture on DR andDE. The axes are the two environment parameters varied in R and E.
Figure 4: Training curves for the PPO-based algorithms on CartPole, all three environment versions.
Figure 5: Video frames of agents trained with A2C on HalfCheetah, trained in the Deterministic (D),Random (R), and Extreme (E) settings (from top to bottom). All agents evaluated in the D setting.
Figure 6: Video frames of agents trained with PPO on HalfCheetah, trained in the Deterministic (D),Random (R), and Extreme (E) settings (from top to bottom). All agents evaluated in the D setting.
