Figure 1: Illustration of our proposed MAXL framework. The Multi-task evaluator takes an inputimage and is trained to predict both the principal class (e.g. Dog), and the auxiliary class (e.g.
Figure 2: (a) Illustration of the two networks which make up our meta auxiliary learning algorithm.
Figure 3:	Learning curves for the CIFAR100 test dataset, comparing MAXL with baseline methods.
Figure 4:	Performance improvement in percentages when training with MAXL compared withsingle-task learning, with 10 principal classes and a range of auxiliary classes.
Figure 5: t-SNE visualisation of the learned final layer of the multi-task evaluator network, trainedwith two combinations of principal and auxiliary class numbers from CIFAR100. Colours representthe principal classes.
Figure 6: Visualisation of 5 test examples with the highest prediction probability, for each of 3randomly selected auxiliary classes, for a number of different principal classes. We present thevisualisation for CIFAR100 (top) when trained with 20 principal classes and 5 auxiliary classes perprincipal class, and for MNIST (bottom) when trained with 10 principal classes and 3 auxiliaryclasses per principal class.
Figure 7: Testing performance on CIFAR10 (bottom) and CIFAR10.1v6 (top) datasets, across 6different numbers of auxiliary classes.
