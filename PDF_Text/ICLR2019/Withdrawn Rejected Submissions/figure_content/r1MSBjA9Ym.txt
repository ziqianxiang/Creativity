Figure 1: Demonstration of the neural network collapse to the mean value (A, with very high prob-ability) or the partial mean value (B, with low probability) for the C0 target function y(x) = |x|.
Figure 2: Similar behavior for the Câˆž target function y(x) = x sin(5x). The network parameters,loss function, and initializations are the same as in Fig. 1. (A) corresponds to the mean value ofthe target function with high probability. (B, C, D) correspond to partial mean values with lowprobability and are induced by different random initializations.
Figure 3: Similar behavior for the L2 target function y(x) = 1{x>0} + 0.2 sin(5x). The networkparameters, loss function, and initializations are the same as in Fig. 1. (A) corresponds to the meanvalue of the target function with high probability. (B, C, D) correspond to partial mean values withlow probability and are induced by different random initializations.
Figure 4: Demonstration of the neural network collapse to the mean value (A, with very high prob-ability) or the partial mean value (B, with low probability) for the C0 2-dimensional (vector) targetfunction y(x) = [|x1 + x2|, |x1 - x2|]. The gradient vanishes in both cases (see Corollaries 5and 6). A 10-layer ReLU neural network with width 4 is employed in both (A) and (B). The biasesare initialized to 0, and the weights are initialized from a symmetric distribution. The loss functionis MSE.
Figure 5: Effect of the loss function on the behavior of the collapse of the neural network. MSE(used in Figs 1, 2, 3) is compared against the MAE. The collapse of the NN is independent of theloss function (see Theorem 4).
Figure 6: Probability of a ReLU NN to collapse and the safe operating region. (A) Probabilityof NN to collapse as a function of the number of layers for different widths. The solid black linerepresents the theoretical probability (Proposition 10). The dash lines represent the approximatedprobability (Theorem 8). The symbols represent our numerical tests. Similar colors correspond tothe same width. A ReLU feed-forward NN is more likely to become a zero function when it isdeeper and narrower. A bias-free ReLU feed-forward NN with din = 1 is employed with weightsrandomly initialized from symmetric distributions. (The last layer also applies activations.) (B)Diagram indicating safe operating regions for a ReLU NN. The dash lines represent Eq. 5 based onTheorem 8 while the symbols represent our numerical tests. The maximum number of layers of aneural network can be used at different width to keep the probability of collapse less than 1% or10%. The region below the blue line is the safe region when we design a neural network. As thewidth increases the theoretical predictions match closer with our numerical simulations.
Figure 7: Effect of initialization on the collapse of NN. Plotted is the probability of collapse ofa bias-free ReLU NN with din = 1 with different width and number of layers. The black filledsymbols correspond to symmetric initialization while the red open symbols correspond to orthogonalinitialization.
