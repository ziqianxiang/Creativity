Figure 1: A schematic for our Deep Perm-Set Network. A structured input, e.g. an RGB image, is fed to aseries of convolutional and fully connected layers with a collection of parameters shown by w. The output layerconsists of three parts shown by α, O1 and O2, which respectively predict the cardinality, the states and thepermutation of the set elements. During the training, π*,k representing a permutation sample (attained by Eq. 5),is used as the ground truth to update the loss f2 (π*,k, O2) and to sort the elements of the ground truth set,s statein the fι(Y∏* k, Oι) term in Eq. 6. During inference, the optimal set Y* is only calculated using the cardinalityɑ and the states Oi outputs. π* is an extra output for ordering representation.
Figure 2: A comparison between the detection performance of (a) Faster-RCNN, (b) YOLO v2 and (c) our setdetector on heavily overlapping pedestrians from MOTChallenge benchmark. Both Faster-RCNN and YOLO v2fail to properly detect heavily occluded pedestrians due to the inevitable NMS heuristic.
Figure 3: (a) The best F1 sCores against the level of objeCt oCClusions CalCulated by interseCtion of union(IoU), (b) PreCision-ReCall Curve, and (C) ROC (miss rate-false positive per image) Curve on pedestrian deteCtiondata for the Competing deteCtors: Faster-RCNN, YOLO v2, our network (w/o Cardinality) and our network (w/Cardinality). Our final deteCtion results are also shown as a single point in the Curves.
Figure 4: A query digit (left) and a set of digits (right) for theproposed CAPTCHA test. The ground truth and our prediCtedsolutions are shown by white and red boxes respeCtively.
Figure 5: A comparison between the detection performance of (a) Faster-RCNN, (b) YOLO and (c) our setdetector on heavily overlapping objects. Both Faster-RCNN and YOLO fail to properly detect heavily occludedobjects due to the inevitable NMS heuristic.
Figure 6: (a) The best F1 scores against the level of object occlusions calculated by intersection of union (IoU),(b) Precision-Recall curve, and (c) ROC (miss rate-false positive per image) curve on synthetic data for thecompeting detectors: Faster-RCNN, YOLO, our network (w/o cardinality) and our network (w/ cardinality). Ourfinal detection results are also shown as a single point in the curves.
Figure 7: The performance of our approach in detectingand also identifying the object instances using permutations.
Figure 8: Training and validation losses of (a) our DeepPerm-Set Network and (b) the set network proposedin (1), for object detection task.
Figure 9: The predicted bounding boxesfrom the set network proposed in (1),when trained for the object detectiontask. In all images, all the boundingboxes are concentrated in the same lo-cation with the exactly same size, whichare simply an average of all the positionsand sizes of the objects in the scene.
Figure 10: A plot, representing the accu-racy of solving the CAPTCHA test us-ing Faster-RCNN against the detectionthreshold.
Figure 11: Further examples showing a perfect prediction of the solution using our Deep Perm-SetNetwork. Each image contains a query digit (bottom left corner) and a set of digits. The ground truthand our predicted solutions are shown by white and red boxes respectively. Note that zero representsnumber 10 for our experiment.
Figure 12: Further examples showing the solution of Faster-RCNN for the test. Each image containsa query digit (bottom left corner) and a set of digits. The ground truth and the predicted solutions forFaster-RCNN are shown by white and red boxes respectively. Faster-RCNN simply learns to detectalmost all the digits while ignoring the logical relationship between the query digit and the set ofdigits.
