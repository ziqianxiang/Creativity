Figure 1: Illustration of the vanilla model with a segment length 4.
Figure 2: Illustration of the Transformer-XL model with a segment length 4.
Figure 3: Visualizing unnormalized relative perplexity gains with r = 0.1.
Figure 4: Perplexity vs context length.
Figure 5: Average attention over the previous 640 tokens, where each row corresponds to a attentionhead and each column corresponds to a relative location. There are totally 160 attention heads, andevery 10 heads come from a single layer. Darker colors indicate higher values.
Figure 6: Visualization of the three heads with a wide attention range. Each row corresponds toa target location/token and each column corresponds to a context location/token. Tokens in thememory that have top 20% attention values are highlighted in red.
Figure 7: Visualization of the three terms in computing the attention score. Each row correspondsto a attention head and each column corresponds to a relative location.
