Figure 1: (a) Illustration of the abstract MDP on Montezuma’ s Revenge. We have superimposeda white grid on top of the original game. At any given time, the agent is in one of the grid cells -each grid cell is an abstract state. In this example, the agent starts at the top of a ladder (yellowdot). The worker then navigates transitions between abstract states (green arrows) to follow a planmade by the manager (red dots). (b) Circles represent abstract states. Shaded circles represent stateswithin the known set. The manager navigates the agent to the fringe of the known set (s3), thenrandomly explores with πd to discover new transitions near s3 (dotted box). (c) The worker extendsthe abstract MDP by learning to navigate to the newly discovered abstract states (dotted arrows).
Figure 2: Comparison of our approach with the prior non-demonstration state-of-the-art approachesin Montezuma’ s Revenge, Pitfall !, and Private Eye. Our approach achieves more thandouble the reward of prior state-of-the-art approaches in all three.
Figure 3: Our method continues to out-perform the prior state-of-the-art on thestochastic version of Private Eye.
Figure 4: Our method outperforms theprior state-of-the-art on a wide rangeof granularities of the state abstractionfunction.
Figure 5: The saw-tooth epsilon schedule used by our skillsincreasing length, summarized in Figure 5. This enables skills that learn quickly to achieve lowvalues of epsilon early on in training, while skills that learn slowly will later explore with highvalues of epsilon over many episodes.
Figure 6:	The number of different transitions each skill can traverse. Skills are sorted by decreasingusage.
Figure 7: Skill reuse in Montezuma’ s Revenge. The same skill is useful in multiple rooms andcan both climb up ladders and jump over a monster.
Figure 8: Display of all the rooms of the Montezuma’ s Revenge pyramid. The agent startsin room 1 and must navigate through the pyramid, picking up objects and dodging monsters tocomplete the new tasks. The end of each task is marked with a star. Example paths for each task aremarked with different colors. Multiple colors indicate sections of the paths that are shared acrossmultiple tasks. (Red: Get key, Yellow: Kill spider, Blue: Enter room 8).
Figure 9: Training curves of SmarthHash on alternate tasks in MONTEZUMA’ S REVENGE, com-pared with the performance of our approach generalizing to the new task. Our approach only re-ceives 1M frames with the new task.
Figure 10: The number of transitions learned by the worker vs. number of training frames. Theworker continues to learn new transitions even late into training, showing almost no signs of slowingdown in Montezuma’ s Revenge and Pitfall !.
Figure 11: Performance of our approach on Private Eye, when decreasing the number of explo-ration episodes (Nvisit). (a) When Nvisit is set to 10, our approach performs even better, achievingnear human-level rewards. (b) Decreasing Nvisit enables the worker to learn more transitions infewer frames.
