Figure 1: Distributed Multi-Agent Policy Gradients: Each agent n (Agn ) starts under policyparametrized by θ and uses it to collect experience τnθ . τnθ is used to minimize agent Agn ’s lossfunction Ln and adapt its policy from θ to θn . Now, Agn uses policy parametrized by θn assumingother agents policies remain θ. The trajectory generated in this case is denoted by τnθ,θn and is used toimprove Agn’s policy by taking gradients w.r.t this intermediate policy. Finally, using this improvedpolicy, we collect another new trajectory τnθ,θn. These new trajectories are used to update θ.
Figure 2: Multi-agent environments for testing: We consider both collaborative as well as com-petitive environments. Left: Collaborative Navigation (with 3 agents) Center Left: CollaborativeNavigation for 10 agents. Center Right: Predator-Prey Right: Survival with many (630) agents(Zheng et al. (2017)) are adapted for our experiments. We setup the following experiments to test outour algorithm :Collaborative Navigation This task consists of N agents and N goals. All agents are identical, andeach agent observes the position of the goals and the other agents relative to its own position. Theagents are collectively rewarded based on the how far any agent is from each goal. Further, the agentsget negative reward for colliding with other agents. This can be seen as a coverage task where allagents must learn to cover all goals without colliding into each other. We test increasing the numberof agents and goal regions and report the minimum reward across all agents.
Figure 3: Min reward vs. number of episodes for Collaborative Navigation: DIMAPG convergesquickly in both scenarios. The protocol followed in the plots involves 5 independent runs for eachalgorithm with different seeds, darker line represents the mean and the shaded lighter region representsthe variance.
Figure 4: Results on Predator Prey. Left, Center: Average predator reward collected over 100episodes after training different policies for predators and preys. In the 3 Predators vs 1 Prey game,the prey is 30% faster than the predators. In the 12 Predators vs 4 Prey, the prey is 50% faster thanthe predators. Right: Avg predator reward vs episodes during training for 3vs1 game.
Figure 5: Performance on Predator Prey populations In this setting we learn two sets of policies,one for the predators and one for the prey. Here, we observe that after training for a fixed number ofepisodes, proposed DIMAPG algorithm is able to learn faster than the other algorithms.
Figure 6: Learned policy on Survivor(N=230) When the number of agents is small, agents preferto eat food instead of killing each other. Most agents survive in this setting.
Figure 7: Learned policy on Survivor(N=630) When the number of agents is much larger than theamount of food in the environment, the agents closer to the food rush in to gather food. We observethat the agents further away (near the walls) form teams and try to take down other agents thusmaximizing reward for the group. This can also be interpreted as follows: Agents who can observethe food within their sensing range choose to rush in food. Agents who do not observe food withintheir sensing range choose to form groups to take down other agents.
