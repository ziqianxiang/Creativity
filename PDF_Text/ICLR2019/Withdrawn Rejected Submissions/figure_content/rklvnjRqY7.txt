Figure 1: (top) Our proposed framework learns an obfuscated representation (feature maps) for image clas-sification that also prevents leakage of users’ privacy. The obfuscator extracts a feature map (the obfuscatedrepresentation) that both prevents the reconstruction of the image and keeps the primary information for theclassification task. The classifier uses the obfuscated feature map to perform the image classification task. Thereconstructor aims to reconstruct the original image from the feature map. The three models are trained usingan adversarial training process. (bottom) The attacker aims to reconstruct a users’ images to eavesdrop theirprivacy. We assume that the attacker has unlimited access to the obfuscator and the feature maps extractedfrom users’ images. The attacker trains their own reconstructor using their own set of images, and attempts toreconstruct the users’ images from the stored feature maps.
Figure 2: The structure of the (a) obfusCator and (b) classifier.
Figure 3: The classification accuracy for different datasets and different training reconstructors.
Figure 4: Reconstruction results using different reconstructors. For each vertical pair, the top image is the inputimage, and the bottom is the reconstructionRaW ImagesRaW ImagesReconstructedImages WithoutAdversarial TrainingReconstructedImages WithAdversarial TrainingReconstructedImages WithoutAdversarial TrainingReconstructedImages WithAdversarial Training .
Figure 5: The comparison between reconstructed images with adversarial training and without adversarialtraining. The number under each image is the PSNR for the reconstruction.
