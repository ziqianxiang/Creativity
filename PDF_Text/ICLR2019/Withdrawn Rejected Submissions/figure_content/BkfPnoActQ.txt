Figure 1: The figure compares our architecture (b) to the one proposed by Horgan et al. (2018) (a).
Figure 2: The figure shows the cumulative undiscounted episode return over time and comparesthe best expert episode to the best Ape-X DQfD episode on three games. On Hero, the algorithmexceeds the expert’s performance, on M ontezuma’ s Revenge, it matches the expert’s score butreaches it faster, and on Ms. Pacman, the expert is still superior.
Figure 3:	Results of our ablation study using the standard network architecture. The experimentwithout expert data (——)was performed with the higher exploration schedule used in (Horgan et al.,2018).
Figure 4:	The figures show how our algorithm compares when we substitute the transformed Bellmanoperator to PopArt and when we substitute the TC loss to constrained TD updates. Note that thescales differ from the ones in Fig. 3 because the experiments only ran for 40 hours.
Figure 5: Training curves on all 42 games. We report the performance using the standard networkarchitecture (Wang et al., 2016) and the slightly deeper version (see Fig. 9).
Figure 6:	The curves show the effect of using clipped/unclipped rewards and low/high discountfactors on all games.
Figure 7:	The human-relative score of Ape-X DQfD (deeper) using the no-ops starts regime. Thealg. score-avg. human scorescore is computed as avg.lumanscorLrandomscore X 100.
Figure 8:	The human-relative score of Ape-X DQfD (deeper) using the human starts regime. Thescore is computed asalg. score—avg. human scoreavg. human score—random score× 100.
Figure 9:	The two network architectures that we used. The upper one is the standard duelingarchitecture of Wang et al. (2016) and the lower one is a slightly wider and deeper version.
