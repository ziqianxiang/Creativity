Figure 1: (a) The original LBFGS-B adversarial image, (b) The image after 3x3 bilateral filteringand (c) The image after 3x3 averaging filtering. The bilateral filter is superior since it removes smallperturbations while preserving sharp edges in the image, keeping it from becoming blurryNetwork	FGSM	MI-FGSM	DeepFool	CW (L2)	L-BFGSInception V3	97.0	97.5	98.8	99.2	97.8InceptionResNet V2	94.2	98.4	96.3	98.8	95.1ResNet V2	96.5	98.0	96.1	98.1	98.0Table 1: Recovery performance for manually chosen bilateral filter parameters. We measure recov-ery by the percentage of examples which, after filtering, revert to the classification label assigned tothe unperturbed image by the CNN. This shows that with adaptively chosen parameters accordingto the attack, we can recover nearly all adversarial examples3.2	Adaptive FilteringOne caveat to the above approach, is that the parameters for the bilateral filter must be carefullychosen to be able to recover the accuracy and confidence of the original classification. Large valuesfor the parameters σs and σr can create an excessively blurred image, and a small filter size Kmay capture insufficient information to remove the adversarial perturbations. With this in mind,we train a small network which will predict the parameters of the bilateral filter (K, σs , σr) for aninput image. This network will serve as a cheap preprocessing step that will remove adversarialperturbations without affecting the underlying class label.
Figure 2: Adversarial images created with BFNet. See the appendix for adversarial images from thesame original images without BFNetNetwork	DeepFool	L-BFGS L∞	L2	L∞	L2Inception V3Natural Inception V3BFNet	0.015	0.43	0.02	0.67 0.621	148.29 0.39	90.52IncResNet V2Natural IncResNet V2BFNet	0.025	0.44	0.06	0.77 0.793	187.45 0.65	90.65Table 4: Performance of BFNet against DeepFool and L-BFGS attacks. We report the average L2and L∞ distance of 1, 000 adversarial images on Inception V3 and Inception-ResNet V2.
Figure 3: The effect of bilateral filtering on adversarial inputs. From left to right, we show theadversarial perturbation, the clean image, the adversarial images generated by the respective attackalgorithms, and the recovered image after bilateral filtering. Note that bilateral filtering does notdestroy image quality, and images can be correctly classified.
Figure 4: Adversarial images generated by DeepFool and L-BFGS without BFNet, to be comparedwith Fig.2Figure 5: The average mini-batch batch accuracy (a) and cross entropy loss (b) for the model trainedwith adversarial training on MNIST. We trained our model to convergence, which happened near20k iterations. We can see that the training is stable and converges to a similar training error as anaturally trained networkSecond, we used an approximate L-BFGS attack where the gradients are computed numerically. Us-ing a numerical gradient avoids the instabilities in iterative attacks which occur in gradient-maskingdefenses. We report the average L2 distance achieved of these attacks against BFNet of 1000 im-ages. We show the results in table 7 From the results given, we can see the BFNet does well againstboth the decision-based attack and the approximate L-BFGS, showing that it is indeed resistant tosuch kind of attacks as well.
Figure 5: The average mini-batch batch accuracy (a) and cross entropy loss (b) for the model trainedwith adversarial training on MNIST. We trained our model to convergence, which happened near20k iterations. We can see that the training is stable and converges to a similar training error as anaturally trained networkSecond, we used an approximate L-BFGS attack where the gradients are computed numerically. Us-ing a numerical gradient avoids the instabilities in iterative attacks which occur in gradient-maskingdefenses. We report the average L2 distance achieved of these attacks against BFNet of 1000 im-ages. We show the results in table 7 From the results given, we can see the BFNet does well againstboth the decision-based attack and the approximate L-BFGS, showing that it is indeed resistant tosuch kind of attacks as well.
Figure 6:	PGD adversarial examples which fool an adversarially trained BFPGD with = 0.3Natural: 7Adversarial: 2Natural: 1Adversarial: 3Natural: 7Adversarial: 11Natural: 6Adversarial: 0Natural: 1Adversarial: 7Natural: 9Adversarial: 5Figure 7:	FGSM adversarial examples which fool adversarially trained BFfgsm with = 0.3Network	Boundary-Attack L2	Approx. L-BFGS L2Inception V3Natural	7.20	0.82Inception V3BFNet	52.562	50.21IncResNet V2Natural	8,74	1.34IncResNet V2BFNet	30.33	34.61
Figure 7:	FGSM adversarial examples which fool adversarially trained BFfgsm with = 0.3Network	Boundary-Attack L2	Approx. L-BFGS L2Inception V3Natural	7.20	0.82Inception V3BFNet	52.562	50.21IncResNet V2Natural	8,74	1.34IncResNet V2BFNet	30.33	34.61Table 7: Performance of BFNet against the Boundary and Approximate L-BFGS black-box attacks.
