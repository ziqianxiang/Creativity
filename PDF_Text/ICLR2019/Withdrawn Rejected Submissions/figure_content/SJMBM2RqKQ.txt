Figure 1: Validation accuracy as a function of pruningpercentage for Split MNISTbeyond which we do not wish to prune because of the significant degrade in the performance. Inthis example we chose to prune by 50% after learning task - which clearly results in limiting ourcapacity if we were to learn more than two tasks with this model.
Figure 2: ACC and BWT on (a) Split MNIST (2 tasks), (b) Permuted MNIST (10 tasks), (c) Alternatingincremental CIFAR10/100 (10 tasks), (d) 8 consecutive tasks shown in Table 16.2	Permuted MNISTPermuted MNIST is a popular variant of MNIST dataset to evaluate continual learning approachesin which each task is considered as a random permutation of the original MNIST pixels. Followingthe literature, we learn a sequence of 10 random permutations and report average accuracy at theend. Figure 2b shows ACC and BWT performance of BLLL-REG and BLLL-PRN against that ofthe state of the art models. We can again observe that BLLL-FT, despite having no intention inpreventing forgetting, exhibits reasonable negative BWT values which makes it a successor overIMM (Lee et al., 2017) and LWF (Li & Hoiem, 2017) baselines. While HAT (Serra et al., 2018) andBLLL-PRN have a similar ACC performance with no forgetting, BLLL-REG shows a slight increasein the tendency to forget (-0.8%) in exchange for a negligible boost in ACC (0.4%). Accuracyevolution over 10 tasks is shown in 3b.
Figure 3: Evolution of the test classification accuracy obtained across tasks when all tasks are learningtasks of CIFAR10. EWC and HAT are both allowed to forget by construction, however, HAT showszero forgetting behavior. While EWC is outperformed by both of our BLLL variants, HAT exhibits1% better ACC over BLLL-PRN. BLLL-REG is able to compensate for that at a cost of 0.7% dropin forgetting. A trade off between ACC and BWT exists when choosing between BLLL-PRN andBLLL-REG depending on what is needed in the problem.
