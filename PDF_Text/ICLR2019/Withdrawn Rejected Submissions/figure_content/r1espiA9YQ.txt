Figure 1: Comparison of SPOS (left) and SVGD (right) on a multi-modedistribution. The circles with different colors are the resulting 100 particles,which are able to spread over all modes for SPOS.
Figure 3: Policy learning with Bayesian exploration in policy-gradient methods on six scenarios withSVPG and SPOS-PG.
Figure 4: Illustration of different algorithms on toy distributions. Dots are the final particles; theFigure 5: Illustration of different algorithms on toy distributions. Dots are the final particles; the blueregions represent densities estimated by the particles. Each column is a distribution case. First row:ground true densities; Second row: standard SGLD; Third row: SVGD; Fourth row: SPOS.
Figure 5: Illustration of different algorithms on toy distributions. Dots are the final particles; the blueregions represent densities estimated by the particles. Each column is a distribution case. First row:ground true densities; Second row: standard SGLD; Third row: SVGD; Fourth row: SPOS.
