Figure 1: Illustration for Bayes/ Empirical Bayes, and Approximate Empirical Bayes.
Figure 2: Generalization of AEB on MNIST and CIFAR10. AEB improves generalization under both minibatchsettings and is most beneficial when training set is small.
Figure 3: Optimization of AEB on MNIST with batch size 2048.
Figure 4: Applying AEB on different lay-ers in neural networks for MNIST withbatch size 2048.
Figure 5: Comparisons of stable ranks and spectralCNNCNN-AEB(a) Stable rank vs training size.
Figure 6: Comparisons of stable ranks and spectral norms from different methods on CIFAR10.
Figure 7: Correlation matrix of the weight matrix in the softmax layer. All the networks share the same structureand optimization protocol. The left two correspond to dataset with training size 600 and the right two with size60,000. Acc means the test set classification accuracy.
Figure 8: Combine AEB with BN and Dropout on MNIST.
Figure 9: Recovered row covariance matrix Σr and column covariance matrix Σc in the priordistribution on MNIST.
Figure 10: Explained variance of different methods on 7 regression tasks from the SARCOS dataset.
