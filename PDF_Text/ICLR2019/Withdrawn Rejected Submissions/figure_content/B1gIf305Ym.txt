Figure 1: Overview of the stages of NSGA-Net. Networks are represented as bit strings, trainedthrough gradient descent, ranking and selection by NSGA-II, search history exploitation throughBOA. Output is a set of networks that span a range of complexity and error objectives.
Figure 2: (1) Black solutions on the con-cave segment of the frontier cannot beachieved through weighted sum; (2) Ifscale of f2(x) is much larger than f1(x),weighted sum solutions are biased to-wards the shaded grey region; (3) greenand red solutions are possible from linearcombination, but obtaining each solutionrequires an independent run.
Figure 3: (a) An example of how non-dominated sorting ranks solutions into different frontiersbased on objectives, fι(∙) and f2(∙). (b) The environmental selection procedure, in which createdoffspring, Qt, networks get merged with parent population, Pt, to create Rt from which the “better"half survives into the next parent population, Pt+1 . Here, better is defined as a hierarchical preferenceof rank and crowding distance. (c) A pictorial description of crowding distance. All points in thisfigure are non-dominated. To select a subset of them, we must rely on crowding distance. Points inthe blue region are preferred to the orange region since they are further from their nearest neighbors.
Figure 4: Encoding: Illustration of a classification network encoded by x = (xp , xo), where xois the operations at a phase (gray boxes, each with a possible maximum of 6 nodes) and xp is theprocessing resolution path (orange boxes that connect the phases). In this example the path, xp, isfixed based on prior knowledge of successful approaches. The phases are described by the bit stringxo which is formatted for readability above. The bits are grouped by dashes to describe what nodethey control. See Appendix E for detailed description of the encoding schemes.
Figure 5: Crossover Example: A crossover (denoted by 0) of a VGG-like structure with a DenseNet-like structure may result in a ResNet-like network. In the figure, red and blue denotes connectionsthat are unique to VGG and DenseNet respectively, and black shows the connections that are commonto both parents. All black bits are retained in the final child encoding, and only the bits that are notcommon between the parents can potentially be selected at random from one of the parent.
Figure 6: Exploitation: Sampling from the Bayesian Network (BN) constructed by NSGA-Net. Thehistograms represent estimates of the conditional distributions between the network structure betweenthe phases explored during the exploration step and updated during the exploitation step (i.e., usingthe population archive). During exploitation, networks are constructed by sampling phases from theBN. Fig. 4 shows the architectures that the sampled bit strings, {x01), ^2, xθ3)} decode to.
Figure 7: An illustration of the computation of HVin a hypothetical bi-objective scenario with twoexperiments. The points on the trade-off frontierand a chosen reference point form a bounding boxfrom which HV is computed. Here, the blue exper-iment would be considered a more desirable resultsince it covers more of the objective space, eventhough all its solutions may not dominate all thepoints shown in orange.
Figure 8: (a) Accuracy Comparison (b) Progression of trade-off frontiers at each stage of NSGA-Net.
Figure 9: (a) Trade-off frontier comparison between random search and NSGA-Net. (b) Normalizedhypervolume statistics over five runs. Higher normalized hypervolume implies better performanceon both criteria. (c) Trade-off frontier comparison with and without crossover. (d) Comparisonbetween sampling from uniformly from the encoding space and the Bayesian Network constructedfrom NSGA-Net exploration population archive.
Figure 10: Robustness Objective: We define a robustness objective under the FGSM Goodfellowet al. (2014) attack as follows: 1) obtain classification accuracy on adversarial images generated byFGSM as we vary , 2) compute the area under the curve (blue line), approximated by the area ofthe green region; 2) normalize the robustness value to the rectangular area formed by the Ideal pointand Nadir point; 3) Ideal point is defined at 100% accuracy at pre-defined maximum value, and thenadir point is defined as the accuracy of random guessing at = 0 (clean images).
Figure 11: Trade-off frontier of the robustness experiments. Color indicates the generation (iteration)at which a network architecture is eliminated from the surviving parent population. The size of eachpoint is proportional to the network architecture’s number of trainable parameters. We note thatnetworks for latter generations form the pareto front (dark blue points).
Figure 12: (a) Examples of the computational blocks discovered with high classification accuracy.
Figure 13: Parallel coordinate plots of the 1,200 network architectures sampled by NSGA-Net. Eachline represents a network architecture, each vertical line is an attribute associated with the network.
Figure 14: Median normalized hypervolume values over three runs of each strategy are compared.
Figure 16:	(a) An example of the path encoding in NSGA-Net. (b) An example of path crossover inNSGA-Net.
Figure 17:	Increase in operation redundancy as node count increases.
Figure 18: Examples of different encoding bit strings that decode to the same network computationblock.
Figure 19: (a) CIFAR-100 results comparing hand-crafted neural networks architectures and neuralarchitecture search found networks in the criterion space. Note that NSGA-Net is a non-dominatedpoint. (b) Trade-off frontiers comparing NSGA-Net and RSearch on CMU-Cars dataset.
Figure 20: Set of networks architectures on the final trade-off front discovered by NSGA-Net.
