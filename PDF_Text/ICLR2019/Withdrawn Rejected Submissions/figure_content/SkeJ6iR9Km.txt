Figure 1: Schematic representation of the variational sparse coding model (right) compared to astandard VAE (left). In both cases an observed variable Xi is assumed to be generated from anunobserved variable z%. Variational sparse coding, however, models sparsity in the latent space withthe Spike and Slab prior distribution. One example is shown for each prior with a sample from theMNIST dataset.
Figure 2: Test set ELBO for the VSC at varying number of latent dimensions. The standard VAEreaches high ELBO for a correct choice of latent space dimensions, but drops rapidly for larger latentspaces. With increasing sparsity in the latent space, the VSC drops in performance at the optimalVaE dimensionality, but remains more stable with larger latent spaces.
Figure 3: Classification performance of VSC and standard VAE at varying number of latent spacedimensions. The VAE reaches its peak performance for optimal choice of latent space dimensions,but yields inefficient codes if the latent space is too large. VSC recovers efficient codes for arbitrarilylarge latent spaces which outperform the VAE ones as classification inputs.
Figure 4: Effect on generation of altering single non-zero elements in VSC latent codes for Fashion-MNIST (Top) and CelebA (Bottom). The original latent codes are shown in blue and those alteredin the latent space are shown in orange. Underneath each code the corresponding reconstruction isshown. The altered elements are highlighted with a coloured circle.
Figure 5: Effect of altering individual non-zero components of the interpolation vector between thesparse codes of two objects. The original latent codes are shown in blue and those altered in thelatent space are shown in orange. The altered elements are highlighted with a coloured circle.
Figure 6: Schematic representation of the reparametrisation of the Spike variable. The variable yi,l,jis drawn in the range covered by the grey square with probability proportional to its height. On theleft, for a spike probability Yij = 1, the variable yi,l,j is drawn to always be greater than zero andthe Spike variable wi,l,j is always one. On the right, for an arbitrary Yi,j∙, the probability density ofyi,l,j is displaced to the left by 1 - Yij and yi,l,j has probability Yij of being ≥ 0, in which casewi,l,j is one, and probability 1 — Yij of being < 0, in which case wi,l,j is zero.
Figure 7:	Examples of sparse codes and reconstruction for the MNIST (top), FaShion-MNIST (mid-dle) and CelebA (bottom) datasets.
Figure 8:	Measured sparsity at varying prior Spike probability α.
Figure 9:	Test sets ELBO evaluation of VSC at varying number of latent space dimensions fordifferent iterations limits. The out most right graphs correspond to those shown in figure 2.
Figure 10: Training and test sets ELBO at varying prior Spike probability α.
Figure 11: Classification performance at varying prior Spike probability α.
Figure 12: Ancestral sampling with a VAE and a VSC trained on the Fashion-MNIST dataset. Thesamples generated by the VSC sometimes result into unnatural combinations of features, SUCh asasymmetric t-shirts and bags handles on pieces of clothing.
Figure 13: Conditional sampling in a VSC trained on the Fashion-MNIST dataset. Samples aredrawn from the prior Gaussian component, but only along the dimensions activated by the recog-nition function, which are different for different observations. As a result, we obtain different sub-generative models that can generate different distinct types of objects in the aggregate used to trainthe model.
