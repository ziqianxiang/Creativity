Figure 1: Context adaptation. A network layer hl is aug-mented with additional context parameters φ (red), which areinitialised to 0 before each adaptation step. The context pa-rameters are updated by gradient descent during each innerloop and during test time. The network parameters θ (green)are only updated in the outer loop and shared across tasks.
Figure 2: Performance after several gradi-ent steps (on the same batch) averaged over1000 unseen tasks. The size of the contextparameter / additional input to MAML is 5.
Figure 4: CAML scales the model weights so thatthe inner learning rate is compensated by the con-text parameters gradients magnitude.
Figure 3: Visualisation of what the context parameters learngiven a new task. In this case we have 2 context parameters, andshown is the value they take after 5 gradient update steps on anew task. Each dot is one random task, with its colour indicatingthe amplitude (left) or phase (right) of that task.
Figure 5: Measuring performance for differentlearning rates shows that CAML is more robustto this hyperparameter than MAML.
Figure 6: 2D navigation task analysis. Figure 6a shows the performance of each method as more gradientupdates are performed. Figure 6b shows the performance of each method after 2 updates as the inner looplearning rate is increased. As in the case of regression CAML is not afected by this parameter. Figure 6cdescribes the goal position of different 2D navigation tasks and the corresponding context parameter activationobtained by performing 2 gradient updates. We can see that the context parameters represent an interpretableembedding of the task at hand. Context parameter 1 seems to encode the y position, while context parameter 2encodes the x position.
