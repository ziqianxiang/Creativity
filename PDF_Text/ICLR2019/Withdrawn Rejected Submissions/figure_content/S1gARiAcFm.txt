Figure 1: (A) Schema of DyMoN architecturewhere x represents the neural network input vec-tor, y represents the predicted network outputin (1), and is a random Gaussian noise vec-tor. P (y |x) represents the distribution of outputsfrom many iterations of the stochastic dynami-Cal process given x, and P(y|x) represents thedistribution of outputs of DyMoN given x andmany different noise vectors. (B) Example inputstates. (C) Learned transition vectors (arrows)and output states from DyMoN.
Figure 2: Jacobian of DyMoNâ€™s transition vector with respect to network inputs on mouse neuronalactivations. (A) Biclustered heatmap of gradient of the transition vector with respect to the networkinputs, with the transition vector on the vertical axis and the inputs on the horizontal axis. (B) Graphbuilt on thresholded gradients (gradients with absolute value less than 0.04 were excluded). Nodesare colored by their degree.
Figure 3: DyMoN on mass cytometry data of T cell development in the thymus. (A) PCA plots ofall 17,000 cells colored by CD8 (Ai) and CD4 (Aii) expression. (B) PCA plots with all cells in greyand DyMoN trajectories in color. Bi shows DyMoN trajectory of CD4+/CD8- T helper cells, andBii shows DyMoN trajectory of CD4-/CD8+ cytotoxic T cells. (C) Shows row z-scored heatmapsof marker expression as a function of the trajectory for each of the two DyMoN trajectories withhierarchically clustered genes on the rows and cells on the columns. (D) Heatmaps of the Jacobiansobtained at the branch point (Di) and at the end of the CD8+/CD4- branch (Dii).
Figure 4: Trajectories generated by DyMoN sampling the neural progenitor (A) and bone progenitor(B) cell states. Trajectories are shown both as MDS against the training data (i) and as heatmaps ofselected transcription factors (ii). From these trajectories, we propose a novel cellular programmingprotocol shown in (iii).
Figure 5: Vector fields indicating the predicted transitional direction by DyMoN on circle (left) andtree (center) datasets. The right-hand branch point of the tree is shown at higher resolution in theright panel. We run DyMoN at discrete points in the state space, with arrows pointing in the directionof prediction. We run DyMoN 40 times and show darker arrows in the more frequently predicteddirections. DyMoN is trained to move both clockwise and counter-clockwise on the circle. For thetree, it begins at the center-left and travels down each of the branches.
Figure 6: Paths generated by the second-order DyMoN trained on a single and double pendulum. Asingle trajectory is shown for each system in (A) and (C) respectively. In (B) and (D) we show the xcoordinate of 500 generated paths starting from an epsilon-difference for the single and the lower ofthe double pendulums, respectively.
Figure 7: Chain generated by the DyMoN visualized on a PCA embedding of the Frey faces dataset(left). The chain is shown in color (indicating time) superimposed over the training data in gray. 10equally spaced faces generated by DyMoN are also shown (right).
Figure 8: Chain of samples (top) and marginal distribution (bottom) drawn from Markov-ChainMonte Carlo (MCMC), DyMoN, Recurrent Neural Network (RNN), Hidden Markov Model (HMM)and Kalman Filter (KF) when trained on the Gaussian Mixture Model.
Figure 9: Embedding of the teapot video dataset colored by the x axis using PCA (left), a three-nodehidden layer of DyMoN (center), and a three-node hidden layer of an RNN (right). We also showsequential images from the original video and a DyMoN-generated chain (far right).
Figure 10: Alternative DyMoN architectures. The stochastic output can be generated in the ambientspace (1) or the latent space of an autoencoder (2, 3). Chains of samples are generated by feedingDyMoN output back in as a new input; this can be passed additionally through the encoder anddecoder as a denoising step (3).
Figure 11: Examples of samples drawn from DyMoN, Recurrent Neural Network (RNN) given eithera full sequence of inputs (Sequence) or a single input padded by zeroes (Snapshot), Hidden MarkovModel (HMM) and Kalman Filter (KF) when trained on the teapot data.
