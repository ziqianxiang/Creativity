Figure 1: GVFs for Policy Gradient Methods4	GVFs for policy gradient methodsPerformance gradient estimation techniques of score function and likelihood ratio, as introducedby Aleksandrov et al. (1968) and Rubinstein (1969), are based on sampling trajectories to estimateboth the value of a policy and the gradient used for improving the policy. Given a score functionf that depends on a random variable sampled from a parameterised distribution q(θ), the gradientwith respect to the parameters is given by VEX〜q(θ)[f (X)] = EX〜q(θ) [f (X)Vθ log q(X； θ)].
Figure 2: Policy gradient updates with TD(λ) estimates. L1 norm of the exact value function vs.
Figure 3: Actor-Critic with bootstrapped gradient estimates. L1 norm of the exact value functionvs. number of rollouts used for policy updates. Top: four-room maze introduced in (Sutton et al.,1999a))(≈ 60 states and ≈ 15 total features). Bottom: large maze grid-world with sparse reward(≈ 3000 states and ≈ 90 total features). Each plot corresponds to a fixed number of steps L taken bythe agent for every rollout, The plots displayed here correspond to the highest performing hyperpa-rameter setting: αA is the learning rate for updates to the policy parameters, αC is the learning ratefor updates to the critic component, and αG is the learning rate for updates to the gradient estimates.
Figure 4: Left: Every option-value corresponds to a separate GVF, concerned both with externalreward R, as well as all other GVFs. Right: In FuN, manager (M) and worker (W) are trainedusing separate GVFs: vM, concerned with external return as corresponding to W’s policy; vW,conditioned on goals specified by M.
Figure 5: Training curves for FuN with high-level choices over GVFs in ATARI games (Bellemareet al., 2013). In Seaquest and Enduro, agents for which the manager uses successor features consis-tently outperform the baseline algorithm. In Gravitar and Ms. Pacman, the proposed modificationshave no impact on performance. See appendix for more results.
Figure 6: Policy gradient updates with TD(λ) estimates. L1 norm of the exact value function vs.
Figure 7: Actor-Critic with bootstrapped gradient estimates - results on four-room maze (in-troduced in (Sutton et al., 1999a)) L1 norm of the exact value function vs. number of rollouts usedfor policy updates. Each plot corresponds to a fixed number of steps L taken by the agent for everyrollout, the learning rate αA for updates to the policy parameters, a learning rate αC for updatesto the critic component, and a learning rate αG for updates to the gradient estimates. Mean andvariance for the value of the policy as a function of policy updates are shown in (blue) for baselineactor-critic, in (green) when actor-critic maintains bootstrapped estimates for parameter updates.
Figure 8: Actor-Critic with bootstrapped gradient estimates - results on large maze grid-worldwith sparse reward. L1 norm of the exact value function vs. number of rollouts used for policyupdates. Each plot corresponds to a fixed number of steps L taken by the agent for every rollout,the learning rate αA for updates to the policy parameters, a learning rate αC for updates to the criticcomponent, and a learning rate αG for updates to the gradient estimates. Mean and variance for thevalue of the policy as a function of policy updates are shown in (blue) for baseline actor-critic, in(green) when actor-critic maintains bootstrapped estimates for parameter updates.
Figure 9: Training curves for FuN with high-level choices over GVFs. Results on ATARIgames (Bellemare et al., 2013) demonstrate gains in data-efficiency for some ATARI games. Ingames such as Seaquest, Alien, and Enduro, agents for which the manager uses successor featuresconsistently outperform the baseline algorithm. In Gravitar, Amidar and Ms. Pacman, the proposedmodifications have no impact on performance. In Frostbite, both versions of the algorithm are usu-ally stuck in a local minimum (average reward ≈ 250), and the noticeable improvement is due toone of the agents luckily getting out of the local minimum for a limited number of steps.
