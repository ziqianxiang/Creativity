Figure 1: The adjusted actor-critic objective compared to alternative policy-gradient objectives onGerman NER with 17 possible output tags. All objectives are maximized using Gradient Ascentwith a fixed step size of 0.5 for actor-critic, and 0.01 for REINFORCE objectives. For REINFORCEobjectives, we set n = l in the Temporal Difference credits (i.e. sum all rewards until the end ofsequence). All methods are trained 20 times with different random seeds using the same hyper-parameters.
Figure 2: The sensitivity of Scheduled Sampling on the choice of sampling schedule on GermanNER. Higher k results in less sampling.
