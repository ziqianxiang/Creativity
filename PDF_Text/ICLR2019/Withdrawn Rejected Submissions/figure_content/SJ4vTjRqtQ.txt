Figure 1: Dynamic Planning Network Architecture. Encoder is comprised of several convolutionallayers and a fully-connected layer. Planning occurs for τ = 1, ..., T steps using the IA and state-transition model. The result of planning is sent to the outer agent before an action at is chosen. Thefully-connected layer within the outer agent, outlined in blue, is used by the planning process.
Figure 2: A single planning step τ . Inner Agent, shown as the blue box with a recursive arrow,performs a step of planning using the state-transition model. Circles containing × indicate multipli-cation and circles with 〜indicate sampling from the Gumbel Softmax distribution.
Figure 4: Randomly generated samples of the Push environment. Each square’s coloring representsa different entity: the agent is shown as red, boxes as aqua, obstacles as black, and goals as grey.
Figure 5: Randomly generated samples of a 16 × 16 Gridworld environment where the agent mustcollect all goals. The agent is shown as red, goals in cyan, obstacles as black, and outside of theenvironment, not visible to the agent, is shown with a black border.
Figure 6: Push Environment. a) Training over varying planning lengths, T = {1, 3, 5}, in the PushEnvironment. b) Training curves of DPN vs. baselines on the Push environment.
Figure 7: Samples of planning patterns the agent uses to solve the Push environment with T = 3.
Figure 8: Gridworld Environment. a) Training curves with DPN compared to various DQN baselineson 16 × 16 Gridworld with 3 goals. b) The performance of each DQN baseline and our model whereAvg. Reward is the average of the last 100 episodes of training.
Figure 9: Varied IA Targets. a) Training curve: Q×KL corresponds to the original target, KL to theKullback-Leibler divergence between hidden states of OA, and Q to the Q-function target (as usedby the OA). b) Training loss: the training loss of the entire architecture. c) Std. of DKL: The Std. ofthe DKL between the hidden states of the OA.
