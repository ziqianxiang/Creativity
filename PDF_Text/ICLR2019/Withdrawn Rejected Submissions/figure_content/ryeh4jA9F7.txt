Figure 1: Example of an adversarial patch that changes the prediction of the neural network whenoverlaid on the raw image. Here, we trained it to predict a toaster.
Figure 3: Illustration of the update of classifier.
Figure 4: Results on CIFAR10 for universal adversarial perturbation. It presents the accuracy andadversarial accuracy of: Fictitious Play (FP), Stochastic Gradient Descent (SGD), and AdversarialTraining (AT).
Figure 5: Results of our experiment on CIFAR100 for universal adversarial perturbation.
Figure 6: Results of our experiment on ImageNet for universal adversarial perturbations.
Figure 7: Results on CIFAR100 for an adversarial patch appearing at a random position on the imageof a diameter 0.4 times the size of the image.
Figure 8: Results on ImageNet for an adversarial patch appearing at a random position on the imageof a diameter 0.5 times the size of the image.
