Figure 1: Intuition of Bamboo’s effect on the DNN decision boundary(a) Without data augmentation(b) Bamboo data augmentationFigure 2: Visualization of Bamboo’s effect on the DNN decision boundarySince the decision boundary of the DNN model tends to have small curvature around training datapoints (Fawzi et al. (2016)), including the augmented data on the ball naturally pushes the decisionboundary further away from the original training data points, therefore increases the robustness ofthe learned model. In such sense, if the DNN model can perfectly fit to the augmented training set,increasing the ball radius will increase the margin between the decision boundary and the originaltraining data, and more points sampled on each ball will make it less likely for the margin to getsmaller than r.
Figure 2: Visualization of Bamboo’s effect on the DNN decision boundarySince the decision boundary of the DNN model tends to have small curvature around training datapoints (Fawzi et al. (2016)), including the augmented data on the ball naturally pushes the decisionboundary further away from the original training data points, therefore increases the robustness ofthe learned model. In such sense, if the DNN model can perfectly fit to the augmented training set,increasing the ball radius will increase the margin between the decision boundary and the originaltraining data, and more points sampled on each ball will make it less likely for the margin to getsmaller than r.
Figure 3: Performance result on MNIST dataset(a) Testing accuracy under different parameters(b) CW robustness under different parametersFigure 4: Decision boundary comparison on MNIST dataset4.3	B oundary visualizationFigure 4 and Figure 5 shows the top 20 smallest decision boundary on random orthogonal directionsfor MNIST and CIFAR-10 testing points respectively. These results provides a visualization of theeffect of different training methods on the decision boundary. From Figure 4a and 5a we can seethat adversarial training methods like FGSM (Goodfellow et al. (2014)) and Madry (Madry et al.
Figure 4: Decision boundary comparison on MNIST dataset4.3	B oundary visualizationFigure 4 and Figure 5 shows the top 20 smallest decision boundary on random orthogonal directionsfor MNIST and CIFAR-10 testing points respectively. These results provides a visualization of theeffect of different training methods on the decision boundary. From Figure 4a and 5a we can seethat adversarial training methods like FGSM (Goodfellow et al. (2014)) and Madry (Madry et al.
Figure 5: Decision boundary comparison on CIFAR-10 datasetTable 1: Performance comparison: bold type marks the best performance, and italics type marksthe second from the best performanceMNIST	Original	FGSM =0.5	DIST c=0.01	PGD = 0.3, 50 iterations	Mixup α = 0.12, 10×data	Ours r = 8, 10×dataCW rob	2.442	2.390	2.5010	2.343	2.803	3.554Test acc	0.9818	0.9817	0.9873	0.9869	0.9904	0.9904FGSM1 acc	0.5382	0.6375	0.8542	0.7511	0.8323	0.9292FGSM3 acc	0.2606	0.8963	0.1169	0.5840	0.2623	0.5558FGSM5 acc	0.1423	0.9390	0.0244	0.1340	0.1344	0.2878PGD 3 acc	0.0126	0.0258	0.0065	0.2534	0.0180	0.1281GAU 5 acc	0.6358	0.6316	0.5735	0.5886	0.5813	0.9556CIFAR-10	Original	FGSM	DIST	PGD	Mixup	Ours		=0.5	c=0.01	= 0.3, 50 iterations	α = 0.12, 16×data	r = 10, 16×dataCW rob	38.010	38.210	38.503	38.108	37.648	38.746Test acc	0.8395	0.7995	0.7935	0.7791	0.8521	0.8249FGSM1 acc	0.4922	0.4927	0.3825	0.4588	0.7483	0.6853FGSM3 acc	0.4463	0.6517	0.2241	0.3848	0.7287	0.6806FGSM5 acc	0.4093	0.7572	0.1998	0.3405	0.7192	0.6721PGD 3 acc	0.2987	0.2233	0.1871	0.5291	0.5018	0.4111GAU 5 acc	0.3701	0.6356	0.6169	0.5390	0.3371	0.6961
