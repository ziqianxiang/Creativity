Figure 1: An example of answering compound questions. Given a question Q, we first identify thetopic entity e with entity linking. By relation detection, a movie-to-actor relation f1, an actor-to-movie relation f2 and a movie-to-writer relation f3 forms a path to the answers Wi . Note that eachrelation fi corresponds to a part of the question. If we decomposes the question in a different way,we may find a movie-to-movie relation g as a shortcut, and g(e) = f2(f1 (e)) = (f2 ◦ f1)(e) holds.
Figure 2: An overview of our model and the flow of data. Two orange rounded rectangles correspondto components of our model, a learning-to-decompose agent and three simple-question answerers.
Figure 3: A zoom-in version of the lower half of figure 2. Our agent consists of two components: aMemory Unit and an Action Unit. The Memory Unit observes current word at each time step t andupdates the state of its own memory. We use a feedforward neural network as policy network for theAction Unit.
Figure 4: A continuous example of figure 1. The hollow circle indicates the corresponding actionthe agent takes for each time step. The upper half is the actual prediction while the lower half is apotential partition. Since we do not allow a word to join two partitions, the agent learns to separate”share” and ”actors” into different partitions to maximize information utilization.
