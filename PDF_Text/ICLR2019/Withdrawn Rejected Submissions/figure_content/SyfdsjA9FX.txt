Figure 1: (a) The architecture of our network. For conditioning, a pre-trained face recognitionnetwork is used. (b) An illustration of the multi-image perceptual loss used, which employs tworeplicas of the same face recognition network.
Figure 2: Training the de-identification network. Shown, in each pair, are the source (left) and output(right) images. During training (but not during test), the target image used is the same as the sourceone and the output maintains the low-level features of the source and distances the high-level featuresfrom it.
Figure 3: Sample results for de-identification (zoom). Triplets of source frame, converted frame andtarget are shown. The modified frame looks similar but the identity is completely different.
Figure 4: Comparison with the work of Samarzija & Ribaric (2014) (taken from that paper’s sampleimage). (a) Original image (also used for the target of our method). (b) Our generated output.
Figure 5: Comparison with Wu et al. (2018) (taken from that paper’s sample image). Row 1 - Originalimages. Row 2 - results of Wu et al. (2018). Row 3 - Our generated outputs. The previous work doesnot maintain expression, pose, and facial hair.
Figure 6: De-Identification applied to the examples labeled as very challenging in the NIST FaceRecognition Challenge (Phillips et al., 2011) (taken from that paper’s sample image, images arezoomed in).
Figure 7: A sequence of images, generated with an incrementally (left-to-right) growing λ. Thegradual identity shift can be observed. (a) Source. (b) λ = -2 ∙ 10-7. (C) λ = -5 ∙ 10-7. (d)λ = -1 ∙ 10-6.(e) λ = -2 ∙ 10-6.
Figure 8: An ablation study. (a) Source image. (b) Our result. (c) No mask. Bad face edge, artifactsnear the mouth, glasses occlusion handled poorly. (d) Adversarial loss on masked output only.
Figure 9: Quantitative ablation study results: for our method - marked as (b) - and the variousvariants in columns (c)-(g) of Fig. 8 we measure the mean pixel-level distance and the mean IDdistance (as evaluated by the differences in the last layer of the VGGFace2 classifier), both in L1.
Figure 10: The images from the user study. Each column is a different individual. The first row arethe gallery images, i.e, the album images the users were asked to select the identity from. The secondrow is the input image. The third row is the output of our method, i.e., the de-identified version of thesecond row.
Figure 11: Same as Fig. 3 but without the zoom.
Figure 12: Same as Fig. 6 but without the zoom.
