Figure 1: Siamese network network structure. The convolutional portion of the network includes 3convolution layers of 16 filters with size 10 × 10 and stride 5 × 5, 32 filters of size 5 × 5 and stride2 × 2 and 32 filters of size 3 × 3 and stride 1 × 1. The features are then flattened and followed bytwo dense layers of 256 and 128 units. The majority of the network uses ReLU activations exceptthe last layer that uses a sigmoid activation. Dropout is used between the convolutional layers. TheRNN-based model uses a GRU layer with 128 hidden units, followed by a dense layer of 128 units.
Figure 2: Still frame shots from trained policy in the humanoid2d environment.
Figure 3: Ablation analysis of the method. We find that training RL policies is sensitive to size anddistribution of rewards. The siamese network benefits from a number of training adjustments thatmake it more suitable for RL.
Figure 4: Baseline comparisons between our sequence-based method, GAIL and TCN. In 4a Wecompare our method to GAIL and a VAE where use using the euclidean distance of the encodings.
Figure 5: Still frame shots of the agent’s motion after training on humanoid3d walking.
Figure 6: Still frame shots of the agent’s motion after training on humanoid3d running.
Figure 7: Training losses for the siamese distance metric. Higher is better as it indicates the distancebetween sequences from the same class are closer.
Figure 8: RL algorithm comparison on humanoid2d environment.
