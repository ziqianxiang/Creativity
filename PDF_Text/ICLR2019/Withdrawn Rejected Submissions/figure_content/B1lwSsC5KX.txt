Figure 1: In/out classification performance (train) on Tiny, for varying image subsets and num-ber of images. The colors indicate the type of data augmentation: purple=none, green=flip,cyan=flip+crop±1, orange=flip+crop±2. The vertical line shows the number of positive imagesplim such that C(plim) is the number of parameters of the network.
Figure 2: Left: Cumulative distribution of the maximum classification score for a sample of 5000images taken from 4 datasets. Imnet1k-train served as the training set and therefore Imnet1k images(both train and val) have higher confidence. Right: binary classification accuracy (%) of a sampleof m elements from the training set Imnet1k-train w.r.t. three other datasets: Imnet1k-val (solid),Imnet22k (dashed) and Yfcc100M (lines). The architecture is indicated by the line color.
Figure 2: most samples coming from the source Imnet1k-train have a very high confidence, while thedistribution of the source Imnet1k-val is more balanced and unrelated sources (Yfcc100M, Imnet22k)tend to have a more uniform distribution.
Figure 3: Histogram of distances of the images of Imnet22k to their nearest neighbor.
Figure 4: Tiny nets.
Figure 5:	Image that appears in the largest number of duplicate versions in Imnet22k (72), with afew of the corresponding synsets.
Figure 6:	Accuracy over iterations of the in/out training on Yfcc100M for different networks andamount of data augmentation (indicated by color: purple= none, green = flip, cyan = flip+crop±5).
Figure 7:	Filters of the first convolutional layer (7x7, 64 filters) obtained when learning to explicitlymemorize if an image was used for training or not.
