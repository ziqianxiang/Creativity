Figure 1: Characteristics of the vMF distribution in a 2-dimensional space. 100 random samples aredrawn from vMF(μ, K) where μ = (1,0)> and K = {0,5, 50}.
Figure 2: The directions of minibatch gradients become more important than their lengths as thebatch size increases. (a) The gradient norm Stochasticity of g(w) with respect to various batch sizesat 5 random points w with mean (black line) and mean±std.(shaded area) in a log-linear scale; (b) Ifthe gradient norm stochasticity is sufficiently low, then the directions of gi (w),s need to be balancedto satisfy P3=1 gi(w) ≈ 0.
Figure 3: (a) Asymptotic angle densities (2) of θ(u, V) = 1∏80 cos-1(u, Vi where U and V areindependent uniformly random unit vectors for each large dimension d. As d → ∞, θ(u, v) tendsto less scattered from 90 (in degree). (b-c) We apply SGD on FNN for MNIST classification withthe batch size 64 and the fixed learning rate 0.01 starting from five randomly initialized parameters.
Figure 4: (a) If the	point	is slightly moved	from w0 to W0 by	e Ei Xi where Xi	=	(Pi -w00)/kpi - w00k and	x0i =	(pi - w0)/kpi -	w0k, then k Pix0ik	< k Pi xik which	is	equiva-lent to κ(w0) < ^(w0); (b) If gi(w0),s are sufficiently parallel to (Pi — w0),s for each i, thenw0 ≈ w0 + η Ei gi (w0). When w0 and w0 are sufficiently close to each other, We also have^(w0) < κ(w0).
Figure 5: We show the average K (black curve) ± std. (shaded area), as the function of the numberof training epochs (in log-log scale) across various batch sizes in MNIST classifications using FNNwith fixed learning rate 0.01 and 5 random initializations. Although K with the large batch sizedecreases more smoothly rather than the small batch size, We observe that K still decreases well withminibatches of size 64. We did not match the ranges of the y-axes across the plots to emphasize thetrend of monotonic decrease.
Figure 6: We show the average K (black curve) ± std. (shaded area), as the function of the numberof training epochs(in log-log scale) for all considered setups. We report the name of architecture,dataset and maximum valid accuracy(mean±std.) of all epochs. Although K decreases eventuallyover time in all the cases, batch normalization (+BN) significantly reduces the variance of the di-rectional stochasticity ((a-d) vs. (e-h)). We also observe that the skip connections make K decreasemonotonically ((c,g) vs. (d,h)). Note the differences in the y-scales.
Figure 7: (First row) We plot the evolution of the training loss (Train loss), validation loss (Validloss), inverse of gradient stochasticity (SNR), inverse of gradient norm stochasticity (normSNR)and directional uniformity K. We normalized each quantity by its maximum value over training foreasier comparison on a single plot. In all the cases, SNR (orange) and K (red) are almost entirelycorrelated with each other, while normSNR is less correlated. (Second row) We further verify thisby illustrating SNR-^ scatter plots (red) and SNR-normSNR scatter plots (blue) in log-log scales.
Figure 8: We show K estimated from {1,000 (black), 2,000 (blue), 3,000 (red)} random samples ofthe vMF distribution with underlying true κ in 10, 000-dimensional space, as the function of κ (inlog-log scale except 0). For large κ, it is well-estimated by K regardless of sample sizes. When thetrue κ approaches 0, we need a larger sample size to more accurately estimate this.
Figure 9: (a,c,e) We plot the evolution of the training loss (Train loss), validation loss (Valid loss),inverse of gradient stochasticity (SNR), inverse of gradient norm stochasticity (normSNR) and di-rectional uniformity κ. We normalized each quantity by its maximum value over training for easiercomparison on a single plot. In allthe cases, SNR (orange) and K (red) are almost entirely correlatedwith each other, while normSNR is less correlated. (b,d,f) We further verify this by illustrating SNR-K scatter plots (red) and SNR-normSNR scatter plots (blue) in log-log scales. These plots suggestthat the SNR is largely driven by the directional uniformity.
Figure 10: (a,c,e) We plot the evolution of the training loss (Train loss), validation loss (Validloss), inverse of gradient stochasticity (SNR), inverse of gradient norm stochasticity (normSNR) anddirectional uniformity κ. We normalized each quantity by its maximum value over training for easiercomparison on a single plot. In allthe cases, SNr (orange) and K (red) are almost entirely correlatedwith each other, while normSNR is less correlated. (b,d,f) We further verify this by illustrating SNR-K scatter plots (red) and SNR-normSNR scatter plots (blue) in log-log scales. These plots suggestthat the SNR is largely driven by the directional uniformity.
Figure 11: (a,c,e) We plot the evolution of the training loss (Train loss), validation loss (Validloss), inverse of gradient stochasticity (SNR), inverse of gradient norm stochasticity (normSNR) anddirectional uniformity ^. We normalized each quantity by its maximum value over training for easiercomparison on a single plot. In allthe cases, SNr (orange) and K (red) are almost entirely correlatedwith each other, while normSNR is less correlated. (b,d,f) We further verify this by illustrating SNR-K scatter plots (red) and SNR-normSNR scatter plots (blue) in log-log scales. These plots suggestthat the SNR is largely driven by the directional uniformity.
Figure 12: (a,c,e) We plot the evolution of the training loss (Train loss), validation loss (Validloss), inverse of gradient stochasticity (SNR), inverse of gradient norm stochasticity (normSNR) anddirectional uniformity κ. We normalized each quantity by its maximum value over training for easiercomparison on a single plot. In allthe cases, SNr (orange) and K (red) are almost entirely correlatedwith each other, while normSNR is less correlated. (b,d,f) We further verify this by illustrating SNR-K scatter plots (red) and SNR-normSNR scatter plots (blue) in log-log scales. These plots suggestthat the SNR is largely driven by the directional uniformity.
