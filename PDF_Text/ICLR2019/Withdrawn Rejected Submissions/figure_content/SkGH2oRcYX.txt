Figure 1: General figure showing the forward model architectures and the networks used for training.
Figure 2: InfoGAN vs Vanilla GANZTo the best of our knowledge, this is the first proposal of using such a framework of conditionaladversarial training for forward-modeling. In this framework, an information-theoretic regularizationenforces high mutual information between latent-codes c and the generator distribution G(z, c) in anunsupervised way. The mutual information I(X, Y ) between two random variables X and Y can beexpressed as the difference of two entropy terms:I(X,Y) = H(X) - H(X|Y) = H(Y) - H(Y|X).
Figure 3: The first row represents the generated samples sequence. The second one is the realsequence.
Figure 4: First row represents the generated samples with Adversarial, Mutual Information, andContent losses. Second one is samples generated by pixelwise loss. Finally, the last row presents theground truth images5 ConclusionIn this paper, we propose a novel method and a model that produces realistic samples of the obser-vation of a given environment conditioned by the previous observation and action. This so-calledforward-model is generic and imposes no specific restrictions over the modalities used as input.
