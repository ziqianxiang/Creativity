Figure 1: Adversarial examples (generated by PGD against a netWork With Random Mask) that can “fool” aCNN With Random Mask. The labels here are the outputs of the netWork being “fooled”. The original imagesfrom CIFAR-10 and more examples can be found in Figure 12 in Appendix E.1.
Figure 2: An illustration of Random Mask applied to three channels of a layer (neuron-wise). Note that thenumber of parameters in the network is not reduced after applying Random Mask.
Figure 3: The first image is the original image, and the other three contain different types of small perturbations.
Figure 5: An example image that is randomly shuffled after being divided into 1 × 1, 2 × 2, 4 × 4 and 8 × 8patches respectively.
Figure 6: The architecture of ResNet-183X3 cosw 643X3 CCmR 64C.2 RESNET-50Similar to ResNet-18, ResNet-50 (He et al., 2016) contains 5 blocks and each block contains several 1 × 1and 3 × 3 convolutional layers (i.e. Bottlenecks). In our experiment, we apply Random Mask to the 3 × 3convolutional layers in the first three “shallow” blocks. The masked layers in the 1st block are marked by thered arrows.
Figure 7: The architecture of ResNet-50C.3 DenseNet- 1 2 1DenseNet-121 (Huang et al., 2017) is another popular network architecture in deep learning researches. Itcontains 5 Dense-Blocks, each of which contains several 1 × 1 and 3 × 3 convolutional layers. Similar to what14we do for ResNet-50, we apply Random Mask to the 3 × 3 convolutional layers in the first three “shallowblocks. The growth rate is set to 32 in our experiments.
Figure 8: The architecture of DenseNetDense-Block 1Dense-Block 2AiPOOlingIXI com?Dense-Block 3AGg POOIing↑IXl CoH寸Dense-Block 4C.4 SENETSENet (Hu et al., 2017a), a network architecture which won the first place in ImageNet contest 2017, isshown in Figure 9. Note that here we use the pre-activation shortcut version of SENet and we apply RandomMask to the convolutional layers in the first 3 SE-blocks.
Figure 9: The architecture of SENetC.5 VGG-19VGG-19 (Simonyan & Zisserman, 2014) is a typical neural network architecture with sixteen 3 × 3 convolu-tional layers and three fully-connected layers. We slightly modified the architecture by replacing the final 3fully connected layers with 1 fully connected layer as is suggested by recent architectures. We apply RandomMask on the first four 3 × 3 convolutional layers.
Figure 10: The architecture of VGG-19D Training Process on CIFAR-10 and MNISTTo guarantee our experiments are reproducible, here we present more details on the training process inour experiments. When training models on CIFAR-10, we first subtract per-pixel mean. Then we apply azero-padding of width 4, a random horizontal flip and a random crop of size 32 × 32 on train data. No otherdata augmentation method is used. We apply SGD with momentum parameter 0.9, weight decay parameter5 × 10-4 and mini-batch size 128 to train on the data for 350 epochs. The learning rate starts from 0.1 and isdivided by 10 when the number of epochs reaches 150 and 250. When training models on MNIST, we firstsubtract per-pixel mean. Then we apply random horizontal flip on train data. We apply SGD with momentumparameter 0.9, weight decay parameter 5 × 10-4 and mini-batch size 128 to train on the data for 50 epochs.
Figure 11: Train and test curve of normal ResNet-18 and Random Masked ResNet-18 on CIFAR-10 andMNIST.
Figure 12: The adversarial examples (upper) shown in Figure 1 along with the original images (lower) fromCIFAR-10.
Figure 13: Adversarial examples (upper) generated from Tiny-ImageNet against ResNet-18 with RandomMask of ratio 0.9 on the 1st, 2nd blocks. along with the original images (lower). The attack methods are PGDwith scale 64 and 32, step size 1 and step number 40 and 80 respectively.
Figure 14: Relationship between defense rate against adversarial examples generated by PGD and testaccuracy with respect to different drop ratios under Madry’s setting (Madry et al., 2017). Each red starrepresents a specific drop ratio with its value written near the star. We can see the trade-off between robustnessand generalization.
Figure 15: Randomly sampled images from Tiny-ImageNet dataset. The network structure used to generatethese images is ResNet-18 with Random Mask of ratio 0.9 on the 1st, 2nd blocks. The attack method is PGDwith perturbation scale 64, step size 1 and step number 80. For each image, we show the image generatedagainst network with Random Mask (upper), the image generated against the normal ResNet-18 (middle) andthe original image (lower).
