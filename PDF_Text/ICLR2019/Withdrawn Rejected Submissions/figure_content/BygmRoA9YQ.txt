Figure 1: Object detection with mixture of pre-processing experts modelimagesFigure 2: Gating network architecture√3	Mixture of Pre-processing ExpertsReasonable amount of image distortion is useful as a regularization when the test data distribution isexpected to be in the same distribution with the training data. In our scenario, the input distribution isdiverse including various resolution and noisy images. Thus, data augmentation increases the overalldetection accuracies, however, it reduces individual accuracy compared to the network trained onlyon the specific data distribution. Table 2 shows that the network trained with clean, low resolutionand noisy images improves detection accuracy for low resolution and noisy images compared to thebaseline at the expense of decreased accuracy.
Figure 2: Gating network architecture√3	Mixture of Pre-processing ExpertsReasonable amount of image distortion is useful as a regularization when the test data distribution isexpected to be in the same distribution with the training data. In our scenario, the input distribution isdiverse including various resolution and noisy images. Thus, data augmentation increases the overalldetection accuracies, however, it reduces individual accuracy compared to the network trained onlyon the specific data distribution. Table 2 shows that the network trained with clean, low resolutionand noisy images improves detection accuracy for low resolution and noisy images compared to thebaseline at the expense of decreased accuracy.
Figure 3: Denoise network architectureFigure 4: Training framework for denoise network with similarity and adversarial lossPre-processing for the noisy images: We consider average filter and denoise neural network as acandidate pre-processing for the noisy images. Average filter was very powerful pre-processor for thenoisy images considering its simpleness, thus, included in our experiments. We also consider U-Netlike denoise neural network (Ronneberger et al., 2015) as shown in Figure 3. We intentionally usethe simple architecture to reduce the computational complexity. All the convolution and transposedconvolution have 3x3 kernels, and the number of filters increases/decreases by 2 when the featuresare down/up sampled with stride 2. As our denoise network is pixel to pixel transformation betweensimilar images, skip connection helps preserve the color information in original images, thus, eventu-ally produce better quality. Average filter at front reduces the random noise in images at the expenseof loss the edge information. Loss of the edge information is recovered by training denoise networkwith adversarial loss (Goodfellow et al., 2014) which will be discussed in the following section.
Figure 4: Training framework for denoise network with similarity and adversarial lossPre-processing for the noisy images: We consider average filter and denoise neural network as acandidate pre-processing for the noisy images. Average filter was very powerful pre-processor for thenoisy images considering its simpleness, thus, included in our experiments. We also consider U-Netlike denoise neural network (Ronneberger et al., 2015) as shown in Figure 3. We intentionally usethe simple architecture to reduce the computational complexity. All the convolution and transposedconvolution have 3x3 kernels, and the number of filters increases/decreases by 2 when the featuresare down/up sampled with stride 2. As our denoise network is pixel to pixel transformation betweensimilar images, skip connection helps preserve the color information in original images, thus, eventu-ally produce better quality. Average filter at front reduces the random noise in images at the expenseof loss the edge information. Loss of the edge information is recovered by training denoise networkwith adversarial loss (Goodfellow et al., 2014) which will be discussed in the following section.
Figure 5: Sample images after the MoPE layer for the model 8 in Table 2Table 3: Average tracking accuracy on MOT17 Train Set. Faster R-CNN Ren et al. (2015) withinception v2 Szegedy et al. (2016) backbone networkMOTA	Model 1	Model 6	Model 7 (OUrs)	Model 8 (Ours)Clean	0.217	0.222	0.224	0.228σ = 0.15	0.161	0.170	0.191	0.1976.1	Object DetectionWe first show the object detection accuracy on MS-COCO validation dataset as shown in Table 2.
Figure 6: Activity classification networkScoreTable 4: Human activity classification accuracy on UCF 101 Dataset. Faster R-CNN Ren et al. (2015)with inception v2 Szegedy et al. (2016) backbone network is used.
