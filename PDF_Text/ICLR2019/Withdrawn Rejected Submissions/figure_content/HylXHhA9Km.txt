Figure 1: Illustration of the distributional pathologies of vanilla networks with Nl = 128 and L =200 layers. (a) Geometric increments of the normalized sensitivity δζl with rapid evolution towardsδζl ' 1. (b) The normalized sensitivity ζl has sub-exponential evolution since it is limited by neuralnetwork pseudo-linearity. (c) 2-dimensional cuts of preactivation feature maps fm(y200, α) usingthe direction of average vector (ν1,c(y200)) and a random orthogonal direction (ν1,c(y200))⊥. (d-e)Evolution of the distributions of μ2(xl) and μ2(sl) with depth showing clear lognormality.
Figure 2: Illustration of the distributional pathology of batch-normalized feedforward nets withNl = 384 and L = 200 layers. (a) Geometric increments δζl and their decomposition as theproduct of two terms: a batch normalization term corresponding to the evolution from (xl-1 , sl-1)to (zl, ul) and a nonlinearity term corresponding to the evolution from (zl, ul) to (xl, sl). (b)The normalized sensitivity ζl has exploding behaviour. (c) Effective ranks of signal and sensitivityconfirm that sensitivity is much better conditioned than signal. There is a clear inverse correlationbetween reff(xl) and the batch normalization term in δZl. (d) Zl becomes ill-behaved as νι(∣zl∣)vanishes and μ4(zl) explodes. This explains the decay of the nonlinearity term in δZl.
Figure 3: Illustration of the well-behaved evolution of batch-normalized resnets with Nl = 384 andL = 500 residual units ofH = 2 layers. (a) Decomposing δζl,h as the product of two terms: a batchnormalization term and a nonlinearity term. (b) The normalized sensitivity ζl has power law evolu-tion. (c) Effective ranks of signal and sensitivity indicate that many directions of signal variance arepreserved. (d) Moments νι(∣zl |) and μ4(zl) indicate that Zl has nearly Gaussian distribution.
Figure 4: Illustration of the normalized sensitivity for fully-connected networks of L layers, with1-dimensional input N0 = 1 and 1-dimensional output NL = 1. The distribution of the input V is amixture of two Gaussians. We show the result of the propagation in three different cases: (a) L = 1layer with sigmoid activation, (b) L = 1 layer with linear activation, (c) L = 25 randomly initializedlayers, with Nl = 100 channels and ReLU activation for 1 ≤ l < L, and linear activation in the finallayer l = L. Top: full input-output mapping (blue curve) and randomly sampled input-output datapoints (red circles). Bottom: histograms of inputs and outputs.
