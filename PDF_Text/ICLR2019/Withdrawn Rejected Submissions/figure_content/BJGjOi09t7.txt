Figure 1: Diagram of an autoencoder designed to perform NMF. The weights of the final layer, Wf,become the directions of the subspace, with the outputs of the hidden layer, h, as the coefficientsin that new subspace. The activation function that produces h must be non-negative as must all theelements of Wf.
Figure 2: General PAE-NMF with stochasticity provided by the input vector .
Figure 3: (Top) Top five are original faces with the equivalent recreated faces below. (Bottom left)Nine stocks with original values (black dashed) and recreated values (red dotted). (Bottom right)1000 recreated elements plotted against the equivalent original.
Figure 4:	(Left) Each small image is one of the 81 reshaped columns of Wf for the faces data-set.
Figure 5:	The distributions of h for one data-point from the faces data-set with r = 9. (Left) Theseplots show the distributions when we include the DKL term. (Right) The DKL term is not appliedduring training. The black dashed line shows results when we train the network deterministicallyusing the median value of the distribution and the blue line is when we trained with random samples.
Figure 6:	The left images (top and bottom) are similar to one another and we plot their distributionsas black dashed lines in the plots to the right. The right images are very different to the left, we plottheir distributions as red dotted lines. The similar images have very similar distributions for thesedata-points.
Figure 7: (Left) Sampling from the distributions of the faces data-set with r = 9. Four original facesare on the left column, with faces drawn deterministically from the centre of the distribution next tothem and three sampled faces along the next three columns. (Right) Sampling from the FTSE 100data-set with r = 9 for four different stocks. The solid black line is the real data and the dotted linesshow three sampled versions.
