Figure 1: Graphical model representation of GD, minibatch SGD and backdrop with Bernoullimasking. The solid and dashed lines represent the forward pass and gradient computation respectively.
Figure 2: Backdrop on subsampleswith Bernoulli masking. The dottedplate denotes the fact that the subsam-ples are generically not iid.
Figure 3: ROC AUC vs. mask probability p fordifferent batch sizes.
Figure 4: The results of the CIFAR10 experiment with non-decomposable loss. The solid lines in (a) denoteLdist and the dashed lines represent LdMiBst, the average of Ldist evaluated on minibatches.
Figure 5: Each sample in the two-scale GP dataset is the convolution of two GP processes with different scales.
Figure 6: Cartoon of a masked convolutional network with masking layers at two different scales. The maskinglayers are transparent during the forward pass (solid lines) but block some percentage of the gradients during thebackward pass (denoted by the dashed lines). The exact number of convolution, max pool and masking layersfor each experiment is given in Tab. 1.
Figure 7: Test accuracy vs. mask probability pfor different batch sizes.
Figure 8: Samples from the Ponce texture dataset.
Figure 9: Ratio of test loss with backdrop to testloss without backdrop vs. mask probability p.
