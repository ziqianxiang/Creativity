Figure 1: Three steps of our few-sample knowledge distillation. (1) design student-net from scratch or bycompressing teacher-net; (2) add 1×1 conv-layer at the end of each block of student-net (before ReLU), andalign teacher and student by estimating the parameter using least-squared regression; (3) absorb or merge theadded 1×1 conv-layer into the previous conv-layer to obtain final student-net.
Figure 2:	Accuracy vs #samples on CIFAR-10. Student-net (a) scheme-A (b) scheme-B by filter pruning.
Figure 3:	(a) Layer-level output correlation between "teacher" and "student" before and after FSKD onstudent-nets (scheme-A) by filter pruning. (b) Accuracy change during sequentially block-level alignment.
Figure 4: Accuracy vs #samples for zero student network on (a) CIFAR-10, (b) CIFAR-100.
Figure 5: Accuracy vs #iterations of FSKD-BCD on CIFAR-10. Student-net is scheme-B by filter pruning.
Figure 6: Illustration of FSKD on filter pruning andnetwork slimming. At each block, we copy weightsof the unpruned part in teacher-net to student-net, andalign the feature maps of student-net to those unprunedfeature maps of teacher-net by adding a 1 × 1 conv-layer (red-color) with L2-loss. The added 1 × 1 can bemerged into the previous conv-layer in student-net.
Figure 7: Illustration of FSKD on network decoupling.
Figure 8: Decouple VGG13 into depthwise (DW) and pointwise (PW) conv-layers, and show one PW layerbefore SGD with random initialization (left), after SGD (middle), and after FSKD (right). Note values of the PWtensor are scaled into the range (0,1.0) by the min/max values of the tensor for better visulization.
