Figure 1: Left: Prediction of a small ReLU-network (one hidden layer with 3 neurons) trainedto distinguish samples from two circles. Middle: Characterization of the preimage of first layeractivations into unbounded (infinite), compact (finite) or unique (a single point). Right: Condition ofthe linearization of the first layer at each point.
Figure 2: Gray lines are hyperplanes with normal vectors (arrows) from the rows of A and translationb. Left: Omnidirectional tuple (A, b) forp ∈ R2, as hyperplanes intersect inp and normal vectors areomnidirectional. Two in the middle: Intersection in p, but vector-free halfspaces (hence, not omnidi-rectional). Right: hyperplanes do not intersect in a point, but normal vectors are omnidirectional.
Figure 3: Removal of vectors due to ReLU (red crosses)for the marked points x (left:unbiased setting, right:biased setting). The remaining vectors are only weaklycorrelated to the removed one, thus yielding an unstableinverse.
Figure 4: The number of (in-)finite volumedpreimages of a ReLU layer over the test setof MNIST. Only within the gray strip wesee finitely and infinitely volumed preim-ages.
Figure 5: Blue: WideCIFAR, Red: ThinCIFAR.
Figure 6: Decay of singular values over the lay-ers of the network. Here, each layer includesthe convolution and ReLU-activation. Reportednumber are taken from median over 50 samples.
Figure 7: Invariances of the first layer (100 ReLU neurons) of a vanilla multilayer perceptron (MLP).
Figure 8: Effect of ReLU on the singular values for WideCifar. The curves show the effect in layer 2(layer 3 and 4 in the legend, because ReLU is counted as an extra activation layer) and layer 5 (layer9 and 10), where each curve is the median over 50 samples.
Figure 9: Curve showing how many rows ai satisfy condition 3 from Lemma 9 depending on valuesof constant c. The red line shows the total number of remaining rows after removal by ReLU,M = 5120. Even for small constants c most ai fulfill condition 3, yet not all, which is required bythe lemma to give an upper bound on the smallest singular value. The example is from layer 4 ofWideCIFAR, for only one sample from the test set.
Figure 10: Left: Effect of ReLU on the singular values for ThinCifar. The curves shoW the effect inlayer 2 (layer 3 and 4 in legend, because ReLU is counted as an extra activation layer) and layer 5(layer 9 and 10). Right: Decay of singular values over the layers ThinCifar. Here, each layer includesthe convolution and ReLU-activation. Reported number are taken from median over 50 samples.
Figure 11: Left: Effect of ReLU on the singular values for the MLP on MNIST. The curves show theeffect in layer 1(layer 1 and 2 in legend, because ReLU is counted as an extra activation layer) andlayer 2 (layer 3 and 4). Right: Decay of singular values over the layers of MLP on MNIST. Here,each layer includes the fully-connected layer and ReLU-activation. Reported number are taken frommedian over 50 samples.
Figure 12: Invariances of the first layer (100 ReLU neurons) of a vanilla MLP. (Exact architecture inAppendix A4 Table 1.)Afterwards, we solved following linear programming problem to find a perturbed x:max hc, xisubject toA|y* A0X + b|y* A0 = y* |y* A0A∣y* Y0X + b|y* Y0 W 0X∈ [0, 1]kwhere the features of the first layer are computed viay* = ReLU(AX* + b).
