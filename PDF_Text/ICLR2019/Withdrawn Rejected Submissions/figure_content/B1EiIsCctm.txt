Figure 1: Top row shows a snapshot of the GM-VAE after 10 training steps. Loss curves are shownin (a), the discrete variational distribution in (b) with rows ' representing E{x∣label(x)='}qD (k|x), andreconstructions are shown in (c). Bottom row shows the same snapshot after 6000 training steps.
Figure 2:	Shown in (a) are reconstructions of held-out data from the inferred latent variables. Thefirst, third, etc, rows are the raw data, and the rows below show the corresponding reconstructions.
Figure 3:	Reconstructions from linear interpolations in the continuous latent space between two datapoints (a), and between the prior mode μ0, and the other nine prior modes μk=6 (b). In (a), the truedata points are shown in the first and last column next to their direct reconstructions.
Figure 4:	(a) Reconstructions for an untrained VAE initialized with same parameters as our trainedWAE. (b) Those same reconstructions after a few dozen thousands training steps according to theVAE objective. (c) Learning curves from an untrained VAE initialized with same parameters as ourtrained WAE. (d) Reconstruction loss for different VAE variations.
Figure 5: Visualization of the variational distributions. (a) shows E{x∣label(x)='}qD(k|x) in row`. (b) shows the accuracy as a function of the training steps for our method and the same VAEvariations than Figure 4d. (c) shows z|x 〜Pk qc(z|k, x)qD(k|x) dimensionally reduced usingUMAP (McInnes & Healy, 2018). 1000 encoded test-set digits and 1000 samples from the prior areused. Encoded points are colored by their digit label.
