Figure 1: Actor and learner process pseudocode.
Figure 2: Features used as input to the reward function. Blue color indicates the flow of gradientfrom the discriminator objective. In all cases, D(S) is a single layer network projecting the featuresonto a scalar score. (a) Using features from the value network, which are also changing online withthe discriminator. (b) Using pixels directly; i.e. D(S) becomes a deep convolutional network. Weuse the same architecture as for the value network. (c) Random projections - same as (b) exceptthat gradients do not flow into the convolutional network. (d) Using features learned by contrastivepredictive coding on the expert demonstrations. See section 3.2 for details about the pre-training.
Figure 3: Environments for visual imitation experiments. Top row: simulated Jaco arm withSpaceNavigator teloperation. Bottom row: planar walker running at a target velocity.
Figure 4: Comparison of our methods with D4PG baselines in dense and sparse reward scenarios.
Figure 5: Visualization of CPC predictions on (a) expert sequences and (b) non-expert trajectories.
Figure 6: Jaco stacking ablation experiments. Left: We find that adding layers to the discriminatornetwork does not improve performance, and not doing early termination hurts performance. Right:In the case of 120 demonstrations one of the three seeds failed to take off. However, even with 60demonstrations, our agent can learn stacking equivalently well as with 500.
Figure 7: (a): Walker2D experiments. Both random projections and value net features yield agentsthat learn to run, while pixel features even with norm clipping do not. (b): Training dynamics on Jacoblock stacking with early termination.
Figure 8: Example agent trajectories when trained only with imitation rewards. In the top row, theagent learns to stack in a more efficient way than the demonstrator, taking under 2s while the humanteloperator takes up to 30s. In the bottom row, we see an agent exploit, in which the top block isrolled to the background to give the appearance of a completed stack, without actually stacking.
Figure 9: Overview of our proposed approach. Left: model learning via contrastive predictive coding(CPC). Right: After training and freezing CPC expert model, We train the agent, which makes use ofCPC future predictions. Note that we never need to predict in pixel space.
