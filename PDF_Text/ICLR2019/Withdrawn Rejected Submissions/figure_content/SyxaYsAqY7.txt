Figure 1: Accuracy lower bounds for MNIST data set (left) and CIFAR-10 data set (right) againsttolerable sizes of attacks with various choices of σ in Algorithm 1.
Figure 2: MNIST data set Left: Average '2 norm of the gradients of the loss function for a batch ineach iteration during adversarial attack. Middle: Classification accuracy for Madry’s model underPGD attack and S-O attack. Right: Accuracy for different models under the same S-O attack.
Figure 3: CIFAR-10 data set. We compare the accuracy on adversarial examples with various attacksizes for both '2 (left) and '∞ (right).
Figure 4: The upper bounds under different p(1) and p(2). In both settings, we let the variance ofGaussian noise σ2 = 1. Our bound (red) is strictly higher than the one from PixedDP(blue).
Figure 5: S-O attack compared to C&W attack on MNIST under various `2 sizes.
Figure 6: Accuracy of adversarial training via L∞ and L2 attacks against L∞ S-O attack. The modelbased on L2 is clearly vulnerable to L∞ attacks.
Figure 7: Above: Natural examples from MNIST. The correct labels are 5,9,7,3,4,9,6. Below:Adversarial examples with perturbation size `2 = 2.0. The adversarially trained model predictionsare 3,4,2,8,9,4,5.
