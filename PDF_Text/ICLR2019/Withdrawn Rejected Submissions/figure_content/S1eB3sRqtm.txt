Figure 1: Noise structure in a simple two-dimensional regression problem. a) One-step SGD update. b) One-step SGD update with isotropic Gaussian (Ïƒ = 0.1) noise. c) One-step SGD update with full Fisher noise.
Figure 2: Trajectory using full Fisher noiseversus diagonal Fisher noise for the algorithmin Eqn. 2 used to minimize a two-dimensionalquadratic function. Blue dot indicates the initialparameter value and the green dot shows the finalparameter value. We used a learning rate of 0.1 for500 iterations (plotting every 10 iterations). Ob-serve that adding diagonal Fisher to the true gra-dient achieves faster convergence than full Fisher.
Figure 3: a) Frobenius norm of full Fisher matrix and diagonal Fisher matrix. The model is trained onResNet44 with CIFAR-10. b) Estimation of gradient variances for randomly selected convolutional layerson ResNet44 (CIFAR-10). fullF: LB with full Fisher noise. diagF: LB with diagonal Fisher noise. KfacF: LBwith K-FAC noise. C) Training error with respect to iterations between SB, LB, LB with full Fisher noise anddiagonal Fisher noise on ResNet44 (CIFAR-10). All of the above are trained with the same learning rate. d)The maximum eigenvalue of the Hessian matrix at the end of training. 4Kgbn: LB with Ghost-BN. 4KdiagF:LB with diagonal Fisher noise.
Figure 4: Validation error over number of epochs. Left: Training on CIFAR-10 with VGG-16. Right: Train-ing on CIFAR-100 with resnet44. The reason why 4K with diagonal Fisher converges faster than the 4K GBNbaseline in the beginning is because we need to use a larger learning rate for 4K GBN to get the best gener-alization performance, which makes it actually converge a bit slower in the beginning. If they share the samelearning rate scheme, 4K GBN would converge a bit faster because it is less noisy.
