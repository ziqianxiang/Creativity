Figure 1: Our SEDN model. VAE encodes state frame into latent vector; LSTM encodes currentlatent state and action, predicts next latent state; policy network takes current latent state and hiddenstate from previous timestep of RNN as input, predict action to execute2.2 Environment Dynamics Modeling for Episodic Data EncodingHow to learn a model that can encode episode data τ : s0, a0, s1, a1, . . . sT , aT ? We develop amethod by modeling environment dynamics. Perhaps the earliest work on building predictive modelfor vision based RL tasks was introduced by Schmidhuber & Huber (1991), which proposed usingneural networks to predict attention region given previous frames and actions. After the successof deep learning, Oh et al. (2015) proposed an Encoding-Transformation-Decoding framework foraction conditional video prediction, and applied on Atari games. More recently, Pathak et al. (2018)learns forward and inverse model dynamics to imitate expert’s behavior given only image sequences.
Figure 2: Boxplot of the training process of Frostbite. Maximum reward performance among policynetworks increases significantly after training of VAE and RNN triggered at 8th, 16th generations.
Figure 3: Boxplot of the training process of Atlantis. Maximum reward performance among policynetworks drops significantly after training of VAE and RNN triggered at 8th generations.
Figure 4: Comparison of VAE’s encoding-decoding on Atlantis and Frostbite. Upper: Originalimage; Lower: decoded image from the latent vector. Notice for Frostbite, decoded image agreeswith original image at the 16th frame; but for Atlantis decoder does not recover the original smallobjects at 16th frameSimilar to what Salimans et al. (2017) are facing: ES does not perform well on some easy gameslike Enduro and Breakout. One possible reason is that ES’s optimized policy is close to where itis randomly initialized due to evolution path control under ES, whereas optimal policy for densereward games like Breakout are far away from where it is randomly initialized. Another reasonis that, long episodic time horizon games like Enduro are very inefficient for ES to solve: givenlimited number of frames, a longer episode will result in less number of episodes, hence less andslower updates for ES. In addition, episode rewards ignore temporal reward information over timesteps. This issue is critical in games like Enduro. Enduro is a car racing game, in which agentobtain rewards after passing other cars and penalty if it is passed by other cars, final ranking will beaccounted as the episodic reward after a long episode. A random policy will always obtain a zeroepisodic reward under this case, hence ES learns nothing from such games. These two issues limitthe application of vanilla ES and its variants, remaining to be resolved in future works. A possibledirection is a mixture of ES and RL, e.g. using ES as a warm start of RL policy.
