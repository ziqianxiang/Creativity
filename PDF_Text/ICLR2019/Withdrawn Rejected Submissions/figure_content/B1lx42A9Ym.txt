Figure 1: (a) The Neural Rendering Model (NRM) captures latent variations in the data and yieldsCNNs as its inference. Costs for training CNNs are derived from likelihood estimations in NRM. (b)Graphical model depiction of NRM. Latent variables in NRM depend on each other. Object categoryy decides the class template, and then new latent variables are incorporated at each layer to renderintermediate images (red) with finer details. Finally, pixel noise is added to render the image x.
Figure 3: Rendering from level' to level' 一 1 in the NRM. At each pixel P in the intermediate imageh('), NRM renders iff the template selects latent variable s(',p) = 1. If rendering, the templateΓ(',p) is multiplied by the pixel value h(',p). Then the matrix B(',p) zero-pads the result to thesize of the intermediate image at level' 一 1. Next, the translation matrix T(',p) locally translatesthe rendered image to location specified by the latent variable t(',p). The same rendering repeats atother pixels of h('). NRM adds all rendered images to obtain the intermediate image h(' 一 1).
Figure 2: Reconstructed images at each layer in a 5-layer NRM trained on MNIST with 50K labeled data.
Figure 4: The Max-Min networkMaX-MinXentroPyMinXentroPyrealize this new loss. These networks have two CNN-like branches that share weights. The maxbranch estimates Zmax using ReLU and Max-Pooling, and the min branch estimates zmin using theNegative ReLU, i.e., min(∙, 0), and Min-Pooling. The Max-Min networks can be interpreted asa form of knowledge distillation like the Born Again networks (Furlanello et al., 2018)and theMean Teacher networks. However, instead of a student network learning from a teacher network, inMax-Min networks, two students networks, the Max and the Min networks, cooperate and learn fromeach other during the training.
Figure 5: ResNet building block as in He et al. (2016)xidentitythe ResNet. As mentioned above, identity shortcuts and projection shortcuts are two types of skipconnections in the ResNet. The operator =d implies that discriminative relaxation is applied.
Figure 6: DenseNet building block as in Huang et al. (2017)24Under review as a conference paper at ICLR 2019Appendix CIn this appendix, we provide the proofs for key results in the paper.
