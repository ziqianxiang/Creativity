Figure 1: Illustrated probability simplex with Gaussian-Softmax, GEM, and Dirichlet distributions.
Figure 2: Sub-figures 2a, 2b, and 2c are the graphical notations of the VAEs as latent variablemodels. The solid lines indicate the generative sub-models where the waved lines denote a priordistribution of the latent variables. The dotted lines indicate the inference sub-models. Sub-figure2d denotes a neural network structure corresponding to Sub-figure 2c. Red nodes denote the randomnodes which allow the backpropagation flows to the input.
Figure 3: Latent dimension visualization with reconstruction images and t-SNE latent embeddings.
Figure 4: Sub-figure 4a shows GVAE and GVAE-Softmax have component collapsing issue, whileSBVAE and DirVAE do not. Sub-figure 4b shows that SBVAE has many near-zero output values inthe latent dimensions.
Figure 5: Decoder weight collapsing and t-SNE latent embeddings visualization of GVAE-NF20 onMNIST.
Figure 6: The optimized dimension-wise Î± values from DirVAE-Learning with MNIST.
Figure 7: 20Newsgroups latent document embedding visulaization with t-SNE by replacing themodel prior to the Dirichlet. (Left) ProdLDA+DirVAE, (Middle) NVDM+DirVAE, (Right)GSM+DirVAE.
Figure 8: 20Newsgroups latent document embedding visulaization with t-SNE by replacing themodel prior to the Stick-Breaking. (Left) ProdLDA+SBVAE, (Middle) NVDM+SBVAE, (Right)GSM+SBVAE.
Figure 9: 20Newsgroups latent document embedding visulaization with t-SNE of original models.
