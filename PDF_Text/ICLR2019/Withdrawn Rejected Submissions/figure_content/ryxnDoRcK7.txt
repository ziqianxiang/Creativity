Figure 1: Architecture diagram for the Y-learner.
Figure 2: Testing the importance of co-learning in the Y-learner.
Figure 3: Performance of R, X, Y, S, and T learners on six simulated data benchmark tasks. The datais synthetically generated to make estimating the CATE difficult. We see that the Y-learner deliversthe best performance on simulations 1, 2, and 4. On simulations 3, 5, and 6 it delivers comparablefinal performance to all extant methods. On most simulations, the Y-learner requires the least data tolearn a good CATE estimate.
Figure 4: Total training time in seconds for the S, T, R, X, and Y-learners on simulated dataset 2. Wesee that the X and Y learners are roughly twice as expensive as the simpler T learners. The S-learnerrequires about half the compute of the T-learner, making it the cheapest option. Due to the R-learnerâ€™stwo step estimation procedure, it takes an order of magnitude longer.
Figure 5: Learning curves for the GOTV task. The R, T, and X learners end with final MSEs of 0.8,1.5, and 2.0 respectively. The Y-learner achieves the best performance. The S-learner also does quitewell.
Figure 6: Results on the MNIST task. The X, R, and T learners have fairly flat learning curves andend with MSEs of 12.8, 8.6, and 14.1 respectively, so they are omitted here. The S-learner does muchbetter than the Y-learner until there are around 14,000 points in the training set.
