Figure 1: Channels aggregation network: Blue dotted rectangle represents the channelsaggregation layer. Along with the channel directions the channels aggregation layer is em-bodied inside of CNN, and the final feature prediction is obtained by combining informationfrom all the outputs of NCAL.
Figure 2: The nonlinear channels aggregation networks for end-to-end global channels ag-gregation representations. Frame sequences from a video are taken as inputs and fed forwardtill the end of the nonlinear channels aggregation layer (NCAL). At the NCAL, the videosequences are encoded by channels aggregation operator to produce the same number ofchannels with the convolutional features, and then is fed to the next layer in the network.
Figure 3: Loss fluctuation curve of iteration in the training process. NACN_5a representsthe standard CNN with NCAL placed behind in the 5a convolutional layer. Likewise,NCAN_5b is constructed by positioning NCAL behind the 5b convolutional layer.
Figure 4: The loss of NCAN with its iterations. The first line represent the convergencecomparisons of NACN with the standard CNN on the three splits of UCF101, and thecomparisons of convergence between NACN and the standard CNN is shown in the secondone.
Figure 5: Given arbitrary angle θ and assuming that the end edge of any angle intersectswith the unit circle at the point p , a directed triangle then is construct. Naturally, sin θand cos θ represent two sides of this triangle,sin θ + cos θ ≥ 1,θ ∈ [0, π]In practice, owing to the perio dicity of sinusoidal and cosine functions, the mapping pro-cess may result in the mistake that the feature in different channels is mapped to the samevalue. This mapping mistake will make CNN confused and lose the ability of recognizing thefeatures with characteristics with periodic relations, which reduces the accuracy of recog-nition. To tackle this problem, we propose the kernel function of NCAL G rescales θ to asingle-valued interval θ ∈(0, ∏), such that each feature in the convolutional is enhancedand aggregated by distinctive transformation items. For the back-propagation, the gradientshave been derived in eq. (12), and fortunately is a symmetric positive definite matrix. It isinteresting to note that the proposed nonlinear channels aggregation layer not only captures9Under review as a conference paper at ICLR 2019the global channels relationship but enlarges the gradient in the back-propagation, reducingthe risk of vanishing gradients.
