Figure 1: Illustration of learning rate (left) and batch size (right) schedules of adaptive batch size as afunction of training epochs based on C2 model on Cifar-10.
Figure 2: I3 model on ImageNet. Training set loss (left), and testing set accuracy (right), evaluatedas a function of epochs.
Figure 3: Logistic regression model result. The left figure shows the training loss as a function ofiterations for full gradient, SGD, ABS and ABSA. The right figure shows the result of ABS/ABSAcompared to full gradient with different learning rate.
Figure 4: I1 model on TinyImageNet. Training set loss (left), and testing set accuracy (right),evaluated as a function of epochs. All results correspond to batch size 16384 (please see Table 2 fordetails). As one can see, from epoch 60 to 80, the test performance drops due to overfitting. However,ABSA achieves the best performance with apparently less overfitting (it has higher training loss).
Figure 5: I2 model on ImageNet. Training set loss (a), and testing set accuracy (b), evaluated as afunction of epochs.
Figure 6: Illustration of block Hessian (left). Instead computing the top eigenvalue of whole Hes-sian, we just compute the eigenvalue of the green block. .Top eigenvalues of Block of C1 (right) onCifar-10. The block Hessian is computed by the last two layers of C1. The maximum batch size ofC1 is 16000. The full Hessian is based on BL with batch size of 128.
