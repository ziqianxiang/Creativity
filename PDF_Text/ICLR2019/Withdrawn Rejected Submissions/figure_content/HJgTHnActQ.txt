Figure 1: Image translation settings. (a) Each domain Xi is defined as the subset of data that sharesa particular attribute. An image from each domain Xi is decomposed into a content space C, aforeground style space Sf, and a background style space Sb. After merging them, LOMIT learns toreconstruct the original image. (b) For the cross-domain translation X1 → X2, LOMIT combines aforeground style extracted from X2 with a content, background style code extracted from X1.
Figure 2: Image translation workflow. (a) First, LOMIT jointly generates masks for the input andthe exemplar images through co-segmentation networks. (b) Next, we separate each image of x1 andx2 into the foreground and the background regions, depending on how much each pixel is involvedin image translation. (c,d) By combining the content and the background style code from x1 withthe foreground style code x2, we obtain a translated image x1→2 . Note that LOMIT also learns theopposite-directional image translation x2→1 by interchanging x1 and x2. Finally, LOMIT learnsimage translation using the cycle consistency loss from X1 → X2 → X1 and X2 → X1 → X2 .
Figure 3: Local mask extraction via co-segmentation networks. The blue arrows indicate the forwardpropagation path in generating the mask m2 of x2, which relies on the global average pooled vectorc1attn of the content activation map c1 . The local masks of two images are jointly computed in aninter-dependent manner so that their style codes are interchangeable.
Figure 4: Comparison with the baselines on CelebA (Liu et al., 2015) dataset. Each column denotesthe input and output image of LOMIT and other baseline models, corresponding with the targetattribute shown in the top. Meanwhile, each row indicates the input and output image followed byeach of masks and the generated image of LOMIT and the baseline models.
Figure 5: The result of action unit translation using EmotioNet dataset (Fabian Benitez-Quiroz et al.,2016). In each part, the first, the third, and the last images are an input image, an exemplar image,and a translated output, respectively. Each number in parenthesis indicates AU.
Figure 6: Comparison with StarGAN. The results demonstrates that LOMIT achieves the multi-modality while maintaining a high-quality output. On the other hand, StarGAN (Choi et al., 2018)is only able to generate an unimodal output.
Figure 7: User study results. Details are described in subsection. 5.4.
Figure 8: t-SNE visualization of the content space C and foreground, background style space Sf ,Sb.
Figure 9: Additional result. The first row is the target image, the second row is the exemplar, andthe Third row is a translated result. Note that the translated attributes are facial hair and gender.
Figure 10: Additional result. The first row is the target image, the second row is the exemplar, andthe Third row is a translated result. Note that the translated attributes are facial hair and gender.
Figure 11: Additional result. The first row is the target image, the second row is the exemplar, andthe Third row is a translated result. Note that the translated attributes are facial hair and gender.
