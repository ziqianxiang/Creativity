Figure 1: (Left) The context-free grammar for the language balanced parentheses (BP). (Center) ADFA that recognizes BP up to depth 4. (Right) A deterministic pushdown automaton (DPDA) thatrecognizes BP for all depths. The symbol • is a wildcard and stands for all possible tokens. TheDPDA extrapolates to all sequences of BP, the DFA recognizes only those up to nesting depth 4.
Figure 2: Two possible instances of an sr-RNN corresponding to equations 2&4 and 2&5.
Figure 3: (Left) State transition probabilities for the SR-GRU learned from the data for the Tomita 1grammar, for temperatures T and input 100. S is the start token. States are listed on x-axis, proba-bilities on y-axis. Up to temperature T = 1 the behavior of the trained SR-GRUs is almost identicalto that of a DFA. Despite the availability of 10 centroids, the trained SR-GRUs use the minimal setof states for T ≤ 1. (Right) The extracted DFA for Tomita grammar 1 and temperature T = 0.5.
Figure 4: DFAs corresponding to the Tomita grammars 2-4. The numbers on the states corresponddirectly to the centroid numbers of the learned sr-GRU.
Figure 5: SR-LSTM-P error curveson the small BP validation data.
Figure 6: Visualization of hidden state ht and cell state ct of the LSTM and the SR-LSTM-P fora specific input sequence from BP. The LSTM memorizes the number of open parentheses bothin the hidden and to a lesser extent in the cell state (bold yellow lines). The memorization is notaccomplished with saturated gate outputs and a drift is observable for both vectors. The sr-LSTM-p maintains two distinct hidden states (accept and reject) and does not visibly memorize countsthrough its hidden states. The cell state is used to cleanly memorize the number of open parentheses(bold red line) with saturated gate outputs (±1). A state vector drift is not observable.
Figure 7: Four prototypes gener-ated from SR-LSTM-P (k = 10)centroids (0,4,5,6) forMNIST.
Figure 8: The DFA extracted from the sr-GRU for Tomita grammar 7. The numbers inside thecircles correspond to the centroid indices of the sr-GRU. Double circles indicate accept states.
Figure 9: The error curves on the validation data for the LSTM, SR-LSTM, and SR-LSTM-P(k = 5)on the large BP dataset (left) and the small BP dataset (right).
Figure 10: The error curves on the test data for the L S TM, SR-LSTM, SR-LSTM-P (k = 10) ontheIMDB sentiment analysis dataset. (Left) It shows state-regularized RNNs show better generalizationability. (Right) A 2-layer SR-LSTM-P achieves better error rates when the classification functiononly looks at the last cell state compared to it only looking at the last hidden state.
Figure 11: Visualization of a SR-LSTM-P with k = 5 centroids, a hidden state dimension of 100,trained on the large BP data. (Left) visualization of the learned centroids. (Center) mean transitionprobabilities when ranked highest to lowest. This shows that the transition probabilities are quitespiky. (Right) transition probabilities for a specific input sequence.
Figure 12: Visualizations of the hidden states h ofa vanillaLSTM, an SR-LSTM, and a SR-LSTM-P (k = 10) for a specific input sequence. All models were trained on BP and have hidden statedimension of 100. The LSTM memorizes with its hidden state. The SR-LSTM and SR-LSTM-Putilize few states and have a more stable behavior over time.
Figure 13: Visualization of hidden state ht and cell state ct of the LSTM and the SR-LSTM-Pfor a specific input sequence from BP. The LSTM memorizes the number of open parentheses bothin the hidden and to a lesser extent in the cell state (bold yellow lines). The memorization is notaccomplished With saturated gate outputs and a drift is observable for both vectors. The SR-LSTM-P maintains two distinct hidden states (accept and reject) and does not visibly memorize countsthrough its hidden states. The cell state is used to cleanly memorize the number of open parentheses(bold red line) with saturated gate outputs (±1). A state vector drift is not observable.
Figure 14:	Visualization of the transition probabilities for an SR-LSTM-P with k = 5 centroidstrained on the IMDB dataset for a negative input sentence.
Figure 15:	Visualization of the transition probabilities for an SR-LSTM-P with k = 5 centroidstrained on the IMDB dataset for a positive input sentence.
Figure 16:Visualization of average transition probabilities of the SR-LSTM-P with k = 10 cen-troids, over all test images. Each row represents a digit class (a concept) and each column depictsthe prototype (average transition probability) for each of the centroids.
Figure 17: We can visualize the working ofan sr-LSTM-p on a specific input image by visualizingthe transition probabilities of each of the centroids (here: k = 10). (Top) The visualizetion forsome MNIST images. (Bottom) The visualization for some Fashion-MNIST images. The firstcolumn depicts the input image and the second to to 11th the state transition probability heatmapscorresponding to the 10 centroids.
