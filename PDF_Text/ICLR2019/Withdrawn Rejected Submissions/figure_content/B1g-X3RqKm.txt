Figure 1: There is no data like more data. More training data improves performance (left) andreduces loss (right). The left panel is borrowed with permission from Banko and Brill (2001). Brillhas suggested firing everyone to fund data collection (personal communication), perhaps in jest,though that comment is not in the published paper.
Figure 2: Learning curve (left) and model size (right) results and trends for ResNet image classifi-cation. Note the transition from the small data regime dominated by best guessing to the power-lawscaling around 32,000 training images.
Figure 3: Learning curve (left) and model size (right) results and trends for word language models.
Figure 4: Violin plots of learning curves for neural machine translation. For each training data shardsize, the violin plot shows the distribution of validation errors for 30 repetitions of the experiment.
Figure 5: Neural machine translation learning curves. Left: the learning curves for separate modelsfollow E(m) = αmβg + γ. Right: composite learning curve of best-fit model at each data set size.
Figure 6: Learning curve (left) and model size (right) results and trends for character languagemodels. Note the difference in absolute accuracy and model size, but same βg and βp between SGDand Adam optimizers.
Figure 7: Learning curves for DS2 and attention speech models (left), and learning curves for var-ious DS2 model sizes, 1.7M to 87M parameters (right). Note the similar βg despite of significantdifference in training loss (CTC vs CE) and model architecture (CNN+RNN vs RNN+Attention).
