Figure 1: A schematic of LIT. In LIT, the teacher model’s blocks are used as input to the student model’sblocks during training, except for the first block. Specifically, denoting the blocks S1,...,S4 for the studentand T1,...,T4 for the student, S2(T1) is compared against T2 in training and similarly for deeper parts ofthe network. S1 and T1 are directly compared. LIT additionally compares S and T through the KD loss.
Figure 3: The accuracy of VDCNN on Amazon reviews (full and polarity) trained from scratch, trainedvia KD, and trained via LIT.
Figure 2: The accuracy of ResNet and ResNeXt trained from scratch, trained via KD, and trained viaLIT for CIFAR10/100. The teacher model was ResNet-110 and ResNeXt-110 respectively. As shown,LIT outperforms KD for every student model. Identical student and teacher architectures correspondsto born again networks, which LIT also outperforms. In some cases, KD can reduce the accuracy of thestudent model, as reported in Mishra & Marr (2017).
Figure 4: ResNeXt student models with cardinality 16 trained from a ResNeXt-110 with cardinality 32on CIFAR10. We show that LIT can reduce the cardinality and that LIT outperforms KD.
Figure 5: Selected images from the teacher (six residual blocks), student (two residual blocks), and trainedfrom scratch (two residual blocks) StarGANs. As shown (column two, four), LIT can appear to improveGAN performance while significantly compressing models. We show a randomly selected set of imagesin the Appendix. Best viewed in color.
Figure 6: The size vs accuracy of various ResNets pruned on CIFAR10. LIT outperforms standardpruning Han et al. (2015b).
Figure 7: The accuracy of student models as α (KD’s interpolation factor for the cross-entropy and logitloss) varies for ResNet and ResNeXt on CIFAR10. The optimal α varies by model type.
Figure 8: The accuracy of student models as β (LIT’s interpolation factor between KD loss and IR loss)varies for ResNet and ResNeXt on CIFAR10. As shown, LIT outperforms training only via KD (β= 1)and only via intermediate representations (β=0). The optimal β appears to be lower (i.e., closer to onlyusing the intermediate representation loss) for more accurate models; we hypothesize that more accuratemodels learn more informative intermediate representations, which helps the students learn better.
Figure 9: Randomly selected images from the teacher (six residual blocks, 18 total layers), student (tworesidual blocks, 10 total layers), and trained from scratch (two residual blocks, 10 total layers) StarGANs.
