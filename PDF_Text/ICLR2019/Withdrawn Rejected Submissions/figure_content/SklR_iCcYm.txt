Figure 1: CIFAR-10 classifier model. This standard CNN-based architecture identifies image classeson the CIFAR-10 dataset, serving as a good benchmark for the dataset optimization study in thispaper.
Figure 2: Autoencoder architecture. Data flows from left-to-right through a CNN-based architecture.
Figure 3: Autoencoder inputs, embeddings, and reproductions (top, middle, and bottom rows,respectively). This figure shows how a basic autoencoder architecture is able to capture and reproducerich information from images. Notably, reproductions have purely instrumental value; what mattersis how well they function as a basis for selecting training examples from the full dataset.
Figure 4: Testing accuracies for varying training step splits and dataset filtering techniques. Themerits of an intelligent sampling scheme become clear on training sessions that train on the reduceddataset for a larger percentage of steps.
Figure 5: Testing accuracies for fixed time budget training. There is a clear advantage to optimizedtraining due to the larger quantity of epochs that can be presented during training.
Figure 6: Loss curves for fixed time budget training. The modelâ€™s loss decreases more rapidly, relativeto the elapsed training time, with the reduced dataset than the full dataset.
