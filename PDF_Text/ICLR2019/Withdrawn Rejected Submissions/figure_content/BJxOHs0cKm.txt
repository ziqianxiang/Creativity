Figure 1: Loss Landscape and Predicted Labels of a 5-layer MLP with 2 parameters. The sharpminimum, even though it approximates the true label better, has some complex structures in itspredicted labels, while the flat minimum seems to produce a simpler classification boundary.2(d) Predicted labels by the flat minimumway to incorporate the local property of the solution into the generalization analysis. In particular,Neyshabur et al. (2017) suggests to use the difference between the perturbed loss and the empiricalloss as the sharpness metric. Dziugaite & Roy (2017) tries to optimize the PAC-Bayes boundinstead for a better model generalization. Still some fundamental questions remain unanswered. Inparticular we are interested in the following question:How is model generalization related to local “smoothness” of a solution?In this paper we try to answer the question from the PAC-Bayes perspective. Under mild assumptionson the Hessian of the loss function, we prove the generalization error of the model is related to thisHessian, the Lipschitz constant of the Hessian, the scales of the parameters, as well as the numberof training samples. The analysis also gives rise to a new metric for generalization. Based on this,we can approximately select an optimal perturbation level to aid generalization which interestinglyturns out to be related to Hessian as well. Inspired by this observation, we propose a perturbationbased algorithm that makes use of the estimation of the Hessian to improve model generalization.
Figure 3: Generalization gap and Ψκ as a function of epochs on CIFAR-10 for different batch sizes.
Figure 2: Generalization gap and Ψκ as a function of epochs on MNIST for different batch sizes.
Figure 6: Training and testing accuracy as a function of epochs on CIFAR-10, CIFAR-100 and TinyImageNet. For CIFAR, Adam is used as the optimizer, and the learning rate is set as 10-4. Forthe Tiny ImageNet, SGD is used as the optimizer, and the learning rate is set as 10-2 . The dropoutmethod in the comparison uses 0.1 as the dropout rate. Details can be found in Appendix G.
Figure 7: Training and validation accuracy of PertOPT and Dropout on CIFAR-10. Adam-train, andadam-val use the wide resnet model with 0 dropout rate. Perturbed-val and perturbed-train use thesame wide resnet with 0 dropout rate, but added perturbation according to algorithm 1.
Figure 8: Training and validation accuracy of PertOPT and Dropout on CIFAR-100. Adam-train,and adam-val use the wide resnet model with 0 dropout rate. Perturbed-val and perturbed-train usethe same wide resnet with 0 dropout rate, but added perturbation according to algorithm 1.
Figure 9: Training and validation accuracy of PertOPT and Dropout on Tiny ImageNet. Original-train, and original-test use the wide resnet model with 0 dropout rate. Perturbed-test and perturbed-train use the same wide resnet with 0 dropout rate, but added perturbation according to algorithm1.
