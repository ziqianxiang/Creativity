Figure 1: Left: The standard federated learning framework. The gradients transmitted from eachdevice to the central server is non-sparse, and the shared model learned from the distributed privatedata is non-sparse. Right: The proposed efficient federated learning framework based on unifiedgradient and model sparsification. The gradients transmitted from each device to the central server issparse, and the shared model learned from the distributed private data is sparse.
Figure 2: Communication cost curves during training on five datasets. The horizontal axis representsthe epoch number during training. The vertical axis represents the percentage of gradients transmitted.
Figure 3: The top-1 accuracy of 4, 8 and 16 devices for non-sparse distributed learning and sparsefederated learning.
Figure 4: (a) The percentage of non-zero weights of the model after training for 4, 8, and 16 devices;(b)	the percentage of communication cost for 4, 8 and 16 devices.
Figure 5: The percentage of non-zero weights of 4, 8, and 16 devices for sparse federated learningwith pretrained weights, random weights and pretrained weights with shared α.
Figure 6: The total communication percentage of 4, 8, and 16 devices for sparse federated learningwith pretrained weights, random weights and pretrained weights with shared α.
