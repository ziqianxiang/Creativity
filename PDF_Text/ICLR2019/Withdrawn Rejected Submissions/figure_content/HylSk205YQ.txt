Figure 1: Overview of MADDPG-M. In (a), the N agents learn two hierarchically arranged setsof policies, which are connected through a communication medium. During training, we runcommunication policies at a slower time-scale, i.e. once in every C steps of the action policies,and determine the environmental actions using fixed communication medium over the C steps. In(b), the communication policies are learned within a CTDE paradigm using the cumulative sumof the rewards collected from the environment for these C steps, while we learn action policies indecentralised way using intrinsic rewards estimated with respect to the communication medium.
Figure 2: Comparison between MADDPG-M and 4 baselines for all 6 scenarios. Each bar clustershows the 0-1 normalised mean episode rewards when trained using the corresponding approach,where a higher score is better for the agent. Full results are given in the Appendix.
Figure 3: a) On Alternating - Broadcasting, the reward of MADDPG-M against baseline approachesafter 100,000 episodes. b) On Dynamic - Broadcasting, the rewards of MADDPG-M against DDPG-OC, the accuracy curve of the communication actions to observe the individual performance of ν,and the collected intrinsic rewards to observe the individual performance of μ.
