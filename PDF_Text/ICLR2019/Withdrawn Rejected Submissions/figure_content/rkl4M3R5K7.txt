Figure 1: Illustration of why randomization is necessary to compute optimal adversarial attacks. In this ex-ample using binary linear classifiers, there is a single point that is initially classified correctly by two classifierscι,C2, and a fixed noise budget α in the I2 norm. A naive adversary Who chooses a noise perturbation deter-ministically will always fail to trick the learner since she can always select the remaining classifier. An optimaladversarial attack in this scenario consists of randomizing With equal probability amongst both noise vectors.
Figure 2: Visual comparison of misclassification using state-of-the-art adversarial attacks. We compare thelevel of noise necessary to induce similar levels of misclassification by attacking an ensemble classifier usingthe (from left to right) Fast Gradient Method (FGM), the Madry attack, and the Momentum Iterative Method(MIM) versus applying NSFW (rightmost column) on the same set of classifiers. To induce a maximum of 17%accuracy across all models, we only need to set α to be 300 for NSFW. For the MIM attack on the ensemblewe need to set α = 2000. For FGM and the Madry attack, the noise budget must be further increased to 8000.
Figure 3: Class saliency map with respect to the image displayed in the top row of Figure 2 for each ImageNetclassifier and their ensemble. From left to right: InceptionV3, Xception, ResNet50, DenseNet121, VGG16,and the ensemble classifier of all 5 models.
Figure 4: Results of running NSFW on linear models. On the left, we demonstrate the results of runningNSFW on linear multiclass models using different noise functions and varying the noise budget α. NSFW-Oracle corresponds to running Algorithm 1 using the best response oracle described in Lemma 2. Similarly,NSFW-Untargeted shows the results of running NSFW and applying PGD to a weighted sum of untargetedlosses as in Equation (3). The label iteration method is described below. Lastly, the ensemble attack correspondsto the optimal noise on an equal weights ensemble of models in C. On the right, we illustrate the convergenceof NSFW on linear binary classifiers with maximally different decision boundaries to compare against theconvergence rate observed for neural nets in Figure 5 and better understand when weight adaptivity is necessary.
Figure 5: Fast convergence of NSFW on MNIST and ImageNet deep learning models. NSFW-Untargetedcorresponds to running NSFW and applying PGD on a sum of untargeted reverese hinge losses as describedin Section 3 .1. The dotted lines correspond to running the indicated attack on the ensemble of models in C.
Figure 6: Results of running NSFW for linear binary classifiers and varying the noise budget. As seen in thecase of multiclass classifiers in Figure 4 NSFW equipped with the best response outperforms other approachesat generating noise for linear models. Furthermore, we see there is a performance gap between gradient basedapproaches and the theoretically optimal one that leverages convex programming.
