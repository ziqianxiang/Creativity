Figure 1: Salience map for the first layer (768 inputs, 300 outputs) of the LeNet-300-100model for 3 different criteria with L2 regularization at λ = 0.0001 and λ = 0.5. Note thatless salient areas correspond to edges of the input image, which are empty in MNIST.
Figure 2: Accuracy versus fraction of parameters pruned of the LeNet-300-100 modelfor both Wald (Stat) and magnitude pruning, in both one-shot and iterative modes. Waldpushes out the possibility frontier by circa 0.8%, that is roughly 2130 parameters.
Figure 3: Accuracy versus fraction of parameters pruned of the LeNet5 model for bothWald (Stat) and magnitude pruning, in both one-shot and iterative modes. Wald wasderived for individual/small groups of weights, using it en-mass is detrimental.
Figure 4: Train loss evolution during re-training of the LeNet5 model after pruning 90%of the weights. When used properly, Wald gains performance advantage at the beginningof the re-training process which it retains throughout.
