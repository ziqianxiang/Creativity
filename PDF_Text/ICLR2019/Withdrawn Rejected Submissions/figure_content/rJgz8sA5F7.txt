Figure 1: The overall architecture of our method. Blue arrows depicts the training process for thenew task (Task C) of our method, where Task A and Task B have been already trained. H-Net istrained each time a dataset for a new task is provided and then the C-Net is trained in a standardway such as using the cross-entropy loss. We allow only the trainable filters (Blue parameters) to beupdated. Red arrows shows the inference procedure of our method. H-Net finds out the task index J(Task C) from which the input came and let the C-Net to use the filters specialized for task J (Blue+ Orange + Grey). Class labels are also switched to those of task J .
Figure 2: Trainable parameters WkT (Orange) and frozen parameters WkP (Grey) in the C-Net train-ing process. In the training process, the parameters (Orange), which are trainable parameters exceptfrozen parameters (Grey), are updated. x denotes the spatial dimension of filters.
Figure 3: Error rate comparison between shar-ing the parameters which learned previous taskand random parameters.
Figure 4: MNIST datasetAppendix C	Related workDual memory network system has been suggested to overcome the low performance of the regular-ization methods and the inefficiency of the dynamic methods. Most of these researches try to mimicthe brain system of mammals whose recognition process is believed to be done by the complemen-tary contribution of the hippocampus and the cortex. In several researches, the hippocampus hasbeen realized by networks replaying the previous dataset or determining whether the input is fromthe old dataset (Gepperth & Karaoguz, 2016; Kemker & Kanan, 2017). Shin et al. (2017) proposed a12Under review as a conference paper at ICLR 2019NetFigure 5: SVHN datasetFigure 6: CIFAR-10 datasetmethod of replaying previously encoded experiences using GAN and training the new task withoutthe presence of the old dataset.
Figure 5: SVHN datasetFigure 6: CIFAR-10 datasetmethod of replaying previously encoded experiences using GAN and training the new task withoutthe presence of the old dataset.
Figure 6: CIFAR-10 datasetmethod of replaying previously encoded experiences using GAN and training the new task withoutthe presence of the old dataset.
