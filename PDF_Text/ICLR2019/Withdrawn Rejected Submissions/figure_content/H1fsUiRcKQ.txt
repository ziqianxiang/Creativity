Figure 1: Demonstration of how the bad samples generated by the adversarial training are distributed.
Figure 2: Examples of P(y = 1|x) of smooth (Left) and wiggle (Right) cases. We plot 3 points andtheir adversarial directions on each case.
Figure 3: (Upper) 10 randomly sampled original MNIST dataset. (Middle and Lower) Bad samplesobtained by the classifier learned with and without the regularization term of VAT.
Figure 4: The number of epochs to achieve the prespecified test accuracies (98%, 90% and 80%) withthe four methods for (Left) MNIST, (Middle) SVHN and (Right) CIFAR10 datasets. Bad GAN isoperated without PixelCNN++ for SVHN and CIFAR10 datasets.
Figure 5: (Upper Left) The scatter plot of synthetic data which consist of 1000 unlabeled data (gray)and 4 labeled data for each class (red and blue with black edge). (Upper Right) Accuracies ofunlabeled data for each epochs for VAT and FAT. We use 2-layered NN with 100 hidden units each.
Figure 6: 100 randomly sampled bad images using (Left) FAT and (Right) Bad GAN.
Figure 7: Trace plot of the test accuracies with the four methods for MNIST dataset. 20 randomlysampled data are used as the labeled data and the rest are used as the unlabeled data.
