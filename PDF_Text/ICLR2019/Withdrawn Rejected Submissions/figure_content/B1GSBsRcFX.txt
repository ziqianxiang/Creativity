Figure 1: GRSVNet architecture and the results of different networks with the same VGG-11 baselineachitecture on the SVHN dataset with real data and real labels. (a) GRSVNet architecture (betterunderstood in its special case OLE-GRSVNet detailed in Section 3). (b)-(e) Features of the test datalearned by different networks visualized in 3D using PCA. Note that for OLE-GRSVNet, only fourclasses (out of ten) have nonzero 3D embedding (Theorem 2). (f) Training/testing accuracy.
Figure 2: Training and testing accuracy of different networks on the SVHN dataset with random labelsor random data (Gaussian noise). Note that softmax, sotmax+wd, and softmax+OLE can all perfectly(over)fit the random training data or training data with random labels. However, OLE-GRSVNetrefuses to fit the training data when there is no intrinsically learnable patterns.
Figure 3: Visual illustration of the second toy experiment in Section 4. (a) Three classes of one-dimensional data in R10 . (b) Labels randomly shuffled. (c)-(i) Features extracted by baseline MLPwith softmax classifier or OLE-GRSVNet. Only three layers of MLP are needed for conventionalDNN to perfectly memorize random labels. But even with 100 layers of MLP, OLE-GRSVNet stillrefuses to memorize the random labels because there are no intrinsically learnable patterns.
