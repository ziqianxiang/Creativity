Figure 1: (a) The standard hierarchical Bayesian model for multi-task learning. A set of global parameters θprovides an inductive bias for the estimation of task-specific parameters φj . (b) In a mixture of hierarchicalBayesian models, the cluster assigment of each task-specific parameter set φj is represented with a latentCategorical variable zj . (c) Allowing an unbounded number of mixture components instantiates a non-parametricmodel that has the potential to grow with the data in complexity.
Figure 5: Mixture component validation log-likelihoods (MSE losses) on an evolving dataset that generateseven polynomial regression tasks for 700 iterations, then odd polynomial regression tasks until iteration 1400 atwhich point it generates sinusoidal regression tasks. We plot the validation negative log likelihood of the datafor each task. Note, for example, the change in loss to the second (red) cluster at 700 iterations when the oddpolynomial tasks are introduced.
Figure 6: (Top) An evolving dataset of quadratic, sinusoidal, and logistic regression tasks. The dataset generatesquadratic regression tasks for 700 iterations, then switches to generating sinusoidal regression tasks until iteration5700 at which point it generates logistic regression tasks. (Below) Note the change in responsibilities to thesecond (red) cluster at 700 iterations when the sinusoidal tasks are introduced. At 5700 iterations, the logisticregression tasks (third row) also cause a third (brown) cluster to be introduced.
Figure 7: An evolving dataset of miniImageNet few-shot classification tasks where for the first 20k iterationswe train on the standard dataset, then switch to a ”pencil” effect set of tasks for 10k iterations before finallyswitching to a “blurred” effect set of tasks until 40k. Responsibilities γ(') for each cluster are plotted over time.
