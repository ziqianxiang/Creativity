Figure 1: From left to right, network architecture, 2D PCA of artificial data, activation heatmap ofa standard autoencoder without regularization, and an activation heatmap of an autoencoder withLaplacian smoothing regularization. We use a graph of six nodes with three pairs of connectednodes. The activation of each pair of nodes corresponds to one of the three clusters, where thedifferences in activation between the two nodes corresponds to which of the two subclusters thepoint belongs to. In the model with Laplacian smoothing we are able to clearly decipher the structureof the data. Whereas with the standard autoencoder the overall structure of the data is not clear, forexample, the fact that the the blue and orange points belong different subclusters.
Figure 2: The prediction layer of a capsule network on MNIST uses 10 digit capsules each with16 dimensions. To perform Laplacian smoothing between capsules we create 16 fully connectedgraphs of size 10. Next, we show dimension perturbations for each digit. Each columns showsthe reconstruction when one of the 16 dimensions in the DigitCaps representation is tweaked by0.05 in the interval [-0.25, 0.25]. On the left we see a single dimension in a standard capsule netacross all digits, and on the right we see a dimension on a capsule net trained with Laplace smoothingregularization. With Laplacian smoothing regularization between capsules (right) a single dimensionrepresents line thickness for all digits. Without this regularization each digit responds differently toperturbation of the same dimension.
Figure 3: From left to right, Architecture diagram of the spectral bottleneck layer, PCA plot ofthe data, embedding layer activation heatmaps ordered by location on the generated line with nosmoothing, graph smoothing, and spectral bottleneck modifications. It is difficult to identify thelinear structure of the data in a standard autoencoder. With Laplacian smoothing in a line graph westart to see a diagonal structure emerge. Finally, adding the spectral bottleneck layer we are able toclearly see the one-dimensional structure of the data even in an embedding space with much higherdimensionality than usually possible.
Figure 4: From left to right, shows a 2D embedding (PHATE (Moon et al., 2017)) of the T celldataset, heatmaps of embedding layer activations sorted by branch and trajectory labels with andwithout graph spectral regularization, and finally a correlation matrix between the 20 genes and the20 nodes of the embedding. The data consists of two T Cell developmental trajectories, from the redbranch to the blue branch and from the red branch to the green branch. The gene correlation plotdepicts blue as negative correlation, white as no correlation, and red as positive correlation. As wecan see graph spectral regularization creates an interpretable and biologically relevant embedding,splitting into CD4+ (green) and CD8+ branches (blue).
Figure 5: Shows average activation by digit over a 64 (8x8) 2D grid using Laplacian smoothing andconvolutions following the regularization layer. Next, we segment the embedding space by class tolocalize portions of the embedding associated with each class. Notice that the digit 4 here serves asthe null case and does not show up in the segmentation. Finally, we show the top 10% activation onthe embedding of some sample images. For two digits (9 and 3) we show a normal input, a correctlyclassified but transitional input, and a misclassified input. By inspection of the embedding space wecan see the highlighted regions of the embedding space correlate with the semantic description ofthe digit type.
Figure 6: From left to right, 2D Multidimensional scaling (MDS) plot on the dataset, graph smooth-ing regularization structure, and heatmaps showing activation strength of datapoints on the y-axisordered by rotation and nodes on the x-axis. We can see with too much smoothing (α = 0.1) we geta less meaningful heatmap. But more reasonable values of α result in activations of lower periodboth vertically and horizontally.
