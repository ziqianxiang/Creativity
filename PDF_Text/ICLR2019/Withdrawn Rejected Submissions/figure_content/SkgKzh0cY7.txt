Figure 2: Results of GTA video colorization show that per-frame translation of videos does notpreserve constant colours of objects within the whole sequence. We provide more results and videosin the supplementary video: https://bit.ly/2R5aGgo. Best viewed in color.
Figure 3: Our model consists of two generator networks (F and G) that learn to translate inputvolumetric images from one domain to another, and two discriminator networks (DA and DB) thataim to distinguish between real and fake inputs. Additional cycle consistency property requires thatthe result of translation to the other domain and back is equal to the input video, G(F (x)) â‰ˆ x.
Figure 4: We compare three ways of forming batches used during training of a CycleGAN : (a)random frames from multiple videos, (b) sequential frames from a single video or (c) single 3Dtensor consisting of consecutive frames from a single video. Contrary to the conventional wisdom,our experiments suggest that additional randomness in the batch structure induced in case (a) hurtsthe performance and convergence of the resulting translation model.
Figure 5: Results of unsupervised GTA video-to-segmentation translation with different models. We observedthat frame-level methods diverged significantly more frequently than 3D model (top). No information aboutground truth pairs was used during training. Frame-wise translation (2D) produces plausible images, but doesnot preserve temporal consistency. Forming batches from consecutive frames during training (2D sequence)helps in reducing spatio-temporal artifacts and improves convergence. Additional penalty term on consecutivegenerated frames (const loss) further reduces motion artifacts at the expense of diversity of generated images.
Figure 6: The results of experiments on Volumetric MNSIT dataset: input and output domains contain videoswith global decay-then-rise and rise-then-decay intensity patterns respectively, during models were presentedwith pairs of videos containing different digits. Our experiments show that frame-level approaches are not able tolearn this spatio-temporal pattern and hence cannot perform correct translation whereas our 3D method performsalmost perfectly. Both sequence and sequence+const approaches were able to capture temporal pattern but didnot learn shape correspondence.
Figure 7: Volumetric MNIST results.
Figure 8: Results of MRI-to-CT translation.
Figure 9: Colored 3D MNIST translation results.
