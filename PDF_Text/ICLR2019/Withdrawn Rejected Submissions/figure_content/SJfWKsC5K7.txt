Figure 1: Explainer. We distill knowledge of a performer into an explainer as a paraphrase of theperformer’s representations. The explainer decomposes the prediction score into value componentsof semantic concepts, thereby obtaining quantitative semantic explanations for the performer.
Figure 2: Two typical types of neural networks. (left) A performer models interpretable visualconcepts in its intermediate layers. For example, each filter in a certain conv-layer represents aspecific visual concept. (right) The performer and visual concepts are jointly learned, and they sharefeatures in intermediate layers.
Figure 3: Quantitative explanations for the object classification (top) and the face-attribution pre-diction (bottom) made by performers. For performers oriented to object classification, we annotatedthe part that was represented by each interpretable filter in the performer, and we assigned contri-butions of filters αiyi to object parts (see the appendix). Thus, this figure illustrates contributionsof different object parts. All object parts made positive contributions to the classification score.
Figure 5: Quantitative explanations for the attractive attribute. Bars indicate elementary contribu-tions αi yi from features of different face attributes, rather than the prediction of these attributes. Forexample, the network predicts a negative goatee attribute ygoatee < 0, and this information makes apositive contribution to the target attractive attribute, αi yi > 0.
Figure 9: Quantitative explanations for object classification. We assigned contributions of filtersto their corresponding object parts, so that we obtained contributions of different object parts. Ac-cording to top figures, we found that different images had similar explanations, i.e. the CNN usedsimilar object parts to classify objects. Therefore, we showed the grad-CAM visualization of featuremaps (Selvaraju et al., 2017) on the bottom, which proved this finding.
Figure 10: We visualized interpretable filters in the top conv-layer of a CNN, which were learnedbased on (Zhang et al., 2018b). We projected activation regions on the feature map of the filter ontothe image plane for visualization. Each filter represented a specific object part through differentimages.
