Figure 1: Comparison of the cross-entropy loss obtained with Hypergrad with the parameters sug-gested by Baydin et al. (2018) (solid line) and the optimized ones (dashed line) for a feedforwardneural network on MNIST (top) and VGG16 on CIFAR-10 (bottom), both on the training (left) andthe validation (right) sets.
Figure 2: Train error of lr-decay and hypergradient for VGG16 on CIFAR-10 (left), Resnet18 onCIFAR-10 (left) and Feedforward neural network on MNIST (left). Shaded areas represent the 95%confidance interval around the mean performance over the 5 runs.
Figure 3: Test error of lr-decay and hypergradient for VGG16 on CIFAR-10 (left), Resnet18 onCIFAR-10 (left) and Feedforward neural network on MNIST (left). Shaded areas represent the 95%confidance interval around the mean performance over the 5 runs.
Figure 5: Sensitivity of gradient methods (constant step size) for ResNet18 to the learning rate, onCIFAR-10. The residual net offers a wider region around the best stepsize. The tuning them wouldrequire little effort.
Figure 4: Sensitivity of gradient methods (constant stepsize) for VGG net to the learning rate, onCIFAR-10. On this architecture, these algorithms show a tight region around the best stepsize.
Figure 6: The effect of lr-decay on the learning rate sensitivity for VGG net and ResNet18 onCIFAR-10. lr-decay makes the tuning of ADAM easier. ResNet18 makes the gradient methods lesssensitive to the stepsize choice.
Figure 7: The effect of hypergradient on the learning rate sensitivity for VGG net and ResNet18on CIFAR-10. For a given β parameter, Hypergradient demonstrates some robustness to the initiallearning rate. However to reach higher performance, tuning β is crucial.
