Figure 1: Qualitative performance of ILFB for GANs: We apply ILFB to a dataset of 80% MNIST“good” images + 20% Fashion-MNIST “bad”. The panels show the fake images from 32 randomlychosen (and then fixed) latent vectors, as ILFB iterations update the GAN weights. Baseline is thestandard training of fitting to all samples. We can see that the baseline generates both digit andfashion images, but by the 5th iteration it hones in on digit images.
Figure 2: Qualitative performance of ILFB for GANs: Given training data consisting of 70%CelebA “good” images + 30% CIFAR-10 “bad” images, the four panels above each show the per-formance after iterations of our ILFB algorithm. First we choose 18 random vectors in latent pace,and fix them for all iterations. In each iteration, we retrain the GAN on corresponding selected sam-ples to get new weighs, which are then used to generate the 18 fake images. Baseline refers to thestandard training where we fit to all the samples (this is also our initialization). Visually, we can seethat the generator is able to improve its generation quality and only generate face-like images afterthe 5th iteration.
Figure 3: Illustrative failure case: This figure shows that when the fraction of “bad” samples is toolarge, ILFB cannot clean them out. The setting is exactly the same as in Figure 1, but now with 60%MNIST “good” images + 40% Fashion-MNIST “bad” images. We can see that now the 5th iterationstill retains the fake fashion images. Please see the discussion section for some intuition on this.
Figure 4: Effect of mis-specification for linearregression modelFigure 5: Effect of mis-specification for mixedlinear regressionWe further show the performance of ILFB as sample size increases. In Fig. 6, the asymptoticperformance under different noise levels is shown. For the clean dataset, ILFB is observed to giveconsistent estimate. We observe similar performance in the Gaussian setting as well.
Figure 5: Effect of mis-specification for mixedlinear regressionWe further show the performance of ILFB as sample size increases. In Fig. 6, the asymptoticperformance under different noise levels is shown. For the clean dataset, ILFB is observed to giveconsistent estimate. We observe similar performance in the Gaussian setting as well.
Figure 6: Consistency of ILFB linear regression under different noise levels (σ = 0.1, 1, 10 fromleft to right).
Figure 7: Left: `2 loss of recovered parameter to the true parameter for (a) MLE: naive MLE/OLSestimator; (b) Oracle: MLE estimator for the subset of good samples; (c) Super-Oracle: MLEestimator for the whole set of samples given the correct output for bad samples; (d) R-Init+ILFB:our algorithm with random initialization; (e)M-Init+ILFB: our algorithm with MLE as initialization.
Figure 8: Performance of ILFB for mixed linear regression under different noise levels (σ =0.1, 1, 10 from left to right). For small noise (left plot), the performance of ILFB decreases alongwith the oracle performance. When the noise gets larger (middle plot), we observe a gap betweenILFB and the oracle when sample size n increases. When the noise gets dominate the signal (rightplot), the performance of ILFB and the oracle are not far apart for n ≤ 51200, but the gap will getlarger asymptotically. Notice that for all experiments, the standard deviation of the true signal is 1.
Figure 9: Left: Performance of different algorithms for Gaussian mixture model, measured by the`2 loss of the recovered parameter to the true parameter, σ = 0.15. Right: Convergence speed ofILFB under different noise levels, using both MLE initialization and random initialization.
Figure 10: Performance of ILFB for Gaussian mixture model under different noise levels (σ =0.1, 1, 10 from left to right). For small noise (left plot), the performance of ILFB decreases alongwith the oracle performance. When the noise gets larger (middle plot), we observe a gap betweenILFB and the oracle when sample size n increases. When the noise gets dominate the signal (rightplot), the performance of ILFB and the oracle are not far apart for n ≤ 51200, but the gap will getlarger asymptotically.
