Figure 1: Contour plots of training losses for various problem domains on a log scale. Lighter colors indicatelower loss values. Since we train each batch size for a fixed number of epochs, the total number of trainingiterations scales down linearly. For each loss value, we can observe how many iterations it takes to convergeto that value given a particular batch size, by tracing the level curve for the associated color. For all problems,there is a batch size after which the number of training iterations necessary to converge does not decrease.
Figure 2: On the left: speedup curves when applying several popular techniques to avoid the generalizationgap. Base LR uses a single learning rate for all batch sizes. On the right: the effect of the linear approximationerror on final test accuracy when using the linear LR scaling rule.
Figure 3:	Speedup curves across different problem configurations. Left: different architectures result in differ-ent rates of convergence on CIFAR-10. Right: ResNet34 exhibits different rates of convergence on CIFAR-10,CIFAR-100, and SVHN. Loss thresholds are obtained by computing the lower quartile of loss values achievedby the largest batch size.
Figure 4:	Speedup curves as dataset size varies for different datasets. Even as dataset size increasesback up to the baseline of 100%, there is no noticeable improvement in convergence speed.
Figure 5: Contour plots of test losses for various problem domains on a log scale. The test losses for BLRare on the left, while the losses for the LSR strategy are on the right. Lighter colors indicate lower loss values.
