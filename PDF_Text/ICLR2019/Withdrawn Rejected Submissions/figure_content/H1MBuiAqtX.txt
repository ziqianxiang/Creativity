Figure 1: Top: Continual learning domain. the agent needs to find and pick up objects in the spe-cific order key, lock, door, chest (diagram, top left), while acting in the rich 3D environmentof DM Lab (screenshot, top right). Any deviation from that order resets the sequence (dashed graylines). Bottom: Results. The objective is to pick up as many chests as possible (purple). The bot-tom left plot shows performance relative to final performance of the best baseline (glutton, dashedline) as a function of training. Our method quickly learns to become competent on all 4 subtasks,one at a time. The margin of improvement over the baseline is largest on the hardest subtasks (seesection 4.5 for details). The stacked bar plot (bottom right) shows the average number of collectedobjects after training (1.5B frames). The unicorn can be conditioned on 4 goals, each conditioningshown as a separate stack. E.g. the Unicorn’s door policy typically collects 15 keys, 12 locks,10 doors and 1 chest. Note that the strict ordering imposed by the domain requires that thereare always at least as many keys as locks, etc. Black bars indicate the standard deviation across 5independent runs.
Figure 2: Our UVFA architecture. Observations are processed via a CNN and an LSTM block toproduce a goal-independent representation f (s), which is concatenated with the goal signal g, andthen processed by an MLP into Q(s, a; g).
Figure 3: Left: Inventory stack and pseudo-rewards. Each row illustrates an event where theagent touches an object. The inventory stack (left) tracks the sequence of recently collected objects,the pseudo-reward vector (right) is used to train the agent, and is 4-dimensional in this case becausethere are visible 4 tasks. See section 4.5, where pseudo-rewards require the stack to contain thedependent objects in the correct order (key, lock, door, chest) — any extra object resets thesequence (like the door in row 3). Right: Agent architecture. Rollouts of experience are storedin a global queue. Gradients w.r.t. the loss are propagated through the UVFA. After every trainingstep, all M actors are synchronized with the latest global UVFA parameters.
Figure 4:	Left: Multi-task learning. A single Unicorn agent learns to collect any out of 16 objecttypes in the environment. Each thin red line corresponds to the on-policy performance for one suchtask; the thick line is their average. We observe that performance across all tasks increases together,and much faster, than when learning separately about each task (black). See text for the baselinedescriptions. Right: Off-policy transfer (I): The average performance across the 12 training tasksis shown in green. Without additional training, the same agent is evaluated on the 4 cyan tasks(thin red learning curves): it has learned to generalize well from fully off-policy experience.
Figure 5:	Training and hold-out tasks for each of the three transfer experiments. Each square rep-resents one task that requires collecting objects of a specific shape and color. Hold-out tasks aresurrounded by orange boxes. The augmented training set (III) includes the abstract tasks that rewardjust color or just shape.
Figure 6: Combinatorial zero-shot transfer. Left: during training (green), the Unicorn agent seesonly a subset of 12 tasks, and experience from those 12 behavior policies (II). The transfer evaluationis done by executing the policies for the 4 hold-out tasks (without additional training). After an initialfast uptick, transfer performance drops again, presumably because the agent specializes to only the12 objects that matter in training. Note that it is far above random because even when conditionedon hold-out goal signals, the agent moves around efficiently and picks up transfer objects alongthe way. Right: zero-shot transfer performance is significantly improved when the training tasksinclude the 8 abstract tasks (III), for a total of K0 = 20.
Figure 7: Multi-task learning curves, performance on a subset of 4 out of the 16 tasks, as indicatedby the subplot titles. There are two lines for each color, which are from two separate seeds.
Figure 8: Multi-task learning with 7 objects. Performance is qualitatively very similar to Figure 4with 16 objects, except that learning is overall faster.
Figure 9: Off-policy transfer (I) learning curves. Only the last subplot guitar cyan is a transfertask. See caption of Figure 7.
Figure 10: Zero-shot transfer (II) learning curves on all 4 transfer tasks. See caption of Figure 7.
Figure 11: Augmented zero-shot transfer (III) learning curves on all 4 transfer tasks. See caption ofFigure 7.
Figure 12: Continual learning with depth 3 task dependency. Early learning (left) and late learning(right). The performance gap compared to the expert is much larger in the beginning. See captionof Figure 1.
Figure 13: Continual learning with depth 5 task dependency. This is a partially negative resultbecause the Unicorn learns to eat a lot of cake, but only when following the chest policy. Seecaption of Figure 1.
Figure 14: Continual learning with depth 4 task dependency. Performance curves as a function oflearning samples. Each subplot shows the performance on one of the tasks (e.g. only counting chestrewards on the right-hand side). Thin lines are individual runs (5 seeds), thick line show the meanacross seeds. In all 4 subplots the expert (black) is a single-minded baseline agent maximizing the(very sparse) chest reward only.
Figure 15: Continual learning with depth 3 task dependency. Performance curves as a function oflearning samples. See caption of Figure 14.
Figure 16: Continual learning with depth 5 task dependency. Performance curves as a function oflearning samples. See caption of Figure 14.
