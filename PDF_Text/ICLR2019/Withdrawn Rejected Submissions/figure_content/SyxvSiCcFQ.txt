Figure 1: The structure of the BNN we are training. It has a total of eight binary weights.
Figure 2: A quantum circuit (right) corresponding to the neuron (left). Quantum circuits are read from left toright just like classical computing circuits. The first group of operations on the weights are initialising theminto a superposition of all their values using Hadamard gates (red). The first group of operations on the inputsare encoding the features onto the qubits (blue). They are either blank (if the input values are already -1) orNOT gates if the desired input values are 1. The following three quantum operations are the anti-CNOT gatesthat weight each input according to its corresponding weight (green). The final set of operations (purple) area combination of multi-qubit, controlled and anti-controlled NOT gates that together correspond to the signfunction. They can be seen to flip the state of the ancilla qubit to +1 if either all the weighted input values arein the state +1, or if any two of them are in the state +1. Ancilla qubits are additional ’helper’ qubits.
Figure 3: The change in the probability distribution of output weights under quantum amplitude amplificationafter k iterations where (a) k = 0, i.e random , (b) k is sub-optimal (c) k is near optimal. The black data pointrepresents the optimal configuration.
Figure 4: A plot showing the relationship between the probability of obtaining an optimal set of weightsagainst the number of quantum amplifications, k, for problem 1 (a) and problem 2 (b). Each point represents asimulation of 50 separate runs of the algorithm at the given k and the probability of success of those 50 runs.
Figure 5: A plot comparing the scaling of a quantum search algorithm over a classical one. The quantumdata is the cumulative probability of success over 100 runs of the algorithm. Classical results are analyticallyderived from the known probability of obtaining a solution by random search. The superior scaling of thequantum algorithm becomes more prominent for harder problems.
Figure 6: (a) The meta-BNN we train. The dotted lines represent the presence or absence of connectionswithin the first layer. A meta-learned solution to (b) problem 1 and (c) problem 2a particular set of weights that represents an overfit, run the algorithm again but with a deselectionof that particular set of weights. This can be done by simply changing the sign of the probabilityamplitude corresponding to that state during each iteration of the quantum amplitude amplification.
