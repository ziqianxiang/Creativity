Figure 1: DeepCS encourages agents to visit new places in Montezuma’s Revenge. White sec-tions in the curiosity grid (middle) show which locations have been visited; the unvisited blacksections yield an exploration bonus when touched. The network receives both game input (left) andcuriosity grid (middle) and must learn how to form a map of where the agent has been (hypotheticalillustration, right). The grid is reset when the agent loses all lives and starts a new game, encouragingintra-life exploration irrespective of previous games.
Figure 2: DeepCS improves performance on sparse- and dense-reward Atari games. In Mon-tezuma's Revenge (a challenging SParse-reward game), DeePCS vastly outperforms the naive ex-ploration of A2C alone and matches the average performance of state-of-the-art methods (Table 1,DeePCS vs. PC and PCn). On Seaquest (an easier, dense-reward game), DeePCS nearly doubles themedian Performance of A2C (3443 vs. 1791 Points) and obtains nearly 80,000 Points in one run. Therightmost Plots show intrinsic rewards, quantifying how much of the game world has been exPlored;horizontal bars below each Plot indicate statistical significance. For additional games, see SI Fig. S1.
Figure 3: DeepCS produces agents that explore a large percentage of Montezuma’s Revenge.
Figure 4: We did not expect DeepCS to help Seaquest because the curiosity grid can quicklysaturate. At game start, the grid is unfilled and many intrinsic rewards can be obtained (left).
Figure 5: DeepCS does not need the curiosity grid input to improve domain exploration. Whenthe curiosity grid has been removed (No Grid), agents perform equally well on Seaquest; performancedrops on MR but remains better than A2C alone. Both games suffer when intrinsic rewards areremoved (No Intrinsic), suggesting that intrinsic rewards (not the grid) are the key aspect of DeepCS.
