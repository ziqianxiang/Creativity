Figure 1: Illustration of the interaction between a user and the recommendation system. Green arrowsrepresent the recommender information flow and orange arrows represent user’s information flow.
Figure 2: Architecture of our models parameterized by either position weight (PW) or LSTM.
Figure 3: Cascading Q-networks6 ExperimentsWe conduct three sets of experiments to evaluate our generative adversarial user model (calledGAN user model) and the resulting RL recommendation policy. Our experiments are designed toinvestigate the following questions: (1) Can GAN user model lead to better user behavior prediction?(2) Can GAN user model lead to higher user reward and click rate? and (3) Can GAN user modelhelp reduce the sample complexity of reinforcement learning?6.1	Dataset and feature descriptionWe experimented with 6 real-world datasets: (1) Ant Financial News dataset contains clicksrecords from 50,000 users for one month, involving dozens of thousands of news. On averageeach display set contains 5 news articles. It also contains user-item cross features which are widelyused in this online platform; (2) MovieLens contains a large number of movie ratings, from whichwe randomly sample 1,000 active users. Each display set is simulated by collecting 39 moviesreleased near the time the movie is rated. Movie features are collected from IMDB. Categoricaland descriptive features are encoded as sparse and dense vectors respectively; (3) Last.fm containslistening records from 359,347 users. Each display set is simulated by collecting 9 songs with thenearest time-stamp. (4) Yelp contains users’ reviews to various businesses. Each display set issimulated by collecting 9 businesses with the nearest location. (5) RecSys15 contains click-streamsthat sometimes end with purchase events. (6) Taobao contains the clicking and buying records ofusers in 22 days. We consider the buying records as positive events. (More details in Appendix C)
Figure 4: Comparison of the true trajectory(blue) of a user,s choices, the simulated trajectorypredicted by GAN model (orange curve in uppersub-figure) and the simulated trajectory predictedby W&D-CCF (the orange curve in the lowersub-figure) for the same user. Y-axis represents80 categories of movies.
Figure 5: Cumulative rewards among 1,000 users under the recommendation policies based on different usermodels. The experiments are repeated for 50 times and the standard deviation is plotted as the shaded area.
Figure 6: Each scatter-plot compares Qj* With Q56.4	User model assisted policy adaptationFormer results in section 6.2 and 6.3 have demonstrated that GAN is a better user model and RLpolicy based on it can achieve higher CTR compared to other user models, but this user model maybe misspecified. In this section, We shoW that our GAN model can help an RL policy to quicklyadapt to a neW user. The RL policy assisted by GAN user model is compared With other policies thatare learned from and adapted to online users: (1) CDQN with GAN: cascading Q-netWorks Whichare first trained using the learned GAN user model from other users and then adapted online to aneW user using MAML (Finn et al., 2017). (2) CDQN model free: cascading Q-netWorks Withoutpre-trained by the GAN model. It interacts With and adapts to online users directly. (3) LinUCB: aclassic contextual bandit algorithm Which assumes adversarial user behavior. We choose its strongerversion - LinUCB With hybrid linear models (Li et al., 2010) - to compare With.
Figure 7: Comparison of the averaged click rate averaged over 1,000 users under different recommendationpolicies. X -axis represents hoW many times the recommender interacts With online users. Y -axis is the clickrate. Each point (x, y) means the click rate y is achieved after x times of user interactions.
Figure 8: Two more examples: comparison of the true trajectory(blue) of user,s choices, the simulatedtrajectory predicted by GAN model (orange curve in upper sub-figure) and the simulated trajectory predictedby CCF (orange curve in the lower sub-figure) for the same user. Y-axis represents 80 categories of movies.
Figure 9: Comparison of click rates among 1,000 users under the recommendation policies based on differentuser models. In each figure, red curve represents GAN-DQN policy and blue curve represents the other. Theexperiments are repeated for 50 times and standard deviation is plotted as the shaded area. This figure is similarto figure 5, except that it plots the value of click rates instead of user’s cumulative rewards.
Figure 10: Comparison of the averaged cumulative reward among 1,000 users under different adaptiverecommendation policies. X-axis represents how many times the recommender interacts with online users.
