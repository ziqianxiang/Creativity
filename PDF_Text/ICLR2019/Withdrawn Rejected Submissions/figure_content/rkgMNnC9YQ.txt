Figure 1: Conceptual explainability model with an external observer model Mo explaining the behavior of anAI model M in the presence of prior Mp.
Figure 2: Attentive RNN based embeddingunweighted connections[hj ] decoder hidden stateɛ ∑ j sum operatorΘ encoder hidden state(equal to output embedding)input dataTo train this deep learning architecture, we use a multi-task approach where we train each of thesecomponents jointly. The joint training of this autoencoder with the observer allows us to softlyenforce explainability constraints in this pipeline. While the role of the observer is essentially toestimate the degree of explainability, we use it here to constrain the generation embeddings towardsexplainable models according to the metrics that we defined in the previous section. The encoder anddecoder are trained together to minimize the mean square reconstruction error between X and X,according a loss Lrec expressed as:nkLrec = nk XX((X) j-(X)j)2(11)The regressive observer is trained to provide the best explanations in the MDL sense. In the absenceof prior preferences, its learning is associated with a loss Lag defined as:k	kn
Figure 3: Suggested domain knowledge for the grouping of features into embedding dimensionscohort of 2395 patients who experienced sepsis, resulting in a total of 109886 data samples. Thisdata set is sampled on an hourly basis. Each data sample consists of 37 variables together with atimestamp and a patient ID. The name of these variables are shown on the Y axis of the plot shownon Figure 3.
Figure 4: Distribution of the attention coefficients for λag =1on the left and λag =10on the right.
Figure 5: Distribution of the attention coefficients for λaw =1on the left and λaw =5on the right.
