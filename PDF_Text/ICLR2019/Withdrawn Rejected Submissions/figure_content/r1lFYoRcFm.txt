Figure 1: (a) An agent freely moving in a 2D world might try to reach a goal at position (0, 0) bytaking action a. A sensible reward in this environment is the change in absolute distance to the goal.
Figure 2: An overview of the architecture setup. The agent and the position change predictionnetwork (PCPN) are instantiated with neural networks and trained through quantile regression rein-forcement learning and quantile regression, respectively.
Figure 3: Average number of steps needed to reach the goal over the course of training of an agentthat chooses pairwise angle and step size for pairwise grouped dimensions. Results are shown fora 2, 4, 8 and 16 dimensional environment, corresponding plots are ordered from left to right. Thex-axis shows the number of training steps in millions while the y-axis shows the average episodelength over the last 100 episodes. Training length was fixed to 2,560,000 steps. Plotted is theaverage of 3 training runs with the shaded area indicating the standard deviation between runs. Theplots suggest that the higher dimensional the environment, the more apparent the gain of training onvector rewards is.
Figure 4: Average number of steps needed to reach the goal over the course of training of an agentthat can select the dimension it wants to move in and the amount by how much it wants to move.
Figure 5: Average number of steps needed to reach the goal over the course of training of an agentthat controls a robot arm as depicted in Figure 1(b). The x-axis shows the number of training stepsin millions while the y-axis shows the average episode length over the last 100 episodes. Plotted isthe average of 5 training runs with the shaded area indicating the standard deviation between runs.
