Figure 1: Comparing the distance to decision boundary with the σ for which the error rate in Gaussiannoise is 1%. Each point represents 50 images from the test set, and the median values for eachcoordinate are shown. (The PGD attack was run with = 1, so the distances to the decision boundaryreported here are cut off at 1.) We also see histograms of the x coordinates. (A misclassified point isassigned σ = 0.)0.0	0.1	0.2	0.3	0.4	0.5	0.6Sigma at which error rate is 0.01Note that this equality depends only on the error rate μ and the standard deviation σ of a singlecomponent, and not directly on the dimension. This might seem at odds with the emphasis onhigh-dimensional geometry in Section 3. The dimension does appear if We consider the norm of atypical sample from N(0, σ2I), which is σ√n. As the dimension increases, so does the ratio betweenthe distance to a noisy image and the distance to the decision boundary.
Figure 2: Two-dimensional slices of image space through different triples of points together with theclasses assigned by a trained model. The black circle in both images has radius 31.4, correspondingto noise with σ = 31.4/√n = 0.08.
Figure 3: The performance in Gaussian noise of several previously published defenses for ImageNet,along with a model trained on Gaussian noise at σ = 0.4 for comparison. For each point we ranten trials; the error bars show one standard deviation. All of these defenses are now known notto improve adversarial robustness (Athalye et al., 2018). The defense strategies include bitdepthreduction (Guo et al., 2017), JPEG compression (Guo et al., 2017; Dziugaite et al., 2016; Liu et al.,2018; Aydemir et al., 2018; Das et al., 2018; 2017), Pixel Deflection (Prakash et al., 2018), totalvariance minimization (Guo et al., 2017), respresentation-guided denoising (Liao et al., 2018), andrandom resizing and random padding of the input image (Xie et al., 2017).
Figure 4: The adversarial example phenomenon occurs for noisy images as well as clean ones. Start-ing with a noisy image that that is correctly classified, one can apply carefully crafted imperceptiblenoise to it which causes the model to output an incorrect answer. This occurs even though the errorrate among random Gaussian perturbations of this image is small (less than .1% for the ImageNetpanda shown above). In fact, we prove that the presence of errors in Gaussian noise logically impliesthat small adversarial perturbations exists around noisy images. The only way to “defend” againstsuch adversarial perturbations is to reduce the error rate in Gaussian noise.
Figure 5: These plots give two ways to visualize the relationship between the error rate in noiseand the distance from noisy points to the decision boundary (found using PGD). Each point on eachplot represents one image from the test set. On the left, we compare the error rate of the model onGaussian perturbations at σ = 0.1 to the distance from the median noisy point to its nearest error.
Figure 6: The Gaussian isoperimetric inequality relates the amount of probability mass containedin a set E to the amount contained in its -extension E . A sample from the Gaussian is equallylikely to land in the pink set on the left or the pink set on the right, but the set on the right has alarger -extension. The Gaussian isoperimetric inequality says that the sets with the smallest possible-extensions are half spaces.
Figure 7: Top: The optimal curves on Imagenet for different values of σ. Middle: Visualizingdifferent coordinates of the optimal curves. First, random samples from x + N(0, σI) for differentvalues of σ. Bottom: Images at different l2 distances from the unperturbed clean image. Each imagevisualized is the image at the given l2 distance which minimizes visual similarity according to theSSIM metric. Note that images at l2 < 5 have almost no perceptible change from the clean imagedespite the fact that SSIM visual similarity is minimized.
Figure 8: A slice through a clean test point (black, center image), the closest error found using PGD(blue, top image), and a random error found using Gaussian noise (red, bottom image). For thisvisualization, and all others in this section involving Gaussian noise, we used noise with σ = 0.05, atwhich the error rate was about 1.7%. In all of these images, the black circle indicates the distance atwhich the typical such Gaussian sample will lie. The plot on the right shows the probability that themodel assigned to its chosen class. Green indicates a correct prediction, gray or white is an incorrectprediction, and brighter means more confident.
Figure 9: A slice through a clean test point (black, center image), the closest error found using PGD(blue, top image), and the average of a large number of errors randomly found using Gaussian noise(red, bottom image). The distance from the clean image to the PGD error was 0.12, and the distancefrom the clean image to the averaged error was 0.33. The clean image is assigned the correct classwith probability 99.9995% and the average and PGD errors are assigned the incorrect class withprobabilities 55.3% and 61.4% respectively. However, it is clear from this image that moving even asmall amount into the orange region will increase these latter numbers significantly. For example, theprobability assigned to the PGD error can be increased to 99% by moving it further from the cleanimage in the same direction by a distance of 0.07.
Figure 10: A slice through a clean test point (black, center image), a random error found usingGaussian noise (blue, top image), and the average of a large number of errors randomly found usingGaussian noise (red, bottom image).
Figure 11: A slice through a clean test point (black, center image) and two random errors found usingGaussian noise (blue and red, top and bottom images). Note that both random errors lie very close tothe decision boundary, and in this slice the decision boundary does not appear to come close to theclean image.
Figure 12: A slice through three random errors found using Gaussian noise. (Note, in particular, thatthe black point in this visualization does not correspond to the clean image.)Figure 13: A completely random slice through the clean image.
Figure 13: A completely random slice through the clean image.
Figure 15: The cdf of the error rates in noise for images in the test set. The blue curve correspondsto a model trained and tested on noise with σ = 0.1, and the green curve is for a model trained andtested at σ = 0.3. For example, the left most point on the blue curve indicates that about 40% of testimages had an error rate of at least 10-3.
Figure 14: Some visualizations of the same phenomenon, but using the “pepper noise” discussedin Section 5 rather than Gaussian noise. In all of these visualizations, we see the slice through theclean image (black, center image), the same PGD error as above (red, bottom image), and a randomerror found using pepper noise (blue, top image). In the visualization on the left, we used an amountof noise that places the noisy image further from the clean image than in the Gaussian cases weconsidered above. In the visualization in the center, we selected a noisy image which was assigned toneither the correct class nor the class of the PGD error. In the visualization on the right, we selected anoisy image which was assigned to the same class as the PGD error.
Figure 16: A collection of adversarially chosen model errors. These errors appeared in the ImageNetvalidation set. Despite the high accuracy of the model there remain plenty of errors in the test set thata human would not make.
Figure 17: A collection of adversarially chosen model errors. These errors appeared in the ImageNetvalidation set. Despite the high accuracy of the model there remain plenty of errors in the test set thata human would not make.
Figure 18: Visualizing the severity of PCA noise, along with model errors found in this noisedistribution.
Figure 19: Visualizing the severity of Gaussian noise, along with model errors found in this noisedistribution. Note the model shown here was trained at noise level σ = .6.
Figure 20: Visualizing the severity of pepper noise.
Figure 21: Visualizing the severity of the randomized stAdv attack.
