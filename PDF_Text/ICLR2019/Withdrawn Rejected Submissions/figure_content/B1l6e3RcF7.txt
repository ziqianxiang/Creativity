Figure 1: Plots for VGG-11 architecture trained using SGD on CIFAR-10. Each plot contains thetraining loss for 40 iterations of training at various epochs. Between the training loss at everyconsecutive iteration (vertical gray lines), we uniformly sample 10 points between the parametersbefore and after the training update and calculate the loss at these points. Thus we take a slice of theloss surface between two iterations. These loss values are plotted between every consecutive trainingloss value from training updates. We find that the loss interpolation between consecutive iterationshave a minimum in between in all cases showing barriers are not being crossed. For epochs 25and 100 this is not clearly visible, but we quantitatively record it and discuss it later. The dashedorange line (only shown in the epoch 1 plot) connects the minimum of the loss interpolation betweenconsecutive iterations and is shown to highlight that the valley floor has ups and downs along thepath of SGD (which can be seen for all epochs).
Figure 2: Plots for Resnet-56 architecture trained using SGD on CIFAR-10. All the descriptions aresame as described in figure 1.
Figure 3: Plots for MLP architecture trained using SGD on MNIST. All the descriptions are sameas described in figure 1.
Figure 4: Numbers of barriers found during training loss interpolation for every epoch (450 itera-tions) for VGG-11 on CIFAR-10. We say a barrier exists during a training update step if there existsa point between the parameters before and after an update which has a loss value higher than the lossat either points. Note that even for these barriers, their heights (defined by '(""+'(&+1)-2'(4ä¸€))are substantially smaller compared with the value of loss at the corresponding iterations (not men-tioned here), meaning they are not significant barriers.
Figure 5: Plots for the alignments between mini-batch gradient and hessian with VGG-11 architec-ture trained using SGD on CIFAR-10 at the end of Epoch 5 and Epoch 10. Alignments are calculatedfor mini-batch size 100,1000,10000 and 45000 (dataset size).
Figure 6: Plots for the alignments between mini-batch gradient and hessian with Resnet-56 archi-tecture trained using SGD on CIFAR-100 at the end of Epoch 5 and Epoch 10. All the descriptionsare same as described in figure 5.
Figure 7: Plots of the consine calculated according to Equation 5 for Resnet-56 on CIFAR-10 andWResnet on CIFAR-100 for different batch sizes.
Figure 13: Plots for VGG-11 Epoch 2 trainedusing SGD on CIFAR-10.
Figure 14: Plots for VGG-11 Epoch 25 trainedusing SGD on CIFAR-10.
Figure 8: Plots for Resnet-56 Epoch 1 trainedusing full batch Gradient Descent (GD) onCIFAR-10.
Figure 9: Plots for Resnet-56 Epoch 1 trainedusing SGD on CIFAR-10.
Figure 15: Plots for VGG-11 Epoch 100 trainedusing SGD on CIFAR-10.
Figure 10: Plots for Resnet-56 Epoch 2 trainedusing SGD on CIFAR-10.
Figure 16: Plots for MLP Epoch 1 trained usingfull batch Gradient Descent (GD) on MNIST.
Figure 11: Plots for Resnet-56 Epoch 25 trainedusing SGD on CIFAR-10.
Figure 17: Plots for MLP Epoch 1 trained usingSGD on MNIST.
Figure 12: Plots for Resnet-56 Epoch 100 trainedusing SGD on CIFAR-10.
Figure 23: Plots for VGG-11 Epoch 1 trainedusing learning rate 0.1 batch size 500 on CIFAR-10.
Figure 18: Plots for MLP Epoch 2 trained usingSGD on MNIST.
Figure 24: Plots for VGG-11 Epoch 1 trained us-ing learning rate 0.1 batch size 1000 on CIFAR-10.
Figure 19: Plots for VGG-11 Epoch 1 trained us-ing full batch Gradient Descent (GD) on Tiny-ImageNet.
Figure 25: Plots for Resnet-56 Epoch 1 trainedusing learning rate 0.7 batch size 100 on CIFAR-10.
Figure 20: Plots for VGG-11 Epoch 1 trainedusing SGD on Tiny-ImageNet.
Figure 26: Plots for Resnet-56 Epoch 1 trainedusing learning rate 1 batch size 100 on CIFAR-10.
Figure 21: Plots for VGG-11 Epoch 1 trainedusing learning rate 0.3 batch size 100 on CIFAR-10.
Figure 27: Plots for Resnet-56 Epoch 1 trainedusing learning rate 1 batch size 500 on CIFAR-10.
Figure 22: Plots for VGG-11 Epoch 1 trainedusing learning rate 0.2 batch size 100 on CIFAR-10.
Figure 28: Plots for Resnet-56 Epoch 1 trainedusing learning rate 1 batch size 1000 on CIFAR-10.
