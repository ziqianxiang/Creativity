Figure 1: Implicit vs explicit representations. Explicit voxel and mesh representations areviewpoint-independent and constitutes the complete scene. Our implicit surfel-based represen-tation is viewpoint-dependent and it adapts the resolution to the viewpoint. The full scene is containedin a high-dimensional latent variable and only when the scene is to be rendered, the latent variable isserialized to surfels for a specific view.
Figure 2: Differentiable 3D renderer. (a) A surfel is defined by its position P , normal N, andreflectance ρ. Each surfel maps to an image pixel Pim. (b) The surfel’s color depends on its reflectanceρ and the angles θ between each light I and the surfel’s normal N.
Figure 3: Pix2scene model. Pix2scene generates realistic 3D views of scenes by training on 2Dimages alone. Its decoder generates the surfels depth pz from a noise vector z conditioned on thecamera pose. The surfels normal is estimated from its predicted depth. The surfels are then renderedinto a 2D image and together with image samples from the target distribution are fed to the critic.
Figure 4: Scene reconstruction. Left: Input images of rotated objects into a room with its depth andnormal groundtruth maps. Right: pix2scene reconstructions with its depth and normal maps.
Figure 6: Multiple-shape scenes reconstruction. Implicit 3D reconstruction of scenes composedby multiple ShapeNet objects.
Figure 7: Unconditional scene generation. Generated samples from pix2scene model trained onShapeNet scenes. Left: shaded images; Right: depth mapsFigure 8: Conditional scene generation. Class conditioned generated samples for ShapeNet dataset.
Figure 8: Conditional scene generation. Class conditioned generated samples for ShapeNet dataset.
Figure 9: Manifold exploration. Exploration of the learned manifold of 3D representations. Gener-ated interpolations (middle columns) between two images x1 and x2 (first and last columns).
Figure 10: Sample questions from the 3D-IQ test task. For this ”mental rotation” task, a set ofreference images and 3 possible answers are presented. The goal is to find the rotated version ofthe reference 3D model. To solve this task, the human or the model has to infer the 3D shape of thereference from the 2D image and compare that to the inferred 3D shapes of the answers. The correctanswers to these two examples are in the footnote.
Figure 11: 3D IQ test task. Pix2scene reconstructions of the 3D-IQTT shapes.
Figure 12: Random lights configuration.
Figure 13: 3D-IQTT baseline architecture. The ResNet-50 all share the same weights and wereslightly modified to support our image size. ”FC” stands for fully-connected layer and the hiddennode sizes are 2048, 512, and 256 respectively. The output of the network is encoded as one-hotvector.
Figure 14: Scene reconstruction. (a) Input images of rotated cubes into a room. (b) pix2scenereconstructions with its (c) associated depth maps.
Figure 15: Normal views reconstruction. For each row, the first column is the input image andother columns are the extrapolated normal maps of that image from different views.
Figure 16: Reconstruction of complex scenes. Reconstruction of bedroom scenes and bunny.
