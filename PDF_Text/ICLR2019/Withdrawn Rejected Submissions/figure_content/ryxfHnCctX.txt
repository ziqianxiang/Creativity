Figure 1: Pipline of our method. The main network in this figure is already pre-trained. From leftto right: our subsidiary component training for i-th layer, main network retraining, and subsidiarycomponent training for (i+1)-th layer.
Figure 2: The learning curve for the SubsidiaryComponent. The red line refers to the learningcurve without the distillation loss, and the red4 ExperimentsTo evaluate our method, we conduct sev-eral pruning experiments for VGG-11, Net-In-Net (NIN), and ResNet-18 on CIFAR-10 andImageNet. Since our goal is to simplify binarybackground represents the learning curve varianceof every epoch. The green line and backgroundrepresent the subsidiary component learning curveand the variance of each epoch. Clearly, the distil-lation loss makes the training more stable.
Figure 3: Learning curve for subsidiary compo-nent. We train the subsidiary component with dif-ferent learning rate. These curves are smoothedfor the directly seeing the trend of the learningSubsidiary Component. All the dotted lines rep-resent the learning curve of the large learningrate 10-3, the normal lines represent the learningcurves of the small learning rate 10-4.
Figure 4: Results. We control all methods to have the same PFR and compare their error afterretraining. Compared with the baseline designed by US (in orange and green), performance of ourlearning-based method (in pink) is better with the same PFR. From left to right, the PFR is 33.05%,39.70%, 39.89%, and 21.40%. For ResNet-18 on ImageNet, We pruned 21.4% of the filters whilethe retrained error decreased from 50.02% to 49.87%layer, which makes the training process more stable and converge with high performance. For bothMSF-Layerwise and MSF-Cascade, with the same PCR, the performance is worse than us. With30% 〜40% of pruning filter ratio, the pruned network error rate only increased 1% 〜2%.
Figure 5: Gradient flow of binary neural networks during back-propagation. Rectangles representthe weight tensor and ellipses represent functional operation. In this paper, we use binary operationas a special quantization function. MAC is short for multiplication and accumulate operations, orthe equivalent substitution like XNOR (Biswas & Chandrakasan, 2018) in BNN.
Figure 6: Comparison Our method with MSF-Layerwise. It is for every binary convolution filter inVGG-11. The x-axis is the pruning filter rate. The y-axis is the validation accuracy. As shown infigure, our method prunes more filters with better validation accuracy.
