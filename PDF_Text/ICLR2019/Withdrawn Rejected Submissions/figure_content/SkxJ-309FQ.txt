Figure 1: Hallucination percentages. (A.) Percentage of all test set sentences that can be perturbed to hallucination (usingany type of hallucination token at any location) as a function of model variant: canonical, beam search decoding, numberof hidden dimensions (512 and 1024), and vocabulary size (16K and 32K BPE codes). Red stars indicate model varianthallucination % is statistically lower than the hallucination % in the canonical model. (B.) Hallucination percentages asa function of perturbation token type and position inserted in the input sentence over all canonical model random seeds.
Figure 2: Relationship between BLEU score and hallucination percentage for all models. We do not find hallucinationpercentage decreases as BLEU score increases. We found a correlation coefficient of .33 between BLEU and hallucinationpercentage with a significant p-value (p < 0.001). While the correlation is significant, we cautiously interpret theseresults because we do not fully explore the space of all possible models. The Transformer models are the only models weanalyzed with < 20 BLEU which we explain in section 5.3. For the expansion of acronyms, please also see section 5.3.
Figure 3: Hallucination percentages for modelvariants: dropout, L2E, L2R, L2, CFN, DA,TR, and tied decoder initial state (TDIS). Thered stars denote a statistically significant differ-ence (p < 0.05) between the hallucination %of the model variant and the canonical model.
Figure 4: Effects of data augmented (DA) training when including all perturbation types Vs excluding common andbeginning perturbation types. We trained two models, one including all perturbations types for DA training, and the otherexcluding common and beginning perturbation types. We then examined the hallucination percentage of each perturbationtype for both of these models and studied whether a DA model would be less prone to hallucinate when perturbed withtypes of tokens or positions it had not been trained against. Red star shows that DA w/o beginning or common hadstatistically significantly reduced mean compared to the canonical model trained without DA.
Figure 5: The attention matrix can reveal hallucinations. The attention matrix shows how much weight is applied toeach token in the source sequence while decoding each token in the resulting translated sequence. A. Normal translationsproduce attention matrices that show a distribution of weight across most tokens in the source sequence throughoutdecoding (x-axis source sequence, y-axis decoded sequence). B. However, during hallucinations, the attention networktends to place weight on only a few input tokens. We can see the majority of the weight throughout decoding is placedon the ”.” in the source sequence. C, D. We used an entropy measure to quantify how distributed the attention is overthe input source sequence. Shown are the distributions for normal translations C. and hallucinations D. for the original(blue) and perturbed (green) attention entropy values. Mean of the entropy distributions for hallucinations are statisticallysignificantly different (Mann-Whitney U test, P < 0.05).
Figure 6: We compared the hidden states of the decoder for both types of perturbed sentences, hallucinating (red) andnon-hallucinating (blue), with the hidden states of the decoder of the original sentence. The total area of each histogramis normalized to 1. A. Distance between the hidden state of the decoder for perturbed sentence and the original sentence,kh1(xp) - h1(xp)k. B. Ratio of the norm of the hidden states of the decoder of the perturbed sentence and the originalsentence, |h1 (xp)|/|h1(xo)|. C. The number of original sentences (y-axis) that have 0 < n < 10, (x-axis) perturbeddecodes with E(xo, xp) > 1 (see text).
Figure 7: Schematic of the NMT decoder. The input sequence, xi：S, is encoded by a bidirectional encoder(not shown) into a sequence of encodings, z1:S. The attention network, fatt, computes a weighted sum ofthese encodings (computed weights not shown), based on conditioning information from h and provides theweighted encoding to the 2-layer decoder, fdec, as indicated by the arrows. The decoder proceeds forward intime producing the translation one step at a time. As the decoder proceeds forward, it interacts with both theattention network and also receives as input the decoded output symbol from the previous time step.
Figure 8: Example attention matrices. (left) Attention matrix for the original input sentence. (right) Attention matrix forthe perturbed input sentence. All translations of the perturbed input sentence shown here are hallucinations. All decodingwas done with the canonical model.
Figure 9: Stability analysis of hallucinations vs. normal translations. (top left) Distribution of stability exponents forcanonical model for unperturbed (blue) and perturbed, no hallucination (red) input sequences that resulted in a normaltranslation (Median (dots) and 25% and 75% quartiles (solid lines) shown). (top right) Same, but translations thatcould be perturbed to hallucinate (blue - unperturbed, red - hallucination). Red stars denote statistical significance (Utest, p < 0.001) in the difference in stability exponents between perturbed and unperturbed compared between normaltranslations and hallucinations. (bottom left, right) Same as top, except for model trained with data augmentation.
