Figure 1: Prioritized DDPG vs DDPGOne a few environments such as the Reacher, InvertedDoublePendulum and Walker2d, it can beseen from Figure 1 the prioritized DDPG only ,marginally outperforms the DDPG algorithm.
Figure 2: Prioritized DDPG vs DDPG with adaptive-param noiseFigure 3: Prioritized DDPG vs DDPG with with correlated noise7 ConclusionsTo summarize, this paper discusses the state of the art methods in reinforcement learning with ourimprovements that have led to RL algorithms in continuous state and action spaces that outperformthe existing ones.
Figure 3: Prioritized DDPG vs DDPG with with correlated noise7 ConclusionsTo summarize, this paper discusses the state of the art methods in reinforcement learning with ourimprovements that have led to RL algorithms in continuous state and action spaces that outperformthe existing ones.
Figure 4: Prioritized DDPG vs DDPG with uncorrelated noiseFigure 5: Prioritized DDPG vs DDPG with best results of both algorithms across all noises -adaptive-param, uncorrelated, co related and with no noiseof noises further improve PDDPG to help attain higher rewards. One other important conclusionis that different kinds of noises work better for different environments which was evident in howdrastically the results changed based on the parameter noise.
Figure 5: Prioritized DDPG vs DDPG with best results of both algorithms across all noises -adaptive-param, uncorrelated, co related and with no noiseof noises further improve PDDPG to help attain higher rewards. One other important conclusionis that different kinds of noises work better for different environments which was evident in howdrastically the results changed based on the parameter noise.
