Figure 1: Architecture of a 3-period RKNet. y(d) denotes the system state of period d. y0(d) is theinitial state of period d. yr(d) is the final state after r time-steps in period d. r is the total number oftime-steps in a period. It can vary in different periods. Period 1 and time-step 1 in it are unfoldedas an example. System state changes throughout a period. The final state of a step is estimated asthe initial state of this step adding an increment. This operation originates from RK methods. Toapproximate the increment is the key point in RKNet. The dotted lines are for multiscale featurestrategy.
Figure 2: Architecture of one time-step in ERKNet using an s-stage ERK method. yn is the approx-imation of y(tn). A dense block grows every m times at a growth rate of k to form a convolutionalsubnetwork for generating each hbizi . Here, h is time-step size, bi is coefficient of ERK method,and zi is the slope of each stage in ERK method. The total number of growth is ms in a dense blockin order to generate hbizi for i = 1, . . . , s. An explicit summation layer is added after a denseblock to complete a time-step.
Figure 3: Architecture of one time-step in IRKNet using a 3-stage IRK method. yn is the approx-imation of y(tn). A dense block, which is Stage-I of a clique block, grows k channels every timeto generate the initial value of each hbizi, written as vi . Here, h is time-step size, bi is coefficientof IRK method, and zi is the slope of each stage in IRK method. In Stage-II of a clique block, theconvolutional subnetwork concatenating the current values of hbj zj for j = 1, . . . , 3, j 6= i to up-date every hbi zi alternately. An explicit summation layer is added after a clique block to completea time-step.
Figure 4: Comparison of the DenseNets, CliqueNets and RKNets. The top-1 error rates (single-croptesting) on the ImageNet validation dataset are shown as a function of learned parameters (left) andFLOPs during test-time (right). RKNets compared here are the models shown in Table 3.
