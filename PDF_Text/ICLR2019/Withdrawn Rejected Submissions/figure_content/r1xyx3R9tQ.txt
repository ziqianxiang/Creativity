Figure 1:	Correlation coefficients for our five prototypicality metrics on four common datasets.
Figure 2:	Our five metrics’ least and most prototypical training examples, separated by a red/greenbar, for three classes of MNIST, Fashion-MNIST, and CFIAR-10 (all classes shown in Appendix C).
Figure 3:	Comparing the differences between the metrics on (a) MNIST and (b-d) Fashion-MNIST.
Figure 4: Exceptional “shirts.”6Under review as a conference paper at ICLR 2019(a)	Memorized exceptions in theFashion-MNIST “sneaker” class.
Figure 5:	Our metrics’ prototype and outlier sets reveal interesting examples, which can be clustered.
Figure 6:	Comparing training on the k-most prototypical and k-least prototypical examples (asdetermined by the adv metric). The first row of subfigures plots the model accuracy resultingfrom training on a slice of 5, 000 training examples consecutively ranked by prototypicality; ineach prototypicality increases from left to right (i.e., most extreme outliers are on the far left, mostprotypical training examples are on the far right). On MNIST (a) training on the outliers givesstrictly higher accuracy. However, on Fashion-MNIST (b) and CIFAR-10 (c), given only a limitedamount of training data, prototype-training is better; but if more data is available outlier-trainingbecomes superior. At least for some datasets, heavy data augmentation and regularization tech-niques like dropout can partially recover the utility loss that results from training only with moreprotoypical examples; the dotted line in subfigure (a) shows the result of one such experiment forMNIST.
Figure 7:	MNIST23Under review as a conference paper at ICLR 2019Prototypicality Percentile of Train Data	0.46	0.53	0.63	0.69	0.77	0.81	0.89	0.92	0.97	0.994 J	0.48	0.58	0.72	0.78	0.86	0.91	0.95	0.98	0.99	1.00										8 J	0.48	0.62	0.74	0.83	0.89	0.94	0.97	0.99	1.00	1.00										12 J	0.48	0.63	0.76	0.87	0.92	0.96	0.98	0.99	1.00	1.00										16 J	0.48	0.66	0.78	0.89	0.93	0.97	0.98	0.99	1.00	1.00										20 J	0.47	0.65	0.79	0.90	0.93	0.97	0.98	0.99	1.00	1.00										24 J	0.47	0.65	0.79	0.91	0.94	0.97	0.98	0.99	1.00	1.00										28 J	0.48	0.66	0.79	0.91	0.94	0.97	0.99	0.99	1.00	1.00										32 T	0.46	0.64	0.79	0.89	0.94	0.97	0.98	0.99	1.00	1.00
Figure 8:	Fashion-MNIST24Under review as a conference paper at ICLR 2019Prototypicality Percentile of Train Data	0.36	0.40	0.47	0.54	0.62 0.65	0.71	0.75	0.79	0.854 T	0.36	0.41	0.48	0.58	0.65 0.69	0.74		0.82	0.898 J	0.36	0.42	0.51	0.59	0.69 0.74		0.82	0.87	0.9312 J	0.36	0.44	0.54	0.64	0.72 0.78	0.81	0.84	0.89	0.95									16 J	0.37	0.44	0.53	0.65	0.73 0.79	0.84	0.86	0.90	0.95									20 J	0.38	0.45	0.56	0.67	0.81	0.85	0.88	0.92	0.96									24 J	0.39	0.46	0.57	0.68	0.82	0.88	0.90	0.94	0.97									28 J	0.37	0.46	0.57	0.69	0.83	0.88	0.92	0.95	0.97									32 J	0.37	0.45	0.58	0.69	0.84	0.90	0.92	0.95	0.98									36，	0.37	0.46	0.56	0.69	0.84	0.89	0.92	0.96	0.98
Figure 9:	CIFAR-1025Under review as a conference paper at ICLR 2019D Human S tudy ExampleWe presented Mechanical Turk taskers with the following webpage, asking them to select the worstimage of the nine in the grid.
Figure 10: The blue curves indicate the training accuracy of models trained on slices of 5, 000examples selected according to their prototypicality—as reported on the x-axis. A baseline, obtainedby training the model on the entire dataset is indicated by the dotted-orange line. Models trained onprototypes on Fashion-MNIST and CIFAR-10 are 2× more robust to adversarial examples, whentraining on slices of 5, 000 prototypical examples as opposed to slices of 5, 000 outlier examples.
Figure 11: Uncommon submodes found within the MNIST “1” class, and their HDBSCAN clusters.
Figure 12: Memorized exceptions in the Fashion-MNIST “sneakers,” and their HDBSCAN clusters.
Figure 13: Memorized exceptions in the Fashion-MNIST “shirts,”, and their HDBSCAN clusters.
Figure 14: Canonical prototypes in the CIFAR-10 “airplane” class.
