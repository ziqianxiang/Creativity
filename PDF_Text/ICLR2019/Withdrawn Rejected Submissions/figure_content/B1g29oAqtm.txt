Figure 1: Action-conditional vs. plan-conditional predictors.
Figure 2: Our off-the-shelf planner,based on the cross-entropy method(Botev, 2011). See text for details.
Figure 3: Reward attained at convergence on Swimmer by action-conditional predictors (Range =1) and plan-conditional predictors (Range > 1) trained using different planning horizons. For thisexperiment we use the PCP variant which can make intermediate predictions to allow planninghorizons that are not even multiples of ranges, e.g. a range 40 model with horizon 60.
Figure 4: Reward as a function of planning horizon for Swimmer and HalfCheetah usingground-truth dynamics with our planner. Swimmer performance increases up to a horizon ofH = 150. For HalfCheetah the optimal horizon is around H = 40, after which performancedecreases due to the instability of the simulation when using single- instead of double-precisionfloating point.3The results of this experiment, shown in Figure 4, show that the optimal planning horizons forSwimmer and HalfCheetah are at around 150 and 40 timesteps, respectively. These resultsprovide additional clarity to those presented in the previous experiment. Policies that use the ground-truth dynamics as their predictor perform better as the horizon increases due to the decreased bias ofthe long-horizon reward estimate. The approximate dynamics model from PCPs show similar gains.
Figure 5: Mujoco Swimmer task. Prediction error in estimated distance traveled as a functionof timesteps in the future for two models trained on the same dataset of off-line trajectories: a 1-step predictor (blue), an RNN trained with 200 steps of BPTT (brown), and a 50-step predictor(orange). The 1-step model, which is standard in the literature, exhibits poor long-term predictionsdue to accumulating accumulating errors. By contrast, the 50-step model and the RNN are trainedto minimize long-term error.
Figure 6: Experiments on Swimmer showing the relationship between the predictorâ€™s accuracy atH = 100 steps into the future and the reward obtained by using that predictor for MPC with a hori-zon of H timesteps. These plots include five seeds each of ACP and PCP (R = 50), with each pointrepresenting a particular predictor at some point in the training process. Left: Absolute error at pre-dicting the X-coordinate H steps in the future along a trajectory taken by a trained PPO agent. Ourpolicies select actions based on which plan results in the greatest X-coordinate prediction. Right:`2 loss at predicting the outcome from following a plan sampled from the candidate distribution C .
Figure 7: These plots show the scores achieved by ACP, PCP, RNN, and methods previously pub-lished in the literature. Solid lines indicate average reward; translucent bars indicate the best and theworst performance across 5 seeds. Left: On Swimmer PCP achieves rewards of 185, outperformingany previous model-based or model-free method. In particular, the next-best model-based approachreaches scores less than half those attained by PCP. RNN models outperform ACP but lag behindPCP. Right: On HalfCheetah PCP equals the rewards attained by PPO. PCP outperforms ACPand Nagabandi et al. (2017) despite the planning horizon being a short H = 20.
Figure 8:	Results from using our CEM optimizer and the ground-truth dynamics to plan with varyinghorizons across all 13 MuJoCo environments in OpenAI Gym. We believe these results could behelpful for comparing the relative impact of delayed reward in these environments. In all cases CEMuses a pool of50 candidates and 5 steps of optimization per timestep. Of particular note: (1) In someenvironments, planning with long horizons performs worse than planning with short horizons. Thisis largely due to instability in those simulations, resulting in incorrect model predictions when usingsingle- vs double-precision state encodings and long time horizons. (2) In the Cheetah environment,CEM is able to break the simulation and earn rewards greater than should be physically possible.
Figure 9:	This histogram aggregates the results shown in Figure 8. Of the 13 MuJoCo environmentsin OpenAI Gym, 4 have optimal planning horizons above 100 timesteps.
