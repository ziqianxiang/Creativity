Figure 1: The proposed encoder architecture. Based on the attention map, a series of residualblocks generate a watermark signal specific to the input sample x which will be later merged withthe generated watermark signal. All those convolutions in this encoder use the stride of 1 and thechannel of 3 to maintain the identical input and output dimensions.
Figure 2: Watermarking examples. (b) and (e) are generated watermarks (before being merged withimages). These are cases where watermarks are generated, aided by attention. For (e), there is awatermark in the most lower right corner and its attention also focuses on the same area. However,attention maps sometimes provide similar weights over almost all pixels, in which cases watermarksare scattered over all pixels — examples in Figure 3 correspond to this case.
Figure 3: Examples of watermarking FR images. (d) and (i) are watermarked by the method of (Liuet al., 2018). Others are watermarked by our method. The decoder has 3 convolution layers inthese examples. Note that there are more modifications on the color tone of images as γ increases.
Figure 4: Examples of watermarking ImageNet images. Some dots are marked explicitly to hidewatermarks when γ >= 0.05. Recall that watermarks are hidden in the tone of colors for FRimages. This is a very interesting point because our proposed method can discover two very differentwatermarking schemes for them. This is because adding dots does not make the regularization termgreatly exceed the margin γ. When γ = 0.01, a similar watermarking scheme to the FR exmapleswill be used. This proves that our method is able to fine the best suited watermarking scheme givendata samples. The decoder has 3 convolution layers in these examples. Note that there are moremodifications in general as γ increases. For all cases, the trained decoder can successfully decodetheir watermarks.
Figure 5: The decoding success rate in the ImageNet dataset. We report the decoding success ratefor non-watermarked/watermarked cases with our method after varying the convolution numbers inthe decoder (i.e. decoder Size) and γ._____________________________________________Our method					Decoder size = 1			Decoder size =		3Y = 0.01	γ = 0.05	γ = 0.1	γ = 0.01	γ = 0.05	-γ = 0.1-81.2%∕100.0%	89.2%∕100.0%	92.0%∕100.0%	99.0%∕100.0%	98.0%/99.4%	99.5%/100.0%Figure 6: The decoding success rate in the Flowers datasetOur method					Decoder size = 1			Decoder size = 3		γ = 0.01	γ = 0.05	γ = 0.1	γ = 0.01	γ = 0.05	γ = 0.194.6%∕97.1.0%	96.9%/94.5%	97.9%/99.9%	100.0%∕99.0%	100.0%/99.8%	100.0%/99.0%A Additional Experiment ResultsA.1 ImageNetWe introduce additional experiments that were removed from the main paper. In Table 5, we reportthe decoding success rate for the ImageNet dataset. In all configurations, their success rates are veryhigh. In particular, the decoder with 3 convolution layers provides the highest decoding success rate.
Figure 6: The decoding success rate in the Flowers datasetOur method					Decoder size = 1			Decoder size = 3		γ = 0.01	γ = 0.05	γ = 0.1	γ = 0.01	γ = 0.05	γ = 0.194.6%∕97.1.0%	96.9%/94.5%	97.9%/99.9%	100.0%∕99.0%	100.0%/99.8%	100.0%/99.0%A Additional Experiment ResultsA.1 ImageNetWe introduce additional experiments that were removed from the main paper. In Table 5, we reportthe decoding success rate for the ImageNet dataset. In all configurations, their success rates are veryhigh. In particular, the decoder with 3 convolution layers provides the highest decoding success rate.
