Figure 1: Error caused by biased sampling with batch normalization: Consider a network is trainedon Task 1 and Task 2 separately. (a) During training, batch normalization only estimate the runningmean of each task (b) batch normalization use learned mean for prediction. The classifier fails topredict the right answer in this case.
Figure 2: CIFAR-100. We run a K-means with K equals to 500 after every epoch, then we examinethe components of each cluster. At start of training, features are almost randomly distributed andeach cluster consists an average of over 30 classes. The entropy of each cluster decreases withtraining time. In experiments, we found that the cross entropy loss of many samples are very close tothe cross entropy in its cluster. To some extent, this means they cannot separate from its neighboursunder current learning rate.
Figure 3: Left: training data ordered from easy to hard every epoch. The network fluctuate greatly.
Figure 4: The curve of accuracyfavorable solutions. And we believe a good solution to this problem can bring at least same gain asbatch normalization.
