Figure 1: Screenshots from (a) the Cleanup game, (b) the Harvest game. The size of the agent-centered observation window is shown in (b). The same size observation was used in all experiments.
Figure 2: (a) Agent Aj adjusts policy π7-(s, a∣φ) using off-policy importance weighted actor-critic(V-Trace) (Espeholt et al., 2018) by sampling from a queue with (possibly stale) trajectories recordedfrom 500 actors acting in parallel arenas. (b) The architecture includes intrinsic and extrinsic valueheads, a policy head, and evolution of the reward network.
Figure 3: (a) Agents assigned and evolved with individual reward networks. (b) Assortative match-making, which preferentially plays cooperators with other cooperators and defectors with other de-fectors. (c) A single reward network is sampled from the population and assigned to all players,while 5 policy networks are sampled and assigned to the 5 players individually. After the episode,policy networks evolve according to individual player returns, while reward networks evolve ac-cording to aggregate returns over all players.
Figure 4:	Total episode rewards, aggregated over players. (a), (b): Comparing retrospective(backward-looking) reward evolution with assortative matchmaking and PBT-OnIy baseline inCleanup (a) and Harvest (b). (c), (d): Comparing prospective (forward-looking) with retrospec-tive (backward-looking) reward evolution in Cleanup (c) and Harvest (d). The black dotted lineindicates performance from Hughes et al. (2018). The shaded region shows standard error of themean, taken over the population of agents.
Figure 5:	Social outcome metrics for Harvest. (a) Sustainability. (b) Equality. (c) Total amount oftagging. The shaded region shows the standard error of the mean.
Figure 6: Distribution of layer 2 weights and biases of evolved retrospective shared reward networkat 1.5 × 108 training steps for (a) Cleanup, and (b) Harvest.
Figure 7: Social outcome metrics for Cleanup. (a) Sustainability. (b) Equality. (c) Total amount oftagging. Shaded region = SOM.
