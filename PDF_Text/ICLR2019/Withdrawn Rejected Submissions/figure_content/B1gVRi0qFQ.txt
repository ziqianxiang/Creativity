Figure 1: Experiment results for creating an abstract MDP for a single task: a) cumulative rewardsover time for several state-action block size threshold settings for the task of stacking three pucks,b) comparison with Wolfe & Barto (2006) in the Blocks World environment, c) the minimal abstractMDP for the task of stacking three pucks learned by our algorithm, d) an example of the input to theneural network. All results are averaged over 20 runs with different random seeds.
Figure 2: Cumulative rewards for task transfer experiments: a) transferring the goal option from thetask of stacking two pucks to stacking three pucks; the baseline is a vanilla deep Q-network andweight sharing means keeping the trained weights from the previous task; b) transferring all optionsfrom the task of stacking three pucks to the task of making two stacks of height two. All results areaveraged over 20 runs with different random seeds.
