Figure 1: Impact of batch augmentation (ResNet44 + cutout, Cifar10). We used the original (red)training regime with B = 64, and compared to batch augmentation with M ∈ {2, 4, 8, 16, 32}creating an effective batch of 64 ∙ Mfigure 2, where using a mere M = 6, BA improved the results of a Wide-ResNet model by morethan 0.5% (a relative 17% decrease in error).
Figure 2: Impact of batch augmentation (Wide-Resnet28-10 + cutout, Cifar10). We used the original(red) training regime and compared to batch augmentation with M = 6Moreover, we managed to achieve high validation accuracy much quicker with batch augmentation.
Figure 3: Impact of batch augmentation (ResNet50, ImageNet). We used the original (red) trainingregime and compared to batch augmentation with M = 4Table 1: Validation accuracy (Top1) results for Cifar, ImageNet models. Bottom: test perpleXityresult on Penn-Tree-Bank (PTB) datasetNetwork	Dataset	Baseline	BatchAugmentResNet44 (He et al., 2016)	Cifar10	93.07%	93.65% (M=10)ResNet44 + cutout	Cifar10	93.7%	95.27% (M=40)VGG + cutout (Simonyan & Zisserman, 2014)	Cifar10	93.82%	95.32% (M=32)Wide-ResNet28-10 + cutout (Zagoruyko, 2016)	Cifar10	96.6%	97.15% (M=6)DARTS (Liu et al., 2018)	Cifar10	97.11%	97.64% (M=10)ResNet44 + cutout	Cifar100	72.97%	74.13% (M=40)VGG + cutout	Cifar100	73.03%	75.5% (M=32)Wide-ResNet28-10 + cutout	Cifar100	79.85%	80.13% (M=10)DenseNet100-12 (Huang et al.)	Cifar100	77.73%	78.8% (M=4)AlexNet (Krizhevsky et al., 2012)	ImageNet	58.25%	62.31% (M=8)MobileNet (Howard et al., 2017)	ImageNet	70.6%	71.4% (M=4)ResNet50 (He et al., 2016)	ImageNet	76.3%	76.8% (M=4)Word-level LSTM (Merity et al., 2017)	PTB	58.8 ppl	58.6 ppl (M=10)5Under review as a conference paper at ICLR 2019
Figure 4: Comparison of gradient L2 norm (ResNet44 + cutout, Cifar10, B = 64) between thebaseline (M = 1) and batch augmentation with M ∈ {2, 4, 8, 16, 32}We suggest a theoretical analysis to explain the advantage of BA over traditional large batch meth-ods. We also show that batch augmentation causes a decrease in gradient variance throughout thetraining, which is then reflected in the gradients’ L2 norm used in each optimization step. This maybe used in the future to search and adapt more suitable training hyper-parameters, that will possiblyallow faster convergence and even better performance.
Figure 5: A comparison betWeen (1) baseline B=64 training (2) our batch augmentation (BA)method With M=10 (3) regime adaptation (RA) With B=640 and 10x more epochs(b) Training (dashed) and validation final errorFigure 6: A comparison of gradient norm betWeen (1) baseline B=64 training (2) our batch augmen-tation (BA) method With M=10 (3) regime adaptation (RA) With B=640 and 10x more epochs. Asexpected, BA exhibits a gradient norm smaller than Baseline, but larger than large-batch training.
Figure 6: A comparison of gradient norm betWeen (1) baseline B=64 training (2) our batch augmen-tation (BA) method With M=10 (3) regime adaptation (RA) With B=640 and 10x more epochs. Asexpected, BA exhibits a gradient norm smaller than Baseline, but larger than large-batch training.
