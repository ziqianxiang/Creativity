Figure 1: Horseâ†’zebra image translation. Our model learns to predict an attention map (b) andtranslates the horse to zebra while keeping the background untouched (c). By comparison, Cycle-GAN Zhu et al. (2017a) significantly alters the appearance of the background together with the horse(d).
Figure 2: Model overview. Our generator G consists of a vanilla generator G0 and an attentionbranch Gattn . We train the model using self-regularization perceptual loss and adversarial loss.
Figure 3: Image translation results of horse to zebra Isola et al. (2016) and comparison with UNITand CycleGAN.
Figure 4: Image translation results on more datasets. From top to bottom: apple to orange Isolaet al. (2016), dog to cat Parkhi et al. (2012), photo to DSLR Isola et al. (2016), yosemite summer towinter Isola et al. (2016).
Figure 5: More image translation results. From left to right: edges to shoes Isola et al. (2016); edgesto handbags Isola et al. (2016); SYNTHIA to cityscape Ros et al. (2016); Cordts et al. (2015). Giventhe source and target domains are globally different, the initial translation and final result are similarwith the attention maps focusing on the entire images.
Figure 7: Effects of using different layers as fea-ture extractors. From left to right: input (a), usingthe first two layers of VGG (b), using the last twolayers of VGG (c) and using the first three layersof VGG (d).
Figure 6: Comparing our results w/o attention with baselines. From top to bottom: dawn to night(SYNTHIA Ros et al. (2016)), non-smile to smile (CelebA Liu et al. (2015)) and photos to Van-goh Isola et al. (2016).
Figure 8: Unsupervised map prediction visualization.
Figure 9: Visualization of image translation fromMNIST (a),(d) to USPS (b),(e) and MNIST-M (c),(f).
Figure 10: Visualization of rendered face to real face translation. Table 4: Unsupervised 3DMM pre-(a)(d): input rendered faces; (b)(e): CycleGAN results; (c)(f): diction results (MSE).
