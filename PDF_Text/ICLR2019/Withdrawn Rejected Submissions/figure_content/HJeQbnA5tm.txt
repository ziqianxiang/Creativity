Figure 1: A noisy information bottleneck (a) limits the mutual information I (D, θ) between the dataD and the learned variables θ by making the data only depend on a corrupted version θ of the latent,with a limited I(θ, θ). It can be applied to both (b) supervised with inputs X(n) and labels Y(n) and(c) generative models with latents Y(n) and datapoints X(n).
Figure 2: Gaussian mean field inference of fixed scale on model parameters with a Gaussian prior(a) can be reinterpreted as MAP inference on a model with injected noise (b): The mean of theoriginal inference model correspond to the parameters of the new generative model.
Figure 3: Classifying CIFAR10 with varyingmodel capacities. Large capacities lead to over-fitting while small capacities drown the signal innoise. Each configuration has been evaluated 5times; mean and standard deviation are displayed.
Figure 4: MNIST test reconstruction with a VAE training on 200 samples for various priors andcapacities.
Figure 5: MNIST test reconstruction with a VAE training on varying dataset sizes, architectures,and model capacities.
Figure 6: Test reconstruction means for binarized fashion MNIST trained on 200 samples withPer-Parameter capacities 5, 2 and 1 bits (from top) compared to the true data (bottom).
Figure 7: Gaussian mean field inference on a latent model (a) with the (β-)VAE objective can bereinterpreted as MAP inference on mean and variance latents (b): The output mean and varianceof (β-)VAE's inference model correspond to the latents of the new generative model. This is aper-datapoint-view with data indices (n) and model parameter dependencies omitted.
Figure 8: Relationship between β and capacity I(μi, (μi, σ2)) of each latent component in flexible-scale Gaussian mean field inference with complexity term scaled by β > 0. Values are given inTable 1.
