Figure 1: Calculating Qψ (o, a) with attention foragent i.
Figure 2: Our environments6Under review as a conference paper at ICLR 2019Table 1: Comparison of various methods for multi-agent RL	Base Algorithm	Attention	Centralized Critic(s)	Number of Critics	Multi-task Learning of Critics	Multi-Agent AdvantageMAAC (OUrS)	-	SaC	X	X	N	X	XMAAC (Uniform) (ours)	SAC	-	uniform	X	N	X	XCOMA*	Actor-Critic (On-Policy)		X	1		X	-MADDPGt	DDPG		X	N		COMA+SAC	SaC		X	1		XMADDPG+SAC	SaC		X	N		X	-DDPG*	-	DDPG	-			N	N/A	N/A 一Centralized Critic(s): each agent’s estimate of Qi takes the actions and observations of the other agents into account. Number of Critics:number of separate networks used for predicting Qi for all N agents. Multi-task Learning of Critics: all agents’ estimates of Qi shareinformation in intermediate layers, benefiting from multi-task learning. Multi-Agent Advantage: cf. SeC 3.2 for details. * (Foerster et al.,2018), t (Lowe et al., 2017), * (LiUiCrap et al., 2016)spaCe and the rover’s observation spaCe), rather than being expliCitly part of the model, and is limitedto a few disCrete signals.
Figure 3: (Left) Average Rewards on Cooperative Treasure Collection. (Right) Average Rewards on Rover-Tower. Our model (MAAC) is competitive in both environments. Error bars are a 95% confidence intervalacross 6 runs.
Figure 4:	Attention ”entropy” for each head over the course of training for the four rovers in theRover-Tower environment1.9e ι.8δ1.6V110000200003000040000——Head 3---Uniform Weightsr∙71.9e ι.8δ1.6----Uniform Weights10000
Figure 5:	Attention ”entropy” for each head over the course of training for two collectors in theTreasure Collection Environmentagent is most relevant to estimating the rover’s expecture future returns, and said agent can changedynamically without affecting the performance of the algorithm.
Figure 6: Attention weights when subjected to different Tower pairings for Rover 1 in Rover-Towerenvironmentapproach matches but does not surpass the performance of MADDPG. It is notable, however, boththat attention does not harm performance in simple cases and that our approach handles continuousaction spaces as well.
