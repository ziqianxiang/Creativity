Figure 2: Plate NotationDir(z∖α)for z ∈ Skotherwise,(3)where α is the concentration parameter of the Dirichlet distribution and B(α) = Q(Prk(ai) is theΓ( i αi)normalization factor. Since the LHS (expectation of log probability) has a closed-formed solution,we rewrite the empirical lower bound on given dataset D as follows:L(θ) =	[ψ(αy) - ψ(α0) - DKL(Dir(z∖α)∖∖p(z))](x,y)∈D(4)where α0 is the sum of concentration parameter α over K dimensions. However, it is in generaldifficult to select a perfect model prior to craft a model posterior which induces an the distributionwith the desiredproperties. Here, we assume the prior distribution is as Dirichlet distribution Dir (α)with concentration parameters α and specifically talk about three intuitive prior functions in Figure 3.
Figure 3: An intuitive explanation of different prior functions.
Figure 4: Concentration and confidence distribution before and after smoothing for CIFAR10 underVGG13 architecture with iSUN as out-of-distribution dataset.
Figure 5: Ablation experiments for VGG13 architecture to investigate the impact of our proposedsmoothing with CIFAR10 as in-distribution dataset and iSUN/LSUN as out-of-distribution dataset.
Figure 6: Impact of different prior distributions. The network architecture is VGG13 with CIFAR10as in-distribution dataset and iSUN/LSUN as out-of-distribution dataset.
Figure 7: Impact of different smoothing functions. The network architecture is VGG13 with in-distribution CIFAR10 dataset and out-of-distribution iSUN/LSUN dataset.
Figure 8: The training loss curve under ResNet18 on CIFAR10 dataset for different η is demon-strated on the left side, the accuracy and out-of-distirbution detection results on the right side.
