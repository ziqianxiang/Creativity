Figure 1: Schematic about gradient flows of different architectures during the backward pass. Eachcube indicates a set of hidden states at a time-step. Shading means the signal strength of gradients - adarker color is associated with more reliable gradients. Arrows indicate gradient flows, and thosethicker ones represent more reliable gradients. Blue filling indicates gradients w.r.t. the supervisedloss. Yellow filling indicates gradients w.r.t. the unsupervised loss. Green filling represents a mixtureof the supervised and unsupervised gradients. White filling means there is no gradient flowing throughthe hidden states. (a) Gradient flows of a typical RNN with a supervised loss. (b) Gradient flows of asemi-supervised RNN (Trinh et al., 2018). (c) Gradient flows of our proposed method, which consistsof a shared and private spaces.
Figure 2: Accuracy (left axis) and Loss (rightaxis) vs Shared Proportion. We visualize the re-sults from Table 4Acc.	L Main	L Aux.	S.%	# Param.
Figure 3: Hidden state evolution during training at different epochs.
