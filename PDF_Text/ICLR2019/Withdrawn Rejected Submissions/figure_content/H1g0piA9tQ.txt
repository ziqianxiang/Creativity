Figure 1: (Left) A success-fail curve. The horizontal axis shows the failure rate (fraction ofexamples for which the model returns a response and the response is incorrect) on adversarial ex-amples. The vertical axis shows the success rate (fraction of examples for which the model returnsa response and the response is correct) on clean examples. Each point on the curve corresponds tousing a different confidence threshold. Dashed lines indicate estimates of upper and lower bounds(section 6.1) for thresholds t < 1 where our attack is not ProVably optimal. The upper and lowerbounds are nearly equal so the two bounds are not separately visible at most points. (Right) Wecompare our new MaxConfidence attack to a traditional attack that maximizes the loss (Madryet al., 2017). The traditional attack is designed to cause a high loss, nota high failure rate in the pres-ence of confidence thresholding. Our MaxConfidence attack performs better or equal at eVerypoint on the curVe, with the most pronounced differences in the parts of the curVe where confidencethresholding is the most effectiVe.
Figure 2: (Left) Success-fail curves for three MNIST models on L∞-constrained adversarial exam-ples. Adversarial training offers a high amount of robustness. The regularized model (appendix B.1)performs well but not as well as adversarial training. Note however, that the regularized model isnot specialized for the L∞ setting. The undefended model performs poorly. The staircase patternfor the undefended model results because few examples lie near the middle of the curve, and whendrawing the plot we connect consecutive points in a staircase pattern in order to show the maximumpossible failure rate. A curve formed with linear connections between points may false imply thatsome (success, failure) points are feasible even though they were not reached in the experiment.
Figure 3: (Left) Success-fail curves for three CIFAR-10 models on L∞-constrained adversarialexamples. Adversarial training offers a high amount of robustness. Unlike on MNIST, the regu-larized model does not perform well. (Right) Success-fail curves for the same CIFAR-10 modelson semantic adversarial examples. Like on MNIST, the baseline, with no defense other than confi-dence thresholding, performs best. Unlike on MNIST, here the regularized and adversarially trainedmodels are somewhat comparable. Each model is better for one part of the tradeoff curve. Indeed,their curves intersect at approximately (success=70%, failure=9%). For applications where achiev-ing success is more important, it is preferable to use the regularized model. For applications whereavoiding failure is more important, it is preferable to use the adversarially trained model.
