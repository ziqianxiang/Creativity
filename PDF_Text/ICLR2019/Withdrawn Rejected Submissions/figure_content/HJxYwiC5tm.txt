Figure 1: Examples of jagged predictions of modern deep convolutional neural networks. Top: Anegligible vertical shift of the object (Kuvasz) results in an abrupt decrease in the network’s predictedscore of the correct class. Middle: A tiny increase in the size of the object (Lotion) produces adramatic decrease in the network’s predicted score of the correct class. Bottom: A very small changein the bear’s posture results in an abrupt decrease in the network’s predicted score of the correct class.
Figure 2: Modern deep convolutional neural networks are sensitive to small image translations. A)Comparison of three networks of various depths in the task of vertical image translation depicted infigure 1. Images (rows) are randomly chosen from the ImageNet dataset (Deng et al., 2009), and aresorted by the network’s prediction sum in a descending order. B) More modern networks have morejagged predictions.
Figure 3: The deeper the network, the less shiftable are the feature maps. A) A vertical shift of a"Kuvasz" dog in the image plane. B) Feature maps from three different network architectures inresponse to the translated Kuvasz image. Layer depth assignments reflect the number of trainableconvolutional layers preceding the selected layer. The last layer is always the last convolutional layerin each network.
Figure 4: Average pooling makes VGG representations approximately shiftable. Plotted are sum-mation of feature maps from the last layer of VGG (spatial dimension 7x7) as an input image isvertically translated in the image plane (top), or rescaled (bottom). Left: the original VGG16 with its2x2 max pooling layers. Right: VGG16 where every 2x2 max pooling layer was replaced by a 6x6average pooling layer. More randomly selected images are shown in the appendix.
Figure 5: Photographer’s biases in the ImageNet’s “Tibetan terrier” category. Left: Example ofthe hand-labeling procedure. Middle: Positions of the middle point between the dog’s eyes. Right:Histogram of distances between the dog’s eyes. Notice the bias in both the object’s position and scale.
Figure 6: The performance of modern CNNs on test images from ImageNet that are embedded in arandom location in a larger image is quite poor (less than 50% accuracy). Human performance is notaffected. Right: An example of a full sized image and the same image resized to 100x100.
Figure 7: We also measure jaggedness using the Mean Absolute Difference (MAD) in the probabilityof the correct response as the image is shifted by a single pixel. Results are similar to those using thejaggedness measure described in the text.
Figure 8: Modern deep convolutional neural networks are sensitive to small image translations(Without image downscaling). A) Example of InceptionResNetV2 sensitivity to very small horizontaltranslations. B) Comparison of three networks of various depths (16, 50, and 134 for VGG16,ResNet50, and InceptionResNetV2 respectively) in the task of horizontal image translation. Rowsare sorted by sum. B) Modern networks have more jagged predictions. Jaggedness is calculated bycounting the number of times the network’s predictions had the correct class in its top-5 and after justone pixel shift it moved outside of the top-5 (and also the opposite transition from non-top-5 to top5).
Figure 9: Modern deep convolutional neural networks are sensitive to small image rescalings A)Comparison of three networks of various depths (16, 50, and 134 for VGG16, ResNet50, andInceptionResNetV2 respectively) in the task of image rescaling. Rows are sorted by their sum. B)Quantification of the jagged behaviour of the deep neural networks.
Figure 10: Nonshiftability as a function of depth in the three networks. Nonshiftability is definedas the number of times the global sum of a feature map changes by more than 20% of the mean asthe input is translated. We only consider feature maps where the maximum response was above athreshold. According to our analysis, this measure should be zero if the representation is shiftable.
Figure 11: Another randomly selected image. Plotted are summation of feature maps from the lastlayer of VGG (spatial dimension 7x7) as the input is vertically translated in the image plane (top), orrescaled (bottom). Left: the original VGG16 with its 2x2 max pooling layers. Right: VGG16 whereevery 2x2 max pooling layer was replaced by a 6x6 average pooling layer.
Figure 12: Another randomly selected image. Plotted are summation of feature maps from the lastlayer of VGG (spatial dimension 7x7) as the input is vertically translated in the image plane (top), orrescaled (bottom). Left: the original VGG16 with its 2x2 max pooling layers. Right: VGG16 whereevery 2x2 max pooling layer was replaced by a 6x6 average pooling layer.
Figure 13: Another randomly selected image. Plotted are summation of feature maps from the lastlayer of VGG (spatial dimension 7x7) as the input is vertically translated in the image plane (top), orrescaled (bottom). Left: the original VGG16 with its 2x2 max pooling layers. Right: VGG16 whereevery 2x2 max pooling layer was replaced by a 6x6 average pooling layer.
