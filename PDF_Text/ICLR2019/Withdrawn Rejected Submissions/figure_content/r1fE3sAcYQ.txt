Figure 1: (Left) Two models to be trained (A, B), where A’s parameters are in green and B’s inpurple, and B shares some parameters with A (indicated in green during phase 2). We first trainA to convergence and then train B. (Right) Accuracy of model A as the training of B progresses.
Figure 2: From strict to loose convergence. We conduct experiments on MNIST with models Aand B with shared parameters, and report the accuracy of Model A before training Model B (baseline,green) and the accuracy of Models A and B while training Model B with (orange) or without (blue)WPL. In (a) we show the results for strict convergence: A is initially trained to convergence. Wethen relax this assumption and train A to around 55% (b), 43% (c), and 38% (d) of its optimalaccuracy. We see that WPL is highly effective when A is trained to at least 40% of optimality;below, the Fisher information becomes too inaccurate to provide reliable importance weights. ThusWPL helps to reduce multi-model forgetting, even when the weights are not optimal. WPL reducedforgetting by up to 99.99% for (a) and (b), and by up to 2% for (c).
Figure 3: Error difference during neural architecture search. For each architecture, we computethe RNN error differences err2 - err1, where err1 is the error right after training this architectureand err2 the one after all architectures are trained in the current epoch. We plot (a) the meandifference over all sampled models, (b) the mean difference over the 5 models with lowest err1 , and(c) the max difference over all models. The plots show that WPL reduces multi-model forgetting;the error differences are much closer to 0. Quantitatively, the forgetting reduction can be up to 95%for (a), 59% for (b)and 51% for (c). In (d), we plot the average reward of the sampled architecturesas a function of training iterations. Although WPL initially leads to lower rewards, due to a largeweight α in equation (8), by reducing the forgetting it later allows the controller to sample betterarchitectures, as indicated by the higher reward in the second half.
Figure 4: Comparison of different output dropout rates for NAO. We plot the mean validationperplexity while searching the best architecture (top) and the best 5 model’s error differences (bot-tom) for four different dropout rates. Note that path dropping in NAO prevents learning shortlyafter model initialization. At all the dropout rates, our WPL achieves lower error differences, i.e., itreduces multi-model forgetting, as well as speeds up training.
Figure 5: Error differences when searching for CNN architectures. Quantitatively, the multi-model forgetting effect is reduced by up to 99% for (a), 96% for (b), and 98% for (c).
Figure 6: Best architectures found for RNN and CNN. We display the best architecture found byENAS+WPL, in (a) for the RNN cell, and in (b) and (c) for the CNN normal and reduction cells.
