Figure 1: Equilibrium Normalization ensures the contribution from positive and negative kernelweights to the output remains of the same total magnitude, both compared to each other and betweenepochs. In the case shown of a 3 × 3 kernel (cyclic convolution) against a 3 × 3 image with padding1, this magnitude is Widthout ∙ height°ut = 9.0.
Figure 2: Indications of overfitting, as test lossstarts to increase significantly after the first learn-ing rate decrease.
Figure 3: Shorter duration CIFAR10 training(WRN network)6.3	Street View House NumbersThe SVHN+EXTRA dataset (Netzer et al., 2011) is much larger than CIFAR-10/100 (73,257 +531,131 training instances), so we trained across 2 GPUs, using 2× larger mini-batches (size 256)so as to keep the batch statistics noise (which are computed on a per-gpu basis) the same. Otherhyper-parameters were also kept the same, with the exception that we trained for fewer epochs (withLR reductions moved to epochs 80 and 120). Figure 4c shows that EquiNorm achieves essentiallythe same generalization performance as BatchNorm. On this problem GroupNorm appears inferior,although this may be due to the default group-size of 32 being suboptimal here.
Figure 4: Test set accuracy and loss. Median of 10 runs shown with interquartile regions overlaidwith the exception of the ImageNet plot which uses 3 runs.
