Figure 1: Basic structure of proposed self-driving system. The components which lie in the redbounding box are refered as parts of perception module. The components which lie in the bluebounding box forms the driving module. In order to see the limiation of RGB image, only currentRGB image and driving guidance are used as inputs seperatedly for perception module and drivingmodule.
Figure 2: Screenshots of captured single RGB image and our proposal’s inference results whichconsists of predicted segmentation map and depth map during test in untrained town under untrainedweather. Obviously from the inferenced segmentation and depth maps we get information that thedriving model knows a car is passing by the left side.
Figure 3: Failed straight task in untrained town under untrained soft rain sunset weather. Frompredicted segmentation and depth maps, we know that the driving model thinks that there is a car infront of it but actually there isn’t any car in front of it.
Figure 4: One perceptive of loss surface by linear interpolation of original proposed method weightsand fine-tune method weights. Blue line refers to test loss, red line refers to train loss. From thevisualization perspective it’s possibly that finetune method weights are stuck in a flat surface whilethe original proposed weights sucessfully find a local minimum.
