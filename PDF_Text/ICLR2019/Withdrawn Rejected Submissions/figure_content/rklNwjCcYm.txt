Figure 1: Computation of i-th attention head of 4th word tokenTable 1: Dataset StatisticsSPlit	#TokenS	#EntitieS	Entity type: names	Entity type: valuesTrain Validate Test	1,088,503 147,724 152,728	81,828 11,066 11,257	PERSON, NORP, FAC, ORG, GPE, LOC, PRODUCT, EVENT, WORK_OF_ART, LAW, LANGUAGE	DATE, TIME, PERCENT, MONEY, QUANTITY, ORDINAL, CARDINALTotal	1,388,955	104,151		αi = σSpecifically, given the hidden features H of a whole sequence, we project each hidden state todifferent subspaces, depending on whether it is used as the {q}uery vector to consult other hiddenstates for each word token, the {k}ey vector to compute its dot-similarities with incoming queries, orthe {v}alue vector to be weighted and actually convey information to the querying token. Moreover,as different aspects of a task can call for different attention, multiple “attentions” running in parallelare used, i.e., multi-head attention (Vaswani et al., 2017).
Figure 2: Attention heat maps for “...a meeting at the White house Saturday...
Figure 3: Attention heat maps of “...Dutch into English...” and “...Chinese and English...
