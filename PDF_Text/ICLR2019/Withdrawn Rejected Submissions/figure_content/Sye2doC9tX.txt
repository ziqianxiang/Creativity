Figure 1: The e-Policy means exploration policy, which is used for interacting with the environmentand get exploration experience into data pool. Then the framework samples data from experiencedata pool to update the benchmark policy (b-policy). According to the sample data, we can getthe real reward which is given by the environment. On the other hand, the benchmark policy canalso provide a predict reward. The difference between real reward and predict reward means theuncertainty of benchmark policy in reward space. Reward uncertainty is used to guide explorationstrategy updating.
Figure 2: Grid world environment and experimental results. (a) shows the environment setting ofgrid world, reinforcement learning agent start from the yellow point at the lower left corner. The redand green grids represent the termination grid, where the red grid indicates that the reward is negativeand the green grid indicates that the reward is positive.(b) shows the three optimal paths from thestarting point to the ending states. (c) shows the rewards of eDQN and DQN in the environment asthe steps increases. (d) shows the change in the loss value of each policy as the number of stepsincreases, where bDQN means the benchmark policy in the proposed frame work.
Figure 3: State distribution of DDQN and eDQN at step of 3000-5000-7000-9000. The brighter thecolor of the grid, the more times the grid is accessed. Red represents a large number of visits. Cyanand green indicate moderate number of visits. Blue indicates a small number of visits.
Figure 4: (a) shows the room environment of grid world, the black grid is an obstacle and cannotpass, the dark green grid reward is 1, and the light green grid reward is 10. (b) shows the rewardcurves of eDQN and DQN (c) shows the loss curve of DDQN, bDQN, eDQN(c)Figure 2 (c) shows the reward comparison of eDQN and DDQN, as can be seen from Figure (c),both eDQN and DQN quickly converge to the optimal solution, and eDQN converges faster thanDDQN. Figure 2 (d) shows the change of loss in each method as the number of iterations increases,where bDQN is the benchmark DQN. The loss of bDQN is the intrinsic reward of eDQN. It can beseen from (d) that the bDQN loss is large in the early stage and the bDQN loss becomes smallerin the later stage. In the current reward sparse grid world, eDQN is affected by the uncertainty ofQ-value in the early stage. When the iteration steps are greater than 7k, the loss of bDQN decreasesand tends to be stable. At this point, eDQN is less affected by uncertainty. In addition, figure (d)shows that the loss convergence of eDQN and bDQN is faster than DDQN, which verifies the effectof Figure c on the other hand.
Figure 5: Performance of the method with the number of samples; (a), (b), (c), (d) are performancescomparison of DDQN, noisy DQN, eDQN in terms of the average reward on the SpaceInvaders task,BreakOut task, Enduro task and Pong task, respectively.
