Figure 1: An eleven layer MLP network composed of 10 identical layers (each containing 784 ReLUneurons (Nair & Hinton, 2010)) is applied on a reduced MNIST dataset (LeCun et al., 1998), suchthat training any of the 10 layers in isolation is sufficient to get 100% training accuracy. But willtraining of each layer result in the same test accuracy? This figure shows the test accuracy in functionof the index of the trained layer (in forward pass ordering), after averaging over 10 experiments. Inthis specific example, the test accuracy mostly degrades with the depth of the trained layer, with afinal difference of nearly 20%. The generalization ability induced by a layer’s training is thus heavilyaffected by how the other layers transform the input and feedback signals it receives (everything elsebeing equal for each layer index). Section A.1 (in Supp. Mat.) provides further discussion of thisexperiment.
Figure 2: Analysis of the generalization ability induced by different layer rotation rate configurations(using Layca for training) on the three first tasks of Table 1. The configurations are parametrizedby α, that controls which layers are prioritized (first layers for α < 0, last layers for α > 0, or noprioritization for α = 0), and ρ(0), the initial global learning rate value shared by all layers. (a)Test accuracy in function of α and ρ(0). Two rules of thumb emerge: layer rotation rates should beuniform across layers (i.e. α = 0) and be as high as training allows it (i.e. high ρ(0) values). (b)Layer-wise angle deviation curves (cfr. Section 3.1) generated by different configurations, and theiraccompanying test accuracy (η). ∆η is computed with respect to the high and uniform layer rotationrate configuration (last column), which corresponds to α = 0 and ρ(0) = 3-3 for the three tasks.
Figure 3: Loss curves obtained for different α and ρ(0) values on the tiny-CNN task (for the twoother tasks, see Supp. Mat.), using Layca for training. The more uniform or the higher the layerrotation rates, the higher the plateaus in which the loss gets stuck into. The sudden drop at epoch 70corresponds to a reduction of the global learning rate by a factor 5.
Figure 4: Layer-wise angle deviation curves and the corresponding test accuracies generated bySGD without (1st line) or with (2nd line) weight decay. Colour code, axes and ∆η computation arethe same as in Figure 2b.6Despite extensive learning rate tuning, SGD without weight decay inducestest performances that are significantly below Layca. These results are coherent with our rules ofthumb, as SGD is not able to induce high and uniform layer rotation rates (cfr. 5th column of Figure2b). Surprisingly, weight decay solves SGD’s problems, leading to high and uniform layer rotationrates and test accuracies that are on par with Layca.
Figure 5: Layer-wise angle deviation curves and the corresponding test accuracies generated byadaptive gradient methods (RMSProp, Adam, Adagrad, RMSProp+L2 and Adam+L2 respectivelyfor each task/column) after extensive learning rate tuning. Colour code, axes and ∆η computationare the same as in Figure 2b. Our rules of thumb still apply: the overall worse generalizationability compared to Layca corresponds to low and/or non-uniform layer rotation rate configurations.
Figure 6: Training and test curves of adaptive gradient methods and their non-adaptive equiva-lents7when Layca is applied on the updates of each method to fix the layer rotation rate configura-tion. The curves become indistinguishable, suggesting that adaptive gradient methods’ impact ontraining speed and generalization is only due to their influence on layer rotation rates, and thus totheir layer-level adaptation of the learning rate.
Figure 7: Consistency comparison of layer rotation rates and the norm of updates (another sensiblemetric of layer-level training speed). Using Layca (left) or block-normalized SGD (center) to controlboth metrics, we test if the relation between global initial learning rates and train/test accuracyremains consistent when the initial weights of all layers are rescaled by a constant factor f . Thedifference is striking: block-normalized SGD does not demonstrate Layca’s consistency at all. SGDis also added for comparison (right). Dotted lines correspond to train accuracy, full lines to testaccuracy.
Figure 8: left: Studying the impact of initialization on the MNIST experiment of Figure 1. Weobserve that orthogonal initialization, known for its faithful propagation of signals, increases thetest accuracy of every layer and slightly reduces the generalization gap between the first and lastlayers. right: Supplementary result suggesting that the capacity of layers to improve their ownfeedback signal could be a key asset enabling the first layers’ superior generalization performancein Figure 1. Indeed, this figure shows how the Silhouette score of the first layer’s feedback withrespect to the classes/targets increases even when only the first layer is trained.
Figure 9: Layer-wise angle deviation curves and the corresponding test accuracies generated byLARS with α = 0 and ρ(0) = 3-3. Colour code, axes and ∆η computation are the same as inFigure 2b. Although not performing operations 1 and 4 of Algorithm 2, LARS seems to control layerrotation rates as well as Layca. Indeed, the layer-wise angle deviation curves are indistinguishablefrom the ones in the 5th column of Figure 2b, and the test accuracies are nearly identical.
Figure 10: Loss curves of C10-CNN1 and C100-resnet for different α and ρ(0) values. The resultsconfirm the observations made on tiny-CNN, and extend the analysis to negative α values.
Figure 11:	Adam’s parameter-wise estimates of the second raw moment (uncentered variance) of thegradient during training on C10-CNN1, described for each layer separately through the 10th, 50thand 90th percentiles (represented by the lower bar, the bullet point, and the upper bar respectivelyfor each layer index). The results provide supplementary evidence that the parameter-level statisticsused by adaptive gradient methods vary mostly between layers and negligibly inside layers.
Figure 12:	Visualization of the prioritization schemes as parametrized by α (cfr. Section 4). Thecolours of the lines represent the absolute value of α. Illustration is separated for prioritization ofthe first layers (negative α values) and of the last layers (positive α values). The layer-wise learningrate multipliers (y-axis) depend on the layer’s location in the network (x-axis), which is representedby the layer index l (in forward pass ordering) divided by the number of layers L.
