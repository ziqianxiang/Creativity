Figure 1: The whole search space can be representedby a completely connected DAG. Here node 1 and 6are the input and output node, respectively. The dashedline and dashed circle represent that the correspondingconnections and nodes are removed. For example, theinitial output of node 5 can be calculated by h(5) =O(5) (Pj4=1 h(j)), while it becomes h(5) = O(5) (h(2) +h(4)) for the pruned sub-graph.
Figure 2: An example of search block, which has two levels with two operations: (a) The completelyconnected block. (b) In the search process, we jointly optimize the weights of neural network and theÎ» associated with each edge. (c) The final model after removing useless connections and operations.
Figure 4: Block structures learned on different datasets.
Figure 5: Performance of adaptive FLOPs techniques.
Figure 6: Performance of adaptiveMAC techniqueIt is clear that the networks searched with adaptive FLOPstechnique are consistently better than the ones without underthe same total FLOPs or parameters.
