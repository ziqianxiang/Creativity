Figure 1: Conventional Winograd convolution and sparse Winograd convolution (m = 4, n = 3).
Figure 2: Overview of the SPatial-Winograd pruning.
Figure 3: Pruning of (a) VGG-nagadomi on CIFAR-10 (b) ConvPool-CNN-C on CIFAR-100 withuniform sparsity across layers.
Figure 4: Pruning of ResNet-18 on ImageNet with uniform sparsity across the pruned layers.
Figure 5: Effectiveness of employing importance factor matrix F in (a) Winograd pruning and (b)Winograd retraining.
Figure 6: Accuracy loss of ResNet-18 when incurring 60% Winograd-domain sparsity into differentlayers. Spatial structured pruning is applied with no retraining.
Figure 8: Sparsity of different locations ofWinograd-domain weights for: (a) filterswith 20 weights pruned; (b) filters with atleast one weight remaining. Darker locationshave higher sparsities.
Figure 7: Distribution of 2D filters with dif-ferent numbers of weights pruned.
