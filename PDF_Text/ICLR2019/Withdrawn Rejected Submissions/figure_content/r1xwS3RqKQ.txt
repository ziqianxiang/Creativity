Figure 1: A sample set of DifEN activation functions and their derivatives. The bold blue line is theactivation fucntion, and the orange solid line is its derivative with respect to t. The dashed lines areits derivatives with respect to a, b and c. First and second on the top row from left are ReLU andReQU. The bump in the derivative of ReLU is an artifact of approximating Diracâ€™s delta.
Figure 2: Left: The solutions of ay00 + by0 + cy = u(t) lie on a manifold of functions. Right:Every point on this manifold can be equivalently represented as a point on a 5-dimentional space of{a, b, c, c1, c2} (three shown in the cartoon). The red arrow shows a path an initialized function takesto be gradually transformed to a different one.
Figure 3: Learning the sine function. The differential equation neuron learns the function significant;ybetter than larger baselines.
Figure 4: Fitting an arbitrary mathematical function.
Figure 5: The spectra of functions generated by varying one of a, b, c, and fixing the other two withc1 = c2 = 0.
Figure 6: Diabetes Regression Model Convergence ComparisonFigure 6	above shows that the differential equation network achieves performance on par withsignificantly larger fixed activation networks. We see that the fixed activation networks do not surpassthe single neuron DifEN performance untill they 8 or more neurons.
