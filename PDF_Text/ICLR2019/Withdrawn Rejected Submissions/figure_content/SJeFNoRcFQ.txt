Figure 1: Full and zoomed-in ESD for LeNet5 (Layer FC1) and AlexNet (Layer FC2). Overlaid (inred) are fits of the MP distribution (which fit the bulk very well for LeNet5 but not well for AlexNet).
Figure 2: Taxonomy of trained models. Starting off with an initial random or Random-like model(2(a)), training can lead to a Bulk+ S pikes model (2(c)), with data-dependent spikes on top of arandom-like bulk. Depending on the network size and architecture, properties of training data, etc.,additional training can lead to a Heavy-Tailed model (2(e)), a high-quality model with long-range correlations. An intermediate Bleeding-out model (2(b)), where spikes start to pull outfrom the bulk, and an intermediate Bulk-decay model (2(d)), where correlations start to degradethe separation between the bulk and spikes, leading to a decay of the bulk, are also possible. Inextreme cases, a severely over-regularized model (2(f)) is possible.
Figure 3: Baseline ESD for Layer FC1 of MiniAlexNet, during training.
Figure 4:	Varying Batch Size. Stable Rank and MP Softrank for FC1 (4(a)) and FC2 (4(b)); andTraining and Test Accuracies (4(c)) versus Batch Size for MiniAlexNet.
Figure 5:	Varying Batch Size. ESD for Layer FC1 of MiniAlexNet, with MP fit (in red), for anensemble of 10 runs, for Batch Size ranging from 500 down to 2. Smaller batch size leads to moreimplicitly self-regularized models. We exhibit all 5 of the main phases of training by varying onlythe batch size.
Figure 1: The behavior of two complexity measures, the Matrix Entropy S(W) and the StableRank Rs(W), for Layers FC1 and FC2, during Backprop training, for MLP3. Both measuresdisplay a transition during Backprop training.
Figure 2: Scree plots for initial and final configurations for Layers FC1 and FC2, during Backproptraining, for MLP3.
Figure 3: Histograms of the Singular Values νi and associated Eigenvalues λi = νi2 , comparinginitial Wl0 and final Wl weight matrices (which are N × M , with N = M) for Layer FC2 of aMLP3 trained on CIFAR10.
Figure 4: Marchenko-Pastur (MP) distributions, see Eqns. (8) and (9), as the aspect ratio Q andvariance parameter σ are modified.
Figure 5: The log-log histogram plots of the ESD for three Heavy-Tailed random matrices Mwith same aspect ratio Q = 3, with μ = 1.0, 3.0, 5.0, corresponding to the three Heavy-TailedUniversality classes (0 <μ< 2 vs 2 <μ< 4 and 4 < μ) described in Table 3.
Figure 6: Dependence of α (the fitted PL parameter) on μ (the hypothesized limiting PL Param-eter). In (6(a)), the PL exponent α is fit, using the CSN estimator, for the ESD ρemp(λ) for arandom, rectangular Heavy-Tailed matrix W(μ) (Q = 2,M = 1000), with elements drawn froma Pareto distribution p(x)〜 x-1-μ. For 0 < μ < 2, finite-size effects are modest, and the ESDfollows the theoretical prediction pemp(λ)〜 λ-1-μ/2. For 2 < μ < 4, the ESD still shows roughlylinear behavior, but with significant finite-size effects, giving the more general phenomenologicalrelation pemp(λ)〜 λ-aμ+b. For 4 ‹ μ, the CSN method is known to fail to perform well. In(6(b)) and (6(c)), plots are shown for varying Q, with M and N fixed, respectively.
Figure 7: Full and zoomed-in ESD for LeNet5, Layer FC1. Overlaid (in red) are gross fits of theMP distribution (which fit the bulk of the ESD very well).
Figure 8: ESD for Layer FC1 of AlexNet. Overlaid (in red) are gross fits of the MP distribution.
Figure 9: ESD for Layer FC2 of AlexNet. Overlaid (in red) are gross fits of the MP distribution.
Figure 10: ESD for Layer FC3 of AlexNet. Overlaid (in red) are gross fits of the MP distribution.
Figure 11: ESD for Layers L226 and L302 in InceptionV3, as distributed with pyTorch. Overlaid(in red) are gross fits of the MP distribution (neither of which which fit the ESD well).
Figure 12: Distribution of power law exponents α for linear layers in pre-trained models trainedon ImageNet, available in pyTorch, and for those NLP models, available in AllenNLP.
Figure 13:	Distribution of minimum singular values νmin for linear layers in pre-trained modelstrained on ImageNet, available in pyTorch, and for those NLP models, available in AllenNLP.
Figure 14:	Taxonomy of trained models. Starting off with an initial random or Random-likemodel (14(a)), training can lead to a Bulk+Spikes model (14(c)), with data-dependent spikes ontop of a random-like bulk. Depending on the network size and architecture, properties of trainingdata, etc., additional training can lead to a Heavy-Tailed model (14(e)), a high-quality modelwith long-range correlations. An intermediate Bleeding-out model (14(b)), where spikes startto pull out from the bulk, and an intermediate Bulk-decay model (14(d)), where correlationsstart to degrade the separation between the bulk and spikes, leading to a decay of the bulk, arealso possible. In extreme cases, a severely over-regularized model (14(f)) is possible.
Figure 15:	Pictorial illustration of the 5 Phases of Training and the MP Soft Rankappropriate to model strongly-correlated systems. In these phases, the strongly-correlated modelis still regularized, but in a very non-traditional way. The final phase, the Rank-collapse phase,is a degenerate case that is a prediction of the theory.
Figure 16: log-log histogram plots of the ESD for WFC3 of pre-trained AlexNet (blue) and aHeavy-Tailed random matrix M with same aspect ratio and μ = 2.5 (red)5.6	Rank-collapseIn addition to the 5 main phases, based on MP theory we also expect the existence of an additional“+1” phase, which we call the Rank-collapse Phase, and which is illustrated in Figure 14(f).
Figure 17: Pictorial illustration of MiniAlexNet.
Figure 18: Entropies, Stable RanksGMFH *uect史 q£6Mini-AIexNet:200150ιoo500	25	50	75	100Epoch (e)(c) Training, Test AccuraciesMini-AIexNet Baseline(b) Stable Ranks, and Training and Test Accuracies per Epoch for MiniAlexNet.
Figure 19: Baseline ESD for Layer FC1 of MiniAlexNet, during training.
Figure 20: Baseline ESD for Layer FC2 of MiniAlexNet, during training.
Figure 21: Eigenvector localization metrics for the FC1 layer of MiniAlexNet, for an ensemble of10 runs. Batch size 16, and no weight regularization. Comparison of Bulk and Spike.
Figure 22: ESD for Layer FC1 of MiniAlexNet, with MP fit (in red): initial (0) epoch, single run(in 22(a))); final epoch, single run (in 22(b))); final epoch, ensemble of 10 runs (in 22(c))). Batchsize 16, and no weight regularization. To get a good MP fit, final epochs have 9 spikes removedfrom the bulk (but see Figure 24).
Figure 23: ESD for Layer FC2 of MiniAlexNet, with MP fit (in red): initial (0) epoch, single run(in 23(a)); final epoch, single run (in 23(b)); final epoch, ensemble of 10 runs (in 23(c)). Batchsize 16, and no weight regularization. To get a good MP fit, final epochs have 9 spikes removedfrom the bulk.
Figure 24:	Illustration of fitting λ+. ESD for Layer FC1 of MiniAlexNet, with MP fit (in red):averaged over 10 runs. Batch size 16, and no weight regularization. Compare 24(a) with Fig-ure 22(a), which shows the ESD for a single run. Also, compare 24(b), where λ+ is fit by removing18 eigenvectors, with Figure 22(c), which shows “Bulk + 9 Spikes”.
Figure 25:	Entropy, Stable Rank, and Training and Test Accuracies of last two layers forMiniAlexNet, as a function of the number of epochs of training, without and with explicit L2norm weight regularization.
Figure 26:	Entropy, Stable Rank, and Training and Test Accuracies of last two layers forMiniAlexNet, as a function of the number of epochs of training, without and with explicit Dropout.
Figure 27: ESD for layers FC1 and FC2 of MiniAlexNet, with explicit Dropout. (Compare withFigure 22 (for FC1) and Figure 23 (for FC2).)Figure 28:	ESD for layers FC1 and FC2 of MiniAlexNet, with explicit L2 norm weight regular-ization. (Compare with Figure 22 (for FC1) and Figure 23 (for FC2).)is the following: this leads to a smaller bulk MP variance parameter σm2 p , and thus smaller valuesfor λ+ , when there is a more prominent spike. See Figure 28 for similar results for the ESD forlayers FC1 and FC2 of MiniAlexNet, with explicit L2 norm weight regularization.
Figure 28:	ESD for layers FC1 and FC2 of MiniAlexNet, with explicit L2 norm weight regular-ization. (Compare with Figure 22 (for FC1) and Figure 23 (for FC2).)is the following: this leads to a smaller bulk MP variance parameter σm2 p , and thus smaller valuesfor λ+ , when there is a more prominent spike. See Figure 28 for similar results for the ESD forlayers FC1 and FC2 of MiniAlexNet, with explicit L2 norm weight regularization.
Figure 29:	Varying Batch Size. Stable Rank and MP Softrank for FC1 (29(a)) and FC2 (29(b));and Training and Test Accuracies (29(c)) versus Batch Size for MiniAlexNet.
Figure 30:	Varying Batch Size. ESD for Layer FC1 of MiniAlexNet, with MP fit (in red), foran ensemble of 10 runs, for Batch Size ranging from 500 down to 2. (Compare with Figure 22as a reference.) Smaller batch size leads to more implicitly self-regularized models. For FC1, weexhibit all 5 of the main phases of training by varying only the batch size.
Figure 31: Varying Batch Size. ESD for Layer FC2 of MiniAlexNet, with MP fit (in red), foran ensemble of 10 runs, for Batch Size ranging from 500 down to 2. (Compare with Figure 23as a reference.) Smaller batch size leads to more implicitly self-regularized models. For FC2, weexhibit 4 of the 5 of the main phases of training by varying only the batch size.
