Figure 1: Architecture and graphical model of implicit autoencoders.
Figure 2: MNIST dataset. (a) Original images. (b) Deterministic reconstructions with 20D globalvector. (c) Stochastic reconstructions with 10D global and 100D local vector. (d) Stochasticreconstructions with 5D global and 100D local vector.
Figure 3: SVHN dataset. (a) Original images. (b)Deterministic reconstructions with 150D globalvector. (c) Stochastic reconstructions with 75Dglobal and 1000D local vector.
Figure 4: CelebA dataset. (a) Original images. (b)Deterministic reconstructions with 150D globalvector. (c) Stochastic reconstructions with 50Dglobal and 1000D local vector.
Figure 5:	Learning the mixture of Gaussian distribution by the standard GAN and the IAE.
Figure 6:	Disentangling the content and style of the MNIST digits in an unsupervised fashion withimplicit autoencoders. Each column shows samples of the model from one of the learned clusters.
Figure 7: Architecture and graphical model of flipped implicit autoencoders.
Figure 8: InfoGAN on a toy dataset. (a) Trueposterior. (b) Factorized Gaussian variational pos-terior.
Figure 9: Flipped implicit autoencoder on a toydataset. (a) True posterior. (b) Implicit variationalposterior.
Figure 10: Reconstructions of the flipped implicit autoencoder on the MNIST dataset. Top rowshows the MNIST test images, and bottom row shows the deterministic reconstructions.
