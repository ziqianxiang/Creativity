Figure 1:	Proposed approach for planning a sentence structure with discrete codes. A target sentenceis parsed to obtain its structural tags. Then, based on the tags and source sentence, the coding modelproduces multiple codes containing information about the structural choice. Finally, an NMT modelis trained based on the source and target sentences with the structural codes as prefixes.
Figure 2:	Architecture of code learning model. Discretization bottleneck is indicated by dashedlines.
Figure 3: Visualization of code distribution in IWSLT14 De-En task and ASPEC Ja-En taskIn Fig. 3, we report the distribution of the structural codes learned for all target sentences in bothtasks. In each plot, the Y-axis indicates the total number of sentences that correspond to a specificstructural code. In general, the coding model is able to distribute available discrete codes to differentsentences in the training data. However, we can still observe that four codes in IWSLT14 and onecode in ASPEC task are not assigned to any sentence. Thus, there is room for further improvingthe coding model to exploit the full capacity of the codes, such as using an improved discretizationmethod (Kaiser et al., 2018).
