Figure 1: Three popular distillation methods. From left to right, traditional KD method in Geoffreyet al. (2015), using softmax as knowledge, the student network mimics teacherâ€™s softmax and mini-mize the loss on soft target. Middle one, mentioned in Zagoruyko & Komodakis (2017), named asattention transfer, an additional regularizer has been applied known as attention map, student needsto learn the attention map and soft target. The right part is our neuron manifold transfer, where wetake neuron manifold as knowledge, and it reduces the computational and space cost.
Figure 2: Neuron Manifold of Hinton-1200. White dots are feature points. Blue and red line formthe neuron manifold.
Figure 3: Different knowledge transfer methods on CIFAR10 and CIFAR100. Best view in color.
Figure 4: CIFAR10 Accuracy on validation set within 120 epochs, epoch window size = 5, total =24 windowsTo better understanding, we illustrate the Neuron Manifold Map as Figure 2. We extract out thefist layer of Hinton-1200, and normalize all value in between 0 to 1. We use Moving Least Squaresmethod to approximate the true manifold of such layer. All white dots are feature points and formhyper-ball and the big black dots highlighted indicate the selected representative feature points whichcan best describe both feature properties and geometric properties(relative position and distancepreserved).
