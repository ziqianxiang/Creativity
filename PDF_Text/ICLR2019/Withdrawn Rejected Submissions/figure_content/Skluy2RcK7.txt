Figure 1: Examples of selectivity measures used. (a) Jitterplot of unit 113 in an RNN under thesuperposition constraint selective the letter ‘j’ (b) Jitterplot of a non-selective unit 160 found whenRNN trained on words one-at-a-time; from Bowers et al. (2016). (c) Activation maximization (AM)image of a unit in conv5 of AlexNet that looks like a lighthouse; from Nguyen et al. (2016). (d)Highest activation images for a ‘lamp’ detector with 84% precision in layer pool5 of AlexNet; fromZhou et al. (2015).
Figure 2: Selectivity measures across different layers of AlexNet. Left: top-class selectivity. Middle:precision 100 (the percentage of the top 100 images which are members of the top class). Right:Class-conditional mean activity selectivity (CCMAS).
Figure 3: Data for unit fc6.1199. (a) activation jitterplot black squares: Monarch butterfly images;grey circles: all other classes. (b) histogram of activations of Monarch butterflies, c-e exampleImageNet images With activations of 0, the mean, and the maximum of the range. Unit fc6.1199has a precision of 95% over the top 100 images (98.3% over the top 60) and is thus classified as abutterfly detector, yet there are Monarch butterfly images covering the Whole range of values, With72 images (5.8% of the total) having an activation of 0.0.
Figure 4: Example data from the fc8 and prob layers. (a) jitterplot activations for unit fc8.11 that hasa top-class selectivity of 8.4%. (b) jitterplot activations for prob.11 (i.e. post-softmax) that has top-class selectivity of 95.2%. Activations for the ‘ground truth’ class ‘goldfinch’ are shown as blacksquares, all other classes are shown as colored circles.
Figure 5: Example of where the CCMAS does not match intuitive understandings of selectivity.
Figure 6: Example AM images that were either judged by all participants to contain objects (a-c) or judged by all participants to be uninpterpretable as objects (d-e). The human judgement forconv5.183 was ‘dogs’ and the top-class was ‘flat-coated retriever’. For fc6.319 subjects reported‘green peppers’ or ‘apples’ (all classified as the same broad class in our analysis), and the CCMASand top-class was ’Granny Smith apples’. For fc8.969 humans suggested ‘beverage’ or ‘drink’:ground truth class for this unit is ‘eggnog’. The ground-truth for fc8.865 is‘toy-store’.
Figure A1: Example screen from the identification task shown to participants as part of the instruc-tions. The images included on this practice trial are ImageNet2010 images, not AM images.
Figure A2: The distribution of activations on unit fc6.1199 for all images (left). 82.8% of activationsare zero across all classes, only 5.8% of the butterfly class are zero. Unit fc6.1199 is a butterflydetector under Zhou et la’s precision measure. Bins are 1.0 wide.
Figure A3: Fitting the non-zero activations for all classes (red) and the maximum activation class(black). For the superset of all the classes, the distribution is well-described by exponential-derivedfits, normal-derived fits are bad. For the maximum activating class (butterfly), the distribution has amean and can be well-described as a normal distribution.
Figure A4: Example of where the CCMAS does not match intuitive understandings of selectivity.
Figure A5: Example activation maximisation images for unit conv5.65. These images were judgedby humans to not contain any interpretable objects in common (although the reader may agree thatthere are some shape and colour similarities in the images).
Figure A6: Example activation maximisation images for unit conv5.183. These images were judgedby humans to contain some interpretable images, in this case, of the type ‘dogs’.
Figure A7: Example activation maximisation images for unit fc6.1199. Whilst there are some butter-fly wing shapes in these images, there are not obvious butterflies. N.B. the second highest activatingclass for this unit is ladybirds, and there are some orange round shapes that could conceivably beladybug-alikes.
