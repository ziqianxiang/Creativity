Figure 1: The maximum eigenvalue XmaX and associated critical learning rate η for vanilla (blue)and BatchNorm networks (red) as a function of the BatchNorm parameter γ for different choicesof architecture (fully-connected and convolutional), calculated by theory. (a, c) shows the flatteningeffect of BatchNorm on the loss function for a wide range of hyperparameters and (b, d) furthershow that for sufficiently small γ BatchNorm enables optimization with much higher learning ratethan vanilla networks.
Figure 2: Heatmaps showing test loss as a function of (log10 η, γ) after 5 epochs of training fordifferent choices of dataset and architecture. Results were obtained by averaging 5 random restarts.
Figure 3: Heatmap showing test loss as a function of (log10 η, γ) after 5 epochs of training for vanillafully-connected feed forward network where the relation between weight initialization variance andmaximal learning rate is studied.
Figure 4: Learning curves of (a) VGG16 and (b) Preact-Resnet18 training on CIFAR-10, with thesame hyperparameters except γ initialization. Results support that small γ initialization helps fasterconvergence.
