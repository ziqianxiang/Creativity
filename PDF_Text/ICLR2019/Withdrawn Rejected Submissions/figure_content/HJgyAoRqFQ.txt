Figure 1:	(a) energylandscape, (b) attractornet, (c) attractor net un-folded in time.
Figure 2: Percentage noise variance suppression by an attractor net trained on 3 noise levels (σ). Ineach graph, the number of hidden units in the attractor net (n) and number of attractors to be learned(A) is varied. In all cases, inputs are 50-dimensional and evaluation is based on a fixed σ = 0.250.
Figure 3: SDRNN architecture for sequence pro-cessing, unrolled in time such that each columndenotes a single time step with a correspondinginput and output. The hidden state is denoised bythe attractor net, yielding a cleaned state which iscombined with the next input to determine the nexthidden state.
Figure 4: Parity simulations. Top row shows generalization performance on novel binary sequences;bottom row shows performance on trained sequences with additive noise. Unless otherwise noted, thesimulations of the SDRNN use σ = 0.5, 15 attractor iterations, and L2 regularization (a.k.a. weightdecay) 0.0. Error bars indicate ±1 SEM, based on a correction for confidence intervals with matchedcomparisons (Masson and Loftus, 2003).
Figure 5: Simulation results on majority task with (a) novel and (b) noisy sequences. (c) Rebergrammar. (d) Simulation results on Reber grammar. Error bars indicate ±1 SEM, based on acorrection for confidence intervals with matched comparisons (Masson and Loftus, 2003).
Figure 6: (a) Sym-metry task withf =	1 filler;(b) Symmetry taskwith f = 10 filler;(c) Part-Of-SPeechtagging. Errorbars indicate ±1SEM.
