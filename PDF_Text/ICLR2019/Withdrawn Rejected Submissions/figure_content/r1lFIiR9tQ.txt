Figure 2: Upper and lower bounds on the divergence Df (pθ (x)||p(x)). In our upper bound, both themodel parameters θ and bound parameters φ are adjusted to push down the upper bound, therebydriving down the divergence. In the Fenchel-conjugate approach Nowozin et al. (2016), the lowerbound is made tighter adjusting the bound parameters φ to push up the bound towards the truedivergence, whilst then minimizing this with respect the model parameters θ.
Figure 3: Toy problem. In (a), (b) and (c) we plot the samples of the model pθ (x) trained by threedifferent f-divergences. In (d), (e) and (f) We plot the latent Z by sampling from the trained qφ(z∣χ)for each datapoint x. Note that this is after inverting the noise process, to recover the model on the xspace. See also section Coptimizing the auxiliary bound with respect to only qφ(z∖y) tightens the bound (towards the marginaldivergence) is shoWn in the supplementary materials section A.
Figure 4: MNIST experiment. (a) Samples from the models trained by forward KL (b) Samples fromthe models trained by reverse KL.
Figure 5: CelebA experiment. Image samples from the trained models pθ (x). After VAE initialization,we continued training for an additional epoch with (a) pure forward KL and (b) the reverse KLdivergence.
Figure 6: Target distributionof the toy problem, fromRoth et al. (2017)z ∈ R64 → FC8×8×1024→ DeConv512 → BN → Relu→ DeConv256 → BN → Relu→ DeConv128 → BN → Relu→ DeConv64 → BN → Relu → DeConv3E f -GAN COMPARISONThe mixture of Gaussians we attempt to fit a univariate Gaussian to is plotted in Figure 7.
Figure 7: Mixture of two Gaussians, 0.3N1 + 0.7N2 where Ni = N(μι = 1,σ1 = 0.1) andN2 = N (μ2 = 2,σ2 = 0.5)15Under review as a conference paper at ICLR 2019We plot the lower and upper bounds during training in Figure 8. We can see the upper bound isgenerally faster to converge and less noisy. It also a consistently decreasing objective, whereas thevariational lower bound fluctuates higher and lower in value throughout the training process.
Figure 8: Training runs for fitting a univariate Gaussian to a mixture of two Gaussians by minimizinga variety of f -divergences. On the left we train using the lower bound, on the right with our upperbound.
