Figure 1: Comparison between 3 deep learning models on the MNIST and Fashion- MNIST datasets.
Figure 2: Comparison of GaLU and ReLU networks with a single hidden layer and output in R1 .
Figure 3: Train and test errors for the two models from section 4.2, with n = k = d = 30, m =1000, Ïƒx = 0.1. Both of those graphs show that the generalization error is highly correlated with theoptimal error: it is not true that there is a constant difference between the train error and test error.
Figure 4:	A linear correlation between the generalization gap and the squared norm of the solution.
Figure 5:	We empirically found the minimal num-ber of neurons k such that a one hidden layer networkachieves MSE< 0.3 on the random regression problem.
