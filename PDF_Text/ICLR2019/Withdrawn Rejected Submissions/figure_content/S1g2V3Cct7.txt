Figure 1: Separate, simultaneous, and sequential training: the x-axis denotes environmentsteps summed across all tasks and the y-axis episode score. In “Sequential”, thick linesegments are used to denote the task currently being trained, while thin segments are plot-ted by evaluating performance without learning. In simultaneous training, performance onexplore_object _locations_Small is higher than in separate training, an example of modestconstructive interference. In sequential training, tasks that are not currently being learned exhibitvery dramatic catastrophic forgetting. (See Appendix C for a different plot of these data.)should immediately be good - ideally as good as it was historically. Second, maintenance of oldskills or knowledge should not inhibit further rapid acquisition of a new skill or knowledge. Thesetwo simultaneous constraints - maintaining the old while still adapting to the new - represent thechallenge known as the stability-plasticity dilemma Grossberg (1982). Third, where possible, acontinual learning system should learn new skills that are related to old ones faster than it wouldhave de novo, a property known as constructive interference or positive transfer.
Figure 2: Demonstration of CLEAR on three DMLab tasks, which are trained cyclically in sequence.
Figure 3: Comparison between different proportions of new and replay examples while cyclingtraining among three tasks. We observe that with a 75-25 new-replay split, CLEAR eliminatesmost, but not all, catastrophic forgetting. At the opposite extreme, 100% replay prevents forgettingat the expense of reduced overall performance (with an especially noticeable reduction in earlyperformance on the task rooms_keys_doors_Puzzle). A 50-50 split represents a good tradeoffbetween these extremes. (See Appendix C for a different plot of these data.)replay) by a number of acting networks, for which the weights are asynchronously updated to matchthose of the learner. The network architecture and hyperparameters are chosen as in Espeholt et al.
Figure 4: Comparison of the performance of training with different buffer sizes, using reservoirsampling to ensure that each buffer stores a uniform sample from all past experience. We observeonly minimal difference in performance, with the smallest buffer (storing 1 in 200 experiences)demonstrating some catastrophic forgetting. (See Appendix C for a different plot of these data.)three tasks: (1) Training networks on the individual tasks separately, (2) training a single networkexamples from all tasks simultaneously (which permits interference among tasks), and (3) traininga single network sequentially on examples from one task, then the next task, and so on cyclically.
Figure 5: The DMLab task natlab_varying_map_randomize (brown line) is presented atdifferent positions within a cyclically repeating sequence of three other DMLab tasks. We find that(1) performance on the probe task is independent of its position in the sequence, (2) the effectivenessof CLEAR does not degrade as more experiences and tasks are introduced into the buffer.
Figure 6: Comparison of CLEAR to Progress & Compress (P&C) and Elastic Weight Consolidation(EWC). We find that CLEAR demonstrates comparable or greater performance than these methods,despite being significantly simpler and not requiring any knowledge of boundaries between tasks.
Figure 7: This figure shows the same “probe task” setup as Figure 5, but with CLEAR using 100%replay experience. Performance on the probe task natlabVarying_map_randomized de-creases markedly as it appears later in the sequence of tasks, emphasizing the importance of using ablend of new experience and replay instead of 100% replay.
Figure 8: Quantitative comparison of the final cumulative performance between standard training(“Sequential (no CLEAR)”) and various versions of CLEAR (see Figures 9, 10, 11, and 12 below)on a cyclically repeating sequence of DMLab tasks. We also include the results of training on eachindividual task with a separate network (“Separate”) and on all tasks simultaneously (“Simultane-ous”) instead of sequentially. As described in Section 4.1, these situations represent no-forgettingscenarios and thus present upper bounds on the performance expected in a continual learning setting,where tasks are presented sequentially. Remarkably, CLEAR achieves performance comparable to“Separate” and “Simultaneous”, demonstrating that forgetting is virtually eliminated.
Figure 9: Alternative plot of the experiments shown in Figure 1, showing the difference in cumu-lative performance between training on tasks separately, simultaneously, and sequentially (withoutusing CLEAR). The marked decrease in performance for sequential training is due to catastrophicforgetting. As in our earlier plots, thicker line segments are used to denote times at which thenetwork is gaining new experience on a given task.
Figure 10: Alternative plot of the experiments shown in Figure 2, showing how applying CLEARwhen training on sequentially presented tasks gives almost the same results as training on all taskssimultaneously (compare to sequential and simultaneous training in Figure 9 above). ApplyingCLEAR without behavioral cloning also yields decent results.
Figure 11: Alternative plot of the experiments shown in Figure 3, comparing performance betweenusing CLEAR with 75-25 new-replay experience, 50-50 new-replay experience, and 100% replayexperience. An equal balance of new and replay experience seems to represent a good tradeoff be-tween stability and plasticity, while 100% replay reduces forgetting but lowers performance overall.
Figure 12: Alternative plot of the experiments shown in Figure 4, showing that reduced-size buffersstill allow CLEAR to achieve essentially the same performance.
Figure 13: Quantitative comparison of the final cumulative performance between baseline (standardtraining), CLEAR, Elastic Weight Consolidation (EWC), and Progress & Compress (P&C) (seeFigure 14 below). Overall, CLEAR performs comparably to or better than P&C, and significantlybetter than EWC and baseline.
Figure 14: Alternative plot of the experiments shown in Figure 6, showing that CLEAR attainscomparable or better performance than the more complicated methods Progress & Compress (P&C)and Elastic Weight Consolidation (EWC), which also require information about task boundaries,unlike CLEAR.
