Figure 1: The disjoint setting considered. At task i the training set includes images belonging tocategory i, and the task is to generate samples from all previously seen categories. Here MNIST isused as a visual example,but we experiment in the same way Fashion MNIST and CIFAR10.
Figure 2: Comparison, averaged over 8 seeds, between FID results(left, lower is better) and FittingCapacity results (right, higher is better) with GAN trained on MNIST.
Figure 3: Means and standard deviations over 8 seeds of Fitting Capacity metric evaluation of VAE,CVAE, GAN, CGAN and WGAN. The four considered CL strategies are: Fine Tuning, GenerativeReplay, Rehearsal and EWC. The setting is 10 disjoint tasks on MNIST and Fashion MNIST.
Figure 4: Fitting Capacity results for GAN (top) and VAE (bottom) on MNIST. Each square is theaccuracy on one class for one task. Abscissa is the task index (left: 0 , right: 9) and orderly is theclass index (top: 0, down: 9). The accuracy is proportional to the color (dark blue : 0%, yellow100%)task index	CSk indexFigure 5: Fitting capacity and FID score of Continual Learning methods applied to WGAN_GP, onCIFAR10. For each method, images sampled after the 10 sequential tasks are displayed.
Figure 5: Fitting capacity and FID score of Continual Learning methods applied to WGAN_GP, onCIFAR10. For each method, images sampled after the 10 sequential tasks are displayed.
Figure 6: Samples of a well performing solution : GAN + Generative Replay for each step in asequence of 10 tasks with MNIST and Fashion MNIST.
Figure 7: Test set classification accuracy as a function of number of training samples, on MNIST.
Figure 8: Comparison of the Fitting Capacity and FID results on MNIST.
Figure 9: Comparison of the Fitting Capacity and FID results on Fashion MNIST.
Figure 10: CGAN augmented with EWC. MNIST samples after 5 sequential tasks of 2 digits each.
Figure 11: Reproduction of EWC experiment (Seff et al., 2017) with four tasks. First task with 0and 1 digits, then digits of 2 for task 2, digits of 3 for task 3 etc. When task contains only one class,the Fisher information matrix cannot capture the importance of the class-index input vector becauseit is always fixed to one class. This problem makes the learning setting similar to a non-conditionalmodels one which is known to not work (Seff et al., 2017). As a consequence 0 and 1 are wellprotected when following classes are not.
Figure 12: CGAN results with EWC, Rehearsal and Generative Replay, on 5 sequential tasks of 2digits each. EWC performs well, compared to the results obtained on a 10 sequential task setting.
Figure 13: Samples from GAN and Conditional-GAN for each Continual Learning strategy. Upper-bound refers to Upperbound Model.
Figure 14: MNIST samples for each generative model and each Continual Learning strategy, at theend of training on 10 sequential tasks. The goal is to produce samples from all categories.
Figure 15: Fashion MNIST samples for each generative model and each Continual Learning strategy,at the end of training on 10 sequential tasks. The goal is to produce samples from all categories.
Figure 16: WGAN-GP samples on CIFAR10, with on training for each separate category. Theimplementation we used is available here: https://github.com/caogang/wgan-gp.
Figure 17: WGAN-GP samples on 10 sequential tasks on CIFAR10, with Generative Replay.
