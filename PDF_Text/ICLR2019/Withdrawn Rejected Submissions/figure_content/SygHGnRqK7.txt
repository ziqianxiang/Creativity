Figure 1: Single layer Probabilistic Neural Matching algorithm showing matching of three MLPs.
Figure 2: Baselines satisfying the constraints: Test accuracy and model size comparison for varyingnumber of batches J (total data size fixed, hence decreasing batch size)No. of batches J; No. of hidden layers C=I	No. of batches J; No. of hidden layers C=I	No. of batches J; No. of hidden layers C=I	No. of batches J; No. of hidden layers C=I(a) MNIST homogeneous (b) MNIST heterogeneous (c) CIFAR homogeneous (d) CIFAR heterogeneousFigure 3: Comparison to baselines with extra resources: Test accuracy comparison for varyingnumber of batches J (total data size fixed, hence decreasing batch size)show significant compression over maximum possible 100J. Our experiments demonstrate that itis possible to efficiently perform model averaging of neural networks in the parameter space byaccounting for permutation invariance of the hidden neurons. We reiterate that the global modellearned by PFNM may either be used as final solution for a federated learning problem or serve asan initialization to obtain better performance with distributed optimization, Federated Averaging orKnowledge Distillation when some of our problem constraints are relaxed.
Figure 3: Comparison to baselines with extra resources: Test accuracy comparison for varyingnumber of batches J (total data size fixed, hence decreasing batch size)show significant compression over maximum possible 100J. Our experiments demonstrate that itis possible to efficiently perform model averaging of neural networks in the parameter space byaccounting for permutation invariance of the hidden neurons. We reiterate that the global modellearned by PFNM may either be used as final solution for a federated learning problem or serve asan initialization to obtain better performance with distributed optimization, Federated Averaging orKnowledge Distillation when some of our problem constraints are relaxed.
Figure 4: Comparison to baselines with extra resources: Test accuracy comparison for varyingnumber of layers C with J = 10result in a bad gradient after an epoch). This significantly degrades performance of the downpourSGD and also affects PFNM in the case of heterogeneous CIFAR-10 (Fig. 3d). We observe thatD-SGD at first improves with increasing number of batches and then drops down in performanceabruptly — at first increasing number of batches essentially increases number of communications,since each batch sends gradients to the server, without hurting the quality of gradients, howeverwhen size of batches decreases gradients become worse and D-SGD behaves poorly. On the otherhand, ensemble approaches only require a collection of weak classifiers to perform well, hence theirperformance does not noticeably degrade as the quality of batch neural networks deteriorates. Thisadvantage comes at a price of high computational burden when making a prediction, since we needto do a forward pass for an input observation through each of the batch networks. Interestingly,weighted ensemble performs worse than uniform ensemble on heterogeneous CIFAR-10 case - thisagain could be due to the low quality of batch networks which hurts our method and makes uniformensemble more robust than weighted. In the second experiment we fix J = 10 and consider multi-layer batch neural networks with number of layers C from 1 to 6. We see (Fig. 4) that our multilayerPFNM can handle deep networks as it continues to be comparable to ensemble techniques and out-perform D-SGD. In the Supplementary we analyze sizes of the master neural network learned byPFNM, parameter sensitivity, streaming extension and explore performance of downpour SGD withmore frequent communications. We conclude that for federated learning applications when predic-
Figure 1: Probabilistic Neural Matching algorithm showing matching of three multilayer MLPs.
Figure 2: Network sizes for varying number of batches and layersOoooo100c80604020suo」n①u」①Ae-IJəppzJo -IBqEnNNumber of hidden layers C(b) Varying C, J =10Downpour SGD with frequent communication In the main text we considered downpour SGDwith total of 10 rounds of communication — one after each training epoch. This implies 20J com-munications, 10J for batches to send their gradients to the master server and 10J for the masterserver to send copy of the global neural network to each of the batch neural networks. In ourfederated learning problem setup frequent communication is discouraged, however it is interest-ing to study the minimum number of communications needed for D-SGD to produce competitiveresult. To empirically quantify this we show test accuracy of D-SGD with increasing number ofcommunication rounds on MNIST with heterogeneous partitioning and J = 25 (Fig. 3a). PFNMand ensemble based methods are shown for comparison - they communicate only once (post batchnetworks training) in all of our experiments. We see that in this case D-SGD requires more than4000 communication rounds (8000J communications) to produce good result. Such large amountof communication is impossible in practice for the majority of federate learning scenarios.
Figure 3: Additional experimentsStreaming federated learning experiment To simulate streaming federated learning setup wepartition data into J groups using both homogeneous and heterogeneous strategies as before. Theneach group is randomly split into S parts. At step s =1,...,Sthe part of data indexed by s fromeach of the groups is revealed and used for updating the models. For this experiment we considerheterogeneous partitioning of MNIST with J = 3 batches and S = 15 steps. On every step weevaluate accuracy on the test dataset and summarize performance of all methods in Figure 3b. Forour method, PFNM-Streaming, we perform matching based on the cost computations from Section3 to update the global neural network. We initialize weights of the j -th batch neural network for thenext step s + 1 according to the model posterior after s steps (which is the prior for step s +1):subsample Ljs+1 - γ0 neurons from the global network according to popularity counts {mjs,i }i andconcatenate with γo neurons initialized with μ0 (prior mean before any data is observed, which isset to 0), then add small amount of Gaussian noise. For D-SGD we update global neural networkweights after each step and then use these weights as initialization for batch neural networks onthe next step. To extend ensemble based methods to streaming setting we simply update localneural networks sequentially as the new data becomes available and use them to evaluate ensembleperformance at each step.
Figure 4: Parameter sensitivity analysis for J = 257Under review as a conference paper at ICLR 20190 5 0 5 0 5 0阻97.97.96.如 皈 皈(％) AOE-Jnuue U过一J=IO—J=15-J=25^2	04	06	08σ2(a) MNIST homogeneous....-J=2—J=3....-J=4—J=5--J=6—J=7— J=8—J=9
Figure 5: Sensitivity analysis of σ2 for fixed σ02 = 10 and γ0 =1for varying J8Under review as a conference paper at ICLR 2019ReferencesJeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Andrew Senior,Paul Tucker, Ke Yang, Quoc V Le, et al. Large scale distributed deep networks. In Advances inneural information processing systems, pp. 1223-1231, 2012.
