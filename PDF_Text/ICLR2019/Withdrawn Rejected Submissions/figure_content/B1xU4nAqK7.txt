Figure 1: The unsupervised model-basedRL setting. An unsupervised phase isused to build a predictive model using ourproposed exploration criterion. When atask is then specified, the agent can imme-diately perform with no further training.
Figure 3: Reward functions.
Figure 4: Results on four different Halfcheetah tasks: forwards running, backwards running, for-ward flipping, and backwards flipping. For each of the four experiments, we show performanceduring unsupervised training (left), which generally stays at zero for all methods, since they are notattempting to perform the task, except for the oracle (PETS). Once provided with the task reward(right), our method (state-uncertainty) achieves the best reward in all cases, except when comparedto the oracle on the forward task.
