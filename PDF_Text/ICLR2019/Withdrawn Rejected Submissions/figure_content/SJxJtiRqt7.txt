Figure 1: Model overview. First, multimodal features are created from sound features extracted fromthe input sounds by SoundNet. Then, two GANs are used to generate images from these multimodalfeatures by first converting the multimodal features into image features, then converting these imagefeatures into the output images.
Figure 2: Labeled image/sound pair dataset. We refined the dataset by selecting only pairs wherethe image and sound had at least one of their top 3 labels in common.
Figure 3: Images generated from image features. Here the generated images retained the originalimagesâ€™ characteristics, such as their spatial and color distributions and the objects present, despitebeing restored from reduced information.
Figure 4: Extracting multimodal features by training the multimodal layers. This figure shows howthe proposed multimodal layer (used to reduce the information mismatch between the two modali-ties) is structured and trained, and also how the multimodal features are extracted. The multimodalfeatures are extracted by inputting either both image and sound features, or either feature type indi-vidually.
Figure 5: Visualization of multimodal features. To test whether the generated multimodal featuresencoded features common to the image and sound, we found the sound/image pairs whose multi-modal features were closest (in terms of Euclidean distance). Here, we compare the image and theimage corresponding to the sound, showing that they mostly belong to the same class.
Figure 6: Images generated from sounds. Here, we show example images generated from sounds.
Figure 7: Model performance evaluation. Here, we show the match rates for the classes discrim-inated by the classifier, considering between 1 and 10 of the top classes. The overall match rateswere 1.65%, 8.9%, 16.04%, and 31.92% for the top 1, 3, 5, and 10 classes, respectively.
