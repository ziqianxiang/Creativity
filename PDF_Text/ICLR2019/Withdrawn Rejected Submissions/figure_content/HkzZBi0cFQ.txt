Figure 1: Comparison between layer-wise and channel-wise quantization in a simple convolutionlayer with 2 IFM channels and 1 OFM channel. Qn.m represents a fixed point integer format withn bits for integer part, m bits for fractional part, and 1 bit for the sign. Total bit-width is equal to(n + m + 1) bits. minfl(A, B) returns a format with the minimum fractional length. For layer-wisequantization in (a), all the channels of both the inputs and the kernels are forced to share the biggestinteger part length across all channels in order to retain the most significant bits. Thus, the number ofbits for fractional parts are identical for all paths at the partial sum stage. Channel-wise quantizationmethods in (b) and (c) could spare more bits for fractional part in accumulator and adder due tochannel level granularity, thereby reducing the low precision rounding error during summation.
Figure 2: Quantization policy varies with network configurations. a and w represent activation andweights, respectively. (cw: channel-wise quantization, lw: layer-wise quantization)4Under review as a conference paper at ICLR 2019Figure 3: Superpositioned PDFs of pre-activation values of each channel (GoogLeNet w/ ImageNetdataset). Every distribution is normalized to have unit variance. Y-axis is in log scale. Mean dist.
Figure 3: Superpositioned PDFs of pre-activation values of each channel (GoogLeNet w/ ImageNetdataset). Every distribution is normalized to have unit variance. Y-axis is in log scale. Mean dist.
Figure 4: Effect of profiling dataset size on accuracy with quantization for Inception-v3. MAXmethod requires large number of samples for profiling to reach a stable accuracy. Laplace methodstabilizes quickly with a few samples.
