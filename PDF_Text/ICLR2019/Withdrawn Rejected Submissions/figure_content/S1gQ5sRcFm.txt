Figure 1: JUMP uses a prior Pφ to sample a latent z , conditioned on the input frames and viewpoints.
Figure 2: JUMP is trained using an approximate posterior Pψ that has access to multiple targets.
Figure 3: In the “Color Reaction” narrative, shape 1 moves to shape 2 over frames 1 - 6, shape 2changes color and stays that color from frames 7 - 12. The ground truth is shown in sub-figure A.
Figure 4: In the Traveling Salesman narrative, one shape (in this case the green square) sequentiallymoves towards and visits the other four shapes in some stochastic order. JUMP, sSSM and SV2Pgenerally capture this, but with shape and color artifacts. BADJ is JUMP without the architecturalfeatures used to enforce consistency between target frames. BADJ does not capture a coherentnarrative where the green square visits all four objects (in this case the white pentagon is not visited).
Figure 5: Quantitative comparisons between JUMP and video and scene prediction models.
Figure 6: A cube in a room, with MNIST digits engraved on each face (test-set scene). The bluecones are where the context frames were captured from. The red cones are where the model isqueried. The context frames see three sides of the cube, but the models are tasked to sample fromthe fourth, unseen, side. GQN (right column) independently samples a 0 and 7 for the unseen side,resulting in an inconsistent scene. JUMP samples a consistent digit (2 or 9) for the unseen cube face.
