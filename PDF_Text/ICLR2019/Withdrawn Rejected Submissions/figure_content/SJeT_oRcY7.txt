Figure 1: The network model. A Training with BackProPagation (BP) through one hidden layer. BArchitecture with unsupervised feature learning or fixed Random Projections (RP) in the first layerand a suPervised classifier in the second layer. W stands for feed-forward, V for recurrent, inhibitoryweights (Only used for unsuPervised feature learning of the first layer weights W1). C Illustrationof fully connected and localized recePtive fields of W1 .
Figure 2: Spiking LIF and STDP dynamics. A Dynamics of the pre- and postsynaptic membranepotentials, spike-traces and the weight value (B) ofatoy example with two neurons and one synapse.
Figure 3: MNIST classification with rate networks, according to Figure 1. The test error decreasesfor increasing hidden layer size nh for Principal Component Analysis (PCA), Sparse Coding (SC),fixed Random Projections (RP) and the fully supervised reference algorithms Backpropagation (BP)and Feedback Alignment (FA). The dashed dotted line at 90 % is chance level, the dotted line around14 % is the performance of a Simple Perceptron without hidden layer. The vertical line marks theinput dimension d = 784, i.e. the transition from under- to overcomplete hidden representations.
Figure 4: Effect of localized connectivity on MNIST. A Test error for localized Random Projections(l-RP), dependent on receptive field size P for different hidden layer sizes nh. The optimum at P=10 is more pronounced for large hidden layer sizes. Full connectivity is equivalent to P = 28.
