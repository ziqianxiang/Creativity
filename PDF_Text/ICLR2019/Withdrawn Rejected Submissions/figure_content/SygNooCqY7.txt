Figure 1: Illustration of uses of additive noise as a regularizer for generative models. Adding noiseto samples from a probability density pd results in a smoothing of pd . (a) Shows that matchingsmooth distributions results in a low-quality probability density estimate pg . (b) Shows that match-ing smooth probability densities for two noise instances results already in a much higher qualityestimate pg .
Figure 2: Left column: Random reconstructions from models trained on CelebA without noise-tempering (NT). Right column: Random reconstructions with our proposed method.
Figure 3: Results of the robustness experiments as specified in Table 4 and performed on CIFAR-10. We compare the standard GAN (1st column), a GAN with gradient penalty (2nd column), aGAN with spectral normalization (3rd column) and a GAN with our proposed method (4th column).
Figure 4: Reconstructions from NTGANs trained on 128 Ã— 128 images from the LSUN bedroomsdataset (top) and ImageNet (bottom).
Figure 5: We performed experiments on synthetic 2D data with a standard GAN (top) and a NTGAN(bottom). The ground truth data is shown in red and the model generated data is shown in blue. ForNTGAN we also show samples from the blurred data distribution pd, in green and the blurred modeldistribution pg, in purple.
Figure 6: We show examples of the generated noise (top row) and corresponding noisy trainingexamples (rows 2 to 4). The different columns correspond to different iterations.
Figure 7: We show random reConstruCtions for some of the ablation experiments listed in Table 2.
Figure 8: We show random reconstructions for the robustness experiments (see Table 4). We com-pare a standard GAN (1st column), a GAN with gradient penalty by Roth et al. (2017) (2nd column),a GAN with spectral normalization by Miyato et al. (2018) (3rd column) and a GAN with our pro-posed method (4th column).
