Figure 1: Illustration of Graph Transformer on updating a target graph node (left panel) and itsstacked version (right panel). hi and h indicates latent state of the ith source graph node and theith target graph node respectively, where source graph and target graph are two different graphs. ei7-indicates edge connecting source node i and target node j. GTR evolves a target graph by recurrentlyperforming Source Attention on a source graph in an inter-graph message passing process, and SeWAttention on target graph in an intra-graph message passing process.
Figure 2: Architecture of Graph Transformer for few-shot learning. Images are first fed into aFeature Extractor for extracting features. The few-shot classification weight generator implementedas GTR takes a target graph whose nodes are the novel categories and features are initialized as theextracted visual features of the few training samples of novel categories, and a source graph whosenodes are the base categories and features are the corresponding category prototypes. GTR updatesthe target graph features by considering similarity among node features of the target graph, and thatof the source graph and target graph. The final node features of the target graph are used as thegenerated novel category prototypes for subsequent classification.
Figure 3: Architecture of Graph Transformer for medical abnormality and disease classification.
