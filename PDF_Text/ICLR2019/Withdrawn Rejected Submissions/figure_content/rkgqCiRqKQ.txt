Figure 1: The plans of our synthetic agents on two navigation environments. Actual trajectories could differdue to randomness in the transitions. Green squares indicate positive reward while red squares indicate negativereward, with darker colors indicating higher magnitude of reward.
Figure 2: The architecture and operations on it that we use in our algorithms.
Figure 3: Reward obtained when planning with the inferred reward, as a percentage of the maximum possiblereward, for different bias models and algorithms.
Figure 4: Percent reward obtained for different bias models using variations of Algorithm 2, which does not getaccess to any known rewards. These algorithms can vary along two dimensions - whether they are initializedwith the assumption that the demonstrator is rational, and whether they train the planner and reward jointly orwith coordinate ascent. The original version of Algorithm 2 does initialize, and trains jointly.
