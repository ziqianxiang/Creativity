Figure 1: (1a) Input to PCA is a NxM sized matrix, where M is the number of features among whichwe want to identify redundancy and N is the number of samples we have feature values for. (1b) Asample curve for PCA. On the X axis are the number of principal components and on the Y axis arethe cumulative percentage of the variance of the input data that the components can explain.
Figure 3: Visualization of pruning. The filter to be pruned out is highlighted on the left and thefilter it merges with is highlighted on the right. It can be seen that the merged filter incorporates thediagonal checkerboard pattern from the removed filter.
Figure 4: (4a) Filter to be pruned is shown in black and the one that got changed the most is in blue.
Figure 5: The output of convolving one filter with an input patch can be viewed as the feature valueof that filter. In the centre, the highlighted green pixel is the feature value for the first filter for thefirst input patch. On the right, the same input patch creates feature values for all M filters, resultingin 1 row of the PCA matrix.
Figure 6: Curves for VGG16_BN network trained on CIFAR-10. 6a shows that layers 1-7 (6a & 6b)have increasing sensitivity to pruning, whereas the sensitivity decreased from layers 8 onwards (6c6d). 6a and 6c show the PCA curves for explained variance vs number of principal components and6b and 6d show percentage number of filters vs percentage of original accuracy for different layers7Under review as a conference paper at ICLR 20194 Results and Optimal Network StructureThe results are tabulated and summarized in Table 1. The details of the corresponding networksand experiments are explained in the following sections. A toolkit [Burzawa (2018)] available withPyTorch [Paszke et al. (2017)] was used for profiling networks to get the number of operations andparameters.
Figure 7: 7a and 7b show results for VGG16 trained on CIFAR-10 and 7c and 7d for VGG19 trainedon CIFAR-100. 7a and 7c illustrate how decreasing the number of layers affects accuracy for theoriginal network and the network with only significant filters. 7b and 7d reflect the explained vari-ance targeted in blue and the corresponding accuracy as percentage of original accuracy in orange.
Figure 8: (8a) For AlexNet network on CIFAR100, the curve in blue reflects the explained vari-ance targeted and the curve in orange reflects the corresponding accuracy as percentage of originalaccuracy. (8b) Learned filters of the a convolutional layer with 32 filters on CIFAR-10. (8c) Orthog-onal filters transformed according to principal components, ranked according to the percentage ofvariance of the output activation map they explain4.4	Network 4: VGG19 adapted to ImageNetCarrying out a similar analysis for VGG-19 (with batch normalization) for ImageNet led to thefollowing configuration after PCA analysis: [6, 30, ‘M’, 49, 100, ‘M’, 169, 189, 205, 210, ‘M’,400, 455, 480, 490, ‘M’, 492, 492, 492, 492, ‘M’]. The accuracy dropped by around 0.25% with animprovement of 1.72X and 1.07X in number of operations and parameters respectively.
