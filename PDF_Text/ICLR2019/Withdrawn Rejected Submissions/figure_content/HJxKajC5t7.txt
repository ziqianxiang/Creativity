Figure 1: Schematic comparison of a typical binary neural network generated by existing methods(top half) with that of our method (bottom half). Existing methods are unable to avoid the 32-bitfloating point batch norm, while we simplify it to a comparison operation, which simultaneouslyserves as the activation layer.
Figure 2: The discrete sign (a) and its derivative (b) are shown. (c, d) show that as ν takes valuesof 1, 5, and 20, the scaled tanh converges toward the original sign function and its derivative.
Figure 3: Comparison between time and memory consumption of BN, SBN and BinaryBN for aninput feature map of 256 × 256. For reference, dotted lines show the minimum and maximum numberof parameters that can be found in the BN layers of a VGG-16 network and an AlexNet network.
