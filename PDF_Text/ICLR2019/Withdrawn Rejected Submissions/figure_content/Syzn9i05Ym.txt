Figure 1: Comparison of different methods over GMM synthetiC data. StoChastiC generations fromGAN with logD triCk, WGAN-GP, ExClusive-NRF, Inclusive-NRF generation (i.e. sampling fromthe auxiliary generator) and Inclusive-NRF revision (i.e. after performing sample revision oversamples from the auxiliary generator), are shown in (b)-(f) respeCtively. InClusive-NRF generationand inClusive-NRF revision are two manners to generate samples, given a trained NRF. For bothmanners, the NRF model is trained with the sample revision step. EaCh generation Contains 1,000samples. The learned potentials uθ (x) from exClusive and inClusive NRFs are shown in (g) and (h)respeCtively, where the red dots indiCate the mean of eaCh Gaussian Component. InClusive NRFs areClearly superior in learning data density and sample generation.
Figure 2: SSL toy experiment based on semi-supervised inclusive-NRFs. Each class has 4 labeledpoints, red dots for Class 1 and blue for Class 2. The learned potentials for uθ (x), uθ (x, y = 1) anduθ (x, y = 2) are shown in (b)(C)(d) respeCtively.
Figure 3: Latent space interpolation with inclusive-NRFs on MNIST. The leftmost and rightmostcolumns are from stochastic generations x1 with latent code h1 and x2 with h2 . The columns inbetween correspond to the generations from the latent codes interpolated linearly from h1 to h2 .
Figure 4:	Conditional generated samples from semi-supervised inclusive-NRFs trained on MNIST.
Figure 5: Generated samples from semi-supervised inclusive-NRFs (i.e. trained for SSL) on SVHNand CIFAR-10 are shown in (a) and (b) respectively. Generated samples from unsupervised andsupervised training of inclusive-NRFs on CIFAR-10 are shown in (c) and (d) respectively.
