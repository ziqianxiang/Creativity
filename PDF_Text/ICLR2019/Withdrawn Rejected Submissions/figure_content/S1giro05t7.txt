Figure 1: Deep networks can often make highly confident mistakes when samples are drawn fromoutside the distribution observed during training. Shown are example images that have ages, breeds,or species that are not observed during training and are misclassified by a deep network model withhigh confidence. Using our methods (shown here is G-distillation, to distill an ensemble of networkson both training and unsupervised examples) makes fewer confident errors for novel examples, in-creasing the reliability of prediction confidence overall.
Figure 2: This toy experiment illustrates how classifiers (in this case deep networks) can be confi-dently wrong when attempting to generalize outside the extent of the training data. See Appendix Afor details and extra toy datasets. Figure best read in color.
Figure 3: Our performance on familiar F and novel data N , for tasks with no overlap betweenthe two. Tables: in terms of novel NLL, calibrated methods perform the best, while our methodsoutperform the rest of the single-model methods. Methods perform similarly on familiar NLL,while ours and calibration are more reliable with high-confidence estimates (E99), indicating a bettergeneralization. We perform on the same level on accuracy or mAP as other methods except for G-distill in (c). Graphs: Trade off familiar and novel NLL by smoothing the probability estimates witha higher or lower temperature. Crosses indicate performance without smoothing (Ï„ = 1). Bottom-left is better. Our methods do not outperform single models considering smoothing trade-off.
Figure 4: Our performance on VOC-COCO recognition, where familiar F and novel data N have astrong overlap. Table: for NLL, we outperform other single-model based methods on novel data butunderperform on familiar. Calibration methods do not make much difference. Graph: consideringtrade-off between familiar and novel NLL, G-distillation performs similarly to distillation, whileNCR underperforms. See Figure 3 for details. Note that this figure is zoomed in more.
Figure 5:	A continuation of the toy experiment in Figure 2 with more datasets. Classifiers (in thiscase deep networks) can be confidently wrong when attempting to generalize outside the extentof the training data. On these regions, an ensemble is better behaved and makes less confidentmistakes. G-distill is more able to mimic this behavior than standard distillation. All methodsperform similarly in the familiar region. See Figure 2 for details. Figure best read in color.
Figure 6:	Performances on animal recognition with ImageNet using and distilling from a baggingensemble instead. This reduces familiar performance.
Figure 7:	Performances on gender recognition with LFW+ using DenseNet161 as the backbone.
Figure 8:	USing an enSemble of G-diStilled modelS to further booSt the performance. Althoughwe do obtain a better novel NLL compared to the enSemble, we uSually lag behind the enSembleconSidering the Smoothing trade-off.
