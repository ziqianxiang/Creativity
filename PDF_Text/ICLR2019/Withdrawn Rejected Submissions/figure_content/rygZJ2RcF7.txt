Figure 1: (a) Neuron editing interrupts the standard feedforward process, editing the neurons of atrained encoder/decoder to include the source-to-target variation (Equation 1), and letting the traineddecoder cascade the resulting transformation back into the original data space. (b) The network canincorporate context information, like cell type, that is encoded in other neurons as changes inducedby the editing transformation cascade through the decoder.
Figure 2: Data from CelebA where the source data consists of males with black hair and the targetdata consists of males with blond hair. The extrapolation is then applied to females with black hair.
Figure 3: The results of learning to batch correct a sample based on the difference in a repeatedly-measured control population measured. The true transformation is depicted with a solid arrow andthe learned transformation with a dashed arrow. There is a batch effect that should be corrected inIFNg (horizontal) with true variation that should be preserved in CCR6 (vertical). All of the GANsattempt to get rid of all sources of variation (but do so only partially because the input is out-of-sample). The autoencoder does not move the data at all. Neuron editing corrects the batch effect inIFNg while preserving the true biological variation in CCR6.
Figure 4: (a) The global shift in the twocontrols (light blue to red) is isolated andthis variation is edited into the sample (darkblue to red), with all other variation pre-served. (b) The median change in the sam-ple in each dimension corresponds accu-rately with the evidence in each dimensionin the controls.
Figure 5: Combinatorial drug data showing predicted (blue) and real (red) under both Bez and Dasdrug treatments. The true transformation is depicted with a solid arrow and the predicted transfor-mation with a dashed arrow. All of the GANs have trouble learning to generate the full variabilityof the target distribution, especially from out-of-sample data. The autoencoder does not even makethe slight transformation necessary, and instead leaves the data unchanged. Neuron editing resultsin meaningful edits that best models the true combination.
Figure 6: A comparison of real and predicted means and variances per dimension. Neuron editingmore accurately predicts the change in mean than the GAN and better preserves the true variancethan the GAN. The GAN almost uniformly generates lower variance data than really exist.
