Figure 1: The MCFS problem With class hierarchy information. There are a few coarse classes (blue), but eachcoarse class contains a large number of fine classes (red), and the total number of fine classes is large. Only afew training samples are available for each fine class. The goal is to train a classifier to generate a predictionover all fine classes. In meta-learning, each task is an MCFS problem sampled from a certain distribution. Themeta-learner,s goal is to help train a classifier for any sampled task with better adaptation to few-shot data.
Figure 2: Left: MahiNet. The final fine-class prediction combines predictions based on both fine classes andcoarse classes, each of which is produced by an MLP classifier or/and an attention-based KNN classifier. Topright: KNN classifier with learnable similarity metric and updatable support set. Attention provides a similar-ity metric aj,k between each input sample f and a small support set per class stored in memory Mj,k. Thelearning of KNN classifier aims to optimize 1) the similarity metric parameterized by the attention, detailed inSec. 2.3; and 2) a small support set of feature vectors per class stored in memory, detailed in Sec. 2.4. Bottomright: The memory update mechanism. In meta-learning, the memory stores the features of all training samplesof a task. In supervised learning, the memory is updated during training as follows: for each sample Xi withinan epoch, if the KNN classifier produces correct prediction, f will be merged into the memory; otherwise, fwill be written into a “cache”. At the end of each epoch, we apply clustering to the samples per class stored inthe cache, and use the resultant centroids to replace r slots of the memory with the smallest utility rate.
Figure 3: The training loss and accuracy for 50 way 5 shot relation net during the first 100 iterationsunder different learning rate. Whatever the learning rate is, it quickly converges to a suboptimalpoint: The training loss: ≈0.02 and the training accuracy: ≈2%.
Figure 4: The t-SNE visualization for memory. We randomly sample 50 classes shown as differentcolors. The image feature and the memory feature share the same color while the image feature hashigher transparency.
