Figure 1: Pong Primecomputation (i.e. infinite number of rollouts). However, when a model is estimated from the dataand computational power is bounded, prior works has suggested how to adjust planning horizon toget the best computational performance (Jiang et al., 2015). As of our knowledge there is no workthat additionally investigate the performance of different exploration methods with approximateplanning.
Figure 2: Macro actions(c) Per Action Entropy ofAgent’s Model4	Approximately deterministic model to support fast learningIn the previous section we considered how to plan and perform deep exploration given a transitionand reward models. Now we ask a natural question, what types of model to learn?, and how thesemodels will affect computational tractability of planning and sample efficient learning? In generalsimpler models are easier to learn and requires less data to train, in contrast to complex functionapproximation methods that often requires massive amount of data and suffer from compoundingerrors in lookahead planning (Roderick et al., 2017; Weber et al., 2017). Additionally, simple modelsare tractable to perform deep exploration with model uncertainty and reduce planning time.
Figure 3: Model selection4.2	Model LearningGiven that we are planning at an object level, we hypothesize that even simple models, such as linearand discrete count based models, give sufficient accuracy for planning, since often objects follow avery simple physical laws. More importantly, to ensure ”sufficient accuracy in planning”, we furtherrequire that these models predict transitions and rewards in a deterministic fashion.
Figure 4: performance of SOORL on Pitfall!As described in section 3 we used optimism based exploration method, by assigning reward Rmaxto all unseen interactions and transitions. As we observe reward for each interaction we update themodel, based on model based interval estimation (Strehl & Littman, 2008). Additionally in orderto further incentivize exploration we split the screen into N × M grids and keeping a count of thenumber of times agent visits each grid. The agent is given a reward bonus βn(s)-1/2 based on visitcount n(s).
