Figure 1: 2D example of the objective spacewhere the generator loss is being optimized.
Figure 2: Box-plots corresponding to 30 indepen-dent FID computations with 10000 images. MGDperforms consistently better than other methods,followed by hypervolume maximization. Modelsthat achieved minimum FID at train time wereused. Red and blue dashed lines are the FIDs of arandom generator and real data, respectively.
Figure 3: Time vs. best FID achieved during train-ing for each approach. FID values are computedover 1000 generated images after every epoch.
Figure 4: Norm of the update direction over timefor each method. While Pareto-stationarity isapproximately achieved by all methods, perfor-mance varies relevantly in terms of FID.
Figure 5: Independent FID evaluations for modelsobtained with different runs using distinct slackparameter Î´. Sensitivity reduces as the number ofdiscriminators increases.
Figure 6: Box-plots of 15 independent FID com- Figure 7: FID estimated over 1000 generated im-putations with 10000 images. Dashed lines are ages at train time. Models trained against morereal data (blue) and random generator (red) FIDs. discriminators achieve lower FID.
Figure 8: Stacked MNIST samples for HV trained with 8, 16, and 24 discriminators. Samplesdiversity increases greatly when more discriminators are employed.
Figure 9: Norm of the update direction over time for each method. Higher number of discriminatorsyield loWer norm upon convergence.
Figure 10: Time in seconds per iteration of each method for serial updates of discriminators. Multiplediscriminators approaches considered do not present relevant difference in time per iteration.
Figure 11: CIFAR-10 samples for AVG trained with 8, 16, and 24 discriminators.
Figure 13: CIFAR-10 samples for HV trained with 8, 16, and 24 discriminators.
Figure 12: CIFAR-10 samples for GMAN trained with 8, 16, and 24 discriminators.
Figure 14: CelebA samples for AVG trained with 8, 16, and 24 discriminators.
Figure 15: CelebA samples for GMAN trained with 8, 16, and 24 discriminators.
Figure 16: CelebA samples for HV trained with 8, 16, and 24 discriminators.
Figure 17: 128x128 CelebA samples for HV trained during 24 epochs with 6, 8, and 10 discriminators.
Figure 18: Cats generated using 24 discriminators after 288 training epochs.
Figure 19: Models trained with AVG during 15 epochs using an increasing number of randomprojections and discriminators.
Figure 20:	Losses and nadir point at begin-ning of training.
Figure 21:	Losses and nadir point at t = T ,and nadir point at t = 0 (in red).
Figure 22: Minimum FID during training. X-axis is in minutes. The blue dot is intended to highlightthe moment during training when the minimum FID was reached.
