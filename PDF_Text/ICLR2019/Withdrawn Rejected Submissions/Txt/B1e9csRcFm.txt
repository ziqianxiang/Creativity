Under review as a conference paper at ICLR 2019
The Importance of Norm Regularization in Lin-
ear Graph Embedding: Theoretical Analysis
and Empirical Demonstration
Anonymous authors
Paper under double-blind review
Ab stract
Learning distributed representations for nodes in graphs is a crucial primitive in
network analysis with a wide spectrum of applications. Linear graph embedding
methods learn such representations by optimizing the likelihood of both positive
and negative edges while constraining the dimension of the embedding vectors.
We argue that the generalization performance of these methods is not due to
the dimensionality constraint as commonly believed, but rather the small norm of
embedding vectors. Both theoretical and empirical evidence are provided to support
this argument: (a) we prove that the generalization error of these methods can be
bounded by limiting the norm of vectors, regardless of the embedding dimension;
(b) we show that the generalization performance of linear graph embedding methods
is correlated with the norm of embedding vectors, which is small due to the early
stopping of SGD and the vanishing gradients. We performed extensive experiments
to validate our analysis and showcased the importance of proper norm regularization
in practice.
1	Introduction
Graphs have long been considered as one of the most fundamental structures that can naturally
represent interactions between numerous real-life objects (e.g., the Web, social networks, protein-
protein interaction networks). Graph embedding, whose goal is to learn distributed representations
for nodes while preserving the structure of the given graph, is a fundamental problem in network
analysis that underpins many applications. A handful of graph embedding techniques have been
proposed in recent years (Perozzi et al., 2014; Tang et al., 2015b; Grover & Leskovec, 2016), along
with impressive results in applications like link prediction, text classification (Tang et al., 2015a), and
gene function prediction (Wang et al., 2015).
Linear graph embedding methods preserve graph structures by converting the inner products of the
node embeddings into probability distributions with a softmax function (Perozzi et al., 2014; Tang
et al., 2015b; Grover & Leskovec, 2016). Since the exact softmax objective is computationally
expensive to optimize, the negative sampling technique (Mikolov et al., 2013) is often used in these
methods: instead of optimizing the softmax objective function, we try to maximize the probability of
positive instances while minimizing the probability of some randomly sampled negative instances.
It has been shown that by using this negative sampling technique, these graph embedding methods
are essentially computing a factorization of the adjacency (or proximity) matrix of graph (Levy &
Goldberg, 2014). Hence, it is commonly believed that the key to the generalization performance of
these methods is the dimensionality constraint.
However, in this paper we argue that the key factor to the good generalization of these embedding
methods is not the dimensionality constraint, but rather the small norm of embedding vectors. We
provide both theoretical and empirical evidence to support this argument:
•	Theoretically, we analyze the generalization error of two linear graph embedding hypothesis
spaces (restricting embedding dimension/norm), and show that only the norm-restricted hypothesis
class can theoretically guarantee good generalization in typical parameter settings.
•	Empirically, we show that the success of existing linear graph embedding methods (Perozzi et al.,
2014; Tang et al., 2015b; Grover & Leskovec, 2016) are due to the early stopping of stochastic
1
Under review as a conference paper at ICLR 2019
gradient descent (SGD), which implicitly restricts the norm of embedding vectors. Furthermore,
with prolonged SGD execution and no proper norm regularization, the embedding vectors can
severely overfit the training data.
Paper Outline
The rest of this paper is organized as follows. In Section 2, we review the definition of graph
embedding problem and the general framework of linear graph embedding. In Section 3, we present
both theoretical and empirical evidence to support our argument that the generalization of embedding
vectors is determined by their norm. In Section 4, we present additional experimental results for a
hinge-loss linear graph embedding variant, which further support our argument. In Section 5, we
discuss the new insights that we gained from previous results. Finally in Section 6, we conclude our
paper. Details of the experiment settings, algorithm pseudo-codes, theorem proofs and the discussion
of other related work can all be found in the appendix.
2	Preliminiaries
2.1	The Graph Embedding Problem
We consider a graph G = (V, E), where V is the set of nodes in G, and E is the set of edges between
the nodes in V . For any two nodes u, v ∈ V , an edge (u, v) ∈ E if u and v are connected, and we
assume all edges are unweighted and undirected for simplicity1. The task of graph embedding is to
learn a D-dimensional vector representation xu for each node u ∈ V such that the structure of G
can be maximally preserved. These embedding vectors can then be used as features for subsequent
applications (e.g., node label classification or link prediction).
2.2	The Linear Graph Embedding Framework
Linear graph embedding (Tang et al., 2015b; Grover & Leskovec, 2016) is one of the two major
approaches for computing graph embeddings 2. These methods use the inner products of embedding
vectors to capture the likelihood of edge existence, and are appealing to practitioners due to their
simplicity and good empirical performance. Formally, given a node u and its neighborhood N+ (u) 3,
the probability of observing node v being a neighbor of u is defined as:
p(v |u)
eχp(XT Xv)
P eχp(XTXk)
k∈V
By minimizing the KL-divergence between the embedding-based distribution and the actual neigh-
borhood distribution, the overall objective function is equivalent to:
L = -	log p(v|u)
u∈V v∈N+ (u)
Unfortunately, it is quite problematic to optimize this objective function directly, as the softmax term
involves normalizing over all vertices. To address this issue, the negative sampling (Mikolov et al.,
2013) technique is used to avoid computing gradients over the full softmax function. Intuitively, the
negative sampling technique can be viewed as randomly selecting a set of nodes N- (u) that are
not connected to each node u as its negative neighbors. The embedding vectors are then learned by
minimizing the following objective function instead:
1All linear graph embedding methods discussed in this paper can be generalized to weighted case by
multiplying the weight to the corresponding loss function of each edge. The directed case is usually handled
by associating each node with two embedding vectors for incoming and outgoing edges respectively, which is
equivalent as learning embedding on a transformed undirected bipartite graph.
2The other major approach is to use deep neural network structure to compute the embedding vectors, see the
discussion of other related works in the appendix for details.
3Note that N+(u) can be either the set of direct neighbors in the original graph G (Tang et al., 2015b), or an
expanded neighborhood based on measures like random walk (Grover & Leskovec, 2016).
2
Under review as a conference paper at ICLR 2019
L=-x x logσ(XTXv)—x x K |N+(u)| logσ(-xTXv).	⑴
u v∈N+(u)	u v∈N-(u)	-
where σ(x) = 1/(1 + e-x) is the standard logistic function.
2.3	The Matrix Factorization Interpretation
Although the embedding vectors learned through negative sampling do have good empirical per-
formance, there is very few theoretical analysis of such technique that explains the good empirical
performance. The most well-known analysis of negative sampling was done by Levy & Goldberg
(2014), which claims that the embedding vectors are approximating a low-rank factorization of the
PMI (Pointwise Mutual Information) matrix.
More specifically, the key discovery of Levy & Goldberg (2014) is that when the embedding
dimension is large enough, the optimal solution to Eqn (1) recovers exactly the PMI matrix (up to a
shifted constant, assuming the asymptotic case whereN-(u) = V for all u ∈ V):
∀u, v, XuT Xv = log(
|E|，1(u,v)∈E
|N+(u)||N+(v)|
Based on this result, Levy & Goldberg (2014) suggest that optimizing Eqn (1) under the dimensionality
constraint is equivalent as computing a low-rank factorization of the shifted PMI matrix. This is
currently the mainstream opinion regarding the intuition behind negative sampling. Although Levy
and Goldberg only analyzed negative sampling in the context of word embedding, it is commonly
believed that the same conclusion also holds for graph embedding (Qiu et al., 2018).
3	The Importance of Norm Regularization
As explained in Section 2.3, it is commonly believed that linear graph embedding methods are ap-
proximating a low-rank factorization of PMI matrices. As such, people often deem the dimensionality
constraint of embedding vectors as the key factor to good generalization (Tang et al., 2015b; Grover
& Leskovec, 2016). However, due to the sparsity of real-world networks, the explanation of Levy
& Goldberg is actually very counter-intuitive in the graph embedding setting: the average node
degree usually only ranges from 10 to 100, which is much less than the typical value of embedding
dimension (usually in the range of 100 〜400). Essentially, this means that in the context of graph
embedding, the total number of free parameters is larger than the total number of training data points,
which makes it intuitively very unlikely that the negative sampling model (i.e., Eqn (1)) can inherently
guarantee the generalization of embedding vectors in such scenario, and it is much more plausible if
the observed good empirical performance is due to some other reason.
In this paper, we provide a different explanation to the good empirical performance of linear graph
embedding methods: we argue that the good generalization of linear graph embedding vectors is due
to their small norm, which is in turn caused by the vanishing gradients during the stochastic gradient
descent (SGD) optimization procedure. We provide the following evidence to support this argument:
• In Section 3.1, we theoretically analyze the generalization error of two linear graph embed-
ding variants: one has the standard dimensionality constraints, while the other restricts the
vector norms. Our analysis shows that:
-	The embedding vectors can generalize well to unseen data if their average squared l2
norm is small, and this is always true regardless of the embedding dimension choice.
-	Without norm regularization, the embedding vectors can severely overfit the training
data if the embedding dimension is larger than the average node degree.
• In Section 3.2, we provide empirical evidence that the generalization of linear graph embed-
ding is determined by vector norm instead of embedding dimension. We show that:
-	In practice, the average norm of the embedding vectors is small due to the early stopping
of SGD and the vanishing gradients.
-	The generalization performance of embedding vectors starts to drop when the average
norm of embedding vectors gets large.
-	The dimensionality constraint is only helpful when the embedding dimension is very
small (around 5 ~ 10) and there is no norm regularization.
3
Under review as a conference paper at ICLR 2019
3.1 Generalization Analysis of Two Linear Graph Embedding Variants
In this section, we present a generalization error analysis of linear graph embedding based on
the uniform convergence framework (Bartlett & Mendelson, 2002), which bounds the maximum
difference between the training and generalization error over the entire hypothesis space. We assume
the following statistical model for graph generation: there exists an unknown probability distribution
Q over the Cartesian product V × U of two vertex sets V and U. Each sample (a, b) from Q denotes
an edge connecting a ∈ V and b ∈ U.The set of (positive) training edges E+ consists of the first
m i.i.d. samples from the distribution Q, and the negative edge set E- consists of i.i.d. samples
from the uniform distribution U over V × U. The goal is to use these samples to learn a model that
generalizes well to the underlying distribution Q. We allow either V = U for homogeneous graphs
or V ∩ U = 0 for bipartite graphs.
Denote E± = {(a, b, +1) : (a, b) ∈ E+} ∪ {(a, b, -1) : (a, b) ∈ E-} to be the collection of all
training data, and we assume that data points in E± are actually sampled from a combined distribution
P over V × U × {±1} that generates both positive and negative edges. Using the above notations, the
training error Lt (x) and generalization error Lg (x) of embedding x : (U ∪ V) → RD are defined as
follows:
Lt(x)
∣E11q	X - log σ(yxT Xb)
± (a,b,y)∈E±
Lg (X) = -E(a,b,y)〜P log σ(yxTXb)
In the uniform convergence framework, we try to prove the following statement:
Pr( sup (Lg (X) - Lt(X)) ≤ ) ≥ 1 - δ
x∈H
which bounds the maximum difference between Lt(X) and Lg(X) over all possible embeddings X in
the hypothesis space H. If the above uniform convergence statement is true, then minimizing the
training error Lt(X) would naturally lead to small generalization error Lg(X) with high probability.
Now we present our first technical result, which follows the above framework and bounds the
generalization error of linear graph embedding methods with norm constraints:
Theorem 1. [Generalization of Linear Graph Embedding with Norm Constraints] Let E± =
{(a1, b1, y1), (a2, b2, y2), . . . , (am+m0, bm+m0, ym+m0)} be i.i.d. samples from a distribution P over
V × U × {±1}. Let X : (U ∪ V) → RD to be the embedding for nodes in the graph. Then for any
bounded 1-Lipschitz loss function l : R → [0, B] and CU, CV > 0, with probability 1 - δ (over the
sampling ofE±), the following inequality holds
E(a,b,y)〜P /MT Xb) ≤ -ɪʒ mXm 叫 XTiXbi )+ 2^CUCV E, || A, ∣∣2 +4B∖∕2手百⑵
a m +m0	ai i	m +m0	m +m0
i=1
for all embeddings X satisfying
XkXuk2 ≤CU,XkXvk2 ≤CV
where ∣∣A, ∣∣2 is the spectral norm ofthe randomized adjacency matrix A, defined asfollows:
A (i j)	σij	∃y,	(ui, vj, y)	∈E±
, ,	0	∀y,	(ui, vj, y)	∈/	E±
in which σij are i.i.d. Rademacher random variables.
The proof can be found in the appendix. Intuitively, Theeorem 1 states that with sufficient norm
regularization, linear graph embedding can generalize well regardless of the embedding dimension
(note that term D does not appear in Eqn (2) at all). Theorem 1 also characterizes the importance
of choosing proper regularization in lnorm restricted inear graph embedding: in Eqn (2), both the
training error term m+m P=m' l(yiXTiXbi) and the gap term 2mCUCVE,∣∣A,∣∣2 are dependent
on the value of CU and CV. With larger C values (i.e., weak norm regularization), the training error
would be smaller due to the less restrictive hypothesis space, but the gap term would larger, meaning
that the model will likely overfit the training data. Meanwhile, smaller C values (i.e., strong norm
4
Under review as a conference paper at ICLR 2019
regularization) would lead to more restrictive models, which will not overfit but have larger training
error as trade-off. Therefore, choosing the most proper norm regularization is the key to achieving
optimal generalization performance. A rough estimate of Eσ∣∣Aσ ∣∣2 Can be found in the appendix for
interested readers.
On the other hand, if we restrict only the embedding dimension (i.e., no norm regularization on
embedding vectors), and the embedding dimension is larger than the average degree of the graph,
then it is possible for the embedding vectors to severely overfit the training data. The following
example demonstrates this possibility on a d-regular graph, in which the embedding vectors can
always achieve zero training error even when the edge labels are randomly placed:
Claim 1. Let G = (V, E) be a d-regular graph with n vertices and m = nd/2 labeled edges (with
labels yi ∈ {±1}):
V = {v1, . . . ,vn} E = {(a1,b1,y1), . . . , (am,bm,ym)}
Then for each of the 2m possible label combinations, there exists a embedding x : V → RD with
dimension D = d that achieves perfect classification accuracy on the randomized training dataset:
∀(y1,...,ym) ∈ {±1}m, ∃x: V →RD
s.t. ∀i ∈ {1, . . . , m},	yixaTixbi > 1
The proof can be found in the appendix. In other words, without norm regularization, the number
of training samples required for learning D-dimensional embedding vectors is at least Ω(nD).
Considering the fact that many large-scale graphs are sparse (with average degree < 20) and the
default embedding dimension commonly ranges from 100 to 400, it is highly unlikely that the the
dimensionality constraint by itself could lead to good generalization performance.
3.2 Empirical Evidence on the Cause of Generalization
In this section, we present several sets of experimental results for the standard linear graph embedding,
which collectively suggest that the generalization of these methods are actually determined by vector
norm instead of embedding dimension.
Experiment Setting: We use stochastic gradient descent (SGD) to minimize the following objective:
L = -λ+1	log σ(xuT xv) - λ-1	log σ(-xTu xv) + λr	||xv||22
(u,v)∈E+	(u,v)∈E-	v∈V
Here E+ is the set of edges in the training graph, and E- is the set of negative edges with both
ends sampled uniformly from all vertices. The SGD learning rate is standard: γt = (t + c)-1/2.
Three different datasets are used in the experiments: Tweet, BlogCatalog and YouTube, and
their details can be found in the appendix. The default embedding dimension is D = 100 for all
experiments unless stated otherwise.
(a) Tweet
(b) BlogCatalog
Figure 1: Average l2 Norm during SGD Optimization
(c) YouTube
SGD Optimization Results in Small Vector Norm: Figure 1 shows the average l2 norm of the
embedding vectors during the first 50 SGD epochs (with varying value of λr). As we can see, the
average norm of embedding vectors increases consistently after each epoch, but the increase rate gets
slower as time progresses. In practice, the SGD procedure is often stopped after 10 〜50 epochs
(especially for large scale graphs with millions of vertices4), and the relatively early stopping time
would naturally result in small vector norm.
4Each epoch of SGD has time complexity O(|E|D), where D is the embedding dimensionality (usually
around 100). Therefore in large scale graphs, even a single epoch would require billions of floating point
operations.
5
Under review as a conference paper at ICLR 2019
E-ION OS
—∙- λr= 0
-⅛- Λr= 0.2
—λr = 0.4
N= 0.6
E-ION OS 9σ>e-l<u>4
E-ION 9S 36e-13>4
Number of Epochs	Number of Epochs	Number of Epochs
(a) Tweet	(b) BlogCatalog	(c) YouTube
Figure 2: Average Norm of Stochastic Gradients during SGD Optimization
The Vanishing Gradients: Figure 2 shows the average l2 norm of the stochastic gradients ∂L∕∂xu
during the first 50 SGD epochs:
∂L
∂xu
-	σ(-xuTxv)xv +	σ(xuTxv)xv + 2λrxu
v∈N+ (u)	v∈N- (u)
(3)
From the figure, we can see that the stochastic gradients become smaller during the later stage of
SGD, which is consistent with our earlier observation in Figure 1. This phenomenon can be intuitively
explained as follows: after a few SGD epochs, most of the training data points have already been well
fitted by the embedding vectors, which means that most of the coefficients σ(±xuTxv) in Eqn (3) will
be close to 0 afterwards, and as a result the stochastic gradients will be small in the following epochs.
5 0 5 0 5
7 7 6 6 5
α αo.αo.
UO-S-Ed ωwsφ><
Number of Epochs
(b) BlogCatalog (AP)
(a) Tweet (AP)
(c) YouTube (AP)
(d) BlogCatalog (F1)
Figure 3: Generalization Performance During SGD
Regularization and Early Stopping: Figure 3 shows the generalization performance of embedding
vectors during the first 50 SGD epochs, in which we depicts the resulting average precision (AP)
score5 for link prediction and F1 score for node label classification. As we can see, the generalization
performance of embedding vectors starts to drop after 5 〜20 epochs when λr is small, indicating
that they are overfitting the training dataset afterwards. The generalization performance is worst near
the end of SGD execution when λr = 0, which coincides with the fact that embedding vectors in that
case also have the largest norm among all settings. Thus, Figure 3 and Figure 1 collectively suggest
that the generalization of linear graph embedding is determined by vector norm.
Impact of Embedding Dimension Choice: Figure 4 shows the generalization AP score on Tweet
dataset with varying value of λr and embedding dimension D after 50 epochs. As we can see in
Figure 4, without any norm regularization (λr = 0), the embedding vectors will overfit the training
dataset for any D greater than 10, which is consistent with our analysis in Claim 1. On the other hand,
with larger λr , the impact of embedding dimension choice is significantly less noticeable, indicating
that the primary factor for generalization is the vector norm in such scenarios.
5Average Precision (AP) evaluates the performance on ranking problems: we first compute the precision and
recall value at every position in the ranked sequence, and then view the precision p(r) as a function of recall r.
The average precision is then computed as AveP = R01 p(r)dr.
6
Under review as a conference paper at ICLR 2019
(b) λr = 0.2
(c) λr = 0.6
Figure 4: Impact of Embedding Dimension
Embedding Dimension
(d) λr = 1
4 Demonstrating the Importance of Norm Regularization via
Hinge-Loss Linear Graph Embedding
In this section, we present the experimental results for a non-standard linear graph embedding
formulation, which optimizes the following objective:
L =λ+1	X	h(xTuxv)	+	λ-1	x	h(-χTXv)+λ2r	X ||xv||2	⑷
(u,v)∈E+	(u,v)∈E-	v∈V
By replacing logistic loss with hinge-loss, it is now possible to apply the dual coordinate descent
(DCD) method (Hsieh et al., 2008) for optimization, which circumvents the issue of vanishing
gradients in SGD, allowing us to directly observe the impact of norm regularization. More specifically,
consider all terms in Eqn (4) that are relevant to a particular vertex u:
L(U)=	X	λyi max(1 - yiXTXi,。) + 1|匕『.	(5)
λr	2
(xi,yi)∈D
in which we defined D = {(Xv, +1) : v ∈ N+(u)} ∪ {(Xk, -1) : k ∈ N-(u)}. Since Eqn (5)
takes the same form as a soft-margin linear SVM objective, with Xu being the linear coefficients
and (Xi , yi ) being training data, it allows us to use any SVM solver to optimize Eqn (5), and then
apply it asynchronously on the graph vertices to update their embeddings. The pseudo-code for the
optimization procedure using DCD can be found in the appendix.
2	3	4	5	6
Regularization Coefficient
Regularization Coefficient
Regularization Coefficient
(a) Tweet	(b) BlogCatalog
(c) YouTube
Figure 5: Generalization Average Precision with Varying λr (D = 100)
Impact of Regularization Coefficient: Figure 5 shows the generalization performance of embedding
vectors obtained from DCD procedure (〜20 epochs). As We can see, the quality of embeddings
vectors is very bad when λr ≈ 0, indicating that proper norm regularization is necessary for
generalization. The value of λr also affects the gap betWeen training and testing performance, Which
is consistent With our analysis that λr controls the model capacity of linear graph embedding.
Impact of Embedding Dimension Choice: The choice of embedding dimension D on the other
hand is not very impactful as demonstrated in Figure 6: as long as D is reasonably large (≥ 30),
the exact choice has very little effect on the generalization performance. Even With extremely large
embedding dimension setting (D = 1600). These results are consistent With our theory that the
generalization of linear graph embedding is primarily determined by the norm constraints.
7
Under review as a conference paper at ICLR 2019
UqMEd
(a) Tweet	(b) BlogCatalog	(c) YouTube
Figure 6: Generalization Average Precision with Varying D (λr = 3)
5	Discussion
So far, we have seen many pieces of evidence supporting our argument, suggesting that the general-
ization of embedding vectors in linear graph embedding is determined by the vector norm. Intuitively,
it means that these embedding methods are trying to embed the vertices onto a small sphere centered
around the origin point. The radius of the sphere controls the model capacity, and choosing proper
embedding dimension allows us to control the trade-off between the expressive power of the model
and the computation efficiency.
Note that the connection between norm regularization and generalization performance is actually
very intuitive. To see this, let us consider the semantic meaning of embedding vectors: the probability
of any particular edge (u, v) being positive is equal to
T
Pr(y = 1lu,v) = σ(XT Xv ) = σ( N~~∏urf	llxull2llxv ||2)
||xu||2||xv||2
As we can see, this probability value is determined by three factors:
•	XTu Xv/(||Xu||2||Xv||2), the cosine similarity between Xu and Xv, evaluates the degree of
agreement between the directions of Xu and Xv .
•	||Xu||2 and ||Xv ||2 on the other hand, reflects the degree of confidence we have regarding
the embedding vectors of u and v.
Therefore, by restricting the norm of embedding vectors, we are limiting the confidence level that we
have regarding the embedding vectors, which is indeed intuitively helpful for preventing overfitting.
It is worth noting that our results in this paper do not invalidate the analysis of Levy & Goldberg
(2014), but rather clarifies on some key points: as pointed out by Levy & Goldberg (2014), linear
graph embedding methods are indeed approximating the factorization of PMI matrices. However, as
we have seen in this paper, the embedding vectors are primarily constrained by their norm instead of
embedding dimension, which implies that the resulting factorization is not really a standard low-rank
one, but rather a low-norm factorization:
XuT Xv ≈ PMI(u, v) s.t. X ||Xu||22 ≤ C
u
The low-norm factorization represents an interesting alternative to the standard low-rank factorization,
and our current understanding of such factorization is still very limited. Given the empirical success
of linear graph embedding methods, it would be really helpful if we can have a more in-depth analysis
of such factorization, to deepen our understanding and potentially inspire new algorithms.
6	Conclusion
We have shown that the generalization of linear graph embedding methods are not determined by
the dimensionality constraint but rather the norm of embedding vectors. We proved that limiting the
norm of embedding vectors would lead to good generalization, and showed that the generalization of
existing linear graph embedding methods is due to the early stopping of SGD and vanishing gradients.
We experimentally investigated the impact embedding dimension choice, and demonstrated that such
choice only matters when there is no norm regularization. In most cases, the best generalization
performance is obtained by choosing the optimal value for the norm regularization coefficient, and
in such case the impact of embedding dimension case is negligible. Our findings combined with
the analysis of Levy & Goldberg (2014) suggest that linear graph embedding methods are probably
computing a low-norm factorization of the PMI matrix, which is an interesting alternative to the
standard low-rank factorization and calls for further study.
8
Under review as a conference paper at ICLR 2019
References
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.
Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps and spectral techniques for embedding and
clustering. In NIPS, volume 14,pp. 585-591, 2001.
Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In KDD, pp.
855-864, 2016.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In
Advances in Neural Information Processing Systems, pp. 1024-1034, 2017.
Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin, S. Sathiya Keerthi, and S. Sundararajan. A dual
coordinate descent method for large-scale linear SVM. In ICML, pp. 408-415, 2008.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.
arXiv preprint arXiv:1609.02907, 2016.
Joseph B Kruskal and Myron Wish. Multidimensional scaling, volume 11. Sage, 1978.
Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In Advances
in neural information processing systems, pp. 2177-2185, 2014.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Distributed
representations of words and phrases and their compositionality. In NIPS, pp. 3111-3119, 2013.
Alan Mislove, Massimiliano Marcon, Krishna P. Gummadi, Peter Druschel, and Bobby Bhattacharjee.
Measurement and Analysis of Online Social Networks. In Proceedings of the 5th ACM/Usenix
Internet Measurement Conference (IMC’07), San Diego, CA, October 2007.
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representa-
tions. In KDD, pp. 701-710, 2014.
Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, and Jie Tang. Network embedding as
matrix factorization: Unifying deepwalk, line, pte, and node2vec. In Proceedings of the Eleventh
ACM International Conference on Web Search and Data Mining, pp. 459-467. ACM, 2018.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to
algorithms. Cambridge university press, 2014.
Nathan Srebro, Jason Rennie, and Tommi S Jaakkola. Maximum-margin matrix factorization. In
Advances in neural information processing systems, pp. 1329-1336, 2005.
Jian Tang, Meng Qu, and Qiaozhu Mei. Pte: Predictive text embedding through large-scale heteroge-
neous text networks. In KDD, pp. 1165-1174, 2015a.
Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Large-scale
information network embedding. In WWW, pp. 1067-1077, 2015b.
Joshua B Tenenbaum, Vin De Silva, and John C Langford. A global geometric framework for
nonlinear dimensionality reduction. science, 290(5500):2319-2323, 2000.
Daixin Wang, Peng Cui, and Wenwu Zhu. Structural deep network embedding. In KDD, pp.
1225-1234. ACM, 2016.
Sheng Wang, Hyunghoon Cho, ChengXiang Zhai, Bonnie Berger, and Jian Peng. Exploiting ontology
graph for predicting sparsely annotated gene function. Bioinformatics, 31(12):i357-i364, 2015.
R. Zafarani and H. Liu. Social computing data repository at ASU, 2009. URL http://
socialcomputing.asu.edu.
9
Under review as a conference paper at ICLR 2019
Appendix
Datasets and Experimental Protocols
We use the following three datasets in our experiments:
•	Tweet is an undirected graph that encodes keyword co-occurrence relationships using
TWitter data: We collected ~1.1 million English tweets using Twitter's Streaming API
during 2014 August, and then extracted the most frequent 10,000 keywords as graph nodes
and their co-occurrences as edges. All nodes with more than 2,000 neighbors are removed
as stop words. There are 9,913 nodes and 681,188 edges in total.
•	BlogCatalog (Zafarani & Liu, 2009) is an undirected graph that contains the social
relationships between BlogCatalog users. It consists of 10,312 nodes and 333,983 undirected
edges, and each node belongs to one of the 39 groups.
•	YouTube (Mislove et al., 2007) is a social network among YouTube users. It includes
500,000 nodes and 3,319,221 undirected edges6.
For each positive edge in training and testing datasets, we randomly sampled 4 negative edges, which
are used for learning the embedding vectors (in training dataset) and evaluating average precision (in
testing dataset). In all experiments, λ+ = 1, λ- = 0.03, which achieves the optimal generalization
performance according to cross-validation. All initial coordinates of embedding vectors are uniformly
sampled form [-0.1, 0.1].
Other Related Works
In the early days of graph embedding research, graphs are only used as the intermediate data model
for visualization (Kruskal & Wish, 1978) or non-linear dimension reduction (Tenenbaum et al., 2000;
Belkin & Niyogi, 2001). Typically, the first step is to construct an affinity graph from the features of
the data points, and then the low-dimensional embedding of graph vertices are computed by finding
the eigenvectors of the affinity matrix.
For more recent graph embedding techniques, apart from the linear graph embedding methods
discussed in this paper, there are also methods (Wang et al., 2016; Kipf & Welling, 2016; Hamilton
et al., 2017) that explore the option of using deep neural network structures to compute the embedding
vectors. These methods typically try to learn a deep neural network model that takes the raw features
of graph vertices to compute their low-dimensional embedding vectors: SDNE (Wang et al., 2016)
uses the adjacency list of vertices as input to predict their Laplacian Eigenmaps; GCN (Kipf &
Welling, 2016) aggregates the output of neighboring vertices in previous layer to serve as input
to the current layer (hence the name “graph convolutional network”); GraphSage (Hamilton et al.,
2017) extends GCN by allowing other forms of aggregator (i.e., in addition to the mean aggregator
in GCN). Interestingly though, all these methods use only 2 or 3 neural network layers in their
experiments, and there is also evidence suggesting that using higher number of layer would result
in worse generalization performance (Kipf & Welling, 2016). Therefore, it still feels unclear to us
whether the deep neural network structure is really helpful in the task of graph embedding.
Prior to our work, there are some existing research works suggesting that norm constrained graph
embedding could generalize well. Srebro et al. (2005) studied the problem of computing norm
constrained matrix factorization, and reported superior performance compared to the standard low-
rank matrix factorization on several tasks. Given the connection between matrix factorization and
linear graph embedding (Levy & Goldberg, 2014), the results in our paper is not really that surprising.
6Available at http://socialnetworks.mpi-sws.org/data-imc2007.html. We only used the subgraph induced by
the first 500,000 nodes since our machine doesn’t have sufficient memory for training the whole graph. The
original graph is directed, but we treat it as undirected graph as in Tang et al. (2015b).
10
Under review as a conference paper at ICLR 2019
Proof of Theorem 1
Since E± consists of i.i.d. samples from P, by the uniform convergence theorem (Bartlett &
Mendelson, 2002; Shalev-Shwartz & Ben-David, 2014), with probability 1 - δ:
∀x, s.t.	X kxuk2 ≤ CU, Xkxvk2 ≤ CV ,
u∈U	v∈V
E(a,b,y)〜Pl(yxTXb) ≤ m + m0 X l(yixTixbJ + 2R(HCU ,Cv ) + 4B ^^^^^
i=1
where HCU,CV = {x : Pu∈U kxu k2 ≤ CU, Pv∈V kxvk2 ≤ CV} is the hypothesis set, and
R(HCU,CV) is the empirical Rademacher Complexity of HCU,CV, which has the following explicit
form:
R(HCU,Cv ) =	2 0 Eσa,b〜{-1,1}	SUp	X 小。IlyxTXbi)
m + m	x∈HCU,CV i
Here σa,b are i.i.d. Rademacher random variables: Pr(σa,b = 1) = Pr(σa,b = -1) = 0.5. Since l
is 1-Lipschitz, based on the Contraction Lemma (Shalev-Shwartz & Ben-David, 2014), we have:
R(HCU,Cv ) ≤	1	o Eσa,b〜{-1,1}	SUp	X "bUixTXbi
m + m	x∈HCU,Cv i
=m+m取小〜一，1} x∈hSUp C X。”机XTXb
x∈HCU ,Cv i
Let us denote XU as the |U |d dimensional vector obtained by concatenating all vectors Xu, and XV
as the |V |d dimensional vector obtained by concatenating all vectors Xv :
XU = (Xu1 , Xu2 , . . . , Xu|U| ) XV = (Xv1 , Xv2 , . . . , Xv|v | )
Then We have:
∣∣Xu ∣∣2 ≤ p/Cu ∣∣Xv ∣∣2 ≤ p/CV
The next step is to rewrite the term Pi σai,bixaT xbi in matrix form:
sUp	σai,bixaTixbi
x∈HCU ,Cv i
=	SUp	XT [Aσ 乳 Id]Xv
|| Xu || 2 ≤ √CU, || Xv || 2 ≤ √cv
= PCUkAσ X Idll2/CV
where A 0 B represents the Kronecker product of A and B, and || A∣∣2 represents the spectral norm
of A (i.e., the largest singular value of A).
Finally, since ||A 0 I||2 = ||A||2, we get the desired result in Theorem 1.
Proof Sketch of Claim 1
We provide the sketch of a constructive proof here.
Firstly, we randomly initialize all embedding vectors. Then for each v ∈ V , consider all the relevant
constraints to Xv :
Cv = {(a, b, y) ∈ E : a = v or b = v}
Since G is d-regular, |Cv | ≤ d. Therefore, there always exists vector b ∈ Rd satisfying the following
|Cv | constraints:
∀(a, b, y) ∈ Cv , yXaXb = 1 +
as long as all the referenced embedding vectors are linearly independent.
Choose any vector b0 in a small neighborhood of b that is not the linear combination of any other
d - 1 embedding vectors (this is always possible since the viable set is a d-dimensional sphere minus
a finite number of d _ 1 dimensional subspaces), and set Xv J b0.
Once we have repeated the above procedure for every node in V , it is easy to see that all the constraints
yXaTXb : (a, b, y) ∈ E are now satisfied.
11
Under review as a conference paper at ICLR 2019
Rough Estimation of ||Aσ ∣∣2 ON ERDOS-RENYI Graph
By the definition of spectral norm, ∣∣Aσ ∣∣2 is equal to:
∣∣Aσ ∣∣2 =	SUP	YTAσX
l∣x∣∣2 = l∣y∣∣2 = l,x,y∈Rn
Note that,
y AσX =	σij yi xj
(i,j)∈E
Now let us assume that the graph G is generated from a Erdos-Renyi model (i.e., the probability of
any pair u, v being directed connected is independent), then we have:
yTAσX =	σij eij yi xj
where eij is the boolean random variable indicating whether (i, j) ∈ E.
By Central Limit Theorem,
ΣΣOijCijyiXj 〜N(0, n)
where m is the expected number of edges, and n is the total number of vertices. Then we have,
22
Pr(yτAσx ≥ t) ≈ O(e- 2m )
for all ||X||2 = ||y||2 = 1.
Now let S be an -net of the unit sphere in n dimensional Euclidean space, which has roughly O(-n)
total number of points. Consider any unit vector x, y ∈ Rn, and let xS , yS be the closest point of
x, y in S, then:
yTAσx =(yS +y - yS)TAσ (xS +x - xS)
=ySTAσ xS + (y - yS)TAσxS +yTSAσ(x - xS) + (y - yS)TAσ (x - xS)
≤ySTAσxS + 2n + 2n
since ||Aσ|| ≤ n is always true.
By union bound, the probability that at least one pair of xS, yS ∈ S satisfying ySTAσ xS ≥ t is at
most:
22
Pr(∃xs, YS ∈ S : yTAσXS ≥ t) ≈ O(e-2ne-^m)
Let E = 1/n, t =，8m ln n/n, then the above inequality becomes:
Pr(∃xS,YS ∈ S: YSTAσxS ≥t) ≈O(e-nlnn)
Since ∀xS, YS ∈ S, YSTAσ xS < t implies that
sup	YTAσ x < t + 2En + E2n
llxll2 = llyll2 = 1,χ,y∈Rn
Therefore, We estimate || Aσ || 2 to be of order O (Ym ln n/n).
Pseudocode of Dual Coordinate Descent Algorithm
Algorithm 1 shows the full pseudo-code of the DCD method for optimizing the hinge-loss variant of
linear graph embedding learning.
12
Under review as a conference paper at ICLR 2019
Algorithm 1 DCD Method for Hinge-Loss Linear Graph Embedding
function DCDUPDATE(u, N+(u), N- (u))
D = {(v, +1) : v ∈ N+(u)}∪{(v,-1) : v ∈ N-(u)}
W — P(v,s)∈D αu SXv
for (v, s) ∈ D do
5:	G — swT Xv — 1
U — λs/λr
fmin(G, 0) if auv = 0
max(G, 0) if αuv = U
G	Otherwise
if P G 6= 0 then
Q = Xv Xv
10：	αuv 4- auv
auv — min(max(auv — G/Q), 0, U)
w — w + (αuv — αuv )sxv
end if
end for
15:	Xu 4 w
end function
function MAIN(V, E+, E- , λ+, λ- , λr)
Randomly initialize Xv for all v ∈ V
Initialize αuv 4 0 for all (u, v) ∈ E+ ∪ E-
20:	for t ∈ 1, . . . , T do
for u ∈ V do
N+ (u) 4	{v ∈ V	:	(u,	v) ∈	E+ }
N- (u) 4	{v ∈ V	:	(u,	v) ∈	E-}
DCDUpdate(u, N+(u), N-(u))
25:	end for
end for
end function
13