Under review as a conference paper at ICLR 2019
Optimizing for Generalization in Machine
Learning with Cross-Validation Gradients
Anonymous authors
Paper under double-blind review
Ab stract
Cross-validation is the workhorse of modern applied statistics and machine learn-
ing, as it provides a principled framework for selecting the model that maximizes
generalization performance. In this paper, we show that the cross-validation risk
is differentiable with respect to the hyperparameters and training data for many
common machine learning algorithms, including logistic regression, elastic-net
regression, and support vector machines. Leveraging this property of differentia-
bility, we propose a cross-validation gradient method (CVGM) for hyperparame-
ter optimization. Our method enables efficient optimization in high-dimensional
hyperparameter spaces of the cross-validation risk, the best surrogate of the true
generalization ability of our learning algorithm.
1	Introduction
The ultimate aim of a supervised learning method is generalization, that is, achieving good predic-
tion ability on unseen test data given only a finite set of training data. The generalization capability
of learning algorithms should be the primary criterion for model selection, yet an algorithm’s gen-
eralization capability is a somewhat elusive quantity that is challenging to optimize for. In this
paper we introduce a method to optimize directly for the closest available proxy to generalization
performance: cross-validation loss.
We begin with a formal description of the overall goal in predictive learning, which also serves as an
introduction to notation used throughout the paper. The task of predictive learning involves deriving
a prediction function from a finite set of training data. More formally, suppose that (x, y) ∈ X × Y
have some joint probability distribution. We have access to a finite dataset of N training examples
zi ∈ Z = X × Y drawn i.i.d. from the joint distribution, denoted
S = {z1,z2, . . . ,zN}.
We are given (or specify ourselves) a cost function c(y,y) : Y × Y → R+ that quantifies the
displeasure incurred when y is predicted instead of y. Denoting the function space from input
to outputs as F = YX , we define the loss of a function on a training example z = (x, y) as
l(f, z) = c(f (x), y). Then, given a prediction function f ∈ F, we define the population risk as
E[l(f, z)],
z
and the target function f * ∈ F as the function that minimizes the population risk. The population
risk represents how much loss we incur, on average, on the full joint distribution, and is the quantity
we would like as small as possible. In this paper, we consider parametric prediction functions, that
is, f is parameterized by a vector θ, denoted f(x; θ)1. For example, in linear regression, X = Rn,
Y = R, c(y, y) = (y - y)2, and F is the set of all affine functions parameterized as f (x; θ)=
θTx + θ0.
We are then tasked with designing a learning algorithm A : ZN → F, which is a function that maps
a dataset S to a prediction function. Without substantial knowledge of the actual joint distribution,
or assumptions about the target function f*, it is extremely unlikely that A will ever reproduce the 1
1Nonparametric learning algorithms exist, e.g., k-nearest neighbor, but are challenging to analyze with our
method.
1
Under review as a conference paper at ICLR 2019
exact target function. However, our goal is to minimize the population risk of the learning algorithm
R(A,S)=E[l(A(S),z)]	(1)
z
which is a random variable that depends on S, our dataset. To make this problem of searching
for learning algorithms tractable, we similarly parameterize our learning algorithm A by a vector
α ∈ Rd, denoted Aα. These are known as the “hyperparameters” or “meta-parameters” of the
learning algorithm, and can play many important roles: they can perform regularization, enforce
sparsity, or even guide feature selection (Hastie et al., 2001). The quantity we would then like to
optimize is the expected population risk, or
L(α) =E[R(Aα,S)] .	(2)
It is impossible to exactly calculate (2) with a finite dataset S, as there are two expectations that both
involve an unknown probability distribution. What we can do is construct a Monte Carlo estimate
of the an algorithm’s expected population risk using a technique known as cross-validation. We first
partition S into K partitions Tj, Vj,j = 1,...,K (that is, Tj ∩ Vj = 0 and Tj ∪ Vj = [n]). Then
our cross-validation risk, as a function of α, is
1K1
Lcvg) = K	∣V^∣,ΣS I(Aa(T ),zi)
(3)
and is readily calculated. We first apply the algorithm to each training set and then average the loss
on each corresponding validation set. The first sum in (3) corresponds to the expectation in (2),
and the second sum corresponds to the expectation in (1). Setting K = 1 reduces to simple out-
of-sample validation and an arbitrary K reduces to the common K-fold cross-validation estimate
(provided Tj form a partition of {1,..., N} and ∣Tj ∣ = N 一 K). Thus, this formulation can be
viewed as a generalization of cross-validation. (See Kohavi & John (1995) for a longer discussion
about this general framework.) In cases where the class of models tobe used for learning are known,
we have reduced the predictive learning problem to the problem of selecting of a hyperparameter
vector α to minimize the cross-validation loss. Even in the simplest cases, however, the objective in
(3) is nonconvex in α, and in many cases not even continuous (e.g., the 0 一 1 classification loss),
which can make optimization of this quantity tricky.
1.1	Summary of Results
Our first result is to demonstrate that We can find VaLcv(α) for many common convex machine
learning algorithms (e.g., logistic regression, elastic-net regression, support vector machines), pro-
vided the cross-validation loss function is differentiable (Section 3). In those algorithms, α often
plays the role of regularizer or defines a feature map (in the case of SVM kernels). In the case Where
α is loW-dimensional, (3) can be optimized by exhaustive search Without incurring too much cost.
HoWever, if We Want to design our machine learning algorithms With more expressive regularizations
or feature maps, exhaustive search over our hyperparameter space becomes prohibitive.
Our second contribution is to propose the cross-validation gradient method (CVGM), Which makes it
possible to optimize cross-validation loss over high-dimensional hyperparameter spaces via gradient
descent techniques (Section 4). We test the CVGM on an elastic-net regression problem to optimize
tWo hyperparameters and on a more ambitious synthetic classification problem to optimize an entire
neural netWork that serves as a kernel function. In this case, the parameters of the neural netWork
are the hyperparameters (Section 5).
2	Related Work
There have been many proposed approaches for the problem of hyperparameter optimization, Which
roughly fall into tWo camps based on Whether or not they use gradients.
2.1	Gradient-Free Methods
Exhaustive Search Exhaustive search, also knoWn as grid search, restricts the possible set of α
to a (finite) set {α1, . . . , αn}, usually by discretizing the parameter search space into a regular grid.
2
Under review as a conference paper at ICLR 2019
Then one exhaustively computes (3) for each αi and chooses the argmin. The main disadvantage
of exhaustive search is that its complexity (to find an approximate minimum) scales exponentially
with the dimension d, making it prohibitive for practitioners to successfully apply exhaustive search
to d greater than 5 or 6.
Random Search Random search for hyperparameters involves repeatedly specifying a probability
distribution over Rd, sampling from it, and evaluating (3). Quite unintuitively, random search can be
more efficient than exhaustive search, even with a simple probability distribution. This is because,
in practice, only a few of the hyperparameter dimensions matter (Bergstra & Bengio, 2012).
Bayesian Optimization Bayesian regression allows us to predict a distribution over Lcv (α), fur-
ther allowing us to query α that maximize a surrogate function, e.g., probability of improvement or
expected improvement (MockUs, 1975; Snoek et al., 2012). The regression is usually carried out
with Gaussian processes (GPs) (Rasmussen, 2004). However, random search still remains a fierce
competitor to the (substantially more complicated) Bayesian optimization approach.
2.2	Gradient-Based Methods
Implicit Differentiation Most learning algorithms Aα are solving some parameterized optimiza-
tion problem, that is, optimizing some objective function. Under certain conditions, one can apply
the well-known implicit function theorem (Dontchev & Rockafellar, 2014) to the optimality condi-
tions of the objective function, and calculate the gradients of the loss function. Larsen et al. (1998)
were the first to propose this, in the context of neural networks, when the objective function includes
a regularization term that is linear in the regularization parameters. Bengio (2000) further derived
the gradients for a general (unconstrained and differentiable) training criterion along with an effi-
cient way of calculating the gradient for a quadratic training criterion, and applied the algorithm to
weight decays for linear regression. These results were then extended to support vector machines
(SVMs) (Chapelle et al., 2002; Keerthi et al., 2007) and applied to log-linear models (Foo et al.,
2008) and ridge regression (Pedregosa, 2016).
This paper seeks to generalize these methods and provide exact conditions under which Aα is ac-
tually differentiable. In short, when Aα is a convex optimization problem parameterized by α,
under certain conditions that are satisfied by many common learning algorithms, we can find exact
cross-validation gradients.
Iterative Differentiation In addition to approaches based on implicit differentiation, there are
also approaches based on iterative differentiation, i.e., they unroll the optimization procedure in A
to calculate gradients. Many large-scale machine learning algorithms perform a variation of gradient
descent, and since gradient descent is a sequence of analytic updates to the parameters, Aα can be
unrolled (or “reverse-mode” differentiated) with respect to α by recursively applying the chain rule
to the updates in backwards order. Domke (2012) was the first to propose this, deriving backpropaga-
tion rules for the heavy-ball method and LBFGS. Since most large-scale machine learning problems
in practice are (approximately) solved using variations of the stochastic subgradient method (also
known as SGD), the “learning rate” parameter has a large impact on the convergence and training
speed of nonconvex models, e.g., neural networks. Maclaurin et al. (2015) extended the results of
Domke to the case of stochastic gradient methods, and as a result, the authors were able to update
the learning rate throughout the learning process. The advantage of these methods are that they can
be applied to any large-scale machine learning problem that uses a stochastic subgradient method.
The main limitations of these methods, however, are that the use of finite precision arithmetic when
recursively applying the chain rule can lead to inaccuracies in the gradient calculation and that one
can encounter exploding or vanishing gradients from repeated application of the chain rule.
3	Exact Differentiability of Learning Algorithms
Recent work by Barratt (2018) provided necessary and sufficient conditions for a parameterized
convex optimization problem to be differentiable. We review the results here, and refer the reader to
3
Under review as a conference paper at ICLR 2019
the paper for more details. The setting is a parameterized convex optimization problem
minimize f0 (x, α)
subject to f (x, α)	0,	(4)
h(x, α) = 0.
where x ∈ Rn is the optimization variable, the functions f0 and f are convex for fixed α and h is
affine for fixed a. Let s(α) = (x, λ, V)T denote the optimal x, λ, V for a given a in (4), where λ
and ν are Lagrange multipliers, i.e., that satisfy the Karush-Kuhn-Tucker (KKT) conditions. Then
define the vector-valued function
g(x, λ,ν,α)
■_ _ ,	~	、r
VχL(x,λ, ν, α)
_ . , ~. 、
diag(ʌ)f (x, α).
h(x, a)
(5)
where L is the Lagrangian. The mam result of the paper is that, for an optimal Z = (x, λ, V)
Vαs(α) = -Vzg(xV, λV, VV, α)-1Vαg(xV, λV, VV, α).	(6)
under the assumption that both fi and g are twice differentiable in x and α, strong duality holds, and
Vαg(xV, λ, VV, α) ∈ R(Vxg(xV, λ, VV, α)). In other words, we can get the derivative of (x, λ, V) with
respect to α. We will focus on the derivative of x with respect to α in this paper, however, it would
be interesting to consider the derivative with respect to the dual variables λ and V .
Since most parametric machine learning procedures can be expressed as parameterized convex pro-
grams that satisfy these conditions2, we can conclude that, in many cases, Aα is in fact differentiable.
In fact, many machine learning procedures can even be expressed as quadratic programs (QPs) —
quadratic objectives with affine inequality and equality constraints — and satisfy the conditions for
differentiability, as shown in Amos & Kolter (2017) (assuming a positive definite quadratic). Fur-
ther, if l is differentiable with respect to the parameters of f, we can use the chain rule to find the
gradient of the cross-validation loss with respect to the hyperparameters. We now present several
examples of predictive learning algorithms that are in fact differentiable with respect to their hy-
perparameters. (Additional examples, including the support vector machine, can be found in the
Supplementary Materials.)
Example 3.1 (Logistic regression). In logistic regression, X = Rn andY = {-1, 1}. As is standard
in classification, we model Prob(x = 1). This probability is represented by the “sigmoid” function
p(x; θ) = ι+eχp1-θτχ) and We minimize a loss function that is proportional to the likelihood of the
dataset under this model plus a regularization term
1N
L(θ,C) = 2 θτ θ + C £log(exp(—yiXT θ) + 1).
i=1
This (convex) optimization problem is unconstrained, so the derivative of the optimal solution with
respect to the hyperparameter C is just
VCA= -Vθ2L(θ, C)-1VC[VθL(θ, C)].
Letting ∏ = ι+eχp(-yiθT Xi),the gradient is
N
VθL(θ, C) =θ+CX(πi-1)yixi
i=1
and the Hessian is
Vθ2L(θ, C) = I+ CXTDX
where D is a diagonal matrix with Dii = πi(1 - πi) and the rows ofX are xi, and is guaranteed to
be positive definite. The righthand side is just
N
VCVθf(θ, C) = X(πi - 1)yixi.
i=1
2Two notable exceptions to this are neural networks and decision trees, which both have nonconvex training
criterions and thus one cannot guarantee finding a global minimum.
4
Under review as a conference paper at ICLR 2019
Algorithm 1 CVGM
Require: α0: Initial hyperparameter vector
Require: K : Number of partitions
Require: p: Fraction of samples in training set
1:	Sample K partitions Tj, Vj, j = 1, . . . , K uniformly at random, where |Tj| = bpNc
2:	while αk not converged do
3:	Run the algorithm Aαk (Tj ) separately on each training set
4:	Calculate the gradient
g — Ra
1K 1
K X 西 X I(Aak (Tj ),zi)
j=1	j i∈Vj
5:	Update αk+1 using a gradient method with the gradient g
6:	Project αk+1 onto the constraint set
7:	end while
We can also take the derivative with respect to the training examples xi (or yi with a similar deriva-
tion) using the fact that
RxiRθf(θ,C) = C(πi - 1)yiI.
Example 3.2 (Elastic-net regression). In regression, X = Rn, Y = R, and c(y, y) = (y 一 y)2.
The function f(x; θ) = θTx, where the intercept term is omitted for illustration. Let the ith row of
the data matrix X be equal to xi and the ith entry of the vector y be equal to yi . Elastic-net regres-
sion generalizes ridge and LASSO regression and optimizes the squared penalty with a weighted
combination of `1 and `2 regularizers (Zou & Hastie, 2005), or solves the optimization problem
minimize 2Nllχθ -yk2 + λιllθkι + 2λ2kθk2.
(7)
The objective is convex, but not differentiable. To transform this into a differentiable parameterized
convex optimization problem, we introduce two variables to represent the positive and negative parts
of θ, denoted θp and θn . Then, letting v = [θp θn]T , elastic-net can be expressed as the following
quadratic program (QP) with 2n variables
minimize 1V
τ 岛XTX + λ2I
’[-寺 X T X
v+
(8)
N X N K
一 N X T y + λ1I
N X T y + λ1I _
T
v
subject to v 0.
Since we can differentiate the solution to positive-definite QPs, we can find the gradient of the
optimal solution θ = θp 一 θn with respect to the hyperparameters λ1 and λ2, provided λ2 > 0.
To the best knowledge of the authors, this is the first derivation of the gradients of the elastic-net
solution with respect to elastic-net’s hyperparameters.
4 Cross-Validation Gradient Method (CVGM)
Building off our findings that the solution to many parametric machine learning procedures are
differentiable with respect to their hyperparameters, we can now design an algorithm to minimize
(3).
The algorithm is summarized in Algorithm 1. The algorithm essentially performs projected gradient
descent on (3), restricting α to a pre-defined constraint set. It runs the learning algorithm on the
training part of each cross-validation split (line 3), then calculates the loss on the held-out part of
each cross-validation split, and then uses the chain rule to calculate their gradients, which are then
averaged (line 4). This averaged gradient is then used to update α in a first-order gradient method
(line 5), and then α is projected back onto the constraint set (line 6). Once we run the CVGM to
find a*, We then run the learning algorithm on the full dataset to find the final prediction function
Aα*(S)∙
5
Under review as a conference paper at ICLR 2019
There are several advantages to this method. First, it directly optimizes the quantity of interest using
a gradient-based method, which can be much faster than exhaustive search. Second, if one is smart
with their implementation, computing the gradient in line 4 of the algorithm costs little on top of
evaluating the function Lcv(α) itself. (See Barratt (2018) and Amos & Kolter (2017) for a discussion
of this.) Third, our method plays well with parallel computation. The majority of computation time
is spent finding the gradient in line 4 of the algorithm. Since the gradient operation is linear, we can
split up K runs of the learning algorithm on the K datasets over K processors or compute nodes
and then average the resulting gradients. Also, the algorithm can be run in parallel with different
random initializations of α to find multiple hyperparameter settings.
There are several immediate improvements that can be made to the CVGM as stated. One improve-
ment would be to make the sampling of cross-validation splits uniform, that is, each index appears
an equal number of times in all of the Vj and Vj . This ensures that each data point shows up an
equal number of times in the cross-validation loss. Another improvement would be to use a more
sophisticated optimization method, e.g., accelerated or adaptive methods, but in our experiments we
just use a gradient method with constant step size and found that it works quite well.
Our method requires two parameters: the number of partitions K (the batch size), and the fraction
of samples to include in the training set partition p. We expect that a value of K between 16 and
128 and p > 1 should work well in almost all scenarios. A larger K leads to reduced variance, and
a larger p leads to a reduced number of examples held out for validation.
5	Numerical Experiments
We evaluate our method on synthetic regression and classification data, noting that further in-depth
comparison on real datasets is needed in future work. One benefit of small synthetic experiments is
that the true population risk is readily calculated, and also we can evaluate the performance of the
method in the low-data regime. All of the code to run our experiments is freely available online3.
5.1	Synthetic Regression Data
First, we evaluate our method on synthetic regression data. There are N = 30 observations and
n = 10 features. However, only 8 of the features have non-zero coefficients. We generate data via
the following scikit-learn (Pedregosa et al., 2011) command:
X, y, coef = make_regression(N, n, n_informative=8, noise=100., \
tail_strength=0., coef=True)
We also generate a test set of 1000 examples with the same command for evaluation. As is standard
practice in machine learning, we normalize the features—that is, we normalize each feature to mean
0 and standard deviation 1 across the training set and then this same normalization is applied to the
validation/test set before prediction.
We run an elastic-net regression (see Example 3.2) to learn a linear prediction function. For simplic-
ity, the (unpenalized) intercept is learned using standard linear regression and then subtracted from
y. For our projection step (line 6 of the algorithm), we require λ2 ≥ , where = 1 × 10-7, and
λ1 ≥ 0. We use K = 128, p = .95, and a gradient descent step size of 2 × 10-4. The method is im-
plemented in PyTorch using the qpth library, which is a fast, batched, and differentiable QP library,
making the algorithm efficient and scalable (Amos & Kolter, 2017). The authors note, however,
that one could create a much faster implementation by making the solver specialized for elastic-net
regression.
We compared the CVGM with exhaustive and random search, noting, however, that the hyperpa-
rameter optimization problem is in two dimensions and exhaustive/random search are likely to be
quite competitive. We ran CVGM with an initial λ1 = 1 × 10-2 and λ2 = 1 × 10-4 for 100 steps.
For exhaustive search, we did a grid search over a log scale for λ1 ∈ [1 × 10-4, 1 × 10-1] and
λ2 ∈ [1 × 10-4, 1 × 10-1], and kept the hyperparameters that achieved the lowest cross-validation
loss. For random search, we sampled uniformly at random in a log scale from the same variable
3www.github.com/anonymous/crossval
6
Under review as a conference paper at ICLR 2019
Table 1: Synthetic Regression Results.
Name	Test Loss	Cross-Validation Steps
CVGM	1.080	100
Exhaustive search	1.207	100
Random search	1.084	100
Figure 1: Top: Synthetic classification training data in two dimensions, along with the learned
decision boundary (black). Bottom: Visualization of kernel applied to 1000 (unseen) test data points
in iterations 1, 10, 20, and 100 from left to right, along with the linear classifier in that space (black).
Note that the kernel quickly learns a manifold where the data is (approximately) linearly separable.
The CVGM achieves 89.8 % test accuracy on the test set with only 60 training examples, and the
Bayes (optimal) accuracy is 89.4 %.
ranges as exhaustive search. The resulting final test losses (at iteration 100) of this experiment are
in Table 1 and we also included a plot of the (test loss) progress of the algorithms in Figure 3 in the
Supplementary Materials. CVGM ultimately achieves a test loss of 1.080, lower than the other two
methods, and the test loss (which CVGM has no access to but we do compute during training) is for
the most part monotonically decreasing throughout the procedure.
5.2	Synthetic Classification Data
Next, we experiment with CVGM’s ability to learn kernels from scratch on two dimensional syn-
thetic classification data. We first generate a two dimensional dataset of N examples in polar coor-
dinates from two classes that form rings of different radii and have significant overlap. One class
has the distribution r 〜N(1,.4) and θ 〜Unif(-π, π), and the other class has the distribution
r 〜 N(2,.4) and θ 〜 Unif(-∏, ∏). The data is then transformed into Cartesian coordinates using
the transformation (x, y) = (r cos θ, r sin θ). A training dataset of size N = 60 is displayed in the
top part of Figure 1. Clearly, the Bayes decision rule for this dataset is to separate the classes at
kxk2 = 1.5, and the best a linear classifier can do is 50 % test error. But for the sake of illustration
of our method, we seek to learn a (parameterized) kernel φ that transforms x into a space where the
data is linearly separable, or at least to a space where we can achieve low misclassification loss by
learning a linear classifier with logistic regression.
7
Under review as a conference paper at ICLR 2019
Figure 2: Test loss of the CVGM method, a neural network, and vanilla logistic regression for
various dataset sizes.
We will use a one-layer neural network kernel φα : R2 → R2, or
φα(x) = W2σ(W1x + b1) + b2
with parameters α = [W1, b1, W2, b2]. In our experiments, we use σ(x) = max(0, x), where the
maximum is taken element-wise, and W1 ∈ R64×2 and W2 ∈ R2×64 . We will transform the data
into the new two dimensional space using the neural network, and then fit a linear classifier there
using logistic regression (see Example 3.1). In other words, given φα, we minimize the following
objective
1N
L(θ) = 2 θT θ + C £log(exp(-yiVT θ) + 1).
i=1
where Vi = φα(χi). We can then find the Jacobian of the optimal solution θ? of this objective 等
using arguments in Example 3.1, and then using the chain rule to find derivatives of the optimal
solution with respect to the neural network’s parameters. We use the (differentiable) soft-margin
loss for the cross-validation loss function l, thereby allowing us to find the derivatives of the cross-
validation loss with respect to the neural network parameters. A nice interpretation is that we are
learning a two-layer neural network that first is fed to φα and then to the logistic regression layer,
but the first part of the neural network is trained using the CVGM, and the second is learned through
(standard) logistic regression. The (differentiable) logistic regression layer is implemented as a
modular PyTorch Function, and is in the source code. In our experiments, we fix C = 10,
K = 256, p = .95, and use a gradient descent step size of 1 × 10-1. For the rest of the details of
the experiments, we refer the reader to the source code.
The kernel manifold at select iterations of the CVGM is displayed in the bottom part of Figure 1. Af-
ter about 10 iterations, the method is able to learn a manifold under which the data is (approximately)
linearly separable and achieves a test accuracy of 86.5 %, in comparison to the Bayes-optimal accu-
racy on that test set of 89.2 %.
In a separate experiment, we compared three separate methods: CVGM method with the neural
network kernel as described above, a two-layer neural network with the same architecture as the
CVGM method, and logistic regression. Training was done on dataset sizes from 8 to 200 over 25
random seeds. The CVGM model was trained using gradient descent on the binary cross entropy
loss with a step size of 1 × 10-2 for 100 steps, as we found that optimizing to convergence led to
severe overfitting—this overfitting is more pronounced when there is less data and is likely a conse-
quence of the low-data regime of this experiment. The mean test accuracies of the various learning
algorithms over the random seeds, as well as the Bayes accuracy, are displayed in Figure 2. CVGM
outperforms the two other methods, especially in the low-data regime One important observation is
that CVGM is able to stably learn 322 hyperparameters with as few as 8 data points.
6	Conclusion
By showing that we can in fact differentiate the optimal solution to most convex machine learn-
ing algorithms, we have made the cross-validation loss, which is commonly viewed as a black-box
8
Under review as a conference paper at ICLR 2019
function, a differentiable objective function. This opens up the possibility of optimizing over large
hyperparameter spaces, as demonstrated by our second experiment, where we optimized 322 hyper-
parameters with as few as 20 training examples.
Practitioners know that of the most important parts of machine learning pipelines is feature engi-
neering, which involves applying some function to raw data before feeding it to a machine learning
algorithm. Typically, the optimal features are problem-dependent, requiring experts to spend time
constructing and experimenting with hand-crafted functions. However, with the CVGM, practition-
ers can design differentiable parameterized feature engineering functions for their class of problems,
and optimize the feature engineering pipeline directly for generalization capability using gradient
descent. Hence, we believe that the CVGM we present is a step towards robust automatic feature
learning, a prized goal of machine learning research.
References
Brandon Amos and Zico Kolter. OptNet: Differentiable optimization as a layer in neural networks.
In Proc. Intl. Conf. on Machine Learning (ICML), 2017.
Shane Barratt. On the differentiability of the solution to convex optimization problems. arXiv
preprint arXiv:1804.05098, 2018.
Peter L Bartlett, Michael I Jordan, and Jon D McAuliffe. Convexity, classification, and risk bounds.
Journal of the American Statistical Association,101(473):138-156, 2006.
YoshuaBengio. Gradient-based optimization of hyperparameters. Neural Computation, pp. 1889-
1900, 2000.
James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of
Machine Learning Research (JMLR), pp. 281-305, 2012.
Olivier Bousquet and Andre Elisseeff. Stability and generalization. Journal of Machine Learning
Research (JMLR), 2:499-526, 2002.
Olivier Chapelle, Vladimir Vapnik, Olivier Bousquet, and Sayan Mukherjee. Choosing multiple
parameters for support vector machines. Machine Learning, pp. 131-159, 2002.
Justin Domke. Generic methods for optimization-based modeling. In Proc. Intl. Conf. Artificial
Intelligence and Statistics (AISTATS), pp. 318-326, 2012.
Asen L Dontchev and R Tyrrell Rockafellar. Implicit Functions and Solution Mappings. Springer-
Verlag New York, 2014.
Chuan-sheng Foo, Chuong B Do, and Andrew Y Ng. Efficient multiple hyperparameter learning for
log-linear models. In Advances in Neural Information Processing Systems (NIPS), pp. 377-384,
2008.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning.
Springer Series in Statistics New York, 2001.
S Sathiya Keerthi, Vikas Sindhwani, and Olivier Chapelle. An efficient method for gradient-based
adaptation of hyperparameters in SVM models. In Advances in Neural Information Processing
Systems (NIPS), pp. 673-680, 2007.
Ron Kohavi and George H John. Automatic parameter selection by minimizing estimated error. In
Proc. Intl. Conf. on Machine Learning (ICML), pp. 304-312, 1995.
Jan Larsen, Claus Svarer, Lars Nonboe Andersen, and Lars Kai Hansen. Adaptive regularization in
neural network modeling. In Neural Networks: Tricks of the Trade, pp. 113-132, 1998.
Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimiza-
tion through reversible learning. In Proc. Intl. Conf. on Machine Learning (ICML), pp. 2113-
2122, 2015.
9
Under review as a conference paper at ICLR 2019
J Mockus. On Bayesian methods for seeking the extremum. In Proc. Optimization Techniques IFIP
Technical Conference, pp. 400-404,1975.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research
(JMLR),12:2825-2830, 2011.
Fabian Pedregosa. Hyperparameter optimization with approximate gradient. In Proc. Intl. Conf. on
Machine Learning (ICML), pp. 737-746, 2016.
Carl Edward Rasmussen. Gaussian Processes in Machine Learning. Springer, 2004.
Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical Bayesian optimization of machine
learning algorithms. In Advances in Neural Information Processing Systems (NIPS), pp. 2951-
2959, 2012.
Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic net. Journal of the
Royal Statistical Society: Series B (Statistical Methodology), 67(2):301-320, 2005.
10
Under review as a conference paper at ICLR 2019
Supplementary Materials
A Support Vector Machine Example
Support vector machines perform classification, where X = Rn and Y = {0, 1}. The function
class is again linear, or f (x; θ) = θTx. The loss function used in SVMs is the hinge loss, or
c(y, y) = max(0,1 一 yy). In the '1 and '2-regularized SVM, We optimize
1N	1
L(θ) = N y^maχ(O 1 - yiθ Xi) + λ1 ∣∣θk1 + 2λ2kθk2∙
N i=1	2
Introducing the vectors θp, θn (Where θ = θp 一 θn), variables ti, i = 1, . . . ,N, We can reWrite the
problem as
1N	1
minimize N ɪ2ti + λ11 (θP + θn) + 2λ2kθp 一 θn∣2
N i=1	2
subject to θp	0, θn	0	(9)
ti ≥ 0, i = 1, . . . , N
1 一 yi(θp 一 θn)Txi ≤ ti, i = 1, . . . , N,
Which is a QP With considerable structure. Thus the solution θ is differentiable With respect to λ1
and λ2, again provided λ2 > 0. A similar method, i.e., replacing xi With φ(xi) can be used to
shoW that kernel-based SVMs are also differentiable With respect to the kernel parameters.sectionB:
Learning Loss Functions
B Learning Loss Functions
Much of classification can be vieWed as optimizing a convex surrogate of the 0 一 1 loss func-
tion Bartlett et al. (2006). Thus, a reasonable loss function is a convex combination of such convex
surrogates. Four common loss functions are:
•	hinge: lh(y,y) = max(0,1 — yy).
•	exponential: le(y, y) = exp(—yy).
•	truncated quadratic: lt(y, y) = max{1 — yy, 0}2 * * S.
•	logistic: lι(y,y) = ln(1+exp(-2yy)).
Except for the hinge loss, all of these loss functions are differentiable. Thus We can define the
optimization problem
1N
minimize N E αιt + α2le(θτXi, y) + α3lt(θτXi, yi) + α4l1(θτXi, yi) + R(θ, α)
N i=1	(10)
subject to 1 — yiθτ Xi ≤ ti, ti ≥ 0, i = 1, . . . , N
that is convex and differentiable in α. We can then run a projected gradient method over α in the
probability simplex.
C Connections to S tab ility
Bousquet and Elisseeff Bousquet & Elisseeff (2002) introduced several mathematically precise no-
tions of the “stability” of a learning algorithm. They consider a modified dataset, constructed by
replacing one element:
S = {z1 , . . . , zi-1 , zi, zi+1 , . . . , zN .}
The stability of a learning algorithm A is then defined as
∆= E0 l(A(S), zi0) —l(A(Si),zi0) .
S,zi0
11
Under review as a conference paper at ICLR 2019
Roughly, this corresponds to the difference in the expected loss between A not having access to zi0
and the algorithm having access to zi0 . The main theorem of the paper relates the empirical risk to
population risk
E[R - Remp] =∆.
Because our goal is to optimize E[R], we can achieve this by optimizing the quantity ∆ + E[Remp],
or
∆ + E[Remp] = E l(A(S), zi0) - l(A(Si), zi0) + l(A(S), zi0) .
The last two terms cancel, leaving us with
E[l(A(S),zi0)],
which we approximate with (3). Hence, our CVGM algorithm is implicitly choosing the hyperpa-
rameters to optimize the stability of the learning algorithm.
D Supplementary Figures for Numerical Experiments
iteration
Figure 3: Synthetic regression data. Test loss of random search, CVGM, and exhaustive search, each
run for 100 iterations.
E XOR Experiment
We also experimented with learning a two-dimensional XOR function. The data (N =
100) comes from two classes. One class comes from (χ,y) 〜 U[-3,0.6], U[-0.6,3] or
(x,y) 〜 U[-0.6,3], U[-3,0.6] with equal probability. The other class comes from (x,y) 〜
U[-3,0.6], U[-3,0.6] or (χ,y)〜U[-0.6,3], U[-0.6,3] with equal probability. The details are
similar to our classification experiment, but instead we use a two-layer neural network, with 64
hidden units in the first layer, and 64 hidden units in the second. (This corresponds to 4482 hyper-
parameters.) The results of this experiment are displayed in Figure 4.
12
Under review as a conference paper at ICLR 2019
Figure 4: XOR classification experiment. Top: Synthetic training data in two dimensions, along with
the learned decision boundary (black). Bottom: Visualization of kernel applied to 1000 (unseen) test
data points in iterations 1, 2, 5, and 50 from left to right, along with the linear classifier in that space
(black). The classifier achieves 72 % test accuracy, where the Bayes accuracy is 85.1 %.
13