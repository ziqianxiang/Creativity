Under review as a conference paper at ICLR 2019
Linearizing Visual Processes with Deep Gen-
erative Models
Anonymous authors
Paper under double-blind review
Ab stract
This work studies the problem of modeling non-linear visual processes by leverag-
ing deep generative architectures for learning linear, Gaussian models of observed
sequences. We propose a joint learning framework, combining a multivariate au-
toregressive model and deep convolutional generative networks. After justification
of theoretical assumptions of linearization, we propose an architecture that allows
Variational Autoencoders and Generative Adversarial Networks to simultaneously
learn the non-linear observation as well as the linear state-transition model from a
sequence of observed frames. Finally, we demonstrate our approach on conceptual
toy examples and dynamic textures.
1	Introduction
While classification of image and video with Convolutional Neural Networks (CNN) is becoming
an established practice, unsupervised learning and generative modeling remain to be challenging
problems in deep learning. A generative model of a visual process enables the possibility of gen-
erating sequences of video frames such that the appearance as well as the dynamics approximately
resemble the original training process without copying it. This procedure is typically referred to
as video generation (Vondrick & Torralba (2017); De Souza et al. (2017)) or video synthesis (Liu
et al. (2017b)). More technically, this means that in addition to a suitable probability model for
the individual frames, a probabilistic description for the frame-to-frame transition is also neces-
sary. Analysis and reproduction of visual processes simplifies considerably, if this transition can be
described as a multivariate autoregressive (MAR) model, i.e., as a combination of linear transfor-
mations and Gaussian noise. For instance, linear transformations are easily invertible and by means
of spectral analysis, it can be studied how such a process behaves in the long term.
Realistically, most frame transitions in real-world visual processes unlikely are linear functions.
Nevertheless, unsupervised learning has come up with many approaches to fit MAR models to real-
world processes, for instance by using linear low-rank approximations, as proposed by Doretto et al.
(2003), or sparse approximations of the frames, as proposed by Wei et al. (2017), or applying the
kernel trick to them (Chan & Vasconcelos (2007)).
The success of Generative Adversarial Networks (GAN) introduced by Goodfellow et al. (2014) and
Variational Autoencoders (VAE) introduced by Kingma & Welling (2013) has led to an increased
interest in deep generative learning and it seems natural to apply such techniques to sequential pro-
cesses. We approach this idea from the perspective of linearization, in order to keep the model as
simple as possible. In an analogous way as physicists transforming non-linear differential equations
into linear ones by means of an appropriate change of variables, our approach is to learn latent
representations of visual processes, such that the latent state-to-state transition can be described by
an MAR model. To this end, we jointly learn a non-linear observation and a linear state transi-
tion function by introducing a dynamic layer that can be used in conjunction with deep generative
architectures such as GANs and VAEs.
Notation
In this paper, letters that appear both in roman and italicized type refer to random variables and
realizations thereof, respectively. If not stated otherwise, expected values of more than one random
variables assume statistical independence.
1
Under review as a conference paper at ICLR 2019
2	Related Work
The deep predictive coding network (Chalasani & Principe (2013)), can be considered one of the
earliest approaches of combining MAR models with deep learning. More recently, such a combi-
nation was studied by Liu et al. (2018). The work by Goroshin et al. (2015) deals with linearizing
transformations under uncertainty via neural networks. It resembles our work in that we also focus
on representation learning rather than on a particular type of process. However, these works do not
employ VAEs or GANs.
By contrast, Watter et al. (2015) combine Linear Dynamic Systems (LDSs) with VAEs. However,
the focus of their work is on control rather than on synthesis. Furthermore, their model is locally
linear and the transition distribution is modeled in the variational bound, whereas we model it as
a separate layer. This also is the main difference to the work of Krishnan et al. (2015), in which
VAEs are used as Kalman Filters. The work of Johnson et al. (2016) combines VAEs with linear
dynamic models for forecasting images in video sequences. Mathematically, it is a well-thought out
approach, but proposes a training objective that is considerably more complex than the one proposed
in this work. GANs in combination with MAR models have been studied in experiments of Han et al.
(2017), but the MAR model was learned separately from the GAN.
Theoretical groundwork regarding learned visual transformations has been done by Cohen &
Welling (2014; 2016); Hinton et al. (2011) and Memisevic (2013). In particular, the dynamic layer
proposed in this work bears resemblance to techniques employed in image-to-image translation (Liu
et al. (2017a); Zhu et al. (2017)).
The synthesis of video dynamics by means of neural networks has been discussed, among others by
Vondrick et al. (2016); Xue et al. (2016); Srivastava et al. (2015) and Xie et al. (2017). We would
like to emphasize the difference between video synthesis which is discussed in our work and the
prediction of video frames as studied, for instance, by Mathieu et al. (2015). While the former refers
to the problem of finding a probabilistic model for the spatial and temporal behavior of a visual
process, the latter refers to finding a deterministic mapping from a set of previous frames to one or
several future frames to come. As a consequence, a video synthesis model needs to take care of
long-term behavior. Additionally, the probabilistic nature of video synthesis makes it considerably
harder to evaluate the generated frames, since a unique ground truth cannot be provided, ruling out
classical quantitative quality measures such as mean squared error or structured similarity.
Finally, the core contribution of this work is a combination of neural networks with Markov pro-
cesses. This has been the subject of many works in the recent past. For a broad overview of results
in this field, the reader is referred to Chapter 20 of the book Deep Learning by Goodfellow et al.
(2016).
3	Visual Processes
3.1	Dynamic Systems
The Dynamic Texture (DT) model by Doretto et al. (2003) has popularized LDSs in the modeling of
visual processes. Typically, an LDS is of the following form
ht+1 = Aht + vt,
yt = y + Cht,
(1)
where ht ∈ Rn is the low dimensional state space variable at time t, A ∈ Rn×n the state transition
matrix, yt ∈ Rd the observation at time t and C ∈ Rd×n the observation matrix. The vector
y ∈ Rd represents a constant offset in the observation space. The input term Vt is modeled as i.i.d.
zero-mean Gaussian noise, and is independent of ht .
Real-world visual processes are often highly non-linear and non-Gaussian, hence are considerably
harder to work with. In the following, we define a more general dynamic model that is applicable
to a broad class of visual processes. To keep the problem tractable, we make the assumption of two
properties that were already implicitly assumed in the classical DT model.
Assumption 1. The visual processes of interest are stationary first-order Markov processes.
2
Under review as a conference paper at ICLR 2019
The stationarity property guarantees that synthesis of new sequences will not diverge and the Markov
property facilitates the statistical inference. However, we believe that the method presented in this
work can be generalized to Markov processes of higher orders. We refer to Appendix A.2 for details.
Consider the following generic dynamic equation of observations y in an n-dimensional manifold
M ⊂ Rd.
yt+1 = ψvt (ψ(yt)).	(2)
Again, yt ∈ Rd denotes the observation at time t. The self map 夕：M → M models the predicted
frame transition and describes the deterministic part of the dynamics. In this work, it is assumed
to be differentiable. Furthermore, the function ψvt : M → M describes a displacement by vt in
the in the tangent space TytM of M at yt, followed by a retraction onto M. The displacement is
sampled from i.i.d. zero-mean Gaussian noise. Eq. (1) is a special case of Eq. (2), where 夕(y)=
CAC +(y - y) + y and ψv(y) = y + Cv.
The model Eq. (2) describes a much broader class of visual processes than Eq. (1). However, unlike
model Eq. (2), the state transition in the linear model Eq. (1) enables straightforward prediction,
generation, and analysis of observations. It is thus of great interest to find a model that linearizes
real-world visual processes, so that in some latent state space representation, the state transition
admits the MAR model of the first line of Eq. (1). Specifically, this work focuses on the following
non-linear dynamic system model, i.e., a linear state transition and a non-linear observation mapping
ht+1 = Aht + vt,
yt = C(ht),
(3)
where n < d and C : Rn → M ⊂ Rd is differentiable. Transforming a non-linear model to this
form makes video synthesis as easy as sampling from autoregressive noise.
It is worth noticing that the model in Eq. (3) is not unique with respect to changes of basis in the
state space (Doretto et al. (2003); Afsari & Vidal (2013)). However, ifC is implemented via a neural
network, we can ensure that it accounts for a possible change of basis, e.g. by adding a linear layer
to the input of the network. We can thus make the following assumption on the latent samples ht
without loss of generality.
Assumption 2. The latent samples ht abide a standard normal distribution, i.e., ht 〜N( ∙ ; 0, I).
Remark 1. If the state transition matrix A is given, and the process is stationary, i.e.,
p(ht) = p(ht+1) for all t, then Assumption 2 essentially identifies the process noise model. Namely,
we have
Eht+1 ht+1ht>+1 = Eht,vt (Aht +vt) (Aht +vt)>
= Eht,vt Ahtht>A> + vtht>A> + Ahtvt> + vtvt>
= Eht,vt Ahtht>A> +Evt vtvt>
= AA> + Evt vtvt> .
(4)
In order to make sure that the latent states ht remain standard Gaussian in sequential synthesis
scenarios, i.e., Eht+1 ht+1ht>+1 = I, we just need to ensure that the process noise is zero-mean
and has the covariance matrix I - AA>.	♦
Before we propose an algorithm to jointly learn C and A by means of deep generative architecture,
we further characterize the problem in order to investigate its feasibility and motivate our approach.
For a model Eq. (3) to substitute Eq. (2), it needs to transform the transition 夕 to a multiplication
by a matrix and the perturbation ψvt to a superposition by zero-mean Gaussian noise. The latter is
generally easy to achieve, if the perturbation is sufficiently small and C is a diffeomorphism. In the
remainder of the section, We thus focus on linearization of 夕.
3.2	Local Linearization
A common linearization method in control theory is to approximate the dynamic system equation
by a first-order Taylor polynomial around an equilibrium point (Perko (2013)). Clearly, for this to
be possible, such a point needs to exist. We thus propose the following assumption on the transition
function 夕 ofEq. (2).
3
Under review as a conference paper at ICLR 2019
Assumption 3. The differentiable selfmap φ: M → M has at least one fixed point y*.
However, such an approach might fail for real-world processes due to the curse of dimensionality.
A remedy is to employ an representation function Γ that maps the system observations to a lower-
dimensional latent space prior to performing the linearization. We call such a function a local
linearizer.
Definition 1. Let M ⊂ Rd be an n-dimensional manifold,夕：Rd → Rd be differentiable on
夕(M) = M, and Γ: Rd → Rn be a diffeomorphism on M. The map Γ is said to be a local
linearizer at y* ∈ M of 夕,ifthere exists a matrix Φ ∈ Rn×n such that thefollowing equality holds
true with y ∈ Rd.
Imy)-Γ-1(ΦΓ(y))k
ky-y*k-0	ky - y*k	=.
(5)
Generally speaking, local linearization is made possible by moving the fixed point to the origin of a
new coordinate system. More precisely, the following proposition holds.
Proposition 1. Let M ⊂ Rd be an n-dimensional manifold. Furthermore, let 夕：Rd → Rd be
differentiable on 夕(M) = M, and Γ: Rd → Rn be a diffeomorphism on M. If y* ∈ M is a fixed
point of 夕,then thefollowing map,
Γ0 : Rn → M
y 7→ Γ(y) - Γ(y*),	(6)
locally linearizes 夕,ifthe rows and columns ofthe Jacobi matrix JW of 夕 at y* lie in the row space
of the Jacobi matrix JΓ ofΓ at y*.
Proof. See Appendix A.1	□
The Jacobi matrix property is only needed for consistency with Definition 1, where the limit y → y*
is approached from any possible direction in Rd. It can be ignored, ifwe restrict the analysis entirely
to the manifold M. Proposition 1 states that if 夕 is a differentiable self map on M with a fixed point
y* ∈ M, and a diffeomorphism from M to Rn exists, then there is a representation in which 夕 can
be approximated by a linear function for points on M that are not too far away from y*.
Note that even if the Jacobi matrix JW can be sufficiently well estimated from the data, the linear
approximation directly in the observation space Rd will likely be worse than via reparametrization
of a local linearizer Γ0 that functions as a chart of M. The reason is that predictions directly via JW
can not be assumed to remain on M. Figuratively speaking, the aim of the local linearizer is to bend
the space spanned by the rows and columns of JW to match the shape of M.
3.3	S eparate Learning
Local linearizability is a useful concept to analyze the feasibility of the problem at hand. However,
it does not take into account that two local linearizers can have significantly different linearization
properties on a global scale. Moreover, it does not provide any instructions on how to find an
appropriate representation Γ and matrix Φ. Traditionally, this task is approached by learning Γ
and Φ separately while considering the sampled observations of the process globally, as will be
described in the following.
To characterize the problem of global linearization, we need to introduce a measure. It is sensible
to consider the expected squared distance between the result of a transformation 夕 and its linear
approximation. Since it is not possible to have an analytic expression for the distribution on the data
manifold M, the latent space is often considered as a heuristic. Because it is a common model as-
sumption for deep generative models and in accordance with Assumption 2, we can assume standard
Gaussian distribution for the latent space.
Let Γ : M → Rn denote a data representation mapping and 夕：M → M the transition function to
be linearized. Consider the following expression
q(r,中,Φ)
Ey [kΦΓ(y)- ΓW(y))k2]
2n
(7)
4
Under review as a conference paper at ICLR 2019
The denominator 2n is a normalization factor. If Γ(y) has standard Gaussian distribution, then
q(Γ,夕,Φ) < 1 denotes that the linear prediction with Φ is smaller than the expected distance of two
independently drawn samples. A common method of linearizing 夕，is to first choose a representation
Γ that is assumed to have good linearization properties, and then inferring Φ by minimizing Eq. (7)
(Doretto et al. (2003); Chan & Vasconcelos (2007); Han et al. (2017)). In that case, the solution is
given by
Φ = Ey [ΓW(y))Γ(y)>],	(8)
if Γ(y) has standard Gaussian distribution.
However, the drawback of this approach is the difficulty to find an appropriate model for Γ that can
be assumed to linearize the transformation 夕.Moreover, separate learning implicitly assumes that
a small linearization error in the embedded space will carry over to a small error in the observation
space, but such an assumption is hard to justify given the high dimension of the problem. We
therefore propose to approach the problem by learning the representation and the linear transition
jointly, by approximating the data distribution directly via deep generative models.
4	Joint Learning via Deep Generative Models
4.1	Markov Assumptions
We now turn to the problem of finding a model Eq. (3) that approximates Eq. (2). In the following,
we will provide the preliminaries to lay out a training procedure to infer C and A from observed
sequences of visual processes. A sequence of length N of a visual process Y is viewed as a real-
ization of the random variable yN = [y1, . . . , yN]. The according sequence in the latent space is a
realization of the random variable hN = [h1, . . . , hN]. Consider the random variable
yN = [yι,…，y] := [C(hι),…，C(hN)].	(9)
In order to model Y by Eq. (3), we need to make sure that the probability distributions of yN and
yN coincide for any N ∈ N. By taking into account Assumption 1, this is equivalent to demanding
that the joint probability of two succeeding frames coincide.
The joint probability distribution of ht and ht+1 is zero-mean Gaussian and, more specifically,
Ihhtl ThIl ~N f[h1ι ； O, [i a>d ,	(io)
ht+1	h2	h2 A I
holds due to Assumption 2. To summarize, we are looking for the function C and a matrix A, such
that the random variable
y2 = [C(hι) C(h2)],	(11)
has the same probability distribution as y2 .
4.2	Deep Generative Models
Estimation of probability distributions for high-dimensional data is still an ongoing research topic in
deep learning. The problem is typically framed as an approximation of a function fθ, parameterized
by a vector θ in a finite-dimensional euclidean space. The purpose of fθ is to map realizations of
low-dimensional, standard Gaussian noise to samples that abide the data distribution. Typically, the
function fθ is realized by a neural network with trainable weights θ.
Most of the widely employed deep generative models, including the GAN, and the VAE, adopt this
approach. What these models differ in, is merely the way the network parameters θ are trained. For
this work, we employ the Wasserstein GAN and the VAE to evaluate the proposed method. Due to
space constraints, we will not review these techniques here and refer the reader to Arjovsky et al.
(2017) for the Wasserstein GAN and to Doersch (2016) for the VAE. For the following sections, it
is sufficient to assume that these techniques are capable of approximating a function fθ that maps
from a low-dimensional standard Gaussian to a high-dimensional data distribution.
5
Under review as a conference paper at ICLR 2019
Figure 1: Generative network with dynamic layer. The dashed arrow indicates the trainable param-
eters
4.3	The Dynamic Layer
We now focus on how to learn a matrix A and a function C from a finite sequence yN ∈ Rd×N
of a visual process such that it can be described by Eq. (3). To this end, recall that, as discussed in
Section 4.1, we only need to consider the joint probability of two succeeding observed frames. The
first step is thus to generate a training set of frame pairs from yN as {s1, . . . , sN-1}, where
st = yt> yt>+1>	(12)
denotes a vectorized pair of frames. We treat the samples as realizations of the random variable s
and are looking for an architecture to learn the function C and a matrix A such that the probability
distribution of the random variable
C(h1)> C(h2)>>,	(13)
where h1, h2 have the joint distribution described by Eq. (10), coincides with the probability distri-
bution of s, i.e.
p(s) =pC(h1)> C(h2)>> .	(14)
It turns out that we can use a deep generative architecture to accomplish this task. Remember, that
a function fθ learned by a deep generative model is capable of transforming samples x of standard
Gaussian noise to samples from a high dimensional data distribution. Let θ = (A, B, η) contain the
matrices A, B ∈ Rn×n such that the constraint
AA> + BB> = In	(15)
is fulfilled, and the parameter η that defines the model for Cη = C. Consider the following definition
for fθ .
fθ : R2n → R2d,	x1 7→	Cη (x1)	.	(16)
θ	, x2	Cη (Ax1 + Bx2)
Assume that We can train fθ, as defined in Eq.(16), to map X 〜N(x; 0, I2n) to S = fθ (x) with the
probability distribution
S 〜p(s).	(17)
Then, A and C = Cη indeed fulfill the condition in Eq. (14) for h1 , h2 with joint probability
distribution Eq. (10). In order to implement fθ by a neural network, we propose the architecture
depicted in Fig. 1. The first layer is linear and will be referred to as the dynamic layer in the
following. The dynamic layer implements the multiplication with the matrix
F = IAn B .	(18)
The output of the dynamic layer is split into an upper half h1 and a lower half h2 and both halves
are fed to the subnetwork that implements the observer function Cη . The weights of the dynamic
layer contain the matrices A, B . Thus, they can be trained along with η, by back-propagation of
the stochastic gradient. However, we need to make sure that the stationarity constraint in Eq. (15) is
not violated. This can be done by adding a regularizer to the loss function L of the Deep Generative
Network. We thus substitute L(θ) by
L(θ) = L(θ) + λkAA> + BB> - InkF,	(19)
where λ > 0 is a regularizer constant. Here, L refers to the variational bound for the VAE or to the
Generator loss for the (Wasserstein) GAN.
6
Under review as a conference paper at ICLR 2019
Figure 2: Synthesis result for sequences of MNIST digits with VAE
夕 S<**76
7∕√Γ^7F^76
5	Experiments
5.1	Overview
Due to its impressive results on natural images, we employ the generator of the Deep Convolutional
GAN (DCGAN, Radford et al. (2015)) as the foundation for implementing the observer function
Cη . We use it in combination with an affine layer at the input. This layer serves three purposes.
Firstly, it accounts for changes of bases in the latent space in order to conform with Assumption 2.
Secondly, it includes a bias so that the fixed point of the transition function can be transformed to
the origin according to Proposition 1. Finally, it further reduces the latent dimension to make the
search for the transition matrix A feasible.
We train Cη and A simultaneously by means of the dynamic layer introduced in Section 4.3. To this
end, we integrate the construction in Fig. 1 as the decoder of a VAE (without batch normalization)
and the generator of a Wasserstein GAN (with batch normalization), respectively. As the critic
(discriminator) network of the Wasserstein GAN, we use the original discriminator of the DCGAN
but double the number of output channels in order to match the dimensions of the training frame
pairs. As the encoder of the VAE, we use the discriminator of the DCGAN, but adapt the number of
output channels to be 2n, where n is the latent dimension of the model. The latent dimension is set
to n = 10 for all experiments.
Synthesis is performed by sampling from the MAR model described by
ht+1 = Aht + Bvt,	Vt 〜i.i.d. N(vt, 0, In),	(20)
and mapping the latent states to the observation space by means of Cη . The initial latent state h0
is sampled from Gaussian white noise for the Wasserstein GAN experiments. For the VAE ex-
periments, it is estimated by applying the encoder to an observation frame pair from the training
sequence. PyTorch code for reproducing the experiments will be made available online upon publi-
cation, along with the entire set of results.
5.2	Variational Autoencoder
An isotropic standard Gaussian model N(s; fθ (x), σ2) is assumed for the conditional data distri-
bution. The Adam optimizer With a step size of 2.5 * 10-4 is used to train the architecture. The
regularizer constant is set to λ = 100. We carry out two series of experiments.
In the first series of experiments, We use the MNIST dataset to generate repeating sequences of hand
Written digits. We choose σ = 4 and use sequences of length 10000 for training the netWork for 25
epochs. Most of the number sequences We test can be Well reproduced. Fig. 2 depicts the synthesis
results for the sequences 0123401234... and 4567845678...... Occasional jumps occur, due
to the stochastic nature of the model. An observation We make is that higher values of σ improve
the probability of synthesizing the correct sequence, but decrease the variability of digit shapes.
To investigate the linearization of geometrical transformations, We employ Category 0 of the Small
NORB dataset that contains pictures of miniature animals under 6 different lighting conditions, 9
elevational and 18 azimuthal poses. We train our architecture for 100 epochs with σ = √5 to syn-
thesize sequences depicting azimuthal rotations of 20° per frame. Generally, the synthesis provides
good results. For each one of the five animals, the synthesis yields clearly recognizable rotation
movements. However, the algorithm tends to confuse pairs of opposite poses, as can be observed in
Fig. 3a, where the poses are flipped after the fourth frame in each row. With regard to Section 3.2,
7
Under review as a conference paper at ICLR 2019
Figure 3: Synthesis result for NORB sequences (a) and learned fixed point (b)
Figure 4: Synthesis result for sequences of the UCLA-50 dataset with Wasserstein GAN
two explanations for this can be identified. On the one hand, it is possible that the assumptions
of Proposition 1 are not fulfilled, and no diffeomorphism, i.e. a bijective mapping exists from the
image manifold to the embedded space. In that case, the best we can do is finding a non-injective
local diffeomorphism that assigns more than one point on the image manifold to a point in the em-
bedded space. On the other hand, it is thinkable, that an actual diffeomorphism exists, but that the
euclidean distance implicitly minimized by the VAE is a bad estimation of the geodesic distance on
the manifold. Fig. 3b depicts the learned fixed point of the transition function. It can be thought of
as a rotationally invariant structure under limited exposure of light from above.
5.3	Wasserstein Generative Adversarial Network
We employ the Wasserstein GAN for dynamic texture synthesis experiments and set the regularizer
to λ = 1. We firstly test our method on the cropped UCLA-50 dataset of grayscale DTs. The dataset
contains 50 classes of 4 sequences of length 75 each. We use one class at a time for the experiments.
The architecture is trained for 2000 epochs via RMSPROP with step size 2.5 * 10-4. Fig. 4 depicts
the results for the classes candle and fountain-c-far.
Finally, we evaluate the method on three RGB sequences. We run the optimization via RMSPROP
with step size 5 * 10-5 for 900 epochs. Fig. 5 depicts the result for the firepot, springwater
and waterfountain sequences taken from Xie et al. (2017). Despite the low complexity of our
model, we believe that the difference in quality compared to the state of the art results reported in
Xie et al. (2017) is negligible.
6	Conclusion
This work presents an approach to learn embedded MAR models from image sequences. We moti-
vate the feasibility of this approach by introducing the concept of local linearizability and propose
a joint learning procedure that employs deep generative models in combination with an additional
linear component, the dynamic layer. We report first positive results on low-resolution visual pro-
cesses, where a first-order Markov property can be assumed, and hope to shed some light on the
nature of linearization. A possible future research direction is improving the theoretical understand-
ing of linearizing representations and their applicability outside of stationary visual processes.
8
Under review as a conference paper at ICLR 2019
Figure 5: Synthesis result for RGB sequences. Frames are rescaled to match the original aspect ratio
9
Under review as a conference paper at ICLR 2019
References
Bijan Afsari and Rene Vidal. The alignment distance on spaces of linear dynamical systems. In
Decision and Control (CDC), 2013 IEEE 52nd Annual Conference on, pp. 1162-1167. IEEE,
2013.
Martin Arjovsky, SoUmith Chintala, and Leon Bottou. Wasserstein gan. arXiv preprint
arXiv:1701.07875, 2017.
Rakesh Chalasani and Jose C Principe. Deep predictive coding networks. arXiv preprint
arXiv:1301.3541, 2013.
Antoni B Chan and Nuno Vasconcelos. Classifying video with kernel dynamic textures. In IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-6. IEEE, 2007.
Taco Cohen and Max Welling. Group equivariant convolutional networks. In International Confer-
ence on Machine Learning, pp. 2990-2999, 2016.
Taco S Cohen and Max Welling. Transformation properties of learned visual representations. arXiv
preprint arXiv:1412.7659, 2014.
CR De Souza, A Gaidon, Y Cabon, and AM Lopez Pena. Procedural generation of videos to
train deep action recognition networks. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2017.
Carl Doersch. Tutorial on variational autoencoders. arXiv preprint arXiv:1606.05908, 2016.
Gianfranco Doretto, Alessandro Chiuso, Ying Nian Wu, and Stefano Soatto. Dynamic textures.
International Journal of Computer Vision, 51(2):91-109, 2003.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT press Cambridge, 2016.
Ross Goroshin, Michael F Mathieu, and Yann LeCun. Learning to linearize under uncertainty. In
Advances in Neural Information Processing Systems, pp. 1234-1242, 2015.
Tian Han, Yang Lu, Song-Chun Zhu, and Ying Nian Wu. Alternating back-propagation for generator
network. In AAAI, volume 3, pp. 13, 2017.
Geoffrey E Hinton, Alex Krizhevsky, and Sida D Wang. Transforming auto-encoders. In Interna-
tional Conference on Artificial Neural Networks, pp. 44-51. Springer, 2011.
Matthew Johnson, David K Duvenaud, Alex Wiltschko, Ryan P Adams, and Sandeep R Datta. Com-
posing graphical models with neural networks for structured representations and fast inference.
In Advances in neural information processing systems, pp. 2946-2954, 2016.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Rahul G Krishnan, Uri Shalit, and David Sontag. Deep kalman filters. arXiv preprint
arXiv:1511.05121, 2015.
Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks.
In Advances in Neural Information Processing Systems, pp. 700-708, 2017a.
Wenqian Liu, Abhishek Sharma, Octavia Camps, and Mario Sznaier. Dyan: A dynamical atoms-
based network for video prediction. In Proceedings of the European Conference on Computer
Vision (ECCV), pp. 170-185, 2018.
Ziwei Liu, Raymond Yeh, Xiaoou Tang, Yiming Liu, and Aseem Agarwala. Video frame synthesis
using deep voxel flow. In International Conference on Computer Vision (ICCV), volume 2, 2017b.
10
Under review as a conference paper at ICLR 2019
Michael Mathieu, Camille Couprie, and Yann LeCun. Deep multi-scale video prediction beyond
mean square error. arXiv preprint arXiv:1511.05440, 2015.
Roland Memisevic. Learning to relate images. IEEE transactions on pattern analysis and machine
intelligence, 35(8):1829-1846, 2013.
Lawrence Perko. Differential equations and dynamical systems, volume 7. Springer Science &
Business Media, 2013.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. 2015.
Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov. Unsupervised learning of video
representations using lstms. In International conference on machine learning, pp. 843-852, 2015.
Carl Vondrick and Antonio Torralba. Generating the future with adversarial transformers. In IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 2017.
Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics.
In Advances In Neural Information Processing Systems, pp. 613-621, 2016.
Manuel Watter, Jost Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to control:
A locally linear latent dynamics model for control from raw images. In Advances in neural
information processing systems, pp. 2746-2754, 2015.
Xian Wei, Yuanxiang Li, Hao Shen, Fang Chen, Martin Kleinsteuber, and Zhongfeng Wang. Dy-
namical textures modeling via joint video dictionary learning. IEEE Transactions on Image Pro-
cessing, 26(6):2929-2943, 2017.
Jianwen Xie, Song-Chun Zhu, and Ying Nian Wu. Synthesizing dynamic patterns by spatial-
temporal generative convnet. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 7093-7101, 2017.
Tianfan Xue, Jiajun Wu, Katherine Bouman, and Bill Freeman. Visual dynamics: Probabilistic
future frame synthesis via cross convolutional networks. In Advances in Neural Information
Processing Systems, pp. 91-99, 2016.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation
using cycle-consistent adversarial networks. arXiv preprint, 2017.
A Appendix
A. 1 Proof of Proposition 1
Let Φ ∈ Rn×n be a matrix. Since Γ0 is a diffeomorphism, we define φ = Γ0-1 ◦ Φ ◦ Γ0. We need
to show that
lim	ko(y) - φQ)k = 0
ky-y*k-0	ky - y*k =
(21)
holds. Let Us denote the Jacobian of φ at y* by Jφ. Because Γ0 maps y* to the origin, We can
reformulate the requirement as
lim	kιXy) - y* - φ(y~) + y*k
ky-y*k-0	ky - y*k
= lim	kιXy) - w(y*) - φ(y~) + 0(y*)k
ky-y*k→0	ky - y*k
= li	kιXy) - v(y*) - jΦ(y - y*)k = 0
= ky-y*k-0	ky - y*k	='
This requirement is fulfilled if the Jacobi matrices of 夕 and φ coincide, i.e.
Jφ = Jφ.
(22)
(23)
11
Under review as a conference paper at ICLR 2019
The Jacobian of φ at y* is given, according to he chain rule, by
Jφ = JΓ-1ΦJΓ,	(24)
where Jγ-i ∈ Rd×n is the Jacobian matrix of Γ-1 at Γ(y*). A matrix Φ ∈ Rn×n can be always
found, such that Eq.(23) is fulfilled, if the columns and rows of JW lie in the column space of Jγ-i
and the row space of JΓ, respectively. The column space of JΓ-1 coincides with the row space of
JΓ, due to the identity JΓJΓ-1 = In. The statement of the proposition follows.
A.2 Generalization to Higher-order Markov Processes
The proposed method is designed to linearize first-order Markov processes only. This is in line
with our empirical evaluation, in which the method was not capable of synthesizing sequences with
persisting motions of non-constant velocities, e.g. the videos of running animals in Xie et al. (2017).
However, the presented approach can be theoretically generalized to Markov processes of order
m ≥ 1. We present a procedure that could be employed for this purpose. Further investigation is
necessary to evaluate the feasibility and practical applicability of such an approach.
The general procedure is as follows.
1.	Set up an appropriate block matrix model for the joint probability of m + 1 succeeding
observations.
2.	Build a dynamic layer that performs a multiplication with a lower-triangular (m + 1) ×
(m + 1) block matrix F.
3.	Introduce regularizers to preserve the block Toeplitz structure of the covariance matrix,
4.	Compute the MAR parameters from the resulting covariance matrix.
We illustrate the procedure for the case m = 2.
1. Due to the stationarity assumption, the covariance matrix for three succeeding observations
ht, ht+1, ht+2 has the form
In
Σ2 = Cov(ht, ht+1)>
Cov(ht, ht+2)>
Cov(ht, ht+1)
In
Cov(ht, ht+1)>
Cov(ht, ht+2)
Cov(ht, ht+1)
In
(25)
2.	The matrix describing the dynamic layer has the form
In
F = F1	F2
F3	F4	F5 .
The outputs of the dynamic layer will thus have the distribution
p h1> h2> h3>> = N (h1> h2> h3>> ; 0, FF>).
We thus need to achieve
Γ In	F>	F>
Σ2 ≈ FF> = F1	F1 F1> + F2 F2>	F1 F3> + F2 F4>
F3> F3F1> +F4F2> F3F3> +F4F4> +F5F5>
3.	This can be ensured by using the regularizer
R(F) = λ1kF1F1> +F2F2> -Ink2F +λ2kF3F3> +F4F4> +F5F5> -Ink2F
+λ3kF3F1>+F4F2> -F1k2F,
with λ1, λ2, λ3> 0.
4.	A second-order MAR model has the form
(26)
(27)
(28)
(29)
ht+2 = Aoht + Aiht+1 + Bvt, Vt 〜N(vt, 0, In).	(30)
12
Under review as a conference paper at ICLR 2019
By our assumptions on the process, this yields the system of equations,
Cov(ht, ht) = AoA> + AiA> + A0Cov(ht, ht+1)A>
+ A1C0v(ht, ht+ι)τA> + BBT
Cov(ht, ht+i) =Cov(ht, ht+ι)τAT + A>,
Cov(ht, ht+2) = AT + Cov(ht, ht+ι)A>.
Thus, we can infer A°, Ai and B via solving
FT = FιA> + AT,
F3 = A0 + A1F1,
In = A0AT + A1A> + AoF> AT + Ai Fi A> + BBτ.
(31)
(32)
13