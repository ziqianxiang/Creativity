Under review as a conference paper at ICLR 2019
SHE2 : Stochastic Hamiltonian Exploration
and Exploitation for Derivative-Free Opti-
MIZATION
Anonymous authors
Paper under double-blind review
Ab stract
Derivative-free optimization (DFO) using trust region methods is frequently used
for machine learning applications, such as (hyper-)parameter optimization with-
out the derivatives of objective functions known. Inspired by the recent work
in continuous-time minimizers, our work models the common trust region meth-
ods with the exploration-exploitation using a dynamical system coupling a pair of
dynamical processes. While the first exploration process searches the minimum
of the blackbox function through minimizing a time-evolving surrogation func-
tion, another exploitation process updates the surrogation function time-to-time
using the points traversed by the exploration process. The efficiency of derivative-
free optimization thus depends on how the two processes couple. In this paper,
We propose a novel dynamical system, namely SHE2—Stochastic Hamiltonian
Exploration and Exploitation, that surrogates the subregions of blackbox function
using a time-evolving quadratic function, then explores and tracks the minimum
of the quadratic functions using a fast-converging Hamiltonian system. The SHE2
algorithm is later provided as a discrete-time numerical approximation to the sys-
tem. To further accelerate optimization, We present P-SHE2 that parallelizes mul-
tiple SHE2 threads for concurrent exploration and exploitation. Experiment re-
sults based on a Wide range of machine learning applications shoW that P-SHE2
outperform a boarder range of derivative-free optimization algorithms With faster
convergence speed under the same settings.
1 Introduction
Derivative-free optimization (DFO) techniques (PoWell, 1964), such as Bayesian optimization algo-
rithms (Snoek et al., 2012; Martinez-Cantin, 2014), non-differentiable coordinate descent (Colson
& Toint, 2002), natural gradient method (Grosse & Salakhudinov, 2015; Hasenclever et al., 2017),
and natural evolution strategies (Schaul et al., 2011), have been Widely used for black-box func-
tion optimization. DFO techniques have been vieWed as one of promising solutions, When the
first-order/higher-order derivatives of the objective functions are not available. For example, to
train large-scale machine learning models, parameter tuning is sometimes required. The problem
to find the best parameters from the high-dimensional parameter space is frequently formalized as
a black-box optimization problem, as the function that maps the specific parameter settings to the
performance of models is not knoWn (Golovin et al., 2017; Fazel et al., 2018; Xu et al., 2018b; Liu
et al., 2018). The evaluation of the black-box function is often computationally expensive, and there
thus needs DFO algorithms to converge fast With global/local minimum guarantee.
Backgrounds. To ensure the performance of DFO algorithms, a series of pioneering Work has been
done (Conn et al., 2009; PoWell, 2012; Jamieson et al., 2012; Augustin & Marzouk, 2017; Golovin
et al., 2017). Especially, PoWell et al. (PoWell, 1964; 1994) proposed Trust-Region methods that
intends to “surrogate” the DFO solutions through exploring the minimum in the trust regions of the
blackbox objective functions, Where the trust regions are tightly approximated using model functions
(e.g., quadratic functions or Gaussian process) via interpolation. Such tWo processes for exploration
and exploitation are usually alternatively iterated, so as to pursue the global/local minimum (Au-
gustin & Marzouk, 2017). With exploration and exploitation (Dezza et al., 2017), a Wide range of
algorithms have been proposed using trust region for DFO surrogation (PoWell, 2002; Queipo et al.,
1
Under review as a conference paper at ICLR 2019
2005; Snoek et al., 2012; Swersky et al., 2013; Schulman et al., 2015; Wu et al., 2017; Regier et al.,
2017; Abdolmaleki et al., 2017; Osokin et al., 2017; Lyu et al., 2018; Arenz et al., 2018).
Technical Challenges. Though trust region methods have been successfully used for derivative-free
optimization for decades, the drawbacks of these methods are still significant:
•	The computational and storage complexity for (convex) surrogates is extremely high. To
approximate the trust regions of blackbox functions, quadratic functions (Powell, 2002;
Regier et al., 2017) and Gaussian process (Snoek et al., 2012; Wu et al., 2017; Lyu et al.,
2018) are frequently used as (convex) surrogates. However, fitting the quadratic functions
and Gaussian process through interpolation is quite time-consuming with high sample com-
plexity. For example, using quadratic functions as surrogates (i.e., approximation to the
second-order Taylor’s expansion) needs to estimate the gradient and inverse Hessian ma-
trix (Powell, 2002; Regier et al., 2017), where a large number of samples are required to
avoid ill-conditioned inverse Hessian approximation; while the surrogate function in GP is
nonconvex, which is even more sophisticated to optimize.
•	The convergence of trust region methods cannot be guaranteed for high-dimensional non-
convex DFO. Compared to the derivative-based algorithms such as stochastic gradient de-
scent and accelerated gradient methods (Bottou, 2010; Su et al., 2014), the convergence
of DFO algorithms usually are not theoretically guaranteed. Jamieson et al. (Jamieson
et al., 2012) provided the lower bound for algorithms based on boolean-based comparison
of function evaluation. It shows that DFO algorithms can converge at Ω(1/√T) rate in the
best case (T refers to the total number of iterations), without assumptions on convexity and
smoothness, even when the evaluation of black-box function is noisy.
Our Intuitions. To tackle the technical challenges, we are motivated to study novel trust region
methods with following properties
1.	Low-complexity Quadratic Surrogates with Limited Memory. To lower the computational
complexity, we propose to use quadratic functions with identity Hessian matrices as sur-
rogates. Rather than incorporating all evaluated samples in quadratic form approximation,
our algorithm only works with the most-recently evaluated sample points. In this way, the
memory consumption required can be further reduced. However, the use of identity Hes-
sian matrices for quadratic form loses the information about the distribution (e.g., Fisher
information or covariance (Hansen et al., 2003)) of evaluated sample points.
2.	Fast Quadratic Exploration with Stochastic Hamiltonian Dynamical Systems. Though it is
difficult to improve the convergence rate of the DFO algorithms in general nonconvex set-
tings with less oracle calls (i.e., times of function evaluation), one can make the exploration
over the quadratic trust region even faster. Note that exploration requires to cover a trust re-
gion rather than running on the fastest path (e.g., the gradient flow (Hu & Li, 2017)) towards
the minimum of trust region. In this case, there needs an exploration mechanism traversing
the whole quadratic trust region in a fast manner and (asymptotically) approaching to the
minimum. Figure 1 illustrates the examples of exploration processes over the quadratic
region via its gradient flows (i.e., gradient descent) or using Hamiltonian dynamics with
gradients (Neal et al., 2011) as well as their stochastic variants with explicit perturbation,
all in the same length of time. It shows that the stochastic Hamiltonian dynamics (shown in
Figure 1(d)) can well balance the needs of fast-approaching the minimum while sampling
the quadratic region with its trajectories. Compared to the (stochastic) gradient flow, which
leads to the convergence to the minimum in the fast manner, the stochastic Hamiltonian
system are expected to well explore the quadratic trust region with the convergence kept.
Inspired by theoretical convergence consequences of Hamiltonian dynamics with Quadratic
form (Su et al., 2014; Neal et al., 2011), we propose to use stochastic Hamiltonian dynam-
ical system for exploring the quadratic surrogates.
3.	Multiple Quadratic Trust Regions with Parallel Exploration-Exploitation. Instead of using
one quadratic cone as the surrogate, our method constructs the trust regions using multi-
ple quadratic surrogates, where every surrogate is centered by one sample point. In this
way, the information of multiple sample points can be still preserved. Further, to enjoy the
speedup of parallel computation, the proposed method can be accelerated through explor-
ing the minimum from multiple trust regions (using multiple Hamiltonian dynamical sys-
2
Under review as a conference paper at ICLR 2019
Iaiaiaia
(a) Gradient Flow (b) Hamiltonian (c) Stochastic GF (d) Stochastic HAM
Figure 1: Quadratic Surrogate Exploration via Gradient Flow, Hamiltonian Dynamics with Gradient,
Stochastic Gradient Flow, and Stochastic Hamiltonian Dynamics with Gradient
tems) concurrently while parallelizing the blackbox function evaluation to save the overall
time. Certain synchronization among the Hamiltonian systems might be required to share
knowledge among the parallel exploration-exploitation processes.
Our work is inspired by the recent progress in the continuous-time convex minimizers (Su et al.,
2014; Hu & Li, 2017; Xu et al., 2018a) on convex functions, where the optimization algorithms
are considered as the discrete-time numerical approximation to some (stochastic) ordinary differen-
tial equations (ODEs) or dynamics, such as Ito processes for SGD algorithms (HU & Li, 2017) or
Hamiltonian systems for Nesterov’s accelerated SGD (Su et al., 2014). We intend to first study the
new ODE and dynamical system as a continuous-time DFO minimizer that addresses above three
research issues. With the new ODE, we aim at proposing the discrete-time approximation as the
algorithms for black-box optimization.
Our Contributions. Specifically, we make following contributions. (1) To address the three tech-
nical challenges, a continuous-time minimizer for derivative-free optimization based on a Hamil-
tonian system coupling two processes for exploration and exploitation respectively. (2) Based on
the proposed dynamical system, an algorithm, namely SHE2-Stochastic Hamiltonian Exploration
and Exploitation, as a discrete-time version of the proposed dynamical system, as well as P-SHE2
that parallelizes SHE2 for acceleration. (3) With the proposed algorithms, a series of experiments to
evaluate SHE2 and P-SHE2 using real-world applications. The two algorithms outperform a wide
range of DFO algorithms with better convergence. To the best of our knowledge, this work is the
first to use a Hamiltonian system with coupled process for DFO algorithm design and analysis.
2	Related Work and Preliminaries
In this section, we first review the most relevant work of trust region methods for DFO problem,
then present the preliminaries of this work.
2.1	Trust Region Methods for DFO
The trust region algorithms can be categorized by the model functions used for surrogates. Gener-
ally, there are two types of algorithms adopted: Gaussian Process (GP) (Snoek et al., 2012; Swersky
et al., 2013; Wu et al., 2017; Lyu et al., 2018) or Quadratic functions (Powell, 2002; Regier et al.,
2017; Osokin et al., 2017) for surrogation. Blessed by the power of Bayesian nonparameteric statis-
tics, Gaussian process can well fit the trust regions, with confidence bounds measured, using sam-
ples evaluated by the blackbox function. However, the GP-based surrogation cannot work in high
dimension and cannot scale-up with large number of samples. To solved this problem, GP-based
surrogation algorithms using the kernel gradients (Wu et al., 2017) and mini-batch (Lyu et al., 2018)
have been recently studied.
On the other hand, the quadratic surrogation (Powell, 2002) indeed approximates the trust region
through interpolating the second-order Taylor expansion of the blackbox objective. With incoming
points evaluated, there frequently needs to numerically estimate and adapt the inverse Hessian ma-
trix and gradient vector, which is extremely time-consuming and sample-inefficiency (with sample
complexity O(d2) for d-dimensional DFO (Powell, 2002)). Following such settings, (Regier et al.,
2017) proposed to a second-order algorithm for blackbox variational inference based on quadratic
surrogation, while (Osokin et al., 2017) leveraged a Gaussian Mixture Model (multiple quadratic
surrogations) to fit the policy search space over blackbox probabilistic distribution for policy opti-
mization. A novel convex model generalizing the quadratic surrogation has been recently proposed
to characterize the loss for structured prediction (Arenz et al., 2018).
3
Under review as a conference paper at ICLR 2019
In addition, some evolutionary strategies, such as Covariance Matrix Adaptation Evolution Strategy
(CMA-ES) (Hansen et al., 2003; Abdolmaleki et al., 2017), indeed behave as a sort of quadratic
surrogate as well. Compared to the common quadratic surrogate, CMA-ES models the energy of
blackbox function using a multivariate Gaussian distribution. For every iteration, CMA-ES draws
a batch of multiple samples from the distribution, then statistically updates parameters of the dis-
tribution using the samples with blackbox evaluation. CMA-ES can be further accelerated with
parallel blackbox function evaluation and has been used for hyperparameter optimization of deep
learning (Loshchilov & Hutter, 2016).
2.2	Hamiltonian Systems for Convex Minimization
Here, we review the Nesterov’s accelerated method for quadratic function minimization. We par-
ticularly are interested in the ODE of Nesterov’s accelerated method and interpret behavior of the
ODE as a Hamiltonian dynamical system.
Corollary 1 (ODE of Nesterov’s Accelerated Method). According to Su et al. (2014), the discrete-
time numerical format of the Nesterov’s accelerated method Nesterov (2004) can be viewed as an
ODE as follow.
3
Z(t) + tZ(t)+ VZ f (Z (t))=0,	(1)
where f (X) is defined as the objective function for minimization and Vχ f (Z(t)) refers to the
gradient of the function on the point Z (t). Above ODE can converge with strongly theoretical
consequences if the function f(X) is convex with some smoothness assumptions Su et al. (2014).
Corollary 2 (Convergence ofEq 1 OverQuadraticLoss). Let's set f (X) = 2 ∣∣X 一 X *∣∣2 According
to the ODE analysis of Nestereov’s accelerated method Su et al. (2014), the ODE listed in Eq 1
converges with increasing time t at the following rate:
∣Z(t) -X*112 ≤ 4kz⑼t-X*k2 = O (t2),	(2)
where X(0) refers to the initial status of the ODE. The proof has been given in Su et al. (2014).
3 Stochastic Hamiltonian System for Black-Box Optimization
In this section, we present the proposed Hamiltonian system for Black-Box minimization via ex-
ploration and exploitation. Then, we introduce the algorithms and analyze its approximation to the
dynamical systems.
3.1	The Stochastic Hamiltonian Exploration and Exploitation System
Given a black-box objective function f(X) and X ∈ Rd, we propose to search the minimum of
f(X) in the d-dimensional vector space Rd, using a novel Hamiltonian system, derived from the
ODE of Nesterov’s accelerated method and Eq 1, yet without the derivative of f(X) needed.
Definition 1 (Quadratic Loss Function). Given two d-dimensional vectors X and Y , we character-
izes the Euclid distance between the two vectors using the function as follow.
Q(X,Y ) = 1 ∣X — Y k2,
where the partial derivative of Q on X should be ∂χQ(X, Y)
direction moving from X to Y.
(3)
X 一 Y indicating the fastest
Definition 2 (Stochastic Hamiltonian Exploration and Exploitation). As was shown in Eq. 4, a
Hamiltonian system is designed with following two coupled processes: exploration process X(t)
and exploitation process Y (t), where t refers to the searching time. These two processes are coupled
withn each other. Specifically, the exploration process X(t) in Eq 4 uses a second order ODE to
track the dynamic process Y (t), while the exploiting process Y(t) always memorizes the minimum
point (i.e., X(τ)) that have been reached by X(t)from time 0 to t.
(a): X⑴ + 7X⑴ + ^VQ(X (t),Y (t)) + Z⑴=0,
t	∂X
(b): Y(t) = X(τ) andτ = argmin f(X(τ)),
τ ∈(0,t]
(4)
where (1) ∂χQ(X(t),Y(t)) = X(t) — Y(t) indicates thefastest direction to track Y(t) from X(t);
and (2) the perturbation term ζ(t) referring to an unbiased random noise with controllable bound
4
Under review as a conference paper at ICLR 2019
Algorithm 1 SHE2: Stochastic Hamiltonian Exploration and Exploitation
1:	Xo, Vo 〜randomly from Rd and Y0 J X。/* initialization by randomization*/;
2:	for t = 1, 2, . . . , T do
3:	/* SHE2 X(t) Process Update*/
4:	βt J -(αt + 3/t) and γt J 3/t;
5:	Zt% N(0,I) and Zt J ε ∙Zt∕∣Zt∣2;
6:	Xt J χt-ι + αt ∙ Vt-ι;
7:	V J ½-i + αt ∙匕-1 + βt ∙ Xt-i + γt ∙ Xt + at ∙ Zt;
8:	/* SHE2 Y (t) Process Update*/
9:	if f(Xt) ≤ f(Yt-1) then
10:	Yt J Xt ;
11:	else
12:	Yt J Yt-1;
13:	end if
14:	end for
15:	return YT ;
kZ (t)k2 ≤ would help the system escape from an unstable stationary point in even shorter time.
In the above dynamical system, we treat Y (t) as the minimizer of the black-box function f (X).
Indeed, SHE2 approximates the black-box function f(X) using a simple yet effective quadratic
function, then leverages the ODE listed in Eq 1 to approximate the minimum with the quadratic func-
tion. With the new trajectories traversed by Eq 1, the quadratic function would be updated. Through
repeating such surrogation-approximation-updating procedures, the ODE continuously tracks the
time-dependent evolution of quadratic loss functions and finally stops at a stationary point when the
quadratic loss functions is no longer updated (even with new trajectories traversed).
Remark 1. We can use the analytical results Su et al. (2014) to interpret the dynamical system (in
Eq 4) as an adaptive perturbated dynamical system that intends to minimize the Euclid distance
between X(t) and Y (t) at each time t. The memory complexity of this continuous-time minimizer is
O(1), where a Markov process Y (t) is to used to memorize the status quo of local minimum during
exploration and exploitation.
Theorem 1 (Convergence of SHE2 Dynamics). Let's denote x* as a possible local minimum point
of the landscape function f (x). We have as t → ∞, with high probability, that X (t) → x*, where
X(t) is the solution to (4).
Please refer to the Lemma 1 and Lemma 2 in the Appendix for the proof of above theorems. We will
discuss the rate of convergence, when introducing SHE2 algorithm as a discrete-time approximation
to SHE2.
3.2	SHE2 : Algorithm Design and Analysis
Given a black-box function f(x) and a sequence of non-negative step-size αt (t=0, 1, 2, . . . , T),
which is small enough, as well as the scale of perturbation ε, we propose to implement SHE2as
Algorithm 1. The output of algorithm YT refers to the value of Yt in the last iteration (i.e., the tth
iteration). The whole algorithm only uses the evaluation of function f(x) for comparisons, without
computing its derivatives. In each iteration, only the variable Yt is dedicated to memorize the local
minimum in the sequence ofX1, X2, . . . , Xt. Thus the memory complexity of SHE2 is O(1).
In terms of convergence, Jamieson et al Jamieson et al. (2012) provided an universal lower bound
on the convergence rate of DFO based on the “boolean-valued” comparison of (noisy) function
evaluation. SHE2 should enjoy the same convergence rate Ω(1∕√T) without addressing any further
assumptions.
3.3	Approximation Analysis for SHE2
Here, we would demonstrate that the proposed algorithm behaves as a discrete-time approximation
to the dynamical systems of X(t) and Y (t) addressed in Eq 4, while as αt → 0 the sequence ofXt
and Yt (for 1 ≤ t ≤ T) would converge to the behavior of continuous-time minimizer — coupled
processes X(t) and Y (t).
5
Under review as a conference paper at ICLR 2019
Given an appropriate constant step-size αt → 0 for t = 1, 2..., T, we can rewrite the the sequences
Xt described in lines 4-7 of Algorithm 1 as the following Stochastic Differential Equation (SDE)
of Xω (t) with the random noise ω(t):
Xt- Xt-ι V
---------=Vt-ι
at
Vt - Vt-1	3(Xt - Xt-1)
--------=-----------------
at	t ∙ at
、	+ (Yt-I - Xt-I) + Zt
α=⇒0 X(t) =V(t),
α=⇒0 V(t) = - ∣x(t) - (X(t) - Y(t)) + ζ(t),
(5)
where Z (t) refers to the continuous-time dynamics of sequence Z1,Z2,...,Zτ and ∣Z(t)∣2 = ε for
every time t. Through combining above two ODEs and Lemma 1, we can obtain the SDE of X(t)
based on the perturbation Z(t) as:
X(t) + 3 X(t) + ɪ Q(X (t), Y(t)) + ζ (t) = 0.	(6)
t ∂X
The sequence Yt (t=0, 1, 2,…T) always exploits the minimum point that has been already found
by Xt at time t. Thus, we can consider Yt is the discrete-time of Y (t) that exploits the minimum
traversed by X(t). In this way, we can consider the coupled sequences ofXt and Yt (for 1 ≤ t ≤ T)
as the discrete-time form of the proposed dynamical system with X(t) and Y (t).
4 P-SHE2 : Parallel Stochastic Hamiltonian Exploration and
Exploitation
To enjoy the speedup of parallel computation, we propose a new Hamiltonian dynamical system with
a set of ODEs that leverage multiple pairs of coupled processes for exploration and exploitation in
parallel. Then, we present the algorithm design as a discrete-time approximation to the ODEs.
4.1	The P-SHE2 Dynamical System
Given a black-box objective function f (X) and X ∈ Rd, we propose to search the minimum of
f(X) in the d-dimensional vector space Rd, using following systems.
Definition 3 (Parallel Stochastic Hamiltonian Exploration and Exploitation). As was shown in Eq. 7,
a Hamiltonian system is designed with (1) N pairs of coupled exploration-exploitation processes:
Xi (t) and Yi(t) for 1 ≤ i ≤ N that explores and exploits the minimum in-parallel from N (ran-
dom/unique) starting points, and (2) an overall exploitation process Y (t) memorizing the local
minimum traversed by the all N pairs of coupled processes. Specifically, for each pair of cou-
pled processes, a new surrogation model Qδ(Xi(t), Yi(t), Y (t)) has been proposed to measure the
joint distance from Xi (t) to Yi(t) and Y (t) respectively, where δ > 0 refers to a trade-off factor
weighted-averaging the two distances.
..∙	3 . ∙	∂	， ∙	、
(a): Xi(t) + IXl(t) + dχQδ (Xi(t),Yi(t),Y(t)) + Zi(t) = 0
Qδ(X, Y, Z) = δQ(X, Y) + (1 - δ)Q(X, Z)
(b): Yi(t) = Xi(τ) andτ = argmin f(Xi(τ)),	∀1 ≤ i ≤ N (7)
T ∈(0,t]
・ ∙
(c): Y(t) = Xj(τ) and j = argmin f (Yi(τ))	,
、	1≤j≤N
where 袅Qδ(Xi(t),Υi(t),Υ(t)) = Xi(t) 一 δ ∙ Yi(t) 一 (1 一 δ) ∙ Y(t) indicates thefastest direction
to track Yi (t) and Y (t), jointly, from X (t). In the above dynamical system, we treat Y(t) as the
minimizer of the black-box function f(X).
Remark 2. We understand the dynamical system listed in Eq 7 as a perturbated dynamical sys-
tem with multiple state variables, where all variables are coupled to search the minimum off(X)
through Xi(t) (for 1 ≤ i ≤ N). The memory complexity of this continuous-time minimizer is O(N),
where every Markov process Yi(t) is to used to memorize the status quo of local minimum traversed
by the corresponding processes.
4.2	P-SHE2 : S caling-up SHE2 through Parallelized Function Evaluation
The evaluation of black-box function f(x) is frequently time consuming. Algorithm 2 presents an
algorithm namely P-SHE2that uses N SHE2 threads to evaluate f(x) in parallel. Specifically, in
6
Under review as a conference paper at ICLR 2019
each iteration, P-SHE2 leverages the minimum among the N local minimums searched by the N
parallel SHE2 threads to accelerate the overall search. In P-SHE2 , we use N threads of SHE2 to
search the minimum, where each thread uses three Xtj , Ytj and Vtj trajectories for exploration and
exploitation. The memory complexity of P-SHE2 is O(N).
Algorithm 2 P-SHE2: Stochastic Hamiltonian Exploration and Exploitation
1:	/* Starting P-SHE2 by Initializing N instances of SHE2 with Synchronization*/
2:	for j = 1, 2, 3, . . . , N do
3:	X0, Vj ~ randomly from Rd and Yj J Xj;
4:	end for
5:	argmin	f(X) → Y0;
	∀X∈{Y01,Y02,...,Y0N}
6:	for t = 1, 2 . . . , T do
7:	for j = 1, 2, 3, . . . , N in Parallel do
8:	/* X(t) update for the jth SHE2 thread*/
9:	βt J -(αt + 3/t) and γt J 3/t;
10:	口吧 N(0,I) and Zj J ε ∙Zj∕∣Zj∣2;
11:	Xtj J Xtj-1 + αtVtj-1;
12:	Wj-1 J δ ∙ γ-ι + (1-δ) ∙匕-1;
13:	Vt J V-I + αt ∙ Wj-I + βt ∙ Xj-I + γt ∙ Xj + at ∙ ζj;
14:	/* Y(t) update for the jth SHE2 thread*/
15:	iff(Xtj) ≤f(Ytj-1)then
16:	Ytj J Xtj ;
17:	else
18:	Ytj J Ytj-1;
19:	end if
20:	end for
21:	/* Local Minimum Update*/
22:	argmin	f (X) → Yt ;
	∀X∈{Yt1,Yt2,...,YtN}
23:	end for
24:	return YT ;
4.3	Approximation Analysis forP-SHE2
As at → 0, We can rewrite the sequences Xj, Wj and Vj described in lines 9-12 of Algorithm 2
as the following Ordinary Differential Equations (ODEs) of Xj(t), Wj (t) and V j (t):
，Xj- Xj-1 at	=Vtj-1	αt⇒0 X(t)=	Vj (t) ,	
. Wj-1	=δYtj-1 + (1 - δ)Yt-1	α=t⇒→0 Wj (t)	=δYj(t)+(1-δ)Y(t),	(8)
Vj- V-1	3(Xj- Xj-)	α=⇒0 V j (t)=	3 =-tX j (t)	
	——		 t ∙ at			
at				
	+ (Wtj-1 - Xtj-1) + Ztj	-Qδ(Xj(t),Yj(t),Y(t))+Zj(t),		
where Zj(t) refers to the continuous-time dynamics of sequence Zj,Zj,..., ZT and ∣Zj(t)∣2 = ε for
every time t. Through combining above three ODEs and Eq. 8, we can obtain the ODE ofX(t) as:
∙∙ ∙ . . ∙ ∙ .. ∙
Xj(t) + %Xj(t) + Qδ(Xj(t),Yj(t),Y(t)) - Zj(t)= 0.	(9)
Using same the settings, we can conclude that Xtj would have similar behavior as Xi (t) (for 1 ≤
i ≤ N in Eq 7). Thus, Algorithm 2 can be viewed as a discrete-time approximation of dynamical
systems in Eq 7. Since the sequence Yt always exploits the minimum point that has been found by
all N threads at every time t, we can use the algorithm output YT as the minimizer of f (x).
7
Under review as a conference paper at ICLR 2019
「ranLa’。EnCXCn
(b) Convergence (Franke’s)
P-SHE2(IO)
SHE2
PSO(IO)
PSO(I)
CMA-ES
BOBYQA
L-BFGS
GP-UCB
(C) Time per Iteration (Franke's)
(d) Peaks FunCtion
Iteration Number
(e) ConvergenCe (Peaks)
Figure 2: Performance Comparison using Nonconvex Functions
(f) Time per Iteration (Peaks)

4.4	Connection to Existing Solutions
The proposed P-SHE2 algorithm can be viewed as a particle swarm optimizer Kennedy (2011) with
inverse-scale step-size settings. Compared to Particle Swarm, which usually adopts constant step-
size settings (i.e., αt, βt and Yt are fixed as a constant value), P-SHE2 proposes to use a small at,
while setting βt = -(αt + 3/t) and Yt = 3/t for each (the tth) iteration. Such settings help the
optimizer approximates to the Nesterov,s scheme, so as to enjoy faster convergence speed, under
certain assumption. In terms of contribution, our research made as yet an rigorous analysis for
Particle Swarm through linking it to to Nesterov,s scheme Nesterov (2013); Su et al. (2014).
5 Experiments and Empirical Validation
We provide three sets of experiments to validate our algorithms. In the first set of experiments, we
demonstrate the performance of SHE2 and P-SHE2 to minimize two non-convex functions through
the comparisons to a set of DFO optimizers, including Gaussian Process optimization algorithms
(GP-UCB) (Martinez-Cantin, 2014), Powell,s BOBYQA methods (Powell, 2009), Limited Memory-
BFGS-B (L-BFGS) (Zhu et al., 1997), Covariance Matrix Adaptation Evolution Strategy (CmA-
ES) (Hansen et al., 2003), and Particle Swarm optimizer (PSO) Kennedy (2011). For the second
set of experiments, we use the same set of algorithms to train logistic regression (Lee et al., 2006)
and support vector machine (Cortes & Vapnik, 1995) classifiers, on top of benchmark datasets, for
supervised learning tasks. In the third set, we use P-SHE2 to optimize the hyper-parameters of
ResNet-50 for the performance tuning on Flower 102 and MIT Indoor 67 benchmark datasets under
transfer learning settings.
5.1	Nonconvex Function Minimization
Figure 2 presents the performance comparison between P-SHE2, SHE2 and the baseline algo-
rithms using two 2D benchmark nonconvex functions-Franke, s function and Peaks function. Fig-
ure 2.a and C present the landscape of these two functions, while Figure 2.b and d present the per-
formance evaluation of P-SHE2, SHE2and baseline algorithms on these two functions. All these
algorithms are tuned with best parameters and evaluated for 20 times, while averaged performance
is presented. Specifically, we illustrate how these algorithms would converge with increasing num-
ber of iterations. Obviously, on Franke,s function, only P-SHE2(10), i.e., the P-SHE2algorithm
with N = 10 search threads, CMA-ES, GP-UCB algorithms and BOBYQA converge to the global
minimum, while the rest of algorithms, including SHE2and PSO, converge to the local minimum.
Though P-SHE2 (10) needs more iterations to converge to the global minimum, its per iteration
time consumption is significantly lower than the other three convergeable algorithms (shown in Fig-
ure 2.e). The same comparison result can be also observed from the comparison using Peaks function
(in Figures 2.b and 2.c, only CMA-ES and P-SHE2(10) converge to global minimum in the given
number of iterations). Compared P-SHE2(10) to PSO(10), they both use 10 search threads with the
8
Under review as a conference paper at ICLR 2019
Iteration Number
Iteration Number
Iteration Number
(a) Iris (SVM)
(d) Breast (LR)
(b) Iris (LR)
(e) Wine (SVM)
Figure 3: Loss Minimization via DFO using Benchmark Datasets
(c) Breast (SVM)
6	500 1000 1500 2000 2500 3000
Iteration Number
(f) Wine (LR)
same computational/memory complexity, while P-SHE2(10) converges much faster than PSO(10).
The same phenomena can be also observed from the comparison between SHE2and PSO(1), both of
which search with single thread. We can suggest that the adaptive step-size settings inherent from
Nesterove’s scheme accelerate the convergence speed.
Table 1: Accuracy Comparison for Parameter Optimization on Benchmark Datasets
		Iris				Wine			Breast Cancer	
Algorithms	LR	SVM	LR	SVM	LR	SVM
P-SHE2(1θ0)-	0.952 ± 0.0^23~	0.987 ± 0.037~	0.967 ± 0.034~~	0.961 ± 0.0^27~	0.980 ± 0.0l4-	0.982 ± 0.015
Derivative-Free Solutions						
SHE2	0.493 ± 0.113	0.571 ± 0.065	0.628 ± 0.135	0.689 ± 0.138	0.848 ± 0.079	0.830 ± 0.112
-PSO(IOO)-	0.920 ± 0.059	0.933 ± 0.059	0.922 ± 0.047	0.900 ± 0.045	0.956 ± 0.028	0.947 ± 0.014
PSO(I)	0.361 ± 0.060	0.305 ± 0.034	0.355 ± 0.035	0.326 ± 0.015	0.608 ± 0.320	0.544 ± 0.211
-BOBYQA-	0.793 ± 0.118	0.933 ± 0.059	0.944 ± 0.042	0.953 ± 0.019	0.959 ± 0.022	0.869 ± 0.062
L-BFGS 一	0.747 ± 0.110~	0.600 ± 0.202~	0.772 ± 0.057~	0.828 ± 0.093^^	0.815 ± 0.008^^	0.947 ± 0.0T4~
Derivative-based Solution						
SGD*	0.950 ± 0.026	0.930 ± 0.038	0.975 ± 0.011	0.981 ± 0.016	0.965 ± 0.008	0.897 ± 0.041
5.2	Parameter Optimization for Supervised Learning
We use above algorithms to train logistic regression (LR) and SVM classifiers using Iris (4 features,
3 classes and 150 instances), Breast (32 features, 2 classes and 569 instances) and Wine (13 features,
3 classes and 178 instances) datasets. We treat the loss functions of logistic regression and SVM as
black-box functions and parameters (e.g., projection vector β for logistic regression) as optimiza-
tion outcomes. Note that the number of parameters for multi-class (#class ≥ 3) classification is
#class × #f eatures, e.g., 39 for wine data. We don’t include GP-UCB in the comparison, as it is
extremely time-consuming to scale-up in high-dimensional settings.
Figure 3 demonstrates how loss function could be minimized by above algorithms with iterations
by iterations. For both classifiers on all three datasets, P-SHE2(100)-the P-SHE2 algorithms with
N = 100 search threads, outperforms all rest algorithms with the most significant loss reduction and
the best convergence performance. We also test the accuracy of trained classifiers using the testing
datasets. Table. 1 shows that both classifiers trained by P-SHE2(100) enjoys the best accuracy
among all above DFO algorithms and the accuracy is comparable to those trained using gradient-
based optimizers. All above experiments are carried out under 10-folder cross-validation. Note that
the accuracy of the classifiers trained by P-SHE2 is closed to/or even better than some fine-tuned
gradient-based solutions (Dogan & Tanrikulu, 2013; RoScher & Forstner, 2009).
5.3	Hyperparameter Optimization for Deep Neural Networks
To test the performance of P-SHE2 for derivative-free optimization with noisy black-box function
evaluation, We use P-SHE2 to optimize the hyper-parameter of ResNet-50 networks for Flower
9
Under review as a conference paper at ICLR 2019
(a) Flower 102	(b) MIT Indoor 67
Figure 4: Hyper-Parameter Optimization for ResNet-50 on Benchmark Datasets
102 recognition (Nilsback & Zisserman, 2008) and MIT Indoor 67 classification (Quattoni & Tor-
ralba, 2009) tasks. The two networks are pre-trained using ImageNet (Krizhevsky et al., 2012) and
Place365 datasets (Zhou et al., 2014), respectively. Specifically, we design a black-box function to
package the training procedure of the ResNet-50, where 12 continuous parameters, including the
learning rate of procedure, type of optimizers (after simple discretization), the probabilistic distribu-
tion of image pre-processing operations for randomized data augmentation and so on, are interfaced
as the input of the function while the validation loss of the network is returned as the output. We aim
at searching the optimal parameters with the lowest validation loss. The experiments are all based
on a Xeon E5 cluster with many available TitanX, M40x8, and 1080Ti GPUs.
Our experiments compare P-SHE2 with a wide range of solvers and hyper-parameter tuning tools,
including PSO, CMA-ES (Loshchilov & Hutter, 2016), GP-UCB (Joy et al., 2016) and BOBYQA
under the same pre-training/computing settings. Specifically, we adopt the vanilla implementation
of GP-UCB and BOBYQA (with single search thread), while P-SHE2, PSO and CMA-ES are all
with 10 search threads for parallel optimization. The experimental results show that all these algo-
rithms can well optimize the hyer-parameters of ResNet for the better performance under the same
settings, while P-SHE2 has ever searched the hyperparameters with the lowest validation loss in our
experiments (shown in Figure 4). Due to the restriction of PyBOBYQA API, we can only provide
the function evaluation of the final solution obtained by BOBYQA as a flatline in Figure 4. In fact,
P-SHE2, PSO and CMA-ES may spend more GPU hours than GP-UCB and BOBYQA due to the
parallel search. For the fair comparison, we also evaluate GP-UCB and BOBYQA with more than
100 iterations til the convergence, where GP-UCB can achieve 0.099854 validation error (which is
comparable to the three parallel solvers) for Flower 102 task. Note that we only claim that P-SHE2
can be used for hyerparameter optimization with decent performance. We don’t intend to state that
P-SHE2 is the best for hyerparameter tuning, as the performance of the three parallel solvers are
sometimes randon and indeed close to each other.
6 Discussion and Conclusion
In this paper, We present SHE2and P-SHE2- two derivative-free optimization algorithms that Iever-
age a Hamiltonian exploration and exploitation dynamical systems for black-box function opti-
mization. Under mild condition SHE2 algorithm behaves as a discrete-time approximation to a
Nestereov’s scheme ODE (Su et al., 2014) over the quadratic trust region of the blackbox func-
tion. Moreover, we propose P-SHE2 to further accelerate the minimum search through parallelizing
multiple SHE2-alike search threads with simple synchronization.
Compared to the existing trust region methods, P-SHE2 uses multiple quadratic trust regions with
multiple (coupled) stochastic Hamiltonian dynamics to accelerate the exploration-exploitation pro-
cesses, while avoiding the needs of Hessian matrix estimation for quadratic function approximation.
Instead of interpolating sampled points in one quadratic function, P-SHE2 defacto constructs one
quadratic surrogate (with identity Hessian) for each sampled point and leverages parallel search
threads with parallel black-box function evaluation to boost the performance. Experiment results
show that P-SHE2can compete a wide range of DFO algorithms to minimize nonconvex benchmark
functions, train supervised learning models via parameter optimization, and fine-tune deep neural
networks via hyperparameter optimization.
10
Under review as a conference paper at ICLR 2019
References
Abbas Abdolmaleki, Bob Price, Nuno Lau, Luis Paulo Reis, and Gerhard Neumann. Deriving and
improving cma-es with information geometric trust regions. In Proceedings of the Genetic and
Evolutionary Computation Conference, pp. 657-664. ACM, 2017.
Oleg Arenz, Mingjun Zhong, and Gerhard Neumann. Efficient gradient-free variational inference
using policy search. In International Conference on Machine Learning, pp. 234-243, 2018.
F Augustin and YM Marzouk. A trust-region method for derivative-free nonlinear constrained
stochastic optimization. arXiv preprint arXiv:1703.04156, 2017.
Leon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of
COMPSTAT’2010, pp. 177-186. Springer, 2010.
Beno^t Colson and PhiIiPPe L Toint. A derivative-free algorithm for sparse unconstrained OPtimiza-
tion problems. In Trends in Industrial and Applied Mathematics, pp. 131-147. Springer, 2002.
Andrew R Conn, Katya Scheinberg, and Luis N Vicente. Introduction to derivative-free optimiza-
tion, volume 8. Siam, 2009.
Corinna Cortes and Vladimir VaPnik. SuPPort-vector networks. Machine learning, 20(3):273-297,
1995.
Irene Cogliati Dezza, J Yu Angela, Axel Cleeremans, and William Alexander. Learning the value
of information and reward over time when solving exPloration-exPloitation Problems. Scientific
reports, 7(1):16919, 2017.
Neslihan Dogan and Zuhal Tanrikulu. A comParative analysis of classification algorithms in data
mining for accuracy, sPeed and robustness. Information Technology and Management, 14(2):
105-124, 2013.
Maryam Fazel, Rong Ge, Sham Kakade, and Mehran Mesbahi. Global convergence of Policy gradi-
ent methods for the linear quadratic regulator. In International Conference on Machine Learning,
PP. 1466-1475, 2018.
Mark Freidlin and Alexander Wentzell. Random perturbations of dynamical systems, Third Edition.
SPringer, 2012.
Daniel Golovin, Benjamin Solnik, SubhodeeP Moitra, Greg Kochanski, John Karro, and D Scul-
ley. Google vizier: A service for black-box oPtimization. In Proceedings of the 23rd ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, PP. 1487-1495.
ACM, 2017.
Roger Grosse and Ruslan Salakhudinov. Scaling uP natural gradient by sParsely factorizing the
inverse fisher matrix. In International Conference on Machine Learning, PP. 2304-2313, 2015.
Nikolaus Hansen, Sibylle D Muller, and Petros Koumoutsakos. Reducing the time complexity of
the derandomized evolution strategy with covariance matrix adaPtation (cma-es). Evolutionary
computation, 11(1):1-18, 2003.
Leonard Hasenclever, Stefan Webb, Thibaut Lienart, Sebastian Vollmer, Balaji Lakshminarayanan,
Charles Blundell, and Yee Whye Teh. Distributed bayesian learning with stochastic natural gradi-
ent exPectation ProPagation and the Posterior server. The Journal of Machine Learning Research,
18(1):3744-3780, 2017.
Wenqing Hu and Chris Junchi Li. On the fast convergence of random Perturbations of the gradient
flow. arXiv preprint arXiv:1706.00837, 2017.
Kevin G Jamieson, Robert Nowak, and Ben Recht. Query comPlexity of derivative-free oPtimiza-
tion. In Advances in Neural Information Processing Systems, PP. 2672-2680, 2012.
Tinu Theckel Joy, Santu Rana, Sunil GuPta, and Svetha Venkatesh. HyPerParameter tuning for
big data using bayesian oPtimisation. In Pattern Recognition (ICPR), 2016 23rd International
Conference on, PP. 2574-2579. IEEE, 2016.
11
Under review as a conference paper at ICLR 2019
James Kennedy. Particle swarm optimization. In Encyclopedia of machine learning, pp. 760-766.
Springer, 2011.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
Iutional neural networks. In Advances in neural information processing Systems, pp. 1097-1105,
2012.
Su-In Lee, Honglak Lee, Pieter Abbeel, and Andrew Y Ng. Efficient l1 regularized logistic regres-
sion. In AAAI, volume 6, pp. 40j08, 2006.
Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv
preprint arXiv:1806.09055, 2018.
Ilya Loshchilov and Frank Hutter. Cma-es for hyperparameter optimization of deep neural networks.
ICLR Workshop Track, 2016.
Wenlong Lyu, Fan Yang, Changhao Yan, Dian Zhou, and Xuan Zeng. Batch bayesian optimization
via multi-objective acquisition ensemble for automated analog circuit design. In International
Conference on Machine Learning, pp. 3312-3320, 2018.
Ruben Martinez-Cantin. Bayesopt: A bayesian optimization library for nonlinear optimization,
experimental design and bandits. The Journal of Machine Learning Research, 15(1):3735-3739,
2014.
Radford M Neal et al. Mcmc using hamiltonian dynamics. Handbook of Markov Chain Monte
Carlo, 2(11):2, 2011.
Yurii Nesterov. A method for solving the convex programming problem with convergence rate
O(1∕k2). Dolk. Akad. Nauk. SSSR, 269:543-547.
Yurii Nesterov. Introductory lectures on convex optimization, volume 87. Springer Science &
Business Media, 2004.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2013.
Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number
of classes. In Computer Vision, Graphics & Image Processing, 2008. ICVGIP’08. Sixth Indian
Conference on, pp. 722-729. IEEE, 2008.
Anton Osokin, Francis Bach, and Simon Lacoste-Julien. On structured prediction theory with cal-
ibrated convex surrogate losses. In Advances in Neural Information Processing Systems, pp.
302-313,2017.
Michael JD Powell. An efficient method for finding the minimum of a function of several variables
without calculating derivatives. The computer journal, 7(2):155-162, 1964.
Michael JD Powell. A direct search optimization method that models the objective and constraint
functions by linear interpolation. In Advances in optimization and numerical analysis, pp. 51-67.
Springer, 1994.
Michael JD Powell. Uobyqa: unconstrained optimization by quadratic approximation. Mathematical
Programming, 92(3):555-582, 2002.
Michael JD Powell. The bobyqa algorithm for bound constrained optimization without derivatives.
2009.
MJD Powell. On the convergence of trust region algorithms for unconstrained minimization without
derivatives. Computational Optimization and Applications, 53(2):527-555, 2012.
Ariadna Quattoni and Antonio Torralba. Recognizing indoor scenes. In 2009 IEEE Conference on
Computer Vision and Pattern Recognition, pp. 413T20. IEEE, 2009.
12
Under review as a conference paper at ICLR 2019
Nestor V Queipo, Raphael T Haftka, Wei Shyy, Tushar Goel, Rajkumar Vaidyanathan, and P Kevin
Tucker. Surrogate-based analysis and optimization. Progress in aerospace sciences, 41(1):1-28,
2005.
Jeffrey Regier, Michael I Jordan, and Jon McAuliffe. Fast black-box variational inference through
stochastic trust-region optimization. In Advances in Neural Information Processing Systems, pp.
2402-2411, 2017.
Ribana Roscher and Wolfgang Forstner. Multiclass bounded logistic regression-efficient regular-
ization with interior point method, 2009.
Tom Schaul, Tobias Glasmachers, and Jurgen Schmidhuber. High dimensions and heavy tails for
natural evolution strategies. In Proceedings of the 13th annual conference on Genetic and evolu-
tionary computation, pp. 845-852. ACM, 2011.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning, pp. 1889-1897, 2015.
Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine
learning algorithms. In Advances in neural information processing systems, pp. 2951-2959, 2012.
Weijie Su, Stephen Boyd, and Emmanuel Candes. A differential equation for modeling nesterovs
accelerated gradient method: Theory and insights. In Advances in Neural Information Processing
Systems, pp. 2510-2518, 2014.
Kevin Swersky, Jasper Snoek, and Ryan P Adams. Multi-task bayesian optimization. In Advances
in neural information processing systems, pp. 2004-2012, 2013.
Jian Wu, Matthias Poloczek, Andrew G Wilson, and Peter Frazier. Bayesian optimization with
gradients. In Advances in Neural Information Processing Systems, pp. 5267-5278, 2017.
Pan Xu, Tianhao Wang, and Quanquan Gu. Continuous and discrete-time accelerated stochastic
mirror descent for strongly convex functions. In International Conference on Machine Learning,
pp. 5488-5497, 2018a.
Tianbing Xu, Qiang Liu, Liang Zhao, and Jian Peng. Learning to explore via meta-policy gradient.
In International Conference on Machine Learning, pp. 5459-5468, 2018b.
Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. Learning deep
features for scene recognition using places database. In Advances in neural information process-
ing systems, pp. 487-495, 2014.
Ciyou Zhu, Richard H Byrd, Peihuang Lu, and Jorge Nocedal. Algorithm 778: L-bfgs-b: Fortran
subroutines for large-scale bound-constrained optimization. ACM Transactions on Mathematical
Software (TOMS), 23(4):550-560, 1997.
13
Under review as a conference paper at ICLR 2019
A Proof of the convergence of SHE2 Dynamical System
Our goal is to show that in the system (4) We have X(t) → x* as t → ∞, where x* is a local
minimum point of the landscape function f (x).
Definition 4. We say the point x* is a local minimum point of the function f (x) if and only if
f(x*) ≤ f(x) for any x ∈ U(x*), where U(x*) which is any open neighborhood around the point
x*.
Let us first remove the noise ζ(t) in our system (4). Thus we obtain the following deterministic
dynamical system (X0(t), Y0(t)):
(a)	X0(t) + 3X0(t) + ɪQ(X0(t), Y0(t)) = 0,
t	∂X
(b)	Y0 (t) = arg min f(x) , T(t) = {X0 (τ) : τ ∈ (0, t]} .
x∈T(t)
(10)
In the equations (a) and (b) of (10), the pair of processes (X0 (t), Y0 (t)) is a pair of coupled pro-
cesses. In the next Lemma, we show that X0(t) converges to the minimum point of f(x) along the
trajectory of X0 (t) as t → ∞.
Lemma 1. For the deterministic system (10), we have that kX0 (t) - Y0 (t)k2 → 0 as t → ∞, so
that the coupled process (X 0 (t), Y0(t)) converges to diagonal.
Proof. Set X0(t) = P(t), we can write equation (a) in (10) in Hamiltonian form
( X0(t) = P(t),
1 P(t) = -3P(t) - ɪQ(X0(t),Y0(t)).
t ∂X
P2
Set H(X, Y, P) = — + Q(X, Y). Then we have
ddtH (X 0(t),Y 0(t),P(t))
dt	∂	∂
=P (t) ∙ P(t) + ∂X Q(X 0(t),Y 0(t)) ∙ X 0(t) + ∂Y Q(X 0(t),Y 0(t)) ∙ Y 0(t)
3∂	∂
=P(t) ∙ --tP(t) - ∂XQ(x0(t),Y0(t))) + ∂XQ(x0(t),Y0(t)) ∙P(t)
∂
+∂Y Q(x 0(t),Y 0(t)) ∙ Y 0(t)
3∂
=-tkP(t)k2 + ∂YQ(x0(t),Y0(t)) ∙ Y0(t).
As we have Q(X, Y) =，|X - Y∣∣2, we see from (12) that we have
(11)
(12)
ItH(X0(t),Y0(t),P(t)) = -3∣P(t)k2 - (X0(t) - Y0(t)) ∙ Y0(t).	(13)
Notice that by our construction part (b) of the coupled process (10), we have f(Y0(t)) ≤ f(X0(s))
for 0 ≤ S ≤ t. If X0(t) - Y0(t) = 0, then (X0(t) - Y0(t)) ∙ Y0(t) = 0. If X0(t) - Y0(t) = 0,
then we see that Y0(t) = X0(s0) for some 0 ≤ s0 < t, and f(X0(s0)) < f(X0(t)). By continuity
of the trajectory of X0(t) as well as the function f (x), we see that in this case Y0(t) = 0, so that
(X0(t) - Y0(t)) ∙ Y0(t) = 0. Thus we see that (13) actually gives
ddtH (X 0(t),Y 0(t),P (t)) = -1 ∣P (t)k2 ≤ 0.	(14)
From here, we know that H(X0(t), Y0(t), P (t)) keeps decaying until P(t) → 0 and ∣X0(t) -
Y0(t)∣2 → 0, as desired.	口
14
Under review as a conference paper at ICLR 2019
Since f(Y 0(t)) ≤ min f(X0(s)), Lemma 1 tells us that as t → ∞, the deterministic process
0≤s≤t
X0 (t) in (10) approaches the minimum of f along the trajectory traversed by itself. Let us now add
the noise ζ(t) to part (a) of (10), so that we come back to our original system (4). We would like to
argue that with the noise Z(t), We actually have lim min f (X(S)) = f (x*), and thus X(t) → x*
t→∞ 0≤s≤t
when t → ∞ as desired.
Lemma 2. For the process X(t) in part (a) of the system (4), we have X(t) → x* as t → ∞, where
x* is a local minimum of the landscape function f (x).
ξt
Proof. We first notice that we have Z(t) = ε , where ξt ~ N(0,I) is a sequence of i.i.d.
kξt k2
normal, so that kζ(t)k2 = ε, Eζ(t) = 0. Viewing (10) as a small random perturbation (see (Freidlin
& Wentzell, 2012, Chapter 2, Section 1)) of the system (4) we know that for any δ > 0 fixed, we
have
P 0m≤ta≤xT kX(t) - X0(t)k2 ≥ δ → 0
(15)
as ε → 0. From here we know that the process X(t) behaves close to X0 (t) with high probability,
so that by Lemma 1 we know that with high probability we have
lim f(X(t)) - min f(X(s)) =0.
t→∞	0≤s≤t
(16)
Our next step is to improve the above asymptotic to X(t) → x* as t → ∞. Comparing with (16),
we see that it suffices to show
lim min f(X(s)) = f(x*) .
t→∞ 0≤s≤t
(17)
3
To demonstrate (17), we note that when t is large, we can ignore in (4) the damping term -X(t) and
obtain a friction-less dynamics
(a)	: X(t) + ɪQ(X(t), Y(t)) + Z(t) = 0 ,
∂X
(b)	: Y(t) = X(τ) and τ = argmin f(X(τ)) .
τ ∈(0,t]
(18)
Combining Lemma 1, (15) and
-^Q(X, Y) = X - Y we further see that the term -^Q(X, Y)
∂ X	∂ X
also contribute little in (18). Thus part (a) of (18) reduces to a very simple equation
∙∙,.
X(t) + Z(t)=0 .
(19)
Equation (19) enables the process X(t) to explore locally in an ergodic way its neighborhood points,
so that if (17) is not valid, then X(t + dt) will explore a nearby point at which f(X(t + dt))
is less that min f (X (t)), and thus will move to that point. This leads to a further decay in the
0≤s≤t
value of f (X (t)), which demonstrates that in he limit t → ∞ we must have (17), and the Lemma
concludes.	口
Summarizing, we have the Theorem 1.
B	Discussions about the rate of convergence of SHE2 Dynamical
System
Here we provide a short discussion on the convergence rate of the algorithm SHE2. In the previous
appendix we have demonstrated that the system (4) converges via two steps. Step 1 in Lemma 1
shows that the differential equation modeling Nesterov’s accelerated gradient descent (see Su et al.
(2014)) helps the process X(t) to “catch up” with the minimum point Y(t) on its path. Step 2
in Lemma 2 shows that when t → ∞ the noise term ζ(t) helps the process X(t) to reach local
15
Under review as a conference paper at ICLR 2019
Name	∣	TPye & Range
Hyperparameters related to Training
Batch Size	Integer, [16,32]
Learning Rate	Float, [0.0005, 0.002]
Decay Rate of Learning Rate	Float, [0.7, 0.9]
Regularization coefficient	Float, [1 X e-4, 2 X e-3]
Hyperparameters related to Data Augmentation
Resize Type	Boolean, {,keep aspect ratio', ,ignore aspect ratio,}
Crop Type	Discrete, {‘none’, ‘random', ‘center,}
Horizontal Flip	Float (probability), [0,0.5]
Vertical Flip	Float (probability), [0,0.5]
Rotate	Float, [0,20]
Color Jitter	Float, [0,0.005]
Cutout	Boolean, {‘cutout', 'notcutout'}
Tencrop	Boolean, {'tencrop', ‘not tencrop'}
Table 2: Definitions of Hyperparameters
minimum point of f (x). According to the general theory regrading Nesterov’s accelerated gradient
method (see Nesterov and Su et al. (2014)), we naturally expect that the first step has convergence
rate
f(X(t)) - f(Y (t)) ≤ O(1/t2).
For the second step, we see that at each iteration the process is searching for a direction where the
value of the function f (X (t)) will be decreasing, using the random inputs ζ(t). This leads to an
additional time cost of O(1∕ε) to reach the local minimum point X*.
Remark 3. In the Nesterov's algorithm we actually Ususally do not use ；, but thefollowing scheme:
k-1
Xk = yk-ι - s%f(yk-ι), y = Xk + , In(Xk - χk-ι),
k+2
where s > 0 is the small StePSize. Since ； - ； = " +j!-^ ≈ 1 — / as k is large, we obtain our
-.So in real Nesterov's method one can even choose the - to be
3
t + 2.
t
t
C Hyerparameters for ResNet-50 Training
The hyprparameters for ResNet-50 training on both MIT Indoor 67 and flower 102 datasets are
listed in Tabl 2. The range of all hyperparameters are uniformly mapped to the range of [-10, 10]
for optimization. For example, the Boolean hyperparameter “Resize type” is mapped to [-10, 0) →
“keep aspect ratio” and [0, 10) → “ignore aspect ratio”, respectively.
16