Under review as a conference paper at ICLR 2019
Pseudosaccades: A simple ensemble scheme
FOR IMPROVING CLASSIFICATION PERFORMANCE OF
DEEP NETS
Anonymous authors
Paper under double-blind review
Ab stract
We describe a simple ensemble approach that, unlike conventional ensembles,
uses multiple random data sketches (‘pseudosaccades’) rather than multiple classi-
fiers to improve classification performance. Using this simple, but novel, approach
we obtain statistically significant improvements in classification performance on
AlexNeL GoogLeNet, ResNet-50 and ResNet-152 baselines on Imagenet data -
e.g. of the order of 0.3% to 0.6% in Top-1 accuracy and similar improvements in
Top-k accuracy - essentially nearly for free.
1	Introduction
Deep Neural Networks (DNN) are state-of-the-art tools for various machine learning tasks (LeCun
et al., 2015) and have proved especially useful for image classification tasks. For example, the most
recent winners of the Imagenet challenge 1 have all been DNNs.
Although it is not at all well understood - at least in terms of formal learning-theoretic guarantees
- how and why DNNs perform so well 2, empirical understanding of how to construct a DNN is
substantial and growing, and there are many plausible hypotheses regarding their performance. One
striking example of the latter is that not only do DNNs have clear parallels with aspects of human
visual processing, but in controlled psychological experiments they also match human performance
on visual recognition tasks very closely (Serre et al., 2007).
Taking inspiration from nature, in this paper we show that an approximate analogue for saccades
in human visual processing can improve the performance of a carefully-tuned DNN on an image
classification task that it was explicitly designed to solve. More precisely, we use a very simple
ensemble-like approach that employs voting but, unlike typical ensemble approaches, rather than
learning several similar DNNs and obtaining a weighted combination of votes from that ensemble,
instead we use just a single DNN but feed it as input multiple random low-dimensional sketches of
an image and take the DNN’s vote with itself on these sketches to reach a majority verdict.
Using our simple approach we obtain statistically significant improvements in classification perfor-
mance on AlexNet, GoogLeNet, ResNet-50 and ResNet-152 baselines on Imagenet data - e.g. of
the order of 0.3% to 0.6% in Top-1 accuracy - essentially nearly for free. We carry out a comprehen-
sive empirical exploration of our approach, reporting results using different levels of subsampling
and different ensemble sizes, as well as an initial exploration of whether the improvements have any
identifiable systematic component (such as occurring disproportionately in the same class).
2	Motivation
Our approach is inspired by considering saccades in human visual processing, that is eye movements
that focus attention on elements in a visual scene. The human eye has only afew degrees of visual arc
1http://www.image-net.org/challenges/LSVRC/
2For example, while it is known that the VC dimension of a DNN is upper-bounded by the number of nodes
in the network (Anthony & Bartlett, 2009), it is not known why DNNs dramatically outperform ‘wide’ neural
networks with the same number of nodes but fewer hidden layers.
1
Under review as a conference paper at ICLR 2019
of high-resolution imaging capability, and saccades are a mechanism by which a scene can be esti-
mated from high-resolution subsampling of parts of it. In human visual processing this subsampling
is not uniformly at random - We attend to certain features proportionately more often than others -
but we hypothesised that an evolutionary precursor to saccades could have been something closer to
a uniform random sampling of features in a scene and that (if indeed there Was such a precursor) this
must have conferred some selective advantage in order to propagate. MeanWhile recent theoretical
results in Lim & Durrant (2017) shoW that randomly subsampling roWs and columns from an im-
age Without replacement results in - With high probability - an approximately affine transformation
of the original image. Putting these ideas together, since image labels should remain invariant un-
der affine transformations, We speculated that such subsampling could potentially lead to improved
classification performance, perhaps even for an already highly-accurate classifier, by providing the
classifier With multiple loW-dimensional sketches of the same image in a similar Way that saccadic
sampling ofa scene does. We call this subsampling of roWs and columns ‘pseudosaccades’. Because
image classification inputs can be of varying sizes, While most classification algorithms accept only
a fixed size input, a common preprocessing is to convert them to a (usually smaller) standard-sized
input prior to classification. HoWever as far as We are aWare it has not been much exploited before
that such preprocessing offers an opportunity for generating multiple instances of a particular image.
By extracting pseudosaccade sketches ofan image before applying the standardizing preprocessing,
alloWs the generation - for typical image sizes - of thousands of such instances per image. Moreover
unlike cropping and reflection the resulting pseudosaccade images resemble photographs captured
folloWing a change of camera angle and position, While still keeping the subject central in scene
- see figure 1. Confirming our conjectures, We found that exploiting several of these pseudosac-
cade sketches can, indeed, improve classification performance and in the remainder of this paper We
present and discuss our experimental results.
Original: indri, indris, Indri indri, Indri brevicaudatus (385),	Saccade: Madagascar cat, ring-tailed lemur, Lemur Catta (384),
200
400
600
800
1000
1200
1400
1600
1800
F ■	, IBl
500	1000	1500	2000	2500
50	100	150	200	250	300	350	400	450
Figure 1: TWo images incorrectly classified by Alexnet in their original form (left-hand column),
but correctly classified in pseudosaccade form (right-hand column).
3 Experiments and Results
In this section We present details of our experimental protocol and the results of our experiments.
We shoW that the classification accuracy on a single pseudosaccade version of an image is similar
to the accuracy on the original images, given a suitably high projection dimension. Moreover, using
pseudosaccades as a diversity generator, an ensemble classifier employing several pseudosaccade
versions of each image can consistently outperform the classification accuracy of the same classifier
on the original images.
2
Under review as a conference paper at ICLR 2019
3.1	Dataset
We used the validation dataset from the Imagenet Large Scale Visual Recognition (ILSVR) Chal-
lenge 2012 as described in Russakovsky et al. (2015) for our experiments. This dataset comprises of
50000 images, ranging in size from 56x54 pixels to 5005x3646 pixels, where each image is an ex-
ample from one of 1000 distinct classes. The subject of an image (i.e. its class label) is the dominant,
and usually the central, object in that image and therefore elements of attentive viewing are already
present in these images by virtue of the location of the subject. The classes in this dataset range
from broad categories to fine-grained labels - for example one subset of the labels is a classification
of 120 different breeds of dogs. Table 1 summarizes the main characteristics of this dataset.
3.2	Classifiers
We used the winners of ILSVR Challenge from 2012, 2014 and 2015 namely Alexnet (Krizhevsky
et al., 2012), GoogLeNet (Szegedy et al., 2015), and ResNet-50 and ResNet-152 (He et al., 2016)
to represent the state of art in deep neural network classifiers; These classifiers include many of the
latest developments in the evolution of neural networks and each introduced new architectures and
other innovations such as ReLU activation functions and skip connections, resulting in the high-
est accuracies on the Imagenet Large Scale Visual Recognition Challenge for the years 2012, 2014
and 2015 respectively. We used the MATLAB versions of these DNNs implemented in Matcon-
vnet (Vedaldi & Lenc, 2015) and we used the pretrained weights which are tailored for the ILSVR
task to provide a consistent baseline. We note that the pretrained weights for GoogLeNet uses
weights from Princeton instead of Google, which may affect the accuracy for this DNN compared to
the challenge-winning DNN. Also published accuracies in Krizhevsky et al. (2012); Szegedy et al.
(2015); He et al. (2016) for the ILSVR challenge are on the challenge test dataset, while we used
the validation dataset because it has the labels available. Thus our accuracies for these DNNs show
some discrepancies with those published results. Table 2 is based on a similar table from Alom
et al. (2018) and summarizes the characteristics of these DNNs as well as the baseline accuracies
we obtained on the ILSVR challenge validation dataset using them.
3.3	Experimental Procedure
To obtain baseline accuracies for each of the four DNNs we classified each image in the ILSVR
validation set with no preprocessing, other than that implemented by the DNN itself to standardize
the image sizes. The preprocessing carried out by the DNNs is noted in table 2. We measured
the top-1, top-3 and top-5 accuracy for each classifier on the full validation set of 50000 images -
these are also presented in table 2 and we will refer to these results obtained on the original images
(without subsampling) as the ‘baseline classifier’ results.
For our pseudosaccade approach we first fix the ‘projection dimension’ to be an integer
d ∈ {450, 430, 410, 390, 370, 350, 330, 310, 290, 270, 250, 200, 150} and then randomly sample
min(d, width) columns and min(d, height) rows from the images without replacement. As in the
baseline experiments, we apply no further preprocessing, other than that implemented by the DNN
to standardize input size, and we measure the top-1, top-3 and top-5 accuracy for each DNN on all
50000 images in the ILSVR validation dataset. We refer to these results as the ‘saccade classifier’
results. We also store the scores and the top-5 predicted labels for each combination of sampled
projection dimension d, image, and DNN. Since the obtained accuracies, scores, and labels are re-
alizations of random variables we repeated these experiments for each combination of d, image,
and DNN a total of twenty-four times, and we calculated the means and standard deviations for the
top-1, top-3 and top-5 accuracies.
Keeping d fixed we construct an ensemble of size m ∈ {1, 2, . . . , 15} using the scores of between
one and fifteen saccade classifiers by sampling without replacement m sets of top-5 scores from the
24 sets of stored saccade classifier scores. We combine these to obtain the ensemble decision by
simply summing scores for each label. For each d, m, k triple and each classifier we repeated this
process fifty times and we calculated the corresponding means and standard deviations for the top-1,
top-3, and top-5 accuracy.
3
Under review as a conference paper at ICLR 2019
Table 1: Properties of the Imagenet validation dataset
	Min	Mean	Max
Image Count	50000		
Label Count	1000		
Fine-Grained Labels	120	—		
Height	56	430.25	5005
Width	54	490.37	-4288-
Size	3456	231320	18248230
Table 2: Summary of the DNN classifiers
	AlexNet	GoogLeNet	ReSNet-50	ReSNet-152
Architecture	^CNN	LeNet-Inception	Residual Neural Network	
# Convolution Layers	5	57	50	152
# Fully Connected Layers	3	7	1	1
# Parameters	6ΓM	7M	-25.6M-	-603M-
# Multiply and Accumulates	724M	1.43G	3.9G	11.3G	-
Regularization	Batch Normalization	Local Response Normalization	Batch Normalization	
Image Resizing	bicubic scaling (227x227)	bilinear scaling (224x224)		
Top-1 accuracy	54.70%	65.46%	70.39%	72.45%
Top-3 accuracy	71.68%	82.22%	85.55%	-87.05%-
Top-5 accuracy	77.56% —	86.93%	89.66%	90.66% -
3.4	Results
Baseline results for the four DNNs are given in table 2. Results for our pseudosaccade classification
ensembles are given in tables 3 and 4 for ensembles of size 5 and 10 respectively and as well they
are plotted in figure 2 for all values of d, m and k . In figure 2 the orange plane shows the baseline
accuracy for each classifier and top-k combination within a sub-figure. The surface plots show the
average classification error for a given d, m, k triple using pseudosaccades. From tables 3 and 4
we see that these average outcomes are very stable indeed, and if the projection dimension d is
sufficiently high then even a small ensemble can outperform the DNNs working with the original
images at the 5% level of significance (or better) on Top-1, Top-3 and Top-5 classification accuracy.
On the other hand we see that using a single pseudosaccade representation of each of the images,
although we can match or nearly match the baseline accuracy with a projection dimension as high
as d = 350 (see figure 2), with a lower projection dimension we obtain far worse accuracy than
the baseline. The curve comprising the left-hand boundary of each surface plot shows the average
accuracy for a single pseudosaccade plotted against the projection dimension d. Finally we see that
the accuracy of the ensemble exceeds that of the baseline classifiers, even for a small ensemble of
classifiers and small projection dimension, and this behaviour is consistent across all of the classifier
architectures.
3.5	Further experiments
A natural question, given the improvements from pseudosaccades, is whether an ‘ensemble of en-
sembles’ would improve performance further? We started by looking further into the diversity of the
saccade classifiers. Following Kuncheva & Whitaker (2003), we calculate the correlation between
the saccade classifier errors and the baseline classifier errors using
N11N00 - N01N10
Pij =	,.	..	..	..	.
p (N11 + N10 )(N01 + N00 )(N11 + N01)(N10 + N00)
, where i is the base classifier, andj is the saccade version with, and the definitions forN00, N01, N11
and N10 are given in table 5.
4
Under review as a conference paper at ICLR 2019
Table 3: Ensemble classifier accuracy for ensemble size m = 5 and projection dimensions d ∈
{450, 410, 350, 310, 250}, with the standard deviation from a sample of 50 ensembles. Values with
‘*’ exceeded the top-k accuracies of the baseline classifiers by at least 2 standard deviations
		Alexnet (%)		GoogLeNet (%)~~		ReSNet-50 (%)~~		ReSNet-152 (%厂	
Projection Dimensions		Mean	Std DeV	Mean	Std DeV	Mean	Std Dev	Mean	Std Dev
450	Top-1	55.255	0.044 *	65.918	0.051 *	70.726	0.048 *	72.826	0.043 *
	Top-3	72.235	0.033 *	82.526	0.040*	85.743	0.037 *	87.289	0.050 *
	Top-5	77.996	0.038 *	87.207	0.038 *	89.830	0.049 *	90.901	0.032 *
410	Top-1	55.416	0.044 *	65.910	0.047*	70.629	0.075 *	72.759	0.065 *
	Top-3	72.342	0.041 *	82.490	0.040*	85.730	0.051 *	87.264	0.043 *
	Top-5	78.092	0.038 *	87.194	0.036 *	89.771	-0.057-	90.860	0.038 *
350	Top-1	55.520	0.051 *	65.512	^0060-	69.901	-0.074-	72.308	-Q：047-
	Top-3	72.294	0.056 *	82.153	-0.050-	85.250	-0.069-	86.955	-Q：043-
	Top-5	78.061	0.045 *	86.918	-0.055-	89.307	-0.067-	90.613	-Q：044-
310	Top-1	55.038	0.074 *	64.596	^0068-	68.793	-0.103-	71.269	^0064-
	Top-3	71.989	0.052 *	81.429	-0.047-	84.286	-0.080-	86.253	-Q：047-
	Top-5	77.738	0.057 *	86.261	-0.051-	88.512	-0.069-	90.072	^O37-
250	Top-1	53.299	^0076-	61.188	^0071-	65.511	^0T58	68.588	
	Top-3	70.487	^0046-	78.555	-0.063-	81.596	ɪw-	84.113	ɪm-
	Top-5	76.398	0.066	83.878	0.057	86.247	0.068	88.283	0.076
Table 4: The mean ensemble classifier accuracy for ensemble size m = 10 for projection dimensions
d ∈ {450, 410, 350, 310, 250}, with the standard deviation from a sample of 50 ensembles. Values
with ‘*’ exceeded the top-k accuracies of the baseline classifiers by at least 2 standard deviations
		Alexnet (%)		Googlenet (%)		Resnet-50 (%)		ReSnet-152 (%)	
Projection Dimensions		Mean	Std Dev	Mean	Std Dev	Mean	Std Dev	Mean	Std Dev
450	Top-1	55.348	0.035 *	65.987	0.038 *	70.828	0.032 *	72.894	0.026 *
	Top-3	72.307	0.032 *	82.582	0.036 *	85.802	0.028 *	87.365	0.029 *
	Top-5	78.081	0.030 *	87.266	0.031 *	89.903	0.035 *	90.953	0.024 *
410	Top-1	55.568	0.048 *	66.003	0.041 *	70.734	0.055 *	72.863	0.043 *
	Top-3	72.435	0.031 *	82.568	0.034 *	85.828	0.026 *	87.361	0.028 *
	Top-5	78.195	0.035 *	87.278	0.033 *	89.871	0.039 *	90.938	0.025 *
350	Top-1	55.716	0.048 *	65.672	0.041 *	70.065	^0047-	72.500	^004S-
	Top-3	72.444	0.035 *	82.310	^0U36-	85.385	-Q：048-	87.118	-Q：025-
	Top-5	78.233	0.036 *	87.069	0.033 *	89.460	-Q：046-	90.747	0.025 *
310	Top-1	55.300	0.068 *	64.866	^0043-	69.038	^0054	71.497	^0043-
	Top-3	72.203	0.055 *	81.660	-0.045-	84.524	-Q：073-	86.457	^O36
	Top-5	77.966	0.045 *	86.494	-0.044-	88.733	^O54	90.304	-Q：025-
250	Top-1	53.643	^0050-	61.604	-0.055-	65.864	^0T21	68.956	^0093
	Top-3	70.803	-Q：046-	78.955	-0.056-	81.955	-Q：073-	84.484	-0.082-
	Top-5	76.728	0046	84.268	0.048	86.639	0071	88.624	0.066
In table 6, We see that - based on this summary statistic - the accuracy of the saccade classifiers is
highly correlated with that of the corresponding baseline classifier, indicating to us that the classifier
performance is not greatly reduced by pseudosaccade projection. Table 7 meanWhile shoWs that
although the saccade classifier errors are correlated With one another, this is to a lesser degree than
to the baseline classifiers. These facts suggest that there might be little to gain from combining the
Table 5: 2x2 table of the relationship betWeen the classifiers Di and Dj
	Dj Correct	Dj Wrong
Di Correct	N11	-N10
Di Wrong	Noi	N。。
5
Under review as a conference paper at ICLR 2019
Dimension
Dimension
Dimension
Ensemble Projection
Ensemble Projection
Dimension
Dimension
Size
Size
Projection
Dimension
Projection
Dimension
Projection
Dimension
Projection
Dimension
Ensemble Projection
Dimension
Ensemble Projection
EnSembIe Projection
Dimension
Ensemble Projection
-Hd Dimension
Ensemble Projection
Ensemble Projection
Figure 2:	Accuracy vs ensemble size and projection dimension. Reference plane shows the accuracy
for the baseline classifier
pseudosaccade ensembles from different DNNs into a larger ensemble. However since all of the
accuracies are already high it seemed worthwhile to examine where the improvements were coming
from - were these for similar class labels for every classifier for example?
Digging deeper we observed that the classification accuracy of the individual classes is not uniformly
affected by pseudosaccades. Moreover, at this lower level of granularity we see that the different
architectures do tend to be affected by the pseudosaccades differently.
Tables 10,11 and 9 show lists of predicted class labels for a given class label for ResNet-152, with
projection dimension 390 and ensemble size 5. Note that there are 50 instances in each of the
6
Under review as a conference paper at ICLR 2019
true class labels, and we omitted predicted labels where there was only a single prediction or two
predictions for reasons of space and readability.
In table 10, we present a list of labels for which the ResNet-152 classifier obtained less than 20%
recall - considering this and similar tables for AIexNet and GoogLeNet (omitted due to space Con-
straints) we observed that the ensemble of pseudosaccade classifiers performs similarly to the base-
line classifier on labels that are also difficult for the baseline classifier to predict accurately, but
we also saw that the different classifier architecture have their own sets of ‘difficult labels’ that are
different.
Finally tables 9 and 11 give examples of class labels where the ensemble classifier respectively
gives either a large improvement or is much worse (±10%) on classification accuracy for these
classes. Comparing with the other architectures we found that the saccade classifiers were affected
differently on different classifier architecture.
Thus, although high-level summary statistics seemed to indicate little diversity between the different
ensemble classifiers, a more principled investigation reveals that the errors for both the original
DNNs and for the corresponding pseudosaccade ensembles arise from different classes and different
instances in the dataset.
We therefore constructed two ensemble classifiers - one using the four baseline DNNs and one
which combined four pseudosaccade ensembles to see if further improvements were possible. We
used five-fold cross-validation on the validation set data to train a shallow neural network with a sin-
gle hidden layer with ReLU activations on the baseline scores for 40000 images from the validation
dataset to learn a weighting function for the ensemble of baseline classifiers. We used the average
and maximum scores from pseudosaccade versions of the four DNNs for the same 40000 images to
train a similar network to weight the ‘ensemble of ensembles’. We evaluated both ensembles using
the 10000 remaining held-out images from the validation dataset and estimated the top-1,top-3 and
top-5 accuracies for both ensembles with the cross-validation error. We carried out one round of
five-fold cross-validation for the baseline classifiers and 50 rounds for the pseudosaccade classifiers,
for different d, m, k triples and calculated the mean accuracies and their standard deviations. For
both types of ensemble we saw substantial improvements over the original baseline accuracies and,
consistent with our earlier experiments, the pseudosaccade ensembles were yet again able to outper-
form the ensemble of baseline DNN classifiers. Figure 3, shows the accuracy of the DNN ensemble
versus the pseudosaccade ensembles for different d, m, k triples. The horizontal orange plane in-
dicates the (average) accuracy of the DNN ensemble. The pseudosaccade ensembles outperforms
the increased Top-1 accuracy baseline of 75.78% by 0.3%, and the accuracy of the best perform-
ing classifier Resnet-152 by 3.7%. We conjecture that further, possibly minor, improvements in
accuracy may be possible using a more careful approach to learn the weighting function.
Table 6: The average classifier correlation ρbase,saccade between baseline classifier and saccade
classifiers ____________________________________________________________________________
Saccade Dimensions	AlexNet	GoogLeNet	Resnet-50	Resnet-152
^450	0.8959	0.8952	0.8916	0.8949
^430	0.8851	-0.882	-0.877	0.8811
^410	0.8753	0.8694	0.8615	0.8681
190	^0865-	0.8552	0.8479	0.8552
170	0.8524	0.8368	0.8301	0.8377
150	0.8375	0.8164	0.8078	0.8164
130	0.8221	0.7937	0.7853	0.7943
ʒlθ	0.8039	0.7683	0.7601	0.7715
-290	^0785-	0.7418	0.7335	0.7449
-270	^0764-	0.7119	0.7068	0.7174
"^50	-0:742-	-0.678	-0.676	0.6889
-200	0.6727	0.5729	0.5806	0.5977
150	-	0.5626	0.437	∙	04563	04666
7
Under review as a conference paper at ICLR 2019
Table 7: The average classifier correlation ρsaccade1 ,saccade2 between all pairs of saccade classifiers
Saccade Dimensions	AlexNet	GoogLeNet	ResNet-50	ResNet-152
^450	0.8847	0.8830	0.8809	0.8876
^430	0.8744	0.8702	0.8682	0.8754
^410	0.8657	0.8597	0.8562	0.8648
^390	0.8564	0.8484	0.8456	0.8543
^370	0.8448	0.8344	0.8337	0.8419
^350	0.8320	0.8202	0.8196	0.8266
^330	0.8196	0.8060	0.8059	0.8121
^310	0.8077	0.7917	0.7938	0.7991
^^90	0.7956	0.7771	0.7792	0.7849
^^70	0.7835	0.7621	0.7667	0.7700
^^50	0.7717	0.7466	0.7528	0.7572
^^00	0.7369	0.7027	0.7138	0.7211
150	—	0.6942	0.6473	-	0.6666	0.6750	一
Table 8: Top-k accuracies for the “ensemble of ensembles” for ensemble size m ∈
{1, 5, 10, 15},projection dimensions d ∈ {450, 410, 350, 310, 250}. Entries with ‘*’ denotes sta-
tistically significant improvements over the baseline accuracies (top-1: 75.78%, top-3: 87.65%,
top-5: 90.35%)	_________________________________________________________________________
		One Saccade		5 Saccades		10 Saccades		15 Saccades	
Projection Dimensions		Mean	Std Dev	Mean	Std Dev	Mean	Std Dev	Mean	Std Dev
450	Top-1	75.73	-0∏2	76.08	0.09*	76.16	0.05 *	76.17	0.05 *
	Top-3	87.51	-0∏O	87.98	0.07 *	88.13	0.05 *	88.21	0.04*
	Top-5	90.19	-008	90.74	0.07 *	90.86	0.05 *	90.92	0.04*
410	Top-1	75.55	-0∏4	75.96	0.08 *	76.04	0.06*	76.10	0.05 *
	Top-3	87.38	ɪn	87.99	0.07 *	88.15	0.05 *	88.22	0.03 *
	Top-5	90.12	-008	90.76	0.07*	90.91	0.06*	90.97	0.05 *
350	Top-1	74.65	-0∏5	75.36	ɪn	75.51	^008	75.57	-006
	Top-3	86.83	-009	87.75	-007	87.95	0.06*	88.06	0.05 *
	Top-5	89.61	-0T12	90.57	0.08 *	90.79	0.07 *	90.91	0.05 *
310	Top-1	73.58	-0∏5	74.56	ɪn	74.73	^008	74.80	-006
	Top-3	85.91	-0T12	87.06	-009	87.31	-007	87.46	-005
	Top-5	88.92	-0T13	90.13	-0T1O	90.50	0.07*	90.67	0.06*
250	Top-1	70.34	-0∏9	71.78	ɪn	71.96	ɪn	72.01	-009
	Top-3	83.33	-0T15	85.04	-009	85.35	-008	85.48	-006
	Top-5	86.65	0.17	88.44	0.11	88.89	0.09	89.08	0.06
Figure 3:	Top-1, top-3 and top-5 accuracy of the “ensemble of ensembles” vs ensemble size and
projection dimension
8
Under review as a conference paper at ICLR 2019
Table 9: Labels where ensemble method performed significantly better (≥ 10%) than the baseline
Resnet-152 Imagenet classifier. Number of instances for which the given label was returned by
classifier in brackets.
true label	base label	saccade label	ensemble label
rock crab	dungeness crab (4) rock crab (28) hermit crab (4)	rock crab (30) crayfish (3) hermit crab (3)	dungeness crab (3) rock crab (33) hermit crab (4)
bedlington terrier	bedlington terrier (28)	bedlington terrier (42)	bedlington terrier (43)
labrador retriever	bloodhound (3) saluki (3) golden retriever (3) labrador retriever (34)	saluki (3) labrador retriever (36)	labrador retriever (39)
bell cote	bell cote (26) chime (3) church (11) monastry (4)	bell cote (31) church (8) monastry (4)	bell cote (31) church (9) monastry (4)
bow	bow (30)	bow (32)	bow (35)
necklace	chain (3) necklace (40)	necklace (46)	necklace (46)
pitcher	pitcher (19) vase (4) water jug (6)	pitcher (25) vase(4) water jug (5)	pitcher (24) vase (3) water jug (8)
plastic bag	PlastiC bag (24)		plastic bag (26)	PlastiC bag (29)
hen of the wood	coral fungus (3) hen of the wood (35)	coral fungus (4) hen of the wood (36)	hen of the wood (40)
Table 10: Labels where Resnet-152 Imagenet classifier achieved ≤ 20% recall. Number of instances
for which the given label was returned by classifier in brackets.
true label	base label	saccade label	ensemble label
cassette player	cassette player (10) cd player (4) radio (3) tape player (22)	cassette player (9) cd player (4) tape player (23)	cassette player (9) cd player (4) radio (3) tape player (21)
crt screen	desk (6) desktop computer (8) monitor (4) crt screen (8) television (8)	desk (6) desktop computer (8) monitor (5) crt screen (9) television (9)	desk (5) desktop computer (8) laptop computer (3) monitor (6) crt screen (9) television (9)
sunglass	sunglass (11) sunglasses (19)	sunglass (10) sunglasses (19)	sunglass (11) sunglasses (16)
4 Conclusions and future work
We demonstrated that using a very simple, and computationally cheap, ‘pseudosaccade’ ensemble
learning approach can improve the image classification performance of DNNs. This improvement
is small but statistically significant at the 5% level and requires no complicated training or opti-
mization, simply selecting two integer parameters d and m. In our experiments setting d ≥ 350
and m ≥ 5 worked well, with the approach much more sensitive to over-small values of d than of
m. Following a careful analysis of the sources of error in our classification problem, we showed
that these improvements also propagate to a weighted ensemble of pseudosaccade versions of (off-
the-shelf) DNNs. An open problem is whether a (simple, or low overhead) non-uniform sampling
scheme for constructing pseudosaccade data exists that could improve performance further, possi-
bly mediated by a scene-dependent prior. Human visual processing suggests that such a scheme
should be at least a possibility. We are examining non-uniform sampling schemes such as stratified
sampling, and also techniques such as seam-carving, with a view to progress in this direction.
9
Under review as a conference paper at ICLR 2019
Table 11: Labels where ensemble method performed significantly worse (≥ 10%) than the baseline
Resnet-152 Imagenet classifier. Number of instances for which the given label was returned by
classifier in brackets.
true label	base label	saccade label	ensemble label
mantis	walking stick (3) mantis (37)	walking stick (5) mantis (31)	walking stick (5) mantis (32)
abaya	abaya (41)	abaya (37) cloak (3)		abaya (36) cloak (3)	
perfume	perfume (40)	perfume (35)	perfume (34)
wok	wok (28) hot pot (10)	dutch oven (4) frying pan (3) wok (22) hot Pot (9)	frying Pan (4) wok (23) hot Pot (11)
References
Md Zahangir Alom, Tarek M Taha, Christopher Yakopcic, Stefan Westberg, Mahmudul Hasan,
Brian C Van Esesn, Abdul A S Awwal, and Vijayan K Asari. The history began from alexnet: A
comprehensive survey on deep learning approaches. arXiv preprint arXiv:1803.01164, 2018. 3
Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations. cambridge
university press, 2009. 1
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016. 3
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012. 3
Ludmila I Kuncheva and Christopher J Whitaker. Measures of diversity in classifier ensembles and
their relationship with the ensemble accuracy. Machine learning, 51(2):181-207, 2003. 4
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436, 2015. 1
Nick Lim and Robert J Durrant. Linear dimensionality reduction in linear time: Johnson-
lindenstrauss-type guarantees for random subspace. arXiv preprint arXiv:1705.06408, 2017. 2
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual
recognition challenge. International Journal of Computer Vision, 115(3):211-252, 2015. 3
Thomas Serre, Aude Oliva, and Tomaso Poggio. A feedforward architecture accounts for rapid cat-
egorization. Proceedings of the National Academy of Sciences, 104(15):6424-6429, 2007. ISSN
0027-8424. doi: 10.1073/pnas.0700622104. URL http://www.pnas.org/content/
104/15/6424. 1
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1-9, 2015.
3
Andrea Vedaldi and Karel Lenc. Matconvnet: Convolutional neural networks for matlab. In Pro-
ceedings of the 23rd ACM international conference on Multimedia, pp. 689-692. ACM, 2015.
3
10