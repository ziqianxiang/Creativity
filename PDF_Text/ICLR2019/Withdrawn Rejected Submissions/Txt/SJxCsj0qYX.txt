Under review as a conference paper at ICLR 2019
Stackelberg GAN: Towards Provable Mini-
max Equilibrium via Multi-Generator Archi-
TECTURES
Anonymous authors
Paper under double-blind review
Ab stract
We study the problem of alleviating the instability issue in the GAN training
procedure via new architecture design. The discrepancy between the minimax
and maximin objective values could serve as a proxy for the difficulties that the
alternating gradient descent encounters in the optimization of GANs. In this work,
we give new results on the benefits of multi-generator architecture of GANs. We
show that the minimax gap shrinks to as the number of generators increases
with rate O(1/). This improves over the best-known result of O(1/1 2). At the
core of our techniques is a novel application of Shapley-Folkman lemma to the
generic minimax problem, where in the literature the technique was only known
to work when the objective function is restricted to the Lagrangian function of
a constraint optimization problem. Our proposed Stackelberg GAN performs
well experimentally in both synthetic and real-world datasets, improving Frechet
Inception Distance by 14.61% over the previous multi-generator GANs on the
benchmark datasets.
1 Introduction
Generative Adversarial Nets (GANs) are emerging objects of study in machine learning, computer
vision, natural language processing, and many other domains. In machine learning, study of such a
framework has led to significant advances in adversarial defenses (Xiao et al., 2018; Samangouei et al.,
2018) and machine security (Athalye et al., 2018; Samangouei et al., 2018). In computer vision and
natural language processing, GANs have resulted in improved performance over standard generative
models for images and texts (Goodfellow et al., 2014), such as variational autoencoder (Kingma &
Welling, 2013) and deep Boltzmann machine (Salakhutdinov & Larochelle, 2010). A main technique
to achieve this goal is to play a minimax two-player game between generator and discriminator under
the design that the generator tries to confuse the discriminator with its generated contents and the
discriminator tries to distinguish real images/texts from what the generator creates.
Despite a large amount of variants of GANs, many fundamental questions remain unresolved. One of
the long-standing challenges is designing universal, easy-to-implement architectures that alleviate the
instability issue of GANs training. Ideally, GANs are supposed to solve the minimax optimization
problem (Goodfellow et al., 2014), but in practice alternating gradient descent methods do not clearly
privilege minimax over maximin or vice versa (page 35, Goodfellow (2016)), which may lead to
instability in training if there exists a large discrepancy between the minimax and maximin objective
values. The focus of this work is on improving the stability of such minimax game in the training
process of GANs.
To alleviate the issues caused by the large minimax gap, our study is motivated by the so-called
Stackelberg competition in the domain of game theory. In the Stackelberg leadership model, the
players of this game are one leader and multiple followers, where the leader firm moves first and
then the follower firms move sequentially. It is known that the Stackelberg model can be solved to
find a subgame perfect Nash equilibrium. We apply this idea of Stackelberg leadership model to
the architecture design of GANs. That is, we design an improved GAN architecture with multiple
generators (followers) which team up to play against the discriminator (leader). We therefore name
our model Stackelberg GAN. Our theoretical and experimental results establish that: GANs with
multi-generator architecture have smaller minimax gap, and enjoy more stable training performances.
Our Contributions. This paper tackles the problem of instability during the GAN training procedure
with both theoretical and experimental results. We study this problem by new architecture design.
1
Under review as a conference paper at ICLR 2019
(g) Step 19k.
(d) Step 25k.
(c) Step 19k.
(h) Step 25k.
Figure 1: Left Figure, Top Row: Standard GAN training on a toy 2D mixture of 8 Gaussians. Left
Figure, Bottom Row: Stackelberg GAN training with 8 generator ensembles, each of which is
denoted by one color. Right Figure: Stackelberg GAN training with 10 generator ensembles on
fashion-MNIST dataset without cherry pick, where each row corresponds to one generator.
•	We propose the Stackelberg GAN framework of multiple generators in the GAN architecture.
Our framework is general since it can be applied to all variants of GANs, e.g., vanilla GAN,
Wasserstein GAN, etc. It is built upon the idea of jointly optimizing an ensemble of GAN
losses w.r.t. all pairs of discriminator and generator.
Differences from prior work. Although the idea of having multiple generators in the GAN
architecture is not totally new, e.g., MIX+GAN (Arora et al., 2017), MGAN (Hoang et al.,
2018), MAD-GAN (Ghosh et al., 2017) and GMAN (Durugkar et al., 2016), there are key
differences between Stackelberg GAN and prior work. a) In MGAN (Hoang et al., 2018)
and MAD-GAN (Ghosh et al., 2017), various generators are combined as a mixture of
probabilistic models with assumption that the generators and discriminator have infinite ca-
pacity. Also, they require that the generators share common network parameters. In contrast,
in the Stackelberg GAN model we allow various sampling schemes beyond the mixture
model, e.g., each generator samples a fixed but unequal number of data points independently.
Furthermore, each generator has free parameters. We also make no assumption on the model
capacity in our analysis. This is an important research question as raised by Arora et al.
(2018). b) In MIX+GAN (Arora et al., 2017), the losses are ensembled with learned weights
and an extra regularization term, which discourages the weights being too far away from
uniform. We find it slightly unnecessary because the expressive power of each generator
already allows implicit scaling of each generator. In the Stackelberg GAN, we apply equal
weights for all generators and obtain improved guarantees. c) In GMAN (Durugkar et al.,
2016), there are multiple discriminators while it is unclear in theory why multi-discriminator
architecture works well. In this paper, we provide formal guarantees for our model.
•	We prove that the minimax duality gap shrinks as the number of generators increases (see
Theorem 1 and Corollary 2). Unlike the previous work, our result has no assumption on the
expressive power of generators and discriminator, but instead depends on their non-convexity.
With extra condition on the expressive power of generators, we show that Stackelberg GAN
is able to achieve -approximate equilibrium with O(1/) generators (see Theorem 3). This
improves over the best-known result in (Arora et al., 2017) which requires generators as
many as Oe(1/2). At the core of our techniques is a novel application of the Shapley-
Folkman lemma to the generic minimax problem, where in the literature the technique was
only known to work when the objective function is restricted to the Lagrangian function
of a constrained optimization problem (Zhang et al., 2018). This results in tighter bounds
than that of the covering number argument as in (Arora et al., 2017). We also note that
MIX+GAN is a heuristic model which does not exactly match the theoretical analysis in
(Arora et al., 2017), while this paper provides formal guarantees for the exact model of
Stackelberg GAN.
•	We empirically study the performance of Stackelberg GAN for various synthetic and real
datasets. We observe that without any human assignment, surprisingly, each generator
automatically learns balanced number of modes without any mode being dropped (see
Figure 1). Compared with other multi-generator GANs with the same network capacity, our
experiments show that Stackelberg GAN enjoys 26.76 Frechet Inception Distance on CIFAR-
10 dataset while prior results achieve 31.34 (smaller is better), achieving an improvement of
14.61%.
2
Under review as a conference paper at ICLR 2019
Figure 2: Architecture of Stackelberg GAN. We ensemble the losses of various generator and
discriminator pairs with equal weights.
2	Stackelberg GAN
Before proceeding, we define some notations and formalize our model setup in this section.
Notations. We will use bold lower-case letter to represent vector and lower-case letter to represent
scalar. Specifically, we denote by θ ∈ Rt the parameter vector of discriminator and γ ∈ Rg the
parameter vector of generator. Let Dθ(x) be the output probability of discriminator given input x,
and let Gγ (z) represent the generated vector given random input z. For any function f (u), we denote
by f *(v) := supu{uTV - f (u)} the conjugate function of f. Let CIf be the convex closure of f,
which is defined as the function whose epigraph is the convex closed hull of that of function f . We
define Clf := -Cl(-f). We will use I to represent the number of generators.
2.1	Model Setup
Preliminaries. The key ingredient in the standard GAN is to play a zero-sum two-player game
between a discriminator and a generator — which are often parametrized by deep neural networks
in practice — such that the goal of the generator is to map random noise z to some plausible
images/texts GY(z) and the discriminator Dθ(∙) aims at distinguishing the real images/texts from
what the generator creates.
For every parameter implementations γ and θ of generator and discriminator, respectively, denote by
the payoff value
φ(γ； θ) ：= Eχ~Pdf (Dθ(x)) + Ez~Pzf (1 - Dθ(Gγ(z))),
where f (∙) is some concave, increasing function. Hereby, Pd is the distribution of true images/texts
and Pz is a noise distribution such as Gaussian or uniform distribution. The standard GAN thus
solves the following saddle point problems:
inf sup φ(γ; θ),
γ∈Rg θ∈Rt
or sup inf φ(γ; θ).
θ∈Rt γ∈Rg
(1)
For different choices of function f, problem (1) leads to various variants of GAN. For example, when
f(t) = log t, problem (1) is the classic GAN; when f(t) = t, it reduces to the Wasserstein GAN. We
refer interested readers to the paper of Nowozin et al. (2016) for more variants of GANs.
Stackelberg GAN. Our model of Stackelberg GAN is inspired from the Stackelberg competition
in the domain of game theory. Instead of playing a two-player game as in the standard GAN, in
Stackelberg GAN there are I + 1 players with two firms — one discriminator and I generators. One
can make an analogy between the discriminator (generators) in the Stackelberg GAN and the leader
(followers) in the Stackelberg competition.
Stackelberg GAN is a general framework which can be built on top of all variants of standard GANs.
The objective function is simply an ensemble of losses w.r.t. all possible pairs of generators and
discriminator: Φ(γ1, ..., γI; θ) := PiI=1 φ(γi; θ). Thus it is very easy to implement. The Stackelberg
GAN therefore solves the following saddle point problems:
*
W
inf sup 1Φ(γι, ...,γι; θ),	or
γ1 ,...,γI ∈Rg θ∈Rt I
q* = SUp inf	1Φ(γι,…,γι; θ).
θ∈Rt γ1,...,γI ∈Rg I
3
Under review as a conference paper at ICLR 2019
We term w* - q* the minimax (duality) gap. We note that there are key differences between
the naive ensembling model and ours. In the naive ensembling model, one trains multiple GAN
models independently and averages their outputs. In contrast, our Stackelberg GAN shares a unique
discriminator for various generators, thus requires jointly training. Figure 2 shows the architecture of
our Stackelberg GAN.
How to generate samples from Stackelberg GAN? In the Stackelberg GAN, we expect that each
generator learns only a few modes. In order to generate a sample that may come from all modes, we
use a mixed model. In particular, we generate a uniformly random value i from 1 to I and use the i-th
generator to obtain a new sample. Note that this procedure in independent of the training procedure.
3	Analysis of Stackelberg GAN
In this section, we develop our theoretical contributions and compare our results with the prior work.
3.1	Minimax Duality Gap
We begin with studying the minimax gap of Stackelberg GAN. Our main results show that the
minimax gap shrinks as the number of generators increases.
To proceed, denote by hi(u. := infγi∈Rg(-φ(γi; ∙))*(u∕, where the conjugate operation is w.r.t.
the second argument of φ(γi; ∙). We clarify here that the subscript i in h indicates that the function
hi is derived from the i-th generator. The argument of hi should depend on i, so we denote it by ui .
Intuitively, h serves as an approximate convexification of -φ(γi, ∙) w.r.t the second argument due to
the conjugate operation. Denote by clhi the convex closure of hi :
{t+2	t+2	t+2
X aj hi (uij ) : ue =	ajuij,	aj = 1, aj ≥ 0 .
j=1	j=1	j=1	
CIhi represents the convex relaxation of h because the epigraph of CIhi is exactly the convex hull of
epigraph of h by the definition of clhi. Let ∆minimax = infγι,…γ∈Rg supθ∈Rt 1 Φ(γ1,..., YI; θ)-
infγι,...,γι∈Rg suPθ∈Rt 1Φ(Y1,…，YI； θ), and ∆maXimin = supθ∈Rt infγι,...,γι∈Rg 1 Φ(γι, ■.； YI； θ)-
suPθ∈Rt infγι,...,γι∈Rg 1 Φ(γι,…,YI； θ), where Φ(γι,…,YI; θ) := PI=I blφ(γi; θ) and -Clφ(γi; θ)
is the convex closure of -φ(Yi; θ) w.r.t. argument θ. Therefore, ∆θmaximin + ∆θminimax measures the
non-convexity of objective function w.r.t. argument θ. For example, it is equal to 0 if and only if
φ(Yi; θ) is concave and closed w.r.t. discriminator parameter θ.
We have the following guarantees on the minimax gap of Stackelberg GAN.
Theorem 1. Let ∆iγ := supu∈Rt {hi (u) - Clhi(u)} ≥ 0 and ∆γworst := maxi∈[I] ∆iγ. Denote by t
the number ofparameters ofdiscriminator, i.e., θ ∈ Rt. Suppose that hi(∙) is continuous and domhi
is compact and convex. Then the duality gap can be bounded by
0 ≤ w* - q* ≤ ∆θminimax + ∆θmaximin +,
provided that the number of generators I > t++1 ∆worst.
Remark 1. Theorem 1 makes mild assumption on the continuity of loss and no assumption on
the model capacity of discriminator and generators. The analysis instead depends on their non-
convexity as being parametrized by deep neural networks. In particular, ∆iγ measures the divergence
between the function value of hi and its convex relaxation Clhi; When φ(Yi; θ) is convex w.r.t.
argument Yi, ∆iγ is exactly 0. The constant ∆γworst is the maximal divergence among all generators,
which does not grow with the increase of I. This is because ∆γworst measures the divergence of
only one generator and when each generator for example has the same architecture, we have
∆γworst = ∆1γ = ■■■ = ∆Iγ. Similarly, the terms ∆θminimax and ∆θmaximin characterize the non-convexity
of discriminator. When the discriminator is concave such as logistic regression and support vector
machine, ∆θminimax = ∆θmaximin = 0 and we have the following straightforward corollary about the
minimax duality gap of Stackelberg GAN.
Corollary 2. Under the settings of Theorem 1, when φ(Yi; θ) is concave and closed w.r.t. discrimina-
tor parameter θ and the number ofgenerators I > t++1 ∆worst, we have 0 ≤ w* 一 q* ≤ e.
4
Under review as a conference paper at ICLR 2019
3.2 Existence of Approximate Equilibrium
The results of Theorem 1 and Corollary 2 are independent of model capacity of generators and
discriminator. When we make assumptions on the expressive power of generator as in (Arora et al.,
2017), we have the following guarantee (2) on the existence of 3-approximate equilibrium.
Theorem 3. Under the settings of Theorem 1, suppose that for any ξ > 0, there exists a generator G
such that EX〜Pd,z〜PzkG(Z) 一 x∣∣2 ≤ ξ. Let the discriminator and the generators be L-LiPsChitz
w.r.t. inputs and parameters, respectively. Thenfor any 3 > 0, there exist I = t++1 ∆worst generators
GY 口…,GY * and a discriminator Dθ* such that for some value V ∈ R,
∀Y1,...,YI ∈ Rg,	Φ(γι,...,γι; θ*) ≤ V + 3
∀θ ∈ Rt ,
Φ(γ* ,...,γ* ； θ) ≥ V -3.
(2)
Related Work. While many efforts have been devoted to empirically investigating the performance
of multi-generator GAN, little is known about how many generators are needed so as to achieve
certain equilibrium guarantees. Probably the most relevant prior work to Theorem 3 is that of (Arora
et al., 2017). In particular, Arora et al. (2017) showed that there exist I = 100t ∆2 generators and one
discriminator such that 3-approximate equilibrium can be achieved, provided that for all x and any
ξ > 0, there exists a generator G such that EZ〜PzkG(Z) 一 x∣∣2 ≤ ξ. Hereby, ∆ is a global upper
bound of function |f|, i.e., f ∈ [-∆, ∆]. In comparison, Theorem 3 improves over this result in two
aspects: a) the assumption on the expressive power of generators in (Arora et al., 2017) implies our
condition EX〜Pd,z〜PzkG(Z) 一 x∣∣2 ≤ ξ. Thus our assumption is weaker. b) The required number of
generators in Theorem 3 is as small as t++1 ∆worst. We note that ∆worst《2∆ by the definition of
∆Yworst. Therefore, Theorem 3 requires much fewer generators than that of (Arora et al., 2017).
4 Architecture, Capacity and Mode Collapse/Dropping
In this section, we empirically investigate the effect of network architecture and capacity on the mode
collapse/dropping issues for various multi-generator architecture designs. Hereby, the mode dropping
refers to the phenomenon that generative models simply ignore some hard-to-represent modes of real
distributions, and the mode collapse means that some modes of real distributions are "averaged" by
generative models. For GAN, it is widely believed that the two issues are caused by the large gap
between the minimax and maximin objective function values (see page 35, Goodfellow (2016)).
Our experiments verify that network capacity (change of width and depth) is not very crucial for
resolving the mode collapse issue, though it can alleviate the mode dropping in certain senses. Instead,
the choice of architecture of generators plays a key role. To visualize this discovery, we test the
performance of varying architectures of GANs on a synthetic mixture of Gaussians dataset with 8
modes and 0.01 standard deviation. We observe the following phenomena:
Naively increasing capacity of one-generator architecture does not alleviate mode collapse. It
shows that the multi-generator architecture in the Stackelberg GAN effectively alleviates the mode
collapse issue. Though naively increasing capacity of one-generator architecture alleviates mode
dropping issue, for more challenging mode collapse issue, the effect is not obvious (see Figure 3).
(c) Stackelberg GAN with
8 generators of architecture
2-16-2.
(b) GAN with 1 generator
of architecture 2-128-256-
512-1024-2.
Figure 3: Comparison of mode collapse/dropping issue of one-generator and multi-generator architec-
tures with varying model capacities. (a) and (b) show that increasing the model capacity can alleviate
the mode dropping issue, though it does not alleviate the mode collapse issue. (c) Multi-generator
architecture with even small capacity resolves the mode collapse issue.
Stackelberg GAN outperforms multi-branch models. We compare performance of multi-branch
GAN and Stackelberg GAN with objective functions:
(a) GAN with 1 generator
of architecture 2-128-2.
(Multi-Branch GAN) Φ (| X Yi； θ
1I
vs. (Stackelberg GAN) I E φ(γi; θ).
5
Under review as a conference paper at ICLR 2019
Hereby, the multi-branch GAN has made use of extra information that the real distribution is Gaussian
mixture model with probability distribution function 1 PI=1 PNi (x), so that each γi tries to fit one
component. However, even this we observe that with same model capacity, Stackelberg GAN
significantly outperforms multi-branch GAN (see Figure 4 (a)(c)) even without access to the extra
information. The performance of Stackelberg GAN is also better than multi-branch GAN of much
larger capacity (see Figure 4 (b)(c)).
(b) 8-branch GAN with gen-
erator architecture 2-128-
256-512-1024-2.
(c) Stackelberg GAN with 8
generators of architecture 2-
16-2.
(a) 8-branch GAN with gen-
erator architecture 2-16-2.
Figure 4: Comparison of mode collapse issue of multi-branch and multi-generator architectures with
varying model capacities. (a) and (b) show that increasing the model capacity can alleviate the mode
dropping issue, though it does not alleviate the mode collapse issue. (c) Multi-generator architecture
with much smaller capacity resolves the mode collapse issue.
Generators tend to learn balanced number of modes when they have same capacity. We observe
that for varying number of generators, each generator in the Stackelberg GAN tends to learn equal
number of modes when the modes are symmetric and every generator has same capacity (see Figure
5).
(a) Two generators.	(b) Four generators.	(c) Six generators.
Figure 5: Stackelberg GAN with varying number of generators of architecture 2-128-256-512-1024-2.
5	Experiments
In this section, we verify our theoretical contributions by the experimental validation.
5.1	MNIST Dataset
We first show that Stackelberg GAN generates more diverse images on the MNIST dataset (LeCun
et al., 1998) than classic GAN. We follow the standard preprocessing step that each pixel is normalized
via subtracting it by 0.5 and dividing it by 0.5. The detailed network setups of discriminator and
generators are in Table 4.
Figure 6 shows the diversity of generated digits by Stackelberg GAN with varying number of
generators. When there is only one generator, the digits are not very diverse with many repeated "1"’s
and much fewer "2"’s. As the number of generators increases, the generated images tend to be more
diverse. In particular, for 10-generator Stackelberg GAN, each generator is associated with one or
two digits without any digit being missed.
5.2	Fashion-MNIST Dataset
We also observe better performance by the Stackelberg GAN on the Fashion-MNIST dataset. Fashion-
MNIST is a dataset which consists of 60,000 examples. Each example is a 28 × 28 grayscale image
associating with a label from 10 classes. We follow the standard preprocessing step that each pixel is
normalized via subtracting it by 0.5 and dividing it by 0.5. We specify the detailed network setups of
discriminator and generators in Table 4.
Figure 7 shows the diversity of generated fashions by Stackelberg GAN with varying number of
generators. When there is only one generator, the generated images are not very diverse without
6
Under review as a conference paper at ICLR 2019
S 5 1
Gb 7
SG9
ʃ 6 9
6 5 q
6t7
4 5 7
S 4夕
4 5732 2^夕,，
4 4 V4Z-3Z?。/ 7
/ 1 7 / ZF 7 ( / /
Zf t. I. 7 JJ 7 7 7 ? sʃ
I 0吓<9，697口37
Λ/ ∙yv 41- ΛMt // H /r /r Iv
(/ / / / S / / I
I C H G⅛∙「6 / ⅜> / T-
⅛κ∕ — f / I I J Λ-0 -.7
6 i / 7 ^∙r∙ / q 5，1
2/Q 7∕∕c2q7
Oq/6 7 V 6 S- Il /
Λ7 O I /
? / Crq32
ʃ 7 4» 3 Z
Figure 6: Standard GAN vs. Stackelberg GAN on the MNIST dataset without cherry pick. Left
Figure: Digits generated by the standard GAN. It shows that the standard GAN generates many
"1"’s which are not very diverse. Middle Figure: Digits generated by the Stackelberg GAN with 5
generators, where every two rows correspond to one generator. Right Figure: Digits generated by
the Stackelberg GAN with 10 generators, where each row corresponds to one generator.
F	5	9	5	8	S	5 5/ f
ðaaooð^ooa
(£>	⅛	6	Q	4	4	6。QC
7 7 7 夕7
7彳/1
Crq , 1
7 19 T 7
q q…q
5 3 % 3 5
Figure 7: Generated samples by Stackelberg GAN on CIFAR-10 dataset without cherry pick. Left
Figure: Examples generated by the standard GAN. It shows that the standard GAN fails to generate
bags. Middle Figure: Examples generated by the Stackelberg GAN with 5 generators, where every
two rows correspond to one generator. Right Figure: Examples generated by the Stackelberg GAN
with 10 generators, where each row corresponds to one generator.
any bags being found. As the number of generators increases, the generated images tend to be more
diverse. In particular, for 10-generator Stackelberg GAN, each generator is associated with one class
without any class being missed.
5.3 CIFAR- 1 0 Dataset
We then implement Stackelberg GAN on the CIFAR-10 dataset. CIFAR-10 includes 60,000 32×32
training images, which fall into 10 classes (Krizhevsky & Hinton, 2009)). The architecture of
generators and discriminator follows the design of DCGAN in (Radford et al., 2015). We train models
with 5, 10, and 20 fixed-size generators. The results show that the model with 10 generators performs
the best. We also train 10-generator models where each generator has 2, 3 and 4 convolution layers.
We find that the generator with 2 convolution layers, which is the most shallow one, performs the
best. So we report the results obtained from the model with 10 generators containing 2 convolution
layers. Figure 8a shows the samples produced by different generators. The samples are randomly
drawn instead of being cherry-picked to demonstrate the quality of images generated by our model.
For quantitative evaluation, We use Inception score and Frechet Inception Distance (FID) to measure
the difference between images generated by models and real images.
Results of Inception Score. The Inception score measures the quality of a generated image and
is correlated Well With human’s judgment (Salimans et al., 2016). We report the Inception score
obtained by our Stackelberg GAN and other baseline methods in Table 1. For fair comparison, We
only consider the baseline models Which are completely unsupervised model and do not need any
label information. Instead of directly using the reported Inception scores by original papers, We
replicate the experiment of MGAN using the code, architectures and parameters reported by their
original papers, and evaluate the scores based on the neW experimental results. Table 1 shoWs that our
model achieves a score of 7.62 in CIFAR-10 dataset, Which outperforms the state-of-the-art models.
7
Under review as a conference paper at ICLR 2019
(a) Samples on CIFAR-10. (b) Samples on Tiny ImageNet.
Figure 8: Examples generated by Stackelberg GAN on CIFAR-10 (left) and Tiny ImageNet (right)
without cherry pick, where each row corresponds to samples from one generator.
For fairness, we configure our Stackelberg GAN with the same capacity as MGAN, that is, the two
models have comparative number of total parameters. When the capacity of our Stackelberg GAN is
as small as DCGAN, our model improves over DCGAN significantly.
Results ofFrechet Inception Distance. We then evaluate the performance of models on CIFAR-10
dataset using the FreChet Inception Distance (FID), which better captures the similarity between
generated images and real ones (Heusel et al., 2017). As Table 1 shows, under the same capacity as
DCGAN, our model reduces the FID by 20.74%. Meanwhile, under the same capacity as MGAN,
our model reduces the FID by 14.61%. This improvement further indicates that our Stackelberg GAN
with multiple light-weight generators help improve the quality of the generated images.
Table 1: Quantitative evaluation of various GANs on CIFAR-10 dataset. All results are either reported
by the authors themselves or run by us with codes provided by the authors. Every model is trained
without label. Methods with higher inception score and lower Frechet Inception Distance are better.
Model	Inception Score	Frechet Inception Distance
Real data	11.24 ± 0.16	-
WGAN (Arjovsky et al., 2017)	3.82 ± 0.06	-
MIX+WGAN (Arora et al., 2017)	4.04 ± 0.07	—
Improved-GAN (Salimans et al., 2016)	4.36 ± 0.04	-
ALI (Dumoulin et al., 2017)	5.34 ± 0.05	-
BEGAN (Berthelot et al., 2017)	5.62	
MAGAN (Wang et al., 2017)	5.67	-
GMAN (Durugkar et al., 2016)	6.00 ± 0.19	-
DCGAN (Radford et al., 2015)	6.40 ± 0.05	37.7
Ours (capacity as DCGAN)	7.02 ± 0.07	29.88
D2GAN (Nguyen et al., 2017)	7.15 ± 0.07	-
MAD-GAN (our run) (Ghosh et al., 2017)	6.67 ± 0.07	34.10
MGAN (our run) (Hoang et al., 2018)	7.52 ± 0.1	31.34
Ours (capacity 1×MGAN≈ 1.8XDCGAN)	7.62 ± 0.07	26.76
5.4 Tiny ImageNet Dataset
We also evaluate the performance of Stackelberg GAN on the Tiny ImageNet dataset. The Tiny
ImageNet is a large image dataset, where each image is labelled to indicate the class of the object inside
the image. We resize the figures down to 32 × 32 following the procedure described in (Chrabaszcz
et al., 2017). Figure 8b shows the randomly picked samples generated by 10-generator Stackelberg
GAN. Each row has samples generated from one generator. Since the types of some images in the
Tiny ImageNet are also included in the CIFAR-10, we order the rows in the similar way as Figure 8a .
6	Conclusions
In this work, we tackle the problem of instability during GAN training procedure, which is caused
by the huge gap between minimax and maximin objective values. The core of our techniques is a
multi-generator architecture. We show that the minimax gap shrinks to as the number of generators
increases with rate Oe(1/), when the maximization problem w.r.t. the discriminator is concave.
This improves over the best-known results of O(1/2). Experiments verify the effectiveness of our
proposed methods.
8
Under review as a conference paper at ICLR 2019
References
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein GAN. arXiv preprint
arXiv:1701.07875, 2017.
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium
in generative adversarial nets (GANs). arXiv preprint arXiv:1703.00573, 2017.
Sanjeev Arora, Andrej Risteski, and Yi Zhang. Do GANs learn the distribution? some theory and
empirics. In International Conference on Learning Representations, 2018.
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018.
David Berthelot, Thomas Schumm, and Luke Metz. BEGAN: boundary equilibrium generative
adversarial networks. arXiv preprint arXiv:1703.10717, 2017.
D Bertsekas. Min common/max crossing duality: A geometric view of conjugacy in convex op-
timization. Lab. for Information and Decision Systems, MIT, Tech. Rep. Report LIDS-P-2796,
2009.
Patryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter. A downsampled variant of ImageNet as an
alternative to the CIFAR datasets. arXiv preprint arXiv:1707.08819, 2017.
Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro, Alex Lamb, Martin Arjovsky,
and Aaron Courville. Adversarially learned inference. In International Conference on Learning
Representations, 2017.
Ishan Durugkar, Ian Gemp, and Sridhar Mahadevan. Generative multi-adversarial networks. arXiv
preprint arXiv:1611.01673, 2016.
Arnab Ghosh, Viveka Kulharia, Vinay Namboodiri, Philip HS Torr, and Puneet K Dokania. Multi-
agent diverse generative adversarial networks. In IEEE Conference on Computer Vision and
Pattern Recognition, pp. 8513-8521, 2017.
Ian Goodfellow. NIPS 2016 tutorial: Generative adversarial networks. arXiv preprint
arXiv:1701.00160, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural informa-
tion processing systems, pp. 2672-2680, 2014.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In Advances
in Neural Information Processing Systems, pp. 6626-6637, 2017.
Quan Hoang, Tu Dinh Nguyen, Trung Le, and Dinh Phung. MGAN: Training generative adversarial
nets with multiple generators. 2018.
Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. arXiv preprint
arXiv:1312.6114, 2013.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Tu Nguyen, Trung Le, Hung Vu, and Dinh Phung. Dual discriminator generative adversarial nets. In
Advances in Neural Information Processing Systems, pp. 2670-2680, 2017.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-GAN: Training generative neural samplers
using variational divergence minimization. In Advances in Neural Information Processing Systems,
pp. 271-279, 2016.
9
Under review as a conference paper at ICLR 2019
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Ruslan Salakhutdinov and Hugo Larochelle. Efficient learning of deep Boltzmann machines. In
International Conference OnArtificial Intelligence and Statistics, pp. 693-700, 2010.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training GANs. In Advances in Neural Information Processing Systems,
pp. 2234-2242, 2016.
Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-GAN: Protecting classifiers
against adversarial attacks using generative models. In International Conference on Learning
Representations, 2018.
Ross M Starr. Quasi-equilibria in markets with non-convex preferences. Econometrica: Journal of
the Econometric Society, pp. 25-38, 1969.
Ruohan Wang, Antoine Cully, Hyung Jin Chang, and Yiannis Demiris. MAGAN: Margin adaptation
for generative adversarial networks. arXiv preprint arXiv:1704.03817, 2017.
Chaowei Xiao, Bo Li, Jun-Yan Zhu, Warren He, Mingyan Liu, and Dawn Song. Generating adversarial
examples with adversarial networks. arXiv preprint arXiv:1801.02610, 2018.
Hongyang Zhang, Junru Shao, and Ruslan Salakhutdinov. Deep neural networks with multi-branch
architectures are less non-convex. arXiv preprint arXiv:1806.01845, 2018.
10
Under review as a conference paper at ICLR 2019
A Supplementary Experiments
(a) Step 0.	(b) Step 6k.	(c) Step 13k.
(d) Step 19k.	(e) Step 25k.
(f) Step 0.	(g) Step 6k.	(h) Step 13k.
(i) Step 19k.	(j) Step 25k.
(k) Step 0.	(l) Step 6k.	(m) Step 13k.
Figure 9: Effects of generator architecture of Stackelberg GAN on a toy 2D mixture of Gaussians,
where the number of generators is set to be 8. Top Row: The generators have one hidden layer.
Middle Row: The generators have two hidden layers. Bottom Row: The generators have three
hidden layer. It shows that with the number of hidden layers increasing, each generator tends to learn
more modes. However, mode collapse never happens for all three architectures.
(n) Step 19k.	(o) Step 25k.
Figure 9 shows how the architecture of generators affects the distributions of samples by each
generators. The enlarged versions of samples generated by Stackelberg GAN with architectures
shown in Table 5 and Table 6 are deferred to Figures 10, 11, 12 and 13.
B Proofs of Main Results
B.1 Proofs of Theorem 1 and Corollary 2: Minimax Duality Gap
The statement 0 ≤ w* - q* is by the weak duality. Thus it suffices to prove the other side of the
inequality. All notations in this section are defined in Section 3.1.
We first show that
1
inf 唧 sup 7Φ(Y1 ,∙∙
γ1 ,...,γI ∈Rg θ ∈Rt I
1
∙,Yι； θ) - SUP	inf	φ(Yι,∙∙∙,γι; θ) ≤ e∙
θ ∈Rt γ1 ,...,γI ∈Rg I
Denote by
p(u) :=
γ1 ,.
We have the following lemma.
Lemma 4. We have
.inf ∈Rg 悭{φ(YI ,∙∙∙,γI;θ)-UT θ}
sup inf	Φ(γ1, ∙∙∙,γI; θ) = (clp)(0) ≤ p(0) = inf sup Φ(γ1, ∙∙∙, γI; θ)∙
θ ∈Rt γ1 ,...,γI ∈Rg	γ1 ,...,γI ∈Rg θ ∈Rt
Proof. By the definition of p(0), we have p(0) = inf γ1,...,γI ∈Rg supθ∈Rt Φ(Y1, ∙∙∙, YI; θ). Since
(clp)(∙) is the convex closure of function p(∙) (a.k.a. weak duality theorem), we have (clp)(0) ≤ p(0).
We now show that
∖J
sup inf m Φ(γι,∙∙∙,Yι； θ) = (clp)(0)∙
θ∈Rt γ1,...,γI ∈Rg




11
Under review as a conference paper at ICLR 2019
Note that P(U) = infγι,...,γι∈Rg Pγι,...,γι (u), where Pγι,...,γ (U) = suPθ∈Rt {Φ(γι,...,γι; θ)-
Uτθ} = (-Φ(γι,…，YI; ∙))*(-u), and that
inf {Pγι,...,γι(U) + UTμ} = - SUp {UT(-μ) - Pγι,...,γι(U)}
u∈Rt	u∈Rt
= -(pγι,…,γι)*(-μ) (by the definition of conjugate function) m
(3)
=-(-Φ (Y1,...,YI ；•))**(〃)
= Φ(γι,…,γι; μ). (by conjugate theorem)
So we have
(clp)(0)
sUp i
μ∈Rt u∈Rt
inf {p(u) + UTμ} (by Lemma 8)
SUp inf inf	{pγι,…γ (u) + UTμ} (by the definition of P(U))
μ∈Rt u∈Rt Y1,...,YI ∈Rg	，，
SUp	inf-= ⅛f∕PYι,…,YI (U) + UTμ}
μ∈Rt Y1,…,YI ∈Rg u∈Rt
sup inf	Φ(γι, ...,YI； μ). (by Eqn. (3))
μ∈Rt Y1,…,YI ∈Rg
□
By Lemma 4, it suffices to show P(0) - (clP)(0) ≤ (t + 1)∆γworst. We have the following lemma.
Lemma 5. Under the assumption in Theorem 1, P(0) - (clP)(0) ≤ (t + 1)∆γworst.
Proof. We note that
p(u) := inf sup ]Φ(γι,...,γI; θ) - utθ!
γ1 ,...,γI ∈Rg θ∈Rt
γ1 ,..
inf SUp
.,γI ∈Rg θ∈Rt
cblφ(γi; θ) - UTθ	(by the definition of Φe)
γ1 ,..
inf	-b -clφ(γi; ∙)	(-u) (by the definition of conjugate function)
.,γI ∈Rg	i=1
inf
inf
γ1 ,..
,γI ∈Rg u1 +...+uI =-u
inf
inf
γ1 ,..
,γI ∈Rg u1 +...+uI =-u
IXX (-blφ(Yi2)*(Ui)}
IXX (-Φ(Yi2)*(Ui)}
(by Lemma 7)
(by conjugate theorem)
inf
inf
u1+...+uI=-uγ1,...,γI∈Rg
{(-φ(γir))*(Ul) + ... + (-φ(γι ；∙))*(UI )}
=:	inf	{hι(uι) + ... + hI(uI)},	(by the definition of hi(∙))
u1 +...+uI =-u
where U1, ..., UI, U ∈ Rt. Therefore,
P(0) = inf
u1 ,...,uI ∈Rt
I
hi(Ui),
i=1
I
s.t.	Ui = 0.
i=1
Consider the subset ofRt+1:
Yi ：= {yi ∈ Rt+1 : Vi = [Ui, hi(Ui)], Ui ∈ domhi} , i ∈ [I].
Define the vector summation
Y :=Y1+Y2+...+YI.
Since hi(∙) is continuous and domhi is compact, the set
{(Ui， hi(Ui)) : Ui ∈ domhi}
12
Under review as a conference paper at ICLR 2019
is compact. So Y, conv(Y), Yi, and conv(Yi), i ∈ [I] are all compact sets. According to the
definition of Y and the standard duality argument (Bertsekas, 2009), we have
p(0) = inf {w : there exists (r, w) ∈ Y such that r = 0} ,
and
clp(0) = inf {w : there exists (r, w) ∈ conv (Y) such that r = 0} .
We are going to apply the following Shapley-Folkman lemma.
Lemma 6 (Shapley-Folkman, Starr (1969)). Let Yi, i ∈ [I] be a collection of subsets of Rm. Then
for every y ∈ conv(PiI=1 Yi), there is a subset I (y) ⊆ [I] of size at most m such that
y ∈ X Yi + X ConV(Yi).
i6∈I(y)	i∈I(y)
We apply Lemma 6 to prove Lemma 5 with m = t + 1. Let (r, W) ∈ conv(Y) be such that
r = 0, and W = clp(0).
Applying the above Shapley-Folkman lemma to the set Y = PiI=1 Yi , we have that there are a subset
I ⊆ [I] of size t + 1 and vectors
(ri, Wi) ∈ conv(Yi), i ∈ I and	Ui ∈ domhi, i ∈ I,
such that
Eui + Eri = r = O,	(4)
X hi(ui) + X Wi = clp(0).	(5)
Representing elements of the convex hull of Yi ⊆ Rt+1 by Caratheodory theorem, We have that for
each i ∈ I, there are vectors {uj}j=1 and scalars {aj}j=1 ∈ R such that
t+2
Xaij = 1, aij ≥ 0, j ∈ [t + 2],
j=1
t+2	t+2
r = EajUj =： Ui ∈ domhi,	Wi = Eajhi(uj).	(6)
j=1	j=1
Recall that we define
{t+2	t+2	t+2
X ajhi(uij) : ue = X ajuij, X aj = 1, aj ≥ 0 ,
j=1	i	j=1	i j=1	
and ∆iγ := supu∈Rt{hi(u) - clhi(u)} ≥ 0. We have for i ∈ I,
J	(t+2 . ∖	J
Wi ≥ cl hi Eaj Uj	(by the definition of clhi(∙))
j=1
(t+2 . ∖	,	⑺
≥ hi	aij Uij - ∆iγ (by the definition of ∆iγ)
=hi (Ui) - ∆γ. (by Eqn. (6))
Thus, by Eqns. (4) and (6), we have
I
X^Ui = 0, Ui ∈ domhi, i ∈ [I].	(8)
i=1
13
Under review as a conference paper at ICLR 2019
Therefore, we have
I
p(0) = X hi(Ui) (by Eqn. (8))
i=1
≤ Clp(0) + X ∆γ (by Eqns. (5) and (7))
i∈I
≤ clp(0) + ∣I∣∆worst
=Clp(0) + (t + 1)∆Worst, (by Lemma 6)
as desired.	□
By Lemmas 4 and 5, we have proved that
inf	sup 1Φ(γι,…,γI; θ) — sup
γ1,...,γI ∈Rg θ∈Rt I	θ∈Rt γ1,.
To prove Theorem 1, we note that
1
.吗∈Rg Iφ(YI,…,「I; θ) ≤ J
w* - q*
:= inf	sup 1Φ(γι,...,γI;	θ)	— SuP inf 1Φ(γι,...,γI;	θ)
γ1 ,...,γI ∈Rg	θ∈Rt I	θ∈Rt	γ1 ,...,γI ∈Rg I
11
= inf „ SUp 斤Φ(γι, ...,γI; θ) — inf sup -Φ(γι, ...,γI; θ)
γ1,...,γI ∈Rg θ∈Rt I	γ1,...,γI∈Rg θ∈Rt I
11
+ inf sup -Φ(γι,..., γI; θ) — sup inf	Φ(γι,...,γI; θ)
γ1,...,γI ∈Rg θ∈Rt I	θ∈Rt γ1,...,γI ∈Rg I
11
+ sup inf	Φ(γι,...,γI; θ) — sup inf	Φ(γι,...,γI; θ)
θ∈Rt γ1,...,γI ∈Rg I	θ∈Rt γ1,...,γI ∈Rg I
≤ ∆minimax + ∆maximin + J
as desired.
ɪʌ Tl	I /	/-1∖ ∙	FFF	, T ∙	∙	,	,八	F	^2i I	I EK
When φ(γi; θ) is concave and closed w.r.t. discriminator parameter θ, we have Clφ = φ. Thus,
∆θminimax = ∆θmaximin = 0and0 ≤ w* — q* ≤ J.
B.2 Proofs of Theorem 3: Existence of Approximate Equilibrium
We first show that the equilibrium value V is 2f (1/2). For the discriminator Dθ which only outputs
1/2, it has payoff 2f (1/2) for all possible implementations of generators Gγ1 , ..., GγI . Therefore,
we have V ≥ 2f (1/2). We now show that V ≤ 2f (1/2). We note that by assumption, for any ξ > 0,
there exists a closed neighbour of implementation of generator Gξ such that Ex〜Pd,z〜pz ∣∣Gξ(Z)—
xk2 ≤ ξ for all G0ξ in the neighbour. Such a neighbour exists because the generator is Lipschitz w.r.t.
its parameters. Let the parameter implementation of such neighbour of Gξ be Γ. The Wasserstein
distance between Gξ and Pd is ξ. Since the function f and the discriminator are Lf -Lipschitz and
L-Lipschitz, respectively, we have
IEZ〜Gξf(1 — Dθ(z)) - Ex〜Pdf(1 - Dθ(X))I ≤ O(Lf Lξ).
Thus, for any fixed γ, we have
sup Ex〜Pdf(Dθ(x)) + EZ〜Gξf(1 — Dθ(z))
θ∈Rt
≤ O(Lf Lξ) + sup Ex〜Pdf (Dθ(x)) + Ex〜Pdf (1 — Dθ(x))
θ∈Rt
≤ O(LfLξ) + 2f(1/2) → 2f(1/2), (ξ→+0)
which implies that supθ∈Rt Φ(γ1, ..., γI; θ) = 2f (1/2) for all γ1, ..., γI ∈ Γ. So we have V =
2f (1/2). This means that the discriminator cannot do much better than a random guess.
The above analysis implies that the equilibrium is achieved when Dθ* only outputs 1/2. Denote by
Θ the small closed neighbour of such θ* such that Φ(γ1, ..., γI; θ) is concave w.r.t. θ ∈ Θ for any
fixed γ1 , ..., γI ∈ Γ. We thus focus on the loss on Θ ⊆ Rt and Γ ⊆ Rg :
I
Φ(γι,…,γI; θ) := E [Ex〜Pdf (Dθ(x)) + EZ〜Pzf (1 — Dθ(GYi(Z)))], θ ∈ Θ, γι,…,YI ∈ Γ.
i=1
14
Under review as a conference paper at ICLR 2019
Since Φ(γ1, ..., γI; θ) is concave w.r.t. θ ∈ Θ for all γ1, ..., γI ∈ Γ, by Corollary 2, we have
inf sup1Φ(γι,...,γI; θ) - sup inf 1Φ(γι,...,γI; θ) ≤ e.
γ1 ,...,γI ∈Γ θ∈Θ I	θ∈Θ γ1 ,...,γI ∈Γ I
The optimal implementations of γι,…，YI is achieved by argminγι, ,γι∈γ suPθ∈θ 1 Φ(γι,…，YI; θ).
C Useful Lemmas
Lemma 7. Given the function
(fι + ... + fI )(θ):= fl(θ) + ... + fI (θ),
where f : Rt → R, i ∈ [I ] are closed proper Convexfunctions. Denote by f ㊉...㊉ f∣ the infimal
convolution
(f ㊉…㊉ fi)(u) ：=	Jnf	{fi(ui)+ …+f；(UI)},	U ∈ Rt.
u1 +...+uI =u
Provided that f1 + ... + fI is proper, then we have
(fl + ... + fI)*(U)= cl(f ㊉...㊉ fI)(u),	∀u ∈ Rt.
Proof. For all θ ∈ Rt, we have
fι(θ) + ... + fI(θ) = sup{θτUi - fl (ui)} + …+sup{θτUI - fI (uI)}
u1	uI
= SuP {θT(UI + …+ UI) - f； (UI)-…-fI (UI)}
u1,...,uI
=SuP SuP	{θτU - f；(Ui) - ... - f；(UI)}
u u1 +...+uI =u
= SuP θTU - inf	f1I (U1) - ... - fII (UI)
u	u1 +...+uI =u
=SuP {θτU - (f；㊉…㊉ fIi)(U)}
u
= (fI ㊉…㊉ fI)*(θ).
(9)
Therefore,
ci(fI ㊉…㊉ fI)(U) = ci(fI ㊉...㊉ f；)(U) = (f；㊉...㊉ fIΓ*(U) = (fi +... + fI)i(u),
where the first equality holds because (f；㊉...㊉ f；) is convex, the second quality is by standard
conjugate theorem, and the last equality holds by conjugating the both sides of Eqn. (9).	□
Lemma 8 (Proposition 3.4 (b), Bertsekas (2009)). For any function p(u), denote by q(μ):
infu∈Rt {p(u) + μτu}. We have sup*∈Rt q(μ) = clp(0).
D Distributional approximation properties of Stackelberg GAN
Theorem 9. Suppose that f is strictly concave and the discriminator has infinite capacity. Then, the
global optimum of Stackelberg GAN is achieved if and only if
1I
I EPGγi(z) = Pd.
i=1
Proof. We define
L(Pd, PGY(Z)) = SuP Ex〜Pdf (Dθ(x)) + EZ〜Pzf(I- Dθ(GY(z))).
θ∈Rt
Clearly, the vanilla GAN optimization can be understood as projecting under L:
inf
γ ∈Rg
L(Pd,PGγ(Z)).
15
Under review as a conference paper at ICLR 2019
In the Stackelberg GAN setting, we are projecting under a different distance L which is defined as
1I
L = SUp Ex〜Pdf(Dθ(x)) + 了 EEz〜Pzf(1 — Dθ(GYi(Z)))	(10)
θ∈Rt	I i=1
=L 卜d,I XX PG,.) .	(11)
We note that f is strictly concave and the discriminator has capacity large enough implies the
followings: L(P1, P2), as a function of P2, achieves the global minimum if and only if P2 = P1.
The theorem then follows from this fact and (11).
□
E Network Setup
Table 2: Architecture and hyper-parameters for the mixture of Gaussians dataset.
Operation	Input Dim	Output Dim	BN?	Activation
Generator G(Z) : Z 〜N(0,1)		2		
Linear	2	16	✓	
Linear	16	2		Tanh
Discriminator				
Linear	2	512		Leaky ReLU
Linear	512	256		Leaky ReLU
Linear	256	1		Sigmoid
Number of generators	8			
Batch size for real data	64			
Number of iterations	200			
Slope of Leaky ReLU	0.2			
Learning rate	0.0002			
Optimizer	Adam			
Table 3: Architecture and hyper-parameters for the MNIST datasets.
Operation	Input Dim	Output Dim	BN?	Activation
Generator G(Z) : Z 〜N(0,1)		2		
Linear	100	512	✓	
Linear	512	784		Tanh
Discriminator				
Linear	2	512		Leaky ReLU
Linear	512	256		Leaky ReLU
Linear	256	1		Sigmoid
Number of generators	10
Batch size for real data	100
Slope of Leaky ReLU	0.2
Learning rate	0.0002
Optimizer	Adam
16
Under review as a conference paper at ICLR 2019
Table 4: Architecture and hyper-parameters for the Fashion-MNIST datasets.
Operation	Input Dim	Output Dim	BN?	Activation
Generator G(Z) : Z 〜N(0,1)		2		
Linear	2	128	✓	
Linear	128	256	✓	
Linear	256	512	✓	
Linear	512	1024	✓	
Linear	1024	784		Tanh
Discriminator				
Linear	2	512		Leaky ReLU
Linear	512	256		Leaky ReLU
Linear	256	1		Sigmoid
Number of generators	10			
Batch size for real data	100			
Number of iterations	500			
Slope of Leaky ReLU	0.2			
Learning rate	0.0002			
Optimizer	Adam			
Table 5: Architecture and hyper-parameters for the CIFAR-10 dataset.
Operation	Kernel Strides	Feature maps	BN?	BN center?	Activation
-G(z) : Z 〜Uniform[-1,1]		100	=			
Fully connected		8×8×128	X	X	ReLU
Transposed convolution	5×5	2×2	64	X	X	ReLU
Transposed convolution	5×5	2×2	3	X	X	Tanh
D(x)		^^8×8×256^^			
Convolution	5×5	2×2	128	✓	✓	Leaky ReLU
Convolution	5×5	2×2	256	✓	✓	Leaky ReLU
Convolution	5×5	2×2	512	✓	✓	Leaky ReLU
Fully connected		1	X	X	Sigmoid
Number of generators	10				
Batch size for real data	64				
Batch size for each generator	64				
Number of iterations	100				
Slope of Leaky ReLU	0.2				
Learning rate	0.0002				
Optimizer	Adam(β1 = 0.5,	β2 = 0.999)			
Weight, bias initialization	N(μ = 0,σ 二	=0.01), 0			
17
Under review as a conference paper at ICLR 2019
Table 6: Architecture and hyper-parameters for the Tiny ImageNet dataset.
Operation	Kernel	Strides	Feature maps	BN?	BN center?	Activation
G(z) : Z 〜Uniform[-1,1]			100	=			
Fully connected			8×8×256	X	X	ReLU
Transposed convolution	5×5	2×2	128	X	X	ReLU
Transposed convolution	5×5	2×2	3	X	X	Tanh
D(x)			8×8×256			
Convolution	5×5	2×2	128	✓	✓	Leaky ReLU
Convolution	5×5	2×2	256	✓	✓	Leaky ReLU
Convolution	5×5	2×2	512	✓	✓	Leaky ReLU
Fully connected			1	X	X	Sigmoid
Number of generators	10
Batch size for real data	64
Batch size for each generator
Number of iterations
Slope of Leaky ReLU
Learning rate
Optimizer
Weight, bias initialization
64
300
0.2
0.00001
Adam(β1 = 0.5, β2 = 0.999)
N(μ = 0,σ = 0.01), 0
Figure 10: Examples generated by Stackelberg GAN with 10 generators on CIFAR-10 dataset, where
each row corresponds to samples from one generator.
18
Under review as a conference paper at ICLR 2019
Figure 11: Examples generated by Stackelberg GAN with 10 generators on CIFAR-10 dataset, where
each row corresponds to samples from one generator.
19
Under review as a conference paper at ICLR 2019
Figure 12: Examples generated by Stackelberg GAN with 10 generators on Tiny ImageNet dataset,
where each row corresponds to samples from one generator.
20
Under review as a conference paper at ICLR 2019
Figure 13: Examples generated by Stackelberg GAN with 10 generators on Tiny ImageNet dataset,
where each row corresponds to samples from one generator.
21