Under review as a conference paper at ICLR 2019
Information Maximization Auto-Encoding
Anonymous authors
Paper under double-blind review
Ab stract
We propose the Information Maximization Autoencoder (IMAE), an information
theoretic approach to simultaneously learn continuous and discrete representations
in an unsupervised setting. Unlike the Variational Autoencoder framework, IMAE
starts from a stochastic encoder that seeks to map each input data to a hybrid dis-
crete and continuous representation with the objective of maximizing the mutual
information between the data and their representations. A decoder is included
to approximate the posterior distribution of the data given their representations,
where a high fidelity approximation can be achieved by leveraging the informa-
tive representations. We show that the proposed objective is theoretically valid
and provides a principled framework for understanding the tradeoffs regarding in-
formativeness of each representation factor, disentanglement of representations,
and decoding quality.
1	Introduction
A central tenet for designing and learning a model for data is that the resulting representation
should be compact yet informative. Therefore, the goal of learning can be formulated as find-
ing informative representations about the data under proper constraints. Generative latent variable
models are a popular approach to this problem, where a model parameterized by θ of the form
Pθ(x) = ppθ(x∣z)p(z)dz is used to represent the relationship between the data X and the low di-
mensional latent variable z. The model is optimized by fitting the generative data distribution pθ(x)
to the training data distribution pb(x), which involves maximizing the likelihood for θ. Typically,
this model is intractable even for moderately complicated functions pθ(x|z) with continuous z. To
remedy this issue, variational autoencoder (VAE) (Kingma and Welling, 2013; Rezende et al., 2014)
proposes to maximize the evidence lower bound (ELBO) of the marginal likelihood objective.
However, as was initially pointed out in (Hoffman and Johnson, 2016), maximizing ELBO also
penalizes the mutual information between data and their representations. This in turn makes the
representation learning even harder. Many recent efforts have focused on resolving this problem by
revising ELBO. Generally speaking, these works fall into two lines. One of them targets “disentan-
gled representations” by encouraging the statistical independence between representation compo-
nents (Higgins et al., 2016; Kim and Mnih, 2018; Gao et al., 2018; Chen et al., 2018; Esmaeili et al.,
2018), while the other line of work seeks to control or encourage the mutual information between
data and their representations (Mary Phuong, 2018; Burgess et al., 2018; Alemi et al., 2017; Dupont,
2018; Zhao et al., 2017). However, these approaches either result in an invalid lower bound for the
VAE objective or cannot avoid sacrificing the mutual information.
Instead of building upon the generative latent variable model, we start with a stochastic encoder
Pθ(z|x) and aim at maximizing the mutual information between the data X and its representations z.
In this setting, a reconstruction or generating phase can be obtained as the variational inference of the
true posterior pθ(x|z). By explicitly seeking for informative representations, the proposed model
yields better decoding quality. Moreover, we show that the information maximization objective
naturally induces a balance between the informativeness of each latent factor and the statistical
independence between them, which gives a more principled way to learn semantically meaningful
representations without invalidating ELBO or removing individual terms from it.
Another contribution of this work is proposing a framework for simultaneously learning continuous
and discrete representations for categorical data. Categorical data are ubiquitous in real-world tasks,
where using a hybrid discrete and continuous representation to capture both categorical information
1
Under review as a conference paper at ICLR 2019
and continuous variation in data is more consistent with the natural generation process. In this work,
we focus on categorical data that are similar in nature, i.e., where different categories still share sim-
ilar variations (features). We seek to learn semantically meaningful discrete representations while
maintaining disentanglement of the continuous representations that capture the variations shared
across categories. We show that, compared to the VAE based approaches, our proposed objective
gives a more natural yet effective way for learning these hybrid representations.
2	Related Work
Recently, there has been a surge of interest in learning interpretable representations. Among them,
β-VAE (Higgins et al., 2016) is a popular method for learning disentangled representations, which
modifies ELBO by increasing the penalty on the KL divergence between the variational posterior
and the factorized prior. However, by using large weight for the KL divergence term, β-VAE also pe-
nalizes the mutual information between the data and the latent representations more than a standard
VAE does, resulting in more severe under utilization of the latent representation space.
Several follow up works propose different approaches to address the limitations of β-VAE. (Dupont,
2018; Alemi et al., 2017; Burgess et al., 2018; Mary Phuong, 2018) propose to constrain the mutual
information between the representations and the data by pushing its upper bound, i.e., the KL diver-
gence term in ELBO, towards a progressively increased target value. However, specifying and tuning
this target value can itself be very challenging, which makes this method less practical. Moreover,
this extra constraint results in an invalid lower bound for the VAE objective. Alternatively, (Zhao
et al., 2017) drops the mutual information term in ELBO. By pushing only the aggregated posterior
towards a factorial prior, they implicitly encourage independence across the dimensions of latent
representations without sacrificing the informativeness of the representations. However, simply re-
moving the mutual information term also violates the lower bound of the VAE objective.
Another relevant line of work (Gao et al., 2018; Kim and Mnih, 2018; Chen et al., 2018; Esmaeili
et al., 2018) seek to learn disentangled representations by explicitly encouraging statistical inde-
pendence between latent factors. They all propose to minimize the total correlation term of the
latent representations, either augmented as an extra term to ELBO or obtained by reinterpreting or
re-weighting the terms in the VAE objective, as a way to encourage statistical independence between
the representation components. In contrast, we show that our information maximization objective
inherently contains the total correlation term while simultaneously seeking to maximize the infor-
mativeness of each representation factor.
In this paper, we introduce a different perspective to the growing body of the VAE based approaches
for unsupervised representation learning. Starting by seeking informative representations for the
data, we follow a more intuitive way to maximize the mutual information between the data and
the representations. Moreover, we augment the continuous representation with a discrete one, which
allows more flexibilities to model real world data that are generated from different categories. We in-
voke the information maximization principle (Linsker, 1988; Bell and Sejnowski, 1995) with proper
constraints implied by the objective itself to avoid degenerate solutions. The proposed objective
gives a theoretically elegant yet effective way to learn semantically meaningful representations.
3	Information Maximization Representation Learning
Given data x ∈ Rd, we consider learning a hybrid continuous-discrete representation, denoted
respectively with variables z ∈ RK1 and y ∈ {1, . . . , K2}, using a stochastic encoder parameterized
by θ, i.e., pθ(y, z|x). We seek to learn compact yet semantically meaningful representations in
the sense that they should be low dimensional but informative enough about the data. A natural
approach is to maximize the mutual information (Cover and Thomas, 2012) Iθ (x; y, z) between
the data and its representations under the constraint K1, K2 d. Here the mutual information
between two random variables, e.g., X and z, is defined as I&(x; Z) = Hθ(Z) — Hθ(z|x), where
Hθ(Z) = -Epθ(Z) [logpθ(z)] is the entropy of Z and Hθ(z|x) = -Epg的/)[logpθ(z|x)] is the
conditional entropy of Z given x. The mutual information can be interpreted as the decrease in
uncertainty of one random variable given another random variable. In other words, it quantifies how
much information one random variable has about the other.
2
Under review as a conference paper at ICLR 2019
A probabilistic decoder qφ(x∣y, Z) is adopted to approximate the true posterior pθ(x|y, z), which
can be hard to estimate or even intractable. The dissimilarity between them is optimized by minimiz-
ing the KL divergence DKL (pθ(x|y, z)∣∣qφ(x∣y, z)). In summary, IMAE considers the following,
maximizeθ,φ βoIθ(x;y,Z) - DKL (pθ(x|y,z)∣∣qφ(x∣y,Z)) .	(1)
Given that H(x) is independent of the optimization procedure, we can show that optimizing (1) is
equivalent to optimize the following1,
maximizeθ,φ Iθ(x; y, Z) + Ep@(x,y,z)[log qφ(x∣y, z)], β = βo - 1 > 0 .	(2)
We set β > 0 to balance between maximizing the informativeness of latent representations and
maintaining the decoding quality. The second term is often interpreted as the “reconstruction error”
which can be optimized using the reparameterization tricks proposed by (Kingma and Welling, 2013)
and (Jang et al., 2016) for continuous representation Z and discrete representation y respectively.
Now we introduce proper method to optimize the first term Iθ (x; y, Z) in (2).
3.1	Simultaneously seeking informativenes s and disentanglement
We first show that Iθ (x; y, Z) inherently involves two keys terms that quantify the informativeness
of each representation factor and the statistical dependence between these factors. Assuming the
conditional distribution of the representation (y, Z) given x is factorial, we also assume the marginal
distribution of y and Z are independent, i.e., pθ (y, Z) = pθ (y)pθ (Z), then1
Iθ(χ;y,Z) = Iθ(χ;y) + Pk=IIθ(χ;Zk) - DKL (pθ(Z)IInK=3(Zk)) .	(3)
The first two terms of the RHS quantify how much information each latent factor, i.e., y or Zk, carry
about the data. The last term is known as the total correlation of Z (Watanabe, 1960), which quanti-
fies the statistical independence between the continuous latent factors and achieves the minimum if
and only if they are independent of each other.
As is implied by (3), maximizing Iθ (x; y, Z) can be conducted by maximizing informativeness of
each latent factor while simultaneously promoting statistical independence between the continuous
factors. Various Monte Carlo based sampling strategies have been proposed to optimize the total
correlation term (Chen et al., 2018; Esmaeili et al., 2018); in this work we follow this line (see
Appendix B). Next we proceed by constructing tractable approximations for Iθ(x; Zk) and Iθ(x; y)
respectively.
3.2	Informative continuous representations
Without any constraints, the mutual information Iθ(x; Zk) between a continuous latent factor and
data can be trivially maximized by severely fragmenting the latent space. To be more precise, con-
sider the following proposition. While similar results have likely been established in the information
theory literature, we include this proposition to motivate our objective design.
Proposition 1. Suppose the conditional distribution pθ (Z|x) is a factorial Gaussian distribution
with mean μ(x) and covariance Σ(x). Let σ(x) ∈ RKI denote the diagonal entries of Σ(x), then
Iθ (x;Zk) ≤ 2 log [(Eχ [σ2(x)] +Varx [μk (x)])] - 2 Eχ [log σ2(x)] , k = 1,...,Kι . (4)
The equality in (4) is attained if and only if Zk is Gaussian distributed, given which we have
Iθ(x; Zk) ≥ 1log(l + Varχ [μk(x)] /Eχ[σ2(x)]) , k = 1,...,Kι .	(5)
Note here both μk (x) and σk(x) are random variables. The above result implies that Zk is more
informative about x if it has less uncertainty given x yet captures more variance in data, i.e., σk(x)
is small while μk(x) disperses within a large space. However, this can result in discontinuity of Zk,
where in the extreme case each data sample is associated with a delta distribution in the latent space.
1
Detailed derivation is provided in Appendix A.
3
Under review as a conference paper at ICLR 2019
In light of this, we can make what we described above more precise. A vanishing variance of the
conditional distribution p(zk|x) leads to a plain autoencoder that maps each data sample to a deter-
ministic latent point, which can fragment the latent space in a way that each data sample corresponds
with a delta distribution in the latent spacepθ(Zk∣x(i)) = δ(zki)). On the other hand, Proposition 1
also implies that controlling the variance σk(x) to be finite, Iθ(x; zk) will be maximized by push-
ing μk(x) towards two extremes (±∞). To remedy this issue while achieving the upper bound, a
natural resolution is to squeeze zk within the domain of a Gaussian distribution with finite mean
and variance. By doing so, we can avoid the degenerate solution while achieving a more reasonable
trade-off between enlarging the spread of μk (x) and maintaining the continuity of z. Therefore, We
consider the following as the surrogate for maximizing Iθ(x; zk),
maximize L§(Z) = -PK=IDKL (pθ(zk)∣∣r(zk)).	⑹
Here r(zk) are i.i.d scaled normal distribution with finite variance. That is, we push each pθ(zk)
towards a Gaussian distribution r(Zk) by minimizing the KL divergence between them.
3.3	Informative discrete representations
Unlike the continuous representation, the mutual information Iθ (x; y) between a discrete repre-
sentation and data can be well approximated, given the fact that the cardinality of the space of y
is typically low. To be more specific, given N i.i.d samples {xn }nN=1 of the data, the empirical
estimation ofIθ(x; y) under the conditional distribution pθ (y|xn) follows as
bθ(x; y) = HθW)- Hθ⑻X) = H (NPn=ιPθ(yIxn)) - NPN=IH (Pe(y|Xn)).⑺
As shown in Proposition 2, with a suitably large batch of samples, the empirical mutual information
Iθ(x; y) is a good approximation to Iθ(x; y). This enables us to optimize Iθ(x; y) in a theoretically
justifiable way that is amenable to stochastic gradient descent with minibatches of data.
Proposition 2. Let y be a discrete random variable that belongs to some categorical class C.
Assume the marginal probabilities of the true and the predicted labels are bounded below, i.e.
pθ(y), pbθ(y) ∈ [1/(CK2),1] for all y ∈ C with some constant C >1. Then for any δ ∈ (0, 1),
P (卜θ (x; y) - I-θ (x; y)∣ ≤ K (max{log CK - 1, 1} + e) yiθg(2K2/δ)) ≥ 1 - 2δ .
(8)
Here N denotes the number of samples used to establish Iθ (x; y) according to Eq (7).
Therefore, to maximize the mutual information Iθ(x; y), we consider the following:
max Lθ(y) := Iθ(x; y).	(9)
Maximizing the the mutual information Iθ(x; y) provides a natural way to learn discrete categorical
representations. To see this, notice that Iθ (x; y) contains two fundamental quantities, the category
balance term Hθ(y) and the category separation term Hθ(y∣x). In other words, maximizing Iθ(x; y)
trades off uniformly assigning data over categories and seeking highly confident categorical iden-
tity for each sample x. The maximum is achieved if pθ(y∣x) is deterministic while the marginal
distribution pθ(y) is uniform, that is Hθ(y∣x) = 0 and Hθ(y) = log K2.
Overall Objective As a summary of (3) (6) and (9), our overall objective is
β (max Lθ(z) + Lθ(y) - DKL Ip(Z)∣∣∏κ1ιP(Zk)]) + Ep°(χ,y,z) [logqφ(x∣y, z)].
The first three terms associate with our information maximization objective, while the last one aims
at better approximation of the posterior pθ(x∣y, z). A better balance between these two targets
can be achieved by weighting them differently. One the other hand, the informativeness of each
latent factor can be optimized through Lθ(Z) and Lθ(y), while statistically independent latent con-
tinuous factors can be promoted by minimizing the total correlation term
DKL Ip(Z) llπK1ιp(zk )].
Therefore, trade-offs can be formalized regarding the informativeness of each latent factor, disen-
tanglement of the representation, and better decoding quality. This motivates us to consider the
following objective, let β , γ > 0,
max LIMAE ：= Epθ(χ,y,z) [logqφ(x∣y, z)] + βLθ(y) + βLθ(z) - YDKL 限(z)∣∣∏K=ιPθ(zk)] .	(10)
4
Under review as a conference paper at ICLR 2019
4	Experimental Results
We compare IMAE against various VAE based approaches that are summarized in Figure 1. We
would like to demonstrate that IMAE can (i) successfully learn a hybrid of continuous and discrete
representations, with y matching the intrinsic categorical information ytrue well and z capturing the
disentangled feature information shared across categories; (ii) outperform the VAE based models
by achieving a better trade-off between representation interpretability and decoding quality. We
choose the priors r(z) and r(y) to be the isotropic Gaussian distribution and uniform distribution
respectively. Detailed experimental settings are provided in Appendix G.
LVAE = ∣Ep(y,z∣x) [q(x∣U,Z) - DKL (p(z∣x)||r(z)) - DKL (p(y|x)||r(y)) - ELBO
=Ep(y,z∣χ) [q(x|y,Z)] -I(x;y)-DKL (P(U)||r(y))-I(x;Z)-DKL (P(z)||r(z))
|	{z	} l^{z^^} 1 {z } l^^"} 1 {z }
①	②	③	@	③
β-VAE:①一β (② + ③)一β (④ + ③)InfoVAE:①一β ③一β ③
Joint-VAE:①-β |。+ ③-Cy | - β |@ + ③-Cz |
Figure 1: Summarization of relevant work. β-VAE modifies ELBO by increasing the penalty on the
KL divergence terms. InfoVAE drops the mutual information terms from ELBO. JointVAE seeks
to control the mutual information by pushing the their upper bounds (the associated KL divergence
terms) towards progressively increased values, Cy &Cz. We drop the subscripts θ and φ hereafter.
4.1	Informative representations yield better interpretability
We first qualitatively demonstrate that informative representations can yield better interpretability.
For the continuous representation, Figure 2 validates Proposition 1 by showing that, with roughly
same amount of variance for each latent variable Zk , those achieving high mutual information with
the data have mean values μk (x) of the conditional probability P(Zk |x) disperse across data samples
and variances σk (x) decrease to small values for all data samples. As a qualitative evaluation, we
traverse latent dimensions corresponding with different levels of I(x, Zk). As seen in Figure 2(b)-(d),
informative variables in the continuous representation have uncovered intuitive continuous factors
of the variation in the data, while the factor Z8 has no mutual information with the data and shows
no variation. We observe the same phenomenon for the discrete representation y in Figure 2(e)&(f),
which were obtained with two different values of β and γ, where the more informative one discovers
matches the natural labels better. This provides further evidence for that interpretable latent factors
can be attained by maximizing the mutual information between the representations and the data.
• /(X,Z,)	★	E[σ,(x)2]
IQl VarW,(X)]	+ Var[z,(x)]
•华单中拿★★★★★
(b) I(x, z2) = 1.7	(c) I(x, z4) = 0.9
(d) I(x, z8) = 0
(e) I(x, y)
(f) I(x, y) = 1.7
(a) Sorted dim index
= 2.1
Figure 2: IMAE on MNIST (a) Illustration of Proposition 1. (b)-(d) Latent traverse on the continu-
ous representations Z . The rows are conditioned on the discrete representations y learnt by IMAE,
and the initial value of Z for each row is obtained by feeding the encoder with randomly selected
data corresponds with y. We then manipulate each selected Zk within [-2, 2] while keeping all other
dimensions fixed. (e) & (f) Discrete representations learnt by IMAE with different β values.
5
Under review as a conference paper at ICLR 2019
Accuracy
P(yf。门 MAE)
Figure 3: Tracking the key quantities for different models by sweeping β for all different methods.
We set γ = 2β for IMAE. For each β, we run each method over 10 random initializations.
4.2	Quantitative comparisons
In this section, we perform quantitative evaluations on MNIST (LeCun and Cortes, 2010), Fashion
MNIST (Xiao et al., 2017) and dSprites (Matthey et al., 2017). We show that IMAE achieves better
interpretability vs. decoding quality trade-off.
Unsupervised learning of discrete latent factor Before we present our main results, we first
describe an assumption that we make on the discrete representations. For the discrete representation,
a reasonable assumption is that the conditional distribution p(y|x) should be locally smooth so that
the data samples that are close on their manifold should have high probability of being assigned to
the same category (Agakov, 2005). This assumption is crucial for using neural networks to learn
discrete representations, since it’s easy for a high capacity model to learn a non-smooth function
p(y|x) that can abruptly change its predictions without guaranteeing similar data samples will be
mapped to similar y. To remedy this issue, we adopt the virtual adversarial training (VAT) trick
proposed by (Miyato et al., 2016) and augment Lθ (y) as follows:2
max Lθ(y) := Iθ(x; y) - Eb(X) [max∣∣ηk≤eH (pθ(y|x);pθ(y|x + η)) .	(11)
The second term of RHS regularizes pθ(y∣x) to be consistent within the e norm ball of each data
sample so as to maintain the local smoothness of the prediction model. For fair comparison, we
augment all four methods with VAT. As demonstrated in Appendix D, using VAT is essential for all
of them except β-VAE to learn interpretable discrete representations.
4.2.1	MNIST AND FASHION MNIST
We start by evaluating different methods on MNIST and Fashion MNIST, for which we train over a
range of β values (we set γ = 2β for IMAE).
Discrete representations For the discrete representations, by simply pushing the conditional dis-
tribution p(y|x) towards the uniform distribution r(y), β-VAE sacrifices the mutual information
I(x; y) and hence struggles in learning interpretable discrete representation even with VAT. As a
comparison, InfoVAE performs much better by dropping I(x; y) from ELBO. For data that are
distinctive enough between categories (MNIST), with large β values InfoVAE performs well by
uniformly distributing the whole data over categories through minimizing DKL(p(y)||r(y)) while
simultaneously encouraging local smoothness with VAT. However, InfoVAE struggles with less dis-
tinctive data (Fashion-MNIST), where it cannot give fairly confident category separation by only
2In this paper, we set = 1 across datasets. VAT can be effectively approximated by a pair of forward and
backward passes (Miyato et al., 2016).
6
Under review as a conference paper at ICLR 2019
IMAE
InfoVAE
JointVAE
β-VAE
O∕23VΓ67Stt
aa4G&66666
0∕23SG7703
““““G6&66&
。/23夕f67gq
66646。，，"
τττττττm
7732886666
。/23956789
& 6 6666G464
O∕2367S9SS
αSSS6GGGGG
τττττm-ττ
0∕23y5673t)
t,GG6533322
。/a3q6G7gq
446 GGCi■
O∕23y67MS*
WmmT7
/1—938900
(a)	β = 1
(b)	β = 5
(c)	β = 9
Figure 4: For each image, the first row is the digit type learnt by the model, where each entry is
obtained by feeding the decoder with the averaged z values corresponding with the learnt y. The
second row is obtained by traversing the ”angle” latent factor within [-2, 2] on digit 6. IMAE is
capable of uncovering the underlying discrete factor over a wide range of β values. More inter-
pretable continuous representations can be obtained when the method is capable of learning discrete
representations, since less overlap between the mainfolds of each category is induced.
requiring local smoothness. In contrast, IMAE achieves much better performance by explicitly en-
couraging confident category separation via minimizing the conditional entropy H(y|x), while using
VAT to maintain local smoothness so as to prevent overfitting of neural network. Although JointVAE
performs much better than β-VAE by pushing the upper bound of I(x; y) towards a progressively
increasing target value Cy, we found it can easily get stuck at some bad local optima where I(x; y)
is comparatively large while the accuracy is poor. A heuristic is that once JointVAE enters the local
region of a local optima, progressively increasing Cy only induces oscillation within that region. 3
Informativeness, interpretability and decoding quality As illustrated in Figure 1, by using large
β values, β-VAE sacrifices more mutual information between the data and its representations, which
in turn (see Figure 3) results in less informative representations followed by poor decoding quality.
In contrast, the other three methods can remedy this issue to different degrees, and hence attains
better trade-off regarding informativeness of latent representations and decoding quality. Compared
to JointVAE and InfoVAE, IMAE is more capable of learning discrete presentations over a wide
range of β, γ values, which implies less overlap between the manifolds of different categories is
induced. As a result, IMAE is expected to yield better decoding quality for each category. Although
InfoVAE and JointVAE can also learn comparatively good discrete representations when using large
and small β values respectively, the corresponding results of these two regions associate with either
poor decoding quality or much lower disentanglement score (see section 4.2.2). In contrast, IMAE
consistently performs well with different hyperparameters, especially in the region of interest where
the decoding quality as well as the informativeness of latent representations are good enough.
4.2.2	2D Shapes
In this section, we quantitatively evaluate the disentanglement capability of IMAE on dSprites where
the ground truth factors of both continuous and discrete representaions are available. We use the dis-
entanglement metric proposed by (Chen et al., 2018), which is defined in terms of the gap between
the top two empirical mutual information of each latent representation factor and a ground truth
factor. The disentanglement score is defined as the weighted average of the gaps. A high disentan-
glement score implies that each ground truth factor associates with one single representation factor
that is more informative than the others, i.e., the learnt representation factors are more disentangled.4
Figure 5 shows that, with large β values, β-VAE penalizes the mutual information too much and
this degrades the usefulness of representations. while all other three methods achieve higher disen-
tanglement score with better decoding quality. For JointVAE, higher β values push the upper bound
of mutual information converges to the prefixed target value, it therefore can maintain more mutual
3More results of JointVAE can be found in Appendix F.
4Although the truth discrete factor is provided, we evaluate the disentanglement quality only in terms of
the continuous representations since the pixel-wise difference between different categories are very small. The
results of considering the disentanglement score regrading both y and z is provided in Appendix E.
7
Under review as a conference paper at ICLR 2019
β(y for IMAE)	β(yfor IMAE)	β(yfor IMAE)
(a) IMAE performs well regarding the disentanglement score vs. decoding quality trade-off, especially in
the region of interest where both decoding quality and informativeness of representations are fairly good.
-120	-90	-60	-30
-Reconstruction error
0.0.0.0.0.0.
①」θɔs IU ① IU ①-6U£U ① SQ
5
10-1	IO0	IO1
Total correlation
IMAE
InfoVAE
JointVAE
S-VAE
ιo-1 IO0 ιo1
Total correlation
(b) Negative correlation between total correlation and disentanglement score. It also implies that the
disentanglement score tends to decrease along with the total correlation if using even larger β , due to
the diminishing informativeness of representation factors. In the extreme case, both total correlation
and disentanglement score can degrade to zero.
Figure 5: Disentanglement comparison on dSprites. The results are reported by training each
method with β ∈ [1,10], and we set β = γ∕2 with Y ∈ [1,10] for IMAE. For each β value, every
method is trained over 8 random initializations. Shade regions indicate the 80% confidence intervals.
information between the data and the whole latent representations and give better decoding quality.
However, the disentanglement quality is poor in this region, which implies that simply restricting the
overall capacity of the latent representations is not enough for learning disentangled representations.
While InfoVAE yields comparatively better disentanglement score by pushing the marginal joint
distribution of the representations towards a factorial distribution harder with large values of β , the
associated decoding quality and informativeness of latent representations are both poor. In contrast,
IMAE is capable of achieving better trade-off between the disentanglement score and the decoding
quality in the region of interest where the decoding quality as well as the informativeness are fairly
good. We attribute this to the effect of explicitly seeking for statistically independent latent factors
by minimizing the total correlation term in our objective.
5 Conclusion
We have proposed IMAE, a novel approach for simultaneously learning the categorical information
of data while uncovering latent continuous features shared across categories. Different from VAE,
IMAE starts with a stochastic encoder that seeks to maximize the mutual information between data
and their representations, where a decoder is used to approximate the true posterior distribution
of the data given the representations. This model targets at informative representations directly,
which in turn naturally yields an objective that is capable of simultaneously inducing semantically
meaningful representations and maintaining good decoding quality, which is further demonstrated
by the numerical results.
Unsupervised joint learning of disentangled continuous and discrete representations is a challenging
problem due to the lack of prior for semantic awareness and other inherent difficulties that arise in
learning discrete representations. This work takes a step towards achieving this goal. A limitation
of our model is that it pursues disentanglement by assuming or trying to encourage independent
scalar latent factors, which may not always be sufficient for representing the real data. For example,
data may exhibit category specific variation, or a subset of latent factors might be correlated. This
motivates us to explore more structured disentangled representations; one possible direction is to
encourage group independence. We leave this for future work.
8
Under review as a conference paper at ICLR 2019
References
F. V. Agakov. Variational Information Maximization in Stochastic Environments. PhD thesis, Uni-
versity of Edinburgh, 2005.
A. A. Alemi, B. Poole, I. Fischer, J. V. Dillon, R. A. Saurous, and K. Murphy. An information-
theoretic analysis of deep latent-variable models. arXiv preprint arXiv:1711.00464, 2017.
A.	J. Bell and T. J. Sejnowski. An information-maximization approach to blind separation and blind
deconvolution. Neural computation, 7(6):1129-1159, 1995.
C. P. Burgess, I. Higgins, A. Pal, L. Matthey, N. Watters, G. Desjardins, and A. Lerchner. Under-
standing disentangling in β-vae. arXiv preprint arXiv:1804.03599, 2018.
T. Q. Chen, X. Li, R. Grosse, and D. Duvenaud. Isolating sources of disentanglement in variational
autoencoders. arXiv preprint arXiv:1802.04942, 2018.
T. M. Cover and J. A. Thomas. Elements of information theory. John Wiley & Sons, 2012.
E. Dupont. Joint-vae: Learning disentangled joint continuous and discrete representations. arXiv
preprint arXiv:1804.00104, 2018.
B.	Esmaeili, H. Wu, S. Jain, S. Narayanaswamy, B. Paige, and J.-W. van de Meent. Hierarchical
disentangled representations. arXiv preprint arXiv:1804.02086, 2018.
S. Gao, R. Brekelmans, G. V. Steeg, and A. Galstyan. Auto-encoding total correlation explanation.
arXiv preprint arXiv:1802.05822, 2018.
I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner.
beta-vae: Learning basic visual concepts with a constrained variational framework. 2016.
M.	D. Hoffman and M. J. Johnson. Elbo surgery: yet another way to carve up the variational
evidence lower bound. In Workshop in Advances in Approximate Bayesian Inference, NIPS, 2016.
E. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint
arXiv:1611.01144, 2016.
H. Kim and A. Mnih. Disentangling by factorising. arXiv preprint arXiv:1802.05983, 2018.
D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114,
2013.
Y. LeCun and C. Cortes. MNIST handwritten digit database. 2010. URL http://yann.lecun.
com/exdb/mnist/.
R. Linsker. Self-organization in a perceptual network. Computer, 21(3):105-117, 1988.
N.	K. R. T. S. N. Mary Phuong, Max Welling. The mutual autoencoder: Controlling informa-
tion in latent code representations. 2018. URL https://openreview.net/forum?id=
HkbmWqxCZ.
L. Matthey, I. Higgins, D. Hassabis, and A. Lerchner. dsprites: Disentanglement testing sprites
dataset. https://github.com/deepmind/dsprites-dataset/, 2017.
T. Miyato, A. M. Dai, and I. Goodfellow. Virtual adversarial training for semi-supervised text
classification. 2016.
D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate infer-
ence in deep generative models. arXiv preprint arXiv:1401.4082, 2014.
R.	Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science.
Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2018.
S.	Watanabe. Information theoretical analysis of multivariate correlation. IBM Journal of research
and development, 4(1):66-82, 1960.
9
Under review as a conference paper at ICLR 2019
H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine
learning algorithms, 2017.
S. Zhao, J. Song, and S. Ermon. Infovae: Information maximizing variational autoencoders. arXiv
preprint arXiv:1706.02262, 2017.
10
Under review as a conference paper at ICLR 2019
A Proof of Section 3
Balance between posterior inference fidelity and information maximization Notice that we
can rewrite the mutual information between the data x and its representations as the following,
Iθ(x;y,Z) = H(χ) + Epθ(x,y,z) [logqφ(χ∣y,z)] + DKL [pθ(x|y,z)∣∣qφ(χ∣y,z)] .	(12)
It then follows that,
Iθ(x;y,Z) - DKL (pθ(x|y,z)∣∣qφ(χ∣y,Z)) = H(χ) + Ep0(x,y,z)[logqφ(χ∣y,z)]	(13)
Since H(x) is independent of the optimization procedure, we have the following,
max βIθ(x; y, z) - DKL (pθ(x|y, z)∣∣qφ(x∣y, z)), β > 1
⇒ max (β — 1)Iθ(x； y z) + Ep@(χ,y,z) [log qφ(x∣y, z)]	(14)
where β trade-off the informativeness of the latent representation and generation fidelity.
Decomposition of Iθ (x; y, z) Let b = (z, y) denote the joint random variable consisting of the
continuous random variable b and discrete random variable y.
Note that Iθ(x; y, z) = Iθ(x; b) can be written as:
Iθ(x; b) = -J p(x) / Pθ(b|x) logPθ(b)dbdx + / p(x) / pθ(b∣x)logpθ(b∣x)dbdx
=-J Pθ(b) logPθ(b)db + / p(x) / Pθ(b∣x)logpθ(b∣x)dbdx .
The second term in Eq (15) has the form:
(15)
K1+1
J p(x) J Pθ(b∣x)logPθ(b∣x)dbdx = J P(x) J Pθ(b∣x)logPθ(bk ∣x)dbdx
K1+1
=X Hθ(bk|x),
k=1
where % follows by the assumption that pθ(b|x) is factorial.
For the first term in Eq (15), we have:
(16)
pθ(b)logpθ(b)db =	pθ (b) log
ZZ
pθ(b)
∏Kl+1Pθ (bk)
K1+1
db +	pθ(b) logpθ(bk)db
K1+1
=DKL (pθ(b)ll∏K1+1Pθ(bk)) - X Hθ(bk).
k=1
Substituting Eqs (16) & (17) into Eq (15) yields the result:
(17)
K1 +1
Iθ(x; y, z) = Iθ(x; b) = Hθ(bk) - DKL (pθ(b)∣∣∏K1+1Pθ(bk)) - E H(bk |x)
k=1
K1+1
=E Iθ(x； bk) - DKL (pθ(b)∣∣∏K1+1pθ(bk))
k=1
K1
=Iθ(x； y) + X Iθ(x； Zk) - DKL (pθ(y, z)∣∣pθ(y)∏K1ιPθ(Zk)) . (18)
k=1
Since y and z are assumed to be marginally independent, i.e., pθ(y; z) = pθ(y)pθ(z), then
K1
Iθ(x； y) + XIθ(x; Zk) - DKL (pθ(y, z)∣∣pθ(y)∏K1ιpθ(Zk))
k=1
K1
=Iθ(x； y) + XIθ(x； Zk) - DKL (pθ(z)∣∣∏K1ιPθ(Zk)) .	(19)
k=1
11
Under review as a conference paper at ICLR 2019
Proof of proposition 1
Proof. We start with computing the expectation of zk :
Eθ [zk]
JzkJ Pθ(Zk∣x)p(x)dxdzk = / p(x) / ZkPθ(Zk∣x)dzkdx
=p P(X)μk (X) dx = Ex [μk (X)].
X
Then the variance of zk followed as:
Varθ [zk]
/ z2 / Pθ(zk∣x)p(x)dxdzk - Ex [μk(x)]2
I p(x) / zkpθ(Zk∣x)dzkdx - Ex [μk(x)]2
X	Zk
/ p(x) [σ2(x) + μk(x)2] dx - Ex [μk(x)]2
Ex [σ2(x)] +Varx [μk(x)].
Note that
Iθ(x； zk) = Hθ(zk) - Hθ(zk|x),
for which we have the following,
Hθ(zk |x)
-J p(x) / pθ(zk|x) logpθ(zk∣x)dzdx
2 (P(X) log (2∏eσ2 (x)) dx
2 (log(2πe) + Ex [logσ2(x)]).
(20)
(21)
(22)
(23)
For the entropy of zk, we leverage the fact that Hθ(zk) is upper bounded by the entropy of a
Gaussian distributed random variable with the same mean and variance, that is
Hθ(zk) ≤ 2 (log2πe + log (Ex [σ2(x)] +Varx [μk(x)]))	(24)
Substituting Eqs (23) & (24) into Eq (22) completes the proof.	□
Proof of proposition 2
Proof. Let pθ(y) = N PN=I Pθ(y∣xn) denote the Monte Carlo estimator of the true probability
Pθ(y) = Rχp(x)pθ(y∣x)dx = Ex [pθ(y|x)]. Note that P(y|x) ∈ [0,1] for all X ∈ X, then ap-
plying the Hoeffding’s inequality for bounded random variables [Theorem 2.2.6, (Vershynin, 2018)]
yields,
1 N
NEPθ(y∣Xn) - Ex [Pθ(y∣x)] ≥ t I ≤ 2exp(-2Nt2) (25)
n=1
P (∣pbθ(y) - pθ(y)∣ ≥ t) = P
Let δ0 = 2exp (-2Nt2), it then follows,
P (∣bθ(y) -Pθ(y)∣ < Jlog2NP) ≥ 1 - δ	(26)
Given Eq (26), we first establish the concentration results of the entropy Hpbθ (y) with respect to
the empirical distribution pbθ (y). Assume For all y ∈ C, we have pθ (y), pbθ (y) bounded below by
1/(CK2) for some fixed constant C > 1. This assumption is practical since the distributions of true
12
Under review as a conference paper at ICLR 2019
data and predicted data are approximately uniform and therefore pθ(y), pbθ(y) ≈ 1/K2 for all y ∈ C.
Consider the function tlog t, with derivative 1 + logt ∈ [1 - log CK2, 1] for t ∈ [1/(CK2), 1],
pbθ (y)
施(y)logpθ(y) -pθ(y)logpθ(y)| = I /	(1 + logt)dt
pθ (y)
II pbθ (y)	II II pbθ (y)
≤ I |1 + log t|dtI ≤ I	max{log CK2 - 1, 1}dt
I pθ (y)	I I pθ (y)
≤ max{log CK - 1,1}∣pθ (y) - pθ(y)|
Summing over C gives
∣HHθ(y) - H(y)∣ ≤ K2max{logCK2 - 1,1}∣pθ(y) -pθ(y)| .
Let δ = K2 δ0, then Eq (26) together with Eq (28) yield the following,
(27)
(28)
(29)
-Hθ(y)
< K2 max{log CK2 - 1,1}『吗%20) ≥ 1 - δ
Next We are going to bound the divergence between Hθ(y∖x) and Hθ(y∖x) which are defined as,
1N
H (y∖χ) = - N E fps (y∖χn)log pθ (y∖χn),
Hθ (y∖x)
n=1 y
-	pθ(y∖x) logpθ(y∖x) .
x∈X y
Note that h log h ∈ [-1/e, 0] for all h ∈ [0, 1], then again applying [Theorem 2.2.6, (Vershynin,
2018)] yields,
P (k XPθ(y∖xn)logPθ(y∖xn) - Ep(X) [pθ(y∖x)logPθ(y∖x)] < t) ≤ 2exp (-2t2e2N)
n=1	(30)
Following the similar arguments as before, let δ0 = 2 exp -2t2e2N , then
P (∣E Xpθ(y∖χn)iogpθ(y∖χn) -Ep(X) [pθ(y∖χ)iogpθ(y∖χ)]
∕e2 log(2∕δ0)
V	2N
≤δ0
(31)
Now let δ= K2 δ0, then applying the union bound we have
∖Hbθ(y∖x) - Hθ(y∖x)∖ ≤
y∈C
1N
N fpe(y∖χn)logpθ(y∖χn) - Ep(X) [pθ(y∖χ)logPθ(y∖χ)]
n=1
∕e2 log(2K2∕δ)
≤ Kw -2N—
(32)
hold with probability 1 - δ.
Conclude from Eqs (29) & (32), we have
∣∣∣Iθ(x; y) - Ibθ (x; y)∣∣∣ ≤ ∣∣∣Hθ(y) - Hbθ(y)∣∣∣ + ∣∣∣Hθ(y∖x) - Hbθ(y∖x)∣∣∣
=K2 (max{log CK2 - 1,1} + e) ,log*20 .	(33)
hold with probability at least 1 - 2δ.
□
13
Under review as a conference paper at ICLR 2019
B Approximation of the marginal distribution
Computing the marginal distributions of the continuous representations z and zk requires the entire
dataset, e.g., pθ(Z) = RXpθ(z, x)dx ≈ N PN=I Pθ(z∣x(i)). To scale up our method to large
datasets, We propose to estimate based on the minibatch data, e.g., pθ(z) ≈ 强 PB=I Pθ(z∣x(i)).
Now consider the entropy H(z) of z, which we approximate in the following way,
1B	1B 1B
H(Z) = Ez[logp(z)] ≈ B ɪɪ^logP(z(i)) = B ∑^bg BZPθ(z(i)∣x(j)) .	(34)
We estimate the integral of Z by sampling Z 〜pθ (z|xi) and perform the Monte Carlo approxima-
tion. Although We minimize the unbiased estimator of the loWer bound of the KL divergence, the
term inside the logarithm is a summation of probability densities of Gaussians. In particular, We
record the distribution of the variances output by our encoder and observe that the mean of the vari-
ances of the Gaussians is bounded betWeen 0.2 and 2, Which implies that the values of probability
densities do not range in a large scale. Since logarithm is locally affine, We argue that our bound
in (34) is tight. Other quantities involved in our objective function (10) are estimated in a similar
fashion.
C Connections to VAE
In VAE, they assume a generative model specified by a stochastic decoder pθ(x|z), taking the con-
tinuous representation as an example, and seek an encoder qφ(z|x) as a variational approximation
of the true posterior pθ(z|x). The model is fitted by maximizing the evidence lower bound (ELBO)
of the marginal likelihood,
Ex [logPθ(x)] ≥ L(x,θ,φ) = Eqφ(z∣x) [logPθ(x|z)] - Ex DKL (qφ(z∣x)∣∣r(z))] .	(35)
Here the KL divergence term can be further decomposed as (Hoffman and Johnson, 2016),
Ex [Dkl (qφ(z∣x)∣∣r(z))] = I(x; z) + Ex DKL (qφ(z)∣∣r(z))] .	(36)
That is, minimizing the KL divergence also penalizes the mutual information Iθ (x; Z), thus reduces
the amount of information Z has about x. This can make the inference task qφ(z∣x) hard and
lead to poor reconstructions of x as well. Many recent efforts have been focused on resolving this
problem by revising ELBO. Although approaches differ, it can be summarized as either dropping
the mutual information term in Eq (36), or encouraging statistical independence across the dimen-
sions of Z by increasing the penalty on the total correlation term extracted from the KL divergence
DKL (qφ (Z) || r (Z)) with respect to qφ (z ). However, these approaches either result in an invalid lower
bound for the VAE objective, or cannot avoid minimizing the mutual information Iθ (x; Z) between
the representation and the data.
In contrast, IMAE starts with a stochastic encoder pθ(z|x) and aims at maximizing the mutual
information between the data x and the representations Z from the very beginning. By following
the constraints which are naturally implied by the objective in order to avoid degenerated solutions,
IMAE targets at both informative and statistical independent representations. On the other hand, in
IMAE the decoder qφ(x∣Z) serves as a variational approximation to the true posterior pθ(x|z). As
we will show in Section 4, being able to learn more interpretable representations allows IMAE to
reconstruct and generate data with better quality.
14
Under review as a conference paper at ICLR 2019
D VAT stabilizes the learning of categorical representations
JLSINW
Figure 6: Prevent over confidence predictions by encouraging local smoothness
E Disentanglement quality with respect to both continuous and
discrete representations on 2D shapes
See figure 7.
(a) IMAE performs well regarding the disentanglement score vs. decoding quality trade-off, especially in
the region of interest where both decoding quality and informativeness of representations are fairly good.
(b) Negative correlation between total correlation and disentanglement score. It also implies that the
disentanglement score tends to decrease along with the total correlation if using even larger β , due to
the diminishing informativeness of representation factors. In the extreme case, both total correlation
and disentanglement score can degrade to zero.
Figure 7: Disentanglement comparison on dSprites with respect to both y and z. The results are
reported by training each method with β ∈ [1,10], and we set β = γ∕2 with Y ∈ [1,10] for IMAE.
For each β value, every method is trained over 8 random initializations. Shade regions indicate the
80% confidence intervals.
15
Under review as a conference paper at ICLR 2019
Figure 8: JointVAE with different sets of target vlues (Cy, Cz). For each β value, we train JointVAE
with 10 different random seeds. We augment JointVAE with VAT.
F More results on JointVAE
See figure 8.
G Experimental settings
Table 1: Encoder and Decoder architecture for MNIST and Fashion MNIST.
Encoder	Decoder
Input vectorized 28 × 28 grayscale image	InPut y ∈ R10 and Z ∈ R10
FC. 500 BatChNorm ReLU	FC. 500 ReLU
FC. 2 × 500 BatChNorm ReLU	FC. 500 ReLU
FC.20 (μz,logσz)+10(py)	Fc 28 × 28 Sigmoid 一
Table 2: Encoder and Decoder architecture for dSprites.
Encoder	Decoder
Input vectorized 64 × 64 grayscale image	Input y ∈ R3 and z ∈ R10
FC. 1200 ReLU	FC. 1200 ReLU
FC. 1200 ReLU	FC. 1200 ReLU
FC. 2 × 1200 ReLU	FC. 1200 ReLU
FC.20 (μz ,log σz) + 3(py)	FC. 28 × 28 Sigmoid 一
Training procedure:
•	MNIST & Fashion MNIST: We use momentum to train all models. The initial learning
rate is set as 1e-3, and we decay the learning rate by 0.98 every epoch.
•	dSprites: We use Adam to train all models. The learning rate is set as 1e-3.
16