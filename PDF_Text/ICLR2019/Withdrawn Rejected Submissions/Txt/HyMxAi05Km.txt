Under review as a conference paper at ICLR 2019
Dual Learning: Theoretical Study
and Algorithmic Extensions
Anonymous authors
Paper under double-blind review
Ab stract
Dual learning has been successfully applied in many machine learning applica-
tions including machine translation, image-to-image transformation, etc. The
high-level idea of dual learning is very intuitive: if we map an x from one do-
main to another and then map it back, we should recover the original x. Al-
though its effectiveness has been empirically verified, theoretical understanding
of dual learning is still missing. In this paper, we conduct a theoretical study to
understand why and when dual learning can improve a mapping function. Based
on the theoretical discoveries, we extend dual learning by introducing more re-
lated mappings and propose highly symmetric frameworks, cycle dual learning
and multipath dual learning, in both of which we can leverage the feedback sig-
nals from additional domains to improve the qualities of the mappings. We prove
that both cycle dual learning and multipath dual learning can boost the perfor-
mance of standard dual learning under mild conditions. Experiments on WMT 14
English什German and MUltiUN English什French translations verify our theoret-
ical findings on dual learning, and the results on the translations among English,
French, and Spanish of MultiUN demonstrate the efficacy of cycle dual learning
and multipath dual learning.
1	Introduction
Most machine learning tasks can be formulated as learning a mapping from one domain to another
one, like image classification (from image to label), neural machine translation (from source lan-
guage to target language), speech recognition (from voice to text), etc. Among them, quite a few
tasks are of dual forms, like image classification v.s. image generation (from label to image), the
neural machine translation between two languages (English→French v.s. French→English), speech
recognition v.s. speech synthesis (from text to voice), etc. Such duality can be utilized to improve
the model qualities.
One prominent framework is dual learning, first proposed by He et al. (2016) for machine translation
and then applied to many other applications like image translation (Kim et al., 2017; Zhu et al.,
2017), question answering and generation (Tang et al., 2017), etc. In dual learning, two mapping
functions between two domains are trained simultaneously so that one function is close to the inverse
of the other. The intuition is that, if we translate a sentence from English to French and then translate
the obtained French sentence back to English, we should get the same sentence or a very simlar one.
Dual learning is of great interest because it can accommodate any unidirectional architecture, e.g.
a transformer (Vaswani et al., 2017), and provide a performance boost. Moreover, dual learning
can be used in semi-supervised learning, which is highly desirable since deep neural networks are
generally thirst for labeled data.
Despite of the empirical success of dual learning, the theoretical understanding is largely missing.
In this paper, we conduct both theoretical analyses and empirical studies to answer the following
questions:
Q1: Why and when does dual learning improve a mapping function?
Q2: Can we further improve the performance of a mapping function?
1
Under review as a conference paper at ICLR 2019
1.1	Our Contributions
Our contributions are in two folds: a theoretical study of dual learning and the frameworks of cycle
dual learning and multipath dual learning, which subsume dual learning as a special case. Without
loss of generality, we take machine translation as the example for the study and algorithm presenta-
tion.
Dual learning. We take a novel statistical approach to model the problem. Suppose there are two
baseline translators between two language spaces, one forward and one backward. Based on our
Theorem 1, dual learning outperforms both baseline translators as long as the baseline translators
are better than random mappings under natural assumptions. Empirical studies show that an im-
provement is observed even if the reconstruction is far from perfect.
Cycle dual learning and multipath dual learning. We propose cycle dual learning and multipath
dual learning frameworks by extending dual learning. Both frameworks use dual learning as the
basic building block and leverage a third, a fourth, or more languages to help boost the translator
qualities between the original two languages. We prove that under mild conditions, the two symmet-
ric frameworks outperform dual learning (Theorems 2 and 3). Our experiments on MultiUN dataset
show a significant improvement (1.45 BLEU points, see Table 4) against dual learning.
1.2	Related Work
Dual learning was first proposed by He et al. (2016) in the context of machine translation, where the
two dual translators are updated in a reinforcement learning manner with the reconstructed distortion
as the feedback signal. A similar approach proposed by Cheng et al. (2016) has the same high-level
idea but a very different implementation. Since then dual learning architectures have been proposed
for other applications including image processing (Kim et al., 2017; Zhu et al., 2017), sentiment
analysis (Xia et al., 2017a), image segmentation (Luo et al., 2017), etc.
Built upon the dual learning framework, Xia et al. (2017b) and Wang et al. (2018) consider the joint
distribution constraint, which says the joint distribution of sample over two domains is invariant
when computing from either domain. We relax this constraint for simplicity of analysis. Xia et al.
(2018) proposed model-level dual learning, which shares components between the primary direction
and the dual direction. Dual learning was also leveraged for unsupervised learning (Lample et al.,
2018; Artetxe et al., 2018).
Despite the vast number of works related to dual learning, none of them explain why it works or
when it fails. Galanti et al. (2018) claim that dual learning does not circumvent the alignment
problem, where a sentence is translated wrong by the forward translator but translated back to it by
the backward translator. We show that the alignment problem occurs with a small probability under
dual learning compared to the baseline translator, and this probability can be further reduced by our
cycle dual learning and multipath dual learning. Furthermore, their hypothesis that the translator
should not be too complex is not verified in the context of machine translation.
2	Preliminaries
In this section, we introduce the notations and the essential assumptions used for theoretical analy-
sis. A table summarizing the notations can be found in Appendix A. Let S1, . . . , Sk be k language
spaces, composed of sentences in each language. For any Si, we denote the distribution of sentences
in Si by μ(i) and let X(i) be the random variable, i.e. Pr(X(i) = X) = μ(i)(x). For any x(i) ∈ S%,
we denote the correct translation in Sj by x(j). In practice, we may select a threshold BLEU (Pa-
pineni et al., 2002a) score, above which the translation is considered correct. Let Tij denote the
baseline translator that translates from a sentence in Si to one in Sj, i.e. the desired mapping is
Tij(X(I)) = x(j). When a random sentence x(i) is sampled from Si according to μ(i), it is possible
that this sentence is translated incorrectly. We use accuracy of the translator pij , the accuracy of
the translator, to describe the probability of translating a sentence correctly when this sentence is
randomly sampled from Si according to μ(i). Formally, Pij = Plχ(i)~μ(i)(Tij (x(i)) = X(j))=
Ex(i)~μ(i)[ITij(X(i))=x(j)] = Px(i)∈Si ITij(X(i))=x(j)μ(i)(x(i)), where 1 is the indicator function.
2
Under review as a conference paper at ICLR 2019
We sometimes omit the subscript x(i) 〜μ(i) for simplicity. It is also easy to see that
PMTij (Xei)) = Xj))= E[l Tij(X ⑶)=χ(j) ] = 1 - Pij.	⑴
In order to characterize reconstruction accuracy, we let X (j,r) denote the random variable that fol-
lows the distribution	Tij(X(i))	where X(i)〜μ(i).	We define	μ(j,r)(x)	= Pr(X(j,r)	=	x),
and further define Pri = Prχ(j∕)〜μj,r)(Tji(x(j)) = x(i)) = Eχj,r)〜*(j/)[1 Tji(x(j))=x(i)]=
∑x(j) ∈Sj 1Tji(Xj) )=x(i) μ(j,r) (Xj)). The superscript r means “reconstruction”. The difference be-
tween Pjri and Pji lies in the distributions of samples in space Sj . We assume that Tij and Tji are
independent translators. This is natural because Tij andTji are usually trained independently. Then,
for any j 6= i, we have
Pr(Tij(X(i)) =X(j),Tji(X(j)) = X(i)) = PijPjri.	(2)
Further, we make the following assumption.
Assumption 1. For any j = i, any y(j) ∈ Sj, E[ly(j)=χ(j),Tij(x(i))=y(j)] = I-PL, where m is a
constant.
The interpretation of this assumption is as follows. For all X(i) ∈ Si that are incorrectly translated
into Sj , there are m possibly incorrect translations (in Sj ) with positive probabilities. Assumption 1
states that each of the incorrect translations is chosen uniformly at random.
Justification of Assumption 1. It is generally not true in the real-world since the probabilities
of different translations are usually very different. However, this assumption can be approximated
by partitioning the possibly wrong translations into several clusters, where the probability of the
translation falling into each cluster is close. The best case is that the incorrect translation with the
largest probability Pmax is treated as one cluster. All the other possibly incorrect translations are
clustered so that the probability of each cluster is close to PmaX. Then We have m ≤ 1-pij. A small
pmax
PmaX (a large m) is desirable. Note that the correct translation is indeed also a group of translations
if we set a threshold BLEU score, above which the translation is considered correct.
To analyze the efficacy of dual learning, we consider the following two cases:
(Case 1)Tji(Tij(X(i))) =X(i);	(Case2)Tji(Tij(X(i))) 6= X(i).
Dual learning will detect Case 2 and train the translators so that Case 2 is minimized. We use
superscript d for the translators trained from dual learning. For any given Xi ∈ Si which fails in
reconstruction (Case 2), we define
α= Pr(Tidj(X(i)) =	X(j), Tjdi(Tidj(X(i))) =	X(i)|Case	2)	(3)
β=Pr(Tidj(X(i)) 6=	X(j), Tjdi(Tidj(X(i))) =	X(i)|Case	2)	(4)
γ=Pr(Tjdi(Tidj(X(i))) 6= X(i)|Case 2),	(5)
where “Case 2” denotes the condition Tji(Tij(X(i))) 6= X(i). Here α can be viewed as the probability
of correcting the wrong translations by dual learning, β the probability of the occurrence of the
alignment problem under Case 2, and γ the probability of nonzero reconstruction error. γ models the
imperfectness of dual learning, which should be zero in the ideal case. It is easy to see α+β+γ = 1.
3 Theoretical Study
In this section, we provide a theoretical study of why dual learning outperforms the baseline trans-
latorby the following theorem.
Theorem 1. Under Assumption 1, for any language spaces Si andSj, the accuracy of dual learning
outcome Tj is Pd = (1 — α)pij-pj + α(1 — (I PijmI Ppii), where a is defined in equation 3.
This theorem is proved by explicitly computing the probabilities of each case. During dual learning,
Case 2 is redistributed to α case (a desirable subcase of Case 1, defined in equation 3), β case (an
undesirable but unavoidable subcase of Case 1, defined in equation 4), andγ case (remaining in Case
2, defined in equation 5). See Appendix B for the full proof. We have the following observations
from Theorem 1.
3
Under review as a conference paper at ICLR 2019
Relation to the baseline translators. The accuracy is dual learning improvement is positively
correlated to the baseline translators of both directions. The larger the pij or pjri is, the higher
accuracy of Tidj dual learning can achieve.
The role of m and α. When pij , pjri are fixed, a large m is desired for better accuracy. As α
increases, the outcome of dual learning improves. In the optimistic (but maybe unrealistic) case
where α =1,β = Y = 0,we havePj - Pij = 1 - (1-'蜉1->)-Pij = (I-Pj)(1-1+,洒). This
means dual learning provides an improvement as long as m ≥ 1 and pjri > 0, which is trivial.
A hypothesis. We consider the case where the probabilities of redistribution to α case and β case
are proportional to a and β. Formally, β
Then we have α
mPij Pri(I-Y)
mpijpr^+(l-pi7)(l-pr^)
Pr(Tj(X ⑸)=Xj) ,Tji(Tj (x ⑸))= x ⑸)=	mPj Pri
Pr(Tij(X⑸)=x⑶,Tji(Tij (x⑸)) = x⑸)=(I-Pij )(1-Pji) .
and
Pidj =Pr(Tidj(x(i)) = x(j), Tjdi(x(j)) = x(i))
=mPijPji(1 - γ(1 -PijPri - (I-PjmI-Pji))) =	mPijpjiQ - D	⑹
一	mPijPjri + (I - Pij)(I - Pji)	— mPijPjri + (I - Pij)(I - Pjiy
where Γ = Y (1 -PijPji -(I PjmI Pji)). To compare Pj with the accuracy of the original translator,
we compute the difference
Pd- Pj=Pj (___________mPji (IT)______________1)
ij	mrn-r>ij Pji + (1 - Pij )(1 - Pji)
=p，，(___________mPji .._________- - 1______________S-_____________-)
mPij Pji + (1 - Pij)(1 - Pji)	mPij Pji + (1 - Pij)(1 - Pji)
=	((1 -Pij)((m + 1)Pji - 1)_________________PmPji____________)
Pij mPijPji + (1 -Pij)(1 -Pji)	mPijPji + (1 -Pij)(1 -Pji))
Ideally, we have Y = 0, which means Γ = 0. If Pjji > m+T, the outcome of dual learning is better
than the baseline translator. This means, as long as the overall probability of a correct translation is
greater than choosing from the m + 1 translations (1 correct and m incorrect) uniformly at random,
dual learning outperforms the baseline translator. The expression with the Γ factor is negative, which
is consistent with the intuition that Y should be minimized. For the nonideal case where Γ > 0, if
both Pji > m1+τ and Γ < (1 - Pij)(1 + m - m∣r-) hold, dual learning also improves against the
baseline translator.
Figure 1: The proposed multipath dual learning and cycle dual learning.
4 Extensions: Cycle Dual Learning and Multipath Dual
Learning
In Theorem 1, we found that both Pij and Pjji play a positive role in improving Pidj under mild
assumptions. A natural question is whether this probability could be further enhanced by exploiting
multiple language domains. Therefore, we propose the two frameworks, cycle dual learning and
multipath dual learning, both of which leverage multiple languages and significantly extend the
standard dual learning. In this section, we mainly present the cycle dual learning framework due to
the space constraint.
4
Under review as a conference paper at ICLR 2019
4.1 Framework
The proposed frameworks are illustrated in Figure 1. Let S1 and S2 denote the source language space
and the target language space respectively. To use these frameworks, we first train the following
translators: Si 什 S2, Si 什 Sk and S2什 Sk where k ≥ 3. Then, for cycle dual learning, We
require a sentence from S2 to be very similar to S2 → Sk → S1 → S2 (or equivalently, a sentence
from Si to be very similar to Si → S2 → Sk → Si); for multipath dual learning, we require
the translation Si → S2 to be the very similar Si → Sk → S2 . In this way, we build another
constraint, where the translation Si → S2 could leverage the information pivoted by the domain
Sk . In practice, to use cycle dual learning to enhance the Si → S2 model, we need to minimize
Px(2)∈s2 D(T12(Tki(T2k(x(2)))),x(2)), where D(∙, ∙) measures the differences of two inputs; to
use multipath dual learning, we need to minimize Px(1)∈S D(Ti2(x(i)), Tk2(Tik(x(i)))). Similar
update could also be applied to S2 → Si translation and leverage more language domains. If no
auxiliary domain is provided, both cycle dual learning and multipath dual learning will degenerate
to the standard dual learning. We design sampling based algorithms for the two frameworks. This
section focuses on the cycle dual learning framework with corresponding theoretical analysis. See
Appendix D and F for the multipath dual learning framework. In both frameworks, let θij denote
the parameters of translator Tij.
Algorithm 1: Cycle Dual Learning Framework
1	Input: Samples from spaces Si . . . Sk, initial translators Ti2, T2i and Tii, Tii ∀i = 3, . . . , K;
learning rates η;
2	Train each ofTi2, T2i and Tii, Tii ∀i = 3, . . . , k by dual learning;
3	Randomly sample a k from {3,4, ∙∙∙ , K}; randomly sample one x(1)∈ Si and one x(2) ∈ S2;
4	Generate X(2)by Tk2 ◦ Tik(x(i)) and generate X(1)by Tki ◦ T2k(x(2));
5	Update the parameters of Ti2 and T2i, denoted as θi2 and θ2i, as follows:
θi2 - θi2 + ηVθ12 logP(X⑵|x⑴；θi2); θ2i - θ2i + η%θ21 logP(X⑴|X⑵；。21)；	(7)
Repeat Step 3 to Step 5 until convergence;
4.2 Theoretical Analysis
The theoretical analyses of cycle dual learning and multipath dual learning are highly symmetric. In
this section, we take cycle dual learning as an example. The analysis for multipath dual learning can
be found in Appendix F.
For simplicity, we focus on the triangle structure that contains only Si, S2 and S3. Now dual learning
serves as the basic structure. For simplicity, we use qij to denote the accuracy ofTidj, i.e. qij = pidj.
We further define q32 = PrX⑶〜Td(x(i))(T⅛(x⑶)= x(i)), which as the accuracy of Twi when
the sentence in S3 is translated from a random sentence in S2 . This is analogous to reconstruction
accuracy. To analyze cycle dual learning, we consider the following two cases (analogous to Case 1
and Case 2 for dual learning):
Case 1: X(i) = T3di(T2d3(Tid2(X(i)))); and Case 2: X(i) 6=T3di(T2d3(Tid2(X(i)))).
Cycle dual learning will detect Case 2 and train the translators so that Case 2 is minimized. To
quantify this effect, we define the following probabilities:
α0 = Pr(Tic2(X(i)) = X(2), T3ci(T2c3(X(2))) = X(i)|Case 2)
β0=Pr(Tic2(X(i)) 6=X(2),T3ci(T2c3(Tic2(X(i))))=X(i)|Case2)
γ0 = Pr(X(i) 6=T3ci(T2c3(Tic2(X(i))))|Case2),	(8)
where Case 2 denotes the condition X(i) 6= T3di(T2d3(Tid2(X(i)))) and superscripts d and c denote the
translators obtained from dual learning and cycle dual learning, respectively. α0, β0, γ0 can be viewed
as the probability of correcting the wrong translations by cycle dual learning, the probability of the
occurrence of the alignment problem under Case 2, and the probability of nonzero reconstruction
5
Under review as a conference paper at ICLR 2019
error. γ0 models the imperfectness of dual learning. And we have α0 + β0 + γ0 = 1. We assume that
T2d3 and T3d1 are independent translators. That implies,
Pr(T2d3(x1 (2)) =x(3) * 5,T3d1(x(3)) = x(1)) = q23q3(21)	(9)
We will also need a similar assumption to Assumption 1 as follows.
Assumption 2. For any j = i, any y(j) ∈ Sj, E[Iyj)=Xj),td.(方⑸)=yj)] = IZmj, where m is a
constant.
The interpretation and justification of Assumption 2 is similar to those of Assumption 1. Then we
have the following theorem about the triangle structure. The more general case can be viewed as
adding one path a time so Theorem 2 can be applied.
Theorem 2. Given languages spaces S1, S2, and S3, where the objective is to train a translator
that maps from S1 to S2, the accuracy of cycle dual learning outcome is
P12 = qi2(1 - α0)(q23q31) + (1 - q23)(1 - q3?))
m
o o71	(1 - qi2)(q23 + q(1) - (m + 1)q23q31) + m - 1)
+ α (1-----------------------------5-----------------------
m2
(10)
under Assumption 2.
Theorem 2 is also proved by explicitly computing the probability of each case. The full proof can
be found in the Appendix C. Theorem 2 is much harder to interpret than Theorem 1, but we can still
observe how factors affect the accuracy of T1c2 .
The role of T1d2. The accuracy increases as q12 increases. So a good initial translator is desirable.
However, if T1d2 is arbitrarily bad, it is still possible to improve it significantly. This implies the
application of this framework on low-resource or even zero-shot machine translation.
A similar hypothesis. Similar to the analysis of dual learning, we consider the condition where
α0 =	Pr(Td2(X(I))=X ⑶，τ3ιCT⅞(x ⑶))=X(I))
β0 = Pr(Td2(X(I))=X ⑵,T3ιCT⅛CT⅛(x ⑴)))=X(I))
Then the accuracy simplifies to
and define Γ0 = γ0 Pr(x(1) 6= T3d1(T2d3(T1d2 (x(1))))).
C = α0(1- Γ0) = _________________qi2(q23q37 + (I-如mm-q(I)) )(1- Γ0)________________
α02	α + β0	q]2(q23q(2) + (1 —q23)(1 —q(I)) ) + (1-q12)(q23+q(2)(m+1)q23q32)+m-I)
_	1 - Γ0	1 - Γ0
1 + (1-qi2 )(q23 + q(1)-(m+1)q23q(1)+m-1)	1 + M 1-q12
mqi2(mq23q3l) + (1-q23)(1 —q(2)))	"?
where M = q23+q(1)-(m+1)"23"")+?: 1. We observe that When Y0 = 0 (and therefore Γ0 = 0)
m(mq23 q31 +(1-q23)(1-q31 ))
and M = 1, it simplifies to q12, which is the accuracy of dual learning and that pc12 increases
as M decreases. To characterize the condition when pc12 > q12, we let M < 1. We have
qi3+&-i()m+1)qi3&+？-1 < 1, which leads to
m(m"13"32 +(1-"13)(1-"32 ))
((m + 1)q13 - 1)((m + 1)q3(12) - 1) > 0.
When q13, q32) > m+ι, which means the probability of correctly translating a sentence is greater
than choosing from potentially wrong translations uniformly at random, cycle dual learning outper-
forms dual learning.
We also prove a similar theorem for multipath dual learning (see Appendix F). The accuracy of
multipath dual learning has a similar form. We observe similar empirical performances as well (see
Appendix E).
5 Experiments of Dual Learning
Since previous works (He et al., 2016; Xia et al., 2017a;b; Wang et al., 2018; Xia et al., 2018) have
demonstrated strength of dual learning, we aims at providing some theoretical insights. We choose
6
Under review as a conference paper at ICLR 2019
WMT2014 English什Germen translation1 and MUltiUN (Eisele & Chen, 2010) English什French
translation2 to verify our theoretical analysis for dual learning. For ease of reference, denote English,
French and German as En, Fr and De respectively.
5.1	Settings
Datasets Following the common practice in NMT, for the En什De, we preprocess the data in the
same way as that Used in Vaswani et al. (2017) and eventUally obtain 4.5M training sentence pairs.
We concatenate newstest2012 and newstest2013 as the validation set (6K sentence pairs) and choose
newstest2014 as the test set (3K sentence pairs). For the MultiUN En什Fr translation, We sample
2M/6K/3K sentence pairs as the training/validation/test sets. All the sentence are split into word-
piece following (Johnson et al., 2016). To leverage dual learning as that used in (He et al., 2016),
for WMT'14 En什De translation, we choose 8M monolingual English sentences and 8M monolin-
gial German sentences from newscrawl. For MultiUN En什Fr translation, we randomly sample 1M
English and 1M French sentences as the monolingual data to construct the duality loss.
Architecture & Optimization We use Transformer (Vaswani et al., 2017), the state-of-the-art NMT
system, for each direction. For WMT En什De translation, we choose the transformer-big configu-
ration, in which the word embedding dimension, hidden dimension filter sizes and number of multi-
head attention are 1024, 1024 and 4096 respectively. For MultiUN En什Fr translation, we choose
the transformer-base configuration, in which the aforementioned four numbers are 512, 512, 2048
and 8 respectively. The optimization algorithm is Adam with learning rate 2 × 10-4, β1 = 0.99 and
7168 tokens per GPU. We train our models on 8 M40 GPUs for 7 days.
Evaluation For WMT 2014 En什De translation, following Vaswani et al. (2017), we use beam
search with beam width 4 and length penalty 0.6 to generate candidates. The evaluation metric
is BLEU score (Papineni et al., 2002b), which is a geometric mean of n-gram precisions (n =
1, 2, 3, 4). A large BLEU score indicates a better translation quality.
5.2	Results
Translation qualities The BLEU scores of each translation tasks are summarized in Table 1, in
which the second row and third row represent the results of the standard Transformer and dual
learning. We can see that after applying dual learning, the performances of all tasks are boosted.
Specially, on En→De and De→En translations, we can improve the baseline from 28.40 to 29.97,
and from 32.15 to 35.16. On the other task, dual learning can achieve 0.65 and 0.86 point improve-
ment, which demonstrates its effectiveness. We found that on MultiUN, we do not achieve as much
improvement as WMT. The reason is that MultiUN dataset is a collection of translated documents
from the United Nations, which are usually of formal and simple patterns that are easy to learn. As
a result, introducing more data might not increase the BLEU so much.
	En→De	De→En	En→Fr	Fr→En
Standard	28.40	32.04	50.26	50.56
Dual Learning	29.97	34.93	50.91	51.42
Table 1: BLEU scores of WMT2014 En什De and MultiUN En什Fr translations tasks.
The translator accuracy We interpret the results in terms of accuracy when different thresholds
are selected. We vary the threshold BLEU score from 10 to 40, and the accuracy of each translator
is shown in Table 2. Let Si and S? denote English and German respectively for the En什De task
(English and French respectively for the En什Fr task). Values are percentages of translations that
are above the threshold.
Qualitively, we observe that dual learning outcomes are better than single transformers. More inter-
esting observations lie in the following quantitative analysis.
1Data available at http://www.statmt.org/wmt14/translation-task.html.
2Data available at http://opus.nlpl.eu/MultiUN.php
7
Under review as a conference paper at ICLR 2019
En什De				En什Fr				
Threshold BLEU	p12	p21	p1d2		pd21	p12	p21	p1d2	p2d1		
10	0.42	0.55	0.66	0.75	0.82	0.80	0.82	0.81
20	0.27	0.38	0.55	0.67	0.77	0.74	0.78	0.75
30	0.13	0.21	0.38	0.50	0.67	0.64	0.68	0.66
40	0.06	0.10	0.24	0.32	0.55	0.53	0.56	0.55
Table 2: Accuracy of translators using different threshold BLEU scores.
The roles of m and γ It appears that all theories throughout this paper are independent of the
selection of threshold score. However, in practice, different thresholds result in different m and γ
values. A higher threshold BLEU score usually means there are more incorrect possible translations,
and it will be harder reconstruct the original sentence well. Despite of this, we assume m and γ are
constants and performed a regression analysis using equation 6, where m and γ are independent
variables. we compute p12, p21, and p1d2 for all integer threshold scores between 1 and 100, and fit
equation 6 by minimizing the absolute value of differences between the predicted p1d2 and true p1d2,
resulting in m = 92 and Y = 0.45 for the En什De task (m = 12 and Y = 0.57 for the En什Fr task).
The high γ value on one hand implies that machine translation is generally a hard task, and on the
other hand means there is still space to improve.
6 Experiments of Multipath Dual Learning
To verify the effectiveness of cycle dual learning and multipath dual learning, we focus on the
translation between English (En), French (Fr) and Spanish (Es). Again, we choose the MultiUN
dataset to build the translation models since any two of the aforementioned three languages have
bilingual sentence pairs. We study two different settings, where for each language pair, we are
provided with 2M or 0.2M bilingual sentence pairs. For both settings, we choose 1M monolingual
sentences for each language. We use transformer_base for all experiments in this section, where the
model is a six-block network, with word embedding size, hidden state size and filter size 512, 512
and 2048. The training process is the same as that in Section 5.
	En→Fr	Fr→En	En→Es	Es→En	Es→Fr	Fr→Es
Baseline	50.26	50.56	55.15	55.23	47.75	48.13
Dual Learning	50.91	51.42	55.51	55.77	48.23	48.52
Cycle	51.28	51.89	55.97	56.17	48.62	48.87
Table 3: Experimental Results on MultiUN (2M bilingual data)
The experimental results of using 2M bilingual data and 1M monolingual data are shown in Table 3.
We can see that on average, dual learning can boost the six baselines (i.e., standard transformer) by
0.55 point. Although dual learning can achieve verify high scores on MultiUN translation tasks, our
proposed cycle dual learning can still improve it by 0.41 point on average.
	En→Fr	Fr→En	En→Es	Es→En	Es→Fr	Fr→Es
Baseline	43.12	43.26	49.28	47.80	41.47	41.21
Dual Learning	45.54	45.44	51.07	50.31	42.81	42.57
Cycle	47.23	46.72	52.56	51.65	43.52	44.97
Table 4: Experimental Results on MultiUN (0.2M bilingual data)
The results of using 0.2M bilingual data plus 1M monolingual data is shown in Table 4. We have
the following observations: (1) Since there are fewer bilingual sentences, the baselines of the six
translation tasks are not as good as those in Table 3. (2) For this setting, dual learning can improve
the BLEU scores by 1.93 points on average, which is consistent with the discovery in He et al.
8
Under review as a conference paper at ICLR 2019
(2016) that dual learning can obtain more improvements when the number of bilingual sentences is
small.(3) When cycle dual learning is added to the conventional dual learning, we can achieve an
extra 1.45 improvements.
7 Conclusions
We provide the first theoretical study of dual learning and characterize conditions when dual learning
outperforms baselines. We also propose two algorithmic extensions of dual learning, the cycle dual
learning framework and multipath dual learning framework, which are provably better than dual
learning under mild conditions. Our dual learning experiments demonstrate the efficacy of dual
learning w.r.t. accuracy and provide insights on the potential power of dual learning. Our cycle dual
learning framework achieves a new state-of-the-art BLEU score.
References
Mikel Artetxe, Gorka Labaka, Eneko Agirre, and Kyunghyun Cho. Unsupervised neural machine
translation. In International Conference on Learning Representations (ICLR 2018), 2018.
Yong Cheng, Wei Xu, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. Semi-
supervised learning for neural machine translation. In Proceedings of the 54th Annual Meeting
of the Associationfor Computational Linguistics (Volume 1: Long Papers), volume 1, pp. 1965-
1974, 2016.
Andreas Eisele and Yu Chen. Multiun: A multilingual corpus from united nation documents. In
Proceedings of the Seventh conference on International Language Resources and Evaluation, pp.
2868-2872, 5 2010.
Tomer Galanti, Lior Wolf, and Sagie Benaim. The role of minimal complexity functions in unsu-
pervised learning of semantic mappings. In 6th International Conference on Learning Represen-
tations, 2018.
Di He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu, Tieyan Liu, and Wei-Ying Ma. Dual learning
for machine translation. In Advances in Neural Information Processing Systems, pp. 820-828,
2016.
Melvin Johnson, Mike Schuster, Quoc V Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil
ThoraL Fernanda Viegas, Martin Wattenberg, Greg Corrado, et al. Google's multilingual neural
machine translation system: enabling zero-shot translation. arXiv preprint arXiv:1611.04558,
2016.
Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jung Kwon Lee, and Jiwon Kim. Learning to discover
cross-domain relations with generative adversarial networks. In International Conference on Ma-
chine Learning, pp. 1857-1865, 2017.
Guillaume Lample, Alexis Conneau, Ludovic Denoyer, and Marc’Aurelio Ranzato. Unsupervised
machine translation using monolingual corpora only. In International Conference on Learning
Representations (ICLR 2018), 2018.
Ping Luo, Guangrun Wang, Liang Lin, and Xiaogang Wang. Deep dual learning for semantic image
segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni-
tion, Honolulu, HI, USA, pp. 21-26, 2017.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of the 40th annual meeting on association for
computational linguistics, pp. 311-318. Association for Computational Linguistics, 2002a.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of the 40th annual meeting on association for
computational linguistics, pp. 311-318. Association for Computational Linguistics, 2002b.
Duyu Tang, Nan Duan, Tao Qin, Zhao Yan, and Ming Zhou. Question answering and question
generation as dual tasks. arXiv preprint arXiv:1706.02027, 2017.
9
Under review as a conference paper at ICLR 2019
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-
mation Processing Systems,pp. 5998-6008, 2017.
Yijun Wang, Yingce Xia, Li Zhao, Jiang Bian, Tao Qin, Guiquan Liu, and T Liu. Dual transfer
learning for neural machine translation with marginal distribution regularization. In Proceedings
of the Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
Yingce Xia, Jiang Bian, Tao Qin, Nenghai Yu, and Tie-Yan Liu. Dual inference for machine learn-
ing. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence,
IJCAI-17, pp. 3112-3118, 2017a.
Yingce Xia, Tao Qin, Wei Chen, Jiang Bian, Nenghai Yu, and Tie-Yan Liu. Dual supervised learning.
In International Conference on Machine Learning, pp. 3789-3798, 2017b.
Yingce Xia, Xu Tan, Fei Tian, Tao Qin, Nenghai Yu, and Tie-Yan Liu. Model-level dual learning.
In International Conference on Machine Learning, pp. 3789-3798, 2018.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation
using cycle-consistent adversarial networks. In IEEE International Conference on Computer
Vision, 2017.
10
Under review as a conference paper at ICLR 2019
A Notations
k	number of language spaces
Si	i-th language space
μ(i)	distribution of sentences in Si
X(i)	the random variable that follows μ(i)
x(i)	one sample (sentence) in Si
x(j) the correct translation of x(i) in SJ
Tij	the baseline translatorthat translates from Si to Sj
pij	accuracy of the Tij
Tidj	translator that translates from Si to Sj trained using dual learning
pidj	accuracy of T dij
Ticj	translator that translates from Si to Sj trained using cycle dual learning
picj	accuracy of Ticj
Timj	translator that translates from Si to Sj trained using multipath dual learning
pimj	accuracy of Timj
Table 5: Notations
B	Proof of Theorem 1
Proof. Consider a random sample x(1) and the translation from x(1) ∈ S1 to S2 . Before dual
learning, the accuracy is p12. We analyze the two cases defined earlier in this section.
Case 1.	T21(T12(x(1))) = x(1). Case 1 consists of two subcases:
Case 1.1: T12(x(1)) = x(2);
Case 1.2: T12(x(1)) 6= x(2).
Although Case 1.2 is not desired, dual learning does not detect it. From equation 2, the probability
of Case 1.1 is Pr(T12(x(1)) = x(2), T21(T12(x(1))) = x(1)) = p12pr21, According to Assumption 1,
we have
Pr(T12(x(1)) 6=x(2),T21(T12(x(1))) = x(1))
= X	Pr(T12(x⑴)=y⑵,T21(y⑵)=X⑴)=(I- p12)(1- pr1)
m
y(2) ∈S2 ,y(2) 6=x(2)
Case 2.	T21(T12(x(1))) 6= x(1). Dual learning will train the translators so that this case is minimized.
The probability of this case is simply the complement of Case 1.
Pr(CaSe 2) = 1 -p12prι - (I-p12)(1 -PrI)
m
After dual learning, Case 2 is redistributed to the two subcases of Case 1, with probabilities α and β
respectively. So we have
Pr(Td2(x⑴)=x(2),Tdι(Td2(X⑴))=x(1)) = Pi2Pfι + α(1 - Pi2Pi - (I- p12)(1 - pr')
m
=(1 - α)p12pr1 + α(1 - (I- p12)(I- PrI))	(11)
m
Pr(Td2(X(I)) = X⑵,T2dι(理(X(I))) = X(I)) = (I- β)(1-pi2)(1-PrI) + β(1 -p12prι)
m
Pr(TdI(理(X(I))) = X(I)) = Y(1 - Pi2P3i - (I- p12)(I- PrI)),
m
where equation 11 computes the accuracy of dual learning.	□
11
Under review as a conference paper at ICLR 2019
C Proof of Theorem 2
Proof. We focus on the mapping from x(1) ∈ S1 to S2 and consider the following two cases:
Case1:T3d1(T2d3(T1d2(x(1)))) = x(1);
Case2:T3d1(T2d3(T1d2(x(1)))) 6= x(1).
We first characterize the probability of Case 1.
Case 1: According to whether the translation in S2 is correct, Case 1 can be partitioned into two
subcases:
Case 1.1: T1d2(x(1)) = x(2),T3d1(T2d3(x(2))) = x(1);
Case 1.2: T1d2(x(1)) 6=x(2),T3d1(T2d3(T1d2(x(1)))) =x(1).
Case 1.1. For the path S1 仆 S2, we have Pr(Td2(X(I)) = x(2)) = q12. For the path S2 仆 S3 仆 S1,
since we only care about the translation in S1, we have
Pr(T3d1(T2d3(x(2)))=x(1))=Pr(T2d3(x(2))=x(3))Pr(T3d1(T2d3(x(2)))=x(1)|T2d3(x(2))=x(3))
+Pr(T2d3(x(2)) 6=x(3))Pr(T3d1(T2d3(x(2)))=x(1)|T2d3(x(2)) 6= x(3))
=q23q31) + (Ii)(1-q31))
m
where the first equality is obtained by the law of total probability, and the second quality is obtained
by equation 9 and Assumption 2. Then the probability of Case 1.1 is computed as follows.
Pr(T⅛(x⑴)=X⑵,T⅛C⅛(x⑴))=X⑵)=qι2(q23q3? +(1 - q^ - q3" )	(12)
Case 1.2. For the Si 什 S2 path we have Pr(T⅛(χ⑴)=x(2)) = 1 一 qι2. Let y(2) ∈ S2 denote this
incorrect translation. Now we compute Pr(T3d1(T2d3(X(2))) = y(1)). There are three cases depending
on the translation at S3: X(3), y(3), and others. They are computed as follows.
Pr(T3d1(T2d3(y(2)))=X(1)|T2d3(X(2))=X(3))=q3(11)
Pr(T3d1(T2d3(y(2))) = X(1)|T2d3(X(2)) = y(3))
1 - q3(11)
m
Pr(T3d1(T2d3(y(2)))=X(1)|T2d3(X(2)) 6=X(3),y(3))
ι-q31)
m
Using the law of total probability we have
Pr(Tdi(Td3(y(2))) = X(I)) = q3"1-q23)
q23(I - q31))	(m — I)(I 一 q23)(1 — q31))
+	+	2
m	m2
m(q32 + q31 - 2q23q31)) + (m - I)(I - q23)(I - q3?)
m2
q23 + q31) ― (m + 1)q23q31) + m ― 1
m2
Then the probability of Case 1.2 is
Pr(T1d2(X(1)) 6=X(2),T3d1(T2d3(T1d2(X(1))))=X(1))
(1 - q12)(q23 + q3(11) - (m + 1)q32q3(11) + m - 1)
m2
Case 2. The probability of Case 2 is simply the complement of Case 1.
(13)
Pr(T3d1(T2d3(T1d2(X(1)))) 6= X(1)) = 1 - Pr(T3d1(T2d3(T1d2(X(1)))) = X(1))
=1 - q12 (q23 q3(11) +
(1 一 q23)(I — q31)))
m
(1 一 q12)(q23 + q3(11) 一 (m + 1)q23q3(11) +m 一 1)
m2
(14)
12
Under review as a conference paper at ICLR 2019
Then the accuracy of this triple learning is
p1m2=Pr(T1d2(x(1))=x(2),T3d1(T2d3(x(2)))=x(1))+α0Pr(T3d1(T2d3(T1d2(x(1)))) 6= x(1)), (15)
where α0 is defined in equation 8. equation 10 is obtained by substitute equation 12 and equation 14
into equation 15.	口
D	Multipath Dual Learning Framework
The algorithm of multipath dual learning is shown in Algorithm 2.
Algorithm 2: Multi-path Dual Learning Framework
1	Input: Samples from spaces S1 . . . Sk, initial translators T12 , T21 and T1i , Ti1 ∀i = 3, . . . , K;
learning rates η;
2	Train each of T12, T21 and T1i, Ti1 ∀i = 3, . . . , k by dual learning;
3	Randomly sample a k from {3,4, ∙∙∙ , K}; randomly sample one x(1) ∈ Si and one x(2) ∈ S2；
4	Sample one x(k) ~ Tik(X⑴)，x(2) ~ Tk2(x(k)); x(k) ~ T2k(x(2)) andx(2) ~ Tk2(X(k));
5	Update the parameters of T12 and T21, denoted as θ12 and θ21, as follows:
θi2 - θ12 - ηVθ12 logP(X⑵|x(1); θ12); θ21 - θ21 - ηVθ21 logP(X⑴|x(2); θ21);	(16)
Repeat Step 3 to Step 5 until convergence; * 12
It is well-known that the two gradients in equation 16 are unbiased estimators of
Vθ12Dkl(P(∙∣x(1); Tk2 ◦ Tik)kP(∙∣x(1); θ12)); Vθ21 Dkl(P(∙∣x⑴；Tk2 ◦ Tik)∣∣P(∙∣x⑴；θ12)).
(17
The reason is shown as follows. For any given X(i) ∈ Si,
min DKL(P(∙∣x(1); θik,θk2)∣∣P(∙∣x(1); θ12))
θ12
二 XP(X ⑵ 1x(1) ； θik,θk2)logP (Pu=1θlθk2),	(18
which is equivalent to
max	P (X(2)|X(i); θik, θk2) log P (X(2)|X(i); θi2)
θ12
12 x(2)∈S2
= X X P (X(2), X(k)|X(i); θik, θk2) log P (X(2)|X(i); θi2)
x(2)∈S2 x(k) ∈Sk
= X X P (X(k)|X(i); θik, θk2)P (X(2)|X(k), X(i); θik, θk2) log P (X(2)|X(i); θi2)	(19)
x(2)∈S2 x(k) ∈Sk
= X X P (X(k)|X(i); θik)P (X(2)|X(k); θk2) log P (X(2)|X(i); θi2)
x(2)∈S2 x(k) ∈Sk
=Eχ(k)~P(∙∣χ(ι)Rik)Eχ ⑵ ~P (∙∣χ(k"k2) log P(X ⑵ |x(1)； θ12).
Therefore, We can first sample X(k) from P(∙∣X(1); θ12), then sample X(2) from P(∙∣X(k); θk2), and
then maximize log P(X⑵ |x(1); θ12). Obviously, -Vθ2 log P(X⑵ |x(1); θ12) is the unbiased esti-
mator of Vθ12Dkl(P(∙∣X(1); θik,θk2)∣∣P(∙∣X⑴；θ12)).
The performance of multipath dual learning is shoWn in Table 6 in comparison With other translators.
We observe that multipath dual learning has a very similar performance With cycle dual learning,
Which is consistent With our theoretical analysis.
13
Under review as a conference paper at ICLR 2019
	En→Fr	Fr→En	En→Es	Es→En	Es→Fr	Fr→Es
Baseline	50.26	50.56	55.15	55.23	47.75	48.13
Dual Learning	50.91	51.42	55.51	55.77	48.23	48.52
Multi-path	51.27	51.74	55.89	56.06	48.34	48.76
Cycle	51.28	51.89	55.97	56.17	48.62	48.87
Table 6: Experimental Results on MultiUN (2M bilingual data)
E Derivation of Cycle Dual Learning Algorithm
We follow the notations used in Appendix D. Let R(x(2)) denote the event that after passing S2 →
Sk → S1 → S2, x(2) is reconstructed to x(2). We have that
log P (R(x(2))) = log P (x(2) |start from x(2); θ2k, θk1, θ12)
= X X log P (x(2), x(1), x(k)|start from x(2); θ2k, θk1, θ12)
x(k) ∈Sk x(1)∈S1
=XXlog P(x(1),x(k) ∣startfrom x(2); θ2k,θki)∙
x(k) ∈Sk x(1)∈S1
P (x(2) |start from x(2), x(1), x(k); θ2k, θk1, θ12)
≥XXP(x(1), x(k) |x(2); θ2k, θk1) log P (x(2)|start from x(2), x(1), x(k); θ2k, θk1, θ12)
x(k) ∈Sk x(1)∈S1
= X X P (x(1), x(k)|x(2); θ2k, θk1) log P (x(2)|x(1); θ12)
x(k) ∈Sk x(1)∈S1
= X X P (x(k)|x(2); θ2k)P (x(1)|x(k); θk1) log P(x(2) |x(1); θ12)
x(k) ∈Sk x(1)∈S1
=Eχ(k)〜P(∙∣x ⑵;θ2k)Eχ(ι)〜P (∙∣χ(k))以ι log P(X ⑵ |x(1)； θ12).
(20)
Then, we can use the algorithm proposed in Algorithm 1 to solve the above optimization problem,
i.e., min - log P(R(x(2))).
F Theoretical Analysis for Mutipath Dual Learning
We define q32) = PrX⑶〜Tʤ(x(i))(Td2(x(3)) = x(2)), which as the accuracy of Td when the
sentence in S3 is translated from a random sentence in S1. This is analogous to reconstruction
accuracy, and is reduced to the reconstruction accuracy when S2 = S1 . To analyze multipath
learning, we consider the following cases (analogous to Case 1 and Case 2 for dual learning and
cycle dual learning): Case 1: T12(x(1)) = T32(T13(x(1))); Case 2: T12(x(1)) 6= T32(T13(x(1))).
multipath dual learning will detect Case 2 and train the translators so that the difference between
translations from the two paths is minimized. To quantify this effect, we define the following prob-
abilities:
α0 = Pr(Tm (X(I) ) = χ(2),τm (Tm (X ⑴))=X ⑵ ICase 2)
β0=Pr(T1m2(x(1)) 6=x(2),T3m2(T1m3(x(1)))=T1m2(x(2))|Case2)
γ0=Pr(T2m1(T1m2(X(1))) 6= X(1)|Case 2),	(21)
where Case 2 denotes the condition T12(X(1)) 6= T32(T13(X(1))) and superscripts dand m denote the
translators obtained from dual learning and multipath learning, respectively. α0 , β0 , γ0 can be viewed
as the probability of correcting the wrong translations by multipath learning, the probability of the
occurrence of the alignment problem under Case 2, and the probability of nonzero reconstruction
error. γ0 models the imperfectness of dual learning. And again, we have α0 + β0 + γ0 = 1. We
14
Under review as a conference paper at ICLR 2019
assume that T1d3 and T3d2 are independent translators before multipath learning. That implies,
Pr(T1d3(x(1)) = x(3), T3d2(T1d3(x(1))) = x(2)) = Pr(T1d3(x(1)) = x(3), T3d2(x(3)) = x(2)) = q13q3(12)
(22)
Then we have the following theorem, which focuses one the triangle structure containing only S1,
S2, and S3 . The more general case can be viewed as adding one path a time so Theorem 3 can be
applied.
Theorem 3. Given languages spaces S1, S2, and S3, where the objective is to train a translator
that maps from S1 to S2, the accuracy of two-path learning (Figure 1) outcome is
pm2 = qi2(1 - α0)(qi3q32) +(1 - q13)(1 -	)
m
0	(1 - q12)(q13 + q3(12) - (m+ 1)q13q3(12) + m - 1)
+ α (1-----------------------------5----------------------
m2
under Assumption 2.
(23)
Proof. Similar to the analysis of Theorem 1, we focus on the mapping from x(1) ∈ S1 to S2 and
consider the following two cases:
Case 1:	T1d2(x(1)) = T3d2(T1d3(x(1)));
Case 2:	T1d2(x(1)) 6=T3d2(T1d3(x(1))).
We first characterize the probability of Case 1.
Case 1: According to whether the translation in S2 is correct, Case 1 can be partitioned into two
subcases:
Case 1.1: T1d2(x(1)) = x(2),T2d3(T1d3(x(1))) = x(2);
Case 1.2: T1d2(x(1)) 6=x(2),T2d3(T1d3(x(1))) = T1d2(x(1)).
Case 1.1. For the path Si 什 S?, We have
Pr(T1d2(x(1)) = x(2)) = q12.
For the path Si 什 S3 什 S2, since We only care about the translation in S2, We have
Pr(T3d2(T1d3(x(1)))=x(2))
=Pr(Tid3(x(i))=x(3))Pr(T3d2(Tid3(x(i)))=x(2)|Tid3(x(i))=x(3))
+Pr(Tid3(x(i)) 6=x(3))Pr(T3d2(Tid3(x(i)))=x(2)|Tid3(x(i)) 6= x(3))
—“ fl(i) , (I - q13)(1 - q32))
=q13q32 +------------m---------
Where the first equality is obtained by the laW of total probability, and the second quality is obtained
by equation 22 and Assumption 2. Then the probability of Case 1.1 is computed as folloWs.
Pr(Td2(x(1)) = 2(2),理(塔(2(I))) = x(2)) = qi2(qi3q32) +(1 - 叱 - q3" )	(24)
Case 1.2. For the Si 什 S2 path we have Pr(T⅛(X(I)) = x(2)) = 1 一 q12. Let y(2) ∈ S2 denote this
incorrect translation. NoW We compute Pr(T3d2(Tid3(x(i))) = y(2)). There are three cases depending
on the translation at S3: x(3), y(3), and others. They are computed as folloWs.
1	(i)
Pr(T32(T13(x ⑴))=y ⑵ ∣T13(x(I)) = x(3)) = ―q2
m
Pr(T3d2(Tid3(x(i))) = y(2)|Tid3(x(i)) = y(3)) = q3(i2)
1	(i)
Pr(T32(T3(χ (I))) = y ⑵ ∣T13(X(I)) = x(3),y(3)) =—皿
m
15
Under review as a conference paper at ICLR 2019
Pr(T3d2(T1d3(x(1))) =y(2))
Using the law of total probability we have
q13(I - q32^	q(2)(I - q^	(m - I)(I - q^(I - q32^
+	+	2
m	m	m2
m(qi3 + q32)- 2qi3q32^ + (m -I)(I - q^(I - q32^
m2
q13 + q32) — (m +I)qi3q32) + m - 1
m2
Then the probability of Case 1.2 is
Pr(T1d2(x(1)) 6=x(2),T2d3(T1d3(x(1)))=T1d2(x(1)))
(1 - qi2)(qi3 + q32) - (m +I)qi3q(2) + m - 1)
m2
(25)
Case 2. The probability of Case 2 is simply the complement of Case 1.
Pr(T1d2(x(1)) 6= T3d2(T1d3(x(1)))) = 1 - Pr(T1d2(x(1)) = T3d2(T1d3(x(1))))
=1 - qi2(qi3q32) + 上返&二曲)-
m
(1 - q12)(q13 + q3(12) - (m + 1)q13q3(12) + m - 1)
m2
(26)
Then the accuracy of this two-path learning is
p1m2 = Pr(T1d2(x(1)) = x(2), T2d3(T1d3(x(1))) = x(2)) + α0 Pr(T1d2(x(1)) 6=T3d2(T1d3(x(1)))), (27)
where α0 is defined in equation 21. equation 23 is obtained by substitute equation 24 and equation 26
into equation 27.	□
The role of T1d2. The accuracy increases as q12 increases. So a good original translator trained
from dual learning is desirable. However, if T1d2 is arbitrarily bad, it is still possible to improve
it significantly. This implies the application of this framework on low-resource or even zero-shot
machine translation.
A similar hypothesis. Similar to the analysis of dual learning, we consider the condition where
α0 = Pr(Td2(x⑴)=x⑶，T⅛ (T⅛(x⑴))=x⑵)
β0 = Pr(Td2(x(1))=x⑶，Td3 (T⅛(x⑴))=x(2))
Then the accuracy simplifies to
and define Γ0
γ0Pr(T1d2(x(1)) 6= T3d2(T1d3(x(1)))).
α0(1- Γ0) = ________________qi2(qi3q32)+(…吗-您))(1- Γ0)________________________
α + β0	q] 2(q]3q(I) + (I-q13)(1-q(2) ) ) + (1-q12)(q13+q(2)—(m+I) q13q(2)+m-I)
1 — Γ0	1 — Γ0
1 + (1-qi2)(qi3 + q(2)-(m+1)qi3q(2) +m-1)	1 + M 1-q12
mq12 (mq13q3(12)+(1-q13)(1-q3(12) ))	12
where M = 孤3+"32)-],+1)"13^2)+：-1. When Y0 = 0 (and therefore Γ0 = 0) and M = 1, it
m(mq13q3(12)+(1-q13)(1-q3(12) ))	,
simplifies to q12, which is the accuracy of dual learning. Observing p1m2 increases as M decreases,
we let M < 1 so that p1m2 > q12 . We have
q13+ q32) —(m + 1)qi3q32)+ m - 1
m(mqi3q32) + (1 — qi3)(1 — q32)))
((m+ 1)q13 - 1)((m + 1)q3(12) - 1)
<1
>0
when q13, q32) > mɪpɪ, which means the probability of correctly translating a sentence is greater
than choosing from potentially wrong translations uniformly at random, the two-path learning out-
performs dual learning.
16