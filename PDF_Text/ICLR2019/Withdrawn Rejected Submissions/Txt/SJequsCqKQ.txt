Under review as a conference paper at ICLR 2019
Cautious Deep Learning
Anonymous authors
Paper under double-blind review
Ab stract
Most classifiers operate by selecting the maximum of an estimate of the conditional
distribution p(y|x) where x stands for the features of the instance to be classified
and y denotes its label. This often results in a hubristic bias: overconfidence in
the assignment of a definite label. Usually, the observations are concentrated on a
small volume but the classifier provides definite predictions for the entire space.
We propose constructing conformal prediction setsVovk et al. (2005) which contain
a set of labels rather than a single label. These conformal prediction sets contain
the true label with probability 1 - α. Our construction is based on p(x|y) rather
than p(y|x) which results in a classifier that is very cautious: it outputs the null
set — meaning “I don’t know” — when the object does not resemble the training
examples. An important property of our approach is that classes can be added or
removed without having to retrain the classifier. We demonstrate the performance
on the ImageNet ILSVRC dataset and the CelebA and IMDB-Wiki facial datasets
using high dimensional features obtained from state of the art convolutional neural
networks.
1	Introduction
We consider multiclass classification with a feature space X and labels Y = {1, . . . , k} . Given the
training data (X1, Y1), . . . , (Xn, Yn), the usual goal is to find a prediction function Fb : X 7-→ Y
with low classification error P(Y 6= F(X)) where (X, Y ) is a new observation of an input-output
pair. This type of prediction produces a definite prediction even for cases that are hard to classify.
In this paper we use conformal predictionVovk et al. (2005) where we estimate a set-valued function
C : X 7-→ 2Y with the guarantee that P(Y ∈ C (X)) ≥ 1 - α for all distributions P. This is a
distribution-free confidence guarantee. Here, 1 - α is a user-specified confidence level. We note that
the “classify with a reject option”Herbei & Wegkamp (2006) also allows set-valued predictions but
does not give a confidence guarantee.
The function C can sometimes output the null Set That is, C(X) = 0 for some values of x. This
allows us to distinguish two types of uncertainty. When C(x) is a large set, there are many possible
labels consistent with x. But when x does not resemble the training data, we will get C(x) = 0
alerting us that we have not seen examples like this so far.
There are many ways to construct conformal prediction sets. Our construction is based on finding an
estimate pb(x|y) of p(x|y). We then find an appropriate scalar bty and we set C(x) = {y : pb(x|y) >
bty}. The scalars are chosen so that P (Y ∈ C(X)) ≥ 1 - α. We shall see that this construction
works well when there is a large number of classes as is often the case in deep learning classification
problems. This guarantees that x’s with low probability — that is regions where we have not seen
training data — get classified as 0.
An important property of this approach is that pb(x|y) can be estimated independently for each class.
Therefore, x is predicted to a given class in a standalone fashion which enables adding or removing
classes without the need to retrain the whole classifier. In addition, we empirically demonstrate
that the method we propose is applicable to large-scale high-dimensional data by applying it to the
ImageNet ILSVRC dataset and the CelebA and IMDB-Wiki facial datasets using features obtained
from state of the art convolutional neural networks.
Paper Outline. In section 2 we discuss the difference between p(y|x) andp(x|y). In section 3 we
provide an example to enlighten our motivation. In section 4 we present the general framework of
1
Under review as a conference paper at ICLR 2019
conformal prediction and survey relevant works in the field. In section 5 we formally present our
method. In section 6 we demonstrate the performance of the proposed classifier on the ImageNet
challenge dataset using state of the convolutional neural networks. In section 7 we consider the
problem of gender classification from facial pictures and show that even when current classifiers
fails to generalize from CelebA dataset to IMDB-Wiki dataset, the proposed classifier still provides
sensible results. Section 8 contains our discussion and concluding remarks. The Appendix in the
supplementary material contains some technical details.
Related Work. There is an enormous literature on set-valued prediction. Here we only mention some
of the most relevant references. The idea of conformal prediction originates from Vovk et al. (2005).
There is a large followup literature due to Vovk and his colleagues which we highly recommend for
the interested readers. Statistical theory for conformal methods was developed in Lei (2014); Lei
et al. (2013); Lei & Wasserman (2014), and the multiclass case was studied in Sadinle et al. (2017)
where the goal was to develop small prediction sets based on estimating p(y|x). The authors of that
paper tried to avoid outputting null sets. In this paper, we use this as a feature, similarly to Vovk et al.
(2003). Finally, we mention a related but different technique called classification with the “reject
option”Herbei & Wegkamp (2006). This approach permits one to sometimes refrain from providing a
classification but it does not aim to give confidence guarantees.
2	p(y|x) VERSUS p(x|y)
Most classifiers — including most conformal classifiers — are built by estimating p(y|x). Typically
one sets the predicted label of a new x to be f (x) = arg maxy∈Y {pb(y|x)}. Since p(y|x) =
p(x|y)p(y)/p(x) the prediction involves the balance between p(y) and p(x|y). Of course, in the
special casep(y) = 1/k for all y, we have arg maxy∈γ{p(y∣x)} = arg maxy∈γ{p(x∣y)}.
However, for set-valued classification, p(y|x) can be negatively affected by p(y) and p(x|y). Indeed,
in this case there are significant advantages to using p(x|y) to construct the classifier. Taking p(y)
into account ties the prediction of an observation x with the likelihood of observing that class. Since
there is no restriction on the number of classes, ultimately an observation should be predicted to a
class regardless of the class popularity. Normalizing by p(x) makes the classifier oblivious to the
probability of actually observing x. When p(x) is extremely low (an outlier), p(y|x) still selects the
most likely label out of all tail events. In practice this may result with most of the space classified
with high probability to a handful of classes almost arbitrarily despite the fact that the classifier
has been presented with virtually no information in those areas of the space. This approach might
be necessary if a single class has to be selected ∀x ∈ X . However, if this is not the case, then a
reasonable prediction for an x with small p(x) is the null set.
There are also conformal methods utilizing p(y|x) to predict a set of classes (Sadinle et al., 2017;
Vovk et al., 2003). This methods does not overcome the inherent weakness withinp(y|x). As will
be explained later on, the essence of this methods is to classify x to C (x) = {y | P (y | x) ≥ t} for
some threshold t. Due to the nature ofp(y|x) the points which are most likely to be predicted as the
null set are when P(y = j|x) = 1, for all classes j ∈ Y. But this is exactly the points in space for
which any set valued prediction should predict all class as possible.
As we shall see, conformal predictors based on p(x|y) can overcome all these issues.
3	M otivating Example - Iris Dataset
The Iris flower data set is a benchmark dataset often used to demonstrate classification methods.
It contains four features that were measured from three different Iris species. In this example, for
visualization purposes, we only use two features: the sepal and petal lengths in cm.
Figure 1 shows the decision boundaries for this problem comparing the results of (a) K-nearest
neighbors (KNN), (b) support vector machines with the RBF kernel (SVM) and (c) our conformal
prediction method using an estimate pb(x|y).
Both the KNN and the SVM methods provide sensible boundaries between the class where there are
observations. In areas with low density p(x) the decision boundaries are significantly different. The
SVM classifies almost all of the space to a single class. The KNN creates an infinite strip bounded
2
Under review as a conference paper at ICLR 2019
(a)	(b)	(c)
Figure 1: Classification boundaries for different methods for the Iris dataset. For the conformal
prediction method (c) (with α = 0.05) the overlapping areas are classified as multiple classes and
white areas are classified as the null set. For the standard methods (a-b), the decision boundaries
can change significantly with small changes in some of the data points and the prediction cannot be
justified in most of the space. Online version in color.
between two (almost affine) half spaces. In a hubristic manner, both methods provide very different
predictions with probability near one without sound justification.
The third plot shows the conformal set	C(x)	=	{y	:	pb(x|y)	>	tby}	where the	bty	is chosen as
described in Section 5. The result is a cautious prediction. If a new X falls into a region with little
training data then We output 0. In such cases our proposed method modestly avoids providing any
claim.
4	Conformal Prediction
Let (X1, Y1), . . . , (Xn, Yn) be n independent and identically distributed (iid) pairs of observations
from a distribution P. In set-valued supervised prediction, the goal is to find a set-valued function
C(x) such that
P(Y ∈ C(X)) ≥ 1 - α,	(1)
Where (X, Y ) denotes a neW pair of observations.
Conformal prediction — a method created by Vovk and collaborators Vovk et al. (2005) — provides
a general approach to construct prediction sets based on the observed data Without any distributional
assumptions. The main idea is to construct a conformal score, Which is a real-valued, permutation-
invariant function ψ(z, D) Where z = (x, y) and D denotes the training data. Next We form
an augmented dataset D0 = {(X1,Y1), . . . , (Xn,Yn), (Xn+1,Yn+1)} Where (Xn+1,Yn+1) is set
equal to arbitrary values (x, y). We then define Ri = ψ((Xi, Yi), D0) for i = 1, . . . , n + 1. We
test the hypothesis H0 : Y = y that the neW label Y is equal to y using the p-value π(x, y) =
1/(n + 1) Pin=+11 I(Ri ≥ Rn+1). Then We set C(x) = {y : π(x, y) ≥ α}. Vovk et al. (2005) proves
that P (Y ∈ C(X)) ≥ 1 - α for all distributions P . There is a great flexibility in the choice of
conformity score and 4.1 discusses important examples.
As described above, it is computationally expensive to construct C(x) since We must re-compute the
entire set of conformal scores for each choice of (x, y). This is especially a problem in deep learning
applications Where training is usually expensive. One possibility for overcoming the computational
burden is based on data splitting Where pb(x|y) is estimated from part of the data and the conformal
scores are estimated from the remaining data; see Vovk (2015); Lei & Wasserman (2014). Another
approach is to construct the scores from the original data Without augmentation. In this case, We no
longer have the finite sample guarantee P(Y ∈ C(X)) ≥ 1 - α for all distributions P, but We do get
the asymptotic guarantee P(Y ∈ C(X)) ≥ 1 - α - oP (1) as long as some conditions are satisfied1.
See Sadinle et al. (2017) for further discussion on this point.
4.1	Examples
Here are several knoWn examples for conformal methods used on different problems.
1A sequence of random variables X1, X2, . . . , is op (1) if∀ > 0, limn→∞ P (|Xn| ≥ ) = 0.
3
Under review as a conference paper at ICLR 2019
Supervised Regression. Suppose we are interested in the supervised regression problem. Let
f : X → Y be any regression function learned from training data. Let i denote the residual
error of f on the observation i, that is, i = |f (Xi) - Yi|. Now we form the ordered residuals
€(i)≤ ∙∙∙ ≤ e(n), and then define
C(X) = {y ： |F(X)- y| ≤ e(d(1-α)∙n])}.
If f is a consistent estimator of E[Y |X = X] then P(Y ∈ C(X)) = 1 - α + oP (1). See Lei and
Wasserman Lei & Wasserman (2014).
Unsupervised Prediction. Suppose we observe independent and identically distributed Yi , . . . , Yn ∈
Rd from distribution P. The goal is to construct a prediction set C for new Y . Lei, Robins and
Wasserman Lei et al. (2013) use the level set C = {y: pb(y) > t} where pb is a kernel density
estimator. They show that if t is chosen carefully then P(Y ∈ C) ≥ 1 - α for all P.
Multiclass Classification. There are two notable solutions also using conformal prediction for the
multiclass classification problem which are directly relevant to this work.
Least Ambiguous Set-Valued Classifiers with Bounded Error Levels. Sadinle et al Sadinle et al.
(2017) extended the results of Lei Lei (2014) and defined Ri = pb(Yi | Xi), where pbis any consistent
estimator of p(y|X). They defined the minimal ambiguity as A (C) = E (|C (X)|) which is the
expected size of the prediction set. They proved that out of all the classifiers achieving the desired
1 - α coverage, this solution minimizes the ambiguity. In addition, the paper considers class specific
coverage controlling for every class P (Y ∈ C (X) | Y = y) ≥ 1 - αy.
Universal Predictor. Vovk et al Vovk et al. (2003)Sadinle et al. (2017) introduce the concept of
universal predictor and provide an explicit way to construct one. A universal predictor is the classifier
that produces, asymptotically, no more multiple prediction than any other classifier achieving 1 - α
level coverage. In addition, within the family of all 1 - α classifiers that produce the minimal number
of multiple predictions it also asymptotically obtains at least as many null predictions.
5	The Method
5.1	The Classifier
Let pb(X|y) be an estimate of the density p(X|y) for class Y = y. Define bty to be the empirical 1 - α
quantile of the values {pb(Xi|y)}. That is,
b
ty
sup y:
-l XI(p(Xi∣y) ≥ t) ≥ 1 -
(2)
α
where y = i I(Yi = y). Assuming that y → ∞ and minimal conditions on p(X|y) andpb(X|y),
it can be shown that bty →P ty where ty is the largest t such that Ry>t p(X|y)dX ≥ 1 - α. See Cadre
et al. (2009) and Lei et al. (2013). We set C(X) = {y: pb(X|y) ≥ tby}. We then have the following
proposition which is proved in the appendix.
Proposition 1 Assume the conditions in Cadre et al. (2009) stated also in the appendix. Let (X, Y )
be a new observation. Then |P(Y ∈ C(X)) 一 (1 一 α)∣ -→ 0 as min# —y → ∞.
An exact, finite sample method can be obtained using data splitting. We split the training data into
two parts. Construct pb(X|y) from the first part of the data. Now evaluate {pb(Xi|y)} on the second
part of the data and define bty using these values. We then set C(X) = {y: pb(X|y) ≥ bty}. We then
have:
Proposition 2 Let (X, Y) be a new observation. Then, for every distribution and every sample size,
P(Y∈ C(X)) ≥ 1 -α.
This follows from the theory in Lei & Wasserman (2014). The advantage of the splitting approach
is that there are no conditions on the distribution, and the confidence guarantee is finite sample.
There is no large sample approximation. The disadvantage is that the data splitting can lead to larger
prediction sets. Algorithm 1 describes the training, and Algorithm 2 describes the prediction.
4
Under review as a conference paper at ICLR 2019
Algorithm 1 Training Algorithm
Input: Training data Z = (X, Y ), Class list Y, Confidence level α, Ratio p.
pblist = list; tlist = list	. Initialize lists
for y in Y do	. Loop over all the classes independently
Xtr, Xval J SubsetData (Z, Y,p)	. Split X | y with ratio P
by J LearnDensityEstimatOr (Xtr)
bty J Quantile (pby (Xvyal) , α)	. The validation set α quantile
pblist.append (pby ) ;btlist.append bty
return pblist ; tblist
Algorithm 2 Prediction Algorithm
Input: Input to be predicted x, Trained pblist ; tlist, Class list Y.
C = list	. Initialize C (x)
for y in Y do	. Loop over all the classes independently
if pby (x) ≥ bty then
C.append (y)
return C
5.2	Class Adaptivity
As algorithms 1 and 2 demonstrate, the training and prediction of each class is independent from all
other classes. This makes the method adaptive to addition and removal of classes ad-hoc. Intuitively
speaking, if there is 1 - α probability for the observation to be generated from the class it will be
classified to the class regardless of any other information.
Another desirable property of the method is that it is possible to obtain different coverage levels per
class if the task requires that. This is achieved by setting ty to be the 1 - αy quantile of the values
{pb(Xi|y)}.
5.3	Density Estimation
The density p(x|y) has to be estimated from data. We use the standard kernel density estimation
method, which was shown to be optimal in the conformal setting under weak conditions in Lei,
Robins and Wasserman Lei et al. (2013).
Density estimation in high dimensions is a difficult problem. Nonetheless, as we will show in the
numerical experiments (Section 6), the proposed method works well in these tasks as well. An intuitive
reason for this could be that the accuracy of the conformal prediction does not actually require pb(x|y )
to be close to p(x|y) in L2. Rather, all we need is that the ordering imposed by pb(x|y) approximates
the ordering defined by p(x|y). Specifically, we only need that {(x, x0) : p(x|y) > p(x0|y) + ∆} is
approximated by {(x, x0) : pb(x|y) > pb(x0|y) + ∆} for ∆ > 0. We call this “ordering consistency.”
This is much weaker than the usual requirement that (pb(x|y) - p(x|y))2dx be small. This new
definition and implications on the approximation ofp (x | y) will be further expanded in future work.
6	ImageNet Challenge Example
The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) Deng et al. (2009) is a large
visual dataset of more than 1.2 million images labeled across 1, 000 different classes. It is considered
a large scale complex visual dataset that reflects object recognition state-of-the-art through a yearly
competition.
In this example we apply our conformal image classification method to the ImageNet dataset. We
remove the last layer from the pretrained Xception convolutional neural network Chollet (2016) and
use it as a feature extractor. Each image is represented as a 2, 048 dimensional feature in R2048 .
We learn for each of the 1, 000 classes a unique kernel density estimator trained only on images
within the training set of the given class. When we evaluate results of standard methods we use the
5
Under review as a conference paper at ICLR 2019
(a)	(b)	(c)
Figure 2: (a) Performance plot for the conformal method. Accuracy is empirically linear as a function
of α but affect the number of classes predicted per sample. (b-c) are illustrative examples. When
α = 0.6 both black and red classes are predicted. When α = 0.8 the red classes remain.
Inception-v4 model Szegedy et al. (2017) to avoid correlation between the feature extractor and the
prediction outcome as much as possible.
The Xception model obtains near state-of-the-art results of 0.79 (top-1) and 0.945 (top-5) accuracy
on ImageNet validation set. As a sanity check to the performance of our method, selecting for each
image the highest (and top 5) prediction of pb(x | y) achieves 0.721 (top-1) and 0.863 (top-5) on
ImageNet validation set. We were pleasantly surprised by this result. Each of the pb(x | y)’s were
learned independently possibly discarding relevant information on the relation between the classes.
The kernel density estimation is done in R2,048 and the default bandwidth levels were used to avoid
overfitting the training set. Yet the naive performance is roughly on par with GoogLeNet Szegedy
et al. (2015) the winners of 2014 challenge (top-1: 0.687, top-5: 0.889).
For conformal methods the confidence level is predefined. The method calibrates the number of
classes in the prediction sets to satisfy the desired accuracy level. The the main component affecting
the results is the hyperparameter α. For small values of α the accuracy will be high but so does
the number of classes predicted for every observation. For large values of α more observations are
predicted as the null set and less observations predicted per class. Figure 2 (a) presents the trade-off
between the α level and the number of classes and the proportion of null set predictions for this
example. For example 0.5, accuracy would require on average 2.7 predictions per observation and
0.252 null set predictions. The actual selection of the proper α value is highly dependent on the task.
As discussed earlier, a separate αy for each class can also be used to obtain different accuracy per
class.
Figures 2 (b) and (c) show illustrative results from the ImageNet validation set. (b) presents a picture
of a "Barber Shop". When α = 0.6 the method correctly suggests the right class in addition to several
other relevant outcomes such as "Bakery". When α = 0.8 only the "Barber Shop" remains. (c) show a
"Brain Coral". For α = 0.6 the method still suggests classes which are clearly wrong. As α increases
the number of classes decrease and for α = 0.8 only "Brain Coral" and "Coral Reef " remains, both
which are relevant. At α = 0.9 "Coral Reef " remains, which represents a misclassification following
from the fact that the class threshold is lower than that of "Brain Coral". Eventually at α = 0.95 the
null set is predicted for this picture.
Figure 5 shows a collage of 20 images using α = 0.7. To avoid selection bias we’ve selected the first
20 images in the ImageNet validation set.
6.1	Outliers
Figure 3 (a) shows the outcome when the input is random noise. We set the threshold α = 0.01. This
gives a less conservative classifier that should have the largest amount of false positives. Even with
such a low threshold all 100 random noise images over 1, 000 categories are correctly flagged as
6
Under review as a conference paper at ICLR 2019
OUr Method (α=.5)
•	Null Set
IncePtion-v4 Model:
•	Kite (0.137)
•	BeeEater (0.033)
•	Missle (0.031)
(a)
OUr Method (α=.5):
• Null Set
IncePtion-v4 Model:
•	Coil (0.910)
•	Hay (0.008)
• Maze (0.005)
(b)
OUr Method (α=.55): • Null Set	IncePtion-v4 Model: •	Volleyball (0.388) •	Tennis Ball (0.160) •	Racket (0.157)
(c)
Figure 3: Classification results for (a) random noise; (b) Jackson Pollock "Rabit Hole"; (c) Muhammad
Ali towering over Sonny Liston (1965 rematch). These pictures are outliers for the Imagenet
categories. The left labels of each picture are provided by our method and the right are the results of
the Inception-v4 model Szegedy et al. (2017).
the null set. Evaluating the same sample on the Inception-v4 model Szegedy et al. (2017) results
with a top prediction average of 0.0836 (with 0.028 standard error) to "Kite" and 0.0314 (0.009) to
"Envelope". The top-5 classes together has mean probability of 0.196, much higher than the uniform
distribution expected for prediction of random noise.
Figure 3 (b) show results on Jackson Pollock paintings - an abstract yet more structured dataset.
Testing 11 different paintings with α = 0.5 all result with the null set. When testing the Inception-v4
model output, 7/11 paintings are classified with probability greater than 0.5 to either "Coil", "Ant",
"Poncho", "Spider Web" and "Rapeseed" depending on the image.
Figure 3 (c) is the famous picture of Muhammad Ali knocking out Sonny Liston during the first
round of the 1965 rematch. "Boxing" is not included within in the ImageNet challenge. Our method
correctly chooses the null set with α as low as 0.55. Standard method are forced to associate this
image with one of the classes and choose "Volleyball" with 0.38 probability and the top-5 are all
sport related predictions with 0.781 probability. This is good result given the constraint of selecting a
single class, but demonstrate the impossibility of trying to create classes for all topics.
7	Gender Recognition Example
In the next example we study the problem of gender classification from facial pictures. CelebFaces
Attributes Dataset (CelebA) Liu et al. (2015) is a large-scale face attributes dataset with more
than 200K celebrity images attributed, each with 40 attribute annotations including the gender
(Male/Female). IMDB-Wiki dataset is a similar large scale (500K + images) dataset Rothe et al.
(2016) with images taken from IMDB and Wikipedia.
We train a standard convolutional neural network (5 convolution and 2 dense layers with the corre-
sponding pooling and activation layers) to perform gender classification on CelebA. It converges
well obtaining 0.963 accuracy on a held out test set, but fails to generalize to the IMDB-Wiki dataset
achieving 0.577 accuracy, slightly better than a random guess. The discrepancy between the two
datasets follows from the fact that facial images are reliant on preprocessing to standardize the input.
We have used the default preprocessing provided by the datasets, to reflect a scenario in which the
distribution of the samples changes between the training and the testing. Figure 4 (a) and (b) show
mean pixel values for females pictures within CelebA vs pictures in the IMDB-Wiki dataset. As seen,
the IMDB-Wiki is richer and offers larger variety of human postures.
Although the standard classification method fails in this scenario, the conformal method suggested
in this paper still offers valid and sensible results both on CelebA and IMDB-Wiki when using the
features extracted from the network trained on CelebA. Figure 4 (c) shows the performance of the
method with respect to both dataset. CelebA results are good since they are based on features that
7
Under review as a conference paper at ICLR 2019
(a)	(b)	(c)
Figure 4: (a) Females faces mean pixel values in (a) CelebA; (b) IMDB-Wiki. Within CelebA the
pictures are aliened with fixed posture, explaining why it naively fails to generalize to IMDB-Wiki
images. (c) Performance plots for the conformal method on both CelebA and IMDB-Wiki.
perform well for this dataset. The level of accuracy is roughly 1 - α as expected by the design, while
the proportion of null predictions is roughly α. Therefore for all α there are almost no false positives
and all of the errors are the null set.
The IMDB-Wiki results are not as good, but better than naively using a 0.577 accuracy classifier.
Figure 4 (c) show the classifier performance as a function of α. Both the accuracy and the number of
false positives are tunable. For high values of 1 -α the accuracy is much higher than 0.577, but would
results in a large number of observations predicted as both genders. If cautious and conservative
prediction is required small values of 1 - α would guarantee smaller number of false predictions,
but a large number of null predictions. The suggested conformal method provide a hyper-parameter
controlling which type of errors are created according to the prediction needs, and works even in
cases where standard methods fail.
8	Discussion
In this paper we showed that conformal, set-valued predictors based on pb(x|y) have very good
properties. We obtain a cautious prediction associating an observation with a class only if the there is
high probability of that observation is generated from the class. In most of the space the classifier
predicts the null set. This stands in contrast to standard solutions which provide confident predictions
for the entire space based on data observed from a small area. This can be useful when a large number
of outliers are expected or in which the distribution of the training data won’t fully describe the
distribution of the observations when deployed. Examples of such, are object recognition systems
used online continuously in real life. We also obtain a large set of labels in the set when the object is
ambiguous and is consistent with many different classes. Thus, our method quantifies two types of
uncertainty: ambiguity with respect to the given classes and outlyingness with respect to the given
classes.
In addition, the conformal framework provides our method with its coverage guarantees and class
adaptivity. Itis straightforward to add and remove classes at any stage of the process while controlling
either the overall or class specific coverage level of the method in a highly flexible manner. This de-
sired properties comes with a price. The distribution of p(x|y) for each class is learned independently
and the decision boundaries are indifferent to data not within the class. This might lead to decision
boundaries which are inferior to current methods if the only goal is to distinguish between classes
which are fully described by the training data. Alternative methods to learn p(x|y) taking all the
training data into account might overcome this limitation and can be the focus of future investigations.
During the deployment of the method, evaluation of a large number of kernel density estimators is
required. This is relatively slow compared to current methods. This issue can be addressed in future
research with more efficient ways to learn ordering-consistent approximations of p(x|y) that can be
deployed on GPU’s.
8
TL: Sea Snake
Prediction: Null Set
TL: Alp
Prediction: Ski
TL: Cradle
Prediction: Sleeping
Bag
TL: Garter Snake
Prediction: Null Set
TL: Shetland Sheepdog
Prediction: Shetland
Sheepdog, Collie, Toilet
Paper
TL: Soup Bowl
Prediction: Face
Powder, Soup Bowl,
Tray
TL: Bakery
Prediction: Null Set
TL: Porcupine
Prediction:
Porcupine, Quill
TL: Mousetrap
Prediction: Mousetrap
TL: Angora
Prediction: Null Set
TL: Cougar
Prediction: Cougar
TL: Recreational Vehicle
Prediction: Recreational
Vehicle
TL: Brain Coral
Prediction: Brain Coral,
Water Bottle, Coral
Reef
TL: Guenon
Prediction: Guenon,
Patas
TL: Harvester
Prediction: Null Set
TL: Grey Whale
Prediction: Null Set
TL: Sea Anemone
Prediction: Null Set
TL: Vulture
Prediction: Null Set
TL: Carton
Prediction: Null Set
TL: Crane
Prediction: Crane,
Hook
Figure 5: A collage of the first 20 images in the ImageNet validation set with α = 0.7. TL denotes
the image true label and Prediction is the method output. By design only 0.3 accuracy is expected,
yet both the true and the false predictions are reasonable. Online version in color.
9
Under review as a conference paper at ICLR 2019
References
Benoit Cadre, BrUno Pelletier, and Pierre Pudlo. Clustering by estimation of density level sets at a
fixed probability. 2009.
FrangoiS Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint,
2016.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009.
IEEE Conference on,pp. 248-255. IEEE, 2009.
Radu Herbei and Marten H Wegkamp. Classification with reject option. Canadian Journal of
Statistics, 34(4):709-721, 2006.
Jing Lei. Classification with confidence. Biometrika, 101(4):755-769, 2014.
Jing Lei and Larry Wasserman. Distribution-free prediction bands for non-parametric regression.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 76(1):71-96, 2014.
Jing Lei, James Robins, and Larry Wasserman. Distribution-free prediction sets. Journal of the
American Statistical Association, 108(501):278-287, 2013.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
Proceedings of the IEEE International Conference on Computer Vision, pp. 3730-3738, 2015.
Rasmus Rothe, Radu Timofte, and Luc Van Gool. Deep expectation of real and apparent age from
a single image without facial landmarks. International Journal of Computer Vision (IJCV), July
2016.
Mauricio Sadinle, Jing Lei, and Larry Wasserman. Least ambiguous set-valued classifiers with
bounded error levels. Journal of the American Statistical Association, (just-accepted), 2017.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru
Erhan, Vincent Vanhoucke, Andrew Rabinovich, et al. Going deeper with convolutions. Cvpr,
2015.
Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi. Inception-v4, inception-
resnet and the impact of residual connections on learning. In AAAI, volume 4, pp. 12, 2017.
Vladimir Vovk. Cross-conformal predictors. Annals of Mathematics and Artificial Intelligence, 74
(1-2):9-28, 2015.
Vladimir Vovk, David Lindsay, Ilia Nouretdinov, and Alex Gammerman. Mondrian confidence
machine. Technical Report, 2003.
Vladimir Vovk, Alex Gammerman, and Glenn Shafer. Algorithmic learning in a random world.
Springer Science & Business Media, 2005.
10
Under review as a conference paper at ICLR 2019
Appendix: Details on Proposition 1
Here we provide more details on Proposition 1. We assume that the conditions in Cadre et al. (2009)
hold. In particular, we assume that nhdy /(log n)16 → ∞ and nhyd+4(log n)2 → 0 where hy is the
bandwidth of the density estimator. In addition we assume that X is compact and that miny ny → ∞
where ny = P I(Yi = y).
Let C(x)	=	{y	:	p(x|y)	>	ty}	and C(x)	=	{y	:	pb(x|y)	>	ty}. Note that, conditional on the
training data D,
P(Y ∈
Cb(X)) = X
y
τf 一 A/ ∖∖ ( I ∖7
I(y ∈ C (x))p(x|y)dx
X
y
τf 一 A / ∖ ∖ ^/ I ∖ 7	.
I(y ∈ C (x))pb(x|y)dx +
X
y
Tf 一	^/ I MJ
I(y ∈ C(x))[p(x|y) - pb(x|y)]dx.
From Theorem 2.3 of Cadre et al. (2009) We have that μ({p(x∣y) ≥ t}∆{p(x∣y) ≥ b))=
OP(J1∕(nyhd)) = op⑴ where μ is LebesgUe measure and ∆ denotes the set difference. It
folloWs that
I(y
Cb(x))p(x|y )dx =
I(y ∈ C(x))p(x|y)dx + oP (1)
1 - α + oP (1).
∈
Also,
X
y
7-/ 一 A/	∖∖「/ I ∖	^/ I ∖17
I(y ∈ C(x))[p(x|y) - pb(x|y)]dx
≤X
y
Tl 一 A/ ∖∖l / I ∖	^/ I ʌ I J
I(y ∈ C(x))|p(x|y) - pb(x|y)|dx
y
≤ kmax ∣∣b(χ∣y) -p(χ∣y)∣∣∞ → 0
♦	1	.1	^/ I ∖ ∙	∙	.	. ∙ .t /1	τ. i' 11	.t	丁、/，广 _ A /"∖∖
since, under the conditions, p(x∣y) is consistent in the '∞ norm. It follows that P (Y ∈ C (X))=
1 - α + oP (1) as required.
We should remark that, in the above, we assumed that the number of classes is fixed. If we allow
k to grow the analysis has to change. Summing the errors in the expression above we have that
P(Y ∈ C(X)) = 1 - α + R where now the remainder is
R=O
that R →P 0. However, this
We then need assume that as k increases, the ny grow fast enough so
condition can be weakened by insisting that for all y with ny small, we
force Cb to omit y. If this is
done carefully, then the coverage condition can be preserved and we only need R to be small when
summing over the larger classes. The details of the theory in this case will be reported in future work.
11