Under review as a conference paper at ICLR 2019
Learning Gibbs-regularized GANs with varia-
TIONAL DISCRIMINATOR REPARAMETERIZATION
Anonymous authors
Paper under double-blind review
Ab stract
We propose a novel approach to regularizing generative adversarial networks
(GANs) leveraging learned structured Gibbs distributions. Our method consists of
reparameterizing the discriminator to be an explicit function of two densities: the
generator PDF q and a structured Gibbs distribution ν. Leveraging recent work on
invertible pushforward density estimators, this reparameterization is made possible
by assuming the generator is invertible, which enables the analytic evaluation of
the generator PDF q. We further propose optimizing the Jeffrey divergence, which
balances mode coverage with sample quality. The combination of this loss and
reparameterization allows us to effectively regularize the generator by imposing
structure from domain knowledge on ν, as in classical graphical models. Applying
our method to a vehicle trajectory forecasting task, we observe that we are able
to obtain quantitatively superior mode coverage as well as better-quality samples
compared to traditional methods.
1 Introduction
Although recent progress in GANs and variational methods have significantly advanced the capabili-
ties of generative models for high-dimensional data, many issues still limit the practical application
of such methods. In practice, GANs typically suffer from mode loss, whereas VAEs suffer from poor
sample quality compared to GANs [40, 14, 3]. Several factors may be identified as contributing to
these issues: first, the training loss may optimize for sample quality at the expense of mode coverage
or vice-versa; second, the variance of stochastic gradient estimates may be too high to admit efficient
stochastic optimization; finally, the models may not admit a good regularization scheme via the
imposition of appropriate inductive biases.
We advocate a novel approach to address these concerns, with a particular focus on the latter issue of
regularization. Let p : RN → R+ denote the PDF of a continuous data distribution and q : RN → R+
denote the PDF of a learned model. Following recent work [31], we advocate training a generative
model to minimize the Jeffrey (symmetric KL) divergence minq KL(p, q) + KL(q, p), representing q
in a way that enables it to be efficiently evaluated at any point (i.e., by representing q as a pushforward
distribution induced by an invertible warp [29, 8, 18]). In this work, we propose the key innovation
of applying Fenchel-duality-based variational inference to KL(q, p), which allows the latter to be
optimized without having to explicitly evaluate p. This yields the following variational approximation
of the Jeffrey divergence, which constitutes the training loss for our method:
.E 1	p(X)	E	q(X)	E 1	q(x)
min Ex〜P log / ʌʌ + SUP -E^〜P / ʌʌ + Ex〜q log ,,,
q∈Q	q(x)	ν>0	V (x)	V (x)
(1)
where the variational parameters consist of the function V. We now assert and later elaborate the
following properties of the Jeffrey divergence and its variational approximation (1): first, KL(p, q)
essentially prevents mode loss, whereas KL(q, p) prevents the generation of any samples unlikely
under the data [6, 14]; second, an exactly unbiased stochastic gradient of KL(p, q) may be obtained
without variational inference; finally, the optimal value of V is p. The second key innovation of our
work is to recognize that—in contrast to traditional GANs, which admit no comparable regularization
principle—this model may be effectively regularized by imposing any domain-specific structure
possessed byp on V, which we thence interpret as a structured Gibbs distribution.
These concepts are illustrated in Fig. 1. Figure 1a shows the result of training a model q to optimize
KL(p, q), where q is represented as the pushforward of a Gaussian base distribution under an
1
Under review as a conference paper at ICLR 2019
Baseline: miɪIqKL(P，q)
Red ∙g generated (q)
Green ∙: data (p)
Blue 一: GibbS dist. (v)
CE(p, q) = 4.3
OUr method: ming SUPiy KL⅛, q) - E/~p据 + ESq log 疆
CE(p, q) = 4.9	CE(p, q) = 4.3	CE(p, q) = 4.9
(C)	(d)
(α)	⑹
Figure 1: Different trained models for a 2D toy problem. The data distribution (samples shown as
green dots) consists of a mixture of isotropic Gaussian distributions, arranged in a square grid. Four
different trained models (generator samples shown as red dots) are shown: (a) result of training a
model to optimize KL(p,q), (b-d) result of training models with our method (fine-tuning result of
(a)), varying the form of the structured Gibbs distribution ν . Blue lines show contours of learned
structured GibbS distributions V. In (b-c), the learned parameters of V are μ0, σ0, and β
Figure 2: Our method applied to forecast ego-vehicle trajectories, showing input image and overhead
views with the learned cost - log(V) (overlaid on LIDAR map). Middle shows samples from current
q (red), and true future path (cyan). Note that V has learned to penalize regions with obstacles
(V (x) = log V(x), i.e. higher V (x) is lower traversal cost). Right: after incorporating the learned V,
q and its samples are shifted to avoid high-cost regions, corresponding to suppression of spurious
modes.
autoregressive warp in two dimensions [29, 8, 18]. We observe that the generated samples effectively
cover all the data modes, since a huge penalty is incurred if q(x) is low for any data point X 〜p;
however, the generator additionally places mass outside the support of the data distribution p, because
shifting half of q’s mass onto the support ofp decreases KL(p, q) by no more than log 2; therefore,
this objective effectively under-constrains q . We can resolve the ambiguity by adding the variationally-
approximated KL(q, p) term, which trains log V to approximate log p while penalizing the generation
of samples where log V is low, as shown in Fig. 1b-d. By choosing different forms for log V, we see
that the ambiguity can be resolved in different ways: in Fig. 1b, log V is chosen to have contours
roughly matching the support of p, whereas Fig. 1c-d show the cases where log V is represented as
quadratic and as the output of a multi-layer perceptron, respectively. We see that matching the shape
of log V to the shape of p produces qualitatively good samples, whereas putting too little constraint
on the shape of log V (as in Fig. 1d) yields qualitatively worse results. Estimated cross-entropy values
are also shown for each model, which again demonstrates how KL(p, q) is largely insensitive to the
overall shape of the model q, as long as q covers the modes of p.
Our method may also be viewed as incorporating a kind of f-GAN [26] optimizing a particular
loss (Jeffrey divergence), using a (invertible) generator that admits exact PDF evaluation, and
reparameterizing the discriminator in a certain way. The Fenchel-variational view espoused in [26]
reveals the optimal discriminator T to be a function of the odds ratio: T* = h(q∕p) for some
function h determined by the particular f -divergence chosen. We observe that if q can be evaluated
analytically, then we might as well represent the discriminator in terms of q and some function
V, where V directly approximates p. This change of representation allows us to take any known
2
Under review as a conference paper at ICLR 2019
regularities of p and impose them on ν; e.g., if p is known to be translation-invariant, then we can
safely impose translational invariance on ν without imposing any undue constraints on our model.
Note that we cannot similarly impose p’s structure on T, which may be a complex function even ifp
and q are simple: for example, even if p and q are both bounded above, T may be unbounded, since it
is a function of the ratio of the two.
Fig. 2 illustrates how ν is structured for our application domain of vehicle trajectory forecasting: ν is
represented as a sum of learned spatial rewards, which penalizes trajectories according to a learned
function over spatial positions. Intuitively, this prevents trajectories from colliding with obstacles,
while simultaneously learning the concept of an obstacle.
2 Pushforward representation and implementation
Optimizing Eq. (1) is straightforward except for one subtle point: we must be able to evaluate q
pointwise and differentiate the expression Ex〜q log(q(χ)∕ν(x)) with respect to the parameters of q.
Inspired by prior work [29, 8, 18], we solve both these problems by representing q as the pushforward
of a simple distribution under an invertible warp (also known as a normalizing flow). Suppose μ is a
distribution over a set Z and g : Z → X is a function (the generator) with domain Z. Then we can
define a measure on X as the distribution of g(z) sampling Z from μ——this distribution, denoted here
by g∣μ, is referred to as the pushforward of μ under g.
Now suppose g is parameterized by θ and differentiable in X and θ. By representing q as qθ = gθ ∣μ,
for some simple distribution μ, we can move the derivative wrt. θ inside the expectation by exploiting
the property Ex〜gg∣μ f(χ) = Ez〜μf(gθ(Z)) for all functions f:
d
dθ
Esq lθg 嗯
ν(x)
d	q(x)
dθ Ex~q01“log VP
Ex〜μ
_dl qθ (gθ(X))
dθ V v(gθ(x)).
(2)
This is well-known as the reparameterization trick; it allows us to obtain a low-variance, unbiased es-
timate of the parameter derivatives for learning with SGD. However, one problem remains: evaluating
qθ(x) = (gθ∣μ)(χ), which appears in both the first and last terms of (1). This is solved by assuming
that gθ is invertible: Z := g-1(X). Thus, we have an analytic formula for the pushforward density:
qθ(gθ(Z)) = (gθL)(gθ(Z)) = μ(z) ∣(dgθ)z∣ 1,
(3)
where ∣(dgθ)z| represents the determinant of the Jacobian of gθ evaluated at the point z. This finally
allows us to rewrite Eq. (1) in the following explicit form, after performing some simplifications:
-maxinf Ex〜P log
θ Φ	∣(dgθ )z|
〃(Z)
∣(dgθ )z∣Vφ(X)
+ Ez〜μ log
μ(Z)
∣(dgθ )z ∣Vφ(gθ (z))
(4)
+
A pseudocode summary of our method is given in Algorithm 1. X and Z denote batches of observed
and latent samples, respectively, while subscripts D and G denote either data or generated samples.
A few implementation issues are noted here. Applying the method to a particular problem requires
the implementation of the invertible generator G(∙; θ), the structured Gibbs energy log v(∙; φ), and
the base distribution μ. In order to avoid numerical issues when q is not absolutely continuous wrt.
v (i.e., when V is 0 but q is not), we reparameterize V as V J aq + V, where α is a small number.
Given this assumption, the quantity q(X)∕(αq(X) + ν(X)) can be rewritten in terms of the sigmoid σ.
Optimization proceeds by alternating between minimizing the loss (1) in the generator parameters θ
and maximizing it in the energy parameters φ. However, as noted in Alg. 1, we may alternatively
minimize a different objective for the generator: namely, -Ex〜P log q(χ) + Ex〜q log(q(χ)∕v(x)).
The rationale for the alternative objective is that, as noted in Sec. 3.1, the inner minimization may be
viewed as fitting V to P-in which case, we may approximate KL(q, P) as Ex〜q log(q(χ)∕v(x)). The
alternate generator objective was used for the toy experiment in Fig. 1, whereas the original objective
was used for the trajectory forecasting experiments.
3	Derivations and interpretations
Equation (1) can be derived via a variational lower bound derived from Fenchel conjugacy, using a
technique similar to [25, 26]. Our approach of pairing this convex conjugate with the pushforward
3
Under review as a conference paper at ICLR 2019
Algorithm 1 Pseudocode for C3PO implementation
Require: XD: a batch of training data, Zg : batch of generator noise samples from μ
1:	Zd , det dgχ[ J GT(Xd； θ) .G-1(x) returns g-1(χ) and log det. of Jacobian of g-1 at X
2:	Xg, det dgz@ J G(Zg； θ)	. G(Z) returns gθ (Z) and log det. of Jacobian of gθ at Z
3:	log q(XD) J log μ(ZD) + log | det dgχ∖ |	. Generator PDF at data samples
4:	log q(Xg) J log μ(Zg)+log | det dgz@ |	. Generator PDF at generator samples
5:	log q/v(XD) J log α + log q(XD) — log V(Xd; φ)
6:	log q/v(XG) J log α + log q(Xg) - log V(Xg； φ)
7:	L J BatchMean(— log q(X0) — α-1σ(log q/v(XD)) + logσ(log q/v(XG)) — log a)
8:	for i J 1 . . . N do
9:	θ J θ — βVθL	. Alternative generator loss: L := — log q(X0) + log σ(log q/v(XG))
10:	for j J 1 . . . M do
11:	φJφ+βVφL
direct density estimation motivates our method’s name: Convex Conjugate Coupled Pushforward
Optimization (C3PO). Observe that the Jeffrey divergence can be written as minq -Ex〜P log q(X) +
Ex〜q log q(χ) — Ex〜q logp(x). Since q can be sampled, evaluated, and differentiated (Sec. 2), but P
can only be sampled, our goal in this section is to convert -Ex〜q logp(x) to something expressable
in terms of expectations wrt. p and q. This is achieved by applying the Fenchel-Young inequality to
the function f (p) := — logp, which yields - logP ≥ supλ<o λp — ( —1 — log(—λ)). Substituting
this inequality in place of — logP(X) yields the following lower bound of -Ex〜q logp(x):
/ q(x)(supλp(x)+log(-λ) + 1)dx
X	λ<0
1 + sup
λ<0
q(x) (λ(x)p(x) + log(—λ(x))) dx,
(5)
X
where the sup on the right-hand side is taken over all functions λ : X → R- mapping the domain
to a negative scalar. Observing that / q(x)λ(x)p(x) dx can be written either as Ex〜qλ(x)p(x) or
E^〜pλ(X)q(X), and making the substitution V = —1/λ, we have the equivalent bound
q(X)
-Ex〜q logP(X) ≥ 1 + supEx〜P ———--Ex〜q log V(x).
ν>0	V (x)
(6)
Substituting this the Jeffrey divergence yields Eq. (1).
3.1	Interpretation as learning a Gibbs distribution
Although we have so far viewed our goal as primarily learning q, we now observe that our method can
also be interpreted primarily as a way of learning v as a Gibbs distribution approximating P. Learning
a Gibbs distribution, or a more general energy-based model [19], is a very general and effective way to
impose strong domain-specific regularization on probability distributions over high-dimensional data.
Generally, this is achieved by structuring the energy function Vφ := log vφ to assign similar energies
to similar examples; for example, a convolutional neural network might constitute a good energy
for image generation, since the structure of a CNN encodes some degree of translational invariance.
Unfortunately, inference and learning with high-dimensional Gibbs distributions is difficult.
Our method can be viewed as a way to train a Gibbs distribution that circumvents some of these
difficulties. In this view, the inner optimization in Eq. (1) is interpreted as minimizing a weighted
divergence between P and v ; the weights are exactly q. Specifically, we observe that the inner
optimization in Eq. (1) minimizes the following weighted Itakura-Saito divergence [16, 9]:
TTZq(x) (⅛⅛- log M- 1)dx
(7)
It would seem reasonable to choose the weights q as q = P; since P cannot be evaluated directly,
alternating optimization is a sensible alternative. Minimizing Eq. (7) intuitively gives us the best
Gibbs approximation of P over the support of q.
Intuition for the learning rule of v can be obtained by computing the functional gradient of Eq. (1)
with respect to V (i.e., differentiating with respect to V(x), ∀x. The functional gradient δ,δν expresses
4
Under review as a ConferenCe PaPer at ICLR 2019
FigUre 3: COmPariSOn Of methods On a test SCene from the BEVWORLDIK dataset∙ Left COIUmn:
The input BEV map (roads5∙black』accompanied by00 demonstrations Of POSSible behavior (in
blue)∙ EaCh method is ViSUaHZed With 60 Of each sample，FOrthe methods that Ieam a CoSt map” the
Ieamed COSt map is blended With the input BEV feature map∙
InPUt a2^φerts R2P2 C3PO R2P2 住L CVAEfGANmF 一 fGAN晦rey
								
								BevWorld 1
						-46.6 ±0.5		
1 O O	O I S	O Qi 6 8	O 幻 9 O	O 幻 5 2	1 O	O 幻 ∞ ∞		
								BevworldIK
						-61.6 ±0.6	—(q, Pkde)	
O 幻	O 9	O ’6 9 8	O ∞ 6 5	O ∞ ∞ 6	O b oo 9	O 幻 2 2		
TabIe 一一 ComPariSon Of methodss∙two datasets∙ Left j Single BEVWorld SCene (identical train and
test)" 300 experts，800 PoHCy samples，Right: BEVWOrId00 training SCeneS “000 test SCenes “00
experts≥cenp12 PoHCy SamPIeS≥cene∙ MeanS and their Standard errors are reported。Bold indicates
the best PerfOrming method among methods With nondegenerate ——Hf q) (j∙e∙ fGAN Jeffrey is
degenerate). CVAE∞——Hs q) is estimated Via MC SamPIs∙g 300 times Per SCen 尸 See「37」.
the direction in WhiCh U ShOUId be moved at each POint H in Orderto OPtima=y decrease the ObjeCtive∙
ObSerV5∙g that Ed 〜⅛s∕7s=Ed 〜%s∕7s" We Obtain the following for the functional
gradient Ofthe ObjeCtiVe Wrt∙ log' 5cy51og7H) H qH) (PR>H)——1)∙ We therefore See that
minimizing Eq∙ (1) in 7 raises OrIoWerSIog U at each Point H according to Whetherit exceeds p(n
With a learning rate given by qHL and a unique HXed Point (assuming q V O) OfP = ZA
4 EXPERIMENTS
We CondUCted experiments On four datasets Of VehiCle trajectories” where the ObjeCtiVe is to forecast a
distribution OVerthe VehiCIRS futureOCationS H ∈ ¾0×^ given ConteXtUaIinformation about each
SCene ∙ The ConteXtUaIinfOrmation includes 2 SeCondS Of the VehiCIRS PreViOUS POSitiOPas WeII as
a BirdaS Eye VieW (BEV) map OfViSUaI SCene features。OUr experiments are designed to quantify
two key aspects Of generative modeling二he learned models HkeHhoOd OfheId—oUtteSt data (i∙e∙ the
negative forward CrOSS—entropy ——H?q))“ and the quality Of SamPleSfrOmtheIeamed model (j∙e∙
the negative reverse CrOSS—entropy ——H (q⅛)∙ Our hypotheses are 二)C3PO wsachieve SUPeriOr
SclmP一e—qua一ity performance to OihermeihOdS 2) C3P0 W三 一earn P high—quality q〉PclrticIUy due to its
ability to Perform direct density eva一UCItiOn Cmd OPtimmatiOn Ofq 3" C3P0 W=Z 一earn § interpretable
U that PenaUNeS bad samples.
TWO Ofthe four datasets are SynthetiC (BEVWORLDl and BEVWORLDlK)“ With known road?
enabHng UStO ConStrUCt SamPleS from a reasonable P distribution" approXimate P Via KDE" and
measure the reverse CrOSS—entropy H(q" PKDE) ∙ We also CakUlate the PerCentage Of trajectory POintS
Onthe road as another measure Of SamPIe quality DetailS Of how We generate BEVWORLD are
in the SUPPIement∙ We also experiment With two real—world datasets: the KlTTl dataset” and the
CALIFoRECASTING datase二32一∙
4∙ 1 IMPLEMENTATIoN AND BASELINES
GiVen OUr Setting is that Of forecasting future VehiCIe trajectories" We IeVerage ideas from InVerSe
ReinfOrCement LeanIing-242 43" 4ILS StrUCtUre OUrGibbS energy V as a SPatiaI Cosi map.. Where
Under review as a conference paper at ICLR 2019
Table 2: Comparison of methods in two real-world datasets: Kitti and CaliForecasting. Means
and their standard errors are reported. Bold indicates the best performing method among methods
with nondegenerate -H (p, q) (i.e. fGAN Jeffrey is degenerate).
Kitti	CaliForecasting
Method	-H (p, q)	VrTI(q)	-H (p, q)	VφCALIF(q)
R2P2	63.7 ± 0.8	-744 ± 20	74.1 ± 0.38	-6.50 ± 3.9
C3PO (ours)	61.5 ± 0.7	-457 ± 13	73.5 ± 0.4	57.3 ± 1.4
R2P2 GAIL	54.9 ± 0.7	-693 ± 17	46.9 ± 0.3	-61.1 ± 6.1
CVAE*	9.22 ± 0.9	-555 ± 9.9	10.1 ± 0.9	48.3 ± 1.5
fGAN KL	32.9 ± 1.3	-693 ± 10	9.55 ± 0.02	-568 ± 20
fGAN Reverse KL	12.8 ± 0.08	-1362 ± 33	-89.7 ± 3.1	21.8 ± 2.6
fGAN Jeffrey	-2e4 ± 2e3	-195 ± 4.0	-2e4 ± 7e2	69.5 ± 0.1
log νφ = Vφ(x) = PtT=1 Rφ(xt0, xt1 ; BEV), and Rφ (a, b; BEV) is the output of a CNN that can
be interpolated at 2d positions of the form (a, b). This structure enables ν to penalize trajectories
that travel to locations it perceives to be bad, e.g. locations with obstacles, or locations far from a
perceived road.
We compare our method, C3PO, to several state-of-the-art approaches in imitation learning and
generative modeling: Generative Adversarial Imitation Learning (GAIL) [12], f-GAN [26], the CVAE
method of DESIRE [20], and R2P2[32]. In each baseline, we use architectures as similar to our own
method as possible: the policy (generator) architecture of GAIL and the generator architecture of
f-GAN are identical to the generator architecture of our own approach. The same architecture used
for Vφ in our method was also used for the discriminators in all baselines.
Our implementation of the q architecture is based on R2P2 [31]. One key difference between our
method, C3PO, and R2P2, is that R2P2 does not have an adversarial component; its main focus is
learning the forward KL term, the first component of C3PO’s objective function. R2P2 starts from a
similar objective: the symmetric sum of cross-entropies. However, it relies on a cruder approximation
of H(q, p), because it learns an approximating P for H(q, P) offline, with no interaction from q.
4.2	Synthetic experiments
BEVWORLD 1contains a single scene with 300 samples from p. This setting is unconditional: the
contextual information provided is identical, and not directly useful for modeling p. This setting
also provides a fairer comparison to fGAN methods, which left the extension of fGAN to contextual
settings as future work [26]. Table 1 shows the results of BevWorld 1 experiments. We observe
that R2P2 and C3PO achieve the best -H (p, q) scores, with C3PO outperforming all nondegenerate
methods in its sample quality. This evidence supports hypotheses 1) and 2): C3PO achieves superior
sample quality and high-quality data density. We also observed C3PO to be very stable throughout
training in terms of H(p, q): it was as easy to train it as R2P2 in all experiments.
We also observe that while fGAN KL indirectly learns a q with some support for p, its samples are
quite poor. This matches expectations, as the forward KL divergence fails to impose much penalty
on sample quality [14]. fGAN Jeffrey is the fGAN method most similar to our approach because it
uses the Jeffrey divergence. We observe it to suffer mode collapse in all of our experiments, a similar
result to [26], in which fGAN Jeffrey had the worse test-set likelihood.
Next, we consider BEVWORLD 1 K, in which 100 training scenes, each with 100 samples from p, are
used to learn conditional generative models. There are 1000 scenes in the test set. Table 1 shows
the result of these experiments. We observe results similar to B evWorld 1 K, again supporting
hypotheses 1) and 2). C3PO can learn in the conditional setting to produce high-quality samples
from a distribution with good support of p. We display example results in Fig. 3, and observe several
methods, including C3PO , learn a cost-map representation that penalizes samples not on the road.
This evidence of the learned intuitive perceptual measurement of ν supports hypothesis 3).
6
Under review as a conference paper at ICLR 2019
Figure 4: Left: Comparison of methods under the learned VφKITTI (q) criterion. Each row corresponds
to a method, and each column corresponds to the method’s result on the item at a specific level of
performance, from worst (left) to best (right) of results on 100 scenes. Each image is composed of
the learned VφKITTI (q) blended with the input BEV features, sample trajectories from each method
(red), and the true future (blue). The learned V often penalizes samples that go off of the road or
into obstacles inferred from the features. C3PO usually produces the best samples under this metric.
Right: Evaluation of sample quality on test data from the KITTI dataset (a) and CALIFORECASTING
dataset (b). Each approach generated 12 trajectory samples per scene, and the mean Vφ score was
calculated for each scene. The results are displayed as a normalized cumulative histogram, where
the y-value at x is the percentage of scenes that received V ≤ x. At almost every given V , C3PO is
likelier to have more samples above the given V than other methods.
6
I
S
Figure 5: Comparison of methods on a set of random scenes from the CaliForecasting test set.
Scene indices were selected uniformly at random (once) from the possible test indices. Each image
shows the learned VφCALIF(q) blended with the input BEV features, 12 sample trajectories from each
method (red), and the true future (blue). V learns to penalize samples that go off of the road or into
obstacles.
7
Under review as a conference paper at ICLR 2019
4.3	Real-world experiments
We now experiment with real-world data, in which each method is provided with noisy contextual
information, and is evaluated by its ability to produce a generative model and high-quality samples.
Evaluation of sample quality is more difficult in this scenario: we have only one sample of p in
each scene, precluding construction of pKDE, and there are no labels of physical roads, precluding
computation of on-road statistics. Fortunately, evaluation of -H (p, q) is still possible.
After observing the learned Vφ of C3PO on both synthetic and real data, we found Vφ to be generally
interpretable and intuitively good: it assigns low cost to roads it learns to perceive and it assigns high
cost to obstacles it learns to perceive. We therefore employed our learned model Vφ to quantitatively
evaluate samples from all methods. Fig. 4 illustrates results from several approaches visualized with
V and point cloud features from the BEV, at various quantiles of each method’s performance under
V . Fig. 5 (left) illustrates each method on a set of randomly sampled scenes. In both figures, V
perceives and assigns penalty to regions around obstacles in the point cloud data. Note that this
energy is learned implicitly, because demonstrations from p avoid obstacles. These results provide
further support for hypothesis 3).
Quantitative results are shown in Table 2. We find that results are, overall, similar to results on the
other datasets. C3PO produces a high-performing q in terms of both its likelihood, -H (p, q), as
well as the quality of its samples, V . Additionally, we show evaluation of V for the top methods in
Fig. 4 (right). Together, these results further strengthen evidence in support of all of our hypotheses.
5	Related Work
The initial motivation for our work was the apparent dichotomy between sample quality and mode
coverage in existing deep generative models; this phenomenon has been noted and quantified in
work such as [40, 23, 4, 13, 14]. Our work synthesizes several techniques from prior work to
address these issues, including the Fenchel-variational principle from work such as [25, 26], the
well-known reparameterization trick [17, 35], and analytic pushforward-based density estimators such
as normalizing flows [29, 18], RealNVP [8], and related models [22, 10, 27, 38, 30]. Comparatively
little work has explored combining these methods, with some exceptions. Combining a RealNVP [8]
density estimator with a GAN objective was considered in [11]; however, this is susceptible to the
problems inherent with GANs mentioned in the introduction. A pushforward-based density estimator
was employed in conjunction with variational inference to optimize the classical Bayesian evidence
lower bound in [18], but this cross-entropy objective suffers from the previously-mentioned problems
with optimizing KL(p k q) alone, as do all methods based on this objective, including [8, 10, 27, 38].
Our work is also comparable to the extensive literature on learning deep graphical models, including
variants of Boltzmann machines [1, 34] and various proposals for learning deep CRFs and structured
energy-based models [19] based on inference techniques including convex optimization [28, 7, 2, 36],
various forms of unrolled inference [33, 42, 39], pseudolikelihood [21], and continuous relaxation [5],
among other techniques [13]. Viewed as a way to learn a deep graphical model, the most important
distinguishing factor of our work is the fact that our method does not need to perform inference
directly, as discussed in Sec. (3.1). To the extent that q is viewed as (indirectly) performing inference,
our method and other deep generative models bears some similarity to the wake-sleep algorithm,
which also alternates between optimizing a model distribution and an “inference” distribution using
complementary divergences [13]. Score-matching [15] also learns a Gibbs distribution without
inference; however, unlike our method, score-matching does not learn an inference distribution.
6	Conclusion
We have demonstrated how GANs may be effectively regularized by reparameterizing the discrimina-
tor to be a function of the model density and a structured Gibbs density, showing how this may be
achieved by optimizing a Jeffrey divergence loss, assuming the generator is invertible, and applying
a variational bound based on Fenchel conjugacy. Applied to a trajectory forecasting problem, we
observed superior mode coverage and qualitative results compared to traditional GANs. We hope
to soon demonstrate the general applicability of our approach by applying it to the task of image
generation as well.
8
Under review as a conference paper at ICLR 2019
References
[1]	David H Ackley, Geoffrey E Hinton, and Terrence J Sejnowski. “A learning algorithm for
Boltzmann machines”. In: Readings in Computer Vision. Elsevier, 1987, pp. 522-533.
[2]	Brandon Amos and J Zico Kolter. “Optnet: Differentiable optimization as a layer in neural
networks”. In: arXiv preprint arXiv:1703.00443 (2017).
[3]	Martin Arjovsky, Soumith Chintala, and L6on Bottou. “Wasserstein gan”. In: arXiv preprint
arXiv:1701.07875 (2017).
[4]	S. Barratt and R. Sharma. “A Note on the Inception Score”. In: ArXiv e-prints (Jan. 2018).
arXiv: 1801.01973 [stat.ML].
[5]	David Belanger and Andrew McCallum. “Structured prediction energy networks”. In: Interna-
tional Conference on Machine Learning. 2016, pp. 983-992.
[6]	Christopher M Bishop. Pattern recognition and machine learning. Springer, 2006.
[7]	Liang-Chieh Chen et al. “Learning deep structured models”. In: International Conference on
Machine Learning. 2015, pp. 1785-1794.
[8]	Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. “Density estimation using Real NVP”.
In: arXiv preprint arXiv:1605.08803 (2016).
[9]	Cedric Fevotte, Nancy Bertin, and Jean-Louis Durrieu. “Nonnegative matrix factorization with
the Itakura-Saito divergence: With application to music analysis”. In: Neural computation 21.3
(2009), pp. 793-830.
[10]	Mathieu Germain et al. “Made: Masked autoencoder for distribution estimation”. In: Interna-
tional Conference on Machine Learning. 2015, pp. 881-889.
[11]	Aditya Grover, Manik Dhar, and Stefano Ermon. “Flow-GAN: Combining Maximum Likeli-
hood and Adversarial Learning in Generative Models”. In: Proceedings of the Thirty-Second
AAAI Conference on Artificial Intelligence, New Orleans, Louisiana, USA, February 2-7, 2018.
2018. url: https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/
view/17409.
[12]	Jonathan Ho and Stefano Ermon. “Generative adversarial imitation learning”. In: Advances in
Neural Information Processing Systems. 2016, pp. 4565-4573.
[13]	Zhiting Hu et al. “On Unifying Deep Generative Models”. In: International Conference on
Learning Representations. 2018. URL: https : / / openreview . net / forum ? id=
rylSzl-R-.
[14]	Ferenc HUSZdr. "How (not) to train your generative model: Scheduled sampling, likelihood,
adversary?” In: arXiv preprint arXiv:1511.05101 (2015).
[15]	Aapo Hyvarinen. "Estimation of non-normalized statistical models by score matching”. In:
Journal of Machine Learning Research 6.Apr (2005), pp. 695-709.
[16]	Fumitada Itakura. “Analysis synthesis telephony based on the maximum likelihood method”.
In: The 6th international congress on acoustics, 1968. 1968, pp. 280-292.
[17]	Diederik P Kingma and Max Welling. “Auto-encoding variational bayes”. In: arXiv preprint
arXiv:1312.6114 (2013).
[18]	Diederik P Kingma et al. “Improved Variational Inference with Inverse Autoregressive Flow”.
In: Advances in Neural Information Processing Systems 29. Ed. by D. D. Lee et al. Curran
Associates, Inc., 2016, pp. 4743-4751. url: http://papers.nips.cc/paper/6581-
improved - variational - inference - with - inverse - autoregressive -
flow.pdf.
[19]	Yann LeCun et al. “A tutorial on energy-based learning”. In: Predicting structured data 1.0
(2006).
[20]	Namhoon Lee et al. “Desire: Distant future prediction in dynamic scenes with interacting
agents”. In: (2017).
[21]	Guosheng Lin et al. “Exploring context with deep structured models for semantic segmenta-
tion”. In: IEEE transactions on pattern analysis and machine intelligence (2017).
[22]	Qiang Liu and Dilin Wang. “Stein variational gradient descent: A general purpose bayesian
inference algorithm”. In: Advances In Neural Information Processing Systems. 2016, pp. 2378-
2386.
9
Under review as a conference paper at ICLR 2019
[23]	Luke Metz et al. “Unrolled Generative Adversarial Networks”. In: CoRR abs/1611.02163
(2016). arXiv: 1611.02163. url: http://arxiv.org/abs/1611.02163.
[24]	Andrew Y Ng, Stuart J Russell, et al. “Algorithms for inverse reinforcement learning.” In:
Icml. 2000,pp. 663-670.
[25]	XuanLong Nguyen, Martin J Wainwright, and Michael I Jordan. “Estimating divergence
functionals and the likelihood ratio by convex risk minimization”. In: IEEE Transactions on
Information Theory 56.11 (2010), pp. 5847-5861.
[26]	Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. “f-gan: Training generative neural
samplers using variational divergence minimization”. In: Advances in Neural Information
Processing Systems. 2016, pp. 271-279.
[27]	Aaron van den Oord et al. “Conditional image generation with pixelcnn decoders”. In: Ad-
vances in Neural Information Processing Systems. 2016, pp. 4790-4798.
[28]	Rene Ranftl and Thomas Pock. “A deep variational model for image segmentation”. In:
German Conference on Pattern Recognition. Springer. 2014, pp. 107-118.
[29]	Danilo Jimenez Rezende and Shakir Mohamed. “Variational inference with normalizing flows”.
In: arXiv preprint arXiv:1505.05770 (2015).
[30]	Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. “Stochastic backpropagation
and approximate inference in deep generative models”. In: arXiv preprint arXiv:1401.4082
(2014).
[31]	Nicholas Rhinehart, Kris M. Kitani, and Paul Vernaza. “R2P2: A ReparameteRized Pushfor-
ward Policy for Diverse, Precise Generative Path Forecasting”. In: The European Conference
on Computer Vision (ECCV). Sept. 2018.
[32]	Nicholas Rhinehart, Kris M. Kitani, and Paul Vernaza. “R2P2: A ReparameteRized Pushfor-
ward Policy for Diverse, Precise Generative Path Forecasting”. In: The European Conference
on Computer Vision (ECCV). Sept. 2018.
[33]	Stephane Ross et al. “Learning message-passing inference machines for structured prediction”.
In: Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on. IEEE. 2011,
pp. 2737-2744.
[34]	Ruslan Salakhutdinov, Andriy Mnih, and Geoffrey Hinton. “Restricted Boltzmann machines
for collaborative filtering”. In: Proceedings of the 24th international conference on Machine
learning. ACM. 2007, pp. 791-798.
[35]	John Schulman et al. “Gradient estimation using stochastic computation graphs”. In: Advances
in Neural Information Processing Systems. 2015, pp. 3528-3536.
[36]	Samuel Schulter et al. “Deep Network Flow for Multi-Object Tracking”. In: Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition. 2017, pp. 6951-6960.
[37]	Kihyuk Sohn, Honglak Lee, and Xinchen Yan. “Learning Structured Output Representation
using Deep Conditional Generative Models”. In: Advances in Neural Information Processing
Systems 28. Ed. by C. Cortes et al. Curran Associates, Inc., 2015, pp. 3483-3491. URL:
http :/ /papers. nips . cc/paper/5775- learning - structured - output -
representation-using-deep-conditional-generative-models.pdf.
[38]	Benigno Uria et al. “Neural autoregressive distribution estimation”. In: Journal of Machine
Learning Research 17.205 (2016), pp. 1-37.
[39]	Shenlong Wang, Sanja Fidler, and Raquel Urtasun. “Proximal deep structured models”. In:
Advances in Neural Information Processing Systems. 2016, pp. 865-873.
[40]	Yuhuai Wu et al. “On the Quantitative Analysis of Decoder-Based Generative Models”. In:
CoRR abs/1611.04273 (2016). arXiv: 1611.04273. URL: http://arxiv.org/abs/
1611.04273.
[41]	Markus Wulfmeier, Peter Ondruska, and Ingmar Posner. “Maximum entropy deep inverse
reinforcement learning”. In: arXiv preprint arXiv:1507.04888 (2015).
[42]	Shuai Zheng et al. “Conditional random fields as recurrent neural networks”. In: Proceedings
of the IEEE International Conference on Computer Vision. 2015, pp. 1529-1537.
[43]	Brian D Ziebart et al. “Maximum Entropy Inverse Reinforcement Learning.” In: AAAI. Vol. 8.
Chicago, IL, USA. 2008, pp. 1433-1438.
10