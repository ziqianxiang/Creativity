Under review as a conference paper at ICLR 2019
Generative Adversarial Models for Learning
Private and Fair Representations
Anonymous authors
Paper under double-blind review
Ab stract
We present Generative Adversarial Privacy and Fairness (GAPF), a data-driven
framework for learning private and fair representations of the data. GAPF lever-
ages recent advances in adversarial learning to allow a data holder to learn “uni-
versal” representations that decouple a set of sensitive attributes from the rest of
the dataset. Under GAPF, finding the optimal decorrelation scheme is formulated
as a constrained minimax game between a generative decorrelator and an adver-
sary. We show that for appropriately chosen adversarial loss functions, GAPF
provides privacy guarantees against strong information-theoretic adversaries and
enforces demographic parity. We also evaluate the performance of GAPF on
multi-dimensional Gaussian mixture models and real datasets, and show how a
designer can certify that representations learned under an adversary with a fixed
architecture perform well against more complex adversaries.
Keywords- Data Privacy, Fairness, Adversarial Learning, Generative Adversarial
Networks, Minimax Games, Information Theory
1	Introduction
The use of deep learning algorithms for data analytics has recently seen unprecedented success
for a variety of problems such as image classification, natural language processing, and prediction
of consumer behavior, electricity use, political preferences, to name a few. The success of these
algorithms hinges on the availability of large datasets, that often contain sensitive information, and
thus, may facilitate learning models that inherit societal biases leading to unintended algorithmic
discrimination on legally protected groups such as race or gender. This, in turn, has led to privacy
and fairness concerns and a growing body of research focused on developing representations of the
dataset with fairness and/or privacy guarantees. These techniques predominantly involve designing
randomizing schemes, and in recent years, distinct approaches with provable statistical privacy or
fairness guarantees have emerged.
In the context of privacy, preserving the utility of published datasets while simultaneously provid-
ing provable privacy guarantees is a well-known challenge. While context-free privacy solutions,
such as differential privacy (Dwork et al., 2006b;a; Dwork, 2008; Dwork & Roth, 2014), provide
strong worst-case privacy guarantees, they often lead to a significant reduction in utility. In contrast,
context-aware privacy solutions, e.g., mutual information privacy (Rebollo-Monedero et al., 2010;
Calmon & Fawaz, 2012; Sankar et al., 2013; Salamatian et al., 2015; Basciftci et al., 2016), achieve
improved privacy-utility tradeoff, but assume that the data holder has access to dataset statistics.
In the context of fairness, machine learning models seek to maximize predictive accuracy. Fairness
concerns arise when models learned from datasets that include patterns of societal bias and discrimi-
nation inherit such biases. Thus, there is a need for actively decorrelating sensitive and non-sensitive
data. In the context of publishing datasets or meaningful representations that can be “universally”
used for a variety of learning tasks, modifying the training data is the most appropriate and is the
focus of this work. Fairness can then be achieved by carefully designing objective functions which
approximate a specific fairness definition while simultaneously ensuring maximal utility (Zemel
et al., 2013; Calmon et al., 2017; Ghassami et al., 2018). This, in turn, requires dataset statistics.
Adversarial learning approaches for context-aware privacy and fairness have been studied exten-
sively (Edwards & Storkey, 2015; Abadi & Andersen, 2016; Raval et al., 2017; Huang et al., 2017;
1
Under review as a conference paper at ICLR 2019
Tripathy et al., 2017; Beutel et al., 2017; Madras et al., 2018; Zhang et al., 2018). They allow the data
curator to cleverly decorrelate the sensitive attributes from the rest of the dataset. These approaches
overcome the lack of statistical knowledge by taking a data-driven approach that leverages recent
advancements in generative adversarial networks (GANs) (Goodfellow et al., 2014; Mirza & Osin-
dero, 2014). However, most existing efforts focus on extensive empirical studies without theoretical
verification and focus predominantly on providing guarantees for a specific classification task.
Adversarial loss
Figure 1: Generative adversarial model for privacy and fairness
This work introduces a general framework for context-aware privacy and fairness that we call gen-
erative adversarial privacy and fairness (GAPF) (see Figure 1). We provide precise connections
to information-theoretic privacy and fairness formulations and derive game-theoretically optimal
decorrelation schemes to compare against those learned directly from the data. While our frame-
work can be generalized to learn an arbitrary representation using an encoder-decoder structure, this
paper primarily focuses on learning private/fair representations of the data (of the same dimension).
Our Contributions. We list our main contributions below.
1.	We introduce GAPF, a framework for creating private/fair representations of data using an ad-
versarially trained conditional generative model. Unlike existing works, GAPF can create rep-
resentations that are useful for a variety of classification tasks, without requiring the designer to
model these tasks at training time. We validate this observation via experiments on the GENKI
(Whitehill & Movellan, 2012) and HAR (Anguita et al., 2013) datasets.
2.	We show that via the choice of the adversarial loss function, our framework can capture a rich
class of statistical and information-theoretic adversaries. This allows us to compare data-driven
approaches directly against strong inferential adversaries (e.g., a maximum a posteriori probabil-
ity (MAP) adversary with access to dataset statistics). We also show that by carefully designing
the loss functions in the GAPF framework, we can enforce demographic parity.
3.	We make precise comparison between data-driven privacy/fairness methods and the minimax
game-theoretic GAPF formulation. For Gaussian mixture data, we derive game-theoretically
optimal decorrelation schemes and compare them with those that are directly learned in a data-
driven fashion to show that the gap between theory and practice is negligible. Furthermore,
we propose using mutual information estimators to verify that no adversary (regardless of their
computational power) can reliably infer the sensitive attribute from the learned representation.
Related work. In the context of publishing datasets with privacy and utility guarantees, a number
of similar approaches have been recently considered. We briefly review them here. A detailed
literature review is included in Appendix A. DP-based obfuscators for data publishing have been
considered in (Hamm, 2016; Liu et al., 2017). These novel approaches leverage non-generative
minimax filters and deep auto-encoders to allow non-malicious entities to learn some public features
from the filtered data, while preventing malicious entities from learning other sensitive features.
However, DP can still incur a significant utility loss since it assumes worst-case dataset statistics.
Our approach models a rich class of randomization-based schemes via a generative model that allows
the generative decorrelator to tailor the noise to the dataset.
Our work is closely related to adversarial neural cryptography (Abadi & Andersen, 2016), learn-
ing censored representations (Edwards & Storkey, 2015), privacy preserving image sharing (Raval
et al., 2017), privacy-preserving adversarial networks (Tripathy et al., 2017), and adversarially learn-
ing fair representation (Madras et al., 2018) in which adversarial learning is used to learn how to
protect communications by encryption or hide/remove sensitive information or generate fair repre-
sentation of the data. Similar to these problems, our model includes a minimax formulation and uses
2
Under review as a conference paper at ICLR 2019
adversarial neural networks to learn decorrelation schemes that prevent an adversary from inferring
the sensitive variable. However, most of these papers use non-generative auto-encoders to remove
sensitive information. Instead, we use a GANs-like approach to learn decorrelation schemes. We
also go beyond in formulating a game-theoretic setting subject to a distortion constraint which allows
us to learn private/fair representation for a variety of learning tasks. Enforcing the distortion con-
straint calls for a new training process that relies on the Penalty method or Augmented Lagrangian
method presented in Appendix C. We show that our framework captures a rich class of statistical and
information-theoretic adversaries by changing the loss function. We also compare the performance
of data-driven privacy/fairness methods and the minimax game-theoretic GAPF.
Fair representations using information-theoretic objective functions and constrained optimization
have been proposed in (Calmon et al., 2017; Ghassami et al., 2018). However, both approaches
require the knowledge of dataset statistics, which is very difficult to obtain for real datasets. We
overcome the issue of statistical knowledge by taking a data-driven approach, i.e., learning the
representation from the data directly via adversarial models. In contrast to in-processing approaches
that modify learning algorithms to ensure fair predictions (e..g, using linear programs in (Dwork
et al., 2012; Fish et al., 2016) or via adversarial learning approach in (Zhang et al., 2018)), we
focus on a pre-processing approach to ensure fairness for a variety of learning tasks. Using GANs
to generate synthetic non-sensitive attributes and labels which ensure fairness while preserving the
utility of the data (predicting the label) has been studied in (Xu et al., 2018; Sattigeri et al., 2018).
Rather than using a conditional-generative model to generate synthetic data, we focus on creating
fair/private representations of the original data while preserving the utility of the representations
for a variety of learning tasks by learning nonlinear compression and noise adding schemes via a
generative adversarial model.
2	Generative Adversarial Model for Privacy and Fairnes s
We consider a dataset D with n entries where each entry is denoted as (S, X, Y ) where S ∈ S is the
sensitive variable, X ∈ X is the public variable, and Y ∈ Y is the target (non-sensitive) variable (for
learning). Instances of X, S, and Y are denoted by x, s and y, respectively. We assume that each
entry (X, S, Y ) is independent and identically distributed according to P (X, S, Y ). Notice that we
model (X, S, Y ) jointly in the dataset. However, GAPF does not require the knowledge of Y .
Privacy and fairness. Context-aware notions of privacy model how well an adversary, with access
to the public data X, can infer the sensitive features S from the data. Research on context-aware pri-
vacy focus on privacy that capture a range of adversarial capabilities ranging from a belief refining
adversary using mutual information to quantify privacy to a guessing adversary using a hard-decision
rule. On the other hand, recent results on fairness in learning applications guarantees that for a spe-
cific target variable Y , the prediction of a machine learning model is accurate with respect to (w.r.t.)
Y but unbiased w.r.t. the sensitive variable S. The three oft-used fairness measures are demographic
parity, equalized odds, and equal opportunity. Demographic parity imposes the strongest fairness
requirement via complete independence of Y and S, and thus, least favors (for correlated Y and
S) utility (Hardt et al., 2016). Equalized odds ensures this independence conditioned on the label
Y thereby ensuring equal rates for true and false positives (binary Y) for all demographics. Equal
opportunity ensures equalized odds for the true positive case alone (Hardt et al., 2016).
When publishing a useful representation of the data for multiple users with different learning tasks, it
is difficult to identify a set of target variables (labels) a priori. Thus, our decorrelation scheme does
not restrict itself to a specific Y. Formally, we define the decorrelation schemes as a randomized
mapping given by X = g(X). We note that g(∙) can more generally depend on both X and S but
for the sake of simplicity, we restrict our attention to schemes that only depend on X.
Let h be a decision rule used by the adversary to infer the sensitive variable S as S = h(g(X))
from the representation g(X). We allow for hard decision rules under which h(g(X)) is a direct
estimate of S and soft decision rules under which h(g(X)) = Ph(∙∣g(X)) is a distribution over S.
To quantify the adversary,s performance, We use a loss function '(h(g(X = x)),S = S) defined for
every public-sensitive pair (x, s). Thus, the adversary’s expected loss w.r.t. X and S is L(h, g) ,
E['(h(g(X)), S)], where the expectation is taken over P(X, S) and the randomness in g and h.
3
Under review as a conference paper at ICLR 2019
Intuitively, the generative decorrelator would like to minimize the adversary’s ability to learn S
reliably from the published representation. This can be trivially done by releasing an X independent
of X . However, such an approach provides no utility for data analysts who want to learn non-
sensitive variables from X. To overcome this issue, We capture the loss incurred by perturbing the
original data via a distortion function d(x, x), which measures how far the original data X = X is
from the processed data X= X. Ensuring statistical utility in turn requires constraining the average
distortion E[d(g(X), X)] where the expectation is taken over P(X, S) and the randomness in g.
The data holder would like to find a decorrelation scheme g that is both privacy/fairness preserving
(in the sense that it is difficult for the adversary to learn S from X) and utility preserving (in the
sense that it does not distort the original data too much). In contrast, for a fixed decorrelation scheme
g, the adversary would like to find a (potentially randomized) function h that minimizes its expected
loss, which is equivalent to maximizing the negative of the expected loss. This leads to a constrained
minimax game between the generative decorrelator and the adversary given by
min max - L(h, g), s.t. E[d(g(X), X)] ≤ D,	(1)
g(∙) h(∙)
where the constant D ≥ 0 determines the allowable distortion for the generative decorrelator and
the expectation is taken over P (X, S) and the randomness in g and h.
Our GAPF framework places no restrictions on the adversary. Indeed, different loss functions and
decision rules lead to different adversarial models. In what follows, we consider a general α-loss
function '(h(g(X)), S
a——ι(1 - Ph(s∖g(X))1- 1 ) , α > 1 introduced in (Liao etal., 2018). We
show that α-loss can capture various information-theoretic adversaries ranging from a hard-decision
adversary under the 0-1 loss function '(h(g(X)), S) = Ih(g(x))=s to a soft-decision adversary under
the log-loss function '(h(g(X)),s) = - log Ph(s∣g(X)).
Theorem 1. Under α-loss, the optimal adversary decision rule is a 'α-tilted' Conditional distri-
bution Ph(s∣g(X))
PpP(g∣XX))α . The objective in equation 1 reduces to minIa(g(X); S)一
s∈S	g(•)
Hα(S), where Ia is the Arimoto mutual information and Ha is the Renyi entropy.
Under the hard-decision rules in which the adversary uses a 0-1 loss function, the optimal adversar-
ial strategy simplifies to using a MAP decision rule that maximizes P (S|g(X)). For a soft-decision
adversary under log-loss, the optimal adversarial strategy h is P(s∣g(X)) and the GAPF mini-
max problem in equation 1 simplifies to ming(∙) I(g(X); S) subject to E[d(g(X), X)] ≤ D, where
I(g(X); S) is the mutual information (MI) between g(X) and S.
Corollary 1. Using α-loss, we can obtain a continuous interpolation between a hard-decision ad-
versary under 0-1 loss (α → ∞) and a soft-decision adversary under log-loss function (α → 1).
Proposition 1. Under log-loss, GAPF enforces fairness subject to the distortion constraint. As the
distortion increases, the ensuing fairness guarantee approaches ideal demographic parity.
The proofs of Theorem 1 , Corollary 1 and Proposition 1 are presented in Appendix B. Many notions
of fairness rely on computing probabilities to ensure independence of sensitive and target variables
that are not easy to optimize in a data-driven fashion. In Proposition 1, we propose log-loss (modeled
in practice via cross-entropy) in GAPF as a proxy for enforcing fairness.
Data-driven GAPF. Thus far, we have focused on a setting where the data holder has access to
P(X, S). When P(X, S) is known, the data holder can simply solve the constrained minimax opti-
mization problem in equation 1 (game-theoretic version of GAPF) to obtain a decorrelation scheme
that would perform best against a chosen type of adversary. In the absence of P(X, S), we propose
a data-driven version of GAPF that allows the data holder to learn decorrelation schemes directly
from a dataset D = {(X(i), S(i))}in=1. Under the data-driven version of GAPF, we represent the
decorrelation scheme via a generative model g(X; θp) parameterized by θp. This generative model
takes X as input and outputs X. In the training phase, the data holder learns the optimal parame-
ters θp by competing against a computational adversary: a classifier modeled by a neural network
h(g(X; θp); θa) parameterized by θa. In the evaluation phase, the performance of the learned decor-
relation scheme can be tested under a strong adversary that is computationally unbounded and has
access to dataset statistics. We follow this procedure in the next section.
4
Under review as a conference paper at ICLR 2019
In theory, the functions h and g can be arbitrary. However, in practice, we need to restrict them
to a rich hypothesis class. Figure 1 shows an example of the GAPF model in which the gen-
erative decorrelator and adversary are modeled as deep neural networks. For a fixed h and g,
if S is binary, we can quantify the adversary’s empirical loss using cross entropy Ln (θp , θa) =
n
-1 P S(i) log h(g(x(i); θp); θa) + (1 — S(i))log(1 - h(g(x(i); θp); θa)). It is easy to generalize
i=1
cross entropy to the multi-class case using the softmax function. The optimal model parameters are
the solutions to
min max - Ln (θp,θa),	s.t. ED[d(g(X; θp), X)] ≤ D,	(2)
θp θa
where the expectation is over D and the randomness in g .
The minimax optimization in equation 2 is a two-player non-cooperative game between the genera-
tive decorrelator and the adversary with strategies θp and θa, respectively. In practice, we can learn
the equilibrium of the game using an iterative algorithm (see Algorithm 1 in Appendix C). We first
maximize the negative of the adversary’s loss function in the inner loop to compute the parameters
of h for a fixed g. Then, we minimize the decorrelator’s loss function, which is modeled as the neg-
ative of the adversary’s loss function, to compute the parameters of g for a fixed h. Observe that the
distortion constraint in equation 2 makes our minimax problem different from what is extensively
studied in previous works. To incorporate the distortion constraint, we use the penalty method (Lillo
et al., 1993) to replace the constrained optimization problem by adding a penalty to the objective
function. The penalty consists of a penalty parameter ρt multiplied by a measure of violation of
the constraint at the tth iteration. The constrained optimization problem of the generative decorrela-
tor can be approximated by a series of unconstrained optimization problems with the loss function
Ln (θp, θa) + ρt(max{0, E[d(g(x(i); θp), x(i))] - D})2, where ρt is a penalty coefficient increases
with the number of iterations t. The algorithm and the penalty method are detailed in Appendix C.
3 GAPF for Gaussian Mixture Models
To demonstrate the performance of the decorrelation schemes learned in a data-driven fashion
against a computationally bounded adversary, we evaluate the performance of the learned schemes
against a maximum a posteriori probability (MAP) adversary that has access to distributional infor-
mation and knows the applied decorrelation schemes. First, we derive game-theoretically optimal
decorrelation schemes by considering a MAP adversary. Second, we compare the game-theoretically
optimal scheme against the data-driven decorrelation scheme learned from a synthetic dataset by
competing against a computational adversary (modeled by a multi-layer neural network). To quan-
tify the performance of the learned decorrelation scheme, we compute the accuracy of inferring S
under a MAP adversary that has access to both the joint distribution of (X, S) and the decorrelation
scheme. Furthermore, we use mutual information estimator (detailed in Appendix F) to demon-
strate that GAPF effectively decorrelates the sensitive variables from the data. The details of the
game-theoretically optimal and data-driven GAPF are included in Appendix D.
Game-Theoretical Approach. We focus on a setting where S ∈ {0, 1} and X is an m-dimensional
Gaussian mixture random vector whose mean is dependent on S. Let P(S = 1) = q, X|S =
0 〜N(-μ, Σ) and X|S = 1 〜N(μ, Σ), where μ = (μι,…，μm). We assume that X|S = 0
and X |S = 1 have the same covariance Σ. Both the generative decorrelator and the adversary
have access to P (X, S). In order to have a tractable model for the decorrelator, we mainly focus
on linear (precisely affine) GAPF schemes X = g(X) = X + Z + β, where Z is a zero-mean
multi-dimensional Gaussian random vector. This linear GAPF enables controlling both the mean
and covariance of the privatized data. Although other distributions can be considered, we choose
additive Gaussian noise for tractability reasons. To quantify utility of the learned representation, we
use the '2 distance between X and X to obtain a distortion constraint EX X ∣∣X - X∣∣2 ≤ D.
Without loss of generality, We assume that β = (βι,…，βm) is a constant parameter vector and
Z 〜 N(0, Σp). Let α = P(2μ)T(Σ + Σp)-12μ, following similar analysis in (Gallager, 2013),
we can show that the adversary’s probability of detection is given by
p>Q (-2 + Im (亍)) + α-q)Q (-2 - * (Y ))∙⑶
5
Under review as a conference paper at ICLR 2019
Theorem 2. Consider GAPF schemes g(X) = X + Z + β, where X and Z are multi-
dimensional Gaussian random vectors with diagonal covariance matrices Σ = diag(σ12, ..., σm2 )
and Σp = diag(σp21 , ..., σp2m). The parameters of the minimax optimal decorrelation scheme are
βJ =0,σpi2 =
(√i^ - σ2,0), where λθ is chosen SUCh that P (^-√= - σ2)	= D, ∀i =
{1,...,m}. The accuracy ofthe MAP adversary is given by substituting β/ and σp. into equation 3.
Numerical Results. Figure 2 illustrates the performance of the learned GAPF scheme against a
strong theoretical MAP adversary for a 32-dimensional Gaussian mixture model with P (S = 1) =
0.75 and 0.5. We observe that the inference accuracy of the MAP adversary decreases as the distor-
tion increases and asymptotically approaches (as expected) the prior on the sensitive variable. The
decorrelation scheme obtained via the data-driven approach performs very well when pitted against
the MAP adversary (maximum accuracy difference around 0.7% compared to the theoretical opti-
mal). Furthermore, the estimated mutual information decreases as the distortion increases. In other
words, for the data generated by Gaussian mixture model with binary sensitive variable, the data-
driven version of GAPF can learn decorrelation schemes that perform as well as the decorrelation
schemes computed under the theoretical version of GAPF, given that the generative decorrelator has
access to the statistics of the dataset.
(a) Sensitive variable classification accuracy (b) Estimated mutual information between S and X
Figure 2: Performance of GAPF for Gaussian mixture models
4 GAPF for Real Datasets
We apply our GAPF framework to real-world datasets to demonstrate its effectiveness. The GENKI
dataset consists of 1, 740 training and 200 test samples. Each data sample is a 16 × 16 greyscale
face image with varying facial expressions. Both training and test datasets contain 50% male and
50% female. Among each gender, we have 50% smile and 50% non-smile faces. We consider
gender as sensitive variable S and the image pixels as public variable X . The HAR dataset consists
of 561 features of motion sensor data collected by a smartphone from 30 subjects performing six
activities (walking, walking upstairs, walking downstairs, sitting, standing, laying). We choose
subject identity as sensitive variable S and features of motion sensor data as public variable X . The
dataset is randomly partitioned into 8, 000 training and 2, 299 test samples. We train our model
based on the data-driven GAPF presented in Section 2 using TensorFlow (Abadi et al., 2016).
4.1 Generative Decorrelator and Adversary Model
For the GENKI dataset, we consider two different decorrelator architectures: the feedforward neu-
ral network decorrelator (FNND) and the transposed convolution neural network decorrelator (TC-
NND). The FNND architecture uses a feedforward multi-layer neural network to combine the low-
dimensional random noise (100 × 1) and the original image together (Figure 7). The TCNND takes a
low-dimensional random noise and generates high-dimensional noise using a multi-layer transposed
convolution neural network. The generated high-dimensional noise is added to each pixel of the
original image to produce the processed image (Figure 8). For the HAR dataset, we use the FNND
architecture modeled by a four-layer feedforward neural network. The details of the architectures
for both the generative decorrelator and the adversary are presented in Appendix E.
6
Under review as a conference paper at ICLR 2019
Expression Classification	Original Data		D = 1		D = 3		D = 5	
	Male	Female	Male	Female	Male	Female	Male	Female
False Positive Rate	0.04	0.14	0.1	0.18	0.18	0.16	0.16	0.14
False Negative Rate	0.16	0.02	0.2	0.08	0.26	0.12	0.24	0.24
Table 1: Error rates for expression classification using representation learned by FNND
Expression Classification	Original Data		D = 1		D = 3		D = 5	
	Male	Female	Male	Female	Male	Female	Male	Female
False Positive Rate	0.04	0.14	0.04	0.16	0.06	0.12	0.08	0.16
False Negative Rate	0.16	0.02	0.2	0.08	0.2	0.14	0.18	0.16
Table 2: Error rates for expression classification using representation learned by TCNND
4.2 Illustration of Results
The GENKI Dataset. Figure 3a illustrates the gender classification accuracy of the adversary for
different values of distortion. It can be seen that the adversary’s accuracy of classifying the sensi-
tive variable (gender) decreases progressively as the distortion increases. Given the same distortion
value, FNND achieves lower gender classification accuracy compared to TCNND. An intuitive ex-
planation is that the FNND uses both the noise vector and the original image to generate the pro-
cessed image. However, the TCNND generates the noise mask that is independent of the original
image pixels and adds the noise mask to the original image in the final step. To demonstrate the
effectiveness of the learned GAPF schemes, we compare the gender classification accuracy of the
learned GAPF schemes with adding uniform or Laplace noise. Figure 3a shows that for the same
distortion, the learned GAPF schemes achieve much lower gender classification accuracies than us-
ing uniform or Laplace noise. Furthermore, the estimated mutual information I (X; S) normalized
1 R (、厂 C、 1	1	1Λ 1 ∙ J J ∙	/L・	Ct ∖
by I (X; S) also decreases as the distortion increases (Figure 3b).
(a) Gender vs. expression classification accuracy
Figure 3: Privacy/fairness-utility tradeoff and mutual information estimation for GENKI
(b) Normalized mutual information estimation
To evaluate the influence of GAPF on other non-sensitive variable (Y ) classification tasks, we train
another CNN (see Figure 9) to perform facial expression classification on datasets processed by
different decorrelation schemes. The trained model is then tested on the original test data. In Fig-
ure 3a, we observe that the expression classification accuracy decreases gradually as the distortion
increases. Even for a large distortion value (5 per image), the expression classification accuracy only
1	λ -1 r∖CW ɪɔ aA	aA	a* a a	1 ∙ 1 J 1 ∙ rfc	,・ ɪ / ɪr x k∖ / G / x厂 τ λ∖
decreases by 10%. Furthermore, the estimated normalized mutual information I(X; Y )/I(X; Y )
decreases much slower than I(X; S)/I(X; S) as the distortion increases (Figure 3b).
Table 1 and 2 present different error rates for the facial expression classifiers trained using data rep-
resentations created by different decorrelator architectures. We observe that as distortion increases,
the error rates difference for different sensitive groups decrease. This implies the classifier’s deci-
sion is less biased to the sensitive variables when trained using the processed data. When D = 5,
the differences are already very small. Furthermore, we notice that the FNND architecture performs
better in enforcing fairness but suffers from higher error rate. The images processed by FNND is
shown in Figure 4. The decorrelator changes mostly eyes, nose, mouth, beard, and hair.
7
Under review as a conference paper at ICLR 2019
Figure 4: Perturbed images with different per pixel distortion using FNND
Activity classification accuracy
(a) Identity vs. activity classification accuracy
(b) Normalized mutual information estimation
Figure 5: Privacy/fairness-utility tradeoff and mutual information estimation for HAR
The HAR Dataset. Figure 5a illustrates the activity and identity classification accuracy for different
values of distortion. The adversary’s sensitive variable (identity) classification accuracy decreases
progressively as the distortion increases. When the distortion is small (D = 2), the adversary’s
classification accuracy is already around 27%. If we increase the distortion to 8, the classification
accuracy further decreases to 3.8%. Figure 5a depicts that even for a large distortion value (D = 8),
the activity classification accuracy only decreases by 18% at most. Furthermore, Figure 5b shows
that the estimated normalized mutual information also decreases as the distortion increases.
5 Conclusion
We have introduced a novel adversarial learning framework for creating private/fair representations
of the data with verifiable guarantees. GAPF allows the data holder to learn the decorrelation scheme
directly from the dataset (to be published) without requiring access to dataset statistics. Under
GAPF, finding the optimal decorrelation scheme is formulated as a game between two players: a
generative decorrelator and an adversary. We have shown that for appropriately chosen loss func-
tions, GAPF can provide guarantees against strong information-theoretic adversaries, such as MAP
and MI adversaries. It can also enforce fairness, quantified via demographic parity by using the
log-loss function. We have also validated the performance of GAPF on Gaussian mixture models
and real datasets. There are several fundamental questions that we seek to address. An immediate
one is to develop techniques to rigorously benchmark data-driven results for large datasets against
computable theoretical guarantees. More broadly, it will be interesting to investigate the robustness
and convergence speed of the decorrelation schemes learned in a data-driven fashion. In this paper,
we connect our objective function in GAPF with demographic parity. Since there is no single metric
for fairness, this leaves room for designing objective functions that link to other fairness metrics
such as equalized odds and equal opportunity.
8
Under review as a conference paper at ICLR 2019
References
Martin Abadi and David G Andersen. Learning to protect communications with adversarial neural
cryptography. arXiv preprint arXiv:1610.06918, 2016.
Martin Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S
Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine
learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.
Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, and Jorge Luis Reyes-Ortiz. A public
domain dataset for human activity recognition using smartphones. In ESANN, 2013.
Y. O. Basciftci, Y. Wang, and P. Ishwar. On privacy-utility tradeoffs for constrained data release
mechanisms. In 2016 Information Theory and Applications Workshop (ITA), pp. 1-6, Jan 2016.
doi: 10.1109/ITA.2016.7888175.
Alex Beutel, Jilin Chen, Zhe Zhao, and Ed H Chi. Data decisions and theoretical implications when
adversarially learning fair representations. arXiv preprint arXiv:1707.00075, 2017.
F. P. Calmon and N. Fawaz. Privacy against statistical inference. In Communication, Control, and
Computing (Allerton), 2012 50th Annual Allerton Conference on, pp. 1401-1408, 2012.
Flavio Calmon, Dennis Wei, Bhanukiran Vinzamuri, Karthikeyan Natesan Ramamurthy, and Kush R
Varshney. Optimized pre-processing for discrimination prevention. In Advances in Neural Infor-
mation Processing Systems, pp. 3992-4001, 2017.
Cynthia Dwork. Differential privacy: A survey of results. In International Conference on Theory
and Applications of Models of Computation, pp. 1-19. Springer, 2008.
Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Found. Trends
Theor. Comput. Sci., 9(3-4):211-407, August 2014. ISSN 1551-305X. doi: 10.1561/0400000042.
URL http://dx.doi.org/10.1561/0400000042.
Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our data,
ourselves: Privacy via distributed noise generation. In Annual International Conference on the
Theory and Applications of Cryptographic Techniques, pp. 486-503. Springer, 2006a.
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity
in private data analysis. In Theory of cryptography conference, pp. 265-284. Springer, 2006b.
Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness
through awareness. In Proceedings of the 3rd innovations in theoretical computer science confer-
ence, pp. 214-226. ACM, 2012.
Jonathan Eckstein and W Yao. Augmented lagrangian and alternating direction methods for convex
optimization: A tutorial and some illustrative computational results. RUTCOR Research Reports,
32, 2012.
Harrison Edwards and Amos Storkey. Censoring representations with an adversary. arXiv preprint
arXiv:1511.05897, 2015.
Benjamin Fish, Jeremy Kun, and Ai daim D Lelkes. A confidence-based approach for balancing fair-
ness and accuracy. In Proceedings of the 2016 SIAM International Conference on Data Mining,
pp. 144-152. SIAM, 2016.
Robert G Gallager. Stochastic processes: theory for applications. Cambridge University Press,
2013.
AmirEmad Ghassami, Sajad Khodadadian, and Negar Kiyavash. Fairness in supervised learning:
An information theoretic approach. arXiv preprint arXiv:1801.04378, 2018.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
9
Under review as a conference paper at ICLR 2019
Jihun Hamm. Minimax filter: Learning to preserve privacy from inference attacks. arXiv preprint
arXiv:1610.03577, 2016.
Moritz Hardt, Eric Price, Nati Srebro, et al. Equality of opportunity in supervised learning. In
Advances in neural information processing systems, pp. 3315-3323, 2016.
Chong Huang, Peter Kairouz, Xiao Chen, Lalitha Sankar, and Ram Rajagopal. Context-aware gen-
erative adversarial privacy. Entropy, 19(12):656, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Alexander Kraskov, Harald Stogbauer, and Peter Grassberger. Estimating mutual information. Phys-
ical review E, 69(6):066138, 2004.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Jiachun Liao, Oliver Kosut, Lalitha Sankar, and Flavio P Calmon. Privacy under hard distortion
constraints. arXiv preprint arXiv:1806.00063, 2018.
Walter E Lillo, Mei Heng Loh, Stefen Hui, and Stanislaw H Zak. On solving constrained optimiza-
tion problems with neural networks: A penalty method approach. IEEE Transactions on neural
networks, 4(6):931-940, 1993.
Changchang Liu, Supriyo Chakraborty, and Prateek Mittal. Deeprotect: Enabling inference-based
access control on mobile sensing applications. arXiv preprint arXiv:1702.06159, 2017.
David Madras, Elliot Creager, Toniann Pitassi, and Richard Zemel. Learning adversarially fair and
transferable representations. arXiv preprint arXiv:1802.06309, 2018.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint
arXiv:1411.1784, 2014.
Tan Nguyen and Scott Sanner. Algorithms for direct 0-1 loss optimization in binary classification.
In International Conference on Machine Learning, pp. 1085-1093, 2013.
Nisarg Raval, Ashwin Machanavajjhala, and Landon P Cox. Protecting visual secrets using adver-
sarial nets. In CVPR Workshop Proceedings, 2017.
D. Rebollo-Monedero, J. Forne, and J. Domingo-Ferrer. From t-Closeness-Like Privacy to Postran-
domization via Information Theory. IEEE Transactions on Knowledge and Data Engineering, 22
(11):1623-1636, November 2010. ISSN 1041-4347. doi: 10.1109/TKDE.2009.190.
S. Salamatian, A. Zhang, F. P. Calmon, S. Bhamidipati, N. Fawaz, B. Kveton, P. Oliveira, and
N. Taft. Managing your private and public data: Bringing down inference attacks against
your privacy. 9(7):1240-1255, 2015. doi: 10.1109/JSTSP.2015.2442227. URL http:
//ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7118663.
L. Sankar, S. R. Rajagopalan, and H. V. Poor. Utility-privacy tradeoffs in databases: An information-
theoretic approach. IEEE Transactions on Information Forensics and Security, 8(6):838-852,
2013.
Prasanna Sattigeri, Samuel C Hoffman, Vijil Chenthamarakshan, and Kush R Varshney. Fairness
gan. arXiv preprint arXiv:1805.09910, 2018.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, et al. Going deeper with convolutions.
Cvpr, 2015.
Ardhendu Tripathy, Ye Wang, and Prakash Ishwar. Privacy-preserving adversarial networks. arXiv
preprint arXiv:1712.07008, 2017.
10
Under review as a conference paper at ICLR 2019
Jacob Whitehill and Javier Movellan. Discriminately decreasing discriminability with learned image
filters. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pp.
2488-2495. IEEE, 2012.
Depeng Xu, Shuhan Yuan, Lu Zhang, and Xintao Wu. Fairgan: Fairness-aware generative adversar-
ial networks. arXiv preprint arXiv:1805.11202, 2018.
Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations.
In International Conference on Machine Learning, pp. 325-333, 2013.
Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating unwanted biases with adver-
sarial learning. arXiv preprint arXiv:1801.07593, 2018.
11
Under review as a conference paper at ICLR 2019
Supplementary Material
A Literature Review
In the context of publishing datasets with privacy and utility guarantees, a number of similar ap-
proaches have been recently considered. We briefly review them and clarify how our work is dif-
ferent. DP-based obfuscators for data publishing have been considered in (Hamm, 2016; Liu et al.,
2017). The author in (Hamm, 2016) considers a deterministic, compressive mapping of the input
data with differentially private noise added either before or after the mapping. The approach in (Liu
et al., 2017) relies on using deep auto-encoders to determine the relevant feature space to add dif-
ferentially private noise, thereby eliminating the need to add noise to the original data. These novel
approaches leverage minimax filters and deep auto-encoders to allow non-malicious entities to learn
some public features from the filtered data, while preventing malicious entities from learning other
sensitive features. Both approaches incorporate a notion of context-aware privacy and achieve better
privacy-utility tradeoffs while using DP to enforce privacy. However, DP can still incur a signifi-
cant utility loss since it assumes worst-case dataset statistics. Our approach models a rich class of
randomization-based schemes via a generative model that allows the generative decorrelator to tailor
the noise to the dataset.
Our work is closely related to adversarial neural cryptography (Abadi & Andersen, 2016), learn-
ing censored representations (Edwards & Storkey, 2015), privacy preserving image sharing (Raval
et al., 2017), privacy-preserving adversarial networks (Tripathy et al., 2017), and adversarially learn-
ing fair representation (Madras et al., 2018) in which adversarial learning is used to learn how to
protect communications by encryption or hide/remove sensitive information or generate fair repre-
sentation of the data. Similar to these problems, our model includes a minimax formulation and
uses adversarial neural networks to learn decorrelation schemes. However, in (Edwards & Storkey,
2015; Raval et al., 2017; Madras et al., 2018), the authors use non-generative auto-encoders to re-
move sensitive information. Instead, we use a GANs-like approach to learn decorrelation schemes
that prevent an adversary from inferring the sensitive variable. Furthermore, these formulations uses
weighted combination of different loss functions to balance privacy with utility. We also go beyond
in formulating a game-theoretic setting subject to a distortion constraint. These approaches are not
equivalent because of the non-convexity (resp. concavity) of the minimax problem with respect to
the decorrelator (resp. adversary) neural network parameters and requires new methods to enforce
the distortion constraint during the training process. The distortion constraint allows us to directly
limit the amount of distortion added to learn the private/fair representation for a variety of learn-
ing tasks, which is crucial for preserving the utility of the learned representation. Moreover, we
compare the performance of the decorrelation schemes learned in an adversarial fashion with the
game-theoretically optimal ones for canonical synthetic data models thereby providing formal veri-
fication of decorrelation schemes that are learned by competing against computational adversaries.
Finally, we propose using mutual information as a criterion to certify that the representations we
learned adversarially against an attacker with a fixed architecture generalize against unseen attack-
ers with (possibly) more complex architecture.
Fair representations using information-theoretic objective functions and constrained optimization
have been proposed in (Calmon et al., 2017; Ghassami et al., 2018). However, both approaches
require the knowledge of dataset statistics, which are very difficult to obtain for real datasets. We
overcome the issue of statistical knowledge by taking a data-driven approach, i.e., learning the
representation from the data directly via adversarial models. In contrast to in-processing approaches
that modify learning algorithms to ensure fair predictions (e..g, using linear programs in (Dwork
et al., 2012; Fish et al., 2016) or via adversarial learning approach in (Zhang et al., 2018)), we focus
on a pre-processing approach to ensure fairness for a variety of learning tasks.
Generative adversarial networks (GANs) have recently received a lot of attention in the machine
learning community (Goodfellow et al., 2014; Mirza & Osindero, 2014). Ultimately, deep genera-
tive models hold the promise of discovering and efficiently internalizing the statistics of the target
signal to be generated. Using GANs to generate synthetic non-sensitive attributes and labels which
ensure fairness while preserving the utility of the data (predicting the label) has been studied in (Xu
et al., 2018; Sattigeri et al., 2018). The goal here is to develop a conditional GAN-based model to
ensure fairness in the system by learning to generate a fairer synthetic dataset using an unconstrained
12
Under review as a conference paper at ICLR 2019
minimax game with carefully designed loss functions corresponding with both fairness and utility.
The synthetic data is generated by a conditional generative adversarial network (GAN) which gen-
erates the non-sensitive attributes-label pair given the noise variable and the sensitive attribute. The
utility is preserved by generating data that is very similar to the original data. To ensure fairness,
the generator generates data samples such that an auxiliary classifier trained to predict the sensitive
attribute from the synthetic data performs as poorly as possible. The methods presented in these
papers are very different from our method since we are focusing on creating a fair/private represen-
tations of the original data while preserving the utility of the representation for a variety of learning
tasks. There are different ways for enforcing fairness, and our work presents a framework that aids
in achieving this goal. More work is needed to be done in this area.
B Theoretical Results of GAPF
Our GAPF framework places no restrictions on the adversary. Indeed, different loss functions and
decision rules lead to different adversarial models. In what follows, we will discuss a variety of
loss functions under hard and soft decision rules, and show how our GAPF framework can recover
several popular information theoretic privacy notions. We will also show that we can obtain a con-
tinuous interpolation between a hard-decision adversary under 0-1 loss function and a soft-decision
adversary under log-loss function using the α-loss function.
Hard Decision Rules. When the adversary adopts a hard decision rule, h(g(X)) is an estimate
of S. Under this setting, We can choose '(h(g(X)), S) in a variety of ways. For instance, if S is
continuous, the adversary can attempt to minimize the difference between the estimated and true
sensitive variable values. This can be achieved by considering a squared loss function
'(h(g(X)),S) = (h(g(X))- S)2,	(4)
which is known as the `2 loss. In this case, one can verify that the adversary’s optimal decision
rule is h = E[S∣g(X)], which is the conditional mean of S given g(X). Furthermore, under the
adversary’s optimal decision rule, the minimax problem in equation 1 simplifies to
min -mmse(S|g(X)) = - maxmmse(S|g(X)),
g(∙)	g(∙)
subject to the distortion constraint. Here mmse(S|g(X)) is the resulting minimum mean square
error (MMSE) under h = E[S|g(X)]. Thus, under the '2 loss, GAPF provides privacy guarantees
against an MMSE adversary. On the other hand, when S is discrete (e.g., age, gender, political
affiliation, etc), the adversary can attempt to maximize its classification accuracy. This is achieved
by considering a 0-1 loss function (Nguyen & Sanner, 2013) given by
'(h(g(X)),S) = 0 0 ifh(g(X))= S .	(5)
,	1 otherwise
In this case, one can verify that the adversary’s optimal decision rule is the maximum a posteri-
oriprobability (MAP) decision rule: h = argmaxs∈s P(s∣g(X)), with ties broken uniformly at
random. Moreover, under the MAP decision rule, the minimax problem in equation 1 reduces to
min -(1 - max P (s, g(X))) = min max P (s, g(X)) - 1,	(6)
g(∙)	s∈S	g(∙) s∈S
subject to the distortion constraint. Thus, under a0-1 loss function, the GAPF formulation provides
privacy guarantees against a MAP adversary.
Soft Decision Rules. Instead of a hard decision rule, we can also consider a broader class of soft
decision rules where h(g(X)) is a distribution over S; i.e., h(g(X)) = Ph(s|g(X)) for s ∈ S. In
this context, we can analyze the performance under a log-loss
'(h(g(X)),s)=log Ph(s∖g(X)).	⑺
In this case, the objective of the adversary simplifies to
max -E[log Pd" = -H(SIg(X)),
h(∙)	Ph(s∣g(X))
13
Under review as a conference paper at ICLR 2019
and that the maximization is attained at Ph^(s∣g(X)) = P(s∣g(X)). Therefore, the optimal adver-
sarial decision rule is determined by the true conditional distribution P (s|g(X)), which we assume
is known to the data holder in the game-theoretic setting. Thus, under the log-loss function, the
minimax optimization problem in equation 1 reduces to
min -H(S|g(X)) = min I(g(X); S) - H(S),
gG)	gG)
subject to the distortion constraint. Thus, under the log-loss in equation 7, GAPF is equivalent to
using MI as the privacy metric (Calmon & Fawaz, 2012).
The 0-1 loss captures a strong guessing adversary; in contrast, log-loss or information-loss models
a belief refining adversary.
B.1 Proof of Theorem 1
Consider the α-loss function (Liao et al., 2018)
'(h(g(X )),s)=α-ι (ι- Ph(SIg(X ))1- 1)，	⑻
for any α> 1. Denoting Hαa (S|g(X)) as the Arimoto conditional entropy of order α, one can verify
that
max-E[ʌ 0 - ph(s∖g(χ))1-1) 1 = -Ha(s∣g(χ)),
h(∙) Ia — 1 ∖	a
which is achieved by a 'a-tilted' conditional distribution
Ph(s∖g(x))
P(s∖g(x ))α
P P(s∖g(x))a
s∈S
Under this choice of a decision rule, the objective of the minimax optimization in equation 1 reduces
to
min -Ha(S∖g(X))= minia(g(X); S) - Ha(S),	(9)
gG)	gG)
where Ia is the Arimoto mutual information and Ha is the Renyi entropy.
B.2	Proof of Corollary 1
For large a (a → ∞), this loss approaches that of the 0-1 (MAP) adversary in the limit. As a
decreases, the convexity of the loss function encourages the estimator S to be probabilistic, as it
increasingly rewards correct inferences of lesser and lesser likely outcomes (in contrast to a hard
decision rule by a MAP adversary of the most likely outcome) conditioned on the revealed data. As
a → 1, equation 8 yields the logarithmic loss, and the optimal belief P^ is simply the posterior
belief. Therefore, using a-loss, we can obtain a continuous interpolation between a hard-decision
adversary under 0-1 loss (a → ∞) and a soft-decision adversary under log-loss function (a → 1).
B.3	Proof of Proposition 1
Let’s consider an arbitrary target variable Y which a user is interested in learning from the data. The
objective of the learning task is to train a good model that takes X to predict Y. Thus, We have the
Markov chain: S → X → X → Y, where Y is an estimate of Y from the trained machine learning
model. According to data processing inequality, we have I(S; X) ≥ I(S; Y). As we have shown
in the above analysis, for the log-loss function, the objective of GAPF is equivalent to minimizing
I(S; X) , which is an upperbound on I(S; Y). Notice that demographic parity requires S and Y
to be independent, which is equivalent to I (S; Y) = 0. Since mutual information is non-negative,
GAPF ensures fairness by minimizing an upperbound of I(S; Y) subject to the distortion constraint
under the log-loss function. As the distortion increases, the ensuing fairness guarantee approaches
ideal demographic parity by enforcing I(S; Y) ≤ I(S; X) = 0.
14
Under review as a conference paper at ICLR 2019
C Alternate Minimax Algorithm
In this section, we present the alternate minimax algorithm to learn the GAPF scheme from a dataset.
The alternating minimax privacy preserving algorithm is presented in Algorithm 1. To incorporate
Algorithm 1 Alternating minimax privacy preserving algorithm
Input: dataset D, distortion parameter D, iteration number T
Output: Optimal generative decorrelator parameter θp
procedure ALERNATE MINIMAX(D, D, T)
Initialize θp* 1 * * * and θa1
fort = 1,..., T do
Random minibatch of M datapoints {x(1), ..., x(M)} drawn from full dataset
Generate {X(i),...,X(M)} via X(i)= g(x(i), s(i)； θp)
Update the adversary parameter θat+1 by stochastic gradient ascend for j epochs
1M
θa+1 = θa+αtvθaM £-'(h(x(i)； θa), s(i)),	3 > 0
i=1
Compute the descent direction Vθ⅛ l(θp, θ/+1), where
1M
'(θp, θa+1) =- M £'(h(g(Xco, s(i); θp)； θa+1), Sei))
M i=1
subject to MM PMi[d(g(x(i),s(i)； θp),x(i))] ≤ D
Perform line search along Vθt l(θpt, θat+1) and update
θp+1 = θp -αtVθp'(θp,θa+1)
Exit if solution converged
return θpt+1
the distortion constraint into the learning algorithm, we use the penalty method (Lillo et al., 1993)
and augmented Lagrangian method (Eckstein & Yao, 2012) to replace the constrained optimization
problem by a series of unconstrained problems whose solutions asymptotically converge to the solu-
tion of the constrained problem. Under the penalty method, the unconstrained optimization problem
is formed by adding a penalty to the objective function. The added penalty consists of a penalty
parameter ρt multiplied by a measure of violation of the constraint. The measure of violation is
non-zero when the constraint is violated and is zero if the constraint is not violated. Therefore, in
Algorithm 1, the constrained optimization problem of the decorrelator can be approximated by a
series of unconstrained optimization problems with the loss function
1M	1M
'(θp ,θa+1)=- M χ '(h(g(χ(i) ； θp )； θa+1 ),s(i))+ρt(maχ{0, M χ d(g(x(i)； θp ),x(i))- D})2,
i=1	i=1
(10)
15
Under review as a conference paper at ICLR 2019
where ρt is a penalty coefficient which increases with the number of iterations t. For convex opti-
mization problems, the solution to the series of unconstrained problems will eventually converge to
the solution of the original constrained problem (Lillo et al., 1993).
The augmented Lagrangian method is another approach to enforce equality constraints by penalizing
the objective function whenever the constraints are not satisfied. Different from the penalty method,
the augmented Lagrangian method combines the use of a Lagrange multiplier and a quadratic
penalty term. Note that this method is designed for equality constraints. Therefore, we introduce
a slack variable δ to convert the inequality distortion constraint into an equality constraint. Using
the augmented Lagrangian method, the constrained optimization problem of the decorrelator can be
replaced by a series of unconstrained problems with the loss function given by
1M	ρ1M
'(θp, θa , δ) = - M X '(h(g(x(i); θp); θa+ ), Sei)) + ^2( M X d(g(x(i); θP), Xei)) + δ - D)
M	2M
i=1	i=1
(11)
1M
-λt(M Ed(O(X(i); θP), XCi)) + δ - D),
i=1
where ρt is a penalty coefficient which increases with the number of iterations t and λt is updated
M
according to the rule λt+ι = λt - Pt(吉 P d(g(x(i)； θp),x(i)) + δ - D). For convex optimiza-
i=1
tion problems, the solution to the series of unconstrained problems formulated by the augmented
Lagrangian method also converges to the solution of the original constrained problem (Eckstein &
Yao, 2012).
D GAPF for Gaussian Mixture Models
D.1 Proof of Theorem 2
Proof Since EXX[d(X, X)] = EXXkX - X∣∣2 = EkZ + βk2 = ∣∣βk2 + tr(Σp), the distortion
constraint implies that ∣β∣2 + tr(∑p) ≤ D. Let us consider X = X + Z + β, where β ∈ R
and Σp is a diagonal covariance whose diagonal entries is given by {σp2 , ..., σp2 }. Given the MAP
adversary’s optimal inference accuracy in equation 3, the objective of the decorrelator is to
mΣin Pd(G)	(12)
β,Σp
Define 1--q = η. The gradient of Pd ∂PdG)	1	1 K =P 1-√πe +(1 - p)( I八一 =2√∏(pe ln η + α2√2π Note that 〜-(-2 + 1lnη)2	〜 pe	2	p ---(-2- 1lnn)2	1 - Pe (1 — p)e	2	G) w.r.t.α is given by (-2+aln η)2∖ (	1	1	∖ --—)(-2 - 02ln η)	(13) (1	- (-2- 1lnη)2 ʌ 1	1	1 ,、 j√πe	2 八-2 + 02ln η) (-2 + 1ln η)2	(-2 - 1ln η)2∖ 2	+ (1 - p)e	2	(14) (〜-(-2+1lnη)2	/	-(-2- 1lnη)2 ʌ Pe	2	— (1 — p)e	2	. (-2 - 1ln η)2-(-2 +1 ln η)2	P	2ln η	P	1 	T	 = ɪe T = ɪeln η = 1. 1 — P	1 — P (15) 16
kβk2 + tr(Σp) ≤ D
Under review as a conference paper at ICLR 2019
Therefore, the second term in equation 14 is 0. Furthermore, the first term in equation 14 is always
positive. Thus, Pd(G) is monotonically increasing in α. As a result, the optimization problem in
equation 12 is equivalent to
min
β,Σp
s.t.
(2μ)T (夕 + ςP)12μ
(16)
kβk2 + tr(Σp) ≤ D.
The objective function in equation 16 can be written as
2 [μ1 μ2 ...	μm]
1
σ2+σP1
0
0
1
σ2 + σp2
1
σm +σPm
μι
μ2
-μm-
X	4μ2
i=i σ2 + σPi .
0
0
2
0
0
Thus, the optimization problem in equation 16 is equivalent to
min
β,σp21,...,σp2m
s.t.
m2
X μi
乙σ + σ
i=1 σi + σpi
kβk2 + tr(Σp) ≤ D
σp2 ≥ 0 ∀i ∈ {1, 2, ...m}.
(17)
Since a non-zero β does not affect the objective function but result in positive distortion, the optimal
scheme satisfies β = (0, ..., 0). Furthermore, the Lagrangian of the above optimization problem is
given by
m
m
m2
L(σpi ,...,σpm ,λ)= X σ⅛
+ λ0(	σp2i - D) -	λiσp2i ,
(18)
i=1
i=1
Where λ = {λ0, ..., λm} denotes the Lagrangian multipliers associated With the constraints. Taking
the derivatives ofL(σp21, ..., σp2m, λ) With respect to σp2i, ∀i ∈ {1, ..., m}, We have
dL(σpi ,…，σPm , λ)
∂σ2.
pi
—
R¾J + λ0-λi.
(19)
Notice that the objective function in equation 16 is decreasing in σp2i, ∀i ∈ {1, ..., m}. Thus, the
m
optimal solution σp.2 satisfies P σp.2 = D. By the KKT conditions, we have
i=1
dL(σpi ,…,σPm，λ) I
dσPi	lσpi=σPi2,λ=λ*
(σ2 :σPi2)2 + λ≡ r=0.
(20)
—
Since λ*,i ∈ {0,1,..., m} is dual feasible, We have λ* ≥ 0,i ∈ {0,1,…，m}. Therefore
λ* ≥	μ2
0 ≥ (σ + σPi2)2 .
22
If λg > μ4, we have λ0 > dW* 2)2. ThiS implies λ* > 0. Thus, by complementary slackness,
pi	2
σpi = 0. On the other hand, if λ0 < μ4, we have σpj > 0. Furthermore, by the complementary
σi
slackness condition, λ*σp.2 = 0, ∀σp.2. This implies λ* = 0, ∀σp.2 > 0. As a result, for all
σp.2 > 0, we have
M|
pʌs
(21)
Therefore, σp.2 = max{√* - σ2,0}
丁 2 _L 丁* 2
σi + σPi
m
with P σX2
i=1
D.
Substitute this
optimal solution into equation 3 with α = vz(2μ)T(Σ + Σp)-12μ, we obtain the accuracy of the
MAP adversary.	口
17
Under review as a conference paper at ICLR 2019
(X, S)
Generative
Decorrelator
Figure 6: Neural network structure of linear GAPF for Gaussian mixture data
Adversary
λ
S
We observe that the when σi is greater than some threshold -lμiL, no noise is added to the data
λ0
on this dimension due to the high variance. When σi is smaller than 邛工,the amount of noise
λ0
added to this dimension is proportional to ∣μ∕; this is intuitive since a large ∣μ∕ indicates the two
conditionally Gaussian distributions are further away on this dimension, and thus, distinguishable.
Thus, more noise needs to be added in order to reduce the MAP adversary’s inference accuracy.
D.2 Data-driven Approach
For the data-driven linear GAPF scheme, we assume the generative decorrelator only has access to
the dataset D with n data samples but not the actual distribution of (X, S). Computing the optimal
decorrelation scheme becomes a learning problem. In the training phase, the data holder learns
the parameters of the GAPF scheme by competing against a computational adversary modeled by
a multi-layer neural network. When convergence is reached, we evaluate the performance of the
learned scheme by comparing with the one obtained from the game-theoretic approach. To quantify
the performance of the learned GAPF scheme, we compute the accuracy of inferring S under a
strong MAP adversary that has access to both the joint distribution of (X, S) and the decorrelation
scheme.
Since the sensitive variable S is binary, we measure the training loss of the adversary network by
the empirical log-loss function
1n
Ln(θp,θa) =	-—	£s(i)	log	h(g(x(i)；	Op)； θa) + (1 -	S(i))lθg(1 -	h(g(x(i)；	θp)	θa)) .	(22)
n i=1
For a fixed decorrelator parameter θp, the adversary learns the optimal θ^ by maximiz-
ing equation 22. For a fixed θα, the decorrelator learns the optimal θp by minimizing
-Ln(h(g(X; θp); θa),S) subject to the distortion constraint EXX IlX - X∣∣2 ≤ D.
As shown in Figure 6, the decorrelator is modeled by a two-layer neural network with parameters
θp = {β0, ..., βm, σp0, ..., σpm}, where βk and σpk represent the mean and standard deviation for
each dimension k ∈ {1, ..., m}, respectively. The random noise Z is drawn from a m-dimensional
independent zero-mean standard Gaussian distribution with covariance ∑ι. Thus, We have Xk =
Xk + βk + σpkZk. The adversary, whose goal is to infer S from privatized data X, is modeled by a
three-layer neural network classifier with leaky ReLU activations.
To incorporate the distortion constraint into the learning process, we add a penalty term to the
objective of the decorrelator. Thus, the training loss function of the decorrelator is given by
1n
L(θp,θa) = Ln(θp,θa) + ρt(max{0, - E d(g(x(i); θp), X(i)) - D})2,	(23)
n i=1
18
Under review as a conference paper at ICLR 2019
where ρt is a penalty coefficient which increases with the number of iterations t. The added penalty
consists of a penalty parameter ρ multiplied by a measure of violation of the constraint. This measure
of violation is non-zero when the constraint is violated. Otherwise, it is zero.
D.3 Experiment Setup
We use synthetic data generated by Gaussian mixture model as our first attempt to evaluate the
performance of the learned GAPF schemes. Each dataset contains 20K training samples and 2K
test samples. Each data entry is sampled from an independent multi-dimensional Gaussian mixture
model. We consider two categories of synthetic datasets with P (S = 1) equal to 0.75 and 0.5,
respectively. Both the decorrelator and the adversary in the GAPF framework are trained on Tensor-
flow (Abadi et al., 2016) using Adam optimizer with a learning rate of 0.005 and a minibatch size of
1000. The distortion constraint is enforced by the penalty method as detailed in supplement C (see
equation 10).
E GAPF Architecture for GENKI and HAR Datasets
Figure 7: Feedforward neural network decorrelator
The FNND is modeled by a four-layer feedforward neural network. We first reshape each image
to a vector (256 × 1), and then concatenate it with a 100 × 1 Gaussian random noise vector. Each
entry in the noise vector is sampled independently from a standard Gaussian distribution. We feed
the entire vector to a four-layer fully connected (FC) neural network. Each layer has 256 neurons
with a leaky ReLU activation function. Finally, we reshape the output of the last layer to a 16 × 16
image. To model the TCNND, we first generate a 100 × 1 Gaussian random vector and use a linear
projection to map the noise vector to a 4 × 4 × 256 feature tensor. The feature tensor is then fed
to an initial transposed convolution layer (DeCONV) with 128 filters (filter size 3 × 3, stride 2)
and a ReLU activation, followed by another DeCONV layer with 1 filter (filter size 3 × 3, stride 2)
and a tanh activation. The output of the DeCONV layer is added to the original image to generate
the processed data. For both decorrelators, we add batch normalization (Ioffe & Szegedy, 2015) on
each hidden layer to prevent covariance shift and help gradients to flow. We model the adversary
using convolutional neural networks (CNNs). This architecture outperforms most of other models
for image classification (Krizhevsky et al., 2012; Szegedy et al., 2015).
Figure 9 illustrates the architecture of the adversary. The processed images are fed to two convolu-
tion layers (CONV) whose sizes are 3 × 3 × 32 and 3 × 3 × 64, respectively. Each convolution layer
is followed by ReLU activation and batch normalization. The output of each convolution layer is fed
to a 2 × 2 maxpool layer (POOL) to extract features for classification. The second maxpool layer is
followed by two fully connected layers, each contains 1024 neurons with a batch normalization and
a ReLU activation. Finally, the output of the last fully connected layer is mapped to the output layer,
which contains two neurons capturing the belief of the subject being a male or a female.
19
Under review as a conference paper at ICLR 2019
Figure 8: Transposed convolution neural network decorrelator
Figure 9: Convolutional neural network adversary
For the HAR dataset, We first concatenate the original data with a 100 × 1 Gaussian random noise
vector. We then feed the entire 661 × 1 vector to a Feed Forward neural network with three hidden
fully connected (FC) layers. Each hidden layer has 512 neurons with a leaky ReLU activation.
Finally, we use another FC layer with 561 neurons to generate the processed data. For the adversary,
we use a five-layer feedforward neural network. The hidden layers have 512, 512, 256, and 128
neurons with leaky ReLU activation, respectively. The output of the last hidden layer is mapped to
the output layer, which contains 30 neurons capturing the belief of the subject’s identity. For both
decorrelator and adversary, we add a batch normalization after the output of each hidden layer.
F Mutual Information Estimation
Our GAPF framework offers a scalable way to find a (local) equilibrium in the constrained min-max
optimization, under certain attacks (e.g. attacks based on a neural network). Yet the privatized data,
through our approach, should be immune to any general attacks and ultimately achieving the goal of
decreasing the correlation between the privatized data and the sensitive labels. Therefore we use the
estimated mutual information to certify that the sensitive data indeed is protected via our framework.
We use the nearest k-th neighbor method(Kraskov et al., 2004) to estimate the entropy HH given by
dN
H(X) = ψ(N) - ψ(k) + log(cd) + N 代 log Ti
(24)
20
Under review as a conference paper at ICLR 2019
where r is the distance of the i-th sample Xi to its k-th nearest neighbor, ψ is the digamma function,
d/2
Cd = r(∏+d∕2) in Euclidean norm, and N is the number of samples. Notice that X is learned
representation and S is the sensitive variable. Then, we calculate the mutual information using
ʌ, ʌ ʌ , ʌ. ʌ , ʌ.
I(X; S) = H(X) - H(XIS)
For a binary sensitive variable, we can simplify the empirical MI to
I(X; S) = H(X) - (P(S = 1) H(X ∣S =1)+ P (S = 0) H(X ∣S = 0)),	(25)
where P (S = 1) andP (S = 0) can be approximated by the empirical probability.
One noteworthy difficulty is that X usually lives in high dimensions (e.g. each image has 256
dimensions in GENKI dataset) which is almost impossible to calculate the empirical entropy based
on raw data due to the sample complexity. Thus, we train a neural network that classifies the sensitive
variable from the learned data representation to reduce the dimension of the data. We choose the
layer before the softmax outputs (denoted by Xf) to be the feature embedding that has a much lower
dimension than original X and also captures the information about the sensitive variable. We use
Xf as a surrogate of X for estimating the entropy. The resulting approximate MI is
ʌ , ʌ ʌ , ʌ ʌ , ʌ .
I(Xf ； S) = H(Xf) - H(Xf ∣S)
=H(Xf) - (P(S = 1)H(Xf ∣S =1) + P (S = 0)H(Xf ∣S = 0)).
Following the same manner, the MI between the learned representation X and the label Y is ap-
proximated by I(Xf ; Y), where Xf is the feature embedding that represents a privatized image
X.
For the GENKI dataset, we construct a CNN initialized by two conv blocks, then followed by two
fully connected (FC) layers, and lastly ended with two neurons having the softmax activations. In
each conv block, we have a convolution layer consisting of filters with the size equals 3 × 3 and the
stride equals 1, a 2 × 2 max-pooling layer with the stride equals 2, and a ReLU activation function.
Those two conv blocks have 32 and 64 filters respectively. We flatten out the output of second conv
block yielding a 256 dimension vector. The extracted features from the second conv layers is passed
through the first FC layer with batch normalization and ReLU activation to get a 8-dimensional
vector, followed with the second FC layer to output a 2 dimensional vector that applied with the
softmax function. The aforementioned 8-dimensional vector is the feature embedding vector Xf in
our empirical MI estimation.
Estimating mutual information for HAR dataset has a slightly different challenge, as the alphabetic
size of values that the sensitive label (i.e. identity) can take is 30. Thus, it requires at least 30
neurons prior to the output layer of the corresponding classification task. In fact we pose 128 neurons
before the final softmax output layer in order to get a reasonably good classification accuracy. Using
the 128-dimensional vector as our feature embedding to calculate mutual information is almost
impossible due to the curse of dimensionality. Therefore, we apply Principal Component Analysis
(PCA), shown in Figure 10, and pick the first 12 components to circumvent this issue. The resulting
12-dimensional vector is considered to be an approximate feature embedding that encapsulates the
major information of the processed data.
21
Under review as a conference paper at ICLR 2019
O	10	20	30
Principal components
(a) Top 32 principal components out of the 561 features with dif-
ferent distortion D
Figure 10: PCA for processed data in HAR
22