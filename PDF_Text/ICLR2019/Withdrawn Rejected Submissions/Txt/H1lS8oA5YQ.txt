Under review as a conference paper at ICLR 2019
Feature Attribution As Feature Selection
Anonymous authors
Paper under double-blind review
Ab stract
Feature attribution methods identify “relevant” features as an explanation of a
complex machine learning model. Several feature attribution methods have been
proposed; however, only a few studies have attempted to define the “relevance”
of each feature mathematically. In this study, we formalize the feature attribution
problem as a feature selection problem. In our proposed formalization, there arise
two possible definitions of relevance. We name the feature attribution problems
based on these two relevances as Exclusive Feature Selection (EFS) and Inclusive
Feature Selection (IFS). We show that several existing feature attribution meth-
ods can be interpreted as approximation algorithms for EFS and IFS. Moreover,
through exhaustive experiments, we show that IFS is better suited as the formal-
ization for the feature attribution problem than EFS.
1	Introduction
Feature attribution methods (Simonyan et al., 2013; Springenberg et al., 2014), or saliency maps,
are one of the most popular approaches for explaining the decisions of complex machine learning
models such as deep neural networks. In feature attribution, for each given instance, the feature
attribution methods score how strongly each feature is relevant to the model’s decision. An informal
definition of the feature attribution problem can be described as follows. We note that this definition
is incomplete because the “relevance” of each feature is not defined.
Feature Attribution Problem Given the model f and the target input x ∈ Rd to be explained, score
si ∈ R to each feature xi (i = 1, 2, . . . , d) so that si ≥ sj if the feature xi is more relevant to the
model’s decision than the feature xj .
With feature attribution methods, the relevant features can be obtained as explanations why the mod-
els made certain decisions. For example, in image recognition, feature attribution methods highlight
pixels which the models have focused on, by scoring the relevance of each pixel (Simonyan et al.,
2013; Springenberg et al., 2014; Bach et al., 2015; Smilkov et al., 2017), and in text classification,
they detect the set of words or sentences relevant to the model’s decision by scoring each word or
sentence (Ding et al., 2017; Chen et al., 2018).
The major approaches for feature attribution are based on gradient and its modifications (Simonyan
et al., 2013; Springenberg et al., 2014; Bach et al., 2015; Smilkov et al., 2017; Shrikumar et al.,
2017) and feature occlusions (Zeiler & Fergus, 2014; Zhou et al., 2014).
Most of the studies proposed computational algorithms without defining the “relevance” mathemat-
ically (except for some axiomatic approaches (Sundararajan et al., 2017; Lundberg & Lee, 2017)).
This means that it is not clear what these algorithm outputs, and we cannot compare these outputs
rigorously. To clarify the situation and to establish solid feature attribution methods, we pose the
following research questions: (Q1) how can we define relevance? (Q2) is there a general framework
for the relevance that induces existing feature attribution methods? and (Q3) what is an appropriate
definition of relevance?
In this study, we formalize the feature attribution problem as feature selection problem, and thereby
answer questions (QI)-(Q3). In our proposed formalization, there arise two possible definitions of
relevance. We name the feature attribution problems based on these two relevances as Exclusive
Feature Selection (EFS) and Inclusive Feature Selection (IFS).
Below, we summarize our contributions.
1
Under review as a conference paper at ICLR 2019
Figure 1: Reorganizing feature attribution methods as Exclusive Feature Selection (EFS) and In-
clusive Feature Selection (IFS). See the references for the details of each method: Grad (Simonyan
et al., 2013), Grad×Input (Shrikumar et al., 2016), IntGrad (Sundararajan et al., 2017), Smooth-
Grad (Smilkov et al., 2017; Hooker et al., 2018), LIME (Ribeiro et al., 2016), SHAP (Lundberg &
Lee, 2017), GuidedBP (Springenberg et al., 2014), ExciteBP (Zhang et al., 2016), LRP (Bach et al.,
2015), DeepTaylor (Montavon et al., 2017), DeepLIFT (Shrikumar et al., 2017), Occlusion (Zeiler
& Fergus, 2014), Detector (Zhou et al., 2014), Anchor (Ribeiro et al., 2018), Meaningful Perturba-
tion (Fong & Vedaldi, 2017), and PertMap (Hara et al., 2018; Ikeno & Hara, 2018)
Answer to Q1: We introduce two formalizations, namely EFS and IFS (Section 2). We formal-
ize the feature attribution problem as feature selection problem, because the goal of feature attribu-
tion is to identify the relevant features to the model’s decision. Here, we point out that there are
two possible approaches for characterizing the relevance of the features. In the first approach, EFS,
we exclude some features from the model, and if the model’s decision changes by the exclusion,
we infer the excluded features are relevant since they have certain impacts to the decision. In the
second approach, IFS, we include some features to the model, and if the model’s decision remains
unchanged after the inclusion, we infer the included features are relevant since they are essential to
the decision.
Answer to Q2: The existing methods are based on the relevances of EFS and IFS (Section 3).
We show that several existing feature attribution methods can be interpreted as approximation al-
gorithms for EFS or IFS, as summarized in Figure 1. For example, the gradient-based methods are
one-step gradient descent for the continuous relaxation of EFS and IFS.
Answer to Q3: The relevance based on IFS is better suited for the feature attribution problem
(Section 5). We observe that IFS is better suited as the formalization for the feature attribution prob-
lem than EFS. Through exhaustive experiments, we found two crucial properties of EFS. First, the
optimal solution to EFS is very similar to adversarial example (Szegedy et al., 2013). As adversar-
ial examples generally provide seemingly meaningless attributions, they are not appropriate for the
purpose of explanation. Second, we empirically observe that even a random attribution can perform
comparably well with some of the existing feature attribution methods under the evaluation based
on EFS. This observation indicates that there are only subtle differences between good attributions
and random attributions under EFS. In contrast, unlike EFS, IFS has no trivial drawbacks, and we
argue that IFS would be an appropriate formalization for the feature attribution problem.
In this paper, we use the following notation, and consider the problem setting as follows.
Notation For any positive integer d, [d] denotes the set [d] = {1, 2, . . . , d}. We denote d-
dimensional vectors with all zeros as 0d. For a proposition a, I(a) denotes the indicator of a, i.e.,
I(a) = 1 if a is true, and I(a) = 0 if a is false.
2
Under review as a conference paper at ICLR 2019
Settings In this paper, we consider the classification model f for C categories that return an output
y ∈ RC for a given data x ∈ Rd, i.e., y = f (x). The classification result is determined by
c = argmaxj yj where yj = fj (x) is the j-th element of the output. We assume that the model f is
differentiable with respect to the input x: the target models therefore include linear models, kernel
models with differentiable kernels, and deep neural networks. We assume that the model f and the
target input x to be explained are given and fixed.
2	Feature Attribution as Feature Selection
(Q1) How can we define the relevance?
As an answer to this question, we formalize the feature attribution problem as feature selection
problem, and introduce two definitions of relevance.
Before formalizing the problem, we introduce the idea of data corruption (Samek et al., 2017; Fong
& Vedaldi, 2017), which plays an important role in this study. Here, we consider corrupting the
input data x by overlaying partial features with a noise r ∈ Rd , as follows.
Definition 2.1 (Data Corruption). For a vector x ∈ Rd, the corruption of x with the set S ⊆ [d] and
the vector r ∈ Rd is given by xS,r, which is defined below.
ri if i ∈ S,
xi otherwise.
(2.1)
We refer to the set S as corrupted features.
Here, we assume that the noise r follows a distribution p(r). In Section 5, we introduce two types
of noises r for the images; we overlay the image to be explained with random noises and random
real images.
We now consider the problem formalization. Recall that the objective of feature attribution is to
provide high scores to relevant features to the model’s decision and low scores to irrelevant features.
Our idea is to define the relevance and irrelevance using data corruption. Specifically, in this study,
we consider two types of feature selection problems based on data corruption. We name those two
problems as Exclusive Feature Selection (EFS) and Inclusive Feature Selection (IFS).
2.1	Exclusive Feature Selection (EFS)
One way of measuring the relevance of features is to corrupt some of the features by overlaying
with uninformative values and observe how the model’s decision changes. If the corruption of
certain features leads to a decision change, such features can be considered as “relevant”. We note
that corrupting many features easily leads to a decision change. Therefore, our focus is mainly on a
small number of crucial features that can change the decision. We formalize this idea as Exclusive
Feature Selection (EFS). In EFS, we aim at changing the decision of the model f to a class different
from c by corrupting only a small number of features. See Figure 2 for the idea of EFS. The idea of
EFS was originally proposed for measuring the performance of feature attribution methods (Samek
et al., 2017). Here, we define EFS as follows.
Definition 2.2 (Exclusive Feature Selection (EFS)). Find the feature corruption S ⊆ [d] such that
(i) the number of corrupted features |S| is small, and (ii) the corrupted data xS,r has small intensity
at class c, i.e. fc(xS,r) is small, so that the corrupted data is classified into a different class.
SEFS := argminS⊆[d] |S| + λEr[fc(xS,r)],
(2.2)
where Er denotes the expectation over the noise r, and λ > 0 is a weight parameter determined by
the user.
In this definition, we consider the expected intensity Er [fc(xS,r)] over the noise r so to avoid the
corruption to overfit a specific realization of the noise r.
By using the solution of EFS, we can define relevance as a binary score as follows. That is, in EFS,
the relevant features are the ones when excluded from the data lead to the model’s decision change.
Definition 2.3 (EFS-Relevance). The relevance of each feature xi is defined by si := I(i ∈ SEFS).
3
Under review as a conference paper at ICLR 2019
])r,Sx(cf[rE
6420
...
000
Figure 2: The idea of EFS: Within the trade-off between intensity Er [fc(xS,r)] and the number of
corrupted features |S |, find S that minimizes the intensity and its size. The red pixels indicate the
corrupted features S. The corrupted features S in the second image is optimal in this curve.
])r,Sx(cf[rE
186420
...........
0000
0
Figure 3: The idea of IFS: Within the trade-off between intensity Er [fc (xS,r)] and the number of
corrupted features |S |, find S that maximizes the intensity and its size. The red pixels indicate the
corrupted features S . The corrupted features S in the fourth image is optimal in this curve.
2.2	Inclusive Feature Selection (IFS)
Data corruption can be used for measuring the relevance of features in a way different from that of
EFS. If the corruption of certain features does not change the model’s decision, such features can
be considered as “irrelevant”. We note that zero corruption trivially keeps the decision unchanged.
Therefore, our focus is mainly on a small number of crucial features that have to be kept to maintain
the decision. Therefore, in Inclusive Feature Selection, we aim at maintaining the decision of the
model f in the class c while corrupting as many features as possible. See Figure 3 for the idea of
IFS. Here, we formally define IFS as follows.
Definition 2.4 (Inclusive Feature Selection (IFS)). Find the feature corruption S ⊆ [d] such that (i)
the number of corrupted features |S| is large, and (ii) the corrupted data xS,r has high intensity at
class c, i.e. fc(xS,r) is large, so that the corrupted data is classified to the class c.
SIFS := argmaxS⊆[d] |S| + λEr[fc(xS,r)].	(2.3)
By using the solution of IFS, we can define relevance as follows. In IFS, the relevant features are
the ones that when included in the data keep the model’s decision unchanged.
Definition 2.5 (IFS-Relevance). The relevance of each feature xi is defined by si := I(i ∈/ SIFS).
4
Under review as a conference paper at ICLR 2019
3	Feature Attribution Methods as EFS and IFS
(Q2) Is there a general framework for the relevance that induces existing feature attribution meth-
ods?
As an answer to this question, we show that the existing feature attribution methods can be inter-
preted as approximation algorithms for EFS or IFS. Thus, the relevances considered in the existing
methods are approximated versions of EFS-Relevance and IFS-Relevance. To show this, we clas-
sify the existing methods into three types of approaches for solving EFS and IFS: occlusion-based,
optimization-based, and gradient-based. See Figure 1 for the overview.
3.1	Occlusion-based Approaches
Occlusion-based feature attribution methods (Zeiler & Fergus, 2014; Zhou et al., 2014; Ribeiro
et al., 2018) measure the relevance by partially masking features. In those methods, the features
are masked by sliding windows or patches, and the change of the output fc is computed. This can
be interpreted as an approximation algorithm for solving the problems (2.2) and (2.3). Instead of
searching over exponentially large solution candidates S ⊆ [d], those methods search only over the
subset of the solution candidates. For example, one prepares a set of feature subsets {Sm : Sm ⊆
[d]}mM=1 , and searches for an optimal combination of the subsets by using a greedy algorithm (Zhou
et al., 2014) or by a bandit algorithm (Ribeiro et al., 2018).
3.2	Optimization-based Approaches
In order to introduce the optimization-based feature attribution methods, we consider the continuous
relaxation of EFS and IFS, as follows.
Definition 3.1 (Continuous Corruption). For a vector x ∈ Rd, the continuous corruption of x with
the vector W ∈ [0,1]d and the vector r ∈ Rd is given by Xw,r, which is defined below.
(Xw,r)i = (1 - Wi)Xi + Wirri.	(3.1)
Here, the vector w can be interpreted as the continuous relaxation of the indicator of the set S.
Definition 3.2 (Continuous EFS (C-EFS)). Find the vector W ∈ [0, 1]d such that (i) the amount of
corruption Pid=ι Wi is small, and (ii) the corrupted data Xw,r has small intensity at class c:
WEFS := argminw∈[0,ι]d Pi=ι wi + λEr[fc(xw,r)].	(3.2)
Definition 3.3 (Continuous IFS (C-IFS)). Find the vector W ∈ [0, 1]d such that (i) the amount of
corruption Pid=ι Wi is large, and (ii) the corrupted data Xw,r has large intensity at class c:
WIFS := argmaxw∈[0,1]d Pi=1 Wi + λEr[fc(xw,r )].	(3.3)
For C-EFS and C-IFS, the feature attribution scores can be defined as si = WEFS,i and si = 1 -
WIFS,i, respectively. We note that, for a differentiable model f, the objective functions of C-EFS
(3.2) and C-IFS (3.3) are differentiable. Therefore, these problems can be solved using gradient-
based optimization methods such as SGD and Adam (Kingma & Ba, 2014).
Fong & Vedaldi (2017) first introduced the formulation of C-EFS, and they proposed Meaningful
Perturbation by adding a smoothness penalty term to C-EFS. PertMap (Hara et al., 2018; Ikeno
& Hara, 2018), another optimization-based method, can be interpreted as a variant of C-IFS.
PertMap is equivalent to C-IFS with the term Er[fc(Xw,r)] replaced with the hinge penalty term
Pj=C Ermin(0, fc(Xw,r) — fj(Xw,r))] that penalizes W only when the corrupted data is classified
into other classes. See Appendix A for the detail.
3.3	Gradient-based Approaches
Many feature attribution methods are based on the gradient of the model,s output dfc(X). Here,
∂ xi
we point out that those gradient-based feature attribution methods can be interpreted as one-step
gradient descent/ascent approximations for C-EFS and C-IFS. If we consider solving the problems
5
Under review as a conference paper at ICLR 2019
(3.2) and (3.3) using gradient descent/ascent with the initial w set to zeros, i.e. w =
the first step of the optimization can be expressed as Wi J 0 ± η < Er f c∂Ww,r) I
wi	w=0d
0d, then,
+ λ > H
d∂c(x) (μi 一 Xi) + λ, where η > 0 is the step size and μi := E"%]. Here, because the penalty λ
and the step size η are common across all the features, fXx)(μi 一 Xi) is the essential term that
determines the size of wi . This finding naturally connects EFS/IFS and the gradient-based feature
attribution methods. See Appendix B for the details.
4	EFS and IFS as Evaluation Metric
The ideas of EFS and IFS can be used as metrics for evaluating the performance of feature attribution
methods. Suppose that the feature attribution score s ∈ Rd is given.
EFS-based Metric The principle of EFS is that “crucial attribution should change the model’s
decision by corrupting only a small number of relevant features”. For q ∈ [0, 1], let the set of the
top-100q% relevant features be Sq := {i : si ≥ tq}, where tq is the 100q-th largest percentile in s
so that |Sq | = qd. Then, we can draw a curve showing the trade-off between the ratio of corrupted
relevant features and the degree of the output change (Samek et al., 2017). For example, as the degree
of the output changes, we can use the expected difference in intensity: gcintensity(Sq) := Er[fc(X) 一
fc(XSq,r)], and the probability of label mismatch: gclabel (Sq) := Er [I(c 6= argmaxj fj (XSq,r))].
The trade-off curve usually shows an increasing trend, and the area under the curve can be used as
a measure of how quickly the output changes with an increase in the ratio of corrupted features (see
Figure 4). In this paper, we refer to this area as Area Under the EFS-Curve (AUEC).
IFS-based Metric The principle of IFS is that “crucial attribution should maintain the model’s
output even if many irrelevant features are corrupted”. Similar to EFS, we can construct an IFS-
based metric based on this principle. Let Sq := {i : si ≥ tq} be the top-100q% relevant features,
as defined above. We then corrupt irrelevant features other than Sq, i.e. Sq := [d] \ Sq. We can
then draw a curve showing the trade-off between the ratio of corrupted irrelevant features and the
degree of the output change such as gCntensity (Sq) and gCabel(Sq). The trade-off curve usually shows
an increasing trend with an increase in the ratio of corrupted features (see Figure 4). Therefore, the
area over the curve can be used as a measure of how resistant the model’s decision is against the
feature corruption. In this paper, we refer to this area as Area Over the IFS-Curve (AOIC).
5	EFS VS. IFS
(Q3) What is an appropriate definition of relevance?
To answer this question, we compare EFS and IFS through exhaustive experiments. Our results
indicate that EFS has several drawbacks, and we therefore argue that IFS-Relevance is better suited
for the feature attribution problem.
5.1	Experimental Setup
Models and Data As the target model f to be explained, we adopted three pre-trained models,
namely VGG16 (Simonyan & Zisserman, 2014), ResNet V2 with depth 152 (He et al., 2016), and
Inception V3 (Szegedy et al., 2016), which were distributed at the Tensorflow repository1. As the
target data X to be explained, we selected 200 images from the validation set at ILSVRC2014 (Rus-
sakovsky et al., 2014) which were correctly classified by the three models.
Feature Attribution Methods In the experiments, we adopted several feature attribution meth-
ods for comparison: Grad (Simonyan et al., 2013), Grad×Input (Shrikumar et al., 2016), Guid-
edBP (Springenberg et al., 2014), SmoothGrad (Smilkov et al., 2017; Hooker et al., 2018), Int-
Grad (Sundararajan et al., 2017), LRP (Bach et al., 2015), DeepLIFT (Shrikumar et al., 2017),
Occlusion (Zeiler & Fergus, 2014), and PertMap (Hara et al., 2018; Ikeno & Hara, 2018). Grad,
1https://github.com/tensorflow/models/tree/master/research/slim
6
Under review as a conference paper at ICLR 2019
Grad×Input, GuidedBP, SmoothGrad, and IntGrad were implemented using saliency2 with de-
fault settings, and LRP, DeepLIFT, and Occlusion were implemented using DeepExplain3, where
we set the mask size for Occlusion as 64 × 64 with the stride set to 16. We implemented PertMap
based on the sample code4. We also adopted random attribution as the baseline where the score for
each feature was generated uniformly random over [0, 1].
In addition to the existing feature attribution methods, we implemented the following EFS-based and
IFS-based methods: Greedy-EFS, which solves the problem (2.2) using a greedy algorithm; Grad-
EFS, which solves the problem (3.2) using gradient descent; Greedy-IFS, which solves the problem
(2.3) using a greedy algorithm; and Grad-IFS, which solves the problem (3.3) using gradient ascent.
The details of these methods can be found in Appendix C.
Evaluation For evaluating AUEC and AOIC, we prepared two noise distributions p(r). The first
distribution is a uniform distribution: each r is independently sampled from the uniform distribution
over [0, 1]d. The second distribution is a distribution over real images. We selected 100 images from
the validation set at ILSVRC2014, with no overlap with the 200 images to be explained. Then, from
those 100 images, we randomly selected an image as the noise r. To compute AUEC and AOIC,
we varied the percentile q from zero to one, and for each q , we computed the difference scores
gCabel(Sq) and gCabel(Sq) using empirical averages under those two noise distributions.
5.2	Results
For each model, we computed the attribution scores for all 200 images using each of the 14 feature
attribution methods. We then computed AUEC and AOIC under the two noise distributions.
Our main result is summarized in Table 1. Table 1 shows the AUEC and AOIC for VGG16 for the
200 images under uniform noise. We moved the results for the other models and the corruption
with real images to Appendix D, as those results are similar. Here, we point out that there are three
important observations in the table.
EFS found adversarial example. Grad-EFS attained the highest AUEC. This indicates that Grad-
EFS is nearly optimal under the principle of EFS: Grad-EFS can change the model’s decision by
corrupting only a small number of relevant features. Indeed, as shown in Figure 4, Grad-EFS has
a sharp increase in the EFS-Curve 5. Specifically, it shows that Grad-EFS successfully changed
the model’s decision for more than 80% of the data by corrupting only a few percent of the pixels.
Similar tendencies were also observed for ResNet V2 and Inception V3 (see Appendix D).
An important observation is that the heatmap of Grad-EFS is just a shot noise, as shown in Figure 5.
This is because EFS is very similar to adversarial example (Szegedy et al., 2013). In adversarial
example, one seeks the minimum data perturbation that changes the model’s output. In EFS (2.2),
instead of the data perturbation, one searches for a small number of corrupted features that reduces
the class intensity. Similarly, C-EFS (3.2) searches for a continuous corruption with the minimum
`1 norm.
Random attribution performed comparably well with existing methods. The random attribu-
tion attained AUEC similar to that of methods such as Grad×Input, IntGrad, LRP, and Occlusion,
especially for VGG16. Indeed, as shown in Figure 4, the EFS-Curve of random attribution is close
to those methods. It is a bit surprising to observe that we can attain a good trade-off in EFS just by
randomly scoring each feature without looking at the images. This observation indicates that there
are only subtle differences between good attributions and random attributions under EFS, especially
for VGG16.
Grad-IFS significantly outperformed the other methods. On AOIC, Grad-IFS significantly out-
performed the other methods, and PertMap attained the second best result. As shown in Figure 4,
Grad-IFS is very resistant against the corruption of irrelevant features. Indeed, even if 80% of the
pixels are corrupted, the model’s decision is kept unchanged for more than 80% of the images. This
means that Grad-IFS is capable of identifying irrelevant features better than any other methods. An-
2https://github.com/PAIR-code/saliency
3https://github.com/marcoancona/DeepExplain
4https://github.com/sato9hara/PertMap/
5We selected nine feature attribution methods for the visibility purpose.
7
Under review as a conference paper at ICLR 2019
Table 1: Average AUEC and AOIC un-
der the uniform noise. The top-three
scores are highlighted as 1st*, 2nd**,
and 3rd***.
VGG16
AUEC AOIC
Greedy-EFS	0.844	0.366
Grad-EFS	0.946*	0.195
Greedy-IFS	0.746	0.622*
Grad-IFS	0.873	0.876*
Grad	0.867	0.341
Grad×Input	0.823	0.318
SmoothGrad	0.882	0.593
GuidedBP	0.918**	0.455
IntGrad	0.837	0.346
LRP	0.823	0.318
DeepLIFT	0.862	0.435
Occlusion	0.811	0.559
PertMap	0.886***	0.780*
Random	0.839	0.160
VGG16: EFS-Curve
)S(lebalgegnahCtuptu
	Grad-EFS
■	Grad-IFS
♦ GradxInput	
T SmoothGrad	
	GuidedBP
	IntGrad
	LRP
	PertMap
*	Random
Ratio of Corrupted Relevant Features
VGG16: IFS-Curve
:1
0.8
1
8
.
0
6
.
0
4
.
0
2
.
0
-O
6420
...
000
egnahC tuptu
• Grad-EFS
■ Grad-IFS
♦ Grad X Input
* SmoothGrad
e GuidedBP
• IntGrad
廿 LRP
-0- PertMap
R Random
Ratio of Corrupted Irrelevant Features
Figure 4: Average EFS-Curve and IFS-Curve.
Original Image Grad-EFS Grad-IFS Gradxlnput SmoothGrad GuidedBP IntGrad PertMap
Figure 5: Attributions on VGG16: The red colored pixels are found to be strongly relevant with each
method.
other interesting point that can be seen in Figure 4 is that, the IFS-Curves vary significantly across
different methods. More importantly, unlike EFS, the IFS-Curve can distinguish random attributions
and other attributions well.
Figure 5 shows the examples of the attributions obtained by each method. It is important to note
that the top-three AOIC methods, namely Grad-IFS, PertMap, and SmoothGrad, have highlighted
only the dog face. The high AOICs on these methods indicate that the model has made the decision
based on the dog face. In contrast, the other methods tend to generate noisy attributions over the
entire body of the dog, which are false explanations from the perspective of IFS because their AOICs
are far smaller than that of Grad-IFS: the noisy attributions failed to capture essential pixels in the
image.
6 Conclusion
In this study, we formalized the feature attribution problem as two types of feature selection prob-
lems, which we named as EFS and IFS. Based on EFS and IFS, we clarified that the existing feature
attribution methods can be interpreted as approximation algorithms for EFS and IFS. Then, through
exhaustive experiments, we clarified that IFS is better suited as the formalization for the feature at-
tribution problem; we observed that EFS has several unfavorable properties and concluded that EFS
is not an appropriate formalization.
8
Under review as a conference paper at ICLR 2019
References
Sebastian Bach, Alexander Binder, Gregoire Montavon, Frederick Klauschen, Klaus-Robert Muller,
and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise
relevance propagation. PloS ONE, 10(7):e0130140, 2015.
Jianbo Chen, Le Song, Martin Wainwright, and Michael Jordan. Learning to explain: An
information-theoretic perspective on model interpretation. In Proceedings of the 35th Interna-
tional Conference on Machine Learning, pp. 882-891, 2018.
Yanzhuo Ding, Yang Liu, Huanbo Luan, and Maosong Sun. Visualizing and understanding neural
machine translation. In Proceedings of the 55th Annual Meeting of the Association for Computa-
tional Linguistics, pp. 1150-1159, 2017.
Ruth C Fong and Andrea Vedaldi. Interpretable explanations of black boxes by meaningful per-
turbation. In Proceeding of the IEEE International Conference on Computer Vision, pp. 3449 -
3457, 2017.
Satoshi Hara, Kouichi Ikeno, Tasuku Soma, and Takanori Maehara. Maximally invariant data per-
turbation as explanation. arXiv:1806.07004, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European conference on computer vision, pp. 630-645. Springer, 2016.
Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. Evaluating feature importance
estimates. arXiv:1806.10758, 2018.
Kouichi Ikeno and Satoshi Hara. Maximizing invariant data perturbation with stochastic optimiza-
tion. arXiv:1807.05077, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv:1412.6980,
2014.
Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In Proceed-
ings of Advances in Neural Information Processing Systems, pp. 4765-4774, 2017.
Gregoire Montavon, Sebastian Lapuschkin, Alexander Binder, Wojciech Samek, and Klaus-Robert
Muller. Explaining nonlinear classification decisions with deep taylor decomposition. Pattern
Recognition, 65:211-222, 2017.
Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why should i trust you?: Explaining the
predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference
on knowledge discovery and data mining, pp. 1135-1144. ACM, 2016.
Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Anchors: High-precision model-agnostic
explanations. In Proceedings of the 32nd AAAI Conference on Artificial Intelligence, pp. 1527-
1535, 2018.
Olga Russakovsky, Sanjeev Satheesh, Jia Deng, Sean Ma, Hao Su, Zhiheng Huang, Jonathan
Krause, Andrej Karpathy, Alexander C Berg, Michael Bernstein, et al. Imagenet large scale
visual recognition challenge. Technical report, 2014.
Wojciech Samek, Alexander Binder, GregOire Montavon, Sebastian Lapuschkin, and Klaus-Robert
Muller. Evaluating the visualization of what a deep neural network has learned. IEEE transactions
on neural networks and learning systems, 28(11):2660-2673, 2017.
Avanti Shrikumar, Peyton Greenside, Anna Shcherbina, and Anshul Kundaje. Not just a black box:
Learning important features through propagating activation differences. arXiv:1605.1713, 2016.
Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through
propagating activation differences. In Proceedings of International Conference on Machine
Learning, pp. 3145-3153, 2017.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv:1409.1556, 2014.
9
Under review as a conference paper at ICLR 2019
Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks:
Visualising image classification models and saliency maps. arXiv:1312.6034, 2013.
Daniel Smilkov, Nikhil ThoraL Been Kim, Fernanda Viegas, and Martin Wattenberg. Smoothgrad:
removing noise by adding noise. arXiv:1706.03825, 2017.
Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for
simplicity: The all convolutional net. arXiv:1412.6806, 2014.
Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks.
arXiv:1703.01365, 2017.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv:1312.6199, 2013.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethink-
ing the inception architecture for computer vision. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 2818-2826, 2016.
Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In
Proceedings of European Conference on Computer Vision, pp. 818-833. Springer, 2014.
Jianming Zhang, Zhe Lin, Jonathan Brandt, Xiaohui Shen, and Stan Sclaroff. Top-down neural
attention by excitation backprop. In Proceedings of European Conference on Computer Vision,
pp. 543-559. Springer, 2016.
Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Object detectors
emerge in deep scene cnns. arXiv:1412.6856, 2014.
10
Under review as a conference paper at ICLR 2019
Supplementary Material for “Feature Attri-
bution as Feature Selection”
Preliminaries We define the data-dependent noise by rx as follows:
rx := x + u,
where we assume that the perturbation u follows a distribution p(u) with Eu[u] = 0. The continuous
corruption with the noise rx is then expressed as
(Xw,rx)i = (1 - Wi)Xi + Wirχ,i = Xi + WiUi.
That is, the continuous corruption Xw,rx is centered at the data x. Moreover, Xw,rx distributes
around X with the magnitude w. The data-dependent noise and this continuous corruption plays an
important role when interpreting some of the existing feature attribution methods as EFS and IFS.
A	Optimization-based Feature Attribution Methods as EFS/IFS
Meaningful Perturbation (Fong & Vedaldi, 2017) is a variant of C-EFS defined below:
d
min〉] Wi + λf c(xw,r ) + φsmooth (W) .	(A♦I)
w∈[0,1]d
i=1
The differences from C-EFS (3.2) are (i) it uses a fixed noise r instead of the expectation, and (ii) the
additional penalty term φsmooth (W) is added so that W to be smooth. Here, the smoothness penalty
is essential for Meaningful Perturbation. Fong & Vedaldi (2017) have reported that the solution to
the problem (A.1) without the smoothness penalty tends to be seemingly meaningless attributions.
This is because C-EFS is very similar to the `1 penalized adversarial example, as we have discussed
in Section 5. Therefore, we need to design an appropriate penalty term φsmooth to obtain reasonable
attributions using Meaningful Perturbation.
PertMap (Hara et al., 2018; Ikeno & Hara, 2018) can be interpreted as a variant of C-IFS. PertMap
finds irrelevant features by maximizing the data perturbation. Let Xw,rx be the continuous corruption
of the data X with a data-dependent noise rx, where the parameter W determine the magnitude of
the data perturbation. In PertMap, one seeks for the maximum data perturbation that maintains the
classification result unchanged from the original class c. The optimization problem of W is defined
as follows (Ikeno & Hara, 2018):
d
max V" Wi + λJ2 Ermin(0,fc(Xw,r ) - fj (Xw,r ))].	(A.2)
w∈[0,1]d
i=1	j6=c
This problem is equivalent to C-EFS except for the term Er[fc(Xw,r)] replaced with the hinge
penalty term Pj=C Er [min(0, fc(Xw,r) - fj (Xw,r))] that penalizes W only when the corrupted data
is classified into other classes.
B Gradient-based Feature Attribution Methods as EFS/IFS
Many feature attribution methods are based on the gradient of the model,s output df∂c(x). Here, We
show that those gradient-based feature attribution methods can be interpreted as one-step gradient
descent/ascent approximations to C-EFS and C-IFS. If we consider solving the problems (3.2) and
(3.3) using gradient descent/ascent with the initial W set to zeros, i.e. W = 0d, then, the first step of
the optimization can be expressed as
Wi J o ± η { Er
dfc(xw,r )
∂Wi
w=0d
+ λ∣ (X ji-Xi)+ λ,
∂Xi
(B.1)
where η > 0 is the step size and μ% := E/rj. Here, because the penalty λ and the step size η are
common across all the features, dfc(x)(μi - Xi) is the essential term that determines the size of w%.
We therefore ignore the terms λ and η for simplicity. Then, the relationship between EFS/IFS and
the gradient-based feature attribution methods can be summarized as follows.
11
Under review as a conference paper at ICLR 2019
Grad (Simonyan et al., 2013) Assuming μ% -Xi to be constant across the features, Wi is equivalent
to df∂c(x) UP to the scaling factor μ% — Xi.
Grad × Input (Shrikumar et al., 2016) Assuming μ% to be sufficiently small (μi → 0), Wi is
equivalent to dfc(x) Xi.
Linear Approximation (Bach et al., 2015; Sundararajan et al., 2017; Shrikumar et al., 2017;
Montavon et al., 2017; Ribeiro et al., 2016; Lundberg & Lee, 2017) Several methods consider
the linear aPProximation of the model f in the neighborhood of the inPut X. SPecifically, they
consider the linear model
fc(r) = fc(X) + hW,r - Xi,	(B.2)
and uses the coefficient W as the attribution score. Here, let us assume that the noise r belongs to the
-ball around X, i.e. r ∈ R(X; ) := {r : kr - Xk ≤ }. For differentiable models, we can consider
the first-order Taylor exPansion within R(X; ):
fc(r) = fc(x) + hVfc(x), r - Xi + O(e2).	(B.3)
Thus, in a rough sense, by ignoring the O(2 ) term, the coefficient Wi comPuted by those linear
approximation methods is essentially equivalent to the gradient d∂c(x). Technically, those linear
aPProximation methods consider further imProvements over the Pure gradient, so that the linear
approximation to be valid not only in the infinitesimally small neighborhood of X, but in a finite
range from X. Thus, the computed coefficient Wi can slightly differ from the gradient, nevertheless,
those methods can be still classified as approximations of the gradient.
SmoothGrad (Smilkov et al., 2017; Hooker et al., 2018) In SmoothGrad, the attribution score is
defined as the expectation of the squared gradient over perturbed inputs. Recall that the perturbed
data can be expressed as Xw,rx using the data-dependent noise rχ. Given the perturbation magnitude
W0 > 0, the attribution score is defined as follows (Smilkov et al., 2017; Hooker et al., 2018):
scoreSG := Er
dfC (Xw0,rχ ))
d(Xwo,rχ )i )
(B.4)
Here, we show that this score can be also interpreted as one-step gradient descent/ascent approxi-
mation to C-EFS and C-IFS. Ifwe set the initial W as Wi = W0, the first step of the optimization can
be expressed as
Wi - wo ± η
Er
dfc(Xw,rx )
∂Wi
+λ
w=w0
士ηEr dfc(Xw,rx)	+(W0 ± ηλ)
±ηEu
dfC(Xw,x+U)
d(Xw,x+U)i
ui
w=w0
+ (W0 ± ηλ),
(B.5)
(B.6)
(B.7)
where we used the definition rx = X + u in the last equality. Because W0, η, and λ are constants,
the first term determines Wi . Moreover, by applying the Cauchy-Schwartz inequality, we obtain
dfC(Xw0 ,x+u )
d(Xwo,x+U)i
ui ≤ Er
w=w0
dfc(Xw0,rχ ) ) 2
d(Xwo,rX)i )
(B.8)
If we selected the perturbation u to have a common variance across the features, i.e. EU [ui2] = σ2
for all i ∈ [d], we can conclude that Er ]( f^xwiX))
is equivalent to (B.4).
is the essential term determining W, which
Here, we note out our analysis can explain the success of SmoothGrad. Some studies (Smilkov et al.,
2017; Hooker et al., 2018) have reported that SmoothGrad performs better than other gradient-based
12
Under review as a conference paper at ICLR 2019
feature attribution methods in practice. As one-step gradient descent/ascent approximation to C-EFS
and C-IFS, SmoothGrad starts the optimization from the non-zero point w = w0 while the other
methods starts the optimization from zero w = 0d . If the initial point w0 is carefully chosen, it is
apparent that one-step gradient descent/ascent from w0 can get closer to the solution than starting
the optimization from zero.
C Implementations of EFS/IFS-based Methods
In Section 5, we adopted the EFS-based and IFS-based methods: Greedy-EFS, which solves the
problem (2.2) using a greedy algorithm; Grad-EFS, which solves the problem (3.2) using gradient
descent; Greedy-IFS, which solves the problem (2.3) using a greedy algorithm; and Grad-IFS, which
solves the problem (3.3) using gradient ascent. Each method is implemented as follows.
Greedy-EFS In Greedy-EFS, we first prepared the feature subsets S := {Sm ⊆ [d]}mM=1, where
each subset Sm is constructed by sliding the window over the image. In our implementation, we set
the window size as 64 X 64 and the stride set to 16. We started the greedy algorithm from S = 0,
and iterated the following steps.
1.	Sm := argminsm∈s Er[fc(xs∪Sm,r)].
ʌ , ʌ
2.	S 一 S ∪ Sm, S-S∖{Sm}.
We repeated those steps ten times, and obtained S as an approximation of SEFS . We used a gray
background as the noise r, i.e. p(r) = δ(r = gray).
Greedy-IFS In Greedy-IFS, we prepared the feature subsets S := {Sm ⊆ [d]}mM=1 in the same
way as Greedy-EFS. We started the greedy algorithm from S = [d], and iterated the following steps.
1.	Sm := argmaxsm∈s Er[fc(xs∖Smr)].
ʌ , ʌ
2.	S 一 S \ Sm, S-S∖{Sm}.
We repeated those steps ten times, and obtained S as an approximation of SIFS . We used a gray
background as the noise r, i.e. p(r) = δ(r = gray). We note that Greedy-IFS is essentially the
same as the greedy method proposed by Zhou et al. (2014).
Grad-EFS In Grad-EFS, we used the data-dependent noise rx, where we set the distribution p(u)
to be uniform over [-1, 1]d. In each step of the gradient descent, we approximated the gradient of
C-EFS (3.2) using a batch of random realizations of rx. In our implementation, we set the penalty
weight λ = 10d and the batch size in each gradient approximation to be 32. As the optimization
algorithm, we used Adam (Kingma & Ba, 2014) with the step size set to 0.5 and the remaining
parameters set to default values. We run Adam for 200 steps, and obtained w. We note that Grad-
EFS is essentially the same as Meaningful Perturbation (Fong & Vedaldi, 2017) except that the
smoothness penalty term is removed.
Grad-IFS In Grad-IFS, we used the data-dependent noise rx, where we set the distribution p(u) to
be uniform over [-1, 1]d. In each step of the gradient ascent, we approximated the gradient of C-IFS
(3.3) using a batch of random realizations of rx . In our implementation, we set the penalty weight
λ = d and the batch size in each gradient approximation to be 32. As the optimization algorithm,
we used Adam (Kingma & Ba, 2014) with the step size set to 0.03 and the remaining parameters
set to default values. We run Adam for 1000 steps. In Grad-IFS, we used the averaged parameter
over 1000 steps 10 PI=00 w(t) as w. ThiS is a common technique to improve the quality of the
solution obtained from stochastic optimization algorithms with convex objective functions. We
found that this technique is helpful also for Grad-IFS. We also applied the same averaging technique
to PertMap (Ikeno & Hara, 2018).
D	Experimental Results
Here, we present all the results that are omitted from Section 5 due to the space limitation.
13
Under review as a conference paper at ICLR 2019
D.1 AUEC AND AOIC
Table 2 and Table 3 show average AUEC and AOIC evaluated with uniform noises and random
images, respectively. There are three important observations in these tables.
First, Grad-EFS attained the highest AUEC for VGG16 and Inception V3, and the second highest
AUEC for ResNet V2. This indicates that Grad-EFS is nearly optimal under the principle of EFS:
Grad-EFS can change the model’s decision by corrupting only a small number of relevant features.
Indeed, as shown in Figure 6 and Figure 7, Grad-EFS has a sharp increase in the EFS-Curves. This is
a natural consequence from the fact that EFS is very similar to adversarial example, as we discussed
in Section 5.
Second, especially for VGG16, the random attribution attained AUEC similar to some of the meth-
ods such as Grad×Input, IntGrad, LRP, and Occlusion. Indeed, as in Figure 6 and Figure 7, the
EFS-Curves of the random attribution are close to those methods. It is a bit surprising to observe
that we can attain a good trade-off in EFS just by randomly scoring each feature without looking at
the images. This observation indicates that there are only subtle differences between good attribu-
tions and random attributions under EFS, especially for VGG16.
Third, on AOIC, Grad-IFS consistently outperformed the other methods, and PertMap attained the
second best result. As shown in Figure 6 and Figure 7, Grad-IFS is very resistant against the cor-
ruption of irrelevant features. This means that Grad-IFS is capable of identifying irrelevant features
better than any other methods. Another important point that can be seen in Figure 6 and Figure 7 is
that, the IFS-Curves vary significantly across different methods. More importantly, unlike EFS, the
IFS-Curve can distinguish random attributions and other attributions well.
14
Under review as a conference paper at ICLR 2019
Table 2: Average AUEC and AOIC under the uniform noise. The top-three scores are highlighted as
1st* ,2nd**,and 3rd***.
	VGG16		ResNet V2		Inception V3	
	AUEC	AOIC	AUEC	AOIC	AUEC	AOIC
Greedy-EFS	0.844	0.366	0.858**	0.391	0.837***	0.445
Grad-EFS	0.946*	0.195	0.858**	0.369	0.927*	0.394
Greedy-IFS	0.746	0.622	0.745	0.709***	0.715	0.685
Grad-IFS	0.873	0.876*	0.814	0.829*	0.817	0.868*
Grad	0.867	0.341	0.797	0.416	0.799	0.475
Grad×Input	0.823	0.318	0.750	0.404	0.748	0.441
SmoothGrad	0.882	0.593***	0.840	0.707	0.829	0.744***
GuidedBP	0.918**	0.455	0.874*	0.595	0.826	0.555
IntGrad	0.837	0.346	0.781	0.438	0.775	0.482
LRP	0.823	0.318	0.750	0.404	0.748	0.441
DeepLIFT	0.862	0.435	0.811	0.518	0.788	0.539
Occlusion	0.811	0.559	0.767	0.595	0.709	0.664
PertMap	0.886***	0.780**	0.828	0.765**	0.846**	0.817**
Random	0.839	0.160	0.691	0.308	0.714	0.290
Table 3: Average AUEC and AOIC under random images. The top-three scores are highlighted as
1st*,2nd**,and3rd***.
	VGG16		ResNet V2		Inception V3	
	AUEC	AOIC	AUEC	AOIC	AUEC	AOIC
Greedy-EFS	0.842	0.319	0.857***	0.388	0.857***	0.397
Grad-EFS	0.926*	0.179	0.861**	0.275	0.893*	0.293
Greedy-IFS	0.766	0.524***	0.763	0.589	0.760	0.521
Grad-IFS	0.858	0.630*	0.826	0.688*	0.844	0.705*
Grad	0.865	0.290	0.800	0.375	0.826	0.393
Grad×Input	0.835	0.296	0.775	0.384	0.789	0.396
SmoothGrad	0.880	0.486	0.846	0.647***	0.860	0.650***
GuidedBP	0.905**	0.416	0.876*	0.568	0.839	0.502
IntGrad	0.847	0.318	0.807	0.418	0.815	0.435
LRP	0.835	0.296	0.775	0.384	0.789	0.397
DeepLIFT	0.868	0.378	0.836	0.480	0.826	0.471
Occlusion	0.840	0.487	0.787	0.544	0.769	0.553
PertMap	0.876***	0.553**	0.833	0.660**	0.861**	0.661**
Random	0.842	0.160	0.760	0.237	0.767	0.234
15
Under review as a conference paper at ICLR 2019
* Grad-EFS ■ Grad-IFS ,GradxlnpufSmoothGradwGuidedBP
• IntGrad o LRP @ PertMap * Random
VGG16: EFS-Curve
Output Change gclabel(Sq)	Output Change gclabel (Sq)	Output Change gclabel (Sq
0o
0	0.2	0.4	0.6	0.8
Ratio of Corrupted Relevant Features
ResNet V2: EFS-Curve
1
0.8
0.6
0.4
0.2
0S----------------------------------
0	0.2	0.4	0.6	0.8	1
Ratio of Corrupted Relevant Features
Inception V3: EFS-Curve
(bs) 一。q七 ooβ≡Dlndlno
(BSh。乳6 缺≡o~ndjno
Ratio of Corrupted Irrelevant Features
ResNet V2: IFS-Curve
1
0.8
642
...
000
clg egnahC tuptu
0八------
0	0.2	0.4	0.6	0.8
Ratio of Corrupted Irrelevant Features
Inception V3: IFS-Curve
0	----------
0	0.2	0.4	0.6	0.8
0®----------------------------------
0	0.2	0.4	0.6	0.8	1
Ratio of Corrupted Relevant Features
Ratio of Corrupted Irrelevant Features
Figure 6: Average EFS-Curve and IFS-Curve evaluated with uniform noises.
16
Under review as a conference paper at ICLR 2019
* Grad-EFS ■ Grad-IFS ,GradxlnpufSmoothGradwGuidedBP
• IntGrad o LRP @ PertMap * Random
VGG16: EFS-Curve
)S(lebalgegnahCtuptuO
Ratio of Corrupted Relevant Features
186420
......................
0000
(bs) Wq七 OMrnDlndJnO
Ratio of Corrupted Irrelevant Features
ResNet V2: EFS-Curve
)qS(lebaclg egnahC tuptuO
Ratio of Corrupted Relevant Features
ResNet V2: IFS-Curve
Ratio of Corrupted Irrelevant Features
Inception V3: EFS-Curve
)S(lebalgegnahCtuptu
Ratio of Corrupted Relevant Features
Figure 7: Average EFS-Curve and IFS-Curve evaluated with random images.
(ms) IOMd OMUEqOlndJnO
Inception V3: IFS-Curve
Ratio of Corrupted Irrelevant Features
17
Under review as a conference paper at ICLR 2019
D.2 Example Heatmaps
Figures 8-12 show examples of the heatmaps obtained by several feature attribution methods. There
are two important observations in the figures.
First, the heatmaps of Grad-EFS are mostly shot noises, which are visibly meaningless. These are
natural consequences from the fact that EFS is very similar to adversarial example, as we have
discussed in Section 5.
Second, the high AOIC methods in Table 2 and Table 3, namely Grad-IFS, PertMap, and Smooth-
Grad, have highlighted mostly the faces of the animals. The high AOICs on these methods indicate
that the model has made the decision based mostly on the animal face. In contrast, the other methods
tend to generate noisy attributions over the entire body of the animals, which are false explanations
from the perspective of IFS because their AOICs are far smaller than that of Grad-IFS: the noisy
attributions failed to capture essential pixels in the images.
18
Under review as a conference paper at ICLR 2019
VGG16
Original Image
PeJo£00 ES
ResNet V2
VGG16
sl'aps」。
Inception V3
dmptup=o
VGG16
ResNet V2
VGG16
Inception V3	VGG16	ResNet V2
ωn=⅛E0
ResNet V2
JJndtutu0
Inception V3
VGG16
ResNet V2
Inception V3
VGG16
ResNet V2
Inception V3
ResNet V2	Inception V3
Figure 8: Example Heatmaps: The red colored pixels are found
method.
VGG16
de≡ttud
to be strongly relevant with each
19
Under review as a conference paper at ICLR 2019
Original Image
PeJo£00 ES
VGG16
ResNet V2
Inception V3
Gradxlnput	Grad	Grad-IFS	Greedy-IFS	Grad-EFS Greedy-EFS
VGG16	ResNet V2	Inception V3	VGG16
Inception V3
VGG16
VGG16
VGG16
VGG16
ResNet V2
d
cc
VGG16
ResNet V2
ResNet V2
ResNet V2
ResNet V2
ResNet V2
Inception V3
VGG16
ResNet V2
Inception V3	VGG16
Figure 9: Example Heatmaps: The red colored pixels are found
method.
VGG16
Inception V3
ResNet V2
InCePtiOn V3
to be strongly relevant with each
20
Under review as a conference paper at ICLR 2019
ResNet V2
VGG16
ResNet V2
VGG16
ResNet V2
ResNet V2
VGG16
Pe-IOlnOOES dmptup=o pe∙loψju-
ResNet V2
Original Image
Inception V3
InCePtiOn V3
ResNet V2
VGG16
su-∙APtualo
VGG16
su-∙pe 存
VGG16
d⅛∏
ResNet V2
VGG16
st,APtutu存
Inception V3
ResNet V2
VGG16
JJndtutu0
InCePtiOn V3
ResNet V2
VGG16
ωn=⅛E0
Inception V3
ResNet V2
Uo-SnPUo
InCePtiOn V3
ResNet V2
VGG16

Inception V3
VGG16
de≡ttud
Inception V3
ResNet V2
VGG16
3nd-xpe」0
2
Under review as a conference paper at ICLR 2019
VGG16
ResNet V2
InCePtiOn V3
ResNet V2
JnCePtiOn V3


VGG16	ResNet V2	Inception V3	VGG16	ResNet V2	Inception V3
VGG16	ResNet V2	Inception V3	VGG16	ResNet V2	Inception V3
VGG16	ResNet V2	Inception V3	VGG16	ResNet V2	Inception V3
VGG16	ResNet V2	Inception V3
Inception V3
Figure 11: Example Heatmaps: The red colored pixels are found to be strongly relevant with each
method.
22
Under review as a conference paper at ICLR 2019
Gradxlnput	Grad	Grad-IFS	Greedy-IFS	Grad-EFS Greedy-EFS
VGG16
VGG16
VGG16	ResNet V2	Inception V3
PeJo£00 ES
ResNet V2
ResNet V2
ResNet V2
ResNet V2
dmptup=o
VGG16
ResNet V2	Inception V3
Pe-IOHI-
ResNet V2
VGG16
VGG16
VGG16
Uo-SnPUo
VGG16
VGG16
de≡ttud
ResNet V2
ResNet V2
ResNet V2
ResNet V2
Inception V3
Inception V3
Figure 12: Example Heatmaps: The red colored pixels are found to be strongly relevant with each
method.
23