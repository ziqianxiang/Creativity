Under review as a conference paper at ICLR 2019
Neural Network Bandit Learning by Last
Layer Marginalization
Anonymous authors
Paper under double-blind review
Ab stract
We propose a new method for training neural networks online in a bandit set-
ting. Similar to prior work, we model the uncertainty only in the last layer of the
network, treating the rest of the network as a feature extractor. This allows us
to successfully balance between exploration and exploitation due to the efficient,
closed-form uncertainty estimates available for linear models. To train the rest of
the network, we take advantage of the posterior we have over the last layer, op-
timizing over all values in the last layer distribution weighted by probability. We
derive a closed form, differential approximation to this objective and show em-
pirically over various models and datasets that training the rest of the network in
this fashion leads to both better online and offline performance when compared to
other methods.
1	Introduction
Applying machine learning models to real world applications almost always involves deploying
systems in dynamic, non-stationary environments. This dilemma requires models to be constantly
re-updated with new data in order to maintain a similar model performance across time. Of course,
doing this usually requires the new data to be relabeled, which can be expensive or in some cases,
impossible. In many situations, this new labeled data can be cheaply acquired by utilizing feedback
from the user, where the feedback/reward indicates the quality of the action taken by the model for
the given input. Since the inputs are assumed independent, this task can be framed in the contextual
bandit setting. Learning in this setting requires a balance between exploring uncertain actions (where
we risk performing sub optimal actions) and exploiting actions the model is confident will lead to
high reward (where we risk missing out on discovering better actions).
Methods based on Thompson sampling (TS) or Upper Confidence Bounds (UCB) provide theo-
retically (Auer et al., 2002; Agrawal & Goyal, 2012) and empirically established ways (Li et al.,
2010; Chapelle & Li, 2011) for balancing exploration/exploitation in this setting. Unfortunately,
both methods require estimation of model uncertainty. While this can be done easily for most linear
models, it is a difficult and open problem for large neural network models underlying many mod-
ern machine learning systems. An empirical study by Riquelme et al. (2018) shows that having
good uncertainty estimates is vital for neural networks learning in a bandit setting. Closed formed
uncertainty estimations (and online update formulas) are available for many linear models. Since
the last layer of many neural networks are usually (generalized) linear models, a straightforward
way for learning neural networks in a bandit setting is to estimate the uncertainty (as a distribution
over weights) on the last layer only, holding the previous layers fixed as feature functions which
provide inputs to the linear model. This method (and variants thereof) has been proposed in bandit
settings (Riquelme et al., 2018; Liu et al., 2018) as well as other related settings (Snoek et al., 2015;
O'Donoghue et al., 2018; Lazaro-Gredilla & Figueiras-Vidal, 2010) and has been shown to work
surprisingly well considering its simplicity. This style of methods, which we refer to as Bayesian
last layer or BLL methods, also has the advantage of being both relatively model-agnostic and scal-
able to large models. Of course, BLL methods come with the tacit assumption that the feature
functions defined by the rest of the network output good (linearly separable) representations of our
inputs. This means that, unless the input data distribution is relatively static, the rest of the network
will need to be updated in regular intervals to maintain low regret.
1
Under review as a conference paper at ICLR 2019
In order to maintain low regret, the retraining objective must: 1) allow new data to be incorporated
quickly into the learned model, and 2) prevent previously learned information from being quickly
forgotten. Previous papers retrain BLL methods simply by sampling minibatches from the entire
pool of previously seen data and maximizing log-likelihood over these minibatches, which fails to
meet the first criteria above.
In this paper we present a new retraining objective for BLL methods meeting both requirements. We
avoid retraining the last layer with the entire network (throwing out the uncertainty information we
learned about the last layer) or retraining with the last layer fixed (fixing the last layer to the mean
of its distribution). Instead, we utilize the uncertainty information gathered about the last layer,
and optimize the expected log-likelihood of both new and old data, marginalizing1 over the entire
distribution we have on the last layer. This gives a more robust model that performs relatively well
over all likely values of the last layer. While this objective cannot be exactly computed, we derive
a closed form, differentiable, approximation. We show that this approximation meets both criteria
above, with a likelihood term to maximize that depends only on the new data (meeting the first
point), and a quadratic regularization term that is computed only with previously seen data (meeting
the second point). We show empirically that this method improves regret on the most difficult bandit
tasks studied in Riquelme et al. (2018). We additionally test the method on a large state-of-the-art
recurrent model, creating a bandit task out of a paraphrasing dataset. Finally, we test the method on
convolutional models, constructing a bandit task from a benchmark image classification dataset. We
show that our method is fast to adapt to new data without quickly forgetting previous information.
2	Problem Setting and Related Work
Contextual bandits are a well researched class of sequential decision problems (Wang et al., 2005;
Langford & Zhang, 2007), of which, many variants exist. In this paper, we mainly consider the
multiclass contextual bandit problem studied in Kakade et al. (2008). The problem takes place
over T online rounds. On round t, the learner receives a context xt , predicts a class label yt , and
receives binary reward rt indicating whether the chosen label is correct. No other information is
received about the other classes not picked. In our setting, we assume each class c (the arms of the
bandit) has associated with it a vector zc and that the probability of a class label is modeled by:
p(c|x, zc) = σ(zcTfθ(x)), where σ is the logistic function and fθ is a feature function parameterized
by θ. In our case, fθ defines a neural network, while zc can be seen as the last layer of the network2.
Our goal is to get low regret, R = PT r* - PT ri, where r* is the optimal reward at step i. The key
to getting low regret is employing a policy for balancing exploration and exploitation. If we capture
the uncertainty in each zc at time t by modeling its posterior distribution over previous data Dt-1 as
a multivariate Gaussian, Zc 〜p(zc∣Dt-ι), then we can easily deploy sound exploration strategies
such as Thompson sampling or UCB. If we hold fθ fixed, then we can easily model this distribution
by doing an online Bayesian regression on the outputs of fθ, which gives us closed form formulas
for updating the posterior over the last layer (specifically, its mean and covariance) given a single
datapoint.3 When fθ is a neural network, then this is an instance of a BLL method.
2.1	Bayesian Last Layer methods and the B enefits of Marginalization
BLL methods have been shown to be an effective, model agnostic, and scalable way to deal with
exploration problems involving neural networks. Previous work has found them to be a pragmatic
method for obtaining approximate uncertainty estimates for exploration (Liu et al., 2018; Riquelme
et al., 2018; O’Donoghue et al., 2018; Azizzadenesheli et al., 2018) and as proxies for Gaussian
processes in both Bayesian Optimization problems (Snoek et al., 2015) and general regression tasks
(LazarO-Gredilla & Figueiras-Vidal, 2010).
If fθ is fixed, then zc can be updated efficiently in an online manner. An unanswered question still
remains however: how does one actually update and learn fθ? If you don’t care about achieving
low regret (ie you only care about offline performance), then the answer is easy; just gather your
1We do not marginalize in the sense of ‘summing out’, but rather optimizing over all values of the last layer.
2Note that we make no assumption on the form of fθ, which can be anything differentiable
3Either a Bayesian linear or logistic regression will work. However, the Bayesian linear regression is much
more efficient in an online setting with similar performance so we use it in all of our implementations.
2
Under review as a conference paper at ICLR 2019
data, train fθ offline, possibly with off-policy learning methods (Strehl et al., 2010; Joachims et al.,
2018), and learn the Bayesian regression post-hoc. Of course, if you are concerned about online
performance (regret) then this is not a viable option.
A training method for fθ must take care of two things: when do we update the feature functions and
what do we update them with? A reasonable answer to the former question is to update on a fixed
schedule (every T rounds). In this paper, we focus on answering the latter questions of which there
are two obvious solutions, each with a corresponding problem:
(1)	Sample minibatches only from recent data. Problem: We may overfit on this data and forget old
information.
(2)	Sample minibatches uniformly from the set of all collected data (both recent and old). Problem:
We have lost the ability to adapt quickly to new data. If the input distribution suddenly shifts, we will
likely have to wait many iterations before newer data becomes common enough in our minibatch
samples, all while our regret increases.
One thing to consider is that when it comes time to train our feature functions, we have access to a
distribution over our last layer. If, for example, our distribution has suddenly shifted, then the last
layer distribution should have more variance, ideally placing density on last layers that do well on
older data, as well as those that fit well to the new data. If the distribution remains the same, then
the variance should be low and density should be placed on relatively few values. Intuitively, we
can get the best of both worlds (ability to adapt or retain information when needed) by optimizing
over all values of the last layer weighted by their probability. In the next section, we derive a local
approximation to this objective that shows this indeed is the case.
3	Optimization over the Last Layer Distribution
Let Dt and Dt-1 be the most recent set of data collected online, and the set of all previously collected
data, respectively. Additionally, assume a zero mean Gaussian prior over the last layer, p(z∣θ) that
is constant with respect to θ. Recall that during online training we fix the parameters θ = θt-1, and
model the distribution Q = p(zc|Dt, Dt-1, θt-1). We want a value of θ such that both our data and
the last layer values drawn from Q are likely. Thus our objective is to maximize:
Ez-Q [log p(Dt, Dt-ι,z∣θ)]
(1)
We can write the marginal likelihood as a convolution between a logistic function and a Gaussian
(based on our assumption of zero mean Gaussian prior p(z∣θ)), which can be approximated in closed
form as per MacKay (1992):
P(D∣Θ) = / σ(zT
fθ(D))p(z∣θ)dz ≈ σ
_________μfθ (Xi)__________
,1 + 8 fθ (Xi )T ∑fθ (Xi)
(2)
Where μ is the mean of p(z∣θ). Since We have a zero mean prior, the above term evaluates to σ(0)
whose value is a constant 2.
Using this result, we can rewrite equation (1) as:
Ez-Q[log p(Dt|z, θ)] - KL[Q, p(z|Dt-1, θ)] + c	(3)
Where c is a constant entropy term which can be ignored. For the first expectation term, we can use
a second order Taylor expansion around Ez-Q[log p(Dt|z, θ)] to get a closed form approximation4:
Ez-Q[logP(Dt∣z,θ)] ≈ logE[p(Dt∣z,θ)] - VEr[pDDTθθ]2	(4)
p t z,
This approximation was used in Teh et al. (2007) and shown to work well empirically. The expec-
tations in equation (4) are again logistic/Gaussian convolutions and can be approximated in closed
form via equation (2). A closed form approximation (derived in a similar fashion to equation (2))
can be used to evaluateVar[p(Dt|z, θ)] (Mahajan et al., 2012).
4Since we can easily sample from Q, the term may also be approximated via Monte Carlo. We don’t explore
this approach here.
3
Under review as a conference paper at ICLR 2019
The KL term in equation (3) can also be approximated locally with a second Taylor expansion
around the current value of θ = θt-1. Let K(θ) = KL(Q||p(z|Dt-1, θ)). Then, the second order
Taylor expansion around θ = θt-1 is:
K(θ) ≈ K(θt-1) + K0(θt-1)(θ — θt-ι) + 1(θ — θt-ι)K00(θt-1)(θ — θt-ι)τ	(5)
Utilizing properties of KL divergence, as well as equation (2), it can be derived 5that K0(θt-1) will
evaluate to 0, and K00(θt-ι) Will evaluate to βFp, where β = Ez~PP(DDDt/-1)] and FP is the
Fisher Information Matrix of P = p(z|Dt-1, θt-1). Getting rid of constants, we can write the local
KL approximation (when θ is close to θt-1) as:
KL[Q,p(z∣Dt-ι,θ)] ≈ 1(θ — θt-ι)βFp(θ — θ-ι)T	(6)
The term β defines the ratio between the expected data likelihood given the last layer z distributed
as Z 〜p(z∣Dt-ι, θt-ι) and the expected data likelihood given the last layer is distributed under the
prior p(z∣θ). This indicates that the better our previous values of θt-ι and Z explain the data, the
more we should regularize when incorporating new data (i.e raising the value ofβ). In practice, these
values may be computed or approximated, however for efficiency, we treat β as a hyperparameter,
and linearly increase it throughout the learning process.
Our final objective to optimize is thus:
Ez-Q[logp(Dt,Dt-ι,z∣θ)] ≈ Ez-Q[logP(Dt|z, θ)] — 1(θ — θt-ι)βFp(θ — θt-ι)T	(7)
Notice that the old data is now only used to calculate the Fisher information matrix FP and is not
actually involved in the optimization. Thus, the optimization (at least temporarily) over all our data
can be done by simply drawing minibatches from the new data only, while the old data is only used
to calculate the regularization term. The practical benefit of this is that the regularization term can be
easily computed in parallel while doing the online Bayesian regression and collecting new data. The
quadratic regularization term shares similarities to objective functions in continual learning which
aim to prevent catastrophic forgetting (Kirkpatrick et al., 2016; Ritter et al., 2018).
4	The Full Algorithm
Combining the online learning and the network retraining stages described in the previous section
gives us the general form of the iterative algorithm we study in this paper. The algorithm alternates
between two stages:
Online Phase: As input, take in a set of data Dt-1, the posteriors (one for each arm) of the last
layer conditioned on previous datap(Z|Dt-1) as well as a fixed value of fθ. This phase takes place
over a series of T online rounds. In every round, the learner receives a context xi , and uses the
posteriors over the last layer to decide which arm to pull (via Thompson sampling or UCB). The
learner receives feedback yi upon pulling the arm c, and updates the posterior over Zc with it. After
T rounds, the learner outputs the updated posteriors over Z, and the data collected this phase, Dt .
Offline/Retraining Phase: As input, take in Dt, Dt-1, and the posteriors over Z. Retrain fθ using
method described in Section 3. Set Dt-1 = Dt∪Dt-1. Recompute the posteriors over Z conditioned
on Dt-1 using the new value of fθ. Output Dt-1, fθ, and the updated posteriors p(Z|Dt-1).
The marginalization method described in Section 3 is one type of retraining method. We compare it
against two other methods in the next section and present results for various experiments.
5	Evaluation
We evaluate our technique across a diverse set of datasets and underlying models. As an initial san-
ity check, we first evaluate our method on the three most difficult (but low dimensional) bandit tasks
5Detailed derivations are provided in Appendix 7.1.
4
Under review as a conference paper at ICLR 2019
analyzed in Riquelme et al. (2018). We next look at two higher dimensional problems and mod-
els; one being a Natural Language Processing (NLP) task using a state-of-the-art recurrent model
and the other being a vision task using a convolutional model. In particular, we look at the degree
at which each method can achieve both good online performance (regret) and good offline perfor-
mance (offline test set accuracy), even in the face of large shifts in the data distribution. We provide
additional details on hyperparameters, experimental setup, and dataset information in Appendix 7.4.
All experiments are run 5 times, and reported with the mean and standard error.
5.1	Baseline Methods
We evaluate all the datasets against the baseline presented in Riquelme et al. (2018), as well as a
variant of our proposed method.
Bandit feedback. In this setting the models are trained using bandit feedback as the label:
Marginalize. This is our method of marginalizing over all values of the last layer for the neural
network training. Minibatches are sampled from the new data only and the regularization term is
computed from the old data.
Sample New. This baseline creates minibatches using only the newly collected data, optimizing the
likelihood of the new data only. It is equivalent to our method with a regularization constant of zero.
As mentioned in Section 2.1, this method is good at adapting to new data but has a drawback of
forgetting the old information.
Sample All. This is the retraining method presented in (Riquelme et al., 2018). In this method, a set
number minibatches are created by uniformly sampling from all collected data (both old and new).
SGD gradient updates are then performed using these batches. This method is slow to adapt but
retains older information (refer Section 2.1).
Full feedback. In this setting models are trained using all the labels for the datasets:
Batch Train. When evaluating the offline accuracy, we also give the results for a model that has been
trained on the shuffled data in batch mode, with all the labels for the dataset (i.e. full feedback).
This measures how well we could do given we had access to all the labels (instead of just the bandit
feedback), and trained in a normal offline setting. Surprisingly, as we see in some cases, training
online with marginalization sometimes performs comparable to training offline.
5.2	Low Dimensional Bandit Tasks
We first confirm that our method gives good online performance on simpler, but previously studied,
problems in Riquelme et al. (2018).
We present results on the three hardest bandit tasks analyzed in Riquelme et al. (2018), the Census,
Jester, and Adult dataset. The bandit problems for these datasets are defined as in previous work:
Census and Adult. Both these datasets are used for multiclass classification problem. Census dataset
has 9 classes whereas Adult dataset consists of 14 classes. For both datasets the bandit problem
is created as follows: for each class we assign an arm, and each arm is associated with a logistic
regression (parametrized by a vector) that takes a context as input and returns the expected reward
(0 or 1) for selecting the arm. In the online round we receive a context (feature vector) and pick an
arm according to some policy (like UCB), and receive a reward. Only the picked arm is updated in
each round.
Jester (Goldberg et al., 2001) This dataset consists of jokes with their user rating. For the bandit
problem, the model receives a context representing a user along with 8 jokes out of which it is
required to make recommendation of 1 joke. In this setting each joke is defined as a bandit arm.
The problem here is similar to above with the exception that each arm is associated with a linear
regression and outputs the predicted user rating for the selected joke. The reward returned is the
actual user rating for the selected joke.
5
Under review as a conference paper at ICLR 2019
Dataset	Method	Average Cum. Regret		Relative Regret (%)	
		UCB	TS	UCB	TS
Jester	Marginalize	2.96 ±0.01	2.95 ±0.03	59.3 ±0.4	58.1±0.5
	Sample New	2.96 ±0.02	2.98 ±0.04	59.1 ±0.4	60.6±0.5
	Sample All	3.58 ±0.01	3.54 ±0.03	72.8 ±0.4	71.1±0.5
Census	Marginalize	0.30 ±0.01	0.29 ±0.03	33.0 ±0.3	33.6±0.5
	Sample New	0.29 ±0.02	0.28 ±0.03	33.3 ±0.4	32.9±0.4
	Sample All	0.33 ±0.01	0.34 ±0.02	37.7 ±0.3	38.5±0.4
Adult	Marginalize	0.69 ±0.01	0.71 ±0.01	75.0 ±0.3	76.0±0.3
	Sample New	0.70 ±0.01	0.69 ±0.02	75.6 ±0.3	75.4±0.4
	Sample All	0.79 ±0.01	0.79 ±0.01	85.9 ±0.2	85.2±0.3
Table 1: Low Dimensional Bandit Task Results for both UCB and Thompson Sampling. The table
shows results for average cumulative regret and regret relative to the regret when selecting an arm at
random.
5.2.1	Results and discussion
As previously done in Riquelme et al. (2018), we use a two layer MLP as the underlying model,
using the same configuration across all methods. For Marginalize and Sample New, we perform the
retraining after 1000 rounds. For Sample All we update after 100 rounds just like Riquelme et al.
(2018). In Table 1 we report the average cumulative regret as well as the cumulative regret relative to
a policy that selects arms uniformly at random. We report the results for both Thompson Sampling
(TS) and for UCB policies. Results are similar for either UCB and TS which shows that policies
does not influence performance of the training mechanisms.
On most of the tasks both Marginalize (our method) and Sample New outperforms Sample All
(method used in Riquelme et al. (2018)) in terms of cumulative regret. Both Marginalize and Sample
New techniques are very similar in performance for the three datasets. All the three datasets used
in this experiment are low dimensional, static, and relatively easy to learn, hence there is not much
history to retain for Sample New technique. In the next section we will present results on larger
datasets and also evaluate where we will show that our method performs better than Sample New.
5.3	Paraphrasing Bandit Task
Next we evaluate our method with a bigger and more complex underlying model on the NLP domain.
We selected Bilateral Multi-Perspective Matching (BiMPM) (Wang et al., 2017), a recurrent model
that performs well on several sentence matching tasks, including the paraphrase identification task,
to evaluate our method. The goal of the paraphrase identification task is to determine whether a
sentence is a paraphrase of another sentence, i.e., whether they convey the same meaning.
5.3.1	Dataset and bandit setting
To evaluate whether our algorithm is robust to shifts in data distribution we combined two differ-
ent paraphrase identification datasets: i) The Quora Question Pairs dataset (Quora),6 which con-
tains 400,000 question pairs from the QA website Quora.com, and ii) The MSR Paraphrase Corpus
(MSR) (Dolan et al., 2004), which contains 5,800 pairs of sentences extracted from news articles.
To create an online training dataset we concatenate the MSR training set to a sample of 10,000
examples from the Quora training dataset7.
We run the online algorithms on this dataset to report the regret values, while we report the offline
performance on the MSR and Quora test sets. We use UCB as our search strategy, as it performs sim-
ilarly to Thompson sampling and runs much faster in our implementation. We analyze the following
two bandit tasks:
Multiclass. Like the previous datasets, we create a bandit problem by treating each class as an arm
parameterized by a vector and the contexts as the individual data instances. A reward 1 is awarded
6https://data.quora.com/First-Quora-Dataset-ReleaseQuestion-Pairs
7The Quora dataset is subsampled so that the combined dataset is more balanced in terms of the number of
examples from each dataset.
6
Under review as a conference paper at ICLR 2019
Task	Method	Average Cum. Regret	Offline Quora Acc.	Offline MSR Acc.
	Marginalize	0.304 ±0.003	74.06 ±0.18	-71.16 ±0.18
Multiclass	Sample New	0.322 ±0.002	67.73 ±0.23	70.93 ±0.35
	Sample All	0.329 ±0.003	71.06 ±0.32	68.76 ±0.23
	Marginalize	0.250 ±0.003	72.23 ±0.24	-69.16 ±0.07
Pool	Sample New	0.275 ±0.004	66.43 ±0.20	66.27 ±0.15
	Sample All	0.312 ±0004	72.20 ±0.25	64.79 ±0.14
-	Batch Train	-	74.87 ±0.33	72.63 ±。19
Table 2: Paraphrase Bandit Task results using the Quora/MSR paraphrase datasets.
tt.l6lucclu>-4e-nlunu
1600-
1400-
1200-
1000-
800-
600-
400-
200-
o-
0	1000	2000	3000	4000	5000
Round
Figure 1: Cumulative regret of the methods on the pool based Paraphrased Bandit Task.
for identifying correctly if the two sentences in the pair are paraphrase. For each method, we perform
an offline retraining after 1,000 online rounds.
Pool. Like the multiclass task, the pool based task occurs over a series of rounds. On each round,
the model receives a pool of k(=3) instances, and must select one of them for the user. After that the
model receives a reward based on its selection. The goal of the model is to learn a scoring function
that predicts the expected reward for selecting a certain instance, while at the same time trying to
keep regret low. This setting can be seen as an instance of the bandit problem formulation described
in (Russo & Van Roy, 2014). In our case, our instances are candidate paraphrase pairs, where the
model gets a reward of 1 for returning a valid paraphrase pair, and 0 otherwise. We use the same
implementation and hyperparameters for BiMPM as in (Wang et al., 2017). For Marginalize and
Sample All, we perform the retraining every 500 rounds. Sample New performed poorly offline on
this setting and is thus updated every 1,000 rounds.
5.3.2	Results and discussion
In Table 2 we show that our method Marginalize outperforms both Sample All and Sample New
techniques for both multiclass and pool based tasks. Sample All and Sample New have comparable
cumulative regret. Sample New has worse offline accuracy on Quora dataset (because it forgets old
information), while it has better offline accuracy on MSR (because it is able to adapt quicker). For
Batch train, both multiclass and pool based tasks are same—a binary classification problem. Batch
train performs only slightly better than our method in terms of offline accuracy, where Batch train
gets full feedback, while our method only gets partial (bandit) feedback. Figure 1 further shows
that when the data distribution changes (switching form Quora to MSR in the pool based task)
Marginalize and Sample New are able to adapt much faster than Sample All. Overall Marginalize
achieved a lower regret as well as higher offline accuracy for both the bandit settings.
5.4	Image classification Bandit Task
We additionally evaluate our method using a convolutional neural network, which is a common
network architecture for computer vision applications.
7
Under review as a conference paper at ICLR 2019
TaSk	Method	Average Cum. Regret	Offline Accuracy
	Marginalize	0.477 ±0.002	-59.87 ±0.33-
MUltiClaSS	Sample New	0.487 ±0.003	60.01 ±0.70
	Sample All	0.537 ±0.002	55.21 ±0.41
	Batch Train	-	78.20 ±0.37
	Marginalize	0.205 ±0.001	-84.64 ±0.37-
Pool	Sample New	0.255 ±0.003	82.55 ±0.19
	Sample All	0.214 ±0.009	85.11 ±0.23
	Batch Train	-	87.97 ±0.30
Table 3: Image classification Bandit Task results on CIFAR-10 dataset.
We use CIFAR-10 dataset (Krizhevsky, 2009) for this experiment. It is a commonly used dataset
for image classification task consisting of 60,000 images from 10 different classes. Similar to the
Quora/MSR tasks, we simulate a domain shift by concatenating together two datasets. In this case
we create two data sets from CIFAR-10 by partitioning the dataset into images depicting animals (6
classes) and images depicting transportation (4 labels). As above, we analyze two bandit tasks:
Multiclass. We define the multiclass bandit task similarly as above, for each of the 10 classes
in CIFAR-10, we assign one arm. At each round, the model receives an image, guesses a class,
and receives feedback (0 or 1) for this class only. The task is considerably more difficult than the
multiclass paraphrase bandit due to the number of classes. We use 1,000 rounds for the retraining
frequency for all methods.
Pool. We also define a pool based bandit, similar to the pool based paraphrase bandit, with pool
size k = 5. In this case we turn CIFAR-10 info a binary classification task. We select the two
most difficult classes to classify (airplanes and birds, according to the confusion matrix of our base
CNN model) in CIFAR-10, and denote these as the positive class. Like the previous pool task, the
learner receives a pool of images and must select one. A reward of 1 is given for selecting an image
belonging to the positive class, 0 otherwise. As done in the previous pool task, the data is sorted
as to simulate a change in domain. We use a standard convolutional neural network architecture for
this task, detailed in Appendix 7.5. We use 500 rounds for the retraining frequency for all methods.
5.4.1	Results and discussion
In Table 3 we present results for the image classification bandit task, using average cumulative regret
and offline accuracy as evaluation metrics. Again, Sample New performs better than Sample All
for cumulative regret but under-performs in the offline setting. As expected, our method performs
well for both cumulative regret and offline setting. For the multiclass task, our method performs
significantly lower than batch train. This is not too surprising, for two reasons: i) Training a CNN
architecture takes many more epochs over the data to converge (〜20 in our case) which is not
achieved in a bandit setting; ii) CIFAR-10 has 10 classes, each defining an arm and in our setting;
the bandit algorithms only gets feedback for one class in each round, compared to the full feedback
received in batch train. Effectively, the number of labels per class in cut by a factor of 10. This
is not as much an issue in the pool task, where we can see the results between batch train and the
bandit algorithms are comparable.
6	Conclusion
In this paper we proposed a new method for training neural networks in a bandit setting. We tackle
the problem of exploration-exploitation by estimating uncertainty only in the last layer, allowing
the method to scale to large state-of-the-art models. We take advantage of having a posterior over
the last layer weights by optimizing the rest of the network over all values of the last layer. We
show that method outperforms other methods across a diverse set of underlying models, especially
in online tasks where the distribution shifts rapidly. We leave it as future work to investigate more
sophisticated methods for determining when to retrain the network, how to set the weight (β) of
the regularization term in a more automatic way, and its possible connections to methods used for
continual learning.
8
Under review as a conference paper at ICLR 2019
References
Shipra Agrawal and Navin Goyal. Analysis of thompson sampling for the multi-armed bandit prob-
lem. In COLT 2012 - The 25th Annual Conference on Learning Theory, 2012.
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit
problem. Machine learning, 2002.
Kamyar Azizzadenesheli, Emma Brunskill, and Animashree Anandkumar. Efficient explo-
ration through bayesian deep q-networks. 2018. URL http://arxiv.org/abs/1802.
04412v1.
Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. In Advances in
Neural Information Processing Systems, 2011.
Bill Dolan, Chris Quirk, and Chris Brockett. Unsupervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Proceedings of the 20th international conference
on Computational Linguistics. Association for Computational Linguistics, 2004.
Kenneth Y. Goldberg, Theresa Roeder, Dhruv Gupta, and Chris Perkins. Eigentaste: A constant
time collaborative filtering algorithm. Information Retrieval, 2001.
Thorsten Joachims, Adith Swaminathan, and Maarten de Rijke. Deep learning with logged bandit
feedback. ICLR, 2018.
Sham M. Kakade, Shai Shalev-Shwartz, and Ambuj Tewari. Efficient bandit algorithms for online
multiclass prediction. In Proceedings of the Twenty-Fifth International Conference. ACM, 2008.
James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume Desjardins, An-
drei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis
Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic for-
getting in neural networks. CoRR, 2016. URL http://arxiv.org/abs/1612.00796.
Alex Krizhevsky. Learning Multiple Layers of Features from Tiny Images. Master’s thesis, 2009.
URL http://www.cs.toronto.edu/~kriz∕learning-features-2009-TR.
pdf.
John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side
information. In Advances in Neural Information Processing Systems, 2007.
MigUel Lazaro-Gredilla and Anlbal R Figueiras-Vidal. Marginalized neural network mixtures for
large-scale regression. IEEE transactions on neural networks, 2010.
Lihong Li, Wei Chu, John Langford, and Robert E. Schapire. A contextual-bandit approach to
personalized news article recommendation. In Proceedings of the 19th International Conference
on World Wide Web, 2010.
Bing Liu, Tong Yu, Ian Lane, and Ole J. Mengshoel. Customized nonlinear bandits for online
response selection in neural conversation models. AAAI, 2018. URL http://arxiv.org/
abs/1711.08493v1.
David JC MacKay. The evidence framework applied to classification networks. Neural computation,
1992.
Dhruv Kumar Mahajan, Rajeev Rastogi, Charu Tiwari, and Adway Mitra. Logucb: an explore-
exploit algorithm for comments recommendation. In Proceedings of the 21st ACM international
conference on Information and knowledge management. ACM, 2012.
Brendan O'Donoghue, Ian Osband, Remi Munos, and Volodymyr Mnih. The uncertainty bellman
equation and exploration. In Proceedings of the 35th International Conference on Machine Learn-
ing, 2018.
Carlos Riquelme, George Tucker, and Jasper Snoek. Deep bayesian bandits showdown: An empiri-
cal comparison of bayesian deep networks for thompson sampling. ICLR, 2018.
9
Under review as a conference paper at ICLR 2019
Hippolyt Ritter, Aleksandar Botev, and David Barber. Online structured laplace approximations for
overcoming catastrophic forgetting. NIPS, 2018. URL http://arxiv.org/abs/1805.
07810v1.
Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. Mathematics of
Operations Research, 2014.
Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram,
Md. Mostofa Ali Patwary, Prabhat, and Ryan P. Adams. Scalable bayesian optimization using
deep neural networks. In Proceedings of the 32nd International Conference on Machine Learning,
2015.
Alex Strehl, John Langford, Lihong Li, and Sham M Kakade. Learning from logged implicit explo-
ration data. In Advances in Neural Information Processing Systems, pp. 2217-2225, 2010.
Yee W Teh, David Newman, and Max Welling. A collapsed variational bayesian inference algorithm
for latent dirichlet allocation. In Advances in neural information processing systems, 2007.
Chih-Chun Wang, Sanjeev R. Kulkarni, and H. Vincent Poor. Bandit problems with side observa-
tions. IEEE Transactions on Automatic Control, 2005.
Zhiguo Wang, Wael Hamza, and Radu Florian. Bilateral multi-perspective matching for natural
language sentences. 2017. URL http://arxiv.org/abs/1702.03814v3.
7	Appendix
7.1	Objective Function Derivation
We utilize the same notation here as in Section 3. First, we show how to arrive at equation (3) from
the main objective, equation (1).
Ez-Q[logp(Dt,Dt-ι,z∣θ)] = Ez^Q[logp(Dt∣z, θ)]
+ Ez-Q [log p(z∣Dt-ι,θ)]
+ Ez-Q [log p(Dt-ι ∣θ)]
By marginalizing over our zero mean Gaussian prior and using equation (2), it follows that the
last expectation is approximately a constant, and can be removed. The second expectation is equal
to the negative cross entropy between Q = p(z|Dt, Dt-1, θt-1) and Pθ = p(z|Dt-1, θ). Using
the fact that KL(Q∖∖Pθ) = CE(Q∣∣Pθ) - H(Q), We can replace the negative cross entropy with
-KL(Q∖∖Pθ) - H(Q), where H(Q) is the entropy of Q which is constant and can be ignored,
yielding equation (3).
We next derive the KL term. As mentioned, we approximate the KL term locally around θt-1 with
a second order Taylor expansion. Again, let K(θ) = KL(p(z∖Dt, Dt-1, θt-1)∖∖p(z∖Dt-1, θ)). The
second order Taylor expansion around θ = θt-1 is:
K(θ) ≈ K(θt-1) + K0(θt-1)(θ - θt-ι) + 2(θ - θt-ι)K00(θt-1)(θ - θt-ι)τ
We can rewrite K0(θ) with respect to θ as:
VKL(Q∖∖Pθ)
▽ J(Q log Q - Q log Pθ)dz
VQlogPθdz = Ez-Q[V log p(z∖Dt-1, θ)]
This can also be done for K00. Let Vi indicate either the gradient (i = 1) or the Hessian (i = 2).
Then we can rewrite the above expectation with respect to the distribution P = p(z∖Dt-1, θt-1)
instead:
EZ-Q [Vi log Pθ ] = EZ-P h P(ZlDtηDt-1,θt-1) Vi log Pθ i
p(z∖Dt-1, θt-1)
Using the fact that p(z∖Dt, Dt-1, θt-1) = p(Dt∖θt-1)-1p(Dt∖z, θt-1)p(z∖Dt-1, θt-1), we can
rewrite the above as:
Ez-Q[Vi log Pθ] = p(Dt-1∖θt)-1Ez-P [p(Dt∖z, θt-1)Vi log Pθ]
10
Under review as a conference paper at ICLR 2019
Since p(z∣θ) is assumed constant with respect to θ, and p(Dt-ι∣θ) is approximately constant (using
the same trick with equation (2) previously used), We can rewrite Vi log Pθ as:
Vi logPθ = Vi logp(Dt-ι∣z,θ)+ Vi logp(z∣θ) -Vi logp(D1∣θ) ≈ Vi logp(D1∣z,θ)
With this is mind it follows from the conditional independence of the data that:
Ez 〜Q[Vilog Pθ ] = p(Dt∣θt-ι )-1Ez 〜P [p(Dt∣z,θt-ι)Vi log p(D1∣z,θ)]
=p(Dt∣θt-ι )-1Ez 〜P [p(Dt∣z,θt-ι)]Ez 〜P [Vi log p(Dt-ι∣z,θ)]
=p(Dt∣θt-ι )-1Ez 〜P [p(Dt∣z, θt-ι)]Ez 〜P [Vi log Pθ ]
If we plug in θt-1 into K0(θ), the rightmost expectation in the expression above will be:
Ez 〜p[V log p(z∣Dt-ι ,θt-ι )]= Ez 〜p[V log P ]=0
Thus, K0(θt-1) = 0. If we plug in θt-1 into K00(θ), the rightmost expectation in the expression
above will be:
Ez 〜P [VV log P(ZDt-1,θt-1)] = Ez 〜p[VV log P ] = FP
Where FP is the Fisher Information Matrix. Getting rid of constants, we can write the local KL
approximation (when θ is close to θt-1) as equation (6):
KL[Q,p(z∣Dt-ι,θ)] ≈ 1(θ - θt-ι)βFP(θ - θt-ι)T
where β = Ez~PP(DDDWt-1)]. Replacing above approximation in equation (3) yields the following
equation (7), which is our final objective for optimization:
Ez〜Q[logp(Dt,Dt-ι,ζ∣θ)] ≈ Ez〜Q[logP(Dt∣ζ,θ)] - 2(θ - θ-)0Fp(θ - θt-ι)T
7.2	Practical Considerations
Maintaining the last layer distribution. In the ideal situation we would like the posterior of the
last layer to be conditioned on all currently seen data. However, for any BLL method, whenever
the parameters θ change we must recompute the last layer distribution before going online again
if we wish to keep it conditioned on previous data. This is done by doing a forward pass through
the collected data and computing a regression on the output. For practical reasons, we keep a finite
buffer of the past n instances and only recompute the distribution conditioned on these points. For
all experiments and methods, we set n = 20000.
Computing the fisher information. Computing the Fisher information matrix in equation (7)
requires computing the log likelihood gradient for previously collected data. As above, we only
compute the gradients for the past n instances (where n is the same number as above). For effi-
ciency we only compute the diagonal elements of the matrix. As we already performed a forward
pass to recompute the last layer distribution, this calculation only requires a backwards pass. Fortu-
nately, this backwards pass can be done in parallel with the online learning stage and thus does not
bottleneck the process.
Bayesian linear versus Bayesian logistic regression. Either a Bayesian online logistic or linear
regression may be used to model the posterior over the last layers. While a logistic regression is
a more natural choice for classification tasks, the online Bayesian linear regression is much more
efficient (as it essentially only involves rank one matrix updates). We found that the performance
between the two is similar, and hence in our implementation, we use a linear regression. Rather
than regressing on 0-1 labels, we regress on logit values of 3 (for positive classes, corresponding to
a target probability of 0.95) and -3 (for negative classes, corresponding to a target probability of
0.05).
7.3	Calculating the UCB
Our approach to UCB is essentially the same as that presented by Li et al. (2010), that is, we
essentially run the algorithm LinUCB on top of the last layer of the neural network. Calculating
11
Under review as a conference paper at ICLR 2019
the UCB is straightforward; if Σ is the covariance of our posterior over z, and X is the context, then
with probability at least 1 - δ, the term (1 + vzln(2∕δ)∕2) ,fθ(X)TΣfθ(x) is an upper confidence
bound. The term α = (1 + /ln(2∕δ)∕2) is treated as a hyperparameter. The arm C chosen is the
one whose parameter vector zc maximizes the following:
fθ (X)T Zc + αqfθ(X)T∑Cfθ(X)
7.4 Hyperparameters
The hyperparameter values that are used for all methods across all tasks (the global hyperparameters)
are presented in Table 4. The hyper parameters for the low dimensional bandit tasks (we uses the
same values for each low dimensional dataset), the paraphrase bandit (multiclass and pool), and the
image classification bandit (multiclass and pool) are presented in Table 5. The meanings of the non
obvious hyperparameter values are described below:
Retrain Epochs: For Sample New and Marginalize; how many times to pass over the new data Dt .
Update Frequency: How many online rounds to do before updating the rest of the network offline.
Num Updates: For Sample All; the number of batches to uniformly sample (and update with) from
the entire distribution.
Hyperparameter	Value	Description
Optimizer UCB α Round Robin Init	RMSPrOP (lr=0.0001, decay=0.95) 2.07 (80% confidence interval) 50 per arm	Optimizer and learning rate Constant used in LinUCB Number of times each arm was pulled for init
Table 4: Global (used for all methods) experiment details for low dimensional bandit task.
Task	Method	Batch Size	Retrain Epochs	Update Frequency	Num Updates
Low Dimensional	Marginalize	5	5	1000	-
Low Dimensional	Sample New	32	4	1000	-
Low Dimensional	Sample All	128	-	100	100
Paraphrase Multiclass	Marginalize	5	5	1000	-
Paraphrase Multiclass	Sample New	8	3	1000	-
Paraphrase Multiclass	Sample All	64	-	1000	1000
Paraphrase Pool	Marginalize	5	5	500	-
Paraphrase Pool	Sample New	8	3	1000	-
Paraphrase Pool	Sample All	64	-	500	500
Images Multiclass	Marginalize	5	5	1000	-
Images Multiclass	Sample New	16	4	1000	-
Images Multiclass	Sample All	64	-	1000	1000
Images Pool	Marginalize	5	5	500	-
Images Pool	Sample New	16	4	500	-
Images Pool	Sample All	64	-		500		500
Table 5: Method specific experiment details for all tasks.
7.5 Model Architectures
In this section we detail the architectures used for each task. We use the same underlying model for
each method.
Low dimensional task models. For the Low dimensional tasks, we utilize the same exact Multi
Layer Perceptron (MLP) models as used in Riquelme et al. (2018). The model is a 2 layer MLP with
a hidden layer dimension of 100, with ReLu activation functions.
Paraphrase task. The paraphrase task uses the same BiMPM architecture and parameters used in
Wang et al. (2017). We refer readers to the corresponding paper for details.
Image classification task. The convolutions architecture we use is defined as: (i) Two 3 × 3
convolutional layers with 64 filters and ReLu activations; (ii) A 2 × 2 max pooling layer with a
12
Under review as a conference paper at ICLR 2019
stride of 2; (iii) Dropout layer with drop probability 0.25; (iv) A 3 × 3 and 2 × 2 convolutional layer
both with 128 filters and ReLu activations; (v) A 2 × 2 max pooling layer with a stride of 2; (vi)
Dropout layer with drop probability 0.25; (vii) A fully connected layer with 1024 units, followed by
a tanh activation, followed by another fully connected layer with 100 units and a tanh activation.
We utilize tanh activations at the end as per Snoek et al. (2015), who note that ReLu activations lead
to difficulties in estimating the uncertainty.
13