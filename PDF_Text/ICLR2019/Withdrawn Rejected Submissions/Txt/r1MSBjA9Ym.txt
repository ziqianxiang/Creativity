Under review as a conference paper at ICLR 2019
Collapse of deep and narrow neural nets
Anonymous authors
Paper under double-blind review
Ab stract
Recent theoretical work has demonstrated that deep neural networks have superior
performance over shallow networks, but their training is more difficult, e.g., they
suffer from the vanishing gradient problem. This problem can be typically re-
solved by the rectified linear unit (ReLU) activation. However, here we show that
even for such activation, deep and narrow neural networks (NNs) will converge to
erroneous mean or median states of the target function depending on the loss with
high probability. Deep and narrow NNs are encountered in solving partial differ-
ential equations with high-order derivatives. We demonstrate this collapse of such
NNs both numerically and theoretically, and provide estimates of the probability
of collapse. We also construct a diagram of a safe region for designing NNs that
avoid the collapse to erroneous states. Finally, we examine different ways of ini-
tialization and normalization that may avoid the collapse problem. Asymmetric
initializations may reduce the probability of collapse but do not totally eliminate
it.
1	Introduction
The best-known universal approximation theorems of neural networks (NNs) were obtained almost
three decades ago by Cybenko (1989) and Hornik et al. (1989), stating that every measurable func-
tion can be approximated accurately by a single-hidden-layer neural network, i.e., a shallow neural
network. Although powerful, these results do not provide any information on the required size of a
neural network to achieve a pre-specified accuracy. In Barron (1993), the author analyzed the size
of a neural network to approximate functions using Fourier transforms. Subsequently, in Mhaskar
(1996), the authors considered optimal approximations of smooth and analytic functions in shallow
networks, and demonstrated that -d/n neurons can uniformly approximate any Cn-function on a
compact set in Rd with error . This is an interesting result and it shows that to approximate a
three-dimensional function with accuracy 10-6 we need to design a NN with 1018 neurons for a C1
function, but for a very smooth function, e.g., C6, we only need 1000 neurons. In the last 15 years,
deep neural networks (i.e., networks with a large number of layers) have been used very effectively
in diverse applications.
After some initial debate, at the present time, it seems that deep NNs perform better than shallow
NNs of comparable size, e.g., a 3-layer NN with 10 neurons per layer may be a better approximator
than a 1-layer NN with 30 neurons. From the approximation point of view, there are several theo-
retical results to explain this superior performance. In Eldan & Shamir (2016), the authors showed
that a simple approximately radial function can be approximated by a small 3-layer feed-forward
NN, but it cannot be approximated by any 2-layer network with the same accuracy irrespective of
the activation function, unless its width is exponential in the dimension (see Mhaskar et al. (2017);
Mhaskar & Poggio (2016); Delalleau & Bengio (2011); Poggio et al. (2017) for further discussions).
In Liang & Srikant (2017) (see also Yarotsky (2017)), the authors claimed that for -approximation
of a large class of piecewise smooth functions using the rectified linear unit (ReLU) max(x, 0) ac-
tivation function, a multilayer NN using Θ(log(1/)) layers only needs O(poly log(1/)) neurons,
while Ω(poly(1∕e)) neurons are required by NNs with o(log(1∕e)) layers. That is, the number of
neurons required by a shallow network to approximate a function is exponentially larger than the
corresponding number of neurons needed by a deep network for a given accuracy level of function
approximation. In Petersen & Voigtlaender (2018), the authors studied approximation theory of a
class of (possibly discontinuous) piecewise Cβ functions for ReLU NN, and they found that no
more than O(e-2(d-1)/e) nonzero weights are required to approximate the function in the L2 sense,
1
Under review as a conference paper at ICLR 2019
which proves to be optimal. Under this optimality condition, they also show that a minimum depth
(UP to a multiplicative constant) is given by β∕d to achieve optimal approximation rates. As for the
expressive power of NNs in terms of the width, Lu et al. (2017) showed that any Lebesgue integrable
function from Rd to R can be approximated by a ReLU forward NN of width d + 4 with respect to
L1 distance, and cannot be approximated by any ReLU NN whose width is no more than d. Hanin
& Sellke (2017) showed that any continuous function can be approximated by a ReLU forward NN
of width din + dout, and they also give a quantitative estimate of the depth of the NN; here din and
dout are the dimensions of the input and output, respectively. For classification problems, networks
with a pyramidal structure and a certain class of activation functions need to have width larger than
the input dimension in order to produce disconnected decision regions (Nguyen et al., 2018).
With regards to optimum activation function employed in the NN approximation, before 2010 the
two commonly used non-linear activation functions were the logistic sigmoid 1∕(1 + e-x) and
the hyperbolic tangent (tanh); they are essentially the same function by simple re-scaling, i.e.,
tanh(x) = 2 sigmoid(2x) - 1. The deep neural networks with these two activations are difficult to
train (Glorot & Bengio, 2010). The non-zero mean of the sigmoid induces important singular values
in the Hessian (LeCun et al., 1998), and they both suffer from the vanishing gradient problem,
especially through neurons near saturation (Glorot & Bengio, 2010). In 2011, ReLU was proposed,
which avoids the vanishing gradient problem because of its linearity, and also results in highly sparse
NNs (Glorot et al., 2011). Since then, ReLU and its variants including leaky ReLU (LReLU) (Maas
et al., 2013), parametric ReLU (PReLU) (He et al., 2015) and ELU (Clevert et al., 2015) are favored
in almost all deep learning models. Thus, in this study, we focus on the ReLU activation.
While the aforementioned theoretical results are very powerful, they do not necessarily coincide
with the results of training of NNs in practice which is NP-hard (Sima, 2002). For example, while
the theory may suggest that the approximation of a multi-dimensional smooth function is accurate
for NN with 10 layers and 5 neurons per layer, it may not be possible to realize this NN approxi-
mation in practice. Fukumizu & Amari (2000) first proved that existence of local minima poses a
serious problem in learning of NNs. After that, more work has been done to understand bad local
minima under different assumptions (Zhou & Liang, 2017; Du et al., 2017; Safran & Shamir, 2017;
Wu et al., 2018; Yun et al., 2018). Besides local minima, singularity (Amari et al., 2006) and bad
saddle points (Kawaguchi, 2016) also affect training of NNs. Our paper focuses on a particular kind
of bad local minima, i.e., those encountered in deep and narrow neural networks collapse with high
probability. This is the topic of our work presented in this paper. Our results are summarized in
Fig. 6, which shows a diagram of the safe region of training to achieve the theoretically expected
accuracy. As we show in the next section through numerical simulations as well as in subsequent
sections through theoretical results, there is very high probability that for deep and narrow ReLU
NNs will converge to an erroneous state, which may be the mean value of the function or its partial
mean value. However, if the NN is trained with proper normalization techniques, such as batch nor-
malization (Ioffe & Szegedy, 2015), the collapse can be avoided. Not every normalization technique
is effective, for example, weight normalization (Salimans & Kingma, 2016) leads to the collapse of
the NN.
2	Collapse of deep and narrow neural networks
In this section, we will present several numerical tests for one- and two-dimensional functions of
different regularity to demonstrate that deep and narrow NNs collapse to the mean value or partial
mean value of the function.
It is well known that it is hard to train deep neural networks. Here we show through numerical
simulations that the situation gets even worse if the neural networks is narrow. First, we use a 10-
layer ReLU network with width 2 to approximate y(x) = |x|, and choose the mean squared error
(MSE) as the loss. In fact, y(x) can be represented exactly by a 2-layer ReLU NN with width 2,
|x| = ReLU(x) + ReLU(-x) = [1 1] ReLU( 1 x). However, our numerical tests show that
-1
there is a high probability (~ 90%) for the NN to collapse to the mean value of y(χ) (Fig. 1), no
matter what kernel initializers (He normal (He et al., 2015), LeCun normal (LeCun et al., 1998;
Klambauer et al., 2017), Glorot uniform (Glorot & Bengio, 2010)) or optimizers (first order or
second order including SGD, SGDNesterov (Sutskever et al., 2013), AdaGrad (Duchi et al., 2011),
2
Under review as a conference paper at ICLR 2019
AdaDelta (Zeiler, 2012), RMSProp (Hinton, 2014), Adam (Kingma & Ba, 2015), BFGS (Nocedal
& Wright, 2006), L-BFGS (Byrd et al., 1995)) are employed. The training data were sampled from
a uniform distribution on [-√3, √3], and the minibatch size was chosen as 128 during training. We
find that when this happens, in most cases the bias in the last layer is the mean value of the function
y(x), and the composition of all the previous layers is equivalent to a zero function. It can be proved
that under these conditions, the gradient vanishes, i.e., the optimization stops (Corollary 5). For
functions of different regularity, we observed the same collapse problem, see Fig. 2 for the C∞
function y(x) = x sin(5x) and Fig. 3 for the L2 function y(x) = 1{x>0} + 0.2 sin(5x).
For multi-dimensional inputs and outputs, this collapse phenomenon is also observed in our
simulations. Here, we test the target function y(x) with din = 2 and dout = 2, which
can be represented by a 2-layer neural network with width 4, y(x)
1
|x1 + x2 |
|x1 - x2 |
11
1
1
1
1 ReLU( ι
-1
-1 x). When training a 10-layer ReLU network with width 4, there
1
is a very high probability for the NN to collapse to the mean value or with low probability to the
partial mean value of y(x) (Fig. 4).
Figure 1: Demonstration of the neural network collapse to the mean value (A, with very high prob-
ability) or the partial mean value (B, with low probability) for the C0 target function y(x) = |x|.
The gradient vanishes in both cases (see Corollaries 5 and 6). A 10-layer ReLU neural network
with width 2 is employed in both (A) and (B). The biases are initialized to 0, and the weights are
randomly initialized from a symmetric distribution. The loss function is MSE.
Figure 2: Similar behavior for the C∞ target function y(x) = x sin(5x). The network parameters,
loss function, and initializations are the same as in Fig. 1. (A) corresponds to the mean value of
the target function with high probability. (B, C, D) correspond to partial mean values with low
probability and are induced by different random initializations.
We also observed the same collapse problem for other losses, such as the mean absolute error
(MAE); the results are summarized in Fig. 5 for three different functions with varying regularity.
Furthermore, we find that for MSE loss, the constant is the mean value of the target function, while
for MAE it is the median value.
3	Initialization of ReLU nets
As we demonstrated above, when the weights of the ReLU NN are randomly initialized from a
symmetric distribution, the deep and narrow NN will collapse with high probability. This type of
3
Under review as a conference paper at ICLR 2019
1.5
1
0.5
0
-0.5
ABCD
Figure 3: Similar behavior for the L2 target function y(x) = 1{x>0} + 0.2 sin(5x). The network
parameters, loss function, and initializations are the same as in Fig. 1. (A) corresponds to the mean
value of the target function with high probability. (B, C, D) correspond to partial mean values with
low probability and are induced by different random initializations.
Figure 4: Demonstration of the neural network collapse to the mean value (A, with very high prob-
ability) or the partial mean value (B, with low probability) for the C0 2-dimensional (vector) target
function y(x) = [|x1 + x2|, |x1 - x2|]. The gradient vanishes in both cases (see Corollaries 5
and 6). A 10-layer ReLU neural network with width 4 is employed in both (A) and (B). The biases
are initialized to 0, and the weights are initialized from a symmetric distribution. The loss function
is MSE.
Figure 5: Effect of the loss function on the behavior of the collapse of the neural network. MSE
(used in Figs 1, 2, 3) is compared against the MAE. The collapse of the NN is independent of the
loss function (see Theorem 4).
4
Under review as a conference paper at ICLR 2019
initialization is widely used in real applications. Here, we demonstrate that this initialization avoids
the problem of exploding/vanishing mean activation length, therefore this is beneficial for training
neural networks.
We study a feed-forward neural network N : Rdin → Rdout with L layers and N l neurons in the
layer l (N0 = din, NL = dout). The weights and biases in the layer l are an Nl × Nl-1 weight
matrix Wl and bl ∈ RNl , respectively. The input is x0 ∈ Rdin, and the neural activity in the layer
l is xl ∈ RNl . The feed-forward dynamics is given by
xl = φ(hl)	hl = Wlxl-1 +bl forl = 1,. . . ,L - 1,
N (x0) ≡ xL = hL = WLxL-1 + bL,
where φ is a component-wise activation function.
Following the work in Poole et al. (2016), we investigate how the length of the input propagates
through neural networks. The normalized squared length of the vector before activation at each
layer is defined as
1	Nl
ql = N Xg,
l i=1
(1)
where hli denotes the entry i of the vector hl . If the weights and biases are drawn i.i.d. from a
zero mean Gaussian with variance σw2 /N l-1 and σb2 respectively, then the length at layer l can be
obtained from its previous layer (see the proof in the appendix of (Poole et al., 2016), which we
include here in Appendix A)
E[ql] = σW / Dzφ(qE[ql-1 ⑶2 + σ2,	for l ≥ 2,
2
where Dz = √z e-ɪ is the standard Gaussian measure, and the initial condition is E[q1]
σW q0 + σ2, q0 = N^ x0 ∙ x0. When φ is ReLU, the recursion is simplified to
(2)
E[ql] = σW /DzReLU(qE[qlT⑶2 + σ2 = σW / Dz(qE[qlT⑶2 + σ2
=σWE[ql-1] /∞ z2Dz + σb = σwE[ql-1] /∞ z2Dz + a=专E[ql-1] + 琮.(3)
For ReLU, He normal (He et al., 2015), i.e., σw2 = 2 and σb = 0, is widely used. This choice
guarantees that E[ql] = E[ql-1], which neither shrinks nor expands the inputs. In fact, this result
explains the success of He normal in applications. A parallel work by Hanin & Rolnick (2018)
shows that initializing weights from a symmetric distribution with variance 2/fan-in (fan-in is the
dimension of the input of each layer) avoids the problem of exploding/vanishing mean activation
length. Here we arrived at the same conclusion but with much less work.
4	Theoretical analysis of the collapse problem
In this section, we present the theoretical analysis of the collapse behavior observed in Section 2,
and we also derive an estimate of the probability of this collapse. We start by stating the following
assumptions for a ReLU feed-forward neural network N (x0) : Rdin → Rdout with L layers and Nl
neurons in the layer l (N0 = din, NL = dout):
A1 The domain Ω ⊂ Rdin for N is a connected space with at least two points;
A2 The weight matrix Wl ∈ RN l ×N l-1 of any layer l ∈ {1, 2, . . . , L} is a random matrix,
where the joint distribution of (Wil1, Wil2, . . . , WilN l-1) is absolutely continuous with
respect to Lebesgue measure for i = 1, 2, . . . , Nl .
Remark: We point out here that the connectedness in assumption A1 is a very weak requirement for
the input space. The weights in a neural network are usually sampled independently from continu-
ous distributions in real applications, and thus the assumption A2 is satisfied at the NN initialization
5
Under review as a conference paper at ICLR 2019
stage; during training, the assumption A2 is usually maintained due to stochastic gradients of mini-
batch.
Lemma 1. With assumptions A1 and A2, if N(x0) is a constant function, then there exists a layer
l ∈ {1,...,L 一 1} such that hl ≤ 01 and xl = 0 ∀x0 ∈ Ω, with probability 1 (wp1).
Corollary 2. With assumptions A1 and A2, ifN(x0) is bias-free anda constant function, then there
exists a layer l ∈ {1, . . . , L 一 1} such that for any n ≥ l, it holds hn ≤ 0 and xn = 0 wp1.
Lemma 3. With assumptions A1 and A2, ifN(x0) is a constant function, then any order gradients
of the loss function with respect to the weights and biases in layers 1, . . . , l vanish, where l is the
layer obtained in Lemma 1.
Theorem 4. For a ReLU feed-forward neural network N (x0) with assumption A1, if the assumption
A2 is satisfied during the initialization, and there exists a layer l such that xl(x0) ≡ 0 for any input
x0, thenfor any function y (x0) and x0 ∈ Ω, N is eventually optimized to a constant function when
training by a gradient based optimizer. If using L2 loss and Ex0 [y(x0)] exists, then the resulted
constant is Ex0 [y(x0)], which we write as E[y] if no confusion arises; if using L1 loss and the
median of the distribution of y exists, then the resulted constant is the median.
Remark: See Appendices B, C, D and E for the proofs of Lemma 1, Corollary 2, Lemma 3 and
Theorem 4, respectively. MAE and MSE loss used in practice are discrete versions of L1 and L2
loss, respectively, if the size of minibatch is large.
Corollary 5. With assumptions A1 and A2, for a ReLU feed-forward neural network N(x0) and
any bounded function y(x0), x0 ∈ Ω, if N (x0) is a constant function with the value E[y], then the
gradients of the loss function with respect to any weight or bias vanish when using the L2 loss.
Corollary 5 can be generalized to the following corollary including more general converged mean
states.
Corollary 6. With assumptions A1 and A2, for a ReLU feed-forward neural network N(x0) and
any bounded function y(x0), x0 ∈ Ω, if ∃Kι,..., Kn ⊂ Ω and each Ki is a connected domain
with at least two points, such that
N(x0) = ∕y(χ0)	x0∈ ω \ ∪n=ιKi
Ex0K [y(x0Ki)] x0 ∈ Ki fori = 1, . . . ,n
then the gradients of the loss function with respect to any weight or bias vanish when using the L2
loss. Here x0K is the random variable ofx0 restricted to Ki.
See Appendices F and G for the proofs of Corollaries 5 and 6. We can see that Corollary 5 is a
special case of Corollary 6 with ∪n=ιKi = Ω.
Lemma 7. Let us assume that a one-layer ReLU feed-forward neural network N1 is initialized
independently by symmetric nonzero distributions, i.e., any weight or bias ofN1 is initialized by a
symmetric nonzero distribution, which can be different for different parameters. Then, for any fixed
input the corresponding output is zero with probability (1/2)dout, except the special case where all
biases and the input are zero yielding that the output is always zero.
Theorem 8. Ifa ReLU feed-forward neural network N with L layers assembled width N1, . . . , NL
is initialized randomly by symmetric nonzero distributions for weights and zero biases, then for any
fixed nonzero input, the corresponding output is zero with probability 1 一 ΠlL=1(1 一 (1/2)N l) if the
last layer also employs ReLU activation, otherwise with the probability 1 一 ΠlL=-11(1 一 (1/2)N l ).
See Appendices H and I for the proofs of Lemma 7 and Theorem 8. Although biases are initialized
to 0 in most applications, for the sake of completeness, we also consider the case where biases are
not initialized to 0.
Proposition 9. If a ReLU feed-forward neural network N with L layers assembled width
N1, . . . , NL is initialized randomly by symmetric nonzero distributions (weights and biases), then
for any fixed nonzero input, the corresponding output is zero with probability (1/2)N L if the last
layer also employs ReLU activation, otherwise the output is equal to the last bias bL with probabil-
ity (1/2)NL:
1a ≤ b denotes ai ≤ bi for any index i, i.e., component-wise. Similarly for <, > and ≥.
6
Under review as a conference paper at ICLR 2019
See Appendix J for the proof of Proposition 9. We note that Theorem 8 provides the probability for
any given input, but in Theorem 4 it requires that the entire neural network is a zero function. Hence,
the probability in Theorem 8 is an upper bound. In the following theorem, we give a theoretical
formula of the probability for the NN with width 2.
Proposition 10. Suppose the origin is an interior point of Ω. Consider a bias-free ReLU neural
network with din = 1, width 2 and L layers, and weights are initialized randomly by symmetric
nonzero distributions. Then for this neural network, the probability of being initialized to a constant
function is the last component of πL, where
πL = P L-1π1,
(4)
with π1 and P being the probability distribution after the first layer and the probability transition
matrix when one more layer is added, respectively. Here every layer employs the ReLU activation.
See Appendix K for the derivation of π1 and P. For general cases, we found that it is hard to obtain
an explicit expression for the probability, so we used numerical simulations instead, where 1 million
samples of random initialization are used to calculate each probability estimation. We show both
theoretically (Theorem 8, Propositions 9 and 10) and numerically that NN has the same probability
to collapse no matter what symmetric distributions are used, even if different distributions are used
for different weights. On the other hand, to keep the collapse probability less than p, because
the probability obtained in Theorem 8 is an upper bound, which corresponds to a safer maximum
number of layers, we have that 1 - ΠlL=1(1 - (1/2)N) ≤ p, which implies the upper bound of the
depth of NN
L ≤ ln(1 - P)
-ln(1 - (1/2)N).
(5)
Theorem 8 shows that when the NN gets deeper and narrower, the probability of the NN initialized
to a zero function is higher (Fig. 6A). Hence, we have higher probability of vanishing gradient in
almost all the layers, rather than just some neurons. In our experiments, we also found that there is
very high probability that the gradient is 0 for all parameters except in the last layer, because ReLU
is not used in the last layer. During the optimization, the neural network thus can only optimize the
parameters in the last layer (Theorem 4). When we design a neural network, we should keep the
probability less than 1% or 10%. As a practical guide, we constructed a diagram shown in Fig. 6B
that includes both theoretical predictions and our numerical tests. We see that as the number of
layers increases, the numerical tests match closer the theoretical results. It is clear from the diagram
that a 10-layer NN of width 10 has a probability of only 1% to collapse whereas a 10-layer NN of
width 5 has a probability greater than 10% to collapse; for width of three the probability is greater
than 60%.
5	Training deep and narrow neural networks
In this section, we present some training techniques and examine which ones do not suffer from the
collapse problem.
5.1	Asymmetric weight initialization
Our analysis applies for any symmetric initialization, so it is straightforward to consider asym-
metric initializations. The asymmetric initializations proposed in the literature include orthogonal
initialization (Saxe et al., 2014) and layer-sequential unit-variance (LSUV) initialization (Mishkin
& Matas, 2016). LSUV is the orthogonal initialization combined with rescaling of weights such
that the output of each layer has unit variance. Because weight rescaling cannot make the output
escape from the negative part of ReLU, it is sufficient to consider the orthogonal initialization. The
probability of collapse when using orthogonal initialization is very close to and a little lower than
that when using symmetric distributions (Fig. 7). Therefore, orthogonal initialization cannot treat
the collapse problem.
5.2	Normalization and dropout
As we have shown in the previous section, deep and narrow neural networks cannot be trained
well directly with gradient-based optimizers. Here, we employ several widely used normalization
7
Under review as a conference paper at ICLR 2019
1 .8 .6 .4 .2
0. 0. 0. 0.
A ytilibaborP
Theory ----
Approx.—
Width 2 ■
"Width 3 ∙，
Width 4 ，'
Width 5 ,▲ J
-Width 10 ▼尸
71
0
0	5	10	15
# Layers
000
001
0 1
B1 sreyal # mumixaM
二 Simulation 10%
-Simulation 1%
-APProx. 10%
-Approx. 1%
Safe region -
20	2	4	6	8	10	12	14	16
Width
Figure 6: Probability of a ReLU NN to collapse and the safe operating region. (A) Probability
of NN to collapse as a function of the number of layers for different widths. The solid black line
represents the theoretical probability (Proposition 10). The dash lines represent the approximated
probability (Theorem 8). The symbols represent our numerical tests. Similar colors correspond to
the same width. A ReLU feed-forward NN is more likely to become a zero function when it is
deeper and narrower. A bias-free ReLU feed-forward NN with din = 1 is employed with weights
randomly initialized from symmetric distributions. (The last layer also applies activations.) (B)
Diagram indicating safe operating regions for a ReLU NN. The dash lines represent Eq. 5 based on
Theorem 8 while the symbols represent our numerical tests. The maximum number of layers of a
neural network can be used at different width to keep the probability of collapse less than 1% or
10%. The region below the blue line is the safe region when we design a neural network. As the
width increases the theoretical predictions match closer with our numerical simulations.
.8 .6 .4
00 0
ytilibabor
Width 2 口
Width 3 o
-Width 4 Q
Width 5 A
Width 10 v
B
0.2
0
0
*
5	10
# Layers
15	20

$
会
Figure 7: Effect of initialization on the collapse of NN. Plotted is the probability of collapse of
a bias-free ReLU NN with din = 1 with different width and number of layers. The black filled
symbols correspond to symmetric initialization while the red open symbols correspond to orthogonal
initialization.
8
Under review as a conference paper at ICLR 2019
techniques to train this kind of networks. We do not consider some methods, such as Highway (Sri-
vastava et al., 2015) and ResNet (He et al., 2016), because in these architectures the neural nets
are no longer the standard feed-forward neural networks. Current normalization methods mainly in-
clude batch normalization (BN) (Ioffe & Szegedy, 2015), layer normalization (LN) (Ba et al., 2016),
weight normalization (WN) (Salimans & Kingma, 2016), instance normalization (IN) (Ulyanov
et al., 2016), group normalization (GN) (Wu & He, 2018), and scaled exponential linear units
(SELU) (Klambauer et al., 2017). BN, LN, IN and GN are similar techniques and follow the same
formulation, see Wu & He (2018) for the comparison.
Because we focus on the performance of these normalization methods on narrow nets and the width
of the neural network must be larger than the dimension of the input to achieve a good approxi-
mation, we only test the normalization methods on low dimensional inputs. However, LN, IN and
GN perform normalization on each training data individually, and hence they cannot be used in
our low-dimensional situations. Hence, we only examine BN, WN and SELU. BN is applied be-
fore activations while for SELU LeCun normal initialization is used (Klambauer et al., 2017). Our
simulations show that the neural network can successfully escape from the collapsed areas and ap-
proximate the target function with a small error, when BN or SELU are employed. BN changes
the weights and biases not only depending on the gradients, and different from ReLU the negative
values do not vanish in SELU. However, WN failed because it is only a simple re-parameterization
of the weight vectors.
Moreover, our simulations show that the issue of collapse cannot be solved by dropout, which in-
duces sparsity and more zero activations (Srivastava et al., 2014).
6 Conclusion
We consider here ReLU neural networks for approximating multi-dimensional functions of differ-
ent regularity, and in particular we focus on deep and narrow NNs due to their reportedly good
approximation properties. However, we found that training such NNs is problematic because they
converge to erroneous means or partial means or medians of the target function. We demonstrated
this collapse problem numerically using one- and two-dimensional functions with C0, C∞ and L2
regularity. These numerical results are independent of the optimizers we used; the converged state
depends on the loss but changing the loss function does not lead to correct answers. In particular,
we have observed that the NN with MSE loss converges to the mean or partial mean values while
the NN with MAE loss converges to the median values. This collapse phenomenon is induced by
the symmetric random initialization, which is popular in practice because it maintains the length of
the outputs of each layer as we show theoretically in Section 3.
We analyze theoretically the collapse phenomenon by first proving that if a NN is a constant function
then there must exist a layer with output 0 and the gradients of weights and biases in all the previous
layers vanish (Lemma 1, Corollary 2, and Lemma 3). Subsequently, we prove that if such conditions
are met, then the NN will converge to a constant value depending on the loss function (Theorem 4).
Furthermore, if the output of NN is equal to the mean value of the target function, the gradients of
weights and biases vanish (Corollaries 5 and 6). In Lemma 7 and Theorem 8 and Proposition 9, we
derive estimates of the probability of collapse for general cases, and in Proposition 10, we derive a
more precise estimate for deep NNs with width 2. These theoretical estimates are verified numer-
ically by tests using NNs with different layers and widths. Based on these results, we construct a
diagram which can be used as a practical guideline in designing deep and narrow NNs that do not
suffer from the collapse phenomenon.
Finally, we examine different methods of preventing deep and narrow NNs from converging to erro-
neous states. In particular, we find that asymmetric initializations including orthogonal initialization
and LSUV cannot be used to avoid this collapse. However, some normalization techniques such as
batch normalization and SELU can be used successfully to prevent the collapse of deep and narrow
NNs; on the other hand, weight normalization fails. Similarly, we examine the effect of dropout
which, however, also fails.
9
Under review as a conference paper at ICLR 2019
Acknowledgments
This work received support by the DARPA EQUiPS grant N66001-15-2-4055, the NSF grant DMS-
1736088, the AFOSR grant FA9550-17-1-0013. The research of the second author was partially
supported by the NSF of China 11771083 and the NSF of Fujian 2017J01556, 2016J01013.
References
S. Amari, H. Park, and T. Ozeki. Singularities affect dynamics of learning in neuromanifolds. Neural
computation,18(5):1007-1065, 2006.
J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450,
2016.
A. R. Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE
Transactions on Information Theory, 39(3):930-945, 1993.
R.	H. Byrd, P. Lu, J. Nocedal, and C. Zhu. A limited memory algorithm for bound constrained
optimization. SIAM Journal on Scientific Computing, 16(5):1190-1208, 1995.
D.-A. Clevert, T. Unterthiner, and S. Hochreiter. Fast and accurate deep network learning by expo-
nential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.
G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control,
Signals and Systems, 2(4):303-314, 1989.
O. Delalleau and Y. Bengio. Shallow vs. deep sum-product networks. In Advances in Neural
Information Processing Systems, pp. 666-674, 2011.
S.	Du, J. Lee, Y. Tian, B. Poczos, and A. Singh. Gradient descent learns one-hidden-layer cnn:
Don’t be afraid of spurious local minima. arXiv preprint arXiv:1712.00779, 2017.
J.	Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic
optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
R. Eldan and O. Shamir. The power of depth for feedforward neural networks. In Conference on
Learning Theory, pp. 907-940, 2016.
K.	Fukumizu and S. Amari. Local minima and plateaus in hierarchical structures of multilayer
perceptrons. Neural networks, 13(3):317-327, 2000.
X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks.
In International Conference on Artificial Intelligence and Statistics, pp. 249-256, 2010.
X. Glorot, A. Bordes, and Y. Bengio. Deep sparse rectifier neural networks. In International Con-
ference on Artificial Intelligence and Statistics, pp. 315-323, 2011.
B. Hanin and D. Rolnick. How to start training: The effect of initialization and architecture. arXiv
preprint arXiv:1803.01719, 2018.
B. Hanin and M. Sellke. Approximating continuous functions by relu nets of minimal width. arXiv
preprint arXiv:1710.11278, 2017.
K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level per-
formance on imagenet classification. In IEEE International Conference on Computer Vision, pp.
1026-1034, 2015.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In IEEE
Conference on Computer Vision and Pattern Recognition, pp. 770-778, 2016.
G. Hinton. Overview of mini-batch gradient descent. http://www.cs.toronto.edu/
~tijmen∕csc321∕Slides/lecture_slides_lec6.pdf, 2014.
K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward networks are universal approxi-
mators. Neural Networks, 2(5):359-366, 1989.
10
Under review as a conference paper at ICLR 2019
S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. In International Conference on Machine Learning, 2015.
K. Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information
Processing Systems, pp. 586-594, 2016.
D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference
on Learning Representations, 2015.
G. Klambauer, T. Unterthiner, A. Mayr, and S. Hochreiter. Self-normalizing neural networks. In
Advances in Neural Information Processing Systems, pp. 972-981, 2017.
Y. LeCun, L. Bottou, G. B. Orr, and K.-R. Muller. Efficient backprop. In Neural networks: Tricks
of the trade, pp. 9-50. Springer, 1998.
S. Liang and R. Srikant. Why deep neural networks for function approximation? In International
Conference on Learning Representations, 2017.
Z. Lu, H. Pu, F. Wang, Z. Hu, and L. Wang. The expressive power of neural networks: A view from
the width. In Advances in Neural Information Processing Systems, pp. 6231-6239, 2017.
A. L. Maas, A. Y. Hannun, and A. Y. Ng. Rectifier nonlinearities improve neural network acoustic
models. In International Conference on Machine Learning, volume 30, pp. 3, 2013.
H. Mhaskar. Neural networks for optimal approximation of smooth and analytic functions. Neural
Computation, 8(1):164-177, 1996.
H. Mhaskar, Q. Liao, and T. A. Poggio. When and why are deep networks better than shallow ones?
In Association for the Advancement of Artificial Intelligence, pp. 2343-2349, 2017.
H. N. Mhaskar and T. Poggio. Deep vs. shallow networks: An approximation theory perspective.
Analysis and Applications, 14(06):829-848, 2016.
D. Mishkin and J. Matas. All you need is a good init. In International Conference on Learning
Representations, 2016.
Q. Nguyen, M. Mukkamala, and M. Hein. Neural networks should be wide enough to learn discon-
nected decision regions. In International Conference on Machine Learning, 2018.
J. Nocedal and S. J. Wright. Numerical Optimization. Springer, 2006.
P. Petersen and F. Voigtlaender. Optimal approximation of piecewise smooth functions using deep
relu neural networks. In Conference on Learning Theory, 2018.
T. Poggio, H. Mhaskar, L. Rosasco, B. Miranda, and Q. Liao. Why and when can deep-but not
shallow-networks avoid the curse of dimensionality: a review. International Journal of Automa-
tion and Computing, 14(5):503-519, 2017.
B. Poole, S. Lahiri, M. Raghu, J. Sohl-Dickstein, and S. Ganguli. Exponential expressivity in deep
neural networks through transient chaos. In Advances in Neural Information Processing Systems,
pp. 3360-3368, 2016.
I.	Safran and O. Shamir. Spurious local minima are common in two-layer relu neural networks.
arXiv preprint arXiv:1712.08968, 2017.
T. Salimans and D. P. Kingma. Weight normalization: A simple reparameterization to accelerate
training of deep neural networks. In Advances in Neural Information Processing Systems, pp.
901-909, 2016.
A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learning
in deep linear neural networks. In International Conference on Learning Representations, 2014.
J.	s´ma. Training a single sigmoidal neuron is hard. Neural computation, 14(11):2709-2728, 2002.
11
Under review as a conference paper at ICLR 2019
N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: a simple
way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):
1929-1958, 2014.
R. K. Srivastava, K. Greff, and J. Schmidhuber. Training very deep networks. In Advances in Neural
Information Processing Systems, pp. 2377-2385, 2015.
I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the importance of initialization and momentum
in deep learning. In International Conference on Machine Learning, pp. 1139-1147, 2013.
D. Ulyanov, A. Vedaldi, and V. Lempitsky. Instance Normalization: The Missing Ingredient for
Fast Stylization. ArXiv e-prints, July 2016.
C.	Wu, J. Luo, and J. Lee. No spurious local minima in a two hidden unit relu network. In Interna-
tional Conference on Learning Representations Workshop, 2018.
Y. Wu and K. He. Group normalization. arXiv preprint arXiv:1803.08494, 2018.
D.	Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks, 94:
103-114, 2017.
C. Yun, S. Sra, and Jadbabaie A. Small nonlinearities in activation functions create bad local minima
in neural networks. arXiv preprint arXiv:1802.03487, 2018.
M. D. Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012.
Y. Zhou and Y. Liang. Critical points of neural networks: Analytical forms and landscape properties.
arXiv preprint arXiv:1710.11205, 2017.
A Derivation of the length map
For a single input x0, the normalized squared length of the vector before activation at each layer
propagates through the network. Because the weights and biases are independent from xl-1 and are
drawn i.i.d. from a zero mean Gaussian with variance σw2 /N l-1 and σb2, respectively, then
E[Wiljxlj-1] = E[Wilj]E[xlj-1] =0,
2
Var(WijXjT) = E [(Wjxj-1)2] = E [(Wj)2] E [(xj-1)2] = N-TE [(xj-1)2]
2
=N-Ie [(Φ(hj-1))2],
and for j 6= k,
cov(Wiljxlj-1, Wilkxlk-1) = E[Wiljxlj-1Wilkxlk-1] =0.
We can also see that for a fixedl ≥ 2, hlj-1 (j = 1,2,...,Nl-1)have the same distribution, and
thus Var(Wilj xlj-1) only depends on l. Then PjN=l-1 1 Wilj xlj-1 is a distribution with mean 0 and
variance σw2 E [(φ(hl1-1))2]. Because hli = PjN=l-1 1 Wiljxlj-1 + bli, then
E[hli] =0,	Var(hli)=σw2E[(φ(hl1-1))2] +σb2.
On the other hand,
1	Nl
E[ql] = E N ∑(hi)2 = E[(h1)2] = Var(hl) = *E [(Φ(h1-1))2] + σ2.
Nl i=1
Because hl1-1 = PjN=l-1 2 W1l-j 1xlj-2+bl1-1, ifl = 2 where xj0 is the input, then hl1-1 is a summation
of independent Gaussian random variables and thus is a Gaussian distribution. If l ≥ 3, by central
12
Under review as a conference paper at ICLR 2019
limit theorem, PjN=l-1 2 W1l-j 1xlj-2 converges in distribution to a Gaussian distribution as Nl-2 →
∞, so is hl1-1. Using Var(hl1-1) = E[ql-1], we have
E [(Φ(h1-1))2] = E ](。,E[ql-1] Ph-I=Iɪ!!	→ / Φ(qE[ql-1]z)2Dz,
as Nl-2 → ∞,
z2
where Dz = dz= e-^W is the standard Gaussian measure. Therefore,
2π
E[q2] = σW / Dzφ(PEqTiz)2 + σ2,
E[ql] → σW /Dzφ(qE[ql-1]z)2 + σ2
as Nl-2 → ∞ for l ≥ 3.
B Proof of Lemma 1
Lemma 11. Let A ∈ Rn×m be a random matrix, where {Aij}i∈{1,2,...,n},j ∈{1,2,...,m} are ran-
dom variables, and the joint distribution of (Ai1 , Ai2 , . . . , Aim) is absolutely continuous for
i = 1, 2, . . . , n. If x ∈ Rm is a nonzero column vector, then P(Ax = 0) = 0.
Proof. Let us consider the first value of Ax, i.e., Pjm=1 A1j xj . Because x 6= 0, we have
{Pjm=1 A1j xj = 0} is a hyperplane in Rm whose coordinates are A1j, j = 1, 2, . . . , m. Because
the joint distribution of (A11, A12, . . . , A1m) is absolutely continuous, P(Pjm=1 A1jxj = 0) = 0.
Hence,
mm
0 ≤	P(Ax =	0)	= P(X Aij xj	= 0	∀i	= 1,	2, . . . , n)	≤	P(X A1j xj	= 0) =	0.
Therefore, P(AX = 0) = 0.	□
Now let us go back to the proof of Lemma 1.
Proof. By assumption A2 and Lemma 11, N(X0) = WLXL-1 (X0) + bL is a constant function,
iff XL-1(X0) is a constant function with respect to X0. So we can assume that there is ReLU in the
last layer, and prove that there exists a layer l ∈ {1, . . . , L}, s.t., hl ≤ 0 and Xl = 0 wp1 for every
x0 ∈ Ω. We proceed in two steps.
i)	For L = 1, we have X1 = ReLU(h) = ReLU(WX0 + b) is a constant. If h is not always
≤ 0, then there exists X0 ∈ Ω and k, s.t., hk(X0) > 0. Because Ω ⊂ Rdin is a connected space
with at least two points, then Ω has no isolated points, which implies X0 is not an isolated point.
Since the neural network is a continuous map, Ω1 = {x1(x0) : x0 ∈ Ω} is connected. So there
exists X0 = X0 in the neighborhood of X0, s.t., hk(X0) > 0 and hk(X0) = hk(X0) wp1, because of
P(W(XO - X0) = 0) = 0 by Lemma 11. Hence, x1 (X0) = x1(X0), which contradicts the fact that
X1 is a constant function. Therefore, h ≤ 0 and X1 = 0.
ii)	Assume the theorem is true for L. Then for L + 1, if X1 = 0, choose l = 1 and we are done;
otherwise, consider the NN without the first layer with x1 ∈ Ω1 as the input, denoted N1. By i,
Ω1 is a connected space with at least two points. Because N1 is a constant function of x1 and has
L layers, by induction, there exists a layer whose output is zero. Therefore, for the original neural
network N, the output of such layer is also zero.
By i and ii, the statement is true for any L.	□
13
Under review as a conference paper at ICLR 2019
C Proof of Corollary 2
Proof. By Lemma 1, there exists a layer l ∈ {1, . . . , L - 1}, s.t. hl ≤ 0 and xl = 0 wp1. Because
N is bias-free, hl+1 = Wl+1xl = 0 and xl+1 = ReLU(hl+1) = 0 wp1. By induction, for any
n ≥ l, hn ≤ 0 and Xn = 0 wp1.	口
D	Proof of Lemma 3
Proof. Because xl ≡ 0, it is then obvious by backpropagation.	口
E Proof of Theorem 4
Proof. Because Xl(X0) ≡ 0, N(X0) is a constant function, and then by Lemma 3, gradients of the
loss function w.r.t. the weights and biases in layers 1, . . . , l vanish. Hence, the weights and biases
in layers 1, . . . , l will not change when using a gradient based optimizer, which implies N(X0) is
always a constant function depending on the weights and biases in layers l + 1, . . . , L. Therefore,
N will be optimized to a constant function, which has the smallest loss. For L2 loss, this constant
with the smallest loss is E[y]. For L1 loss, this constant with the smallest loss is its median. 口
F	Proof of Corollary 5
Proof. Because N(X0) is a constant function, by Lemma 1 and Theorem 4, N is optimized to E[y].
Also, since N is equal to E[y], gradients vanish.	□
G	Proof of Corollary 6
Proof. It suffices to show that gradients vanish for x0 ∈ Ki, i = 1,...,n and x0 ∈ Ω \ ∪n=ιKi.
i)	When X0 is restricted on Ki, N(X0) is a constant function with value Ex0 [y(X0K )]. Similar to
Corollary 5, gradients vanish when using the L2 loss.
ii)	For x0 ∈ Ω \ ∪n=ιKi, the loss at x0 is 0, so gradients vanish.
By i and ii, gradients vanish when using the L2 (MSE) loss.	口
H	Proof of Lemma 7
Proof. Let X = (x1, x2, . . . , xdin) be any input, and y = (y1, y2, . . . , ydout) be the corresponding
output. For i = 1, . . . , dout ,
y = ReLU(Wi ∙ x + bi) = ReLU((Wiι,...,widin,bi) ∙ (x1,x2,...,xdin, 1)).
Because (wi1, . . . , widin, bi) is a (din + 1)-dim vector initialized by a symmetric distribution, then
P((Wi1,...,widin ,bi) ∙ (x1,χ2, . . .,xdin , I) > O) = J
So P(yi = 0) = 2, and then P(y = 0) = Πd=utP(yi = 0) = (2)dout. Here P denotes the
probability.	口
I	Proof of Theorem 8
Proof. If the last layer also employs ReLU activation, by Lemma 7, P(Xl = 0|Xl-1 6= 0) =
(1/2)N l for l = 1, . . . , L. Then, for any fixed input X0 6= 0,
P(XL 6= 0) = P(XL-1 6= 0)P(XL 6= 0|XL-1 6= 0) + P(XL-1 = 0)P(XL 6= 0|XL-1 = 0)
=P(XLT = 0)P(XL = 0∣xl-1 = 0) = P(XLT = 0)(1-(1/2)N l)=…=∏=i(1-(1∕2)n l).
14
Under review as a conference paper at ICLR 2019
The last equality holds because P(x0 6= 0) = 1.
If in the last layer we do not apply ReLU activation, then P(xL 6= 0) = P(xL-1 6= 0) = ΠlL=-11 (1 -
(1/2)N l).	口
J	Proof of Proposition 9
Proof. If the last layer also has ReLU activation, by Lemma 7,
P(xL = 0) = P(xL-1 6= 0)P(xL = 0|xL-1 6= 0) + P(xL-1 = 0)P(xL = 0|xL-1 = 0)
= P(xL-1 6= 0)(1/2)NL + P(xL-1 = 0)(1/2)NL = (1/2)NL.
If the last layer does not have ReLU activation, and L ≥ 2, then
P(xL = bn) = P(xL-1 = 0) = (1/2)NL-1.
For L = 1, N is a single layer perceptron, which is a trivial case.	口
K Proof of Proposition 10
Proof. We consider a ReLU neural network with din = 1 and each hidden layer with width 2.
Because all biases are zero, then it is easy to see the following fact: when the input is 0, the output
of any neuron in any layer is 0; when the input is negative, the output of any neuron in any layer is a
linear function with respect to the input; when the input is positive, the output of any neuron in any
layer is also a linear function with respect to the input. Because the origin is an interior point of Ω,
then it suffices to consider a subset [-a, a] ⊂ Ω with a ∈ R+. The output of each hidden layer has
16 possible cases:
case (1):
ω1 x
ω2x
ωjX
ωgx
x ∈ [0, a]
x ∈ [-a, 0]
case (2):
ω1 x
ω2 x
ω^X
0
X ∈ [0, a]
X ∈ [-a, 0]
case (3): 1	ω1 X ω2 X (ω0 Xj	, X ∈ [0, a] , , X ∈ [-a, 0]	case (4): 1	ω1 X ω2 X ( 00 j ,	, X ∈ [0, a] X ∈ [-a, 0]
I	ω01X	, X ∈ [0, a]	1	(ω01Xj	, X ∈ [0, a]
case (5):	*	,	case (6):	*	
	ω ω*x 1 ω2*X	, X ∈ [-a, 0]		ω1*X 0	, X ∈ [-a, 0]
II	ω01X	, X ∈ [0, a]	II		ω01X	, X ∈ [0, a]
case (7):	( ω02*X j	, X ∈ [-a, 0]	, case (8):	(00j,	X ∈ [-a, 0]
case (9):	( ω02*X j	, X ∈ [0, a] ,	case (10):	( ω02*X j	, X ∈ [0, a]
[	( ωω21*XX j	, X ∈ [-a, 0]	[	ω1*X 0	, X ∈ [-a, 0]
15
Under review as a conference paper at ICLR 2019
case (11):	(ω0x )，X ∈ [0,a]	八 J ( ω0x )，x ∈ [0,a] 0	, case (12):	0	, (ω2x J , x ∈ [-a, 0]	I ( 0 ) , x ∈ [-a, 0]
II	00	,	x ∈ [0, a]	II	00	,	x ∈ [0, a]
case (13):	∕' J∖	, case (14):4/1 J∖ Ux) , x ∈ [-a, 0]	I (ω0 x)，x ∈ [-a, 0]
II	00	,	x ∈ [0, a]	II	00	, x ∈ [0, a]
case (15):	0	, case (16):	0 (ω2x J , x ∈ [-a, 0]	I ( 0 ) , x ∈ [-a, 0]
where wι, w2, wɪ, w2 are some coefficients.
Each case in the lth hidden layer may also induce all 16 cases in the (l + 1)th layer. For any given
case in the lth hidden layer, we will compute the probabilities of these 16 cases for the (l + 1)th
layer as follows.
i) Case (1)
Note that ω = (ω1,ω2) lies in the first quadrant, and ω* = (ωj, ωg) lies in the third quadrant. Then
the output of the next layer is
{( ReLU((A11ω1 + A12ω2)x) ʌ
ReLU((A21ω1 + A22ω2)x)	,
R ReLU((AiW； + Ai2ω2)x) ʌ
y ReLU((A2iω: + A22ω2)x) J ，
x ∈ [0, a]
x ∈ [-a, 0]
Since the matrix ( A11 A12 ) is random, for fixed ω and ω*, the probability of case (1) is
4ω,ω*)
卜 2π
assume
that
2
.Without loss of generality, We can assume that ∣∣ωk = ∣∣ω*k = 1, and hence We can
that ω = (Cos θ, Sin θ), θ ∈ (0, ∏) and ω* = (Cos ψ, Sin ψ), ψ ∈ (π, 3∏). It is easy to see
∠(ω, ω*)
ψ - θ,	ψ ≤ θ + π
2π + θ - ψ, ψ > θ + π
Since ω, ω* are random, the probability of case (1) is
与 I'π dθ
π2 0
£ )2 dψ=96
Similarly, the probability of cases (6), (11) and (16) in the (l + 1)th layer are also 祭.For cases (2),
(3), (5), (8), (9), (12), (14) and (15), the probability is
22 ∕2 dθ /3π ∠3 ω*)
Π2J0 dθ Jn
For cases (4), (7), (10) and (13), the probability is
2π — ∠(ω, ω*)
2π
dψ = 312
22「dθ
π2 0
2π — ∠(ω, ω*)
2π
2 dψ J
W 96
ii) Case (2) (the same method can be applied for cases (3), (5) and (9))
16
Under review as a conference paper at ICLR 2019
Note that in this case We can assume that ω = (Cos θ, Sin θ), θ ∈ (0, ∏2) and ω* = (-1,0) is a
constant vector. It is easy to see that ∠(ω, ω*) = ∏ - θ, and hence the probabilities of cases (1),
(6), (11) and (16) are
2 [π (∠ωωι Y dθ = L
∏ 00 <	2∏	J 48
Similarly, the probabilities of cases (2), (3), (5), (8), (9), (12), (14) and (15) are
2 广 ∠(ω, ω*) 2π - ∠(ω, ω*)	_ ɪ
π Jo	2π ∙	2π	= 24,
and the probabilities of cases (4), (7), (10) and (13) are
2 J： (∖山)2 dθ=48.
iii)	Case (4) (the same method can be applied for cases (8) and (12))
The output of the next layer is
( ReLU((A11ω1 + A12ω2)x)
ReLU((A21ω1 + A22ω2)x)
( 00	,
x ∈ [0, a]
x ∈ [-a, 0]
It is easy to see that the probabilities of cases (4), (8), (12) and (16) are 4, and the probabilities of
all other cases are 0.
iv)	Case (6) (the same method can be applied for case (11))
The output of the next layer is
ReLU(A11ω1x)
ReLU(A21ω1x)
ReLU(Aιιωjx)
ReLU(A2iωjx)
x ∈ [0, a]
x ∈ [-a, 0]
Note that in this case, ωι > 0 and ωj < 0, and thus it is not hard to see that the probabilities of
cases (1), (6), (11) and (16) are 4, and the probabilities of all the other cases are 0.
v)	Case (7) (the same method can be applied for case (10))
The output of the next layer is
ReLU(A11ω1 x)
ReLU(A21ω1x)
ReLU(Ai2ωgx)
ReLU(A22ωgx)
x ∈ [0, a]
x ∈ [-a, 0]
Therefore, the probabilities of all the 16 cases are 表.
vi)	Case (13) (the same method can be applied for cases (14) and (15))
Similar to the argument of the case (4), it is easily to see that the probabilities for cases (13), (14),
(15) and (16) are 14, and the probabilities for all other cases are 0.
vii)	Case (16)
The output of the next layer is the case (16) With probability 1.
17
Under review as a conference paper at ICLR 2019
By i, ii, iii, iv, v, vi and vii, we can get the probability transition matrix
1-40
O 1-40
O 1-40
Ooo 1-40 O O
■
00001
O 1-40 Ool-
O 1-40 Oool
77¥-¥-¥-¥-¥7¥-¥-¥-¥-^7¥-¥-¥-¥-¥_
-4
-4
-4
-4
-4
一16
-8
-4
O O 0 1-40 O 0 1-40 O 0 1-40 O 0 1-4
l-¥-¥-¥-¥-¥-¥-¥-¥-¥-¥-¥-¥-¥-¥-¥-^
1-40 O O 0 1-40 O O 0 1-40 O O 0 1-4
7卒-罕苧-1-翠卒卒苧苧-¥卒-举-竿-¥-翠布
O O 0 1-40 O 0 1-40 O 0 1-40 O 0 1-4
7卒-罕-罕-1-翠卒-竿-1¥卒卒¥卒-1-翠 7
7卒-罕-举-1-翠卒-竽苧-举-¥卒-举-竿-举-举石
举-苧-%71-擘-1 军否苧-1-擘-%7
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
O 1-41-41-41_
0
0
0
0
0
0
0
0
0
0
0
O 1-41-41-41_
0
0
0
0
0
0
0
0
0
0
0
O 1-41-41-41_
0
0
0
0
!-¥-¥-¥-¥-¥-¥-¥-¥-¥-¥-¥-¥-¥-¥-¥_
=
P
where Pji is the probability of that the (l + 1)th layer is case j when the ith layer is case i.
Furthermore, direct computations show that the probability distribution vector of the first hidden
layer π 1 is
T
1
1
1
1
0,0,0,4,0,0,4, 0,0,4,0,0,4, 0,0,0
Therefore, the probability distribution of the lth hidden layer is
πl = Pl-1π1.
□
18