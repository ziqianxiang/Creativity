Under review as a conference paper at ICLR 2019
A Synaptic Neural Network
and Synapse Learning
Anonymous authors
Paper under double-blind review
Ab stract
A Synaptic Neural Network (SynaNN) consists of synapses and neurons. Inspired
by the synapse research of neuroscience, we built a synapse model with a nonlinear
synapse function of excitatory and inhibitory channel probabilities. Introduced
the concept of surprisal space and constructed a commutative diagram, we proved
that the inhibitory probability function -log(1-exp(-x)) in surprisal space is the
topologically conjugate function of the inhibitory complementary probability 1-x in
probability space. Furthermore, we found that the derivative of the synapse over the
parameter in the surprisal space is equal to the negative Bose-Einstein distribution.
In addition, we constructed a fully connected synapse graph (tensor) as a synapse
block of a synaptic neural network. Moreover, we proved the gradient formula
of a cross-entropy loss function over parameters, so synapse learning can work
with the gradient descent and backpropagation algorithms. In the proof-of-concept
experiment, we performed an MNIST training and testing on the MLP model with
synapse network as hidden layers.
1	Introduction
Synapses play an important role in biological neural networks (Hubel & Kandel (1979)). They are
joint points of neurons’ connection with the capability of learning and memory in neural networks.
Based on the analysis of excitatory and inhibitory channels of synapses(Hubel & Kandel (1979)), we
proposed a probability model (Feller (2008) for probability introduction) of the synapse together with
a non-linear function of excitatory and inhibitory probabilities (Li (2017)(synapse function)). Inspired
by the concept of surprisal from (Jones (1979)(self-information), Levine (2009), Bernstein & Levine
(1972)(surprisal analysis), Levy (2008)(surprisal theory in language)) or negative logarithmic space
(Miyashita et al. (2016)), we proposed the concept of surprisal space and represented the synapse
function as the addition of the excitatory function and inhibitory function in the surprisal space. By
applying a commutative diagram, we figured out the fine structure of inhibitory function and proved
that it was the topologically conjugate function of an inhibitory function. Moreover, we discovered
(rediscovered) that the derivative of the inhibitory function over parameter was equal to the negative
Bose-Einstein distribution (Nave (2018)). Furthermore, we constructed a fully connected synapse
graph and figured out its synapse tensor expression. From synapse tensor and a cross-entropy loss
function, we found and proved its gradient formula that was the basis for gradient descent learning
and using backpropagation algorithm. In surprisal space, the parameter (weight) updating for learning
was the addition of the value of the negative Bose-Einstein distribution. Finally, we designed the
program to implement a Multiple Layer Perceptrons (MLP) (Minsky et al. (2017)) for MNIST (LeCun
et al. (2010)) and tested it to achieve the near equal accuracy of standard MLP in the same setting.
1.1	Background
Hodgkin and Huxley presented a physiological neuron model that described the electronic potential
of the membrane between a neuron and a synapse with a differential equation (Hodgkin & Huxley
(1952)). Later, neuron scientists have found that a synapse might have a complicated channel structure
with rich chemical and electronic properties (Lodish et al. (1995)(biological synapse), Destexhe
et al. (1994)(computing synaptic conductances), Abbott & Nelson (2000)(synaptic plasticity)). Other
synapse models based on differential equations had been proposed and been simulated by analogy
1
Under review as a conference paper at ICLR 2019
circuits like Spiking Neural Network (SNN) (Kumar (2017) (differential equations), Lin et al. (2018)
(Intel’s SNN Loihi)). In these approaches, synapses acted as linear amplifiers with adjustable
coefficients. An example was the analog circuit implementation of Hopfield neural network (Hopfield
& Tank (1986)(analog neural circuits)).
In this paper, we proposed a simple synapse model represented by the joint opening probability of
excitatory and inhibitory channels in a synapse. It was described as a non-linear computable synapse
function. This neuroscience-inspired model was motivated on our unpublished research to solve
optimization problems by neural networks.
To do learning by gradient descent and backpropagation algorithm (Goodfellow et al. (2016)(book on
deep learning)), because of the differentiable of the synapse function in the synaptic neural network,
we could compute Jacobian matrix explicitly and compute the gradient of the cross-entropy loss
function over parameters. Therefore, we provided a detailed proof of the formula of gradients in
Appendix A
In the process of analyzing Jacobian matrix, we found that the derivative of the inhibitory function
log(1 - e-x) was equal to the 1/(ex - 1) which was the formula of Bose-Einstein distribution
(Einstein (1924)(quantum ideal gas)). In statistical physics and thermodynamics, Bose-Einstein
distribution had been concluded from the geometric series of the Bose statistics.
A dual space analysis was an efficient scientific method. After successful expressing fully-connected
synapse network in a logarithmic matrix, we started to consider log function and log space. The
concept of surprisal (where was the first definition of surprisal?), which was the measurement of
surprise from Information Theory (Shannon (1948)), gave us hints. Original surprisal was defined on
the random variable, however, it was convenient to consider the probability itself as a variable. So we
introduced the surprisal space with a mapping function -log(p). The motivation was to transform any
points from probability space to surprisal space and in reverse.
In surprisal space, a synapse function was the addition of an excitatory identity function and an
inhibitory function. Although we had figured out the inhibitory function being -log(1 - e-x), we
wanted to know its structure and what class it belonged to.
This was a procedure that we rediscovered the way to construct a commutative diagram for synapse
inhibitory function Diagram (2.2.3). In 1903, Mathematician Bertrand Russell presented the first
commutative diagram in his book (Russell (2009)) before the category theory. You can find a good
introduction of applied category theory by (Bradley (2018)). In this paper, we did not require to know
category theory.
The basic idea was to given two spaces and two points in source space which have corresponding
points in target space by a continuous and inverse mapping function from source space to target space,
plus, a function that maps start point to the endpoint in the same source space. Our question is to
find the function that maps the corresponding start point to the corresponding endpoint in the target
space (refer to diagram 2.2.3). There are two paths from source start point to target endpoint: one
is from top-left, go right and down to bottom-right; another is from top-left, go down and right to
bottom-right. The solution is to solve the equation that has the same target endpoint.
We found that the synapse inhibitory function -log(1 - e-x) was also a topologically conjugate
function. Therefore, the synaptic neural network has the same dynamical behavior in both probability
space and surprisal space. To convince that the synaptic neural network can work for learning and
using the backpropagation algorithm, we proved the gradients of loss function by applying basic
calculus. In surprisal space, the negative Bose-Einstein distribution was applied to the updating of
parameters in the learning of synaptic neural network. Finally, we implemented a MNIST experiment
of MLP to be the proof-of-concept.
1.2	Contributions
1)	present a neuroscience-inspired synapse model and a synapse function based on the opening
probability of channels. 2) defined surprisal space to link information theory to the synaptic neural
network. 3) figure out function G(x) = -log(1 - e-x) as the inhibitory part of a synapse. 4)
find the derivative of G(x) to be the formula of negative Bose-Einstein distribution. 5) discover
G(x) to be a topologically conjugate function of the complementary probability. 6) represent fully-
2
Under review as a conference paper at ICLR 2019
connected synapse as a synapse tensor. 7) express synapse learning of gradient descent as a negative
Bose-Einstein distribution.
2	S ynaptic Neural Network ( S ynaNN)
A Synaptic Neural Network (SynaNN) contains non-linear synapse networks that connect to neurons.
A synapse consists of an input from the excitatory-channel, an input from the inhibitory-channel, and
an output channel which sends a value to other synapses or neurons. Synapses may form a graph to
receive inputs from neurons and send outputs to other neurons. In advance, many synapse graphs can
connect to neurons to construct a neuron graph. In traditional neural network, its synapse graph is
simply the wight matrix or tensor.
2.1	Synapse in Probability Space
2.1.1	Biological Synapse
Changes in neurons and synaptic membranes (i.e. potential gate control channel and chemical gate
control channel show selectivity and threshold) explain the interactions between neurons and synapses
(Torre & Poggio (1978)). The process of the chemical tokens (neurotransmitters) affecting the control
channel of the chemical gate is accomplished by a random process of mixing tokens of the small
bulbs on the membrane. Because of the randomness, a probabilistic model does make sense for the
computational model of the biological synapse (Hubel & Kandel (1979)).
In a synapse, the Na+ channel illustrates the effect of an excitatory-channel. The Na+ channels allow
the Na+ ions flow in the membrane and make the conductivity increase, then produce excitatory
post-synapse potential. The K+ channels illustrate the effects of inhibitory channels. The K+ channel
that lets the K+ ions flow out of the membrane shows the inhibition. This makes the control channel of
potential gate closing and generates inhibitory post-potential of the synapse. Other kinds of channels
(i.e. Ca channel) have more complicated effects. Biological experiments show that there are only two
types of channels in a synapse while a neuron may have more types of channels on the membrane.
Experiments illustrate that while a neuron is firing, it generates a series of spiking pulse where the
spiking rate (frequency) reflects the strength of stimulation.
2.1.2	Neuroscience-inspired synapse model
From neuroscience, there are many types of chemical channels in the membrane of a synapse. They
have the following properties: 1) the opening properties of ion channels reflect the activity of synapses.
2) the excitatory and inhibitory channels are two key types of ion channels. 3) the random properties
of channels release the statistical behavior of synapses.
From the basic properties of synapses, we proposed the synapse model below:
1)	The open probability x of the excitatory channel (α-channel) is equal to the number of open
excitatory channels divided by the total number of excitatory channels of a synapse. 2) The open
probability y of the inhibitory channel (β-channel) is equal to the number of open inhibitory channels
divided by the total number of inhibitory channels of a synapse. 3) The joint probability of a synapse
that affects the activation of the connected output neuron is the product of the probability of excitatory
channel and the complementary probability of the inhibitory channel. 4) There are two parameters to
control excitatory channel and inhibitory channel respectively.
Given two random variables (X, Y ), their probabilities (x, y), and two parameters (α, β), the joint
probability distribution function S(x, y; α, β) for X, Y (the joint probability ofa synapse that activates
the connected neuron) is defined as
S(x, y; α, β) = αx(1 - βy)	(1)
where x ∈ (0, 1) is the open probability of all excitatory channels and α > 0 is the parameter of
the excitatory channels; y ∈ (0, 1) is the open probability of all inhibitory channels and β ∈ (0, 1)
is the parameter of the inhibitory channels. The symbol semicolon “;” separates the variables and
parameters in the definition of function S.
3
Under review as a conference paper at ICLR 2019
2.2	Synapse in surprisal space
Surprisal (self-information) is a measure of the surprise in the unit of bit, nat, or hartley when a
random variable is sampled. Surprisal is a fundamental concept of information theory and other basic
concepts such as entropy can be represented as the function of surprisal. The concept of surprisal has
been successfully used in molecular chemistry and natural language research.
2.2.1	Surprisal space
Given a random variable X with value x, the probability of occurrence of x is p(x). The standard
definitions of Surprisal Ip(x) is the measure of the surprise in the unit of a bit (base 2), a nat (base
e), or a hartley (base 10) when the random variable X is sampled at x. Surprisal is the negative
logarithmic probability of x such that Ip(x) = -log(p(x)). Ignored random variable X, we can
consider p(x) as a variable in Probability Range Space or simply called Probability Space in the
context of this paper which is the open interval (0,1) of real numbers.
Surprisal Function is defined as I : (0, 1) → (0, ∞) and I(x) = -log(x) where x ∈ (0, 1) is an
open interval in R+. Its inverse function is I-1(u) = e-u where u ∈ R+. Since surprisal function
I(x) is bijective, exists inverse and is continuous, I(x) is a homeomorphism.
Surprisal Space S is the mapping space of the Probability Space P with the negative logarithmic
function which is a bijective mapping from the open interval (0, 1) of real numbers to the real open
interval (0, ∞) = R+ .
S = {s ∈ (0, ∞) : s = -log(p), f orall p ∈ (0, 1)}	(2)
The probability space P and the surprisal space S are topological spaces of real open interval (0,1)
and positive real numbers R+ that inherit the topology of real line respectively.
2.2.2	Surprisal synapse
Given variables u, v ∈ S and parameters θ, γ ∈ S which are equal to variables -log(x), -log(y) and
parameters -log(α), -log(β) respectively. The Surprisal Synapse LS(u, v; θ, γ) ∈ S is defined
as,
LS(u, v; θ, γ) = -log(S(x, y; α, β))	(3)
Expanding the right side, there is LS (u, v; θ, γ) = (-log(αx)) + (-log(1 - βy)). The first part is
an identity mapping plus a parameter. To understand the second part more, we need to figure out its
structure and class.
2.2.3	Topological conjugacy
Theorem 1 (Topologically conjugate function). Given y = F(x) where F(x) = 1 - x; x, y ∈ P,
(u, v) = I(x, y) where u, v ∈ S, and the homeomorphism I (x) = -log(x) from P to S, then function
G such that v = G(u) is
G(U) = IoFoI-1(U) = -log(1 - e-U)	(4)
Proof. Building a commutative diagram with the homeomorphism I(x) below,
F
X ∈ P------> y ∈ P
I	I
u ∈ S ——→ V ∈ S
G
The proof is to figure out the equivalent of two paths from x to v. One path is from top x, go right to y
and go down to bottom so v = I (F (x)). Another path is from top x, go down to u and go right to
bottom so v = G o I, thus, I(F(x)) = G (I (x)). Let o be the composition of functions, the previous
4
Under review as a conference paper at ICLR 2019
equation is I ◦ F = G ◦ I. Applying I-1 on both right sides and compute G on given functions, we
ProvedEq.(4).	□
Given two topological spaces P and S, continuous function F : P → P and G : S → S as well as
homeomorPhism I : P → S, if I ◦ F = G ◦ I, then G is called the toPologically conjugated function
of the function F in the standard definition. From Theorem 1, sPecially G(u) = -log(1 - e-u) is
the toPologically conjugate function of the comPlementary Probability function 1 - x. Features:
i)	The iterated function F and its toPologically conjugate function G have the same dynamics. ii) They
have the same maPPed fixed Point where F : x = 1/2 and G : u = -log(1/2). iii) I(x) = -log(x)
is a infinite differentiable and continuous function in real oPen interval (0,1).
Let Parametric function be D(u; θ) = u + θ, the surPrisal synaPse is
LS(u,v; θ,γ) = D(u; θ) +(IoFoIT)(D(v; Y))	(5)
From Eq.(5), the universal function of a surPrisal synaPse is the addition of the excitatory function
and the toPologically conjugate inhibitory function in surPrisal sPace. By constructed a commutative
diagram, we figured out the elegant structure and toPological conjugacy of the function -log(1-e-u),
which is a new examPle of the commutative diagram and the toPological conjugate function from
synaPtic neural network. A bridge has been built to connect the synaPtic neural network to the
category theory, the toPology, and the dynamical system.
2.2.4	Bose-Einstein distribution
It is interesting to find the connection between the surPrisal synaPse and the toPologically conjugate
function. Furthermore, we are going to figure out the connection between the surPrisal synaPse and
the Bose-Einstein distribution. The Bose-Einstein distribution (BED) is rePresented as the formula
f (E) = AeE∕kτ-ι where f(E) is the probability that a particle has the energy E in temperature T. k is
Boltzmann constant, A is the coefficient (Nave (2018)).
Theorem 2. The BED function is defined as BED(v; Y) = eγ+V-] where variable V ∈ S, parameter
γ ∈ S, and V + Y ≥ ln(2), so that 0 ≤ BED(v; Y) ≤ 1, then there is ∂YG(D(v; Y)) = BED(v; Y) or
∂1
T(-log(1 - e-D(v;Y))) = -^7-.--	(6)
∂y	eD(v;Y) - 1	` '
Proof. Proved by computing the derivative of the function on left side.	□
Recall that D(V; Y) = V+Y, the derivative of the topologically conjugate function G over parameter Y
is equal to the negative Bose-Einstein distribution. The gradient of the surprisal synapse LS(u, V; θ, Y)
is
(TT； ⅛,∣-； dΓ)LS(u, V； θ, Y) = (1; 1,-BED(v; y)； -BED(v, Y))	⑺
∂u ∂θ ∂V ∂Y
This is a connection between surprisal synapse and statistical physics. In physics, BED(V； Y) can be
thought of as the probability that boson particles remain in V energy level with an initial value Y.
2.3	Synapse graph and tensor
Generally, a biological neuron consists of a soma, an axon, and dendrites. Synapses are distributed
on dendritic trees and the axon connects to other neurons in the longer distance. A synapse graph is
the set of synapses on dendritic trees of a neuron. A synapse can connect its output to an input of
a neuron or to an input of another synapse. A synapse has two inputs: one is excitatory input and
another is inhibitory input. Typically neurons receive signals via the synapses on dendrites and send
out spiking plus to an axon (Hubel & Kandel (1979)).
Assume that the total number of input of the synapse graph equals the total number of outputs, the
fully-connected synapse graph is defined as
n
yi(x;βi) = Xiɪɪ(1 — βijXj), for all i ∈ [1,n]	(8)
j=1
5
Under review as a conference paper at ICLR 2019
where X = (xi, ∙∙∙ ,Xn), Xi ∈ (0,1) and y = (yι, ∙∙∙ ,yn) are row vectors of probability distribution;
βi = (βii, ,…，βin), 0 < βij < 1 are row vectors of parameters; β = matrix{βij } is the matrix of
all parameters. α = 1 is assigned to Eq.1 to simplify the computing.
An output yi of the fully-connected synapse graph is constructed by linking the output of a synapse
to the excitatory input of another synapse in a chain while the inhibitory input of each synapse is the
output of neuron xi in series. In the case of the diagonal value βii is zero, there is no self-correlated
factor in the ith item.
This fully-connected synapse graph represents that only neuron itself acts as excitation all its con-
nected synapses act as inhibition. This follows the observation of neuroscience that most synapses
act as inhibition.
Theorem 3 (Synapse tensor formula). The following synapse tensor formula Eq.9 is equivalent to
fully-connected synapse graph defined in the Eq.8
log(y) = log(x) +1 |x| * log( 1 ∣β∣ - diag(x) * βτ)	⑼
or ICy) = I(X) + 1 |x| *I(1∣β∣ 一 diag(X) * βτ) where x, y, and β are distribution vectors and
parameter matrix. βτ is the transpose of the matrix β. 1|x| is the row vector of all real number ones
and 1 ∣β∣ is the matrix of all real number ones that have the same size and dimension of x and β
respectively. Moreover, the * is the matrix multiplication, diag(x) is the diagonal matrix of the row
vector x, and the log is the logarithm of the tensor (matrix).
Proof. Applying the log on both sides of the definition Eq.(8) and completes the matrix multipli-
cations in the fully-connected synapse graph, we proved the formula Eq.(9). Furthermore, by the
definition of I(x), we have the expression of surprisal synapse.	□
2.4 Synapse learning
To prove that synapse learning of synaptic neural network is compatible with the standard backprop-
agation algorithm, we are going to apply cross-entropy as the loss function and use gradient descent
to minimize that loss function.
2.4. 1 Gradient of loss function
The basic idea of deep learning is to apply gradient descent optimization algorithm to update the
parameters of the deep neural network and achieve a global minimization of the loss function
(Goodfellow et al. (2016)).
Theorem 4 (Gradient equation). Let the loss function L(o, o) ofthe fully-connected synapse graph
Eq.8be equal to the sum ofcross-entropy L(0, o) = 一 Ei 0ilog(θi), then its parameter gradient is
∂L(0,o)	/ 八、 -yiXj
一 = (OiT 一 Oi)-一
∂βij	1 一 βij xj
(10)
or ∂L(0,O)∕∂βij = (oiT — 0i)∂log(S(yi, Xj; 1, βij))∕∂βj where 0 is the target vector and o is the
output vector and the fully-connected synapse graph outputs through a softmax activation function
that is oj = sof tmaX(yj).
Proof. The proof is given in the Appendix A.
□
2.4.2 Gradient and Bose-Einstein distribution
Considering the surprisal space, let (Uk,vk,γki) = -log(xk, yk,βki), the fully-connected synapse
graph is denoted as Vk = Uk + Pii(-log(1 — e-(Yki+Ui))) , Compute the gradient over parameters
∂vk	`X e-(Yki+Ui) ∂γki X. e-(Yki+Ui)
∂γpq = -N 1 — e-(Yki+Ui) ∂γpq = -N 1 - e-(Yi)队跖
(11)
6
Under review as a conference paper at ICLR 2019
because only When k = P and i = q, two δ are 1, so ∂vp =
and reformulating, we have
∂v =	-1
∂γij	eγi +uj — 1
1-—(Yz+uqq)) . Replacing the indexes
(12)
The right side of Eq.(12) is the negative Bose-Einstein Distribution in the surprisal space.
To compute the loss function in surprisal space, we convert the target vector o and output vector o to
surprisal space as (O, o), so the new loss function is L(t, t) = Pk tk * tk. The log function has been
removed in L(t, t) because log is implied in the surprisal space. Without using an activation function,
there is tk = vk . By Eq.(12),
∂L(t,t)	Xɔ ∂L(t, t) ∂vk	-ti
∂γij	∂v ∂vk ∂γij	eγi+uj - 1
We can apply error back-propagation to implement gradient descent for synapse learning.
∂L(^,t)
Yij - Yij-η^γr
(13)
(14)
where η is the learning rate.
The equation Eq.(14) illustrates that the learning of synaptic neural network follows the Bose-Einstein
statistics in the surprisal space. This paper “Memory as an equilibrium Bose gas” by (Frohlich (1968),
Pascual-Leone (1970)) showed that memory maybe possible to be represented as the equilibrium of
Bose gas.
3	Experiments
3.1	SynaMLP : Synaptic Multiple Layer Perceptrons
We are going to illustrate a Synaptic Neural Network implementation SynaMLP with the connection
of Multiple Layer Perceptrons (MLP) (Minsky et al. (2017)).

Figure 1: SynaMLP: (green, blue, red) dots are (input, hidden, output) layers.

SynaMLP has an input layer for down-sampling and an output layer for classification. The hidden
layer is a block of fully-connected synapse tensor. The inputs of the block are neurons from the input
layer and the outputs of the block are neurons to the output layer. The block is the implementation
of synapse tensor in Eq.(9). The activation functions are connected synapse tensor to the output
layer. Moreover, the input of the block is a probability distribution. The block can be thought of the
replacement of the weight layer in a standard neural network.
7
Under review as a conference paper at ICLR 2019
3.2	MNIST Experiment
To proof-of-concept, we implemented the SynaMLP with MNIST. Hand-written digital MNIST data
sets are used for training and testing in machine learning. It is split into three parts: 60,000 data points
of training data (mnist.train), 10,000 points of test data (mnist.test), and 5,000 points of validation
data (mnist.validation) (LeCun et al. (2010)).
	Synapse MLP	Dense MLP
Iteration	10001	10001
Test loss	0.0942904587357543	0.0906127168575471
Test accuracy	0.9802000087499618	0.9830000066757202
Table 1: SynaMLP MNIST Testing
The MNIST SynaMLP training and testing is implemented by Python, Keras and Tensorflow (Abadi
et al. (2016)) from the revision of the example of mnist_mlp.py in Keras distribution. The synapse
tensor is designed to be a class to replace Dense in Keras. The layer sequence is as below,
Layer Type	Output Shape	Param #
Dense[1]	(None, 300)	235500
Batch-normalization[1]	(None, 300)	1200
Activation[1]	(None, 300)	0
Synapse[1]	(None, 300)	90000
Batch-normalization[2]	(None, 300)	1200
Dense[2]	(None, 10)	3010
Optimizer	Adam	default
Table 2: SynaMLP Layers
In the comparison experiment, SynaNN MLP and traditional MLP generated the similar test accuracy
of around 98%. We applied a softmax activation function in front of the input of synapse to avoid
the error of NAN (computing value out of the domain). In fact, synaptic neural network handles a
probability distribution (vector from neurons).
4	Conclusion
In this paper, we presented and analyzed a Synaptic Neural Network (SynaNN). We found the fine
structure of synapse and the construction of synapse network as well as the BE distribution in the
gradient descent learning. In surprisal space, the input of a neuron is the addition of the identity
function and the sum of topologically conjugate functions of inhibitory synapses which is the sum of
bits of information. The formula of surprisal synapse function is defined as
LS(u,v; θ,γ) = (θ + u) + (I ◦F ◦IT)(Y + V))	(15)
The non-linear synaptic neural network may be implemented by physical or chemical components.
Instead of using a simple linear synapse function, more synapse functions maybe found in the
researches and applications of neural network.
8
Under review as a conference paper at ICLR 2019
References
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for large-scale
machine learning. In OSDI, volume 16, pp. 265-283, 2016.
Larry F Abbott and Sacha B Nelson. Synaptic plasticity: taming the beast. Nature neuroscience, 3
(11s):1178, 2000.
RB Bernstein and RD Levine. Entropy and chemical change. i. characterization of product (and
reactant) energy distributions in reactive molecular collisions: information and entropy deficiency.
The Journal of Chemical Physics, 57(1):434-449, 1972.
Tai-Danae Bradley. What is applied category theory? arXiv preprint arXiv:1809.05923, 2018.
Alain Destexhe, Zachary F Mainen, and Terrence J Sejnowski. An efficient method for computing
synaptic conductances based on a kinetic model of receptor binding. Neural computation, 6(1):
14-18, 1994.
Albert Einstein. Quantum theory of a monoatomic ideal gas, 1924. URL http://www.thphys.
uni-heidelberg.de/~amendola/otherstuff/einstein-paper-v2.pdf.
Willliam Feller. An introduction to probability theory and its applications, volume 2. John Wiley &
Sons, 2008.
Herbert Frohlich. Long-range coherence and energy storage in biological systems. International
Journal of Quantum Chemistry, 2(5):641-649, 1968.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT press Cambridge, 2016.
Alan L Hodgkin and Andrew F Huxley. A quantitative description of membrane current and its
application to conduction and excitation in nerve. The Journal of physiology, 117(4):500-544,
1952.
John J Hopfield and David W Tank. Computing with neural circuits: A model. Science, 233(4764):
625-633, 1986.
David H. Hubel and Eric R. Kandel. The brain. Scientific American, 241(3):45-53, 1979.
Douglas Samuel Jones. Elementary information theory. Clarendon Press, 1979.
Arvind Kumar. Simple neuron and synapse models, 2017. URL https://www.kth.se/
social/files/570b469bf276542ebe37fe9f/SimpleNeuronModels_2016_
full.pdf.
Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. AT&T Labs, 2, 2010.
Raphael D Levine. Molecular reaction dynamics. Cambridge University Press, 2009.
Roger Levy. Expectation-based syntactic comprehension. Cognition, 106(3):1126-1177, 2008.
Chang Li. A nonlinear synaptic neural network based on excitation and inhibition, 2017. URL
https://www.researchgate.net/publication/320557823_A_Non-linear_
Synaptic_Neural_Network_Based_on_Excitation_and_Inhibition.
Chit-Kwan Lin, Andreas Wild, Gautham N Chinya, Yongqiang Cao, Mike Davies, Daniel M Lavery,
and Hong Wang. Programming spiking neural networks on intel’s loihi. Computer, 51(3):
52-61, 2018. URL https://www.researchgate.net/publication/323434400_
Programming_Spiking_Neural_Networks_on_Intel_Loihi.
Harvey Lodish, Arnold Berk, S Lawrence Zipursky, Paul Matsudaira, David Baltimore, James Darnell,
et al. Molecular cell biology, volume 3. WH Freeman New York, 1995.
Marvin Minsky, Seymour A Papert, and Leon Bottou. Perceptrons: An introduction to computational
geometry. MIT press, 2017.
9
Under review as a conference paper at ICLR 2019
Daisuke Miyashita, Edward H Lee, and Boris Murmann. Convolutional neural networks using
logarithmic data representation. arXiv preprint arXiv:1603.01025, 2016.
Carl R. Nave. The bose-einstein distribution, 2018. URL http://hyperphysics.phy-astr.
gsu.edu/hbase/quantum/disbe.html.
Juan Pascual-Leone. A mathematical model for the transition rule in piaget’s developmental stages.
Acta Psychologica, 32:301-345,1970.
Bertrand Russell. Principles of mathematics. Routledge, 2009.
Claude Elwood Shannon. A mathematical theory of communication. Bell system technical journal,
27(3):379-423, 1948.
V Torre and T Poggio. A synaptic mechanism possibly underlying directional selectivity to motion.
Proc. R. Soc. Lond. B, 202(1148):409-416, 1978.
Appendix A Gradient Equation Theorem
Let the loss function L(o, o) of the fully-connected synapse graph Eq.8 be equal to the sum of
cross-entropy L(o, o) = - Pi ^ilog(oi), then its item of parameter gradient is
∂L(o,o) _	-yiXj
∂βij	°"	°i 1-βj Xj
or
∂L(o,o)	/	∂log(S(yi,Xj; 1,βij))
-βj- =(OiT-Oi)------------M----------
(16)
(17)
where o is the target vector and o is the output vector and the fully-connected synapse graph outputs
through a softmax activation function.
Proof. Given oj = sof tmax(yj ), then
∂log(oj) _
∂yk
where δjk is the Kronecker delta and τ
δjk - ok ,
∂L(o, o)
∂yk
okτ - Ok
(18)
Eq.8, we can compute the partial derivative over parameters as,
Zj Oj is a constant. After applying log on both sides of
∂log(yk)
∂βpq
Σ-xi	dBki _	-Xi δ δ -	—Xq	δ
i 1 - βkiXi 百=J 1 - βkiXi kp iq = 1 - βkqXq kp
(19)
then applying log derivative on the left side and multiple yk on both sides, we have
∂yk =	-Xqyk δ
西=1 - βkqXq kp
(20)
Since
∂L(o, o)
∂βpq
∂L(0,o) ∂yk
∂yk ∂βpq
(21)
from Eq.18 and Eq.20, we have
dL(O, O)	X(	八、
^βpτ = ”-ok)
k
-Xqyk	-Xqyp
I _ β. τ δkp = (OpT - Op)I _ β T
- βkq Xq	- βpq Xq
(22)
Σ
k
replace index p,q by i,j in Eq.22, We proved the gradient formula Eq.10 which is Eq.4 as well. □
10