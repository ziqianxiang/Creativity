Under review as a conference paper at ICLR 2019
Neural separation of observed and
UNOBSERVED DISTRIBUTIONS
Anonymous authors
Paper under double-blind review
Ab stract
Separating mixed distributions is a long standing challenge for machine learning
and signal processing. Applications include: single-channel multi-speaker separa-
tion (cocktail party problem), singing voice separation and separating reflections
from images. Most current methods either rely on making strong assumptions on
the source distributions (e.g. sparsity, low rank, repetitiveness) or rely on hav-
ing training samples of each source in the mixture. In this work, we tackle the
scenario of extracting an unobserved distribution additively mixed with a signal
from an observed (arbitrary) distribution. We introduce a new method: Neural
Egg Separation - an iterative method that learns to separate the known distri-
bution from progressively finer estimates of the unknown distribution. In some
settings, Neural Egg Separation is initialization sensitive, we therefore introduce
GLO Masking which ensures a good initialization. Extensive experiments show
that our method outperforms current methods that use the same level of supervi-
sion and often achieves similar performance to full supervision.
1	Introduction
Humans are remarkably good at separating data coming from a mixture of distributions, e.g. hearing
a person speaking in a crowded cocktail party. Artificial intelligence, on the the hand, is far less adept
at separating mixed signals. This is an important ability as signals in nature are typically mixed,
e.g. speakers are often mixed with other speakers or environmental sounds, objects in images are
typically seen along other objects as well as the background. Understanding mixed signals is harder
than understanding pure sources, making source separation an important research topic.
Mixed signal separation appears in many scenarios corresponding to different degrees of supervi-
sion. Most previous work focused on the following settings:
Full supervision: The learner has access to a training set including samples of mixed signals {yi } ∈
Y as well as the ground truth sources of the same signals {bi } ∈ B and {xi} ∈ X (such that
yi = xi + bi). Having such strong supervision is very potent, allowing the learner to directly learn
a mapping from the mixed signal yi to its sources (xi , bi). Obtaining such strong supervision is
typically unrealistic, as it requires manual separation of mixed signals. Consider for example a
musical performance, humans are often able to separate out the different sounds of the individual
instruments, despite never having heard them play in isolation. The fully supervised setting does
not allow the clean extraction of signals that cannot be observed in isolation e.g. music of a street
performer, car engine noises or reflections in shop windows.
Synthetic full supervision: The learner has access to a training set containing samples from the
mixed signal {yi} ∈ Y as well as samples from all source distributions {bj} ∈ B and {xk} ∈ X .
The learner however does not have access to paired sets of the mixed and unmixed signal ground
truth (that is for any given yi in the training set, bi and xi are unknown). This supervision setting is
more realistic than the fully supervised case, and occurs when each of the source distributions can
be sampled in its pure form (e.g. we can record a violin and piano separately in a studio and can thus
obtain unmixed samples of each of their distributions). It is typically solved by learning to separate
synthetic mixtures bj + xk of randomly sampled bj and xk.
No supervision: The learner only has access to training samples of the mixed signal Y but not to
sources B and X . Although this settings puts the least requirements on the training dataset, it is
1
Under review as a conference paper at ICLR 2019
a hard problem and can be poorly specified in the absence of strong assumptions and priors. It is
generally necessary to make strong assumptions on the properties of the component signals (e.g.
smoothness, low rank, periodicity) in order to make progress in separation. This unfortunately
severely limits the applicability of such methods.
In this work we concentrate on the semi-supervised setting: unmixing of signals in the case where
the mixture Y consists of a signal coming from an unobserved distribution X and another signal
from an observed distribution B (i.e. the learner has access to a training set of clean samples such
that {bj } ∈ B along with different mixed samples {yi} ∈ Y). One possible way of obtaining such
supervision, is to label every signal sample by a label, indicating if the sample comes only from
the observed distribution B or if it is a mixture of both distributions B + X . The task is to learn a
parametric function able to separate the mixed signal yi ∈ Y into sources xi ∈ X and bi ∈ B s.t.
yi = bi + xi . Such supervision is much more generally available than full supervision, while the
separation problem becomes much simpler than when fully unsupervised.
We introduce a novel method: Neural Egg Separation (NES) - consisting of i) iterative estimation
of samples from the unobserved distribution X ii) synthesis of mixed signals from known samples
of B and estimated samples of X iii) training of separation regressor to separate the mixed signal.
Iterative refinement of the estimated samples of X significantly increases the accuracy of the learned
masking function. As an iterative technique, NES can be initialization sensitive. We therefore
introduce another method - GLO Masking (GLOM) - to provide NES with a strong initialization.
Our method trains two deep generators end-to-end using GLO to model the observed and unobserved
sources (B and X). NES is very effective when X and B are uncorrelated, whereas initialization by
GLOM is most important when X and B are strongly correlated such as e.g. separation of musical
instruments. Initialization by GLOM was found to be much more effective than by adversarial
methods.
Experiments are conducted across multiple domains (image, music, voice) validating the effective-
ness of our method, and its superiority over current methods that use the same level of supervision.
Our semi-supervised method is often competitive with the fully supervised baseline. It makes few
assumptions on the nature of the component signals and requires lightweight supervision.
2	Previous Work
Source separation: Separation of mixed signals has been extensively researched. In this work,
we focus on single channel separation. Unsupervised (blind) single-channel methods include: ICA
(Davies & James, 2007) and RPCA (Huang et al., 2012). These methods attempt to use coarse priors
about the signals such as low rank, sparsity or non-gaussianity. HMM can be used as a temporal
prior for longer clips (Roweis, 2001), however here we do not assume long clips. Supervised source
separation has also been extensively researched, classic techniques often used learned dictionaries
for each source e.g. NMF (Wilson et al., 2008). Recently, neural network-based gained popularity,
usually learning a regression between the mixed and unmixed signals either directly (Huang et al.,
2014) or by regressing the mask (Wang et al., 2014; Yu et al., 2017). Some methods were devised to
exploit the temporal nature of long audio signal by using RNNs (Mimilakis et al., 2017), in this work
we concentrate on separation of short audio clips and consider such line of works as orthogonal.
One related direction is Generative Adversarial Source Separation (Stoller et al., 2017; Subakan &
Smaragdis, 2017) that uses adversarial training to match the unmixed source distributions. This is
needed to deal with correlated sources for which learning a regressor on synthetic mixtures is less
effective. We present an Adversarial Masking (AM) method that tackles the semi-supervised rather
than the fully supervised scenario and overcomes mixture collapse issues not present in the fully
supervised case. We found that non-adversarial methods perform better for the initialization task.
The most related set of works is semi-supervised audio source separation (Smaragdis et al., 2007;
Barker & Virtanen, 2014), which like our work attempt to separate mixtures Y given only samples
from the distribution of one source B. Typically NMF or PLCA (which is a similar algorithm
with a probabilistic formulation) are used. We show experimentally that our method significantly
outperforms NMF.
Disentanglement: Similarly to source separation, disentanglement also deals with separation in
terms of creating a disentangled representation of a source signal, however its aim is to uncover
2
Under review as a conference paper at ICLR 2019
latent factors of variation in the signal such as style and content or shape and color e.g. Denton et al.
(2017); Higgins et al. (2016). Differently from disentanglement, our task is separating signals rather
than the latent representation.
Generative Models: Generative models learn the distribution of a signal directly. Classical ap-
proaches include: SVD for general signals and NMF (Lee & Seung, 2001) for non-negative signals.
Recently several deep learning approaches dominated generative modeling including: GAN (Good-
fellow et al., 2016), VAE (Kingma & Welling, 2013) and GLO (Bojanowski et al., 2018). Adversar-
ial training (for GANs) is rather tricky and often leads to mode-collapse. GLO is non-adversarial and
allows for direct latent optimization for each source making it more suitable than VAE and GAN.
3	Neural Egg Separation (NES)
In this section we present our method for separating a mixture of sources of known and unknown
distributions. We denote the mixture samples yi , the samples with the observed distribution bi and
the samples from the unobserved distribution xi. Our objective is to learn a parametric function T(),
such that bi = T(yi).
Full Supervision: In the fully supervised setting (where pairs of yi and bi are available) this task
reduces to a standard supervised regression problem, in which a parametric function T() (typically
a deep neural network) is used to directly optimize:
arg min L = E'(T (yi),bi)
Ti
(1)
Where typically ` is the Euclidean or the L1 loss. In this work we use `() = L1 ().
Mixed-unmixed pairs are usually unavailable, but in some cases it is possible to obtain a training
set which includes unrelated samples xj and bk e.g. (Wang et al., 2014; Yu et al., 2017). Methods
typically randomly sample xj and bk sources and synthetically create mixtures yjk = xj + bk . The
synthetic pairs (bk, yjk) can then be used to optimize Eq. 1. Note that in cases where X and B are
correlated (e.g. vocals and instrumental accompaniment which are temporally dependent), random
synthetic mixtures ofx and b might not be representative ofy and fail to generalize on real mixtures.
Semi-Supervision: In many scenarios, clean samples of both mixture components are not available.
Consider for example a street musical performance. Although crowd noises without street perform-
ers can be easily observed, street music without crowd noises are much harder to come by. In this
case therefore samples from the distribution of crowd noise B are available, whereas the samples
from the distribution of the music X are unobserved. Samples from the distribution of the mixed
signal Y i.e. the crowd noise mixed with the musical performance are also available.
The example above illustrates a class of problems for which the distribution of the mixture and a
single source are available, but the distribution of another source is unknown. In such cases, it is not
possible to optimize Eq. 1 directly due to the unavailability of pairs of b and y.
Neural Egg Separation: Fully-supervised optimization (as in Eq. 1) is very effective when pairs of
bi and yi are available. We present a novel algorithm, which iteratively solves the semi-supervised
task as a sequence of supervised problems without any clean training examples of X . We name the
method Neural Egg Separation (NES), as it is akin to the technique commonly used for separating
egg whites and yolks.
The core idea of our method is that although no clean samples from X are given, it is still possible to
learn to separate mixtures of observed samples bj from distribution B combined with some estimates
of the unobserved distribution samples χ¾. Synthetic mixtures are created by randomly sampling an
approximate sample Xi from the unobserved distribution and combining with training sample bj:
yij = bj + Xi
(2)
thereby creating pairs 函也)for supervised training. Note that the distribution of synthetic mix-
tures yij might be different from the real mixture sample distribution y7-, but the assumption (which
is empirically validated) is that it will eventually converge to the correct distribution.
3
Under review as a conference paper at ICLR 2019
During each iteration of NES, a neural separation function T () is trained on the created pairs by
optimizing the following term:
T = arg min Lι(T(bj + Xi),bj)	(3)
At the end of each iteration, the separation function T() can be used to approximately separate the
training mixture samples yi into their sources:
~ 一, ~
bi = T(yi),χi = y - bi	(4)
The refined X domain estimates Xi are used for creating synthetic pairs for finetuning T() in the
next iteration (as in Eq. 3).
The above method relies on having an estimate of the unobserved distribution samples as input to
the first iteration. One simple scheme is to initialize the estimates of the unobserved distribution
samples in the first iteration as Xi = C ∙ yi, where C is a constant fraction (typically 0.5). Although
this initialization is very naive, we show that it achieves very competitive performance in cases
where the sources are independent. More advanced initializations will be discussed below.
At test time, separation is simply carried out by a single application of the trained separation function
T () (exactly as in Eq. 4).
Data: Mixture samples {yi }, Observed source samples {bj }
Result: Separation function T()
Initialize synthetic unobservable samples with Xi J C ∙ yi or using AM or GLOM;
Initialize T () with random weights;
while iter < N do
Synthesize mixtures yi7- = bj + Xi for all bj in B;
Optimize separation function for P epochs: T = arg minτ Pi Lι(T(yj), bi);
Update estimates of unobserved distribution samples: Xi = yi — T(yi)
end
Algorithm 1: NES Algorithm
Our full algorithm is described in Alg. 1. For optimization, we use SGD using ADAM update with
a learning rate of 0.01. In total we perform N = 10 iterations, each consisting of optimization ofT
and estimation of Xi, P = 25 epochs are used for each optimization ofEq. 3.
GLO Masking: NES is very powerful in practice despite its apparent simplicity. There are some
cases for which it can be improved upon. As with other synthetic mixture methods, it does not
take into account correlation between X and B e.g. vocals and instrumental tracks are highly re-
lated, whereas randomly sampling pairs of vocals and instrumental tracks is likely to synthesize
mixtures quite different from Y. Another issue is finding a good initialization—this tends to affect
performance more strongly when X and B are dependent.
We present our method GLO Masking (GLOM), which separates the mixture by a distributional
constraint enforced via GLO generative modeling of the source signals. GLO (Bojanowski et al.,
2018) learns a generator G(), which takes a latent code zb and attempts to reconstruct an image or
a spectrogram: b = G(zb). In training, GLO learns end-to-end both the parameters of the generator
G() as well as a latent code zb for every training sample b. It trains per-sample latent codes by direct
gradient descent over the values of zb (similar to word embeddings), rather than by a feedforward
encoder used by autoencoders (e.g. zb = E(b)). This makes it particularly suitable for our scenario.
Let us define the set of latent codes: Z = [z1..zN]. The optimization is therefore:
arg min	kb, G(zb)k
Z,G b∈B
(5)
We propose GLO Masking, which jointly trains generators: GB() for B and GX() for X such
that their sum results in mixture samples y = GB (zyB) + GX (zyX). We use the supervision of
the observed source B to train GB (), while the mixture Y contributes residuals that supervise the
4
Under review as a conference paper at ICLR 2019
training ofGX(). We also jointly train the latent codes for all training images: Zb ∈ Z for all b ∈ B,
and ZyB ∈ ZB , ZyX ∈ ZX for all y ∈ Y . The optimization problem is:
arg min E l∣b, GB (zb)k + λ ∙ £ ∣∣y, GB (ZB) + Gχ (Zy )∣∣
Z,ZB,ZX,GB,GX b∈B	y∈Y
(6)
As GLO is able to overfit arbitrary distributions, it was found that constraining each latent code vec-
tor Z to lie within the unit ball Z ∙ Z ≤ 1 is required for generalization. Eq. 6 can either be optimized
end-to-end, or the left-hand term can be optimized first to yield Z, GB (), then the right-hand term
is optimized to yield ZB , ZX , GX (). Both optimization procedures yield similar performance (but
separate training does not require setting λ). Once GB () and GX () are trained, for a new mixture
sample we infer its latent codes:
arg min ly, GB (ZyB ) + GX (ZyX)l
zx,zb
Our estimate for the sources is then:
b = GB (Zy)	X = Gy (ZX)
(7)
(8)
Masking Function: In separation problems, we can exploit the special properties of the task e.g.
that the mixed signal yi is the sum of two positive signals xi and bi . Instead of synthesizing the new
sample, we can instead simply learn a separation mask m(), specifying the fraction of the signal
which comes from B. The attractive feature of the mask is always being in the range [0, 1] (in the
case of positive additive mixtures of signals). Even a constant mask will preserve all signal gradients
(at the cost of introducing spurious gradients too). Mathematically this can be written as:
T(y) = ym(y)
(9)
For NES (and baseline AM described below), we implement the mapping function T (yi) using the
product of the masking function yi ∙ m(yi). In practice We find that learning a masking function
yields much better results than synthesizing the signal directly (in line with other works e.g. Wang
et al. (2014); Gabbay et al. (2017)).
GLOM models each source separately and is therefore unable to learn the mask directly. Instead we
refine its estimate by computing an effective mask from the element-wise ratio of estimated sources:
GB(Zb)
m = ---------：__：--------：——-
gb (Zb) + gx (Zx)
(10)
Initializing Neural Egg Separation by GLOM: Due to the iterative nature of NES, it can be
improved by a good initialization. We therefore devise the following method: i) Train GLOM
on the training set and infer the mask for each mixture. This is operated on images or mel-scale
spectrograms at 64 × 64 resolutions ii) For audio: upsample the mask to the resolution of the high-
resolution linear spectrogram and compute an estimate of the X source linear spectrogram on the
training set iii) Run NES on the observed B spectrograms and estimated X spectrograms. We find
experimentally that this initialization scheme improves NES to the point of being competitive with
fully-supervised training in most settings.
4	Experiments
To evaluate the performance of our method, we conducted experiments on distributions taken from
multiple real-world domains: images, speech and music, in cases where the two signals are corre-
lated and uncorrelated.
We evaluated our method against 3 baseline methods:
Constant Mask (Const): This baseline uses the original mixture as the estimate.
Semi-supervised Non-negative Matrix Factorization (SS-NMF): This baseline method, proposed by
Smaragdis et al. (2007), first trains a set ofl bases on the observed distribution samples B by Sparse
5
Under review as a conference paper at ICLR 2019
Figure 1: A Qualitative Separation Comparison on Mixed Bag and Shoe Images
Const NMF AM GLOM NES FT SUP GT
NMF (Hoyer, 2004; Kim & Park, 2007). It factorizes B = Hb * Wb, with activations Hb and
bases Wb, all matrices are non-negative. The optimization is solved using the Non-negative Least
Squares solver by Kim & Park (2011). It then proceeds to train another factorization on the mixture
Y training samples with 2l bases, where the first l bases (Wb) are fixed to those computed in the
previous stage: Y = Hyb * Wb + HyX * Wx. The separated sources are then: X = hyχ * Wx and
b = hyb * Wb. More details can be found in the appendix B.
Adversarial Masking (AM): As an additional contribution, we introduce a new semi-supervised
method based on adversarial training, to improve over the shallow NMF baseline. AM trains a
masking function m() so that after masking, the training mixtures are indistinguishable from the
distribution of source B under an adversarial discriminator D(). The loss functions (using LS-GAN
(Mao et al., 2017)) are given by:
arg min LD =	D(y m(y))2 +	(D(b) - 1)2	arg min Lm =	(D(y m(y)) - 1)2
D	y∈Y	b∈B	m	y∈Y
(11)
Differently from CycleGAN (Zhu et al., 2017) and DiscoGAN (Kim et al., 2017), AM is not bidi-
rectional and cannot use cycle constraints. We have found that adding magnitude prior L1(m(y), 1)
improves performance and helps prevent collapse. To partially alleviate mode collapse, we use
Spectral Norm (Miyato et al., 2018) on the discriminator.
We evaluated our proposed methods:
GLO Masking (GLOM): GLO Masking on mel-spectrograms or images at 64 × 64 resolution.
Neural Egg Separation (NES): The NES method detailed in Sec. 3. Initializing X estimates using a
constant (0.5) mask over Y training samples.
Finetuning (FT): Initializing NES with the X estimates obtained by GLO Masking.
To upper bound the performance of our method, we also compute a fully supervised baseline, for
which paired data of bi ∈ B, xi ∈ X and yi ∈ Y are available. We train a masking function with
the same architecture as used by all other regression methods to directly regress synthetic mixtures
to unmixed sources. This method uses more supervision than our method and is an upper bound.
6
Under review as a conference paper at ICLR 2019
Table 1: Image SeParation AccUracy (PSNR dB/SSIM)
X	B	Const	NMF	AM	GLOM	NES	FT	Supervised
0-4	5-9	10.6/0.65	16.5/0.71	17.8/0.83	15.1/0.76	23.4/0.95	23.9/0.95	24.1/0.96
5-9	0-4	10.8/0.65	15.5/0.66	18.2/0.84	15.3/0.79	23.4/0.95	23.8/0.95	24.4/0.96
Bags	Shoes	6.9/0.48	13.9/0.48	15.5/0.67	15.1/0.66	22.3/0.85	22.7/0.86	22.9/0.86
Shoes	Bags	10.8/0.65	11.8/0.51	16.2/0.65	14.8/0.65	22.4/0.85	22.8/0.86	22.8/0.86
More implementation details can be found in appendix A.
4.1	Separating Mixed Images
In this section we evalUate the effectiveness of oUr method on image mixtUres. We condUct exPeri-
ments both on the simPler MNIST dataset and more comPlex Shoes and Handbags datasets.
4.1.1	MNIST
To evalUate the qUality of oUr method on image seParation, we design the following exPerimental
Protocol. We sPlit the MNIST dataset (LeCUn & Cortes, 2010) into two classes, the first consisting
of the digits 0-4 and the second consisting of the digits 5-9. We condUct exPeriments where one
soUrce has an observed distribUtion B while the other soUrce has an Unobserved distribUtion X . We
Use 12k B training images as the B training set, while for each of the other 12k B training images,
we randomly samPle a X image and additively combine the images to create the Y training set. We
evalUate the Performance of oUr method on 5000 Y images similarly created from the test set of X
and B. The exPeriment was rePeated for both directions i.e. 0-4 being B while 5-9 in X, as well as
0-4 being X while 5-9 in B.
In Tab. 1, we rePort oUr resUlts on this task. For each exPeriment, the toP row Presents the resUlts
(PSNR and SSIM) on the X test set. DUe to the simPlicity of the dataset, NMF achieved reasonable
Performance on this dataset. GLOM achieves better SSIM bUt worse PSNR than NMF while AM
Performed 1-2dB better. NES achieves mUch stronger Performance than all other methods, achieving
aboUt 1dB worse than the fUlly sUPervised Performance. Initializing NES with the masks obtained by
GLOM, resUlts in similar Performance to the fUlly-sUPervised UPPer boUnd. FT from AM (nUmbers
for finetUning from AM were omitted from the tables for clarity, as they were inferior to finetUning
from GLOM in all exPeriments) achieved similar Performance (24.0/0.95 and 23.8/0.95) to FT
from GLOM.
4.1.2	Bags and Shoes
In order to evalUate oUr method on more realistic images, we evalUate on seParating mixtUres con-
sisting of Pairs of images samPled from the Handbags (ZhU et al., 2016) and Shoes (YU & GraUman,
2014) datasets, which are commonly Used for evalUation of conditional image generation methods.
To create each Y mixtUre image, we randomly samPle a shoe image from the Shoes dataset and a
handbag image from the Handbags dataset and sUm them. For the observed distribUtion, we samPle
another 5000 different images from a single dataset. We evalUate oUr method both for cases when
the X class is Shoes and when it is Handbags.
From the resUlts in Tab. 1, we can observe that NMF failed to Preserve fine details, Penalizing its
Performance metrics. GLOM (which Used a VGG PercePtUal loss) Performed mUch better, dUe to
greater exPressiveness. AM Performance was similar to GLOM on this task, as the PercePtUal loss
and stability of training of non-adversarial models helPed GLOM greatly. NES Performed mUch
better than all other methods, even when initialized from a constant mask. FinetUning from GLOM,
helPed NES achieve stronger Performance, nearly identical to the fUlly-sUPervised UPPer boUnd.
It Performed better than finetUning from AM (not shown in table) which achieved 22.5/0.85 and
22.7/0.86 . Similar conclUsions can be drawn from the qUalitative comParison in the figUre above.
7
Under review as a conference paper at ICLR 2019
Table 2: Speech Separation Accuracy (PSNR dB)
X	B	Const	NMF	AM	GLOM	NES	FT	Supervised
Speech	Noise	0.0	2.4	5.7	3.3	7.5	7.5	8.3
4.2	Separating Speech and Environmental Noise
Separating environmental noise from speech is a long standing problem in signal processing. Al-
though supervision for both human speech and natural noises can generally be obtained, we use this
task as a benchmark to evaluate our method’s performance on audio signals where X and B are not
dependent. This benchmark is a proxy for tasks for which a clean training set of X sounds cannot be
obtained e.g. for animal sounds in the wild, background sounds training without animal noises can
easily be obtained, but clean sounds made by the animal with no background sounds are unlikely to
be available.
We obtain clean speech segments from the Oxford-BBC Lip Reading in the Wild (LRW) Dataset
(Chung & Zisserman, 2016), and resample the audio to 16 kHz. Audio segments from ESC-50
(Piczak, 2015), a dataset of environmental audio recordings organized into 50 semantic classes, are
used as additive noise. Noisy speech clips are created synthetically by first splitting clean speech
into clips with duration of 0.5 seconds, and adding a random noise clip, such that the resulting SNR
is zero. We then compute a mel-scale spectrogram with 64 bins, using STFT with window size of 25
ms, hop length of 10 ms, and FFT size of 512, resulting in an input audio feature of 64 × 64 scalars.
Finally, power-law compression is performed with p = 0.3, i.e. A0.3, where A is the input audio
feature.
From the results in Tab. 2, we can observe that GLOM, performed better than Semi-Supervised
NMF by about 1dB better. AM training, performed about 2dB better than GLOM. Due to the
independence between the sources in this task, NES performed very well, even when trained from a
constant mask initialization. Performance was less than 1dB lower than the fully supervised result
(while not requiring any clean speech samples). In this setting due to the strong performance of NES,
initializing NES with the speech estimates obtained by GLOM (or AM), did not yield improved
performance.
4.3	Music Separation
Separating vocal music into singing voice and instrumental music as well as instrumental music
and drums has been a standard task for the signal processing community. Here our objective is
to understand the behavior of our method in settings where X and B are dependent (which makes
synthesis by addition of random X and B training samples a less accurate approximation).
For this task we use the MUSDB18 Dataset (Rafii et al., 2017), which, for each music track, com-
prises separate signal streams of the mixture, drums, bass, the rest of the accompaniment, and the
vocals. We convert the audio tracks to mono, resample to 20480 Hz, and then follow the procedure
detailed in Sec. 4.2 to obtain input audio features.
From the results in Tab. 3, we can observe that NMF was the worst performer in this setting (as its
simple bases do not generalize well between songs). GLOM was able to do much better than NMF
and was even competitive with NES on Vocal-Instrumental separation. Due to the dependence be-
tween the two sources and low SNR, initialization proved important for NES. Constant initialization
NES performed similarly to AM and GLOM. Finetuning NES from GLOM masks performed much
better than all other methods and was competitive with the supervised baseline. GLOM was much
better than AM initialization (not shown in table) that achieved 0.9 and 2.9.
5	Discussion
GLO vs. Adversarial Masking: GLO Masking as a stand alone technique usually performed worse
than Adversarial Masking. On the other hand, finetuning from GLO masks was far better than
finetuning from adversarial masks. We speculate that mode collapse, inherent in adversarial training,
8
Under review as a conference paper at ICLR 2019
Table 3: MUsic SeParation AccUracy (Median SDR dB)
X	B	Const	NMF	AM	GLOM	NES	FT	Supervised
Vocals	InstrUmental	-3.5	0.0	0.3	0.6	0.3	2.1	2.7
DrUms	InstrUmental	-3.3	-0.5	1.5	0.8	1.3	3.5	3.6
makes the adversarial masks a lower bound on the X source distribution. GLOM can result in
models that are too loose (i.e. that also encode samPles oUtside of X). BUt as an initialization for
NES finetUning, it is better to have a model that is too loose than a model which is too tight.
Supervision Protocol: SUPervision is imPortant for soUrce seParation. ComPletely blind soUrce
seParation is not well sPecified and simPly Using general signal statistics is generally Unlikely to yield
comPetitive resUlts. Obtaining fUll sUPervision by Providing a labeled mask for training mixtUres is
Unrealistic bUt even synthetic sUPervision in the form of a large training set of clean samPles from
each soUrce distribUtion might be Unavailable as some soUnds are never observed on their own (e.g.
soUnds of car wheels). OUr setting significantly redUces the reqUired sUPervision to sPecifying if
a certain soUnd samPle contains or does not contain the Unobserved soUrce. SUch sUPervision can
be qUite easily and inexPensively Provided. For fUrther samPle efficiency increases, we hyPothesize
that it woUld be Possible to label only a limited set of examPles as containing the target soUnd and
not, and to Use this seed dataset to finetUne a deeP soUnd classifier to extract more examPles from an
Unlabeled dataset. We leave this investigation to fUtUre work.
Signal-Specific Losses: To showcase the generality of oUr method, we chose not to encode task
sPecific constraints. In Practical aPPlications of oUr method however we believe that Using signal-
sPecific constraints can increase Performance. ExamPles of sUch constraints inclUde: rePetitiveness
of mUsic (Rafii & Pardo, 2011), sParsity of singing voice, smoothness of natUral images.
Non-Adversarial Alternatives: The good Performance of GLOM vs. AM on the vocals seParation
task, sUggests that non-adversarial generative methods may be sUPerior to adversarial methods for
seParation. This has also been observed in other maPPing tasks e.g. the imProved Performance of
NAM (Hoshen & Wolf, 2018) over DCGAN (Radford et al., 2015).
Convergence of NES: A Perfect signal seParation fUnction is a stable global minimUm of NES as
i) the synthetic mixtUres are eqUal to real mixtUres ii) real mixtUres are Perfectly seParated. In all
NES exPeriments (with constant, AM or GLOM initialization), NES converged after no more than
10 iterations, tyPically to different local minima. Itis emPirically evident that NES is not gUaranteed
to converge to a global minimUm (althoUgh it converges to good local minima). We defer formal
convergence analysis of NES to fUtUre work.
6	Conclusions
In this PaPer we ProPosed a novel method—NeUral Egg SeParation—for seParating mixtUres of
observed and Unobserved distribUtions. We showed that carefUl initialization Using GLO Masking
imProves resUlts in challenging cases. OUr method achieves mUch better Performance than other
methods and was UsUally comPetitive with fUll-sUPervision.
References
Tom Barker and TUomas Virtanen. Semi-sUPervised non-negative tensor factorisation of modUlation
sPectrograms for monaUral sPeech seParation. In 2014 International Joint Conference on Neural
Networks (IJCNN),pp. 3556-3561. IEEE, 2014.
Piotr Bojanowski, Armand JoUlin, David LoPez-Paz, and ArthUr Szlam. OPtimizing the latent sPace
of generative networks. In ICML’18, 2018.
J. S. ChUng and A. Zisserman. Lip reading in the wild. In Asian Conference on Computer Vision,
2016.
Mike E Davies and Christopher J James. SoUrce separation Using single channel ica. Signal Pro-
cessing, 87(8):1819-1832, 2007.
9
Under review as a conference paper at ICLR 2019
Emily L Denton et al. Unsupervised learning of disentangled representations from video. In Ad-
Vances in Neural Information Processing Systems,pp. 4414-4423, 2017.
Aviv Gabbay, Ariel Ephrat, Tavi Halperin, and Shmuel Peleg. Seeing through noise: Visually driven
speaker separation and enhancement. arXiv preprint arXiv:1708.06767, 2017.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT Press, 2016.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. 2016.
Yedid Hoshen and Lior Wolf. Nam: Non-adversarial unsupervised domain mapping. In ECCV’18,
2018.
Patrik O Hoyer. Non-negative matrix factorization with sparseness constraints. Journal of machine
learning research, 5(Nov):1457-1469, 2004.
Po-Sen Huang, Scott Deeann Chen, Paris Smaragdis, and Mark Hasegawa-Johnson. Singing-voice
separation from monaural recordings using robust principal component analysis. In Acoustics,
Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on, pp. 57-60.
IEEE, 2012.
Po-Sen Huang, Minje Kim, Mark Hasegawa-Johnson, and Paris Smaragdis. Deep learning for
monaural speech separation. In ICASSP, 2014.
Hyunsoo Kim and Haesun Park. Sparse non-negative matrix factorizations via alternating non-
negativity-constrained least squares for microarray data analysis. Bioinformatics, 23(12):1495-
1502, 2007.
Jingu Kim and Haesun Park. Fast nonnegative matrix factorization: An active-set-like method and
comparisons. SIAM Journal on Scientific Computing, 33(6):3261-3281, 2011.
Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jungkwon Lee, and Jiwon Kim. Learning to discover
cross-domain relations with generative adversarial networks. arXiv preprint arXiv:1703.05192,
2017.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010.
Daniel D Lee andH Sebastian Seung. Algorithms for non-negative matrix factorization. In Advances
in neural information processing systems, pp. 556-562, 2001.
Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley.
Least squares generative adversarial networks. In Computer Vision (ICCV), 2017 IEEE Interna-
tional Conference on, pp. 2813-2821. IEEE, 2017.
Stylianos Ioannis Mimilakis, Konstantinos Drossos, Joao F Santos, Gerald Schuller, TUomas Virta-
nen, and Yoshua Bengio. Monaural singing voice separation with skip-filtering connections and
recurrent inference of time-frequency mask. arXiv preprint arXiv:1711.01437, 2017.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. In International Conference on Learning Representations,
2018.
Karol J. Piczak. ESC: Dataset for Environmental Sound Classification. In Proceedings of the 23rd
Annual ACM Conference on Multimedia, pp. 1015-1018. ACM Press, 2015.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
10
Under review as a conference paper at ICLR 2019
Zafar Rafii and Bryan Pardo. A simple music/voice separation method based on the extraction of the
repeating musical structure. In Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE
International Conference on,pp. 221-224. IEEE, 2011.
Zafar Rafii, Antoine Liutkus, Fabian-Robert Stoter, Stylianos Ioannis Mimilakis, and Rachel Bittner.
The MUSDB18 corpus for music separation, December 2017. URL https://doi.org/10.
5281/zenodo.1117372.
Sam T Roweis. One microphone source separation. In Advances in neural information processing
systems, pp. 793-799, 2001.
Paris Smaragdis, Bhiksha Raj, and Madhusudana Shashanka. Supervised and semi-supervised sep-
aration of sounds from single-channel mixtures. In International Conference on Independent
Component Analysis and Signal Separation, pp. 414-421. Springer, 2007.
Daniel Stoller, Sebastian Ewert, and Simon Dixon. Adversarial semi-supervised audio source sepa-
ration applied to singing voice extraction. arXiv preprint arXiv:1711.00048, 2017.
Cem Subakan and Paris Smaragdis. Generative adversarial source separation. arXiv preprint
arXiv:1710.10779, 2017.
Yuxuan Wang, Arun Narayanan, and DeLiang Wang. On training targets for supervised speech
separation. TASLP, 2014.
Kevin W Wilson, Bhiksha Raj, Paris Smaragdis, and Ajay Divakaran. Speech denoising using
nonnegative matrix factorization with priors. In Acoustics, Speech and Signal Processing, 2008.
ICASSP 2008. IEEE International Conference on, pp. 4029-4032. IEEE, 2008.
Aron Yu and Kristen Grauman. Fine-grained visual comparisons with local learning. In CVPR,
2014.
Dong Yu, Morten Kolbæk, Zheng-HUa Tan, and Jesper Jensen. Permutation invariant training of
deep models for speaker-independent multi-talker speech separation. In ICASSP, 2017.
Jun-Yan Zhu, Philipp KrahenbUhl, Eli Shechtman, and Alexei A Efros. Generative visual manipu-
lation on the natural image manifold. In European Conference on Computer Vision, pp. 597-613.
Springer, 2016.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image translation
using cycle-consistent adversarial networks. In The IEEE International Conference on Computer
Vision (ICCV), Oct 2017.
A Architectures
In this section we give details of the architectures used in our experiments:
A.1 Image Architectures:
GLOM Generator: GLOM trains generators for X and B, each generates an image given a latent
vector. The architecture followed the architecture first employed by DCGAN (Radford et al., 2015).
We used 64 filters across MNIST and Shoes-Bags experiments. MNIST had one fewer layer, owing
to its 32 × 32 resolution. Generators were followed by sigmoid layers to ensure outputs within [0, 1].
GAN Discriminator: The discriminator used by Adversarial Masking is a DCGAN discriminator
with filter dimensionality of 64. SpectralNorm is implemented exactly as described in (Miyato et al.,
2018).
Masking Network: Adversarial Masking, NES and the fully supervised baseline all use the same
masking function architecture. The masking function takes a mixed image and outputs a mask,
that when multiplied by the image results in the B source: b = y ∙ m(y). The architecture is an
autoencoder similar to the one used in DiscoGAN (Kim et al., 2017). MNIST has two fewer layers
owing to its lower resolution. We used 64 filters on the top and bottom layers, and doubling / halving
the filter number before / after the autoencoder mid-way layer.
11
Under review as a conference paper at ICLR 2019
A.2 Audio Architectures:
GLOM and AM use the same generator and discriminator architectures respectively for audio as
they do for images. They operate on mel-scale spectrogram at 64 × 64 resolution.
Masking Network: The generator for AM operates on 64 × 64 mel-scale audio spectrograms. It
consists of 3 convolutional and 3 deconvolutional layers with stride 2 and no pooling. Outputs of
convolutional layers are normalized with BatchNorm and rectified with ReLU activation, except for
the last layer where sigmoid is used.
In addition to the LSGAN loss, an additional magnitude loss is used, with relative weight of λ = 1.
NES and the supervised method operate on full linear spectrogram of dimensions 257 × 64, without
compression. They use the same DiscoGAN architecture, which contains two additional convolu-
tional and deconvolutional layers.
B	NMF Semi-Supervised Separation:
In this section we describe our implementation of the NMF semi-supervised source separation base-
line (Smaragdis et al., 2007). NMF trains a decomposition: B = WZ where W are the weights and
Z = [z1, ..., zN] are the per sample latent codes. Both W and Z are non-negative. Regularization is
important for the performance of the method. We follow (Hoyer, 2004; Kim & Park, 2007) and use
L1 regularization to ensure sparsity of the weights. The optimization problem therefore becomes:
L = ∣∣B,WZk2 + α∣W 1∣2 = ∣∣[B;0],[WZ; √ ∙ W 1]k2 s.t. W,Z ≥ 0	(12)
This equation is solved iteratively using non-negative least squares (NNLS) with the solver by Kim
& Park (2011). The Z iteration solves the following NNLS problem:
argmin∣B,WZ∣22 s.t. Z ≥ 0	(13)
Z
The W iteration optimizes the following NNLS problem:
L = ∣∣[BT, 0],[ZTWT, √α ∙ W 1T]∣2 s.t. WT ≥ 0	(14)
W and Z iterations are optimized until convergence.
Following Smaragdis et al. (2007), we first train sparse NMF for the training B samples: B =
WBZB. Using the weights from this stage, we proceed to train another NMF decomposition on the
residuals of the mixture:
L= ∣Y,WBZB + WXZX∣ s.t. WB,WX,ZB,ZX ≥ 0	(15)
The WX iteration consists of NNLS optimization:
arg min ∣(Y - WBZB)T, (ZX)T(WX)T∣ s.t. WX ≥ 0	(16)
WX
The Z iteration consists of NNLS optimization of both ZB and ZX on the mixture samples:
argmin ∣∣Y, [W B; W X ][Z B; Z X ]∣ s.t. Z B ,Z X ≥ 0	(17)
ZB,ZX
In the above we neglected the sparsity constraint for pedagogical reasons. It is implemented exactly
as in Eq. 14.
At inference time, the optimization is equivalent to Eq. 17. After inference of ZB and ZX , our
estimated B and Z sources are given by:
B = W B Z B	X = W X Z X	(18)
In our experiments we used Z dimension of 50 and sparsity α = 1.0. The hyper-parameters were
determined by performance on a small validation set.
12
Under review as a conference paper at ICLR 2019
Figure 2: A qualitative comparison of speech and noise mixtures separated by GLO and GLOM, as
well as NES after k iterations. NES(k) denotes NES after k iterations. Note that GLO and GLOM
share the same mask since GLOM is generated by the mask computed from GLO.
C	Further qualitative analysis of GLOM and NES
We present a qualitative analysis of the results of GLOM and NES. To understand the quality of
generations of GLO and the effect of the masking function, we present in Fig.2 the results of the
GLO generations given different mixtures from the Speech dataset. We also show the results after
the masking operation described in Eq. 10. It can be observed that GLO captures the general features
of the sources, but is not able to exactly capture fine detail. The masking operation in GLOM helps
it recover more fine-grained details, and results in much cleaner separations.
We also show in Fig.2 the evolution of NES as a function of iteration for the same examples. NES(k)
denotes the result of NES after k iterations. It can be seen the NES converges quite quickly, and
results improve further with increasing iterations. In Fig.3, we can observe the performance of
NES on the Speech dataset in terms of SDR as a function of iteration. The results are in line with
the qualitative examples presented before, NES converges quickly but makes further gains with
increasing iterations.
13
Under review as a conference paper at ICLR 2019
Figure 3: Speech test set separation quality as a function of NES iteration (SDR dB)
14