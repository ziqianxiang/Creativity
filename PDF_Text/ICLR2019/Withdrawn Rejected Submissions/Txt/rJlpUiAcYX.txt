Under review as a conference paper at ICLR 2019
Holographic and other Point Set Distances
for Machine Learning
Anonymous authors
Paper under double-blind review
Ab stract
We introduce an analytic distance function for moderately sized point sets of
known cardinality that is shown to have very desirable properties, both as a loss
function as well as a regularizer for machine learning applications. We compare
our novel construction to other point set distance functions and show proof of con-
cept experiments for training neural networks end-to-end on point set prediction
tasks such as object detection.
1 Introduction
Parametric machine learning models, like artificial neural networks, are routinely trained by empir-
ical risk minimization. If we aim to predict an output y ∈ Y from an input x ∈ X, we collect a
training setD of (x, y) pairs and train a parametrized prediction function hθ : X → Y by minimizing
R(θ) := |D| X L(hθ(x),y).
(x,y)∈D
(1)
Here, L: Y×Y→ R is a loss function that assigns a scalar loss value to the prediction y = hθ (x)
for the true value y. Classical machine learning problems allow for standard choices for the loss
function. E.g., for regression problems, where y, y ∈ R, it is common to use the squared loss
L(y,y) = (y - y)2.
Assume, however, that we want to train a machine learning system which—given some input—
predicts a set of points. In particular, the ordering of the points is neither semantically meaningful
nor in any way consistent across data instances. As an example, one may consider an object detection
task such as finding the positions of the black stones on an image of a real-world game of Nine Men’s
Morris. What should the loss function be in such a case?
Naively, we might just impose an arbitrary ordering to the sets and treat them as ordered tuples.
However, this conceals an important property of the task, the permutation invariance, from our
machine learning system. This property might in fact be crucial to learn such a task efficiently.
Instead, we would like our loss function to define a meaningful distance between two sets, which
requires such a loss to be permutation-invariant.
1.1	Permutation-Invariant Loss Functions
We consider distance functions a for pair of finite sets of points from Rd and assume the cardinal-
ity N to be the same for both sets and known in advance. While we are concerned with unordered
sets of points, representing them in a machine-learning context will require us to impose some or-
dering to its elements. Hence, we will represent a set {z1, . . . , zN} ⊂ Rd of points as an ordered
tuple Z = (zι,..., ZN) and define the shorthand ZN := (Rd)N for the space of such tuples.
If a loss function L(Z,Z) operating on such ordered tuples is supposed to define a meaningful
distance between the underlying sets, it will have to be invariant to their arbitrary ordering. Hence,
for Z = (z1, . . . , zN) ∈ ZN and a permutation σ ∈ SN, we define σ(Z) := (zσ(1), . . . , zσ(N)) and
demand the following property.
1
Under review as a conference paper at ICLR 2019
Definition 1 (Permutation-invariant loss). We call a function L : ZN × ZN → R permutation-
invariant ifand only iffor any ZZ, Z ∈ ZN and permutations π,σ ∈ SN, we have
, ʌ. .. , ʌ
L(π(Z),σ(Z))= L(Z,Z).	(2)
1.2	Related Work
While distance measures for sets have been studied in Mathematics and theoretical computer sci-
ence, they have rarely been investigated in the context of machine learning. A natural way to define
a point set distance is to explicitly find a “best” matching between target and prediction points and
take the distance between the re-ordered tuples, e.g.,
LHUng(Z,Z)= min X kzi- Zσ(i)k2.	⑶
σ∈SNi∈[N]
This has been proposed Under the name Hungarian loss by Stewart et al. (2016) for object detec-
tion. The name refers to the Hungarian algorithm (KUhn, 1955), which can be Used to solve the
assignment problem. To oUr knowledge, that is the first work to propose a permUtation-invariant
loss fUnction for training a machine learning model. This loss fUnction is intUitive and is a simple,
well-conditioned qUadratic fUnction of Z in regions where the optimal permUtation is constant. BUt
it also has its drawbacks. Its gradient is Undefined at “transition points” with more than one optimal
permUtation. More importantly, having to solve the assignment problem (Eq. 3) for each single eval-
Uation of the loss fUnction is problematic in a machine learning context, where we nowadays rely
on simple, highly-efficient compUtational modUles in frameworks sUch as TensorFlow (Abadi et al.,
2016). It can also be compUtationally slow, since the HUngarian algorithm is O(N3) and does not
lend itself to a highly-parallelized implementation on GPUs.
Rezatofighi et al. (2018) propose to Use a neUral network to jointly model the elements of a set as
well as the permUtation in which they appear. This approach gives rise to an alternating optimization
procedUre, where the first step solves for an optimal permUtation Using the HUngarian algorithm
and the second step Updates the weights of the neUral network. We note that their probabilistic
formUlation provides an elegant framework to learn the cardinality of the set alongside its elements.
While this is certainly important in practice, in this paper we focUs on investigating properties of
some point set distance fUnctions, assUming the cardinality to be known. FUtUre work coUld integrate
these loss fUnctions with the cardinality learning procedUre of Rezatofighi et al. (2018) to replace
the cUmbersome inner-loop solUtion of an optimal assignment problem.
Zaheer et al. (2017) discUss neUral network architectUres that take sets as inputs. They find a general
characterization of permUtation-invariant fUnctions and Use that insight to devise neUral network
architectUres which operate on sets in a permUtation-invariant fashion.
1.3	Scope of This Work
In this work, we consider some alternative permUtation-invariant point set distance fUnctions, all of
which have rUnning time O(N2) and can be implemented with operations that are readily available
in aUtomatic differentiation frameworks like TensorFlow. In particUlar, they do not rely on the
solUtion of an assignment problem akin to Eq. (3). We pay specific attention to properties relevant
to machine learning, in particUlar, how amenable sUch distances are to gradient-based nUmerical
optimization.
As one option, we propose a novel distance fUnction for point sets, which we name “holographic
loss”, since it is a metric distance on fingerprints of point sets that “holographically” encodes their
strUctUre, i.e. moving any point in the set will collectively (and analytically) change all entries in the
fingerprint-vector. We show that it has favorable properties in addition to being analytic everywhere,
sUch as having a diagonal Hessian at the minima, which fUrthermore all are global and correspond
to exactly matched Up point sets. Also, this loss fUnction has a natUral generalization to mUlti-sets.
We present proof of concept experiments on a simple object detection task based on Mnist digits,
where we train a convolUtional neUral network to directly predict the locations of digits in an image.
These experiments show that end-to-end training with a simple permUtation-invariant loss fUnction
is a viable approach for problems that can be formUlated as a point set prediction task.
2
Under review as a conference paper at ICLR 2019
(a) Hausdorff.
(b) SMD.	(c) Holographic.
Figure 1: Contour plots of different permutation-invariant point set loss function on a simple prob-
lem of matching two one-dimensional target points located at -1 and 1.
2	Hausdorff Distance and Sum of Minimal Distances-Squared
Distance functions for sets have been studied at least since the days of Hausdorff’s 1914 book on set
theory Hausdorff (1914). Considering Hausdorff’s construction in our context, i.e. as a (2nd order)
loss function for finite point sets, the one-sided HaUsdorff distance is given by LOSH(Z,Z)2 =
maXi∈[N] mi□j∈[N] Ilzi - Zj∣∣2, which is the maximum squared distance of any target point to its
closest prediction point. The symmetric Hausdorff distance is
Lh(Z, Z)2 = max (LOSH(Z, Z)2, LOSH(Z, Z)2) .	(4)
LH,2 is easily computed with O(N2 ) effort, almost everywhere differentiable, and makes intuitive
sense as a machine learning loss function. However, it has some properties that might not be desir-
able in a machine learning context. Since the Hausdorff distance is determined by a single “worst-
offender” pair of target and prediction points, each gradient descent step will only adjust the position
of a single prediction point, until there is an almost-tie between different point-pairs, and henceforth
subsequent gradient steps will jump around and keep almost-tied point pairs almost-tied. It stands
to reason that this might, especially with some optimization strategies, be undesirable compared to
loss functions whose gradient collectively moves all points.
A possible remedy to that problem is to sum the minimal distances-squared instead of tak-
ing their maximum. The one-sided sum of minimal distances-squared is LOSSMD(Z,Z)2 =
N PN=1 minj∈[N] kzi - ^jk2 and the (symmetric) sum of minimal distances-squared is
LSMD(Z, Z)2 = 1 (LOSSMD(Z, Z)2 + LOSSMD(°, Z)2) .	(5)
We do not consider the one-sided variants of these two distance functions in our comparison. The
one-sided variants centered on the prediction points will not make sense as a machine learning loss
function, since they become zero when matching all predicted points to any one of the target points,
e.g., Zj = zι for all j. The one-sided variants centered on the target points, while more sensible,
have simple failure cases as well, such as the one depicted in Figure 4. We thus restrict our attention
to the symmetric loss functions LH and LSMD. Figures 1a and 1b show contour plots for these
two functions on a simple problem with two one-dimensional target points, while 1c shows our
construction, described in the next section.
3	“Holographic” Loss
3.1	Set Fingerprints
A general strategy to devise permutation-invariant loss functions is to (differentiably) map tuples Z
representing a point set (or multi-set) to a “fingerprint” vector f (Z) in some fingerprint-space F
3
Under review as a conference paper at ICLR 2019
such that f(Z) = f(Z) if and only if Z = σ(Z) for some permutation σ. A permutation-invariant
loss function can then be defined via (some) metric distance in F. Any such loss function will inherit
properties of the distance in F, such as positive definiteness, symmetry, or the triangle inequality.
In one dimension, a suggestive choice for a fingerprint function on tuples of N points would be a
vector of moments Mk(Z) := PiN=1 zik. The mathematical appendix of Zaheer et al. (2017) iden-
tifies the moments of points in a set as an universal permutation-invariant fingerprint, in the sense
that every other such fingerprint can be defined in terms of these quantities. This suggest using
squared-euclidean-distance-of-moments-vectors as a loss function. However, this has some funda-
mental problems, such as poor conditioning, and it is non-trivial to generalize to higher-dimensional
points. See Appendix B for a detailed discussion.
3.2	EXPLOITING THE 2-D {POINT SET} ⇔ {POLYNOMIAL} CORRESPONDENCE
In this section, we want to focus our attention to two-dimensional point sets (or multi-sets), and
consider one-dimensional sets as a special case with all points having a 2nd coordinate of zero. We
will subsequently consider generalizations to higher dimensions.
We start by identifying a two-dimensional point z = (x, y) ∈ R2 with the complex number z =
x + iy ∈ C. With this, we can map any tuple Z = (z1, . . . , zN) that represents a (multi-)set to a
complex polynomial
PZ (U) = (U - Z1 )(u - Z2)…(U - ZN).	(6)
Note that PZ = PZ if and only if Z = σ(Z) for some permutation σ ∈ SN. Furthermore, such
a monic (i.e. having leading coefficient 1) polynomial of degree N is uniquely determined by its
values at N distinct points. This suggests to choose a set U = (U1, . . . , UN) ofN distinct evaluation
points and to define a fingerprint as
f(Z) = (PZ(U1), . . . , PZ (UN)).	(7)
Taking the real-valued L2 distance in CN gives us the “holographic” loss function
LUol(Z,Z)2
1N	1N
N E IPZ (Ui)- PZ (Ui)F = N E (PZ (Ui)- PZ (Ui)) ∙ (pZ (Ui)- PZ (Ui)).
N i=1	N i=1
(8)
While the underlying fingerprint is holomorphic, i.e. complex differentiable, the distance-squared
function is not, due to anti-linearity of the underlying complex scalar product in the first argument.
The adjective ‘holographic’ here refers to this function depending collectively on the locations of all
points, unlike Hausdorff distance, which for a generic configuration will not change when slightly
changing the positions of any points other than the specific pair that determines its value. Also,
for U 6= Z, correctly matching up the candidate and target points means matching up both the
complex amplitude as well as the complex phase at the evaluation-points. However, the analogy
with holography ends here, there is no deeper correspondence beyond that.
Proposition 1. The Holographic loss trivially has the following properties:
•	Positive definiteness: LIHfO(Z, Z)2 ≥ 0 and LIHfo(Z, Z)2 = 0 ifand only if Z = σ(Z).
•	Symmetry: LU(Z, Z)2 = LU(Z, Z)2
•	Triangle inequality: JL*l(Z, Z)2 ≤ JL*l(Z, Z0)2 + JL*l(Z0, Z)2.
•	L Uol (Z, Z )2 is quadratic in each individual Zi s real and also imaginary part when keeping
all other point-coordinates fixed.
3.3	Evaluating at the Target Points
If the target points form a set rather than a multi-set (i.e. there are no duplicates), we have the option
to choose the set U of evaluation points as identical to the target points Z. Then, since PZ(zi) = 0
4
Under review as a conference paper at ICLR 2019
by construction, we get the simpler loss function
2
N	NN	N N
LHol(Z,Z)2 = N X |Pz(Zi)I2 = N X Y(Zi- Zj) = N XY ∣Zi - ^j∣2.	(9)
i=1	i=1 j=1	i=1 j=1
In the two-dimensional case, note that the squared absolute value of the complex number Zi - Zj is
identical to the squared Euclidean distance ∣∣Zi - Zj ∣∣2 of the two points in R2:
NN
LHol(Z,Z)2 = NN XYkZi- Zjk2.	(10)
N i=1 j=1
This target-centered version will be used in the practical applications presented here. For each
target point, we take the product of squared distances to all prediction points and then sum this
quantity over all target points. If a target point Zi is matched closely by any of the prediction points,
the product of squared distances will tend to be small and, consequently, Zi will not contribute
significantly to the loss. Note that, unlike Eq. (8), this loss function will no longer be symmetric.
Other than that, it inherits the properties of Proposition 1.
3.4	Higher-Dimensional Points
There is no higher-dimensional equivalent of the polynomial construction in Eq. (6), since there is no
known generalization of the Fundamental theorem of Algebra to higher-dimensional real algebras
beyond D = 2. However, we can heuristically extend Eq. (10), which is a simple sum of products
of squared Euclidean distances, to point sets of arbitrary dimension. As we show in the next section,
for D = 2, the correspondence between point sets and polynomials can be used to prove some
favorable properties of the loss function. We do not yet have a proof for these properties for D 6= 2.
3.5	Gradients and S tationary Points
The gradient of the Holographic loss with an arbitrary set of evaluation points (Eq. 8) can be com-
puted using complex backpropagation of errors, see Appendix A. Here, we restrict our attention to
the gradient ofEq.(10) with respect to Xk and yk, which evaluates to
(∂LHol(Z,Z)2 ∂LHol(ZZ)2∖ _	2 X ( Y	2、 ʌ
∂x-∂Xk-, -∂^	= = -N 入	l=⅛ JZi-	Zjk	)	(Xi- Xk,	yi	- yk).	(II)
This gradient has a rather intuitive interpretation (also for D > 2). The negative gradient is a
weighted sum of the vectors (Zi - Zk), which pulls the prediction Zk in the direction of target point
Zi. The weight of this attractive force in the direction of Zi is α Qj=k ∣∣Zi - Zjk2, which measures
how well point Zi is already matched by any prediction point other than Zk. Unlike the Hausdorff
distance, the gradient of the Holographic loss thus adjusts the position of all points simultaneously.
We can get some intuition for the dynamics induced by this gradient from example solutions of the
gradient flow ODE, see Appendix C.
Stationary Points Obviously, any permutation-invariant loss function will be non-convex by the
mere fact that it has multiple global minima. Beyond that, the Holographic loss will have additional
stationary points. For example, for Z = ((0, 0), (1, 1)), the gradient vanishes at the non-optimal
configuration Z = ((0.5,0.5), (0.5,0.5)). The following Proposition shows that, for D = 2, these
non-optimal stationary points can not be local minima. The proof (see Appendix A) relies on com-
plex polynomials and, thus, only holds for two-dimensional point sets. While we conjecture that this
property holds for the Holographic loss in Eq. (10) for points of any dimension, we currently do not
have a proof for this.
Proposition 2. Let D = 2 and point sets Z, U be given, possibly U = Z. Then Z* is a local
minimum ofthefunction Z → LIU)I(Z,Z)2 ifand only if Z* = σ(Z) for some permutation σ ∈ SN.
In this case we have LHUol(Z*, Z)2 = 0 and, thus, all minima are global.
5
Under review as a conference paper at ICLR 2019
Table 1: Properties of point set loss functions.
Loss Function	Complex.	Analytic	GPU-Friendly	Non-Sparse Grad.	Multi-Set Compatible
Hungarian	O(N3)	No	No	Yes	Yes
Hausdorff	O(N2)	No	Yes	No	No
SMD	O(N2)	No	Yes	Yes	No
Holographic	O(N2)	Yes	Yes	Yes	in D = 2
3.6	Behavior Near Minima
The following proposition characterizes the behavior of the Holographic loss near optima up to
second order. The proof can be found in Appendix A.
Proposition 3. Let Z be fixed and define LZ (Z)2 = LHol(Z, Z)2, where Z is treated as a RD∙N
vector. At any global optimum where Z = σ(Z) with σ ∈ SN, the Hessian matrix ∂j∂kLZ(Z)2 ∈
RD∙N×D∙N is diagonal and its elements are
2
N ɪɪ	Ilzi - zj Il , i ∈ [N],	(12)
j∈[Ν ]∖{i}
each appearing D times.
The nature of the diagonal elements, which are also the eigenvalues of the Hessian, suggests that the
problem can become ill-conditioned if the pairwise distances between points have vastly different
scales, e.g., if there is a pair of points that is very close to each other relative to all the other points.
One might even be worried whether minimizing the Holographic loss to numerical accuracy cor-
rectly matches up point sets in such pathological situations. In Appendix B, we show that this not
a problem even for sets with about 40 randomly sampled points. This behavior is to a large extent
due to the property of the Hessian always being diagonal at minima. Since the eigendirections of the
Hessian coincide with the coordinate directions, floating point numerics can represent the gradients
in these eigendirections with high relative precision despite their vastly different scales.
This concludes our description of the Holographic loss. Table 1 gives an overview of different
properties of the loss functions under consideration.
4	Experiments
We apply the three point set loss functions—Hausdorff, sum of minimal distances-squared, and
Holographic loss—to a simple object detection toy tasks. The purpose of these experiments is to
demonstrate the viablity of end-to-end training with simple permutation-invariant loss functions
without explicitly modelling permutations or having to solve optimal assignment problems.
Object detection is one of the most prominent tasks in the field of computer vision. Given an input
image, the task is to predict the locations (e.g., center points or bounding boxes) of objects shown
in this image. Since there usually will not be any meaningful or consistent ordering of objects in an
image, this is inherently a point set prediction task. However, existing approaches to object detection
avoid treating it as such. Instead, state-of-the-art object detectors are carefully engineered systems
combining multiple components. R-CNN (Girshick et al., 2014; Girshick, 2015; Ren et al., 2015)
generates proposal regions hypothesizing object locations and subsequently assigns a score to each
region independently, indicating how likely it is to actually contain an object. YOLO (Redmon
et al., 2016) divides an image into a grid and asks each grid cell to predict a pre-specified number
of likely object bounding boxes, together with confidence scores. For both R-CNN and YOLO,
generating a final output set involves post processing steps like non-maximum suppression. None
of these systems learn the map from an image input to the object locations end-to-end. We want to
demonstrate that this is possible to do with any of the three point set loss functions discussed in this
paper.
Dataset For this proof of concept experiment, we create a simple object detection data set, MNIST-
Detect, with a fixed number of objects (Mnist digits) in each image. To generate an MnistDe-
tect image, we sample four Mnist digits, crop them to a tight bounding box and rescale them
6
Under review as a conference paper at ICLR 2019
Table 2: MNISTDetect test set results. Each line corresponds to the same neural network trained
with a specific loss function.
Centers (Hung. loss) Bounding boxes (detection rate)
Trained on Hausdorff	7.71	76.03
Trained on SMD	14.67	80.64
Trained on Holographic	7.89	79.20
by a factor chosen uniformly at random from {0.5, 0.6, 0.7, 0.8, 0.9}. We then place them in ran-
dom locations of a 50 × 50 pixels image, allowing overlap of the bounding boxes, but not of the
actual digits. Finally, we add noise to the image. Figure 2 shows some examples. Each example
is annotated with the center location as well as the bounding box of each digit with no particular
ordering.
Neural Architecture and Training We use a very simple convolutional neural network that takes
such an MNISTDETECT image as input and applies three convolutional layers (with 64 filters of3× 3
pixels each) and two fully-connected layers (500 and 200 neurons, respectively), all with ReLU
activation. The output layer is of size 4D with no non-linearity and is reshaped to a 4 × D array,
where each row is supposed to predict the location of one of the four digits in the image. We
performed separate experiments for predicting center locations (D = 2) and bounding boxes (D =
4). The network is trained end-to-end using Hausdorff, SMD, and Holographic loss. For each loss
function, we train the network for a fixed number of epochs using the Adam optimizer (Kingma &
Ba, 2014) and a fixed mini-batch size of 128. We evaluate the loss on a validation set after each
epoch of training and retain the weights that achieve minimal validation loss. The step size is tuned
for each loss function independently via a grid search, but turned out to be the same (0.001) for all
loss functions.1
Evaluation For each loss function, we evaluate the network that achieved minimal validation loss
on a held-out test set. As “impartial” comparison metrics, we report the Hungarian loss for center
point prediction and detection rate for bounding box prediction. In the latter case, we count a digit
as successfully detected if its bounding box has an intersection over union (IoU) larger than 0.5 with
any of the predicted bounding boxes.
Table 2 shows quantitative results and Figure 2 depicts some qualitative results for bounding box
prediction. With all three loss functions, the neural network manages to learn the task reasonably
well even though it is a simplistic CNN architecture that has not been tuned to the task at all.
5 Conclusion
We discussed a novel point set loss function, which is analytic everywhere, vis-a-vis two other sim-
ple alternatives with matching computational complexity for point set prediction tasks as alternatives
to more involved approaches (Stewart et al., 2016; Rezatofighi et al., 2018). Proof of concept experi-
ments showed that end-to-end training with such simple loss functions is a viable approach for point
set prediction tasks, such as object detection. We expect that simple constructions such as the “holo-
graphic” point set distance introduced here may turn out useful not only for point set predictions, but
also as a regularizer, for example to encourage (unsupervised) clustering to align its clusters with
the clusters found by an earlier version of the model.
References
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, MatthieU
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: a system for large-
scale machine learning. In OSDI, volume 16, pp. 265-283, 2016.
1This stands to reason, since we used the same architecture with all loss functions and the Adam optimizer
is invariant to a rescaling of its objective function.
7
Under review as a conference paper at ICLR 2019
(a) Hausdorff.
(b) SMD.
(c) Holographic.
Figure 2: Qualitative test set results for bounding box prediction on the MnistDetect dataset
with a convolutional neural network, trained end-to-end with different loss functions. Groundtruth
bounding boxes in green, predictions in red.
Vladimir Arnold. On functions of three variables. In Proceedings of the USSR Academy of Sciences,
volume 114,pp. 679-681,1957.
Branko Curgus and Vania Mascioni. Roots and polynomials as homeomorphic spaces. Expositiones
Mathematicae, 24(1):81-95, 2006.
Ross Girshick. Fast R-CNN. In Proceedings of the IEEE International Conference on Computer
Vision, pp. 1440-1448, 2015.
Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for ac-
curate object detection and semantic segmentation. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 580-587, 2014.
Felix Hausdorff. GrundzugederMengenIehre. 1914.
Raphael Hunger. An introduction to complex differentials and complex differentiability. Munich
University of Technology, Inst. for Circuit Theory and Signal Processing, 2007.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Andrey Kolmogorov. On the representations of continuous functions of several variables by super-
positions of continuous functions of fewer variables. In Proceedings of the USSR Academy of
Sciences, volume 108, pp. 179-182, 1956.
Harold W Kuhn. The Hungarian method for the assignment problem. Naval research logistics
quarterly, 2(1-2):83-97, 1955.
Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified,
real-time object detection. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 779-788, 2016.
8
Under review as a conference paper at ICLR 2019
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-time object
detection with region proposal networks. In Advances in Neural Information Processing Systems,
pp. 91-99, 2015.
S Hamid Rezatofighi, Roman Kaskman, Farbod T Motlagh, Qinfeng Shi, Daniel Cremers, Laura
Leal-Taixe, and Ian Reid. Deep perm-set net: Learn to predict sets with unknown permutation
and cardinality using deep neural networks. arXiv preprint arXiv:1805.00613, 2018.
Russell Stewart, Mykhaylo Andriluka, and Andrew Y Ng. End-to-end people detection in crowded
scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
2325-2333, 2016.
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan R Salakhutdinov,
and Alexander J Smola. Deep sets. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30,
pp. 3391-3401. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/
6931-deep-sets.pdf.
9
Under review as a conference paper at ICLR 2019
A Mathematical Details
A.1 Proof of Proposition 2
We will need the following Lemma, which states that the roots of a complex polynomial continu-
ously depend on its coefficients.
Lemma 1. Let F(u) = uN + PkN=-01 ckuk be a monic complex polynomial and factor it as F(u) =
(U 一 aι)(u — a2)…(U 一 a，N) with ak ∈ C some ordering ofthe roots. Then,for every ε > 0 there
exists δ > 0 such that every polynomial G(u) = uN + PkN=-01 dkuk with |dk - ck| < δ can be
written as G(U) = (U — bι)(u — b2)…(U — bN) where |bk — ak | < ε.
Proof. This is a reformulation of the well-known continuity result, a proof of which can be found,
for example, in Curgus & Mascioni (2006). One notes that, to first order in a small shift E in the
coefficients ofa polynomial, the shift of the zeros can be found by a single step of Newton-Raphson
iteration, except at higher-degree zeros (where the derivative of the polynomial in the denominator
also has a zero).	□
We can now prove the Proposition.
Proofor Proposition 2. We abbreviate L(Z, Z)2 := LUol(Z, Z)2 = N PN=I |Pz(Ui) — PZ(u^)∣2.
We already observed that L(Z, Z)2 = 0 if and only if Z = σ(Z) for some σ ∈ SN. Now assume
L(Z, Z)2 6= 0. To see that Z can not be a local minimum, we need to show that for any ε > 0 there
is Z0 with d(Z, Z0) < ε such that L(Z0, Z) < L(Z, Z), where d(Z, Z0) := PN=I |Zi — ^0∣2. To
construct such a Z0, we define the polynomial Qλ : C → C,
Qλ(U) ：= PZ(U) + λ(Pz(u) — PZ(u)),	λ ∈ [0,1],	(13)
which linearly interpolates between PZ and PZ. Note that Qλ is monic for λ ∈ [0,1] and Qo = PZ.
Let Zλ0 be some ordering of the roots of Qλ . Then
1N	1N
L(Zλ,Z) = N E lQλ(Ui)- PZ(Ui)I = N EIPZ(Ui) + λ(PZ(Ui) — pz(Ui)) - PZ(Ui)I
1N
=(1 — λ)2N E ∣PZ(Ui) — Pz(Ui)I2 = (1 — λ2)L(Z, Z).
N i=1
(14)
Since we assumed L(Z, Z) > 0, this means that L(Zλ0 , Z) < L(Z, Z) for any λ > 0. It remains to
show that for any ε > 0 there is λε > 0 and a permutation σ such that d(Z, σ(Zλ0 )) < ε. However,
since the coefficients of Qλ continuously depend on λ, this follows as a simple consequence of
Lemma 1.	□
A.2 Proof of Proposition 3
Proofof Proposition 3. Denote by zk,ι := [Zk]ι the l-th coordinate of the k-th prediction point (and
likewise for target points). We have previously found the first derivatives of the loss w.r.t. the ele-
ments of Z to be
dLZ(Z)2 _	2 X	/YNrll	ʌ ∣∣2^	( ʌ 、	门 G
~~∂Z =	一 N	H kzi 一	Zjk	(Zi,l	一 zk,l).	(15)
k,l	i=1 j=1,j6=k
10
Under review as a conference paper at ICLR 2019
We can easily calculate the second derivatives to be
∂ 2Lz (C)2
dz2,ι
∂2Lz (Z)2
2N	N
NXi	Y	Hz，-^jk2),
i=1	j=1,j 6=k
∂Zk,m∂Zk,ι
∂2Lz (C)2
∂Zko,m∂Zk,ι
0,
m 6= l,
(16)
N X I Y	kzi - Zj k2 I (zi,m - zk0,m)(zi,l - zk,l),
i=1 ∖j∈{k,k0}	)
k0 6= k.
At any optimum, the latter becomes zero, since for each zi there is exactly one j ∈ [N] such
that Zj = z，. This makes the Hessian matrix diagonal. Furthermore, the diagonal elements simplify
at an optimum: Letting σ be the specific permutation of this optimum, such that Zj = Zσ(j), We get
∂2Lz (C)2
dzk,ι
2N N
N Xl Y	kZi-Zσ(j)k2
，=1	j=1,j 6=k
(17)
The product becomes zero for all but one i: the one Where σ(j) 6= i for all j ∈ [N]\{k}, Which is
exactly σ-1(k). The diagonal element becomes
A ∂Z 产 =N Y kzσ-1(k) - zσ(j)k2 = N Y	kzσ-1(k) - Zjk 2 ∙	(18)
∂zk,l	N j6=k	N j6=σ-1(k)
This is independent of l and thus appears D times for l = 1, . . . , D and the set of diagonal elements
is the same for each optimum (different optima correspond to different permutations σ, Which only
changes the ordering).	□
A.3 Gradients of the Holographic Loss
In D = 2, the Holographic loss (Eq. 8) is polynomial in the 2N real coordinates of N points, so
a gradient can be obtained via backpropagation in the usual Way. When using a set of evaluation-
points U that does not coincide With the target points C, there is some additional structure available
due to the loss being based on a complex polynomial (hence complex-differentiable) that can be
exploited to structurally simplify the computation. To apply Wirtinger calculus (a readable intro-
duction can be found in Hunger (2007)), We reWrite our loss function (considering C and U fixed)
NN
L(Z) ：= LUol(Z, C)2 = X |Pz(Ui) - PZ(Ui)∣2 = X
，=1	，=1
NN
ɪɪ(ui - Zj) - ɪɪ(ui - Zj)
j=1	j=1
NN	N
X	Y(Ui-Zj) - Y(Ui- Zj)
i=1	j=1	j=1
as L(Z)2 = L(Z, Z*)2,where
NN	N
L(V, W)2 ：=X	Y(Ui-Zj)-Y(Ui-vj)
i=1 j=1	j=1
NN
Y(U-；)- Y (编
j=1	j=1
(19)
NN
Y (u： - Zj) - Y(u： - Wj)
j=1	j=1
(20)
Then, L2 is a complex-differentiable function on CN X CN, and the usual reasoning for sensitivity
backpropagation also applies in this complex case.
Informally, denote the sensitivites of L2 w.r.t. Vj and Wj by σvj and σwj. It turns out that Ow§ = σνj.
Now, we ultimately want to know the sensitivities on the (X, y)-coordinates of the candidate points,
that is, the real and imaginary parts of the Zj. Thanks to complex differentiability, we know that
1	♦	1 1	T T 7^ t' t ∙ -I -I IrF	t	1 ∙ -I ∕'
changing Vj → Vj + EC and keeping W fixed will change L by EC ∙ Ov§ 一 and correspondingly for
2
11
Under review as a conference paper at ICLR 2019
wj. Now, ifwe simultaneously change (vj, wj) → (vj+R, wj+R) with R real, which corresponds
to changing (Xj, y) → (Xj + er, yj), L Will change by ER ∙ σvj + ER ∙ σwj = ER ∙ σvj + ER ∙ σvj =
2eRRe σvj .
Likewise, changing (Xj, yj) → (Xj, yj- + ER) with ER real changes (vj∙, Wj) → (Vj + iER, Wj 一 5er),
and hence L Will change by zer ∙ Ov§ — zer ∙ σwj = zer ∙ Ov§ — zer ∙ σVj = -2ERIm σvj. This, then,
tells us how to link up the real sensitivities (gradient components) with the complex sensitivities:
Qxj = 2Re σvj,	σyj∙ = - 2Im σvj∙.	(21)
B Difference of Moments
B.1	Moments as Set Fingerprints
Restricting our attention to one-dimensional points, an obvious choice for a permutation-invariant
fingerprint would be moments of point sets. With N = |Z|, the k-th moment of Z is Mk(Z) :=
PiN=1 zik. Knowing the values of these elementary-symmetric functions for k = 1, . . . , N com-
pletely determines the point set, making mZ = (M1 (Z), . . . , MN (z))T ∈ RN a valid set fin-
gerprint. Indeed, as explained in Appendix A of Zaheer et al. (2017), this is in some sense the
fundamental permutation-invariant set fingerprint, since every symmetric function on sets of N real
numbers can be written in the form ρ(mZ) with some suitable function ρ. As explained there, this
nicely parallels the Kolmogorov-Arnold theorem (Kolmogorov, 1956; Arnold, 1957) for the sym-
metric case.
However, this insight is only of limited use for constructing a practical permutation-invariant loss
function, since numerical conditioning aspects have to be taken into account. Specifically, simply
taking the L2 -distance between these moments fingerprints
NN	N
LMom(Z, z)2 = N X X Zk - X Zk
k=1 i=1	i=1
2
(22)
turns out to have fundamental problems matching up even moderately-sized real point sets, see
Figure 3 for a 2 — d example. Beyond 〜7 points, there are ways to collectively shift points such
that the impact of the distortion on low-order moments is compensated, while the impact on high-
order moments falls below the numerical accuracy threshold. Typically, the end result of a failed
attempt to match up N real points using LMom,2 will have points near zero being considerably off
the target locations.
In the same experiment, the Holographic loss reliably matches up point sets with more than 40
randomly sampled points. If we cast Z → LUol(Z, Z)2 into the ρ(mz) language it takes on the
form of a polynomial in all the point coordinates (in two dimensions: Zj = (Xj, yj∙)) of the general
form
P(Z) = X CiXξi,1 X2i,2 …XNN …yηi,1 yηi,2 …yN,N	(23)
i
with all exponents ξi,k, ηi,k being either 0, 1, or 2. This largely avoids numerical problems due to
the locations of zeroes of higher-degree polynomials being ill-conditioned w.r.t. coefficients. So,
the “holographic” loss function can be considered as “maximally simple” in the sense of being a
quadratic function when considering each input-coordinate separately. For large point sets, it may
be useful to generalize the “holographic” loss function by also allowing weights for the contributions
coming from different evaluation-points, in order to minimize the discrepancies between diagonal
entries of the Hessian.
B.2	Moments-Loss in Higher Dimensions
It is not obvious how one would extend the moments-based loss to higher dimensions. Using just
the N ∙ D moments for every coordinate, PN=I Zk j, will not give US a proper set fingerprint, since
it does not discriminate between point sets obtained by shuffling the values of any one coordinate
12
Under review as a conference paper at ICLR 2019
between points. That is, it could not discriminate {(x0, y0); (x1, y1)} from {(x0, y1); (x1, y0)}. A
more appropriate choice are the moments
N
M(ki ,k2,…,kD)(Z) = XX zi,ι zi,2 …ZiD	(24)
i=1
for Pj kj ≤ N . However, this scales infavorably with N and one would naturally expect the
same numerical problems we have seen in one dimension to persist in higher dimensions. This
is confirmed in the investigations described next, where we used as a fingerprint the vector of all
moments up to total scaling dimension N in D = 2, i.e. xayb with a + b ≤ N .
B.3	Matching up Point Sets with Moments-Loss and Holographic Loss
In Figure 3 we show experiments matching up sets of randomly sampled points by minimizing
LMom,2 and LHol,2 to numerical accuracy.
We see that, using 64-bit floating point numerics, the Holographic loss can in principle be used to
match up sets with more than 40 randomly sampled points.
Figure 3: Performance of the ‘holographic’ loss function measured in terms of residual Hausdorff-
distance between candidate- and target-set after minimizing log(10-100 + LHUo=l,Z2 (C, U)) with
SciPy's scipy.optimize.fmin 上fgs to gtol 10-8 for 1000 examples (per number of points)
of point-configurations C, Z where each point is drawn uniformly from the 2-d unit disc. Solid lines:
percentiles for minimizing as described. Dashed lines: percentiles when artificially limiting numeri-
cal accuracy of the potential and gradient by adding and then substracting again 1.0. This effectively
turns 64-bit floating point arithmetics into 53-bit fixed-point arithmetics by fixing the floating ex-
ponent. Under these conditions, the loss function behaves as is expected from condition number
analysis, being useful to about N = 15 points. When allowing the floating point exponent to adapt,
this loss function can in principle match up point sets well beyond N = 40 points when using 64-bit
float arithmetics.
C	Miscellaneous
C.1 Failure Case of One-Sided Hausdorff/SMD
Figure 4 depicts a simple failure case of the one-sided versions of the Hausdorff distance and the
sum of minimal distances.
13
Under review as a conference paper at ICLR 2019
×
1 - ∙
×
O -∙	∙
-1 -
• targets
× predictions
0	12
Figure 4: A configuration that leads to a local minimum of the target-centered onesided versions
of the Hausdorff distance and the sum of minimal distances. All three target points have the same
closest prediction point, which is located exactly in their “center of mass”.
C.2 Example Solutions of the Gradient Flow ODE
To gain some intuition for the dynamics induced by the Holographic loss function, we consider some
example solutions of the gradient flow ODE
Z(t = 0) ：= Zstart,
d	∂LHol(Z,Z)2
dtzk ⑴=—∂^k—
(25)
Figure 5 shows multiple instances of matching up sets of five two-dimensional points. Lines show
how the prediction points move according to the ODE (25). Along each line, markers are placed
indicating exponentially spaced time-steps tn = 2n ∙ ∆t with every fifth marker being drawn in
black. (These do not correspond to iterations of gradient descent, but to the exact solution of the
ODE. In the limit of infinitely small step size, the gradient descent dynamics would approach these
curves.)
14
Under review as a conference paper at ICLR 2019
Figure 5: Gradient flows illustrating the behavior of the Holographic loss function. Left column: If
there are reasonably clear ways to match up the two point sets, the gradient flow tends to find that
identification and moves each point simultaneously in a meaningful way. This allows each entry of
the gradient to forward meaningful information towards earlier layers in ML training, unlike with
Hausdorff-distance. Right column: In more complex cases (such as two points almost coinciding),
multiple points may travel alongside one another until the degeneracy finally gets broken.
15