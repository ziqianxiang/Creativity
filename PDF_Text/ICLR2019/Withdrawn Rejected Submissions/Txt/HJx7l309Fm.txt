Under review as a conference paper at ICLR 2019
Actor-Attention-Critic for Multi-Agent Re-
inforcement Learning
Anonymous authors
Paper under double-blind review
Ab stract
Reinforcement learning in multi-agent scenarios is important for real-world ap-
plications but presents challenges beyond those seen in single-agent settings. We
present an actor-critic algorithm that trains decentralized policies in multi-agent
settings, using centrally computed critics that share an attention mechanism which
selects relevant information for each agent at every timestep. This attention mech-
anism enables more effective and scalable learning in complex multi-agent envi-
ronments, when compared to recent approaches. Our approach is applicable not
only to cooperative settings with shared rewards, but also individualized reward
settings, including adversarial settings, and it makes no assumptions about the
action spaces of the agents. As such, it is flexible enough to be applied to most
multi-agent learning problems.
1	Introduction
Reinforcement learning has recently made exciting progress in many domains, including Atari
games (Mnih et al., 2015), the ancient Chinese board game, Go (Silver et al., 2016), and complex
continuous control tasks involving locomotion (Lillicrap et al., 2016; Schulman et al., 2015; 2017;
Heess et al., 2017). While most reinforcement learning paradigms focus on single agents acting in
a static environment (or against themselves in the case of Go), real-world agents often compete or
cooperate with other agents in a dynamically shifting environment. In order to learn effectively in
multi-agent environments, agents must not only learn the dynamics of their environment, but also
those of the other learning agents present.
To this end, several approaches for multi-agent reinforcement learning have been developed. The
simplest approach is to train each agent independently to maximize their individual reward, while
treating other agents as part of the environment. However, this approach violates the basic assump-
tion underlying reinforcement learning, that the environment should be stationary and Markovian.
Any single agent’s environment is dynamic and nonstationary due to other agents’ changing policies.
As such, standard algorithms developed for stationary Markov decision processes fail.
At the other end of the spectrum, all agents can be collectively modeled as a single-agent whose
action space is the joint action space of all agents (BUSoniU et al., 2010). While allowing Coordi-
nated behaviors across agents, this approach is not scalable due to the action space size increasing
exponentially with the nUmber of agents. It also demands a high degree of commUnication dUring
execUtion, as the central policy mUst collect observations from and distribUte actions to the individ-
Ual agents. In real-world settings, this demand can be problematic.
Recent work (Lowe et al., 2017; Foerster et al., 2018) attempts to combine the strengths of these
two approaches. In particUlar, a critic (or a nUmber of critics) is centrally learned with information
from all agents. The actors, however, receive information only from their corresponding agents.
ThUs, dUring testing, execUting the policies does not reqUire the knowledge of other agents’ actions.
This paradigm circUmvents the challenge of non-Markovian and non-stationary environments dUr-
ing learning. Despite those progresses, however, algorithms for mUlti-agent reinforcement learning
are still far from being scalable (to a larger nUmber of agents) and being generically applicable to
environments and tasks that are co-operative (sharing a global reward), competitive, or mixed.
OUr approach extends these prior works in several directions. The main idea is to centrally learn a
critic with an attention mechanism. The intUition behind oUr idea is that in many real-world environ-
1
Under review as a conference paper at ICLR 2019
ments, it is beneficial for agents to know what other agents it should pay attention to. For example,
a soccer defender needs to pay attention to attackers in their vicinity as well as the player with the
ball, while she/he rarely needs to pay attention to the opposing team’s goalie. The specific attackers
that the defender is paying attention to can change at different parts of the game, depending on the
formation and strategy of the opponent. A typical centralized approach to multi-agent reinforce-
ment learning does not take these dynamics into account, instead simply considering all agents at
all timepoints. Our attention mechanism is able to dynamically select which agents to attend to at
each time point, improving performance in multi-agent domains with complex interactions.
The proposed approach has an input space linearly increasing with respect to the number of agents,
as opposed to the quadratic increase in a previous approach Lowe et al. (2017). It also works well
in co-operative, competitive, and mixed environments, exceeding the capability of some prior work
that focuses only on co-operative environments Foerster et al. (2018).
We have validated our approach on two simulated environments and tasks. We plan to release the
code for both the model and the environments after the reviewing period ends.
The rest of the paper is organized as follows. In section 2, we discuss related work, followed by a
detailed description of our approach in section 3. We report experimental studies in section 4 and
conclude in section 5.
2	Related Work
MUlti-Agent Reinforcement Learning (MARL) is a long studied problem (BUSoniU et al., 2010).
Topics within MARL are diverse, ranging from learning communication between cooperative
agents (Tan, 1993; Fischer et al., 2004) to algorithms for optimal play in competitive set-
tings (Littman, 1994), thoUgh, Until recently, they have been focUsed on simple gridworld envi-
ronments with tabUlar learning methods.
As deep learning based approaches to reinforcement learning have grown more popUlar, they have,
natUrally, been applied to the MARL setting (TampUU et al., 2017; GUpta et al., 2017), allowing
mUlti-agent learning in high-dimensional/continUoUs state spaces; however, naive applications of
Deep RL methods to MARL natUrally encoUnter some limitations, sUch as nonstationarity of the
environment from the perspective of individUal agents (Foerster et al., 2017; Lowe et al., 2017;
Foerster et al., 2018), lack of coordination/commUnication in cooperative settings (SUkhbaatar et al.,
2016; Mordatch & Abbeel, 2018; Lowe et al., 2017; Foerster et al., 2016), credit assignment in
cooperative settings with global rewards (Rashid et al., 2018; SUnehag et al., 2018; Foerster et al.,
2018), and the failUre to take opponent strategies into accoUnt when learning agent policies (He
et al., 2016).
Most relevant to this work are recent, non-attention approaches that propose an actor-critic frame-
work consisting of centralized training with decentralized execUtion (Lowe et al., 2017; Foerster
et al., 2018), as well as some approaches that Utilize attention in a fUlly centralized mUlti-agent
setting (Choi et al., 2017; Jiang & LU, 2018). Lowe et al. (2017) investigate the challenges of mUlti-
agent learning in mixed reward environments (BUsSoniU et al., 2010). They propose an actor-critic
method that Uses separate centralized critics for each agent which take in all other agents’ actions
and observations as inpUt, while training policies that are conditioned only on local information.
This practice redUces the non-stationarity of mUlti-agent environments, as considering the actions
of other agents to be part of the environment makes the state transition dynamics stable from the
perspective of one agent. In practice, these ideas greatly stabilize learning, dUe to redUced variance
in the valUe fUnction estimates.
Similarly Foerster et al. (2018) introdUce a centralized critic for cooperative settings with shared re-
wards. Their method incorporates a ”coUnterfactUal baseline” for calcUlating the advantage fUnction
which is able to marginalize a single agent’s actions while keeping others fixed. This method allows
for complex mUlti-agent credit assignment, as the advantage fUnction only encoUrages actions that
directly inflUence an agent’s rewards.
Attention models have recently emerged as a sUccessfUl approach to intelligently selecting contex-
tUal information, with applications in compUter vision (Ba et al., 2015; Mnih et al., 2014), natUral
2
Under review as a conference paper at ICLR 2019
language processing(Vaswani et al., 2017; Bahdanau et al., 2015; Lin et al., 2017), and reinforcement
learning (Oh et al., 2016).
In a similar vein, Jiang & Lu (2018) proposed an attention-based actor-critic algorithm for MARL.
This work follows the alternative paradigm of centralizing policies while keeping the critics decen-
tralized. Their focus is on learning an attention model for sharing information between the policies.
As such, this approach is complementary to ours, and a combination of both approaches could yield
further performance benefits in cases where centralized policies are desirable.
Our proposed approach is more flexible than the aformentioned approaches for MARL. Our algo-
rithm is able to train policies in environments with any reward setup, different action spaces for each
agent, a variance-reducing baseline that only marginalizes the relevant agent’s actions, and with a
set of centralized critics that dynamically attend to the relevant information for each agent at each
time point. As such, our approach is more scalable to the number of agents, and is more broadly
applicable to different types of environments.
3	Our Approach
We start by introducing the necessary notation and basic building blocks for our approach. We then
describe our ideas in detail.
3.1	Notation and Background
We consider the framework of Markov Games (Littman, 1994), which is a multi-agent extension
of Markov Decision Processes. They are defined by a set of states, S, action sets for each of N
agents, A1, ..., AN, a state transition function, T : S × A1 × ... × AN → P(S), which defines the
probability distribution over possible next states, given the current state and actions for each agent,
and a reward function for each agent that also depends on the global state and actions of all agents,
Ri : S × A1 × ... × AN → R. We will specifically be considering a partially observable variant
in which an agent, i receives an observation, oi , which contains partial information from the global
state, s ∈ S. Each agent learns a policy, πi : Oi → P (Ai) which maps each agent’s observation to
a distribution over it’s set of actions. The agents aim to learn a policy that maximizes their expected
discounted returns, Ji(∏i) = E°ι〜∏ι,……〜∏n,s〜T[P∞=o Ytrit(St,aιt,..,aNt)], where Y ∈ [0,1]
is the discount factor that determines how much the policy favors immediate reward over long-term
gain.
Policy Gradients Policy gradient techniques (Sutton et al., 2000; Williams, 1992) aim to estimate
the gradient of an agent’s expected returns with respect to the parameters of its policy. This gradient
estimate takes the following form:
∞
Vθ J(∏θ) = Ea〜∏θ Vθ log(∏θ(at∣St)) E YtTIrt0(St, at0)	(1)
t0=t
Actor-Critic and Soft Actor-Critic The term Pt∞0=t Yt0-trt0 (St0 , at0) in the policy gradient es-
timator leads to high variance, as these returns can vary drastically between episodes. Actor-critic
methods (Konda & Tsitsiklis, 2000) aim to ameliorate this issue by using a function approximation
of the expected returns, and replacing the original return term in the policy gradient estimator with
this function. One specific instance of actor-critic methods learns a function to estimate expected dis-
counted returns, given a state and action, Qψ(St, at) = E[Pt∞0=t Yt0-trt0(St0, at0)], learned through
temporal-difference learning by minimizing the regression loss:
Lq(Ψ) = Es,a,r,s0 [(Qψ (s, a) - y)2] , where y = r(s, a) + YE。，〜∏3) [Qψ(s', a')]	⑵
where Qψ is the target Q-ValUe function.
To encourage exploration and avoid converging to non-optimal deterministic policies, recent ap-
proaches of maximum entropy reinforcement learning learn a soft value function by modifying the
policy gradient to incorporate an entropy term (Haarnoja et al., 2018):
Vθ J(∏θ) = Ea〜∏θ [Vθ log(∏θ(a∣s))(a log(∏θ(a∣s)) - Qψ(s,a) + b(s))]	(3)
3
Under review as a conference paper at ICLR 2019
where b(s) is a state-dependent baseline (for the Q-value function). The loss function for temporal-
difference learning of the value function is also revised accordingly with a new target:
y = r(S, a) + YEaO 〜π(s0) [Qψ(S0, aO)- α log(πθ (a0|sO))]	⑷
While an estimate of the value function Vφ(s) can be used a baseline, we provide an alternative that
further reduces variance and addresses credit assignment in the multi-agent setting in section 3.2.
3.2 Multiple-Actor-Attention-Critic (MAAC)
The main idea behind our multi-agent learning approach is to learn the critic for each agent by
selectively paying attention to other agents’ actions. This is the same paradigm of training critics
centrally (to overcome the challenge of non-stationary non-Markovian environments) and executing
learned policies distributedly. Figure 1 illustrates the main components of our approach.
Attention The attention mechanism functions in a manner similar to a differentiable key-value
memory model (Graves et al., 2014; Oh et al., 2016). Intuitively, each agent queries the other
agents for information about their observations and actions and incorporates that information into
the estimate of its value function. This paradigm was chosen, in contrast to other attention-based
approaches, as it doesn't make any assumptions about the temporal or spatial locality of the inputs,
as opposed to approaches taken in the natural language processing and computer vision fields.
To calculate the Q-value function Qψ (o, a) for
the agent i, the critic receives the observations,
o = (oι,…，on), and actions, a = (aι,…，aN),
for all agents indexed by i ∈ {1... N}. We
represent the set of all agents except i as \i and
We index this set with j. Qψ (o, a) is a function
of agent i's observation and action, as well as
other agents, contributions:
Qψ (o,a) = fi(gi(oi ,ai),xi)	(5)
where f is a two-layer multi-layer perceptron
(MLP), while gi is a one-layer MLP embedding
function. The contribution from other agents,
Xi, is a weighted sum of each agent,s value:
xi = Eaj Vj = Eajhygj (Oj ,aj))
j=i	j=i
unique to each agent
shared among agents
Figure 1: Calculating Qψ (o, a) with attention for
agent i.
where the value, vj is a function of agent j ,s
embedding, encoded with an embedding function and then linearly transformed by a shared matrix
V. h is an element-wise nonlinearity (we have used leaky ReLU).
The attention weight aj compares the embedding ej with ei = gi(oi, ai), using a bilinear mapping
(ie, the query-key system) and passes the similarity value between these two embeddings into a
softmax
ɑj Z exp(eTWTWqei)	(6)
where Wq transforms ei into a “query” and Wk transforms ej into a “key”. The matching is then
scaled by the dimensionality of these two matrices to prevent vanishing gradients (Vaswani et al.,
2017).
In our experiments, we have used multiple attention heads (Vaswani et al., 2017). In this case, each
head, using a separate set of parameters (Wk , Wq, V), gives rise to an aggregated contribution from
all other agents to the agent i and we simply concatenate the contributions from all heads as a single
vector. Crucially, each head can focus on a different weighted mixture of agents.
Note that the weights for extracting selectors, keys, and values are shared across all agents, which
encourages a common embedding space. The sharing of critic parameters between agents is possi-
ble, even in adversarial settings, because multi-agent value-function approximation is, essentially,
4
Under review as a conference paper at ICLR 2019
a multi-task regression problem. This method can easily be extended to include additional infor-
mation, beyond local observations and actions, at training time, including the global state if it is
available, simply by adding additional encoders, e. (We do not consider this case in our experi-
ments, however, as our approach is effective in combining local observations to predict expected
returns in environments where the global state may not be available).
Learning with Attentive Critics All critics are updated together to minimize a joint regression
loss function, due to the parameter sharing:
N
Lq(Ψ) = XE(o,a,r,o0)〜D [(Qψ(o, a) - Ui)2], where
i=1
(7)
yi = ri + YEa05(。0) [Qψ (O0,aO)- α log(π(¾ (ai|o；H
where ψ and θ are the parameters of the target critics and target policies respectively. Note that
Qiψ , the action-value estimate for agent i, receives observations and actions for all agents. α is
the temperature parameter determining the balance between maximizing entropy and rewards. The
individual policies are updated with the following gradient:
Vθi J(∏θ ) = Ea〜∏θ
[Vθi log(∏θi(ai∣0i))(α
log(πθi (ai|oi)) - Qiψ(o,a) + b(o, a\i))
(8)
where b(o, a\i) is the multi-agent baseline used to calculate the advantage function decribed in the
following section. Note that we are sampling all actions, a, from all agents’ current policies in
order to calculate the gradient estimate for agent i, unlike in the MADDPG algorithm Lowe et al.
(2017), where the other agents’ actions are sampled from the replay buffer, potentially causing
overgeneralization where agents fail to coordinate based on their current policies Wei et al. (2018).
Full training details and hyperparameters can be found in the appendix 6.1.
Multi-Agent Advantage Function As shown in Foerster et al. (2018), an advantage function
using a baseline that only marginalizes out the actions of the given agent from Qiψ (o, a), can help
solve the multi-agent credit assignment problem. In other words, by comparing the value of a
specific action to the value of the average action for the agent, with all other agents fixed, we can
learn whether said action will cause an increase in expected return or whether any increase in reward
is attributed to the actions of other agents. The form of this advantage function is shown below:
Ai (o, a) = Qiψ (o, a) - b(o, a\i)), where
b(O, a∖i)) = Eai~∏i(θi) [Qy (O, (ai, a∖i ))|
(9)
Using our attention mechanism, we can implement a more general and flexible form of a multi-
agent baseline that, unlike the advantage function proposed in Foerster et al. (2018), doesn’t assume
the same action space for each agent, doesn’t require a global reward, and attends dynamically to
other agents, as in our Q-function. This is made simple by the natural decomposition of an agents
encoding, ei, and the weighted sum of encodings of other agents, xi, in our attention model.
Concretely, in the case of discrete policies, we can calculate our baseline in a single forward pass by
outputting the expected return Qi(O, (ai, a∖i)) for every possible action, ai ∈ Ai, that agent i can
take. We can then calculate the expectation exactly:
Eai ~∏i(θi) Qiψ(O, (ai, a∖i)) = X n(ai|Oi)Qi (O, (ai,a∖i))
a0i∈Ai
In order to do so, we must remove ai from the input of Qi , and output a value for every action.
We add an observation-encoder, ei = gio(Oi), for each agent, using these encodings in place of the
ei = gi(Oi, ai) described above, and modify fi such that it outputs a value for each possible action,
rather than the single input action. In the case of continuous policies, we do not need to add any
parameters, as we can simply estimate the expectation in Equation 9 by sampling actions from our
policy and averaging their Q-values, though, this comes at the cost of multiple expensive passes
through the network.
5
Under review as a conference paper at ICLR 2019
4 Experiments
4.1	Setup
We construct two environments that test various capabilities of our
approach (MAAC) and baselines. We investigate in two main di-
rections. First, we study the scalability of different methods as the
number of agents grows. We hypothesize that the current approach of
concatenating all agents’ observations (often used as a global state to
be shared among agents) and actions in order to centralize critics does
not scale well. To this end, we implement a cooperative environment,
Cooperative Treasure Collection, with shared rewards where we can
vary the total number of agents. The experimental results in sec 4.3
validate our claim.
Secondly, we want to evaluate each method’s ability to attend to infor-
mation relevant to rewards. Moreover, the relevance (to rewards) can
dynamically change during an episode. This is analogous to real-life
tasks such as the soccer example presented earlier. To this end, we
implement a Rover-Tower task environment where randomly paired
agents communicate information and coordinate.
The two environments are implemented in the multi-agent particle
environment framework1 introduced by Mordatch & Abbeel (2018),
and extended by Lowe et al. (2017). We found this framework use-
ful for creating environments involving complex interaction between
agents, while keeping the control and perception problems simple, as
we are primarily interested in addressing agent interaction. To further
simplify the control problem, we use discrete action spaces, allow-
ing agents to move up, down, left, right, or stay; however, the agents
may not immediately move exactly in the specified direction, as the
task framework incorporates a basic physics engine where agents’
momentums are taken into account. Fig. 2 illustrates the two envi-
ronments.
Cooperative Treasure Collection The cooperative environment in
Figure 2a) involves 8 total agents, 6 of which are ”treasure hunters”
and 2 of which are “treasure banks”, which each correspond to a
different color of treasure. The role of the hunters is to collect the
treasure of any color, which re-spawn randomly upon being collected
(with a total of 6), and then “deposit” the treasure into the correctly
colored “bank”. The role of each bank is to simply gather as much
treasure as possible from the hunters. All agents are able to see each
others’ positions with respect to their own. Hunters receive a global
reward for the successful collection of treasure and all agents receive a global reward for the de-
positing of treasure. Hunters are additionally penalized for colliding with each other. As such, the
task contains a mixture of shared and individual rewards and requires different “modes of attention”
which depend on the agent’s state and other agents’ potential for affecting its rewards.
Rover-Tower The environment in Figure 2b involves 8 total agents, 4 of which are “rovers” and
another 4 which are “towers”. At each episode, rovers and towers are randomly paired. The pair is
negatively rewarded by the distance of the rover to its goal. The task can be thought of as a navigation
task on an alien planet with limited infrastructure and low visibility. The rovers are unable to see
in their surroundings and must rely on communication from the towers, which are able to locate
the rovers as well as their destinations and can send one of five discrete communication messages
to their paired rover. Note that communication is highly restricted and different from centralized
policy approaches Jiang & Lu (2018), which allow for free transfer of continuous information among
policies. In our setup, the communication is integrated into the environment (in the tower’s action
1https://github.com/openai/multiagent-particle-envs
(a) Cooperative Treasure
Collection. The small grey
agents are “hunters” who
collect the colored trea-
sure, and deposit them with
the correctly colored large
“bank” agents.
(b) Rover-Tower. Each
grey “Tower” is paired with
a “Rover” and a destina-
tion (color of rover corre-
sponds to its destination).
Their goal is to communi-
cate with the ”Rover” such
that it moves toward the
destination.
Figure 2: Our environments
6
Under review as a conference paper at ICLR 2019
Table 1: Comparison of various methods for multi-agent RL
	Base Algorithm	Attention	Centralized Critic(s)	Number of Critics	Multi-task Learning of Critics	Multi-Agent Advantage
MAAC (OUrS)	-	SaC	X	X	N	X	X
MAAC (Uniform) (ours)	SAC	-	uniform	X	N	X	X
COMA*	Actor-Critic (On-Policy)		X	1		X	-
MADDPGt	DDPG		X	N		
COMA+SAC	SaC		X	1		X
MADDPG+SAC	SaC		X	N		X	-
DDPG*	-	DDPG	-			N	N/A	N/A 一
Centralized Critic(s): each agent’s estimate of Qi takes the actions and observations of the other agents into account. Number of Critics:
number of separate networks used for predicting Qi for all N agents. Multi-task Learning of Critics: all agents’ estimates of Qi share
information in intermediate layers, benefiting from multi-task learning. Multi-Agent Advantage: cf. SeC 3.2 for details. * (Foerster et al.,
2018), t (Lowe et al., 2017), * (LiUiCrap et al., 2016)
spaCe and the rover’s observation spaCe), rather than being expliCitly part of the model, and is limited
to a few disCrete signals.
4.2	Baselines
We Compare to two reCently proposed approaChes for Centralized training of deCentralized poliCies:
MADDPG (Lowe et al., 2017) and COMA (Foerster et al., 2018), as well as a single-agent RL
approaCh, DDPG, trained separately for eaCh agent.
In order to enable learning in disCrete aCtion spaCes for both MADDPG and DDPG, where deter-
ministiC poliCies are not possible, we use the Gumbel-Softmax reparametrization triCk (Jang et al.,
2017). We will refer to these modified versions as MADDPG (DisCrete) and DDPG (DisCrete). For
a detailed desCription of this reparametrization, see the appendix 6.2. We use soft aCtor CritiC to op-
timize. Thus, in order to have fair Comparisons, we additionally implement MADDPG and COMA
with Soft ACtor-CritiC, named as MADDPG+SAC and COMA+SAC.
We also Consider an ablated version of our model as a variant of our approaCh. In this model, we
use uniform attention by fixing the attention weight αj (Eq. 6) to be 1/(N - 1). This restriCtion
prevents the model from foCusing its attention on speCifiC agents.
All methods are implemented suCh that their approximate total number of parameters (aCross agents)
are equal to our method, and eaCh model is trained with 6 random seeds eaCh. Hyperparameters for
eaCh underlying algorithm are tuned based on performanCe and kept Constant aCross all variants of
CritiC arChiteCtures for that algorithm. A thorough Comparison of all baselines is summarized in
Table 1.
4.3	Results and Analysis
Fig. 3 illustrates averaged rewards per episode by various methods. The proposed approaCh (MAAC)
is Competitive with other approaChes being Compared. In what follows, we provide detailed analysis.
Impact of Rewards and Required Attention Uniform attention is Competitive with our approaCh
in the Cooperative Treasure ColleCtion (CTC) environment, but not in Rover-Tower. On the other
hand, both MADDPG (DisCrete) and MADDPG+SAC perform well on Rover-Tower, though they
do not on CTC. Both variants of COMA do not fare well in our environments. DDPG, arguably a
weaker baseline, performs surprisingly well in CTC, but does poorly in Rover-Tower.
In CTC, the rewards are shared aCross agents thus an agent’s CritiC does not need to foCus on infor-
mation from speCifiC agents in order to CalCulate its expeCted rewards. Moreover, eaCh agent’s loCal
observation provides enough information to make a deCent prediCtion of its expeCted rewards. This
might explain why MAAC (uniform) whiCh attends to other agents equally, and DDPG (being very
unattentive to other agents) perform well.
On the other hand, rewards in the Rover-Tower environment for a speCifiC agent are tied to another
single agent’s observations. This environment exemplifies a Class of sCenarios where dynamiC atten-
tion Can be benefiCial: when subgroups of agents are interaCting and performing Coordinated tasks
with separate rewards, but the groups do not remain statiC. This explains why MAAC (uniform)
7
Under review as a conference paper at ICLR 2019
——MAAC
MAAC (Uniform)
——MADDPG (Discrete)
——MAD D PG+SAC
——COMA
——COMA+SAC
IOO
80
60
40
20
0
-20
0	10000	20000	30000	40000
Training Episodes
Figure 3: (Left) Average Rewards on Cooperative Treasure Collection. (Right) Average Rewards on Rover-
Tower. Our model (MAAC) is competitive in both environments. Error bars are a 95% confidence interval
across 6 runs.
125
100
75
50
25
0
-25
-50
0	10000	20000	30000	40000
Training Episodes
perform poorly and DDPG completely breaks down, as knowing information from another specific
agent is crucial in predicting expected rewards.
COMA uses a single centralized network for predicting Q-values for all agents with separate forward
passes. Thus, this approach may perform best in environments with global rewards and agents with
similar action spaces. However, our environments have agents with differing roles (and non-global
rewards in the case of Rover-Tower). Thus both variants of COMA do not fare well.
MADDPG (and its variant) is a very strong method. However, we suspect its low performance in
CTC is due to this environment’s relatively large observation spaces for all agents, as the MADDPG
critic concatenates observations for all agents into a single input vector for each agent’s critic. Our
next experiments confirm this hypothesis.
Scalability We compare the average rewards attained by both ap-
proaches (normalized by the range of rewards attained in the en-
vironment, as differing the number of agents changes the nature
of rewards in each environment), and show that the improvement
of our approach MAAC over MADDPG+SAC grows with respect
to the number of agents. As suspected, MADDPG-like critics use
all information non-selectively, while our approach can learn which
agents to pay more attention through the attention mechanism. Thus
Table 2: MAAC improves
over MADDPG+SAC
# agents	4	8	16
Percentage	^20^	"49^	-33-
our approach scales better when the number of agents increases. In future research we will continue
to improve the scalability when the number of agents further increases by sharing policies among
agents, and performing attention on sub-groups (of agents).
While the Rover-Tower task has a lot of agents, each agent only gets information about its paired
agent - in other words, the task itself has an intrinsically smaller number of “other agents”(Con-
ditioned on each agent) than the CTC environment. As a future direction, we are creating more
complicated environments where each agent needs to cope with a large group of agents where selec-
tive attention is needed. This naturally models real-life scenarios that multiple agents are organized
in clusters/sub-societies (school, work, family, etc) where the agent needs to interact with a small
number of agents from many groups. We anticipate that in such complicated scenarios, our ap-
proach, combined with some advantages exhibited by other approaches would do well.
5 Conclusion
We propose an algorithm for training decentralized policies in multi-agent settings. The key idea
is to utilize attention in order to select relevant information for estimating critics. We analyze the
performance of the proposed approach with respect to the number of agents, different configurations
of rewards, and the span of relevant observational information. Empirical results are promising and
we intend to extend to highly complicated and dynamic environments.
8
Under review as a conference paper at ICLR 2019
References
Jimmy Ba, Volodymyr Mnih, and Koray Kavukcuoglu. Multiple object recognition with visual
attention. In International Conference on Learning Representations, 2015.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In International Conference on Learning Representations, 2015.
LUcian Buyoniu, Robert BabUska, and Bart De Schutter. Multi-agent reinforcement learning: An
overview. In Innovations in multi-agent Systems and applications-1, pp. 183-221. Springer, 2010.
Jinyoung Choi, Beom-Jin Lee, and Byoung-Tak Zhang. Multi-focus attention network for efficient
deep reinforcement learning. arXiv preprint arXiv:1712.04603, December 2017.
Felix Fischer, Michael Rovatsos, and Gerhard Weiss. Hierarchical reinforcement learning in
communication-mediated multiagent coordination. In Proceedings of the Third International
Joint Conference on Autonomous Agents and Multiagent Systems-Volume 3, pp. 1334-1335. IEEE
Computer Society, 2004.
Jakob Foerster, Ioannis Alexandros Assael, Nando de Freitas, and Shimon Whiteson. Learning to
communicate with deep multi-agent reinforcement learning. In Advances in Neural Information
Processing Systems, pp. 2137-2145, 2016.
Jakob Foerster, Nantas Nardelli, Gregory Farquhar, Triantafyllos Afouras, Philip H. S. Torr, Push-
meet Kohli, and Shimon Whiteson. Stabilising experience replay for deep multi-agent reinforce-
ment learning. In Proceedings of the 34th International Conference on Machine Learning, vol-
ume 70 of Proceedings of Machine Learning Research, pp. 1146-1155, International Convention
Centre, Sydney, Australia, 06-11 Aug 2017.
Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual multi-agent policy gradients, 2018.
Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint
arXiv:1410.5401, 2014.
Jayesh K Gupta, Maxim Egorov, and Mykel Kochenderfer. Cooperative multi-agent control using
deep reinforcement learning. In Autonomous Agents and Multiagent Systems, Lecture Notes in
Computer Science, pp. 66-83. Springer, Cham, May 2017.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In Proceedings of the 35th
International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning
Research, pp. 1861-1870, Stockholmsmssan, Stockholm Sweden, 10-15 Jul 2018.
He He, Jordan Boyd-Graber, Kevin Kwok, and Hal DaUme III. Opponent modeling in deep rein-
forcement learning. In International Conference on Machine Learning, pp. 1804-1813, 2016.
Nicolas Heess, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, Yuval Tassa, Tom Erez,
Ziyu Wang, Ali Eslami, Martin Riedmiller, et al. Emergence of locomotion behaviours in rich
environments. arXiv preprint arXiv:1707.02286, 2017.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In
International Conference on Learning Representations, 2017.
Jiechuan Jiang and Zongqing Lu. Learning attentional communication for multi-agent cooperation.
arXiv preprint arXiv:1805.07733, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations, 2014.
Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in neural information
processing systems, pp. 1008-1014, 2000.
9
Under review as a conference paper at ICLR 2019
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In Inter-
national Conference on Learning Representations, 2016.
Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and
Yoshua Bengio. A structured self-attentive sentence embedding. In International Conference on
Learning Representations, 2017.
Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In
Machine Learning Proceedings 1994, pp. 157-163. Elsevier,1994.
Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent
actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information
Processing Systems, pp. 6382-6393, 2017.
Volodymyr Mnih, Nicolas Heess, Alex Graves, et al. Recurrent models of visual attention. In
Advances in neural information processing systems, pp. 2204-2212, 2014.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529, 2015.
Igor Mordatch and Pieter Abbeel. Emergence of grounded compositional language in multi-agent
populations, 2018.
Junhyuk Oh, Valliappa Chockalingam, Honglak Lee, et al. Control of memory, active perception,
and action in minecraft. In International Conference on Machine Learning, pp. 2790-2799, 2016.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and
Shimon Whiteson. QMIX: Monotonic value function factorisation for deep multi-agent rein-
forcement learning. In Proceedings of the 35th International Conference on Machine Learning,
volume 80 of Proceedings of Machine Learning Research, pp. 4295-4304, Stockholmsmssan,
Stockholm Sweden, 10-15 Jul 2018.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning, pp. 1889-1897, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484-489, 2016.
Sainbayar Sukhbaatar, Rob Fergus, et al. Learning multiagent communication with backpropaga-
tion. In Advances in Neural Information Processing Systems, pp. 2244-2252, 2016.
Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max
Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z. Leibo, Karl Tuyls, and Thore Graepel. Value-
decomposition networks for cooperative multi-agent learning based on team reward. In Proceed-
ings of the 17th International Conference on Autonomous Agents and MultiAgent Systems, AA-
MAS ’18, pp. 2085-2087, Richland, SC, 2018. International Foundation for Autonomous Agents
and Multiagent Systems.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. In Advances in neural informa-
tion processing systems, pp. 1057-1063, 2000.
Ardi Tampuu, Tambet Matiisen, Dorian Kodelja, Ilya Kuzovkin, Kristjan Korjus, Juhan Aru, Jaan
Aru, and Raul Vicente. Multiagent cooperation and competition with deep reinforcement learning.
PLoS One, 12(4):e0172395, April 2017.
10
Under review as a conference paper at ICLR 2019
Ming Tan. Multi-agent reinforcement learning: independent versus cooperative agents. In Proceed-
ings of the Tenth International Conference on International Conference on Machine Learning,
pp. 330-337. Morgan KaUfmann Publishers Inc., 1993.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-
mation Processing Systems, pp. 6000-6010, 2017.
Ermo Wei, Drew Wicke, David Freelan, and Sean Luke. Multiagent soft q-learning. arXiv preprint
arXiv:1804.09817, 2018.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. In Reinforcement Learning, pp. 5-32. Springer, 1992.
11
Under review as a conference paper at ICLR 2019
6	Appendix
Algorithm 1 Training Procedure for Attention-Actor-Critic
1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20:	Initialize E parallel environments with N agents, each Initialize replay buffer, D Tupdate《-0 for iep = 1 . . . num episodes do Reset environments, and get initial oie for each agent, i for t = 1 . . . steps per episode do Select actions ae 〜∏(∙∣oe) for each agent, i, in each environment, e Send actions to all parallel environments and get o0ie, rie for all agents in all environments Store transitions (o1...N, a1...N, r1...N, o01...N) for all environments in D Tupdate = Tupdate + E if Tupdate ≥ min steps per update then for j = 1 . . . num critic updates do Sample minibatch B J m X (oι…N, aι...N, rι...N, o；…N)〜D UPDATECRITIC(B) end for for j = 1 . . . num policy updates do Sample m × (o；…N)〜D UPDATEPOLICIES(o1B...N) end for Update target critic and policy parameters: ψ = τψ + (1 - τ)ψ θ = τθ+(1 — T )θ
21:	Tupdate J 0
22:	end if
23:	end for
24:	end for
25:
26:	function UPDATECRITIC(B)
27:	Unpack minibatch (o；B...N, a；B...N, r；B...N, o0；B...N) J B
28:	Calculate Qiψ(o；B...N, a；B...N) for all i in parallel
29:	0	力/ 0R∖ Calculate a” 〜πθ (θiB) using target policies
30:	Calculate Qiψ (o0；B...N, a0；B...N) for all i in parallel, using target critic
31:	Update critic: N Lq(Ψ) = XE(o,a,r,o"D [(Qψ(o,a) - Ui)2], where	(⑼ yi = ri + YEa0 〜∏i(o0) [Qψ (O0, a0) - a log(πθi (ailoi))]
32: ɔɔ.	end function
33: 34:	function UPDATEPOLICIES(o；B...N)
35:	Calculate °Bn 〜∏θ(oiB), i ∈ 1... N
36:	Calculate Qiψ (o；B...N, a；B...N) for all i in parallel
37:	Update policies: Vθi J(∏θ) = Ea^πθ [Vθilog(∏θi(ai∣0i))(a log(∏θi(a∕θi)) - Qψ(o,a) + b(o, a))]	(11)
38:	end function
12
Under review as a conference paper at ICLR 2019
6.1	Training Procedure
We train using Soft Actor-Critic (Haarnoja et al., 2018), an off-policy, actor-critic method for max-
imum entropy reinforcement learning. Our training procedure consists of performing 12 parallel
rollouts, and adding a tuple of (ot, at, rt, ot+1)1...N to a replay buffer (with maximum length 1e6)
for each timepoint. We reset each environment after every 100 steps (an episode). After 100 steps
(across all rollouts), we perform 4 updates for the attention critic and for all policies. For each update
we sample minibatches of 1024 timepoints from the replay buffer and then perform gradient descent
on the Q-function loss objective (7), as well as the policy objective (8), using Adam (Kingma &
Ba, 2014) as the optimizer for both with a learning rate of 0.001. These updates can be computed
efficiently in parallel (across agents) using a GPU. After the updates are complete, we update the pa-
rameters ψ of our target critic Qψ to move toward our learned critic's parameters, ψ, as in LillicraP
et al. (2016); Haarnoja et al. (2018): ψ = (1 - T)ψ + τψ, where T is the update rate (set to 0.002
for attention parameters and 0.04 for all other parameters). Using a target critic has been shown
to stabilize the use of experience replay for off-policy reinforcement learning with neural network
function approximators (Mnih et al., 2015; Lillicrap et al., 2016). We update the parameters of the
target policies, G in the same manner. We use a discount factor, γ, of 0.99. All networks (separate
policies and contained within the centralized critics) use a hidden dimension of 128 and Leaky Rec-
tified Linear Units as the nonlinearity. We use 0.2 as our temperature setting for Soft Actor-Critic.
Additionally, we typically use 4 attention heads in our attention critics unless otherwise specified.
6.2	Reparametrization of DDPG/MADDPG for Discrete Action Spaces
In order to compare to DDPG and MADDPG in our environments with discrete action spaces, we
must make a slight modification to the basic algorithm. This modification is first suggested by Lowe
et al. (2017) in order to enable policies that output discrete communication messages. Consider the
original DDPG policy gradient which takes advantage of the fact that you can easily calculate the
gradient of the output of a deterministic policy with respect to its parameters.
Vθ J = Es〜ρ [VaQ(s, a)∣a=μ(s)Vθμ(s∣θ)]
Rather than policies that deterministically output an action from within a continuous action space,
we use policies that produce differentiable samples through a Gumbel-Softmax distribution (Jang
et al., 2017). Using differentiable samples allows us to use the gradient of expected returns to train
policies without using the log derivative trick, just as in DDPG.
vθJ = Es〜ρ,a〜π(s) [vaQ(s, a)vθa
6.3	Visualizing Attention
In order to understand how the use of attention evolves over the course of training, we examine the
”entropy” of the attention weights for each agent for each of the four attention heads that we use in
both tasks (Figures 4 and 5). The black bars indicate the maximum possible entropy (i.e. uniform
attention across all agents). Lower entropy indicates that the head is focusing on specific agents, with
an entropy of 0 indicating attention focusing on one agent. In Rover-Tower, we plot the attention
entropy for each rover. Interestingly, each agent appears to use a different combination of the four
heads, but their use is not mutually exclusive, indicating that the inclusion of separate attention heads
for each agent is not necessary. This differential use of attention heads is sensible due to the nature
of rewards in this environment (i.e. individualized rewards). In the case of Treasure Collection, we
find that all agents use the attention heads similarly, which is unsurprising considering that rewards
are shared in that environment.
In order to inspect how the attention mechanism is working on a more fine-grained level, we visu-
alize the attention weights for one of the rovers in Rover-Tower (Figure 6), from the head that the
agent appears to use the most (determined by looking at Figure 4), while changing the tower that
said rover is paired to. In these plots, we ignore the weights over other rovers for simplicity since
these are always near zero. We find that the rover learns to strongly attend to the tower that it is
paired with, without any explicit supervision signal to do so. The model implicitly learns which
13
Under review as a conference paper at ICLR 2019
Figure 4:	Attention ”entropy” for each head over the course of training for the four rovers in the
Rover-Tower environment
1.9
e ι.8
δ
1.6
V1
10000
20000
30000
40000
——Head 3
---Uniform Weights
r∙7
1.9
e ι.8
δ
1.6
----Uniform Weights
10000
20000
30000
40000
1.5 j~r-
0
50000
1.5 j~r-
0
50000
Figure 5:	Attention ”entropy” for each head over the course of training for two collectors in the
Treasure Collection Environment
agent is most relevant to estimating the rover’s expecture future returns, and said agent can change
dynamically without affecting the performance of the algorithm.
6.4 Continuous Action Spaces
MADDPG	MAAC
-2.47 ± 0.05一	-2.49 ± 0.1Γ
In order to test our model’s ability to handle continuous action
spaces, we add a network for each agent to learn a state-value Table 3: Cooperative Navigation
function Vi(o, a\i), which uses the same weighted attention em- (Continuous)
bedding over other agents as Qi(o, a). The loss functions to
learn both networks are provided by Haarnoja et al. (2018). We
test on an environment introduced in Lowe et al. (2017) called
Cooperative Navigation and compare to MADDPG. Our results
are presented in Table 3. This task does not require attention, as
all agents are relevant to each others rewards at each time step. As such, it is unsurprising that our
14
Under review as a conference paper at ICLR 2019
Figure 6: Attention weights when subjected to different Tower pairings for Rover 1 in Rover-Tower
environment
approach matches but does not surpass the performance of MADDPG. It is notable, however, both
that attention does not harm performance in simple cases and that our approach handles continuous
action spaces as well.
15