Under review as a conference paper at ICLR 2019
Stochastic Gradient Descent Learns
State Equations with Nonlinear Activations
Anonymous authors
Paper under double-blind review
Ab stract
We study discrete time dynamical systems governed by the state equation ht+1 =
φ(Aht + But). Here A, B are weight matrices, φ is an activation function, and
ut is the input data. This relation is the backbone of recurrent neural networks
(e.g. LSTMs) which have broad applications in sequential learning tasks. We utilize
stochastic gradient descent to learn the weight matrices from a finite input/state
trajectory {ut , ht}tN=0 . We prove that SGD estimate linearly converges to the
ground truth weights while using near-optimal sample size. Our results apply to
increasing activations whose derivatives are bounded away from zero. The analysis
is based on i) a novel SGD convergence result with nonlinear activations and ii)
careful statistical characterization of the state vector. Numerical experiments verify
the fast convergence of SGD on ReLU and leaky ReLU in consistence with our
theory.
1	Introduction
A wide range of problems involve sequential data with a natural temporal ordering. Examples include
natural language processing, time series prediction, system identification, and control design, among
others. State-of-the-art algorithms for sequential problems often stem from dynamical systems theory
and are tailored to learn from temporally dependent data. Linear models and algorithms; such as
Kalman filter, PID controller, and linear dynamical systems, have a long history and are utilized in
control theory since 1960's with great success (Brown et al. (1992); Ho & Kalman (1966); Astrom
& Hagglund (1995)). More recently, nonlinear models such as recurrent neural networks (RNN)
found applications in complex tasks such as machine translation and speech recognition (Bahdanau
et al. (2014); Graves et al. (2013); Hochreiter & Schmidhuber (1997)). Unlike feedforward neural
networks, RNNs are dynamical systems that use their internal state to process inputs. The goal of this
work is to shed light on the inner workings of RNNs from a theoretical point of view. In particular,
we focus on the RNN state equation which is characterized by a nonlinear activation function φ, state
weight matrix A, and input weight matrix B as follows
ht+1 = φ(Aht + But),
(1.1)
Here ht is the state vector and ut is the input data at timestamp t. This equation is the source of
dynamic behavior of RNNs and distinguishes RNN from feedforward networks. The weight matrices
A and B govern the dynamics of the state equation and are inferred from data. We will explore
the statistical and computational efficiency of stochastic gradient descent (SGD) for learning these
weight matrices.
Contributions: Suppose we are given a finite trajectory of input/state pairs (ut , ht)tN=0 generated
from the state equation (1.1). We consider a least-squares regression obtained from N equations;
with inputs (ut, ht)tN=1 and outputs (ht+1)tN=1. For a class of activation functions including leaky
ReLU and for stable systems1, we show that SGD linearly converges to the ground truth weight
matrices while requiring near-optimal trajectory length N . In particular, the required sample size is
O(n + p) where n and p are the dimensions of the state and input vectors respectively. The results
are extended to unstable systems when the samples are collected from multiple independent RNN
trajectories rather than a single trajectory. Our theory applies to increasing activation functions
1 Throughout this work, a system is called stable if the spectral norm of the state matrix A is less than 1.
1
Under review as a conference paper at ICLR 2019
whose derivatives are bounded away from zero, which includes leaky ReLU, and Gaussian input
data. Numerical experiments on ReLU and leaky ReLU corroborate our theory and demonstrate that
SGD converges faster as the activation slope increases. To obtain our results, we i) characterize the
statistical properties of the state vector (e.g. well-conditioned covariance) and ii) derive a novel SGD
convergence result with nonlinear activations; which may be of independent interest. As a whole, this
paper provides a step towards foundational understanding of RNN training via SGD.
1.1	Related Work
Our work is related to the recent optimization and statistics literature on linear dynamical systems
(LDS) and neural networks.
Linear dynamical systems: The state-equation (1.1) reduces to aLDS when φ is the linear activation
(φ(x) = x). Identifying the weight matrices is a core problem in linear system identification and is
related to the optimal control problem (e.g. linear quadratic regulator) with unknown system dynamics.
While these problems are studied since 1950's (Ljung (1998; 1987); Astrom & Eykhoff (1971)),
our work is closer to the recent literature that provides data dependent bounds and characterize the
non-asymptotic learning performance. Recht and coauthors have a series of papers exploring optimal
control problem (Simchowitz et al. (2018); Tu et al. (2018; 2017)). In particular, Hardt et al. (2016)
shows gradient descent learns single-input-single-output (SISO) LDS with polynomial guarantees.
Oymak & Ozay (2018) and Faradonbeh et al. (2018) provide sample complexity bounds for learning
LDS. Sanandaji et al. (2011b;a); Pereira et al. (2010) study the identification of sparse systems.
Neural networks: There is a growing literature on the theoretical aspects of deep learning and
provable algorithms for training neural networks. Most of the existing results are concerned with
feedforward networks. Ge et al. (2017); Li & Yuan (2017); Mei et al. (2018b); Soltanolkotabi
(2017); Janzamin et al. (2015); Zhong et al. (2017b) consider learning fully-connected shallow
networks with gradient descent. Mei et al. (2018a); Soltanolkotabi et al. (2017); Foster et al. (2018)
analyze empirical landscape of related nonlinear learning problems. Brutzkus & Globerson (2017);
Zhong et al. (2017a); Du et al. (2017); Goel et al. (2018) address convolutional neural networks;
which is an efficient weight-sharing architecture. Brutzkus et al. (2017); Wang et al. (2018) studies
over-parameterized networks when data is linearly separable. Janzamin et al. (2015); Oymak &
Soltanolkotabi (2018) utilize tensor decomposition techniques for learning feedforward neural nets.
For recurrent networks, Sedghi & Anandkumar (2016) proposed tensor algorithms with polynomial
guarantees and Khrulkov et al. (2017) studied their expressive power. More recently, Miller & Hardt
(2018) showed that stable RNNs can be approximated by feed-forward networks.
2	Problem Setup
We first introduce the notation. k ∙ ∣∣ returns the spectral norm of a matrix and Smin(∙) returns the
minimum singular value. The activation φ : R → R applies entry-wise if its input is a vector.
Throughout, φ is assumed to be a 1-Lipschitz function. With proper scaling of its parameters, the
system (1.1) with a Lipschitz activation can be transformed into a system with 1-Lipschitz activation.
The functions Σ[∙] and var[∙] return the covariance of a random vector and variance of a random
variable respectively. In is the identity matrix of size n X n. Normal distribution with mean μ
and covariance Σ is denoted by N(μ, Σ). Throughout, c, C, co, cι,... denote positive absolute
constants.
Setup: We consider the dynamical system parametrized by an activation function φ(∙) and weight
matrices A ∈ Rn×n, B ∈ Rn×p as described in (1.1). Here, ht is the n dimensional state-vector and
ut is the p dimensional input to the system at time t. As mentioned previously, (1.1) corresponds to
the state equation of a recurrent neural network. For most RNNs of interest, the state ht is hidden and
we only get to interact with ht via an additional output equation. For Elman networks Elman (1990),
this equation is characterized by some output activation φy and output weights C , D as follows
yt = φy(Cht + Dut).	(2.1)
In this work, our attention is restricted to the state equation (1.1); which corresponds to setting
yt = ht+1 in the output equation. To analyze (1.1) in a non-asymptotic data-dependent setup, we
assume a finite input/state trajectory of {ut, ht}tN=0 generated by some ground truth weight matrices
2
Under review as a conference paper at ICLR 2019
Algorithm 1 Learning state equations with nonlinear activations
1:	Inputs: (yt, ht, Ut)N=I sampled from a trajectory. Scaling μ, learning rate η. Initialization
A0, B0.
2:	Outputs: Estimates A, B of the weight matrices A, B .
3:	Xt J [μhT uT]T for 1 ≤ t ≤ N.
4:	Θo J [μ-1Ao Bo]
5:	for τ from 1 to END do
6:	Pick γτ from {1, 2, . . . , N} uniformly at random.
7:	Θτ J Θτ-1- NLYT (Θτ-i)
8:	end for
9:	return [A B] J Θend μIn	0 .
0	Ip
(A, B). Our goal is learning the unknown weights A and B in a data and computationally efficient
way. In essence, we will show that, if the trajectory length satisfies N & n + p, SGD can quickly and
provably accomplish this goal using a constant step size.
Appoach: Our approach is described in Algorithm 1. It takes two hyperparameters; the scaling factor
μ and learning rate η. Using the RNN trajectory, We construct N triples of the form {ut, %, ht+ι}N=ι.
We formulate a regression problem by defining the output vector yt, input vector xt, and the target
parameter C as folloWs
yt = ht+ι , Xt = [μhtl ∈ Rn+p , C = [μ-1A B] ∈ Rn×(n+p).	(2.2)
ut
With this reparameterization, We find the input/output identity yt = φ(CXt). We Will consider the
least-squares regression given by
1N	1
L(θ) = N J2Lt(θ) Where Lt(θ) = 2kyt - φ(θxt)k22.	(2.3)
For learning the ground truth parameter C, We utilize SGD on the loss function (2.3) With a constant
learning rate η. Starting from an initial point Θ0, after END SGD iterations, Algrorithm 1 returns an
estimate C = ΘEND. Estimates of A and B are decoded from the left and right submatrices of C
respectively.
3 Main Results
3.1	Preliminaries
The analysis of the state equation naturally depends on the choice of the activation function; Which is
the source of nonlinearity. We first define a class of Lipschitz and increasing activation functions.
Definition 3.1 (β-increasing activation). Given 1 ≥ β ≥ 0, the activation function φ satisfies
φ(0) = 0 and 1 ≥ φ0(x) ≥ β for all x ∈ R.
Our results Will apply to strictly increasing activations Where φ is β-increasing for some β > 0.
Observe that, this excludes ReLU activation Which has zero derivative for negative values. HoWever,
it includes Leaky ReLU Which is a generalization of ReLU. Parameterized by 1 ≥ β ≥ 0, Leaky
ReLU is a β-increasing function given by
LReLU(x) = max(βx, x).	(3.1)
In general, given an increasing and 1-Lipschitz activation φ, a β-increasing function φβ can be
obtained by blending φ With the linear activation, i.e. φβ (x) = (1 - β)φ(x) + βx.
A critical property that enables SGD is that the state-vector covariance Σ[ht] is Well-conditioned
under proper assumptions. The lemma beloW provides upper and loWer bounds on this covariance
matrix in terms of problem variables.
3
Under review as a conference paper at ICLR 2019
Lemma 3.2 (State vector covariance). Consider the state equation (1.1) where h° = 0 and Utli吟
N(0, Ip). Define the upper bound term Bt as
B TB OAE
Bt = kBW 1 -kA∣∣2 .
(3.2)
•	Suppose φ is 1-Lipschitz and φ(0) = 0. Then, for all t ≥ 0, Σ[ht] Bt2In.
•	Suppose φ is a β-increasing function andp ≥ n. Then, Σ[ht] β2smin(B)2In.
As a natural extension from linear dynamical systems, we will say the system is stable if kAk < 1
and unstable otherwise. For activations we consider, stability implies that if the input is set to 0, state
vector ht will exponentially converge to 0 i.e. the system forgets the past states quickly. This is also
the reason (Bt)t≥0 sequence converges for stable systems and diverges otherwise. The condition
number of the covariance will play a critical role in our analysis. Using Lemma 3.2, this number can
be upper bounded by ρ defined as
(b∞ Y= ( kBk Y 1
IeSmin(B)J	ISmin(B)J β 2(1-||4||2).
(3.3)
Observe that, the condition number of B appears inside the ρ term.
3.2	Learning from Single Trajectory
Our main result applies to stable systems (kAk < 1) and provides a non-asymptotic convergence
guarantee for SGD in terms of the upper bound on the state vector covariance. This result characterizes
the sample complexity and the rate of convergence of SGD; and also provides insights into the role of
activation function and the spectral norm of A.
Theorem 3.3 (Main result). Let {ut, ht+1}tN=1 be a finite trajectory generated from the state equation
(1.1). Suppose k Ak < 1, φ is β-increasing, ho = 0, P ≥ n, and UJ吟 N(0, Ip). Let P be same as
(3.3) and c, C, c0 be properly chosen absolute constants. Pick the trajectory length N to satisfy
N ≥ CLρ2(n+p),
where L = 1 一 Ilog(CAP). Pick scaling μ = 1∕B∞, learning rate η = co PnieI+p), and consider the
loss function (2.3). With probability 1 — 4N exp(-100n) — 8L exp(-O( ^^)), starting from an
initial point Θo, for all τ ≥ 0, the SGD iterations described in Algorithm 1 satisfies
β4
E[kΘτ - CkF] ≤ (1 - co9 2 β	ʌ )τkΘo - CkF.	(3.4)
2ρ2 n(n + p)
Here the expectation is over the randomness of the SGD updates.
Sample complexity: Theorem 3.3 essentially requires N & (n + p)∕β4 samples for learning. This
can be seen by unpacking (3.3) and ignoring the logarithmic L term and the condition number of
B. Observe that O(n + p) growth achieves near-optimal sample size for our problem. Each state
equation (1.1) consists of n sub-equations (one for each entry of ht+1). We collect N state equations
to obtain a system of Nn equations. On the other hand, the total number of unknown parameters
in A and B are n(n + p). This implies Theorem 3.3 is applicable as soon as the problem is mildly
overdetermined i.e. Nn & n(n + p).
Computational complexity: Theorem 3.3 requires O(n(n + p) log ε) iterations to reach ε-
neighborhood of the ground truth. Our analysis reveals that, this rate can be accelerated if the
state vector is zero-mean. This happens for odd activation functions satisfying φ(-x) = -φ(x)
(e.g. linear activation). The result below is a corollary and requires ×n less iterations.
Theorem 3.4 (Faster learning for odd activations). Consider the same setup provided in Theorem 3.3.
Additionally, assume that φ is an odd function. Pick scaling μ = 1 ∕B∞, learning rate η = co。(： p),
and consider the loss function (2.3). With probability 1 - 4N exp(-100n) - 8L exp(-O( LN)),
starting from an initial point Θo, for all τ ≥ 0, the SGD iterations described in Algorithm 1 satisfies
β4
E[kΘτ - CkF] ≤ (1 - co2ρ2β + P) )τkΘo - CkF,	(3.5)
where the expectation is over the randomness of the SGD updates.
4
Under review as a conference paper at ICLR 2019
Another aspect of the convergence rate is the dependence on β. In terms of β, the SGD error (3.4)
decays as (1 - O(β8))τ. While it is not clear how optimal is the exponent 8, numerical experiments
in Section 6 demonstrate that larger β indeed results in drastically faster convergence.
4 Main Ideas and Proof Strategy
We first outline our high-level proof strategy for Theorem 3.3; which brings together ideas from
statistics and optimization.
1.	We first show that input data is well-behaved by proving that state-vector ht has a well-
conditioned covariance as discussed in Lemma 3.2 and shown in Appendix B. The key idea
is if φ is β-increasing, then the random input data ut provides sufficient excitation for the
output state ht+1.
2.	Even if individual samples are well-behaved, analyzing (2.3) is still challenging due to
temporal dependencies between the samples. These dependencies prevent us from directly
using statistical learning results that typically assume i.i.d. samples. We show that the
dependency between samples at time t and t + T decay exponentially fast in separation T
(for stable systems). This is outlined in Appendix C.
3.	This observation allows us to obtain nearly independent data by subsampling the original
trajectory to get (hiT , uiT)i≥0. Thanks to exponential decay, a logarithmically small T
can be chosen to generate large subtrajectories of size N/T . Appendix D uses additional
perturbation arguments to establish the well-behavedness of the overall data matrix.
4.	To conclude, we obtain a deterministic result which establishes fast convergence result for
β-increasing activations and well-behaved dataset. This is provided in Theorem 4.1 and
proved in Appendix A.
The first three steps are related to the statistical nature of the problem which can be decoupled
from the last step. Specifically, the last step derives a deterministic result that establishes the linear
convergence of SGD for β-increasing functions. For linear convergence proofs, a typical strategy is
showing the strong convexity of the loss function i.e. showing that, for some α > 0 and all points
v, u, the gradient satisfies
hVL(v) 一 VL(u), V — Ui ≥ α∣∣v — 〃忆.
The core idea of our convergence result is that the strong convexity parameter of the loss function with
β-increasing activations can be connected to the loss function with linear activations. In particular,
recalling (2.3), set ytlin = Cxt and define the linear loss to be
1N
Llin(Θ) = 2N X kytin - Θxtk22.
2N i=1
Denoting the strong convexity parameter of the original loss by αφ and that of linear loss by αlin , we
argue that αφ ≥ β2 αlin; which allows us to establish a convergence result as soon as αlin is strictly
positive. Next result is our SGD convergence theorem which follows from this discussion.
Theorem 4.1 (Deterministic convergence). Suppose a data set {xi , yi}iN=1 is given; where output yi
is related to input xi via yi = φ(hxi, θi) for some θ ∈ Rn. Suppose β > 0 and φ is a β -increasing.
Let γ+ ≥ γ- > 0 be scalars. Assume that input samples satisfy the bounds
1N
Y+In 占 N 2 XixT 占 Y-In ,	l∣Xik22 ≤ B for all i.
Let {rτ}τ∞=0 be a sequence of i.i.d. integers uniformly distributed between 1 to N. Then, starting
β2γ
from an arbitrary point θo, setting learning rate η = YʃB , for all T ≥ 0, the SGD iterations for
quadratic loss
θτ+1 = θτ — η(φ(xrTτ θτ) — yrτ )φ0 (xrTτ θτ)xrτ,	(4.1)
5
Under review as a conference paper at ICLR 2019
satisfies the error bound
β4γ2
E[kθτ - θkl] ≤kθo - θkl (1 - T)τ,	(4.2)
γ+B
where the expectation is over the random selection of the SGD iterations {rτ }τ∞=0.
This theorem provides a clean convergence rate for SGD for β-increasing activations and naturally
generalizes standard results on linear regression which corresponds to β = 1. We remark that related
results appear in the literature on generalized linear models. Kakade et al. (2011); Foster et al.
(2018); Mei et al. (2018a) provide learning theoretic loss/gradient/hessian convergence results for
isotonic regression, robust regression, and β-increasing activations. Goel et al. (2018) establishes a
similar result for leaky ReLU activations under the assumption of symmetric input distribution and
infinitely many samples (i.e. in population limit). Compared to these, we establish a deterministic
linear convergence guarantee for SGD that works whenever the data matrix is full rank. We believe
extensions to proximal gradient methods might be beneficial for high-dimensional nonlinear problems
(e.g. sparse/low-rank approximation, manifold constraints Cai et al. (2010); Beck & Teboulle (2009);
Oymak et al. (2018); Agarwal et al. (2010); Pereira et al. (2010)) and is left as a future work.
To derive our main results in Section 3, we need to address the first three steps outlined earlier and
determine the conditions under which Theorem 4.1 is applicable to the data obtained from RNN state
equation with high probability. Below we provide desirable characteristics of the state vector; which
enables our statistical results.
Assumption 1 (Well-behaved state vector). Let L > 1 be an integer. There exists positive scalars
γ十,γ-,θ and an absolute constant C > 0 such that θ ≤ 3√n and thefollowing holds
•	Lower bound: Σ[hL-1]	γ-In,
•	Upper bound: for all t, the state vector satisfies
Σ[ht] W Y+In	,	∣∣ht	— E[ht]kψ2	≤	C√Y+	and	∣∣E[ht]k'2	≤	θ√Y+.	(4.3)
Here ∣∣∙∣∣ψ2 returns the subgaussian norm of a vector (see Def 5.22 ofVershynin (2010)).
Assumption 1 ensures that covariance is well-conditioned, state vector is well-concentrated, and it
has a reasonably small expectation. Our next theorem establishes statistical guarantees for learning
the RNN state equation based on this assumption.
Theorem 4.2 (General result). Let {ut, ht+1}tN=1 be a length N trajectory of the state equation (1.1).
Suppose k A∣ < 1, φ is β-increasing, h° = 0, and Ut 啊，N(0, Ip). Given scalars γ+ ≥ Y- > 0,
Set the COnditiOn number as P = γ+∕γ-. For absolute constants C, c, co > 0, choose trajectory
length N to satisfy
N ≥ CLp2 (n + P) where L =「1 — Iog (CnP)].
log ∣A∣
Suppose Assumption 1 holds with L, γ+,γ-,θ. Pick scaling to be μ = 1∕√γ+ and learning rate to
be η = co ρ(θ+√β)2(n+p). With probability 1 一 4N exp(-100n) — 8L exp(-O( LN)), startingfrom
Θ0, for all τ ≥ 0, the SGD iterations on loss (2.3) as described in Algorithm 1 satisfies
β4
E[∣Θτ - CkF] ≤ (1 - Co717-√⅛τ2--)τkΘo - CkF,	(4.4)
2P2 (θ + 2)2 (n + p)
where the expectation is over the randomness of SGD updates.
The advantage of this theorem is that, it isolates the optimization problem from the statistical properties
of state vector. If one can prove tighter bounds on achievable (Y+, Y-, θ), it will immediately imply
improved performance for SGD. In particular, Theorems 3.3 and 3.4 are simple corollaries of Theorem
4.2 with proper choices.
•	Theorem 3.3 follows by setting γ+ = B∞, γ- = β2Smin(B)2, and θ = √n.
•	Theorem 3.4 follows by setting Y+ = B∞2 , Y- = β2smin(B)2, and θ = 0.
6
Under review as a conference paper at ICLR 2019
5 Learning Unstable Systems
So far, we considered learning from a single RNN trajectory for stable systems (kAk < 1). For such
systems, as the time goes on, the impact of the earlier states disappear. In our analysis, this allows us
to split a single trajectory into multiple nearly-independent trajectories. This approach will not work
for unstable systems (A is arbitrary) where the impact of older states may be amplified over time. To
address this, we consider a model where the data is sampled from multiple independent trajectories.
Suppose N independent trajectories of the state-equation (1.1) are available. Pick some integer
T0 ≥ 1. Denoting the ith trajectory by the triple (ht(i+)1, ht(i), ut(i))t≥0, we collect a single sample
from each trajectory at time T0 to obtain the triple (h(Ti)+1, h(Ti) , u(Ti)). To utilize the existing
optimization framework (2.3); for 1 ≤ i ≤ N, we set,
(yi, hi, ui) = (hT0+1 , hT0 , uT0 ).	(5.1)
With this setup, we can again use the SGD Algorithm 1 to learn the weights A and B . The crucial
difference compared to Section 3 is that, the samples (yi, hi, ui)iN=1 are now independent of each
other; hence, the analysis is simplified. As previously, having an upper bound on the condition
number of the state-vector covariance is critical. This upper bound can be shown to be ρ defined as
ρ
{ρ
if n > 1
1-β2 * |A|2
1-(β∣A∣)2T0
BT2
ifn=1 where P= β2Sm⅛F.
(5.2)
The ρ term is similar to the earlier definition (3.3); however it involves Bt° rather than B∞. This
modification is indeed necessary since B∞ = ∞ when kAk > 1. On the other hand, note that, BT2
grows proportional to kAk2T0 ; which results in exponentially bad condition number in T0 . Our ρ
definition remedies this issue for single-output systems; where n = 1 and A is a scalar. In particular,
when β = 1 (e.g. φ is linear) ρ becomes equal to the correct value 12. The next theorem provides our
result on unstable systems in terms of this condition number and other model parameters.
Theorem 5.1 (Unstable systems). Suppose we are given N independent trajectories (h(ti) , ut(i))t≥0
for 1 ≤ i ≤ N. Each trajectory is sampled at time T0 to obtain N samples (yi , hi , ui)iN=1 where the
ith sample is given by (5.1). Suppose the sample size satisfies
N ≥ Cρ2(n + p)
where P is given by (5.2). Assume the initial states are 0, φ is β-increasing, P ≥ n, and Ut L吟
N(0, Ip). Set scaling μ = 1 /，Bt°, learning rate η
co PnIel+p), and run SGD over the equations
described in (2.2) and (2.3). Starting from Θ0, with probability 1 - 2N exp(-100(n + p)) -
4exp(-O(PN2)), all SGD iterations satisfy
β4
E[kθτ - C kF ] ≤ (1 - co 2ρ2 nβn+P) )τkθo - C kF
where the expectation is over the randomness of the SGD updates.
6	Numerical Experiments
We conducted experiments on ReLU and Leaky ReLU activations. Let us first describe the experi-
mental setup. We pick the state dimension n = 50 and the input dimension P = 100. We choose the
ground truth matrix A to be a scaled random unitary matrix; which ensures that all singular values of
A are equal. B is generated with i.i.d. N(0, 1) entries. Instead of using the theoretical scaling choice,
We determine the scaling μ from empirical covariance matrices outlined in Algorithm 2. Similar to
our proof strategy, this algorithm equalizes the spectral norms of the input and state covariances to
speed up convergence. We also empirically determined the learning rate and used η = 1/100 in all
experiments.
2 Clearly, any nonzero 1 × 1 covariance matrix has condition number 1. However, due to subtleties in the proof
strategy, we don’t use ρ = 1 for β < 1. Obtaining tighter bounds on the subgaussian norm of the state-vector
would help resolve this issue.
7
Under review as a conference paper at ICLR 2019
Algorithm 2 Empirical hyperparameter selection.
1:	Inputs: (ht, ut)tN=1 sampled from a trajectory.
2:	Outputs: Scaling μ.
3:	Form the empirical covariance matrix Σh from {ht}tN=1
4:	Form the empirical covariance matrix Σu from {ut }tN=1
5:	return pk∑uk∕k∑hk.
01234567
O ------ -
Iooooooo
JoXl山 P ①z=LLLION
IOT
0	10000 20000 30000 40000 50000
Iteration
JOXI山 P①而LLLION
10-≡
0	10000 20000 30000 40000 50000
Iteration
(a)	(b)
Figure 1:	SGD convergence behavior for Leaky ReLUs with varying minimum slope β . Figures a) and b)
repeat the same experiments. The difference is the spectral norm of the ground truth state matrix A.
Evaluation: We consider two performance measures in the experiments. Let C be an estimate of
the ground truth parameter C = [μ-1A B]. The first measure is the normalized error defined as
kC - CkF/kc kF. The second measure is the normalized loss defined as
P= kyt- Φ(C χt)k22
-P=I⅛-.
In all experiments, we run Algorithm 1 for 50000 SGD iterations and plot these measures as a function
of τ; by using the estimate available at the end of the τth SGD iteration for 0 ≤ τ ≤ 50000. Each
curve is obtained by averaging the outcomes of 20 independent realizations.Our first experiments
use N = 500; which is mildly larger than the total dimension n + p = 150. In Figure 1, we plot
the Leaky ReLU errors with varying slopes as described in (3.1). Here β = 0 corresponds to ReLU
and β = 1 is the linear model. In consistence with our theory, SGD achieves linear convergence and
as β increases, the rate of convergence drastically improves3 . The improvement is more visible for
less stable systems driven by A with a larger spectral norm. In particular, while ReLU converges for
small kAk, SGD gets stuck before reaching the ground truth when kAk = 0.8.
To understand, how well SGD fits the training data, in Figure 2a, we plotted the normalized loss
for ReLU activation. For more unstable system (kAk = 0.9), training loss stagnates in a similar
fashion to the parameter error. We also verified that the norm of the overall gradient kVL(Θτ)∣∣f
continues to decay (where Θτ is the τth SGD iterate); which implies that SGD converges before
reaching a global minima. As A becomes more stable, rate of convergence improves and linear rate
is visible. Finally, to better understand the population landscape of the quadratic loss with ReLU
activations, Figure 2b repeats the same ReLU experiments while increasing the sample size five
times to N = 2500. For this more overdetermined problem, SGD converges even for kAk = 0.9;
indicating that
•	population landscape of loss with ReLU activation is well-behaved,
•	however ReLU problem requires more data compared to the Leaky ReLU for finding global
minima.
Overall, as predicted by our theory, experiments verify that SGD indeed quickly finds the optimal
weight matrices of the state equation (1.1) and as the activation slope β increases, the convergence
rate improves.
3 Note that convergence becomes faster for larger β under the realizable model i.e. there exists a ground truth
state equation with activation slope β that can fit the observed trajectory. This is consistent with the technical
setup our results are proven. Also note that the data distribution in the experiments changes with the activation
slope β . If the dataset is fixed and not realizable, the results may be different as we vary the slope β .
8
Under review as a conference paper at ICLR 2019
111
sso-jp ① z=roLLLION
IOOOO 20000 30000 40000 50000
Iteration
sso-jp ① z=roLLLION
Iteration
(a)	(b)
Figure 2:	SGD convergence behavior for ReLU with varying spectral norm of the state matrix A. Figures a)
and b) repeats the same experiments. The difference is that a) uses N = 500 trajectory length whereas b) uses
N = 2500 (i.e. ×5 more data). Shaded regions highlight the one standard deviation around the mean.
7 Conclusions
This work showed that SGD can learn the nonlinear dynamical system (1.1); which is characterized by
weight matrices and an activation function. This problem is of interest for recurrent neural networks
as well as nonlinear system identification. We showed that efficient learning is possible with optimal
sample complexity and good computational performance. Our results apply to strictly increasing
activations such as Leaky ReLU. We empirically showed that Leaky ReLU converges faster than
ReLU and requires less samples; in consistence with our theory. We list a few unanswered problems
that would provide further insights into recurrent neural networks.
•	Covariance of the state-vector: Our results depend on the covariance of the state-vector and
requires it to be positive definite. One might be able to improve the current bounds on the condition
number and relax the assumptions on the activation function. Deriving similar performance bounds
for ReLU is particularly interesting.
•	Hidden state: For RNNs, the state vector is hidden and is observed through an additional equation
(2.1); which further complicates the optimization landscape. Even for linear dynamical systems,
learning the (A, B, C, D) system ((1.1), (2.1)) is a non-trivial task Ho & Kalman (1966); Hardt et al.
(2016). What can be said when we add the nonlinear activations?
•	Classification task: In this work, we used normally distributed input and least-squares regression
for our theoretical guarantees. More realistic input distributions might provide better insight into
contemporary problems, such as natural language processing; where the goal is closer to classification
(e.g. finding the best translation from another language).
9
Under review as a conference paper at ICLR 2019
References
Alekh Agarwal, Sahand Negahban, and Martin J Wainwright. Fast global convergence rates of gradient methods
for high-dimensional statistical recovery. In Advances in Neural Information Processing Systems, pp. 37-45,
2010.
Karl Johan Astrom and Peter Eykhoff. System identification-a survey. Automatica, 7(2):123-162, 1971.
Karl Johan Astrom and Tore Hagglund. PID controllers: theory, design, and tuning, volume 2. Instrument
society of America Research Triangle Park, NC, 1995.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to
align and translate. arXiv preprint arXiv:1409.0473, 2014.
Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.
SIAM journal on imaging sciences, 2(1):183-202, 2009.
Robert Grover Brown, Patrick YC Hwang, et al. Introduction to random signals and applied Kalman filtering,
volume 3. Wiley New York, 1992.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian inputs. arXiv
preprint arXiv:1702.07966, 2017.
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. Sgd learns over-parameterized
networks that provably generalize on linearly separable data. arXiv preprint arXiv:1710.10174, 2017.
Jian-Feng Cai, Emmanuel J CandBs, and Zuowei Shen. A singular value thresholding algorithm for matrix
completion. SIAM Journal on Optimization, 20(4):1956-1982, 2010.
S. Dirksen. Tail bounds via generic chaining. arXiv preprint arXiv:1309.3522, 2013.
Simon S Du, Jason D Lee, and Yuandong Tian. When is a convolutional filter easy to learn? arXiv preprint
arXiv:1709.06129, 2017.
Jeffrey L Elman. Finding structure in time. Cognitive science, 14(2):179-211, 1990.
Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, and George Michailidis. Finite time identification in
unstable linear systems. Automatica, 96:342-353, 2018.
Dylan J Foster, Ayush Sekhari, and Karthik Sridharan. Uniform convergence of gradients for non-convex
learning and optimization. NIPS, 2018.
Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape design.
arXiv preprint arXiv:1711.00501, 2017.
Surbhi Goel, Adam Klivans, and Raghu Meka. Learning one convolutional layer with overlapping patches.
arXiv preprint arXiv:1802.02547, 2018.
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent neural
networks. In Acoustics, speech and signal processing (icassp), 2013 ieee international conference on, pp.
6645-6649. IEEE, 2013.
Moritz Hardt, Tengyu Ma, and Benjamin Recht. Gradient descent learns linear dynamical systems. arXiv
preprint arXiv:1609.05191, 2016.
BL Ho and Rudolph E Kalman. Effective construction of linear state-variable models from input/output functions.
at-Automatisierungstechnik, 14(1-12):545-548, 1966.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural COmPUtatiOn, 9(8):1735-1780,
1997.
Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of non-convexity: Guaranteed
training of neural networks using tensor methods. arXiv PrePrint arXiv:1506.08473, 2015.
Sham M Kakade, Varun Kanade, Ohad Shamir, and Adam Kalai. Efficient learning of generalized linear and
single index models with isotonic regression. In Advances in Neural Information Processing Systems, pp.
927-935, 2011.
Valentin Khrulkov, Alexander Novikov, and Ivan Oseledets. Expressive power of recurrent neural networks.
arXiv PrePrint arXiv:1711.00811, 2017.
10
Under review as a conference paper at ICLR 2019
Michel Ledoux. The concentration of measure phenomenon. American Mathematical Soc., 2001.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. In Advances
in Neural Information Processing Systems, pp. 597-607, 2017.
Lennart Ljung. System identification: theory for the user. Prentice-hall, 1987.
Lennart Ljung. System identification. In Signal analysis and prediction, pp. 163-173. Springer, 1998.
Song Mei, Yu Bai, Andrea Montanari, et al. The landscape of empirical risk for nonconvex losses. The Annals
of Statistics, 46(6A):2747-2774, 2018a.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-layers neural
networks. arXiv preprint arXiv:1804.06561, 2018b.
John Miller and Moritz Hardt. When recurrent models don’t need to be recurrent. arXiv preprint
arXiv:1805.10369, 2018.
Samet Oymak and Necmiye Ozay. Non-asymptotic identification of lti systems from a single trajectory. arXiv
preprint arXiv:1806.05722, 2018.
Samet Oymak and Mahdi Soltanolkotabi. End-to-end learning of a convolutional neural network via deep tensor
decomposition. arXiv preprint arXiv:1805.06523, 2018.
Samet Oymak, Benjamin Recht, and Mahdi Soltanolkotabi. Sharp time-data tradeoffs for linear inverse problems.
IEEE Transactions on Information Theory, 64(6):4129-4158, 2018.
Jos6 Pereira, Morteza Ibrahimi, and Andrea Montanari. Learning networks of stochastic differential equations.
In Advances in Neural Information Processing Systems, pp. 172-180, 2010.
Borhan M Sanandaji, Tyrone L Vincent, and Michael B Wakin. Exact topology identification of large-scale
interconnected dynamical systems from compressive observations. In American Control Conference (ACC),
2011, pp. 649-656. IEEE, 2011a.
Borhan M Sanandaji, Tyrone L Vincent, Michael B Wakin, Roland T6th, and Kameshwar Poolla. Compressive
system identification of lti and ltv arx models. In Decision and Control and European Control Conference
(CDC-ECC), 2011 50th IEEE Conference on, pp. 791-798. IEEE, 2011b.
Hanie Sedghi and Anima Anandkumar. Training input-output recurrent neural networks through spectral
methods. arXiv preprint arXiv:1603.00954, 2016.
Max Simchowitz, Horia Mania, Stephen Tu, Michael I Jordan, and Benjamin Recht. Learning without mixing:
Towards a sharp analysis of linear system identification. arXiv preprint arXiv:1802.08334, 2018.
Mahdi Soltanolkotabi. Learning relus via gradient descent. arXiv preprint arXiv:1705.04591, 2017.
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization landscape
of over-parameterized shallow neural networks. arXiv preprint arXiv:1707.04926, 2017.
Michel Talagrand. Gaussian processes and the generic chaining. In Upper and Lower Bounds for Stochastic
Processes, pp. 13-73. Springer, 2014.
Stephen Tu, Ross Boczar, Andrew Packard, and Benjamin Recht. Non-asymptotic analysis of robust control
from coarse-grained identification. arXiv preprint arXiv:1707.04791, 2017.
Stephen Tu, Ross Boczar, and Benjamin Recht. On the approximation of toeplitz operators for nonparametric
h∞-norm estimation. In 2018 Annual American Control Conference (ACC), pp. 1867-1872. IEEE, 2018.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
arXiv:1011.3027, 2010.
Gang Wang, Georgios B Giannakis, and Jie Chen. Learning relu networks on linearly separable data: Algorithm,
optimality, and generalization. arXiv preprint arXiv:1808.04685, 2018.
Kai Zhong, Zhao Song, and Inderjit S Dhillon. Learning non-overlapping convolutional neural networks with
multiple kernels. arXiv preprint arXiv:1711.03440, 2017a.
Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees for one-
hidden-layer neural networks. arXiv preprint arXiv:1706.03175, 2017b.
11
Under review as a conference paper at ICLR 2019
A Deterministic Convergence Result for SGD
ProofofTheorem 4.1. Given two distinct scalars a,b; define φ0(a,b) = OSa-φ(bb ∙ Φ0(a, b) ≥ β since Φ is
β-increasing. Define wτ to be the residual wτ = θτ - θ. Observing
φ(xrTτ θτ) - yrτ = φ0 (xrTτ θτ, xrTτ θ)xrTτ wτ ,
the SGD recursion obeys
l∣Wτ +lk22 =IIwT 一 η(Φ(xTT θτ) - yrτ )φ0(xTT θτ)Xrτ k22 .
= kwτ - ηxrτ φ0(xrTτ θτ)φ0 (xrTτ θτ , xrTτ θ)xrTτ wτ l`22
= l(I - ηGrτ)wτ l`22
where Grτ = xrτφ0(xrTτ θτ)φ0(xrTτ θτ, xrTτ θ)xrTτ . Since φ is 1-Lipschitz and β-increasing, Grτ is a positive-
semidefinite matrix satisfying
xrτ xrτ	Grτ	β xrτ xrτ ,
GrTτ Grτ	xrτ xrTτ xrτ xrTτ	Bxrτ xrTτ .
Consequently, we find the following bounds in expectation
γ+In	E[Grτ]	β2γ-In,	(A.1)
E[GrTτ Grτ]	Bγ+In.
Observe that (A.1) essentially lower bounds the strong convexity parameter of the problem with β2γ-; which is
the strong convexity of the identical problem with the linear activation (i.e. β = 1). However, we only consider
strong convexity around the ground truth parameter θ i.e. we restricted our attention to (θ, θτ) pairs. With this,
wτ+1 can be controlled as,
E[kwτ +1k22] = E[k(I - ηGrτ )wτ k22 ]
= lwτ l`22 - 2η E[wτT Grτwτ] + η2 E[wτT GrTτ Grτ wτ]
≤ lwτ l`22 (1 - 2ηβ2γ- + η2Bγ+).
β2γ
Setting η = B , We find the advertised bound
E[kwτ +ιk22] ≤ E[kwτk22](1- β4⅛).
2	2	γ+ B
Applying induction over the iterations τ, we find the advertised bound (4.2)
E[kwT k22 ] ≤ kw0k22 (I — Yj Y- )T.
2	2	γ+ B
□
Lemma A.1 (Merging L splits). Assume matrices X(i) ∈ RNi ×q are given for 1 ≤ i ≤ L. Suppose for all
1 ≤ i ≤ L, rows of X(i) has '2 norm at most VZB and each X(i) satisfies
X㈤T X⑶
Y+In h ---N------占 Y-In .
「X⑴]
X⑵
Set N = Pi=ι Ni and form the concatenated matrix X =	.	. Denote i th row of X by Xi. Then, for
.
.
X(L)
each i, lxi l`22 ≤ B and
XT X 1 N
Y+In 占	N = N X Xixi - Y-In ∙
i=1
Proof. The bound on the rows lXi l`2 directly follows by assumption. For the remaining result, first observe
thatXTX = PiL=1X(i)TX(i). Next, we have
LL	L
NY+In = XNiY+In - XX(i)TX(i) - XNiY-In =NY-In.
Combining these two yields the desired upper/lower bounds on XT X /N.
□
12
Under review as a conference paper at ICLR 2019
B	Properties of the nonlinear state equations
This section characterizes the properties of the state vector ht when input sequence is normally distributed.
These bounds will be crucial for obtaining upper and lower bounds for the singular values of the data matrix
X = [x1 . . . xN]T described in (2.2). For probabilistic arguments, we will use the properties of subgaussian
random variables. Orlicz norm provides a general framework that subsumes subgaussianity.
Definition B.1 (Orlicz norms). For a scalar random variable Orlicz-a norm is defined as
kXkψa =supk-1/a(E[|X|k])1/k
k≥1
Orlicz-a norm of a vector x ∈ Rp is defined as kxkψa = supv∈Bp kvT xkψa where Bp is the unit `2 ball. The
SubexPOnentialnOrmisthe Orlicz-1 norm ∣∣∙∣∣ψι and the subgaussian norm is the Orlicz- 2 norm ∣∣∙∣∣ψ2.
Lemma B.2 (Lipschitz properties of the state vector). Consider the state equation (1.1). Suppose activation φ
is 1-LiPschitz. Observe that ht+1 is a deterministic function of the inPut sequence {uτ }tτ=0. Fixing all vectors
{ui}i6=τ (i.e. all excePt uτ), ht+1 is kAkt-τ kB k LiPschitz function of uτ for 0 ≤ τ ≤ t.
Proof. Fixing {ui}i6=τ, denote ht+1 as a function of uτ by ht+1 (uτ). Given a pair of vectors uτ, u0τ using
1-Lipschitzness of φ, for any t > τ , we have
∣∣ht+ι(uτ) — ht+ι(uτ)k'2 ≤ kΦ(Aht(uτ) + But)- φ(Aht(U) + But)Il'2
≤ ∣∣A(ht(uτ) — ht(uT))k'2
≤ l∣A∣kht(uτ) — ht(uT)k'2.
Proceeding with this recursion until t = τ , we find
∣∣ht+ι(uτ) — ht+ι(u'τ)k'2 ≤ l∣Akt-τ∣hτ +ι(uτ) — hτ+ι(uT)k'2
≤	∣∣Akt-τkΦ(Ahτ + BuT) — Φ(Ahτ + BuT)k'2
≤	∣Akt-τ∣Bkkuτ — uTk'2.
This bound implies ht+ι(uτ) is ∣∣ A∣t-τ∣B∣ LiPsChitz function of uτ.	□
LemmaB.3 (Upper bound). Consider the state equation governed by equation (1.1). Suppose ut '％ . N(0,Ip),
φ is 1-Lipschitz, φ(0) = 0 and h0 = 0. Recall the definition (3.2) of Bt. We have the following properties
•	ht is a Bt-Lipschitz function of the vector qt = [u0T . . . utT-1]T ∈ Rtp.
•	There exists an absolute constant c > 0 such that ∣ht — E[ht]∣ψ2 ≤ cBt and Σ[ht]	Bt2In.
•	ht satisfies
E[∣htk22] ≤ tr(BBT)1一檄；≤ min{n,p}B2.
1 — ∣A∣
Also, there exists an absolute constant c > 0 such that for any m ≥ n, with probability 1 —
2exp(—100m), ∣∣ht∣∣'2 ≤ c√mBt.
Proof. i) Bounding Lipschitz constant: Observe that ht is a deterministic function of qt i.e. ht = f (qt) for
some function f. To bound Lipschitz constant of f, for all (deterministic) vector pairs qt and qt, we find a scalar
Lf satisfying,
kf(qt) — f(qt)k'2 ≤ Lfkqt - qt忆.	(b.i)
Define the vectors, {ai }it=0, as follows
T	T T	TT
ai = [u0 . . . ui-1 ui . . . ut-1] .
Observing that ao = qt, at = qt, we write the telescopic sum,
t-1
kf(qt) — f (qt)k'2 ≤ X kf(ai+ι) — f (ai)k'2.
i=0
Focusing on the individual terms f (ai+ι) — f (ai), observe that the only difference is the ui, ui terms. Viewing
ht as a function of ui and applying Lemma B.2,
kf(ai+ι) — f(ai)∣'2 ≤ IlAktT-ikBkkui - u。忆.
13
Under review as a conference paper at ICLR 2019
To bound the sum, we apply the Cauchy-Schwarz inequality; which yields
t-1
If(qt) — f(qt)| ≤ X kAk j-i∣∣Bkkui - Uik'2
i=0
≤
t-1	t-1
(X kAk2(j-i)kBk2 产(X kui — 同区产
i=0	i=0
V--------{z--------}
l∣qt-qt ll'2
八 ∣Bk2(1-kAk2t)
≤ V -T-IAF- kqt - qtk'2
=Btkqt- qt k'2.
(B.2)
The final line achieves the inequality (B.1) with Lf = Bt hence ht is Bt Lipschitz function of qt.
ii)	Bounding Subgaussian norm: When Ut i吧.N(0, Ip), the vector qt is distributed as N(0, Itp). Since ht a
Bt Lipschitz function of qt, for any fixed unit length vector v, αv := vTht = vTf(qt) is still Bt-Lipschitz
function of qt. Hence, using Gaussian concentration of Lipschitz functions, αv satisfies
P(∣αv
—E[αv]∣ ≥ t) ≤ 2exp(-
This implies that for any v, αv - E[αv] is O(Bt) subgaussian Vershynin (2010). This is true for all unit v,
hence using Definition B.1, the vector ht satisfies kht - E[ht]kψ2 ≤ O(Bt) as well. Secondly, Bt-Lipschitz
function of a Gaussian vector obeys the variance inequality var[αv] ≤ Bt2 (page 49 of Ledoux (2001)), which
implies the covariance bound
Σ[ht]	Bt2In.
iii)	Bounding '2-norm: To obtain this result, We first bound E[∣∣ht k^]. Since φ is T-Lipschitz and φ(0) = 0,
we have the deterministic relation
kht+1k'2 ≤ IlAht + Butk'2.
Taking squares of both sides, expanding the right hand side, and using the independence of ht , ut and the
covariance information of ut, We obtain
E[kht+ιk22] ≤ E[kAht + Butk22] = E[∣Ahtk22]+ 阻口田血脸]	(B.3)
≤ IAk2 E[khtk22]+ tr(BBT).	(B.4)
NoW that the recursion is established, expanding ht on the right hand side until h0 = 0, We obtain
1 - ∣∣A∣产t+1)
1-kAk2
t
E[kht+ιk22] ≤ X kAk2itr(BBT) ≤ tr(BBT)
i=0
NoW using the fact that tr(BBT) ≤ rank(B)kBk2 ≤ min{n, p}kBk2, We find
E[kht+1k'2]2 ≤ E[kht+ιk22] ≤ min{n,p}B2+ι.
Finally, using the fact that ht is Bt-LiPSChitz function and utilizing Gaussian concentration of qt 〜N(0, Itp),
We find
P(Ilht+1∣∣'2 - E[kht+ιk'2] ≥ t) ≤ eχp(-2B2).
Setting t = (C — 1)√mBt for sufficiently large c > 0, we find P(∣∣ht∣∣'2 ≥ √nBt + (c — 1)√mBt) ≤
exp(-100m).	□
Lemma B.4 (Odd activations). Suppose φ is strictly increasing and obeys φ(x) = -φ(-x) for all x and
ho = 0. Consider the state equation (1.1) driven Ut i% N(0, Ip). We have that E[ht] = 0.
Proof. We will inductively show that {ht}t≥0 has a symmetric distribution around 0. Suppose the vector ht
satisfies this assumption. Let S ⊂ Rn be a set. We will argue that P(ht+1 ⊂ S) = P(ht+1 ⊂ -S). Since φ is
strictly increasing, it is bijective on vectors, and we can define the unique inverse set S0 = φ-1(S). Also since φ
is odd, φ(-S0) = -S. Since ht, Ut are independent and symmetric, we reach the desired conclusion as follows
P(ht+1 ⊂	S) =	P(Aht + BUt ⊂	S0) = P(A(-ht) + B(-Ut)	⊂S0)	(B.5)
=	P(Aht + BUt ⊂	-S0) = P(φ(Aht + BUt) ⊂	φ(-S0))	= P(ht+1	⊂ -S).	(B.6)
□
14
Under review as a conference paper at ICLR 2019
Theorem B.5 (State-vector lower bound). Consider the nonlinear state equation (1.1) with {ut}t≥o 乂
N (0, Ip). Suppose φ is a β-increasing function for some constant β > 0. For any t ≥ 1, the state vector obeys
Σ[ht]	β2smin(BBT)In.
Proof. The proof is an application of Lemma B.7. The main idea is to write ht as sum of two independent
vectors, one of which has independent entries. Consider a multivariate Gaussian vector g 〜 N (0, Σ). g
is statistically identical to gι + g2 where gι 〜N(0, Smin(Σ)Id) and g2 〜N(0, Σ — Smin(∑)Id) are
independent multivariate Gaussians.
Since BUt 〜N(0, BBT), setting ∑ = BBT and Smin = Smin(∑), We have that BUt 〜gι + g2 where
gι, g2 are independent and gι 〜N(0, SminIn) and g2 〜N(0, Σ — SminIn). Consequently, we may write
BUt + Aht 〜gι + g2 + Aht.
For lower bound, the crucial component will be the g1 term; which has i.i.d. entries. Applying Lemma B.7 by
setting x = g1 and y = g2 + Aht, and using the fact that ht, g1, g2 are all independent of each other, we find
the advertised bound, for all t ≥ 0, via
Σ[ht+1] = Σ[φ(g1 +g2+Aht)]	β2sminIn.
□
The next theorem applies to multiple-input-single-output (MISO) systems where A is a scalar and B is a row
vector. The goal is refining the lower bound of Theorem B.5.
Theorem B.6 (MISO lower bound). Consider the setup of Theorem B.5 with single output i.e. n = 1. For any
t ≥ 1, the state vector obeys
var[ht] ≥ β2kBk22 11-Jβ!A)2t.
Proof. For any random variable X, applying Lemma B.7, we have var[φ(X)] ≥ β2var[X]. Recursively, this
yields
var[ht+ι] = var[φ(Aht + BUt)] ≥ β2var[Aht + But] = β2(∣A∣2var[ht] 十[田底).
Expanding these inequalities till h0, we obtain the desired bound
t
var[ht] ≥ X(βi∣A∣i-1kBk'2)2.
i=1
□
Lemma B.7 (Vector lower bound). Suppose φ is a β-increasing function. Let x = [x1 . . . xn]T be a vector
with i.i.d. entries distributed as Xi 〜X. Let y be a random vector independent of X. Then,
Σ[φ(x + y)]	β2var[X]In.
Proof. We first apply law of total covariance (e.g. Lemma B.8) to simplify the problem using the following
lower bound based on the independence of X and y,
Σ[φ(X + y)]	Ey[Σ[φ(X + y) y]]	(B.7)
= Ey[Σx[φ(X + y)]].	(B.8)
Now, focusing on the covariance Σx [φ(X + y)], fixing a realization of y, and using the fact that X has
i.i.d. entries; φ(X + y) has independent entries as φ applies entry-wise. This implies that Σx [φ(X + y)] is a
diagonal matrix. Consequently, its lowest eigenvalue is the minimum variance over all entries,
Σx [φ(X + y)]	min var[φ(Xi + yi)]In .
Fortunately, Lemma B.9 provides the lower bound var[φ(Xi + yi)] ≥ β2var[X]. Since this lower bound holds
for any fixed realization of y, it still holds after taking expectation over y; which concludes the proof. □
The next two lemmas are helper results for Lemma B.7 and are provided for the sake of completeness.
Lemma B.8 (Law of total covariance). Let X, y be two random vectors and assume y has finite covariance.
Then
Σ[y] = E[Σ[y X]] + Σ[E[y X]].
15
Under review as a conference paper at ICLR 2019
Proof. First, write Σ[y] = E[yyT] - E[y] E[yT]. Then, applying the law of total expectation to each term,
Σ[y] = E[E[yyT x]] - E[E[y x]]E[E[yT x]].
Next, we can write the conditional expectation as E[E[yyT x]] = E[Σ[y x]] + E[E[y x] E[y x]]T. To
conclude, we obtain the covariance of E[y x] via the difference,
E[E[y I χ] E[y I χ]]T - E[E[y ∣ x]] E[E[yT ∣ x]] = ∑[E[y ∣ x]],
which yields the desired bound.	口
Lemma B.9 (Scalar lower bound). Suppose φ is a β-increasing function with β > 0 as defined in Definition
3.1. Given a random variable X and a scalar y, we have
var[φ(X + y)] ≥ β2var[X].
Proof. Since φ is β-increasing, it is invertible and φ-1 is strictly increasing. Additionally, φ-1 is 1 /β Lipschitz
since,
∣φ(a) - φ(b)∣≥ β∣a - b| =⇒ |a - b| ≥ β∣φ-1(a) - φ-1 (b)|.
Using this observation and the fact that E[X] minimizes E(X - α)2 over α, var[φ(X + y)] can be lower
bounded as follows
var[φ(X + y)] =E(φ(X+y)-E[φ(X+y)])2
≥ β2 E((X + y) - φ-1(E[φ(X + y)]))2
≥β2E(X+y-E[X+y])2
= β2E(X- EX)2 = β2var[X].
Note that, the final line is the desired conclusion.	口
C Truncating Stable Systems
One of the challenges in analyzing dynamical systems is the fact that samples from the same trajectory
have temporal dependence. This section shows that, for stable systems, the impact of the past states decay
exponentially fast and the system can be approximated by using the recent inputs only. We first define the
truncation of the state vector.
Definition C.1 (Truncated state vector). Suppose φ(0) = 0, initial condition h0 = 0, and consider the state
equation (1.1). Given a timestamp t, L-truncation Ofthe state vector ht is denoted by ht,L and is equal to qt
where
qτ+1 = φ(Aqτ + Bu0τ)
q0 = 0
(C.1)
is the state vector generated by the inputs u0τ satisfying
0
uτ
(0 ifτ < t - L
uτ else
In words, L truncated state vector ht,L is obtained by unrolling ht until time t — L and setting the contribution
of the state vector ht-L to 0. This way, ht,L depends only on the variables {u『}T=t-L∙
The following lemma states that impact of truncation can be made fairly small for stable systems (kAk < 1).
Lemma C.2 (Truncation impact - deterministic). Consider the state vector ht and its L-truncation ht,L from
Definition C.1. Suppose φ is 1-Lipschitz. We have that
Ilht- ht,Lk'2 ≤
0 if t ≤ L
1 kAkLkht-L∣'2 else
Proof. When t ≤ L, Definition C.1 implies uT = Uτ hence ht = qt = ht,L. When t > L, we again use
Definition C.1 and recall that u0τ = 0 until time τ = t - L - 1. For all t - L < τ ≤ t, using 1-Lipschitzness
of φ, we have that
IlhT - qτI® = kΦ(Ahτ-ι + Buτ-ι) - Φ(Aqτ-ι + Buτ-1)k'2
≤ Il(AhT-1 + Buτ-ι) - (Aqτ-ι + Buτ-1)k'2
≤ ∣∣A(hτ-1 - qτ-1)k'2 ≤ IlAkkhT-I- qτ-1k'2.
Applying this recursion between t - L < τ ≤ t and using the fact that qt-L = 0 implies the advertised result
Iht- qt∣∣'2 ≤ IAILIht-L - qt-L I'2
≤ ||4内&一忆.
□
16
Under review as a conference paper at ICLR 2019
C.1 Near independence of sub -trajectories
We will now argue that, for stable systems, a single trajectory can be split into multiple nearly independent
trajectories. First, we describe how the sub-trajectories are constructed.
Definition C.3 (Sub-trajectory). Let sampling rate L ≥ 1 and offset 1 ≤ T ≤ L be two integers. Let N = NT
be the largest integer obeying (N — 1)L + T ≤ N. We SamPle the trajectory {ht, Ut}N=o at the points
T,T + L,...,T + (N — 1)L + T and define the Tth sub-trajectory as
(h(i) , u(i)) := (h(i,τT) , u(i,τT) ) = (h(i-1)L+τT, u(i-1)L+τT).
Definition C.4 (Truncated sub-trajectory). Consider the state equation (1.1) and recall Definition C.1. Given
offset T and sampling rate L, for 1 ≤ i ≤ N, the ith truncated sub-trajectory states are {h(i) }NN=I where the ith
state is defined as
h = hL(i-1)+τT,L-1 .
The truncated samples are independent of each other as shown in the next lemma.
Lemma C.5. Consider the truncated states of Definition C.4. If (1.1) is generated by independent vec-
tors {ut}t≥o, for any offset T and sampling rate L, the vectors {h(i) }N==ι, {u(i) }N==ι are all independent of
each other.
Proof. By construction h(i) only depends on the vectors {u『}L=-j)+)-；+] ∙ Note that the dependence ranges
[L(i - 2) + T + 1, L(i — 1) + T — 1] are disjoint intervals for different i's; hence (h(i))N=I are independent of
each other. To show the independence of u(i) and h(i); observe that inputs u(i) = UL(i-ι)+τ have timestamp
T modulo L; which is not covered by the dependence range of (h(i))N=].	口
If the input is randomly generated, Lemma C.2 can be combined with a probabilistic bound on ht, to show that
truncated states h(i) are fairly close to the actual states h(i).
Lemma C.6 (Truncation impact - random). Given offset T and sampling rate L, consider the state vectors
Ofthe sub-trajectory {h(i)}N=ι and L — 1 -truncations (h⑸)N=ι∙ Suppose {ut}t≥o li* N(0, Ip), ∣∣A∣∣ < 1,
h0 = 0, φ is 1-Lipschitz, and φ(0) = 0. Also suppose upper bound (4.3) of Assumption 1 holds for some θ ≤
√n, γ+ > 0. There exists an absolute constant c > 0 such that with probability at least 1 — 2N exp(—100n),
for all 1 ≤ i ≤ N, the following bound holds
kh(i) — h(i)k'2 ≤ c√nkAkL-1√γ+.
In particular, we can always pick γ+ = B∞2 (via Lemma B.3).
Proof. Using Assumption 1, we can apply Lemma F.3 on vectors {h(i-2)L+τT+1}iN=1. Using a union bound,
with desired probability, all vectors obey
∣∣h(i-2)L+τ+1 — E[h(i-2)L+τ+1] k'2 ≤ (C — I)√nγ+,
for sufficiently large c. Since θ ≤ √n, triangle inequality implies ∣∣h(i-2)L+τ+1k'2 ≤ c√nγ+. Now, applying
Lemma C.2, for all 1 ≤ i ≤ N, we find
kh(i) — h (i)k'2 = kh(i-1)L+τ — h(i-1)L+T,L-1k'2
≤ ∣A∣LTkh(i-2)L+T+lk'2
≤ CkAkLT√n泮
□
D	Properties of the data matrix
This section utilizes the probabilistic estimates from Section B to provide bounds on the condition number of
data matrices obtained from the RNN trajectory (1.1). Following (2.2), these matrices H, U and X are defined
as
H =	[h1	. . .	hN]T	, U = H =	[u1	. . . uN]T , X =	[x1	. . .	xN]T.	(D.1)
The challenge is that, the state matrix H has dependent rows; which will be addressed by carefully splitting
the trajectory {ut, ht}tN=0 into multiple sub-trajectories which are internally weakly dependent as discussed in
Section C. We first define the matrices obtained from these sub-trajectories.
17
Under review as a conference paper at ICLR 2019
Definition D.1. Given sampling rate L and offset T, consider the L-subsampled trajectory {h(i), u(i)}N=ι as
described in Definitions C.3 and C.4. Define the matrices H = H(T) ∈ RN×n, H = H(T) ∈ RN×n, U =
U(T) ∈ RN×p, and X = X(T) ∈ RN×(n+p) as
H = [h ⑴...h (N)]t, H = [h ⑴... h(N)]T, U = [u(1) ... U(N)]t , X = [μH U].
Lemma D.2 (Handling perturbation). Consider the nonlinear state equation (1.1). Given sampling rate L > 0
and offset T, consider the matrices H, H, X of Definition D.1 and let Q = [γ-1/2H U] ∈ RN×(n+p).
Suppose Assumption 1 holds, φ is β-increasing, and Ut '％ N(0, Ip). There exists an absolute constant
γ2	γ2
C > 0 such that if N ≥ C Y+ (n + P), with probability 1 — 8exp(-c Y- N), for all matrices M obeying
IlM — H k ≤ 中-N, the perturbed Q matrices given by,
Q = [γ-"MU ],	(D.2)
satisfy
石T A 〜
(θ + √2)2 占 QNQ 之 2γ+.	(D.3)
Proof. This result is a direct application of Theorem F.1 after determining minimum/maximum eigenvalues of
population covariance. The cross covariance obeys E[HTU] = 0 due to independence. Also, for i > 1, the
truncated state vector h(i) is statistically identical to hL-ι hence Σ[h(i)]占 γ-In. Consequently, Σ[u(i)] = Ip,
LΣ[h(i)] W In for all i and γ-In W -1-Σ[h(i)] for all i > 1. Hence, setting qi
田h (i)
U(i)
for all i > 1
Y-In W Σ[qi] W In.
γ+
Set the matrix Q = [q2 ... qN]t and note that Q = [qi QT]t. Applying Theorem F.1 on Q and Corollary
F.2 on Q, we find that, with the desired probability,
θ + P3/2 ≥ √^kQk ≥ √^ Smin(Q) ≥ √^ Smin(Q) ≥ JN	Y- ≥ 0.99 × ∖ Y- ∙
√Ν	√Ν	√Ν	VN 3 3γ+	V 3Y+
Setting E = M — H and observing Q = Q + [y-1/2E 0], the impact of the perturbation E can be bounded
naively via Smin(Q) — γ-1∕2∣∣Ek ≤ Smin(Q) ≤ IlQk ≤ IlQk + γ-1∕2kEk∙ Using the assumed bound on
∣∣E∣, this yields
θ + √2 ≥ √^IQk ≥ √^ Smin(Q) ≥ J Y--.
√Ν	√N	V 2Y+
This final inequality is identical to the desired bound (D.3).	□
Theorem D.3 (Data matrix condition). Consider the nonlinear state-equation (1.1). Given Y+ ≥ Y- > 0,
define the condition number P = -+. For some absolute constants c,C > 0, pick a trajectory length N where
L=d1- log(CAl) e , NO=b L c≥ Cp2(n+p),
and pick scaling μ = -L=. Suppose ∣Ak < 1, Φ is β-increasing, Ut '% N(0, Ip), and Assumption
-+
1 holds with Y+ , Y- , θ, L. Matrix X = [x1 . . . xN]T of (D.1) satisfies the following with probability
1 — 4N exp(-100n) — 8L exp(-O(No∕ρ2)).
•	Each row of X has '2 norm at most C0√p + n where co is an absolute constant.
•	X T X obeys the bound
XT	XTX	I
(Θ + √2)2In + p ± -N- 士 ρ-1In + p∕2.
(D.4)
Proof. The first statement on '2-norm bound can be concluded from Lemma D.4 and holds with probability
1 — 2N exp(-100(n+p)). To show the second statement, for a fixed offset 1 ≤ T ≤ L, consider Definition D.1
and the matrices H(T), U(T), —(T). Observe that X is obtained by merging multiple sub-trajectory matrices
18
Under review as a conference paper at ICLR 2019
{X(T) }L=ι. We will first show the advertised bound for an individual X(T) by applying Lemma D.2 and then
apply Lemma A.1 to obtain the bound on the combined matrix X .
Recall that NT is the length of the τth sub-trajectory i.e. number of rows of X(τ). By construction 2No ≥
NT ≥ No for all 1 ≤ T ≤ L. Given 1 ≤ T ≤ L and triple HH⑺,H⑺,U(T), set Q = [μHH(T) U(T)]. Since
No is chosen to be large enough, applying Theorem D.2 with μ = 1/√γ+ choice, and noting P = γ+ /γ-, we
find that, with probability 1 — 4exp(-ci No/ρ2), all matrices M satisfying ∣∣M — Hi (t) k ≤ √γ-No∕10 and
Q as in (D.2) obeys
—	石T石
(Θ + √2)2 ^ QNQ 之 ρ-1∕2.
(D.5)
Let us call this Event 1. Ib proceed, we will argue that with high probability ∣∣HH(T) — HH(t) ∣ is small so that
the bound above is applicable with M = HH(T) choice; which sets Q = X(T) in (D.5). Applying Lemma C.6,
we find that, with probability 1 — 2Nt exp(-100n),
IlHH(T)- H(T)k ≤ √2Nomax{kh(i) - h⑶也} ≤ co√2No√nY+kAkL-1.
Let us call this Event 2. We will show that our choice of L ensures right hand side is small enough and guarantees
IIHH(T) — HH(T) k ≤ √γ-No∕10. Set C = max{200co, 1}. Desired claim follows by taking logarithms of
upper/lower bounds and cancelling out √zNq terms as follows
Co√nkAkL-1√Y+ ≤ √‰∕10√2 ^⇒ (L — 1) log ∣∣A∣ + log √cnρ ≤ 0
_ log CnP ≤ L
2log kAk ≤
, T l-1	log (CnP)I
U L = d1-IoiWe.
(D.6)
(D.7)
(D.8)
Here we use the fact that log kAk < 0 since kAk < 1 and CnP ≥ 0. Consequently, both Event 1 and Event 2
hold with probability 1 — 4exp(-cιNo∕ρ2) — 2Nt exp(-100n), implying (D.5) holds with Q = X(T). Union
bounding this over 1 ≤ T ≤ L, (D.5) uniformly holds with Q = X(T) and all rows of X are '2-bounded with
probability 1 — 4Nexp(-100n) — 8Lexp(-c1No∕ρ2). Applying Lemma A.1 on (X(T))L=1, we conclude
with the bound (D.4) on the merged matrix X.	□
Lemma D.4 ('2-bound on rows). Consider the setup ofTheorem D.3. With probability 1 — 2N exp(-100(n +
P)), each ro`w of X has '2 -norm at most c√p + n for some constant c > 0.
Proof. The tth row of X is equal to Xt = [√hγ= UT]t. Since ∣∣ht — E[ht]kψ2 ≤ O(√γ+) and ∣∣utkψ2 ≤
O(1), we have that kxt — E[xt]kψ2 ≤ O(1). Now, applying Lemma F.3 on all rows {xt}tN=1, and using a
union bound, with probability at least 1 — 2Nexp(-100(n + p)), we have that ∣∣xt — E[xt]k'2 ≤ c√n + p
for all t. To conclude, note that ∣E[xt] k'2 = ∣∣E[ht]k'2∕√Y+ ≤ θ ≤ 3√n via Assumption 1.	□
E	Proofs of Main Results
E.1 Proof of Lemma 3.2
The statement follows from upper bound Lemma B.3 and lower bound Lemma B.5.
E.2 Proof of Theorem 4.2
Proof. To prove this theorem, we combine Theorem D.3 with deterministic SGD convergence result of Theorem
4.1. Applying Theorem D.3, with the desired probability, inequality (D.4) holds and for all i, input data satisfies
the bound ∣∣Xik'2 ≤ ʌ/(n + p)∕(2co) for a sufficiently small constant co > 0. As the next step, we will argue
that these two events imply the convergence of SGD.
Let θ(i) , c(i) ∈ Rn+p denote the ith rows of Θ, C respectively. Observe that the square-loss is separable along
the rows of C via kΘ — Ck2F = Pin=1 kθ(i) — c(i) k`22 . Hence, SGD updates each row c(i) via its own state
equation
yt,i = φ(Dc(i), xtE),
where yt,i is the ith entry of yt. Consequently, we can establish the convergence result for an individual row
of C. Convergence of all individual rows will imply the convergence of the overall matrix ΘT to the ground
19
Under review as a conference paper at ICLR 2019
truth C. Pick a row index i (1 ≤ i ≤ n), set c = c(i) and denote ith row of Θτ by θτ . Also denote the label
corresponding to ith row by yt = yt,i. With this notation, SGD over (2.3) runs SGD over the ith row with
equations yt = φ(hc, xti) and with loss functions
N1
L(θ) = NT XLt(θ), Lt(θ) = Myt- φ(<θ, xti))2.
t=1
Substituting our high-probability bounds on xt (e.g. (D.4)) into Theorem 4.1, we can set B = (n + p)/(2c0),
γ+ = (θ + vz2)2, and Y- = ρ-1∕2. Consequently, using the learning rate η =c0 "W" +Q , for all τ ≥ 0,
the τth SGD iteration θτ obeys
E[kθτ - ck22] ≤ kθ0 - ck22 (I-
)τ
2(θ + V2)2(n + P)
(E.1)
where the expectation is over the random selection of SGD updates. This establishes the convergence for a
particular row of C. Summing up these inequalities (E.1) over all rows θτ(1), . . . , θτ(n) (which converge to
C(I),..., c(n) respectively) yields the targeted bound (4.4).	口
E.3 Proofs of main results on stable systems
E.3.1 Proof of Theorem 3.3
Proof. Applying Lemmas B.3 and 3.2, independent of L, Assumption 1 holds with parameters
Y+ =	B∞	,	Y-	=	β2Smin(B)2	, θ =	√6n -	√2	≥	√n.
This yields (θ + √2)2 = 6n. Hence, We can apply Theorem 4.2 with the learning rate η = co 6pn：：+p)where
P =___B∞______= γ+	(E2)
P	β2Smin(B)2	Y- ,	( . )
β2η
and convergence rate 1 — βγη. To conclude With the stated result, we use the change of variable co/6 → co. 口
E.3.2 Proof of Theorem 3.4
Proof. The proof is similar to that of Theorem 3.3. Applying Lemmas B.3, B.4, and 3.2, independent of L,
Assumption 1 holds with parameters
Y+ = B∞2 , Y- = smin (B)2	, θ = 0.
β2
Hence, we again apply Theorem 4.2 with the learning rate η = co2ρ⅞+p) where P is given by (E.2). Use the
change of variable co/2 → co to conclude with the stated result.	口
E.4 Learning unstable systems
In a similar fashion to Section 4, we provide a more general result on unstable systems that makes a parametric
assumption on the statistical properties of the state vector.
Assumption 2 (Well-behaved state vector - single timestamp). Given timestamp T⅛ > 0, there exists positive
scalars γ+,γ-,θ and an absolute constant C > 0 such that θ ≤ 3 ʌ/n and the following holds
Y+In 士	∑[hτo]	±	Y-In	,	∣∣hτo	一	E[hτo]kψ2 ≤ C√Y+	and	∣∣E[ht]∣∣'2 ≤	θ√Y+.	(E.3)
The next theorem provides the parametrized result on unstable systems based on this assumption.
Theorem E.1 (Unstable system - general). Suppose we are given N independent trajectories (ht(i), ut(i))t≥o
for 1 ≤ i ≤ N. Sample each trajectory at time To to obtain N samples (yi , hi , ui)iN=1 where ith sample is
(yi,hi,ui) = (h(Ti0)+1,h(Ti0),u(Ti0)).
Let C, co > 0 be absolute constants. Suppose Assumption 1 holds with To and sample size satisfies N ≥
Cρ2(n + P) where P = y+∕y- . Assume φ is β-increasing, zero initial state conditions, and Ut '吧' N(0,Ip).
β	β	β2
Set scaling to be μ = 1/λ∕Y+ and learning rate to be η = co P(θ+√β)2(n+p) ∙ StartingfrOm Θo, we run SGD
over the equations described in (2.2) and (2.3). With probability 1 — 2N exp(—100(n + P)) — 4exp(-O( N)),
all iterates satisfy
E[∣θi - CkF] ≤ (1 - co 0 2心 耍2(〜、)τkθo - CkF,
2P2(θ + 2)2(n +P)
where the expectation is over the randomness of the SGD updates.
20
Under review as a conference paper at ICLR 2019
Proof. Set Xi = [γ-1∕2hT uT]T and X = [xι ... XN]T. Since X has i.i.d. rows, We can apply Theorem
F.1 and Lemma F.3 to find with the desired probability that
•	Rows of Xi satisfy ∣∣Xi — E[xi] kψ2 ≤ O(1) and E[∣∣Xi∣∣'2] ≤ 3√n, hence all rows of X obeys
l∣Xik'2 ≤ √(n + p)∕(2co),
•	X satisfies
(θ + √2)2 ^ XNX Z ρ-1∕2.
To proceed, using Y- = ρ-1∕2, B = (n + p)∕(2co), and γ+ = (θ + vz2)2, We apply Theorem 4.1 on the loss
function (2.3); which yields the desired result.	□
E.5 Proof of Theorem 5.1
Proof. The proof is a corollary of Theorem E.1. We need to substitute the proper values in Assumption 2.
Applying Lemma B.3, we can substitute γ+ = BT。and θ = vz6n — 2≥ ≥ √n. Next, we need to find a lower
bound. Applying Lemma 3.2 for n > 1 and Lemma B.6 for n = 1, we can substitute γ- = γ+∕ρ with the ρ
definition of (5.2). With these, the result follows as an immediate corollary of Theorem E.1.	□
F	Supplementary Statistical Results
The following theorem bounds the empirical covariance of matrices with independent subgaussian rows. Given
a random vector X, define the de-biasing operation as zm(X) = X — E[X].
Theorem F.1. Let A ∈ Rn×d be a matrix with independent subgaussian rows {ai}in=1 satisfying
∣∣zm(ai) kψ2 ≤ O(K) and Σ[ai] W K2Id for some K > 0 and ∣∣E[ai] k'2 ≤ θ. Suppose Σ[ai] Z λId. Sup-
pose n ≥ O(K4d∕λ2). Then, each of the following happens with probability at least 1 — 2 exp(—cK -4 λ2 n),
•	θ + √3∕2K ≥ √nkA∣.
•	Suppose all rows of A have equal expectations. Then √n Smin (A) ≥ ,2λ∕3.
Proof. Let E = E [A], A = A — E [A],%=zm(aj We will decompose A = A + E hence we will first
focus on bounding the upper and lower singular values of A by studying the random processes Xv = ∣Avk22
and Yv = Xv — E[Xv] over the unit sphere Sd-1. First, we provide a deviation bound for the quantity
supv∈Sd-1 |Yv |. To achieve this, we will utilize Talagrand’s mixed tail bound and show that increments of Yv
are subexpoential. Pick two unit vectors v, u ∈ Rd . Write X = u + v , y = u — v . We have that
n
Xu — Xv = l∣Au∣∣22-∣∣Av∣∣22 = IlA(X + y)∕2∣22-∣A(x — y)∕2∣22 = XT √iτ Ay = X(αT χ)(以T y).
i=1
Letting X = x∕∣x∣'2 , y = y∕∣∣y∣∣'2, observe that, multiplication of subgaussians XTdi, yTα,i obey
k(XTai)(yTai)kψι ≤ O(kXk'2kyk'2K2) ≤ O(K2kyk'2).
Centering this subexponential variable around zero introduces a factor of 2 when bounding subexponential norm
and yields ∣∣(xtdi)(yTdi) — E[(xtdi)(yTa勿)]|向 ≤ O(K2∣∣y∣∣'2). Now, using the fact that YU — Yv is
sum of n independent zero-mean subexponential random variables, we have the tail bound
t2	t
P(n IYu — γvl≥t) ≤ 2exp(Tnmin{K4∣^∣-,K2∣⅛}).
Applying Talagrand,s chaining bound for mixed tail processes with distance metrics p2 = K √k'2 ,pι =
K nk'2, (Theorem 3.5 of Dirksen (2013) or Theorem 2.2.23 of Talagrand (2014)) and using the fact that for
unit sphere Sd-1, Talagrand’s γ functionals (see Talagrand (2014)) obey γ1(Sd-1), γ22(Sd-1) ≤ O(d),
n-1 SuP |YV∣ ≤ CK2(√d∕n + d∕n + t∕√n),	(F.1)
v∈Sd-1
21
Under review as a conference paper at ICLR 2019
√n kAk≤√n(
With probability 1 — 2exp(- min{t2, √nt}). Since n ≥ Cλ-2K4d for sufficiently large C > 0, picking
t = ι1cK-2λ√n, with probability 1 — 2 exp(-O(K-4λ2n)), we ensure that right hand side of(F.1)is less
than λ∕8. This leads to the following inequalities
1	λ	9K2	1	7λ
n kA	A - E[A A] k	≤ 8	=⇒	& Id	占 nA	A 占 ɪId.	(Fz
9 ι .. -..	, -,	7Γ~
=⇒ 8K ≥ √nkAk ≥ Smin(A) ≥ V8λ.
Upper bound on spectral norm: For spectral norm of A, we use the triangle inequality
IlEk + kAk) ≤ max kE[ai]忆 + 9K/8 ≤ θ + P3∕2K.
1≤i≤n
Lower bound on minimum singular value: This part assumes that all row expectations are same. Denote the
size n all ones vector by 1n and define the process Zv =言IT Av. Observe that AT 1n = Pn=1 ① ∈ Rd
is a vector satisfying ∣AT 1n∕√n∣Ψ2 ≤ O(K). Hence, again using n ≥ CK4λ-2d for sufficiently large
C > 0, applying Lemma F.3 with m = c0K-4λ2n > d by picking a sufficiently small constant c0 > 1∕C,
with probability at least 1 - 2 exp(-100c0 K -4 λ2 n)
√^ SUP IZv| = ；kAT 1nk'2 ≤ 75κκ-2λ ≤ 今.
√n kvk'2 = 1	n	12	12
Let P = In — 11n1T be the projection onto the orthogonal complement of the all ones vector. Note that
PEv = 0 as the rows of E are equal. With this observation, with desired probability, for any unit length v,
kAvk'2 ≥ kPAvk'2 = kPAvk'2 ≥ kAvk'2 - IZvI	(F.3)
≥ Smin(A) - sup |Zv ∣ ≥ (P7∕8 - 1∕12)√λn,	(F.4)
v∈Sd-1
which implies Smin(A)∕√n ≥ ,2λ∕3.	□
The corollary below is obtained by slightly modifying the proof above by using n k AT A — E [AT A] k ≤ K
in line (F.2) and only focusing on the spectral norm bound.
Corollary F.2. Let A ∈ Rn×d be a matrix with independent {ai}in=1 subgaussian rows satisfying
∣∣zm(ai) kψ2 ≤ O(K) and Σ[ai] W K 2Id for some K > 0 and kE[a"k'2 ≤ θ ∙ Suppose Σ[ai]占 λId.
Suppose n ≥ O(K2d). Then, with probability at least 1 - 4 exp(-cK -2 n),
θ + P3∕2K ≥
√n kAk∙
The following lemma is fairly standard and is proved for the sake of completeness.
Lemma F.3 (Subgaussian vector length). Let a ∈ Rn be a zero-mean subgaussian vector with kakψ2 ≤ L.
Then, for any m ≥ n, there exists C > 0 such that
P(kak'2 ≤ CL√m) ≥ 1 — 2exp(-100m).
Proof. We can pick a 1/2 cover C of the unit '2-sphere with size log ∣C∣ ≤ 2n. For any V ∈ C, subgaussianity
implies, P(IvTa∣ ≥ t) ≤ 2exp(-∙2L■). Setting t = CL√m for sufficiently large constant C > 0, and union
bounding over all v ∈ C, we find
P( \ ∣∣vk'2 ≤ CL√m) ≥ 1 — 2 exp(2n —
v∈C
To conclude, let v(a) ∈ C be a’s neighbor satisfying kv -
cC2 L2 m
CL ) ≤ 1 - 2exp(-100m).
2L
ɪ-a- k'9 ≤ 1/2. Hence, we have
ak`2	`2	,
∣∣ak'2	≤	k(a	- v(a))Tak'2	+	∣∣vTak'2	≤ ∣∣ak'2/2 + CL√m	=⇒	∣∣ak'2 ≤	2CL√m.
To conclude, use the change of variable C → C/2.
□
22