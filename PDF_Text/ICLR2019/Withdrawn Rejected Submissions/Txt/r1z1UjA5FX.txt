Under review as a conference paper at ICLR 2019
Adversarial Defense via Data Dependent Ac-
tivation Function and Total Variation Mini-
MIZATION
Anonymous authors
Paper under double-blind review
Ab stract
We improve the robustness of deep neural nets to adversarial attacks by using
an interpolating function as the output activation. This data-dependent activation
function remarkably improves both classification accuracy and stability to adver-
sarial perturbations. Together with the total variation minimization of adversar-
ial images and augmented training, under the strongest attack, we achieve up to
20.6%, 50.7%, and 68.7% accuracy improvement with respect to the fast gradient
sign method, iterative fast gradient sign method, and Carlini-Wagner L2 attacks,
respectively. Our defense strategy can be added to many of the existing meth-
ods. We give an intuitive explanation of our defense strategy via analyzing the
geometry of the feature space. For reproducibility, the code will be available on
GitHub.
1	Introduction
The adversarial vulnerability (Szegedy et al., 2013) of deep neural nets (DNNs) threatens their ap-
plicability in security critical tasks, e.g., autonomous cars (Akhtar & Mian, 2018), robotics (Guisti
et al., 2016), DNN-based malware detection systems (Papernot et al., 2016b; Grosse et al., 2016).
Since the pioneering work by Szegedy et al. (2013), many advanced adversarial attack schemes
have been devised to generate imperceptible perturbations to sufficiently fool the DNNs (Goodfel-
low et al., 2014; Papernot et al., 2016a; Carlini & Wagner, 2016; Wu et al., 2018; Ilyas et al., 2018;
Athalye et al., 2018b). And not only are adversarial attacks successful in white-box attacks, i.e.
when the adversary has access to the DNN parameters, but they are also successful in black-box
attacks, i.e. it has no access to the parameters. Black-box attacks are successful because one can
perturb an image so it misclassifies on one DNN, and the same perturbed image also has a significant
chance to be misclassified by another DNN; this is known as transferability of adversarial examples
(Papernot et al. (2016d)). Due to this transferability, it is very easy to attack neural nets in a black-
box fashion (Liu et al., 2016; Brendel et al., 2017). In fact, there exist universal perturbations that
can imperceptibly perturb any image and cause misclassification for any given network (Moosavi-
Dezfooli et al. (2017)). There is much recent research on designing advanced adversarial attacks
and defending against adversarial perturbation.
In this work, we propose to defend against adversarial attacks by changing the DNNs’ output acti-
vation function to a manifold-interpolating function, in order to seamlessly utilize the training data’s
information when performing inference. Together with the total variation minimization (TVM) and
augmented training, we show state-of-the-art defense results on the CIFAR-10 benchmark. More-
over, we show that adversarial images generated from attacking the DNNs with an interpolating
function are more transferable to other DNNs, than those resulting from attacking standard DNNs.
2	Related Work
Defensive distillation was recently proposed to increase the stability of DNNs which dramatically
reduces the success rate of adversarial attacks (Papernot et al., 2016c), and a related approach (Tramr
et al. (2018)) cleverly modifies the training data to increase robustness against black-box attacks, and
adversarial attacks in general. To counter the adversarial perturbations, Guo et al. (2018) proposed to
1
Under review as a conference paper at ICLR 2019
use image transformation, e.g., bit-depth reduction, JPEG compression, TVM, and image quilting.
A similar idea of denoising the input was later explored by Moosavi-Dezfooli et al. (2018), where
they divide the input into patches, denoise each patch, and then reconstruct the image. These input
transformations are intended to be non-differentiable, thus making adversarial attacks more difficult,
especially for gradient-based attacks. Song et al. (2018) noticed that small adversarial perturbations
shift the distribution of adversarial images far from the distribution of clean images. Therefore they
proposed to purify the adversarial images by PixelDefend. Adversarial training is another family of
defense methods to improve the stability of DNNs (Goodfellow et al., 2014; Mardy et al., 2018; Na
et al., 2018). And GANs are also employed for adversarial defense (Samangouei et al., 2018). In
(Athalye et al., 2018a), the authors proposed a straight-through estimation of the gradient to attack
the defense methods that is based on the obfuscated gradient. Meanwhile, many advanced attack
methods have been proposed to attack the DNNs (Wu et al., 2018; Ilyas et al., 2018).
Instead of using softmax functions as the DNNs’ output activation, Wang et al. (2018) utilized a
class of non-parametric interpolating functions. This is a combination of both deep and manifold
learning which causes the DNNs to sufficiently utilize the geometric information of the training data.
The authors show a significant amount of generalization accuracy improvement, and the results are
more stable when one only has a limited amount of training data.
3	Deep Neural Nets with Data-Dependent Activation Function
In this section, we summarize the architecture, training, and testing procedures of the DNNs with the
data-dependent activation (Wang et al., 2018). An overview of training and testing of the standard
DNNs with softmax output activation is shown in Fig. 1 (a) and (b), respectively. In the kth iteration
of training, given a mini-batch of training data X, Y , the procedure is:
Forward propagation: Transform X into features by a DNN block (ensemble of convolutional lay-
ers, nonlinearities and others), and then pass this output through the softmax activation to obtain the
predictions Y :
Y = SoftmaX(DNN(X, Θk-1), WkT).
Then the loss is computed (e.g., cross entropy) between Y and Y: L = Loss(Y, Y).
Backpropagation: Update weights (Θk-1, Wk-1) by gradient descent (learning rate γ):
〜
Wk = WkT-Ydf ∙ ∂γ, θk = θk-1
∂L	∂ Yf
—Y•
∂ Yf ∂Xf
〜
∂ X
∂Θ.
Once the model is optimized, the predicted labels for testing data X are:
Yf= SoftmaX(DNN(X, Θ), W).
Wang et al. (2018) proposed to replace the data-agnostic softmax activation by a data-dependent
interpolating function, defined in the next section.
(X,Y)		X		(X,Y),(Xte,Yie)		X,(Xte,Yie)	
				(X, X*e) = DNNg X加,Θ)			
X = DVN(X, Θ)							
I t =SOftmaH区,W) I 1 LOSSMW)		文= DNNx θ)		(X,⅛e) = Buffer(X5Xie5W5)		I	(X, Xfe) = ONN(X, Xfe, Θ)	
				I q = Linear区、WL) 卜—卜= WNZYjt 刘1丫写 ]			
		Y = Softma工区,W)		I	I I	I I I I I I	-055(V.Y)	I	I ^∙S.s(Y,Y)		Y = WN LL(X,Xte, Yte)	
(a)	(b)	(c)	(d)						
Figure 1: Training and testing procedures of the DNNs with softmax and WNLL functions as the last
activation layer. (a) and (b) show the training and testing steps for the standard DNNs, respectively;
(c) and (d) illustrate the training and testing procedure of the WNLL activated DNNs, respectively.
2
Under review as a conference paper at ICLR 2019
3.1 Manifold Interpolation - A Harmonic Extension Approach
Let X = {xι, x2, ∙…，Xn} be a set of points in a high dimensional manifold M ⊂ Rd and
Xte = {x1e, x2e,…，Xm} be a subset of X which are labeled with label function g(x). We want
to interpolate a function u that is defined on the entire manifold and can be used to label the entire
dataset X. The harmonic extension is a natural and elegant approach to find such an interpolating
function, which is defined by minimizing the Dirichlet energy functional:
E(u) = 2 X w(χ, y)(u(χ)- u(y))2,	(1)
x,y∈X
with the boundary condition:
u(χ) = g(χ), χ ∈ Xte,
where w(x, y) is a weight function, typically chosen to be Gaussian: w(x, y) = exp(一 ||x[y|| )
with σ being a scaling parameter. The Euler-Lagrange equation for Eq. (1) is:
(uP(xy∈)X=(w(x, y) + w(y, x)) (u(x) 一 u(y)) =0 x ∈ X/Xte	(2)
g(x)	x ∈ Xte .
By solving the linear system (Eq. (2)), we obtain labels u(x) for unlabeled data x ∈ X/Xte. This
interpolation becomes invalid when the labeled data is tiny, i.e., |Xte | |X/Xte |. To resolve this
issue, the weights of the labeled data is increased in the Euler-Lagrange equation, which gives:
(py∈x(W(X, y) + w(y, X))(U(X) - u(y)) +
< (1X¾ 一 1)Py∈χte w(y, X)(U(X) - u(y)) = 0 X ∈ X∕Xte	(3)
Iu(X) = g(X)	X ∈ Xte.
The solution U(X) to Eq. (3) is named weighted nonlocal Laplacian (WNLL), denoted as
WNLL(X, Xte, Yte). For classification tasks, g(X) is the one-hot labels for the example X.
3.2 Training and Testing the DNNs with Data-Dependent Activation Function
In both training and testing of the WNLL-activated DNNs, we need to reserve a small portion of
data/label pairs, denoted as (Xte, Yte), to interpolate the label for new data Y. We name the re-
served data (Xte, Yte) as the template. Directly replacing softmax by WNLL has difficulties in
back propagation, namely the true gradient ∂L is difficult to compute since WNLL defines a very
complex implicit function. Instead, to train WNLL-activated DNNs, a proxy via an auxiliary neural
net (Fig.1(c)) is employed. On top of the original DNNs, we add a buffer block (a fully connected
layer followed by a ReLU), and followed by two parallel branches, WNLL and the linear (fully
connected) layers. The auxiliary DNNs can be trained by alternating between training DNNs with
linear and WNLL activations, respectively. The training loss of the WNLL activation function is
backpropped via a straight-through estimation approach (Athalye et al., 2018a; Bengio et al., 2013).
At test time, we remove the linear classifier from the neural nets and use the DNN and buffer blocks
together with WNLL to predict new data (Fig. 1 (d)); here for simplicity, we merge the buffer block
to the DNN block. For a given set of testing data X, and the labeled template {(Xte, Yte)}, the
predicted labels for X is given by
Y = WNLL(DNN(X, Xte, Θ), Yte).
4 Adversarial Attacks
We consider three benchmark attack methods in this work, namely, the fast gradient sign method
(FGSM) (Goodfellow et al., 2014), iterative FGSM (IFGSM) (Kurakin et al., 2016), and Carlini-
Wagner’s L2 (CW-L2) (Carlini & Wagner, 2016) attacks. We denote the classifier defined by the
DNNs with softmax activation as y = f (θ, x) for a given instance (x, y). FGSM finds the adversarial
image x0 by maximizing the loss L(x0, y), subject to the l∞ perturbation ||x0-x∣∣∞ ≤ E with E as the
3
Under review as a conference paper at ICLR 2019
attack strength. Under the first order approximation i.e., L(x0, y) = L(x, y) + NxLg y)T ∙ (x0 -x),
the optimal perturbation is given by
X = X + E sign ∙ (VxL(x, θ)).	(4)
IFGSM iterates FGSM to generate enhanced adversarial images, i.e.,
x(m) = XgT) + 〜sign (Vx(m-1) L(X(mT),y)) ,	(5)
where m = 1, ∙∙∙ , M, X(O) = X and x0 = X(M), with M be the number of iterations.
The CW-L2 attack is proposed to circumvent defensive distillation. For a given image-label pair
(X, y ), and ∀t 6= y, CW-L2 searches the adversarial image that will be classified to class t by
solving the optimization problem:
min ∣∣δ∣∣2,	subject to f (x + δ) = t, X + δ ∈ [0,1]n,	(6)
δ
where δ is the adversarial perturbation (for simplicity, we ignore the dependence of θ in f).
The equality constraint in Eq. (6) is hard to satisfy, so instead Carlini et al. consider the surrogate
g(X) = max max(Z(X)i) - Z(X)y, 0 ,	(7)
i6=y
where Z(X) is the logit vector for an input X, i.e., output of the neural net before the softmax layer.
Z(X)i is the logit value corresponding to class i. It is easy to see that f(X + δ) = t is equivalent to
g(X + δ) ≤ 0. Therefore, the problem in Eq. (6) can be reformulated as
min ∣∣δ∣∣2 + C ∙ g(X + δ)	subject to X + δ ∈ [0,1]n,	(8)
δ
where c ≥ 0 is the Lagrangian multiplier.
By letting δ = 2 (tanh(w) + 1) - x, Eq. (8) can be converted to an unconstrained optimization
problem. Moreover, Carlini et al. introduce the confidence parameter κ into the above formulation.
Above all, CW-L2 attacks seek adversarial images by solving the following problem
min ||； (tanh(w) + 1) — X∣∣2 + C ∙ max (一κ, max(Z(ɪ(tanh(w)) + 1)i) — Z(ɪ(tanh(w)) + 1)y) . (9)
This unconstrained optimization problem can be solved efficiently by the Adam optimizer (Kingma
& Ba, 2014). All three of the attacks clip the values of the adversarial image X0 to between 0 and 1.
4.1	Adversarial Attack for DNNs with WNLL Activation Function
In this work, we focus on untargeted attacks and defend against them. For a given small batch
of testing images (X, Y) and template (Xte, Yte), we denote the DNNs modified with WNLL as
output activation as Y = WNLL(Z({X, Xte}), Yte), where Z({X, Xte}) is the composition of
the DNN and buffer blocks as shown in Fig. 1 (c). By ignoring dependence of the loss function on the
parameters, the loss function for DNNs with WNLL activation can be written as L(X, Y, Xte, Yte).
The above attacks for DNNs with WNLL activation on the batch of images, X, are formulated below.
•	FGSM
X0 = X + 〜sign (VXL(X, Y, Xte, Yte)).
•	IFGSM
X(m) = X(m-1) + 〜sign(Vx(m-i)L(X(m-1), Y, Xte, Yte)),
where m = 1,2,…，N; X(O) = X and X0 = X(M).
•	CW-L2
mWn || 2 (tanh(W) + 1)- X||2+
(10)
(11)
(12)
c ∙ max - —κ, max(Z(U(tanh(W)) + 1)i) — Z(U(tanh(W)) + 1)γ
i6=Y	2	2
where i is the logit values of the input images X.
4
Under review as a conference paper at ICLR 2019
Based on our numerical experiments, the batch size of X has minimal influence on the adversarial
attack and defense. In all of our experiments we choose the batch size of X to be 500. Similar to
Wang et al. (2018), we choose the size of the template to be 500.
We apply the above attack methods to ResNet-56 (He et al., 2016) with either softmax or WNLL
as the output activation function. For IFGSM, we run 10 iterations of Eqs. (5) and (11) to attack
DNNs with two different output activations, respectively. For CW-L2 attacks (Eqs. (9, 12)) in both
scenarios, we set the parameters c = 10 and κ = 0. Figure 2 depicts three randomly selected images
(horse, automobile, airplane) from the CIFAR-10 dataset, their adversarial versions by different
attack methods on ResNet-56 with two kinds of activation functions, and the TV minimized images.
All attacks successfully fool the classifiers to classify any of them correctly. Figure 2 (a) shows
that FGSM and IFGSM with perturbation = 0.02 changes the contrast of the images, while it
is still easy for humans to correctly classify them. The adversarial images of the CW-L2 attacks
are imperceptible, however they are extremely strong in fooling DNNs. Figure 2 (b) shows the
images of (a) with a stronger attack, = 0.08. With a larger , the adversarial images become more
noisy. The TV minimized images of Fig. 2 (a) and (b) are shown in Fig. 2 (c) and (d), respectively.
The TVM removes a significant amount of detailed information from the original and adversarial
images, meanwhile it also makes it harder for humans to classify both the TV-minimized version of
the original and adversarial images. Visually, it is hard to discern the adversarial images resulting
from attacking the DNNs with two types of output layers.
(a)
(d)
(b)	(c)
Figure 2:	Samples from CIFAR-10. Panel (a): from the top to the last rows show the original,
adversarial images by attacking ResNet-56 with FGSM, IFGSM, CW-L2 ( = 0.02); and attacking
the ResNet-56 with WNLL as output activation. Panel (b) corresponding to those in panel (a) with
= 0.08. Charts (c) and (d) corresponding to the TV minimized images in (a) and (b), respectively.
5 Analysis of the Geometry of Features
We consider the geometry of features of the original and adversarial images. We randomly select
1000 training and 100 testing images from the airplane and automobile classes, respectively. We
consider two visualization strategies for ResNet-56 with softmax activation: (1) extract the original
64D features output from the layer before the softmax, and (2) apply the principle component anal-
ysis (PCA) to reduce them to 2D. However, the principle components (PCs) do not encode the entire
geometric information of the features. Alternatively, we add a 2 by 2 fully connected (FC) layer
before the softmax, then utilize the 2D features output from this newly added layer. We verify that
the newly added layer does not change the performance of ResNet-56 as shown in Fig. 3, and that
the training and testing performance remains essentially the same for these two cases.
Figure 4 (a) and (b) show the 2D features generated by ResNet-56 with additional FC layer for the
original and adversarial testing images, respectively, where we generate the adversarial images by
using FGSM ( = 0.02). Before adversarial perturbation (Fig. 4 (a)), there is a straight line that
can easily separate the two classes. The small perturbation causes the features to overlap and there
is no linear classifier that can easily separate these two classes (Fig. 4 (b)). The first two PCs of
the 64D features of the clean and adversarial images are shown in Fig. 4 (c) and (d), respectively.
5
Under review as a conference paper at ICLR 2019
(a)
(b)
Figure 3:	Training and testing epochs v.s. accuracy of ResNet-56 on CIFAR-10. (a): without the
additional FC layer; (b): with the additional FC layer.
Again, the PCs are well separated for clean images, while adversarial perturbation causes overlap
and concentration.
The bottom charts of Fig. 4 depict the first two PCs of the 64D features output from the layer
before the WNLL. The distributions of the unperturbed training and testing data are the same, as
illustrated in panels (e) and (f). The new features are better separated which indicates that DNNs
with WNLL are more robust to small random perturbation. Panels (g) and (h) plot the features of the
adversarial and TV minimized adversarial images in the test set. The adversarial attacks move the
automobiles’ features to the airplanes’ region and TVM helps to eliminate the outliers. Based on our
computation, most of the adversarial images of the airplane classes can be correctly classified with
the interpolating function. The training data guides the interpolating function to classify adversarial
images correctly. The fact that the adversarial perturbations change the features’ distribution was
also noticed in (Song et al., 2018).
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
Figure 4:	Visualization of the features learned by DNNs with softmax ((a), (b), (c), (d)) and WNLL
((e), (f), (g), (h)) activation functions. (a) and (b) plot the 2D features of the original and adversarial
testing images; (c) and (d) are the first two principle components of the 64D features for the original
and adversarial testing images, respectively. Charts (e), (f) plot the first two components of the
training and testing features learned by ResNet-56 with WNLL activation; (g) and (h) show the two
principle components of the adversarial images and TV minimized adversarial images for the test
set.
6	Adversarial Defense by Interpolating Function and TVM
To defend against adversarials, we combine the ideas of data-dependent activation, input transfor-
mation, and training data augmentation. We train ResNet-56, respectively, on the original training
data, the TV minimized training data, and a combination of the previous two. On top of the data-
dependent activation output and augmented training, we further apply the TVM (Rudin et al., 1992)
used by Guo et al. (2018) to transform the adversarial images to boost defensive performance. The
basic idea is to reconstruct the simplest image z from the sub-sampled image, X x, with X the
mask filled by a Bernoulli binary random variable, by solving the following TVM problem
min ||(1 — X) Θ (Z - x)∣∣2 + λτv ∙ TV2(z),
z
6
Under review as a conference paper at ICLR 2019
Table 1: Mutual classification accuracy on the adversarial images resulting from attacking ResNet-56 with
the softmax and the WNLL activation functions.
Attack Method
Training data	= 0.02	= 0.04	= 0.06	= 0.08
= 0.1
Classification accuracy of ResNet-56 with softmax on adversarial images produced by attacking ResNet-56 with WNLL
FGSM	Original data	59.6	59.5	58.0	56.3	54.3
FGSM	TVM data	50.7	40.6	41.2	37.4	34.5
FGSM	Original + TVM data	62.9	61.7	60.6	59.4	58.9
IFGSM	Original data	49.1	43.6	40.4	36.8	34.8
IFGSM	TVM data	30.3	23.7	20.1	18.0	17.3
IFGSM	Original + TVM data	53.9	49.2	44.7	41.9	39.9
CW-L2	Original data	54.7	54.2	54.4	53.8	54.0
CW-L2	TVM data	59.8	59.5	58.7	59.8	59.1
CW-L2	Original + TVM data	81.5	81.5	81.8	81.2	81.5
Classification accuracy of ResNet-56 with WNLL on			adversarial images produced by attacking ResNet-56 with SoftmaX			
FGSM	Original data	65.4	65.9	63.6	61.7	60.5
FGSM	TVM data	61.5	56.7	50.8	44.7	41.0
FGSM	Original + TVM data	69.7	67.6	65.5	64.8	63.4
IFGSM	Original data	51.9	43.9	38.9	35.4	34.2
IFGSM	TVM data	32.1	22.8	19.5	17.8	16.1
IFGSM	Original + TVM data	60.0	53.0	47.5	41.6	38.4
CW-L2	Original data	81.5	81.4	81.5	81.6	81.4
CW-L2	TVM data	57.6	58.4	57.8	58.4	58.4
CW-L2	Original + TVM data	90.6	90.6	90.5	90.1	90.4
where λT V > 0 is the regularization constant.
7	Numerical Results
7.1	Transferability of the Adversarial Images
To verify the efficacy of attack methods for DNNs with WNLL output activation, we consider the
transferability of adversarial images. We train ResNet-56 on the aforementioned three types of train-
ing data with either softmax or WNLL activation. After the DNNs are trained, we attack them by
FGSM, IFGSM, and CW-L2 with different . Finally, we classify the adversarial images by using
ResNet-56 with the opponent activation. We list the mutual classification accuracy on adversarial
images in Table. 1. The adversarial images resulting from attacking DNNs with two types of activa-
tion functions are both transferable, as the mutual classification accuracy is significantly lower than
testing on the clean images. Overall we see a remarkably higher accuracy when applying ResNet-56
with WNLL activation to classify the adversarial images resulting from attacking ResNet-56 with
softmax activation. For instance, for DNNs that are trained on the original images and attacked
by FGSM, DNNs with the WNLL classifier have at least 5.4% higher accuracy (56.3% v.s. 61.7%
( = 0.08)). The accuracy improvement is more significant in many other scenarios.
7.2	Adversarial Defense
Figure 5 plots the result of adversarial defense by combining the WNLL activation, TVM, and
training data augmentation. Panels (a), (b) and (c) show the testing accuracy of ResNet-56 with
and without defense on CIFAR-10 data for FGSM, IFGSM, and CW-L2, respectively. It can be
observed that with increasing attack strength, , the testing accuracy decreases rapidly. FGSM is
a relatively weak attack method, as the accuracy remains above 53.5% ( = 0.1) even with the
strongest attack. Meanwhile, the defense maintains accuracy above 71.8% ( = 0.02). Figure 5
(b) and (c) show that both IFGSM and CW-L2 can fool ResNet-56 near completely even with small
. The defense maintains the accuracy above 68.0%, 57.2%, respectively, under the CW-L2 and
IFGSM attacks. Compared to state-of-the-art defensive methods on CIFAR-10, PixelDefend, our
method is much simpler and faster. Without adversarial training, we have shown our defense is more
7
Under review as a conference paper at ICLR 2019
Table 2: Testing accuracy of ResNet-56 on the adversarial/TVM CIFAR-10 dataset. The testing
accuracy without any defense are in red italic; and the results with all three defenses are in boldface.
Attack Method	Training data	=0	e = 0.02	=0.04	= 0.06	=0.08	= 0.1
Vanilla ResNet-56							
FGSM	Original data	93.0	60.4/39.4	60.3/39.4	58.2/40.2	55.8/30.9	53.5/40.1
FGSM	TVM data	88.3	54.1/39.6	49.5/41.6	43.6/44.3	39.5/45.1	35.9/45.0
FGSM	Original + TVM data	93.1	63.2/66.6	62.7/67.8	62.4/68.7	62.0/68.1	61.3/68.7
IFGSM	Original data	93.0	20.6/35.0	11.6/32.3	8.6/31.0	7.5/28.8	6.5/27.6
IFGSM	TVM data	88.3	10.3/32.9	6.7/31.1	6.1/31.7	6.1/30.8	6.0/29.2
IFGSM	Original + TVM data	93.1	32.1/61.5	24.5/57.4	20.1/54.1	17.1/51.3	15.9/48.9
CW-L2	Original data	93.0	4.7/36.8	3.5/36.4	0/36.8	0/36.8	0/35.9
CW-L2	TVM data	88.3	8.2/36.5	8.1/36.0	8.0/35.9	8.0/35.8	8.0/36.3
CW-L2	Original + TVM data	93.1	13.6/62.2	13.6/62.2	13.0/62.1	12.0/62.1	12.0/61.9
Data-DePendent Activated ReSNet-56							
FGSM	Original data	94.5	71.1/49.9	72.1/51.1	71.3/51.7	70.6/52.2	67.3/51.8
FGSM	TVM data	90.6	62.6/49.3	56.8/54.1	52.1/56.2	46.0/56.6	41.0/57.1
FGSM	Original + TVM data	94.7	70.6/71.8	68.8/73.1	67.2/74.9	66.9/73.6	63.7/74.1
IFGSM	Original data	94.5	43.7/44.7	35.3/42.1	31.3/39.5	28.2/37.8	27.0/35.5
IFGSM	TVM data	90.6	12.1/44.3	7.1/41.1	7.2/37.4	6.9/37.2	6.8/35.3
IFGSM	Original + TVM data	94.7	35.0/67.4	25.1/64.9	20.5/61.9	17.5/58.7	16.3/57.2
CW-L2	Original data	94.5	11.9/40.1	11.7/40.8	11.0/40.8	10.8/41.2	10.8/40.5
CW-L2	TVM data	90.6	52.6/48.5	52.7/48.4	52.2/45.8	52.8/47.7	51.9/44.8
CW-L2	Original + TVM data	94.7	61.6/68.6	61.1/68.0	61.9/68.1	61.2/69.2	61.5/68.7
stable to IFGSM, and more stable to all three attacks under the strongest attack than PixelDefend
Song et al. (2018). Moreover, our defense strategy is additive to adversarial training and many other
defenses including PixelDefend.
(a)
(c)
Figure 5: Attack strength v.s. accuracy without defense, and defending by WNLL activation, TVM
and augmented training. (a), (b), (c) plot results for FGSM, IFGSM, and CW-L2 attack, respectively.
To analyze the defensive contribution from each component of the defensive strategy, we separate the
three parts and list the testing accuracy in Table. 2. Simple TVM cannot defend FGSM attacks except
when the DNNs are trained on the augmented data, as shown in the first and fourth horizontal blocks
of the table. WNLL activation improves the testing accuracy of adversarial attacks significantly and
persistently. Augmented training can improve the stability consistently as well.
8 Concluding Remarks
In this paper, by analyzing the influence of adversarial perturbations on the geometric structure of the
DNNs’ features, we propose to defend against adversarial attack by applying a data-dependent acti-
vation function, total variation minimization on the adversarial images, and training data augmenta-
tion. Results on ResNet-56 with CIFAR-10 benchmark reveal that the defense improves robustness
to adversarial perturbation significantly. Total variation minimization simplifies the adversarial im-
ages, which is very useful in removing adversarial perturbation. Another interesting direction to
explore is to apply other denoising methods to remove adversarial perturbation.
8
Under review as a conference paper at ICLR 2019
Acknowledgments
Use unnumbered third level headings for the acknowledgments. All acknowledgments, including
those to funding agencies, go at the end of the paper.
References
N. Akhtar and A. Mian. Threat of adversarial attacks on deep learning in computer vision: A survey.
arXiv Preprint arXiv:1801.00553, 2018.
A. Athalye, N. Carlini, and D. Wagner. Obfuscated gradients give a false sense of security: Cir-
CUmventing defenses to adversarial examples. InternationaI Conference on Machine Learning,
2018a.
A. Athalye, L. Engstrom, A. Ilyas, and K. Kwok. Synthesizing robust adversarial examples.
InternationaI Conference on Machine Learning, 2018b.
Y. Bengio, N. Leonard, and A. CoUrville. Estimating or propagating gradients throUgh stochastic
neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.
W. Brendel, J. RaUber, and M. Bethge. Decision-based adversarial attacks: Reliable attacks against
black-box machine learning models. arXiv preprint arXiv:1712.04248, 2017.
N. Carlini and D.A. Wagner. Towards evaluating the robustness of neural networks. IEEE EUrOpean
SympOSiUm on SecUrity and Privacy, pp. 39-57, 2016.
I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. arXiv
preprintarXiv:1412.6275, 2014.
K. Grosse, N. Papernot, P. Manoharan, M. Backes, and P. McDaniel. Adversarial perturbations
against deep neural networks for malware classification. arXiv preprint arXiv:1606.04435, 2016.
A. Guisti, J. Guzzi, D.C. Ciresan, F.L. He, J.P. Rodriguez, F. Fontana, M. Faessler, C. Forster,
J. Schmidhuber, G. Di Carlo, and et al. A machine learning approach to visual perception of
forecast trails for mobile robots. IEEE Robotics and AUtOmatiOn Letters, pp. 661-667, 2016.
C.	Guo, M. Cisse, and L. van der Maaten. Countering adversarial images using input transformation.
International COnference on Learning RepreSentations, 2018.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, pp.
770-778, 2016.
A. Ilyas, L. Engstrom, A. Athalye, and J. Lin. Black-box adversarial attacks with limited queries
and information. International COnference on Machine Learning, 2018.
D.	Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,
2014.
A. Kurakin, I. J. Goodfellow, and S. Bengio. Adversarial examples in the physical world. arXiv
preprint arXiv:1607.02533, 2016.
Y. Liu, X. Chen, C. Liu, and D. Song. Delving into transferable adversarial examples and black-box
attacks. arXiv preprint arXiv:1611.02770, 2016.
A. Mardy, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models
resistant to adversarial attacks. International COnference on Learning RepreSentations, 2018.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal
adversarial perturbations. In The IEEE COnference on COmpUter ViSiOn and Pattern RecOgnitiOn
(CVPR), July 2017.
Seyed-Mohsen Moosavi-Dezfooli, Ashish Shrivastava, and Oncel Tuzel. Divide, denoise, and de-
fend against adversarial attacks. CoRR, abs/1802.06806, 2018. URL http://arxiv.org/
abs/1802.06806.
9
Under review as a conference paper at ICLR 2019
T. Na, J. H. Ko, and S. Mukhopadhyay. Cascade adversarial machine learning regularized with a
unified embedding. Intemational Conference on Learning Representations, 2018.
N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z.B. Celik, and A. Swami. The limitations of
deep learning in adversarial settings. IEEE EUropean SymposiUm on SecUrity and Privacy, pp.
372-387, 2016a.
N. Papernot, P. McDaniel, A. Sinha, and M. Wellman. Sok: Towards the science of secUrity and
privacy in machien learning. arXiv preprint arXiv:1611.03814, 2016b.
N. Papernot, P. McDaniel, X. WU, S. Jha, and A. Swami. Distillation as a defense to adversarial
perturbations against deep neural networks. IEEE European Symposium on Security and Privacy,
2016c.
Nicolas Papernot, Patrick D. McDaniel, and Ian J. Goodfellow. Transferability in machine learning:
from phenomena to black-box attacks using adversarial samples. CoRR, abs/1605.07277, 2016d.
URL http://arxiv.org/abs/1605.07277.
L. Rudin, S. Osher, and E. Fatemi. Nonlinear total variation based noise removal algorithms. Physica
D: nonlinear phenomena, pp. 259-268, 1992.
P. Samangouei, M. Kabkab, and R. Chellappa. Defense-gan: Protecting classifiers against adversaial
attacks using generative models. International Conference on Learning Representations, 2018.
Y. Song, T. Kim, S. Nowozin, S. Ermon, and N. Kushman. Pixeldefend: Leveraging generative mod-
els to understand and defend against adversarial examples. Intemational Conference on Learning
Representations, 2018.
C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, and I. Goodfellow. Intriguing properties
of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Florian Tramr, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick Mc-
Daniel. Ensemble adversarial training: Attacks and defenses. In Intemational Conference
on Learning Representations, 2018. URL https://openreview.net/forum2id=
rkZvSe-RZ.
B. Wang, X. Luo, Z. Li, W. Zhu, Z. Shi, and S. Osher. Deep neural nets with interpolating function
as output activation. AdVances in NeUraI Information Processing Systems, 2018.
X. Wu, U. Jang, J. Chen, L. Chen, and S. Jha. Reinforcing adversarial robustness using model
confidence induced by adversarial training. InternationaI Conference on Machine Learning, 2018.
10