Under review as a conference paper at ICLR 2019
ON BREIMANâ€™S DILEMMA IN NEURAL NETWORKS: SUCCESS AND FAILURE OF NORMALIZED MARGINS
Anonymous authors
Paper under double-blind review
ABSTRACT
A belief persists long in machine learning that enlargement of margins over trainï¿¾ing data accounts for the resistance of models to overfitting by increasing the
robustness. Yet Breiman shows a dilemma (Breiman, 1999) that a uniform imï¿¾provement on margin distribution does not necessarily reduces generalization erï¿¾ror. In this paper, we revisit Breimanâ€™s dilemma in deep neural networks with
recently proposed normalized margins using Lipschitz constant bound by spectral
norm products. With both simplified theory and extensive experiments, Breimanâ€™s
dilemma is shown to rely on dynamics of normalized margin distributions, that re-
flects the trade-off between model expression power and data complexity. When
the complexity of data is comparable to the model expression power in the sense
that training and test data share similar phase transitions in normalized margin
dynamics, Rademacher complexities of Lipschitz-normalized networks can be reï¿¾garded as small constants and two efficient ways are derived via classic marginï¿¾based generalization bounds to successfully predict the trend of generalization
error. On the other hand, over-expressed models that exhibit uniform improveï¿¾ments on training normalized margins may lose such a prediction power and fail
to prevent the overfitting.
1 INTRODUCTION
Margin, as a measurement of the robustness allowing some perturbations on classifier without changï¿¾ing its decision on training data, has a long history in characterizing the performance of classification
algorithms in machine learning. As early as Novikoff (1962), it played a central role in the proof on
finite-stopping or convergence of perceptron algorithm when training data is separable. Equipped
with convex optimization technique, a plethora of large margin classifiers are triggered by supï¿¾port vector machines (Cortes & Vapnik, 1995; Vapnik, 1998). AdaBoost, an iterative algorithm to
combine an ensemble of classifiers proposed by Freund & Schapire (1997), often exhibits a resisï¿¾tance to overfitting phenomenon that during the training process the generalization error keeps on
non-increasing when the training error drops to zero. Toward deciphering the such a resistance of
overfitting phenomenon, Schapire et al. (1998) proposed an explanation that the training process
keeps on improving a notion of classification margins in boosting, among later works on consisï¿¾tency of boosting with early stopping regularization (BÂ¨uhlmann & Yu, 2002; Zhang & Yu, 2005;
Yao et al., 2007). Lately such a resistance to overfitting is again observed in deep neural networks
with overparameterized models (Zhang et al., 2016). A renaissance of margin theory is proposed by
Bartlett et al. (2017) with a normalization of network using Lipschitz constants bounded by products
of operator spectral norms. It inspires many further investigations in various settings (Miyato et al.,
2018; Neyshabur et al., 2018; Liao et al., 2018).
However, the improvement of margin distributions does not necessarily guarantee a better generalï¿¾ization performance, which is at least traced back to (Breiman, 1999) in his effort to understanding
AdaBoost. In this work, Breiman designed an algorithm arc-gv such that the margin can be maxiï¿¾mized via a prediction game, then he demonstrated an example that one can achieve uniformly larger
margin distributions on training data than AdaBoost but suffer a higher generalization error. In the
end of this paper, Breiman made the following comments with a dilemma:
â€The results above leave us in a quandary. The laboratory results for various arcing algorithms are
excellent, but the theory is in disarray. The evidence is that if we try too hard to make the margins
1
Under review as a conference paper at ICLR 2019
larger, then overfitting sets in. ... My sense of it is that we just do not understand enough about what
is going on.â€
Breimanâ€™s dilemma triggers some further explorations to understand the limitation of margin theory
in boosting (Reyzin & Schapire, 2006; Wang et al., 2008; 2011). In particular, Reyzin & Schapire
(2006) points out that the trees found by arg-gv have larger model complexity in terms of deeper
average depth than AdaBoost, suggesting that margin maximization in arc-gv does not necessarily
control the model complexity. The latter works provide tighter bounds based on VC-dimension and
optimized quantile training margins, which however do not apply to over-parametrized models in
deep neural networks and the case where the training margin distributions are uniformly improved.
In this paper, we are going to revisit Breimanâ€™s dilemma in the scenario of deep neural networks.
Both the success and failure can be seen on normalized margin based bounds on generalization error.
First of all, letâ€™s look at the following illustration example.
Example (Breimanâ€™s Dilemma with a CNN). A basic 5-layer convolutional neural network of c
channels (see Section 3 for details) is trained with CIFAR-10 dataset whose 10 percent labels are
randomly permuted. When c = 50 with 92, 610 parameters, Figure 1 (a) shows the training error
and generalization (test) error in solid curves. From the generalization error in (a) one can see
that overfitting indeed happens after about 10 epochs, despite that training error continuously drops
down to zero. One can successfully predict such an overfitting phenomenon from Figure 1 (b), the
evolution of normalized margin distributions defined later in this paper. In (b), while small margins
are monotonically improved during training, large margins undergoes a phase transition from inï¿¾crease to decrease around 10 epochs such that one can predict the tendency of generalization error
in (a) using large margin dynamics. Two particular sections of large margin dynamics are highï¿¾lighted in (b), one at 8.3 on x-axis that measures the percentage of normalized training margins no
more than 8.3 (training margin error) and the other at 0.8 on y-axis that measures the normalized
margins at quantile q = 0.8 (i.e. 1/Î³Ë†q,t). Both of them meet the tendency of generalization error
in (a) and find good early stopping time to avoid overfitting. However, as we increase the channel
number to c = 400 with about 5.8M parameters and retrain the model, (c) shows a similar overfitï¿¾ting phenomenon in generalization error; on the other hand, (d) exhibits a monotonic improvement
of normalized margin distributions without a phase transition during the training and thus fails to
capture the overfitting. This demonstrates the Breimanâ€™s dilemma in CNN.
(a) (b)
(c) (d)
Figure 1: Demonstration of Breimanâ€™s Dilemma in Convolutional Neural Networks.
A key insight behind this dilemma, is that one needs a trade-off between the model expression
power and the complexity of the dataset to endorse margin bounds a prediction power. On one hand,
when the model has a limited expression power relative to the training dataset, in the sense that the
2
Under review as a conference paper at ICLR 2019
training margin distributions CAN NOT be uniformly improved during training, the generalization
or test error may be predicted from dynamics of normalized margin distributions. On the other hand,
if we push too hard to improve the margin by giving model too much degree of freedom such that the
training margins are uniformly improved during training process, the predictability may be lost. A
trade-off is thus necessary to balance the complexity of model and dataset, otherwise one is doomed
to meet Breimanâ€™s dilemma when the models arbitrarily increase the expression power.
The example above shows that the expression power of models relative to the complexity of dataset,
can be observed from the dynamics of normalized margins in training, instead of counting the numï¿¾ber of parameters in neural networks. In the sequel, our main contributions are to make these precise
by revisiting the Rademacher complexity bounds with Lipschitz constants (Bartlett et al., 2017).
â€¢ With the Lipschitz-normalized margins, a linear inequality is established between trainï¿¾ing margin and test margin in Theorem 1. When both training and test normalized margin
distributions undergo similar phase transitions on increase-decrease during the training proï¿¾cess, one may predict the generalization error based on the training margins as illustrated
in Figure 1. â€¢ In a dual direction, one can define a quantile margin via the inverse of margin distribution
functions, to establish another linear inequality between the inverse quantile margins and
the test margins as shown in Theorem 2. Quantile margin is far easier to tune in practice
and enjoys a stronger prediction power exploiting an adaptive selection of margins along
model training.
â€¢ In all cases, Breimanâ€™s dilemma may fail both of the methods above when dynamics of norï¿¾malized training margins undergo different phase transitions to that of test margins during
training, where a uniform improvement of margins results in overfitting.
Section 2 describes our method to derive the two linear inequalities of generalization bounds above.
Extensive experimental results are shown in Section 3 and Appendix with basic CNNs, AlexNet,
VGG, ResNet, and various datasets including CIFAR10, CIFAR100, and mini-Imagenet.
2 METHOD
Let X be the input space (e.g. X âŠ‚ RCÃ—WÃ—H in image classification) and Y := {1, . . . , K} be
the space of K classes. Consider a sample set of n observations S = {(x1, y1), . . . ,(xn, yn) : xi âˆˆ X , yi âˆˆ Y} that are drawn i.i.d. from PX,Y . For any function f : X â†’ R, let Pf = R X f(X)dP
be the population expectation and Pnf = (1/n)P ni=1 f(xi) be the sample average.
Define F to be the space of functions represented by neural networks,
F = {f : X â†’ RK, f(x) = WlÏƒl(xl)+bl
, xi = Ïƒi(Wiâˆ’1xiâˆ’1+biâˆ’1), i = 1, . . . , l, x0 = x}, (1)
where l is the depth of the network, Wi
is the weight matrix corresponding to a linear operator on
xi and Ïƒi stands for either element-wise activation function (e.g. ReLU) or pooling operator that
are assumed to be Lipschitz bounded with constant LÏƒi
and satisfying Ïƒi(0) = 0. For example, in
convolutional network, Wixi + bi = wi âˆ— xi + bi where âˆ— stands for the convolution between input
tensor xl and kernel tensor wl
. We equip F with the Lipschitz semi-norm, for each f, k fk F := sup
x6=x0 k f(x) âˆ’ f(x0 )k 2 k x âˆ’ x0 k 2 â‰¤ LÏƒ l Y i=1
k
Wik Ïƒ := Lf , (2)
where k Â· kÏƒ is the spectral norm and LÏƒ = Q Li=1 LÏƒi
. For all the examples in this paper, we use
ReLU activation Ïƒi
that leads to LÏƒi = 1. Moreover we consider the following family of hypothesis
mapping,
H = {h(x) = [f(x)]y : X â†’ R, f âˆˆ F, y âˆˆ Y}, (3)
where [Â·]j denotes the j
th coordinate and we further define the following class induced by Lipschitz
semi-norm bound on F, HL = {h(x) = [f(x)]y : X â†’ R, h(x) = [f(x)]y âˆˆ H with k fk F â‰¤ L, y âˆˆ Y}. (4)
3
Under review as a conference paper at ICLR 2019
Lastly, rather than merely looking at whether a prediction f(x) on y is correct or not, we also
consider the margin defined as Î¶(f(x), y) = [f(x)]yâˆ’max{j:j6=y}[f(x)]j . Therefore, we can define
the ramp loss and margin error depending on the confidence of predictions. Given two thresholds
Î³2 > Î³1 â‰¥ 0, define a ramp loss to be
`
(Î³1,Î³2)(Î¶) =
ï£±ï£²ï£³ 1 Î¶ < Î³1, âˆ’ 1âˆ† (Î¶ âˆ’ Î³2) Î³1 â‰¤ Î¶ â‰¤ Î³2, 0 Î¶ > Î³2,
where âˆ† := Î³2 âˆ’ Î³1. In particular Î³1 = 0 and Î³2 = Î³, we also write ` Î³ = ` Î³ for simplicity. Define
the margin error to measure if f has margin no more than a threshold Î³, eÎ³(f(x), y) =  1 Î¶(f(x), y) â‰¤ Î³ 0 Î¶(f(x), y) > Î³ . (5)
In particular, e0(f(x), y) is the common mis-classification error and E[e0(f(x), y)] =
P[Î¶(f(x), y) < 0]. Note that e0 â‰¤ ` Î³ â‰¤ eÎ³, and ` Î³ is Lipschitz bounded by 1/Î³.
The central question we try to answer is, can we find a proper upper bound to predict the tendency
of the generalization error along training, such that one can early stop the training near the epoch
that P[Î¶(ft(x), y) < 0] is minimized? The answer is both a yes and a no!
We begin with the following lemma, as a typical result in multi-label classification from the uniform
law of large numbers (Koltchinskii et al., 2002).
Lemma 2.1. Given a Î³0 > 0, then, for any Î´ âˆˆ (0, 1), with probability at least 1 âˆ’ Î´, the following
holds for any f âˆˆ F with k fk F â‰¤ L, E[` Î³0 (f(x), y)] â‰¤ 1n nXi=1
[` Î³0 (f(xi), yi)] + 2K2 Î³0 Rn(HL) + r log(1/Î´) 2n
(6)
where
Rn(HL) = Exi,Îµi
sup
hâˆˆHL 1n nXi=1
Îµih(xi) (7)
is the Rademacher complexity of function class HL with respect to n samples, and the expectation
is taken over xi
, Îµi, i = 1, ..., n.
Unfortunately, direct application of such bound for a constant Î³0 will suffer from the so-called
scaling problem. The following proposition gives an lower bound of Rademacher complexity term,
whose proof is provided in Appendix D.
Proposition 1. Consider the networks with ReLU activation functions. For any L > 0, there holds,
Rn(HL) â‰¥ CLES[q x21 + . . . + x2n] (8)
where C > 0 is a constant that does not depend on S.
The lemma tells us if L â†’ âˆž, upper bound (6) becomes trivial since Rn(HL) â†’ âˆž. In fact, both
Telgarsky (2013) and Soudry et al. (2018) show that with gradient descent, the norm of estimatorâ€™s
weight in logistic regression and general boosting (including exponential loss), respectively, will go
to infinity at a growth rate log(t) when the data is linearly separable. As for the deep neural network
with cross-entropy loss, the input of last layer is usually be viewed as features extracted from original
input. Training the last layer with other layers fixed is exactly a logistic regression, and the feature is
linearly separable as long as the training error achieves zero. Therefore, without any normalization,
the hypothesis space along training has no upper bound on L and the upper bound (6) is useless.
Besides, even for a fixed L, the complexity term Rn(HL) is computationally intractable.
The first remedy is to restrict our attention on H1 by normalizing f with its Lipschitz semi-norm
k
fk F or its upper bounds. Note that a normalized network fËœ = f /C has the same mis-classification
error as f for all C > 0. For the choice of C, itâ€™s hard in practice to directly compute the Lipschitz
semi-norm of a network, but instead some approximate estimates on the upper bound Lf in (2) are
available as discussed in Appendix A. In the sequel, let fËœ = f /Lf be the normalized network and
4
Under review as a conference paper at ICLR 2019
Ëœh = h/Lf = Î¶(f, y)/Lf = Î¶( Ëœf, y) âˆˆ H1 be the corresponding normalized hypothesis function.
Now a simple idea is to regard Rn(H1) as a constant and predict the tendency of generalization
error via training margin error of the normalized network, that avoids the scaling problem and the
computation of complexity term. The following theorem makes this precise.
Theorem 1. Given Î³1 and Î³2 such that Î³2 > Î³1 â‰¥ 0 and âˆ† := Î³2 âˆ’ Î³1 â‰¥ 0, for any Î´ > 0, with
probability at least 1 âˆ’ Î´, along the training epoch t = 1, . . . , T, the following holds for each ft, P[Î¶(fËœt(x), y) < Î³1] â‰¤ Pn1[Î¶(fËœt(x), y) < Î³2] + CHâˆ† + r
log(1/Î´) 2n
(9)
where CH = 2K2Rn(H1).
Remark. In particular, when we take Î³1 = 0 and Î³2 = Î³ > 0, the bound above becomes,
P[Î¶(ft(x), y) < 0] â‰¤ Pn[Î¶(fËœt(xi), yi) < Î³] + CHÎ³ + r
log(1/Î´) 2n
(10)
Theorem 1 says, we can bound the normalized test margin distribution P[Î¶(fËœt(x), y) < Î³1] by the
normalized training margin distribution Pn[Î¶(fËœt(x), y) < Î³2]. Recently Liao et al. (2018) investiï¿¾gates for normalized networks, the strong linear relationship between cross-entropy training loss and
test loss when the training epochs are large enough. As a contrast, we consider the whole training
process and normalized margins. In particular, we hope to predict the trend of generalization error
by choosing Î³1 = 0 and a proper Î³. For this purpose, the following facts are important. First, we
do not expect the bound, for example (10), is tight for every choice of Î³ > 0, instead we hope there
exists some Î³ such that the training margin error nearly monotonically changes with generalization
error. Figure 2 shows the existence of such Î³ such that the training margin error successfully recover
the tendency of generalization error on CIFAR10 dataset. Moreover, in Appendix Figure 8 shows
the rank correlation between training margin error at various Î³ and training/test error. Second, the
normalizing factor is not necessarily to be an upper bound of Lipschitz semi-norm. The key point is
to prevent the complexity term of the normalized network going to infinity. Since for any constant
c > 0, normalization by Â¯L = cL works in practice where the constant could be absorbed to Î³, we
could ignore the Lipschitz constant introduced by general activation functions in the middle layers.
However, it is a natural question whether a reasonable Î³ with prediction power exists. A simple
example in Figure 1 shows, once the training margin distribution is uniformly improved, dynamic
of training margin error fails to detect the minimum of generalization error in the early stage. This
is because when network structure becomes complex enough, the training margin distribution could
be more easily improved but the the generalization error may overfit. This is exactly the same
observation in Breiman (1999) to doubt the margin theory in boosting type algorithms. More detailed
discussions will be given in Section 3.2.
The most serious limitation of Theorem 1 lies in we must fix a Î³ along the complete training process.
In fact, the first term and second term in the bound (10) vary in the opposite directions with respect
to Î³, and thus different ft may prefer different Î³ for a trade-off. As in Figure 1 (b) of the example,
while choosing Î³ is to fix an x-coordinate section of margin distributions, its dual is to look for a
y-section which leads to different margins for different ft. This motivates the quantile margin in the
following theorem. Let Î³Ë†q,f be the q
th quantile margin of the network f with respect to sample S, Î³Ë†q,f = inf {Î³ : Pn1[Î¶(f(xi), yi) â‰¤ Î³] â‰¥ q} . (11)
Theorem 2. Assume the input space is bounded by M > 0, that is k xk 2 â‰¤ M, âˆ€x âˆˆ X . Given a
quantile q âˆˆ [0, 1], for any Î´ âˆˆ (0, 1) and Ï„ > 0, the following holds with probability at least 1 âˆ’ Î´
for all ft satisfying Î³Ë†q,fËœt
> Ï„ , P[Î¶(ft(x), y) < 0] â‰¤ Cq + CH Î³Ë†q,fËœt
(12)
Cq = q + q
log(2/Î´) 2n + q
log log2
(4(M+l)/Ï„) n
and CH = 4K2Rn(H1).
Remark. We simply denote Î³q,t for Î³q,fËœt
when there is no confusion.
5
Under review as a conference paper at ICLR 2019
Compared with the bound (10), (12) make the choice of Î³ varying with ft and the cost is an adï¿¾ditional constant term Cq2
and the constraint Î³Ë†q,t > Ï„ that typically holds for large enough q in
practice. In applications, stochastic gradient descent (SGD) often effectively improves the trainï¿¾ing margin distributions along the drops of training errors, a small enough Ï„ and large enough
q usually meet Î³Ë†q,t > Ï„ . Moreover, even with the choice Ï„ = exp(âˆ’B), constant term
p
[log log2
(4(M + l)/Ï„ )]/n = O(p log B/n) is still negligible and thus very little cost is paid
in the upper bound.
In practice, tuning q âˆˆ [0, 1] is far easier than tuning Î³ > 0 directly and setting a large enough
q â‰¥ 0.9 usually provides us lots of information about the generalization performance. The quantile
margin works effectively when the dynamics of large margin distributions reflects the behavior of
generalization error, e.g. Figure 1. In this case, after certain epochs of training, the large margins
have to be sacrificed to further improve small margins to reduce the training loss, that typically
indicates a possible saturation or overfitting in test error.
3 EXPERIMENTAL RESULTS
We briefly introduce the network and dataset used in the experiments. For the network, we first
consider the convolutional neural network with very simple structure basic CNN(c). The structure
is shown in Appendix Figure 7. Basically, it has five convolutional layers with c channels at each
and one fully connected layer, where c will be specified in concrete examples. Second, we consider
more practical network structure, AlexNet (Krizhevsky et al., 2012), VGGNet-16 (Simonyan & Zisï¿¾serman, 2014) and ResNet-18 (He et al., 2016). For the dataset, we consider CIFAR10, CIFAR100
(Krizhevsky & Hinton, 2009) and Mini-ImageNet (Vinyals et al., 2016).
The spirit of the following experiments is to show, when and how, the margin bound could be used
to predict the tendency of generalization or test error along the training path?
3.1 SUCCESS: TRAINING MARGIN ERROR AND QUANTILE MARGIN
This section is to apply Theorem 1 and Theorem 2 to predict the tendency of generalization error.
Letâ€™s firstly consider training a basic CNN(50) on CIFAR10 dataset with and without random noise.
The relations between generalization error and training margin error eÎ³(fËœ(x), y) with Î³ = 9.8,
inverse quantile margin 1/Î³Ë†q,t with q = 0.6 are shown in Figure 2. In this simple example where
the net is light and the dataset is simple, the linear bounds (9) and (12) show a good prediction
power: they stop either near the epoch of sufficient training (Left, original data) or where even an
overfitting occurs (Right, 10 percents label corrupted).
Figure 2: Success examples. Net structure: basic CNN (50). Dataset: Original CIFAR10 (Left)
and CIFAR10 with 10 percents label corrupted (Right). In each figure, we show training error (red
solid), training margin error Î³ = 10 (red dash) and inverse quantile margin (red dotted) with q = 0.6
and generalization error (blue solid). The marker â€xâ€ in each curve indicates the global minimum
along epoch 1, . . . , T. Both training margin error and inverse quantile margin successfully predict
the tendency of generalization error.
A few discussions are given below.
1. There exists a trade-off on the choice of Î³ from the linear bounds (9) (and parallel arguï¿¾ments hold for q). The training margin error with a small Î³ is close to the training error,
6
Under review as a conference paper at ICLR 2019
while a large Î³ is close to generalization error and itâ€™s illustrated in Appendix Figure 8
where we show the Spearmanâ€™s Ï rank correlation1 between training margin error and
training error, generalization error against threshold Î³.
2. The training margin error (or inverse quantile margin) is closely related to the dynamics
of training margin distributions. For certain choice of Î³, if the curve of training margin
error (with respect to epoch) is V-shape, the corresponding dynamics of training margin
distributions will have a cross-over, where the low margins have a monotonic increase and
the large margins undergo a phase transition from increase to decrease, as illustrated by the
red arrow in Figure 1 (b).
3. Dynamics of quantile margins can adaptively select Î³t for each ft without access to the
complexity term. Unlike merely looking at the training margin error with a fixed Î³, quantile
margin bound (12) shows a stronger prediction power than (10) and even be able to capture
more local information as illustrated in Figure 3. The generalization error curve has two
valleys corresponding to a local optimum and a global optimum, and the quantile margin
curve with q = 0.95 successfully identifies both. However, if we consider the dynamics of
training margin errors, itâ€™s rarely possible to recover the two valleys at the same time since
their critical thresholds Î³t1
and Î³t2
are different. Another example of ResNet is given in
Appendix Figure 9.
Figure 3: Inverse quantile margin. Net structure: Basic CNN. Dataset: CIFAR10 with 10 percents
label corrupted. Left: the dynamic of generalization error (blue) and inverse quantile margin with
q = 0.95 (red). Two local minimums are marked by â€xâ€ in each dynamic. Right: dynamic of
training margin distribution and two distributions when local minimum occurs are highlighted with
red color. The inverse quantile margin successfully captures two local minimums of test error.
3.2 FAILURE: BREIMANâ€™S DILEMMA IN OVER-PARAMETERIZED MODELS
In this section, we explore the normalized margin dynamics with over-parameterized models whose
expression power might be greater than data complexity. We conduct experiments in the following
two scenarios.
1. In the first experiment shown in Figure 4, we fix the dataset to be CIFAR10 with 10 percent
of labels randomly permuted, and gradually increase the channels from basic CNN(50) to
basic CNN(400). As the channel number increases, dynamics of the normalized training
margins in the first row change from a phase transition with a cross-over in large margins to
a monotone improvement of margin distributions. This phenomenon is not a surprise since
with a strong representation power, the whole training margin distribution can be monotonï¿¾ically improved without sacrificing the large margins. On the other hand, the generalization
or test error can never be monotonically improved. In the second row, heatmaps depict rank
correlations of dynamics between training and test margin errors, which clearly show the
phase transitions for CNN(50) and CNN(100) and its disappearance for CNN(400).
2. In the second experiment shown in 5, we compare the normalized margin dynamics of
training CNN(400) and ResNet18 on two different datasets, CIFAR100 (the simpler) and
Mini-ImageNet (the more complex). It shows that: (a) CNN(400) (5.8M parameters) does
not have an over-representation power on CIFAR100, whose normalized training margin
1The Spearmanâ€™s Ï rank correlation measures how two variables are correlated up to a monotone transform
and a larger correlation means a closer tendency.
7
Under review as a conference paper at ICLR 2019
dynamics exhibits a phase transition â€“ a sacrifice of large margins to improve small margins
during training; (b) ResNet18 (11M parameters) exhibits an over-representation power on
CIFAR100 via a monotone improvement on training margins, but loses such a power in
Mini-ImageNet with the phase transitions in margin dynamics.
More experiments including AlexNet and VGG16 are shown in Appendix Figure 11.
This phenomenon is not unfamiliar to us, since Breiman (Breiman, 1999) has pointed out that the
improvement of training margins is not enough to guarantee a small generalization or test error in
the boosting type algorithms. In this paper Breiman designed an algorithm, called arc-gv, enjoyï¿¾ing an uniformly better training margin distribution comparing with Adaboost but suffer a higher
generalization error. Now again we find the same phenomenon ubiquitous in deep neural networks.
Figure 4: Breimanâ€™s Dilemma I. Net structure: Basic CNN(50) (Left), Basic CNN(100) (Middle),
Basic CNN(400) (Right) . Dataset: CIFAR10 with 10 percent labels corrupted. Top: dynamics of
training margin distributions. Bottom: heatmaps of Spearmanâ€™s Ï correlation between test margin
error P[eÎ³1 (fËœ(x), y)] and training margin error Pn[eÎ³2 (fËœ(xi), yi)], where (x, y)-coordinates stand
for (Î³1, Î³2). With a fixed dataset, we explore how the expression power of the network influences
the phase transitions of margin dynamics. The cross-over in the dynamics of training margin distriï¿¾butions becomes obscure and eventually disappears as the channel number increases. A clear phase
transition is illustrated via the heatmap, where the training margin dynamics are highly correlated
with test margin dynamics when we use Basic CNN(50) and CNN(100) (the area on the diagonal is
light in the left and middle) and the training margin dynamics is very distinct to test error (Î³1 â‰¤ 0)
in CNN(100) (right).
Figure 5: Breimanâ€™s Dilemma II. Net structure: Basic CNN(400) (Left), ResNet18 (Middle, Right).
Dataset: CIFAR100 (Left, Middle), Mini-ImageNet (Right) with 10 percent labels corrupted. With
a fixed network structure, we further explore how the complexity of dataset influences the margin
dynamics. Taking ResNet18 as an example, margin dynamics on CIFAR100 doesnâ€™t have any crossï¿¾over (phase transition), but on Mini-Imagenet a cross-over occurs.
In the end, itâ€™s worth mentioning different choices of the normalization factor estimates may affect
the range of predictability. In all experiments above, normalization factor is estimated via an upper
bound on spectral norm given in Appendix A (Lemma A.1 in Section A). One could also use power
iteration (Miyato et al., 2018) to present a more precise estimation on spectral norm. It turns out
a more accurate estimation of spectral norm can extend the range of predictability, but Breimanâ€™s
dilemma is still there when the balance between model expression power and dataset complexity is
broken. More experiments on this aspect can be found in Figure 10 in Appendix.
8
Under review as a conference paper at ICLR 2019
4 CONCLUSION
In this paper, we show that Breimanâ€™s dilemma is ubiquitous in deep learning, in addition to previous
studies on Boosting algorithms. We exhibit that Breimanâ€™s dilemma is closely related to the tradeï¿¾off between model expression power and data complexity. A novel perspective on phase transitions
in dynamics of Lipschitz-normalized margin distributions is proposed to inspect when the model
has over-representation power compared to the dataset, instead of merely counting the number of
parameters. A data-driven early stopping rule by monitoring the margin dynamics is a future direcï¿¾tion to explore. Lipschitz semi-norm plays an important role in normalizing or regularizing neural
networks, e.g. in GANs (Kodali et al., 2017; Miyato et al., 2018), therefore a more careful treatment
deserves further pursuits.
REFERENCES
Peter Bartlett, Dylan J. Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for
neural networks. In The 31st Conference on Neural Information Processing Systems (NIPS),
Long Beach, CA, USA. 2017.
Leo Breiman. Prediction games and arcing algorithms. Neural computation, 11(7):1493â€“1517,
1999.
Peter BÂ¨uhlmann and Bin Yu. Boosting with the l2-loss: Regression and classification. Journal of
American Statistical Association, 98:324â€“340, 2002.
Corinna Cortes and Vladimir N. Vapnik. Support-vector networks. Machine Learning, 20(3):273â€“
297, 1995.
Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an
application to boosting. Journal of Computer and System Sciences, 55:119, 1997.
Gene H Golub and Henk A Van der Vorst. Eigenvalue computation in the 20th century. In Numerical
analysis: historical developments in the 20th century, pp. 209â€“239. Elsevier, 2001.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition
(CVPR), pp. 770â€“778, 2016.
Naveen Kodali, Jacob Abernethy, James Hays, and Zsolt Kira. On convergence and stability of gans.
arXiv preprint arXiv:1705.07215, 2017.
Vladimir Koltchinskii, Dmitry Panchenko, et al. Empirical margin distributions and bounding the
generalization error of combined classifiers. The Annals of Statistics, 30(1):1â€“50, 2002.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Techï¿¾nical report, Citeseer, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep conï¿¾volutional neural networks. In Advances in neural information processing systems (NIPS), pp.
1097â€“1105, 2012.
M. Ledoux and M. Talagrand. Probability in Banach Spaces: Isoperimetry and Processes. Springerï¿¾Verlag, 1991.
Qianli Liao, Brando Miranda, Andrzej Banburski, Jack Hidary, and Tomaso Poggio. A surprising
linear relationship predicts test performance in deep networks. MIT CBMM memo, No. 91, 2018.
Ron Meir and Tong Zhang. Generalization error bounds for bayesian mixture algorithms. Journal
of Machine Learning Research, 4:839â€“860, 2003.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. In The 6th International Conference on Learning Represenï¿¾tations (ICLR), 2018.
9
Under review as a conference paper at ICLR 2019
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning.
MIT press, 2012.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A pac-bayesian approach to
spectrally-normalized margin bounds for neural networks. In The 6th International Conference
on Learning Representations (ICLR), 2018.
A. B. J. Novikoff. On convergence proofs on perceptrons. In Proceedings of the Symposium on the
Mathematical Theory of Automata, volume 12, pp. 615â€“622, 1962.
Lev Reyzin and Robert E Schapire. How boosting the margin can also boost classifier complexity.
In Proceedings of the 23rd international conference on Machine learning, pp. 753â€“760. ACM,
2006.
Robert E. Schapire, Yoav Freund, Peter Bartlett, and Wee Sun Lee. Boosting the margin: a new
explanation for the effectiveness of voting methods. The Annals of Statistics, 26(5):1651â€“1686,
1998.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Daniel Soudry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent on separable
data. In The 6th International Conference on Learning Representations (ICLR), 2018.
Matus Telgarsky. Margins, shrinkage, and boosting. In Proceedings of the 30th International Conï¿¾ference on Machine Learning (ICML), 2013.
Vladimir N. Vapnik. Statistical Learning Theory. John Wiley & Sons, Inc., 1998.
Oriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wierstra, et al. Matching networks for one
shot learning. In Advances in Neural Information Processing Systems, pp. 3630â€“3638, 2016.
Martin J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge Series
in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019.
Liwei Wang, Masashi Sugiyama, Cheng Yang, Zhi-Hua Zhou, and Jufu Feng. On the margin exï¿¾planation of boosting algorithms. In The 21th Annual Conference on Learning Theory (COLT).
2008.
Liwei Wang, Masashi Sugiyama, Cheng Yang, Zhi-Hua Zhou, and Jufu Feng. A refined margin
analysis for boosting algorithms via equilibrium margin. Journal of Machine Learning Research,
12:1835â€“1863, 2011.
Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto. On early stopping in gradient descent learnï¿¾ing. Constructive Approximation, 26(2):289â€“315, 2007.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
Tong Zhang and Bin Yu. Boosting with early stopping: Convergence and consistency. Annals of
Statistics, 33(4):1538â€“1579, 2005.
10
Under review as a conference paper at ICLR 2019
A ESTIMATE OF NORMALIZATION FACTORS
In this section we discuss how to estimate the Lipschitz constant bound in (2). Given an operator
W associated with a convolutional kernel w, i.e. W x = w âˆ— x, there are two ways to estimate its
operator norm. We begin with a useful lemma,
Lemma A.1. For convolution operator with kernel w, i.e. W x := w âˆ— x, there holds
k
w âˆ— xk 2 â‰¤ kwk 1k xk 2.
In other words, k Wk Ïƒ â‰¤ kwk 1.
Proof.
k
w âˆ— xk 22 = X u (X v x(u)w(u âˆ’ v))2 = X u (X v (x(u)p w(u âˆ’ v)) Â· p w(u âˆ’ v))2 â‰¤ X u (X v x(u)2w(u âˆ’ v))(X
v w(u âˆ’ v)), = k wk 21k xk 22
where the second last step is due to Cauchy-Schwartz inequality.
A. ` 1-norm. The convolutional operator (spectral) norm can be upper bounded by the ` 1-norm
of its kernels, i.e. k Wk Ïƒ â‰¤ kwk 1. This is a simple way but the bound gets loose when the
channel numbers increase.
B. Power iteration. A fast approximation for the spectral norm of the operator matrix is given
in (Miyato et al., 2018) in GANs that is based on power iterations (Golub & Van der Vorst,
2001). Yet as a shortcoming, it is not easy to apply to the ResNets.
We compare two estimation in Appendix Figure 10. It turns out both of them have prediction power
on the tendency of generalization error and both of them will fail when the network has large enough
expression power. Though using ` 1 norm of kernel is extremely efficient, the power iteration method
may be tighter and has a wider range of predictability.
In the remaining of this section, we will particularly discuss the treatment of ResNets. ResNet is
usually a composition of the basic blocks shown in Figure 6 with short-cut structure. The following
method is used in this paper to estimate the upper bound of operator or spectral norm of such a basic
block of ResNet.
Figure 6: A basic block in ResNets used in this paper. The shortcut consists of one block with
convolutional and batch-normalization layers, while the main stream has two blocks. ResNets are
constructed as a cascading of several basic blocks of various sizes.
(a) Convolution layer: its operator norm can be bounded either by the ` 1 norm of kernel or by
power iteration above.
(b) Batch Normalization (BN): in training process, BN normalizes samples by x+ = (x âˆ’ ÂµB)/p Ïƒ2B +  , where ÂµB, Ïƒ2B are mean and variance of batch samples, while keeping
an online averaging as ÂµË† and ÏƒË†2
. Then BN rescales x+ by estimated parameters Ë†Î±, Î²Ë†
and output xË† = Ë†Î±x+ + Ë†Î². Therefore the whole rescaling of BN on the kernel tensor w
11
Under review as a conference paper at ICLR 2019
of the convolution layer is Ë†w = wË†Î±/âˆšÏƒË†2 +  and its corresponding rescaled operator is
k
Ë†Wk Ïƒ = k Wk Ïƒ Ë†Î±/âˆšÏƒË†2 +  .
(b) Activation and pooling: their Lipschitz constants LÏƒ can be known a priori, e.g. LÏƒ = 1
for ReLU and hence can be ignored. In general, LÏƒ can not be ignored if they are in the
shortcut as discussed below.
(d) Shortcut: In residue net with basic block in Figure 6, one has to treat the mainstream
(Block2, Block3) and the shortcut Block1 separately. Since k f + gk F â‰¤ kfk F + k gk F ,
in this paper we take the Lipschitz upper bound by LÏƒout(k Ë†W1k Ïƒ + LÏƒin k
Ë†W2k Ïƒk Ë†W3k Ïƒ),
where k Ë†Wik Ïƒ denotes a spectral norm estimate of BN-rescaled convolutional operator Wi.
In particular LÏƒout can be ignored since all paths are normalized by the same constant while
LÏƒin can not be ignored due to its asymmetry.
B STRUCTURE OF BASIC CNN
Figure 7: Illustration of the structure of basic CNN.
C EXPERIMENTS
C.1 SPEARMANâ€™S Ï RANK CORRELATION COEFFICIENT
Figure 8: Spearmanâ€™s Ï rank correlation at different Î³ and q. Dataset: CIFAR10 (Left) and CIï¿¾FAR10 with 10 percents label corrupted (Right). Net structure: Basic CNN(50). Left: training
margin error and generalization error (Blue), training error (Red). Right: inverse quantile margin
and generalization error (Blue), training error (Red). The dynamic of large margin is closely related
to the generalization error.
12
Under review as a conference paper at ICLR 2019
C.2 EXAMPLES: TWO LOCAL MINIMUMS IN RESNET18
Figure 9: Dynamic of inverse quantile margin. Data, CIFAR10 with 10 percents label corrupted.
Network, ResNet18. Normalization factor, spectral complexity estimated by power iteration. Left:
the dynamic of generalization error and inverse quantile margin with q = 0.95. Overfitting occurs
and two local minimums are marked with x in each dynamic. The dash line highlight where the
margin distribution is uniformly improved. Right: dynamic of training margin distribution. Two
distributions when local minimum of generalization error occurs are highlighted with red color.
The picture is slight different here, since after the first (better) local minimum, the training margin
distribution is uniformly improved without reducing generalization error. Therefore, we could not
expect the inverse quantile margin to reflect the tendency of generalization error globally, especially
the order of two local minimums. However, around epochs when local minimum occurs, the training
margin distribution still has a cross-over, and thus the inverse quantile margin could reflect the
tendency locally.
C.3 ESTIMATED BY POWER ITERATION AND KERNEL l1 NORM
Figure 10: Power iteration: success and failure. Top: spectral norm in Lf is estimated by the corï¿¾responding kernel l1 norm. Bottom: spectral norm is estimated by Power Iteration. Net structure,
Basic CNN with channels 50(Top, Left), 100(Top, Middle), 400(Top Right), 200(Bottom, Left),
600(Bottom, Middle), 900(Bottom, Right). Dataset: CIFAR10 with 10 percents corrupted. A
more accurate estimation of spectral norm can extend the range of predictability, but eventually
face Breimanâ€™s dilemma if the balance between model expression power and dataset complexity is
broken.
13
Under review as a conference paper at ICLR 2019
C.4 SUCCESS AND FAILURE IN MORE PRACTICAL NETWORK AND DATASET
Figure 11: Examples on more practical network and dataset. The dataset and network we apï¿¾plied is listed in each row. Left: curve of training error, generalization error, training margin erï¿¾ror and inverse quantile margin. Middle: dynamic of training margin distribution. Right: heatmap
of Spearmanâ€™s Ï correlation between test margin error E[eÎ³1 (fËœ(x), y)] and training margin error
(1/n)P [eÎ³2 (fËœ(xi), yi)] against (Î³1, Î³2).
14
Under review as a conference paper at ICLR 2019
C.5 DYNAMIC OF TEST MARGIN DISTRIBUTION
Figure 12: Comparison between dynamic of test margin distribution and training margin distribuï¿¾tion. Top: training margin distribution. Bottom: test margin distribution. Net structure: Basic CNN
with channels 50 (Left), 100 (Middle) and 400 (Right). When model becomes complex, the dynamic
of training margin distribution lose the predictability on test margin distribution.
D PROOFS
D.1 AUXILIARY LEMMAS
Lemma 2.1 follows from the following Lemma D.1 by applying it to bounded function ` Î³(f(x), y)
whose range is [0, 1].
Lemma D.1. For any Î´ âˆˆ (0, 1) and bounded-value functions FB := {f : X â†’ R : k fk âˆž â‰¤ B},
the following holds with probability at least 1 âˆ’ Î´,
sup
fâˆˆFB Enf(x) âˆ’ Ef(x) â‰¤ 2Rn(FB) + Br
log(1/Î´) 2n
(13)
where
Rn(F) = E sup
fâˆˆF
1n nXi=1
Îµif(xi) (14)
is the Rademacher Complexity of function class F.
For completeness, we include its proof that also needs the following well-known McDiarmidâ€™s inï¿¾equality (see, e.g. Wainwright (2019)).
Lemma D.2 (McDiarmidâ€™s Bounded Difference Inequality). For Bi-bounded difference functions
h : X â†’ R s.t. |h(xi
, xâˆ’i) âˆ’ h(x0i
, xâˆ’i)| â‰¤ Bi, P {Enh âˆ’ Exh(x) â‰¥ Îµ} â‰¤ exp  âˆ’ 2 2 P ni=1 Bi2  ,
15
Under review as a conference paper at ICLR 2019
Proof of Lemma D.1. It suffices to show that for fÂ¯ = f(x) âˆ’ Ef(x),
sup
fâˆˆFB EnfÂ¯ = sup
fâˆˆFB EnfÂ¯âˆ’ E sup
fâˆˆFB En Â¯f + E sup
fâˆˆFB EnfÂ¯ (15)
where with probability at least 1 âˆ’ Î´,
sup
fâˆˆFB EnfÂ¯âˆ’ E sup
fâˆˆFB EnfÂ¯ â‰¤ Br
log 1/Î´
2n
(16)
by McDiarmidâ€™s bounded difference inequality, and
E sup
fâˆˆFB EnfÂ¯ â‰¤ 2Rn(F) (17)
using Rademacher complexity.
To see (16), we are going to show that supfâˆˆFB EnfÂ¯ is a bounded difference function. Consider
g(xn1
) = EnfÂ¯ = 1n P ni=1 f(xi) âˆ’ Exf(x). Assume that the i-th argument xi changes to x0i
, then
for every g, g(xi
, xâˆ’i) âˆ’ sup
g g(x0i
, xâˆ’i) â‰¤ g(xi
, xâˆ’i) âˆ’ g(x0i
, xâˆ’i) â‰¤ 1n[f(xi) âˆ’ f(x0i
)]
â‰¤ Bn .
Hence supg g(xi
, xâˆ’i) âˆ’ supg g(x0i
, xâˆ’i) â‰¤ B/n, which implies that supfâˆˆFB EnfÂ¯ is a B/nï¿¾bounded difference function. Then (16) follows from the McDiarmidâ€™s inequality (Lemma D.2)
using Bi = B/n and Î´ = exp(âˆ’2nÎµ2/B2).
As to (17),
E sup
fâˆˆFB EnfÂ¯ = Exn1
sup
fâˆˆFB Eyn1 [Enf(xn1 ) âˆ’ Enf(yn1
)]
â‰¤ Exn1
,yn1
sup
fâˆˆFB [Enf(xn1 ) âˆ’ Enf(yn1
)]
= Exn1
,yn1
sup
fâˆˆFB EÎµn1 1n nXi=1
Îµi (f(xi) âˆ’ f(yi)), Îµi âˆˆ {Â±1} âˆ¼ B(n, 1/2)
â‰¤ Exn1
,yn1
,Îµn1
sup
fâˆˆFB 1n nXi=1
(Îµif(xi) âˆ’ Îµif(yi))
â‰¤ 2Exn1
,Îµn1
sup
fâˆˆFB 1n nXi=1
Îµif(xi) = 2R(FB)
that ends the proof.
We also need the following contraction inequality of Rademacher Complexity (Ledoux & Talagrand,
1991; Meir & Zhang, 2003).
Lemma D.3 (Rademacher Contraction Inequality). For any Lipschitz function: Ï† : R â†’ R such
that |Ï†(x) âˆ’ Ï†(y)| â‰¤ L|x âˆ’ y|, R(Ï† â—¦ F) â‰¤ LR(F).
Ledoux & Talagrand (1991) has an additional factor 2 in the contraction inequality which is dropped
in Meir & Zhang (2003). Its current form is stated in Mohri et al. (2012) as Talagrandâ€™s Lemma
(Lemma 4.2).
Beyond, we further introduce the family,
G = {g(x, y) = Î¶(f(x), y) : X Ã— Y â†’ R, f âˆˆ F}, (18)
and the sub-family constraint in Lipschitz semi-norm on f, GL = {g(x, y) = Î¶(f(x), y) : X Ã— Y â†’ R, f âˆˆ F with k fk F â‰¤ L}. (19)
The following lemma (Koltchinskii et al., 2002) allows us to bound the Rademacher complexity
term of Rn(G) by Rn(H),
16
Under review as a conference paper at ICLR 2019
Lemma D.4. Rn(GL) â‰¤ K2Rn(HL)
Proof of Lemma D.4. Rn(GL) = 1nES, sup
k
fkâ‰¤L nXi=1

iÎ¶(f(xi), yi), = 1nES, sup
k
fkâ‰¤L nXi=1
X yâˆˆY

iÎ¶(f(xi), yi)1[yi = y], = 1n X yâˆˆY
ES, sup
k
fkâ‰¤L nXi=1

iÎ¶(f(xi), yi)1[yi = y], = 1n X yâˆˆY
ES,[ sup
k
fkâ‰¤L nXi=1

i(2 Â· 1[yi = y] âˆ’ 1 2 + 12
)], â‰¤ 12n X yâˆˆY
ES,[ sup
k
fkâ‰¤L nXi=1

i(2 Â· 1[yi = y] âˆ’ 1)
2
] + 12n X yâˆˆY
ES,[ sup
k
fkâ‰¤L mXi=1

iÎ¶(f(xi), y)]), = 1n X yâˆˆY
ES,[ sup
k
fkâ‰¤L mXi=1

iÎ¶(f(xi), y)]), = 1n X yâˆˆY
ES,[ sup
k
fkâ‰¤L mXi=1

i([f(xi)]y âˆ’ max
y0 6 =y[f(xi)]y0 )], â‰¤ 1n X yâˆˆY
ES,[ sup
k
fkâ‰¤L mXi=1

i[f(xi)]y] + 1n X yâˆˆY
ES,[ sup
k
fkâ‰¤L mXi=1

i max
y0 6 =y[f(xi)]y0 ], â‰¤ Kn ES,[ sup
hâˆˆHL mXi=1

ih(xi)] + K(K âˆ’ 1)
n ES,[ sup
hâˆˆHL mXi=1

ih(xi)], = K2Rn(HL),
where the last inequality is implied from Rn({max(f1, . . . , fM) : fi âˆˆ Fi}) â‰¤ P Mm=1 Rn(Fm) (Koltchinskii et al., 2002; Mohri et al., 2012).
D.2 PROOF OF PROPOSITION 1
Proof of Proposition 1. Without loss of generality, we assume LÏƒi = 1, i = 1, . . . , l. Let T (r) =:
{t(x) = w Â· x : k wk 2 â‰¤ r} be the class of linear function with Lipschitz semi-norm less than r and
we show that for each t âˆˆ T (L/2), there exists f âˆˆ F with k fk F â‰¤ L and y0 âˆˆ {1, . . . , K} such
that h(x) = [f(x)]y0 âˆˆ HL. Letâ€™s t(x) = w0 Â· x with k w0k 2 â‰¤ L/2, we construct the network
f(x) = WlÏƒl(xl) + bl
, xi = Ïƒi(Wiâˆ’1xiâˆ’1 + biâˆ’1), i = 1, . . . , l, x0 = x as follows,
â€¢ x1 = Ïƒ1(W1x) = (Ïƒ1(w0 Â· x), Ïƒ1(âˆ’w0 Â· x), 0, . . . , 0)
â€¢ xk = Ïƒk(Wkxkâˆ’1) = (Ïƒk([xkâˆ’1]1), Ïƒk([xkâˆ’1]2), 0, . . . , 0), k = 2, . . . , l âˆ’ 1 â€¢ xl = Wlxlâˆ’1 = ([xlâˆ’1]1 âˆ’ [xlâˆ’1]2, 0, . . . , 0)
By construction above, we let h(x) = [f(x)]1, h(x) = Ïƒ1(w0 Â· x) âˆ’ Ïƒ1(âˆ’w0 Â· x), = w0 Â· x,
17
Under review as a conference paper at ICLR 2019
where k fk F â‰¤ Î li=1k Wik Ïƒ = 2L/2 = L, and thus h âˆˆ HL by definition. Therefore,
Rn(HL) â‰¥ Rn(T (L/2)), = ESE sup
k
wkâ‰¤L/2 nXi=1

iw Â· xi, = L2 ESE k nXi=1

ixik 2, â‰¥ CLESvuut nXi=1
k
xik 2,
where the second equality is implied from Cauchy-Schwarz inequality and the last inequality is
implied from Khintchine inequality.
D.3 PROOF OF THEOREM 1
Proof of Theorem 1. Consider l(Î³1,Î³2)(Î¶(fËœ(x), y)), where fËœ := f /Lf is the normalized network,
Î¶(fËœ(x), y) âˆˆ G1. Then for any Î³2 > Î³1 â‰¥ 0, P[Î¶(fËœ(x), y) < Î³1] â‰¤ P[` (Î³1,Î³2)(Î¶(fËœ(x), y)], â‰¤ Pn` (Î³1,Î³2)(fËœ(x), y) + 2Rn(l(Î³1,Î³2) â—¦ G1) + r log(1/Î´) 2n , â‰¤ Pn` (Î³1,Î³2)(fËœ(x), y) + 2âˆ†Rn(G1) + r log(1/Î´) 2n , â‰¤ Pn` Î³1,Î³2 (fËœ(x), y) + 2K2 âˆ† Rn(H1) + r log(1/Î´) 2n , â‰¤ Pn` Î³2 (fËœ(x), y) + 2K2 âˆ† Rn(H1) + r log(1/Î´) 2n ,
where the first and last inequality is implied from 1[Î¶ < Î³1] â‰¤ ` (Î³1,Î³2)(Î¶) â‰¤ 1[Î¶ < Î³2], the second
inequality is a direct consequence of Lemma D.1, the third inequality results from Rademacher
Contraction Inequality (Lemma D.3) and finally the fourth equation is implied from Lemma D.4.
D.4 PROOF OF THEOREM 2
Proof of Theorem 2. Firstly, we show after normalization, the normalize margin has an upper bound,
k
f(x)k 2 = k ÏƒL(WLxLâˆ’1 + bL)k 2, â‰¤ LÏƒL k WLxLâˆ’1 + bLk 2, â‰¤ (LÏƒL k Â¯WLk Ïƒ)(k xLâˆ’1k 2 + 1)
. . .
â‰¤ Î Li=1(LÏƒi k Â¯Wik Ïƒ)k xk 2 + Î£Li=1(Î Lj=i(LÏƒi k Â¯Wik Ïƒ)),
where xi = Ïƒi(Wixiâˆ’1 + bi) with x0 = x, Â¯Wi = (Wi
, bi) and LÏƒi
is the Lipschitz constant of
activation function Ïƒi with Ïƒi(0) = 0, i = 1, . . . , L. Then, for normalized network fËœ = f /Lf with
18
Under review as a conference paper at ICLR 2019
Lf = Î Li=1(LÏƒi k Â¯Wik Ïƒ) and k xk 2 â‰¤ M,kfËœ(x)k 2 â‰¤ M + L.
Therefore Î¶(fËœ(x), y) â‰¤ 2k fËœ(x)k 2 = 2(M + L) =: M1, and the quantile margin is also bounded
Î³Ë†q,t â‰¤ M1 for all q âˆˆ (0, 1), t = 1, . . . , T.
The remaining proof is standard. For any  > 0, we take a sequence of  k and Î³k, k = 1, 2, . . . by

k =  + q
log k n
and Î³k = M12âˆ’k
. Then by Theorem 1, P(Ak) â‰¤ exp(âˆ’2n2k),
where Ak is the event P[Î¶(fËœt(x), y) < 0] > Pn[Î¶(fËœ(x), y) < Î³k] + 2K2 Î³k R(H1) +  k, and the
probability is taken over samples {x1, ...xn}. We further consider the probability for none of Ak
occurs,
P(âˆƒAk) â‰¤ Î£âˆžk=1P(Ak), â‰¤ Î£âˆžk=1 k12
exp(âˆ’2n2), â‰¤ 2 exp(âˆ’2n2).
Hence, fix a q âˆˆ [0, 1], for any t = 1, . . . , T, as long as Î³Ë†q,t > 0, there exists a kË† â‰¥ 1 such that,
Î³Ë†k+1 â‰¤ Î³Ë†q,t < Î³Ë†k. (20)
Therefore,
AË†k+1 âŠ‡ P[Î¶(fËœt(x), y) < 0] > Pn[Î¶(fËœt(x), y) < Î³Ë†q,t] + 2K2 Î³Ë†k+1
R(H1) +  Ë†k+1, âŠ‡ P[Î¶(fËœt(x), y) < 0] > Pn[Î¶(fËœt(x), y) < Î³Ë†q,t] + 4K2 Î³Ë†q,t
R(H1) +  Ë†k+1, = P[Î¶(fËœt(x), y) < 0] > Pn[Î¶(fËœt(x), y) > Î³Ë†q,t] + 4K2 Ë†Î³q,t
R(H1) +  + s
log(kË†n
+ 1)
, âŠ‡ P[Î¶(fËœt(x), y) < 0] > Pn[Î¶(fËœt(x), y) > Î³Ë†q,t] + 4K2 Ë†Î³q,t
R(H1) +  + r
log log2
(2
nM1/Î³Ë†q,t).
The first inequality is implied from Pn[Î¶(fËœt(x), y) < Î³Ë†q,t] > Pn[Î¶(fËœt(x), y) < Î³Ë†k+1], since Î³Ë†k+1 â‰¤ Î³Ë†q,t. The second inequality is implied from Î³Ë†q,t < 2Î³Ë†k+1 and thus, 1/Î³Ë†k+1 < 2/Î³Ë†q,t. The third
equality is the direct definition of  Ë†k
. The last inequality is implied from kË† + 1 = log2(M1/Î³Ë†k+1)
and again, 1/Î³Ë†k+1 < 2/Î³Ë†q,t. The conclusion is proved immediately if we do a transform from  to
Î´.
19
