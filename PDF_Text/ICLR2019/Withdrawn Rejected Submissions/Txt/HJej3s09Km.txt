Under review as a conference paper at ICLR 2019
ON THE EFFECT OF THE ACTIVATION FUNCTION
ON THE DISTRIBUTION OF HIDDEN NODES
IN A DEEP NETWORK
Anonymous authors
Paper under double-blind review
ABSTRACT
We analyze the joint probability distribution on the lengths of the vectors of hidden
variables in different layers of a fully connected deep network, when the weights
and biases are chosen randomly according to Gaussian distributions, and the input
is in {âˆ’1, 1}N . We show that, if the activation function Ï† satisfies a minimal set
of assumptions, satisfied by all activation functions that we know that are used
in practice, then, as the width of the network gets large, the â€œlength processâ€
converges in probability to a length map that is determined as a simple function of
the variances of the random weights and biases, and the activation function Ï†.
We also show that this convergence may fail for Ï† that violate our assumptions.
1 INTRODUCTION
The size of the weights of a deep network must be managed delicately. If they are too large, signals
blow up as they travel through the network, leading to numerical problems, and if they are too small,
the signals fade away. The practical state of the art in deep learning made a significant step forward
due to schemes for initializing the weights that aimed in different ways at maintaining roughly
the same scale for the hidden variables before and after a layer [9, 4]. Later work [7, 14, 2] took
into account the effect of the non-linearities on the length dynamics of a deep network, informing
initialization policies in a more refined way.
In this paper, we continue this line of work, theoretically analyzing what might be called the â€œlength
processâ€. That is, for a given input, chosen for simplicity from {âˆ’1, 1}N , we study the probability
distribution over the lengths of the vectors of hidden variables, when the parameters of a deep
network are chosen randomly. We analyze the case of fully connected networks, with the same
activation function Ï† at each hidden node and N hidden variables in each layer. As in [14], we
consider the case where weights between nodes are chosen from a zero-mean Gaussian with variance
Ïƒ2w/N, and where the biases are chosen from a zero-mean distribution with variance Ïƒb2.
Our first result holds for activation functions Ï† that satisfy the following properties: (a) the restriction
of Ï† to any finite interval is bounded; (b) as z gets large, |Ï†(z)| = exp(o(z2
)); (c) Ï† is measurable.
We refer to such Ï† as permissible. Note that conditions (a) and (c) both hold for any non-decreasing
Ï†.
We show that, for all permissible Ï† and all Ïƒw and Ïƒb, as N gets large, the length process converges
in probability to a length map that is a simple function of Ï†, Ïƒw and Ïƒb. This length map was first
discovered in [14], where it was claimed that it holds for all Ï†; it has since been used in a number of
other papers [15, 17, 12, 10, 16, 1, 13, 5].
In Section 4, to motivate our new analysis, we provide examples of Ï† that are not permissible that
lead the length processes with arguably surprising properties. For example, we show that, for arbiï¿¾trarily small positive Ïƒw, even if Ïƒb = 0, for Ï†(z) = 1/z, the distribution of values of each of the
hidden nodes in the second layer diverges as N gets large. For finite N, each node has a Cauchy
distribution, which already has infinite variance, and as N gets large, the scale parameter of the
Cauchy distribution gets larger, leading to divergence. We also show that the hidden variables in the
1
Under review as a conference paper at ICLR 2019
second layer may not be independent, even for some permissible Ï† like the ReLU. The results of
this section contradict claims made in [14, 10].
Section 5 describes some simulation experiments verifying some of the findings of the paper, and
illustrating the dependence among the values of the hidden nodes.
Our analysis of the convergence of the length map borrows ideas from Daniely, et al. [2], who
studied the properties of the mapping from inputs to hidden representations resulting from random
Gaussian initialization. Their theory applies in the case of activation functions with certain smoothï¿¾ness properties, and to a wide variety of architectures. Our analysis treats a wider variety of values
of Ïƒw and Ïƒb, and uses weaker assumptions on Ï†.
2 PRELIMINARIES
2.1 NOTATION
For n âˆˆ N, we use [n] to denote the set {1, 2, . . . , n}. If T is a n Ã— m Ã— p tensor, then, for i âˆˆ [n],
let Ti,:,: = h Ti,j,ki jk, and define Ti,j,:
, etc., analogously.
2.2 THE FINITE CASE
Consider a deep fully connected width-N network with D layers. Let W âˆˆ RDÃ—NÃ—N . An activaï¿¾tion function Ï† maps R to R; we will also use Ï† to denote the function from RN to RN obtained
by applying Ï† componentwise. Computation of the neural activity vectors x0,:
, ..., xD,: âˆˆ RN and
preactivations h1,:
, ..., hD,: âˆˆ RN proceeds in the standard way as follows:
h`,: = W`,:,:x` âˆ’1,: + b`,: x`,: = Ï†(h`,:), for ` = 1, . . . , D.
We will study the process arising from fixing an arbitrary input x0,: âˆˆ {âˆ’1, 1}N and choosing the
parameters independently at random: the entries of W are sampled from Gauss  0, Ïƒ2wN 
, and the
entries of b from Gauss ï¿¾ 0, Ïƒb2
. For each ` âˆˆ [D], define q` = 1N P Ni=1 h2
`,i.
Note that for all ` â‰¥ 1, all the components of h`,: and x`,: are identically distributed.
2.3 THE WIDE-NETWORK LIMIT
For the purpose of defining a limit, assume that, for a fixed, arbitrary function Ï‡ : N â†’ {âˆ’1, 1}, for
finite N, we have x0,: = (Ï‡(1), ..., Ï‡(N)). For ` > 0, if the limit exists (in the sense of â€œconvergence
in distributionâ€), let x` be a random variable whose distribution is the limit of the distribution of x`,1
as N goes to infinity. Define h` and q` similarly.
2.4 TOTAL VARIATION DISTANCE
If P and Q are probability distributions, then dT V (P, Q) = supE P(E) âˆ’ Q(E), and if p and q are
their densities, dT V (P, Q) = 12 R |p(x) âˆ’ q(x)| dx.
3 CONVERGENCE IN PROBABILITY
In this section we characterize the length map of the hidden nodes of a deep network, for all activaï¿¾tion functions satisfying the following assumptions.
Definition 1 An activation function Ï† is permissible if, (a) the restriction of Ï† to any finite interval
is bounded; (b) |Ï†(x)| = exp(o(x2
)) as |x| gets large.1
; and (c) Ï† is measurable.
Conditions (b) and (c) ensure that a key integral can be computed. The proof of Lemma 1 is in
Appendix A.
1 This condition may be expanded as follows, limsupxâ†’âˆž
log |Ï†(x)| x2 = 0 and limsupxâ†’âˆ’âˆž
log |Ï†(x)| x2 = 0. 2
Under review as a conference paper at ICLR 2019
Lemma 1 If Ï† is permissible, then, for all positive constants c, the function g defined by g(x) =
Ï†(cx)2
exp(âˆ’x2/2) is integrable.
Now, we recall the definition of a length map from [14]; we will prove that the the length process
converges to this length map. Define qËœ0, ..., qËœD and rËœ0, ..., rËœD recursively as follows. First qËœ0 = rËœ0 = 1. Then, for ` > 0, qËœ` = Ïƒ2wrËœ` âˆ’1 + Ïƒb2
and
rËœ` = EzâˆˆGauss(0,1)[Ï†(p qËœ` z)2].
If Ï† is permissible, then, since Ï†(cz)2
exp(âˆ’z2/2) is integrable for all c, we have that
qËœ0, ..., qËœD, rËœ0, ..., rËœD are well-defined finite real numbers.
The following theorem shows that the length map q0, ..., qD converges in probability to qËœ0, ..., qËœD.
Theorem 2 For any permissible Ï†, Ïƒw, Ïƒb â‰¥ 0, any depth D, and any , Î´ > 0, there is an N0 such
that, for all N â‰¥ N0, with probability 1 âˆ’ Î´, for all ` âˆˆ {0, ..., D}, we have |q` âˆ’ qËœ` | â‰¤ .
The rest of this section is devoted to proving Theorem 2. Our proof will use the weak law of large
numbers.
Lemma 3 ([3]) For any random variable X with a finite expectation, and any , Î´ > 0, there is an
N0 such that, for all N â‰¥ N0, if X1, ..., XN are i.i.d. with the same distribution as X, then
Pr 

 

 E[X] âˆ’ 1N NXi=1
Xi > ! â‰¤ Î´.
In order to divide our analysis into cases, we need the following lemma, whose proof is in Apï¿¾pendix B.
Lemma 4 If Ï† is permissible and not zero a.e., for all Ïƒw > 0, for all ` âˆˆ {0, ..., D}, qËœ` > 0 and
rËœ` > 0.
We will also need a lemma that shows that small changes in Ïƒ lead to small changes in Gauss(0, Ïƒ2).
Lemma 5 (see [8]) There is an absolute constant C such that, for all Ïƒ1, Ïƒ2 > 0, dT V (Gauss(0, Ïƒ21), Gauss(0, Ïƒ22
)) â‰¤ C |Ïƒ1âˆ’Ïƒ2| Ïƒ1 .
The following technical lemma is proved in Appendix C.
Lemma 6 If Ï† is permissible, for all 0 < r â‰¤ s, for all Î² > 0, there is an a â‰¥ 0 such that, for all
q âˆˆ [r, s], R aâˆž Ï†(âˆšqz)2
exp(âˆ’z2/2) dz â‰¤ Î² and R âˆ’a
âˆ’âˆž Ï†(âˆšqz)2
exp(âˆ’z2/2) dz â‰¤ Î².
Armed with these lemmas, we are ready to prove Theorem 2.
First, if Ï† is zero a.e., or if Ïƒw = 0, Theorem 2 follows directly from Lemma 3, together with a
union bound over the layers. Assume for the rest of the proof that Ï†(x) is not zero a.e., and that
Ïƒw > 0, so that qËœ` > 0 and rËœ` > 0 for all ` .
For each ` âˆˆ [D], define r` = 1N P Ni=1 x2
`,i.
Our proof of Theorem 2 is by induction. The inductive hypothesis is that, for any , Î´ > 0 there
is an N0 such that, if N â‰¥ N0, then, with probability 1 âˆ’ Î´, for all ` 0 â‰¤ ` , |q` 0 âˆ’ qËœ` 0 | â‰¤  and
|r` 0 âˆ’ rËœ` 0 | â‰¤  .
The base case holds because q0 = Ëœq0 = r0 = Ëœr0 = 1, no matter what the value of N is.
Now for the induction step; choose ` > 0, 0 <  < min{qËœ` /4, rËœ` } and 0 < Î´ â‰¤ 1/2. (Note that
these choices are without loss of generality.) Let  0 âˆˆ (0, ) take a value that will be described later,
using quantities from the analysis. By the inductive hypothesis, whatever the value of  0 , there is an
N00
such that, if N â‰¥ N00
, then, with probability 1 âˆ’ Î´/2, for all ` 0 â‰¤ ` âˆ’ 1, we have |q` 0 âˆ’ qËœ` 0 | â‰¤  0
and |r` 0 âˆ’ rËœ` 0 | â‰¤  0 . Thus, to establish the inductive step, it suffices to show that, after conditioning
3
Under review as a conference paper at ICLR 2019
on the random choices before the ` th layer, if |q` âˆ’1 âˆ’ qËœ` âˆ’1| â‰¤  0 , and |r` âˆ’1 âˆ’ rËœ` âˆ’1| â‰¤  0 , there is
an N` such that, if N â‰¥ N` , then with probability at least 1 âˆ’ Î´/2 with respect only to the random
choices of W`,:,: and b`,:
, that |q` âˆ’ qËœ` | â‰¤  and |r` âˆ’ rËœ` | â‰¤  . Given such an N` , the inductive step
can be satisfied by letting N0 be the maximum of N00
and N` .
Let us do that. For the rest of the proof of the inductive step, let us condition on outcomes of the
layers before layer ` , and reason about the randomness only in the ` th layer. Let us further assume
that |q` âˆ’1 âˆ’ qËœ` âˆ’1| â‰¤  0 and |r` âˆ’1 âˆ’ rËœ` âˆ’1| â‰¤  0 .
Recall that q` = 1N P Ni=1 h2
`,i. Since we have conditioned on the values of h` âˆ’1,1, ..., h` âˆ’1,N ,
each component of h`,i is obtained by taking the dot-product of x` âˆ’1,: = Ï†(h` âˆ’1,:) with W`,i,:
and adding an independent b`,i. Thus, conditioned on h` âˆ’1,1, ..., h` âˆ’1,N , we have that h`,1, ..., h`,N
are independent. Also, since x` âˆ’1,:
is fixed by conditioning, each h`,i has an identical Gaussian
distribution.
Since each component of W and b has zero mean, each h`,i has zero mean.
Choose an arbitrary i âˆˆ [N]. Since x` âˆ’1,:
is fixed by conditioning and W`,i,1, ..., W`,i,N and b`,i are
independent,
E[q` ] = E[h2
`,i] = Ïƒb2 + Ïƒ2wN X j x2`âˆ’1,j = Ïƒb2 + Ïƒ2wr` âˆ’1
def = q` . (1)
We wish to emphasize the q` is determined as a function of random outcomes before the ` th layer,
and thus a fixed, nonrandom quantity, regarding the randomization of the ` th layer. By the inductive
hypothesis, we have
|E[q` ] âˆ’ qËœ` | = |E[h2
`,i] âˆ’ qËœ` | = |q` âˆ’ qËœ` | = Ïƒ2w|r` âˆ’1 âˆ’ rËœ` âˆ’1| â‰¤  0 Ïƒ2w. (2)
The key consequence of this might be paraphrased by saying that, to establish the portion of the inï¿¾ductive step regarding q` , it suffices for q` to be close to its mean. Now, we want to prove something
similar for r` . We have
E[r` ] = 1N NXi=1
E[x2
`,i] = 1N NXi=1
E[Ï†(h`,i)2
] = E[Ï†(h`,1)2],
since h`,1, ..., h`,N are i.i.d. Recall that, earlier, we showed that h`,i âˆ¼ Gauss(0, q` ). Thus
E[r` ] = Ezâˆ¼Gauss(0,q` )[Ï†(z)2
] = Ezâˆ¼Gauss(0,1)[Ï†(p q` z)2
] = r
21Ï€ Z Ï†(p q` z)2
exp(âˆ’z2/2) dz.
which gives
|E[r` ] âˆ’ rËœ` | â‰¤
  Ezâˆ¼Gauss(0,q` )[Ï†(z)2] âˆ’ Ezâˆ¼Gauss(0,qËœ` )[Ï†(z)2] .
Since |q` âˆ’ qËœ` | â‰¤  0 Ïƒ2w and we may choose  0 to ensure  0 â‰¤ Ëœq`2Ïƒ2w
, we have qËœ` /2 â‰¤ q` â‰¤ 2Ëœq` .
For Î² > 0 and Îº âˆˆ (0, 1/2) to be named later, by Lemma 6, we can choose a such that, for all
q âˆˆ [Ëœq` /2, 2Ëœq` ], Z âˆ’a
âˆ’âˆž
Ï†(âˆš
qz)2
exp(âˆ’z2/2) dz â‰¤ Î²/2 and Z
aâˆž Ï†(âˆš
qz)2
exp(âˆ’z2/2) dz â‰¤ Î²/2
and âˆš21
Ï€q
R
aâˆ’a
exp  âˆ’z22q 
dz â‰¥ 1 âˆ’ Îº. Choose such an a. 4
Under review as a conference paper at ICLR 2019
We claim that
 
 R aâˆ’a Ï†(âˆšqz)2
exp(âˆ’z2/2) dz âˆ’ R Ï†(âˆšqz)2
exp(âˆ’z2/2) dz

 â‰¤ Î² for all qËœ` /2 < q â‰¤ 2Ëœq` . Choose such a q. We have




Z
aâˆ’a Ï†(âˆš
qz)2
exp(âˆ’z2/2) dz âˆ’ Z Ï†(âˆš
qz)2
exp(âˆ’z2/2) dz



= Z âˆ’a
âˆ’âˆž
Ï†(âˆš
qz)2
exp(âˆ’z2/2) dz + Z aâˆž Ï†(âˆš
qz)2
exp(âˆ’z2/2) dz
â‰¤ 2 max  Z
âˆ’a
âˆ’âˆž
Ï†(âˆš
qz)2
exp(âˆ’z2/2) dz, Z
âˆža Ï†(âˆš
qz)2
exp(âˆ’z2/2) dz
â‰¤ Î².
So now we are trying to bound
   R aâˆ’a Ï†(âˆšq` z)2
exp(âˆ’z2/2) dz âˆ’ R aâˆ’a Ï†(âˆšqËœ` z)2
exp(âˆ’z2/2) dz


using qËœ` /2 â‰¤ q` â‰¤ 2Ëœq` .
Using changes of variables, we have




Z
aâˆ’a Ï†(p q` z)2
exp(âˆ’z2/2) dz âˆ’ Z âˆ’aa Ï†(p qËœ` z)2
exp(âˆ’z2/2) dz



=      1âˆšq` Z aâˆšq` âˆ’aâˆšq` Ï†(z)2
exp  âˆ’ z2 2q` 
dz âˆ’ âˆš1qËœ` Z aâˆšqËœ` âˆ’aâˆšqËœ` Ï†(z)2
exp  âˆ’ z2
2Ëœq` 
dz



 .
Since Ï† is permissible, Ï†2
is bounded on [âˆ’aâˆš
2Ëœq` , aâˆš
2Ëœq` ]. If P is the distribution obtained by conï¿¾ditioning Gauss(0, q` ) on [âˆ’aâˆšq` , aâˆšq` ], and ËœP by conditioning Gauss(0, qËœ` ) on [âˆ’aâˆšqËœ` , aâˆšqËœ` ],
then if M = âˆš2Ï€ supzâˆˆ[âˆ’aâˆš
2Ëœq` ,aâˆš
2Ëœq` ] Ï†(z)2
, since q` â‰¤ 2Ëœq` ,      1âˆšq` Z aâˆšq` âˆ’aâˆšq` Ï†(z)2
exp(âˆ’ z2 2q` ) dz âˆ’ 1âˆšqËœ` Z aâˆšqËœ` âˆ’aâˆšqËœ` Ï†(z)2
exp(âˆ’ z2
2Ëœq` ) dz



 â‰¤ M dT V (P, PËœ).
But since, for Îº < 1/2, conditioning on an event of probability at least 1 âˆ’ Îº only changes a
distribution by total variation distance at most 2Îº, and therefore, applying Lemma 5 along with the
fact that |q` âˆ’ qËœ` | â‰¤  0 Ïƒ2w, for the constant C from Lemma 5, we get
dT V (P, PËœ) â‰¤ 4Îº + dT V (Gauss(0, q` ), Gauss(0, qËœ` ))
â‰¤ 4Îº + C|âˆšq` âˆ’ âˆšqËœ` | âˆšqËœ`
= 4Îº + C|q` âˆ’ Ëœq` | |âˆšq` + âˆšqËœ` |âˆšqËœ` â‰¤ 4Îº +
C0 Ïƒ2 qËœ` w .
Tracing back, we have




Z
âˆ’aa Ï†(p q` z)2
exp(âˆ’z2/2) dz âˆ’ Z âˆ’aa Ï†(p qËœ` z)2
exp(âˆ’z2/2) dz


 â‰¤ M  4Îº +
C0 Ïƒ2w qËœ` 
which implies
|E[r` ] âˆ’ rËœ` | â‰¤

 
 Z Ï†(p q` z)2
exp(âˆ’z2/2) dz âˆ’ Z Ï†(p qËœ` z)2
exp(âˆ’z2/2) dz



â‰¤ M  4Îº +
C0 Ïƒ2w qËœ` 
+ 2Î².
If Îº = min{ 24
M , 13 }, Î² =  12 , and  0 = min n  2 ,  2Ïƒ2w , Ëœq`2Ïƒ2w , qËœ`  6CMÏƒ2w o
this implies |E[r` ] âˆ’ rËœ` | â‰¤
/2. 5
Under review as a conference paper at ICLR 2019
Recall that q` is an average of N identically distributed random variables with a mean between 0
and 2Ëœq` (which is therefore finite) and r` is an average of N identically distributed random variables,
each with mean between 0 and rËœ` +/2 â‰¤ 2Ëœr` . Applying the weak law of large numbers (Lemma 3),
there is an N` such that, if N â‰¥ N` , with probability at least 1 âˆ’ Î´/2, both |q` âˆ’ E[q` ]| â‰¤ /2 and
|r` âˆ’ E[r` ]| â‰¤ /2 hold, which in turn implies |q` âˆ’ qËœ` | â‰¤  and |r` âˆ’ rËœ` | â‰¤  , completing the proof
of the inductive step, and therefore the proof of Theorem 2.
4 DIVERSITY OF BEHAVIOR IN THE DISTRIBUTION OF HIDDEN NODES
In this section, we show that, for some activation functions, the probability distribution of hidden
nodes can have some surprising properties.
4.1 NON-GAUSSIAN
In this subsection, we will show that the hidden variables are sometimes not Gaussian. Our proof
will refer to the Cauchy distribution.
Definition 2 A distribution over the reals that, for x0 âˆˆ R and Î³ > 0, has a density f given by
f(x) = 1
Ï€Î³h 1+( xâˆ’Î³x0 )2i is a Cauchy distribution, denoted by Cauchy(x0, Î³). Cauchy(0, 1) is the
standard Cauchy distribution.
Lemma 7 ([6]) If X1, ..., Xn are i.i.d. random variables with a Cauchy distribution, then
1n P ni=1 Xi has the same distribution.
Lemma 8 ([11]) If U and V are zero-mean normally distributed random variables with the same
variance, then U/V has the standard Cauchy distribution.
The following shows that there is a Ï† such that the limiting h2
is not defined. It contradicts claims
made on line 7 of Section A.1 of [14] and line 7 of Section 2.2 of [10].
Proposition 9 There is a Ï† such that, for every Ïƒw > 0, if Ïƒb = 0, then (a) for finite N, h2,1 does
not have a Gaussian distribution, and (b) h2,1 diverges as N goes to infinity.
Proof: Consider Ï† defined by Ï†(y) =  1/y if y 6 = 0
0 if y = 0.
Fix a value of N and Ïƒw > 0, and take Ïƒb = 0. Each component of h1,:
is a sum of zero-mean
Gaussians with variance Ïƒ2w/N; thus, for all i, h1,i âˆ¼ Gauss(0, Ïƒ2w). Now, almost surely, h2,1 = P Nj=1 W2,1,jÏ†(h1,j ) = P Nj=1 W2,1,j/h1,j . By Lemma 8, for each j, W2,1,j/h1,j has a Cauchy
distribution, and since (NW2,1,1), ...,(NW2,1,N ) âˆ¼ Gauss(0, NÏƒ2w), recalling that h1,1, ..., h1,N âˆ¼
Gauss(0, Ïƒ2w), we have that NW2,1,1/h1,1, ..., NW2,1,N /h1N are i.i.d. Cauchy(0, âˆšN). Applying
Lemma 7, h2,1 = P Nj=1 W2,1,jÏ†(h2,j ) = 1N P Nj=1 NW2,1,jÏ†(h1,j ) is also Cauchy(0, âˆšN).
So, for all N, h2,1 is Cauchy(0, âˆšN). Suppose that h2,1 converged in distribution to some disï¿¾tribution P. Since the cdf of P can have at most countably many discontinuities, we can cover
the real line by a countable set of finite-length intervals [a1, b1], [a2, b2], ... whose endpoints are
points of continuity for P. Since Cauchy(0, âˆšN) converges to P in distribution, for any i, P([ai
, bi
]) â‰¤ limNâ†’âˆž
|biâˆ’ai| Ï€âˆšN
= 0. Thus, the probability assigned by P to the entire real line
is 0, a contradiction.
4.2 INDEPENDENCE
The following contradicts a claim made on line 8 of Section A.1 of [14].
Theorem 10 If Ï† is either the ReLU or the Heaviside function, then, for every Ïƒw > 0, Ïƒb â‰¥ 0, and
N â‰¥ 2, (h2,1, ..., h2,N ) are not independent.
6
Under review as a conference paper at ICLR 2019
Proof: We will show that E[h22,1h22,2] 6 = E[h22,1]E[h22,2], which will imply that h2,1 and h2,2 are not
independent.
As mentioned earlier, because each component of h1,:
is the dot product of x0,: with an independent
row of W1,:,: plus an independent component of b1,:
, the components of h1,: are independent, and
since x1,: = Ï†(h1,:), this implies that the components of x1,: are independent. Since each row of
W1,:,: and each component of the bias vector has the same distribution, x1,:
is i.i.d.
We have
E[h22,1
] = E ï£®ï£¯ï£°ï£®ï£°ï£«ï£­iXâˆˆ[N] W2,1,ix1,i
ï£¶ï£¸ + b2,1ï£¹ï£»2ï£¹ï£ºï£» = X (i,j)âˆˆ[N]2 E [W2,1,iW2,1,jx1,ix1,j ] + X
iâˆˆ[N] E [W2,1,ix1,ib2,1] + E  b22,1 .
The components of W2,:,: and x1,:
, along with b2,1, are mutually independent, so terms in the double
sum with i 6 = j have zero expectation, and E[h22,1
] =  P iâˆˆ[N] E  W22,1,i E  x21,i  + E[b22,1]. For
a random variable x with the same distribution as the components of x1,:
, this implies
E[h22,1
] = Ïƒ2wE  x2 + Ïƒb2. (3)
Similarly,
E[h22,1h22,2] = E ï£®ï£¯ï£°ï£®ï£°iXâˆˆ[N] W2,1,ix1,i + b2,1ï£¹ï£»2 ï£®ï£°iXâˆˆ[N] W2,2,ix1,i + b2,2ï£¹ï£»2ï£¹ï£ºï£» = X (i,j,r,s)âˆˆ[N]4 E[W2,1,iW2,1,jW2,2,rW2,2,sx1,ix1,jx1,rx1,s]
+ 2 X
(i,j,r)âˆˆ[N]3 E[W2,1,iW2,1,jW2,2,rx1,ix1,jx1,rb2,2]+2
(i,r,s
X)âˆˆ[N]3 E[W2,1,iW2,2,rW2,2,sx1,ix1,rx1,sb2,1]
+ 4 X
(i,r)âˆˆ[N]2 E[W2,1,iW2,2,rx1,ix1,rb2,1b2,2] + X (i,j)âˆˆ[N]2 E[W2,1,iW2,1,jx1,ix1,j b22,2
] +
(r,s
X)âˆˆ[N]2 E[W2,2,rW2,2,sx1,rx1,sb22,1]
+ 2 X
iâˆˆ[N] E[W2,1,ix1,ib2,1b22,2
] + 2
rXâˆˆ[N] E[W2,2,rx1,rb22,1b2,2] + E[b22,1b22,2] = X (i,r)âˆˆ[N]2,i6=r E[W22,1,iW22,2,r]E[x21,i]E[x21,r] +
iXâˆˆ[N] E[W22,1,iW22,2,i]E[x41,i] + X iâˆˆ[N] E[W22,1,i]E[x21,i]E[b22,2
] +
rXâˆˆ[N] E[W22,2,r]E[x21,r]E[b22,1] + E[b21,2b22,2] = (N2 âˆ’ N)Ïƒ4wE[x2]2 N2 +
NÏƒ4wE[x4] N2 + 2NÏƒ2wE[x2]Ïƒb2 N + Ïƒb4 = Ïƒ4wE[x2]2 + Ïƒ4w(E[x4] âˆ’ E[x2]2) N
+ 2Ïƒ2wÏƒb2E[x2
] + Ïƒb4.
Putting this together with (3), we have
E[h22,1h22,2] âˆ’ E[h22,1]E[h22,2
] = Ïƒ4w(E[x4] âˆ’ E[x2]2) N . (4)
7
Under review as a conference paper at ICLR 2019
(a) N = 10 (b) N = 100 (c) N = 1000
Figure 1: Histograms of h[2, :], averaged over 100 random initializations, for N âˆˆ {10, 100, 1000},
along with Cauchy(0, âˆšN) (shown in green) and Gauss(0, Ïƒ2) for Ïƒ estimated from the data
(shown in red).
Now, we calculate the difference using (4) for the Heaviside and ReLU functions.
Heaviside. Suppose Ï† is Heaviside function, i.e. Ï†(z) is the indicator function for z > 0. In this
case, since the components of h1,: are symmetric about 0, the distribution of x1,:
is uniform over
{0, 1}N . Thus E[x4
] = E[x2
] = 1/2, and so (4) gives E[h22,1h22,2] âˆ’ E[h22,1]E[h22,2
] = 3Ïƒ4w 4N 6 = 0.
ReLU. Next, we consider the case that Ï† is the ReLU. Recalling that, for all i, h1,i âˆ¼ Gauss(0, Ïƒ2w),
we have E[x2
] = âˆš21
Ï€Ïƒ2w R 0âˆž z2
exp  âˆ’z2 2Ïƒ2w 
dz. By symmetry this is 12Ezâˆ¼Gauss(0,Ïƒ2w)[z2
] = Ïƒ2w/2.
Similarly, E[x4
] = 12Ezâˆ¼Gauss(0,Ïƒ2w)[z4
] = 3Ïƒ4 2
. Plugging these into (4) we get that, in the case the
Ï† is the ReLU, that
E[h22,1h22,2] âˆ’ E[h22,1]E[h22,2
] = Ïƒ4w ï¿¾ (3/2)Ïƒ4w âˆ’ Ïƒ4w/4 N = 5Ïƒ8w 4N > 0,
completing the proof.
4.3 UNDEFINED LENGTH MAP
Here, we show, informally, that for Ï† at the boundary of the second condition in the definition
of permissibility, the recursive formula defining the length map qËœ` breaks down. Roughly, this
condition cannot be relaxed.
Proposition 11 For any Î± > 0, if Ï† is defined by Ï†(x) = exp(Î±x2), there exists a Ïƒw, Ïƒb s.t. qËœ` , rËœ`
is undefined for all ` â‰¥ 2.
Proof: Suppose Ïƒ2w + Ïƒb2 = 14Î±2 . Then qËœ1 = 14Î±2 , so that
rËœ1 = 1 âˆš2Ï€ Z âˆž
âˆ’âˆž
Ï†(p qËœ1z) exp  âˆ’z22 
dz = âˆš12Ï€ Z âˆž
âˆ’âˆž
exp(Î±p qËœ1z2
) exp  âˆ’z22 
dz
= 1 âˆš2Ï€ Z âˆž
âˆ’âˆž
exp(z2/2) exp  âˆ’z22 
dz = âˆž,
and downsteam values of qËœ` and rËœ` are undefined.
5 EXPERIMENTS
Our first experiment fixed x[0, :] = (1, ..., 1), Ïƒw = 1, Ïƒb = 0, Ï†(z) = 1/z.
For each N âˆˆ {10, 100, 1000}, we (a) initialized the weights 100 times, (b) plotted the hisï¿¾tograms of all of the values of h[2, :], along with the Cauchy(0, âˆšN) distribution from the proof
of Proposition 9, and Gauss(0, Ïƒ2) for Ïƒ estimated from the data. Consistent with the theory, the
Cauchy(0, âˆšN) distribution fits the data well.
To illustrate the fact that the values in the second hidden layer are not independent, for N = 1000
and the parameters otherwise as in the other experiment, we plotted histograms of the values seen
8
Under review as a conference paper at ICLR 2019
Figure 2: Histograms of h[2, :] for nine random weight initializations.
in the second layer for nine random initializations of the weights in Figure 2. When some of the
values in the first hidden layer have unusually small magnitude, then the values in the second hidden
layer coordinately tend to be large. This is in contrast with the claim made at the end of Section
2.2 of [10]. Note that this is consistent with Theorem 2 establishing convergence in probability for
permissible Ï†, since the Ï† used in this experiment is not permissible.
REFERENCES
[1] M. Chen, J. Pennington, and S. S. Schoenholz. Dynamical isometry and a mean field theory
of RNNs: Gating enables signal propagation in recurrent neural networks. arXiv preprint
arXiv:1806.05394, 2018.
[2] A. Daniely, R. Frostig, and Y. Singer. Toward deeper understanding of neural networks: The
power of initialization and a dual view on expressivity. In Advances In Neural Information
Processing Systems, pages 2253â€“2261, 2016.
[3] W. Feller. An introduction to probability theory and its applications. John Wiley & Sons, 2008.
[4] X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence
and statistics, pages 249â€“256, 2010.
[5] S. Hayou, A. Doucet, and J. Rousseau. On the selection of initialization and activation function
for deep neural networks. arXiv preprint arXiv:1805.08266, 2018.
[6] M. Hazewinkel. Cauchy distribution. In Encyclopaedia of Mathematics: Volume 6. Springer
Science & Business Media, 2013.
[7] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level
performance on imagenet classification. In Proceedings of the IEEE international conference
on computer vision, pages 1026â€“1034, 2015.
[8] B. Klartag. A central limit theorem for convex sets. Inventiones mathematicae, 168(1):91â€“131,
2007.
[9] Y. A. LeCun, L. Bottou, G. B. Orr, and K. MÂ¨uller. Efficient backprop. In Neural networks:
Tricks of the trade. Springer, 1998.
9
Under review as a conference paper at ICLR 2019
[10] J. Lee, Y. Bahri, R. Novak, S. S. Schoenholz, J. Pennington, and J. Sohl-Dickstein. Deep neural
networks as gaussian processes. ICLR, 2018.
[11] R. Lupton. Statistics in theory and practice. Princeton University Press, 1993.
[12] J. Pennington, S. Schoenholz, and S. Ganguli. Resurrecting the sigmoid in deep learning
through dynamical isometry: theory and practice. In Advances in neural information processï¿¾ing systems, pages 4785â€“4795, 2017.
[13] J. Pennington, S. S. Schoenholz, and S. Ganguli. The emergence of spectral universality in
deep networks. arXiv preprint arXiv:1802.09979, 2018.
[14] B. Poole, S. Lahiri, M. Raghu, J. Sohl-Dickstein, and S. Ganguli. Exponential expressivity in
deep neural networks through transient chaos. In Advances in neural information processing
systems, pages 3360â€“3368, 2016.
[15] S. S. Schoenholz, J. Gilmer, S. Ganguli, and J. Sohl-Dickstein. Deep information propagation.
arXiv preprint arXiv:1611.01232, 2016.
[16] L. Xiao, Y. Bahri, J. Sohl-Dickstein, S. S. Schoenholz, and J. Pennington. Dynamical isometry
and a mean field theory of CNNs: How to train 10,000-layer vanilla convolutional neural
networks. arXiv preprint arXiv:1806.05393, 2018.
[17] G. Yang and S. Schoenholz. Mean field residual networks: On the edge of chaos. In Advances
in neural information processing systems, pages 7103â€“7114, 2017.
A PROOF OF LEMMA 1
Choose c > 0. Since limsupxâ†’âˆž
log |Ï†(x)| x2 = 0 and limsupxâ†’âˆ’âˆž
log |Ï†(x)| x2 = 0, we also have
limsupxâ†’âˆž
log |Ï†(cx)| x2 = 0 and limsupxâ†’âˆ’âˆž
log |Ï†(cx)| x2 = 0. Thus, there is an a such that, for all
x 6âˆˆ [âˆ’a, a], log |Ï†(cx)| â‰¤ x28
, which implies Ï†(cx)2 â‰¤ exp  x24 
. Since Ï† is permissible, it is
bounded on [âˆ’a, a]. Thus, we have
Z
Ï†(cx)2
exp(âˆ’x2/2) dx
= Z âˆ’a
âˆ’âˆž
Ï†(cx)2
exp(âˆ’x2/2)dx + Z âˆ’aa Ï†(cx)2
exp(âˆ’x2/2)dx + Z aâˆž Ï†(cx)2
exp(âˆ’x2/2)dx
â‰¤ Z âˆ’a
âˆ’âˆž
exp(âˆ’x2/4)dx + sup
xâˆˆ[âˆ’a,a] Ï†(cx)2! Z âˆ’aa
exp(âˆ’x2/2)dx + Z aâˆž
exp(âˆ’x2/4)dx
< âˆž
completing the proof.
B PROOF OF LEMMA 4
The proof is by induction. The base case holds since qËœ0 = Ëœr0 = 1.
To prove the inductive step, we need the following lemma.
Lemma 12 If Ï† is not zero a.e., then, for all c > 0, EzâˆˆGauss(0,1)(Ï†(cz)2) > 0.
Proof: If Âµ is the Lebesgue measure, since
Âµ({x âˆˆ R : Ï†2(cx) > 0}) = limnâ†’âˆž
Âµ({x : Ï†2(cx) > 1/n} âˆ© [âˆ’n, n]) > 0,
there exists n such that Âµ({x : Ï†2(cx) > 1/n} âˆ© [âˆ’n, n]) > 0. For such an n, we have
EzâˆˆGauss(0,1)(Ï†(cz)2) â‰¥ 1neâˆ’n2/2Âµ({x : Ï†2(cx) > 1/n} âˆ© [âˆ’n, n]) > 0.
Returning to the proof of Lemma 4, by the inductive hypothesis, rËœ` âˆ’1 > 0, which, since Ïƒw > 0,
implies qËœ` > 0. Applying Lemma 12 yields rËœ` > 0.
10
Under review as a conference paper at ICLR 2019
C PROOF OF LEMMA 6
Since limsupxâ†’âˆž
log |Ï†(x)| x2 = 0 there is an b such that, for all x â‰¥ b, log |Ï†(x)| â‰¤ x28s
, which implies
Ï†(x)2 â‰¤ exp  x24s 
. Now, choose q âˆˆ [r, s]. For a = b/âˆšr, we then have
Z
âˆža Ï†(âˆš
qx)2
exp(âˆ’x2/2) dx
= 1âˆšq Z aâˆžâˆšq Ï†(z)2
exp  âˆ’z2 2q 
dz
â‰¤ 1âˆšq Z aâˆžâˆšq
exp 
z2 4s
exp  âˆ’z2 2q 
dz
â‰¤ 1âˆšq Z aâˆžâˆšq
exp  âˆ’z2 4q 
dz
â‰¤ 1âˆšq Z bâˆž
exp  âˆ’z2 4q 
dz.
By increasing b if necessary, we can ensure âˆš1q R bâˆž exp  âˆ’z24q 
dz â‰¤ Î² which
then gives R âˆža Ï†(âˆšqx)2
exp(âˆ’x2/2) dx â‰¤ Î². A symmetric argument yields
R
a
âˆ’âˆž Ï†(âˆšqx)2
exp(âˆ’x2/2) dx â‰¤ Î², completing the proof.
11
