Under review as a conference paper at ICLR 2019
Composing Entropic Policies using Divergence
Correction
Anonymous authors
Paper under double-blind review
Ab stract
Deep reinforcement learning (RL) algorithms have made great strides in recent
years. An important remaining challenge is the ability to quickly transfer exist-
ing skills to novel tasks, and to combine existing skills with newly acquired ones.
In domains where tasks are solved by composing skills this capacity holds the
promise of dramatically reducing the data requirements of deep RL algorithms,
and hence increasing their applicability. Recent work has studied ways of com-
posing behaviors represented in the form of action-value functions. We analyze
these methods to highlight their strengths and weaknesses, and point out situa-
tions where each of them is susceptible to poor performance. To perform this
analysis we extend generalized policy improvement to the max-entropy frame-
work and introduce a method for the practical implementation of successor fea-
tures in continuous action spaces. Then we propose a novel approach which, in
principle, recovers the optimal policy during transfer. This method works by ex-
plicitly learning the (discounted, future) divergence between policies. We study
this approach in the tabular case and propose a scalable variant that is applicable
in multi-dimensional continuous action spaces. We compare our approach with
existing ones on a range of non-trivial continuous control problems with com-
positional structure, and demonstrate qualitatively better performance despite not
requiring simultaneous observation of all task rewards.
1	Introduction
Reinforcement learning algorithms coupled with powerful function approximators have recently
achieved a series of successes (Mnih et al., 2015; Silver et al., 2016; Lillicrap et al., 2015; Kalash-
nikov et al., 2018). Unfortunately, while being extremely powerful, deep reinforcement learning
(DRL) algorithms often require a large number of interactions with the environment to achieve good
results, partially because they are often applied “from scratch” rather than in settings where they can
leverage existing experience. This reduces their applicability in domains where generating experi-
ence is expensive, or learning from scratch is challenging.
The data efficiency of DRL algorithms is affected by various factors and significant research effort
has been directed at achieving improvements (e.g. Popov et al., 2017). At the same time the de-
velopment of basic locomotor behavior in humans can, in fact, require large amounts of experience
and practice (Adolph et al., 2012), and it can take significant effort and training to master complex,
high-speed skills (Haith & Krakauer, 2013). Once such skills have been acquired, however, humans
rapidly put them to work in new contexts and to solve new tasks, suggesting transfer learning as an
important mechanism.
Transfer learning has been explored extensively in multiple fields of the machine learning commu-
nity (see e.g. Weiss et al., 2016, for a recent review). In RL and robotics the transfer of knowledge
from one task to another has been studied from a variety of angles.
For the purpose of this paper we are interested in methods that are suitable for transfer in the context
of high-dimensional motor control problems. We further focus on model-free approaches, which are
evident in human motor control (Haith & Krakauer, 2013), and have recently been used by a variety
of scalable deep RL methods (e.g. Lillicrap et al., 2015; Mnih et al., 2015; Schulman et al., 2017;
Kalashnikov et al., 2018).
1
Under review as a conference paper at ICLR 2019
Transfer may be especially valuable in domains where a small set of skills can be composed, in
different combinations, to solve a variety of tasks. Different notions of compositionality have been
considered in the RL and robotics literature. For instance, ‘options’ are associated with discrete
units of behavior that can be sequenced, thus emphasizing composition in time (Precup et al., 1998).
In this paper we are concerned with a rather distinct notion of compositionality, namely how to com-
bine and blend potentially concurrent behaviors. This form of composition is particularly relevant
in high-dimensional continuous action spaces, where it is possible to achieve more than one task
simultaneously (e.g. walking somewhere while juggling).
One approach to this challenge is via the composition of task rewards. Specifically, we are interested
in the following question: If we have previously solved a set of tasks with similar transition dynamics
but different reward functions, how can we leverage this knowledge to solve new tasks which can be
expressed as a convex combination of those rewards functions?
This question has recently been studied in two independent lines of work: by Barreto et al. (2017;
2018) in the context of successor feature (SF) representations used for Generalized Policy Improve-
ment (GPI) with deterministic policies, and by Haarnoja et al. (2018a); van Niekerk et al. (2018) in
the context of maximum entropy policies. These approaches operate in distinct frameworks but both
achieve skill composition by combining the Q-functions associated with previously learned skills.
We clarify the relationship between the two approaches and show that both can perform well in some
situations but achieve poor results in others, often in complementary ways. We introduce a novel
method of behavior composition that that can consistently achieve good performance.
Our contributions are as follows:
1.	We introduce succcessor features (SF) in the context of maximum entropy and extend the
GPI theorem to this case (max-ent GPI).
2.	We provide an analysis of when GPI, and compositional “optimism” (Haarnoja et al.,
2018a) of entropy-regularized policies transfer. We construct both tabular and continuous
action tasks where both fail to transfer well.
3.	We propose a correction term - which We call Divergence Correction (DC)- based on the
Renyi divergence between policies which allows us, in principle, to recover the optimal
policy for transfer for any convex combination of rewards.
4.	We demonstrate a practical implementation of these methods in continuous action spaces
using adaptive importance sampling and compare the approaches introduced here: max-ent
GPI and DC with optimism(Haarnoja et al., 2018a) and Conditional Q functions (Schaul
et al., 2015) in a variety of non-trivial continuous action transfer tasks.
2	Background
2.1	Multi-task RL
We consider Markov Decision Processes defined by the tuple M containing: a state space S, action
space A, a start state distribution p(s1), a transition function p(st+1 |st, at), a discount γ ∈ [0, 1) and
a reward function r(st, at, st+ι). The objective of RL is to find a policy π(a∣s) : S → P(A) which
maximises the discounted expected return from any state J(π) = Eπ,M [Pτ∞=t γτ-trτ] where the
expected reward is dependent on the policy π and the MDP M.
We formalize transfer as in Barreto et al. (2017); Haarnoja et al. (2018a), as the desire to perform
well across all tasks in a set M ∈ T0 after having learned policies for tasks M ∈ T, without addi-
tional experience. We assume that T and T0 are related in two ways: all tasks share the same state
transition function, and tasks in T0 can be expressed as convex combinations of rewards associated
with tasks in set T. So ifwe write the reward functions for tasks in T as the vector φ = (r1, r2, . . . ),
tasks in T0 can be expressed as Tw = φ ∙ w.
We focus on combinations of two policies rb = bri + (1 - b)rj but the methods can be extended to
more than two tasks. We refer to a transfer method as optimal, ifit achieves optimal returns on tasks
in T0, using only experience on tasks T.
2
Under review as a conference paper at ICLR 2019
2.2	Successor Features
Successor Features (SF) (Dayan, 1993) and Generalised Policy Improvement (GPI) (Barreto et al.,
2017; 2018) provide a principled solution to transfer in the setting defined above. SF make the
additional assumption that the reward feature φ is fully observable, that is, the agent has access to
the rewards of all tasks in T but not T0 during training on each individual task.
The key observation of SF representations is that linearity of the reward rw with respect to the
features φ implies the following decomposition of the value policy of π:
∞
Qw(st,at) = En X YrTφτ ∙ w|at
τ=t
∞
En EYT-tΦτ|at ∙ W ≡ Ψπ(st,at) ∙ W
i=t
(1)
where ψn is the expected discounted sum of features φ induced by policy π . This decomposition
allows us to compute the action-value for π on any task w by learning ψn .
If we have a set of policies π1, π2, ..., πn indexed by i, SF and GPI provide a principled approach
to transfer on task πw. Namely, we act according to the deterministic GPI policy πwGPI(st) ≡
arg maxat QGwP I (st, at)) where
QGPI(St,at) ≡ maxi Qw (st,at) =maxi ψni (s,a) ∙ W	(2)
The GPI theorem guarantees the GPI policy has a return at least as good as any component policy,
GPI
that is, Vwn	(s) ≥ maxi Vwni (s) ∀s ∈ S.
2.3	Maximum Entropy RL
The maximum entropy (max-ent) RL objective augments the reward to favor entropic solutions
J(π) = EnM [P∞=τ Yτ-t(rτ + αH[π(∙lsτ川	⑶
where α is a parameter that determines the relative importance of the entropy term.
This objective has been considered in a number of works including Kappen (2005); Todorov (2009);
Haarnoja et al. (2017; 2018a); Ziebart et al. (2008); Fox et al. (2015).
We define the action-value Qn associated with eq. 3 as
Qn(st, at) ≡ r + En [Σ∞=t+1 ITT(IrT + αH[∏(∙∣Sτ)])]
(notice Qn(st, at) does not include any entropy terms for the state st). Soft Q iteration
Q(St, at) J r(st, at, st+1) + YEp(St+ι∣st,at) [V(St+1)]
V(St) J En [Q(st, at)] + αH]π(∙
|St)] = αlog
A
exp(1 Q(st,at))da ≡ αlogZ(St)
α
(4)
(5)
(6)
where ∏(at∣St) α exp( 1 Q(St,at)) converges to the optimal policy with standard assumptions
(Haarnoja et al., 2017).
3	Composing Policies in Max-Ent Reinforcement Learning
In this section we present two novel approaches for max-ent transfer learning. In section 4 we then
outline a practical method for making use of these results.
3.1	Max-Ent Successor Features and Generalized Policy Improvement
We introduce max-ent SF, which provide a practical method for computing the value of a maximum
entropy policy under any convex combination of rewards. We then show the GPI theorem (Barreto
et al., 2017) holds for maximum entropy policies.
We define the action-dependent SF to include the entropy of the policy, excluding the current state,
analogous to the max-entropy definition of Qn in (4):
ψn (St, at) ≡ φt + En [∑∞=i+1 Yτ-t(φτ + α1 ∙ H[n(・|S)])] = φt + YEp(St+ι∣st,at) Y(St+1)]
(7)
3
Under review as a conference paper at ICLR 2019
where 1 is a vector of ones of the same dimensionality as φ and we define the state-dependent
successor features as the expected ψπ in analogy with V π (s):
Υπ(s) ≡ Ea〜∏(∙∣s) [ψπ(s, a)] + α1 ∙ H[∏(∙∣s)].	(8)
The max-entropy action-value of π for any convex combination of rewards w is then given by
QW(s, a) = ψπ (s, a) ∙ w. Max-ent SF allow Us to estimate the action-value of previous policies on
a new task. We show that, as in the deterministic case, there is a principled way to combine multiple
policies using their action-values on task w.
Theorem 3.1 (Max-Ent Generalized Policy Improvement) Let π1, π2, ..., πn be n policies with
α-max-ent action-value functions Q1, Q2, ..., Qn and value functions V 1, V 2, ..., Vn. Define
π(a∣s) H exp (1 maxi Qi(s, a)).
Then,
Qπ (s, a) ≥ maxQi(s, a) for all s ∈ S and all a ∈ A,	(9)
i
V π(s) ≥ max V i(s) for all s ∈ S,	(10)
i
where Qπ (s, a) and Vπ(s) are the α-max-ent action-value and value function respectively of π.
Proof: See appendix A.1. In our setup, we learn ψπi (s, a), the SFs of policies πi for each task in
T, we define the max-ent GPI policy for task W ∈ T0 as ∏WPI(a∣s) h exp( 1 maxi Qwi(s, a))=
exp( 1 maxi ψπi (s, a) ∙ w).
3.2 Divergence Correction (DC)
Haarnoja et al. (2018a) introduced a simple approach to policy composition by estimating the action-
value for the transfer task rb = bri + (1 - b)rj from the optimal action-values of the component
tasks Qi and Qj
QbOpt(s, a) ≡ bQi(s, a) + (1 - b)Qj (s, a).	(11)
When using Boltzmann policies defined by Q, the resulting policy, πOpt(a∣s) h exp( 1 QOpt(s, a)),
is the product distribution of the two component policies. We refer to πbOpt as the compositionally
“optimistic” (CO) policy, as it acts according to the optimistic assumption that the optimal returns
of Qi and Qj will be, simultaneously, achievable1.
Both max-ent GPI we presented above, and CO can, in different ways, fail to transfer well in some
situations (see fig. 1 for some examples in tabular case). Neither approach consistently performs
optimally during transfer, even if all component terms are known exactly. We desire a solution for
transfer that, in principle, can perform optimally.
Here we show, at the cost of learning a function conditional on the task weightings b, itis in principle
possible to recover the optimal policy for the transfer tasks, without direct experience on those tasks,
by correcting for the compositional optimism bias in QbOpt . For simplicity, as in Haarnoja et al.
(2018a), we restrict this to the case with only 2 tasks, but it can be extended to multiple tasks.
The correction term for CO uses a property noted, but not exploited in Haarnoja et al. (2018a). The
bias in QOpt is related to the the discounted sum OfRenyi divergences of the two component policies.
Intuitively, if the two policies result in trajectories with low divergence between the policies in each
state, the CO assumption that both policies can achieve good returns is approximately correct. When
the divergences are large, the CO assumption is being overly optimistic and the correction term will
be large.
Theorem 3.2 (DC Optimality) Let πi , πj be α max-ent optimal policies for tasks with rewards ri
and rj with max-ent action-value functions Qi, Qj. Define Cb∞(st, at) as the fixed point of
Cbk+1) (St,at) = -αYEp(st+ι∣st,at) [log RA πi(at+1lst+1)bπj'(at+1|st+1)(1-b) exp(-1 Cbk)(St+1, at+1 ))dat+1]
1Compositional optimism is not the same as optimism under uncertainty, often used in RL for exploration.
4
Under review as a conference paper at ICLR 2019
Given the ConditionsforSof Q convergence, the max-ent optimal Qb (s, a) for r = br + (1 — b)r7-
is
Qbb(s,a) = bQi(s,a) + (1 — b)Qj (s, a) — Cb∞(s, a) ∀s ∈ S,a ∈ A,b ∈ [0, 1].
Proof: See appendix A.2. We call this Divergence Correction (DC) as the quantity Cb∞ is related to
the Renyi divergence between policies (See appendix A.2 for details). Learning C∞ does not require
any additional information (in principle) than that required to learn policies πi and πj . Unlike with
SF, it is not necessary to observe other task features while training the policies. On the other hand,
unlike with GPI, which can be used to naturally combine any number of tasks with arbitrary weight
vectors w, in order to apply DC one must estimate Cb∞(s, a) for all values of b. so the complexity
of learning C∞ increases significantly if more than 2 tasks are combined.
Supplementary Table 1 provides a comparison on the properties of the methods we consider here.
We also compare with simply learning a conditional Q function Q(s, a|b) (CondQ) (e.g. Schaul
et al., 2015; Andrychowicz et al., 2017). As with GPI, this requires observing the full set of task
features φ, in order to compute rb for arbitrary b.
In this section we have introduced two new theoretical approaches to max-ent transfer composition:
max-ent GPI and DC. We have shown how these are related to relevant prior methods. In the next
section we address the question of how to practically learn and sample with these approaches in
continuous action spaces.
4	Adaptive Importance S ampling for B oltzman Policies
Algorithm
The control of robotic systems with high-dimensional continuous action spaces is a promising use
case for the ideas presented in this paper. Such control problems may allow for multiple solutions,
and can exhibit exploitable compositional structure. Unfortunately, learning and sampling of gen-
eral Boltzmann policies defined over continuous action spaces is challenging. While this can be
mitigated by learning a parametric sampling distribution, during transfer we want to sample from
the Boltzmann policy associated with a newly synthesized action-value function without having to
learn such an approximation first. To address this issue we introduce Adaptive Importance Sampling
for Boltzmann Policies (AISBP), a method which provides a practical solution to this challenge.
In the following we parametrise all functions with neural nets (denoting parameters by the subscript
θ), including the soft action-value for reward i: Qiθ (s, a); the associated soft value function Vθi (s)
and a proposal distribution qθi (a|s), the role of which we explain below. We use an off-policy
algorithm, so that experience generated by training on policy i can be used to improve policy j . This
is especially important since our analysis requires the action-value Qi(s, a) to be known in all states.
This is less likely to be the case for a on on-policy algorithm, that only updates Qi using trajectories
generated by policy πi. During training experience generated by all tasks are stored in a replay buffer
R, and mini-batches are sampled uniformly and used to update all function approximators. Soft Q
iteration (see eq. 4) is used to learn Qi and V i . These updates are, in principle, straightforward
using transitions sampled from the replay buffer.
Sampling from the Boltzmann policy defined by QΘq, πi(a∣s) α exp 1 QΘq (s, a) is challenging as
is estimating the partition function (the log of which is also the value, c.f. Eq. 6). One approach is
to fit an expressible, tractable sampler, such as a stochastic neural network to approximate πi (e.g.
Haarnoja et al., 2018a). This approach works well when learning a single policy. However, during
transfer this may require learning a new sampler for each new value composition. AISBP instead
uses importance sampling to sample π and estimate the partition function. The scalability of this
approach is improved by using using a learned proposal distribution qθq (a|s), and by observing that
modern architectures allow for efficient batch computation ofa large number of importance samples.
To facilitate transfer we restrict the parametric form of the proposals to mixtures of (truncated) Nor-
mal distributions. The well-known result that the product of Normal distributions can be computed
in closed-form then allows us to construct effective compositional proposals during transfer.
More formally, for each policy in T we learn an action-value Qiθ (s, a), and value Vθi (s) network,
and a proposal distribution qθi (a|s) (we drop the task index i here when writing the losses for nota-
5
Under review as a conference paper at ICLR 2019
tional clarify, and write the losses for a single policy). The proposal distribution is a mixture of M
truncated Normal distributions NT, truncated to the square a ∈ [-1, 1)n with diagonal covariances
qθq ⑷S) = MM PMM=1 NT(a; μmq (s),σm (S),-1, I)	(12)
The proposal distribution is optimized by minimizing the forward KL divergence with the Boltz-
mann policy π(a∣s) α exp 1 Qθq (s, a). This KL is “zero avoiding" and over-estimates the support
of π (Murphy, 2012) which is desirable for a proposal distribution (Gu et al., 2015),
L(θq) = Er [Ea〜∏(∙∣s)[log∏(a∣st) - log qθq (a|st)]]
(13)
where the expectation is over the replay buffer state density.
The inner expectation in the proposal loss itself requires sampling from π. We approximate his
expectation by self-normalized importance sampling and use a target proposal distribution p(at|St)
which is a mixture distribution consisting of the proposals for all policies along with a uniform
distribution. For batchsize B and N proposal samples the estimator of the proposal loss is then
1BN
L(θq) ≈ -⅛ΣΣwkl log qθq (a|St); wk0 l
k=1 l=1
1 (QΘq (Sk,akI))
p(aki |sk)
Wkl = PNwkl 0 .(14)
m=1 wkm
The value function loss is defined as the L2 error on the Soft Q estimate of value
L(θV) =ER
(15)
which is estimated using importance sampling to compute the integral.
L(θv) ≈ 2B PB=1 (Vθv (sl) — α log Zf ； Z = U PkLexp(B(θQ(Ssll,7[ .	(16)
This introduces bias due to the finite-sample approximation of the expectation inside the (concave)
log. In practice we found this estimator sufficiently accurate, provided the proposal distribution was
close to π . We also use importance sampling to sample from π while acting.
The action-value loss is just the L2 norm with the Soft Q target:
LIeQ) =ER 2(QΘq (St, at) - (r(st, at, st+1) + γVθV (st+1)))2 ∙	(17)
To improve stability we employ target networks for the value VθV0 and proposal qθq0 networks (Mnih
et al., 2015; Lillicrap et al., 2015) We also parameterize Q as an advantage QθQ (S, a) = VθV (S) +
AθA (S, a) (Baird, 1994; Wang et al., 2015; Harmon et al., 1995) which is more stable when the
advantage is small compared with the value. The full algorithm is give in Algorithm Box 1 and
more details are provided in appendix C.
4.1	Importance Sampled Max-Ent GPI
The same importance sampling approach can also be used to estimate max-ent SF. Max-ent GPI
requires us to learn the expected (maximum entropy) features ψi for each policy πi , in order to
estimate its (entropic) value under a new convex combination task w. This requires that experience
tuple in the replay contain the full feature vector φ, rather than just the reward for the policy which
generated the experience ri . Given this information ψθψ and ΥθΥ can be learned with analogous
updates to V and Q, which again requires importance sampling to estimate Υ.
As with VθV , we use a target network for Υθ0 and advantage parametrization. We found that,
because these updates when using experience shared between tasks is far off-policy, it is necessary
to have a longer target update period than for V. Full details are of the losses and samplers are in
appendix C.
6
Under review as a conference paper at ICLR 2019
Algorithm 1 AISBP training algorithm
Initialize proposal network θq, value network parameters θv and action-value network parameters
θQ and replay R
while training do	. in parallel on each actor
Obtain parameters θ from learner
Sample task i 〜T
Roll out episode using qθ^ to importance sample ∏i(a∣s) a exp 1 Q^q (s, a)
Add experience to replay R
end while
while training do	. in parallel on the learner
Sample SARS tuple from R
Improve L(θq), L(θV), L(θQ )
Improve additional losses for transfer L(θΥ), L(θψ), L(θC), L(θVb) L(θQb),
if target update period then
Update target network parameters θv0 - θv, θqo — θq, θγo - θγ, θ% 一 θvb
end if
end while
4.2	Divergence Correction
All that is required for transfer using compositional optimism (eq. 11, Haarnoja et al. (2018a)) is the
max-ent action values of each task, so no additional training is required beyond the base policies.
In section 3.2 we have shown that if we can learn the fixed point of Cb∞ (s, a) we can correct this
compositional optimism and recover the optimal action-value Qb (s, a).
We exploit the recursive relationship in Cb∞(s, a) to fit a neural net CθC (s, a, b) with a TD(0) esti-
mator. This requires learning a conditional estimator for any value of b, so as to support arbitrary
task combinations. Fortunately, since Cb∞ depends only on the policies and transition function it is
possible to learn an estimator Cb∞ for different values of b by sampling b during each update. As
before, we use target networks and an advantage parametrization for CθC (s, a, b)
We learn Cb∞ as CθC (s, a, b), for each pair of policies πi, πj resulting in the loss
L(θc) = Es 〜R,b〜U (o,i)[1 (Cθc(s,a,b) + αγEp3∣s,a) [log JA exp(b log ∏i(a0 |s0)+	(18)
(1 — b)∏j(a0∣s0)---Cθco (s0, a0, b))da0])2].
As with other integrals of the action space, we approximate this loss using importance sampling to
estimate the integral. Note that, unlike GPI and CondQ (next section), learning Cb∞ does not require
observing φ while training.
We also considered a heuristic approach where We learned C only for b = 2 (this is typically
approximately the largest divergence). This avoids the complexity of a conditional estimator and
We estimate C∞ as C∞(s, a) 〜 4b(1 — b)C∞2(s, a). This heuristic, We denote DC-Cheap, can
be motivated by considering Gaussian policies with similar variance (see appendix D) The max-ent
GPI bound can be used to correct for over-estimates of the heuristic Cb∞, QDC-Cheap+GP I (s, a) =
max(QOPT(s, a) - C∞(s, a), QGPI(s, a)).
4.3	COND Q
As a baseline, we directly learn a conditional Q function using a similar approach to DC of sampling
b each update Q(s, a, b) (Schaul et al., 2015). This, like GPI but unlike DC, requires observing φ
during training so the reward on task b can be estimated. We provide the full details in appendix C.
4.4	Sampling Compositional Policies
During transfer we would like to be able to sample from the Boltzmann policy defined by our
estimate of the transfer action-value Qb (the estimate is computed using the methods we enumerated
7
Under review as a conference paper at ICLR 2019
above) without having to, offline, learn a new proposal or sampling distribution first (which is the
approach employed by Haarnoja et al. (2018a)).
As outlined earlier, we chose the proposal distributions so that the product of proposals is tractable,
meaning We can sample from qj(a|s) H (q^(a∣s))b(qj(a∣s))(1-b). This is a good proposal dis-
tribution when the CO bias is low, since QbOpt defines a Boltzmann policy which is the product of
the base policies2 HoWever, When Cb∞ (s, a) is large, meaning the CO bias is large, qij may not be
a good proposal, as We shoW in the experiments. In this case none of the existing proposal distribu-
tions may be a good fit. Therefore We sample from a mixture distribution of all policies, all policy
products and the uniform distribution.
Pb(als) ≡ 4(qθq ⑷S) + qjq ⑷S) + qij ⑷S) + 备)	(19)
Where VA is the volume of the action space. Empirically, We find this is sufficient to result in good
performance during transfer. The algorithm for transfer is given in supplementary algorithm 2.
5	Experiments
5.1	Discrete, tabular environment
We first consider some illustrative tabular cases of compositional transfer. These highlight situations
in Which GPI and CO transfer can perform poorly (Figure 1). As expected, We find that GPI performs
Well When the optimal transfer policy is close to one of the existing policies; CO performs Well When
both subtask policies are compatible. The task We refer to as “tricky” is illustrative of in Which the
optimal policy for the transfer task does not resemble either existing policy: In the grid World non-
overlapping reWards for each task are provided in one corner of the grid World, While loWer value
overlapping reWards are provided in the other corner (cf. Fig. 1). As a consequence both GPI and
CO perform poorly While DC performs Well in all cases.
5.2	Continuous action spaces
We next compare the different approaches in more challenging continuous control tasks. We train
max-ent policies to solve individual tasks using the importance sampling approach from section
4 and then assess transfer on convex combinations of the reWards. All approaches use the same
experience and proposal distribution.
Figure 2 examines the transfer policies in detail in a simple point-mass task and shoWs hoW the
estimated Cb∞ corrects the CO QOpt and dramatically changes the policy.
We then examine conceptually similar tasks in more difficult domains: a 5 DOF planar manipulator
reaching task (figure 3), 3 DOF jumping ball and 8 DOF ant (figure 4). We see that DC recovers a
qualitatively better policy in all cases. The performance of GPI depends noticeably on the choice of
α. DC-Cheap, Which is a simpler heuristic, performs almost as Well as DC in the tasks We consider
except for the point mass task. When bounded by GPI (DC-Cheap+GPI) it performs Well for the
point mass task as Well, suggesting simple approximations of Cb∞ may be sufficient in some cases.3
We focussed on “tricky” tasks as they are challenging form of transfer. In general, We Would expect
DC to perform Well in most situations Where OC performs Well, since in this case the correction term
Cb∞ that DC must learn is inconsequential (OC is equivalent to assuming Cb∞ = 0). Supplementary
figure 5 demonstrates on a task With non-composible solutions (i.e. Cb∞ is large and potentially
challenging to learn), DC continues to perform as Well as GPI, slightly better than CondQ, and as
expected, OC performs poorly.
6 Discussion
We have presented tWo approaches to transfer learning via convex combinations of reWards in the
maximum entropy frameWork: max-ent GPI and DC. We have shoWn that, under standard assump-
2πOpt(a∣s) X exp 1 QOpt(s,a) = exp( 1 (Q1(s,a) + Q2(s,a)) = ∏ι(a∣s)∏2(a|s).
3 We provide videos of the more interesting tasks at https://tinyurl.com/yaplfwaq.
8
Under review as a conference paper at ICLR 2019
(a) (L)eft task
(b) (T)tricky task 1	(c) (T)tricky task 2	(d) LR regret	(e) LU regret	⑴ T regret
(g)OPt LR
(h) GPI LU
(i) GPIT	(j) DC T	(k) D1/2
Figure 1: Policy composition in the tabular case. All tasks are in an infinite-horizon tabular 8x8
world. The action space is the 4 diagonal movements (actions at the boundary transition back to
the same state) (a-c) shows 3 reward functions (color indicates reward, dark blue r = +1, light
blue r = 0.75). The arrows indicate the action likelihoods for the max-ent optimal policy for each
task. (d-f) The log regret for the max-ent returns for 3 qualitatively distinct compositional tasks
rb = bri+(1-b)rj, using different approaches to transfer from the base policies. The compositional
tasks we consider are left-right (LR), left-up (LU) and the “tricky“ tasks (T).
(d)	GPI performs well when the subtasks are incompatible, meaning the optimal policy is near one
of the component policies. (g) CO performs poorly in these situations, resulting in indecision about
which subtask to commit to.
(e)	Conversely, when the subpolicies are compatible, such as on the LU task, CO transfers well
while the GPI policy (h) does not consistently take advantage of the compatibility of the two tasks
to simultaneously achieve both subgoals.
(f)	Neither GPI nor CO policies (i shows the GPI policy, but CO is similar) perform well when the
optimal transfer policy is dissimilar to either existing task policy. The two tricky task policies are
compatible in many states but have a high-divergence in the bottom-left corner since the rewards are
non-overlapping there (k), thus the optimal policy on the composed task is to move to the top right
corner where there are overlapping rewards. By learning, and correcting for, this future divergence
between policies, DC results in optimal policies for all task combinations including tricky (j).
(a) Trajectories
(b) Returns	⑹ D2	(d) QOpt	(e) C∞	⑴ QDC
Figure 2: Tricky point mass. The continuous “tricky” task with a simple 2-D velocity controlled
pointed mass. (a) Environment and example trajectories. The rewards are (r1 = 1, r2 = 0), (0, 1)
and (0.75, 0.75) for the green, red and yellow squares. Lines show sampled trajectories (starting in
the center) for the compositional task r1/2 with CO (red), GPI (blue) and DC (black). Only DC, DC
heuristics and CondQ (not shown) find the optimal transfer policy of navigating to yellow reward
area for the joint task which is the optimal solution for the compositional task. (b) The returns for
each transfer method. DC and CondQ methods recover significantly better performance than GPI,
and the CO policy performs poorly. (c) The Renyi divergence of the two base policies as a function
of position: the two policies are compatible except near the bottom left corner where the rewards
are non-overlapping. (d) QOpt at the center position for the combined task. As both policies prefer
moving left and down, most of the energy is on these actions. (e) However, the future divergence
C∞2 under these actions is high, which results in the (f) DC differing significantly from CO.
tions, the max-ent GPI policy performs at least as well as its component policies, and that DC
recovers the optimal transfer policy. Todorov (2009) and (Saxe et al., 2017; van Niekerk et al.,
2018) previously considered optimal composition of max-ent policies. However, these approaches
9
Under review as a conference paper at ICLR 2019
(a) Planar manipulator tricky
CO
GPI
DC
CondQ
(c) Returns
(b) Finger Position
Figure 3: “Tricky” task with planar manipulator. The “tricky” tasks with a 5D torque-controlled
planar manipulator. The training tasks consists of (mutually exclusive) rewards of (1, 0), (0, 1) when
the finger is at the green and red targets respectively and reward (0.75, 0.75) at the blue target. (b)
Finger position at the end of the trajectories starting from randomly sampled start states) for the
transfer task with circles indicating the rewards. DC and CondQ trajectories reach towards the
blue target (the optimal solution) while CO and GPI trajectories primarily reach towards one of the
suboptimal partial solutions. (c) The returns on the transfer tasks (shaded bars show SEM, 5 seeds).
(a) Ant
(b) JB Returns	(c) Ant returns	(d) Trajectories
Figure 4: “Tricky” task with mobile bodies. “Tricky” task with two bodies: a 3 DOF jumping
ball (supplementary figure 6) and (a) 8 DOF ant (both torque controlled). The task has rewards
(1, 0), (0, 1) in the green and red boxes respectively and (0.75, 0.75) in the blue square. (b-c) Re-
turns for both walkers when started in the center position. CO approach does not recover the optimal
policy for the compositional task while the other approaches largely do, although CondQ does not
learn a good policy on the ant (shaded bars show SEM, 3 seeds for jumping ball, 5 seeds for ant). (e)
Sampled trajectories of the ant on the transfer task starting from a neutral position for b = 1. GPI
and DC consistently go to the blue square (optimal), CondQ and CO do not.
require stronger assumptions than max-ent SF or DC, namely that reward states are absorbing and
that the joint reward is restricted to the softmax of the component rewards (soft OR). By contrast,
DC does not restrict the class of MDPs and learns how compatible policies are, allowing approxi-
mate recovery of optimal transfer policies both when the component rewards are jointly achievable
(AND), and when only one sub-goal can be achieved (OR).
We have compared our methods with conditional action-value functions (CondQ) (Schaul et al.,
2015, e.g.) and optimistic policy combination (Haarnoja et al., 2018a). Further, we have presented
AISBP, a practical algorithm for training DC and max-ent GPI models in continuous action spaces
using adaptive importance sampling. We have compared these approaches, along with heuristic
approximations of DC, and demonstrated that DC recovers an approximately optimal policy during
transfer across a variety of high-dimensional control tasks. Empirically we have found CondQ may
be harder to learn than DC, and it requires additional observation of φ during training.
References
Karen E Adolph, Whitney G Cole, Meghana Komati, Jessie S Garciaguirre, Daryaneh Badaly,
Jesse M Lingeman, Gladys LY Chan, and Rachel B Sotsky. How do you learn to walk? thousands
10
Under review as a conference paper at ICLR 2019
of steps and dozens of falls per day. Psychological science, 23(11):1387-1394, 2012.
Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob
McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience re-
play. In Advances in Neural Information Processing Systems, pp. 5048-5058, 2017.
Leemon C Baird. Reinforcement learning in continuous time: Advantage updating. In Neural
Networks, 1994. IEEE World Congress on Computational Intelligence., 1994 IEEE International
Conference on, volume 4, pp. 2448-2453. IEEE, 1994.
Andre Barreto, Will Dabney, Remi Munos, Jonathan J Hunt, Tom SchaUL Hado P van Hasselt, and
David Silver. Successor features for transfer in reinforcement learning. In Advances in neural
information processing systems, pp. 4055-4065, 2017.
Andre Barreto, Diana Borsa, John Quan, Tom Schaul, David Silver, Matteo Hessel, Daniel
Mankowitz, Augustin Zidek, and Remi Munos. Transfer in deep reinforcement learning using
successor features and generalised policy improvement. In Proceedings of the International Con-
ference on Machine Learning, pp. 501-510, 2018.
Djork-Arne Clevert, Thomas Unterthiner, and SePP Hochreiter. Fast and accurate deep network
learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.
Peter Dayan. Improving generalization for temporal difference learning: The successor representa-
tion. Neural Computation, 5(4):613-624, 1993.
Roy Fox, Ari Pakman, and Naftali Tishby. Taming the noise in reinforcement learning via soft
updates. arXiv preprint arXiv:1512.08562, 2015.
Manuel Gil, Fady Alajaji, and Tamas Linder. Renyi divergence measures for commonly used uni-
variate continuous distributions. Information Sciences, 249:124-131, 2013.
Shixiang Gu, Zoubin Ghahramani, and Richard E Turner. Neural adaptive sequential monte carlo.
In Advances in Neural Information Processing Systems, pp. 2629-2637, 2015.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with
deep energy-based policies. arXiv preprint arXiv:1702.08165, 2017.
Tuomas Haarnoja, Vitchyr Pong, Aurick Zhou, Murtaza Dalal, Pieter Abbeel, and Sergey
Levine. Composable deep reinforcement learning for robotic manipulation. arXiv preprint
arXiv:1803.06773, 2018a.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-
policy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint
arXiv:1801.01290, 2018b.
Adrian M Haith and John W Krakauer. Model-based and model-free mechanisms of human motor
learning. In Progress in motor control, pp. 1-21. Springer, 2013.
Mance E Harmon, Leemon C Baird III, and A Harry Klopf. Advantage updating applied to a
differential game. In Advances in neural information processing systems, pp. 353-360, 1995.
Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre
Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. Qt-opt: Scalable deep
reinforcement learning for vision-based robotic manipulation. arXiv preprint arXiv:1806.10293,
2018.
Hilbert J Kappen. Path integrals and symmetry breaking for optimal control theory. Journal of
statistical mechanics: theory and experiment, 2005(11):P11011, 2005.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
11
Under review as a conference paper at ICLR 2019
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529, 2015.
Kevin P. Murphy. Machine Learning: A Probabilistic Perspective. The MIT Press, 2012. ISBN
0262018020, 9780262018029.
Ivaylo Popov, Nicolas Heess, Timothy Lillicrap, Roland Hafner, Gabriel Barth-Maron, Matej Ve-
cerik, Thomas Lampe, Yuval Tassa, Tom Erez, and Martin Riedmiller. Data-efficient deep rein-
forcement learning for dexterous manipulation. arXiv preprint arXiv:1704.03073, 2017.
Doina Precup, Richard S Sutton, and Satinder Singh. Theoretical results on reinforcement learning
with temporally abstract options. In European conference on machine learning, pp. 382-393.
Springer, 1998.
Andrew M Saxe, Adam Christopher Earle, and Benjamin S Rosman. Hierarchy through composition
with multitask lmdps. 2017.
Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approxima-
tors. In International Conference on Machine Learning, pp. 1312-1320, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484, 2016.
Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Bud-
den, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv
preprint arXiv:1801.00690, 2018.
Emanuel Todorov. Compositionality of optimal control laws. In Advances in Neural Information
Processing Systems, pp. 1856-1864, 2009.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026-
5033. IEEE, 2012.
Benjamin van Niekerk, Steven James, Adam Earle, and Benjamin Rosman. Will it blend? compos-
ing value functions in reinforcement learning. arXiv preprint arXiv:1807.04439, 2018.
Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Van Hasselt, Marc Lanctot, and Nando De Freitas.
Dueling network architectures for deep reinforcement learning. arXiv preprint arXiv:1511.06581,
2015.
Karl R. Weiss, Taghi M. Khoshgoftaar, and Dingding Wang. A survey of transfer learning. Journal
of Big Data, 3:1-40, 2016.
Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse
reinforcement learning. In AAAI, volume 8, pp. 1433-1438. Chicago, IL, USA, 2008.
12
Under review as a conference paper at ICLR 2019
A	Proofs
A.1 Max-Ent Generalized Policy Improvement
Theorem 3.1 (Max-Ent Generalized Policy Improvement) Let π1 , π2, ..., πn be n policies with
α-max-ent action-value functions Q1, Q2, ..., Qn and value functions V 1, V 2, ..., Vn. Define
π(a∣s) H exp (1 maxi Qi(s, a)).
Then,
Qπ (s, a) ≥ maxQi(s, a) for all s ∈ S and all a ∈ A,	(9)
i
V π(s) ≥ max V i(s) for all s ∈ S,	(10)
i
where Qπ (s, a) and V π(s) are the α-max-ent action-value and value function respectively of π.
For brevity we denote Qmax ≡ maxi Qi . Define the soft Bellman operator associated with policy π
as
TnQ(s, a) ≡ r(s, a, s0) + γEp(s0∣s,a) [αH[∏(∙∣s0)] + E°,〜∏(.∣s,) [Q(s0, a0)]].
Haarnoja et al. (2018b) have pointed out that the soft Bellman operator Tπ corresponds to a con-
ventional, “hard”, Bellman operator defined over the same MDP but with reward rπ (s, a, s0) =
r(s, a, s0) + YaEp(So∣s,a) [H[∏(∙∣s0)]]. Thus, as long as r(s, a, s0) and H[π(∙∣s0)] are bounded, Tπ
is a contraction with Qπ as its fixed point. Appplying Tπ to Qmax(s, a) we have:
TπQmax(s, a) =r(s,a,s0)+γE
s0〜p(∙∣ s,a),a0〜π(∙∣s0) [-αlogπ(a0∣s0) + Qmax(S0, a0)]
0	exp(α-1Qmax(s0, a0))	max 0 0
=r(S, a,s ) + γEs0〜p(∙∣s,a),a0〜π(∙∣s0) -α log -0τ^T~Γ∖--------+ Q	(s , a )
Zπ(s0)
=r(s,a, s0) + γEs0〜p(∙∣s,a) [αlogZπ(s0)].
Similarly, ifwe apply Tπi, the soft Bellman operator induced by policy πi, to Qmax(S, a), we obtain:
TniQmax(S,a)= r(s,a, s0) + YE§o〜以心⑷〃〜冗认心，)[-αlogπi(a0∣s0) + Qmax(S0,a0)].
We now note that the Kullback-Leibler divergence between πi and π can be written as
DκL(∏i(∙∣s)k∏(∙∣s)) = Ea〜∏i(∙∣s) [log∏i(a∣s) - log∏(a∣s)]
=Ea〜∏i(∙∣s) log∏i(a∣s) - αQmax(S,a)+logZπ(s).
The quantity above, which is always nonnegative, will be useful in the subsequent derivations. Next
we write
TnQmax(S,a)-TπiQmax(S,a) = 7旧§，〜似心⑷[αlogZπ(s0) - E。，〜*小)[-alogπi(a0∣S0) + Qmax(S0,a0)]]
=γEs0〜p(∙∣s,a) [EaO〜∏i(∙∣sθ)[α log Zπ (s') + a log ∏i(a0∣S0) - Qmax(S0, a0)]]
=YEsO〜p(∙∣s,a) [aDKL(ni(ISO)kπ(ISO))]
≥ 0.	(20)
From (20) we have that
TπQmax(S,a) ≥ TπiQmax(S, a) ≥ TπiQi(S,a) = Qi(S,a) for all i.
Using the contraction and monotonicity of the soft Bellman operator Tπ we have
Qπ(S,a) = lim (Tπ)kQmax(S, a) ≥ Qi(S,a) for all i.
k→∞
We have just showed (9). In order to show (10), we note that
Vπ(s) ≡ aH[∏(∙∣s)] + Ea〜∏ [Qπ(s, a)]
≥ αH[∏(∙∣s)] + Ea〜∏ [Qmax(S, a)]
= alogZπ(S).	(21)
13
Under review as a conference paper at ICLR 2019
Similarly, we have, for all i,
Vi(s) = Ea~∏i(∙∣s) [Qi(s, a) - αlogπi(a∣s)]
≤ Ea~∏i(∙∣s) [Qmax(s, a) - αlog∏i(a∣s)]
=αlogZπ(S)- αDκL(∏i(∙∣s)k∏(∙∣s))
≤ α log Zπ(s).	(22)
The bound (10) follows from (21) and (22).
A.2 DC PROOF
Theorem 3.2 (DC Optimality) Let πi , πj be α max-ent optimal policies for tasks with rewards ri
and rj with max-ent action-value functions Qi, Qj. Define Cb∞(St, at) as the fixed point of
Cbk+1)(st,at) = -αYEp(st+ι∣st,at) [log RA ni(at+1|st+1)bπj(at+1|st+I)(I-b) exp(-1 Cbk)(St+1, at+1 ))dat+l]
Given the ConditionsforSof Q convergence, the max-ent optimal Qb(S) a) for r = br + (1 — b)r7-
is
Qbb(S,a) = bQi(S,a) + (1 - b)Qj (S, a) - Cb∞(S, a) ∀S ∈ S,a ∈ A,b ∈ [0, 1].
We follow a similar approach to Haarnoja et al. (2018a) but without making approximations and
generalizing to all convex combinations.
First note that since πi and πj are optimal then ∏i(a∣s) = exp(11(Q(S) a) - Vi(s))).
For brevity we use S and S0 notation rather than writing the time index.
Define
Q(b0)(S)a) ≡ bQi(S)a) +(1 -b)Qj(S)a)	(23)
C(0)(S)a) ≡0	(24)
and consider soft Q-iteration on rb starting from Q(b0). We prove, inductively, that at each iteration
Q(bk+1) = bQi(S,a)+ (1 -b)Qj(S,a) - C(k+1)(S, a).
This is true by definition for k = 0.
Qbk+1)(S,a)= rb(S,a) + YaEp(So∣s,a) log
= rb(S, a)+
exp 工Qbk)(S0, a0)da0
A αb
(25)
(26)
YaEp(So∣s,a) log [ exp(-(bQz(S0) a0) + (1 - b)Qj(S0,a0) - C(k)(S0, a0)))da0
Aα
= rb(S, a)+
(27)
Ep(s0∣sa) bVi(S0) + (1 — b)Vj(s0) + αlog / exp(blog∏i(a0∣S0) + (1 — b) log∏j(a0∣S0)—工C(k)(S0, a0))da0
,A	α
= bQi(S, a) + (1 - b)Qj (S, a)+	(28)
αγEp(s0∣s,a) log/ exp(blog∏ι(a0∣So) + (1 - b) log∏2(a0∣S0) - 1 C(k)(S0,a0))da0
bQi(S)a) +(1 - b)Qi(S) a) - Cb(k+1)(S) a).
(29)
Since soft Q-iteration converges to the a max-ent optimal soft Q then equation 31 holds at the limit.
One can get an intuition for Cb∞ (S, a) by noting that
CbI)(S, a) = YaEp(S0∣s,a) [(1 - b) Db (∏i(∙∣s)∣∣∏2(∙∣s))]	(30)
where Db is the Renyi divergence of order b. C∞(s, a) can be seen as the discount sum of diver-
gences, weighted by the unnormalized product distribution ∏1(a∣S)b∏2 (a∣S)1-b.
14
Under review as a conference paper at ICLR 2019
A.3 n policies
It is possible to extend Theorem 3.2 to the case with N policies in a straightforward way.
Theorem A.1 (Multi-policy DC Optimality) Let π1, π2, ..., πN be α max-ent optimal policies for
tasks with rewards r1, r2, ..., rN with max-ent action-value functions Q1, Q2, ..., QN.
Define Cw∞ (st , at ) as the fixed point of
CWk+1)(st,at) = -αγ Ep(st+ι∣st ,at) [log RA (QN=I ni(at+1|st+I)Wi ) exp(- a Cwk)(St+1, at+1))dat+1i
Given the conditions for Soft Q convergence, the max-ent optimal QW (s, a) for and convex Combi-
nation of rewards rw = PiN=1 riwi is
QW (s,a) = PN=ι WiQiGa)- C∞(s,a)
N
∀S ∈ S, a ∈ A, w ∈ {w|	wi = 1 and wi ≥ 0}
i=1
Note that wi refers to component i of the vector wi .
The proof is very similar to the two reward case above.
Define
N
Q(w0) ≡ XwiQi(S,a)	(31)
i=1
Cw(0) ≡ 0	(32)
and again consider soft Q-iteration on rw . We prove by induction that at each iteration
N
Q(wk+1)(S,a) =XwiQi(S,a) - Cw(k+1)(S, a)	(33)
i=1
Again, this is true by definition for k = 0. Now we consider a step of Soft Q iteration
Q(wk+1)
=TW(s,a) + YaEp(So∣s,a) log/ exp 1 Qwk)(S', a')da'
(34)
= rw (S, a) + γ Ep(s0 |s,a)
Z exp - X WiQi(S', a') - Cw(k) (S, a) da'
(35)
X WiVi(s') + αlogZ exp (X Wi log∏i(a[s') - 1 Cwk)(S', a')) da'
i=1	i=1	(36)
N
EWiQls, a) + αγEp(s0∣s,a)
i=1
N
XWiQi(S,a) -Cw(k+1)(S,a)
i=1
N1
log J exp(ɪ2 Wi log∏i(a'∣s')-C(k)(s', a'))da'
(37)
(38)
Since soft Q-iteration converges to the - max-ent optimal soft Q then QW(s,a)	=
PiN=1 WiQi(S, a) - Cw(k+1)(S, a) for all S ∈ S, a ∈ A.
Note that, in practise, estimating CW∞ may be more challenging for larger N . For compositions of
many policies, GPI may be more practical.
15
Under review as a conference paper at ICLR 2019
B Theoretical properties of the composition methods
Method	Optimal	Bounded loss	Requires φ	Requires f (s, a|b)
-CO				
CondQ	X	na	X	X
-GPi		X	X	一	
^C	X	na		X
Table 1: Theoretical properties of different approaches to max-ent transfer. The methods compared
are: CO, CondQ, max-ent GPI (over a fixed, finite set of policies), and DC. The columns indicate
whether the transfer policy is optimal, the regret of the transfer policy is bounded, whether rewards
for all tasks φ need to be observed simultaneously during training and whether the method requires
learning a function conditional on the transfer task b, f(s, a|b). DC is the only method that both
recovers (in principle) the optimal policy and does not require observing φ during training.
C Algorithm details
C.1 Transfer algorithm
Algorithm 2 AISBP transfer algorithm
Load trained parameters Θq, θq, θsfQ, θc, θQb.
Accept transfer task parameter b, transfer method ∈ CO, GPI, DC, CondQ.
while testing do
Importance sample transfer policy ∏b(a∣s) α exp 1 Qmethod(s, a) with mixture proposal
Pb⑷ * s)θq .
end while
C.2 All losses and estimators
We use neural networks to parametrize all quantities. For each policy we learn an action-value
QθQ (s, a), value VθV (s) and proposal distribution qθq (a|s). We use target networks for the proposal
distribution qθ0 (a|s) and value Vθ0 (s).
Here we enumerate all of the losses and their estimators. We use temporal difference (TD(0)) learn-
ing for all the RL losses, so all losses are valid off-policy. We use a replay buffer R and learn by
sampling minibatches of SARS tuples of size B, we index over the batch dimension with l and use
s0l to denote the state following sl, so the tuple consists of (sl, al, rl, s0l). For importance sampled
estimators we sample N actions for each state sl and use alk to denote sample k for state l.
We learn a set of n policies, one for each task in T indexed by i. However, we write the losses for a
single policy and drop i for notational simplicity.
C.2.1	Proposal loss
The proposal loss minimizes the KL divergence between the Boltzmann distribution π(a∣s) 8
exp(2Q(s, a)) and the proposal distribution.
L(θq) = Er [Ea〜∏(∙∣s)[log∏(a∣st) - log qθq (a|st)]]
(39)
As described in the text, this loss is estimated using importance sampling with a mixture distribu-
tion p(a|s) containing equally weighted components consisting of the target proposal distribution
qθ0 (a|s) for all policies and the uniform distribution.
11
P⑷S) = n+1 VA +
(40)
16
Under review as a conference paper at ICLR 2019
where V A is the volume of the action space (which is always bounded in our case).
The proposal loss is estimated using self-normalized importance sampling
C.2.2 Value loss
The soft value loss is
1BN
L(θq) ≈ - BEEwkl log qθq (alst),
wk0 l
1 (QΘq (Sk, akl)) ;
p(akl∣Sk)	;
wkl
wkl0
1 wkm
L(θV) =ER
1	12
2(Vθv (St) - α log J exp(αQθQ (st,a))da)2
(41)
(42)
(43)
We estimate this using importance sampling with the proposal distribution qθq (a|S) which is trying
to fit the policy π.
L(θv) ≈ 2B X (Vθv (si) — α log Z)2	(44)
l=1
7 —	I S eχp( 1 QθQ(si,aik))]	∕z1<∖
Z = N 2 qθq (aik∣sι) J	(45)
C.2.3 Action-value loss
The TD(0) loss for QθQ is
L(θQ) =ER 2(QΘq (St, at) - (r(st, at, st+1) + γVθV (st+1)))2	(46)
This does not require importance sampling to estimate and can be straightforwardly estimated as
1B
L(θQ) ≈ 2B E(QΘq (sl,al) - (rl + γ%V (SO)))2.	(47)
i=1
The action-value is parametrized as an advantage function QθQ (S, a) = Vθ0 (S) + AθA (S, a).
C.2.4 State Dependent Successor Features loss
To facilitate max-ent GPI we learn successor features for each policy, both state-action dependent
features ψθψ (S, a) and state-dependent ΥθΥ (S). As with value, we use a target network for the
state-dependent features Υθ0 (S)
L(θY)=ER 2(γθγ (St)- Eat〜∏(at∣st)[ψθψ (st, at) + α1(-qΘq (st, at) + α log Z(St))])2
This loss is estimated using self-normalized importance sampling with proposal qθq
1BN
L(θγ) ≈ 2B ^X^X wlk [(ψθ ψ (S l, alk ) - QθQ (S l, alk ) + α log Z (Si))2] ,
exp(1 Qi(Si ,ai k))
wlk Z ---a---:-7--.
qθq (alk |Si)
(48)
(49)
17
Under review as a conference paper at ICLR 2019
We use the importance sampled estimate of Z from eq 47, rather than the value network which may
be lagging the true partition function. As with the value estimate, we use self-normalized importance
sampling to avoid the importance weights depending on α log Z(sl) (this introduces a bias, but in
practise appears to work well).
C.2.5 State-Action Dependent Successor Features loss
The state-action dependent successor feature loss is
L(θψ) =Er 2(Ψθq(st,at) - (φ(st,at,st+ι) + γΥθγ(st+ι)))2 .	(50)
for which we use the following estimator
1B
L(θψi) ≈ 2b y^(ψθψ (Sl ,aI)-(Ol+Y γθγ (Sl))).	(5i)
l=1
ψθψ is parametrized as a “psi-vantage” ψθψ (S, a) = Υθ0 (S) + ψθA (S, a).
C.2.6 DC correction
We learn the divergence correction for each pair of policies ∏i(a∣s), ∏(a∣s). As described in the
text, in order to learn CθC (S, a, b) for all b ∈ [0, 1], we sample b. We also use a target network
Cθ0 (S, a, b). The loss is then
L(θc) = Es 〜R,b〜U (o,i)[ 1 (Cθc(s,a,b) + αγEp3∣s,a) [log JA exp(b log ∏i(a0 |s0)+	(52)
(1 — b)∏j(a0∣s0)—工°"。0 (s' a0, b))da0])2]∙
This loss is challenging to estimate, due to the dependence on two policies. We importance sample
using a mixture of all proposal distributions uniform p(a|S) (equation 40). We denote the samples
of b 〜U(0,1) for each batch entry bl. The importance sampled estimator is then
1 B	1 N Cθta0 rget(S0l, a0lk, bl)	2
L(θC) ≈ N X (Cθc (Sl,al,bl) - αγ log [N X	Cp(alk0 |sl)-D , (53)
Cθarget(sl, alk ,bl) ≡ exp( — (blQθQ (SI, alk) + (1 - bl)QθQ (sl, alk) - CθC (Sl alk ,bl))∙	(54)
We parametrized CθC as an advantage function CθC (S, a, b) = CθA (S, a, b) + CθB (S, b) with an
additional loss to constrain this parametrization
L(Θb) = Ea〜q(∙∣s)-R 1(CΘCa (S,a, b))2	(55)
which can be straightforwardly estimated by sampling from q
1 BN
L(Θb) ≈ 2NB XX(CACA (Sl，a，k,bl))	(56)
l=1 k=1
CondQ We also consider, as a control, learning the action-value function conditional on b directly
(Schaul et al., 2015), in a similar way to the DC correction. We learn both a conditional value
VθV (S， b) and QθQ (S， a， b), again by sampling b uniformly each update.
L(BVb ) = ER,b〜U(0,1)
LθQ = ER,b〜U (0,1)
11
2(Vθvb (S，b) — — log	exp(—Q&Qb (S，a，b)))2
2 (QθQb (S，a，b) - (rb + γVθVb (SO，b)))2，
(57)
(58)
18
Under review as a conference paper at ICLR 2019
where computing rb for arbitrary b requires φ to have been observed.
We estimate Cond-Q with the same importance samples as C from p(a|s) and again sample
b 〜 U(0,1) for each entry in the batch. We use target networks for Vθ" (s, b) and parametrize
QθQ (s, a, b) = VθV0 (s, b) + AθA (s, a, b).
The conditional value estimator is
L(θV) ≈
l=1
exP(αQθQb (sι,aik,bι)
p(aik ∣sι)
(59)
and action-value estimator is
1B	2
L(θQ) ≈ 2B X (QθQb (Sl, al ,bl) - (rb + γVθ0vb (sl, bl)f)
(60)
D Justification for the DC-Cheap heuristic
We wish to estimate Cb∞(s, a) (defined in Theorem A.1) while avoiding learning a conditional func-
tion of b. We make two (substantial) assumptions to arrive at this approximation.
Firstly, We assume policies ∏i(a∣s), ∏j(a|s) are Gaussian
∏i(a|s) = exp (- (a - μi(2)) )	(61)
2σ(s)2
and the variance σ(s) is the same for both policies given a state (it may vary across states).
Secondly, we assume Cb(k) (s, a) = Cb(k)(s) is independent of action. This is approximately correct
when nearby states have similar Renyi divergences between policies.
We make use of a result by Gil et al. (2013) that states that the Renyi divergence of order b for two
Gaussians of the same variance is
Db (N (μi,σ)kN (μ2,σ)) = 1 "μl -2 μ2) .	(62)
2	σ2
We first define
Gb(S) ≡ (1 -b)Db (∏i(∙∣s)k∏j(∙∣s)) = — log/
πi (a|S)b πj (a|S)(1-b) da.
From equation 61
Gb(S) = 4b(1 - b)Gi (s).
(63)
(64)
Given these assumptions we show inductively that Cb(k)(S, a) = 4b(1 - b)C1(k/2) (S, a) ∀k, b ∈ [0, 1].
Since Cb(0) (S, a) = 0 ∀b ∈ [0, 1], a ∈ A, S ∈ S this is true for k = 0. We show it holds inductively
Cbk+1)(s,a) = -αYEp(s0∣s,a)
log / πi(a0∣s0)b∏j(a0∣s0)(1-b) exp(-LCbk)(S0, a0))da0
Aα
YEp(SIs,αJαGb(s0) + Cbk)(S0)]
4(1 - b)bC 1k+1)(S,a).
2
(65)
(66)
(67)
Obviously these assumptions are not justified. However, note that we estimate the true divergence
for C∞2, i.e. without any assumptions of Gaussian policies and this heuristic is used to estimate
C∞ from C∞2∙ In practise, we find this heuristic works in many situations where the policies have
similar variance, particulary when bounded by GPI.
19
Under review as a conference paper at ICLR 2019
0.35
0.10
0.30
5 O
2 2
0.0.
UJn+J0工
Co GPI DC CondQ
Method
0.6-
…CO
--GPI
-DC
u」nl①H
a	bc
Figure 5: (a) Trajectories of the ant during transfer on non-composable subtasks. In this experiment
the two base tasks consists of rewards at the red and green square respectively. As expected, in
this task, where the two base tasks have no compositional solution, CO (red) performs poorly with
trajectories that end up between the two solutions. GPI (blue) performs well, as does DC (black).
CondQ does slightly worse.
(b)	Box-plot of returns from 5 seeds (at b = 0.5).
(c)	Returns as a function of b, SEM across 5 seeds is plotted, but is smaller than the line thickness.
E	Additional Experiment
F	Experiment details
All control tasks were simulated using the MuJoCo physics simulator and constructed using the DM
control suite (Tassa et al., 2018) which uses the MuJoCo physics simulator (Todorov et al., 2012).
Figure 6: Jumping ball tricky task
The point mass was velocity controlled, all other tasks were torque controlled. The planar manipu-
lator task was based off the planar manipulator in the DM control suite. The reward in all tasks was
sparse as described in the main text.
During training for all tasks we start states from the randomly sampled positions and orientations.
For the point mass, jumping ball and ant we evaluated transfer starting from the center (in the walker
environments, the starting orientation was randomly sampled during transfer, the point mass does not
have an orientation). For the planar manipulator transfer was tested from same random distribution
as in training. For all tasks were learned infinite time horizon policies.
Transfer is made challenging by the need for good exploration. That was not the focus on this work.
We aided exploration in several ways: during training we acted according to a higher-temperature
policy αe = 2α. We also sampled actions uniformly in an -greedy fashion with = 0.1 and added
Gaussian exploration noise during training. This was sufficient to explore the state space for most
20
Under review as a conference paper at ICLR 2019
tasks. For the planar manipulator and the jumping ball, we found it necessary to induce behavior
tasks by learning tasks for reaching the blue target. This behavior policy was, of course, only used
for experience and not during transfer.
Below we list the hyper-parameters and networks use for all experiment. The discount γ and α
were the only sensitive parameters that we needed to vary between tasks to adjust for the differing
magnitudes of returns and sensitivity of the action space between bodies. If α is too small then the
policies often only find one solution and all transfer approaches behave similarly, while for large α
the resulting policies are too stochastic and do not perform well.
Proposal learning rate	10-3
All other learning rates	10-4
Value target update period	200
Proposal target update period	200
Υ target update period	500
Number of importance samples for all estimators during learning	200
Number of importance samples for acting during training	50
Number of importance samples for acting during transfer	1000
Table 2: Parameters the same across all experiments
The state vector was preprocessed by a linear projection of 3× its dimension and then a tanh non-
linearity. All action-state networks (Q, ψ, C) consisted of 3 hidden layers with elu non-linearities
(Clevert et al., 2015), with both action and preprocessed state projected by linear layers to be of the
same dimensionality and used for input the first layer. All value networks and proposal networks
consisted of 2 layers with elu non-linearities. The number of neurons in each layer was varied
between environments, but was kept the same in all networks and layers (we did not sweep over this
parameter, but choose a reasonable number based on our prior on the complexity of the task).
Below we list the per task hyper-parameters
Task	Number of units	α	γ
Point mass	22	1	0.99
Planar Manipulator	192	0.05	0.99
Jumping Ball	192	0.2	0.9
Ant	252	0.1	0.95
Table 3: Parameters varied between experiments
21