Under review as a conference paper at ICLR 2019
Invariant-equivariant representation learn-
ing for multi-class data
Anonymous authors
Paper under double-blind review
Ab stract
Representations learnt through deep neural networks tend to be highly informa-
tive, but opaque in terms of what information they learn to encode. We introduce
an approach to probabilistic modelling that learns to represent data with two sepa-
rate deep representations: an invariant representation that encodes the information
of the class from which the data belongs, and an equivariant representation that
encodes the symmetry transformation defining the particular data point within the
class manifold (equivariant in the sense that the representation varies naturally
with symmetry transformations). This approach to representation learning is con-
ceptually transparent, easy to implement, and in-principle generally applicable to
any data comprised of discrete classes of continuous distributions (e.g. objects
in images, topics in language, individuals in behavioural data). We demonstrate
qualitatively compelling representation learning and competitive quantitative per-
formance, in both supervised and semi-supervised settings, versus comparable
modelling approaches in the literature with little fine tuning.
1	Introduction
Representation learning (Bengio et al., 2013) is part of the foundation of deep learning; powerful
deep neural network models appear to derive their performance from sequentially representing data
in more-and-more refined structures, tailored to the training task.
However, representation learning has a broader impact than just model performance. Transfer-
able representations are leveraged efficiently for new tasks (Mikolov et al., 2013), representations
are used for human interpretation of machine learning models (Mahendran & Vedaldi, 2015), and
meaningfully structured (disentangled) representations can be used for model control (e.g. semi-
supervised learning as in Kingma et al. (2014), topic modelling as in Blei et al. (2003)).
Consequently, it is often preferable to have interpretable data representations within a model, in the
sense that the information contained in the representation is easily understood and the representation
can be used to control the output of the model (e.g. to generate data of a given class or with a
particular characteristic). Unfortunately, there is often a tension between optimal model performance
and cleanly disentangled or controllable representations.
To overcome this, some practitioners have proposed modifying their model’s objective functions by
inserting parameters in front of particular terms (Bowman et al., 2016; Higgins et al., 2017), while
others have sought to modify the associated generative models (Mansbridge et al., 2018). Further
still, attempts have been made to build the symmetries of the data directly into the neural network
architecture in order to force the learning of latent variables that transform meaningfully under those
symmetries (Sabour et al., 2017). The diversity and marginal success of these approaches point to
the importance and difficulty of learning meaningful representations in deep generative modelling.
In this work we present an approach to probabilistic modelling of data comprised of a finite number
of distinct classes, each described by a smooth manifold of instantiations of that class. For conve-
nience, we call our approach EquiVAE for Equivariant Variational Autoencoder. EquiVAE is a
probabilistic model with 2 latent variables: an invariant latent that represents the global class in-
formation, and an equivariant latent that smoothly interpolates between all of the members of that
class. The EquiVAE approach is general in that the symmetry group of the manifold must not be
specified (as in for example Cohen & Welling (2014); Falorsi et al. (2018)), and it can be used for
1
Under review as a conference paper at ICLR 2019
any number of classes and any dimensionality of both underlying representations. The price that
must be paid for this level of model control and flexibility is that some labelled data is needed in
order to provide the concept of class invariance versus equivariance to the model.
The endeavor to model the content and the style of data separately is certainly not new to this
work (Tenenbaum & Freeman, 2000). Reed et al. (2014) and Radford et al. (2016) go further,
disentangling the continuous sources of variation in their representations using a clamping technique
that exposes specific latent components to a single source of variation in the data during training.
In the same vein, other approaches have used penalty terms in the objective function that encourage
the learning of disentangled representations (Cheung et al., 2014; Chen et al., 2016).
EquiVAE does not require any modification to the training algorithm, nor additional penalty terms
in the objective function in order to bifurcate the information stored in the two latent variables. This
is due to the way in which multiple data points are used to reconstruct a single data point from the
same-class manifold, which we consider the primary novel aspect of our approach. In particular, our
invariant representation takes as input multiple data points that come from the same class, but are
different from the data point to be reconstructed. This invariant representation thus directly learns
to encode the information common to the overall class, but not the individual data point, simply due
to the information flowing through it.
Of further note, we deliberately use a deterministic latent for the invariant representation, and a
stochastic latent for the smooth equivariant representation (an idea also employed by Zhu et al.
(2014)). This choice is why we do not need to explicitly force the equivariant latent to not contain
any class-level information: it is available and easier to access from the deterministic latent.
EquiVAE is also comparable to Siddharth et al. (2017), where the authors leverage labelled data ex-
plicitly in their generative model in order to force the VAE latent to learn the non-class information
(Makhzani et al. (2016) do similarly using adversarial training). The primary difference between
those works and ours is that EquiVAE provides a non-trivial representation of the global informa-
tion instead of simply using the integer-valued label. Furthermore, this invariant representation can
be deterministically evaluated directly on unlabelled data. Practitioners can reuse this embedding
on unlabelled data in downstream tasks, along with the equivariant encoder if needed. The invariant
representation provides more information than a simple prediction of the class-label distribution.
The encoding procedure for the invariant representation in EquiVAE is partially inspired by Eslami
et al. (2018), who use images from various, known coordinates in a scene in order to reconstruct
a new image of that scene at new, known coordinates. In contrast, we do not have access to the
exact coordinates of the class instance, which in our case corresponds to the unknown, non-trivial
manifold structure of the class; we must infer these manifold coordinates in an unsupervised way.
Garnelo et al. (2018a;b) similarly explore the simultaneous usage of multiple data points in genera-
tive modelling in order to better capture modelling uncertainty.
2	Equivariant Variational Autoencoders
We consider a generative model for data comprised of a finite set of distinct classes, each of which
occupies a smooth manifold of instantiations. For example, images of distinct objects where each
object might be in any pose, or sentences describing distinct sets of topics. Such data should be
described by a generative model with two latent variables, the first describing which of the objects
the data belongs to, and the second describing the particular instantiation of the object (e.g. its pose).
In this way the object-identity latent variable r would be invariant under the transformations that
cover the set of possible instantiations of the object, and the instantiation-specifc latent variable v
should be equivariant under such transformations. Note that the class label y is itself an invari-
ant representation of the class, however, we seek a higher-dimensional latent vector r that has the
capacity to represent rich information relevant to the class of the data point, rather than just its label.
Denoting an individual data point as xn with associated class label yn, and the full set of class-y
labeled data {xn|label(xn) = y} as Dlyab, we write such a generative model as:
dvn drn Pθ (Xn∣rn,Vn) δ 鼠一『①黑 \ {Xn})) P(Vn) p(jn)
(1)
2
Under review as a conference paper at ICLR 2019
where θ are the parameters of the generative model. We make explicit with a δ function the condi-
tional dependency ofpθ on the deterministically calculable representation rn of the global properties
of class yn. The distribution p(yn) is a categorical distribution with weights given by the relative
frequency of each class and the prior distribution p(vn) is taken to be a unit normal describing the
set of smooth transformations that cover the class-yn manifold.
To guarantee that rn will learn an invariant representation of information common to the class-yn
data, we use a technique inspired by Generative Query Networks (Eslami et al., 2018). Instead of
encoding the information of a single data point (xn , yn) into rn, we provide samples from the whole
class-yn manifold. That is, we compute the invariant latent as:
concise	m
r(Diyn \ {xn}) no-on Tyn = Ex 〜Dyn\{xn} [fθiηv (X)] ≈	X f‰v (Xi)	⑵
m i=1
where θinv are the parameters of this embedding. We expiicitiy exciude the data point at hand Xn
from this expectation vaiue; in the infinite iabeiied data iimit, the probabiiity of sampiing Xn from
Diyanb wouid vanish. We inciude the simpiified notation ryn for subsequent mathematicai ciarity.
This procedure invaiidates the assumption that the data is generated i.i.d. conditioned on a set of
modei parameters, since ry is computed using a number of other data points Xi with iabei y. For
notationai simpiicity, we wiii suppress this fact, and consider the iikeiihood p(X, y) as if it were
i.i.d. per data point. It is not difficuit to augment the equations that foiiow to incorporate the fuii
dependencies, but we find this to obfuscate the discussion (see Appendix B for fuii derivations). We
ignore the bias introduced from the non-i.i.d. generation process; this couid be avoided by hoiding
out a dedicated iabeiied data set (Garneio et ai., 2018a;b), but we find it empiricaiiy insignificant.
The primary purpose of our approach to the invariant representation used in Equation 2 is to pro-
vide exactiy the information needed to iearn a giobai-ciass embedding: nameiy, to iearn what the
eiements of the ciass manifoid have in common. However, our approach provides a secondary ad-
vantage. During training we wiii use vaiues of m (see Equation 2) sampied uniformiy between 1
and some smaii maximai vaiue mmax . The ry embedding wiii thus iearn to work weii for various
vaiues of m, inciuding m = 1. Consequentiy, at inference time, any uniabeiied data point X can be
immediateiy embedded via fθinv (X). This is ideai for downstream usage; we wiii use this technique
in Section 3.1 to competitiveiy ciassify uniabeiied test-set data using oniy fθinv (X).
In order to approximate the integrai in Equation 1 over the equivariant iatent, we use variationai
inference foiiowing the standard VAE approach (Kingma & Weiiing, 2014; Rezende et ai., 2014):
qφcοv (v|ry ,x) = N(μφcοv(ry ,x),σφcov (ry ,x)I)	⑶
where φcov are the parameters of the variationai distribution over the equivariant iatent. Note that
v is inferred from ry (and X), not y, since ry is posited to be a muiti-dimensionai iatent vector that
represents the rich set of giobai properties of the ciass y, rather than just its iabei. As is shown
empiricaiiy in Section 3, v oniy iearns to store the intra-ciass variations, rather than the ciass-iabei
information. This is because the ciass-iabei information is easier to access directiy from the deter-
ministic representation ry, rather than indirectiy through the stochastic v. We choose to provide both
ry and X to qφcov (v|ry, X) in order to provide the variationai distribution with more flexibiiity.
We thus arrive at a iower bound on log p(X, y) given in Equation 1 foiiowing the standard arguments:
Liab = Eq(v|ry,x) log p(X|ry, v) - DKL q(v|ry, X)p(v) + log p(y)	(4)
where the various modei parameters are suppressed for ciarity.
The intuition associated with the inference of ry from muitipie same-ciass, but compiementary data
points, as weii as the inference of v from ry and X is depicted heuristicaiiy in Figure 1.
EquiVAE is designed to iearn a representation that stores giobai-ciass information, and as such, it
can be used for semi-supervised iearning. Thus, an objective function for uniabeiied data must be
specified to accompany the iabeiied-data objective function given in Equation 4.
We marginaiise over the iabei in p(X, y) (Equation 1) when a data point is uniabeiied. In order to
perform variationai inference in this case, we use a variationai distribution of the form:
q(v,y|x) = qφcοv (v|ry,x) qφy^ (y|x)	⑸
3
Under review as a conference paper at ICLR 2019
{χ1,X2, . . . , Xrn}——> Ty
(embedding global class-
manifold information)
x,ry ——> v
(embedding info, associated
with instance x given class)
Figure 1: Heuristic depiction of class-y manifold. The invariant latent ry will encode global-
manifold information, whereas equivariant latent v will encode coordinates of x on the manifold.
where qφcov (v |ry, x) is the same distribution as is used in the labelled case, given in Equation 3.
The unlabelled setting requires an additional inference distribution to infer the label y, which is
achieved with qφy-post (y|x), parametrised by φy-post. Once y is inferred from qφy-post (y|x), ry can be
deterministically calculated using Equation 2 from the labelled data set Dlyab for class y, of which x
is no longer a part. With ry and x, the equivariant latent v is inferred via qφcov (v|ry, x).
Using this variational inference procedure, we arrive at a lower bound for log p(x):
Lunlab = Eq(y|x) Eq(v|ry,x) logp(x|ry,v) - DKLq(v|ry, x)p(v)	- DKLq(y|x)p(y)	(6)
where the model parameters are again suppressed for clarity.
We will compute the expectation over the discrete distribution q(y|x) in Equation 6 exactly in order
to avoid the problem of back propagating through discrete variables. However, this expectation
could be calculated by sampling using standard techniques (Brooks et al., 2011; Jang et al., 2017;
Maddison et al., 2017).
Therefore, the evidence lower bound objective for semi-supervised learning becomes:
log p(x) +	logp(x,y) ≥ Lsemi =	Lunlab +	Llab	(7)
x, unlab.	(x,y), lab.	x, unlab.	(x,y), lab.
In order to ensure that qφy-post (y|x) does not collapse into the local minimum of predicting a single
label for every x value, we add log qφy-post (y|x) to Llab. This is done in Kingma et al. (2014) and
Siddharth et al. (2017), however, we do not add any hyperparameter in front of this term unlike those
works. We also do not add a hyperparameter up-weighting Llab overall, as is done in Siddharth et al.
(2017). The only hyperparameter tuning we perform is to choose latent dimensionality (either 8 or
16) and to choose mmax, between 1 and which m (see Equation 2) varies uniformly during training.
3	Results
We carry out experiments on both the MNIST data set (LeCun et al., 1998) and the Street View
House Numbers (SVHN) data set (Netzer et al., 2011). These data sets are appropriately modelled
with EquiVAE, since digits from a particular class live on a smooth, a-priori-unknown manifold.
EQUIVAE requires some labelled data. Forcing the model to reconstruct x through a representation
ry that only has access to other members of the y class is what forces ry to represent the common
information of that class, rather than a representation of the particular instantiation x. Thus, the
requirement of some labelled data is at the heart of EquiVAE. Indeed, we trained several versions
of the EQUIVAE generative model in the unsupervised setting, allowing r to receive x directly.
The results were as expected: the equivariant latent is completely unused, with the model unable to
reconstruct the structure in each class, as it essentially becomes a deterministic autoencoder.
In Section 3.1, we study the invariant-equivariant properties of the representations learnt with
EquiVAE in the supervised setting, where we have access to the full set of labelled training data.
The semi-supervised learning setting is discussed in Section 3.2. The details of the experimental
setup used in this section are provided in Appendix A.
4
Under review as a conference paper at ICLR 2019
Figure 2: Validation-set results for EquiVAE on MNIST. Invariant (left) and equivariant (middle)
latent representations are shown reduced to 2D using UMAP. Learning curves are shown (right) with
the ELBO broken down into the sum of the reconstruction and (negative) KL terms, as in Equation 4.
3.1	Supervised learning
With the full training data set labelled, EquiVAE is able to learn to optimise both equivariant
and invariant representations at every training step. The supervised EquiVAE (objective given in
Equation 4) converges in approximately 40 epochs on the MNIST training datset of 55,000 data
points and in 90 epochs on the SVHN training datset of 70,000 data points.
The training curves on MNIST are shown on the right in Figure 2. The equivariant latent is learning
to represent non-trivial information from the data as evidenced by the KL between the equivariant
variational distribution and its prior not vanishing at convergence.
However, when visualised in 2 dimensions using UMAP for dimensional reduction (McInnes &
Healy, 2018), the equivariant latent v appears not to distinguish between digit classes, as is seen in
the uniformity of the class-coloured labels in the middle plot of Figure 2. The apparent uniformity
of v reflects two facts: the first is that, given that the generative model gets access to another latent
containing the global class information, the equivariant latent does not need to distinguish between
classes. The second is that the equivariant manifolds should be similar across all MNIST digits.
Indeed, they all include rotations, stretches, stroke thickness, among smooth transformations.
Finally, on the left in Figure 2, the invariant representation vectors ry are shown, dimensionally
reduced to 2 dimensions using UMAP, and coloured according to the label of the class. These
representations are well separated for each class. Each class has some spread in its invariant rep-
resentations due to the fact that we choose relatively small numbers of complementary samples, m
(see Equation 2). The model shown in Figure 2 had m randomly selected between 1 and 7 during
training, with m = 5 used for visualisation. The outlier points in this plot are exaggerated by the
dimensional reduction; we show this below in Figure 4 by considering a EquiVAE with 2D latent.
The SVHN results are similar to Figure 2, with slightly less uniformity in the equivariant latent.
All visualisations in this work, including those in Figure 2, use data from the validation set (5,000
images for MNIST; 3,257 for SVHN). We reserve the test set (10,000 images for MNIST; 26,032 for
SVHN) for computing the accuracy values provided. We did not look at this test set during training
and hyperparameter tuning. In terms of hyperparameter tuning, we only tuned the number of epochs
for training, the range of m values (i.e. mmax = 7 for MNIST; mmax = 10 for SVHN), and chose
between 8 and 16 for the dimensionality of both latents (16 chosen for both data sets). We fixed the
architecture at the outset to have ample capacity for this task, but did not vary it in our experiments.
In order to really see what information is stored in the equivariant and invariant latents, we consider
them in the context of the generative model. We show reconstructed images in various latent-variable
configurations in Figure 3. To show samples from the equivariant priorp(v) we fix a single invariant
representation ry for each class y by taking the mean ry over each class in the validation set. On
the left in Figure 3 we show random samples from p(v) reconstructed along with ry for each class
y ascending from 0 to 9 in each column. Two properties stand out from these samples: firstly, the
5
Under review as a conference paper at ICLR 2019
Figure 3: Generated images (left) sampled from the prior p(v) for each ry, (middle) reconstructed
from equivariant interpolations between the embeddings of same-class digits with fixed ry, and
(right) reconstructed from latent pairs (ryi , vj) where (ryi , vi) is an encoded image with y = i.
。夕00 OOQdOO
夕qq 夕q∙夕夕4σ-
*∕9gpB∕oegt>*
7777771777
64666』2/G4
5S5555 s£55
diaru∙y4^∙u'uiuiv
C033333 彳彳 33
22∕c42-∕22Ia
FrFFM7F^SFY
77777777Vq
bbGb6G6664
夕ʃʃʃʃrrrrʃ
HqH4MMU:Tyjr
3333333333
乙乙2.Z-ZX7-
H H n Jf ∕∙ /I 1/ /1 // 1/
Gooooooooo
<P∕N3√S6 7 Oe 9
Λu∕λ3⅛54 70flβf
Z?/J3，54 7,夕
0J,o3^5α7S。
?//3ys4 7g夕
i9∕∕3v5∙4 7夕夕
O 11 3 9 S/W «r Oo QI
(9∕G3√567夕9
/∕NyFr/ 7，，
4 // τ√∙ TJ /f ʃ Zi 7- ʃ /■
samples are (almost) all from the correct class for MNIST (SVHN), showing that the invariant latent
is representing the class information well. Secondly, there is appreciable variance within each class,
showing that samples from the prior p(v) are able to represent the intra-class variations.
The middle plot in Figure 3 shows interpolations between actual digits of the same class (the top
and bottom rows of each subfigure), with the invariant representation fixed throughout. These in-
terpolations are smooth, as is expected from interpolations of a VAE latent, and cover the trajectory
between the two images well. This again supports the argument that the equivariant representation v ,
as a stochastic latent variable, is appropriate for representing the smooth intra-class transformations.
To create the right-hand side of Figure 3, a validation-set image for each digit i is encoded to create
the set of latents {ryi=i, vi}i9=0, from which we reconstruct images using the latent pairs (ryi =i, vj)
for i, j = 0, . . . , 9. Thus we see in each row a single digit and in each column a single style. It
is most apparent in the SVHN results that the equivariant latent controls all stylistic aspects of the
image, including the non-central digits, whereas the invariant latent controls only the central digit.
In Figure 4 we show an EquiVAE trained with 2-dimensional invariant and equivariant latents
on MNIST for clearer visualisation of the latent space. On the right, reconstructions are shown
with fixed ry for each y = 3, 4, 5, 6 and with the identical set of evenly spaced v over a grid
spanning from -2 to +2 in each coordinate (2 prior standard deviations). The stylistic variations
appear to be similar for the same values of v across different digits, as was partially evidence on the
right of Figure 3. On the left of Figure 4 we see where images in the validation set are encoded,
showing significant distance between clusters when dimensional reduction is not used. Finally, in
the middle of Figure 4 we see the evenly-spaced reconstruction of the full invariant latent space.
This shows that, though the invariant latent is not storing stylistic information, it does contain the
relative similarity of the base version of each digit. This lends justification to our assertion that the
invariant latent ry represents more information that just the label y.
6
Under review as a conference paper at ICLR 2019
Large scale variation of invariant rep
2-sigma around 2D equivariant rep
*”3¥4444,6
√ y ¥ ¥ 44,6
“y4 q4d 666G
“√zrq”>3G6GQ
α "MHr。SJSU
Js，33SS∙Γ5S
333336 5 S 5 5
3335355555
ʒʒʒʒ ⅜S55S5
3y55551g夕夕99
 
33g555gqqo90
33Ms5sggg99 7
333ss∙qqggg97
3M3Mssqggoo7
333?Crqgqgg7 7
333MgqOoO∙qg77
3339030^0∙0βo∙77 7
333920∙oβo∙Q∙77 7
aa3□Qc∙0∙0∙g777
aaɔ□□□c∙o∙g?J
2D invariant rep latent space
Figure 4: Latent variables for EquiVAE with 2 dimensional latents. The invariant latent space (left),
reconstructions from evenly-spaced variations of ry covering the full space at fixed v = ~0 (middle),
and reconstructions from 2-prior-standard-deviation variations of v at fixed ry (right) are shown.
OOOO
OOOO
OOOD
OOOO
OOOZ
Table 1: Supervised error rates on MNIST (10,000 images) and SVHN (26,032 images) test sets.
Technique
Error rate
ISlN≡NHAS
EQUIVAE Benchmark neural classifier	0.84 ± 0.03
EQUIVAE (neural classifier using fθinv (x))	0.82 ± 0.03
EQUIVAE (distance based on fθinv (x))	0.82 ± 0.05
Stacked VAE (M1+M2) (Kingma et al., 2014)	0.96
Adversarial Autoencoders (Makhzani et al., 2016)	0.85 ± 0.02
EQUIVAE Benchmark neural classifier	10.04 ± 0.14
EQUIVAE (neural classifier using fθinv (x))	11.97 ± 0.34
EQUIVAE (distance based on fθinv (x))	12.30 ± 0.28
We have thus seen that the invariant representation ry in EQUIVAE learns to represent global-
class information and the equivariant representation v learns to represent local, smooth, intra-class
information. This is what we expected from the theoretical considerations given in Section 2.
We now show quantitatively that the invariant representation ry learns the class information by
showing that it alone can predict the class y as well as a dedicated classifier. In order to predict an
unknown label, we employ a direct technique to compute the invariant representation ry from x.
We simply use fθinv (x) from Equation 2. We can then pass fθinv (x) into a neural classifier, or we
can find the nearest cluster mean ry from the training set and assign class probabilities according
to p(label(x) = y) ɑ exp(-∣∣fθinv(x) 一 ry||2). We find that classifying test-set images using this
0-parameter distance metric performs as well as using a neural classifier (2-layer dense dropout
network with 128, 64 neurons per layer) with fθinv(x) as input. Note that using p(y∣x) a p(x, y) is
roughly equivalent to our distance-based classifier, as p(y|x) will be maximal when ry (computed
from the training data with label y) is most similar to fθinv (x). Thus, our distance-based technique is
a more-direct approach to classification than using p(y|x).
Our results are shown in Table 1, along with the error rate of a dedicated, end-to-end neural network
classifier, identical in architecture to that of fθinv (x), with 2 dropout layers added. This benchmark
classifier performs similarly on MNIST (slightly better on SVHN) to the simple classifier based
on finding the nearest training-set cluster to fθinv (x). This is a strong result, as our classification
algorithm based on fθinv (x) has no direct classification objective in its training, see Equation 4. Our
uncertainty bands quoted in Table 1 use the standard error on the mean (standard deviation divided
by √N — 1), with N = 5 trials.
Results from selected, relevant works from the literature are also shown in Table 1. Kingma et al.
(2014) do not provide error bars, and they only provide a fully supervised result for their most-
7
Under review as a conference paper at ICLR 2019
Table 2: Semi-supervised test-set error rates for various labelled-data-set sizes.
	Labels	EquiVAE	Benchmark	Siddharthet al.(2017)	Kingma et al. (2014)
	100	8.90 ± 0.70	21.91 ± 0.66	9.71 ± 0.91	(M2) 11.97 ± 1.71
	600	3.99 ± 0.17	6.64 ± 0.35	3.84 ± 0.86	4.94 ± 0.13
	1000	3.34 ± 0.17	5.43 ± 0.31	2.88 ± 0.79	3.60 ± 0.56
	3000	2.23 ± 0.14	2.96 ± 0.11	1.57 ± 0.93	3.92 ± 0.63
	1000	37.95 ± 0.66	39.64 ± 1.47	38.91 ± 1.06	(M1+M2) 36.02 ± 0.10
	3000	24.95 ± 0.57	25.50 ± 0.91	29.07 ± 0.83	—
powerful, stacked / pre-trained VAE M1+M2, but it appears as if their learnt representation is less
accurate in its classification of unlabelled data. Makhzani et al. (2016) perform slightly worse than
EquiVAE, but within error bars. Makhzani et al. (2016) train for 100 times more epochs than we do,
and use over 10 times more parameters in their model, although they use shallower dense networks.
Note also that Kingma et al. (2014) and Makhzani et al. (2016) train on a training set of 50,000
MNIST images, whereas we use 55,000. We are unable to compare to Siddharth et al. (2017) as
they do not provide fully supervised results on MNIST, and none of these comparable approaches
provide fully supervised results on SVHN.
3.2	Semi-supervised learning
For semi-supervised learning, we maximise Lsemi given in Equation 7. Test-set classification error
rates are presented in Table 2 for varying numbers of labelled data. We compare to a benchmark
classifier with similar architecture to qφy-post (y|x) (see Equation 5) trained only on the labelled data,
as well as to similar VAE-based semi-supervised work. The number of training epochs are chosen
to be 20, 25, 30, and 35, for data set sizes 100, 600, 1000, and 3000, respectively, on MNIST, and
20 and 30 epochs for data set sizes 1000, and 3000, respectively, on SVHN. We use an 8D latent
space and mmax = 4 for MNIST, and an 16D latent space and mmax = 10 for SVHN. Otherwise,
no hyperparameter tuning is performed. Each of our experiments is run 5 times to get the mean and
(standard) error on the estimate of the mean in Table 2.
We find that EquiVAE performs better than the benchmark classifier with the same architecture
(plus two dropout layers appended) trained only on the labelled data, especially for small labelled
data sets. Furthermore, EquiVAE performs competitively (within error bars or better) relative to
its most similar comparison, Siddharth et al. (2017), which is a VAE-based probabilistic model that
treats the labels and the style of the data separately. Given its relative simplicity, rapid convergence
(20-35 epochs), and lack of hyperparameter tuning performed, we consider this to be an indication
that EquiVAE is an effective approach to jointly learning invariant and equivariant representations,
including in the regime of limited labelled data.
4	Conclusions
We have introduced a technique for jointly learning invariant and equivariant representations of data
comprised of discrete classes of continuous values. The invariant representation encodes global
information about the given class manifold which is ensured by the procedure of reconstructing a
data point through complementary samples from the same class. The equivariant representation
is a stochastic VAE latent that learns the smooth set of transformations that cover the instances
of data on that class manifold. We showed that the invariant latents are so widely separated that a
99.18% accuracy can be achieved on MNIST (87.70% on SVHN) with a simple 0-parameter distance
metric based on the invariant embedding. The equivariant latent learns to cover the manifold for
each class of data with qualitatively excellent samples and interpolations for each class. Finally,
we showed that semi-supervised learning based on such latent variable models is competitive with
similar approaches in the literature with essentially no hyperparameter tuning.
8
Under review as a conference paper at ICLR 2019
References
Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013.
D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. Journal of Machine Learning
Research, 2003.
S. R. Bowman, L. Vilnis, O. Vinyals, A. Dai, R. Jozefowicz, and S. Bengio. Generating sentences
from a continuous space. In Conference on Computational Natural Language Learning, 2016.
S. Brooks, A. Gelman, G. Jones, and M. Xiao-Li. Handbook of Markov chain monte carlo. CRC
press, 2011.
X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel. Infogan: Interpretable
representation learning by information maximizing generative adversarial nets. In Advances in
Neural Information Processing Systems, 2016.
B. Cheung, J. A. Livezey, A. K. Bansal, and B. A. Olshausen. Discovering hidden factors of variation
in deep networks. arXiv:1412.6583, 2014.
T. Cohen and M. Welling. Learning the irreducible representations of commutative lie groups. In
International Conference on Machine Learning, 2014.
S. M. A. Eslami, D. J. Rezende, F. Besse, F. Viola, A. S. Morcos, M. Garnelo, A. Ruderman, A. A.
Rusu, I. Danihelka, K. Gregor, D. P. Reichert, L. Buesing, T. Weber, O. Vinyals, D. Rosenbaum,
N. Rabinowitz, H. King, C. Hillier, M. Botvinick, D. Wierstra, K. Kavukcuoglu, and D. Hassabis.
Neural scene representation and rendering. Science, 2018.
L.	Falorsi, P. de Haan, T. R. Davidson, N. D. Cao, M. Weiler, P. Forre, and T. S. Cohen. Explorations
in homeomorphic variational auto-encoding. arXiv preprint 1807.04689, 2018.
M.	Garnelo, D. Rosenbaum, C. Maddison, T. Ramalho, D. Saxton, M. Shanahan, Y. W. Teh,
D. Rezende, and S. M. A. Eslami. Conditional neural processes. In International Conference
on Machine Learning, 2018a.
M.	Garnelo, J. Schwarz, D. Rosenbaum, F. Viola, D. J. Rezende, S. M. A. Eslami, and Y. W. Teh.
Neural processes. arXiv:1807.01622, 2018b.
I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Ler-
chner. beta-vae: Learning basic visual concepts with a constrained variational framework. In
International Conference on Learning Representations, 2017.
E. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax. In International
Conference on Learning Representations, 2017.
D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference
on Learning Representations, 2015.
D. P. Kingma and M. Welling. Auto-encoding variational bayes. In International Conference on
Learning Representations, 2014.
D. P. Kingma, S. Mohamed, D. J. Rezende, and M. Welling. Semi-supervised learning with deep
generative models. In Advances in Neural Information Processing Systems. 2014.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. In Proceedings of the IEEE, 1998.
C. J. Maddison, A. Mnih, and Y. W. Teh. The concrete distribution: a continuous relaxation of
discrete random variables. In International Conference on Learning Representations, 2017.
A. Mahendran and A. Vedaldi. Understanding deep image representations by inverting them. In The
IEEE Conference on Computer Vision and Pattern Recognition, 2015.
9
Under review as a conference paper at ICLR 2019
A. Makhzani, J. Shlens, N. Jaitly, and I. Goodfellow. Adversarial autoencoders. In International
Conference on Learning Representations, 2016.
A. Mansbridge, R. Fierimonte, I. Feige, and D. Barber. Improving latent variable descriptiveness
with autogen. arXiv preprint 1806.04480, 2018.
L. McInnes and J. Healy. UMAP: uniform manifold approximation and projection for dimension
reduction. arXiv preprint 1802.03426, 2018.
T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words
and phrases and their compositionality. In Advances in Neural Information Processing Systems.
2013.
Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural im-
ages with unsupervised feature learning. In Advances in Neural Information Processing Systems
Workshop on Deep Learning and Unsupervised Feature Learning, 2011.
A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional
generative adversarial networks. In International Conference on Learning Representations, 2016.
S. Reed, K. Sohn, Y. Zhang, and H. Lee. Learning to disentangle factors of variation with manifold
interaction. In International Conference on Machine Learning, 2014.
D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate infer-
ence in deep generative models. In International Conference on Machine Learning, 2014.
S. Sabour, N. Frosst, and G. E. Hinton. Dynamic routing between capsules. In Advances in Neural
Information Processing Systems. 2017.
N. Siddharth, B. T. Paige, J.-W. Van de Meent, A. Desmaison, N. Goodman, P. Kohli, F. Wood, and
P. Torr. Learning disentangled representations with semi-supervised deep generative models. In
Advances in Neural Information Processing Systems, 2017.
J. Tenenbaum and W. T. Freeman. Separating style and content with bilinear models. Neural
Computation, 2000.
Z. Zhu, P. Luo, X. Wang, and X. Tang. Multi-view perceptron: a deep model for learning face
identity and view representations. In Advances in Neural Information Processing Systems, 2014.
A	Experimental setup
In this appendix we provide details of the experimental setup that was used to generate the results
from Section 3.
For our implementation of EquiVAE, we use relatively standard neural networks. All of our ex-
periments use implementations with well under 1 million parameters in total, converge within a few
hours (on a Tesla K80 GPU), and are exposed to minimal hyperparameter tuning.
In particular, for the deterministic class-representation vector ry given in Equation 2, we parametrise
fθinv (x) using a 5-layer, stride-2 (stride-1 first layer), with 5x5 kernal size, convolution network,
followed by a dense hidden layer. The mean of these m embeddings fθinv (xiy) is taken, followed
then by another dense hidden layer, and the final linear dense output layer. This is shown for a y = 6
MNIST digit in the top shaded box of Figure 5. Our implementation uses (8, 16, 32, 64, 64) filters
in the convolution layers, and (128, 64) hidden units in the two subsequent dense layers for a 16
dimensional latent (the number of units in the dense layers are halved when using 8 dimensional
latents, as in our semi-supervised experiments on MNIST).
We parametrise the approximate posterior distribution qφcov(v|ry, x) over the equivariant latent as
a diagonal-covariance normal distribution, N(μφcov (ry, x), σφ ° (ry, x)), following the SGVB algo-
rithm (Kingma & Welling, 2014; Rezende et al., 2014). For μφcov (ry, x) and σφcov (ry, x), We use the
identical convolution architecture as for the invariant embedding network as an initial embedding
for the data point x. This embedding is then concatenated with the output of a single dense layer
10
Under review as a conference paper at ICLR 2019
Figure 5: Example of variational encoding of a particular MNIST digit x (from the 6 class) both in
terms of its invariant representation (top) and its equivariant representation (bottom).
that transforms ry, the output of which is then passed to one more dense hidden layer for each μ and
σ2 separately. This is shown in the bottom shaded box of Figure 5.
The generative model pθ (x|ry, v) is based on the DCGAN-style transposed convolutions (Radford
et al., 2016), and is assumed to be a Bernoulli distribution for MNIST (Gaussian distribution for
SVHN) over the conditionally independent image pixels. Both the invariant representation ry and
the equivariant representation v, are separately passed through a single-layer dense network before
being concatenated and passed through another dense layer. This flat embedding that combines
both representations is then transpose convolved to get the output image in a way the mirrors the
5-layer convolution network used to embed the representations in the first place. That is, we use
(64, 128) hidden units in the first two dense layers, and then (64, 32, 16, 8, ncolours) filters in each
transpose convolution layer, all with 5x5 kernals and stride 2, except the last layer, which is a stride-
1 convolution layer (with padding to accommodate different image sizes).
In our semi-supervised experiments, we implement qφy-post (y|x) using the same (5-CNN, 1-
dense) encoding block to provide an initial embedding for x. This is then concatenated with
stop-grad(fθinv(x)) and passed to a 2-layer dense dropout network with (128, 64) units. The use
of stop_grad(fθinv (x)) is simply that fθinv (x) is learning a highly relevant, invariant representation of
x that qφy-post (y|x) might as well get access to. However, we do not allow gradients to pass through
this operation since fθinv (x) is meant to learn from the complementary data of known same-class
members only.
As discussed in Section 2, the number of complementary samples m used to reconstruct ry (see
Equation 2) is chosen randomly at each training step in order to ensure that ry is insensitive to m.
For our supervised experiments where labelled data are plentiful, m is randomly select between 1
and mmax with mmax = 7 for MNIST (mmax = 10 for SVHN), whereas in the semi-supervised case
mmax = 4 for MNIST (mmax = 10 for SVHN).
We perform standard, mild preprocessing on our data sets. MNIST is normalised so that each pixel
value lies between 0 and 1. SVHN is normalised so that each pixel has zero mean and unit standard
deviation over the entire dataset.
Finally, all activation functions that are not fixed by model outputs are taken to be rectified linear
units. We use Adam (Kingma & Ba, 2015) for training with default settings, and choose a batch size
of 32 at the beginning of training, which we double successively throughout training.
11
Under review as a conference paper at ICLR 2019
B Derivation of likelihood lower bounds
In this appendix we detail the derivations of the log-likelihood lower bounds that were provided in
Section 2.
EquiVAE is relevant when a non-empty set of labelled data is available. We write the data set as
D=Dlab∪Dunlab= {xn,yn}nN=lab1∪{xn}nN=un1lab	(8)
We also decompose Dlab = ∪yDlyab, where Dlyab is the set of labelled instantiations x with label y.
In particular, in what follows we think of Dlyab as containing only the images x, not the labels, since
they are specified by the index on the set. We require at least two labelled data points from each
class, so that |Dlyab | ≥ 2 ∀y.
We would like to maximise the log likelihood that our model generates both the labelled and the
unlabelled data, which we write as:
log p(D) = log p(Dlab) + log p(Dunlab |Dlab)	(9)
where we make explicit here the usage of labelled data in the unlabelled generative model.
For convenience, we begin by repeating the generative model for the labelled data in Equation 1,
except with the deterministic integral over rn completed:
Nlab
P({xn,yn}N=bl) = Y /
n=1
dvn pθ xn|r(Dlyanb \ {xn}),vn p(vn) p(yn)
(10)
We will simplify the notation by writing xcn = Dlyanb \ {xn} and ryn,xcn = r(Dlyanb \ {xn}), but keep
all other details explicit.
We seek to construct a lower bound on logp({xn, yn}nN=lab1), namely the log likelihood of the labelled
data, using the following variational distribution over vn (Equation 3):
qφcov (VnIryn,c ,Xn) = N (μφcov (ryn,c ,xn),σk (ryn,c ,Xn)I)	(II)
Indeed,
Nlab
log P{{xn,yn}N=1) = X log Eqφcοv(vn∣ryn,d ,xn)
n=1
Pθ (xn lryn,c , Vn) P(Vn) P(Jn)
qφcοv (VnIryn,c , Xn)
Jensen’s
≥
Nlab
qφcov (vn |ryn ,xdn ,xn ) og
n=1
Pθ(xnKyn,C ,vn) P(Vn) P(jn)
qφcοv (VnIryn,c , Xn)
(12)
Nlab
Eqφ Eqφcοv (vn∣ryn,d ,≈n) log Pθ (Xn|ryn,C ,Vn) - DKL [qφcοv (VnIryn ,c ,xn)" P(Vn)] + log P(Jn)
n=1
Which coincides with the notationally simplified lower bound objective function given in Equation 4.
We now turn to the lower bound on the unlabelled data. To start, we marginalise over the labels on
the unlabelled dataset:
Nunlab
P({xn}n=nib∣Dlab) = YX	dVn Pθ(xn]r(DIyn ),Vn) P(Vn) P(Jn)
n=1 yn
(13)
where We no longer need to remove Xn from Dyb in r(∙) since for the unlabelled data, Xn ∈ Dyn.
As was done for the labelled data, we construct a lower bound using variational inference. However,
in this case, we require a variational distribution over both Jn and Vn . We take:
=qφcov (VnIr(Dyn ), Xn ) qφy-post (yn|Xn)
(14)
q(Vn , Jn IXn , Dlab )
12
Under review as a conference paper at ICLR 2019
which gives
Nunlab
log p({Xn}N=ιb Dlab =log Y
Eqφy-postSnl Xn)EqΦcov (VnIr(Dyn ),xn)
n=1
Pθ(χnK(Dyb ),vn) P(Vn) PQn)	^
qφcov (vn 卜(DIyn ) ,xn ) qφy-post 3n |Xn) 一
Jensen’s
≥
Nunlab
∑S Eqφy-pos"ynlXn)EqΦcov(VnIr(Dyn ),xn)log
n=1
Pθ(xnK(Dyb ),vn) P(Vn) PQn)
qφcov (vn 卜(DIyn ),xn) qφy-post (yn|xn)
Nunlab
X Eqφy-post(yn∣χn) EqΦcov(vn∣r(Dyn),χn)logPθ(xMr(DIyn),vn
n=1
(15)
-DKL [q°cov(vn|r (DIyn ),χn)∣∣p(vn)]	- DKL [qφy-p°st Qn |xn) ∣∣pQn)]
Thus, we have Equation 6 augmented with the notational decorations that were omitted in Section 2.
Therefore, the objective
Nunlab	Nlab
L = X L(unnl)ab + X Ll(anb)	(16)
n=1	n=1
given in Equation 7, with Ll(anb) given in Equation 4 and L(unnl)ab given in Equation 6, is a lower bound
on the data log likelihood.
13