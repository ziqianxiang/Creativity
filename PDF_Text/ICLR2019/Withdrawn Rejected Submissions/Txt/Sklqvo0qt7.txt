Under review as a conference paper at ICLR 2019
A Priori Estimates of the Generalization Er-
ror for Two-layer Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
New estimates for the generalization error are established for a nonlinear regres-
sion problem using a two-layer neural network model. These new estimates are
a priori in nature in the sense that the bounds depend only on some norms of the
underlying functions to be fitted, not the parameters in the model. In contrast,
most existing results for neural networks are a posteriori in nature in the sense
that the bounds depend on some norms of the model parameters. The error rates
are comparable to that of the Monte Carlo method in terms of the size of the
dataset. Moreover, these bounds are equally effective in the over-parametrized
regime when the network size is much larger than the size of the dataset.
1	Introduction
One of the most important theoretical challenges in machine learning comes from the fact that clas-
sical learning theory cannot explain the effectiveness of over-parametrized models in which the
number of parameters is much larger than the size of the training set. This is especially the case for
neural network models, which have achieved remarkable performance for a wide variety of prob-
lems (Bahdanau et al., 2015; Krizhevsky et al., 2012; Silver et al., 2016). Therefore, understanding
the mechanism behind these successes is critical, which requires developing new analytical tools
that can work effectively in the over-parametrized regime (Zhang et al., 2017).
Our work is partly motivated by the situation in classical approximation theory and finite element
analysis. There are two kinds of error bounds in finite element analysis depending on whether the
target solution (the ground truth) or the numerical solution enters into the bounds. Let f * and f be
the true solution and the “numerical solution”, respectively. In “a priori” error estimates, only norms
of the true solution enter into the bounds, namely
kfn — f*kl ≤ Ckf*k2.
In “a posteriori” error estimates, the norms of the numerical solution enter into the bounds:
..^ . .. .. ^ ..
kfn — f*kl ≤ Ckfnk3.
Here ∣∣ ∙ ∣∣ι, k ∙ ∣∣2, k ∙ ∣∣3 denote various norms.
In this language, most recent theoretical efforts (Neyshabur et al., 2015; Bartlett et al., 2017;
Golowich et al., 2018; Neyshabur et al., 2017; 2018a;b) on estimating the generalization error of
neural networks should be viewed as “a posteriori” analysis, since all the bounds depend on some
norms of the solutions. Unfortunately, as observed in Arora et al. (2018) and Neyshabur et al.
(2018b), the numerical values of these norms are always huge, yielding vacuous estimates.
In this paper we pursue a different line of attack by providing “a priori” analysis. For this purpose,
a suitably regularized two-layer network is considered. It is proved that the generalization error of
regularized solutions is asymptotically sharp with constants depending only on the properties of the
target function. Numerical experiments show that these a priori bounds are non-vacuous (Dziugaite
& Roy, 2017) for datasets of practical interests, such as MNIST and CIFAR-10. In addition, our
experimental results also suggest that such regularization terms are necessary in order for the model
to be “well-posed” (see Section 6 for the precise meaning).
1
Under review as a conference paper at ICLR 2019
1.1	Setup
We will focus on the regression problem. Let f * : Ω → R be the target function, with Ω = [-1,1]d,
and S = {(xi, yi)}in=1 denotes the training set. Here {xi}in=1 are i.i.d samples drawn from an
underlying distribution π with supp(π) ⊂ Ω, and y% = f * (xi)+ εi, with ε being the noise. Our aim
is to recover f* by fitting S using a two-layer fully connected neural network with ReLU (rectified
linear units) activation:
m
f(x; θ) = E akσ(bk ∙ X + Ck),	(1)
k=1
where σ(t) = max(0, t) is the ReLU function, bk ∈ Rd, and θ = {(ak, bk, ck)}km=1 represents all
the parameters to be learned from the training data. m denotes the network width. To control the
complexity of networks, we use the following scale-invariant norm.
Definition 1 (Path norm). For a two-layer ReLU network (1), the path norm is defined as
m
kθkP = X |ak|(kbkk1 + |ck|).
k=1
Definition 2 (Spectral norm). Given f ∈ L2(Ω), denote F ∈ L2(Rd) as an extension of f to Rd.
Let F denote the Fourier transform of F, then f (x) = JRd eihx,ωiF(ω)dω ∀X ∈ Ω. We define the
spectral norm of f as follows
Yf) =	2, dnf	, k kωk2lF3ldω∙	⑵
F∈L2(Rd),F∣Ω = f∣Ω √Rd
Assumption 1 (Target function). Following Breiman (1993) and Klusowski & Barron (2016), we
consider target functions that have finite spectral norm. By defining
Fs= L2(Ω) ∩ { f (x) : Ω → Rl γ(f) < ∞, kf ∣∣∞ ≤ 1},	⑶
We assume that f* ∈ Fs.
Assumption 2 (Noise). We assume the noise has zero mean, and its probability distribution has an
exponentially decaying tail, i.e.,
t2
E[ε] = 0,	P[∣ε∣ > t] ≤ c0e σ ∀t ≥ τ0.	(4)
Here c0 , τ0 and σ are constants.
The ultimate aim is to minimize the generalization error (expected risk) L(θ) = Ex,y [(f (x; θ)-y)2].
In practice, We only have at our disposal the empirical risk L(θ) = n Pn=1(f (xi； θ) - y芥.The
generalization gap is defined as the difference between expected and empirical risk. We also define
the truncated risks by LB (θ) = Eχ,y [(f(x; θ) - y)2 ∧ B2], L B (θ) = 1 Pn=1(f (xi； θ) - yi)2 ∧ B2.
2	Preliminary
In this section, we summarize some results on the approximation error and generalization bound
for two-layer ReLU networks, whose proofs are deferred to Appendix A and B. These results are
required by our subsequent a priori analysis.
2.1	Approximation Properties
Most of the content is adapted from Barron (1993); Breiman (1993) and Klusowski & Barron (2016).
Proposition 1. For any f ∈ Fs, it has an integral representation as follows
f (x) - f (0) - X ∙Vf(0)
v
{-1,1}×[0,1]×Rd
h(x； z, t, ω)dp(z, t, ω),
where v < 2γ(f) and
s(z, t, ω) = - sign cos(kωk1t - zb(ω))
h(x; z,t, ω) = s(z,t, ω) (zx ∙ ω∕∣∣ω∣∣ι —力)十.
2
Under review as a conference paper at ICLR 2019
For simplicity, in the rest of this paper, We assume Vf (0) = 0, f (0) = 0. We take m samples Tm =
{(z1, t1, ω1), . . . , (zm, tm, ωm)} with (zi, ti, ωi) randomly drawn from p(z, t, ω), and consider the
empirical average fm,(x) = m Pm=I h(x; zi,ti, ω., which is exactly a two-layer ReLU network
of Width m. The central limitmtheorem (CLT) tells us that the approximation error is roughly
1m
E(z,t,ω) [h(x; z,t, ω)]——Eh(x; zfe,tfe, ωk)
mk=1
Var(z,t,ω)[h(x; z, t, ω)]
≈
m
So as long as we can bound the variance at the right-hand side, we will have an estimate of the
approximation error. The following result formalizes this intuition.
Theorem 2.	For any distribution π with SuPP(π) ⊂ Ω and any f ∈ Fs, there exists a two-layer
network f(x; θ) of width m such that
Ex〜∏lf(x)- f(x;在)|2 ≤ 16γf.
m
Furthermore kθkP ≤ 4γ(f), which means that the Path norm of θ can be bounded by the sPectral
norm of the target function.
XX h(zi) — Ez[h(z)]∣ ≤ 2Es[R(H)] + C『卜丫他)
i=1
2.2 Estimating the Generalization Gap
Definition 3 (Rademacher complexity). Let H be a hypothesis space, i.e. a set of functions.
The Rademacher complexity of H with respect to samples S = (z1, z2, . . . , zn) is defined as
R(H) = nEξ[suph∈H pn=ι h(zi)ξi], where {ξi}n=ι are independently random variables with
P(ξi = +1)= P(ξi = -1) = 1.
The generalization gap can be estimated via Rademacher complexity by the following theorem (see
Bartlett & Mendelson (2002) and Shalev-Shwartz & Ben-David (2014) ).
Theorem 3.	Fix a hyPothesis sPace H, and suPPose that for any h ∈ H and z, |h(z)| ≤ c. Then for
any δ > 0, with Probability at least 1 - δ over the choice of S = (z1, z2, . . . , zn ), we have
sup I1
h∈H n
About the Rademacher complexity of two-layer networks (1), we have the following result.
Lemma 1. Let z = (x, y) and h(z; θ) = `(f (x; θ), y). Consider all the two-layer networks with
path norm bounded by Q, i.e. HQ := {h(z; θ) ∣∣∣θ∣∣p ≤ Q}. If loss function '(y,仍 is L-Lipschitz
continuous with resPect to y, then we have
R(HQ) ≤ QLr 2ion(2d)
Applying Theorem 3 and Lemma 1 gives us the following generalization bound.
Theorem 4 (A posterior generalization bound). For any δ > 0, with probability at least 1 - δ over
the choice of the training set S, we have for any two-layer network f(x; θ), the following result
holds:
|Lb(θ) - LB(θ)I ≤ 4B(kθkp + 1)r2∙M2+ B2r2log(2c(1 + kθ⅛∕δ),	(5)
nn
where c = Pk∞=1 1∕k2.
3 Main Results
We see that the path norm of the special solution θ which achieves the optimal approximation error
is independent of the network size, and this norm can also be used to bound the generalization
gap (Theorem 4). Therefore, if the path norm is suitably penalized during training, we should be
able to control the generalization gap without harming the approximation accuracy. One possible
implementation of this idea is through the structural empirical risk minimization (Vapnik, 1998) as
follows.
3
Under review as a conference paper at ICLR 2019
Definition 4 (Path-norm regularized estimator). Let the path-norm regularized risk defined as
Jλ(θ) := LBn (θ)+ λBn ∕2lθg叫1+ kθkp),	(6)
where Bn = 2 + max{τ0, σ log n} and λ is a positive constant. The condition on λ will be given
below. The path-norm regularized estimator is defined as
θn = argmin Jλ(θ).	(7)
It is worth noting that the minimizer is not necessarily unique, and θn should be understood as any
of the minimizers. About this estimator, we have the following result.
Theorem 5 (Main Result). Under Assumption 1 and 2, there exists a constant C depending only
on σ, c0 such that for any δ > 0 and λ ≥ 4, with probability at least 1 - δ over the choice of the
training set S, the generalization error of estimator (7) satisfies
E|f (x; θn) - f *(x)∣2 ≤ Cγ2f + CBn= (λγ(f *)Pl0g(2d) + Plog(nc∕δ)) .	(8)
mn
Here ^(f *) = max{γ(f *),1}∙
Since Bn depends on the magnitude of noise, we actually prove
E∣f(x; θn) - f *(x)∣2
O( ml) + O(尸存)
O(ml) + O jog2 (n)qlogd+ogn
if σ ≤ ɪ
log n
if σ > IoCn,
where C is a constant. This means that the noise introduces at most an extra logarithmic term.
Moreover, if the probability distribution function of the noise decays sufficiently fast (for example,
bounded noise σ = 0), the logarithmic term can even be eliminated.
Remark 1. It should be noted that both terms at the right hand side of the above result has a Monte
Carlo nature, as can be seen later in the proof∙ From this viewpoint, the result is quite sharp∙ The
dimensional dependence is mainly reflected in the norm γ(f *) (see Barron (1993))∙
Comparison with existing results Klusowski & Barron (2016) analyzed a similar problem. How-
ever they require the network width m to be the orders of poly(n). In contrast, our results allow the
network width to be arbitrarily large. See Table 1 for the detailed comparison between our results
and theirs.
noise	zero	sub-Gaussian
Our results	1 I ( log d+log n ʌ 1/2 m +(	n	)	m+log2(n) (log^)1/2
Results of Klusowski & Barron (2016)	(T/	博)1/4	一
Table 1: Comparison between our work and Klusowski & Barron (2016).
4 Proof of Main Results
4.1	Noiseless Case
For this case, we provide a rather short and intuitive sketch of proof, which helps to clarify the main
idea of the complete proof in the next section. In the noiseless case, σ = 0, τ0 = 0, thus Bn = 2.
The solution θ constructed in Theorem 2 satisfies L(θ) = O(m-1) and kθkP = O(1). According
to Theorem 4, we have L(θ) = O(m-1) + O(n-1/2) 1. Hence the corresponding regularized risk
satisfies
Jλ(θ) = O(m-1) + O(n-1/2). 1
1Asymptotic notation O(∙) is similar to O(∙) but with logarithmic terms ignored.
4
Under review as a conference paper at ICLR 2019
ɪʌ	∙	,1	∙	∕λ ∙ ,1 X 1
By comparing the minimizer θn with θ, we have
Jλ(θn) ≤ Jλ(θ) = O(m-1) + O(n-1/2).
Furthermore ∣∣θn∣∣p ≤ O(n1/2m-1). By Theorem 4, we have
L(θn) ≤ L(θn) + 4《2lT22(kθnkp + 1)+ O (n-" ∕bg( k θn ∣∣P )) .	(9)
As long as λ ≥ 4, we have
L(θn) ≤ Jλ(θn) + O (n-1∕2 Jlog(∣∣θn∣∣p)) = O(m-1) + O(n-1/2).
We thus complete the proof.
This analysis highly relies on the fact that the approximation error and generalization gap can be
controlled by the path norm simultaneously.
4.2	Noisy Case
In the presence of noise, the expected risk can be decomposed into three terms
Eχ,ylf(x; θ) - y|2 = Eχ,ε∣f(x; θ) - f*(x) -ε∣2
=Eχ,ε∣f(x; θ) - f*(x)∣2 +2Eχ,ε[(f(x; θ) - f*(x))ε]+ E[ε2].
Since ε is independent of x and E[ε] = 0, we have
L(θ)= Exlf (x； θ) - f*(x)∣2 + E[ε2].
This suggests that, in spite of noise, we still have
argmin© L(θ) = argmin© Eχ∣f(x; θ) - f *(x)∣2,	(10)
and the latter is what we really want to minimize.
We first need to address the issue that L(θ) - L(θ) can be arbitrarily large, due to the presence of the
noise. Let us consider the truncated risk LB (θ) = E[(f (x； θ) - y)2 ∧ B2], which has the following
property, whose proof is deferred to Appendix C.
Lemma 2. Under Assumption 2, we have
SuP IL(O) - LBn (O)I ≤ √n ,
By triangle inequality, we have ∣L(Θ)∣ ≤ ∣L(Θ)-Lb. (θ)∣ + ∣LBn (θ)∣ = 2c√σ2 + |L Bn (θ)∣. Therefore
this lemma tell us that as long as we can control the truncated risk, then the original risk will be
controlled accordingly.
Proposition 6. Let O be the solution constructed in Theorem 2. Then with probability at least 1 - δ,
we have
Jλ(θ ≤L(O)+2√σ	+	√n	(Y(f *)	(3+5(λ+4) Plog⑶苗+Plog(2c/o),
where Y(f) = max{γ(f ),1}
Proof. According to Definition 4 and the property that ∣∣θkp ≤ 4γ(f *), the regularized cost of θ
must satisfies
Jλ(θ) = LBn (θ) + λBnj W(也(∣θ∣p + 1)
(≤) LBn(O) + (4 + λ)BnJ^(kθkp + 1) + Bn S 2log(2c(1 + kθkP PS
n	n	nn
(≤) L(θ) + 2√σ2 + (λ + 4)Bn ”迈(4γ(f *) + 1) + Bn 严囚五f≡,
n	n	nn
(11)
5
Under review as a conference paper at ICLR 2019
where ⑴，(2) follow the Theorem 4 and Lemma 2, respectively. The last term can be simplified by
using Ja + b ≤ √α + Gb and log(1 + a) ≤ a for a ≥ 0,b ≥ 0. So We have
,2log(2c(1+4γ(f*))2∕δ) ≤ √2log(2c∕δ) + √4log(1 + 4γ (f *))
≤ 2,log(2c∕δ) + 4Pf
≤ 2Plog(2c∕δ) + 4γ(f *),
where ^(f *) = max(γ(f *), 1). By plugging it into Equation (11), and using ^(f *) ≥ 1, Bn ≥ 1,
we have
Jλ(石)≤ L(θ)+—√n—+ √== (γ(f *) (3+5(λ+4)Pιog(2d)) + plogi2。6)).
□
Proposition 7 (Properties of the regularized estimator). The path-norm regularized estimator θn
satisfies:
.ʌ . . ~.
Jλ(θn) ≤ Jλ(θ)
kθnkp ≤ LB—y导 Ja
Proof. The first claim follows from the definition of θn For the second claim, we have
λq⅛p kθnkp ≤ Jλ(θn) ≤ Jλ(θ), so kθnkp ≤ λ-1B-1q 温旃 Jλ(θ).	□
Remark 2. The above proposition establishes the connection between the regularized solution and
the special solution θ constructed in Theorem 2. According to Proposition 6, we can conclude
that the upper bound of the generalization gap satisfies k√P = O(L(京))+ O(γ(f *)n-1/2) →
O(L(θ)), as n → ∞. It suggests that our regularization term is added appropriately, which forces
the generalization gap to be roughly in the same order of approximation error.
Proof of Theorem 5. We are now ready to prove our main theorem. Let C1 = 2c0σ2 . Lemma 2
implies that L(θn ≤ LBn (θn + √n. Then we have
L(θn) ≤) LBn (θn) +4Bn(kθn kp + D ^^ + B^ SiIilZkln+ ^
i
log(2c(1 + ∣∣θn∣p )2∕δ) + C
n
≤ Jλ(θn) + Bn
Where (1) follows from the a posteriori generalization bound in Theorem 4, and (3) is due to λ ≥ 4.
Furthermore,
,log(2c(1 + kθnkp)2∕δ) ≤ plog(2nc∕δ) + ,2log(1 + n-V2∣∣θnkp)
≤ plog(2nc∕δ) + J2n-"kθnkp.
By plugging it back and simplifying the right hand side according to Proposition 6 and Proposition 7,
we conclude that there exists a constant C2 such that
.ʌ . . ~.
L(θn) ≤ L(θ) + C
Bn
√n
(λγ(f *)Plog(2d) + Plog(nc∕δ)).
By applying the decomposition that L(θ) = E|f (x; θ)-f * (x) ∣2+E[ε2], and the result ofTheorem 2,
we obtain
E∣f(x; θm) - f *(x)∣2 ≤ CYf + C √n (λ^(f *)Plog(2d) + Plog(nc∕δ)).
Here the constant C depends only on σ, c0.
Remark 3. From the proof, we can see that the requirement of λ ≥ 4 is due to constant 4 appears
in the upper bound of the generalization gap. If we have a sharper generalization bound, then λ
could be set smaller.
6
Under review as a conference paper at ICLR 2019
5	Numerical Experiments
We evaluate the properties of the regularized estimator on both MNIST2 (LeCun et al., 1998) and
CIFAR-103 (Krizhevsky & Hinton, 2009) datasets. Each example in MNIST is a 28 × 28 grayscale
image, while each example in CIFAR-10 is a 32 × 32 × 3 color image. To be consistent with our
setup in theoretical analysis, we restrict ourselves to a binary classification problem. For MNIST, we
map numbers {0, 1, 2, 3, 4} to label 0 and {5, 6, 7, 8, 9} to 1. For CIFAR-10, we select the examples
with labels 0 and 1 to construct our new training and validation sets. Thus, our new MNIST has
60, 000 training examples, and CIFAR-10 has 10, 000 training examples. The mean squared error
rather than cross entropy is used as our loss function.
Following the standard strategy (He et al., 2015), the two-layer ReLU network is initialized using
ai 〜N(0, 2κ), bi,j〜N(0, 2κ∕d), Ci = 0. We Use K =1 and train the models using the Adam
optimizer (Kingma & Ba, 2015) for T = 10, 000 steps, unless it is specified otherwise. The initial
learning rate is set to be 0.001, and it is then multiplied by a decay factor of 0.1 at 0.7T and again
at 0.9T . We set the trade-off parameter λ = 0.1 for regularized models. Although the theoretical
results suggest λ ≥ 4, we find in practice usually a smaller λ can achieve better test performance.
5.1	The Non-vacuous Upper Bound of the Generalization Gap
Theorem 4 shows that the generalization gap can be bounded by kθ√nP- UP to some constants. To see
how this works in practice, we trained both regularized models with λ = 0.1 and un-regularized
models (λ = 0) for fixed network width m =10,000. To cover the over-parameterization regime,
we also consider n = 100 where m∕n = 100	1. The results are summarized in Table 2.
Table 2: Comparison of regularized (λ = 0.1) and un-regularized (λ = 0) models. The experiments are
repeated for 5 times, and the mean values are reported.
dataset	λ	n	training accuracy	test accuracy	kθkp √n
CIFAR-10	0	-104- 100	100% 100%	845% 70.5%	58 507
	0.1	-104- 100	874% 91.0%	80% 72.0%	0.14 0.43
MNIST	0	6 X 104 100	100% 100%	988% 78.7%	58 162
	0.1	6 × 104 100	98T% 100%	978% 74.9%	0.27 0.41
Figure 1: Comparison of path norms between regularized and un-regularized solutions for varying widths.
As we can see, the test accuracies of regularized and un-regularized solutions are generally compa-
rable, but the upper bounds of generalization gap k√p are dramatically different. Specifically, for
un-regularized models, the bounds are always vacuous, since they are several orders of magnitude
larger than the naively upper bound 1. This is consistent with the observations in Arora et al. (2018)
2http://yann.lecun.com/exdb/mnist/
3https://www.cs.toronto.edu/~kriz/cifar.html
7
Under review as a conference paper at ICLR 2019
and Neyshabur et al. (2018b). However, for regularized models, the bounds are non-vacuous, al-
though they are still far from the true values. These numerical observations are consistent with our
theoretical prediction in Proposition 7.
To further explore the impact of over-parameterization, we trained various models with different
widths. For both datasets, all the training examples are used. In Figure 1, we display how the upper
bound k√nP of the learned solution varies with the network width. We find that this quantity for
the regularized model is almost constant, whereas for the original model it increases with network
width. This provides numerical evidence that our theoretical results hold for the network with an
arbitrary width.
5.2	Dependence on the Initialization
Since the neural network model is non-convex, it is interesting to see how the initialization affects
the performance of the solutions, especially in the over-parametrized regime. To this end, we fix
m = 10000, n = 100 and vary the variance of random initialization κ. The results are reported in
Figure 2. In general, we find that regularized models are much more stable than the un-regularized
models. For large initialization, the regularized model always performs significantly better.
Figure 2: Test accuracies of solutions obtained from different initializations. Each experiment is repeated for
5 times, and we report the mean and standard deviation.
6	Concluding Remarks
The most unsatisfactory aspect of our result is that it is proved for the regularized model. In practice
it is uncommon to add explicit regularizations. Instead, practitioners rely on the so-called “implicit
regularization” (Zhang et al., 2017; Neyshabur, 2017). At the moment it is unclear where the “im-
plicit regularization” comes from and how it actually works. But there are overwhelming evidence
that by tuning extensively the details of the optimization procedure, including the algorithm, the
initialization, the hyper-parameters, etc., one can find solutions with superior performance on the
test data. The disadvantage is that excessive tuning and serious experience is required to find good
solutions. Until we have a good understanding about the mysteries surrounding implicit regular-
ization, the business of parameter tuning will remain an art. In contrast, the regularized model is
rather robust and much more fool-proof. Borrowing the terminology from mathematical physics,
we consider the regularized model to be “well-posed” and the original model to be “ill-posed” .
There are two clear paths moving forward. One is to study other regularized models. In fact to avoid
the slight loss of test accuracy shown for the MNIST dataset in Figure 1, one can consider regular-
izations that vanish for small values of the path norm. Our main results should hold for this kind
of regularizations. The other is to study the so-called “implicit regularization”. Recently, assuming
that the data is well-separated, Brutzkus et al. (2018); Li & Liang (2018) proved that for two-layer
networks, the number of iterations required for SGD to achieve certain accuracy for the classifica-
tion problem is independent of the network size. Implicit regularization has also been studied in
other problems, such as logistic regression (Soudry et al., 2018) and matrix factorization (Li et al.,
2018; Gunasekar et al., 2017).
8
Under review as a conference paper at ICLR 2019
References
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. In Proceedings of the 35th International Conference on
Machine Learning, volume 80,pp. 254-263. PMLR, 10-15 JUl 2018.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In International Conference on Learning Representations, 2015.
Andrew R. Barron. Universal approximation bounds for superpositions of a sigmoidal function.
IEEE Transactions on Information theory, 39(3):930-945, 1993.
Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems 30, pp. 6240-6249.
2017.
Leo Breiman. Hinging hyperplanes for regression, classification, and function approximation. IEEE
Transactions on Information Theory, 39(3):999-1013, 1993.
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. Sgd learns over-
parameterized networks that provably generalize on linearly separable data. In International
Conference on Learning Representations, 2018.
Gintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. In Proceedings
of the Thirty-Third Conference on Uncertainty in Artificial Intelligence, UAI, 2017.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neural networks. In Proceedings of the 31st Conference On Learning Theory, volume 75, pp.
297-299. PMLR, 2018.
Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro.
Implicit regularization in matrix factorization. In Advances in Neural Information Processing
Systems, pp. 6151-6159, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE International
Conference on Computer Vision, pp. 1026-1034, 2015.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations, 2015.
Jason M Klusowski and Andrew R Barron. Risk bounds for high-dimensional ridge function com-
binations including neural networks. arXiv preprint arXiv:1607.01434, 2016.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-
nical report, Citeseer, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In Advances in Neural Information Processing Systems, 2018.
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized
matrix sensing and neural networks with quadratic activations. In Conference On Learning The-
ory, pp. 2-47, 2018.
9
Under review as a conference paper at ICLR 2019
Behnam Neyshabur. Implicit regularization in deep learning. arXiv preprint arXiv:1709.01953,
2017.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Conference on Learning Theory, pp. 1376-1401, 2015.
Behnam Neyshabur, Srinadh Bhojanapalli, David Mcallester, and Nati Srebro. Exploring generaliza-
tion in deep learning. In Advances in Neural Information Processing Systems 30, pp. 5949-5958.
2017.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-bayesian approach to
spectrally-normalized margin bounds for neural networks. In International Conference on Learn-
ing Representations, 2018a.
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. To-
wards understanding the role of over-parametrization in generalization of neural networks. arXiv
preprint arXiv:1805.12076, 2018b.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-
rithms. Cambridge university press, 2014.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman,
Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine
Leach, Koray Kavukcuoglu, and Demis Hassabis. Mastering the game of go with deep neural
networks and tree search. Nature, 529(7587):484-489, 2016.
Daniel Soudry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent on separable
data. In International Conference on Learning Representations, 2018.
Vladimir N. Vapnik. Statistical learning theory, volume 1. Wiley New York, 1998.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In International Conference on Learning Rep-
resentations, 2017.
10
Under review as a conference paper at ICLR 2019
A Proofs for Approximation Properties
Proof of Proposition 1 By an abuse of notation, let f be its own L2 extension in Rd. Since
f ∈ L2(Rd), f (x) — X ∙ Vf (0) — f (0) can be written as
•x — iω ∙ x — 1)f(ω)dω.
(12)
Note that the following identity
—	(z — s)+eis + (—z — s)+e-is ds = eiz — iz — 1
0
holds when |z | ≤ c. Choosing C = ∣∣ω∣k, Z = ω ∙ x, we have
|z| ≤ kωk1kxk∞ ≤ c.
Let s = kωk1t, 0 ≤ t ≤ 1, and ω = ω∕kω∣∣ ι, We have
- kωk
• x — t)+eikωklt + (—ω ∙ x — t)+e-ikωk1t] dt = eiω∙x — iω ∙ X — 1.	(13)
Let f(ω) = eib(ω) ∣jT(ω)∣, inserting (13) into (12) yields
f(x)- X ∙ Vf (0) - f(0)
g(t, ω)dtdω,
where
g(t, ω) = -kωkl∣|f(ω)| [(ω ∙ X — t)+cos(kω∣∣ιt + b(ω)) + (—ω ∙ X — t)+cos(kω∣∣ιt — b(ω))].
Consider a density on {0, 1} × [0, 1] × Rd defined by
p(z,t, ω) = ∣f(ω)∣kωk2∣ cos(∣∣ω∣∣ι t — zb(ω)) |/v
(14)
where the normalized constant v is given by
v
If(ω)∣kωk2 (| cos(kω∣∣ιt + b(ω)) | + | cos (|心||/ — b(ω)) |) dωdt
(15)
Since f belongs to Fs, so we have
v ≤ 2γ(f) < +∞,
therefore the density p(z, t, ω) is well-defined. To simplify the notations, we let
(16)
Then we have
s(z, t, ω) = —sign cos(kωk1t — zb(ω))
h(x; z, t, ω) = s(z,t, ω) (zω^ ∙ X 一t)十.
(17)
(18)
f (x) — X ∙ Vf (0) — f (0) = Vl } [	]	4 h(x; z,t, ω)dp(z,t, ω).
(19)
Since X = (X)+ — (—X)+, we obtain
f (x) = f (0) + (x ∙Vf(0))+ —
X ∙ Vf (0))+ + V	h(X; z, t, ω)dp(z, t, ω).
Therefore f ∈ H σ.
11
Under review as a conference paper at ICLR 2019
Proof of Theorem 2 Let fm(x) = m Pm=I h(x; zi, ti, ωi) be the Monte-Carlo estimator, We
have
ETmExIf (x) - fm(x)∣2 = ExETmIf (x) - fm(x)∣2
v2
=mEx (E(z,t,ω)[h2(x; z,t, ω)] - f2(x))
v2
≤ mExE(z,t,ω)[h2 (x； z,t, ω)]
Furthermore, for any fixed x, the variance can be upper bounded since
E(z,t,ω)[h2(x; z,t, ω)] ≤ E(z,t,ω) [(zθ ∙ X - t)+i
≤ E(z,t,ω) [(∣ω ∙ x∣ + t)2i
≤ 4.
Hence We have
ETmExIf(x) - fm(x)I2 ≤ 4v2 ≤ 16γf
m	mm
Therefore there must exist a set of Tm, such that the corresponding empirical average satisfies
ExIf - fm∣2 ≤ 16γf .
m
Due to the special structure of the Monte-Carlo estimator, We have IakI = v/m, kbkk1 = 1, IckI ≤
1. It follows Equation (16) that kθ∣∣p ≤ 2v ≤ 4γ(f).
B Proofs for Generalization Bounds
Before to provide the upper bound for the Rademacher complexity of two-layer networks, we first
need the following two lemmas.
Lemma 3 (Lemma 26.11 of Shalev-Shwartz & Ben-David (2014)). Let S = (x1, . . . , xn) be n
vectors in Rd. Then the RademaCher CompIexity of Hi = {x → U ∙ X IkukI ≤ 1} has thefolloWing
upper bound,
R(HI) ≤ max |乂|上严叵
in
The above lemma characterizes the Rademacher complexity of a linear predictor with `1 norm
bounded by 1. To handle the influence of nonlinear activation function, we need the following
contraction lemma.
Lemma 4 (Lemma 26.9 of Shalev-Shwartz & Ben-David (2014)). Let φi : R 7→ R be a ρ-LipsChitz
funCtion, i.e. for all α, β ∈ R we have Iφi(α) - φi(β)I ≤ ρIα - βI. For any a ∈ Rn, let φ(a) =
(φ1 (a1), . . . , φn(an)), then we have
^ . . ^ .
R(φ OH) ≤ PR(H)
We are now ready to characterize the Rademacher complexity of two-layer networks. We use the
path norm to control the complexity of the network.
Lemma 5. Let FQ = {fm(x; θ) I kθkP ≤ Q} be the set of two-layer networks with path norm
bounded by Q, then we have
R(Fq) ≤ Qr^d
12
Under review as a conference paper at ICLR 2019
Proof. To simplify the proof, we let ck = 0, otherwise we can define bk = (bkT , ck)T and x
(xT,1)T.
nm
nRR(FQ)= Eξ[ SUp Xξi Xakkbkkισ(bTXi)]
kθkP≤Q i=1	k=1
nm
≤ Eξ [ sUp	ξi	akkbkk1σ(ukTxi)]
kθkP≤Q,kukk1=1 i=1 k=1
mn
= Eξ[ sUp	akkbkk1	ξiσ(ukTxi)]
kθkP≤Q,kukk1=1 k=1	i=1
mn
≤ Eξ[ sUp	|akkbkk1| sUp |	ξiσ(uT xi)|]
kθkP ≤Q k=1	kuk1=1 i=1
nn
≤ QEξ [ sUp | X ξiσ(UTXi) |] ≤ QEξ [ sUp | X ξiσ(UT Xi) |]
kuk1=1 i=1	kuk1≤1 i=1
n
= QEξ [ sUp X ξiσ(UTXi)]
kuk1≤1 i=1
Since σ is a 1-Lipschitz continuous, by applying Lemma 4 and Lemma 3, we obtain
R(FQ) ≤ Q^).
□
Proof of Lemma 1 Since for any yi, '(y,yi) = (y - yi)2 ∧ B2 is 2B-Lipschitz ContinU-
ous, by applying the contraction property of Rademacher complexity and Lemma 5, for HQ =
{' ◦ f ∣f ∈ Fq} We have
R(HQ) ≤ 2BqJ 2log(2d).
Directly applying Theorem 3 yields the resUlt.
Proposition 8. For the truncated risk, we have, with probability at least 1 - δ,
sup |Gb (θ)∣≤ 4BQr2^)+ B2r^δ)	(20)
kθkP≤Q	n	n
Proof of Theorem 4 Consider the decomposition F = ∪l∞=1Fl, Where Fl = {fm(X; θ) | kθkP ≤
l}. Let δι = 亮 where C = P∞=1γ ⅛∙. According to Theorem 8, if we fixed l in advance, then with
probability at least 1 - δl over the choice of S,
SUp ∣Gn(θ)∣ ≤ 4BlJ2log(2d) + B2r2log(2.l).
kθkP ≤l	n	n
So the probability that there exists at least one l sUch that (B) fails is at most Pl∞=1 δl = δ. In other
words, with probability at least 1 - δ, the ineqUality (B) holds for all l.
Given an arbitrary set of parameters θ, denote l0 = min{l | kθkP ≤ l}, then l0 ≤ kθkP + 1.
EqUation (B) implies that
∣Gn(θ)∣≤ 4Bl0Bg"/三/δ)
nn
≤ 4B(kθkp + 1)r"叵 + B2r 2lθg(2c(1 + kθkP)2∕δ).
nn
13
Under review as a conference paper at ICLR 2019
C THE POOF of Lemma 2
Proof. Let Z = ∕m(x; θ) - f * (x) - ε, then for any B ≥ 2 + τo, we have
∣L(θ) - Lb(θ)∣ = E [(Z2 - B2)1∣z∣≥b]
r∞	∞
=	P{Z2 - B2 ≥ t2}dt2 ≤ P{∣ZI ≥ √B2 +12}dt2
00
≤ j P{∣ε∣ ≥ √B2 +12 - 2}dt2
0
Z ∞	2	∞
e-2σ2 ds2 = 2c0σ2	e-sds
=2c0σ2e-B2∕2σ2
Since Bn ≥ 2 + max{τ0, σ2 logn}, we have 2c0σ2e-2n ≤ 2c0σ2n-1/2. Therefore,
SuP IL ⑻—⅛τ(θ)∣] ≤ 2√n-.
□
14