Under review as a conference paper at ICLR 2019
Answer-based Adversarial Training for
Generating Clarification Questions
Anonymous authors
Paper under double-blind review
Ab stract
We propose a generative adversarial training approach for the problem of clari-
fication question generation. Our approach generates clarification questions with
the goal of eliciting new information that would make the given context more
complete. We develop a Generative Adversarial Network (GAN) where the gen-
erator is a sequence-to-sequence model and the discriminator is a utility function
that models the value of updating the context with the answer to the clarification
question. We evaluate on two datasets, using both automatic metrics and human
judgments of usefulness, specificity and relevance, showing that our approach out-
performs both a retrieval-based model and ablations that exclude the utility model
and the adversarial training.
1 Introduction
A goal of natural language processing is to develop techniques that enable machines to process
naturally occurring language. However, not all language is clear and, as humans, we may not always
understand each other (Grice, 1975); in cases of gaps or mismatches in knowledge, we tend to ask
questions (Graesser et al., 2008). In this work, we focus on the task of automatically generating
clarification questions: questions that ask for information that is missing from a given linguistic
context. Our clarification question generation model builds on the sequence-to-sequence approach
that has proven effective for several language generation tasks (Sutskever et al., 2014; Serban et al.,
2016; Yin et al., 2016; Du et al., 2017). Unfortunately, training a sequence-to-sequence model
directly on context/question pairs yields generated questions that are highly generic1, corroborating
a common finding in dialog systems (Li et al., 2016b). Our goal is to be able to generate questions
that are useful and specific.
To achieve this, We begin with a recent observation of Rao & DaUme In (2018), Who considered
the task of question reranking: the system should learn to generate clarification questions whose
ansWers have high utility, Which they defined as the likelihood that this qUestion WoUld lead to an
ansWer that Will make the context more complete (§ 2.3). Inspired by this, We constrUct a qUestion
generation model that first generates a qUestion given a context, and then generates a hypothetical
ansWer to that qUestion. Given this (context, qUestion, ansWer) tUple, We train a Utility calcUlator to
estimate the UsefUlness of this qUestion. We then shoW that this Utility calcUlator can be generalized
Using ideas for generative adversarial netWorks (GoodfelloW et al., 2014) for text (YU et al., 2017),
Wherein the Utility predictor plays the role of the “discriminator” and the qUestion generator is the
“generator” (§2.2), Which We train Using the MIXER algorithm (Ranzato et al., 2015).
We evalUate oUr approach on tWo qUestion generation datasets: for posts on Stack Exchange and
for Amazon prodUct descriptions (FigUre 1). Using both aUtomatic metrics and hUman evalUation,
We demonstrate that oUr adversarially trained model generates a more diverse set of qUestions than
all the baseline models. FUrthermore, We find that althoUgh all models generate qUestions that are
relevant to the context at hand, oUr adversarially-trained model generates qUestions that are more
specific to the context.2
1For instance, in the context of asking qUestions aboUt home appliances, freqUently asking like “What are
the dimensions?” or “Is it made in China?”
2Code and data release: All code Will be released Under a license at least as permissive as MIT; all data
Will be made available after pUblication sUbject to alloWance by the original licenses.
1
Under review as a conference paper at ICLR 2019
Figure 1: Sample product descrip-
tion paired with a clarification ques-
tion from amazon.com.
Figure 2: Overview of our GAN-based clarification ques-
tion generation model (refer preamble of § 2)
2	Training a Clarification Question Generator
Our goal is to build a model that, given a context, can generate an appropriate clarification ques-
tion. As a running example, we will use the Amazon setting: where the dataset consists of (context,
question, answer) triples where the context is the product description, question is clarification ques-
tion about that product that (preferably) is not already answered in the description and answer is
the seller’s (or other users’) reply to the question. Representationally, our question generator is a
standard sequence-to-sequence model with attention (§ 2.1). The learning problem is: how to train
the sequence-to-sequence model to produce good question.
An overview of our training setup is shown in Figure 2. Given a context, our question generator
outputs a question. In order to evaluate the usefulness of this question, we then have a second
sequence-to-sequence model called the “answer generator” that generates a hypothetical answer
based on the context and the question (§2.5). This (context, question and answer) triple is fed into a
Utility calculator, whose initial goal is to estimate the probability that this question/answer pair is
useful in this context (§ 2.3). This UTILITY is treated as a reward, which is used to update the ques-
tion generator using the MIXER (Ranzato et al., 2015) algorithm (§ 2.2). Finally, we reinterpret the
answer-generator-plus-utility-calculator component as a discriminator for differentiating between
true (context, question, answer) triples and synthetic triples (§ 2.4), and optimize this adversarial
objective using Mixer.
2.1	Sequence-to-sequence Model for Question Generation
We use a standard attention based sequence-to-sequence model (Luong et al., 2015) for our question
generator. Given an input sequence (context) c = (c1, c2, ..., cN), this model generates an output
sequence (question) q = (q1, q2, ..., qT). The architecture of this model is an encoder-decoder with
attention. The encoder is a recurrent neural network (RNN) operating over the input word embed-
dings to compute a source context representation C. The decoder uses this source representation to
generate the target sequence one word at a time:
T
T
P(q∣Ct) = Up(qt∣q1,q2,…，qt-ι,Ct) = ɪɪ softmax(Wsht) ; where h = tanh(Wc[Ct; ht])
t=1
t=1
〜
(1)
In Eq 1, ht is the attentional hidden state of the RNN at time t and Ws and Wc are parameters of the
model (details in Appendix A). The predicted token qt is the token in the vocabulary that is assigned
the highest probability using the softmax function. The standard training objective for sequence-to-
sequence model is to maximize the log-likelihood of all (c, q) pairs in the training data D which is
2
Under review as a conference paper at ICLR 2019
equivalent to minimizing the loss,
T
Lmle(D) = - Σ Σlogp(qt|q1,q2, ...,qt-1,c)	(2)
(c,q)∈D t=1
2.2	Training the Generator to Optimize Question Utility
Training sequence-to-sequence models for the task of clarification question generation (with context
as input and question as output) using maximum likelihood objective unfortunately leads to the
generation of highly generic questions, such as “What are the dimensions?” when asking questions
about home appliances. This issue has been observed in dialog generation as well (Li et al., 2016b).
Recently Rao & Daume In (2018) observed that usefulness of a question can be better measured
as the utility that would be obtained if the context were updated with the answer to the proposed
question. We use this observation to define a Utility based reward function and train the question
generator to optimize this reward. We train the Utility reward to predict the likelihood that a
question would generate an answer that would increase the utility of the context by adding useful
information to it (see §2.3 for details).
Similar to optimizing metrics like Bleu and Rouge, this Utility function also operates on dis-
crete text outputs, which makes optimization difficult due to non-differentiability. A successful
recent approach dealing with the non-differentiability while also retaining some advantages of max-
imum likelihood training is the Mixed Incremental Cross-Entropy Reinforce (Ranzato et al., 2015)
algorithm (MIXER). In MIXER, the overall loss L is differentiated as in REINFORCE (Williams,
1992):
L(θ) = -Eqs〜Pθr(qs) ； VθL(θ) = -Eqs〜Pθr(qs)Vθ logPθ(qs)	(3)
where ys is a random output sample according to the model pθ, where θ are the parameters of the
network. We then approximate the expected gradient using a single sample qs = (q1s, q2s, ..., qTs )
from the model distribution (pθ). In REINFORCE, the policy is initialized random, which can cause
long convergence times. To solve this, Mixer starts by optimizing maximum likelihood and slowly
shifts to optimizing the expected reward from Eq 3. For the initial ∆ time steps, MIXER optimizes
Lmle and for the remaining (T - ∆) time steps, it optimizes the external reward.
In our model, we minimize the UTILITY-based loss Lmax-utility defined as:
T
Lmax-utility = -(r(qp) - r(qb))	logp(qt|q1,q2, ...,qt-1,ct)	(4)
t=1
where r(qp) is the UTILITY based reward on the predicted question and r(qb) is a baseline reward
introduced to reduce the high variance otherwise observed when using Reinforce.
In Mixer, the baseline is estimated using a linear regressor that takes in the current hidden states of
the model as input and is trained to minimize the mean squared error (||r(qp) - r(qb)||)2. Instead
we use a self-critical training approach Rennie et al. (2017) where the baseline is estimated using
the reward obtained by the current model under greedy decoding during test time.
2.3	Estimating a Utility Function from Historical Data
Given a (context, question, answer) triple, Rao & DaUme III (2018) introduce a utility function
UTILITY(c, q, a) to calculate the value of updating a context c with the answer a to a clarification
question q. The inspiration for thier utility function is to estimate the probability that an answer
would be a meaningful addition to a context, and treat this as a binary classification problem where
the positive instances are the true (context, question, answer) triples in the dataset whereas the
negative instances are contexts paired with a random (question, answer) from the dataset. The
model we use is to first embed of the words in the context c, then use an LSTM (long-short term
memory) (Hochreiter & SChmidhuber, 1997) to generate a neural representation c of the context by
averaging the output of each of the hidden states. Similarly, We obtain a neural representation q and
a of q and a respectively using question and answer LSTM models. Finally, a feed forward neural
network FUTILITY (cq, qq, qa) predicts the usefulness of the question.
3
Under review as a conference paper at ICLR 2019
2.4	Utility GAN for Clarification Question Generation
The Utility function trained on true vs random samples from real data (as described in the pre-
vious section) can be a weak reward signal for questions generated by a model due to the large
discrepancy between the true data and the model’s outputs. In order to strengthen the reward signal,
we reinterpret the Utility function (coupled with the answer generator) as a discriminator in an
adversarial learning setting. That is, instead of taking the Utility calculator to be a fixed model
that outputs the expected quality of a question/answer pair, we additionally optimize it to distinguish
between true question/answer pairs and model-generated ones. This reinterpretation turns our model
into a form of a generative adversarial network (GAN) (Goodfellow et al., 2014).
A GAN is a training procedure for “generative” models that can be interpreted as a game between a
generator and a discriminator. The generator is an arbitrary model g ∈ G that produces outputs (in
our case, questions). The discriminator is another model d ∈ D that attempts to classify between true
outputs and model-generated outputs. The goal of the generator is to generate data such that it can
fool the discriminator; the goal of the discriminator is to be able to successfully distinguish between
real and generated data. In the process of trying to fool the discriminator, the generator produces
data that is as close as possible to the real data distribution. Generically, the GAN objective is:
LGAN(D, G)=max min Ex 〜P ^g "(，)+Ez 〜Pzlog(I -/⑶))	(5)
where X is sampled from the true data distribution p, and Z is sampled from a prior defined on input
noise variables pz.
Although GANs have been successfully used for image tasks, training GANs for text generation is
challenging due to the discrete nature of outputs in text. The discrete outputs from the generator
make it difficult to pass the gradient update from the discriminator to the generator. Recently, Yu
et al. (2017) proposed a sequence GAN model for text generation to overcome this issue. They
treat their generator as an agent and use the discriminator as a reward function to update the gener-
ative model using reinforcement learning techniques. Our GAN-based approach is inspired by this
sequence GAN model with two main modifications: a) We use the Mixer algorithm as our gener-
ator (§2.2) instead of policy gradient approach; and b) We use the UTILITY function (§ 2.3) as our
discriminator instead of a convolutional neural network (CNN).
In our model, the answer is an latent variable: we do not actually use it anywhere except to train the
discriminator. Because of this, we train our discriminator using (context, true question, generated
answer) triples as positive instances and (context, generated question, generated answer) triples as
the negative instances. Formally, our objective function is:
LGAN-U (U, M) = max min Eq〜P log u(c, q, A(c, q)) + Ec〜P log(1 — u(c, m(c), A(c, m(c))))	(6)
u∈U m∈M
where U is the Utility discriminator, M is the Mixer generator, P is our data of (context, question,
answer) triples and A is our answer generator.
2.5	Pretraining
Question Generator. We pretrain our question generator using the sequence-to-sequence model§2.1
where we define the input sequence as the context and the output sequence as the question. This
answer generator is trained to maximize the log-likelihood of all ([context+question], answer) pairs
in the training data. Parameters of this model are updated during adversarial training.
Answer Generator. We pretrain our answer generator using the sequence-to-sequence model§ 2.1
where we define the input sequence as the concatenation of the context and the question and the
output sequence as the answer. This answer generator is trained to maximize the log-likelihood of
all (context, question) pairs in the training data. Unlike the question generator, the parameters of the
answer generator are kept fixed during the adversarial training.
Discriminator. We pretrain the discriminator using (context, question, answer) triples from the
training data. For positive instances, we use a context and its true question, answer and for negative
instances, we use the same context but randomly sample a question from the training data (and use
the answer paired with that random question).
4
Under review as a conference paper at ICLR 2019
3	Experimental Results
We base our experimental design on the following research questions:
1.	Do generation models outperform simpler retrieval baselines?
2.	Does optimizing the Utility reward improve over maximum likelihood training?
3.	Does using adversarial training improve over optimizing the pretrained Utility?
4.	How do the models perform when evaluated for nuances such as specificity and usefulness?
3.1	Datasets
We evaluate our model on two datasets. The first is from StackExchange and was curated by Rao
& DaUme In (2018); the second is from Amazon, curated by McAuley & Yang (2016), and has not
previously been used for the task of question generation.
StackExchange. This dataset consists of posts, questions asked to that post on stackexchange.com
(and answers) collected from three related subdomains on stackexchage.com (askubuntu, unix and
superuser). Additionally, for 500 instances each from the tune and the test set, the dataset includes 1
to 5 other questions identified as valid questions by expert human annotators from a pool of candidate
questions. This dataset consists of 61, 681 training, 7710 validation and 7709 test examples.
Amazon. Each instance consists of a question asked about a product on amazon.com combined
with other information (product ID, question type “Yes/No”, answer type, answer and answer time).
To obtain the description of the product, we use the metadata information contained in the amazon
reviews dataset (McAuley et al., 2015). We consider at most 10 questions for each product. This
dataset includes several different product categories. We choose the Home and Kitchen cate-
gory since it contains a high number of questions and is relatively easy category for human based
evaluation. This dataset consists of 19, 119 training, 2435 validation and 2305 test examples, and
each product description contains between 3 and 10 questions (average: 7).
3.2	Baselines and Ablated Models
We compare three variants (ablations) of our proposed approach, together with an information re-
trieval baseline: GAN-Utility is our full model which is a UTILITY function based GAN train-
ing (§ 2.4) including the UTILITY discriminator, a MIXER question generator and a sequence-to-
sequence based answer generator. Max-Utility is our reinforcement learning baseline with a pre-
trained question generator described model (§ 2.2) without the adversarial training. MLE is the
question generator model pretrained on context, question pairs using maximum likelihood objective
(§ 2.1). Lucene3 is a TF-IDF (term frequency-inverse document frequency) based document rank-
ing system which given a document, retrieves N other documents that are most similar to the given
document. Given a context, we use Lucene to retrieve top 10 contexts that are most similar to the
given context. We randomly choose a question from the 10 questions paired with these contexts to
construct our Lucene baseline4. Experimental details of all our models are described in Appendix B.
3.3	Evaluation Metrics
We evaluate initially with several automated evaluation metrics, and then more substantially based
on crowdsourced human judgments.
Automatic metrics include: Diversity, which calculates the proportion of unique trigrams5 in the
output to measure the diversity as commonly used to evaluate dialogue generation (Li et al., 2016b);
Bleu (Papineni et al., 2002), which evaluate n-gram precision between a predicted sentence and
reference sentences; and Meteor (Banerjee & Lavie, 2005), which is similar to BLEU but includes
stemmed and synonym matches when measuring the similarity between the predicted sequence and
the reference sequences.
3https://lucene.apache.org/
4For the Amazon dataset, we ignore questions asked to products of the same brand as the given product
since Amazon replicates questions across same brand allowing the true question to be included in that set.
5We report trigrams, but bigrams and unigrams follow similar trends.
5
Under review as a conference paper at ICLR 2019
Model	Diversity	Amazon Bleu	Meteor	StackExchange		
				DIVERSITY	Bleu	Meteor
Reference	0.6934	—	—	0.7509	—	—
Lucene	0.6289	4.26	10.85	0.7453	1.63	7.96
MLE	0.1059	17.02	12.72	0.2183	3.49	8.49
Max-Utility	0.1214	16.77	12.69	0.2508	3.89	8.79
GAN-Utility	0.1296	15.20	12.82	0.2256	4.26	8.99
Table 1: Diversity as measured by the proportion of unique trigrams in model outputs. Bleu and
Meteor scores using up to 10 references for the Amazon dataset and up to six references for the
StackExchange dataset. Numbers in bold are the highest among the models. All results for Amazon
are on the entire test set whereas for StackExchange they are on the 500 instances of the test set that
have multiple references.
Human judgments involve showing contexts and generated questions to crowdworkers6 and ask-
ing them to evaluate the questions along several axes. Roughly, we ask for the following five judg-
ments for each question (exact wordings in Appendix C): Is it relevant (yes/no); Is it grammatical
(yes/comprehensible/incomprehensible); How specific is it to this product (four options from “spe-
cific to only this product” to “generic to any product”); Does this question ask for new information
not contained in the discription (completely/somewhat/no); and How useful is this question to a po-
tential buyer (four options from “should be included in the description” to “useful only to the person
asking”). For the last three questions, we also allowed a “not applicable” response in the case that
the question was either ungrammatical or irrelevant.
3.4	Automatic Metric Results
Table 1 shows the results on the two datasets when evaluated according to automatic metrics.
In the Amazon dataset, GAN-Utility outperforms all ablations on Diversity, suggesting that it
produces more diverse outputs. Lucene, on the other hand, has the highest Diversity since it con-
sists of human generated questions, which tend to be more diverse because they are much longer
compared to model generated questions. This comes at the cost of lower match with the reference
as visible in the Bleu and Meteor scores. In terms of Bleu and Meteor, there is inconsis-
tency. Although GAN-Utility outperforms all baselines according to Meteor, the fully ablated
MLE model has a higher Bleu score. This is because Bleu score looks for exact n-gram matches
and since MLE produces more generic outputs, it is much more likely that it will match one of 10
references compared to the specific/diverse outputs of GAN-Utility, since one of those ten is highly
likely to itself be generic.
In the StackExchange dataset GAN-Utility outperforms all ablations on both Bleu and Meteor.
Unlike in the Amazon dataset, MLE does not outperform GAN-Utility in Bleu. This is because the
MLE outputs in this dataset are not as generic as in the amazon dataset due to the highly technical
nature of contexts in StackExchange. As in the Amazon dataset, GAN-Utility outperforms MLE
on Diversity. Interestingly, the Max-Utility ablation achieves a higher Diversity score than
GAN-Utility. On manual analysis we find that Max-Utility produces longer outputs compared to
GAN-Utility but at the cost of being less grammatical.
3.5	Human Judgements Analysis
Table 2 shows the numeric results of human-based evaluation performed on the reference and the
system outputs on 500 random samples from the test set of the Amazon dataset.7 These results
overall show that the GAN-Utility model successfully generates the most specific questions, while
being equally good at seeking new information and being useful to potential buyers. All approaches
produce relevant, grammatical questions. All our models are all equally good at seeking new in-
formation, but are weaker than Lucene, which performs better according to new information but at
6We use Figure-Eight, https://www.figure-eight.com. We paid crowdworkers 5 cents per judg-
ment.
7We could not ask crowdworkers evaluate the StackExchange data due to its highly technical nature.
6
Under review as a conference paper at ICLR 2019
Model	Relevant [0-1]	Grammatical [0-2]	Specific [0-4]	New Info [0-3]	Useful [0-1]
Reference	0.96	1.97	3.07	2.68	0.79
Lucene	0.90	195	2.80	2.56	0.77
MLE	0.94	1.91	2.84	2.48	0.93
Max-Utility	0.94	1.91	2.88	2.47	0.90
GAN-Utility	0.95	1.91	2.99	2.51	0.94
Table 2: Results of human judgments on model generated questions on 500 sample Home & Kitchen
product descriptions. The options described in § 3.3 are converted to corresponding numeric range
(as described in Appendix C). The difference between the bold and the non-bold numbers is statis-
tically insignificant with p <0.001. Reference is excluded in the significance calculation.
Specificity
300
LLLlrtF
Ref Lucene MLE Max-Utility GAN-UtiItty
■ This product ・ Similar Products ・ Products of this type
■ Products in Home & Kitchen ■ N/A
Figure 4: Results of human judgements on the
usefulness criteria.
Figure 3: Results of human judgements on the
specificity criteria.
the cost of much lower specificity and slightly lower relevance. Our models are all equally good
also at generating useful questions: their usefulness score is significantly better than both Lucene
and Reference, largely because Lucene and Reference tend to ask questions that are more often
useful only to the person asking the question, making them less useful for potential other buyers
(see Figure 4). Our full model, GAN-Utility, performs significantly better when measured by speci-
ficity to the product, which aligns with the higher Diversity score obtained by GAN-Utility under
automatic metric evaluation.
4	Related Work
Question Generation. Most previous work on question generation has been on generating read-
ing comprehension style questions i.e. questions that ask about information present in a given text
(Heilman, 2011; Rus et al., 2010; 2011; Duan et al., 2017). Outside reading comprehension ques-
tions, Labutov et al. (2015) use crowdsourcing to generate question templates, Liu et al. (2010) use
templated questions to help authors write better related work sections, Mostafazadeh et al. (2016)
introduced visual question answer tasking that focuses on generating natural and engaging questions
about an image. Mostafazadeh et al. (2017) introduced an extension of this task called the Image
Grounded Conversation task where they use both the image and some initial textual context to gen-
erate a natural follow-up question and a response to that question. Buck et al. (2017) propose an
active question answering model where they build an agent that learns to reformulate the question to
be asked to a question-answering system so as to elicit the best possible answers. Duan et al. (2017)
extract large number of question-answer pairs from community question answering forums and use
them to train a model that can generate a natural question given a passage.
Neural Models and Adversarial Training for Text Generation. Neural network based models
have had significant success at a variety of text generation tasks, including machine translation
(Bahdanau et al., 2015; Luong et al., 2015), summarization (Nallapati et al., 2016), dialog (Serban
et al., 2016; Bordes et al., 2016; Li et al., 2016a; Serban et al., 2017), textual style transfer (Jhamtani
et al., 2017; Kabbara & Cheung, 2016; Rao & Tetreault, 2018) and question answering (Yin et al.,
2016; Serban et al., 2016).Our task is most similar to dialog, in which a wide variety of possible
outputs are acceptable, and where lack of specificity in generated outputs is common. We addresses
7
Under review as a conference paper at ICLR 2019
Title	HOST Freeze Cooling Wine Glass		
Product	Say goodbye to lukeWarm drinks With the FREEZE!		
Description	Just pop in the freezer, pour and enjoy perfectly chilled Wine.		
	No need to preplan by chilling your Wine or drink beforehand.		
		Specific	Useful
Reference	ours have an odd odor to them. hoW do We get rid of that	4	1
Lucene	Where is the vinoice Wine pourer With chill rod made ? usa ?	3	3
MLE	does this come With a straW ?	3	4
Max-Utility	does this come With a straW ?	3	1
GAN-Utility	does this come With a lid ?	4	4
Title	Miracle WhiSk SelfMixer Stainless Steel No Batteries Hand Push		
Product	Put an end to cramped hands and splattered messes!		
Description	The Miracle Whisk is a cordless, battery-free hand operated Whisk.		
	Simply push doWn on the handle and the Whisk smoothly and		
	easily rotates to mix, froth or Whip your recipes to perfection.		
		Specific	Useful
Reference	dimensions pl	1	4
Lucene	does the poWer button stay at the “ on ” position When you push it ,	4	3
	or do you have to keep holding it to make the attachment spin ?		
MLE	does it Work on 220v ?	2	3
Max-Utility	does this Work on a glass top mixer ?	4	4
GAN-Utility	does it come With the beaters ?	3	4
Table 3: Example outputs from each of the systems for a single product description
this challenge using an adversarial network approach (Goodfellow et al., 2014), a training procedure
that can generate natural-looking outputs, which have been effective for natural image generation
(Denton et al., 2015). Due to the challenges in optimizing over discrete output spaces like text, Yu
et al. (2017) introduced a Seq(uence)GAN approach where they overcome this issue by using Re-
inforce to optimize. Li et al. (2017) train an adversarial model similar to SeqGAN for generating
next utterance in a dialog given a context. However, unlike our work, their discriminator is a binary
classifier trained to distinguish between human and machine generated utterances. Finally, Fedus
et al. (2018) introduce an actor-critic conditional GAN for filling in missing text conditioned on the
surrounding context.
5	Conclusion
In this work, we describe a novel approach to the problem of clarification question generation. Given
a context, We use the observation of Rao & DaUme In (2018) that the usefulness of a clarification
question can be measured by the value of updating the context with an answer to the question. We
use a sequence-to-sequence model to generate a question given a context and a second sequence-
to-sequence model to generate an ansWer given the context and the question. Given the (context,
predicted question, predicted ansWer) triple We calculator the utility of this triple and use it as a
reWard to retrain the question generator using reinforcement learning based Mixer model. Further,
to improve upon the utility function, We reinterpret it as a discriminator in an adversarial setting
and train both the utility function and the Mixer model in a minimax fashion. We find that our
adversarial training approach produces more diverse questions compared to both a model trained
using maximum likelihood objective and a model trained using utility reWard based reinforcement
learning. There are several avenues of future Work in this area. FolloWing Mostafazadeh et al.
(2016), We could combine text input With image input to generate more relevant questions. Because
some questions can be ansWered by looking at the product image in the Amazon dataset (McAuley &
Yang, 2016), this could help generate more relevant and useful questions. As in most One significant
research challenge in the space of free text generation problems When the set of possible outputs is
large, is that of automatic evaluation (LoWe et al., 2016): in our results We saW some correlation
betWeen human judgments and automatic metrics, but not enough to trust the automatic metrics
completely. Lastly, integrating such a question generation model into a real World platform like
StackExchange or Amazon to understand the real utility of such models and to unearth additional
research questions.
8
Under review as a conference paper at ICLR 2019
References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In ICLR, 2015.
Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved
correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic
evaluation measuresfor machine translation and/or summarization, pp. 65-72, 2005.
Antoine Bordes, Y-Lan Boureau, and Jason Weston. Learning end-to-end goal-oriented dialog.
arXiv preprint arXiv:1605.07683, 2016.
Christian Buck, Jannis Bulian, Massimiliano Ciaramita, Andrea Gesmundo, Neil Houlsby, Woj-
ciech Gajewski, and Wei Wang. Ask the right questions: Active question reformulation with
reinforcement learning. arXiv preprint arXiv:1705.07830, 2017.
Emily L Denton, Soumith Chintala, Arthur Szlam, and Rob Fergus. Deep generative image models
using a Laplacian pyramid of adversarial networks. In Advances in Neural Information Processing
Systems, pp. 1486-1494, 2015.
Xinya Du, Junru Shao, and Claire Cardie. Learning to ask: Neural question generation for reading
comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), volume 1, pp. 1342-1352, 2017.
Nan Duan, Duyu Tang, Peng Chen, and Ming Zhou. Question generation for question answering.
In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,
pp. 866-874, 2017.
William Fedus, Ian Goodfellow, and Andrew M Dai. MaskGAN: Better text generation via filling
in the _. arXivpreprint arXiv:180L 07736, 2018.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor-
mation Processing Systems, pp. 2672-2680, 2014.
Art Graesser, Vasile Rus, and Zhiqiang Cai. Question classification schemes. In Proc. of the Work-
shop on Question Generation, 2008.
H Paul Grice. Logic and conversation. 1975, pp. 41-58, 1975.
Michael Heilman. Automatic factual question generation from text. PhD thesis, Carnegie Mellon
University, 2011.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Harsh Jhamtani, Varun Gangal, Eduard Hovy, and Eric Nyberg. Shakespearizing modern language
using copy-enriched sequence to sequence models. In Proceedings of the Workshop on Stylistic
Variation, pp. 10-19, 2017.
Jad Kabbara and Jackie Chi Kit Cheung. Stylistic transfer in natural language generation systems
using recurrent neural networks. In Proceedings of the Workshop on Uphill Battles in Language
Processing: Scaling Early Achievements to Robust Methods, pp. 43-47, 2016.
Igor Labutov, Sumit Basu, and Lucy Vanderwende. Deep questions without deep understanding. In
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the
7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),
volume 1, pp. 889-898, 2015.
Jiwei Li, Michel Galley, Chris Brockett, Georgios Spithourakis, Jianfeng Gao, and Bill Dolan. A
persona-based neural conversation model. In Proceedings of the 54th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pp. 994-1003,
2016a.
9
Under review as a conference paper at ICLR 2019
Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, Michel Galley, and Jianfeng Gao. Deep rein-
forcement learning for dialogue generation. In Proceedings of the 2016 Conference on Empirical
Methods in Natural Language Processing, pp. 1192-1202, 2016b.
JiWei Li, Will Monroe, Tianlin Shi, Sebastien Jean, Alan Ritter, and Dan Jurafsky. Adversarial
learning for neural dialogue generation. In Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing, pp. 2157-2169, 2017.
Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and Maosong Sun. Automatic keyphrase extraction via
topic decomposition. In Proceedings of the 2010 conference on empirical methods in natural
language processing, pp. 366-376. Association for Computational Linguistics, 2010.
Ryan LoWe, Iulian V. Serban, Mike NoseWorthy, Laurent Charlin, and Joelle Pineau. On the evalu-
ation of dialogue systems With next utterance classification. In SIGDIAL, 2016.
Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based
neural machine translation. In Proceedings of the 2015 Conference on Empirical Methods in
Natural Language Processing, pp. 1412-1421, 2015.
Julian McAuley and Alex Yang. Addressing complex and subjective product-related queries With
customer revieWs. In Proceedings of the 25th International Conference on World Wide Web, pp.
625-635. International World Wide Web Conferences Steering Committee, 2016.
Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel. Image-based rec-
ommendations on styles and substitutes. In Proceedings of the 38th International ACM SIGIR
Conference on Research and Development in Information Retrieval, pp. 43-52. ACM, 2015.
Nasrin Mostafazadeh, Ishan Misra, Jacob Devlin, Margaret Mitchell, Xiaodong He, and Lucy Van-
derWende. Generating natural questions about an image. In Proceedings of the 54th Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1,
pp. 1802-1813, 2016.
Nasrin Mostafazadeh, Chris Brockett, Bill Dolan, Michel Galley, Jianfeng Gao, Georgios Sp-
ithourakis, and Lucy VanderWende. Image-grounded conversations: Multimodal context for natu-
ral question and response generation. In Proceedings of the Eighth International Joint Conference
on Natural Language Processing (Volume 1: Long Papers), volume 1, pp. 462-472, 2017.
Ramesh Nallapati, BoWen Zhou, Cicero dos Santos, Caglar Gulcehre, and Bing Xiang. Abstractive
text summarization using sequence-to-sequence rnns and beyond. In Proceedings of The 20th
SIGNLL Conference on Computational Natural Language Learning, pp. 280-290, 2016.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of the 40th annual meeting on association for
computational linguistics, pp. 311-318. Association for Computational Linguistics, 2002.
Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for Word
representation. In Proceedings of the 2014 conference on empirical methods in natural language
processing (EMNLP), pp. 1532-1543, 2014.
Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level train-
ing With recurrent neural netWorks. arXiv preprint arXiv:1511.06732, 2015.
Sudha Rao and Hal Daume III. Learning to ask good questions: Ranking clarification questions
using neural expected value of perfect information. In Proceedings of the 56th Annual Meeting of
the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, 2018.
Sudha Rao and Joel Tetreault. Dear Sir or Madam, May I introduce the GYAFC Dataset: Corpus,
Benchmarks and Metrics for Formality Style Transfer. In HLT-NAACL. The Association for
Computational Linguistics, 2018.
Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jarret Ross, and Vaibhava Goel. Self-critical
sequence training for image captioning. In CVPR, volume 1, pp. 3, 2017.
10
Under review as a conference paper at ICLR 2019
Vasile Rus, Brendan Wyse, Paul Piwek, Mihai Lintean, Svetlana Stoyanchev, and Cristian
Moldovan. The first question generation shared task evaluation challenge. In Proceedings of
the 6th International Natural Language Generation Conference, pp. 251-257. Association for
Computational Linguistics, 2010.
Vasile Rus, Paul Piwek, Svetlana Stoyanchev, Brendan Wyse, Mihai Lintean, and Cristian
Moldovan. Question generation shared task and evaluation challenge: Status report. In Proceed-
ings of the 13th European Workshop on Natural Language Generation, pp. 318-320. Association
for Computational Linguistics, 2011.
Iulian Vlad Serban, Alessandro Sordoni, Yoshua Bengio, Aaron C Courville, and Joelle Pineau.
Building end-to-end dialogue systems using generative hierarchical neural network models. In
AAAI, volume 16, pp. 3776-3784, 2016.
Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron C
Courville, and Yoshua Bengio. A hierarchical latent variable encoder-decoder model for gen-
erating dialogues. In AAAI, pp. 3295-3301, 2017.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
In Advances in Neural Information Processing Systems, pp. 3104-3112, 2014.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229-256, 1992.
Jun Yin, Xin Jiang, Zhengdong Lu, Lifeng Shang, Hang Li, and Xiaoming Li. Neural generative
question answering. In Proceedings of the Twenty-Fifth International Joint Conference on Artifi-
cial Intelligence, pp. 2972-2978. AAAI Press, 2016.
Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. SeqGAN: Sequence generative adversarial nets
with policy gradient. In arxiv, 2017.
11
Under review as a conference paper at ICLR 2019
Answer-based Adversarial Training for
Generating Clarification Questions
[[Supplementary Material]]
A	Details of sequence-to-sequence model
In this section, we describe the attention based sequence-to-sequence model introduced in §2.1 of the
main paper. In Eq 1, ht is the attentional hidden state of the RNN at time t obtained by concatenating
the target hidden state h and the source-side context vector ct, and Ws is a linear transformation that
maps ht to an output vocabulary-sized vector. The predicted token qt is the token in the vocabulary
that is assigned the highest probability using the softmax function. Each attentional hidden state ht
depends on a distinct input context vector cct computed using a global attention mechanism over the
input hidden states as:
N
cct =	ant hn	(7)
n=1
ant = align(hn, ht) = exp htTWahn	Xexp htTWahn0	(8)
n0
The attention weights ant is calculated based on the alignment score between the source hidden state
hn and the current target hidden state ht .
B Experimental Details
In this section, we describe the details of our experimental setup. We preprocess all inputs (context,
question and answers) using tokenization and lowercasing. We set the max length of context to be
100, question to be 20 and answer to be 20. Our sequence-to-sequence model (§ 2.1) operates on
word embeddings which are pretrained on in domain data using Glove (Pennington et al., 2014). We
use embeddings of size 200 and a vocabulary with cut off frequency set to 10. During train time,
we use teacher forcing. During test time, we use beam search decoding with beam size 5. We use a
hidden layer of size two for both the encoder and decoder recurrent neural network models with size
of hidden unit set to 100. We use a dropout of 0.5 and learning ratio of 0.0001 In the Mixer model,
we start with ∆ = T and decrease it by 2 for every epoch (we found decreasing ∆ to 0 is ineffective
for our task, hence we stop at 2).
C Details of human based evaluation
In this section, we describe in detail the human based evaluation methodology introduced in §3.3 of
the main paper.
Relevance: We ask a Yes-No question: ”Is the question on topic”
Grammaticality: We ask ”Is the question grammatical?”, and let workers choose from: [Gram-
matical, Comprehensible and Incomprehensible].
Specificity: We ask ”How specific is the question?” and let workers choose from:
1.	Specific pretty much only to this product
2.	Specific to this and other very similar products (same product from different manufacturer)
3.	Generic enough to be applicable to many other products of this type
4.	Generic enough to be applicable to any product under Home and Kitchen
5.	N/A (Not applicable): Question is not on topic OR is incomprehensible
Seeking new information: We ask “Does the question ask for new information currently not in-
cluded in the description?” and let workers choose from: [Completely, Somewhat, No, N/A]
Usefulness: We ask “How useful is the question to a potential buyer (or a current user) of the
product?” and let workers choose from:
12
Under review as a conference paper at ICLR 2019
1.	Useful enough to be included in the product description
2.	Useful to a large number of potential buyers (or current users)
3.	Useful to a small number of potential buyers (or current users)
4.	Useful only to the person asking the question
5.	N/A (Not applicable): Not on topic OR incomprehensible OR not asking new information
13