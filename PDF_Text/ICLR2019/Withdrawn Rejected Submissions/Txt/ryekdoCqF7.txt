Under review as a conference paper at ICLR 2019
Incremental training of multi-generative ad-
VERSARIAL NETWORKS
Anonymous authors
Paper under double-blind review
Abstract
Generative neural networks map a standard, probability distribution to a complex
high-dimensional distribution, which represents the real world data set. However,
a determinate input distribution as well as a specific architecture of neural net-
works may impose limitations on capturing the diversity in the high dimensional
target space. To resolve this difficulty, we propose a training framework that
greedily produce a series of generative adversarial networks that incrementally
capture the diversity of the target space. We show theoretically and empirically
that our training algorithm converges to the theoretically optimal distribution, the
projection of the real distribution onto the convex hull of the network’s distribution
space.
1 Introduction
Generative Adversarial Nets (GAN) Goodfellow et al. (2014) is a framework of estimating genera-
tive models. The main idea Goodfellow (2017) is to train two target network models simultaneously,
in which one, called the generator, aims to generate samples that resemble those from the data dis-
tribution, while the other, called the discriminator, aims to distinguish the samples by the generator
from the real data. Naturally, this type of training framework admits a nice interpretation as a two-
person zero-sum game and interesting game theoretical properties, such as uniqueness of the optimal
solution, have been derived Goodfellow et al. (2014). It is further proved that such adversarial pro-
cess minimizes certain divergences, such as Shannon divergence, between the generated distribution
and the data distribution.
Simply put, the goal of training a GAN is to search for a distribution in the range of the generator
that best approximates the data distribution. The range is often defined by the input latent variable
z and its specific architecture, i.e., Π = {G(z, θ), θ ∈ Θ}. When the range is general enough, one
could possibly find the real data distribution. However, in practice, the range is usually insufficient
to perfectly describe the real data, which is typically of high dimension. As a result, what we search
for is in fact the I-ProjeCtion Csiszar & Shields (2004) of the real data distribution on Π.
Consider two cases:
1.	The range of the generator Π is convex (see figure 1(a)), oritis not convex but the Projection
of the real data distribution on Π,s convex hull (CONV Π) is in Π (see figure 1(b)).
2.	The range of the generator is non-convex and the Projection of the real data distribution in
CONV Π is not in the range Π (see figure 1(c)).
(a) Convex
(b) Non-Convex
(c) Non-Convex
Figure 1: Different cases of the distribution projection on Π,s convex hull
In case 1, one can find the oPtimal distribution in Π to aPProximate real data set in CONV Π. But
in case 2, using standard GANs with a single generator, one can only find the distribution in Π
1
Under review as a conference paper at ICLR 2019
that is nearest to the projection. It then makes sense to train multiple generators and use a convex
combination of them to better approximate the data distribution (than using a single generator in the
non-convex case (see figure 1(c))).
The above argument is based on the assumption that one could achieve global optimality by training,
while this is not the case in general. When reaching a local optimal distribution, in order to improve
performance, do we need to add more generators and restart training? In this paper, we put forward
a sequential training procedure that adds generators one by one to improve the performance, without
retraining the previously added generators. Our contributions can be summarized as follows.
•	We derive an objective function tailored for such a incremental training process. The ob-
jective function takes both the real data distribution and the pre-learned distribution into
consideration. We show that with this new objective, we actually maximize marginal con-
tribution when adding a new generator. We also put forward an incremental training algo-
rithm based on the new objective function.
•	We prove that our algorithm always converges to the projection of real data distribution
to the convex hull of the ranges of generators, which is the optimal solution with multiple
generators. This property continues to hold in online settings where target distribution
changes dynamically.
•	Our experiments show that our algorithm can overcome the local optimal issue mentioned
above. We perform experiments on a synthetic dataset as well as two real world datasets,
e.g., CelebA and MNIST, and conclude that our algorithm could improve the mixture dis-
tribution even in the case where the range is not sufficient enough.
•	Experiments also show that, compared with previous methods, our algorithm is fast and
stable in reducing the divergence between mixture distribution and the real data.
1.1	Related works
Recently, there have been intensive researches on improving the performance of generative adver-
sarial neural networks. Two lines of works are closely related to our paper. They focus mainly on
improving the discriminator and the generator respectively.
1.1.1	Improving the discriminator
The Unrolled GAN introduced by Metz et al. (2016) improves the discriminator by unrolling op-
timizing the objective during training, which stabilizes training and effectively reduces the mode
collapse. D2GAN proposed by Nguyen et al. (2017) utilizes two discriminators to minimize the
KL-divergence and the reverse KL-divergence respectively. It treats different modes more fairly,
and thus avoids mode collapse. DFM introduced by Warde-Farley & Bengio (2016) brings a De-
noising AutoEncoder (DAE) into the generator’s objective to minimize the reconstruction error in
order to get more information from the target manifold. Mroueh et al. (2017) proposed McGan
based on mean and covariance feature matching to stabilize the training of GANs. Finally, WGAN
introduced by Arjovsky et al. (2017) employs the Wasserstein distance, which is a more appropriate
measure of performance, and achieves more stable performance.
These works are different from ours since they focus on the discriminator by measuring the diver-
gence between the generated data and the real data more precisely. However, our work fixes the
discriminator and tries to enrich the expressiveness of the generator by combining multiple genera-
tors.
1.1.2	Improving the generator
Wang et al. (2016) proposes two methods to improve the training process. The first is self-
ensembling GANs, which assembles the generators from different epochs to stabilize training. The
other is Cascade GAN, where the authors train new generator using the data points with highest
values from the discriminator. These two methods are heuristic ways to improve training, but with
no theoretical guarantee.
Hoang et al. (2017) and Ghosh et al. (2017) proposed the methods called MGAN and multi-agent
GANs respectively. The former introduces a classifier into the discriminator to catch different
2
Under review as a conference paper at ICLR 2019
modes, while the later employ a new component into the generators’ objective to promote diversity.
Arora et al. (2017) introduces a new metric on distributions and proposes a MIX+GAN to search
for an equilibrium. But all these methods need to train the multiple generators simultaneously, and
none of them can deal with the case when the training process reaches a local optima. Also, these
models lack flexibility, in the sense that when one tries to change the number of generators, all the
generators need to be retrained.
Another closely related work is Tolstikhin et al. (2017), in which the authors propose a method called
AdaGAN, which is based on a robust reweighting scheme on the data set inspired from boosting. The
idea is that the new generators should focus more on the previous bad training data. But AdaGAN
and other boosting-like algorithms are based on the assumption that one generator could catch some
modes precisely, which may not be reasonable since the generator always learns to generate the
average samples among the real data set in order to obtain low divergence, especially when the
generator’s range is under condition of Figure 1(c). In Section 5, we compare our algorithm with
AdaGAN with different dataset.
2	Preliminaries
A GAN Goodfellow et al. (2014) takes samples (a.k.a. latent variables z) from a simple and standard
distribution as its input and generates samples in a high dimensional space to approximate the target
distribution. This is done by training a generative neural network and an auxiliary discriminative
neural network alternatively. An f-GAN Nowozin et al. (2016) generalizes the adversarial training
as minimizing the f-divergence between the real data distribution and the generated distribution,
defined as Df (pkq) = Rx qf箱) dx. A GAN is a special f-GAN that minimizes the Jensen-
Shannon divergence. The general objective function of an f-GAN can be defined as follows:
minmax F (θ, ξ) = Ex 〜Pdata [Tξ (X)I + Ex 〜Pθ [-f *(Tξ(X))I.
θξ
Here f * is the conjugate function of f in f-divergence; T represents a neural network regarded as
the corresponding discriminator; finally, θ and ξ denote the parameters of the generator and the
discriminator, respectively.
2.1	Generator Group
The adversarial training method proposed by Goodfellow et al. (2014) is playing a minimax game
between the generator and the discriminator. Such a method can be caught in local optima and thus
is undesirable (e.g., mode collapse).
In this paper we propose a novel framework to train multiple generators sequentially: We maintain
a group of generators (empty at the beginning) as well as their corresponding weights, then add
new generators into the group one by one and rebalance the weights. In particular, only the newly
added generator at each step is trained. The purpose here is to augment the capacity of the group of
generators and mitigate the local optima issue.
Define the distribution range ofa generator as Π = {p | p = G(z, θ), θ ∈ Θ}, i.e., the set of distribu-
tions that the generator can produce with different parameter θ. The distribution range is determined
by the distribution of input z and the architecture of the generative network. Define a generator group
as G = {G1, G2, , . . . , Gn} , where Gi is the generator added in step i. We associate each genera-
tor with a weight 3% > 0. Then the mixed distribution of the group is: PPre(X)=4 Pn=ι ωipi(x),
where pi(X) = Gi(z) and Φn = Pin=1 ωi is the sum of weights. When a new generator Gn+1 joins
in the group G, the group becomes G0 = G ∪ {Gn+1 } and the mixed distribution becomes
1	n+1
Pnow (X) = φ 〉： ωipi (X)
n+1 i=1
1
Φn+1
(φn ∙ PPre(X) + ωn+1 ∙ pn+1 (X)) .
3	An incremental training framework
In this section, we describe how we use a generator group to improve the performance and tackle the
local optima issue mentioned previously. To train such a generator group, we propose an incremental
3
Under review as a conference paper at ICLR 2019
training algorithm (algorithm 1) adding generators to the group sequentially. In algorithm 1, we use
Algorithm 1 Incremental Training Algorithm
Predetermined: Preaι, pz, Gi(z), i ∈{1,2,...,N} ωi, i ∈ {1, 2,...,N + 1}
Input preal, pz, ωi, i > 0.
Output generator group G.
Initialize i - 1.
repeat
Build and initialize generator Gi using the same network structure.
Set target distribution for Gi to be ptarget
Train Gi to minimize D(ptarget, pi).
φi∙preal - Pj<i ωj Pj
3i
i — i + 1.
until Convergence
D(∙, ∙) to denote the “distance” between two distributions, which can be any divergence (e.g., f-
divergence or Wasserstein distance) or a general norm.
The key step in algorithm 1 is the choice of the target distribution for training Gi . Ideally, if
D(Ptarget ,pi) = 0, then We have Pi = Ptarget. In this case, Preal = φ~ Pj=I ωj pj and after
adding Gi, the generator group G can perfectly produce the desired distribution preal . However, in
general, we have D(Ptarget, Pi) 6= 0 and our algorithm proceeds in a greedy fashion, i.e., it always
maximizes the marginal contribution of Gi to the generator group G. We devote the rest of this
section to proving the above statement.
3.1	Marginal contribution maximization at each round
In algorithm 1, we use different loss functions for each generators. The marginal contribution of the
(N + 1)-th generator is as follows when we adopt f-divergence as the distance measure:
V (GN +1 ) = Df (Ppre ||Preal ) - Df (Pnow ||Preal )
Preal
dx (1)
x
To get a better approximation to the real distribution, we fix the existing generators in the group
and tune the parameters of the new generator to minimize the distance between the new group and
the real distribution. In fact, this is equivalent to maximizing the marginal contribution of the new
generator V (GN+1) by selecting GN+1 = arg maxG0 V (G0N+1). Formally,
Proposition 1. For any distribution Preal and any existing generator group G = (G1, G2, . . . , GN),
the optimal target distribution PN+1for a new generator GN+ι to join the group G is
PN+ι(X)
φ +「PreaI(X)- φ ∙ Ppre(X)] + , Where[∙]+ = maχ{∙, 0}
ωN+1
To show this, we first introduce the χ2-divergence.
Definition 1 (χ2 -divergence). The χ2-divergence between distribution P and q is: Dχ2 (P||q)
(P(X)-q(X))2
-q(X)-
x
dX. Note that χ2-divergence is a special case of the f-divergence: Dχ2 (P||q)
Df (P||q), when f(u) = (u - 1)2
In fact, with some mild assumptions on f, the f -divergence is well-approximated by χ2-divergence
when P and q are close. The following lemma can be obtained via Taylor expansion Csiszar &
Shields (2004).
Lemma 1. For any f-divergence with f (u), iff(u) is twice differentiable at u = 1 and f00(1) > 0,
then fOrany q andP close to q we have: Df (P∣∣q)〜 f 2(I) Dχ2 (P∣∣q).
Proof of proposition 1. We rewrite the objective function equation 1 for χ2 -divergence:
V (GN+1 )
X
Preal
dX
X
(Preal - Ppre) - (Preal - Pnow )
dX.
Preal
4
Under review as a conference paper at ICLR 2019
Algorithm 2 Training GN+1 by Gradient De-
Scent____________________________________
Input： Preal, Pz, 4 -N +ι and Gi(∙, θi), ω一
i ∈ {1,2,...,N}
repeat
Sample {xjreal} from Preal
Sample {zj} from Pz
for i = 1 to N do
Obtain samples by Gi (zj , θi) as {xjG }
in Group G
end for
Generate samples {xjgen} by GN+1
Update ξt+1 = ξt + ηVξ F (θN 十口 ξt)
Update θN+ι = θNN +1 -ηVθ F (θNN +1, ξt)
until Convergence
Output： GN +1 (∙, Θn +1) * 4
Figure 2: A framework for training GN+1
Based on the former definition, we obtain V(GN +1) = R ω +>[一必常1'；+1+，.。*+1+*] dx, where
,	N+1 x	(ΦN+1)2preal	,
A = 2(φN +1 ∙ Preal - φN ∙ Ppre) and B = (φN +1 + φN)ppre - 2φN +1 ∙ Ppre ∙ Preal.
To maximize the quadratic function V(GN +1), We have PN +1 = [a/2-n +1]+, which concludes
the proof.	□
3.2 ALGORITHMS TO TRAIN Gi
According to algorithm 1, in each round, a new generator GN+1 is added and the loss function is
set to be D(Ptarget, PN+1). Therefore, when training each generator Gi, the target distribution only
depends on the real distribution and the previous generators in G. In particular, both of them are
already known (figure 2).
To minimize D(Ptarget, PGN+1 ), we conduct adversarial training by using an auxiliary discriminator
T:
F (θ, ξ) = Ex 〜Ptarget [Tξ(x)]+ Ex 〜pθl "ξ(x))],
where by the linearity of expectation:
EX~ptarget [Tξ (X)]
{φN +1Ex 〜preai [Tξ (X)] 一 φN Ex 〜ppreΒξ (X)]}
ωN+1
{φN +1Ex 〜preai [Tξ (X)] - PN=I ωi ∙ Ex 〜& [Tξ (X)]}
ωN+1
Based on these, we propose an incremental training algorithm for GN+1 as algorithm 2.
4	Theoretical Analyses
In this section, we show that although our framework which trains each generator in a greedy way,
the output distribution of the generator group will always converge. Furthermore, the converged
distribution is the closest one to the target distribution among the set of all possible distributions that
a group of generators can produce (i.e., the optimal one within the distribution range of the group of
generators).
Recall our notation that the distribution range of a generator is Π. By taking a convex combination of
multiple generators (with the same architecture), the set of all possible output distributions becomes
the convex hull of Π: CONV Π = {p(x) | p(x)=段 PN=I ωiPi(X)}, where 3% > 0 is the weight
of the i-th generator, Pi (X) = Gi (z), 1 ≤ i ≤ N, and Gi represent the generative neural networks
with the same architecture but different parameters. One can consider the parameters φωi. as the
probability of choosing the i-th generator as the final output.
5
Under review as a conference paper at ICLR 2019
• Real data
1 First G
2 Second G
3 Third G
4 Fourth G
Figure 3: We train 4 generators to catch the modes
(i.e., 8 Gaussian distributions).
4.1 Convergence Analysis
ΦJrqx-IΛI PUe Eea UΘΦM4θq
θυujsEP U-ESJeSSeM
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22
Number of Generators
Figure 4: Wasserstein distance between preal
and pnow as the number of generators increases.
Our algorithm greedily optimizes each GN+1 to minimize D(ptarget,pGN+1). By the Pinsker’s
inequality, the total variation distance between ptarget and pGN+1 is upper bounded by
PD(Ptarget,PgN+1)∕2 and We Can easily extend it to χ2-divergence by DKL(p||q) ≤ Dχ2 (p||q) +
0.42. In other words, while greedily optimizing each GN+1, the distance between ptarget and
pGN+1 is also approximately minimized. Hence it is reasonable to assume that for each GN+1, its
distance to ptarget is approximately minimized With some tolerance ≥ 0, i.e., kpGN+1 -ptargetk ≤
inf kp - ptarget k + . Under such an assumption, our algorithm approximately converges to the the
optimal distribution in CONV Π:
Proposition 2. For any Π that is connected and bounded, algorithm 2 approximately converges to
the optimal distribution within the closure of the convex hull CONV Π of Π.
To simplify the argument, We fix each ωi to be 1 and embed the discrete probability distributions
into a Hilbert space. In this case, each GN+1 approximately minimizes the distance to ptarget =
(N + 1)preal - PiN=1 pGi can be formalized as:
kpGN+1 - ptarget k ≤ inf p∈Π kp - ptarget k + ,
and our algorithm approximately converges to the optimal distribution in CONV Π if as N → ∞,
k N Pi=1 PGN — Preal Il ≤ infp∈CONV Π kp — Preal Il + c∙
Then proposition 2 is implied by the folloWing lemma.
Lemma 2. Consider a connected and bounded subset Π of a Hilbert space H and any target P ∈ H.
Let {p： }∞=1 be a SeqUenCe of points in Π such that for Ptarget = (n + 1)p — nTn,
kpn+1 - Ptargetk ≤ inf kp - PtargetIl + 3
p∈Π
(?)
where Tn = 1 En=I Pn and C ≥ 0 is a constant. Thenfor any δ > 0, there exists N > 0 such that
∀n > N, kTn - Pk ≤ inf ∣∣P — pk + (1 + δ)e.
p∈CONV Π
Corollary 1. With the finite change of target distribution, algorithm 2 can converge to the new
optimal distribution within CONV Π.
Due to the space limit, we send the proof to the appendix.
Based on corollary 1, regardless the change of target distribution, as long as it is an finite variation,
algorithm 2 can converge to the projection of new target distribution. Due to the sequential nature
and the above theoretical guarantee, our algorithm naturally generalizes the dynamic online settings.
5 Experiments
We test our algorithm on a synthesized Gaussian distribution dataset and two well-known real world
datasets: CelebA and MNIST, which are the complex high dimensional real world distributions.
6
Under review as a conference paper at ICLR 2019
2 O B 6 4
1 1
EG。E-屯 s∖Q16s
Jo ue
2 O B 6 4
1 1
EG。E-屯 s∖Q16s
Jo EOEE-T-
1 2 3 4 5 6 7 B 9 10 11 12 13 14 15 16 17 IB 19 20	1 2 3 4 5 6 7 B 9 10 11 12 13 14 15 16 17 IB 19 2。
Number of Generators	Number of Generators
Figure 5: The mean Iog(DX(Preal∣∣Pnow)) of Figure 6: Theminimum Iog(DX(Preal||pn°w))
30 repetition experiments .	of 30 repetition experiments .
We design the experiment to test our sequential training algorithm. The main purpose is not to
demonstrate high quality results, e.g., high definition pictures, but to show that our algorithm can
search for an appropriate distribution that significantly improved the performance of mixture distri-
butions as the number of generators increase, especially when the generator’s range is rather limited.
In all experiments, we use the Adam optimizer Kingma & Ba (2014) with learning rate of 5 × 10-5,
and β1 = 0.5, β2 = 0.9. Finally, we set weights ωi = 1 for convenience.
Metric. As the method mentioned in Nowozin et al. (2016), when we fix the generator, we can
train an auxiliary neural network to maximize the derived lower bound to measure the divergence
between the generated data and the real data, i.e., Df(P ||Q). Based on these theories, we import an
auxiliary neural network to measure the performance of different methods. The architecture of the
auxiliary neural network is the same as the discriminator used in each experiment. We train it for 50
epoches, which is enough to measure the differences. Then we take the mean value of the last 100
iterations as the final output.
Synthesized data. In this part, we design some experiments in R2 space. The dataset is sampled
from 8 independent two-dimensional Gaussian distributions (i.e., the blue points in figure 3). The
model is previously proposed by Metz et al. (2016).
Firstly, following the experiment designed in Metz et al. (2016) and Hoang et al. (2017), we choose
the latent variable Z in a high dimensional space as Z 〜 N(0, I256), i.e., the distribution projection
is likely to be in the generator’s range, which meets the condition of figure 1(a) or figure 1(b). In
figure 3, the blue points are the real data while the corresponding colored number represents the data
points generated by each generator respectively. As figure 3 shows, we train up to 4 generators to
approximate the data distribution and the first generator tends to catch the data with high probability
around the centre of each Gaussian. As the number of generators increasing, generated data tends to
cover the data away from the centre in order to be complementary to previous mixture distributions
and thus gains a considerable marginal profit. These results demonstrate our marginal maximization
algorithm can promote the mixture distributions to cover the data with low probabilities.
Secondly, We reduce the dimension of Z to 1, i.e., Z 〜 N(0,1) and simplify the corresponding
network architecture, so that the condition of figure 1(c) is likely met. In this part, we compare our
algorithm With the state of the art incremental training method AdaGAN Tolstikhin et al. (2017)
and the baseline method Orignal GAN.1 We train up to 20 generators in each experiment With the
same starting generator (i.e., identical first generator for each method), then measure the DX2 (P||q)
betWeen real distribution and the generated mixed distribution. We repeat the experiment for 30
times to reduce the effect of random noises. Figure 5 and figure 6 illustrate the average and the
best performance With different numbers of generators, respectively. According to the result, our
algorithm approaches to Preal faster than the other tWo methods and achieves the best performance
among all three methods.
In summary, our algorithm outperforms the other tWo both in terms of the speed of converging to
the real distribution and the quality of the final result under the case of figure 1(c).
MNIST. In this experiment, We run our algorithm on the MNIST dataset LeCun et al. (1998). We
design this experiment to measure the performance of our algorithm for a more complex data distri-
bution. We choose the latent variable as Z 〜N(0,1) to limit the corresponding generator range and
1The only difference is the target distribution, for Original GAN, We train each generator With the real data
distribution, i.e., preal, then make a convex combination of them.
7
Under review as a conference paper at ICLR 2019
9.ibx 一 IAl PUQ ffσα uə ① M4-J0 q
①UUfS-P U-ESJ①SSeM
Number of Generators
Figure 7: W-distance between mixture distribu-
tion and the real data.
SOPUe(ŋlfuɑ ⊂ΦΦ5Φ^
①UU£MP U-BiS.IBSSPM
Figure 8: W-distance between distribution
Gi (z, θ) and the real data .

use the Wasserstein distance to measure the gap between mixed distribution and the real dataset.2
Then we train up to 22 generators to approximate the real distribution and the result is showed in
figure 4. Our algorithm outperforms the Original GAN but is inferior to the AdaGAN with the first
8 generators. As the number of generators increases, AdaGAN seems to run into a bottleneck while
both our algorithm and the Original GAN gradually approximate to the real data distribution.
In order to analysis the convergence, we further train up to 100 generators with both our algorithm
and the original GAN. In figure 7, the horizontal dash lines represent the minimum value of the
Wasserstein distance for the two method respectively. As showed in figure, the distance gradu-
ally decrease with the number of generators increasing, and our algorithm is much faster to reduce
the distance and can even obtain a better performance. More over, as we tends to investigate the
property of each generator in the generators’ group, we measure the Wasserstein distance between
distribution G(z, θ) and the real data, the experiment result is showed in figure 8 and the dash lines
in figure 8 represent the mean value of the 100 generators. Interestingly, the result shows that, in
each generator, original GAN tends to search a distribution in the distribution range that is closer to
the real data distribution, while our algorithm is searching for a distribution that is complementary
to the generators’ group (i.e., a huge decrease in the mixture condition in figure 7) even if its own
performance is poor (i.e., a high distance in the in figure 8).
CelebA. We also conduct our experiment on the CelebA dataset Liu et al. (2015). As shown in figure
1, we start with an identical generator and train up to 6 generators using different methods. The
measured Wasserstein distance between mixed distribution of Group G and the real-data distribution
is showed in figure 1. In this experiment, we use the training method WGAN-GP proposed by
Gulrajani et al. (2017). The experiment results indicates that our algorithm outperforms the other
two methods after the second generator. It demonstrates the potential of our algorithm applying to
real world datasets.
Table 1: Wasserstein distance (WD) between Group G and the CelebA dataset with different num-
bers of generators (i.e., |G|).
Method	|G|=1	|G|=2	∣G∣=3	|G|=4	|G|=5	|G|=6
Original GAN		588.482	337.859	307.117	189.149	146.062
AdaGAN	789.655	607.619	569.948	322.936	206.326	248.529
Incremental GAN		704.004	277.993	154.544	206.693	52.9456
6 Future work
We intend to further investigate the problem of how to accelerate the convergence via optimal gen-
erator weights, i.e., ωi . Moreover, by measuring the relativity between the current group G and the
dynamically changing target distribution, we intend to apply our algorithm to online learning.
References
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference
on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of
Proceedings of Machine Learning Research, pp. 214—223. PMLR, 2017.
2This is a similar method to the metric defined previously, the only difference is that we use WGAN-GP
method instead of f-GAN.
8
Under review as a conference paper at ICLR 2019
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium
in generative adversarial nets (gans). In Doina Precup and Yee Whye Teh (eds.), Proceedings of
the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-
11 August 2017, volume 70 of Proceedings ofMachine Learning Research,pp. 224-232. PMLR,
2017.
Imre Csiszar and Paul C. Shields. Information theory and statistics: A tutorial. Foundations and
Trends in Communications and Information Theory, 1(4), 2004. doi: 10.1561/0100000004.
Arnab Ghosh, Viveka Kulharia, Vinay P. Namboodiri, Philip H. S. Torr, and Puneet Kumar Dokania.
Multi-agent diverse generative adversarial networks. CoRR, abs/1704.02906, 2017.
Ian J. Goodfellow. NIPS 2016 tutorial: Generative adversarial networks. CoRR, abs/1701.00160,
2017.
Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In Zoubin Ghahramani,
Max Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger (eds.), Advances in
Neural Information Processing Systems 27: Annual Conference on Neural Information Process-
ing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pp. 2672-2680, 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C. Courville. Im-
proved training of wasserstein gans. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio,
Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances
in Neural Information Processing Systems 30: Annual Conference on Neural Information Pro-
cessing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pp. 5769-5779, 2017.
Quan Hoang, Tu Dinh Nguyen, Trung Le, and Dinh Q. Phung. Multi-generator generative adversar-
ial nets. CoRR, abs/1708.02556, 2017.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR,
abs/1412.6980, 2014.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of International Conference on Computer Vision (ICCV), 2015.
Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial
networks. CoRR, abs/1611.02163, 2016.
Youssef Mroueh, Tom Sercu, and Vaibhava Goel. Mcgan: Mean and covariance feature matching
GAN. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Confer-
ence on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70
of Proceedings of Machine Learning Research, pp. 2527-2535. PMLR, 2017.
Tu Nguyen, Trung Le, Hung Vu, and Dinh Q. Phung. Dual discriminator generative adversarial nets.
In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N.
Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems
30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017,
Long Beach, CA, USA, pp. 2667-2677, 2017.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers
using variational divergence minimization. In Daniel D. Lee, Masashi Sugiyama, Ulrike von
Luxburg, Isabelle Guyon, and Roman Garnett (eds.), Advances in Neural Information Processing
Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-
10, 2016, Barcelona, Spain, pp. 271-279, 2016.
Ilya O. Tolstikhin, Sylvain Gelly, Olivier Bousquet, Carl-Johann Simon-Gabriel, and Bernhard
Scholkopf. Adagan: Boosting generative models. In Isabelle Guyon, Ulrike von Luxburg, Samy
Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Ad-
vances in Neural Information Processing Systems 30: Annual Conference on Neural Information
Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pp. 5430-5439, 2017.
9
Under review as a conference paper at ICLR 2019
Yaxing Wang, Lichao Zhang, and Joost van de Weijer. Ensembles of generative adversarial net-
works. CoRR, abs/1612.00991, 2016.
David Warde-Farley and Yoshua Bengio. Improving generative adversarial networks with denoising
feature matching. 2016.
10
Under review as a conference paper at ICLR 2019
A Missing Proofs
A.1 Proof of Lemma 2
Proof of lemma 2. Without loss of generality, we can assume ρ = 0, since otherwise we can add an
offset -ρ to the Hilbert space H.
Denote dn = n∣∣Tn∣∣ = k PZi p"L Then according to equation ?,
dn+1 = (n + 1)kTn+1 k ≤ inf kp + nTn k + .
p∈Π
Let P ∈ Π be the point that minimizes the distance between -nTn and its projection p⊥ on line
-nTn, i.e.,P = argmi∖∈∏ ∣∣p⊥ + n‰k, wherep⊥ = hpnnTniH ∙ nT.
Then We can further bound dn+i by ∣p + nTn∣,
dn+i - e ≤kP + nTnk = PkP — P⊥k2 + kP⊥ + nTnk2.
On one hand, since Tn ∈ CONV Π, p⊥ can be seen as the projection of P on Tn as well, hence
IlP — P⊥ k ≤ IlP - Tn∣∣. Note that Π is bounded, therefore ∣p - Tnk is bounded by the diameter of
Π, denoted as d ≥ 0.
On the other hand, suppose that p* is the point closest to P = 0 within CONV Π, i.e., p* =
arg minp∈CONV ∏ ∣∣P∣∣∙ Since Π is connected, therefore the projection of Π on —nTn is the same
with the projection of Conv Π. Hence,
kP⊥ + nTnk ≤ ∣∣P⊥ + nTnk ≤ I∣P* + nTnk ≤ ∣∣P*k + dn.
In other words,
dn+1 - e ≤ Pd + (∣P*k + dn).
If kP*k = infp∈C0NV ∏ ∣∣P∣ > 0, then we have dn = n∣Tnk ≥ n∣P*∣. Hence
dn+1 ≤ E + ∣∣P* k + dn + d2∕2(IlP* k + dn)
≤ E + ∣∣P* k + dn + d2∕2(n + 1) ∣∣P* k
≤ (∣P*k+ E)(n +1)+ d2∕2∣P*k∙ ln(n +1)∙
Then for any δ > 0, let N be suficiantly large such that InNN ≤ 2δe∣P*k∕d2, we have
kTn - Pk = dn/n ≤	inf	IIp - Pk +E + δE.
p∈C0NV ∏
Otherwise, ∣P*k = 0 and dn+i ≤ E + d2cP + dn. Note that the upper bound is increasing in dn,
hence ∀n > 0, dn ≤ d*n for d*n defined as follows:
do =0, dn+i =E + Jd2 + dn2.
For which, we can easily prove by induction that dn ≤ n + √nd. Therefore ∣∣Tnk = dn ≤
E + d/√n, which immediately completes the proof.	口
A.2 Proof of Corollary 1
Proof of corollary 1. Without loss of generality, we assume the optimal projection of target distri-
bution is changed from P to P0 after n0 iterations, where n0 ∈ R+ is a constant value. Then we
can derive kTn+n。一 P0k ≤ n"nTn-P k + n"nTnn-P k, where n ∈ R+ is the training iteration after
change.
Then based on lemma 2, we obtain lim kTn - P0k ≤ infp∈∏ kP - P0k + E. On the other side,
n→+∞
for a specific n0, kTn0 - P0k ≤ kTn0 - Pk + kP - P0k is a bounded value if the variation of target
distribution is limited.
Finally, we can obtain lim kTn+no 一 P0k ≤ infp∈∏ ∣∣P 一 ρ0∣ + e, which concludes the proof. 口
n→+∞
11
Under review as a conference paper at ICLR 2019
A.3 THE GAP BETWEEN KL DISTANCE AND χ2 DISTANCE
Lemma 3. At worst case, DKL(p||q) ≤ Dχ2 (p||q) + 0.42.
Proof. According to f-divergence, We have Df (p||q) = JX q(χ) ∙ f (P(X)) dx. For KL-divergence
and χ2 -divergence, the corresponding f(t) are fKL(t) = t log(t) and fχ2 (t) = (t-1)2 respectively.
Import an auxiliary function as:
F(t) =fχ2(t) -fKL(t)
Then based on the monotonicity of F(t), We have F (t)min ≥ -0.42.
dx
+ 0.42 dx
DKL(p||q) =	p(x)fKL
x
≤	p(x)
x
= Dχ2 (p||q) + 0.42
Which conclude the proof.
□
12