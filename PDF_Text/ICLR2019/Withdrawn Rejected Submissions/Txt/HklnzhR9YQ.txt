Under review as a conference paper at ICLR 2019
Approximation and non-parametric estimation
of ResNet-type convolutional neural net-
works via block-sparse fully-connected neu-
RAL NETWORKS
Anonymous authors
Paper under double-blind review
Ab stract
We develop new approximation and statistical learning theories of convolutional
neural networks (CNNs) via the ResNet-type structure where the channel size, fil-
ter size, and width are fixed. It is shown that a ResNet-type CNN is a universal
approximator and its expression ability is no worse than fully-connected neural
networks (FNNs) with a block-sparse structure even if the size of each layer in the
CNN is fixed. Our result is general in the sense that we can automatically trans-
late any approximation rate achieved by block-sparse FNNs into that by CNNs.
Thanks to the general theory, it is shown that learning on CNNs satisfies optimal-
ity in approximation and estimation of several important function classes.
As applications, we consider two types of function classes to be estimated: the
Barron class and Holder class. We prove the clipped empirical risk minimization
(ERM) estimator can achieve the same rate as FNNs even the channel size, filter
size, and width of CNNs are constant with respect to the sample size. This is
minimax optimal (UP to logarithmic factors) for the Holder class. Our proof is
based on sophisticated evaluations of the covering number of CNNs and the non-
trivial parameter rescaling technique to control the Lipschitz constant of CNNs to
be constructed.
1	Introduction
Convolutional Neural Network (CNN) is one of the most popular architectures in deep learning
research, with various applications such as computer vision (Krizhevsky et al. (2012)), natural
language processing (Wu et al. (2016)), and sequence analysis in bioinformatics (Alipanahi et al.
(2015), Zhou & Troyanskaya (2015)). Despite practical popularity, theoretical justification for the
power of CNNs is still scarce from the viewpoint of statistical learning theory.
For fully-connected neural networks (FNNs), there is a lot of existing work, dating back to the
80’s, for theoretical explanation regarding their approximation ability (Cybenko (1989), Barron
(1993), Lu et al. (2017), Yarotsky (2017), and Petersen & Voigtlaender (2017)) and generalization
power (Barron (1994), Arora et al. (2018), and Suzuki (2018)). See also Pinkus (2005) and Kainen
et al. (2013) for surveys of earlier works. Although less common compared to FNNs, recently,
statistical learning theory for CNNs has been studied, both about approximation ability (Zhou
(2018), Yarotsky (2018), Petersen & Voigtlaender (2018)) and about generalization power (Zhou
& Feng (2018)). One of the standard approaches is to relate the approximation ability of CNNs with
that of FNNs, either deep or shallow. For example, Zhou (2018) proved that CNNs are a universal
approximator of the Barron class (Barron (1993), Klusowski & Barron (2016)), which is a histor-
ically important function class in the approximation theory. Their approach is to approximate the
function using a 2-layered FNN (i.e., an FNN with a single hidden layer) with the ReLU activation
function (Krizhevsky et al. (2012)) and transform the FNN into a CNN. Very recently independent
of ours, Petersen & Voigtlaender (2018) showed any function realizable with an FNN can extend to
an equivariant function realizable by a CNN that has the same order of parameters. However, to the
best of our knowledge, no CNNS that achieves the minimax optimal rate (Tsybakov (2008), Gine
& Nickl (2015)) in important function classes, including the Holder class, can keep the number of
1
Under review as a conference paper at ICLR 2019
units in each layer constant with respect to the sample size. Architectures that have extremely large
depth, while moderate channel size and width have become feasible, thanks to recent methods such
as identity mappings (He et al. (2016), Huang et al. (2018)), sophisticated initialization schemes (He
et al. (2015), Chen et al. (2018)), and normalization techniques (Ioffe & Szegedy (2015), Miyato
et al. (2018)). Therefore, we would argue that there are growing demands for theories which can
accommodate such constant-size architectures.
In this paper, we analyze the learning ability of ResNet-type ReLU CNNs which have identity map-
pings and constant-width residual blocks with fixed-size filters. There are mainly two reasons that
motivate us to study this type of CNNs. First, although ResNet is the de facto architecture in vari-
ous practical applications, the approximation theory for ResNet has not been explored extensively,
especially from the viewpoint of the relationship between FNNs and CNNs. Second, constant-width
CNNs are critical building blocks not only in ResNet but also in various modern CNNs such as
Inception (Szegedy et al. (2015)), DenseNet (Huang et al. (2017)), and U-Net (Ronneberger et al.
(2015)), to name a few. Our strategy is to replicate the learning ability of FNNs by constructing tai-
lored ResNet-type CNNs. To do so, we pay attention to the block-sparse structure ofan FNN, which
roughly means that it consists ofa linear combination of multiple (possibly dense) FNNs (we define
it rigorously in the subsequent sections). Block-sparseness decreases the model complexity coming
from the combinatorial sparsity patterns and promotes better bounds. Therefore, it is often utilized,
both implicitly or explicitly, in the approximation and learning theory OfFNNS (e.g., Bolcskei et al.
(2017), Yarotsky (2018)). We first prove that if an FNN is block-sparse with M blocks (M -way
block-sparse FNN), we can realize the FNN with a ResNet-type CNN with O(M) additional param-
eters, which are often negligible since the original FNN already has Ω(M) parameters. Using this
approximation, we give the upper bound of the estimation error of CNNs in terms of the approx-
imation errors of block sparse FNNs and the model complexity of CNNs. Our result is general in
the sense that it is not restricted to a specific function class, as long as we can approximate it using
block-sparse FNNs.
To demonstrate the wide applicability of our methods, we derive the approximation and estimation
errors for two types of function classes with the same strategy: the Barron class (of parameter s = 2)
and Holder class. We prove, as corollaries, that our CNNs can achieve the approximation error of
order O(M-DD2) for the Barron class and O(M-D) for the β-Holder class and the estimation
〜	- D + 2	〜	2β
error of order Op(N 2(D+1)) for the Barron class and Op(N- 2β+D ) for the β-Holder class, where
M is the number of parameters (we used M here, same as the number of blocks because it will
turn out that CNNs have O(M) blocks for these cases), N is the sample size, and D is the input
dimension. These rates are same as the ones for FNNs ever known in the existing literature. An
important consequence of our theory is that the ResNet-type CNN can achieve the minimax optimal
estimation error (UP to logarithmic factors) for β-Holder class even if its filter size, channel size and
width are constant with respect to the sample size, as opposed to existing works such as Yarotsky
(2017) and Petersen & Voigtlaender (2018), where optimal FNNs or CNNs could have a width or a
channel size goes to infinity as N → ∞.
In summary, the contributions of our work are as follows:
•	We develop the approximation theory for CNNs via ResNet-type architectures with
constant-width residual blocks. We prove any M -way block-sparse FNN is realizable such
a CNN with O(M) additional parameters. That means if FNNs can approximate a func-
tion with O(M) parameters, we can approximate the function with CNNs at the same rate
(Theorem 1).
•	We derive the upper bound of the estimation error in terms of the approximation error of
FNNs and the model complexity of CNNs (Theorem 2). This result gives the sufficient
conditions to derive the same estimation error as that of FNNs (Corollary 1).
•	We apply our general theory to the Barron class and Holder class and derive the approxima-
tion (Corollary 2 and 4) and estimation (Corollary 3 and 5) error rates, which are identical
to those for FNNs, even if the CNNs have constant channel and filter size with respect to
the sample size. In particular, this is minimax optimal for the Holder case.
2
Under review as a conference paper at ICLR 2019
	Zhou (2018)	Petersen & Voigtlaender 	(2018)		Ours
CNN type	Conventional	Conventional	ReSNet
Function type	Barron (s = 2)	Any 	(FNNS)		Any (block-sparse FNNs)
Channel size (Dense FNN case)	1	≥ 1	≥ 1
Channel size (β-Holder case)	N.A.	 ,	D、 O(ε-β)	。⑴
Width	Increasing	FiXed	Fixed
Filter size	Fixed	Full	Fixed
Norm bound	No	Yes	Yes
Padding	Yes	No	Yes
Table 1: Comparison of CNN architectures. “Channel size (Dense FNN case)”: The number of
channels needed to realize a function represented by a fixed-width dense FNN. “Channel size (β-
Holder case)”： The number of Channles needed to approximate a β-H0lder function with accuracy
ε measured by the sup norm. “Increasing”: The width of layer is monotonically increasing. “Full”:
Filter size is as large as the layer width. “Padding”： Whether the theory includes convolution oper-
ations with padding.
2	Related work
We summarize in Table 1 the differences in the CNN architectures between our work and Zhou
(2018) and Petersen & Voigtlaender (2018), which established the approximation theory of CNNs
via FNNs. First and foremost, Zhou (2018) only considered a specific function class — the Barron
class — as a target function class, although their method is applicable to any function class that can
be realized by a 2-layered ReLU FNN. Regarding the architecture, they considered CNNs with a
single channel and whose width is “linearly increasing” (Zhou (2018)) layer by layer. For regression
or classification problems, it is rare to use such an architecture. In addition, since they did not
bound the norm of parameters in the approximating CNNs, we cannot derive the estimation error
from this method. Petersen & Voigtlaender (2018) fully utilized the group invariance structure of
underlying input spaces to construct CNNs. Such a structure makes theoretical analysis easier,
especially for investigating the equivariance properties of CNNs since it enables us to incorporate
mathematical tools such as group theory, Fourier analysis, and representation theory. Although their
results are quite strong in that it is applicable to any function that can be approximated by FNNs, their
assumption on the group structure excludes the padding convolution layer, an important and popular
type of convolution operations. Another point is that if we simply apply their construction method
to derive the estimation error for (equivariant) Holder functions, combined with the approximation
D
result of Yarotsky (2017), the resulting CNN that achieves the minimax optimal rate has O(ε-β)
channels where ε is the approximation error threshold. It is partly because their construction is not
aware of the internal sparse structure of approximating FNNs. Finally, the filter size of their CNN is
as large as the input dimension. As opposed to these two works, we employ padding- and ResNet-
type CNNs which have multiple channels, fixed-size filters, and constant widths. Like Petersen &
Voigtlaender (2018), our result is applicable to any function, as long as the FNNs tobe approximated
are block sparse, including the Barron and Holder cases. If We apply our theorem to these classes,
we can show that the optimal CNNs can achieve the same approximation and estimation rate as
FNNs, while the number of channels is independent of the sample size. Further, this is minimax
optimal up to the logarithmic factors for the Holder class.
Due to its practical success, theoretical analysis for ResNet has been explored recently (e.g., Lin &
Jegelka (2018), Lu et al. (2018), Nitanda & Suzuki (2018), and Huang et al. (2018)). From the view-
point of statistical learning theory, Nitanda & Suzuki (2018) and Huang et al. (2018) investigated
the generalization power of ResNet from the perspective of the boosting interpretation. However,
they did not discuss the function approximation ability of ResNet. To the best of our knowledge, our
theory is the first work to provide the approximation ability of the CNN class that can accommodate
the ResNet-type ones.
3
Under review as a conference paper at ICLR 2019
We import the approximation theories for FNNs, especially ones for the Barron class and Holder
class. The approximation theory for the Barron class has been investigated in e.g., Barron
(1993), Klusowski & Barron (2016), and Lee et al. (2017). Originally Barron (1993) considered
the parameter s = 1 (see Definition 3) and the activation function σ satisfying σ(z) → 1 as z → ∞
and σ(z) → 0 as z → -∞. Later, Klusowski & Barron (2016) studied the approximation the-
ory with s = 2 and proved that 2-layered ReLU FNNs with M hidden units can approximate
functions of this class with the order of O(M-DD2). Yarotsky (2017) proved FNNs with O(S)
non-zero parameters can approximate β-Holder continuous functions with the order of O(S-β).
Using this bound, Schmidt-Hieber (2017) proved that the estimation error of the ERM estimator is
〜	2β
O(N- 2β+D ), which is minimax optimal up to logarithmic factors (see, e.g., Tsybakov (2008)).
3	Problem setting
3.1	Empirical risk minimization
We consider a regression task in this paper. Let X be a [-1, 1]D-valued random variable with un-
known probability distribution PX and ξ be an independent random noise drawn from the Gaussian
distribution with an unknown variance σ2: ξ 〜N(0, σ2) (σ > 0). Let f ◦ be an unknown deter-
ministic function f◦ : [-1,1]D → R (we will characterize f◦ rigorously in the theorems later).
We define a random variable Y by Y := f ◦(X) + ξ. We denote the joint distribution of (X, Y)
by P. Suppose we are given a dataset D = ((x1, y1), . . . , (xN, yN)) independently and identically
sampled from the distribution P, we want to estimate the true function f ◦ from the finite dataset D.
We evaluate the performance of an estimator by the squared error. For a measurable function f :
[-1, 1]d → R, we define the empirical error of f by TRD(f) := PN=1(yn - f(xn))2 and the
estimation error by R(f) := EX,Y (f(X) - Y)2 . Given a subset F of measurable functions
from [-1, 1]D → R, we consider the clipped empirical risk minimization (ERM) estimator f of F
that satisfies
f := clip[fmin] where fmin ∈ arg min RD (clip[f]).
f∈F
Here, clip is the clipping operator defined by clip[f ] := (f ∨-∣∣f ∖∣∞) ∧ Ilf ∖∣∞. For a measurable
function f : [-1, 1]D → R, we define the L2-norm (weighted by PX) and the sup norm of f by
kf∣L2(PX) := (R[-1,1]D f2(x)dPχ(x))2 and ∣∣f k∞ := suPχ∈[-1,1]D |f(x)|, respectively. Let
L2(Px ) be the set of measurable functions f such that ∣f ∣L2 (PX)< ∞ with the norm ∣∣ ∙ ∣L2 (PX).
The task is to estimate the approximation error minf ∈f ∣∣f - f ◦ ∣∞ and the estimation error of the
clipped ERM estimator: R(f) - R(f ◦). Note that the estimation error is a random variable with
respect the choice of the training dataset D. By the definition of R and the independence of X and
ξ, the estimation error equals to ∣∣f - f◦ ||：2(PX).
3.2	Convolutional neural networks
In this section, we define CNNs used in this paper. For this purpose, it is convenient to intro-
duce `0, the set of real-valued sequences whose finitely many elements are non-zero: `0 := {w =
(wn)n∈N>0 | ∃N ∈ N>0 s.t. wn = 0, ∀n ≥ N}. w = (w1 , . . . , wK) ∈ RK can be regarded as an
element of`0 by setting wn = 0 for all n > K. Likewise, for C, C0 ∈ N>0, which will be the input
and output channel sizes, respectively, we can think of (wk,j,i)k∈[K],j∈[C0],i∈[C] ∈ RK×C0×C as an
element of 'C ×C. For a filter W = (wn,j,i)n∈N>o,i∈[c],j∈[c/] ∈ 'C ×C, we define the one-sided
padding and stride-one convolution by w as an order-4 tensor LwD = ((LwD)βα,,ji) ∈ RD×D×C 0×C by
(Lw)β,j := w(α-β+1),j,i if0 ≤ α - β ≤ D - 1
D α,i 0	otherwise.
Here, i (resp. j) runs through 1 to C (resp. C0) and α and β runs through 1 to D. Since we fix
the input dimension D throughout the paper, we will omit the subscript D and write as Lw if it is
obvious from context.
4
Under review as a conference paper at ICLR 2019
Figure 1: ResNet-type CNN defined in Definition 1. Variables are as in Definition 1.
Remark 1. For K ≤ K0, we can embed RK into RK0 by inserting zeros: w = (w1 , . . . , wK) 7→
w0 = (w1, . . . , wK, 0, . . . , 0). It is easy to show Lw = Lw0. Using this equality, we can expand a
size-K filter to size-K0.
We can interpret Lw as a linear mapping from RD×C to RD×C0. Specifically, for x = (xα,i)α,i ∈
RD×C, we define (yβ,j)β,j = Lw(x) ∈ RD×C0 by
yβ,j := X(Lw)βα,,ji xα,i.
i,α
Next, we define the building block of CNNs: convolutional layers and fully-connected layers. Let
C, C0, K ∈ N>0 be the input channel size, output channel size, and filter size, respectively. For a
weight tensor w ∈ RK×C0 ×C, a bias vector b ∈ RC0 , and an activation function σ : R → R, we
define the convolutional layer Convw,b : RDXC → RDXC0 by ConvW,b(x) := σ(Lw(x) - ID 0 b)
where, 0 is the outer product of vectors and σ is applied in element-wise manner. Similarly, let
W ∈ RD XC, b ∈ R, and σ : R → R, we define the fully-connected layer FCσW,b : RD XC → R
by FCW,b(a) = σ(vec(W)>vec(a) — b). Here, vec(∙) is the vectorization operator that flattens a
matrix into a vector.
Finally, we define the ResNet-type CNN as a sequential concatenation of one convolution block, M
residual blocks, and one fully-connected layer. Figure 1 is the schematic view of the CNN we adopt
in this paper.
Definition 1 (Convolutional Neural Networks (CNNs)). Let M ∈ N>0 and Lm ∈ N>0, which will
be the number of residual blocks and the depth of m-th block, respectively. Let Cm(l) , Km(l) be the
channel size and filter size of the l-th layer of the m-th block for m = 0, . . . , M 1 and l ∈ [Lm]. We
assume C0L0) = C(LI)=…=CMLM)∙ Let Wm ∈ RKm) XCm)XCm I) and b，∈ R be the weight
tensors and biases of l-th layer of the m-th block in the convolution part, respectively. Finally, let
W ∈ RDXC0 0 and b ∈ R be the weight matrix and the bias for the fully-connected layer part,
respectively. For θ := ((wm(l))m,l, (b(ml))m,l, W, b) and an activation function σ : R → R, we define
CNNθσ : RD → RD, the CNN constructed from θ, by
CNNσ := FCW,b ◦ (ConvwM,b` + id) ◦ (ConvwM_i—_i + id)。…
◦ (ConvwI ,bι + id)。Convw0,b0 ,
where Convwm ,bm := ConvwmLm),b^)^ Convw(Lm-Fm T)。^^。Convwm)潟):
RD XC0(L0) → RD XC0(L0) is the identity function.
Although CNNθσ in this definition has a fully-connected layer, we refer to the stack of convolutional
layers both with or without the final fully-connect layer as a CNN in this paper. We say a linear
convolutional layer or a linear CNN when the activation function σ is the identity function and a
ReLU convolution layer or a ReLU CNN when σ is ReLU defined by ReLU(x) = x∨0. We borrow
the term from ResNet and call Convσwm,bm (m > 0) and id in the above definition the m-th residual
block and the m-th identity mapping, respectively. We say a 4-tuple θ is compatible with (Cm(l))m,l
and (Km(l))m,l when each component of θ satisfies the aforementioned dimension conditions.
1Note that m starts from 0. It is convenient for our purpose.
5
Under review as a conference paper at ICLR 2019
Figure 2: Schematic view of a block-sparse FNN. Variables are as in Definition 2.
For architecture parameters C = (Cm(l))m,l and K = (Km(l) )m,l (m = 0, . . . , M, l ∈ [Lm]), and
norm parameters for convolution layers B (conv) > 0 and for fully-connected layers B(fc) > 0, we
define F (CNN) = FC(C,KNN,B) (conv),B(fc) , the hypothesis class consisting of ReLU CNNs, as follows:
(	θ = ((Wm))m,ι, (b(m)m,ι, W, b) is compatible with (C, K),、
F(CNN) := C CNNReLU maxm=0,…,M,ι∈[Lm] IIwm)k∞ ∨ Ilbm)k∞ ≤ B(COnv),	> .
I	kWk∞ ∨kbk∞ ≤ B(fc)
Here, the domain of CNNs is restricted to [-1, 1]D. Note that we impose norm constraints to
the convolution part and fully-connected part separately. We emphasize that we do not impose any
sparse constraints (e.g., restricting the number of non-zero parameters in a CNN to some fixed value)
to F(CNN)
, as opposed to previous literature such as Yarotsky (2017), Schmidt-Hieber (2017), and
Imaizumi & Fukumizu (2018). Since the notation is cluttered, we sometimes omit the subscripts as
we do in the above.
Remark 2. In this paper, we adopted one-sided padding, which is not often used practically, in
order to make proofs simple. However, with slight modifications, all statements are true for equally-
padded convolutions, the widely employed padding style which adds (approximately) same numbers
of zeros to both ends of an input signal, with the exception that the filter size K is restricted to
K ≤ [弓」instead of K ≤ D 一 1. We also discuss our design choice, especially the comparison
with the original ResNet proposed in He et al. (2016) in Section G of the appendix.
3.3 Block-sparse fully-connected neural networks
In this section, we mathematically define FNNs we consider in this paper, in parallel with the CNN
case. Our FNN, which we coin a block-sparse FNN, consists of M possibly dense FNNs (blocks)
concatenated in parallel, followed by a single fully-connected layer. We sketch the architecture of a
block-sparse FNN in Figure 2.
Definition 2 (Fully-connected Neural Networks (FNNs)). Let M ∈ N>0 be the number of blocks
in an FNN. Let Dm = (Dm(1),.. . ,Dm(Lm)) ∈ NL>m0 be the sequence of intermediate dimensions
of the m-th block, where Lm ∈ N>0 is the depth of the m-th block for m ∈ [M] 2. Let
Wm(l)	∈ RDm(l)×Dm(l-1) and b(ml)
∈ R be the weight matrix and the bias of the l-th layer of m-
th block (with the convention Dm(0) = D). Let wm ∈ RDm(Lm) be the weight (sub)vector of the
final fully-connected layer corresponding to the m-th block and b ∈ R be the bias for the last
layer. For θ = ((Wm(l))m,l, (b(ml))m,l, (wm)m, b) and an activation function σ : R → R, we define
FNNθσ : RD → R, the block-sparse FNN constructed from θ, by
M
FNNσ := X wmFCWm,bm (∙) -b,
m=1
2Be aware that contrary to the CNN case, m starts from 1 here.
6
Under review as a conference paper at ICLR 2019
where FCσW b
m, m
FCWmLm),bmLm) ◦…FCWmI)碌>
We call a block-sparse FNN with M blocks a M -way block-sparse FNN. We say θ is compati-
ble with (Dm(l))m,l when each component of θ matches the dimension conditions determined by
(Dm(l) )m,l, as we did in the CNN case. Note that when Lm = 1 for all m ∈ [M], the block-
sparse FNN is a 2-layered neural network with D0 := PmM=1 Dm(1) hidden units of the form
f(x) = PdD=0 1 bdσ(ad>x - td) - b where ad ∈ RD and bd,td, b ∈ R.
For an architecture D = (Dm(l))m∈[M],l∈[Lm ] and norm parameters for the block part B(bs) > 0 and
for the final layer B(fin) > 0, we define F (FNN) = FD(FNBN(b)s) B(fin) , the set of function realizable by
FNNs:	,	,
[	θ = ((Wmm，)m,i,(bm)m,i, (wm)m, b) is compatible With D, 1
F(FNN) := F FNNReLU maXm∈[M],ι∈[Lm](kwWk∞ ∨ kbm)k∞) ≤ B(bS),	卜
[	maxm∈[M] IlwmI∣∞ ∨ |b| ≤ B(fin).	J
Again, the domain is restricted to [-1, 1]D. Similar to the CNN case, We sometimes remove sub-
scripts of the function class for simplicity.
4 Main theorems
With the preparation in the previous sections, We state our main results of this paper. We only
describe statements of theorems and corollaries and key ideas in the main article. All complete
proofs are deferred to the appendix.
4.1	Approximation
Our first main theorem claims that any M -Way block-sparse FNN is realizable by a ResNet-type
CNN With fixed-sized channels and filters by adding O(M) parameters, if We treat the Widths Dm(l)
of the FNN as constants With respect to M .
Theorem 1. Let M ∈ N>o, K ∈ {2,...D} and Lo := ∣"KK-1]∙ Let Lm ∈ N>o, Dm) ∈ N>o
(m ∈ [M]), and D = (Dm(l))m∈[M],l∈[Lm ]. Then, there exist L0m ∈ N>0 (m = 0, . . . , M), C =
(Cm(l))m=0,...,M,l∈[L0m ], andK = (Km(l))m=0,...,M,l∈[L0m ] satisfying the following conditions:
1.	L00	≤	1, L0m	≤	Lm	+	L0	(m ∈	[M]),
2.	max	Cm(l) ≤ 4 max	Dm(l), and
m=0,...,M,l∈[L0m ] m	m∈[M],l∈[Lm ] m
3.
max	Km(l) ≤ K
m=0,...,M,l∈[L0m ]
such that, for any B(bs), B(fin) > 0, we have
F (FNN)	⊂ F (CNN)
D,B(bs),B(fin) ⊂ C,K,B(conv),B(fc),
that is, any FNN in FD(F,NBN(b)s),B(fin) can be realized by a CNN in FC(C,KNN,B) (conv),B(fc). Here, B(conv)
B(bS) and BfC) = Bfin)(1 ∨ 3).	’'	'
(1)
An immediate consequence of this theorem is that if We can approximate a function f ◦ with a block-
sparse FNN, we can also approximate f ◦ with a CNN.
4.2	Estimation
z ʌ	1	∙ . 1	F	1 .1	. ∙	. ∙	i' . Λ	1 ∙	1 1-1»» r	.	P
Our second main theorem bounds the estimation error of the clipped ERM estimator f .
7
Under review as a conference paper at ICLR 2019
Theorem 2. Let f ◦ : RD → R be a measurabkfUnction and B(bs),B(fin) > 0. Let M, K, Lo, Lm,
D, B(conv) and B (fc) as in Theorem 1. Suppose L0m , C, K satisfies the equation (1) of Theorem 1
for B(bs) and B(fin) (their existence is ensured for any B (bs) and B(fin) if they satisfy the conditions
1-3. of Theorem 1). Suppose that the covering nubmer of F(CNN) ：= FCCNNB (Conv) B(fc)is larger
than 3. Then, the clipped ERM estimator f in F := {clip[f] | f ∈ F(CNN)} satisfies
ED kf - f °kL2(Pχ ) ≤ C (f ∈舞NN) kf — f °k∞ + mnNf2 lOg(2M1BN)).
(2)
Here, F(FNN)= FDFBN)S)6出口), C > 0 is a universal constant, F := kf J∞ ∨ 2, and B
B (conv) ∨ B(fc). M1 and M2 are defined by
MM
Y(1+ρm)	1+ XL0mρ+m
m=0	m=0
M L0m	0
M2 := XXm Cm(l-1)Cm(l)Km(l) + Cm(l) + C0(L00)D + 1,
m=0 l=1
WherePm := QLmO CmT)Km)B(COnV) and Pm= QLmO(I ∨ CmT)Km)B(COnV)).
The first term of (2) is the approximation error achieved by F(FNN). On the other hand, M1 and
M2 are determined by the architectural parameters of F (CNN) — M1 corresponds to the Lipschitz
constant of a function realized by a CNN and M2 is the number of parameters, including zeros, of
a CNN. Therefore, the second term of (2) represents the model complexity of F(CNN). There is a
trade-off between the two terms. Using appropriately chosen M to balance them, we can evaluate
the order of estimation error with respect to the sample size N .
Corollary 1. Under the same assumptions as Theorem 2, suppose further lOg M1 (B (COnV) ∨B (fC)) =
O⑴ as afunctιon of M. If inf f ∈f (FNN) ∣∣f 一 f ◦ k∞ = O(M-γ1) and M2 = O(M Y) forsome con-
stant γ1, γ2 > 0 independent ofM, then, the clipped ERM estimator f ofF achieves the estimation
error kf ◦一 和 L2(px ) = Op (N - 2γ1+γ2).
5 Application of main theorems
5.1	Barron class
The Barron class is an example of the function class that can be approximated by block-sparse
FNNs. We employ the definition of Barron functions used in Klusowski & Barron (2016).
Definition 3 (Barron class). We say a measurablefUnction f ◦ : [—1,1]D → R is a Barronfunction
with the parameter s > 0 if f ◦ admits the Fourier representation (i.e., f °(x) = FF[f◦]) and
Vf ◦ := JRD ∣∣wk2 |F[f°](w)∣ dw < ∞. Here, F and F are the Fourier transformation and the
inverse Fourier transformation, respectively.
KlUsoWski & Barron (2016) studied the approximation of the Barron function f ◦ with the parameter
s = 2 by a linear combination ofM ridge functions (i.e., a 2-layered ReLU FNN). Specifically, they
showed that there exists a function fM of the form
1M
fM := f°(0)+ Vf°>(0)X + Mfbm(amX — tm)+	⑶
m=1
with |bm| ≤ 1, ∣am∣ι = 1 and |tm| ≤ 1, SUCh that ∣f◦ — fM∣∣∞ = O (M-(2+*)). Using this
approximator fM, we can derive the same approximation order using CNNs by applying Theorem
1 with Li = .…=LM = 1 and D(I) = .…=DM = 1.
8
Under review as a conference paper at ICLR 2019
Corollary 2. Let f ◦ : [—1,1]D → R be a Barron function with the parameter S = 2 such that
f °(0) = 0 and Vf °(0) = 0d. Then, for any K = 2,...,D, there exists a CNN f(CNN) With M
residual blocks, each of which has depth O(1) and at most 4 channels, and whose filter size is at
most K, such that ∣∣f ◦ — f(CNN) k∞ = O (M-(1+击)
We have one design choice when we apply Corollary 1 to derive the estimation error: how to set
B(bs) and B(fin). Looking at (3), the naive choice would be B(bs) := 1 and B(fin) := M. However,
this cannot satisfy the assumption on M1 of Corollary 1, due to the term QmM=0 (1 + ρm) whose
logarithm is O(M). We want its logarithm to be O(1). In order to do that, we change the relative
scale between parameters in the block-sparse part and the fully-connected part using the homoge-
neous property of the ReLU function: ReLU(ax) = aReLU(x) for a > 0. The rescaling operation
enables Us to choose B(bs) := M and B(fin) = 1 to meet the assumption of Corollary 1. By setting
γι = 2 + D and γ2 = 1, we obtain the desired estimation error.
Corollary 3. There exist the number of residual blocks M = O (N 2+2D J, depth of each residual
block L = O (1), channel size C = O(1), filter size K ∈ {2, . . . , D}, and norm bounds for the con-
volution part B(COnv) = O (N- 2+2D ), and for the fully-connected part BfC) = O (N 2+2D )
such that for sufficiently large N , the clipped ERM estimator f of F := {clip[f] | f ∈
FCCKN),B(conv),B(fc)} achieves the estimation error ∣∣f◦ - f∣∣L2(PX) = Op (N-2⅞+⅛). Here,
Cm(l) =C,Km(l) =Kform=0,...,M,l ∈ [L] and define C = (Cm(l))m,l,K= (Km(l))m,l.
5.2	HOLDERCLASS
We next consider the approximation and error rates of CNNs when the true function is a β-Holder
function.
Definition 4 (Holder class). Let β > 0, f ◦ : [—1, 1]d → R is a B-Holderfunction if
kf °kβ
Σ
0≤∣α∣<bβC
k∂ αf °k∞ + E sup
∣α∣=bβC x=y
∣∂αf°(χ)- ∂αf°(y)∣
|x - y∣β-bβc
< ∞.
Here, α = (α1, . . . , αD) is a multi-index. That is, ∂αf :
∂lαf
∂xj1 …∂xDD
and |a| := PD=I α√.
Yarotsky (2017) showed that FNNs with O(S) non-zero parameters can approximate any D variate
C TT.. 一 C	.∙	/C 一 c、	T J	1	C 3/「_e、C I	TL 1	∕CC<r∖ I	1	.	.∣
β-Holder function (β > 0) with the order of O(S D ). Schmidt-Hieber (2017) also proved a similar
statement using a different construction method. They only specified their width (Schmidt-Hieber
(2017) only), depth, and non-zero parameter counts of the approximating FNN and did not write in
detail how non-zero parameters are distributed explicitly in the statements (see Theorem 1 of Yarot-
sky (2017) and Theorem 5 of Schmidt-Hieber (2017)). However, ifwe carefully look at their proofs,
we find that we can transform the FNNs they constructed into the block-sparse ones. Therefore,
we can utilize these FNNs and apply Theorem 1. To meet the assumption of Corollary 1, we again
rescale the parameters of the FNNs, as we did in the Barron class case, so that log Mi = O(1). We
can derive the approximation and estimation errors by setting γι = D and γ2 = 1.
Corollary 4. Let β > 0, and f ◦ : [—1,1]D → R be a β-Htjlder function. Then, for any K =
2, . . . , D, there exists a CNN f (CNN) with O(M) residual blocks, each of which has depth O(log M)
and O(1) channels, and Whosefilter size is at most K, such that ∣∣f ° — f(CNN) ∣∞ = O (M-β).
Corollary 5. There exist the number of residual blocks M = O (N 2β+D ), depth of each residual
block L = O (1), channel size C = O(1), filter size K ∈ {2, . . . , D}, norm bounds for the convo-
lution part B(COnv) = O(1), and for the fully-connected part B(fC) > 0 (log B(fC) = O(log N ))
such that for sufficiently large N , the clipped ERM estimator f of F := {clip[f] | f ∈
FCCKN),B(conv),B(fc)} achieves the estimation error ∣f° — f∣∣L2(PX) = Op (N-2β+D). Here,
Cm(l) =C,Km(l) =Kform=0,...,M,l ∈ [L] and define C = (Cm(l))m,l,K= (Km(l))m,l.
9
Under review as a conference paper at ICLR 2019
Since the estimation error rate of the β-Holder class is Op (N- 2β+D) (see, e.g., Tsybakov (2008)),
Corollary 5 implies that our CNN can achieve the minimax optimal rate up to logarithmic factors
even the width D, the channel size C, and the filter size K are constant with respect to the sample
size N .
6 Conclusion
In this paper, we established new approximation and statistical learning theories for CNNs by utiliz-
ing the ResNet-type architecture of CNNs and the block-sparse structure of FNNs. We proved that
any M -way block-sparse FNN is realizable using CNNs with O(M) additional parameters, when
the width of the FNN is fixed. Using this result, we derived the approximation and estimation errors
for CNNs from those for block-sparse FNNs. Our theory is general because it does not depend on
a specific function class, as long as we can approximate it with block-sparse FNNs. To demonstrate
the wide applicability of our results, we derived the approximation and error rates for the Barron
class and Holder class in almost same manner and showed that the estimation error of CNNS is same
as that of FNNs, even if the CNNs have a constant channel size, filter size, and width with respect
to the sample size. The key techniques were careful evaluations of the Lipschitz constant of CNNs
and non-trivial weight parameter rescaling of FNNs.
One of the interesting open questions is the role of the weight rescaling. We critically use the
homogeneous property of the ReLU activation function to change the relative scale between the
block-sparse part and the fully-connected part, if it were not for this property, the estimation error
rate would be worse. The general theory for rescaling, not restricted to the Barron nor Holder class
would be beneficial for deeper understanding of the relationship between the approximation and
estimation capabilities of FNNs and CNNs.
Another question is when the approximation and estimation error rates of CNNs can exceed that
of FNNs. We can derive the same rates as FNNs essentially because we can realize block-sparse
FNNs using CNNs that have the same order of parameters (see Theorem 1). Therefore, if we dig
into the internal structure of FNNs, like repetition, more carefully, the CNNs might need fewer
parameters and can achieve better estimation error rate. Note that there is no hope to enhance this
rate for the Holder case (up to logarithmic factors) because the estimation rate using FNNs is already
minimax optimal. It is left for future research which function classes and constraints of FNNs, like
block-sparseness, we should choose.
References
Babak Alipanahi, Andrew Delong, Matthew T Weirauch, and Brendan J Frey. Predicting the se-
quence specificities of dna-and rna-binding proteins by deep learning. Nature biotechnology, 33
(8):831, 2015.
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. arXiv preprint arXiv:1802.05296, 2018.
Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function.
IEEE Transactions on Information theory, 39(3):930-945, 1993.
Andrew R Barron. Approximation and estimation bounds for artificial neural networks. Machine
learning, 14(1):115-133, 1994.
Helmut Bolcskei, PhiliPP Grohs, Gitta KUtyniok, and PhiliPP Petersen. Optimal approximation with
sparsely connected deep neural networks. arXiv preprint arXiv:1705.01714, 2017.
Minmin Chen, Jeffrey Pennington, and Samuel Schoenholz. Dynamical isometry and a mean field
theory of RNNs: Gating enables signal ProPagation in recurrent neural networks. In Jennifer Dy
and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learn-
ing, volume 80 of Proceedings of Machine Learning Research, pp. 873-882, Stockholmsmssan,
Stockholm Sweden, 10-15 Jul 2018. PMLR. URL http://Proceedings .mlr.press/
v80/chen18i.html.
10
Under review as a conference paper at ICLR 2019
George Cybenko. Approximation by superpositions ofa sigmoidal function. Mathematics of control,
signals and Systems, 2(4):303-314,1989.
Evarist Gine and Richard NickL Mathematical foundations of infinite-dimensional statistical mod-
els, volume 40. Cambridge University Press, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Furong Huang, Jordan Ash, John Langford, and Robert Schapire. Learning deep ResNet blocks
sequentially using boosting theory. In Jennifer Dy and Andreas Krause (eds.), Proceedings of
the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
Learning Research, pp. 2058-2067, Stockholmsmssan, Stockholm Sweden, 10-15 Jul 2018.
PMLR. URL http://proceedings.mlr.press/v80/huang18b.html.
Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2017.
Masaaki Imaizumi and Kenji Fukumizu. Deep neural networks learn non-smooth functions effec-
tively. arXiv preprint arXiv:1802.04474, 2018.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In Francis Bach and David Blei (eds.), Proceedings of the 32nd
International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning
Research, pp. 448-456, Lille, France, 07-09 Jul 2015. PMLR. URL http://proceedings.
mlr.press/v37/ioffe15.html.
Paul C. Kainen, Vra Krkov, and Marcello Sanguineti. Approximating Multivariable Functions by
Feedforward Neural Nets., volume 49 of Handbook on Neural Information Processing, pp. 143-
181. Springer, 2013.
Jason M Klusowski and Andrew R Barron. Approximation by combinations of relu and squared
relu ridge functions with `1 and `0 controls. arXiv preprint arXiv:1607.07819, 2016.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (eds.),
Advances in Neural Information Processing Systems 25, pp. 1097-1105. Curran Associates, Inc.,
2012.
Holden Lee, Rong Ge, Tengyu Ma, Andrej Risteski, and Sanjeev Arora. On the ability of neural
nets to express distributions. In Satyen Kale and Ohad Shamir (eds.), Proceedings of the 2017
Conference on Learning Theory, volume 65 of Proceedings of Machine Learning Research, pp.
1271-1296, Amsterdam, Netherlands, 07-10 Jul 2017. PMLR. URL http://proceedings.
mlr.press/v65/lee17a.html.
Hongzhou Lin and Stefanie Jegelka. Resnet with one-neuron hidden layers is a universal approxi-
mator. arXiv preprint arXiv:1806.10909, 2018.
Yiping Lu, Aoxiao Zhong, Quanzheng Li, and Bin Dong. Beyond finite layer neural networks:
Bridging deep architectures and numerical differential equations. In Jennifer Dy and Andreas
Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, vol-
ume 80 of Proceedings of Machine Learning Research, pp. 3276-3285, Stockholmsmssan, Stock-
holm Sweden, 10-15 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/
lu18d.html.
Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The expressive power of
neural networks: A view from the width. In Advances in Neural Information Processing Systems,
pp. 6231-6239, 2017.
11
Under review as a conference paper at ICLR 2019
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. In International Conference on Learning Representations,
2018. URL https://openreview.net/forum?id=B1QRgziT-.
Atsushi Nitanda and Taiji Suzuki. Functional gradient boosting based on residual network per-
ception. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International
Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research,
pp. 3819-3828, Stockholmsmssan, Stockholm Sweden, 10-15 Jul 2018. PMLR. URL http:
//proceedings.mlr.press/v80/nitanda18a.html.
Philipp Petersen and Felix Voigtlaender. Optimal approximation of piecewise smooth functions
using deep relu neural networks. arXiv preprint arXiv:1709.05289, 2017.
Philipp Petersen and Felix Voigtlaender. Equivalence of approximation by convolutional neural
networks and fully-connected networks. arXiv preprint arXiv:1809.00973, 2018.
Allan Pinkus. Density in approximation theory. Surveys in Approximation Theory (SAT)[electronic
only], 1:1-45, 2005. URL http://eudml.org/doc/51470.
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedi-
cal image segmentation. In International Conference on Medical image computing and computer-
assisted intervention, pp. 234-241. Springer, 2015.
Johannes Schmidt-Hieber. Nonparametric regression using deep neural networks with relu activation
function. arXiv preprint arXiv:1708.06633, 2017.
Taiji Suzuki. Fast generalization error bound of deep learning from a kernel perspective. In Amos
Storkey and Fernando Perez-Cruz (eds.), Proceedings of the Twenty-First International Confer-
ence on Artificial Intelligence and Statistics, volume 84 of Proceedings of Machine Learning
Research, pp. 1397-1406, Playa Blanca, Lanzarote, Canary Islands, 09-11 Apr 2018. PMLR.
URL http://proceedings.mlr.press/v84/suzuki18a.html.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1-9, 2015.
Alexandre B. Tsybakov. Introduction to Nonparametric Estimation. Springer Publishing Company,
Incorporated, 1st edition, 2008. ISBN 0387790519, 9780387790510.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine trans-
lation system: Bridging the gap between human and machine translation. arXiv preprint
arXiv:1609.08144, 2016.
Dmitry Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks, 94:
103-114, 2017.
Dmitry Yarotsky. Universal approximations of invariant maps by neural networks. arXiv preprint
arXiv:1804.10306, 2018.
Ding-Xuan Zhou. Universality of deep convolutional neural networks. arXiv preprint
arXiv:1805.10769, 2018.
Jian Zhou and Olga G Troyanskaya. Predicting effects of noncoding variants with deep learning-
based sequence model. Nature methods, 12(10):931, 2015.
Pan Zhou and Jiashi Feng. Understanding generalization and optimization performance of deep
CNNs. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Con-
ference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp.
5960-5969, Stockholmsmssan, Stockholm Sweden, 10-15 Jul 2018. PMLR. URL http:
//proceedings.mlr.press/v80/zhou18a.html.
12
Under review as a conference paper at ICLR 2019
A Notation
For tensor a, a+ := a∨0 where maximum operation is performed in element-wise manner. Similarly
a- := -(-a ∨ 0). Note that a = a+ - a- holds for any tensor a. For normed spaces (V, k ∙
k v), (W, k ∙ IlW) and linear operator T : V → W We denote the operator norm of T by ∣∣T∣∣op :=
supkvk =1 kTvkW. For a sequence w = (w(1), . . . , w(L)) and l ≤ l0, we denote its subsequence
from the l-th to l0-th elements by w[l : l0] := (w(l), . . . , w(l0)). 1P equals to 1 if the statement P is
true, equals to 0 otherWise.
B	Proof Overview
B.1	Theorem 1
For f (FNN) ∈ F (FNN) , We realize a CNN f (CNN) using M residual blocks by “serializing” blocks
in the FNN and converting them into convolution layers.
First, We double the channel size using the m = 0 part of CNN (i.e., D0(L0) = 2). We Will use the
first channel for storing the original input signal for feeding to doWnstream (i.e., m ≥ 1) blocks and
the second one for accumulating the output of each blocks, that is, Pmm=1 wm> FCRWemLU,bm (x) Where
wm is the Weight of the final fully-connected layer corresponding to the m-th dense block.
For m = 1, . . . , M, We create the m-th residual block from the m-th block of f (FNN) . First, We
shoW that for any a ∈ RD and t ∈ R, there exists L0-layered 4-channel ReLU CNN With O(D)
parameters Whose first output coordinate equals to a ridge function x 7→ (a>x - t)+ (Lemma 1
and Lemma 2). Since the first layer of m-th block is concatenation of Dm(1) hinge functions, it is
realizable by a 4Dm(1)-channel ReLU CNN With L0-layers.
For the l-th layer of the m-th block (m ∈ [M], l = 2, . . . , L(ml)), We prepare Dm(l) size-1 filters made
from the Weight parameters of the corresponding layer of the FNN. Observing that the convolution
operation With size-1 filter is equivalent to a dimension-Wise affine transformation, the first coordi-
nate of the output of l-th layer of the CNN is inductively same as that of the m-th block of the FNN.
After computing the m-th block FNN using convolutions, We add its output to the accumulating
channel in the identity mapping.
Finally, We pick the first coordinate of the accumulating channel and subtract the bias term using the
final affine transformation.
B.2	Theorem 2 and Corollary 1
We relate the approximation error of Theorem 2 With the estimation error using the covering number
of the hypothesis class F(CNN). Although there are several theorems of this type, We employ the one
in Schmidt-Hieber (2017) due to its convenient form (Lemma 5). We can prove that the logarithm
of the covering number is upper bounded by M2 log((B(Conv) ∨ B(fc))Mι∕ε) (Lemma 4) using the
similar techniques to the one in Schmidt-Hieber (2017). Theorem 2 is the immediate consequence
of these tWo lemmas.
To prove Cororraly 1, We set M = O(Nα) for some α ≥ 0. Then, under the assumption of the
corolarry, we have ∣∣f ◦ — /够⑺)=O (max (N-2αγ1, Nαγ2-1)) from Theorem 2. The order of
the right hand side With respect to N is minimized When α = 2γ]+γ2. By substituting α, we can
prove Corollary 1.
C Proof of Theorem 1
C.1 Decomposition of affine transformation
The following lemma shows that any affine transformation is realizable with a
conventional CNN (without the final fully-connect layer).
Sl -
layered linear
13
Under review as a conference paper at ICLR 2019
Lemma 1. Leta ∈ RD, t ∈ R, K ∈ {2,. . . ,D
-1}, and Lo := ∣^K-1]
. Then, there exists
(RK×2×1
w(l) ∈	RK	×2×2
[rk×1×2
(for l = 1)
(for l = 2, . . . , L0 - 1)
(for l = L0)
and b ∈ R such that
L0	L0
1.	Xkw(l)k0+Xkb(l)k0≤D+L0,
l=1]	l=1
2.	max kwmk∞ = kak∞, max kb(l)k∞ = |t|, and
l∈[Lo]	l∈[L0]
3.	Conviwd,b : RD → RD satisfies Conviwd,b (x) = a>x - t for any x ∈ [-1, 1]D.
Proof. First, observe that the convolutional layer constructed from u = [u1 . . . uK ]> ∈
RK ×1×1 takes the inner product with the first K elements of the input signal: Lu(x) = PkK=1 ukxk.
In particular, u = [0 . . . 0 1]> ∈ RK ×1×1 works as the “left-translation” by K - 1. There-
fore, we should define w so that it takes the inner product with the K left-most elements in the
first channel and shift the input signal by K - 1 with the second channel. Specifically, we define
w = (w(1), . . . , w(L0)) by
0 a(L0-1)K +1
..
..
..
(W(LO))：i：= 0	aD
`	人,1,：	0	0
..
..
..
00
We set b := ( 0, . . . , 0 , t). Then w and b satisfy the condition of the lemma.
'{z~*}
L0 - 1 times
□
C.2 Transformation of a linear CNN into a ReLU CNN
The following lemma shows that we can convert any linear CNN to a ReLU CNN that has approx-
imately 4 times larger parameters. This type of lemma is also found in Petersen & Voigtlaender
(2017) (Lemma 2.3).
Lemma 2. Let C = (C(1), . . . , C(L)) ∈ NL>0 be channel sizes K = (K(1), . . . , K(L)) ∈ NL>0
be filter sizes. Let w(l) ∈ RK (l) ×Cl×C(l) and b(l) ∈ R(l). Consider the linear convolution layers
constructed from w and b: fid := Conviwd,b : RD → RD×C(L) NL>0 where w = (w(l))l and b =
(b(l))ι. Then, there exists a pair W = (W(l))ι∈[L], b = (b(l))ι∈[L] where W(l) ∈ RK(l) ×2C(l) ×2C(l 1)
and b)(-l) ∈ R2C(l) such that
L	LL	L
1.	X kW(l)k0 ≤ 4X kw(l)ko，X k)(l)ko ≤ X k)(l)ko，
l=1	l=1	l=1	l=1
14
Under review as a conference paper at ICLR 2019
2.	max ∣∣W(l)k∞ = max ∣∣w(l)k∞, max ∣∣b(l)k∞ = max∣b(l) ∣∞, and
l∈[L]	l∈[L]	l∈[L]	l∈[L]
3.	fReLU := ConvweLU : RD → RD×2C(L), satisfies fReLu(∙) = (fid(∙) + , fid(∙)-).
>Λ	C I-V T 、 C	~	Λ ?	Cll
Proof. We define W and b as follows:
(W⑴)—f (W(I))k,：,： ] for k = ι	κ⑴
(w )k,:,: = -(w(1))	for k = 1, . . . , K ,
(W(I)) =11W ①限; -(W(7)k,:,:] for k = 1 ∙∙∙ K(I)
(W )k，：，： = [-(W(I))k,：,： (W(I))k,J	k , K ,
By definition, a pair (W, b) satisfies the conditions (1) and (2). For any X ∈ RD, We set y(l):
Conviwd[1：l],b[1：l](x) ∈ RC(l)×D. We will prove
ConVweLU向i：i](X) = [y+) y-)i>
(4)
for l = 1,...,L by induction. Note that we obtain fReLU(∙) = (fid+(∙), fid-(∙)) by setting l = L.
For l = 1, by definition of W(I) we have,
(W(I))a,:,：/e'：=
J (W⑴)a,:,：xe,：
[-(W(I))a,：*，：
for any α,β ∈ [D]. Summing them UP and using the definition of b(I) yield
[Lw⑴(X)- 1D 0 b(1)]>
.	Lw⑴(x) - 1D 0 b(1)
- Lw(1) (X) -1D0b(1)
Suppose (4) holds up to l (l < L), by the definition of W(l+1),
e,：
(W(l+1))a,：,： (y ⑷产
(W(l+1))a,：,：	-(W(l+1))a,：,：] (y(l))e,：
-	(W(l + 1))a,：,：	(W(I+1%,：,」[必产
(W(l+1))a,：,： ((y+))e,： - (y-))β,)
-	(W(I+1))a,：,： ((y+))e,： - (y-))β,)
(W(l+1))a,：,：(y(l))e,：]
-	(W(I+ 1))a,：,：(y(I))β,[
for any α,β ∈ [D]. Again, by taking the summation and using the definition of b(l+1), we get
[Lw(l+1) ([y+),y-)])- 1D 0 b(1)]>
Lw(l+1)(y(l)) - 1D 0 b(l+1)
- Lw(l+1) (y(l)) - 1D 0 b(l+1)
By applying ReLU, we get
Convp(l+1),ReLU [ (l) (l)] = ReLU [ (l+1) - (l+1)]	(5)
ConVw(1+1) ,b(i+i) Uy+ ,y- ]J =ReLU Qy , y	]J .	(5)
By using the induction hypothesis, we get
ConVweL：U1+I)],b[i：(i+1)](X)=ConV≤+1⅛+U) (",“-)])
= ReLU [y(l+1), -y(l+1)]
= [y+(l+1), -y-(l+1)]
Therefore, the claim holds for l + 1. By induction, the claim holds for L, which is what we want to
prove.	□
15
Under review as a conference paper at ICLR 2019
C.3 Concatenation of CNNs
We can concatenate two CNNs with the same depths and filter sizes in parallel. Although it is
almost trivial, we state it formally as a proposition. In the following proposition, C(0) and C0 (0) is
not necessarily 1.
Proposition 1. LetC = (C(l))l∈[L], C0 = (C0(l))l∈[L], andK = (K(l))l∈[L] ∈ NL>0. Let w(l) ∈
RK(l) ×C(l) ×C (l-1), b ∈ RC(l) and denote w = (w(l))l and b = (b(l))l. We define w0 and b0 in the
same way, with the exception that C(l) is replaced with C0(l). We defineW = (W(1),..., W(L)) and
b = (b ⑴,...,b(L)) by
(W(I))k,:,: := w：	0⑷ ∈ R(C(I)+c0(l))×(c(i + c0(lT))
b(l) ：=；((Il)J ∈ R(C(I)+c0(l))
for l ∈ [L] and k ∈ [K(l)]. Then, we have,
Convw ,b([x x0])= [ConvW,b(X) ConvwO"(XO)]
for any x, x0 ∈ RD×C(0) and any σ : R → R.	□
Note that by the definition of k ∙ ∣∣o and ∣∣ ∙ k∞,we have
LL
X ∣W(l)k0 = X kw(l)ko + kw0(l)ko,
l=1	l=1
LL
X kb(l)ko = X kb(l)ko + kb0(l)ko,
l=1	l=1
max Ilw(I)k∞ = max ∣∣w(l)k∞ ∨ IlwO(I)k∞,
l∈[L]	l∈[L]
and
max kb(l)k∞=maχ kb(l)k∞ ∨kb0(l)k∞∙
C.4 Proof of Theorem 1
By the definition of FD(F,NBN(b)s),B(fin), there exists a 4-tuple θ = ((Wm(l))m,l, (b(ml))m,l, (wm)m,b)
compatible with (Dm(l))m,l (m ∈ [M] and l ∈ [Lm]) such that
max (∣Wm(l)∣∞∨∣b(ml)∣∞) ≤B(bs),	max ∣wm∣∞ ∨ |b| ≤B(fin),
m∈[M],l∈[Lm]	m∈[M]
and f (FNN) = FNNθReLU . We will construct the desired CNN consisting of M residual blocks,
whose m-th residual block is made from the ingredients of the corresponding m-th block in f (FNN)
(specifically, Wm := (Wm(l))l∈[Lm], bm := (b(ml))l∈[Lm], and wm).
[The m = 0 Block]: We prepare a single convolutional layer with 2 output channels and 2 size-1
filters suth that the first filter works as the identity function and the second filter inserts zeros to the
second channel. Weight parameters of this convolutional layer are all zeros except single one. We
denote this block by Conv0 .
[The m = 1, . . . , M Blocks]: For fixed m ∈ [M], we first create a CNN realizing FCRWemLU,bm. We
treat the first layer (i.e. l = 1) of FCRWeLU,b as concatenation of Dm(1) hinge functions RD 3 X 7→
fd(X) := ((Wm(1))dX - b(m1))+ for d ∈ [Dm(1)]. Here, (Wm(1))d ∈ R1×D is the d-th row of the matrix
Wm(1) ∈ RDm(1) ×D. We apply Lemma 1 and Lemma 2 and obtain ReLU CNNs realizing the hinge
functions. By combining them in parallel using Proposition 1, we have a learnable parameter θm(1)
16
Under review as a conference paper at ICLR 2019
such that the ReLU CNN ConvR(e1L) U : RD×2 → RD×2D(m1) constructed from θm(1) satisfies
θm
COnVReL U([x x0]>)ι = fι(X) * …fD(I)(X) *]>∙
m
Since we double the channel size in the m = 0 part, the identity mapping has 2 channels. Therefore,
we made COnVR(e1L)U so that it has 2 input channels and neglects the input signals coming from the
θm
second one. This is possible by adding filters consisting of zeros appropriately.
Next, for l-th layer (l = 2, . . . , Lm), we prepare size-1 filters wm(2) ∈ R1m×Dm() ×2D() for l = 2 and
wm(l) ∈ R1×Dm(l)×2Dm(l-1) forl = 3,.. .,Dm(Lm) defined by
W)m2) 乳[1 0] if l = 2
Wm(l)	ifl=3,...,Dm(Lm),
where 0 is the Kronecker product of matrices. Intuitively, the l = 2 layer will pick all odd indices
of the output of COnVR(e1L) U and apply the fully-connected layer. Note that COnVR(el)LU (l ≥ 2) just
θm	θm
rearranges parameters of FCRWemLU,bm .
We construct CNNs from θm(l) := (wm(l), b(ml)) (l ≥ 2) and concatenate them along with COnVR(e1L) U:
θm
ConVm ：= COnVRLm) ◦…。COnVRi2LLU ◦ COnVReLU
θm	θm	θm
The output dimension of COnVm is either RD×2Dm(Lm) (ifLm = 1) or RD×Dm(Lm) (if Lm ≥ 2)., We
denote the output channel size (either 2Dm(Lm) or Dm(Lm)) by Dm(out). By the inductive calculation,
we have
COnVm (X)1
FCRWemLU,bm(X)0 [1 0] ifLm = 1
FCRWemLU,bm(X)	ifLm ≥ 2 .
By definition, COnVm has the depth of L0 + Lm - 1, at most 4Dm(1) ∨ maxl=2,...Lm Dm(l) ≤
4 maxl∈[Lm] Dm(l) channels. The ∞-norm of its parameters does not exceed that of parameters in
ReLU
FCWm,bm.
Next, We consider the filter Wm ∈ R1 ×2×Dmout) defined by
(Wm)1,:,:=
B(bs)
B(fin)
O…	0
Wm 0 [0	1]
O…0]
Wm
ifLm = 1
ifLm≥2
Then, ConVm ：= COnVwm,0 adds the output of m-th residual block, weighted by wm, to the sec-
ond channel in the identity connections, while keeping the first channel intact. Note that the final
layer of each residual block does not have the ReLU activation. By definition, COnV0m has Dm(Lm)
parameters.
Given COnVm and COnV0m for each m ∈ [M], we construct a CNN realizing FNNθReLU . Let
f (conv) : RD → RD be the sequential interleaving concatenation of COnVm and COnV0m, that is,
f (conv) ：= (ConVM。COnVM + I )◦•••◦ (ConVI。COnV ι + I)。COnV0.
Then, we have
(bs) M
#(ConV) _ B ∖、	> 口ReLU
f 1	= Bfiy 乙 wmF CWm,bm
m=1
(the subscript 1 represents the first coordinate).
[Final Fully-connected Layer] Finally, we set W := Bfn
0 …θ] ∈ RD and put FCf,b
on top of f (ConV) to pick the first coordinate of f (ConV) and subtract the bias term. By definition,
f(CNN) := FCiwd,b 。 f(ConV) satisfies f(CNN) =f(FNN).
17
Under review as a conference paper at ICLR 2019
[Condition Check]: We will check f (FNN) satisfies the desired conditions. (Condition 1): By
definition the 0-th residual block Conv0 has L00 = 1 layer. Since Convm and Conv0m has L0 +
Lm - 1 and 1 layers, respectively, the m(≥ 1)-thresidualblockoff(CNN) has L0m = L0 + Lm
layers. (Condition 2): Convm has at most 4 maxl∈[Lm] Dm(l) channels and Conv0m has at most 2
channels, respectively. Therefore, the channel size of f (CNN) is at most 4maxm∈[M],l∈[Lm] Dm(l).
(Condition 3): Since each filter of Conv(m) and Conv0m is at most K, the filter size of CNN
is also at most K. (Conditions on B(conv) and B(fin)): Parameters of f (conv) are either 0, or
parameters of FCWLUWE, whose absolute value is bounded by B(bs), or Bfn)wm,. Since We have
kwmk∞ ≤ B(fin), the ∞-norm of parameters in f (CNN) is bounded by B(bs). The parameters of
the final fully-connected layer FCw,b is either B(fin), 0,
f ∨ B(fin).
or b, therefore their norm is bounded by
□
Remark 3. Another way to construct a CNN which is identical (as a function) to a given FNN is
as follows. First, we use a “rotation” convolution with D filters, each of which has a size D, to
serialize all input signals to channels of a single input dimension. Then, apply size-1 convolution
layers, whose l-th layer consisting of appropriately arranged weight parameters of the l-th layer of
the FNN. This is essentially what Petersen & Voigtlaender (2018) does to prove the existence of a
CNN equivalent to a given FNN. To restrict the size of filters to K, we should further replace the the
first convolution layer with O(D/K) convolution layers with size-K filters. We can show essentially
same statement using this construction method.
D Proof of Theorem 2
D.1 Covering number of CNNs
The goal of this section is to prove Lemma 4, stated in Section D.1.5, that evaluates the covering
number of the set of functions realized by CNNs F(CNN).
D.1.1 Bounds for convolutional layers
We assume w, w0 ∈ RK ×J ×I, b, b0 ∈ R, and x ∈ RD×I unless specified. We have in mind that
the activation function σ is either the ReLU function or the identity function id. But the following
proposition holds for any 1-Lipschitz function such that σ(0) = 0. Remember that we can treat
Lw as a linear operator from RD×I to RD×J. We endow RD×I and RD×J with the sup norm and
denote the operator norm Lw by kLw kop .
Proposition 2. It holds that kLw kop ≤ IKkwk∞.
Proof. Write w = (wkji)k∈[K],j∈[J],i∈[I], Lw = ((Lw)βα,,ji )α,β∈[D],j∈[J],i∈[I] . For any x =
(xα,i)α∈[D],i∈[I] ∈ RD×I, the sup norm of y := (yβj)β∈[D]j∈[J] = Lw(x) is evaluated as fol-
lows:
l∣yk∞ = max [yβ,j |
β,j
≤ max X l(Lw )α,j∣∣χα"l
α,i
≤ max χ ∣(lw )α,j∣kχ∣∞
α,i
=maxΣS lw(α-β+1),j,ilkxk∞
α,i
≤ max E (1{w(α-β+1),j,i = 0}) kwk∞kxk∞
α,i
≤ IKlwl∞lxl∞
□
18
Under review as a conference paper at ICLR 2019
Proposition 3. It holds that kConvσw,b(x)k∞ ≤ kLw kopkxk∞ + |b|.
Proof.
kConvW,b(x)k∞ ≤kσ(Lw(X)- Id 乳 b)k∞
≤ IlLw (X)- 1D 0 bk∞
≤kLw(x)k∞ + klD 0 bk∞
≤kLw kopkxk∞ + ∣b∣.
□
Proposition 4. The Lipschitz constant of Convσw,b is bounded by ILw Iop.
Proof. For any X, X0 ∈ RD×I ,
IConvσw,b(X) - Convσw,b(X0)I∞ = Iσ(Lw(X)-1D0b) -σ(Lw(X0)-1D0b)I∞
≤I (Lw(X)-1D0b)-(Lw(X0)-1D0b)I∞
≤ILw(X-X0)I∞
≤ ILwIopIX-X0I∞.
Note that the first inequality holds because the ReLU function is 1-Lipschitz.	□
Proposition 5. It holds that IConvσw,b(X) - Convσw0,b0 (X)I ≤ ILw-w0 IopIXI∞ + |b - b0|.
Proof.
IConvσw,b(X) - Convσw0,b0 (X)I = Iσ(Lw(X) - 1D 0 b) - σ(Lw0 (X) - 1D 0 b0)I∞
≤ I(Lw(X)-1D0b)-(Lw0(X)-1D0b0)I
=ILw(X)-Lw0(X)I +I1D0(b-b0)I∞
≤kLw-w0 kopkxk∞ + ∣b — b0l
□
D.1.2 Bounds for fully-connected layers
In the following propositions in this subsection, we assume W, W0 ∈ RD×C, b, b0 ∈ R, and X ∈
RD×C. Again, these propositions hold for any 1-Lipschitz function σ : R → R such that σ(0) = 0.
But σ = ReLU or id is enough for us.
Proposition 6. It holds that ∣FCW,b(x)∣ ≤ IlW IlOllW k∞∣∣x∣∣∞ + |b| ∙
Proof.
∣FCW,b(x)∣ ≤ ∣vec(W)>vec(x) - b|
≤ |vec(W)>vec(X)| + |b|
≤ X Wα,iXα,i + |b|
The number of non-zero summand in the summation is at most IWI0 and each summand is bounded
by ∣∣W k∞ ∣∣x∣∣∞ Therefore, we have ∣FCW,b(χ)∣ ≤ IlW IlOIlW k∞∣∣x∣∣∞ + ∣∣b∣∣∞.	□
Proposition 7. The Lipschitz constant of FCσW,b is bounded by IWI0IWI∞.
Proof. For any X, X0 ∈ RD×C,
∣FCW,b(x) - FCW,b(x0)∣ ≤ ∣∣(vec(W)>vec(x) - b) - (Vec(W)>vec(x0) - b)∣∣
≤ Ivec(W)> (vec(X) - vec(X0))I
≤ IWIO IWI∞ Ivec(X) - vec(X0)I∞.
□
19
Under review as a conference paper at ICLR 2019
Proposition 8. Itholds that ∣FC^b(x) - FCW，b，(x)l ≤ (kWko + kW0ko)kW - W 1∣∞kx∣∣∞ +
|b-b0|.
Proof.
∣FCW,b(x) - FCW0,b0(x)l ≤ ∣(vec(W)>vec(x) - b) - (Vec(W0)>vec(x) - b0)∣
= |(vec(W - W0)>vec(x) - (b - b0)|
≤ |(vec(W - W0)>vec(x)| + |b - b0|
≤ kW - W0k0kW - W0k∞kxk∞ + |b - b0|
≤ (kWk0 + kW0k0)kW - W0k∞kxk∞ + |b - b0|
□
D.1.3 Bounds for residual blocks
In this section, we denote the architecture of CNNs by C = (C(l))l∈[L] ∈ NL>0 and K =
(K(l) )l∈[L] ∈ NL>0 and the norm constraint on the convolution part by B(conv) (C(0) need not
equal to 1 in this section). Let wl, w0(l) ∈ RK(l) ×Cl ×C(l-1) and b(l), b0(l) ∈ R. We denote
w := (w(l))l∈[L], b := (b(l))l∈[L], w0 := (w0(l))l∈[L], and b := (b(l))l∈[L].
For 1 ≤ l ≤ l0 ≤ L, we denote ρ(l, l0) :
(C (i-1) K (i) B (conv) ).
Qli0=l(C(i-1)K(i)B(conv)) and ρ+(l, l0) := Qli0=l 1 ∨
Proposition 9. Let l ∈ [L]. We assume maxl∈[L] kw(l) k∞ ∨ kb(l) k∞ ≤ B(conv). Then, for any
X ∈ [-1,1]D×C(0), we have ∣∣ConvW[Li],b[Li](x)k∞ ≤ ρ(1,l)∣∣x∣∣∞ + B(Conv)lρ+(1,l).
Proof. We write in shorthand as C[s：t] := Convw[s：t],b[s：t]. Using Proposition 3 recursively, We get
kC["(x)k∞ ≤kLw(T∣opkC[u(x)k∞ + kb叫∞
l	ll
≤ kxk∞ Y kLw(i)kop + X kb(i-1)k∞ Y kLw(j)kop + kb(l)k∞.
By Proposition 2 and assumptions kw(i) k∞ ≤ B(conv) and kb(i) k∞ ≤ B(conv), itis further bounded
by
l	ll
kxk∞ Y(C(i-1)K(i)B(conv)) + B(conv) X Y(C(j-1)K(j)B(conv)) +B(conv)
i=1	i=2 j=i
≤ ρ(1, l)kxk∞ + B(conv)lρ+(1, l)
□
Proposition 10. Let ε > 0, suppose maxl∈[L] kw(l) -w0(l)k∞ ≤ ε and maxl∈[L] kb(l) - b0(l)k∞ ≤
ε ,then ||C[i：l] -。0上勿(x)k∞ ≤ (Lρ(1,L)∣∣x∣∣∞ + (1 ∨ B (Conv))L2 ρ+(1,l))ε forany X ∈ RD×C(0).
Proof. For any l ∈ [L], we have
C[0l+1:L] ◦ (Cl - Cl0) ◦ C[1:l-1](X)
≤ kC0i+i：L]。(Cl- Cl) ◦ C[1：1-1](x)k∞
≤ P(I + 1,L) Il(Cl - C0)。C[Ll-i](x)∣∣∞ (by Proposition 2 and4)
≤ ρ(l+ 1,L) ρ(l, l)kC[1:l-1]k∞ ε + ε (by Proposition 2 and 5)
≤ ρ(l + 1, L) ρ(l, l)(ρ(1, l - 1)kXk∞ + B(Conv)(l - 1)ρ+(1, l - 1)) + 1 ε (by Proposition 9)
20
Under review as a conference paper at ICLR 2019
= ρ(1, L)kxk∞ + (1 ∨ B(conv))lρ+(1, L) ε	(6)
Therefore,
L
kC[1:L] (x) - C[01:L] (x)k∞ ≤ X kC[l+1:L] ◦ (Cl - Cl0) ◦ C[1:l-1] (x)k∞
l=1
≤ (Lρ(1, L)kxk∞ + (1 ∨ B(conv))L2ρ+(1, l))ε
□
D.1.4 Putting them all
LetM ∈ N>0, Lm ∈ N>0, Cm(l), Km(l) ∈ N>0, C := (Cm(l))m,l, andK := (Km(l))m,l form =
0,...,Mandl ∈ [Lm]. Let θ = ((wm(l))m,l, (b(ml))m,l, W, b) and θ0 = ((w0(ml))m,l, (b0(ml))m,l, W 0, b0)
be tuples compatible with (C, K) such that CNNθReLU, CNNθR0eLU ∈ FC(C,KNN,B) (conv),B(fc) for some
S ∈ N>0 and B(conv), B(fc) > 0. We denote the l-th convolution layer of the m-th block by Cm(l)
and the m-th residual block of by Cm :
C(l) := (Conviwdm(l)	(ifl=Lm)
m	ConvRe(lL)U (otherwise)
wm
Cm ：= Cmm。…。cm).
Also, we denote by C[m:m0] the subnetwork of ConvθReLU between the m-th and m0-th block. That
is,
C	J (Cm0	+ I)。…。(Cm +	I)	(if m ≥ I)
C[m：m0] ：=	l(Cm0	+ I)。…。Cm	(if m = 0)
for m, m0 = 0, . . . , M. We define C0(ml), Cm0 and C[0m:m0] similarly for θ0.
Proposition 11. For m = 0, . . . M and x ∈ [-1, 1]D, we have kC[0:m] (x)k∞ ≤ (1 ∨
B(conv))(Qim=0(1+ρi)) 1 + Pim=0 Liρi+ . Here, ρm and ρ+m are constants defined in Theorem
2.
Proof. By using Proposition 9 inductively, we have
kC[0m](x)k∞ ≤ kCm(C[0:m-1](X))+ C[0:m-1] (X) k ∞
≤	k(1 + Pm)C[0:m-1] (x) + B(ConV)Lm ρ+m )k∞
≤	(1+ Pm) kC[0:m-1] (x)k∞ + B(Conv)LmPm
• ∙ ∙
m	mm
≤ kC0(x)k∞ Y(1 + Pi) + B(Conv) X LiPi+ Y (1+Pj)
i=1	i=1	j=i+1
m	mm
≤P0Y(1+Pi)+B(Conv)XLiPi+ Y (1+Pj)
i=1	i=0	j=i+1
mm
≤ (1∨B(Conv)) Y(1 + Pi)	1+XLiPi+ .
□
Lemma 3. Let ε > 0. Suppose θ and θ0 are within distance ε, that is, maxm,l kwm(l) - w0(ml)k∞ ≤ ε,
kb(ml) -b0(ml)k∞ ≤ε, kW - W0k∞ ≤ε,andkb-b0k∞ ≤ε. Then, kCNNθReLU -CNNθR0eLUk∞ ≤
M1 ε where M1 is the function defined in Theorem 2.
21
Under review as a conference paper at ICLR 2019
Proof. For any x ∈ [-1, 1]D, we have
CNNθReLU (x) - CNNθR0eLU
(x) = FCiWd,b ◦ C[0:M] (x) - FCiWd 0,b0 ◦ C[00:M] (x)
=	FCiWd,b - FCiWd 0,b0	◦ C[0:M] (x)
M
+ X FCiWd 0,b0 ◦ C[m+1:M] ◦ (Cm - Cm0 ) ◦ C[00:m-1](x). (7)
m=0
We will bound each term of (7). By Proposition 8 and Proposition 11,
FCiWd,b - FCiWd 0,b0 ◦ C[0:M] (x)
≤ (IlWIlo + IIWOkO)IlW - W0k∞kC[0M(X)k∞ + Ilb- b0k∞
≤ 2C0(L0)DkC[0:M] (x)k∞ε + ε
≤ 2C0(L0)D(1 ∨ B(conv))	YM	(1+ρm)	1+	XM	Lmρ+m	ε+ε
m=0	m=0
≤ 3C0(L0)D(1 ∨ B(conv))	YM	(1+ρm)	1+	XM	Lmρ+m	ε
m=0	m=0
(8)
On the other hand, for m = 0, . . . , M,
FCiWd 0,b0 ◦ C[0m+1:M] ◦ (Cm - Cm0 ) ◦ C[0:m-1](x)
≤ l∣W0IoIW‘^憎馆+®] ◦ (Cm — Cm) ◦ C[Lm-i](x)I∞ (by Proposition7)
≤ CθL0 )°吕出憎情+1网◦ (Cm - Cm) ◦ C[6m-1](X)I∞
≤ COL)DB(fc) ( Y Pi) Il(Cm - Cm) ◦ C[o：m-1](x)∣∣∞ (by Proposition 2 and4)
i=m+1
≤ CL0 )DB(fc) ( Y Pi) (PmkCRm-1] I∞ ε + ε (by Proposition 2 and 5)
i=m+1
M	m-1	m-1
Y Pi	Pm(1∨B(conv))	Y(1+Pi)	1+ X LiPi+ +1 ε
i=m+1	i=o	i=o
(by Proposition 9)
≤ 2Co(L0)DB(fc)(1 ∨ B(conv))	YM(1+Pi)	1+XM LiPi+	ε
(9)
By applying (8) and (9) to (7), we have
। CNNReLU(X) — CNNRoeLU(X)I
≤ 3Co(L0)D(1 ∨ B(conv))	YM (1+Pm)	1+ XM LmP+m ε
m=o	m=o
+ 2MCo(L0)DB(fc)(1 ∨ B(conv))	YM (1+Pm)	1+ XM LmP+m ε
m=o	m=o
≤ (2M + 3)Co(L0)D(1 ∨ B(fc))(1 ∨ B(conv))	YM (1+Pm)	1+ XM LmP+m ε
m=o	m=o
= M1 ε.
□
22
Under review as a conference paper at ICLR 2019
D.1.5 Bounds for covering number of CNNs
For a metric space (M0 , d) and ε > 0, we denote the (external) covering number of M ⊂ M0
by N(ε,M,d): N(ε,M,d) := inf{N ∈ N | ∃f1,...,fN ∈ M0 s.t. ∀f ∈ M,∃n ∈
[N] s.t. d(f, fn) ≤ ε}.
Lemma 4. Let B := B(conv) ∨ B(fc). For ε > 0, we have N(ε, F(CNN),卜 ∣∣∞) ≤ (2BM1 )M2.
Proof. The idea of the proof is same as that of Lemma 12 of Schmidt-Hieber (2017). We divide the
interval of each parameter range ([-B(COnV), B(COnV)] or [-B(fc), Bfc)]) into bins With width M
(i.e., 2B (conv)M1ε-1 or 2B (fc)M1ε-1 bins for each interval). If f, f0 ∈ F (CNN) can be realized by
parameters such that every pair of corresponding parameters are in a same bin, then, ∣f - f0 ∣∞ ≤ ε
by Lemma 3. We make a subset F0 of F (CNN) by picking up every combination of bins for M2
parameters. Then, for each f ∈ F (CNN), there exists f0 ∈ F0 such that ∣f - f0∣∞ ≤ ε. There are
at most 2BM1ε-1 choices of bins for each parameter. Therefore, the cardinality of F0 is at most
(2BMι ) M2	口
D.2 Proof of Theorem 2 and Corollary 1
We use the lemma in Schmidt-Hieber (2017) to bound the estimation error of the clipped ERM
estimator f. Since our problem setting is slightly different from one in the paper, we restate the
statement.
Lemma 5 (cf. Schmidt-Hieber (2017) Lemma 10). Let F be a family of measurable functions from
[-1, 1]D to R. Let f be the clipped ERM estimator of the regression problem described in Section
3.1. Suppose the covering number of F satisfies N(ε, F, ∣ ∙ ∣∞) ≥ 3. Then, ED ∣∣f ◦ 一 f kL2(px) ≤
4 (inff∈Fkf 一 f°∣L2(PX) + (56logN(F, N, k ∙ k∞)+ 180) FN), Where F = RσF ∨ kfσ∞ ∨1
and RF := sup{∣f ∣∞ | f ∈ F}.
Proof. Basically, we convert our problem setting so that it fits to the assumptions of Lemma 10
of Schmidt-Hieber (2017) and apply the lemma to it. For f : [-1, 1]d → [-σF, σF], we define
A[f] ： [O, 1]d → [0,2F] by A[f](x0) := 1 f (2x0-1)+F. Let fι be the (non-clipped) ERM etimator
of F. We define X0 := 1 (X + 1), f。:= A[f ◦], Y0 := f。(X) + ξ0, F = {A[f] | f ∈F}, f :=
A[f1], and D = ((Xn,ynXn∈[N] where Xn := 2(Xn + 1) and yn := f。(Xn + 1 (yn - f ◦(xn)).
Then, the probability that D0 is drawn from P00N is same as the probability that D is drawn from
P 0N where P0 is thejoint distribution of (X0, Y0). Also, we can show that f0 is the ERM estimator
of the regression problem Y0 = f。+ξ0 using the dataset D0: f1 ∈ argmm∕ ,三厂,Rd，(f0). We apply
the Lemma 10 of Schmidt-Hieber (2017) with n J N, d J D, ε J 1, δ J N, ∆n J O, F0 — F,
F J 2F, f J f10 and use the fact that the estimation error of the clipped ERM estimator is no
worse than that of the ERM estimator, that is, ∣f。- f ∣∣L2 (PX) ≤ ∣∣f。- f ι k ：2 (PX) to conclude. □
ProofofTheorem 2. By definition of ∣ ∙ ∣∣∞, we have ∣∣f - f °∣∣L2(Pχ) ≤ ∣∣f - f°∣∣∞ for any
f ∈ F. ByLemma 4, logN := logN(N, F(CNN),∣∣ ∙ ∣∞) ≤ M2 log(2BM1N), where B =
B(COnV) ∨ B(fC). Therefore, by Lemma 5,
∣f◦ - fl%
一.0	,. 一 .F2
f l∣L2(Pχ) + (56logN +180)N
f°∣∞ + MNF log(2BM1N)).
□
23
Under review as a conference paper at ICLR 2019
Proof of Corollay 1. We only care the order with respect to N in the O-notation. Set M = bNαc
for α ≥ 0. Using the assumptions of the corollary, the estimation error is
kf ◦ - fkL2(Pχ) = O (max (N-2αγ1 ,N αγ2-1)) ∙
by Theorem 2. The order of the right hand side with respect to N is minimized when α
By substituting α, we can show Corollary 1.
1
2γι +Y2 .
□
E	Proof of Corollary 2 and Corollary 3
By Theorem 2 of Klusowski & Barron (2016), for each M ∈ N>0, there exists
M	M>
f(FNN) ：= Mm X "x- tm)+ = X bm feX- Q
m=1	m=1	+
With |bm| ≤ 1, kamkι = 1, and |tm| ≤ 1 such that kf °-f(FNN)k∞ ≤ Cv f ◦ √log M + D M- 2 - D1
where C > 0 is a universal constant. We set Lm - 1, Dm) - 1, B(bs) 一 M, Bfin) 一 1
(m ∈ [M]) in the Theorem 1, then, We have f (FNN) ∈ FD(FN,BN()bs),B(fin) . By applying Theorem 1,
there exists a CNN f(CNN) ∈ FC(C,KNN,B) (conv),B(fc) such that f(FNN) = f(CNN). Here, C = (Cm(1))m
with Cm = 4, K = (Km1Jm with Km) = K, B(COnV)=焉,and Bfc) = M. This proves
Corollary 2.
With these evaluations, we have M1 = O(M 3) (note that since B(cOnv) = M, we have QM=0(1 +
ρm) = O(1)). In addition, B(cOnv) is O(1) and B(fc) is O(M). Therefore, we have log M1B =
O(1). Since M2 = O(M), we can use Corollary 1 with γι = ɪ + D, γ2 = 1. Since we have
M = O (N2γ1+γ2 ) by the proof of Corollary 1, we can derive the bounds for B(conv), and B(fc)
with respect to N.
F	Proof of Corollary 4 and Corollary 5
We first prove the scaling property of the FNN class.
Lemma 6. Let M ∈ N>0, Lm ∈ N>0, and Dm(l) ∈ N>0 for m ∈ [M] and l ∈ [Lm]. Let
B(bs), B(fin) > 0. Then, for any k ≥ 1, we have FD(F,NBN(b)s) ,B(fin) ⊂ FD(F,Nk-N1)B(bs) ,kL B(fin) where
L := maxm∈[M] Lm is the maximum depth of the blocks.
Proof. Let θ = ((Wm(l))m,l, (b(ml))m,l, (wm)m, b) be the parameter of an FNN and suppose that
FNNθReLU ∈ FD(F,NBN(b)s),B(fin). We define θ0 :=((W0(ml))m,l,(b0(ml))m,l,(wm0 ), b0) by
w0m ：= k-LmWm)	b0m ：= k-Lm bm	Wm ：= kLwm	bo ：= b.
Since k ≥ 1, we have FNNθR0eLU ∈ FD(FNk-N1)B(bs) kL B(fin) . Also, by the homogeneous property of the
ReLU function (i.e., ReLU(ax) = aReLU(x) for a > 0), we have FNNReLU = FNNReLU.	□
Next, we prove the existence of a block-sparse FNN with constant-width blocks that optimally
approximates a given β-Holder function. It is almost same as the proof of Theorem 5 of Schmidt-
Hieber (2017). However, we need to construct the FNN so that it has a block-sparse structure.
Lemma 7 (cf. Schmidt-Hieber (2017) Theorem 5 ). Let β > 0, M ∈ N>o and f ° : [-1, 1]d → R
be a β -Hoilder function. Then, there exists D0 := O(1) ∈ N>o, L0 := O (log M) > 0 (Ci and
C2 are constants independent of M) and a block-sparse FNN f (FNN) ∈ FDFNNMkfh Such that
Ilf ° — f (FNN)II ∞ = O(M- Dd ). Here, we set Lm := L0 and Dm := D0 for all m ∈ [M] and
l ∈ [Lm] and define D := (Dm(l))m,l.
24
Under review as a conference paper at ICLR 2019
Proof. First, We prove the lemma when the domain of f ◦ is [0,1]D. Let M0 be the largest interger
satisfying (M0 + I)D ≤ M. Let Γ(M0)=(备)D ∩ [0, 1]d = {MT | m0 ∈ {0,..., M0}D} be the
set of lattice points in [0, 1]d3. Note that the cardinality of Γ(M0) is (M0 + 1)D. Let Pef ◦ be the
Taylor expansion of f ◦ UP to order [βC at a ∈ [0, 1]d:
(Paf ◦)(X)= X 物『相-a)"
0≤∣α∣<β
For a ∈ [0, 1]D, we define a hat-shaped fUnction Ha : [0, 1]D → [0, 1] by
D
Ha(x) := Y(M0-1 - |xj - aj|+).
Note that we have Pα∈r(M，)Ha(X) = 1, i.e., they areapartition of unity. Let Pβ f ◦ be the weighted
sUm of the Taylor expansions at lattice points of Γ(M0):
(Pβ f °)(x):= M 0D X (Pβ f °)(x)Hα(x).
a∈D(M 0)
By Lemma 7 of Schmidt-Hieber (2017), we have
kPβf◦ - f°k∞ ≤ kf°kβM0-β.
Let m be an interger specified later and set L := (m + 5)「log? De. By the proof of Lemma 8 of
Schmidt-Hieber (2017), for any a ∈ Γ(M0), there exists an FNN Hata : [0, 1]D → [0, 1] whose
depth and width are at most 2 + L and 6D, respectively and whose parameters have sup-norm 1,
such that
kHata -Hak∞ ≤ 3D2-m.
Next, let B := 2∣∣f °∣∣β and C°,β be the number of distinct D-variate monomials of degree up to
bβc. By the equation (7.11) of Schmidt-Hieber (2017), for any a ∈ Γ(M), there exists an FNN
Qa : [0, 1]d → [0,1] 3 4 whose depth and width are 1 + L and 6DC0,β respectively and whose
parameters have sup-norm 1, such that
Qa -
≤ 3D2-m
Thirdly, by Lemma 4 of Schmidt-Hieber (2017), there exists an FNN Mult : [0, 1]2 → [0, 1], whose
depth and width are m + 4 and 6, respectively and whose parameters have sup-norm 1 such that
|Mult(X, y) - Xy| ≤ 2-m
for any X, y ∈ [0, 1]. For each a ∈ Γ(M0), we combine Hata and Qa using Mult and constitute
a block of the block-sparse FNN corresponding to a ∈ Γ(M) by FCa ：= Mult(Qa(∙), Hata(∙)).
Then, we have
≤ 2-m+ 3D2-m+ 3D2-m
∞
≤ 3D+12-m.
We define f(FNN)(x) := Pa∈r(M)(BMODFCa(X))-B. By construction, f (FNN) isablock-sparse
FNN with (M 0+1)d(≤ M) blocks each of which has depth and width at most L := 2+L* + (m+4)
3Schmidt-Hieber (2017) used D(M 0) to denote this set of lattice points. We used different character to
avoid notational conflict.
4We prepare Qa for each a ∈ Γ(M) as opposed to the original proof of Schmidt-Hieber (2017), in which
Qa’s shared the layers the except the final one and were collectively denoted by Q1.
25
Under review as a conference paper at ICLR 2019
and D0 := 6(CD,β + 1)D, respectively. The norms of the block-sparse part and the finally fully-
connected layer are 1 and BM0D (≤ BM), respectively. In addition, we have
If (FNN)(X)-(P β f-)(x)1
≤ X	BM 0DFCa(X)-	((Pfx)+	1)	Ha (x)	+ B 1 - M 0D	X	Ha(X)
a∈Γ(M)	a∈Γ(M0)
≤ (M0+1)D × BM0D3D+12-m
≤3D+12-mBM2
for any X ∈ [0, 1]D. Therefore,
If (FNN)(X)- f ◦ (x)| ≤ If (FNN)-(Pβ f-)(χ)∣ + I (Pβ f ◦ )(x) - f°(x)∣
≤ B3D+1M22-m + Ilf-kβM-β
≤ 2∣∣f°∣∣β3D+1M22-m + kf-∣∣βM-β2β.
We set m =「log? M2+De e, then, We have L0 = O (log M), D0 = O(1), and
kf(FNN)- f°k ≤ kf°kβ(2 ∙ 3D+1 + 2β)M-β.
By the defnition of f (FNN) We have f (FNN) ∈ FDFNN)产 ∣∣βM.
When the domain of f◦ is [-1,1]D, we should add the function X → ɪ(x + 1) = ɪ(x + 1)+ 一
2(-x 一 1)+ as a first layer of each block to fit the range into [0,1]D. Specifically, suppose the
first layer of m-th block in f (FNN) is X 7→ ReLU(W X - b), then the first tWo layers become
X → ReLU( [ 1 (x + 1) — 1 (x + 1)]) and [yι y2] → ReLU(WyI — Wy2 — b), respectively.
Since this transformation does not change the maximum sup norm of parameters in the block-sparse
and the order of L0 and D0, the resulting FNN is still belongs to FDFNN)产 ∣∣m.	□
Proof of Corollary 4 and Corollary 5. In this proof, We only care the dependence on M in the
O-notation. Let M := 2∣f°∣∣βM. By Lemma 7, there exists f(FNN) ∈ FDFNM SUCh that
kf (FNN) — f-k∞ = O (M- DD ) (L0, D0, and D as in Lemma 7). Let k := 16D0K (M L10 ∧ 1)-1 =
16D0K(eC0 ∧ 1)-1 ≥ 1 where C0 is a constant such that L0 = C0 log M. Using, Lemma 6, there
exists /(FNN) ∈ F(FNNllU / such that/(FNN) = f (FNN). We apply Theorem 1 to F(FNNlIU /
J	D,k-1,kL0 M	J	J	YYJ	D,k-1,kL0 M
and find f(CNN) ∈ FC(C,KNN,B)(conv),B(fc) such that L ≤ M(L0+L0), C := (Cm(l))m=0,...,M,l∈[Lm]
with Cm(l)	≤	4D0,	K :=	(Km(l))m=0,...,M,l∈[Lm]	with	Km(l)	≤ K, B(conv) =	k-1,	B(fc)	=
kL (k ∨ 1)M = kL0+1M, and f (CNN)=『(FNN). By definition, we have B(conv) = k-1 = O(1)
and log B(fc) = (L0 + 1)k + log(MM) = O (log M). This proves Corollary 4.
By the definition of k and the bound on Cmm and Km), we have CmT)Km)k-1 ≤ ɪM-L0. There-
fore, we have ρm ≤ QlL=01(Cm(l-1)Km(l)k-1) ≤ M-1 and hence QmM=0(1 + ρm) = O(1). Since
CmT)Km)k-1 ≤ 2 for sufficiently large M, we have Pm = 1 for sufficiently large M. In addi-
tion, we have Iog(B(COnV) ∨ B(fC)) = O(1). Combining them, we have log M1 = O(1) and hence
log MI(B(COnV) ∨ B(fC)) = O(1). For M2, we can bound it by M2 = O(M log M) using bounds
for Cm), Km) and L0. Therefore, we can apply Corollary 2 with γ1 = D, γ2 = 1 and obtain the
desired estimation error. Since we have M = O (N2γ1⅛) by the proof of Corollary 1, we can
derive the bounds for Lm,, B(conv), and B(fC) with respect to N.	□
G Comparison of our CNNs and original ResNet
There are several differences between the CNN in this paper and the original ResNet, aside from
the number of layers. First and foremost, our CNN does not have pooling nor Batch Normalization
26
Under review as a conference paper at ICLR 2019
(Ioffe & Szegedy (2015)) layers. It is left for future research whether our result can extend to the
ResNet-type CNNs with pooling or Batch Normalization layers. Second, our CNN does not have
ReLU activation after the junction points and the final layer of the 0-th block, while they have in the
original ResNet. We choose this design to make proofs simpler. We can easily extend our results
to the architecture that adds the ReLU activations to those points with slight modifications using
similar techniques appeared in Lemma 2 of the appendix.
27