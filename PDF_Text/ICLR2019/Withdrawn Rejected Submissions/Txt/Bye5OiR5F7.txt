Under review as a conference paper at ICLR 2019
Wasserstein Proximal of GANs
Anonymous authors
Paper under double-blind review
Ab stract
We introduce a new method for training GANs by applying the Wasserstein-2
metric proximal on the generators. This approach is based on the gradient op-
erator induced by optimal transport theory, which connects the geometry of the
sample space and the parameter space in implicit deep generative models. From
this theory, we obtain an easy-to-implement regularizer for the parameter updates.
Our experiments demonstrate that this method improves the speed and stability in
training GANs in terms of Wallclock time and Frechet Inception Distance (FID)
learning curves.
1	Introduction
Generative Adversarial NetWorks (GANs) (GoodfelloW et al., 2014) are a poWerful approach to
learning generative models. Here, a discriminator tries to tell apart the data generated from a real
source and the data generated by a generator, Whereas the generator tries to fool the discriminator.
This adversarial game is formulated as an optimization problem over an implicit generative model
for the generator. An implicit generative model is a parametrized family of functions mapping a
noise source to sample space. In trying to fool the discriminator, the generator should try to recreate
the density distribution from the real source.
The problem of matching a target density can be formulated as the minimization of a discrepancy
measure. The Kullback-Leibler (KL) divergence is known to be difficult when the distributions
have a loW dimensional support set, as is commonly the case in applications With structured data
and high dimensional sample spaces. An alternative approach to define a discrepancy measure
between densities is optimal transport, a.k.a. Wasserstein distance, or Earth Mover’s distance. This
has been used recently to define the loss function for learning generative models (Montavon et al.,
2016; Frogner et al., 2015). In particular, the Wasserstein GAN (Arjovsky et al., 2017) has attracted
much interest in recent years.
Besides defining the loss function, optimal transport can also be used to introduce structures serving
the optimization itself, in terms of the gradient operator. In full probability space, this is known as
the Wasserstein steepest descent flow (Jordan et al., 1998; Otto, 2001). In this paper we derive the
Wasserstein steepest descent flow for deep generative models in GANs. We use the Wasserstein-
2 metric function, which allows us to obtain a Riemannian structure and a corresponding natural
(i.e., Riemannian) gradient. A well known example of a natural gradient is the Fisher-Rao natural
gradient, which is induced by the KL divergence. In learning problems, one often finds that the
natural gradients can offer advantages compared to the Euclidean gradient (Amari, 1998; 2016).
In GANs, because of the low dimensional support sets and the associated difficulties with the KL
divergence, the Fisher-Rao natural gradient is problematic. Therefore, we propose to use the gradient
operator induced by the Wasserstein-2 metric (Li & Montufar, 2018a;b).
We compute the proximal operator for the generators of GANs, where the regularization is the
squared constrained Wasserstein-2 distance. In practice, the constrained distance can be approx-
imated by a simple neural network. In implicit generative models, the constrained Wasserstein-2
metric exhibits a simple structure. We generalize the metric and introduce the relaxed proximal
operator for generators, which allows us to further simplify the computation. The resulting relaxed
proximal operator involves only the difference of outputs, so that the proximal computation has very
simple parameter updates. The method can be easily implemented and used as a drop-in regularizer
for the generator updates.
1
Under review as a conference paper at ICLR 2019
This paper is organized as follows. In Section 2, we briefly introduce the Wasserstein natural gra-
dient. A Wasserstein proximal method is introduced in Algorithm 1. In Section 3, we demonstrate
the effectiveness of the proposed methods in experiments with various types of GANs. Section 4
reviews related work.
2	Wasserstein proximal
In this section, we briefly present optimal transport and its proximal operator on a parameter space.
We then apply them to the optimization problems of GANs.
2.1	Wasserstein natural gradient
Optimal transportation defines a class of distance functions between probability densities. Given a
pair ρ0, ρ1 ∈ Pp(Rn) of probability densities with finite p-th moment,
Wp(ρ0,ρ1)p =inf
/
Rn×Rn
kx - ykpπ(x, y)dxdy,
(1)
where the infimum is over all joint probability densities π(x, y) with marginals ρ0(x), ρ1(y). In the
literature (see Villani, 2009), Wp is referred to as the Wasserstein-p distance. In this paper, we focus
on the case p = 2, and further denote W2 by W.
Following Benamou & Brenier (2000), the Wasserstein-2 distance has a dynamical formulation as
a trajectory transporting the initial density ρ0 to the final density ρ1 along a trajectory of minimal
kinetic energy. The classic theory does not consider the setting where the density path is constrained
to lie within a parametrized model. In the following we extend the classic theory to cover parameter-
ized density models. Consider a parameterized probability ρ(θ, x), with parameter space Θ ⊂ Rd.
Suppose that ρ(θ, x) is locally injective as a mapping from Θ to P2(Rn). Then the Wasserstein-2
metric function constrained to the parameter space is given as follows (See Li & Montufar, 2018a).
Theorem 1	(Constrained Wasserstein-2 metric) The constrained Wasserstein-2 metric function
dW : Θ × Θ → R+ has the following formulation:
dW(θ0,θ1)2 = inf {/ Z ∣∣VΦ(t,x)k2ρ(θ(t),x)dxdt:
∂tρ(θ(t),x) + V ∙ (ρ(θ(t),x)VΦ(t,x)) = 0, θ(0) = θo, θ(1) = θι},
where the infimum is among all feasible Borel potential functions Φ : [0, 1] × Rn → R and contin-
Uous parameter paths θ: [0,1] → Rd. Here V∙ and V are the divergence and gradient operators
over Rn.
We note that the constrained metric on parameter space can be different from the Wasserstein-2
distance on the full density set. The metric dW can be used to define a steepest descent optimization
scheme. This can be formulated in two general ways.
One way is in terms of the corresponding Riemannian structure, i.e., an inner product between
tangent vectors. A well known example is the Fisher natural gradient (Amari, 1998; 2016). The
constrained Wasserstein-2 metric allows us to obtain a Riemannian metric structure, from which
we obtain the following constrained Wasserstein-2 gradient. We also call it Wasserstein natural
gradient.
Theorem 2	(Wasserstein natural gradient) Given a loss function F : Θ → R, the Wasserstein
gradient operator is given by
VθWF (θ) = G(θ)-1VθF (θ),
where G(θ) = (G(θ)ij)1≤i,j≤d ∈ Rd×d is given by
G(θ)ij =
Rn
VΦi (x)VΦj (x)ρ(θ, x)dx.
Herefor each i ∈ {1,…，d}, Φi: Rn → R is a solution (up to additive constants) of 品ρ(θ, x) +
V ∙ (ρ(θ,x)VΦi(x)) = 0.
2
Under review as a conference paper at ICLR 2019
Here VW represents the natural gradient operator with respect to the constrained Wasserstein met-
ric, Vθ represents the ordinary Euclidean gradient operator, and G is the matrix representing the
Wasserstein Riemannian metric. The steepest descent flow is given by
dtθ(t) = -G(θ(t))-1Vθ F (θ(t)).	(2)
The corresponding gradient descent iteration (forward Euler method) satisfies
θk+1= θk - hG(θk)-1VθF (θk),
where h > 0 is the step size. Often in practice, the computation of matrix G(θ)-1 is difficult.
The second way of obtaining a numerical scheme for equation 2 is in terms of the proximal operator.
This is the backward Euler method, also named Jordan-Kinderlehrer-Otto (JKO) scheme (Jordan
et al., 1998), which is given by
θk+1 =argmin F (θ) + ɪ dw (θ,θk )2.	(3)
θ∈Θ	2h
Here, at each step, the distance of the parameter update acts as a regularization to the original loss
function.
Computing dW is also often challenging. However, we can approximate the dW distance locally by a
second order Taylor expansion. This approximation is particularly tractable within the parameterized
setting that we discussed above.
This allows us to derive other first order schemes, such as the Semi-Backward Euler method:
Proposition 3 (Semi-Backward Euler method) The Semi-Backward Euler method for the gradi-
ent flow of loss function F : Θ → R is given by
θk+1 = arg min	F(θ) +	ɪ sup	/	Φ(x)(ρ(θ,x)	— ρ(θk,x))	— ^(VΦ(x))2ρ(θk,x)dx,
θ∈Θ	h Φ	Rn	2
where the supremum is taken over Φ : Rn → R with sufficient regularity for the integral to be well
defined.
The Semi-Backward Euler method is often easier to approximate than the forward Euler method,
because it does not require computing and inverting G(θ), and it is often simpler than the backward
Euler method (JKO), because the constrained optimization over Φ is more tractable than the time-
dependent constraint involved in computing dW .
We implement the Semi-Backward Euler method in implicit generative model as follows. For each
parameter θ ∈ Rd, let the generator be given by gθ : Rm → Rn; z 7→ x = g(θ, z). This takes an
input noise prior Z 〜p(z) ∈ P2(Rm) to an output sample with density given by X = g(θ, Z)〜
ρ(θ, x). Here Rd is the parameter space, Rm is the latent space, and Rn is the sample space.
In this case, the update in Proposition 3 forms
θk+1 = arg min sup F (θ) + 1EZ 〜p(z)[Φ(g(θ,Z)) — Φ(g(θk, Z)) — 1 ∣∣VχΦ(g(θk ,Z))k2].
θ∈Θ Φ	h	2
In practice, we apply a neural network to approximate variable Φ. See details in Appendix G.
2.2	Regularization on generators
In fact, the constrained Wasserstein-2 metric in implicit generative models allows for yet a simpler
formulation. This reformulation allows us to define the relaxed Wasserstein metric, and further
introduces a simple algorithm for proximal operator on generators.
Proposition 4 (Constrained Wasserstein-2 metric in implicit generative models)
1d
dW (θ0, θ1) = inf { J EZ 〜P(Z)Il dt,g(θ(t), Z)k dt:
ddtg(θ(t),Z) -VχΦ(t,g(θ(t),Z))=0, θ(0) = θo, θ(1)= θι},
where the infimum is among all feasible Borel potential functions Φ : [0, 1] × Rn → R and continu-
ous parameter paths θ : [0, 1] → Rd.
3
Under review as a conference paper at ICLR 2019
Here the constrained Wasserstein metric requires that the derivative of the generator g w.r.t. θ ∈ Rd
be a gradient vector field of Φ w.r.t x ∈ Rn. In other words, if we denote x(t) = g(θ(t), Z), then
dtχ(t) = VχΦ(t, χ(t)).	(Gradient constraint)
The gradient constraint is satisfied if the samPle sPace is 1 dimensional, i.e., n = 1. In general, this is
not true. Here Φ(t, x) is the other function dePending on the Parameter sPace Θ. Finding Φ involves
comPutational difficulties. Fitting the gradient constraint is an oPen Problem for the comPutations
of Wasserstein Proximal oPerator.
For simPle comPutations, we withdraw the gradient constraint and consider a relaxed Wassersetin
metric on Parameter sPace:
1d
d(θo,θι)2 = inf { / EZ〜p(z)kdtg(θ(t),Z)k2dt: θ(0) = θo, θ(1) = θι}.
We approximate the relaxed Wasserstein proximal operator based on the new metric d to obtain
θk+1 = argm∈in F (θ) + £ EZ 〜p(z)kg(θ, Z)-g(θk, Z)k2,
(4)
where the infimum is among all feasible continuous parameter path θ : [0, 1] → Rd.
In fact, when the sample space is high dimensional, i.e., n > 1, the above update is not exactly the
Wasserstein proximal. Instead, it simply regularizes the generator by the expectation of squared
difference in sample space.
Algorithm 1 Relaxed Wasserstein Proximal, where Fω is a parameterized function to minimize
Require: Fω , a parameterized function to minimize (e.g. Wasserstein-1 with a parameterized dis-
criminator). gθ the generator.
Require: h proximal step-size, B batch size.
Require: OptimizerF and Optimizerg .
Require: max iterations, and generator iterations
for k = 0 to max iterations do
Sample real data {xi}iB=1 and latent data {zi }iB=1
ωk — OPtimiZerFω (B P3 Fω (gθ (Zi)))
for ` = 0 to generator iterations do
SamPle latent data {zi }iB=1
θk — OPtimize” (-1 PB=IFω(gθ(Zi)) + h∣∣gθ(Zi)- gθk-ι(Zi)k2)
end for
end for
2.3	Illustration of Wasserstein proximal
We present a toy example to illustrate the effectiveness of Wasserstein proximal operator in GANs.
Consider a family of distribution with two weighted delta measures. Let Θ = {θ = (a, b) : a <
0, b > 0}, and define
ρ(θ, x) = αδa(x) + (1- α)δb(x),
where α ∈ [0, 1] is a given ratio and δa(x) is the delta function supported at point a. See Figure 2.3.
In this model, for a loss function F : Θ → R, the proximal regularization is given as follows:
θk+1 = arg min F(θ) + ɪd(θ,θk)2,
θ∈Θ	2h
where θ = (a, b) and θk = (ak, bk). We check the following commonly used statistical distance
(divergence) functions d between parameters θ and θk .
4
Under review as a conference paper at ICLR 2019
α I
a a*
1-α
b* b
0
Figure 1: Illustration of the example from Section 2.3. The Wasserstein proximal penalizes param-
eter steps in proportion to the mass being transported, which results in updates pointing towards
the minimum of the loss function. The Euclidean proximal penalizes all parameters equally, which
results in updates naively orthogonal to the level sets of the loss function.
1.	Wasserstein-2 distance:
dW(θ,θk)2 =α(a-ak)2+(1-α)(b-bk)2;
2.	Euclidean distance:
dE(θ,θk)2 = (a-ak)2+(b-bk)2;
3.	KunbaCk-Leibler divergence:
dκL(pθ ∣∣Pθk) = / ρ(θ,x)log P，，kX)\ dx = ∞;
Rn	ρ(θ , x)
4. L2-distance:
dL2 (ρθ, ρθk)2
=[IPGX)
Rn
- ρ(θk, X)|2dX = ∞.
Here the KL divergence and L2-distance cannot measure the difference of probability models. The
Wasserstein-2 and Euclidean distances still work in these cases. In addition, the Euclidean distance
dE does not depend on the structure of model ρ(θ, X), while the constrained Wasserstein-2 metric
dW does.
Proposition 5 Given θ* = (a*, b*) ∈ Θ, consider the Wasserstein-I metric as the loss function,
i.e.,
Fw1 (θ) = WI(Pθ,pθ*) = α∣a - a*I + (I - α)|b- b*|.
Denote θk+1 = argminθ FWI (θ) + 或dw(θ, θk)2, and θk+1 = argminθ FWI (θ) + 或dE(θ, θk)2.
For each stepsize h > 0, then
FW1(θEk+1) ≥FW1(θWk+1).
On each step of the update, the solution obtained by Wasserstein proximal decreases the objective
function further than the one by Euclidean proximal. Here the proof is based on a simple fact of the
shrinkage operator, see details in Appendix B.
This example introduces a case that Wasserstein-2 proximal works better than Euclidean proximal
for the Wasserstein-1 loss function.
3	Experiments on GANs
Here we present numerical experiments using the Relaxed Wasserstein Proximal (RWP) algorithm
and the Semi-Backward Euler (SBE) method in order to perform Wasserstein gradient-descent on
various GANs. We find that the Relaxed Wasserstein Proximal provides both better speed (measured
by wallclock) and stability in training GANs.
5
Under review as a conference paper at ICLR 2019
3.1	Results of Relaxed Wasserstein Proximal
The Relaxed Wasserstein Proximal (RWP) algorithm is intended to be an easy-to-implement, drop-in
replacement to improve speed and convergence of GAN training. It does this by applying regular-
ization on the generator during training. This is novel as most GAN training focuses on regularizing
the discriminator, e.g. with a gradient penalty (Gulrajani et al., 2017b; Petzka et al., 2017; Kodali
et al., 2018; Adler & Lunz, 2018; Miyato et al., 2018), and there has been limited exploration in reg-
ularizing the generator (Chen et al., 2016). Specifically, we modify the update rule for the generator
by:
•	Update for ` number of iterations before updating the discriminator:
θ J Optimizerg
Originanoss + 2hkgθ - gθk-1k
So two hyperparameters are introduced: the proximal step-size h, and the number of iterations `. In
some GANs, one may update the discriminator a number of times and then update the generator a
number of times, and then repeat; we will call one loop of this update an outer-iteration.
A more detailed description of the algorithm is given in Appendix D.
We test the Relaxed Wassersteing Proximal regularization on three GAN types:
•	Standard GANs (Goodfellow et al., 2014),
•	WGAN-GP (Gulrajani et al., 2017a), and
•	DRAGAN (Kodali et al., 2018).
We use the CIFAR-10 dataset (Krizhevsky, 2009), and the aligned and cropped CelebA dataset (Liu
et al., 2015). And we utilize the DCGAN (Radford et al., 2015) architecture for the discriminator
and generator. To measure the quality of generated samples, We employ the Frechet Inception Dis-
tance (FID) (Heusel et al., 2017) both to measure performance and to measure convergence of GAN
training (loWer FID is better); We used 10,000 generated images to measure the FID. For CIFAR-10,
We measure the FID every 1000 outer-iterations, and for CelebA We measure the FID every 10,000
outer-iterations.
Our particular hyperparameter choices for training are given in Appendix C. Note that since We
intend RWP to be a drop-in regularization, the non-RWP hyperparameters (i.e. not h nor `) are
chosen to Work Well before applying RWP.
To summarize our results, the Relaxed Wasserstein Proximal regularization improves both the speed
(Wallclock) and stability of convergence. It is a tricky to compare the result of using RWP, as it
performs multiple generator iterations. We thus align the comparison according to Wallclock time
(this procedure Was also used by Heusel et al., 2017). In Figure 2 We see that our regularization
improves convergence speed (measured in Wallclock time), and also obtains a loWer FID for all GAN
types. In particular, in DRAGAN We see a20% improvement in sample quality according to the FID.
The same results are also found for the CelebA dataset, shoWn in Figure 3. We note that multiple
generator iterations Will sometimes prevent Standard GANs on CelebA from learning initially, at
Which point We restart the algorithm, and once it starts learning then the run is successful. This is
practically very easy to detect and provides minimal trouble, so Figure 3 focuses on successful runs.
We predict this defect Will be rectified With a more stable loss function, such as WGAN-GP, or With
different h,s and ',s.
We also examine the effect of multiple generator updates compared to discriminator updates. More
specifically, in RWP since We update the generator multiple times before updating the discriminator,
then it is Worth examining the effect of not using the regularization. We see in Figure 4 that even
using the most stable GAN type out of the three - WGAN-GP — if we omit regularization then the
FID has high variance and even tends to rise in the end. But With RWP, the FID converges With more
stability and achieves a lower FID.
Samples from the models are provided in Appendix E. We also performed latent space walks (Rad-
ford et al., 2015) to show RWP regularization does not cause the GAN to memorize. For details see
Appendix F.
6
Under review as a conference paper at ICLR 2019
Walldock (minutes)
Wallclock (minutes)
Figure 2: The effect of using RWP regularization, on the CIFAR-10 dataset. The experiments are
averaged over 5 runs. The bold lines are the average, and the enveloping lines are the minimum
and maximum. From the three graphs, we see that using the easy-to-implement RWP regularization
improves speed as measured by wallclock time, and it also is able to achieve a lower FID.
----------
0505050505
65544<33<221
(α-LL.)ajuulsasxa uosd3uu-13quφ,lL
FID vs Outer-Iterations
80
Figure 3: The effect of Relaxed Figure 4: An experiment demon- Figure 5: The effect of the
Wasserstein Proximal (RWP) strating the effect of perform- Semi-Backward Euler (SBE)
regularization on Standard ing 10 generator iterations per method, on the CIFAR-10
GANs, on the CelebA dataset. outer-iteration with and without dataset. As we observe, the
The experiment was averaged RWP, where an outer-iteration training is comparable to the
over 5 runs. The bold lines are is a single loop of: a num- standard way of training using
the average, and the envelop- ber of discriminator iterations, the WGAN-GP loss. The
ing lines are the minium and then a number of generator it- experiment was averaged over
maximum. Here we see RWP erations. This experiment goes 5 runs. The bold lines is the
regularization improves the to 1,000,000 outer-iterations to average, and the enveloping
speed (via wallclock time), and show long-term behavior. With lines are the minimum and
achieves a lower FID. We note RWP regularization we obtain maximum.
multiple generator iterations convergence, as well as lower
might cause initial learning FID. Without RWP, the training
to fail, but once it starts then is highly variable and the FID is
it remains successful. This is even on a rising trend in the end.
practically easy to detect, so we
show successful runs.
7
Under review as a conference paper at ICLR 2019
3.2	Results of Semi-backward Euler method
The training of Semi-Backward Euler (SBE) is a more complicated. Here we attempt to approximate
three functions: the usual discriminator and generator, and the potential function Φp . The algorithm
and particular hyperparameter settings are presented in the appendix in Section G. We present our
attempts at optimizing over the three networks in Figure 5. Since both the standard WGAN-GP
and the SBE on WGAN-GP had the same generator iterations, then we align according to this.
As we see, the Semi-Backward Euler method is comparable to norm WGAN-GP. We leave deeper
investigation of the Semi-Backward Euler method for future work.
4	Related Works
In the literature, many different aspects of optimal transport have been applied into machine learning
and GANs.
1.	Loss function. Many studies apply the Wasserstein distance as the loss function. There are mainly
two reasons for using the Wasserstein loss function (Frogner et al., 2015; Montavon et al., 2016).
On the one hand, the Wasserstein distance is a statistical distance depending on the metric of the
sample space. So it introduces a statistical estimator, named the the minimal Wasserstein estimator
(Bassetti et al., 2006), depending on the geometry of the data. On the other hand, the Wasser-
stein distance is useful for comparing probability distributions supported on lower dimensional sets.
This is often intractable for other divergence functions. In GANs, these properties have been lever-
aged in Wasserstein GAN (Arjovsky et al., 2017). In this case, the loss function is chosen as the
Wasserstein-1 distance function. In its computations, the discriminator, also called the Kantorovich
dual variable, needs to satisfy the 1-Lipschitz condition. Many studies work on the regularization of
the discriminator in order to satisfy this condition (Gulrajani et al., 2017b; Petzka et al., 2017).
2.	Gradient flows in full probability set. The Wasserstein-2 metric provides a metric tensor structure
(Lott, 2007; Otto, 2001; Li, 2018), under which the probability space forms an infinite dimensional
Riemannian manifold, named the density manifold (Lafferty, 1988). The gradient flow in the den-
sity manifold links with many transport-related partial differential equations (Villani, 2009; Nelson,
1985). A famous example is that the Fokker-Planck equation, the probability transition equation
of Langevin dynamics, is the gradient flow of the KL divergence function. In this perspective,
two angles have been developed in the learning communities. Firstly, many groups try to leverage
the gradient flow structure in probability space supported on the parameter space. They study the
stochastic gradient descent by the transition equation in the probability over parameters (Mei et al.,
2018). Secondly, many nonparametric models have been studied, such as the Stein gradient descent
method (Liu, 2017). It can be viewed as the generalization of Wasserstein gradient flow. In addition,
Frogner & Poggio (2018) consider an approximate inference method for computing Wasserstein
gradient flow in full probability set. Here an approximation towards Kantorovich dual variables is
introduced.
3.	Gradient flow constrained on parameter space. The Wasserstein structure can also be constrained
on parameter space. Carlen & Gangbo (2003) studied the constrained Wasserstein gradient with
fixed mean and variance. Here the density subset is still infinite dimensional. Many approaches also
focus on Gaussian families or elliptical distributions (Takatsu, 2011). The Wasserstein gradient flow
in Gaussian family has been studied by Malago et al. (2018).
Compared to previous works, our approach applies the Wasserstein gradient to work on general
implicit generative models.
5	Discussion
In this work, we apply the constrained Wasserstein gradient and its relaxations on implicit gener-
ative models. Whereas much work has focused on regularizing the discriminator, in this work we
focus on regularizing the generator. For Wasserstein GAN (with gradient penalty), we compute the
Wasserstein-2 gradient flow of Wasserstein-1 distance on parameter space. Experimentally, the pro-
posed method allows us to obtain a better minimizer in the sense of FID, with faster convergence
speeds in wall-clock time.
8
Under review as a conference paper at ICLR 2019
References
J. Adler and S. Lunz. Banach Wasserstein GAN. ArXiv e-prints, June 2018.
S Amari. Natural Gradient Works Efficiently in Learning. Neural Computation, 10(2):251-276,
1998.
S Amari. Information Geometry and Its Applications. Number volume 194 in Applied mathematical
sciences. Springer, Japan, 2016.
LUigi Ambrosio, Nicola Gigli, and Savare Giuseppe. Gradient Flows: In Metric Spaces and in the
Space of Probability Measures. BirkhaUser Basel, Basel, 2005.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein GAN. arXiv:170L07875 [cs,
stat], 2017.
Federico Bassetti, Antonella Bodini, and Eugenio Regazzini. On minimum kantorovich distance
estimators. Statistics & Probability Letters, 76(12):1298 - 1302, 2006. ISSN 0167-7152. doi: ht
tps://doi.org/10.1016/j.spl.2006.02.001. URL http://www.sciencedirect.com/scie
nce/article/pii/S0167715206000381.
Jean-David Benamou and Yann Brenier. A computational fluid mechanics solution to the Monge-
Kantorovich mass transfer problem. Numerische Mathematik, 84(3):375-393, 2000.
E. A. Carlen and W. Gangbo. Constrained Steepest Descent in the 2-Wasserstein Metric. Annals of
Mathematics, 157(3):807-846, 2003.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets.
In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in
Neural Information Processing Systems 29, pp. 2172-2180. Curran Associates, Inc., 2016. URL
http://papers.nips.cc/paper/6399- infogan- interpretable- represent
ation-learning-by-information-maximizing-generative-adversarial
-nets.pdf.
C. Frogner and T. Poggio. Approximate inference with Wasserstein gradient flows. ArXiv e-prints,
June 2018.
Charlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya-Polo, and Tomaso Poggio.
Learning with a Wasserstein Loss. arXiv:1506.05439 [cs, stat], 2015.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling,
C. Cortes, N. D. Lawrence, and K. Q. Weinberger (eds.), Advances in Neural Information Pro-
cessing Systems 27, pp. 2672-2680. Curran Associates, Inc., 2014. URL http://papers.n
ips.cc/paper/5423-generative-adversarial-nets.pdf.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved training of wasserstein gans. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30,
pp. 5767-5777. Curran Associates, Inc., 2017a. URL http://papers.nips.cc/paper
/7159-improved-training-of-wasserstein-gans.pdf.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved training of wasserstein gans. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30,
pp. 5767-5777. Curran Associates, Inc., 2017b. URL http://papers.nips.cc/paper
/7159-improved-training-of-wasserstein-gans.pdf.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Ad-
vances in Neural Information Processing Systems 30, pp. 6626-6637. Curran Associates, Inc.,
2017. URL http://papers.nips.cc/paper/7240-gans-trained-by-a-two-t
ime-scale-update-rule-converge-to-a-local-nash-equilibrium.pdf.
9
Under review as a conference paper at ICLR 2019
Richard Jordan, David Kinderlehrer, and Felix Otto. The Variational Formulation of the Fokker-
Planck Equation. SIAM Journal on Mathematical Analysis, 29(1):1-17,1998.
Naveen Kodali, James Hays, Jacob Abernethy, and Zsolt Kira. On convergence and stability of
GANs, 2018. URL https://openreview.net/forum?id=ryepFJbA-.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University
of Toronto, 2009.
John D. Lafferty. The density manifold and configuration space quantization. Transactions of the
American Mathematical Society, 305(2):699-741, 1988.
J. Lei Ba, J. R. Kiros, and G. E. Hinton. Layer Normalization. ArXiv e-prints, July 2016.
Wuchen Li. Geometry of probability simplex via optimal transport. arXiv:1803.06360 [math],
2018.
WUchen Li and Guido Montufar. Natural gradient via optimal transport. arXiv:1803.07033 [cs,
math], 2018a.
Wuchen Li and Guido MOntUfar. Ricci curvature for parametric statistics via optimal transport.
arXiv:1807.07095 [cs, math, stat], 2018b.
Wuchen Li and Stanley Osher. Constrained dynamical optimal transport and its Lagrangian formu-
lation. arXiv:1807.00937 [math], 2018.
Qiang Liu. Stein Variational Gradient Descent as Gradient Flow. arXiv:1704.07520 [stat], 2017.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of International Conference on Computer Vision (ICCV), December 2015.
John Lott. Some Geometric Calculations on Wasserstein Space. Communications in Mathematical
Physics, 277(2):423-437, 2007.
Luigi Malago, Luigi Montrucchio, and Giovanni Pistone. Wasserstein Riemannian Geometry of
Positive Definite Matrices. arXiv:1801.09269 [math, stat], 2018.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-
layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665-E7671,
2018.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. In International Conference on Learning Representations,
2018. URL https://openreview.net/forum?id=B1QRgziT-.
GregOire Montavon, Klaus-Robert Muller, and Marco Cuturi. Wasserstein Training of Restricted
Boltzmann Machines. InD. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.),
Advances in Neural Information Processing Systems 29, pp. 3718-3726. Curran Associates, Inc.,
2016.
Edward Nelson. Quantum Fluctuations. Princeton series in physics. Princeton University Press,
Princeton, N.J, 1985.
Felix Otto. The geometry of dissipative evolution equations the porous medium equation. Commu-
nications in Partial Differential Equations, 26(1-2):101-174, 2001.
Henning Petzka, Asja Fischer, and Denis Lukovnicov. On the regularization of Wasserstein GANs.
arXiv:1709.08894 [cs, stat], 2017.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. CoRR, abs/1511.06434, 2015. URL http://ar
xiv.org/abs/1511.06434.
Asuka Takatsu. Wasserstein geometry of Gaussian measures. Osaka Journal of Mathematics, 48
(4):1005-1026, 2011.
Cedric Villani. Optimal Transport: Old and New. Number 338 in Grundlehren der mathematischen
Wissenschaften. Springer, Berlin, 2009.
10
Under review as a conference paper at ICLR 2019
Appendix
A Review of Wasserstein statistical manifold
In the full probability set, we consider a metric function W2 : P2 (Rn) × P2 (Rn) → R+,
W2(ρ0,ρ1)2 = inf n [ k ∣∣VΦ(t,x)k2ρ(t,x)dxdt:
Φt	0 Rn
∂tρ(t,x) + V ∙ (ρ(t,x)VΦ(t,x)) = 0, ρ(0,x) = ρo(x), ρ(1,x) = ρι(x)},
(5)
where the infimum is taken among all feasible Borel potential functions Φ : [0, 1] × Rn → R and
continuous density path ρ: [0, 1] × Rn → R+ satisfying the continuity equation.
The variational formulation in equation 5 introduces a Riemannian structure in density space. Con-
sider the set of smooth and strictly positive probability densities
P+ = ρ ∈ C∞(Rn): ρ(x) > 0,
ρ(x)dx = 1
Rn
⊂ P2 (Rn).
σ ∈ F :	σ(x)dx = 0 .
Rn
Denote F := C∞(Rn) the set of smooth real valued functions. The tangent space of P+ is given by
TρP+
Given Φ ∈ F and ρ ∈ P+, define
Vφ(x) := -V∙ (ρ(x)VΦ(x)).
Thus Vφ ∈ TρP+. The elliptic operator V ∙ (ρV) identifies the function Φ modulo additive constants
with the tangent vector VΦ of the space of densities.
Given ρ ∈ P+, σi ∈ TρP+, i = 1, 2, define
gρW(σ1,σ2)
/
Rn
(VΦ1(x), VΦ2(x))ρ(x)dx,
where Φi(x) ∈ F/R, such that -V ∙ (ρVΦi) = σi.
The inner product gW endows P+ with a Riemannian metric tensor. In other words, the variational
problem equation 5 is a geometric action energy in (P+, gW ).
Given a loss function F : P+ → R, the Wasserstein gradient operator in (P+, gW) is given as
follows.	gradw F (P) = -V∙(PV [ (ρ)).
δρ(x)
Thus the gradient flow satisfies
dP = -gradwF(P) = V∙ (ρV-^-F(ρ)).
∂t	δρ(x)
More analytical results on the Wasserstein-2 gradient flow are provided in Ambrosio et al. (2005).
We next consider Wasserstein-2 metric and gradient operator constrained on statistical models. A
statistical model is defined by a triplet (Θ, Rn, P). For simple presentation of paper, we assume
Θ ⊂ Rd and P: Θ → P(Rn) is a parameterization function. In this case, P(Θ) ⊂ P(Rn). We
assume that the parameterization map P is locally injective and under suitable regularities. We
define a Riemannian metric g on P(Θ) by pulling back the Wasserstein-2 metric tensor gW.
Definition 6 (Wasserstein statistical manifold) Given θ ∈ Θ and σi ∈ TθΘ, i = 1, 2, we define
gθ(σ1,σ2)
/
Rn
VΦ1 (x)VΦ2 (x)P(θ, x)dx,
where
-V∙ (ρ(θ,x)VΦi(x)) = (Vθρ(θ,x),σi).
Here Vθ P =(晶 ρ(θ, Xy)Id=I ∈ Rd and (∙, ∙) is an Euclidean inner product in Rd.
11
Under review as a conference paper at ICLR 2019
In particular, we denote
gθ (σ, σ) = σTG(θ)σ,
where G(θ) = (G(θ)ij)1≤i,j≤d ∈ Rd×d is the associated metric tensor defined in Theorem 2.
Here we assume that G(θ) is smooth and positive definite, so that (Θ, gθ) forms a smooth Rie-
mannian manifold. In this case, Theorem 2 studies the constrained Wassertein gradient operator in
parameter space.
B Proofs of Wasserstein natural gradient
Proof of Theorem 1 The distance dW can be written into the action function in Wasserstein statistical
manifold. In other words, consider
dw(θ0,θ1)2 = inf n /1 θ(t)TG(θ(t))θ(t): θ(0) = θo, θ(1) = θι}
where the infimum is taken over θ(t) ∈ C1([0, 1], Θ). Following the definition of metric tensor in
definition 6, we have
θ(t)TG(θ(t))θ(t) = ( (VΦ(t,x))2ρ(θ(t),x)dx,
Rn
with Φ(t, x) satisfying
.
∂tρ(θ(t),x)= Vθ ρ(θ(t),x)θ(t) = -V(ρ(θ(t),x)VΦ(t,x)).
We finish the proof.
Proof of Theorem 2 The gradient operator on a Riemannian manifold (Θ, gθ) is defined as follows.
For any σ ∈ TθΘ, then the Riemannian gradient VθWF (θ) ∈ TθΘ satisfies
gθ (σ, VθW F (θ)) = (Vθ F (θ), σ).
In other words,
θTG(θ)VW F (θ) = Vθ F (θ)Tσ.
Since θ ⊂ Rd and G(θ) is positive definite, then
VθW F (θ) = G(θ)-1VθF(θ).
Proof of Proposition 3. We next present the derivation of the proposed semi-backward method.
Claim: Denote kθ - θk k = h, then
(θk - θ)TG(θk)(θk -θ) = dW (θ, θk)2 + O(h2),	(6)
and
1(θk — θ)TG(θk)(θk — θ) + O(h2) =SUp I Φ(x)(ρ(θ,x) — ρ(θk,x)) — 1 ∣∣VΦ(x)k2ρ(θk,x)dx.
2	Φ Rn	2
(7)
ProofofClaim. We next prove the claim. Denote the geodesic Path θ* (t), t ∈ [0,1], With θ* (0) = θ,
θ*(1) = θk, s.t.
dw(θ,θk)2 = ( (=θ*(t))TG(θ*(t))=θ*(t)dt.
0 dt	dt
We reparameterize the time of θ* (t) into the time interval [0,h]. Denote T = ht and θ(τ) = θ*(ht).
Thus θ(τ) = θk + θ-h∣k-τ + O(τ2) and *θ(τ) = θ-θk + O(T),
dw (θ, θk)2 =h
I -dθ(τ)TG(θ(τ))-dθ(τ)dτ
0 dT	dT
=h
Γh θ - θk
J0 (
θ	θk
+ O(h))T G(θk + O(h))( -—— + O(h))dτ
h
=( - k)TG(k)( - k) + O(h2),
12
Under review as a conference paper at ICLR 2019
which proves equation 6.
We next prove equation 7. On the L.H.S. of equation 7,
Vθρ(θk, x)(θ - θk) = ρ(θ, x) - ρ(θk, x) + O(h).
From the definition of G(θ),
1(θ - θk)TG(θk)(θ - θk) = 1 Z (VΦ(x))2ρ(θk,x)dx,
2	2 Rn
where
-V ∙ (ρ(θk, x)VΦ(x)) = Vθρ(θk,x)(θ - θk) = ρ(θk,x) + O(h).
On the R.H.S. of equation 7, the maximizer Φ* satisfies
ρ(θ,x) — ρ(θk,x) + V∙ (ρ(θk,x)VΦ*(x)) =0.
Applying equation 8 into the R.H.S. of equation 7, we have
/ Φ*(x)(ρ(θ,x) - ρ(θk,x)) - 1 ∣∣VΦ*(x)k2ρ(θk,x)dx
Rn	2
=/ Φ*(x)[-V∙ (ρ(θk,x)VΦ*(x)] - 1 VΦ*(x)ρ(θk,x)dx
Rn	2
=Z ∣VΦ*(x)k2ρ(θk,x) - 1 ∣∣VΦ*(x)k2ρ(θk,x)dx
Rn	2
=1 Z ∣VΦ*(x)k2ρ(θk,x)dx.
2 Rn
Comparing the L.H.S. and R.H.S. of equation 7, we prove the claim.
From the claim,
(8)
θk+1 =argmin F (θ) + 1 dW (θ,θk )2
gθ∈θ ( ) + h 2
= argminF(θ) + ɪ(θk - θ)TG(θk)(θk - θ) + O(h)
θ∈Θ	2h
=arg min F (θ) + ɪ sup / Φ(x)(ρ(θ, x) — ρ(θk, x)) - -ɪkVΦ(x)k2ρ(θk ,x)dx + O(h).
θ∈Θ	h Φ Rn	2
Thus we derive a consistent numerical method in time, known as the Semi-backward method:
θk+1 = θk - hG(θk)-1VθF (θk+1).
Proof of Proposition 4. This result is proven in Li & Osher (2018). We present it here for the
completion of paper. The implicit model is given by the following push-forward relation. Denote
gθ#p(z) = ρ(θ,x), i.e.,
f(g(θ, z))p(z)dz =	f (x)ρ(θ, x)dx,	for any f ∈ Cc∞(Rn).
Rm	Rn
(9)
Given the gradient constraint
-dg(θ(t),z) = VΦ(t,g(θ(t),z)),
dt
we shall show that the probability density transition equation of g(θ(t), z) satisfies the constrained
continuity equation
∂
∂tρ(θ(t), x) + V ∙ (ρ(θ(t), x)VΦ(t, x)) = 0,	(10)
and
EZ 〜p(z)k -∣g(θ(t),Z )∣2 = [ ∣∣VΦ(t,x))k2ρ(θ(t),x)dx.	(11)
dt	Rn
13
Under review as a conference paper at ICLR 2019
On the one hand, consider f ∈ Cc∞ (Rn), then
士EZ〜p(z)f (g(θ(t), Z)) = -d [ f (g(θ(t),z))p(z)dz
dt	dt
Rm
=-d f f(χ)ρ(θ(t),χ)dχ
dt Rn
∂
=	f (x) MP(θ(t),x)dx,
Rn	∂t
where the second equality holds from the push forward relation in equation 9.
On the other hand, consider
(12)
-d EZ 〜p(z)f(g(θ(t),Z))= Iimn EZ 〜p(z)
dt	∆t→0
f(g(θ(t + ∆t), Z) - f(g(θ(t), Z))
∆t
lim
∆t→0 Rm
f(g(θ(t + ∆t), z)) - f(g(θ(t), z))
∆t
p(z)dz
/
Rm
Z
Rm
Z
Rn
Vf (g(θ(t), Z))ddtg(θ(t),z)p(z)dz
Vf (g(θ(t), z))VΦ(t, g(θ(t),z))p(z)dz
Vf (x)VΦ(t,x)ρ(θ(t),x)dx
(13)
=-I f (x)V ∙ (VΦ(t, x)ρ(θ(t), x))dx,
Rn
where V, V∙ are gradient and divergence operators w.r.t. X ∈ Rn. The second to last equality holds
from the push forward relation equation 9, and the last equality holds using the integration by parts
w.r.t. x. Since equation 12 equals equation 13 for any f ∈ Cc∞(Rn), we prove equation 10.
In addition, by the definition of the push forward operator equation 9, we have
EZ 〜p(z)k dg(θ(t),Z )k2 = I
dt	Rn
=Z
Rn
kVΦ(t,g(θ(t),z))k2p(z)dz
∣∣VΦ(t, x)k2ρ(θ(t), x)dx.
Thus we prove equation 11.
Proof of Proposition 5. This example allows us to compute the proximal operator explicitly. On the
one hand, we compute the Wasserstein proximal operator explicitly:
θk+1 = (aW+1,bW+1) = arg min Fwι (θ) + ɪdw (θ,θk)2
θ	2h
arg min α∣a - a* | + (1 - α) |b - b*| + 3(α∣a - ak |2 + (1 - α)∣b - bk |).
(a,b)	2h
I.e.,
akW+1
Here
arg min |a - a*| + T^-Ia - ak∣2, bW+ι = arg min |b - b*| + ɪlb - bk |2.
a	2h	+1	b	2h
(ak - h
ak+1 = Shrinka* (ak,h) = < ak + h
I a*
Similarly, bkW+1
Shrinkb*(bk, h).
if ak > a* + h;
if ak < a* — h;
otherwise.
On the other hand, we calculate the Euclidean proximal operator explicitly:
θE+1 = (aE+1,bE+1 )=argmin Fwι (θ) + ɪ &e (θ,θk )2
θ	2h
arg min α∣a - a* | + (1 - α) |b - b*| + ɪ(la - ak |2 + |b - bk∣2).
(a,b)	2h
14
Under review as a conference paper at ICLR 2019
I.e.,
aE+1 = arg min ɑ∣a - a" + T^-Ia - ak |2,	bkE+1 = argmin(1 - α)∣b - b* | + ɪ |b - bk |2.
a	2h	b	2h
Here
(ak — αh
aE+1 = Shrinka* (ak, αh) = < ak + αh
I a*
if ak > a* + αh;
if ak < a* — ah;
otherwiSe.
Similarly, bE+1 = Shrinkb*(bk,(1 - α)h).
Here we only need to check that for all poSSible caSeS, FW1 (θEk+1) > FW1 (θWk+1). If ak > a* + h
and bk > b* + h, then
hh
FWI(θw ) =α[(a - a - h) + 2] +(I - α)[(b - b - h) + 2]
=α(ak - a*) + (1 - α)(bk - b*) - 2,
and
FWI (θEE+1) =α[(ak - a* - αh)] + (	-H(I - α)[(bk - b* - αh)] + --X7~-
2h	2h
=α(ak - a*) + (1 - α)(bk - b*) - ∣[α2 + (1 - α)2].
Since α ∈ [0, 1], then α2 + (1 - α)2 ≤ [α + (1 - α)]2 = 1, then FW1 (θWk+1) ≤ FW1 (θEk+1). In
other caSeS, the proof followS Similarly. We finiSh the proof.
C Hyperparameters for Relaxed Wasserstein Proximal
EXPERIMENTS
The following hyperparameter SettingS for the Relaxed WaSSerStein Proximal experimentS in Section
3.1 are:
•	A batch Size of 64 for all experimentS.
•	For CIFAR-10 with WGAN-GP: The Adam optimizer with learning rate 0.0001, β1 = 0.5,
and β2 = 0.9 for both the generator and diScriminator. We uSed a latent Space dimenSion
of 128, h = 0.1, and ` = 10 generator iterationS.
•	For CIFAR-10 with Standard and DRAGAN: The Adam optimizer with learning rate
0.0002, β1 = 0.1, and β2 = 0.999 for both the generator and diScriminator. We uSed
a latent Space dimenSion of 100, h = 0.2, and ` = 5 generator iterationS.
•	For aligned and cropped CelebA with Standard: The Adam optimizer with learning rate
0.0002, β1 = 0.5, and β2 = 0.999 for both the generator and diScriminator. We uSed a
latent Space dimenSion of 100, h = 0.2, and ` = 5 generator iterationS.
D A practical description of the Relaxed Wasserstein Proximal
AS mentioned in Section 3.1, the Relaxed WaSSerStein Proximal iS meant to be an eaSy-to-implement,
drop-in regularization. For inStructional purpoSeS, we take a Specific example to ShowcaSe the al-
gorithm: Relaxed WaSSerStein Proximal on Standard GANS (with non-Saturating gradient for the
generator):
•	Given:
-	A generator gθ, and discriminator Dω,
-	The distance function Fω(gθ) = Ex〜reai[log(Dω(x))] - Ez〜N(o,1)[log(1 -
Dω(gθ(z))],
15
Under review as a conference paper at ICLR 2019
-	Choice of optimizers, Adamω and Adamg,
-	Proximal step-sizes h, and generator iterations ', and
-	Batch size B .
Then the algorithm follows:
1.	Sample real data {xi}iB=1, and latent data {zi}iB=1.
2.	Update the discriminator:
1B	1B
ωk — Adamω ( -B 2log(Dω(Xi)) - B ɪ^log(l - Dω(gθ(Zi)))
3.	Sample latent data {zi}iB=1
4.	Perform Adam gradient descent ` number of times:
θk J Adamg
1B	1B1
BX log(Dω (gθ(Zi)))-B X 2hkgθ (Ziik-I (Zi)k2)
for ` number of times.
5.	Repeat the above until a chosen stopping condition (e.g. maximum number of iterations).
As one can analyze above, the only difference between the standard way of training GANs and using
the Relaxed Wasserstein Proximal, are the kgg(Zi) - ggk-1 (Zi)k22 terms and the number of generator
iterations `. Note that in this paper, we call a single loop of updating a discriminator a number of
times and then updating the generator a number of a time, an outer-iteration.
E Generated samples from the model
In Figure 6, we have samples generated from a Standard GAN with RWP regularization, trained on
the CelebA dataset. The FID of these images was 17.105.
In Figure 7, we have samples generated from WGAN-GP with RWP , trained on the CIFAR-10
dataset. The FID for these images is 38.3.
16
Under review as a conference paper at ICLR 2019
Figure 6: A sample of images generated by RWP regularization on Standard GANs, on CelebA.
Figure 7: A sample of images generated by RWP regularization on WGAN-GP, on CIFAR-10.
17
Under review as a conference paper at ICLR 2019
F Latent space walk
Radford et al. (2015) suggest that walking in the latent space could detect whether a generator was
memorizing. We see in Figure 8 and Figure 9 that we have smooth transitions, so this is not the case
for GANs with RWP regularization.
Figure 8: A latent space walk for a network with RWP regularization on Standard GANs, on CelebA.
As we have smooth transitions, this shows the generator is not overfitting. The latent space walk is
done by interpolating between 4 points in the latent space.
Figure 9: A latent space walk for a network with RWP regularization on WGAN-GP, on CIFAR-10.
As we have smooth transitions, this shows the generator is not overfitting. The latent space walk is
done by interpolating between 4 points in the latent space.
G Algorithm and particular hyperparameters for the
Semi-backward Euler method
The specific hyperparameter settings used for the Semi-Backward Euler (SBE) on WGAN-GP,
trained on CIFAR-10, are:
•	A batch size of 64.
•	The DCGAN architecture for the discriminator and generator. A one-hidden-layer fully-
connected network (a.k.a. MLP) for the potential Φp. We also used layer-normalization
(Lei Ba et al. (2016)) for each layer.
•	We used the Adam optimizer with learning rate 0.0002, β1 = 0.1, and β2 = 0.999 for both
the generator, discriminator, and potential Φp. We used a latent space dimension of 100,
and h = 0.2.
•	Every outer-iteration loop, we updated the discriminator 5 times (as suggested in WGAN-
GP), the generator once, and the potential 5 times. Note an outer-iteration is defined as one
loop of: updating the discriminator a number of times, updating the potential a number of
times, and updating the generator a number of times.
18
Under review as a conference paper at ICLR 2019
Algorithm 2 Semi-backward Euler method, where Fω is a parameterized function to minimize.
Require: Fω , a parameterized function to minimize (e.g. Wasserstein-1 with a parameterized dis-
criminator). gθ the generator. Φp the potential.
Require: h the proximal step-size, m the batch size.
Require: OptimizerF , Optimizerg , and OptimizerΦ
Require: The number of generator iterations and p iterations to do per update.
1:	for k = 0 to max iterations do
2:	Sample real data {xi}iB=1 and latent data {zi}iB=1.
3:	ωk —OPtimiZe%ω (B1 P=凡(gθ(Zi)))
4:	for s = 0 to phi iterations do
5:	SamPle latent data {zi}iB=1
6： Pk —Optimize®(J1 B PBL1 Φp(gθ(Zi))- Φp(gθ- (Zi))- 1VΦp(gθ%-ι (Zi)))
7:	end for
8:	for ` = 0 to generator iterations do
9:	Sample latent data {Zi}iB=1
10： θk J OPtimizergθ (B1 PB=I Fω (gθ (Zi)) + 1 (φp(gθ (Zi))- φp(gθk-1 (Zi )) - 1 vφp(gθk-1 (Zi ))))
11:	end for
12:	end for
19