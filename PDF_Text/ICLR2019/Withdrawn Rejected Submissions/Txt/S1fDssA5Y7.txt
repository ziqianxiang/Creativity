Under review as a conference paper at ICLR 2019
Distributionally Robust Optimization Leads
to Better Generalization: on SGD and Beyond
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we adopt distributionally robust optimization (DRO) (Ben-Tal et al.,
2013) in hope to achieve a better generalization in deep learning tasks. We estab-
lish the generalization guarantees and analyze the localized Rademacher complex-
ity for DRO, and conduct experiments to show that DRO obtains a better perfor-
mance. We reveal the profound connection between SGD and DRO, i.e., selecting
a batch can be viewed as choosing a distribution over the training set. From this
perspective, we prove that SGD is prone to escape from bad stationary points and
small batch SGD outperforms large batch SGD. We give an upper bound for the
robust loss when SGD converges and keeps stable. We propose a novel Weighted
SGD (WSGD) algorithm framework, which assigns high-variance weights to the
data of the current batch. We devise a practical implement of WSGD that can
directly optimize the robust loss. We test our algorithm on CIFAR-10 and CIFAR-
100, and WSGD achieves significant improvements over the conventional SGD.
1 Introduction
In recent years, supervised learning based on deep neural networks (DNNs) has achieved state-of-
the-art performances in various tasks of different domains, such as computer vision and natural
language processing (Goodfellow et al., 2016). However, theoretical explanations for the general-
ization ability of deep learning remain challenging. Roughly speaking, supervised learning aims to
learn a model from the training data. Let Θ denote the parameter space of the model, and Z denote
the sample space. Let `(z, θ) : Z × Θ → R+ denote the loss function related to the parameter θ and
the instance z. In the general setting of supervised learning, there is a fixed but unknown distribution
over Z (denoted P). We attempt to find a θ ∈ Θ, which gives a small generalization error. That is,
we address the following optimization problem:
minR(θ)，EP'(z, θ).
(1)
However, the real distribution P is unknown, so we consider an empirical distribution. Given
the training samples S = (z1,z2,…，zm), the empirical distribution is defined as Pm(Z) =
mm Pm=I I[z=zi], where I is the indicator function. The loss related to this distribution is called
the empirical loss, which is
m
1
R(θ)，Epm'(z,θ) = - £'(&/),	⑵
m i=1
and we minimize the empirical loss instead. This procedure is referred to as Empirical Risk Mini-
mization (ERM).
The overwhelming capacity and the paucity of data make deep learning architectures susceptible to
over-fitting. This urges us to propose a better approach to the minimization problem (1). Suppose
there is a distribution family M = {Pλ : λ ∈ Λ} parametrized by λ, and the underlying distribution
P = Pλ0 ∈ M, where λ0 is fixed but unknown. It is a common case in deep learning tasks that
the structure of the data is complicated while the amount of data is small. Hence, when we use
an estimator λ to estimate the parameter λ0 , the variance of λ would be quite large. Thus, rather
than directly minimizing the true expected loss R(θ) = EPλ `(z, θ), we minimize a distributionally
1
Under review as a conference paper at ICLR 2019
robust loss alternatively, which can ensure the true expected loss to be small with high probability.
The distributionally robust loss is defined as:
R(θ,K)，sup Epλ'(z,θ),
λ∈K
where K ⊂ Λ is a neighbourhood of the estimator λ. This procedure is called Distributionally
Robust Optimization (DRO). Because we only have access to the given training dataset, we construct
an unbiased1 estimation of the expected loss Epλ'(z, θ):
ɪ X修…
m i=1 Pλ0 (zi)
This can be viewed as the expected loss with respect to a perturbed empirical generalized distribu-
tion2 pλ = m Pm=I Ppλ((z)) I[z=zi] , so the DRO problem becomes minimizing the empirical distri-
butionally robust loss:
R(θ,K)，sup Ep '(z,θ) = sup X Pλ(zi)、'(zi,θ).
λ∈K Pλ	λ∈K i=1 mPλ0 (zi)
Keeping the motivations above in mind, we focus our attention on the robust loss, defined as the
maximal possible weighted loss over a pre-specified weight set P for simplicity:
m
Rs,P(θ)，sup XPi'(zi, θ).
p∈P i=1
In deep learning, the high capacity of the model and the non-convexity of the objective function
lead to an extremely complicated landscape and induce a lot of minima, and different minima have
different generalization performances. In this case, an optimization algorithm is both an optimizer
and a minimum selector, and the minimum preference of the optimization algorithm is crucial in
deep learning. Stochastic Gradient Descent (SGD) and its variants are widely used to minimize the
empirical loss (2). The simplest mini-batch SGD consists of iterative sampling of a batch of data
and then updating the parameter:
θt+ι = θt - ηt(~τp∏ X Re'(Zi,θ/),
|Bt | zi∈Bt
where Bt ⊂ S is the sampled mini-batch, ηt is the learning rate, and |Bt | denotes the batch size.
We interpret SGD from another perspective: at each iteration, SGD randomly chooses an empirical
distribution over the data and performs a gradient descent update w.r.t. the corresponding loss. This
interpretation shows a profound connection between SGD and DRO. Motivated by the connection,
we develop a brand new version of SGD that can achieve a better generalization performance. The
main contribution of this paper is summarized as follows:
•	We clarify the connection between mini-batch SGD and DRO. We show that SGD helps
escape from bad stationary points and the robust loss of local minima where SGD converges
is not large.
•	We analyze the generalization property of robust loss and show the trade off: the robust
loss generalizes well when P is relatively small, but a larger P is beneficial to reducing the
localized Rademacher complexity.
•	We propose a framework of improved SGD algorithm, which enables the algorithm to
explore more empirical distributions over the data. We give a practical improved SGD
algorithm that can directly optimize the robust loss.
1To make the fraction reasonable, we assume Pλ0 (z) > 0 whenever Pλ(z) > 0.
2
2Strictly speaking, this is not a distribution because Pλ(Z) may not equals to 1, but its expectation is 1 (the
expectation is taken over the sample S). Nonetheless, we can view Pbλ as a measure over Z .
2
Under review as a conference paper at ICLR 2019
2	Related Work
SGD is widely used in large-scale machine learning problems. The convergence theory of SGD in
convex cases is solid (Ghadimi & Lan, 2013; Shamir & Zhang, 2013). However, due to the non-
convexity of deep learning models, the behaviour of SGD is more complicated. Dauphin et al. (2014)
argued that it is the prevalence of saddle points rather than local minima that causes the difficulty
in high dimensional non-convex optimization. Ge et al. (2015) analyzed the property of perturbed
GD (i.e., gradient descent with an isotropic noise injected). They showed that perturbed GD has the
ability to escape from saddle points in a polynomial number of iterations, and then converges to a
second-order stationary point.
It is also well known that adding perturbation and noise at different levels of the training processes
helps generalization, such as dropout (Srivastava et al., 2014), and random noise injection to the
input data (Chapelle et al., 2001). The good performance of SGD might benefit from the noise.
Keskar et al. (2016) discovered that large-batch training will incur a degradation in the generalization
performance of the model. They explained this phenomenon with the concept of sharp minima,
and carried out numerical experiments to support the idea that SGD with large batch size tends to
converge to sharp minima, which leads to a bad generalization ability. However, Dinh et al. (2017)
provided theoretical results indicating that most existing definitions of sharpness and flatness of
minima can not be applied to some commonly used non-negative homogeneous activation functions
such as ReLU.
DRO was originally applied to decision making under uncertainty. Namkoong & Duchi (2017)
introduced DRO to machine learning, and viewed it as a variance-based regularization that allows
Us to automatically trade-off between bias and variance. They proved an O(ml) approximation to
the generalization error R(θ),in contrast to the general O( √=) approximation of the empirical risk.
Recently, DRO is used to achieve fairness without demographic information in fair machine learning
(Hashimoto et al., 2018).
In learning theory, Rademacher complexity is widely used to measure the complexity ofa hypothesis
set (Bartlett & Mendelson, 2002). Different from the general Rademacher complexity which con-
siders the entire hypothesis set, localized Rademacher complexity only cares about the near optimal
hypothesis (Bartlett et al., 2002). In fact, the hypothesis selected by a learning algorithm usually has
better performance than the worst hypothesis. Thus, localized Rademacher complexity is usually
much smaller than the general Rademacher complexity.
3	Notation and Preliminaries
In our work, we assume the loss function `(z, θ) is bounded between 0 and 1, and is continu-
ously differentiable. Let Σ denote the hyperplane {p : Pim=1 pi = 1} and Σ+ denote the simplex
{p ：	Pm=Ipi =	1, P ≥ 0}.	Denote po	=	(mm,...,	m1 )>.	For P =	(p1,p2,... ,pm)>	∈ ∑, let
Rs,p(Θ) denote Pm=I pi'(zi, θ). Then the ERM loss can be written as Rs,p0 (θ), and the robust loss
w.r.t. the weight set P can be viewed as:
^ , _. ^ , _,
RbS,P (θ) = sup RbS,p(θ).
p∈P
Let conv(P) denote the convex hull ofP. Given a v ∈ Σ, let σ(v) be the set of all the vectors whose
entries are permutations of v’s. Let P(v) denote the convex hull of σ(v).
We define P(k) as the set of all the vectors that have k 1 's and (m - k) 0's:
m
P(k) = {p = kV : V ∈ {0,1}m , X Vi = k}.
i=1
Clearly, P(k) has a natural connection to SGD with batch size k: for any batch sampled from the
dataset, there is a corresponding P ∈ P(k), where p = 1 if the i-th instance is in the batch.
In the remainder of the paper, we assume that P is symmetric, i.e., if P ∈ P, then σ(P) ⊂ P. This
follows that P0 ∈ conv(P). And we assume that P is a compact set. We assume the entries of
3
Under review as a conference paper at ICLR 2019
p ∈ P sums to 1, but we relax the restrict that each entry ofp ∈ P is non-negative, i.e., we assume
P⊂Σ.
To describe the properties of the set P, we use the notation RADp(P) to denote the minimum
radius of the Lp ball centered at p0 that contains P, and we use radp(P) to denote the maximum
radius of the Lp ball centered at p0 whose intersection with Σ is contained within P . We denote
kPkp = supp∈P kpkp, and kPk0 as the maximum number of non-zero entries ofp ∈ P.
Definition 1. A function f : Θ → R is L-Lipschitz, if:
∣f(θι)- f(θ2)l≤ Lkθι- θ2k,∀θ1,θ2 ∈ Θ.
Definition 2. Afunction f : Θ → R is μ-strongly convex if f (θ) — 2kθ∣∣2 is convex.
Some basic properties of the robust loss RS,P (θ) are summarized in Appendix A.
4	Connection between SGD and DRO
SGD has long been regarded as an excellent optimizer. It is generally believed that the good per-
formance of SGD is due to the noise introduced by stochastic gradient. But the existing results
commonly view stochastic gradient as an oracle that provides an unbiased estimation of the true
gradient, ignoring the fact that the noise is data-dependent. In this section, we directly analyze the
good properties of mini-batch SGD from the distributionally robust perspective.
At each iteration, SGD calculates the gradient of RS,p(θ), where p is the weight vector correspond-
ing to the current batch, and performs the update θ J θ — η^Rs,p(θ). This can be viewed as
changing the distribution over the data. Thus SGD can improve the distributional robustness of the
model. The following theorem shows that changing the distribution helps mini-batch SGD escape
from some bad stationary points, which is characterized by the heterogeneity of gradients of each
individual instance.
Theorem 1. Let S be a fixed sample of size m. For a fixed θ ∈ Θ, define a matrix G =
(Vθ '(zι,θ),…，Vθ '(zm,θ)), whose i-th column is the gradient of the i -th loss function w.r.t. θ.
For SGD with batch size k, denote p as the random weight vector uniformly sampled from P (k).
Then the expected squared update amount can be calculated by:
EkVRS,p(θ)k2 = kvRs,po(θ)k2 + km-k (tr(G>G) - kvRs,p0(θ)∣∣2).
p	k(m— 1) m
When	G)》kvRs,po (θ) ∣∣2, the gradient of the ERM loss is relatively small, but the average
squared length of the gradients gι,…，gm is large. This happens only if those gradients lie in some
opposite directions and cancel with each other. In this case, if we leave out the instance with the
largest gradient norm, then the average gradient of the rest of the sample would be almost as large as
the mm of the largest gradient norm, and the local landscape of the average loss function would shift
dramatically. This indicates the model pays too much attention to this specific instance and fails to
learn the intrinsic structure of the data, which can be viewed as a signal of over-fitting. We call this
a bad stationary point. So the scale of tr(GmG) and ∣VRs,po (θ)∣2 captures the over-fitting tendency
of a model by measuring the heterogeneity of gradients of the individual instance.
Furthermore, we can know from Theorem 1 that for any fixed θ, EkVRbS,p0 (θ)k22 decreases mono-
p
tonically when k becomes larger, indicating the update amount of small batch SGD is larger than
that of large batch SGD in expectation. Therefore, small batch SGD has a stronger ability to capture
the non-uniformity of gradients and escape from bad stationary points.
Intuitively, if SGD converges and keeps stable when the distribution changes, the weighted average
loss RS,p will not be too large, otherwise SGD has the tendency to escape. To theoretically prove
it, we need to focus our attention to some simple case. If SGD has escaped from bad stationary
points and the deep learning model really captures some intrinsic structure of the data, then it is
conceivable that a batch of the instances Bt is able to reflect some common features of the entire
sample S, so the local landscape of the average loss function will not shift too much when we only
consider a batch of the data. In this case, we have the following theorem.
4
Under review as a conference paper at ICLR 2019
Theorem 2.	Assume the loss function is L-Lipschitz. Suppose the SGD algorithm with constant
learning rate η converges to and stays in B, where B = {θ : kθ - θ0k2 ≤ B} is a ball that contains
■ "	r I' Ji Λ Λ 1 τ~i	C	.ι . ∖ / — ∕T> / 7 ∖ 1Γ)	.	ι ι	r τ~i	ι
a minimum of ERM loss RS,p0. Suppose that ∀p ∈ P (k), B contains a local minimum of RS,p and
^
Rs,p is μ-strongly convex on B. Further assume that the minimum value of the batch average loss
D
RS,p
in B is not too large, i.e., min RS,p(θ) ≤ min RS,p0 (θ) + δ. Then the robust loss RS,P(k) (θ)
can be bounded by:
RS,P(k) (θ) ≤ θm0∈inB RS,p0
2B2
(θ0) + r + 2LB + δ, ∀θ ∈ B.
μη2
5 Theoretical Analysis
5.1 Generalization Guarantee of DRO
In this subsection, we introduce the concept of Robust Rademacher Complexity and use it to derive
the generalization guarantee for robust loss, which provides justifications for DRO. We bound the
expectation of robust loss by empirical robust loss plus a complexity term with high probability.
Then we utilize the notion of covering number to show that the generalization of robust loss is not
too hard compared to that of ERM loss.
Definition 3 (Empirical Robust Rademacher Complexity). Let S = (zι,…，Zm) be a fixed sample
of size m. The empirical robust Rademacher complexity of the parameter space Θ and the weight
set P w.r.t. the sample S is:
m
RS(Θ, P) = E sup sup T σiPi'(zi,θ),
σ θ∈Θ p∈P i=1
where σ = (σ1, . . . , σm)> is a random vector with independent entries uniformly from {-1, 1}.
Theorem 3.	For any δ > 0, with probability at least 1 - δ, the following holds for all θ ∈ Θ:
2
ERso,P(θ) ≤ Rs,p(θ) + 2Rs(Θ, P) + 3m∣∣P∣∣∞ ʌ/赤 log $.
For a fixed sample S of size m, denote Θ(S)
{('(zι,θ),∙∙∙ ,'(zm,θ))> : θ ∈ θ} and
Θ(S,P) = {(mpι'(zι, θ),…，mpm'(zm, θ))> : θ ∈ Θ,P ∈ P}. For a norm ∣∣ ∙ k in Rm, de-
note the dual norm by k ∙ k*. For e > 0, let N(A, ∣ ∙ ∣, E) denote the covering number of the set
A ∈ Rm, which is the minimum cardinality of the e-net w.r.t. the norm ∣ ∙ ∣. Denote by d*(m) an
upper bound of ∣σ∣* = k(σι,…，σm)>∣*.
Theorem 4.	For any e > 0, the empirical Rademacher complexity and the empirical robust
Rademacher complexity can be respectively bounded by:
RS (θ)<∖ ∕2l°g N®S ),∣H∣,e) +
m
ed* (m)
m
and
RS(θ, P) ≤ (1 + RAD2(P))J2logN(θ(s,P),k ∙ k,e) + ed*(m).
mm
(3)
(4)
^
^
^
^
^
Notice that θ(S, P) is generated from θ(S) by having all the points perturbed a little bit, so itis con-
ceivable that these two sets are close, implying that their covering numbers are close. Specifically,
if we define
T	C Il	Il
d =	sup inf kx - yk
x∈Θ(S,P) y∈Θ(S)
as the maximum distance of a point in θ(S, P) deviating from the set θ(S), then it follows that an
e-net of θ(S) is also an (e + d)-net of θ(S, P). To compare the right-hand sides of the covering
5
Under review as a conference paper at ICLR 2019
number guarantees (3) and (4), we change to + d in (4), and we have:
(1 + RAD2 (P ))J2l°g nθs P 川工三 +
m
/	. S∖ j /	∖
(E + d)d*(m)
m
≤ (1 + RAD2(P))J2logN(Q(S)JHg + EdKm) + dl*(m.
m	mm
When We use the Lp norm, one can easily show that d ≤ mRADp(P), and kσ∣∣* can also be
bounded as kσ∣∣* ≤ mq = mP-λ, so d*(m) = m^-ɪ. Then the extra term 前*厂 can be bounded
by m1-PRADp(P).
Those bounds quantify how the size of P will influence the generalization ability, and give a suffi-
cient condition on the size of P to ensure that the covering number bounds are close. Conceivably,
when P is sufficiently small, the generalization of robust loss is not too difficult. It is worth noticing
.,,	...... ∙≈r	. ,	, ................................ _	,
that the inequality N(Θ(S, P), k∙k, e + d) ≤ N(Θ(S), k ∙ k, E) is quite loose when the set Θ(S) is
some sort of “solid,” and this is often the case in deep neural networks, whose capacities are thought
to be very high. In one word, it is completely feasible to consider DRO when using DNNs.
5.2 Localized Rademacher Complexity
Next, we analyze the localized Rademacher complexity based on robust loss. Localized Rademacher
complexity measures the complexity of near optimal hypotheses, which is much smaller than the
entire hypothesis space. Here we assume the hypothesis set is restricted to a L2 norm ball.3
DRO can be viewed as a variance-based regularization technique (Namkoong & Duchi, 2017). A
low robust loss means a low variance of the losses ('(zι,θ),…，'(zm, θ)), since otherwise one
can choose a p ∈ P that puts more weight on the instances with a large loss, and obtains a high
robust loss. A larger size of P means a more radical way to assign weights, indicating that DRO
with large P renders a stricter regularization on the variance of the losses. Rademacher complexity
measures the ability of a hypothesis set to fit high variance random noises, and naturally a low
variance of the losses means a small Rademacher complexity. The following theorem reveals the
connection between Rademacher complexity and DRO, and provides a model-free bound on the
localized empirical Rademacher complexity.
Theorem 5. Assume the loss function is L-Lipschitz w.r.t. θ for all z. Consider the following hy-
pothesis set:
Θc = {θ : ESRbs,p(θ) ≤ c,∣∣θ∣∣2 <r}.
For any E > 0, let N(Θc, E) denote the covering number of Θc. Then for all 0 < δ < 1, with
probability ≥ 1 - δ, the localized empirical Rademacher complexity can be bounded as:
RS (Qc) ≤ √(1 + ~i=~~x~~7----------∩^γc ) (C + EkPkO kPk∞ L+kPkg' / 3 lθg " ( :c，)).
m	mrad∞ (conv(P))	2	δ
Conceivably, when m and c are fixed, increasing the size ofP reduces the set Θc. This bound shows
that increasing the size of P also reduces the complexity of Θc under certain conditions. For exam-
ple, let P be the intersection of Σ and a L∞ ball with radius ρ centered at p0 . When ρ is relatively
small compared to *,increasing P does not change ∣∣P∣∣∞ too much, but (Conv(P))is inversely
proportional to ρ. Therefore, increasing P can reduce the scale of kPon∞(p)) dramatically.
6 Weighted SGD
Motivated by the DRO and the previous theoretical analyses, we propose a new variant of SGD
algorithms in Algorithm 1. We will see that this variant is simple and practical for applications.
3 When we train models with a constant weight decay λwd, generally the L2 norm of final parameters will
not excess ʌ/f (0)∕λwd, where f is the loss function. This is because when the algorithm minimizing L(θ)=
f(θ) + λwdkθk22 outputs θ0, we generally haveL(θ0) ≤ L(0), hence, λwdkθ0k22 ≤ f(θ0) + λwdkθ0k22 ≤ f(0).
6
Under review as a conference paper at ICLR 2019
We have emphasized the significance of distributional robustness before. The critical factors that
influence distributional robustness are the size of P and the value of the corresponding robust loss
RS,P (θ). In order to obtain a more distributionally robust model, a direct approach is to increase the
size of P. In the conventional SGD, the weight of the data in the current batch is 1, and the other is
0. Due to the limited computational resources, we still only have access to one mini-batch at each
iteration, but we can assign high-variance weights to the instances of the mini-batch. This implies
that our algorithm enjoys the idea behind importance sampling. Therefore, our algorithm explores a
wider region of empirical distributions, yielding a more distributionally robust solution.
Algorithm 1 Weighted Stochastic Gradient Descent (WSGD)
Require: Initial parameter: θ0, Weight Generator: G, Learning Rate: ηt, Total Iteration: T, Train-
ing Data: S = {z1, z2 , . . . , zm}.
1:	for t = 1 to T do
2:	Select a mini-batch: Bt = {zj1 , zj2 , . . . , zj|B | }
3:	Generate weight: (w1,w2,... ,w∣Bt∣) J G
4:	Calculate normalization factor: W = P|iB=t1| wi
5:	Calculate stochastic gradient: gt =需 PiBI w"θ'(Zji ,θt-1)
6:	Update parameter: θt = θt-1 - ηtgt
7:	end for
8:	return θT
We turn to the choices of the weight generator. In particular, we recommend two approaches:
•	G1(q, r): wi = r if `(zji, θt-1) is among the q|Bt| smallest elements of the losses
('(Zji ,θt-ι),…，'(ZjBt ,θt-ι)), otherwise Wi = 1;
•	G2(q, r): Randomly select q|Bt| of the indices {ji} and set their weights to r. Set the
others to 1,
where q ∈ [0, 1] is the proportion of the instances whose weight is set to r, and r is a pre-specified
constant. Notice that r is not necessarily non-negative.
We have conducted experiments to show that Algorithm 1 with generator G1 achieves a better per-
formance on classification tasks than the conventional SGD (see Section 7). We refer to Algorithm
1 with weight generator G1(q, r) as WSGD(q, r).
Different from the conventional SGD, WSGD(q, r) directly optimizes a robust loss. It is easy to
prove that there exists a vector v(q, r) ∈ Σ such that the stochastic gradient in WSGD(q, r) is an
unbiased estimation of the gradient of RS,P(v(q,r)) (θ) at differentiable points (see Appendix C for
details). Furthermore, the stochastic gradient in WSGD(q, r) is bounded by the Lipshitz constant
of ` when r is non-negative. Taking advantages of previous work, we can give the convergence
guarantee for WSGD(q, r) with a non-negative r in the strongly convex case:
Corollary 1 (Shamir & Zhang 2013). Suppose the loss function '(z, θ) is μ-Strongly convex and
1
L-Lipshitz. Consider WSGD(q, r) with step size η = ~1 and r is non-negative. Let RS P『))be
the optimal value of RS,P(v(q,r))(θ). Then for all T > 1, it holds that:
E[RS,P (v(q,r))
(θT) - RS,P(v(q,r))] ≤
17L2(1+log(T))
μT
It is worth pointing out that G2 usually yields a lower test loss in our trial. Strictly speaking, this
means G2 has a stronger generalization ability. However, the loss we choose in classification tasks
is different from the true loss (0-1 loss), thus the algorithm with alow test loss not always has a low
prediction error. We still recommend G2 because it may achieve a good performance on tasks which
we can deal with the true loss directly.
7
Under review as a conference paper at ICLR 2019
7	Experiments
In this section, we compare WSGD with the conventional stochastic gradient descent, which is
regarded as the optimizer with the best generalization. We carry out the experiment on the CIFAR-
10 and CIFAR-100 datasets (Krizhevsky & Hinton, 2009), which both have 50k training data and
10k test data. On CIFAR-10, we apply our algorithm and SGD to two small networks and two large
networks: ResNet-44, ResNet-56, VGG-16 and ResNet-34 (He et al., 2016; Simonyan & Zisserman,
2014). On CIFAR-100, we apply our algorithm and SGD to three large networks: VGG-16, ResNet-
34 and DenseNet-121 (Huang et al., 2017). We train the networks following the data augmentation
in Lee et al. (2015): 4 pixels are padded on each side, and a 32 × 32 crop is randomly sampled
from the padded image or its horizontal flip. We only do evaluation on the original 32 × 32 image
while testing. In order to compare the generalization ability of optimization algorithms, we train
the models for 600 epochs to ensure convergence. We set the initial learning rate to 0.1, and divide
it by 10 at 300 and 450 epochs. The choice of hyper-parameters in WSGD(q, r) is listed in the
experiment results. Empirically speaking, the hyper-parameters are mainly related to the dataset and
network architecture, and a smaller r is suitable for larger networks.
We summarize the experiment results on CIFAR-10 and CIFAR-100 in Table 1. For each network
and algorithm, we report the test accuracy, the L2 ball and L∞ ball robust loss. Lp ball robust loss is
the robust loss with P as the intersection of Σ+ and the Lp ball centered atp0 with radius radp(Σ+).
In other words, we consider the largest Lp ball in Σ centered p0 that has positive entries.
From table 1 we can see that WSGD outperforms SGD in all experiments. On CIFAR-10, WSGD
decreases the misclassification rate by ≈ 5‰ except VGG-16. On CIFAR-100, WSGD decreases the
misclassification rate by 5‰ to 15‰ . Moreover, the models trained by WSGD have a lower L2 ball
and L∞ ball robust loss than SGD. The good performance shows WSGD improves the distributional
robustness of the models.
Table 1: Top 1 accuracy(%) and robust loss(×10-3) on CIFAR 10 and CIFAR 100
Network	Params	Acc	L2-ball	L∞-ball	Method	Dataset
ResNet-44	0.66M	93.60	0.5902	1.116	SGD	CIFAR-10
ResNet-44	0.66M	94.24	0.4892	0.9507	WSGD(0.5,0.3)	CIFAR-10
ResNet-56	0.86M	93.86	0.3762	0.6745	SGD	CIFAR-10
ResNet-56	0.86M	94.33	0.3070	0.5835	WSGD(0.5,0.4)	CIFAR-10
VGG-16	14.74M	94.05	0.9617	1.112	SGD	CIFAR-10
VGG-16	14.74M	94.30	0.8807	1.004	WSGD(0.5,0.2)	CIFAR-10
ResNet-34	21.30M	95.13	0.7113	0.9379	SGD	CIFAR-10
ResNet-34	21.30M	95.65	0.6422	0.8381	WSGD(0.5,0)	CIFAR-10
VGG-16	14.78M	72.13	1.535^^	2.942	SGD	CIFAR-100
VGG-16	14.78M	73.60	1.209	2.346	WSGD(0.5,0)	CIFAR-100
ResNet-34	21.35M	77.89	1.473	2.562	SGD	CIFAR-100
ResNet-34	21.35M	78.43	0.9876	1.871	WSGD(0.5,0)	CIFAR-100
DenseNet-121	7.13M	79.31	3.359	4.614	SGD	CIAFR-100
DenseNet-121	7.13M	80.35	1.196	2.152	WSGD(0.5,-0.1)	CIAFR-100
8	Discussion and Conclusion
In this paper we have theoretically analyzed the good property of DRO, and revealed the pro-
found connection between SGD and DRO. Accordingly we have proposed a practical algorithm
that can utilize the non-isotropic noise of the stochastic gradient. We have tested WSGD algorithm
on CIFAR-10 and CIFAR-100, achieving significant improvements compared to SGD.
We hope this paper can inspire works that theoretically study the generalization ability of optimiza-
tion algorithms. We think one should pay attention to the loss of each individual instance, rather than
the average of the entire sample. We expect future works on SGD without simplification to isotropic
noise, and algorithms that take advantage of the non-isotropic noise of the stochastic gradient.
8
Under review as a conference paper at ICLR 2019
References
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.
Peter L Bartlett, Olivier Bousquet, and Shahar Mendelson. Localized rademacher complexities. In
International Conference on Computational Learning Theory, pp. 44—58. Springer, 2002.
Aharon Ben-Tal, Dick Den Hertog, Anja De Waegenaere, Bertrand Melenberg, and Gijs Rennen.
Robust solutions of optimization problems affected by uncertain probabilities. Management Sci-
ence, 59(2):341-357, 2013.
Olivier Chapelle, Jason Weston, Leon Bottou, and Vladimir Vapnik. Vicinal risk minimization. In
Advances in neural information processing systems, pp. 416-422, 2001.
Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua
Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex op-
timization. In Advances in neural information processing systems, pp. 2933-2941, 2014.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize
for deep nets. arXiv preprint arXiv:1703.04933, 2017.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle pointsonline stochastic
gradient for tensor decomposition. In Conference on Learning Theory, pp. 797-842, 2015.
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochas-
tic programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT press Cambridge, 2016.
Tatsunori B Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness without
demographics in repeated loss minimization. arXiv preprint arXiv:1806.08010, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In CVPR, volume 1, pp. 3, 2017.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv
preprint arXiv:1609.04836, 2016.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-
nical report, Citeseer, 2009.
Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu. Deeply-
supervised nets. In Artificial Intelligence and Statistics, pp. 562-570, 2015.
Pascal Massart. Some applications of concentration inequalities to statistics. In Annales-Faculte des
Sciences Toulouse Mathematiques, volume 9, pp. 245-303. Universite Paul Sabatier, 2000.
Colin McDiarmid. On the method of bounded differences. Surveys in combinatorics, 141(1):148-
188, 1989.
Hongseok Namkoong and John C Duchi. Variance-based regularization with convex objectives. In
Advances in Neural Information Processing Systems, pp. 2971-2980, 2017.
Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization: Conver-
gence results and optimal averaging schemes. In International Conference on Machine Learning,
pp. 71-79, 2013.
9
Under review as a conference paper at ICLR 2019
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine
LearningResearch,15(1):1929-1958, 2014.
10
Under review as a conference paper at ICLR 2019
A Some Properties of Robust loss
Lemma 1. Let 'ι, ∙∙∙ ,'m be arbitrary real numbers. Let Conv(P) denote the convex hull of P,
then
mm
sup	pi`i =	sup	pi`i .
p∈P i=1	p∈conv(P) i=1
Lemma 2. Let 'ι, ∙∙∙ ,'m be arbitrary positive numbers. Assume P ∈ Σ and is symmetric. Then
the following holds for all p ⊂ P:
mm
| y^Pi'il ≤ SUp fp'0'i∙
i=1	p0∈P i=1
Lemma 3. Assume the loss function `(z, θ) is L-Lipschitz with respect to θfor all z, then for any
fixed sample S of size m, RS,P (θ) is kP k0 kP k∞ L-Lipschitz
Proof.
.^ , . ^	...
|RbS,P (θ) - RbS,P (θ )|
=i SUp y^Pi'(zi,θ) - SUp y^pi'(zi,θ0)∣
p∈P	p∈P
≤ max SUpXPi (' (zi, θ) - '(zi,θ0)), sup XPi (' (Zi,θ0) - '(zi,θ))
p∈P	p∈P
Notice that ∣'(zi,θ) - '(zi, θ0)∣ ≤ L∣∣θ - θ0k for all Zi, We have:
| XPi (' (Zi,θ)-'(zi,θ0)) |
≤ X ∣Pi∣Lkθ - θ0k
≤ kPk0kPk∞Lkθ - θ0k.
One can develop various bounds on the Lipschitz constant of Rs,p(θ).	□
Lemma 4 (Convexity). Assume that all entries of P ∈ P is non-negative. Futher assume that '(z, θ)
is convex with respect to θ for all Z, then for a fix sample of size m, the robust loss RS,P(θ) is convex.
m
And denote p* = arg sup Rsp(θ), then for any gi ∈ ∂θ '(zi, θ),	P↑gi is in the subgradient of
p∈P	i=1
τ~i ∕c^∖	.	■ . /t
RS,P (θ) at point θ.
Proof. For any p ∈ P, RS,p(θ) is a convex combination of convex functions, thus it is convex.
RS,P is supremum over a family of convex functions, thus it is also convex. To complete the proof,
by the definition of subgradient, we have the following holds for all i = 1, ∙∙∙ ,m:
'(zi,θ0) ≥ '(zi,θ) + hgi,θ0 - θi.
Then,
m
Rs,p* (θ0) = X PWZe ,θ0)
i=1
mm
≥	XP↑'(zi,θ) + hXP*gi,θ0-θi.
i=1	i=1
So,
^ ，一 ^ ，一
Rs,P(θ0) ≥ Rs,p* (θ0)
mm
≥	XP*'(zi,θ) + hXP*gi,θ0 - θi
i=1	i=1
m
=	RbS,P(θ)+hXPi*gi,θ0-θi.
i=1
11
Under review as a conference paper at ICLR 2019
□
Lemma 5. Assume the loss function '(z, θ) is μ-strongly convex (convex), then for any P ⊂ ∑+,
RSP IS μ-strongly convex (convex).
-r .	___ /	•»	. ι ι 「	. ∙ λ/ zt∖ . √	i - Γr> -ι Λ .ι . ι	ι . ι τ~i -
Lemma 6. Assume the loss function `(z, θ) takes values in [0, 1], then the robust loss RS,P is
bounded by 1+∣2PkI
B	Proof of the Theorems
Theorem 1. Let S be a fixed sample of size m. For a fixed θ ∈ Θ, define a matrix G =
(Vθ '(zι,θ),…，Vθ '(zm,θ)), whose i-th COlumn is the gradient of the i -th loss function w.r.t. θ.
For SGD with batch size k, denote p as the random weight vector uniformly sampled from P (k).
Then the expected squared update amount can be calculated by:
EkVRS,p(θ)k2 = kvRs,po(θ)k2 + km -k (tr(G>G) -kvRs,po(θ)k2).
p	k(m -1)	m
Proof.
EkVRbS,p(θ)k22 = EkGpk22
pp
= EkG(p -p0)k22 + kGp0k22
p
= E(p -p0)>G>G(p -p0) + kGp0 k22
p
= Eptr (p -p0)>G>G(p -p0) + kGp0k22
= Etr (p -p0)(p -p0)>G>G + kGp0k22
p
= tr Ep(p -p0)(p -p0)>G>G + kGp0k22
Ep(p-p0)(p-p0)>=Ep pp> + p0p0> - pp0> - p0p>
Ep pp> -p0p0>
One can easily calculate that E [pp>]=	/ ɪ	k-1	...	k-1 ∖ mk	m(m-1)k	m(m-1)k k-1	1	...	k-1
	m(m-1)k	mk	m(m-1)k .	... .	.. k-1	…	k-1	1
	∖m(m-1)k	m(m-1)k	mk /
when P is sampled from P(k), namely k entries of P are randomly set to ɪ, while other entries are
0. And
/ W
m2
> m2
Po Po =	.
.
.
1-^2
m2
1
~
m2
1
m2
∖
m2
m2
4)
m2
12
Under review as a conference paper at ICLR 2019
Then
tr E(p - p0)(p - p0)>G>G
p
tr(G>G) +
1> GTGI ∖
m
(1>G>G1 - tr(G>G))
k-1
1>G>G1
m
tr(G>G) -
m(m - 1)k
m - k	ttr(G>G)	>r,>r,
‘而二!)(	- p0G Gp0
- tr(G>
□
Theorem 2. Assume the loss function is L-Lipschitz. Suppose the SGD algorithm with constant
learning rate η converges to and stays in B, where B = {θ : kθ - θ0k2 ≤ B} is a ball that contains
■ "	r I' Ji Λ Λ 1 τ~i	C	.ι . ∖ / — ∕T> / 7 ∖ 1Γ)	.	ι ι	r τ~i	ι
a minimum of ERM loss RS,p0. Suppose that ∀p ∈ P (k), B contains a local minimum of RS,p and
^
Rs,p is μ-strongly convex on B. Further assume that the minimum value of the batch average loss
D
RS,p
^
^
^
in B is not too large, i.e., min RS,p(θ) ≤ min RS,p0 (θ) + δ. Then the robust loss RS,P(k) (θ)
can be bounded by:
^
^
RS,P(k) (θ) ≤ θm0∈inB RS,p0
2B2
(θ0) + r + 2LB + δ, ∀θ ∈ B.
μη2
Proof. We count the iteration after SGD converges to B and stays in B. We denote θt the parameter
at iteration t and p(t) the weight at iteration t.
Due to the μ-strongly convexity of Rs,p(θ),∀p ∈ P(k) we have:
1
27∣NRs,p(t"θt)k2 ≥ Rbs,p(t) (θt) - mmin Rbs,p(t) (θ )
2μ	θ ∈B
^ , _ . ^ , _.. _
≥ RbS,p(t) (θt) - θm0∈inB RbS,p0 (θ ) - δ.
Since the algorithm stays in B, the norm of the stochastic gradient can be bounded by 2B. Thus:
2B2
Rbs,p(t) (θt) ≤ m⅛B RS,P0 (θ)+—2+δ∙
θ ∈B	μη
^
^
Plus, the weighted average loss RS,p(t) is L-Lipschitz, so RS,p(t) (θ) can be bounded by:
2B2
Rbs,p(t) (θ) ≤ m0⅛n RS,po (θ ) +	2 + 2LB + δ, ∀θ ∈ B.
θ ∈B	μη
As SGD algorithm keeps stable in B, the above equality holds for all p(t) ∈ P(k). Finally we have:
2B2
RSP(k)(θ) ≤ min rS,P0 (θ ) +-----2 + 2LB + δ, ∀θ ∈ B.
' '	θ0∈B	μη2
□
Definition 1 (Robust Rademacher Complexity). For any m ≥ 1, the robust Rademacher complexity
of the parameter space Θ and the weight set P is defined as:
Rm(Θ,P) =E hRbS(Θ,P)i .
Theorem 3. For any δ > 0, with probability at least 1- δ, the following holds for all θ ∈ Θ:
^
^
^
E0RS0,P(θ) ≤ RS,P (θ) + 2RS(Θ, P) + 3mkP k∞
21m log2.
13
Under review as a conference paper at ICLR 2019
Proof. For simplicity, let Rm,P (θ)
, where S0 is m examples i.i.d. drawn from the
underlying distribution. For a sample S = (zι,…，zm) of size m, denote
Φ(S)
sup
θ∈Θ
Rm,P (θ) - RbS,P (θ) .
T i'	1	.	~第/	~	∖	. 1
If We change Zi to Zi and get S = (zι,…，zi,…，zm), then
Φ(S) - Φ(Se) ≤ sup RbSe P (θ) - RbS,P
θ∈Θ	S,P
≤ sup supPi('(zi,θ) - '(Zi,θ))
θ∈Θ p∈P
≤ kPk∞.
….一 一 一 , ≈.. ______________________ _ 一	.一. . 一. . . _ ..
Similarly We have ∣Φ(S) - Φ(S)∣ ≤ ∣∣P∣∣∞. Then by McDiarmid'smeqUahty(McDiarmid, 1989),
for any δ > 0, the following holds with probability at least 1 - 2:
Φ(S) ≤ E [Φ(S)] + mkPk∞ ↑j2m logδ.
Next we are to bound E[Φ(S)]. By symmetrization, we have:
ES[Φ(S)]
E sup
S θ∈Θ
Rm,P (θ) - RbS,P (θ)
ES θs∈uΘp SE0RbS0,P(θ)
- RS,P
≤ E sup
S,S0 θ∈Θ
RbS0,P (θ)
β ,
- RS,P
m
≤ S号 Sup PuP [X pi"")- '(z0,θ))
m
= E SuP SuP T σipi ('(Zi, θ) - '(z0 ,θ))
σ,S,S0 θ∈Θ p∈P i=0
m
≤ E SuPSuP	σipi'(zi,θ) + E sup sup
σ,S,S0 θ∈Θ p∈P i=0	σ,S,S0 θ∈Θ p∈P
=2Rm(Θ,P).
m
X -σipi'(zi ,θ)
i=0
This shows that
sup hRm,P - RbS,P (θ)i ≤ 2Rm(θ, P) + mkPk∞ j 2m lθgδ^
holds with probability at least 1 - δ.
To complete the proof, next we show that
Rm(Θ, P) ≤ RS (Θ, P)+ m∣∣Pk∞J 4 log 2
2m δ
holds with probability at least 1 - δ.
If the sample S and S differs only at the j-th instance, i.e., Zi = Zei for all i 6= j, then
m
RS (Θ, P) — RS(Θ, P) = E sup sup 2, σipi'(zi, θ) — E sup sup
σ θ∈Θ p∈P i=0	σ θ∈Θ p∈P
≤ E sup sup [σjpj ('(zj, θ) — '(zj ,θ))]
σ θ∈Θ p∈P
m
Xσipi'(zi, θ)
i=0
≤ kPk∞.
Similarly, we have |RS(Θ, P) - Re(Θ, P)| ≤ ∣∣P∣∣∞. Then the result holds immediately from
McDiarmid,s inequality.	□
14
Under review as a conference paper at ICLR 2019
Theorem 4.	For any > 0, the empirical Rademacher complexity and the empirical robust
Rademacher complexity can be respectively bounded by:
RS (θ)<ι Fo NWS MHlY) +
m
ed* (m)
m
and	____________________
RS (θ, P) ≤ (1 + RAD2(P ))J2lθg N 份⑸ P ),k ∙ k,C) + ed*(m).
mm
Proof. First, we introduce a lemma.
Lemma 7 (Massart 2000). Let A be a finite set in Rm, with r = maxx∈A kxk2, then the following
holds:
E
σ
m
Xσixi
—
x∈A i=1	m
≤ r,2log |A|
m
Denote N as the c-net of Θ(S) where |N| = N(Θ(S), ∣∣∙ ∣∣, c). Notice that '(zi, θ) ∈ [0,1], then
r ≤ √m. Applying Massart's lemma to N, We get:
E
σ
m
Xσixi
≤∑~ ≤ V
x∈N i=1 m
2log N(Θ(S ),∣H∣,e)
m
For any x ∈ Θ(S), there exists a x0 ∈ N, such that ∣x - x0∣ ≤ C. Then
m
σiXi
m
i=1
m 0m
V σixi . V
m^t m ɪ-,
σi (xi - x0i)
i=1
m0
≤ X σixi
一 m m
i=1
i=1
+ cd*(m)
m
m
Then
RbS(Θ) = E
σ
≤E
σ
sup
x∈Θ(S)
sup
x∈Θ(S)
m
σixi
m
i=1
m0
X σiXi
m
i=1
+ cd* (m)
m
)
≤E
σ
m0
Xσix0i
--------
x0∈N i=1	m
+ cd*(m)
m
≤
2logN(Θ(S), ∣ ∙ k,c) + ed*(m)
m
m
which completes the first part of the theorem. The second part is similar, but one should notice that
in this case r = max^)∣∣x∣∣2 ≤ √m∣∣P∣∣2 = √m(1 + RAD2(P)).	□
Theorem 5.	Assume the loss function is L-Lipschitz w.r.t. θ for all z. Consider the following hy-
pothesis set:
θc = {θ : ESRbs,p(θ) ≤ c,∣∣θ∣∣2 <r}.
For any c > 0, let N(Θc, c) denote the covering number of Θc. Then for all 0 < δ < 1, with
probability ≥ 1 - δ, the localized empirical Rademacher complexity can be bounded as:
R(θc) ≤ √m (1+ √mrad∞4conv(P))) (C + CkPkOkPk∞小刊^m lθg
15
Under review as a conference paper at ICLR 2019
Proof. Notice that `(z, θ) ∈ [0, 1] for all z and θ, then if we change a single sample zi of S
(zι,…，Zi,…，Zm) to form S0 = (zι,…，z0,…，Zm), We have
^
^
RS,P (θ) - RS0,P (θ)
sup RS,p(θ) - sup RS0,p(θ)
p∈P
,^
p∈P
^
≤ sup(RS,p(θ) - RS0,p(θ))
p∈P
=supPi('(zi,θ) - '(zi,θ))
p∈P
≤ l∣P∣l∞.
I ʌ .1	.	. I	∕C∖	/ Zi∖ I	I l∕T›l I	I ʌ » . iʌ -	「，♦	1
By the same argument, we can get ∣RSp(θ) - RSo P(θ)∣ ≤ ∣∣P∣∣∞. By MCDiarmid S inequal-
ity(McDiarmid, 1989), we have:
P {RS,P(θ) ≥ ERS,P(θ)+1} ≤ exp { m-pɪ }.
Let N denotes the minimum e-net of Θc, where |N| = N(Θc, e). Then We have
P n∃θ0 ∈ N， RbS,P (θ0) ≥ c + to
≤N (Θc,e)P {Rs,p (θ) ≥ C + t}
≤ N(Θc, )P nRbS,P
≤ N (Θc, ) exp
(θ) ≥ ES RbS,P (θ) + to
-2t2
m∣∣P∣l∞
-2t2
In other words, with probability larger than 1 - N(Θc, e) exp ∣ -？ ʃ, we have
sup RS,P (θ) < c + t.
θ∈N
By the definition of e-net, for any θ ∈ Θo there exists a θ0 ∈ N, such that ∣∣θ - θ0∣∣ < e. Let LS
denotes an upper bound of the Lipschitz constant of RS(θ), then we have
sup RbS,P (θ) < c + t + LS,
θ∈Θc
(5)
-2t2
with probability larger than 1 - N(Θc, e) exp < -IElg 〉.
lmllPll∞ J
Recall that assuming the loss function '(z, θ) is L-LiPschitz with respect to θ for all z, then the
LiPschitz constant of RS (θ) can be bounded by kPk0kPk∞L.
For those samples S = (zι,…，zm,) satisfying the universal bound (5), the empirical Rademacher
complexity can be bounded as well. The idea of the proof involves the representation of σ as a
convex combination of several vectors in P.
RbS(Θc) = E
σ
θs∈uΘpc
σi'(zi,θ)"
=∆ E
σ
σ ∙ '(S, θ)
sup ----------
θ∈Θc m
Recall that po = (ɪ,
m
conv(P). We hope that
1>
一 and by our assumption of the symmetry of P, we have po ∈
m
σ=ap0+b(PI- PO),
m
16
Under review as a conference paper at ICLR 2019
where p1 ∈ conv(P). Summing up all the coordinates of both sides of the equation, we have:
∑›i
m
al ∙ Po + b (1 ∙ Pi - 1 ∙ Po) = a.
P σi
Define σ =-----, Then
m
b(Pi - Po) = (σi -―,
mm
σm _ £∖>
mm
(6)
To derive the optimal bound, take the L∞ norm of both sides of (6), we get
ιbιkp1- p0k∞=maχ n| m- m |，
σm - σ∣0 ≤ -2.
mm m
Notice that kP1 - Pok∞ ≥ rad∞(conv(P)), so |b| ≤
2
mrad∞(conv(P)) .
Summarizing our result, we have:
σ ∙ '(S, θ) = [σpo + b(pi - Po)] ∙ '(S,θ)
m
≤∣σ - b∣∣Po ∙ '(S,Θ)∣ + 网∣pi∙ '(S,Θ)∣
…∙ . ., ^
≤ (Iσ | + 2∖b∖)RbS,P (θc).
Then
^ ... . ., ^
RS (Θc) ≤ E (∣σ∣ +2∣b∣) Rs,p (Θc)
σ
≤ fqEiσ∖2 +--—7----RbSP) RS,P (θC)
σ	mrad∞ (conv(P))
14
= √^ ( 1 + r- X (---RbSP ) RS,P(θc).
m	mrad∞(conv(P))
□
C The Details of WSGD
We denote the vector v(q, r) = (v1, v2, . . . , vm):
Vi = W (r X Cm-iCiBiT-1+	X	Cm-iCiBiTT
W ∖ q∣Bt∣>j≥0	∣Bt∣>j≥q∣Bt∣
m
where W is the normalizer s.t. i=i vi = 1. Then the stochastic gradient calculated in WSGD(q, r)
is an unbiased estimation of the gradient of RS,P(v(q,r)) (θ) at differentiable point. Recall that σ(v)
is the set of all the vectors whose entries are permutations of v’s and P(v) denote the convex hull
of σ(v).
17