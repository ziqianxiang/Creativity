Under review as a conference paper at ICLR 2019
Optimization on Multiple Manifolds
Anonymous authors
Paper under double-blind review
Ab stract
Optimization on manifold has been widely used in machine learning, to handle
optimization problems with constraint. Most previous works focus on the case
with a single manifold. However, in practice it is quite common that the optimiza-
tion problem involves more than one constraints, (each constraint corresponding
to one manifold). It is not clear in general how to optimize on multiple manifolds
effectively and provably especially when the intersection of multiple manifolds
is not a manifold or cannot be easily calculated. We propose a unified algorith-
m framework to handle the optimization on multiple manifolds. Specifically, we
integrate information from multiple manifolds and move along an ensemble di-
rection by viewing the information from each manifold as a drift and adding them
together. We prove the convergence properties of the proposed algorithms. We
also apply the algorithms into training neural network with batch normalization
layers and achieve preferable empirical results.
1	Introduction
Machine learning problem is often formulated as optimization problem. It is common that the
optimization problem comes with multiple constraints due to practical scenarios or human prior
knowledge that adding some of them help model achieve a better result. One way to handle these
constraints is adding regularization terms to the objective, such as the `1 and `2 regularization.
However, it is hard to adjust the hyper-parameters of the regularization terms to guarantee that the
original constraints get satisfied.
Another way to deal with the constraints is to optimize on manifolds determined by the constraints.
Then the optimization problem becomes unconstrained on the manifold, which could be easy to
solve technically. Furthermore, optimization on manifold indicates optimizing on a more compact
space, and may bring performance gain when training neural networks, e.g., (Meng et al., 2018; Cho
& Lee, 2017).
Most previous works on manifold optimization focus on a single manifold (Zhang & Sra, 2016;
Wei et al., 2016). However, in practice, we often face more than one constraints, each of them
corresponding to one manifold. If we still solve the optimization problem with multiple constraints
by method on manifold, we need to handle it on the intersection of multiple manifolds, which may
no longer be a manifold (Pathak et al., 2015). Due to this, traditional optimization methods on
manifold does not work in this case.
In this paper, we consider the problem of optimization on multiple manifolds. Specifically, the
problem is written as
arg min f (x),	(1)
x∈Tin=1 Mi
where each Mi is a manifold. We propose a method solving this problem by choosing the moving
direction as -Vf (x)(on manifold is -gradf (x)) with several drifts which are derived from the
descent information on other manifolds. By this method, we get sequence that has information from
all manifolds.
1.1	Related work
There are several articles discussing the problem of optimization on manifold. Most of them focus
on a single manifold. Readers can find a good summary about this topic and the advantages of op-
1
Under review as a conference paper at ICLR 2019
timization on manifold in (Absil et al., 2009). Recently, popular first order algorithms in Euclidean
space are studied in the manifold setting, e.g., the convergence of gradient descent (Zhang & Sra,
2016; Boumal et al., 2017), sub-gradient method (Hosseini & Uschmajew, 2017), stochastic vari-
ance reduction gradient (SVRG) (Zhang et al., 2016) and the gradient descent with momentum (Liu
et al., 2017).
Riemann approaches (Cho & Lee, 2017; Huang et al., 2017) have also been applied to train deep
neural network by noticing that the parameters of the neural network with batch normalization live
on Grassmann manifold and Oblique manifold, respectively.
1.2	Contribution
(1)	This paper introduces an algorithm to deal with optimization with multiple manifolds. The
algorithm adds drifts obtained from other manifolds to the moving direction, in order to incorporate
the information from multiple manifolds during the optimization process.
(2)	We prove the convergence of this algorithm under a very general framework. The proof is also
applicable to the convergence of many other algorithms including gradient descent with momentum
and gradient descent with regularization. Moreover, our proof does not depend on the choices of
Retrx on the manifold.
2	Optimization With Multiple Manifolds
2.1	Basic definition and lemma
The specific definition of manifold M can be found in any topology book. For better understanding,
we introduce several properties of manifold here. A manifold is a subspace of Rn . For a given point
x ∈ M, it has a tangent space TxM which is a linear space but M may not. For gradient descent
method, the iterates are generated via
χk+ι = Xk - ηVf (xk),
η is step length. However, the iterate generated by gradient descent may not on manifold anymore
because manifold is not a linear space. To fix this, we introduce a retraction function Retrx (η) :
TxM → M to determine how point moves on manifold. Specifically, if M is Rn, the Retrx
becomes x + η. We can consider η in Retrx as the moving direction of the iterating point. Then, the
gradient descent on manifold (Boumal et al., 2017; Zhang & Sra, 2016) is given by
Xk+1 = Retrχ(-Igradf (xk)),	(2)
L
where gradf (x) is Riemannian gradient. Riemannian gradient is the orthogonal projection of gra-
dient Vf (x) to tangent space TxM as Vf (x) may not in tangent space TxM and the moving
direction on manifold is only decided by the vector in TxM. All of notations related to manifold
can be referred to (Absil et al., 2009).
We next use a lemma to describe a property of the minimum point of the problem arg minx∈M f (x),
which is a special case of Yang et al., 2014, Corollary 4.2 and Boumal et al., 2017, Proposition 1.
Lemma 2.1 Letx be a local optimum for the optimization problem arg minx∈M f(x), which means
there exists a neighborhood Ux of x satisfy f(x) ≤ f(y) for y ∈ Ux. If f(x) is differential in x,
then kgradf (x)k = 0.
We see that gradf (x) plays a role of Vf (x) on manifold. Similar as Boumal et al. (2017) discussed,
we assume function has the property of Lipschtiz gradient. The definition of Lipschtiz gradient is
Definition 2.1 (Lipschtiz gradient) For any two points x, y in the manifold M, f(x) satisfy:
f(y) ≤ f(X) + hgradf(x),Retr-1(y)i + 刍|Retr-1(y)k2
Then we say that f satisfies the Lipschtiz gradient condition.
2
Under review as a conference paper at ICLR 2019
We next introduce a condition that guarantees the convergence of iterative algorithms.
Definition 2.2 (Descent condition) For a sequence {xk} and ak > 0, if
f(xk) - f (xk+1) ≥ akkgradf(xk)k2,
then we say the sequence satisfies the descent condition.
2.2	Gradient descent on two manifolds
First, we introduce a theorem to describe the convergence when the object function f is lower finite,
i.e., there exists a f * such that f (x) ≥ f * > -∞ for all x, and the iterates satisfy descent condition.
This theorem plays a key role in proof of the rest theorems.
Theorem 2.1	If f is lower finite, and the iteration sequence {xk} satisfies the descent condition for
any given {ak}, where each ak > 0. Then lim infk→∞ ak kgradf (xk)k = 0
Proof 1 The proof is available in Supplemental.
For better presentation, we first describe the algorithm under the circumstance of two manifolds.
Considering the objective function f constrained on two manifolds M1 , M2, we aim to find the
minimum point on M1 T M2 . Since M1 T M2 may not be a manifold, previous methods on
manifold optimization cannot apply directly. We propose a method that integrates information from
two manifolds over the optimization process.
Specifically, we construct two sequences {xk }, {yk }, each on one manifold respectively. We add a
drift which contains information from the other manifold to the original gradient descent on manifold
(equation 2). The updating rules are
Xk+1 = RetrxJ- 1(ak1)gradf (x®) + b(j1)h(j1))],	(3)
L
yk+ι = Retryk[-7(ak2)gradf (yk) + bk2)hk2))]	⑷
L
If bk = 0 in (equation 3) and (equation 4), the updating rules reduce to normal gradient descent on
manifold equation 2. The drift hk is in the tangent space TxM of each manifold, which represents
information from the other manifold. We call this algorithm gradient descent on manifold with
drifting, whose procedure is described in Algorithm 1.
Algorithm 1 Gradient descent with drift on manifold
Input δ > 0, x0 ∈ M1, y0 ∈ M2, Retrx, ε > 0
k→0
while kgradf (xk)k > ε or kgradf(yk)k > ε do
δ ≤ a(k1) ≤ 2
δ ≤ a(k2) ≤ 2
Calculating gradf(Xk) = PxI)▽/(Xk), gradf (yk) = Pf(2)Vf (yk).
Px(k1) and Py(k2) are respectively projection matrix of tangent space Txk, Tyk.
Obtain h(k1) ∈ Txk M1 and h(k2) ∈ TykM2.
分⑴ _ 2(1-akI))hgradf(χk),hk")
k =	khk1)k2
分⑵ _ 2(1-ak2))hgradf(yk),hk2)i
k =	khk2)k2
Xk+1 = Retrx J-L (ak1)gradf(xk) + bk1)hk1))]	update step
yk+1 = Retryk [-L(ak2)gradf (yk) + bk2)hk2))]	update step
k J k + 1
end while
return xk, yk
3
Under review as a conference paper at ICLR 2019
We next present the convergence theorem of this algorithm, which illustrates how we set ak and bk
in the algorithm.
Theorem 2.2	For function f(x) is lower finite, and Lipschtiz gradient. Ifwe construct the sequence
{xk} like equation (3), and for any 0 < δ < 2, we control δ ≤ ak ≤ 2. Setting
b _ 2(1 — ak)hgradf(xk), h®)
k =	khkk
(5)
then xk convergence to a local minimizer.
Proof 2 The proof is based on construction of the descent condition (equation 12) and is available
in Supplemental.
From the construction of bk, we can see that the smaller the correlation between gradf (xk)
and hk is, the smaller effect the information from M2 brings. In fact, we set h(k1) :=
Ilgradf(Xk)k Pxk)graf (yk) , where Pxk) is the projection matrix to tangent space TXkMi. Sim-
kPxk gradf (yk )k	k	k
ilarly we set h(k2) which exchanges xk and Px(ki) with yk and Py(k2)(projection matrix of tangent space
TykM2). The drift intuitively gives xk a force moving towards the minimizer on the other manifold.
If the two manifolds are Rn , then xk and yk are symmetry with each other. We have
(xk+ι = Xk- L (ak1)vf (xk) + bk1)hkI))	(6)
(yk+ι = yk - L(ak2)Vf(yk) + bk2)hk2)).
If the equation system is stable and X0 , y0 are mutually close, the distance between Xk and yk will be
small when k → ∞. By Schwarz inequality, we see bk ≤ 2(1 - ak). Since Ihk I = Igradf (Xk)I,
the scale of the drift is the same as the original Riemannian gradient. Hence, information from
another manifold will not affect much, when the points Xk and yk are close to a minimizer. We can
control the contribution of the information from the other manifold by adjusting ak. For instance,
ak = 1 indicates we do not integrate information from the other manifold.
We can also prove the convergence rate of this algorithm.
Theorem 2.3 If f is Iowerfinite satisfy f (x) ≥ f * > -∞, we choose ak as max{ ɪ^ɪ,δ}, 0 < δ <
2. Then, for any ε > 0, making sure ∣∣gradf (Xk)∣ < ε needs at most (dexp { Mx?晨'--+ ∏2 }］一
1)∧ d2f (χ0)-2、) e iterations.
ε2 (2δ-δ2 )
Proof 3 The proof is delegated to Supplemental.
Theorem 2.3	states the number of iterations we need to achieve a specific accuracy. Here we can
adjust ak as long as δ < ak < 2.
2.3 GRADIENT DESCENT ON n MANIFOLDS
In this subsection, we describe our algorithm for the case with multiple (more than 2) manifolds.
Suppose we have n manifolds, Mi,…，Mn, and sequence on manifold Mi is denoted as {χki)}.
In the following, we use sequence {X(ki)} on Mi as an example, and other sequences on other
manifolds can be derived accordingly. Let gk(i) ∈ TxMi denote the drift from manifold Mi (gk(i) is
gradf (X(ki)). Then let the updating rule be
1n
xk+i = RetrXk	-L X aki)gki)	.	⑺
i=i
4
Under review as a conference paper at ICLR 2019
Since f satisfies Lipschtiz gradient condition(2.1), we have
f(x(k1)
)-f(x
j=1
We choose a(k1) such that 0 < δ < a(k1) < 2. For j = 2 …n, we choose aj) such that
n
X aj,hgki),gj)i = 2(I- ak1))hgk1),gki)i	for i = 2 …n	⑻
j=2
then
n
aki) hgk1),gki)i- j X aki)a j) hgki),g j)i — | a^ki) hgk1),gki)i =0	、=…n
j=1
The way of choosing a(kj) is to obtain the descent condition. Specifically, when n = 2, the solution
of (equation 8) becomes bk in (equation 5). If a(kj) solve the linear equation system (8), we get the
descent condition (equation 12). From theorem 2.1, we prove the convergence of the updating rule
(equation 7) for the case with n manifolds. Writing (equation 8) in the matrix form, we have
Gk αk = βk ,
where	Gk	=	(hgki),gj)i)ij,	i,j from 2 to	n,	and	αk	=(遛,…，a，))T,βk	= (2(1 -
af))hgk1),gk2)i,…，2(1 - af))hgf),gkn)i)T. If Gk is invertible, the linear equation system has
an unique solution.
3 Apply Our Algorithm to Train Neural Network
3.1	Neural Network with Batch Normalization
Batch normalization has been widely used since its proposition (Ioffe & Szegedy, 2015). It trans-
forms the input value to a neuron from z = wTx to
BN(W)= Z-EI = WT(X- E(X)).
VZVar(Z)	√wt VχW
We can calculate the derivative as follows
∂BN (W)	X - E(X)	WT(X - E(X))VxW
=—.
dw	P WT VxW	(WT Vχw)2
For any a = 0,a ∈ R, we see that BN(W) = BN(ow) and dBNιWw) = a dBN(w). These
equations mean that after a batch normalization, the scale of parameter has no relationship with the
output value, but scale of gradient is opposite with the scale of parameter. Cho & Lee (2017) have
discussed that batch normalization could have an adverse effect in terms of optimization since there
can be an infinite number of networks, with the same forward path but different scaling, which may
converge to different local optima owing to different gradients.
To avoid this phenomenon, we can eliminate the effect of scale by considering the weight W on
the Grassmann manifold or Oblique manifold. On these two manifolds, we can ignore the scale of
parameter. Cho & Lee (2017); Huang et al. (2017) respectively discuss that BN(W) has same image
space on G(1, n) and St(n, 1) as well as Rn, where G(1, n) is a Grassmann manifold and St(n, 1)
is an Oblique manifold. Due to these, we can consider applying optimization on manifold to batch
5
Under review as a conference paper at ICLR 2019
normalization problem. However, the property of these two manifold implies that we can actually
optimize on the intersection of two manifolds. Since optimization on a manifold rely on Riemannian
gradient gradf (x) and Retrx, for a specific Retrx (9) of Grassmann manifold G(1, n), we get a unit
point x when η = -gradf (x) = 0 in formula (9). The condition kgradf (x)k = 0 means we obtain
a unit critical point on Grassmann manifold which is also on Oblique manifold.
The specific discussion of Grassmann manifold and Oblique manifold can be found in (Absil et al.,
2009). G(1, n) is a quotient manifold defined on a vector space, it regards vector with same direction
as same element. For example (1, 1, 1) and (10, 10, 10) correspond to same element. We represent
elements on G(1, n) with same direction by choosing one of them as representation element. Oblique
manifold is given by St(n,p) = {X ∈ Rn×p : ddiag(XTX) = Ip}, where ddiag(∙) is diagonal
matrix of a matrix.
We have discussed above that iteration point on G(1, n) would be a unit point when it’s a local
minimizer. Due to this, the local minimizer we find is actually live on the intersection of St(n, 1)
and G(1, n). Hence, training neural network with batch normalized weights can be converted to the
problem
arg min	f(x).
x∈G (1,n) T St(n,1)
Let Riemannian gradient be projection of Vf (x) to tangent space of x. On G(1, n), We have
Px1)(η) = η-XTη kxx∣2
gradf (X)= PxI)(Vf(X)) = Vf(X)-(XT Vf(X)) ⅛
Retr(x1)(η)
Pk Coskηk + osin kηk
(9)
On St(n, 1), we have
Px(2) (η ) = η - Xddiag(XT η )
gradf (X) = Px(2) (Vf (X)) = Vf (X) - Xddiag(XT Vf (X))
Retr(x2) (η)
X+η
IIx + ηk
the Px is the projection matrix onto the tangent space at X. These results can be derived from the
general formulas from (Absil & Gallivan, 2006) and (Edelman et al., 1999).
In backward process of training neural network, weight parameter of each layer is a matrix. Hence,
we get gradient to a matrix in every layer. To make calculation easier, we treat the gradient matrix
and parameters matrix as vector. For example a m × n gradient matrix can be viewed as a m × n
dimensional vector. Then we apply Algorithm 1 to update parameters, which means we optimize on
a product manifold
G(1, kι) X …G(1, kn),	St(kι, 1) X …St(kn, 1)
ki is number of parameters for the i-th hidden layer, and n is number of hidden layers. We need to
operate algorithm for parameter vector on each hidden layer. In other words, we update parameters
layer by layer.
3.2	Experiment
In this section, we use data set CIFAR-10 and CIFAR-100 (Krizhevsky & Hinton, 2009) to test our
algorithm. These two data sets are color images respectively have 10 and 100 classes, each of them
has 50,000 training images and 10,000 test images. The deep neural network we used is Wide-
ResNet (Zagoruyko & Komodakis, 2016), it output a vector which describe the probability of a data
divided into each class.
In every hidden layer of neural network, we apply batch normalization to weight parameters and
treat them as a vector. We have already discussed that minimizers of a neural network with batch
normalized weights live on the intersection of Grassmann manifolds and Oblique manifold. Hence,
we can train neural network with batch normalized weights by our algorithm(1). The biases of every
6
Under review as a conference paper at ICLR 2019
hidden layer is unrelated to batch normalization and are updated by SGD. For every training step,
We calculate mean loss S Pxg∈s l(f (Xi, θ), yi) of a mini batch to substitute the real loss function
Ex [l(f (x, θ), y)], where S is batch size.
The process of algorithm on tWo manifolds folloWs Algorithm 1, Where the tWo manifolds are
G(1, n) and St(n, 1), respectively. In Algorithm 1, We choose
h(I)	d	Pxk gradf (yk))
hk = kgradf (xk )kkPx1) gradf (yk )k,
kgradf(yk)k
Pyk) gradf(χk))
kPy2)gradf (Xk )∣
and a?) = a, = maχ{昌1, δ}. In the updating rules of Xk and yk, we add a norm-clip to vectors
(a(k1) gradf (Xk) + b(k1) h(k1) ) and (a(k2) gradf (yk) + b(k2) h(k2) ). Then We times η to the tWo vectors,
where η is the learning rate.
In the experiments, we compare three methods: 1) stochastic gradient descent on manifold with
drifting (Drift-SGDM), 2) stochastic gradient descent on manifold Boumal et al. (2017) (SGDM),
and 3) stochastic gradient descent (SGD). In Algorithm 1, we can get two sequences each corre-
sponding to a model on a manifold. We predict output class by adding two output vectors of two
models and choosing the biggest as prediction class.
For Drift-SGDM (Algorithm 1), we set δ = 0.9 and initial learning rate ηm = 0.4 for weights
parameters which is multiplied by 0.4 at 60, 120, and 160 epochs. Initial learning rate η for biases
is 0.01 which is multiplied by 0.4 at 60, 120, and 160 epochs. Norm clip is 0.1. Training batch size
is 128. The number of training epochs is 200.
For SGDM, we choose a = 1 in Algorithm 1. The other settings are the same as Drift-SGDM. That
a = 1 in Algorithm 1 means that SGDM optimizes on each manifold individually
We set SGD as baseline. The learning rate is 0.2 which is multiplied by 0.2 at epoch 60,120 and
160. Weight decay is set as 0.0005, but we do not apply weight decay for algorithms on manifold.
All other settings are the same as the above two algorithms.
(a) Training loss for WRN-28-10 (b) Accuracy for WRN-28-10 on (c) Accuracy for WRN-16-4 on
on CIFAR-100	CIFAR-100	CIFAR-100
Figure 1: Some results based on Wide-ResNet. WRN-d-k denotes a wide residual network that has
d convolutional layers and a widening factor k
About Drift-SGDM and SGDM, the loss is achieved from the average of two model. The parameter
scale of the two model can be different, because they respectively live on Grassmann manifold and
Oblique manifold. Due to this, the comparing between Drift-SGDM and SGDM is more reasonable.
We also give the accuracy curve and a tubular of accuracy rate on test sets to validate our algorithms.
Dataset		CIFAR-10					CIFAR-100		
Model	SGD	Drift-SGDM	SGDM	SGD	Drift-SGDM	SGDM
WRN-52-1	93.3	92.88	93.11	71.07	69.61	69.58
WRN-16-4	94.46	94.16	94.16	74.74	76.4	76.04
WRN-28-10	95.59	95.07	94.79	78.88	78.92	79.37
Table 1: Accuracy rate on test sets of data.
7
Under review as a conference paper at ICLR 2019
We see that our algorithm perform better on larger neural network. Our algorithm does not have
regularization term, and it does not perform well in the aspect of generalization. We can actually
add a regularization term like in (Cho & Lee, 2017) to achieve better generalization.
We choose δ in Algorithm 1 as 0.9. Since b(ki) ≤ 2(1 - a(ki) ) where i = 1, 2 as we have discussed
in section 2, we see drift term b(ki)h(ki) in Algorithm 1 doesn’t affect much to iteration point. We can
actually set a smaller δ to enhance the influence of drift term b(ki)h(ki).
4 Conclusion
In this paper, we derive an intuitively method to approach optimization problem with multiple con-
straints which corresponds to optimizing on the intersection of multiple manifolds. Specifically,
the method is integrating information among all manifolds to determine minimum points on each
manifold. We don’t add extra conditions to constraints of optimization problem, as long as each
constraint can be converted to a manifold. In the future, we may add some conditions to manifolds
which derive a conclusion that minimum points on each manifold achieved by our algorithm are
close with other. If this conclusion is established, the problem of optimization on intersection of
multiple manifolds is solved.
According to the updating rule (equation 3), we can derive many other algorithms, because the drift
hk in (equation 3) is flexible. On the other hand, Retrx on our algorithm does not limit to a specific
one. Since there are some results for Retrx = Expx , for example Corollary 8 in (Zhang & Sra,
2016), we may get more elegant results by using Expx as retraction function in our algorithm.
The manifolds we encounter in optimization are mainly embedded sub-manifold and quotient mani-
fold (Absil et al., 2009). Embedded sub-manifold is F-1(y) for a smooth function F : M1 → M2,
where M1, M2 are two manifolds and y ∈ M2. Quotient manifold is a quotient topology space
generalized by a specific equivalence relationship 〜.In this paper, We use Oblique manifold and
Grassmann manifold which are embedded sub-manifold and quotient manifold respectively.
The difficulty We faced in optimization on manifold is calculating tangent space TxM and Rieman-
nian gradient gradf (x). Giving a exact formula of a tangent space TxM is not a easy problem.
On the other hand, since Riemannian gradient is Vf (x) projected to a tangent space TXM, finding
projection matrix to a specific space Tx M is nontrivial.
8
Under review as a conference paper at ICLR 2019
Acknowledgments
Use unnumbered third level headings for the acknowledgments. All acknowledgments, including
those to funding agencies, go at the end of the paper.
References
P. A. Absil and K. A. Gallivan. Joint diagonalization on the oblique manifold for independent
component analysis. ICASSP, 2006.
P.-A. Absil, Robert. Mahony, and Rodolphe Sepulchre. Optimization algorithms on matrix mani-
folds. Princeton Press, 2009.
Nicolas Boumal, P.-A. Absil, and Coralia Cartis. Global rates of convergence for nonconvex opti-
mization on manifolds. arxiv preprint arxiv:1605.08101, 2017.
Minhyung Cho and Jaehyung Lee. Riemannian approach to batch normalization. Advances in
Neural Information Processing Systems 30 (NIPS 2017), 2017.
Alan Edelman, T. A. Arias, and Steven.T. Smith. The geometry of algorithms with orthogonality
constraints. Society for Industrial and Applied Mathematics, 1999.
Seyedehsomayeh Hosseini and Andr Uschmajew. A riemannian gradient sampling algorithm for
nonsmooth optimization on manifolds. Siam Journal on Optimization, 27, 2017.
Lei Huang, Xianglong Liu, Bo Lang, and Bo Li. Projection based weight normalization for deep
neural networks. arxiv preprint arXiv:1710.02338, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by re-
ducing internal covariate shift. In Proceedings of The 32nd International Conference on Machine
Learning,ICML,pp. 448-456, 2015.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. De-
partment of Computer Science, University of Toronto, 2009.
Yuanyuan Liu, Fanhua Shang, James Cheng, Hong Cheng, and Licheng Jiao. Accelerated first-order
methods for geodesically convex optimization on riemannian manifolds. Neural Information
Processing Systems (NIPS), pp. 4875-4884, 2017.
Qi Meng, Wei Chen, Shuxin Zheng, Qiwei Ye, and Tie-Yan Liu. Optimizing neural networks in the
equivalent class space. arXiv preprint arXiv:1802.03713, 2018.
Deepak Pathak, Philipp Krahenbuhl, and Trevor Darrell. Constrained convolutional neural networks
for weakly supervised segmentation. In Proceedings of the IEEE International Conference on
Computer Vision, pp. 1796-1804, 2015.
Songui Wang, Jianhong Shi, Suju Yin, and Mixia Wu. Introduction to linear model. Science press
Beijing, 2003.
Ke Wei, Jian-Feng Cai, Tony F Chan, and Shingyu Leung. Guarantees of riemannian optimization
for low rank matrix recovery. SIAM Journal on Matrix Analysis and Applications, 37(3):1198-
1222, 2016.
Weihong Yang, Leihong Zhang, and Ruyi Song. Optimality conditions for the nonlinear program-
ming problems on riemannian manifolds. Pacific Journal of Optimization, 10:415-434, 2014.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arxiv preprint arX-
iv:1605.07146v4, 2016.
Hongyi Zhang and Suvrit Sra. First-order methods for geodesically convex optimization. In Con-
ference on Learning Theory, pp. 1617-1638, 2016.
Hongyi Zhang, Sashank.J. Reddi, and Sra Suvrit. Riemannian svrg:fast stochastic optimization on
riemannian manifolds. Advances in Neural Information Processing Systems 29 (NIPS 2016),
2016.
Vladimir. A. Zorich. Mathematical Analysis I. Spring-Verlag, 2002.
9
Under review as a conference paper at ICLR 2019
A Discussion about the framework of gradient descent with
DRIFT
A.1 Connection with classical algorithms
In this section, we study the frame work of gradient descent with drift. In a special case, we regard
Rn as a manifold. Then, Rienmann gradient gradf (x) = Vf (x), tangent space TxM = Rn and
Retrx(η) = x + η. In Algorithm (1), we set
hk+1 =-斤 (ak V f (Xk ) + bk hk ),
L
where
δ∕zl /2 b _ 2(1 — akMVf(Xk),hki
δ < ak < 2,	bk =	II 2 ∣∣2	.
khkk2
Then we have
Xk+1 = Xk 一 L(akVf (Xk) + bkhk)
=xk 一 L (ak V f (Xk ) 一 L (ak-1Vf(Xk-1) + bk-1hk-1)),
which is exactly a kind of gradient descent with momentum. And this algorithm is convergence as
we proved. On the other hand, if choosing hk as gradient of a regularization term R(X) on Xk. For
example, hk becomes 2Xk when R(X) = kXk2 . The iteration point in Algorithm (1) is achieved by
gradient descent with regularization term.
A.2 A stochastic drift
The drift in (equation 3) we have discussed is non-stochastic. But actually, we can change the drift
as a stochastic term to construct a non-descent algorithm. Meanwhile, stochastic drift gives iteration
sequence ability of jumping from local minimizer. The update rule is
Xk+1 = Retrxk [—v(ak gradf (Xk)+ bk Pxk ξk),]	(10)
L
where ξk is a random vector with mean vector μ, covariance matrix Σ. The process of this algorithm
is Algorithm 2.
Algorithm 2 Non-descent method with stochastic noise
Input 0 < δ < 2, X0 ∈ M, Retrx, ε > 0
k→1
while kgradf (X)k > ε do
Sample ξk with mean vector μ and covariance matrix Σ
bk = k
a = (1 - kkgradf (xk )k	IIgradf(Xk )k ≥ k
k = V	k gradf (Xk )k< 1
Xk+1 = Retrxk [—L(akgradf (Xk) + bkPxkξk)]	update step
Pxk is projection matrix to tangent space TxM
k - k + 1
end while
return Xk
We give convergence theorem of Algorithm 2. The proof implies that this algorithm is non-descent,
it also shows how we set ak and bk.
TheoremA.1 Forfunction f (x) ≥ f * > -∞, and Lipschtiz gradient. Ifweconstructthesequence
∞
{Xk} like (equation 10), choosing {ak}, {bk} satisfy	bk < ∞ and
k=1
ak =	δ1
1
kkgradf(xk)k
kgradf(xk)k
kgradf(xk)k
1-k 1-k
10
Under review as a conference paper at ICLR 2019
where 0 < δ < 2, we have lim infk→∞ kgradf (xk)k2 = 0.
In this theorem, bk control the speed of back fire. The noise ξk in (equation 10) has small effect to
iteration process when k is large, because sequence is about to be stable after enough iterations. But
in beginning of iteration procedure, noise ξk effects much which give iteration sequence ability of
jumping from local minimizer.
B Proof of theorems
In this section, we give proof of theorems in this paper. The proof of Theorem 2.1 is
Proof 4 (proof of Theorem 2.1) According to definition 2.2 of descent condition, we have
k-1
f(x0) - f(xk) ≥ Xaikgradf(xi)k2,
i=0
for any k. Since f is lower finite, we have
∞
X aikgradf(xi)k2 ≤ f (xo) - f * < ∞,
i=0
where f(x) ≥ f* > -∞, it means lim inf k→∞ ak kgradf (xk)k = 0.
The proof of Theorem 2.2 is
Proof 5 (proof of Theorem 2.2) Since f satisfy Lischtiz gradient(2.1), we have
f(xk) — f(xk+ι) ≥ 1(ak - a)kgradf(xk)k2 + Lbk(1 - a*)(gradf(xk), h®)
b2
— 察 khkk2
2L
By the definition of bk, we got
f(xk) — f(xk+ι) ≥ (ak - ak)kgradf(xk)k2.
a2
Since ak ——2k > 0, by Theorem 2.1, we have
lim inf (a® - T)kgradf(xk)『=0.
k→∞	2
The definition of ak implies that kgradf (x)k2 → 0.
The proof of Theorem 2.3 gives convergence rate of gradient descent on manifold with drift.
Proof 6 (proof of Theorem 2.3) For any ε > 0, assuming kgradf (xi)k ≥ ε, i ∈ N until i
Then
(12)
k.
k-1
f(XO) - f(Xk) ≥ X[(i+1-
i=0 i+ 1
1	δ2
2(i+ip) ∨ (δ - Rkgradf(Xi)k
≥ [(log (k + 1) -
∞	1	δ2
2(i + 1)2) ∨ k(δ - y)kgradf(χi)k2,
∞2
here We use the relationship log (1 + x) ≤ X when X ≥ 0. Since P 2(i+i)2 = 为 < ∞(Zorich,
i=0
2002), for i ≤ k - 1, we have
i o∙^v∖ f f (χ0) - f * , π2
lθg(k + 1) ≤ kgradf(Xi)∣∣2 +12
and
k ≤	f(X0)-f*
^^≤	.
一(δ - δ2)kgradf(Xi)k2
Since ∣∣gradf(Xi)k ≥ ε when i ≤ k - 1, k ≤ (「exp { f50-* + ∏2}] - 1) ∧ d2!f(20"f,]∙ ■
11
Under review as a conference paper at ICLR 2019
Before proof Theorem A.1, we need two lemmas.
Lemma B.1 A random vector with Ex = μ and Covx = Σ. Thenfor any symmetric matrix A, we
have
E(XT Ax) = μτ Aμ + tr(AΣ).
This lemma can be derived from Wang et al. (2003)’s theorem 3.2.1 of Page 57. The other lemma is
Lemma B.2 If A and Σ are n × n symmetric matrix, then tr(AΣ) ≤ tr(A)λmax, where λmax is
the largest engine value of Σ
Proof 7 (proof of Lemma B.2) According to spectral decomposition of symmetric matrix A, A can
n
be written as P λiγiγiT, where λi is engine value ofA and γi is engine vector corresponding to λi.
i=1
They satisfy
γiTγj = δij ,
δij is Kronecker delta. Hence
nn	n
tr(AΣ) = X tr(λiγiγiTΣ) = Xtr(λiγiTΣγi) ≤ λmax Xtr(λi) = λmaxtr(A).
i=1	i=1	i=1
Here we use Rayleigh theorem of theorem 2.4.(Wang et al., 2003)
Proof 8 (proof of Theorem A.1) Since Pxk is a projection matrix, which is a symmetric idempotent
matrix. Because f satisfies Lipschtiz gradient(2.1), we have
f(xι) - f (Xk) ≥ 1
L
]X(ai-----2^ )kgradf (xi)k2 + bi(I - ai)εi Jiη
where εi = (Pxi gradf (xi))T ξi = gradf(xi))Tξi, ηi = ξiTPxTiPxiξi = ξiTPxiξi. Due to the two
random variables, algorithm (2) is not necessary descent. Σ is a symmetric positive definite matrix.
By Schwarz equality and definition of ai , we have
k-1	k-1
EX[bi (1 - ai )εi] ≤ E Xbi(1- ai)kgradf(xi)kkξik
i=1	i=1
k-1
≤ Xbi (E(1 - ai)2kgradf(xi)k2)2(E|£『)2
i=1
k-1
=Xbi (E(1- ai)2kgradf(xi)k2)2 [||〃『+ tr(Σ)]1
i=1
≤ [kμk2+ ≤ tr(∑)]1XX bi.
i=1
By Fatou’s lemma, we have
k-1	k-1
E lim inf [X bi(1 - ai)εi] ≤ lim inf E[X bi(1 - ai)εi]
k→∞	k→∞
i=1	i=1
k-1	∞
≤ kl→rn [kμk2+ ≤ tr(£)] 1 X 彳=[kμk2+ ≤ tr(£)] 1 X W < ∞,
i=1
i=1
which implies
k-1
lim inf bi (1 - ai)εi < ∞ a.s.
k→∞
i=1
12
Under review as a conference paper at ICLR 2019
Since tr(Pxi) = rank(Pxi) and the largest engine value of Pxk is 1, we have
E(ηi) = E(ξTPxiξi) = μτPxiμ + tr(Pχi∑) ≤ kμk2 + λmCχd,
where d is the dimension of x and λmax is the largest engine value of Σ. By Levy’s theorem, we
have
k-1 b2	k-1 b2	k-1	b2
ELlim	Σ 十ηi] =	.lim	E[E 十ηi]	≤	lim	ΣS(kμk2 + λmaxd) 十	< ∞,
k→∞	2	k→∞	2	k→∞	2
i=1	i=1	i=1
∞ b2
which implies E 方 η is finite almost surely. Hence
i=1
X(ai - a)kgradf(ik)k2 ≤ L(f(xι) - f*) - liminf
2	k→∞
i=1
then we have
a.s.
i=1
i=1
Iiminf(ak - ^k)∣Igradf(Xk)∣∣2 = 0 a.s.
k→∞	2
(13)
by the definition of ak(equation 11), equation (13) is equivalent to lim infk→∞ kgradf (xk)k2 = 0
almost surely.
13