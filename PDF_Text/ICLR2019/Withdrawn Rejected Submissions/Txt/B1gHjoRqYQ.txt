Under review as a conference paper at ICLR 2019
An Efficient and Margin-Approaching
Zero-Confidence Adversarial Attack
Anonymous authors
Paper under double-blind review
Ab stract
There are two major paradigms of white-box adversarial attacks that attempt to
impose input perturbations. The first paradigm, called the fix-perturbation attack,
crafts adversarial samples within a given perturbation level. The second paradigm,
called the zero-confidence attack, finds the smallest perturbation needed to cause
misclassification, also known as the margin of an input feature. While the former
paradigm is well-resolved, the latter is not. Existing zero-confidence attacks either
introduce significant approximation errors, or are too time-consuming. We there-
fore propose MarginAttack, a zero-confidence attack framework that is able
to compute the margin with improved accuracy and efficiency. Our experiments
show that MarginAttack is able to compute a smaller margin than the state-of-
the-art zero-confidence attacks, and matches the state-of-the-art fix-perturbation
attacks. In addition, it runs significantly faster than the Carlini-Wagner attack,
currently the most accurate zero-confidence attack algorithm.
1	Introduction
Adversarial attack refers to the task of finding small and imperceptible input transformations that
cause a neural network classifier to misclassify. White-box attacks are a subset of attacks that have
access to gradient information of the target network. In this paper, we will focus on the white-box at-
tacks. An important class of input transformations is adding small perturbations to the input. There
are two major paradigms of adversarial attacks that attempt to impose input perturbations. The
first paradigm, called the fix-perturbation attack, tries to find perturbations that are most likely to
cause misclassification, with the constraint that the norm of the perturbations cannot exceed a given
level. Since the perturbation level is fixed, fix-perturbation attacks may fail to find any adversarial
samples for inputs that are far away from the decision boundary. The second paradigm, called the
zero-confidence attack, tries to find the smallest perturbations that are guaranteed to cause misclas-
sification, regardless of how large the perturbations are. Since they aim to minimize the perturbation
norm, zero-confidence attacks usually find adversarial samples that ride right on the decision bound-
aries, and hence the name “zero-confidence”. The resulting perturbation norm is also known as the
margin of an input feature to the decision boundary. Both of these paradigms are essentially con-
strained optimization problems. The former has a simple convex constraint (perturbation norm), but
a non-convex target (classification loss or logit differences). In contrast, the latter has a non-convex
constraint (classification loss or logit differences), but a simple convex target (perturbation norm).
Despite their similarity as optimization problems, the two paradigms differ significantly in terms of
difficulty. The fix-perturbation attack problem is easier. The state-of-the-art algorithms, including
projected gradient descent (PGD) (Madry et al., 2017) and distributional adversarial attack (Zheng
et al., 2018), can achieve both high efficiency and high success rate, and often come with theoret-
ical convergence guarantee. On the other hand, the zero-confidence attack problem is much more
challenging. Existing methods are either not strong enough or too slow. For example, DeepFool
(Moosavi Dezfooli et al., 2016) and fast gradient sign method (FGSM) (Goodfellow et al., 2014;
Kurakin et al., 2016a;b) linearizes the constraint, and solves the simplified optimization problem
with a simple convex target and a linear constraint. However, due to the linearization approximation
errors, the solution can be far from optimal. As another extreme, L-BFGS (Szegedy et al., 2013)
and Carlini-Wagner (CW) (Carlini & Wagner, 2017) convert the optimization problem into a La-
grangian, and the Lagrangian multiplier is determined through grid search or binary search. These
attacks are generally much stronger and theoretically grounded, but can be very slow.
1
Under review as a conference paper at ICLR 2019
The necessity of developing a better zero-confidence attack is evident. The zero-confidence attack
paradigm is a more realistic attack setting. More importantly, it aims to measure the margin of each
individual token, which lends more insight into the data distribution and adversarial robustness.
Motivated by this, we propose MarginAttack, a zero-confidence attack framework that is able to
compute the margin with improved accuracy and efficiency. Specifically, MarginAttack iterates
between two moves. The first move, called restoration move, linearizes the constraint and solves the
simplified optimization problem, just like DeepFool and FGSM; the second move, called projection
move, explores even smaller perturbations without changing the constraint values significantly. By
construction, MarginAttack inherits the efficiency in DeepFool and FGSM, and improves over
them in terms of accuracy with a convergence guarantee. Our experiments show that MarginAt-
tack attack is able to compute a smaller margin than the state-of-the-art zero-confidence attacks,
and matches the state-of-the-art fix-perturbation attacks. In addition, it runs significantly faster than
CW, and in some cases comparable to DeepFool and FGSM.
2	Related Works
In addition to the aforementioned state-of-the-art attacks, there are a couple of other works that
attempt to explore the margin. Jacobian-based saliency map attack (Papernot et al., 2016) is among
the earliest works that apply gradient information to guide the crafting of adversarial examples. It
chooses to perturb the input features whose gradient is consistent with the adversarial goal. One-
pixel attack (Su et al., 2017) finds adversarial examples by perturbing only one pixel, which can
be regarded as finding the `0 margin of the inputs. Ilyas et al. (2018) converts PGD into a zero-
confidence attack by searching different perturbation levels, but this again can be time-consuming
because it needs to solve multiple optimization subproblems. Weng et al. proposed a metric called
CLEVER (Weng et al., 2018), which estimates an upper-bound of the margins. Unfortunately, recent
work (Goodfellow, 2018) has shown that CLEVER can overestimate the margins due to gradient
masking (Papernot et al., 2017). The above are a just a small subset of white-box attack algorithms
that are relevant to our work. For an overview of the field, we refer readers to Akhtar & Mian (2018).
The MarginAttac k framework is inspired by the Rosen’s algorithm (Rosen, 1961) for constraint
optimization problems. However, there are several important distinctions. First, the Rosen’s algo-
rithm rests on some unrealistic assumptions for neural networks, e.g. continuously differentiable
constraints, while MarginAttack has a convergence guarantee with a more realistic set of as-
sumptions. Second, the Rosen’s algorithm requires a step size search for each iteration, which
can be time-consuming, whereas MarginAttac k will work with a simple diminishing step size
scheme. Most importantly, as will be shown later, MarginAttack refers to a large class of attack
algorithms depending on how the two parameters, a(k) and b(k), are set, and the Rosen’s algorithm
only fits into one of the settings, which only works well under the `2 norm. For other norms, there
exist other parameter settings that are much more effective. As another highlight, the convergence
guarantee of MarginAttack holds for all the settings that satisfy some moderate assumptions.
3	The MarginAttack Algorithm
In this section, we will formally introduce the algorithm and discuss its convergence properties. In
the paper, we will denote scalars with non-bolded letters, e.g. a or A; column vectors with lower-
cased, bolded letters, e.g. a; matrix with upper-cased, bolded letters, e.g. A; sets with upper-cased
double-stoke letters, e.g. A; gradient of a function f (x) evaluated at X = xo as Vf (xo).
3.1	Problem Formulation
Given a classifier whose output logits are denoted as lo(x), lι(x), ∙∙∙ ,lc-ι(x), where C is the total
number of classes, for any data token (x0, t), where x0 is an n-dimensional input feature vector,
and t ∈ {0,…，C 一 1} is its label, MARGINATTACK computes
x* = arg min d(x — xo), s.t. c(x) ≤ 0,	(1)
x
where d(∙) is a norm. In this paper We only consider '2 and '∞ norms, but the proposed method is
generalizable to other norms. For non-targeted adversarial attacks, the constraint is defined as
c(x) = lt(x) — max li (x) — ε,	(2)
i6=t
2
Under review as a conference paper at ICLR 2019
where ε is the offset parameter. As a common practice, ε is often set to a small negative number to
ensure that the adversarial sample lies on the incorrect side of the decision boundary. In this paper,
we will only consider non-targeted attack, but all the discussions are applicable to targeted attacks
(i.e. c(x) = maxi6=a li(x) - la(x) - ε for a target class a).
3.2	The MarginAttack Procedure
MarginAttack alternately performs the restoration move and the projection move. Specifically,
denote the solution after the k-th iteration as x(k). Then the two steps are:
Restoration Move: The restoration move tries to hop to the constraint boundary, i.e. c(x) = 0 with
the shortest hop. Formally, it solves:
z(k) = arg min d(x — x(k)), s.t. VTc(x(k))(x — x(k)) = —α(k)c(x(k)).	(3)
x
where α(k) is the step size within [0, 1]. Notice that the left hand side of the constraint in Eq. (3)
is the first-order Taylor approximation of c(z(k)) - c(x(k)), so this constraint tries to move point
closer to c(x) = 0 by α(k). It can be shown, from the dual-norm theory,1 that the solution to (3) is
z(k) = x(k)
a(k)C(X(Q)S(X(Q)
VT c(x(k) )s(x(k))
(4)
—
s(x)is defined such that VTc(x)s(x) = d*(VTc(x)), where d* (∙) is the dual norm of d(∙). Specif-
ically, noticing that the dual norm of the 'p norm is the '(-—1 norm, we have
s(X)
Vc(X)/kVc(X)k2
sign(Vc(X))
if d(∙) is the '2 norm
if d(∙) is the '∞ norm
(5)
As mentioned, Eq. (4) is similar to DeepFool under '2 norm, and to FGSM under '∞ norm. There-
fore, we can expect that the restoration move should effectively hop towards the decision boundary,
but the hop direction may not be optimal. That is why we need the next move.
Projection Move: The projection move tries to move closer to x0 while ensuring that c(x) will not
change drastically. Formally,
X(k+1) = z(k) — β(k)a(k)Vd(z(k) — X0) — β(k)b(k)s(z(k))	(6)
where β(k) is the step size within [0, 1]; a(k) and b(k) are two scalars, which will be specified
later. As an intuitive explanation on Eq. (3), notice that the second term, which we will call the
distance reduction term, reduces the distance to x0 , whereas the third term, which we will call the
constraint reduction term, reduces the the constraint (because s(z(k)) and Vc(z(k)) has a positive
inner product). Therefore, the projection move essentially strikes a balance between reduction in
distance and reduction in constraint.
a(k) and b(k) can have two designs. The first design is to ensure the constraint values are roughly
the same after the move, i.e. c(z(k)) - c(x(k+1)) ≈ 0. By Taylor approximation, we have
VT c(z(k))(X(k+1) — z(k)) = 0,	(7)
whose solution is
b(k) = a(k)VT c(z(k) )Vd(z(k)— xo)
b =	VT c(z(k))s(z(k))	.	(8)
Another design is to ensure the perturbation norm reduces roughly by β(k), i.e. d(x(k+1) - x0) ≈
(1 - β(k))d(z(k) - x0). By Taylor approximation, we have
VT d(z(k) — X0)(X(k+1) — z(k)) = β(k)d(z(k) — X0),	(9)
whose solution is
a(k) = 1 —
Mk)VT d(z(k) — xo)S(Z(Q)
VT d(z(k) — xo) Vd(z(k) — xo)
(10)
It should be noted that Eqs. (8) and (10) are just two specific choices for a(k) and b(k). It turns out
that MARGINATTACK will work with a convergence guarantee for a wide range of bounded a(k)s
and b(k)s that satisfy some conditions, as will be shown in section 3.4. Therefore, MARGINAT-
tack provides a general and flexible framework for zero-confidence adversarial attack designs. In
practice, we find that Eq. (8) works better for '2 norm, and Eq. (8) works better for '∞ norm.
1See Thm. 2 in the appendix for a detailed proof.
3
Under review as a conference paper at ICLR 2019
3.3	How MarginAttack Works
Figure 1 illustrates a typical convergence path of MARGINATTACK using `2 norm and Eq. (8) as
an example. The red dots on the right denote the original inputs x0 and its closest point on the
decision boundary, x*. Suppose after iteration k, MARGINATTACK reaches x(k), denoted by the
green dot on the left. The restoration move travels directly towards the decision boundary by finding
the normal direction to the current constraint contour. Then, the projection move travels along
the tangent plane of the current constraint contour to reduce the distance to x0 while preventing
the constraint value from deviating much. As intuitively expected, the iteration should eventually
approach x*. Figure 2 plots an empirical convergence curve of the perturbation norm and constraint
value of MARGINATTACK-'2 on a randomly chosen CIFAR image. Each move from a triangle to
a circle dot is a restoration move, and from circle to triangle a projection move. The red line is the
smoothed version. As can be seen, a restoration move reduces the constraint value while slightly
increasing the constraint norm, and a projection move reduces the perturbation norm while slightly
affecting the constraint value. Both curves can eventually converge.
3.4	The Convergence Guarantee
The constraint function c(x) in Eq. (2) is nonconvex, thus the convergence analysis for MARGINAT-
tack is limited to the vicinity of a unique local optimum, as stated in the following theorem.
Theorem 1. Denote x* as one local optimumforEq. (1). Assume Vc(x*) exists. Define projection
matrices
P = I - s(x*)(VTc(x*)s(x*))-1VTc(x*)	(11)
Consider the neighborhood B = {x : kP[x - χ*]k2 ≤ X, |c(x)| ≤ C} that satisfies the following
assumptions:
1.	(Differentiability) ∀x ∈ B, Vc(x) exists, but can be discontinuous, i.e. all the discontinuity
points of the gradient in B are jump discontinuities;
2.	(Lipschitz Continuity at x*) ∀x ∈ B, ks(x) - s(x*)k2 ≤ Lsks(x*)k2 kx - x* k2;
3.	(Bounded Gradient Norm) ∀x ∈ B, 0 < m ≤ kVc(x)k2 ≤ M;
4.	(Bounded Gradient Difference) ∃δ > 0, ∀x, y ∈ B s.t. y - x = ls(x) for some l,
VTc(y)s(x) ≥ δVTc(x)s(x);
5.	(Constraint Convexity) ∃γ ∈ (0, 1), ∀x ∈ B,
(a(k) Vd(x - x0) + b(k)s(x))TPTP(x - x0) ≥ γ(x - x0)TPTP(x - x0);
6.	(Unique Optimality) x* is the only global optimum within B;
7.	(Constant Bounded Restoration Step Size) α(k) = α < Ma;2
8.	(Shrinking Projected Step Size) β(k) < β∕(k + ko)ν, where 0 < ν < 1 and β ≤ Me, ko > mk;3
|a(k) | < Ma, |b(k)| < Mb;
9.	(Presence in Neighborhood) ∃K, x(K) ∈ int[B], i.e. the interior ofB.
Then we have the convergence guarantee limk→∞ kx(k) - x* k2 = 0.
The proof will be presented in the appendix. Here are a few remarks. First, assumption 1 allows
jump discontinuities in Vc(x) almost everywhere, which is a very practical assumption for deep
neural networks. Most neural network operations, such as ReLU and max-pooling, as well as the
max operation in Eq. (2), introduce nothing beyond jump discontinuities in gradient.
2See Eq. (19) for the definition of Mα in the appendix.
3See Eqs. (50) and (20) for the definitions of Mβ and mk in the appendix.
4
Under review as a conference paper at ICLR 2019
Second, assumption 3 does require the constraint gradient to be lower bounded, which may lead
to concerns that MarginAttack may fail in the presence of gradient masking (Papernot et al.,
2017). However, notice that the gradient boundedness assumption is only imposed in B, which is in
the vicinity of the decision boundary, whereas gradient masking is most likely to appear away from
the decision boundary and where the input features are populated. Besides, as will be discussed
later, a random initialization as in PGD will be adopted to bypass regions with gradient masking.
Experiments on adversarially trained models also verify the robustness of MarginAttack.
Finally, assumption 5 essentially stipulates that c(x) is convex or “not too concave” in B (and thus
so is the constraint set c(x) ≤ 0), so that the first order optimality condition can readily imply local
minimum instead of a local maximum. In fact, it can be shown that assumption 5 can be implied if
c(x) is convex in B.4
3.5	Additional Implementation Details
There are a few additional implementation details as outlined below.
Box Constraint: In many applications, each dimension of the input features should be bounded,
i.e. x ∈ [xmin, xmax]n. To impose the box constraint, the restoration move problem as in Eq. (3) is
modified as
z(k) = arg min	d(x — x(k)), s.t. VTc(x(k))(x — x(k)) = —α(k)c(x(k)),
x∈[xmin,xmax]n
(12)
whose solution is
z(k = Proj[xmin,xmax]n {z(k)}, where Z(k = x(k)
—
α(k)c(x(k)) + Pi∈ιc Vic(x(k))(z(k) — x(k))	(k)
Pi∈I Vic(x(k))si(x(k))	S(X ).
(13)
Proj(∙) is an operator that projects the vector in its argument onto the subset in its subscript. I is a
set of indices with which the elements in z(k) satisfy the box constraint, and IC is its complement.
I is determined by running Eq. (13) iteratively and updating I after each iterations.
Unlike other attack algorithms that simply project the solution onto the constraint box, MarginAt-
TACK incorporates the box constraint in a principled way, such that any local optimal solution x*
will be an invariant point of the restoration move. Thus the convergence is faster.
Target Scan: According to Eq. (2), each restoration move essentially approaches the adversarial
class with the highest logit, but the class with the highest logit may not be the closest. To mitigate
the problem, we follow a similar approach adopted in DeepFool, which we call target scan. Target
scan performs a target-specific restoration move towards each class, and chooses the move with
the shortest distance. Formally, target scan introduces a set of target-specific constraints {ci(x) =
lt(x) - li(x) - ε}. A restoration move with target scan solves
z(k) = arg min d(z(k,i) — x0)	(14)
i∈A
where z(k,i) is the solution to Eqs. (3) or (12) with c(x(k)) replaced with ci(x(k)), and thus is equal
to Eqs. (4) or (13) with c(x(k)) replaced with ci(x(k)). A is a set of candidate adversarial calsses,
which can be all the incorrect classes if the number of classes is small, or which can be a subset
of the adversarial classes with the highest logits otherwise. Experiments show that target scan is
necessary only in the first few restoration moves, when the closest and highest adversarial classes
are likely to be distinct. Therefore, the computation cost will not increase too much.
Initialization: The initialization of x(0) can be either deterministic or random as follows
x(0) = xo (Deterministic), x(0) = xo + u, U 〜U{[-u, u]n} (Random) (15)
where U {[-u, u]n} denotes the uniform random distribution in [-u, u]n. Similar to PGD, we can
perform multiple trials with random initialization to find a better local optimum.
Final Tuning MARGINATTACK can only cause misclassification when c(x) ≤ ε. To make sure
the attack is successful, the final iterations of MarginAttack consists of restoration moves only,
4See Thm. 3 in the appendix.
5
Under review as a conference paper at ICLR 2019
Figure 1: A convergence path of MarginAttack.
Figure 2: An empirical convergence curve of pertur-
bation norm (left) and constraint value (right).
Algorithm 1: MARGINATTACK Procedure
Input : A set of logit functions lo：c-i(x);
an input feature xo and its label t;
Output: A solution x* to Eq. (1)
Initialize x(0) according to Eq. (15);
for k < number of iterations do
if k < number of target SCan iterations then
I Do target scan restoration move as in
I Eq. (14);
else
I Do regular restoration move as in
I Eqs.(3) or(12);
end
if k < final tuning iteration then
I Do projection move as in Eqs. (6);
else
I Skip projection move: x(k+1) = z(k);
end
end
x* = x(k).
and no projection moves, until a misclassification is caused. This can also ensure the final solution
satisfies the box constraint (because only the restoration move incorporates the box constraint).
Summary: Alg. 1 summarizes the MARGINATTACK procedure. As for the complexity, each
restoration move or projection move requires only one backward propagation, and thus the com-
putational complexity of each move is comparable to one iteration of most attack algorithms.
4	Experiments
This section compares MarginAttack with several state-of-the-art adversarial attack algorithms
in terms of the perturbation norm and computation time on image classification benchmarks.
4.1	Attacking Regular Models
4.1.1	Configurations
Three regularly trained models are evaluated on.
•	MNIST (LeCun et al., 1998): The classifier is a stack of two 5 × 5 convolutional layers with 32
and 64 filters respectively, followed by two fully-connected layers with 1,024 hidden units.
•	CIFAR10 (Krizhevsky & Hinton, 2009): The classifier is a pre-trained ResNet32 (He et al., 2016)
provided by TensorFlow.5.
•	ImageNet (Russakovsky et al., 2015): The classifier is a pre-trained ResNet50 (He et al., 2016)
provided by TensorFlow Keras6. Evaluation is on a validation subset containing 10,000 images.
The range of each pixel is [0, 1] for MNIST, and [0, 255] for CIFAR10 and ImageNet. The settings
of MarginAttack and baselines are listed below. Unless stated otherwise, the baseline algorithms
are implemented by cleverhans (Nicolas Papernot, 2017). The hyperparameters are set to defaults if
not specifically stated.
•	CW (Carlini & Wagner, 2017): The target and evaluation norm is `2 . The learning rate is set to
0.05 for MNIST, 0.001 for CIFAR10 and 0.01 for ImageNet, which are tuned to its best perfor-
mance. The number of binary steps for multiplier search is 10.
5https://github.com/tensorflow/models/tree/master/official
6https://www.tensorflow.org/api_docs/python/tf/keras/applications/
ResNet50
6
Under review as a conference paper at ICLR 2019
MNlSTf2 attack
1.0
Cifari2 attack
// / ∖ ---- MARGIN
------------------ CW
I ---- DeepFooI
i1--I-------i-----1-------1-
0.5	1.0	1.5	2.0 2.5 3.0
Distortion (∕2 distance)
we」SSaUUnS
25	50	75	100	125	0
Distortion (∕2 distance)
Cifarfg attack
——MARGIN
CW
---DeepFooI
ι.0
c,s∙i SSaUUnS
ImageNet Z2 attack
50	100	150
Distortion (f2 distance)
ImageNetE8 attack
200
1.0
0.8
ω
≡ 0.6
W
(υ
∪ 0.4
n
S
0.2
0.0
0.0
MNISTiw attack
0.1	0.2	0.3	0.4	0.5	0
Distortion (<s> distance)
c,s∙i SSaUUnS
0.2
1	2	3	4	0.0
Distortion (fn distance)
c,s∙i SSaUUnS
0.5	1.0	1.5
Distortion (f0, distance)
2.0
Figure 3: Adversarial attacks on (left) MNIST, (middle) Cifar, and (right) ImageNet dataset.
•	DeepFool (Moosavi Dezfooli et al., 2016): The evaluation norm is '2.
•	FGSM (Goodfellow et al., 2014): FGSM is implemented by authors. The step size is searched to
achieve zero-confidence attack. The evaluation distance metric is '∞.
•	PGD (Madry et al., 2017): The target and evaluation norm are '∞. The learning rate is set to 0.01
for MNIST, and 0.05 for CIFAR10 and 0.1 for ImageNet.
•	MarginAttack: Two versions of MARGINATTACK are implemented, whose target and eval-
uation norms are '2, and '∞, respectively. The hyperparmeters are detailed in Table 4 in the
appendix. The first 10 restoration moves are with target scan, and the last 20 moves are all
restoration moves.
The number of iterations/moves is set to 2,000 for CW, 200 with 10 random starts for PGD and
MarginAttack (except for ImageNet where there is only one random run), and 200 for the rest.
4.1.2 Results and Analyses
Except for PGD, all the other attacks are zero-confidence attacks. For these attacks, we plot the
CDF of the margins of the validation data, which can also be interpreted as the percentage success
rate of these attacks as a function of perturbation level. Figure 3 plots the success rate curves, where
the upper panel shows the '2 attacks, and the lower one shows '∞ attacks. As can be observed, the
MarginAttack curves are above all other algorithms at all perturbation levels and in all datasets.
CW is very close to MarginAttack on MNIST and CIFAR10, but MarginAttack maintains
a 3% advantage on MNIST and 1% on CIFAR10. It seems that CW is unable to converge well
within 2,000 iterations on ImageNet, although the learning rate has been tuned to maximize its
performance. MarginAttack, on the other hand, converges more efficiently and consistently.
To obtain a success rate curve for PGD, we have to run the attack again and again for many different
perturbation levels, which can be time-consuming for large datasets (this shows an advantage of
zero-confidence attacks over fix-perturbation attacks). Instead, we choose four perturbation levels
for each attack scenario to compare. The perturbation levels are chosen to roughly follow the 0.2,
0.4, 0.6 and 0.8 quantiles of the MarginAttack margins. Table 1 compares the success rates
under the chosen quantiles among the '∞ attacks. We can see that MARGINATTACK outperforms
PGD under all the perturbation levels, and that both significantly dominate FGSM.
4.2 Attacking Adversarially Trained Model
We also evaluate MarginAttack on the MNIST Adversarial Examples Challenge7, which is a
challenge of attacking an MNIST model adversarially trained using PGD with 0.3 perturbation level.
7https://github.com/MadryLab/mnist_challenge
7
Under review as a conference paper at ICLR 2019
Table 1: Success rate (%) of adversarial attacks under given perturbation norms.
Algorithm	MNIST	CIFAR	IMAGENET
0.06 / 0.08/0.10/0.12	0.2 / 0.4 /0.6/ 1	0.05 / 0.1 / 0.2 / 0.4
FGSM	7.55/13.9/24.9/35.4^^18.5/31.0/41.1/54.7^^39.8/47.2/60.1 /75.3
PGD	17.1 /42.2/73.7/91.8	18.9 / 38.9 / 59.1 / 84.1	40.4 / 49.8 / 68.8 / 90.6
Ours	18.1 / 43.0 / 74.1 / 92.1	21.1 /42.2/62.6/87.3	41.5 / 51.3 / 69.0 / 90.8
Table 3: Running time comparison (in sec-
onds) on a single batch of images.
Table 2: Success rate under 0.3 perturbation norm
of the MNIST Adversarial Examples Challenge.
Algorithm	Success Rate (%)	Algorithm	Mnist	Cifar	ImageNet
Zheng et al. (2018)	11.21	CW	16.02	234.75	872.28
MarginAttack ('∞)	11.16	DeepFool	1.14	21.26	44.41
1st-Order on Logit Diff	11.15	PGD	0.87	33.17	46.3
PGD on Cross-Entropy Loss	10.38	FSGM	0.11	0.95	10.05
PGD on CW Loss	10.29	Ours ('2)	3.01	51.03	248.82
Same as the PGD baseline listed, MarginAttack is run with 50 random starts, and the initializa-
tion perturbation range U = 0.3. The number of moves is 500. The target norm is '∞. bn, = 5 and
an is set as in Eq. (10). The rest of the configuration is the same as in the previous experiments.
Table 2 lists the success rates of different attacks under 0.3 perturbation level. The baseline algo-
rithms are all fix-perturbation attacks, and their results are excerpted from the challenge white-box
attack leaderboard. As can be seen, MarginAttack, as the only zero-confidence attack algorithm,
has the second best result, which shows that it performs competitively against the state-of-the-art
fix-perturbation attacks.
4.3 Convergence
We would like to revisit the convergence plot of the constraint value c(x) and perturbation norm
d(x) of as in Fig. 2. We can see that MARGINATTACK converges very quickly. In the example
shown in the figure, it is able to converge within 20 moves. Therefore, MarginAttack can be
greatly accelerated. If margin accuracy is the priority, a large number of moves, e.g. 200 as in our
experiment, would help. However, if efficiency is the priory, a small number of moves, e.g. 30,
suffices to produce a decent attack.
To further assess the efficiency of MarginAttack, Tab. 3 compares the running time (in seconds)
of attacking one batch of images, implemented on a single NVIDIA TESLA P100 GPU. The batch
size is 200 for MNIST and CIFAR10, and 100 for ImageNet. The settings are the same as stated
in section 4.1, except that for a better comparison, the number of iterations of CW is cut down to
200, and PGD and MarginAttack runs one random pass, so that all the algorithms have the same
iteration/moves. Only the `2 versions of MARGINATTACK are shown because the other versions
have similar run times. As shown, running time of MarginAttack is much shorter than CW, and
is comparable to DeepFool and PGD. CW is significantly slower that the other algorithms because
it has to run multiple trials to search for the best Lagrange multiplier. Note that DeepFool and CW
enable early stop, but MarginAttack does not. Considering MarginAttack’s fast convergence
rate, the running time can be further reduced by early stop.
5 conclusion
We have proposed MarginAttack, a novel zero-confidence adversarial attack algorithm that is
better able to find a smaller perturbation that results in misclassification. Both theoretical and em-
pirical analyses have demonstrated that MarginAttack is an efficient, reliable and accurate adver-
sarial attack algorithm, and establishes a new state-of-the-art among zero-confidence attacks. What
is more, MARGINATTACK still has room for improvement. So far, only two settings of a(k) and b(k)
are developed, but MarginAttack will work for many other settings, as long as assumption 5 is
satisfied. Authors hereby encourage exploring novel and better settings for the MarginAttack
framework, and promote MarginAttack as a new robustness evaluation measure or baseline in
the field of adversarial attack and defense.
8
Under review as a conference paper at ICLR 2019
References
Naveed Akhtar and Ajmal Mian. Threat of adversarial attacks on deep learning in computer vision:
A survey. arXiv preprint arXiv:1801.00553, 2018.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In
Security and Privacy (SP), 2017 IEEE Symposium on,pp. 39-57. IEEE, 2017.
Ian Goodfellow. Gradient masking causes CLEVER to overestimate adversarial perturbation size.
arXiv preprint arXiv:1804.07870, 2018.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adversarial attacks with
limited queries and information. arXiv preprint arXiv:1804.08598, 2018.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world.
arXiv preprint arXiv:1607.02533, 2016a.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv
preprint arXiv:1611.01236, 2016b.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Seyed Mohsen Moosavi Dezfooli, Alhussein Fawzi, and Pascal Frossard. DeepFool: a simple and
accurate method to fool deep neural networks. In Proceedings of 2016 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), number EPFL-CONF-218057, 2016.
Ian Goodfellow Reuben Feinman Fartash Faghri Alexander Matyasko Karen Hambardzumyan Yi-
Lin Juang Alexey Kurakin Ryan Sheatsley Abhibhav Garg Yen-Chen Lin Nicolas Papernot,
Nicholas Carlini. cleverhans v2.0.0: an adversarial machine learning library. arXiv preprint
arXiv:1610.00768, 2017.
Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram
Swami. The limitations of deep learning in adversarial settings. In Security and Privacy (Eu-
roS&P), 2016 IEEE European Symposium on, pp. 372-387. IEEE, 2016.
Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram
Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM
on Asia Conference on Computer and Communications Security, pp. 506-519. ACM, 2017.
JB Rosen. The gradient projection method for nonlinear programming. part ii. nonlinear constraints.
Journal of the Society for Industrial and Applied Mathematics, 9(4):514-532, 1961.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual
recognition challenge. International Journal of Computer Vision, 115(3):211-252, 2015.
Jiawei Su, Danilo Vasconcellos Vargas, and Sakurai Kouichi. One pixel attack for fooling deep
neural networks. arXiv preprint arXiv:1710.08864, 2017.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
9
Under review as a conference paper at ICLR 2019
Tsui-Wei Weng, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Dong Su, Yupeng Gao, Cho-Jui Hsieh, and
Luca Daniel. Evaluating the robustness of neural networks: An extreme value theory approach.
arXiv preprint arXiv:1801.10578, 2018.
Tianhang Zheng, Changyou Chen, and Kui Ren. Distributionally adversarial attack. arXiv preprint
arXiv:1808.05537, 2018.
10
Under review as a conference paper at ICLR 2019
Appendix
A Proving Thm. 1
This supplementary material aims to prove Thm. 1. Without the loss of generality, K in Eq. (9) in
set to 0. Before we prove the theorem, we need to introduce some lemmas.
Lemma 1.1. If assumption 3 in Thm. 1 holds, then ∀x ∈ B
VT c(x)s(x) m
Ils(x)k2	≥ √n
(16)
Proof. According to Eq. (5), for `2 norm,
for '∞ norm,
VTc(X)S(X)
Ils(x)k2
IVc(x)I2 ≥ m >
m
√n
VTc(x)s(x)	=	IIVc(X)kι	≥	IIVc(X)∣2 ≥	m
Ils(x)k2	— Ils(x)k2	—	√n —	√n
Lemma 1.2. Given all the assumptions in Thm. 1, where
Mα = min
1, sup	x,y∈B:
∃l,y-x=ls(x)
VT c(y)s(X)	mγ
VTc(x)s(x), 2nLsK
mk= (AT/2v - 1)1
and assuming X(k), z(k) ∈ B, ∀k, then we have
|c(X(k))| ≤ κβ(k)
where
B	c(X(0))
K = max < —----------,——77^- >
l√A(1 -√A), β ⑼ /
A and B are defined in Eq. (32).
According to assumption 8, this implies
lim |c(X(k))| = 0
k→∞
at the rate of at least 1∕nν.
(17)
(18)
□
(19)
(20)
(21)
(22)
(23)
Proof. As a digression, the second term in Eq. (19) is well defined, because
VT c(y)s(x)
VT c(x)s(x)
is upper bounded by Lem. 1.1 and assumptions 3.
Back to proving the lemma, we will prove that each restoration move will bring c(X(k)) closer to 0,
while each projection move will not change c(X(k)) much.
First, for the restoration move
|c(z(k))| ≤ |c(X(k)) + VT c(ξ)(z(k) -X(k))|
c(X(k)) -
α(k)VT c(ξ)
c(x(k))s(x(k))
VT c(x(k))s(x(k))
VT c(ξ)s(X(k))
1 — α-------------------
VT c(x(k))s(x(k))
|c(X(k))|
(24)
≤ (1 — αδ)∣c(x(k))∣
11
Under review as a conference paper at ICLR 2019
The first line is from the generalization of Mean-Value Theorem with jump discontinuities, and
ξ = tz(k) + (1 - t)x(k) and t is a real number in [0, 1]. The second line is from Eq. (4). The last
line is from assumptions 4 and 7 and Eq. (19).
Next, for the projection move
|c(x(k+1))| ≤ |c(z(k))| +Mkx(k+1) - z(k)k2
=∣c(z(k))∣ + β(k)M∣∣a(k)Vd(z(k) - x0) + b(k)s(z(k))k2	(25)
≤ |c(z(k))| + β(k)M Wk)|Vd(z(k) - xo)∣2 + 从k)ks(z(k))k2)
The first line is from the fact that assumption 3 implies that c(x) is M -Lipschitz continuous.
Both ∣Vd(z(k) - x0)∣2 and ∣s(z(k))∣2 is upper-bounded, i.e.
∣Vd(z(k) - x0)∣2 < Md,	∣s(z(k))∣2 < Ms	(26)
for some Md and Ms. To see this, for `2 norm
∣Vd(z(k) -x0)∣2 = 2∣z(k) -x0∣2 ≤2b, ∣s(z(k))∣2 = 1	(27)
where b is defined as the maximum perturbation norm ('2) within B, i.e.
b = max∣x0 - x∣2	(28)
x∈B
which is well defined because B is a tight set. For '∞ norm,
∣vd(z(k) - χ0)k2 ≤ √n,	∣s(z(k))∣2 ≤ √n	(29)
Note that Eq. (26) also holds for other norms. With Eq. (26) and assumption 8, Eq. (25) becomes
|c(x(k+1))| ≤ |c(z(k))| + β(k)M (MaMd + MbMs)	(30)
Combining Eqs. (24) and (30) we have
|c(x(k+1))| ≤ A|c(x(k))| + β(k)B	(31)
where
A = 1 - αδ
B=M(MaMd+MbMs)	(32)
According to assumption 7, 0 < A < 1. Also, according to Eq. (20), β(k)∕β(k+1) ≤ A-1/2, ∀k,
Divide Eq (31) by β(k), we have
|c(x(k+I))I < |c(x(k+1))|
β(k + 1)
√Aβ(k)
≤ AA-
|c(x(k))|
β (k)
and thus
If
|c(x(k+1))I _ B	≤√A (|c(x(k))I _ B
β(k+1)	√A(1 -√A) ~ I β(k)	√A(1 -√A)
Ic(X(O) )∣	B
β(O)	√A(1 - √A)
≤0
Then Eq. (34) implies
Ic(x(k))I	B
----TTT-- - ---------------
β(k)-----√A(1 - √A)
≤ 0, ∀k
Otherwise, Eq. (34) implies
|c(x(k))I _ B ≤ Ic(X(O))I _ B	∀k
β(k)	√A(1 -√A) - β(O)	√A(1 - √A),
(33)
(34)
(35)
(36)
B
+ 1
This concludes the proof.
□
12
Under review as a conference paper at ICLR 2019
Lemma 1.3. Given all the assumptions in Thm. 1, and assuming x(k), z(k) ∈ B, ∀k, we have
lim kP (x(k) - x0)k22 = 0
k→∞
(37)
Proof. First, for restoration move
kP [z(k) -x0]k22
= kP [x(k) - x0]
≤ kP [x(k) - x0]
= kP [x(k) - x0]
= kP [x(k) - x0]
= kP [x(k) - x0]
≤ kP [x(k) - x0]
≤ kP [x(k) - x0]
≤ kP [x(k) - x0]
≤ kP [x(k) - x0]
22 + 2[z(k) - x0]TPTP [z(k) - x(k)] - kP [z(k) - x(k)]k22
22 + 2kP [z(k) - x0]k2kP [z(k) -x(k)]k2
22 + 2αkP [z(k) -x0]k2
22 + 2αkP [z(k) -x0]k2
22 + 2αkP [z(k) -x0]k2
c(x(k))
VT c(x(k))s(x(k))
c(x(k))
VT c(x(k))s(x(k))
P s(x(k))
2
P [s(x(k)) — s(x*)]
c(x(k))VTc(x*)(s(x(k)) - s(x*)) , *)
VTc(x(k))s(x(k))VTc(x*)s(x*) S X
2 + 2an|c(x(k))|kp Wk)- χo ]k2∣
m
2 + 2αnLsκβ(k) kPWk)-X0]∣∣2k
ks(x(k)) - s(x*)k2
ks(x(k))k2
x(k) — X*∣∣2
m
2 + 2αnLsκβ(k) kP[z(k) - X0]k2 [kP[χ(k) -
2+
2+
m
2ɑnLsκβ(k)(m + M √n)
x0]k2 +
c(x(k)) + M kP [x(k) - x0]k2
δm2
kP [z(k) - x0]k2kP [x(k) - x0]k2 +
δm∕√n
2α√nLsK2 β(k)2
Line 4 is given by Eq. (3). Line 5 is derived from Lem. 1.1. The last line is from Lem. 1.2.
δm2
(38)
2
2
Eq. (38) implies
(kP [z(k)-x0]k2-r1(k)kP [x(k)-x0]k2)(kP [z(k)-x0]k2-r2(k)kP [x(k)-x0]k2) ≤
2α√nLsK2 β(k)2
δm2
where
(39)
1	2αnLsκβ(k)
2αnLsκβ(k) Y +4] < 0
1	2αnLsκβ(k)
(2°nLsκβ (k) Y +4	> 0
(40)
2
—
m
m
2
m
m
It can easily be shown that
(k)	(k)2	(k)2	(0)2
-r1	= —r1--------= r1— ≥ r1—
r(k)	-r(k)r(k)	4	-	4
Therefore
(0)2
kP[z(k) - X0]k2 - r(k)kP[x(k) - X0]k2 ≥ r1^(∖∖P[z(k) - X0]k2 + r2k) kP[x(k) - X0]k2)
Combining Eqs. (39) and (42), we have
kP[z(k) - X0]k2 ≤ r(k)2kP[x(k) - xo]k2 + Y'KOT"
δm2 r1
< (1+2αnLmκβ(k) j ∖p [χ(k) - χ0]k2+，二 TT
(41)
(42)
(43)
13
Under review as a conference paper at ICLR 2019
Next, for projection move
kP[x(k+1) -x0]k22
= kP [z(k) - x0]k22 + 2[x(k+1) - z(k)]TP [z(k) -x0] + kx(k+1) - z(k)k22
=∣∣P[z(k) - xo]k2 + 2β(k)[a(k)Vd(z(k) - xo) + b(k)s(z(k))]TP[z(k) - xo]	(44)
+ β(k)2ka(k)Vd(z(k) - xo) + b(k)s(z(k))∣2
≤ (1 - 2β(k)γ)∣P [z(k) - xo]∣22 + β(k)2B2
The second equality is from Eq. (6). The last line is from assumption 5 and Eq. (31).
Next, combining Eqs. (43) and (44), we have
∣∣x(k+1) - P[x(k+1)]∣2 ≤ (1+ 2αnLsκβ(k)) (1 - 2β(k)γ)∣χ(k) - P[χ(k)]∣2 + β(k)2D
2m	2
≤ (1 - 2β(k)F)∣x(k) - P [x(k)]∣22 +β(k)2D
(45)
where
D = E + B2
b _ 8α√nLsK2
=δm2r 严	(46)
2αnLsκ
F = Y-----------> 0 (Assumption 7)
m
According to assumption 8, limk→∞ β(k) = 0. Thus, ∀ > 0, ∃K(), ∀k > K(), we have
β(k) ≤ 2γε∕D. Therefore
∣x(k+1) - P [x(k+1)]∣22 - ≤ (1 - 2F β(k))(∣x(k) - P [x(k)]∣22 - )	(47)
Finally, notice that by assumption 8
K
lim Y(1 - 2β(k)F) =0	(48)
K→∞
k=1
then
lim ∣x(k) - P [x(k)]∣22 ≤	(49)
k→∞
which holds ∀ > 0. This concludes the proof.	□
Lemma 1.4. Given all the assumptions in Thm. 1, where Mα and mk defined in Eqs. (19) and (20),
and
“	C	∕C(1 - A) 2FX 1	/
Mβ = min f,V⅜"j, 丁，2F，V-
X
r(O)2D∕2F + E
E
where A, B, D and E are defined in Eqs. (32) and (46), the following inequalities hold ∀k.
|c(x(k))| ≤C
|c(z(k))| ≤ C
∣P [x(k) - xo]∣22 ≤ X
∣P [z(k) - xo]∣22 ≤ X
(50)
(51)
i.e. x(k), z(k) ∈ B,∀k.
Proof. We will prove it by mathematical induction.
Base Case: From assumption 9, we have |c(x(o))| ≤C and ∣P [x(o) - xo]∣22 ≤ X. Thus Eqs. (24)
and (43) hold for k = 0. From Eq. (24) and assumption 7, we have |c(z(o))| ≤ |c(x(o))| ≤C . From
Eqs. (43) and (50), we have ∣P[z(o) - xo]∣22 ≤ X.
14
Under review as a conference paper at ICLR 2019
Step Case: Assume Eq. (51) holds ∀k ≤ K, then Eqs. (31) and (45) holds ∀k ≤ K.
• Proving |c(x(K+1) )| ≤ C:
From Eq. (31),
If
Then
|c(x(K+1))| -
β(K)2B
1 - A
≤ A |c(x(K))|
|c(x(K))| ≤
β(K)2B
1 - A
|c(x(K+1))| ≤
β(K)2B
1 - A
β(0)2B
≤ 1 - A
where the last inequality is given by Eq. (50).
β(K)2B ∖
T-A )
≤C
Otherwise
|c(x(K+1))| ≤ |c(x(K))| ≤ C
• Proving kP [x(K+1) - x0]k22 ≤ X:
From Eq. (45)
kP[χ(K+1) - χo]k2 - βKD ≤ (1 - 2β(K)F) (kP[χ(K)-X川2 - βKD
2F	2F
Notice that from Eq. (50), 0 ≤ (1 - 2β(0)F) ≤ (1 - 2β(K)F) < 1.
If
kP[x(K)-X0]k2 ≤ β(JD
2F
Then
kP[X(K+1)-X0]k2 ≤ 耳 ≤ β2FD ≤ X
where the last inequality is given by Eq. (50).
Otherwise
kP[X(K+1) -X0]k22 ≤ kP[X(K) -X0]k22 ≤X
• Proving |c(z(K+1))| ≤ C:
Since we have established |c(X(K+1))| ≤ C, Eq. 24 holds for k = K + 1. Therefore
|c(z(K+1))| =A|c(X(K+1))| ≤ |c(X(K+1))| ≤C
• Proving kP[z(K+1) - X0]k22 ≤ X:
Since we have established kP[X(K+1) - X0]k22 ≤ X, Eq. (43) holds for k = K + 1.
From Eqs. (56) and (57), we can establish, through recursion, that
kP[x(k) - X0]k2 ≤ max {β2FD, kP[x(O)- X0]∣" ,∀k ≤ K +1
Therefore,
kP[z(K+1) -x0]k22 ≤r1(K+1)2kP[x(K+1) -x0]k22+β(K+1)2E
≤ r(K+1)2 max {β2FD, kP[x(0) - X0]∣" + β(K+1)2E
≤ r(0)2 max { β2FD, kP[x(O)-X0] k2 } + β(O)2E
≤X
The first line is given by Eq. (43). The last line is given by Eq. (50).
(52)
(53)
(54)
(55)
(56)
(57)
(58)
(59)
(60)
□
15
Under review as a conference paper at ICLR 2019
Lemma 1.5. Under the assumptions in Thm. 1
P [x* — xo] = 0
(61)
Proof. From Thm. 2, a solution, denoted as x0, to
min d(x — x0)
x	(62)
s.t.VT c(x*)(x — x*) = 0
would satisfy
P[x0 — x0] = 0	(63)
If P[x* — xo] = 0, there are two possibilities. The first possibility is that x* is not a solution to
Eq. (62), which contradicts with the first order optimality condition that x* must satisfy.
The second possibility is there are multiple solutions to the problem in Eq. (62), and x0 and x* are
both its solutions. This can happen if d(∙) is 'ι or '∞ norm. By definition
VTc(x*)(x0 — x*) = 0	(64)
Since x* is a local minimum to Eq. (1), ∃j ∈ I, ε < 1, ∀δ < ε, xδ = δx* + (1 — δ)x0, s.t.
c(xδ) > 0, xδ ∈ Bj	(65)
Otherwise, if c(xδ) ≤ 0, then xδ is a feasible solution to the problem in Eq. (1) and
d(xδ — xo) ≤ δd(x* — xo) + (1 — δ)d(x0 — xo) = d(x* — xo)	(66)
which contradicts with the assumption that x* is a unique local optimum in B.
Eq. (65) implies
VjTc(xδ)(xδ — x*) = (1 — δ)VjTc(xδ)(x0 — x*) > 0	(67)
On the other hand, notice that Eq. (63) implies s(x*) = λ(xo — x0), λ > 0. For 'i∕'∞ cases, Sj(x)
takes discrete values. Therefore, to satisfy assumption 2, sj(xδ) = sj(x*), which implies
0 = d(x*) — d(x*) ≥ —	λjVTcj(xδ) +	λiVTci(x*)	(x* — x0), λi > 0, i ∈ I
i∈I,i6=j
(68)
The first inequality is because —
λjVTcj(xδ) + Pi∈I,i6=j λiVTci(x*)	∈ VsT d(x0 — xo).
Eqs. (67) and (68) cause a contradiction.
□
Now we are ready to prove Thm. 1.
Proof of Thm. 1. From Lems. 1.2, 1.3 and 1.4, we can established that Eqs. (21) and (37) holds
under all the assumptions in Thm. 1. The only thing we need to prove is that Eqs. (21) and (37)
necessarily implies limk→ kx(k) — xok2 = 0.
First, from Lem. 1.5
kP [x* —xo]k2 =0	(69)
Then, ∀x0 ∈ B s.t. kP [x0 — xo]k22 = 0, we have x0 — x = λs(x0). From assumption 4, we know
that c(x0) is monotonic along x0 — x = λs(x0). Therefore, x* is the only point in B that satisfies
kP [x0 — xo]k22 = 0 and c(x0) = 0.
Also, notice that P[x — xo] and C(X) are both continuous mappings. This concludes the proof. □
Theorem 2. The solution to
is
arg min d(x)
x
s.t. VTc(x0)x = b
bs(x0)
X	VT c(x0 )s(x0)
(70)
(71)
16
Under review as a conference paper at ICLR 2019
Proof. Decompose x = λy, where d(y) = 1. Then Eq. (70) can be rewritten as
arg min λ
λ,y∙∙d(y)=1
s.t. λVTc(x0)y = b
(72)
Notice that the product of λ and VTc(x0)y is constant, so if λ is to be minimized, then VTc(x0)y
needs to be maximized. Namely, y can be determined by solving
max VTc(x0)y	(73)
y:d(y)=1
which is the definition of dual norm. Therefore
y = s(x0)	(74)
Plug Eq. (74) into the constraint in Eq. (72), we can solve for λ. This concludes the proof.
□
As a remark, Thm. 2 is applicable to the optimization problems in Eqs. (3) and (62) by changing the
variable X = X - xo and redefining b accordingly.
Theorem 3. For `2 norm, if all the assumptions in Thm. 1, but assumption 5, hold, and
∃ma > 0, s.t. a(k) ≥ ma	(75)
also assuming C(X) is convex in B, then ∃B0 = {x : ∣∣P[x 一 x*]∣∣2 ≤ X0, |c(x)| ≤ C0} ⊂ B, s.t.
assumption 5 hold.
Proof. Since c(X) is convex in B, we have ∀X
(VTc(X) — VTc(x*))(x — x*) ≥ 0	(76)
Further, assume X satisfies
VT c(x*)(x — x*) = 0	(77)
Then we have
P T P (X — x*) = P (X — x*) = x — x*	(78)
where the first equality is from the fact that P is an orthogonal projection matrix under `2 norm;
the second equality is from the fact that the projection subspace of P is orthogonal to Vc(X*) by
construction.
Also, from Lem. 1.5, we have
P(X — X*) = P(X — X0)	(79)
Plug Eqs. (77) to (79) into (76), we have
VTc(X)PTP(X — X*) ≥ 0	(80)
On the other hand, let γ = ma/2kX* — X0k2, then ∀x satisfying Eq. (77) and X 6= X*
a(k)
a(k)VT d(x — Xo)P T P(X — Xo) = η---π-(x - Xo)T P T P (X - Xo)
kX — X0k2
≥ II *ma II (x — xo)TPTP(x — Xo)
kX* — Xok2
> γ(X — Xo)TPTP(X — Xo)
(81)
where the second line comes from Eq. (75) and the fact that X* is the optimal solution to the problem
in Eq. (62). Combining Eqs. (80) and (81), we know that assumption 5 holds with strict inequality
for X satisfying Eq. (77) and X 6= X*.
VTc(X)PTP(X —X*), VT d(X — Xo)PTP(X — Xo) and γ(X — Xo)TPTP(X — Xo) are continuous
functions, and therefore ∃B0 where assumption 5 also holds. This concludes the proof.
□
B Hyperparameter Settings for MarginAttack
Table 4 list the hyperparameter settings for MarginAttack.
17
Under review as a conference paper at ICLR 2019
Table 4: Hyperparameter settings for MarginAttack.
Hyper- parameters	MNIST	`2 Cifar	ImageNet	Mnist	'∞ Cifar0	ImageNet
a(k)	-Eq. (8)^^	Eq. (8)	^^Eq. (8)	0.1	1	0.3
b(k)	1	1	1	Eq. (10)	Eq. (10)	Eq. (10)
α	1	1	1	0.2	0.2	0.2
β(k)	(k + 1)-0.5	(k + 1)-0.5	(k + 1)-0.5	(k + 1)-1	(k + 1)-1	(k + 1)-1
u	0.05	0.1	0	0.05	0.1	0
ε		-0.01						
18