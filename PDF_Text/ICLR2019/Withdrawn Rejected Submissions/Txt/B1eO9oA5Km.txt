Under review as a conference paper at ICLR 2019
A Guider Network for Multi-Dual Learning
Anonymous authors
Paper under double-blind review
Ab stract
A large amount of parallel data is needed to train a strong neural machine translation
(NMT) system. This is a major challenge for low-resource languages. Building on
recent work on unsupervised and semi-supervised methods, we propose a multi-
dual learning framework to improve the performance of NMT by using an almost
infinite amount of available monolingual data and some parallel data of other
languages. Since our framework involves multiple languages and components, we
further propose a timing optimization method that uses reinforcement learning
(RL) to optimally schedule the different components in order to avoid imbalanced
training. Experimental results demonstrate the validity of our model, and confirm
its superiority to existing dual learning methods.
1	Introduction
Neural machine translation (NMT) has significantly improved the quality of machine translation in
recent several years (He et al., 2017; Cho et al., 2014; Bahdanau et al., 2014; Sutskever et al., 2014).
The availability of high quality large-scale parallel bilingual corpora made this success possible (He
et al., 2016; Lample et al., 2017; Artetxe et al., 2017; Koehn & Knowles, 2017). However, collecting
such parallel corpora remained to be a practical challenge for the large majority of language pairs,
such as Basque and German-Russian.
To address this issue, different methods have been proposed recently. They can be broadly classified
into two categories: (1) low parallel data of source and target languages are required but heavily
depend on third-party resources, such as a large amount of parallel data of another languages (Dong
et al., 2015; Firat et al., 2016a; ZoPh & Knight, 2016; Johnson et al., 2016; Libovicky & Helcl, 2017),
a special dataset in which same word has same distribution between source and target (Artetxe
et al., 2017; LamPle et al., 2017), images or other multi-media resources corresPonding exPlanation
languages (Chen et al., 2018; Cheng et al., 2017; Johnson et al., 2016). (2) using monolingual data to
imProve the NMT Performance, but a Pre-trained high quality NMT model is required. For examPle,
Zhang & Zong (2016); Zhang et al. (2018) enlarged the training dataset by using a Pre-trained
translation model to translate the monolingual data into Pseudo bilingual sentence Pairs to address
the shortage of Parallel training data.
Recent dual learning Provided a way to enhance model’s Performance using unlabeled data (He et al.,
2016; Xia et al., 2017; Wang et al., 2018; Luo et al., 2017; Yi et al., 2017; Zhu et al., 2017). As
described in the left Panel of Figure 1, dual learning uPdates the whole system according to Yi0 which
is a forward translation can reconstruct X better and has high language fluency (scored by language
model) among all the samPled Y0 (He et al., 2016; Tu et al., 2017). However, the best direction to
uPdate Parameters heavily relies on the quality of samPled translations Y10, Y20, ... which may be far
from real translations Y due to inaccurate translations existing in the samPled ones, esPecially for the
case where there is no enough data to train a NMT model with high Performance. In that case, the
dual learning framework suffers from learning a Poor maPPing between X and Yi0 . Existing dual
learning cannot Provide another oPtimal uPdating direction to helP train a strong NMT system.
InsPired by the two kinds of work, we ProPose a new guided learning framework (GLF) that can
alleviate the disadvantages in the Prior works and address the Problem of Parallel corPora shortage.
The key comPonent in our framework is a guider network (GN), and different from existing dual
learning, GN can Provide new learning directions for NMT by leveraging monolingual data of
different languages. SPecifically, the ProPosed GN is trained using monolingual data and then used to
1
Under review as a conference paper at ICLR 2019
Translation X->Y
X ♦
Reconstruction
Translation Y->X
(Dual Learning)
Figure 1: The primary idea of our proposal (right) and dual learning (left)
Encoder's
hidden
Decoder's
hidden
Same
information
space
(OUr proposal)
guide the training of NMT.1 Note that GN has a flexible structure, which can reconstruct the input
source sentence not only from the encoder but also from the decoders of NMT systems for different
languages. In that case, as shown in the right panel in Figure 1, GN keeps the hidden states of encoder
and decoder consistent and forces the NMT system to map them to the same information space. Well
matched encoder and decoder will bring a better word alignment and yield better translation as we
will see in Section 3. And GN thus offers a better updating direction for the NMT model in the dual
learning process. Further, benefiting from the flexible structure of GN, we expand GLF to address
multilingual translation completed by guiding all the decoders of different languages at the meantime
through GN. Ensemble output of different languages can be regard as a normalization operation that
can accelerate deep network training according to Ioffe & Szegedy (2015).
With an increasing number of languages, training order for different components in a complex model
significantly affects the final training results since different models have different convergence rates.
In traditional fixed order sequential training setting, different convergence rates will cause some
models to be over-fitted while the others still have not converged. In that case, the over-fitted models
will damage the whole system and yield poor performance. To address this problem, we propose
a novel timing optimization method by leveraging reinforcement learning (RL) (Mnih et al., 2013;
2015; Wu et al., 2018) to schedule the training order. To the best of our knowledge, this issue has not
been studied before.
2	Background and Related Work
Multilingual Promotion: Recent NMT studies achieved better performances by extending sequence-
to-sequence model architectures to multilingual translation. By allocating separate encoder and
decoder for each language, different interactive modes have been found effective, such as, one source
to many targets (Dong et al., 2015), many sources to one target (Zoph & Knight, 2016; Firat et al.,
2016b), many sources to many targets (Luong et al., 2015; Firat et al., 2016a), while some gain
enhancement by leveraging universal encoder-decoder (Johnson et al., 2016; Ha et al., 2016). The
others use an inter-lingual to improve the translation quality (Lu et al., 2018).
Monolingual Promotion: The most relevant work to our model is dual learning (He et al., 2016),
which enables an NMT system to automatically learn from unlabeled data through a dual-learning
game. It aims to train two dual NMTs in a semi-supervised way. It translates monolingual data of
source to the target side using one of the two NMTs, then translates it back to the original source
by the other one. Then the whole process can be implemented from a dual view. This framework
considers the language model score and reconstruction quality as rewards and trains using a policy
gradient method. Some previous works (Xia et al., 2017; Wang et al., 2018) utilized language model
to enhance NMT systems. Methods mentioned above achieved some improvements, although not in
an end-to-end manner. Others take unlabeled data to reinforce the encoder (Zhang & Zong, 2016) or
the decoder (Sennrich et al., 2016) directly. In this paper, we propose a new framework to exploit
unlabeled data in an end-to-end way, which improves the encoder and decoder.
Module Scheduling: Multi-lingual models naturally come up with a scheduling issue for it consists
of several models, i.e., how to decide which component of the whole system to train next. Recently,
methods in Wang et al. (2016); Sung et al. (2017) used on a “teacher-like” critic net to guide the
basic task’s training in a meta-learning setting. Yang Fan introduces a "teacher" network trained by
reinforcement learning to obtain the next piece of pre-divided data set, which is able to accelerate
the basic task’s training. On another purpose of enhancing the basic model performance, (Wu et al.,
2018) suggested that learning a critic can help ease the problem of biased unlabeled data, and increase
the classification accuracy.
1The training data includes monolingual data and parallel bilingual data of some other languages.
2
Under review as a conference paper at ICLR 2019
In our work, unlike multi-lingual translation model in Firat et al. (2016a), which was trained in a
fixed schedule, we rely on an additional "teacher-like" model to decide which module should train
next in order to obtain a balance gain and a better performance. We utilize the deep Q-learning (Mnih
et al., 2013; Wang et al., 2015; Van Hasselt et al., 2016) algorithm to train a critic network to schedule
the training of models.
3	Guided Learning Framework (GLF)
This section describes the proposed Guided Learning Framework and its main component guider
network (GN) which is also called the general generator since it can be used as a generator directly.
We first describe the correlation between the source and target languages using the bi-directional
entropy and information space (Section 3.1). The correlation is the basis of GN. Then we present the
architecture of GLF and the guider network in Section 3.2.
3.1	Information Space Behind Hidden States
Attention mechanism (Bahdanau et al., 2014; Luong et al., 2015) is an effective method to improve the
performance of sequence-to-sequence (seq2seq) models (Sutskever et al., 2014) (the basic framework
of NMT). In fact, attention is a kind of operation in the information space of hidden states. We
thus use attention to explain the information space. The well-known attention mechanism can be
formulated by ci = Ptn=1 αti ht where ht is the hidden state of encoder at position t, αti is the
weighting coefficient indicating how well ht and hi match, ci is the context vector associated with
the hidden state hi in decoder, and then ci is involved to predict the output word directly. Here we
propose ci to be a vector in a space expanded by basis (h1, h2, ..., hN). We call this space (which can
be used to represent the input sentence) the Information Space of the Encoder (ISE). Corresponding
to this, we also get a Information Space of the Decoder, (ISD).
We call a ISE (or ISD) CISE (or CISD), if the basis of the ISE (or ISD) can fully represent the whole
sentence. That is, a one-to-one mapping exists between the words (with different meanings) and their
corresponding hidden states (Bahdanau et al., 2014). CISE can ensure a good translation since it can
provide sufficient information of the source sentence which is needed by the decoder to yield a better
translation. CISD can indicates the quality of obtained translation. That is because a good translation’s
hidden states must expand a CISD. However, how to judge whether the ISD (or ISE) is CISD (or
CISE) is the major challenge. Here we propose a bi-direction attention entropy (BDE) to judge it:
NM
B = -	pij log pij + pji log pji	(1)
i=1 j=1
where pij is the forward matching probability from the i-th
hidden state in encoder hi (column vector) to the j-th hidden
state in decoder hj (column vector), and pji is the backward
matching probability. The only difference between the forward
and backward matching probability is the softmax scope: pij =
PMlij— ； Pji = PNeij-, eij = hT ∙ hj scores how well hT
j=1 eij	i=1 eij
and hj match.
Epoch
Figure 2: Experimental observation
of the bi-direction attention entropy.
According to information theory and the attention mechanism
(Bahdanau et al., 2014), it is clear that we will get a low BDE
if and only if: 1) Each hidden state in source (or target) attends
to a few hidden states in target (or source), which can also be considered that ISE and ISD are able
to represent each other. 2) ISE should be CISE (the hidden states of different words in the input
sentence have different and corresponding hidden state representations). These two conditions mean
that a low BDE ensures a full coverage of the meanings of translation and input sentence. In that
case, ISD will tend to be CISD with the decrease of B, and a better translation can be generated. We
can arrive at the same conclusions from experiments (see Figure 2) which also demonstrates that B is
more sensitive to the change in BLEU score compared to existing indicators (e.g. PPL, ROUGEL and
METEOR).
3
Under review as a conference paper at ICLR 2019
X'
ɪ
General
Generator
X'
input X
General
Generator
reconstruct the
input X
Decoder 2
Decoder 1
Decoder 3
Decoders for
different languages
(Systems building on GN, MLF)
Encoder
夺■
Figure 3: Overview of GLF and GN
Note that BDE doesn’t consider the word orders in translation, thus we need to use language models
to ensure a fluent translation. Based on the properties of information space, we develop a novel
guided learning framework called multi-dual learning framework to use both monolingual data and
third party resources to improve the performance of neural machine translation (see Section 3.2).
3.2	Architecture of GLF
Figure 3 shows the overall architecture of the proposed guider network (GN) and guided learning
framework in which the guider network is the key component. We elaborate each component first
and then discuss the models in GLF.
•	Encoder and Decoder Each language has an unique encoder for encoding its source data to a
dense context vector which is the same in (Dong et al., 2015; Zoph & Knight, 2016; Firat et al.,
2016a). One unique encoder enables the model to obtain the specific linguistic information from
different languages. We denote encoder for the ith language enci . Each language has an unique
decoder for receiving the encoded context vector and decoding it to generate a sentence into its own
language. We denote decoder for the ith language deci .
•	Guider Network GN is based on the information space discussed above. The goal of GN is to use
the monolingual data to improve the performance of NMT. One major challenge is that it is impossible
to know the corresponding translations for the monolingual data and thus it cannot enhance an NMT
system directly. Fortunately, based on the analysis of BDE in Section 3.1, we can minimize BDE to
provide supervising information.2 In that case, we propose a guider network which can be used to 1)
train auto-encoder (which will be discussed below) using monolingual data to force the ISE of NMT
to be CISE (which is a condition to get a low BDE); 2) guide the NMT system to minimize BDE to
force the hidden states of decoder consistent with the hidden states of encoder and thus yielding a
better translation. We denote GNi for GN of the ith language.
Given the component introduced above, we can build the GLF which consists of three different
kinds of modules: auto-encoder (AE), guided dual learning (GDL) and vanilla NMT (NMT). We use
parallel data to implement NMT training and monolingual data to conduct AE and GDL training.
Auto-Encoder (AE): For the ith language, as shown on the right part of Figure 3, we first combine
GNi and enci to form an auto-encoder AEi . AEi can be well trained on a large amount of monolin-
gual data in such a case we can obtained a well trained GNi and enci . Thus auto-encoder provides
two conditions for applying the guider network in the next guided dual learning step. That is after
training, 1) the ISE of enci tends to be CISE with the increase of training data since we take the ISE
as the input of GNi to reconstruct the source sentence Si = GNi(ISEi, θ) which needs a CISE to
well reconstruct, 2) GNi can achieve one-to-one mappings between ISEi (CISEi for well trained
GNi) and Si . θ is the parameter of GNi . We rely on optimizing the reconstruction loss to update
AE, which is a cross-entropy between reconstructions and ground-truth sentences.
Guided Dual Learning (GDL): After training in AE, GN can be used to guide the training of
NMT system, and we name this method by guided dual learning. We denote the guided dual learning
method as GDLi for ith language, which consists of NMTi and GNi. As in dual learning (He et al.,
2016), to utilize monolingual data, one must leverage on language models and reconstruction losses
to form scores for policy gradient. Considering limited sample numbers, it is insufficient to provide
a low variance gradient to update NMT models. Besides, due to the discretization involving the
whole process, this leads to a non-end-to-end process. The proposed GDL enables an end-to-end
2Note that it’s also reasonable to regard Formula 1 as an constraint and add it to the objective function of
NMT directly. However, we can’t gain much in such way since that the ISE produced by NMT might far from
CISE since it heavily depends on the quality of the NMT for which we may have little data to train in some
languages.
4
Under review as a conference paper at ICLR 2019
training process, providing better update directions and abandoning any discretizations. In this
process, one monolingual data Si of language i would first be translated to hidden states (ISD) of deci
through NMTi, then ISDi is used to reconstruct Si = GNi(ISEi, θ). If we regard θ and the given
Si as the input of GNi and ground-truth, respectively, and treat the ISDi (generated by NMT) as the
parameters of GNi, then ISEi that corresponds to Si can be considered as the optimal location ofGNi.
ISDi can approach ISEi by minimizing the distance between Si and GN (ISDi, θ) since stochastic
optimization method will optimize the parameters to optimal location and the exotic features of
non-convex optimization do not appear to cause difficulty for recurrent models of sequences according
to Goodfellow et al. (2014b). Note that this is a new optimization idea that optimizes the input using
a well trained network.
One major challenge in implementing GDL is that it needs the abilities to reconstruct the input source
sentence not only from the encoder (as it trains) but also from the decoders of different languages.
We propose a general input method using attention mechanism for GN to address this problem.
The GN architecture is shown on the left of Figure 3. As we can see, GN is composed of an input
transformation component to adapt different inputs (e.g. ISD and ISE) and a recurrent neural network
as the generator to reconstruct the original input sentence. The input transformation component
decides which hidden state of encoder or decoder ht should be inputted at time step t by itself through
the attention mechanism, until “<eos>” symbol is predicted:
ht = maxpooling(softmax(cT ∙ [hi, h2,…,hτ])); Ct = LSTM(Ct-ι, ht-ι)	(2)
where ct is the hidden state of input transformation network (we use LSTM), h0 is the embedding of
“<bos>” which is the beginning symbol for the input transformation network. We rely on optimizing
the reconstruction loss to update GDL, which is a cross-entropy between reconstructions and ground-
truth sentences.
Multilingual Guided Dual Learning (M GDL): Multiple NMT systems for different languages
can be used to form a multilingual guided dual learning module (Mgdl). We first denote NMTi,j as
the NMT system which translates the ith language (encoded by enCi) to the jth language (generated
by deCi). As shown on the right part of Figure 3, a Mgdl can be built by taking a GN over multiple
NMT systems. Note that all the NMT systems involved to build the Mgdl should have the same
source language associate with GN. In that case, GN can guide the training of Mgdl according to the
reconstruction error. For instance, Mgdli , which takes language i as the source language, consists
of GNi and multiple NMT systems denoted by NMTi,* where * denotes several kinds of target
languages. Since one language has it own specific encoder and decoder as we discussed above,
NMTi,* consists of one encoder enCi and multiple decoders. GNi is used to reconstruct the source
sentence by taking input the ISD of those decoders.
4	Model Scheduling
Many existing studies on different domains consist of multiple models such as, Dual learning (He
et al., 2016), Generative Adversarial Net (Goodfellow et al., 2014a). Our GDL (especially Mgdl)
is also the case. Model scheduling matters due to different convergence speeds of different models.
Fixed sequential scheduling induces imbalance between models such that some models have already
converged but are still being trained, while others still need further training. Our goal is to enable
the system to train faster while maintaining a balanced convergence between models to achieve a
better performance. Since each model has a different role in the overall training phase, e.g. AE and
NMT work first since they greatly affect the quality of reconstruction. As the training goes on, the
dual model becomes more important because it uses the monolingual data to improve the whole
system. However, the extent to which these modules contribute varies with the training process going
on, which is also affected by the models convergence rates and language properties. Therefore, we
divide the whole process of training into several sub-processes and thus the environment of each
sub-processes can be assumed unchanged. That means, in each sub-process, considering the training
steps with a same training status of the whole system, one specific schedule action should correspond
to a deterministic feedback.
Thus we consider each sub-process a MDP and use deep Q-learning (DQN) (Mnih et al., 2013) for
the scheduling model. In each step, we propose to select the next part to train according to the model
status in the past few steps. We use M to denote the number of models, i.e., M equals to the total
5
Under review as a conference paper at ICLR 2019
number of NMT, GDL, AE. For the reinforcement algorithm, we define state, action and reward
(st , at , rt ) below.
S is a set of states. The st ∈ S represents the model status at time step t which consists of several
consistent observations ot . We adopt different evaluation metrics of the model to compose ot : 1)
training perplexity plt ∈ RM, where plit denotes the perplexity of the ith model at time step t. 2)
training accuracy act ∈ RM, where acit denotes the accuracy of the ith model at time step t. 3)
history schedule times Ht ∈ RM, Hit denotes the total schedule times of model i. Specifically, ot
are computed as:
ot = [softmax(plt); softmax(act)]	(3)
where ; denotes the concatenation of two vectors. Finally, we compose st as st = [ot-τ, ..., ot]. The
scheduling model determines the action at according to st which decides the training models in the
next time step. at represents the action taken in step t, at ∈ (1, 2, ..., M). After at is implemented,
we receive a reward rt .
rt ∈ R is the reward, which is a metric to characterize the degree of improvement of the
model after the last action, therefore in order to encourage the balance and fast convergence of
each model, it is natural to compose the reward with the change of evaluation metrics of each
model. For rt, one can utilize the absolute value of change of the evaluation metric of the models,
e.g. ∆plt = plt - plt+1. However, different models have distinct magnitude of convergence
speed , so that absolute value of change may incur imbalanced reward. Therefore, we leverage on
relative rate of change of plt and act to compose rt i.e. ∆plt = (plt - plt+1)/plt of plt and
∆act = (act - act+1)/act of act, ∆plt, ∆act ∈ (0, 1), which are uniform metric for model with
distinct convergence speed. Finally reward is obtained as:
r0t = [∆plt ; ∆act] p; r1t = max(αat -
Hatt
PMi Hi
(4)
)
we use rt = sigmoid(r0t + r1t) as the final reward where r0t is the reward stands for training gains.
is the pointwise multiplication. p ∈ R2M is a hyper parameter to apply different attention on
every model. The more attention of the ith model, the larger pi is set. Moreover, taking into account
the different expectations of the two evaluation metrics (perplexity and accuracy), we give two
different signs, i.e. We set pi：M Y 0 and PM +i：2M * 0 as we want perplexity to drop and accuracy
to increase. r0t penalizes action that correspond to less gains, which is a positive feedback for fast
training while a preventer from overfitting when model would receive low or even negative gain as it
is about to overfit. rit stands for penalty for violating balance of at . αi denotes the expecting rate for
the ith model to train, PiM=i αi = 1. We slightly penalize model violating the hyper rate, which is a
useful guide when at the beginning of the scheduling.
At the beginning of each sub-process, according to DQN, we set exploration rate (i.e. for
greedy algorithm) to 1.0 and record current GLF parameters. Then we decrease to the lowest
value gradually. During this exploration period, we sample data corresponding to current Q-net and
implement the update of GLF. After drops to lowest value, we reset the GLF to the status before
exploration and perform the real update process by a trained schedule model and the lowest rate
until entering the next sub-process.
5	Experiments
In this section, we conduct experiments of our system on machine translation task on one dataset. A
key factor in the choice of dataset was the fact that we require parallel data (also test set) among all
different language pairs which limits us from working with the widely used WMT dataset. We chose
the United Nations Parallel Corpus (UN corpus)(Ziemski et al., 2016) while in (He et al., 2016), they
use WMT15 dataset. We use three languages in that dataset: English(en), French(f r), Spanish(es).
The corpus contains pairwise aligned documents as well as a fully aligned sub-corpora for these three
languages, thus it allows the control needed for our experiments, without having to resort on human
ratings. Moreover, the corpus provides official development and test sets composed of the documents
released in 2015. Both sets comprise 4,000 sentences, aligned for all these three languages. This
allows experiments to be evaluated, and replicated, in all directions.
6
Under review as a conference paper at ICLR 2019
We implement three experiments to test the ability for out model to gain enhancement in multilingual
data and monolingual data: 1) translation in bilingual Setting(en 什 fr or en 什 es or fr 什 es) and
trilingual setting(en 什 fr and en 什 es and fr 什 es). 2) translation comparing dual learning(He
et al., 2016) under the same experimental settings. 3) one-shot translation exploring. We test the
ability of our system to perform one-shot learning, i.e. using full parallel data of two of the three
languages while very few data of the third one.
5.1	Experiments Setting
In all experiment, we implement all NMT model with the same hyper-parameters. For training,
we set the hidden state size to 512, word embedding size to 256, vocabulary size to 30000. The
dropout rate is 0.3 and the initial parameter range is [-0.1, +0.1]. We use LSTM network and
utilize SGD optimizer and set learning rate to 0.3. We also re-scale the normalized gradient
when norm > 5. In bilingual translation. We set Q-net’s mid layer dim 20 while 40 in trilingual
translation. We propose a RL-based (reinforcement learning) scheduling model (RSM for short)
using DQN (Mnih et al., 2013) with dueling architecture (Wang et al., 2015) and double Q-learning
(Van Hasselt et al., 2016), which enables fast and low-variance convergences for RSM. We set
αi = 1/M, pi = -1, pi+M = 1, ∀i ∈ (1, ..., M) and adopt a two-layers neural network as the
Q-net in our RSM. For Q-net, we use optimizer ADAM with learning rate 0.0001 and copy target
Q-net parameters per 100 steps. We regard 300 steps as a sub-process and set lowest ε to 0.3 with
0.0035 ε decay rate per step. In each step, we schedule a module of GLF to train 5000 pieces of
data corresponding to the schedule model. For evaluation, we use multi-bleu metric to evaluate all
translation which is obtained via script multi-bleu.perl3
Table 1: Results are on 1M parallel data and 1M monolingual data. 2L represents only using one pair
languages, (i.e. results en → fr, fr → en are from one experiments) while 3L represents using en, fr
and es to conduct multilingual translation in one experiment leveraging GLF.
	dual learning	GLF 2L	GLF 3L
en → fr	40:35	41.70	43.50
fr → en	42.06 —	43.12	44.57	一
Monolingual Enhancement We first use 1M parallel data and 1M monolingual data of en 什 fr to
train GLF and baseline dual learning, then use 1M parallel data and 1M monolingual data on three
language pairs(en 什 fr, en 什 es, es 什 fr). For dual learning, on either source and target side, We
use another 1M monolingual data to train a language model. We set NMTs in dual learning the same
as ours and other hyper parameters are the same as elaborated in (He et al., 2016). All other settings
of experiments are as mentioned above. The experimental results are shoWn in Table 1. From the
table we can see GLF outperforms baseline in en 什 fr translation. As results illustrate, GLF utilize
monolingual data better. GLF excels baseline 1.35 bleu score in en → fr and 1.06 bleu score in
fr → en. We observe that forming en 什 es and es 什 fr translations under GLF further improves
en 什 fr translation, which gains 1.80 and 1.45 bleu scores in en → fr and fr → en respectively.
The result suggests more languages in GLF enhance the translation quality.
Table 2: Results are on 0.5M parallel data and 0.5M monolingual data. f denotes for RSM.
	en → fr	fr → en	en → es	es → en	fr → es	es → fr
GLF 2L	31.47	33.31	^^41.72^^	40.33	34.95	32.29
GLF 3L	33.11	33.94	42.10	43.20	35.66	34.04
GLF3Lf	34.57	34.90	42.57	44.15	36.42	34.43
We conduct more experiments to detect mutual improvements between multiple languages in GLF.
Note more parallel languages and monolingual enable NMT model to learn more sentence patterns and
alignments. Due to limited computation resources, we can’t conduct more languages experiments on a
large scale, however it’s expected that more languages should bring advanced improvements according
to the tendency. We use 0.5M parallel data in each translation pair (en 什 fr, en 什 es, es 什 fr),
0.5M monolingual data of each language to conduct three experiments on each translation pair
respectively. Then we also use the same data conducting multilingual translation in one experiment.
Table 2 illustrates more results on multilingual enhancement. According to the results, languages gain
3https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl
7
Under review as a conference paper at ICLR 2019
Table 3: LoW resource results. In this experiment, We only use 10k data on es - fr while 0.5M on
either en - fr and fr - es. f denotes for DQN scheduling. ] denotes for sequential scheduling.
NMT are trained on the 10k data. GLF-2L-full denotes the results training by GLF on 0.5M data.
	NMT	GLF-2L-full	GLF3L]	GLF3Lf
fr → es	6.92	3495	-1ΣT8-	-2374-
es → fr	8.26	32.29 —	12.43	19.78
mutual enhancements in GLF. Trilingual translation excels bilingual one, Which confirms previous
statement that GLF is a extendable frameWork and is able to utilize monolingual data in other
languages. As the results demonstrate, GLF achieves better results in all translations When extends to
more languages. Considering the flexible architecture of GLF, one is able to utilize more plentiful
linguistic information.
⑶ en o fr PPL
(b) en o fr ACC
(c) fr o es PPL
StepsfiiiO
(d) fr o es ACC
Figure 4: Accuracy increasing comparison. f denotes With RSM. one unit of X axis denotes 1750
steps. (a),(b) are from trilingual translation While (c),(d) are from loW resource translation in Which
only 10k data for fr - es.
Scheduling Learning: As We mentioned above in section 4, according to GLF, there comes up With
a natural issue: scheduling problem. In this part, We conduct experiments to testify the effectiveness
of the RSM. We also set a baseline of sequential scheduling, in Which We schedule each module one
by one. The results are provided in Table 2. Depending on the RSM, We obtain further improvements
in all translations and the maximum gain is up to 1.46 bleu score.
We also conduct experiments to examine the fast convergence under scheduling. Results are in Figure
4, we show that GLF with RSM converges faster. For en - fr translation, both the decrease of Ppl
and increase of acc are faster. In loW resource translation Which We Will elaborate in next experiment,
due to different magnitude of data size, each module has distinct convergence speed between each
other. In all, RSM effects well to solve the imbalance convergence and boost the training.
Low-Resource Learning Another finding is that GLF is able to utilize low resource data and get
better performance. As shown in table 3. We use either 0.5M for en - fr and en - es while only
use 10k for es - fr, but the GLF 3L] improves 5.26/4.17 bleu scores over NMT. Moreover, GLF
with RSM gains further 11.56/7.35 bleu scores.
6	Conclusion
We propose a novel framework to improve NMT which utilize multilingual and monolingual data
as more effective as possible. The advantages of our approach are three-folds. First, it overcomes
discretization of dual learning when dealing with monolingual data and comes up with an end-to-end
training method. Second, it is extendable to multi-languages and will access to improvements when
adding more languages. Third, we design a proper schedule mechanism which can be easily performed
in other frameworks consist of multiple modules to enable the whole system to converge fast while
8
Under review as a conference paper at ICLR 2019
remaining balance between each modules. We finally achieve better performance comparing to
baselines and testify the effectiveness of every part via experiments. Considering limited computation
resources, we didn’t conduct larger data experiments such as 10M and more language sources and
targets. For future work, we plan to utilize larger dataset and more languages to test the robustness of
the whole system.
References
Mikel Artetxe, Gorka Labaka, Eneko Agirre, and Kyunghyun Cho. Unsupervised neural machine
translation. arXiv preprint arXiv:1710.11041, 2017.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
Yun Chen, Yang Liu, and Victor OK Li. Zero-resource neural machine translation with multi-agent
communication game. arXiv preprint arXiv:1802.03116, 2018.
Yong Cheng, Qian Yang, Yang Liu, Maosong Sun, and Wei Xu. Joint training for pivot-based neural
machine translation. In Proceedings of IJCAI, 2017.
Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for
statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.
Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, and Haifeng Wang. Multi-task learning for multiple
language translation. In Proceedings of the 53rd Annual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint Conference on Natural Language Processing
(Volume 1: Long Papers), volume 1,pp. 1723-1732, 2015.
Orhan Firat, Kyunghyun Cho, and Yoshua Bengio. Multi-way, multilingual neural machine translation
with a shared attention mechanism. In Proceedings of NAACL-HLT, pp. 866-875, 2016a.
Orhan Firat, Baskaran Sankaran, Yaser Al-Onaizan, Fatos T Yarman Vural, and Kyunghyun Cho.
Zero-resource translation with multi-lingual neural machine translation. 2016b.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, pp. 2672-2680, 2014a.
Ian J Goodfellow, Oriol Vinyals, and Andrew M Saxe. Qualitatively characterizing neural network
optimization problems. arXiv preprint arXiv:1412.6544, 2014b.
Thanh-Le Ha, Jan Niehues, and Alexander Waibel. Toward multilingual neural machine translation
with universal encoder and decoder, 2016.
Di He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu, Tieyan Liu, and Wei-Ying Ma. Dual learning
for machine translation. In NIPS, pp. 820-828, 2016.
Di He, Hanqing Lu, Yingce Xia, Tao Qin, Liwei Wang, and Tieyan Liu. Decoding with value
networks for neural machine translation. In NIPS, pp. 177-186, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Melvin Johnson, Mike Schuster, Quoc V Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil
Thorat, Fernanda Viegas, Martin Wattenberg, Greg Corrado, et al. Google,s multilingual neural
machine translation system: enabling zero-shot translation. arXiv preprint arXiv:1611.04558,
2016.
Philipp Koehn and Rebecca Knowles. Six challenges for neural machine translation. arXiv preprint
arXiv:1706.03872, 2017.
Guillaume Lample, Ludovic Denoyer, and Marc’Aurelio Ranzato. Unsupervised machine translation
using monolingual corpora only. arXiv preprint arXiv:1711.00043, 2017.
9
Under review as a conference paper at ICLR 2019
Jindrich Libovicky and Jindfich HelcL Attention strategies for multi-source SeqUence-to-sequence
learning. In Proceedings of the 55th Annual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), volume 2,pp.196-202, 2017.
Yichao Lu, Phillip Keung, Faisal Ladhak, Vikas Bhardwaj, Shaonan Zhang, and Jason Sun. A neural
interlingua for multilingual machine translation, 2018.
Ping Luo, Guangrun Wang, Liang Lin, and Xiaogang Wang. Deep dual learning for semantic image
segmentation. In Proceedings ofthe IEEE CVPR, pp. 2718-2726, 2017.
Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based
neural machine translation. In Proceedings of the 2015 EMNLP, pp. 1412-1421, 2015.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
through deep reinforcement learning. Nature, 518(7540):529, 2015.
Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation mod-
els with monolingual data. In Proceedings of the 54th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), volume 1, pp. 86-96, 2016.
Flood Sung, Li Zhang, Tao Xiang, Timothy Hospedales, and Yongxin Yang. Learning to learn:
Meta-critic networks for sample efficient learning. arXiv preprint arXiv:1706.09529, 2017.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
In NIPS, pp. 3104-3112, 2014.
Zhaopeng Tu, Yang Liu, Lifeng Shang, Xiaohua Liu, and Hang Li. Neural machine translation with
reconstruction. In AAAI, pp. 3097-3103, 2017.
Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. In AAAI, volume 2, pp. 5. Phoenix, AZ, 2016.
Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos,
Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. arXiv
preprint arXiv:1611.05763, 2016.
Yijun Wang, Yingce Xia, Li Zhao, Jiang Bian, Tao Qin, Guiquan Liu, and Tie-Yan Liu. Dual transfer
learning for neural machine translation with marginal distribution regularization. In AAAI, 2018.
Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Van Hasselt, Marc Lanctot, and Nando De Freitas.
Dueling network architectures for deep reinforcement learning, 2015.
Jiawei Wu, Lei Li, and William Yang Wang. Reinforced co-training. arXiv preprint arXiv:1804.06035,
2018.
Yingce Xia, Tao Qin, Wei Chen, Jiang Bian, Nenghai Yu, and Tie-Yan Liu. Dual supervised learning.
In International Conference on Machine Learning, pp. 3789-3798, 2017.
Tao Qin Xiang-Yang Li Tie-Yan Liu Yang Fan, Fei Tian. Learning to teach.
Zili Yi, Hao Zhang, Ping Tan, and Minglun Gong. Dualgan: Unsupervised dual learning for
image-to-image translation. In Proceedings of the IEEE CVPR, pp. 2849-2857, 2017.
Jiajun Zhang and Chengqing Zong. Exploiting source-side monolingual data in neural machine
translation. In Proceedings of the 2016 EMNLP, pp. 1535-1545, 2016.
Zhirui Zhang, Shujie Liu, Mu Li, Ming Zhou, and Enhong Chen. Joint training for neural machine
translation models with monolingual data. arXiv preprint arXiv:1803.00353, 2018.
10
Under review as a conference paper at ICLR 2019
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation
using cycle-consistent adversarial networks. In Proceedings of the IEEE CVPR, pp. 2223-2232,
2017.
Michal Ziemski, Marcin Junczys-Dowmunt, and Bruno Pouliquen. The united nations parallel corpus
v1. 0. In LREC, 2016.
Barret Zoph and Kevin Knight. Multi-source neural translation. In Proceedings of NAACL-HLT, pp.
30-34, 2016.
11