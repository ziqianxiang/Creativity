Under review as a conference paper at ICLR 2019
LAPLACIAN SMOOTHING GRADIENT DESCENT
Anonymous authors
Paper under double-blind review
ABSTRACT
We propose a class of very simple modifications of gradient descent and stochastic
gradient descent. We show that when applied to a large variety of machine learn￾ing problems, ranging from softmax regression to deep neural nets, the proposed
surrogates can dramatically reduce the variance and improve the generalization
accuracy. The methods only involve multiplying the usual (stochastic) gradient by
the inverse of a positive definitive matrix coming from the discrete Laplacian or
its high order generalizations. The theory of Hamilton-Jacobi partial differential
equations demonstrates that the implicit version of new algorithm is almost the
same as doing gradient descent on a new function which (i) has the same global
minima as the original function and (ii) is “more convex”. We show that optimiza￾tion algorithms with these surrogates converge uniformly in the discrete Sobolev
Hpσ
sense and reduce the optimality gap for convex optimization problems. We
implement our algorithm into both PyTorch and Tensorflow platforms which only
involves changing of a few lines of code. The code will be available on Github.
1 INTRODUCTION
Stochastic gradient descent (SGD) has been the workhorse for solving large-scale machine learning
problems (Bottou et al., 2018). It gives rise to a family of algorithms that make training of deep
neural nets (DNN) practical, which is believed to somehow implicitly smooth the loss function of
the DNN (Jastrzebski et al., 2018). Many efforts have been carried out to improve training and gen￾eralization of DNN by directly searching for flat minima (Keskar et al., 2017; Chaudhari et al., 2017;
2016). An alternative view of SGD’s magic comes from the theory of uniform stability (Bousquet &
Elisseeff, 2002; Duchi et al., 2011; Hardt et al., 2016; Bottou et al., 2016; Gonen & Shalev-Shwartz,
2017).
The noise in SGD, on the one hand, helps gradient-based optimization algorithms circumvent spu￾rious local minima and reach those that generalize well (Schmidhuber, 2014). On the other hand,
it slows down the convergence of regular gradient descent (GD). To recover the linear convergence
rate for strongly convex functions, several interesting variance reduction algorithms have been pro￾posed, e.g., SAGA (Defazio & Bach, 2014) and SVRG (Johoson & Zhang, 2013). These algorithms
have a certain amount of difficulty in training DNN. SAGA has a relatively high space complexity
in storing the gradient for many samples. SVRG requires computation of the full batch gradient.
In this work, we propose a carefully designed positive definite matrix to smooth and to reduce
variance of the (stochastic) gradient on-the-fly. The resulting surrogate tends to reduce noise in SGD
and improve training of DNN. We call this procedure Laplacian smoothing. The gradient smoothing
can be done by multiplying the gradient by the inverse of the following circulant convolution matrix
Aσ :=

1 + 2σ −σ 0 . . . 0 −σ −σ 1 + 2σ −σ . . . 0 0
0 −σ 1 + 2σ . . . 0 0
−
. . . . . . . . . . . . . . . . . .
σ 0 0 . . . −σ 1 + 2σ
(1)
for some positive constant σ ≥ 0. In fact, we can write Aσ = I−σL, where I is the identity matrix,
and L is the discrete one-dimensional Laplacian which acts on indices. We define the (periodic)
1
Under review as a conference paper at ICLR 2019
forward finite difference matrix as
D+ = −1 1 0 . . . 0 0
0 −1 1 . . . 0 0
0 0 −1 . . . 0 0
. . . . . . . . . . . . . . . . . .
1 0 0 . . . 0 −1 .
Then, we have Aσ = I − σD−D+, where D− = −D>+ is the backward finite difference. The
resulting Laplacian smoothing stochastic gradient descent (LS-SGD) requires negligible extra com￾putational cost and generalizes better than the standard SGD. When the Hessian has a poor condition
number, gradient descent performs poorly. In this case, the derivative increases rapidly in one direc￾tion, while increasing slowly in others. Gradient smoothing can avoid jitter along steep directions
and help make progress in shallow directions (Li & et al, 2018). Moreover, we show that the oper￾ator A−σ 1 plays role as a denoiser which enables better convergence in the presence of a very noisy
stochastic gradient. The implicit version of our proposed approach is linked to an unusual Hamilton￾Jacobi partial differential equation (HJ-PDE) whose solution makes the original loss function more
convex while retaining its flat (and global) minima, and essentially works on this surrogate function
with a much better landscape.
2 HAMILTON-JACOBI PDES AND CONVEXIFICATION
Machine learning problems are generally formulated as finding the optimal parameters w of a para￾metric function y = h(x, w), such that for an input x, the output y is close to the ground-truth. The
optimal w can be obtained by minimizing an empirical risk function, f(X, Y, w) .= f(w), given the
training data {X, Y }. We start from the following unusual HJ-PDE with f(w) as initial condition

ut + 12 
 ∇wu, A−1 σ ∇wu = 0, (w, t) ∈ Ω × [0, ∞) u(w, 0) = f(w), w ∈ Ω
(2)
By the Hopf-Lax formula (Evans, 2010), the unique viscosity solution to Eq. (2) is represented by
u(w, t) = inf
v n f(v) +
21t 
 v − w, Aσ(v − w) o .
This viscosity solution u(w, t) makes f(w) ”more convex”, an intuitive definition and theoretical
explanation of ”more convex” can be found in (Chaudhari et al., 2017; 2016), by bringing down the
local maxima while retaining and widening local minima. An illustration of this is shown in Fig. 1.
If we perform the smoothing GD with proper step size on the function u(w, t), it is easier to reach
the global or at least a flat minima of the original nonconvex function f(w).
Figure 1: f(w) = k wk 2￾ 1 + 12
sin(2πk wk ) is made more convex by solving Eq.(2). The plot
shows the cross section of the 5D problem with σ = 1 and different t values.
Proposition 1. Suppose f(w) is differentiable, the LS-GD on u(w, t) wk+1 = wk − tA−σ 1∇wu(wk
, t)
is equivalent to the smoothing implicit GD on f(w) wk+1 = wk − tA−σ 1∇f(wk+1). (3)
All the proofs here and below are provided in the appendix.
2
Under review as a conference paper at ICLR 2019
2.1 LAPLACIAN SMOOTHING GRADIENT DESCENT
Laplacian smoothing implicit gradient descent requires inner iterations as used in (Chaudhari et al.,
2017), which is computationally expensive. We consider the following explicit scheme
wk+1 = wk − γkA−σ 1∇f(wk).
Intuitively, compared to the standard GD, this scheme smooths the gradient on-the-fly by an elliptic
smoothing operator. We adopt fast Fourier transform (FFT) to compute A−σ 1∇f(wk), which is
available in both PyTorch (Paszke et al., 2017) and TensorFlow (Abadi et al., 2016). Given a vector
g, a smoothed vector d can be obtained by computing d = A−σ 1g. This is equivalent to g = d − σv ∗ d, where v = [−2, 1, 0, · · · , 0, 1]> and ∗ is the convolution operator. Therefore
d = ifft  fft(g) 1 − σ · fft(v) ,
where we use component-wise division, fft and ifft are the FFT and inverse FFT, respectively.
Hence, the gradient smoothing can be done in quasilinear time. This additional time complexity
is almost the same as performing a one step update on the weights vector w. For many machine
learning models, we may need to concatenate the parameters into a vector. This reshape might lead
to some ambiguity, nevertheless, based on our tests, both row and column majored reshaping work
for the LS-GD algorithm. Moreover, in deep learning cases, the weights in different layers might
have different physical meanings. We then perform layer-wise gradient smoothing, instead.
Remark 1. In image processing, the Sobolev gradient (Jung et al., 2009) involves a multi￾dimensional Laplacian operator which operates on w, is different from the one-dimensional discrete
Laplacian operator employed in our LS-GD scheme, which operates on indices.
We first show that LS-GD can help bypass sharp minima and reach the global minima. We consider
the following function, in which we ‘drill’ narrow holes on a smooth convex function,
f(x, y, z) = −4e−((x−π)2+(y−π)2+(z−π)2) − 4X i
cos(x) cos(y)e−β((x−r sin( i2 )−π)2+(y−r cos( i2 )−π)2), (4)
where the summation is taken over the index set {i ∈ N| 0 ≤ i < 4π}, r and β are the parameters
that determine the location and narrowness of the local minima and are set to 1 and 1 √
500 , respec￾tively. We do GD and LS-GD starting from a random point in the neighborhoods of the narrow
minima, i.e., (x0, y0, z0) ∈ {S i Uδ(r sin( i2
) + π, r cos( i2
) + π)| 0 ≤ i < 4π, i ∈ N}, where Uδ(P)
is a neighborhood of the point P with radius δ. Our experiments (Fig. 2) show that, if δ ≤ 0.2, GD
will converge to narrow local minima, while LS-GD convergences to wider global minima.
(a) (b)
Figure 2: Demo of GD and LS-GD. Panel (a) depicts the slice of the function (Eq.(4)) with z = 2.34;
panel (b) shows the paths of GD (red) and LS-GD (black). We take the step size to be 0.02 for both
GD and LS-GD. σ = 1.0 is utilized for LS-GD.
2.2 GENERALIZED SMOOTHING GRADIENT DESCENT
We can generalize Aσ to the nth order discrete hyper-diffusion operator as follows
I + (−1)nσLn .= Anσ.
Each row of the discrete Laplacian operator L consists of an appropriate arrangement of weights in
central finite difference approximation to the 2nd order derivative. Similarly, each row of Ln is an
arrangement of the weights of the central finite difference to approximate the 2nth order derivative.
3
Under review as a conference paper at ICLR 2019
Remark 2. The nth order smoothing operator I + (−1)nσLn can only be applied to the problem
with dimension at least 2n + 1. Otherwise, we need to add dummy variables to the object function.
Again, we apply FFT to compute the smoothed gradient vector. For a given gradient vector g, the
smoothed surrogate, (Anσ)−1g .= d, can be obtained by solving g = d + (−1)nσvn ∗ d, where
vn = (cnn+1, cnn+2, · · · , cn2n+1, 0, · · · , 0, cn1
, cn2 , · · · , cnn−1
, cnn) is a vector of the same dimension as
the gradient to be smoothed. And the coefficient vector cn = (cn1
, cn2 , · · · , cn2n+1) can be obtained
recursively by the following formula
c1 = (1, −2, 1), cni = 1 i = 1, 2n + 1
−2cn1−1 + cn2−1 i = 2, 2n cni−−11 − 2cni −1 + cn−1 i+1 otherwise.
Remark 3. The computational complexities for different order smoothing schemes are the same
when the FFT is utilized for computing the surrogate gradient.
3 REDUCE OPTIMALITY GAP IN SGD
We show advantages of the LS-(S)GD and generalized schemes for convex optimization. Consider
finding the minima x∗ of the quadratic function f(x) defined in Eq. (5) by different schemes.
f(x1, x2, · · · , x100) =
50
X
i=1
x22i−1 +
50
X
i=1
x22i
102 . (5)
To simulate SGD, we add Gaussian noise to the gradient vector, i.e., at a given point x, we have
˜∇ f(x) := ∇f(x) +  N (0, I),
where the scalar  controls the noise level, N (0, I) is the vector with zero mean and unit variance
in each coordinate. The corresponding numerical schemes can be formulated as
xk+1 = xk − ηk(Anσ)−1 ˜∇ f(xk), (6)
where σ is the smoothing parameter selected to be 10.0 to kill the intense noise. We take dimin￾ishing step sizes with initial values 0.1 for SGD/smoothed SGD; 0.9 and 1.8 for GD/smoothed GD,
respectively. Without noise, the smoothing allows us to take larger step sizes, rounding to the first
digit, 0.9 and 1.9 are the largest suitable step size for GD and smoothed version here. We compare
constant learning rate and exponentially decaying learning rate, i.e., after every 1000 iteration, the
learning rate is divided by 10. We apply different schemes that corresponding to n = 0, 1, 2 in
Eq. (6) to the problem Eq. (5), with the initial point x0 = (1, 1, · · · , 1).
Figure. 3 shows the iteration v.s. optimality gap when the constant learning rate is applied to different
noise levels. In the noise free case, all three schemes converge linearly, but gradient smoothing has
a smaller decay constant due to its increased condition number. When there is noise, our smoothed
gradient helps to reduce the optimality gap and converges faster after a few iterations.
(a)  = 0 (b)  = 0.05 (c)  = 0.1 (d)  = 0.5
Figure 3: Iterations v.s. optimality gap for GD and smoothed GD with order 1 and 2 for the problem
in Eq.(5). Constant step size was used.
The exponentially decaying learning rate helps our smoothed SGD to reach a point with a smaller
optimality gap, and the higher order smoothing further reduce the optimality gap, as shown in Fig. 4.
One simple reason for this in the noisy case is because of the noise removal properties of the smooth￾ing operators. The influence of the learning rate is still under investigation. We establish the conver￾gence of our proposed smoothing gradient descent algorithms.
4
Under review as a conference paper at ICLR 2019
3.1 SOME PROPERTIES OF LAPLACIAN SMOOTHING GRADIENT DESCENT
We say the objective function f has L-Lipschitz gradient, if for any w,u ∈ Rm, we have k∇f(w)− ∇f(u)k ≤ Lk w − uk , and f is a-strongly convex, if h∇f(w) − ∇f(u), w − ui ≥ ak w − uk 2.
We define the vector norm induced by any matrix A as k wk A := p h w, Awi .
(a)  = 0 (b)  = 0.05 (c)  = 0.1 (d)  = 0.5
Figure 4: Iterations v.s. optimality gap for GD and smoothed GD with order 1 and 2 for the problem
in Eq.(5). Exponentially decaying step size is utilized here.
Proposition 2. Suppose f is convex with the global minimizer w∗
, and f ∗ = f(w∗). Consider the
following iteration with constant learning rate η > 0 wk+1 = wk − η(Anσ)−1gk,
where gk
is the sampled gradient in the kth iteration at wk
satisfying E[gk
] = ∇f(wk). De￾note GAnσ
:= limK→∞
1K P K−1 k=0 k gkk 2(Anσ )−1 and wK := P
K−1 k=0 wk/K the ergodic average of
iterates. Then the optimality gap is
lim
K→∞
E[f(wK)] − f ∗ ≤
ηGAnσ 2 .
Note that k gk (Anσ )−1 generally decreases in n unless g is constant, which indicates that a bigger n
implies smaller optimality gap. This is consistent with the experimental results above.
Proposition 3. Suppose f is L-Lipschitz smooth and a-strongly convex with the global minimizer
w∗
. Consider the generalized smoothing gradient descent algorithm
wk+1 = wk − ηk(Anσ)−1gk,
where gk
is the sampled gradient in the kth iteration at wk
satisfying E  gk = ∇f(wk) and
E h k gkk 2(Anσ )−1 i ≤ C0 + C1k∇f(wk)k 2
for all k ∈ N. If we take ηk = kC
+1 for some C > 0, then
we have
E h k wk − w∗k 2Anσ i = E  k wk − w∗k 2 + σk Dn+(wk − w∗)k 2 = O  1 k + 1
,
i.e., we have Hσn uniform convergence in σ of {wk} in expectation. The Hσn norm of w is defined
by k wk nσ
:= k wk Anσ = p h w, Anσwi .
Proposition 4. Consider the algorithm wk+1 = wk−ηk(Anσ)−1∇f(wk). Suppose f is convex and
L-Lipschitz smooth. If the step size satisfies 0 < ¯η ≤ η ≤ ¯η < 2L
. Then limt→∞ k∇f(wk)k → 0.
Moreover, if the Hessian ∇2f of f is continuous with w∗ being the global minimizer of f, and
η¯k∇2fk < 1, then k wk − w∗k Anσ → 0 as k → ∞, and the convergence is linear and independent
of σ.
In what follows, we present the noise reduction properties of the proposed smoothing operator A−σ 1.
Proposition 5. For any vector g ∈ Rm, d = A−σ 1g, let jmax = arg maxi di and jmin =
arg mini di
. We have maxi di = djmax ≤ gjmax ≤ maxi gi and mini di = djmin ≥ gjmin ≥ mini gi.
Proposition 6. The operator A−σ 1 preserves the sum of components. For any g ∈ Rm and d = A−σ 1g, we have P j dj = P j gj , or equivalently, 1> d = 1> g.
Proposition 7. Given any vector g ∈ Rm and d = A−1 σ g, then
k
dk + σ k D+dk 2 k dk
≤ kgk .
The above inequality is strict unless g = d is a constant vector. In particular, we have k dk ≤ kgk
and k D+dk ≤ √1σ k gk . 5
Under review as a conference paper at ICLR 2019
Let g be the noise vector contained in the stochastic gradient, the above results imply that the extreme
values in A−σ 1g are smaller than those in g (in magnitude), and it also has a much smaller ` 2 norm.
Proposition 8. For any g ∈ Rm, define Var(g) := 1m k gk 2− 1> g m  2
be the variance of components
in g. Let d = A−σ 1g, then
Var(d) ≤ Var(g) − 2σ k D+dk 2 m − σ2 k D+dk 4 mk dk 2 .
The inequality is strict unless g = d is a constant vector.
Proposition 8 shows that the component-wise variance of A−σ 1g is considerably less than that of g,
unless g is a constant vector. Our last result shows that A−σ 1g has diminishing ` 1 norm of finite
difference of all orders. This is an excellent desnoising result.
Proposition 9. Given vectors g and d = A−σ 1g, for any p ∈ N, it holds that k D+p dk 1 ≤ kD+p gk 1.
The inequality is strict unless D+p g is a constant vector.
Remark 4. The above proofs generalize for n > 1, except for Propositions 5 and 9.
3.2 SOFTMAX REGRESSION
Consider applying the proposed optimization schemes to Softmax regression. We run 200 epochs
of SGD and different order smoothing algorithms to maximize the likelihood of Softmax regression
with batch size 100. Based on the results from previous section, we apply the exponentially decay
learning rate with initial value 0.1 and decay 10 times after every 50 epochs. We train the model with
only 10 % randomly selected MNIST training data and test the trained model on the entire testing
images. We further compare with SVRG under the same setting. Figure. 5 shows the histograms
of generalization accuracy of Softmax regression model trained by SGD ((a)); SVRG ((b)); LS￾SGD (order 1) ((c)); LS-SGD (oder 2) ((d)). It is seen that SVRG improves the generalization with
higher average accuracy. But the first and second order smoothing schemes significantly improve
averaged generalization accuracy by more than 1% and reduce the variance over 100 independent
trials. The training loss of these 100 experiments by different optimization algorithms are shown in
the appendix.
(a) SGD (b) SVRG (c) LS-GD: Order 1 (d) LS-GD Order 2
Figure 5: Testing accuracy of Softmax model trained on randomly selected 10% MNIST data.
4 APPLICATIONS TO DEEP NEURAL NETS
4.1 TRAIN NEURAL NETS WITH SMALL BATCH SIZE
Many advanced artificial intelligence tasks make high demand on training neural nets with extremely
small batch size. The milestone technique for this is group normalization (Wu & He, 2018). In this
section, we show that LS-SGD successfully trains DNN with extremely small batch size. We con￾sider LeNet-5 devised by (LeCun et al., 1998) for MNIST classification. Our network architecture
is as follows
LeNet-5: input28×28 → conv20,5,2 → conv50,5,2 → fc512 → softmax.
The notation convc,k,m denotes a 2D convolutional layer with c output channels, each of which is
the sum of a channel-wise convolution operation on the input using a learnable kernel of size k×k, it
further adds ReLU nonlinearity and max pooling with stride size m. fc512 is an affine transformation
that transforms the input to a vector of dimension 512. Finally, the tensors are activated by a softmax
function. The MNIST data is first passed to the layer input28×28, and further processed by this
6
Under review as a conference paper at ICLR 2019
hierarchical structure. We run 100 epochs of both SGD and LS-SGD with initial learning rate 0.01
and divide by 5 after 50 epochs, and use a weight decay of 0.0001 and momentum of 0.9. Figure.
6(a) plots the generalization accuracy on the test set with the LeNet5 trained with different batch
sizes. For each batch size, LS-SGD with σ = 1.0 keeps the testing accuracy more than 99.4%, SGD
reduce the accuracy to 97% when batch size 4 is used. The classification become just a random
guess, when the model is trained by SGD with batch size 2. Small batch size leads to large noise in
the gradient, which may make the noisy gradient not along the decent direction, However, Lapacian
smoothing rescues this by killing the noise.
Figure 6: (a). Testing accuracy of LeNet5 trained by SGD/LS-SGD on MNIST with various batch
sizes. (b). The evolution of the pre-activated ResNet56’s training and generalization accuracy by
SGD and LS-SGD. (Start from the 20-th epoch.)
4.2 IMPROVE GENERALIZATION ACCURACY
The skip connections in ResNet smooth the landscape of the loss function of the classical CNN
(He et al., 2016; Li et al., 2017). This means that ResNet has fewer sharp minima. On Cifar10
(Krizhevsky, 2009), we compare the performance of LS-SGD and SGD on ResNet with the pre￾activated ResNet56 as an illustration. We take the same training strategy as that used in (He et al.,
2016), except that we run 200 epochs with the learning rate decaying by a factor of 5 after every 40
epochs. For ResNet, instead of applying LS-SGD for all epochs, we only use LS-SGD in the first 40
epochs, and the remaining training is carried out by SGD. The parameter σ is set to 1.0. Figure 6(b)
depicts one path of the training and generalization accuracy of the neural nets trained by SGD and
LS-SGD, respectively. It is seen that, even though the training accuracy obtained by SGD is higher
than that by LS-SGD, the generalization is however inferior to that of LS-SGD. We conjecture
that this is due to the fact that SGD gets trapped into some sharp but deeper minimum, which fits
better than a flat minimum but generalizes worse. We carry out 25 replicas of this experiments, the
histograms of the corresponding accuracy are shown in Fig. 7.
SGD LS-SGD with σ = 1.0
Figure 7: The histogram of the generalization accuracy of the pre-activated ResNet56 on Cifar10
trained with LS-SGD over 25 independent experiments.
4.3 TRAINING WASSERSTERIN GAN
Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) are notoriously delicate and
unstable to train (Arjovsky & Bottou, 2017). In (M. Arjovsky & Bottou, 2017), Wasserstein-GANs
(WGANs) are introduced to combat the instability in the training GANs. In addition to being more
robust in training parameters and network architecture, WGANs provide a reliable estimate of the
Earth Mover (EM) metric which correlates well with the quality of the generated samples. Nonethe￾less, WGANs training becomes unstable with a large learning rate or when used with a momentum
based optimizer (M. Arjovsky & Bottou, 2017). In this section, we demonstrate that the gradient
smoothing technique in this paper alleviates the instability in the training, and improves the qual￾ity of generated samples. Since WGANs with weight clipping are typically trained with RMSProp
7
Under review as a conference paper at ICLR 2019
(Tieleman & Hinton, 2012), we propose replacing the gradient g by a smoothed version gσ = A−σ 1g,
and also update the running averages using gσ instead of g. We name this algorithm LS-RMSProp.
To accentuate the instability in training and demonstrate the effects of gradient smoothing, we delib￾erately use a large learning rate for training the generator. We compare the regular RMSProp with
the LS-RMSProp. The learning rate for the critic is kept small and trained approximately to conver￾gence so that the critic loss is still an effective approximation to the Wasserstein distance.To control
the number of unknowns in the experiment and make a meaningful comparison using the critic loss,
we use the classical RMSProp for the critic, and only apply LS-RMSProp to the generator.
RMSProp LS-RMSProp, σ = 3.0
Figure 8: Critic loss with learning rate lrD = 0.0001, lrG = 0.005 for RMSProp (Left) and LS￾RMSProp (Right), trained for 20K iterations. We apply a mean filter of window size 13 for better
visualization. The loss from LS-RMSProp is visibly less noisy.
We train the WGANs on the MNIST dataset using the DCGAN (Radford et al., 2015) for both the
critic and generator. In Figure 8 (left), we observe the loss for RMSProp trained with a large learning
rate has multiple sharp spikes, indicating instability in the training process. The samples generated
are also lower in quality, containing noisy spots as shown in Figure 9 (a). In contrast, the curve
of training loss for LS-RMSProp is smoother and exhibits fewer spikes. The generated samples
as shown in Fig. 9 (b) are also of better quality and visibly less noisy. The generated characters
shown in Fig. 9 (b) are more realistic compared to the ones shown in Fig. 9 (a). The effects are less
pronounced with a small learning rate, but still result in a modest improvement in sample quality as
shown in Figure 9 (c) and (d).We also apply LS-RMSProp for training the critic, but do not see a
clear improvement in the quality. This may be because the critic is already trained near optimality
during each iteration, and does not benefit much from gradient smoothing.
RMSProp LS-RMSProp, σ = 3.0 RMSProp LS-RMSProp, σ = 3.0
(a) (b) (c) (d)
Figure 9: Samples from WGANs trained with RMSProp (a, c) and LS-RMSProp (b, d). The learning
rate is set to lrD = 0.0001, lrG = 0.005 for both RMSProp and LS-RMSProp in (a) and (b). And
lrD = 0.0001, lrG = 0.0001 are used for both RMSProp and LS-RMSProp in (c) and (d). The
critic is trained for 5 iterations per step of the generator, and 200 iterations per every 500 steps of
the generator.
4.4 DEEP REINFORCEMENT LEARNING
Finally, we apply the LS-SGD to deep reinforcement learning. We provide a detailed discussion and
present the numerical result in the appendix.
5 CONCLUDING REMARKS
Motivated by the theory of Hamilton-Jacobi partial differential equations, we proposed Laplacian
smoothing gradient descent and its high order generalizations. This simple modification dramat￾ically reduces the optimality gap in stochastic gradient descent and helps to find better minima.
Extensive numerical examples ranging from toy cases to shallow and deep neural nets to genera￾tive adversarial networks and to deep reinforcement learning, all demonstrate the advantage of the
proposed smoothed gradient. Several issues remain, in particular devising an on-the-fly adaptive
method for choosing the smoothing parameter σ instead of using a fixed value.
8
Under review as a conference paper at ICLR 2019
REFERENCES
M. Abadi, A. Agarwal, and et al. Tensorflow: Large-scale machine learning on heterogeneous
distributed systems. arXiv preprint arXiv:1603.04467, 2016.
M. Arjovsky and L. Bottou. Towards principled methods for training generative adversarial net￾works. arXiv preprint arXiv:1701.04862, 2017.
D. P. Bertsekas. Nonlinear programming. Athena scientific Belmont, 1999.
L. Bottou, E. Frank, and J. Nocedal. Optimization methods for large-scale machine learning. arXiv
preprint arXiv:1606.04838, 2016.
L. Bottou, E. F. Curtis, and J. Nocedal. Optimization methods for large-scale machine learning.
SIAM Review, 60(2):223–311, 2018.
O. Bousquet and A. Elisseeff. Stability and generalization. Journal of Machine Learning Research,
2:499–526, 2002.
G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai
gym. arXiv preprint arXiv:1606.01540, 2016.
P. Chaudhari, A. Choromanska, S. Soatto, Y. LeCun, C. Baldassi Carlo, C. Borgs, J. Chayes, L. Sa￾gun, and R. Zecchina. Entropy-sgd: Biasing gradient descent into wide valleys. arXiv preprint
arXiv:1611.01838, 2016.
P. Chaudhari, A. Oberman, S. Osher, S. Soatto, and C. Guillame. Deep relaxation: partial differential
equations for optimizing deep neural networks. arXiv preprint arXiv:1704.04932, 2017.
A. Defazio and F. Bach. Saga: A fast incremental gradient method with support for non-strongly
convex composite objectives. In Advances in Neural Information Processing Systems, 2014.
J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic
optimization. Journal of Machine Learning Research, 12:2121–2159, 2011.
L.C. Evans. Partial differential equations. 2010.
A. Gonen and S. Shalev-Shwartz. Fast rates for empirical risk minimization of strict saddle prob￾lems. arXiv preprint arXiv:1701.04271, 2017.
I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. C. Courville, and
Y. Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems,
pp. 2672–2680, 2014.
M. Hardt, B. Recht, and Y. Singer. Train faster, generalize better: Stability of stochastic gradient
descent. In International Conference on Learning Representations, 2016.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings
of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.
S. Jastrzebski, Z. Kenton, N. Ballas, A. Fischer, Y. Bengio, and A. Storkey. Dnn’s sharpest directions
along the sgd trajectory. arXiv preprint arXiv:1807.05031, 2018.
R. Johoson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduc￾tion. In Advances in Neural Information Processing Systems, 2013.
M. Jung, G. Chung, G. Sundaramoorthi, L. Vese, and A. Yuille. Sobolev gradients and joint vari￾ational image segmentation, denoising, and deblurring. In Computational Imaging VII, volume
7246, pp. 72460I. International Society for Optics and Photonics, 2009.
N. Keskar, M. Shirish, N. Dheevatsa, S. Jorge, Mikhail, P. Tang, and P. Tak. On large-batch training
for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2017.
A. Krizhevsky. Learning multiple layers of features from tiny images. 2009.
9
Under review as a conference paper at ICLR 2019
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 81:2278–2324, 1998.
F. Li and et al. Cs231n: Convolutional neural networks for visual recognition. 2018.
H. Li, Z. Xu, G. Taylor, and T. Goldstein. Visualizing the loss landscape of neural nets. arXiv
preprint arXiv:1712.09913, 2017.
S. Chintala M. Arjovsky and L. Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.
Mnih and et al. Human-level control through deep reinforcement learning. Nature, 518:529–533,
2015.
V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller.
Playing Atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.
A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga,
and A. Lerer. Automatic differentiation in pytorch. 2017.
A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional
generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
J Schmidhuber. Deep learning in neural networks: An overview. arXiv preprint arXiv:1404.7828,
2014.
D. Silver and et al. Mastering the game of go with deep neural networks and tree search. Nature,
529:484–489, 2016.
T. Tieleman and G. Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its
recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26–31, 2012.
Y. Wu and K. He. Group normalization. In European Conference on Computer Vision, 2018.
10
Under review as a conference paper at ICLR 2019
6 APPENDIX
6.1 TECHNICAL PROOFS
Proposition 1. Suppose f(w) is differentiable, the Laplacian smoothing GD update on u(w, t) wk+1 = wk − tA−σ 1∇wu(wk
, t)
permits the smoothing implicit gradient descent on f(w) wk+1 = wk − tA−σ 1∇f(wk+1).
Proof of Proposition 1. We define
z(w, v, t) := f(v) + 12th v − w, Aσ(v − w)i ,
and rewrite u(w, t) = infv z(w, v, t) as z(w, v(w, t), t), where v(w, t) = arg minv z(w, v, t).
Then by the Euler-Lagrange equation,
∇wu(w, t) = ∇wz(w, v(w, t), t) = Jwv(w, t)∇vz(w, v(w, t), t) + ∇wz(w, v(w, t), t),
where Jwv(w, t) is the Jacobian matrix of v w.r.t. w. Notice that ∇vz(w, v(w, t), t) = 0, ∇wu(w, t) = ∇wz(w, v(w, t), t) = −1t Aσ(v(w, t) − w).
Letting w = wk
and wk+1 = v(wk
, t) = arg minv z(wk, v, t) in the above equalities, we have
∇wu(wk
, t) = −1t Aσ(wk+1 − wk).
In summary, the gradient descent wk+1 = wk − tA−σ 1∇wu(wk
, t) is equivalent to the proximal
point iteration wk+1 = arg minv f(v) + 21t h v − wk, Aσ(v − wk)i , which yields wk+1 = wk − tA−σ 1∇f(wk+1).
Proposition 2. Suppose f is convex with the global minimizer w∗
, and f ∗ = f(w∗). Consider the
following iteration with constant learning rate η > 0 wk+1 = wk − η(Anσ)−1gk
where gk
is the sampled gradient in the kth iteration at wk
satisfying E[gk
] = ∇f(wk). De￾note GAnσ
:= limK→∞
1K P K−1 k=0 k gkk 2(Anσ )−1 and wK := P
K−1 k=0 wk/K the ergodic average of
iterates. Then the optimality gap is
lim
K→∞
E[f(wK)] − f ∗ ≤
ηGAnσ 2 .
Proof. Since f is convex, we have
h∇f(wk), wk − w∗
i ≥ f(wk) − f ∗. (7)
Furthermore,
E[k wk+1 − w∗k 2Anσ
] = E[k wk − η(Anσ)−1gk − w∗k 2Anσ ] = E[k wk − w∗k 2Anσ ] − 2ηE[h gk, wk − w∗i
] + η2E[k (Anσ)−1gtk 2Anσ ] ≤ E[k wk − w∗k 2Anσ ] − 2ηE[h∇f(wk), wk − w∗i
] + η2k gkk 2(Anσ )−1 ≤ E[k wk − w∗k 2Anσ ] − 2η(E[f(wk
)] − f ∗
) + η2k gkk 2(Anσ )−1 ,
where the last inequality is due to (7). We rearrange the terms and arrive at
E[f(wk
)] − f ∗ ≤ 12η (E[k wk − w∗k 2Anσ ] − E[k wk+1 − w∗k 2Anσ
]) +
ηk gkk 2(Anσ )−1 2 .
Summing over k from 0 to K − 1 and averaging and using the convexity of f, we have
E[f(wK)] − f ∗ ≤ P K−1 k=0 E[f(wk
)]
K − f ∗ ≤ 1 2ηK
E[k w0 − w∗k 2Anσ
] +
P
K−1 k=0 k gkk 2(Anσ )−1 2K
η.
Taking the limit as K → ∞ above establishes the result.
11
Under review as a conference paper at ICLR 2019
Proposition 3. Suppose f is L-Lipschitz smooth and a-strongly convex. Consider the generalized
smoothing gradient descent algorithm
wk+1 = wk − ηk(Anσ)−1gk,
where gk
is the sampled gradient in the kth iteration at wk
satisfying E  gk = ∇f(wk) and
E h k gkk 2(Anσ )−1 i ≤ C0 + C1k∇f(wk)k 2
for all k ∈ N. If we take ηk = kC
+1 for some C > 0, then
we have
E h k wk − w∗k 2Anσ i = E  k wk − w∗k 2 + σk Dn+(wk − w∗)k 2 = O  1 k + 1
,
i.e., we have Hσn uniform convergence in σ of {wk} in expectation. The Hσn norm of w is defined
by k wk nσ
:= k wk Anσ = p h w, Anσwi .
Proof of Proposition 3. Since ∇f(w∗
) = 0, by strong convexity of f, we have
h∇f(wk), wk − w∗i = h∇f(wk) − ∇f(w∗), wk − w∗
i ≥ ak wk − w∗k 2.
Moreover, by L-smoothness of f and the fact that k Anσk = 1, we also have
k∇f(wk)k = k∇f(wk) − ∇f(w∗)k ≤ Lk wk − w∗
k ≤ Lk wk − w∗k Anσ .
Hence,
E[k wk+1 − w∗k 2Anσ
] = E[k wk − η(Anσ)−1gk − w∗k 2Anσ ] = E[k wk − w∗k 2Anσ ] − 2ηkE  h gk, wk − w∗i  + η2kE[k gkk 2(Anσ )−1 ] = E[k wk − w∗k 2Anσ ] − 2ηkh∇f(wk), wk − w∗i + η2kE[k gkk 2(Anσ )−1 ] ≤ (1 − 2ηka)E h k wk − w∗k 2Anσ i + η2k ￾ C0 + C1E[k∇f(wk)k 2] ≤ ￾ 1 − 2ηka + η2kL2C1 E h k wk − w∗k 2Anσ i + η2kC0,
where in the first inequality we used k (Anσ)−1k = 1 for all σ and n. Taking ηk = kC
+1
for some proper C > 0 and using induction, one can show that E h k wk − w∗k 2Anσ i = E  k wk − w∗k 2 + σk Dn+(wk − w∗)k  = O( 1 k+1 ).
Proposition 4. Consider the algorithm wk+1 = wk−ηk(Anσ)−1∇f(wk). Suppose f is L-Lipschitz
smooth and 0 < ¯η ≤ η ≤ ¯η < 2L
. Then limt→∞ k∇f(wk)k → 0. Moreover, if the Hessian ∇2f of
f is continuous with w∗ being the minimizer of f, and η¯k∇2fk < 1, then k wk − w∗k Anσ → 0 as
k → ∞, and the convergence is linear.
Proof of Proposition 4. By the Lipschitz continuity of ∇f and the descent lemma (Bertsekas,
1999), we have
f(wk+1) = f(wk − ηk(Anσ)−1∇f(wk
))
≤ f(wk) − ηkh∇f(wk),(Anσ)−1∇f(wk
))i + η2kL2 k (Anσ)−1∇f(wk)k 2 ≤ f(wk) − ηkk∇f(wk)k 2(Anσ )−1 + η2kL2
k∇f(wk)k 2(Anσ )−1 ≤ f(wk) − ¯η  1 − ¯ηL
2 
k∇f(wk)k 2(Anσ )−1 .
Summing the above inequality over k, we have
¯η  1 − ¯ηL
2  ∞Xk=0
k∇f(wk)k 2(Anσ )−1 ≤ f(w0) − lim
k→∞
f(wk) < ∞.
12
Under review as a conference paper at ICLR 2019
Therefore, k∇f(wk)k 2(Anσ )−1 → 0, and thus k∇f(wk)k → 0.
For the second claim, we have
wk+1 − w∗ = wk − w∗ − ηk(Anσ)−1(∇f(wk) − ∇f(w∗
))
= wk − w∗ − ηk(Anσ)−1  Z 01 ∇2f(w∗ + τ (wk+1 − w∗
)) · (wk − w∗
)dτ = wk − w∗ − ηk(Anσ)−1  Z 01 ∇2f(w∗ + τ (wk+1 − w∗
))dτ · (wk − w∗)
= (Anσ)− 12  I − ηk(Anσ)− 12 Z 01 ∇2f(w∗ + τ (wk+1 − w∗
))dτ (Anσ)− 12 ) (Anσ) 12 (wk − w∗)
Therefore,
k
wk+1 − w∗k Anσ ≤     I − ηt(Anσ)− 12 Z 10 ∇2f(w∗ + τ (wk+1 − w∗
))dτ (Anσ)− 12     k wk − w∗k Anσ .
So if ηkk∇2fk ≤ 1 k (Anσ )−1k = 1, the result follows.
Proposition 5. For any vector g ∈ Rm, d = A−σ 1g, let jmax = arg maxi di and jmin =
arg mini di
. We have maxi di = djmax ≤ gjmax ≤ maxi gi and mini di = djmin ≥ gjmin ≥ mini gi.
Proof of Proposition 5. Since g = Aσd, it holds that
gjmax = djmax + σ(2djmax − djmax−1 − djmax+1),
where periodicity of subindex are used if necessary. Since 2djmax − djmax−1 − djmax+1 ≥ 0, We
have maxi di = djmax ≤ gjmax ≤ maxi gi
. A similar argument can show that mini di = djmin ≥ gjmin ≥ mini gi.
Proposition 6. The operator A−σ 1 preserves the sum of components. For any g ∈ Rm and d = A−σ 1g, we have P j dj = P j gj , or equivalently, 1> d = 1> g.
Proof of Proposition 6. Since g = Aσd, X i gi = 1> g = 1> (I + σD>+D+)d = 1> d = X i di,
where we used D+1 = 0.
Proposition 7. Given any vector g ∈ Rm and d = A−σ 1g, then
k
dk + σ k D+dk 2 k dk
≤ kgk .
The above inequality is strict unless g = d is a constant vector. In particular, we have k dk ≤ kgk
and k D+dk ≤ √1σ k gk .
Proof of Proposition 7. By the definition of Aσ, g = Aσd = (I − σD−D+)d = d + σD>+D+d. (8)
Therefore, pre-multiplying by d> on both sides, we have
k
dk 2 + σk D+dk 2 = d> g ≤ kdkk gk .
In particular, k dk ≤ kgk and σk D+dk 2 ≤ kdkk gk ≤ kgk 2
, so k D+dk ≤ √1σ k gk . All the
inequalities are strict unless k D+dk = 0, and g = d is a constant vector.
13
Under review as a conference paper at ICLR 2019
Proposition 8. For any g ∈ Rm, define Var(g) := 1m k gk 2− 1> g m  2
be the variance of components
in g. Let d = A−σ 1g, then
Var(d) ≤ Var(g) − 2σ k D+dk 2 m − σ2 k D+dk 4 mk dk 2 .
The inequality is strict unless g = d is a constant vector.
Proof of Proposition 8. Since 1> g = 1> d and k dk + σ k D+dk 2 k dk ≤ kgk ,
Var(g) ≥ 1m  k dk 2 + 2σk D+dk 2 + σ2 k D+dk 4 k dk 2  −  1> dn  2
= Var(d) + 2σ k D+dk 2 m + σ2 k D+dk 4 mk dk 2 .
The inequality is strict unless k D+dk = 0, and g = d is a constant vector.
Proposition 9. Given vectors g and d = A−σ 1g, for any p ∈ N, it holds that k D+p dk 1 ≤ kD+p gk 1.
The inequality is strict unless D+p g is a constant vector.
Proof of Proposition 9. Since (1 + 2σ)di = gi + σdi+1 + σdi−1, for any p ∈ N, we have
(1 + 2σ)(D+p d)i = (D+p g)i + σ(D+p d)i+1 + σ(D+p d)i−1.
So
(1 + 2σ)|(D+p d)i
| ≤ |(D+p g)i| + σ|(D+p d)i+1| + σ|(D+p d)i−1|.
The inequality is strict if there are sign changes among the (D+p d)i−1, (D+p d)i, (D+p d)i+1. Sum￾ming over i and using periodicity, we have
(1 + 2σ) mXi=1
|(D+p d)i
| ≤
mXi=1
|(D+p g)i| + 2σ mXi=1
|(D+p d)i|,
and the result follows. The inequality is strict unless D+p g is a constant vector.
6.2 ITERATION V.S. LOSS FOR SOFTMAX REGRESSION
In this part, we show the training loss evolution in training Softmax regression model, respectively,
by SGD, SVRG, LSGD with first and second order smoothing. As illustrated in Fig. 10, all the
optimization algorithms reduce loss of the model on the training set. At each iteration, among 100
independent experiments, SGD has the largest variance, SGD with first order smoothed gradient
significantly reduces the variance of loss function. The second order smoothing can further reduce
variance of loss. The variance of loss in each iteration among 100 experiments is minimized when
SVRG is use to train the Softmax model. However, the generalization performance of the model
trained by SVRG is not as good as the ones trained by LS-SGD or higher order smoothed gradient
descent.
6.3 DEEP REINFORCEMENT LEARNING
Deep reinforcement learning (DRL) has been applied to playing games including Cartpole (Brock￾man et al., 2016), Atari (Mnih et al., 2013), Go (Silver & et al, 2016; Mnih & et al, 2015). DNN plays
a vital role in approximating the Q-function or policy function. We apply the Laplacian smoothed
gradient to train the policy function to play the Cartpole game. We apply the standard procedure to
train the policy function by using the policy gradient (Brockman et al., 2016). We use the following
network to approximate the policy function:
input4 → fc20 → relu → fc2 → softmax.
14
Under review as a conference paper at ICLR 2019
(a) SGD (b) SVRG
(c) LS-GD: Order 1 (d) LS-GD: Order 2
Figure 10: Iterations v.s. loss for GD, SVRG, and smoothed GD with order 1 and 2 for training the
softmax regression model.
The network is trained by RMSProp and LS-RMSProp with σ = 1.0, respectively. The learning
rate and other related parameters are set to be the default ones in PyTorch. The training is stopped
once the average duration of 5 consecutive episodes is more than 490. In each training episode,
we set the maximal steps to be 500. Left and right panels of Fig. 11 depict a training procedure
by using RMSProp and LS-RMSProp, respectively. We see that Laplacian smoothed gradient takes
fewer episodes to reach the stopping criterion. Moreover, we run the above experiments 5 times
independently, and apply the trained model to play Cartpole. The game lasts more than 1000 steps
for all the 5 models trained by LS-RMSProp, while only 3 of them lasts more than 1000 steps when
the model is trained by vanilla RMSProp.
RMSProp LS-RMSProp, σ = 1.0
Figure 11: Durations of the cartpole game in the training procedure. Left and right are training
procedure by RMSProp and LS-RMSProp with σ = 1.0, respectively.
15
