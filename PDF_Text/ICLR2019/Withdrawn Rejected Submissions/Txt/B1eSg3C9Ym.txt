Under review as a conference paper at ICLR 2019
Mean-field Analysis of Batch Normalization
Anonymous authors
Paper under double-blind review
Ab stract
Batch Normalization (BatchNorm) is an extremely useful component of modern
neural network architectures, enabling optimization using higher learning rates
and achieving faster convergence. In this paper, we use mean-field theory to ana-
lytically quantify the impact of BatchNorm on the geometry of the loss landscape
for multi-layer networks consisting of fully-connected and convolutional layers.
We show that it has a flattening effect on the loss landscape, as quantified by the
maximum eigenvalue of the Fisher Information Matrix. These findings are then
used to justify the use of larger learning rates for networks that use BatchNorm,
and we provide quantitative characterization of the maximal allowable learning
rate to ensure convergence. Experiments support our theoretically predicted max-
imum learning rate, and furthermore suggest that networks with smaller values of
the BatchNorm parameter γ achieve lower loss after the same number of epochs
of training.
1	Introduction
Deep neural networks have achieved remarkable success in the past decade on tasks that were out
of reach prior to the era of deep learning (Krizhevsky et al., 2012; He et al., 2016b). Amongst
the myriad reasons for these successes are powerful computational resources, large datasets, new
optimization algorithms, and modern architecture designs (Russakovsky et al., 2015; Kingma &
Ba, 2015). In many modern deep learning architectures, one key component is batch normalization
(BatchNorm). BatchNorm is a module that can be introduced in layers of deep neural networks
that normalizes hidden layer outputs to have a common first and second moment. Empirically,
BatchNorm enables optimization using much larger learning rates, and achieves better convergence
(Ioffe & Szegedy, 2015).
Despite significant practical success, a theoretical understanding of BatchNorm is still lacking. A
widely held view is that BatchNorm improves training by “reducing of internal covariate shift” (ICF)
(Ioffe & Szegedy, 2015). Internal covariate shift refers to the change in the input distribution of in-
ternal layers of the deep network due to changes of the weights. Recent results (Santurkar et al.,
2018), however, cast doubt on the ICF expalanation, by demonstrating that noisy BatchNorm in-
creases ICF yet still improves training as in regular BatchNorm. This raises the question of whether
the utility of BatchNorm is indeed related to the reduction of ICF. Instead, it is argued by Santurkar
et al. (2018) that BatchNorm actually improves the Lipschitzness of the loss and gradient.
Meanwhile, dynamical mean-field theory (Sompolinsky & Zippelius, 1982), a powerful theoretical
technique, has recently been applied by Poole et al. (2016) to ensembles of multi-layer random
neural networks. This theory studies networks with an i.i.d. Gaussian distribution of weights and
biases. Most recent work focuses on the analysis of order parameter flows and their fixed points
(Schoenholz et al., 2017; Xiao et al., 2018; Yang & Schoenholz, 2017), including their stability
and decay rates. Importantly, Karakida et al. (2018) also successfully used mean-field analysis to
estimate the spectral properties of the Fisher Information Matrix.
In this paper, we analytically quantify the impact of BatchNorm on the landscape of the loss function,
by using mean-field theory to estimate the spectral properties of the Fisher Information Matrix (FIM)
for typical batch-normalized neural networks. In particular, it is shown that BatchNorm reduces the
maximal eigenvalue of the FIM provided that the normalization coefficient γ is not too large. By
drawing on results linking Fisher Information to the geometry of the loss function, we explain how
BatchNorm neural networks can be trained with a larger learning rate without leading to parameter
1
Under review as a conference paper at ICLR 2019
explosion, and provide upper bounds on the learning rate in terms of the BatchNorm parameters.
As an additional contribution motivated by our theoretical findings, we demonstrate an empirical
correlation between the BatchNorm parameter γ and test loss. In particular, networks with smaller
γ achieve lower loss after a fixed number of training epochs.
2	Preliminaries
In our theoretical analysis, we employ the recent application of mean-field theory to neural networks
which studies an ensemble of random neural networks with pre-defined i.i.d. Gaussian weights
and biases. In this section, we provide background information and briefly recall the formalism of
Karakida et al. (2018) which first computes spectral properties of the Fisher Information of a neural
network and then relates it to the maximal stable learning rate.
2.1	Fisher Information Matrix and Learning Dynamics
Given a data distribution D over the set of instance-label pairs X × Y, a family of parametrized
functions fθ : X → Y and a loss function l(f, y), our focus will be to ensure convergence of the
following gradient descent with momentum update rule:
Θt+1 = θt - ηVL(θt) + μ(θt - θt-i) ,	(1)
where L(θ) is the unobserved population loss,
L(θ) := E(x,y)〜D [l(fθ(x),y)].	⑵
In practice, the parameters are determined by minimizing an empirical estimate of equation 2 using a
stochastic generalization (SGD) of the update rule equation 1. We neglect this difference by always
working in the asymptotic limit of large sample size and moreover assuming full-batch gradient
updates.
Suppose the loss function can be expressed in terms of a parametric family of positive densities
as l(fθ(x), y) =: - logpθ(x, y). This assumption holds true for a large class of losses including
squared loss and cross-entropy loss. Let Iθ denote the Fisher Information Matrix (FIM) associated
with the parametric family induced by the loss,
Iθ := E(x,y)〜Pθ [Vθ logPθ(x, y)乳 Vθ logpθ(x, y)] ,	(3)
where 0 denotes Kronecker product and Pθ denotes the probability distribution over XXY with
density pθ (x, y). Recall that under suitable regularity conditions the following identity holds:
Iθ = -E(x,y)〜Pθ HeSSlog Pθ(X,y)
θ
(4)
where HeSSθ denotes the Hessian with respect to θ. The above right-hand side is closely related to
the Hessian of the population loss,
Hess (L(θ)) = -E(x,y)〜D Hess logPθ (x,y),	⑸
θ
where we interchanged the Hessian with the expectation value. In fact, if we assume that the esti-
mation problem is well-specified so that there exist parameters θ* such that the data distribution is
generated by Pθ* = D, then We obtain the following equality between the Hessian of the population
loss and the FIM evaluated at the optimal parameters,
HeSS(L(θ*)) = Iθ* .	(6)
If θ is initialized in a sufficiently small neighborhood of θ*, then by expanding the population loss
L(θ) to quadratic order about θ* one can show that a necessary condition for convergence is that the
step size is bounded from above by (LeCun et al., 2012; Karakida et al., 2018)1,
η<η :=_2(1 + μμ_ = 2(1 + μ)	⑺
λmaχ(Hess(L(θ*)))	λmaχ(Iθ*),	' '
1In the quadratic approximation to the loss, the optimal learning rate is in fact η"2.
2
Under review as a conference paper at ICLR 2019
where λmax(M) denotes the largest eigenvalue of the matrix M. Rather than computing the optimal
parameters θ* directly, We follow the strategy of Karakida et al. (2018) by estimating the following
quantity and arguing that the distribution of the weights and biases is not significantly impacted by
the training dynamics,
XmaX ：= E [λmaχ (Iθ)] ,	(8)
θ
where Eθ denotes the expectation value with respect to the weights and biases. This heuristic was
shown to yield a remarkably accurate prediction of the maximal learning rate in (Karakida et al.,
2018).
In this paper we adopt the data modeling assumption that the joint density factors as pθ (x, y) =
p(x)pθ(y | x) where p(∙) denotes the probability density of the marginal distribution of the covari-
ates, which is independent of θ. Under this factorization assumption, the FIM simplifies to
Iθ = E(χ,y)〜Pθ [V logPθ(y | x)脸 Ne logPθ (y | x)] .	(9)
Focusing on the Gaussian conditional model pe(y |x) 8 exp( 1 ∣∣fe(x) 一 y∣∣2), the FIM further
simplifies to
Ie = Ex〜D [Vefe(x) 0Vefe(x)] .	(10)
The family of parametrized functions fθ ： RN0 → RNL is chosen to be the family of functions
computed by a multi-layer neural network architecture with N0 input nodes, NL output nodes and
L ≥ 1 layers. In this paper, we consider neural networks consisting of fully-connected (FC) and
convolutional (Conv) layers, with and without batch normalization. The pointwise activation is
denoted by σ, which is taken to be the rectified linear unit (ReLU) in this paper. Our analysis can
be straightforwardly extended to other architectures and non-linearities. We use hle(x) to denote the
output of layer l and the input to layer l + 1. Clearly we have he0(x) = x and heL(x) = fe(x).
3	Theory
In this section we focus on applying dynamical mean-field theory to study the effect of introducing
batch normalization modules into a deep neural network by estimating the largest eigenvalue of the
FIM. This estimate, in turn, provides an upper bound on the largest learning rate for which the learn-
ing dynamics is stable. This section is structured as follows: We first define various thermodynamic
quantities (order parameters, 6 for fully-connected layers and 9 for convolutional layers) that satisfy
recursion relations in the mean-field approximation. Then we present an estimate of λXmaX in terms
of these order parameters, generalizing a result of Karakida et al. (2018). Using this estimate, we
study how XmaX and % are affected by BatchNorm and calculate their dependence on the Batch-
Norm coefficient γ. Detailed derivations of the order parameters, their recursions, and the associated
eigenvalue bound are deferred to the Supplementary Material.
3.1	Fully Connected Layers
A general fully connected layer with input activation hl (x) and output pre-activation zl+1(x) is
described by the affine transformation,
zl+1(x) ：= Wl+1hl (x) + bl+1 ,	(11)
where Wl+1 ∈ RNl+1 ×Nl, bl+1 ∈ RNl+1 and Nl denotes the number of units in layer l. In the
framework of mean-field theory, we will consider an ensemble of neural networks with Gaussian
random weights and biases distributed as follows,
[Wl+1]ij 〜N(0,σW/Ni) ,	bi+1 〜N(0,σ2) .	(12)
In the case of a standard fully connected layer, the input activation satisfies the recursions hl (x) =
σ(zl (x)), where σ denotes the pointwise activation.
A batch-normalized fully connected layer, in contrast, satisfies the following recursion,
hl (x) ：= σ (Z (Xsl 〃 Θ Yl + βι) ,	(13)
3
Under review as a conference paper at ICLR 2019
where Θ denotes the elementwise (Hadamard) product, μl ∈ RNl and (sl)2 := Sl ΘSl ∈ RNl denote
the mean and variance of the pre-activation layers with respect to the data distribution,
μl := E [zl(x)] ,	(14)
(sl)2 ：= E[(zl(x)-μl)2] .	(15)
The weights and biases are drawn from the same distributions as in the standard, no BatchNorm,
case. In addition, we now have the BatchNorm parameters γl+1, βl+1 ∈ RNl+1 which are assumed
to be non-random for simplicity,
γl [i] = γl ,	βl [i] = 0 .	(16)
3.1.1	Order Parameters and Their Recursions
To investigate the spectral properties of the FIM, we define the following order parameters,
ql := ɪ E [kzl(x)k2] ,	^ := ɪ E [khl(x)k2] ,	(17)
Nl x,θ	Nl x,θ
qxy = B E Jhzl(X),zl⑻i] ,	qXy = B E Jhhl(X),hl⑻i] ,	(18)
Nl x,y,θ	Nl x,y,θ
ql:= E [kδl (x)k2],	或y：= E [hδl(x),δl(y)i] ,	(19)
x,θ	x,y,θ
where ∣∣ • ∣∣ denotes the Euclidean norm and δl(x) := f (x). Here We assume that the data X are
drawn i.i.d. from a distribution with mean 0 and variance 1, and also that the last layer is linear for
classification. We then have the base cases: q0 = 0, ^0y = 1, qL = GLy = 1. The order parameters
in the absence of BatchNorm satisfy the following recursions derived in Karakida et al. (2018),
ql = % ,	(20)
qXy = 2∏ (J1 - Cxy + ^y------+ cχy SinT Cxy) , QI)
σ2 ql+1
qxy = w2∏Xy(2 + SinT Cxy) ,	(22)
where Cxy := qxl y/ql. In the case of batch normalization we find the following recursions, which are
derived in the Supplementary Material,
ql = σ2 + σ22 ql-1 ,
qxy = σb +
σwqxy ,
2 2 l+1
~l = Yl QW q_
q = 1 -^丁 ,
Yl2σW q%'
ql
(23)
(24)
(25)
4
3.2	Convolutional Layer
The mean field theory of convolutional layer was first studied by Xiao et al. (2018). In this paper, the
results of the preceding section also apply to structured affine transformations including convolu-
tional layers. Let Kl denote the set of allowable spatial locations of the the lth layer feature map and
let Fl+1 index the sites of the convolutional kernel applied to that layer. Let Cl denote the number
of input channels. The output of a general convolutional layer is of the form,
zαl+1(X) = X Wβl+1hlα+β(X)+bl+1 ,	(26)
β∈Fl+1
where α ∈ Kl+1, Wβl+1 ∈ RCl+1 ×Cl and bl+1 ∈ RCl+1. The weights and biases are now distributed
as
[Wα+1]ij 〜N(0,σW/Nl) ,	bi+1 〜N(0,σξ) .	(27)
4
Under review as a conference paper at ICLR 2019
where now Nl := Cl |Fl+1 |. As in the fully connected case, we consider convolutional layers
with both vanilla activation functions of the form hla(x) := σ(zal (x)) as well as batch normalized
convolutional layers, for which the input activations satisfy the recursive identity,
ha(x):=σ (z⅜* θ γl+βl
(28)
3.2.1	Order Parameters and Their recursions
Similar to the definitions for fully connected layer, we define the following set of order parameters:
ql :	=--E E Cl a x,θ		kzal (x)k2	,		ql ：二	=C1lExEθ [同(切2],				(29)
qxl y :	F E	x,Ey,θ hzal (x), zal (y)i			qxy :"	1 =4	Eax,Ey,θhhla(x),hla(y)i			,	(30)
qae,xy :	=ɪ E Cl a6=e		E hzal (x), zel (y)i x,y,θ		, qaβ,xy :"	=ɪ E Cl a6=e		E hhla(x), hle(y)i x,y,θ		,	(31)
九	=EE a x,θ	kδal (x)k2	,			qxy :	=E a	E x,y,θ	hδal (x), δal (y)i	,	(32)
qaβ,xy :一	=E a6=e	E hδal (x), δel (y)i x,y,θ		,						(33)
where now δ% := ∂fθ/∂zla in analogy with the fully connected layer. The expectations over ɑ
and β are with respect to the uniform measure over the set of allowed indices. For a standard
convolutional layer without BatchNorm, the order parameters can be shown to satisfy the following
recursion relations:
, 727
题 2 d 方σ-2w 2σw
q = σ2 + σW qi-1 ,
qxy = σb + σwqxy ,
β,xy = σb + σwqaβ,xy
2
ql = σw ql+1 ,
===
yy
q lx x l
G 国 方
la
2	cxyπ
1 - Cxy +	2	+ Cxy Sm
1	2	. Caeπ	∙
1 - Cae +	2----+ Cae Sil
=
y
,x
e
灵
2 + sin-1 Cxy
π + sin-1 Cae
(34)
(35)
(36)
(37)
(38)
In the case of convolutional layers with BatchNorm, the following recursions hold:
ql = σ2+ σW qι-1 ,
qxy = σ2+ σW qx- 1
ql
=σ2+ σW q0e1xy
2 2 l+1
_ γl σw q__
=F 丁 ,
2 2 l+1
=Y2σW qaβ,xy
—4	ql	,
(39)
(40)
(41)
(42)
(43)
where Cxy := qxl y/ql and Cae := qal e,xy /ql . The derivations of the recursion relations for both
vanilla and batch-normalized convolutional layers are deferred to the Supplementary Material.
5
Under review as a conference paper at ICLR 2019
3.3	Eigenvalue bound and thermodynamic variables
The order parameters derived in the previous section are useful because they allow us to gain infor-
mation about the maximal eigenvalue XmaX of the FIM. We derived a generalization of (Karakida
et al., 2018, Theorem 6) to allow for the inclusion of batch normalization and convolutional layers.
In particular, we obtain a lower bound on the maximal eigenvalue λXmaX in terms of the previously
introduced order parameters which satisfy the stated recursion relations in the mean-field approxi-
mation.
Claim 3.1. If the layer dimension Nl of the fully connected layers and the number of channels Cl
of the convolutional layers satisfy Nl	1 and Cl	1 for 0 < l < L, we have,
λXmaX ≥ X fl ,	(44)
l∈[L]
where fι = Nl-Iqx-1 Gxy forfuUy connected layers and
fι = Nl-ι [(∣κι∣- IWaβ,xy+服]h(∣κlι - i)qχy+qx- 1i ,	(45)
for convolutional layers, where recall Nl-1 = Cl-1|Fl |. The index sets Fl and Kl are defined in
section 3.2. The order parameters are defined in the previous subsection.
Now we are ready to calculate the lower bound on λXmaX fora given model architecture by calculating
the order parameters using their recursions. In the next section, we will focus on the numerical
analysis of these recursion relations as well as present experiments that support our calculation.
4	Numerical Analysis and Experiments
In order to understand the effect of BatchNorm on the loss landscape, we theoretically compute λXmaX
as a function of the BatchNorm parameter γ, for both fully connected and convolutional architectures
(Fig. 1) with and without BatchNorm. For γ . 3 (typical for deep network initialization (Ioffe &
Szegedy, 2015)) BatchNorm significantly reduces λXmaX compared to the vanilla networks. As a
direct consequence of this, the theory predicts that batch normalized networks can be trained using
significantly higher learning rates than their vanilla counterparts.
We tested the above theoretical prediction by training the same architectures on MNIST and CIFAR-
10 datasets, for different values of η and γ, starting from randomly initialized networks with same
variances employed in the mean-field theory calculations. As shown in Fig. 2, the (γ, log10 η)-plane
clearly partitions into distinct phases characterized by convergent and non-convergent optimization
dynamics, and our theoretically predicted upper bound % closely agrees with the experimentally
determined phase boundary. The experiment of vanilla network is shown in 6.4 as a baseline.
In addition to the striking match between our theoretical prediction and the experimentally deter-
mined phase boundaries, the experimental results also suggest a tendency for smaller γ-initiations
to produce lower values of test loss after a fixed number of epochs, i.e. faster convergence. We
leave detailed investigation of this initialization scheme to future work. Also, dark strips can be
observed in the heatmaps indicate the optimal learning rates for optimization, which is around %/2
and consistent with LeCun et al. (2012) in the quadratic approximation to the loss. Our analysis
also suggests that small γ initialization benefits the convergence of training. Additional experiments
supporting this intuition can be found in Section 6.5 of Supplementary Material.
The architectural design for our experiments is as follows. In the fully connected architecture, we
choose L = 4 layers with Nl = 1000 hidden units per layer except the final (linear) layer which
has NL = 10 outputs. Batch normalization is applied after each linear operation except for the
final linear output layer. The convolutional network has a similar structure with L = 4 layers. The
first three are convolutional layers with filter size 3, stride 2, and number of channels C1 = 30,
C2 = 60, C3 = 90. The final layer is a fully connected output layer to perform classification. The
other architectural/optimization hyperparameters were chosen to be σw2 = 2, σb2 = 0.5, β = 0 and
μ = 0.9. Momentum μ here was set to be 0.9 to match the value frequently used in practice, which
only affects the dependency of η on FIM.
6
Under review as a conference paper at ICLR 2019
0.1 0.9 1.7 2.5 3.3 4.1
ɑ(b) Fully Connected
0.0
-0.5-
-1.0
-1.5-
-2.0
-2.5
-3.0
0.1 0.9 1.7 2.5 3.3 4.1
Y
(c),	Convo IUtional
3500
3000
2500
S 2000-
二 1500-
1000-
500-
o-.
0.1 0.9 1.7 2.5 3.3 4.1
Y
(d) Convolutional
0.5
o.o-
-0.5
g -ɪ ɑ
§ -1.5-
- -2.0
-2.5
-3.0
0.1 0.9 1.7 2.5 3.3 4.1
Y
Figure 1: The maximum eigenvalue XmaX and associated critical learning rate η for vanilla (blue)
and BatchNorm networks (red) as a function of the BatchNorm parameter γ for different choices
of architecture (fully-connected and convolutional), calculated by theory. (a, c) shows the flattening
effect of BatchNorm on the loss function for a wide range of hyperparameters and (b, d) further
show that for sufficiently small γ BatchNorm enables optimization with much higher learning rate
than vanilla networks.
Figure 2: Heatmaps showing test loss as a function of (log10 η, γ) after 5 epochs of training for
different choices of dataset and architecture. Results were obtained by averaging 5 random restarts.
The white region indicates parameter explosion for at least one of the runs. The red line shows the
theoretical prediction for the maximal learning rate η. The dark band on the heatmaps for CIFAR-
10 approximately tracks the optimal learning rate η"2 in the quadratic approximation to the loss.
Note the log scale for the learning rate, so the theory matches the experiments over three orders of
magnitude for η .
(c) Convolutional, CIFAR-IO
60
50
40
30
20
10
Y
5	Conclusion and Future Work
In this paper, we studied the impact of BatchNorm on the loss surface of multi-layer neural networks
and its implication for training dynamics. By developing recursion relations for the relevant order
parameters, the maximum eigenvalue of the Fisher Information matrix λXmaX can be estimated and
related to the maximal learning rate. The theory correctly predicts that adding BatchNorm with small
γ allows the training algorithm to exploit much larger learning rates, which speeds up convergence.
The experiments also suggest that using a smaller γ results in a lower test loss for a fixed number of
training epochs. This suggests that initialization with smaller γ may help the optimization process
in deep learning models, which will be interesting for future study.
The close agreement between theoretical predictions and the experimentally determined phase
boundaries strongly supports the validity of our analysis, despite the non-rigorous nature of the
derivations. Although similar approaches have been used in other work (Poole et al., 2016; Schoen-
holz et al., 2017; Yang & Schoenholz, 2017; Xiao et al., 2018; Karakida et al., 2018), we hope that
future work will place these results on a firmer mathematical footing. Furthermore, our BatchNorm
analysis is not limited to the convolutional and fully-connected architectures we considered in this
paper and can be extended to arbitrary feedforward architectures such as ResNets.
7
Under review as a conference paper at ICLR 2019
References
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. arXiv preprint arXiv:1603.05027, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2016b.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Ryo Karakida, Shotaro Akaho, and Shun ichi Amari. Universal statistics of fisher information in
deep neural networks: Mean field approach. arXiv preprint arXiv:1806.01316, 2018.
Diederik P. Kingma and Jimmy Lei Ba. Adam: A method for stochastic optimization. In Interna-
tional Conference on Learning Representations (ICLR), 2015.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. pp. 10971105, 2012.
Yann A LeCun, Leon Bottou, Genevieve B Orr, and Klaus-Robert Muller. Efficient backprop. In
Neural networks: Tricks ofthe trade, pp. 9-48. Springer, 2012.
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponen-
tial expressivity in deep neural networks through transient chaos. pp. 3360-3368, 2016.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
Imagenet large scale visual recognition challenge. 2015.
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch nor-
malization help optimization? (no, it is not about internal covariate shift). arXiv preprint
arXiv:1805.11604, 2018.
Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information
propagation. In International Conference on Learning Representations (ICLR), 2017.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Haim Sompolinsky and Annette Zippelius. Relaxational dynamics of the edwards-anderson model
and the mean-field theory of spin-glasses. Physical Review B, 25(11):6860, 1982.
Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel S. Schoenholz, and Jeffrey Penning-
ton. Dynamical isometry and a mean field theory of cnns: How to train 10,000-layer vanilla
convolutional neural networks. In International Conference on Machine Learning (ICML), 2018.
Greg Yang and Samuel S. Schoenholz. Mean field residual networks: On the edge of chaos. In
Advances in Neural Information Processing Systems (NIPS), 2017.
8
Under review as a conference paper at ICLR 2019
6	Supplementary Material
This section provides non-rigorous derivations of the order parameters, their recursions, and the
associated eigenvalue bound. Despite the non-rigorous nature of these calculations, we remark that
similar reasoning has been successfully used in a number of related works on mean-field theory,
demonstrating impressive agreement with experiments (Poole et al., 2016; Schoenholz et al., 2017;
Yang & Schoenholz, 2017; Xiao et al., 2018; Karakida et al., 2018).
6.1	Recursions for fully connected layers
Claim 6.1. Theforwardrecursionsfor 0 ≤ l ≤ L 一 1 are ql+1 = σ2 + σW ^ and qX+y1 = σ2 + σW ^xy
where ^ = γ2∕2 and qlxy = γ2∕(2π) for l ∈ [L — 1] whereas ^0 = 1 and ^°y = 0.
Derivation. In general, we have
ɪEhzX+1(x),zx+1(y)i = σ2 + σ2hhθ(x),hθIy)) .	(46)
Nx+1 θ	Nx
Thus, setting l = 0 we obtain q1 = σw2 + σb2 if we assume Ekxk2 = N0, and qx1y = σb2 since
x
E hx, yi = hE x,E yi = 0.
x,y
Recall (for l > 0) zx+1(x) = Wx+1σχ (UX(x) Θ YX) + bl+1 where UI(x) = [zx(x) — μx]∕sx so
yτ~Ehzl+1(X),zl + 1⑻〉=σ2 + σW <σl(ul(X) θ Yl),σl(ul⑻ θ Yl)》.	(47)
NX+1 θ	NX
Therefore, setting X = y and taking expectation values over X gives,
ql+1 ：= ɪ Ekzl+1(χ)k2 , Nl+1 X,θ	(48)
2 =σ2 + 看 E khl(x)k2 , Nl X,θ	(49)
=σ2 + σWql ,	(50)
ql = ∖ E khl(x)k2 , Nl X,θ	(51)
=σW E σ2(γl UX(X) [1]), X,θ	(52)
' σw2	Dz σl2 (Yl z) ,	(53)
Yx2 	 2 ,	(54)
where we have approximated each component of the random vector Ul (X) as a standard Gaussian.
Similarly, taking expectations over X, y gives
qx+y1 = σ2 + σW E/l(ul(X)[1] Yl)σl(ul(y)[1] Yl) .	(55)
x,y,θ
Consider the approximation in which the random pair (Ul (X)[1], Ul(y)[1]) is Gaussian distributed
with zero mean and covariance,
Σl = Σlxx	Σlxy	= E Ul(X)[1]2	E Ul(X)[1]Ul(y)[1]	(56)
Σ := Σlxy	Σlyy := E Ul(X)[1]Ul(y)[1]	E Ul(y)[1]2	.	(56)
Then the recursion becomes
qX+1' σ2 + σW/Dz1 Dz2 σl( ∖∕ςXXZ zlYl)σl [q^ ]CXy z1 + q1- (Cxy)2 z2)Yl i , (57)
9
Under review as a conference paper at ICLR 2019
where clxy := Σlxy/ ΣlxxΣlyy. Observe that by independence of x and y,
E [uι(x)uι (y)] = E Mx"'®十 由 Ul (Zl(X)+ Z ⑹) 一，
x,y	x,y	sl2
_ EχZι(x)EyZι(y) + μ - μι (Exzl(x) + EyZι(y))
=0.
Thus Σlxy = 0 and consequently,
qx+1 = σ2 + σW Gxy
qXy = YL .
xy 2π
(58)
(59)
(60)
(61)
(62)
□
The derivation here assumes an infinitely large dataset. For limited dataset size m, an error is intro-
duced from the non-zero ratio of m/mx6=y where mx6=y denotes the total number of sample pairs
(x, y) where x 6= y. We have mx6=y = m2 -m and therefore the error is O(1/m) which is negligible
for most of the frequently-used datasets.
Claim 6.2. The backward recursions for l ∈ [1,L — 1] are ql = γl2σw	and Gxy = γl4σw qxy-
With base cases qL = qxLy = 1.
Derivation. For each layer l ∈ [L] and for each unit i ∈ [Nl] define δl [i] ∈ RNL by
δl[i]∙=包L
δ [i] := ∂zl[i].
In particular, since we assume linear output δL [i] = ei and thus
qL = E kδL[∙]k2 = 1 ,
x,θ
(63)
(64)
where 1 ∈ RNL is the vector of ones. For ease of presentation and without loss of generality we
restrict to the first component qGL[1] = 1 and abuse notation by writing qGL = 1. For l < L we have
by the chain rule,
δl[i]= X ⅞l+⅛1 δl+1[k] ,	(65)
k∈[Nl+1]
=Y|σ0 (Ul[i] γl[i]) X [Wl+1]kiδl+1[k] .	(66)
Hence,
δl(x)[i]δl(y) = Y∣2σl (ul(x)[i] γl[i]) σl (ul(y)[i] Yl田	X	[wl+1]ki[wl+1]kθiδl+1[k]δl+1[k0]
k,k0∈[Nl+1]
(67)
Following the usual assumption that the back-propagated gradient is independent of the forward
signal we obtain,
E hδl(x),δl(y)i = N X E ” σ (ul (x)[i] γl[i])σ0 (ul(y)[i] γl[i]) E hδl+1(x),δl+1(y)i
i∈[Nl]	(68)
10
Under review as a conference paper at ICLR 2019
Setting x = y and taking expectation values over x we obtain,
qι = E kδl(x)k2 , x,θ	(69)
‘ qq+1 γ2σWZDzσ0(γιz)2,	(70)
22 =γ2σW qι+ι 2	qι	.	(71)
Similarly, taking expectation values over x, y we obtain,
qlχy = E4 hδl(x),δl(y)i ,	(72)
x,y,θ
=σwY2qxy- Z DzIDz2“假％ γι)σ0 hq∑yy ⑶ zι+q1- (Cxy)2 Z)”], (73)
=σwγ2 翠.	(74)
4	ql
□
6.2 Recursions for convolutional layers
Recall that the output of a general convolutional layer is of the form,
zαl+1(x) = X Wβl+1hlα+β(x)+bl+1 ,	(75)
β∈Fl+1
where α ∈ Kl+1. As a concrete example, consider CIFAR-10 input of dimension 32 × 32 × 3
which is mapped by a convolutional layer with 3 × 3 kernels, stride 2 and 20 output channels and no
padding. Then C0 = 3, C1 = 20, |K0| = 1024 and |F1| = 9 and |K1| = 225.
We begin by deriving some useful identities for convolutional layers, before specializing to the
batch-normalized and vanilla networks. For each channel i ∈ [Cl], we have,
Eθ zαl+1(x)[i]zβl+1(y)[i] =σb2+Eθ	[Wβl+11]ij1[Wβl+21]ij2hlα+β1(x)[j1]hlβ+β2(y)[j2] ,
(β1 ,j1)∈Fl+1 ×[Cl]
(β2,j2)∈Fl+1 ×[Cl]
2
=σ2 + Nw X E <h0ɑ+δ(X),hβ+δ ⑻〉.	(76)
l δ∈Fl+1
Hence,
ɪE DzZl+1Xx^^),：e+r^ = σ2 + σ2 X E <hα+δ(x),hβ+δ(y)> ,	(77)
Cl+1 θ	Nl	θ
δ∈Fl+1
where Nl := Cl|Fl+1|.
Moreover, let us introduce the following shorthand,
E khlα (x)k2 := α	二 ɪ X M(x)k2 ,	(78) |Kl| α∈Kl
E hhlα (x), hlα (y)i := α	二看 X <hα(χ)M(y)> ,	(79) |Kl| α∈Kl
E	hlα (x),hlβ(y) := α6=β	二 ∣Kll(∣Ki∣- 1) .,β∈X..=S)	(80)
11
Under review as a conference paper at ICLR 2019
Then we can write the recursion relations as,
ql+1 := ɪEE kz∣+1(x)k2 ,
Cl+1 αx,θ
2
=E σ2 + Nw X EEkha+δ(x)k2	,
l δ∈Fl+1
2
=σ2 + σ EEkha(X)k2 ,
Cl x,θ α
=σ2 + σWql ,
^ :=71 EEkha(X)k2 .
Cl x,θ a
Furthermore we have,
qX+1 = ɪE E hza+1(χ),za+1(y)i ,
Cl+1 a x,y,θ
2
=E σb + # X Ee hha+δ (x),hla+δ (y)i
x,y	Nl	θ a
δ∈Fl+1
=σb + σw qxy ,
qXy := τ1 E EAhha(χ),ha(y)i.
Cl a x,y,θ
Finally, we have,
qal+β1,xy := E E hzal+1(X), zβl+1(y)i	,
,y	a6=β x,y,θ
E
x,y
E
x,y
2
σ2 + Nw X EaEehhβ+δ(x),ha+δ(y)i	,
Nl δ∈Fl+1 a6=β
2
σ2 + -wE EW(x),hla(y))	,
Cl θ a6=β
——σb + σwqaβ,xy ,
qaβ,xy := T1 ER EA hha(x),hβ (J))，
aβ,xy	Cl a6=β x,y,θ a β
where we used,
X X	hla+δ(X), hlβ+δ(y) =
δ∈F1 + 1 a,β∈Kι×Kr.a=β
X X hla+δ(X), X hlβ+δ(y)	- X hla+δ(X),hla+δ(y)
δ∈Fl+1	a∈Kl	β∈Kl	a∈Kl
= |Fl+1|	X hla(X), X hlβ(y)	- X hla(X),hla(y)
a∈Kl	β∈Kl	a∈Kl
= |Fl+1|	X	hla(X), hlβ(y)	.
a,β∈Kι ×K0a=β
(81)
(82)
(83)
(84)
(85)
(86)
(87)
(88)
(89)
(90)
(91)
(92)
(93)
(94)
(95)
(96)
(97)
(98)
6.2.1 Batch Normalization
Claim 6.3. The forward recursions for 0 ≤ l ≤ L 一 1 are ql+1 = -2 + σww ^, qX+1 = -2 + σw, ^Xy
and ql+,xy = -2 + -Wqαβ,xy WherefOr each l ∈ [L — 1] we have q1 = Y2/2 and qXy = qαβ,xy =
Yι2∕(2π).
12
Under review as a conference paper at ICLR 2019
Derivation. Recalling that ha(x) = σι(ula(x) Θ Yl) where Ua(X) := Za(SJμ and substituting into equation 85, equation 89 and equation 94 we obtain,	α	
^ ' / Dzσ2(γlz),	(99)
Yl2 =	 2 ,	(100)
q(y ` / Dzi Dz2 σl(γlzι)σl(γ1z2),	(101)
Y2 — 2π ,	(102)
qαβ,xy ' / Dzi DZ2 σl(γlZi )σl(γ1Z2),	(103)
Y 	 2π ,	(104)
	□
Claim 6.4. The backward recursions are ql = IJwYIqI + , qlxy = σw Yqqxy , and 盘产 Xy =	σW Y2⅜‰ _	4ql	.
In order to derive the backward recursion, define (for each l ∈ [L], i ∈ [Cl], α ∈ Kl)	
δi [i] = JdhL α[] = ∂zα[i].	(105)
Then by the chain rule,	
∂zl+i[k] δaH=	X	S1 δβ+1[k]. (β,k)∈Kl+1×[Cl+1]	a	(106)
Now,	
zβ+1[k] =	X	[Wlβ+1]kjσι (uβ+eo[j] YljD + bl+1[k], (β0,j)∈Fl+1 ×[Cl]	(107)
⅞θ[ =	X	[Wβ+ 1]kj σ0(uβ+eo [j] YljD -j δij δa,β+β0 , a	(β0,j)∈Fl+1×[Cl]	sβ+β0 [j]	(108)
=Wa-β ]kiσ0 (ua[i] γl[i]) sllʌi].	(109)
Thus,
δα [i] = sγ⅛ σ0 (“aw] Mi[)	x	[wβ+ι]kiδα-1β 肉.	(“。)
α	(β,k)∈Fl+1 ×[Cl+1]
By the distributional assumption on the weights, for each (α, β) ∈ Kl × Kl we have,
2
E	X	[Wβ+ 1]kii[Wβ+1]k2iδa-1β1 [kι]δβ+1β2[k2] = NW X E hδa-1δ(χ),δβ+1δ(y)i.
(β1 ,k1 )∈Fl+1 ×[Cl+1 ]	δ∈Fl+1
(β2 ,k2 )∈Fl+1 ×[Cl+1 ]
(111)
Thus, under the usual independence assumptions,
2
E hhδIa(χ),δβ(y)i = Nw X E
l i∈[Cl]
Yι[i]2	σ0
sa[i]sβ [i] l
(Ua(X)[i] γι[i]) σ0 (Ue(y)[i] γι[i])	E E hδ∣+1δ(x),δβ-1δ(y)i .
δ∈Fl+1
(112)
13
Under review as a conference paper at ICLR 2019
Setting α = β, x = y, averaging over α and taking the expectation value over x,
q ：= EE kδα(x)k2 ,
α x,θ
=N22CιEθ X Ekδα-δ(x)k2，
l q , δ∈Fl+1
22
=λT YlC1∣F1+11EE kδα(x)k2 ,
Nl 2ql	α x,θ α
=σW 22ql+1
=2ql .
Similarly, setting α = β, averaging over α and taking expectation values over x, y gives
qqxl y :=E E hδαl (x), δαl (y)i
α x,y,θ
=σW 22ql+1
=4ql	.
(113)
(114)
(115)
(116)
(117)
(118)
Now,
hδαl+-1δ(x),δβl+-1δ(y)i
α,β∈K1 + 1×K1 + 1.α=β
δ∈Fl+1
X [* X δα+1δ(χ), X δβ+1δG
δ∈Fl+1	α∈Kl+1	β ∈Kl+1
-	hδαl+-1δ(x),δαl+-1δ(y)i
α∈Kl+1
|Fl+1 |
X δαl+1(x), X δβl+1(y) -
α∈Kl+1	β∈Kl
hδαl+1(x), δαl+1(y)i
α∈Kl+1
|Fl+1|	X	δαl+1(x), δβl+1(y)
ɑ,β∈Kι + ι ×Kι + ι:α=β
Let us further assume that
1
|K1 I(∣K1∣-1)
X	x,Ey,θ δαl+1(x),δβl+1(y)
α,β∈Kι×Kι∙∙α=β ，y，
1
|Kl + 1|(|Kl + 1| - 1)
X	x,Ey,θ δαl+1(x),δβl+1(y)
α,β∈K1 + 1 ×Kι + ι-.α=β
(119)
(120)
(121)
(122)
(123)
(124)
Thus, taking expectation values over (x, y) and averaging over the allowable indices such that α 6= β
we obtain,
αEβ IE*⑺总(y)i〕=中σwγ2α=β LE,θ"(x),δβ+1(y)i] ∙	(125)
It follows that
qqαl β,xy := E Eθδαl (x), δβl (y)
α6=β x,y,θ
σ2γ2qql+1
_ σw 2 qɑβ,xy
=42l.
(126)
(127)
14
Under review as a conference paper at ICLR 2019
6.2.2 VANILLA CNN
Claim 6.5. The forward recursions are ql+1 = σ2 + σWql, qX+y1 = σ2 + σW^xy and qll+βxy =
σ2 + σWqαβ,xy where
ql = 2 ql ,	(128)
qxy = 2∏ hq1 - cXy + ^y----+ cxy SinT(Cxy R ,	(129)
qαβ,xy = 2∏ hq1 - cαβ,xy ++ cαβ,xy sin I(Caβ,xyR ,	(130)
and cxy := qxy/ql，cαβ,xy := qαβ,xy /q.
Derivation. Substituting into equation 85, equation 89 and equation 94 we obtain,
^l := / Dzσ2(√qz), ql = —— 2 ,	(131) (132)
qxy = / Dz1 Dz2 σl(√ql zlYl)σl h√ql gyzl + q1 - (clxy)2 z⅛) Yl ],	(133)
=2qq∏ [J1 - CXy + ^y	+ cxy sin-1 (CXy)],	(134)
qαβ,xy = / DzIDz2 σl(√ql zlYl)σl [√qF (c0ɑβ,xy z1 + J1 - (c'ɑβ,xy )2 z2) Yl],	(135)
=2∏ [1- - c2αβ,xy + αβ^y + caβ,xy SinT (caβ,xy )].	(136)
□
Claim 6.6. The backward recursions are	ql	=	σw2 + ,	GXy	=	σwqxy	[∏	+ sin-1(CXy)]	and
σ2 ql+1
qaβ,xy = W 2αβ,x [π2 + sin l(cɑβ,xy )] .
Derivation. In the backward direction, we have
∂zβl+1[k]
δα [i]=	χ	焉"δβ+1[k],	(137)
(β,k)∈Kl+1 ×[Cl+1]	α
= σ'l(Z. [i])	X	[Wβ+1]kiδα+1β [k]	(138)
(β,k)∈Fl+1 ×[Cl+1 ]
Under the usual independence assumptions,
2
E hδlα(x),δβ(y)i = σwE S0 (Za(x)) ,σ0 (zβ(y))〉X hδ^(x),δβ-1δ(y)i .	(139)
θ	Nl θ	δ∈Fl+1
15
Under review as a conference paper at ICLR 2019
Thus,
ql ：= E E kδα(x)k2 ,
α x,θ
=σW ql+1
=-2-
qxy=E E ,hδα(X),δα(y)i,
α x,y,θ
σW 厦+1 ∏ ,	.-1	、
= ^Γ~ U + sin	Cxy)
qαβ,χy =OE0 xEθ <δα(X),δβ⑻〉,
α6=β x,y,θ
2 l+1
―σwqαβ,xy ∏ c∙ -1 z. ʌ
=2∏	(2 + sin	cαβ,xyj
6.3 Derivation of Claim 3. 1
Recall that the Fisher information matrix Iθ ∈ Rn×n is given by
Iθ = EJVθfθ(x) 0Vθfθ (x)].
x〜D
(140)
(141)
(142)
(143)
(144)
(145)
□
(146)
The claim is that the maximum eigenvalue of the Fisher Information Matrix is bounded as follows
E(x,y)〜DhVθfθ(x), Vθfθ(y)i ≤ λmaχ(Iθ) ≤ Ex〜D(Vjfθ(x), Vθfθ(x)i .	(147)
The upper bound follows from convexity of λmaχ(∙). Karakida et al. (2018) obtain the lower bound
by considering the empirical estimate of the Fisher Information Matrix,
m
1
Iθ = — V Vj fθ(Xi) 0 Vjfθ(Xi),
m
i=1
=—Bm Bm .
mm
(148)
(149)
where We have defined the matrix Bm ∈ Rm×n with components [Bm]j := df∂θxi). We can then
define a symmetric matrix Fbj ∈ Rm×m with the same eigenvalues as Ibj,
Fbθ = —BmBm ,	(15O)
mm
The maximal eigenvalue of Ij is thus computed by the Rayleigh quotient,
ʌ ∕jt ∖ a / π ∖	/ π ∖	∕yly∖
λmax(Ij ) = λmax (Fj ) = max hv, Fj vi .	(151)
v:kvk=1
Letting 1 ∈ Rm denote the vector of ones,
λ f'T ∖、
λmax(Ij ) ≥
(√1m 1,Fbθ √1m1),
m12	X〈Vefθ(xi), Vθfθ(Xj)〉,
i,j∈[m]
which is the the plug-in estimator (V-statistic). Thus, for a layered neural network we have
λmax(Iθ) ≥ m XX <Vθ"θ (x), Vθιfθ (y)〉,
(x,y) l∈[L]
=: X fl .
l∈[L]
(152)
(153)
(154)
(155)
16
Under review as a conference paper at ICLR 2019
Specializing to a fully-connected neural network we obtain,
fl
+ ∕∂∕θ (x) ∂fθ(y) ∖	∕∂∕θ (x) ∂fθ (y)'
+ ∖ ∂γι , ∂γι ∕ + ∖ ∂βl , ∂βl /
` ɪ X / ∂fθ (x) ∂fθ (y) ∖
一m2 乙 ∖ ∂wl , ∂wl	,
(x,y)
=m X(hl-1(x),hl-1(y))(δl(x),δl(y)),
(x,y)
(156)
(157)
(158)
(159)
'm Xhj(χ),hj(y)>	m2 XG(X),δl(y))	,	(160)
(x,y)	(x,y)
=Nl-Iqx- 1 qXy .	(161)
In the first approximation we use the fact that the terms with respect to b, γ, β are Nl times smaller
than the term with respect to w. The second approximation uses the assumption that forward and
backward order parameters are independent. The last approximation is for m 1.
For convolutional layers we have
f J X Γ/∂fθ(x) ∂fθ⑹ ∖ + / ∂fθ(x) ∂fθ⑹ ∖
fl = m2	∖ ∂bl , ∂bl / + ∖ ∂Wl , ∂Wl
(x,y)
+ / ∂fθ(x) ∂fθ(y) ∖ + / ∂fθ (x) ∂fθ (y) ∖]
+ ∖ ∂γl , ∂γl /+ ∖ ∂βl , ∂βl / ],
‘ ɪ X / ∂fθ (x) ∂fθ (y) ∖
一m2 乙 ∖ ∂wl , ∂wl	,
(x,y)
=m XXX hδβ(x),δβ0(y)ihhα+1β(x),hα+1βo(y)i,
(x,y) α∈Fl β,β0 ∈Kl
(162)
(163)
(164)
(165)
1
而
W X X hδβ(x),δβ0⑹)
(x,y) β,β0∈Kl
mXX X hhα+1β(x),hα+1βo(y)i	,
(x,y) α∈Fl β,β0∈Kl
(166)
[∣κll(∣κlI-1)九,xy + IKMxy] ]Cl-1lFllllKllll2lKll-1)qj,xy + Cl-M1Fl1 ^x-1	,
(167)
Nl-1 [(lκll-Dqaβx + 服][(lκll-IWaeIxy + qx-1i ,	(侬)
where Nl-1 = Cl-1 lFl l. In the first approximation we use the fact that the terms with respect to
b, γ, β are cl times smaller than the term with respect to w . The second approximation uses the
assumption that forward and backward order parameters are independent.
6.4 Baseline
Baseline was an experiment of vanilla fully-connected networks trained on MNIST, with various σw
weight initializations. Result is shown in Fig. 3.
6.5 Additional Experiments
Our theory predicts that smaller γ has the effect of greatly reducing the λmax of the FIM, and empir-
ically networks with BatchNorm converge faster. Following this intuition, we performed additional
17
Under review as a conference paper at ICLR 2019
Figure 3: Heatmap showing test loss as a function of (log10 η, γ) after 5 epochs of training for vanilla
fully-connected feed forward network where the relation between weight initialization variance and
maximal learning rate is studied.
experiments with VGG16 Simonyan & Zisserman (2014) and Preact-Resnet18 He et al. (2016a),
with various γ initializations, fixed learning rate 0.1, momentum 0.9 and weight decay 0.0005,
trained on CIFAR-10. The result is average over 5 different independent trainings. We find that
smaller γ initialization indeed increases the speed of convergence, as shown below in Fig. 4.
Aualnuue
VGG16
----test, γ= 0.1
----train, γ= 0.1
----test,γ=l
----train, y= 1
IOO
80
60
40
20
Preact-ResnetlB
0	20	40	60	80	100	0	20	40	60	80	100
epoch	epoch
Figure 4: Learning curves of (a) VGG16 and (b) Preact-Resnet18 training on CIFAR-10, with the
same hyperparameters except γ initialization. Results support that small γ initialization helps faster
convergence.
18