Under review as a conference paper at ICLR 2019
Interpreting Layered Neural Networks via
Hierarchical Modular Representation
Anonymous authors
Paper under double-blind review
Ab stract
Interpreting the prediction mechanism of complex models is currently one of the
most important tasks in the machine learning field, especially with layered neural
networks, which have achieved high predictive performance with various prac-
tical data sets. To reveal the global structure of a trained neural network in an
interpretable way, a series of clustering methods have been proposed, which de-
compose the units into clusters according to the similarity of their inference roles.
The main problems in these studies were that (1) we have no prior knowledge
about the optimal resolution for the decomposition, or the appropriate number of
clusters, and (2) there was no method with which to acquire knowledge about
whether the outputs of each cluster have a positive or negative correlation with
the input and output dimension values. In this paper, to solve these problems, we
propose a method for obtaining a hierarchical modular representation of a layered
neural network. The application of a hierarchical clustering method to a trained
network reveals a tree-structured relationship among hidden layer units, based on
their feature vectors defined by their correlation with the input and output dimen-
sion values.
1 Introduction
To construct a method for interpreting the prediction mechanism of complex statistical models is
currently one of the most important tasks in the machine learning field, especially with layered
neural networks (or LNNs), which have achieved high predictive performance in various practical
tasks. Due to their complex hierarchical structure and the nonlinear parameters that they use to
process the input data, we cannot understand the function of a trained LNN as it is, and we need
some kind of approximation method to convert the original function of an LNN into a simpler
interpretable representation.
Recently, various methods have been proposed for interpreting the function of an LNN, and they
can be roughly classified into (1) the approximation of an LNN with an interpretable model, and (2)
the investigation of the roles of the partial structures constituting an LNN (e.g. units or layers). As
for approach (1), various methods have been investigated for approximating an LNN with a linear
model (Lundberg & Lee, 2017; Nagamine & Mesgarani, 2017; Ribeiro et al., 2016) or a decision
tree (Craven & Shavlik, 1996; Johansson & Niklasson, 2009; Krishnan et al., 1999; Thiagarajan
et al., 2016). For image classification tasks in particular, methods for visualizing an LNN function
have been extensively studied in terms of which part of an input image affects the prediction re-
sult (Ancona et al., 2018; Bach et al., 2015; Shrikumar et al., 2017; Simonyan et al., 2014; Smilkov
et al., 2017; Springenberg et al., 2015; Sundararajan et al., 2017). Approach (2) has been studied by
several authors who examined the function of a given part of an LNN (Alain & Bengio, 2017; Luo
et al., 2016; Raghu et al., 2017; Zahavy et al., 2016). There has also been an approach designed to
automatically extract the cluster structure of a trained LNN (Watanabe et al., 2017b;a; 2018a) based
on network analysis.
Although the above studies have made it possible to provide us with an interpretable representation
of an LNN function with a fixed resolution (or number of clusters), there is a problem in that we
do not know in advance the optimal resolution for interpreting the original network. In the methods
described in the previous studies (Watanabe et al., 2017a;b; 2018a;b;c), the unit clustering results
may change greatly with the cluster size setting, and there is no criterion for determining the optimal
1
Under review as a conference paper at ICLR 2019
cluster size. Another problem is that the previous studies could only provide us with information
about the magnitude of the relationship between a cluster and each input or output dimension value,
and we could not determine whether this relationship was positive or negative.
In this paper, we propose a method for extracting a hierarchical modular representation from a
trained LNN, which provides us with both hierarchical clustering results with every possible num-
ber of clusters and the function of each cluster. Our proposed method mainly consists of three parts:
(a) training an LNN for a given data set based on error back propagation, (b) determining the feature
vectors of each hidden layer unit based on its correlation with the input and output dimension val-
ues, and (c) the hierarchical clustering of the feature vectors. Unlike the clustering methods in the
previous studies, the role of each cluster is computed as a centroid of the feature vectors defined by
the correlations in step (b), which enables us to know the representative mapping performed by the
cluster in terms of both sign and magnitude for each input or output dimension.
We show experimentally the effectiveness of our proposed method in interpreting the internal mech-
anism of a trained LNN, by applying it to two kinds of data sets: the MNIST data set that contains
digit image data and a sequential data set of food consumer price indices. Based on the experimental
results for the extracted hierarchical cluster structure and the role of each cluster, we discuss how
the overall LNN function is structured as a collection of individual units.
2 Training a Layered Neural Network
An LNN can be trained to approximate the input-output relationship of an arbitrary data set (x, y)
that consists of input data x ∈ RM and output data y ∈ RN, by using a function f (x, w) from
x ∈ RM and a parameter w ∈ RL to RN . An LNN parameter is defined by w = {ωidj , θid},
where ωidj is the connection weight between the i-th unit in a depth d layer and the j -th unit in
a depth d + 1 layer, and θid is the bias of the i-th unit in the depth d layer. Here, d = 1 and
d = d0, respectively, correspond to the input and output layers. The LNN function f(x, w) is a
set of functions {fj (x, w)} for all output dimensions j, each of which is defined by fj (x, w) =
σ(Pi ωidj0-1oid0-1 + θjd0-1). Here, σ(x) = 1/(1 + exp(-x)), and oid is the output value of the i-th
unit in the depth d layer and oi1 = xi holds in the input layer. Such output values in each layer are
given by ojd = σ(Piωidj-1oid-1 +θjd-1).
The purpose of training an LNN is to find an optimal parameter w to approximate the true input-
output relationship with a finite size training data set {(Xn, Yn)}nn1=1, where n1 is the sample size.
The training error E(W) of an LNN is given by E(W) = n1 Pn=I IlYn - f (Xn, w)k2, where ∣∣∙ k
is the Euclidean norm of RN .
Since the minimization of the training error E(W) leads to overfitting to a training data set, we adopt
the L1 regularization method (Ishikawa, 1990; Tibshirani, 1996) to delete redundant connection
weights and obtain a sparse solution. Here, the objective function to be minimized is given by
H(W) = n1 E(W) + λPdij ∣ωj∣, where λ is a hyperparameter used to determine the strength
of regularization. The minimization of such a function H(W) with the stochastic steepest descent
method can be executed by an iterative update of the parameters from the output layer to the input
layer, which is called error back propagation (Rumelhart et al., 1986; Werbos, 1974). The parameter
update is given by
∆ωidj-1 = -η(δjdoid-1 + λ sgn(ωidj-1)), ∆θjd = -ηδjd,
where δjd0 = (ojd0 - yj) (ojd0 (1 - ojd0 ) + 1), and δjd = Plkd=+11 δkd+1ωjdk (ojd (1 - ojd) + 1)
for d = do - 1,…，2. Here, y§ is the j-th output dimension value of a randomly chosen n-
th sample (Xn, Yn), 1is a hyperparameter for the LNN convergence, and η is the step size for
training time t that is determined such that η(t) 8 1/t. In the experiments, We adopt e1 = O.001 and
η = 0.7 × a1n1/(a1n1 + 5t), where a1is the mean iteration number for LNN training per dataset.
2
Under review as a conference paper at ICLR 2019
3 Hierarchical Modular Representation of LNNs
3.1	Determining Feature Vectors of Hidden Layer Units
To apply hierarchical clustering to a trained LNN, we define a feature vector for each hidden layer
unit. Let vk be the feature vector of the k-th hidden layer unit in a hidden layer. Such a feature
vector should reflect the role of its corresponding unit in LNN inference. Here, we propose defining
such a feature vector vk of the k-th hidden layer unit based on its correlations between each input or
output dimension. In previous studies (Watanabe et al., 2018b;c), methods have been proposed for
determining the role of a unit or a unit cluster based on the square root error. However, these methods
can only provide us with knowledge about the magnitude of the effect of each input dimension on a
unit and the effect of a unit on each output dimension, not information about how a hidden layer unit
is affected by each input dimension and how each output dimension is affected by a hidden layer
unit. In other words, there is no method that can reveal whether an increase in the input dimension
value has a positive or negative effect on the output value of a hidden layer unit, or whether an
increase in the output value of a hidden layer unit has a positive or negative effect on the output
dimension value. To obtain such sign information regarding the roles of each hidden layer unit, we
use the following definition based on the correlation.
Definition 1 (Effect of i-th input dimension on k-th hidden layer unit). We define the effect of
the i-th input dimension on the k-th hidden layer unit as viink, where
v
in
ik
E[(X(n)- EXn)])(Okn)- E[Okn)川
jEh(x(m - Exn)])2]e [(Okn)- E[Okn)巾
Here, E[∙] represents the meanfor all the data samples, X(n) is the i-th input dimension value ofthe
n-th data sample, and O(kn) is the output of the k-th hidden layer unit for the n-th input data sample.
Definition 2 (Effect of k-th hidden layer unit on j -th output dimension). We define the effect of
the k-th hidden layer unit on the j-th output dimension as vkouj t, where
out	E h(οkn)- E[οkn)])(yjn) - E[yjn)])i
Vkj =	2	2 .
yEh(οkn) -E[οkn)]) iE[(yjn) - e[yjn)]) i
Here, yj(n) is the value of the j -th output layer unit for the n-th input data sample.
We define a feature vector of each hidden layer unit based on the above definitions.
Definition 3 (Feature vector of k-th hidden layer unit). We define the feature vector of the k-th
hidden layer unit as Vk ≡ [v^, •…,Vink, vout, ∙∙∙ , vou；]∙ Here, io and jo, respectively, represent
the dimensions of the input and output data.
Alignment of signs of feature vectors based on cosine similarity The feature vectors of Defini-
tion 3 represent the roles of the hidden layer units in terms of input-output mapping. When inter-
preting such roles of hidden layer units, it is natural to regard the roles of any pair of units (k1, k2)
as being the same iff they satisfy Vk1 = Vk2 or Vk1 = -Vk2. The latter condition corresponds to the
case where the k1-th and k2-th units have the same correlations with input and output dimensions
except that their signs are the opposite, as depicted in Figure 1. To regard the roles of unit pairs that
satisfy one of the above conditions as the same, we propose an algorithm for aligning the signs of
the feature vectors based on cosine similarity (Algorithm 1). By randomly selecting a feature vector
and aligning its sign according to the sum of the cosine similarities with all the other feature vectors,
the sum of the cosine similarities of all the pairs of feature vectors increases monotonically. We
show experimentally the effect of this sign alignment algorithm in Appendix 2.
3.2	Hierarchical Clustering of Units in a Trained LNN
Once we have obtained the feature vectors of all the hidden layer units as described in section 3.1,
we can extract a hierarchical modular representation of an LNN by applying hierarchical clustering
3
Under review as a conference paper at ICLR 2019
k1-th hidden unit
Algorithm 1 Alignment of signs of feature
vectors based on cosine similarity
1:	Let Vk and a° respectively be the fea-
ture vector for the k-th hidden layer
unit and the number of iterations.
2:	for a = 1 to a0 do
3:	Randomly choose the k-th hidden
layer unit according to the uniform
distribution.
4:	if P …/ V 0	< 0 then
乙l=k √Vk∙VkVVLVI
5： Vk4——Vk.
6:	end if
7:	end for
SUOQE-φtoo
1
0.5
0
-0.5
-1
Input dimensions
0.5
0
-0.5
-1
X→ Y
Output dimensions
k2-th hidden unit
SUOQE-φtoo
1
0.5
0
-0.5
-1
Input dimensions
-X → -Y
Same function
Output dimensions
Figure 1: An example of two hidden layer units with
the same function. The corresponding feature vectors
are the same, except that their signs are opposite.
to the feature vectors. Among the several existing methods for such hierarchical clustering including
single-link and complete-link, Ward’s method (Ward, 1963) has been shown experimentally to be
effective in terms of its classification sensitivity, so we employ this method in our experiments.
We start with k0 individual hidden layer units, and sequentially combine clusters with the minimum
error sum of squares (ESS), which is given by
ESS≡X( X kvkk2-∣⅛l X vkl),	⑴
m k:uk ∈Cm	1 'm k∙Uk∈Cm
where Uk and Vk, respectively, are the k-th hidden layer unit (k = 1, •一，ko) and its corresponding
feature vector, Cm is the unit set assigned to the m-th cluster, and | ∙ | represents the cluster size.
From Equation (1), the ESS is the value given by first computing the cluster size (|Cm|) times the
variance of the feature vectors in each cluster, and then by taking the sum of all these values for all
the clusters. When combining a pair of clusters (Cm1 , Cm2) into one cluster, the ESS increases by
∆ESS
|Cmi||Cm2| Il 1 X v _	1 X
|Cm| 十 |Cm | Il |Cm」	乙 k - |Cm2 | 乙
m1	m2	m1	k:uk ∈Cm1	m2	k:uk ∈Cm2
Vk2
(2)
Therefore, in each iteration, we do not have to compute the error sum of squares for all the clusters,
instead we simply have to compute the error increase ∆ESS given by Equation (2) for all the pairs
of current clusters (Cm1 , Cm2), find the optimal pair of clusters that achieves the minimum error
increase, and combine them. We describe the whole procedure of Ward’s method in Algorithm 2.
This procedure to combine a pair of clusters is repeated until all the hidden layer units are assigned to
one cluster, and from the clustering result {Cm)} in each iteration t = 1,…，k0 一 1, we can obtain
a hierarchical modular representation ofan LNN, which connects the two extreme resolutions given
by “all units are in a single cluster” and “all clusters consist of a single unit.” The role of each
extracted cluster can be determined from the centroid of the feature vectors of the units assigned to
the cluster, which can be interpreted as a representative input-output mapping of the cluster.
4 Experiments
We apply our proposed method to two kinds of data sets to show its effectiveness in interpreting the
mechanism of trained LNNs. The experimental settings are detailed in the Appendix 3. In Appendix
1, we provide a qualitative comparison with the previous method (Watanabe et al., 2018c).
4.1	Experiment Using the MNIST Data Set
First, we applied our proposed method to an LNN trained with the MNIST data set (LeCun et al.,
1998) to recognize 10 types of digits from input images. Before the LNN training, we sharpened the
top, bottom, left and right margins and then resized the images to 14 × 14 pixels. Figure 2 shows
sample images for each class of digits. Although our proposed method provided us with a clustering
4
Under review as a conference paper at ICLR 2019
Algorithm 2 Ward’s hierarchical clustering method (Ward, 1963)
1:	Let Uk and Vk ,respectively, be the k-th hidden layer unit (k = 1, ∙∙∙ ,k°) and its corresponding
feature vector, and let {Cm(t) } be the unit set assigned to the m-th cluster in the t-th iteration
(m = 1,…，ko -1 + 1). Initially, We set t — 1 and CmI) - {um}.
2:	for t = 2 to k0 - 1 do
3:	(Cmt- 1),C(t-1)) - argmin(c(t-i),c(t-i))∆ESS(C(tτ),C(tτ)), where
AESSIcCO) ≡ |CC+CC0| Il |C| X vk - |C| X
k:uk∈C	Muk∈C0
Here, we assume m1 < m2 .
4:	Update the clusters as follows:
	CmT	(t-1)	(t-1) mi	∪ m2 (t-1) m (t-1) Cm+1	(m = m1) (1 ≤ m ≤ m2 - 1, m 6= m1) . (m2 ≤ m ≤ k0 - t + 1)
5: end for			
result for all the possible resolutions or the numbers of clusters c, we have only plotted the results
for c = 4, 8, 16, for ease of visibility. Figures 3 and 4, respectively, show the hierarchical cluster
structure extracted from the trained LNN and the roles or representative input-output mappings of the
extracted clusters. From these figures, we can gain knowledge about the LNN structure as follows.
-	At the coarsest resolution, the main function of the trained LNN is decomposed into Clusters 1, 2,
3 and 4. Cluster 1 captures the input information about black pixels in the shape of a 6 and white
pixels in the shape of a 7, and it has a positive and negative correlation with the output dimensions
corresponding to “6” and “7”, respectively. Cluster 2 correlates negatively with the region in the
shape of a 9, and positively with the other areas. It has a positive correlation with the recognition of
“2” and “6,” and it has a negative one with “0,” “4” and “9.” Cluster 3 correlates positively with the
black pixels in the left part of an image, and it has a positive correlation with “0,” “4” and “6,” and
a negative correlation with “3” and “7.” Cluster 4 captures the 0-shaped region, and it has a larger
correlation with the output of “0” compared with the other digits.
-	Cluster 2 is decomposed into three smaller clusters, 7, 8 and 9. Cluster 7 captures similar input
information to Cluster 2, and it also correlates strongly with the lower area of an image. This cluster
mainly affects the recognition result for “5” and “6.” Cluster 8 uses the input information of the area
with the shape ofa 9, however, its main recognition target is “2.” Cluster 9 correlates positively with
the area extending from the upper right to the lower left of an image, and it correlates negatively
with the digits “4” and “9.”
-	Cluster 8 consists of two smaller clusters, 17 and 18. Cluster 17 is mainly affected by the upper part
and lower right part of an image, and the absolute value of its correlations with output dimensions
are all less than 0.2, while the role of Cluster 18 is almost the same as that of Cluster 8.
4.2	Experiment Using the Consumer Price Index Data Set
We also applied the proposed method to an LNN trained with a data set of a consumer price in-
dex (e Stat, 2018) to predict the consumer price indices of taro, radish and carrot for a month from
36 months’ input data. With this data set, we plotted the results for c = 3, 6, 12, where c is the
number of clusters. Figures 5 and 6, respectively, show the hierarchical cluster structure extracted
from the trained LNN and the roles or representative input-output mappings of the extracted clusters.
From these figures, we can gain knowledge about the LNN structure as follows.
-	Clusters 1, 2 and 3 represent the main input-output function of the hidden layer units. Interest-
ingly, all of these clusters have similar correlations with the output dimensions (0 < radish < taro
< carrot). However, these three clusters use different input information: Cluster 1 strongly reflects
seasonal information, and its correlation is especially high with the consumer price indices of the
5
Under review as a conference paper at ICLR 2019
three vegetables one month before and one, two and three years earlier. Cluster 3 also reflects sea-
sonal information, however, the absolute values of the correlations are less than 0.3 and it correlates
strongly with the input information of eight, 20 and 32 months before. On the other hand, Cluster 2
does not use such a seasonal effect very much, and it is affected almost equally by the information
of all months, except the recent information of radish from nine months before.
-	Cluster 1 is composed of smaller clusters of 16 and 17. Cluster 16 is mainly used to predict the
consumer price index of taro and it strongly correlates with the input information for taro from one
month before and one, two and three years before. Compared with Cluster 16, Cluster 17 affects the
three output dimensions more equally.
-	Cluster 7is a part of Cluster 3, and consists of smaller clusters of 11, 12 and 13. These clusters have
mutually different relationships with the output dimension values: Cluster 11 correlates positively
with consumer price indices of taro and carrot, and negatively with that of radish. It mainly uses
recent information about carrot (within a year) and the values of taro of five, 17 and 29 months
before. Cluster 13 is mainly used to predict the radish output value. It has a positive correlation with
the input information for taro, radish and carrot of about six, 18 and 30 months earlier, and it has
a negative correlation with values for one month before and one, two and three years before. The
absolute values of the correlations between Cluster 12 and the output dimension values are less than
0.2, so, unlike with Clusters 11 and 13, it does not significantly affect the prediction result.
5	Discussion
Here, we discuss our proposed method for obtaining a hierarchical modular representation from the
perspectives of statistical evaluation and visualization.
Our proposed method provides us with a series of clustering results for an arbitrary cluster size,
and the resulting structure does not change if we use the same criterion (e.g. error sum of squares
for Ward’s method) for evaluating the similarity of the feature vectors. However, there is no way
to determine which criterion yields the optimal clustering result to represent a trained LNN, due
to the fact that interpretability of acquired knowledge cannot be formulated mathematically (al-
though there has been an attempt to quantify the interpretability for a specific task, especially image
recognition (Bau et al., 2017)). This problem makes it impossible to compare different methods
for interpreting LNNs quantitatively, as pointed out in the previous studies (Lipton, 2016; Doshi-
Velez & Kim, 2017). Therefore, the provision of a statistical evaluation method as regards both
interpretability and accuracy for the resulting cluster structure constitutes important future work.
Although we can apply our proposed method to an arbitrary network structure, as long as it contains
a set of units that outputs some value for a given input data sample, the visualization of the resulting
hierarchical modular representations becomes more difficult with a deeper and a larger scale network
structure, since a cluster may contain units in mutually distant layers. Additionally, the number
of possible cluster sizes increases with the scale (or the number of units) of a network, and so it
is necessary to construct a method for automatically selecting a set of representative resolutions,
instead of visualizing the entire hierarchical cluster structure.
6	Conclusion
Finding a way to unravel the function of a trained LNN is an important issue in the machine learning
field. While LNNs have achieved high prediction accuracy with various data sets, their highly com-
plex and nonlinear parameters have made it difficult to interpret their internal inference mechanism.
Recent studies have enabled us to decompose a trained LNN into simpler cluster structure, however,
there is no method for (1) determining the optimal number of clusters, or (2) knowing whether the
outputs of each cluster have a positive or negative correlation with the input and output dimension
values. In this paper, we proposed a method for extracting the hierarchical modular representation
of a trained LNN, which consists of sequential clustering results with every possible number of clus-
ters. By determining the feature vectors of the hidden layer units based on their correlations with
input and output dimension values, it also enabled us to know what range of input each cluster maps
to what range of output. We showed the effectiveness of our proposed method experimentally by
applying it to two kinds of practical data sets and by interpreting the resulting cluster structure.
6
Under review as a conference paper at ICLR 2019
Figure 2: Input image examples of MNIST data set.
Figure 3: Hierarchical clusters of an LNN (MNIST data set).
Cluster 1
-0.2
-0.2	0	0.2
-0.4
Input dimensions
0.4
0.2
0
0 1 23456789
Output dimensions
Cluster 2
-0.2
-0.2
0
0.2
-0.4
Input dimensions
0.4
0.2
0
0123456789
Output dimensions
Cluster 3
0.4
0.2
0
-0.2
-0.2	0	0.2
-0.4
Input dimensions

0123456789
Output dimensions
Cluster 4
-0.2
-0.2	0	0.2
-0.4
Input dimensions
0.4
0.2
0
0123456789
Output dimensions
Figure 4: Representative input-output mappings of extracted clusters.
7
Under review as a conference paper at ICLR 2019
0.8
a≈geo
geo
oueo
--Oue。
oueo
♦--Oue。
oueo
oueo
Sgg
oug
gg
gg
oug
oug
gg
oug
gg
gg
SS-gg
-OUg
a0Ug
agg
agg
agg
agg
-OUg
agg
agg
segg
-gg
-gg
-gg
-gg
-gg
-gg
-"p⅛
-"p⅛
s'!"p"¾
S"p⅛
-"p⅛
一"p"¾
"p⅛
-"p"¾
-"p"¾
Stl
tl
tl
tl
tl
tl
tl
tl
tl
tl
Satl
S-tl
atl
atl
atl
atl
atl
atl
atl
s-tl
sε-tl
tl
tl
tl
tl
tl
=MRtl
一eel
- eel
一eel
l eel
S- eel
- eel
- eel
- eel
- eel
- eel
- eel
- eel
- eel
- eel
Seel
eel
eel
eel
eel
& eel
& eel
eel
eel
eel
seel
eel
eel
eel
eel
eel
eel
Figure 5: Hierarchical clusters of an LNN (food consumer price index data set).
Cluster 1
ClUSter 4



0.6
0.4
0.2
0
-0.2
-0.4
⅛J	Lil	Ilhmll		
				-
0.
0..
0..
◎
0..
0
-0.2
-0.4
m


0
0
' .∙∙^ .：,•■, ...*l	.5..i∙	∙ S. ――"
、' '	、、、/ H …/
Input dmeiSons
ClUSter 5


0
-02
-04
-,'W
Output dimensions
S 芦、/、,
«。嫡嫡嫡


¥
Output dimensions
0.8
06 I
o' I
02
02
04
kJ扁/I	师Ii疝Ll	IMINl师 I
		
0
.S* * <s^ 0	.,`*
,Z ✓ ✓ Z ✓ .
-02
-04
, # ®0 ** *，
Input dme∣S0ns



Input dimensions
≠ $ ⅛ft W
√∙ <¥ J J
Output dimenSons
Cluster 2
ClUSter 6
0.8
0.8
0.6
0.4
0.2
0
-0.2
-0.4
0..
0.
0..
0..
◎
0

02
02
04
06 ■
04 I
IiiiiiiiiiiiiiiiiiIIII	NIIIiii 伽 Ir	■!■■I
		
0
-02
-04



-,'w
Output dimenSons

9

Input dimensions
Cluster 3
0.8
0.6
0.4
0.2
0
-0.2
-0.4
		内加1删
		-


Input dimensions
CIUSter 10
ClUSter 11
0.8
06
04 ■
02
02
04
	L“/LI疝	让“_刘曲,1【加％/制
		
■08
06
04
02
0
-02
-04
& 建 √s £
* <* M M
,/ / / /
Input dmenSons
√, **
f Xz Z Z z .
-∙W
OUput dme∣S0ns
08
02
06
04
02
04
08
06
04 ■
02 ,
M喻TM扁此砸Mh而h.
0.2
04
/	/ e* F &
-/■，，送 , S
Irpt dimensions
CIUSter 13
WW址喂
MItMG “/加
τιr
JkJIlkJIL
ιr.*u,.*nι.'∖




08
08
06
04
02
0
-02
06∣
02
» 彳,令,0
a ≠0zz√
s'ZZZ
Input dmeiSons
0 .g∙	，0 ʌ
/ 6 6' # /
-04
02
CIUSter 16
0
0
08
06
04
02
0
-02
-04
交及* .*■
<# <, <#
■ √∙ F .> $ Λ ** * ,∙S> 0 ,*∙,
- t.'' i'' Λ' ''' ''' ≈∙' X —Z
Input dme∣S0ns
*
-∙w
Output dimenSons
04
-∙W
Ouatdmensons
-0.2
-0.4
Output dimensions
Cluster 7
0.8
06
04 ■
02
02
04
加聊kj瓶.	.,帆州心血	出佩怖M崛
		
0
-02
-04
'	.∙^ / 3 5 i∙卢 J" ,√>	J
<' <' t,'s i's Λs z z HW' Z /
Input dmeiSons
/.Λ
0.8



-∙∙i>V
Output dimenSons
ClUSter 8
0.8
0.6
0.4
0.2
0
-0.2
-0.4
<W
Output dimensions
同机而∏l制螂I
/ «，«，/
CIUSter 14
0 ,#> f .0 ; ≠ 6 0 $ $
r≠ Z Z Z '*Z Z ZZZ
I3 dmenso∣s
9/
> X域
Cluster 17

0
0
0

06
04
02
02
0.4
Oupt dmenso∣s
06
04
02
02
04
Outpt dimensions
LllIllLJlmm⅛j∣m MlMMi
08
06
04
02
0.
J .∙>∙∙	..∙>	.∙S- 3* .∙'' .∙∙∙∙ .∙^'
，4 # # 4 ^zz ZZZZ
I3 dmenSo∣s
/ Z / / /
02
04
Outpt dimenSons
02 '
02
04
06 ■
04 ■
Mh⅜L,,j	11%.,MnL “加、.、	n∏k.-MlL,"M∏h. .M
		
0
-02
∕∙ ,∙1∙∙ .∙∙' .∙∙∙∙ .∙^ √
‘/ / 0b 0
46 * * * & -{'∙ .?
Input dmeiSons



-04	I
-∙w
Output dimenSons
ClUSter 9
0.8
02
02
04
06 ■
04 1
LAjlJ	ILTJIIL⅛J	
	Ill1	1U1	J	”…,中		廿二
0
-02
-04
S F B 5 ..,≠	y ≠ ,s' ,«
4 -,,ZZZZ Z zZ ✓ ✓
nput dmeiSons
/.Λ




-,'w
Output dimenSons
CIUSter 12
08
06
04
02
-02
-04
0
-02
-04
* M M
# # ,«* ,⅛*
ZZZZZ
Input dme∣S0ns
a* * ,f' 0 .*
‘ a，a"，/ .
CIUSter 15



Output dimenSons
0.8

06 ■
04 ]
02
02
04
Z S ,W 4 .令.0
z √i '≠XZ√
Input dimenSons
0 .g∙	，0 ʌ
S 6 潦' , /


0
-02
-04
-,'w
OUput dmeiSons
CIUSter 18
08
06
04 ■
02 '
02
04
配M,捅,	M “述 Wj帆u∙	杯IhMM加 j
n ɪɪ		
0
-02
-04
/ !■
√, <, <#
1, ..»■
zzzzz
Input dmeiSons
总篇速,e* ʌ
. / / / M



Output dimenSons
Figure 6: Representative input-output mappings of extracted clusters.
8
Under review as a conference paper at ICLR 2019
References
G. Alain and Y. Bengio. Understanding intermediate layers using linear classifier probes. In ICLR
2017 Workshop, 2017.
M. Ancona, E. Ceolini, A. C. Oztireli, and M. Gross. Towards better understanding of gradient-
based attribution methods for deep neural networks. In International Conference on Learning
Representations, 2018.
S. Bach, A. Binder, G. Montavon, F. Klauschen, K-R. Muller, and W. Samek. On pixel-wise expla-
nations for non-linear classifier decisions by layer-wise relevance propagation. PLOS ONE, 10:
1-46, 2015.
D. Bau, B. Zhou, A. Khosla, A. Oliva, and A. Torralba. Network dissection: Quantifying inter-
pretability of deep visual representations. In Computer Vision and Pattern Recognition, 2017.
M. Craven and J. W. Shavlik. Extracting tree-structured representations of trained networks. In
Advances in Neural Information Processing Systems 8, pp. 24-30, 1996.
F. Doshi-Velez and B. Kim. Towards a rigorous science of interpretable machine learning.
arXiv:1702.08608, 2017.
e Stat. Consumer price index of food nationwide from January 1970 to January 2018. https:
//www.e-stat.go.jp/dbview?sid=0003143513, 2018.
M. Ishikawa. A structural connectionist learning algorithm with forgetting. Journal of Japanese
Society for Artificial Intelligence, 5:595-603, 1990.
U. Johansson and L. Niklasson. Evolving decision trees using oracle guides. In 2009 IEEE Sympo-
sium on Computational Intelligence and Data Mining, pp. 238-244, 2009.
R.	Krishnan, G. Sivakumar, and P. Bhattacharya. Extracting decision trees from trained neural
networks. Pattern Recognition, 32(12):1999-2009, 1999.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. In Proceedings of the IEEE, volume 86, pp. 2278-2324, 1998.
Z. C. Lipton. The mythos of model interpretability. In Proceedings of the 2016 ICML Workshop on
Human Interpretability in Machine Learning, 2016.
S.	M. Lundberg and S. Lee. A unified approach to interpreting model predictions. In Advances in
Neural Information Processing Systems 30, pp. 4765-4774, 2017.
W. Luo, Y. Li, R. Urtasun, and R. Zemel. Understanding the effective receptive field in deep convo-
lutional neural networks. In Advances in Neural Information Processing Systems 29, pp. 4898-
4906, 2016.
T.	Nagamine and N. Mesgarani. Understanding the representation and computation of multilayer
perceptrons: A case study in speech recognition. In Proceedings of the 34th International Con-
ference on Machine Learning, pp. 2564-2573, 2017.
M. Raghu, J. Gilmer, J. Yosinski, and J. Sohl-Dickstein. SVCCA: Singular vector canonical correla-
tion analysis for deep learning dynamics and interpretability. In Advances in Neural Information
Processing Systems 30, pp. 6076-6085, 2017.
M. T. Ribeiro, S. Singh, and C. Guestrin. “Why should I trust you?”: Explaining the predictions of
any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, pp. 1135-1144, 2016.
D. Rumelhart, G. Hinton, and R. Williams. Learning representations by back-propagating errors.
Nature, 323:533-536, 1986.
A. Shrikumar, P. Greenside, and A. Kundaje. Learning important features through propagating
activation differences. In Proceedings of the 34th International Conference on Machine Learning,
pp. 3145-3153, 2017.
9
Under review as a conference paper at ICLR 2019
K. Simonyan, A. Vedaldi, and A. Zisserman. Deep inside convolutional networks: Visualising image
classification models and saliency maps. In ICLR 2014 Workshop, 2014.
D. Smilkov, N. ThoraL B. Kim, F. Viegas, and M. Wattenberg. Smoothgrad: removing noise by
adding noise. arXiv:1706.03825, 2017.
J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller. Striving for simplicity: The all
convolutional net. In ICLR 2015 Workshop, 2015.
M. Sundararajan, A. Taly, and Q. Yan. Axiomatic attribution for deep networks. In Proceedings of
the 34th International Conference on Machine Learning,pp. 3319-3328, 2017.
J. J. Thiagarajan, B. Kailkhura, P. Sattigeri, and K. N. Ramamurthy. Treeview: Peeking into deep
neural networks via feature-space partitioning. In NIPS 2016 Workshop on Interpretable Machine
Learning in Complex Systems, 2016.
R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
Society, Series B, 58(1):267-288, 1996.
J. H. Ward. Hierarchical grouping to optimize an objective function. Journal of the American
Statistical Association, 58(301):236-244, 1963.
C. Watanabe, K. Hiramatsu, and K. Kashino. Recursive extraction of modular structure from layered
neural networks using variational Bayes method. In Proceedings of Discovery Science 2017,
Lecture Notes in Computer Science, volume 10558, pp. 207-222, 2017a.
C. Watanabe, K. Hiramatsu, and K. Kashino. Modular representation of autoencoder networks.
In Proceedings of 2017 IEEE Symposium on Deep Learning, 2017 IEEE Symposium Series on
Computational Intelligence, 2017b.
C. Watanabe, K. Hiramatsu, and K. Kashino. Modular representation of layered neural networks.
Neural Networks, 97:62-73, 2018a.
C. Watanabe, K. Hiramatsu, and K. Kashino. Understanding community structure in layered neural
networks. arXiv:1804.04778, 2018b.
C. Watanabe, K. Hiramatsu, and K. Kashino. Knowledge discovery from layered neural networks
based on non-negative task decomposition. arXiv:1805.07137v2, 2018c.
P. Werbos. Beyond regression : new tools for prediction and analysis in the behavioral sciences.
PhD thesis, Harvard University, 1974.
T. Zahavy, N. Ben-Zrihem, and S. Mannor. Graying the black box: Understanding DQNs. In
Proceedings of the 33rd International Conference on Machine Learning, pp. 1899-1908, 2016.
10
Under review as a conference paper at ICLR 2019
Appendix 1:	Comparison with Clustering Method Based on
Non-negative Matrix Factorization
Here, we show the effectiveness of our proposed method by comparing it with the clustering
method based on non-negative matrix factorization (or NNMF), which was proposed in a previ-
ous study (Watanabe et al., 2018c). We applied this NNMF-based clustering method to the same
data sets that we used in the experiments described in section 4. In the previous study (Watanabe
et al., 2018c), the feature vectors of the hidden layer units are defined by the magnitude of the ef-
fect of each input dimension value on a cluster and the effect of a cluster on each output dimension
value, computed by the square root error of the unit output values. By definition, the elements of
such feature vectors are all non-negative, which is a necessary condition for applying NNMF to the
feature vectors.
We applied the NNMF-based clustering method to the trained network with exactly the same pa-
rameter as the network shown in Figures 3 and 5. With the MNIST data set (LeCun et al., 1998)
and the data set of a consumer price index (e Stat, 2018), respectively, we decomposed the trained
networks into 16 and 12 clusters. With both data sets, we set the number of iterations of the NNMF
algorithm at 1000. We applied the NNMF algorithm for 10000 times, and used the best result in
terms of the approximation error. Initial values of the two low-dimensional matrices were randomly
chosen according to the normal distribution N (0.5, 0.5).
Figures 7, 8, 9, and 10 show the resulting cluster structures and the representative roles of the
clusters. Comparing these figures with the results in Figures 3, 4, 5, and 6, we can observe that the
previous NNMF-based method could not capture the structures of the input and output dimension
values in as much detail as our proposed method, since it does not take the sign information into
account. Furthermore, with the NNMF-based method, we should define the number of clusters in
advance, and we cannot observe the hierarchical structure of clusters to find the optimal resolution
for interpreting the roles of partial structures of an LNN.
Appendix 2:	Effect of Sign Alignment of Feature Vectors
Here, we discuss the effect of the sign alignment of the feature vectors based on cosine similarity
(Algorithm 1).
Figures 11 shows the effect of the sign alignment of the feature vectors extracted from an LNN
trained with the MNIST data set (LeCun et al., 1998). The left and center figures, respectively, show
the feature vectors before and after the alignment of the signs. The right figure shows the monotonic
increase of the sum of the cosine similarities through the alignment algorithm. Figure 12 shows the
dendrograms of the hierarchical clustering results with the original feature vectors of Definition 3
and with the feature vectors after the alignment of the signs. From this figure, we can observe that
the height of the dendrogram, which shows the similarity of all the hidden layer units, is higher with
the original feature vectors than with the feature vectors after the sign alignment. In other words, it
was shown that the algorithm successfully aligned the feature vectors so that they became similar to
each other. Figures 13 and 14 show the effect of the sign alignment of the feature vectors extracted
from an LNN trained with the food consumer price index data set (e Stat, 2018). These figures show
similar results to those of the MNIST data set.
Appendix 3:	Experimental Settings
Here, we detail the experimental settings. E1 and E2, respectively, represent the settings of the
experiments described in sections 4.1 and 4.2.
-	The training sample size n1 was: 500 per class (E1), and 270 (E2).
-	We normalized the input data so that the minimum and maximum values of an element, respec-
tively, were -1 and 1. Similarly, we normalized the output data so that the minimum and maximum
values of an element, respectively, were 0.01 and 0.99.
-	The mean iteration number for LNN training per dataset a1 was: 100 per class (E1), and 500 (E2).
11
Under review as a conference paper at ICLR 2019
-	We generated the initial connection weights and biases of a layered neural network as follows:
ωdj i.i.d. N(0, 0.5), θd i.i.d. N(0, 0.5).
-	The hyperparameter of the L1 regularization λ was: 1.1 × 10-5 (E1), and 2 × 10-5 (E2).
-	As regards the LNN training with the MNIST data set, we chose training data with the following
deterministic procedure to stabilize the training. Let Zn(k) ≡ {Xn(k), Yn(k)} be the n-th training data
sample in class k . The training data were chosen in the following order:
(1)	(10)
ZI , ∙∙∙ , z1
(1)	(10)
ZI , ∙∙∙ , z1
Z21),…,Z210),…,Zn?,…,Zn;0),
-	The iteration number for the alignment of the signs of the feature vectors a0 was: 5000 (E1 and
E2).
-	The weight removing hyperparameter ξ was: 0.6 (E1), and 0.001 (E2) In Figures 3 and 5, we only
draw connections where the absolute values of weights were ξ or more.
12
Under review as a conference paper at ICLR 2019
0	1	23456789
Figure 7: Cluster structure of an LNN acquired by non-negative matrix factorization (MNIST data
set).
Cluster 1
Cluster 2
Cluster 3
Input dimensions
0	5	10
10
5
0 1 23456789
Output dimensions
Cluster 5
Input dimensions
0	5
10
5
0 1 23456789
Output dimensions
Cluster 9
Input dimensions
0	5
10
5
0 1 23456789
Output dimensions
Input dimensions
0	5
Output dimensions
Output dimensions
Cluster 6
Output dimensions
0 1 23456789
Output dimensions
Cluster 8
Input dimensions
0	5	10
10
5
0 1 23456789
Output dimensions
10
5
0123456789
Output dimensions
Cluster 11
Output dimensions
Cluster 12
Output dimensions
Cluster 13
Cluster 14
Cluster 15
Cluster 16
Input dimensions
0	5
10
5
0 1 23456789
Output dimensions
Output dimensions
Input dimensions
0	0.5
0 1 23456789
Output dimensions
Output dimensions
Figure 8: Representative input-output mappings of extracted clusters.
13
Under review as a conference paper at ICLR 2019
S--Oug
=OUg
-.--OUg
=0Ug
--0Ug
-.--OUg
=OIlg
=OlIg
=OIlg
二Oug
百.二OUg
-0Ug
一.二。一g
一.二OUg
一.二OUg
一&.二OUg
一改.二OUg
H.二OIlg
一改.二01Ig
一.--oug
耳-OUg
-oug
一.--Oug
一.二OUg
一3.二OUg
一.--Oug
-.--OUg
"eπ⅛
"fπ⅛
"p"¾
"fπ⅛
"fπ⅛
"p"¾
."fπ⅛
"p"¾
"p"¾
S工I"⅛l
-I"⅛l
-肾8
-肾8
一工肾@
-肾8
-肾8
-肾8
-肾8
-I"⅛l
一a。-fπ⅛
I"⅛l
一.~-p"⅛
一.~-p"⅛
(W.--p"⅛
一也f"⅛
一-fπ⅛
豆f"⅛
a-p"⅛
@f"⅛
(8。-p"⅛
I"⅛l
一.--p"⅛
一.~-p"⅛
一.~-p"⅛
-.-肾@
一肾@
S二品一
-吊一
一吊一
-吊一
-m一
一m一
一m一
-吊一
一m一
一m一
S吊一
目一
目一
吊一
一吊一
吊一
&m一
E一m一
目一
国m一
S2m一
目一
Sm一
?吊一
吊一
Sm一
目一
Figure 9:	Cluster structure of an LNN acquired by non-negative matrix factorization (food con-
sumer price index data set).
Cluster 1
Cluster 2
15
10
τL, ,∣J , lfl,rl
P
Input dmenscn≡
l,, Fll i.i r
15
10
5
Output dmenSons
15
10
5
0
Input dmensons
LU
“J
15
10
5
OUtpJt dimensions
,s .件,、》或 N ..
X Z Z CZ Z Z，
0
、》速、4e...
-> .⅛- -⅛-科 ʌ4
/ 6於 $
0
Cluster 3
Cluster 4
15
10
Input dmensons
15
10
5
OUtpJt dmensonι≡
15
10
,LiLhLiLlLt.,
a、承、》√⅛
ʃ ʃ ʃ ʃ ʃ
Input dmensons
.♦L J‘ J".一』,一im.Lj
?"岁ea
Z Z z Cz Z。/
15
10
5
Output dimensions
LhI
S、承、分e,q 、
X Z /，Z Z
r
5
0
0


0
Cluster 5
Cluster 6
15
10
0
15
10
5
Output dmensons
15
10
5
ZZZZ Z
Input dmensons
15
10
5
OUPUt dimensions
ZZZZ Z
Input dmenscn≡
z.
? / Z
t HLtU,t 1

/ Z Z 5«
0
0
於於於

Cluster 7
Cluster 8
15
10
15
10
0
0
Input dmensons
Input dmensons
Cluster 9
Cluster 11
15
10
Input dmensons
,1τ 1 ,t 1, . ,L 1
0
15
10
5
OUtpJt dmensons
15
10
5
OUtpJt dmensonι≡
15
10
5
OUtpJt dmenscnι≡
15
10
5
0
邕S
15
J..,,,
' Z承
Input dmensons
Cluster 10
j τ J , ,tL JL " L .Jt"1
15
10
5
OUtpJt dimensions
10
5
Ju
0
15
10
5
0
Input dmensons
0
Cluster 12
Input dmensons
15
10
5
OUtpJt dimensions
15
10
5
0
OUtpJt dimensions
»	Jl∙您. ɔ
Z Z 拱，Z，
0

Figure 10:	Representative input-output mappings of extracted clusters.
14
Under review as a conference paper at ICLR 2019
35 -
Original feature vectors
-0.5	0	0.5
Feature vectors after aligning signs
-0.5	0	0.5
Iterations
Figure 11: Left: Feature vectors of Definition 3. Each row corresponds to a feature vector for a
hidden layer unit. Center: Feature vectors after the alignment of the signs. Right: Sum of the
cosine similarities of all the pairs of feature vectors (MNIST data set).
Dendrogram of NOT aligned feature vectors
30 -
25 -
20 -
Dendrogram of aligned feature vectors
35---------------------------------------------------------------------------------------------------------------
Figure 12: Dendrograms of the hierarchical clustering results with the original feature vectors of
Definition 3 (top) and with the feature vectors after the alignment of the signs (bottom).
15
Under review as a conference paper at ICLR 2019
Original feature vectors
-0.5	0	0.5
Feature vectors after aligning signs	16000
-0.6 -0.4 -0.2	0	0.2	0.4	0.6	0.8
-2000
14000
12000
110000
S
80 8000
2000
0
O O
O O
O O
6 4
u00EnS
1000	2000	3000	4000	5000
Iterations
Figure 13: Left: Feature vectors of Definition 3. Each row corresponds to a feature vector for a
hidden layer unit. Center: Feature vectors after the alignment of the signs. Right: Sum of the
cosine similarities of all the pairs of feature vectors (food consumer price index data set).
Figure 14: Dendrograms of the hierarchical clustering results with the original feature vectors of
Definition 3 (top) and with the feature vectors after the alignment of the signs (bottom).
16