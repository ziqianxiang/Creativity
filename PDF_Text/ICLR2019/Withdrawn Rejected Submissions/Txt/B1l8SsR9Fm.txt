Under review as a conference paper at ICLR 2019
Learning and Data Selection in Big Datasets
Anonymous authors
Paper under double-blind review
Ab stract
Finding a dataset of minimal cardinality to characterize the optimal parameters of a
model is of paramount importance in machine learning and distributed optimization
over a network. This paper investigates the compressibility of large datasets. More
specifically, we propose a framework that jointly learns the input-output mapping
as well as the most representative samples of the dataset (sufficient dataset). Our
analytical results show that the cardinality of the sufficient dataset increases sub-
linearly with respect to the original dataset size. Numerical evaluations of real
datasets reveal a large compressibility, up to 95%, without a noticeable drop in the
learnability performance, measured by the generalization error.
1	Introduction
During the last decade, new artificial intelligence methods have offered outstanding prediction
performance on complex tasks including face and speech recognition (Zhao et al., 2003; Schalkwyk
et al., 2010; Hinton et al., 2012), autonomous driving (Michels et al., 2005), and medicine (Kourou
et al., 2015). To achieve such amazing results, state-of-the-art machine learning methods often need
to be trained on increasingly large datasets. For example, (MNIST) is a typical dataset for natural
image processing of handwritten digits with more than 70,000 samples, and (MovieLens) 20M is
a typical dataset for recommendation systems that includes more than 20,000,000 ratings. As we
show throughout this paper, most of the samples in these datasets are redundant, carrying almost no
additional information for the learning task. A fundamental open question in learning theory is how
to characterize and algorithmically identify a small set of critical samples, hereafter called a small
representative dataset, that best describes an unknown model. Studying its behavior around those
critical samples helps us to better understand the unknown model as well as the inefficiencies of the
sample acquisition process in the original dataset. For instance, in a multi-agent system, it may be
enough to share these small representative datasets among the agents instead of the original big ones,
leading to a significant reduction in power consumption and networking latency (Jiang et al., 2018).
Experiment design (Sacks et al., 1989) or active learning (Settles, 2012) provides algorithmic
approaches to obtain a minimal set of samples to be labeled by an “oracle” (e.g., a human annotator).
Active learning is well-motivated in many modern machine learning applications where obtaining a
new labeled training sample is expensive. The main components of active learning are a parameterized
model, a measure of the model’s uncertainty, and an acquisition function that decides based on the
model’s uncertainty the next sample to be labeled. This approach has several challenges including lack
of scalability to high-dimensional data (Settles, 2012) (which has been partially addressed in some
recent publications (Gal et al., 2017)), lack of theoretical guarantees, and lack of formal uncertainty
measure. More importantly, the acquisition function is usually greedy in the sense that it sequentially
finds the new samples to be labeled one-by-one. Consequently, the resulting sub-sampled dataset
may not necessarily be a small representative dataset due to the greedy nature of active learning.
In this paper, we investigate a different approach than experiment design or active learning. Instead of
reducing the total labeling cost (like active learning), we focus on the following scenario: how to find
the a small representative set of samples with cardinality K in a big dataset of labeled samples with
cardinality N( K)? Using optimization theory, we establish a scalable algorithm with provable
theoretical guarantees to jointly find K most representative samples. We also show fundamental
relations between K and N to guarantee any arbitrary learning performance. Our framework and
algorithmic solution approaches can be very useful tools to better understand compressibility of
the existing datasets and to improve distributed learning over a multi-agent systems and Internet-of-
1
Under review as a conference paper at ICLR 2019
Things (see Appendix B). Moreover, further analysis of the representative dataset with respect to the
original big dataset of an unknown model helps understand the sources of inefficiency in the current
sampling process and how to improve it for other similar models.
The main research question of this work, and in particular our order analysis, is closely related to
the sample complexity (Clarkson et al., 2012), information-theoretic concepts of sampling (Jerri,
1977) like the Shannon-Nyquist sampling, compression (Cover & Thomas, 2012), and compressive
sensing (Donoho, 2006) when the function is sparse in some predetermined basis. All these methods
address the following question: how many samples are required to reconstruct a function with a
predefined error? We show that the size of such a compressed dataset grows sub-linearly with respect
to the cardinality of the original dataset.
In this study, we investigate compressibility of large datasets and develop a general framework
for function approximation in which choosing the samples that best describe the function is done
jointly with learning the function itself. We formulate a corresponding mixed integer non-linear
program, and propose an iterative algorithm that alternates between a data selection step and a
function approximation step. We show the convergence of the proposed algorithm to a stationary
point of the problem, despite the combinatorial nature of the learning task. We then demonstrate
that our algorithm outputs a small dataset of carefully chosen samples that solves a learning task, as
accurately as if it were solved using original large dataset. Comprehensive numerical analyses on
synthetic and real datasets reveal that our algorithm can significantly compress the datasets, by as
much as 95%, with almost no noticeable penalty in the learning performance.
The rest of the paper is organized as follows. Section 2 presents the problem setting and our
algorithmic solution approach. Section 3 provides main theoretical results. We apply our algorithms
on synthetic and real datasets in Section 4, and then conclude the paper in Section 5. Due to lack of
space, we have moved all the proofs and some applications to the appendix.
Notation: Normal font a or A, bold font a, and calligraphic font A denote scalar, vector, and set,
respectively. |A| is the cardinality of set A. I is indicator function. aT denotes the transpose of a,
and kak0 and kak2 are its l0 and l2 norms. 1 is a vector of all ones of proper size. For any integer N,
[N] denotes set {1, 2, . . . , N}.
2	Setting and Solution Approach
2.1	Problem Setting
Consider input space X , output space Y, an unknown function f : X 7→ Y from some function
space F, an index set [N] := {1, . . . , N} for N ∈ N+, and a dataset of training samples D =
{(xi, f(xi))}i∈[N], where xi ∈ X. In a classical regression problem, we use the dataset D to learn
f , namely find a function h : X 7→ Y that has a minimal distance (for some distance measure, also
called loss) to the true function f. Formally, for a given loss function ` : X × Y × Y 7→ [0, ∞], the
regression task solves the following empirical risk minimization problem:
(P 1): h? ∈ argmin ɪ X	' (xi, f(xi), h(xi)) .	(1)
h∈F N	i∈[N]
Here, we assume that h ∈ F. However, considering a different function class for h would not change
the generality of our results.
In many applications (see Appendix B) one might not want to work with the entire dataset D, e.g.,
due to its large size, but rather with a small subset E ⊆ D, where possibly |E |	|D|. We associate
a binary variable zi with each training sample (xi, f(xi)) such that zi = I{(xi, f(xi)) ∈ E},
representing sample (xi, f (Xi)) being selected or dropped. Letting Z = [zι, ∙ ∙ ∙ , ZN]t, the novel
problem of jointly learning h and selecting E can be formulated as
(P2):	(h?, z?) ∈ arg min g(h, z) ：= TT- X, "i' (xi,f (xi),h(xi))	(2a)
h∈F,z	1Tz	i∈[N]
s.t. gι(h) := Nn XiHN] ' (xi,f (Xi), h(xi)) ≤ e ,	(2b)
g2(z) := 1Tz ≥ K , z ∈ {0, 1}N ,	(2c)
2
Under review as a conference paper at ICLR 2019
where constraint (2b) prevents overfitting when “generalizing” from E to D, and constraint (2c)
prevents degenerate/trivial solutions to the problem (e.g., where E empty). We show later that K
is a very important parameter that trades off the compression rate, defined as 1 - |E |/|D|, and the
generalization error of learning with E .
Let 'i(h) := '(xi, f (xi), h(xi)) denote the loss corresponding to sample (xi, f (Xi)). Followings
are some assumptions used throughout the paper, which are prevalent in the learning literature.
Assumption 1. `i(h) is continuous and convex in h.
Assumption 2. The original dataset D is duplicate-free, namely xm 6= xn for all m, n ∈ [N] and
xm, xn ∈ X. Moreover, for all h ∈ F, xm 6= xn implies `m (h) 6= `n (h).
Note that `i(h) does not have to be smooth and differentiable in general. We stress the existence of a
large family of loss functions, including Lp spaces, for which both assumptions hold, as exemplified in
Appendix C. Moreover, the convexity in Assumption 1 may also be relaxed at the expense of a weaker
convergence property using the block successive upper-bound minimization framework (Razaviyayn
et al., 2013). Finally, if the dataset contains some duplicated samples, we can add insignificant
perturbations to satisfy Assumption 2, without affecting the information content (Bertsekas, 1998).
2.2 Solution Approach
(P2) is a non-convex combinatorial optimization problem with coupling cost and constraint functions.
In the following, we provide a solution approach based on block-coordinate descent (BCD), which
splits (P2) into two subproblems: (P 2a) for data selection and (P 2b) for function approximation.
Let h(k) and z(k) be the value of h and z at iteration k. BCD yields the following update rules:
(P 2a) : z(k+1) ∈ arg minz∈{0,1}N g(h(k), z), s.t. g2(z) ≥ K ,	(3)
(P 2b) :	h(k+1) ∈ arg minh∈F g(h, z(k+1)), s.t. g1(h) ≤ .	(4)
The data selection (D-)step (P 2a) is optimized for a given hypothesis h. Then, in the function
approximation (F-)step (P 2b), the hypothesis is optimized for the updated compressed dataset z(k+1).
Next, we derive solutions to each step.
2.2.1	D-STEP
Given h(k) , namely the value of h at iteration k, we let Cik) = '(xi, f (xi), h(k)(xi)) and c(k)=
[c(1k), c(2k), . . . , c(Nk)], for notation simplicity. Then, the first subproblem is written as
arg minz∈{0,1}N zT c(k) /1T z , s.t. zT1 = K,	(5)
where we have used the fact that zT1 ≥ K holds with equality; shown in Proposition 1. Thus,
1Tz = K can be removed from the denominator to equivalently1 write (5) as
(P 2a) : z(k+1) ∈ arg minz∈{0,1}N zT c(k) , s.t. zT1 = K .	(6)
Though combinatorial, the form in (P 2a) allows to easily derive its solution, as shown by Proposi-
tion 1, i.e., zi(k+1) = I{i ∈ S(k)}, where S(k) is the set of K-indices in c(k) with the smallest values.
In other words, the optimal solution is obtained by selecting the smallest K elements of c(k), and
setting the corresponding indices of z to 1.
2.2.2	F-STEP
Given the updated data selection, z(k+1), we use the fact that 1Tz(k+1) = K and rewrite (P2b) as
(P2b) : arg min g(h) := ^X	z(k+1)'i(h)	(7a)
h∈F	i=1 i
s.t. g1(h) := XN `i(h) ≤ .	(7b)
i=1
1Two problems are equivalent, if the optimal solution to one can be obtained from the other, and vice-
versa (Boyd & Vandenberghe, 2004).
3
Under review as a conference paper at ICLR 2019
Algorithm 1 Alternating Data Selection and Function Approximation (DF)
// Initialize z(1) = 1
for k = 1, 2, 3, . . . do
// D-step
Compute ci(k) , ∀i ∈ [N]
Update z(k+1) by solving (P2a) in (5), using Proposition 1
// F-step
Update h(k+1) by solving (P 2b) in (7)
Stop if |g(h(k+1), z(k+1)) - g(h(k), z(k))| < γ
It follows from zi(k+1) ∈ {0, 1} that both the cost and constraints in (P 2b) consist of convex
combinations of the loss, `i, assumed convex (Assumption 1): Thus, (P 2b) is convex in h (Boyd &
Vandenberghe, 2004, Chap. 3). In the following section, we show that there exist loss functions that
lead to closed-form solutions.
Algorithm 1 summarizes our alternating data selection and function approximation steps. We establish
the algorithm’s convergence to a stationary point of (P2) in Proposition 2.
2.3 Special Case 1: Linear Regression and Data Selection
We specialize our approach to a linear regression problem with data selection where h(xi) = xiT w
for w, xi ∈ Rd, d being the dimension of each sample. Optimization problem (P2) reduces to
arwmin Xi∈[N]l⅛ (XTW -〃*)2
s.t. N Xi∈[N] (xTW - fWA2 ≤ e,
1T z ≥ K , z ∈ {0, 1}N .
The D-step is identical to (P 2a), which can be solved by Proposition 1. Given z(k+1), the F-step
reduces to the following quadratically-constrained quadratic programming:
W(k+1) = arg min A(k+1) XTW - f(X)2 ,	s.t. XTW - f(X)2 ≤ ,
where A(k+1) ：= diag(,z(k+1),…,Jzjk+。} X := [xι,…，XN], and f (X):=
[f(x1), . . . , f (xN)]T . The problem is convex and can be solved using standard Lagrangian tech-
niques to yield
W(k+1) = X	A(k+1)2 + λIN XT -1 X A(k+1) + λIN f(X),
where λ ≥ 0 is a Lagrange multiplier that satisfies the complementary slackness condition and can
be found using 1D search methods.
Note that computational complexity of W(k+1) is dominated by the matrix inversion, which is in the
order of O(d3) and does not scale with the size of the training set, N. However, ifit is still considered
significant in some applications, W(k+1) can be obtained using stochastic gradient-based methods
whose computational complexity is O(d∕α) for accuracy threshold α > 0.
2.4 Special Case 2: Robust Learning
Consider the following continuous relaxation of (P2), where z ∈ {0, 1}N is relaxed to z ∈ [0 , 1]N,
(P3):	ahgmZn 1⅛ Xi∈[N]zi 'i㈤
s.t. N Xi∈[N ] 'i(h) ≤ e, IT Z ≥ K, z ∈ [0,1]N .
4
Under review as a conference paper at ICLR 2019
Thus, zi can be seen as a non-negative weight assigned to sample (xi, f(xi)) in that training set,
representing level of confidence in its quality (higher values of zi imply better qualities). From this
perspective, the resulting problem becomes a robust learning problem, in presence of non-uniform
sample quality: learning h, jointly with the best samples of the training set. Some applications include
de-noising sensor measurements and outlier detection.
We can use Algorithm 1 to address (P 3), with a minor modification in the D-step. Note that
relaxing the binary constraint implies that the D-step cannot be solved using the simple method of
Proposition 1. However, we use the linear program relaxation of (P2a),
arg minz∈[0,1]N zT c(k) , s.t. 1Tz = K ,	(8)
which can be efficiently solved using standard linear program solvers. In Appendix A.6, we have
shown that optimization problem (8) is equivalent to (P2a), and therefore the linear program
relaxation is optimal.
3 Main Theoretical Results
In this section, we present main results of this paper. Detailed proofs are available in Appendix.
Proposition 1 (Solution of (P 2a)). Under Assumption 2, we define an index sequence j such that
for any jm,jn ∈ [N], cj(k) < c(jk) iff m < n, where ci(k) is defined in Section 2.2.1. The solution of
the data selection subproblem is
z(k+1) =	1, if
i	0, if
i = j1,j2, . . . ,jK
i = jK+1 , . . . , jN ,
(9)
and kz(k+1) k0 = K.
Proposition 1 implies that the optimal solution to the binary data selection subproblem is simple:
evaluate the loss function for all training samples of D using h(k), sort the values, and keep K data
samples having the smallest losses. Next, we establish the convergence of our BCD-based algorithm
to a stationary point of (P 2).
Proposition 2 (Convergence). Let {g(h(k), z(k))}k≥1 denote the sequence generated by the BCD
updates in Algorithm 1. Then, this sequence monotonically decreases with each update, i.e.,
g(h(k), z(k)) ≥g(h(k),z(k+1)) ≥ g(h(k+1), z(k+1)), k = 1,2,3,...
and converges to a stationary point of (P 2).
Proposition 3 (Computational Complexity). The D-step is run in O(N), and the F-step has the
same complexity as of (P 1), per iteration of Algorithm 1.
To analyze the asymptotic behavior of our approach, we make additional assumptions on the class of
loss functions, and on F . In particular, we assume that ` belongs to the Lp space and F is the space
of L-Lipschitz functions defined on some compact support.
Proposition 4 (Sample Complexity). Assume that the samples are noiseless, and F is the set
of L-Lipschitz functions defined on interval [0, T]d. Consider optimization problem (P 2). Let
g (h*, z?) := (ITz?) Ei∈[n] z?' (xi,f(xi),h?(Xiy). For any arbitrary constant δ > 0, the
following holds: When '(Xm,f (xm),h*(xm)) := |f (xm) - h? (Xm )[2, g (h?, z?) ≤ δ if K ≥
d(1 + 2LT/d∕δ)de, where」•] is the ceiling function.
Corollary 1 (Asymptotic Sample Complexity). Consider the assumptions of Proposition 4. Define
compression ratio as CR := 1 - K∕N. As N grows large:
∀δ > 0, ∃K ≤ N s.t. g (h?, z?) < δ , and CR → 1 .
Proposition 4 and Corollary 1 imply that the sufficient dataset E? (which can be used to learn any
function f in class F with any arbitrary accuracy) is the output of a sub-linear sub-sampler (our
Algorithm 1) of the original big dataset, namely K∕N → 0 asymptotically. In other words, as we
experimentally show in the next section, most of the existing big datasets are highly redundant, and
5
Under review as a conference paper at ICLR 2019
the redundancy brings almost no additional gain for the accuracy of the learning task. Finding this
sufficient dataset is of manageable complexity, as specified in Proposition 3.
4	Experimental Results
In this section, we first present two toy examples to illustrate (P2) and algorithmic solution. We
then focus on real databases and evaluate the effectiveness of our approach on finding the small
representative dataset with a negligible loss in the generalization capability.
4.1	Experimental Setting
In every experiment, we select the input space X, output space Y, mapping f, training dataset D, test
dataset T, and hypothesis class H. We then run Algorithm 1 to find the optimal compressed dataset
E? ⊆ D. We run this experiment for different values of CR.
To evaluate the true generalization capability of E?, we run a conventional regression problem (P 1)
using both D and E? , find the corresponding optimal approximation function, and then evaluate their
accuracy on T. We denote the normalized generalization error by e(D, T),
∑i∈T ' (Xi,f (Xi),h?(Xi))
Pi∈T ' (Xi,f (Xi), 0)
(10)
where h? is found by running (P1) with D. When we find h? by running (P1) with E?, (10) shows
e(E?, T). When ` is the L2-norm, (10) reduces to the normalized square error, Pi∈T |f(Xi) -
h?(Xi)|2/ Pi∈T |f(Xi)|2. This normalization is to have a fair comparison of the generalization error
over various test datasets, which may have different norms. We say that our compressed dataset E ?
performs as well as D when e(E?, T) is close to e(D, T). Throughout the following experimental
studies, we observe that the lower the compression ratio, the lower the gap |e(D, T) - e(E?, T)|.
However, for big datasets, we can substantially compress the dataset without any noticeable drop in
the gap, indicating a high inefficiency in the data generation process.
4.2	Illustrative Examples
In our first example, we pick a very smooth function f. Let X = [0, 8], F = H = Poly(10), where
Poly(n) is polynomial functions of degree n. f(x) is given in figure 1(a). Our original dataset
D is 100 equidistant samples in X . We add i.i.d. Gaussian noise of standard deviation 0.05 to
f (x). Figure 1(a) shows an example of an optimal compressed dataset E? computed for K = 12,
along with the learned hypothesis h?, which almost perfectly coincide with the true function f.
Note that due to the random noise in D, the selected samples would be different in every run. To
evaluate the true generalization capability of our algorithm, we run a Monte Carlo simulation for 100
random realizations of the noise on dataset, find E? and h? for each realization, and compute e(D, T)
and e(E?, T) from (10). Note that T is the set of 1000 equidistant examples in X. Figure 1(b)
reports the average generalization error against the CR. As expected, the higher the compression
ratio the higher the generalization error. The tail drop of this function is at least as a fast as a double
exponential function of CR, implying that for a wide range of CR values, the extra generalization
error |e(E?, T) - e(D, T)| is negligible. In particular, in the example of Figure 1(b) with N = 100,
70% compression leads to only 6 × 10-5 extra generalization error.
Figure 2 illustrates the performance of our optimization problem on a less smooth function than that
of figure 1(a). In particular, we consider the function of figure 2(a) which is from F = Poly(15),
X = [0, 1], N = 100 equidistant samples from X in D, and 1000 equidistant samples from X in T.
We observe a large compression of the dataset in figure 2(b), where only |e(E?, T) - e(D, T)| =
2.3 × 10-5 extra true generalization error after 60% compression. Moreover, we can see the double
exponential tail of e(E?, T), which implies that only a small dataset of a few carefully chosen samples
are enough to have a learning with a sufficiently good generalization capability. However, for a fixed
error gap |e(E?, T) - e(D, T)|, functions having more variation are less compressible, since more
samples are needed to maintain the same error gap.
6
Under review as a conference paper at ICLR 2019
1.5
0.5
-0.5
Compression ratio
(b) Generalization error
,ɪoɪɪə Uolieziiejəuə0
1.50-----------2-----------4-----------6
8
x
(a) Function f and an example of E? with K = 12
Figure 1: Learning compressed data set E? and optimal hypothesis h? with a dataset of size N = 100. Function
f ∈ Poly(10) + noise. Selected samples in E?(c D) are denoted by circles. In the example of (a), f and h?
are visually indistinguishable. The higher the compression ratios the higher the generalization error. e(E? , T)
behaves like a double exponential function of CR. For a wide range of CR values there is almost no loss
compared to e(D, T).
0-----------------------------------------------------
0	0.2	0.4	0.6	0.8	1
x
(a) Function f and an example of E? with K = 25.
0
0.2
0.4
0.6	0.75
Compression ratio
10-6
(b) Generalization error
jojjə Uo=EZWəuə0
Figure 2: Learning compressed data set E? and optimal hypothesis h? with a dataset of size N = 100. Function
f ∈ Poly(15) + noise. The higher the compression ratios the higher the generalization error. The double
exponential behavior of CR, observed also in figure 1(b), is visible in (b).
4.3	Real Datasets
Motivated by the excellent performance of the proposed algorithm on simple syntectic data, in this
section, we apply Algorithm 1 on real databases listed in Table 1, available on Statlib (Sta) and UCI
repositories (UCI). These databases have been extensively used in relevant machine learning and
signal processing applications (Schapire, 1999; Aharon et al., 2006; Zhang & Li, 2010; Zhou et al.,
2014; Tang et al., 2016; Chatterjee et al., 2017).
For the learning task, and without loss of generality, we use the recently proposed extreme learning
machine (ELM) architecture (Huang et al., 2006; 2012), due to its implementation efficiency, good
regression and classification performance, and convexity of the resulting optimization problem. An
ELM typically uses a few hidden layers, each having many nodes, to project the input data vectors
to high dimensional feature vectors. Then, a linear projection is used at the last layer to recover the
output vector. An interesting property of ELM is the ability to use instances of random matrices in
mapping to feature vectors (as opposed to usual deep neural networks in which these weight matrices
should be optimized), and therefore we need to optimize only the weights of the last layer. In our
implementation, we have used a single hidden layer, an instance of random matrix between input
layer and hidden layer R, an element-wise rectifier linear unit (ReLU) function at each hidden node
that returns σ(∙) = max(∙, 0), weights to the output layer W. Given dataset D with N samples
and a binary selection vector z(k+1), we use the following optimization problem in the F-step of
Algorithm 1 at iterate k:
W(k+1) = argmin 1τ J) Xi∈[N] z(k+1) kf(◎)- Wσ(Rg)k2 + 入 k Wk2
s	.t. N Xi∈[N] kf (Xi)- Wσ(Rxi)k2 ≤ e ,
7
Under review as a conference paper at ICLR 2019
Table 1: Databases for regression task.
Database	# Training samples	# Test samples	Input dimension
Bodyfat	168	84	14
Housing	337	169	13
Space-ga	2,071	1,036	6
YearPredictionMSD	463,715	51,630	90
Power Consumption	1,556,445	518,814	9
Table 2: Regression performance on real databases. The reported values are “average ± standard deviation” of
the normalized true generalization error. “CR” stands for compression ratio. The values on row CR = 0%
corresponds to e(D, T).
CR	Bodyfat	Housing	Space-ga	YearPredictionMSD	Power Consumption
0%	0.0245 ± 0.0051	0.0301 ± 0.0056	0.1323 ± 0.0134	0.0082 ± 0.0007	0.0142 ± 0.0008
25%	0.0294 ± 0.0058	0.0345 ± 0.0071	0.1325 ± 0.0142	0.0083 ± 0.0007	0.0144 ± 0.0008
50%	0.0333 ± 0.0067	0.0323 ± 0.0080	0.1338 ± 0.0175	0.0085 ± 0.0008	0.0144 ± 0.0009
75%	0.0360 ± 0.0060	0.0374 ± 0.0076	0.1351 ± 0.0169	0.0086 ± 0.0010	0.0145 ± 0.0008
80%	0.0417 ± 0.0077	0.0382 ± 0.0076	0.1354 ± 0.0171	0.0086 ± 0.0009	0.0145 ± 0.0008
95%	0.0630 ± 0.0134	0.0538 ± 0.0122	0.1386 ± 0.0217	0.0088 ± 0.0012	0.0153 ± 0.0011
where the second term in the objective function is the Tikonov regularization with parameter λ that
alleviates the over-fitting problem. This convex quadratically constrained quadratic program can be
efficiently solved by existing optimization toolboxes (Grant & Boyd, 2014). Due to the randomness
in R, we repeat experiments 100 times and report the mean value and standard deviation of the
performance results.
Table 2 shows the regression performance of various databases, given in Table 1. From this table, our
approach can substantially compress the training datasets with a controlled loss on the generalization
error. This compressibility increases by the dataset size and decreases by the input dimension.
For instance, 75% compression in Bodyfat results in a noticeable performance drop, while a big
dataset like YearPredictionMSD can be compressed by 95% without a significant loss in the learning
performance. Moreover, these results empirically show the sub-linear characteristic of the sufficient
dataset for any fixed δ, namely CR → 1 as N → ∞. The results suggest that a small set of carefully
selected samples are enough to run the learning task. As we have discussed in Appendix B, a
similar preprocessing to identify and send only a small representative dataset could be inevitable to
implement distributed learning over communication-constrained networks, e.g., intra-body networks
or Internet-of-Things.
Further Discussions: In all the simulation experiments, we have observed a very fast convergence of
the proposed Algorithm 1, usually after afew iterations in small datasets (e.g., Bodyfat) and afew tens
of iterations in large datasets (e.g., Abalone) among D-step and F-step. Computational complexity of
each step is characterized in Proposition 3. Both bigger datasets and larger NK correspond to larger
search spaces and consequently slower convergence rate.
5 Conclusions
We addressed the compressibility of large datasets, namely the problem of finding dataset of minimal
cardinality (small representative dataset) for a learning task. We developed a framework that jointly
learns the input-output mapping and the representative dataset. We showed that its cardinality
increases sub-linearly with respect to that of the original dataset. While an asymptotic compressibility
of almost 100% is available in theory, we have observed that real datasets may be compressed as
much as 95% without any noticeable drop of the learning performance. These results challenge the
efficiency and benefits of the existing approaches to create big datasets and serve as benchmark for
distributed learning over communication-limited networks.
8
Under review as a conference paper at ICLR 2019
References
StatLib Repository. [Online] http://lib.stat.cmu.edu/datasets/, Accessed: 2018-09-
25.
UCI Machine Learning Repository. [Online] http://mlr.cs.umass.edu/ml, Accessed:
2018-09-25.
Michal Aharon, Michael Elad, and Alfred Bruckstein. k-SVD: An algorithm for designing over-
complete dictionaries for sparse representation. IEEE Transactions on Signal Processing, 54(11):
4311-4322, November 2006.
Dimitri P Bertsekas. Network optimization: Continuous and discrete models. Citeseer, 1998.
D.P. Bertsekas. Nonlinear Programming. Athena Scientific, 2 edition, 1999.
Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
ISBN 0521833787.
Saikat Chatterjee, Alireza M Javid, Mostafa Sadeghi, Partha P Mitra, and Mikael Skoglund. Progres-
sive learning for systematic design of large neural networks. arXiv preprint arXiv:1710.08177,
2017.
Kenneth L Clarkson, Elad Hazan, and David P Woodruff. Sublinear optimization for machine
learning. Journal of the ACM, 59(5):23, 2012.
Thomas M Cover and Joy A Thomas. Elements of Information Theory. John Wiley & Sons, 2012.
David L Donoho. Compressed sensing. IEEE Transactions on Information Theory, 52(4):1289-1306,
April 2006.
Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with image data.
arXiv preprint arXiv:1703.02910, 2017.
Michael Grant and Stephen Boyd. CVX: Matlab software for disciplined convex programming,
version 2.1, March 2014. [Online] http://cvxr.com/cvx, Accessed: 2018-09-25.
Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly,
Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep neural networks
for acoustic modeling in speech recognition: The shared views of four research groups. IEEE
Signal Processing Magazine, 29(6):82-97, 2012.
Guang-Bin Huang, Qin-Yu Zhu, and Chee-Kheong Siew. Extreme learning machine: Theory and
applications. Neurocomputing, 70(1-3):489-501, December 2006.
Guang-Bin Huang, Hongming Zhou, Xiaojian Ding, and Rui Zhang. Extreme learning machine for
regression and multiclass classification. IEEE Transactions on Systems, Man, and Cybernetics,
Part B (Cybernetics), 42(2):513-529, April 2012.
Abdul J Jerri. The shannon sampling theorem-Its various extensions and applications: A tutorial
review. Proceedings of the IEEE, 65(11):1565-1596, November 1977.
Xiaolin Jiang, Hossein S. Ghadikolaei, Gabor Fodor, Eytan Modiano, Zhibo Pang, Michele Zorzi, and
Carlo Fischione. Low-latency networking: Where latency lurks and how to tame it. Proceedings
of the IEEE, 2018. Accepted, To Appear.
Konstantina Kourou, Themis P Exarchos, Konstantinos P Exarchos, Michalis V Karamouzis, and Dim-
itrios I Fotiadis. Machine learning applications in cancer prognosis and prediction. Computational
and Structural Biotechnology Journal, 13:8-17, 2015.
Sindri Magnusson, ChinWendU Enyioha, Na Li, Carlo Fischione, and Vahid Tarokh. Convergence
of limited communication gradient methods. IEEE Transactions on Automatic Control, 63(5):
1356-1371, May 2018.
9
Under review as a conference paper at ICLR 2019
Jeff Michels, Ashutosh Saxena, and Andrew Y Ng. High speed obstacle avoidance using monocular
vision and reinforcement learning. In Proceedings of International Conference on Machine
Learning,pp. 593-600. ACM, 2005.
MNIST. MNIST Data Set. [Online] http://yann.lecun.com/exdb/mnist, Accessed:
2018-09-25.
MovieLens. MovieLens Data Set. [Online] https://grouplens.org/datasets/
movielens, Accessed: 2018-09-25.
Angelia Nedic and Dimitri P Bertsekas. Incremental subgradient methods for nondifferentiable
optimization. SIAM Journal on Optimization, 12(1):109-138, 2001.
Meisam Razaviyayn, Mingyi Hong, and Zhi-Quan Luo. A unified convergence analysis of block
successive minimization methods for nonsmooth optimization. SIAM Journal on Optimization, 23
(2):1126-1153, June 2013.
Jerome Sacks, William J Welch, Toby J Mitchell, and Henry P Wynn. Design and analysis of
computer experiments. Statistical Science, pp. 409-423, 1989.
Johan Schalkwyk, Doug Beeferman, Frangoise Beaufays, Bill Byrne, Ciprian Chelba, Mike Cohen,
Maryam Kamvar, and Brian Strope. “your word is my command”: Google search by voice: a case
study. In Advances in Speech Recognition, pp. 61-90. Springer, 2010.
Robert E Schapire. A brief introduction to boosting. In International Joint Conference on Artificial
Intelligence, volume 2, pp. 1401-1406, 1999.
Burr Settles. Active learning. Synthesis Lectures on Artificial Intelligence and Machine Learning, 6
(1):1-114, 2012.
Virginia Smith, Simone Forte, Ma Chenxin, Martin Takdc, Michael I Jordan, and Martin Jaggi.
CoCoA: A general framework for communication-efficient distributed optimization. Journal of
Machine Learning Research, 18:230, 2018.
Jiexiong Tang, Chenwei Deng, and Guang-Bin Huang. Extreme learning machine for multilayer
perceptron. IEEE Transactions on Neural Networks and Learning Systems, 27(4):809-821, April
2016.
John N Tsitsiklis and Zhi-Quan Luo. Communication complexity of convex optimization. Journal of
Complexity, 3(3):231-243, 1987.
Qiang Zhang and Baoxin Li. Discriminative K-SVD for dictionary learning in face recognition. In in
Proceedings of IEEE Computer Vision and Pattern Recognition, pp. 2691-2698, 2010.
Wenyi Zhao, Rama Chellappa, P Jonathon Phillips, and Azriel Rosenfeld. Face recognition: A
literature survey. ACM Computing Surveys, 35(4):399-458, December 2003.
Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. Learning deep
features for scene recognition using places database. In or scene recognition using places dataAd-
vances in Neural Information Processing Systems (NIPS), pp. 487-495, 2014.
10
Under review as a conference paper at ICLR 2019
Appendices
A Proofs
A.1 Proposition 1
Observe for z(k+1) of (9) that g2(z(k+1)) = K, which satisfies the constraint of optimization problem
(P2a). For index sequence j, introduced in Proposition 1, define Cjk) := ' (Xji, f (Xji), h(k)(xji)).
By definition, cjk) < Cjk) < …< cjk). We use the following lemmas:
Lemma 1. For any m ≤ K, the solution of (P 2a) satisfies zj(k+1) = 1.
Lemma 2. For any m > K, the solution of (P 2a) satisfies zj(k+1) = 0.
From Lemmas 1 and 2, kz(k+1)k0 = K and
(k+1)	1
zi	= 0
which completes the proof.
i = j1,j2, . . . ,jK
i = jK+1, . . . ,jN ,
A.2 Proposition 2
Note that the constraint on z must be closed and convex, as a sufficient condition for convergence of
BCD. Clearly this is not the case with z ∈ {0, 1}N in (Q1). Leveraging the equivalence between
(P2) and its linear program relaxation, (P 3), the constraint z ∈ [0 , 1]N is closed and convex. Since
a unique minimizer is found at each update, convergence to a stationary point follows from the
standard convergence results Bertsekas (1999)[Chap 2.7].
A.3 Proposition 4
We start by proving Proposition 4 for d = 1. To this end, we first introduce a variant of (P2) in
which we define K equidistant marks in x ∈ X = [0, T] and project E? to this set of marks, namely
we replace every entry in E? by its closest mark (measured by the Euclidian distance). Moreover, we
limit H to the class of L-Lipschitz functions passing through those marks. We first observe that the
approximation error of the solution of (P2) is upper bounded by that of the variant. In the following,
we derive the bound of Proposition 4 using the variant problem.
Divide entire domain X by K marks to some K - 1 disjoint sets {Si | Si∈[K-1] Si = X , Si T Sj =
φ, ∀i, j ∈ [K - 1]}. Define without loss of generality Si = [xi-1, xi) for sorted xi, and define
x0 := 0 and xK-1 := T. Note that F is the set of L-Lipschitz functions, samples are noiseless, and
{xi} are in the compressed dataset. Figure A.1 illustrates the function class F and three potential
examples for f(x) and h(x).
Define 'χ := ' (x, f (x), h?(x)). Let μχ be the probability measure on X that generates input samples
x. We have
Ex'x = ' 'x dμx
X
E	'si dμsi ,
i∈[K-1]	Si
(A.1)
where Si ∈ Si, {μsj are sub-probability measures on sets {Si}, and 5∑i∈[κ-i] μsi = 1. From
the extreme value theorem, there exists 'max for every interval Si such that 电 ≤ 'max, Nsi ∈ Si.
Therefore, Eχ'χ ≤ maxi 'max. Consider the following lemma:
Lemma 3. For our variant problem, |f (X) - h(X)| ≤ 2LkXk for all X ∈ Si and all i, where kXk is
the L2-norm of vector X.
The proof os Lemma 3 is straightforward after noting that f(X) - h(X) is a 2L-Lipschitz function.
1
Under review as a conference paper at ICLR 2019
Figure A.1: Illustration of the functional class F. Input space X is divided into disjoint sets {Si }. F is the set
of all L-Lipschitz functions passing through samples/marks {xi}i∈[K] . All functions f, h ∈ F lie in the dashed
red parallelograms. The slopes of these parallelograms are ±L. Three possible functions are shown in the figure.
Consider loss function Qx = |f (x) - h(x)∣2. When d = 1, it is easy to see from Lemma 3 and
figure A.1 that 'max ≤ 4L2(xi - xi-1)2 for every set Si, where xi - xi-1 is the measure of set Si.
Now, since sets {Si}, i ∈ [K - 1] have the same measure (defined based on equidistant grid points),
we have xi - xi-1 = T/(K - 1), so
4L2T2
Ex'x ≤ max 'max ≤ ------------y .
≤ i Si ≤ (K - 1)2
By setting g (h?, z?) ≤ Ex'x ≤ δ, we get K ≥ 1 + 2LT/√δ.
For d > 1, we can define equidistant marks on every coordinate of X and define a grid of (K1/d - 1)d
disjoint sets {Si}i, where we have assumed that K1/d is an integer number to avoid unnecessary nota-
tion complications. The distance between two consecutive marks on every coordinate is T /(K 1/d -1),
and therefore from Lemma 3
Ex ≤ max 'max ≤ 2L Tyd )
x ≤ i	Si ≤	K1/d - 1
By setting g (h?, z?) ≤ Eχ'χ ≤ δ,we get K ≥ (1 + 2LT
d
. This completes the proof.
A.4 Lemma 1
Assume Pi∈[N] zi(k+1) = Pi∈[N] zj(k+1) = M ≥ K. For k = 1, if zj(k+1) = 1 the statement holds.
If zj(k+1) = 0, then take any n for which zj(k+1) = 1 and observe that the following inequality holds
by definition of index set j :
(k+1)	(k)	P	(k+1)	(k)	+	(k)	P	(k+1)	(k)	+	(k)
∙i∈[N] zji	Cji	=乙i∈[N]∖{n}	Zji	Cji	+	Cjn	≥ 乙i∈[N]∖{n} Zji	Cji	+ j
M	=	M	≥	M
since C(jk) < C(jk) for any n. This completes the proof for k = 1. For k = 2 ≤ K, if Zj(k+1) = 1 the
1n	2
statement holds. If Zj(k+1) = 0, then take any n ≥ 3 for which Zj(k+1) = 1. Use Zj(k+1) = 1 and
observe that
Σ
i∈[N]
Z(k+1) C(k)
(k)
Cj1
+ C(jkn) +
Σ
i∈[N]∖{1,n}
Z(k+1)C(k)
C(jk1)+C(jk2)
+ P	Zj(ik+1)Cj(ik)
i∈[N ]∖{1,n}
≥
M
M
M
since C(jk) < C(jk) for any n > 2. We can use the same arguments recursively to prove that Zj(k+1) = 1
for any m ≤ K .
2
Under review as a conference paper at ICLR 2019
A.5 Lemma 2
Assume Pi∈[N] zj(k+1) = M ≥ K. By Lemma 1, zj(k+1) = 1 for all i ≤ K. We should show that
Pi∈K] j ≤ Pi∈[κ]*)+ PN=K+1 咪+1)c(?
K -	M
for any zj(k+1). This is clearly true as the left-hand-side is the average of the K smallest values of the
loss function on dataset of size N . In particular,
Pi∈[K] cj(ik) + PiN=K+1 zj(ik+1)c(jki)
M
M-K
z------A------{
≥ Pi∈κ]斓 + j + j + …+ j
≥	M
P	(k)	(M-K)Kc(jkK)
=Li∈[K] Cji	+____________
= K +
(k)
≥) Ei∈K] Cji
一 K ,
≥0
—}----------------------{
- (M - K) X cj(ik)
i∈[K]
MK
where (a) holds as KC(jk) ≥ Pi∈[K] Cj(k). This completes the proof.
A.6 Optimality of (8)
To prove the optimality of (8), recall that 1Tz = K in (P 2a) is of the form Az = B, where A is
a totally unimodular matrix, and B is an integer. Thus, optimization problem (8) is equivalent to
(P2a), and the linear program relaxation is optimal.
B Applications to Multi-agent Systems and Internet-of-Things
Consider a network of agents with some abstract form of very limited networking capacity (the
so-called communication-limited networks in optimization and control literature (Smith et al., 2018;
Magnusson et al., 2018; Nedic & Bertsekas, 2θ01; Tsitsiklis & Luo, 1987)). The limitation can
be, among others, due to low-power operation of agents (sensor nodes) in Internet-of-Things or
low channel capacity in harsh wireless environments, like intra-body networks. Consequently, we
have a very limited transmission rate among various agents. Each agent has a local dataset, e.g.,
some measurements, and they should all share the datasets to jointly run an optimization/learning
task within a short deadline. Due to the tight deadline and limited communication capability, we
cannot send all entries of the datasets to all other agents by the deadline. An important question
here is to decide, locally at every agent, which data should be shared (data selection). To address
this problem, one may consider the existence of an oracle that has access to all the datasets, finds
the minimal representative dataset, and then informs all the agents what to share. Clearly, this
oracle is not practical, rather it gives a theoretical benchmark on the performance of various solution
approaches and on the cardinality of the minimal representative dataset. Our (P2) models the oracle,
and our algorithmic solution approaches characterize the solution of the oracle. We can also get
useful insights on the “optimal” sub-sampling per agent, which can be exploited to develop practical
solution approaches for the original problem. Therefore, our results are of paramount importance for
the interesting problem of low-latency learning and inference over a communication-limited network.
C Additional Examples
The following example shows the generality of Assumptions 1 and 2.
Example 4. Let P denote the space of polynomial functions on R, f (x) = ex (∈ P), h(x) =
PnN=-01 xn/n! (∈ P) be the first N(< ∞) terms of the Taylor expansion of f (x), and `n(h) :=
|f(xn) - h(xn)|2. `n(h) is compatible with Assumption 1. Moreover, for almost any xn, xm ∈ R
3
Under review as a conference paper at ICLR 2019
(except a set of Lebesgue measure 0) such that xn 6= xm, we have `n(h) 6= `m(h), so Assumption 2
holds.
This example be easily generalized to the class of problems we study in this paper.
4