Under review as a conference paper at ICLR 2019
Accelerated Gradient Flow for
Probability distributions
Anonymous authors
Paper under double-blind review
Ab stract
This paper presents a methodology and numerical algorithms for constructing
accelerated gradient flows on the space of probability distributions. In particular,
we extend the recent variational formulation of accelerated gradient methods
in Wibisono et al. (2016) from vector valued variables to probability distributions.
The variational problem is modeled as a mean-field optimal control problem. The
maximum principle of optimal control theory is used to derive Hamilton’s equations
for the optimal gradient flow. The Hamilton’s equation are shown to achieve the
accelerated form of density transport from any initial probability distribution
to a target probability distribution. A quantitative estimate on the asymptotic
convergence rate is provided based on a Lyapunov function construction, when
the objective functional is displacement convex. Two numerical approximations
are presented to implement the Hamilton’s equations as a system of N interacting
particles. The continuous limit of the Nesterov’s algorithm is shown to be a special
case with N = 1. The algorithm is illustrated with numerical examples.
1 Introduction
Optimization on the space of probability distributions is important to a number of machine learning
models including variational inference (Blei et al., 2017), generative models (Goodfellow et al.,
2014; Arjovsky et al., 2017), and policy optimization in reinforcement learning (Sutton et al., 2000).
A number of recent studies have considered solution approaches to these problems based upon a
construction of gradient flow on the space of probability distributions (Zhang et al., 2018; Liu &
Wang, 2016; Frogner & Poggio, 2018; Chizat & Bach, 2018; Richemond & Maginnis, 2017; Chen
et al., 2018). Such constructions are useful for convergence analysis as well as development of
numerical algorithms.
In this paper, We propose a methodology and numerical algorithms that achieve accelerated gradient
flows on the space of probability distributions. The proposed numerical algorithms are related to
yet distinct from the accelerated stochastic gradient descent (Jain et al., 2017) and Hamiltonian
Markov chain Monte-Carlo (MCMC) algorithms (Neal et al., 2011; Cheng et al., 2017). The proposed
methodology extends the variational formulation of (Wibisono et al., 2016) from vector valued
variables to probability distributions. The original formulation of Wibisono et al. (2016) Was used to
derive and analyze the convergence properties of a large class of accelerated optimization algorithms,
most significant of Which is the continuous-time limit of the Nesterov’s algorithm (Su et al., 2014).
In this paper, the limit is referred to as the Nesterov’s ordinary differential equation (ODE).
1
Under review as a conference paper at ICLR 2019
The extension proposed in our work is based upon a generalization of the formula for the Lagrangian
in Wibisono et al. (2016): (i) the kinetic energy term is replaced with the expected value of kinetic
energy; and (ii) the potential energy term is replaced with a suitably defined functional on the space of
probability distributions. The variational problem is to obtain a trajectory in the space of probability
distributions that minimizes the action integral of the Lagrangian.
The variational problem is modeled as a mean-field optimal problem. The maximum principle
of the optimal control theory is used to derive the Hamilton’s equations which represent the first
order optimality conditions. The Hamilton’s equations provide a generalization of the Nesterov’s
ODE to the space of probability distributions. A candidate Lyapunov function is proposed for
the convergence analysis of the solution of the Hamilton’s equations. In this way, quantitative
estimates on convergence rate are obtained for the case when the objective functional is displacement
convex (McCann, 1997). Table 1 provides a summary of the relationship between the original
variational formulation in Wibisono et al. (2016) and the extension proposed in this paper.
We also consider the important special case when the objective functional is the relative entropy
functional D(ρ∣ρ∞) defined with respect to a target probability distribution ρ∞. In this case, the
accelerated gradient flow is shown to be related to the continuous limit of the Hamiltonian Monte-
Carlo algorithm (Cheng et al., 2017) (Remark 1). The Hamilton’s equations are finite-dimensional
for the special case when the initial and the target probability distributions are both Gaussian. In
this case, the mean evolves according to the Nesterov’s ODE. For the general case, the Lyapunov
function-based convergence analysis applies when the target distribution is log-concave.
As a final contribution, the proposed methodology is used to obtain a numerical algorithm. The
algorithm is an interacting particle system that empirically approximates the distribution with a finite
but large number of N particles. The difficult part of this construction is the approximation of the
interaction term between particles. For this purpose, two types of approximations are described: (i)
Gaussian approximation which is asymptotically (as N → ∞) exact in Gaussian settings; and (ii)
Diffusion map approximation which is computationally more demanding but asymptotically exact for
a general class of distributions.
The outline of the remainder of this paper is as follows: Sec. 2 provides a brief review of the
variational formulation in Wibisono et al. (2016). The proposed extension to the space of probability
distribution appears in Sec. 3 where the main result is also described. The numerical algorithm
along with the results of numerical experiments appear in Sec. 4. Comparisons with MCMC and
Hamiltonian MCMC are also described. The conclusions appear in Sec. 5.
Notation: The gradient and divergence operators are denoted as V and ▽• respectively. With multiple
variables, Vz denotes the gradient with respect to the variable z. Therefore, the divergence of the
vector field U is V ∙ U(x) = P：=i VxnUn(X) The space of absolutely continuous probability
measures on Rd with finite second moments is denoted by Pac,2(Rd). The Wasserstein gradient and
the Gateaux derivative of a functional F is denoted as VPF(P) and ∂ρ (P) respectively (see Appendix C
for definition). The probability distribution of a random variable Z is denoted as Law(Z).
2
Under review as a conference paper at ICLR 2019
	Vector	Probability distribution
State-space Objective function Lagrangian Lyapunov funct.	Rd f (X) eαt+γt (1 ∣e-atu∣2 - eβtf(x)) 1 ∣x + e-"Yty - X∣2 +eβt(f(x) - f (X))	P2(Rd) F(P) = D(PIPg) eɑt+γt Eh 2 ∣e-αt U∣2-eβtlog( ρ∞⅛)] 2E[∣Xt + e-γtYt - Tρ∞ (Xt) ∣2] +eβt(F(Pt) - F(ρ∞))
Table 1: Summary of the variational formulations for vectors and probability distributions.
2 Review of the variational formulation of Wibisono et al. (2016)
The basic problem is to minimize a C 1 smooth convex function f on Rd. The standard form of the
gradient descent algorithm for this problem is an ODE:
dX = -Vf(Xj	t ≥ 0	(1)
Accelerated forms of this algorithm are obtained based on a variational formulation due to Wibisono
et al. (2016). The formulation is briefly reviewed here using an optimal control formalism. The
Lagrangian L : R+ × Rd × Rd → R is defined as
L(t,x,u):= eαt+γt ( ɪ ∣e-atu|2 — eβt f(χ) )	(2)
1.	-{_	' potential energy
kinetic energy
where t ≥ 0 is the time, x ∈ Rd is the state, u ∈ Rd is the velocity or control input, and the
time-varying parameters αt, βt, γt satisfy the following scaling conditions: αt = log p - log t,
βt = plogt + log C, and γt = plogt where p ≥ 2 and C > 0 are constants.
The variational problem is
Minimize
u
Subject to
The Hamiltonian function is
∞
J(u) =
0
L(t, Xt, ut) dt
dXt
dt
ut,
X0 = x0
(3)
H(t,x,y,u) = y ∙ U — L(t, x, U)	(4)
where y ∈ Rd is dual variable and y ∙ U denotes the dot product between vectors y and u.
According to the Pontryagin,s Maximum Principle, the optimal control UJ=	=
arg max H(t, Xt, Yt, v) = eαt-γtYt. The resulting Hamilton’s equations are
v
-ddXt = +Vy H (t,Xt,Yt,ut) = eαt-γt Yt, Xo = xo	(5a)
-dYt = -VxH (t,Xt,匕，ut) = -eαt+βt+γt Vf (Xt),	K = yo	(5b)
The system (5) is an example of accelerated gradient descent algorithm. Specifically, if the parameters
αt, βt, γt are defined using p = 2, one obtains the continuous-time limit of the Nesterov’s accelerated
algorithm. It is referred to as the Nesterov’s ODE in this paper.
For this system, a Lyapunov function is as follows:
12
V(t,x,y) = 2 ∣x + e γty - x∣ + eβt (f (x) — f (x))	(6)
3
Under review as a conference paper at ICLR 2019
where X ∈ argminχ f (x). It is shown in Wibisono et al. (2016) that upon differentiating along the
solution trajectory,品 V(t,Xt,Yt) ≤ 0. This yields the following convergence rate:
f (Xt) - f (X) ≤ O(e-βt),	∀t ≥ 0	⑺
3 Variational formulation for probability distributions
3.1	Motivation and background
Let F : Pac,2(Rd) → R be a functional on the space of probability distributions. Consider the problem
of minimizing F(ρ). The (Wasserstein) gradient flow with respect to F(ρ) is
务=V∙ (ρtVρF(ρt))	(8)
where VPF(P) is the Wasserstein gradient of F.
An important example is the relative entropy functional where F(ρ) = D(ρ∣ρ∞):=
RRd log( pP(xX))ρ(χ) dx where ρ∞ ∈ Pac,2(Rd) is referred to as the target distribution. The gra-
dient of relative entropy is given by VPF(P) = V log(六).The gradient flow
^∂tt = -V ∙ (PtVIog(P∞)) + ∆ρt
(9)
is the Fokker-Planck equation (Jordan et al., 1998). The gradient flow achieves the density transport
from an initial probability distribution ρ0 to the target (here, also equilibrium) probability distribu-
tion P∞ ; and underlies the construction and the analysis of Markov chain Monte-Carlo (MCMC)
algorithms. The simplest MCMC algorithm is the Langevin stochastic differential equation (SDE):
dXt = -Vf (Xt) dt + √2dBt, Xo 〜ρo
where Bt is the standard Brownian motion inRd .
The main problem of this paper is to construct an accelerated form of the gradient flow (8). The
proposed solution is based upon a variational formulation. As tabulated in Table 1, the solution
represents a generalization of Wibisono et al. (2016) from its original deterministic finite-dimensional
to now probabilistic infinite-dimensional settings.
The variational problem can be expressed in two equivalent forms: (i) The probabilistic form is
described next in the main body of the paper; and (ii) The partial differential equation (PDE) form
appears in the Appendix. The probabilistic form is stressed here because it represents a direct
generalization of the Nesterov’s ODE and because it is closer to the numerical algorithm.
3.2	Probabilistic form of the variational problem
Consider the stochastic process {Xt}t≥0 that takes values inRd and evolves according to:
dXt ―〃	YC
=Ut, Xo 〜Po
dt
where the control input {Ut}t≥o also takes values in Rd, and Po ∈ Pac,2 (Rd) is the probability
distribution of the initial condition Xo . It is noted that the randomness here comes only from the
random initial condition.
Cl	. 1	1 "	/'	. •	1 "	/' . 1	∕'	L∕∖	C 7^^1 /	\ / \ 1	ml T	♦	I
Suppose the objective functional is of the form F(P) =	F (P, X)P(X) dX. The Lagrangian L :
R+ ×Rd × Pac,2 (Rd) ×Rd →Ris defined as
L(t, x, P, u) := eαt+γt ( Je-αtu|2 - eβtF(p, x) )	(10)
1. -{_ ' potential energy
kinetic energy
4
Under review as a conference paper at ICLR 2019
This formula is a natural generalization of the Lagrangian (2) and the parameters αt , βt , γt are defined
exactly the same as in the finite-dimensional case. The stochastic optimal control problem is:
Minimize J(u) = E
∞L(t,Xt,ρt,Ut)dt
0
dXt
Subjectto R = Ut, XLPP
(11)
where ρt = Law(Xt) ∈ Pac,2(Rd) is the probability density function of the random variable Xt.
The Hamiltonian function H : R+ × Rd × Pac,2 (Rd) × Rd × Rd → R for this problem is given
by (Carmona & Delarue, 2017, Sec. 6.2.3):
H(t, x, ρ,y,u) := U ∙ y — L(t, x,ρ, U)	(12)
where y ∈ Rd is the dual variable.
3.3 Main result
Theorem 1. Consider the variational problem (11).
(i)	The optimal control Ut = eαt-γtYt where the optimal trajectory {(Xt, Yt)}t≥o evolves
according to the Hamilton’s equations:
dX = Ut = eαt-γtYt,	X。〜P0	(13a)
学=-eαt+βt+γt VPF(Pt )(Xt),	Yp = Vφp(Xp)	(13b)
where φp is any Convexfunction and Pt := Law(Xt).
(ii)	Suppose also that the functional F is displacement convex and P∞ is its minimizer. Define
the energy along the optimal trajectory
V(t) = 2E[∣Xt + e-γtYt - τρ∞ (Xt)I2]+ eβt (F(P)-F(ρ∞))	(14)
where the map Tρρ∞ : Rd → Rd is the optimal transport map from Pt to P∞. Suppose also that
the following technical assumption holds: E[(Xt + e-γt Yt — Tρ∞ (Xt)) ∙ ~d TP∞ (Xt)] = 0.
Then ddV (t) ≤ 0. Consequently, thefollowing rate ofconvergence is obtained along the optimal
trajectory:
F(Pt) - F(P∞) ≤ O(e-βt), ∀t ≥0	(15)
Proof sketch. The Hamilton’s equations are derived using the standard mean-field optimal control
theory Carmona & Delarue (2017). The Lyapunov function argument is based upon the variational
inequality characterization of a displacement convex function (Ambrosio et al., 2008, Eq. 10.1.7).
The detailed proof appears in the Appendix. We expect that the technical assumption is not necessary.
This is the subject of the continuing work.	□
3.4	Relative entropy as the functional
In the remainder of this paper, we assume that the functional F(ρ) = D(ρ∣ρ∞) is the relative entropy
where P∞ ∈ Pac,2(Rd) is a given target probability distribution. In this case the Hamilton’s equations
are given by
dXt
dt
eαt-γtYt
X。〜P。
dYt
dt
-eαt+βt+γt (Vf(Xt) + V log(Pt(Xt)), YP = Vφp (Xp)
(16a)
(16b)
5
Under review as a conference paper at ICLR 2019
where ρt = Law(Xt) and f = - log(ρ∞). Moreover, if f is convex (or equivalently ρ∞ is log-
concave), then F is displacement convex with the unique minimizer at ρ∞ and the convergence
estimate is given by D(ρt∣ρ∞) ≤ O(e-βt).
Remark 1. The Hamilton’s equations (16) with the relative entropy functional is related to the
under-damped Langevin equation (Cheng et al., 2017). The difference is that the deterministic term
V log(ρt) in (16) is replaced with a random Brownian motion term in the under-damped Langevin
equation. More detailed comparison appears in the Appendix D.
3.5	Quadratic Gaussian case
Suppose the initial distribution ρ0 and the target distribution ρ∞ are both Gaussian, denoted as
N (mo, ∑o) and N (x,Q), respectively. This is equivalent to the objective function f (x) being
quadratic of the form f (x) = 1 (X — x)>Q-1(x 一 x). Therefore, this problem is referred to as the
quadratic Gaussian case. The following Proposition shows that the mean of the stochastic process
(Xt , Yt ) evolves according to the Nesterov ODE (5):
Proposition 1. (Quadratic Gaussian case) Consider the variational problem (11) for the quadratic
Gaussian case. Then
(i)	The stochastic process (Xt, Yt) is a Gaussian process. The Hamilton’s equations are given
by:
空=eαt-γt匕， $=-eαt+βt+γt(QT(Xt- X)- ∑-1(Xt - mt))
where mt and Σt are the mean and the covariance of Xt.
(ii)	Upon taking the expectation of both sides, and denoting nt := E[Yt]
dmt
dt
eαt-γtnt,
-dnt = -eαt+βt+γt Q-1(mt - χ)
dt	X-----{-----}
▽ f (mt)
which is identical to Nesterov ODE (5).
4 Numerical algorithm
The proposed numerical algorithm is based upon an interacting particle implementation of the
Hamilton’s equation (16). Consider a system of N particles {(Xti, Yti)}iN=1 that evolve according to:
dXti
dt
dYti
dt
eat-γt Yi,	X0 〜P0
-eαt+βt+γt(Vf(Xti)+ It(N)(Xti)),	Y0i=Vφ0(X0i)
7
interaction term
The interaction term It(N) is an empirical approximation of the V log(ρt) term in (16). We propose
two types of empirical approximations as follows:
1.	Gaussian approximation: Suppose the density is approximated as a Gaussian N(mt, Σt). In this
case, V log(ρt(X)) = -Σt-1(X - mt). This motivates the following empirical approximation of the
interaction term:
It(N)(X) = -Σt(N)-1(X - m(tN))
(18)
6
Under review as a conference paper at ICLR 2019
Algorithm 1 Interacting particle implementation of the accelerated gradient flow
Input: ρ0, φ0, N, t0, ∆t, p, C, K
Output: {Xki }iN=,1K,k=0
Initialize {X0}N=ι 削 Po,均=Vφ°(X0)
Compute I0(N) (X0i) with (18) or (19)
for k = 0 to K - 1 do
tk+ 2 = tk + 1 ʌt
Yi+1 = Yk- 2 Cptz:；('Vf(Xii)+ IkN )(xk ))ʌt
Xk+ι = Xk + 乐 Yiʌ
k+2
Compute Ik(N+1) (Xki+1) with (18)or (19)
珠+1 = Yki+1 - 2 Cptkp-1(Vf(Xk+1) + IkN (Xk+i))ʌt
tk+1 = tk+1 + 2 ʌt
end for
where m(N) := N-1 PN=I Xi is the empirical mean and Σ(N) := N-ɪ PN=I(Xt 一 m(N))(Xi —
mtkN ))> is the empirical covariance.
Even though the approximation is asymptotically (as N → ∞) exact only under the Gaussian
assumption, it may be used in a more general settings, particularly when the density ρt is unimodal.
The situation is analogous to the (Bayesian) filtering problem, where an ensemble Kalman filter is
used as an approximate solution for non-Gaussian distributions (Evensen, 2003).
2. Diffusion map approximation: This is based upon the diffusion map approximation of the
weighted Laplacian operator (Coifman & Lafon, 2006; Hein et al., 2007). For a C2 function f, the
weighted Laplacian is defined as ʌpf := ɪ V ∙ (PVf). Denote e(χ) = X as the coordinate function
on Rd. It is a straightforward calculation to show that V log(ρ) = ʌpe. This allows one to use
the diffusion map approximation of the weighted Laplacian to approximate the interaction term as
follows:
(DM) IkN)(XiJ jPSXXxXjXX
(19)
ge(x,y)
√P 幺1 g 式 y,xi
where the kernel k(x y)
is constructed empirically in terms of the Gaussian
kernel g(x y) = exp(-|x - y|2/(4)). The parameter is referred to as the kernel bandwidth. The
approximation is asymptotically exact as E J 0 and N ↑ ∞. The approximation error is of order
O(e) + O( √^1 d/4) where the first term is referred to as the bias error and the second term is referred
to as the variance error (Hein et al., 2007). The variance error is the dominant term in the error for
small values of E, whereas the bias error is the dominant term for large values of E (see Figure 3(d)).
The resulting interacting particle algorithm is tabulated in Table 1. The symplectic method proposed
in (Betancourt et al., 2018) is used to carry out the numerical integration. The algorithm is applied to
two examples as described in the following sections.
Remark 2. For the case where there is only one particle ( N = 1), the interaction term is zero and
the system (17) reduces to the Nesterov ODE (5).
Remark 3. (Comparison with density estimation) The diffusion map approximation algorithm is
conceptually different from an explicit density estimation-based approach. A basic density estimation
is to approximate P(X) ≈ N PN=I ge(x, Xi) where ge(x, y) is the Gaussian kernel. Using such an
7
Under review as a conference paper at ICLR 2019
(a)
Figure 1: Simulation result for the Gaussian case (Example 4.1): (a) The time traces of the particles;
(b) The KL-divergence as a function of time.
(b)
approximation, the interaction term is approximated as
(DE)
It(N)(Xti)
ɪ P= ge(χi,χj)(χj- Xi)
曝―Pj= ge(XiX)一
(20)
Despite the apparent similarity of the two formulae, (19) for diffusion map approximation and (20)
for density estimation, the nature of the two approximations is different. The difference arises
because the kernel k(x, y) in (19) is data-dependent whereas the kernel in (20) is not. While
both approximations are exact in the asymptotic limit as N ↑ ∞ and e 1 0, they exhibit different
convergence rates. Numerical experiments presented in Figure 3(a)-(d) show that the diffusion map
approximation has a much smaller variance for intermediate values of N. Theoretical understanding
of the difference is the subject of continuing work.
4.1	Gaussian Example
Consider the Gaussian example as described in Sec. 3.5. The simulation results for the scalar (d = 1)
case with initial distribution ρo = N(2,4) and target distribution N(x, Q) where x = -5.0 and
Q = 0.25 is depicted in Figure 1-(a)-(b). For this simulation, the numerical parameters are as follows:
N = 100, φ0(x) = 0.5(x - 2), t0 = 1, ∆t = 0.1, p = 2,C = 0.625, and K = 400. The result
numerically verifies the O(e-βt) = O(表)convergence rate derived in Theorem 1 for the case where
the target distribution is Gaussian.
4.2	Non-Gaussian example
This example involves a non-Gaussian target distribution ρ∞ = 2N(-m, σ2) + 2N(m, σ2) which
is a mixture of two one-dimensional Gaussians with m = 2.0 and σ2 = 0.8. The simulation results
are depicted in Figure 2-(a)-(b). The numerical parameters are same as in the Example 4.1. The
interaction term is approximated using the diffusion map approximation with e = 0.01. The numerical
result depicted in Figure 2-(a) show that the diffusion map algorithm converges to the mixture of
Gaussian target distribution. The result depicted in Figure 2-(b) suggests that the convergence rate
O(e-βt) also appears to hold for this non-log-concave target distribution. Theoretical justification of
this is subject of continuing work.
8
Under review as a conference paper at ICLR 2019
Figure 2: Simulation result for the non-Gaussian case (Example 4.2): (a) The time traces of the
particles; (b) The KL-divergence as a function of time.
4.3	Comparison with MCMC and HMCMC
This section contains numerical experiment comparing the performance of the accelerated algorithm 1
using the diffusion map (DM) approximation (19) and the density estimation (DE)-based approxima-
tion (20) with the Markov chain Monte-Carlo (MCMC) algorithm studied in Durmus & Moulines
(2016) and the Hamiltonian MCMC algorithm studied in Cheng et al. (2017).
We consider the problem setting of the mixture of Gaussians as in example 4.2. All algorithms are
simulated with a fixed step-size of ∆t = 0.1 for K = 1000 iterations. The performance is measured
by computing the mean-squared error in estimating the expectation of the function ψ(x) = x1x≥0.
The mean-square error at the k-th iteration is computed by averaging the error over M = 100 runs:
1M
m∙s∙ek = ME
m=1
("Xψ (Xikm)- /ψ (X)ρ∞(X) dx!
(21)
The numerical results are depicted in Figure 3. Figure 3(a) depicts the m.s.e as a function ofN . It is
observed that the accelerated algorithm 1 with the diffusion map approximation admits an order of
magnitude better m.s.e for the same number of particles. It is also observed that the m.s.e decreases
rapidly for intermediate values of N before saturating for large values of N , where the bias term
dominates (see discussion following Eq. 19).
Figure 3(b) depicts the m.s.e as a function of the number of iterations for a fixed number of particles
N = 100. It is observed that the accelerated algorithm 1 displays the quickest convergence amongst
the algorithms tested.
Figure 3(c) depicts the average computational time per iteration as a function of the number of
samples N . The computational time of the diffusion map approximation scales as O(N 2) because it
involves computing aN × N matrix [k (Xi, Xj)]iN,j=1, while the computational cost of the MCMC
and HMCMC algorithms scale as O(N ). The computational complexity may be improved by (i)
exploiting the sparsity structure of the N × N matrix ; (ii) sub-sampling the particles in computing the
empirical averages; (iii) adaptively updating the N × N matrix according to a certain error criteria.
Finally, we provide comparison between diffusion map approximation (20) and the density-based
approximation (20): Figure 3(d) depicts the m.s.e for these two approximations as a function of the
kernel-bandwidth for a fixed number of particles N = 00. For very large and for very small values
9
Under review as a conference paper at ICLR 2019
12 3
- - -
Ooo
111
①∙E
O O O O O
Illll
①∙E
——Accel. (DM)
--Accel. (DE)
-MCMC
……HMCMC
IO0
IO1	IO2	IO3
iteration
(b)
IO1
IO0
IO-1
(υ10^2
g IO-3
10^4
10~5
IO-6
ιo2	ιb3	10-3 ιo-2 io-ɪ 100	101
N	ε
(C)	(d)
Figure 3: Simulation-based comparison of the performance of the accelerated algorithm 1 using
the diffusion map (DM) approximation (19), the density estimation (DE)-based approximation (20)
with the MCMC and HMCMC algorithms: (a) the mean-squared error (m.s.e) (21) as a function of
the number of samples N ; (b) the m.s.e as a function of the number of iterations; (c) the average
computational time per iteration as a function of the number of samples; (d) m.s.e comparison
between the diffusion map and the density estimation-based approaches as a function of the kernel
bandwidth .
of , where bias and variance dominates the error, respectively, the two algorithms have similar m.s.e.
However, for intermediate values of , the diffusion map approximation has smaller variance, and
thus lower m.s.e.
5 Conclusion and directions for future work
The main contribution of this paper is to extend the variational formulation of Wibisono et al. (2016)
to obtain theoretical results and numerical algorithms for accelerated gradient flow in the space of
probability distributions. In continuous-time settings, bounds on convergence rate are derived based
on a Lyapunov function argument. Two numerical algorithms based upon an interacting particle
representation are presented and illustrated with examples. As has been the case in finite-dimensional
settings, the theoretical framework is expected to be useful in this regard. Some direction for future
include: (i) removing the technical assumption in the proof of the Theorem 1; (ii) analysis of the
convergence under the weaker assumption that the target distribution satisfies only a spectral gap
condition; and (iii) analysis of the numerical algorithms in the finite-N and in the finite ∆t cases.
10
Under review as a conference paper at ICLR 2019
References
LUigi Ambrosio, Nicola Gigli, and GiUsePPe Savar6. Gradient flows: in metric SPaCeS and in the
SPaCe of PrObability measures. Springer Science & Business Media, 2008.
Martin Arjovsky, Soumith Chintala, and L6on Bottou. Wasserstein gan. arXiv PrePrint
arXiv:1701.07875, 2017.
Michael Betancourt, Michael I Jordan, and Ashia C Wilson. On SymPleCtiC OPtimization. arXiv
PrePrint arXiv:1802.03653, 2018.
David M Blei, AlP Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians.
JOUrnaI of the AmeriCan Statistical Association, 112(518):859-877, 2017.
Rene Carmona and FrangoiS Delarue. PrObabiliStiC TheOry of Mean Field GameS With APPliCatiOnS
I-II. SPringer, 2017.
Changyou Chen, Ruiyi Zhang, Wenlin Wang, Bai Li, and Liqun Chen. A unified Particle-oPtimization
framework for scalable bayesian SamPling. arXiv PrePrint arXiv:1805.11659, 2018.
Xiang Cheng, Niladri S Chatterji, Peter L Bartlett, and Michael I Jordan. UnderdamPed langevin
mcmc: A non-asymPtotic analysis. arXiv PrePrint arXiv:1707.03663, 2017.
Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-Parameterized
models using OPtimaI transPort. arXiv PrePrint arXiv:1805.09545, 2018.
Ronald R Coifman and StePhane Lafon. Diffusion maPs. APPlied and ComPutational harmonic
analysis, 21(1):5-30, 2006.
Alain Durmus and Eric Moulines. High-dimensional bayesian inference via the unadjusted langevin
algorithm. arXiv PrePrint arXiv:1605.01559, 2016.
Geir Evensen. The ensemble kalman filter: Theoretical formulation and Practical imPlementation.
OCean dynamics, 53(4):343-367, 2003.
Charlie Frogner and Tomaso Poggio. APPrOXimate inference with wasserstein gradient flows. arXiv
PrePrint arXiv:1806.04542, 2018.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In AdvanCeS in neural
information PrOCeSSing systems, pp. 2672-2680, 2014.
Matthias Hein, Jean-Yves Audibert, and Ulrike von LuXburg. GraPh laPlacians and their convergence
on random neighborhood graPhs. JOUmal of MaChine Learning ReSearch, 8(Jun):1325-1368,
2007.
Prateek Jain, Sham M Kakade, Rahul Kidambi, Praneeth NetraPalli, and Aaron Sidford. Accelerating
stochastic gradient descent. arXiv PrePrint arXiv:1704.08227, 2017.
Richard Jordan, David Kinderlehrer, and FeliX Otto. The variational formulation of the fokker-Planck
equation. SIAM jOUrnaI on mathematical analysis, 29(1):1-17, 1998.
Qiang Liu and Dilin Wang. Stein variational gradient descent: A general PurPose bayesian inference
algorithm. In AdvanCeS In NeUral InfOrmatiOn PrOCeSSing Systems, pp. 2378-2386, 2016.
11
Under review as a conference paper at ICLR 2019
Robert J McCann. A convexity principle for interacting gases. AdVanCeS in mathematics, 128(1):
153-179,1997.
Radford M Neal et al. Mcmc using hamiltonian dynamics. HandbOOk OfMarkOV Chain Monte Carlo,
2(11):2, 2011.
Pierre H Richemond and Brendan Maginnis. On wasserstein reinforcement learning and the fokker-
planck equation. arXiv PrePrint arXiv:1712.07185, 2017.
Weijie Su, Stephen Boyd, and Emmanuel Candes. A differential equation for modeling nesteroV’s
accelerated gradient method: Theory and insights. In AdVanCeS in NeUraI InfOrmatiOn Processing
Systems, pp. 2510-2518, 2014.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient meth-
ods for reinforcement learning with function approximation. In AdVanCeS in neural information
processing systems, pp. 1057-1063, 2000.
Andre Wibisono, Ashia C Wilson, and Michael I Jordan. A variational perspective on accelerated
methods in optimization. PrOCeedingS of the NatiOnaI ACademy of Sciences, pp. 201614734, 2016.
Ruiyi Zhang, Changyou Chen, Chunyuan Li, and Lawrence Carin. Policy optimization as wasserstein
gradient flows. arXiv PrePrint arXiv:1808.03030, 2018.
A PDE formulation of the variational problem
An equivalent pde formulation is obtained by considering the stochastic optimal control problem (11)
as a deterministic optimal control problem on the space of the probability distributions. Specifically,
the process {ρt}t≥0 is a deterministic process that takes values in Pac,2(Rd) and evolves according
to the continuity equation
∂ρt
∂t
-▽ ∙ (ρtut)
where ut : Rd → Rd is now a time-varying vector field. The Lagrangian L : R+ × Pac,2 (Rd) ×
L2(Rd; Rd) → R is defined as:
L(t, ρ, u) := eαt+γt
7
Rd
ɪ ∣e-αtu(x) ∣2ρ(x) dx — eβtF(ρ)
(22)
The optimal control problem is:
Minimize
Subject to
∞
L(t, ρt, ut) dt
0
dpt + ▽• (ρtut) = 0
∂t
(23)
The Hamiltonian function H : R+ × Pac,2 (Rd) × C(Rd; R) × L2(Rd; Rd) → R is
H(t,ρ, Φ, u) := hVφ, u>L2(ρ) — L(t,ρ, U)	(24)
where φ ∈ C(Rd; R) is the dual variable and the inner-product 即Φ,UiL2(ρ) := JRd Vφ(χ) ∙
u(x)ρ(x) dx
12
Under review as a conference paper at ICLR 2019
B Restatement of the main res ult and its proof
We restate Theorem 1 below which now includes the pde formulation as well.
Theorem 2. Consider the variational problem (11)-(23).
(i)	For the Probabilisticform (11) ofthe variational problem, the optimal control Ut = eɑt-Yt Yt,
where the optimal trajectory {(Xt, Yt)}t≥0 evolves according to the Hamilton’s odes:
牛=Ut = eαt-γt Yt, Xo 〜P0	(25a)
dY = -eαt+βt+γt VPF (Pt)(Xt),	Y0 = Vφo(Xo)	(25b)
where φ0 is a convex function, and ρt = Law(Xt).
(ii)	For the pde form (23) of the variational problem, the optimal control is utt = eαt-γtVφt(x),
where the optimal trajectory {(ρt, φt)}t≥0 evolves according to the Hamilton’s pdes:
∂ρt
-7- = -V∙ (ρt e t YtVφt), initial condn. ρo	(26a)
∂t	×---{z---}
Ut
∂Φt = -eαt-γt lvφtl2 - eαt+γt+βt VPF(P)	(26b)
(iii)	The solutions of the two forms are equivalent in the following sense:
Law(Xt) = Pt,	Ut = ut(Xt), Yt = Vφt(Xt)
(iv)	Suppose additionally that the functional F is displacement convex and P∞ is its minimizer.
Define
V(t) = 1 E(∣Xt + e-γtYt - Tp∞ (Xt)∣2) + eβt (F(P)- F(ρ∞))	(27)
where the map TPPt∞ : Rd → Rd is the optimal transport map from Pt to P∞. Suppose also that
the following technical assumption holds: E[(Xt + e-γt Yt 一 Tρ∞ (Xt)) ∙ 亮 TP∞ (Xt)] = 0.
Then 箸(t) ≤ 0. Consequently, thefollowing rate ofconvergence is obtained along the optimal
trajectory
F(Pt) -F(P∞) ≤ O(e-βt),	∀t ≥0
Proof. (i) The Hamiltonian function defined in (12) is equal to
H(t, x,ρ,y,u) = y ∙ U — eγt-αt ^∣u∣2 + eαt+γtβtF(ρ, x)
after inserting the formula for the Lagrangian. According to the maximum principle in prob-
abilistic form for (mean-field) optimal control problems (see (Carmona & Delarue, 2017,
Sec. 6.2.3)), the optimal control law Utt = arg minv H(t, Xt, Pt, Yt, v) = eαt-YtYt and the
Hamilton’s equations are
dXt = +Vy H(t,Xt,ρt,Yt,Ut = Ut = eαt-γt Yt
dYt = -VχH(t,Xt,ρt,Yt,Ut) - i[VρH(t,Xt,ρt,Yt,Ut)(Xt)]
where Xt , Yt , Utt are independent copies of Xt , Yt , Utt . The derivatives
VxH(t, x, P, y, u) = eαt+βt+γt VxF(ρ, x)
VρH(t, x, ρ, y, u) = eαt+βt+γt VρF(ρ, x)
13
Under review as a conference paper at ICLR 2019
It follows that
dYt — -.αt+βt+γt
dt
VxF(Pt,Xt) + E[VρF(ρt,Xt)(Xt)])
=-eαt+βt+γt VPF(P)(Xt)
1	1 .1 1 r∙ ∙ . ∙ r- / ∖	C τ^ι /	\ / \ 1	1 . ι ∙ t . ∙ . ∕<~ι	C -ɪ-ʌ ι
where we used the definition F(P) = F (x, P)P(x) dx and the identity (Carmona & Delarue,
2017, Sec. 5.2.2 Example 3)
VρF(P)(x)
Vx F(ρ,x) + /
__ 二， ............
VρF(ρ, x)(X)P(X) dx
(ii)	The Hamiltonian function defined in (24) is equal to
/
H(t, P, φ, u)
Vφ(x) ∙ U(X) — leγt-αt ∣u(x)∣2
P(X) dX + eαt+γt+βt F(P)
after inserting the formula for the Lagrangian. According to the maximum principle for pde
formulation of mean-field optimal control problems (see (Carmona & Delarue, 2017, Sec.
6.2.4)) the optimal control vector field is UJ= = argmi∏v H(t, ρt, φt, V) = eɑt-γt Vφt and the
Hamilton’s equations are:
dPt _ ,
西=+
Pt,φt,ut) = -V ∙ (PtVu；)
等=—∂∂P(t,Pt,Φt,ut) = —(Vφ ∙ U* — eγt-αt 2∣u=∣2 + eαt+γt+βt∂P(Pt))
inserting the formula Ut* = eαt-γtVφt concludes the result.
(iii)	Consider the (Pt, φt) defined from (26). The distribution Pt is identified with a stochastic
~
~	一 一	UV	_	__ ,	, ʌ .	一	，二、	一	一-	.，二、
process Xt such that ^Xtt = eαt-γt Vφt(Xt) and Law(JXt) = Pt. Then define Yt = Vφt(Xt).
Taking the time derivative shows that
1√>	1	1 6	C /
dYt _ dV7zλ∕ 寸、—v72. /q \dXt_L pdφtfγ∖
^dT = dtNMXtt = V φt(Xt)F + V西(Xt)
=eαt-γt V2 φt(Xt)Vφt(Xt) — eαt-γt V2φt(Xt)Vφt(Xt) — eαt+βt+γt V ∂P (Pt)(Xt)
=-eαt+βt+γt V ∂P (Pt)(Xt)
=-eαt+βt+γt VPF(Pt)(Xt)
with the initial condition Y0 = Vφ0(X0), where we used the identity VxdP (p) = VPF(P) (Car-
mona & Delarue, 2017, Prop. 5.48). Therefore the equations for Xt and Yt are identical. Hence
one can identify (Xt ,Yt) with (Xt ,Yt).
(iv)	The energy functional
V(t) = 2E [∣Xt + e-γt
Yt — τρ∞ (Xt )∣2] + eβt (F(P)-F(P∞))
{Z^^
first term
Then the derivative of the first term is
}
^^^^{^^^^~
second term
E [(Xt + e-γtYt — Tρ∞(Xt)) ∙ (eαt-γtYt - Yte-γtYt — eαt+βt VpF(Pt)(Xt) + ξ(Tρ∞ (Xt)))]
where ξ(Tp∞ (Xt)):=京TP∞ (Xt). Using the scaling condition Yt = eat the derivative of the
first term simplifies to
E [(Xt + e-γt匕—TP∞(Xt)) ∙ (—eαt+βtVpF(Pt)(Xt) + ξ(Tp∞(Xt)))]
14
Under review as a conference paper at ICLR 2019
UPon using the technical assumption, E[(Xt + e-γt匕一Tρ∞(Xt)) ∙ ξ(Tρ∞(Xt))] = 0 the
derivative of the first term simplifies to
E [(Xt + e-γtYt - Tp∞ (Xt)) ∙(-e…JF(Pt)(Xt))]
The derivative of the second term is
-djt(second term) = Bteet(F(Pt) - F(ρ∞)) + eβt -ddtF(Pt)
=eαt+βt(F(ρt) - F(ρ∞)) + eβtE[VρF(ρt)(Xt)eαt-γt匕]
where we used the scaling condition βt = eαt and the chain-rule for the Wasserstein gradi-
ent (Ambrosio et al., 2008, Ch. 10, E. Chain rule). Adding the derivative of the first and second
term yields:
$(t) = eαt+βt (F(Pt)- F(ρ∞) - E [(Xt- Tf (Xt)) ∙VρF(Pt)(Xt)])
which is negative by variational inequality characterization of the displacement convex function
F(P) (Ambrosio et al., 2008, Eq. 10.1.7).
We expect that the technical assumption can be removed. This is the subject of the continuing
work.
□
C WASSERSTEIN GRADIENT AND GATEAUX DERIVATIVE
This section contains definitions of the Wasserstein gradient and Gateaux derivative (Ambrosio et al.,
2008; Carmona & Delarue, 2017).
Let F : Pac,2(Rd) → R be a (smooth) functional on the space of probability distributions.
Gateaux derivative: The Gateaux derivative of F at P ∈ Pac,2 (Rd) is a real-valued function on Rd
denoted as ∂ρρ (P) : Rd → R. It is defined as a function that satisfies the identity
It F(Pt)L0=∕d dP(P)(X)(-v∙(P(X)U(X)))dx
for all path Pt in Pac,2 (Rd) such that 等=-V ∙ (Ptu) with po = P ∈ Pac,2(Rd).
Wasserstein gradient: The Wasserstein gradient of F at P is a vector-field on Rd denoted as Vρ F(P) :
Rd → Rd . It is defined as a vector-field that satisfies the identity
-dF(Pt)	= ∣' VPF(P)(X)
dt	t=0 Rd
• U(X) p(x) dx
for all path Pt in Pac,2 (Rd) such that 等=-V • (ρtu) with ρο = P ∈ Pac,2(Rd).
The two definitions imply the following relationship (Carmona & Delarue, 2017, Prop. 5.48):
∂F
VρF(ρ)(∙) = Vχ ∂ρ (P)(∙)
15
Under review as a conference paper at ICLR 2019
Example: Let F(P) = R log( PP(X)J)ρ(χ) dx be the relative entropy functional. Consider a path Pt in
Pac,2(Rd) such that d∂pt = -V∙ 3u) with Po = P ∈ Paj(Rd). Then
d F(C)= ∕ιo√ Pt(X)) dPt (x)dx + ［皿(x)dx
dtF(Pt) = ∕log(P∞(x))西(X)dx + J 西(X)dx
=—l log( Pt(X))V ∙ (Pt(X)U(X))dX
P∞(X)
= / Vx log( Pt(X)) ∙ U(X) Pt (x) dX
P∞(X)
where the divergence theorem is used in the last step. The definitions of the Gateaux derivative and
Wasserstein gradient imply
dP(P)(X)=log( ρ∞xX))
VPF(P)(X) = Vx log( ρ∞(⅛)
D Relationship with the under-damped Langevin equation
A basic form of the under-damped (or second order) Langevin equation is given in Cheng et al. (2017)
dXt = vt dt
dvt = —γvt dt — Vf(Xt) dt + √2dBt
(28)
where {Bt}t≥0 is the standard Brownian motion.
Consider next, the the accelerated flow (16). Denote vt := eαt-γtYt. Then, with an appropriate
choice of scaling parameters (e.g. at = 0, βt = 0 and Yt = -Yt):
dXt = vt dt
t t	(29)
dvt = —Yvt dt — Vf(Xt)dt — Vx log(Pt(Xt))
The scaling parameters are chosen here for the sake of comparison and do not satisfy the ideal scaling
conditions of Wibisono et al. (2016).
The sdes (28) and (29) are similar except that the stochastic term √2 dBt in (28) is replaced with a
deterministic term —Vx log(Pt(Xt)) in (29). Because of this difference, the resulting distributions
are different. Let pt(X, v) denote the joint distribution on (Xt, vt) of (28) and let qt(X, v) denote the
joint distribution on (Xt, vt) of (29). Then the corresponding Fokker-Planck equations are:
-Vx ∙ (pt(X,v)v) + Vv ∙ (pt(X,v)(γv + Vf (x))) + ∆vpt(X,v)
彻"
(x,v) = -Vx ∙ (qt(X,v)v) + Vv ∙ (qt(X,v)(γv + Vf (x))) + Vv ∙ (qt(X,y)Vx log(Pt(/)))
where Pt(X) = qt(X, v) dv is the marginal of qt(X, y) on X. The final term in the Fokker-Planck
equations are clearly different. The joint distributions are different as well.
The situation is in contrast to the first order Langevin equation, where the stochastic term √2 dBt and
-V log(Pt(Xt)) are equivalent, in the sense that the resulting distributions have the same marginal
distribution as a function of time. To illustrate this point, consider the following two forms of the
Langevin equation:
dXt = -Vf (Xt) dt + √2dBt
dXt = -Vf(Xt)dt-Vlog(Pt(Xt))
(30)
(31)
16
Under review as a conference paper at ICLR 2019
Let pt(x) denote the distribution of Xt of (30) and let qt(x) denote the distribution of Xt of (31).
The corresponding Fokker-Planck equations are as follows
c⅛p闻一t
-	V ∙ (pt(χ)Vf (x)) + ∆pt(χ)
-	V ∙ (qt(x)Vf (x)) + V ∙ (qt(x)Vlog(ρt(x)))
-	V∙ (qt(x)Vf(x)) + V ∙ (qt(x)Vlog(qt(x)))
-	V∙ (qt(x)Vf(x)) + ∆qt(x)
where we used ρt(x) = qt(x). In particular, this implies that the marginal probability distribution of
the stochastic process Xt are the same for first order Langevin sde (30) and (31) .
17