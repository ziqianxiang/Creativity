Under review as a conference paper at ICLR 2019
Automata Guided Skill Composition
Anonymous authors
Paper under double-blind review
Ab stract
Skills learned through reinforcement learning often generalize poorly across tasks
and re-training is necessary when presented with a new task. We present a frame-
work that combines techniques in formal methods with reinforcement learning
(RL) that allows for the construction of new skills from existing ones with no ad-
ditional exploration necessary. Our method also allows for convenient specifica-
tion of complex temporal dependent tasks using logical expressions. We provide
theoretical results for our composition technique and evaluate on a simple grid
world simulation as well as a robotic manipulation task.
1	Introduction
Policies learned using reinforcement learning aim to maximize the given reward function and are
often difficult to transfer to other problem domains. Skill composition is the process of constructing
new skills out of existing ones (policies) with little to no additional learning. In stochastic opti-
mal control, this idea has been adopted by Todorov (2009) and Da Silva et al. (2009) to construct
provably optimal control laws based on linearly solvable Markov decision processes.
Temporal logic (TL) is a formal language commonly used in software and digital circuit verification
(Baier & Katoen, 2008) as well as formal synthesis (Belta et al., 2017). It allows for convenient
expression of complex behaviors and causal relationships. TL has been used by (Tabuada & Pappas,
2004), (Fainekos et al., 2006), (Fainekos et al., 2005) to synthesize provably correct control policies.
Aksaray et al. (2016) have also combined TL with Q-learning to learn satisfiable policies in discrete
state and action spaces.
We make the distinction between skill composition and multi-task learning/meta-learning where the
latter often requires a predefined set of tasks/task distributions to learn and generalize from, whereas
the focus of the former is to construct new policies from a library of already learned policies that
achieve new tasks (often some combination of the constituent tasks) with little to no additional con-
straints on task distribution at learning time. In this work, we focus on skill composition with policies
learned using automata guided reinforcement learning (Li et al., 2018). We adopt the syntactically
co-safe truncated linear temporal logic (scTLTL) as the task specification language. Compared to
most heuristic reward structures used in the RL literature, formal specification language has the
advantage of semantic rigor and interpretability.
In our framework, skill composition is accomplished by taking the product of finite state automata
(FSA). Instead of interpolating/extrapolating among learned skills/latent features, our method is
based on graph manipulation of the FSA. Therefore, the outcome is much more transparent. Com-
pared with previous work on skill composition, we impose no constraints on the policy representa-
tion or the problem class. We validate our framework in simulation (discrete state and action spaces)
and experimentally on a Baxter robot (continuous state and action spaces).
2	Related Work
Recent efforts in skill composition have mainly adopted the approach of combining value functions
learned using different rewards. Peng et al. (2018) constructs a composite policy by combining
the value functions of individual policies using the Boltzmann distribution. With a similar goal,
Zhu et al. (2017) achieves task space transfer using deep successor representations (Kulkarni et al.,
2016). However, it is required that the reward function be represented as a linear combination of
state-action features.
1
Under review as a conference paper at ICLR 2019
Haarnoja et al. (2018) have showed that when using energy-based models (Haarnoja et al., 2017),
an approximately optimal composite policy can result from taking the average of the Q-functions of
existing policies. The resulting composite policy achieves the -AN D- task composition i.e. the
composite policy maximizes the average reward of individual tasks.
van Niekerk et al. (2018) have taken this idea a step further and showed that by combining individual
Q-functions using the log-sum-exponential function, the -OR- task composition (the composite
policy maximizes the (soft) maximum of the reward of constituent tasks) can be achieved optimally.
We build on the results of (van Niekerk et al., 2018) and show that incorporating temporal logic
allows us to compose tasks of greater logical complexity with higher interpretability. Our composite
policy is optimal in both -AND- and -OR- task compositions.
3	Preliminaries
3.1	Entropy-Regularized Reinforcement Learning and Q-Composition
We start with the definition of a Markov Decision Process.
Definition 1. An MDP is defined as a tuple M = (S,A,p(∙∣∙, ∙),r(∙, ∙, •)〉, where S ⊆ IRn is the
state space ; A ⊆ IRm is the action space (S and A can also be discrete sets); p : S × A × S → [0, 1]
is the transition function with p(s0|s, a) being the conditional probability density of taking action
a ∈ A at state s ∈ S and ending up in state s0 ∈ S; r : S × A × S → IR is the reward function with
r(s, a, s0) being the reward obtained by executing action a at state s and transitioning to s0.
In entropy-regularized reinforcement learning (Schulman et al., 2017), the goal is to maximize the
following objective
T-1
J (π) = X En [rt + αH(π(∙lst))],	⑴
t=0
where ∏ : S X A → [0,1] is a stochastic policy. En is the expectation following ∏. H(∏(∙∣st)) is the
entropy of π. α is the temperature parameter. In the limit α → 0, Equation (1) becomes the standard
RL objective. The soft Q-learning algorithm introduced by (Haarnoja et al., 2017) optimizes the
above objective and finds a policy represented by an energy-based model
π?(at∣st) H exp(-E(st, at))	(2)
where E(st, at) is an energy function that can be represented by a function approximator.
Let rent = rt + αH(π(∙∣st)), the state-action value function (Q-function) following π is defined
as Qπ(s, a) = Eπ [PtT=-01 rtent|s0 = s, a0 = a]. Suppose we have a set of n tasks indexed by
i, i ∈ {0, ..., n}, each task is defined by an MDP Mi that differs only in their reward function
ri. Let Qπαi be the optimal entropy-regularized Q-function. Authors of (van Niekerk et al., 2018)
provide the following results
Theorem 1. Define vectors r = [r1, ..., rn], Qπα? = [Qπα1 , ..., Qπαn]. Given a set of non-
negative weights w with ||w|| = 1, the optimal Q-function for a new task defined by r =
αlog(∣∣ exp(r∕ɑ)∣∣w) is given by
Qα? = αlog(ll exp(Qα?∕α)∣∣w),	(3)
where ∣∣∙∣∣w is the weighted 1-norm.
The authors proceed to provide the following corollary
Corollary 1. max Qπα? ↑ Q0π? as α → 0, where Q0π? is the optimal Q-function for the objective
J(π) =PtT=-01Eπ[rt].
Corollary 1 states that in the low temperature limit, the maximum of the optimal entropy-regularized
Q-functions approaches the standard optimal Q-function
2
Under review as a conference paper at ICLR 2019
3.2	scTLTL and Finite State Automata
We consider tasks specified with syntactically co-safe Truncated Linear Temporal Logic (scTLTL)
which is derived from truncated linear temporal logic(TLTL) (Li et al., 2018). The syntax of scTLTL
is defined as
Φ ：= > | f(s) < c l-Φ | Φ ∧ Ψ]◊◊ | ΦU Ψ | Φ TΨ | O φ
(4)
where > is the True Boolean constant. s ∈ S is a MDP state in Definition 1. f(s) < c is a
predicate over the MDP states where c ∈ IR.  (negation/not), ∧ (conjunction/and) are Boolean
connectives. ◊ (eventually), U (until), T (then), O (next), are temporal operators.⇒ (implication)
and and ∨ (disjunction/or) can be derived from the above operators.
We denote st ∈ S to be the MDP state at time t, and st:t+k to be a sequence of states (state trajectory)
from time t to t + k, i.e., st:t+k = stst+1...st+k. The Boolean semantics of scTLTL is defined as:
St:t+k	|= f(S) < c	⇔	f(St) < c,
St:t+k	=-φ	⇔	-(St:t+k = φ),
St:t+k	|= φ ⇒ ψ	⇔	(St:t+k |= φ) ⇒ (St:t+k |= ψ),
St:t+k	|= φ ∧ ψ	⇔	(St:t+k |= φ) ∧ (St:t+k |= ψ),
St:t+k	|= φ ∨ ψ	⇔	(St:t+k |= φ) ∨ (St:t+k |= ψ),
St:t+k	|= Oφ	⇔	(St+1:t+k |= φ) ∧ (k > 0),
St:t+k	=◊	⇔	∃t0 ∈ [t, t + k) St0:t+k |= φ,
St:t+k	|= φ U ψ	⇔	∃t0 ∈ [t,t+ k) S.t. St0:t+k |= ψ ∧ (∀t00 ∈ [t, t0) St00:t0 |= φ),
St:t+k	|= φ T ψ	⇔	∃t0 ∈ [t,t+ k) S.t. St0:t+k |= ψ ∧ (∃t00 ∈ [t, t0) St00:t0 |= φ).
A trajectory S0:T is said to satisfy formula φ if S0:T |= φ.			
The quantitative semantics (also referred to as robustness) is defined recursively as
P(St:t+k, T)
P(St：t+k,f (st) < C)
P(St：t+k, -φ)
P(St:t+k, φ ⇒ ψ)
P(St:t+k, φ1 ∧ φ2)
P(St:t+k, φ1 ∨ φ2)
P(St:t+k, Oφ)
P(St:t+k, Qφ~)
P(St:t+k, φ U ψ)
P(St:t+k, φ T ψ)
Pmax,
c - f(St),
- P(St:t+k, φ),
max(-P(St:t+k, φ), P(St:t+k, ψ))
min(P(St:t+k, φ1), P(St:t+k, φ2)),
max(P(St:t+k, φ1), P(St:t+k, φ2)),
P(st+1：t+k ,φ) (k > O),
max (P(St0:t+k, φ)),
t0∈[t,t+k)
max (min(P(St0:t+k, ψ),
t0∈[t,t+k)
min P(St00:t0, φ))),
t00∈[t,t0)
max (min(P(St0:t+k, ψ),
t0∈[t,t+k)
max P(St00:t0, φ))),
t00∈[t,t0)
where Pmax represents the maximum robustness value. A robustness of greater than zero implies
that St:t+k satisfies φ and vice versa (P(St:t+k, φ) > 0 ⇒ St:t+k |= φ and P(St:t+k, φ) < 0 ⇒
St:t+k 6|= φ). The robustness is used as a measure of the level of satisfaction of a trajectory S0:T with
respect to a scTLTL formula φ.
3
Under review as a conference paper at ICLR 2019
Definition 2. An FSA corresponding to a scTLTL formula φ. is defined as a tuple Aφ =
hQφ, Ψφ, qφ,o,Pφ(∙∣∙), Fφi, where Qφ is a set of automaton states; Ψφ is the input alphabet (a set
of first order logic formula); qφ,0 ∈ Qφ is the initial state; pφ : Qφ × Qφ → [0, 1] is a conditional
probability defined as
pφ(qφ,j lqφ,i) = ( 0
or
pφ(qφ,j “i,S) = {0
ψqφ,i,qφ,j is true
otherwise.
ρ(S, ψqφ,i,qφ,j) > 0
otherwise.
(5)
Fφ is a set of final automaton states.
Here qφ,i is the ith automaton state of Aφ. ψqφ,i ,qφ,j ∈ Ψφ is the predicate guarding the transi-
tion from qφ,i to qφ,j. Because ψqφ,i,qφ,j is a predicate without temporal operators, the robustness
p(st∙t+k,ψqφ,i,qφ,j) is only evaluated at st. Therefore, We use the shorthand ρ(st,ψqφ,i,qφ,j)=
p(st：t+k, ψqφ,i,qφ,j). The translation from a TLTL formula to a FSA can be done automatically with
available packages like Lomap (Vasile, 2017).
3.3 FSA Augmented MDP
The FSA Augmented MDP is defined as follows
Definition 3. (Li et al., 2018) An FSA augmented MDP corresponding to scTLTL formula φ
(constructed from FSA hQφ, Ψφ, qφ,o,Pφ(∙∣∙), Fφ and MDP hS, A,p(∙∣∙, ∙),r(∙, ∙, •)〉)is defined as
Mφ = hS, A,p(∙∣∙, ∙), r(∙, ∙), Fφi where S ⊆ S X Qφ, P(Sls, a) IS the probability of transitioning
to s0 given S and a,
"a)= p((s0,qφ )1(Sa) = {p(S01s,a)P%qw 夕) = 1	⑹
pφ is defined in Equation (5). rS : S × S → IR is the FSA augmented reward function, defined by
rS(sS,sS0)=ρ(s0,Dφqφ),
(7)
where Dφ = Wqφ∈Ωqφ ψqφ ,q0 represents the disjunction of all predicates guarding the transitions
that originatefrom q$ (Ωqφ is the set ofautomata states that are connected with q through outgoing
edges).
A policy πφ? is said to satisfy φ if
∏φ = argmaxEπφ[1(ρ(s0:T,φ) > 0)].	(8)
πφ
where 1(p(s0：T, φ) > 0) is an indicator function with value 1 if p(s0：T, φ) > 0 and 0 otherwise.
As is mentioned in the original paper, there can be multiple policies that meet the requirement of
Equation (8), therefore, a discount factor is used to find a maximally satisfying policy - one that
leads to satisfaction in the least amount of time.
The FSA augmented MDP Mφ establishes a connection between the TL specification and the stan-
dard reinforcement learning problem. A policy learned using Mφ has implicit knowledge of the
FSA through the automaton state qφ ∈ Qφ. We will take advantage of this characteristic in our skill
composition framework.
4
Under review as a conference paper at ICLR 2019
Figure 1 : FSA for (a) φ1 = ♦a ∧ ♦b. (b) φ2 = ♦c. (c) φ∧ = φ1 ∧ φ2 .
4	Problem Formulation
Problem 1. Given two ScTLTLformuIa φι and φ2 and their optimal Q-functions Qφι and Qφ2,
obtain the optimal policy πφ? that satisfies φ∧ = φ1 ∧ φ2 and πφ? that satisfies φ∨ = φ1 ∨ φ2.
Here Qφ and Qφ can be the optimal Q-functions for the entropy-regularized MDP or the stan-
dard MDP. Problem 1 defines the problem of skill composition: given two policies each satisfying
a scTLTL specification, construct the policy that satisfies the conjunction (-AN D-)/disjunction
(-OR-) of the given specifications. Solving this problem is useful when we want to break a com-
plex task into simple and manageable components, learn a policy that satisfies each component and
”stitch” all the components together so that the original task is satisfied. It can also be the case that
as the scope of the task grows with time, the original task specification is amended with new items.
Instead of having to re-learn the task from scratch, we can learn only policies that satisfies the new
items and combine them with the old policy.
5	Automata Guided S kill Composition
In this section, we provide a solution for Problem 1 by constructing the FSA of φ∧ from that of φ1
and φ2 and using φ∧ to synthesize the policy for the combined skill. We start with the following
definition.
Definition 4. Given Aφ1 = hQφ1, Ψφ1, qφ1,0,pφ1,Fφ1i and Aφ2 = hQφ2, Ψφ2,qφ2,0,pφ2,Fφ2i
corresponding to formulas φ1 and φ2, the FSA ofφ∧ = φ1 ∧φ2 is the product automaton of Aφ1 and
Aφ1, i.e. Aφ∧ = φ1 ∧ φ2 = Aφ1 × Aφ2 = hQφ∧,Ψφ∧,qφ∧,0,pφ∧,Fφ∧i where Qφ∧ ⊆ Qφ1 × Qφ2
is the set of product automaton states, qφ∧,0 = (qφ1,0, qφ2,0) is the product initial state, Fφ∧ ⊆
Fφ1 ∩ Fφ2 are the final accepting states. Following Definition 2, for states qφ∧ = (qφ1 , qφ2) ∈ Qφ∧
and qφ0 = (qφ0 , qφ0 ) ∈ Qφ∧, the transition probability pφ∧ is defined as
pφ∧ (qφ ∧ lqφ∧)
10
pφi (qφ Jqφ1)PΦ2gφ 2 M) = 1
otherwise.
(9)
5
Under review as a conference paper at ICLR 2019
Example 1. Figure 1 illustrates the FSA of Aφ1 and Aφ2 and their product automaton Aφ∧. Here
φι = ♦r ∧ ♦g which entails that both r and g needs to be true at least once (order does not matter),
and φ2 = ♦b. The resultant product corresponds to the formula φ = ♦r ∧ ♦g ∧ ♦b.
We provide the following theorem on automata guided skill composition
Theorem 2. Let Qπφ? = [Qπφ1 , ..., Qπφn] be a vector with entries Qφπi being the optimal Q-function
π?
for the FSA augmented MDP Mφi. The optimal Q-function for Mφ∧ where φ∧ = i φi is Qφ∧ =
max(Qπφ?).
Proof. For qφ∧ = (qφ1, qφ2) ∈ Qφ∧, let Ψqφ∧, Ψqφ1 and Ψqφ2 denote the set of predicates guarding
the edges originating from qφ∧ , qφ1 and qφ2 respectively. Equation (9) entails that a transition at qφ∧
in the product automaton Aφ∧ exists only if corresponding transitions at qφ1 , qφ2 exist in Aφ1 and
Aφ2	respectively. Therefore,	ψqφ∧,qφ0 ∧	=	ψqφ1,qφ0 1	∧ψqφ2,qφ02,	for	ψqφ∧,qφ0 ∧	∈	Ψqφ∧, ψqφ1,qφ0 1	∈
Ψqφ1, ψqφ2,qφ2 ∈ Ψqφ2 (here qφi is a state SUCh thatPφ"qφi*) = 1). Therefore, We have
Dqφ∧ =	_ (ψ 0 ∧ψ 0 )	(10)
φ∧	qφ1 ,qφ1	qφ2 ,qφ2
qφ1 ,qφ2
Where qφ0 , qφ0 don’t eqUal to qφ1 , qφ2 at the same time (to avoid self looping edges). Using the faCt
that ψqφ 国力 = - W ψqφ & and repeatedly applying the distributive laws (∆∧Ω 1) ∨ (∆∧Ω2)=
qφ0i 6=qφi	i
∆ ∧ (Ωι ∨ Ω2) and (∆ ∨ Ωι) ∧ (∆ ∨ Ω2) = ∆ ∨ (Ωι ∧ Ω2) to Dφφ∧, we arrive at
Dφ∧∧= ( V	ψqφι,qφ 1) ∨ ( V	ψqφ2 ,qφ 2) = D 小蝶∙	(11)
qφ01 6=qφ1	qφ02 6=qφ2
Let rφ∧, rφι, rφ^ and Sφ∧, S@、, S@? be the reward functions and states for FSA augmented MDP
Mφ∧ , Mφ1 , Mφ2 respeCtively. sφ∧ , sφ1 , sφ2 are the states for the Corresponding MDPs. Plugging
Equation (11) into Equation (7) and using the robustness definition for disjunction results in
rΦ∧ (sΦ∧ ,sφ∧) = P(Sφ∧ ,Dφ∧∧)
=ρ(s0φ∧,Dφqφ11∨Dφqφ22)
=max (P(Sφι, Dφφ1 ),P(Sφ2, Dφφ2))
=max (rφl (sφl ,sφι ), rφ2 (sφ2 ,sφ2 )).
(12)
Looking at Theorem 1, the log-sum-exp of the composite reward r= α log(∣∣ exp(r∕α) ||w) is in fact
an approximation of the maximum function. In the low temperature limit we have r → max(r) as
ɑ → 0. Applying Corollary 1 results in Theorem 2.	□
Having obtained the optimal Q-function, a policy can be constructed by taking the greedy step with
respective to the Q-function in the discrete action case. For the case of continuous action space
where the policy is represented by a function approximator, the policy update procedure in actor-
critic methods can be used to extract a policy from the Q-function.
In our framework, -AND- and -OR- task compositions follow the same procedure (Theorem
2). The only difference is the termination condition. For -AN D- task, the final state Fφ∧ = T Fφi
in Definition 4 needs to be reached (i.e. all the constituent FSAs are required to reach terminal state,
as in state qφ∧,f in Figure 1 ). Whereas for the -OR- task, only Fφ∨ = S Fφi needs to be
reached (one of states qφ∧,2, qφ∧,4, qφ∧,5, qφ∧,6, qφ∧,f in Figure 1 ). A summary of the composition
procedure is provided in Algorithm 1.
In Algorithm 1, steps 3 and 4 seeks to obtain the optimal policy and Q-function using any off-policy
actor critic RL algorithm. Bφ1,2 are the replay buffers collected while training for each skill. Step
6
Under review as a conference paper at ICLR 2019
Algorithm 1 Automata Guided Skill Composition
1:	Inputs: scTLTL task specification φ1 and φ2 , randomly initialized policies πφ1 , πφ2 and action-
value functions Qφπφ1 , Qπφφ2 . State and action spaces of the MDP.
2:	Construct FSA augmented MDPs Mφ1 and Mφ2	. using Definition 3
π?
3:	πφ∖, Qφ[1, Bφι - ActorCritic(Mφι)	. learns the optimal policy and Q-function
π?
4:	n?2 ,Qφφ2, BΦ2 - ACtOrCritiC(Mφ2)
π?	π?	π?
5:	QΦ 2∧Φ 2 = max(QΦ 1 , QΦ 2 ) . construct the optimal composed Q-function using Theorem 2
6:	Bφ∧ — COnstruCtProduCtBuffer(Bφι, Bφ2)
π?
7:	%∧φ2 - EXtraCtPOlΛcyQφ2∧φ22, Bφ∧)
8:	return ^2∧φ2
6 constructs the product replay buffer for policy extraction. This step is necessary because each
Bφi contains state of form (s, qi), i ∈ {1, 2} whereas the composed policy takes state (s, q1, q2)
as input (as in Definition 4). Therefore, we transform each experience ((s, qi), a, (s0, qi0), r) to
((s, qi, qj6=i), a, (s0, qi0, qj0 6=i), r) where qj6=i is chosen at random from the automaton states of Aφj
and qj0 6=i is calculated from Equation (6). The reward r will not be used in policy extraction as
the Q-function will not be updated. Step 7 extracts the optimal composed policy from the optimal
composed Q-function (this corresponds to running only the policy update step in the actor critic
algorithm).
6 Case Studies
We evaluate the our composition method in two environments. The first is a simple 2D grid world
environment that is used as for proof of concept and policy visualization. The second is a robot
manipulation environment.
6.1	Grid World
Consider an agent that navigates in a 8 × 10 grid world. Its MDP state space is S : X × Y where
X, y ∈ X, Y are its integer coordinates on the grid. The action space is A : [up, down, left, right,
stay]. The transition is such that for each action command, the agent follows that command with
probability 0.8 or chooses a random action with probability 0.2. We train the agent on two tasks,
φ1 = ♦r ∧ ♦g and φ2 = ♦b (same as in Example 1). The regions are defined by the predicates
r = (1 < X < 3) ∧ (1 < y < 3) and g = (4 < X < 6) ∧ (4 < y < 6). Because the
coordinates are integers, a and b define a point goal rather than regions. φ2 expresses a similar task
for b = (1 < X < 3) ∧ (6 < y < 8). Figure 1 shows the FSA for each task.
We apply standard tabular Q-learning Watkins (1989) on the FSA augmented MDP of this environ-
ment. For all experiments, we use a discount factor of 0.95, learning rate of 0.1, episode horizon of
200 steps, a random exploration policy and a total number of 2000 update steps which is enough to
reach convergence (learning curve is not presented here as it is not the focus of this paper).
Figure 2	(a) and (b) show the learned optimal policies extracted by πφ? (X, y, qφi ) =
argmaxQφ. (x, y,qφi, a). We plot π?, (x,y,qφj for each qφi and observe that each represents a
a
sub-policy whose goal is given by Equation 7.
Figure 2 (c) shows the composed policy of φ∧ = φ1 ∧ φ2 using Theorem 2. It can be observed that
the composed policy is able to act optimally in terms maximizing the expected sum of discounted
rewards given by Equation (12). Following the composed policy and transitioning the FSA in Fig-
ure 1 (b) will in fact satisfy φ∧ (-AN D-). As discussed in the previous section, if the -OR-
task is desired, following the same composed policy and terminate at any of the states qφ∧,2, qφ∧,4,
qφ∧,5, qφ∧,6, qφ∧,f will satisfy φ∨ = φ1 ∨ φ2.
7
Under review as a conference paper at ICLR 2019
Figure 2 : Policies for (a) φ1 = ♦a ∧ ♦b. (b) φ2 = ♦c. (c) φ∧ = φ1 ∧ φ2 . The agent moves in a 8 × 10
gridworld with 3 labeled regions. The agent has actions [up, down, left, right, stay] where the directional actions
are represented by arrows, stay is represented by the blue dot.
Figure 3 : The FSA for (a):φtraverse = Q(ψr ∧ Q(ψg ∧ Qψb)) b φinterrupt = (ψhand-insight ⇒
Qψh) U ψb. The subscripts φι and φ2 are dropped for clarity.
6.2	Robotic Manipulation
In this sub-section, we test our method on a more complex manipulation task.
6.2.1	Experiment Setup
Figure 4 (a) presents our experiment setup. Our policy controls the 7 degree-of-freedom joint veloc-
ities of the right arm of a Baxter robot. In front of the robot are three circular regions (red, green,
8
Under review as a conference paper at ICLR 2019
Figure 4 : (a): The upper figure shows our simulation environment in V-REP Rohmer et al. (2013) and the
lower shows the corresponding experimental environment. (b) An execution trace of policy πφ∧ where φ∧ =
φtraverse ∧ φinterrupt .
blue plates) and it has to learn to traverse in user specified ways. The positions of the plates are
tracked by motion capture systems and thus fully observable. In addition, we also track the position
of one of the user’s hands (by wearing a glove with trackers attached). Our MDP state space is 22
dimensional that includes 7 joint angles, xyz positions of the three regions (denoted by pr , pg , pb),
the user’s hand (ph) and the robot’s end-effector (pee). State and action spaces are continuous in
this case.
We define the following predicates
1.	ψi = ||pi - pee|| < , i ∈ {r, g, b, h} where is a threshold which we set to be 5
centimeters. ψi constrains the relative distance between the robot’s end-effector and the
selected object.
2.	ψhand-insight = (Xmin < px < xmax) ∧ (ymin < py < ymax) ∧ (zmin < pz < zmax).
This predicate evaluates to true if the user’s hand appears in the cubic region defined by
[xmin, xmax, ymin , ymax , zmin, zmax]. In this experiment, we take this region to be 40
centimeters above the table (length and width the same as the table).
We test our method on the following composition task
1.
2.
3.
φtraverse = 6(ψr ∧Q(ψg ∧ Qψb))
Description: traverse the three regions in the order of red, green, blue.
φinterrupt = (ψhand_in.Sight ⇒ Qψh) U ψb
Description: before reaching the blue region, if the user’s hand appears in sight, then even-
tually reach for the user’s hand and the blue region, otherwise just reach for the blue region.
φ∧ = φtr
averSe ∧ φinterrupt
Description: conjunction of the first two tasks.
The FSAs for φ1 and φ2 are presented in Figure 3 . The FSA for φ∧ (14 nodes and 72 edges) is not
presented here due to space constraints.
6.2.2	Implementation Details
Our policy and Q-function are represented by a neural network (3 layers each with 300, 200, 100
ReLU units). For tasks φtraverSe and φinterrupt , the input state space is 23 dimensional (22 continu-
ous dimensional MDP state and 1 discrete dimension for the automaton state). For task φ∧ , the state
space is 24 dimensional (2 discrete dimensions for automaton states of φtraverSe and φinterrupt).
We use soft Q-learning (SQL) (Haarnoja et al., 2017) to learn the optimal policies and Q-functions
for φtraverSe and φinterrupt and Theorem 2 to compose the Q-functions. The policy for task φ∧ is
extracted from the composite Q-function by running the same algorithm on already collected expe-
rience without updating the Q-function (similar procedure as (Haarnoja et al., 2018)). After each
9
Under review as a conference paper at ICLR 2019
Figure 5 : (left) Learning curve of discounted return (discount factor 0.98). The plot shows mean and one
standard deviation calculated over 5 episodes. (right) Mean and standard deviation of episode length (steps)
averaged over 5 episodes(a smaller number means faster accomplishment of the task).
episode, the joint angles, the FSA state, the position of the plates as well as the position of the hand
(represented by the yellow sphere in Figure 4 (a)) are randomly reset (within certain boundaries) to
ensure generalization across different task configurations. The robot is controlled at 20 Hz. Each
episode is 100 time-steps (about 5 seconds). The episode restarts if the final automaton state qf is
reached. During training, we perform 100 policy and Q-function updates every 5 episodes of explo-
ration. All of our training is performed in simulation and for this set of tasks, the policy is able to
transfer to the real robot without further fine-tuning.
6.2.3	Results and Discussion
In Figure 5 (left), we report the discounted return as a function of policy update steps for task φ∧ .
5 evaluation episodes are collected after each set policy updates to calculate the performance statis-
tics. As comparison, we learn the same task using SQL with FSA augmented MDP. We can see that
our composition method takes less update steps to reach a policy that achieves higher returns with
lower variance than the policy obtained from learning. Figure 5 (right) shows the episode length
as a function of policy update (upper bound clipped at 100 steps). As mentioned in the previous
section, a shorter episode length indicates faster accomplishment of the task. It can be observed that
both the composition and learning method result in high variances likely due to the randomized task
configuration (some plate/joint/hand configurations make the task easier to accomplish than oth-
ers). However, the policy obtained from composition achieves a noticeable decrease in the average
episode length.
It is important to note that the wall time for learning a policy is significantly longer than that from
composition. For robotic tasks with relatively simple policy representations (feed-forward neu-
ral networks), learning time is dominated by the time used to collect experiences and the average
episode length (recall that we update the policy 100 times with each 5 episodes of exploration). Since
skill composition uses already collected experience, obtaining a policy can be much faster. Table
1 shows the mean training time and standard deviation (over 5 random seeds) for each task (tasks
φtraverse, φinterrupt and φ∧(learned) are trained for 80K policy updates. φ∧(composed) is trained
for 40K policy updates). In general, training time is shorter for tasks with higher episodic success
rate and shorter episode length. We also show the task success rate evaluated on the real robot over
20 evaluation trials. Task success is evaluated by calculating the robustness of the trajectories result-
ing from executing each policy. A robustness of greater than 0 evaluates to success and vice versa.
πφ∧ (learned) fails to complete the task even though a convergence is reached during training. This
is likely due to the large FSA of φ∧ with complex per-step reward (Dφq in Equation (7)) which makes
learning difficult. Figure 4 (b) shows an evaluation run of the composed policy for task φ∧ .
7 Conclusion
We provide a technique that takes advantage of the product of finite state automata to perform
deterministic skill composition. Our method is able to synthesize optimal composite policies for
-AN D- and -OR- tasks. We provide theoretical results on our method and show its effective-
ness on a grid world simulation and a real world robotic task. For future work, we will adapt our
10
Under review as a conference paper at ICLR 2019
Table 1: Results on Training and Evaluation Performance
φtraverse	φinterrupt	Φ∧ (learned)	φ∧ (Composed)
Mean training time/std (Hour)	4.2/0.1	4.7/0.3	5.8/0.5	0.8/0.04
Mean task success rate/std	85.4%/2.3%	80.2% / 2.5%	3.3%/ 1%	68.9%/ 3%
Mean discounted return/std	-13.2/2.3	-10.3/3.5	-40.6/10.5	-20.1/8.3
method to the more general case of task-space transfer - given a library of optimal policies (Q-
functions) that each satisfies its own specification, construct a policy that satisfies a specification
that’s an arbitrary (temporal) logical combination of the constituent specifications.
References
Derya Aksaray, Austin Jones, Zhaodan Kong, Mac Schwager, and Calin Belta. Q-learning for robust
satisfaction of signal temporal logic specifications. In Decision and Control (CDC), 2016 IEEE
55th Conference on, pp. 6565-6570. IEEE, 2016.
Christel Baier and JooSt-Pieter Katoen. PrinCiPleS of model CheCking. MIT press, 2008.
Calin Belta, Boyan Yordanov, and Ebru Aydin Gol. Formal MethodS for DiSCrete-Time DynamiCaI
Systems. Springer, 2017.
Marco Da Silva, Fredo Durand, and Jovan Popovic. Linear bellman combination for control of
character animation. ACm transactions on graphics (tog), 28(3):82, 2009.
Georgios E Fainekos, Hadas Kress-Gazit, and George J Pappas. Hybrid controllers for path plan-
ning: A temporal logic approach. In DeCiSiOn and Control, 2005 and 2005 EUrOPean Control
Conference. CDC-ECC'05. 44th IEEE COnference on, pp. 4885^890. IEEE, 2005.
Georgios E Fainekos, Savvas G Loizou, and George J Pappas. Translating temporal logic to con-
troller specifications. In DeCiSiOn and Control, 2006 45th IEEE COnferenCe on, pp. 899-904.
IEEE, 2006.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with
deep energy-based policies. In ICML, 2017.
Tuomas Haarnoja, Vitchyr Pong, Aurick Zhou, Murtaza Dalal, Pieter Abbeel, and Sergey
Levine. Composable deep reinforcement learning for robotic manipulation. arXiv PrePrint
arXiv:1803.06773, 2018.
Tejas D. Kulkarni, Karthik R. Narasimhan, Ardavan Saeedi, and Joshua B. Tenenbaum. Hierarchi-
cal Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation.
PrePrint arXiv:1604.06057, 2016.
Xiao Li, Yao Ma, and Calin Belta. Automata guided reinforcement learning with demonstrations.
arXiv PrePrint arXiv:1809.06305, 2018.
Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel van de Panne. Deepmimic: Example-
guided deep reinforcement learning of physics-based character skills. CoRR, abs/1804.02717,
2018.
Eric Rohmer, Surya P. N. Singh, and Marc Freese. V-rep: A versatile and scalable robot simulation
framework. 2013 IEEE/RSJ International COnferenCe on Intelligent Robots and Systems, pp.
1321-1326, 2013.
John Schulman, Pieter Abbeel, and Xi Chen. Equivalence between policy gradients and soft q-
learning. CoRR, abs/1704.06440, 2017.
Paulo Tabuada and George J Pappas. Linear temporal logic control of linear systems. IEEE
TranSaCtiOnS on AUtOmatiC Control, 2004.
11
Under review as a conference paper at ICLR 2019
EmanUel Todorov. Compositionality of optimal control laws. In Advances in Neural Information
Processing Systems, pp.1856-1864, 2009.
Benjamin van Niekerk, Steven James, Adam Christopher Earle, and Benjamin Rosman. Will it
blend? composing value functions in reinforcement learning. CoRR, abs/1807.04439, 2018.
C Vasile. GithUb repository, 2017.
Christopher John Cornish Hellaby Watkins. Learning From Delayed Rewards. PhD thesis, King,s
College, Cambridge, England, 1989.
Yuke Zhu, Daniel Gordon, Eric Kolve, Dieter Fox, Li Fei-Fei, Abhinav Gupta, Roozbeh Mottaghi,
and Ali Farhadi. Visual semantic planning using deep successor representations. arXiv PrePrint
ArXiv:1705.08080, pp. 1-13, 2017.
12