Under review as a conference paper at ICLR 2019
An Analysis of Composite Neural Network
Performance from Function Composition Per-
SPECTIVE
Anonymous authors
Paper under double-blind review
Ab stract
This work investigates the performance of a composite neural network, which
is composed of pre-trained neural network models and non-instantiated neural
network models, connected to form a rooted directed graph. A pre-trained neural
network model is generally a well trained neural network model targeted for
a specific function. The advantages of adopting such a pre-trained model in a
composite neural network are two folds. One is to benefit from other’s intelligence
and diligence, and the other is saving the efforts in data preparation and resources
and time in training. However, the overall performance of composite neural network
is still not clear. In this work, we prove that a composite neural network, with
high probability, performs better than any of its pre-trained components under
certain assumptions. In addition, if an extra pre-trained component is added to a
composite network, with high probability the overall performance will be improved.
In the empirical evaluations, distinctively different applications support the above
findings.
1	Introduction
Deep learning has been a great success in dealing with natural signals, e.g., images and voices, as well
as artifact signals, e.g., nature language, while it is still in the early stage in handling sophisticated
social and natural applications shaped by very diverse factors (e.g., stock market prediction), or
resulted from complicated processes (e.g., pollution level prediction). One of distinctive features
of the complicated applications is their applicable data sources are boundless. Consequently, their
solutions need frequent revisions. Although neural networks can approximate arbitrary functions as
close as possible (Hornik, 1991), the major reason for not existing such competent neural networks for
those complicated applications is their problems are hardly fully understood and their applicable data
sources cannot be identified all at once. By far the best practice is the developers pick a seemly neural
network with available data to hope for the best. The apparent drawbacks, besides the performance,
are the lack of flexibility in new data source emergence, better problem decomposition, and the
opportunity of employing proven efforts from others. On the other hand, some adopts a composition
of several neural network models, based on function composition using domain knowledge.
An emerging trend of deep learning solution development is to employ well crafted pre-trained neural
networks (i.e., neural network models with instantiated weights), especially used as a component in a
composited neural network model. Most popular pre-trained neural network models are well fine
tuned with adequate training data, and made available to the public, either free or as a commercial
product. During the training phase of composite neural network, the weights of pre-trained models
are frozen to maintain its good quality and save the training time, while the weights of their outgoing
edges are trainable. In some cases as in the transfer learning, the weights of pre-trained neural
network are used as initial values in the training phase of composite neural network. It is intuitive that
a composite neural network should perform better than any of its components. The ensemble learning
(Freund & Schapire, 1997; Zhou, 2012) and the transfer learning (Galanti et al., 2016) have great
success and are popular when pre-trained models are considered. However, the following example
shows some aspects missed by these two methods, and requests for more complicated composite
function.
1
Under review as a conference paper at ICLR 2019
Example 1. Assume there is a set of locations indexed as X = {(0, 0), (0, 1), (1, 0), (1, 0)}
with the corresponding values Y = (0, 1, 1, 0). Obviously, the observed function is the XOR
(Goodfellow et al., 2016). Now consider three models: f1(x1, x2) := x1, f2(x1, x2) := x2, and
f3(x1, x2) := x1x2. Their corresponding output vectors are (0, 0, 1, 1), (0, 1, 0, 1), (0, 0, 0, 1) with
bit-wise accuracy 50%, 50%, 25%, respectively. This means that the AdaBoosting algorithm will
exclude fι and f2 in the ensemble since their coefficients are ɪ ln 1-0%% = 0. On the other hand, in
the transfer learning, f3 is fine-tuned by applying the gradient descent method with respect to L2 loss
on wf3 = wx1 x2 to transfer the source task distribution to that of the target task. The result comes to
w = 0, and f3 is excluded. Now consider g1(x1, x2) = α1f1 + α2f2 and apply the back-propagation
method with respect to the L2 loss. The results are αι = α2 = 3, with loss ∣. If further define
g2(x1, x2) = w1g1 +w2f3, the back-propagation yields g2 = 3g1 - 2f3 = x1 +x2 - 2x1x2 with the
output (0, 1, 1, 0). The final g2 computes Y with loss 0. This example shows the power of composite
function.
Composite Neural Network. In the transfer learning, how to overcome the negative transfer (a
phenomenon of a pre-trained model has negative impact on the target task) is an important issue (Seah
et al., 2013). In the ensemble learning, it is well known that the adding more pre-trained models, it is
not always true to have the better accuracy of the ensemble (Zhou et al., 2002). Furthermore, Opitz &
Maclin (1999) pointed that the ensemble by boosting having less accuracy than a single pre-trained
model often happens for neural networks. In the unsupervised learning context, some experimental
research concludes that although layer-wise pre-training can be significantly helpful, on average it
is slightly harmful (Goodfellow et al., 2016). These empirical evidences suggest that in spite of
the success of the ensemble learning and the transfer learning, the conditions that composite neural
network can perform better is unclear, especially in the deep neural networks training process. The
topology of a composite neural network can be represented as a rooted directed graph. For instance,
an ensemble learning can be represented as 1-level graph, while a composite neural network with
several pre-trained models that each is designed to solve a certain problem corresponds to a more
complicated graph. It is desired to discover a mathematical theory, in addition to employing domain
knowledge, to construct a composite neural network with guaranteed overall performance. In this
work, we investigate the mathematical theory to ensure the overall performance of a composite neural
network is better than any a pre-trained component, regardless the way of composition, to allow deep
learning application developer great freedom in constructing a high performance composite neural
network.
Contributions. In this work, we proved that a composite neural network with high probability
performs better than any of its pre-trained components under certain assumptions. In addition, if
extra pre-trained component is added into a composite network, with high probability the overall
performance will be improved. In the empirical evaluations, distinctively different applications
support the above findings.
2	Preliminaries
In this Section, we introduce some notations and definitions about composite neural network. Pa-
rameters N,K, d, dj, dj1, and dj2 are positive integers. Denote {1, ..., K} as [K] and [K] ∪ {0}
as [K]+ . Let σ : R → R be a differentiable activation function, such as the Logistic function
σ(z) = 1/(1 + e-z) and the hyperbolic tangent σ(z) = (ez - e-z)/(ez + e-z). For simplicity of
notation, we sometimes abuse σ as a vector value function. A typical one hidden layer neural network
can be formally presented as w1,1σ Pid=1 w0,ixi + w0,0 + w1,0.We abbreviate it as fσ,W(x),
where W is the matrix defined byw1,1, w1,0, ..., w0,1, w0,0. Recursively applying this representation
can obtain the neural network with more hidden layers. If there is no ambiguity on the activation
function, then it can be skipped as fW(x). Now assume a set of neural networks {fWj (xj)}jK=1 is
given, where Wj is the real number matrix defining the neural network fWj : Rdj1 ×dj2 → Rdj , and
xj ∈ Rdj1 ×dj2 is the input matrix of the j th neural network. For different fWj , the corresponding
dj, dj1 and dj2 can be different. For each j ∈ [K], let Dj = {(x(ji), yj(i)) ∈ R(dj1 ×dj2)×dj }iN=1
be a set of labeled data (for the jth neural network). For each i ∈ [N], let x(i) = (x(1i) , . . . , x(Ki)),
y(i) = (y1(i), . . . ,yK(i)), andD = {(x(i), y(i))}iN=1.
2
Under review as a conference paper at ICLR 2019
For a pre-trained model (component), we mean Wj is fixed after its training process, and then
we denote fWj as fj for simplicity. On the other hand, a component fWj is non-instantiated
means Wj is still free. A deep feedforward neural network is a hierarchical acyclic graph, i.e.
a directed tree. In this viewpoint, a feedforward neural network can be presented as a series of
function compositions. For given {fWj (xj)}jK=1, we assume θj ∈ Rdj , j ∈ [K], which make the
product θj fWj (xj) is well-defined. Denote f0 as the constant function 1, then the liner combination
with a bias is defined as as Θ(f1, ..., fK) = Pj∈[K]+ θjfj (xj). Hence, an L layers of neural
network can be denoted as ㊀⑷◦ σ ◦…。㊀⑼(x). A composite neural network defined by
components fWj (xj) can be designed as an directed tree. For instance, a composite neural network
σ2 (θ1,0 + θ1,1f4(x4) + θ1,2σ1(θ0,0 +θ0,1f1(x1) +θ0,2fW2(x2) + θ0,3f3(x3)))canbedenotedas
σ2 ◦ Θ1 (f4, σ1 ◦ Θ0(f1, fW2 , f3)), where f1 and f3 are pre-trained and fW2 is non-instantiated.
Note that in this work Dj is the default training data of component fj of composite neural network,
but Dj can be different from the training data deciding the frozen weights in the pre-trained fj .
Let h~, bi be the standard inner product of a and b, and || ∙ || be the corresponding norm. For a
composite neural network, the training algorithm is the gradient descent back-propagation algorithm
and the loss function is the L2-norm of the difference vector. In particular, for a composite neural
network gθ~ the total loss on the data set D is
L~(x； g~ = h~θ(X) - y,~~(X) - yi = ||g~(X) - y||2	(I)
This in fact is PN=I (g(x(i)) - y(i))2. By the definition of g~(∙), this total loss in fact depends on the
given data X, the components defined by {Θj}jK=1, the output activation σ, and the weight vector w~.
Similarly, let L(fj (Xj)) be the loss function of a single component fi. Our goal is to find a feasible θ
s.t. Lθ~ (X; g) < minj∈[K] L(fj (Xj )).
3 Problem Settings and Results Overview
The problems considered in this work are as follows:
P1. What are the conditions that the pre-trained components must satisfy so that them can strictly
improve the accuracy of the whole composition?
P2. Will more pre-trained components improve the accuracy of the whole composition?
Let f~j be the output vector of the jth pre-trained component, and BK be the set of unit vectors in RK .
A1. Linearly Independent components (LIC) Assumption:
∀t ∈ [K],轼βj} ⊂ R, s.t. ft = Pj∈[κ]∖{t} βjfj.
A2. No Perfect component (NPC) Assumption:
minj∙∈[κ] {Pi∈[N] fj (Xji)) - y(i)} > e*, where e* > 0 is constant.
Our results are as follows:
Theorem 1. Assume the set of components {fj (Xj)}jK=1 satisfies LIC. Let g be Θ(f1, ..., fK). With
probability at least 1 一 ∏KN ,there is a vector θ ∈ RK \ BK s.t. L~ (x; g) < min7∙∈ [κ]{L(fj (Xj))}.
Theorem 2. Assume the set of pre-trained components {fj (Xj )}jK=1 satisfies both NPC and LIC,
and g be σ ◦ Θ(fι,..., fκ). Then with probability at least 1 一 ∏KN there exists W s.t. Lw (x; g) <
minj∈[K] L(fj(Xj)).
Theorem 3. Assume the set of components {fj (Xj)}jK=1 satisfies LIC. Let gK-1 = Θ(f1, ...fK-1)
and gκ = Θ(fι,…fκ). With probability at least 1 — ∏^ , there is a vector W ∈ RK \ BRK s.t.
Lw~ (X; gK ) < Lw~ (X; gK-1 ).
Theorem 1, and 2 together answer Problem P1, and Theorem 3 answers Problem P2.
3
Under review as a conference paper at ICLR 2019
4	Related Work
Our framework is related but not the same with the models such as transfer learning. (Erhan et al.,
2010; Kandaswamy et al., 2014; Yao & Doretto, 2010) and ensemble leaning (Zhou, 2012).
Transfer Learning. Typically transfer learning deals with two data sets with different distributions,
source and target domains. A neural network, such as an auto-encoder, is trained with source domain
data and corresponding task, and then part of its weights are taken out and plugged into other neural
network, which will be trained with target domain data and task. The transplanted weights can be
kept fixed during the consequent steps or trainable for the fine-tune purpose (Erhan et al., 2010). For
multi-source transfer, algorithms of boosting based are studied in the paper (Yao & Doretto, 2010).
Kandaswamy et al. (Kandaswamy et al., 2014) proposed a method of cascading several pre-trained
layers to improve the performance. Transfer learning is considered as a special case of composite
neural network that the transfered knowledge can be viewed as a pre-trained component.
Ensemble (Bagging and Boosting). Since the Bagging needs to group data by sampling and the
Boosting needs to tune the probability of data (Zhou et al., 2002), these frameworks are different
from composite neural network. However, there are fine research results revealing many properties
r∙	♦	.	/-ɪ-ʌ ⅛	1 ♦ C ¥	1	r-∖rxrx A y 1	1 1	.	1 CCCC Γ-rt	.	1 CCCc∖ I -
for accuracy improvement (Dzeroski & Zenko, 2004; Gashler et al., 2008; ZhoU et al., 2002). For
example, it is known that in the ensemble framework, low diversity between members can be harmful
to the accuracy of their ensemble (Dzeroski & Zenko, 2004; Gashler et al., 2008). In this work, We
consider neural network training, but not data processing.
Ensemble (Stacking). Among the ensemble methods, the stacking is closely related to our frame-
work. The idea of stacked generalization (Wolpert, 1992), in Wolpert’s terminology, is to combine
two levels of generalizers. The original data are taken by several level 0 generalizers, then their
outputs are concatenated as an input vector to the level 1 generalizer. According to the empirical
study of Ting and Witten (Ting & Witten, 1999), the probability of the outputs of level 0, instead of
their values, is critical to accuracy. Besides, multi-linear regression is the best level 1 generalizer,
and non-negative weights restriction is necessary for regression problem while not for classification
problem. In (Breiman, 1996), it restricts non-negative combination weights to prevent from poor
generalization error and concludes the restriction of the sum of weights equals to 1 is not necessary
(Breiman, 1996). In (Hashem, 1997), Hashem showed that linear dependence of components could
be, but not always, harmful to ensemble accuracy, while in our work, it allows a mix of pre-defined
and undefined components as well as negative weights to provide flexibility in solution design.
Recently Proposed Frameworks. In You et al. (2017), Shan You et al. proposed a student-teacher
framework where the outputs of pre-trained teachers are averaged as the knowledge for the student
network. A test time combination of multiple trained predictors was proposed by Kim, Tompkin,
and Richardt In Kim et al. (2017), that the combination weights are decided during test time. In
above frameworks, the usage of pre-trained neural networks generally improves the accuracy of their
combination.
5	Theoretical Analysis
This section provides analyses of the loss function of composite neural network with the introduction
of pre-trained components. For the complete proofs, please refer to Supplementary Material. Observe
that for given pre-trained components {fj}jK=1, a composite neural network can be defined recursively
by postorder subtrees search. For instance, σ2 ◦ Θ1 (f4, σ1 ◦ Θ0(f1, f2, f3)) can be presented as
σ2 ◦ Θ1(f4, g1), and g1 = σ1 ◦ Θ0(f1, f2, f3). Without loss of generality, we assume dj = d = 1
for all j ∈ [K] in the following proofs. We denote fj the vector (fj(x(1)),…，fj(X(N))), as the
sequence of fj during the training phase. Similarly, y := (y(1),…,y(N)). Let ej- be an unit vector
in the standard basis of RK for each j ∈ [K], i.e. ~ι = (1,0,…,0) and e2 = (0,1,0,…,0), etc.
Let BK be the set containing all these standard unit-length basis of RK .
Theorem 1.	Assume the set of components {fj (Xj)}jK=1 satisfies LIC. Let g be Θ(f1, ..., fK). With
probability at least 1 一 ∏∣N ,there is a vector θ ∈ RK \ BK s.t. L~ (x; g) < min7∙∈[K] {L(fj (Xj))}.
4
Under review as a conference paper at ICLR 2019
Proof. (Proof Sketch) The whole proof is split to Lemma 5.1,5.2,5.3. Note that g(∙) is the linear
combination of θ~ and {fj (xj)}jK=1. It is well known (Friedman et al., 2001) that to search the
minimizer θ for Lθ~, i.e. to solve a least square error problem, is equivalent to find an inverse matrix
defined by {fj (xj)}jK=1. Since {fj (xj)}jK=1 satisfy LIC, the inverse matrix can be written down
concretely, which proves the existence. Furthermore, if this solved minimizer ~ is not eS for some
S ∈ [K] then the g~* has lower loss than fs. Lemma 5.3 argues that the probability of ~ = es is at
most the probability of the event hf - ~y, fi = 0, where f is uniformly taken from the vector set of
the same length of f - y.	□
The statements of Lemmas needed by previous Theorem are as follows.
Lemma 5.1. There exists θ~∈ RK+1 s.t. Lθ~ x; Θ(0)(f1, ...fK) ≤ minj ∈[K]+ {L(fj (xj))}.
This Lemma deals with the existence of the solution of the inequality. But our goal is to find a
solution such that the loss is strictly less than any pre-trained component.
Lemma 5.2. Denote IL~ the indicator variable for the event that at least one of	~ej	∈	BRK	is the
minimizer of L~. Then Pr {Il~ = 1} < ∏kn, i.e. Pr {Il~ = 0} ≥ 1 - ∏KN.
Lemma 5.3. Define F(y~, L(f)) = f~∈ RN : f~ - ~y = Lf for given ~y and	f~.	Then we have
Pr~∈F(~,L(f)) nhf- y,fi =0O < πe1N.
The above Lemmas prove Theorem 1. The following corollary is the closed from of the optimal
weights.
Corollary 5.1. The closed form of the minimizer is:
[θt]t∈[K]+ = hhf~s,f~tiis-,t∈[K]+ × hhf~s,~yiis∈[K]+.
In the following, we deal with σ ◦ Θ(f1, ..., fK ) and Θ1 ◦ σ ◦ Θ(f1, ..., fK ).
Theorem 2.	Assume the set of pre-trained components {fj (xj )}jK=1 satisfies both NPC and LIC,
K
and g be σ ◦ Θ(fι,..., fκ). Then with probability at least 1 一 ∏^ there exists θ s.t. L~ (x; g) <
minj∈[K] L(fj(xj)).
Proof. (Proof Sketch) The whole proof is split to Lemma 5.4,5.5, and 5.6. The idea is to find an
interval in the domain of σ such that the output can approximate linear function as well as possible.
Then in this interval, the activation σ can approximate any given pre-trained component. However,
under the assumptions LIC and NPC the gradient of the loss L is not zero with high probability. Since
the training is based on the gradient descent algorithm, this none-zero gradient leads the direction of
updating process to obtain a lower loss.	□
Lemma 5.4. Let N, K and j ∈ [K] be fixed. For small enough , there exists θ ∈ ZF,1, and
0 < α ∈ R s.t. Iσ ◦ θ(0)(f1, ..., fK) - jx) | < C.
Lemma 5.5. AssumeNPCholds with e* > 0. If θe*/3 satisfies ∣σ ◦ Θ(0)(f1,..., fκ)(x) — fj(x)| <
*
3N for any j ∈ [K]+, then V~L(θe*/3) = 0.
Lemma 5.6. If	θe*/3	makes	V~L(θe*/3)	=	0,	then there exist θ s.t.	L~ (x;	g)	<
minj∈[K]+ L(fj(xj)).
Now we consider the difference of losses ofσ ◦ Θ0(f1, ..., fK ) and σ ◦ Θ(f1, ..., fK -1).
Theorem 3.	Assume the set of components {fj (xj)}jK=1 satisfies LIC. Let gK -1 = Θ(f1, ...fK -1)
and gκ = Θ(fι,…fκ). With probability at least 1 — ∏K , there is a vector θ ∈ RK \ BRK s.t.
L~(x； gκ) < L~(x； gκ-ι).
5
Under review as a conference paper at ICLR 2019
Table 1: Validation Error of Image Classification
	ResNet50	SIFT	composite
number of parameter	25636712	2884200	3000
Validation Error (%)	70.1184	13.1252	71.5079
Proof. (Proof Sketch) The idea is directly solve the inequality for the case of K = 2, and then
generalize the result to larger K.	□
The following provides a generalized error bound for a composite neural network.
Theorem 4. Assume pre-trained components {fj}jK=1 satisfy LIC and NPC. Let {GE (fj)}jK=1 be
corresponding generalization errors of {f }j=ι, and ㊀⑷◦ o(l)◦•••◦。⑴◦㊀⑼⑴，…，fκ) be
the composite neural network. Denote the generalization error, E{L(Θ(l) ◦ σg ◦ ∙∙∙ ◦ σ(i) ◦
Θ(0)(f1, ..., fK ))}, of the composite neural network as E{LΘ,f1,...,fK}. Then with high probability,
there exist a setting of {㊀")，…，㊀：。)} such that E{Lθ,外,…,左} ≤ Θ")(GE(fι),…GE(fκ)).
Proof. (Proof Sketch) We apply the idea similar to Kawaguchi (2016): the exception of non-liner
activations is same with the exception of liner activations. Previous theorems provide that with high
probability there exists the solution of Θ(i), ∀i ∈ [L]+ s.t. each Θ(i)+1σΘ(i) approximates a degree
one polynomial AΘ(i)+1σΘ(i),1 as well as possible. If the weights are obey the normal distribution,
then E{Lθ,fι,...,fκ }≤ Θ*2)(GE (fι),...GE (fκ)).	□
6	Empirical Studies
This section is to numerically verify the performance of composite network for two distinctively dif-
ferent applications, image classification and PM2.5 prediction. For image classification, we examined
two pre-trained components, the ResNet50 (He et al., 2016) from Keras and the SIFT algorithm(Lowe,
1999) from OpenCV, running on the benchmark of ImageNet competition(Russakovsky et al., 2015).
For PM2.5 prediction, we implemented several models running on the open data of local weather
bureau and environment protection agency to predict the PM2.5 level in the future hours.
6.1	ImageNet Classification
We chose Resnet50 as the pre-trained baseline model and the SIFT model as an auxiliary model to
form a composite neural network to validate the proposed theory. The experiments are conducted on
the 1000-class single-label classification task of the ImageNet dataset, which has been a well received
benchmark for image classification applications. A reason to choose the SIFT (Scale-Invariant Feature
Transform) algorithm is that its function is very different from ResNet and it is interesting to see if
the performance of ResNet50 can be improved as predicted from our theory.
We trained the SIFT model using the images of ImageNet, and directed the output to a CNN to extract
useful features before merging with ResNet50 output. In the composite model, the softmax functions
of both ResNet50 and SIFT model are removed that the outputs of length 1000 of both models are
merged before the final softmax stage. During the training process of composite network, the weights
of ResNet50 and SIFT model are fixed, and only the connecting weights and bias are trained.
The ResNet50 was from He et al. that its Top-1 accuracy in our context was lower than reported in
(He et al., 2016) since we did not do any fine tuning and data preprocessing. In the Figure 1, it shows
the composite network has higher accuracy than ResNet50 during almost the complete testing run.
Table 1 shows the same result that the composite network performs better too. The experiment results
support the claims of this work that a composite network performs better than any of its components,
and more components work better than less components.
6
Under review as a conference paper at ICLR 2019
Figure 1: Image Classification Validation Accuracy
6.2	PM2.5 Prediction
The PM2.5 prediction problem is to forecast the particle density of fine atmospheric matter with
the diameter at most 2.5 μm (PM2.5) in the future hours, mainly, for the next 12, 24, 48, 72
hours. The datasets used are open data provided by two sources including Environmental Protection
Administration (EPA)1 , and Center Weather Bureau (CWB)2 . The EPA dataset contains 21 observed
features, including the speed and direction of wind, temperature, relative humidity, PM2.5 and PM10
density, etc., from 18 monitoring stations, with one record per hour. The CWB has seventy monitoring
stations, one record per 6 hours, containing 26 features, such as temperature, dew point, precipitation,
wind speed and direction, etc. We partitioned the observed area into a grid of 1140 km2 with 1 km×1
km blocks and aligned the both dataset into one-hour period. We called the two datasets as air quality
and weather condition dataset.
We selected ConvLSTM (Convolution LSTM) and FNN (fully connected neural network) as the
components used in this experiment. The reason to select ConvLSTM is that the dispersion of PM2.5
is both spatially and temporally dependent and ConvLSTM is considered capable of catching the
dependency, and FNN is a fundamental neural network that acts as the auxiliary component in the
experiment.
The prediction models were trained with the data of 2014 and 2015 years, then the 2016 data was
used for testing. We considered two function compositions, the linear combination Θ and the Logistic
function σ1 (as Theorem 2), to combine the two components to examine the applicability of the
proposed theorems.
We trained and tested both ConvLSTM and FNN using air quality dataset (Dataset A) and weather
condition dataset (Dataset B) separately as the baselines (denoted as f1, f2, f3 and f4) and their
training error and testing error in MSE are list in the first part of Table 2. Then we composited
FNNs using Dataset A and Dataset B, each FNN can be pre-trained (denoted as x) or non-instantiated
(denoted as o). In addition, we used both linear and Sigmoid activation functions. As a result, we had
eight combinations, as list in the part two. We treated ConvLSTM in the same way and the outcomes
were in the part 3. Finally, we composited using one FNN and one ConvLSTM that each was the
best in their category, and the resulting composite network was a tree of depth 2. For instance, the
candidate of COnVLSTM of part 4 for 12 hours prediction was the 4th row (i.e., Θ(f.,ff)) of part 3.
Their training and testing errors in MSE were listed in the part 4.
From the empirical study results, it shows mostly the proposed theorems are followed. While the
composite networks with all pre-trained components may not perform better than others in their
category, (which is not a surprise), what we expect to see is after adding a new component, the
composite network has improVement oVer the preVious one. For example, the σ ◦ Θ(f3× , f4× ) has
strictly better accuracy than both f3 and f4 for all future predictions. Another example is the
NEXT 48 hr, σ ◦ Θ(C×, F×) also has strictly better accuracy than both C = σ ◦ Θ(f,,ff) and
F = σ ◦ Θ(f ,f4).
1https://opendata.epa.goV.tw/Home
2http://opendata.cwb.goV.tw/index
7
Under review as a conference paper at ICLR 2019
Table 2: Training and Testing Errors of PM2.5 Prediction
Model (input)	Next 12 hr		Next 24 hr		Next 48 hr		Next 72 hr	
	TarinError	TestError	TarinError	TestError	TarinError	TestError	TarinError	TestError
f1: FNN-A	100.1812	92.8528	134.7095	118.6065	141.6287	136.8358	148.1807	143.9980
f2 : FNN-B	134.1137	120.0019	139.8016	128.5960	136.7693	134.9001	142.7637	140.8650
f3 : ConvLSTM-A	54.2775	88.8156	57.5677	111.9122	74.8937	129.7418	77.7394	132.8923
f4 : ConvLSTM-B	67.8625	118.4351	73.1519	125.6062	68.7069	137.0789	84.9656	138.6642
Θ(f1× ,f2× )	99.7005	F:90.0214	130.7800	115.9283	139.9744	F:132.4764	144.6826	F:137.8403
θ(f×,fD	95.6804	93.0173	120.3185	117.9781	134.3893	134.0270	139.6226	140.5209
Θ(f1,f×)	95.8110	93.1131	121.9737	117.7771	134.0676	135.2255	136.2009	144.0116
Θ(f1,f2)	101.1584	90.2671	126.6807	114.5264	132.6726	132.8069	139.2339	139.3322
σ ◦ Θ(f× ,f×)	102.7556	90.6280	133.1453	117.7397	135.9256	133.2544	145.1052	139.2513
σ ◦ Θ(f× ,f2)	98.1241	93.1098	127.4999	118.8107	135.1553	134.1469	137.7562	142.1778
σ ◦ Θ(f1f)	94.9931	91.4667	124.5461	117.7332	131.2684	135.1281	140.1604	144.5220
σ ◦ Θ(f1,f2)	98.2596	91.3646	124.4182	F: 114.2274	134.5078	132.8316	138.0456	139.8351
Θ(f3×,f4×)	27.1760	85.8922	49.1624	108.3157	37.2116	123.2186	60.6415	131.0565
Θ(f×,f4)	27.1519	81.7688	42.7932	104.2375	33.2831	C:110.4213	74.3055	C:110.9952
Θ(f3,f×)	27.4436	78.5360	44.3214	107.0898	31.4910	129.1829	68.4413	139.5661
Θ(f3,f4)	26.3063	C:70.8670	40.1879	100.8474	25.3312	119.1634	76.1782	120.6814
σ ◦ Θ(f×,f×)	28.3844	84.9029	43.2981	109.3709	31.2413	123.0041	63.7286	130.4122
σ ◦ Θ(f× f)	27.5981	80.7848	44.4197	98.1051	29.7649	111.6793	69.3182	117.0719
σ ◦ Θ(f3,f×)	26.4125	78.3990	42.3181	103.3361	30.4183	128.4138	64.4193	136.6043
σ ◦ Θ(f")	26.5131	75.5778	42.3912	C:94.6242	27.5812	112.8075	70.5132	117.6480
Θ(C×,F×)	26.6556	70.1159	34.6885	92.2737	29.6484	107.6833	52.0060	H:110.1283
Θ(C×,F o)	24.2349	67.4414	31.7202	90.7795	30.1328	H:105.1804	46.9994	111.2227
Θ(C°,F ×)	22.7651	75.9468	24.2132	96.3126	24.4488	114.1803	49.0663	117.2747
Θ(Co,Fo)	21.9103	68.0660	20.9072	91.9323	23.2868	112.8605	30.6875	113.6968
σ ◦ Θ(C×,F×)	26.5950	69.1897	34.7400	92.4715	29.9215	108.7482	52.3708	111.5474
σ ◦ Θ(C ×,Fo)	24.0223	H:66.4733	28.8401	H:90.7257	28.5033	108.8896	46.2711	110.1613
σ ◦ Θ(Co,F×)	22.4443	83.5953	22.5040	96.4027	28.4714	112.0727	35.3947	114.5947
σ ◦ Θ(Co,Fo)	38.4899	67.1819	17.6041	92.2343	33.9710	105.7977	40.2934	110.3585
Let f5 be non-instantiated CNN with future rain fall input.								
σ ◦ Θ(H×,f5)	24.1487	65.5776	30.6243	87.2777	55.9261	102.2878	50.5158	108.8087
σ ◦ Θ(H。,於)	58.0852	68.8111	22.8776	90.2324	39.0996	112.1639	36.0659	109.8240
Part 1: pre-trained components fi, i ∈ [4]. Part 2: composite fι and f2 by linear Θ(∙) or logistic σ ◦ Θ(∙); similar for Part3-5
× : un-trainable component, i.e. pre-trained. ◦: trainable component (original weights was deleted).
The best model of Part 2 (/Part 3) was assigned as composite model F (/C) which will be used in Part 4.
The best model of Part 4 was assigned as composite model H which will be used in Part 5.
7	Conclusion
In this work, we investigated the composite neural network with pre-trained components problem
and showed that the overall performance of a composite neural network is better than any of its
components, and more components perform better than less components. In addition, the developed
theory consider all differentiable activation functions.
While the proposed theory ensures the overall performance improvement, it is still not clear how
to decompose a complicated problem into components and how to construct them into a composite
neural network in order to have an acceptable performance. Another problem worth some thinking is
when the performance improvement will diminish (by power law or exponentially decay) even adding
more components. However, in the real world applications, the amount of data, data distribution and
data quality will highly affect the performance.
8
Under review as a conference paper at ICLR 2019
References
Leo Breiman. Stacked regressions. Machine learning, 24(1):49-64, 1996.
Saso Dzeroski and Bernard Zenko. Is combining classifiers with stacking better than selecting the
best one? Machine learning, 54(3):255-273, 2004.
Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent, and
Samy Bengio. Why does unsupervised pre-training help deep learning? Journal of Machine
Learning Research, 11(Feb):625-660, 2010.
Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an
application to boosting. Journal of computer and system sciences, 55(1):119-139, 1997.
Jerome Friedman, Trevor Hastie, and Robert Tibshirani. The elements of statistical learning, volume 1.
Springer series in statistics New York, 2001.
Tomer Galanti, Lior Wolf, and Tamir Hazan. A theoretical framework for deep transfer learning.
Information and Inference: A Journal of the IMA, 5(2):159-209, 2016.
Mike Gashler, Christophe Giraud-Carrier, and Tony Martinez. Decision tree ensemble: Small
heterogeneous is better than large homogeneous. In Machine Learning and Applications, 2008.
ICMLA’08. Seventh International Conference on, pp. 900-905. IEEE, 2008.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT Press, 2016.
Sherif Hashem. Optimal linear combinations of neural networks. Neural networks, 10(4):599-614,
1997.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Roger A Horn and Charles R Johnson. Matrix analysis. Cambridge university press, second edition,
2012.
Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4(2):
251-257, 1991.
Kwang In Kim, James Tompkin, and Christian Richardt. Predictor combination at test time. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3553-3561,
2017.
Chetak Kandaswamy, Luls M Silva, Luis A Alexandre, Ricardo Sousa, Jorge M Santos, and
Joaquim Marques de Sa. Improving transfer learning accuracy by reusing stacked denoising
autoencoders. In Systems, Man and Cybernetics (SMC), 2014 IEEE International Conference on,
pp. 1380-1387. IEEE, 2014.
Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information
Processing Systems, pp. 586-594, 2016.
Yann LeCun, D Touresky, G Hinton, and T Sejnowski. A theoretical framework for back-propagation.
In Proceedings of the 1988 connectionist models summer school, pp. 21-28. CMU, Pittsburgh, Pa:
Morgan Kaufmann, 1988.
David G Lowe. Object recognition from local scale-invariant features. In Computer vision, 1999.
The proceedings of the seventh IEEE international conference on, volume 2, pp. 1150-1157. Ieee,
1999.
David Opitz and Richard Maclin. Popular ensemble methods: An empirical study. Journal of
artificial intelligence research, 11:169-198, 1999.
9
Under review as a conference paper at ICLR 2019
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition
challenge. International Journal of Computer Vision, 115(3):211-252, 2015.
Chun-Wei Seah, Yew-Soon Ong, and Ivor W Tsang. Combating negative transfer from predictive
distribution differences. IEEE transactions on cybernetics, 43(4):1153-1165, 2013.
Kai Ming Ting and Ian H Witten. Issues in stacked generalization. Journal of artificial intelligence
research, 10:271-289, 1999.
David H Wolpert. Stacked generalization. Neural networks, 5(2):241-259, 1992.
Yi Yao and Gianfranco Doretto. Boosting for transfer learning with multiple sources. In Computer
vision and pattern recognition (CVPR), 2010 IEEE conference on, pp. 1855-1862. IEEE, 2010.
Shan You, Chang Xu, Chao Xu, and Dacheng Tao. Learning from multiple teacher networks. In
Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, pp. 1285-1294. ACM, 2017.
Zhi-Hua Zhou. Ensemble methods: foundations and algorithms. CRC press, 2012.
Zhi-Hua Zhou, Jianxin Wu, and Wei Tang. Ensembling neural networks: many could be better than
all. Artificial intelligence, 137(1-2):239-263, 2002.
10
Under review as a conference paper at ICLR 2019
8 Supplementary material
For self-contained, we list some common Taylor expansion in the following.
Logistic: S(Z) := 1+e-z = 2 + 1Z - 418z3 + 480z5 - 80740 z7 + O(Z9), ∀z ∈ R,
Hyperbolic Tan: tanH(Z)=e:+城二=Z - 3z3 + 15z5 + O(z7), ∀∣z∣ ≤ π2
arcTan: arctan(Z)= Z — 3z3 + 1 z5 + +Ο(z7), ∀∣z∣ ≤ 1.
Definition 1. Given an activation σ(Z) and its Taylor expansion Tσ(Z), let Aσ,D (Z) be the truncated
the monomials of degree at most D from Tσ(Z). We define Aσ,D (Z) as the D-degree Taylor approxi-
mation polynomial, and Rσ,D+1(Z) as the remainder part such that Tσ(Z) = Aσ,D (Z) + Rσ,D+1(Z).
For instance, if we set D = 3 then the Taylor expansion of Logistic function S(Z) is separated as the
approximation part As(z),3(z) = 1 + 4Z —表z3 and the remainder part Rs(z),4(z)= 忐z5 + Ο(z7).
Proposition 8.1. (Error Bound of The Remainder)
Let S(Z) be the Logistic function. Consider the approximation AS(z),≤D (Z) and the remainder
RS(Z)D+i(z) defined as above. For given E ∈ (0, 1000) and D ∈ N, if |z| < e1/(D+2), then
|S(Z) - AS(z),D(Z)| = |RS(z),D+1(Z)| <.
Proof. Note that if E < 1 then for all D ∈ N, E1/(D+1) < 1. If |Z| < E1/3 and D = 1, then
1	1	17
IRS(Z),d+i(z)| ≤ -48Z +480Z - 80640Z + O(Z )
1
< 24e <e.
The general case (D ≥ 2) can be proven by the same argument as above.
□
This Proposition means that for a suitable range of Z, the Logistic function can be seen as a linear
function with the error at most E.
Definition 2. For the Logistic activation σ(Z) = S(Z), E > 0 and given polynomial degree D, we
define Zd,g = {z ∈ R : Q(z) — Aσ,D(z)| < e}. Furthermore, for given Components {fj : j ∈
[K]} = F, we consider the variable Z = Θ(f1, ..., fK) and define
Zf,d,c = n~∈ RK+1 ： Z = Θ(f1,…,fκ), ∣σ(Z) - Aσ,D(z)| < e}.
Observe that if the parameters E, F,and |F | = K are fixed, then ZF,D, ⊂ ZF,D+1, ⊂ RK+1.
8.1 Function Composition by Linear Combination
Recall that for a set of pre-trained components {fj(xj) : j ∈ [K]}, Θ(0) (f1, ...fκ) =
Pj∈[κ]+ θ0,j fj, where f0 = 1. For simplicity, we consider Θ(1) (z) = αz. This means
θ(1) ◦ σ ◦ θ(0) (f1,…fκ) = θ1,1σ (Pj∈K]+ θ0,jfj) + θ1,0.
Theorem 1 is a consequence of the following lemmas:
Proof. (of Lemma 5.1)
For simplicity of notations, let g(x) = Θ(0) (f1, ...fκ), hence g(x) = Pj∈[κ]+ θjfj (xj). Also
recall that L~ (x; g) = PN=I (g(x(i)) - y(i))2. To prove the existence of the minimizer, it is enough
to solve the equations of critical points, in the case of a quadratic object function. That is, to solve the
set of equations:
V~L (χ; g)
∂ ∂L	∂L ∖
(西,...,∂θκ)
(0,...,0)T,
11
Under review as a conference paper at ICLR 2019
where for each s, t ∈ [K]+,
∂L =2 XX (g(x⑴)-y(i, fs (Xw = 2 XX I X θj fj (Xji))-y⑴)∙ fs(x⑴)
s	i=1	i=1 j∈[K]+
=21 X θjhfs,fji-hfs,yi I .
j∈[K]+
Hence, to solve V~L (x; g) = 0 is equivalent to solve [hfS,ft)] ∣ [+ X [θt]t∈[κ]+ =
hhf~s, y~ii	+, where hhf~s,f~tii	+ is a (K + 1) by (K + 1) matrix, [θt]t∈[K]+ and
hhf0s, 0yii	are both 1 by (K + 1).
s∈[K]+
Note that linear independence of {f~j }j∈[K]+ makes hf~s , f~ti	a positive-definite Gram
matrix (Horn & Johnson, 2012) , which means the inversion hhf~s, f~tii	exists. Then θ~ is
s,t∈[K]+
solved:
[θt]t∈[K]+ = hhf0s,f0tiis-,t∈[K]+ × hhf0s, 0yiis∈[K]+
(2)
The above shows the existence of the critical points. On the other hand, since Lθ~ (x; g) is the
summation of square terms, i.e. paraboloid, the the critical points can only be the minimum.
The meaning of the gradient on a function surface is the direction that increases the function value
most efficiently. Hence, if the gradient is not the zero vector then the corresponding point can not be
the minimizer of the function surface. Recall for any s ∈ [k],
[西]t∈[κ]+ ∣~=~s
2 hf0s - 0y, f0tit∈[K]+ .
Before the proof of Lemma 5.2, we need the upper bound of the probability of some events. Note
that 0y is defined according to the given training data, and for each j ∈ [K]+ the length of f0j - 0y,
i.e. f0 - y0, is also given. The question is, for fixed y0 what is the probability of selected f0 is
perpendicular to f0 - 0y? A folklore approach is considering that {f0 = (f(x(1)), ..., f(x(N)))} obeys
the normal distribution, and setting the mean of f(x(i)) as y(i) for each i ∈ [N]. In the following we
propose another simple probability argument to obtain a loose upper bound.
.	_	.	.	一 ，τ≠ ,H	_	, τ÷	一 .一. 一. 一	.
Proof. (of Lemma 5.3) Observe that hf0 - 0y, f0i = 0 ⇔ (f0 - 0y) ⊥ f0, which implies the angle
between them, ∠f-~) ~, is in the interval [π-^, π+^] for small E ∈ R+, as shown in the left part of
Figure 2. The red, orange, and blue vectors show three possibles of the pair of f and f - 0y. The
length of f - 0y is fixed since f and y0 are given, but the angle between f - 0y and 0y can decide whether
(f - 0y) ⊥ f . The gray circle collects all possible end-point of the vector f - 0y emission from
the end-point of 0y. Although on the whole circle there are exactly two specific angles 3 can satisfy
(f0 - 0y) ⊥ f0, we give a loose small interval E with respect to π. In particularly, we set 0 < E < e-N .
π	π-E	π+E
~∈fPU){∠f-~),~二2} ≤ ~∈F‰)1F ≤ ∠f-~),~ ≤ 丁)
E1
π<πeN.
□
Now we are ready to proof Lemma 5.2.
3That is, two points on the circumference, which is in fact measure zero on all possible angle [0, 2π)
12
Under review as a conference paper at ICLR 2019
Figure 2: An illustration of Lemma 5.3 (the left) and Lemma 5.4 (the right)
Proof. (of Lemma 5.2)
We denote A the event that at least one of ~ ∈ Brk is the minimizer of L(~) for convenience.
IL (~) = 1 ⇔ the event A is true
⇒ [就]i∈[K]+ l~=es = 2 f - y,~〉k[K]+ = [0]K×1 for somes ∈ K1 +
，丁	→ ,	，丁	，一、	，丁	，丁、
⇒h~ι - y,/ι〉=0 ∧ h/ι - y,~2〉=0 ∧ …NR- y,fK〉= 0
，丁	，一、	，一	，―、	，丁	，一、
or ∙ ∙ ∙ or hyK - y,∕ι〉= 0 Zyκ - y,y2)= 0 ∧ ∙ ∙ ∙ ∧〈yκ - y,∕κ)= 0
Hence, for given y and L( f j) = ∣∣y —矶 ,∀ ∈ [K]+, we have
Pr{□l(~) = ι}≤ X Pr{〈y-y,∕i〉=0八〈y-y,∕2〉=0∧…Ny-y,y)=。}
j ∈[K ] +
≤K ∙ Pr {hfi - y,fii =0}
K
<----.
<πeN ,
1	,1	1 ∙	1 ∙. ∙ 1	1	/1	7	■一 Γ TZ-^∖ 4-	1
where the second inequality is based on the symmetry between fs and ft for any s,t ∈ [K] +, and the
last inequality is by Lemma 5.3.	□
Proof. (of Theorem 3) We start from a simple case:
Claim: ∃β ∈ R s.t.
X (fι(Xi) - y)2- X (fι(Xi) + βf2(Xi)-%)2 > 0.
i∈[N ]	i∈[N ]
Proof.
X (fι(Xi)- yi¥ - X (fι(χi) + Bf2(Xi)-y)2
i∈[N ]	i∈[N ]
=X [(fι (Xi)-y)2-(fι (Xi) + βf2 (Xi)-y)2]
i∈[N ]
=-工 fi (Xi)2 B2 + 2 工(f2(Xi)yi - f2(Xi)fi (Xi)) β
i∈[N ]
i∈[N ]
Observe that the above is a quadratic equation of β with negative leading coefficient. Hence, to obtain
the maximum of the difference, we can set
B =
Ei∈[N ] (f2 (Xi )y - f2 (Xi )fi (Xi ))
∑i∈[N] fi(Xi)2
→ → .
M - fi,f2〉
f ,f2〉
13
Under review as a conference paper at ICLR 2019
Note that if h~y - f1, f2i = 0 then the last pre-trained component is no need to be added. We aim
to calculate the probability of this case. Observe that h~y - f1, f2i = 0 ⇔ (~y - f1) ⊥ f2. This
condition is different from previous Lemma. Here we have to find the upper bound of the probability
of (~y - f1) ⊥ f2 for given f1 and ~y. As shown in the left part of Figure 2), the angle between f2
and y must be in a specific interval, say [π-^, π+^ ] for small E ∈ R+. In order to be concrete, We set
0 < < e-N.
Pr n(~- fI) ⊥ ~0 ≤ - < -IN.
f~∈F(~y,1)	π	πeN
□
The general case can be reduced to the above claim by considering gK-1 as f1 and θkfK as βf2 .
Furthermore, since there there K possibles the be selected as the least pre-trained component, the
probability is upper bounded by ∏KN
□
8.2	Function Composition by Non-Linear Activation
Proof. (of Lemma 5.4) Although the lemma is an existence statement, We give a constructive proof
here. By setting D = 1 in Proposition 8.1, We knoW that for Logistic S(z) and 0 < - < 1/1000, the
degree-one Taylor approximation As(z),ι = ∣ + 4Z with the remainder ∣Rs(z),2∣ < -. Define M :=
10 ∙ maXj∈[κ] + ,i∈[N]{∣fj(xi)∣}. Hence by setting Z = fjMxj), we have ∣S (jj) - 2 - f4j ∣ <
-.This means that for the givenj ∈ [K], θj = M, θ0 = and for all j0 = j, θjo = 0. Furthermore,
α = 4M.	□
This lemma implies that the S(Z) can approximate linear function as possible in a non-zero length
interval, hence if the scaling of θ is allowed then Theorem 1 can be applied.
Corollary 8.1. If the activation function is the Logistic, S(Z), and {fj (xj)}jK=1 satisfies LIC, then
with high probability there is a vector θ s.t. Lθ~ x; σ ◦ Θ(0) (f1, ..., fK ) < minj∈[K ]+ L(fj (xj)).
*
Proof. Set - of above Lemma as 3N, then previous Lemma shows that there exists θ which maps
*
{fj } into Zσoθ(o) ι %. Since the output of σ ◦㊀⑼ is a linear function with error at most 3N, we
have the same conclusion.	□
*
Proof. Let g(x) = σ ◦ Θ(o)(f1,..., fκ)(x) for short. First observe that |g(x) - f (x)| < 3N ⇒
∀i ∈ [N],g(x⑴)-fj(Xci)) > -N. Then
X (g(χ(i)) -y⑶)=X {(g(χ(i)) - fj (X ⑶))+ (fj (X ⑶)-y(i))} >n .言 + e* = 2f > 0.
i∈[N]	i∈[N]
On the other hand, it can be calculated that
∂L ∂L ∂L ∂L T
V~L(X； g"~=~'*∕3=(西 ,西,...,∂θK，诟)l~=瓦 */3 3
where [a]T is the transpose of the matrix [a]. Also note that ∂∂L |~=~ * ɜ = 2 ∙
Pi∈[N]团x(i)) - y(i)) ∙S(Z), and ∂L0l~=~v*∕3	=	2 ∙ Pi∈[N] (g(x(i)) - y(i)) ∙ 1. SinCe
Pi∈[N] (g(x(i)) - y(i)) > 0, we can conclude V~L (x; g) |~=~«*/3 = ~.	□
Proof. (of Lemma 5.6 ) By previous Lemma, it is valid to consider the best performance component,
fj*; i.e. L(fj*) = minj∈[κ]+ L(fj(Xj)). Since V~L(θe*/3) = 0, by definition of the gradient,
moving along the direction of -V~L(θζ*/3)/ ∣∣V~L(~e*/3)∣∣ with a step-size α > 0 must strictly
decrease the value of Lθ~ (x; g). W.L.O.G, we can assume this α is optimal; that is, if α > r > 0 then
L(θe*∕3) > L(θe*∕3) - r ∙ V~L(θe*∕3)/ ∣∣V~L(θe*∕3)∣∣, while if r = α + δ for some δ > 0 then
14
Under review as a conference paper at ICLR 2019
→
→
→
→
L(~e*/3) ≤ L(~e*/3) - r ∙ V~L(~e*/3)/ ∣∣ V~L(~e*/3)∣∣. The issue is how to find a proper step-size
r > 0? We consider the line search approach such that:
r* = arg min < L ∣ ~0 一 r ∙ ∣∣'~(A)
r∈R I ∖	∣V~L(~0)
This outputs F * then we can make sure that L (~0) > L (~0) — r ∙ V~L(~0)/ ∣∣V~L(~0)∣∣. Since
the underlining θ0 is θ* /3 that makes the loss is the same with the best one. Hence, we have
(~0 一 r* ∙ V~L(θe*/3)/ l∣V~L(~^*∕3)ll) to fit our goal (beating the best one).	□
Combining these lemmas can obtain the conclusion of Theorem 2.
Corollary 8.2. The process of Lemma 5.5 converges.
Proof. (of 8.2) It is know that if a monotone decreasing sequence is bounded below, then this
sequence convergences....... (revising) Repeating the process of Lemma 2, we can obtain a strictly
decreasing sequence: L(θ0) > L(θ1) > L(θ2) > ........... Note that ∀i, (θi) ≥ 0. This means the
sequence is monotone decreasing and bounded below, so theoretically it converges by monotone
convergence theorem of mathematical analysis. Algorithmically, the gradient descent based sequence
finding process stops at some term with Vθ~L(θθ0) = 0, which is a (local) minimum or a saddle
point.	□
Corollary 8.3. If the assumptions in Theorem 2 are satisfied, then with height probability there exists
θθ0 s.t. Lθ~0 is a (local) minimum or a saddle point, while Lθ~0 (x; g) < minj∈[K]+ |L(fj (xj))| still
holds.
Assume the pre-trained component set {fj(xj)}jK=1 satisfies both NPC and LIC, then there exists θθ
s.t. Lθ~ (x; g) < minj∈[K]+ L(fj (xj)).
In the previous proof, the critical properties of activations are local linearity and differentiability.
Hence, it is not hard to check that if We replace σ(∙) in Eq. (1) with other common activations, the
conclusion still holds. By local linearity we mean that on a non-zero length interval in its domain, the
function can approximate the linear mapping as well as possible.
Corollary 8.4. Theorem 1 and 2 can apply on any activations with local linearity and differentiability.
Based on Corollary 5.1 and Lemma 5.6, it is natural to obtain the process of finding θθ: by gradient
descent or the closed form of Corollary 5.1. We can compute the optimal weights for the bottom
Sigmiod block. On the other hand, after random initializing, the parameters of un-trained components
in the Relu or Tanh blocks are assigned. This implies they can be treated as the all pre-trained case
in Theorem 1 or 2. In fact, given the outputs from bottom level block then Corollary 5.1 provides
weights improving the accuracy. Then it goes to the next up level block until the top, which is the
forwarding steps of Back-propagation LeCun et al. (1988). Hence with initialization for un-trained
component, Corollary 5.1 is essentially the same as Back-propagation.
8.3	A Mix of Pre-trained and Un-trained Components
Now we first consider some of {fΘj (xj)}jK=1 are pre-trained and some are un-trained, and then
investigate the hierarchical combination of both kinds of components. In particularly, Eq. (1) can be
re-written as g(x) = w0 ∙ σ (Θ1f1 + Θ2fθ2) + bo, where fι is a pre-trained component and fθ2 is
un-trained. Since Θ2 is not fixed, it can not be checked that LIC and NPC assumptions are satisfied.
On the other hand, after initialization, fΘ2 can be seen as a pre-trained component at any a snapshot
during training phase.
Theorem 5. In the end of an weight updating iteration, if the components f1 and fΘ2 satisfy LIC and
NPC assumptions, then with high probability wθ updated in the next iteration can improve the loss.
15
Under review as a conference paper at ICLR 2019
Proof. Recall the training algorithm is the backpropagation algorithm. Also note that according to
Eq. (1), the order of updating is θ first and then Θ2 . We denote in the end of iteration i the value of
θ~ and Θ2 as θ~(iter=i) and Θ(2iter=i), respectively. With randomized initialization, Θ2 is assigned as
Θ(2iter=0) before the execution of the iteration 1. Then in each iteration i ≥ 1, g(x) is a combination
of fixed parameter components. Hence this can reduce to the all pre-trained cases, and can apply
Theorem 1 and 2.	□
Lemma 8.1. For a given data set X, let ~g := (g1, ..., gN) and ~y = (y1, ..., yN). If h~g, ~g - ~yi 6= 0,
then there exists α ∈ R s.t.
(αg(xi) - yi)2 <	(g(xi) - yi)2
i∈[N]	i∈[N]
Proof. It is equivalent to show the inequality
X (αg(xi) - yi)2 - X (g(xi) - yi)2 < 0
i∈[N]	i∈[N]
has a real number solution.
X h(αg(xi) - yi)2 - (g(xi) - yi)2i
i∈[N]
=I X g(xi)2 I ɑ2 + I -2 X g(xi)yi I α + I - X g(xi)2 + 2g(xi)yi I
i∈[N]	i∈[N]	i∈[N]
=h~g, ~giα2 + (-2h~g, ~yi) α + (-h~g, ~gi + 2h~g, y~i) .
This is a quadratic inequality of α, hence if
(-2h~g, y~i)2 - 4 (h~g, ~gi) (-h~g, ~gi + 2h~g, ~yi) ≥0,
then there exists at least one real solution.	□
Now we first consider some of {fΘj (xj)}jK=1 are pre-trained and some are un-trained, and then
investigate the hierarchical combination of both kinds of components. In particularly, Eq. (1) can be
re-written as g(x) = wo ∙ σ (θιfι + Θ2fθ2) + bo, where fι is a pre-trained component and fθ2 is
un-trained. Since Θ2 is not fixed, it can not be checked that LIC and NPC assumptions are satisfied.
On the other hand, after initialization, fΘ2 can be seen as a pre-trained component at any a snapshot
during training phase.
8.4 Generalization Error Analysis
Theorem 4. Assume pre-trained components {fj}jK=1 satisfy LIC and NPC. Let {GE (fj)}jK=1 be
corresponding generalization errors of {f }j=ι, and ㊀⑷◦ o(l)◦•••◦。⑴◦㊀⑼⑴，…，fκ) be
the composite neural network. Denote the generalization error, E{L(Θ(l) ◦ oq ◦ ∙∙∙ ◦ σ.) ◦
Θ(o)(f1, ..., fK ))}, of the composite neural network as E{LΘ,f1,...,fK}. Suppose the learned weights
obey the normal distribution. Then with high probability, there exist a setting of {Θ[l> …，㊀：。)}
such that E{Lθ,fι,..,fκ} ≤ Θ[L)(GE(fι),∙∙.GE(fκ)).
Proof. (of Theorem 4) (Proof Sketch) We apply the idea similar to Kawaguchi (2016): the exception
of non-liner activations is same with the exception of liner activations. Previous theorems provide that
with high probability there exists the solution of Θ(i), ∀i ∈ [L]+ s.t. each Θ(i)+1σΘ(i) approximates
a degree one polynomial AΘ(i)+1 σΘ(i),1 as well as possible. If the weights are obey the normal
distribution, then E{Lθ,fι,...,fK}≤ 3")(GE(fi),...Ge(fκ)).	□
16