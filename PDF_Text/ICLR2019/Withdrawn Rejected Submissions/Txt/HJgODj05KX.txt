Under review as a conference paper at ICLR 2019
A preconditioned accelerated stochastic
GRADIENT DESCENT ALGORITHM
Anonymous authors
Paper under double-blind review
Ab stract
We propose a preconditioned accelerated stochastic gradient method suitable for
large scale optimization. We derive sufficient convergence conditions for the min-
imization of convex functions using a generic class of diagonal preconditioners
and provide a formal convergence proof based on a framework originally used for
on-line learning. Inspired by recent popular adaptive per-feature algorithms, we
propose a specific preconditioner based on the second moment of the gradient.
The sufficient convergence conditions motivate a critical adaptation of the per-
feature updates in order to ensure convergence. We show empirical results for the
minimization of convex and non-convex cost functions, in the context of neural
network training. The method compares favorably with respect to current, first
order, stochastic optimization methods.
1 Introduction
Large scale optimization tasks require computationally fast algorithms and have strict memory re-
quirements. This makes gradient descent methods a prime choice in many areas of science due to
their simplicity and light computational burden. Motivated by the need to further lower the com-
putational and memory load, stochastic gradient descent (SGD) methods (Robbins & Monro, 1951;
Tieleman & Hinton, 2012; Kingma & Ba, 2014) have become the default choice of optimization
algorithm for machine learning, deep learning and more generally large scale data processing (Deng
et al., 2013; Schmidhuber, 2015). Various acceleration techniques have been devised to improve
the convergence speed of SGD. They can be generally grouped into two classes. A first class of
approaches use inertia or averaging techniques to improve the quality of the stochastic gradient
(Sutskever et al., 2013; Nesterov, 1983). The second class applies preconditioning or per-feature
adaptation to improve the overall conditioning of the optimization task, and thus the speed up the
convergence (Tieleman & Hinton, 2012; Kingma & Ba, 2014; Duchi et al., 2011).
In this paper we propose a preconditioned accelerated stochastic gradient descent (PA-SGD) method
with a generic bounded preconditioner and analyze its convergence properties for convex cost func-
tions. The method combines both approaches by coupling Nesterov’s accelerated gradient descent
(Nesterov, 1983, NAGD) with a varying diagonal preconditioner and a stochastic approximation of
the gradients. The convergence results assume that the algorithms step size and acceleration coeffi-
cient are decaying. For the preconditioner, a sufficient condition for convergence is a bound on the
rate of variation between two consecutive steps.
The generic preconditioner can be understood as per-feature adaptation of learning rate similar to
many popular methods such as ADAM (Kingma & Ba, 2014), RMSProp (Tieleman & Hinton, 2012)
or NADAM (Dozat, 2016). In contrast, the update equations of popular per-feature adaptive algo-
rithms, such as ADAM, do not directly control the rate of variation of the per-feature scaling and
can have a divergent behavior (Sashank J. Reddi & Kumar, 2018). They add a small constant to
the decaying exponential sum of the squared gradient to ensure numerical stability. Very recently,
Sashank J. Reddi & Kumar (2018) proposed a long term memory for the per-feature adaptation to
guarantee convergence for momentum methods. Note that our work is using a generic per-feature
preconditioner coupled with Nesterov’s accelerated gradient. It only shares similar requirements,
namely a bounded variation for the preconditioner. NADAM also combines per-feature adaptation
with Nesterov’s accelerated gradient. It relies on the gradient momentum to provide an improved
gradient for a Nesterov acceleration scheme and it does not precondition directly the instantaneous
1
Under review as a conference paper at ICLR 2019
gradient, as we propose here. NADAM uses exactly the same per-feature adaptation strategy as
ADAM and thus inherits the same convergence problem. Huang et al. (2017) and Johnson et al.
(2016) also provide experimental evidence that ADAM is outperformed by SGD due to poor con-
vergence. Since in this work we reconcile ideas from ADAM and Nestrov, the added extra update
rules for the per-feature adaptation can also help with ADAM’s convergence on convex cases.
This paper is organized as follows. We continue by introducing the main details of the algorithm in
Section 2. Thereafter, an analysis of the convergence for convex functions and the assumptions re-
quired for the algorithm’s convergence are presented in Section 3. We provide extensive simulation
results showcasing the capabilities of the new method for both convex and non-convex cost func-
tions in Section 4. We compare the proposed PA-SGD algorithm with ADAM, AMSGrad (Sashank
J. Reddi & Kumar, 2018), and the stochastic adaptation of NAGD, herein denoted as NASGD. Fi-
nally, in Section 5 we summarize our main contributions.
2 The PA-SGD algorithm
Our proposed method combines Nesterov’s accelerated gradient descent algorithm (Nesterov, 1983)
with a diagonal preconditioning matrix that is applied directly to the instantaneous gradient at each
iteration t. To introduce the algorithm we assume a sequence of cost functions ft at each given
instant t. Such functions can be introduced either through the sampled data, by using subset of the
available data, or due to the inherent properties of a function f which is sampled at each time instant
t. We aim to find the solution θ that minimizes f . Starting from Nesterov’s accelerated gradient
descent method (Sutskever et al., 2013; Nesterov, 1983) we derive a stochastic version
θt+1	= θt + at+1
at+1	= μtat - αtVft(θt + μtat),
(1)
where at each iteration we have access to the stochastic realization ft . Here we denote by θt the
solution at iteration t. The vector at represents the accelerated gradient, that is used as a descent
direction. We allow for a time varying step size αt as well as for a time varying acceleration pa-
rameter μt. By performing a change of variable θt = θt + μtat in (1) and introducing the diagonal
preconditioning matrix Pt with diagonal elements pt,i, we arrive at the proposed preconditioned
accelerated stochastic gradient descent method (PA-SGD)
J θt+ι = θt - μtat + (I + μt+1) at+ι
a at+ι = μtat - αtptgt∙
(2)
Here, for ease of notation, We denote gt = Vft(θt), with each element denoted by gt,i.
2.1	Convergence assumptions
To guarantee convergence on convex cases, we require the following sufficient assumptions: the
diagonal elements of the preconditioning matrix Pt are positive and there exists a lower bound for
them; the resulting preconditioned absolute values of the gradient are bounded. Formally we have
P1i ≤ CP，	pt,i > 0，	∀i
kPt |gt| k∞ ≤Cpg,
(A1)
where we denoted by |gt | the absolute value for each element in gt . Additionally, we require that
the acceleration μt and step size at are decaying as
μt = μ0λ ,
αt = α0t-c
0<λ<1
0<c<1,
(A2)
where generally λ is chosen close to 1 and c is chosen close to 0.5. These assumptions are used
in other algorithms such as ADAM. In this work, we argue that for convergence on convex cases
we need an extra assumption, namely we require the pre-conditioner to satisfy a bounded rate of
change,
αtpt,i ≤ αt-1pt-1,i,	∀i,	(A3)
as a sufficient condition for guaranteed convergence on convex cases. The requirement specified
by (A3) represents a key difference between our pre-conditioner and the per-feature weights of
ADAM (Kingma & Ba, 2014).
2
Under review as a conference paper at ICLR 2019
2.2	Preconditioner design
There are several preconditioners that meet the assumptions (A1) and (A3). Motivated by the success
of the per-feature adaptation, we propose the following preconditioner
pt,i =	(q (i→t7+ε)
vt,i = βvt-1,i + (I- β)g2,i	⑶
αt2(1-βt)
vt,i = max (^vt,i, S a： ](1 —et-i) vt-1,i J ,
as it contains two major well known techniques, namely the NASGD and adaptive per-feature meth-
ods such as ADAM. We note that for s = 0, the choice of preconditioning in ADAM is recovered
(Kingma & Ba, 2014). However, the overall algorithmic updates remain different since here we use
the preconditioner for a Nestrov’s accelerated gradient scheme. The quantity ε > 0 represents a
small constant that is needed for numerical stability to avoid pt,i increasing to ∞ if vt,i approaches
0. The exponential window averaging factor denoted by β is configured such that 0 < β < 1.
The stability parameter s controls the rate of variation of the preconditioner. Intuitively, it is a
balance between convergence rate and stability. We show in Section 3 that the preconditioner from
(3), with s = 1, meets assumptions (A1) and (A3). For a convex cost function and s < 1, the updates
do not necessarily converge depending on the choice of step size and acceleration parameter value.
Along the same lines, Sashank J. Reddi & Kumar (2018) noted that ADAM and other algorithms
that use similar per-feature adaptation such as NADAM are lacking a mechanism to control the rate
of change, and as a result may diverge in convex cases. This contribution and Sashank J. Reddi &
Kumar (2018) propose different strategies for the preconditioner and the overall algorithm.
3 Convergence properties
Our proof concerns convex cost function. We analyze the convergence in the online learning frame-
work proposed by Zinkevich (2003). A sequence of convex cost functions ft(θt) for t = 1 : T is
assumed. At each iteration t, we aim to estimate the parameters θt with respect to ft . The frame-
work from Zinkevich (2003) considers the convergence of the sequence of estimates θt and their
associated cost functions f with respect to the best solution θ* = arg minθ PT=I ft(θ) by defining
the regret function
T
R(T) = X (ft(θt) - ft(θ*)).	(4)
t=1
The regret function R(T) maps the goodness of fit of the sequence ft(θt) compared to the ideal
solution θ*.
For convergence guarantees, we assume bounded gradients and bounded variation in the parameter
estimates, similarly to Kingma & Ba (2014),
kθt- θ*k∞ ≤ Cθ
kgt k∞ ≤ Cg .
(A4)
It can be easily verified that the preconditioner defined by (3) satisfies the assumptions (A1) and
(A3). Assumption (A4) combined with the use of ε and the property that the quantity vt,i ≥ (1 -
β)gt2,i ensures (A1). A choice s = 1, as a sufficient condition for convergence in case of convex cost
functions, and the use of the max operator ensure that (A3) is satisfied.
Proof outline: to prove convergence under assumptions (A1), (A2), (A3) and (A4), we show
that limτ→∞ RTT) = 0. To this end, we upper-bound 凡+1,/, and gt,i (θt,i - θ*) , and use these
bounds to eventually find an upper bound for R(T ).
Lemma 1. Given a convex function f : Rd → R, then for any x, y ∈ Rd we have f(y) ≥
f (x) + Vf (χ)t(y - x).
Property 2. Under assumptions (A1), (A2), (A3), and for a given iteration t and coefficient index i
of the accelerated gradient |at+1,i| is bounded from above
as
λt
|at+1,i | ≤ α0Cpg t-c +
1 - μo
(5)
3
Under review as a conference paper at ICLR 2019
Proof. From (2) we can expand ∖at+ι,i∖ such that
∖at+1,i∖
<(A1)
<(A1)
<(A2)
∖ - &tPt,igt,i + μtat,i∖
∖ - atPt,igt,i - Pj=I a7-Pj,igj,i Πk=7-+1 μk∖
&tPt,i∖gt,i∖ + Pj=I ajPj,i∖gj,i∖ Qk=j+1 Mk
αtCpg + Pj=1 αjCpg Πk=j+1 μk
α0Cpg (t c + Pj=1 j。口[=升1 μ0λk)
αoCpg ft-c + Pj=I j-cΜ0^j»)
α0Cpg (t-c + λt Pj=I μ0).
(6)
≤
≤
We have used ∏k=7-+1 λk < λt since 0 < λ < 1 and j-c < 1. Replacing the geometric progression
with its upper bound we arrive at (5).	■
Property 3. Under assumptions (A1), (A2), (A4) andfor any given iteration t and coefficient index
i the quantity gt,i (θt,i — θ*) is boundedfrom above as
gt,i(θt,i - θ*)	<	⅛7
⅛ [ɑ
[tc (θt,i - θi )2 - tc (θt+1,i - θ*)2]
+ 2μθ)2 (t-c + 2τ⅛0 +
+
λ2ttc
(i-"0 )2
+
⑺
Proof. From (2) we have
θt+1 = θt+ μtμt+1at - (I + μt+1) αtPtgt∙
(8)
By explicitly writing (8) for a component i and by subtracting from both sides of (8) the ideal
solution θ* and squaring the resulting equality we get
(θt+1,i - θi )2 =	(θt,i - θi )2 + (-μtɑt,i + (1+ μt+1) αt+1,i)2 +
2 (μtμt+1αt,i - (I + μt+1) αtpt,igt,i) (θt,i - θt) ∙
(9)
Here we also used (2) to replace μtμt+1 at — (1 + μt+1) &tPtgt by -μta + (1 + μt+1) at+1.
Rewriting the equality to express gt,i (θt,i - θ*) we arrive at
gt,i (θt,i - θ*)
2(1⅛μt+ι )αtpt,i
[(θt,i - θ*)2 - (θt+1,i - θ*)2 +
(-μtat,i + (1+ μt+ι,i) ɑt⅛ι,i)2 + 2μtμt+ιαt
2(1+μt+ι )ɑtpt,i
[(θt,i- θty2 - (θt+ι,i- θt )2
:,i(θt,i-刎
+
(μt∖ɑt,i∖ + (1 + μt+1) ∖at+1,i ∖)2 + 2μtμt+1∖αt,i∖∖ (θt,i - θi ) ∖]
(10)
<(A4)
2(1+μt+ι )ɑtpt,i
[(θt,i - θ*)2 - (θt+1,i - θ*)2 +
(Mtlat,i∖ + (I + μt+1) ∖at+1,i∖)2 + 2μtμt+1Cθ ∖αt,i∖].
Applying (5) and relying on (A2) produces
gt,i (θt,i - θ*)	<(5)，(A2)
2(1+μt+ι)ɑtPt,i
α0CPg (μθλt (t-c + J2而
[(θt,i - θ*)2 - (θt+1,i - θ*)2 +
(1 + μoχt+1) ((t + 1)-c + iλ≡))
2μ0a0CθCPgλtλt+1 (t-c + 1-*0 ) i	(11)
<(AI),(A2) 2(1+μ0λt+i)α0pt,i [ (θt,i - 传)2 - (θt+1,i -传)2 +
00Cpg (1+2μ0λt)2 (t-c + 瓷0) +
2μ0α0CθCPgʌtʌt+1 卜-C + ɪɪθ) i ,
which, under assumptions (A1) and (A2), further reduces to (7).	■
<
1
1
1
1
+
2
+
4
Under review as a conference paper at ICLR 2019
Theorem 4. Under assumptions (A1), (A2), (A3) and (A4) the regret function R(T) is bounded
from above as
R(T) ≤ 2Cα0 Pd=IhTcC2 + α2CPg (1 +2μ0)2 ( (1-μ02(1-λ) + (1-μc,)21(1-λ2)2ɔ +
2μ0α0CθCpg (1-1χ2 + (l-μ0)(l-λ3)2 ) + α2CPg (I + 2μ0)2 PT=I t-ci	(12)
≤ O (T c + PtT=1 t-c .
Proof. We construct an upper bound for (4) by relying on Lemma 1. As such we have
ft(θt) - ft(θ*) ≤ g↑ (θt- θ*).	(13)
Writing (13) for each component i, summing for t = 1 : T, we have
R(T) =	Pd=I P=1 gt,i(θt,i-θi)
≤⑺ Pd=I PLI 2⅛ [tc (θt,i - θ∙D2 - tc (θt+1,i - θ?)1 +
2¾ hα2Cpg(I + 2μ0)2 (t-c + 21λμ0 + (iλ-μto)2) +
2μ0α0CθCpg (λ2t + λ-μoo)]
≤	2⅛ Pd=Ihp⅛ (θι,i - θn2+PT=2 ((S - ⅛≡) (θt,i - θ:)2) -	(14)
PTi (θτ +1,i -优)2i +
200 ha2Cpg (1 + 2μ0)2 PT=1 (t-c + 2 1⅛0 + (1λ-μt0)2 ) +
2μ0α0CθCpg PT=I (λ2t + λ-μo) i.
Relying on (A4) and using (A3) written as Ptc- -	> 0 for ɑt = α0t-c, We have
R(T) ≤(A2) 2⅛ Pd=I hC2+C2 PT=2 (PG - P⅛c) - S (θτ+ι,i - θr)2 ]
+ 200 hα2Cpg (I + 2μ0)2 PT=I (t-c + 2T⅛0 + (i-μto)2 ) +
2μ0α0Cθ Cpg PT=ι (λ2t+ι-μ0) ]	(15)
≤(A3) 200 Pd=IhTCC + α2Cpg (1 + 2μ0)2 PT=1 (t-c + 2ɪ + (⅛0y) +
2μ0α0CθCpg PT=ι (λ2t + i-to)].
The resulting inequality can be bounded using the upper bounds for the geometric series PtT=1 λt ≤
ɪ-ɪ and PT=I λ2t ≤ ɪ--ʌɪ and for the arithmetic-geometric series PT=ι λ2ttc ≤ (1工产 and
PT=I λ3ttc ≤ (]_；3)2, respectively. This produces the bound from (12).	■
Corollary 1. Under the assumptions from Theorem 4 it follows that
“竽=0.
(16)
Proof. This result can be obtained directly from Theorem 4 using the fact that the divergence rate
of the hyper-harmonic series PT=I t-c, for 0 < c < 1, is proportional to T 1-c. Thus, if 0 < c < 1
all terms from (12) grow with a rate slower than T.	■
4	Simulations and results
We study the convergence properties of the proposed PA-SGD method in comparison with ADAM
(Kingma & Ba, 2014), AMSGrad (Sashank J. Reddi & Kumar, 2018) and NASGD (Sutskever et al.,
2013; Nesterov, 1983) as defined in (1). For all simulations we use the MNIST hand written number
database (Lecun et al., 1998). All experiments are performed in Matlab. Throughout the simulations,
5
Under review as a conference paper at ICLR 2019

4.5
4
3.5
Tl r
0 »0.15
M 0.1
10.05
PA-SGD1 网=0.99, ɑo = IOlS
— —ADAM, 0ifi = 0.99,，)= IO-1
----AMSGradl ft,0 = 0.9, a0 = IOT
----NASGD, μo = 0.995, α0 = IoT
——-LS Optimum
5
4.5
4
3.5
----PA-SGD1 μ0 = 0.9, αβ = 10-2
----PA-SGD1 μ0 = 0.95, a0 = 10-2
....PA-SGD, μ0 = 0.99, a0 = 10^2
--PA-SGD, μ0 = 0.995, a0 = 10-2
---PA-SGD1 μ0 = 0.9, a0 = IOT
— —PA-SGD, μo = 0.95, a。= IO-3
....PA-SGD, μ0 = 0.99, a0 = IO-3
----PA-SGD, μ0 = 0.995, a0 = Io-S
——-LS Optimum
20	40	60	80	100	120	140
20	40	60	80	100	120	140
I
# epochs
# epochs
Figure 1: Convex cost function: evolution of the least squares cost for (left) the best performing
parameter configuration of the tested algorithms; (right) several choices of acceleration parameter
μo for PA-SGD. The new PA-SGD converges faster and achieves an almost optimal cost function
value after 10 - 20 epochs. ADAM with β1,0 = 0.9 and α0 = 10-1 has similar convergence as
AMSGrad and was omitted. We also include a zoom-in, in log-log scale, of the gap between the
least squares cost and that of the other methods.
for all tested algorithms we use a stochastic mini batch of size 128. We report the evolution of the
cost function values across iterations. We present the best evolution of the tested algorithms, the
configuration parameters being selected using a grid search. For better visibility we only present the
convergence trend by averaging the results across multiple iterations.
All algorithms are configured to start from the same identical initialization, generated to be normally
distributed with variance 0.1. For ADAM and AMSGrad we use the same parameter names as
presented by Kingma & Ba (2014) and Sashank J. Reddi & Kumar (2018), respectively, namely β1,t
for the gradient momentum parameter, β2 as the equivalent of β from (3) in PA-SGD and αt as the
step size. The values for the gradient momentum are chosen β = 0.999 and β2 = 0.999. The decay
rate of the acceleration parameter from (A2), λ = 1 - 10-8, and the numerical stability constant,
ε = 10-8, are kept constant across all experiments. The other individual configuration parameters
are reported for each algorithm and for each test case.
We perform two classes of experiments. The first experiment set investigates the convergence on
convex problems. In this setup we evaluate the performance of the algorithm under the assumptions
that guarantee theoretical convergence. We perform a least squares regression directly with respect
to the labels and a logistic regression experiment to classify digit 5. The next experiments look into
the empirical performance for the training of neural network models. In this case the cost functions
are non-convex and the experiments fall outside the scope of our convergence proof. Here we use
constant step sizes α, a choice consistent with typical deep learning experiments. For our algorithm,
the stability parameter s is set to 0. For the first practical experiment, we train a neural network
with two 32 neurons hidden layers to predict the MNIST numerical label. All activation functions
are set to tanh and the labels are appropriately scaled. We use a mean squared error (MSE) as a
cost function. The second experiment involves the training of similar neural network having two 32
neuron hidden layers with ReLu (Hahnloser et al., 2000; Glorot et al., 2011) activation functions,
resulting in a sub-gradient based optimization. We use this setup for a softmax multinomial logistic
regression with respect to the 10 classes in the dataset. Both problems exhibit sparse gradients which
have been shown to be problematic for the stochastic gradient descent methods (Duchi et al., 2011).
4.1	Convex cost functions
For the experimental setup involving the convex cost functions we perform a grid search
for the best parameters, for each algorithm, by varying the configuration parameters μo ∈
{0.9, 0.95, 0.99, 0.995} for PA-SGD and NASGD, and β1,0 ∈ {0.9, 0.95, 0.99, 0.995} for ADAM
and AMSGrad and α0 ∈ {100, 10-1, 10-2, 10-3, 10-4} for all algorithms. For the step size de-
crease, we use c = 0.5 in all simulations.
For the linear regression of the MNIST data, the cost function is convex and a closed form least
squares (LS) solution exists. We use it as the optimal solution, to judge the behavior of the algo-
rithms. In Figure 1, on the left, we present the results obtained with the best configuration for all
algorithms. The convergence rate of the proposed method is better when compared to that of the
6
Under review as a conference paper at ICLR 2019
# epochs
δ
O
Figure 2: Convex cost function: evolution of the logistic regression cost for (left) the best performing
parameter configuration of the tested algorithms; (right) several choices of acceleration parameter
μo for PA-SGD. The new PA-SGD exhibits faster convergence than both ADAM and AMSGrad.
ADAM with β1,0 = 0.9 and α0 = 10-1 has similar convergence as AMSGrad and was omitted. We
also include a zoom-in, in log-log scale.
0.1
0.095
0.09
0.085
0.08
0.075
，~~ PA-SGD, μ∣∣ = 0.9, a⅛ = IO-2
——PA-SGD, μo = 0.95, o⅛ = 10^2
....PA-SGD, μa = 0.99, a0 = IOT
--PA-SGD, μ0 = 0.995, a0 = 10^2
----PA-SGD, μ∣∣ = 0.9, a⅛ = 10-s
——PA-SGD, μo = 0.95, α∣> = 10^a
....PA-SGD, μ0 = 0.99, a0 = IOT
——PA-SGD, μo = 0.995, αφ = 10^a
20	40	60	80	100	120	140
# epochs
δ
O
10-1
50	100	150	200	250	300	350
# epochs
δ
O
10-1
10-2
10-3
10-4
Figure 3: Neural network training: (left) evolution of the mean squared error cost; we also include
a zoom-in, in log-log scale; (right) evolution of the multinomial logistic regression cost. The best
performing parameter configurations are presented. For low value of the acceleration parameter
μo, PA-SGD exhibits comparable convergence with respect to ADAM and AMSGrad. Our method
shows benefit when a larger acceleration parameter μ° is used, which allows for a lower final cost
albeit with slower initial convergence speed. AMSGrad has a comparable behavior with ADAM
with all parameter choices. For the logistic regression we note that towards the final stages of
convergence all methods exhibit large fluctuations of the cost function values.
----PA-SGD, μ0 = 0.9, α = IoT
——PA-SGD1 M = 0.95, a = IO-5
....ADAM, β1,0 = 0.9, a = IOT
— —ADAM, βιlQ = 0.95, α = 10^4
---AMSGradj βi,<s = 0.9, α = 1(Γ8
— —AMSGrad,仇 0 = 0.95, a = IO-4
50	100	150	200	250	300	350
# epochs

other tested methods, reaching within 3% - 4% of the optimal LS solution in around 20 epochs. It
shows the best performance for a larger acceleration parameters μ0 = 0.99 and α° = 10-3. For a
better understanding of the convergence characteristics, we also present the convergence behavior
as a function of the parameters μo in Figure 1, on the right, for two choices of the step size a0. The
choice of μo moderates the convergence rate and its influence is coupled to that of the step size. If
the step size is smaller, having a larger acceleration is beneficial, while for larger step sizes a very
large acceleration is detrimental. The simulations involving the logistic regression task, presented in
Figure 2, show a good behavior for our method, PA-SGD having a similar cost function value at 50
epochs as ADAM at 150. Our proposed method benefits from a smaller step size. We note that the
convergence plots of ADAM and AMSGrad almost overlap.
In both experiments our method convergences at a faster rate than ADAM and AMSGrad. This is
achieved for a smaller step size. Additionally, PA-SGD benefits from having a larger acceleration
parameter μ0.
4.2	Neural network experiments, non-convex cost functions
For the experimental setup involving the neural network training we perform a similar grid search as
before. We use a constant step size α using c = 0. For PA-SGD we set s = 0 in all simulations. Un-
der this setup we only compare against ADAM and AMSGrad. We vary the acceleration parameter
μo ∈ {0.9,0.95,0.99,0.995} for PA-SGD, momentum parameter β1,0 ∈ {0.9,0.95,0.99,0.995}
for ADAM and AMSGrad and step size α ∈ {10-2, 10-3, 10-4, 10-5, 10-6} for all algorithms.
In Figure 3 we compare the evolution for the proposed PA-SGD method for the two neural network
training tasks. The first experiment involving the matching of the MNIST numerical labels in terms
7
Under review as a conference paper at ICLR 2019
of MSE our method compares favorably with respect to ADAM and AMSGrad. For the classification
task, PA-SGD requires a lower step size and a larger acceleration to outperform the other methods
in the later stages of the convergence. This has the drawback of a slower initial convergence. Again,
as observed in both graphs, PA-SGD achieves a lower cost function value when a smaller step size
is coupled with a larger acceleration parameter μ0.
A good practical range for the configuration parameters proved to be α ∈ [10-6, 10-4] and μo ∈
[0.9, 0.99]. In practice we have observed that a lower step size works best when coupled with an
acceleration parameter μo closer to 0.99.
5	Conclusions
In this work, we proposed a preconditioned accelerated gradient method that combines Nesterov’s
accelerated gradient descent with a class of diagonal preconditioners, in a stochastic setting. We
provided a regret bound, and we proved the algorithm’s convergence for the minimization of convex
cost functions. We also showcased empirically the properties of the algorithm for the minimiza-
tion of convex and non-convex stochastic cost functions that occur usually in the context machine
learning. The proposed PA-SGD method compares favorably with current stochastic optimization
methods in terms of convergence speed while maintaining the same low computational complexity.
This makes it well suitable for solving large scale, high dimensional optimization tasks.
References
Li Deng, Jinyu Li, Jui-Ting Huang, Kaisheng Yao, Dong Yu, Frank Seide, Michael Seltzer, Geoff
Zweig, Xiaodong He, Jason Williams, et al. Recent advances in deep learning for speech research
at Microsoft. In ICASSP, pp. 8604-8608. IEEE, 2013.
Timothy Dozat. Incorporating Nesterov momentum into ADAM. In Conf. Learning Representa-
tions, 2016.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. J. Machine Learn. Res., 12(Jul):2121-2159, 2011.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In
Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,
pp. 315-323, 2011.
Richard HR Hahnloser, Rahul Sarpeshkar, Misha A Mahowald, Rodney J Douglas, and H Sebastian
Seung. Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit.
Nature, 405(6789):947, 2000.
Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, volume 1, pp. 3, 2017.
Melvin Johnson, Mike Schuster, Quoc V Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil
Thorat, Fernanda Viegas, Martin Wattenberg, Greg Corrado, et al. Google's multilingual neural
machine translation system: enabling zero-shot translation. arXiv preprint arXiv:1611.04558,
2016.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2014.
Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recog-
nition. IEEE Proc., 86(11):2278-2324, Nov 1998.
Yurii Nesterov. A method of solving a convex programming problem with convergence rate o(吉).
Soviet Mathematics Doklady, 1983.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathemati-
cal statistics, pp. 400-407, 1951.
8
Under review as a conference paper at ICLR 2019
Satyen Kale Sashank J. Reddi and Sanjiv Kumar. On the convergence of ADAM and beyond. In
ICLR, 2018.
Jurgen Schmidhuber. Deep learning in neural networks: An overview. Neural networks, 61:85-117,
2015.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initializa-
tion and momentum in deep learning. In ICML, 2013.
T. Tieleman and Geoffrey Hinton. Coursera: Neural networks for machine learning, lecture 6.5.
Technical report, 2012.
Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In
ICML, 2003.
9