Under review as a conference paper at ICLR 2019
Identifying Generalization Properties in Neu-
ral Networks
Anonymous authors
Paper under double-blind review
Ab stract
While it has not yet been proven, empirical evidence suggests that model general-
ization is related to local properties of the optima which can be described via the
Hessian. We connect model generalization with the local property of a solution
under the PAC-Bayes paradigm. In particular, we prove that model generalization
ability is related to the Hessian, the higher-order “smoothness” terms character-
ized by the Lipschitz constant of the Hessian, and the scales of the parameters.
Guided by the proof, we propose a metric to score the generalization capability of
the model, as well as an algorithm that optimizes the perturbed model accordingly.
1	Introduction
Deep models have proven to work well in applications such as computer vision (Krizhevsky et al.,
2012) (He et al., 2014) (Karpathy et al., 2014), speech recognition (Mohamed et al., 2012) (Hinton
et al., 2012), and natural language processing (Socher et al., 2013) (Graves, 2013) (McCann et al.,
2018). Many deep models have millions of parameters, which is more than the number of training
samples, but the models still generalize well (Huang et al., 2017).
On the other hand, classical learning theory suggests the model generalization capability is closely
related to the “complexity” of the hypothesis space, usually measured in terms of number of param-
eters, Rademacher complexity or VC-dimension. This seems to be a contradiction to the empirical
observations that over-parameterized models generalize well on the test data1. Indeed, even if the
hypothesis space is complex, the final solution learned from a given training set may still be simple.
This suggests the generalization capability of the model is also related to the property of the solution.
Keskar et al. (2017) and Chaudhari et al. (2017) empirically observe that the generalization ability of
a model is related to the spectrum of the Hessian matrix V2L(w*) evaluated at the solution, and large
eigenvalues of the V2L(w*) often leads to poor model generalization. Also, (KeSkar et al., 2017),
(Chaudhari et al., 2017) and (Novak et al., 2018b) introduce several different metrics to measure
the “sharpness” of the solution, and demonstrate the connection between the sharpness metric and
the generalization empirically. Dinh et al. (2017) later points out that most of the Hessian-based
sharpness measures are problematic and cannot be applied directly to explain generalization. In
particular, they show that the geometry of the parameters in RELU-MLP can be modified drastically
by re-parameterization.
Another line of work originates from Bayesian analysis. Mackay (1995) first introduced Taylor ex-
pansion to approximate the (log) posterior, and considered the second-order term, characterized by
the Hessian of the loss function, as a way of evaluating the model simplicity, or “Occam factor”.
Recently Smith & Le (2018) use this factor to penalize sharp minima, and determine the optimal
batch size. Germain et al. (2016) connect the PAC-Bayes bound and the Bayesian marginal likeli-
hood when the loss is (bounded) negative log-likelihood, which leads to an alternative perspective on
Occam’s razor. (Langford & Caruana, 2001), and more recently, (Harvey et al., 2017) (Neyshabur
et al., 2017) (Neyshabur et al., 2018) use PAC-Bayes bound to analyze the generalization behavior
of the deep models.
Since the PAC-Bayes bound holds uniformly for all “posteriors”, it also holds for some particular
“posterior”, for example, the solution parameter perturbed with noise. This provides a natural
1For example over-parameterized neural network can fit any function of sample size n, making the
Rademacher complexity large, but empirically those neural networks generalizes well. (Zhang et al., 2016)
1
Under review as a conference paper at ICLR 2019
(a) Loss landscape. The color on the loss surface shows
the pacGen scores. The color on the bottom plane
shows an approximated generalization bound.
(b) Sample distribution
(c) Predicted labels by the sharp minimum
Figure 1: Loss Landscape and Predicted Labels of a 5-layer MLP with 2 parameters. The sharp
minimum, even though it approximates the true label better, has some complex structures in its
predicted labels, while the flat minimum seems to produce a simpler classification boundary.2
(d) Predicted labels by the flat minimum
way to incorporate the local property of the solution into the generalization analysis. In particular,
Neyshabur et al. (2017) suggests to use the difference between the perturbed loss and the empirical
loss as the sharpness metric. Dziugaite & Roy (2017) tries to optimize the PAC-Bayes bound
instead for a better model generalization. Still some fundamental questions remain unanswered. In
particular we are interested in the following question:
How is model generalization related to local “smoothness” of a solution?
In this paper we try to answer the question from the PAC-Bayes perspective. Under mild assumptions
on the Hessian of the loss function, we prove the generalization error of the model is related to this
Hessian, the Lipschitz constant of the Hessian, the scales of the parameters, as well as the number
of training samples. The analysis also gives rise to a new metric for generalization. Based on this,
we can approximately select an optimal perturbation level to aid generalization which interestingly
turns out to be related to Hessian as well. Inspired by this observation, we propose a perturbation
based algorithm that makes use of the estimation of the Hessian to improve model generalization.
2	PAC-Bayes and Model Generalization
We consider the supervised learning in PAC-Bayes scenario (McAllester, 2003) (McAllester, 1998)
(McAllester, 1999) (Langford & Shawe-Taylor, 2002). Suppose we have a labeled data set S =
{si = (xi, yi) | i ∈ {1, . . . , n}, xi ∈ Rd, yi ∈ {0, 1}k}, where (xi, yi) are sampled i.i.d. from a
distribution Xi,yi ~ DS.
2The variables from different layers are shared so that the model only has two free parameters w1 and w2 .
The bound in (a) is approximated with η = 39 using inequality (8)
2
Under review as a conference paper at ICLR 2019
The PAC-Bayes paradigm assumes probability measures over the function class F : X → Y. In
particular, it assumes a “posterior” distribution Df as well as a “prior” distribution πf over the
function class F. We are interested in minimizing the expected loss, in terms of both the random
draw of samples as well as the random draw of functions:
L(Df, Ds)= Ef 〜Df Eχ,y 〜Dsl(f, x, y).
Correspondingly, the empirical loss in the PAC-Bayes paradigm is the expected loss over the draw
of functions from the posterior:
1n
L(Df, S )= Ef 〜Df n£l(f,Xi,yi)	(1)
i=1
PAC-Bayes theory suggests the gap between the expected loss and the empirical loss is bounded
by a term that is related to the KL divergence between Df and πf (McAllester, 1999) (Langford
& Shawe-Taylor, 2002). In particular, if the function f is parameterized as f(w) with w ∈ W,
when Dw is perturbed around any w, we have the following PAC-Bayes bound (Seldin et al., 2012a)
(Seldin et al., 2012b) (Neyshabur et al., 2017) (Neyshabur et al., 2018):
Theorem 1 (PAC-Bayes-Hoeffding Perturbation). Let l(f, x, y) ∈ [0, 1], and π be any fixed distri-
bution over the parameters W. For any δ > 0 and η > 0, with probability at least 1 - δ over the
draw of n samples, for any w and any random perturbation u,
Eu[L(w + u)] ≤ Eu[L(w + u)] + KL(W + ullπ)+log 1 + ɪ	⑵
η	2n
One may further optimize ηto get a bound that scales approximately as Eu[L(w +u)] . Eu[L(w +
u)] + 2 J KL(w+unπ)+log 1 (Seldin et al., 2012b).3 A nice property of the perturbation bound (2) is
it connects the generalization with the local properties around the solution w through some pertur-
bation u around w. In particular, suppose L(w*) is a local optimum, when the perturbation level of
U is small, Eu[L(w* + u)] tends to be small, but KL(W* + u∣∣∏) may be large since the posterior
is too “focused” on a small neighboring area around w*, and vice versa. As a consequence, We may
need to search for an “optimal” perturbation level for u so that the bound is minimized.
3	Main Result
While some researchers have already discovered empirically the generalization ability of the models
is related to the second order information around the local optima, to the best of our knowledge there
is no work on how to connect the Hessian matrix V2L(w) with the model generalization rigorously.
In this section we introduce the local smoothness assumption, as well as our main theorem.
It may be unrealistic to assume global smoothness properties for the deep models. Usually the
assumptions only hold in a small local neighborhood N eigh(W*) around a reference point W*. In
this paper we define the neighborhood set as
Neighκ (W*) = {W | |Wi - Wi* | ≤ κi ∀i}
where κi ∈ R+ is the “radius” of the i-th coordinate. In our draft we focus on a particular type of
radius Ki (w*) = γ∣w*∣ + G but our argument holds for other types of radius, too.
In order to geta control of the deviation of the optimal solution we need to assume in N eighγ,(W*),
the empirical loss function L in (1) is Hessian Lipschitz, which is defined as:
Definition 1 (Hessian Lipschitz). A twice differentiablefUnction f (∙) is P-Hessian LiPschitz if:
∀W1, W2, kV2f (W1) — V2f (W2 )k ≤ P∣∣W1 - W2∣∣,	(3)
where k ∙ k is the operator norm.
The Hessian Lipschitz condition has been used in the numeric optimization community to model
the smoothness of the second-order gradients (Nesterov & Polyak, 2006) (Carmon et al., 2018) (Jin
et al., 2018). In the rest of the draft we always assume the following:
3Since η cannot depend on the data, one has to build a grid and use the union bound.
3
Under review as a conference paper at ICLR 2019
(4)
Assumption 1. In NeighK(w ) the empirical loss L(W) defined in (1) is convex, and P-Hessian
Lipschitz.
For the uniform perturbation, the following theorem holds:
Theorem 2. Suppose the loss function l(f, x, y) ∈ [0, 1], and model weights are bounded |wi| +
Ki(W) ≤ Ti ∀i.. With probability at least 1 一 δ over the draw of n samples, for any W ∈ Rm such
that assumption 1 holds
Eu[L(W + u)] ≤ L(W) + O (J m + Pi lon σi +log 1
where Ui 〜 U(一宿心,σi) are i.i.d. uniformly distributed random variables, and
σi(w,η, Y) = min (j√≡(V2∕(W)∕31+ ρmi∕2κi(W)∕9), Ki(W)
Theorem 2 says if we choose the perturbation levels carefully, the expected loss of a uniformly
perturbed model is controlled. The bound is related to the diagonal element of Hessian (logarithmic),
the Lipschitz constant ρ of the Hessian (logarithmic), the neighborhood scales characterized by
κ (logarithmic), the number of parameters m, and the number of samples n. Also roughly the
perturbation level is inversely related to JVh L, suggesting the model be perturbed more along the
coordinates that are “flat”.4
Similar argument can be made on the truncated Gaussian perturbation, which is presented in Ap-
pendix B. In the next section we walk through some intuitions of our arguments.
4 Connecting Generalization and Hessian
Suppose the empirical loss function L(W) satisfies the local Hessian Lipschitz condition, then by
Lemma 1 in (Nesterov & Polyak, 2006), the perturbation of the function around a fixed point can be
bounded by terms up to the third-order,
L(W + u) ≤ L(W) + VL(w)tu + ；UTV2L(w)u + ；Pkuk3 for W + U ∈ NeighK(W~)	(5)
For perturbations with zero expectation, i.e., E[u] = 0, the linear term in (5), Eu [vL(w)tu] = 0.
Because the perturbation ui for different parameters are independent, the second order term can also
be simplified, since E/2uτV2L(w)u] = 2 Pi V2,iL(W)E[u2].
Considering (2),(5) and assumption 1, it is straightforward to see the bound below holds with prob-
ability at least 1 一 δ
Eu[L(w* + u)] ≤ L(w*) + 1 X V2,iL(W*)E[u2] + PE[kuk3] + KL(W* + ullπ)+log 1 + ɪ
2	,	6	η	2n
(6)
Suppose Ui 〜U(一σi, σi), and σi ≤ Ki(W) ∀i. That is, the “posterior” distribution of the model
parameters are uniform distribution, and the distribution supports vary for different parameters. We
also assume the perturbed parameters are bounded, i.e., |Wi| + Ki(W) ≤ τi ∀i.5 If we choose the
prior πtobe Ui 〜U(-Ti,Ti),andthen KL(w + u∣∣π) = Ei Tog(Ti∕σi).
The third order term in (6) is bounded by
PE[kuk3] ≤ 嚏E[kuk3] ≤ 嚏 X Ki(W)E[u2]=展 X Ki(W)σ2,
6	6	6	18
ii
4Unfortunately the bound in theorem 2 does not explain the over-parameterization phenomenon since when
m n the right hand side explodes.
5One may also assume the same τ for all parameters for a simpler argument. The proof procedure goes
through in a similar way.
4
Under review as a conference paper at ICLR 2019
where We use the inequality ∣∣uk2 ≤ m 1 ∣∣uk3 and m is the number of parameters. PlUging in (6),
we get
Eu [L(w + u)] ≤ L(W) + 1 X V2,∕(w)σ2 + ρm/2 X κi(w)σf + Pi log	+log 1 + ɪ
6	,	18	η	2n
(7)
Solve for σ that minimizes the right hand side, and we have the following lemma:
Lemma 3. Suppose the loss function l(f, x, y) ∈ [0, 1], and model weights are bounded |wi| +
κi (w) ≤ τi ∀i. Given any δ > 0 andη > 0, with probability at least 1 - δ over the draw of n
samples, for any w* ∈ Rm such that assumption 1 holds,
*	m *	m/2 + Pi log 券 + log1 η
Eu [L(W + U)] ≤ L(W ) +---------------ɪ--------+ ʒ-	(8)
η	2n
where Ui 〜U (―σ*,σ*) are i.i.d. uniformly perturbed random variables, and
σ:(W* ,η,γ)=min (jη(V2,iL(w*)∕3+1ρmi∕2κi(w*)∕9))，“(”)) .	⑼
In our experiment, we simply treat η as a hyper-parameter. Other other hand, one may further build
a weighted grid over η and optimize for the best η (Seldin et al., 2012b). That leads to Theorem 2.
Details of the proof are presented in the Appendix C and D.
5	On the Re-parameterization of RELU-MLP
Dinh et al. (2017) points out the spectrum of V2 L itself is not enough to determine the generalization
power. In particular, for a multi-layer perceptron with RELU as the activation function, one may
re-parameterize the model and scale the Hessian spectrum arbitrarily without affecting the model
prediction and generalization when cross entropy (negative log likelihood) is used as the loss and
W* is the “true” parameter of the sample distribution.
In general our bound does not assume the loss to be the cross entropy. Also we do not assume the
model is RELU-MLP. As a result we would not expect our bound stays exactly the same during
the re-parameterization. On the other hand, the optimal perturbation levels in our bound scales
inversely when the parameters scale, so the bound only changes approximately with a speed of
logarithmic factor. According to Lemma (3), if we use the optimal σ* on the right hand side of
the bound, V2L(w), ρ, and W* are all behind the logarithmic function. As a consequence, for
RELU-MLP, if we do the re-parameterization trick, the change of the bound is small.
In the next two sections we introduce some heuristic-based approximations enlightened by the
bound, as well as some interesting empirical observations.
6	An Approximate Generalization Metric
Assuming L(W) is locally convex around w*, so that v2,iL(w*) ≥ 0 for all i. If we look at Lemma
3,	for fixed m and n, the only relevant term is P- log T. Replacing the optimal σ*, and using
i	σi
|Wi | + κi (W) to approximate τi, we come up with PAC-Bayes based Generalization metric, called
pacGen,6
Ψκ(L,w*) = Xlog ((|w*| + Ki(w*))max QViLlwn + ρ(w*)√mκi(w*), K [*)
6 Even though we assume the local convexity in our metric, in application we may calculate the metric on
every points. When V2,i-L(w*) + ρ(w*)√mκi(w*) < 0 we simply treat it as 0.
5
Under review as a conference paper at ICLR 2019

(a) Test Loss - Train Loss (MNIST)
(b) Ψκ (MNIST)
(a) Test Loss - Train LoSS(CIFAR-10)
Figure 3: Generalization gap and Ψκ as a function of epochs on CIFAR-10 for different batch sizes.
SGD is used as the optimizer, and the learning rate is set as 0.01 for all configurations.
Figure 2: Generalization gap and Ψκ as a function of epochs on MNIST for different batch sizes.
(b) Ψκ (CIFAR-10)
A self-explained toy example is displayed in Figure 1. To calculate the metric on real-world data we
need to estimate the diagonal elements of the Hessian V2Lt as well as the Lipschitz constant P of the
Hessian. For efficiency concern We follow Adam (Kingma & Ba, 2014) and approximate V2j by
(VL[i])2 . Also we use the exponential smoothing technique with β = 0.999 as in (Kingma & Ba,
2014).
To estimate ρ, we first estimate the Hessian of a randomly perturbed model V2L(w + u), and then
approximate P by P = maxi R Mw+ui'-J L(WX . For the neighborhood radius K we use Y = 0.1
|ui|
and = 0.1 for all the experiments in this section.
We used the same model without dropout from the PyTorch example 7. Fixing the learning rate as
0.1, we vary the batch size for training. The gap between the test loss and the training loss, and the
metric Ψκ(L, w*) are plotted in Figure 2. We had the same observation as in (KeSkar et al., 2017)
that as the batch size grows, the gap between the test loss and the training loss tends to get larger.
Our proposed metric Ψκ (L, w*) also shows the exact same trend. Note we do not use LR annealing
heuristics as in (Goyal et al., 2017) which enables large batch training.
Similarly we also carry out experiment by fixing the training batch size as 256, and varying the learn-
ing rate. Figure 4 shows generalization gap and Ψκ(L, w*) as a function of epochs. It is observed
that as the learning rate decreases, the gap between the test loss and the training loss increases. And
the proposed metric Ψκ(L, w*) shows similar trend compared to the actual generalization gap. Sim-
ilar trends can be observed if we run the same model on CIFAR-10 (Krizhevsky, 2009) as shown in
Figure 3 and Figure 5.
7	A Perturbed Optimization Algorithm
Adding noise to the model for better generalization has proven successful both empirically and
theoretically (ZhU et al., 2018) (Hoffer et al., 2017) (Jastrzebski et al., 2017) (Dziugaite & Roy,
2017) (Novak et al., 2018a). Instead of only minimizing the empirical loss, (Langford & Caruana,
7https://github.com/pytorch/examples/tree/master/mnist
6
Under review as a conference paper at ICLR 2019
SGD is used as the optimizer, and the batch size is set as 256 for all configurations. As the learning
rates. SGD is used as the optimizer, and the batch size is set as 256 for all configurations.
2001) and (Dziugaite & Roy, 2017) assume different perturbation levels on different parameters,
and minimize the generalization bound led by PAC-Bayes for better model generalization. However
how to integrate the smoothness property of the local optima is not clear.
The right hand side of (2) has Eu [L(w + u)]. This suggests rather than minimizing the empirical
loss L(w), we should optimize the perturbed empirical loss Eu[L(w +u)] instead for a better model
generalization power.
We introduce a systematic way to perturb the model weights based on the PAC-Bayes bound. Again
we use the same exponential smoothing technique as in Adam (Kingma & Ba, 2014) to estimate
the Hessian V2L. The details of the algorithm is presented in Algorithm 1, where we treat η as a
hyper-parameter.
Even though in theoretical analysis Eu [VL ∙ u] = 0, in applications, VL ∙ U won t be zero especially
when we only implement 1 trial of perturbation. On the other hand, if the gradient VL is close to
zero, then the first order term can be ignored. As a consequence, in Algorithm 1 we only perturb
the parameters that have small gradients whose absolute value is below β2 . For efficiency issues
we used a per-parameter ρi capturing the variation of the diagonal element of Hessian. Also we
decrease the perturbation level with a log factor as the epoch increases.
We compare the perturbed algorithm against the original optimization method on CIFAR-10,
CIFAR-100 (Krizhevsky, 2009), and Tiny ImageNet8. The results are shown in Figure 6. We use the
Wide-ResNet (Zagoruyko & Komodakis, 2018) as the prediction model.9 The depth of the chosen
model is 58, and the widen-factor is set as 3. The dropout layers are turned off. For CIFAR-10
and CIFAR-100, we use Adam with a learning rate of 10-4, and the batch size is 128. For the
perturbation parameters we use η = 0.01, γ = 10, and =1e-5. For Tiny ImageNet, we use SGD
with learning rate 10-2, and the batch size is 200. For the perturbed SGD we set η = 100, γ = 1,
8https://tiny-imagenet.herokuapp.com/
9https://github.com/meliketoy/wide-resnet.pytorch/blob/master/networks/
wide_resnet.py
7
Under review as a conference paper at ICLR 2019
Algorithm 1 Perturbed OPT
Require: η, γ = 0.1, β1 = 0.999, β2 = 0.1, =1e-5.
1:
2:
3:
4:
5:
6:
7:
8:
Initialization: σ* — 0 for all i. t — 0, ho — 0
for epoch in 1, . . . , N do
for minibatch in one epoch do
for all i do
if t > 0 then
[i] J |ht[i] —ht-i[i]|
P[i],	Ilwt-Wt-ik
κ[i] J log(ljpoch) lwt-1[i]1 + e
σi — min
log(1+epoch)y∕η(ht [i]+p[i]∙κ[i])
• 1∣gt [i]∣<β2
1
9:	ut[i]〜 U (-σi, σi) (sample a set of perturbations)
10:	gt+1 J VwLt(Wt + Ut) (get stochastic gradients w.r.t. perturbed loss)
11:	ht+1 J β1ht + (1 - β1)gt2+1 (update second moment estimate)
12:	wt+1 J OPT(wt) (update w using off-the-shell algorithms)
13:	tJt+1
epoch
(a) CIFAR-10
epoch
(b) CIFAR-100
Figure 6: Training and testing accuracy as a function of epochs on CIFAR-10, CIFAR-100 and Tiny
ImageNet. For CIFAR, Adam is used as the optimizer, and the learning rate is set as 10-4. For
the Tiny ImageNet, SGD is used as the optimizer, and the learning rate is set as 10-2 . The dropout
method in the comparison uses 0.1 as the dropout rate. Details can be found in Appendix G.
--original-test
--ori gina.1—train.
--perturbed-test
--perturbed—train
--dropout-test
--dropout—train
epoch
(c) Tiny ImageNet
and =1e-5. Also we use the validation set as the test set for the Tiny ImageNet. We observe the
effect with perturbation appears similar to regularization. With the perturbation, the accuracy on the
training set tends to decrease, but the test on the validation set increases. The perturbedOPT also
works better than dropout possibly due to the fact that the it puts different levels of perturbation on
different parameters according to the local smoothness structures, while only one dropout rate is set
for the all the parameters across the model for the dropout method.
8 Conclusion
We connect the smoothness of the solution with the model generalization in the PAC-Bayes frame-
work. We prove that the generalization power of a model is related to the Hessian and the smoothness
of the solution, the scales of the parameters, as well as the number of training samples. In partic-
ular, we prove that the best perturbation level scales roughly as the inverse of the square root of
the Hessian, which mostly cancels out scaling effect in the re-parameterization suggested by (Dinh
et al., 2017). To the best of our knowledge, this is the first work that integrate Hessian in the model
generalization bound rigorously. It also roughly explains the effect of re-parameterization over the
generalization. Based on our generalization bound, we propose a new metric to test the model gen-
eralization and a new perturbation algorithm that adjusts the perturbation levels according to the
Hessian. Finally, we empirically demonstrate the effect of our algorithm is similar to a regularizer
in its ability to attain better performance on unseen data.
8
Under review as a conference paper at ICLR 2019
References
Yair Carmon, John C. Duchi, Oliver Hinder, and Aaron Sidford. Accelerated methods for nonconvex
optimization. SIAM Journal on Optimization, 2018.
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian
Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing gradient
descent into wide valleys. International Conference on Learning Representations (ICLR), 2017.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize
for deep nets. International Conference on Machine Learning (ICML), 2017.
Gintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. Uncertainty in
Artificial Intelligence (UAI), 2017.
Pascal Germain, Francis Bach, Alexandre Lacoste, and Simon Lacoste-Julien. Pac-bayesian theory
meets bayesian inference. Conference on Neural Information Processing Systems (NIPS), 2016.
Priya Goyal, Piotr Dollar, Ross B. Girshick, Pieter Noordhuis, LUkasz Wesolowski, AaPo Kyrola,
Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training ima-
genet in 1 hour. http://arxiv.org/abs/1706.02677, 2017.
Alex Graves. Generating sequences with recurrent neural networks. http://arxiv.org/abs/1308.0850,
2013.
Nick Harvey, ChristoPher Liaw, and Abbas Mehrabian. Nearly-tight VC-dimension bounds for
Piecewise linear neural networks. Conference on Learning Theory (COLT), 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. SPatial Pyramid Pooling in deeP con-
volutional networks for visual recognition. European Conference on Computer Vision (ECCV),
2014.
Geoffrey Hinton, Li Deng, Dong Yu, George Dahl, Abdel rahman Mohamed, NavdeeP Jaitly, An-
drew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara Sainath, and Brian Kingsbury. DeeP
neural networks for acoustic modeling in sPeech recognition. Signal Processing Magazine, 2012.
Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the gener-
alization gaP in large batch training of neural networks. International Conference on Neural
Information Processing Systems (NIPS), 2017.
Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
2017.
StaniSIaW JaStrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer,
Yoshua Bengio, and Amos Storkey. Three factors influencing minima in sgd.
https://arxiv.org/abs/1711.04623, 2017.
Chi Jin, Praneeth Netrapalli, and Michael I. Jordan. Accelerated gradient descent escapes saddle
points faster than gradient descent. Conference On Learning Theory (COLT), 2018.
Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, and Li Fei-
Fei. Large-scale video classification with convolutional neural networks. IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2014.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. Inter-
national Conference on Learning Representations (ICLR), 2017.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International
Conference on Learning Representations (ICLR), 2014.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Tech Report, 2009.
9
Under review as a conference paper at ICLR 2019
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep con-
volutional neural networks. International Conference on Neural Information Processing Systems
(NIPS), 2012.
John Langford and Rich Caruana. (not) bounding the true error. Advances in Neural Information
Processing Systems (NIPS), 2001.
John Langford and John Shawe-Taylor. Pac-bayes & margins. International Conference on Neural
Information Processing Systems (NIPS), 2002.
David J C Mackay. Probable networks and plausible predictions - a review of practical bayesian
methods for supervised neural networks. Network Computation in Neural Systems, 1995.
David A. McAllester. Some pac-bayesian theorems. Conference on Learning Theory (COLT), 1998.
David A. McAllester. Pac-bayesian model averaging. Conference on Learning Theory (COLT),
1999.
David A. McAllester. Simplified pac-bayesian margin bounds. Conference on Learning Theory
(COLT), 2003.
Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language
decathlon: Multitask learning as question answering. https://arxiv.org/abs/1806.08730, 2018.
A. Mohamed, G. E. Dahl, and G. Hinton. Acoustic modeling using deep belief networks. IEEE
Transactions on Audio, Speech, and Language Processing, 2012.
Yurii Nesterov and Boris Polyak. Cubic regularization of newton method and its global performance.
Mathematical Programming, 2006.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. Exploring gener-
alization in deep learning. Conference on Neural Information Processing Systems (NIPS), 2017.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A pac-bayesian approach to
spectrally-normalized margin bounds for neural networks. International Conference on Learning
Representations (ICLR), 2018.
Roman Novak, Yasaman Bahri, Daniel A. Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein.
Sensitivity and generalization in neural networks: an empirical study. International Conference
on Learning Representations (ICLR), 2018a.
Roman Novak, Yasaman Bahri, Daniel A. Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein.
Sensitivity and generalization in neural networks: an empirical study. International Conference
on Learning Representations (ICLR), 2018b.
Y. Seldin, F. Laviolette, and J. Shawe-Taylor. Pac-bayesian analysis of supervised, unsupervised,
and reinforcement learning. International Conference on Machine Learning (Tutorials), 2012a.
Yevgeny Seldin, Francois Laviolette, Nicolo Cesa-Bianchi, John Shawe-Taylor, and Peter Auer.
Pac-bayesian inequalities for martingales. IEEE Transactions on Information Theory, 2012b.
Samuel L. Smith and Quoc V. Le. A bayesian perspective on generalization and stochastic gradient
descent. International Conference on Learning Representations (ICLR), 2018.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng,
and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment
treebank. Conference on Empirical Methods in Natural Language Processing (EMNLP), 2013.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. British Machine Vision Confer-
ence (BMVC), 2018.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. International Conference on Learning Repre-
sentations (ICLR), 2016.
Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, and Jinwen Ma. The anisotropic noise in
stochastic gradient descent: Its behavior of escaping from minima and regularization effects.
https://arxiv.org/abs/1803.00195, 2018.
10
Under review as a conference paper at ICLR 2019
A Details of The Toy Example
This section discusses the details of the toy example shown in Figure (1). We construct a small 2-
dimensional sample set from a mixture of 3 Gaussians, and then binarize the labels by thresholding
them from the median value. The sample distribution is shown in Figure 1b. For the model we use
a 5-layer MLP with sigmoid as the activation and cross entropy as the loss. There are no bias terms
in the linear layers, and the weights are shared. For the shared 2-by-2 linear coefficient matrix, we
treat two entries as constants and optimize the other 2 entries. In this way the whole model has only
two free parameters w1 and w2 .
The model is trained using 100 samples. Fixing the samples, we plot the loss function with respect
to the model variables L(w1, w2), as shown in Figure 1a. Many local optima are observed even in
this simple two-dimensional toy example. In particular: a sharp one, marked by the vertical green
line, and a flat one, marked by the vertical red line. The colors on the loss surface display the values
of the generalization metric scores (pacGen) defined in Section 6. Smaller metric value indicates
better generalization power.
As displayed in the figure, the metric score around the global optimum, indicated by the vertical
green bar, is high, suggesting possible poor generalization capability as compared to the local opti-
mum indicated by the red bar. We also plot a plane on the bottom of the figure. The color projected
on the bottom plane indicates an approximated generalization bound, which considers both the loss
and the generalization metric.10 The local optimum indicated by the red bar, though has a slightly
higher loss, has a similar overall bound compared to the “sharp” global optimum.
On the other hand, fixing the parameter w1 and w2, we may also plot the labels predicted by the
model given the samples. Here we plot the prediction from both the sharp minimum (Figure 1c)
and the flat minimum (Figure 1d). The sharp minimum, even though it approximates the true label
better, has some complex structures in its predicted labels, while the flat minimum seems to produce
a simpler classification boundary.
B Truncated Gaus sian
Because the Gaussian distribution is not bounded but the inequality (5) requires bounded pertur-
bation, we first truncate the distribution. The procedure of truncation is similar to the proof in
(Neyshabur et al., 2018) and (McAllester, 2003).
Let Ui 〜N(0,σ2). Denote the truncated Gaussian as NKi(°，σ2). If Ui 〜NKi(0, σf) then
PKi(U) = W { p(Ui)
if |Ui| < κi(w)
o.w.
(10)
Now let’s look at the event
E = {U | |Ui| < κi (w) ∀ i}
(11)
If ∀i σi < √2 κ;-W), ,by union bound P(E) ≥ 1/2. Here erfT is the inverse Gaussian error
e	( 2m )
function defined as erf(x) = √2∏ ʃX e-t dt, and m is the number of parameters. Following a similar
procedure as in the proof of Lemma 1 in (Neyshabur et al., 2018),
KL(W + U∣∣π) ≤ 2(KL(w + u∣∣∏) + 1)	(12)
Suppose the coefficients are bounded such that Pi wi2 ≤ τ , where τ is a constant. Choose the prior
π as N(0, τI), and we have
KL(W + u∣∣π) ≤ ɪ(mlogT — ^Xlogσ2 — m + — X σi2 + 1)	(13)
ii
10the bound was approximated with η = 39 using inequality (8)
11
Under review as a conference paper at ICLR 2019
Notice that after the truncation the variance only becomes smaller, so the bound of (6) for the trun-
cated Gaussian becomes
Eu [L(W+U)] ≤L(W)+2 χ v2,iL(W)蟾+ρm6— χ Ki(W)蟾
ii
+ mlogT -Pilogσ2- m+TPiσ2 + 1+2logδ + η (14)
2η	2n
Again when L(W) is convex around W* such that V2L(w*) ≥ 0, solve for the best σi and we get
the following lemma:
Lemma 4. Suppose the loss function l(f, x, y) ∈ [0, 1], and model weights are bounded i Wi2 ≤ τ.
For any δ > 0 and η, with probability at least 1 - δ over the draw of n samples, for any W* ∈ Rm
such that assumption 1 holds,
Eu [L(W* + U)] ≤ L(W*) + m log T-Pi log： + 1 + 2log1 + 2n
where Ui 〜 NKi (0, σ*) are i.i.d. random variables distributed as truncated Gaussian,
σ* = min (j .	1 ”	, LKi(W*)∣
IV ηV2,iL(W*) + ρηmm1/2Ki(W*) + 1 √2erf-1(我)
(15)
(16)
and σi*2 is the i-th diagonal element in Σ*.
Again we have an extra term η, which may be further optimized over a grid to get a tighter bound.
In our algorithm we treat ηas a hyper-parameter instead.
C Proof of Lemma 3
Proof. We rewrite the inequality (7) below
Eu [L(w + u)] ≤ L(W) + 1 X V2,iL(W)σ2 + ρm/2 X Ki(WS + Pi log σi +log 1 + ɪ
6	,	18	η	2n
(17)
The terms related to σi on the right hand side of (17) are
6V2,L(W)σ2 + 哈Ki(W)σ2 -等	(18)
Since the assumption is V2,iL(W*) ≥ 0 for all i, V2∕(W) + pm1/2Ki(W)/3 > 0. Solving for σ
that minimizes the right hand side of (17), and we have
0*(W，η, Y) = min ( jη(V2∕(W)∕3Lm1∕2κiii, Ki(W)
(19)
The term 6 Pi V2,iL(W)σ2 + ρm1g- Pi Ki(W)σ2 on the right hand side of (7) is monotonically
increasing w.r.t. σ2, so
6 x ViL(W)σ*+Pm^ χ Ki(W)σ*2
ii
≤ X( 1 VKiL(W) + 若Ki
i
1
η(V2,iL(W)∕3 + ρm1∕2Ki(W)∕9)
m
2η
(20)
Combine the inequality (20), and the equation (19) with (17), and we complete the proof.
□
12
Under review as a conference paper at ICLR 2019
D Proof of Theorem 2
Proof. Combining (4) and (7), we get
Eu[L(W + u)] ≤ L(W) + 1 rm + Pi log σi + log δ
2n	η
+ 2n
The following proof is similar to the proof of Theorem 6 in (Seldin et al., 2012b). Note the η in
Lemma (3) cannot depend on the data. In order to optimize η we need to build a grid of the form
ηj = ej jn log ；
for j ≥ 0.
For a given value of Pi log σi, We pick n, such that
；$ Ilog (W+1!%.
Where bxc is the largest integer value smaller than x. Set δj = δ2-(j+1), and take a Weighted union
bound over ηj-s With Weights 2-(j+1), and We have With probability at least 1 - δ,
.	1 m	Pi log σi + log1 + log2 (2 + log ( Piooi 外 +1
Eu[L(W + u)] ≤ L(W) + 51/— + (1 + 1/e)t ----------------------------ʌ----------------------
2 n	2n
Simplify the right hand side and We complete the proof.
□
E Proof of Lemma 4
Proof. We first reWrite the inequality (14) beloW:
Eu[L(w* + U)] ≤ L(w*)+2 X V2,∕(w*)σ2 + (Pm/ X Ki(w*)σ2
ii
+ — logT — Pi log σ2 — — + T Piσ + 1 + 2log1 + ɪ
+	2	+ 2n
The terms related to σi on the right hand side of (14) is
p—1/2	, *\	1 \ 2
KMW ) + 行卜
log σ2
2η
Take gradients w.r.t. σ%, when V2L ≥ 0, We get the optimal σi,
_*
σi
1__________________Ki(w*)
ρηm1/2Ki(w*) + T , √2ef-1(2m)
Note the first term in (21) is monotonously increasing w.r.t. σi , so
(1 v2,iL(w*) + pmΓ~Ki(W*)+ 2⅞) σi2
≤ (1 v2L(w*) + PmLKi(W*)+ 2Tη) ηV2L(w*) + Pnm/2Ki(WyZJ
1
=—
2η
(21)
(22)
Summing over parameters and combine (14), we complete the proof.
□
13
Under review as a conference paper at ICLR 2019
F A Lemma ab out Eigenvalues of Hessian and Generalization
By extrema of the Rayleigh quotient, the quadratic term on the right hand side of inequality (5) is
further bounded by
UTV2L(w)u ≤ λmaχ(V2L(w))kuk2.	(23)
This is consistent with the empirical observations of Keskar et al. (2017) that the generalization
ability of the model is related to the eigenvalues of V2L(w). The inequality (23) still holds even
if the perturbations ui and uj are correlated. We add another lemma about correlated perturbations
below.
Lemma 5. Suppose the loss function l(f, x, y) ∈ [0, 1]. Let π be any distribution on the parameters
that is independent from the data. Given δ > 0 η > 0, with probability at least 1 - δ over the draw
of n samples, for any local optimal w* such that VL(w*) = 0, L(W) satisfies the local P-Hessian
Lipschitz condition in NeighK(W*, and any random perturbation u, s.t., |ui| ≤ Ki(W* ∀i, we
have
Eu[L(W* + U)] ≤ L(w*) + 2λmaχ (v2L(w*)) XE[u2] + PE[ku『]
i
KL(W* + u∣∣∏) + log 1 η
+---------η--------」+ 2n
(24)
Proof. The proof of the Lemma 5 is straightforward. Since VL(W*) = 0, the first order term is zero
at the local optimal point even if E[U] 6= 0. By extrema of the Rayleigh quotient, the quadratic term
on the right hand side of inequality (5) is further bounded by
UtV2L(w)u ≤ λmaχ (v2L(w)) ∣∣uk2.	(25)
Due to the linearity of the expected value,
E[utV2L(w)u] ≤ λmaχ (v2L(w)) XE[u2],	(26)
i
which does not assume independence among the perturbations Ui and Uj for i 6= j .
□
G PertOPT v.s. Dropout: An Empirical Comparison
This section contains several figures comparing dropout and the proposed perturbation algorithm.
Dropout can be viewed as multiplicative perturbation using Bernoulli distribution. It has already
been widely used in almost every deep models. For comparison we present results using the exact
same wide resnet architectures except the dropout layers are turned on or off. We report the accuracy
with dropout rate of 0.0, 0.1, 0.3, and 0.5 on CIFAR-10 and CIFAR-100. For Tiny ImageNet we
report the result with dropout rate being 0.0, 0.1, and 0.3. Again for the pertOPT algorithm all the
dropout layers are turned off.
The depth of the chosen wide resnet model (Zagoruyko & Komodakis, 2018) is 58, and the widen-
factor is set as 3. For CIFAR-10 and CIFAR-100, we use Adam with a learning rate of 10-4, and
the batch size is 128. For the perturbation parameters we use η = 0.01, γ = 10, and =1e-5. For
Tiny ImageNet, we use SGD with learning rate 10-2, and the batch size is 200. For the perturbed
SGD we set η = 100, γ = 1, and =1e-5. Also we use the validation set as the test set for the Tiny
ImageNet.
Figure (7), (8), and (9) show the accuracy versus epochs for training and validation in CIFAR-
10, CIFAR-100, and Tiny ImageNet respectively. It is pretty clear that with added dropout the
validation/test accuracy got boosted compared to the original method. For CIFAR-10, dropout rate
0.3 seems to work best compared to all the other dropout configurations. For CIFAR-100 and Tiny
14
Under review as a conference paper at ICLR 2019
—adam-val
—adam-train
—perturbed-val
—perturbed-train
—dropoutθ , 3-val
—dropoutθ , 3-train
—dropoutθ , 5-val
—dropoutθ , 5-train
—dropoutθ ,1-val
—dropoutθ ,1-train
Figure 7: Training and validation accuracy of PertOPT and Dropout on CIFAR-10. Adam-train, and
adam-val use the wide resnet model with 0 dropout rate. Perturbed-val and perturbed-train use the
same wide resnet with 0 dropout rate, but added perturbation according to algorithm 1.
—adam-val
—adam-train
—perturbed-val
—perturbed-train
—dropoutθ , 3-val
——dropoutθ , 3-train
—dropoutθ , 5-val
——dropoutθ , 5-train
—dropoutθ ,1-val
——dropoutθ ,1-train
Figure 8: Training and validation accuracy of PertOPT and Dropout on CIFAR-100. Adam-train,
and adam-val use the wide resnet model with 0 dropout rate. Perturbed-val and perturbed-train use
the same wide resnet with 0 dropout rate, but added perturbation according to algorithm 1.
ImageNet, dropout 0.1 seems to work better. This may be due to the fact that CIFAR-10 has less
training samples so more regularization is needed to prevent overfit.
Although both perturbedOPT and dropout can be viewed as certain kind of regularization, in all
experiments the perturbed algorithm shows better performance on the validation/test data sets com-
pared to the dropout methods. One possible explanation is maybe the perturbed algorithm puts
different levels of perturbation on different parameters according to the local smoothness structures,
while only one dropout rate is set for all the parameters across the model.
15
Under review as a conference paper at ICLR 2019
——original-test
—original-train
—perturbed-test
—perturbed-train
——dropoutθ , 3-test
—dropoutθ , 3-train
——dropoutθ ,1-test
—dropoutθ ,1-train
Figure 9: Training and validation accuracy of PertOPT and Dropout on Tiny ImageNet. Original-
train, and original-test use the wide resnet model with 0 dropout rate. Perturbed-test and perturbed-
train use the same wide resnet with 0 dropout rate, but added perturbation according to algorithm
1.
16