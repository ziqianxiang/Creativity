Under review as a conference paper at ICLR 2019
Efficient Exploration through Bayesian Deep
Q-Networks
Anonymous authors
Paper under double-blind review
Ab stract
We propose Bayesian Deep Q-Networks (BDQN), a principled and a practical
Deep Reinforcement Learning (DRL) algorithm for Markov decision processes
(MDP). It combines Thompson sampling with deep-Q networks (DQN). Thompson
sampling ensures efficient exploration-exploitation trade-off in high dimensions.
It is typically carried out through posterior sampling over the model parameters,
which makes it computationally expensive. To overcome this limitation, we directly
incorporate uncertainty over the value (Q) function. Further, we only introduce
randomness in the last layer (i.e. the output layer) of the DQN and use independent
Gaussian priors on the weights. This allows us to efficiently carry out Thompson
sampling through Gaussian sampling and Bayesian Linear Regression (BLR),
which has fast closed-form updates. The rest of the layers of the Q network are
trained through back propagation, as in a standard DQN. We apply our method to a
wide range of Atari games in Arcade Learning Environments and compare BDQN
to a powerful baseline: the double deep Q-network (DDQN). Since BDQN carries
out more efficient exploration, it is able to reach higher rewards substantially faster:
in less than 5M±1M interactions for almost half of the games to reach DDQN
scores while a typical run of DDQN is 50-200M. We also establish theoretical
guarantees for the special case when the feature representation is d-dimensional
and fixed. We show that the Bayesian regret of posterior sampling RL (PSRL) and
frequentist regret of the optimism in the face of uncertainly (OFU) after N time
step are upper bounded by O(d√N) which are tight up-to logarithmic factors. To
the best of our knowledge, these are the first model free theoretical guarantee for
continuous state-action space MDP, beyond the tabular setting.
1	Introduction
One of the central challenges in reinforcement learning (RL) is to design efficient exploration-
exploitation trade-off that also scales to high-dimensional state and action spaces. Recently deep RL
has shown good promise in being able scale to high-dimensional (continuous) spaces. These successes
are mainly demonstrated in simulated domains where exploration is considered to be inexpensive and
simple exploration strategies are deployed, e.g. ε-greedy which uniformly explores over all the actions
with ε probability. Such exploration strategies inherently inefficient for complex high-dimensional
environments. On the other hand, more sophisticated strategies have mostly been limited to low
dimensional MDPs. For example, OFU is only practical when the domain is small enough to be
represented with lookup tables for the Q-values (Jaksch et al., 2010; Brafman & Tennenholtz, 2003).
An alternative to optimism-under-uncertainty is Thompson Sampling (TS), a general sampling and
randomization approach (in both frequentist and Bayesian settings) (Thompson, 1933). Under the
Bayesian framework, Thompson sampling maintains a prior distribution over the environmental
models (i.e. models of reward and dynamics), and updating the posterior based on observations
collected. An action is chosen from the posterior distribution, the belief, such that it maximizes the
expected return. Thompson sampling has been observed to work significantly better than optimistic
approaches in many low dimensional settings such as contextual bandits (Chapelle & Li, 2011), small
MDPs (Osband et al., 2013) and also has strong theoretical bounds (Russo & Van Roy, 2014a;b;
Agrawal & Goyal, 2012; Osband et al., 2013; Abbasi-Yadkori & Szepesvari, 2015).
1
Under review as a conference paper at ICLR 2019
In the MDP setting, (model-based) Thompson Sampling involves sampling the parameters of reward
and dynamics models, performing MDP planning using the sampled model and then computing the
policy (Strens, 2000; Osband et al., 2013; Osband & Van Roy, 2014b;a). However, the computational
costs of posterior sampling and planning scales as cubic in the problem dimension, which makes it
intractable for large MDPs. Therefore, some form of function approximation is required to scale
Thompson Sampling to high dimensions, and this can be either of the model, the Q-value function, or
the policy. To address this, Osband et al. (2014) introduced randomized least-squares value iteration
(RLSVI) which combines linear value function approximation with Bayesian regression to directly
sample the value-function weights from a distribution. The authors prove a regret bound for this
approach in tabular MDPs. This has been extended to continuous spaces by Osband et al. (2016),
where deep networks are used to approximate the Q function. Through a bootstrapped-ensemble
approach, several deep-Q network (DQN) models are trained in parallel to approximate the posterior
distribution. Other works use the posterior distribution over the parameters of each node in the network
and employ variational approximation (Lipton et al., 2016b) or use noisy networks (Fortunato et al.,
2017). These approaches significantly increase the computation cost compared to the standard DQN.
For instance, the bootstrapped-ensemble incurs a computation overhead that is linear in the number
of bootstrap models. Moreover, despite principled design of the methods in the above works, they
only produced modest gains over DQN in empirical studies.
Contribution 1 - Design of BDQN: We introduce Bayesian Deep Q-NetWork (BDQN), a ThomPson-
sampling algorithm for deep RL. It is a simple approach that extends randomized least-squares value
iteration (Osband et al., 2014) to deep netWorks. We introduce stochasticity only in the last layer
of the Q-netWork using independent Gaussian priors on the Weights. This alloWs us to do efficient
approximate Thompson sampling1 using Bayesian linear regression (BLR), Which has fast closed-
form updates and sampling from the resulting posterior distribution. The rest of the Q-netWork is
trained through usual back propagation.
Contribution 2 - Strong empirical results for BDQN: We test BDQN on a Wide range of Atari
games (Bellemare et al., 2013; Machado et al., 2017), and compare our results to a poWerful baseline:
Double DQN (DDQN) (Van Hasselt et al., 2016) a bias-reduced extension of DQN. BDQN and
DDQN use the same netWork architecture, and folloW the same target objective, and differ only in
the Way they select actions: DDQN uses ε-greedy exploration While BDQN performs (approximate)
Thompson sampling.
We found that BDQN is able to reach much higher cumulative reWards, and also at a higher speed,
compared to DDQN on all the tested games. We also found that BDQN can be trained With much
higher learning rates compared to DDQN. This is intuitive since BDQN has better exploration strategy.
The cumulative reWard (score) for BDQN at the end of training improves by a median of 300% With
a maximum of 80K% in these games. Also, BDQN has 300% ± 40% (mean and standard deviation)
improvement over these games on area under the performance measure. This can be considered as a
surrogate for sample complexity and regret. Indeed, no single measure of performance provides a
complete picture of an algorithm, and We present detailed experiments in Section 5.
In terms of computational cost, BDQN is only slightly more expensive compared to DQN and DDQN.
For the DQN in Atari games, this is the cost of inverting a 512 × 512 matrix every 100,000 time steps,
Which is negligible. On the other hand, more sophisticated Bayesian RL techniques are significantly
more expensive and have not lead to large gains over DQN and DDQN (Osband et al., 2016).
Contribution 3 - Tight Bayesian and frequentist regret upper bounds for continuous MDPs:
We establish theoretical guarantees for the special case When the feature representation is fixed (i.e.
all layers except the last), and not learnt. We consider episodic MDPs With continuous space of states
and actions such that the Q-function is a linear function of a given d-dimensional feature map. We
shoW that When PSRL and OFUare deployed, respectively, the Bayesian regret and frequentist regret
after N time step are upper bounded by Oe(d√N), which is shown to be tight up-to logarithmic
factors. Since linear bandits are a special case of episodic continuous MDPs (With horizon length
1), it implies that our regret bounds are tight in the dimension d and in the number of episodes. The
Bayesian bound matches the Bayesian regret bound of linear bandits (Russo & Van Roy, 2014a)
and the frequentist bound matches the frequentist regret bound for linear bandits (Abbasi-Yadkori
1 BDQN approximates the posterior resulting in approximating Thompson sample. In the remaining, we
state Thompson Sampling and its approximation as Thomson samples unless we specify.
2
Under review as a conference paper at ICLR 2019
et al., 2011). To the best of our knowledge, these are the first model free theoretical guarantee for
continuous MDPs beyond the tabular setting.
Thus, our proposed approach has several desirable features - faster learning and better sample Com-
plexity due to targeted exploration, negligible computational overhead due to simplicity, significant
improvement in experiments, and tight theoretical bounds.
2	THOMPSON S AMPLING VS ε-GREEDY AND B OLTZMANN EXPLORATION
In a value approximation RL algorithm, there are different ways to manage the exploration-
exploitation trade-off. DQN uses a naive ε-greedy for exploration, where with ε probability it
chooses a random action and with 1 - ε probability it chooses the greedy action based on the esti-
mated Q function. Note that there are only point estimates of the Q function in DQN. In contrast,
our proposed Bayesian approach BDQN maintains uncertainties over the estimated Q function, and
employs it to carry out Thompson Sampling based exploration. Here, we demonstrate the fundamental
benefits of Thompson Sampling over ε-greedy and Boltzmann exploration strategies using simple
examples. In Table 1, we list the three strategies and their properties.
ε-greedy is the simplest exploration strategy since it is uniformly random over all the non-greedy
actions. Boltzmann exploration is an intermediate strategy since it uses the estimated Q function to
sample the actions. However, it does not maintain uncertainties over the Q function estimation. In
contrast, Thompson sampling also incorporates uncertainties over Q estimation and utilizes most
information for exploration strategy.
Consider the example in Figure 1(a) with current estimates and uncertainties of the Q function over
different actions. ε-greedy is wasteful since it assigns uniform probability to explore over 5 and
6, which are obviously sub-optimal when the uncertainty estimates are available. In this setting, a
possible remedy is Boltzmann exploration since it assigns lower probability to actions 5 and 6 but
randomizes with almost the same probabilities over the remaining actions.
Table 1: Characteristics of Thompson Sampling, ε-greedy, and Boltzmann exploration what informa-
tion they use for exploration
Strategy	∣ Greedy-Action ∣ Estimated Q-values ∣ Estimated uncertainties
ε-greedy	∣	✓	∣	X	∣	X
Boltzmann exploration ∣	✓	∣	✓	∣	X
Thompson Sampling ∣	✓	∣	✓	∣	✓
However, Boltzmann exploration is sub-optimal in settings where there is high variances. For example
if the current Q estimate is according to Figure 1(b), then Boltzmann exploration assigns almost
equal probability to actions 5 and 6, even though action 6 has much higher uncertainty and needs to
be explored more. Thus, both ε-greedy and Boltzmann exploration strategies are sub-optimal since
they do not maintain an uncertainty estimate over the Q function estimation. In contrast, Thompson
sampling uses both estimated Q function and its uncertainty estimates to carry out the most efficient
exploration.
3 Bayesian Deep Q-Networks
Consider an MDP M as a tuple hX, A, P, P0, R, γi, with state space X, action space A, transition
kernel P , initial state distribution P0 , accompanied with reward function of R, and discount factor
0 ≤ γ < 1. In value based model free RL, the core of most prominent approaches is to learn the
Q-function through minimizing the Bellman residual (Lagoudakis & Parr, 2003; Antos et al., 2008)
L(Q) = Enh(Q(x, a) — r - γQ(x0, ^))2]	⑴
and temporal difference (TD) update (Tesauro, 1995) where the tuple (x, a, r, x0) consists of a
consecutive experiences under a behavior policy π. Mnih et al. (2015) carries the same idea, and
propose DQN where the Q-function is parameterized by a deep network and a = arg max。，Q(x0, a0).
In order to reduce the bias of the estimator, DQN utilizes a target network Qtarget , target value
3
Under review as a conference paper at ICLR 2019
9∙-4∙s6,I00u∩ PW3o,p。⅛日-9H
⑶
9∙-4∙s6,I00u∩ PW3o,p。⅛日-9H
(b)
Figure 1: Thompson Sampling vs ε-greedy and Boltzmann exploration. (a) ε-greedy is wasteful
since it assigns uniform probability to explore over 5 and 6, which are obviously sub-optimal when
the uncertainty estimates are available. Boltzmann exploration randomizes over actions even if the
optimal action is identifies. (b) Boltzmann exploration does not incorporate uncertainties over the
estimated action-values and chooses actions 5 and 6 with similar probabilities while action 6 is
significantly more uncertain. Thomson Sampling is a simple remedy to all these issues.
Algorithm 1 BDQN
1:	Initialize θ, θtarget, Wa, wiaarget, Covα ∀a, and B
2:	for episode = 1,2,3. . . do
3:	for t = 1 to the end of episode do
4:	Draw sample Wa ~ N (Wltaarget, Cova) ∀α every Tsample
5:	Set θtarget — θ every Ttarget
6:	Update Watarget and Cova, ∀a using B experiences every T Bayes target
7:	Execute at = arg maxa0 Wa>0 φθ (xt), observe reward rt, successor State xt+1
8:	Store transition (xt, at, rt, xt+1) in the replay buffer
9:	Sample a random minibatch of transitions (xτ , aτ, rτ, xτ+1) from replay buffer
rτ	terminal xτ +1
rτ + wt0arget > Φθtarget(xτ+1), a := argmaxaoWaOφθ(xτ+1 )non-terminal xτ+1
11:	θ J θ - η ∙ Vθ(yτ - WaTφθ(xτ))2
10:	yτ J
y = r + YQtarget(χ0, a), and approaches the regression on the empirical estimates of the loss
L(Q, Qtarget);
L(Q, Qtarget) = Eπ h(Q(x,a)-y)2i	(2)
i.e., Lb(Q, Qtarget). A DQN agent, once in a while updates the Qtarget network and sets it to the Q
network, follows the regression in Eq.2 with the new target value and provides a biased estimator
of the target. To mitigate the bias in this estimator, Van Hasselt et al. (2016) proposes DDQN and
instead use a = arg maxao Qtarget (χ0, a0). We deploy this approach for the rest of this paper.
DQN architecture consists of a deep neural network where the Q-function is approximated as a
linear function of the feature representation layer φθ(x) ∈ Rd parametrized by θ, i.e., for all pairs
of state-action ∀a ∈ A, x ∈ X, we have Q(x, a) = φθ(x)a Wa with Wa ∈ Rd, the output layer.
Consequently, the target model has the same architecture as the Q, and consists of φθtarget (∙) ∈ Rd,
the feature representation of target network, and Wtargeta , ∀a ∈ A the target weight. For a given
tuple of experiences (x,a,r, x0), and a = arg maxaoφθtarget Waarget
Q(x, a) = φθ(x)αWa → y := r + γφθtarget (x0)Wtarget^
The regression in Eq. 2 induces linear regression in the learning of the output layer, i.e., Wa ’s. In
this work, we utilize the DQN architecture and instead propose to use BLR (Rasmussen & Williams,
4
Under review as a conference paper at ICLR 2019
2006) in learning of the output layer. Through BLR, We efficiently approximate the distribution
over the Q-ValUes, capture the uncertainty over the Q-fucntion estimation, and design a efficient
exploration and exploitation strategy using Thompson Sampling.
By deploying BLR on the feature representation layer, we ap-
proximate the posterior distribution of each w。，resulting in the
posterior distribution of the Q-function. As in BLR methods,
we maintain a Gaussian prior N(0, σ2I) with the target value
y 〜 w> φθ (x) + e for each weight vector where e 〜N(0, σ2)
is an i.i.d. Gaussian noise.
Given a experience replay buffer D = {xT, aτ,yτ}D=ι, We
construct |A| (number of actions) disjoint datasets Da for
each action with a「= a. For each action a, we construct
a matrix Φg ∈ Rd×lDal, the concatenation of feature vectors
{φθ (χi)}iD1l, and yα ∈ R1Da |, the concatenation of target val-
ues in set Da. Finally, we approximate the posterior distribution
of Wa as follows:
Figure 2: BDQN deploys Thomp-
son Sampling to, sample Wa ∀a ∈
A around the empirical mean Wa
with Wa the underlying parameter
of interest.
Wa 〜N(Wa,Cova) , wa :=	Covaφaya, Cova :=(^MM> +	1	⑶
Fig. 2 expresses that the covariance matrix induces an ellipsoid around the estimated mean of the
approximated posterior and samples drawn through Thompson Sampling are mainly close to this mean.
A sample of Q(x, a) is Wa>φθ(x) where Wa is drawn from the posterior distribution Fig. 2. In BDQN,
every T sample times step, we draw a new Wa, ∀a ∈ A and follow the resulting policy, i.e., aTS :=
maxa Wa>φθ(x). We simultaneously train the feature network under the loss (yτ - Wa> φθ (xτ))2
with xτ , aτ , yτ experiences from the replay buffer i.e.
θ 一 θ - η ∙ Vθ(yτ - ∖w>tφθ(XT)]aJ2	⑷
We update the target network every T target steps and set θtarget to θ. With the period of T Bayes target,
we update the posterior distribution using a minibatch of B randomly chosen experiences in the
replay buffer, and set the Wlaarget = Wa, ∀a ∈ A which is the mean of the posterior distribution(
more details in Section A.5. We describe BDQN algorithm in Alg. 1.
4 Bayesian Regret B ound
Algorithm 2 PSRL		Algorithm 3 OFU	
1:	Input: the prior and likelihood	1:	Input: σ, λ and δ
2:	for episode t= 1,2,. . . do	2:	for episode = 1,2,. . . do
3:	ωt 〜posterior distribution	3:	choose optimistic πet in Ct-1 (δ)
4:	for h = 0 to the end of episode do	4:	
5:	Follow πt policy induced by ωt	5:	for h = 1 to the end of episode do Follow πet policy
6:	Update the posterior	6:	Update the confidence Ct(δ)
In this section we provide the analysis of Bayesian regret upper bound of PSRL Alg. 2 and frequentis
regret upper bound of optimism Alg. 3 when the feature representation is given and fixed. Consider
a finite horizon stochastic MDP M := hX, A, P, P0, R, γ, Hi, an MDP with horizon length H and
0 ≤ γ ≤ 1. We consider a class of MDPs where the optimal Q-function is a linear transformation
of φ(∙, ∙) := X ×A→ Rd, i.e., Q∏: (x, a) := φ(x, a)>ωα, ∀x, a ∈ X ×A, where ωα ∈ Rd and
πα : X → A as πα (x) := arg maXa∈A Q∏* (x, a). Let Vω denote the corresponding value function.
The following is the generative model of the environment;
R := ΨR = Φ(x□H}, a{1:H})ωa+Ψν(x{1:H}, a{1:H}), where , Ψ :
1	γ	γ2 . . . γH-1
0	1	Y	…γH-2
0 ... ... ...	1
5
Under review as a conference paper at ICLR 2019
The vector R ∈ RH is the random vector of rewards in an episode and R ∈ RH the corresponding
per step return. x{1:H}, a{1:H} is the sequence of states and actions, the matrix 小(/{1:"},。{1:H}) ∈
RH×d is row-wise concatenation of their features and V(X{1:H},a{1:H}) ∈ Rd the noise. Alg. 2
maintains a prior over the vector ω* and updating the posterior over time. At the beginning of an
episode t, the agent draws ωt from the posterior, and follows its induced policy πt where a :=
arg maxa∈A φ> (x, a)ωt, ∀x ∈ X . Alg. 3, at the beginning of t’th episode, exploits the so-far
collected samples and estimates ω* up to a high probability confidence interval Ct-ι i.e., ω* ∈ Ct
with high probability. At each time step h, given a state xth, the agent follows the optimistic policy;
πet(xth) = arg maxa∈A maxω∈Ct-1 φ>(Xth, a)ω. Through exploration and exploitation, we show
that the confidence set Ct shrinks with the rate of O (1∕√t) resulting in less and less per step regret
(Lemma 1 in Appendix B). Similar to the linear bandit (Abbasi-Yadkori et al., 2011), consider the
following generic assumptions,
•	The noise vector ν is sub-Gaussian vector with parameter σ . (Assumption 1 in Appendix B)
•	∣∣ω*k2 ≤ Lω and trace (Φ(∙)Φ(∙)>) ≤ L,a.s.
•	Maximum expected cumulative reward of an episode condition on the states of that episode
is in [0, 1].
For any prior and likelihood satisfying the mentioned assumptions, we have;
Theorem 1	(Bayesian Regret). For an episodic MDP with episode length H, γ = 1, feature mapping
φ(x, a) ∈ Rd, the Thompson sampling on ω, Alg. 2, after T episodes, guarantees;
一 T	"I
BayesRegT ： = E X 收* -嘴*] = O (d√TΗlog(T))
t
Proof is Appendix B.2. Note that N = TH is the number of agent-environment interactions.
Theorem 2	(Frequentist Regret). For an episodic MDP with episode length H, γ = 1, feature
mapping φ(x, a) ∈ Rd, the optimism on ω, Alg. 3, after T episodes, guarantees;
一 T	1
RegT : = E X [v∏ω** — Vet*] ∣ω* = O (d√THlog(T))
t
Proof is given in the Appendix B.1.
Remark 1. For any discount factor γ, 0 ≤ Y ≤ 1, both theorems 1 and 2 hold if we replace √H
with a smaller constant ∣[1, γ, γ2 *, . . . , γH-1]∣2 in the theorem statements (see Section B.3).
These bounds are similar to those in linear bandits (Abbasi-Yadkori et al., 2011; Russo & Van Roy,
2014a) and linear quadratic control (Abbasi-Yadkori & Szepesvari, 2011), i.e. O(d√T). Since
for H = 1, this problem reduces to linear bandit and for linear bandit the lower bound is Ω(d√T)
therefore, our bound is order optimal in d and T.
5	Experiments
We apply BDQN on a variety of Atari games using the Arcade Learning Environment (Bellemare et al.,
2013) through OpenAI Gym2 (Brockman et al., 2016). As a baseline, we run the DDQN algorithm
and evaluate BDQN on the measures of sample complexity and score. All the implementations are
coded in MXNet framework (Chen et al., 2015). The details on architecture, Appendix A.1, learning
rate Appendix A.3, computation A.4. In Appendix A.2 we describe how we spend less than two days
on a single game for the hyper parameter choices which is another evidence of significance of BDQN.
2Each input frame is a pixel-max of the two consecutive frames. We detailed the environment setting in the
implementation code
6
Under review as a conference paper at ICLR 2019
Table 2: Comparison of scores and sample complexities (scores in the first two columns are average
of 100 consecutive episodes). The scores of DDQN+ are the reported scores of DDQN in Van Hasselt
et al. (2016) after running it for 200M interactions at evaluation time where the ε = 0.001. Bootstrap
DQN (Osband et al., 2016), CTS, Pixel, Reactor (Ostrovski et al., 2017) are borrowed from the
original papers. For NoisyNet (Fortunato et al., 2017), the scores of NoisyDQN are reported. Sample
complexity, SC: the number of samples the BDQN requires to beat the human score (Mnih et al.,
2015)(“ - ” means BDQN could not beat human score). SC+: the number of interactions the BDQN
requires to beat the score of DDQN+.
Game	BDQN DDQN DDQN+			Bootstrap NoisyNet		CTS	Pixel Reactor Human			SC	SC+ I Step	
Amidar	5.52k	0.99k	0.7k	1.27k	1.5k	1.03k	0.62k	1.18k	1.7k	22.9M	4.4M	100M
Alien	3k	2.9k	2.9k	2.44k	2.9k	1.9k	1.7k	3.5k	6.9k	-	36.27M	100M
Assault	8.84k	2.23k	5.02k	8.05k	3.1k	2.88k	1.25k	3.5k	1.5k	1.6M	24.3M	100M
Asteroids	14.1k	0.56k	0.93k	1.03k	2.1k	3.95k	0.9k	1.75k	13.1k	58.2M	9.7M	100M
Asterix	58.4k	11k	15.15k	19.7k	11.0	9.55k	1.4k	6.2k	8.5k	3.6M	5.7M	100M
BeamRider	8.7k	4.2k	7.6k	23.4k	14.7k	7.0k	3k	3.8k	5.8k	4.0M	8.1M	70M
BattleZone	65.2k	23.2k	24.7k	36.7k	11.9k	7.97k	10k	45k	38k	25.1M	14.9M	50M
Atlantis	3.24M	39.7k	64.76k	99.4k	7.9k	1.8M	40k	9.5M	29k	3.3M	5.1M	40M
DemonAttack	11.1k	3.8k	9.7k	82.6k	26.7k	39.3k	1.3k	7k	3.4k	2.0M	19.9M	40M
Centipede	7.3k	6.4k	4.1k	4.55k	3.35k	5.4k	1.8k	3.5k	12k	-	4.2M	40M
BankHeist	0.72k	0.34k	0.72k	1.21k	0.64k	1.3k	0.42k	1.1k	0.72k	2.1M	10.1M	40M
CrazyClimber	124k	84k	102k	138k	121k	112.9k	75k	119k	35.4k	0.12M	2.1M	40M
ChopCmd3	72.5k	0.5k	4.6k	4.1k	5.3k	5.1k	2.5k	4.8k	9.9k	4.4M	2.2M	40M
Enduro	1.12k	0.38k	0.32k	1.59k	0.91k	0.69k 0.19k		2.49k	0.31k	0.82M	0.8M	30M
Pong	21	18.82	21	20.9	21	20.8	17	20	9.3	1.2M	2.4M	5M
Baselines: We implemented DDQN and BDQN exactly the same way as described in Van Hasselt
et al. (2016). We also attempted to implement a few other deep RL methods that employ strategic
exploration, e.g., (Osband et al., 2016; Bellemare et al., 2016). Unfortunately we encountered
several implementation challenges where neither code nor the implementation details was publicly
available. Despite the motivation of this work on sample complexity, since we do not have access to
the performance plots of these methods, the least is to report their final scores. To try to illustrate
the performance of our approach we instead, extracted the best reported scores from a number of
state-of-the-art deep RL methods and include them in Table 2, which is the only way to bring a
comparison. We compare against DDQN, as well as DDQN+ which is the reported scores of DDQN
in Van Hasselt et al. (2016) at evaluation time where the ε = 0.001. Furthermore, we compared
against scores of Bootstrap DQN (Osband et al., 2016), NoisyNet (Fortunato et al., 2017), CTS,
Pixel, Reactor (Ostrovski et al., 2017) which are borrowed from the original papers. For NoisyNet,
the scores of NoisyDQN are reported. We also provided the sample complexity, SC: the number of
interactions BDQN requires to beat the human score (Mnih et al., 2015)(“ - ” means BDQN could
not beat human score) and SC+ : the number of interactions the BDQN requires to beat the score
of DDQN+ . Note that these are not perfect comparisons, as there are additional details that are not
included in the papers, i.e. it is hard to just compare the reported results (an issue that has been
discussed extensively recently, e.g. (Henderson et al., 2017)).4. Moreover, when the regret analysis of
an algorithm is considered, no evaluation phase required, and the reported results of BDQN are those
while exploration. It is worth noting that, the scores during evaluation are much higher than those
during the exploration and exploitation period, Appendix A.8.
Results: The results are provided in Fig. 3. We observe that BDQN significantly improve the sample
complexity of DDQN and reaches the highest score of DDQN in much fewer number of interactions
than DDQN needs. We expected BDQN, due to it better exploration-exploitation strategy, to improve
the regret and enhance the sample complexity, but we also observed a significantly improvement
in scores. It worth noting that since BDQN is designed to minimize the regret, and the study in
Fig. 3 also for sample complexity analysis, either of the reported BDQN and DDQN scores are while
4To further reproducibility, we released our codes and trained models. Since DRL experiments are expensive,
we also have released the recorded arrays of returns. We also implemented bootstrapped DQN (Osband et al.,
2016) and released the code but we were not able to reproduce their results beyond the performance of random
policy
7
Under review as a conference paper at ICLR 2019
exploration. For example, DDQN gives score of 18.82 during the learning phase, but setting ε to zero,
it mostly gives the score of 21. In addition to the Table 2, we also provided the score ratio as well
area under the performance plot ratio comparisons in Table 3.
Figure 3: The comparison between DDQN and BDQN
For the game Atlantis, DDQN+ gives score of 64.67k during the evaluation phase, while BDQN
reaches score of 3.24M after 20M interactions. As it is been shown in Fig. 3, BDQN saturates
for Atlantis after 20M interactions. We realized that BDQN reaches the internal OpenAIGym limit
of max_episode, where relaxing it improves score after 15M steps to 62M, Appendix A.7. We
observe that BDQN immediately learns significantly better policies due to its efficient explore/exploit
in a much shorter period of time. Since BDQN on game Atlantis promise a big jump around time step
20M, we ran it five more times in order to make sure it was not just a coincidence Appendix A.7
Fig. 7. For the game Pong, we ran the experiment for a longer period but just plotted the beginning of
it in order to observe the difference. Due to cost of deep RL methods, for some games, we run the
experiment until a plateau is reached.
6	Related Work
The complexity of the exploration-exploitation trade-off has been deeply investigated in RL literature
for both continuous and discrete MDPs (Kearns & Singh, 2002; Brafman & Tennenholtz, 2003;
Asmuth et al., 2009; Kakade et al., 2003; Ortner & Ryabko, 2012). Jaksch et al. (2010) investigates
the regret analysis of MDPs with finite state and action where Optimism in Face of Uncertainty (OFU)
principle is deployed to guarantee a regret upper bound, while Ortner & Ryabko (2012) relaxes it
to a continuous state space and propose a sublinear regret bound. Azizzadenesheli et al. (2016a)
deploys OFUand propose a regret upper bound for Partially Observable MDPs (POMDPs) using
spectral methods (Anandkumar et al., 2014). Furthermore, Bartok et al. (2014) tackles a general case
of partial monitoring games and provides minimax regret guarantee. For linear quadratic models
OFU is deployed to provide an optimal regret bound (AbbaSi-Yadkori & Szepesvari, 2011).
In multi-arm bandit, there are compelling empirical pieces of evidence that Thompson Sampling
sometimes provides better results than optimism-under-uncertainty approaches (Chapelle & Li, 2011),
while also the performance guarantees are preserved (Russo & Van Roy, 2014a; Agrawal & Goyal,
2012). A natural adaptation of this algorithm to RL, posterior sampling RL (PSRL) Strens (2000)
also shown to have good frequentist and Bayesian performance guarantees (Osband et al., 2013;
Abbasi-Yadkori & Szepesvari, 2015).
Even though the theoretical RL addresses the exploration and exploitation trade-offs, these problems
are still prominent in empirical reinforcement learning research (Mnih et al., 2015; Abel et al., 2016;
Azizzadenesheli et al., 2016b). On the empirical side, the recent success in the video games has
sparked a flurry of research interest. Following the success of Deep RL on Atari games (Mnih et al.,
2015) and the board game Go (Silver et al., 2017), many researchers have begun exploring practical
8
Under review as a conference paper at ICLR 2019
applications of deep reinforcement learning (DRL). Some investigated applications include, robotics
(Levine et al., 2016), self-driving cars (Shalev-Shwartz et al., 2016), and safety (Lipton et al., 2016a).
Inevitably for PSRL, the act of posterior sampling for policy or value is computationally intractable
with large systems, so PSRL can not be easily leveraged to high dimensional problems (Ghavamzadeh
et al., 2015; Engel et al., 2003; Dearden et al., 1998; Tziortziotis et al., 2013). To remedy these
failings Osband et al. (2017) consider the use of randomized value functions. For finite state-action
space MDP, Osband et al. (2014) propose posterior sampling directly on the space of Q-functions and
provide a strong Bayesian regret bound guarantee. To approximate the posterior, they use BLR on
one-hot encoding of state-action and improve the computation complexity of PSRL. The approach
deployed in BDQN is strongly related and similar to this work, and is a generalization to continues
state-action space MDPs.
To combat the computational and scalability shortcomings, Osband et al. (2016) suggests a
bootstrapped-ensemble approach that trains several models in parallel to approximate the poste-
rior distribution. Other works suggest using a variational approximation to the Q-networks (Lipton
et al., 2016b) or a concurrent work on noisy network (Fortunato et al., 2017). However, most of these
approaches significantly increase the computational cost of DQN and neither approach produced
much beyond modest gains on Atari games. Interestingly, the Bayesian approach as a technique for
learning a neural network has been deployed for object recognition and image caption generation
where its significant advantage has been verified Snoek et al. (2015).
Concurrently, Levine et al. (2017) proposes least squares temporal difference which learns a linear
model on the feature representation in order to estimate the Q-function while ε-greedy exploration
is employed and improvement on 5 tested Atari games is provided. Out of these 5 games, one is
common with our set of 15 games which BDQN outperform it by factor of 360% (w.r.t. the score
reported in their paper). As motivated by theoretical understanding, our empirical study shows that
performing Bayesian regression instead, and sampling from the result, can yield a substantial benefit,
indicating that it is not just the higher data efficiency at the last layer, but that leveraging an explicit
uncertainty representation over the value function is of substantial benefit.
Drop-out, as another randomized exploration method is proposed by Gal & Ghahramani (2016)
but Osband et al. (2016) investigates the sufficiency of the estimated uncertainty and hardness in
driving suitable exploitation out of it. We also implemented the dropout version of DDQN and
compared its performance against BDQN, DDQN, DDQN+ , and the random policy (the policy which
chooses actions uniformly at random) but did not observe much gain beyond the performance of the
random policy (See section A.6). As stated before, in spite of the novelties proposed by the methods,
mentioned in this section, neither of them, including TSbased approaches, produced much beyond
modest gains on Atari games while BDQN provides significant improvements in terms of both sample
complexity and final performance.
7	Conclusion
In this work we proposed BDQN, a practical Thompson sampling based RL algorithm which provides
efficient exploration/exploitation in a computationally efficient manner. It involved making simple
modifications to the DDQN architecture by replacing the last layer with Bayesian linear regression.
Under the Gaussian prior, we obtained fast closed-form updates for the posterior distribution. We
demonstrated significantly faster training and much better performance in many games compared to
the reported results of a wide number of state-of-the-art baselines. We also established theoretical
guarantees for an episodic MDP with continuous state and action spaces in the special case where the
feature representation is fixed. We derived an order-optimal frequentist and Bayesian regret bound of
Oe(d√N) after N time steps.
In future, we plan to extend the analysis to the more general frequentist setting. The current theoretical
guarantees are mainly developed for the class of linear functions. For general class of functions,
optimism in the face of uncertainty is deployed to guarantee a tight probably approximately correct
(PAC) bound (Jiang et al., 2016) but the proposed algorithm requires solving a NP-hard problem at
each iteration. We aim to further provide a tight theoretical bound through Thomson sampling while
also preserving computational feasibility.
9
Under review as a conference paper at ICLR 2019
References
Yasin Abbasi-Yadkori and Csaba Szepesvari. Regret bounds for the adaptive control of linear
quadratic systems. In COLT 2011 - The 24th Annual Conference on Learning Theory, June 9-11,
2011, Budapest, Hungary, 2011.
Yasin Abbasi-Yadkori and Csaba Szepesvari. Bayesian optimal control of smoothly parameterized
systems. In UAI, pp. 1-11, 2015.
Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic
bandits. In Advances in Neural Information Processing Systems 24 - NIPS, pp. 2312-2320, 2011.
David Abel, Alekh Agarwal, Fernando Diaz, Akshay Krishnamurthy, and Robert E Schapire. Ex-
ploratory gradient boosting for reinforcement learning in complex domains. arXiv, 2016.
Shipra Agrawal and Navin Goyal. Analysis of thompson sampling for the multi-armed bandit problem.
In COLT, 2012.
Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor
decompositions for learning latent variable models. The Journal of Machine Learning Research,
15(1):2773-2832, 2014.
AndraS Antos, Csaba Szepesvari, and Remi Munos. Learning near-optimal policies with bellman-
residual minimization based fitted policy iteration and a single sample path. Machine Learning,
2008.
John Asmuth, Lihong Li, Michael L Littman, Ali Nouri, and David Wingate. A bayesian sampling
approach to exploration in reinforcement learning. In Proceedings of the Twenty-Fifth Conference
on Uncertainty in Artificial Intelligence, 2009.
Kamyar Azizzadenesheli, Alessandro Lazaric, and Animashree Anandkumar. Reinforcement learning
of pomdps using spectral methods. In Proceedings of the 29th Annual Conference on Learning
Theory (COLT), 2016a.
Kamyar Azizzadenesheli, Alessandro Lazaric, and Animashree Anandkumar. Reinforcement learning
in rich-observation mdps using spectral methods. arXiv preprint arXiv:1611.03907, 2016b.
Gabor Bartok, Dean P Foster, David Pal, Alexander Rakhlin, and Csaba Szepesvari. Partial monitor-
ingclassification, regret bounds, and algorithms. Mathematics of Operations Research, 2014.
Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos.
Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information
Processing Systems, pp. 1471-1479, 2016.
Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environ-
ment: An evaluation platform for general agents. J. Artif. Intell. Res.(JAIR), 2013.
Ronen I Brafman and Moshe Tennenholtz. R-max-a general polynomial time algorithm for near-
optimal reinforcement learning. The Journal of Machine Learning Research, 3:213-231, 2003.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016.
Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. In Advances in
neural information processing systems, pp. 2249-2257, 2011.
Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu,
Chiyuan Zhang, and Zheng Zhang. Mxnet: A flexible and efficient machine learning library for
heterogeneous distributed systems. arXiv, 2015.
Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff
functions. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and
Statistics, pp. 208-214, 2011.
10
Under review as a conference paper at ICLR 2019
Varsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under bandit
feedback. 2008.
Victor H de la Pena, Michael J Klass, and Tze Leung Lai. Self-normalized processes: exponential
inequalities, moment bounds and iterated logarithm laws. Annals of probability, pp. 1902-1933,
2004.
Richard Dearden, Nir Friedman, and Stuart Russell. Bayesian q-learning. In AAAI/IAAI, pp. 761-768,
1998.
Guneet S Dhillon, Kamyar Azizzadenesheli, Zachary C Lipton, Jeremy Bernstein, Jean Kossaifi, Aran
Khanna, and Anima Anandkumar. Stochastic activation pruning for robust adversarial defense.
arXiv preprint arXiv:1803.01442, 2018.
Yaakov Engel, Shie Mannor, and Ron Meir. Bayes meets bellman: The gaussian process approach to
temporal difference learning. In Proceedings of the 20th International Conference on Machine
Learning (ICML), 2003.
Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves,
Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, et al. Noisy networks for exploration.
arXiv, 2017.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. In ICML, 2016.
Javier Garcia and Fernando Fernandez. A comprehensive survey on safe reinforcement learning.
Journal of Machine Learning Research, 16(1):1437-1480, 2015.
Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, Aviv Tamar, et al. Bayesian reinforcement
learning: A survey. Foundations and TrendsR in Machine Learning, 2015.
Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger.
Deep reinforcement learning that matters. arXiv, 2017.
Daniel Hsu, Sham Kakade, Tong Zhang, et al. A tail inequality for quadratic forms of subgaussian
random vectors. Electronic Communications in Probability, 17, 2012.
Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement
learning. Journal of Machine Learning Research, 2010.
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contex-
tual decision processes with low bellman rank are pac-learnable. arXiv, 2016.
Sham Kakade, Michael J Kearns, and John Langford. Exploration in metric state spaces. In
Proceedings of the 20th International Conference on Machine Learning (ICML-03), pp. 306-312,
2003.
Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time.
Machine Learning, 49(2-3):209-232, 2002.
Michail G Lagoudakis and Ronald Parr. Least-squares policy iteration. Journal of machine learning
research, 4(Dec):1107-1149, 2003.
Nir Levine, Tom Zahavy, Daniel J Mankowitz, Aviv Tamar, and Shie Mannor. Shallow updates for
deep reinforcement learning. arXiv, 2017.
Sergey Levine et al. End-to-end training of deep visuomotor policies. JMLR, 2016.
Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to
personalized news article recommendation. In Proceedings of the 19th international conference on
World wide web, pp. 661-670. ACM, 2010.
Zachary C Lipton, Kamyar Azizzadenesheli, Abhishek Kumar, Lihong Li, Jianfeng Gao, and
Li Deng. Combating reinforcement learning’s sisyphean curse with intrinsic fear. arXiv preprint
arXiv:1611.01211, 2016a.
11
Under review as a conference paper at ICLR 2019
Zachary C Lipton, Jianfeng Gao, Lihong Li, Xiujun Li, Faisal Ahmed, and Li Deng. Efficient
exploration for dialogue policy learning with bbq networks & replay buffer spiking. arXiv preprint
arXiv:1608.05081, 2016b.
Marlos C Machado, Marc G Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, and Michael
Bowling. Revisiting the arcade learning environment: Evaluation protocols and open problems for
general agents. arXiv preprint arXiv:1709.06009, 2017.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
through deep reinforcement learning. Nature, 2015.
Ronald Ortner and Daniil Ryabko. Online regret bounds for undiscounted continuous reinforcement
learning. In Advances in Neural Information Processing Systems, pp. 1763-1771, 2012.
Ian Osband and Benjamin Van Roy. Model-based reinforcement learning and the eluder dimension.
In Advances in Neural Information Processing Systems, pp. 1466-1474, 2014a.
Ian Osband and Benjamin Van Roy. Near-optimal reinforcement learning in factored mdps. In
Advances in Neural Information Processing Systems, pp. 604-612, 2014b.
Ian Osband, Dan Russo, and Benjamin Van Roy. (more) efficient reinforcement learning via posterior
sampling. In Advances in Neural Information Processing Systems, 2013.
Ian Osband, Benjamin Van Roy, and Zheng Wen. Generalization and exploration via randomized
value functions. arXiv, 2014.
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via
bootstrapped dqn. In Advances in Neural Information Processing Systems, 2016.
Ian Osband, Daniel Russo, Zheng Wen, and Benjamin Van Roy. Deep exploration via randomized
value functions. arXiv, 2017.
Georg Ostrovski, Marc G Bellemare, Aaron van den Oord, and Remi Munos. Count-based exploration
with neural density models. arXiv, 2017.
Carl Edward Rasmussen and Christopher KI Williams. Gaussian processes for machine learning,
volume 1. MIT press Cambridge, 2006.
Paat Rusmevichientong and John N Tsitsiklis. Linearly parameterized bandits. Mathematics of
Operations Research, 35(2):395-411, 2010.
Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. Mathematics of
Operations Research, 39(4):1221-1243, 2014a.
Daniel Russo and Benjamin Van Roy. Learning to optimize via information-directed sampling. pp.
1583-1591, 2014b.
Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement
learning for autonomous driving. arXiv, 2016.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without
human knowledge. Nature, 2017.
Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram,
Mostofa Patwary, Mr Prabhat, and Ryan Adams. Scalable bayesian optimization using deep neural
networks. In ICML, 2015.
Malcolm Strens. A bayesian framework for reinforcement learning. In ICML, 2000.
Gerald Tesauro. Temporal difference learning and td-gammon. Communications of the ACM, 38(3):
58-68, 1995.
12
Under review as a conference paper at ICLR 2019
William R Thompson. On the likelihood that one unknown probability exceeds another in view of
the evidence of two samples. Biometrika, 1933.
Nikolaos Tziortziotis, Christos Dimitrakakis, and Konstantinos Blekas. Linear bayesian reinforcement
learning. In IJCAI 2013, Proceedings of the 23rd International Joint Conference on Artificial
Intelligence, 2013.
Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. In AAAI, 2016.
13
Under review as a conference paper at ICLR 2019
A Appendix
In the main text, for simplicity, we use the terms ”BLR” for i.i.d. samples and BLR for non i.i.d.
samples exchangeable, even though, technically, the do not have equal meaning. In RL, the data is not
i.i.d, and we extend the BLR to non i.i.d. setting by deploying additional Martingale type argument
and handles the data with temporal dependency.
Table 3: 1st column: score ratio of BDQN to DDQN run for same number of time steps. 2nd column:
score ratio of BDQN to DDQN+. 3rd column: score ratio of BDQN to human scores reported at
Mnih et al. (2015). 4th column: Area under the performance plot ration (AuPPr) of BDQN to DDQN.
AuPPr is the integral of area under the performance plot ration. For Pong, since the scores start form
-21, we shift it up by 21. 5th column: Sample complexity, SC: the number of samples the BDQN
requires to beat the human score (Mnih et al., 2015)(“ - ” means BDQN could not beat human score).
6th column: SC+: the number of samples the BDQN requires to beat the score of DDQN+. We run
both BDQN and DDQN for the same number of times steps, stated in the last column.
Game	BDQN DDQN	BDQN DDQN+	BDQN Human	AuPPr	SC	SC+	Steps
Amidar	558%	788%	325%	280%	22.9M	4.4M	100M
Alien	103%	103%	43%	110%	-	36.27M	100M
Assault	396%	176%	589%	290%	1.6M	24.3M	100M
Asteroids	2517%	1516%	108%	680%	58.2M	9.7M	100M
Asterix	531%	385%	687%	590%	3.6M	5.7M	100M
BeamRider	207%	114%	150%	210%	4.0M	8.1M	70M
BattleZone	281%	253%	172%	180%	25.1M	14.9M	50M
Atlantis	80604%	49413%	11172%	380%	3.3M	5.1M	40M
DemonAttack	292%	114%	326%	310%	2.0M	19.9M	40M
Centipede	114%	178%	61%	105%	-	4.2M	40M
BankHeist	211%	100%	100%	250%	2.1M	10.1M	40M
CrazyClimber	148%	122%	350%	150%	0.12M	2.1M	40M
ChopperCommand	14500%	1576%	732%	270%	4.4M	2.2M	40M
Enduro	295%	350%	361%	300%	0.82M	0.8M	30M
Pong	112%	100%	226%	130%	1.2M	2.4M	5M
A.1 Network architecture:
The input to the network part of BDQN is 4 × 84 × 84 tensor with a rescaled and averaged over
channels of the last four observations. The first convolution layer has 32 filters of size 8 with a stride
of 4. The second convolution layer has 64 filters of size 4 with stride 2. The last convolution layer
has 64 filters of size 3 followed by a fully connected layer with size 512. We add a BLR layer on top
of this.
A.2 Choice of hyper-parameters:
For BDQN, we set the values of W target to the mean of the posterior distribution over the weights
of BLR with covariances Cov and draw W from this posterior. For the fixed W and W target, we
randomly initialize the parameters of network part of BDQN, θ, and train it using RMSProp, with
learning rate of 0.0025, and a momentum of 0.95, inspired by (Mnih et al., 2015) where the discount
factor is γ = 0.99, the number of steps between target updates Ttarget = 10k steps, and weights W
are re-sampled from their posterior distribution every T sample steps. We update the network part
of BDQN every 4 steps by uniformly at random sampling a mini-batch of size 32 samples from the
replay buffer. We update the posterior distribution of the weight set W every T Bayes target using
mini-batch of size B (if the size of replay buffer is less than B at the current step, we choose the
minimum of these two ), with entries sampled uniformly form replay buffer. The experience replay
contains the 1M most recent transitions. Further hyper-parameters are equivalent to ones in DQN
setting.
14
Under review as a conference paper at ICLR 2019
For the BLR, we have noise variance σ, variance of prior over weights σ, sample size B, posterior
update period T Bayes target, and the posterior sampling period T sample. To optimize for this set of
hyper-parameters we set up a very simple, fast, and cheap hyper-parameter tuning procedure which
proves the robustness of BDQN. To find the first three, we set up a simple hyper-parameter search.
We used a pretrained DQN model for the game of Assault, and removed the last fully connected layer
in order to have access to its already trained feature representation. Then we tried combination of
B = {Ttarget, 10 ∙ Ttarget}, σ = {1,0.1,0.001}, and σe = {1,10} and test for 1000 episode of the
game. We set these parameters to their best B = 10 ∙ Ttarget, σ = 0.001, σ = 1.
The above hyper-parameter tuning is cheap and fast since it requires only a few times the B number of
forwarding passes. For the remaining parameters, we ran BDQN ( with weights randomly initialized)
on the same game, Assault, for 5M time steps, with a set of TBayes target = {Ttarget, 10 ∙ Ttarget}
and Tsample = { Tt10get, T：0；et}, where BDQN performed better with choice of TBayes target =
10 ∙ Ttarget. For both choices of Tsample, it performs almost equal and we choose the higher one to
reduce the computation cost. We started off with the learning rate of 0.0025 and did not tune for that.
Thanks to the efficient Thompson sampling exploration and closed form BLR, BDQN can learn a
better policy in an even shorter period of time. In contrast, it is well known for DQN based methods
that changing the learning rate causes a major degradation in the performance (Fig. 4). The proposed
hyper-parameter search is very simple and an exhaustive hyper-parameter search is likely to provide
even better performance.
A.3 Learning rate:
It is well known that DQN and DDQN are sensitive to the learning rate and change of learning rate
can degrade the performance to even worse than random policy. We tried the same learning rate as
BDQN, 0.0025, for DDQN and observed that its performance drops. Fig. 4 shows that the DDQN
with higher learning rates learns as good as BDQN at the very beginning but it can not maintain the
rate of improvement and degrade even worse than the original DDQN with learning rate of 0.00025.
Figure 4: Effect of learning rate on DDQN
A.4 Computational and sample cost comparison:
For a given period of game time, the number of the backward pass in both BDQN and DQN are the
same where for BDQN it is cheaper since it has one layer (the last layer) less than DQN. In the sense
of fairness in sample usage, for example in duration of 10 ∙ TBayes target = 100k, all the layers of
both BDQN and DQN, except the last layer, sees the same number of samples, but the last layer of
BDQN sees 16 times fewer samples compared to the last layer of DQN. The last layer of DQN for a
duration of 100k, observes 25k = 100k/4 (4 is back prob period) mini batches of size 32, which is
16 ∙ 100k, where the last layer of BDQN just observes samples size of B = 100k. As it is mentioned
in Alg. 1, to update the posterior distribution, BDQN draws B samples from the replay buffer and
needs to compute the feature vector of them. Therefore, during the 100k interactions for the learning
procedure, DDQN does 32 * 25k of forward passes and 32 * 25k of backward passes, while BDQN
15
Under review as a conference paper at ICLR 2019
Table 4: The comparison of BDQN, DDQN, Dropout-DDQN and random policy. Dropout-DDQN
as another randomization strategy provides a deficient estimation of uncertainty and results in poor
exploration/exploitation trade-off.
Game	IBDQNlDDQNIDDQN+∣DroPout-DDQNlRandomPoliCyISteP
CrazyClimber	124k	84k	102k	19k	11k	40M
Atlantis	3.24M	39.7k	64.76k	7.7k	12.85k	40M
Enduro	1.12k	0.38k	0.32k	0.27k	0	30M
Pong	21	18.82	21	-18	-20.7	5M
does same number of backward passes (cheaper since there is no backward pass for the final layer)
and 36 * 25k of forward passes. One can easily relax it by parallelizing this step along the main body
of BDQN or dePloying on-line Posterior uPdate methods.
A.5 Thompson sampling frequency:
The choice of Thompson sampling update frequency can be crucial from domain to domain. Theoret-
ically, we show that for episodic learning, the choice of sampling at the beginning of each episode, or
a bounded number of episodes is desired. If one chooses T sample too short, then computed gradient
for backpropagation of the feature representation is not going to be useful since the gradient get
noisier and the loss function is changing too frequently. On the other hand, the network tries to find a
feature representation which is suitable for a wide range of different weights of the last layer, results
in improper waste of model capacity. If the Thompson sampling update frequency is too low, then
it is far from being Thompson sampling and losses the randomized exploration property. We are
interested in a choice of T sample which is in the order of upper bound on the average length of each
episode of the Atari games. The current choice of T sample is suitable for a variety of Atari games
since the length of each episode is in range of O(T sample) and is infrequent enough to make the
feature representation robust to big changes.
For the RL problems with shorter a horizon We suggest to introduce two more parameters, Tsample and
each Wa where Tsample, the period that of each Wa is sampled out of posterior, is much smaller than
Tsample and Wa, ∀a are used for Thompson sampling where Wa , ∀a are used for backpropagation of
feature representation. For game Assault, we tried using Tsample and each Wa but did not observe
much a difference, and set them to T sample and each Wa. But for RL setting with a shorter horizon,
we suggest using them.
A.6 Dropout as a randomized exploration strategy
Dropout, as another randomized exploration method, is proposed by Gal & Ghahramani (2016), but
Osband et al. (2016) argue about the deficiency of the estimated uncertainty and hardness in driving a
suitable exploration and exploitation trade-off from it (Appendix A in (Osband et al., 2016)). They
argue that Gal & Ghahramani (2016) does not address the fundamental issue that for large networks
trained to convergence all dropout samples may converge to every single datapoint. As also observed
by (Dhillon et al., 2018), dropout might results in a ensemble of many models, but all almost the
same (converge to the very same model behavior). We also implemented the dropout version of
DDQN, Dropout-DDQN, and ran it on four randomly chosen Atari games (among those we ran for
less than 50M time steps). We observed that the randomization in Dropout-DDQN is deficient and
results in performances worse than DDQN on these four Atari games, Fig. 5. In Table 4 we compare
the performance of BDQN, DDQN, DDQN+, and Dropout-DDQN, as well as the performance of the
random policy, borrowed from Mnih et al. (2015). We observe that the Dropout-DDQN not only does
not outperform the plain ε-greedy DDQN, it also sometimes underperforms the random policy. For
the game Pong, we also ran Dropout-DDQN for 50M time steps but its average performance did not
get any better than -17. For the experimental study we used the default dropout rate of 0.5 to mitigate
its collapsing issue.
16
Under review as a conference paper at ICLR 2019
Figure 5: The comparison between DDQN, BDQN and Dropout-DDQN
A.7 Further investigation on Atlantis:
After removing the maximum episode length limit for the game Atlantis, BDQN gets the score of
62M. This episode is long enough to fill half of the replay buffer and make the model perfect for the
later part of the game but losing the crafted skill for the beginning of the game. We observe in Fig. 6
that after losing the game in a long episode, the agent forgets a bit of its skill and loses few games
but wraps up immediately and gets to score of 30M . To overcome this issue, one can expand the
replay buffer size, stochastically store samples in the reply buffer where the later samples get stored
with lowest chance, or train new models for the later parts of the episode. There are many possible
cures for this interesting observation and while we are comparing against DDQN, we do not want to
advance BDQN structure-wise.
Figure 6: BDQN on Atlantis after removing the limit on max of episode length hits the score of 62M
in 16M samples.
Figure 7: A couple of more runs of BDQN where the jump around 15M constantly happens
A.8 Further discussion on Reproducibility
In Table 2, we provide the scores of bootstrap DQN (Osband et al., 2016) and NoisyNet5Fortunato
et al. (2017) along with BDQN. These score are directly copied from their original papers and we did
not make any change to them. We also desired to report the scores of count-based method (Ostrovski
et al., 2017), but unfortunately there is no table of score in that paper in order to provide them here.
In order to make it easier for the readers to compare against the results in Ostrovski et al. (2017), we
visually approximated their plotted curves for CT S, P ixel, and Reactor, and added them to the
Table 2. We added these numbers just for the convenience of the readers. Surely we do not argue any
scientific meaning for them and leave it to the readers to interpret them.
Table 2 shows a significant improvement of BDQN over these baselines. Despite the simplicity and
negligible computation overhead of BDQN over DDQN, we can not scientifically claim that BDQN
5This work does not have scores of Noisy-net with DDQN objective function but it has Noisy-net with DQN
objective which are the scores reported in Table 2
17
Under review as a conference paper at ICLR 2019
outperforms these baselines by just looking at the scores in Table2 because we are not aware of
their detailed implementation as well as environment details. For example, in this work, we directly
implemented DDQN by following the implementation details mentioned in the original DDQN paper
and the scores of our DDQN implementation during the evaluation time almost matches the scores of
DDQN reported in the original paper. But the reported scores of implemented DDQN in Osband et al.
(2016) are much different from the reported score in the original DDQN paper.
A.9 A short discussion on safety
In BDQN, as mentioned in Eq. 3, the prior and likelihood are conjugate of each others. Therefore, we
have a closed form posterior distribution of the discounted return, PtN=0 γtrt |x0 = x, a0 = a, Da ,
approximated as
N σ12φφθ(X)TΞaΦay, φθ(X)TΞaφθ(x))
One can use this distribution and come UP With a safe RL criterion for the agent (Garcia & Fernandez,
2015). Consider the following example; for two actions with the same mean, if the estimated variance
over the return increases, then the action becomes more unsafe Fig. 8. By just looking at the low and
high probability events of returns under different actions we can approximate whether an action is
safe to take.
Figure 8: Two actions, with the same mean, but the one with higher variance on the return might be
less safe than the one with narrower variance on the return.
B	BAYESIAN AND FREQUENTIST REGRET, PROOF OF THEOREMS 1,2
Modeling: We consider the following linear model over the Q functions and returns for episodic
MDPs with episode length of H. We consider the discount factor to be any number 0 ≤ Y ≤ 1. Let
ω* denotes the parameter corresponding the underlying model. For a given policy π and the time
step ht in the episode, condition on Xh0, Ah0 we have PH=h0 Rh = φ(Xh0, Ah0 )tω* + PH=ho Vh
with φ(X, A) ∈ Rd. The noise process is correlated, dependent on the agent policy, and is not mean
zero unless under policy π* .
ΨR = Φ(X{1:H},A{1H})ω* + Ψν(X口町,A{1H}) , where , Ψ :=
1 γ γ 2
01 γ
00 1
.
.
......
0......
γH-1
Y H-2
.
:	(5)
.
.
.
1
The vector R ∈ RH is a random vector, as stack of rewards in a episode, i.e., Rh = rh . The matrix
Φ(X{1:H}, X{1:H}) ∈ RHXd is a row-wised concatenation of sequence of state-action features,
Φh = φ(Xh, Ah) with Φh denotes the h’th row.
18
Under review as a conference paper at ICLR 2019
In order to provide more intuition on the model, we can exploit the property of the matrix Ψ where
■ 1 -γ 0 ... 0 -
0 1 -γ . . . 0
Ψ-1
00 1 ...
0
1
and rewrite Eq. 5 as follows:
Rt = Ψ-1Φtω + νt	(6)
We denote Ψ0 := Ψ-1>. We build our analysis based on the former notation. We consider the feature
representing matrix and weights for any X{1:H} , A{1:H} to satisfy;
trace (φ(XnH∖A□H})Φ(X{1:H},A{1:H})>) ≤ L, |卬|2 ≤ Lω
The vector V(X{1:H},A{1:H}) ∈ RH is a state-action and policy dependent noise vector. The
MDP assumption is crucial for the analysis in this paper. The noise νh encodes the randomness in
transition to state Xh, possible stochasticity in policy i.e., Ah, reward Rh. For any h, the noises after
time step h, {νh, νh+1, . . . , νH}, are conditionally independent from the noises before time step
h0, {ν1, ν2, . . . , νh-1} given (Xh, Ah). Moreover, since νh encodes the stochasticity in transition
and reward at time step h, conditional distribution of νh |(Xh, Ah) is mean zero and independent
{ν0,... {νh-1, vh+1,..., vH} other of since φ(Xh, Ah)>ω* represent the true Q value of Xh, Ah.
The agent policy at each episode is denoted as πt . In the remaining, we show how to estimate
ω* through data collected by following ∏t. We denote ω^t, the estimation of ω*. We show over
time the estimation concentrates around ω* under the desire matrix norm. We further show, We
can deploy this analysis an construct two algorithms, one based on PSRL, and another Optimism
OFUto guaranteed Bayesian and frequentist regret upper bounds respectively. The main body of the
following analyses on concentration of measure is a matrix extension of contextual linear bandit
analyses Abbasi-Yadkori et al. (2011); Chu et al. (2011); Li et al. (2010); Rusmevichientong &
Tsitsiklis (2010); Dani et al. (2008); Russo & Van Roy (2014a) and self normalized processes (de la
Pena et al., 2004). Our regret analysis extends prior analysis in frequentist and Bayesian regret
literature Osband et al. (2013); Jaksch et al. (2010).
We restate the Eq. 5 in its abstract way;
Rt ：= ΨRt =Φtω + vt , where Vt := Ψvt	(7)
We consider the discount factor to be any number 0 ≤ γ ≤ 1 for the modeling and for the sake
of analysis we consider γ = 1 from now on which we later argue the bounds also hold for any
0≤γ≤1.
Assumption 1 (Sub-Gaussian random vector (Hsu et al., 2012)). The noise model at time t, condi-
tioned on the filtration Ft-1 at time t is sub-Gaussian random vector, i.e. there exists a parameter
σ ≥ 0 such that ∀α ∈ Rd
EhexP (α>Vt)忖-^ ≤ exp (Ilak2σ2∕2)
The filtration at time t also includes X{1:H}, A{1:H}. Asm. 1 also implies that E
hv t国
0 which
means that after drawing the matrix Φt, the random variable Vt is means zero. A similar assumption
on the noise model is assumed in prior analyses of linear bandit (Abbasi-Yadkori et al., 2011).
Furthermore, we consider the maximum expected cumulative reward condition on states of a episode
is at most 1.
Defined the following quantities for self-normalized processes;
tt
St := X φ>vi,	Xt := X φ>φi,	Xt = Xt + X
i	i=0
where Xe is a ridge regularization matrix and usually is equal to λI .
19
Under review as a conference paper at ICLR 2019
Lemma 1 (Confidence ellipsoid for problem in Eq. 5). For a matrix problem in Eq. 5, given a
sequence of {Φi, Ri}iT=1, under the Asm. 1 and the following estimator for ωbt
Qt= (X Φ>Φi + λi!	XX (Φ>Ri)
with probability at least 1 - δ
kωτ - ω*∣∣χt ≤ θt(δ) : σP2log(1∕δ)+ dlog(1 + tL2∕λ) + N/Lω
for all t ≤ T where ∣∣ω*k2 ≤ Lω and trace (Φ(∙)Φ(∙)>) ≤ L, and therefore the confidence set is
Ct(δ):= {ω ∈ Rd : ∣ωt - ω∣χt ≤ θt(δ)}
Let ΘT denote the event that the confidence bounds in Lemma 1 holds.
Lemma 2 (Determinant Lemma). Forasequence PT log(1 + ∣∣Φtkχ-ι) we have
T
Xlog (1 + kΦtkχ-ι) ≤ dlog(λ + TL2/d)	(8)
t
Regret Definition We defined Vπω, the value of policy π applied on model specified by ω in Eq. 5.
We restate the definition of the regret and Bayesian regret.
RegT : = E
Where πt is the agent policy at time t
When there is a prior over the ω* the expected Bayesian regret might be the target of the study.
BayesRegT : = E
T
∑hV∏ω** - V∏ω*i
t
B.1 Optimism: Regret bound of Alg. 3
Given samples by following the agent policy πt0 for each episode t0 ≤ t we estimate the ωbt as follows;
小>电+λI!	£(小>即
Lemma 1 states that under event Θτ,|® — ω* |鼠 ≤ θt(δ). For an observed state Xj11, the optimistic
policy is
πet(Xth) = arg max max φ> (Xth, a)ω
a∈A ω∈Ct-1 (δ)
Furthermore, we define state and policy dependent optimistic parameter ωet(π) as follows;
ωet(π) := arg max φ(Xt1 , π(Xt1))>ω
ω∈Ct-1(δ)	t	t
For ease of notation we drop the state in the notation. Following OFU, Alg. 3, we set πt = πet denote
the optimistic policy. By the definition we have
VettM) (Xh) ≥ v⅛b)(χh)
Therefore, under the event ΘT we have
20
Under review as a conference paper at ICLR 2019
I *
ω
RegT : = E
^^{^™
∆th=1
≤E
T
X [vωt(πt)(χi) - v∏*t(π*)(xi)+vω** (XI)- vω* (Xi)] ∣ω*
t
(πet)
v et(π*)
π*
(X1) ∣ω*
E
t
T
t
|
〜
}
^{z
≤0
}
Resulting in
T
RegT ≤ E X [vωt(et)(Xt1) - Vω* (X1)] ∣ω
t
Lets defined Vπω(Xt1, . . . , Xth0; h : π0) as the value function following policy π for h time step then
switching to policy π0 on the model ω and observing Xt1, . . . Xth0 .
T
RegT ≤ E X [vωt(et)(Xt1) - vω* (Xt1)] ∣ω*
t
T
=E X [vωt(et)(Xt1) - vω* (X1;1 : n*)+ vω* (X1;1 : π*)-vω* (X1)] ∣ω
t
Given the generative model in Eq.5 we have
T
RegT ≤ E X [vωt(πt)(Xl) - vω*(X1; 1 : ∏*) + vω* (Xi; 1: ∏*) - vω* (X1)] ∣ω*
t
T
=E X [φ(Xi,et(Xi))>ωt(∏t) - φ(Xi,∏t(Xi))τω* + vω* (Xi;1 : ∏*) - vω* (Xi)] ∣ω*
t
T
X
t
E
φ(Xi,∏t(Xi))τ (ωt(∏t) - ω*)+ vω* (Xi;1 : π*) - vω* (Xt) ∣ω*
X------------------------------------------{------------}
(∆th=2)
For ∆th=2 we deploy the similar decomposition and upper bound as ∆th=i
∆t2 := Vπeωt*(Xti; 1: π*) - Vπeωt*(Xti)
Since for both of Vπeω* (Xti; 1 : π*) and Vπeω* (Xti) we follow the same policy on the same model for 1
time step, the reward at the first time step has the same distribution, therefore we have;
∆h=2 = E hvω* xi: ∏*) - vω* 区)闻”]
rustling in
∆h=2 = E hvω* xi: ∏*) - vω* (X2)Xι,ω*]
≤ E Φ(X2, et(X2))τ (ωt(et) - ω*) + vω* W, 2: ∏*) - vω* (x2) X1,ω*
'-------------{--------------}
(∆th=3)
21
Under review as a conference paper at ICLR 2019
Similarly we can defined ∆th=3 , . . . ∆th=H . Therefore;
TH
RegT ≤ E XXφ(Xh,et(Xh))> (ωt(et) - ω*) ∣ω*
th
T
=E X l>Φt (ωt(et)- ω*) ∣ω*
t
T
=E X l>Φtχ-1/2χ1/2 (ωt(∏t) - ω*) ∣ω*
t
-	T	-
≤ E √HX kΦtkχ-ι∣∣ωt(et)-ω*kχt∣ω*
t
-	T	-
≤ E √HX kΦtkχ-ι2θt-i(δ)∣ω*	⑼
t
Since the maximum expected cumulative reward, condition on states of a episode is at most 1, we
have;
-T	-
RegT ≤ √HE Xmin{kΦtkχ-12θt-i(δ), 1}∣ω*
t
-TH	一
≤ √H E XX
2θt-i(δ)min{∣∣Φtkχ-],1}∣ω[
Moreover, at time T, we can use Jensen’s inequality, exploit the fact that θt(δ) is an increasing
function of t and have
RegT ≤ 2E

T
THθT(δ)2 Xmin{∣∣Φtkχ-ι,1}∣ω*
t
≤ 2 (σ√2 log (1∕δ) + dlog(1 + TL2/λ) + λ1^Lω) E
T
TH X min{∣∣Φtkχ-ι,1}∣ω*
t

(10)
Now, using the fact that for any scalar α such that 0 ≤ α ≤ 1, then α ≤ 2 log(1 + α) we can rewrite
the latter part of Eq. 10
T
Xlog (1 + kφtkχ-1)
t

T
Xmin{kΦtkχ-ι,1}≤ 2
t
By applying the Lemma 2 and substituting the RHS of Eq. 8 into Eq. 10, we get
RegT ≤ 2 (σ√2log (1∕δ) + dlog(1 + TL2/λ) + λ1^Lω) √2THdlog(λ + TL2Id (11)
with probability at least 1 - δ. If we set δ = 1/T then the probability that the event ΘT holds is
1 - 1/T and we get regret of at most the RHS of Eq. 11, otherwise with probability at most 1/T we
get maximum regret of T , therefore
RegT ≤ 1 + 2 (σ√2log (T) + dlog(1 + TL2∕λ) + λ1^Lω) √2THdlog(λ + TL2/d)
and theorem statement follows.
22
Under review as a conference paper at ICLR 2019
B.2 Bayesian Regret of Alg. 2
The analysis developed in the previous section, up to some minor modification, e.g., change of
strategy to PSRL, directly applies to Bayesian regret bound, with a farther expectation over models.
When there is a prior over the ω* the expected Bayesian regret might be the target of the study.
T
BayeSRegT ： = E X [呼* - V∏t*]
t
T
E X hVπω** - Vπωt* Hti
Here Ht is a multivariate random sequence which indicates history at the beginning of episode t and
πt is the policy following PSRL. For the remaining πt denotes the PSRL policy. As it mentioned in
the Alg. 2, at the beginning of an episode, we draw ωt for the posterior and the corresponding policy
is
πt(Xth) := arg max φ(Xth, a)>ωt
t	a∈A t
Condition on the history Ht, samples by following the agent policy πt0 for each episode t0 ≤ t, similar
to previous section we estimate the ωbt as follows;
bt ：= XXΦ Φ>Φi + λi!	X (Φ>Ri)
Lemma 1 states that under event Θt, k@t - ω* ||又= ≤ θt(δ). Conditioned on Ht, the ωt and ω* are
equally distributed, then we have
E hVπωe*t(π*)Hti =E hVπωett(πt)Hti
Therefore, under event Θt holds for all ω* We have
T
BayeSRegT :
EE	vω** (XI)-** (Xt1)Ht
X '---------------}
t
^^{^™
∆th=1
T
X E [vωt(πt) (Xi) - v∏e*t(π*)(χl)+vω** (χl)-埠* X)Ht]
t
T
t
Vπωe*t(π*)(Xt1) |Ht
-- /
^z
≤0
Resulting in
T
BayeSRegT ≤ X E[vω"t)(xi)- vω* (χi)∣Ht]
t
23
Under review as a conference paper at ICLR 2019
Similar to optimism and defining Vπω(Xt1, . . . , Xth0; h : π0) accomponeid with Eq.5 we have;
T
BayeSRegT ≤ E X [唠E)(XI)-喘* X)∣Ht]
t
T
=E X [v∏etE)(XI)- V∏ω* (X1；1 : π*)+ V∏ω* (X1；1 : ∏*) - V∏ω* X)|Ht]
t
≤E
=E
T
X [φ(xl,∏t(xl))>ωt(∏t) - Φ(x1,∏t(Xt1))>ω* + v∏ω*(xl；i ： ∏*) - v∏ω*(x1)∣Ht]
t
T
X
t
φ(Xl,∏t(Xl))> (ωt(∏t) - ω*)+ V∏ω* (X1;1: ：*)- V∏ω* (X1) ∣Ht
.	(∆h=2)	.
For ∆th=2 we deploy the similar decomposition and upper bound as ∆th=1
△2 = V∏ω* (X1 ；i：n*) - V∏ω* (X1)
Since for both of Vπωt* (Xt1； 1 :∏*) and V∏ω* (X1) We follow the same distribution over policies and
models for 1 time step, the reward at the first time step has the same distribution, therefore we have;
△h=2 = E [v∏ω*(x2；i ： ∏*) - v∏ω* (χ2)Xι, Hti
rustling in
△h=2 = E [v∏ω*(x2； ι ： ∏*) - vω* (χ2)X1, Hti
≤ E Φ(X2,∏t(X2))> (ωt(∏t) - ω*) + V∏ω* (X2；2 : π*) - V∏t* (X2) X1, Ht
'------------{-------------}
(∆th=3)
Similarly we can defined △th=3 , . . . △th=H. The condition on Ht was required to come up with the
mentioned decomposition through △th and it is not needed anymore, therefore;
TH
BayesRegT ≤ E XXΦ(Xh,∏t(Xh))> @tg) - ω*)	(12)
th
T
=XE [1 >Φt (ωt(∏t) - ω*)]	(13)
t
T
=E X l>Φtχ-ιr2χ1/2 (ωt(∏t) - ω*)	(14)
t
-	T	-
≤ E √HX kΦtkχ-ι kωt(∏t)-ω*kχt	(15)
t
-	T	-
≤ E √HX kΦtkχ-ι2θt-ι(δ)	(16)
t
Again similar to optimism we have the maximum expected cumulative reward condition on states of
a episode is at most 1, we have;
-T	-
BayesRegT ≤ √HE Xmin{kΦtkχ-12θt-1(δ), 1}
t
-TH
≤ √HE JXX 2θt-i(δ)min{kφtkχ-ι,1}
24
Under review as a conference paper at ICLR 2019
Moreover, at time T, we can use Jensen’s inequality, exploit the fact that θt (δ) is an increasing
function of t and have
BayesRegT ≤ 2E
∖
T
THθτ(δ)2 Xmin{∣∣Φtkχ-ι,1}
t
≤ 2 (σ,2log(1∕δ) + dlog(1 + TL/λ) + λ"2Lω) E
T
TH X min{kφtkχ-i, 1}
t
∖
≤ 2 (σ,2log(1∕δ) + dlog(1 + TL2∕λ) + λ"2Lω) ,2THdlog(λ + TL2/d)
(17)
Under event ΘT which holds with probability at least 1 - δ. If we set δ = 1/T then the probability
that the event Θ holds is 1 - 1/T and we get regret of at most the RHS of Eq. 17, otherwise with
probability at most 1/T we get maximum regret of T, therefore
BayesRegT ≤ 1 + 2 卜，2 log (T) + dlog(1 + TL2∕λ) + 火松LG ,2THdlog(λ + TL2∕d)
and theorem statement follows.
B.3	Discount factor
Through the analysis of the theorems 1 and 2 we studied the regret upper bounds for undiscounted
reward MDPs when γ = 1. If we consider the problem expression as follows
Rt := ΨRt Vt := Ψνt
the parameter Vt for a general matrix Ψ represent the Stochasticity of the return under any discount
factor 0 ≤ γ ≤ 1. Under the general discount factor, the Assumption 1 describes the sub-Gaussian
characteristics of Vt. Moreover under discounted return, the value function represent the expectation
of the discounted return, resulting in a potentially smaller Lω . Furthermore, in the decompositions
derived in the Eqs. 9 and 12 regarding ∆th we require to aggregate the regret while discounting the
per step regrets. It means We replace 1 with [1, γ,γ2,..., YHT]> in those equations. Consequently,
instead of considering the k 1 k 2 to upper bound the regrets, we are interested in the discounted returns
and consider k[1, γ, γ2, . . . , γH-1]k2 to come up with the corresponding upper bounds and replace
√H in the final statements of the theorems 1 and 2 with a discount factor dependent and smaller
value k[1, γ, γ2, . . . , γH-1]k2.
B.4	Proof of Lemmas
Lemma 3. Let α ∈ Rd be be an arbitrary vector and for any t ≥ 0 define
Ma := eχp (X α φ Vi - 1 kφiαk2 J
Then, for a stopping time under the filtration {Ft}t∞=0, Mτλ ≤ 1.
Proof. Lemma 3 We first show that {Mtα}t∞=0 is a supermartingale sequence. Let
Da = exp (α>φ^ — 1 kΦi.k2)
Therefore, we can rewrite E [Mtα] as follows:
E[Mta|Ft-1] = E D1a... Dta-1Dta|Ft-1 =D1a...Dta-1E[Dta|Ft-1] ≤ Mta-1
The last inequality follows since E [Dta|Ft-1] ≤ 1 due to Assumption 1, therefore since for the first
time step E [M?]≤ 1, then E [Ma ] ≤ 1. For a stopping time T, Define a variable M:="2由"T}
and since E [Mf ] = E [liminft→∞ M：]
holds.
≤ lim inft→∞ E [m：] ≤ 1, therefore the Lemma 3
□
25
Under review as a conference paper at ICLR 2019
Lemma 4. [Extension to Self-normalized bound in Abbasi-Yadkori et al. (2011)] For a stopping time
τ and filtration {Ft}t∞=0, with probability at least 1 - δ
kSτk2-1 ≤ 2σ2 log(det(χt)1/2∕et(e)-1/2 )
χτ	δ
Proof. of Lemma 4. Given the definition of the parameters of self-normalized process, we can
rewrite Mtα as follows;
Ma = exp (α>σSt - 1 kαkχt)
Consider Ω as a Gaussian random vector and f (Ω = α) denotes the density with covariance matrix
of χ-1. Define Mt := E [MΩ∣F∞]. Therefore we have E [Mt] = E [E [Mjp∣Ω]] ≤ 1. Therefore
Mt = ɪaexP (α-S- - 1 kαkχt) f (a)dα
= LeXP (1 kα -χ-1SVσkXt + 1 kSJσkX-ι) f(α)dα
=j*⅞eχp (1 ksVσkχ-1) Z ∕χp (1 kα - χ-1SJσkXt + 1 kαkχ)dα
Since χt and χe are positive semi definite and positive definite respectively, we have
kα - χ-1 St∕σkXt + kak2e = ∣∣α -仅 + χ-1) &/。|片十》+ kχ-1St∕σ^^t - kSt∕σk2χ+χt)-ι
=kα- (e + X-1) St∕σkX+χt + kSt∕σ∣∣X-ι -kSt∕σ∣∣2χ+χt)-i
Therefore,
Mt =	exp (2 kSVσk(χ+χt)τ) ɪaexp (2 kα - (e + χ-1) SJσkχ+χt) dα
=(deg‰ V exP (1 kSt∕σk2」)
Since E [Mτ] ≤ 1 we have
P	kSτ k(2χe+χτ)-1
det(e + χτ中/!! ≤ E kXP(2kSτ∕σk2χ+χτ)-1)]
δ det (e)1/2	~	( det(χ+χτ) ：/2
( δ det(χ) J
≤δ
Where the Markov inequality is deployed for the final step. The stopping is considered to be the time
step as the first time in the sequence when the concentration in the Lemma 4 does not hold. □
Proof of Lemma 1. Given the estimator ωbt we have the following:
(X φ>φi + λI! X(φ> (φiω* + Vi))
(X Φ>Φi + λI! X (Φ>Vi)
Φ> Φi + λI ω*
+ X Φi> Φi + λI
-λ (X Φ>Φi + λl) ω*
26
Under review as a conference paper at ICLR 2019
therefore, for any vector ζ ∈ Rd
Z>bt - ζ>ω* = ζ> (XX Φ>Φi + λI!	XX (Φ>Vi)
(XX Φ>Φi + λI! ω*
As a results, applying CaUchy-SchWarz inequality and inequalities ∣∣ω*kχ-ι
1 l∣ω*∣2 We get
≤ λ(χt)kω*k2 ≤
t
∣Z>ωt- Z>ω*∣≤kZkχ-ιk X(Φ>Vi) kχ-ι + λ∣Zkχ-1kω*kχ-1
i
≤kZkχ-ι (k XX (Φ>Vi) kχ-ι + λ"kω*∣"
Where applying self normalized Lemma 4, With probability at least 1 - δ
IZ>ωt - Z>ω*∣ ≤ kζkχ-i (2σ log (de?： ! + λ"Lω)
t	δ det (χe)
hold for any Z. By plugging in Z = Xt (ωt 一 ω*) we get the following;
∣ωt - ω*kχt ≤ θt(δ) = σ"og (det(Xt)1/2 det(λI)-1/j + λ1/2Lω
The det(Xt) can be written as det(χj = Qd aj therefore, trace(χt) = Pd aj. We also know that;
(Y aj )1/d ≤ Pdαj
j
A matrix extension to Lemma 10 and 11 in Abbasi-Yadkori et al. (2011) results in det(Xt) ≤
(trace(Xt)) while we have trace(χj = trace(λI) + trace(Pt Φ>Φi) ≤ dλ + tL,
1 / 、	(dλ + tL∖ d
det(Xt) ≤ (——J	(18)
therefore the main statement of Lemma 1 goes through.	□
Proof. Lemma 2 We have the following for the determinant of det (XT)
det(Xτ) = det (XT-1 + φTφt )
=det (IH + ΦtX-1ΦT) det (XT_J
T
=∏ det (IH + ΦtX-1Φ>) det(e)
t
From linear algebra, and given a time step t, we define the characteristic polynomial of the matrix
ΦtX-1Φ> as;
CPt(λ) = det(λIH - ΦtX-1Φ>).
If the set {λ1, λ2,..., λd} represent the eigenvalues of ΦtX-1Φ> where λi ≥ λj∙ if i ≥ j, one can
rewrite CPt(λ) as follows;
CP(λ) = (λ - λ1)(λ - λ2) ∙∙∙ (λ - λH
27
Under review as a conference paper at ICLR 2019
Now by adding the matrix IH to Φtχ-1Φ>, We get CP∕(λ) = det(λlH - IH - Φtχ-1Φ>),
therefore;
CPt (λ) = (λ - 1 - λι)(λ - 1 - λ2) ∙∙∙ (λ - 1 - λH)
and the eigenvalues of IH + Φtχ-1Φ> are shifted eigenvalues of Φtχ-1Φ> by +1, i.e. {λι +
1,λ2 + 1,...,λd + 1}. Since ξ>Φtχ-1Φ> ξ = (φχJ2>ξ)	(φχ-1∕2> ξ) all the eigenvalues
{λ1, λ2, . . . , λd} are non-negative. As a results,
det (IH + φtX-1φ>) ≥ (1 + kφtkχ-i)
As a consequence;
T
det(Xτ) ≥ det(e) Y(1 + |西
t
Moreover we have
T
2Xlog(1 + l∣Φtkχ-ι) ≤ 2(log(det(Xτ)) - log(det(e))) ≤ 2dlog(λ + TL2/d)	(19)
t
and the statement follows.
□
28