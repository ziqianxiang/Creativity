Under review as a conference paper at ICLR 2019
Machine Translation With Weakly Paired
Bilingual Documents
Anonymous authors
Paper under double-blind review
Ab stract
Neural machine translation, which achieves near human-level performance in
some languages, strongly relies on the availability of large amounts of parallel
sentences, which hinders its applicability to low-resource language pairs. Recent
works explore the possibility of unsupervised machine translation with monolin-
gual data only, leading to much lower accuracy compared with the supervised one.
Observing that weakly paired bilingual documents are much easier to collect than
bilingual sentences, e.g., from Wikipedia, news websites or books, in this paper,
we investigate the training of translation models with weakly paired bilingual doc-
uments. Our approach contains two components/steps. First, we provide a simple
approach to mine implicitly bilingual sentence pairs from document pairs which
can then be used as supervised signals for training. Second, we leverage the topic
consistency of two weakly paired documents and learn the sentence-to-sentence
translation by constraining the word distribution-level alignments. We evaluate
our proposed method on weakly paired documents from Wikipedia on four tasks,
the widely used WMT16 German什English and WMT13 SPanish什English tasks,
and obtain 24.1/30.3 and 28.1/27.6 BLEU points separately, outperforming state-
of-the-art unsuPervised results by more than 5 BLEU Points and reducing the gaP
between unsuPervised translation and suPervised translation uP to 50%.
1	Introduction
Neural Machine Translation (NMT) is a great success of deeP learning for natural language Pro-
cessing. Thanks to recently develoPed advanced neural network architectures (Cho et al., 2014;
Sutskever et al., 2014; Bahdanau et al., 2015; Jean et al., 2015; Vaswani et al., 2017; Gehring et al.,
2017), NMT has significantly outPerformed statistical machine translation and reached near human-
level Performance for several language Pairs (Wu et al., 2016; Hassan et al., 2018). Such break-
throughs heavily dePend on the availability of large scale of bilingual sentence Pairs. Taking the
WMT14 English→French task as an examPle, NMT uses 38 million Parallel sentence Pairs for
training (Vaswani et al., 2017). As bilingual sentence Pairs are costly to collect, the success of NMT
is not fully realized on the vast majority of language Pairs, esPecially for low-resource languages.
Recently, Artetxe et al. (2017); LamPle et al. (2017) tackle this challenge by training NMT models
using only monolingual data, which achieves considerably good accuracy but still far away from
that of the state-of-the-art suPervised models.
While it is costly to collect bilingual sentence Pairs by human translation, we notice that there
exist many weakly Paired bilingual documents on the Web. For examPle, for the entity “machine
learning”, WikiPedia has multiPle articles in different languages, e.g., the English article and German
article. The two articles have very similar content, but they are not sentence-by-sentence translations,
since they may be indePendently created by different PeoPle. Similarly, an English news in BBC
and a Chinese news in China Daily talk about the same event but maybe with differences in details.
Furthermore, a PoPular novel in different languages is usually liberal translation instead of literal
translation. We call such weakly aligned documents weakly paired bilingual documents. In this
PaPer, we exPlore a new direction of learning NMT models from weakly Paired documents, which
has several advantages. First, weakly Paired documents are much easier to obtain than bilingual
sentence Pairs. We can obtain bilingual document Pairs from WikiPedia Pages, from aligned news
articles on international news websites, even from books. Second, such weakly Paired documents
have great coverage of different languages. For examPle, WikiPedia covers 178 languages and most
1
Under review as a conference paper at ICLR 2019
of them have paired pages to English. This means that learning translation models from paired
bilingual documents are possible for many language pairs.
Weakly aligned document pairs can be utilized for NMT training from two aspects. First, although
such two documents are not exactly sentence-by-sentence translations, itis possible that one specific
sentence in one document is the translation of one sentence in the other document. Such a sentence
pair can be used as bilingual signals for model training. For example in Wikipedia, although the
web structure and paragraphs are generally different for entity “Beijing” in English and “Pekin” in
French, we found the first sentences of the pages are quite semantically similar, in which both of
them define Beijing as the capital of China (English: Beijing, formerly romanized as Peking, is the
capital of the People’s Republic of China. French: Pekin, galement appele Beijing, est la capitale
de la Rpublique populaire de Chine.) The challenge is how to mine such sentence pairs from those
document pairs. Second, although the sentences in two weakly paired documents are not aligned,
the topics of the documents are well aligned. Such topic alignment is a strong signal that can be
used to train NMT models.
In this paper, we focus on Wikipedia data and propose a method to train the machine translation
models by leveraging weakly paired bilingual documents from Wikipedia. The key idea of our
method is to mine implicitly aligned sentence pairs and leverage topic alignment as regularization.
First, we provide a simple and efficient method to mine bilingual sentence pairs from each weakly
aligned document pair. We first train cross-lingual word embeddings from two monolingual corpora
(one for each language) using MUSE (Conneau et al., 2017), and then use weighted average of
embeddings of words in a sentence as the sentence embedding. With the sentence embedding,
we select the sentence pairs with large cosine similarity as bilingual sentence pairs and use them
as supervised signals to train NMT models. Second, many previous works suggest that the word
distribution can be used to well characterize the topic of the document (Petterson et al., 2010; Du
et al., 2015; Funatsu et al., 2014; Pedrosa et al., 2016; Chemudugunta et al., 2007). To leverage the
topic consistency between two weakly paired documents, we minimize the KL-divergence of the
word distributions between the ground-truth document and the model-generated document.
Taking Wikipedia corpus as the training data, we test our method on the widely used WMT16
German什English and WMT13 SPanish什English translation tasks. Our method achieves 24.1/30.3
BLEU points for WMT16 German什EngliSh translations and 28.1/27.6 BLEU points for WMT13
SPaniSh什EngliSh translations, outperforming the state-of-the-art unsupervised method by more
than 5 BLEU points and reduce the gap between unsupervised translation and supervised translation
up to 50%.
2	Related work
Using monolingual data to boost the machine translation performance has attracted a lot of attention
in the literature (Gulcehre et al., 2015; Sennrich et al., 2016a; Zhang & Zong, 2016; Wu et al., 2018;
He et al., 2016), especially when the bilingual supervision is limited. Sennrich et al. (2016a) propose
the back-translation approach which is a popular and effective way to augment the training bilingual
sentence pairs with the target-side monolingual data. He et al. (2016) leverage both the source-side
and target-side monolingual data in a dual learning framework. However, these methods still require
a relatively large amount of labeled bilingual data. Recently, Lample et al. (2017) and Artetxe et al.
(2017) make an initial study of unsupervised machine translation, in which the model is trained from
the monolingual data only. Lample et al. (2017) leverage two key components to learn translation
models from monolingual data: 1) suitable initialization of the translation model by cross-lingual
word embeddings, 2) denoising auto-encoder as language model and reconstruction loss based on
translation-back-translation. Both works leverage monolingual sentences only but do not leverage
rich weakly paired documents from Web.
We study how to leverage document pairs for learning translation without sentence pairs and imple-
ment the proposed approach using Wikipedia data. Leveraging the free online Wikipedia database
as an additional source to improve the natural language processing tasks has also attracted interest
in recent years. For example, Conneau et al. (2017) show that word translation can be effectively
learned based on the embedding trained from Wikipedia. This embedding further becomes one of
the key components for unsupervised machine translation. Different from using Wikipedia to train
warm-start word embeddings, we aim to leverage more and stronger signals from such weakly paired
2
Under review as a conference paper at ICLR 2019
documents to train translation model without parallel sentence pairs. Besides, Halek et al. (2011) use
the category information in Wikipedia corpus to improve the translation of named entities. Drexler
et al. (2014) incorporate language models from target language documents that are comparable to
the source documents in Wikipedia pages to improve the document translation.
There are several works aim at extracting potential sentence pairs from comparable corpus, but most
of them rely on a set of bilingual sentence pairs to train a model and use this model to select sentence
pairs. For example, Adafre & De Rijke (2006) and Yasuda & Sumita (2008) use a strong machine
translation system to obtain a rough translation of a given page in one language into another, and
then calculate word overlap or BLEU score between sentences as measure. Smith et al. (2010) and
Munteanu & Marcu (2005) develop a ranking model/binary classifier to learn how likely a sentence
in target language is a translation of the source language using parallel corpora. However, in our
scenario, we have no bilingual sentence pairs available. That is, we have no bilingual sentence pairs
to train such a model to further select new sentence pairs.
Our work is also related to document-to-document translation (Tu et al., 2018), but with differ-
ent goals and settings. The goal of document-to-document translation is to enhance sentence-to-
sentence translation with stronger signals beyond sentence pairs by using richer inputs, e.g., the topic
information from the document that contains this sentence. During training, it takes one sentence
as well as the cross-sentence (document-level) information as input and predicts the ground-truth
translation sentence in other languages. Therefore, training a document-to-document translation
model requires bilingual sentence pairs and their surrounding contexts. In our scenario, our goal is
to learn a sentence-to-sentence translation model with weaker signals than sentence pairs. We target
to extract useful information from weakly paired documents to train a translation model without
human-labeled bilingual data.
3	Our method
We develop two ways to leverage weakly paired documents: mining implicitly aligned sentence
pairs from the document pairs and aligning the topic distributions of two documents in a weakly
aligned pair. Before diving into details, we first introduce some notations.
Denote D = {(diX, diY )}, i ∈ {1, 2, ..., M} as the set of weakly paired documents, in which docu-
ment diX is aligned to diY . For example, diX and diY are two cross-lingual linked Wikipedia pages.
Denote niX and niY as the number of sentences in document diX and diY respectively. Note that
usually niX 6= niY .
Without any confusions, we denote x as a sentence in language X and y as a sentence in language Y .
We denote enc as the encoder for language X and Y , which maps a sentence x or y into a sequence
of real vectors using parameter θenc. We use dec with parameter θdec as the decoder, which takes the
encoded vectors and target language tag (X or Y ) as inputs and outputs a probability distribution
over sentences in the target language. Let θ denote all the parameters of the translation model.
Similar to Artetxe et al. (2017); Lample et al. (2017), such a model can handle both X → Y
translation and Y → X translation.
3.1	Mining implicitly aligned sentence pairs
Different language versions of Wikipedia pages about the same entity/event are usually created by
different people speaking different native languages, and therefore most sentences in two weakly
aligned documents are not aligned. Even though, there is still a small chance that some bilingual
sentences are aligned, and we try to mine such implicitly aligned sentence pairs and use them as
supervision for NMT model training.
Imankulova et al. (2017) extract bilingual sentence pairs from Wikipedia using a well-trained trans-
lation model learned from supervised sentence pairs. This method does not work for us since we
do not have aligned bilingual sentence pairs. Instead, our idea is to compute the similarity of two
bilingual sentences using their cross-lingual sentence embeddings and choose the pairs with large
similarity as aligned bilingual sentence pairs.
3
Under review as a conference paper at ICLR 2019
Sentence embedding is widely used to measure textual similarity in text classification tasks (Arora
et al., 2017; Le & Mikolov, 2014; Wieting et al., 2015). Arora et al. (2017) compute the weighted
average of the word embedding in one sentence where the weight depends on word frequency and
then project away the weighted average sentence embeddings from their first principal component.
This method achieves good performance on a range of monolingual textual similarity task. We
extend their method from monolingual sentence embedding to cross-lingual sentence embedding,
given the cross-lingual word emebddings are pre-trained using MUSE (Conneau et al., 2017). The
detailed method is described as follows.
For each word w, we denote ew as the word embedding trained from MUSE (Conneau et al., 2017),
p(w) as the estimated frequency from another document and a as a predefined parameter to calculate
the weight of word embedding. We denote ^s as the weighted average sentence embedding of
sentence s and E as the embedding matrix of all the sentences over the monolingual corpus. Then
we remove the first principal components u1 of E for every weighted average sentence embedding
es and use the resulting embedding e§ as the final sentence embedding, i.e.,
u1
=X —a-e,,1
W∈s a+P(W),
→ PCA(E),
T
=es - UIUI es .
(1)
(2)
(3)
Based on the sentence embedding, we estimate the similarity between two sentences in different
languages by their cosine similarity sim(sx ,sY)= ＜。；『：联/.For each weakly aligned document
pair (diX, diY), we have niX × niY pairs of sentences and form a bipartite graph between sentences
in two documents where the weight of an edge between two cross-lingual sentences is their cosine
similarity score. The goal is to find the most confident edges (sentence pairs) from this weighted
bipartite graph. We adopt a greedy selection approach with two constraints: The first constraint is
that the weight of a selected edge must be larger than threshold c1 , which is to ensure that the two
sentences are similar enough. The second constraint is that the weight of a selected edge must be
larger than the weights of all other edges connected to these two nodes (sentences) by threshold
c2. This ensures that the pair we selected is unique enough. Denote S = {(sjX, sjY)} as the set of
selected sentence pairs. We use those pairs as supervision for model training, i.e., minimizing the
negative log-likelihood as below.
Lp(S； θ) =	& X logPX→y(SYIsX； θ) + & X	logPy→x(SXIsY； θ) (4)
| | (sX,sY)∈S	| | (sX,sY)∈S
3.2	Aligning Topic Distribution
Although cross-lingual linked Wikipedia pages are not aligned in sentences, they are usually aligned
in topic distribution because they talk about the same event or entity. For example, the English top-
ical words “politician”, “United State” and “president” will appear in the English page for “Donald
Trump”, and similar topical words in French will appear in the corresponding French page. That
being said, ifwe translate an article from English to French sentence-by-sentence, the word distribu-
tion of the translated article should be generally similar to the word distribution of the corresponding
article in French. Here we leverage the document-level word-distribution alignment to enhance and
regularize the training of a translation model.
Given an NMT model, we first translate a document diX through sentence translation and obtain a
document dY. Then We evaluate the word distributions between the generated document d[ and the
ground-truth document diY, and use such signal to optimize the model. However, straight-forward
loss design, e.g., KL-divergence or Wasserstein distance between the word distributions of dY and
diY is not differentiable with respect to the NMT model, due to the non-differentiable operation
(greedy search or beam search) while generating dγ.
To address this challenge, we need to design some loss function that is smooth with respect to
the model parameters. Our proposal is assuming each generated sentence ^Y ∈ d is fixed and
“refeeding” the pair (sXk, SY) to the model to get the probability distribution over all the words at
each position. We calculate the word distribution by averaging word probability distributions over
4
Under review as a conference paper at ICLR 2019
all sentences and positions of the generated document. Mathematically, we have
PwYMlsXk,4 sYk,<t) 〜PX→Y(WYISXk, SYk,<t； θ),	⑸
P(WY； dχ,θ) « X P(Wt|^Yk,<t),	⑹
i,k,t
where P(WY ; diX , θ) “acts” as the model-generated word distribution for document diX . Note that
this averaged distribution is differentiable to model parameters. The word distribution of the ground-
truth document dγ is P (wy ; dγ) = ##WenjdY. We simply use KL-divergence loss as the objec-
tive. Then the document alignment loss for X → Y translation is defined as
Ld(D； θ,X → Y) = ɪ X	KL(P(wY； dY)I∣P(wY； dX,θ)).	⑺
|	| (diX,diY)∈D
The corresponding document loss for Y → X translation can be defined in the same way. The final
loss is as follows, which can be optimized using backpropagation thanks to its smoothness.
Ld(D; θ) = Ld(D; θ,Χ → Y) + Ld(D; θ,Y → X)	(8)
3.3	Overall algorithm
In addition to the above two ways of using Wikipedia data, the sentences in the weakly paired
documents can be used as monolingual data to optimize the losses of the unsupervised machine
translation. Therefore, our proposed losses can be combined with the loss functions of unsupervised
machine translation. Here we first recap unsupervised machine translation (Lample et al., 2017),
and then present our overall algorithm in Algorithm 1.
The unsupervised machine translation considers two loss functions. Given a monolingual sentence
s in language X/Y, the denoising auto-encoder loss is defined as Ldae = Es〜X [log P(SIc(S)； θ)] +
Es〜Y[logP(s∣c(s); θ)], where c(.) is to drop and SWaP words in sentence s. As for the reconstruc-
tion loss, given S in language X/Y and the translated sentence S0 in Y/X by model PX→Y /PY→X,
the reconstruction loss is defined as Lrec = Es〜X [log PY→χ (s∣s0; θ)] + Es〜Y [log Pχ→Y(s∣s0; θ)].
Denote the combination of the monolingual data in language X and Y as M . We define the overall
loss on monolingual data M as Lm (M; θ) = Ldae + Lrec. Finally, the overall training objective of
our algorithm is to minimize the following loss function with hyperparameters α and β :
L = Lm(M; θ) + αLp(S; θ) + βLd(D; θ).	(9)
Algorithm 1 Training Algorithm
Require: Initial translation model with parameter θ; monolingual dataset M, implicitly aligned
sentence pairs dataset S, weakly paired documents dataset D; optimizer Opt
1:	while not converged do
2:	Randomly sample a mini-batch monolingual sentences from M, implicitly aligned sentence
pairs from S and weakly paired documents from D
3:	Calculate loss Lm, Lp and Ld
4:	Update θ by minimizing the objective Eqn. (9) using optimizer Opt
5:	end while
4 Experiments
We test our method on several benchmark translation tasks. We first describe the data preparation
and experimental design, and then present the main results, followed by some deep studies.
5
Under review as a conference paper at ICLR 2019
4.1	Data preparation
Wikimedia offers free copies of all available contents on Wikipedia for multiple languages. We
download the language specific Wikipedia contents1 in XML format, and use WikiExtractor2 to
extract and clean the texts. The numbers of Wikipedia documents are listed in Table 1. We then
use the sentence tokenizer from toolkit NLTK to generate segmented sentences from Wikipedia
documents.
Many Wikipedia pages contain external links to the pages that describe the same entity but in dif-
ferent languages. We extract weakly paired documents using these external links. We filter out a
document pair if any document in the pair contains less than 5 sentences. We also remove the sen-
tences longer than 100 words. We conduct experiments on two language pairs: English-German
(En-De for short) translation and English-Spanish (En-Es for short) translation. Statistics of the
processed WikiPedia documents are provided in Table 1.
We use the monolingual data as in Lample et al. (2017;	Language	∣ #Wiki Documents
2018) together with Wikipedia document pairs to train	English	5,684,240
NMT models. For the En-De task, we use all available	German	2,201,782
sentences from the WMT monolingual News Crawl	Spanish	1,389,469
datasets from year 2007 to 2017 containting about 50		一
million sentences for each language. For En-Es, we	Task	#Document Pairs
use News Crawl datasets from year 2007 to 2012 con-	English-German	948,631
taining about 10 million sentences. The trainslation	English-Spanish	836,564
models are evaluated on newstest 2016 dataset for En-		—
De and newstest 2013 dataset for En-Es which are
widely used (Koehn & Knowles, 2017).
Table 1: Statistics of Wikipedia data, in-
cluding numbers of documents and weakly
paired documents.
4.2	Experimental Design
To mine implicitly aligned sentences from weakly paired documents, we use the open-sourced word
embeddings trained by Fasttext (Joulin et al., 2016) and use MUSE3 to build cross-lingual word
embeddings. We then generate sentence embeddings with the weighted average of cross-lingual
word embeddings and further remove the top-1 principal components of sentence embedding matrix
as introduced in Section 3.1. We set the two thresholds c1 = 0.7 and c2 = 0.1 respectively when
selecting sentence pairs, and the parameter a to calculate the weight of word embedding is 0.001.
To translate a document diX for topic alignment, we use greedy search.
For the training of translation models, the monolingual datasets, Wikipedia document pairs and
mined sentence pairs are jointly processed by BPE (Sennrich et al., 2016b) with 60, 000 codes. For
model initialization, we follow Lample et al. (2018), which uses cross-lingual BPE embeddings to
initialize the shared lookup tables, and the cross-lingual BPE embeddings are trained by fastText
with embedding dimension 512, a context windows of size 5 and 10 negative samples. We adopt the
Transformer (Vaswani et al., 2017) architecture in our experiments. We stack 6 layers in both the
encoder and the decoder. Following Lample et al. (2018), we share the lookup tables between the
encoder and the encoder, and between the source and target languages. The dimension of the hidden
state is 512. The weights α and β of the loss functions are set to be 1 and 0.05. For training, we
use Adam optimizer (Kingma & Ba, 2014) and the same learning rate scheduler as used in Vaswani
et al. (2017). For decoding, we use beam search with beam width 4 and length penalty 0.6, and the
BLEU (Papineni et al., 2002) score is measured by multi-bleu.perl script4.
1For example, we download English/German Wikipedia contents from https://dumps.wikimedia.
org/enwiki, and https://dumps.wikimedia.org/dewiki/.
2https://github.com/attardi/wikiextractor
3https://github.com/facebookresearch/MUSE
4https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/
multi- bleu.perl
6
Under review as a conference paper at ICLR 2019
Method	En一De	De→En	En→Es	Es→En
Lample et al. (2017)	9.6	13.3	-	-
Yang et al. (2018)	10.9	14.6	-	-
NMT (Lample et al., 2018)	17.2	21.0	19.7	20.0
PBSMT (Lample et al., 2018)	17.9	22.9	-	-
PBSMT + NMT (Lample et al., 2018)	20.2	25.2	-	-
NMT + First Wiki Sentence	16.3	19.3	17.3	18.3
NMT + Document Translation	12.0	14.9	14.5	15.3
Ours	24.2	30.3	28.1	27.6
Supervised	33.6	38.2	33.2	32.9
Table 2: BLEU scores compared with previous approaches. ‘First Wiki Sentence’ and ‘Document
Translation’ settings are introduced in Section 4.3.
4.3	Main Results
Our method is compared with several previous works (Lample et al., 2018; 2017; Yang et al., 2018)
in Table 2. We also consider some simple and heuristic ways of using the weakly paired documents
from Wikipedia as baselines. One baseline, referred as “NMT + First Wikipedia Sentence”, is to
directly use the first sentence of the aligned documents as an aligned sentence pair to train NMT
model. The motivation behind it is that usually the first sentence of a Wikipedia document sum-
marizes the main content of the document, which is more likely to be similar across languages.
The second baseline, referred as “NMT + Document Translation”, is to treat the weakly aligned
documents as two long sentences and use them as a bilingual sentence pair to train NMT models.
From Table 2, our approach achieves BLEU score of 24.1 and 30.3 on En→De and De→En trans-
lations respectively, which are the state-of-the-art numbers. Previous best performance is achieved
by Lample et al. (2018): combining phrase-based approach and neural machine translation together,
they achieve 20.2 and 25.2 BLEU scores on En→De and De→En translation. Our approach out-
performs their method by more than 4 and 5 BLEU points on En→De and De→En respectively.
For the En-Es, we achieve 28.1 and 27.6 BLEU scores on En→Es and Es→En respectively, with
more than 8 and 7 points improvement over unsupervised Transformer baseline models. For the two
heuristic baselines, we find that they hurt the performance of NMT models with more than 2 points
decrease in terms of BLEU score. The results indicate that the careful utilization of Wikipedia data
is important, which is also verified by the superior performance of our approach.
Furthermore, we report the supervised result in the last row of Table 2. The supervised setting is
conducted on the full training set on WMT16 En-De translation with 4.5 million parallel sentences,
and WMT13 En-Es translation with 3.8 million parallel sentences. As shown in the table, our
approach takes a big step towards the supervised result, and reduce the gap between unsupervised
translation and supervised translation up to 50%.
4.4	Further Studies
In this subsection, we provide an ablation study to our method, check the performance variance
with respect to the thresholds for mining implicit sentence pairs, and finally present several cases of
mined sentence pairs. The studies are conducted on the En-De translation pair.
Ablation Study			
Our method leverages weakly paired docu-	Our Method	En→De	De→En
ments in two ways. To better understand the importance of the two ways, we report results from an ablation study in Table 3. From the ta- ble, we can see that removing the topic align- ment, the accuracy drops with more than 1 BLEU points. Without the implicitly aligned	with Lp and Ld without Ld without Lp	24.2 22.9 18.5	30.3 28.7 23.3
	Table 3: Ablation study of our method on English- German translation tasks.		
7
Under review as a conference paper at ICLR 2019
28
34
26
24
22
20
En → De, Ci = 0.70
En →De, Ci = 0.75
32
De → Fn, Ci = 0.70
De → Fn, Ci = 0.75
LILi ⅛Jdι
18
0.0
0.2
24
0.0
0.2
(a)	En→De BLEU performance.
(b)	De→En BLEU performance.
Figure 1: BLEU performance w.r.t. different thresholds for English-German translation pair.
sentence pairs, the accuracy decreases with about 6 BLEU points. These findings clearly demon-
strate that both the two ways are important, and both contribute to the improvement of translation
accuracy.
Impact of Sentence Quality
As introduced before, to control the data qual-
ity of our mined sentence pairs, We set two
constraints with threshold ci and c2. Here
We present the paired sentence number and
the final BLEU scores with respect to these
two thresholds in Table 4 and Figure 1. We
vary the threshold ci in {0.70,0.75} and c?
in {0.0,0.1,0.2}. For ci ≤ 0.70, we ob-
serve that the sentence quality is poor, while for
ci ≥ 0.75, we can only extract few pairs. As
ci /c2	English-German		
	0.0	0.1	0.2
0.70	257,947	199,965	132,403
0.75	100,497	84,271	58,814
Table 4: Number of the mined sentence pairs w.r.t.
different thresholds for English-German.
shown in the table and the figure, with more strict constraint, we obtain fewer sentence pairs but
with higher quality. We can see that there is a clear trade-off between the data quality and the data
number. A good configuration is ci = 0.7 and c2 = 0.1.
Case Study of Mined Sentence Pairs
We present three cases of our selected sentence pairs in Table 5. We can see that our approach
can mine high-quality pairs, such as the first case, in which one sentence is a good translation of
the other one. Besides, our method can select interesting paired sentences with similar, if not the
exactly same, semantics. As shown in the second case, the two sentences are almost semantically the
same, while the German word “gewahlt”(“elected” in English) is not included in the corresponding
English sentence. Also in the last case, the detailed description “15 Meter hoch” (which means “15
meters high” in English) in the German sentence is missed in the English sentence. Although they
are not exact translations, such sentence pairs are still very helpful for NMT model training, as
demonstrated by the results of our method.
5 Conclusion and Future Work
In this work, we proposed a general method to train neural machine translation models using weakly
paired bilingual documents from the Web, e.g., Wikipedia. Our approach contains two key compo-
nents: mining the implicitly aligned sentence pairs and aligning topic distributions. Experiments on
public test benchmarks verify the effectiveness of our method.
8
Under review as a conference paper at ICLR 2019
English There are several different types of roles to be used in different situations .
German Es gibt VersChiedene Arten Von Rollen , die in UntersChiedliChen Situationen eingesetzt Werden .
English He Was a member of the SWedish ACademy from 1912 .
German 1912 wurde er Zum Mitglied der Schwedischen Akademie gewahlt.
English	The statue and its marble base stand tall .
German	Die Statue ist, mit ihrem Sockel aus Marmor , 15 Meter hoch .
Table 5: Cases-studies of the sentenCe pairs mined by our approaCh for English-German.
For future work, we will apply our method to more language pairs, such as other hundreds of lan-
guages supported by Wikipedia. Second, we will study unsuperVised machine translation using
weakly paired documents from other data resources, such as news websites. Third, we will inVesti-
gate better ways to utilize such weakly paired documents, going beyond mining sentence pairs and
aligning topic distributions.
References
Sisay Fissaha Adafre and Maarten De Rijke. Finding similar sentences across multiple languages
in wikipedia. In Proceedings of the Workshop on NEW TEXT Wikis and blogs and other dynamic
text sources, 2006.
SanjeeV Arora, Yingyu Liang, and Tengyu Ma. A simple but tough-to-beat baseline for sentence
embeddings. In ICLR, 2017.
Mikel Artetxe, Gorka Labaka, Eneko Agirre, and Kyunghyun Cho. UnsuperVised neural machine
translation. arXiv preprint arXiv:1710.11041, 2017.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. ICLR, 2015.
Chaitanya Chemudugunta, Padhraic Smyth, and Mark SteyVers. Modeling general and specific
aspects of documents with a probabilistic topic model. In Advances in neural information pro-
cessing Systems, pp. 241-248, 2007.
Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder
for statistical machine translation. In EMNLP, pp. 1724-1734. Association for Computational
Linguistics, October 2014.
Alexis Conneau, Guillaume Lample, Marc,Aurelio Ranzato, Ludovic Denoyer, and HerVe Jegou.
Word translation without parallel data. arXiv preprint arXiv:1710.04087, 2017.
Jennifer Drexler, Pushpendre Rastogi, Jacqueline Aguilar, Benjamin Van Durme, and Matt Post. A
wikipedia-based corpus for contextualized machine translation. In LREC, pp. 3593-3596, 2014.
Jianguang Du, Jing Jiang, Dandan Song, and Lejian Liao. Topic modeling with document relatiVe
similarities. IJCAI, 2015.
Toshiaki Funatsu, Yoichi Tomiura, Emi Ishita, and Kosuke Furusawa. Extracting representatiVe
words of a topic determined by latent dirichlet allocation. in eknow 2014. In The Sixth Interna-
tional Conference on Information, Process, and Knowledge Management, pp. 112-117, 2014.
Jonas Gehring, Michael Auli, DaVid Grangier, Denis Yarats, and Yann N Dauphin. ConVolutional
sequence to sequence learning. arXiv preprint arXiv:1705.03122, 2017.
Caglar Gulcehre, Orhan Firat, KelVin Xu, Kyunghyun Cho, Loic Barrault, Huei-Chi Lin, Fethi
Bougares, Holger Schwenk, and Yoshua Bengio. On using monolingual corpora in neural ma-
chine translation. arXiv preprint arXiv:1503.03535, 2015.
9
Under review as a conference paper at ICLR 2019
Ondrej Halek, Rudolf Rosa, Ales Tamchyna, and Ondrej Bojar. Named entities from WikiPedia for
machine translation. In ITAT., pp. 23-30, 2011.
Hany Hassan, Anthony Aue, Chang Chen, Vishal ChoWdhary, Jonathan Clark, Christian Federmann,
Xuedong Huang, Marcin Junczys-DoWmunt, William LeWis, Mu Li, et al. Achieving human
parity on automatic chinese to english neWs translation. arXiv preprint arXiv:1803.05567, 2018.
Di He, Yingce Xia, Tao Qin, LiWei Wang, Nenghai Yu, Tieyan Liu, and Wei-Ying Ma. Dual learning
for machine translation. In Advances in Neural Information Processing Systems, pp. 820-828,
2016.
Aizhan Imankulova, Takayuki Sato, and Mamoru Komachi. Improving loW-resource neural machine
translation With filtered pseudo-parallel corpus. In Proceedings of the 4th Workshop on Asian
Translation (WAT2017), pp. 70-78, 2017.
Sebastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. On using very large
target vocabulary for neural machine translation. In ACL-IJCNLP, pp. 1-10. Association for
Computational Linguistics, July 2015.
Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Herve Jegou, and Tomas
Mikolov. Fasttext.zip: Compressing text classification models. arXiv preprint arXiv:1612.03651,
2016.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Philipp Koehn and Rebecca Knowles. Six challenges for neural machine translation. In Proceedings
of the First Workshop on Neural Machine Translation, pp. 28-39, 2017.
Guillaume Lample, Ludovic Denoyer, and Marc’Aurelio Ranzato. Unsupervised machine transla-
tion using monolingual corpora only. arXiv preprint arXiv:1711.00043, 2017.
Guillaume Lample, Myle Ott, Alexis Conneau, Ludovic Denoyer, and Marc’Aurelio Ranzato.
Phrase-based & neural unsupervised machine translation. arXiv preprint arXiv:1804.07755, 2018.
Quoc Le and Tomas Mikolov. Distributed representations of sentences and documents. In Interna-
tional Conference on Machine Learning, pp. 1188-1196, 2014.
Dragos Stefan Munteanu and Daniel Marcu. Improving machine translation performance by ex-
ploiting non-parallel corpora. Computational Linguistics, 31(4):477-504, 2005.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic eval-
uation of machine translation. In ACL, pp. 311-318. Association for Computational Linguistics,
2002.
Gabriel Pedrosa, Marcelo Pita, Paulo Bicalho, Anisio Lacerda, and Gisele L Pappa. Topic modeling
for short texts with co-occurrence frequency-based expansion. In 2016 5th Brazilian Conference
on Intelligent Systems (BRACIS), pp. 277-282. IEEE, 2016.
James Petterson, Wray Buntine, Shravan M Narayanamurthy, TiberiO S Caetano, and Alex J Smola.
Word features for latent dirichlet allocation. In Advances in Neural Information Processing Sys-
tems, pp. 1921-1929, 2010.
Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation models
with monolingual data. In ACL, 2016a.
Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with
subword units. In ACL, volume 1, pp. 1715-1725, 2016b.
Jason R Smith, Chris Quirk, and Kristina Toutanova. Extracting parallel sentences from compa-
rable corpora using document level alignment. In NAACL-HLT, pp. 403-411. Association for
Computational Linguistics, 2010.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
In Advances in neural information processing systems, pp. 3104-3112, 2014.
10
Under review as a conference paper at ICLR 2019
Zhaopeng Tu, Yang Liu, Shuming Shi, and Tong Zhang. Learning to remember translation history
with a continuous cache. Transactions of the Association of Computational Linguistics, 6:407-
420, 2018.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-
mation Processing Systems, pp. 6000-6010, 2017.
John Wieting, Mohit Bansal, Kevin Gimpel, Karen Livescu, and Dan Roth. From paraphrase
database to compositional paraphrase model and back. arXiv preprint arXiv:1506.03487, 2015.
Lijun Wu, Fei Tian, Tao Qin, Jianhuang Lai, and Tie-Yan Liu. A study of reinforcement learning
for neural machine translation. arXiv preprint arXiv:1808.08866, 2018.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine transla-
tion system: Bridging the gap between human and machine translation. In Transactions of the
Association for Computational Linguistics, 2016.
Zhen Yang, Wei Chen, Feng Wang, and Bo Xu. Unsupervised neural machine translation with
weight sharing. arXiv preprint arXiv:1804.09057, 2018.
Keiji Yasuda and Eiichiro Sumita. Method for building sentence-aligned corpus from wikipedia. In
2008 AAAI Workshop on Wikipedia and Artificial Intelligence (WikiAI08), pp. 263-268, 2008.
Jiajun Zhang and Chengqing Zong. Exploiting source-side monolingual data in neural machine
translation. In EMNLP, pp. 1535-1545, 2016.
11