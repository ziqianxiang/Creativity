Under review as a conference paper at ICLR 2019
Improved Learning of One-hidden-layer Con-
volutional Neural Networks with Overlaps
Anonymous authors
Paper under double-blind review
Ab stract
We propose a new algorithm to learn a one-hidden-layer convolutional neural net-
work where both the convolutional weights and the outputs weights are parameters
to be learned. Our algorithm works for a general class of (potentially overlapping)
patches, including commonly used structures for computer vision tasks. Our al-
gorithm draws ideas from (1) isotonic regression for learning neural networks and
(2) landscape analysis of non-convex matrix factorization problems. We believe
these findings may inspire further development in designing provable algorithms
for learning neural networks and other complex models. While our focus is theo-
retical, we also present experiments that illustrate our theoretical findings.
1	Introduction
Giving provably efficient algorithms for learning neural networks is a core challenge in machine
learning theory. The case of convolutional architectures has recently attracted much interest due to
their many practical applications. Recently Brutzkus & Globerson (2017) showed that distribution-
free learning of one simple non-overlapping convolutional filter is NP-hard. A natural open question
is whether we can design provably efficient algorithms to learn convolutional neural networks under
mild assumptions.
We consider a convolutional neural network of the form
k
f (x, w, a) = X ajσ (WJPjx)	(1)
j“1
where w P Rr is a shared convolutional filter, a P Rk is the second linear layer and
Pj = r 0 I 0	SP RrXd
j	lomon lomon	lomon
(j´l)s	r	d´pj´1qs'r
selects the ((j — 1)s ' 1)-th to ((j — 1)s ' r)-th coordinates of X with stride S and σ (∙) is the
activation function. Note here that both w and a are unknown vectors to be learned and there may
be overlapping patches because the stride size s may be smaller than the filter size r.
Our Contributions We give the first efficient algorithm that can provably learn a convolutional
neural network with two unknown layers with commonly used overlapping patches. Our main result
is the following theorem.
Theorem 1.1 (Main Theorem (Informal)). Suppose S 2[2U ' 1 and the marginal distribution is
symmetric and isotropic. Then the convolutional neural network defined in equation 1 with piecewise
linear activation functions is learnable in polynomial time.
We refer readers to Theorem 3.1 for the precise statement.
Technical Insights Our algorithm is a novel combination of the algorithm for isotonic regression
and the landscape analysis of non-convex problems. First, inspired by recent work on isotonic
regression, we extend the idea in Goel et al. (2018) to reduce learning a CNN with piecewise linear
activation to learning a convolutional neural network with linear activation (c.f. Section 4). Second,
1
Under review as a conference paper at ICLR 2019
we show learning a linear convolutional filter can be reduced to a non-convex matrix factorization
problem which admits a provably efficient algorithm based on non-convex geometry (Ge et al.,
2017a). Third, in analyzing our algorithm, we present a robust analysis of Convotron algorithm
proposed by Goel et al. (2018), in which we draw connections to the spectral properties of Toeplitz
matrices. We believe these ideas may inspire further development in designing provable learning
algorithms for neural networks and other complex models.
Related Work From the point of view of learning theory, it is well known that training is compu-
tational infeasible in the worst case (Goel et al., 2016; Brutzkus & Globerson, 2017). Thus distribu-
tional assumptions are needed for efficient learning. A line of research has focused on analyzing the
dynamics of gradient descent conditioned on the input distribution being standard Gaussian (Tian,
2017; Soltanolkotabi, 2017; Li & Yuan, 2017; Zhong et al., 2017b; Brutzkus & Globerson, 2017;
Zhong et al., 2017a; Du et al., 2017). Specifically for convolutional nets, existing analyses heavily
relied on the analytical formulas which can only be derived if the input is Gaussian and patches are
non-overlapping.
Recent work has tried to relax the Gaussian input assumption and the non-overlapping structure
for learning convolutional filters. Du et al. (2017) showed if the patches are sufficiently close to
each other then stochastic gradient descent can recover the true filter. Goel et al. (2018) proposed a
modified iterative algorithm inspired from isotonic regression that gives the first recovery guarantees
for learning a filter for commonly used overlapping patches under much weaker assumptions on the
distribution. However, these two analyses only work for learning one unknown convoutional filter.
Moving away from gradient descent, various works have shown positive results for learning general
simple fully connected neural networks in polynomial time and sample complexity under certain
assumptions using techniques such as kernel methods (Goel et al., 2016; Zhang et al., 2015; Goel &
Klivans, 2017a;b) and tensor decomposition (Sedghi & Anandkumar, 2014; Janzamin et al., 2015).
The main drawbacks include the shift to improper learning for kernel methods and the knowledge of
the probability density function for tensor methods. In contrast to this, our algorithm is proper and
does not assume that the input distribution is known.
Learning a neural network is often formulated as a non-convex problem. If the objective function
satisfies (1) all saddle points and local maxima are strict (i.e., there exists a direction with negative
curvature), and (2) all local minima are global (no spurious local minmum), then noise-injected
(stochastic) gradient descent (Ge et al., 2015; Jin et al., 2017) finds a global minimum in polynomial
time. Recent work has studied these properties for the landscape of neural networks (Kawaguchi,
2016; Choromanska et al., 2015; Hardt & Ma, 2016; Haeffele & Vidal, 2015; Mei et al., 2016;
Freeman & Bruna, 2016; Safran & Shamir, 2016; Zhou & Feng, 2017; Nguyen & Hein, 2017a;b;
Ge et al., 2017b; Zhou & Feng, 2017; Safran & Shamir, 2017; Du & Lee, 2018). A crucial step in
our algorithm is reducing the convolutional neural network learning problem to matrix factorization
and using the geometric properties of matrix factorization.
2	Preliminaries
We use bold-faced letters for vectors and matrices. We use ∣∣∙}2 to denote the Euclidean norm of a
finite-dimensional vector. For a matrix A, we use λmax pAq to denote its eigenvalue and λmin pAq
its smallest singular value. Let O(∙) and Ω (∙) denote standard Big-O and Big-Omega notations,
only hiding absolute constants.
In our setting, we have n data points txi, yi uin“1 where xi P Rd and y P R. We assume the
label is generated by a two-layer convolutional neural network with filter size r, stride s and k
hidden neurons. Compactly We can write the formula in the following form: yi = f (xi, w*, a*),
xi „ Z where the prediction function f is defined in equation 1. To obtain a proper scaling, we let
}w* ∣∣2 }a* }2 “ σι. We also define the induced patch matrix as
P (χ) = [Pιχ ... PkXsP RrXk
which will play an important role in our algorithm design. Our goal is to properly learn this con-
volutional neural network, i.e., design a polynomial time algorithm which outputs a pair pw, aq that
satisfies Eχ~z ∣(f(w, a, x) — f(w*, a*, x))2] ≤ e.
2
Under review as a conference paper at ICLR 2019
Algorithm 1 Learning One-hidden-Layer Convolutional Network
Input: Input distribution Z. Number of iterations: T1, T2. Number of samples: T3. Step sizes:
ηι > 0, η2 > 0.
Output: Parameters of the one-hidden-layer CNN: w and a.
1:	Stage 1: Run Double Convotron (Algorithm 2) for T1 iterations with step size η1 to obtain apT1q.
2:	Stage 2: RUn Convotron (Algorithm 3) using apT1q and —aPTIq for T2 iterations and step size
η2 to obtain wp'q and wp´).
3:	Stage 3: Choose parameters with lower empirical loss on T3 samples drawn from Z from
'wp'q, apTq) and (wp´), —apTq).
3 Main Result
In this section we describe our main result. We first list our main assumptions, followed by the
detailed description of our algorithm. Lastly we state the main theorem which gives the convergence
guarantees of our algorithm.
3.1	Assumptions
Our first assumption is on the input distribution Z . We assume the input distribution is symmetric,
bounded and has identity covariance. The symmetry assumption is used in Goel et al. (2018) and
many learning theory papers Baum (1990). The identity covariance assumption is true if the data
whitened. Further, in many architectures, the input of certain layers is assumed having these proper-
ties because of the use of batch normalization (Ioffe & Szegedy, 2015) or other techniques. Lastly,
the boundedness is a standard regularity assumption to exclude pathological input distributions.
We remark that this assumption considerably weaker than the standard Gaussian input distribution
assumption used in Tian (2017); Zhong et al. (2017a); Du et al. (2017), which has the rotational
invariant property.
Assumption 3.1 (Input Distribution Assumptions). We assume the input distribution satisfies the
following conditions.
•	Symmetry: P pxq “ P p—xq .
•	Identity covariance: Ex„Z xxT “ I.
•	Boundedness: @x 〜 Z, }x}2 ≤ B almost Surelyforsome B > 0.
Our second assumption is on the patch structure. In this paper we assume the stride is larger than
half of the filter size. This is indeed true for a wide range of convolutional neural network used in
computer vision. For example some architecture has convolutional filter of size 3 and stride 2 and
some use non-overlapping architectures (He et al., 2016).
Assumption 3.2 (Large Stride). S 对 2 U +1.
Next we assume the activation function is piecewise linear. Commonly used activation functions
like rectified linear unit (ReLU), Leaky ReLU and linear activation all belong to this class.
Assumption 3.3 (Piece-wise Linear Activation).
/、	(X	if X20
σpxq = (αx	if x < 0 .
3.2	Algorithm
Now we are ready to describe our algorithm (see Algorithm 1). The algorithm has three stages, first
we learn the outer layer weights upto sign, second we use these fixed outer weights to recover the
filter weight and last we choose the best weight combination thus recovered.
3
Under review as a conference paper at ICLR 2019
Stage 1: Learning the Non-overlapping Part of the Convolutional Filter and Linear Weights
Our first observation is even if there may be overlapping patches, as long as there exists some non-
overlapping part, we can learn this part and the second layer jointly. To be specific, with filter size
being r and stride being s, if S2[2U ' 1, for j = 1....,k We define the selection matrix for the
non-overlapping part of each patch
Note that for any j1 ‰ j2, there is no overlapping betWeen the selected coordinates by Pjnon and
Pnon. Therefore, for a filter w, there is a segment [w『´s`i,..., Wss with length (2s — r) which acts
on the non-overlapping part of each patches. We denote wnon “ rwr´s`1, . . . , wss and our goal in
this stage is to learn Wnon and a* jointly.
In this stage, our algorithm proceeds as follows. Given wnon, a and a sample px, y), we define
2
g (Wnon , a, x, y) “
1 ' Y
k1
(f(Wnon, a, X)— y) X aiPnonX ' j
j“1	4
Wnon }22 — }a}22 Wnon (2)
h pwnon , a, x, y) “
(/(Wnon, a, X)´ j)
J non
wnonP1 x
J non
wnon k x
+ 4 (}a}2 — }wnon}2) a
(3)
2
1 ' γ
where f (wnon, a, x) “ X：=i ajσ (WJonPnonX) is the prediction function only using Wnon
As will be apparent in Section 4, g and h are unbiased estimates of the gradient for the loss func-
tion of learning a linear CNN. The term 1 θ∣Wnon}2 — }a}2) Wnon and 4 (}a}2 — }Wnon}2) a are
is the gradient induced by the regularization ɪ (∣Wnon}2 - }a}2) , which is used to balance the
magnitude between Wnon and a and make the algorithm more stable.
With some initialization Wpn0oqn and ap0q, we use the following iterative updates inspired by isotonic
regression (Goel et al., 2018), for t “ 0, . . . , T1 — 1
WntonIqDWnttqn—ηιg (Wnton, aptq, χptq,yptq) ` 小&二,	⑷
a(t'1q Daptq — ηιh ´Wntqn, aptq, χ(tq,y(tq) ` ηιξatq	(5)
where ηι > 0 is the step size parameter, ξWLn and 出t are uniformly sampled a unit sphere and
at iteration we use a fresh sample (xptq,yptq). Here we add isotropic noise gWLn and ξt because
the objective function for learning a linear CNN is non-convex and there may exist saddle points.
Adding noise can help escape from these saddle points. We refer readers to Ge et al. (2015) for more
technical details regarding this. As will be apparent in Section 4, after sufficient iterations, we obtain
a pair WpT1q, apT1q such that either it is close to the truth (Wn*on, a*) or close to the negative of
the truth (—Wn*on, —a*).
Remark 3.1 (Non-overlapping Patches). If there is no overlap between patches, we can skip Stage
2 because after Stage 1 we have already learned a and Wnon “ W.
Stage 2: Convotron with fixed Linear Layer In Stage 1 we have learned a good approximation to
the second layer (either apT1 q or —apT1q). Therefore, the problem reduces to learning a convolutional
filter. We run Convotron (Algorithm 3) proposed in Goel et al. (2018) using apT1q and —apT1q to
obtain corresponding weight vectors w(`)and w(T. We show that the Convotron analysis can be
extended to handle approximately known outer layer weight vectors.
Stage 3: Validation In stage 2 we have obtained two possible solutions (wp'q,apTq) and
(w(-q, —a(Tq). We know at least one of them is close to the ground truth. Closeness in ground
truth implies small squared loss (c.f. Lemma A.1). In the last stage we use a validation set to choose
4
Under review as a conference paper at ICLR 2019
Algorithm 2 Double Convotron
Initialize WnOqn P R2s—r and ap0q P Rk randomly
for t “ 1 to T do
Draw pxptq, yptqq „ Z
Compute g Wnon , a, xptq, yptq and h Wnon , a, xptq, yptq according to equation 2 and equa-
tion 3.
Set WntOnD= Wnton ´ ηιg (Wnton, aptq, xptq, yptq) ' ηιξWLn
Set apt+1) = apt)— ηιh ´wnon, aptq, xptq,yptq) ' ”总)
Return apT +1)
Algorithm 3 Convotron (Goel et al., 2018)
Initialize w1 := 0 P Rr.
for t = 1 to T do
Draw pxptq, yptqq „ Z
Let gpt) = Py㈤-f(w(t), a, xptq)) (∑k=1 aiPixptq)
Set wpt+1q = Wpt ` ηg(tq
Return WT `1
the right one. To do this, We simply use T3 = Poly (k, B, ɪ) fresh samples and output the solution
which gives lower squared error.
(w, a)=	argmin	〒幺Iypi) — f w, a,
(w,a)PtpWp'q ,apT )),(wp´) ,´a(T q)}	3 i“1
(6)
Since We draW many samples, the empirical estimates Will be close to the true loss using standard
concentration bounds and choosing the minimum Will give us the correct solution.
3.3	Main Theorem
The folloWing theorem shoWs that Algorithm 1 is guaranteed to learn the target convolutinoal neural
netWork in polynomial time. To our knoWledge, this is the first polynomial time proper learning
algorithm for convolutional neural netWork With overlapping patches.
Theorem 3.1 (Theorem 1.1 (Formal)). Under Assumptions 3.1-3.3, if we set T1, T2, T3 =
Ω (Poly (k, B, ɪ)a and η1, η2 = O (poly (ɪ, B, E)) then with high probability, Algorithm 1 re-
turns a pair (w, a) which satisfies
Eχ~z [(f (w, a, x) — f (w*, a*, x))[ ≤ e.
4	Proofs and Technical Insights
In this section We list our key ideas used for designing the Algorithm 1 and proving its correctness.
We discuss the analysis stage-Wise for ease of understnading.
4.1	Analysis of Stage 1
Learning a non-overlapping CNN with linear activation. We first consider the problem of learn-
ing a convolutional neural netWork With linear activation function and non-overlapping patches. For
this setting, We can Write the prediction function in a compact form:
f (w, a, x) = wJP (x) a = xP (x) , waJy.
The label also admits this form y = xP (x) , w* (a*)Jy. A natural Way to learn w* and a* is to
consider solving a square loss minimization problem:
min ' (w, a, x) = (〈P (x), WaJy ´ XP (x), w* (a*)J〉).
5
Under review as a conference paper at ICLR 2019
Now, taking expectation with respect to x, we have
L (w, a) “ >>waj — w* (a*)J>>	(7)
where the last step we used our assumptions that patches are non-overlapping and the covariance of
x is the identity. From equation 7, it is now apparent that the population L2 loss is just the standard
loss for rank-1 matrix factorization problem.
Recent advances in non-convex optimization shows the following regularized loss function
Lreg pw, aq = 2 ]waJ ´ w*(a*)J>>F + 8 (}W}2 ´ }a}2).	⑻
satisfies all local minima are global and all saddles points and local maxima has a negative cur-
vature Ge et al. (2017a) and thus allows simple local search algorithm to find a global minimum.
Though the objective function in equation 8 is a population risk, we can obtain its stochastic gradient
by our samples if we use fresh sample at each iteration. We define
gpwq	= ´f (wptq, aptq, xptq)	—	y(tq)	P	(Xt) aptq + 1 ^∣wptq∣2 ´ >>at>>2) Wpt	(9)
gf)	= ´f (wptq, aptq, xptq)	—	y(tt)	P	(xt)J wptq + 1 ^∣at∣2 TWptq∣j) aptq	(⑼
where xptq , yptq is the sample we use in the t-th iteration. In expectation this is the standard
gradient descent algorithm for solving equation 8:
Ex [gwwql = BLregR 叫,Ex [gatql = BLreg Bw√a(tt).
With this stochastic gradient oracle at hand, we can implement the noise-injected stochastic gradient
descent proposed in Ge et al. (2015).
wpt'1q = wptq — ηgpWq + ηξpWq,	apt'1q = wptq — ηgapq + 祗t
where ξwptq and ξaptq are sampled from a unit sphere. Theorem 6 in Ge et al. (2015) implies after
polynomial iterations, this iterative procedure returns an -optimal solution of the objective func-
tion equation 8 with high probability.
Learning non-overlapping part of a CNN with piece-wise linear activation function Now we
consider piece-wise linear activation function. Our main observation is that we can still obtain a
stochastic gradient oracle for the linear convolutional neural network using equation 2 and equa-
tion 3. Formally, we have the following theorem.
Lemma 4.1 (Properties of Stochastic Gradient for Linear CNN). Define
Lreg (Wnon, a) = g IWnonaJ — Wnon 3*)[1 + 8 (}wnon}2 — }a}2)
Under Assumption 3.1, we have
Ex rg (wnon , a, x, y)s =
BLreg (wnon, a)
Bwnon
,Ex rh(wnon , a, x, y)s =
where g (wnon, a, x, y) andh (wnon, a, x, y) are defined in equation 2 and equation 3, respectively.
Further, if }wnon}2 = O(poly (σ1)), }a}2 = O(poly (σ1)), then the differences are also bounded
BLreg (wnon, a) ∣	∣	BLreg (wnon, a) ∣
g (Wnon, a, x,y)------W---------- ≤ D, h (Wnon, a, x, y)-------F----------	≤ D
Bwnon	∣	∣	Ba	∣2
for some D = O (poly (B, k, σ1)).
Here the expectation of g and h are equal to the gradient of the objective function for linear CNN
because we assume the input distribution is symmetric and the activation function is piece-wise
linear. This observation has been stated in Goel et al. (2018) and based on this property, Goel et al.
6
Under review as a conference paper at ICLR 2019
(2018) proposed Convotron algorithm (Algorithm 3), which we use in our stage 2. Lemma 4.1 is a
natural extension of Lemma 2 of Goel et al. (2018) that we show even for one-hidden-layer CNN,
we can still obtain an unbiased estimate of the gradient descent for linear CNN.
Now with Lemma 4.1 at hand, we can use the theory from non-convex matrix factorization. Ge
et al. (2017a) has shown if ηι “ O (poly (ɪ, -B, E)) then for all iterates, with high probability,
›wpntoqn› “ Oppoly pσ1qq, }at}2 “ Oppoly pσ1qq. Therefore, we can apply the algorithmic result in
Ge et al. (2015) and obtain the following convergence theorem.
Theorem 4.1 (Convergence of Stage 1). If wp0q “ O '?^T), ap0q “ O (?^), and m “
O (poly (1, -B, e)) then after Ti “ O (poly (r, k, B, ɪ)) we have
apT1q	a*
ft¾ ´m;
apT1q	a*
Ft1< 'w
≤ e.
2
4.2	Analysis of Stage 2
After Stage 1, we have approximately recovered the outer layer weights. We use these as fixed
weights and run Convotron to obtain the filter weights. The analysis of Convotron inherently handles
average pooling as the outer layer. Here we extend the analysis of Convotron to handle any fixed
outer layer weights and also handle noise in these outer layer weights. Formally, we obtain the
following theorem:
Theorem 4.2. (Learning the Convolutional Filter) Suppose ∣∣a 一 a*}2 ≤ E Jbr E ≤ k33*} and
without loss of generality1 let ∣∣a∣2 “ }a*∣2 “ L For suitably chosen η “ O (poly (1, -1)),
Convotron (modified) returns w such that with a constant probability, ∣w ´ w*|2 ≤ OPk3 |w* ∣ Eq
in polypk, ∣w* ∣ , B, logp1{Eqq iterations.
Note that we present the theorem and proof for covariance being identity and no noise in the label
but it can be easily extended to handle non-identity convariance with good condition number and
bounded (in expectation) probabilistic concept noise.
Our analysis closely follows that from Goel et al. (2018). However, in contrast to the known second
layer setting considered in Goel et al. (2018), we only know an approximation to the second layer
and a robust analysis is needed. Another difficulty arises from the fact that the convergence rate
depends on the least eigenvalue of Pa :“ Xy … a%ajPiPj. By simple algebra, we can show
that the matrix has the following form:
’1	if i “ j
pa(i, j)= V Xk´1 aiai`i if |i ´ j|“ S
’%0	otherwise.
Using property of Toeplitz matrices, we show the least eigenvalue of Pa is lower bounded by 1 ´
(仔1) (c.f. Theorem A.2) for all a with norm 1.
cos
4.3	Analysis of Stage 3
Here we show how we can pick the correct hypothesis from the two possible hypothesis. Under our
assumptions, the individual loss (ypiq ´ f(w, a, xpiqqq2 is bounded. sThus, a direct application of
Hoeffding inequality gives the following guarantee.
Theorem 4.3. Suppose T3 “ Ω (poly (r, k,B, ɪ)) and let (w, a). If either (wp'q, aT1) or
(wp^q, —aT1) has population risk smaller than 2e, then let (w, a) be the output according to equa-
tion 6, then with high probability
Eχ~z “f (w, a, X ´ f (w*, a*, x))2‰ ≤ e.
1 Note that we can assume that the outer layers have norm 1 by using the normalized weight vectors since
the activations are scale invariant.
7
Under review as a conference paper at ICLR 2019
—
*a dna a neewteb elgnA
.5 1 .5
*a dna a neewteb elgnA
(a) G: Stage 1	(b) G: Stage 2	(c) U: Stage 1	(d) U: Stage 2
Figure 1: Evaluation of Algorithm 1. Gaussian input (G): 1a and 1b. Uniform input (U): 1c and 1d.
4.4	Putting Things Together: Proof of Theorem 3.1
Now we put our analyses for Stage 1-3 together and prove Theorem 3.1. By Theorem 4.1, we know
We have apT1q such that >>aτ1 - a* >> ≤ O ^2j2^σ^~) (without loss of generality, We assume a and
a* are normalized) with ηι “ O (Poly(1, B,高,e)) and ∣Sι∣ “ Poly 'r, k,B,σι, ɪ).
Now with Theorem 4.2, we know with η “ O (poly (1,右))and ∣S21 “ O (poly (k, σι, log ɪ))
we have either >>wp'q 一 w*] ≤ …合注 or >∣wpT 一 w*[ ≤ …合注.Lastly, the following
lemma bounds the loss of each instance in terms of the closeness of parameters.
Lemma 4.2. For any a and w, we have
´f pw*,a*,Xq —f (wptq,a, χ)) ≤ 2k (}a}2 }w ´ w*}2 + }a ´ a*}2 }w*}2) }χ}2.
Therefore, we know either (wp'q, apτ1q) or (wp´), —apτ1q) achieves e prediction error. Now com-
bining Theorem 4.3 and Lemma A.1 we obtain the desired result.
5	Experiments
In this section we use simulations to verify the effectiveness of our proposed method. We fix input
dimension d “ 160 and filter size r “ 16 for all experiments and vary the stride size s “ 9, 12, 16.
For all experiments, we generate w* and a* from a standard Gaussian distribution and use 10, 000
samples to calculate the test error. Note in Stage 2 of Algorithm we need to test a “ apT1q and
—apT1 q . Here we only report the one with better performance in the Stage 2 because in Stage 3 we
can decide which one is better. To measure the performance of Stage 1, we use the angle between
at an a* (in radians). We first test Gaussian input distribution χ „ Np0, Iq. Figure 1a shows the
convergence in Stage 1 of Algorithm 1 with T1 “ 10000 and η1 “ 0.0001. Figure 1b shows the
convergence in Stage 2 of Algorithm 1 with T2 “ 10000 and η “ 0.0001. We then test uniform
input distribution X 〜Unif[—√3, √3Sd (this distribution has identity covariance). Figure 1c shows
the convergence in Stage 1 of Algorithm 1 with T1 “ 40000 and η1 “ 0.0001. Figure 1d shows the
convergence in Stage 2 of Algorithm 1 with T2 “ 100000 and η2 “ 0.00001. Note for both input
distributions and all choices of stride size, our algorithm achieves low test error..
6	Conclusion and Future Work
In this paper, we propose the first efficient algorithm for learning a one-hidden-layer convolutional
neural network with possibly overlapping patches. Our algorithm draws ideas from isotonic regres-
sion, landscape analysis of non-convex problem and spectral analysis of Toeplitz matrices. These
findings can inspire further development in this field.
Our next step is extend our ideas to design provable algorithms that can learn complicated models
consisting of multiple filters. To solve this problem, we believe the recent progress on landscape
design (Ge et al., 2017b) may be useful.
8
Under review as a conference paper at ICLR 2019
References
Eric B Baum. A polynomial time algorithm that learns two hidden unit nets. Neural Computation,
2(4):510-522,1990.
Albrecht Boeottcher and Sergei M Grudsky. Spectral properties of banded ToePlitz matrices, Vol-
ume 96. Siam, 2005.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian
inputs. arXiv preprint arXiv:1702.07966, 2017.
Anna Choromanska, Mikael Henaff, Michael Mathieu, Gerard Ben Arous, and Yann LeCun. The
loss surfaces of multilayer networks. In Artificial Intelligence and Statistics, pp. 192-204, 2015.
Simon S Du and Jason D Lee. On the power of over-parametrization in neural networks with
quadratic activation. arXiv preprint arXiv:1803.01206, 2018.
Simon S Du, Jason D Lee, Yuandong Tian, Barnabas Poczos, and Aarti Singh. Gradient de-
scent learns one-hidden-layer cnn: Don’t be afraid of spurious local minima. arXiv preprint
arXiv:1712.00779, 2017.
C Daniel Freeman and Joan Bruna. Topology and geometry of half-rectified network optimization.
arXiv preprint arXiv:1611.01540, 2016.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points ´ online stochastic
gradient for tensor decomposition. In Proceedings of The 28th Conference on Learning Theory,
pp. 797-842, 2015.
Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems:
A unified geometric analysis. In Proceedings of the 34th International Conference on Machine
Learning, pp. 1233-1242, 2017a.
Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape
design. arXiv preprint arXiv:1711.00501, 2017b.
Surbhi Goel and Adam Klivans. Eigenvalue decay implies polynomial-time learnability for neural
networks. arXiv preprint arXiv:1708.03708, 2017a.
Surbhi Goel and Adam Klivans. Learning depth-three neural networks in polynomial time. arXiv
preprint arXiv:1709.06010, 2017b.
Surbhi Goel, Varun Kanade, Adam Klivans, and Justin Thaler. Reliably learning the ReLU in
polynomial time. arXiv preprint arXiv:1611.10258, 2016.
Surbhi Goel, Adam Klivans, and Raghu Meka. Learning one convolutional layer with overlapping
patches. arXiv preprint arXiv:1802.02547, 2018.
Benjamin D Haeffele and Rene Vidal. Global optimality in tensor factorization, deep learning, and
beyond. arXiv preprint arXiv:1506.07540, 2015.
Moritz Hardt and Tengyu Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231,
2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of non-convexity: Guar-
anteed training of neural networks using tensor methods. arXiv preprint arXiv:1506.08473, 2015.
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M. Kakade, and Michael I. Jordan. How to escape sad-
dle points efficiently. In Proceedings of the 34th International Conference on Machine Learning,
pp. 1724-1732, 2017.
Kenji Kawaguchi. Deep learning without poor local minima. In Advances In Neural Information
Processing Systems, pp. 586-594, 2016.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with ReLU activa-
tion. arXiv preprint arXiv:1705.09886, 2017.
Song Mei, Yu Bai, and Andrea Montanari. The landscape of empirical risk for non-convex losses.
arXiv preprint arXiv:1607.06534, 2016.
Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. arXiv
preprint arXiv:1704.08045, 2017a.
Quynh Nguyen and Matthias Hein. The loss surface and expressivity of deep convolutional neural
networks. arXiv preprint arXiv:1710.10928, 2017b.
Itay Safran and Ohad Shamir. On the quality of the initial basin in overspecified neural networks.
In International Conference on Machine Learning, pp. 774-782, 2016.
9
Under review as a conference paper at ICLR 2019
Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks.
arXiv preprint arXiv:1712.08968, 2017.
Hanie Sedghi and Anima Anandkumar. Provable methods for training neural networks with sparse
connectivity. arXiv preprint arXiv:1412.2693, 2014.
Mahdi Soltanolkotabi. Learning ReLUs via gradient descent. arXiv preprint arXiv:1705.04591,
2017.
Yuandong Tian. An analytical formula of population gradient for two-layered ReLU network and its
applications in convergence and critical point analysis. arXiv preprint arXiv:1703.00560, 2017.
Eric W Weisstein. Gershgorin circle theorem. 2003.
Yuchen Zhang, Jason D Lee, Martin J Wainwright, and Michael I Jordan. Learning halfspaces and
neural networks with random initialization. arXiv preprint arXiv:1511.07948, 2015.
Kai Zhong, Zhao Song, and Inderjit S Dhillon. Learning non-overlapping convolutional neural
networks with multiple kernels. arXiv preprint arXiv:1711.03440, 2017a.
Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees
for one-hidden-layer neural networks. arXiv preprint arXiv:1706.03175, 2017b.
Pan Zhou and Jiashi Feng. The landscape of deep learning algorithms. arXiv preprint
arXiv:1705.07038, 2017.
10
Under review as a conference paper at ICLR 2019
A Useful Lemmas/Theorems
In this section we present a few lemmas/theorems that are useful for our analysis.
Proof of Lemma 4.2. Observe that,
´f (w*, a*, x) — f (wptq, a, x))2
≤ 2 ´(f (w*, a*, x) ´ f (w*, a, x))2 + (f (w*, a, x) ´ f (w, a, x))2)
since (a + b)2 ≤ 2 'a2 + b2) for all a, b P R.
The first term can be bounded as follows,
(f(w*,a*,x) ´ f(w*,a,x))2
≤
≤
(E (a* ´ ai)σ((w*)T PiX))
住尾 ´ ai∣∣(w*)T Pix|)
住 |a* ´ ai∣}w*}2 }x}2)
≤ k}a* ´ a}22 }w*}22 }x}22 .
Here the first inequality follows from observing that σ (a) ≤ |a| and the last follows from }v}1 ≤
?k ∣∣v}2 for all V P Rk.
Similarly, the other term can be bounded as follows,
(f (w*, a, x) ´ f (w, a, x))2 “
≤
≤
(g ai (σ ´(w*)T Pix) ´ σ (WTPiX)))
(X |ai||(w* ´ W)T PiXI)
k2
X |ai | ∣W* ´ W∣2 ∣x∣2
≤ k ∣a∣22 ∣W* ´ W∣22 ∣x∣22 .
Here we use the Lipschitz property of σ to get the first inequality. The lemma follows from combin-
ing the above two.	口
The following lemma extends this to the overall loss.
Lemma A.1. For any a and W,
E[(f(w*, a*, x)— f (wpt), a, x))2] ≤ 2kB (}a}2 }w ´ w*}2 + }a ´ a*}2 }w*}2).
The following lemma from Goel et al. (2018) is key to our analysis.
Lemma A.2 (Lemma 1 of Goel et al. (2018)). For all a, b P Rn, if Z is symmetric then,
Ex„z[σ (aTx)(bTX)S= 1 '。Eχ~z[(aTx) (bTx)].
The following well-known theorem is useful for bounding eigenvalues of matrices.
Theorem A.1 (Gershgorin Circle Theorem Weisstein (2003)). For a n X n matrix A, define Ri :=
Xn=I j‰i ∣Ai,j I∙ Each eigenvalue of A must lie in at least one ofthe disks {z : ∣Z — Ai,i∣ ≤ Ri}.
11
Under review as a conference paper at ICLR 2019
The following lemma bounds the eigenvalue of the weighted patch matrices.
Theorem A.2. For all a P SkT,
λmin (Pa)21 — CoS
(k⅛)
and
λmax (Pa) ≤ 1 + COS
(U).
Proof. Since S》[2U + 1, only adjacent patches overlap, and it is easy to verify that the matrix Pa
has the following structure:
$’1	ifi “ j
Pa (i,jq= V ∑31 aiai`i if |i 一j| “ S
’%0	otherwise.
Using the Gershgorin Circle Theorem, stated below, we can bound the eigenvalues, λmin (Pa) >
1 -1∑31 aiai`1 I and λmax (Pa) ≤ 1 + ∑k=i1 aiai'11.
To bound the eigenvalues, we will bound ∣∑k] aiai'i∣ by maximizing it over all a such that }a}2 “
1. We have max {∣∑k]
aiai`1 ∣∣	“
max {∑31
aiai`1
since the maximum can be achieved by
setting all ai to be non-negative. This can alternatively be viewed as max}a} “1 aTMa “ λmax (M)
where M is a tridiagonal symmetric Toeplitz matrix as follows:
M(i,j) “ 10{2
if|i—j| “ 1
otherwise.
It is well known that the eigenvalues of this matrix are of the form cos
忌）
Boeottcher & Grudsky (2005)). The maximum eigenvalue is thus cos
result.
for i “ 1, . . . , k (c.f.
(帚)
This gives us the
□
B Omitted Proofs
B.1 Proof of Lemma 4.1
First by definition, we have
2	k1
Ex rg (wnon , a, x, yqs “Ex 1+- f (wnon, a, xq ´ y) X ajPj°nx + 4 (}wnon}2 ´ }a}2)Wnon
1+γ	j 1	4
Because the input distribution is symmetric and the covariance is identity, by Lemma A.2, we have
k	kk
Ex 1- f(Wnon, a, x) X ajPnonx =Ex 1— X ajσ (WJonPnonx)∑ ajPnonx
1 + γ	j “1	1 + γ j “1	j“1
kk
“Ex X ajwnJonPjnonx X ajPjnonx
j“1	j“1
“ }a}2 wnon .
Similarly, we have
k
Ex y ∑ ajPnonx = aja*w*.
j“1
Also recall
BLreg (W, a)
Bw
=}a}2 Wnon — aJa*W* + 41 (}Wnon }2 — }a}2) Wnon.
12
Under review as a conference paper at ICLR 2019
Thus
Ex rg pwnon , a, x, yqs “
BLreg Pw, a)
Bw
The proof for h pwnon , a, x, yq is similar.
To obtain a bound of the gradient, note that
kk
∑ ajσ (WJonPnonx)g ajPnonx
j“1	j“1
kk
≤ ∑ IajiwJonPnonχ∣ ∑ ajpnonχ
j“1	›j“1	›
k
≤ max Iaj i Σ }wnon}2 >PnonX>2 ∙ }a}2 }χ}2
j	j“1
Wk }a}2 }x}2 }w}2
“poly Pk, σ1, B) .
Similar argument applies to y X：=i ajPnonx.
B.2 Proof of Theorem 4.2
We follow the Convotron analysis and include the changes. Define St “ t(X1, y1) , . . . , (Xt, yt)u.
The modified gradient update is as follows,
gpt) “ (yt ´ f (wpt), a, Xt))	∑ aiPiXt
The dynamics of Convotron can then be expressed as follows:
Eχt,yt r>wptq ´ w*>2Twpt+1)- w*>2|StTs = 2ηEχt,yj(w* ´ wptq)Tgptq⑸—/—力旧〜小[∣∣gptq∣∣2|St—J
We have,
´ Wpt))Tgptq∣Stτ
w* ´ wptq)	(yt ´ f (Wpt), a, Xt)) (∑ aiPixt)卜_ J
5 ´ wptq)	(f (w*, a*, Xt) ´ f (wptq, a, Xt)) (∑ aiPiXt)卜_1
1≤i,j≤k
1 ' a
2 T
Extr
^σ ((w*)T PiXt) — aiσ ((Wptq) PiXt)) (aj (w*)
Σ Ext 1((
:ij<k L∖ ∖
1≤i,j≤k
ai* (w*)T ´ ai (wpt))	PiXt	aj (w*)T ´ aj
aj (wptq) )P∙Xt∣Stτ]
(wptq) ) PjXt|St—1
(11)
T
ai* (w*)T ´ ai (wpt))	PiExtrXtXtTsPjT (aj w* ´ aj wpt))
1	'	α	G	(
2	Z	V
1≤i,j≤k '
1	'	α	G	(
2	Z	∖
1≤i,j≤k、
1 ' a
2
(12)
ai* (w*)T	´	ai (w*)T	`	ai	(w*)T ´	ai	(wpt))	Pi PjT	(aj w* ´	aj wpt) )
´ (w*)T)	Pa	(w* ´	w*)'	∑	(a*	´	ai) aj	(w*)T PiPT	(w* ´	Wpt)))
1≤i,j≤k	)
(13)
13
Under review as a conference paper at ICLR 2019
> 1'2α ʌmin	PPaq	>wptq ´ W*>jTw*}2	£	同 ´ * Pi || || £ 。，3 Pj
∖	IWiWk	>2 l∣1≤j≤k
Wptq ´ W* ››
(14)
1 ' ɑ
2
2
>
´ w*>2 ´ k }w*}}a* ´ a}2 1闾2 >wptq ´ w*>J
T (λmin PPaq >wptq ´ W*>2 ´ k }W*}2
Wpt)—w*>2)
(11) follows from using Lemma A.2, (13) follows from defining Pa :“ 2ι≤i jwk a%ajPiPj, (12)
follows from setting the covariance matrix to be identity and (14) follows from observing that Pa is
symmetric, thus @x, xTPax 2 λmin PPaq }x}22 as well as lower bounding the second term in terms
of the norms of the corresponding parts.
Now we bound the variance of gptq .
Eχt,ytrι∣gptqιι2∣StτS
´f
, a,
£ aiPixJ 卜´i
2
≤ λmax PPaq Exj´f (w*, a*, Xt) — f (wptq, a, Xt)) ∣∣Xt∣∣2 St—1
≤ 2kλmax (Pa)
a}2 >wptq ´ w*>2 + }a ´ a*}2 }w*}2)Ext ”网4]
≤ 2kBλmax (Pa) (Wptq ´ w*>2 + e2 }w*}2)
(15)
(16)
(17)
(15) follows from observing that ∣ IXk=I aiPiX∣ ∣ ≤ λmax (Pa) }x}2 for all X and (16) follows from
Lemma 4.2.
Combining the above equations and taking expectation over St´i,we get
Est r>wpt'1q — w*>JW '1 ´ 2ηβ + η2γ) ES-口亚⑴—w*>J + 2ηɑ^st^ r>wptq — w*>J + η2χe2
W '1 — 2ηβ + η2γ) ES一 [>wptq — w*> J + 2ηδe Jes—[>wptq — w*>2] + η2χe2
for β “ 12αλmin (Pa), Y = 2λmax (Pa) kB, δ “ 12ak }w*b andX = 2λmax (Pa) kB }w*}2.
From Theorem A.2, We have that λmin (Pa) = 1 ´ cos (k'ɪ) “ Ω (1∕k2) (by Taylor expansion)
implying β = ω '1∕k2) and Y = O (kB), X = O ´kB }w*}2).
We set η = βmin ^ɪ, χ0. First We show that ES— r>wptq — w*>j] W 1 for all iterations t. We
prove this inductively. For t = 1, since w1 = 0, this is satisfied. Let us assume it holds for iteration
t, then we have that,
EStrIWpt" w*>2s
≤ '1 ´ 2ηβ + η2γ) Es-1 r>wptq -
≤ 1 ´ 2ηβ + η2γ + 2ηδe + η2χe
≤ 1 一 2ηβ + ηβ + 2ηδe + ηβe
W 1 ´ η (β ´(δ + β) E)W 1
´ w*1S + 2ηδeʌ/ESj1[>wpt) ´ w*>2] + η2χe2
2
The last inequality follows from E W k33*} W δ'β. ThUS We have that for each iteration,
Es- r>wptq — w*>2s W 1. Substituting this in the recurrence and solving the recurrence gives
14
Under review as a conference paper at ICLR 2019
us,
Est r>wpt'1q ´ w*∣1sw '1 ´ 2ηβ ' η2γ) EStτ[>wptq ´ w*>J ' 2ηδe ' η2χe2
≤	pi 一 ηβ)Esjιr>wptq ´ w*；S ' 2ηδe ' ηβe2
t´1
≤	pi ´ ηβ)t }wι ´ w* }2 + '2ηδe ' ηβe2) £ (1 ´ ηβ)i
i“0
≤ (1 ´ ηβqt + 2δττ + e2
β
Thus for T “ O (nβ log 'ɪ)), We have,
Estr>wpt'1q ´ w*>2s≤ O ^δ) “ O 'k3 }w*}2 e) .
NoW using Markov’s inequality, We knoW that the above holds for some constant probability.
B.3 Proof of Theorem 4.3
For i “ T2 + 1,..., T3, define zpiq “ 'ypiq — f 'w, a, xpiq))2. Using our assumptions, We know
zpiq ≤ O (poly (r, k, B)) almost surely. Now applying Hoeffding inequality we obtain our desired
result.
15