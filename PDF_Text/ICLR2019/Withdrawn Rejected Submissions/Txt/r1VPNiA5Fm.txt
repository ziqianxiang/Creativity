Under review as a conference paper at ICLR 2019
The Universal Approximation Power of Finite-
Width Deep ReLU Networks
Anonymous authors
Paper under double-blind review
Ab stract
We show that finite-width deep ReLU neural networks yield rate-distortion op-
timal approximation (BoIcskei et al., 2018) of a wide class of functions, includ-
ing polynomials, windowed sinusoidal functions, one-dimensional oscillatory tex-
tures, and the Weierstrass function, a fractal function which is continuous but
nowhere differentiable. Together with the recently established universal approxi-
mation result for affine function systems (Bolcskei et al., 2018), this demonstrates
that deep neural networks approximate vastly different signal structures generated
by the affine group, the Weyl-Heisenberg group, or through warping, and even cer-
tain fractals, all with approximation error decaying exponentially in the number of
neurons. We also prove that in the approximation of sufficiently smooth functions
finite-width deep networks require strictly fewer neurons than finite-depth wide
networks.
1	Introduction
A theory establishing a link between the complexity of a neural network and the complexity of the
function class to be approximated by the network was recently developed in Bolcskei et al. (2018).
Based on this framework, it was shown (Bolcskei et al., 2018) that all affine function classes are opti-
mally representable by neural networks in the sense of Kolmogorov rate-distortion theory (Donoho,
1993; Grohs, 2015). Equivalently, this means that the approximation error decays exponentially in
the number of neurons employed in the approximation.
The present paper explores the question of whether the universality result established in Bolcskei
et al. (2018) extends beyond affine function classes and answers it in the affirmative. Specifically, we
consider the approximation of polynomials, windowed sinusoidal functions (GrOChenig & Samarah,
2000; Grochenig, 2001), one-dimensional oscillatory textures according to Demanet & Ying (2007),
and the Weierstrass function, a fractal function which is continuous everywhere and differentiable
nowhere. The central conclusion of this paper is that finite-width ReLU networks of depth scaling
poly-logarithmically in the inverse of the approximation error lead to exponentially decaying ap-
proximation error for all these different signal structures. This result is established by building on
a recent breakthrough in Yarotsky (2016) and recognizing that the width of networks approximat-
ing polynomials need not scale linearly in the degree of the polynomial, but can actually be finite
independently of the degree. This insight will also allow a sharp statement on the benefit of depth;
specifically, we prove that in the approximation of sufficiently smooth functions finite-width deep
networks require strictly fewer neurons than finite-depth wide networks.
Notation. For the function f(χ): Rd → R, we define IIfkL∞(ω) ：= inf{C ≥ 0 : |f(x)| ≤
C, for all X ∈ Ω}. For a vector b ∈ Rd, we let ∣∣b∣∣∞ := maxi=ι,…,d |bi|, similarly we write
kAk∞ := maxi,j |Ai,j | for the matrix A ∈ Rm×n. We denote the identity matrix of size n × n by
In. Throughout, log stands for the logarithm to base 2.
2	Setup and basic ReLU calculus
We start by defining ReLU neural networks.
1
Under review as a conference paper at ICLR 2019
Definition 2.1. Let L, N0, N1, . . . , NL ∈ N. A map Φ : RN0 → RNL given by
Φ(x) =	W2(ρ(W1(x))),
Φ(x) =	WL(ρ(WL-1(ρ(...ρ(W1(x)))))),
L=2
L≥3
(1)
with affine linear maps w` : RN'-1 → RN', ' ∈ {1, 2,..., L}, and the ReLU activation function
ρ(x) = max(x, 0), x ∈ R, acting component-wise (i.e., ρ(x1, . . . , xN) := (ρ(x1), . . . , ρ(xN))) is
called a ReLU neural network. The map w` corresponding to layer ' is given by W'(x) = A'x + b`,
with a` ∈ RN'×N'-1 and b` ∈ RN'. We define the network connectivity as the total number ofnon-
Zero entries in the matrices a`, ' ∈ {1,2,..., L}, and the vectors b`, ' ∈ {1, 2,..., L}. The
depth of the network or, equivalently, the number of layers is L(Φ) := L and its width W(Φ) :=
max'=。,…,l n`. We further denote by B(Φ) := maxg=ι,…,l max{∣∣A'k∞,kb`∣∣∞} the maximum
absolute value of the weights in the network.
We designate the class of ReLU networks Φ : Rd → RNL with no more than L layers, width
no more than M, input dimension d, and output dimension NL by N N L,M,d,NL . Note that the
connectivity of Φ is upper-bounded by LM(M + 1).
For later use we record three technical results. We first record a technical lemma on the composition
of neural networks.1
Lemma 2.2. Let L1,L2,M1,M2,d1,d2,NL1,NL2 ∈ N, Φ1 ∈ NNL1,M1,d1,NL1,
and Φ2 ∈ NNL2 ,M2 ,d2 ,NL with NL1 = d2.	Then, there exists a network
Ψ ∈ NNL1+L2,max{2NL1,M1,M2},d1,NL2 with B(Ψ) = max{B(Φ1), B(Φ2)}, satisfying Ψ(x) =
Φ2(Φ1(x)),for all x ∈ Rd1.
Before we can formalize the concept of a linear combination of neural networks, we need a result
that shows how to augment network depth while retaining the networks input-output relation
Lemma 2.3. Let L, M, K, d ∈ N, Φ1 ∈ N N L,M,d,1, and K > L. Then, there exists a correspond-
ing network Φ2 ∈ N N K,max{2,M},d,1 such that Φ2(x) = Φ1(x) for all x ∈ Rd. Moreover, the
weights ofΦ2 consist of the weights ofΦ1 and ±1’s.
The next result formalizes the concept of a linear combination of neural networks.
Lemma 2.4. LetN,Li,Mi,di ∈ N, ai ∈ R, Φi ∈ NNLi,Mi,di,1, i = 1,2,...,N, d= PiN=1di.
Then, there exist networks Φ1 ∈ N N L,M,d,N and Φ2 ∈ N N L,M,d,1 with L = maxi Li, and
M ≤ PiN=1 max{2, Mi} satisfying
Φ1(x) = (Φ1(x1) Φ2(x2)	. . .	ΦN(xN))T and
N
Φ2(x) =	aiΦi(xi),
i=1
for all x = (x1T, x2T, . . . , xTN)T ∈ Rd with xi ∈ Rdi, i = 1, 2, . . . , N. Moreover, the weights ofΦ1
consist of the weights of the networks Φi, i = 1, 2, . . . , N, and ±1’s. The weights ofΦ2 consist of
the weights ofΦ1 and {a1, a2, ..., aN}.
Remark 2.5. Note that if the networks Φi in Lemma 2.4 have shared inputs, the resulting networks
Φ1 and Φ2 will have fewer than d = PiN=1 di inputs.
3	Approximation of polynomials
This section shows how the multiplication operation and polynomials can be approximated to within
error with ReLU networks of finite width and of depth poly-logarithmic in 1/. We also note that
the approximation results throughout the paper guarantee that the magnitude of the weights in the
network does not grow faster than polynomially in the size of the domain over which approximation
takes place. Although not shown here for space constraints, the combination of finite width, depth
1The proofs of the following two lemmata, along with all other proofs not provided in the paper, can be
found in the supplement.
2
Under review as a conference paper at ICLR 2019
scaling poly-logarithmically in 1/, and weights growing no faster than polynomially guarantees
rate-distortion optimality in the sense of BoIcskei et al. (2018) and hence exponential error decay.
Previous results on the approximation of the multiplication operation and of polynomials through
finite-width ReLU networks reported in Hanin & Sellke (2017); Yarotsky (2018) do not come with
bounds on the network weights, have depth scaling polynomially in 1/, and therefore do not allow
to conclude rate-distortion optimality.
The proof ideas for the results in this section are inspired by Yarotsky (2016) and by the “sawtooth”
construction of Telgarsky (2015). In contrast to Yarotsky (2016), we consider networks without
“skip connections” and of finite and explicitly specified width. Before starting with the approxima-
tion ofx2, we note that all our results apply to the multivariate case as well, but we restrict ourselves
to the univariate case for simplicity of exposition.
Proposition 3.1. There exists a constant C > 0 such that for all ∈ (0, 1/2) there is a network
Φ ∈ N N ∞,4,1,1 satisfying L(Φ) ≤ C log(-1), B(Φ) ≤ 4, Φ (0) = 0, and
kΦ(x) - x2 kL∞([0,1]) ≤ .	(2)
With Proposition 3.1 we are now ready to show how ReLU networks can approximate the multipli-
cation operation, which will then lead us to the approximation of arbitrary powers of x.
Proposition 3.2. There exists a constant C > 0 such that for all D ∈ R+ and ∈ (0, 1/2) there is a
network ΦD, ∈ N N ∞,12,2,1 satisfying L(ΦD,) ≤ C log(dDe2-1), B(ΦD,) ≤ max{4, 2dDe2},
kΦD,(x, y) - xykL∞([-D,D]2) ≤ ,	(3)
and ΦD, (0, x) = ΦD, (x, 0) = 0, for all x ∈ R.
The next result establishes that arbitrary polynomials can be approximated by ReLU networks of
finite and explicitly specified width and of depth growing logarithmically in the inverse of the ap-
proximation error. In particular, the width of the approximating network does not grow with the
degree of the polynomial as is the case in Yarotsky (2016), Ding et al. (2018), Liang & Srikant
(2017). This finite-width aspect is central to the approximation of sinusoidal functions by ReLU
networks as described in the next section.
Proposition 3.3. There exists a constant C > 0 such that for all m ∈ N, A ∈ R+, pm (x) =
im=0 aixi with maxi=0,...,m |ai| = A, D ∈ R+, and ∈ (0, 1/2), there is a network Φpm,D, ∈
N N ∞,16,1,1 satisfying L(Φpm,D,) ≤ Cm(log(dAe) + log(-1) + m log(dDe) + log(m)),
B(Φpm,D,) ≤ max{A, 8dDe2m-2}, and
kΦpm,D, - pmkL∞([-D,D]) ≤ .	(4)
Proof. We start by noting that for m = 1 the resulting affine function p1 (x) = a0 + a1x can be
realized exactly, i.e., with = 0, by a network of depth L = 2 with
W1(x) = -aa11x+ -aa00
and A2 = (1 -1), b2 = 0. The proof for m ≥ 2 will be effected by realizing the monomials xk, k ≥
2, through iterative composition of multiplication networks and combining this with a construction
which uses the network realizing xk not only as a building block in the network realizing xk+1 but
also to construct the network approximating the partial sum Pik=0 aixi in parallel.
We start by setting HDk ,η := dDek + η Psk=-02 dDes, k ∈ N, and let ΦHk ,η, D ∈ R+, k ∈ N,
η ∈ (0, 1/2), be multiplication networks according to Proposition 3.2. For D ∈ R+, k ∈ N,
η ∈ (0, 1/2), we then recursively define ΨkD,η according to Ψ0D,η(x) = 1, Ψ1D,η(x) = x, and
ΨkD,η(x) = ΦHk-1,η(x, ΨkD-,η1 (x)), k ≥ 2. Note that ΨkD,η(x) can be realized through a neural
network for all k ∈ N thanks to Lemma 2.2 and the fact that, as already noted above for the case
m = 1, any affine function can be realized through a neural network.
We first show by induction that
k-2
kΨkD,η(x) - xk kL∞([-D,D]) ≤ η XdDes,	(5)
s=0
3
Under review as a conference paper at ICLR 2019
for all η ∈ (0, 1/2), k ≥ 2. The base case k = 2 follows from
kΨ2D,η (x) - x2 kL∞ ([-D,D]) = kΦHD1 ,η,η(x, x) - x2kL∞([-D,D]) ≤ η.
We proceed to establishing the induction step (k - 1) → k. The induction assumption is
k-3
kΨkD-,η1(x) - xk-1kL∞([-D,D]) ≤ η XdDes.	(6)
s=0
Since kΨkD-,η1kL∞([-D,D]) ≤ kxk-1kL∞([-D,D]) + kΨkD-,η1(x) - xk-1kL∞([-D,D]) ≤ HDk-,η1, Propo-
sition 3.2 implies that
kΨkD,η(x) - xkkL∞([-D,D]) ≤ kΦHk-1,η (x, ΨkD-,η1(x)) - xΨkD-,η1(x)kL∞([-D,D])
D,η
+「max lxlkψD-;(X)-XkTkL∞([-D,D])
[-D,D]
k-3	k-2
≤η+dDeηXdDes=ηXdDes,
s=0	s=0
which completes the proof of the induction step.
We are now ready to proceed to the construction of the network Φpm,D, approximating the polyno-
mial pm(X) = Pim=0 aiXi. To this end, we first note that the identity mapping X 7→ X and the linear
combination X, y 7→ X + ai-1y are affine transformations and can thus be realized by a network of
depth L = 2. By Lemma 2.4 there hence exists a constant C2 such that for every m ≥ 2, pm(X) =
Pm=O a'χ', i ∈ {2, 3, ...,m}, η ∈ (0,1/2) there is a network 夕 pm,D,η ∈ NN∞ ,16,3,3 With
L3ipm,D,η ) ≤ C2 IOg(I"Hi-ne/T)，and BWpm,D,η) ≤ max{4, 2dHi-ηT, maxi∈{0,...,m} |ai|}
realizing the map
(X s y)> → X s + ai-1y ΦHi-1,η (X, y)	.
The statements in the following apply for all m ∈ N, A ∈ R+, pm (X) = Pim=0 aiXi with
maxi=0,...,m |ai| ≤ A, D ∈ R+, and ∈ (0, 1/2). The network Φpm,D, approximating the
polynomial pm (X) = Pim=0 aiXi is now constructed according to
with η := (dAem2dDem )-1. This yields
m
Φpm,D,(X) = X ai ΨiD,η (X), for all X ∈ R.
i=0
Hence equation 5 implies
m	m	i-2
Φpm,D,(X)-pm L∞([-D,D]) ≤XlailkψD,ne(X)-XikL∞([-D,D]) ≤X |ai| η XdDes
m
≤ η max	|ai| X(i - 1)dDei-2 ≤ Am2dDem-2η ≤ .
i∈{2,...,m}	i=2
Thanks to its compositional structure, the width of Φpm ,D, equals the maximum width of the indi-
vidual networks in the composition, i.e., W (Φpm,D,) ≤ 16. Since HDi-,η1 ≤ 2dDem-1, for i ≤ m,
we further have
mm
L(Φpm,D,e) ≤ X L(&m,D,n) ≤ X C lθg(dHDle2η-1)
i=2	i=2
≤ C2 m (lOg(dAe) + lOg(-1) + (3m - 2) lOg(dDe) + 2 lOg(m) + 2)
≤ 4C2m (lOg(dAe) + lOg(-1) + m lOg(dDe) + lOg(m)).
4
Under review as a conference paper at ICLR 2019
Finally, we note that
B(Φpm,D,) = max{1, |a0|, |am |
iuJmaX IBMm,D,nJ} ≤ max{A, 8dDe2m-2}.
i∈{2,3,...,m}
This finalizes the proof.
□
We conclude this section with a result on ReLU networks approximating smooth functions with
exponential accuracy. The proof of this statement, provided in the supplement, is based on the
theory developed above.
Definition 3.4. For D ∈ R+, let the set SD ⊆ C∞([-D, D], R) be given by
SD = nf ∈ C∞([-D, D], R) : kf(n)(x)kL∞([-D,D]) ≤n!, foralln∈N0o.	(7)
Lemma 3.5. There exist constants C > 0 and a polynomial π such that for all D ∈ R+, f ∈ SD ,
and	∈(0,	1/2), there is a network	Ψf,	∈	N N ∞,23,1,1 satisfying	L(Ψf,)	≤ CdDe(log(-1))2,
B(Ψf,) ≤ maX{1/D, dDe}π(-1), and
kΨf, - f kL∞([-D,D]) ≤ .	(8)
Note that for expositional simplicity Lemma 3.5 covers functions defined on symmetric intervals
[-D, D]. Close inspection of the proof reveals, however, that for arbitrary intervals [a, b] and func-
tions f ∈ C∞([a, b], R) with kf(n) kL∞([a,b]) ≤ n!, for all n ∈ N, the same statement (with D
replaced by b - a in the bounds on L(Ψf,), B(Ψf,)) holds.
4 Approximation of sinusoidal functions
We are now ready to proceed to the approximation of sinusoidal functions.
Theorem 4.1. There exists a constant C such that for every a, D ∈ R+, ∈(0, 1/2), there is a
network Ψa,D, ∈ N N ∞,16,1,1 satisfying L(Ψa,D,) ≤ C((log(1/))2 + log(daDe)), B(Ψa,D,) ≤
C, and
∣∣ψa,D,e - Cos(a ∙ )∣L∞([-D,D]) ≤ J
Proof. We start by approximating x 7→ cos(2πx) on [0, 1]. To this end note the MacLaurin series
representation
∞n
Cos(X) =	————x2n
( X 乙(2n)!
n=0
∀x ∈ R.
Thanks to the Taylor theorem with remainder in Lagrange form, we have, for all x ∈ [0, 1],
N ( 1)n
cos(2πx) - £ -x-nlτrx^n ≤
n=0 (2n)!
(2πx)2N +1
(2N +1)!
摆]"(2N+ι)(2πt)ι≤ ∣N++;.
(9)
Next observe that n! ≥ (ne)ne, for all n ∈ N, which implies,
(2π)4N+2 V (4π2)2N+1 V / 4π2e ∖2N +1
(2N +1)! ≤ (2N+1 )2N+ιe ≤(2N +J
for all N ∈ N.
(10)
With N := d2π2e log(2/J)e, we get, for all J∈ (0, 1/2),
(4π2e λ2Ne + 1 = (	4π2e	λ2d2π2e log(2∕e)e+1 ≤ 2-d2∏2e log(2∕e)]
12Νe +1√	- 12d2π2e log(2∕e)e +1√	一
≤ 2-log(2/e) = j
-	2.
(11)
Noting that Ci ：= ImaXn∈n0 (号2
< ∞ and N ≤ C2 log(J-1), for all J ∈ (0, 1/2), with
C2 := 4π2e + 1, application of Proposition 3.3 to
N ( 1)n
Pm(X)= PN (X) =ETy4~(2πx)2 ,
n=0 (2n)!
5
Under review as a conference paper at ICLR 2019
with D = 1, establishes the following: There is a constant C3 such that, for all ∈ (0, 1/2), there is
a network Φe∕2 satisfying
-pN4∞([-ι,i]) ≤ i,
(12)
with W(Φe∕2) ≤ 16, B(Φe∕2) ≤ C3, and
L(Φe∕2) ≤ C3Ne(log(C1) + log(2∕e) + Ne log⑴ + log(Ne)) ≤ C4(log(e-1))2,
where C4 := C2C3(1 + C1 + C2). Combining equation 9, equation 10, equation 11, and equa-
tion 12, it follows that the network Φe∕2 approximates the function x 7→ cos(2πx) on [0, 1] to within
accuracy , i.e., for all ∈ (0, 1∕2), we have
∣∣φe∕2 - cos(2π ∙ )kL∞([0,1]) ≤ C.	(13)
We next extend this result to the approximation ofx 7→ cos(ax) on the interval [-1, 1] for arbitrary
a ∈ R+. This will be accomplished by exploiting that x 7→ cos(2πx) is 1-periodic and even.
First recall the “sawtooth” functions gs : [0, 1] → [0, 1], s ∈ N, as defined in equation 24. It is
straightforward, albeit somewhat tedious, to see that, for all s ∈ N0, x ∈ [0, 1],
cos(2π2sx) = cos(2πgs(x)).
Similarly, it follows that cos(2π2sx) = cos(2πgs(∣x∣)), for all S ∈ N0, X ∈ [-1,1]. Next, note that
for every a ∈ R+, there exists a Ca ∈ (1∕2, 1] such that a∕(2π) = Ca2dlog(a)-log(2π)e ; we thus
have, for all a ∈ R+, x ∈ [-1, 1],
cos(ax) = Cos(2∏2dlθg(a)ig(2π)eCaX) = cos(2ngdiog(a)-iog(2n)e (Ca |x| )).
Since gdlog(a)-log(2π)e (Ca|x|) ∈ [0, 1], for all a ∈ R+, x ∈ [-1, 1], it follows from equation 13 that
llφe∕2 (gdlog(a)-log(2π)e (CaIxI)) - cos(2πgdlog(a)-log(2π)e (CaIxI))II
L∞([-1,1])	(14)
= llΦe∕2 gdlog(a)-log(2π)e (Ca IxI) - cos(ax)llL∞([-1,1]) ≤ C.
Now recall that x 7→ IxI = ρ(x) + ρ(-x) can be implemented by a 2-layer network and consider the
realization of x 7→ gdlog(a)-log(2π)e (Cax), a ∈ R+, as developed in the proof of Proposition 3.1.
Applying Lemma 2.2 twice, then establishes, thanks to (14), the existence ofa constant C5 such that
the network
Ψa,e := Φe∕2 (gdlog(a)-log(2π)e (Ca IxI))
approximates x 7→ cos(ax) on [-1, 1] with accuracy C, while satisfying L(Ψa,e)	≤
C5((log(1∕C))2 + log(dae)), W(Ψa,e) ≤ 16, and B(Ψa,e) ≤ C5.
Finally, we consider the approximation of x 7→ cos(ax) on intervals [-D, D], for arbitrary D ≥ 1.
To this end, we define, for all a ∈ R+, D ∈ [1, ∞), C ∈ (0, 1∕2), the network Ψa,D,e(x) :=
ΨaD,e( D) and observe that
sup	IΨa,D,e(x) - cos(ax)I = sup IΨa,D,e (Dy) - cos(aDy)I
x∈[-D,D]	y∈[-1,1]
= sup IΨaD,e (y) - cos(aDy)I ≤ C.
y∈[-1,1]
This concludes the proof.
□
An approximation result for sin(ax) can be obtained directly from Theorem 4.1 simply by noting
that sin(x) = cos(x - π∕2), which can be realized by the concatenation of a neural network that
performs an affine transformation and a network that approximates cos(x). The formal statement is
as follows.
Corollary 4.2. There exists a constant C > 0 such that for every a, D ∈ R+, b ∈ R, C ∈ (0, 1∕2)
there is a network Ψa,b,D,e ∈ NN ∞,16,1,1 satisfying
kψa,b,D,e - cos(a ∙ - b) k L∞([-D,D]) ≤ c,	(15)
with L(Ψa,b,D,e) ≤ C((log(C-1))2 + log(daD + IbIe)) and B(Ψa,b,D,e) ≤ C.
6
Under review as a conference paper at ICLR 2019
Figure 1: Left: A function in F1,100. Right: The function W我,?
5	Oscillatory Textures and the Weierstrass Function
Consider the following function class consisting of one-dimensional “oscillatory textures” according
to Demanet & Ying (2007).
Definition 5.1. Let the sets FD,a, D, a ∈ R+, be given by
FD,a = {cos(ag)h: g, h ∈ SD} .	(16)
The efficient approximation of functions in FD,a with a large is a notoriously difficult problem
due to the combination of the rapidly oscillating cosine term and the warping g. The best available
approximation results in the literature (Demanet & Ying, 2007) are based on wave-atom dictionaries2
and yield low-order polynomial approximation rates. In what follows we show that finite-width deep
networks drastically improve these results to exponential approximation rates.
Proposition 5.2. There exist a constant C > 0 and a polynomial π such that for all D, a ∈ R+ , f ∈
FD,a, and ∈ (0, 1/2) there is a network Γf, ∈ N N ∞,46,1,1 which satisfies
kΓf, - f kL∞ ([-D,D]) ≤ ,	(17)
with L(Γf,) ≤ C(dDe(log(-1))2 + log(daDe)) and B(Γf,) ≤ max{1/D, π(-1, dDe, dae)}.
Finally, we show how the Weierstrass function—a fractal function, which is continuous everywhere
but differentiable nowhere—can be approximated with exponential accuracy by deep ReLU net-
works. Specifically, we consider
∞
Wp,a(x) = X pk cos(ak πx),	for p ∈ (0, 1/2], a ∈ R+.	(18)
k=0
Let a = - lθg(a). It is well known (ZygmUnd, 2002) that Wp,a, possesses Holder smoothness a
which may be arbitrarily small, depending on p and a, see Figure 1 right. While classical approxi-
mation methods, for instance sparse approximation in frames, are not suitable owing to the warping
operation, it turns out that deep finite-width networks achieve exponential approximation rate. The
corresponding formal statement is as follows.
Proposition 5.3. There exists a constant C > 0 such that, for all ∈ (0, 1/2), p ∈ (0, 1/2], a ∈ R+,
D ≥ 1, there is a network Ψp,a,D, ∈ N N ∞,20,1,1 satisfying
kΨp,a,D, - Wp,a kL∞([-D,D]) ≤ ,	(19)
with L(Ψp,a,D,) ≤ C ((log(1/))3 + 2(log(1/))2 log(dae) + log(1/) log(D)) and B(Ψp,a,D,) ≤
C.
2To be precise, the results in Demanet & Ying (2007) are concerned with the two-dimensional case, whereas
we focus on the one-dimensional case. Note, however, that all our results can be readily extended to the
multivariate case.
7
Under review as a conference paper at ICLR 2019
6 Finite depth is not enough
We next show that, in the approximation of periodic functions, finite-width deep networks require
asymptotically smaller connectivity than finite-depth wide networks. This statement is then extended
to sufficiently smooth non-periodic functions, thereby formalizing the benefit of deep networks over
shallow networks in the approximation of a broad class of functions.
We start with preparatory material taken from Telgarsky (2015).
Definition 6.1 (Telgarsky (2015)). Let k ∈ N. A function f : R → R is called k-sawtooth if it is
piecewise linear with no more than k pieces, i.e., its domain R can be partitioned into k intervals
such that f is linear on each of these intervals.
Lemma 6.2 (Telgarsky (2015)). Every Φ ∈ NNL,M,1,1 is (2M)L -sawtooth.
Definition 6.3. For a non-constant u-periodic function f ∈ C (R), we define
ξ(f) :
inf
δ∈[0,u),
c,d∈R
kf (x) - (cx + d)kL∞([δ,u+δ]).
The quantity ξ(f) measures the error incurred by the best linear approximation of f on any segment
of length equal to the period of f ; It can hence be interpreted as quantifying the non-linearity of
f . The next result states that finite-depth networks with width scaling poly-logarithmically in the
highest frequency of the periodic function to be approximated can not achieve arbitrarily small
approximation error.
Proposition 6.4. Let f ∈ C(R) be a non-constant u-periodic function, let L ∈ N and π a polyno-
mial. Then there exists a ∈ R+ such that for every network Φ ∈ NNL,M,1,1 with M ≤ π(log(a))
it holds that
kf(ax) - Φ(x)kL∞([0,u]) ≥ξ(f)>0.
Application of Proposition 6.4 shows that finite-depth networks, owing to ξ(cos) > 0, require faster
than poly-logarithmic growth of connectivity in a to approximate x 7→ cos(ax) with arbitrarily
small error, whereas finite-width networks, thanks to Theorem 4.1, can accomplish this with poly-
logarithmic growth in connectivity. The next result, taken from (Frenzen et al., 2010), allows us to
extend this conclusion to non-periodic sufficiently smooth functions.
Theorem 6.5 (Frenzen et al. (2010)). Let f ∈ C3([a, b]) and consider a piecewise linear approx-
imation of f on [a, b] that is accurate to within in the L∞ ([a, b])-norm. The minimal number of
linear pieces required to accomplish this scales according to
c
S(C) ~ ~=, E → 0, where C
ι a ___________
4 J VZf00(X)|dx.
Combining this with Lemma 6.2 yields the following statement on the depth-width tradeoff of net-
works approximating three-times continuously differentiable functions.
Theorem 6.6. Let f ∈ C3([a, b]) with Ra Pf 00(x)∣dx > 0, L ∈ N, and let π be a polynomial.
Then there exists C > 0 such that for every network Φ ∈ NNL,M,1,1 with M ≤ π(log(C-1)) it
holds that
kf - ΦkL∞([a,b]) ≥ C.
This shows that any function which is at least three times continuously differentiable cannot be ap-
proximated by finite-depth networks of connectivity scaling poly-logarithmically. In contrast, as
Proposition 3.3 and Theorem 4.1 show, finite-width networks can approximate various interesting
types of smooth functions such as polynomials and sinusoidal functions at poly-logarithmic connec-
tivity growth rates. Further results on the limitations of finite-depth networks akin to Theorem 6.6
were reported recently in Petersen & Voigtlaender (2017).
8
Under review as a conference paper at ICLR 2019
References
H. Bolcskei, P. Grohs, G. KUtyniok, and P. Petersen. Optimal approximation with sparsely connected
deep neural networks. arXiv:1705.01714, 2018.
L. Demanet and L. Ying. Wave atoms and sparsity of oscillatory patterns. Applied and Computa-
tional Harmonic Analysis, 23(3):368-387, 2007.
Y. Ding, J. LiU, and Y. Shi. On the Universal approximability of qUantized ReLU neUral networks.
arXiv:1802.03646, 2018.
D. L. Donoho. Unconditional bases are optimal bases for data compression and for statistical esti-
mation. Applied and Computational Harmonic Analysis, 1:100-115, 1993.
C. L. Frenzen, T. Sasao, and J. T. BUtler. On the nUmber of segments needed in a piecewise linear
approximation. Journal of Computational and Applied Mathematics, 234(2):437 - 446, 2010.
K. Grochenig. Foundations OfTime-Frequency Analysis. Birkhauser Basel, 2001.
K. Grochenig and S. Samarah. Nonlinear approximation with local Fourier bases. Constructive
Approximation, 16(3):317-331, Jul 2000.
P. Grohs. Optimally sparse data representations. Applied and Numerical Harmonic Analysis, 68:
199-248, 2015.
B. Hanin and M. Sellke. Approximating continuous functions by ReLU nets of minimal width.
arXiv:1710.11278, 2017.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. 2016 IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778, 2016.
S. Liang and R. Srikant. Why deep neural networks for function approximation? In ICLR, 2017.
P. Petersen and F. Voigtlaender. Optimal approximation of piecewise smooth functions using deep
ReLU neural networks. ArXiv e-prints, September 2017.
M. Telgarsky. Representation benefits of deep feedforward networks. arXiv:1509.08101, 2015.
D. Yarotsky. Error bounds for approximations with deep ReLU networks. arXiv:1610.01145, 2016.
D. Yarotsky. Optimal approximation of continuous functions by very deep relu networks.
arXiv:1802.03620, 2018.
A. Zygmund. Trigonometric series. Cambridge University Press, 2002.
9
Under review as a conference paper at ICLR 2019
A Proofs
A.1 Proof of Lemma 2.2
Proof. The proof is based on the identity x = ρ(x) - ρ(-x). First, note that by Definition 2.1, we
can write
Φ1(x) = WL11(ρ(. . . W11(x))) and Φ2(x) = WL22(ρ(. . . W12(x))).
Next, define the affine map given by W(x) = W2 ( Qnli	-INLI) x)，for X ∈ R2NLi, and note
that thanks to
W12(Φ1(x)) =Wf ρ	-WWL11	(ρ(...W11(x)))	,
the map
Ψ(x) = WL22 ρ...W22 ρWfρ-WWL111	(ρ(...W11(x)))
satisfies Ψ(x) = Φ2(Φ1(x)), for all x ∈ Rd1, with L(Ψ) = L1 + L2, M(Ψ) ≤ 2M1 + 2M2,
W(Ψι) ≤ max{2NL1, W(Φι), W(Φ2)}, and B(Ψ) ≤ max{B(Φ1), B(Φ2)}.	□
A.2 Proof of Lemma 2.3
Proof. The proof is based on the identity x = ρ(x) - ρ(-x). First, note that by equation 1 we can
write Φ1(x) = WL( ρ ( . . . W1(x))). For K = L + 1, Φ2 is given by
Φ2(x) = (1 -1)ρ	-WWL	(ρ(...W1(x))) ∈ NNL+1,max{2,M},d,1.	(20)
For K > L + 1, consider the network
0	ρ(Φ1(x))	1	0	WL
Φ1 (x) =	ρ(-Φ1(x))	=	0	1	ρ -WL	( ρ ( . . . W1(x)))	∈ N N L+1,max{2,M},d,2,
(21)
which satisfies W(Φ01) = max{2, W(Φ1)}. Next, we note that for every network of the form
Ψ(x) = I2 ρ ( . . . ), the network
Ψ0(x) := I2 ρ (Ψ(x)),	(22)
satisfies Ψ0(x) = Ψ(x), for all x ∈ Rd, L(Ψ0) = L(Ψ) + 1, and W(Ψ0) = max{W(Ψ), 2}.
Moreover, the weights of Ψ0 consist of the weights of Ψ and {1}. Noting that Φ01 in (21) is of
the form I2 ρ ( . . . ) and iteratively applying the operation in equation 22 K - L - 2 times to Φ01,
we obtain a network Φ010 ∈ N N K -1,max{2,M},d,2 . The proof is concluded by noting that Φ2 =
(1 - 1)ρ (Φ?) ∈ NNκ,mɑχ{2,M},d,ι Satisfies Φ2(x) = Φι(χ), forall X ∈ Rd.	□
A.3 Proof of Lemma 2.4
Proof. Apply Lemma 2.3 to the networks Φi to get corresponding networks Φi of depth L and set
Φ1(x) ：= (Φ 1(x1), Φ2(X2), ..., ΦN(XN))>, Φ2(x) ：= (ai,a2, ..., &N)Φ1(x).	□
A.4 Proof of Proposition 3.1
Proof. Consider the function g : [0, 1] → [0, 1],
2x,
g(x) =	,
2(1 - x)
if x <
1-2
1-2
≥
along with the “sawtooth” functions given by its s-fold composition
gs := g ◦ g {•••◦ g, S ≥ 2,
(23)
(24)
z
s
10
Under review as a conference paper at ICLR 2019
and set g0(x) := x, g1 (x) := g(x). We next briefly review a fundamental result from Yarotsky
(2016) showing how the function f (x) := x2 , x ∈ [0, 1], can be approximated by linear combina-
tions of “sawtooth” functions gs. Specifically, let fm be the piecewise linear interpolation of f with
2m + 1 uniformly spaced “knots” according to
fm(2m)=(2m), k=0,..., 2m, m ∈ N0.
The function fm approximates f with error m = 2-2m-2 in the sense of
kfm(x) - x2 kL∞ [0,1] ≤ 2-2m-2.
Next, note that we can refine interpolation in the sense of going from fm-1 to fm by adjustment
with a sawtooth function according to
fm(X) = fm-1(x) - gmm
(25)
This leads to the representation
fm(X) = X - ^X £2s ) .	(26)
While Yarotsky’s construction Yarotsky (2016) is finalized by realizing equation 26 through a deep
ReLU network of width 3 with the help of skip connections He et al. (2016), i.e., connections
between nodes in non-consecutive layers, we proceed by constructing an equivalent (in terms of
input-output relation) network without skip connections and of width 4. As g(X) = 2ρ(X) - 4ρ(X -
1/2) + 2ρ(X - 1), it follows that
gm = 2ρ(gm-1 ) - 4ρ(gm-1 - 1/2) + 2ρ(gm-1 - 1),	(27)
and since fm = ρ(fm), ∀m ∈ N0, equation 25 can be rewritten as
fm = ρ(fm-1) - 2-2m 2ρ(gm-1) - 4ρ(gm-1 - 1/2) + 2ρ(gm-1 - 1).	(28)
Equivalently, equation 27 and equation 28 can be cast as a composition of affine linear maps and a
ReLU nonlinearity according to
fgmm = W1 ρW2fgmm--11, (29)
with
W1 (X) =	-2-22m+1
-4
2-2m+2
2
-2-2m+1
10
W2(χ) =	10
01
Applying equation 29 iteratively initialized with g0 (X) = X, f0 (X) = X yields
0
112).
0
(30)
(31)
and hence shows that fm can be realized through a network in N N m+1,4,1,1 with weights bounded
(in magnitude) by 4. Since m = 2-2m-2 and hence log(1/m) = 2m + 2, the statement follows
upon noting that fm(0) = 0, ∀m ∈ N0.	□
A.5 Proof of Proposition 3.2
Proof. The proof is based on the identity
Xy = 2((χ + y)2 - χ2 - y2),	(32)
which shows how multiplication can be implemented through the squaring operation. Let Ψδ(X) be
a neural network approximating X2 according to Proposition 3.1, i.e., kΨδ(X) - X2 kL∞[0,1] ≤ δ,
11
Under review as a conference paper at ICLR 2019
Ψδ (0) = 0. We first extend this approximation result to the interval [-D, D], D > 1. Specifically,
we need to find a ReLU network realization of x2 and y2 over [-D, D] and of (x + y)2 over
[-D, D]2. This will be accomplished by first noting that
4dDe2ψδ (2⅛⅛ I2
≤ 4dDe2δ,
L∞([-D,D])
(33)
and likewise
4dD]2Ψδ (生常)-(x + y)	≤ 4dD]2δ.	(34)
L∞([-D,D])2
The network X → Ψδ(|x|) has one layer more than the network X → Ψδ(x) as it implements
|x| = ρ(x) + ρ(-x) in its first layer. Next we define for every D ∈ R+, δ ∈ (0, 1/2) the network
φD,δ(X,力=2dDe2卜δ(lx+yp) -ψδ(2|xe) -ψδ(导),	(35)
and observe that Lemma 2.4 implies that there exists a constant C > 0 such that for all D ∈ R+, δ ∈
(0,1/2), x ∈ R it holds that L(ΦD,δ) ≤ Clog(δ-1), W(Φ3,δ) = 12, B(ΦD,δ) ≤ max{4,2「D]2}
and ΦD δx, 0) = ΦD δ (0, x) = 0. Using Equation 32 in combination with Equation 33 and EqUa-
tion 34 yields
φD,δ(χ,y) - 2((χ + y)2 - χ2 -y2) Il	≤ 6dDe2s.
L∞([-D,D])2
The proof is completed by taking for every D ∈ R+, E ∈ (0,1/2) the network Φd,∈ := ΦD,δn e
with δD,e := 61D]2 .	fi
A.6 Proof of Lemma 3.5
Proof. We first consider the case D = 1. A fundamental result on Chebyshev interpolation, see e.g.
(Liang & Srikant, 2017, Lemma 3), guarantees, for all f ∈ S1, n ∈ N, the existence ofa polynomial
Pf,n of degree n such that
kf - Pf,nkL∞([-1,1]) ≤ 2n(n1+1)! kf (n+1)kL∞([-1,1]) ≤ 21n .	(36)
Writing the polynomials Pf,n as Pf,n = Pjn=0 af,n,jxj , crude—but sufficient for our purposes—
estimates show that there exists a constant c > 0 such that for all f ∈ S1, n ∈ N it holds that
Af,n:=j=m0,a..x.,n|af,n,j| ≤ 2cn.
Application of Proposition 3.3 to Pf,n establishes the existence of a constant C1 > 0 such that
for all f ∈ Si, n ∈ N, E ∈ (0,1/2), there is a network Φpf n,1,e∕2 ∈ NN∞,16,1,1 satisfying
B(Φpf,n,1,e∕2) ≤ max{Af,n, 8} ≤ max{2cn, 8},	‘
L(Φpf,n,1,e∕2) ≤ Cιn(cn + log(2∕e) + log(n)),	(37)
and
kφPf,n,1,e∕2 - Pf,nkL∞([-1,1]) ≤ f ∙	(38)
In the following, we set ne = dlog(2/E)e and Ψf,e = Φpf,n ,1,e∕2. Combining equation 36 and
equation 38 establishes that for all f ∈ S1, E ∈ (0, 1/2),
kΨf,e - fkL∞([-1,1]) ≤ kΨf,e - Pf,n kL∞([-1,1]) + kPf,n - fkL∞([-1,1])
≤ f + 2b ≤ 21 + f = E.
Using dlog(2/E)e ≤ 2 log(2/E) and log(2/E) ≤ 2 log(1/E), for all E ∈ (0, 1/2), in equation 37
implies the existence of a constant C2 such that for all f ∈ S1, E ∈ (0, 1/2),
L(Ψf,e)=L(ΦPf,n,1,e∕2) ≤ C2(log(E-1))2.	(39)
12
Under review as a conference paper at ICLR 2019
By the same token there exists a polynomial π1 such that
B(Ψf,e) = B(Φpf,n ,1,e∕2) ≤ max{2", 8} ≤ ∏ι(e-1).
This completes the proof for the case D = 1.
We next prove the statement for D ∈ (0, 1). To this end, we start by noting that for g ∈ SD,
with D ∈ (0, 1), the function fg : [-1, 1] → R, x 7→ g(Dx) is in S1. Hence, there exists, for every
g ∈ Sd, E ∈ (0,1∕2),anetworkΨfg,e ∈ NN∞,16,1,1 satisfyingsupχ∈[-ι,i] ∣Ψfg,e(χ)-fg(x)| ≤ 3
L(Ψfg,) ≤ C2(log(1/))2, and B(Ψfg,) ≤ π1(-1). The claim is established by taking the
network approximating g(χ) to be Ψf e(χ) := Ψfg,e(D) and noting that
SUp	Mfg,e(X) - g(X)I =	SUp	lψfg ,e( DD ) - fg ( D )|
x∈[-D,D]	x∈[-D,D]
=SUp ∣Ψfg ,e(x)- fg (x) I ≤ 3,
x∈[-1,1]
L(Ψ0fg,) ≤ C2(log(1∕3))2, W(Ψ0fg,) ≤ 16, and B(Ψfg,) ≤ (1∕D)π1(3-1).
It remains to prove the statement for the case D > 1. This will be accomplished by approximating f
on intervals of length 2 (or less) and stitching the resulting approximations together using a localized
partition of unity. To this end consider a, b ∈ R such that 1 ≤ b - a ≤ 2, and let h ∈ C∞ ([a, b], R)
with ∣∣h(n)∣∣L∞([a,b]) ≤ n!, for all n ∈ N0. Next, note that the function x → h (b-aX + b++a)
is in S1. Hence, there exists, for every 3 ∈ (0, 1∕2), a network Ψ0h, ∈ N N ∞,16,1,1 such that
supx∈[-1,1] 1ψh,e(X) - h (b-aX + b++a) | ≤ G L(ψh,J ≤ C2(log(1/E)j2, and B(ψh,e) ≤ π1(ET).
The networks Ψh,e(x) := Ψh e (b2^X - 1+a) then satisfy
sup ∣Ψh,e(X) - h(X)I = sup ∣Ψh,e(y) - h(b-2ay + b++a) I ≤ 3	(40)
x∈[a,b]	y∈[-1,1]
L(Ψh,) ≤ C2(log(1/E))2, W(Ψh,) ≤ 16, and B(Ψh,) ≤ max{2, IbI + IaI}π1(E-1). Now, for
D > 1, let ND ∈ N be such that 1 ≤ ND ≤ 2 and consider the intervals
lD,k ：= h ⅛Dd , (k+DD i , k ∈ {-Nd, ..., Nd}.
By equation 40 it follows that, for all D > 1, f ∈ SD, k ∈ {-ND, . . . , ND}, and E ∈ (0, 1/2),
there exists a network Ψf,k, ∈ N N ∞,16,1,1 satisfying
sup ∣Ψf,k,e(X) - f (x)I ≤ f,	(41)
x∈ID,k
L(Ψf,k,) ≤ C2(log(4/E))2, and B(Ψf,k,) ≤ max{2, 2IkI}π1(E-1). We next build a partition
of unity through ReLU networks. Specifically, let χ(X) = ρ(X + 1) - 2ρ(X) + ρ(X - 1), set
XD,k(X) = χ(NDx - k), D > 1, k ∈ Z, and note that χD,k ∈ NN2,3,1,1. ThiS yields a partition of
unity according to
χD,k(X) = 1,	for all X ∈R.	(42)
k∈Z
For D > 1, f ∈ SD, E ∈ (0, 1/2), let f : R → R be given by
ND
f(X) := X Φ2,∕4 (χD,k(X), Ψf,k,(X)),	(43)
k=-ND
where Φ2,∕4 is the multiplication network from Proposition 3.2. Note that If (X)I ≤ 1, for all
X ∈ [-D, D], and IχD,k (X)I ≤ 1, for all X ∈ [-D, D], k ∈ {-ND, . . . , ND}. Observe further
that, for each X ∈ [-D, D], there are no more than 2 indices k such that χD,k(X) 6= 0. Proposition
3.2 therefore implies that the sum in equation 43 has no more than 2 non-zero terms for each X ∈
[-D, D]. Combining equation 41, equation 42, and Proposition 3.2, and noting that supp(χD,k) =
ID,k, hence yields
kf - f kL∞ ([-D,D]) ≤ E,
13
Under review as a conference paper at ICLR 2019
for all D > 1, f ∈ SD, ∈ (0, 1/2). It remains to be shown that the functions f can be realized
by networks with the desired properties. To this end, consider for every D > 1, f ∈ SD, k ∈
{1, . . . ,2ND + 1}, ∈ (0, 1/2), the network αf,k, ∈ N N ∞,19,1,1 given by
αf,k,e(X)= φ2,e∕4(XD,k-(ND + 1)(X), ψf,k-(N0+ 1),e(X)),
and the network βf,k, ∈ NN ∞,23,3,3 according to
X1
βf,k,(X1, X2, X3) := αf,k,(X2) .
Further, setβ0(X) := (X, 0, 0)T and let A ∈ R3×3 be such that A(y1, y2,y3)T = (y1, y1, y2 +y3)T,
for all y1, y2, y3 ∈ R. We can now define, for every D > 1, f ∈ SD,	∈ (0, 1/2), the network
Ψf, ∈ N N ∞,23,1,1 given by
Ψf,(X) :=(0 1 1) βf,2ND+1,(Aβf,2ND,(... (Aβf,1,(Aβ0(X))))).
Direct calculation shows that f(X) = Ψf,(X), for all D > 1, f ∈ SD,	∈ (0, 1/2), X ∈ R.
Furthermore, thanks to Proposition 3.2, there exists a constant C3 > 0 such that, for all D > 1,
f ∈ SD, ∈ (0, 1/2),
W(Ψf,) ≤ 4 + max W(αf,k,) ≤ 23,
k∈{1,...,2ND +1}
2ND+1	2ND+1
L(Ψf,e)=2+ X L(βf,k,e)=2+ X (L(Φ2,e∕4)+max{L(χk-(ND + 1)), L(Ψf,k-(ND + 1),J})
k=1	k=1
≤ 2 + (2ND + 1)(C1 log(16-1) + max{2, C2(log(4-1))2}) ≤ C3D(log(-1))2,
and
B(Ψf ) = max	B(αf k ) ≤ max{8, 4D, max{2, 8D}π1(-1)} ≤ C0 dDeπ1(-1).
,	k∈{1,...,2ND+1}	, ,
(44)
This completes the proof.	□
A.7 Proof of Corollary 4.2
Proof. For every a, D ∈ R+, b ∈ R,	∈ (0, 1/2) take the network given by Ψa,b,D,(X) :=
ΨaD+ |b| (x 一 b) with Ψ L)+ 回 e according to Theorem 4.1 and observe that
,	+ a ,	,	+ a ,
sup	∣Ψa,b,L,e(x) — cos(ax — b)| = Sup	∣ΨaL+ 回 F(y) — cos(ay)l ≤ J
x∈[-DL]	[-(L+ 粤),L+ 粤], + a ,
Applying Theorem 4.1 completes the proof.
□
A.8 Proof of Proposition 5.2
Proof. For all D, a ∈ R+, f ∈ FL,a, let gf, hf ∈ SL be functions such that f = cos(agf)hf
holds. Note that Lemma 3.5 guarantees the existence of a constant C1 > 0 and a polynomial
π1 such that for all D, a ∈ R+, f ∈ FL,a, J ∈ (0, 1/2) there are networks Ψhf,, Ψgf, ∈
NN ∞,23,1,1 WhiChsatisfy L(Ψhf,” L(Ψgf,') ≤ CJD] (log([ 1⅛ ]-1))2, B(Ψhf J B(Ψgf ；) ≤
maχ{1∕D, dDeπι([ ɪ ]-1)} and
kψgf ,e - gf l∣L∞([-D,D]),	kψhf,e - hf ∣∣L∞([-D,D]) ≤ ^dae .	(45)
Theorem 4.1 further ensures the existence of a constant C2 > 0 such that for all a, D ∈ R+,
J ∈ (0, 1∕2) there is a neural network Φa,L, ∈ N N ∞,16,1,1 which satisfies L(Φa,L,) ≤
C2((log(1∕J))2 + log(daDe)), B(Φa,L,) ≤ C2, and
∣∣φa,D,e - cos(a ∙ )kL∞([-D,D]) ≤ f .	(46)
14
Under review as a conference paper at ICLR 2019
Further, thanks to Proposition 3.2, there exists a constant C3 > 0 such that for all ∈ (0, 1/2) there
is a network μe ∈ NN∞,12,2,1 which satisfies L(μe) ≤ C3 log(1∕e), B(μe) ≤ max{4, 2「。]2},
and
SUp	∣μe(χ,y) - xy| ≤ 3.	(47)
x,y∈[-D,D]
For all D, a ∈ R+, f ∈ FD,a, ∈ (0, 1/2) we define the neural networks
Γf,e := μ"a,D,e(Ψgf ,e), Ψhf ,”	(48)
First, observe that Equation 45, Equation 46, and supx∈R | 备 cos(ax)∣ = a imply for all X ∈
[-D, D] that
lφa,D,e(ψgf ,e(X))- CoS(agf (X))I ≤ lφa,D,e(ψgf ,e(x)) - cos(aψgf ,e(X))I
+ | cos(aΨgf,(x)) - cos(agf (x))|
≤ f +a ⅛e ≤ §.
Combining this with Equation 45, Equation 47, and k CoS kL∞ [-D,D] , kf kL∞[-D,D] ≤ 1 yields for
all X ∈ [-D, D] that
Ff,e(X)- f (X)I = |〃e&a,D,e@gf,e(X)), ψhf ,e (X))-COS(agf (X)) hf (X)I
≤ lμe(φa,D,e(ψgf,e(X)), ψhf ,e(X))- φa,D,e(ψgf ,e (X))ψhf ,e (X)I
+ IΦa,D,(Ψgf,(X))Ψhf,(X) - CoS(agf (X))hf (X)I
≤ f + 需 + 1⅛ + 得 ⅛e ≤ e.
By construction there exists a constant C4 and a polynomial π2 such that for all D, a ∈ R+,
f ∈ FD,a, ∈ (0, 1/2) it holds that W (Γf,f) = 46,
L(Γf,f) ≤ Lg + max{L(Φa,D,f) + L(Ψg,,e), L(Ψhf,e)} ≤ C4」D] ((log(e-1))2 + log(da])),
and
B(Γf,f) ≤ max{B(μe), B(Φa,D,J Bψ ,e), B(Ψhf ,e)} ≤ max{1∕D, ∏2L1,dD],da])}.
This completes the proof.	口
A.9 Proof of Proposition 5.3
Proof. For every N ∈ N, p ∈ (0, 1∕2), a ∈ R+, X ∈ R, let SN,p,a (X) = PkN=0 pk CoS(ak πX). The
geometric sum formula ensures that
∞∞
ISN,p,a(X)- Wp,a(X)I≤ X Ipk cos(ak∏x)I≤ X Pk = ɪ - ⅛++1 ≤ 2-N.	(49)
k=N +1	k=N +1
Let Nf := dlog(2∕)], ∈ (0, 1∕2). Next note that Theorem 4.1 ensures the existence of a constant
C1 > 0 such that for all a, D ∈ R+, k ∈ N0, ∈ (0, 1∕2) there is a network φak,D,f ∈ N N ∞,16,1,1
which satisfies L(φak,D,f) ≤ C1((log(-1))2 + log(dakπD])), B(φak,D,f) ≤ C1,and
kΦak,D,f - cos(ak ∏ ∙ )∣∣L∞([-D,D]) ≤ 4 .	(50)
Thanks to X = ρ(X) - ρ(-X) and Lemma 2.3, there exists, for every L ∈ N, a neural network
τL ∈ N N L,2,1,1 which satisfies for all X ∈ R that τL (X) = X. For all p ∈ (0, 1∕2), a, D ∈ R+,
k ∈ N0, ∈ (0, 1∕2), define the neural networks
ΨDa,0(x)=卜φa0,D,e(X)) and ΨDa,k(X1,X2,X3)=卜Φak,D,e(x2)j , k > 0, (51)
and let A ∈ R3×3 be such that A(y1, y2, y3)T = (y1, y1, y2 + y3)T, for all y ∈ R3. Consider now,
forp ∈ (0, 1∕2), a, D ∈ R+, ∈ (0, 1∕2), the network Ψp,a,D,f defined by
Ψp,a,D,f(X) := (0 1	1)ψDp,,af,N(AψDp,,af,N-1(...(AψDp,,af,0(X)))).	(52)
15
Under review as a conference paper at ICLR 2019
Note Equation 50 combined with the geometric sum formula implies that for all p ∈ (0, 1/2),
a, D ∈ R+, ∈ (0, 1/2), x ∈ [-D, D] we have
N	N
∣Ψp,a,D,e(x) - SNe,p,a (x) | = EpkΦa%,D,e(x) - Epkcos(ak∏x)
N	∞
≤ XPk IΦak,D,e(x) — cos(akπx)∣ ≤ e X 2-k = e.
k=0	k=1
Combining this with Equation 49 establishes that for all p ∈ (0, 1/2), a, D ∈ R+,	∈ (0, 1/2),
x ∈ [-D, D] it holds
∣Ψp,a,D,e(x) — Wp,a(x)∣ ≤ 2-dlOg( 1 )e + N ≤ N + f = ^
By construction there exists a constant C2 such that for all p ∈ (0, 1/2), a, D ∈ R+, ∈ (0, 1/2)
we have W(Ψp,a,D,e) = 20,
N
L(Ψp,a,D,e) ≤XL(φak,D,e) ≤ (Ne+1)C1((log(-1))2+log(daNπDe))
≤ C2((log(-1))3 + (log(-1))2 log(dae) + log(-1) log(D)),
and
B(Ψp,a,D,e) =	max	B(φak,D,e) = C1.
k∈{0,...,N}
This completes the proof.
□
A.10 Proof of Proposition 6.4
Proof. First note that for every polynomial π it holds that ∏(log(t)) ∈ O(t), t → ∞. Since X →
(2π(x))L is a polynomial, there exists a ∈ N such that a > (2π(log(a)))L. Lemma 6.2 now implies
that any network Φ ∈ N N L,M,1,1 with M ≤ π(log(a)) is (2π(log(a)))L-sawtooth and therefore
has less than a-many different linear pieces. Hence there exists an interval [u1, u2] ⊆ [0, u] with
u2 — u1 ≥ (u/a) on which Φ is linear. Since u2 — u1 ≥ (u/a) the interval supports a full period of
f (a ∙) and We can therefore conclude that
kf (a ∙ ) - φkL∞([0,u]) ≥ kf (a∙) - φl∣L∞([u1,u2]) ≥
inf
δ∈[0,u),
c,d∈R
kf (x) — (cx + d)kL∞ ([δ,u+δ])
ξ(f).
Finally note, that ξ(f) > 0 holds by assumption, since any continuous u-periodic function Which is
linear on an interval of length u must be constant.
□
A.11 Proof of Theorem 6.6
Proof. The proof Will be effected by contradiction. Assume that for every > 0 there exists a
netWork Φe ∈ N N L,M,1,1 With M ≤ π(log(-1)) and kf — ΦekL∞([a,b]) ≤ . Since every ReLU
netWork is pieceWise linear We can noW apply Theorem 6.5 to conclude that there exists a constant
C such that for all e > 0 the network Φe must have at least Ce-2 many different linear pieces.
This leads to a contradiction as, by assumption combined With Lemma 6.2, Φe is (2π(log(-1)))L-
sawtooth and it holds for every polynomial π that Π(log(e-1)) ∈ o(e-1/2), e → 0.	口
16