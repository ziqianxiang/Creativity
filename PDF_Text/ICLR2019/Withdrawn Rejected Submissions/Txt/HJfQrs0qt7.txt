Under review as a conference paper at ICLR 2019
Convergence Properties of Deep Neural
Networks on Separable Data
Anonymous authors
Paper under double-blind review
Abstract
While a lot of progress has been made in recent years, the dynamics of learning
in deep nonlinear neural networks remain to this day largely misunderstood. In
this work, we study the case of binary classification and prove various properties
of learning in such networks under strong assumptions such as linear separability
of the data. Extending existing results from the linear case, we confirm empirical
observations by proving that the classification error also follows a sigmoidal shape
in nonlinear architectures. We show that given proper initialization, learning
expounds parallel independent modes and that certain regions of parameter space
might lead to failed training. We also demonstrate that input norm and features’
frequency in the dataset lead to distinct convergence speeds which might shed
some light on the generalization capabilities of deep neural networks. We provide
a comparison between the dynamics of learning with cross-entropy and hinge
losses, which could prove useful to understand recent progress in the training
of generative adversarial networks. Finally, we identify a phenomenon that we
baptize gradient starvation where the most frequent features in a dataset prevent
the learning of other less frequent but equally informative features.
1 Introduction
Due to extremely complex interactions between millions of parameters, nonlinear activation functions
and optimization techniques, the dynamics of learning observed in deep neural networks remain much
of a mystery to this day. What principles govern the evolution of the neural network weights? Why
does the training error evolve as it does? How do data and optimization techniques like stochastic
gradient descent interact? Where does the implicit regularization of deep neural networks trained
with stochastic gradient descent come from? Shedding some light on those questions would make
training neural networks more understandable, and potentially pave the way to better techniques.
It is commonly accepted that learning is composed of alternating phases: plateaus where the error
remains fairly constant and periods of fast improvement where a lot of progress is made over the
course of few epochs (Saxe et al., 2013a). Theoretic explanations of that phenomenon exist in the
case of regression on linear neural networks (Saxe, 2015) but extensions to the nonlinear case (Heskes
& Kappen, 1993; Raghu et al., 2017; Arora et al., 2018) fail to provide analytical solutions.
It has been observed in countless experiments that deep networks present strong generalization
abilities. Those abilities are however difficult to ground in solid theoretical foundations. The fact
that deep network have millions of parameters - a number sometimes orders of magnitude larger
than the dataset size - contradicts the expectations set by classic statistical learning theory on the
necessity of regularizers (Vapnik, 1998; Poggio et al., 2004). This observation drove Zhang et al.
(2016) to suggest the existence of an implicit regularization happening during the training of deep
neural networks. Advani & Saxe (2017) show that the dynamics of gradient descent can protect
against overfitting in large networks. Kleinberg et al. (2018) also offer some explanations of the
phenomenon but understanding its roots remains an open problem.
In this work, we study the learning dynamics of a deep nonlinear neural network - i.e. how its
weights and outputs evolve throughout learning - trained on a standard classification task using two
different losses: the cross-entropy and the hinge loss. We mainly focus on binary classification,
some of the results and properties can however be extended to the multi-class case. The questions
we address in Sections 3, 4 and 5 respectively can be summarized as follows:
1
Under review as a conference paper at ICLR 2019
How does the confidence of a classifier evolve throughout learning?
How does the loss used during training impact its dynamics?
Which properties of the features present in a dataset impact learning, and how?
Independent mode learning We show that, similarly to the case of linear networks and under certain
initial conditions, learning happens independently between different classes, i.e. classes induce a
partition of the network activations, corresponding to orthogonal modes of the data.
Learning dynamics We prove that in accordance to experimental findings, the hidden activations
and the classification error of the network show a sigmoidal shape with slow learning at the beginning
followed by fast saturation of the curve. We also characterize a region in the initialization space
where learning is frozen or eventually dies out.
Hinge loss We study how using the hinge loss impacts learning and quantitatively compare it to the
classic cross-entropy loss. We show that the hinge loss allows one to solve a classification task much
faster, by providing strong gradients no matter how close to convergence the neural network is.
Gradient starvation Finally, we identify a phenomenon that we call gradient starvation where the
most frequent features present in the dataset starve the learning of other very informative but less
frequent features. Gradient starvation occurs naturally when training a neural network with gradient
descent and might be part of the explanation as to why neural networks generalize so well. They
intrinsically implement a variant of Occam’s razor (Ariew, 1976): the simplest explanation is the
one they converge to first.
2 Setup and notations
We are interested in a simple binary classification task, solved by training a deep neural network
with gradient descent. This simple setup encompasses for instance the training of generative ad-
versarial networks discriminators. Some of our results extend to multi-class classification, but,
for the sake of conciseness, that case is treated in Appendix A. We let D = {(xi, li)}1≤i≤n ⊂
Rd × {1, 2} denote our dataset of vectors and labels. The classifier we consider is a sim-
ple neural network with one hidden layer of h neurons and a ReLU non-linearity (see Fig. 1).
The output of the network is passed through a function
denoted o which is either the sigmoid σ in the binary
cross-entropy case or the identity in the hinge loss case.
The full function can be written as
Pt(x) := o(ut (x)) := o(ZtT (Wtx)+),
where Wt is an h × d matrix and Zt a vector of length
h. In the C -class case, Zt is instead a C × h matrix
and o the softmax function. An extension to deeper
networks can be found in Appendix B. Wt and Zt are Figure 1: Network Architecture
the parameters of the neural network. The subscript
+ (resp. t) denotes the positive part of a real number
(resp. the state of the element at time step t of training).
The superscript T stands for the transpose operation. In the case of cross-entropy, Pt (x) represents
the probability that x belongs to class 1, for the hinge loss, Pt (x) is trained to reach {1, -1} for
classes 1 and 2. For a given class k, we let Dk denote the set of vectors belonging to it. We make
the assumption
(H1) For any x, x0 ∈ Dk, xTx0 > 0. For any x ∈ D1 and x0 ∈ D2, xTx0 ≤ 0.
It implies linear separability of the data, an assumption often necessary in theoretical studies (Soudry
et al., 2017; Liao & Couillet, 2018; Nacson et al., 2018; Xu et al., 2018) and the positioning of the
origin between the two sets. It is a very strong assumption which admittedly bypasses a large part
of the deep learning dynamics. Nevertheless, it allows the discovery of interesting properties and is
potentially a first step towards understanding behaviors observed in more general settings.
2
Under review as a conference paper at ICLR 2019
3 Learning dynamics for binary cross-entropy
In this section, we focus on the case of the binary cross-entropy loss
LBCE(Wt,Zt; x) = -lχ∈Dι log(σ(ZT(Wtx)+)) - lχ∈02 log(1 - σ(ZT(Wtx)+)),
and train our network using stochastic gradient descent to minimize LBCE .
3.1	Independent modes of learning
Our first lemma states that over the course of training and under suitable initialization, the active
neurons of the hidden layer remain the same for each datapoint, and the coordinates of Zt remain of
the same sign. To prove it, we let wti denote the i-th row of Wt and make the additional assumptions:
there exists a partition {I1, I2} of {1, . . . , h} such that with k ∈ {1, 2}
(H2) For any i ∈ Ik, x ∈ Dk and x0 ∈/ Dk, w0ix > 0 and w0i x0 ≤ 0.
(H3) The i-th coordinate of Z0 is positive if i ∈ I1, negative otherwise.
Assumption (H2) states that at the beginning of training, data points from different classes do not
activate the same neurons. Itis an analogue to the orthogonal initialization used in Saxe et al. (2013b).
In Appendix A.8, we show that relaxing it hints towards an extended period of slow learning in the
early stages of training. (H3) is introduced for Lemma 3.1 and Theorem 3.2 but will be relaxed later.
Lemma 3.1. For any k ∈ {1, 2}, x ∈ Dk and t ≥ 0, the only non-negative elements of Wtx are the
ones with an index i ∈ Ik. The signs of the coordinates of Zt remain the same throughout training.
This lemma proves that updates to the parameters of our network are fully decoupled from one class
to the other. An update for a data point in Dk will only influence the corresponding active rows and
elements of Wt and Zt . This "independent mode learning" is an equivalent of the results by Saxe
et al. (2013b) in a non-linear network trained on the cross entropy loss. The proof of the lemma and
its extension to N - 1 hidden layers and multi-class classification can be found in Appendix A.
3.2	Learning dynamics
We are now interested in the actual dynamics of learning, and move from discrete updates to
continuous ones by considering an infinitesimal learning rate α (Heskes & Kappen, 1993). Lemma
3.1 can easily be extended to this setting. For simplicity we assume h = 2, but similar results hold
for arbitrary h (see Appendix A.4). For the moment, we maintain the assumptions (H1-3).
Theorem 3.2. Assuming that each class k contains the same vector xk repeated |Dk| times, then the
output of the classifier on Dk verifies (with pk = |Dk|/|D| the fraction of D belonging to Dk):
Pt (xk ∈ Dk )
σ(u(kxkkpkt)),
where u is defined below. The classification curves are sigmoidal and can be found on Fig. 2 Right.
Proof. To simplify the notations, we arbitrarily assume that I1 = {1} and we write wt and zt
the row and element modified by an update made using x ∈ D1 (the case of D2 can be treated
symmetrically). By the independence above, we know that wt and zt are only affected by updates
from D1. This greatly simplifies our evolution equations to: wt0 = δf (x)zt xT and zt0 = δf (x) wtx
where δf (x) is the gradient of the loss with respect to the pre-sigmoid output of the network ut(x):
δf (x) = 1{k=1} - σ(ut(x)) and the prime indicates a time derivative. We let yt = wtx, which gives
xTx zt
1 + eytzt
1 + eytzt
(1)
Writing kxk2 = xTx, we see that the quantity yt2 - kxk2 zt2 is an invariant of the problem, so its
solutions live on hyperbolas of equation y2 - kxk2z2 = ±c with c := |y02 - kxk2z02|.
We only treat the case of a degenerate hyperbola c = 0 i.e. y02 = kxk2z02, and refer the interested
reader to Appendix A.3 for the full derivation. In the case c = 0, we have ∀t, yt = kxkzt (those
quantities are both positive as x ∈ D1 and (H2-3)). u(t) := ztwtx = ztyt thus follows the equation
3
Under review as a conference paper at ICLR 2019
Figure 2: Left. Phase diagram representing the dynamics of learning for the couple (zt, yt) depending
on its initialization. yt is the value for the class considered, in which all examples have lined up.
Each couple lives on a hyperbola. The slope of the linear curves is equal ±kxk (set to 0.7 in this
diagram). The green region represents the initializations of (y, z) where the classification task will
be solved by the network. In the red region, learning does not start (the neuron is inactive at the
beginning of training) or collapses as the neuron dies off when y reaches 0. The ci points show the
three cases from Section 3.3. Right. yt and Pt(x ∈ D1) = σ(ztyt) for different values of c and kxk.
u0(t) = 2kxku(t)σ(-u(t)). One can see the equivalence between our evolution equation and
Eq. (10) in Saxe et al. (2013b). Its analytical solution is (see Appendix A.3):
u(t) = (log+Ei)<-1>(2kxkt + log(u0) + Ei(u0)) ,	(2)
where Ei is the exponential integral (Wiki., 2018) and <-1> denotes the inverse function.
We let U denote that function for ∣∣xk = 1. For X ∈ D2, the degeneracy assumption becomes y0 =
-kxkz0. It can be shown similarly that v(t) := ztyt verifies the equation v0(t) = 2kxkv(t)σ(v(t))
with a negative initial condition (H2-3). In other words, u and v follow symmetric trajectories on the
Positive/negative real line. Below, U (resp. V) denote those two trajectories for ∣∣x∣ = 1 and initial
conditions u0 > 0 (resp. v0 < 0). Let us now write p1 = |D1 |/|D| the fraction of points belonging
to D1. Because we sample randomly from the dataset, this amounts to sampling p1 (resp. 1 - p1)
points from D1 (resp. D2) for each time unit during training,	i.e.	to rescaling the time axis by	p1	for
D1 and 1 - p1	for D2 . This allows us to quantify the network’s	performance at any time t:
P	Pt(X ∈ Di) = σ(U(∣∣x∣∣pιt))	if X ∈ Di	芭
[	Pt(x ∈ D2) = σ(-V(∣∣x∣∣(1 — pi)t))	if x ∈ D2	(，
In particular, the convergence of U(t) to +∞ can be bounded using our results: convergence happens
at a rate slower than log(t) (Appendix A.3), a fact proved on its own by Soudry et al. (2017).	□
Interpretation Fig. 2 Right. shows the learning dynamics for different values of ∣X∣ and c. One
common characteristic between all the curves is their sigmoidal shape. Learning is slow at first, then
accelerates before saturating. This is aligned with empirical results from the literature. We also
see on e.g. the blue and yellow curves that a larger ∣X∣ (or similarly a larger p) converges much
faster. The effect ofconthe dynamics can mostly been seen at the beginning of training (for instance
on the green and yellow curves). It fades as convergence happens, corresponding to points of the
hyperbolas getting closer to the asymptote y = ∣X∣z, see Fig. 2 Left. and below for more details.
We can characterize the convergence speeds more quantitatively with the following corollary.
Corollary 3.3. Let δ be the required accuracy on the classification task (i.e. Pt(X ∈ Di) ≥ 1 — δ
for X ∈ Di). Under certain assumptions, the times tf and t2 required to reach that accuracy for
each classifier verify ∣⅜ ≈ kkχ1∣ ɪ--p^ where ∣∣Xk ∣∣ is the norm of vector ∣∣Xk ∣∣ from class k.
The proof can be found in Appendix A.5. More frequent classes and larger inputs will be classified
at a given level of confidence faster. The class frequency observation is fairly straightforward as
updates on a more frequent class occur at a higher rate. As far as input sizes are considered, this can
be seen as an analogous to the results from Saxe et al. (2013b) stating that input-output correlations
4
Under review as a conference paper at ICLR 2019
drive the speed of learning. Because a sigmoid is applied on the network output, its (pre-sigmoid)
targets are sent to ±∞. A larger input is more correlated with its target and converges faster.
On the assumptions The assumption that each class only contains one vector allows us to obtain the
first closed-form solutions of the learning dynamics for the binary cross-entropy. It can be relaxed
to classes containing orthogonal datapoints (see Appendix A.7) which still remains restrictive. A
possible interpretation is the following: if one were to consider a deep neural network that has learnt
two discriminative features for the two classes, applying classic SGD on those features would result
in a learning rate proportional to the prominence of those two features in the original dataset, and to
learning curves of that exact shape. It is worth noting that such shapes are regularly observed by ML
practitioners (Saxe et al., 2013b), our results reveal insights - otherwise unobtainable - into them.
3.3	Phase diagram
In this section, we build the phase diagram of Fig. 2 Left. The notations follow Theorem 3.2, in
particular yt = wtx. So far, we have considered points in the top-right quadrant. In that region, the
couple (zt, yt) lives on a hyperbola of equation y2 - kxk2z2 = ±c where c ≥ 0. The sign in the
equation is defined by the position of (z0, y0) relative to the function y = kxkz (positive if above,
negative otherwise). If (z0, y0) is originally on that line, it will remain there throughout training. We
now explore the rest of the parameter space by relaxing some of our assumptions. We still consider
a point x ∈ D1 , the diagram for D2 can be obtained by mirroring the z axis.
Assumption (H2). Let us first consider the simple case of w0x = y0 < 0. The neuron is initially
inactive because of the ReLU. No updates will ever be made to wt during training. This corresponds
to the bottom half of the phase diagram, the parameters are frozen (also see Advani & Saxe (2017)).
Assumption (H3). We now assume that z0 ≤ 0. In that case, a simple extension of Lemma 3.1 shows
that learning still happens independently on each row of wt. The outcome from Theorem 3.2 is still
valid: the couple (zt, yt) lives on a hyperbola. It is however not guaranteed anymore that yt shall
remain positive throughout training. There are three possible situations (numbered 1 to 3), each
represented by the corresponding point on the diagram.
1)	If y0 = -kxkz0, then the points (zt , yt) are stuck in the top-left quadrant and converge to
zero. The equation verified by the logit u(t) is u0(t) = -2kxku(t)σ(-u(t)) (see Appendix A.6).
2)	If y0 > -kxkz0, the points (zt, yt) move on the hyperbola towards the top-right quadrant, at
which point zt becomes positive and yt starts increasing again.
3)	If y0 < -kxkz0, the points (zt, yt) move on the hyperbola towards the bottom-left quadrant,
at which point yt becomes negative. When that happens, the neuron dies out, learning stops.
Only in the second case will the classifier end up solving the task: random initialization only functions
in certain parts of the (yt , zt) space. Those findings are summarized in the phase diagram Fig. 2
Left. The red region represents the initialization where the network will not be able to solve the task.
Failure modes Assumption (H2) essentially means that the network’s first layer is able to separate
the data at t = 0. It is remarkable that even under (H2) the model fails on a non-zero measure set of
the initialization space (top-left red region of Fig. 2 Left): in that region, it converges to a classifier
assigning a probability of 0.5 to the true class (dash-dotted curves in Fig. 3 Right).
3.4	Relaxing Assumption (H2)
Assumption (H2) states that for any i ∈ Ik, x ∈ Dk and x0 ∈/ Dk, w0ix > 0 and w0i x0 ≤ 0. We
now relax it by assuming that a point x2 in D2 verifies w01x2 > 0 and we study the evolution of wt1.
We consider updates coming from sampling equally x1 from D1 and x2 from D2 . To simplify the
analysis, we assume that x1T x2 = 0 and write αt = wt1x1 (equivalent to yt above) and βt = wt1x2.
The triplet (αt, βt, zt) satisfies the following system of ODEs (see Appendix A.8 for the derivation):
αt = lx1k2z^ ,	β0 = -Jχ2k2zt- ,	Zt =	- -Jβ- .	(4)
t	1 + eztαt	t 1 + e-ztβt	t 1 + eztαt	1 + e-ztβt
From our relaxation of (H2), we have α0 , β0 > 0. Since the system is symmetric under the
transformation (αt, βt,zt) → (βt, αt, -zt), we can assume z0 ≥ 0. Due to the ReLU activations
5
Under review as a conference paper at ICLR 2019
Figure 3: Left. Solutions of (4) for different initializations and c = 1. Right. Values of αt, βt and
Pt the confidence of the classifier on an example from class D1 for three different initializations.
The “full” curves correspond to (α0, β0, z0) = (0.1, 0.1, 0.1) i.e. a trajectory in the green region
where βt reaches 0 (orange curve). The confidence on class D1 tends to 1 (green curve). The
“dashed” curves correspond to (α0, β0, z0) = (0.2, 0.9, 0.2) i.e. a trajectory in the yellow region,
corresponding to αt reaching 0 (red curve). The confidence on D1 goes to 0.5 in that case (brown
curve), and the confidence on class D2 goes to 1 (not shown). The “dash-dotted" curves correspond
to (α0, β0, z0) = (1, 0, -1.1) and are an instance of the aforementioned failure mode: αt (or
equivalently yt) tends to 0 (pink curve), βt (not shown) remains 0 and Pt tends to 0.5 (grey curve).
of the network, whenever αt or βt reaches zero, it becomes constant and its contribution to zt
disappears: the model reaches the independent modes of learning regime from Section 3.1 and
evolves according to the results above (e.g. green hyperbolas in the plane β = 0 on Fig. 3 Left). If
βt (resp. αt) reaches 0 at some point, D1 (resp. D2) becomes the only class activating the neuron.
Let us now characterize how the initialization of the network influences the outcome of learning.
Theorem 3.4. Letting C := α0/∣∣x11∣2 + β2∕∣∣x21∣2 - z2 ,the solutions of (4) verify for all t ≥ 0,
ɑ2∕∣∣x11∣2 + β2∕∣∣x21∣2 — z2 = c. In other terms, they live on hyperboloids (see Fig. 3 Left).
If c ≤ 0, βt reaches 0 at some point during training (Fig. 6 of the Appendix). If c > 0, there exists a
curve Cc (shown in black on Fig. 3 Left) such that as t → +∞, for any initialization (α0, β0, z0) ∈ Cc:
αt → (可 + W厂1/2√c,	βt → (k1¥ + W厂1/2√c,	Zt → 0.
That curve defines two regions of the initialization space. In one, colored yellow on Fig. 3 Left, the
trajectories verify αt = 0 for some t. In the other, colored green, βt reaches 0 at some point.
Interpretation The proof of the theorem can be found in Appendix A.8. Fig. 3 Left shows some
solutions of (4). Concretely, we see from the equations that the sign of zt determines whether αt and
βt increase or decrease, and how fast they do so. zt’s evolution on the other hand is the result of a
competition between αt and βt . If z0 and/or α0 are sufficiently large, βt will decrease fast and long
enough to reach 0 at some point (green curves). Conversely, for a large β0 , zt can reach 0 before
βt . When that is the case, αt then decreases until it reaches 0 (yellow curves). We plot examples
of those behaviors in Fig. 3 Right. We notice in particular the classic sigmoidal shape appearing,
even when Assumption (H2) is violated. This can be explained as follows. In the regime of small
initializations (customary in deep learning), the competition between αt , βt and zt happens in a part
of parameter space where all the weights are small (i.e. where the confidence of the network is close
to 0.5). When one class finally prevails over the other, e.g. βt reaching 0 (orange curve in the plot),
the analytical solutions from previous sections apply and the sigmoidal shape arises.
4	On the hinge loss
Recent results in the field of generative adversarial networks have resurrected the hinge loss (Miyato
et al., 2018). While its exact impact on performance is unclear, we run a small experiment to show
its ability to generate better samples than the customary cross-entropy (see Fig. 4 and Appendix E).
In order to perhaps uncover reasons behind its efficiency, we extend our results to the hinge loss:
LH(Wt, Zt; x) = max(0,1 — ZT(Wtx)+ ∙ (口x∈Dι  口x∈D2)).
6
Under review as a conference paper at ICLR 2019
Figure 4: Left. The three figures on the left are the result of training a generative adversarial network
on 8 Gaussians (see Appendix E for details on the experiment). The samples from the hinge loss are
incomparably better. Right. Comparison between hinge loss and binary cross-entropy: training time
required to reach a confidence δ on the classification problem. Subplot: Solutions of Eqs. 3 and 5.
The hinge loss is non differentiable, but one can simply consider that learning stops as soon as the
output of the network reaches 1 (resp. -1) for class D1 (resp. D2). Under the same assumptions than
in Theorem 3.2 (some of which can be relaxed, see Appendix C), we have the following result:
Theorem 4.1. For x ∈ D1 (for D2 it is simply the opposite), the output u(t) of the network verifies
u(t) = min(1, uo e2pkxkt),	u(t) = min(1,需 sinh(θ0 + 2p∣∣x∣∣t)),	(5)
2kxk
where θ0 = CoSh-1 (y0 + kχk z0) and the left and right equations correspond to C = 0 and c = 0.
Proof. Simple computations show that the dynamics of the system are governed by yt0 = xTx zt
and zt0 = yt. Following the method from Section 3, we see that u0(t) = 2kxku(t) in the case where
c = 0, leading to to the result. When c 6= 0, a classic hyperbolic change of variables allows to find
the solution. Its full derivation is presented in Appendix C.	□
The learning curves are plotted in Fig. 4 Right. We notice a hard sigmoidal shape corresponding
to learning stopping when ut reaches 1. Confidence increases exponentially in t, much faster than
for binary cross-entropy (all other parameters kept equal) where u(t)〜log(t). With δ the required
confidence for our classifier, the time t* required to reach δ can easily be computed. We plot it in
Fig. 4 Right which confirms visually that the hinge loss converges much faster. We also notice the
expected divergence of t* for the binary cross entropy as δ reaches 1 (training never converges in
that case). We refer the interested reader to Appendix C for a more general treatment of the Hinge
loss, which fully relaxes the assumption on the number of points in the classes.
5	Gradient Starvation
In this section, we attempt to quantify the impact of feature frequency inside a given class. We keep
our simplified framework and consider that the input x to our network is composed of two underlying
features x1 ∈ Rd1 and x2 ∈ Rd2 with d = d1 + d2. We let (x1, x2) ∈ Rd denote the concatenation
of the vectors x1 and x2. We assume that all the points in class D1 contain the feature x1 but only a
fraction λ of them contains the feature x2. This is equivalent to making continuous gradient updates
using the vector (x1, x2) with a rate λ and the vector (x1, 0) with a rate 1 - λ. We also assume that
those features are fully informative for D1 - i.e. are absent from class D2. A network trained using
gradient descent on the dataset we just described has the following property
Even though the feature represented by x2 is fully informative of the class, the network will not
classify a sample containing only x2 with high confidence.
It is the result of a phenomenon we coin gradient starvation where the most frequent features starve
the gradient for the least frequent ones, resulting in a slower learning of those:
Theorem 5.1. Let δ be our confidence requirement on class D1 i.e. training stops as soon as
∀x ∈ D1, Pt(x ∈ D1) ≥ 1 - δ, and let t* denote that instant. Then, under some mild assumptions,
Pt ((0,x2) ∈ DI) ≤ . e-λiog(.).	(6)
7
Under review as a conference paper at ICLR 2019
Proof. From Lemma 3.1, assumptions (H2-3) are sufficient to guarantee independent mode learning
as well as positiveness ofzt. We decompose wt = (αtx1, βtx2)+(x1⊥, x2⊥) where x1T x1⊥ = x2T x2⊥ =
0, and assume that α0 ≥ β0 /λ > 0 (in App. D, we relax some of those assumptions and prove an
equivalent result). The evolution equation for Wtis Wt = ".血:.with X = (χ1,χ2) (resp. (xi, 0))
at an λ (resp. 1 - λ) rate. Projecting on x1 and x2 gives
α = λ—y +(1 - λ)——t—,	βt = λ—.	(7)
t 1 + ezt (αt+βt)	1 + eztαt	t 1 + ezt (αt+βt )
From zt > 0, we see that βt is an increasing function of time, which guarantees βt > 0, and
“t ≥ βt+(1 - λ)1+ ezztαt+βt) =(I + 1~λ^ )β0 = λ .
This proves that Vt ≥ 0, at ≥ βt∕λ. We now consider t* such that zt*αt* = log( 1-δ). It is the
smallest t such that Pt ((x1, x2) ∈ D1) ≥ Pt ((x1, 0) ∈ D1) = 1 - δ, its existence is guaranteed by
zt and αt being increasing (we also assume that t = 0 does not verify those (in)equalities). We get
Pt* ((0, X2) ∈ D1) =-----1-L ≤ ---------1--------=-------1__rɪ .
t 11 , 2	"	1 + e-zt* βt* _ 1 + e-λzt* αt*	1 + e-λ iog( 1-δ)
As can be seen in Eq. 7, the presence of at - which detects feature xι 一 in the denominator of βt
greatly reduces its value, thus preventing the network from learning x2 properly.	口
①.Irnro①M-①」笛」co ⅛0u
Figure 5: Upper bound on Pt* ((0, x2) ∈ D1) as a
function of 1 - δ for different values of λ.
Table 1: Gradient starvation
δ	λ			
	0.5	0.2	0.1	0.01
99%	91%	71%	61%	51%
99.99%	99%	86%	72%	52%
Table 2: Accuracy on the cats and dogs
dataset (Real means the untouched test set)
Training Testing Testing (Real)
100%	100%	43.2%
In Fig. 5 Left., we plot for different values of λ the confidence of the network when classifying x2 as
a function of its confidence on x1 (see App. D. for more details). The gap between the two is very
significant: with e.g. λ = 0.1 and δ = 10-4 (Table 1), Pt* ((0, x2) ∈ D1) ≤ 72%! Even though x2
is exclusively present in D1 , and is thus extremely informative, the network is unable to classify it.
Experiment To validate those findings empirically, we design an artificial experiment based on the
cats and dogs dataset (Kaggle, 2018). We create a very strong, perfectly discriminative feature by
making the dog pictures brighter, and the cat pictures darker. We then train a standard deep neural
network to classify the modified images and measure its performance on the untouched testing set.
Results The results can be seen in Table 2. The network perfectly learns to classify both the train and
test modified set, but utterly fails on the real test data. This proves that the handcrafted light feature
was learnt by the network, and is used exclusively to classify images. All the features allowing to
recognize a cat from a dog are still present in the data, but the low level features (e.g. the presence of
whiskers, how edges combine to form the shape of the animals and so on) are far less frequent than
the light intensity, and thus were not learnt. The most frequent feature starved all the others.
6	Related work
The learning dynamics of neural networks have been explored for decades. Baldi & Hornik (1989)
studied the energy landscape of linear networks and the fixed point structure of gradient descent
8
Under review as a conference paper at ICLR 2019
learning in that context. Heskes & Kappen (1993) developed a theory encompassing stochastic
gradient descent and parameter dynamics and wrote down their evolution equations in on-line
learning. However, those equations are heavily nonlinear and do not have closed-form solutions in
the general case. Saxe et al. (2013b) study the case of deep linear networks trained with regression.
They prove the existence of nonlinear learning phenomena similar to those seen in simulations of
nonlinear networks and provide exact solutions to the dynamics of learning in the linear case. Some
of our results are an extension of theirs to nonlinear networks. Choromanska et al. (2014); Raghu
et al. (2017); Saxe (2015); Yosinski et al. (2014) also focus on neural network dynamics, while
Nacson et al. (2018); Xu et al. (2018); Soudry et al. (2017) study the convergence rate of learning on
separable data. Arora et al. (2018) prove that overparameterization can lead to faster optimization.
Recent work in the domain of generative adversarial networks (Goodfellow et al., 2014) has shown the
resurgence of the hinge loss (Rosasco et al., 2004). In particular, part of the success encountered by
Miyato et al. (2018) is due to their use of that specific loss function. Their main contribution however
is a spectral normalization technique that produces state-of-the-art results on image generation. Their
paper is part of a larger trend focusing on the spectra of neural network weight matrices and their
evolution during learning (Vorontsov et al., 2017; Odena et al., 2018; Pennington et al., 2017). It,
nevertheless, remains a poorly understood subject.
Zhang et al. (2016) performed some experiments proving that deep neural networks expound a so-
called implicit regularization. Even though they have the ability to entirely memorize the dataset, they
still converge to solutions that generalize well. A variety of explanations for that phenomenon have
been advanced: correlation between flatness of minima and generalization (Hochreiter & Schmid-
huber, 1997), natural convergence of stochastic gradient descent towards such minima (Kleinberg
et al., 2018), built-in hierarchical representations (LeCun et al., 2015), gradient descent naturally
protecting against overfitting (Advani & Saxe, 2017), and structure of deep networks biasing learning
towards simpler functions (Neyshabur et al., 2014; Perez et al., 2018). Our results from Section 5
suggest that gradient descent indeed has a beneficial effect, but can also hurt in some situations.
7	Discussion
In order to obtain closed form solutions for the learning dynamics, we made the extremely simplifying
assumption that each class only contains one point. We leave overcoming that limitation to future
work. In the spirit of the proof in Section 5 where we considered two datapoints, we might be able
to obtain upper and lower bounds on the learning dynamics.
Our comparison between the cross-entropy and the hinge losses reveals fundamental differences.
It is noteworthy that the hinge loss is an important ingredient of the recently introduced spectral
normalization (Miyato et al., 2018). The fast convergence of networks trained with the hinge loss
might in part explain its beneficial impact. A deeper analysis of the connections between the two
would lead to a better understanding of the performance of the algorithm.
In this paper, we introduce the concept of gradient starvation and suggest that it might be a plausible
explanation for the generalization abilities of deep neural networks. By focusing most of the learning
on the frequent features of the dataset, it makes the network ignore the idiosyncrasies of individual
datapoints. That rather desirable property has a downside however: very informative but rare features
will not be learnt during training. This strongly limits the ability of the network to transfer to different
data distributions where e.g. the rare feature exists on its own.
References
Madhu S. Advani and Andrew M. Saxe. High-dimensional dynamics of generalization error in neural
networks. CoRR, abs/1710.03667, 2017.
Roger Ariew. Ockham’s Razor: A Historical and Philosophical Analysis of Ockham’s Principle of
Parsimony. PhD thesis, University of Illinois at Urbana-Champaign, 1976.
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. CoRR, abs/1802.06509, 2018. URL http://arxiv.org/
abs/1802.06509.
9
Under review as a conference paper at ICLR 2019
P. Baldi and K. Hornik. Neural networks and principal component analysis: Learning from examples
without local minima. Neural Networks, 2:53-58,1989.
Anna Choromanska, Mikael Henaff, Michael Mathieu, Gerard Ben Arous, and Yann LeCun. The
loss surface of multilayer networks. CoRR, abs/1412.0233, 2014. URL http://arxiv.org/
abs/1412.0233.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural in-
formation processing systems, pp. 2672-2680, 2014. URL http://papers.nips.cc/paper/
5423-generative-adversarial-nets.pdf.
Tom M. Heskes and Bert Kappen. On-line learning processes in artificial neural networks, 1993.
S. Hochreiter and J. Schmidhuber. Flat minima. Neural Computation, 9(1):1-42, 1997.
Kaggle. Dogs vs. cats, 2018. URL https://www.kaggle.com/c/dogs-vs-cats.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2014. URL http:
//arxiv.org/abs/1412.6980. cite arxiv:1412.6980Comment: Published as a conference paper
at the 3rd International Conference for Learning Representations, San Diego, 2015.
Robert Kleinberg, Yuanzhi Li, and Yang Yuan. An alternative view: When does sgd escape local
minima? CoRR, abs/1802.06175, 2018. URL http://dblp.uni-trier.de/db/journals/
corr/corr1802.html#abs-1802-06175.
Yann LeCun, Yoshua Bengio, and Geoffrey E. Hinton. Deep learning. Nature, 521(7553):436-
444, 2015. URL http://dblp.uni-trier.de/db/journals/nature/nature521.html#
LeCunBH15.
Zhenyu Liao and Romain Couillet. The dynamics of learning: A random matrix approach. In
Jennifer G. Dy and Andreas Krause (eds.), ICML, volume 80 of JMLR Workshop and Conference
Proceedings, pp. 3078-3087. JMLR.org, 2018. URL http://dblp.uni-trier.de/db/conf/
icml/icml2018.html#LiaoC18a.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral Normalization
for Generative Adversarial Networks, 2018. arXiv:1802.05957v1.
Mor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Nathan Srebro, and Daniel Soudry. Convergence
of gradient descent on separable data. CoRR, abs/1803.01905, 2018. URL http://dblp.
uni-trier.de/db/journals/corr/corr1803.html#abs-1803-01905.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias:
On the role of implicit regularization in deep learning. CoRR, abs/1412.6614, 2014. URL
http://dblp.uni-trier.de/db/journals/corr/corr1412.html#NeyshaburTS14.
Augustus Odena, Jacob Buckman, Catherine Olsson, Tom B. Brown, Christopher Olah, Colin Raffel,
and Ian J. Goodfellow. Is generator conditioning causally related to gan performance? CoRR,
abs/1802.08768, 2018. URL http://dblp.uni-trier.de/db/journals/corr/corr1802.
html#abs-1802-08768.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. In NIPS-W, 2017.
Jeffrey Pennington, Samuel S. Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep
learning through dynamical isometry: theory and practice. In Isabelle Guyon, Ulrike von Luxburg,
Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.),
NIPS, pp. 4788-4798, 2017. URL http://dblp.uni-trier.de/db/conf/nips/nips2017.
html#PenningtonSG17.
Guillermo Valle Perez, Chico Q. Camargo, and Ard A. Louis. Deep learning generalizes because the
parameter-function map is biased towards simple functions. CoRR, abs/1805.08522, 2018. URL
http://dblp.uni-trier.de/db/journals/corr/corr1805.html#abs-1805-08522.
10
Under review as a conference paper at ICLR 2019
Tomaso Poggio, Ryan Rifkin, Sayan Mukherjee, and Partha Niyogi. General conditions for predic-
tivity in learning theory. Nature, 428(6981):419-422, 2004.
Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. Svcca: Singular vec-
tor canonical correlation analysis for deep understanding and improvement. arXiv preprint
arXiv:1706.05806, 2017.
Lorenzo Rosasco, Ernesto De Vito, Andrea Caponnetto, Michele Piana, and Alessandro Verri.
Are loss functions all the same?. Neural Computation, 16(5):1063-107, 2004. URL http:
//dblp.uni-trier.de/db/journals/neco/neco16.html#RosascoVCPV04.
Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Learning hierarchical categories in
deep neural networks. In Markus Knauff, Michael Pauen, Natalie Sebanz, and Ipke Wachsmuth
(eds.), CogSci. cognitivesciencesociety.org, 2013a. ISBN 978-0-9768318-9-1. URL http://
dblp.uni-trier.de/db/conf/cogsci/cogsci2013.html#SaxeMG13.
Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear
dynamics of learning in deep linear neural networks. cite arxiv:1312.6120, 2013b. URL http:
//arxiv.org/abs/1312.6120.
Andrew Michael Saxe. Deep Linear Neural Networks: A Theory of Learning in the Brain and Mind.
PhD thesis, Stanford University, 2015.
K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition.
CoRR, abs/1409.1556, 2014.
Daniel Soudry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent on separable
data. CoRR, abs/1710.10345, 2017. URL http://dblp.uni-trier.de/db/journals/corr/
corr1710.html#abs-1710-10345.
Morris Tenenbaum and Harry Pollard. Ordinary Differential Equations : An Elementary Textbook for
Students of Mathematics, Engineering, and the Sciences. Dover, October 1985. ISBN 0486649407.
URL http://www.worldcat.org/isbn/0486649407.
Vladimir Naumovich Vapnik. Statistical Learning Theory. Wiley-Interscience, September 1998.
Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. On orthogonality and learning
recurrent networks with long term dependencies. CoRR, abs/1702.00071, 2017. URL http:
//arxiv.org/abs/1702.00071.
Wiki. Exponential integral, 2018. URL https://en.wikipedia.org/wiki/Exponential_
integral.
Tengyu Xu, Yi Zhou, Kaiyi Ji, and Yingbin Liang. Convergence of sgd in learning relu models
with separable data. CoRR, abs/1806.04339, 2018. URL http://dblp.uni-trier.de/db/
journals/corr/corr1806.html#abs-1806-04339.
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep
neural networks? In Advances in neural information processing systems, pp. 3320-3328, 2014.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. CoRR, abs/1611.03530, 2016. URL http:
//dblp.uni-trier.de/db/journals/corr/corr1611.html#ZhangBHRV16.
11
Under review as a conference paper at ICLR 2019
Appendix A.
A.1 Proof of Lemma 3.1
In this section, we prove the following lemma:
Lemma 3.1 For any k ∈ {1, 2}, x ∈ Dk and t ≥ 0, the only non-negative elements of Wtx are the
ones with an index i ∈ Ik. The signs of the coordinates of Zt remain the same throughout training.
Proof. We prove this with a simple induction. The claim is true at t = 0 from assumptions (H1-3).
Let us assume that at time set t, the different parts of the lemma are true. The SGD updates to the
weights stemming from a single observation x ∈ Dk (the extension to a mini-batch is straightforward)
with a learning rate α are:
∆Zt(x) = αδf (x) (Wtx)+	∆Wt(x) = αδf (x) (ZT 0 ek) xτ,	(8)
where 0 denotes the element-wise product of two vectors and ek ∈ Rh is the binary vector with
ones on the indices from Ik. δf (x) is the gradient of the loss with respect to the pre-sigmoid
output of the network Ft(x): δf (x) = 1{k=1} - σ(Ft(x)). The updates to Zt are positive on its
indices belonging to I1 , and negative otherwise, proving the second claim of the lemma. Moving
to Wt, only its rows and hidden neurons with indices i ∈ Ik are modified. For an element x0 ∈ D,
Wt+1x0 = Wtx0 + αδf (x)(Pi∈I zi)kxk0 where zi is the i-th element of Zt. By induction,
Pi∈I zi has the same sign as δf (x) (positive on D1, negative on D2). From (H1), kxk0 is positive
(resp. negative) if x0 is in Dk (resp. otherwise). The update keeps the k-th neuron active (resp.
inactive).	□
A.2 Extension of Lemma 3.1 to multi-class classification
Let us consider a simple C-classification task. D = {(xi, yi)}1≤i≤n ⊂ Rd × C is our dataset of
vectors/labels where C denotes both the set of possible labels and its cardinal depending on the
context. The classifier we are training is a simple neural network with one hidden layer ofC neurons,
a ReLU non-linearity and a softmax σ. The full function is written as
Pt(X)= σ(Ft(x)) := σ(Zt(Wtx)+),	(9)
where Wt (resp. Zt) is a C × d (resp. C × C) weight matrix andσ the softmax function. The subscript
+ denotes the positive part of a real number. Pt(x) is a C-vector representing the probability that x
belongs to each class. The subscript t denotes the state of the element at time step t of training.
For a given class k, we let Dk be the set of vectors belonging to class k. We make the following
assumptions, with k 6= k0 ∈ C two arbitrary classes
(H4) For any x, x0 ∈ Dk, xTx0 > 0. For any x ∈ Dk and x0 ∈ Dk0 , xTx0 ≤ 0.
(H5) For any x ∈ Dk and x0 ∈ D \ Dk, w0kx > 0 and w0k x0 ≤ 0, where w0k is the k-th row ofW0.
(H6) Z0 is initialized randomly to positive numbers on the diagonal, and non-positive elsewhere.
Those assumptions are straightforward extensions of the ones from the main text. We train the
classifier using stochastic gradient descent on the cross-entropy loss of our problem. Ft+1 is the
state of the neural network after one SGD update to Ft. Our first lemma states that over the course
of training, the active neurons of the hidden layer remain the same for each element of the dataset,
and the elements of Zt remain of the same sign.
Lemma A.1. For any k ∈ C, x ∈ Dk and t ≥ 0, the only non-negative element of Wtx is its k-th
element and all diagonal (resp. non-diagonal) elements of Zt are positive (resp. negative).
Proof. We prove this with a simple induction. The claim is true at t = 0 from assumptions (H4-6).
Let us assume that at time set t, the different parts of the lemma are true. The SGD updates to
the weight matrices stemming from a single observation X ∈ Dk* (the extension to a mini-batch is
straightforward) and a learning rate α are:
δZt = αVyL (WtX)T	δWt = α(ZT RyL 0 ek* )xT,	(10)
12
Under review as a conference paper at ICLR 2019
where 0 denotes the element-wise product of two vectors and ek* the k basis vector of RC. NyL
is the gradient of log Ft(χ)k* (the cross entropy loss for a sample from class k*) with respect to
the output of Zt. One can show that (NyL)k*
e _ eFt(X)k*
-Pj eFt(x)j
and (VyL)i = - PeFff )j for
je
i = k i.e. VyL = ek* 一 Yt(x). The update to Zt is non-negative on the diagonal, and non-positive
elsewhere, which proves the second claim of the lemma. As far as Wt is concerned, only its k*-
th row is modified. For an element x0 ∈ D, Wt+1 x0 = Wtx0 + KxT x0ek* where K is the dot
product between the k*-th column of Zt and VyL. By assumptions on the data, XTχ0 is positive
(resp. negative) if χ0 is in Dk* (resp. otherwise), so the update keeps the k*-th neuron active (resp.
inactive).
□
The proof can be extended to any number of hidden layers h ≥ C by modifying assumptions (H5-6)
using a partition {I1, . . . ,IC} of {1, . . . , h} similar to the binary classification case. Each set Ii in
the partition describes the neurons active at the initialization of the network for an element x ∈ Di .
The modified assumptions are:
(H5’) For any i ∈ Ik, x ∈ Dk and x0 ∈ D \ Dk, w0ix > 0 and w0i x0 ≤ 0.
(H6’) For any i ∈ Ik,j ∈/ Ik, (Z0)ki > 0 and (Z0)kj ≤ 0.
The lemma then translates to those inequalities remaining true at any time t ≥ 0.
A.3 Proofs for Theorem 3.2
In this section we develop the proof of Theorem 3.2 of the main text.
Proof. We consider an update made using x ∈ D1. Writing yt = wtx, our system follows the system
of ordinary differential equations
xTx zt
1 + eytzt
y
1 + eytzt
(11)
Writing xTx = kxk2, we see that yt yt0 = kxk2ztzt0. The quantity yt2 一 kxk2zt2 is thus an invariant
of the problem. With c := |y02 一 kxk2z02|, the solutions of Eq. 11 live on hyperbolas of equation
(y2	-	kxk2z2	=	C	if y 一	kxk2ζ2 > O,
<	y2	-	kxk2z2	=	-c	if y —	kxk2z2 < O,	(12)
∖	y2	-	kxk2z2	=	0	if Vo -	kxk2zo = 0.
We start by treating the case of a degenerate hyperbola c = O. We have ∀t, yt = kxkzt (those
quantities are both positive as x ∈ D1 and (H2-3)). We let u(t) := ztwtx = ztyt and see by
combining the two equations in Eq. 11 that
0(+∖	2kxku⑴
U (t) = T+eu(t).
(13)
For any uf ≥ uo, let t = u<-1> (uf) (u is a bijection from R+ → [uo, +∞[ since its derivative is
strictly positive). We have
1	uf 1 + ey	1 u
t =2M L 丁dy =丽(g( uo)+ (Uf )一	(UO)),	(14)
where Ei(X) = 一 f∞ e-udu is the exponential integral. In the end, with the superscript <-l>
denoting the inverse function
Uf = U(t) = (log+Ei)<-1>(2kXkt + log(Uo) + Ei(Uo)) .
We let U denote the function u(t) above for ∣∣xk = 1, the solution for ∣∣xk = 1 can easily be inferred
by a rescaling of t in U. For X ∈ D?, the system of ODES is
—
kxk2 Zt
1 + e-ytzt ,
Vt
1 + e-yt zt
(15)
13
Under review as a conference paper at ICLR 2019
the degeneracy assumption becomes y0 = -kxkz0 (we know from (H2) that y0 > 0 and from
(H3) that	z0	<	0).	It can be shown similarly that	v(t) :=	ztyt	verifies the equation	v0(t)	=
2kxkv(t)σ(v(t)) with a negative initial condition. In other words, u and v follow symmetric
trajectories on the Positive/negative real line. Below, U and V denote those two trajectories for
kxk = 1 and initial conditions u0 > 0 and v0 < 0.
Let us now write p1 = |D1 |/|D| the fraction of Points in the dataset belonging to D1 . Because we
samPle randomly from the dataset, this amounts to samPling p1 (resP. 1 - p1) Points from D1 (resP.
D2) for each time unit during training, ie to rescaling the time axis by p1 for D1 and 1 - p1 for D2.
Formally, this allows us to quantify the Performance of the network at any time t
(Pt(X ∈ Di) = σ(u(kx∣∣pιt))
I Pt(X ∈ Dz)= σ(-v(Ilxk(I-pι)t))
ifx ∈ D1,
ifX ∈ D2,
(16)
which concludes the Proof for c = 0.
From Eq. 14, we also see that
t
1 uf 1 + ey
2kxk Uo0	y y 2
uf
u0
e 2 dy
u0、
-e ɪ),
(17)
1
丽
1	, Uf
而e
y
where we used the inequality ∀y ≥ 0, e2 > y. It eventually gives Us
u(t) ≤ 2log(kx∣∣t + eU0),
(18)
a result in line with convergence rates obtained in Soudry et al. (2017).
Let us now study the non-degenerate case. We aPPly the classic change of coordinates
厂hΛ yt = √c cosh(2),	Zt =	=√∣ 一画	smh(2)	if y02 > IxI2Z02,	(19)
yt = √c sinh( 2),	Zt =	=√c kxk	cosh(2)	if y02 < IxI2Z02 .	(20)
Since yt2 + kXk2Zt2 = c cosh(θ) and ytZt	c -	 2kxk	sinh(θ), we see that		
0	ν	y2 + Ilxk2 z2 (ytZt)	=	- + eytzt	=	c E	cosh(θ)θ0 =	c cosh(θ) -1 + ecsinh(θ)∕2kxk ,	(21)
where the first equality used the system of equations Eq. 11. This gives us the dynamics of θ as
2∣xk
1 + ecsinh(θ)∕2kxk ,
with an initial condition θo = cosh-1 (ycι + kχk Z ). For any θf ≥ θo, we see that t = θ<-1>(θf)
verifies
θf 1 + ec sinh(θ)∕2kxk
t =	-ɪ——---------------dθ.
Jθo	2kxk
There is no closed-form solution for that integral (that we know of). It can however be comPuted
numerically. On Fig. 2 Right of the main text, we Plot the curves for yt and σ(ztyt) for different
values ofc and kXk. We obtain a sigmoidal shaPe similar to Previously made emPirical observations.
We notice in particular that for larger values of ∣∣χ∣ the function converges faster.	□
A.4 Extension to h hidden neurons
We now extend the result from Theorem 3.2 to the case with h hidden neurons. Let us still consider
an update made on X ∈ D1 . We know from assumptions and by Lemma 3.1 that the only active
neurons in the network are indexed by I1 . The network weights follow the evolution equations:
(wti)0
xT Zi
1 + ePj∈Il zt WtX
(zti)0
i
wtiX
14
Under review as a conference paper at ICLR 2019
We similarly define yti = wtix which brings
Ti	i
(yi)0 = _P	Zjj,	(zi)0 = p yt j j.
1 + e j∈I1 zt yt	e j∈I1 zt yt
The couple (yti, zti) follows the same hyperbolic invariance, defined by a constant ci. In the case
where ∀i ∈ I1, ci = 0, we see that uit := ztiyti verifies
(Uty=	2kp∣ Utj
1 + e j∈I1 ut
and a simple summation on i shows that Ut := Pi∈I Uit follows Eq. 13. The dynamics of the logit
in this case are identical to the single active neuron case, the only difference is potentially its initial
value.
A.5 Proof of Corollary 3.3
Corollary 3.3 Let δ be the required accuracy on the classification task (i.e. Pt(x ∈ D1) ≥ 1 - δ
for X ∈ Di). Under certain assumptions, the times t； and t2 required to reach that accuracy for each
classifier verify t⅜ ≈ kχ1∣ Y-P where ∣∣Xk ∣∣ is the norm of vector ∣∣Xkk from class k.
Proof. We consider the case c = 0. The classification error drops at a rate proportional to kx1kp
for D1 and to ∣x2∣(1 - p) for D2. More precisely, let us assume that U0 ≤ |v0| and look for a
classification confidence of 1 一 δ. We write Uf = σ<-1>(1 一 δ) = log(1∕δ — 1), tv = U<-1>(∣vo∣)
and
t* = U<τ>(uf) = 1(log(lθg(1∕δ- 1)) + Ei(log(1∕δ - 1)) 一 Ei(uo)),
2	U0
t； represents the time taken to reach confidence δ with an initialization U0 , and tv the time to reach
v0 starting in U0 . We see that
t；
P(X ∈ Di) ≥ 1 一 δ ^⇒ t ≥ tγ = -——	if x ∈ Di,
1	∣x1∣p
t； 一 t
P(X ∈ D2) ≥ 1 一 δ ^⇒ t ≥ t2 = η—--------------ʌ	if X ∈ D2 .
2	∣X2 ∣(1 一 p)
The ratio between the convergence times reads M = kx1k ɪ-p (1 - *). One sees that if the weight
initializations U0 and v0 are close (i.e. tv is small) and the confidence requirement large (i.e. t； is
large), the ratio is approximately k|11 I-P.	□
A.6 Top-left quadrant initialization
When the initial conditions of the network verify yo = 一||/曝0, then at all time t, yt = 一||/||々.
Plugging that equality in Eq. 11 results in
u0(t) = 一2限，(?
' '	1 + eu(t
where again U(t) = ztyt. This means that the logit is negative and increasing. Let U0 < Uf < 0, we
see that the time at which U reaches Uf verifies
t
—
1	uf 1 + ey	1	-u0 1 + e-y
丽 Jo0 丁dy =丽 Lf -dd ≥ logi0) 一 logif) .	(22)
t diverges to +∞ as Uf tends to 0 from below. The logit converges to 0 without ever reaching it.
15
Under review as a conference paper at ICLR 2019
A.7 Relaxing the single datapoint assumption
One of the major assumptions made in the main text is the fact that each class contains a single
element. In this section, we slightly relax it to the case where the points ({xi}1≤i≤m ⊂ Rd) in
a class are all orthogonal to one another1 (while still verifying Assumption (H1)). In that case,
each presentation of a training vector will only affect wt in the direction of that specific vector. Let
yti denote wtxi, the unnormalized component of wt along xi . We consider a batch update on the
weights of the neural network. In that case:
i iY = kxik2 Zt
(yt) = 1 + ezty ,
(Zt)0
m
X
=1
yi
1 + eztyi
Assuming that the vectors all have the same norm (denoted kxk below) and that the y0i are all equal,
then that equality remains true at all time (they follow the same update equation). We let yt denote
that value:
kxk2 Zt	m
Wt) 1 + eztyt ,	( t 1 + eztyt .
A new invariant appears in those equations: c := |my02 - kxk2Z02|. In the case c = 0 (the other case
can be treated as above), we obtain the following evolution equation for the logit of any point in the
class:
u0(t) = 2 √mkxku⑴
We end UP with a similar equation than before except for the √m factor, which boosts the convergence
speed. However, one should not forget that we are now training on a full batch (i.e. on m points)
during each unit of time. Performing the same number of updates for the single point class would
generate a m factor in the convergence speed of U (one √m factor for each y and Z functions). The
slower convergence for the more general case can be explained by the fact that each point is making
an update on wt in its own direction. That direction being orthogonal to all others points makes it
useless for their classification.
A.8 Relaxing assumption (H2)
Let us first recall that the assumption states:
(H2) For any i ∈ Ik, x ∈ Dk and x0 ∈/ Dk, w0i x > 0 and w0i x0 ≤ 0.
We now assume that h = 2 and study the evolution of the first row wt1 of matrix Wt , written wt
in the following. We relax assumption (H2) by assuming that there is a point x2 in D2 such that
w0x > 0. And we consider updates to wt coming from sampling equally x1 from D1 and x2 from
D2 . The evolution equations can be written as
TT
x1 Zt	x2 Zt
-------------	
1 + eztwtx1-1 + e-ztwtx2
wt x1	wt x2
-------------------------
1 + eztwtx1	1 + e-ztwtx2
In order to make the analysis simpler, we assume that x1T x2 = 0. We write αt = wtx1 and
βt = wtx2 . Any component of w0 orthogonal to both x1 and x2 will be untouched by the updates,
and does not affect the classification performance of the network. This gives us
α =	,
t 1 + eztαt
kx2k2zt
1 + e-ztβt
αt	βt
-----------------------T-
1 + eztαt	1 + e-ztβt
(23)
By assumption, we know that α0 , β0 > 0. The system of ODEs (23) is invariant through the
transformation (αt, βt, Zt) → (βt, αt, -Zt) so it is sufficient to study the case Z0 ≥ 0. Let us now
state the theorem from the main text.
Theorem 3.4. Letting C := α2∕kxιk2 + β2∕∣∣x2∣∣2 — z2 ,the solutions of (23) verify for all t ≥ 0,
α”kx1k2+ e2/kx2k2 -z2 = c. In other terms, they live on hyperboloids (see Fig. 3 from the main
text and Fig. 6).
1This implies in particular m ≤ d.
16
Under review as a conference paper at ICLR 2019
1.6
1.4
1.2
1.0
0.8
0.6
0.4
0.2
0.0
00 °2 °4 0.6 08
a 101∙2	ι,6
2.0
1.8
1.6
1.2 1,4Z
1.0
0.8
Figure 6: Solutions of the ODE system for different initializations and c = -1. Trajectories live
on a hyperboloid of two sheets. Any initialization on that surface will result in βt reaching 0, or
in other terms in class D1 prevailing. This curve and Fig. 3 from the main text are plotted with
kx1 k = kx2 k = 1.
If c ≤ 0, βt reaches 0 at some point during training (see Fig. 6). If c > 0, there exists a curve Cc on
the hyperboloid such that as t → +∞, for any initialization (α0, β0, z0) ∈ Cc:
αt → (kxik2 + kx2k2 厂“2√c，	βt → (kxik2 + kx^2 厂“2√c，	Zt →0 ∙
That curve defines two regions of the initialization space. In one (colored yellow on Fig. 3 Left),
trajectories verify αt = 0 for some t, in the other (colored green) βt reaches 0 at some point∙
Proof It is easy to see from the system of ODEs (23) that (a2)0∕Hx1k2 + (β2)0∕∣∣x2∣∣2 - (Z) = 0,
which directly gives the invariance of a2∕∣∣xι ∣∣2 + β2∕∣∣x21∣2 - z2. ThiS implies that the trajectories
(αt , βt , Zt ) live on hyperboloids.
If c < 0, the hyperboloid has two sheets (see Fig. 6). In particular, the trajectories verify: zt2 =
αt2∕∣x1∣2 + βt2∕∣x2∣2 - c ≥ -c, which means that zt is bounded away from 0. As long as βt ≥ 0,
We have β0 = - 1嵋-2言 ≤ kχ2k2c. This implies that βt will reach 0 in a finite time since it
decreases at a rate larger than a strictly positive number. At that point, the ReLU ensures that βt
does not evolve anymore, and that βt ’s contribution to zt disappears. The evolution equations turn
into the ones studied in the previous paragraphs, plotted as the green hyperbola in the plane β = 0
in Fig. 6.
If c = 0, we know that zt2 = αt2∕∣x1∣2 + βt2∕∣x2 ∣2 ≥ αt2∕∣x1∣2 ≥ α02∕∣x1∣2 (αt increases as
long as zt is positive), so the same argument about βt holds.
If c > 0,let US first note that the point ((^1p +	)-1/2 √C, (^IP +	)-1/2 √C, 0) belongs to
the hyperboloid and is stationary (the three derivatives are 0). Classic results on ordinary differential
eqUations (TenenbaUm & Pollard, 1985) then give the resUlt. Finding a closed-form solUtion to the
shape of Cc is to the best of oUr knowledge impossible. One can however obtain an approximation
by considering a point ((+	)-"√c - e, (^IF +	)-"2√c + e, 0) for a small e
and applying finite difference methods to the evolution equations (23) to build the trajectory. □
Appendix B:	Deeper Neural Networks
In this section we study the case of a deeper network with N - 1 hidden layers. Similarly to above, we
can prove the existence of independent modes of learning. To that end, neurons need to be activated
in a disjoint manner from one class to the other. Due to the growing complexity of the interactions
between the parameters of the network, this requires very strong assumptions on the initialization of
17
Under review as a conference paper at ICLR 2019
the network and on the shape of the network. Assuming that the network is written as
Pt (x) := σ(Ztr (ZN-2 …(ZI(Wtx)+)+ ...)+),
with Wt an h × d matrix and for all 1 ≤ i ≤ N - 2, Zti an h × h matrix. We maintain assumption
(H2) from the main text, and extend (H3) to all Zti by assuming that they are diagonal, and that the
j-th element of their diagonal is positive if j ∈ I1 , negative otherwise. To simplify notations, we go
back to assuming h = 2 and take an update on x ∈ D1 . Only the first elements of every matrix are
modified, we write them zti and keep the notations zt , wt and yt . We can then write the evolution
equations:
0 =	ZN	2 …Ztyt	i 0Y =	ZtZN	2 …zt+1zt	1 …yt	0 =	kxk2ZtzN	2 …Zt
Zt =	1 + eu(t)	,	(Zt) =	1 + eu(t)	,	yt =	1 + eu(t)	,
With u(t) = Zt Πz00 yt. Assuming that z° = Zt = ... = ZN-1 = 循,We see that those equals
remain true throughout training. This gives us
Zt =( 1+ e〃(tJ，	u(t) = (Zt)Nkxk，	u0(t) = N(Zt)N 1Ztkxk .
Combining those equations gives us the ODE verified by the logit of our system
N kxk2∕N u2-2∕N (t)
(t) =	1 + e”(t)
(24)
The solution of that equation for N = 4 and N = 8 can be found on Fig. 7. Here too, a sigmoidal
shape appears during the learning process. The effect of kxk reduces as N groWs due to the poWer
2/N, hoWever, larger values still converge faster (e.g. the blue and yelloW curves). Additionally, as
noted in Saxe et al. (2013b) for linear netWorks: the deeper the network, the faster the learning. This
fact is studied in more details in Arora et al. (2018) Where depth is shoWn to accelerate convergence
in some cases.
Figure 7: Logit u(t) and confidence Pt for different number of layers and values of kxk.
II×II=O∙1
ll×ll=0∙5
ll×ll=θ∙i
ll×ll=θ∙i
ll×ll=θ∙5
ll×ll=θ∙i
Appendix C:	Hinge Loss
C.1 Proof of Theorem 4.1
In this section we prove Theorem 4.1 from the main text on the dynamics of learning in the case of
the Hinge loss:
LH(Wt, Zt; x) = max(0,1 - ZT(Wtx)+ ∙ (lχ∈°ι - lχ∈D?)).
Let us consider updates made after observing a point x ∈ D1, the converse can be treated similarly
with a simple change of sign. The system of ordinary differential equations verified by the parameters
of our network is:
yt0 = kxk2 Zt ,
Zt0 = yt .
(25)
18
Under review as a conference paper at ICLR 2019
The same relation between yt and zt appears in those equations than in the cross-entropy case.
Defining c similarly, we have u0(t) = 2kxku(t) in the case where c = 0, leading to u(t) = u0e2kxkt.
When c 6= 0, the same change of variables can be applied and leads to
L θ zθ0 ll ll ʌ
yt = √ccosh(y + kxkt),
Zt =音 Smh(θ20 + kXkt),
c
Ut = -7—ʊ-Sinh(θo + 2kxkt), (26)
2kxk
with θo = cosh-1(y + kχk Z ). Those equations are only valid until Ut reaches 12. At that point
learning stops, the network has converged. If the initialization is such that that condition is already
verified, then the weights will not change as they already solve the task. The learning curves
along with the initialization diagram can be found in Fig. 8. We notice a hard sigmoidal shape,
corresponding to learning stopping when Ut reaches 1.
C.2 General treatment
Let us now consider the general case of a class containing an arbitrary number of points D1 =
{xi}1≤i≤m ⊂ Rd. We consider the case of updates done in full batches (standard gradient descent
in other words). In that case, we see that the network obeys the following dynamics:
m
wt0 = zt	xiT ,
i=1
m
zt0 = wt	xiT .
m
Letting X =	xi denote the sum of all the datapoints in that class, we see that those dynamics boil
down to our previous treatment for a single point. The same cases appear, depending on the value
of c := |(w0X)2 - kXk2z02|. We explicitly treat the c > 0 case. Following the methods above, we
see that: Wt = yt^^ + w⊥ where yt = √ccosh(θ0 + ∣∣X∣∣t) and WT is the component of wo
orthogonal to X (and thus unchanged during training). With zt and Ut defined as above (with X
instead of x), an arbitrary example x is then classified as
P(x ∈ D1)
X T x
Ztyt ∣Xp
+ ZtW⊥x = Ut Xx2 + ZtW⊥x .
∣X∣2
Appendix D:	Gradient Starvation
In this section, we prove a relaxed version of Theorem 5.1 from the main text:
Theorem D.2. Let δ be our confidence requirement on class D1 i.e. the training stops as soon as
∀x ∈ D1, Pt(X ∈ D1) ≥ 1 一 δ, Let t* denote that instant i.e. zt*αt* = log( 1-δ). If βo < 0, the
inequality (9) from the main text is valid, Otherwise, with W0 = (α0X1, β0X2 ) + (X1⊥, X2⊥), we have
Pt* ((0, x2) ∈ DI) ≤
1
1 + e-λ log(1-δ)-zt* (β0-aɑ0)
Proof. We start with the β0 < 0 case. If βt* is negative, the result from the main text clearly holds.
Otherwise, there exists t < t* such that βtj = 0 (βt is increasing). The proof of Theorem 5.1 from
the main text can then directly be applied to [t, t*]. If βo > 0, the inequality on at and β0 holds and
gives βt ≤ βo + (at 一 α0)λ. Plugging it into Pt* ((0, x2) ∈ D1) concludes our proof.	□
In the main text, we assume that β0 一 αα0 < 0 and obtain a bound on the confidence which is
independent from a0 and β0 . Using that bound allows to obtain Fig. 9, but is partly unfair as the
initialization of the network is already favoring the strong feature.
However, we note that under small random initialization Zt* and at* are of the same order of mag-
nitude and (βo 一 αɑ0) is very small compared to log( 1-δ). The additional term in the denominator
thus has a limited effect on the exponential, gradient starvation is still happening (a fact confirmed
2We are considering class {1} here, but the equivalent can be proven for class {-1}.
19
Under review as a conference paper at ICLR 2019
by the experiment on the cats and dogs dataset). In the main text, Fig. 5 plots the upper bound for a
fair initialization α° = βo = 0.1 (in that case, We need to assume that zt* = at*).
Figure 8: Solution of Eq.26.
Figure 9: Upper bound on Pt* ((0, x2) ∈ D1) as
a function of 1 - δ for different values of λ.
Appendix E:	Experimental Details
E.1 Mixture of Gaussian experiment
In this experiment, the data is constructed using eight independent Gaussian distributions around a
unit circle. The variance of each Gaussian is chosen such that all eight modes of the data are separated
by regions of loW data probability, but still contain a reasonable amount of variance. This simple
experiment resembles multi-modal datasets. Although this task might seem simple, in practice many
generative adversarial netWorks fail to capture all the modes. This problem is generally knoWn as
mode collapse.
As shoWn in the main text, using the hinge loss instead of the common binary cross-entropy loss
alleviates the problem significantly. The architectures used for the generator and discriminator both
consist of four hidden layers Where each layer has 256 hidden units. As a common choice, a ReLU is
used as the non-linearity function for hidden units. The length of the noise input vector is 128. The
Adam optimizer (Kingma & Ba, 2014) Was applied during training With α = 10-4, β1 = 0.5 and
β2 = 0.9. The PyTorch frameWork (Paszke et al., 2017) Was used to conduct the experiment.
E.2 Dogs vs. Cats classification with light effect
For the purpose of highlighting the fact that the most frequent feature starved all the others, We
conducted an experiment on a classification task. We modified the cats and dogs dataset (Kaggle,
2018) by setting the cats images to be lighter than the dogs images. To do so, each pixel in a cat
image is scaled to be betWeen 0 and 127 While each pixel in a dog image is scaled to be betWeen
128 and 255. The dataset consists of 12500 images of each class. The classifier has an architecture
similar to VGG16 (Simonyan & Zisserman, 2014). In order to isolate the effect of the induced bias,
no regularization Was applied. The Adam optimizer Was applied here as Well during training With
α = 10-4, β1 = 0.9 and β2 = 0.99. The PyTorch frameWork Was used to conduct the experiment.
20