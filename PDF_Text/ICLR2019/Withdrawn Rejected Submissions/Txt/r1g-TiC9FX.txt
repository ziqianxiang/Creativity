Under review as a conference paper at ICLR 2019
Neural Collobrative Networks
Anonymous authors
Paper under double-blind review
Ab stract
This paper presents a conceptually general and modularized neural collaborative
network (NCN), which overcomes the limitations of the traditional convolutional
neural networks (CNNs) in several aspects. Firstly, our NCN can directly handle
non-Euclidean data without any pre-processing (e.g., graph normalizations) by
defining a simple yet basic unit named neuron array for feature representation.
Secondly, our NCN is capable of achieving both rotational equivariance and invari-
ance properties via a simple yet powerful neuron collaboration mechanism, which
imposes a “glocal” operation to capture both global and local information among
neuron arrays within each layer. Thirdly, compared to the state-of-the-art networks
that using large CNN kernels, our NCN with considerably fewer parameters can
also achieve their strengths in feature learning by only exploiting highly efficient
1 × 1 convolution operations. Extensive experimental analyses on learning fea-
ture representation, handling novel viewpoints, and handling non-euclidean data
demonstrate that our NCN can not only achieve state-of-the-art performance but
also overcome the limitation of the conventional CNNs. 1.
1	Introduction
Artificial neural networks have been extensively studied and applied over the past three decades,
achieving remarkable accomplishments in artificial intelligence. Among such networks, two types
of neural networks have had a large impact on the research community. The first type is the
multi-layer perceptron (MLP), which first became popular and effective via the development of the
back-propagation training algorithm Rumelhart et al. (1986); Lecun (1987). However, because each
neuron of the hidden layer is assigned with a private weight, the number of parameters inside MLP
increases quite rapidly and easily leads to overfitting. Moreover, MLP has difficulty in representing
the spatial structure of 2D data (e.g., images). The second type is convolutional neural networks
(CNNs) LeCun et al. (1990). Motivated by the biological visual cortex model, CNNs propose to
group adjacent neurons to share identical weights and represent 2D data by capturing the local pattern
(i.e., receptive field) of each neuron.
Although CNNs have been proven to be significantly superior over MLP, they have several drawbacks,
as highlighted by Sabour et al. (2017). Specifically, due to only abstracting information from local
neighborhood pixels, the convolution operation inside each layer of CNNs ignores global information
and fails to achieve rotational equivariance and invariance. Consequently, CNNs can not well handle
image data with general rotational changes. Hence, unlike the human brain that does not depend
on view angle to recognize objects, CNNs require a large amount of training data under various
viewpoints as an important regularization for avoiding misrecognition. Moreover, unlike MLP,
conventional CNNs cannot be directly applied for non-Euclidean data (e.g., graph data), which are
quite common in the area of machine learning.
Attempting to address the aforementioned issues, several new networks Sabour et al. (2017); Hinton
et al. (2018); Wang et al. (2017) have recently been proposed and achieved promising results.
However, all of these methods have not been studied further or deeper to evolve CNNs, i.e., they
still employ additional convolutional layers for feature learning. In contrast, this work focuses on
developing a completely modularized network named neural collaborative network (NCN), which
inherits the strengths of both MLP and CNNs and overcomes their drawbacks by introducing a simple
1The source codes will be released to facilite future researches after the review period for ensuring the
anonymity
1
Under review as a conference paper at ICLR 2019
yet powerful neuron collaborative mechanism. Specifically, our NCN defines a simple yet basic unit
named neuron array, which is a unit of vector in a meta data. As depicted in Fig. 1, a neuron array
may be seen as a pixel of an image Fig. 1(a), a sampling of an audio Fig. 1(b), and a node of a general
graph Fig. 1(c).
Given the input neuron arrays, our proposed NCN enforces
that they work collaboratively to represent each other ac-
cording to our proposed neuron collaborative mechanism,
which defines that the output for each neuron array is a
linear combination of all the neuron arrays within this
layer. Considering the limited compuation and memory
resource, we developed a so-called “glocal” operation to
efficiently obtain the collaborative representation. The
advantages of our proposed NCN over the traditional con-
volutional networks are: i) Rotational equivariance and
invariance properties. In contrast to convolution opera-
tions that use non-rotatable filters, the output of each layer
inside our NCN is rotation-equivariant to the rotated input
by enabling all the operations for the neuron arrays to have
synchronous rotations. Thanks to the inherent equivari-
c dimensions
Neuron Arrays
(a) Image (b) Audio	(C) Graph data
Figure 1: Neuron arrays are presented in
form of orange cylinder in (a) an image, (b)
an audio, and (c) a general graph.
ance property of our NCN, we can further achieve the invariance property via common global average
pooling for classification-based tasks. ii) Abstracting glocal information. Our NCN considers the re-
lationships among neuron arrays as well as their relative location information in a data-driven manner.
In this way, our NCN is capable of capturing global and local (i.e., glocal) structural dependencies.
iii) Supporting non-Euclidean data. Since our defined neuron arrays are basically vectors similar to
MLP, both Euclidean data (e.g., image, video and voice) and non-Euclidean data can be used directly
without pre-processing steps.
The main contributions of this paper are two-fold. Firstly, we propose a conceptually general yet
powerful network, which is capable of generating powerful feature representations with rotational
equivariance and invariance properties. Secondly, to the best of our knowledge, we are the first to
design a completely modularized network, which overcomes the limitation of the existing CNNs
while preserving their strength in feature learning. Extensive experiments demonstrate the superiority
of our proposed NCN over the compared state-of-the-art network architectures (i.e., ResNet101 He
et al. (2016b)) on the ImageNet benchmark Russakovsky et al. (2015), and our NCN can achieve
comparable performance to the state-of-the-art methods on the CIFAR10 and Cora documentation
classification benchmarks. Moreover, comprehensive experimental analyses validate the equivariance
and invariance properties of our NCN.
2	Neural Collaborative Networks
In this section, we first present the neuron collaborative mechanism with detailed descriptions and
comprehensive explanations. Then, we further theoretically prove that our NCN has an inherent
rotational equivariance property and can obtain an invariance property via a simple step. Finally, we
discuss our NCN with related works, and clarify its compatibility with existing CNN techniques or
tricks.
Neuron Collaborative Mechanism. The motivation of this mechanism is to jointly leverage the
collaborative representation among neurons. Specifically, suppose the input data (e.g.image, voice,
graph matrix or their features) is fed into N neuron arrays I = {Ii}iN=1, where Ii is a vector of c
dimensions (see Fig. 1). Inspired by Cai et al. (2016); Zhang et al. (2011), we propose to obtain the
output yi for each neuron array Ii via a linear combination of all the neuron arrays I as follows:
yi = Idi	(1)
where D = {di}iN=1 ∈ RN ×N denotes the collaborative matrix, which consists of relationships
among the input neuron array set I = {Ii}iN=1. For instance, given an image with n pixels, a
“collaborative matrix” denotes an n × n matrix measuring the degree of relevance between any two
pixels. In this work, we propose to calculate D by employing a completely learnable parameter ωD as
follows:
Dij=	f(ωjli);	s.t. ∀ a,b, ωDb	=	ωjif∣a -	b|	=	|i-j|,	(2)
2
Under review as a conference paper at ICLR 2019
where i and j are the indexes that enumerate all items of I. f (∙) denotes a typical mentioned
non-linear mapping. In our experiments, we choose the combination of LayerNorm Ba et al.
(2016)+ReLU Krizhevsky et al. (2012) or BatchNorm Ioffe & Szegedy (2015)+ReLU Krizhevsky
et al. (2012) for their widely application inside CNNs. To achieve rotational equivariance and invari-
ance, we introduce an index-based constraint for the learnable parameter ωD . In fact, irrespective
of how the viewpoint of the input data changes, the relative spatial distance of its items remains
unchanged. Therefore, we consider the distance in indexes as a hard constraint by enforcing that some
neuron arrays with the same spatial distance share the same collaboration parameter. Specifically, for
any four neuron arrays with indexes a, b, i and j , we enforce that their collaboration parameter ωaDb
equals ωiDj when their relative distances are identical. In this work, we directly employ the l1-norm,
i.e., |a - b| = |i - j|, for ease of implementation. Note that, the distance is defined as the spatial
distance between pixels for an image, while the distance denotes the Dijkstra shortest path distance
between nodes for the graph data.
In addition to the implicit collaboration measurement for D, our NCN has also considered the explicit
collaboration measurement as Krahenbuhl & Koltun (2011). Specifically, we additionally introduce
another neuron array set V = {vi}iN=1 with the parameter ωV to calculate a new collaborative matrix
A = {ai}iN=1. For each neuron array Ii, we first perform a simple neuron self-transformation (NST)
as Vi = f (ωvIi), where ωv ∈ Rd×c is the parameter inside NST for transformation and f (∙) implies
aforementioned non-linear mapping. Then, we have:
Aij =	f (ωiAj [-kvi	- vjk2;vi;vj]),	s.t. ∀a,b,	ωaAb	=	ωiAj	if |a -	b|	= |i -j|,	(3)
where the optimization of ωiAj is similar to that of ωiDj . Similar to D, the collaborative representation
y0i for Ii by using ai over the entire I is defined as: y0i = Iai . Actually, the filter learned by our
NCN is radial symmetry (also known as rotational symmetry). Therefore, the output feature map of
these filters is ensured to be rotational invariant. This property distinguishes our NCN with Graph
CNN Kipf & Welling (2017). More importantly, our NCN not only achieves rotational invariance
properties but also enables powerful representation learning. This may imply that rotational-invariant
filters are sufficient to learn strong models on complicated computer vision problems.
Actually, D and A are usually quite large under the practical use. Hence, directly calculating D is
infeasible due to its memory and computation consuming. To address this issue and capture structural
dependencies from both global and local perspectives, we design a “glocal” operation to efficiently
calculate D and A. We also define the upsampling and downsampling operations of neuron arrays, i.e.,
if the stride s does not equal 1, then we will achieve upsampled (s < 1) and downsampled (s > 1)
results. Specifically, if s < 1, then we employ the bilinear interpolation strategy to increase the number
of neuron arrays by generating a new neuron array from two adjacent neuron arrays. If s > 1, then
we directly discard the neuron array for every s - 1. Specifically, we first downsample the input data
using the downsampling operation to extract only few representative neuron arrays (e.g.16 (=4×4)
representative neuron arrays). Then, we calculate their D and A and further directly interpolate them
to the whole neuron arrays via a nearest-neighbor strategy. According to our experimental results,
this step can well capture global information of neuron arrays and greatly improve efficiency. As
a compensation, we impose a local region definition, which focuses on calculating the correlations
among neighborhood neuron arrays to capture local structural information. Specifically, the i and j in
Eq. (1) subject to dis(i, j) < ε, where dis(∙) denotes the shortest path distance between i and j, and
ε is a threshold (e.g.ε = 16). In this way, we can obtain local collaborative representation y0i0 and y0i00.
Finally, our proposed NCN directly fuses these two types of collaborative representations to obtain
the final output zi for Ii as follows:
zi = f (ωz[yi; y0i; y0i0; y0i00]),	(4)
where [•;•;•;•] denotes the concatenate operation and ωz represents the parameters for the fusion.
With this simple yet powerful neuron collaborative mechanism, we can design any types of NCNs.
Rotational Equivariance Property. Our proposed NCN can always achieve rotational equivariance.
We present our clarifications as follows. Suppose that we have input image data with two arbitrary
pixels, i.e., i and j with coordinates (x, y) and (u, q), respectively. After rotating θ degrees, the
new coordinates for i and j are T(i, θ) = (x cos θ - y sin θ, x sinθ + y cos θ) and T(j, θ) =
(u cos θ - qsinθ, u sin θ + q cos θ), respectively. T denotes the rotation function. Meanwhile, we
have the following equation:
Ii = IT (i,θ) ; Ij = IT (j,θ) .	(5)
3
Under review as a conference paper at ICLR 2019
To demonstrate the rotational invariance of our NCN (denoted as the function g(∙)), our objective
is to prove that g(IT (i,θ)) = T(g(Ii), θ), i.e., the output of our NCN for the rotated input should be
identical to rotating the output of the original input by the same angle.
We have the following equations via Eq. (4):
g(IT (i,θ) ) = f (ωz[yT (i,θ) ; y0T (i,θ)]).	(6)
For clarity, we only consider y0T (i,θ). A similar derivation procedure can also be applied to yT (i,θ)
without difficulty. Then, we have
g(IT(i,θ)) = f (ωzT (X, θ)ai); T (g(Ii), θ) =T(f(ωzXai),θ) =f(ωzT(X,θ)aT(i,θ))	(7)
where the derivation is based on the fact that ωz are shared among these neuron arrays irrelevant to i.
Therefore, we only need to prove that ai = aT (i,θ), i.e., Aij = AT (i,θ)T (j,θ) . According to Eq. (3),
we denote Aij as h(kvi - vj k2). In the following, we prove that kvT (i,θ) - vT (j,θ) k2 = kvi - vj k2
detailed in the supplementary material. In this way, we can obtain g(IT (i,θ)) = T (g(Ii), θ), which is
occasionally called rotational equivariance and is quite beneficial for many types of tasks, such as
document classification and semantic image segmentation.
Rotational Invariance Property. For the general classification task, it is desirable to learn a
representation that is invariant to any viewpoint, i.e., rotational invariance. To achieve this goal, we
propose simply performing a widely used yet efficient operation, i.e., global average pooling, on the
rotational equivariance features obtained above. Next we prove this invariance property. Specifically,
our goal is to prove that S (I) = S (T (I, θ)), where the function S (∙) denotes the final feature
representation of our NCN. By incorporating the global average pooling, we have the following:
1N	1N	1N	1 N
S (I) = N ∑g(Ii); S (T (I,θ)) = N ∑g(Iτ (i,θ)) = N ∑T (g(Ii),θ) = N T (∑ g(Ii),θ),
where the derivation is based on the rotational equivariance obtained above. Because PiN=1 g(Ii)
denotes an image with a single pixel, T (PiN=1 g(Ii), θ) = PiN=1 g(Ii). Hence, S(I) = S(T(I, θ)),
which proves that the rotation cannot change the feature representation of our NCN.
Since all of the aforementioned formulations are fully differentiable, we can directly exploit the
standard backpropagation algorithm with stochastic gradient descent (SGD) LeCun et al. (1990) to
optimize all the parameters in the training phase. For the testing, we also directly perform the forward
propagation for obtaining the final output zi for each input Ii . During training, we optimize ωD via
ω j := ωj + lr P∀a,b,∣a-b∣ = ∣i-j∣ OωDb, where IT denotes the learning rate.
Comparison with Non-local Networks (NonlocalNet). NCN advances NonlocalNet in three as-
pects. First, the NonlocalNet used non-local operations among pixels to capture long-range depen-
dencies within feature maps. Differently, our NCN employs a new neural collaboration mechanism
to handle both global and local information of input data. Our proposed mechanism is capable of
explicitly modeling the structural correlation within images. The weak accuracy of NonlocalNet in the
following experiment section shows that NonlocalNet still relies on the convolutional operations for
capturing structural and contextual dependencies, proving that it also suffers from the limitations of
the conventional CNNs. Second, different from NCN, NonlocalNet cannot enable rotation-invariant
representation learning. Last, even the global operation in NCN is different from NonlocalNet. As
described above, our global operation is built upon the patches (using downsampling operation). This
enables NCN to well capture global information and greatly improve efficiency. The experimental
results also verify the superior of NCN over NonlocalNet.
Compatibility with CNN Tricks and Techniques. Our proposed NCN is quite compatible with most
existing tricks and techniques for CNNs. For instance, through embedding a batch normalization Ioffe
& Szegedy (2015) layer into every non-linear mapping function f (∙), our NCN can support a large
learning rate for high learning efficiency. Meanwhile, we can also exploit the residual connection
strategy He et al. (2016b) to create a short-cut inside our NCN. 3
3	Experiments
3.1 Learning Feature Representation (ImageNet)
4
Under review as a conference paper at ICLR 2019
We have compared our NCN with a quite represen-
tative and cutting-edge network, i.e., ResNet101 He
et al. (2016a), on the ImageNet 2012 classification
benchmark Russakovsky et al. (2015), which con-
	Top-1 Acc. (%)	Top-5 Acc. (%)	# Params
	 ResNet101	77.4	93.7	84.45M
NCN	79.2	94.3	67.11M
tains 1.28M images with 1, 000 categories for train-
Table 1: ImageNet val top-1 and top-5 accuracies.
ing. The bottleneck of ResNet101 has three layers, each of which includes convolution, ReLU
and batch normalization (BN) Ioffe & Szegedy (2015), to abstract informative patterns from data.
Similarly, our proposed NCN always consists of “three layers”: two to calculate the collaborative rep-
resentation {Y, Y0, Y0, Y000} and the other one to obtain their fusion Z. Therefore we directly replace
all the residual bottlenecks inside ResNet101 with our method with the same dimensions(i.e.channels
in ResNet) and feature map size, we conduct the comparison under the same single-crop & single-
scale evaluation settings as He et al. (2016a), which is the current standard criterion used by Szegedy
et al. (2015); Dai et al. (2017).
Tab. 1 reports the top-1/top-5 accuracies of our NCN and ResNet101 on the 50k validation images.
As shown in Tab. 1, our NCN surpasses ResNet101 by a clear margin in terms of both the top-1 and
top-5 accuracies. In particular, our NCN performs approximately 2% better than ResNet101 on the
top-1 accuracy (79.2% vs. 77.4%) thanks to our proposed neural collaboration mechanism. Note
that, our reproduced top-1 accuracy (77.4%) for ResNet101 actually equal to the official result (Caffe
implementation: 76.4%; Tensorflow: 77.4%). This result demonstrates that our proposed NCN can
learn more discriminative representations than ResNet101. Meanwhile, Tab. 1 clearly shows that
the model complexity of our NCN is significantly lower (approximately 21% less) than ResNet101
(67.11M vs. 84.45M) even though they both have the same network depths. The reason for this result
is that most of the parameters inside the bottleneck of our NCN are from the NST function, which is
9 times more efficient than the 3×3 convolution inside the bottleneck of ResNet. In summary, these
results demonstrate the effectiveness and efficiency of our proposed NCN.
3.2	Handling Novel Viewpoints
To verify the superiority of our NCN for handling novel viewpoints, we have conducted experiments
on the CIFAR10 Krizhevsky & Hinton (2009) and smallNORB LeCun et al. (2004) benchmarks.
CIFAR10: consists of 50k training images and 10k testing images for 10 classes. According to the
novel viewpoint setting, we employ a limited range of viewpoints for training, and we further test
on a completely different range of viewpoints. As we know all the categories of objects (e.g., truck,
horse, ship, and so forth) in CIFAR10 can be regarded as never upside-down. Intuitively, the images
from both the original training and testing sets of CIFAR10 are all in the “normal” viewpoints, which
are consistent with human perception or experience. To evaluate the rotational equivariance and
invariance capacity of our proposed NCN, we simulate a new test set for the CIFAR10 benchmark by
using the “novel” viewpoints. Specifically, all images are flipped vertically from the original test set.
Implementation Details: The detailed ar-
chitecture of our NCN for this benchmark
is summarized in Tab. 2. Since the inputs
of our NCN are 32 × 32 images with per-
pixel mean subtracted, we first exploit 5
NCN layers with 32×32 neuron arrays
in 16 dimensions to capture its structural
information. Then, we use the above-
mentioned downsampling strategy to re-
	Layers 1-5 (NCN Layer)	Layers 6-10 (NCN Layer)	Layers 11-15 (NCN Layer)	Layer 16 (Softmax)
Number Dimensions	-32 × 32- 16	-16 × 16- 32	8×8 64	- -
Table 2: Detailed architecture of our NCN used on the CIFAR10
benchmark. Note that “Number” denotes the number of neuron ar-
rays within each layer, while “Dimensions” denotes the dimension
of each neuron array.
duce the number of neuron arrays from 32×32 to 16×16 and feed the reduced neuron arrays into
5 NCN layers with 16×16 neuron arrays in 32 dimensions. Similarly, we further downsample the
neuron arrays from 16×16 to 8×8 and feed them into 5 NCN layers with 8×8 in 64 dimensions.
Note that we gradually increase the dimension of the neuron array inside the NCN layer to capture
richer and more abstract information for retaining a discriminative representation. Finally, we build a
softmax layer upon the last NCN layer to complete the final recognition task.
Tab. 8 presents the accuracy comparison of both normal views and novel views for all the
compared methods. In terms of normal viewpoints, it is clear that our NCN consistently out-
performs ResNet (93.6% vs. 92.4%) and obtains superior accuracy over all the competing
5
Under review as a conference paper at ICLR 2019
methods. Importantly, as for novel viewpoints, our NCN achieves an incredible accuracy of
93.6%, which is exactly the same as the accuracy for normal viewpoints (i.e., ‘Acc. Retain’
equals 100% in Tab. 8). However, the performance of ResNet decreases from 92.44% to
43.80% due to the above-mentioned drawback of CNNs. This profound comparison clearly
justifies that our NCN is a fundamental solution to address the issue of novel viewpoints.
Moreover, Tab. 8 also presents the model complexity comparison be-
tween our NCN and ResNet on the CIFAR10 benchmark. As shown
in Tab. 8, by using only 47.8% of the parameters of ResNet, our NCN
performs approximately 1.2% better than ResNet on the normal view-
points and 53.1% better than ResNet on the novel viewpoints.This
result further demonstrates the effectiveness and lightweight prop-
erty of our proposed NCN.
SmallNORB: Actually, learning viewpoint invariant knowledge is
crucial in visual representation, and this issue was also addressed
by the recently proposed Capsules Sabour et al. (2017); Hinton et al.
(2018). Next we compare with our method with Capsules to evaluate
NCN’s ability on learning viewpoint invariant knowledge. We use
the same benchmark of handling 3D viewpoint change (smallNORB
LeCun et al. (2004)) as Capsules did. All methods are trained on
one-third of the training data containing azimuth viewpoints of (300,
320, 340, 0, 20, 40) and tested on the two-thirds of the test data that
contained azimuth viewpoints from 60 to 280. Note that there’s no
azimuth viewpoint overlap between training and testing. Results in
Table 4 show that compared with the baseline CNN and Capsules,
our NCN reduce the test error rate on novel viewpoints by about 45%
and 18.5%, respectively, verifying the effectiveness of our NCN in
handling novel viewpoints.
Method	Novel	Normal
MaxoUt	-	90.62%
MIN	-	91.81%
DSN	-	91.78%
Highway	-	91.20%
DynamiC-CapsUle	41.11%	^^89.4%
MatriX-CapsUle	-	88.1%
ResNet-20	42.8%	91.8%
ResNet-32	40.5%	92.4%
ResNet-182	42.3%	94.4%
NCN-32	93.6%	93.6%
NCN-182	95.0%	95.0%
Table 3: Comparisons of the ex-
panded CIFAR10 accuracy between
ResNet and NCN on both “normal”
and “novel” viewpoints, where nor-
mal indicates that the model is eval-
uated on the original test set, while
novel indicates that the model is eval-
uated on our simulated test set. ‘Acc.
Retain’ denotes the rate of novel and
normal view accuracy. (a) CIFAR10
accuracy: the results are from OFFI-
CIAL reports are in blue.
3.3	Evaluations on Non-Euclidean Data (Cora)
A common form of
graph-structured data is
a network of docu-
ments. For example,
scientific documents in
Networks	CNN	Dynamic-Capsule	Matrix-Capsule	NCN
Novel View Test	Novel = 20% Normal = 3.7%	-Novel = 20%- Normal = 3.7%	Novel = 13.5%[15], Normal = 3.7%[15]	Novel = 11.0% Normal = 3.7%
Normal View Test	2.56%	2.7%	1.4%[15]	1.38%
a database are related to Table 4: Comparisons among CNN, Capsules and NCN on smallNORB.
each other through cita-
tions and references. Each document is a vertex in the graph with certain features, and a citation is an
edge from one vertex to the other. Administrators of such large networks may desire to automatically
label documents according to their relationships to the remainder of the literature. To demonstrate
the compatibility of our NCN for non-Euclidean data, we adapt our proposed NCN to tackle such
a vertex classification task on the Cora benchmark Sen et al. (2008), which is a large network of
scientific publications connected through citations.
The vertex features in this case are binary word vectors that indicate the
presence of a word from a dictionary of 1,433 unique words. There are 2708
publications classified under 7 different categories - case based, genetic
algorithms, neural networks, probabilistic methods, reinforcement learning,
rule learning and theory. There is an edge connection from a cited article to
a citing article and another edge connection from a citing article to a cited
article. These edge features are also binary representations. We use a quite
simple architecture: three NCN layers with outputs of 1,000 dimensions,
followed by a classifier layer with outputs of7 dimensions. The last classifier
layer computes the prediction of each vertex. As for the Cora dataset is
quite small, we perform 10-fold cross validations to form the training and
test set for a fair comparison as the majority of methods Belkin et al. (2006);
Kipf & Welling (2017).
Method	Accuracy (%)
ManiReg	59.5	=
SemiEmb	59.0
LP	68.0
DeepWalk	67.2
ICA	75.1
Planetoid*	75.7
Graph-CNN	81.5
MoNet	81.7
GAT	83.0
NCN	81.9
Table 5: Comparison with
state-of-the-art on Cora doc-
ument classification.
6
Under review as a conference paper at ICLR 2019
Comparisons with State-of-the-art Methods. In Tab. 5, we present the
comparison with the current state-of-the-art approaches. It is clear that our NCN (81.9%) achieves
comparable performance to the best of all competitive methods, e.g., Graph-CNN Kipf & Welling
(2017) (81.5%). This comparison once again verifies the effectiveness of our NCN.
Neuron Collaboration vs. Non-
Collaboration. We first evaluate
the effectiveness of neural collab-
oration by constructing two differ-	-
ent networks, i. e.our NCN and neu-	E
ral non-collaborative net. The non-	j=
collaborative net is obtained by di-
rectly replacing each NCN layer with
a simple non-linear NST.	00
Figure 2:	Testing ErrorEoponchsthe Cora benchmark.
As shown in Fig. 2, our NCN consis-
tently reaches substantially lower testing errors than the compared non-collaborative net during the
entire training phase. Finally, our NCN obtains approximately 8% lower testing error rate than our
NCN (26.9% vs. 18.1%). This difference demonstrates that the learning ability of our NCN is much
better than that of neural non-collaborative net. Actually, neural non-collaborative net only performs
NST for each neuron array, i.e., message communication between any two neurons is blocked within
the same layer. This leads to neglecting some critical information (e.g., citation information in
this task). In summary, our NCN fully exploits all the information among neurons, and performs
substantially better than neural non-collaborative net. This comparison verifies the contribution of
our neural collaboration mechanism.
Glocal vs. Non-local. In the previous experiments, we proved
that the collaborative representation among neurons is beneficial
for training. Now, we concentrate on investigating the structure
manner of collaboration. In Tab. 6, we compare two operations:
(A) Glocal operation. Structure messages (i.e., graph adjacency
matrix) are explicitly incorporated in the collaborative represen-
tation, and (B) Non-local operation. No structure information is
considered in our neural collaboration mechanism. This compar-
ison is consistent with Section 2.
Method	TestAcc. (%)	Train Acc. (%)
-Giocai-	819	99.9999
Non-local	75.6	99.9982
Table 6: Analysis of glocal operation on
Cora document classification.
Tab. 6 shows that glocal operation inherent in NCN is considerably better than the non-local operation
(81.9% vs. 75.6%). We argue that this result is because the structure of the graph has been isolated in
non-local operation. In particular, if we randomly shuffle the nodes in the graph, there will be no
impact on the accuracy of non-local operation, which contradicts the fact that swapping nodes of a
graph generally changes the property of a graph. Isolating the graph structure leads to suboptimal
performance, which suggests the reasonableness of our used glocal operation.
3.4	Discovering Neuron Relationships
After training, we exhaustively forward-propagate all images of the CIFAR10 dataset to obtain their
collaborative matrices of the neuron arrays, which are from the last NCN layer. Two motivating
examples from the normal and novel views are depicted in Fig. 3. As shown, there are 8×8
collaborative matrices for 8×8 neuron arrays. Each collaborative matrix is also 8×8, which implies
the relationship of its corresponding neuron array and all the 8×8 neuron arrays (including itself).
For instance, the patch in the gray rectangle can be directly obtained by a weighted combination of
all image patches with their correlations to that patch as weights. As shown in Fig. 3(a), it is clear
that those neuron arrays with high correlations to the 18-th neuron array are exactly corresponding
to the patches that belong to the horse category. As for Fig. 3(b), its 8×8 collaborative matrices are
exactly the same as that of Fig. 3(a) after rotating by 180 degrees. This result further justifies the
rotational equivariance property of our proposed NCN.
3.5	Comparison with Non-local Neural Networks
7
Under review as a conference paper at ICLR 2019
The 18-th
The 46-th
^ c c …“,Correlation Matrix
Correlation Matrix
.-— 4 ,∣ J -	The corresponding
........	image resigon
8x8 Correlation Matries
for Neuron Arrays
(a)
FE≠.⅛√∙ .1 5
Frr . «4ll
-≡ - ZLJ ∙
T■ / r 肖.Y
W ∙ * ⅛ 二
SalSHraB
8x8 Correlation Matries
for Neuron Arrays
(b)
The corresponding
image resigon


Figure 3:	Analysis on correlations of the neuron arrays, which are from the last NCN layer.
We verify the superior of NCN over non-local neural
networks (NonlocalNet) in three different aspects. First,
we show whether NonlocalNet can handle viewpoint
changes. Specifically, we insert NonlocalNet into every
convolutional layer of ResNet (whose architecture is con-
sistent with Tab. 2), denoted as ResNet+NonlocalNet.
Table 7(a) is a comparison on handling the new ob-
ject view on CIFAR10. The above results illustrate
that the performance gain by ResNet+NonlocalNet is
marginal and thus NonlocalNet cannot handle the view-
point changes.
In another experiment of standard image classification,
we show whether NonlocalNet can be modularized in
representation learning. Specifically, we replace the con-
volutional operations in ResNet by our NST (neuron self-
transformation operation), denoted as NonlocalNet+NST.
(a) Handing Viewpoint Change		
Networks	I ResNet ∣ ResNet+NonlocalNet ∣	I NCN
Acc.	I 40.5% I	40.9%	I	I 93.6%
(b) Standard Image Classification		
Networks	I ResNet ∣ ResNet+NonlocalNet ∣	I NCN
Acc.	I 92.4% I	74.2%	∣	I 93.6%
(c) Efficiency Analysis
Networks	ResNet	NCN
Training Time	13.7	20.6
Testing Time	4.9	6.9
Memory (GB)	8.5	11
MACC (GFLOPS)	7.6	11.2
Table 7: Analysis on CIFAR10.
The results on CIFAR10 are reported in Table 7(b). The weak accuracy of NonlocalNet+NST shows
that NonlocalNet still relies on the convolutional operations for capturing structural and contextual
dependencies, although it can handle global information of input data. In contrast, our NCN can
model both local and global structural information in an explicit way, achieving the best performance.
3.6 Efficiency Analysis
We use a desktop with 8 Titan Xp GPUs to perform the training and testing. The time costs in
millisecond per image per GPU are reported in Table 7(c). As shown, CNN is 33% and 42% faster
than our NCN in the training and testing phase, respectively. The FLOPs comparison of MACC
demonstrates that our NCN can achieve new state-of-the-art performance by requiring only about
47% more operations. Actually, this additional operations is acceptable.
4	Related Work
Evolutions of Convolutional Neural Networks: Although significant progress has been achieved
in the architecture design of CNNs from LeNet LeCun et al. (1998) to more recent deep and powerful
networks (e.g., ResNet He et al. (2016b)), evolving the structure of CNNs to overcome their drawbacks
is also quite crucial and a long-standing problem in machine learning (e.g.Li et al. (2017)). This
issue motivates many researchers to extend CNNs to include rotational equivariance Oyallon &
Mallat (2015); Cohen & Welling (2016); Dieleman et al. (2016); Dai et al. (2017); Worrall et al.
(2017); Henriques & Vedaldi (2017); Cohen et al. (2017); Cohen & Welling (2017); Wang et al.
(2017); Cohen et al. (2018). Specifically, Dai et al. Dai et al. (2017) proposed to enhance the
transformation modeling capability of CNNs by introducing learnable offsets to augment the spatial
sampling locations within the feature map. Worrall et al. Worrall et al. (2017) presented harmonic
networks to obtain rotation-invariant feature maps by returning the maximal response and orientation
of circular harmonic filters. Sabour et al. Sabour et al. (2017) proposed employing a group of neurons
8
Under review as a conference paper at ICLR 2019
named a capsule to represent the instantiation parameters of a specific type of entity, such as an
object and an object part. Building upon the work of Sabour et al. (2017), Hinton et al. Hinton et al.
(2018) further presented a new type of capsule that has a logistic unit to represent the presence of
an entity and a 4×4 pose matrix to represent the pose o f that entity. Motivated by the self-attention
mechanism Vaswani et al. (2017), Wang et al. Wang et al. (2017) incorporated non-local operations
into CNNs as a generic family of building blocks for capturing long-range dependencies. Although
these methods achieved promising results, they still require the use of several convolutional layers for
feature representation. Moreover, several limited attempts Kipf & Welling (2017); Such et al. (2017)
have been made to extend CNNs for handling graph data. For instance, Kipf et al. Kipf & Welling
(2017) presented a layer-wise propagation rule for CNNs to operate directly on graph-structured data.
Such et al. Such et al. (2017) defined filters as polynomials of functions of the graph adjacency matrix
for unstructured graph data. However, these variants of CNNs cannot use a unified framework to
support both signals/images and graphs.
Actually, NCN advances them in four aspects. First, as R2 pointed out, they have “weaker empirical
performance” than NCN. Second, their computational cost is high, e.g.Defferrard et al. (2016) is
7× slower than the compared CNN. Third, a graph normalization is required to pre-process their
input data, while the ambiguity problem of the graph norm remains unsolved. Fourth, the learned
filters of these methods usually depend on a specific graph structure, leading to their limitations of
generalization. Our NCN overcomes these limitations and beyond, thanks to the proposed neuron
collaboration mechanism with the glocal operation.
Collaborative Representation: The collaborative representation mechanism corresponds to repre-
senting a test sample as a sparse linear combination of all training samples, and it has successfully
been applied in sparse-representation-based methods Wright et al. (2009); Zhang et al. (2011); Cai
et al. (2016) for visual recognition and in non-local techniques Buades et al. (2005); Protter et al.
(2009) for image restoration. For instance, Zhang et al. Zhang et al. (2011) proposed an effective
collaborative-representation-based classifier by utilizing the l2-norm regularizer for the collaborative
representation of a test sample from the training samples across all classes. Considering an image
patch as a sample, Buades et al. Buades et al. (2005) proposed reconstructing a given local patch
for image restoration via the collaborative representation of non-local similar patches, which are
similar to this local patch and collected throughout the entire image. Inspired by these methods, we
propose modeling the collaborative representation of neuron arrays to replace the conventional convo-
lutional filtering. In contrast to these method that their defined collaborative mechanism ignore the
local structural information, our proposed NCN consider the global and local structural information,
simultaneously.
5	Conclusion
This paper presented a concise NCN to be a promissing substitute for overcoming the limitations
of widely used deep CNNs without losing their strengths in feature learning. Specifically, our NCN
advances in naturally considering the relationships among neurons to obtain non-local representations
with equivariance and invariance properties. We further applied our proposed NCN for the recognition
tasks of both Euclidean data and non-Euclidean data. Extensive experimental analyses from a variety
of aspects justify the superiority of our NCN. In the future, we will extend our work to be suitable for
more general tasks to demonstrate its superiority.
References
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. In arXiv:1607.06450 [stat.ML],
2016.
Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A geometric framework for
learning from labeled and unlabeled examples. Journal ofmaChine learning research, 7(Nov):2399-2434,
2006.
A. Buades, B. Coll, and J.-M. Morel. A non-local algorithm for image denoising. In CVPR, 2005.
Sijia Cai, Lei Zhang, Wangmeng Zuo, and Xiangchu Feng. A probabilistic collaborative representation based
approach for pattern classification. In CVPR, 2016.
9
Under review as a conference paper at ICLR 2019
Taco S. Cohen and Max Welling. Steerable cnns. In ICLR, 2017.
TaCo S. Cohen, Mario Geiger, Jonas Kohler, and Max Welling. Convolutional networks for spherical signals. In
arXiv:1709.04893 [cs.LG], 2017.
Taco S. Cohen, Mario Geiger, Jonas Kohler, and Max Welling. Spherical cnns. In ICLR, 2018.
T.S. Cohen and M. Welling. Group equivariant convolutional networks. In ICML, 2016.
Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable convolu-
tional networks. In ICCV, 2017.
Michael Defferrard, Xavier Bresson, and Pierre VandergheynSt Convolutional neural networks on graphs with
fast localized spectral filtering. In NIPS, 2016.
Sander Dieleman, Jeffrey De Fauw, and Koray Kavukcuoglu. Exploiting cyclic symmetry in convolutional
neural networks. In ICML, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
CVPR, 2016b.
Joao F. Henriques and Andrea Vedaldi. Warped convolutions: Efficient invariance to spatial transformations. In
ICML, 2017.
Geoffrey E Hinton, Sara Sabour, and Nicholas Frosst. Matrix capsules with em routing. In ICLR, 2018.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. In International conference on machine learning, pp. 448-456, 2015.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. ICLR,
2017.
P. Krahenbuhl and V. Koltun. Efficient inference in fully connected crfs with gaussian edge potentials. In NIPS,
2011.
A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks.
In NIPS, pp. 1097-1105, 2012.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.
Proceedings of the IEEE, 11(6):2278-2324, 1998.
Yann Lecun. PhD thesis: Modeles connexionnistes de l’apprentissage (connectionist learning models). Universite
P. et M. Curie (Paris 6), 6 1987.
Yann LeCun, Bernhard E Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne E Hubbard, and
Lawrence D Jackel. Handwritten digit recognition with a back-propagation network. In Advances in neural
information processing systems, pp. 396-404, 1990.
Yann LeCun, Fu Jie Huang, and Leon Bottou. Learning methods for generic object recognition with invariance
to pose and lighting. In Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the
2004 IEEE Computer Society Conference on, volume 2, pp. II-104. IEEE, 2004.
Xilai Li, Tianfu Wu, Xi Song, and Hamid Krim. Aognets: Deep and-or grammar networks for visual recognition.
arXiv preprint arXiv:1711.05847, 2017.
Edouard Oyallon and St6phane Mallat. Deep roto-translation scattering for object classification. In CVPR, 2015.
M. Protter, M. Elad, H. Takeda, and P. Milanfar. Generalizing the nonlocal-means to super-resolution reconstruc-
tion. IEEE Trans. Image Processing, 18(1):36-51, 2009.
D. Rumelhart, G. Hinton, and R. Williams. Learning internal representations by backpropagating errors. Parallel
distributed processing: Explorations in the microstructure of cognition, 1986.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej
Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge.
International Journal of Computer Vision, 115(3):211-252, 2015.
10
Under review as a conference paper at ICLR 2019
Sara Sabour, Nicholas Frosst, and Geoffrey Hinton. Dynamic routing between capsules. In NIPS, 2017.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective
classification in network data. AI magazine, 29(3):93, 2008.
FeliPe Petroski Such, Shagan Sah, Miguel Dominguez, Suhas Pillai, Chao Zhang, Andrew Michael, Nathan D.
Cahill, and Raymond W. Ptucha. Robust spatial filtering with graph convolutional neural networks. J. Sel.
Topics Signal Processing,11(6):884-896, 2017.
C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deePer with convolutions. In CVPR, PP. 1-9, 2015.
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention
is all you need. In NIPS, 2017.
Xiaolong Wang, Ross Girshick, Abhinav GuPta, and Kaiming He. Non-local neural networks. In
arXiv:1711.07971 [cs.LG], 2017.
Daniel E. Worrall, StePhan J. Garbin, Daniyar Turmukhambetov, and Gabriel J. Brostow. Harmonic networks:
DeeP translation and rotation equivariance. In CVPR, 2017.
J. Wright, A.Y. Yang, A. Ganesh, S.S. Sastry, and Y. Ma. Robust face recognition via sParse rePresentation.
IEEE Trans. Pattern Anal. Machine Intell., 31(2):210-227, 2009.
Lei Zhang, Meng Yang, and Xiangchu Feng. SParse rePresentation or collaborative rePresentation: Which helPs
face recognition? In ICCV, 2011.
6	Appendix
6.1 Rotational Equivariance Property
Our ProPosed NCN can always achieve rotational equivariance. We Present our clarifications
as follows. SuPPose that we have inPut image data with two arbitrary Pixels, i.e., i and j with
coordinates (x, y) and (u, q), resPectively. After rotating θ degrees, the new coordinates for i and j
are T(i, θ) = (x cos θ - y sin θ, x sin θ + y cos θ) and T (j, θ) = (u cos θ - q sin θ, u sin θ + q cos θ),
resPectively. T denotes the rotation function. Meanwhile, we have the following equation:
Ii = IT (i,θ)	(8)
Ij = IT (j,θ) .
To demonstrate the rotational invariance of our NCL (denoted as the function g(∙)), our objective is to
Prove that g(IT (i,θ)) = T (g(Ii), θ), i.e., the outPut of NCL for the rotated inPut should be identical
to rotating the outPut of NCL for the original inPut by the same angle.
We have the following equations via Eq. (4)@PaPer:
g(IT(i,θ)) = f ([yT (i,θ) ; y0T (i,θ)]; ωz)	(9)
For clarity, we only consider y0T (i,θ). A similar derivation Procedure can also be aPPlied to yT (i,θ)
without difficulty. Then, we have
g(Iτ(i,θ)) = f(T(X, θ)ai; ωz)
T (g(Ii),θ) = T (f (Xai； ωz),θ)	(10)
=f(T (X, θ)aτ (i,θ); ωz)
where the third line is derived from the second line based on the fact that ωz are shared among
these neuron arrays irrelevant to i. Therefore, we only need to Prove that ai = aT(i,θ), i.e., Aij =
AT (i,θ)T (j,θ). According to Eq. (3), we denote Aij as h(kvi - vj k2, kpi - pj k2). Now, we first Prove
that kpT(i,θ) -pT(j,θ)k2 equals kpi -pjk2:
kpT (i,θ) - pT (j,θ) k2
= k(x cos θ - y sin θ, x sin θ + y cos θ)
- (u cos θ - qsinθ, usinθ + q cos θ)k2	(11)
= (x-u)2 + (y-q)2
= kpi - pj k .
11
Under review as a conference paper at ICLR 2019
We then prove that kvT (i,θ) - vT (j,θ) k2 equals kvi - vj k2. According to Eq. (8), we have the
following:
kvT (i,θ) - vT (j,θ) k2
= kf(IT(i,θ); ωv) - f (IT (j,θ); ωv)k2	(12)
=kf (Ii；ωv) - f (Ij；ωv)k2
= kvi - vjk2
where the third line is obtained by incorporating Eq. (8) into the second line. In this way, we can
obtain g(IT (i,θ) ) = T (g(Ii), θ), which is occasionally called rotational equivariance and is quite
beneficial for many types of tasks, such as document classification and semantic image segmentation.
6.2	Rotational Invariance Property
For the general classification task, it is desirable to learn a representation that is invariant to any
viewpoint, i.e., rotational invariance. To achieve this goal, we propose simply performing a widely
used yet efficient operation, i.e., global average pooling, on the rotational equivariance features
obtained above. Specifically, our goal is to prove that S (I) = S (T (I, θ)), where the function S (∙)
denotes the final feature representation of our NCN. By incorporating the global average pooling, we
have the following:
1N
S(I ) = N X g(Ii)
i=1
1N
S(T(I,θ)) = N Eg(IT(i,θ))
i=1
1N
=NN X T (g(Ii),θ)
i=1
1N
=N T (X g(Ii),θ)
(13)
where the third line is obtained from the second line based on the rotational equivariance obtained
above. Because PiN=1 g(Ii) denotes animage with a single pixel, T (PiN=1 g(Ii), θ) = PiN=1g(Ii).
Hence, S(I) = S(T(I, θ)), which proves that the rotation cannot change the feature representation
of our NCL.
6.3	More Ablation Study
To ablatively analyze data aug-
mentation, we have augmented the training images with dif- ferent magnitudes of rotations. The above results show that aug-	Rotation Range		-0-	(-45,45)	(-90, 90)	(-135,135)	(-180,180)
	Normal View	CNN NCN	92.4 93.6	-902- 91.0	-86.4- 85.4	841 84.1	813 82.4
	Normal View	CNN NCN	40.5 93.6	-41.8- 91.0	-481- 85.4	600 84.1	817 82.4
menting data may be helpful to CNN for handling the viewpoint change but it usually causes the lower recognition accuracy due to increasing the difficulty of	Table 8: Comparisons of the expanded CIFAR-10 accuracy between CNN and NCN on both “normal” and “novel” viewpoints, where normal indicates that the model is evaluated on the original test set, while novel indicates that the model is evaluated on our simulated test set.						
pattern recognition. Without relying on the data augmentation, our NCN can well solve the novel
view challenge.
12