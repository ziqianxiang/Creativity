Under review as a conference paper at ICLR 2019
Approximation capability of neural networks
on spaces of probability measures and tree-
STRUCTURED DOMAINS
Anonymous authors
Paper under double-blind review
Ab stract
This paper extends the proof of density of neural networks in the space of continu-
ous (or even measurable) functions on Euclidean spaces to functions on compact
sets of probability measures. By doing so the work parallels a more then a decade
old results on mean-map embedding of probability measures in reproducing ker-
nel Hilbert spaces. The work has wide practical consequences for multi-instance
learning, where it theoretically justifies some recently proposed constructions.
The result is then extended to Cartesian products, yielding universal approxima-
tion theorem for tree-structured domains, which naturally occur in data-exchange
formats like JSON, XML, YAML, AVRO, and ProtoBuffer. This has important
practical implications, as it enables to automatically create an architecture of neural
networks for processing structured data (AutoML paradigms), as demonstrated by
an accompanied library for JSON format.
1 Motivation
Prevalent machine learning methods assume their in-
put to be a vector or a matrix of a fixed dimension, or a
sequence, but many sources of data have the structure
of a tree, imposed by data formats like JSON, XML,
YAML, Avro, or ProtoBuffer (see Figure 1 for an ex-
ample). While the obvious complication is that such a
tree structure is more complicated than having a single
variable, these formats also contain some “elementary”
entries which are already difficult to handle in isola-
tion. Beside strings, for which a plethora conversions
to real-valued vectors exists (one-hot encoding, his-
tograms of n-gram models, word2vec (Mikolov et al.,
2013), output of a recurrent network, etc.), the most
problematic elements seem to be unordered lists (sets)
of records (such as the "workouts" element and all of
the subkeys of "speedData" in Figure 1), whose length
can differ from sample to sample and the classifier
processing this input needs to be able to cope with this
variability.
{"weekNumber":"39",
"workouts":[
{ "sport":"running",
"distance":19738,
"duration":1500,
"calories":375,
"avgPace":76,
"speedData":{
"speed":[10,9,8],
"altitude":[100,104,103,81],
"labels":["0.0km","6.6km
","13.2km","19.7km"]}},
{"sport":"swimming",
"distance":664,
"duration":1800,
"calories":250,
"avgPace":2711}]}
Figure 1: Example of JSON document,
adapted from https://github.com/
vaadin/fitness-tracker-demo
The variability exemplified above by "workouts" and "speedData" is the defining feature of Multi-
instance learning (MIL) problems (also called Deep Sets in Zaheer et al. (2017)), where it is intuitive
to define a sample as a collection of feature vectors. Although all vectors within the collection have
the same dimension, their number can differ from sample to sample. In MIL nomenclature, a sample
is called a bag and an individual vector an instance. The difference between sequences and bags is
that the order of instances in the bag is not important and the output of the classifier should be the
same for an arbitrary permutation of instances in the vector.
MIL was introduced in Dietterich et al. (1997) as a solution for a problem of learning a classifier on
instances from labels available on the level of a whole bag. To date, many approaches to solve the
1
Under review as a conference paper at ICLR 2019
problem have been proposed, and the reader is referred to Amores (2013) for an excellent review
and taxonomy. The setting has emerged from the assumption of a bag being considered positive if at
least one instance was positive. This assumption is nowadays used for problems with weakly-labeled
data (Bergamo & Torresani, 2010). While many different definitions of the problem have been
introduced (see Foulds & Frank (2010) for a review), this work adopts a general definition of Muandet
et al. (2012), where each sample (bag) is viewed as a probability distribution observed through a
set of realizations (instances) of a random variable with this distribution. Rather than working with
vectors, matrices or sequences, the classifier therefore classifies probability measures.
Independent works of Zaheer et al. (2017); Edwards & Storkey (2017) and Pevny & Somol (2017)
have proposed an adaptation of neural networks to MIL problems (hereinafter called MIL NN). The
adaptation uses two feed-forward neural networks, where the first network takes as an input individual
instances, its output is an element-wise averaged, and the resulting vector describing the whole bag is
sent to the second network. This simple approach yields a very general, well performing and robust
algorithm, which has been reported by all three works. Since then, the MIL NN has been used in
numerous applications, for example in causal reasoning (Santoro et al., 2017), in computer vision to
process point clouds (Su et al., 2018; Xu et al., 2018), in medicine to predict prostate cancer (Ing
et al., 2018), in training generative adversarial networks (Ing et al., 2018), or to process network
traffic to detect infected computers (Pevny & Somol, 2016). The last work has demonstrated that the
MIL NN construction can be nested (using sets of sets as an input), which allows the neural network
to handle data with a hierarchical structure.
The wide-spread use of neural networks is theoretically justified by their universal approximation
property - the fact that any continuous function on (a compact subset of) a Euclidean space to real
numbers can be approximated by a neural network with arbitrary precision (Hornik, 1991; Leshno
et al., 1993). However, despite their good performance and increasing popularity, no general analogy
of the universal approximation theorem has been proven for MIL NNs. This would require showing
that MIL NNs are dense in the space of continuous functions from the space of probability measures
to real numbers and - to the best of our knowledge - the only result in this direction is restricted to
input domains with finite cardinality (Zaheer et al., 2017).
This work fills this gap by formally proving that MIL NNs with two non-linear layers, a linear output
layer and mean aggregation after the first layer are dense in the space of continuous functions from
the space of probability measures to real numbers (Theorem 2 and Corollary 3). In Theorem 5,
the proof is extended to data with an arbitrary tree-like schema (XML, JSON, ProtoBuffer). The
reasoning behind the proofs comes from kernel embedding of distributions (mean map) (Smola et al.,
2007; Sriperumbudur et al., 2008) and related work on Maximum Mean Discrepancy (Gretton et al.,
2012). This work can therefore be viewed as a formal adaptation of these tools to neural networks.
While these results are not surprising, the authors believe that as the number of applications of NNs
to MIL and tree-structured data grows, it becomes important to have a formal proof of the soundness
of this approach.
The paper only contains theoretical results — for experimental comparison to prior art, the reader
is referred to Zaheer et al. (2017); Edwards & Storkey (2017); Pevny & Somol (2017); Santoro
et al. (2017); Su et al. (2018); Xu et al. (2018); Ing et al. (2018); Pevny & Somol (2016). However,
the authors provide a proof of concept demonstration of processing JSON data at https:
//codeocean.com/capsule/182df525-8417-441f-80ef-4d3c02fea970/?ID=
f4d3be809b14466c87c45dfabbaccd32.
2 Notation and summary of relevant work
This section provides background for the proposed extensions of the universal approximation the-
orem (Hornik, 1991; Leshno et al., 1993). For convenience, it also summarizes solutions to multi-
instance learning problems proposed in Pevny & Somol (2017); Edwards & Storkey (2017).
By C(K, R) we denote the space of continuous functions from K to R endowed with the topology of
uniform convergence. Recall that this topology is metrizable by the supremum metric ||f - g||sup =
supx∈K |f (x) -g(x)|.
Throughout the text, X will be an arbitrary metric space and PX will be some compact set of
(Borel) probability measures on X . Perhaps the most useful example of this setting is when X
2
Under review as a conference paper at ICLR 2019
is a compact metric space and PX = P(X) is the space of all Borel probability measures on
X . Endowing PX with the w? topology turns it into a compact metric space (the metric being
ρ* (p, q) = Pn 2-n ∙ | R fndp - R fndq∣ for some dense subset {fn | n ∈ N} ⊂ C(X, R)- see for
example Proposition 62 from Habala et al. (1996)). Alternatively, one can define metric on P(X)
using for example integral probability metrics (Muller, 1997) or total variation. In this sense, the
results presented below are general, as they are not tied to any particular topology.
2.1	UNIVERSAL APPROXIMATION THEOREM ON COMPACT SUBSETS OF Rd
The next definition introduces set of affine functions forming the base of linear and non-linear layers
of neural networks.
Definition 1. For any d ∈ N, Ad is the set of all affine functions on Rd i.e.
Ad = {a : Rd → R| a(x) = WTX + b,w ∈ Rd, b ∈ R} .	(1)
The main result of Leshno et al. (1993) states that feed-forward neural networks with a single non-
linear hidden layer and linear output layer (hereinafter called Σ-networks) are dense in the space of
continuous functions. Lemma 1.1 then implies that the same holds for measurable functions.
Theorem 1 (Universal approximation theorem on Rd). For any non-polynomial measurable function
σ on R and every d ∈ N, the following family of functions is dense in C(Rd, R):
Σ(σ, Ad) = f : Rd → R
n
f(x) =	αiσ(ai (x)), n ∈ N, αi ∈ R, ai ∈ Ad
i=1
)
(2)
The key insight of the theorem isn’t that a single non-linear layer suffices, but the fact that any
continuous function can be approximated by neural networks. Recall that for K ⊂ Rd compact, any
f ∈ C(K, R) can be continuolusly extended to Rd, and thus the same result holds for C(K, R). Note
that if σ was a polynomial of order k, Σ(σ, Ad) would only contain polynomials of order ≤ k.
The following metric corresponds to the notion of convergence in measure:
Definition 2 (Def. 2.9 from Hornik (1991)). For a Borelprobability measure μ on X, define a metric
Pμ(f,g) = inf {e > 0 | μ ({x ∈ X； If (x) - g(x)l ≥ e}) < e }	(3)
on M(X, R), where M(X, R) denotes the collection of all (Borel) measurable functions.
Note that for finite μ, the uniform convergence implies convergence in ρμ (Hornik, 1991, L. A.1):
Lemma 1.1. For every finite Borel measure μ on a compact K, C (K, R) is ρμ -dense in M (K, R).
2.2 Multi-instance neural networks
In Multi-instance learning it is assumed that a sample x consists of multiple vectors of a fixed
dimension, i.e. x = {x1, . . . , xl}, xi ∈ Rd. Furthermore, it is assumed that labels are provided on
the level of samples x, rather than on the level of individual instances xi.
To adapt feed-forward neural networks to MIL problems, the following construction has been
proposed in PeVny & Somol (2017); Edwards & Storkey (2017). Assuming mean aggregation
function, the network consists of two feed-forward neural networks φ : Rd → Rk and ψ : Rk → Ro .
The output of function is calculated as follows:
f (x) = ψ	X φ(xi)) ,	(4)
where d, k, o is the dimension of the input, output of the first neural network, and the output. This
construction also allows the use of other aggregation functions such as maximum.
The general definition of a MIL problem (Muandet et al., 2012) adopted here views instances xi of a
single sample x as realizations of a random variable with distribution p ∈ PX , where PX is a set
of probability measures on X . This means that the sample is not a single vector but a probability
distribution observed through a finite number of realizations of the corresponding random variable.
3
Under review as a conference paper at ICLR 2019
The main result of Section 3 is that the set of neural networks with (i) φ being a single non-linear layer,
(ii) ψ being one non-linear layer followed by a linear layer, and (iii) the aggregation function being
mean as in Equation (4) is dense in the space C(PX, R) of continuous functions on any compact set
of probability measures. Lemma 1.1 extends the result to the space of measurable functions.
The theoretical analysis assumes functions f : PX → R of the form
f(p)
ψ	φ(x)dp(x) ,
(5)
whereas in practice P can only be observed through a finite set of observations X = {xi 〜p|i ∈
{1, . . . , l}}. This might seem as a discrepancy, but the sample x can be interpreted as a mixture of
Dirac probability measures Px = : P；=i δχi. By definition of px, we immediately get
φ(x)dpx(x) = / 1 X φ(x)dδXi(X) = 1 Xφ(xi),
i=1	i=1
from which it easy to recover Equation (4). Since Px approaches P as increases, f(X) can be seen as
an estimate of f (P). Indeed, if the non-linearities in neural networks implementing functions φ and
ψ are continuous, the function f is bounded and from Hoeffding’s inequality (Hoeffding, 1963) it
follows that P (|f (P) - f (X)| ≥ t) ≤ 2 exp(-ct2l2) for some constant c > 0.
3 Universal approximation theorem for probability spaces
To extend Theorem 1 to spaces of probability measures, the following definition introduces the set of
functions which represent the layer that embedds probability measures into R.
Definition 3. For any X and set of functions F ⊂ {f : X → R}, we define AF as
AF =	f : PX → R
f(P) = b + Xwi Z fi(x)dP(x),m ∈ N, wi, b ∈ R, fi ∈ F .
(6)
AF can be viewed as an analogy of affine functions defined by Equation (1) in the context of
probability measures PX on X .
Remark. Let X ⊂ Rd and suppose that F only contains the basic projections πi : x ∈ Rd 7→ xi ∈ R.
If PX = {δχ∣x ∈ X} is the set of Dirac measures, then AFcoincides with Ad.
Using AF, the following definition extends the Σ-networks from Theorem 1 to probability spaces.
Definition 4 (Σ-networks). For any X, set of functions F = {f : X → R}, and a measurable
function σ : R → R, let Σ(σ, AF) be class of functions f : PX → R
Σ(σ, AF) = f :PX →R
n
f (P) =	αiσ(ai (P)), n ∈ N, αi ∈ R, ai ∈ AF
i=1
)
(7)
The main theorem of this work can now be presented. As illustrated in a corollary below, when applied
to F = Σ(σ, Ad) it states that three-layer neural networks, where first two layers are non-linear
interposed with an integration (average) layer, allow arbitrarily precise approximations of continuous
function on PX. (In other words this class of networks is dense in C(PX, R).)
Theorem 2.	Let PX be a compact set of Borel probability measures on a metric space X, F be a set
of continuous functions dense in C(X, R), and finally σ : R → R be a measurable non-polynomial
function. Then the set of functions Σ(σ, AF) is dense in C(PX, R).
Using Lemma 1.1, an immediate corollary is that a similar result holds for measurable funcitons:
Corollary 1 (Density of MIL NN in M(PX, R)). Under the assumptions of Theorem 2, Σ(σ, AF)
is ρμ-dense in M (PX, R) for any finite Borel measure μ on X.
The proof of Theorem 2 is similar to the proof of Theorem 2.4 from Hornik (1991). One of the
ingredients of the proof is the classical Stone-Weierstrass theorem (Stone, 1948). Recall that a
collection of functions is an algebra ifit is closed under multiplication and linear combinations.
4
Under review as a conference paper at ICLR 2019
Stone-Weierstrass Theorem. Let A ⊂ C(K, R) be an algebra of functions on a compact K. If
(i)	A separates points in K: (∀x, y ∈ K, x 6= y)(∃f ∈ A) : f(x) 6= f(y) and
(ii)	A vanishes at no point of K: (∀x ∈ K)(∃f ∈ A) : f(x) 6= 0,
then the uniform closure of A is equal to C(K, R).
Since Σ(σ, AF) is not closed under multiplication, we cannot apply the SW theorem directly. Instead,
we firstly prove the density of the class of ΣΠ networks (Theorem 3) which does form an algebra,
and then we extend the result to Σ-networks.
Theorem 3.	Let PX be a compact set of Borel probability measures on a metric space X, and F be
a dense subset of C(X, R). Then the following set of functions is dense in C(PX, R):
ΣΠ(F) = f : PX → R
n	li
f (p) = X αi Y	fij dp, n, li ∈ N, αi ∈ R, fij ∈ F .
i=1	j=1	
The proof shall use the following immediate corollary of Lemma 9.3.2 from Dudley (2002).
Lemma 3.1 (Lemma 9.3.2 of Dudley (2002)). Let (K, ρ) be a metric space and letp and q be two
Borel probability measures on K. Ifp 6= q, then we have fdp 6= fdq for some f ∈ C(K, R).
Proof of Theorem 3. Since ΣΠ(F) is clearly an algebra of continuous functions on PX, it suffices to
verify the assumptions of the SW theorem (separation and non-vanishing properties).
(i)	Separation: Let p1,p2 ∈ PX be distinct. By Lemma 3.1 there is some > 0 and f ∈ C(X, R)
such that fdp1 - fdp2 = 3. Since F is dense in C(X , R), there exists g ∈ F such that
maxx∈X |f (x) - g(x)| < . Using triangle inequality yields
fdp1 -	fdp2 =
≤
f(x)
- g(x) + g(x)dp1 (x) -
f(x) - g(x)dp1 (x)
f(x)
- g(x) + g(x)dp2 (x)
f(x) - g(x)dp2 (x)
+
g(x)dp1 (x) -
g(x)dp2 (x)
≤ 2
gdp1 -
gdp2
Denoting fg(p) = gdp, it is trivial to see that fg ∈ ΣΠ(F). It follows that ≤ |fg(p1) - fg(p2)|,
implying that ΣΠ(F) separates the points of X.
(ii)	Non-vanishing: Let p ∈ PX. Choose f ∈ C(X, R) such that f(x) = 1. Since F is dense in
C(X, R) there exists g ∈ F such that maxχ∈χ |f (x) - g(x)| ≤ 2. Since ʃ |f - g|dp ≤ 1, We get
1 =	fdp =	(f - g + g)dp =	(f (x) - g(x))dp(x) +	gdp
≤ / If (x) - g(χ)∣dp(χ) + / gdp ≤ 1 + / gdp.
Denote fg(q) = ʃ g dq, fg ∈ ΣΠ(F). It follows that fg(P) ≥ 2, and hence ΣΠ(F) vanishes at no
point of PX .
Since the assumptions of SW theorem are satisfied, ΣΠ(F) is dense in C(PX , R).
□
The folloWing simple lemma Will be useful in proving Theorem 2.
Lemma 3.2. If G is dense in C(Y, R), then for any h : X → Y, the collection of functions
{g ◦ h| g ∈ G} is dense in {φ ◦ h| φ ∈ C(Y, R)}.
5
Under review as a conference paper at ICLR 2019
Proof. Let g ∈ C(Y, R) and g* ∈ G be such that maxy∈γ |g(y) - g*(y)∣ ≤ e. Then We have
max |f(x) -g*(h(x))l = max ∣g(h(x)) - g*(h(x))l ≤ max |g(y) - g*(y)∣ ≤ e, (8)
x∈X	x∈X	y∈Y
which proves the lemma.	□
of Theorem 2. Theorem 2 is a consequence of Theorem 3 and Σ-networks being dense in
, R) for any k.
Let X, F, PX, and σ be as in the assumptions of the theorem. Let f * ∈ C(PX, R) and fix e > 0.
Then, there exist f ∈ ΣΠ(F) such that maxp∈p* |f (P) - f *(p)∣ ≤ j. This function is of the form
n li
f(p) = Xαi Y	fijdp
i=1	j=1
for some αi ∈ R and fij ∈ F. Moreover f can be written as a composition f = g ◦ h, where
h ： P e Px → (/ fιιdpj f12dp,..., / 狐“ dp),
n li
g : (x11, x12 , . . . , xnln) 7→ αi	xij ∈ R.
i=1	j=1
(9)
(10)
Denoting s = Pin=1 li, we identify the range of h and the domain ofg with Rs.
Since g is clearly continuous and Σ(σ, As) is dense in C(Rs, R) (by Theorem 1) there exists
g ∈ Σ(σ, As) such that maxy∈γ |g(y) - g(y)∣ ≤ j. It follows that f := g ◦ h satisfies
max |f*(P) - fg(P)| = max |f*(P) - f(P) + f(P) - gg(h(P))|
p∈PX	p∈PX
≤ max |f*(P) - f (P)| + max |f (P) - gg(h(P))|
p∈PX	p∈PX
≤ 2 + 2 =e
(by Lemma 3.2).
Since g ∈ Σ(σ, As), it is easy to see that f belongs to Σ(σ, AF), which concludes the proof. □
The function h in the above construction (Equation (9)) can be seen as a feature extraction layer
embedding the space of probability measures into a Euclidean space. It is similar to a mean-
map (Smola et al., 2007; Sriperumbudur et al., 2008) — a well-established paradigm in kernel
machines — in the sense that it characterizes a class of probability measures but, unlike mean-map,
only in parts where positive and negative samples differ.
4 Universal approximation theorem for product spaces
The next result is the extension of the universal approximation theorem to product spaces, which
naturally occur in structured data. The motivation here is for example if one sample consists of some
real vector x, set of vectors xi1 in=1 1 and another set of vectors xi2 in=2 1 .
Theorem 4. Let Xi ×…×Xι be a Cartesian product Ofmetric compacts, Fi, i = 1,... ,l be dense
subsets of C(Xi, R), and σ : R → R be a measurable function which is not an algebraic polynomial.
Then Σ(σ, AF1 ×∙..×Fl) is dense in C(Xi X …×Xι, R), where
Σ(σ, AF1×...×Fl) = /f ： Xi ×∙∙∙×Xι → R f(xi,. .. ,xι) = X αiσ(bi + X Wij ai7- (Xi)
i=1	j=1
n ∈ N, αi, bi, wij ∈ R, aij (x) ∈ Fj
6
Under review as a conference paper at ICLR 2019
The theorem is general in the sense that it covers cases where some Xi are compact sets of probability
measures as defined in Section 2, some are subsets of Euclidean spaces, and others can be general
compact spaces for which the corresponding sets of continuous function are dense in C(Xi, R).
The theorem is a simple consequence of the following corollary of Stone-Weierstrass theorem.
Corollary 2. For K1 and K2 compact, the following set of functions is dense in C(K1 × K2, R)
)
f :K1 ×K2→
n
R f(x,y) = X fi(x)gi(y), n ∈ N, fi ∈C(K1,R),gi ∈C(K2,R)
i=1
Proof of Theorem 4. The proof is technically similar to the proof of Theorem 2. Specifically, let f
be a continuous function on Xi ×∙∙∙×Xι and e > 0. By the aforementioned corollary of the SW
theorem, there are some fij ∈ Fj, i = 1, . . . , n, j = 1, . . . , l such that
nl
(…Xmax …XlIf(X)- XYfij(Xi)<.
i=1 j=1
Again, the above function can be written as a composition of two functions
h ： x ∈	Xi × …× Xi → (fιι (x1),f12(x2),... ,fni (Xl))	∈ Rnl,	(11)
g : X ∈	Rnl 7→Xn Yl Xij ∈ R.	(12)
i=i j=i
Since	g is continuous,	Theorem 1 can be applied to obtain a function	g of the form	g(x) =
Pin=i	αiσ(bi	+	ai (X)),	for some αi ∈ R and ai ∈ Anl, which approximates	g	with	error at
most e. Applying Lemma 3.2 to g, h, and g concludes the proof.
□
5 Multi-instance learning and tree structured data
The following corollary of Theorem 2 justifies the embedding paradigm of Zaheer et al. (2017);
Edwards & Storkey (2017); Pevny & Somol (2017) to MIL problems:
Corollary 3 (Density of MIL NN in C(PX, R)). Let X be a compact subset of Rd and PX a
compact set of probability measures on X. Then any function f ∈ C(PX, R) can be arbitrarily
closely approximated by a three-layer neural network composed of two non-linear layers with integral
(mean) aggregation layer between them, and a linear output layer.
If F in Theorem 2 is set to all feed-forward networks with a single non-linear layer (that is, when
F = Σ(σ, Ad)) then the theorem says that for every f ∈ C(PX, R) and > 0, there is some
f ∈ Σ(σ, /£(%/&))) such that maxp∈p* ∣f (P) - f *(p)∣ < e. This f can be written as
f(p) = Wi (σ(W2 (/ W3 (σ (W4x)) dp(x)))),
where for brevity the bias vectors are omitted, σ and / are element-wise, and W(.)are matrices
of appropriate sizes. Since the integral in the middle is linear with respect to the matrix-vector
multiplication, W2 and W3 can be replaced by a single matrix, which proves the corollary:
f(p) = Wi (σ(W2 (/ σ (W3x) dp(x)))).
Since Theorem 2 does not have any special conditions on X except to be compact metric space and
F to be continuous and uniformly dense in X , the theorem can be used as an induction step and the
construction can be repeated.
For example, consider a compact set of probability measures PPX on a PX. Then the space of neural
networks with four layers is dense in C(PPX, R). The network consists of three non-linear layers
with integration (mean) layer between them, and the last layer which is linear.
The above induction is summarized in the following theorem.
7
Under review as a conference paper at ICLR 2019
Theorem 5. Let S be the class of spaces which (i) contains all compact subsets of Rd, d ∈ N, (ii) is
closed under finite cartesian products, and (iii) for each X ∈ S we have P(X ) ∈ S.1 Then for each
X ∈ S, every continuous function on X can be arbitrarilly well approximated by neural networks.
By Lemma 1.1, an analogous result holds for measurable functions.
Proof. It suffices to show that S is contained in the class W of all compact metric spaces X for
which functions realized by neural networks are dense in C(W, R). By Theorem 1, W satisfies (i).
The properties (ii) and (iii) hold for W by Theorems 4 and 2. It follows that W ⊃ S.	□
6	Related Work
Works most similar to this one are on kernel mean embedding (Smola et al., 2007; Sriperumbudur
et al., 2008), showing that a probability measure can be uniquely embedded into high-dimensional
space using characteristic kernel. Kernel mean embedding is widely used in Maximum Mean Discrep-
ancy (Gretton et al., 2012) and in Support Measure Machines (Muandet et al., 2012; Christmann &
Steinwart, 2010), and is to our knowledge the only algorithm with proven approximation capabilities
comparable to the present work. Unfortunately its worst-case complexity of O(l3b2), where l is
the number of bags and b is the average size of a bag, prevents it from scaling to problems above
thousands of bags.
The MIL problem has been studied in Vinyals et al. (2016) proposing to use a LSTM network
augmented by memory. The reduction from sets to vectors is indirect by computing a weighted
average over elements in an associative memory. Therefore the aggregation tackled here is an integral
part of architecture. The paper lacks any approximation guarantees.
Problems, where input data has a tree structure, naturally occur in language models, where they
are typically solved by recurrent neural networks (Irsoy & Cardie, 2014; Socher et al., 2013). The
difference between these models is that the tree is typically binary and all leaves are homogeneous
in the sense that either each of them is a vector representation of a word or each of them is a vector
representation of an internal node. Contrary, here it is assumed that the tree can have an arbitrary
number of heterogeneous leaves following a certain fixed scheme.
Due to lack of space, the authors cannot list all works on MIL. The reader is instead invited to look at
the excellent overview in Amores (2013) and the works listed in the introductory part of this paper.
7	Conclusion
This work has been motivated by recently proposed solutions to multi-instance learning Zaheer
et al. (2017); Pevny & Somol (2017); Edwards & Storkey (2017) and by mean-map embedding of
probability measures Sriperumbudur et al. (2008). It generalizes the universal approximation theorem
of neural networks to compact sets of probability measures over compact subsets of Euclidean spaces.
Therefore, it can be seen as an adaptation of the mean-map framework to the world of neural networks,
which is important for comparing probability measures and for multi-instance learning, and it proves
the soundness of the constructions of Pevny & Somol (2017); Edwards & Storkey (2017).
The universal approximation theorem is extended to inputs with a tree schema (structure) which, being
the basis of many data exchange formats like JSON, XML, ProtoBuffer, Avro, etc., are nowadays
ubiquitous. This theoretically justifies applications of (MIL) neural networks in this setting.
As the presented proof relies on the Stone-Weierstrass theorem, it restricts non-linear functions in
neural networks to be continuous in all but the last non-linear layer. Although this does not have an
impact on practical applications (all commonly use nonlinear functions within neural networks are
continuous) it would be interesting to generalize the result to non-continuous non-linearities, as has
been done for feed-forward neural networks in Leshno et al. (1993).
1Here we assume that P (X) is endowed with the metric ρ* from Section 2.
8
Under review as a conference paper at ICLR 2019
References
Jaume Amores. Multiple instance classification: Review, taxonomy and comparative study. Artif.
Intell.,201:81-105, AUgUst 2013. ISSN 0004-3702. doi: 10.1016∕j.artint.2013.06.003. URL
http://dx.doi.org/10.1016/j.artint.2013.06.003.
Alessandro Bergamo and Lorenzo Torresani. Exploiting weakly-labeled web images to improve
object classification: a domain adaptation approach. In Advances in neural information processing
systems, pp. 181-189, 2010.
Andreas Christmann and Ingo Steinwart. Universal kernels on non-standard inpUt
spaces. In J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel,
and A. CUlotta (eds.), Advances in Neural Information Processing Systems 23, pp.
406-414. CUrran Associates, Inc., 2010. URL http://papers.nips.cc/paper/
4168-universal-kernels-on-non-standard-input-spaces.pdf.
Thomas G Dietterich, Richard H Lathrop, and Tomgs Lozano-PCrez. Solving the multiple instance
problem with axis-parallel rectangles. Artificial intelligence, 89(1):31-71, 1997.
R. M. DUdley. Real Analysis and Probability. Cambridge University Press, 2002.
Harrison Edwards and Amos Storkey. Towards a Neural Statistician. 2 2017.
James FoUlds and Eibe Frank. A review of mUlti-instance learning assUmptions. The Knowledge
Engineering Review, 25(01):1-25, 2010.
Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Scholkopf, and Alexander Smola.
A kernel two-sample test. J. Mach. Learn. Res., 13:723-773, March 2012. ISSN 1532-4435. URL
http://dl.acm.org/citation.cfm?id=2188385.2188410.
Petr Habala, Petr Hajek, and Vaclav Zizler. Introduction to Banach spaces. Matfyzpress, VydaVateIStVi
MatematiCko-fyzikglni fakulty Univerzity Karlovy, 1996.
Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the
American Statistical Association, 58(301):13-30, 1963.
Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neu-
ral Networks, 4(2):251 - 257, 1991. ISSN 0893-6080. doi: https://doi.org/10.
1016/0893-6080(91)90009-T. URL http://www.sciencedirect.com/science/
article/pii/089360809190009T.
Nathan Ing, Jakub M Tomczak, Eric Miller, Isla P Garraway, Max Welling, Beatrice S Knudsen, and
Arkadiusz Gertych. A deep multiple instance model to predict prostate cancer metastasis from
nuclear morphology. 2018.
Ozan Irsoy and Claire Cardie. Deep recursive neural networks for compositionality in
language. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q.
Weinberger (eds.), Advances in Neural Information Processing Systems 27, pp. 2096-
2104. Curran Associates, Inc., 2014. URL http://papers.nips.cc/paper/
5551-deep-recursive-neural-networks- for-compositionality-in- language.
pdf.
Moshe Leshno, Vladimir Ya. Lin, Allan Pinkus, and Shimon Schocken. Multilayer feedforward
networks with a nonpolynomial activation function can approximate any function. Neural Networks,
6(6):861 - 867, 1993. ISSN 0893-6080.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations
of words and phrases and their compositionality. In Advances in neural information processing
systems, pp. 3111-3119, 2013.
Krikamol Muandet, Kenji Fukumizu, Francesco Dinuzzo, and Bernhard Scholkopf. Learning
from distributions via support measure machines. In F. Pereira, C. J. C. Burges, L. Bot-
tou, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 25,
pp. 10-18. Curran Associates, Inc., 2012. URL http://papers.nips.cc/paper/
9
Under review as a conference paper at ICLR 2019
4825- learning- from- distributions- via- support- measure- machines.
pdf.
Alfred Muller. Integral probability metrics and their generating classes of functions. Advances in
Applied Probability, 29(2):429-443,1997. ISSN00018678. URL http://www.jstor.org/
stable/1428011.
Tomas Pevny and Petr Somol. Discriminative models for multi-instance problems with tree structure.
In Proceedings of the 2016 ACM Workshop on Artificial Intelligence and Security, AISec ’16, pp.
83-91, New York, NY, USA, 2016. ACM. ISBN 978-1-4503-4573-6. doi: 10.1145/2996758.
2996761. URL http://doi.acm.org/10.1145/2996758.2996761.
Tomds Pevny and Petr Somol. Using neural network formalism to solve multiple-instance problems.
In Fengyu Cong, Andrew Leung, and Qinglai Wei (eds.), Advances in Neural Networks - ISNN
2017, pp. 135-142, Cham, 2017. Springer International Publishing. ISBN 978-3-319-59072-1.
Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter
Battaglia, and Tim Lillicrap. A simple neural network module for relational reasoning. In
Advances in neural information processing systems, pp. 4967-4976, 2017.
Alex Smola, Arthur Gretton, Le Song, and Bernhard Scholkopf. A hilbert space embedding for
distributions. In Marcus Hutter, Rocco A. Servedio, and Eiji Takimoto (eds.), Algorithmic Learning
Theory, pp. 13-31, Berlin, Heidelberg, 2007. Springer Berlin Heidelberg. ISBN 978-3-540-75225-
7.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and
Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank.
In Proceedings of the 2013 conference on empirical methods in natural language processing, pp.
1631-1642, 2013.
Bharath K Sriperumbudur, Arthur Gretton, Kenji Fukumizu, Gert Lanckriet, and Bernhard Scholkopf.
Injective hilbert space embeddings of probability measures. 2008.
M. H. Stone. The generalized weierstrass approximation theorem. Mathematics Magazine, 21
(4):167-184, 1948. ISSN 0025570X, 19300980. URL http://www.jstor.org/stable/
3029750.
Hang Su, Varun Jampani, Deqing Sun, Subhransu Maji, Evangelos Kalogerakis, Ming-Hsuan Yang,
and Jan Kautz. Splatnet: Sparse lattice networks for point cloud processing. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pp. 2530-2539, 2018.
Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order matters: Sequence to sequence for
sets. In International Conference on Learning Representations (ICLR), 2016. URL http:
//arxiv.org/abs/1511.06391.
Yifan Xu, Tianqi Fan, Mingye Xu, Long Zeng, and Yu Qiao. Spidercnn: Deep learning on point sets
with parameterized convolutional filters. arXiv preprint arXiv:1803.11527, 2018.
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan R Salakhutdinov,
and Alexander J Smola. Deep sets. In Advances in Neural Information Processing Systems, pp.
3391-3401, 2017.
10