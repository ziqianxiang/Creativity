Under review as a conference paper at ICLR 2019
Guaranteed Recovery of One-Hidden-Layer
Neural Networks via Cross Entropy
Anonymous authors
Paper under double-blind review
Ab stract
We study model recovery for data classification, where the training labels are
generated from a one-hidden-layer fully-connected neural network with sigmoid
activations, and the goal is to recover the weight vectors of the neural network. We
prove that under Gaussian inputs, the empirical risk function using cross entropy
exhibits strong convexity and smoothness uniformly in a local neighborhood of the
ground truth, as soon as the sample complexity is sufficiently large. This implies
that if initialized in this neighborhood, which can be achieved via the tensor method,
gradient descent converges linearly to a critical point that is provably close to the
ground truth without requiring a fresh set of samples at each iteration. To the best
of our knowledge, this is the first global convergence guarantee established for the
empirical risk minimization using cross entropy via gradient descent for learning
one-hidden-layer neural networks, at the near-optimal sample and computational
complexity with respect to the network input dimension.
1 Introduction
Neural networks have attracted a significant amount of research interest in recent years due to the
success of deep neural networks (LeCun et al., 2015) in practical domains such as computer vision
and artificial intelligence (Russakovsky et al., 2015; He et al., 2016; Silver et al., 2016). However,
the theoretical underpinnings behind such success remains mysterious to a large extent. Efforts have
been taken to understand which classes of functions can be represented by deep neural networks
(Cybenko, 1989; Hornik et al., 1989; Barron, 1993; Telgarsky, 2016), when (stochastic) gradient
descent is effective for optimizing a non-convex loss function (Dauphin et al., 2014), and why these
networks generalize well (Zhang et al., 2016; Bartlett et al., 2017; Brutzkus et al., 2017).
One important line of research that has attracted extensive attention is a model-recovery setup, i.e.,
given that the training samples (xi, y/ 〜(x, y) are generated i.i.d. from a distribution D based on a
neural network model with the ground truth parameter W?, the goal is to recover the underlying model
parameter W?, which is important for the network to generalize well (Mondelli & Montanari, 2018).
Previous studies along this topic can be mainly divided into two types of data generations. First, a
regression problem, for example, assumes that each sample y is generated as y = PK=I 0(w?> x),
where wk ∈ Rd is the weight vector of the kth neuron, 1 ≤ k ≤ K, and the input x ∈ Rd is Gaussian.
This type of regression problem has been studied in various settings. In particular, (Soltanolkotabi,
2017) studied the single-neuron model under ReLU activation, (Zhong et al., 2017b) studied the one-
hidden-layer multi-neuron network model, and (Li & Yuan, 2017) studied a two-layer feedforward
networks with ReLU activations and identity mapping. Second, for a classification problem, suppose
each label y ∈ {0,1} is drawn under the conditional distribution P(y = 1|x) = -KK PK=I 0(w?>x),
where wk? ∈ Rd is the weight vector of the kth neuron, 1 ≤ k ≤ K, and the input x ∈ Rd is Gaussian.
Such a problem has been studied in (Mei et al., 2016) in the case with a single neuron.
For both the regression and the classification settings, in order to recover the neural network pa-
rameters, all previous studies considered (stochastic) gradient descent over the squared loss, i.e.,
'qu(W; x,y) = 1 (y — Kk X φ (w> x))
i=1
(1)
1
Under review as a conference paper at ICLR 2019
which yields gradient and Hessian in relatively simple forms to assist the landscape characterization
of the function as well as model recovery analysis.
However, for the classification problem, the cross entropy objective used in practice takes the
following form
1K
' (W; χ,y) = -y ∙ log Kf φ (w> χ)
i=1
-(1 - y) ∙ log
(2)
The geometry as well as the model recovery problem based on the entropy loss function have not
yet been understood. It is expected that such a loss function is very challenging to analyze, not just
because it is nonconvex with multiple neurons, but also because the gradient and Hessian take much
more complicated forms compared with the squared loss. The main focus of this paper is to develop
technical analysis for guaranteed model recovery under the challenging cross entropy loss function in
eq. (2) for the classification problem in the multi-neuron case.
Furthermore, previous studies provided two types of statistical guarantees for such model recovery
problems using the squared loss. More specifically, (Zhong et al., 2017b) showed that in the local
neighborhood of the ground truth, the Hessian of the empirical loss function is positive definite
for each given point under independent high probability event. Hence, their guarantee for gradient
descent to converge to the ground truth requires a fresh set of samples at every iteration, thus the
total sample complexity will depend on the number of iterations. On the other hand, studies such as
(Mei et al., 2016; Soltanolkotabi, 2017) establish certain types of uniform geometry such as strong
convexity so that resampling per iteration is not needed for gradient descent to have guaranteed linear
convergence as long as it enters such a local neighborhood. However, such a stronger statistical
guarantee without per-iteration resampling have only been shown for the squared loss function. In this
paper, we aim at developing such a strong statistical guarantee for the loss function in eq. (2), which
is much more challenging but more practical than the squared loss for the classification problem.
1.1	Our Contributions
This study provides the first performance guarantee for the recovery of one-hidden-layer neural
networks using the cross entropy loss function, to the best of our knowledge. More specifically, our
contributions are summarized as follows.
•	For multi-neuron classification problem with sigmoid activations, we show that, if the input is
Gaussian, the empirical risk function fn(W) = 1 PN1 ' (W； Xi) based on the cross entropy
loss in eq. (2) is uniformly strongly convex in a local neighborhood of the ground truth W? of size
O(1/K3/2) as soon as the sample size is O(dK5 log2 d), where d is the input dimension and K is
the number of neurons.
We further show that, if initialized in this neighborhood, gradient descent converges linearly to a
critical point Wcn (which we show to exist), with a sample complexity of O(dK5 log2 d), which is
near-optimal up to a polynomial factor in K and log d. Due to the nature of quantized labels here,
the recover of W? is only up to certain statistical accuracy, and Wcn converges to W? at a rate
of O( ∖∕dK9/2 log n/n) in the Frobenius norm. Furthermore, such a convergence guarantee does
not require a fresh set of samples at each iteration due to the uniform strong convexity in the local
neighborhood. To obtain -accuracy, it requires a computational complexity of O(ndK2 log(1/)).
•	We adopt the tensor method proposed in (Zhong et al., 2017b), and show it provably provides
an initialization in the neighborhood of the ground truth. In particular, our proof replaces the
homogeneous assumption on activation functions in (Zhong et al., 2017b) by a mild condition
on the curvature of activation functions around W?, which holds for a larger class of activation
functions including sigmoid and tanh.
In order to analyze the challenging cross-entropy loss function, our proof develops various new
machineries in order to exploit the statistical information of the geometric curvatures, including the
gradient and Hessian of the empirical risk, and to develop covering arguments to guarantee uniform
concentrations. Our technique also yields similar performance guarantees for the classification
problem using the squared loss in eq. (1), which we omit due to space limitations, as it is easier to
analyze than cross entropy.
2
Under review as a conference paper at ICLR 2019
1.2	Related Work
Due to page limitations we focus on the most relevant literature on theoretical and algorithmic aspects
of learning shallow neural networks via nonconvex optimization.
The parameter recovery viewpoint is relevant to the success of non-convex learning in signal process-
ing problems such as matrix completion, phase retrieval, blind deconvolution, dictionary learning
and tensor decomposition (SUn & Luo, 2016; Candes et al., 2015; Ge & Ma, 2017; Ge et al., 2016;
Sun et al., 2015; Bhojanapalli et al., 2016; Ma et al., 2017), to name a few. The statistical model
for data generation effectively removes worst-case instances and allows us to focus on average-case
performance, which often possess much benign geometric properties that enable global convergence
of simple local search algorithms.
The studies of one-hidden-layer network model can be further categorized into two classes, landscape
analysis and model recovery. In the landscape analysis, it is known that if the network size is large
enough compared to the data input, then there are no spurious local minima in the optimization
landscape, and all local minima are global (Soltanolkotabi et al., 2017; Boob & Lan, 2017; Safran
& Shamir, 2016; Nguyen & Hein, 2017). For the case with multiple neurons (2 ≤ K ≤ d) in the
under-parameterized setting, the work of Tian (Tian, 2017) studied the landscape of the population
squared loss surface with ReLU activations. In particular, there exist spurious bad local minima in the
optimization landscape (Ge et al., 2017; Safran & Shamir, 2017) even at the population level. Zhong
et. al. (Zhong et al., 2017b) provided several important characterizations for the local Hessian for the
regression problem for a variety of activation functions for the squared loss.
In the model recovery problem, the number of neurons is smaller than the dimension of inputs. In the
case with a single neuron (K = 1), under Gaussian input, (Soltanolkotabi, 2017) showed that gradient
descent converges linearly when the activation function is ReLU, i.e. φ(z) = max{z, 0}, with a zero
initialization, as long as the sample complexity is O(d) for the regression problem. On the other end,
(Mei et al., 2016) showed that when φ(∙) has bounded first, second and third derivatives, there is no
other critical points than the unique global minimum (within a constrained region of interest), and
(projected) gradient descent converges linearly with an arbitrary initialization, as long as the sample
complexity is O(d log2 d) with sub-Gaussian inputs for the classification problem using the squared
loss. Moreover, (Zhong et al., 2017b) shows that the ground truth From a technical perspective, our
study differs from all the aforementioned work in that the cross entropy loss function we analyze has
a very different form. Furthermore, we study the model recovery classification problem under the
multi-neuron case, which has not been studied before.
Finally, we note that several papers study one-hidden-layer or two-layer neural networks with different
structures under Gaussian input. For example, (Brutzkus & Globerson, 2017; Du et al., 2017a;b;
Zhong et al., 2017a) studied the non-overlapping convolutional neural network, (Li & Yuan, 2017)
studied a two-layer feedforward networks with ReLU activations and identity mapping, and (Feizi
et al., 2017) introduced the Porcupine Neural Network. These results are not directly comparable to
ours since both the networks and the loss functions are different.
1.3	Paper Organization and Notations
The rest of the paper is organized as follows. Section 2 describes the problem formulation. Section 3
presents the main results on local geometry and local linear convergence of gradient descent. Section 4
discusses the initialization method. Numerical examples are demonstrated in Section 5, and finally,
conclusions are drawn in Section 6.
Throughout this paper, we use boldface letters to denote vectors and matrices, e.g. w and W. The
transpose of W is denoted by W>, and kW k, kW kF denote the spectral norm and the Frobenius
norm. For a positive semidefinite (PSD) matrix A, we write A 0. The identity matrix is denoted
by I. The gradient and the Hessian of a function f (W) is denoted by Vf (W) and V2f(W),
respectively. Let σi (W) denote the i-th singular value of W. Denote k ∙ j∣ψι as the sub-exponential
norm of a random variable. We use c, C, C1 , . . . to denote constants whose values may vary from
line to line. For nonnegative functions f(x) and g(x), f(x) = O (g(x)) means there exist positive
constants C and a such that	f (x)	≤ Cg(X)	for all X ≥	a; f (x)	= Ω (g(x))	means there exist positive
constants c and a such that	f(x)	≥ cg(x)	for all x ≥	a.
3
Under review as a conference paper at ICLR 2019
2 Problem Formulation
We first describe the generative model for training data, and then describe the gradient descent
algorithm for learning the network weights.
2.1	Model
Suppose We are given n training samples {(xi, yi)}n=ι 〜(x, y) that are drawn i.i.d., where X 〜
N(0, I). Assume the activation function is sigmoid, i.e. φ (z) = 1/(1 + e-z) for all z. Conditioned
on x ∈ Rd, we consider the classification setting, where y is mapped to a discrete label using the
one-hidden layer neural network model as follows:
1K
P(y = 1|x) = k£0(w?>x).
(3)
k=1
and P(y = 0|x) = 1 - P(y = 1|x), where K is the number of neurons.
Our goal is to estimate W? = [w?, •…，WK], via minimizing the following empirical risk function:
1n
fn(W) = n ∑' (W； Xi),
(4)
i=1
where ` (W; x) := ` (W; x, y) is the cross entropy loss, i.e., the negative log-likelihood function,
i.e.,
' (W; x)
i=1
十 (I - y) ∙ log 1- - K X φ (w>χ))].
K
Let w = vec(W) = [w>,…，WK ]> ∈ RdK be the vectorized form of W. With slight abuse of
notation, we denote the gradient and Hessian of '(W; x) with respect to the vector w.
2.2	Gradient Descent
To estimate W?, since (4) is a highly nonconvex function, vanilla gradient descent with an arbitrary
initialization may get stuck at local minima. Therefore, we implement the gradient descent algorithm
with a well-designed initialization scheme that is described in detail in Section 4. The update rule is
given as
Wt+ι = Wt- ηVfn (Wt),
where η is the step size. The algorithm is summarized in Algorithm 1.
Algorithm 1 Gradient Descent
Input: Training data {(xi, yi)}in=1, step size η, iteration T
Initialization: Wo — Initialization ({(xi, yi)}rn=1)
Gradient Descent: for t = 0,1,…，T
Wt+1 = Wt -ηVfn (Wt) .
Output: WT
We note that throughout the execution of the algorithm, the same set of training samples is used
which is the standard implementation of gradient descent. This is in sharp contrast to existing work
such as Zhong et al. (2017b) that employs the impractical scheme of resampling, where a fresh set of
training samples is used at every iteration of gradient descent.
3 Main Results
Before stating our main results, we first introduce an important quantity regarding φ(z) that captures
the geometric properties of the loss function, distilled in (Zhong et al., 2017b).
4
Under review as a conference paper at ICLR 2019
Figure 1: ρ (σ) for sigmoid activation.
Definition 1. Let αq(σ) = Ez~n(0,1)[φ0(σ ∙ z)zq],∀q ∈ {0,1, 2}, and βq(σ) = E>~n(0,i)[φ02(σ ∙
z)zq], ∀q ∈ {0, 2}. Define ρ(σ) as
ρ(σ) = min{β0 (σ) - α02 (σ) - α12 (σ),
β2 (σ) - α21(σ) - α22 (σ)}.
Note that the definition here is different from that in (Zhong et al., 2017b, Property 3.2) but consistent
with (Zhong et al., 2017b, Lemma D.4) which removes the third term in (Zhong et al., 2017b, Property
3.2). For the activation function considered in this paper, the first two terms suffice. We depict ρ(σ)
as a function of σ in a certain range for the sigmoid activation in Fig. 1. It is easy to observe that
ρ(σ ) > 0 for all σ > 0.
3.1	Local Strong Convexity
We first characterize the local strong convexity of fn(W) in a neighborhood of the ground truth W?.
Let B (W?, r) denote a Euclidean ball centered at W? ∈ Rd×K with a radius r, i.e.
B (W?,r) = {W ∈ Rd×K : kW - W?kF ≤ r}.
Let σ% := σ% (W?) denote the i-th singular value of W?. Let the condition number be K = σ∖/σκ,
and λ = QiK=1 (σi /σK). The following theorem guarantees the Hessian of the empirical risk function
fn(W) in the local neighborhood of W? is positive definite with high probability.
Theorem 1. For the classification model (3) with sigmoid activation function, assume kW?kF ≤ 1,
then there exists some constant C, such that if
n ≥ C ∙ dK5 log2 d ∙ (-K2λτ),
ρ(σK)
then with probability at least 1 - d-10, for all W ∈ B(W?, r),
Ω (与∙ ρ⅛Kl) ∙ I WV2fn (W) W C ∙ I
K2	κ2λ
hold, where r := min { -C ∙ PKK，。彳}∙
We note that all column permutations of W? are equivalent global minima of the loss function, and
Theorem 1 applies to all such permutation matrices of W? . The proof of Theorem 1 is outlined in
Appendix A. Theorem 1 guarantees that the Hessian of the empirical cross-entropy loss function
fn(W) is positive definite (PD) in a neighborhood of the ground truth W?, as long as ρ(σK) > 0
(i.e. W? is full-column rank), when the sample size n is sufficiently large for the sigmoid activation.
The bounds in Theorem 1 depend on the dimension parameters of the network (n and K), as well
as the activation function and the ground truth (ρ(σK),λ). As a special case, suppose W? is
composed of orthonormal columns with ρ(σκ) = O(1), K = 1, λ = 1. Then, Theorem 1 guarantees
Ω(l∕κ2)I W V2fn (W) W C within the neighborhood B(W?, Ω(1∕K√K)), as soon as the
sample complexity n = Ω(dK5 log2 d). The sample complexity is order-wise near-optimal in d up
to polynomial factors of K and log d, since the number of unknown parameters is dK.
5
Under review as a conference paper at ICLR 2019
3.2	Performance Guarantees of Gradient Descent
For the classification problem, due to the nature of quantized labels, W? is no longer a critical point
of fn(W). By the strong convexity of the empirical risk function fn(W) in the local neighborhood
of W?, there can exist at most one critical point in B(W?, r), which is the unique local minimizer in
B (W?, r) if it exists. The following theorem shows that there indeed exists such a critical point Wcn,
which is provably close to the ground truth W?, and gradient descent converges linearly to Wcn .
Theorem 2. For the classification model (3) with sigmoid activation function, and assume kW?kF ≤
Lthere exist some constants C, Ci > 0 such that if the sample size n ≥ C ∙ dK5 log2 d ∙
22
κ2λ ʌ
P(σκ))
then with probability at least 1 - d-10, there exists a unique critical point Wcn in B(W?, r) with
r := min {κc∕∑ ∙ PKK), 07}, which satisfies
- W*I∣F ≤ CI
κ9/4k2x ∕dl
ρ(σK)
og n
n
(5)
Moreover, if the initial point W0 ∈ B (W?, r), then gradient descent converges linearly to Wcn, i.e.
where Hmin = Ω
Wt - Wcn	≤ (1 - Hminη)t W0 - Wcn
PK2K)), as long as the SteP Size η = Ω (表∙ ρ⅛2K)
(6)
Similarly to Theorem 1, Theorem 2 also holds for all column permutations of W?. The proof can
be found in Appendix B. Theorem 2 guarantees that there exists a critical point Wn in B(W?, r)
which converges to W? at the rate of O(K9/4，dlogn/n), and therefore W? can be recovered
consistently as n goes to infinity. Moreover, gradient descent converges linearly to Wcn at a linear rate,
as long as it is initialized in the basin of attraction. To achieve -accuracy, i.e. Wt - Wcn ≤ , it
requires a computational complexity of O ndK2 log (1/) , which is linear in n, d and log(1/).
4 Initialization
Our initialization adopts the tensor method proposed in (Zhong et al., 2017b). In this section, we first
briefly describe this method, and then present the performance guarantee of the initialization with
remarks on the differences from that in (Zhong et al., 2017b).
4.1	Preliminary and Algorithm
This subsection briefly introduces the tensor method proposed in (Zhong et al., 2017b), to which a
reader can refer for more details. We first define a product 0 as follows. If V ∈ Rd is a vector and I
is the identity matrix, then v01 = Pdd=ι[v 0 ej 0 ej + ej 0 V 0 ej + ej 0 ej 0 v]. If M is a
symmetric rank-r matrix factorized as M = Pir=1 sivivi> and I is the identity matrix, then
r d6
M0e I=XsiXXAl,i,j,
where A1,i,j = Vi 0 Vi 0 ej 0 ej, A2,i,j = Vi 0 ej 0 Vi 0 ej, A3,i,j = ej 0 Vi 0 Vi 0 ej,
A4,i,j = Vi 0 ej 0 ej 0 Vi, A5,i,j = ej 0 Vi 0 ej 0 Vi and A6,i,j = ej 0 ej 0 Vi 0 Vi.
Definition 2. Define M1, M2, M3, M4 and m1,i, m2,i, m3,i, m4,i as follows:
Mi = E[y ∙ x],	,
M2 = E[y ∙ (X 0 X - I)],
M3 = E[y ∙ (x03 - x01)],〜	〜
M4 = E[y ∙ (x04 — (x 0 x)01 + 101)],
mi,i = YI(Hw?k),
m2,i = Y2(kw?k) - Y0(kw?k),
m3,i = Y3(kw?k) - 3Yl(kwtk),
m4,i = Y4(kw?k) + 3Y0(kw?k) - 6Y2(kw?k),
where Yj(σ) = Ez~n(0,1)[φ(σ ∙ z)zj], ∀j = 0,1, 2, 3,4.
6
Under review as a conference paper at ICLR 2019
Definition 3. Let α ∈ Rd denote a randomly picked vector. We define P2 and P3 as follows: P2 =
Mj2 (I, I, α,…，α) ,1 where j2 = min{j ≥ 2|Mj = 0} ,and P3 = Mj3 (I, I, I, α,…，α),
where j3 = min{j ≥ 3|Mj 6= 0}.
We further denote W = w∕kw∣∣. The initialization algorithm based on the tensor method is
summarized in Algorithm 2, which includes two major steps. Step 1 first estimates the di-
rection of each column of W ? by decomposing P2 to approximate the subspace spanned by
{w?, W?,…，WK} (denoted by V), then reduces the third-order tensor P3 to a lower-dimension
tensor R3 = P3 (V, V, V) ∈ RK×K×K, and applys non-orthogonal tensor decomposition on R3
to output the estimate SiV>W?, where Si ∈ {1, -1} is a random sign. Step 2 approximates the
magnitude of Wi? and the sign si by solving a linear system of equations.
Algorithm 2 Initialization via Tensor Method
Input: Partition n pairs of data {(xi, yi)}in=1 into three parts D1, D2, D3.
Output:
1:
2:
3:
4:
5:
Estimate Pb2 of P2 from data set D1 .
V — PowerMethod(E, K).
Estimate Rb3 of P3(V, V, V) from data set D2.
一、 ,^ .
{ubi}i∈[K] — KCL(Rb3》
{w(0)}i∈[K] - RECMAG(V, {Ubi}i∈K], D3).
4.2	Performance Guarantee of Initialization
For the classification problem, we make the following technical assumptions, similarly in (Zhong
et al., 2017b, Assumption 5.3) for the regression problem.
Assumption 1. The activation function φ(z) satisfies the following conditions:1. If Mj 6= 0, then
K
i=1
j-2	丁
wi?wi?> = 0
∀j,
K
X mj,i (Wfa)j-3 (V>W?)Vec((V>W?)(V>W?)>)> = 0
i=1
for j ≥ 3
2. At least one of M3 and M4 is non-zero.
Furthermore, we do not require the homogeneous assumption ((i.e., φ(az) = apz for an integer
p)) required in (Zhong et al., 2017b), which can be restrictive. Instead, we assume the following
condition on the curvature of the activation function around the ground truth, which holds for a larger
class of activation functions such as sigmoid and tanh.
Assumption 2. Let l1 be the index of the first nonzero Mi where i = 1, . . . , 4. For the activation
function φ (∙), there exists a positive constant δ such that miι,i(∙) is strictly monotone over the
interval (||w?k — δ, ||w?k + δ), and the derivative of miι,i(∙) is lower bounded by some constant
for all i.
We next present the performance guarantee for the initialization algorithm in the following theorem.
Theorem 3. For the classification model (3), under Assumptions 1 and 2, if the sample size n ≥
dpoly (K, κ, t, log d, 1/), then the output W0 ∈ Rd×K of Algorithm 2 satisfies
kW0-W?kF ≤ poly (K, κ) kW ?kF,	(7)
with probability at least 1 — d-Q(t).
The proof of Theorem 3 consists of (a) showing the estimation of the direction of W? is sufficiently
accurate and (b) showing the approximation of the norm of W? is accurate enough. Our proof of
part (a) is the same as that in (Zhong et al., 2017b), but our argument in part (b) is different, where
we relax the homogeneous assumption on activation functions. More details can be found in the
supplementary materials in Appendix C.
1See (15) in the supplemental materials for definition.
7
Under review as a conference paper at ICLR 2019
n/d log(n)
LU
S
W
N
10-1
10-2
(c)
50	100	150	200
n/d log(n)
Figure 2: Fix K = 3. (a) Success rate of converging to the same local minima with respect to
the sample complexity for various d; (b) Average estimation error of gradient descent in a local
neighborhood of the ground truth with respect to the sample complexity for various d; (c) Average
estimation error of gradient descent using different objective functions in a local neighborhood of the
ground truth with respect to the sample complexity when d = 20.
5 Numerical Experiments
In this section, we first implement gradient descent to verify that the empirical risk function is strongly
convex in the local region around W ? . If we initialize multiple times in such a local region, it is
expected that gradient descent converges to the same
critical point Wcn, with the
same set of training
samples. Given a set of training samples, we randomly initialize multiple times, and then calculate
the variance of the output of gradient descent. Denote the output of the 'th run as WF) = Vec(Wn)
and the mean of the runs as W. The error is calculated as SDn = ʌ/1 PL=I |陶户-Wk2, where
L = 20 is the total number of random initializations. Adopted in (Mei et al., 2016), it quantifies the
standard deviation of the estimator Wcn under different initializations with the same set of training
samples. We say an experiment is successful, if SDn ≤ 10-2.
Figure 2 (a) shows the successful rate of gradient descent by averaging over 50 sets of training samples
for each pair of n and d, where K = 3 and d = 15, 20, 25 respectively. The maximum iterations for
gradient descent is set as itermax = 3500. It can be seen that as long as the sample complexity is
large enough, gradient descent converges to the same local minima with high probability.
We next show that the statistical accuracy of the local minimizer for gradient descent if it is ini-
tialized close enough to the ground truth. Suppose we initialize around the ground truth such
that k Wo - W*∣∣f ≤ 0.1 ∙ k W?∣∣f. We calculate the average estimation error as PL=IIlWn(') -
W?k2F/(LkW?k2F) over L = 100 Monte Carlo simulations with random initializations. Fig. 2 (b)
shows the average estimation error with respect to the sample complexity when K = 3 and
d = 20, 35, 50 respectively. It can be seen that the estimation error decreases gracefully as we
increase the sample size and matches with the theoretical prediction of error rates reasonably well.
We further compare the performance of gradient descent algorithm applied to both the cross entropy
loss and the squared loss, respectively. As shown in Fig 2 (c), when K = 3, d = 20, cross entropy
loss with gradient descent achieves a much lower error than the squared loss. Clearly, the cross
entropy loss is favored in the classification problem over the squared loss.
6 Conclusions
In this paper, we have studied the model recovery of a one-hidden-layer neural network using the
cross entropy loss in a multi-neuron classification problem. In particular, we have characterized
the sample complexity to guarantee local strong convexity in a neighborhood (whose size we have
characterized as well) of the ground truth when the training data are generated from a classification
model. This guarantees that with high probability, gradient descent converges linearly to the ground
truth if initialized properly. In the future, it will be interesting to extend the analysis in this paper
to more general class of activation functions, particularly ReLU-like activations; and more general
network structures, such as convolutional neural networks (Du et al., 2017b; Zhong et al., 2017a).
8
Under review as a conference paper at ICLR 2019
References
Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE
Transactions on Information theory, 39(3):930-945,1993.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pp. 6241-6250, 2017.
Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Global optimality of local search for
low rank matrix recovery. In Advances in Neural Information Processing Systems, pp. 3873-3881,
2016.
Digvijay Boob and Guanghui Lan. Theoretical properties of the global optimizer of two layer neural
network. arXiv preprint arXiv:1710.11241, 2017.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian
inputs. arXiv preprint arXiv:1702.07966, 2017.
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. Sgd learns over-
parameterized networks that provably generalize on linearly separable data. arXiv preprint
arXiv:1710.10174, 2017.
E.J. Candes, X. Li, and M. Soltanolkotabi. Phase retrieval via Wirtinger flow: Theory and algorithms.
IEEE Transactions on Information Theory, 61(4):1985-2007, April 2015.
George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control,
signals and systems, 2(4):303-314, 1989.
Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua
Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex
optimization. In Advances in neural information processing systems, pp. 2933-2941, 2014.
Simon S Du, Jason D Lee, and Yuandong Tian. When is a convolutional filter easy to learn? arXiv
preprint arXiv:1709.06129, 2017a.
Simon S Du, Jason D Lee, Yuandong Tian, Barnabas Poczos, and Aarti Singh. Gradient descent learns
one-hidden-layer cnn: Don’t be afraid of spurious local minima. arXiv preprint arXiv:1712.00779,
2017b.
Soheil Feizi, Hamid Javadi, Jesse Zhang, and David Tse. Porcupine neural networks:(almost) all
local optima are global. arXiv preprint arXiv:1710.02196, 2017.
Rong Ge and Tengyu Ma. On the optimization landscape of tensor decompositions. arXiv preprint
arXiv:1706.05598, 2017.
Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. In
Advances in Neural Information Processing Systems, pp. 2973-2981, 2016.
Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape
design. arXiv preprint arXiv:1711.00501, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are
universal approximators. Neural networks, 2(5):359-366, 1989.
Serge Lang. Real and functional analysis. Springer-Verlag, New York,, 10:11-13, 1993.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436-444,
2015.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation.
In Advances in Neural Information Processing Systems, pp. 597-607, 2017.
9
Under review as a conference paper at ICLR 2019
Cong Ma, Kaizheng Wang, Yuejie Chi, and Yuxin Chen. Implicit regularization in nonconvex
statistical estimation: Gradient descent converges linearly for phase retrieval, matrix completion
and blind deconvolution. arXiv preprint arXiv:1711.10467, 2017.
Song Mei, Yu Bai, and Andrea Montanari. The landscape of empirical risk for non-convex losses.
arXiv preprint arXiv:1607.06534, 2016.
Marco Mondelli and Andrea Montanari. On the connection between learning two-layers neural
networks and tensor decomposition. arXiv preprint arXiv:1802.07301, 2018.
Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. In Interna-
tional Conference on Machine Learning, pp. 2603-2612, 2017.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition
challenge. International Journal of Computer Vision, 115(3):211-252, 2015.
Itay Safran and Ohad Shamir. On the quality of the initial basin in overspecified neural networks. In
International Conference on Machine Learning, pp. 774-782, 2016.
Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks.
arXiv preprint arXiv:1712.08968, 2017.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484-489, 2016.
Mahdi Soltanolkotabi. Learning relus via gradient descent. arXiv preprint arXiv:1705.04591, 2017.
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization
landscape of over-parameterized shallow neural networks. arXiv preprint arXiv:1707.04926, 2017.
J. Sun, Q. Qu, and J. Wright. Complete dictionary recovery using nonconvex optimization. Interna-
tional Conference on Machine Learning, pp. 2351-2360, 2015.
Ruoyu Sun and Zhi-Quan Luo. Guaranteed matrix completion via non-convex factorization. IEEE
Transactions on Information Theory, 62(11):6535-6579, 2016.
Matus Telgarsky. benefits of depth in neural networks. In Conference on Learning Theory, pp.
1517-1539, 2016.
Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its
applications in convergence and critical point analysis. arXiv preprint arXiv:1703.00560, 2017.
R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. Compressed Sensing,
Theory and Applications, pp. 210 - 268, 2012.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
arXiv:1011.3027, 2010.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
Kai Zhong, Zhao Song, and Inderjit S Dhillon. Learning non-overlapping convolutional neural
networks with multiple kernels. arXiv preprint arXiv:1711.03440, 2017a.
Kai Zhong, Zhao Song, Prateek Jain, Peter L. Bartlett, and Inderjit S. Dhillon. Recovery guarantees
for one-hidden-layer neural networks. In Proceedings of the 34th International Conference on
Machine Learning, volume 70, pp. 4140-4149, 2017b.
10
Under review as a conference paper at ICLR 2019
A	Proof of Theorem 1
To begin, denote the population loss function as
f(W)= E [fn(W)]= E [' (W; x)],	(8)
where the expectation is taken with respect to the distribution of the training sample (x; y).
The proof of Theorem 1 follows the following steps:
1.	We first show that the Hessian V2f (W) of the population loss function is smooth with respect to
V2f(W?) (Lemma 1);
2.	We then show that V2f(W) satisfies local strong convexity and smoothness in a neighborhood of
W?, B(W?, r) with appropriately chosen radius by leveraging similar properties of V2f(W?)
(Lemma 2);
3.	Next, we show that the Hessian of the empirical loss function V2fn(W) is close to its popular
counterpart V2f(W) uniformly in B(W?, r) with high probability (Lemma 3).
4.	Finally, putting all the arguments together, we establish V2fn(W) satisfies local strong convexity
and smoothness in B(W?, r).
We will first show that the Hessian of the population risk is smooth enough around W? in the
following lemma.
Lemma 1. For sigmoid activations, assume kW? kF ≤ 1, we have
kV2f (W) - V2f (W?) k ≤ 台∙ k W - W?kF,	(9)
K 2
holds for some large enough constant C, when kW - W? kF ≤ 0.7.
The proof is given in Appendix D.2. Lemma 1 together with the fact that V2f (W ?) be lower and
upper bounded, will allow us to bound V2f(W) in a neighborhood around ground truth, given below.
Lemma 2 (Local Strong Convexity and Smoothness of Population Loss). For sigmoid activations,
there exists some constant C, such that
若∙ ρ⅞K) ∙ I W V2f(W) W C ∙ I,
K2	κ2λ
holdsfor all W ∈ B(W?,r) with r := min {旨∙ %K),。彳}∙
The proof is given in Appendix D.3. The next step is to show the Hessian of the empirical loss
function is close to the Hessian of the population loss function in a uniform sense, which can be
summarized as following.
Lemma 3. For sigmoid activations, there exists constant C such that as long as n ≥ C ∙ dK log dK,
with probability at least 1 - d-10, the following holds
sup	kv2fn (W) - V2f (W?) k ≤ CrdKlogn ,
W ∈B(W ? ,r)	n
where r := min { -C ∙ pKK, 07}∙
The proof can be found in Appendix D.4.
The final step is to combine Lemma 3 and Lemma 1 to obtain Theorem 1 as follows,
Proof of Theorem 1. By Lemma 3 and Lemma 2, we have with probability at least 1 - d-10,
V2fn(W)之 V2f (W) - ∣∣V2fn (W) - V2f(W)∣∣ ∙ I
占 Ω (K2 ∙笔K) ∙ I - Ω (c ∙严叵)∙ I.
11
Under review as a conference paper at ICLR 2019
As long as the sample size n is set such that
C IdK log n / 1 P (σκ)
C ∙ V -n- ≤ K2 ∙ ~72Γ,
i.e. n ≥ C ∙ dK5 log2 d ∙ (^^) 2 , we have
v2fn(W) 七 ω (K ^ ρ⅛1) J
holds for all W ∈ B (W?, r). Similarly, we have
V2fn(W) W C ∙ I
holds for all W ∈ B (W?, r).
□
B Proof of Theorem 2
We have established that fn (W) is strongly convex in B(W?, r) in Theorem 1, thus there exists at
most one critical point in B(W?, r). The proof of Theorem 2 follows the steps below:
1.	We first show that the gradient vfn (W) concentrates around vf (W) in B(W?, r) (Lemma 4),
and then invoke (Mei et al., 2016, Theorem 2) to guarantee there indeed exists a critical point Wn
in B(W?, r);
2.	We next show Wn is close to W? and gradient descent converges linearly to Wn with a properly
chosen step size.
The following lemma establishes that vfn (W) uniformly concentrates around vf (W).
Lemma 4. For sigmoid activation function, assume kW?kF ≤ 1, there exists constant C such that
as long as n ≥ CdK log(dK), with probability at least 1 - d-10, the following holds
sup	kVfn (W) - Vf(W)k ≤ Cj√Klogn
W ∈B(W ?,r)	n
% *, 0.7o.
where r := min
Notice that for the population risk function, f(W), W? is the unique critical point in B(W?, r) due
to local strong convexity. With Lemma 3 and Lemma 4, we can invoke (Mei et al., 2016, Theorem 2),
which guarantees the following.
Corollary 1. There exists one and only one critical point Wn ∈ B (W*,r) that satisfies
vfn (Wcn) =0.
We first show that Wcn is close to W?. By the intermediate value theorem, ∃W0 ∈ B (W?, r) such
that
fn Wcn =fn(W?)+ Vfn(W?),vec Wcn -W?
+ 2vec (Wn - W?)> V2fn (W0) Vec (Wn - W?)
≤ fn (W?),
where the last inequality follows from the optimality of Wcn. By Theorem 1, we have
2vec (Wn - W?)> V2fn (W0)vec (Wn - W?) ≥ Ω (K2 ∙ ρKσKl) ∣∣Wn
(10)
- W?2F .
(11)
12
Under review as a conference paper at ICLR 2019
On the other hand, by the Cauchy-Schwarz inequality, we have
KVfn (W ? ) , Vec (Wn - W ?)E| ≤ kVfn (W ?) k 2 k Wn - W *∣∣F
≤ Ω rd dK 1/n log n! kccn - W ? kF,	(12)
where the last line follows from Lemma 4. Plugging (11) and (12) into (10), we have
kWn- W*kF ≤ Ω Kκ9κ2λrdogn! .	(13)
ρ (σK)	n
Now we have established there indeed exists a critical point in B(W ?, r). We can establish local linear
convergence of gradient descent as below. Let Wt be the estimate at the t-th iteration. According to
the update rule, we have
Wt+1 -
Wcn2 = Wt -ηVfn(Wt) -
Wcn
2
F
kWt -	Wcnk2F	+ η2kVfn	(Wt) k2F	-2η	Vfn	(Wt), Vec	Wt	-Wcn	.
(14)
Moreover, by the fundamental theorem of calculus (Lang, 1993), Vfn (Wt) can be written as
Vfn(Wt) = Vfn (Wt) -Vfn Wcn
= Z 1V2fn (Wcn+γ(Wt -Wcndγ Vec(Wt - Wcn ,
where W(γ) = Wcn + γ Wt - Wcn for γ ∈ [0, 1]. By Theorem 1, we have
Hmin ∙ I W V2 fn (W (Y)) W Hmax ∙ I,
where Hmin = Ω (κ12 ∙ pκσK)) and Hmax = C. Therefore, We have
kVfn (Wt) k2F ≤ Hm2 axWt - Wcn2F.
Hence,
kWt+1 -Wcnk2F ≤
≤
(1 - 2ηHmm + η2H2ax) k Wt — WnkF
(l- 2ηHmin) kWt — WnkF
as long as We set η < HHmn := Ω
max
the local minimizer Wcn .
PgK)、
~K2λ)
In summary, gradient descent converges linearly to
C Proof of Theorem 3
The proof contains tWo parts. Part (a) proves that the estimation of the direction of W? is sufficiently
accurate, Which folloWs the arguments similar to those in (Zhong et al., 2017b) and is only briefly
summarized beloW. Part (b) is different, Where We do not require the homogeneous condition for the
activation function, and instead, our proof is based on a mild condition in Assumption 2. We detail
our proof in part (b).
We first define a tensor operation as folloWs. For a tensor T ∈ Rn1 ×n2×n3 and three matrices
A ∈ Rn1×d1 , B ∈ Rn2 ×d2, C ∈ Rn3×d3, the (i, j, k)-th entry of the tensor T (A, B, C) is given
by
n1 n2 n3
Ti0,j0,k0Ai0,iBj0,jCk0,k.	(15)
i0 j0 k0
13
Under review as a conference paper at ICLR 2019
(a)	In order to estimate the direction of each wi for i = 1, . . . , K, (Zhong et al., 2017b) shows that
for the regression problem, if the sample size n ≥ dpoly (K, κ, t, log d), then
∣∣wi? - SiVuiIl ≤ EPoly (K, κ)	(16)
holds with high probability. Such a result also holds for the classification problem with only slight
difference in the proof as we describe as follows. The main idea of the proof is to bound the estimation
error of P2 and R3 via Bernstein inequality. For the regression problem, Bernstein inequality was
applied to terms associated with each neuron individually, and the bounds were then put together via
triangle inequality in (Zhong et al., 2017b), whereas for the classification problem here, we apply
Bernstein inequality to terms associated with all neurons all together. Another difference is that the
label yi of the classification model is bounded by nature, whereas the output yi in the regression
model needs to be upper bounded via homogeneously bounded conditions of the activation function.
A reader can refer to (Zhong et al., 2017b) for the details of the proof for this part.
(b)	In order to estimate ∣wi ∣ for i = 1, . . . , K, we provide a different proof from (Zhong et al.,
2017b), which does not require the homogeneous condition on the activation function, but assumes a
more relaxed condition in Assumption 2.
We define a quantity Q1 as follows:
Qi = Mii (I, α,…
(17)
'---{z----}
(l1-1)
where l1 is the first non-zero index such that Ml1 6= 0. For example, if l1 = 3, then Q1 takes the
following form
1K	2
QI = M3 (I, α, α) = K X m3,i(kWik) (α>Wi) wi,
K i=1
where w = w∕∣w∣ and by definition
m3,i(kw?k) = E [φ (1w?k ∙ Z) z3] - 3E [φ (1w?k ∙ Z) z].
(18)
(19)
Clearly, Qi has information of ∣wi* ∣, which can be estimated by solving the following optimization
problem:
K
1
β? = argminβ∈RK K E βiSiW - Qi ,
i=i
where each entry of the solution takes the form
β* = s3m3,i(kw* k) (αT siwi*)2 .
(20)
(21)
τ ..	∙	...	1.	..	1	...	. Λ Z ..	.一	,	∙	∙	一 、C /A ɪrʌ Z ..	...	.
In the initialization, we substitute Qi (estimated from training data) for Qi, VUbi (estimated in part
^
^
(a)) for SiW* into (20), and obtain an estimate β of β?. We then substitute β for β? and Vui for
SiW* into (21) to obtain an estimate bi of ∣∣w*∣ via the following equation
βbi = s3m3,i(bi) (ατVUi)2 .
(22)
Furthermore, since ml1,i(x) has fixed sign for x > 0 and for li ≥ 1, Si can be estimated correctly
from the sign of βi for i = 1, . . . , K.
For notational simplicity, let β*,i := §39%；W?)2 and βi,i :
become
ʌ
βi
s3(ατ V Ui)
2 , and then (21) and (22)
**
βi,i = m3,i (abi), βi,i = m3,i (∣wi ∣).
(23)
By Assumption 2 and (21), there exists a constant δ0 > 0 such that the inverse function g(∙) of m3,1(∙)
has upper-bounded derivative in the interval (βi*,i - δ0, βi*,i + δ0), i.e., |g0(x)| < Γ for a constant Γ.
By employing the result in (Zhong et al., 2017b), if the sample size n ≥ dPoly (K, κ, t, log d), then
?	?	*	1	*	r	r r
Qi and Qi, VUi and SiWi can be arbitrarily close so that ∣β*,i - βι,i∣ < mm{δ0, √=τ}.
Thus, by (23) and mean value theorem, we obtain
. ^
∣bi-kw*k∣ = ∣g0(ξ)∣∣β*,i -βι,i∣
(24)
1	A ♦ F ,	CA	ι 公	t t	I / / >≈∖ I ,	I ■	EI	Γ∙	l^	Il Alll / r
where ξ is between β*,i	and βι,i, and hence	∣g0(ξ)∣ <	Γ.	Therefore,	|bi	- ∣∣w*∣∣ ≤	√^, which is
the desired result.
14
Under review as a conference paper at ICLR 2019
D Proof of Technical Lemmas
D.1 Preliminaries
We introduce some useful definitions and results that will be used in the proofs. The first one is the
definition of norms of random variable, i.e.
Definition 4 (Sub-gaussian and Sub-exponential norm). The sub-gaussian norm ofa random variable
X, denotes as kXkψ2, is defined as
11
∣∣Xkψ2=supP-2 (E[∣X∣p])p ,	(25)
p≥1
and the sub-exponential norm of X, denoted as kX kψ1 , is defined as
kX kψ1 =sup P-1(E[∣X∣p])1 .	(26)
p≥1
The definition is summarized from (Vershynin, 2012, Def 5.7,Def 5.13), and if kX kψ2 is upper
bounded, then X is a sub-gaussian random variable and it satisfies
P (X| > t) ≤ exp(1 - ct2∕∣∣Xkψ2) for all t ≥ 0.	(27)
Next we provide the calculations of the gradient and Hessian of E [` (W; x)]. Let’s denote p (W)
-K PK=ι φ (w>x), and then
E Γd'(W； X) ] = E -ɪ I	pK=ι φ (w?>x) -	pK=ι φ (w>x)
I	dwj」—[K I" PK=ι Φ (w>x))(l-Kk PK=ι Φ (w>x))
E
p (W?) - p (W)
P (W )(1 - P (W))
E ΓNY (W； x)
∂wj∂wl
ξj,ι (W)
(P (W)(1 - P (W)))2
• xx>
(28)
(29)
E
where if j 6= l,
ξj,ι (W) = K2φ0 (w>x) φ0 (w>x) ∙ (P (W)2 + P (W?) - 2p (W*) P (W)),
and if j = l,
ξj,j (W) = Kφ0 (w>x)2 ∙ (p (W)2 + p (W?) - 2p (W?) p (W))
-KKφ" (Wjx) (p (W?) - P (W)) (p (W) (1 - P (W))).
D.2 Proof of Lemma 1
Proof. Let ∆ = N2f(W) - N2f(W?). For each (j, l) ∈ [K] × [K], let ∆j,l ∈ Rd×d denote the
(j, l)-th block of ∆. Let a = [a1>, • • • , a>K]> ∈ RdK. Since by definition,
kN2f(W) - N2f(W?)k
max a>(N2f(W) - N2f(W?))a
kak=1
KK
kmaka=x1Xj=1Xl=1aj>∆j,lal.
(30)
15
Under review as a conference paper at ICLR 2019
Next we will evaluate ∆j,l. From (29) we can write the hessian block more concisely as
fW=E [gj,l(W) ∙xx>,
j
(31)
where gj,ι (W) =(p(w；(；-WW)))2 ∈ R, and then by the mean value theorem, we can write gj,ι (W)
as	P P
gj,l(W)=gj,l(W?)+XK
k=1
∂wek
, wk - wk?
(32)
where W = η ∙ W + (1 — η) W? for some η ∈ (0,1). Thus we can calculate ∆j,ι as
∆	= ∂2f(W) - ∂2f (W?)
j,l	∂ wj∂wl	∂ wj∂ w?
=E [gj,ι (W) ∙ xxτ] - E [gj,ι (W?) ∙ xx>]
E
X ^jW-, wk - w?)). xxτ
(33)
and plug it back to (30) we can obtain
kV2f(W )-V2f(W ?)k
KK
= kmaka=x1XXaj>∆j,lal
j=1 l=1
max
kak=1
max
kak=1
KK
XXE
KK
XXE
kXK=1
∂gj,l
∂we
W
wk - w?) I ∙ (a>x) (a>x)
Tj,l,k hx, wk - w?i	∙ (a>x) (a>x)
K K u
≤ max
kak=1 j=1 l=1
K
XTj2,l,k
k=1
K
X (hx, wk — w?i)2 (a>χ)2 (a>χ)2
k=1
E
∖
E
KK
≤ max
kak=1
XXtX E [Tj2,1,k] ∙ tX kwk -
j=1 l=1
w?k2 ∙ llɑjk2 ∙ kaιk2,
(34)
k=1
k=1
K
∖
K
for the third equality we have used the fact that
∂gj,l(W )
∂Wk
can be written as Tj,ι,k ∙ x, where Tj,ι,k ∈ R,
since the variable of gj,l
is in the form of wi> x. and for the last two inequalities, we have used
Cauchy-Schwarz inequality. Our next goal is to upper bound E Tj2,l,k . Further since
∂gj,l(W)
∂wk
1	∂
K2 •一
φ0(w> x)φ0(w>x)∙(p(W )2+p(W ? )-2P(W ?)p(W))
(P(W )(i-PeWy)T
∂wk
which aligns with X and the scalar coefficient is upper bounded by 木∙
C
(p(W )(1-P(W )))3,
since
φ (∙), φ0 (∙), φ00 (∙) are all upper bounded, thus We leave only the denominator. And then
E [T2ι,k] ≤ K ∙ E
≤ — ∙ ekWkF
≤ K4 e ,
holds for some constant C, where the second inequality follows from Lemma 5.
(35)
16
Under review as a conference paper at ICLR 2019
Lemma 5. Let X 〜N (0, I), t = max {∣∣w1∣∣2,…∣∣wκ∣∣2} and Z ∈ Z such that Z ≥ 1, for the
sigmoid activation function φ (x)= 订土—三,thefollowing
E ------------------------------------------r ≤ C ∙ et2,	(36)
[∖PK ∑K=1 φ (w>x) (1 - KK ∑K=1 φ (w> x)) ) _
holds for a large enough constant C which depends on the constant Z.
Plugging (35) into (34), we can obtain
∣v2f (W) - V2f (W?)k ≤ -C3ekWkF ∙ k W - W?kF ∙ max XXXX kaj∣2kaι∣2
K2	kak=1 M M
≤ ~C1 ekWkF ∙ ∣W - W?kF,
一K 2
(37)
2
Further since ekWkF ≤ C ∙ (1 + ∣∣ W 一 W?∣∣f) when ∣∣ W 一 W?|f ≤ 0.7, where We have used
the assumption that ∣W? ∣F ≤ 1 thus we can conclude that if ∣W - W? ∣F ≤ 0.7, then
∣v2f(W) - V2f(W?)∣ ≤二∣W - W*∣∣F	(38)
K 2
holds for some constant C.	□
D.3 Proof of Lemma 2
Proof. We will first present upper and lower bounds of the Hessian of the population risk at ground
truth, i.e. v2f(W?), and then apply Lemma 1 to obtain a uniform bound in the neighborhood of
W? . As a reminder,
∂2f (W?)
∂wj
∂2f(W ?) = E
∂wj∂wl
1
K2
1
K2
φ0 wj?> x φ0 wl?> x
(-K PK=1 φ (w?>x)) (1 - -K PK=I φ (w?>x))
(39)
(40)
and let a = [a],…，aK]> ∈ RdK, We can write
v2f(W?)A	min aτV2f (W?) a ∙ I
kak2=1
min ɪ E
kak2=1K2
(PK=I φ (w?>x) (a>x))
4
A min --E
kak2=1K2
"ΣK=ι φ (w?Tx))(I- KK ΣK=ι φ (w?Tx)
(X φ (w?Tx) (aτx))
(41)
the second inequality holds due to the fact that(K∙ ΣK=ιφ (w?Tx))(I- KK ΣK=ιφ (w?Tx)) ≤
4, and the last inequality follows from (Zhong et al., 2017b, Lemmas D.4 and D.6).
A W ∙答∙ I
17
Under review as a conference paper at ICLR 2019
Further more, We can UPPder bound V2f (W?) as
V2f (W?)
by Cauchy-SchWarz inequality
1	CK2 K > 2
Y max —^-E ---〉 Iai x)	∙ I
kak2=1K2	4	i
i=1
C ∙ I,
(42)
where for the third and fourth inequality we have used the fact that φ (w?> x)(1 - φ (w?>x)) ≤ 4
and
KK	K	K
XXφ(w?>x) (1-φ(w?>x)) ≥ Xφ(w?>x) (1-φ(w?>x)) = Xφ(w?>x).
i=1 j=1	i=1	i=1
Thus together with the lower bound (41) we can conclude that
W ∙ p(σK ∙ I YV2f (W?) Y C ∙ I,
K2	κ2λ
From Lemma 1, we have
C
-1 kW - W?kF,
K 2
therefore, when kW? - WkF ≤ 0.7 and
今∙kW - W?kF ≤ W ∙ ρ(σK),
K 2	K2 κ2λ
i.e., when ∣∣ W - W*∣∣f ≤ min {旨∙ PKK), 0"} for some constant C, we have
σmin (V2f (W)) ≥ σmin (V2f (W?)) -∣V2f (W) - V2f (W?) k
& JL ∙P(σκ) - CL∣W - W?k & JL ∙P(σκ)
& K2	κ2λ	k 1 k	kF & K2	κ2λ .
Moreover, within the same neighborhood, by the triangle inequality we have
kV2f(W)k ≤ kV2f(W)-V2f(W?)k+kV2f(W?)k .C.
(43)
(44)
(45)
(46)
(47)
□
D.4 Proof of Lemma 3
Proof. We adaPt the analysis in (Mei et al., 2016) to our setting. Let N be the -covering number
of the Euclidean ball B (W?, r). It is known that log N ≤ dK log (3r/) (Vershynin, 2010).
Let We = {Wι, •…,WN } be the E-COVer set with Ne elements. For any W ∈ B (W?, r), let
j (W) = argminj∈[Ne] kW - Wj(w)||f ≤ E forall W ∈ B (W?, r).
18
Under review as a conference paper at ICLR 2019
For any W ∈ R (W?, r), we have
1 n
∣∣v2fn (W) -V2/(W)∣∣ ≤ - £[v2,(W;g)-V2'(W∙(W);xi)]
i=1
1 二。，	、	…L
+ - £ v2' (Wy(W); Xi) - E [V2' (W7.(w); x)]
n i=1
+ ∣∣E [V24 (Wy(W); x)] - E [V24 (W; x)]∣∣.
Hence, we have
P( sup	∣∣V2∕n (W) -V2 f (W )∣∣ ≥ t ) ≤ P(At)+ P (Bt) + P(G),
W∈ ∈B(W ?,r)	J
where the events At, Bt and Ct are defined as
ZI /	1
At = < sup —
(W∈B(W*,r) n
n
X [V24 (W; Xi)- V2' (Wy(W); Xi)]
i=1
≥ l),
Bt =	sup
[w ∈We
1 3 C .	LC.	「
-E V2' (W; Xi) - E [V22 (W; x)]
i=1
Ct
sup
(W ∈B(W *,r)
∣∣E[V2' (W∙(W); x)] - E [V22 (W; x)] k ≥
t}.
(48)
(49)
(50)
(51)
t
≥ —
一3
,
In the sequel, we will bound the terms P (At), P (Bt), and P (Ct), separately.
1.	Upper bound P (Bt). Before continuing, let us state a simple technical lemma that is useful for
our proof, whose proof can be found in (Mei et al., 2016).
Lemma 6. Let M ∈ Rd×d be a SymmetriC d × d matrix and Ve be an E-cover of unit-Euclidean-
norm ball B (0,1) ,then
l∣MIl ≤	1	sup | hv, Mυ) |.	(52)
1 - 2e v∈ye
Let Vι be a G)-cover of the ball B(0,1) = {W ∈ Rd×K : ∣ W∣∣f = 1}, where log |V11 ≤
dK log 12. From Lemma 6, we know that
1 n
-E V2' (W; xi) - E [V24 (W; x)]
i=1
≤ 2 sup
v∈ V1
4
卜 G X V2' (W； Xi)-
E [V2' (W; x)]) V).
Taking the union bound over We and Vi yields
(53)
P (Bt) ≤ P	sup
y w ∈we,v∈vi
1 n
—£〈V，(V2' (W; Xi) - E [V2' (W; x)]) V)
n i=1
≤ edK(log 誓 +log 12) sup P ( 11 X〈v, (V2' (W; Xi) - E [V2' (W; x)]) v)
w∈we,v∈vi y∣n 片
t
≥ —
一6
(54)
Let Gi =〈v, (V2' (W; Xi) - E [V2' (W; x)]) V) whereE[Gi] = 0. Let a = [a>,…，ɑɪ] ∈
RdK. Then we can show that 快/|耽 is upper bounded, which we summariz as follows.
Lemma 7. There exists some constant C such that
IlGikψι ≤ C :≡ τ2.
19
Under review as a conference paper at ICLR 2019
Applying the Bernstein inequality for sub-exponential random variables (Mei et al., 2016, Theorem
9) to (54), We have for fixed W ∈ We, V ∈ Vi,
4
P (F XX(V, (V2'(W; Xi) - E [V2' (W; x)]) V)
≤ 2 exp
(55)
for some universal constant c. As a result,
P (Bt) ≤ 2 exp (—c ∙ n ∙ min (1,g) + dK log — + dK log 12).	(56)
Thus as long as
t>C ∙ max h IT 4(dK log 等 + log * 4), T 2(dK log 誓 +log 4)
nn
for some large enough constant C, We have P (Bt) ≤ 2.
2.	Upper bound P (At) and P (Ct). These tWo events Will be bounded in a similar Way. Let J?
satisfy
kV2' (W, x) -V2' (W0, x) k V
W=W0∈B(W?,r)	kW - W0kF	— ?
(58)
Let us look at the deterministic event Ct first. Since
sup	kE [V2' (Wj(w)； x)] - E [V2' (W; x)] k
W ∈B(W ?,r)
≤ Su kE[V2' (Wj(W); x)] - E[V2' (W; x)] k
—W∈B(W?,r)	k W - Wj(W) kF
≤ J? ∙ e.
sup kW - Wj(W) kF
W∈B(W? ,r)
(59)
Therefore, Ct holds as long as
t ≥ 3J? ∙ e.	(60)
We can bound the event At as beloW.
P (At) = P ( sup -
W∈B(W? ,r) n
3
≤ -E sup
t	W ∈B(W ?,r)
(61)
n	t
£[V2' (W; Xi)-V2' (Wy(W); xi)] ≥
i=1	3
1 n
-E [V2' (W; Xi)- V2' (Wj(W); Xi)]
n i=1
3
≤ -E	SuP	IlV2' (W; Xi)- V2' (Wj(W); Xi)Il
t	W ∈B(W ?,r)
≤ -E	sup	kV2' (W； Xi)-V2' (Wj(W)； Xi) k
t	W ∈B(W ?,r)	kW-Wj(W)kF
sup	kW - Wj(W)kF
W∈B(W? ,r)
J
t
(62)
Where (61) folloWs from the Markov inequality. Thus, taking
6J?
t ≥ 丁
(63)
ensures that P (At) ≤ δ. It now boils down to control the quantity J?, which we have the following
lemma, Whose proof is in Appendix E.3.
20
Under review as a conference paper at ICLR 2019
Lemma 8. There exists some constant C such that
E
sup
W 6=W 0∈B(W ?,r)
∣∣V2' (W, x) -V2' (W0, x) k
∣W - W0∣F
≤ C ∙ d√K ≡ J?.
(64)
3.	Final step. Let E = 6」：TdK, δ = d-10 plugging into (57) we need
t>τ 2 ∙max ∖ nd1K，C T
(dK log(36rnd11K) + log 4) (dK log(36rnd11K) + log 4)
n
n
Since the middle term can be expressed as
dK log(36rnd11K) + 10log d V dK log n + dK log 36r + 11dK log dK + 10log d
(65)
n
n
n
n
n
when n ≥ C ∙ dK log dK for some large enough constant C, the first term, dK log n dominants
and is on the order of dK log dK. Moreover, it decreases as n increases when n ≥ 3. Thus we can
set
t≥τ2
IdK log(36rnd11K) + log 4)
(67)
which holds as t ≥ C0 ∙ τ2 JdKyg n for some constant C0.
By setting t := Cτ2 JdKjg n for sufficiently large C, as long as n ≥ C0 ∙ dK log dK,
P f SUp	∣V2fn (W) - V2f(W )k ≥ Cτ 2A ldK log n ) ≤ d-10.
W ∈B(W ?,r)	n
(68)
D.5 Proof of Lemma 4
Proof. We nedd the following Lemma for the proof. Lemma 9
Lemma 9. Assume X 〜N (0, I). Let U be a fixed unit norm vector U = [u],…，uK] ∈ RdK
with ∣u∣2 = 1, the following
∣u>V' (W; x) ∣ψ2 ≤√K,
hold.
By a similar argument (details omitted) as the proof of Lemma 3, and applies Lemma 9, we can get
the following concentration inequality:
sup	∣Vfn (W) -Vf (W) ∣2 ≤ C ∙1 ∕d√Klogn,
W ∈B(W ? ,r)	n
holds with probability at least 1 一 d-10, as long as the sample size n ≥ C ∙ dK log(dK).
(69)
E Proof of Auxiliary Lemmas
E.1 Proof of Lemma 5
Proof. We can rewrite the left-hand side as
E (K2xxφ(w>χ)(1 -φ(w>χ))
i=1 j=1
-z
(70)
n
□
□
21
Under review as a conference paper at ICLR 2019
which is upper bounded by E [K2 PK=I PK=I (φ (w›x)(1 - φ (w>x)))-z], since f (x)=
is convex for x > 0 and z ≥ 1. And apply Cauchy-Schwarz inequality we can have
E [(0 (w> x)(1—φ(w> X)))-Zi ≤ r h。(w>x)-2zi ∙ r h(1—φ (w>x))-2zi.
x-z
(71)
Further since φX) = 1 + e-x, 1—卜方)=1 + ex and g = w>X 〜N(0, σi2 = Ilwik2), then We can
exactly calculate the two terms in the above equation, i.e.,
E [φ (g)-2z] = E [(1 + e-g)2z] = E
(72)
and in the same way,
E [(1- Φ (g))-2zi = E [(1 + eg)2zi = X (2lz)e(七),	(73)
since g is a Gaussian random which is a symmetric random variable. Plugging this back into (71) we
can conclude that for t = max(kwι∣2, ∙∙∙ , kwκ∣∣2) andP ≥ 1,
E Il K
p
≤ C ∙ et2,
(74)
holds.
□
E.2 Proof of Lemma 7
Proof. The sub-exponential norm of Gi can be bounded as
∣Gikψι ≤ k〈u, V2' (W; Z) U ∣ψι + ∣V2f (W; Z) k,
where ∣∣V2f (W; Z) ∣∣ is upper bounded by C according to lemma 2, and denote the (j, l)-th block
of V2' (W; z) as aj,ι ∙ xx>, we can write
KK
k〈u,v2'(W；Z)U kψι ≤ XXkαj,ι ∙u>χχ>uι∣∣ψι
l=1 j =1
K K	1
≤ XXsup t-1 (EIaj,1 ∙ u>xx>u") t .	(75)
l=1 j =1 t≥1
Note that
• for j 6= l
1 φ0 (w>x) φ0 (w>x) ∙ (P (W)2 + y — 2y ∙ p (W))
aj,l = K2	(P (W )(1- P (W )))2	,
further since
P (W)2 + y - 2y ∙ P (W) ≤
P(W)2
(1 -P(W))2
then
K 2	(1-p(W ))2
1 φ0 (wj>x)φ0 (wl> x)
K2	P(W )2
MI ≤
p (w) > 2
P (W) ≤ 2
(76)
(77)
(78)
22
Under review as a conference paper at ICLR 2019
moreover,
,_ .o 11 £	， 丁 、\2 ι ,丁、	, 丁 、
p (W)= (κ∑Sφ (WJx)) ≥ Kφ (w>x)φ WJx)
1 W「∖2	1 , 「一
(1 -P(W))2 = (1 — K 工φ (WJx)) ≥ K (1 - φ (WJx))(I- φ (Wjx))
and recall that φ (x) (1 - φ (x)) = φ0 (x), together we can obtain
1-21-2
• for j = l:
1 φ0 (wjx) φ0 (WJx) ∙ (P (W)2 + y — 2y ∙ P (W)
l%" ≤ K2	(P (W)(1- P (W)))2
1 Φ" (WJx) (y - P (W))
+ K p (W )(1 - p (W)),
the first term is upper bounded by a constant, and for the second term
φ" (WJx) (1{y=i}- P (W)) J φ⅛⅛ ≤ K y = 0
P (W )(1 - P (W))	- ɪ φJ(jχ) ≤ K y = 1
(80)
(81)
where we have used the fact that the second derivative is φ0 (x) = φ (x) (1 - φ (x)) (1 - 2φ (x)),
the absolute value of which can be upper bounded by φ (x) or 1 - φ (x). Thus we can show that
M l≤ C
(82)
Finally, we conclude that
Ml ≤ C
holds for all j, l. And
K K	ι
k〈u, V24 (W; z) U ∣∣ψ1 ≤ C ∙ XXsup t-1E [| (UJxxτU∕ l)t]t
l=1j=1 t≥1
≤ c ∙ xxsup t_1 (rE [ (UJx) 2ti ∙ rE [(UTx)1j
KK
≤ C ∙ XX llu7' ∣2∣ul ∣∣2 ∙ suP
l=1j=1	p≥1
t-1 ((2t - 1)!!)t
KK
≤ C XX IIUjk2∣U112
l=1j=1
≤ CXX ∣Uj∣2 + Mk2
-l=1j=1	2
≤ C :≡ τ2
(83)
(84)
(85)
(86)
(87)
(88)
(89)
Thus we can conclude that
∣Gikψι ≤ C :≡ τ2
□
23
Under review as a conference paper at ICLR 2019
E.3 Proof of Lemma 8
Proof. As noted before, We can write the (j,l)-th block of V2' (W; z) as gj,ι (W) xx>, where
gj,l (W)
ξj,ι (W)
(P (W)(1 - P (W)))2
(90)
then we can obtain the following bound,
KK
kV2' (W； Z)- V2' (W0; z) k ≤ XX ∣gj,ι (W) - gj,ι (W0) ∣∙kxx>k.	(91)
j=1 l=1
Using the same method as shown in the proof of Lemma 1, we can upper bound |gj,l (W)-gj,l (W0) |
as
|gj,l(W)-gj,l(W0)| ≤
∙kxk2 ∙ √K ∙kW - W0kF
(92)
・ _____________ ，- _____________ ~ , > - ________________________ ________ 一一 ■
where W = ηW + (1 - η) W0 for η ∈ (0, 1). And thus, when kW - W0kF ≤ 0.7 we have
E
sup
W 6=W0
∣∣V2'(W) -V2' (W0) k
∣∣W - W0∣F
≤ -C ∙ K2 ∙ E
—K 2
(93)
≤ C ∙ d√K
Thus we only need to set J? ≥ C ∙ d√K for some large enough C.
□
E.4 Proof of Lemma 9
Proof. By definition, we have
hV' (W), Ui = X( d≡G
k=1	k
_ ι X	(	(y-k?Pi=Iφ(w>χ))∙φ0(w>x))	>
K k=1	∖ KK	PK=1 φ (w>x)(1- Ki PK=I	φ (w>x))J	k	，
and then we can upper bound the sub-gaussian norm as
∣ hV'(W),ui∣ψ2≤
φ0(w>x)∙u>x	u>x
(1-k1 PK=1 φ(w>x)) k
φ0 (w>x)∙u>x
uk>x
ψ2
≤ PkK=1 ∣uk>x∣ψ2
ψ2
≤ PkK=1 ∣uk>x∣ψ2
y = 0
y = 1
Thus we can have
K
khV'(W),uikψ2 ≤ X kuk∣2√K,	(94)
k=1
and conclude that the directional gradient is √K-SUb-GaUSsian.	□
24