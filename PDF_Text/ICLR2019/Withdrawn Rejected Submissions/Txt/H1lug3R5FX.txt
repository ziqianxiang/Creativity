Under review as a conference paper at ICLR 2019
On the Geometry of Adversarial Examples
Anonymous authors
Paper under double-blind review
Ab stract
Adversarial examples are a pervasive phenomenon of machine learning models
where seemingly imperceptible perturbations to the input lead to misclassifica-
tions for otherwise statistically accurate models. We propose a geometric frame-
work, drawing on tools from the manifold reconstruction literature, to analyze the
high-dimensional geometry of adversarial examples. In particular, we highlight
the importance of codimension: for low-dimensional data manifolds embedded
in high-dimensional space there are many directions off the manifold in which to
construct adversarial examples. Adversarial examples are a natural consequence
of learning a decision boundary that classifies the low-dimensional data mani-
fold well, but classifies points near the manifold incorrectly. Using our geometric
framework we prove (1) a tradeoff between robustness under different norms, (2)
that adversarial training in balls around the data is sample inefficient, and (3) suffi-
cient sampling conditions under which nearest neighbor classifiers and ball-based
adversarial training are robust.
1	Introduction
Deep learning at scale has led to breakthroughs on important problems in computer vi-
sion (Krizhevsky et al. (2012)), natural language processing (Wu et al. (2016)), and robotics (Levine
et al. (2015)). Shortly thereafter, the interesting phenomena of adversarial examples was observed.
A seemingly ubiquitous property of machine learning models where perturbations of the input
that are imperceptible to humans reliably lead to confident incorrect classifications (Szegedy et al.
(2013); Goodfellow et al. (2014)). What has ensued is a standard story from the security litera-
ture: a game of cat and mouse where defenses are proposed only to be quickly defeated by stronger
attacks (Athalye et al. (2018)). This has led researchers to develop methods which are provably ro-
bust under specific attack models (Madry et al. (2018); Wong & Kolter (2018); Sinha et al. (2018);
Raghunathan et al. (2018)). As machine learning proliferates into society, including security-critical
settings like health care (Esteva et al. (2017)) or autonomous vehicles (Codevilla et al. (2018)), it is
crucial to develop methods that allow us to understand the vulnerability of our models and design
appropriate counter-measures.
In this paper, we propose a geometric framework for analyzing the phenomenon of adversarial ex-
amples. We leverage the observation that datasets encountered in practice exhibit low-dimensional
structure despite being embedded in very high-dimensional input spaces. This property is colloqui-
ally referred to as the “Manifold Hypothesis”: the idea that low-dimensional structure of ‘real’ data
leads to tractable learning. We model data as being sampled from class-specific low-dimensional
manifolds embedded in a high-dimensional space. We consider a threat model where an adversary
may choose any point on the data manifold to perturb by in order to fool a classifier. In order
to be robust to such an adversary, a classifier must be correct everywhere in an -tube around the
data manifold. Observe that, even though the data manifold is a low-dimensional object, this tube
has the same dimension as the entire space the manifold is embedded in. Our analysis argues that
adversarial examples are a natural consequence of learning a decision boundary that classifies all
points on a low-dimensional data manifold correctly, but classifies many points near the manifold
incorrectly. The high codimension, the difference between the dimension of the data manifold and
the dimension of the embedding space, is a key source of the pervasiveness of adversarial examples.
Our paper makes the following contributions. First, we develop a geometric framework, inspired by
the manifold reconstruction literature, that formalizes the manifold hypothesis described above and
our attack model. Second, we highlight the role codimension plays in vulnerability to adversarial
1
Under review as a conference paper at ICLR 2019
Figure 1: Examples of the decision axis Λ2, shown here in green, for different data manifolds.
Intuitively, the decision axis captures an optimal decision boundary between the data manifolds. It,s
optimal in the sense that each point on the decision axis is as far away from each data manifold as
possible. Notice that in the first example, the decision axis coincides with the maximum margin line.
examples. As the codimension increases, there are an increasing number of directions off the data
manifold in which to construct adversarial perturbations. Prior work has attributed vulnerability to
adversarial examples to input dimension (Gilmer et al. (2018)). This is the first work that investigates
the role of codimension in adversarial examples. Interestingly, we find that different classification
algorithms are less sensitive to changes in codimension. Third, we apply this framework to prove
the following results: (1) we show that the choice of norm to restrict an adversary is important in that
there exists a tradeoff between being robust to different norms: We present a classification problem
where improving robustness under the ∣∣∙k∞ norm requires a loss of Ω(1-1∕√d) in robustness tothe
k ∙ k2 norm; (2) we show that a common approach, training against adversarial examples drawn from
balls around the training set, is insufficient to learn robust decision boundaries with realistic amounts
of data; and (3) we show that nearest neighbor classifiers do not suffer from this insufficiency, due
to geometric properties of their decision boundary away from data, and thus represent a potentially
robust classification algorithm. Finally we provide experimental evidence on synthetic datasets and
MNIST that support our theoretical results.
2	Related Work
This paper approaches the problem of adversarial examples using techniques and intuition from the
manifold reconstruction literature. Both fields have a great deal of prior work, so we focus on only
the most related papers here.
2.1	Adversarial Examples
Some previous work has considered the relationships between adversarial examples and high di-
mensional geometry. Franceschi et al. (2018) explore the robustness of classifiers to random noise
in terms of distance to the decision boundary, under the assumption that the decision boundary is
locally flat. The work of Gilmer et al. (2018) experimentally evaluated the setting of two concentric
under-sampled 499-spheres embedded in R500 , and concluded that adversarial examples occur on
the data manifold. In contrast, we present a geometric framework for proving robustness guaran-
tees for learning algorithms, that makes no assumptions on the decision boundary. We carefully
sample the data manifold in order to highlight the importance of codimension; adversarial examples
exist even when the manifold is perfectly classified. Additionally we explore the importance of the
spacing between the constituent data manifolds and sampling requirements for learning algorithms.
Wang et al. (2018) explore the robustness of k-nearest neighbor classifiers to adversarial examples.
In the setting where the Bayes optimal classifier is uncertain about the true label of each point,
they show that k-nearest neighbors is not robust if k is a small constant. They also show that if
k ∈ Ω(√dnlogn), then k-nearest neighbors is robust. Using our geometric framework we show a
complementary result: in the setting where each point is certain of its label, 1-nearest neighbors is
robust to adversarial examples.
2
Under review as a conference paper at ICLR 2019
The decision and medial axes defined in Section 3 are maximum margin decision boundaries. Hard
margin SVMs define define a linear separator with maximum margin, maximum distance from the
training data (Cortes & Vapnik (1995)). Kernel methods allow for maximum margin decision bound-
aries that are non-linear by using additional features to project the data into a higher-dimensional
feature space (Shawe-Taylor & Cristianini (2004)). The decision and medial axes generalize the
notion of maximum margin to account for the arbitrary curvature of the data manifolds. There have
been attempts to incorporate maximum margins into deep learning (Sun et al. (2016); Liu et al.
(2016); Liang et al. (2017); Elsayed et al. (2018)), often by designing loss functions that encourage
large margins at either the output (Sun et al. (2016)) or at any layer (Elsayed et al. (2018)). In con-
trast, the decision axis is defined on the input space and we use it as an analysis tool for proving
robustness guarantees.
2.2	Manifold Reconstruction
Manifold reconstruction is the problem of discovering the structure of a k-dimensional manifold
embedded in Rd, given only a set of points sampled from the manifold. A large vein of research in
manifold reconstruction develops algorithms that are provably good: if the points sampled from the
underlying manifold are sufficiently dense, these algorithms are guaranteed to produce a geomet-
rically accurate representation of the unknown manifold with the correct topology. The output of
these algorithms is often a simplicial complex, a set of simplices such as triangles, tetrahedra, and
higher-dimensional variants, that approximate the unknown manifold. In particular these algorithms
output subsets of the Delaunay triangulation, which, along with their dual the Voronoi diagram, have
properties that aid in proving geometric and topological guarantees (Edelsbrunner & Shah (1997)).
The field first focused on curve reconstruction in R2 (Amenta et al. (1998)) and subsequently in
R3 (Dey & Kumar (1999)). Soon after algorithms were developed for surface reconstruction in R3,
both in the noise-free setting (Amenta & Bern (1999); Amenta et al. (2002)) and in the presence of
noise (Dey & Goswami (2004)). We borrow heavily from the analysis tools of these early works,
including the medial axis and the reach. However we emphasize that we have adapted these tools to
the learning setting. To the best of our knowledge, our work is the first to consider the medial axis
under different norms.
In higher-dimensional embedding spaces (large d), manifold reconstruction algorithms face the
curse of dimensionality. In particular, the Delaunay triangulation, which forms the bedrock of algo-
rithms in low-dimensions, of n vertices in Rd can have UP to Θ(nddSe) SimPlices. To circumvent the
curse of dimensionality, algorithms were proposed that compute subsets of the Delaunay triangula-
tion restricted to the k-dimensional tangent sPaces of the manifold at each samPle Point (Boissonnat
& Ghosh (2014)). Unfortunately, Progress on higher-dimensional manifolds has been limited due
to the Presence of so-called “sliver” simPlices, Poorly shaPed simPlices that cause in-consistences
between the local triangulations constructed in each tangent sPace (Cheng et al. (2005); Boissonnat
& Ghosh (2014)). Techniques that Provably remove sliver simPlices have Prohibitive samPling re-
quirements (Cheng et al. (2000); Boissonnat & Ghosh (2014)). Even in the sPecial case of surfaces
(k = 2) embedded in high dimensions (d > 3), algorithms with Practical samPling requirements
have only recently been ProPosed (Khoury & Shewchuk (2016)). Our use of tubular neighborhoods
as a tool for analysis is borrowed from Dey et al. (2005) and Khoury & Shewchuk (2016).
In this PaPer we are interested in learning robust decision boundaries, not reconstructing the underly-
ing data manifolds, and so we avoid the use of Delaunay triangulations and their difficulties entirely.
In Section 6 we Present robustness guarantees for two learning algorithms in terms of a samPling
condition on the underlying manifold. These samPling requirements scale with the dimension of the
underlying manifold k, not with the dimension of the embedding sPace d.
3	The Geometry of Data
We model data as being samPled from a set of low-dimensional manifolds (with or without bound-
ary) embedded in a high-dimensional sPace Rd . We use k to denote the dimension of a manifold
M ⊂ Rd. The sPecial case of a 1-manifold is called a curve, and a 2-manifold is a surface. The
codimension ofM is d- k, the difference between the dimension of the manifold and the dimension
3
Under review as a conference paper at ICLR 2019
of the embedding space. The “Manifold Hypothesis” is the observation that in practice, data is often
sampled from manifolds, usually of high codimension.
In this paper we are primarily interested in the classification problem. Thus we model data as being
sampled from C class manifolds M1, . . . , MC, one for each class. When we wish to refer to the
entire space from which a dataset is sampled, we refer to the data manifold M = ∪1≤j≤C Mj . We
often work with a finite sample of n points, X ⊂ M, and we write X = {X1 , X2, . . . , Xn}. Each
sample point Xi has an accompanying class label yi ∈ {1, 2, . . . , C} indicating which manifold
Myi the point Xi is sampled from.
Consider a ∣∣ ∙ ∣∣p-ball B centered at some point C ∈ Rd and imagine growing B by increasing its
radius starting from zero. For nearly all starting points c, the ball B eventually intersects one, and
only one, of the Mi's. Thus the nearest point to C on M, in the norm ∣∣ ∙ ∣∣p, lies on Mi. (Note that
the nearest point on Mi need not be unique.)
The decision axis Λp of M is the set of points C such that the boundary of B intersects two or more
of the Mi, but the interior of B does not intersect M at all. In other words, the decision axis Λp is
the set of points that have two or more closest points, in the norm ∣∣ ∙ ∣∣p, on distinct class manifolds.
See Figure 1. The decision axis is inspired by the medial axis, which was first proposed by Blum
(1967) in the context of image analysis and subsequently modified for the purposes of curve and
surface reconstruction by Amenta et al. (1998; 2002). We have modified the definition to account
for multiple class manifolds and have renamed our variant in order to avoid confusion in the future.
The decision axis Λp can intuitively be thought of as a decision boundary that is optimal in the
following sense. First, Λp separates the class manifolds when they do not intersect (Lemma 7).
Second, each point of Λp is as far away from the class manifolds as possible in the norm ∣ ∙ ∣p.
As shown in the leftmost example in Figure 1, in the case of two linearly separable circles of equal
radius, the decision axis Λ2 is exactly the line that separates the data with maximum margin. For
arbitrary manifolds, Λp generalizes the notion of maximum margin to account for the arbitrary
curvature of the class manifolds.
Let T ⊂ Rd be any set. The reach rchp (T ; M) of M is defined as infx∈M,y∈T ∣x - y∣p. When
M is compact, the reach is achieved by the point on M that is closest to T under the ∣∣∙∣∣p norm.
We will drop M from the notation when it is understood from context.
Finally, an -tubular neighborhood of M is defined as M,p = {x ∈ Rd : infy∈M ∣x - y∣p ≤ }.
That is, Me,p is the set of all points whose distance to M under the metric induced by ∣∣ ∙ ∣∣p is less
than . Note that while M is k-dimensional, M,p is always d-dimensional. Tubular neighborhoods
are how we rigorously define adversarial examples. Consider a classifier f : Rd → [C] for M.
An -adversarial example is a point x ∈ Mi,p such that f (x) 6= i. A classifier f is robust to all
-adversarial examples when f correctly classifies not only M, but all of M,p. Thus the problem
of being robust to adversarial examples is rightly seen as one of generalization. In this paper we will
be primarily concerned with exploring the conditions under which we can provably learn a decision
boundary that correctly classifies M,p. When < rchp Λp, the decision axis Λp is one decision
boundary that correctly classifies M,p (Corollary 9). Throughout the remainder of the paper we
will drop the p in M,p from the notation, instead writing M; the norm will always be clear from
context.
The geometric quantities defined above can be defined more generally for any distance metric d(∙, ∙).
In this paper We will focus exclusively on the metrics induced by the norms ∣∣ ∙ ∣∣p for P > 0. The
decision axis under ∣∣ ∙ ∣∣2 is in general not identical to the decision axis under ∣∣ ∙ ∣∣∞. In Section
4 we will prove that since Λ2 is not identical to Λ∞ there exists a tradeoff in the robustness of any
decision boundary between the two norms.
4	A Provab le Tradeoff in Robustness B etween Norms
Schott et al. (2018) explore the vulnerability of robust classifiers to attacks under different norms.
In particular, they take the robust pretrained classifier of Madry et al. (2018), which was trained to
be robust to ∣∣ ∙ ∣∣∞-perturbations, and subject it to ∣∣ ∙ ∣o and ∣∣ ∙ ∣∣2 attacks. They show that accuracy
drops to 0% under ∣∣ ∙ ∣o attacks and to 35% under ∣∣ ∙ ∣2. Here we explain why poor robustness under
the norm ∣∣ ∙ ∣∣2 should be expected.
4
Under review as a conference paper at ICLR 2019
Robustness Tradeoff with FGSM Attack
Figure 2: As the dimension increases, the rch2 (Λ∞; Si ∪ S2) decreases, and so an ∣∣ ∙ ∣∣∞ robust
classifier is less robust to ∣∣ ∙ ∣2 attacks. The dashed lines are placed at 1∕√d, where our theoretical
results predict We should start finding ∣∣ ∙ ∣∣2 adversarial examples. We use the robust ∣∙∣∞ loss of
Wong & Kolter (2018)
We say a decision boundary Df for a classifier f is e-robust in the ∣∣ ∙ ∣∣p norm if e < rchp Df. In
words, starting from any point x ∈ M, a perturbation ηx must have p-norm greater than rchp Df
to cross the decision boundary. The most robust decision boundary to ∣∣ ∙ ∣∣p-perturbations is Λp. In
Theorem 1 we construct a learning setting where Λ2 is distinct from Λ∞. Thus, in general, no single
decision boundary can be optimally robust in all norms.
Theorem 1.	Let S1, S2 ⊂ Rd+1 be two concentric d-spheres with radii r1 < r2 respectively. Let
S = Si ∪ S2 and let Λ2, Λ∞ be the ∣∣ ∙ ∣2 and ∣ ∙ ∣∞ decision axesof S. Then Λ2 = Λ∞. Furthermore
rch2 Λ∞ ∈ O(rch2 Λ2∕√d).
From Theorem 1 we conclude that the minimum distance from Si to Λ∞ under the ∣∣ ∙ ∣2 norm is
upper bounded as rch2 Λ∞ ∈ O(rch2 Λ2 /√d). If a classifier f is trained to learn Λ∞, an adversary,
starting on Si, can construct an ∣∣ ∙ ∣2 adversarial example for a perturbation as small as O(1∕√d).
Thus we should expect f to be less robust to ∣∣ ∙ 12-perturbations. Figure 2 verifies this result
experimentally. The proof of Theorem 1 is provided in Appendix A
We expect that Λ2 6= Λ∞ is the common case in practice. For example, Theorem 1 extends im-
mediately to concentric cylinders and intertwined tori by considering 2-dimensional planar cross-
sections. In general, we expect that Λ2 6= Λ∞ in situations where a 2-dimensional cross-section
with M has nontrivial curvature.
Theorem 1 is important because, even in recent literature, researchers have attributed this phenom-
ena to overfitting. Schott et al. (2018) state that “the widely recognized and by far most successful
defense by Madry et al. (1) overfits on the L∞ metric (it’s highly susceptible to L2 and L0 per-
turbations)” (emphasis ours). We disagree; the Madry et al. (2018) classifier performed exactly as
intended. It learned a decision boundary that is robust under ∣∣ ∙ ∣∣∞, which we have shown is quite
different from the most robust decision boundary under ∣∣ ∙ ∣∣2.
Interestingly, the proposed models of Schott et al. (2018) also suffer from this tradeoff. Their model
ABS has accuracy 80% to ∣∣ ∙ ∣∣2 attacks but drops to 8% for ∣∣ ∙ ∣∣∞. Similarly their model ABS
Binary has accuracy 77% to ∣∣∙∣∣∞ attacks but drops to 39% for ∣∣ ∙ ∣2 attacks.
We reiterate, in general, no single decision boundary can be optimally robust in all norms.
5 X IS A POOR MODEL OF M
Madry et al. (2018) suggest training a robust classifier with the help of an adversary which, at
each iteration, produces e-perturbations around the training set that are incorrectly classified. In
our notation, this corresponds to learning a decision boundary that correctly classifies X = {x ∈
Rd : ∣x-Xi∣2 ≤ e for some training point Xi}. We believe this approach is insufficiently robust in
practice, as X is often a poor model for M. In this section, we show that the volume vol X is often
5
Under review as a conference paper at ICLR 2019
Figure 3: Left: To construct an δ-cover We place sample points, shown here in black, along a
regular grid with spacing ∆. The blue points are the furthest points of Π from the sample. To cover
Π we need ∆ = 2δ∕√k. Right: An illustration of the lower bound technique used in Equation 3.
The volume vol Πδ shown in the black dashed lines, is bounded from below by placing a (d - k)-
dimensional ball of radius δ at each point of Π, shown in green. In this illustration, a 1-dimensional
manifold is embedded in 2 dimensions, so these balls are 1-dimensional line segments.
a vanishingly small percentage of vol Me. These results shed light on why the ball-based learning
algorithm L defined in Section 6 is so much less sample-efficient than nearest neighbor classifiers.
In Section 7.1 we experimentally verify these observations by showing that in high-dimensional
space it is easy to find adversarial examples even after training against a strong adversary. For the
remainder of this section we will consider the ∣∣ ∙ k2 norm.
Theorem 2.	Let M ⊂ Rd be a k-dimensional manifold embedded in Rd such that volk M < ∞.
Let X ⊂ M be a finite set of points sampled from M. Suppose that ≤ rch2 Ξ where Ξ is the
medial axis of M, defined as in Dey (2007). Then the percentage of Me covered by Xe is upper
bounded by
山I ≤ TkkR崂 + I) —x| ∈ O ((工 V，|X|
vol Me — Γ(d +1)	Volk M	d - k Volk M
(1)
As the codimension (d - k) → ∞, Equation 1 approaches 0, for any fixed |X|.
In high codimension, even moderate under-sampling ofM leads to a significant loss of coverage of
Me because the volume of the union of balls centered at the samples shrinks faster than the volume
of Me . Theorem 2 states that in high codimensions the fraction of Me covered by Xe goes to 0.
Almost nothing is covered by Xe for training set sizes that are realistic in practice. Thus X e is a
poor model of Me, and high classificaiton accuracy on Xe does not imply high accuracy in Me .
The proof of Theorem 2 is given in Appendix A.
Note that an alternative way of defining the ratio vol Xe/ vol Me is as vol (Xe ∩ Me)/ vol Me.
This is equivalent in our setting since X ⊂ M and so Xe ⊂ Me .
For the remainder of the section we provide intuition for Theorem 2 by considering the special case
of k-dimensional planes. Define Π = {x ∈ Rd : ' ≤ xι,..., Xk ≤ μ and xk+ι = ... = Xd = 0};
that is Π is a subset of the χι-.. .-Xk-plane bounded between the coordinates [', μ]. A δ-cover of a
manifold M in the norm ∣∣ ∙ ∣2 is a finite set of points X such that for every X ∈ M there exists Xi
such that ∣X - Xi ∣2 ≤ δ. It is easy to construct an explicit δ-cover X of Π: place sample points at
the vertices of a regular grid, shown in Figure 3 by the black vertices. The centers of the cubes of this
regular grid, shown in blue in Figure 3, are the furthest points from the samples. The distance from
the vertices of the grid to the centers is √k∆∕2 where ∆ is the spacing between points along an axis
of the grid. To construct a δ-cover we need √k∆∕2 = δ which gives a spacing of ∆ = 2δ∕√k. The
size of this sample is |X | = (^¾-)) . Note that |X | scales exponentially in k, the dimension of
Π, not in d, the dimension of the embedding space.
6
Under review as a conference paper at ICLR 2019
Recall that Πδ is the δ-tubular neighborhood of Π. The δ-balls around X, which comprise Xδ, cover
Π and so any robust approach that guarantees correct classification within Xδ will achieve perfect
accuracy on Π. However, we will show that Xδ covers only a vanishingly small fraction of Πδ . Let
Bδ denote the d-ball of radius δ centered at the origin. An upper bound on the volume of X δ is
vol Xδ ≤ vol Bδ |X| =	丁/2 δd (√k(μ - ') ] =	∏d/2	δ(d-k)
一 δl 1 Γ( d + 1)	[	2δ	J Γ( d + 1)
Next we bound the volume vol Πδ from below. Intuitively, a lower bound on the volume can be
derived by placing a (d - k)-dimensional ball in the normal space at each point of Π and integrating
the volumes. Figure 3 (Right) illustrates the lower bound argument in the case of k = 1, d = 2.
vol Πδ ≥ vold-k Bδd-k volk Π
∏(d-k)∕2
Γ (⅛k + 1)
δd-k(μ - ')k.
(3)
Combining Equations 2 and 3 gives an upper bound on the percentage of Πδ that is covered by X .
volXδ V πk∕2Γ (d-k + 1)√√k∖
vol∏δ ≤ Γ (d + 1)(^T J
(4)
Notice that the factors involving δ and (μ 一 ') cancel. Figure 4 (Left) shows that this expression
approaches 0 as the codimension (d - k) ofΠ increases.
Suppose we set δ = 1 and construct a 1-cover ofΠ. The number of points necessary to cover Π with
balls of radius 1 depends only on k, not the embedding dimension d. However the number of points
necessary to cover the tubular neighborhood Π1 with balls of radius 1 increases depends on both k
and d. In Theorem 3 we derive a lower bound on the number of samples necessary to cover Π1.
Theorem 3.	Let Π be a bounded k-flat as described above, bounded along each axis by ' < μ.
Let n denote the number of samples necessary to cover the 1-tubular neighborhood Π1 of Π with
k ∙ ∣∣2 -balls Ofradius 1. That is let n be the minimum value for which there exists a finite Sample X
of size n such that Π1 ⊂ ∪x∈X B (x, 1) = X1. Then
n ≥ J7( +；)(μ 一 ')k ∈ Ω ((F Y" (μ 一 Q .	⑸
一 Γ (d-k + 1)'产' U 2π 7	!
Theorem 3 states that, in general, it takes many fewer samples to accurately model M than to model
M. Figure 4 (Right) compares the number of points necessary to construct a 1-cover of Π with
the lower bound on the number necessary to cover Π1 from Theorem 3. The number of points
necessary to cover Π1 increases as Ω ((d - k)k∕2), scaling polynomially in d and exponentially in
k. In contrast, the number necessary to construct a 1-cover of Π remains constant asd increases,
depending only on k. The proof of Theorem 3 is given in Appendix A.
Our lower bound of Ω ((d — k)k∕2) samples is similar to the work of Schmidt et al. (2018) who
prove that, in the simple Gaussian setting, robustness requires as much as Ω(√d) more samples.
Their arguments are statistical while ours are geometric.
Approaches that produce robust classifiers by generating adversarial examples in the -balls centered
on the training set do not accurately model M, and it will take many more samples to do so. If
the method behaves arbitrarily outside of the -balls that define X, adversarial examples will still
exist and it will likely be easy to find them. The reason deep learning has performed so well on a
variety of tasks, in spite of the brittleness made apparent by adversarial examples, is because it is
much easier to perform well on M than it is to perform well on M.
6 Provably Robust Classifiers
Adversarial training, the process of training on adversarial examples generated ina ∣∣ ∙ ∣p-ball around
the training data, is a very natural approach to constructing robust models (Goodfellow et al. (2014);
7
Under review as a conference paper at ICLR 2019
—I-SamPIe of Π
—Lower Bound for Πδ
Figure 4: We plot the upper bound in Equation 4 on the left. As the codimension increases, the
percentage of volume of Π1 covered by 1-balls around the 1-sample approaches 0. On the right We
plot the number of samples necessary to cover Π, shown in blue, against the number of samples
necessary to cover Π1, shown in orange, as the codimension increases.
Madry et al. (2018)). In our notation this corresponds to training on samples drawn from Xe for
some e. While natural, we show that there are simple settings where this approach is much less
sample-efficient than other classification algorithms, if the only guarantee is correctness in Xe.
Define a learning algorithm L with the property that, given a training set X ⊂ M sampled from
a manifold M, L outputs a model fl such that for every X ∈ X with label y, and every X ∈
B(χ, rchp Λp), ∕l(X) = ∕l(x) = y. Here B(x, r) denotes the ball centered at X of radius r in the
relevant norm. That is, L learns a model that outputs the same label for any ∣∣ ∙ kp-perturbation of X
up to rchp Λp as it outputs for X. L is our theoretical model of adversarial training (Goodfellow et al.
(2014); Madry et al. (2018)). Theorem 4 states that L is sample inefficient in high codimensions.
Theorem 4.	There exists a classification algorithm A that, for a particular choice of M, correctly
classifies M using exponentially fewer samples than are required for L to correctly classify M.
Theorem 4 follows from Theorems 5 and 6. In Theorems 5 and 6 we will prove that a nearest
neighbor classifier fnn is one such classification algorithm. Nearest neighbor classifiers are naturally
robust in high codimensions because the Voronoi cells of X are elongated in the directions normal
to M when X is dense (Dey (2007)).
Recall that a δ-cover of a manifold M in the norm ∣∣ ∙ ∣p is a finite set of points X such that for every
X ∈ M there exists Xi such that ∣X - Xi∣p ≤ δ. Theorem 5 gives a sufficient sampling condition
for fL to correctly classify M for all manifolds M. Theorem 5 also provides a sufficient sampling
condition for a nearest neighbor classifier fnn to correctly classify M, which is substantially less
dense than that of fL. Thus different classification algorithms have different sampling requirements
in high codimensions.
Theorem 5.	Let M ⊂ Rd be a k-dimensional manifold and let e < rchp Λp for any p. Let fnn be
a nearest neighbor classifier and let fL be the output of a learning algorithm L as described above.
Let Xnn , XL ⊂ M denote the training sets for fnn and L respectively. We have the following
sampling guarantees:
1.	If Xnn is a δ-cover for δ ≤ 2(rchp Λp - e) then fnn correctly classifies M.
2.	If XL is a δ-cover for δ ≤ rchp Λp - e then fL correctly classifies M.
The bounds on δ in Theorem 5 are sufficient, but they are not always necessary. There exist mani-
folds where the bounds in Theorem 5 are pessimistic, and less dense samples corresponding to larger
values of δ would suffice. In Theorem 6 we show a setting where bounds on δ similar to those in
Theorem 5 are necessary. In this setting, the difference of a factor of 2 in δ between the sampling
requirements of fnn and fL leads to an exponential gap between the sizes of Xnn and XL necessary
to achieve the same amount of robustness.
Consider two subsets of k-flats Π1, Π2, as defined in Section 5, where Π1 lies in the subspace Xd = 0
and ∏2 lies in the subspace Xd = 1; thus rch2 Λ2 = 1. In the ∣∣ ∙ ∣2 norm we can show that the gap
in Theorem 5 is necessary for Π = Π1 ∪ Π2. Furthermore the bounds we derive for δ-covers for Π
for both fnn and fL are tight. Combined with well-known properties of covers, we get that the ratio
|XL |/|Xnn | is exponential in k.
8
Under review as a conference paper at ICLR 2019
FGSM Attack VS CodimenSion (Adam)	BIM Attack VS CodimenSion (Adam)	Adverarial Training with BIM Attack (Adam)
—Codim=I
Codim=10
—codim=100
—codim=500
Figure 5: As the codimension increases the robustness of decision boundaries learned by Adam on
naturally trained networks decreases steadily. Left: Effectiveness of FGSM attacks as codimension
increases. Center: BIM attacks. Right: Training using the adversarial training procedure of Madry
et al. (2018) is no guarantee of robustness; as the codimension increases it becomes easier to find
adversarial examples using BIM attacks. Appendix B.4 shows the performance on nearest neighbor
on this data, which is essentially perfect accuracy for all e.
Theorem 6.	Let Π = Π1 ∪ Π2 as described above. Let Xnn , XL ⊂ Π be minimum training sets
necessary to guarantee that fnn and fL correctly classify M. Then we have that
XLI ≥ 2k/2
κnj ≥
(6)
We have shown that both L and nearest neighbor classifiers learn robust decision boundaries when
provided sufficiently dense samples of M. However there are settings where nearest neighbors
is exponentially more sample-efficient than L in achieving the same amount of robustness. We
experimentally verify these theoretical results in Section 7.1. Proofs for all of the results in this
section are provided in Appendix A.
7	Experiments
7.1	High Codimension Reduces Robustness
Section 5 suggests that as the codimension increases it should become easier to find adversarial ex-
amples. To verify this, we introduce two synthetic datasets, Circles and Planes, which allow us
to carefully vary the codimension while maintaining dense samples. The Circles dataset consists
of two concentric circles in the x1-x2-plane, with rch2 Λ2 = 1. We densely sample 1000 random
points on each circle for both the training and the test sets. The Planes dataset consists of two
2-dimensional planes, the first in the xd = 0 and the second in xd = 2, so that rch2 Λ2 = 1. We
sample the training set at the vertices of the grid described in Section 5, and the test set at the cen-
ters of the grid cubes, the blue points in Figure 3. Further details are provided in Appendix E and
visualizations in Appendix H.
We consider two attacks, the fast gradient sign method (FGSM) (Goodfellow et al. (2014)) and the
basic iterative method (BIM) (Kurakin et al. (2016)) under ∣∣ ∙ ∣∣2. We use the implementations pro-
vided in the cleverhans library (Papernot et al. (2018)). Further implementation details are provided
in Appendix E. Our experimental results are averaged over 20 retrainings of our model architecture,
using Adam (Kingma & Ba (2015)). Further implementation details are provided in Appendix E.
Figure 5(Left, Center) shows FGSM and BIM attacks on the Circles dataset as we vary the codi-
mension. For both attacks we see a steady decrease in robustness as we increase the codimension,
on average.
Madry et al. (2018) propose training against a PGD adversary to improve robustness. Section 5
suggests that this should be insufficient to guarantee robustness, as X is often a poor model for
Me. We train against a PGD adversary with e = 1 under ∣∣ ∙ 12-perturbations on the Planes
dataset. Figure 5 (Right) shows that it is still easy to find adversarial examples for e < 1 and
that as the codimension increases we can find adversarial examples for decreasing values of e. In
contrast, nearest neighbor achieves perfect robustness for all epsilon on this data (see Appendix B.4
for details).
9
Under review as a conference paper at ICLR 2019
Figure 6: Robustness of nearest neighbors on MNIST. Left: Performance on l∞ BIM attack against
a naturally trained model. Center: The same for the adversarially trained convolutional models of
Madry et al. (2018). Right: Performance of the robust model and nearest neighbors on examples
generated by a custom attack on nearest neighbors.
7.2	MNIST
To explore performance on a more realistic dataset, we compared nearest neighbors with robust
and natural models on MNIST. We considered two attacks: BIM under l∞ norm against the natural
and robust models as well as a custom attack against nearest neighbors. Each of these attacks are
generated from the MNIST test set. Architecture details can be found in Appendix E. Figure 6 (Left)
shows that nearest neighbors is substantially more robust to BIM attacks than the naturally trained
model. Figure 6 (Center) shows that nearest neighbors is comparable to the robust model up to
= 0.3, which is the value for which the robust model was trained. After = 0.3, nearest neighbors
is substantially more robust to BIM attacks than the robust model. At = 0.5, nearest neighbors
maintains accuracy of 78% to adversarial perturbations that cause the accuracy of the robust model
to drop to 0%. In Appendix B.2 we provide a similar result for FGSM attacks.
Figure 6 (Right) shows the performance of nearest neighbors and the robust model on adversarial
examples generated for nearest neighbors. The nearest neighbor attacks are generated as follows:
iteratively find the k nearest neighbors and compute an attack direction by walking away from the
neighbors in the true class and toward the neighbors in other classes. We find that nearest neighbors
is able to be tricked by this approach, but the robust model is not. This indicates that the errors of
these models are distinct and suggests that ensemble methods may effectively get the best of both
worlds. Additionally, a closer investigation shows strong qualitative differences between the BIM
adversarial examples and the examples generated for nearest neighbors. Appendix J argues that the
adversarial examples that fool nearest neighbor line up better with human intuition.
8	Conclusion
We have presented a geometric framework for proving robustness guarantees for learning algo-
rithms. Our framework is general and can be used to describe the robustness of any classifier. We
have shown that no single model can be simultaneously robust to attacks under all norms and that
nearest neighbor classifiers are theoretically more sample efficient than adversarial training. Most
importantly, we have highlighted the role of codimension in contributing to adversarial examples
and verified our theoretical contributions with experimental results.
We believe that a geometric understanding of the decision boundaries learned by deep networks
will lead to both new geometrically inspired attacks and defenses. In Appendix C we provide a
novel gradient-free geometric attack in support of this claim. Finally we believe future work into the
geometric properties of decision boundaries learned by various optimization procedures will provide
new techniques for black-box attacks.
References
Nina Amenta and Marshall W. Bern. Surface reconstruction by voronoi filtering. Discrete & Com-
putational Geometry, 22, 1999.
Nina Amenta, Marshall W. Bern, and David Eppstein. The crust and the beta-skeleton: Combinato-
rial curve reconstruction. Graphical Models and Image Processing, 1998.
10
Under review as a conference paper at ICLR 2019
Nina Amenta, Sunghee Choi, Tamal K. Dey, and N. Leekha. A simple algorithm for homeomor-
phic surface reconstruction. International Journal of Computational Geometry and Applications,
2002.
Anish Athalye, Nicholas Carlini, and David A. Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. In ICML, 2018.
Harry Blum. A transformation for extracting new descriptors of shape. Models for Perception of
Speech and Visual Forms, 1967.
Jean-Daniel Boissonnat and Arijit Ghosh. Manifold reconstruction using tangential delaunay com-
plexes. Discrete & Computational Geometry, 51, 2014.
Wieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-based adversarial attacks: Reliable
attacks against black-box machine learning models. In ICLR, 2018.
Siu-Wing Cheng, Tamal K. Dey, Herbert Edelsbrunner, Michael A. Facello, and Shang-Hua Teng.
Sliver exudation. Journal of the ACM, 47, 2000.
Siu-Wing Cheng, Tamal K. Dey, and Edgar A. Ramos. Manifold reconstruction from point samples.
In Proceedings of the Symposium on Discrete Algorithms (SODA), 2005.
Siu-Wing Cheng, Tamal K. Dey, and Jonathan Richard Shewchuk. Delaunay Mesh Generation.
CRC Press, Boca Raton, Florida, December 2012.
FeliPe Codevilla, Matthias Muller, Alexey Dosovitskiy, Antonio Lopez, and Vladlen Koltun. End-
to-end driving via conditional imitation learning. In ICRA, 2018.
Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine Learning, 20, 1995.
Tamal K. Dey. Curve and Surface Reconstruction: Algorithms with Mathematical Analysis. Cam-
bridge University Press, 2007.
Tamal K. Dey and Samrat Goswami. Provable surface reconstruction from noisy samples. In Pro-
ceedings of the Symposium on Computational Geometry (SoCG), 2004.
Tamal K. Dey and Piyush Kumar. A simple provable algorithm for curve reconstruction. In Pro-
ceedings of the Symposium on Discrete Algorithms (SODA), 1999.
Tamal K. Dey, Joachim Giesen, Edgar A. Ramos, and Bardia Sadri. Critical points of the distance to
an epsilon-sampling of a surface and flow-complex-based surface reconstruction. In Proceedings
of the Symposium on Computational Geometry (SoCG), 2005.
Herbert Edelsbrunner and Nimish R. Shah. Triangulating Topological Spaces. International Journal
of Computational Geometry and Applications, August 1997.
Gamaleldin F. Elsayed, Dilip Krishnan, Hossein Mobahi, Kevin Regan, and Samy Bengio. Large
margin deep networks for classification. CoRR, abs/1803.05598, 2018. URL http://arxiv.
org/abs/1803.05598.
Andre Esteva, Brett Kuprel, Roberto A Novoa, Justin Ko, Susan M Swetter, Helen M Blau, and
Sebastian Thrun. Dermatologist-level classification of skin cancer with deep neural networks.
Nature, 2017.
Jean-Yves Franceschi, Alhussein Fawzi, and Omar Fawzi. Robustness of classifiers to uniform lp
and gaussian noise. In AISTATS, 2018.
Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S. Schoenholz, Maithra Raghu, Martin Wat-
tenberg, and Ian J. Goodfellow. Adversarial spheres. CoRR, abs/1801.02774, 2018. URL
http://arxiv.org/abs/1801.02774.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In ICLR, 2014.
11
Under review as a conference paper at ICLR 2019
Marc Khoury and Jonathan Richard Shewchuk. Fixed points of the restricted delaunay triangulation
operator. In Proceedings of the Symposium on Computational Geometry (SoCG), 2016.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convo-
lutional neural networks. In NIPS, 2012.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world. In
ICLR Workshop Track, 2016.
Sergey Levine, Nolan Wagener, and Pieter Abbeel. Learning contact-rich manipulation skills with
guided policy search. In ICRA, 2015.
Xuezhi Liang, Xiaobo Wang, Zhen Lei, Shengcai Liao, and Stan Z. Li. Soft-margin softmax for
deep classification. In ICONIP, 2017.
Weiyang Liu, Yandong Wen, Zhiding Yu, and Meng Yang. Large-margin softmax loss for convolu-
tional neural networks. In ICML, 2016.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In ICLR, 2018.
Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram
Swami. Practical black-box attacks against machine learning. In Proceedings of the Asia Confer-
ence on Computer and Communications Security. ACM, 2017.
Nicolas Papernot, Fartash Faghri, Nicholas Carlini, Ian Goodfellow, Reuben Feinman, Alexey Ku-
rakin, Cihang Xie, Yash Sharma, Tom Brown, Aurko Roy, Alexander Matyasko, Vahid Behzadan,
Karen Hambardzumyan, Zhishuai Zhang, Yi-Lin Juang, Zhi Li, Ryan Sheatsley, Abhibhav Garg,
Jonathan Uesato, Willi Gierke, Yinpeng Dong, David Berthelot, Paul Hendricks, Jonas Rauber,
and Rujun Long. Technical report on the cleverhans v2.1.0 adversarial examples library. arXiv
preprint arXiv:1610.00768, 2018.
Maithra Raghu, Ben Poole, Jon M. Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the
expressive power of deep neural networks. In ICML, 2017.
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial exam-
ples. In ICLR, 2018.
Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Ad-
versarially robust generalization requires more data. In NIPS, 2018.
Lukas Schott, Jonas Rauber, Matthias Bethge, and Wieland Brendel. Towards the first adversar-
ially robust neural network model on MNIST. CoRR, abs/1805.09190, 2018. URL https:
//arxiv.org/abs/1805.09190.
John Shawe-Taylor and Nello Cristianini. Kernel Methods for Pattern Analysis. Cambridge Univer-
sity Press, 2004.
Aman Sinha, Hongseok Namkoong, and John Duchi. Certifying some distributional robustness with
principled adversarial training. In ICLR, 2018.
Daniel Soudry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent on separable
data. In ICLR, 2018.
Shizhao Sun, Wei Chen, Liwei Wang, Xiaoguang Liu, and Tie-Yan Liu. On the depth of deep neural
networks: A theoretical view. In AAAI, 2016.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. CoRR, abs/1312.6199, 2013. URL
http://arxiv.org/abs/1312.6199.
Yizhen Wang, Somesh Jha, and Kamalika Chaudhuri. Analyzing the robustness of nearest neighbors
to adversarial examples. In ICML, 2018.
12
Under review as a conference paper at ICLR 2019
Ashia C. Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. In NIPS, 2017.
Eric Wong and J. Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In ICML, 2018.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin John-
son, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa,
Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa,
Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google’s neural
machine translation system: Bridging the gap between human and machine translation. CoRR,
abs/1609.08144, 2016. URL http://arxiv.org/abs/1609.08144.
Tianhang Zheng, Changyou Chen, and Kui Ren. Distributionally adversarial attack. CoRR,
abs/1808.05537, 2018. URL http://arxiv.org/abs/1808.05537.
A Omitted Proofs
A.1 Auxiliary Lemmas
Lemma 7. Let Mi, M2 ⊂ Rd be k-dimensional manifolds such that M∩M2 = 0. Let Λp be their
decision axis for any p and let γ : [0, 1] → Rd be any path such that γ(0) ∈ M1 and γ(1) ∈ M2.
Then γ ∩ M 6= 0, that is γ must cross the decision axis.
Proof. Define f1, f2 : [0, 1] → R as f1(t) = d(γ(t), M1) and f2(t) = d(γ(t), M2). Consider the
function g(t) = f1(t) - f2(t). Since M1 ∩ M2 = 0 and γ starts on M1 and terminates on M2
the function g(0) < 0 and g(1) > 0. Then, since g is continuous, the Intermediate Value Theorem
implies that there exists t1 ∈ [0, 1] such that g(t1) = 0. Thus d(γ(t1), M1) = d(γ(t1), M2), which
implies that γ(tι) is on the decision axis Λ.	□
Theorem 8. Let f be any classifier on M = M1 ∪ M2. The maximum accuracy achievable,
assuming a uniform distribution, on M is
1 vol(M1 ∩ M2)
1	-----ΓT~~:---：~:~T .
2	vol(M1 ∪ M2)
(7)
Proof. It is clearly optimal to classify points in vol(M1 \ M2 ) as class 1 and to classify points in
vol(M2 \ M1) as class 2. Such a classifier can only be wrong when points lie in this intersection.
For points in this intersection, the probability of a misclassification is 2 for any classification that f
makes. Thus, the probability of misclassification is
Ivol(MI ∩M2)
2vol(M1 ∪M2).
□
Corollary 9. For < rchp (Λp; M) there exists a decision boundary that correctly classifies M.
Proof. For E < rchp Λp, Me ∩ Λp = 0 and so Λp is one such decision boundary.	□
A.2 Proof of Theorem 1
Proof. The decision axis under ∣∣ ∙ ∣∣2, Λ2, is just the d-sphere with radius (ri + r2)∕2. However,
Λ∞ is not identical to Λ2 in this setting; in fact most Λ∞ of approaches S1 as d increases.
The geometry of a ∣∣ ∙ ∣∣∞-ball Bδ centered at m ∈ Rd with radius ∆ is that of a hypercube centered
at m with side length 2∆. To find a point on Λ∞ we place B∆ tangent to the north pole q of Si so
that the corners ofB∆ touch S2. The north pole has coordinate representation q = (0, . . . , 0, ri), the
13
Under review as a conference paper at ICLR 2019
center m = (0, . . . , 0, r1 + ∆), and a corner of B∆ can be expressed as p = (∆, . . . , ∆, r1 + 2∆).
Additionally we have the constraint that ∣p∣2 = r2 since p ∈ S2. Then we can solve for ∆ as
r22 = ∣p∣22 = (d - 1)∆2 + (r1 + 2∆)2 = (d + 3)∆2 + 4r1∆ + r12;
∆
d+3
where the last step follows from the quadratic formula and the fact that ∆ > 0. For fixed r1 , r2, the
value ∆ scales as O(1∕√d). Itfollows that rch2 Λ∞ ∈ O(rch2 Λ2∕√d).
□
A.3 Proof of Theorem 2
Proof. Assuming the balls centered on the samples in X are disjoint we get the upper bound
volX ≤ volB|X|
∏d∕2
r( d + 1)
d|X|.
(8)
This is identical to the reasoning in Equation 2.
The medial axis Ξ of M is defined as the closure of the set of all points in Rd that have two or more
closest points on M in the norm ∣∣ ∙ ∣∣2. The medial axis Ξ is similar to the decision axis Λ2, except
that the nearest points do not need to be on distinct class manifolds. For ≤ rch2 Ξ, we have the
lower bound
vol M ≥ vold-k Bd-k volk M
∏(d-k)∕2
□ d-k + 1)
d-k volk M.
(9)
Combining Equations 8 and 9 gives the result. To get the asymptotic result we apply Stirling’s
approximation to get
γ( d-k + 1) ≈
Γ( d + 1)〜
(2e)k/2
(2e)k/2
(2e)k/2
(d ― k)(d-k+1)/2
d(d+1)∕2
(d-k )(d+1)∕2
(d — k)k/2
(1― d)(d+1)/2
2
d-k
(d - k)k/2
k/2

The last step follows from the fact that lim√→∞(1 一 k/d)(d+1)/2 = e-k/2, where e is the base of
the natural logarithm.	□
A.4 Proof of Theorem 3
Proof. We first construct an upper bound by generously assuming that the balls centered at the
samples are disjoint. That is
vol X δ n vol Bδ
vol Πδ — vol Πδ .
(10)
To guarantee that Π1 ⊂ ∪x∈XB(x, 1) = X1 we set the left hand side of Equation 10 equal to 1 and
solve for n.
I _ vol Xδ
vol Πδ
≤
n
≥
≥
n vol Bδ
vol Πδ
vol Πδ
vol Bδ
π-k∕2Γ (d + 1) (μ 一 C k
Γ (d-k + 1)
δ
14
Under review as a conference paper at ICLR 2019
The last inequality follows from Equation 3. Setting δ = 1 gives the result. The asymptotic result is
similar to the argument in the proof of Theorem 2.	□
A.5 Proof of Theorem 5
Proof. We begin by proving (1). Let q ∈ M be any point in M . Suppose without loss of
generality that q ∈ Mi for some class i. The distance d(q, Mj) from q to any other data manifold
Mj, and thus any sample on Mj, is lower bounded by d(q, Mj) ≥ 2 rchp Λp - . It is then both
necessary and sufficient that there exists a x ∈ Mi such that d(q, x) < 2 rchp Λp - for fnn(q) = i.
(Necessary since a properly placed sample on Mj can achieve the lower bound on d(q, Mj ).) The
distance from q to the nearest sample x on Mi is d(q, x) ≤ + δ for some δ > 0. The question is
how large can we allow δ to be and still guarantee that fnn correctly classifies M ? We need
d(q, x) ≤ + δ ≤ 2 rchp Λp - ≤ d(q, Mj)
which implies that δ ≤ 2(rchp Λp-). It follows that a δ-cover with δ = 2(rchp Λp-) is sufficient,
and in some cases necessary, to guarantee that fnn correctly classifies M .
Next we prove (2). As before let q ∈ Mi. It is both necessary and sufficient for q ∈ Brchp Λp (x) for
some sample x ∈ Mi to guarantee that fL(q) = i, by definition of L. The distance to the nearest
sample X on Mi is d(q, x) ≤ E + δ for some δ > 0. Thus it suffices that δ ≤ rchp Λp - e.	□
A.6 Proof of Theorem 6
Proof. Let q ∈ Π1. Since Π1 is flat, the distance to from q to the nearest sample x ∈ Π1 is bounded
as ∣∣q - χ∣∣2 ≤ √e2 + δ2. For fnn(q) = 1 We need that kq - xk2 ≤ 2 - E, and so it suffices that
δ ≤ 2√1 - E. In this setting, this is also necessary; should δ be any larger a property placed sample
on Π2 can claim q in its Voronoi cell.
Similarly for fL(q) = 1 We need that kq - xk2 ≤ 1, and so it suffices that δ ≤ ʌ/l — e2 . In this
setting, this is also necessary; should δ be any larger, q lies outside of every ∣∣ ∙ ∣∣2-ball Bι(χ) and so
L is free to learn a decision boundary that misclassifies q.
Let N(δ, M) denote the size of the minimum δ-cover of M. Since Π is flat (has no curvature)
and since the intersection of Π With a d-ball centered at a point on Π is a k-ball, a standard volume
argument can be applied in the affine subspace aff Π to conclude that N(δ, Π) ∈ Θ (Volk Π∕δk).
So We have
N(√T-? ,Π) =2k( ɪ V2
N (2√T-E, Π)	V + E)
≥ 2k/2
Since Π is constant in both settings, the factor volk Π as Well as the constant factors hidden by
Θ(∙) cancel. (Note that we are using the fact that ∏ι, ∏2 have finite k-dimensional volume.) The
inequality folloWs from the fact that the expression (1 + E)-k/2 is monotonically decreasing on the
interval [0,1] and takes value 2-k/2 at E = 1.	□
B Additional Experiments
We present additional experiments to support our theoretical predictions. We reproduce the results
of Section 7 using different optimization algorithms (Section B.1) and attack methods (Section B.2).
These additional experiments are consistent with our conclusions in Section 7. Additionally we
provide evidence that adversarial perturbations lie mostly in the directions of the normal space (Sec-
tion B.3). We show that a nearest neighbor classifier is robust in high codimensions (Section B.4).
Finally we show that increasing the sampling density substantially does not notably improve the
robustness of adversarial training (Section B.5).
15
Under review as a conference paper at ICLR 2019
B.1	Reproducing Results using SGD
In Section 7.1 we showed that increasing the codimension reduces the robustness of the decision
boundaries learned by Adam on Circles. In Figure 7 we reproduce this result using SGD. Again
we see that as we increase the codimension the robustness decreases. SGD presents with much less
variances than Adam, which we attribute to implicit regularization that has been observed for SGD
(Soudry et al. (2018))
FGSM Attack vs Codimension(SGD)	BIM Attack vs Codimension(SGD)
Figure 7: As in the case of training with Adam, we see a steady decrease in robustness on the
Circles dataset as the codimension increases when training with SGD.
Next we consider the adversarial training procedure of Madry et al. (2018) using SGD instead of
Adam. We note that the authors of Madry et al. (2018) use Adam in their own experiments. Figure 8
shows that the result is consist with the result in Section 7.1. Again SGD presents with lower
variance.
Adverarial Training with BIM Attack(SGD)
Figure 8: Adverarial training with a PGD adversary, as in Figure 5, using SGD. Similarly we see a
drop in robustness as the codimension increases.
B.2	Reproducing Results using FGSM
In Section 7.1 We evaluated the robustness of nearest neighbors against BIM attacks under the ∣∣∙∣∣ ∞
on MNIST. In Figure 9 we evaluate the robustness of nearest neighbors against FGSM attacks under
the k ∙ k∞ on MNIST. We use the naturally pretrained (natural) and adversarially pretrained (robust)
convolutional models provided by Madry et al. (2018)1. Figure 9 (Left) shoWs that nearest neighbors
is substantially more robust to FGSM attacks than the naturally trained model. Figure 9 (Right)
shoWs that nearest neighbors is comparable to the robust model up to = 0.3, Which is the value for
Which the robust model Was trained. After = 0.3, nearest neighbors is substantially more robust to
FGSM attacks than the robust model. At = 0.5, nearest neighbors maintains accuracy of 78% to
adversarial perturbations that cause the accuracy of the robust model to drop to 39%.
1https://github.com/MadryLab/mnist_challenge
16
Under review as a conference paper at ICLR 2019
Robustness of Natural Model vs NN
—Natural
—1-NN
Figure 9: Robustness of nearest neighbors against the naturally trained (left) and adversarially
trained (right) convolutional models of Madry et al. (2018) against FGSM attacks under the ∣∣∙∣∣∞
norm on MNIST.
Robustness of Robust Model vs NN
—Robust
—1-NN
B.3	Adversarial Perturbations are in the Directions Normal to the Data
Manifold
Let ηx be an adversarial perturbation generated by FGSM with = 1 at x ∈ M. Note that the
adversarial example is constructed as X = X + &. In Figure 10 We plot a histogram of the angles
∠(ηx , NxM) between ηx and the normal space NxM for the CIRCLES dataset in codimensions
1, 10, 100, and 500. In codimension 1, 88% of adversarial perturbations make an angle of less than
10° with the normal space. Similarly in codimension 10, 97%, in codimension 100, 96%, and in
codimension 500, 93%. As Figure 10 shoWs, nearly all adversarial perturbations make an angle less
than 20° with the normal space. Our results are averaged over 20 retrainings of the model using
SGD.
Throughout this paper we’ve argued that high codimension is a key source of the pervasiveness
of adversarial examples. Figure 10 shows that adversarial perturbations are well aligned with the
normal space. When the codimension is high, there are many directions normal to the manifold and
thus many directions in which to construct adversarial perturbations.
B.4	Nearest Neighbors is Robust in High Codimension
In Section 7.1 we showed that the robustness of learned decision boundaries decreased as the codi-
mension increased. In Figure 11 we repeat the experiment in Figure 5, in which we measured the
robustnesss of our neural network models to FGSM attacks as the codimension increased. We repeat
this experiment using nearest neighbors to classify the adversarial examples generated by FGSM.
Figure 11 shows that nearest neighbors is robust even when the codimension is high, as long as the
low-dimensional data manifold is well sampled. This is a consquence of the fact that the Voronoi
cells of the samples are elongated in the directions normal to the data manifold when the sample is
dense.
B.5	Sampling More Densely
The PLANES dataset is sampled so that the trianing set is a 1-cover of the underlying planes, which
requires 450 sample points. Figure 13 shows the results of increasing the sampling density to a 0.5-
cover (1682 samples) and a 0.25-cover (6498 samples). Increasing the sampling density improves
the robustness of adversarial training at the same codimension and particularly in low-codimension.
However adversarial training with a substantially larger training set does not produce a classifier as
robust as a nearest neighbor classifier on a much smaller training set. Nearest neighbors is much
more sample efficient than adversarial training, as predicted by Theorem 5 and experimentally veri-
fied in Section B.4.
17
Under review as a conference paper at ICLR 2019
COdimensiOn 100
COdimensiOn 500
Frequency
800
Frequency
800
Figure 10: Histograms of the angle deviations of FGSM perturbations from the normal space for
the Circles dataset in codimensions 1 (upper right), 10 (upper left), 100 (lower left), 500 (lower
right). Nearly all perturbations make an angle of less than 20。with the normal space.
Accuracy
1.0
0.0
0.0
FGSM Attack VS CodimenSion for NN
0.8
0.6
0.4
0.2
0.2
0.4
0.6
0.8
1.
—codim=1
codim=10
Codim=100
—codim=500
Accuracy
BIM Attack VS CodimenSion for NN
—codim=1
codim=10
codim=100
—codim=500
.0
,e
Figure 11:	The FGSM (left) and BIM (right) perturbations that fooled our deep networks are cor-
rectly classified by a nearest neighbor classifier. Nearest neighbor classifiers are robust in high
codimension settings because their decision boundaries are elongated in the directions normal to the
data manifold.
C A Gradient-Free Geometric Attack
Most current attacks rely on the gradient of the loss function at a test sample to find a direction
towards the decision boundary. Partial resistance against such attacks can be achieved by obfuscating
the gradients, but Athalye et al. (2018) showed how to circumvent such defenses. Brendel et al.
(2018) propose a gradient-free attack for ∣∣ ∙ ∣∣2, that starts from a misclassifed point and walks
toward the original point.
In this section we propose a gradient-free attack that only requires oracle access to a model, meaning
We only query the model for a prediction. Consider a point X ∈ Xtest and the ∣∣ ∙ ∣∣p-ball Br (x)
centered at x of radius r. To construct an adversarial perturbation ηx ∈ Br (x), giving an adversarial
example X = x+加，We project every point in Xtest onto Br (x) and query the oracle for a prediction
18
Under review as a conference paper at ICLR 2019
Accuracy
1.0
0.0
0.0
NN for BIM Attack With Adversarial Training
0.8
0.6
0.4
0.2
0.2
0.4
0.6
0.8
Codim=I
Codim= 10
Codim= 100
codim=500
1.01
'£
Figure 12:	The BIM perturbations that fooled the adversarially trained model using the procedure
suggested by Madry et al. (2018) are correctly classied by a nearest neighbor classifier.
1-cover	0.5-Cover	0.25-Cover
—Codim=I
Codim=10
—codim=100
—codim=500
Figure 13: Adversarial training OfMadry et al. (2018) on the Planes dataset with a 1-cover (left),
consisting of 450 samples, a 0.5-cover (center), 1682 samples, and a 0.25-Cover (right), 6498 sam-
ples. Increasing the sampling density improves robustness at the same codimension. However even
training on a significantly denser training set does not produce a classifier as robust as a nearest
neighbor classifier on a much sparser training set, Figure 12
for each point. If y ∈ Xtest projected to a point y0 that the model classified differently than x, we
take ηx = y0 - x, otherwise ηx = 0. This incredibly simple attack reduces the accuracy of the
pretrained robust model of Madry et al. (2018) for ∣∣ ∙ k∞ and e = 0.3 to 90.6%, less than two
percent shy of the current SOTA for whitebox attacks, 88.79% (Zheng et al. (2018)).
Simple datasets, such as Circles and Planes, allow us to diagnose issues in learning algorithms
in settings where we understand how the algorithm should behave. For example Athalye et al. (2018)
state that the work of Madry et al. (2018) does not suffer from obfuscated gradients. In Appendix D
we provide evidence that Madry et al. (2018) does suffer from the obfuscated gradients problem,
failing one of Athalye et al. (2018)’s criteria for detecting obfuscated gradients.
D	The Madry Defense Suffers from Obfuscated Gradients
Athalye et al. (2018) identified the problem of “obfuscated gradients”, a type of a gradient masking
(Papernot et al. (2017)) that many proposed defenses employed to defend against adversarial exam-
ples. They identified three different types of obfuscated gradients: shattered gradients, stochastic
gradeints, and exploding/vanishing gradients. They examined nine recently proposed defenses, con-
cluded that seven suffered from at least one type of obfuscated gradient, and showed how to circum-
vent each type of obfuscated gradient and thus each defense that employed obfuscated gradients.
Regarding the work of Madry et al. (2018), Athalye et al. (2018) stated “We believe this approach
does not cause obfuscated gradients”. They note that “our experiments with optimization based
attacks do succeed with some probability”. In this section we provide evidence that the defense of
Madry et al. (2018) does suffer from obfuscated gradients, specifically shattered gradients. Shattered
gradients occur when a defence causes the gradient field to be “nonexistent or incorrect” (Athalye
et al. (2018)). Specifically we provide evidence that the defense of Madry et al. (2018) works by
shattering the gradient field of the loss function around the data manifolds.
19
Under review as a conference paper at ICLR 2019
In Figure 14 (Left) we show the normalized gradient field of the loss function for a network trained
on a 2-dimensional version of our Planes dataset using the adversarial training procedure of Madry
et al. (2018) with aPGD adversary. While the gradients have meaningful directions, Figure 14 (Left)
shows that magnitude of the gradient field is nearly 0 everywhere around the data manifolds, which
are at y = 0 and y = 2. The only notable gradients are near the decision axis which is at y = 1.
Normalized Gradient Field of Loss Function
Figure 14: (Left) The normalized gradient field of the loss for an adversarially trained network.
(Right) The magnitudes of the gradient. Notice that the gradients are largely 0 except at the decision
axis y = 1.
One criteria that Athalye et al. (2018) propose for identifying obfuscated gradients is whether one-
step attacks perform better than iterative attacks. The reason this criteria is useful for identifying
obfuscated gradients is because one-step attacks like FGSM first normalize the gradient, ignoring
its magnitude, then take as large of a step as allowed in the direction of the normalized gradient. So
long as the gradient on the manifold points towards the decision boundary, FGSM will be effective
at finding an adversarial example.
In Figure 15 we show the adversarial examples generated using PGD (left), FGSM (center), and BIM
(right) for = 1 starting at the test set for the PLANES dataset. FGSM produces adversarial examples
at the decision axis y = 1, exactly where we would expect. Notice that all of the adversarial
perturbation is normal to the data manifold, suggesting that the gradient on the manifold points
towards the decision boundary. However the adversarial examples produced by PGD lie closer to
the manifold from which the example was generated.
PGD splits the total perturbation between both the normal and the tangent spaces of the data mani-
fold, as shown by the arrows in Figure 15. This suggests that, when trained adversarially, the network
learned a gradient field that has small but correct gradients on the data manifold, but gradients that
curve in the tangent directions immediately off the manifold.
Lastly notice that BIM, another iterative method, also produces adversarial examples that are near
the decision axis. Athalye et al. (2018) cite success with iterative based optimization procedures as
evidence against obfuscated gradients. However BIM also ignores the magnitude of the gradient, as
it simply applies FGSM iteratively. The network has learned a gradient field that is overfit to the
particulars of the PGD attack. BIM successfully navigates this gradient field, while PGD does not.
While the network is robust to PGD attacks at test time, it is less robust to FGSM and BIM attacks.
E	Implementation Details
In Section 7 we introduced two synthetic datasets, Circles and Planes. The Circles dataset
consists of two concentric circles, the first with radius r1 = 1 and the second with radius r2 = 3, so
that the rch = 1. The PLANES dataset consists of two 2-dimensional planes, the first in the subspace
defined by xd = 0, and the second in xd = 2, so that rch = 1. The first two axis of both planes are
20
Under review as a conference paper at ICLR 2019
PGD Adversarial Examples
FGSM Adversarial Examples
2 0
BIM Adversarial Examples
2 0
Figure 15: Adverarial examples generated using PGD (left), FGSM (center), and BIM (right). While
the network is robust to PGD attacks, FGSM and BIM attacks are more effective because they ignore
the magnitude of the gradient. For PGD we draw arrows from the test sample to the adversarial
example generated from that point to aid the reader.
bounded as -10 ≤ x1, x2 ≤ 10, while x3 = . . . = xd-1 = 0. Both planes are sampled as described
in Section 5, so that X 1 covers the underlying planes, where X is the training set.
We consider three attacks, FGSM, BIM, and PGD, primarily under the ∣∣ ∙ k2 norm. For the iterative
attacks BIM and PGD, we set the number of iterations to 30 with a step size of step = 0.05 per
iteration.
Our controlled experiments on synthetic data consider a fully connected network with 1 hidden
layer, 100 hidden units, and ReLU activations. This model architecture is more than capable of
representing a nearly perfect robust decision boundary for both Circles and Planes, the latter
of which is linearly separable. We set the learning rate for Adam as α = 0.1, which we found to
work best for our datasets. The parameters for the exponential decay of the first and second moment
estimates were set to β1 = 0.9 and β2 = 0.999. We set the learning rate for SGD as α = 0.1 and
decrease the learning rate by a factor of 10 every 100 epochs. We train all of our models for 250
epochs, following Wilson et al. (2017).
All of our experiments are implemented using PyTorch. When comparing against a published result
we use publicly available repositories, if able. For the robust loss of Wong & Kolter (2018), we use
the code provided by the authors2.The provided implementation3 of the adversarial training proce-
dure ofMadry et al. (2018) considers a PGD adversary with ∣∣ ∙ ∣∞-perturbations. We reimplemented
their adversarial training procedure for ∣∣ ∙ 12-perturbations following their implementation and using
the PGD attack implemented in the cleverhans library (Papernot et al. (2018)).
The models of Madry et al. (2018) consist of two convolutional layers with 32 and 64 filters respec-
tively, each followed by 2 × 2 max pooling. After the two convolutional layers, there are two fully
connected layers each with 1024 hidden units.
F VOLUME ARGUMENTS FOR d- S PHERES
Let S ⊂ Rd+1 be a unit d-sphere embedded in Rd+1. The volume of S is given by
πd∕2((1 + e)d-(1-e)d)
vol S ——-------------J-------
r(i + d)
(11)
where Γ denotes the gamma function. Let X ⊂ S be a finite sample of size n of S . The set X is
the set of all e perturbations of points in X under the norm ∣∣ ∙ ∣∣2. How well does Xe approximate
S as a function of n, d and ?
To answer this question we upper bound the ratio volX/ vol S by generously assuming that the
balls B(Xi, e) are disjoint. The resulting upper bound is
volX n vol B	ned
vol Se ≤ vol Se ——(1 + e)d - (1 - e)d
2https://github.com/locuslab/convex_adversarial
3https://github.com/MadryLab/mnist_challenge
(12)
21
Under review as a conference paper at ICLR 2019
In Figure 16 we show three different views of this bound. In Figure 16 (Left) we set n = 1012 and
plot four different values of ; in each case the percentage of volume of S that is covered by X
quickly approaches 0. Similarly, in Figure 16 (Center), if we fix = 1 and plot four different values
of n, in each CaSe We have the same result. Finally in Figure 16 (Right) We plot a lower bound
on number of samples necessary to cover S£ by Xe for four different values of e; in each case the
number of samples necessary grows exponentially with the dimension.
Sample SiZe
—E=0.1
—e=0.2
—e=0.3
—e=0.4
Figure 16: Three different perspectives on our upper bound in Equation 12. (Left, Center) In each
case the percentage of S£ covered by X£ goes to 0. (Right) The number of points necessary to cover
S' by X' grows exponentially with the dimension.
G VORONOI Diagrams and DELAUNAY Triangulations
Let X ⊂ Rd be a finite set of n points. The Voronoi diagram of X, denoted Vor X, under the metric
d(∙, ∙) is a subdivision of Rd into n cells where each cell is defined as
VorV = {x ∈ Rd : d(x,v) ≤ d(x,u),∀u ∈ X∖{v}}.	(13)
In words, the Voronoi cell Vor V of V ∈ X is the set of all points in Rd that are closer to V than any
other sample point u 6= v in X . The Voronoi diagram is then defined as the set of all Voronoi cells,
Vor X = {Vor v : V ∈ X}. When d(∙, ∙) is induced by the norm ∣∣ ∙ ∣∣2, the Voronoi cells are convex.
see Figure 17.
Figure 17: The Voronoi diagram ofa set of points in R2 (left) and its dual the Delaunay triangulation
(right).
The Delaunay triangulation of X , denoted Del X is a triangulation of the convex hull of X into
d-simplices. Every d-simplex τ ∈ Del X, as well as every lower-dimensional face of τ , has the
defining property that there exists an empty circumscribing ball B such that the vertices of τ lie on
the boundary of B and the interior of B is free from any points in X . see Figure 17. This empty
circumscribing ball property of Delaunay triangulations implies many desirable properties that are
useful in mesh generation (Cheng et al. (2012)) and manifold reconstruction (Edelsbrunner & shah
(1997)). The Delaunay triangulation of a point set always exists, but is not unique in general.
There exists a well known duality between the Voronoi diagram and the Delaunay triangulation of
X. For every j-dimensional face σ ∈ VorX there exist a dual (d - j)-dimensional simplex denoted
σ* ∈ Del X whose d — j + 1 vertices are the d — j + 1 vertices of X whose Voronoi cells intersect
at σ. In particular, every d-cell of VorX is dual to the vertex of DelX that generates that cell, and
every (d — 1)-face of Vor X is dual to an edge of Del X .
22
Under review as a conference paper at ICLR 2019
A nearest neighbor classifier fnn given a query point q simply returns the class of the point in X that
generated the Voronoi cell in which q lies. Thus the decision boundary of fnn is the union of (d - 1)
and lower dimensional Voronoi faces. Furthermore, when X is a dense sample of a manifold M,
the Voronoi cells are well known to be elongated in the directions normal to M Dey (2007). This
fact underlies many of our results.
23
Under review as a conference paper at ICLR 2019
H VISUALIZATION OF DATASETS
In Figure 18 we provide visualizations of our two synthetic datasets, CIRCLES (left) and PLANES
(right).
Figure 18: We create two synthetic datasets which allow us to perform controlled experiments on
the affect of codimension on adversarial examples.
24
Under review as a conference paper at ICLR 2019
I Visualization of Decision Boundaries
In Figure 19 we provide visualizations of the decision boundaries learned by (a-d) our fully con-
nected network architecture with cross entropy loss for various optimization procedures and various
training lengths, (e) our fully connected network architecture trained using the robust loss of Wong
& Kolter (2018) for k ∙ k∞-perturbations, and (f) a nearest neighbor classifier for ∣∣ ∙ k2 on the train-
ing set. Specifically we train on the CIRCLES dataset, embedded in R3. The training set is entirely
contained in the xy-plane. We then visualize cross sections of the decision boundary for various
values of Z ∈ [-5, 5]. We color points labeled as in the same class as the outer circle with the color
blue and points labeled as in the same class as the inner circle as orange. Figure 19 shows the cross
sections of the decision boundaries, averaged over 20 retrainings. The visualization shows how var-
ious optimization algorithms learn decision boundaries that extend into the normal directions where
no data is provided.
(a) Decision boundary learned by running SGD for 25 epochs, averaged over 20 trainings.
(b) Decision boundary learned by running SGD for 250 epochs, averaged over 20 trainings.
(c) Decision boundary learned by running Adam for 25 epochs, averaged over 20 trainings.
(d) Decision boundary learned by running Adam for 250 epochs, averaged over 20 trainings.
-5.0 -4.5 -4.0 -3.5 -3.0 -2.5 -2.0 -1.5 -1.0 -0.5	0.0	0.5	1.0	1.5	2.0	2.5	3.0	3.5	4.0	4.5	5.0
□□□□□□□□□□□□□□□□□□□DO
(e)	Decision boundary learned using the robust loss of Wong & Kolter (2018) for the ∣∣∙∣∣∞ norm and running
Adam for 250 epochs, averaged over 20 trainings.
-5.0 -4.5 -4.o -3.5 -3.0 -2.5 -2.0 -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.o 4.5 5.0
□□□□□□□□□□□□□□□□□□□□□
(f)	Decision boundary of a nearest neighbor classifier for the ∣∣ ∙∣∣2 norm.
Figure 19: The training set lies entirely in the xy-plane, shown here at Z = 0. We visualize cross sec-
tions of the decision boundary for z ∈ [-5, 5] for various optimization algorithms training for differ-
ent lengths of time. The results show how various optimization algorithm learn decision boundaries
that extend into the normal directions in which there is no data provided. We average the decision
boundary over 20 retrainings, so faded results indicated how frequently a point was labeled a specific
class.
25
Under review as a conference paper at ICLR 2019
J Comparing Nearest Neighbor Adversarial Examples with BIM
Attack
BIM
Unperturbed
Nearest
Neighbors
Figure 20: Comparison of adversarial examples for nearest neighbor with adversarial examples
for Madry et al. (2018). The top row is the original data that the examples were generated from.
Each figure is labelled with the predictions from robust neural network. We observe an immediate
qualitative difference between the nearest neighbor examples and the BiM examples: the nearest
neighbors ones are starting to look like numbers from a target class! in fact, we can reasonably
argue that the classifications of the robust model that don’t change represent as much of an error
and being fooled by a standard adversarial example. For example the center right image would be
classified as an 8 by most people, but the neural network is confident it is a0. This provides evidence
that nearest neighbors is doing a better job of the learning the human decision boundary between
numbers.
26